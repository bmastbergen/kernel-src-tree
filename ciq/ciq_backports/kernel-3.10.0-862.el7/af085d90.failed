stacktrace/x86: add function for detecting reliable stack traces

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Josh Poimboeuf <jpoimboe@redhat.com>
commit af085d9084b48530153f51e6cad19fd0b1a13ed7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/af085d90.failed

For live patching and possibly other use cases, a stack trace is only
useful if it can be assured that it's completely reliable.  Add a new
save_stack_trace_tsk_reliable() function to achieve that.

Note that if the target task isn't the current task, and the target task
is allowed to run, then it could be writing the stack while the unwinder
is reading it, resulting in possible corruption.  So the caller of
save_stack_trace_tsk_reliable() must ensure that the task is either
'current' or inactive.

save_stack_trace_tsk_reliable() relies on the x86 unwinder's detection
of pt_regs on the stack.  If the pt_regs are not user-mode registers
from a syscall, then they indicate an in-kernel interrupt or exception
(e.g. preemption or a page fault), in which case the stack is considered
unreliable due to the nature of frame pointers.

It also relies on the x86 unwinder's detection of other issues, such as:

- corrupted stack data
- stack grows the wrong way
- stack walk doesn't reach the bottom
- user didn't provide a large enough entries array

Such issues are reported by checking unwind_error() and !unwind_done().

Also add CONFIG_HAVE_RELIABLE_STACKTRACE so arch-independent code can
determine at build time whether the function is implemented.

	Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
	Reviewed-by: Miroslav Benes <mbenes@suse.cz>
	Acked-by: Ingo Molnar <mingo@kernel.org>	# for the x86 changes
	Signed-off-by: Jiri Kosina <jkosina@suse.cz>
(cherry picked from commit af085d9084b48530153f51e6cad19fd0b1a13ed7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/Kconfig
#	arch/x86/Kconfig
#	arch/x86/kernel/unwind_frame.c
#	include/linux/stacktrace.h
diff --cc arch/Kconfig
index 4fa0b0631a54,6ad00ad73459..000000000000
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@@ -549,6 -700,36 +549,39 @@@ config ARCH_MMAP_RND_COMPAT_BIT
  	  This value can be changed after boot using the
  	  /proc/sys/vm/mmap_rnd_compat_bits tunable
  
++<<<<<<< HEAD
++=======
+ config HAVE_COPY_THREAD_TLS
+ 	bool
+ 	help
+ 	  Architecture provides copy_thread_tls to accept tls argument via
+ 	  normal C parameter passing, rather than extracting the syscall
+ 	  argument from pt_regs.
+ 
+ config HAVE_STACK_VALIDATION
+ 	bool
+ 	help
+ 	  Architecture supports the 'objtool check' host tool command, which
+ 	  performs compile-time stack metadata validation.
+ 
+ config HAVE_RELIABLE_STACKTRACE
+ 	bool
+ 	help
+ 	  Architecture has a save_stack_trace_tsk_reliable() function which
+ 	  only returns a stack trace if it can guarantee the trace is reliable.
+ 
+ config HAVE_ARCH_HASH
+ 	bool
+ 	default n
+ 	help
+ 	  If this is set, the architecture provides an <asm/hash.h>
+ 	  file which provides platform-specific implementations of some
+ 	  functions in <linux/hash.h> or fs/namei.c.
+ 
+ config ISA_BUS_API
+ 	def_bool ISA
+ 
++>>>>>>> af085d9084b4 (stacktrace/x86: add function for detecting reliable stack traces)
  #
  # ABI hall of shame
  #
diff --cc arch/x86/Kconfig
index 616af1b6625a,2a26852c11b6..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -78,62 -159,23 +78,70 @@@ config X8
  	select HAVE_PERF_EVENTS_NMI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
++<<<<<<< HEAD
 +	select HAVE_DEBUG_KMEMLEAK
 +	select ANON_INODES
 +	select HAVE_ALIGNED_STRUCT_PAGE if SLUB
 +	select HAVE_CMPXCHG_LOCAL
 +	select HAVE_CMPXCHG_DOUBLE
 +	select HAVE_ARCH_KMEMCHECK
++=======
+ 	select HAVE_REGS_AND_STACK_ACCESS_API
+ 	select HAVE_RELIABLE_STACKTRACE		if X86_64 && FRAME_POINTER && STACK_VALIDATION
+ 	select HAVE_STACK_VALIDATION		if X86_64
+ 	select HAVE_SYSCALL_TRACEPOINTS
+ 	select HAVE_UNSTABLE_SCHED_CLOCK
++>>>>>>> af085d9084b4 (stacktrace/x86: add function for detecting reliable stack traces)
  	select HAVE_USER_RETURN_NOTIFIER
 -	select IRQ_FORCED_THREADING
 -	select PERF_EVENTS
 -	select RTC_LIB
 -	select RTC_MC146818_LIB
 +	select ARCH_HAS_ELF_RANDOMIZE
 +	select HAVE_ARCH_JUMP_LABEL
 +	select HAVE_TEXT_POKE_SMP
 +	select HAVE_GENERIC_HARDIRQS
 +	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
  	select SPARSE_IRQ
 -	select SRCU
 -	select SYSCTL_EXCEPTION_TRACE
 -	select THREAD_INFO_IN_TASK
 -	select USER_STACKTRACE_SUPPORT
 +	select GENERIC_FIND_FIRST_BIT
 +	select GENERIC_IRQ_PROBE
 +	select GENERIC_PENDING_IRQ if SMP
 +	select GENERIC_IRQ_SHOW
 +	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 +	select IRQ_FORCED_THREADING
 +	select USE_GENERIC_SMP_HELPERS if SMP
 +	select HAVE_BPF_JIT if X86_64
 +	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 +	select HAVE_ARCH_HUGE_VMAP if X86_64 || (X86_32 && X86_PAE)
 +	select CLKEVT_I8253
 +	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 +	select GENERIC_IOMAP
 +	select DCACHE_WORD_ACCESS
 +	select GENERIC_SMP_IDLE_THREAD
 +	select ARCH_WANT_IPC_PARSE_VERSION if X86_32
 +	select HAVE_ARCH_SECCOMP_FILTER
 +	select BUILDTIME_EXTABLE_SORT
 +	select GENERIC_CMOS_UPDATE
 +	select HAVE_ARCH_SOFT_DIRTY
 +	select GENERIC_CLOCKEVENTS
 +	select ARCH_CLOCKSOURCE_DATA if X86_64
 +	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
 +	select GENERIC_CLOCKEVENTS_BROADCAST if X86_64 || (X86_32 && X86_LOCAL_APIC)
 +	select GENERIC_TIME_VSYSCALL if X86_64
 +	select KTIME_SCALAR if X86_32
 +	select GENERIC_STRNCPY_FROM_USER
 +	select GENERIC_STRNLEN_USER
 +	select HAVE_CONTEXT_TRACKING if X86_64
 +	select HAVE_IRQ_TIME_ACCOUNTING
  	select VIRT_TO_BUS
 -	select X86_FEATURE_NAMES		if PROC_FS
 +	select MODULES_USE_ELF_REL if X86_32
 +	select MODULES_USE_ELF_RELA if X86_64
 +	select CLONE_BACKWARDS if X86_32
 +	select ARCH_USE_BUILTIN_BSWAP
 +	select ARCH_USE_QUEUED_SPINLOCKS
 +	select ARCH_USE_QUEUED_RWLOCKS
 +	select OLD_SIGSUSPEND3 if X86_32 || IA32_EMULATION
 +	select OLD_SIGACTION if X86_32
 +	select COMPAT_OLD_SIGACTION if IA32_EMULATION
 +	select RTC_LIB
 +	select HAVE_CC_STACKPROTECTOR
 +	select HAVE_STACK_VALIDATION if X86_64
  
  config INSTRUCTION_DECODER
  	def_bool y
diff --cc arch/x86/kernel/unwind_frame.c
index a2456d4d286a,5ed43910e04b..000000000000
--- a/arch/x86/kernel/unwind_frame.c
+++ b/arch/x86/kernel/unwind_frame.c
@@@ -48,15 -145,114 +48,48 @@@ bool unwind_next_frame(struct unwind_st
  	if (unwind_done(state))
  		return false;
  
 -	/* have we reached the end? */
 -	if (state->regs && user_mode(state->regs))
 -		goto the_end;
 -
 -	if (is_last_task_frame(state)) {
 -		regs = task_pt_regs(state->task);
 -
 -		/*
 -		 * kthreads (other than the boot CPU's idle thread) have some
 -		 * partial regs at the end of their stack which were placed
 -		 * there by copy_thread_tls().  But the regs don't have any
 -		 * useful information, so we can skip them.
 -		 *
 -		 * This user_mode() check is slightly broader than a PF_KTHREAD
 -		 * check because it also catches the awkward situation where a
 -		 * newly forked kthread transitions into a user task by calling
 -		 * do_execve(), which eventually clears PF_KTHREAD.
 -		 */
 -		if (!user_mode(regs))
 -			goto the_end;
 -
 -		/*
 -		 * We're almost at the end, but not quite: there's still the
 -		 * syscall regs frame.  Entry code doesn't encode the regs
 -		 * pointer for syscalls, so we have to set it manually.
 -		 */
 -		state->regs = regs;
 -		state->bp = NULL;
 -		return true;
 -	}
 -
 -	/* get the next frame pointer */
 -	if (state->regs)
 -		next_bp = (unsigned long *)state->regs->bp;
 -	else
 -		next_bp = (unsigned long *)READ_ONCE_TASK_STACK(state->task,*state->bp);
 -
 -	/* is the next frame pointer an encoded pointer to pt_regs? */
 -	regs = decode_frame_pointer(next_bp);
 -	if (regs) {
 -		next_frame = (unsigned long *)regs;
 -		next_len = sizeof(*regs);
 -	} else {
 -		next_frame = next_bp;
 -		next_len = FRAME_HEADER_SIZE;
 -	}
 +	next_bp = (unsigned long *)*state->bp;
  
  	/* make sure the next frame's data is accessible */
 -	if (!update_stack_state(state, next_frame, next_len)) {
 -		/*
 -		 * Don't warn on bad regs->bp.  An interrupt in entry code
 -		 * might cause a false positive warning.
 -		 */
 -		if (state->regs)
 -			goto the_end;
 -
 -		goto bad_address;
 -	}
 -
 -	/* Make sure it only unwinds up and doesn't overlap the last frame: */
 -	if (state->stack_info.type == prev_type) {
 -		if (state->regs && (void *)next_frame < (void *)state->regs + regs_size(state->regs))
 -			goto bad_address;
 -
 -		if (state->bp && (void *)next_frame < (void *)state->bp + FRAME_HEADER_SIZE)
 -			goto bad_address;
 -	}
 +	if (!update_stack_state(state, next_bp, FRAME_HEADER_SIZE))
 +		return false;
  
  	/* move to the next frame */
 -	if (regs) {
 -		state->regs = regs;
 -		state->bp = NULL;
 -	} else {
 -		state->bp = next_bp;
 -		state->regs = NULL;
 -	}
 -
 +	state->bp = next_bp;
  	return true;
++<<<<<<< HEAD
++=======
+ 
+ bad_address:
+ 	state->error = true;
+ 
+ 	/*
+ 	 * When unwinding a non-current task, the task might actually be
+ 	 * running on another CPU, in which case it could be modifying its
+ 	 * stack while we're reading it.  This is generally not a problem and
+ 	 * can be ignored as long as the caller understands that unwinding
+ 	 * another task will not always succeed.
+ 	 */
+ 	if (state->task != current)
+ 		goto the_end;
+ 
+ 	if (state->regs) {
+ 		printk_deferred_once(KERN_WARNING
+ 			"WARNING: kernel stack regs at %p in %s:%d has bad 'bp' value %p\n",
+ 			state->regs, state->task->comm,
+ 			state->task->pid, next_frame);
+ 		unwind_dump(state, (unsigned long *)state->regs);
+ 	} else {
+ 		printk_deferred_once(KERN_WARNING
+ 			"WARNING: kernel stack frame pointer at %p in %s:%d has bad value %p\n",
+ 			state->bp, state->task->comm,
+ 			state->task->pid, next_frame);
+ 		unwind_dump(state, state->bp);
+ 	}
+ the_end:
+ 	state->stack_info.type = STACK_TYPE_UNKNOWN;
+ 	return false;
++>>>>>>> af085d9084b4 (stacktrace/x86: add function for detecting reliable stack traces)
  }
  EXPORT_SYMBOL_GPL(unwind_next_frame);
  
diff --cc include/linux/stacktrace.h
index 115b570e3bff,4205f71a5f0e..000000000000
--- a/include/linux/stacktrace.h
+++ b/include/linux/stacktrace.h
@@@ -18,8 -18,12 +18,10 @@@ extern void save_stack_trace_regs(struc
  				  struct stack_trace *trace);
  extern void save_stack_trace_tsk(struct task_struct *tsk,
  				struct stack_trace *trace);
+ extern int save_stack_trace_tsk_reliable(struct task_struct *tsk,
+ 					 struct stack_trace *trace);
  
  extern void print_stack_trace(struct stack_trace *trace, int spaces);
 -extern int snprint_stack_trace(char *buf, size_t size,
 -			struct stack_trace *trace, int spaces);
  
  #ifdef CONFIG_USER_STACKTRACE_SUPPORT
  extern void save_stack_trace_user(struct stack_trace *trace);
@@@ -32,6 -36,8 +34,12 @@@
  # define save_stack_trace_tsk(tsk, trace)		do { } while (0)
  # define save_stack_trace_user(trace)			do { } while (0)
  # define print_stack_trace(trace, spaces)		do { } while (0)
++<<<<<<< HEAD
 +#endif
++=======
+ # define snprint_stack_trace(buf, size, trace, spaces)	do { } while (0)
+ # define save_stack_trace_tsk_reliable(tsk, trace)	({ -ENOSYS; })
+ #endif /* CONFIG_STACKTRACE */
++>>>>>>> af085d9084b4 (stacktrace/x86: add function for detecting reliable stack traces)
  
- #endif
+ #endif /* __LINUX_STACKTRACE_H */
* Unmerged path arch/Kconfig
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/include/asm/unwind.h b/arch/x86/include/asm/unwind.h
index c4b6d1cafa46..63b1656ff0ef 100644
--- a/arch/x86/include/asm/unwind.h
+++ b/arch/x86/include/asm/unwind.h
@@ -11,6 +11,7 @@ struct unwind_state {
 	unsigned long stack_mask;
 	struct task_struct *task;
 	int graph_idx;
+	bool error;
 #ifdef CONFIG_FRAME_POINTER
 	unsigned long *bp;
 #else
@@ -37,6 +38,11 @@ void unwind_start(struct unwind_state *state, struct task_struct *task,
 	__unwind_start(state, task, regs, first_frame);
 }
 
+static inline bool unwind_error(struct unwind_state *state)
+{
+	return state->error;
+}
+
 #ifdef CONFIG_FRAME_POINTER
 
 static inline
diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c
index 9ee98eefc44d..30affe8c3945 100644
--- a/arch/x86/kernel/stacktrace.c
+++ b/arch/x86/kernel/stacktrace.c
@@ -85,6 +85,101 @@ void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 
+#ifdef CONFIG_HAVE_RELIABLE_STACKTRACE
+
+#define STACKTRACE_DUMP_ONCE(task) ({				\
+	static bool __section(.data.unlikely) __dumped;		\
+								\
+	if (!__dumped) {					\
+		__dumped = true;				\
+		WARN_ON(1);					\
+		show_stack(task, NULL);				\
+	}							\
+})
+
+static int __save_stack_trace_reliable(struct stack_trace *trace,
+				       struct task_struct *task)
+{
+	struct unwind_state state;
+	struct pt_regs *regs;
+	unsigned long addr;
+
+	for (unwind_start(&state, task, NULL, NULL); !unwind_done(&state);
+	     unwind_next_frame(&state)) {
+
+		regs = unwind_get_entry_regs(&state);
+		if (regs) {
+			/*
+			 * Kernel mode registers on the stack indicate an
+			 * in-kernel interrupt or exception (e.g., preemption
+			 * or a page fault), which can make frame pointers
+			 * unreliable.
+			 */
+			if (!user_mode(regs))
+				return -EINVAL;
+
+			/*
+			 * The last frame contains the user mode syscall
+			 * pt_regs.  Skip it and finish the unwind.
+			 */
+			unwind_next_frame(&state);
+			if (!unwind_done(&state)) {
+				STACKTRACE_DUMP_ONCE(task);
+				return -EINVAL;
+			}
+			break;
+		}
+
+		addr = unwind_get_return_address(&state);
+
+		/*
+		 * A NULL or invalid return address probably means there's some
+		 * generated code which __kernel_text_address() doesn't know
+		 * about.
+		 */
+		if (!addr) {
+			STACKTRACE_DUMP_ONCE(task);
+			return -EINVAL;
+		}
+
+		if (save_stack_address(trace, addr, false))
+			return -EINVAL;
+	}
+
+	/* Check for stack corruption */
+	if (unwind_error(&state)) {
+		STACKTRACE_DUMP_ONCE(task);
+		return -EINVAL;
+	}
+
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+
+	return 0;
+}
+
+/*
+ * This function returns an error if it detects any unreliable features of the
+ * stack.  Otherwise it guarantees that the stack trace is reliable.
+ *
+ * If the task is not 'current', the caller *must* ensure the task is inactive.
+ */
+int save_stack_trace_tsk_reliable(struct task_struct *tsk,
+				  struct stack_trace *trace)
+{
+	int ret;
+
+	if (!try_get_task_stack(tsk))
+		return -EINVAL;
+
+	ret = __save_stack_trace_reliable(trace, tsk);
+
+	put_task_stack(tsk);
+
+	return ret;
+}
+#endif /* CONFIG_HAVE_RELIABLE_STACKTRACE */
+
 /* Userspace stacktrace - based on kernel/trace/trace_sysprof.c */
 
 struct stack_frame_user {
@@ -147,4 +242,3 @@ void save_stack_trace_user(struct stack_trace *trace)
 	if (trace->nr_entries < trace->max_entries)
 		trace->entries[trace->nr_entries++] = ULONG_MAX;
 }
-
* Unmerged path arch/x86/kernel/unwind_frame.c
* Unmerged path include/linux/stacktrace.h
diff --git a/kernel/stacktrace.c b/kernel/stacktrace.c
index 00fe55cc5a82..a47ae65b277a 100644
--- a/kernel/stacktrace.c
+++ b/kernel/stacktrace.c
@@ -26,8 +26,8 @@ void print_stack_trace(struct stack_trace *trace, int spaces)
 EXPORT_SYMBOL_GPL(print_stack_trace);
 
 /*
- * Architectures that do not implement save_stack_trace_tsk or
- * save_stack_trace_regs get this weak alias and a once-per-bootup warning
+ * Architectures that do not implement save_stack_trace_*()
+ * get these weak aliases and once-per-bootup warnings
  * (whenever this facility is utilized - for example by procfs):
  */
 __weak void
@@ -41,3 +41,11 @@ save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
 {
 	WARN_ONCE(1, KERN_INFO "save_stack_trace_regs() not implemented yet.\n");
 }
+
+__weak int
+save_stack_trace_tsk_reliable(struct task_struct *tsk,
+			      struct stack_trace *trace)
+{
+	WARN_ONCE(1, KERN_INFO "save_stack_tsk_reliable() not implemented yet.\n");
+	return -ENOSYS;
+}
