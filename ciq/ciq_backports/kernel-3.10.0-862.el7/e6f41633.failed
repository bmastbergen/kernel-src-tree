target/sbc: Add LBPRZ attribute + control CDB emulation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jamie Pocas <jamie.pocas@emc.com>
commit e6f41633cb79b55ead84b023c02035322c7827e7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e6f41633.failed

This change sets the LBPRZ flag in EVPD page b2h and READ CAPACITY (16)
based on a new unmap_zeroes_data device attribute. This flag is set
automatically for iblock based on underlying block device queue's
discard_zeroes_data flag.

	Signed-off-by: Jamie Pocas <jamie.pocas@emc.com>
	Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>
(cherry picked from commit e6f41633cb79b55ead84b023c02035322c7827e7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/target/target_core_configfs.c
diff --cc drivers/target/target_core_configfs.c
index 60cccf317ddb,affe4c393ebc..000000000000
--- a/drivers/target/target_core_configfs.c
+++ b/drivers/target/target_core_configfs.c
@@@ -584,17 -458,638 +584,621 @@@ EXPORT_SYMBOL(target_fabric_configfs_de
  // Stop functions called by external Target Fabrics Modules
  //############################################################################*/
  
 -static inline struct se_dev_attrib *to_attrib(struct config_item *item)
 -{
 -	return container_of(to_config_group(item), struct se_dev_attrib,
 -			da_group);
 -}
 -
  /* Start functions for struct config_item_type tb_dev_attrib_cit */
 -#define DEF_CONFIGFS_ATTRIB_SHOW(_name)					\
 -static ssize_t _name##_show(struct config_item *item, char *page)	\
 -{									\
 -	return snprintf(page, PAGE_SIZE, "%u\n", to_attrib(item)->_name); \
 -}
  
++<<<<<<< HEAD
 +CONFIGFS_EATTR_STRUCT(target_core_dev_attrib, se_dev_attrib);
 +CONFIGFS_EATTR_OPS(target_core_dev_attrib, se_dev_attrib, da_group);
 +
 +static struct configfs_item_operations target_core_dev_attrib_ops = {
 +	.show_attribute		= target_core_dev_attrib_attr_show,
 +	.store_attribute	= target_core_dev_attrib_attr_store,
++=======
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_model_alias);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_dpo);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_fua_write);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_fua_read);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_write_cache);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_ua_intlck_ctrl);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_tas);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_tpu);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_tpws);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_caw);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_3pc);
+ DEF_CONFIGFS_ATTRIB_SHOW(pi_prot_type);
+ DEF_CONFIGFS_ATTRIB_SHOW(hw_pi_prot_type);
+ DEF_CONFIGFS_ATTRIB_SHOW(pi_prot_format);
+ DEF_CONFIGFS_ATTRIB_SHOW(enforce_pr_isids);
+ DEF_CONFIGFS_ATTRIB_SHOW(is_nonrot);
+ DEF_CONFIGFS_ATTRIB_SHOW(emulate_rest_reord);
+ DEF_CONFIGFS_ATTRIB_SHOW(force_pr_aptpl);
+ DEF_CONFIGFS_ATTRIB_SHOW(hw_block_size);
+ DEF_CONFIGFS_ATTRIB_SHOW(block_size);
+ DEF_CONFIGFS_ATTRIB_SHOW(hw_max_sectors);
+ DEF_CONFIGFS_ATTRIB_SHOW(optimal_sectors);
+ DEF_CONFIGFS_ATTRIB_SHOW(hw_queue_depth);
+ DEF_CONFIGFS_ATTRIB_SHOW(queue_depth);
+ DEF_CONFIGFS_ATTRIB_SHOW(max_unmap_lba_count);
+ DEF_CONFIGFS_ATTRIB_SHOW(max_unmap_block_desc_count);
+ DEF_CONFIGFS_ATTRIB_SHOW(unmap_granularity);
+ DEF_CONFIGFS_ATTRIB_SHOW(unmap_granularity_alignment);
+ DEF_CONFIGFS_ATTRIB_SHOW(unmap_zeroes_data);
+ DEF_CONFIGFS_ATTRIB_SHOW(max_write_same_len);
+ 
+ #define DEF_CONFIGFS_ATTRIB_STORE_U32(_name)				\
+ static ssize_t _name##_store(struct config_item *item, const char *page,\
+ 		size_t count)						\
+ {									\
+ 	struct se_dev_attrib *da = to_attrib(item);			\
+ 	u32 val;							\
+ 	int ret;							\
+ 									\
+ 	ret = kstrtou32(page, 0, &val);					\
+ 	if (ret < 0)							\
+ 		return ret;						\
+ 	da->_name = val;						\
+ 	return count;							\
+ }
+ 
+ DEF_CONFIGFS_ATTRIB_STORE_U32(max_unmap_lba_count);
+ DEF_CONFIGFS_ATTRIB_STORE_U32(max_unmap_block_desc_count);
+ DEF_CONFIGFS_ATTRIB_STORE_U32(unmap_granularity);
+ DEF_CONFIGFS_ATTRIB_STORE_U32(unmap_granularity_alignment);
+ DEF_CONFIGFS_ATTRIB_STORE_U32(max_write_same_len);
+ 
+ #define DEF_CONFIGFS_ATTRIB_STORE_BOOL(_name)				\
+ static ssize_t _name##_store(struct config_item *item, const char *page,	\
+ 		size_t count)						\
+ {									\
+ 	struct se_dev_attrib *da = to_attrib(item);			\
+ 	bool flag;							\
+ 	int ret;							\
+ 									\
+ 	ret = strtobool(page, &flag);					\
+ 	if (ret < 0)							\
+ 		return ret;						\
+ 	da->_name = flag;						\
+ 	return count;							\
+ }
+ 
+ DEF_CONFIGFS_ATTRIB_STORE_BOOL(emulate_fua_write);
+ DEF_CONFIGFS_ATTRIB_STORE_BOOL(emulate_caw);
+ DEF_CONFIGFS_ATTRIB_STORE_BOOL(emulate_3pc);
+ DEF_CONFIGFS_ATTRIB_STORE_BOOL(enforce_pr_isids);
+ DEF_CONFIGFS_ATTRIB_STORE_BOOL(is_nonrot);
+ 
+ #define DEF_CONFIGFS_ATTRIB_STORE_STUB(_name)				\
+ static ssize_t _name##_store(struct config_item *item, const char *page,\
+ 		size_t count)						\
+ {									\
+ 	printk_once(KERN_WARNING					\
+ 		"ignoring deprecated ##_name## attribute\n");	\
+ 	return count;							\
+ }
+ 
+ DEF_CONFIGFS_ATTRIB_STORE_STUB(emulate_dpo);
+ DEF_CONFIGFS_ATTRIB_STORE_STUB(emulate_fua_read);
+ 
+ static void dev_set_t10_wwn_model_alias(struct se_device *dev)
+ {
+ 	const char *configname;
+ 
+ 	configname = config_item_name(&dev->dev_group.cg_item);
+ 	if (strlen(configname) >= 16) {
+ 		pr_warn("dev[%p]: Backstore name '%s' is too long for "
+ 			"INQUIRY_MODEL, truncating to 16 bytes\n", dev,
+ 			configname);
+ 	}
+ 	snprintf(&dev->t10_wwn.model[0], 16, "%s", configname);
+ }
+ 
+ static ssize_t emulate_model_alias_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	struct se_device *dev = da->da_dev;
+ 	bool flag;
+ 	int ret;
+ 
+ 	if (dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change model alias"
+ 			" while export_count is %d\n",
+ 			dev, dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (flag) {
+ 		dev_set_t10_wwn_model_alias(dev);
+ 	} else {
+ 		strncpy(&dev->t10_wwn.model[0],
+ 			dev->transport->inquiry_prod, 16);
+ 	}
+ 	da->emulate_model_alias = flag;
+ 	return count;
+ }
+ 
+ static ssize_t emulate_write_cache_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (flag && da->da_dev->transport->get_write_cache) {
+ 		pr_err("emulate_write_cache not supported for this device\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	da->emulate_write_cache = flag;
+ 	pr_debug("dev[%p]: SE Device WRITE_CACHE_EMULATION flag: %d\n",
+ 			da->da_dev, flag);
+ 	return count;
+ }
+ 
+ static ssize_t emulate_ua_intlck_ctrl_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	u32 val;
+ 	int ret;
+ 
+ 	ret = kstrtou32(page, 0, &val);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (val != 0 && val != 1 && val != 2) {
+ 		pr_err("Illegal value %d\n", val);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (da->da_dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change SE Device"
+ 			" UA_INTRLCK_CTRL while export_count is %d\n",
+ 			da->da_dev, da->da_dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 	da->emulate_ua_intlck_ctrl = val;
+ 	pr_debug("dev[%p]: SE Device UA_INTRLCK_CTRL flag: %d\n",
+ 		da->da_dev, val);
+ 	return count;
+ }
+ 
+ static ssize_t emulate_tas_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (da->da_dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change SE Device TAS while"
+ 			" export_count is %d\n",
+ 			da->da_dev, da->da_dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 	da->emulate_tas = flag;
+ 	pr_debug("dev[%p]: SE Device TASK_ABORTED status bit: %s\n",
+ 		da->da_dev, flag ? "Enabled" : "Disabled");
+ 
+ 	return count;
+ }
+ 
+ static ssize_t emulate_tpu_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/*
+ 	 * We expect this value to be non-zero when generic Block Layer
+ 	 * Discard supported is detected iblock_create_virtdevice().
+ 	 */
+ 	if (flag && !da->max_unmap_block_desc_count) {
+ 		pr_err("Generic Block Discard not supported\n");
+ 		return -ENOSYS;
+ 	}
+ 
+ 	da->emulate_tpu = flag;
+ 	pr_debug("dev[%p]: SE Device Thin Provisioning UNMAP bit: %d\n",
+ 		da->da_dev, flag);
+ 	return count;
+ }
+ 
+ static ssize_t emulate_tpws_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/*
+ 	 * We expect this value to be non-zero when generic Block Layer
+ 	 * Discard supported is detected iblock_create_virtdevice().
+ 	 */
+ 	if (flag && !da->max_unmap_block_desc_count) {
+ 		pr_err("Generic Block Discard not supported\n");
+ 		return -ENOSYS;
+ 	}
+ 
+ 	da->emulate_tpws = flag;
+ 	pr_debug("dev[%p]: SE Device Thin Provisioning WRITE_SAME: %d\n",
+ 				da->da_dev, flag);
+ 	return count;
+ }
+ 
+ static ssize_t pi_prot_type_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	int old_prot = da->pi_prot_type, ret;
+ 	struct se_device *dev = da->da_dev;
+ 	u32 flag;
+ 
+ 	ret = kstrtou32(page, 0, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (flag != 0 && flag != 1 && flag != 2 && flag != 3) {
+ 		pr_err("Illegal value %d for pi_prot_type\n", flag);
+ 		return -EINVAL;
+ 	}
+ 	if (flag == 2) {
+ 		pr_err("DIF TYPE2 protection currently not supported\n");
+ 		return -ENOSYS;
+ 	}
+ 	if (da->hw_pi_prot_type) {
+ 		pr_warn("DIF protection enabled on underlying hardware,"
+ 			" ignoring\n");
+ 		return count;
+ 	}
+ 	if (!dev->transport->init_prot || !dev->transport->free_prot) {
+ 		/* 0 is only allowed value for non-supporting backends */
+ 		if (flag == 0)
+ 			return count;
+ 
+ 		pr_err("DIF protection not supported by backend: %s\n",
+ 		       dev->transport->name);
+ 		return -ENOSYS;
+ 	}
+ 	if (!(dev->dev_flags & DF_CONFIGURED)) {
+ 		pr_err("DIF protection requires device to be configured\n");
+ 		return -ENODEV;
+ 	}
+ 	if (dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change SE Device PROT type while"
+ 		       " export_count is %d\n", dev, dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 
+ 	da->pi_prot_type = flag;
+ 
+ 	if (flag && !old_prot) {
+ 		ret = dev->transport->init_prot(dev);
+ 		if (ret) {
+ 			da->pi_prot_type = old_prot;
+ 			return ret;
+ 		}
+ 
+ 	} else if (!flag && old_prot) {
+ 		dev->transport->free_prot(dev);
+ 	}
+ 
+ 	pr_debug("dev[%p]: SE Device Protection Type: %d\n", dev, flag);
+ 	return count;
+ }
+ 
+ static ssize_t pi_prot_format_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	struct se_device *dev = da->da_dev;
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (!flag)
+ 		return count;
+ 
+ 	if (!dev->transport->format_prot) {
+ 		pr_err("DIF protection format not supported by backend %s\n",
+ 		       dev->transport->name);
+ 		return -ENOSYS;
+ 	}
+ 	if (!(dev->dev_flags & DF_CONFIGURED)) {
+ 		pr_err("DIF protection format requires device to be configured\n");
+ 		return -ENODEV;
+ 	}
+ 	if (dev->export_count) {
+ 		pr_err("dev[%p]: Unable to format SE Device PROT type while"
+ 		       " export_count is %d\n", dev, dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 
+ 	ret = dev->transport->format_prot(dev);
+ 	if (ret)
+ 		return ret;
+ 
+ 	pr_debug("dev[%p]: SE Device Protection Format complete\n", dev);
+ 	return count;
+ }
+ 
+ static ssize_t force_pr_aptpl_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 	if (da->da_dev->export_count) {
+ 		pr_err("dev[%p]: Unable to set force_pr_aptpl while"
+ 		       " export_count is %d\n",
+ 		       da->da_dev, da->da_dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 
+ 	da->force_pr_aptpl = flag;
+ 	pr_debug("dev[%p]: SE Device force_pr_aptpl: %d\n", da->da_dev, flag);
+ 	return count;
+ }
+ 
+ static ssize_t emulate_rest_reord_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (flag != 0) {
+ 		printk(KERN_ERR "dev[%p]: SE Device emulation of restricted"
+ 			" reordering not implemented\n", da->da_dev);
+ 		return -ENOSYS;
+ 	}
+ 	da->emulate_rest_reord = flag;
+ 	pr_debug("dev[%p]: SE Device emulate_rest_reord: %d\n",
+ 		da->da_dev, flag);
+ 	return count;
+ }
+ 
+ static ssize_t unmap_zeroes_data_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	bool flag;
+ 	int ret;
+ 
+ 	ret = strtobool(page, &flag);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (da->da_dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change SE Device"
+ 		       " unmap_zeroes_data while export_count is %d\n",
+ 		       da->da_dev, da->da_dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 	/*
+ 	 * We expect this value to be non-zero when generic Block Layer
+ 	 * Discard supported is detected iblock_configure_device().
+ 	 */
+ 	if (flag && !da->max_unmap_block_desc_count) {
+ 		pr_err("dev[%p]: Thin Provisioning LBPRZ will not be set"
+ 		       " because max_unmap_block_desc_count is zero\n",
+ 		       da->da_dev);
+ 	       return -ENOSYS;
+ 	}
+ 	da->unmap_zeroes_data = flag;
+ 	pr_debug("dev[%p]: SE Device Thin Provisioning LBPRZ bit: %d\n",
+ 		 da->da_dev, flag);
+ 	return 0;
+ }
+ 
+ /*
+  * Note, this can only be called on unexported SE Device Object.
+  */
+ static ssize_t queue_depth_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	struct se_device *dev = da->da_dev;
+ 	u32 val;
+ 	int ret;
+ 
+ 	ret = kstrtou32(page, 0, &val);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change SE Device TCQ while"
+ 			" export_count is %d\n",
+ 			dev, dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 	if (!val) {
+ 		pr_err("dev[%p]: Illegal ZERO value for queue_depth\n", dev);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (val > dev->dev_attrib.queue_depth) {
+ 		if (val > dev->dev_attrib.hw_queue_depth) {
+ 			pr_err("dev[%p]: Passed queue_depth:"
+ 				" %u exceeds TCM/SE_Device MAX"
+ 				" TCQ: %u\n", dev, val,
+ 				dev->dev_attrib.hw_queue_depth);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	da->queue_depth = dev->queue_depth = val;
+ 	pr_debug("dev[%p]: SE Device TCQ Depth changed to: %u\n", dev, val);
+ 	return count;
+ }
+ 
+ static ssize_t optimal_sectors_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	u32 val;
+ 	int ret;
+ 
+ 	ret = kstrtou32(page, 0, &val);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (da->da_dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change SE Device"
+ 			" optimal_sectors while export_count is %d\n",
+ 			da->da_dev, da->da_dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 	if (val > da->hw_max_sectors) {
+ 		pr_err("dev[%p]: Passed optimal_sectors %u cannot be"
+ 			" greater than hw_max_sectors: %u\n",
+ 			da->da_dev, val, da->hw_max_sectors);
+ 		return -EINVAL;
+ 	}
+ 
+ 	da->optimal_sectors = val;
+ 	pr_debug("dev[%p]: SE Device optimal_sectors changed to %u\n",
+ 			da->da_dev, val);
+ 	return count;
+ }
+ 
+ static ssize_t block_size_store(struct config_item *item,
+ 		const char *page, size_t count)
+ {
+ 	struct se_dev_attrib *da = to_attrib(item);
+ 	u32 val;
+ 	int ret;
+ 
+ 	ret = kstrtou32(page, 0, &val);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (da->da_dev->export_count) {
+ 		pr_err("dev[%p]: Unable to change SE Device block_size"
+ 			" while export_count is %d\n",
+ 			da->da_dev, da->da_dev->export_count);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (val != 512 && val != 1024 && val != 2048 && val != 4096) {
+ 		pr_err("dev[%p]: Illegal value for block_device: %u"
+ 			" for SE device, must be 512, 1024, 2048 or 4096\n",
+ 			da->da_dev, val);
+ 		return -EINVAL;
+ 	}
+ 
+ 	da->block_size = val;
+ 	if (da->max_bytes_per_io)
+ 		da->hw_max_sectors = da->max_bytes_per_io / val;
+ 
+ 	pr_debug("dev[%p]: SE Device block_size changed to %u\n",
+ 			da->da_dev, val);
+ 	return count;
+ }
+ 
+ CONFIGFS_ATTR(, emulate_model_alias);
+ CONFIGFS_ATTR(, emulate_dpo);
+ CONFIGFS_ATTR(, emulate_fua_write);
+ CONFIGFS_ATTR(, emulate_fua_read);
+ CONFIGFS_ATTR(, emulate_write_cache);
+ CONFIGFS_ATTR(, emulate_ua_intlck_ctrl);
+ CONFIGFS_ATTR(, emulate_tas);
+ CONFIGFS_ATTR(, emulate_tpu);
+ CONFIGFS_ATTR(, emulate_tpws);
+ CONFIGFS_ATTR(, emulate_caw);
+ CONFIGFS_ATTR(, emulate_3pc);
+ CONFIGFS_ATTR(, pi_prot_type);
+ CONFIGFS_ATTR_RO(, hw_pi_prot_type);
+ CONFIGFS_ATTR(, pi_prot_format);
+ CONFIGFS_ATTR(, enforce_pr_isids);
+ CONFIGFS_ATTR(, is_nonrot);
+ CONFIGFS_ATTR(, emulate_rest_reord);
+ CONFIGFS_ATTR(, force_pr_aptpl);
+ CONFIGFS_ATTR_RO(, hw_block_size);
+ CONFIGFS_ATTR(, block_size);
+ CONFIGFS_ATTR_RO(, hw_max_sectors);
+ CONFIGFS_ATTR(, optimal_sectors);
+ CONFIGFS_ATTR_RO(, hw_queue_depth);
+ CONFIGFS_ATTR(, queue_depth);
+ CONFIGFS_ATTR(, max_unmap_lba_count);
+ CONFIGFS_ATTR(, max_unmap_block_desc_count);
+ CONFIGFS_ATTR(, unmap_granularity);
+ CONFIGFS_ATTR(, unmap_granularity_alignment);
+ CONFIGFS_ATTR(, unmap_zeroes_data);
+ CONFIGFS_ATTR(, max_write_same_len);
+ 
+ /*
+  * dev_attrib attributes for devices using the target core SBC/SPC
+  * interpreter.  Any backend using spc_parse_cdb should be using
+  * these.
+  */
+ struct configfs_attribute *sbc_attrib_attrs[] = {
+ 	&attr_emulate_model_alias,
+ 	&attr_emulate_dpo,
+ 	&attr_emulate_fua_write,
+ 	&attr_emulate_fua_read,
+ 	&attr_emulate_write_cache,
+ 	&attr_emulate_ua_intlck_ctrl,
+ 	&attr_emulate_tas,
+ 	&attr_emulate_tpu,
+ 	&attr_emulate_tpws,
+ 	&attr_emulate_caw,
+ 	&attr_emulate_3pc,
+ 	&attr_pi_prot_type,
+ 	&attr_hw_pi_prot_type,
+ 	&attr_pi_prot_format,
+ 	&attr_enforce_pr_isids,
+ 	&attr_is_nonrot,
+ 	&attr_emulate_rest_reord,
+ 	&attr_force_pr_aptpl,
+ 	&attr_hw_block_size,
+ 	&attr_block_size,
+ 	&attr_hw_max_sectors,
+ 	&attr_optimal_sectors,
+ 	&attr_hw_queue_depth,
+ 	&attr_queue_depth,
+ 	&attr_max_unmap_lba_count,
+ 	&attr_max_unmap_block_desc_count,
+ 	&attr_unmap_granularity,
+ 	&attr_unmap_granularity_alignment,
+ 	&attr_unmap_zeroes_data,
+ 	&attr_max_write_same_len,
+ 	NULL,
++>>>>>>> e6f41633cb79 (target/sbc: Add LBPRZ attribute + control CDB emulation)
  };
 -EXPORT_SYMBOL(sbc_attrib_attrs);
 -
 -/*
 - * Minimal dev_attrib attributes for devices passing through CDBs.
 - * In this case we only provide a few read-only attributes for
 - * backwards compatibility.
 - */
 -struct configfs_attribute *passthrough_attrib_attrs[] = {
 -	&attr_hw_pi_prot_type,
 -	&attr_hw_block_size,
 -	&attr_hw_max_sectors,
 -	&attr_hw_queue_depth,
 -	NULL,
 -};
 -EXPORT_SYMBOL(passthrough_attrib_attrs);
  
 -TB_CIT_SETUP_DRV(dev_attrib, NULL, NULL);
 +TB_CIT_SETUP(dev_attrib, &target_core_dev_attrib_ops, NULL, NULL);
  
  /* End functions for struct config_item_type tb_dev_attrib_cit */
  
* Unmerged path drivers/target/target_core_configfs.c
diff --git a/drivers/target/target_core_device.c b/drivers/target/target_core_device.c
index 1e65ea95d7f2..e12e9f6e4787 100644
--- a/drivers/target/target_core_device.c
+++ b/drivers/target/target_core_device.c
@@ -1533,6 +1533,8 @@ struct se_device *target_alloc_device(struct se_hba *hba, const char *name)
 	dev->dev_attrib.unmap_granularity = DA_UNMAP_GRANULARITY_DEFAULT;
 	dev->dev_attrib.unmap_granularity_alignment =
 				DA_UNMAP_GRANULARITY_ALIGNMENT_DEFAULT;
+	dev->dev_attrib.unmap_zeroes_data =
+				DA_UNMAP_ZEROES_DATA_DEFAULT;
 	dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;
 
 	xcopy_lun = &dev->xcopy_lun;
diff --git a/drivers/target/target_core_iblock.c b/drivers/target/target_core_iblock.c
index 4ff1c8404aab..2eeb9670c0a9 100644
--- a/drivers/target/target_core_iblock.c
+++ b/drivers/target/target_core_iblock.c
@@ -144,6 +144,8 @@ static int iblock_configure_device(struct se_device *dev)
 				q->limits.discard_granularity >> 9;
 		dev->dev_attrib.unmap_granularity_alignment =
 				q->limits.discard_alignment;
+		dev->dev_attrib.unmap_zeroes_data =
+				q->limits.discard_zeroes_data;
 
 		pr_debug("IBLOCK: BLOCK Discard support available,"
 				" disabled by default\n");
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index 34e4e2e2de91..8464afb93ef6 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -140,9 +140,17 @@ sbc_emulate_readcapacity_16(struct se_cmd *cmd)
 	 * Set Thin Provisioning Enable bit following sbc3r22 in section
 	 * READ CAPACITY (16) byte 14 if emulate_tpu or emulate_tpws is enabled.
 	 */
-	if (dev->dev_attrib.emulate_tpu || dev->dev_attrib.emulate_tpws)
+	if (dev->dev_attrib.emulate_tpu || dev->dev_attrib.emulate_tpws) {
 		buf[14] |= 0x80;
 
+		/*
+		 * LBPRZ signifies that zeroes will be read back from an LBA after
+		 * an UNMAP or WRITE SAME w/ unmap bit (sbc3r36 5.16.2)
+		 */
+		if (dev->dev_attrib.unmap_zeroes_data)
+			buf[14] |= 0x40;
+	}
+
 	rbuf = transport_kmap_data_sg(cmd);
 	if (rbuf) {
 		memcpy(rbuf, buf, min_t(u32, sizeof(buf), cmd->data_length));
diff --git a/drivers/target/target_core_spc.c b/drivers/target/target_core_spc.c
index 819803d44776..9422d19c562c 100644
--- a/drivers/target/target_core_spc.c
+++ b/drivers/target/target_core_spc.c
@@ -647,6 +647,18 @@ spc_emulate_evpd_b2(struct se_cmd *cmd, unsigned char *buf)
 	if (dev->dev_attrib.emulate_tpws != 0)
 		buf[5] |= 0x40 | 0x20;
 
+	/*
+	 * The unmap_zeroes_data set means that the underlying device supports
+	 * REQ_DISCARD and has the discard_zeroes_data bit set. This satisfies
+	 * the SBC requirements for LBPRZ, meaning that a subsequent read
+	 * will return zeroes after an UNMAP or WRITE SAME (16) to an LBA
+	 * See sbc4r36 6.6.4.
+	 */
+	if (((dev->dev_attrib.emulate_tpu != 0) ||
+	     (dev->dev_attrib.emulate_tpws != 0)) &&
+	     (dev->dev_attrib.unmap_zeroes_data != 0))
+		buf[5] |= 0x04;
+
 	return 0;
 }
 
diff --git a/include/target/target_core_base.h b/include/target/target_core_base.h
index 6a27f0cf6579..d98f7f04458c 100644
--- a/include/target/target_core_base.h
+++ b/include/target/target_core_base.h
@@ -75,6 +75,8 @@
 #define DA_UNMAP_GRANULARITY_DEFAULT		0
 /* Default unmap_granularity_alignment */
 #define DA_UNMAP_GRANULARITY_ALIGNMENT_DEFAULT	0
+/* Default unmap_zeroes_data */
+#define DA_UNMAP_ZEROES_DATA_DEFAULT		0
 /* Default max_write_same_len, disabled by default */
 #define DA_MAX_WRITE_SAME_LEN			0
 /* Use a model alias based on the configfs backend device name */
@@ -685,6 +687,7 @@ struct se_dev_attrib {
 	int		force_pr_aptpl;
 	int		is_nonrot;
 	int		emulate_rest_reord;
+	int		unmap_zeroes_data;
 	u32		hw_block_size;
 	u32		block_size;
 	u32		hw_max_sectors;
