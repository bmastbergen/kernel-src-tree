scsi: lpfc: Replace PCI pool old API

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] lpfc: Replace PCI pool old API (Dick Kennedy) [1385844 1461977 1387768]
Rebuild_FUZZ: 90.91%
commit-author Romain Perier <romain.perier@collabora.com>
commit 771db5c0e3f5da592a871c4d457ea73df76ded12
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/771db5c0.failed

The PCI pool API is deprecated. This commit replaces the PCI pool old
API by the appropriate function with the DMA pool API. It also updates
some comments, accordingly.

	Signed-off-by: Romain Perier <romain.perier@collabora.com>
	Reviewed-by: Peter Senna Tschudin <peter.senna@collabora.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 771db5c0e3f5da592a871c4d457ea73df76ded12)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc.h
#	drivers/scsi/lpfc/lpfc_init.c
#	drivers/scsi/lpfc/lpfc_mem.c
#	drivers/scsi/lpfc/lpfc_nvme.c
#	drivers/scsi/lpfc/lpfc_nvmet.c
#	drivers/scsi/lpfc/lpfc_scsi.c
#	drivers/scsi/lpfc/lpfc_sli.c
diff --cc drivers/scsi/lpfc/lpfc.h
index 61c6751c0584,7039549cad02..000000000000
--- a/drivers/scsi/lpfc/lpfc.h
+++ b/drivers/scsi/lpfc/lpfc.h
@@@ -863,12 -946,14 +863,23 @@@ struct lpfc_hba 
  	struct list_head active_rrq_list;
  	spinlock_t hbalock;
  
++<<<<<<< HEAD
 +	/* pci_mem_pools */
 +	struct pci_pool *lpfc_scsi_dma_buf_pool;
 +	struct pci_pool *lpfc_mbuf_pool;
 +	struct pci_pool *lpfc_hrb_pool;	/* header receive buffer pool */
 +	struct pci_pool *lpfc_drb_pool; /* data receive buffer pool */
 +	struct pci_pool *lpfc_hbq_pool;	/* SLI3 hbq buffer pool */
++=======
+ 	/* dma_mem_pools */
+ 	struct dma_pool *lpfc_sg_dma_buf_pool;
+ 	struct dma_pool *lpfc_mbuf_pool;
+ 	struct dma_pool *lpfc_hrb_pool;	/* header receive buffer pool */
+ 	struct dma_pool *lpfc_drb_pool; /* data receive buffer pool */
+ 	struct dma_pool *lpfc_nvmet_drb_pool; /* data receive buffer pool */
+ 	struct dma_pool *lpfc_hbq_pool;	/* SLI3 hbq buffer pool */
+ 	struct dma_pool *txrdy_payload_pool;
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  	struct lpfc_dma_pool lpfc_mbuf_safety_pool;
  
  	mempool_t *mbox_mem_pool;
diff --cc drivers/scsi/lpfc/lpfc_init.c
index bad9d528b08b,9294c89c7ccd..000000000000
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@@ -3128,7 -3275,7 +3128,11 @@@ lpfc_scsi_free(struct lpfc_hba *phba
  	list_for_each_entry_safe(sb, sb_next, &phba->lpfc_scsi_buf_list_put,
  				 list) {
  		list_del(&sb->list);
++<<<<<<< HEAD
 +		pci_pool_free(phba->lpfc_scsi_dma_buf_pool, sb->data,
++=======
+ 		dma_pool_free(phba->lpfc_sg_dma_buf_pool, sb->data,
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  			      sb->dma_handle);
  		kfree(sb);
  		phba->total_scsi_bufs--;
@@@ -3139,25 -3286,58 +3143,75 @@@
  	list_for_each_entry_safe(sb, sb_next, &phba->lpfc_scsi_buf_list_get,
  				 list) {
  		list_del(&sb->list);
++<<<<<<< HEAD
 +		pci_pool_free(phba->lpfc_scsi_dma_buf_pool, sb->data,
++=======
+ 		dma_pool_free(phba->lpfc_sg_dma_buf_pool, sb->data,
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  			      sb->dma_handle);
  		kfree(sb);
  		phba->total_scsi_bufs--;
  	}
  	spin_unlock(&phba->scsi_buf_list_get_lock);
 +
 +	/* Release all the lpfc_iocbq entries maintained by this host. */
 +	list_for_each_entry_safe(io, io_next, &phba->lpfc_iocb_list, list) {
 +		list_del(&io->list);
 +		kfree(io);
 +		phba->total_iocbq_bufs--;
 +	}
 +
  	spin_unlock_irq(&phba->hbalock);
  }
 +
  /**
++<<<<<<< HEAD
 + * lpfc_sli4_xri_sgl_update - update xri-sgl sizing and mapping
++=======
+  * lpfc_nvme_free - Free all the NVME buffers and IOCBs from driver lists
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine is to free all the NVME buffers and IOCBs from the driver
+  * list back to kernel. It is called from lpfc_pci_remove_one to free
+  * the internal resources before the device is removed from the system.
+  **/
+ static void
+ lpfc_nvme_free(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_nvme_buf *lpfc_ncmd, *lpfc_ncmd_next;
+ 
+ 	if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME))
+ 		return;
+ 
+ 	spin_lock_irq(&phba->hbalock);
+ 
+ 	/* Release all the lpfc_nvme_bufs maintained by this host. */
+ 	spin_lock(&phba->nvme_buf_list_put_lock);
+ 	list_for_each_entry_safe(lpfc_ncmd, lpfc_ncmd_next,
+ 				 &phba->lpfc_nvme_buf_list_put, list) {
+ 		list_del(&lpfc_ncmd->list);
+ 		dma_pool_free(phba->lpfc_sg_dma_buf_pool, lpfc_ncmd->data,
+ 			      lpfc_ncmd->dma_handle);
+ 		kfree(lpfc_ncmd);
+ 		phba->total_nvme_bufs--;
+ 	}
+ 	spin_unlock(&phba->nvme_buf_list_put_lock);
+ 
+ 	spin_lock(&phba->nvme_buf_list_get_lock);
+ 	list_for_each_entry_safe(lpfc_ncmd, lpfc_ncmd_next,
+ 				 &phba->lpfc_nvme_buf_list_get, list) {
+ 		list_del(&lpfc_ncmd->list);
+ 		dma_pool_free(phba->lpfc_sg_dma_buf_pool, lpfc_ncmd->data,
+ 			      lpfc_ncmd->dma_handle);
+ 		kfree(lpfc_ncmd);
+ 		phba->total_nvme_bufs--;
+ 	}
+ 	spin_unlock(&phba->nvme_buf_list_get_lock);
+ 	spin_unlock_irq(&phba->hbalock);
+ }
+ /**
+  * lpfc_sli4_els_sgl_update - update ELS xri-sgl sizing and mapping
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
   * @phba: pointer to lpfc hba data structure.
   *
   * This routine first calculates the sizes of the current els and allocated
@@@ -3309,7 -3640,7 +3363,11 @@@ lpfc_sli4_xri_sgl_update(struct lpfc_hb
  			list_remove_head(&scsi_sgl_list, psb,
  					 struct lpfc_scsi_buf, list);
  			if (psb) {
++<<<<<<< HEAD
 +				pci_pool_free(phba->lpfc_scsi_dma_buf_pool,
++=======
+ 				dma_pool_free(phba->lpfc_sg_dma_buf_pool,
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  					      psb->data, psb->dma_handle);
  				kfree(psb);
  			}
@@@ -3387,6 -3716,105 +3445,108 @@@ lpfc_get_wwpn(struct lpfc_hba *phba
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_sli4_nvme_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_nvme_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_nvme_buf *lpfc_ncmd = NULL, *lpfc_ncmd_next = NULL;
+ 	uint16_t i, lxri, els_xri_cnt;
+ 	uint16_t nvme_xri_cnt, nvme_xri_max;
+ 	LIST_HEAD(nvme_sgl_list);
+ 	int rc;
+ 
+ 	phba->total_nvme_bufs = 0;
+ 
+ 	if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME))
+ 		return 0;
+ 	/*
+ 	 * update on pci function's allocated nvme xri-sgl list
+ 	 */
+ 
+ 	/* maximum number of xris available for nvme buffers */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	nvme_xri_max = phba->sli4_hba.max_cfg_param.max_xri - els_xri_cnt;
+ 	phba->sli4_hba.nvme_xri_max = nvme_xri_max;
+ 	phba->sli4_hba.nvme_xri_max -= phba->sli4_hba.scsi_xri_max;
+ 
+ 	lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 			"6074 Current allocated NVME xri-sgl count:%d, "
+ 			"maximum  NVME xri count:%d\n",
+ 			phba->sli4_hba.nvme_xri_cnt,
+ 			phba->sli4_hba.nvme_xri_max);
+ 
+ 	spin_lock_irq(&phba->nvme_buf_list_get_lock);
+ 	spin_lock(&phba->nvme_buf_list_put_lock);
+ 	list_splice_init(&phba->lpfc_nvme_buf_list_get, &nvme_sgl_list);
+ 	list_splice(&phba->lpfc_nvme_buf_list_put, &nvme_sgl_list);
+ 	spin_unlock(&phba->nvme_buf_list_put_lock);
+ 	spin_unlock_irq(&phba->nvme_buf_list_get_lock);
+ 
+ 	if (phba->sli4_hba.nvme_xri_cnt > phba->sli4_hba.nvme_xri_max) {
+ 		/* max nvme xri shrunk below the allocated nvme buffers */
+ 		spin_lock_irq(&phba->nvme_buf_list_get_lock);
+ 		nvme_xri_cnt = phba->sli4_hba.nvme_xri_cnt -
+ 					phba->sli4_hba.nvme_xri_max;
+ 		spin_unlock_irq(&phba->nvme_buf_list_get_lock);
+ 		/* release the extra allocated nvme buffers */
+ 		for (i = 0; i < nvme_xri_cnt; i++) {
+ 			list_remove_head(&nvme_sgl_list, lpfc_ncmd,
+ 					 struct lpfc_nvme_buf, list);
+ 			if (lpfc_ncmd) {
+ 				dma_pool_free(phba->lpfc_sg_dma_buf_pool,
+ 					      lpfc_ncmd->data,
+ 					      lpfc_ncmd->dma_handle);
+ 				kfree(lpfc_ncmd);
+ 			}
+ 		}
+ 		spin_lock_irq(&phba->nvme_buf_list_get_lock);
+ 		phba->sli4_hba.nvme_xri_cnt -= nvme_xri_cnt;
+ 		spin_unlock_irq(&phba->nvme_buf_list_get_lock);
+ 	}
+ 
+ 	/* update xris associated to remaining allocated nvme buffers */
+ 	lpfc_ncmd = NULL;
+ 	lpfc_ncmd_next = NULL;
+ 	list_for_each_entry_safe(lpfc_ncmd, lpfc_ncmd_next,
+ 				 &nvme_sgl_list, list) {
+ 		lxri = lpfc_sli4_next_xritag(phba);
+ 		if (lxri == NO_XRI) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 					"6075 Failed to allocate xri for "
+ 					"nvme buffer\n");
+ 			rc = -ENOMEM;
+ 			goto out_free_mem;
+ 		}
+ 		lpfc_ncmd->cur_iocbq.sli4_lxritag = lxri;
+ 		lpfc_ncmd->cur_iocbq.sli4_xritag = phba->sli4_hba.xri_ids[lxri];
+ 	}
+ 	spin_lock_irq(&phba->nvme_buf_list_get_lock);
+ 	spin_lock(&phba->nvme_buf_list_put_lock);
+ 	list_splice_init(&nvme_sgl_list, &phba->lpfc_nvme_buf_list_get);
+ 	INIT_LIST_HEAD(&phba->lpfc_nvme_buf_list_put);
+ 	spin_unlock(&phba->nvme_buf_list_put_lock);
+ 	spin_unlock_irq(&phba->nvme_buf_list_get_lock);
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_nvme_free(phba);
+ 	return rc;
+ }
+ 
+ /**
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
   * lpfc_create_port - Create an FC port
   * @phba: pointer to lpfc hba data structure.
   * @instance: a unique integer ID to this FC port.
@@@ -6320,6 -6842,23 +6480,26 @@@ lpfc_create_shost(struct lpfc_hba *phba
  
  	shost = lpfc_shost_from_vport(vport);
  	phba->pport = vport;
++<<<<<<< HEAD
++=======
+ 
+ 	if (phba->nvmet_support) {
+ 		/* Only 1 vport (pport) will support NVME target */
+ 		if (phba->txrdy_payload_pool == NULL) {
+ 			phba->txrdy_payload_pool = dma_pool_create(
+ 				"txrdy_pool", &phba->pcidev->dev,
+ 				TXRDY_PAYLOAD_LEN, 16, 0);
+ 			if (phba->txrdy_payload_pool) {
+ 				phba->targetport = NULL;
+ 				phba->cfg_enable_fc4_type = LPFC_ENABLE_NVME;
+ 				lpfc_printf_log(phba, KERN_INFO,
+ 						LOG_INIT | LOG_NVME_DISC,
+ 						"6076 NVME Target Found\n");
+ 			}
+ 		}
+ 	}
+ 
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  	lpfc_debugfs_initialize(vport);
  	/* Put reference to SCSI host to driver's device private data */
  	pci_set_drvdata(phba->pcidev, shost);
diff --cc drivers/scsi/lpfc/lpfc_mem.c
index 3fa65338d3f5,56faeb049b4a..000000000000
--- a/drivers/scsi/lpfc/lpfc_mem.c
+++ b/drivers/scsi/lpfc/lpfc_mem.c
@@@ -90,23 -96,25 +90,45 @@@ lpfc_mem_alloc(struct lpfc_hba *phba, i
  		else
  			i = SLI4_PAGE_SIZE;
  
++<<<<<<< HEAD
 +		phba->lpfc_scsi_dma_buf_pool =
 +			pci_pool_create("lpfc_scsi_dma_buf_pool",
 +				phba->pcidev,
 +				phba->cfg_sg_dma_buf_size,
 +				i,
 +				0);
 +	} else {
 +		phba->lpfc_scsi_dma_buf_pool =
 +			pci_pool_create("lpfc_scsi_dma_buf_pool",
 +				phba->pcidev, phba->cfg_sg_dma_buf_size,
 +				align, 0);
 +	}
 +
 +	if (!phba->lpfc_scsi_dma_buf_pool)
 +		goto fail;
 +
 +	phba->lpfc_mbuf_pool = pci_pool_create("lpfc_mbuf_pool", phba->pcidev,
++=======
+ 		phba->lpfc_sg_dma_buf_pool =
+ 			dma_pool_create("lpfc_sg_dma_buf_pool",
+ 					&phba->pcidev->dev,
+ 					phba->cfg_sg_dma_buf_size,
+ 					i, 0);
+ 		if (!phba->lpfc_sg_dma_buf_pool)
+ 			goto fail;
+ 
+ 	} else {
+ 		phba->lpfc_sg_dma_buf_pool =
+ 			dma_pool_create("lpfc_sg_dma_buf_pool",
+ 					&phba->pcidev->dev, phba->cfg_sg_dma_buf_size,
+ 					align, 0);
+ 
+ 		if (!phba->lpfc_sg_dma_buf_pool)
+ 			goto fail;
+ 	}
+ 
+ 	phba->lpfc_mbuf_pool = dma_pool_create("lpfc_mbuf_pool", &phba->pcidev->dev,
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  							LPFC_BPL_SIZE,
  							align, 0);
  	if (!phba->lpfc_mbuf_pool)
@@@ -176,8 -184,11 +198,14 @@@
  	}
  
  	return 0;
++<<<<<<< HEAD
++=======
+ fail_free_drb_pool:
+ 	dma_pool_destroy(phba->lpfc_drb_pool);
+ 	phba->lpfc_drb_pool = NULL;
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
   fail_free_hrb_pool:
- 	pci_pool_destroy(phba->lpfc_hrb_pool);
+ 	dma_pool_destroy(phba->lpfc_hrb_pool);
  	phba->lpfc_hrb_pool = NULL;
   fail_free_rrq_mem_pool:
  	mempool_destroy(phba->rrq_pool);
@@@ -194,15 -205,30 +222,38 @@@
  						 pool->elements[i].phys);
  	kfree(pool->elements);
   fail_free_lpfc_mbuf_pool:
- 	pci_pool_destroy(phba->lpfc_mbuf_pool);
+ 	dma_pool_destroy(phba->lpfc_mbuf_pool);
  	phba->lpfc_mbuf_pool = NULL;
   fail_free_dma_buf_pool:
++<<<<<<< HEAD
 +	pci_pool_destroy(phba->lpfc_scsi_dma_buf_pool);
 +	phba->lpfc_scsi_dma_buf_pool = NULL;
++=======
+ 	dma_pool_destroy(phba->lpfc_sg_dma_buf_pool);
+ 	phba->lpfc_sg_dma_buf_pool = NULL;
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
   fail:
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
++=======
+ int
+ lpfc_nvmet_mem_alloc(struct lpfc_hba *phba)
+ {
+ 	phba->lpfc_nvmet_drb_pool =
+ 		dma_pool_create("lpfc_nvmet_drb_pool",
+ 				&phba->pcidev->dev, LPFC_NVMET_DATA_BUF_SIZE,
+ 				SGL_ALIGN_SZ, 0);
+ 	if (!phba->lpfc_nvmet_drb_pool) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"6024 Can't enable NVME Target - no memory\n");
+ 		return -ENOMEM;
+ 	}
+ 	return 0;
+ }
+ 
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  /**
   * lpfc_mem_free - Frees memory allocated by lpfc_mem_alloc
   * @phba: HBA to free memory for
@@@ -221,15 -247,21 +272,27 @@@ lpfc_mem_free(struct lpfc_hba *phba
  
  	/* Free HBQ pools */
  	lpfc_sli_hbqbuf_free_all(phba);
++<<<<<<< HEAD
++=======
+ 	if (phba->lpfc_nvmet_drb_pool)
+ 		dma_pool_destroy(phba->lpfc_nvmet_drb_pool);
+ 	phba->lpfc_nvmet_drb_pool = NULL;
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  	if (phba->lpfc_drb_pool)
- 		pci_pool_destroy(phba->lpfc_drb_pool);
+ 		dma_pool_destroy(phba->lpfc_drb_pool);
  	phba->lpfc_drb_pool = NULL;
  	if (phba->lpfc_hrb_pool)
- 		pci_pool_destroy(phba->lpfc_hrb_pool);
+ 		dma_pool_destroy(phba->lpfc_hrb_pool);
  	phba->lpfc_hrb_pool = NULL;
++<<<<<<< HEAD
++=======
+ 	if (phba->txrdy_payload_pool)
+ 		dma_pool_destroy(phba->txrdy_payload_pool);
+ 	phba->txrdy_payload_pool = NULL;
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  
  	if (phba->lpfc_hbq_pool)
- 		pci_pool_destroy(phba->lpfc_hbq_pool);
+ 		dma_pool_destroy(phba->lpfc_hbq_pool);
  	phba->lpfc_hbq_pool = NULL;
  
  	if (phba->rrq_pool)
@@@ -258,8 -290,8 +321,13 @@@
  	phba->lpfc_mbuf_pool = NULL;
  
  	/* Free DMA buffer memory pool */
++<<<<<<< HEAD
 +	pci_pool_destroy(phba->lpfc_scsi_dma_buf_pool);
 +	phba->lpfc_scsi_dma_buf_pool = NULL;
++=======
+ 	dma_pool_destroy(phba->lpfc_sg_dma_buf_pool);
+ 	phba->lpfc_sg_dma_buf_pool = NULL;
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  
  	/* Free Device Data memory pool */
  	if (phba->device_data_mem_pool) {
@@@ -431,6 -463,44 +499,47 @@@ lpfc_mbuf_free(struct lpfc_hba * phba, 
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_nvmet_buf_alloc - Allocate an nvmet_buf from the
+  * lpfc_sg_dma_buf_pool PCI pool
+  * @phba: HBA which owns the pool to allocate from
+  * @mem_flags: indicates if this is a priority (MEM_PRI) allocation
+  * @handle: used to return the DMA-mapped address of the nvmet_buf
+  *
+  * Description: Allocates a DMA-mapped buffer from the lpfc_sg_dma_buf_pool
+  * PCI pool.  Allocates from generic dma_pool_alloc function.
+  *
+  * Returns:
+  *   pointer to the allocated nvmet_buf on success
+  *   NULL on failure
+  **/
+ void *
+ lpfc_nvmet_buf_alloc(struct lpfc_hba *phba, int mem_flags, dma_addr_t *handle)
+ {
+ 	void *ret;
+ 
+ 	ret = dma_pool_alloc(phba->lpfc_sg_dma_buf_pool, GFP_KERNEL, handle);
+ 	return ret;
+ }
+ 
+ /**
+  * lpfc_nvmet_buf_free - Free an nvmet_buf from the lpfc_sg_dma_buf_pool
+  * PCI pool
+  * @phba: HBA which owns the pool to return to
+  * @virt: nvmet_buf to free
+  * @dma: the DMA-mapped address of the lpfc_sg_dma_buf_pool to be freed
+  *
+  * Returns: None
+  **/
+ void
+ lpfc_nvmet_buf_free(struct lpfc_hba *phba, void *virt, dma_addr_t dma)
+ {
+ 	dma_pool_free(phba->lpfc_sg_dma_buf_pool, virt, dma);
+ }
+ 
+ /**
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
   * lpfc_els_hbq_alloc - Allocate an HBQ buffer
   * @phba: HBA to allocate HBQ buffer for
   *
@@@ -537,10 -607,70 +646,74 @@@ lpfc_sli4_rb_alloc(struct lpfc_hba *phb
  void
  lpfc_sli4_rb_free(struct lpfc_hba *phba, struct hbq_dmabuf *dmab)
  {
- 	pci_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
- 	pci_pool_free(phba->lpfc_drb_pool, dmab->dbuf.virt, dmab->dbuf.phys);
+ 	dma_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
+ 	dma_pool_free(phba->lpfc_drb_pool, dmab->dbuf.virt, dmab->dbuf.phys);
  	kfree(dmab);
++<<<<<<< HEAD
 +	return;
++=======
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_alloc - Allocate an SLI4 Receive buffer
+  * @phba: HBA to allocate a receive buffer for
+  *
+  * Description: Allocates a DMA-mapped receive buffer from the lpfc_hrb_pool PCI
+  * pool along a non-DMA-mapped container for it.
+  *
+  * Notes: Not interrupt-safe.  Must be called with no locks held.
+  *
+  * Returns:
+  *   pointer to HBQ on success
+  *   NULL on failure
+  **/
+ struct rqb_dmabuf *
+ lpfc_sli4_nvmet_alloc(struct lpfc_hba *phba)
+ {
+ 	struct rqb_dmabuf *dma_buf;
+ 
+ 	dma_buf = kzalloc(sizeof(struct rqb_dmabuf), GFP_KERNEL);
+ 	if (!dma_buf)
+ 		return NULL;
+ 
+ 	dma_buf->hbuf.virt = dma_pool_alloc(phba->lpfc_hrb_pool, GFP_KERNEL,
+ 					    &dma_buf->hbuf.phys);
+ 	if (!dma_buf->hbuf.virt) {
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->dbuf.virt = dma_pool_alloc(phba->lpfc_nvmet_drb_pool,
+ 					    GFP_KERNEL, &dma_buf->dbuf.phys);
+ 	if (!dma_buf->dbuf.virt) {
+ 		dma_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->total_size = LPFC_NVMET_DATA_BUF_SIZE;
+ 	return dma_buf;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_free - Frees a receive buffer
+  * @phba: HBA buffer was allocated for
+  * @dmab: DMA Buffer container returned by lpfc_sli4_rbq_alloc
+  *
+  * Description: Frees both the container and the DMA-mapped buffers returned by
+  * lpfc_sli4_nvmet_alloc.
+  *
+  * Notes: Can be called with or without locks held.
+  *
+  * Returns: None
+  **/
+ void
+ lpfc_sli4_nvmet_free(struct lpfc_hba *phba, struct rqb_dmabuf *dmab)
+ {
+ 	dma_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
+ 	dma_pool_free(phba->lpfc_nvmet_drb_pool,
+ 		      dmab->dbuf.virt, dmab->dbuf.phys);
+ 	kfree(dmab);
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  }
  
  /**
diff --cc drivers/scsi/lpfc/lpfc_scsi.c
index 5a4a91df6be5,1a6f122bb25d..000000000000
--- a/drivers/scsi/lpfc/lpfc_scsi.c
+++ b/drivers/scsi/lpfc/lpfc_scsi.c
@@@ -534,7 -416,7 +534,11 @@@ lpfc_new_scsi_buf_s3(struct lpfc_vport 
  		 * struct fcp_cmnd, struct fcp_rsp and the number of bde's
  		 * necessary to support the sg_tablesize.
  		 */
++<<<<<<< HEAD
 +		psb->data = pci_pool_alloc(phba->lpfc_scsi_dma_buf_pool,
++=======
+ 		psb->data = dma_pool_zalloc(phba->lpfc_sg_dma_buf_pool,
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  					GFP_KERNEL, &psb->dma_handle);
  		if (!psb->data) {
  			kfree(psb);
@@@ -547,8 -427,8 +551,13 @@@
  		/* Allocate iotag for psb->cur_iocbq. */
  		iotag = lpfc_sli_next_iotag(phba, &psb->cur_iocbq);
  		if (iotag == 0) {
++<<<<<<< HEAD
 +			pci_pool_free(phba->lpfc_scsi_dma_buf_pool,
 +					psb->data, psb->dma_handle);
++=======
+ 			dma_pool_free(phba->lpfc_sg_dma_buf_pool,
+ 				      psb->data, psb->dma_handle);
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  			kfree(psb);
  			break;
  		}
@@@ -942,7 -826,7 +951,11 @@@ lpfc_new_scsi_buf_s4(struct lpfc_vport 
  		 * for the struct fcp_cmnd, struct fcp_rsp and the number
  		 * of bde's necessary to support the sg_tablesize.
  		 */
++<<<<<<< HEAD
 +		psb->data = pci_pool_alloc(phba->lpfc_scsi_dma_buf_pool,
++=======
+ 		psb->data = dma_pool_zalloc(phba->lpfc_sg_dma_buf_pool,
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  						GFP_KERNEL, &psb->dma_handle);
  		if (!psb->data) {
  			kfree(psb);
@@@ -956,7 -839,7 +969,11 @@@
  		 */
  		if (phba->cfg_enable_bg  && (((unsigned long)(psb->data) &
  		    (unsigned long)(SLI4_PAGE_SIZE - 1)) != 0)) {
++<<<<<<< HEAD
 +			pci_pool_free(phba->lpfc_scsi_dma_buf_pool,
++=======
+ 			dma_pool_free(phba->lpfc_sg_dma_buf_pool,
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  				      psb->data, psb->dma_handle);
  			kfree(psb);
  			break;
@@@ -965,8 -848,8 +982,13 @@@
  
  		lxri = lpfc_sli4_next_xritag(phba);
  		if (lxri == NO_XRI) {
++<<<<<<< HEAD
 +			pci_pool_free(phba->lpfc_scsi_dma_buf_pool,
 +			      psb->data, psb->dma_handle);
++=======
+ 			dma_pool_free(phba->lpfc_sg_dma_buf_pool,
+ 				      psb->data, psb->dma_handle);
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  			kfree(psb);
  			break;
  		}
@@@ -974,8 -857,8 +996,13 @@@
  		/* Allocate iotag for psb->cur_iocbq. */
  		iotag = lpfc_sli_next_iotag(phba, &psb->cur_iocbq);
  		if (iotag == 0) {
++<<<<<<< HEAD
 +			pci_pool_free(phba->lpfc_scsi_dma_buf_pool,
 +				psb->data, psb->dma_handle);
++=======
+ 			dma_pool_free(phba->lpfc_sg_dma_buf_pool,
+ 				      psb->data, psb->dma_handle);
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  			kfree(psb);
  			lpfc_printf_log(phba, KERN_ERR, LOG_FCP,
  					"3368 Failed to allocate IOTAG for"
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index afe166ddbf5a,0a8c9b59365a..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -15676,6 -17044,96 +15676,99 @@@ lpfc_sli4_send_seq_to_ulp(struct lpfc_v
  	lpfc_sli_release_iocbq(phba, iocbq);
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ lpfc_sli4_mds_loopback_cmpl(struct lpfc_hba *phba, struct lpfc_iocbq *cmdiocb,
+ 			    struct lpfc_iocbq *rspiocb)
+ {
+ 	struct lpfc_dmabuf *pcmd = cmdiocb->context2;
+ 
+ 	if (pcmd && pcmd->virt)
+ 		dma_pool_free(phba->lpfc_drb_pool, pcmd->virt, pcmd->phys);
+ 	kfree(pcmd);
+ 	lpfc_sli_release_iocbq(phba, cmdiocb);
+ }
+ 
+ static void
+ lpfc_sli4_handle_mds_loopback(struct lpfc_vport *vport,
+ 			      struct hbq_dmabuf *dmabuf)
+ {
+ 	struct fc_frame_header *fc_hdr;
+ 	struct lpfc_hba *phba = vport->phba;
+ 	struct lpfc_iocbq *iocbq = NULL;
+ 	union  lpfc_wqe *wqe;
+ 	struct lpfc_dmabuf *pcmd = NULL;
+ 	uint32_t frame_len;
+ 	int rc;
+ 
+ 	fc_hdr = (struct fc_frame_header *)dmabuf->hbuf.virt;
+ 	frame_len = bf_get(lpfc_rcqe_length, &dmabuf->cq_event.cqe.rcqe_cmpl);
+ 
+ 	/* Send the received frame back */
+ 	iocbq = lpfc_sli_get_iocbq(phba);
+ 	if (!iocbq)
+ 		goto exit;
+ 
+ 	/* Allocate buffer for command payload */
+ 	pcmd = kmalloc(sizeof(struct lpfc_dmabuf), GFP_KERNEL);
+ 	if (pcmd)
+ 		pcmd->virt = dma_pool_alloc(phba->lpfc_drb_pool, GFP_KERNEL,
+ 					    &pcmd->phys);
+ 	if (!pcmd || !pcmd->virt)
+ 		goto exit;
+ 
+ 	INIT_LIST_HEAD(&pcmd->list);
+ 
+ 	/* copyin the payload */
+ 	memcpy(pcmd->virt, dmabuf->dbuf.virt, frame_len);
+ 
+ 	/* fill in BDE's for command */
+ 	iocbq->iocb.un.xseq64.bdl.addrHigh = putPaddrHigh(pcmd->phys);
+ 	iocbq->iocb.un.xseq64.bdl.addrLow = putPaddrLow(pcmd->phys);
+ 	iocbq->iocb.un.xseq64.bdl.bdeFlags = BUFF_TYPE_BDE_64;
+ 	iocbq->iocb.un.xseq64.bdl.bdeSize = frame_len;
+ 
+ 	iocbq->context2 = pcmd;
+ 	iocbq->vport = vport;
+ 	iocbq->iocb_flag &= ~LPFC_FIP_ELS_ID_MASK;
+ 	iocbq->iocb_flag |= LPFC_USE_FCPWQIDX;
+ 
+ 	/*
+ 	 * Setup rest of the iocb as though it were a WQE
+ 	 * Build the SEND_FRAME WQE
+ 	 */
+ 	wqe = (union lpfc_wqe *)&iocbq->iocb;
+ 
+ 	wqe->send_frame.frame_len = frame_len;
+ 	wqe->send_frame.fc_hdr_wd0 = be32_to_cpu(*((uint32_t *)fc_hdr));
+ 	wqe->send_frame.fc_hdr_wd1 = be32_to_cpu(*((uint32_t *)fc_hdr + 1));
+ 	wqe->send_frame.fc_hdr_wd2 = be32_to_cpu(*((uint32_t *)fc_hdr + 2));
+ 	wqe->send_frame.fc_hdr_wd3 = be32_to_cpu(*((uint32_t *)fc_hdr + 3));
+ 	wqe->send_frame.fc_hdr_wd4 = be32_to_cpu(*((uint32_t *)fc_hdr + 4));
+ 	wqe->send_frame.fc_hdr_wd5 = be32_to_cpu(*((uint32_t *)fc_hdr + 5));
+ 
+ 	iocbq->iocb.ulpCommand = CMD_SEND_FRAME;
+ 	iocbq->iocb.ulpLe = 1;
+ 	iocbq->iocb_cmpl = lpfc_sli4_mds_loopback_cmpl;
+ 	rc = lpfc_sli_issue_iocb(phba, LPFC_ELS_RING, iocbq, 0);
+ 	if (rc == IOCB_ERROR)
+ 		goto exit;
+ 
+ 	lpfc_in_buf_free(phba, &dmabuf->dbuf);
+ 	return;
+ 
+ exit:
+ 	lpfc_printf_log(phba, KERN_WARNING, LOG_SLI,
+ 			"2023 Unable to process MDS loopback frame\n");
+ 	if (pcmd && pcmd->virt)
+ 		dma_pool_free(phba->lpfc_drb_pool, pcmd->virt, pcmd->phys);
+ 	kfree(pcmd);
+ 	lpfc_sli_release_iocbq(phba, iocbq);
+ 	lpfc_in_buf_free(phba, &dmabuf->dbuf);
+ }
+ 
++>>>>>>> 771db5c0e3f5 (scsi: lpfc: Replace PCI pool old API)
  /**
   * lpfc_sli4_handle_received_buffer - Handle received buffers from firmware
   * @phba: Pointer to HBA context object.
* Unmerged path drivers/scsi/lpfc/lpfc_nvme.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc.h
* Unmerged path drivers/scsi/lpfc/lpfc_init.c
* Unmerged path drivers/scsi/lpfc/lpfc_mem.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvme.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_scsi.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
