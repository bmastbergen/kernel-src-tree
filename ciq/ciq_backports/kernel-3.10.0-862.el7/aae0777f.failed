KVM: PPC: Book3S HV: Split HPT allocation from activation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author David Gibson <david@gibson.dropbear.id.au>
commit aae0777f1e8224b4fbb78b2c692060852ee750c8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/aae0777f.failed

Currently, kvmppc_alloc_hpt() both allocates a new hashed page table (HPT)
and sets it up as the active page table for a VM.  For the upcoming HPT
resize implementation we're going to want to allocate HPTs separately from
activating them.

So, split the allocation itself out into kvmppc_allocate_hpt() and perform
the activation with a new kvmppc_set_hpt() function.  Likewise we split
kvmppc_free_hpt(), which just frees the HPT, from kvmppc_release_hpt()
which unsets it as an active HPT, then frees it.

We also move the logic to fall back to smaller HPT sizes if the first try
fails into the single caller which used that behaviour,
kvmppc_hv_setup_htab_rma().  This introduces a slight semantic change, in
that previously if the initial attempt at CMA allocation failed, we would
fall back to attempting smaller sizes with the page allocator.  Now, we
try first CMA, then the page allocator at each size.  As far as I can tell
this change should be harmless.

To match, we make kvmppc_free_hpt() just free the actual HPT itself.  The
call to kvmppc_free_lpid() that was there, we move to the single caller.

	Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
(cherry picked from commit aae0777f1e8224b4fbb78b2c692060852ee750c8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/kvm_book3s_64.h
#	arch/powerpc/kvm/book3s_64_mmu_hv.c
#	arch/powerpc/kvm/book3s_hv.c
diff --cc arch/powerpc/include/asm/kvm_book3s_64.h
index 67964ff815d3,d9b48f5bb606..000000000000
--- a/arch/powerpc/include/asm/kvm_book3s_64.h
+++ b/arch/powerpc/include/asm/kvm_book3s_64.h
@@@ -20,6 -20,12 +20,15 @@@
  #ifndef __ASM_KVM_BOOK3S_64_H__
  #define __ASM_KVM_BOOK3S_64_H__
  
++<<<<<<< HEAD
++=======
+ #include <asm/book3s/64/mmu-hash.h>
+ 
+ /* Power architecture requires HPT is at least 256kiB, at most 64TiB */
+ #define PPC_MIN_HPT_ORDER	18
+ #define PPC_MAX_HPT_ORDER	46
+ 
++>>>>>>> aae0777f1e82 (KVM: PPC: Book3S HV: Split HPT allocation from activation)
  #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
  static inline struct kvmppc_book3s_shadow_vcpu *svcpu_get(struct kvm_vcpu *vcpu)
  {
diff --cc arch/powerpc/kvm/book3s_64_mmu_hv.c
index 283e37e10f56,62d132a3cec5..000000000000
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@@ -38,79 -38,68 +38,109 @@@
  #include <asm/ppc-opcode.h>
  #include <asm/cputable.h>
  
 +#include "book3s_hv_cma.h"
  #include "trace_hv.h"
  
- /* Power architecture requires HPT is at least 256kB */
- #define PPC_MIN_HPT_ORDER	18
- 
  static long kvmppc_virtmode_do_h_enter(struct kvm *kvm, unsigned long flags,
  				long pte_index, unsigned long pteh,
  				unsigned long ptel, unsigned long *pte_idx_ret);
  static void kvmppc_rmap_reset(struct kvm *kvm);
  
- long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)
+ int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order)
  {
  	unsigned long hpt = 0;
- 	struct revmap_entry *rev;
+ 	int cma = 0;
  	struct page *page = NULL;
- 	long order = KVM_DEFAULT_HPT_ORDER;
+ 	struct revmap_entry *rev;
+ 	unsigned long npte;
  
- 	if (htab_orderp) {
- 		order = *htab_orderp;
- 		if (order < PPC_MIN_HPT_ORDER)
- 			order = PPC_MIN_HPT_ORDER;
- 	}
+ 	if ((order < PPC_MIN_HPT_ORDER) || (order > PPC_MAX_HPT_ORDER))
+ 		return -EINVAL;
  
++<<<<<<< HEAD
 +	kvm->arch.hpt_cma_alloc = 0;
 +	VM_BUG_ON(order < KVM_CMA_CHUNK_ORDER);
 +	page = kvm_alloc_hpt(1 << (order - PAGE_SHIFT));
 +	if (page) {
 +		hpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));
 +		kvm->arch.hpt_cma_alloc = 1;
++=======
+ 	page = kvm_alloc_hpt_cma(1ul << (order - PAGE_SHIFT));
+ 	if (page) {
+ 		hpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));
+ 		memset((void *)hpt, 0, (1ul << order));
+ 		cma = 1;
++>>>>>>> aae0777f1e82 (KVM: PPC: Book3S HV: Split HPT allocation from activation)
  	}
  
- 	/* Lastly try successively smaller sizes from the page allocator */
- 	/* Only do this if userspace didn't specify a size via ioctl */
- 	while (!hpt && order > PPC_MIN_HPT_ORDER && !htab_orderp) {
- 		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT|
- 				       __GFP_NOWARN, order - PAGE_SHIFT);
- 		if (!hpt)
- 			--order;
- 	}
+ 	if (!hpt)
+ 		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT
+ 				       |__GFP_NOWARN, order - PAGE_SHIFT);
  
  	if (!hpt)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	kvm->arch.hpt_virt = hpt;
 +	kvm->arch.hpt_order = order;
 +	/* HPTEs are 2**4 bytes long */
 +	kvm->arch.hpt_npte = 1ul << (order - 4);
 +	/* 128 (2**7) bytes in each HPTEG */
 +	kvm->arch.hpt_mask = (1ul << (order - 7)) - 1;
 +
 +	/* Allocate reverse map array */
 +	rev = vmalloc(sizeof(struct revmap_entry) * kvm->arch.hpt_npte);
++=======
+ 	/* HPTEs are 2**4 bytes long */
+ 	npte = 1ul << (order - 4);
+ 
+ 	/* Allocate reverse map array */
+ 	rev = vmalloc(sizeof(struct revmap_entry) * npte);
++>>>>>>> aae0777f1e82 (KVM: PPC: Book3S HV: Split HPT allocation from activation)
  	if (!rev) {
- 		pr_err("kvmppc_alloc_hpt: Couldn't alloc reverse map array\n");
- 		goto out_freehpt;
+ 		pr_err("kvmppc_allocate_hpt: Couldn't alloc reverse map array\n");
+ 		if (cma)
+ 			kvm_free_hpt_cma(page, 1 << (order - PAGE_SHIFT));
+ 		else
+ 			free_pages(hpt, order - PAGE_SHIFT);
+ 		return -ENOMEM;
  	}
++<<<<<<< HEAD
 +	kvm->arch.revmap = rev;
 +	kvm->arch.sdr1 = __pa(hpt) | (order - 18);
 +
 +	pr_info("KVM guest htab at %lx (order %ld), LPID %x\n",
 +		hpt, order, kvm->arch.lpid);
 +
 +	if (htab_orderp)
 +		*htab_orderp = order;
 +	return 0;
 +
 + out_freehpt:
 +	if (kvm->arch.hpt_cma_alloc)
 +		kvm_release_hpt(page, 1 << (order - PAGE_SHIFT));
 +	else
 +		free_pages(hpt, order - PAGE_SHIFT);
 +	return -ENOMEM;
++=======
+ 
+ 	info->order = order;
+ 	info->virt = hpt;
+ 	info->cma = cma;
+ 	info->rev = rev;
+ 
+ 	return 0;
+ }
+ 
+ void kvmppc_set_hpt(struct kvm *kvm, struct kvm_hpt_info *info)
+ {
+ 	atomic64_set(&kvm->arch.mmio_update, 0);
+ 	kvm->arch.hpt = *info;
+ 	kvm->arch.sdr1 = __pa(info->virt) | (info->order - 18);
+ 
+ 	pr_info("KVM guest htab at %lx (order %ld), LPID %x\n",
+ 		info->virt, (long)info->order, kvm->arch.lpid);
++>>>>>>> aae0777f1e82 (KVM: PPC: Book3S HV: Split HPT allocation from activation)
  }
  
  long kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp)
@@@ -149,16 -145,16 +183,27 @@@
  	return err;
  }
  
- void kvmppc_free_hpt(struct kvm *kvm)
+ void kvmppc_free_hpt(struct kvm_hpt_info *info)
  {
++<<<<<<< HEAD
 +	kvmppc_free_lpid(kvm->arch.lpid);
 +	vfree(kvm->arch.revmap);
 +	if (kvm->arch.hpt_cma_alloc)
 +		kvm_release_hpt(virt_to_page(kvm->arch.hpt_virt),
 +				1 << (kvm->arch.hpt_order - PAGE_SHIFT));
 +	else
 +		free_pages(kvm->arch.hpt_virt,
 +			   kvm->arch.hpt_order - PAGE_SHIFT);
++=======
+ 	vfree(info->rev);
+ 	if (info->cma)
+ 		kvm_free_hpt_cma(virt_to_page(info->virt),
+ 				 1 << (info->order - PAGE_SHIFT));
+ 	else if (info->virt)
+ 		free_pages(info->virt, info->order - PAGE_SHIFT);
+ 	info->virt = 0;
+ 	info->order = 0;
++>>>>>>> aae0777f1e82 (KVM: PPC: Book3S HV: Split HPT allocation from activation)
  }
  
  /* Bits in first HPTE dword for pagesize 4k, 64k or 16M */
diff --cc arch/powerpc/kvm/book3s_hv.c
index 92357b82799a,19987e4343c3..000000000000
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@@ -3017,9 -3197,18 +3017,24 @@@ static int kvmppc_hv_setup_htab_rma(str
  		goto out;	/* another vcpu beat us to it */
  
  	/* Allocate hashed page table (if not done already) and reset it */
++<<<<<<< HEAD
 +	if (!kvm->arch.hpt_virt) {
 +		err = kvmppc_alloc_hpt(kvm, NULL);
 +		if (err) {
++=======
+ 	if (!kvm->arch.hpt.virt) {
+ 		int order = KVM_DEFAULT_HPT_ORDER;
+ 		struct kvm_hpt_info info;
+ 
+ 		err = kvmppc_allocate_hpt(&info, order);
+ 		/* If we get here, it means userspace didn't specify a
+ 		 * size explicitly.  So, try successively smaller
+ 		 * sizes if the default failed. */
+ 		while ((err == -ENOMEM) && --order >= PPC_MIN_HPT_ORDER)
+ 			err  = kvmppc_allocate_hpt(&info, order);
+ 
+ 		if (err < 0) {
++>>>>>>> aae0777f1e82 (KVM: PPC: Book3S HV: Split HPT allocation from activation)
  			pr_err("KVM: Couldn't alloc HPT\n");
  			goto out;
  		}
@@@ -3262,7 -3473,12 +3279,16 @@@ static void kvmppc_core_destroy_vm_hv(s
  
  	kvmppc_free_vcores(kvm);
  
++<<<<<<< HEAD
 +	kvmppc_free_hpt(kvm);
++=======
+ 	kvmppc_free_lpid(kvm->arch.lpid);
+ 
+ 	if (kvm_is_radix(kvm))
+ 		kvmppc_free_radix(kvm);
+ 	else
+ 		kvmppc_free_hpt(&kvm->arch.hpt);
++>>>>>>> aae0777f1e82 (KVM: PPC: Book3S HV: Split HPT allocation from activation)
  
  	kvmppc_free_pimap(kvm);
  }
* Unmerged path arch/powerpc/include/asm/kvm_book3s_64.h
diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index 4fff076a809b..4d8787c46e0f 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -156,9 +156,10 @@ extern void kvmppc_core_destroy_mmu(struct kvm_vcpu *vcpu);
 extern int kvmppc_kvm_pv(struct kvm_vcpu *vcpu);
 extern void kvmppc_map_magic(struct kvm_vcpu *vcpu);
 
-extern long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp);
+extern int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order);
+extern void kvmppc_set_hpt(struct kvm *kvm, struct kvm_hpt_info *info);
 extern long kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp);
-extern void kvmppc_free_hpt(struct kvm *kvm);
+extern void kvmppc_free_hpt(struct kvm_hpt_info *info);
 extern long kvmppc_prepare_vrma(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem);
 extern void kvmppc_map_vrma(struct kvm_vcpu *vcpu,
* Unmerged path arch/powerpc/kvm/book3s_64_mmu_hv.c
* Unmerged path arch/powerpc/kvm/book3s_hv.c
