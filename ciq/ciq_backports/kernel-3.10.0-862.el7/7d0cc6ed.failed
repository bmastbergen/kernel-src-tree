IB/mlx5: Add MR cache for large UMR regions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Artemy Kovalyov <artemyko@mellanox.com>
commit 7d0cc6edcc7011133c45f62a7796a98b8cb5da0f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7d0cc6ed.failed

In this change we turn mlx5_ib_update_mtt() into generic
mlx5_ib_update_xlt() to perfrom HCA translation table modifiactions
supporting both atomic and process contexts and not limited by number
of modified entries.
Using this function we increase preallocated MRs up to 16GB.

	Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 7d0cc6edcc7011133c45f62a7796a98b8cb5da0f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 64dcc8f7a254,f4ecc10cbbba..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -762,93 -760,6 +760,96 @@@ static int use_umr(struct mlx5_ib_dev *
  	return order <= MLX5_MAX_UMR_SHIFT;
  }
  
++<<<<<<< HEAD
 +static int dma_map_mr_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			  int npages, int page_shift, int *size,
 +			  __be64 **mr_pas, dma_addr_t *dma)
 +{
 +	__be64 *pas;
 +	struct device *ddev = dev->ib_dev.dev.parent;
 +
 +	/*
 +	 * UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
 +	 * To avoid copying garbage after the pas array, we allocate
 +	 * a little more.
 +	 */
 +	*size = ALIGN(sizeof(struct mlx5_mtt) * npages, MLX5_UMR_MTT_ALIGNMENT);
 +	*mr_pas = kmalloc(*size + MLX5_UMR_ALIGN - 1, GFP_KERNEL);
 +	if (!(*mr_pas))
 +		return -ENOMEM;
 +
 +	pas = PTR_ALIGN(*mr_pas, MLX5_UMR_ALIGN);
 +	mlx5_ib_populate_pas(dev, umem, page_shift, pas, MLX5_IB_MTT_PRESENT);
 +	/* Clear padding after the actual pages. */
 +	memset(pas + npages, 0, *size - npages * sizeof(struct mlx5_mtt));
 +
 +	*dma = dma_map_single(ddev, pas, *size, DMA_TO_DEVICE);
 +	if (dma_mapping_error(ddev, *dma)) {
 +		kfree(*mr_pas);
 +		return -ENOMEM;
 +	}
 +
 +	return 0;
 +}
 +
 +static void prep_umr_wqe_common(struct ib_pd *pd, struct ib_send_wr *wr,
 +				struct ib_sge *sg, u64 dma, int n, u32 key,
 +				int page_shift)
 +{
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct mlx5_umr_wr *umrwr = umr_wr(wr);
 +
 +	sg->addr = dma;
 +	sg->length = ALIGN(sizeof(struct mlx5_mtt) * n,
 +			   MLX5_IB_UMR_XLT_ALIGNMENT);
 +	sg->lkey = dev->umrc.pd->local_dma_lkey;
 +
 +	wr->next = NULL;
 +	wr->sg_list = sg;
 +	if (n)
 +		wr->num_sge = 1;
 +	else
 +		wr->num_sge = 0;
 +
 +	wr->opcode = MLX5_IB_WR_UMR;
 +
 +	umrwr->xlt_size = sg->length;
 +	umrwr->page_shift = page_shift;
 +	umrwr->mkey = key;
 +}
 +
 +static void prep_umr_reg_wqe(struct ib_pd *pd, struct ib_send_wr *wr,
 +			     struct ib_sge *sg, u64 dma, int n, u32 key,
 +			     int page_shift, u64 virt_addr, u64 len,
 +			     int access_flags)
 +{
 +	struct mlx5_umr_wr *umrwr = umr_wr(wr);
 +
 +	prep_umr_wqe_common(pd, wr, sg, dma, n, key, page_shift);
 +
 +	wr->send_flags = MLX5_IB_SEND_UMR_ENABLE_MR |
 +			 MLX5_IB_SEND_UMR_UPDATE_TRANSLATION |
 +			 MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
 +
 +	umrwr->virt_addr = virt_addr;
 +	umrwr->length = len;
 +	umrwr->access_flags = access_flags;
 +	umrwr->pd = pd;
 +}
 +
 +static void prep_umr_unreg_wqe(struct mlx5_ib_dev *dev,
 +			       struct ib_send_wr *wr, u32 key)
 +{
 +	struct mlx5_umr_wr *umrwr = umr_wr(wr);
 +
 +	wr->send_flags = MLX5_IB_SEND_UMR_DISABLE_MR |
 +			 MLX5_IB_SEND_UMR_FAIL_IF_FREE;
 +	wr->opcode = MLX5_IB_WR_UMR;
 +	umrwr->mkey = key;
 +}
 +
++=======
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
  static int mr_umem_get(struct ib_pd *pd, u64 start, u64 length,
  		       int access_flags, struct ib_umem **umem,
  		       int *npages, int *page_shift, int *ncont,
@@@ -900,16 -838,7 +901,15 @@@ static struct mlx5_ib_mr *reg_umr(struc
  				  int page_shift, int order, int access_flags)
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
++<<<<<<< HEAD
 +	struct device *ddev = dev->ib_dev.dev.parent;
 +	struct umr_common *umrc = &dev->umrc;
 +	struct mlx5_ib_umr_context umr_context;
 +	struct mlx5_umr_wr umrwr = {};
 +	struct ib_send_wr *bad;
++=======
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
  	struct mlx5_ib_mr *mr;
- 	struct ib_sge sg;
- 	int size;
- 	__be64 *mr_pas;
- 	dma_addr_t dma;
  	int err = 0;
  	int i;
  
@@@ -928,43 -857,17 +928,54 @@@
  	if (!mr)
  		return ERR_PTR(-EAGAIN);
  
++<<<<<<< HEAD
 +	err = dma_map_mr_pas(dev, umem, npages, page_shift, &size, &mr_pas,
 +			     &dma);
 +	if (err)
 +		goto free_mr;
 +
 +	mlx5_ib_init_umr_context(&umr_context);
 +
 +	umrwr.wr.wr_cqe = &umr_context.cqe;
 +	prep_umr_reg_wqe(pd, &umrwr.wr, &sg, dma, npages, mr->mmkey.key,
 +			 page_shift, virt_addr, len, access_flags);
 +
 +	down(&umrc->sem);
 +	err = ib_post_send(umrc->qp, &umrwr.wr, &bad);
 +	if (err) {
 +		mlx5_ib_warn(dev, "post send failed, err %d\n", err);
 +		goto unmap_dma;
 +	} else {
 +		wait_for_completion(&umr_context.done);
 +		if (umr_context.status != IB_WC_SUCCESS) {
 +			mlx5_ib_warn(dev, "reg umr failed\n");
 +			err = -EFAULT;
 +		}
 +	}
 +
++=======
+ 	mr->ibmr.pd = pd;
+ 	mr->umem = umem;
+ 	mr->access_flags = access_flags;
+ 	mr->desc_size = sizeof(struct mlx5_mtt);
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
  	mr->mmkey.iova = virt_addr;
  	mr->mmkey.size = len;
  	mr->mmkey.pd = to_mpd(pd)->pdn;
  
- 	mr->live = 1;
+ 	err = mlx5_ib_update_xlt(mr, 0, npages, page_shift,
+ 				 MLX5_IB_UPD_XLT_ENABLE);
  
++<<<<<<< HEAD
 +unmap_dma:
 +	up(&umrc->sem);
 +	dma_unmap_single(ddev, dma, size, DMA_TO_DEVICE);
 +
 +	kfree(mr_pas);
 +
 +free_mr:
++=======
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
  	if (err) {
  		free_cached_mr(dev, mr);
  		return ERR_PTR(err);
@@@ -973,19 -878,42 +986,50 @@@
  	return mr;
  }
  
- #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
- int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index, int npages,
- 		       int zap)
+ static inline int populate_xlt(struct mlx5_ib_mr *mr, int idx, int npages,
+ 			       void *xlt, int page_shift, size_t size,
+ 			       int flags)
  {
  	struct mlx5_ib_dev *dev = mr->dev;
+ 	struct ib_umem *umem = mr->umem;
+ 
+ 	npages = min_t(size_t, npages, ib_umem_num_pages(umem) - idx);
+ 
+ 	if (!(flags & MLX5_IB_UPD_XLT_ZAP)) {
+ 		__mlx5_ib_populate_pas(dev, umem, page_shift,
+ 				       idx, npages, xlt,
+ 				       MLX5_IB_MTT_PRESENT);
+ 		/* Clear padding after the pages
+ 		 * brought from the umem.
+ 		 */
+ 		memset(xlt + (npages * sizeof(struct mlx5_mtt)), 0,
+ 		       size - npages * sizeof(struct mlx5_mtt));
+ 	}
+ 
+ 	return npages;
+ }
+ 
+ #define MLX5_MAX_UMR_CHUNK ((1 << (MLX5_MAX_UMR_SHIFT + 4)) - \
+ 			    MLX5_UMR_MTT_ALIGNMENT)
+ #define MLX5_SPARE_UMR_CHUNK 0x10000
+ 
+ int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
+ 		       int page_shift, int flags)
+ {
+ 	struct mlx5_ib_dev *dev = mr->dev;
++<<<<<<< HEAD
 +	struct device *ddev = dev->ib_dev.dev.parent;
 +	struct umr_common *umrc = &dev->umrc;
 +	struct mlx5_ib_umr_context umr_context;
 +	struct ib_umem *umem = mr->umem;
++=======
+ 	struct device *ddev = dev->ib_dev.dma_device;
+ 	struct mlx5_ib_ucontext *uctx = NULL;
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
  	int size;
- 	__be64 *pas;
+ 	void *xlt;
  	dma_addr_t dma;
 +	struct ib_send_wr *bad;
  	struct mlx5_umr_wr wr;
  	struct ib_sge sg;
  	int err = 0;
@@@ -1052,39 -992,28 +1108,47 @@@
  
  		dma_sync_single_for_device(ddev, dma, size, DMA_TO_DEVICE);
  
++<<<<<<< HEAD
 +		mlx5_ib_init_umr_context(&umr_context);
 +
 +		memset(&wr, 0, sizeof(wr));
 +		wr.wr.wr_cqe = &umr_context.cqe;
++=======
+ 		sg.length = ALIGN(npages * desc_size,
+ 				  MLX5_UMR_MTT_ALIGNMENT);
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
+ 
+ 		if (pages_mapped + pages_iter >= pages_to_map) {
+ 			if (flags & MLX5_IB_UPD_XLT_ENABLE)
+ 				wr.wr.send_flags |=
+ 					MLX5_IB_SEND_UMR_ENABLE_MR |
+ 					MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS |
+ 					MLX5_IB_SEND_UMR_UPDATE_TRANSLATION;
+ 			if (flags & MLX5_IB_UPD_XLT_PD ||
+ 			    flags & MLX5_IB_UPD_XLT_ACCESS)
+ 				wr.wr.send_flags |=
+ 					MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
+ 			if (flags & MLX5_IB_UPD_XLT_ADDR)
+ 				wr.wr.send_flags |=
+ 					MLX5_IB_SEND_UMR_UPDATE_TRANSLATION;
+ 		}
  
- 		sg.addr = dma;
- 		sg.length = ALIGN(npages * sizeof(struct mlx5_mtt),
- 				MLX5_UMR_MTT_ALIGNMENT);
- 		sg.lkey = dev->umrc.pd->local_dma_lkey;
- 
- 		wr.wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE |
- 				   MLX5_IB_SEND_UMR_UPDATE_XLT;
- 		wr.wr.sg_list = &sg;
- 		wr.wr.num_sge = 1;
- 		wr.wr.opcode = MLX5_IB_WR_UMR;
+ 		wr.offset = idx * desc_size;
  		wr.xlt_size = sg.length;
- 		wr.page_shift = PAGE_SHIFT;
- 		wr.mkey = mr->mmkey.key;
- 		wr.offset = start_page_index * sizeof(struct mlx5_mtt);
  
 -		err = mlx5_ib_post_send_wait(dev, &wr);
 +		down(&umrc->sem);
 +		err = ib_post_send(umrc->qp, &wr.wr, &bad);
 +		if (err) {
 +			mlx5_ib_err(dev, "UMR post send failed, err %d\n", err);
 +		} else {
 +			wait_for_completion(&umr_context.done);
 +			if (umr_context.status != IB_WC_SUCCESS) {
 +				mlx5_ib_err(dev, "UMR completion failed, code %d\n",
 +					    umr_context.status);
 +				err = -EFAULT;
 +			}
 +		}
 +		up(&umrc->sem);
  	}
  	dma_unmap_single(ddev, dma, size, DMA_TO_DEVICE);
  
@@@ -1265,66 -1189,25 +1328,65 @@@ static int unreg_umr(struct mlx5_ib_de
  	if (mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR)
  		return 0;
  
++<<<<<<< HEAD
 +	mlx5_ib_init_umr_context(&umr_context);
 +
 +	umrwr.wr.wr_cqe = &umr_context.cqe;
 +	prep_umr_unreg_wqe(dev, &umrwr.wr, mr->mmkey.key);
++=======
+ 	umrwr.wr.send_flags = MLX5_IB_SEND_UMR_DISABLE_MR |
+ 			      MLX5_IB_SEND_UMR_FAIL_IF_FREE;
+ 	umrwr.wr.opcode = MLX5_IB_WR_UMR;
+ 	umrwr.mkey = mr->mmkey.key;
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
 +
 +	down(&umrc->sem);
 +	err = ib_post_send(umrc->qp, &umrwr.wr, &bad);
 +	if (err) {
 +		up(&umrc->sem);
 +		mlx5_ib_dbg(dev, "err %d\n", err);
 +		goto error;
 +	} else {
 +		wait_for_completion(&umr_context.done);
 +		up(&umrc->sem);
 +	}
 +	if (umr_context.status != IB_WC_SUCCESS) {
 +		mlx5_ib_warn(dev, "unreg umr failed\n");
 +		err = -EFAULT;
 +		goto error;
 +	}
 +	return 0;
  
 -	return mlx5_ib_post_send_wait(dev, &umrwr);
 +error:
 +	return err;
  }
  
- static int rereg_umr(struct ib_pd *pd, struct mlx5_ib_mr *mr, u64 virt_addr,
- 		     u64 length, int npages, int page_shift, int order,
+ static int rereg_umr(struct ib_pd *pd, struct mlx5_ib_mr *mr,
  		     int access_flags, int flags)
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
++<<<<<<< HEAD
 +	struct device *ddev = dev->ib_dev.dev.parent;
 +	struct mlx5_ib_umr_context umr_context;
 +	struct ib_send_wr *bad;
 +	struct mlx5_umr_wr umrwr = {};
 +	struct ib_sge sg;
 +	struct umr_common *umrc = &dev->umrc;
 +	dma_addr_t dma = 0;
 +	__be64 *mr_pas = NULL;
 +	int size;
++=======
+ 	struct mlx5_umr_wr umrwr = {};
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
  	int err;
  
 +	mlx5_ib_init_umr_context(&umr_context);
 +
 +	umrwr.wr.wr_cqe = &umr_context.cqe;
  	umrwr.wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE;
  
- 	if (flags & IB_MR_REREG_TRANS) {
- 		err = dma_map_mr_pas(dev, mr->umem, npages, page_shift, &size,
- 				     &mr_pas, &dma);
- 		if (err)
- 			return err;
- 
- 		umrwr.virt_addr = virt_addr;
- 		umrwr.length = length;
- 		umrwr.wr.send_flags |= MLX5_IB_SEND_UMR_UPDATE_TRANSLATION;
- 	}
- 
- 	prep_umr_wqe_common(pd, &umrwr.wr, &sg, dma, npages, mr->mmkey.key,
- 			    page_shift);
+ 	umrwr.wr.opcode = MLX5_IB_WR_UMR;
+ 	umrwr.mkey = mr->mmkey.key;
  
  	if (flags & IB_MR_REREG_PD || flags & IB_MR_REREG_ACCESS) {
  		umrwr.pd = pd;
@@@ -1332,26 -1215,8 +1394,31 @@@
  		umrwr.wr.send_flags |= MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
  	}
  
++<<<<<<< HEAD
 +	/* post send request to UMR QP */
 +	down(&umrc->sem);
 +	err = ib_post_send(umrc->qp, &umrwr.wr, &bad);
 +
 +	if (err) {
 +		mlx5_ib_warn(dev, "post send failed, err %d\n", err);
 +	} else {
 +		wait_for_completion(&umr_context.done);
 +		if (umr_context.status != IB_WC_SUCCESS) {
 +			mlx5_ib_warn(dev, "reg umr failed (%u)\n",
 +				     umr_context.status);
 +			err = -EFAULT;
 +		}
 +	}
 +
 +	up(&umrc->sem);
 +	if (flags & IB_MR_REREG_TRANS) {
 +		dma_unmap_single(ddev, dma, size, DMA_TO_DEVICE);
 +		kfree(mr_pas);
 +	}
++=======
+ 	err = mlx5_ib_post_send_wait(dev, &umrwr);
+ 
++>>>>>>> 7d0cc6edcc70 (IB/mlx5: Add MR cache for large UMR regions)
  	return err;
  }
  
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index c7eb0387ed42..059b7dc11b02 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -1122,11 +1122,18 @@ static struct ib_ucontext *mlx5_ib_alloc_ucontext(struct ib_device *ibdev,
 	context->ibucontext.invalidate_range = &mlx5_ib_invalidate_range;
 #endif
 
+	context->upd_xlt_page = __get_free_page(GFP_KERNEL);
+	if (!context->upd_xlt_page) {
+		err = -ENOMEM;
+		goto out_uars;
+	}
+	mutex_init(&context->upd_xlt_page_mutex);
+
 	if (MLX5_CAP_GEN(dev->mdev, log_max_transport_domain)) {
 		err = mlx5_core_alloc_transport_domain(dev->mdev,
 						       &context->tdn);
 		if (err)
-			goto out_uars;
+			goto out_page;
 	}
 
 	INIT_LIST_HEAD(&context->vma_private_list);
@@ -1178,6 +1185,9 @@ out_td:
 	if (MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))
 		mlx5_core_dealloc_transport_domain(dev->mdev, context->tdn);
 
+out_page:
+	free_page(context->upd_xlt_page);
+
 out_uars:
 	for (i--; i >= 0; i--)
 		mlx5_cmd_free_uar(dev->mdev, uars[i].index);
@@ -1205,6 +1215,8 @@ static int mlx5_ib_dealloc_ucontext(struct ib_ucontext *ibcontext)
 	if (MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))
 		mlx5_core_dealloc_transport_domain(dev->mdev, context->tdn);
 
+	free_page(context->upd_xlt_page);
+
 	for (i = 0; i < uuari->num_uars; i++) {
 		if (mlx5_cmd_free_uar(dev->mdev, uuari->uars[i].index))
 			mlx5_ib_warn(dev, "failed to free UAR 0x%x\n", uuari->uars[i].index);
diff --git a/drivers/infiniband/hw/mlx5/mem.c b/drivers/infiniband/hw/mlx5/mem.c
index 6851357c16f4..778d8a18925f 100644
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@ -159,7 +159,7 @@ void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 	unsigned long umem_page_shift = ilog2(umem->page_size);
 	int shift = page_shift - umem_page_shift;
 	int mask = (1 << shift) - 1;
-	int i, k;
+	int i, k, idx;
 	u64 cur = 0;
 	u64 base;
 	int len;
@@ -185,18 +185,36 @@ void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {
 		len = sg_dma_len(sg) >> umem_page_shift;
 		base = sg_dma_address(sg);
-		for (k = 0; k < len; k++) {
+
+		/* Skip elements below offset */
+		if (i + len < offset << shift) {
+			i += len;
+			continue;
+		}
+
+		/* Skip pages below offset */
+		if (i < offset << shift) {
+			k = (offset << shift) - i;
+			i = offset << shift;
+		} else {
+			k = 0;
+		}
+
+		for (; k < len; k++) {
 			if (!(i & mask)) {
 				cur = base + (k << umem_page_shift);
 				cur |= access_flags;
+				idx = (i >> shift) - offset;
 
-				pas[i >> shift] = cpu_to_be64(cur);
+				pas[idx] = cpu_to_be64(cur);
 				mlx5_ib_dbg(dev, "pas[%d] 0x%llx\n",
-					    i >> shift, be64_to_cpu(pas[i >> shift]));
-			}  else
-				mlx5_ib_dbg(dev, "=====> 0x%llx\n",
-					    base + (k << umem_page_shift));
+					    i >> shift, be64_to_cpu(pas[idx]));
+			}
 			i++;
+
+			/* Stop after num_pages reached */
+			if (i >> shift >= offset + num_pages)
+				return;
 		}
 	}
 }
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index dfad8ddf0c5a..d4ca2202fa29 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -125,6 +125,10 @@ struct mlx5_ib_ucontext {
 	/* Transport Domain number */
 	u32			tdn;
 	struct list_head	vma_private_list;
+
+	unsigned long		upd_xlt_page;
+	/* protect ODP/KSM */
+	struct mutex		upd_xlt_page_mutex;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
@@ -192,6 +196,13 @@ struct mlx5_ib_flow_db {
 #define MLX5_IB_UMR_OCTOWORD	       16
 #define MLX5_IB_UMR_XLT_ALIGNMENT      64
 
+#define MLX5_IB_UPD_XLT_ZAP	      BIT(0)
+#define MLX5_IB_UPD_XLT_ENABLE	      BIT(1)
+#define MLX5_IB_UPD_XLT_ATOMIC	      BIT(2)
+#define MLX5_IB_UPD_XLT_ADDR	      BIT(3)
+#define MLX5_IB_UPD_XLT_PD	      BIT(4)
+#define MLX5_IB_UPD_XLT_ACCESS	      BIT(5)
+
 /* Private QP creation flags to be passed in ib_qp_init_attr.create_flags.
  *
  * These flags are intended for internal use by the mlx5_ib driver, and they
@@ -794,8 +805,8 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 			       struct ib_udata *udata);
 int mlx5_ib_dealloc_mw(struct ib_mw *mw);
-int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
-		       int npages, int zap);
+int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
+		       int page_shift, int flags);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 			  u64 length, u64 virt_addr, int access_flags,
 			  struct ib_pd *pd, struct ib_udata *udata);
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
diff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c
index 1e73c127feb7..cfd7ee500c47 100644
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@ -91,16 +91,21 @@ void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
 			u64 umr_offset = idx & umr_block_mask;
 
 			if (in_block && umr_offset == 0) {
-				mlx5_ib_update_mtt(mr, blk_start_idx,
-						   idx - blk_start_idx, 1);
+				mlx5_ib_update_xlt(mr, blk_start_idx,
+						   idx - blk_start_idx,
+						   PAGE_SHIFT,
+						   MLX5_IB_UPD_XLT_ZAP |
+						   MLX5_IB_UPD_XLT_ATOMIC);
 				in_block = 0;
 			}
 		}
 	}
 	if (in_block)
-		mlx5_ib_update_mtt(mr, blk_start_idx, idx - blk_start_idx + 1,
-				   1);
-
+		mlx5_ib_update_xlt(mr, blk_start_idx,
+				   idx - blk_start_idx + 1,
+				   PAGE_SHIFT,
+				   MLX5_IB_UPD_XLT_ZAP |
+				   MLX5_IB_UPD_XLT_ATOMIC);
 	/*
 	 * We are now sure that the device will not access the
 	 * memory. We can safely unmap it, and mark it as dirty if
@@ -257,7 +262,9 @@ static int pagefault_single_data_segment(struct mlx5_ib_qp *qp,
 			 * this MR, since ib_umem_odp_map_dma_pages already
 			 * checks this.
 			 */
-			ret = mlx5_ib_update_mtt(mr, start_idx, npages, 0);
+			ret = mlx5_ib_update_xlt(mr, start_idx, npages,
+						 PAGE_SHIFT,
+						 MLX5_IB_UPD_XLT_ATOMIC);
 		} else {
 			ret = -EAGAIN;
 		}
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/main.c b/drivers/net/ethernet/mellanox/mlx5/core/main.c
index ddd26354b516..8ce514545aed 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -152,6 +152,26 @@ static struct mlx5_profile profile[] = {
 			.size	= 8,
 			.limit	= 4
 		},
+		.mr_cache[16]	= {
+			.size	= 8,
+			.limit	= 4
+		},
+		.mr_cache[17]	= {
+			.size	= 8,
+			.limit	= 4
+		},
+		.mr_cache[18]	= {
+			.size	= 8,
+			.limit	= 4
+		},
+		.mr_cache[19]	= {
+			.size	= 4,
+			.limit	= 2
+		},
+		.mr_cache[20]	= {
+			.size	= 4,
+			.limit	= 2
+		},
 	},
 };
 
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index 04cc9ef8d7e9..aef4594a5dd0 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -973,7 +973,7 @@ enum {
 };
 
 enum {
-	MAX_MR_CACHE_ENTRIES    = 16,
+	MAX_MR_CACHE_ENTRIES    = 21,
 };
 
 enum {
