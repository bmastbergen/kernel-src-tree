nfp: store dma direction in data path structure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit c487e6b199eab3af14e54406d97cc3c149e591e1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c487e6b1.failed

Instead of testing if xdp_prog is present store the dma direction
in data path structure.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c487e6b199eab3af14e54406d97cc3c149e591e1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/nfp_net.h
#	drivers/net/ethernet/netronome/nfp/nfp_net_common.c
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net.h
index 1826ee93d1da,db92463da440..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net.h
@@@ -417,18 -425,80 +417,88 @@@ static inline bool nfp_net_fw_ver_eq(st
  	       fw_ver->minor == minor;
  }
  
++<<<<<<< HEAD
++=======
+ struct nfp_stat_pair {
+ 	u64 pkts;
+ 	u64 bytes;
+ };
+ 
+ /**
+  * struct nfp_net_dp - NFP network device datapath data structure
+  * @dev:		Backpointer to struct device
+  * @netdev:		Backpointer to net_device structure
+  * @is_vf:		Is the driver attached to a VF?
+  * @bpf_offload_skip_sw:  Offloaded BPF program will not be rerun by cls_bpf
+  * @bpf_offload_xdp:	Offloaded BPF program is XDP
+  * @chained_metadata_format:  Firemware will use new metadata format
+  * @rx_dma_dir:		Mapping direction for RX buffers
+  * @ctrl:		Local copy of the control register/word.
+  * @fl_bufsz:		Currently configured size of the freelist buffers
+  * @rx_offset:		Offset in the RX buffers where packet data starts
+  * @xdp_prog:		Installed XDP program
+  * @tx_rings:		Array of pre-allocated TX ring structures
+  * @rx_rings:		Array of pre-allocated RX ring structures
+  * @ctrl_bar:		Pointer to mapped control BAR
+  *
+  * @txd_cnt:		Size of the TX ring in number of descriptors
+  * @rxd_cnt:		Size of the RX ring in number of descriptors
+  * @num_r_vecs:		Number of used ring vectors
+  * @num_tx_rings:	Currently configured number of TX rings
+  * @num_stack_tx_rings:	Number of TX rings used by the stack (not XDP)
+  * @num_rx_rings:	Currently configured number of RX rings
+  * @mtu:		Device MTU
+  */
+ struct nfp_net_dp {
+ 	struct device *dev;
+ 	struct net_device *netdev;
+ 
+ 	u8 is_vf:1;
+ 	u8 bpf_offload_skip_sw:1;
+ 	u8 bpf_offload_xdp:1;
+ 	u8 chained_metadata_format:1;
+ 
+ 	u8 rx_dma_dir;
+ 
+ 	u32 ctrl;
+ 	u32 fl_bufsz;
+ 
+ 	u32 rx_offset;
+ 
+ 	struct bpf_prog *xdp_prog;
+ 
+ 	struct nfp_net_tx_ring *tx_rings;
+ 	struct nfp_net_rx_ring *rx_rings;
+ 
+ 	u8 __iomem *ctrl_bar;
+ 
+ 	/* Cold data follows */
+ 
+ 	unsigned int txd_cnt;
+ 	unsigned int rxd_cnt;
+ 
+ 	unsigned int num_r_vecs;
+ 
+ 	unsigned int num_tx_rings;
+ 	unsigned int num_stack_tx_rings;
+ 	unsigned int num_rx_rings;
+ 
+ 	unsigned int mtu;
+ };
+ 
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  /**
   * struct nfp_net - NFP network device structure
 - * @dp:			Datapath structure
 - * @fw_ver:		Firmware version
 + * @pdev:               Backpointer to PCI device
 + * @netdev:             Backpointer to net_device structure
 + * @is_vf:              Is the driver attached to a VF?
 + * @fw_loaded:          Is the firmware loaded?
 + * @ctrl:               Local copy of the control register/word.
 + * @fl_bufsz:           Currently configured size of the freelist buffers
 + * @rx_offset:		Offset in the RX buffers where packet data starts
 + * @fw_ver:             Firmware version
   * @cap:                Capabilities advertised by the Firmware
   * @max_mtu:            Maximum support MTU advertised by the Firmware
 - * @rss_hfunc:		RSS selected hash function
   * @rss_cfg:            RSS configuration
   * @rss_key:            RSS secret key
   * @rss_itbl:           RSS indirection table
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index e0a7eb1db7a9,ab03f2f301cd..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@@ -80,20 -85,18 +80,35 @@@ void nfp_net_get_fw_version(struct nfp_
  	put_unaligned_le32(reg, fw_ver);
  }
  
++<<<<<<< HEAD
 +static dma_addr_t
 +nfp_net_dma_map_rx(struct nfp_net *nn, void *frag, unsigned int bufsz,
 +		   int direction)
 +{
 +	return dma_map_single(&nn->pdev->dev, frag + NFP_NET_RX_BUF_HEADROOM,
 +			      bufsz - NFP_NET_RX_BUF_NON_DATA, direction);
 +}
 +
 +static void
 +nfp_net_dma_unmap_rx(struct nfp_net *nn, dma_addr_t dma_addr,
 +		     unsigned int bufsz, int direction)
 +{
 +	dma_unmap_single(&nn->pdev->dev, dma_addr,
 +			 bufsz - NFP_NET_RX_BUF_NON_DATA, direction);
++=======
+ static dma_addr_t nfp_net_dma_map_rx(struct nfp_net_dp *dp, void *frag)
+ {
+ 	return dma_map_single(dp->dev, frag + NFP_NET_RX_BUF_HEADROOM,
+ 			      dp->fl_bufsz - NFP_NET_RX_BUF_NON_DATA,
+ 			      dp->rx_dma_dir);
+ }
+ 
+ static void nfp_net_dma_unmap_rx(struct nfp_net_dp *dp, dma_addr_t dma_addr)
+ {
+ 	dma_unmap_single(dp->dev, dma_addr,
+ 			 dp->fl_bufsz - NFP_NET_RX_BUF_NON_DATA,
+ 			 dp->rx_dma_dir);
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  }
  
  /* Firmware reconfig
@@@ -957,49 -963,104 +972,107 @@@ static void nfp_net_tx_complete(struct 
  		  tx_ring->rd_p, tx_ring->wr_p, tx_ring->cnt);
  }
  
++<<<<<<< HEAD
++=======
+ static void nfp_net_xdp_complete(struct nfp_net_tx_ring *tx_ring)
+ {
+ 	struct nfp_net_r_vector *r_vec = tx_ring->r_vec;
+ 	struct nfp_net_dp *dp = &r_vec->nfp_net->dp;
+ 	u32 done_pkts = 0, done_bytes = 0;
+ 	int idx, todo;
+ 	u32 qcp_rd_p;
+ 
+ 	/* Work out how many descriptors have been transmitted */
+ 	qcp_rd_p = nfp_qcp_rd_ptr_read(tx_ring->qcp_q);
+ 
+ 	if (qcp_rd_p == tx_ring->qcp_rd_p)
+ 		return;
+ 
+ 	if (qcp_rd_p > tx_ring->qcp_rd_p)
+ 		todo = qcp_rd_p - tx_ring->qcp_rd_p;
+ 	else
+ 		todo = qcp_rd_p + tx_ring->cnt - tx_ring->qcp_rd_p;
+ 
+ 	while (todo--) {
+ 		idx = tx_ring->rd_p & (tx_ring->cnt - 1);
+ 		tx_ring->rd_p++;
+ 
+ 		if (!tx_ring->txbufs[idx].frag)
+ 			continue;
+ 
+ 		nfp_net_dma_unmap_rx(dp, tx_ring->txbufs[idx].dma_addr);
+ 		__free_page(virt_to_page(tx_ring->txbufs[idx].frag));
+ 
+ 		done_pkts++;
+ 		done_bytes += tx_ring->txbufs[idx].real_len;
+ 
+ 		tx_ring->txbufs[idx].dma_addr = 0;
+ 		tx_ring->txbufs[idx].frag = NULL;
+ 		tx_ring->txbufs[idx].fidx = -2;
+ 	}
+ 
+ 	tx_ring->qcp_rd_p = qcp_rd_p;
+ 
+ 	u64_stats_update_begin(&r_vec->tx_sync);
+ 	r_vec->tx_bytes += done_bytes;
+ 	r_vec->tx_pkts += done_pkts;
+ 	u64_stats_update_end(&r_vec->tx_sync);
+ 
+ 	WARN_ONCE(tx_ring->wr_p - tx_ring->rd_p > tx_ring->cnt,
+ 		  "TX ring corruption rd_p=%u wr_p=%u cnt=%u\n",
+ 		  tx_ring->rd_p, tx_ring->wr_p, tx_ring->cnt);
+ }
+ 
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  /**
   * nfp_net_tx_ring_reset() - Free any untransmitted buffers and reset pointers
 - * @dp:		NFP Net data path struct
 + * @nn:		NFP Net device
   * @tx_ring:	TX ring structure
   *
   * Assumes that the device is stopped
   */
  static void
 -nfp_net_tx_ring_reset(struct nfp_net_dp *dp, struct nfp_net_tx_ring *tx_ring)
 +nfp_net_tx_ring_reset(struct nfp_net *nn, struct nfp_net_tx_ring *tx_ring)
  {
 -	struct nfp_net_r_vector *r_vec = tx_ring->r_vec;
  	const struct skb_frag_struct *frag;
  	struct netdev_queue *nd_q;
 +	struct pci_dev *pdev = nn->pdev;
  
  	while (tx_ring->rd_p != tx_ring->wr_p) {
 -		struct nfp_net_tx_buf *tx_buf;
 -		int idx;
 +		int nr_frags, fidx, idx;
 +		struct sk_buff *skb;
  
  		idx = tx_ring->rd_p & (tx_ring->cnt - 1);
 -		tx_buf = &tx_ring->txbufs[idx];
 +		skb = tx_ring->txbufs[idx].skb;
 +		nr_frags = skb_shinfo(skb)->nr_frags;
 +		fidx = tx_ring->txbufs[idx].fidx;
  
++<<<<<<< HEAD
 +		if (fidx == -1) {
 +			/* unmap head */
 +			dma_unmap_single(&pdev->dev,
 +					 tx_ring->txbufs[idx].dma_addr,
 +					 skb_headlen(skb), DMA_TO_DEVICE);
++=======
+ 		if (tx_ring == r_vec->xdp_ring) {
+ 			nfp_net_dma_unmap_rx(dp, tx_buf->dma_addr);
+ 			__free_page(virt_to_page(tx_ring->txbufs[idx].frag));
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  		} else {
 -			struct sk_buff *skb = tx_ring->txbufs[idx].skb;
 -			int nr_frags = skb_shinfo(skb)->nr_frags;
 -
 -			if (tx_buf->fidx == -1) {
 -				/* unmap head */
 -				dma_unmap_single(dp->dev, tx_buf->dma_addr,
 -						 skb_headlen(skb),
 -						 DMA_TO_DEVICE);
 -			} else {
 -				/* unmap fragment */
 -				frag = &skb_shinfo(skb)->frags[tx_buf->fidx];
 -				dma_unmap_page(dp->dev, tx_buf->dma_addr,
 -					       skb_frag_size(frag),
 -					       DMA_TO_DEVICE);
 -			}
 -
 -			/* check for last gather fragment */
 -			if (tx_buf->fidx == nr_frags - 1)
 -				dev_kfree_skb_any(skb);
 +			/* unmap fragment */
 +			frag = &skb_shinfo(skb)->frags[fidx];
 +			dma_unmap_page(&pdev->dev,
 +				       tx_ring->txbufs[idx].dma_addr,
 +				       skb_frag_size(frag), DMA_TO_DEVICE);
  		}
  
 -		tx_buf->dma_addr = 0;
 -		tx_buf->skb = NULL;
 -		tx_buf->fidx = -2;
 +		/* check for last gather fragment */
 +		if (fidx == nr_frags - 1)
 +			dev_kfree_skb_any(skb);
 +
 +		tx_ring->txbufs[idx].dma_addr = 0;
 +		tx_ring->txbufs[idx].skb = NULL;
 +		tx_ring->txbufs[idx].fidx = -2;
  
  		tx_ring->qcp_rd_p++;
  		tx_ring->rd_p++;
@@@ -1059,29 -1132,31 +1132,43 @@@ nfp_net_calc_fl_bufsz(struct nfp_net *n
   * Return: allocated page frag or NULL on failure.
   */
  static void *
 -nfp_net_rx_alloc_one(struct nfp_net_dp *dp, struct nfp_net_rx_ring *rx_ring,
 -		     dma_addr_t *dma_addr)
 +nfp_net_rx_alloc_one(struct nfp_net_rx_ring *rx_ring, dma_addr_t *dma_addr,
 +		     unsigned int fl_bufsz)
  {
++<<<<<<< HEAD
 +	struct nfp_net *nn = rx_ring->r_vec->nfp_net;
++=======
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  	void *frag;
  
 -	if (!dp->xdp_prog)
 -		frag = netdev_alloc_frag(dp->fl_bufsz);
 -	else
 -		frag = page_address(alloc_page(GFP_KERNEL | __GFP_COLD));
 +	frag = netdev_alloc_frag(fl_bufsz);
  	if (!frag) {
 -		nn_dp_warn(dp, "Failed to alloc receive page frag\n");
 +		nn_warn_ratelimit(nn, "Failed to alloc receive page frag\n");
  		return NULL;
  	}
  
++<<<<<<< HEAD
 +	*dma_addr = nfp_net_dma_map_rx(nn, frag, fl_bufsz, DMA_FROM_DEVICE);
 +	if (dma_mapping_error(&nn->pdev->dev, *dma_addr)) {
 +		skb_free_frag(frag);
 +		nn_warn_ratelimit(nn, "Failed to map DMA RX buffer\n");
++=======
+ 	*dma_addr = nfp_net_dma_map_rx(dp, frag);
+ 	if (dma_mapping_error(dp->dev, *dma_addr)) {
+ 		nfp_net_free_frag(frag, dp->xdp_prog);
+ 		nn_dp_warn(dp, "Failed to map DMA RX buffer\n");
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  		return NULL;
  	}
  
  	return frag;
  }
  
++<<<<<<< HEAD
 +static void *nfp_net_napi_alloc_one(struct nfp_net *nn, dma_addr_t *dma_addr)
++=======
+ static void *nfp_net_napi_alloc_one(struct nfp_net_dp *dp, dma_addr_t *dma_addr)
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  {
  	void *frag;
  
@@@ -1091,10 -1169,10 +1178,17 @@@
  		return NULL;
  	}
  
++<<<<<<< HEAD
 +	*dma_addr = nfp_net_dma_map_rx(nn, frag, nn->fl_bufsz, DMA_FROM_DEVICE);
 +	if (dma_mapping_error(&nn->pdev->dev, *dma_addr)) {
 +		skb_free_frag(frag);
 +		nn_warn_ratelimit(nn, "Failed to map DMA RX buffer\n");
++=======
+ 	*dma_addr = nfp_net_dma_map_rx(dp, frag);
+ 	if (dma_mapping_error(dp->dev, *dma_addr)) {
+ 		nfp_net_free_frag(frag, dp->xdp_prog);
+ 		nn_dp_warn(dp, "Failed to map DMA RX buffer\n");
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  		return NULL;
  	}
  
@@@ -1182,9 -1261,8 +1276,14 @@@ nfp_net_rx_ring_bufs_free(struct nfp_ne
  		if (!rx_ring->rxbufs[i].frag)
  			continue;
  
++<<<<<<< HEAD
 +		nfp_net_dma_unmap_rx(nn, rx_ring->rxbufs[i].dma_addr,
 +				     rx_ring->bufsz, DMA_FROM_DEVICE);
 +		skb_free_frag(rx_ring->rxbufs[i].frag);
++=======
+ 		nfp_net_dma_unmap_rx(dp, rx_ring->rxbufs[i].dma_addr);
+ 		nfp_net_free_frag(rx_ring->rxbufs[i].frag, dp->xdp_prog);
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  		rx_ring->rxbufs[i].dma_addr = 0;
  		rx_ring->rxbufs[i].frag = NULL;
  	}
@@@ -1351,6 -1450,69 +1450,72 @@@ nfp_net_rx_drop(struct nfp_net_r_vecto
  		dev_kfree_skb_any(skb);
  }
  
++<<<<<<< HEAD
++=======
+ static bool
+ nfp_net_tx_xdp_buf(struct nfp_net_dp *dp, struct nfp_net_rx_ring *rx_ring,
+ 		   struct nfp_net_tx_ring *tx_ring,
+ 		   struct nfp_net_rx_buf *rxbuf, unsigned int pkt_off,
+ 		   unsigned int pkt_len)
+ {
+ 	struct nfp_net_tx_buf *txbuf;
+ 	struct nfp_net_tx_desc *txd;
+ 	dma_addr_t new_dma_addr;
+ 	void *new_frag;
+ 	int wr_idx;
+ 
+ 	if (unlikely(nfp_net_tx_full(tx_ring, 1))) {
+ 		nfp_net_rx_drop(rx_ring->r_vec, rx_ring, rxbuf, NULL);
+ 		return false;
+ 	}
+ 
+ 	new_frag = nfp_net_napi_alloc_one(dp, &new_dma_addr);
+ 	if (unlikely(!new_frag)) {
+ 		nfp_net_rx_drop(rx_ring->r_vec, rx_ring, rxbuf, NULL);
+ 		return false;
+ 	}
+ 	nfp_net_rx_give_one(rx_ring, new_frag, new_dma_addr);
+ 
+ 	wr_idx = tx_ring->wr_p & (tx_ring->cnt - 1);
+ 
+ 	/* Stash the soft descriptor of the head then initialize it */
+ 	txbuf = &tx_ring->txbufs[wr_idx];
+ 	txbuf->frag = rxbuf->frag;
+ 	txbuf->dma_addr = rxbuf->dma_addr;
+ 	txbuf->fidx = -1;
+ 	txbuf->pkt_cnt = 1;
+ 	txbuf->real_len = pkt_len;
+ 
+ 	dma_sync_single_for_device(dp->dev, rxbuf->dma_addr + pkt_off,
+ 				   pkt_len, DMA_BIDIRECTIONAL);
+ 
+ 	/* Build TX descriptor */
+ 	txd = &tx_ring->txds[wr_idx];
+ 	txd->offset_eop = PCIE_DESC_TX_EOP;
+ 	txd->dma_len = cpu_to_le16(pkt_len);
+ 	nfp_desc_set_dma_addr(txd, rxbuf->dma_addr + pkt_off);
+ 	txd->data_len = cpu_to_le16(pkt_len);
+ 
+ 	txd->flags = 0;
+ 	txd->mss = 0;
+ 	txd->l4_offset = 0;
+ 
+ 	tx_ring->wr_p++;
+ 	tx_ring->wr_ptr_add++;
+ 	return true;
+ }
+ 
+ static int nfp_net_run_xdp(struct bpf_prog *prog, void *data, unsigned int len)
+ {
+ 	struct xdp_buff xdp;
+ 
+ 	xdp.data = data;
+ 	xdp.data_end = data + len;
+ 
+ 	return bpf_prog_run_xdp(prog, &xdp);
+ }
+ 
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  /**
   * nfp_net_rx() - receive up to @budget packets on @rx_ring
   * @rx_ring:   RX ring to receive from
@@@ -1370,6 -1535,11 +1535,14 @@@ static int nfp_net_rx(struct nfp_net_rx
  	int pkts_polled = 0;
  	int idx;
  
++<<<<<<< HEAD
++=======
+ 	rcu_read_lock();
+ 	xdp_prog = READ_ONCE(dp->xdp_prog);
+ 	true_bufsz = xdp_prog ? PAGE_SIZE : dp->fl_bufsz;
+ 	tx_ring = r_vec->xdp_ring;
+ 
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  	while (pkts_polled < budget) {
  		unsigned int meta_len, data_len, data_off, pkt_len, pkt_off;
  		struct nfp_net_rx_buf *rxbuf;
@@@ -1425,17 -1626,13 +1598,25 @@@
  			nfp_net_rx_drop(r_vec, rx_ring, rxbuf, NULL);
  			continue;
  		}
++<<<<<<< HEAD
 +
 +		nfp_net_set_hash(nn->netdev, skb, rxd);
 +
 +		new_frag = nfp_net_napi_alloc_one(nn, &new_dma_addr);
++=======
+ 		new_frag = nfp_net_napi_alloc_one(dp, &new_dma_addr);
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  		if (unlikely(!new_frag)) {
  			nfp_net_rx_drop(r_vec, rx_ring, rxbuf, skb);
  			continue;
  		}
  
++<<<<<<< HEAD
 +		nfp_net_dma_unmap_rx(nn, rx_ring->rxbufs[idx].dma_addr,
 +				     nn->fl_bufsz, DMA_FROM_DEVICE);
++=======
+ 		nfp_net_dma_unmap_rx(dp, rxbuf->dma_addr);
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  
  		nfp_net_rx_give_one(rx_ring, new_frag, new_dma_addr);
  
@@@ -2595,8 -2832,90 +2776,94 @@@ static void nfp_net_del_vxlan_port(stru
  		nfp_net_set_vxlan_port(nn, idx, 0);
  }
  
++<<<<<<< HEAD
++=======
+ static int nfp_net_xdp_offload(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct tc_cls_bpf_offload cmd = {
+ 		.prog = prog,
+ 	};
+ 	int ret;
+ 
+ 	if (!nfp_net_ebpf_capable(nn))
+ 		return -EINVAL;
+ 
+ 	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF) {
+ 		if (!nn->dp.bpf_offload_xdp)
+ 			return prog ? -EBUSY : 0;
+ 		cmd.command = prog ? TC_CLSBPF_REPLACE : TC_CLSBPF_DESTROY;
+ 	} else {
+ 		if (!prog)
+ 			return 0;
+ 		cmd.command = TC_CLSBPF_ADD;
+ 	}
+ 
+ 	ret = nfp_net_bpf_offload(nn, &cmd);
+ 	/* Stop offload if replace not possible */
+ 	if (ret && cmd.command == TC_CLSBPF_REPLACE)
+ 		nfp_net_xdp_offload(nn, NULL);
+ 	nn->dp.bpf_offload_xdp = prog && !ret;
+ 	return ret;
+ }
+ 
+ static int nfp_net_xdp_setup(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct bpf_prog *old_prog = nn->dp.xdp_prog;
+ 	struct nfp_net_dp *dp;
+ 	int err;
+ 
+ 	if (prog && prog->xdp_adjust_head) {
+ 		nn_err(nn, "Does not support bpf_xdp_adjust_head()\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 	if (!prog && !nn->dp.xdp_prog)
+ 		return 0;
+ 	if (prog && nn->dp.xdp_prog) {
+ 		prog = xchg(&nn->dp.xdp_prog, prog);
+ 		bpf_prog_put(prog);
+ 		nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 		return 0;
+ 	}
+ 
+ 	dp = nfp_net_clone_dp(nn);
+ 	if (!dp)
+ 		return -ENOMEM;
+ 
+ 	dp->xdp_prog = prog;
+ 	dp->num_tx_rings += prog ? nn->dp.num_rx_rings : -nn->dp.num_rx_rings;
+ 	dp->rx_dma_dir = prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
+ 
+ 	/* We need RX reconfig to remap the buffers (BIDIR vs FROM_DEV) */
+ 	err = nfp_net_ring_reconfig(nn, dp);
+ 	if (err)
+ 		return err;
+ 
+ 	if (old_prog)
+ 		bpf_prog_put(old_prog);
+ 
+ 	nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_net_xdp(struct net_device *netdev, struct netdev_xdp *xdp)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 
+ 	switch (xdp->command) {
+ 	case XDP_SETUP_PROG:
+ 		return nfp_net_xdp_setup(nn, xdp->prog);
+ 	case XDP_QUERY_PROG:
+ 		xdp->prog_attached = !!nn->dp.xdp_prog;
+ 		return 0;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  static const struct net_device_ops nfp_net_netdev_ops = {
 +	.ndo_size		= sizeof(struct net_device_ops),
  	.ndo_open		= nfp_net_netdev_open,
  	.ndo_stop		= nfp_net_netdev_close,
  	.ndo_start_xmit		= nfp_net_tx,
@@@ -2745,6 -3113,10 +3012,13 @@@ int nfp_net_netdev_init(struct net_devi
  	struct nfp_net *nn = netdev_priv(netdev);
  	int err;
  
++<<<<<<< HEAD
++=======
+ 	nn->dp.chained_metadata_format = nn->fw_ver.major > 3;
+ 
+ 	nn->dp.rx_dma_dir = DMA_FROM_DEVICE;
+ 
++>>>>>>> c487e6b199ea (nfp: store dma direction in data path structure)
  	/* Get some of the read-only fields from the BAR */
  	nn->cap = nn_readl(nn, NFP_NET_CFG_CAP);
  	nn->max_mtu = nn_readl(nn, NFP_NET_CFG_MAX_MTU);
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net.h
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_common.c
