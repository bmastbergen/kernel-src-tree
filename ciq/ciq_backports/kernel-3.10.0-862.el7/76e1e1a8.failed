nfp: use dp to carry mtu at reconfig time

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 76e1e1a89351832ea5d9f7b57677e7420ba6bc92
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/76e1e1a8.failed

Move the mtu member from ring set to data path struct.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 76e1e1a89351832ea5d9f7b57677e7420ba6bc92)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/nfp_net.h
#	drivers/net/ethernet/netronome/nfp/nfp_net_common.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net.h
index 1826ee93d1da,84774c281b61..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net.h
@@@ -417,18 -425,77 +417,85 @@@ static inline bool nfp_net_fw_ver_eq(st
  	       fw_ver->minor == minor;
  }
  
++<<<<<<< HEAD
++=======
+ struct nfp_stat_pair {
+ 	u64 pkts;
+ 	u64 bytes;
+ };
+ 
+ /**
+  * struct nfp_net_dp - NFP network device datapath data structure
+  * @dev:		Backpointer to struct device
+  * @netdev:		Backpointer to net_device structure
+  * @is_vf:		Is the driver attached to a VF?
+  * @bpf_offload_skip_sw:  Offloaded BPF program will not be rerun by cls_bpf
+  * @bpf_offload_xdp:	Offloaded BPF program is XDP
+  * @chained_metadata_format:  Firemware will use new metadata format
+  * @ctrl:		Local copy of the control register/word.
+  * @fl_bufsz:		Currently configured size of the freelist buffers
+  * @rx_offset:		Offset in the RX buffers where packet data starts
+  * @xdp_prog:		Installed XDP program
+  * @tx_rings:		Array of pre-allocated TX ring structures
+  * @rx_rings:		Array of pre-allocated RX ring structures
+  * @ctrl_bar:		Pointer to mapped control BAR
+  *
+  * @txd_cnt:		Size of the TX ring in number of descriptors
+  * @rxd_cnt:		Size of the RX ring in number of descriptors
+  * @num_r_vecs:		Number of used ring vectors
+  * @num_tx_rings:	Currently configured number of TX rings
+  * @num_stack_tx_rings:	Number of TX rings used by the stack (not XDP)
+  * @num_rx_rings:	Currently configured number of RX rings
+  * @mtu:		Device MTU
+  */
+ struct nfp_net_dp {
+ 	struct device *dev;
+ 	struct net_device *netdev;
+ 
+ 	unsigned is_vf:1;
+ 	unsigned bpf_offload_skip_sw:1;
+ 	unsigned bpf_offload_xdp:1;
+ 	unsigned chained_metadata_format:1;
+ 
+ 	u32 ctrl;
+ 	u32 fl_bufsz;
+ 
+ 	u32 rx_offset;
+ 
+ 	struct bpf_prog *xdp_prog;
+ 
+ 	struct nfp_net_tx_ring *tx_rings;
+ 	struct nfp_net_rx_ring *rx_rings;
+ 
+ 	u8 __iomem *ctrl_bar;
+ 
+ 	/* Cold data follows */
+ 
+ 	unsigned int txd_cnt;
+ 	unsigned int rxd_cnt;
+ 
+ 	unsigned int num_r_vecs;
+ 
+ 	unsigned int num_tx_rings;
+ 	unsigned int num_stack_tx_rings;
+ 	unsigned int num_rx_rings;
+ 
+ 	unsigned int mtu;
+ };
+ 
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  /**
   * struct nfp_net - NFP network device structure
 - * @dp:			Datapath structure
 - * @fw_ver:		Firmware version
 + * @pdev:               Backpointer to PCI device
 + * @netdev:             Backpointer to net_device structure
 + * @is_vf:              Is the driver attached to a VF?
 + * @fw_loaded:          Is the firmware loaded?
 + * @ctrl:               Local copy of the control register/word.
 + * @fl_bufsz:           Currently configured size of the freelist buffers
 + * @rx_offset:		Offset in the RX buffers where packet data starts
 + * @fw_ver:             Firmware version
   * @cap:                Capabilities advertised by the Firmware
   * @max_mtu:            Maximum support MTU advertised by the Firmware
 - * @rss_hfunc:		RSS selected hash function
   * @rss_cfg:            RSS configuration
   * @rss_key:            RSS secret key
   * @rss_itbl:           RSS indirection table
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index e0a7eb1db7a9,862e86cb5688..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@@ -1031,16 -1099,16 +1031,25 @@@ static void nfp_net_tx_timeout(struct n
  /* Receive processing
   */
  static unsigned int
++<<<<<<< HEAD
 +nfp_net_calc_fl_bufsz(struct nfp_net *nn, unsigned int mtu)
++=======
+ nfp_net_calc_fl_bufsz(struct nfp_net_dp *dp)
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  {
  	unsigned int fl_bufsz;
  
  	fl_bufsz = NFP_NET_RX_BUF_HEADROOM;
 -	if (dp->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC)
 +	if (nn->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC)
  		fl_bufsz += NFP_NET_MAX_PREPEND;
  	else
++<<<<<<< HEAD
 +		fl_bufsz += nn->rx_offset;
 +	fl_bufsz += ETH_HLEN + VLAN_HLEN * 2 + mtu;
++=======
+ 		fl_bufsz += dp->rx_offset;
+ 	fl_bufsz += ETH_HLEN + VLAN_HLEN * 2 + dp->mtu;
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  
  	fl_bufsz = SKB_DATA_ALIGN(fl_bufsz);
  	fl_bufsz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
@@@ -1701,20 -1928,24 +1710,35 @@@ err_free_ring
  }
  
  static void
 -nfp_net_rx_ring_set_swap(struct nfp_net *nn, struct nfp_net_dp *dp,
 -			 struct nfp_net_ring_set *s)
 +nfp_net_rx_ring_set_swap(struct nfp_net *nn, struct nfp_net_ring_set *s)
  {
  	struct nfp_net_ring_set new = *s;
 -	struct nfp_net_dp new_dp = *dp;
  
++<<<<<<< HEAD
 +	s->mtu = nn->netdev->mtu;
 +	s->dcnt = nn->rxd_cnt;
 +	s->rings = nn->rx_rings;
 +	s->n_rings = nn->num_rx_rings;
 +
 +	nn->netdev->mtu = new.mtu;
 +	nn->fl_bufsz = nfp_net_calc_fl_bufsz(nn, new.mtu);
 +	nn->rxd_cnt = new.dcnt;
 +	nn->rx_rings = new.rings;
 +	nn->num_rx_rings = new.n_rings;
++=======
+ 	dp->fl_bufsz = nn->dp.fl_bufsz;
+ 	dp->mtu = nn->dp.netdev->mtu;
+ 	s->dcnt = nn->dp.rxd_cnt;
+ 	s->rings = nn->dp.rx_rings;
+ 	s->n_rings = nn->dp.num_rx_rings;
+ 
+ 	nn->dp.mtu = new_dp.mtu;
+ 	nn->dp.netdev->mtu = new_dp.mtu;
+ 	nn->dp.fl_bufsz = new_dp.fl_bufsz;
+ 	nn->dp.rxd_cnt = new.dcnt;
+ 	nn->dp.rx_rings = new.rings;
+ 	nn->dp.num_rx_rings = new.n_rings;
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  }
  
  static void
@@@ -2019,13 -2255,12 +2043,18 @@@ static int nfp_net_netdev_open(struct n
  {
  	struct nfp_net *nn = netdev_priv(netdev);
  	struct nfp_net_ring_set rx = {
++<<<<<<< HEAD
 +		.n_rings = nn->num_rx_rings,
 +		.mtu = nn->netdev->mtu,
 +		.dcnt = nn->rxd_cnt,
++=======
+ 		.n_rings = nn->dp.num_rx_rings,
+ 		.dcnt = nn->dp.rxd_cnt,
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  	};
  	struct nfp_net_ring_set tx = {
 -		.n_rings = nn->dp.num_tx_rings,
 -		.dcnt = nn->dp.txd_cnt,
 +		.n_rings = nn->num_tx_rings,
 +		.dcnt = nn->txd_cnt,
  	};
  	int err, r;
  
@@@ -2220,7 -2457,17 +2249,21 @@@ static void nfp_net_rss_init_itbl(struc
  
  	for (i = 0; i < sizeof(nn->rss_itbl); i++)
  		nn->rss_itbl[i] =
++<<<<<<< HEAD
 +			ethtool_rxfh_indir_default(i, nn->num_rx_rings);
++=======
+ 			ethtool_rxfh_indir_default(i, nn->dp.num_rx_rings);
+ }
+ 
+ static void nfp_net_dp_swap(struct nfp_net *nn, struct nfp_net_dp *dp)
+ {
+ 	struct nfp_net_dp new_dp = *dp;
+ 
+ 	*dp = nn->dp;
+ 	nn->dp = new_dp;
+ 
+ 	nn->dp.netdev->mtu = new_dp.mtu;
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  }
  
  static int
@@@ -2259,37 -2508,89 +2302,51 @@@ nfp_net_ring_swap_enable(struct nfp_ne
  	return __nfp_net_set_config_and_enable(nn);
  }
  
 -struct nfp_net_dp *nfp_net_clone_dp(struct nfp_net *nn)
 -{
 -	struct nfp_net_dp *new;
 -
 -	new = kmalloc(sizeof(*new), GFP_KERNEL);
 -	if (!new)
 -		return NULL;
 -
 -	*new = nn->dp;
 -
 -	/* Clear things which need to be recomputed */
 -	new->fl_bufsz = 0;
 -	new->tx_rings = NULL;
 -	new->rx_rings = NULL;
 -	new->num_r_vecs = 0;
 -	new->num_stack_tx_rings = 0;
 -
 -	return new;
 -}
 -
 -static int
 -nfp_net_check_config(struct nfp_net *nn, struct nfp_net_dp *dp,
 -		     struct bpf_prog *xdp_prog,
 -		     struct nfp_net_ring_set *rx, struct nfp_net_ring_set *tx)
 -{
 -	/* XDP-enabled tests */
 -	if (!xdp_prog)
 -		return 0;
 -	if (dp->fl_bufsz > PAGE_SIZE) {
 -		nn_warn(nn, "MTU too large w/ XDP enabled\n");
 -		return -EINVAL;
 -	}
 -	if (tx && tx->n_rings > nn->max_tx_rings) {
 -		nn_warn(nn, "Insufficient number of TX rings w/ XDP enabled\n");
 -		return -EINVAL;
 -	}
 -
 -	return 0;
 -}
 -
  static void
 -nfp_net_ring_reconfig_down(struct nfp_net *nn, struct nfp_net_dp *dp,
 -			   struct bpf_prog **xdp_prog,
 +nfp_net_ring_reconfig_down(struct nfp_net *nn,
  			   struct nfp_net_ring_set *rx,
 -			   struct nfp_net_ring_set *tx)
 -{
 -	nfp_net_dp_swap(nn, dp);
 -
 +			   struct nfp_net_ring_set *tx,
 +			   unsigned int num_vecs)
 +{
 +	nn->netdev->mtu = rx ? rx->mtu : nn->netdev->mtu;
 +	nn->fl_bufsz = nfp_net_calc_fl_bufsz(nn, nn->netdev->mtu);
 +	nn->rxd_cnt = rx ? rx->dcnt : nn->rxd_cnt;
 +	nn->txd_cnt = tx ? tx->dcnt : nn->txd_cnt;
 +	nn->num_rx_rings = rx ? rx->n_rings : nn->num_rx_rings;
 +	nn->num_tx_rings = tx ? tx->n_rings : nn->num_tx_rings;
 +	nn->num_r_vecs = num_vecs;
 +
++<<<<<<< HEAD
 +	if (!netif_is_rxfh_configured(nn->netdev))
++=======
+ 	nn->dp.rxd_cnt = rx ? rx->dcnt : nn->dp.rxd_cnt;
+ 	nn->dp.txd_cnt = tx ? tx->dcnt : nn->dp.txd_cnt;
+ 	nn->dp.num_rx_rings = rx ? rx->n_rings : nn->dp.num_rx_rings;
+ 	nn->dp.num_tx_rings = tx ? tx->n_rings : nn->dp.num_tx_rings;
+ 	*xdp_prog = xchg(&nn->dp.xdp_prog, *xdp_prog);
+ 
+ 	if (!netif_is_rxfh_configured(nn->dp.netdev))
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  		nfp_net_rss_init_itbl(nn);
  }
  
  int
 -nfp_net_ring_reconfig(struct nfp_net *nn, struct nfp_net_dp *dp,
 -		      struct bpf_prog **xdp_prog,
 -		      struct nfp_net_ring_set *rx, struct nfp_net_ring_set *tx)
 +nfp_net_ring_reconfig(struct nfp_net *nn, struct nfp_net_ring_set *rx,
 +		      struct nfp_net_ring_set *tx)
  {
 -	int r, err;
 +	unsigned int num_vecs, r;
 +	int err;
  
++<<<<<<< HEAD
 +	num_vecs = max(rx ? rx->n_rings : nn->num_rx_rings,
 +		       tx ? tx->n_rings : nn->num_tx_rings);
++=======
+ 	dp->fl_bufsz = nfp_net_calc_fl_bufsz(dp);
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  
 -	dp->num_stack_tx_rings = tx ? tx->n_rings : dp->num_tx_rings;
 -	if (*xdp_prog)
 -		dp->num_stack_tx_rings -= rx ? rx->n_rings : dp->num_rx_rings;
 -
 -	dp->num_r_vecs = max(rx ? rx->n_rings : dp->num_rx_rings,
 -			     dp->num_stack_tx_rings);
 -
 -	err = nfp_net_check_config(nn, dp, *xdp_prog, rx, tx);
 -	if (err)
 -		goto exit_free_dp;
 -
 -	if (!netif_running(dp->netdev)) {
 -		nfp_net_ring_reconfig_down(nn, dp, xdp_prog, rx, tx);
 -
 -		err = 0;
 -		goto exit_free_dp;
 +	if (!netif_running(nn->netdev)) {
 +		nfp_net_ring_reconfig_down(nn, rx, tx, num_vecs);
 +		return 0;
  	}
  
  	/* Prepare new rings */
@@@ -2354,16 -2658,22 +2411,31 @@@ static int nfp_net_change_mtu(struct ne
  {
  	struct nfp_net *nn = netdev_priv(netdev);
  	struct nfp_net_ring_set rx = {
++<<<<<<< HEAD
 +		.n_rings = nn->num_rx_rings,
 +		.mtu = new_mtu,
 +		.dcnt = nn->rxd_cnt,
++=======
+ 		.n_rings = nn->dp.num_rx_rings,
+ 		.dcnt = nn->dp.rxd_cnt,
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  	};
 -	struct nfp_net_dp *dp;
  
++<<<<<<< HEAD
 +	return nfp_net_ring_reconfig(nn, &rx, NULL);
++=======
+ 	dp = nfp_net_clone_dp(nn);
+ 	if (!dp)
+ 		return -ENOMEM;
+ 
+ 	dp->mtu = new_mtu;
+ 
+ 	return nfp_net_ring_reconfig(nn, dp, &nn->dp.xdp_prog, &rx, NULL);
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  }
  
 -static void nfp_net_stat64(struct net_device *netdev,
 -			   struct rtnl_link_stats64 *stats)
 +static struct rtnl_link_stats64 *nfp_net_stat64(struct net_device *netdev,
 +						struct rtnl_link_stats64 *stats)
  {
  	struct nfp_net *nn = netdev_priv(netdev);
  	int r;
@@@ -2595,8 -2957,96 +2667,100 @@@ static void nfp_net_del_vxlan_port(stru
  		nfp_net_set_vxlan_port(nn, idx, 0);
  }
  
++<<<<<<< HEAD
++=======
+ static int nfp_net_xdp_offload(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct tc_cls_bpf_offload cmd = {
+ 		.prog = prog,
+ 	};
+ 	int ret;
+ 
+ 	if (!nfp_net_ebpf_capable(nn))
+ 		return -EINVAL;
+ 
+ 	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF) {
+ 		if (!nn->dp.bpf_offload_xdp)
+ 			return prog ? -EBUSY : 0;
+ 		cmd.command = prog ? TC_CLSBPF_REPLACE : TC_CLSBPF_DESTROY;
+ 	} else {
+ 		if (!prog)
+ 			return 0;
+ 		cmd.command = TC_CLSBPF_ADD;
+ 	}
+ 
+ 	ret = nfp_net_bpf_offload(nn, &cmd);
+ 	/* Stop offload if replace not possible */
+ 	if (ret && cmd.command == TC_CLSBPF_REPLACE)
+ 		nfp_net_xdp_offload(nn, NULL);
+ 	nn->dp.bpf_offload_xdp = prog && !ret;
+ 	return ret;
+ }
+ 
+ static int nfp_net_xdp_setup(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_net_ring_set rx = {
+ 		.n_rings = nn->dp.num_rx_rings,
+ 		.dcnt = nn->dp.rxd_cnt,
+ 	};
+ 	struct nfp_net_ring_set tx = {
+ 		.n_rings = nn->dp.num_tx_rings,
+ 		.dcnt = nn->dp.txd_cnt,
+ 	};
+ 	struct nfp_net_dp *dp;
+ 	int err;
+ 
+ 	if (prog && prog->xdp_adjust_head) {
+ 		nn_err(nn, "Does not support bpf_xdp_adjust_head()\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 	if (!prog && !nn->dp.xdp_prog)
+ 		return 0;
+ 	if (prog && nn->dp.xdp_prog) {
+ 		prog = xchg(&nn->dp.xdp_prog, prog);
+ 		bpf_prog_put(prog);
+ 		nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 		return 0;
+ 	}
+ 
+ 	dp = nfp_net_clone_dp(nn);
+ 	if (!dp)
+ 		return -ENOMEM;
+ 
+ 	tx.n_rings += prog ? nn->dp.num_rx_rings : -nn->dp.num_rx_rings;
+ 
+ 	/* We need RX reconfig to remap the buffers (BIDIR vs FROM_DEV) */
+ 	err = nfp_net_ring_reconfig(nn, dp, &prog, &rx, &tx);
+ 	if (err)
+ 		return err;
+ 
+ 	/* @prog got swapped and is now the old one */
+ 	if (prog)
+ 		bpf_prog_put(prog);
+ 
+ 	nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_net_xdp(struct net_device *netdev, struct netdev_xdp *xdp)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 
+ 	switch (xdp->command) {
+ 	case XDP_SETUP_PROG:
+ 		return nfp_net_xdp_setup(nn, xdp->prog);
+ 	case XDP_QUERY_PROG:
+ 		xdp->prog_attached = !!nn->dp.xdp_prog;
+ 		return 0;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  static const struct net_device_ops nfp_net_netdev_ops = {
 +	.ndo_size		= sizeof(struct net_device_ops),
  	.ndo_open		= nfp_net_netdev_open,
  	.ndo_stop		= nfp_net_netdev_close,
  	.ndo_start_xmit		= nfp_net_tx,
@@@ -2762,7 -3263,8 +2926,12 @@@ int nfp_net_netdev_init(struct net_devi
  		netdev->mtu = nn->max_mtu;
  	else
  		netdev->mtu = NFP_NET_DEFAULT_MTU;
++<<<<<<< HEAD
 +	nn->fl_bufsz = nfp_net_calc_fl_bufsz(nn, netdev->mtu);
++=======
+ 	nn->dp.mtu = netdev->mtu;
+ 	nn->dp.fl_bufsz = nfp_net_calc_fl_bufsz(&nn->dp);
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  
  	/* Advertise/enable offloads based on capabilities
  	 *
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
index dfbf6b94ff5b,eccb01f3659f..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
@@@ -175,8 -188,7 +175,12 @@@ static int nfp_net_set_ring_size(struc
  {
  	struct nfp_net_ring_set *reconfig_rx = NULL, *reconfig_tx = NULL;
  	struct nfp_net_ring_set rx = {
++<<<<<<< HEAD
 +		.n_rings = nn->num_rx_rings,
 +		.mtu = nn->netdev->mtu,
++=======
+ 		.n_rings = nn->dp.num_rx_rings,
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  		.dcnt = rxd_cnt,
  	};
  	struct nfp_net_ring_set tx = {
@@@ -738,8 -769,7 +742,12 @@@ static int nfp_net_set_num_rings(struc
  	struct nfp_net_ring_set *reconfig_rx = NULL, *reconfig_tx = NULL;
  	struct nfp_net_ring_set rx = {
  		.n_rings = total_rx,
++<<<<<<< HEAD
 +		.mtu = nn->netdev->mtu,
 +		.dcnt = nn->rxd_cnt,
++=======
+ 		.dcnt = nn->dp.rxd_cnt,
++>>>>>>> 76e1e1a89351 (nfp: use dp to carry mtu at reconfig time)
  	};
  	struct nfp_net_ring_set tx = {
  		.n_rings = total_tx,
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net.h
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_common.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
