KVM: LAPIC: Fix lapic timer injection delay

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Wanpeng Li <wanpeng.li@hotmail.com>
commit c853354429f7ec88f9cdde4e46e69a2c0e3c8310
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c8533544.failed

If the TSC deadline timer is programmed really close to the deadline or
even in the past, the computation in vmx_set_hv_timer will program the
absolute target tsc value to vmcs preemption timer field w/ delta == 0,
then plays a vmentry and an upcoming vmx preemption timer fire vmexit
dance, the lapic timer injection is delayed due to this duration. Actually
the lapic timer which is emulated by hrtimer can handle this correctly.

This patch fixes it by firing the lapic timer and injecting a timer interrupt
immediately during the next vmentry if the TSC deadline timer is programmed
really close to the deadline or even in the past. This saves ~300 cycles on
the tsc_deadline_timer test of apic.flat.

	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c853354429f7ec88f9cdde4e46e69a2c0e3c8310)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/lapic.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/lapic.c
index 0e8704ed991e,2819d4c123eb..000000000000
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@@ -1373,53 -1411,200 +1373,241 @@@ static void start_sw_tscdeadline(struc
  	local_irq_restore(flags);
  }
  
++<<<<<<< HEAD
++=======
+ static void start_sw_period(struct kvm_lapic *apic)
+ {
+ 	if (!apic->lapic_timer.period)
+ 		return;
+ 
+ 	if (apic_lvtt_oneshot(apic) &&
+ 	    ktime_after(ktime_get(),
+ 			apic->lapic_timer.target_expiration)) {
+ 		apic_timer_expired(apic);
+ 		return;
+ 	}
+ 
+ 	hrtimer_start(&apic->lapic_timer.timer,
+ 		apic->lapic_timer.target_expiration,
+ 		HRTIMER_MODE_ABS_PINNED);
+ }
+ 
+ static bool set_target_expiration(struct kvm_lapic *apic)
+ {
+ 	ktime_t now;
+ 	u64 tscl = rdtsc();
+ 
+ 	now = ktime_get();
+ 	apic->lapic_timer.period = (u64)kvm_lapic_get_reg(apic, APIC_TMICT)
+ 		* APIC_BUS_CYCLE_NS * apic->divide_count;
+ 
+ 	if (!apic->lapic_timer.period)
+ 		return false;
+ 
+ 	/*
+ 	 * Do not allow the guest to program periodic timers with small
+ 	 * interval, since the hrtimers are not throttled by the host
+ 	 * scheduler.
+ 	 */
+ 	if (apic_lvtt_period(apic)) {
+ 		s64 min_period = min_timer_period_us * 1000LL;
+ 
+ 		if (apic->lapic_timer.period < min_period) {
+ 			pr_info_ratelimited(
+ 			    "kvm: vcpu %i: requested %lld ns "
+ 			    "lapic timer period limited to %lld ns\n",
+ 			    apic->vcpu->vcpu_id,
+ 			    apic->lapic_timer.period, min_period);
+ 			apic->lapic_timer.period = min_period;
+ 		}
+ 	}
+ 
+ 	apic_debug("%s: bus cycle is %" PRId64 "ns, now 0x%016"
+ 		   PRIx64 ", "
+ 		   "timer initial count 0x%x, period %lldns, "
+ 		   "expire @ 0x%016" PRIx64 ".\n", __func__,
+ 		   APIC_BUS_CYCLE_NS, ktime_to_ns(now),
+ 		   kvm_lapic_get_reg(apic, APIC_TMICT),
+ 		   apic->lapic_timer.period,
+ 		   ktime_to_ns(ktime_add_ns(now,
+ 				apic->lapic_timer.period)));
+ 
+ 	apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ 		nsec_to_cycles(apic->vcpu, apic->lapic_timer.period);
+ 	apic->lapic_timer.target_expiration = ktime_add_ns(now, apic->lapic_timer.period);
+ 
+ 	return true;
+ }
+ 
+ static void advance_periodic_target_expiration(struct kvm_lapic *apic)
+ {
+ 	apic->lapic_timer.tscdeadline +=
+ 		nsec_to_cycles(apic->vcpu, apic->lapic_timer.period);
+ 	apic->lapic_timer.target_expiration =
+ 		ktime_add_ns(apic->lapic_timer.target_expiration,
+ 				apic->lapic_timer.period);
+ }
+ 
+ bool kvm_lapic_hv_timer_in_use(struct kvm_vcpu *vcpu)
+ {
+ 	if (!lapic_in_kernel(vcpu))
+ 		return false;
+ 
+ 	return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_hv_timer_in_use);
+ 
+ static void cancel_hv_timer(struct kvm_lapic *apic)
+ {
+ 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+ 	preempt_disable();
+ 	kvm_x86_ops->cancel_hv_timer(apic->vcpu);
+ 	apic->lapic_timer.hv_timer_in_use = false;
+ 	preempt_enable();
+ }
+ 
+ static bool start_hv_timer(struct kvm_lapic *apic)
+ {
+ 	struct kvm_timer *ktimer = &apic->lapic_timer;
+ 	int r;
+ 
+ 	if (!kvm_x86_ops->set_hv_timer)
+ 		return false;
+ 
+ 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
+ 		return false;
+ 
+ 	r = kvm_x86_ops->set_hv_timer(apic->vcpu, ktimer->tscdeadline);
+ 	if (r < 0)
+ 		return false;
+ 
+ 	ktimer->hv_timer_in_use = true;
+ 	hrtimer_cancel(&ktimer->timer);
+ 
+ 	/*
+ 	 * Also recheck ktimer->pending, in case the sw timer triggered in
+ 	 * the window.  For periodic timer, leave the hv timer running for
+ 	 * simplicity, and the deadline will be recomputed on the next vmexit.
+ 	 */
+ 	if (!apic_lvtt_period(apic) && (r || atomic_read(&ktimer->pending))) {
+ 		if (r)
+ 			apic_timer_expired(apic);
+ 		return false;
+ 	}
+ 
+ 	trace_kvm_hv_timer_state(apic->vcpu->vcpu_id, true);
+ 	return true;
+ }
+ 
+ static void start_sw_timer(struct kvm_lapic *apic)
+ {
+ 	struct kvm_timer *ktimer = &apic->lapic_timer;
+ 	if (apic->lapic_timer.hv_timer_in_use)
+ 		cancel_hv_timer(apic);
+ 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
+ 		return;
+ 
+ 	if (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))
+ 		start_sw_period(apic);
+ 	else if (apic_lvtt_tscdeadline(apic))
+ 		start_sw_tscdeadline(apic);
+ 	trace_kvm_hv_timer_state(apic->vcpu->vcpu_id, false);
+ }
+ 
+ static void restart_apic_timer(struct kvm_lapic *apic)
+ {
+ 	if (!start_hv_timer(apic))
+ 		start_sw_timer(apic);
+ }
+ 
+ void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+ 	WARN_ON(swait_active(&vcpu->wq));
+ 	cancel_hv_timer(apic);
+ 	apic_timer_expired(apic);
+ 
+ 	if (apic_lvtt_period(apic) && apic->lapic_timer.period) {
+ 		advance_periodic_target_expiration(apic);
+ 		restart_apic_timer(apic);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);
+ 
+ void kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	restart_apic_timer(vcpu->arch.apic);
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_hv_timer);
+ 
+ void kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	/* Possibly the TSC deadline timer is not enabled yet */
+ 	if (apic->lapic_timer.hv_timer_in_use)
+ 		start_sw_timer(apic);
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_sw_timer);
+ 
+ void kvm_lapic_restart_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+ 	restart_apic_timer(apic);
+ }
+ 
++>>>>>>> c853354429f7 (KVM: LAPIC: Fix lapic timer injection delay)
  static void start_apic_timer(struct kvm_lapic *apic)
  {
 +	ktime_t now;
  	atomic_set(&apic->lapic_timer.pending, 0);
  
 -	if ((apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))
 -	    && !set_target_expiration(apic))
 -		return;
 +	if (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic)) {
 +		/* lapic timer in oneshot or periodic mode */
 +		now = apic->lapic_timer.timer.base->get_time();
 +		apic->lapic_timer.period = (u64)kvm_lapic_get_reg(apic, APIC_TMICT)
 +			    * APIC_BUS_CYCLE_NS * apic->divide_count;
  
 -	restart_apic_timer(apic);
 +		if (!apic->lapic_timer.period)
 +			return;
 +		/*
 +		 * Do not allow the guest to program periodic timers with small
 +		 * interval, since the hrtimers are not throttled by the host
 +		 * scheduler.
 +		 */
 +		if (apic_lvtt_period(apic)) {
 +			s64 min_period = min_timer_period_us * 1000LL;
 +
 +			if (apic->lapic_timer.period < min_period) {
 +				pr_info_ratelimited(
 +				    "kvm: vcpu %i: requested %lld ns "
 +				    "lapic timer period limited to %lld ns\n",
 +				    apic->vcpu->vcpu_id,
 +				    apic->lapic_timer.period, min_period);
 +				apic->lapic_timer.period = min_period;
 +			}
 +		}
 +
 +		hrtimer_start(&apic->lapic_timer.timer,
 +			      ktime_add_ns(now, apic->lapic_timer.period),
 +			      HRTIMER_MODE_ABS_PINNED);
 +
 +		apic_debug("%s: bus cycle is %" PRId64 "ns, now 0x%016"
 +			   PRIx64 ", "
 +			   "timer initial count 0x%x, period %lldns, "
 +			   "expire @ 0x%016" PRIx64 ".\n", __func__,
 +			   APIC_BUS_CYCLE_NS, ktime_to_ns(now),
 +			   kvm_lapic_get_reg(apic, APIC_TMICT),
 +			   apic->lapic_timer.period,
 +			   ktime_to_ns(ktime_add_ns(now,
 +					apic->lapic_timer.period)));
 +	} else if (apic_lvtt_tscdeadline(apic)) {
 +		start_sw_tscdeadline(apic);
 +	}
  }
  
  static void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)
diff --cc arch/x86/kvm/vmx.c
index 04ec8bdd2903,92ddea08f999..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -10617,6 -11101,65 +10617,68 @@@ static int vmx_check_intercept(struct k
  	return X86EMUL_CONTINUE;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_64
+ /* (a << shift) / divisor, return 1 if overflow otherwise 0 */
+ static inline int u64_shl_div_u64(u64 a, unsigned int shift,
+ 				  u64 divisor, u64 *result)
+ {
+ 	u64 low = a << shift, high = a >> (64 - shift);
+ 
+ 	/* To avoid the overflow on divq */
+ 	if (high >= divisor)
+ 		return 1;
+ 
+ 	/* Low hold the result, high hold rem which is discarded */
+ 	asm("divq %2\n\t" : "=a" (low), "=d" (high) :
+ 	    "rm" (divisor), "0" (low), "1" (high));
+ 	*result = low;
+ 
+ 	return 0;
+ }
+ 
+ static int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 	u64 tscl = rdtsc();
+ 	u64 guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ 	u64 delta_tsc = max(guest_deadline_tsc, guest_tscl) - guest_tscl;
+ 
+ 	/* Convert to host delta tsc if tsc scaling is enabled */
+ 	if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+ 			u64_shl_div_u64(delta_tsc,
+ 				kvm_tsc_scaling_ratio_frac_bits,
+ 				vcpu->arch.tsc_scaling_ratio,
+ 				&delta_tsc))
+ 		return -ERANGE;
+ 
+ 	/*
+ 	 * If the delta tsc can't fit in the 32 bit after the multi shift,
+ 	 * we can't use the preemption timer.
+ 	 * It's possible that it fits on later vmentries, but checking
+ 	 * on every vmentry is costly so we just use an hrtimer.
+ 	 */
+ 	if (delta_tsc >> (cpu_preemption_timer_multi + 32))
+ 		return -ERANGE;
+ 
+ 	vmx->hv_deadline_tsc = tscl + delta_tsc;
+ 	vmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,
+ 			PIN_BASED_VMX_PREEMPTION_TIMER);
+ 
+ 	return delta_tsc == 0;
+ }
+ 
+ static void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 	vmx->hv_deadline_tsc = -1;
+ 	vmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,
+ 			PIN_BASED_VMX_PREEMPTION_TIMER);
+ }
+ #endif
+ 
++>>>>>>> c853354429f7 (KVM: LAPIC: Fix lapic timer injection delay)
  static void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)
  {
  	if (ple_gap)
* Unmerged path arch/x86/kvm/lapic.c
* Unmerged path arch/x86/kvm/vmx.c
