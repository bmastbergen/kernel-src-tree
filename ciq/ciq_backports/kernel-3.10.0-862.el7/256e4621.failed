iommu/amd: Make use of the generic IOVA allocator

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] amd: Make use of the generic IOVA allocator (Jerry Snitselaar) [1411581]
Rebuild_FUZZ: 93.48%
commit-author Joerg Roedel <jroedel@suse.de>
commit 256e4621c21aa1bf704e1a12e643923fdb732d04
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/256e4621.failed

Remove the old address allocation code and make use of the
generic IOVA allocator that is also used by other dma-ops
implementations.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 256e4621c21aa1bf704e1a12e643923fdb732d04)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index c9d79e07ea12,77be2d0558bd..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -1598,143 -1649,32 +1598,148 @@@ out_free
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
 +static dma_addr_t dma_ops_aperture_alloc(struct dma_ops_domain *dom,
 +					 struct aperture_range *range,
 +					 unsigned long pages,
 +					 unsigned long dma_mask,
 +					 unsigned long boundary_size,
 +					 unsigned long align_mask)
++=======
+ static unsigned long dma_ops_alloc_iova(struct device *dev,
+ 					struct dma_ops_domain *dma_dom,
+ 					unsigned int pages, u64 dma_mask)
++>>>>>>> 256e4621c21a (iommu/amd: Make use of the generic IOVA allocator)
  {
- 	unsigned long offset, limit, flags;
- 	dma_addr_t address;
- 	bool flush = false;
+ 	unsigned long pfn = 0;
  
- 	offset = range->offset >> PAGE_SHIFT;
- 	limit  = iommu_device_max_index(APERTURE_RANGE_PAGES, offset,
- 					dma_mask >> PAGE_SHIFT);
+ 	pages = __roundup_pow_of_two(pages);
  
++<<<<<<< HEAD
 +	spin_lock_irqsave(&range->bitmap_lock, flags);
 +	address = iommu_area_alloc(range->bitmap, limit, range->next_bit,
 +				   pages, offset, boundary_size, align_mask);
 +	if (address == -1) {
 +		/* Nothing found, retry one time */
 +		address = iommu_area_alloc(range->bitmap, limit,
 +					   0, pages, offset, boundary_size,
 +					   align_mask);
 +		flush = true;
 +	}
++=======
+ 	if (dma_mask > DMA_BIT_MASK(32))
+ 		pfn = alloc_iova_fast(&dma_dom->iovad, pages,
+ 				      IOVA_PFN(DMA_BIT_MASK(32)));
  
- 	if (address != -1)
- 		range->next_bit = address + pages;
- 
- 	spin_unlock_irqrestore(&range->bitmap_lock, flags);
+ 	if (!pfn)
+ 		pfn = alloc_iova_fast(&dma_dom->iovad, pages, IOVA_PFN(dma_mask));
++>>>>>>> 256e4621c21a (iommu/amd: Make use of the generic IOVA allocator)
  
- 	if (flush) {
- 		domain_flush_tlb(&dom->domain);
- 		domain_flush_complete(&dom->domain);
- 	}
- 
- 	return address;
+ 	return (pfn << PAGE_SHIFT);
  }
  
- static unsigned long dma_ops_area_alloc(struct device *dev,
- 					struct dma_ops_domain *dom,
- 					unsigned int pages,
- 					unsigned long align_mask,
- 					u64 dma_mask)
+ static void dma_ops_free_iova(struct dma_ops_domain *dma_dom,
+ 			      unsigned long address,
+ 			      unsigned int pages)
  {
++<<<<<<< HEAD
 +	unsigned long boundary_size, mask;
 +	unsigned long address = -1;
 +	int start = dom->next_index;
 +	int i;
 +
 +	mask = dma_get_seg_boundary(dev);
 +
 +	boundary_size = mask + 1 ? ALIGN(mask + 1, PAGE_SIZE) >> PAGE_SHIFT :
 +				   1UL << (BITS_PER_LONG - PAGE_SHIFT);
 +
 +	for (i = 0; i < APERTURE_MAX_RANGES; ++i) {
 +		struct aperture_range *range;
 +
 +		range = dom->aperture[(start + i) % APERTURE_MAX_RANGES];
 +
 +		if (!range || range->offset >= dma_mask)
 +			continue;
 +
 +		address = dma_ops_aperture_alloc(dom, range, pages,
 +						 dma_mask, boundary_size,
 +						 align_mask);
 +		if (address != -1) {
 +			address = range->offset + (address << PAGE_SHIFT);
 +			dom->next_index = i;
 +			break;
 +		}
 +	}
 +
 +	return address;
 +}
 +
 +static unsigned long dma_ops_alloc_addresses(struct device *dev,
 +					     struct dma_ops_domain *dom,
 +					     unsigned int pages,
 +					     unsigned long align_mask,
 +					     u64 dma_mask)
 +{
 +	unsigned long address = -1;
 +
 +#ifdef CONFIG_IOMMU_STRESS
 +	dom->next_index = 0;
 +#endif
 +
 +	while (address == -1) {
 +		address = dma_ops_area_alloc(dev, dom, pages,
 +					     align_mask, dma_mask);
 +
 +		if (address == -1 && alloc_new_range(dom, false, GFP_ATOMIC))
 +			break;
 +	}
 +
 +	if (unlikely(address == -1))
 +		address = DMA_ERROR_CODE;
 +
 +	WARN_ON((address + (PAGE_SIZE*pages)) > dom->aperture_size);
 +
 +	return address;
 +}
 +
 +/*
 + * The address free function.
 + *
 + * called with domain->lock held
 + */
 +static void dma_ops_free_addresses(struct dma_ops_domain *dom,
 +				   unsigned long address,
 +				   unsigned int pages)
 +{
 +	unsigned i = address >> APERTURE_RANGE_SHIFT;
 +	struct aperture_range *range = dom->aperture[i];
 +	unsigned long flags;
 +
 +	BUG_ON(i >= APERTURE_MAX_RANGES || range == NULL);
 +
 +#ifdef CONFIG_IOMMU_STRESS
 +	if (i < 4)
 +		return;
 +#endif
 +
 +	if (amd_iommu_unmap_flush) {
 +		domain_flush_tlb(&dom->domain);
 +		domain_flush_complete(&dom->domain);
 +	}
 +
 +	address = (address % APERTURE_RANGE_SIZE) >> PAGE_SHIFT;
 +
 +	spin_lock_irqsave(&range->bitmap_lock, flags);
 +	if (address + pages > range->next_bit)
 +		range->next_bit = address + pages;
 +	bitmap_clear(range->bitmap, address, pages);
 +	spin_unlock_irqrestore(&range->bitmap_lock, flags);
++=======
+ 	pages = __roundup_pow_of_two(pages);
+ 	address >>= PAGE_SHIFT;
++>>>>>>> 256e4621c21a (iommu/amd: Make use of the generic IOVA allocator)
  
+ 	free_iova_fast(&dma_dom->iovad, address, pages);
  }
  
  /****************************************************************************
* Unmerged path drivers/iommu/amd_iommu.c
