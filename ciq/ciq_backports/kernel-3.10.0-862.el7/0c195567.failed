netvsc: transparent VF management

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author stephen hemminger <stephen@networkplumber.org>
commit 0c195567a8f6e82ea5535cd9f1d54a1626dd233e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/0c195567.failed

This patch implements transparent fail over from synthetic NIC to
SR-IOV virtual function NIC in Hyper-V environment. It is a better
alternative to using bonding as is done now. Instead, the receive and
transmit fail over is done internally inside the driver.

Using bonding driver has lots of issues because it depends on the
script being run early enough in the boot process and with sufficient
information to make the association. This patch moves all that
functionality into the kernel.

	Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 0c195567a8f6e82ea5535cd9f1d54a1626dd233e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/hyperv/netvsc_drv.c
diff --cc drivers/net/hyperv/netvsc_drv.c
index d4dc9c189b85,c71728d82049..000000000000
--- a/drivers/net/hyperv/netvsc_drv.c
+++ b/drivers/net/hyperv/netvsc_drv.c
@@@ -34,6 -33,9 +34,12 @@@
  #include <linux/if_vlan.h>
  #include <linux/in.h>
  #include <linux/slab.h>
++<<<<<<< HEAD
++=======
+ #include <linux/rtnetlink.h>
+ #include <linux/netpoll.h>
+ 
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  #include <net/arp.h>
  #include <net/route.h>
  #include <net/sock.h>
@@@ -105,7 -72,8 +111,12 @@@ static void netvsc_set_multicast_list(s
  static int netvsc_open(struct net_device *net)
  {
  	struct net_device_context *ndev_ctx = netdev_priv(net);
++<<<<<<< HEAD
 +	struct netvsc_device *nvdev = ndev_ctx->nvdev;
++=======
+ 	struct net_device *vf_netdev = rtnl_dereference(ndev_ctx->vf_netdev);
+ 	struct netvsc_device *nvdev = rtnl_dereference(ndev_ctx->nvdev);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	struct rndis_device *rdev;
  	int ret = 0;
  
@@@ -130,7 -110,9 +153,13 @@@
  static int netvsc_close(struct net_device *net)
  {
  	struct net_device_context *net_device_ctx = netdev_priv(net);
++<<<<<<< HEAD
 +	struct netvsc_device *nvdev = net_device_ctx->nvdev;
++=======
+ 	struct net_device *vf_netdev
+ 		= rtnl_dereference(net_device_ctx->vf_netdev);
+ 	struct netvsc_device *nvdev = rtnl_dereference(net_device_ctx->nvdev);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	int ret;
  	u32 aread, i, msec = 10, retry = 0, retry_max = 20;
  	struct vmbus_channel *chn;
@@@ -212,32 -243,20 +244,49 @@@ static void *init_ppi_data(struct rndis
   *
   * TODO support XPS - but get_xps_queue not exported
   */
++<<<<<<< HEAD
 +static u16 netvsc_select_queue(struct net_device *ndev, struct sk_buff *skb,
 +		       void *accel_priv, select_queue_fallback_t fallback)
 +{
 +	struct net_device_context *net_device_ctx = netdev_priv(ndev);
 +	struct netvsc_device *nvsc_dev = net_device_ctx->nvdev;
 +	struct sock *sk = skb->sk;
 +	int q_idx = sk_tx_queue_get(sk);
 +
 +	if (q_idx < 0 || skb->ooo_okay ||
 +	    q_idx >= ndev->real_num_tx_queues) {
 +		u16 hash = __skb_tx_hash(ndev, skb, VRSS_SEND_TAB_SIZE);
 +		int new_idx;
 +
 +		new_idx = nvsc_dev->send_table[hash]
 +			% nvsc_dev->num_chn;
 +
 +		if (q_idx != new_idx && sk &&
 +		    sk_fullsock(sk) && rcu_access_pointer(sk->sk_dst_cache))
 +			sk_tx_queue_set(sk, new_idx);
 +
 +		q_idx = new_idx;
 +	}
 +
 +	if (unlikely(!nvsc_dev->chn_table[q_idx]))
 +		q_idx = 0;
 +
++=======
+ static u16 netvsc_pick_tx(struct net_device *ndev, struct sk_buff *skb)
+ {
+ 	int q_idx = sk_tx_queue_get(skb->sk);
+ 
+ 	if (q_idx < 0 || skb->ooo_okay || q_idx >= ndev->real_num_tx_queues) {
+ 		/* If forwarding a packet, we use the recorded queue when
+ 		 * available for better cache locality.
+ 		 */
+ 		if (skb_rx_queue_recorded(skb))
+ 			q_idx = skb_get_rx_queue(skb);
+ 		else
+ 			q_idx = netvsc_get_tx_queue(ndev, skb, q_idx);
+ 	}
+ 
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	return q_idx;
  }
  
@@@ -342,35 -384,54 +415,62 @@@ static int netvsc_get_slots(struct sk_b
  	return slots + frag_slots;
  }
  
 -static u32 net_checksum_info(struct sk_buff *skb)
 +static u32 get_net_transport_info(struct sk_buff *skb, u32 *trans_off)
  {
 -	if (skb->protocol == htons(ETH_P_IP)) {
 -		struct iphdr *ip = ip_hdr(skb);
 +	u32 ret_val = TRANSPORT_INFO_NOT_IP;
  
 -		if (ip->protocol == IPPROTO_TCP)
 -			return TRANSPORT_INFO_IPV4_TCP;
 -		else if (ip->protocol == IPPROTO_UDP)
 -			return TRANSPORT_INFO_IPV4_UDP;
 -	} else {
 -		struct ipv6hdr *ip6 = ipv6_hdr(skb);
 +	if ((eth_hdr(skb)->h_proto != htons(ETH_P_IP)) &&
 +		(eth_hdr(skb)->h_proto != htons(ETH_P_IPV6))) {
 +		goto not_ip;
 +	}
  
 -		if (ip6->nexthdr == IPPROTO_TCP)
 -			return TRANSPORT_INFO_IPV6_TCP;
 -		else if (ip6->nexthdr == IPPROTO_UDP)
 -			return TRANSPORT_INFO_IPV6_UDP;
 +	*trans_off = skb_transport_offset(skb);
 +
 +	if ((eth_hdr(skb)->h_proto == htons(ETH_P_IP))) {
 +		struct iphdr *iphdr = ip_hdr(skb);
 +
 +		if (iphdr->protocol == IPPROTO_TCP)
 +			ret_val = TRANSPORT_INFO_IPV4_TCP;
 +		else if (iphdr->protocol == IPPROTO_UDP)
 +			ret_val = TRANSPORT_INFO_IPV4_UDP;
 +	} else {
 +		if (ipv6_hdr(skb)->nexthdr == IPPROTO_TCP)
 +			ret_val = TRANSPORT_INFO_IPV6_TCP;
 +		else if (ipv6_hdr(skb)->nexthdr == IPPROTO_UDP)
 +			ret_val = TRANSPORT_INFO_IPV6_UDP;
  	}
  
 -	return TRANSPORT_INFO_NOT_IP;
 +not_ip:
 +	return ret_val;
  }
  
+ /* Send skb on the slave VF device. */
+ static int netvsc_vf_xmit(struct net_device *net, struct net_device *vf_netdev,
+ 			  struct sk_buff *skb)
+ {
+ 	struct net_device_context *ndev_ctx = netdev_priv(net);
+ 	unsigned int len = skb->len;
+ 	int rc;
+ 
+ 	skb->dev = vf_netdev;
+ 	skb->queue_mapping = qdisc_skb_cb(skb)->slave_dev_queue_mapping;
+ 
+ 	rc = dev_queue_xmit(skb);
+ 	if (likely(rc == NET_XMIT_SUCCESS || rc == NET_XMIT_CN)) {
+ 		struct netvsc_vf_pcpu_stats *pcpu_stats
+ 			= this_cpu_ptr(ndev_ctx->vf_stats);
+ 
+ 		u64_stats_update_begin(&pcpu_stats->syncp);
+ 		pcpu_stats->tx_packets++;
+ 		pcpu_stats->tx_bytes += len;
+ 		u64_stats_update_end(&pcpu_stats->syncp);
+ 	} else {
+ 		this_cpu_inc(ndev_ctx->vf_stats->tx_dropped);
+ 	}
+ 
+ 	return rc;
+ }
+ 
  static int netvsc_start_xmit(struct sk_buff *skb, struct net_device *net)
  {
  	struct net_device_context *net_device_ctx = netdev_priv(net);
@@@ -379,15 -440,20 +479,24 @@@
  	unsigned int num_data_pgs;
  	struct rndis_message *rndis_msg;
  	struct rndis_packet *rndis_pkt;
+ 	struct net_device *vf_netdev;
  	u32 rndis_msg_size;
  	struct rndis_per_packet_info *ppi;
 +	struct ndis_tcp_ip_checksum_info *csum_info;
 +	int  hdr_offset = 0; /* silence GCC4.8 complains in RHEL7 */
 +	u32 net_trans_info;
  	u32 hash;
 -	struct hv_page_buffer pb[MAX_PAGE_BUFFER_COUNT];
 +	struct hv_page_buffer page_buf[MAX_PAGE_BUFFER_COUNT];
 +	struct hv_page_buffer *pb = page_buf;
  
+ 	/* if VF is present and up then redirect packets
+ 	 * already called with rcu_read_lock_bh
+ 	 */
+ 	vf_netdev = rcu_dereference_bh(net_device_ctx->vf_netdev);
+ 	if (vf_netdev && netif_running(vf_netdev) &&
+ 	    !netpoll_tx_running(net))
+ 		return netvsc_vf_xmit(net, vf_netdev, skb);
+ 
  	/* We will atmost need two pages to describe the rndis
  	 * header. We can only transmit MAX_PAGE_BUFFER_COUNT number
  	 * of pages in a single packet. If skb is scattered around
@@@ -658,24 -729,21 +767,31 @@@ int netvsc_recv_callback(struct net_dev
  			 const struct ndis_pkt_8021q_info *vlan)
  {
  	struct net_device_context *net_device_ctx = netdev_priv(net);
++<<<<<<< HEAD
 +	struct net_device *vf_netdev;
++=======
+ 	struct netvsc_device *net_device;
+ 	u16 q_idx = channel->offermsg.offer.sub_channel_index;
+ 	struct netvsc_channel *nvchan;
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	struct sk_buff *skb;
  	struct netvsc_stats *rx_stats;
  
  	if (net->reg_state != NETREG_REGISTERED)
  		return NVSP_STAT_FAIL;
  
- 	/*
- 	 * If necessary, inject this packet into the VF interface.
- 	 * On Hyper-V, multicast and brodcast packets are only delivered
- 	 * to the synthetic interface (after subjecting these to
- 	 * policy filters on the host). Deliver these via the VF
- 	 * interface in the guest.
- 	 */
  	rcu_read_lock();
++<<<<<<< HEAD
 +	vf_netdev = rcu_dereference(net_device_ctx->vf_netdev);
 +	if (vf_netdev && (vf_netdev->flags & IFF_UP))
 +		net = vf_netdev;
++=======
+ 	net_device = rcu_dereference(net_device_ctx->nvdev);
+ 	if (unlikely(!net_device))
+ 		goto drop;
+ 
+ 	nvchan = &net_device->chan_table[q_idx];
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  
  	/* Allocate a skb - TODO direct I/O to pages? */
  	skb = netvsc_alloc_recv_skb(net, &nvchan->napi,
@@@ -686,9 -755,7 +802,13 @@@
  		return NVSP_STAT_FAIL;
  	}
  
++<<<<<<< HEAD
 +	if (net != vf_netdev)
 +		skb_record_rx_queue(skb,
 +				    channel->offermsg.offer.sub_channel_index);
++=======
+ 	skb_record_rx_queue(skb, q_idx);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  
  	/*
  	 * Even if injecting the packet, record the statistics
@@@ -893,42 -915,53 +1013,76 @@@ static int netvsc_set_link_ksettings(st
  static int netvsc_change_mtu(struct net_device *ndev, int mtu)
  {
  	struct net_device_context *ndevctx = netdev_priv(ndev);
++<<<<<<< HEAD
 +	struct netvsc_device *nvdev = ndevctx->nvdev;
++=======
+ 	struct net_device *vf_netdev = rtnl_dereference(ndevctx->vf_netdev);
+ 	struct netvsc_device *nvdev = rtnl_dereference(ndevctx->nvdev);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	struct hv_device *hdev = ndevctx->device_ctx;
 -	int orig_mtu = ndev->mtu;
  	struct netvsc_device_info device_info;
 -	bool was_opened;
 +	int limit = ETH_DATA_LEN;
 +	u32 num_chn;
  	int ret = 0;
  
 -	if (!nvdev || nvdev->destroy)
 +	if (ndevctx->start_remove || !nvdev || nvdev->destroy)
  		return -ENODEV;
  
++<<<<<<< HEAD
 +	if (nvdev->nvsp_version >= NVSP_PROTOCOL_VERSION_2)
 +		limit = NETVSC_MTU - ETH_HLEN;
++=======
+ 	/* Change MTU of underlying VF netdev first. */
+ 	if (vf_netdev) {
+ 		ret = dev_set_mtu(vf_netdev, mtu);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	netif_device_detach(ndev);
+ 	was_opened = rndis_filter_opened(nvdev);
+ 	if (was_opened)
+ 		rndis_filter_close(nvdev);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  
 -	memset(&device_info, 0, sizeof(device_info));
 -	device_info.ring_size = ring_size;
 -	device_info.num_chn = nvdev->num_chn;
 +	if (mtu < NETVSC_MTU_MIN || mtu > limit)
 +		return -EINVAL;
 +
 +	ret = netvsc_close(ndev);
 +	if (ret)
 +		goto out;
  
 -	rndis_filter_device_remove(hdev, nvdev);
 +	num_chn = nvdev->num_chn;
  
 -	ndev->mtu = mtu;
 +	ndevctx->start_remove = true;
 +	rndis_filter_device_remove(hdev);
  
 -	nvdev = rndis_filter_device_add(hdev, &device_info);
 -	if (IS_ERR(nvdev)) {
 -		ret = PTR_ERR(nvdev);
 +	ndev->mtu = mtu;
  
 +	memset(&device_info, 0, sizeof(device_info));
 +	device_info.ring_size = ring_size;
 +	device_info.num_chn = num_chn;
 +	device_info.max_num_vrss_chns = max_num_vrss_chns;
 +	rndis_filter_device_add(hdev, &device_info);
 +
++<<<<<<< HEAD
 +out:
 +	netvsc_open(ndev);
 +	ndevctx->start_remove = false;
++=======
+ 		/* Attempt rollback to original MTU */
+ 		ndev->mtu = orig_mtu;
+ 		rndis_filter_device_add(hdev, &device_info);
+ 
+ 		if (vf_netdev)
+ 			dev_set_mtu(vf_netdev, orig_mtu);
+ 	}
+ 
+ 	if (was_opened)
+ 		rndis_filter_open(nvdev);
+ 
+ 	netif_device_attach(ndev);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  
  	/* We may have missed link change notifications */
  	schedule_delayed_work(&ndevctx->dwork, 0);
@@@ -936,47 -969,84 +1090,107 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
 +static struct rtnl_link_stats64 *netvsc_get_stats64(struct net_device *net,
 +						    struct rtnl_link_stats64 *t)
 +{
 +	struct net_device_context *ndev_ctx = netdev_priv(net);
 +	int cpu;
 +
 +	for_each_possible_cpu(cpu) {
 +		struct netvsc_stats *tx_stats = per_cpu_ptr(ndev_ctx->tx_stats,
 +							    cpu);
 +		struct netvsc_stats *rx_stats = per_cpu_ptr(ndev_ctx->rx_stats,
 +							    cpu);
 +		u64 tx_packets, tx_bytes, rx_packets, rx_bytes, rx_multicast;
++=======
+ static void netvsc_get_vf_stats(struct net_device *net,
+ 				struct netvsc_vf_pcpu_stats *tot)
+ {
+ 	struct net_device_context *ndev_ctx = netdev_priv(net);
+ 	int i;
+ 
+ 	memset(tot, 0, sizeof(*tot));
+ 
+ 	for_each_possible_cpu(i) {
+ 		const struct netvsc_vf_pcpu_stats *stats
+ 			= per_cpu_ptr(ndev_ctx->vf_stats, i);
+ 		u64 rx_packets, rx_bytes, tx_packets, tx_bytes;
+ 		unsigned int start;
+ 
+ 		do {
+ 			start = u64_stats_fetch_begin_irq(&stats->syncp);
+ 			rx_packets = stats->rx_packets;
+ 			tx_packets = stats->tx_packets;
+ 			rx_bytes = stats->rx_bytes;
+ 			tx_bytes = stats->tx_bytes;
+ 		} while (u64_stats_fetch_retry_irq(&stats->syncp, start));
+ 
+ 		tot->rx_packets += rx_packets;
+ 		tot->tx_packets += tx_packets;
+ 		tot->rx_bytes   += rx_bytes;
+ 		tot->tx_bytes   += tx_bytes;
+ 		tot->tx_dropped += stats->tx_dropped;
+ 	}
+ }
+ 
+ static void netvsc_get_stats64(struct net_device *net,
+ 			       struct rtnl_link_stats64 *t)
+ {
+ 	struct net_device_context *ndev_ctx = netdev_priv(net);
+ 	struct netvsc_device *nvdev = rcu_dereference_rtnl(ndev_ctx->nvdev);
+ 	struct netvsc_vf_pcpu_stats vf_tot;
+ 		int i;
+ 
+ 	if (!nvdev)
+ 		return;
+ 
+ 	netdev_stats_to_stats64(t, &net->stats);
+ 
+ 	netvsc_get_vf_stats(net, &vf_tot);
+ 	t->rx_packets += vf_tot.rx_packets;
+ 	t->tx_packets += vf_tot.tx_packets;
+ 	t->rx_bytes   += vf_tot.rx_bytes;
+ 	t->tx_bytes   += vf_tot.tx_bytes;
+ 	t->tx_dropped += vf_tot.tx_dropped;
+ 
+ 	for (i = 0; i < nvdev->num_chn; i++) {
+ 		const struct netvsc_channel *nvchan = &nvdev->chan_table[i];
+ 		const struct netvsc_stats *stats;
+ 		u64 packets, bytes, multicast;
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  		unsigned int start;
  
 -		stats = &nvchan->tx_stats;
  		do {
 -			start = u64_stats_fetch_begin_irq(&stats->syncp);
 -			packets = stats->packets;
 -			bytes = stats->bytes;
 -		} while (u64_stats_fetch_retry_irq(&stats->syncp, start));
 -
 -		t->tx_bytes	+= bytes;
 -		t->tx_packets	+= packets;
 +			start = u64_stats_fetch_begin_irq(&tx_stats->syncp);
 +			tx_packets = tx_stats->packets;
 +			tx_bytes = tx_stats->bytes;
 +		} while (u64_stats_fetch_retry_irq(&tx_stats->syncp, start));
  
 -		stats = &nvchan->rx_stats;
  		do {
 -			start = u64_stats_fetch_begin_irq(&stats->syncp);
 -			packets = stats->packets;
 -			bytes = stats->bytes;
 -			multicast = stats->multicast + stats->broadcast;
 -		} while (u64_stats_fetch_retry_irq(&stats->syncp, start));
 -
 -		t->rx_bytes	+= bytes;
 -		t->rx_packets	+= packets;
 -		t->multicast	+= multicast;
 +			start = u64_stats_fetch_begin_irq(&rx_stats->syncp);
 +			rx_packets = rx_stats->packets;
 +			rx_bytes = rx_stats->bytes;
 +			rx_multicast = rx_stats->multicast + rx_stats->broadcast;
 +		} while (u64_stats_fetch_retry_irq(&rx_stats->syncp, start));
 +
 +		t->tx_bytes	+= tx_bytes;
 +		t->tx_packets	+= tx_packets;
 +		t->rx_bytes	+= rx_bytes;
 +		t->rx_packets	+= rx_packets;
 +		t->multicast	+= rx_multicast;
  	}
++<<<<<<< HEAD
 +
 +	t->tx_dropped	= net->stats.tx_dropped;
 +	t->tx_errors	= net->stats.tx_errors;
 +
 +	t->rx_dropped	= net->stats.rx_dropped;
 +	t->rx_errors	= net->stats.rx_errors;
 +
 +	return t;
++=======
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  }
  
  static int netvsc_set_mac_addr(struct net_device *ndev, void *p)
@@@ -1012,13 -1087,33 +1226,34 @@@ static const struct 
  	{ "tx_no_space",  offsetof(struct netvsc_ethtool_stats, tx_no_space) },
  	{ "tx_too_big",	  offsetof(struct netvsc_ethtool_stats, tx_too_big) },
  	{ "tx_busy",	  offsetof(struct netvsc_ethtool_stats, tx_busy) },
+ }, vf_stats[] = {
+ 	{ "vf_rx_packets", offsetof(struct netvsc_vf_pcpu_stats, rx_packets) },
+ 	{ "vf_rx_bytes",   offsetof(struct netvsc_vf_pcpu_stats, rx_bytes) },
+ 	{ "vf_tx_packets", offsetof(struct netvsc_vf_pcpu_stats, tx_packets) },
+ 	{ "vf_tx_bytes",   offsetof(struct netvsc_vf_pcpu_stats, tx_bytes) },
+ 	{ "vf_tx_dropped", offsetof(struct netvsc_vf_pcpu_stats, tx_dropped) },
  };
  
++<<<<<<< HEAD
++=======
+ #define NETVSC_GLOBAL_STATS_LEN	ARRAY_SIZE(netvsc_stats)
+ #define NETVSC_VF_STATS_LEN	ARRAY_SIZE(vf_stats)
+ 
+ /* 4 statistics per queue (rx/tx packets/bytes) */
+ #define NETVSC_QUEUE_STATS_LEN(dev) ((dev)->num_chn * 4)
+ 
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  static int netvsc_get_sset_count(struct net_device *dev, int string_set)
  {
 -	struct net_device_context *ndc = netdev_priv(dev);
 -	struct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);
 -
 -	if (!nvdev)
 -		return -ENODEV;
 -
  	switch (string_set) {
  	case ETH_SS_STATS:
++<<<<<<< HEAD
 +		return ARRAY_SIZE(netvsc_stats);
++=======
+ 		return NETVSC_GLOBAL_STATS_LEN
+ 			+ NETVSC_VF_STATS_LEN
+ 			+ NETVSC_QUEUE_STATS_LEN(nvdev);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	default:
  		return -EINVAL;
  	}
@@@ -1028,22 -1123,79 +1263,82 @@@ static void netvsc_get_ethtool_stats(st
  				     struct ethtool_stats *stats, u64 *data)
  {
  	struct net_device_context *ndc = netdev_priv(dev);
 -	struct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);
  	const void *nds = &ndc->eth_stats;
++<<<<<<< HEAD
 +	int i;
++=======
+ 	const struct netvsc_stats *qstats;
+ 	struct netvsc_vf_pcpu_stats sum;
+ 	unsigned int start;
+ 	u64 packets, bytes;
+ 	int i, j;
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  
 -	if (!nvdev)
 -		return;
 -
 -	for (i = 0; i < NETVSC_GLOBAL_STATS_LEN; i++)
 +	for (i = 0; i < ARRAY_SIZE(netvsc_stats); i++)
  		data[i] = *(unsigned long *)(nds + netvsc_stats[i].offset);
++<<<<<<< HEAD
++=======
+ 
+ 	netvsc_get_vf_stats(dev, &sum);
+ 	for (j = 0; j < NETVSC_VF_STATS_LEN; j++)
+ 		data[i++] = *(u64 *)((void *)&sum + vf_stats[j].offset);
+ 
+ 	for (j = 0; j < nvdev->num_chn; j++) {
+ 		qstats = &nvdev->chan_table[j].tx_stats;
+ 
+ 		do {
+ 			start = u64_stats_fetch_begin_irq(&qstats->syncp);
+ 			packets = qstats->packets;
+ 			bytes = qstats->bytes;
+ 		} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));
+ 		data[i++] = packets;
+ 		data[i++] = bytes;
+ 
+ 		qstats = &nvdev->chan_table[j].rx_stats;
+ 		do {
+ 			start = u64_stats_fetch_begin_irq(&qstats->syncp);
+ 			packets = qstats->packets;
+ 			bytes = qstats->bytes;
+ 		} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));
+ 		data[i++] = packets;
+ 		data[i++] = bytes;
+ 	}
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  }
  
  static void netvsc_get_strings(struct net_device *dev, u32 stringset, u8 *data)
  {
 -	struct net_device_context *ndc = netdev_priv(dev);
 -	struct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);
 -	u8 *p = data;
  	int i;
  
 -	if (!nvdev)
 -		return;
 -
  	switch (stringset) {
  	case ETH_SS_STATS:
++<<<<<<< HEAD
 +		for (i = 0; i < ARRAY_SIZE(netvsc_stats); i++)
 +			memcpy(data + i * ETH_GSTRING_LEN,
 +			       netvsc_stats[i].name, ETH_GSTRING_LEN);
++=======
+ 		for (i = 0; i < ARRAY_SIZE(netvsc_stats); i++) {
+ 			memcpy(p, netvsc_stats[i].name, ETH_GSTRING_LEN);
+ 			p += ETH_GSTRING_LEN;
+ 		}
+ 
+ 		for (i = 0; i < ARRAY_SIZE(vf_stats); i++) {
+ 			memcpy(p, vf_stats[i].name, ETH_GSTRING_LEN);
+ 			p += ETH_GSTRING_LEN;
+ 		}
+ 
+ 		for (i = 0; i < nvdev->num_chn; i++) {
+ 			sprintf(p, "tx_queue_%u_packets", i);
+ 			p += ETH_GSTRING_LEN;
+ 			sprintf(p, "tx_queue_%u_bytes", i);
+ 			p += ETH_GSTRING_LEN;
+ 			sprintf(p, "rx_queue_%u_packets", i);
+ 			p += ETH_GSTRING_LEN;
+ 			sprintf(p, "rx_queue_%u_bytes", i);
+ 			p += ETH_GSTRING_LEN;
+ 		}
+ 
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  		break;
  	}
  }
@@@ -1392,61 -1645,59 +1786,77 @@@ static int netvsc_register_vf(struct ne
  	return NOTIFY_OK;
  }
  
- static int netvsc_vf_up(struct net_device *vf_netdev)
+ /* Change datapath */
+ static void netvsc_vf_update(struct work_struct *w)
  {
- 	struct net_device *ndev;
+ 	struct net_device_context *ndev_ctx
+ 		= container_of(w, struct net_device_context, vf_notify);
+ 	struct net_device *ndev = hv_get_drvdata(ndev_ctx->device_ctx);
  	struct netvsc_device *netvsc_dev;
- 	struct net_device_context *net_device_ctx;
+ 	struct net_device *vf_netdev;
+ 	bool vf_is_up;
  
- 	ndev = get_netvsc_byref(vf_netdev);
- 	if (!ndev)
- 		return NOTIFY_DONE;
+ 	rtnl_lock();
+ 	vf_netdev = rtnl_dereference(ndev_ctx->vf_netdev);
+ 	if (!vf_netdev)
+ 		goto unlock;
  
++<<<<<<< HEAD
 +	net_device_ctx = netdev_priv(ndev);
 +	netvsc_dev = net_device_ctx->nvdev;
++=======
+ 	netvsc_dev = rtnl_dereference(ndev_ctx->nvdev);
+ 	if (!netvsc_dev)
+ 		goto unlock;
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
+ 
+ 	vf_is_up = netif_running(vf_netdev);
+ 	if (vf_is_up != ndev_ctx->datapath) {
+ 		if (vf_is_up) {
+ 			netdev_info(ndev, "VF up: %s\n", vf_netdev->name);
+ 			rndis_filter_open(netvsc_dev);
+ 			netvsc_switch_datapath(ndev, true);
+ 			netdev_info(ndev, "Data path switched to VF: %s\n",
+ 				    vf_netdev->name);
+ 		} else {
+ 			netdev_info(ndev, "VF down: %s\n", vf_netdev->name);
+ 			netvsc_switch_datapath(ndev, false);
+ 			rndis_filter_close(netvsc_dev);
+ 			netdev_info(ndev, "Data path switched from VF: %s\n",
+ 				    vf_netdev->name);
+ 		}
  
- 	netdev_info(ndev, "VF up: %s\n", vf_netdev->name);
- 
- 	/*
- 	 * Open the device before switching data path.
- 	 */
- 	rndis_filter_open(netvsc_dev);
- 
- 	/*
- 	 * notify the host to switch the data path.
- 	 */
- 	netvsc_switch_datapath(ndev, true);
- 	netdev_info(ndev, "Data path switched to VF: %s\n", vf_netdev->name);
- 
- 	netif_carrier_off(ndev);
- 
- 	/* Now notify peers through VF device. */
- 	call_netdevice_notifiers(NETDEV_NOTIFY_PEERS, vf_netdev);
- 
- 	return NOTIFY_OK;
+ 		/* Now notify peers through VF device. */
+ 		call_netdevice_notifiers(NETDEV_NOTIFY_PEERS, ndev);
+ 	}
+ unlock:
+ 	rtnl_unlock();
  }
  
- static int netvsc_vf_down(struct net_device *vf_netdev)
+ static int netvsc_vf_notify(struct net_device *vf_netdev)
  {
- 	struct net_device *ndev;
- 	struct netvsc_device *netvsc_dev;
  	struct net_device_context *net_device_ctx;
+ 	struct net_device *ndev;
  
  	ndev = get_netvsc_byref(vf_netdev);
  	if (!ndev)
  		return NOTIFY_DONE;
  
  	net_device_ctx = netdev_priv(ndev);
++<<<<<<< HEAD
 +	netvsc_dev = net_device_ctx->nvdev;
 +
 +	netdev_info(ndev, "VF down: %s\n", vf_netdev->name);
 +	netvsc_switch_datapath(ndev, false);
 +	netdev_info(ndev, "Data path switched from VF: %s\n", vf_netdev->name);
 +	rndis_filter_close(netvsc_dev);
 +	netif_carrier_on(ndev);
 +
 +	/* Now notify peers through netvsc device. */
 +	call_netdevice_notifiers(NETDEV_NOTIFY_PEERS, ndev);
++=======
+ 	schedule_work(&net_device_ctx->vf_notify);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  
  	return NOTIFY_OK;
  }
@@@ -1477,12 -1731,12 +1890,12 @@@ static int netvsc_probe(struct hv_devic
  	struct net_device_context *net_device_ctx;
  	struct netvsc_device_info device_info;
  	struct netvsc_device *nvdev;
- 	int ret;
+ 	int ret = -ENOMEM;
  
  	net = alloc_etherdev_mq(sizeof(struct net_device_context),
 -				VRSS_CHANNEL_MAX);
 +				num_online_cpus());
  	if (!net)
- 		return -ENOMEM;
+ 		goto no_net;
  
  	netif_carrier_off(net);
  
@@@ -1516,13 -1755,16 +1929,20 @@@
  
  	spin_lock_init(&net_device_ctx->lock);
  	INIT_LIST_HEAD(&net_device_ctx->reconfig_events);
+ 	INIT_WORK(&net_device_ctx->vf_takeover, netvsc_vf_setup);
+ 	INIT_WORK(&net_device_ctx->vf_notify, netvsc_vf_update);
+ 
+ 	net_device_ctx->vf_stats
+ 		= netdev_alloc_pcpu_stats(struct netvsc_vf_pcpu_stats);
+ 	if (!net_device_ctx->vf_stats)
+ 		goto no_stats;
  
  	net->netdev_ops = &device_ops;
 -	net->ethtool_ops = &ethtool_ops;
 +
 +	net->hw_features = NETVSC_HW_FEATURES;
 +	net->features = NETVSC_HW_FEATURES | NETIF_F_HW_VLAN_CTAG_TX;
 +
 +	SET_ETHTOOL_OPS(net, &ethtool_ops);
  	SET_NETDEV_DEV(net, &dev->device);
  
  	/* We always need headroom for rndis header */
@@@ -1531,26 -1773,39 +1951,35 @@@
  	/* Notify the netvsc driver of the new device */
  	memset(&device_info, 0, sizeof(device_info));
  	device_info.ring_size = ring_size;
 -	device_info.num_chn = VRSS_CHANNEL_DEFAULT;
 -
 -	nvdev = rndis_filter_device_add(dev, &device_info);
 -	if (IS_ERR(nvdev)) {
 -		ret = PTR_ERR(nvdev);
 +	device_info.max_num_vrss_chns = max_num_vrss_chns;
 +	ret = rndis_filter_device_add(dev, &device_info);
 +	if (ret != 0) {
  		netdev_err(net, "unable to add netvsc device (ret %d)\n", ret);
++<<<<<<< HEAD
 +		netvsc_free_netdev(net);
 +		hv_set_drvdata(dev, NULL);
 +		return ret;
++=======
+ 		goto rndis_failed;
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	}
+ 
  	memcpy(net->dev_addr, device_info.mac_adr, ETH_ALEN);
  
 -	/* hw_features computed in rndis_filter_device_add */
 -	net->features = net->hw_features |
 -		NETIF_F_HIGHDMA | NETIF_F_SG |
 -		NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;
 -	net->vlan_features = net->features;
 -
 +	nvdev = net_device_ctx->nvdev;
  	netif_set_real_num_tx_queues(net, nvdev->num_chn);
  	netif_set_real_num_rx_queues(net, nvdev->num_chn);
 -
 -	netdev_lockdep_set_classes(net);
 -
 -	/* MTU range: 68 - 1500 or 65521 */
 -	net->min_mtu = NETVSC_MTU_MIN;
 -	if (nvdev->nvsp_version >= NVSP_PROTOCOL_VERSION_2)
 -		net->max_mtu = NETVSC_MTU - ETH_HLEN;
 -	else
 -		net->max_mtu = ETH_DATA_LEN;
 +	netif_set_gso_max_size(net, NETVSC_GSO_MAX_SIZE);
  
  	ret = register_netdev(net);
  	if (ret != 0) {
  		pr_err("Unable to register netdev.\n");
++<<<<<<< HEAD
 +		rndis_filter_device_remove(dev);
 +		netvsc_free_netdev(net);
++=======
+ 		goto register_failed;
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	}
  
  	return ret;
@@@ -1593,7 -1852,8 +2032,12 @@@ static int netvsc_remove(struct hv_devi
  
  	hv_set_drvdata(dev, NULL);
  
++<<<<<<< HEAD
 +	netvsc_free_netdev(net);
++=======
+ 	free_percpu(ndev_ctx->vf_stats);
+ 	free_netdev(net);
++>>>>>>> 0c195567a8f6 (netvsc: transparent VF management)
  	return 0;
  }
  
diff --git a/drivers/net/hyperv/hyperv_net.h b/drivers/net/hyperv/hyperv_net.h
index a32d7f1b2505..8328ee1cb3db 100644
--- a/drivers/net/hyperv/hyperv_net.h
+++ b/drivers/net/hyperv/hyperv_net.h
@@ -672,6 +672,15 @@ struct netvsc_ethtool_stats {
 	unsigned long tx_busy;
 };
 
+struct netvsc_vf_pcpu_stats {
+	u64     rx_packets;
+	u64     rx_bytes;
+	u64     tx_packets;
+	u64     tx_bytes;
+	struct u64_stats_sync   syncp;
+	u32	tx_dropped;
+};
+
 struct netvsc_reconfig {
 	struct list_head list;
 	u32 event;
@@ -708,6 +717,9 @@ struct net_device_context {
 
 	/* State to manage the associated VF interface. */
 	struct net_device __rcu *vf_netdev;
+	struct netvsc_vf_pcpu_stats __percpu *vf_stats;
+	struct work_struct vf_takeover;
+	struct work_struct vf_notify;
 
 	/* 1: allocated, serial number is valid. 0: not allocated */
 	u32 vf_alloc;
* Unmerged path drivers/net/hyperv/netvsc_drv.c
