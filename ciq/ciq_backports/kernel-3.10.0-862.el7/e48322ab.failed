mm: cma: split cma-reserved in dmesg log

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] cma: split cma-reserved in dmesg log (Serhii Popovych) [1430990]
Rebuild_FUZZ: 94.74%
commit-author Pintu Kumar <pintu.k@samsung.com>
commit e48322abb061d75096fe52d71886b237e7ae7bfb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e48322ab.failed

When the system boots up, in the dmesg logs we can see the memory
statistics along with total reserved as below.  Memory: 458840k/458840k
available, 65448k reserved, 0K highmem

When CMA is enabled, still the total reserved memory remains the same.
However, the CMA memory is not considered as reserved.  But, when we see
/proc/meminfo, the CMA memory is part of free memory.  This creates
confusion.  This patch corrects the problem by properly subtracting the
CMA reserved memory from the total reserved memory in dmesg logs.

Below is the dmesg snapshot from an arm based device with 512MB RAM and
12MB single CMA region.

Before this change:
  Memory: 458840k/458840k available, 65448k reserved, 0K highmem

After this change:
  Memory: 458840k/458840k available, 53160k reserved, 12288k cma-reserved, 0K highmem

	Signed-off-by: Pintu Kumar <pintu.k@samsung.com>
	Signed-off-by: Vishnu Pratap Singh <vishnu.ps@samsung.com>
	Acked-by: Michal Nazarewicz <mina86@mina86.com>
	Cc: Rafael Aquini <aquini@redhat.com>
	Cc: Jerome Marchand <jmarchan@redhat.com>
	Cc: Marek Szyprowski <m.szyprowski@samsung.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e48322abb061d75096fe52d71886b237e7ae7bfb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/cma.h
#	mm/cma.c
* Unmerged path include/linux/cma.h
* Unmerged path mm/cma.c
* Unmerged path include/linux/cma.h
* Unmerged path mm/cma.c
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 793cdb35fdad..36a58e4e4b54 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -113,6 +113,7 @@ static DEFINE_SPINLOCK(managed_page_count_lock);
 
 unsigned long totalram_pages __read_mostly;
 unsigned long totalreserve_pages __read_mostly;
+unsigned long totalcma_pages __read_mostly;
 /*
  * When calculating the number of globally allowed dirty pages, there
  * is a certain number of per-zone reserves that should not be
@@ -6177,7 +6178,7 @@ void __init mem_init_print_info(const char *str)
 
 	printk("Memory: %luK/%luK available "
 	       "(%luK kernel code, %luK rwdata, %luK rodata, "
-	       "%luK init, %luK bss, %luK reserved"
+	       "%luK init, %luK bss, %luK reserved, %luK cma-reserved"
 #ifdef	CONFIG_HIGHMEM
 	       ", %luK highmem"
 #endif
@@ -6185,7 +6186,8 @@ void __init mem_init_print_info(const char *str)
 	       nr_free_pages() << (PAGE_SHIFT-10), physpages << (PAGE_SHIFT-10),
 	       codesize >> 10, datasize >> 10, rosize >> 10,
 	       (init_data_size + init_code_size) >> 10, bss_size >> 10,
-	       (physpages - totalram_pages) << (PAGE_SHIFT-10),
+	       (physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT-10),
+	       totalcma_pages << (PAGE_SHIFT-10),
 #ifdef	CONFIG_HIGHMEM
 	       totalhigh_pages << (PAGE_SHIFT-10),
 #endif
