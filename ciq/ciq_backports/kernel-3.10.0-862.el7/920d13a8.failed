nvme-pci: factor out the cqe reading mechanics from __nvme_process_cq

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [nvme] pci: factor out the cqe reading mechanics from __nvme_process_cq (David Milburn) [1457880 1456486 1454365]
Rebuild_FUZZ: 96.24%
commit-author Sagi Grimberg <sagi@grimberg.me>
commit 920d13a884c0595451658a7b48af8ac16918628f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/920d13a8.failed

Also, maintain a consumed counter to rely on for doorbell and
cqe_seen update instead of directly relying on the cq head and phase.

	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 920d13a884c0595451658a7b48af8ac16918628f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/pci.c
diff --cc drivers/nvme/host/pci.c
index 9cfb96d101ef,d309b6c90511..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -592,58 -730,85 +592,141 @@@ static inline bool nvme_cqe_valid(struc
  	return (le16_to_cpu(nvmeq->cqes[head].status) & 1) == phase;
  }
  
++<<<<<<< HEAD
 +static int nvme_process_cq(struct nvme_queue *nvmeq)
++=======
+ static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
  {
- 	u16 head, phase;
+ 	u16 head = nvmeq->cq_head;
  
+ 	if (likely(nvmeq->cq_vector >= 0)) {
+ 		if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
+ 						      nvmeq->dbbuf_cq_ei))
+ 			writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
+ 	}
+ }
+ 
+ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq,
+ 		struct nvme_completion *cqe)
+ {
+ 	struct request *req;
+ 
+ 	if (unlikely(cqe->command_id >= nvmeq->q_depth)) {
+ 		dev_warn(nvmeq->dev->ctrl.device,
+ 			"invalid id %d completed on queue %d\n",
+ 			cqe->command_id, le16_to_cpu(cqe->sq_id));
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * AEN requests are special as they don't time out and can
+ 	 * survive any kind of queue freeze and often don't respond to
+ 	 * aborts.  We don't even bother to allocate a struct request
+ 	 * for them but rather special case them here.
+ 	 */
+ 	if (unlikely(nvmeq->qid == 0 &&
+ 			cqe->command_id >= NVME_AQ_BLKMQ_DEPTH)) {
+ 		nvme_complete_async_event(&nvmeq->dev->ctrl,
+ 				cqe->status, &cqe->result);
+ 		return;
+ 	}
+ 
+ 	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+ 	nvme_end_request(req, cqe->status, cqe->result);
+ }
+ 
+ static inline bool nvme_read_cqe(struct nvme_queue *nvmeq,
+ 		struct nvme_completion *cqe)
++>>>>>>> 920d13a884c0 (nvme-pci: factor out the cqe reading mechanics from __nvme_process_cq)
+ {
+ 	if (nvme_cqe_valid(nvmeq, nvmeq->cq_head, nvmeq->cq_phase)) {
+ 		*cqe = nvmeq->cqes[nvmeq->cq_head];
+ 
++<<<<<<< HEAD
 +	head = nvmeq->cq_head;
 +	phase = nvmeq->cq_phase;
 +
 +	while (nvme_cqe_valid(nvmeq, head, phase)) {
 +		struct nvme_completion cqe = nvmeq->cqes[head];
 +		struct request *req;
 +
 +		if (++head == nvmeq->q_depth) {
 +			head = 0;
 +			phase = !phase;
++=======
+ 		if (++nvmeq->cq_head == nvmeq->q_depth) {
+ 			nvmeq->cq_head = 0;
+ 			nvmeq->cq_phase = !nvmeq->cq_phase;
++>>>>>>> 920d13a884c0 (nvme-pci: factor out the cqe reading mechanics from __nvme_process_cq)
  		}
+ 		return true;
+ 	}
+ 	return false;
+ }
  
++<<<<<<< HEAD
 +
 +		if (unlikely(cqe.command_id >= nvmeq->q_depth)) {
 +			dev_warn(nvmeq->dev->ctrl.device,
 +				"invalid id %d completed on queue %d\n",
 +				cqe.command_id, le16_to_cpu(cqe.sq_id));
 +			continue;
 +		}
 +
 +		/*
 +		 * AEN requests are special as they don't time out and can
 +		 * survive any kind of queue freeze and often don't respond to
 +		 * aborts.  We don't even bother to allocate a struct request
 +		 * for them but rather special case them here.
 +		 */
 +		if (unlikely(nvmeq->qid == 0 &&
 +				cqe.command_id >= NVME_AQ_BLKMQ_DEPTH)) {
 +			nvme_complete_async_event(&nvmeq->dev->ctrl,
 +					cqe.status, &cqe.result);
 +			continue;
 +		}
 +
 +		req = blk_mq_tag_to_rq(*nvmeq->tags, cqe.command_id);
 +		nvme_req(req)->result = cqe.result;
 +		blk_mq_complete_request(req, le16_to_cpu(cqe.status) >> 1);
 +	}
 +
 +	if (head == nvmeq->cq_head && phase == nvmeq->cq_phase)
 +		return 0;
 +
 +	if (likely(nvmeq->cq_vector >= 0))
 +		writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
 +	nvmeq->cq_head = head;
 +	nvmeq->cq_phase = phase;
 +
 +	nvmeq->cqe_seen = 1;
 +	return 1;
++=======
+ static void __nvme_process_cq(struct nvme_queue *nvmeq, int *tag)
+ {
+ 	struct nvme_completion cqe;
+ 	int consumed = 0;
+ 
+ 	while (nvme_read_cqe(nvmeq, &cqe)) {
+ 		nvme_handle_cqe(nvmeq, &cqe);
+ 		consumed++;
+ 
+ 		if (tag && *tag == cqe.command_id) {
+ 			*tag = -1;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (consumed) {
+ 		nvme_ring_cq_doorbell(nvmeq);
+ 		nvmeq->cqe_seen = 1;
+ 	}
+ }
+ 
+ static void nvme_process_cq(struct nvme_queue *nvmeq)
+ {
+ 	__nvme_process_cq(nvmeq, NULL);
++>>>>>>> 920d13a884c0 (nvme-pci: factor out the cqe reading mechanics from __nvme_process_cq)
  }
  
  static irqreturn_t nvme_irq(int irq, void *data)
* Unmerged path drivers/nvme/host/pci.c
