nfp: create control vNICs and wire up rx/tx

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 02082701b974eea3afdb4ac25ab613adabebe41a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/02082701.failed

When driver encounters an nfp_app which has a control message handler
defined, allocate a control vNIC.  This control channel will be used
to exchange data with the application FW such as flow table programming,
statistics and global datapath control.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 02082701b974eea3afdb4ac25ab613adabebe41a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/nfp_app.c
#	drivers/net/ethernet/netronome/nfp/nfp_app.h
#	drivers/net/ethernet/netronome/nfp/nfp_main.h
#	drivers/net/ethernet/netronome/nfp/nfp_net_common.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_main.c
diff --cc drivers/net/ethernet/netronome/nfp/nfp_main.h
index 1ac430fbaa18,37832853b0b3..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_main.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_main.h
@@@ -57,28 -61,36 +57,45 @@@ struct nfp_eth_table
   * struct nfp_pf - NFP PF-specific device structure
   * @pdev:		Backpointer to PCI device
   * @cpp:		Pointer to the CPP handle
++<<<<<<< HEAD
 + * @ctrl_area:		Pointer to the CPP area for the control BAR
 + * @tx_area:		Pointer to the CPP area for the TX queues
 + * @rx_area:		Pointer to the CPP area for the FL/RX queues
 + * @irq_entries:	Array of MSI-X entries for all ports
++=======
+  * @app:		Pointer to the APP handle
+  * @data_vnic_bar:	Pointer to the CPP area for the data vNICs' BARs
+  * @ctrl_vnic_bar:	Pointer to the CPP area for the ctrl vNIC's BAR
+  * @qc_area:		Pointer to the CPP area for the queues
+  * @irq_entries:	Array of MSI-X entries for all vNICs
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
   * @limit_vfs:		Number of VFs supported by firmware (~0 for PCI limit)
   * @num_vfs:		Number of SR-IOV VFs enabled
   * @fw_loaded:		Is the firmware loaded?
+  * @ctrl_vnic:		Pointer to the control vNIC if available
   * @eth_tbl:		NSP ETH table
 - * @nspi:		NSP identification info
 - * @hwmon_dev:		pointer to hwmon device
   * @ddir:		Per-device debugfs directory
 - * @max_data_vnics:	Number of data vNICs app firmware supports
 - * @num_vnics:		Number of vNICs spawned
 - * @vnics:		Linked list of vNIC structures (struct nfp_net)
 - * @ports:		Linked list of port structures (struct nfp_port)
 + * @num_ports:		Number of adapter ports
 + * @ports:		Linked list of port structures (struct nfp_net)
 + * @port_lock:		Protects @ports, @num_ports, @num_netdevs
   * @port_refresh_work:	Work entry for taking netdevs out
 - * @lock:		Protects all fields which may change after probe
   */
  struct nfp_pf {
  	struct pci_dev *pdev;
  
  	struct nfp_cpp *cpp;
  
++<<<<<<< HEAD
 +	struct nfp_cpp_area *ctrl_area;
 +	struct nfp_cpp_area *tx_area;
 +	struct nfp_cpp_area *rx_area;
++=======
+ 	struct nfp_app *app;
+ 
+ 	struct nfp_cpp_area *data_vnic_bar;
+ 	struct nfp_cpp_area *ctrl_vnic_bar;
+ 	struct nfp_cpp_area *qc_area;
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  
  	struct msix_entry *irq_entries;
  
@@@ -87,7 -99,12 +104,9 @@@
  
  	bool fw_loaded;
  
+ 	struct nfp_net *ctrl_vnic;
+ 
  	struct nfp_eth_table *eth_tbl;
 -	struct nfp_nsp_identify *nspi;
 -
 -	struct device *hwmon_dev;
  
  	struct dentry *ddir;
  
@@@ -102,4 -122,16 +121,17 @@@ extern struct pci_driver nfp_netvf_pci_
  int nfp_net_pci_probe(struct nfp_pf *pf);
  void nfp_net_pci_remove(struct nfp_pf *pf);
  
++<<<<<<< HEAD
++=======
+ int nfp_hwmon_register(struct nfp_pf *pf);
+ void nfp_hwmon_unregister(struct nfp_pf *pf);
+ 
+ struct nfp_eth_table_port *
+ nfp_net_find_port(struct nfp_eth_table *eth_tbl, unsigned int id);
+ void
+ nfp_net_get_mac_addr(struct nfp_net *nn, struct nfp_cpp *cpp, unsigned int id);
+ 
+ bool nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb);
+ 
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  #endif /* NFP_MAIN_H */
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 5094c56dbda7,4f0df63de626..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@@ -1498,6 -1754,231 +1498,234 @@@ static int nfp_net_poll(struct napi_str
  	return pkts_polled;
  }
  
++<<<<<<< HEAD
++=======
+ /* Control device data path
+  */
+ 
+ static bool
+ nfp_ctrl_tx_one(struct nfp_net *nn, struct nfp_net_r_vector *r_vec,
+ 		struct sk_buff *skb, bool old)
+ {
+ 	unsigned int real_len = skb->len, meta_len = 0;
+ 	struct nfp_net_tx_ring *tx_ring;
+ 	struct nfp_net_tx_buf *txbuf;
+ 	struct nfp_net_tx_desc *txd;
+ 	struct nfp_net_dp *dp;
+ 	dma_addr_t dma_addr;
+ 	int wr_idx;
+ 
+ 	dp = &r_vec->nfp_net->dp;
+ 	tx_ring = r_vec->tx_ring;
+ 
+ 	if (WARN_ON_ONCE(skb_shinfo(skb)->nr_frags)) {
+ 		nn_dp_warn(dp, "Driver's CTRL TX does not implement gather\n");
+ 		goto err_free;
+ 	}
+ 
+ 	if (unlikely(nfp_net_tx_full(tx_ring, 1))) {
+ 		u64_stats_update_begin(&r_vec->tx_sync);
+ 		r_vec->tx_busy++;
+ 		u64_stats_update_end(&r_vec->tx_sync);
+ 		if (!old)
+ 			__skb_queue_tail(&r_vec->queue, skb);
+ 		else
+ 			__skb_queue_head(&r_vec->queue, skb);
+ 		return true;
+ 	}
+ 
+ 	if (nfp_app_ctrl_has_meta(nn->app)) {
+ 		if (unlikely(skb_headroom(skb) < 8)) {
+ 			nn_dp_warn(dp, "CTRL TX on skb without headroom\n");
+ 			goto err_free;
+ 		}
+ 		meta_len = 8;
+ 		put_unaligned_be32(NFP_META_PORT_ID_CTRL, skb_push(skb, 4));
+ 		put_unaligned_be32(NFP_NET_META_PORTID, skb_push(skb, 4));
+ 	}
+ 
+ 	/* Start with the head skbuf */
+ 	dma_addr = dma_map_single(dp->dev, skb->data, skb_headlen(skb),
+ 				  DMA_TO_DEVICE);
+ 	if (dma_mapping_error(dp->dev, dma_addr))
+ 		goto err_dma_warn;
+ 
+ 	wr_idx = D_IDX(tx_ring, tx_ring->wr_p);
+ 
+ 	/* Stash the soft descriptor of the head then initialize it */
+ 	txbuf = &tx_ring->txbufs[wr_idx];
+ 	txbuf->skb = skb;
+ 	txbuf->dma_addr = dma_addr;
+ 	txbuf->fidx = -1;
+ 	txbuf->pkt_cnt = 1;
+ 	txbuf->real_len = real_len;
+ 
+ 	/* Build TX descriptor */
+ 	txd = &tx_ring->txds[wr_idx];
+ 	txd->offset_eop = meta_len | PCIE_DESC_TX_EOP;
+ 	txd->dma_len = cpu_to_le16(skb_headlen(skb));
+ 	nfp_desc_set_dma_addr(txd, dma_addr);
+ 	txd->data_len = cpu_to_le16(skb->len);
+ 
+ 	txd->flags = 0;
+ 	txd->mss = 0;
+ 	txd->lso_hdrlen = 0;
+ 
+ 	tx_ring->wr_p++;
+ 	tx_ring->wr_ptr_add++;
+ 	nfp_net_tx_xmit_more_flush(tx_ring);
+ 
+ 	return false;
+ 
+ err_dma_warn:
+ 	nn_dp_warn(dp, "Failed to DMA map TX CTRL buffer\n");
+ err_free:
+ 	u64_stats_update_begin(&r_vec->tx_sync);
+ 	r_vec->tx_errors++;
+ 	u64_stats_update_end(&r_vec->tx_sync);
+ 	dev_kfree_skb_any(skb);
+ 	return false;
+ }
+ 
+ bool nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb)
+ {
+ 	struct nfp_net_r_vector *r_vec = &nn->r_vecs[0];
+ 	bool ret;
+ 
+ 	spin_lock_bh(&r_vec->lock);
+ 	ret = nfp_ctrl_tx_one(nn, r_vec, skb, false);
+ 	spin_unlock_bh(&r_vec->lock);
+ 
+ 	return ret;
+ }
+ 
+ static void __nfp_ctrl_tx_queued(struct nfp_net_r_vector *r_vec)
+ {
+ 	struct sk_buff *skb;
+ 
+ 	while ((skb = __skb_dequeue(&r_vec->queue)))
+ 		if (nfp_ctrl_tx_one(r_vec->nfp_net, r_vec, skb, true))
+ 			return;
+ }
+ 
+ static bool
+ nfp_ctrl_meta_ok(struct nfp_net *nn, void *data, unsigned int meta_len)
+ {
+ 	u32 meta_type, meta_tag;
+ 
+ 	if (!nfp_app_ctrl_has_meta(nn->app))
+ 		return !meta_len;
+ 
+ 	if (meta_len != 8)
+ 		return false;
+ 
+ 	meta_type = get_unaligned_be32(data);
+ 	meta_tag = get_unaligned_be32(data + 4);
+ 
+ 	return (meta_type == NFP_NET_META_PORTID &&
+ 		meta_tag == NFP_META_PORT_ID_CTRL);
+ }
+ 
+ static bool
+ nfp_ctrl_rx_one(struct nfp_net *nn, struct nfp_net_dp *dp,
+ 		struct nfp_net_r_vector *r_vec, struct nfp_net_rx_ring *rx_ring)
+ {
+ 	unsigned int meta_len, data_len, meta_off, pkt_len, pkt_off;
+ 	struct nfp_net_rx_buf *rxbuf;
+ 	struct nfp_net_rx_desc *rxd;
+ 	dma_addr_t new_dma_addr;
+ 	struct sk_buff *skb;
+ 	void *new_frag;
+ 	int idx;
+ 
+ 	idx = D_IDX(rx_ring, rx_ring->rd_p);
+ 
+ 	rxd = &rx_ring->rxds[idx];
+ 	if (!(rxd->rxd.meta_len_dd & PCIE_DESC_RX_DD))
+ 		return false;
+ 
+ 	/* Memory barrier to ensure that we won't do other reads
+ 	 * before the DD bit.
+ 	 */
+ 	dma_rmb();
+ 
+ 	rx_ring->rd_p++;
+ 
+ 	rxbuf =	&rx_ring->rxbufs[idx];
+ 	meta_len = rxd->rxd.meta_len_dd & PCIE_DESC_RX_META_LEN_MASK;
+ 	data_len = le16_to_cpu(rxd->rxd.data_len);
+ 	pkt_len = data_len - meta_len;
+ 
+ 	pkt_off = NFP_NET_RX_BUF_HEADROOM + dp->rx_dma_off;
+ 	if (dp->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC)
+ 		pkt_off += meta_len;
+ 	else
+ 		pkt_off += dp->rx_offset;
+ 	meta_off = pkt_off - meta_len;
+ 
+ 	/* Stats update */
+ 	u64_stats_update_begin(&r_vec->rx_sync);
+ 	r_vec->rx_pkts++;
+ 	r_vec->rx_bytes += pkt_len;
+ 	u64_stats_update_end(&r_vec->rx_sync);
+ 
+ 	nfp_net_dma_sync_cpu_rx(dp, rxbuf->dma_addr + meta_off,	data_len);
+ 
+ 	if (unlikely(!nfp_ctrl_meta_ok(nn, rxbuf->frag + meta_off, meta_len))) {
+ 		nn_dp_warn(dp, "incorrect metadata for ctrl packet (%d)\n",
+ 			   meta_len);
+ 		nfp_net_rx_drop(dp, r_vec, rx_ring, rxbuf, NULL);
+ 		return true;
+ 	}
+ 
+ 	skb = build_skb(rxbuf->frag, dp->fl_bufsz);
+ 	if (unlikely(!skb)) {
+ 		nfp_net_rx_drop(dp, r_vec, rx_ring, rxbuf, NULL);
+ 		return true;
+ 	}
+ 	new_frag = nfp_net_napi_alloc_one(dp, &new_dma_addr);
+ 	if (unlikely(!new_frag)) {
+ 		nfp_net_rx_drop(dp, r_vec, rx_ring, rxbuf, skb);
+ 		return true;
+ 	}
+ 
+ 	nfp_net_dma_unmap_rx(dp, rxbuf->dma_addr);
+ 
+ 	nfp_net_rx_give_one(dp, rx_ring, new_frag, new_dma_addr);
+ 
+ 	skb_reserve(skb, pkt_off);
+ 	skb_put(skb, pkt_len);
+ 
+ 	nfp_app_ctrl_rx(nn->app, skb);
+ 
+ 	return true;
+ }
+ 
+ static void nfp_ctrl_rx(struct nfp_net_r_vector *r_vec)
+ {
+ 	struct nfp_net_rx_ring *rx_ring = r_vec->rx_ring;
+ 	struct nfp_net *nn = r_vec->nfp_net;
+ 	struct nfp_net_dp *dp = &nn->dp;
+ 
+ 	while (nfp_ctrl_rx_one(nn, dp, r_vec, rx_ring))
+ 		continue;
+ }
+ 
+ static void nfp_ctrl_poll(unsigned long arg)
+ {
+ 	struct nfp_net_r_vector *r_vec = (void *)arg;
+ 
+ 	spin_lock_bh(&r_vec->lock);
+ 	nfp_net_tx_complete(r_vec->tx_ring);
+ 	__nfp_ctrl_tx_queued(r_vec);
+ 	spin_unlock_bh(&r_vec->lock);
+ 
+ 	nfp_ctrl_rx(r_vec);
+ 
+ 	nfp_net_irq_unmask(r_vec->nfp_net, r_vec->irq_entry);
+ }
+ 
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  /* Setup and Configuration
   */
  
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_main.c
index acc8cfe284f3,db12700b5afc..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_main.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_main.c
@@@ -207,77 -212,65 +207,83 @@@ static unsigned int nfp_net_pf_get_num_
  	return val;
  }
  
 -static int nfp_net_pf_get_num_ports(struct nfp_pf *pf)
 +static unsigned int
 +nfp_net_pf_total_qcs(struct nfp_pf *pf, void __iomem *ctrl_bar,
 +		     unsigned int stride, u32 start_off, u32 num_off)
  {
 -	return nfp_net_pf_rtsym_read_optional(pf, "nfd_cfg_pf%u_num_ports", 1);
 -}
 +	unsigned int i, min_qc, max_qc;
  
 -static int nfp_net_pf_get_app_id(struct nfp_pf *pf)
 -{
 -	return nfp_net_pf_rtsym_read_optional(pf, "_pf%u_net_app_id",
 -					      NFP_APP_CORE_NIC);
 +	min_qc = readl(ctrl_bar + start_off);
 +	max_qc = min_qc;
 +
 +	for (i = 0; i < pf->num_ports; i++) {
 +		/* To make our lives simpler only accept configuration where
 +		 * queues are allocated to PFs in order (queues of PFn all have
 +		 * indexes lower than PFn+1).
 +		 */
 +		if (max_qc > readl(ctrl_bar + start_off))
 +			return 0;
 +
 +		max_qc = readl(ctrl_bar + start_off);
 +		max_qc += readl(ctrl_bar + num_off) * stride;
 +		ctrl_bar += NFP_PF_CSR_SLICE_SIZE;
 +	}
 +
 +	return max_qc - min_qc;
  }
  
 -static u8 __iomem *
 -nfp_net_pf_map_rtsym(struct nfp_pf *pf, const char *name, const char *sym_fmt,
 -		     unsigned int min_size, struct nfp_cpp_area **area)
 +static u8 __iomem *nfp_net_pf_map_ctrl_bar(struct nfp_pf *pf)
  {
 -	const struct nfp_rtsym *sym;
 +	const struct nfp_rtsym *ctrl_sym;
 +	u8 __iomem *ctrl_bar;
  	char pf_symbol[256];
 -	u8 __iomem *mem;
  
 -	snprintf(pf_symbol, sizeof(pf_symbol), sym_fmt,
 +	snprintf(pf_symbol, sizeof(pf_symbol), "_pf%u_net_bar0",
  		 nfp_cppcore_pcie_unit(pf->cpp));
  
 -	sym = nfp_rtsym_lookup(pf->cpp, pf_symbol);
 -	if (!sym) {
 -		nfp_err(pf->cpp, "Failed to find PF symbol %s\n", pf_symbol);
 -		return (u8 __iomem *)ERR_PTR(-ENOENT);
 +	ctrl_sym = nfp_rtsym_lookup(pf->cpp, pf_symbol);
 +	if (!ctrl_sym) {
 +		dev_err(&pf->pdev->dev,
 +			"Failed to find PF BAR0 symbol %s\n", pf_symbol);
 +		return NULL;
  	}
  
 -	if (sym->size < min_size) {
 -		nfp_err(pf->cpp, "PF symbol %s too small\n", pf_symbol);
 -		return (u8 __iomem *)ERR_PTR(-EINVAL);
 +	if (ctrl_sym->size < pf->num_ports * NFP_PF_CSR_SLICE_SIZE) {
 +		dev_err(&pf->pdev->dev,
 +			"PF BAR0 too small to contain %d ports\n",
 +			pf->num_ports);
 +		return NULL;
  	}
  
 -	mem = nfp_net_map_area(pf->cpp, name, sym->domain, sym->target,
 -			       sym->addr, sym->size, area);
 -	if (IS_ERR(mem)) {
 -		nfp_err(pf->cpp, "Failed to map PF symbol %s: %ld\n",
 -			pf_symbol, PTR_ERR(mem));
 -		return mem;
 +	ctrl_bar = nfp_net_map_area(pf->cpp, "net.ctrl",
 +				    ctrl_sym->domain, ctrl_sym->target,
 +				    ctrl_sym->addr, ctrl_sym->size,
 +				    &pf->ctrl_area);
 +	if (IS_ERR(ctrl_bar)) {
 +		dev_err(&pf->pdev->dev, "Failed to map PF BAR0: %ld\n",
 +			PTR_ERR(ctrl_bar));
 +		return NULL;
  	}
  
 -	return mem;
 -}
 -
 -static void nfp_net_pf_free_vnic(struct nfp_pf *pf, struct nfp_net *nn)
 -{
 -	nfp_port_free(nn->port);
 -	list_del(&nn->vnic_list);
 -	pf->num_vnics--;
 -	nfp_net_free(nn);
 +	return ctrl_bar;
  }
  
 -static void nfp_net_pf_free_vnics(struct nfp_pf *pf)
 +static void nfp_net_pf_free_netdevs(struct nfp_pf *pf)
  {
- 	struct nfp_net *nn;
+ 	struct nfp_net *nn, *next;
  
++<<<<<<< HEAD
 +	while (!list_empty(&pf->ports)) {
 +		nn = list_first_entry(&pf->ports, struct nfp_net, port_list);
 +		list_del(&nn->port_list);
 +
 +		nfp_net_netdev_free(nn);
 +	}
++=======
+ 	list_for_each_entry_safe(nn, next, &pf->vnics, vnic_list)
+ 		if (nfp_net_is_data_vnic(nn))
+ 			nfp_net_pf_free_vnic(pf, nn);
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  }
  
  static struct nfp_net *
@@@ -305,6 -301,17 +311,20 @@@ nfp_net_pf_alloc_port_netdev(struct nfp
  	nn->stride_rx = stride;
  	nn->stride_tx = stride;
  
++<<<<<<< HEAD
++=======
+ 	if (needs_netdev) {
+ 		err = nfp_app_vnic_init(pf->app, nn, eth_id);
+ 		if (err) {
+ 			nfp_net_free(nn);
+ 			return ERR_PTR(err);
+ 		}
+ 	}
+ 
+ 	pf->num_vnics++;
+ 	list_add_tail(&nn->vnic_list, &pf->vnics);
+ 
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  	return nn;
  }
  
@@@ -419,13 -426,30 +439,20 @@@ nfp_net_pf_spawn_netdevs(struct nfp_pf 
  		nfp_net_irqs_assign(nn, &pf->irq_entries[num_irqs - irqs_left],
  				    n);
  		irqs_left -= n;
 -		vnics_left--;
 +		ports_left--;
  	}
  
 -	return 0;
 -}
 -
 -static void nfp_net_pf_free_irqs(struct nfp_pf *pf)
 -{
 -	nfp_net_irqs_disable(pf->pdev);
 -	kfree(pf->irq_entries);
 -}
 -
 -static int nfp_net_pf_init_vnics(struct nfp_pf *pf)
 -{
 -	struct nfp_net *nn;
 -	unsigned int id;
 -	int err;
 -
 -	/* Finish vNIC init and register */
 +	/* Finish netdev init and register */
  	id = 0;
++<<<<<<< HEAD
 +	list_for_each_entry(nn, &pf->ports, port_list) {
 +		err = nfp_net_pf_init_port_netdev(pf, nn, id);
++=======
+ 	list_for_each_entry(nn, &pf->vnics, vnic_list) {
+ 		if (!nfp_net_is_data_vnic(nn))
+ 			continue;
+ 		err = nfp_net_pf_init_vnic(pf, nn, id);
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  		if (err)
  			goto err_prev_deinit;
  
@@@ -435,91 -459,244 +462,207 @@@
  	return 0;
  
  err_prev_deinit:
++<<<<<<< HEAD
 +	list_for_each_entry_continue_reverse(nn, &pf->ports, port_list) {
 +		nfp_net_debugfs_dir_clean(&nn->debugfs_dir);
 +		nfp_net_netdev_clean(nn->netdev);
 +	}
 +	nfp_net_irqs_disable(pf->pdev);
 +err_vec_free:
 +	kfree(pf->irq_entries);
 +err_nn_free:
 +	nfp_net_pf_free_netdevs(pf);
 +	return err;
 +}
 +
++=======
+ 	list_for_each_entry_continue_reverse(nn, &pf->vnics, vnic_list)
+ 		if (nfp_net_is_data_vnic(nn))
+ 			nfp_net_pf_clean_vnic(pf, nn);
+ 	return err;
+ }
+ 
+ static int
+ nfp_net_pf_app_init(struct nfp_pf *pf, u8 __iomem *qc_bar, unsigned int stride)
+ {
+ 	u8 __iomem *ctrl_bar;
+ 	int err;
+ 
+ 	pf->app = nfp_app_alloc(pf, nfp_net_pf_get_app_id(pf));
+ 	if (IS_ERR(pf->app))
+ 		return PTR_ERR(pf->app);
+ 
+ 	err = nfp_app_init(pf->app);
+ 	if (err)
+ 		goto err_free;
+ 
+ 	if (!nfp_app_needs_ctrl_vnic(pf->app))
+ 		return 0;
+ 
+ 	ctrl_bar = nfp_net_pf_map_rtsym(pf, "net.ctrl", "_pf%u_net_ctrl_bar",
+ 					NFP_PF_CSR_SLICE_SIZE,
+ 					&pf->ctrl_vnic_bar);
+ 	if (IS_ERR(ctrl_bar)) {
+ 		err = PTR_ERR(ctrl_bar);
+ 		goto err_free;
+ 	}
+ 
+ 	pf->ctrl_vnic =	nfp_net_pf_alloc_vnic(pf, false, ctrl_bar, qc_bar,
+ 					      stride, 0);
+ 	if (IS_ERR(pf->ctrl_vnic)) {
+ 		err = PTR_ERR(pf->ctrl_vnic);
+ 		goto err_unmap;
+ 	}
+ 
+ 	return 0;
+ 
+ err_unmap:
+ 	nfp_cpp_area_release_free(pf->ctrl_vnic_bar);
+ err_free:
+ 	nfp_app_free(pf->app);
+ 	return err;
+ }
+ 
+ static void nfp_net_pf_app_clean(struct nfp_pf *pf)
+ {
+ 	if (pf->ctrl_vnic) {
+ 		nfp_net_pf_free_vnic(pf, pf->ctrl_vnic);
+ 		nfp_cpp_area_release_free(pf->ctrl_vnic_bar);
+ 	}
+ 	nfp_app_free(pf->app);
+ 	pf->app = NULL;
+ }
+ 
+ static int nfp_net_pf_app_start_ctrl(struct nfp_pf *pf)
+ {
+ 	int err;
+ 
+ 	if (!pf->ctrl_vnic)
+ 		return 0;
+ 	err = nfp_net_pf_init_vnic(pf, pf->ctrl_vnic, 0);
+ 	if (err)
+ 		return err;
+ 
+ 	err = nfp_ctrl_open(pf->ctrl_vnic);
+ 	if (err)
+ 		goto err_clean_ctrl;
+ 
+ 	return 0;
+ 
+ err_clean_ctrl:
+ 	nfp_net_pf_clean_vnic(pf, pf->ctrl_vnic);
+ 	return err;
+ }
+ 
+ static void nfp_net_pf_app_stop_ctrl(struct nfp_pf *pf)
+ {
+ 	if (!pf->ctrl_vnic)
+ 		return;
+ 	nfp_ctrl_close(pf->ctrl_vnic);
+ 	nfp_net_pf_clean_vnic(pf, pf->ctrl_vnic);
+ }
+ 
+ static int nfp_net_pf_app_start(struct nfp_pf *pf)
+ {
+ 	int err;
+ 
+ 	err = nfp_net_pf_app_start_ctrl(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	err = nfp_app_start(pf->app, pf->ctrl_vnic);
+ 	if (err)
+ 		goto err_ctrl_stop;
+ 
+ 	return 0;
+ 
+ err_ctrl_stop:
+ 	nfp_net_pf_app_stop_ctrl(pf);
+ 	return err;
+ }
+ 
+ static void nfp_net_pf_app_stop(struct nfp_pf *pf)
+ {
+ 	nfp_app_stop(pf->app);
+ 	nfp_net_pf_app_stop_ctrl(pf);
+ }
+ 
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  static void nfp_net_pci_remove_finish(struct nfp_pf *pf)
  {
+ 	nfp_net_pf_app_stop(pf);
+ 	/* stop app first, to avoid double free of ctrl vNIC's ddir */
  	nfp_net_debugfs_dir_clean(&pf->ddir);
  
 -	nfp_net_pf_free_irqs(pf);
 -
 -	nfp_net_pf_app_clean(pf);
 -
 -	nfp_cpp_area_release_free(pf->qc_area);
 -	nfp_cpp_area_release_free(pf->data_vnic_bar);
 -}
 -
 -static int
 -nfp_net_eth_port_update(struct nfp_cpp *cpp, struct nfp_port *port,
 -			struct nfp_eth_table *eth_table)
 -{
 -	struct nfp_eth_table_port *eth_port;
 -
 -	ASSERT_RTNL();
 -
 -	eth_port = nfp_net_find_port(eth_table, port->eth_id);
 -	if (!eth_port) {
 -		set_bit(NFP_PORT_CHANGED, &port->flags);
 -		nfp_warn(cpp, "Warning: port #%d not present after reconfig\n",
 -			 port->eth_id);
 -		return -EIO;
 -	}
 -	if (eth_port->override_changed) {
 -		nfp_warn(cpp, "Port #%d config changed, unregistering. Reboot required before port will be operational again.\n", port->eth_id);
 -		port->type = NFP_PORT_INVALID;
 -	}
 -
 -	memcpy(port->eth_port, eth_port, sizeof(*eth_port));
 +	nfp_net_irqs_disable(pf->pdev);
 +	kfree(pf->irq_entries);
  
 -	return 0;
 +	nfp_cpp_area_release_free(pf->rx_area);
 +	nfp_cpp_area_release_free(pf->tx_area);
 +	nfp_cpp_area_release_free(pf->ctrl_area);
  }
  
 -int nfp_net_refresh_port_table_sync(struct nfp_pf *pf)
 +static void nfp_net_refresh_netdevs(struct work_struct *work)
  {
 -	struct nfp_eth_table *eth_table;
 +	struct nfp_pf *pf = container_of(work, struct nfp_pf,
 +					 port_refresh_work);
  	struct nfp_net *nn, *next;
 -	struct nfp_port *port;
  
 -	lockdep_assert_held(&pf->lock);
 +	mutex_lock(&pf->port_lock);
  
  	/* Check for nfp_net_pci_remove() racing against us */
 -	if (list_empty(&pf->vnics))
 -		return 0;
 -
 -	/* Update state of all ports */
 -	rtnl_lock();
 -	list_for_each_entry(port, &pf->ports, port_list)
 -		clear_bit(NFP_PORT_CHANGED, &port->flags);
 -
 -	eth_table = nfp_eth_read_ports(pf->cpp);
 -	if (!eth_table) {
 -		list_for_each_entry(port, &pf->ports, port_list)
 -			if (__nfp_port_get_eth_port(port))
 -				set_bit(NFP_PORT_CHANGED, &port->flags);
 -		rtnl_unlock();
 -		nfp_err(pf->cpp, "Error refreshing port config!\n");
 -		return -EIO;
 -	}
 +	if (list_empty(&pf->ports))
 +		goto out;
  
 -	list_for_each_entry(port, &pf->ports, port_list)
 -		if (__nfp_port_get_eth_port(port))
 -			nfp_net_eth_port_update(pf->cpp, port, eth_table);
 -	rtnl_unlock();
 +	list_for_each_entry_safe(nn, next, &pf->ports, port_list) {
 +		if (!nn->eth_port) {
 +			nfp_warn(pf->cpp, "Warning: port not present after reconfig\n");
 +			continue;
 +		}
 +		if (!nn->eth_port->override_changed)
 +			continue;
  
 -	kfree(eth_table);
 +		nn_warn(nn, "Port config changed, unregistering. Reboot required before port will be operational again.\n");
  
 -	/* Shoot off the ports which became invalid */
 -	list_for_each_entry_safe(nn, next, &pf->vnics, vnic_list) {
 -		if (!nn->port || nn->port->type != NFP_PORT_INVALID)
 -			continue;
 +		nfp_net_debugfs_dir_clean(&nn->debugfs_dir);
 +		nfp_net_netdev_clean(nn->dp.netdev);
  
 -		nfp_net_pf_clean_vnic(pf, nn);
 -		nfp_net_pf_free_vnic(pf, nn);
 +		list_del(&nn->port_list);
 +		pf->num_netdevs--;
 +		nfp_net_netdev_free(nn);
  	}
  
 -	if (list_empty(&pf->vnics))
 +	if (list_empty(&pf->ports))
  		nfp_net_pci_remove_finish(pf);
 -
 -	return 0;
 -}
 -
 -static void nfp_net_refresh_vnics(struct work_struct *work)
 -{
 -	struct nfp_pf *pf = container_of(work, struct nfp_pf,
 -					 port_refresh_work);
 -
 -	mutex_lock(&pf->lock);
 -	nfp_net_refresh_port_table_sync(pf);
 -	mutex_unlock(&pf->lock);
 +out:
 +	mutex_unlock(&pf->port_lock);
  }
  
 -void nfp_net_refresh_port_table(struct nfp_port *port)
 +void nfp_net_refresh_port_config(struct nfp_net *nn)
  {
 -	struct nfp_pf *pf = port->app->pf;
 -
 -	set_bit(NFP_PORT_CHANGED, &port->flags);
 +	struct nfp_pf *pf = pci_get_drvdata(nn->pdev);
 +	struct nfp_eth_table *old_table;
  
 -	schedule_work(&pf->port_refresh_work);
 -}
 +	ASSERT_RTNL();
  
 -int nfp_net_refresh_eth_port(struct nfp_port *port)
 -{
 -	struct nfp_cpp *cpp = port->app->cpp;
 -	struct nfp_eth_table *eth_table;
 -	int ret;
 +	old_table = pf->eth_tbl;
  
 -	clear_bit(NFP_PORT_CHANGED, &port->flags);
 +	list_for_each_entry(nn, &pf->ports, port_list)
 +		nfp_net_link_changed_read_clear(nn);
  
 -	eth_table = nfp_eth_read_ports(cpp);
 -	if (!eth_table) {
 -		set_bit(NFP_PORT_CHANGED, &port->flags);
 -		nfp_err(cpp, "Error refreshing port state table!\n");
 -		return -EIO;
 +	pf->eth_tbl = nfp_eth_read_ports(pf->cpp);
 +	if (!pf->eth_tbl) {
 +		pf->eth_tbl = old_table;
 +		nfp_err(pf->cpp, "Error refreshing port config!\n");
 +		return;
  	}
  
 -	ret = nfp_net_eth_port_update(cpp, port, eth_table);
 +	list_for_each_entry(nn, &pf->ports, port_list)
 +		nn->eth_port = nfp_net_find_port(pf, nn->eth_port->eth_index);
  
 -	kfree(eth_table);
 +	kfree(old_table);
  
 -	return ret;
 +	schedule_work(&pf->port_refresh_work);
  }
  
  /*
@@@ -579,44 -761,19 +722,50 @@@ int nfp_net_pci_probe(struct nfp_pf *pf
  		}
  	}
  
 -	/* Map queues */
 -	qc_bar = nfp_net_map_area(pf->cpp, "net.qc", 0, 0,
 -				  NFP_PCIE_QUEUE(0), NFP_QCP_QUEUE_AREA_SZ,
 -				  &pf->qc_area);
 -	if (IS_ERR(qc_bar)) {
 -		nfp_err(pf->cpp, "Failed to map Queue Controller area.\n");
 -		err = PTR_ERR(qc_bar);
 +	/* Find how many QC structs need to be mapped */
 +	total_tx_qcs = nfp_net_pf_total_qcs(pf, ctrl_bar, stride,
 +					    NFP_NET_CFG_START_TXQ,
 +					    NFP_NET_CFG_MAX_TXRINGS);
 +	total_rx_qcs = nfp_net_pf_total_qcs(pf, ctrl_bar, stride,
 +					    NFP_NET_CFG_START_RXQ,
 +					    NFP_NET_CFG_MAX_RXRINGS);
 +	if (!total_tx_qcs || !total_rx_qcs) {
 +		nfp_err(pf->cpp, "Invalid PF QC configuration [%d,%d]\n",
 +			total_tx_qcs, total_rx_qcs);
 +		err = -EINVAL;
 +		goto err_ctrl_unmap;
 +	}
 +
++<<<<<<< HEAD
 +	tx_area_sz = NFP_QCP_QUEUE_ADDR_SZ * total_tx_qcs;
 +	rx_area_sz = NFP_QCP_QUEUE_ADDR_SZ * total_rx_qcs;
 +
 +	/* Map TX queues */
 +	start_q = readl(ctrl_bar + NFP_NET_CFG_START_TXQ);
 +	tx_bar = nfp_net_map_area(pf->cpp, "net.tx", 0, 0,
 +				  NFP_PCIE_QUEUE(start_q),
 +				  tx_area_sz, &pf->tx_area);
 +	if (IS_ERR(tx_bar)) {
 +		nfp_err(pf->cpp, "Failed to map TX area.\n");
 +		err = PTR_ERR(tx_bar);
  		goto err_ctrl_unmap;
  	}
  
 +	/* Map RX queues */
 +	start_q = readl(ctrl_bar + NFP_NET_CFG_START_RXQ);
 +	rx_bar = nfp_net_map_area(pf->cpp, "net.rx", 0, 0,
 +				  NFP_PCIE_QUEUE(start_q),
 +				  rx_area_sz, &pf->rx_area);
 +	if (IS_ERR(rx_bar)) {
 +		nfp_err(pf->cpp, "Failed to map RX area.\n");
 +		err = PTR_ERR(rx_bar);
 +		goto err_unmap_tx;
 +	}
++=======
+ 	err = nfp_net_pf_app_init(pf, qc_bar, stride);
+ 	if (err)
+ 		goto err_unmap_qc;
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  
  	pf->ddir = nfp_net_debugfs_device_add(pf->pdev);
  
@@@ -625,19 -782,37 +774,44 @@@
  	if (err)
  		goto err_clean_ddir;
  
++<<<<<<< HEAD
 +	mutex_unlock(&pf->port_lock);
 +
 +	return 0;
 +
++=======
+ 	err = nfp_net_pf_alloc_irqs(pf);
+ 	if (err)
+ 		goto err_free_vnics;
+ 
+ 	err = nfp_net_pf_app_start(pf);
+ 	if (err)
+ 		goto err_free_irqs;
+ 
+ 	err = nfp_net_pf_init_vnics(pf);
+ 	if (err)
+ 		goto err_stop_app;
+ 
+ 	mutex_unlock(&pf->lock);
+ 
+ 	return 0;
+ 
+ err_stop_app:
+ 	nfp_net_pf_app_stop(pf);
+ err_free_irqs:
+ 	nfp_net_pf_free_irqs(pf);
+ err_free_vnics:
+ 	nfp_net_pf_free_vnics(pf);
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  err_clean_ddir:
  	nfp_net_debugfs_dir_clean(&pf->ddir);
 -	nfp_net_pf_app_clean(pf);
 -err_unmap_qc:
 -	nfp_cpp_area_release_free(pf->qc_area);
 +	nfp_cpp_area_release_free(pf->rx_area);
 +err_unmap_tx:
 +	nfp_cpp_area_release_free(pf->tx_area);
  err_ctrl_unmap:
 -	nfp_cpp_area_release_free(pf->data_vnic_bar);
 +	nfp_cpp_area_release_free(pf->ctrl_area);
  err_unlock:
 -	mutex_unlock(&pf->lock);
 +	mutex_unlock(&pf->port_lock);
  	return err;
  }
  
@@@ -645,17 -820,15 +819,23 @@@ void nfp_net_pci_remove(struct nfp_pf *
  {
  	struct nfp_net *nn;
  
 -	mutex_lock(&pf->lock);
 -	if (list_empty(&pf->vnics))
 +	mutex_lock(&pf->port_lock);
 +	if (list_empty(&pf->ports))
  		goto out;
  
++<<<<<<< HEAD
 +	list_for_each_entry(nn, &pf->ports, port_list) {
 +		nfp_net_debugfs_dir_clean(&nn->debugfs_dir);
++=======
+ 	list_for_each_entry(nn, &pf->vnics, vnic_list)
+ 		if (nfp_net_is_data_vnic(nn))
+ 			nfp_net_pf_clean_vnic(pf, nn);
++>>>>>>> 02082701b974 (nfp: create control vNICs and wire up rx/tx)
  
 -	nfp_net_pf_free_vnics(pf);
 +		nfp_net_netdev_clean(nn->netdev);
 +	}
 +
 +	nfp_net_pf_free_netdevs(pf);
  
  	nfp_net_pci_remove_finish(pf);
  out:
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_app.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_app.h
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_app.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_app.h
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_main.h
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_common.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_main.c
