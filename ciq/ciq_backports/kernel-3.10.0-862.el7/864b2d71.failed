net/mlx5e: Generalize tx helper functions for different SQ types

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Generalize tx helper functions for different SQ types (Don Dutile) [1456694 1499362]
Rebuild_FUZZ: 96.77%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 864b2d715300d9082747fb5de2bb277359c75bff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/864b2d71.failed

In the next patches we will introduce different SQ types, for that we here
generalize some TX helper functions to work with more basic SQ parameters,
in order to re-use them for the different SQ types.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 864b2d715300d9082747fb5de2bb277359c75bff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 16c2c2d53ebb,50f895fa5f31..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -286,13 -293,93 +286,97 @@@ struct mlx5e_cq 
  	struct mlx5_frag_wq_ctrl   wq_ctrl;
  } ____cacheline_aligned_in_smp;
  
 -struct mlx5e_tx_wqe_info {
 -	u32 num_bytes;
 -	u8  num_wqebbs;
 -	u8  num_dma;
 -};
 -
 +struct mlx5e_rq;
 +typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
 +				       struct mlx5_cqe64 *cqe);
 +typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
 +				  u16 ix);
 +
++<<<<<<< HEAD
 +typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq *rq, u16 ix);
++=======
+ enum mlx5e_dma_map_type {
+ 	MLX5E_DMA_MAP_SINGLE,
+ 	MLX5E_DMA_MAP_PAGE
+ };
+ 
+ struct mlx5e_sq_dma {
+ 	dma_addr_t              addr;
+ 	u32                     size;
+ 	enum mlx5e_dma_map_type type;
+ };
+ 
+ enum {
+ 	MLX5E_SQ_STATE_ENABLED,
+ };
+ 
+ struct mlx5e_sq_wqe_info {
+ 	u8  opcode;
+ 	u8  num_wqebbs;
+ };
+ 
+ enum mlx5e_sq_type {
+ 	MLX5E_SQ_TXQ,
+ 	MLX5E_SQ_ICO,
+ 	MLX5E_SQ_XDP
+ };
+ 
+ struct mlx5e_sq {
+ 	/* data path */
+ 
+ 	/* dirtied @completion */
+ 	u16                        cc;
+ 	u32                        dma_fifo_cc;
+ 
+ 	/* dirtied @xmit */
+ 	u16                        pc ____cacheline_aligned_in_smp;
+ 	u32                        dma_fifo_pc;
+ 	struct mlx5e_sq_stats      stats;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* pointers to per tx element info: write@xmit, read@completion */
+ 	union {
+ 		struct {
+ 			struct sk_buff           **skb;
+ 			struct mlx5e_sq_dma       *dma_fifo;
+ 			struct mlx5e_tx_wqe_info  *wqe_info;
+ 		} txq;
+ 		struct mlx5e_sq_wqe_info *ico_wqe;
+ 		struct {
+ 			struct mlx5e_dma_info     *di;
+ 			bool                       doorbell;
+ 		} xdp;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	u32                        dma_fifo_mask;
+ 	void __iomem              *uar_map;
+ 	struct netdev_queue       *txq;
+ 	u32                        sqn;
+ 	u16                        max_inline;
+ 	u8                         min_inline_mode;
+ 	u16                        edge;
+ 	struct device             *pdev;
+ 	struct mlx5e_tstamp       *tstamp;
+ 	__be32                     mkey_be;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ 	int                        tc;
+ 	u32                        rate_limit;
+ 	u8                         type;
+ } ____cacheline_aligned_in_smp;
+ 
+ static inline bool
+ mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
+ {
+ 	return (((wq->sz_m1 & (cc - pc)) >= n) || (cc == pc));
+ }
++>>>>>>> 864b2d715300 (net/mlx5e: Generalize tx helper functions for different SQ types)
  
  struct mlx5e_dma_info {
  	struct page	*page;
@@@ -794,11 -806,29 +877,37 @@@ void mlx5e_set_rx_cq_mode_params(struc
  				 u8 cq_period_mode);
  void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type);
  
++<<<<<<< HEAD
 +static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 +				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)
 +{
 +	u16 ofst = MLX5_BF_OFFSET + sq->bf_offset;
 +
++=======
+ static inline
+ struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)
+ {
+ 	u16                         pi   = *pc & wq->sz_m1;
+ 	struct mlx5e_tx_wqe        *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 	struct mlx5_wqe_ctrl_seg   *cseg = &wqe->ctrl;
+ 
+ 	memset(cseg, 0, sizeof(*cseg));
+ 
+ 	cseg->opmod_idx_opcode = cpu_to_be32((*pc << 8) | MLX5_OPCODE_NOP);
+ 	cseg->qpn_ds           = cpu_to_be32((sqn << 8) | 0x01);
+ 
+ 	(*pc)++;
+ 
+ 	return wqe;
+ }
+ 
+ static inline
+ void mlx5e_notify_hw(struct mlx5_wq_cyc *wq, u16 pc,
+ 		     void __iomem *uar_map,
+ 		     struct mlx5_wqe_ctrl_seg *ctrl)
+ {
+ 	ctrl->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
++>>>>>>> 864b2d715300 (net/mlx5e: Generalize tx helper functions for different SQ types)
  	/* ensure wqe is visible to device before updating doorbell record */
  	dma_wmb();
  
@@@ -808,14 -838,8 +917,18 @@@
  	 * doorbell
  	 */
  	wmb();
 +	if (bf_sz)
 +		__iowrite64_copy(sq->uar_map + ofst, ctrl, bf_sz);
 +	else
 +		mlx5_write64((__be32 *)ctrl, sq->uar_map + ofst, NULL);
 +	/* flush the write-combining mapped buffer */
 +	wmb();
  
++<<<<<<< HEAD
 +	sq->bf_offset ^= sq->bf_buf_size;
++=======
+ 	mlx5_write64((__be32 *)ctrl, uar_map, NULL);
++>>>>>>> 864b2d715300 (net/mlx5e: Generalize tx helper functions for different SQ types)
  }
  
  static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index e8c9b2d23033,141dcc486063..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -352,7 -354,7 +353,11 @@@ static inline void mlx5e_post_umr_wqe(s
  	sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_UMR;
  	sq->db.ico_wqe[pi].num_wqebbs = num_wqebbs;
  	sq->pc += num_wqebbs;
++<<<<<<< HEAD
 +	mlx5e_tx_notify_hw(sq, &wqe->ctrl, 0);
++=======
+ 	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &wqe->ctrl);
++>>>>>>> 864b2d715300 (net/mlx5e: Generalize tx helper functions for different SQ types)
  }
  
  static int mlx5e_alloc_rx_umr_mpwqe(struct mlx5e_rq *rq,
@@@ -636,6 -638,120 +641,123 @@@ static inline void mlx5e_complete_rx_cq
  	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
  }
  
++<<<<<<< HEAD
++=======
+ static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_sq *sq)
+ {
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u16 pi = (sq->pc - 1) & wq->sz_m1; /* last pi */
+ 
+ 	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 
+ 	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &wqe->ctrl);
+ }
+ 
+ static inline bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq,
+ 					struct mlx5e_dma_info *di,
+ 					const struct xdp_buff *xdp)
+ {
+ 	struct mlx5e_sq          *sq   = &rq->xdpsq;
+ 	struct mlx5_wq_cyc       *wq   = &sq->wq;
+ 	u16                      pi    = sq->pc & wq->sz_m1;
+ 	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 
+ 	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 
+ 	ptrdiff_t data_offset = xdp->data - xdp->data_hard_start;
+ 	dma_addr_t dma_addr  = di->addr + data_offset;
+ 	unsigned int dma_len = xdp->data_end - xdp->data;
+ 
+ 	prefetchw(wqe);
+ 
+ 	if (unlikely(dma_len < MLX5E_XDP_MIN_INLINE ||
+ 		     MLX5E_SW2HW_MTU(rq->netdev->mtu) < dma_len)) {
+ 		rq->stats.xdp_drop++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return false;
+ 	}
+ 
+ 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, 1))) {
+ 		if (sq->db.xdp.doorbell) {
+ 			/* SQ is full, ring doorbell */
+ 			mlx5e_xmit_xdp_doorbell(sq);
+ 			sq->db.xdp.doorbell = false;
+ 		}
+ 		rq->stats.xdp_tx_full++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return false;
+ 	}
+ 
+ 	dma_sync_single_for_device(sq->pdev, dma_addr, dma_len, PCI_DMA_TODEVICE);
+ 
+ 	cseg->fm_ce_se = 0;
+ 
+ 	dseg = (struct mlx5_wqe_data_seg *)eseg + 1;
+ 
+ 	/* copy the inline part if required */
+ 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+ 		memcpy(eseg->inline_hdr.start, xdp->data, MLX5E_XDP_MIN_INLINE);
+ 		eseg->inline_hdr.sz = cpu_to_be16(MLX5E_XDP_MIN_INLINE);
+ 		dma_len  -= MLX5E_XDP_MIN_INLINE;
+ 		dma_addr += MLX5E_XDP_MIN_INLINE;
+ 		dseg++;
+ 	}
+ 
+ 	/* write the dma part */
+ 	dseg->addr       = cpu_to_be64(dma_addr);
+ 	dseg->byte_count = cpu_to_be32(dma_len);
+ 
+ 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_SEND);
+ 
+ 	sq->db.xdp.di[pi] = *di;
+ 	sq->pc++;
+ 
+ 	sq->db.xdp.doorbell = true;
+ 	rq->stats.xdp_tx++;
+ 	return true;
+ }
+ 
+ /* returns true if packet was consumed by xdp */
+ static inline int mlx5e_xdp_handle(struct mlx5e_rq *rq,
+ 				   struct mlx5e_dma_info *di,
+ 				   void *va, u16 *rx_headroom, u32 *len)
+ {
+ 	const struct bpf_prog *prog = READ_ONCE(rq->xdp_prog);
+ 	struct xdp_buff xdp;
+ 	u32 act;
+ 
+ 	if (!prog)
+ 		return false;
+ 
+ 	xdp.data = va + *rx_headroom;
+ 	xdp.data_end = xdp.data + *len;
+ 	xdp.data_hard_start = va;
+ 
+ 	act = bpf_prog_run_xdp(prog, &xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		*rx_headroom = xdp.data - xdp.data_hard_start;
+ 		*len = xdp.data_end - xdp.data;
+ 		return false;
+ 	case XDP_TX:
+ 		if (unlikely(!mlx5e_xmit_xdp_frame(rq, di, &xdp)))
+ 			trace_xdp_exception(rq->netdev, prog, act);
+ 		return true;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(rq->netdev, prog, act);
+ 	case XDP_DROP:
+ 		rq->stats.xdp_drop++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return true;
+ 	}
+ }
+ 
++>>>>>>> 864b2d715300 (net/mlx5e: Generalize tx helper functions for different SQ types)
  static inline
  struct sk_buff *skb_from_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
  			     u16 wqe_counter, u32 cqe_bcnt)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 2a270903b57d,897eaea6f51f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -38,29 -38,6 +38,32 @@@
  #define MLX5E_SQ_STOP_ROOM (MLX5_SEND_WQE_MAX_WQEBBS +\
  			    MLX5E_SQ_NOPS_ROOM)
  
++<<<<<<< HEAD
 +void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw)
 +{
 +	struct mlx5_wq_cyc                *wq  = &sq->wq;
 +
 +	u16 pi = sq->pc & wq->sz_m1;
 +	struct mlx5e_tx_wqe              *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 +
 +	struct mlx5_wqe_ctrl_seg         *cseg = &wqe->ctrl;
 +
 +	memset(cseg, 0, sizeof(*cseg));
 +
 +	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_NOP);
 +	cseg->qpn_ds           = cpu_to_be32((sq->sqn << 8) | 0x01);
 +
 +	sq->pc++;
 +	sq->stats.nop++;
 +
 +	if (notify_hw) {
 +		cseg->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
 +		mlx5e_tx_notify_hw(sq, &wqe->ctrl, 0);
 +	}
 +}
 +
++=======
++>>>>>>> 864b2d715300 (net/mlx5e: Generalize tx helper functions for different SQ types)
  static inline void mlx5e_tx_dma_unmap(struct device *pdev,
  				      struct mlx5e_sq_dma *dma)
  {
@@@ -361,25 -315,16 +365,31 @@@ static netdev_tx_t mlx5e_sq_xmit(struc
  	}
  
  	sq->stats.xmit_more += skb->xmit_more;
++<<<<<<< HEAD
 +	if (!skb->xmit_more || netif_xmit_stopped(sq->txq)) {
 +		int bf_sz = 0;
 +
 +		if (bf && test_bit(MLX5E_SQ_STATE_BF_ENABLE, &sq->state))
 +			bf_sz = wi->num_wqebbs << 3;
 +
 +		cseg->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
 +		mlx5e_tx_notify_hw(sq, &wqe->ctrl, bf_sz);
 +	}
++=======
+ 	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
+ 		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
++>>>>>>> 864b2d715300 (net/mlx5e: Generalize tx helper functions for different SQ types)
  
  	/* fill sq edge with nops to avoid wqe wrap around */
  	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
  		sq->db.txq.skb[pi] = NULL;
- 		mlx5e_send_nop(sq, false);
+ 		mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+ 		sq->stats.nop++;
  	}
  
 +	if (bf)
 +		sq->bf_budget--;
 +
  	return NETDEV_TX_OK;
  
  dma_unmap_wqe_err:
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index edb21d8194bc..3cc0cab647e9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -819,6 +819,7 @@ static int mlx5e_open_rq(struct mlx5e_channel *c,
 {
 	struct mlx5e_sq *sq = &c->icosq;
 	u16 pi = sq->pc & sq->wq.sz_m1;
+	struct mlx5e_tx_wqe *nopwqe;
 	int err;
 
 	err = mlx5e_create_rq(c, param, rq);
@@ -839,8 +840,9 @@ static int mlx5e_open_rq(struct mlx5e_channel *c,
 
 	sq->db.ico_wqe[pi].opcode     = MLX5_OPCODE_NOP;
 	sq->db.ico_wqe[pi].num_wqebbs = 1;
-	mlx5e_send_nop(sq, true); /* trigger mlx5e_post_rx_wqes() */
-
+	nopwqe = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &nopwqe->ctrl);
+	sq->stats.nop++; /* TODO no need for SQ stats in ico */
 	return 0;
 
 err_disable_rq:
@@ -1161,9 +1163,12 @@ static void mlx5e_close_sq(struct mlx5e_sq *sq)
 		netif_tx_disable_queue(sq->txq);
 
 		/* last doorbell out, godspeed .. */
-		if (mlx5e_sq_has_room_for(sq, 1)) {
+		if (mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, 1)) {
+			struct mlx5e_tx_wqe *nop;
+
 			sq->db.txq.skb[(sq->pc & sq->wq.sz_m1)] = NULL;
-			mlx5e_send_nop(sq, true);
+			nop = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+			mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &nop->ctrl);
 		}
 	}
 
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
