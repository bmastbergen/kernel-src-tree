crypto: chcr - Ensure Destination sg entry size less than 2k

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [crypto] chcr - Ensure Destination sg entry size less than 2k (Arjun Vynipadath) [1458315]
Rebuild_FUZZ: 92.86%
commit-author Harsh Jain <harsh@chelsio.com>
commit 738bff48871902bd37c56e6c6b492bb8e37315d5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/738bff48.failed

Allocate new sg list in case received destination sg list has entry
greater that 2k.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 738bff48871902bd37c56e6c6b492bb8e37315d5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/chelsio/chcr_crypto.h
diff --cc drivers/crypto/chelsio/chcr_algo.c
index bda117371c9f,9a84ffa2adf3..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -105,25 -153,32 +105,34 @@@ int chcr_handle_resp(struct crypto_asyn
  	struct chcr_context *ctx = crypto_tfm_ctx(tfm);
  	struct uld_ctx *u_ctx = ULD_CTX(ctx);
  	struct chcr_req_ctx ctx_req;
 +	struct cpl_fw6_pld *fw6_pld;
  	unsigned int digestsize, updated_digestsize;
 -	struct adapter *adap = padap(ctx->dev);
  
  	switch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {
 -	case CRYPTO_ALG_TYPE_AEAD:
 -		ctx_req.req.aead_req = aead_request_cast(req);
 -		ctx_req.ctx.reqctx = aead_request_ctx(ctx_req.req.aead_req);
 -		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.ctx.reqctx->dst,
 -			     ctx_req.ctx.reqctx->dst_nents, DMA_FROM_DEVICE);
 -		if (ctx_req.ctx.reqctx->skb) {
 -			kfree_skb(ctx_req.ctx.reqctx->skb);
 -			ctx_req.ctx.reqctx->skb = NULL;
 +	case CRYPTO_ALG_TYPE_BLKCIPHER:
 +		ctx_req.req.ablk_req = (struct ablkcipher_request *)req;
 +		ctx_req.ctx.ablk_ctx =
 +			ablkcipher_request_ctx(ctx_req.req.ablk_req);
 +		if (!error_status) {
 +			fw6_pld = (struct cpl_fw6_pld *)input;
 +			memcpy(ctx_req.req.ablk_req->info, &fw6_pld->data[2],
 +			       AES_BLOCK_SIZE);
  		}
++<<<<<<< HEAD
 +		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.req.ablk_req->dst,
 +			     ctx_req.ctx.ablk_ctx->dst_nents, DMA_FROM_DEVICE);
 +		if (ctx_req.ctx.ablk_ctx->skb) {
 +			kfree_skb(ctx_req.ctx.ablk_ctx->skb);
 +			ctx_req.ctx.ablk_ctx->skb = NULL;
++=======
+ 		free_new_sg(ctx_req.ctx.reqctx->newdstsg);
+ 		ctx_req.ctx.reqctx->newdstsg = NULL;
+ 		if (ctx_req.ctx.reqctx->verify == VERIFY_SW) {
+ 			chcr_verify_tag(ctx_req.req.aead_req, input,
+ 					&err);
+ 			ctx_req.ctx.reqctx->verify = VERIFY_HW;
++>>>>>>> 738bff488719 (crypto: chcr - Ensure Destination sg entry size less than 2k)
  		}
 -		ctx_req.req.aead_req->base.complete(req, err);
 -		break;
 -
 -	case CRYPTO_ALG_TYPE_ABLKCIPHER:
 -		 err = chcr_handle_cipher_resp(ablkcipher_request_cast(req),
 -					       input, err);
  		break;
  
  	case CRYPTO_ALG_TYPE_AHASH:
@@@ -620,27 -781,389 +629,244 @@@ static int chcr_aes_cbc_setkey(struct c
  	ablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CBC;
  	return 0;
  badkey_err:
 -	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
 +	crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
  	ablkctx->enckey_len = 0;
 -
 -	return err;
 -}
 -
 -static int chcr_aes_ctr_setkey(struct crypto_ablkcipher *cipher,
 -				   const u8 *key,
 -				   unsigned int keylen)
 -{
 -	struct chcr_context *ctx = crypto_ablkcipher_ctx(cipher);
 -	struct ablk_ctx *ablkctx = ABLK_CTX(ctx);
 -	unsigned int ck_size, context_size;
 -	u16 alignment = 0;
 -	int err;
 -
 -	err = chcr_cipher_fallback_setkey(cipher, key, keylen);
 -	if (err)
 -		goto badkey_err;
 -	ck_size = chcr_keyctx_ck_size(keylen);
 -	alignment = (ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192) ? 8 : 0;
 -	memcpy(ablkctx->key, key, keylen);
 -	ablkctx->enckey_len = keylen;
 -	context_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +
 -			keylen + alignment) >> 4;
 -
 -	ablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,
 -						0, 0, context_size);
 -	ablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CTR;
 -
 -	return 0;
 -badkey_err:
 -	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
 -	ablkctx->enckey_len = 0;
 -
 -	return err;
 -}
 -
 -static int chcr_aes_rfc3686_setkey(struct crypto_ablkcipher *cipher,
 -				   const u8 *key,
 -				   unsigned int keylen)
 -{
 -	struct chcr_context *ctx = crypto_ablkcipher_ctx(cipher);
 -	struct ablk_ctx *ablkctx = ABLK_CTX(ctx);
 -	unsigned int ck_size, context_size;
 -	u16 alignment = 0;
 -	int err;
 -
 -	if (keylen < CTR_RFC3686_NONCE_SIZE)
 -		return -EINVAL;
 -	memcpy(ablkctx->nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),
 -	       CTR_RFC3686_NONCE_SIZE);
 -
 -	keylen -= CTR_RFC3686_NONCE_SIZE;
 -	err = chcr_cipher_fallback_setkey(cipher, key, keylen);
 -	if (err)
 -		goto badkey_err;
 -
 -	ck_size = chcr_keyctx_ck_size(keylen);
 -	alignment = (ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192) ? 8 : 0;
 -	memcpy(ablkctx->key, key, keylen);
 -	ablkctx->enckey_len = keylen;
 -	context_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +
 -			keylen + alignment) >> 4;
 -
 -	ablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,
 -						0, 0, context_size);
 -	ablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CTR;
 -
 -	return 0;
 -badkey_err:
 -	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
 -	ablkctx->enckey_len = 0;
 -
 -	return err;
 -}
 -static void ctr_add_iv(u8 *dstiv, u8 *srciv, u32 add)
 -{
 -	unsigned int size = AES_BLOCK_SIZE;
 -	__be32 *b = (__be32 *)(dstiv + size);
 -	u32 c, prev;
 -
 -	memcpy(dstiv, srciv, AES_BLOCK_SIZE);
 -	for (; size >= 4; size -= 4) {
 -		prev = be32_to_cpu(*--b);
 -		c = prev + add;
 -		*b = cpu_to_be32(c);
 -		if (prev < c)
 -			break;
 -		add = 1;
 -	}
 -
 -}
 -
 -static unsigned int adjust_ctr_overflow(u8 *iv, u32 bytes)
 -{
 -	__be32 *b = (__be32 *)(iv + AES_BLOCK_SIZE);
 -	u64 c;
 -	u32 temp = be32_to_cpu(*--b);
 -
 -	temp = ~temp;
 -	c = (u64)temp +  1; // No of block can processed withou overflow
 -	if ((bytes / AES_BLOCK_SIZE) > c)
 -		bytes = c * AES_BLOCK_SIZE;
 -	return bytes;
 -}
 -
 -static int chcr_update_tweak(struct ablkcipher_request *req, u8 *iv)
 -{
 -	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
 -	struct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);
 -	struct ablk_ctx *ablkctx = ABLK_CTX(ctx);
 -	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
 -	struct crypto_cipher *cipher;
 -	int ret, i;
 -	u8 *key;
 -	unsigned int keylen;
 -
 -	cipher = crypto_alloc_cipher("aes-generic", 0, 0);
 -	memcpy(iv, req->info, AES_BLOCK_SIZE);
 -
 -	if (IS_ERR(cipher)) {
 -		ret = -ENOMEM;
 -		goto out;
 -	}
 -	keylen = ablkctx->enckey_len / 2;
 -	key = ablkctx->key + keylen;
 -	ret = crypto_cipher_setkey(cipher, key, keylen);
 -	if (ret)
 -		goto out1;
 -
 -	crypto_cipher_encrypt_one(cipher, iv, iv);
 -	for (i = 0; i < (reqctx->processed / AES_BLOCK_SIZE); i++)
 -		gf128mul_x_ble((le128 *)iv, (le128 *)iv);
 -
 -	crypto_cipher_decrypt_one(cipher, iv, iv);
 -out1:
 -	crypto_free_cipher(cipher);
 -out:
 -	return ret;
 +	return -EINVAL;
  }
  
 -static int chcr_update_cipher_iv(struct ablkcipher_request *req,
 -				   struct cpl_fw6_pld *fw6_pld, u8 *iv)
 +static int cxgb4_is_crypto_q_full(struct net_device *dev, unsigned int idx)
  {
 -	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
 -	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
 -	int subtype = get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm));
 +	struct adapter *adap = netdev2adap(dev);
 +	struct sge_uld_txq_info *txq_info =
 +		adap->sge.uld_txq_info[CXGB4_TX_CRYPTO];
 +	struct sge_uld_txq *txq;
  	int ret = 0;
  
 -	if (subtype == CRYPTO_ALG_SUB_TYPE_CTR)
 -		ctr_add_iv(iv, req->info, (reqctx->processed /
 -			   AES_BLOCK_SIZE));
 -	else if (subtype == CRYPTO_ALG_SUB_TYPE_CTR_RFC3686)
 -		*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +
 -			CTR_RFC3686_IV_SIZE) = cpu_to_be32((reqctx->processed /
 -						AES_BLOCK_SIZE) + 1);
 -	else if (subtype == CRYPTO_ALG_SUB_TYPE_XTS)
 -		ret = chcr_update_tweak(req, iv);
 -	else if (subtype == CRYPTO_ALG_SUB_TYPE_CBC) {
 -		if (reqctx->op)
 -			sg_pcopy_to_buffer(req->src, sg_nents(req->src), iv,
 -					   16,
 -					   reqctx->processed - AES_BLOCK_SIZE);
 -		else
 -			memcpy(iv, &fw6_pld->data[2], AES_BLOCK_SIZE);
 -	}
 -
 +	local_bh_disable();
 +	txq = &txq_info->uldtxq[idx];
 +	spin_lock(&txq->sendq.lock);
 +	if (txq->full)
 +		ret = -1;
 +	spin_unlock(&txq->sendq.lock);
 +	local_bh_enable();
  	return ret;
++<<<<<<< HEAD
++=======
+ 
+ }
+ 
+ /* We need separate function for final iv because in rfc3686  Initial counter
+  * starts from 1 and buffer size of iv is 8 byte only which remains constant
+  * for subsequent update requests
+  */
+ 
+ static int chcr_final_cipher_iv(struct ablkcipher_request *req,
+ 				   struct cpl_fw6_pld *fw6_pld, u8 *iv)
+ {
+ 	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 	int subtype = get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm));
+ 	int ret = 0;
+ 
+ 	if (subtype == CRYPTO_ALG_SUB_TYPE_CTR)
+ 		ctr_add_iv(iv, req->info, (reqctx->processed /
+ 			   AES_BLOCK_SIZE));
+ 	else if (subtype == CRYPTO_ALG_SUB_TYPE_XTS)
+ 		ret = chcr_update_tweak(req, iv);
+ 	else if (subtype == CRYPTO_ALG_SUB_TYPE_CBC) {
+ 		if (reqctx->op)
+ 			sg_pcopy_to_buffer(req->src, sg_nents(req->src), iv,
+ 					   16,
+ 					   reqctx->processed - AES_BLOCK_SIZE);
+ 		else
+ 			memcpy(iv, &fw6_pld->data[2], AES_BLOCK_SIZE);
+ 
+ 	}
+ 	return ret;
+ 
+ }
+ 
+ 
+ static int chcr_handle_cipher_resp(struct ablkcipher_request *req,
+ 				   unsigned char *input, int err)
+ {
+ 	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct ablk_ctx *ablkctx = ABLK_CTX(ctx);
+ 	struct sk_buff *skb;
+ 	struct cpl_fw6_pld *fw6_pld = (struct cpl_fw6_pld *)input;
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 	struct  cipher_wr_param wrparam;
+ 	int bytes;
+ 
+ 	dma_unmap_sg(&u_ctx->lldi.pdev->dev, reqctx->dst, reqctx->dst_nents,
+ 		     DMA_FROM_DEVICE);
+ 
+ 	if (reqctx->skb) {
+ 		kfree_skb(reqctx->skb);
+ 		reqctx->skb = NULL;
+ 	}
+ 	if (err)
+ 		goto complete;
+ 
+ 	if (req->nbytes == reqctx->processed) {
+ 		err = chcr_final_cipher_iv(req, fw6_pld, req->info);
+ 		goto complete;
+ 	}
+ 
+ 	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 					    ctx->tx_qidx))) {
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {
+ 			err = -EBUSY;
+ 			goto complete;
+ 		}
+ 
+ 	}
+ 	wrparam.srcsg = scatterwalk_ffwd(reqctx->srcffwd, req->src,
+ 				       reqctx->processed);
+ 	reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, reqctx->dstsg,
+ 					 reqctx->processed);
+ 	if (!wrparam.srcsg || !reqctx->dst) {
+ 		pr_err("Input sg list length less that nbytes\n");
+ 		err = -EINVAL;
+ 		goto complete;
+ 	}
+ 	bytes = chcr_sg_ent_in_wr(wrparam.srcsg, reqctx->dst, 1,
+ 				 SPACE_LEFT(ablkctx->enckey_len),
+ 				 &wrparam.snent, &reqctx->dst_nents);
+ 	if ((bytes + reqctx->processed) >= req->nbytes)
+ 		bytes  = req->nbytes - reqctx->processed;
+ 	else
+ 		bytes = ROUND_16(bytes);
+ 	err = chcr_update_cipher_iv(req, fw6_pld, reqctx->iv);
+ 	if (err)
+ 		goto complete;
+ 
+ 	if (unlikely(bytes == 0)) {
+ 		err = chcr_cipher_fallback(ablkctx->sw_cipher,
+ 				     req->base.flags,
+ 				     wrparam.srcsg,
+ 				     reqctx->dst,
+ 				     req->nbytes - reqctx->processed,
+ 				     reqctx->iv,
+ 				     reqctx->op);
+ 		goto complete;
+ 	}
+ 
+ 	if (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==
+ 	    CRYPTO_ALG_SUB_TYPE_CTR)
+ 		bytes = adjust_ctr_overflow(reqctx->iv, bytes);
+ 	reqctx->processed += bytes;
+ 	wrparam.qid = u_ctx->lldi.rxq_ids[ctx->rx_qidx];
+ 	wrparam.req = req;
+ 	wrparam.bytes = bytes;
+ 	skb = create_cipher_wr(&wrparam);
+ 	if (IS_ERR(skb)) {
+ 		pr_err("chcr : %s : Failed to form WR. No memory\n", __func__);
+ 		err = PTR_ERR(skb);
+ 		goto complete;
+ 	}
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
+ 	chcr_send_wr(skb);
+ 	return 0;
+ complete:
+ 	free_new_sg(reqctx->newdstsg);
+ 	reqctx->newdstsg = NULL;
+ 	req->base.complete(&req->base, err);
+ 	return err;
+ }
+ 
+ static int process_cipher(struct ablkcipher_request *req,
+ 				  unsigned short qid,
+ 				  struct sk_buff **skb,
+ 				  unsigned short op_type)
+ {
+ 	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+ 	unsigned int ivsize = crypto_ablkcipher_ivsize(tfm);
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 	struct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);
+ 	struct ablk_ctx *ablkctx = ABLK_CTX(ctx);
+ 	struct	cipher_wr_param wrparam;
+ 	int bytes, nents, err = -EINVAL;
+ 
+ 	reqctx->newdstsg = NULL;
+ 	reqctx->processed = 0;
+ 	if (!req->info)
+ 		goto error;
+ 	if ((ablkctx->enckey_len == 0) || (ivsize > AES_BLOCK_SIZE) ||
+ 	    (req->nbytes == 0) ||
+ 	    (req->nbytes % crypto_ablkcipher_blocksize(tfm))) {
+ 		pr_err("AES: Invalid value of Key Len %d nbytes %d IV Len %d\n",
+ 		       ablkctx->enckey_len, req->nbytes, ivsize);
+ 		goto error;
+ 	}
+ 	wrparam.srcsg = req->src;
+ 	if (is_newsg(req->dst, &nents)) {
+ 		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
+ 		if (IS_ERR(reqctx->newdstsg))
+ 			return PTR_ERR(reqctx->newdstsg);
+ 		reqctx->dstsg = reqctx->newdstsg;
+ 	} else {
+ 		reqctx->dstsg = req->dst;
+ 	}
+ 	bytes = chcr_sg_ent_in_wr(wrparam.srcsg, reqctx->dstsg, MIN_CIPHER_SG,
+ 				 SPACE_LEFT(ablkctx->enckey_len),
+ 				 &wrparam.snent,
+ 				 &reqctx->dst_nents);
+ 	if ((bytes + reqctx->processed) >= req->nbytes)
+ 		bytes  = req->nbytes - reqctx->processed;
+ 	else
+ 		bytes = ROUND_16(bytes);
+ 	if (unlikely(bytes > req->nbytes))
+ 		bytes = req->nbytes;
+ 	if (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==
+ 				  CRYPTO_ALG_SUB_TYPE_CTR) {
+ 		bytes = adjust_ctr_overflow(req->info, bytes);
+ 	}
+ 	if (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==
+ 	    CRYPTO_ALG_SUB_TYPE_CTR_RFC3686) {
+ 		memcpy(reqctx->iv, ablkctx->nonce, CTR_RFC3686_NONCE_SIZE);
+ 		memcpy(reqctx->iv + CTR_RFC3686_NONCE_SIZE, req->info,
+ 				CTR_RFC3686_IV_SIZE);
+ 
+ 		/* initialize counter portion of counter block */
+ 		*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +
+ 			CTR_RFC3686_IV_SIZE) = cpu_to_be32(1);
+ 
+ 	} else {
+ 
+ 		memcpy(reqctx->iv, req->info, ivsize);
+ 	}
+ 	if (unlikely(bytes == 0)) {
+ 		err = chcr_cipher_fallback(ablkctx->sw_cipher,
+ 					   req->base.flags,
+ 					   req->src,
+ 					   req->dst,
+ 					   req->nbytes,
+ 					   req->info,
+ 					   op_type);
+ 		goto error;
+ 	}
+ 	reqctx->processed = bytes;
+ 	reqctx->dst = reqctx->dstsg;
+ 	reqctx->op = op_type;
+ 	wrparam.qid = qid;
+ 	wrparam.req = req;
+ 	wrparam.bytes = bytes;
+ 	*skb = create_cipher_wr(&wrparam);
+ 	if (IS_ERR(*skb)) {
+ 		err = PTR_ERR(*skb);
+ 		goto error;
+ 	}
+ 
+ 	return 0;
+ error:
+ 	free_new_sg(reqctx->newdstsg);
+ 	reqctx->newdstsg = NULL;
+ 	return err;
++>>>>>>> 738bff488719 (crypto: chcr - Ensure Destination sg entry size less than 2k)
  }
  
  static int chcr_aes_encrypt(struct ablkcipher_request *req)
@@@ -1260,6 -1821,1350 +1486,1353 @@@ static void chcr_hmac_cra_exit(struct c
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int is_newsg(struct scatterlist *sgl, unsigned int *newents)
+ {
+ 	int nents = 0;
+ 	int ret = 0;
+ 
+ 	while (sgl) {
+ 		if (sgl->length > CHCR_SG_SIZE)
+ 			ret = 1;
+ 		nents += DIV_ROUND_UP(sgl->length, CHCR_SG_SIZE);
+ 		sgl = sg_next(sgl);
+ 	}
+ 	*newents = nents;
+ 	return ret;
+ }
+ 
+ static inline void free_new_sg(struct scatterlist *sgl)
+ {
+ 	kfree(sgl);
+ }
+ 
+ static struct scatterlist *alloc_new_sg(struct scatterlist *sgl,
+ 				       unsigned int nents)
+ {
+ 	struct scatterlist *newsg, *sg;
+ 	int i, len, processed = 0;
+ 	struct page *spage;
+ 	int offset;
+ 
+ 	newsg = kmalloc_array(nents, sizeof(struct scatterlist), GFP_KERNEL);
+ 	if (!newsg)
+ 		return ERR_PTR(-ENOMEM);
+ 	sg = newsg;
+ 	sg_init_table(sg, nents);
+ 	offset = sgl->offset;
+ 	spage = sg_page(sgl);
+ 	for (i = 0; i < nents; i++) {
+ 		len = min_t(u32, sgl->length - processed, CHCR_SG_SIZE);
+ 		sg_set_page(sg, spage, len, offset);
+ 		processed += len;
+ 		offset += len;
+ 		if (offset >= PAGE_SIZE) {
+ 			offset = offset % PAGE_SIZE;
+ 			spage++;
+ 		}
+ 		if (processed == sgl->length) {
+ 			processed = 0;
+ 			sgl = sg_next(sgl);
+ 			if (!sgl)
+ 				break;
+ 			spage = sg_page(sgl);
+ 			offset = sgl->offset;
+ 		}
+ 		sg = sg_next(sg);
+ 	}
+ 	return newsg;
+ }
+ 
+ static int chcr_copy_assoc(struct aead_request *req,
+ 				struct chcr_aead_ctx *ctx)
+ {
+ 	SKCIPHER_REQUEST_ON_STACK(skreq, ctx->null);
+ 
+ 	skcipher_request_set_tfm(skreq, ctx->null);
+ 	skcipher_request_set_callback(skreq, aead_request_flags(req),
+ 			NULL, NULL);
+ 	skcipher_request_set_crypt(skreq, req->src, req->dst, req->assoclen,
+ 			NULL);
+ 
+ 	return crypto_skcipher_encrypt(skreq);
+ }
+ static int chcr_aead_need_fallback(struct aead_request *req, int src_nent,
+ 				   int aadmax, int wrlen,
+ 				   unsigned short op_type)
+ {
+ 	unsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));
+ 
+ 	if (((req->cryptlen - (op_type ? authsize : 0)) == 0) ||
+ 	    (req->assoclen > aadmax) ||
+ 	    (src_nent > MAX_SKB_FRAGS) ||
+ 	    (wrlen > MAX_WR_SIZE))
+ 		return 1;
+ 	return 0;
+ }
+ 
+ static int chcr_aead_fallback(struct aead_request *req, unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 
+ 	aead_request_set_tfm(subreq, aeadctx->sw_cipher);
+ 	aead_request_set_callback(subreq, req->base.flags,
+ 				  req->base.complete, req->base.data);
+ 	 aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 				 req->iv);
+ 	 aead_request_set_ad(subreq, req->assoclen);
+ 	return op_type ? crypto_aead_decrypt(subreq) :
+ 		crypto_aead_encrypt(subreq);
+ }
+ 
+ static struct sk_buff *create_authenc_wr(struct aead_request *req,
+ 					 unsigned short qid,
+ 					 int size,
+ 					 unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = crypto_aead_ivsize(tfm), dst_size = 0;
+ 	unsigned int   kctx_len = 0, nents;
+ 	unsigned short stop_offset = 0;
+ 	unsigned int  assoclen = req->assoclen;
+ 	unsigned int  authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	int null = 0;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 	struct adapter *adap = padap(ctx->dev);
+ 
+ 	reqctx->newdstsg = NULL;
+ 	dst_size = req->assoclen + req->cryptlen + (op_type ? -authsize :
+ 						   authsize);
+ 	if (aeadctx->enckey_len == 0 || (req->cryptlen <= 0))
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error)
+ 			return ERR_PTR(error);
+ 	}
+ 	if (dst_size && is_newsg(req->dst, &nents)) {
+ 		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
+ 		if (IS_ERR(reqctx->newdstsg))
+ 			return ERR_CAST(reqctx->newdstsg);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,
+ 					       reqctx->newdstsg, req->assoclen);
+ 	} else {
+ 		if (req->src == req->dst)
+ 			reqctx->dst = src;
+ 		else
+ 			reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,
+ 						       req->dst, req->assoclen);
+ 	}
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {
+ 		null = 1;
+ 		assoclen = 0;
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("AUTHENC:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)
+ 		- sizeof(chcr_req->key_ctx);
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_AUTH_SG,
+ 			T6_MAX_AAD_SIZE,
+ 			transhdr_len + (sgl_len(src_nent + MIN_AUTH_SG) * 8),
+ 				op_type)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		free_new_sg(reqctx->newdstsg);
+ 		reqctx->newdstsg = NULL;
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	/* LLD is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	/* Write WR */
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	stop_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 
+ 	/*
+ 	 * Input order	is AAD,IV and Payload. where IV should be included as
+ 	 * the part of authdata. All other fields should be filled according
+ 	 * to the hardware spec
+ 	 */
+ 	chcr_req->sec_cpl.op_ivinsrtofst =
+ 		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,
+ 				       (ivsize ? (assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1,
+ 					(stop_offset & 0x1F0) >> 4);
+ 	chcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(
+ 					stop_offset & 0xF,
+ 					null ? 0 : assoclen + ivsize + 1,
+ 					stop_offset, stop_offset);
+ 	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_CBC,
+ 					actx->auth_mode, aeadctx->hmac_ctrl,
+ 					ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					 0, 1, dst_size);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	if (op_type == CHCR_ENCRYPT_OP)
+ 		memcpy(chcr_req->key_ctx.key, aeadctx->key,
+ 		       aeadctx->enckey_len);
+ 	else
+ 		memcpy(chcr_req->key_ctx.key, actx->dec_rrkey,
+ 		       aeadctx->enckey_len);
+ 
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<
+ 					4), actx->h_iopad, kctx_len -
+ 				(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 					reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 
+ 	if (assoclen) {
+ 		/* AAD buffer in */
+ 		write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 
+ 	}
+ 	write_buffer_to_skb(skb, &frags, req->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	atomic_inc(&adap->chcr_stats.cipher_rqst);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
+ 		   sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 
+ 	return skb;
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	free_new_sg(reqctx->newdstsg);
+ 	reqctx->newdstsg = NULL;
+ 	return ERR_PTR(error);
+ }
+ 
+ static int set_msg_len(u8 *block, unsigned int msglen, int csize)
+ {
+ 	__be32 data;
+ 
+ 	memset(block, 0, csize);
+ 	block += csize;
+ 
+ 	if (csize >= 4)
+ 		csize = 4;
+ 	else if (msglen > (unsigned int)(1 << (8 * csize)))
+ 		return -EOVERFLOW;
+ 
+ 	data = cpu_to_be32(msglen);
+ 	memcpy(block - csize, (u8 *)&data + 4 - csize, csize);
+ 
+ 	return 0;
+ }
+ 
+ static void generate_b0(struct aead_request *req,
+ 			struct chcr_aead_ctx *aeadctx,
+ 			unsigned short op_type)
+ {
+ 	unsigned int l, lp, m;
+ 	int rc;
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	u8 *b0 = reqctx->scratch_pad;
+ 
+ 	m = crypto_aead_authsize(aead);
+ 
+ 	memcpy(b0, reqctx->iv, 16);
+ 
+ 	lp = b0[0];
+ 	l = lp + 1;
+ 
+ 	/* set m, bits 3-5 */
+ 	*b0 |= (8 * ((m - 2) / 2));
+ 
+ 	/* set adata, bit 6, if associated data is used */
+ 	if (req->assoclen)
+ 		*b0 |= 64;
+ 	rc = set_msg_len(b0 + 16 - l,
+ 			 (op_type == CHCR_DECRYPT_OP) ?
+ 			 req->cryptlen - m : req->cryptlen, l);
+ }
+ 
+ static inline int crypto_ccm_check_iv(const u8 *iv)
+ {
+ 	/* 2 <= L <= 8, so 1 <= L' <= 7. */
+ 	if (iv[0] < 1 || iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int ccm_format_packet(struct aead_request *req,
+ 			     struct chcr_aead_ctx *aeadctx,
+ 			     unsigned int sub_type,
+ 			     unsigned short op_type)
+ {
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int rc = 0;
+ 
+ 	if (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		reqctx->iv[0] = 3;
+ 		memcpy(reqctx->iv + 1, &aeadctx->salt[0], 3);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 		memset(reqctx->iv + 12, 0, 4);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen - 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 16);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen);
+ 	}
+ 	generate_b0(req, aeadctx, op_type);
+ 	/* zero the ctr value */
+ 	memset(reqctx->iv + 15 - reqctx->iv[0], 0, reqctx->iv[0] + 1);
+ 	return rc;
+ }
+ 
+ static void fill_sec_cpl_for_aead(struct cpl_tx_sec_pdu *sec_cpl,
+ 				  unsigned int dst_size,
+ 				  struct aead_request *req,
+ 				  unsigned short op_type,
+ 					  struct chcr_context *chcrctx)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int cipher_mode = CHCR_SCMD_CIPHER_MODE_AES_CCM;
+ 	unsigned int mac_mode = CHCR_SCMD_AUTH_MODE_CBCMAC;
+ 	unsigned int c_id = chcrctx->dev->rx_channel_id;
+ 	unsigned int ccm_xtra;
+ 	unsigned char tag_offset = 0, auth_offset = 0;
+ 	unsigned int assoclen;
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 		assoclen = req->assoclen - 8;
+ 	else
+ 		assoclen = req->assoclen;
+ 	ccm_xtra = CCM_B0_SIZE +
+ 		((assoclen) ? CCM_AAD_FIELD_SIZE : 0);
+ 
+ 	auth_offset = req->cryptlen ?
+ 		(assoclen + ivsize + 1 + ccm_xtra) : 0;
+ 	if (op_type == CHCR_DECRYPT_OP) {
+ 		if (crypto_aead_authsize(tfm) != req->cryptlen)
+ 			tag_offset = crypto_aead_authsize(tfm);
+ 		else
+ 			auth_offset = 0;
+ 	}
+ 
+ 
+ 	sec_cpl->op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(c_id,
+ 					 2, (ivsize ?  (assoclen + 1) :  0) +
+ 					 ccm_xtra);
+ 	sec_cpl->pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen + ccm_xtra);
+ 	/* For CCM there wil be b0 always. So AAD start will be 1 always */
+ 	sec_cpl->aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					1, assoclen + ccm_xtra, assoclen
+ 					+ ivsize + 1 + ccm_xtra, 0);
+ 
+ 	sec_cpl->cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(0,
+ 					auth_offset, tag_offset,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 :
+ 					crypto_aead_authsize(tfm));
+ 	sec_cpl->seqno_numivs =  FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 : 1,
+ 					cipher_mode, mac_mode,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 
+ 	sec_cpl->ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1, 0,
+ 					1, dst_size);
+ }
+ 
+ int aead_ccm_validate_input(unsigned short op_type,
+ 			    struct aead_request *req,
+ 			    struct chcr_aead_ctx *aeadctx,
+ 			    unsigned int sub_type)
+ {
+ 	if (sub_type != CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		if (crypto_ccm_check_iv(req->iv)) {
+ 			pr_err("CCM: IV check fails\n");
+ 			return -EINVAL;
+ 		}
+ 	} else {
+ 		if (req->assoclen != 16 && req->assoclen != 20) {
+ 			pr_err("RFC4309: Invalid AAD length %d\n",
+ 			       req->assoclen);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	if (aeadctx->enckey_len == 0) {
+ 		pr_err("CCM: Encryption key not set\n");
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ unsigned int fill_aead_req_fields(struct sk_buff *skb,
+ 				  struct aead_request *req,
+ 				  struct scatterlist *src,
+ 				  unsigned int ivsize,
+ 				  struct chcr_aead_ctx *aeadctx)
+ {
+ 	unsigned int frags = 0;
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	/* b0 and aad length(if available) */
+ 
+ 	write_buffer_to_skb(skb, &frags, reqctx->scratch_pad, CCM_B0_SIZE +
+ 				(req->assoclen ?  CCM_AAD_FIELD_SIZE : 0));
+ 	if (req->assoclen) {
+ 		if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 			write_sg_to_skb(skb, &frags, req->src,
+ 					req->assoclen - 8);
+ 		else
+ 			write_sg_to_skb(skb, &frags, req->src, req->assoclen);
+ 	}
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	if (req->cryptlen)
+ 		write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 
+ 	return frags;
+ }
+ 
+ static struct sk_buff *create_aead_ccm_wr(struct aead_request *req,
+ 					  unsigned short qid,
+ 					  int size,
+ 					  unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len, ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len, nents;
+ 	unsigned int sub_type;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 	struct adapter *adap = padap(ctx->dev);
+ 
+ 	dst_size = req->assoclen + req->cryptlen + (op_type ? -authsize :
+ 						   authsize);
+ 	reqctx->newdstsg = NULL;
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	sub_type = get_aead_subtype(tfm);
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error) {
+ 			pr_err("AAD copy to destination buffer fails\n");
+ 			return ERR_PTR(error);
+ 		}
+ 	}
+ 	if (dst_size && is_newsg(req->dst, &nents)) {
+ 		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
+ 		if (IS_ERR(reqctx->newdstsg))
+ 			return ERR_CAST(reqctx->newdstsg);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,
+ 					       reqctx->newdstsg, req->assoclen);
+ 	} else {
+ 		if (req->src == req->dst)
+ 			reqctx->dst = src;
+ 		else
+ 			reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,
+ 						       req->dst, req->assoclen);
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("CCM:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 	error = aead_ccm_validate_input(op_type, req, aeadctx, sub_type);
+ 	if (error)
+ 		goto err;
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) * 2;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_CCM_SG,
+ 			    T6_MAX_AAD_SIZE - 18,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_CCM_SG) * 8),
+ 			    op_type)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		free_new_sg(reqctx->newdstsg);
+ 		reqctx->newdstsg = NULL;
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)),  flags);
+ 
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	fill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, op_type, ctx);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 					16), aeadctx->key, aeadctx->enckey_len);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	error = ccm_format_packet(req, aeadctx, sub_type, op_type);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 				 reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	frags = fill_aead_req_fields(skb, req, src, ivsize, aeadctx);
+ 	atomic_inc(&adap->chcr_stats.aead_rqst);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, 0, 1,
+ 		    sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ dstmap_fail:
+ 	kfree_skb(skb);
+ err:
+ 	free_new_sg(reqctx->newdstsg);
+ 	reqctx->newdstsg = NULL;
+ 	return ERR_PTR(error);
+ }
+ 
+ static struct sk_buff *create_gcm_wr(struct aead_request *req,
+ 				     unsigned short qid,
+ 				     int size,
+ 				     unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len, nents, assoclen = req->assoclen;
+ 	unsigned char tag_offset = 0;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 	struct adapter *adap = padap(ctx->dev);
+ 
+ 	reqctx->newdstsg = NULL;
+ 	dst_size = assoclen + req->cryptlen + (op_type ? -authsize :
+ 						    authsize);
+ 	/* validate key size */
+ 	if (aeadctx->enckey_len == 0)
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, assoclen);
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error)
+ 			return	ERR_PTR(error);
+ 	}
+ 
+ 	if (dst_size && is_newsg(req->dst, &nents)) {
+ 		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
+ 		if (IS_ERR(reqctx->newdstsg))
+ 			return ERR_CAST(reqctx->newdstsg);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,
+ 					       reqctx->newdstsg, assoclen);
+ 	} else {
+ 		if (req->src == req->dst)
+ 			reqctx->dst = src;
+ 		else
+ 			reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,
+ 						       req->dst, assoclen);
+ 	}
+ 
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("GCM:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_GCM_SG,
+ 			    T6_MAX_AAD_SIZE,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_GCM_SG) * 8),
+ 			    op_type)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		free_new_sg(reqctx->newdstsg);
+ 		reqctx->newdstsg = NULL;
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	/* NIC driver is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)
+ 		assoclen = req->assoclen - 8;
+ 
+ 	tag_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 	chcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(
+ 					ctx->dev->rx_channel_id, 2, (ivsize ?
+ 					(assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1, 0);
+ 		chcr_req->sec_cpl.cipherstop_lo_authinsert =
+ 			FILL_SEC_CPL_AUTHINSERT(0, assoclen + ivsize + 1,
+ 						tag_offset, tag_offset);
+ 		chcr_req->sec_cpl.seqno_numivs =
+ 			FILL_SEC_CPL_SCMD0_SEQNO(op_type, (op_type ==
+ 					CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_GCM,
+ 					CHCR_SCMD_AUTH_MODE_GHASH,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					0, 1, dst_size);
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 				16), GCM_CTX(aeadctx)->ghash_h, AEAD_H_SIZE);
+ 
+ 	/* prepare a 16 byte iv */
+ 	/* S   A   L  T |  IV | 0x00000001 */
+ 	if (get_aead_subtype(tfm) ==
+ 	    CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) {
+ 		memcpy(reqctx->iv, aeadctx->salt, 4);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 12);
+ 	}
+ 	*((unsigned int *)(reqctx->iv + 12)) = htonl(0x01);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 					  reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	atomic_inc(&adap->chcr_stats.aead_rqst);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
+ 			sizeof(struct cpl_rx_phys_dsgl) + dst_size,
+ 			reqctx->verify);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ 
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	free_new_sg(reqctx->newdstsg);
+ 	reqctx->newdstsg = NULL;
+ 	return ERR_PTR(error);
+ }
+ 
+ 
+ 
+ static int chcr_aead_cra_init(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_alg *alg = crypto_aead_alg(tfm);
+ 
+ 	aeadctx->sw_cipher = crypto_alloc_aead(alg->base.cra_name, 0,
+ 					       CRYPTO_ALG_NEED_FALLBACK |
+ 					       CRYPTO_ALG_ASYNC);
+ 	if  (IS_ERR(aeadctx->sw_cipher))
+ 		return PTR_ERR(aeadctx->sw_cipher);
+ 	crypto_aead_set_reqsize(tfm, max(sizeof(struct chcr_aead_reqctx),
+ 				 sizeof(struct aead_request) +
+ 				 crypto_aead_reqsize(aeadctx->sw_cipher)));
+ 	aeadctx->null = crypto_get_default_null_skcipher();
+ 	if (IS_ERR(aeadctx->null))
+ 		return PTR_ERR(aeadctx->null);
+ 	return chcr_device_init(ctx);
+ }
+ 
+ static void chcr_aead_cra_exit(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 
+ 	crypto_put_default_null_skcipher();
+ 	crypto_free_aead(aeadctx->sw_cipher);
+ }
+ 
+ static int chcr_authenc_null_setauthsize(struct crypto_aead *tfm,
+ 					unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NOP;
+ 	aeadctx->mayverify = VERIFY_HW;
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ static int chcr_authenc_setauthsize(struct crypto_aead *tfm,
+ 				    unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	u32 maxauth = crypto_aead_maxauthsize(tfm);
+ 
+ 	/*SHA1 authsize in ipsec is 12 instead of 10 i.e maxauthsize / 2 is not
+ 	 * true for sha1. authsize == 12 condition should be before
+ 	 * authsize == (maxauth >> 1)
+ 	 */
+ 	if (authsize == ICV_4) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_6) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_10) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_12) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_14) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == (maxauth >> 1)) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == maxauth) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ 
+ static int chcr_gcm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_13:
+ 	case ICV_15:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 		break;
+ 	default:
+ 
+ 		  crypto_tfm_set_flags((struct crypto_tfm *) tfm,
+ 			CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_4106_4309_setauthsize(struct crypto_aead *tfm,
+ 					  unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_setauthsize(struct crypto_aead *tfm,
+ 				unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_6:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_10:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_common_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	unsigned char ck_size, mk_size;
+ 	int key_ctx_size = 0;
+ 
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4)  * 2;
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		mk_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, mk_size, 0, 0,
+ 						key_ctx_size >> 4);
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 
+ 	return 0;
+ }
+ 
+ static int chcr_aead_ccm_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	int error;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &
+ 			      CRYPTO_TFM_REQ_MASK);
+ 	error = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (error)
+ 		return error;
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_aead_rfc4309_setkey(struct crypto_aead *aead, const u8 *key,
+ 				    unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	int error;
+ 
+ 	if (keylen < 3) {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &
+ 			      CRYPTO_TFM_REQ_MASK);
+ 	error = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (error)
+ 		return error;
+ 	keylen -= 3;
+ 	memcpy(aeadctx->salt, key + keylen, 3);
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_gcm_setkey(struct crypto_aead *aead, const u8 *key,
+ 			   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_gcm_ctx *gctx = GCM_CTX(aeadctx);
+ 	struct crypto_cipher *cipher;
+ 	unsigned int ck_size;
+ 	int ret = 0, key_ctx_size = 0;
+ 
+ 	aeadctx->enckey_len = 0;
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (get_aead_subtype(aead) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&
+ 	    keylen > 3) {
+ 		keylen -= 4;  /* nonce/salt is present in the last 4 bytes */
+ 		memcpy(aeadctx->salt, key + keylen, 4);
+ 	}
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		pr_err("GCM: Invalid key length %d\n", keylen);
+ 		ret = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,
+ 						CHCR_KEYCTX_MAC_KEY_SIZE_128,
+ 						0, 0,
+ 						key_ctx_size >> 4);
+ 	/* Calculate the H = CIPH(K, 0 repeated 16 times).
+ 	 * It will go in key context
+ 	 */
+ 	cipher = crypto_alloc_cipher("aes-generic", 0, 0);
+ 	if (IS_ERR(cipher)) {
+ 		aeadctx->enckey_len = 0;
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	ret = crypto_cipher_setkey(cipher, key, keylen);
+ 	if (ret) {
+ 		aeadctx->enckey_len = 0;
+ 		goto out1;
+ 	}
+ 	memset(gctx->ghash_h, 0, AEAD_H_SIZE);
+ 	crypto_cipher_encrypt_one(cipher, gctx->ghash_h, gctx->ghash_h);
+ 
+ out1:
+ 	crypto_free_cipher(cipher);
+ out:
+ 	return ret;
+ }
+ 
+ static int chcr_authenc_setkey(struct crypto_aead *authenc, const u8 *key,
+ 				   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	/* it contains auth and cipher key both*/
+ 	struct crypto_authenc_keys keys;
+ 	unsigned int bs;
+ 	unsigned int max_authsize = crypto_aead_alg(authenc)->maxauthsize;
+ 	int err = 0, i, key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 	unsigned char pad[CHCR_HASH_MAX_BLOCK_SIZE_128] = { 0 };
+ 	struct crypto_shash *base_hash = ERR_PTR(-EINVAL);
+ 	struct algo_param param;
+ 	int align;
+ 	u8 *o_ptr = NULL;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 
+ 	if (get_alg_config(&param, max_authsize)) {
+ 		pr_err("chcr : Unsupported digest size\n");
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 
+ 	/* Copy only encryption key. We use authkey to generate h(ipad) and
+ 	 * h(opad) so authkey is not needed again. authkeylen size have the
+ 	 * size of the hash digest size.
+ 	 */
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 			    aeadctx->enckey_len << 3);
+ 
+ 	base_hash  = chcr_alloc_shash(max_authsize);
+ 	if (IS_ERR(base_hash)) {
+ 		pr_err("chcr : Base driver cannot be loaded\n");
+ 		aeadctx->enckey_len = 0;
+ 		return -EINVAL;
+ 	}
+ 	{
+ 		SHASH_DESC_ON_STACK(shash, base_hash);
+ 		shash->tfm = base_hash;
+ 		shash->flags = crypto_shash_get_flags(base_hash);
+ 		bs = crypto_shash_blocksize(base_hash);
+ 		align = KEYCTX_ALIGN_PAD(max_authsize);
+ 		o_ptr =  actx->h_iopad + param.result_size + align;
+ 
+ 		if (keys.authkeylen > bs) {
+ 			err = crypto_shash_digest(shash, keys.authkey,
+ 						  keys.authkeylen,
+ 						  o_ptr);
+ 			if (err) {
+ 				pr_err("chcr : Base driver cannot be loaded\n");
+ 				goto out;
+ 			}
+ 			keys.authkeylen = max_authsize;
+ 		} else
+ 			memcpy(o_ptr, keys.authkey, keys.authkeylen);
+ 
+ 		/* Compute the ipad-digest*/
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= IPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, actx->h_iopad,
+ 					      max_authsize))
+ 			goto out;
+ 		/* Compute the opad-digest */
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= OPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, o_ptr, max_authsize))
+ 			goto out;
+ 
+ 		/* convert the ipad and opad digest to network order */
+ 		chcr_change_order(actx->h_iopad, param.result_size);
+ 		chcr_change_order(o_ptr, param.result_size);
+ 		key_ctx_len = sizeof(struct _key_ctx) +
+ 			((DIV_ROUND_UP(keys.enckeylen, 16)) << 4) +
+ 			(param.result_size + align) * 2;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, param.mk_size,
+ 						0, 1, key_ctx_len >> 4);
+ 		actx->auth_mode = param.auth_mode;
+ 		chcr_free_shash(base_hash);
+ 
+ 		return 0;
+ 	}
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	if (!IS_ERR(base_hash))
+ 		chcr_free_shash(base_hash);
+ 	return -EINVAL;
+ }
+ 
+ static int chcr_aead_digest_null_setkey(struct crypto_aead *authenc,
+ 					const u8 *key, unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct crypto_authenc_keys keys;
+ 	int err;
+ 	/* it contains auth and cipher key both*/
+ 	int key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 				    aeadctx->enckey_len << 3);
+ 	key_ctx_len =  sizeof(struct _key_ctx)
+ 		+ ((DIV_ROUND_UP(keys.enckeylen, 16)) << 4);
+ 
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY, 0,
+ 						0, key_ctx_len >> 4);
+ 	actx->auth_mode = CHCR_SCMD_AUTH_MODE_NOP;
+ 	return 0;
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	return -EINVAL;
+ }
+ static int chcr_aead_encrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 
+ 	reqctx->verify = VERIFY_HW;
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int size;
+ 
+ 	if (aeadctx->mayverify == VERIFY_SW) {
+ 		size = crypto_aead_maxauthsize(tfm);
+ 		reqctx->verify = VERIFY_SW;
+ 	} else {
+ 		size = 0;
+ 		reqctx->verify = VERIFY_HW;
+ 	}
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_op(struct aead_request *req,
+ 			  unsigned short op_type,
+ 			  int size,
+ 			  create_wr_t create_wr_fn)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx;
+ 	struct sk_buff *skb;
+ 
+ 	if (!ctx->dev) {
+ 		pr_err("chcr : %s : No crypto device.\n", __func__);
+ 		return -ENXIO;
+ 	}
+ 	u_ctx = ULD_CTX(ctx);
+ 	if (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 				   ctx->tx_qidx)) {
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
+ 			return -EBUSY;
+ 	}
+ 
+ 	/* Form a WR from req */
+ 	skb = create_wr_fn(req, u_ctx->lldi.rxq_ids[ctx->rx_qidx], size,
+ 			   op_type);
+ 
+ 	if (IS_ERR(skb) || !skb)
+ 		return PTR_ERR(skb);
+ 
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
+ 	chcr_send_wr(skb);
+ 	return -EINPROGRESS;
+ }
++>>>>>>> 738bff488719 (crypto: chcr - Ensure Destination sg entry size less than 2k)
  static struct chcr_alg_template driver_algs[] = {
  	/* AES-CBC */
  	{
diff --cc drivers/crypto/chelsio/chcr_crypto.h
index c00250405824,a4f95b014b46..000000000000
--- a/drivers/crypto/chelsio/chcr_crypto.h
+++ b/drivers/crypto/chelsio/chcr_crypto.h
@@@ -120,12 -154,55 +121,56 @@@
  /* Aligned to 128 bit boundary */
  
  struct ablk_ctx {
 -	struct crypto_skcipher *sw_cipher;
  	__be32 key_ctx_hdr;
  	unsigned int enckey_len;
 -	unsigned char ciph_mode;
  	u8 key[CHCR_AES_MAX_KEY_LEN];
 -	u8 nonce[4];
 +	unsigned char ciph_mode;
  	u8 rrkey[AES_MAX_KEY_SIZE];
  };
++<<<<<<< HEAD
++=======
+ struct chcr_aead_reqctx {
+ 	struct	sk_buff	*skb;
+ 	struct scatterlist *dst;
+ 	struct scatterlist *newdstsg;
+ 	struct scatterlist srcffwd[2];
+ 	struct scatterlist dstffwd[2];
+ 	short int dst_nents;
+ 	u16 verify;
+ 	u8 iv[CHCR_MAX_CRYPTO_IV_LEN];
+ 	unsigned char scratch_pad[MAX_SCRATCH_PAD_SIZE];
+ };
+ 
+ struct chcr_gcm_ctx {
+ 	u8 ghash_h[AEAD_H_SIZE];
+ };
+ 
+ struct chcr_authenc_ctx {
+ 	u8 dec_rrkey[AES_MAX_KEY_SIZE];
+ 	u8 h_iopad[2 * CHCR_HASH_MAX_DIGEST_SIZE];
+ 	unsigned char auth_mode;
+ };
+ 
+ struct __aead_ctx {
+ 	struct chcr_gcm_ctx gcm[0];
+ 	struct chcr_authenc_ctx authenc[0];
+ };
+ 
+ 
+ 
+ struct chcr_aead_ctx {
+ 	__be32 key_ctx_hdr;
+ 	unsigned int enckey_len;
+ 	struct crypto_skcipher *null;
+ 	struct crypto_aead *sw_cipher;
+ 	u8 salt[MAX_SALT];
+ 	u8 key[CHCR_AES_MAX_KEY_LEN];
+ 	u16 hmac_ctrl;
+ 	u16 mayverify;
+ 	struct	__aead_ctx ctx[0];
+ };
+ 
++>>>>>>> 738bff488719 (crypto: chcr - Ensure Destination sg entry size less than 2k)
  
  
  struct hmac_ctx {
@@@ -191,9 -279,20 +236,23 @@@ struct sge_opaque_hdr 
  	dma_addr_t addr[MAX_SKB_FRAGS + 1];
  };
  
 -typedef struct sk_buff *(*create_wr_t)(struct aead_request *req,
 +typedef struct sk_buff *(*create_wr_t)(struct crypto_async_request *req,
 +				       struct chcr_context *ctx,
  				       unsigned short qid,
 -				       int size,
  				       unsigned short op_type);
  
++<<<<<<< HEAD
++=======
+ static int chcr_aead_op(struct aead_request *req_base,
+ 			  unsigned short op_type,
+ 			  int size,
+ 			  create_wr_t create_wr_fn);
+ static inline int get_aead_subtype(struct crypto_aead *aead);
+ static int is_newsg(struct scatterlist *sgl, unsigned int *newents);
+ static struct scatterlist *alloc_new_sg(struct scatterlist *sgl,
+ 					unsigned int nents);
+ static inline void free_new_sg(struct scatterlist *sgl);
+ static int chcr_handle_cipher_resp(struct ablkcipher_request *req,
+ 				   unsigned char *input, int err);
++>>>>>>> 738bff488719 (crypto: chcr - Ensure Destination sg entry size less than 2k)
  #endif /* __CHCR_CRYPTO_H__ */
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/chelsio/chcr_crypto.h
