cpufreq: intel_pstate: One set of global limits in active mode

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: One set of global limits in active mode (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 92.17%
commit-author Rafael J. Wysocki <rafael.j.wysocki@intel.com>
commit 7de32556dfc62b9e1203730cc26b71292da8a244
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7de32556.failed

In the active mode intel_pstate currently uses two sets of global
limits, each associated with one of the possible scaling_governor
settings in that mode: "powersave" or "performance".

The driver switches over from one of those sets to the other
depending on the scaling_governor setting for the last CPU whose
per-policy cpufreq interface in sysfs was last used to change
parameters exposed in there.  That obviously leads to no end of
issues when the scaling_governor settings differ between CPUs.

The most recent issue was introduced by commit a240c4aa5d0f (cpufreq:
intel_pstate: Do not reinit performance limits in ->setpolicy)
that eliminated the reinitialization of "performance" limits in
intel_pstate_set_policy() preventing the max limit from being set
to anything below 100, among other things.

Namely, an undesirable side effect of commit a240c4aa5d0f is that
now, after setting scaling_governor to "performance" in the active
mode, the per-policy limits for the CPU in question go to the highest
level and stay there even when it is switched back to "powersave"
later.

As it turns out, some distributions set scaling_governor to
"performance" temporarily for all CPUs to speed-up system
initialization, so that change causes them to misbehave later.

To fix that, get rid of the performance/powersave global limits
split and use just one set of global limits for everything.

From the user's persepctive, after this modification, when
scaling_governor is switched from "performance" to "powersave"
or the other way around on one CPU, the limits settings (ie. the
global max/min_perf_pct and per-policy scaling_max/min_freq for
any CPUs) will not change.  Still, switching from "performance"
to "powersave" or the other way around changes the way in which
P-states are selected and in particular "performance" causes the
driver to always request the highest P-state it is allowed to ask
for for the given CPU.

Fixes: a240c4aa5d0f (cpufreq: intel_pstate: Do not reinit performance limits in ->setpolicy)
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 7de32556dfc62b9e1203730cc26b71292da8a244)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index 9b85edc571c0,7b07803e7042..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -270,77 -364,19 +270,86 @@@ static int hwp_active __read_mostly
  static bool acpi_ppc;
  #endif
  
++<<<<<<< HEAD
 +/**
 + * struct perf_limits - Store user and policy limits
 + * @no_turbo:		User requested turbo state from intel_pstate sysfs
 + * @turbo_disabled:	Platform turbo status either from msr
 + *			MSR_IA32_MISC_ENABLE or when maximum available pstate
 + *			matches the maximum turbo pstate
 + * @max_perf_pct:	Effective maximum performance limit in percentage, this
 + *			is minimum of either limits enforced by cpufreq policy
 + *			or limits from user set limits via intel_pstate sysfs
 + * @min_perf_pct:	Effective minimum performance limit in percentage, this
 + *			is maximum of either limits enforced by cpufreq policy
 + *			or limits from user set limits via intel_pstate sysfs
 + * @max_perf:		This is a scaled value between 0 to 255 for max_perf_pct
 + *			This value is used to limit max pstate
 + * @min_perf:		This is a scaled value between 0 to 255 for min_perf_pct
 + *			This value is used to limit min pstate
 + * @max_policy_pct:	The maximum performance in percentage enforced by
 + *			cpufreq setpolicy interface
 + * @max_sysfs_pct:	The maximum performance in percentage enforced by
 + *			intel pstate sysfs interface
 + * @min_policy_pct:	The minimum performance in percentage enforced by
 + *			cpufreq setpolicy interface
 + * @min_sysfs_pct:	The minimum performance in percentage enforced by
 + *			intel pstate sysfs interface
 + *
 + * Storage for user and policy defined limits.
 + */
 +struct perf_limits {
 +	int no_turbo;
 +	int turbo_disabled;
 +	int max_perf_pct;
 +	int min_perf_pct;
 +	int32_t max_perf;
 +	int32_t min_perf;
 +	int max_policy_pct;
 +	int max_sysfs_pct;
 +	int min_policy_pct;
 +	int min_sysfs_pct;
 +};
++=======
+ static struct perf_limits global;
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
 +
 +static struct perf_limits performance_limits = {
 +	.no_turbo = 0,
 +	.turbo_disabled = 0,
 +	.max_perf_pct = 100,
 +	.max_perf = int_tofp(1),
 +	.min_perf_pct = 100,
 +	.min_perf = int_tofp(1),
 +	.max_policy_pct = 100,
 +	.max_sysfs_pct = 100,
 +	.min_policy_pct = 0,
 +	.min_sysfs_pct = 0,
 +};
  
 -static void intel_pstate_init_limits(struct perf_limits *limits)
 -{
 -	memset(limits, 0, sizeof(*limits));
 -	limits->max_perf_pct = 100;
 -	limits->max_perf = int_ext_tofp(1);
 -	limits->max_policy_pct = 100;
 -	limits->max_sysfs_pct = 100;
 -}
++<<<<<<< HEAD
 +static struct perf_limits powersave_limits = {
 +	.no_turbo = 0,
 +	.turbo_disabled = 0,
 +	.max_perf_pct = 100,
 +	.max_perf = int_tofp(1),
 +	.min_perf_pct = 0,
 +	.min_perf = 0,
 +	.max_policy_pct = 100,
 +	.max_sysfs_pct = 100,
 +	.min_policy_pct = 0,
 +	.min_sysfs_pct = 0,
 +};
  
 +#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE
 +static struct perf_limits *limits = &performance_limits;
 +#else
 +static struct perf_limits *limits = &powersave_limits;
 +#endif
++=======
+ static DEFINE_MUTEX(intel_pstate_driver_lock);
+ static DEFINE_MUTEX(intel_pstate_limits_lock);
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
  #ifdef CONFIG_ACPI
  
@@@ -422,11 -494,10 +431,15 @@@ static void intel_pstate_init_acpi_perf
  	 * max frequency, which will cause a reduced performance as
  	 * this driver uses real max turbo frequency as the max
  	 * frequency. So correct this frequency in _PSS table to
 -	 * correct max turbo frequency based on the turbo state.
 +	 * correct max turbo frequency based on the turbo ratio.
  	 * Also need to convert to MHz as _PSS freq is in MHz.
  	 */
++<<<<<<< HEAD
 +	turbo_pss_ctl = convert_to_native_pstate_format(cpu, 0);
 +	if (turbo_pss_ctl > cpu->pstate.max_pstate)
++=======
+ 	if (!global.turbo_disabled)
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  		cpu->acpi_perf_data.states[0].core_frequency =
  					policy->cpuinfo.max_freq / 1000;
  	cpu->valid_pss_table = true;
@@@ -551,40 -621,351 +564,319 @@@ static inline void update_turbo_state(v
  		 cpu->pstate.max_pstate == cpu->pstate.turbo_pstate);
  }
  
 -static s16 intel_pstate_get_epb(struct cpudata *cpu_data)
 +static void intel_pstate_hwp_set(const struct cpumask *cpumask)
  {
++<<<<<<< HEAD
 +	int min, hw_min, max, hw_max, cpu, range, adj_range;
++=======
+ 	u64 epb;
+ 	int ret;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_EPB))
+ 		return -ENXIO;
+ 
+ 	ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
+ 	if (ret)
+ 		return (s16)ret;
+ 
+ 	return (s16)(epb & 0x0f);
+ }
+ 
+ static s16 intel_pstate_get_epp(struct cpudata *cpu_data, u64 hwp_req_data)
+ {
+ 	s16 epp;
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		/*
+ 		 * When hwp_req_data is 0, means that caller didn't read
+ 		 * MSR_HWP_REQUEST, so need to read and get EPP.
+ 		 */
+ 		if (!hwp_req_data) {
+ 			epp = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST,
+ 					    &hwp_req_data);
+ 			if (epp)
+ 				return epp;
+ 		}
+ 		epp = (hwp_req_data >> 24) & 0xff;
+ 	} else {
+ 		/* When there is no EPP present, HWP uses EPB settings */
+ 		epp = intel_pstate_get_epb(cpu_data);
+ 	}
+ 
+ 	return epp;
+ }
+ 
+ static int intel_pstate_set_epb(int cpu, s16 pref)
+ {
+ 	u64 epb;
+ 	int ret;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_EPB))
+ 		return -ENXIO;
+ 
+ 	ret = rdmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
+ 	if (ret)
+ 		return ret;
+ 
+ 	epb = (epb & ~0x0f) | pref;
+ 	wrmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, epb);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * EPP/EPB display strings corresponding to EPP index in the
+  * energy_perf_strings[]
+  *	index		String
+  *-------------------------------------
+  *	0		default
+  *	1		performance
+  *	2		balance_performance
+  *	3		balance_power
+  *	4		power
+  */
+ static const char * const energy_perf_strings[] = {
+ 	"default",
+ 	"performance",
+ 	"balance_performance",
+ 	"balance_power",
+ 	"power",
+ 	NULL
+ };
+ 
+ static int intel_pstate_get_energy_pref_index(struct cpudata *cpu_data)
+ {
+ 	s16 epp;
+ 	int index = -EINVAL;
+ 
+ 	epp = intel_pstate_get_epp(cpu_data, 0);
+ 	if (epp < 0)
+ 		return epp;
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		/*
+ 		 * Range:
+ 		 *	0x00-0x3F	:	Performance
+ 		 *	0x40-0x7F	:	Balance performance
+ 		 *	0x80-0xBF	:	Balance power
+ 		 *	0xC0-0xFF	:	Power
+ 		 * The EPP is a 8 bit value, but our ranges restrict the
+ 		 * value which can be set. Here only using top two bits
+ 		 * effectively.
+ 		 */
+ 		index = (epp >> 6) + 1;
+ 	} else if (static_cpu_has(X86_FEATURE_EPB)) {
+ 		/*
+ 		 * Range:
+ 		 *	0x00-0x03	:	Performance
+ 		 *	0x04-0x07	:	Balance performance
+ 		 *	0x08-0x0B	:	Balance power
+ 		 *	0x0C-0x0F	:	Power
+ 		 * The EPB is a 4 bit value, but our ranges restrict the
+ 		 * value which can be set. Here only using top two bits
+ 		 * effectively.
+ 		 */
+ 		index = (epp >> 2) + 1;
+ 	}
+ 
+ 	return index;
+ }
+ 
+ static int intel_pstate_set_energy_pref_index(struct cpudata *cpu_data,
+ 					      int pref_index)
+ {
+ 	int epp = -EINVAL;
+ 	int ret;
+ 
+ 	if (!pref_index)
+ 		epp = cpu_data->epp_default;
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		u64 value;
+ 
+ 		ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, &value);
+ 		if (ret)
+ 			goto return_pref;
+ 
+ 		value &= ~GENMASK_ULL(31, 24);
+ 
+ 		/*
+ 		 * If epp is not default, convert from index into
+ 		 * energy_perf_strings to epp value, by shifting 6
+ 		 * bits left to use only top two bits in epp.
+ 		 * The resultant epp need to shifted by 24 bits to
+ 		 * epp position in MSR_HWP_REQUEST.
+ 		 */
+ 		if (epp == -EINVAL)
+ 			epp = (pref_index - 1) << 6;
+ 
+ 		value |= (u64)epp << 24;
+ 		ret = wrmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, value);
+ 	} else {
+ 		if (epp == -EINVAL)
+ 			epp = (pref_index - 1) << 2;
+ 		ret = intel_pstate_set_epb(cpu_data->cpu, epp);
+ 	}
+ return_pref:
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t show_energy_performance_available_preferences(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	int i = 0;
+ 	int ret = 0;
+ 
+ 	while (energy_perf_strings[i] != NULL)
+ 		ret += sprintf(&buf[ret], "%s ", energy_perf_strings[i++]);
+ 
+ 	ret += sprintf(&buf[ret], "\n");
+ 
+ 	return ret;
+ }
+ 
+ cpufreq_freq_attr_ro(energy_performance_available_preferences);
+ 
+ static ssize_t store_energy_performance_preference(
+ 		struct cpufreq_policy *policy, const char *buf, size_t count)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 	char str_preference[21];
+ 	int ret, i = 0;
+ 
+ 	ret = sscanf(buf, "%20s", str_preference);
+ 	if (ret != 1)
+ 		return -EINVAL;
+ 
+ 	while (energy_perf_strings[i] != NULL) {
+ 		if (!strcmp(str_preference, energy_perf_strings[i])) {
+ 			intel_pstate_set_energy_pref_index(cpu_data, i);
+ 			return count;
+ 		}
+ 		++i;
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ static ssize_t show_energy_performance_preference(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 	int preference;
+ 
+ 	preference = intel_pstate_get_energy_pref_index(cpu_data);
+ 	if (preference < 0)
+ 		return preference;
+ 
+ 	return  sprintf(buf, "%s\n", energy_perf_strings[preference]);
+ }
+ 
+ cpufreq_freq_attr_rw(energy_performance_preference);
+ 
+ static struct freq_attr *hwp_cpufreq_attrs[] = {
+ 	&energy_performance_preference,
+ 	&energy_performance_available_preferences,
+ 	NULL,
+ };
+ 
+ static void intel_pstate_hwp_set(struct cpufreq_policy *policy)
+ {
+ 	int min, hw_min, max, hw_max, cpu;
+ 	struct perf_limits *perf_limits = &global;
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  	u64 value, cap;
  
 -	for_each_cpu(cpu, policy->cpus) {
 -		struct cpudata *cpu_data = all_cpu_data[cpu];
 -		s16 epp;
 -
 -		if (per_cpu_limits)
 -			perf_limits = all_cpu_data[cpu]->perf_limits;
 -
 +	for_each_cpu(cpu, cpumask) {
  		rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
  		hw_min = HWP_LOWEST_PERF(cap);
- 		if (limits->no_turbo)
+ 		if (global.no_turbo)
  			hw_max = HWP_GUARANTEED_PERF(cap);
  		else
  			hw_max = HWP_HIGHEST_PERF(cap);
++<<<<<<< HEAD
 +		range = hw_max - hw_min;
++=======
+ 
+ 		max = fp_ext_toint(hw_max * perf_limits->max_perf);
+ 		if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE)
+ 			min = max;
+ 		else
+ 			min = fp_ext_toint(hw_max * perf_limits->min_perf);
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
  		rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
 -
 +		adj_range = limits->min_perf_pct * range / 100;
 +		min = hw_min + adj_range;
  		value &= ~HWP_MIN_PERF(~0L);
  		value |= HWP_MIN_PERF(min);
  
++<<<<<<< HEAD
 +		adj_range = limits->max_perf_pct * range / 100;
 +		max = hw_min + adj_range;
 +
++=======
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  		value &= ~HWP_MAX_PERF(~0L);
  		value |= HWP_MAX_PERF(max);
 -
 -		if (cpu_data->epp_policy == cpu_data->policy)
 -			goto skip_epp;
 -
 -		cpu_data->epp_policy = cpu_data->policy;
 -
 -		if (cpu_data->epp_saved >= 0) {
 -			epp = cpu_data->epp_saved;
 -			cpu_data->epp_saved = -EINVAL;
 -			goto update_epp;
 -		}
 -
 -		if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE) {
 -			epp = intel_pstate_get_epp(cpu_data, value);
 -			cpu_data->epp_powersave = epp;
 -			/* If EPP read was failed, then don't try to write */
 -			if (epp < 0)
 -				goto skip_epp;
 -
 -
 -			epp = 0;
 -		} else {
 -			/* skip setting EPP, when saved value is invalid */
 -			if (cpu_data->epp_powersave < 0)
 -				goto skip_epp;
 -
 -			/*
 -			 * No need to restore EPP when it is not zero. This
 -			 * means:
 -			 *  - Policy is not changed
 -			 *  - user has manually changed
 -			 *  - Error reading EPB
 -			 */
 -			epp = intel_pstate_get_epp(cpu_data, value);
 -			if (epp)
 -				goto skip_epp;
 -
 -			epp = cpu_data->epp_powersave;
 -		}
 -update_epp:
 -		if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -			value &= ~GENMASK_ULL(31, 24);
 -			value |= (u64)epp << 24;
 -		} else {
 -			intel_pstate_set_epb(cpu, epp);
 -		}
 -skip_epp:
  		wrmsrl_on_cpu(cpu, MSR_HWP_REQUEST, value);
  	}
  }
  
 -static int intel_pstate_hwp_set_policy(struct cpufreq_policy *policy)
 +static void intel_pstate_hwp_set_online_cpus(void)
  {
++<<<<<<< HEAD
 +	get_online_cpus();
 +	intel_pstate_hwp_set(cpu_online_mask);
 +	put_online_cpus();
++=======
+ 	if (hwp_active)
+ 		intel_pstate_hwp_set(policy);
+ 
+ 	return 0;
+ }
+ 
+ static int intel_pstate_hwp_save_state(struct cpufreq_policy *policy)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 
+ 	if (!hwp_active)
+ 		return 0;
+ 
+ 	cpu_data->epp_saved = intel_pstate_get_epp(cpu_data, 0);
+ 
+ 	return 0;
+ }
+ 
+ static int intel_pstate_resume(struct cpufreq_policy *policy)
+ {
+ 	int ret;
+ 
+ 	if (!hwp_active)
+ 		return 0;
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 
+ 	all_cpu_data[policy->cpu]->epp_policy = 0;
+ 
+ 	ret = intel_pstate_hwp_set_policy(policy);
+ 
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	return ret;
+ }
+ 
+ static void intel_pstate_update_policies(void)
+ {
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu)
+ 		cpufreq_update_policy(cpu);
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  }
  
  /************************** debugfs begin ************************/
@@@ -641,9 -1044,37 +933,9 @@@ static void __init intel_pstate_debug_e
  	static ssize_t show_##file_name					\
  	(struct kobject *kobj, struct attribute *attr, char *buf)	\
  	{								\
- 		return sprintf(buf, "%u\n", limits->object);		\
+ 		return sprintf(buf, "%u\n", global.object);		\
  	}
  
 -static ssize_t intel_pstate_show_status(char *buf);
 -static int intel_pstate_update_status(const char *buf, size_t size);
 -
 -static ssize_t show_status(struct kobject *kobj,
 -			   struct attribute *attr, char *buf)
 -{
 -	ssize_t ret;
 -
 -	mutex_lock(&intel_pstate_driver_lock);
 -	ret = intel_pstate_show_status(buf);
 -	mutex_unlock(&intel_pstate_driver_lock);
 -
 -	return ret;
 -}
 -
 -static ssize_t store_status(struct kobject *a, struct attribute *b,
 -			    const char *buf, size_t count)
 -{
 -	char *p = memchr(buf, '\n', count);
 -	int ret;
 -
 -	mutex_lock(&intel_pstate_driver_lock);
 -	ret = intel_pstate_update_status(buf, p ? p - buf : count);
 -	mutex_unlock(&intel_pstate_driver_lock);
 -
 -	return ret < 0 ? ret : count;
 -}
 -
  static ssize_t show_turbo_pct(struct kobject *kobj,
  				struct attribute *attr, char *buf)
  {
@@@ -676,12 -1127,21 +968,12 @@@ static ssize_t show_no_turbo(struct kob
  {
  	ssize_t ret;
  
 -	mutex_lock(&intel_pstate_driver_lock);
 -
 -	if (!driver_registered) {
 -		mutex_unlock(&intel_pstate_driver_lock);
 -		return -EAGAIN;
 -	}
 -
  	update_turbo_state();
- 	if (limits->turbo_disabled)
- 		ret = sprintf(buf, "%u\n", limits->turbo_disabled);
+ 	if (global.turbo_disabled)
+ 		ret = sprintf(buf, "%u\n", global.turbo_disabled);
  	else
- 		ret = sprintf(buf, "%u\n", limits->no_turbo);
+ 		ret = sprintf(buf, "%u\n", global.no_turbo);
  
 -	mutex_unlock(&intel_pstate_driver_lock);
 -
  	return ret;
  }
  
@@@ -695,16 -1155,30 +987,26 @@@ static ssize_t store_no_turbo(struct ko
  	if (ret != 1)
  		return -EINVAL;
  
 -	mutex_lock(&intel_pstate_driver_lock);
 -
 -	if (!driver_registered) {
 -		mutex_unlock(&intel_pstate_driver_lock);
 -		return -EAGAIN;
 -	}
 -
 -	mutex_lock(&intel_pstate_limits_lock);
 -
  	update_turbo_state();
- 	if (limits->turbo_disabled) {
+ 	if (global.turbo_disabled) {
  		pr_warn("Turbo disabled by BIOS or unavailable on processor\n");
 -		mutex_unlock(&intel_pstate_limits_lock);
 -		mutex_unlock(&intel_pstate_driver_lock);
  		return -EPERM;
  	}
  
++<<<<<<< HEAD
 +	limits->no_turbo = clamp_t(int, input, 0, 1);
 +
 +	if (hwp_active)
 +		intel_pstate_hwp_set_online_cpus();
++=======
+ 	global.no_turbo = clamp_t(int, input, 0, 1);
+ 
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	intel_pstate_update_policies();
+ 
+ 	mutex_unlock(&intel_pstate_driver_lock);
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
  	return count;
  }
@@@ -719,18 -1193,27 +1021,41 @@@ static ssize_t store_max_perf_pct(struc
  	if (ret != 1)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	limits->max_sysfs_pct = clamp_t(int, input, 0 , 100);
 +	limits->max_perf_pct = min(limits->max_policy_pct,
 +				   limits->max_sysfs_pct);
 +	limits->max_perf_pct = max(limits->min_policy_pct,
 +				   limits->max_perf_pct);
 +	limits->max_perf_pct = max(limits->min_perf_pct,
 +				   limits->max_perf_pct);
 +	limits->max_perf = div_fp(int_tofp(limits->max_perf_pct),
 +				  int_tofp(100));
++=======
+ 	mutex_lock(&intel_pstate_driver_lock);
+ 
+ 	if (!driver_registered) {
+ 		mutex_unlock(&intel_pstate_driver_lock);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 
+ 	global.max_sysfs_pct = clamp_t(int, input, 0 , 100);
+ 	global.max_perf_pct = min(global.max_policy_pct, global.max_sysfs_pct);
+ 	global.max_perf_pct = max(global.min_policy_pct, global.max_perf_pct);
+ 	global.max_perf_pct = max(global.min_perf_pct, global.max_perf_pct);
+ 	global.max_perf = percent_ext_fp(global.max_perf_pct);
+ 
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	intel_pstate_update_policies();
+ 
+ 	mutex_unlock(&intel_pstate_driver_lock);
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
 +	if (hwp_active)
 +		intel_pstate_hwp_set_online_cpus();
  	return count;
  }
  
@@@ -744,18 -1227,27 +1069,41 @@@ static ssize_t store_min_perf_pct(struc
  	if (ret != 1)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	limits->min_sysfs_pct = clamp_t(int, input, 0 , 100);
 +	limits->min_perf_pct = max(limits->min_policy_pct,
 +				   limits->min_sysfs_pct);
 +	limits->min_perf_pct = min(limits->max_policy_pct,
 +				   limits->min_perf_pct);
 +	limits->min_perf_pct = min(limits->max_perf_pct,
 +				   limits->min_perf_pct);
 +	limits->min_perf = div_fp(int_tofp(limits->min_perf_pct),
 +				  int_tofp(100));
++=======
+ 	mutex_lock(&intel_pstate_driver_lock);
+ 
+ 	if (!driver_registered) {
+ 		mutex_unlock(&intel_pstate_driver_lock);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 
+ 	global.min_sysfs_pct = clamp_t(int, input, 0 , 100);
+ 	global.min_perf_pct = max(global.min_policy_pct, global.min_sysfs_pct);
+ 	global.min_perf_pct = min(global.max_policy_pct, global.min_perf_pct);
+ 	global.min_perf_pct = min(global.max_perf_pct, global.min_perf_pct);
+ 	global.min_perf = percent_ext_fp(global.min_perf_pct);
+ 
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	intel_pstate_update_policies();
+ 
+ 	mutex_unlock(&intel_pstate_driver_lock);
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
 +	if (hwp_active)
 +		intel_pstate_hwp_set_online_cpus();
  	return count;
  }
  
@@@ -1010,10 -1535,10 +1358,10 @@@ static void core_set_pstate(struct cpud
  	u64 val;
  
  	val = (u64)pstate << 8;
- 	if (limits->no_turbo && !limits->turbo_disabled)
+ 	if (global.no_turbo && !global.turbo_disabled)
  		val |= (u64)1 << 32;
  
 -	return val;
 +	wrmsrl_on_cpu(cpudata->cpu, MSR_IA32_PERF_CTL, val);
  }
  
  static int knl_get_turbo_pstate(void)
@@@ -1095,10 -1661,14 +1443,14 @@@ static void intel_pstate_get_min_max(st
  	int max_perf = cpu->pstate.turbo_pstate;
  	int max_perf_adj;
  	int min_perf;
++<<<<<<< HEAD
++=======
+ 	struct perf_limits *perf_limits = &global;
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
- 	if (limits->no_turbo || limits->turbo_disabled)
+ 	if (global.no_turbo || global.turbo_disabled)
  		max_perf = cpu->pstate.max_pstate;
  
 -	if (per_cpu_limits)
 -		perf_limits = cpu->perf_limits;
 -
  	/*
  	 * performance can be limited by user through sysfs, by cpufreq
  	 * policy, or by cpu specific default values determined through
@@@ -1218,36 -1785,38 +1570,55 @@@ static inline int32_t get_avg_pstate(st
  static inline int32_t get_target_pstate_use_cpu_load(struct cpudata *cpu)
  {
  	struct sample *sample = &cpu->sample;
 -	int32_t busy_frac, boost;
 -	int target, avg_pstate;
 -
 +	u64 cummulative_iowait, delta_iowait_us;
 +	u64 delta_iowait_mperf;
 +	u64 mperf, now;
 +	int32_t cpu_load;
 +
++<<<<<<< HEAD
 +	cummulative_iowait = get_cpu_iowait_time_us(cpu->cpu, &now);
++=======
+ 	busy_frac = div_fp(sample->mperf, sample->tsc);
+ 
+ 	boost = cpu->iowait_boost;
+ 	cpu->iowait_boost >>= 1;
+ 
+ 	if (busy_frac < boost)
+ 		busy_frac = boost;
+ 
+ 	sample->busy_scaled = busy_frac * 100;
+ 
+ 	target = global.no_turbo || global.turbo_disabled ?
+ 			cpu->pstate.max_pstate : cpu->pstate.turbo_pstate;
+ 	target += target >> 2;
+ 	target = mul_fp(target, busy_frac);
+ 	if (target < cpu->pstate.min_pstate)
+ 		target = cpu->pstate.min_pstate;
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
  	/*
 -	 * If the average P-state during the previous cycle was higher than the
 -	 * current target, add 50% of the difference to the target to reduce
 -	 * possible performance oscillations and offset possible performance
 -	 * loss related to moving the workload from one CPU to another within
 -	 * a package/module.
 +	 * Convert iowait time into number of IO cycles spent at max_freq.
 +	 * IO is considered as busy only for the cpu_load algorithm. For
 +	 * performance this is not needed since we always try to reach the
 +	 * maximum P-State, so we are already boosting the IOs.
  	 */
 -	avg_pstate = get_avg_pstate(cpu);
 -	if (avg_pstate > target)
 -		target += (avg_pstate - target) >> 1;
 +	delta_iowait_us = cummulative_iowait - cpu->prev_cummulative_iowait;
 +	delta_iowait_mperf = div64_u64(delta_iowait_us * cpu->pstate.scaling *
 +		cpu->pstate.max_pstate, MSEC_PER_SEC);
 +
 +	mperf = cpu->sample.mperf + delta_iowait_mperf;
 +	cpu->prev_cummulative_iowait = cummulative_iowait;
  
 -	return target;
 +	/*
 +	 * The load can be estimated as the ratio of the mperf counter
 +	 * running at a constant frequency during active periods
 +	 * (C0) and the time stamp counter running at the same frequency
 +	 * also during C-states.
 +	 */
 +	cpu_load = div64_u64(int_tofp(100) * mperf, sample->tsc);
 +	cpu->sample.busy_scaled = cpu_load;
 +
 +	return get_avg_pstate(cpu) - pid_calc(&cpu->pid, cpu_load);
  }
  
  static inline int32_t get_target_pstate_use_performance(struct cpudata *cpu)
@@@ -1436,50 -2093,46 +1807,75 @@@ static unsigned int intel_pstate_get(un
  
  static int intel_pstate_set_policy(struct cpufreq_policy *policy)
  {
++<<<<<<< HEAD
++=======
+ 	struct cpudata *cpu;
+ 	struct perf_limits *perf_limits = &global;
+ 
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  	if (!policy->cpuinfo.max_freq)
  		return -ENODEV;
  
  	pr_debug("set_policy cpuinfo.max %u policy->max %u\n",
  		 policy->cpuinfo.max_freq, policy->max);
  
 -	cpu = all_cpu_data[policy->cpu];
 -	cpu->policy = policy->policy;
 -
 -	if (cpu->pstate.max_pstate_physical > cpu->pstate.max_pstate &&
 -	    policy->max < policy->cpuinfo.max_freq &&
 -	    policy->max > cpu->pstate.max_pstate * cpu->pstate.scaling) {
 -		pr_debug("policy->max > max non turbo frequency\n");
 -		policy->max = policy->cpuinfo.max_freq;
 +	if (policy->policy == CPUFREQ_POLICY_PERFORMANCE &&
 +	    policy->max >= policy->cpuinfo.max_freq) {
 +		pr_debug("intel_pstate: set performance\n");
 +		limits = &performance_limits;
 +		if (hwp_active)
 +			intel_pstate_hwp_set(policy->cpus);
 +		return 0;
  	}
  
 -	if (per_cpu_limits)
 -		perf_limits = cpu->perf_limits;
 -
 -	mutex_lock(&intel_pstate_limits_lock);
 +	pr_debug("intel_pstate: set powersave\n");
 +	limits = &powersave_limits;
 +	limits->min_policy_pct = (policy->min * 100) / policy->cpuinfo.max_freq;
 +	limits->min_policy_pct = clamp_t(int, limits->min_policy_pct, 0 , 100);
 +	limits->max_policy_pct = DIV_ROUND_UP(policy->max * 100,
 +					      policy->cpuinfo.max_freq);
 +	limits->max_policy_pct = clamp_t(int, limits->max_policy_pct, 0 , 100);
 +
 +	/* Normalize user input to [min_policy_pct, max_policy_pct] */
 +	limits->min_perf_pct = max(limits->min_policy_pct,
 +				   limits->min_sysfs_pct);
 +	limits->min_perf_pct = min(limits->max_policy_pct,
 +				   limits->min_perf_pct);
 +	limits->max_perf_pct = min(limits->max_policy_pct,
 +				   limits->max_sysfs_pct);
 +	limits->max_perf_pct = max(limits->min_policy_pct,
 +				   limits->max_perf_pct);
 +
++<<<<<<< HEAD
 +	/* Make sure min_perf_pct <= max_perf_pct */
 +	limits->min_perf_pct = min(limits->max_perf_pct, limits->min_perf_pct);
 +
 +	limits->min_perf = div_fp(int_tofp(limits->min_perf_pct),
 +				  int_tofp(100));
 +	limits->max_perf = div_fp(int_tofp(limits->max_perf_pct),
 +				  int_tofp(100));
 +	limits->max_perf = round_up(limits->max_perf, FRAC_BITS);
  
 +	if (hwp_active)
 +		intel_pstate_hwp_set(policy->cpus);
++=======
+ 	intel_pstate_update_perf_limits(policy, perf_limits);
+ 
+ 	if (cpu->policy == CPUFREQ_POLICY_PERFORMANCE) {
+ 		/*
+ 		 * NOHZ_FULL CPUs need this as the governor callback may not
+ 		 * be invoked on them.
+ 		 */
+ 		intel_pstate_clear_update_util_hook(policy->cpu);
+ 		intel_pstate_max_within_limits(cpu);
+ 	}
+ 
+ 	intel_pstate_set_update_util_hook(policy->cpu);
+ 
+ 	intel_pstate_hwp_set_policy(policy);
+ 
+ 	mutex_unlock(&intel_pstate_limits_lock);
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
  	return 0;
  }
@@@ -1506,6 -2152,17 +1895,20 @@@ static int intel_pstate_verify_policy(s
  	    policy->policy != CPUFREQ_POLICY_PERFORMANCE)
  		return -EINVAL;
  
++<<<<<<< HEAD
++=======
+ 	/* When per-CPU limits are used, sysfs limits are not used */
+ 	if (!per_cpu_limits) {
+ 		unsigned int max_freq, min_freq;
+ 
+ 		max_freq = policy->cpuinfo.max_freq *
+ 					global.max_sysfs_pct / 100;
+ 		min_freq = policy->cpuinfo.max_freq *
+ 					global.min_sysfs_pct / 100;
+ 		cpufreq_verify_within_limits(policy, min_freq, max_freq);
+ 	}
+ 
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  	return 0;
  }
  
@@@ -1556,9 -2221,20 +1959,22 @@@ static int intel_pstate_cpu_init(struc
  	return 0;
  }
  
 -static int intel_pstate_cpu_init(struct cpufreq_policy *policy)
 +static int intel_pstate_cpu_exit(struct cpufreq_policy *policy)
  {
++<<<<<<< HEAD
 +	intel_pstate_exit_perf_limits(policy);
++=======
+ 	int ret = __intel_pstate_cpu_init(policy);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	policy->cpuinfo.transition_latency = CPUFREQ_ETERNAL;
+ 	if (IS_ENABLED(CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE))
+ 		policy->policy = CPUFREQ_POLICY_PERFORMANCE;
+ 	else
+ 		policy->policy = CPUFREQ_POLICY_POWERSAVE;
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  
  	return 0;
  }
@@@ -1574,6 -2252,221 +1990,224 @@@ static struct cpufreq_driver intel_psta
  	.name		= "intel_pstate",
  };
  
++<<<<<<< HEAD
++=======
+ static int intel_cpufreq_verify_policy(struct cpufreq_policy *policy)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 
+ 	update_turbo_state();
+ 	policy->cpuinfo.max_freq = global.turbo_disabled ?
+ 			cpu->pstate.max_freq : cpu->pstate.turbo_freq;
+ 
+ 	cpufreq_verify_within_cpu_limits(policy);
+ 
+ 	return 0;
+ }
+ 
+ static unsigned int intel_cpufreq_turbo_update(struct cpudata *cpu,
+ 					       struct cpufreq_policy *policy,
+ 					       unsigned int target_freq)
+ {
+ 	unsigned int max_freq;
+ 
+ 	update_turbo_state();
+ 
+ 	max_freq = global.no_turbo || global.turbo_disabled ?
+ 			cpu->pstate.max_freq : cpu->pstate.turbo_freq;
+ 	policy->cpuinfo.max_freq = max_freq;
+ 	if (policy->max > max_freq)
+ 		policy->max = max_freq;
+ 
+ 	if (target_freq > max_freq)
+ 		target_freq = max_freq;
+ 
+ 	return target_freq;
+ }
+ 
+ static int intel_cpufreq_target(struct cpufreq_policy *policy,
+ 				unsigned int target_freq,
+ 				unsigned int relation)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 	struct cpufreq_freqs freqs;
+ 	int target_pstate;
+ 
+ 	freqs.old = policy->cur;
+ 	freqs.new = intel_cpufreq_turbo_update(cpu, policy, target_freq);
+ 
+ 	cpufreq_freq_transition_begin(policy, &freqs);
+ 	switch (relation) {
+ 	case CPUFREQ_RELATION_L:
+ 		target_pstate = DIV_ROUND_UP(freqs.new, cpu->pstate.scaling);
+ 		break;
+ 	case CPUFREQ_RELATION_H:
+ 		target_pstate = freqs.new / cpu->pstate.scaling;
+ 		break;
+ 	default:
+ 		target_pstate = DIV_ROUND_CLOSEST(freqs.new, cpu->pstate.scaling);
+ 		break;
+ 	}
+ 	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
+ 	if (target_pstate != cpu->pstate.current_pstate) {
+ 		cpu->pstate.current_pstate = target_pstate;
+ 		wrmsrl_on_cpu(policy->cpu, MSR_IA32_PERF_CTL,
+ 			      pstate_funcs.get_val(cpu, target_pstate));
+ 	}
+ 	freqs.new = target_pstate * cpu->pstate.scaling;
+ 	cpufreq_freq_transition_end(policy, &freqs, false);
+ 
+ 	return 0;
+ }
+ 
+ static unsigned int intel_cpufreq_fast_switch(struct cpufreq_policy *policy,
+ 					      unsigned int target_freq)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 	int target_pstate;
+ 
+ 	target_freq = intel_cpufreq_turbo_update(cpu, policy, target_freq);
+ 	target_pstate = DIV_ROUND_UP(target_freq, cpu->pstate.scaling);
+ 	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
+ 	intel_pstate_update_pstate(cpu, target_pstate);
+ 	return target_pstate * cpu->pstate.scaling;
+ }
+ 
+ static int intel_cpufreq_cpu_init(struct cpufreq_policy *policy)
+ {
+ 	int ret = __intel_pstate_cpu_init(policy);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	policy->cpuinfo.transition_latency = INTEL_CPUFREQ_TRANSITION_LATENCY;
+ 	/* This reflects the intel_pstate_get_cpu_pstates() setting. */
+ 	policy->cur = policy->cpuinfo.min_freq;
+ 
+ 	return 0;
+ }
+ 
+ static struct cpufreq_driver intel_cpufreq = {
+ 	.flags		= CPUFREQ_CONST_LOOPS,
+ 	.verify		= intel_cpufreq_verify_policy,
+ 	.target		= intel_cpufreq_target,
+ 	.fast_switch	= intel_cpufreq_fast_switch,
+ 	.init		= intel_cpufreq_cpu_init,
+ 	.exit		= intel_pstate_cpu_exit,
+ 	.stop_cpu	= intel_cpufreq_stop_cpu,
+ 	.name		= "intel_cpufreq",
+ };
+ 
+ static struct cpufreq_driver *intel_pstate_driver = &intel_pstate;
+ 
+ static void intel_pstate_driver_cleanup(void)
+ {
+ 	unsigned int cpu;
+ 
+ 	get_online_cpus();
+ 	for_each_online_cpu(cpu) {
+ 		if (all_cpu_data[cpu]) {
+ 			if (intel_pstate_driver == &intel_pstate)
+ 				intel_pstate_clear_update_util_hook(cpu);
+ 
+ 			kfree(all_cpu_data[cpu]);
+ 			all_cpu_data[cpu] = NULL;
+ 		}
+ 	}
+ 	put_online_cpus();
+ }
+ 
+ static int intel_pstate_register_driver(void)
+ {
+ 	int ret;
+ 
+ 	intel_pstate_init_limits(&global);
+ 
+ 	ret = cpufreq_register_driver(intel_pstate_driver);
+ 	if (ret) {
+ 		intel_pstate_driver_cleanup();
+ 		return ret;
+ 	}
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 	driver_registered = true;
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	if (intel_pstate_driver == &intel_pstate && !hwp_active &&
+ 	    pstate_funcs.get_target_pstate != get_target_pstate_use_cpu_load)
+ 		intel_pstate_debug_expose_params();
+ 
+ 	return 0;
+ }
+ 
+ static int intel_pstate_unregister_driver(void)
+ {
+ 	if (hwp_active)
+ 		return -EBUSY;
+ 
+ 	if (intel_pstate_driver == &intel_pstate && !hwp_active &&
+ 	    pstate_funcs.get_target_pstate != get_target_pstate_use_cpu_load)
+ 		intel_pstate_debug_hide_params();
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 	driver_registered = false;
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	cpufreq_unregister_driver(intel_pstate_driver);
+ 	intel_pstate_driver_cleanup();
+ 
+ 	return 0;
+ }
+ 
+ static ssize_t intel_pstate_show_status(char *buf)
+ {
+ 	if (!driver_registered)
+ 		return sprintf(buf, "off\n");
+ 
+ 	return sprintf(buf, "%s\n", intel_pstate_driver == &intel_pstate ?
+ 					"active" : "passive");
+ }
+ 
+ static int intel_pstate_update_status(const char *buf, size_t size)
+ {
+ 	int ret;
+ 
+ 	if (size == 3 && !strncmp(buf, "off", size))
+ 		return driver_registered ?
+ 			intel_pstate_unregister_driver() : -EINVAL;
+ 
+ 	if (size == 6 && !strncmp(buf, "active", size)) {
+ 		if (driver_registered) {
+ 			if (intel_pstate_driver == &intel_pstate)
+ 				return 0;
+ 
+ 			ret = intel_pstate_unregister_driver();
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		intel_pstate_driver = &intel_pstate;
+ 		return intel_pstate_register_driver();
+ 	}
+ 
+ 	if (size == 7 && !strncmp(buf, "passive", size)) {
+ 		if (driver_registered) {
+ 			if (intel_pstate_driver != &intel_pstate)
+ 				return 0;
+ 
+ 			ret = intel_pstate_unregister_driver();
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		intel_pstate_driver = &intel_cpufreq;
+ 		return intel_pstate_register_driver();
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> 7de32556dfc6 (cpufreq: intel_pstate: One set of global limits in active mode)
  static int no_load __initdata;
  static int no_hwp __initdata;
  static int hwp_only __initdata;
* Unmerged path drivers/cpufreq/intel_pstate.c
