KVM: x86: simplify handling of PKRU

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit b9dd21e104bcd45e124acfe978a79df71259e59b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b9dd21e1.failed

Move it to struct kvm_arch_vcpu, replacing guest_pkru_valid with a
simple comparison against the host value of the register.  The write of
PKRU in addition can be skipped if the guest has not enabled the feature.
Once we do this, we need not test OSPKE in the host anymore, because
guest_CR4.PKE=1 implies host_CR4.PKE=1.

The static PKU test is kept to elide the code on older CPUs.

	Suggested-by: Yang Zhang <zy107165@alibaba-inc.com>
Fixes: 1be0e61c1f255faaeab04a390e00c8b9b9042870
	Cc: stable@vger.kernel.org
	Reviewed-by: David Hildenbrand <david@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit b9dd21e104bcd45e124acfe978a79df71259e59b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/svm.c
index 7ad17773012b,af256b786a70..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -4960,11 -5408,6 +4955,14 @@@ static struct kvm_x86_ops svm_x86_ops 
  	.get_rflags = svm_get_rflags,
  	.set_rflags = svm_set_rflags,
  
++<<<<<<< HEAD
 +	.get_pkru = svm_get_pkru,
 +
 +	.fpu_activate = svm_fpu_activate,
 +	.fpu_deactivate = svm_fpu_deactivate,
 +
++=======
++>>>>>>> b9dd21e104bc (KVM: x86: simplify handling of PKRU)
  	.tlb_flush = svm_flush_tlb,
  
  	.run = svm_vcpu_run,
diff --cc arch/x86/kvm/vmx.c
index 4a55409187cf,c6ef2940119b..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -607,8 -631,13 +607,13 @@@ struct vcpu_vmx 
  #define PML_ENTITY_NUM		512
  	struct page *pml_pg;
  
 -	/* apic deadline value in host tsc */
 -	u64 hv_deadline_tsc;
 -
  	u64 current_tsc_ratio;
  
++<<<<<<< HEAD
++=======
+ 	u32 host_pkru;
+ 
++>>>>>>> b9dd21e104bc (KVM: x86: simplify handling of PKRU)
  	/*
  	 * Only bits masked by msr_ia32_feature_control_valid_bits can be set in
  	 * msr_ia32_feature_control. FEATURE_CONTROL_LOCKED is always included
@@@ -2236,13 -2376,11 +2241,8 @@@ static void vmx_set_rflags(struct kvm_v
  		rflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;
  	}
  	vmcs_writel(GUEST_RFLAGS, rflags);
 -
 -	if ((old_rflags ^ to_vmx(vcpu)->rflags) & X86_EFLAGS_VM)
 -		to_vmx(vcpu)->emulation_required = emulation_required(vcpu);
  }
  
- static u32 vmx_get_pkru(struct kvm_vcpu *vcpu)
- {
- 	return to_vmx(vcpu)->guest_pkru;
- }
- 
  static u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
  {
  	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
@@@ -8559,6 -9013,11 +8559,14 @@@ static void __noclone vmx_vcpu_run(stru
  	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
  		vmx_set_interrupt_shadow(vcpu, 0);
  
++<<<<<<< HEAD
++=======
+ 	if (static_cpu_has(X86_FEATURE_PKU) &&
+ 	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&
+ 	    vcpu->arch.pkru != vmx->host_pkru)
+ 		__write_pkru(vcpu->arch.pkru);
+ 
++>>>>>>> b9dd21e104bc (KVM: x86: simplify handling of PKRU)
  	atomic_switch_perf_msrs(vmx);
  	debugctlmsr = get_debugctlmsr();
  
@@@ -8699,6 -9160,18 +8707,21 @@@
  	vmx->exit_reason = vmcs_read32(VM_EXIT_REASON);
  
  	/*
++<<<<<<< HEAD
++=======
+ 	 * eager fpu is enabled if PKEY is supported and CR4 is switched
+ 	 * back on host, so it is safe to read guest PKRU from current
+ 	 * XSAVE.
+ 	 */
+ 	if (static_cpu_has(X86_FEATURE_PKU) &&
+ 	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {
+ 		vcpu->arch.pkru = __read_pkru();
+ 		if (vcpu->arch.pkru != vmx->host_pkru)
+ 			__write_pkru(vmx->host_pkru);
+ 	}
+ 
+ 	/*
++>>>>>>> b9dd21e104bc (KVM: x86: simplify handling of PKRU)
  	 * the KVM_REQ_EVENT optimization bit is only on for one entry, and if
  	 * we did not inject a still-pending event to L1 now because of
  	 * nested_run_pending, we need to re-enable this bit.
@@@ -10909,11 -11675,6 +10932,14 @@@ static struct kvm_x86_ops vmx_x86_ops 
  	.get_rflags = vmx_get_rflags,
  	.set_rflags = vmx_set_rflags,
  
++<<<<<<< HEAD
 +	.get_pkru = vmx_get_pkru,
 +
 +	.fpu_activate = vmx_fpu_activate,
 +	.fpu_deactivate = vmx_fpu_deactivate,
 +
++=======
++>>>>>>> b9dd21e104bc (KVM: x86: simplify handling of PKRU)
  	.tlb_flush = vmx_flush_tlb,
  
  	.run = vmx_vcpu_run,
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 98670a995e4c..6f361e9300c9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -417,6 +417,7 @@ struct kvm_vcpu_arch {
 	unsigned long cr4;
 	unsigned long cr4_guest_owned_bits;
 	unsigned long cr8;
+	u32 pkru;
 	u32 hflags;
 	u64 efer;
 	u64 apic_base;
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 762cdf2595f9..e1e89ee4af75 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -84,11 +84,6 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_register_read(vcpu, VCPU_REGS_RDX) & -1u) << 32);
 }
 
-static inline u32 kvm_read_pkru(struct kvm_vcpu *vcpu)
-{
-	return kvm_x86_ops->get_pkru(vcpu);
-}
-
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags |= HF_GUEST_MASK;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index b70df72e2b33..f59e9a3fba5f 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -186,7 +186,7 @@ static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 		* index of the protection domain, so pte_pkey * 2 is
 		* is the index of the first bit for the domain.
 		*/
-		pkru_bits = (kvm_read_pkru(vcpu) >> (pte_pkey * 2)) & 3;
+		pkru_bits = (vcpu->arch.pkru >> (pte_pkey * 2)) & 3;
 
 		/* clear present bit, replace PFEC.RSVD with ACC_USER_MASK. */
 		offset = pfec - 1 +
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx.c
