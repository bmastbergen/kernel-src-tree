blk-stat: move BLK_RQ_STAT_BATCH definition to blk-stat.c

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Omar Sandoval <osandov@fb.com>
commit 4875253fddd7b6d322f028ad023d44b6efb7f73b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/4875253f.failed

This is an implementation detail that no-one outside of blk-stat.c uses.

	Signed-off-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 4875253fddd7b6d322f028ad023d44b6efb7f73b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-stat.c
#	include/linux/blk_types.h
diff --cc include/linux/blk_types.h
index 822005a81879,e213c5e7500b..000000000000
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@@ -226,66 -206,95 +226,78 @@@ enum rq_flag_bits 
  
  #define REQ_FAILFAST_MASK \
  	(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT | REQ_FAILFAST_DRIVER)
 +#define REQ_COMMON_MASK \
 +	(REQ_WRITE | REQ_FAILFAST_MASK | REQ_SYNC | REQ_META | REQ_PRIO | \
 +	 REQ_DISCARD | REQ_WRITE_SAME | REQ_NOIDLE | REQ_FLUSH | REQ_FUA | \
 +	 REQ_SECURE)
 +#define REQ_CLONE_MASK		REQ_COMMON_MASK
  
 -#define REQ_NOMERGE_FLAGS \
 -	(REQ_NOMERGE | REQ_PREFLUSH | REQ_FUA)
 -
 -#define bio_op(bio) \
 -	((bio)->bi_opf & REQ_OP_MASK)
 -#define req_op(req) \
 -	((req)->cmd_flags & REQ_OP_MASK)
 +#define BIO_NO_ADVANCE_ITER_MASK	(REQ_DISCARD|REQ_WRITE_SAME)
  
 -/* obsolete, don't use in new code */
 -static inline void bio_set_op_attrs(struct bio *bio, unsigned op,
 -		unsigned op_flags)
 -{
 -	bio->bi_opf = op | op_flags;
 -}
 +/* This mask is used for both bio and request merge checking */
 +#define REQ_NOMERGE_FLAGS \
 +	(REQ_NOMERGE | REQ_STARTED | REQ_SOFTBARRIER | REQ_FLUSH | REQ_FUA | REQ_FLUSH_SEQ)
  
 -static inline bool op_is_write(unsigned int op)
 -{
 -	return (op & 1);
 -}
 +#define REQ_RAHEAD		(1ULL << __REQ_RAHEAD)
 +#define REQ_THROTTLED		(1ULL << __REQ_THROTTLED)
  
 -/*
 - * Check if the bio or request is one that needs special treatment in the
 - * flush state machine.
 - */
 -static inline bool op_is_flush(unsigned int op)
 -{
 -	return op & (REQ_FUA | REQ_PREFLUSH);
 -}
 +#define REQ_SORTED		(1ULL << __REQ_SORTED)
 +#define REQ_SOFTBARRIER		(1ULL << __REQ_SOFTBARRIER)
 +#define REQ_FUA			(1ULL << __REQ_FUA)
 +#define REQ_NOMERGE		(1ULL << __REQ_NOMERGE)
 +#define REQ_STARTED		(1ULL << __REQ_STARTED)
 +#define REQ_DONTPREP		(1ULL << __REQ_DONTPREP)
 +#define REQ_QUEUED		(1ULL << __REQ_QUEUED)
 +#define REQ_ELVPRIV		(1ULL << __REQ_ELVPRIV)
 +#define REQ_FAILED		(1ULL << __REQ_FAILED)
 +#define REQ_QUIET		(1ULL << __REQ_QUIET)
 +#define REQ_PREEMPT		(1ULL << __REQ_PREEMPT)
 +#define REQ_ALLOCED		(1ULL << __REQ_ALLOCED)
 +#define REQ_COPY_USER		(1ULL << __REQ_COPY_USER)
 +#define REQ_FLUSH		(1ULL << __REQ_FLUSH)
 +#define REQ_FLUSH_SEQ		(1ULL << __REQ_FLUSH_SEQ)
 +#define REQ_IO_STAT		(1ULL << __REQ_IO_STAT)
 +#define REQ_MIXED_MERGE		(1ULL << __REQ_MIXED_MERGE)
 +#define REQ_SECURE		(1ULL << __REQ_SECURE)
 +#define REQ_KERNEL		(1ULL << __REQ_KERNEL)
 +#define REQ_PM			(1ULL << __REQ_PM)
 +#define REQ_HASHED		(1ULL << __REQ_HASHED)
 +#define REQ_MQ_INFLIGHT		(1ULL << __REQ_MQ_INFLIGHT)
 +
 +enum req_op {
 +	REQ_OP_READ,
 +	REQ_OP_WRITE		= REQ_WRITE,
 +	REQ_OP_DISCARD		= REQ_DISCARD,
 +	REQ_OP_WRITE_SAME	= REQ_WRITE_SAME,
 +};
  
++<<<<<<< HEAD
  /*
 - * Reads are always treated as synchronous, as are requests with the FUA or
 - * PREFLUSH flag.  Other operations may be marked as synchronous using the
 - * REQ_SYNC flag.
 + * tmp cpmpat. Users used to set the write bit for all non reads, but
 + * we will be dropping the bitmap use for ops. Support both until
 + * the end of the patchset.
   */
 -static inline bool op_is_sync(unsigned int op)
 +static inline int op_from_rq_bits(u64 flags)
  {
 -	return (op & REQ_OP_MASK) == REQ_OP_READ ||
 -		(op & (REQ_SYNC | REQ_FUA | REQ_PREFLUSH));
 +	if (flags & REQ_OP_DISCARD)
 +		return REQ_OP_DISCARD;
 +	else if (flags & REQ_OP_WRITE_SAME)
 +		return REQ_OP_WRITE_SAME;
 +	else if (flags & REQ_OP_WRITE)
 +		return REQ_OP_WRITE;
 +	else
 +		return REQ_OP_READ;
  }
 -
 -typedef unsigned int blk_qc_t;
 -#define BLK_QC_T_NONE		-1U
 -#define BLK_QC_T_SHIFT		16
 -#define BLK_QC_T_INTERNAL	(1U << 31)
 -
 -static inline bool blk_qc_t_valid(blk_qc_t cookie)
 -{
 -	return cookie != BLK_QC_T_NONE;
 -}
 -
 -static inline blk_qc_t blk_tag_to_qc_t(unsigned int tag, unsigned int queue_num,
 -				       bool internal)
 -{
 -	blk_qc_t ret = tag | (queue_num << BLK_QC_T_SHIFT);
 -
 -	if (internal)
 -		ret |= BLK_QC_T_INTERNAL;
 -
 -	return ret;
 -}
 -
 -static inline unsigned int blk_qc_t_to_queue_num(blk_qc_t cookie)
 -{
 -	return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
 -}
 -
 -static inline unsigned int blk_qc_t_to_tag(blk_qc_t cookie)
 -{
 -	return cookie & ((1u << BLK_QC_T_SHIFT) - 1);
 -}
 -
 -static inline bool blk_qc_t_is_internal(blk_qc_t cookie)
 -{
 -	return (cookie & BLK_QC_T_INTERNAL) != 0;
 -}
 -
 -struct blk_issue_stat {
 -	u64 time;
 -};
 -
++=======
+ struct blk_rq_stat {
+ 	s64 mean;
+ 	u64 min;
+ 	u64 max;
+ 	s32 nr_samples;
+ 	s32 nr_batch;
+ 	u64 batch;
+ 	s64 time;
+ };
++>>>>>>> 4875253fddd7 (blk-stat: move BLK_RQ_STAT_BATCH definition to blk-stat.c)
  
  #endif /* __LINUX_BLK_TYPES_H */
* Unmerged path block/blk-stat.c
* Unmerged path block/blk-stat.c
* Unmerged path include/linux/blk_types.h
