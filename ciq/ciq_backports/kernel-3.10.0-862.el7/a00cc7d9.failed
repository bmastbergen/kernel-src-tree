mm, x86: add support for PUD-sized transparent hugepages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] dd support for PUD-sized transparent hugepages (Larry Woodman) [1457572 1457561]
Rebuild_FUZZ: 90.20%
commit-author Matthew Wilcox <willy@linux.intel.com>
commit a00cc7d9dd93d66a3fb83fc52aa57a4bec51c517
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/a00cc7d9.failed

The current transparent hugepage code only supports PMDs.  This patch
adds support for transparent use of PUDs with DAX.  It does not include
support for anonymous pages.  x86 support code also added.

Most of this patch simply parallels the work that was done for huge
PMDs.  The only major difference is how the new ->pud_entry method in
mm_walk works.  The ->pmd_entry method replaces the ->pte_entry method,
whereas the ->pud_entry method works along with either ->pmd_entry or
->pte_entry.  The pagewalk code takes care of locking the PUD before
calling ->pud_walk, so handlers do not need to worry whether the PUD is
stable.

[dave.jiang@intel.com: fix SMP x86 32bit build for native_pud_clear()]
  Link: http://lkml.kernel.org/r/148719066814.31111.3239231168815337012.stgit@djiang5-desk3.ch.intel.com
[dave.jiang@intel.com: native_pud_clear missing on i386 build]
  Link: http://lkml.kernel.org/r/148640375195.69754.3315433724330910314.stgit@djiang5-desk3.ch.intel.com
Link: http://lkml.kernel.org/r/148545059381.17912.8602162635537598445.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
	Tested-by: Alexander Kapshuk <alexander.kapshuk@gmail.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Jan Kara <jack@suse.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Nilesh Choudhury <nilesh.choudhury@oracle.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a00cc7d9dd93d66a3fb83fc52aa57a4bec51c517)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/Kconfig
#	arch/x86/Kconfig
#	arch/x86/include/asm/pgtable-2level.h
#	arch/x86/include/asm/pgtable-3level.h
#	arch/x86/include/asm/pgtable.h
#	include/asm-generic/pgtable.h
#	include/asm-generic/tlb.h
#	include/linux/huge_mm.h
#	include/linux/mm.h
#	include/linux/mmu_notifier.h
#	mm/gup.c
#	mm/huge_memory.c
#	mm/memory.c
#	mm/pagewalk.c
#	mm/pgtable-generic.c
diff --cc arch/Kconfig
index 4fa0b0631a54,d0012add6b19..000000000000
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@@ -436,10 -571,13 +436,17 @@@ config HAVE_IRQ_TIME_ACCOUNTIN
  config HAVE_ARCH_TRANSPARENT_HUGEPAGE
  	bool
  
++<<<<<<< HEAD
 +config HAVE_ARCH_SOFT_DIRTY
++=======
+ config HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ 	bool
+ 
+ config HAVE_ARCH_HUGE_VMAP
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  	bool
  
 -config HAVE_ARCH_SOFT_DIRTY
 +config HAVE_ARCH_HUGE_VMAP
  	bool
  
  config HAVE_MOD_ARCH_SPECIFIC
diff --cc arch/x86/Kconfig
index 0912e61d8b57,33007aa74111..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -15,66 -21,141 +15,148 @@@ config X86_3
  config X86_64
  	def_bool y
  	depends on 64BIT
 -	# Options that are inherently 64-bit kernel only:
 -	select ARCH_HAS_GIGANTIC_PAGE
 -	select ARCH_SUPPORTS_INT128
 -	select ARCH_USE_CMPXCHG_LOCKREF
 -	select HAVE_ARCH_SOFT_DIRTY
 -	select MODULES_USE_ELF_RELA
  	select X86_DEV_DMA_OPS
 +	select ARCH_USE_CMPXCHG_LOCKREF
  
 -#
 -# Arch settings
 -#
 -# ( Note that options that are marked 'if X86_64' could in principle be
 -#   ported to 32-bit as well. )
 -#
 +### Arch settings
  config X86
  	def_bool y
 -	#
 -	# Note: keep this list sorted alphabetically
 -	#
 -	select ACPI_LEGACY_TABLES_LOOKUP	if ACPI
 -	select ACPI_SYSTEM_POWER_STATES_SUPPORT	if ACPI
 -	select ANON_INODES
 -	select ARCH_CLOCKSOURCE_DATA
 -	select ARCH_DISCARD_MEMBLOCK
 -	select ARCH_HAS_ACPI_TABLE_UPGRADE	if ACPI
 -	select ARCH_HAS_DEBUG_VIRTUAL
 -	select ARCH_HAS_DEVMEM_IS_ALLOWED
 -	select ARCH_HAS_ELF_RANDOMIZE
 -	select ARCH_HAS_FAST_MULTIPLIER
 -	select ARCH_HAS_GCOV_PROFILE_ALL
 -	select ARCH_HAS_KCOV			if X86_64
 -	select ARCH_HAS_MMIO_FLUSH
 +	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
  	select ARCH_HAS_PMEM_API		if X86_64
++<<<<<<< HEAD
 +	select ARCH_HAS_MMIO_FLUSH
 +	select HAVE_AOUT if X86_32
 +	select HAVE_UNSTABLE_SCHED_CLOCK
 +	select ARCH_SUPPORTS_NUMA_BALANCING
 +	select ARCH_SUPPORTS_INT128 if X86_64
 +	select ARCH_WANTS_PROT_NUMA_PROT_NONE
++=======
+ 	select ARCH_HAS_SET_MEMORY
+ 	select ARCH_HAS_SG_CHAIN
+ 	select ARCH_HAS_STRICT_KERNEL_RWX
+ 	select ARCH_HAS_STRICT_MODULE_RWX
+ 	select ARCH_HAS_UBSAN_SANITIZE_ALL
+ 	select ARCH_HAVE_NMI_SAFE_CMPXCHG
+ 	select ARCH_MIGHT_HAVE_ACPI_PDC		if ACPI
+ 	select ARCH_MIGHT_HAVE_PC_PARPORT
+ 	select ARCH_MIGHT_HAVE_PC_SERIO
+ 	select ARCH_SUPPORTS_ATOMIC_RMW
+ 	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
+ 	select ARCH_USE_BUILTIN_BSWAP
+ 	select ARCH_USE_QUEUED_RWLOCKS
+ 	select ARCH_USE_QUEUED_SPINLOCKS
+ 	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
+ 	select ARCH_WANT_FRAME_POINTERS
+ 	select ARCH_WANTS_DYNAMIC_TASK_STRUCT
+ 	select BUILDTIME_EXTABLE_SORT
+ 	select CLKEVT_I8253
+ 	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
+ 	select CLOCKSOURCE_WATCHDOG
+ 	select DCACHE_WORD_ACCESS
+ 	select EDAC_ATOMIC_SCRUB
+ 	select EDAC_SUPPORT
+ 	select GENERIC_CLOCKEVENTS
+ 	select GENERIC_CLOCKEVENTS_BROADCAST	if X86_64 || (X86_32 && X86_LOCAL_APIC)
+ 	select GENERIC_CLOCKEVENTS_MIN_ADJUST
+ 	select GENERIC_CMOS_UPDATE
+ 	select GENERIC_CPU_AUTOPROBE
+ 	select GENERIC_EARLY_IOREMAP
+ 	select GENERIC_FIND_FIRST_BIT
+ 	select GENERIC_IOMAP
+ 	select GENERIC_IRQ_PROBE
+ 	select GENERIC_IRQ_SHOW
+ 	select GENERIC_PENDING_IRQ		if SMP
+ 	select GENERIC_SMP_IDLE_THREAD
+ 	select GENERIC_STRNCPY_FROM_USER
+ 	select GENERIC_STRNLEN_USER
+ 	select GENERIC_TIME_VSYSCALL
+ 	select HAVE_ACPI_APEI			if ACPI
+ 	select HAVE_ACPI_APEI_NMI		if ACPI
+ 	select HAVE_ALIGNED_STRUCT_PAGE		if SLUB
+ 	select HAVE_ARCH_AUDITSYSCALL
+ 	select HAVE_ARCH_HARDENED_USERCOPY
+ 	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
+ 	select HAVE_ARCH_JUMP_LABEL
+ 	select HAVE_ARCH_KASAN			if X86_64 && SPARSEMEM_VMEMMAP
+ 	select HAVE_ARCH_KGDB
+ 	select HAVE_ARCH_KMEMCHECK
+ 	select HAVE_ARCH_MMAP_RND_BITS		if MMU
+ 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
+ 	select HAVE_ARCH_SECCOMP_FILTER
+ 	select HAVE_ARCH_TRACEHOOK
+ 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
+ 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD if X86_64
+ 	select HAVE_ARCH_VMAP_STACK		if X86_64
+ 	select HAVE_ARCH_WITHIN_STACK_FRAMES
+ 	select HAVE_CC_STACKPROTECTOR
+ 	select HAVE_CMPXCHG_DOUBLE
+ 	select HAVE_CMPXCHG_LOCAL
+ 	select HAVE_CONTEXT_TRACKING		if X86_64
+ 	select HAVE_COPY_THREAD_TLS
+ 	select HAVE_C_RECORDMCOUNT
+ 	select HAVE_DEBUG_KMEMLEAK
+ 	select HAVE_DEBUG_STACKOVERFLOW
+ 	select HAVE_DMA_API_DEBUG
+ 	select HAVE_DMA_CONTIGUOUS
+ 	select HAVE_DYNAMIC_FTRACE
+ 	select HAVE_DYNAMIC_FTRACE_WITH_REGS
+ 	select HAVE_EBPF_JIT			if X86_64
+ 	select HAVE_EFFICIENT_UNALIGNED_ACCESS
+ 	select HAVE_EXIT_THREAD
+ 	select HAVE_FENTRY			if X86_64
+ 	select HAVE_FTRACE_MCOUNT_RECORD
+ 	select HAVE_FUNCTION_GRAPH_TRACER
+ 	select HAVE_FUNCTION_TRACER
+ 	select HAVE_GCC_PLUGINS
+ 	select HAVE_HW_BREAKPOINT
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  	select HAVE_IDE
 +	select HAVE_OPROFILE
 +	select HAVE_PCSPKR_PLATFORM
 +	select HAVE_PERF_EVENTS
  	select HAVE_IOREMAP_PROT
 -	select HAVE_IRQ_EXIT_ON_IRQ_STACK	if X86_64
 -	select HAVE_IRQ_TIME_ACCOUNTING
 -	select HAVE_KERNEL_BZIP2
 -	select HAVE_KERNEL_GZIP
 -	select HAVE_KERNEL_LZ4
 -	select HAVE_KERNEL_LZMA
 -	select HAVE_KERNEL_LZO
 -	select HAVE_KERNEL_XZ
  	select HAVE_KPROBES
 -	select HAVE_KPROBES_ON_FTRACE
 -	select HAVE_KRETPROBES
 -	select HAVE_KVM
 -	select HAVE_LIVEPATCH			if X86_64
  	select HAVE_MEMBLOCK
  	select HAVE_MEMBLOCK_NODE_MAP
 -	select HAVE_MIXED_BREAKPOINTS_REGS
 -	select HAVE_NMI
 -	select HAVE_OPROFILE
 +	select ARCH_DISCARD_MEMBLOCK
 +	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 +	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
 +	select ARCH_WANT_OPTIONAL_GPIOLIB
 +	select ARCH_WANT_FRAME_POINTERS
 +	select HAVE_DMA_ATTRS
 +	select HAVE_DMA_CONTIGUOUS if !SWIOTLB
 +	select HAVE_KRETPROBES
  	select HAVE_OPTPROBES
 -	select HAVE_PCSPKR_PLATFORM
 -	select HAVE_PERF_EVENTS
 +	select HAVE_KPROBES_ON_FTRACE
 +	select HAVE_FTRACE_MCOUNT_RECORD
 +	select HAVE_FENTRY if X86_64
 +	select HAVE_ARCH_MMAP_RND_BITS		if MMU
 +	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
 +	select HAVE_C_RECORDMCOUNT
 +	select HAVE_DYNAMIC_FTRACE
 +	select HAVE_DYNAMIC_FTRACE_WITH_REGS
 +	select HAVE_FUNCTION_TRACER
 +	select HAVE_FUNCTION_GRAPH_TRACER
 +	select HAVE_FUNCTION_GRAPH_FP_TEST
 +	select HAVE_SYSCALL_TRACEPOINTS
 +	select SYSCTL_EXCEPTION_TRACE
 +	select HAVE_KVM
 +	select HAVE_ARCH_KGDB
 +	select HAVE_ARCH_TRACEHOOK
 +	select HAVE_GENERIC_DMA_COHERENT if X86_32
 +	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 +	select USER_STACKTRACE_SUPPORT
 +	select HAVE_REGS_AND_STACK_ACCESS_API
 +	select HAVE_DMA_API_DEBUG
 +	select HAVE_KERNEL_GZIP
 +	select HAVE_KERNEL_BZIP2
 +	select HAVE_KERNEL_LZMA
 +	select HAVE_KERNEL_XZ
 +	select HAVE_KERNEL_LZO
 +	select HAVE_HW_BREAKPOINT
 +	select HAVE_MIXED_BREAKPOINTS_REGS
 +	select PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
diff --cc arch/x86/include/asm/pgtable-2level.h
index 5316b4e531c5,a8b96e708c2b..000000000000
--- a/arch/x86/include/asm/pgtable-2level.h
+++ b/arch/x86/include/asm/pgtable-2level.h
@@@ -60,93 -63,25 +68,111 @@@ static inline pmd_t native_pmdp_get_and
  #define native_pmdp_get_and_clear(xp) native_local_pmdp_get_and_clear(xp)
  #endif
  
++<<<<<<< HEAD
 +#ifdef CONFIG_MEM_SOFT_DIRTY
 +
 +/*
 + * Bits _PAGE_BIT_PRESENT, _PAGE_BIT_FILE, _PAGE_BIT_SOFT_DIRTY and
 + * _PAGE_BIT_PROTNONE are taken, split up the 28 bits of offset
 + * into this range.
 + */
 +#define PTE_FILE_MAX_BITS	28
 +#define PTE_FILE_SHIFT1		(_PAGE_BIT_PRESENT + 1)
 +#define PTE_FILE_SHIFT2		(_PAGE_BIT_FILE + 1)
 +#define PTE_FILE_SHIFT3		(_PAGE_BIT_PROTNONE + 1)
 +#define PTE_FILE_SHIFT4		(_PAGE_BIT_SOFT_DIRTY + 1)
 +#define PTE_FILE_BITS1		(PTE_FILE_SHIFT2 - PTE_FILE_SHIFT1 - 1)
 +#define PTE_FILE_BITS2		(PTE_FILE_SHIFT3 - PTE_FILE_SHIFT2 - 1)
 +#define PTE_FILE_BITS3		(PTE_FILE_SHIFT4 - PTE_FILE_SHIFT3 - 1)
 +
 +#define pte_to_pgoff(pte)						\
 +	((((pte).pte_low >> (PTE_FILE_SHIFT1))				\
 +	  & ((1U << PTE_FILE_BITS1) - 1)))				\
 +	+ ((((pte).pte_low >> (PTE_FILE_SHIFT2))			\
 +	    & ((1U << PTE_FILE_BITS2) - 1))				\
 +	   << (PTE_FILE_BITS1))						\
 +	+ ((((pte).pte_low >> (PTE_FILE_SHIFT3))			\
 +	    & ((1U << PTE_FILE_BITS3) - 1))				\
 +	   << (PTE_FILE_BITS1 + PTE_FILE_BITS2))			\
 +	+ ((((pte).pte_low >> (PTE_FILE_SHIFT4)))			\
 +	    << (PTE_FILE_BITS1 + PTE_FILE_BITS2 + PTE_FILE_BITS3))
 +
 +#define pgoff_to_pte(off)						\
 +	((pte_t) { .pte_low =						\
 +	 ((((off)) & ((1U << PTE_FILE_BITS1) - 1)) << PTE_FILE_SHIFT1)	\
 +	 + ((((off) >> PTE_FILE_BITS1)					\
 +	     & ((1U << PTE_FILE_BITS2) - 1))				\
 +	    << PTE_FILE_SHIFT2)						\
 +	 + ((((off) >> (PTE_FILE_BITS1 + PTE_FILE_BITS2))		\
 +	     & ((1U << PTE_FILE_BITS3) - 1))				\
 +	    << PTE_FILE_SHIFT3)						\
 +	 + ((((off) >>							\
 +	      (PTE_FILE_BITS1 + PTE_FILE_BITS2 + PTE_FILE_BITS3)))	\
 +	    << PTE_FILE_SHIFT4)						\
 +	 + _PAGE_FILE })
 +
 +#else /* CONFIG_MEM_SOFT_DIRTY */
 +
 +/*
 + * Bits _PAGE_BIT_PRESENT, _PAGE_BIT_FILE and _PAGE_BIT_PROTNONE are taken,
 + * split up the 29 bits of offset into this range.
 + */
 +#define PTE_FILE_MAX_BITS	29
 +#define PTE_FILE_SHIFT1		(_PAGE_BIT_PRESENT + 1)
 +#if _PAGE_BIT_FILE < _PAGE_BIT_PROTNONE
 +#define PTE_FILE_SHIFT2		(_PAGE_BIT_FILE + 1)
 +#define PTE_FILE_SHIFT3		(_PAGE_BIT_PROTNONE + 1)
 +#else
 +#define PTE_FILE_SHIFT2		(_PAGE_BIT_PROTNONE + 1)
 +#define PTE_FILE_SHIFT3		(_PAGE_BIT_FILE + 1)
 +#endif
 +#define PTE_FILE_BITS1		(PTE_FILE_SHIFT2 - PTE_FILE_SHIFT1 - 1)
 +#define PTE_FILE_BITS2		(PTE_FILE_SHIFT3 - PTE_FILE_SHIFT2 - 1)
 +
 +#define pte_to_pgoff(pte)						\
 +	((((pte).pte_low >> PTE_FILE_SHIFT1)				\
 +	  & ((1U << PTE_FILE_BITS1) - 1))				\
 +	 + ((((pte).pte_low >> PTE_FILE_SHIFT2)				\
 +	     & ((1U << PTE_FILE_BITS2) - 1)) << PTE_FILE_BITS1)		\
 +	 + (((pte).pte_low >> PTE_FILE_SHIFT3)				\
 +	    << (PTE_FILE_BITS1 + PTE_FILE_BITS2)))
 +
 +#define pgoff_to_pte(off)						\
 +	((pte_t) { .pte_low =						\
 +	 (((off) & ((1U << PTE_FILE_BITS1) - 1)) << PTE_FILE_SHIFT1)	\
 +	 + ((((off) >> PTE_FILE_BITS1) & ((1U << PTE_FILE_BITS2) - 1))	\
 +	    << PTE_FILE_SHIFT2)						\
 +	 + (((off) >> (PTE_FILE_BITS1 + PTE_FILE_BITS2))		\
 +	    << PTE_FILE_SHIFT3)						\
 +	 + _PAGE_FILE })
 +
 +#endif /* CONFIG_MEM_SOFT_DIRTY */
++=======
+ #ifdef CONFIG_SMP
+ static inline pud_t native_pudp_get_and_clear(pud_t *xp)
+ {
+ 	return __pud(xchg((pudval_t *)xp, 0));
+ }
+ #else
+ #define native_pudp_get_and_clear(xp) native_local_pudp_get_and_clear(xp)
+ #endif
+ 
+ /* Bit manipulation helper on pte/pgoff entry */
+ static inline unsigned long pte_bitop(unsigned long value, unsigned int rightshift,
+ 				      unsigned long mask, unsigned int leftshift)
+ {
+ 	return ((value >> rightshift) & mask) << leftshift;
+ }
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
  /* Encode and de-code a swap entry */
 -#define SWP_TYPE_BITS 5
 +#if _PAGE_BIT_FILE < _PAGE_BIT_PROTNONE
 +#define SWP_TYPE_BITS (_PAGE_BIT_FILE - _PAGE_BIT_PRESENT - 1)
  #define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 1)
 +#else
 +#define SWP_TYPE_BITS (_PAGE_BIT_PROTNONE - _PAGE_BIT_PRESENT - 1)
 +#define SWP_OFFSET_SHIFT (_PAGE_BIT_FILE + 1)
 +#endif
  
  #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > SWP_TYPE_BITS)
  
diff --cc arch/x86/include/asm/pgtable-3level.h
index 2ead1f1201a8,8f50fb3f04e1..000000000000
--- a/arch/x86/include/asm/pgtable-3level.h
+++ b/arch/x86/include/asm/pgtable-3level.h
@@@ -129,9 -121,14 +129,15 @@@ static inline void native_pmd_clear(pmd
  	*(tmp + 1) = 0;
  }
  
+ #ifndef CONFIG_SMP
+ static inline void native_pud_clear(pud_t *pudp)
+ {
+ }
+ #endif
+ 
  static inline void pud_clear(pud_t *pudp)
  {
 +	mm_track_pud(pudp);
  	set_pud(pudp, __pud(0));
  
  	/*
@@@ -189,17 -182,29 +195,43 @@@ static inline pmd_t native_pmdp_get_and
  #define native_pmdp_get_and_clear(xp) native_local_pmdp_get_and_clear(xp)
  #endif
  
++<<<<<<< HEAD
 +/*
 + * Bits 0, 6 and 7 are taken in the low part of the pte,
 + * put the 32 bits of offset into the high part.
 + *
 + * For soft-dirty tracking 11 bit is taken from
 + * the low part of pte as well.
 + */
 +#define pte_to_pgoff(pte) ((pte).pte_high)
 +#define pgoff_to_pte(off)						\
 +	((pte_t) { { .pte_low = _PAGE_FILE, .pte_high = (off) } })
 +#define PTE_FILE_MAX_BITS       32
++=======
+ #ifdef CONFIG_SMP
+ union split_pud {
+ 	struct {
+ 		u32 pud_low;
+ 		u32 pud_high;
+ 	};
+ 	pud_t pud;
+ };
+ 
+ static inline pud_t native_pudp_get_and_clear(pud_t *pudp)
+ {
+ 	union split_pud res, *orig = (union split_pud *)pudp;
+ 
+ 	/* xchg acts as a barrier before setting of the high bits */
+ 	res.pud_low = xchg(&orig->pud_low, 0);
+ 	res.pud_high = orig->pud_high;
+ 	orig->pud_high = 0;
+ 
+ 	return res.pud;
+ }
+ #else
+ #define native_pudp_get_and_clear(xp) native_local_pudp_get_and_clear(xp)
+ #endif
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
  /* Encode and de-code a swap entry */
  #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > 5)
diff --cc arch/x86/include/asm/pgtable.h
index 50bdb1a560e0,1cfb36b8c024..000000000000
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@@ -170,20 -187,42 +181,53 @@@ static inline int pmd_large(pmd_t pte
  }
  
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +static inline int pmd_trans_splitting(pmd_t pmd)
 +{
 +	return pmd_val(pmd) & _PAGE_SPLITTING;
 +}
 +
  static inline int pmd_trans_huge(pmd_t pmd)
  {
 -	return (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;
 +	return pmd_val(pmd) & _PAGE_PSE;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static inline int pud_trans_huge(pud_t pud)
+ {
+ 	return (pud_val(pud) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;
+ }
+ #endif
+ 
+ #define has_transparent_hugepage has_transparent_hugepage
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  static inline int has_transparent_hugepage(void)
  {
 -	return boot_cpu_has(X86_FEATURE_PSE);
 +	return cpu_has_pse;
  }
++<<<<<<< HEAD
++=======
+ 
+ #ifdef __HAVE_ARCH_PTE_DEVMAP
+ static inline int pmd_devmap(pmd_t pmd)
+ {
+ 	return !!(pmd_val(pmd) & _PAGE_DEVMAP);
+ }
+ 
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static inline int pud_devmap(pud_t pud)
+ {
+ 	return !!(pud_val(pud) & _PAGE_DEVMAP);
+ }
+ #else
+ static inline int pud_devmap(pud_t pud)
+ {
+ 	return 0;
+ }
+ #endif
+ #endif
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  
  static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
@@@ -316,9 -360,69 +360,72 @@@ static inline pmd_t pmd_mkwrite(pmd_t p
  
  static inline pmd_t pmd_mknotpresent(pmd_t pmd)
  {
 -	return pmd_clear_flags(pmd, _PAGE_PRESENT | _PAGE_PROTNONE);
 +	return pmd_clear_flags(pmd, _PAGE_PRESENT);
  }
  
++<<<<<<< HEAD
++=======
+ static inline pud_t pud_set_flags(pud_t pud, pudval_t set)
+ {
+ 	pudval_t v = native_pud_val(pud);
+ 
+ 	return __pud(v | set);
+ }
+ 
+ static inline pud_t pud_clear_flags(pud_t pud, pudval_t clear)
+ {
+ 	pudval_t v = native_pud_val(pud);
+ 
+ 	return __pud(v & ~clear);
+ }
+ 
+ static inline pud_t pud_mkold(pud_t pud)
+ {
+ 	return pud_clear_flags(pud, _PAGE_ACCESSED);
+ }
+ 
+ static inline pud_t pud_mkclean(pud_t pud)
+ {
+ 	return pud_clear_flags(pud, _PAGE_DIRTY);
+ }
+ 
+ static inline pud_t pud_wrprotect(pud_t pud)
+ {
+ 	return pud_clear_flags(pud, _PAGE_RW);
+ }
+ 
+ static inline pud_t pud_mkdirty(pud_t pud)
+ {
+ 	return pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pud_t pud_mkdevmap(pud_t pud)
+ {
+ 	return pud_set_flags(pud, _PAGE_DEVMAP);
+ }
+ 
+ static inline pud_t pud_mkhuge(pud_t pud)
+ {
+ 	return pud_set_flags(pud, _PAGE_PSE);
+ }
+ 
+ static inline pud_t pud_mkyoung(pud_t pud)
+ {
+ 	return pud_set_flags(pud, _PAGE_ACCESSED);
+ }
+ 
+ static inline pud_t pud_mkwrite(pud_t pud)
+ {
+ 	return pud_set_flags(pud, _PAGE_RW);
+ }
+ 
+ static inline pud_t pud_mknotpresent(pud_t pud)
+ {
+ 	return pud_clear_flags(pud, _PAGE_PRESENT | _PAGE_PROTNONE);
+ }
+ 
+ #ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  static inline int pte_soft_dirty(pte_t pte)
  {
  	return pte_flags(pte) & _PAGE_SOFT_DIRTY;
@@@ -339,35 -448,27 +451,53 @@@ static inline pmd_t pmd_mksoft_dirty(pm
  	return pmd_set_flags(pmd, _PAGE_SOFT_DIRTY);
  }
  
++<<<<<<< HEAD
 +static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
 +{
 +	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
 +}
 +
 +static inline int pte_swp_soft_dirty(pte_t pte)
 +{
 +	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
 +}
 +
 +static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
 +{
 +	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
 +}
 +
 +static inline pte_t pte_file_clear_soft_dirty(pte_t pte)
++=======
+ static inline pud_t pud_mksoft_dirty(pud_t pud)
+ {
+ 	return pud_set_flags(pud, _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pte_t pte_clear_soft_dirty(pte_t pte)
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  {
  	return pte_clear_flags(pte, _PAGE_SOFT_DIRTY);
  }
  
 -static inline pmd_t pmd_clear_soft_dirty(pmd_t pmd)
 +static inline pte_t pte_file_mksoft_dirty(pte_t pte)
  {
 -	return pmd_clear_flags(pmd, _PAGE_SOFT_DIRTY);
 +	return pte_set_flags(pte, _PAGE_SOFT_DIRTY);
  }
  
++<<<<<<< HEAD
 +static inline int pte_file_soft_dirty(pte_t pte)
 +{
 +	return pte_flags(pte) & _PAGE_SOFT_DIRTY;
 +}
++=======
+ static inline pud_t pud_clear_soft_dirty(pud_t pud)
+ {
+ 	return pud_clear_flags(pud, _PAGE_SOFT_DIRTY);
+ }
+ 
+ #endif /* CONFIG_HAVE_ARCH_SOFT_DIRTY */
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
  /*
   * Mask out unsupported bits in a present pgprot.  Non-present pgprots
@@@ -870,15 -1006,20 +1025,22 @@@ static inline int pmd_write(pmd_t pmd
  	return pmd_flags(pmd) & _PAGE_RW;
  }
  
 -#define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR
 -static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,
 +#define __HAVE_ARCH_PMDP_GET_AND_CLEAR
 +static inline pmd_t pmdp_get_and_clear(struct mm_struct *mm, unsigned long addr,
  				       pmd_t *pmdp)
  {
 -	return native_pmdp_get_and_clear(pmdp);
 +	pmd_t pmd = native_pmdp_get_and_clear(pmdp);
 +	pmd_update(mm, addr, pmdp);
 +	return pmd;
  }
  
+ #define __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR
+ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
+ 					unsigned long addr, pud_t *pudp)
+ {
+ 	return native_pudp_get_and_clear(pudp);
+ }
+ 
  #define __HAVE_ARCH_PMDP_SET_WRPROTECT
  static inline void pmdp_set_wrprotect(struct mm_struct *mm,
  				      unsigned long addr, pmd_t *pmdp)
@@@ -928,7 -1068,58 +1090,11 @@@ static inline void update_mmu_cache_pmd
  		unsigned long addr, pmd_t *pmd)
  {
  }
+ static inline void update_mmu_cache_pud(struct vm_area_struct *vma,
+ 		unsigned long addr, pud_t *pud)
+ {
+ }
  
 -#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
 -static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
 -{
 -	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
 -}
 -
 -static inline int pte_swp_soft_dirty(pte_t pte)
 -{
 -	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
 -}
 -
 -static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
 -{
 -	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
 -}
 -#endif
 -
 -#define PKRU_AD_BIT 0x1
 -#define PKRU_WD_BIT 0x2
 -#define PKRU_BITS_PER_PKEY 2
 -
 -static inline bool __pkru_allows_read(u32 pkru, u16 pkey)
 -{
 -	int pkru_pkey_bits = pkey * PKRU_BITS_PER_PKEY;
 -	return !(pkru & (PKRU_AD_BIT << pkru_pkey_bits));
 -}
 -
 -static inline bool __pkru_allows_write(u32 pkru, u16 pkey)
 -{
 -	int pkru_pkey_bits = pkey * PKRU_BITS_PER_PKEY;
 -	/*
 -	 * Access-disable disables writes too so we need to check
 -	 * both bits here.
 -	 */
 -	return !(pkru & ((PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits));
 -}
 -
 -static inline u16 pte_flags_pkey(unsigned long pte_flags)
 -{
 -#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
 -	/* ifdef to avoid doing 59-bit shift on 32-bit values */
 -	return (pte_flags & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;
 -#else
 -	return 0;
 -#endif
 -}
 -
  #include <asm-generic/pgtable.h>
  #endif	/* __ASSEMBLY__ */
  
diff --cc include/asm-generic/pgtable.h
index 987c42c28cfd,a0aba0f9c57b..000000000000
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@@ -30,6 -35,26 +30,28 @@@ extern int ptep_set_access_flags(struc
  extern int pmdp_set_access_flags(struct vm_area_struct *vma,
  				 unsigned long address, pmd_t *pmdp,
  				 pmd_t entry, int dirty);
++<<<<<<< HEAD
++=======
+ extern int pudp_set_access_flags(struct vm_area_struct *vma,
+ 				 unsigned long address, pud_t *pudp,
+ 				 pud_t entry, int dirty);
+ #else
+ static inline int pmdp_set_access_flags(struct vm_area_struct *vma,
+ 					unsigned long address, pmd_t *pmdp,
+ 					pmd_t entry, int dirty)
+ {
+ 	BUILD_BUG();
+ 	return 0;
+ }
+ static inline int pudp_set_access_flags(struct vm_area_struct *vma,
+ 					unsigned long address, pud_t *pudp,
+ 					pud_t entry, int dirty)
+ {
+ 	BUILD_BUG();
+ 	return 0;
+ }
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #endif
  
  #ifndef __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG
@@@ -93,19 -131,50 +115,61 @@@ static inline pte_t ptep_get_and_clear(
  }
  #endif
  
++<<<<<<< HEAD
 +#ifndef __HAVE_ARCH_PMDP_GET_AND_CLEAR
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +static inline pmd_t pmdp_get_and_clear(struct mm_struct *mm,
 +				       unsigned long address,
 +				       pmd_t *pmdp)
++=======
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ #ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR
+ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
+ 					    unsigned long address,
+ 					    pmd_t *pmdp)
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  {
  	pmd_t pmd = *pmdp;
  	pmd_clear(pmdp);
  	return pmd;
  }
+ #endif /* __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR */
+ #ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR
+ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
+ 					    unsigned long address,
+ 					    pud_t *pudp)
+ {
+ 	pud_t pud = *pudp;
+ 
+ 	pud_clear(pudp);
+ 	return pud;
+ }
+ #endif /* __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR */
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+ 
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ #ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR_FULL
+ static inline pmd_t pmdp_huge_get_and_clear_full(struct mm_struct *mm,
+ 					    unsigned long address, pmd_t *pmdp,
+ 					    int full)
+ {
+ 	return pmdp_huge_get_and_clear(mm, address, pmdp);
+ }
  #endif
  
+ #ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR_FULL
+ static inline pud_t pudp_huge_get_and_clear_full(struct mm_struct *mm,
+ 					    unsigned long address, pud_t *pudp,
+ 					    int full)
+ {
+ 	return pudp_huge_get_and_clear(mm, address, pudp);
+ }
+ #endif
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+ 
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #ifndef __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL
  static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
  					    unsigned long address, pte_t *ptep,
@@@ -138,10 -207,13 +202,13 @@@ extern pte_t ptep_clear_flush(struct vm
  			      pte_t *ptep);
  #endif
  
 -#ifndef __HAVE_ARCH_PMDP_HUGE_CLEAR_FLUSH
 -extern pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma,
 +#ifndef __HAVE_ARCH_PMDP_CLEAR_FLUSH
 +extern pmd_t pmdp_clear_flush(struct vm_area_struct *vma,
  			      unsigned long address,
  			      pmd_t *pmdp);
+ extern pud_t pudp_huge_clear_flush(struct vm_area_struct *vma,
+ 			      unsigned long address,
+ 			      pud_t *pudp);
  #endif
  
  #ifndef __HAVE_ARCH_PTEP_SET_WRPROTECT
@@@ -169,10 -241,38 +236,27 @@@ static inline void pmdp_set_wrprotect(s
  }
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  #endif
+ #ifndef __HAVE_ARCH_PUDP_SET_WRPROTECT
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static inline void pudp_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long address, pud_t *pudp)
+ {
+ 	pud_t old_pud = *pudp;
+ 
+ 	set_pud_at(mm, address, pudp, pud_wrprotect(old_pud));
+ }
+ #else
+ static inline void pudp_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long address, pud_t *pudp)
+ {
+ 	BUILD_BUG();
+ }
+ #endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
+ #endif
  
 -#ifndef pmdp_collapse_flush
 -#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,
 +#ifndef __HAVE_ARCH_PMDP_SPLITTING_FLUSH
 +extern void pmdp_splitting_flush(struct vm_area_struct *vma,
  				 unsigned long address, pmd_t *pmdp);
 -#else
 -static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,
 -					unsigned long address,
 -					pmd_t *pmdp)
 -{
 -	BUILD_BUG();
 -	return *pmdp;
 -}
 -#define pmdp_collapse_flush pmdp_collapse_flush
 -#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  #endif
  
  #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
@@@ -205,9 -331,15 +294,15 @@@ static inline int pud_same(pud_t pud_a
  #else /* CONFIG_TRANSPARENT_HUGEPAGE */
  static inline int pmd_same(pmd_t pmd_a, pmd_t pmd_b)
  {
 -	BUILD_BUG();
 +	BUG();
  	return 0;
  }
+ 
+ static inline int pud_same(pud_t pud_a, pud_t pud_b)
+ {
+ 	BUILD_BUG();
+ 	return 0;
+ }
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  #endif
  
@@@ -857,6 -843,40 +961,31 @@@ static inline int pmd_clear_huge(pmd_t 
  }
  #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
  
++<<<<<<< HEAD
++=======
+ #ifndef __HAVE_ARCH_FLUSH_PMD_TLB_RANGE
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ /*
+  * ARCHes with special requirements for evicting THP backing TLB entries can
+  * implement this. Otherwise also, it can help optimize normal TLB flush in
+  * THP regime. stock flush_tlb_range() typically has optimization to nuke the
+  * entire TLB TLB if flush span is greater than a threshold, which will
+  * likely be true for a single huge page. Thus a single thp flush will
+  * invalidate the entire TLB which is not desitable.
+  * e.g. see arch/arc: flush_pmd_tlb_range
+  */
+ #define flush_pmd_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
+ #define flush_pud_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
+ #else
+ #define flush_pmd_tlb_range(vma, addr, end)	BUILD_BUG()
+ #define flush_pud_tlb_range(vma, addr, end)	BUILD_BUG()
+ #endif
+ #endif
+ 
+ struct file;
+ int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,
+ 			unsigned long size, pgprot_t *vma_prot);
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #endif /* !__ASSEMBLY__ */
  
 -#ifndef io_remap_pfn_range
 -#define io_remap_pfn_range remap_pfn_range
 -#endif
 -
 -#ifndef has_transparent_hugepage
 -#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -#define has_transparent_hugepage() 1
 -#else
 -#define has_transparent_hugepage() 0
 -#endif
 -#endif
 -
  #endif /* _ASM_GENERIC_PGTABLE_H */
diff --cc include/asm-generic/tlb.h
index 5672d7ea1fa0,4329bc6ef04b..000000000000
--- a/include/asm-generic/tlb.h
+++ b/include/asm-generic/tlb.h
@@@ -149,15 -226,47 +149,50 @@@ static inline void tlb_remove_page(stru
  #define __tlb_remove_pmd_tlb_entry(tlb, pmdp, address) do {} while (0)
  #endif
  
 -#define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)			\
 -	do {								\
 -		__tlb_adjust_range(tlb, address, HPAGE_PMD_SIZE);	\
 -		__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);		\
 +#define tlb_remove_pmd_tlb_entry(tlb, pmdp, address)		\
 +	do {							\
 +		tlb->need_flush = 1;				\
 +		__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);	\
  	} while (0)
  
++<<<<<<< HEAD
++=======
+ /**
+  * tlb_remove_pud_tlb_entry - remember a pud mapping for later tlb
+  * invalidation. This is a nop so far, because only x86 needs it.
+  */
+ #ifndef __tlb_remove_pud_tlb_entry
+ #define __tlb_remove_pud_tlb_entry(tlb, pudp, address) do {} while (0)
+ #endif
+ 
+ #define tlb_remove_pud_tlb_entry(tlb, pudp, address)			\
+ 	do {								\
+ 		__tlb_adjust_range(tlb, address, HPAGE_PUD_SIZE);	\
+ 		__tlb_remove_pud_tlb_entry(tlb, pudp, address);		\
+ 	} while (0)
+ 
+ /*
+  * For things like page tables caches (ie caching addresses "inside" the
+  * page tables, like x86 does), for legacy reasons, flushing an
+  * individual page had better flush the page table caches behind it. This
+  * is definitely how x86 works, for example. And if you have an
+  * architected non-legacy page table cache (which I'm not aware of
+  * anybody actually doing), you're going to have some architecturally
+  * explicit flushing for that, likely *separate* from a regular TLB entry
+  * flush, and thus you'd need more than just some range expansion..
+  *
+  * So if we ever find an architecture
+  * that would want something that odd, I think it is up to that
+  * architecture to do its own odd thing, not cause pain for others
+  * http://lkml.kernel.org/r/CA+55aFzBggoXtNXQeng5d_mRoDnaMBE5Y+URs+PHR67nUpMtaw@mail.gmail.com
+  *
+  * For now w.r.t page table cache, mark the range_size as PAGE_SIZE
+  */
+ 
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #define pte_free_tlb(tlb, ptep, address)			\
  	do {							\
 -		__tlb_adjust_range(tlb, address, PAGE_SIZE);	\
 +		tlb->need_flush = 1;				\
  		__pte_free_tlb(tlb, ptep, address);		\
  	} while (0)
  
diff --cc include/linux/huge_mm.h
index 2f1205c8c5a1,a3762d49ba39..000000000000
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@@ -8,13 -5,20 +8,30 @@@ extern int do_huge_pmd_anonymous_page(s
  extern int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
  			 pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
  			 struct vm_area_struct *vma);
++<<<<<<< HEAD
 +extern void huge_pmd_set_accessed(struct mm_struct *mm,
 +				  struct vm_area_struct *vma,
 +				  unsigned long address, pmd_t *pmd,
 +				  pmd_t orig_pmd, int dirty);
 +extern int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
 +			       unsigned long address, pmd_t *pmd,
 +			       pmd_t orig_pmd);
++=======
+ extern void huge_pmd_set_accessed(struct vm_fault *vmf, pmd_t orig_pmd);
+ extern int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+ 			 pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
+ 			 struct vm_area_struct *vma);
+ 
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ extern void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud);
+ #else
+ static inline void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud)
+ {
+ }
+ #endif
+ 
+ extern int do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd);
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  extern struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
  					  unsigned long addr,
  					  pmd_t *pmd,
@@@ -33,10 -41,10 +53,17 @@@ extern int move_huge_pmd(struct vm_area
  extern int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
  			unsigned long addr, pgprot_t newprot,
  			int prot_numa);
++<<<<<<< HEAD
 +int vmf_insert_pfn_pmd(struct vm_area_struct *, unsigned long addr, pmd_t *,
 +			pfn_t pfn, bool write);
 +extern void put_huge_zero_page(void);
 +
++=======
+ int vmf_insert_pfn_pmd(struct vm_area_struct *vma, unsigned long addr,
+ 			pmd_t *pmd, pfn_t pfn, bool write);
+ int vmf_insert_pfn_pud(struct vm_area_struct *vma, unsigned long addr,
+ 			pud_t *pud, pfn_t pfn, bool write);
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  enum transparent_hugepage_flag {
  	TRANSPARENT_HUGEPAGE_FLAG,
  	TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,
@@@ -105,46 -118,64 +136,81 @@@ static inline int split_huge_page(struc
  {
  	return split_huge_page_to_list(page, NULL);
  }
 -void deferred_split_huge_page(struct page *page);
 -
 -void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 -		unsigned long address, bool freeze, struct page *page);
 -
 -#define split_huge_pmd(__vma, __pmd, __address)				\
 +extern void __split_huge_page_pmd(struct vm_area_struct *vma,
 +		unsigned long address, pmd_t *pmd);
 +#define split_huge_page_pmd(__vma, __address, __pmd)			\
  	do {								\
  		pmd_t *____pmd = (__pmd);				\
 -		if (pmd_trans_huge(*____pmd)				\
 -					|| pmd_devmap(*____pmd))	\
 -			__split_huge_pmd(__vma, __pmd, __address,	\
 -						false, NULL);		\
 +		if (unlikely(pmd_trans_huge(*____pmd)))			\
 +			__split_huge_page_pmd(__vma, __address,		\
 +					____pmd);			\
  	}  while (0)
++<<<<<<< HEAD
 +#define wait_split_huge_page(__anon_vma, __pmd)				\
 +	do {								\
 +		pmd_t *____pmd = (__pmd);				\
 +		anon_vma_lock_write(__anon_vma);			\
 +		anon_vma_unlock_write(__anon_vma);			\
 +		BUG_ON(pmd_trans_splitting(*____pmd) ||			\
 +		       pmd_trans_huge(*____pmd));			\
 +	} while (0)
 +extern void split_huge_page_pmd_mm(struct mm_struct *mm, unsigned long address,
 +		pmd_t *pmd);
 +#if HPAGE_PMD_ORDER > MAX_ORDER
 +#error "hugepages can't be allocated by the buddy allocator"
 +#endif
++=======
+ 
+ 
+ void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,
+ 		bool freeze, struct page *page);
+ 
+ void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,
+ 		unsigned long address);
+ 
+ #define split_huge_pud(__vma, __pud, __address)				\
+ 	do {								\
+ 		pud_t *____pud = (__pud);				\
+ 		if (pud_trans_huge(*____pud)				\
+ 					|| pud_devmap(*____pud))	\
+ 			__split_huge_pud(__vma, __pud, __address);	\
+ 	}  while (0)
+ 
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  extern int hugepage_madvise(struct vm_area_struct *vma,
  			    unsigned long *vm_flags, int advice);
  extern void vma_adjust_trans_huge(struct vm_area_struct *vma,
  				    unsigned long start,
  				    unsigned long end,
  				    long adjust_next);
++<<<<<<< HEAD
 +extern int __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,
 +		spinlock_t **ptl);
++=======
+ extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,
+ 		struct vm_area_struct *vma);
+ extern spinlock_t *__pud_trans_huge_lock(pud_t *pud,
+ 		struct vm_area_struct *vma);
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  /* mmap_sem must be held on entry */
 -static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,
 -		struct vm_area_struct *vma)
 +static inline int pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,
 +		spinlock_t **ptl)
  {
 -	VM_BUG_ON_VMA(!rwsem_is_locked(&vma->vm_mm->mmap_sem), vma);
 -	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))
 -		return __pmd_trans_huge_lock(pmd, vma);
 +	VM_BUG_ON(!rwsem_is_locked(&vma->vm_mm->mmap_sem));
 +	if (pmd_trans_huge(*pmd))
 +		return __pmd_trans_huge_lock(pmd, vma, ptl);
  	else
 -		return NULL;
 +		return 0;
  }
+ static inline spinlock_t *pud_trans_huge_lock(pud_t *pud,
+ 		struct vm_area_struct *vma)
+ {
+ 	VM_BUG_ON_VMA(!rwsem_is_locked(&vma->vm_mm->mmap_sem), vma);
+ 	if (pud_trans_huge(*pud) || pud_devmap(*pud))
+ 		return __pud_trans_huge_lock(pud, vma);
+ 	else
+ 		return NULL;
+ }
  static inline int hpage_nr_pages(struct page *page)
  {
  	if (unlikely(PageTransHuge(page)))
@@@ -152,13 -183,12 +218,22 @@@
  	return 1;
  }
  
++<<<<<<< HEAD
 +extern int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 +				unsigned long addr, pmd_t pmd, pmd_t *pmdp);
 +
 +static inline bool is_trans_huge_page_release(struct page *page)
 +{
 +	return (unsigned long) page & 1;
 +}
++=======
+ struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,
+ 		pmd_t *pmd, int flags);
+ struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,
+ 		pud_t *pud, int flags);
+ 
+ extern int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t orig_pmd);
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
  extern struct page *huge_zero_page;
  
@@@ -232,7 -202,15 +307,19 @@@ static inline bool is_huge_zero_pmd(pmd
  	return is_huge_zero_page(pmd_page(pmd));
  }
  
++<<<<<<< HEAD
 +struct page *get_huge_zero_page(void);
++=======
+ static inline bool is_huge_zero_pud(pud_t pud)
+ {
+ 	return false;
+ }
+ 
+ struct page *mm_get_huge_zero_page(struct mm_struct *mm);
+ void mm_put_huge_zero_page(struct mm_struct *mm);
+ 
+ #define mk_huge_pmd(page, prot) pmd_mkhuge(mk_pmd(page, prot))
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
  #else /* CONFIG_TRANSPARENT_HUGEPAGE */
  #define HPAGE_PMD_SHIFT ({ BUILD_BUG(); 0; })
@@@ -253,12 -240,18 +344,24 @@@ static inline int split_huge_page(struc
  {
  	return 0;
  }
 -static inline void deferred_split_huge_page(struct page *page) {}
 -#define split_huge_pmd(__vma, __pmd, __address)	\
 +#define split_huge_page_pmd(__vma, __address, __pmd)	\
  	do { } while (0)
 +#define wait_split_huge_page(__anon_vma, __pmd)	\
 +	do { } while (0)
 +#define split_huge_page_pmd_mm(__mm, __address, __pmd)	\
 +	do { } while (0)
++<<<<<<< HEAD
++=======
+ 
+ static inline void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+ 		unsigned long address, bool freeze, struct page *page) {}
+ static inline void split_huge_pmd_address(struct vm_area_struct *vma,
+ 		unsigned long address, bool freeze, struct page *page) {}
+ 
+ #define split_huge_pud(__vma, __pmd, __address)	\
+ 	do { } while (0)
+ 
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  static inline int hugepage_madvise(struct vm_area_struct *vma,
  				   unsigned long *vm_flags, int advice)
  {
@@@ -271,8 -264,18 +374,23 @@@ static inline void vma_adjust_trans_hug
  					 long adjust_next)
  {
  }
++<<<<<<< HEAD
 +static inline int pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,
 +		spinlock_t **ptl)
++=======
+ static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,
+ 		struct vm_area_struct *vma)
+ {
+ 	return NULL;
+ }
+ static inline spinlock_t *pud_trans_huge_lock(pud_t *pud,
+ 		struct vm_area_struct *vma)
+ {
+ 	return NULL;
+ }
+ 
+ static inline int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t orig_pmd)
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  {
  	return 0;
  }
@@@ -306,6 -285,27 +424,30 @@@ static inline bool is_huge_zero_page(st
  	return false;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool is_huge_zero_pud(pud_t pud)
+ {
+ 	return false;
+ }
+ 
+ static inline void mm_put_huge_zero_page(struct mm_struct *mm)
+ {
+ 	return;
+ }
+ 
+ static inline struct page *follow_devmap_pmd(struct vm_area_struct *vma,
+ 		unsigned long addr, pmd_t *pmd, int flags)
+ {
+ 	return NULL;
+ }
+ 
+ static inline struct page *follow_devmap_pud(struct vm_area_struct *vma,
+ 		unsigned long addr, pud_t *pud, int flags)
+ {
+ 	return NULL;
+ }
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  
  #endif /* _LINUX_HUGE_MM_H */
diff --cc include/linux/mm.h
index 3416fff96060,d8b75d7d6a9e..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -314,17 -419,16 +314,25 @@@ struct inode
  #define page_private(page)		((page)->private)
  #define set_page_private(page, v)	((page)->private = (v))
  
 -#if !defined(__HAVE_ARCH_PTE_DEVMAP) || !defined(CONFIG_TRANSPARENT_HUGEPAGE)
 -static inline int pmd_devmap(pmd_t pmd)
 +/* It's valid only if the page is free path or free_list */
 +static inline void set_freepage_migratetype(struct page *page, int migratetype)
  {
 -	return 0;
 +	page->index = migratetype;
 +}
 +
 +/* It's valid only if the page is free path or free_list */
 +static inline int get_freepage_migratetype(struct page *page)
 +{
 +	return page->index;
  }
++<<<<<<< HEAD
++=======
+ static inline int pud_devmap(pud_t pud)
+ {
+ 	return 0;
+ }
+ #endif
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
  /*
   * FIXME: take this include out, include page-flags.h in
@@@ -1180,8 -1203,10 +1188,15 @@@ void unmap_vmas(struct mmu_gather *tlb
  
  /**
   * mm_walk - callbacks for walk_page_range
++<<<<<<< HEAD
 + * @pgd_entry: if set, called for each non-empty PGD (top-level) entry
 + * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry
++=======
+  * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry
+  *	       this handler should only handle pud_trans_huge() puds.
+  *	       the pmd_entry or pte_entry callbacks will be used for
+  *	       regular PUDs.
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
   * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry
   *	       this handler is required to be able to handle
   *	       pmd_trans_huge() pmds.  They may simply choose to
@@@ -1189,16 -1214,20 +1204,21 @@@
   * @pte_entry: if set, called for each non-empty PTE (4th-level) entry
   * @pte_hole: if set, called for each hole at all levels
   * @hugetlb_entry: if set, called for each hugetlb entry
 - * @test_walk: caller specific callback function to determine whether
 - *             we walk over the current vma or not. Returning 0
 - *             value means "do page table walk over the current vma,"
 - *             and a negative one means "abort current page table walk
 - *             right now." 1 means "skip the current vma."
 - * @mm:        mm_struct representing the target process of page table walk
 - * @vma:       vma currently walked (NULL if walking outside vmas)
 - * @private:   private data for callbacks' usage
 + *		   *Caution*: The caller must hold mmap_sem() if @hugetlb_entry
 + * 			      is used.
   *
 - * (see the comment on walk_page_range() for more details)
 + * (see walk_page_range for more details)
   */
  struct mm_walk {
++<<<<<<< HEAD
 +	int (*pgd_entry)(pgd_t *pgd, unsigned long addr,
 +			 unsigned long next, struct mm_walk *walk);
 +	int (*pud_entry)(pud_t *pud, unsigned long addr,
 +	                 unsigned long next, struct mm_walk *walk);
++=======
+ 	int (*pud_entry)(pud_t *pud, unsigned long addr,
+ 			 unsigned long next, struct mm_walk *walk);
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  	int (*pmd_entry)(pmd_t *pmd, unsigned long addr,
  			 unsigned long next, struct mm_walk *walk);
  	int (*pte_entry)(pte_t *pte, unsigned long addr,
@@@ -1674,6 -1811,26 +1694,29 @@@ static inline spinlock_t *pmd_lock(stru
  	return ptl;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * No scalability reason to split PUD locks yet, but follow the same pattern
+  * as the PMD locks to make it easier if we decide to.  The VM should not be
+  * considered ready to switch to split PUD locks yet; there may be places
+  * which need to be converted from page_table_lock.
+  */
+ static inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)
+ {
+ 	return &mm->page_table_lock;
+ }
+ 
+ static inline spinlock_t *pud_lock(struct mm_struct *mm, pud_t *pud)
+ {
+ 	spinlock_t *ptl = pud_lockptr(mm, pud);
+ 
+ 	spin_lock(ptl);
+ 	return ptl;
+ }
+ 
+ extern void __init pagecache_init(void);
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  extern void free_area_init(unsigned long * zones_size);
  extern void free_area_init_node(int nid, unsigned long * zones_size,
  		unsigned long zone_start_pfn, unsigned long *zholes_size);
diff --cc include/linux/mmu_notifier.h
index 06146969ac0c,51891fb0d3ce..000000000000
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@@ -370,7 -381,20 +370,24 @@@ static inline void mmu_notifier_mm_dest
  	___pmd;								\
  })
  
++<<<<<<< HEAD
 +#define pmdp_get_and_clear_notify(__mm, __haddr, __pmd)			\
++=======
+ #define pudp_huge_clear_flush_notify(__vma, __haddr, __pud)		\
+ ({									\
+ 	unsigned long ___haddr = __haddr & HPAGE_PUD_MASK;		\
+ 	struct mm_struct *___mm = (__vma)->vm_mm;			\
+ 	pud_t ___pud;							\
+ 									\
+ 	___pud = pudp_huge_clear_flush(__vma, __haddr, __pud);		\
+ 	mmu_notifier_invalidate_range(___mm, ___haddr,			\
+ 				      ___haddr + HPAGE_PUD_SIZE);	\
+ 									\
+ 	___pud;								\
+ })
+ 
+ #define pmdp_huge_get_and_clear_notify(__mm, __haddr, __pmd)		\
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  ({									\
  	unsigned long ___haddr = __haddr & HPAGE_PMD_MASK;		\
  	pmd_t ___pmd;							\
@@@ -459,9 -484,12 +476,15 @@@ static inline void mmu_notifier_mm_dest
  
  #define ptep_clear_flush_young_notify ptep_clear_flush_young
  #define pmdp_clear_flush_young_notify pmdp_clear_flush_young
 -#define ptep_clear_young_notify ptep_test_and_clear_young
 -#define pmdp_clear_young_notify pmdp_test_and_clear_young
  #define	ptep_clear_flush_notify ptep_clear_flush
++<<<<<<< HEAD
 +#define pmdp_clear_flush_notify pmdp_clear_flush
 +#define pmdp_get_and_clear_notify pmdp_get_and_clear
++=======
+ #define pmdp_huge_clear_flush_notify pmdp_huge_clear_flush
+ #define pudp_huge_clear_flush_notify pudp_huge_clear_flush
+ #define pmdp_huge_get_and_clear_notify pmdp_huge_get_and_clear
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #define set_pte_at_notify set_pte_at
  
  #endif /* CONFIG_MMU_NOTIFIER */
diff --cc mm/gup.c
index 3166366affd5,1e67461b2733..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -254,20 -204,118 +254,99 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -/**
 - * follow_page_mask - look up a page descriptor from a user-virtual address
 - * @vma: vm_area_struct mapping @address
 - * @address: virtual address to look up
 - * @flags: flags modifying lookup behaviour
 - * @page_mask: on output, *page_mask is set according to the size of the page
 - *
 - * @flags can have FOLL_ flags set, defined in <linux/mm.h>
 - *
 - * Returns the mapped (struct page *), %NULL if no mapping exists, or
 - * an error pointer if there is a mapping to something not represented
 - * by a page descriptor (see also vm_normal_page()).
 - */
 -struct page *follow_page_mask(struct vm_area_struct *vma,
 -			      unsigned long address, unsigned int flags,
 -			      unsigned int *page_mask)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	*page_mask = 0;
 -
 -	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 -	if (!IS_ERR(page)) {
 -		BUG_ON(flags & FOLL_GET);
  		return page;
 -	}
  
++<<<<<<< HEAD
 +no_page_table:
 +	/*
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
 +	 */
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
++=======
+ 	pgd = pgd_offset(mm, address);
+ 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pud = pud_offset(pgd, address);
+ 	if (pud_none(*pud))
+ 		return no_page_table(vma, flags);
+ 	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pud(mm, address, pud, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (pud_devmap(*pud)) {
+ 		ptl = pud_lock(mm, pud);
+ 		page = follow_devmap_pud(vma, address, pud, flags);
+ 		spin_unlock(ptl);
+ 		if (page)
+ 			return page;
+ 	}
+ 	if (unlikely(pud_bad(*pud)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pmd(mm, address, pmd, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_devmap(*pmd)) {
+ 		ptl = pmd_lock(mm, pmd);
+ 		page = follow_devmap_pmd(vma, address, pmd, flags);
+ 		spin_unlock(ptl);
+ 		if (page)
+ 			return page;
+ 	}
+ 	if (likely(!pmd_trans_huge(*pmd)))
+ 		return follow_page_pte(vma, address, pmd, flags);
+ 
+ 	ptl = pmd_lock(mm, pmd);
+ 	if (unlikely(!pmd_trans_huge(*pmd))) {
+ 		spin_unlock(ptl);
+ 		return follow_page_pte(vma, address, pmd, flags);
+ 	}
+ 	if (flags & FOLL_SPLIT) {
+ 		int ret;
+ 		page = pmd_page(*pmd);
+ 		if (is_huge_zero_page(page)) {
+ 			spin_unlock(ptl);
+ 			ret = 0;
+ 			split_huge_pmd(vma, pmd, address);
+ 			if (pmd_trans_unstable(pmd))
+ 				ret = -EBUSY;
+ 		} else {
+ 			get_page(page);
+ 			spin_unlock(ptl);
+ 			lock_page(page);
+ 			ret = split_huge_page(page);
+ 			unlock_page(page);
+ 			put_page(page);
+ 			if (pmd_none(*pmd))
+ 				return no_page_table(vma, flags);
+ 		}
+ 
+ 		return ret ? ERR_PTR(ret) :
+ 			follow_page_pte(vma, address, pmd, flags);
+ 	}
+ 
+ 	page = follow_trans_huge_pmd(vma, address, pmd, flags);
+ 	spin_unlock(ptl);
+ 	*page_mask = HPAGE_PMD_NR - 1;
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  	return page;
  }
  
diff --cc mm/huge_memory.c
index 329799c812d7,85742ac5b32e..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -898,6 -757,123 +898,126 @@@ int vmf_insert_pfn_pmd(struct vm_area_s
  }
  EXPORT_SYMBOL_GPL(vmf_insert_pfn_pmd);
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static pud_t maybe_pud_mkwrite(pud_t pud, struct vm_area_struct *vma)
+ {
+ 	if (likely(vma->vm_flags & VM_WRITE))
+ 		pud = pud_mkwrite(pud);
+ 	return pud;
+ }
+ 
+ static void insert_pfn_pud(struct vm_area_struct *vma, unsigned long addr,
+ 		pud_t *pud, pfn_t pfn, pgprot_t prot, bool write)
+ {
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	pud_t entry;
+ 	spinlock_t *ptl;
+ 
+ 	ptl = pud_lock(mm, pud);
+ 	entry = pud_mkhuge(pfn_t_pud(pfn, prot));
+ 	if (pfn_t_devmap(pfn))
+ 		entry = pud_mkdevmap(entry);
+ 	if (write) {
+ 		entry = pud_mkyoung(pud_mkdirty(entry));
+ 		entry = maybe_pud_mkwrite(entry, vma);
+ 	}
+ 	set_pud_at(mm, addr, pud, entry);
+ 	update_mmu_cache_pud(vma, addr, pud);
+ 	spin_unlock(ptl);
+ }
+ 
+ int vmf_insert_pfn_pud(struct vm_area_struct *vma, unsigned long addr,
+ 			pud_t *pud, pfn_t pfn, bool write)
+ {
+ 	pgprot_t pgprot = vma->vm_page_prot;
+ 	/*
+ 	 * If we had pud_special, we could avoid all these restrictions,
+ 	 * but we need to be consistent with PTEs and architectures that
+ 	 * can't support a 'special' bit.
+ 	 */
+ 	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
+ 	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
+ 						(VM_PFNMAP|VM_MIXEDMAP));
+ 	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
+ 	BUG_ON(!pfn_t_devmap(pfn));
+ 
+ 	if (addr < vma->vm_start || addr >= vma->vm_end)
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	track_pfn_insert(vma, &pgprot, pfn);
+ 
+ 	insert_pfn_pud(vma, addr, pud, pfn, pgprot, write);
+ 	return VM_FAULT_NOPAGE;
+ }
+ EXPORT_SYMBOL_GPL(vmf_insert_pfn_pud);
+ #endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
+ 
+ static void touch_pmd(struct vm_area_struct *vma, unsigned long addr,
+ 		pmd_t *pmd)
+ {
+ 	pmd_t _pmd;
+ 
+ 	/*
+ 	 * We should set the dirty bit only for FOLL_WRITE but for now
+ 	 * the dirty bit in the pmd is meaningless.  And if the dirty
+ 	 * bit will become meaningful and we'll only set it with
+ 	 * FOLL_WRITE, an atomic set_bit will be required on the pmd to
+ 	 * set the young bit, instead of the current set_pmd_at.
+ 	 */
+ 	_pmd = pmd_mkyoung(pmd_mkdirty(*pmd));
+ 	if (pmdp_set_access_flags(vma, addr & HPAGE_PMD_MASK,
+ 				pmd, _pmd,  1))
+ 		update_mmu_cache_pmd(vma, addr, pmd);
+ }
+ 
+ struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,
+ 		pmd_t *pmd, int flags)
+ {
+ 	unsigned long pfn = pmd_pfn(*pmd);
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	struct dev_pagemap *pgmap;
+ 	struct page *page;
+ 
+ 	assert_spin_locked(pmd_lockptr(mm, pmd));
+ 
+ 	/*
+ 	 * When we COW a devmap PMD entry, we split it into PTEs, so we should
+ 	 * not be in this function with `flags & FOLL_COW` set.
+ 	 */
+ 	WARN_ONCE(flags & FOLL_COW, "mm: In follow_devmap_pmd with FOLL_COW set");
+ 
+ 	if (flags & FOLL_WRITE && !pmd_write(*pmd))
+ 		return NULL;
+ 
+ 	if (pmd_present(*pmd) && pmd_devmap(*pmd))
+ 		/* pass */;
+ 	else
+ 		return NULL;
+ 
+ 	if (flags & FOLL_TOUCH)
+ 		touch_pmd(vma, addr, pmd);
+ 
+ 	/*
+ 	 * device mapped pages can only be returned if the
+ 	 * caller will manage the page reference count.
+ 	 */
+ 	if (!(flags & FOLL_GET))
+ 		return ERR_PTR(-EEXIST);
+ 
+ 	pfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;
+ 	pgmap = get_dev_pagemap(pfn, NULL);
+ 	if (!pgmap)
+ 		return ERR_PTR(-EFAULT);
+ 	page = pfn_to_page(pfn);
+ 	get_page(page);
+ 	put_dev_pagemap(pgmap);
+ 
+ 	return page;
+ }
+ 
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
  		  pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
  		  struct vm_area_struct *vma)
@@@ -971,18 -941,131 +1091,139 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
 +void huge_pmd_set_accessed(struct mm_struct *mm,
 +			   struct vm_area_struct *vma,
 +			   unsigned long address,
 +			   pmd_t *pmd, pmd_t orig_pmd,
 +			   int dirty)
++=======
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static void touch_pud(struct vm_area_struct *vma, unsigned long addr,
+ 		pud_t *pud)
+ {
+ 	pud_t _pud;
+ 
+ 	/*
+ 	 * We should set the dirty bit only for FOLL_WRITE but for now
+ 	 * the dirty bit in the pud is meaningless.  And if the dirty
+ 	 * bit will become meaningful and we'll only set it with
+ 	 * FOLL_WRITE, an atomic set_bit will be required on the pud to
+ 	 * set the young bit, instead of the current set_pud_at.
+ 	 */
+ 	_pud = pud_mkyoung(pud_mkdirty(*pud));
+ 	if (pudp_set_access_flags(vma, addr & HPAGE_PUD_MASK,
+ 				pud, _pud,  1))
+ 		update_mmu_cache_pud(vma, addr, pud);
+ }
+ 
+ struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,
+ 		pud_t *pud, int flags)
+ {
+ 	unsigned long pfn = pud_pfn(*pud);
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	struct dev_pagemap *pgmap;
+ 	struct page *page;
+ 
+ 	assert_spin_locked(pud_lockptr(mm, pud));
+ 
+ 	if (flags & FOLL_WRITE && !pud_write(*pud))
+ 		return NULL;
+ 
+ 	if (pud_present(*pud) && pud_devmap(*pud))
+ 		/* pass */;
+ 	else
+ 		return NULL;
+ 
+ 	if (flags & FOLL_TOUCH)
+ 		touch_pud(vma, addr, pud);
+ 
+ 	/*
+ 	 * device mapped pages can only be returned if the
+ 	 * caller will manage the page reference count.
+ 	 */
+ 	if (!(flags & FOLL_GET))
+ 		return ERR_PTR(-EEXIST);
+ 
+ 	pfn += (addr & ~PUD_MASK) >> PAGE_SHIFT;
+ 	pgmap = get_dev_pagemap(pfn, NULL);
+ 	if (!pgmap)
+ 		return ERR_PTR(-EFAULT);
+ 	page = pfn_to_page(pfn);
+ 	get_page(page);
+ 	put_dev_pagemap(pgmap);
+ 
+ 	return page;
+ }
+ 
+ int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+ 		  pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
+ 		  struct vm_area_struct *vma)
+ {
+ 	spinlock_t *dst_ptl, *src_ptl;
+ 	pud_t pud;
+ 	int ret;
+ 
+ 	dst_ptl = pud_lock(dst_mm, dst_pud);
+ 	src_ptl = pud_lockptr(src_mm, src_pud);
+ 	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
+ 
+ 	ret = -EAGAIN;
+ 	pud = *src_pud;
+ 	if (unlikely(!pud_trans_huge(pud) && !pud_devmap(pud)))
+ 		goto out_unlock;
+ 
+ 	/*
+ 	 * When page table lock is held, the huge zero pud should not be
+ 	 * under splitting since we don't split the page itself, only pud to
+ 	 * a page table.
+ 	 */
+ 	if (is_huge_zero_pud(pud)) {
+ 		/* No huge zero pud yet */
+ 	}
+ 
+ 	pudp_set_wrprotect(src_mm, addr, src_pud);
+ 	pud = pud_mkold(pud_wrprotect(pud));
+ 	set_pud_at(dst_mm, addr, dst_pud, pud);
+ 
+ 	ret = 0;
+ out_unlock:
+ 	spin_unlock(src_ptl);
+ 	spin_unlock(dst_ptl);
+ 	return ret;
+ }
+ 
+ void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud)
+ {
+ 	pud_t entry;
+ 	unsigned long haddr;
+ 	bool write = vmf->flags & FAULT_FLAG_WRITE;
+ 
+ 	vmf->ptl = pud_lock(vmf->vma->vm_mm, vmf->pud);
+ 	if (unlikely(!pud_same(*vmf->pud, orig_pud)))
+ 		goto unlock;
+ 
+ 	entry = pud_mkyoung(orig_pud);
+ 	if (write)
+ 		entry = pud_mkdirty(entry);
+ 	haddr = vmf->address & HPAGE_PUD_MASK;
+ 	if (pudp_set_access_flags(vmf->vma, haddr, vmf->pud, entry, write))
+ 		update_mmu_cache_pud(vmf->vma, vmf->address, vmf->pud);
+ 
+ unlock:
+ 	spin_unlock(vmf->ptl);
+ }
+ #endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
+ 
+ void huge_pmd_set_accessed(struct vm_fault *vmf, pmd_t orig_pmd)
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  {
 +	spinlock_t *ptl;
  	pmd_t entry;
  	unsigned long haddr;
 -	bool write = vmf->flags & FAULT_FLAG_WRITE;
  
 -	vmf->ptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);
 -	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
 +	ptl = pmd_lock(mm, pmd);
 +	if (unlikely(!pmd_same(*pmd, orig_pmd)))
  		goto unlock;
  
  	entry = pmd_mkyoung(orig_pmd);
@@@ -1664,685 -1773,484 +1905,686 @@@ int __pmd_trans_huge_lock(pmd_t *pmd, s
  }
  
  /*
 - * Returns true if a given pud maps a thp, false otherwise.
 + * This function returns whether a given @page is mapped onto the @address
 + * in the virtual space of @mm.
   *
 - * Note that if it returns true, this routine returns without unlocking page
 - * table lock. So callers must unlock it.
 + * When it's true, this function returns *pmd with holding the page table lock
 + * and passing it back to the caller via @ptl.
 + * If it's false, returns NULL without holding the page table lock.
   */
 -spinlock_t *__pud_trans_huge_lock(pud_t *pud, struct vm_area_struct *vma)
 +pmd_t *page_check_address_pmd(struct page *page,
 +			      struct mm_struct *mm,
 +			      unsigned long address,
 +			      enum page_check_address_pmd_flag flag,
 +			      spinlock_t **ptl)
  {
 -	spinlock_t *ptl;
 +	pgd_t *pgd;
 +	pud_t *pud;
 +	pmd_t *pmd;
  
 -	ptl = pud_lock(vma->vm_mm, pud);
 -	if (likely(pud_trans_huge(*pud) || pud_devmap(*pud)))
 -		return ptl;
 -	spin_unlock(ptl);
 -	return NULL;
 -}
 +	if (address & ~HPAGE_PMD_MASK)
 +		return NULL;
  
 -#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
 -int zap_huge_pud(struct mmu_gather *tlb, struct vm_area_struct *vma,
 -		 pud_t *pud, unsigned long addr)
 -{
 -	pud_t orig_pud;
 -	spinlock_t *ptl;
 +	pgd = pgd_offset(mm, address);
 +	if (!pgd_present(*pgd))
 +		return NULL;
 +	pud = pud_offset(pgd, address);
 +	if (!pud_present(*pud))
 +		return NULL;
 +	pmd = pmd_offset(pud, address);
  
 -	ptl = __pud_trans_huge_lock(pud, vma);
 -	if (!ptl)
 -		return 0;
 +	*ptl = pmd_lock(mm, pmd);
 +	if (!pmd_present(*pmd))
 +		goto unlock;
 +	if (pmd_page(*pmd) != page)
 +		goto unlock;
  	/*
 -	 * For architectures like ppc64 we look at deposited pgtable
 -	 * when calling pudp_huge_get_and_clear. So do the
 -	 * pgtable_trans_huge_withdraw after finishing pudp related
 -	 * operations.
 +	 * split_vma() may create temporary aliased mappings. There is
 +	 * no risk as long as all huge pmd are found and have their
 +	 * splitting bit set before __split_huge_page_refcount
 +	 * runs. Finding the same huge pmd more than once during the
 +	 * same rmap walk is not a problem.
  	 */
 -	orig_pud = pudp_huge_get_and_clear_full(tlb->mm, addr, pud,
 -			tlb->fullmm);
 -	tlb_remove_pud_tlb_entry(tlb, pud, addr);
 -	if (vma_is_dax(vma)) {
 -		spin_unlock(ptl);
 -		/* No zero page support yet */
 -	} else {
 -		/* No support for anonymous PUD pages yet */
 -		BUG();
 +	if (flag == PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG &&
 +	    pmd_trans_splitting(*pmd))
 +		goto unlock;
 +	if (pmd_trans_huge(*pmd)) {
 +		VM_BUG_ON(flag == PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG &&
 +			  !pmd_trans_splitting(*pmd));
 +		return pmd;
  	}
 -	return 1;
 +unlock:
 +	spin_unlock(*ptl);
 +	return NULL;
  }
  
 -static void __split_huge_pud_locked(struct vm_area_struct *vma, pud_t *pud,
 -		unsigned long haddr)
++<<<<<<< HEAD
 +static int __split_huge_page_splitting(struct page *page,
 +				       struct vm_area_struct *vma,
 +				       unsigned long address)
  {
 -	VM_BUG_ON(haddr & ~HPAGE_PUD_MASK);
 -	VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
 -	VM_BUG_ON_VMA(vma->vm_end < haddr + HPAGE_PUD_SIZE, vma);
 -	VM_BUG_ON(!pud_trans_huge(*pud) && !pud_devmap(*pud));
 -
 -	count_vm_event(THP_SPLIT_PMD);
 +	struct mm_struct *mm = vma->vm_mm;
 +	spinlock_t *ptl;
 +	pmd_t *pmd;
 +	int ret = 0;
 +	/* For mmu_notifiers */
 +	const unsigned long mmun_start = address;
 +	const unsigned long mmun_end   = address + HPAGE_PMD_SIZE;
 +
 +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 +	pmd = page_check_address_pmd(page, mm, address,
 +			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &ptl);
 +	if (pmd) {
 +		/*
 +		 * We can't temporarily set the pmd to null in order
 +		 * to split it, the pmd must remain marked huge at all
 +		 * times or the VM won't take the pmd_trans_huge paths
 +		 * and it won't wait on the anon_vma->root->rwsem to
 +		 * serialize against split_huge_page*.
 +		 */
 +		pmdp_splitting_flush(vma, address, pmd);
 +		ret = 1;
 +		spin_unlock(ptl);
 +	}
 +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
  
 -	pudp_huge_clear_flush_notify(vma, haddr, pud);
 +	return ret;
  }
  
 -void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,
 -		unsigned long address)
 +static void __split_huge_page_refcount(struct page *page,
 +				       struct list_head *list)
  {
 -	spinlock_t *ptl;
 -	struct mm_struct *mm = vma->vm_mm;
 -	unsigned long haddr = address & HPAGE_PUD_MASK;
 +	int i;
 +	struct zone *zone = page_zone(page);
 +	struct lruvec *lruvec;
 +	int tail_count = 0;
 +	int mmu_gather_count;
  
 -	mmu_notifier_invalidate_range_start(mm, haddr, haddr + HPAGE_PUD_SIZE);
 -	ptl = pud_lock(mm, pud);
 -	if (unlikely(!pud_trans_huge(*pud) && !pud_devmap(*pud)))
 -		goto out;
 -	__split_huge_pud_locked(vma, pud, haddr);
 +	/* prevent PageLRU to go away from under us, and freeze lru stats */
 +	spin_lock_irq(&zone->lru_lock);
 +	lruvec = mem_cgroup_page_lruvec(page, zone);
  
 -out:
 -	spin_unlock(ptl);
 -	mmu_notifier_invalidate_range_end(mm, haddr, haddr + HPAGE_PUD_SIZE);
 -}
 -#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
 +	/*
 +	 * No mmu_gather_count increase can happen anymore because
 +	 * here all pmds are already pmd_trans_splitting(). No
 +	 * decrease can happen either because it's only decreased
 +	 * while holding the lru_lock. So here the mmu_gather_count is
 +	 * already stable so store it on the stack. Then it'll be
 +	 * overwritten when the page_tail->index is initialized.
 +	 */
 +	mmu_gather_count = trans_huge_mmu_gather_count(page);
  
 -static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 -		unsigned long haddr, pmd_t *pmd)
 -{
 -	struct mm_struct *mm = vma->vm_mm;
 -	pgtable_t pgtable;
 -	pmd_t _pmd;
 -	int i;
 +	compound_lock(page);
 +	/* complete memcg works before add pages to LRU */
 +	mem_cgroup_split_huge_fixup(page);
  
 -	/* leave pmd empty until pte is filled */
 -	pmdp_huge_clear_flush_notify(vma, haddr, pmd);
 +	for (i = HPAGE_PMD_NR - 1; i >= 1; i--) {
 +		struct page *page_tail = page + i;
 +
 +		/* tail_page->_mapcount cannot change */
 +		BUG_ON(page_mapcount(page_tail) < 0);
 +		tail_count += page_mapcount(page_tail);
 +		/* check for overflow */
 +		BUG_ON(tail_count < 0);
 +		BUG_ON(atomic_read(&page_tail->_count) != 0);
 +		/*
 +		 * tail_page->_count is zero and not changing from
 +		 * under us. But get_page_unless_zero() may be running
 +		 * from under us on the tail_page. If we used
 +		 * atomic_set() below instead of atomic_add(), we
 +		 * would then run atomic_set() concurrently with
 +		 * get_page_unless_zero(), and atomic_set() is
 +		 * implemented in C not using locked ops. spin_unlock
 +		 * on x86 sometime uses locked ops because of PPro
 +		 * errata 66, 92, so unless somebody can guarantee
 +		 * atomic_set() here would be safe on all archs (and
 +		 * not only on x86), it's safer to use atomic_add().
 +		 */
 +		atomic_add(page_mapcount(page) + page_mapcount(page_tail) +
 +			   mmu_gather_count + 1, &page_tail->_count);
  
 -	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 -	pmd_populate(mm, &_pmd, pgtable);
 +		/* after clearing PageTail the gup refcount can be released */
 +		smp_mb();
  
 -	for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
 -		pte_t *pte, entry;
 -		entry = pfn_pte(my_zero_pfn(haddr), vma->vm_page_prot);
 -		entry = pte_mkspecial(entry);
 -		pte = pte_offset_map(&_pmd, haddr);
 -		VM_BUG_ON(!pte_none(*pte));
 -		set_pte_at(mm, haddr, pte, entry);
 -		pte_unmap(pte);
 +		/*
 +		 * retain hwpoison flag of the poisoned tail page:
 +		 *   fix for the unsuitable process killed on Guest Machine(KVM)
 +		 *   by the memory-failure.
 +		 */
 +		page_tail->flags &= ~PAGE_FLAGS_CHECK_AT_PREP | __PG_HWPOISON;
 +		page_tail->flags |= (page->flags &
 +				     ((1L << PG_referenced) |
 +				      (1L << PG_swapbacked) |
 +				      (1L << PG_mlocked) |
 +				      (1L << PG_uptodate) |
 +				      (1L << PG_active) |
 +				      (1L << PG_unevictable)));
 +		page_tail->flags |= (1L << PG_dirty);
 +
 +		/* clear PageTail before overwriting first_page */
 +		smp_wmb();
 +
 +		/*
 +		 * __split_huge_page_splitting() already set the
 +		 * splitting bit in all pmd that could map this
 +		 * hugepage, that will ensure no CPU can alter the
 +		 * mapcount on the head page. The mapcount is only
 +		 * accounted in the head page and it has to be
 +		 * transferred to all tail pages in the below code. So
 +		 * for this code to be safe, the split the mapcount
 +		 * can't change. But that doesn't mean userland can't
 +		 * keep changing and reading the page contents while
 +		 * we transfer the mapcount, so the pmd splitting
 +		 * status is achieved setting a reserved bit in the
 +		 * pmd, not by clearing the present bit.
 +		*/
 +		page_tail->_mapcount = page->_mapcount;
 +
 +		BUG_ON(page_tail->mapping);
 +		page_tail->mapping = page->mapping;
 +
 +		page_tail->index = page->index + i;
 +		page_cpupid_xchg_last(page_tail, page_cpupid_last(page));
 +
 +		BUG_ON(!PageAnon(page_tail));
 +		BUG_ON(!PageUptodate(page_tail));
 +		BUG_ON(!PageDirty(page_tail));
 +		BUG_ON(!PageSwapBacked(page_tail));
 +
 +		lru_add_page_tail(page, page_tail, lruvec, list);
 +	}
 +	atomic_sub(tail_count, &page->_count);
 +	BUG_ON(atomic_read(&page->_count) <= 0);
 +
 +	__mod_zone_page_state(zone, NR_ANON_TRANSPARENT_HUGEPAGES, -1);
 +	__mod_zone_page_state(zone, NR_ANON_PAGES, HPAGE_PMD_NR);
 +
 +	ClearPageCompound(page);
 +	compound_unlock(page);
 +	spin_unlock_irq(&zone->lru_lock);
 +
 +	for (i = 1; i < HPAGE_PMD_NR; i++) {
 +		struct page *page_tail = page + i;
 +		BUG_ON(page_count(page_tail) <= 0);
 +		/*
 +		 * Tail pages may be freed if there wasn't any mapping
 +		 * like if add_to_swap() is running on a lru page that
 +		 * had its mapping zapped. And freeing these pages
 +		 * requires taking the lru_lock so we do the put_page
 +		 * of the tail pages after the split is complete.
 +		 */
 +		put_page(page_tail);
  	}
 -	smp_wmb(); /* make pte visible before pmd */
 -	pmd_populate(mm, pmd, pgtable);
 +
 +	/*
 +	 * Only the head page (now become a regular page) is required
 +	 * to be pinned by the caller.
 +	 */
 +	BUG_ON(page_count(page) <= 0);
  }
  
 -static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 -		unsigned long haddr, bool freeze)
 +static int __split_huge_page_map(struct page *page,
 +				 struct vm_area_struct *vma,
 +				 unsigned long address)
  {
  	struct mm_struct *mm = vma->vm_mm;
 -	struct page *page;
 +	spinlock_t *ptl;
 +	pmd_t *pmd, _pmd = {0};
 +	int ret = 0, i;
  	pgtable_t pgtable;
 -	pmd_t _pmd;
 -	bool young, write, dirty, soft_dirty;
 -	unsigned long addr;
 -	int i;
 -
 -	VM_BUG_ON(haddr & ~HPAGE_PMD_MASK);
 -	VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
 -	VM_BUG_ON_VMA(vma->vm_end < haddr + HPAGE_PMD_SIZE, vma);
 -	VM_BUG_ON(!pmd_trans_huge(*pmd) && !pmd_devmap(*pmd));
 +	unsigned long haddr;
  
 -	count_vm_event(THP_SPLIT_PMD);
 +	pmd = page_check_address_pmd(page, mm, address,
 +			PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG, &ptl);
 +	if (pmd) {
 +		pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 +		pmd_populate(mm, &_pmd, pgtable);
 +
 +		haddr = address;
 +		for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
 +			pte_t *pte, entry;
 +			BUG_ON(PageCompound(page+i));
 +			entry = mk_pte(page + i, vma->vm_page_prot);
 +			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +			if (!pmd_write(*pmd))
 +				entry = pte_wrprotect(entry);
 +			else
 +				BUG_ON(page_mapcount(page) != 1);
 +			if (!pmd_young(*pmd))
 +				entry = pte_mkold(entry);
 +			if (pmd_numa(*pmd))
 +				entry = pte_mknuma(entry);
 +			pte = pte_offset_map(&_pmd, haddr);
 +			BUG_ON(!pte_none(*pte));
 +			set_pte_at(mm, haddr, pte, entry);
 +			pte_unmap(pte);
 +		}
  
 -	if (!vma_is_anonymous(vma)) {
 -		_pmd = pmdp_huge_clear_flush_notify(vma, haddr, pmd);
 +		smp_wmb(); /* make pte visible before pmd */
  		/*
 -		 * We are going to unmap this huge page. So
 -		 * just go ahead and zap it
 +		 * Up to this point the pmd is present and huge and
 +		 * userland has the whole access to the hugepage
 +		 * during the split (which happens in place). If we
 +		 * overwrite the pmd with the not-huge version
 +		 * pointing to the pte here (which of course we could
 +		 * if all CPUs were bug free), userland could trigger
 +		 * a small page size TLB miss on the small sized TLB
 +		 * while the hugepage TLB entry is still established
 +		 * in the huge TLB. Some CPU doesn't like that. See
 +		 * http://support.amd.com/us/Processor_TechDocs/41322.pdf,
 +		 * Erratum 383 on page 93. Intel should be safe but is
 +		 * also warns that it's only safe if the permission
 +		 * and cache attributes of the two entries loaded in
 +		 * the two TLB is identical (which should be the case
 +		 * here). But it is generally safer to never allow
 +		 * small and huge TLB entries for the same virtual
 +		 * address to be loaded simultaneously. So instead of
 +		 * doing "pmd_populate(); flush_tlb_range();" we first
 +		 * mark the current pmd notpresent (atomically because
 +		 * here the pmd_trans_huge and pmd_trans_splitting
 +		 * must remain set at all times on the pmd until the
 +		 * split is complete for this pmd), then we flush the
 +		 * SMP TLB and finally we write the non-huge version
 +		 * of the pmd entry with pmd_populate.
  		 */
 -		if (arch_needs_pgtable_deposit())
 -			zap_deposited_table(mm, pmd);
 -		if (vma_is_dax(vma))
 -			return;
 -		page = pmd_page(_pmd);
 -		if (!PageReferenced(page) && pmd_young(_pmd))
 -			SetPageReferenced(page);
 -		page_remove_rmap(page, true);
 -		put_page(page);
 -		add_mm_counter(mm, MM_FILEPAGES, -HPAGE_PMD_NR);
 -		return;
 -	} else if (is_huge_zero_pmd(*pmd)) {
 -		return __split_huge_zero_page_pmd(vma, haddr, pmd);
 +		pmdp_invalidate(vma, address, pmd);
 +		pmd_populate(mm, pmd, pgtable);
 +		ret = 1;
 +		spin_unlock(ptl);
  	}
  
 -	page = pmd_page(*pmd);
 -	VM_BUG_ON_PAGE(!page_count(page), page);
 -	page_ref_add(page, HPAGE_PMD_NR - 1);
 -	write = pmd_write(*pmd);
 -	young = pmd_young(*pmd);
 -	dirty = pmd_dirty(*pmd);
 -	soft_dirty = pmd_soft_dirty(*pmd);
 -
 -	pmdp_huge_split_prepare(vma, haddr, pmd);
 -	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 -	pmd_populate(mm, &_pmd, pgtable);
 +	return ret;
 +}
  
 -	for (i = 0, addr = haddr; i < HPAGE_PMD_NR; i++, addr += PAGE_SIZE) {
 -		pte_t entry, *pte;
 -		/*
 -		 * Note that NUMA hinting access restrictions are not
 -		 * transferred to avoid any possibility of altering
 -		 * permissions across VMAs.
 -		 */
 -		if (freeze) {
 -			swp_entry_t swp_entry;
 -			swp_entry = make_migration_entry(page + i, write);
 -			entry = swp_entry_to_pte(swp_entry);
 -			if (soft_dirty)
 -				entry = pte_swp_mksoft_dirty(entry);
 -		} else {
 -			entry = mk_pte(page + i, READ_ONCE(vma->vm_page_prot));
 -			entry = maybe_mkwrite(entry, vma);
 -			if (!write)
 -				entry = pte_wrprotect(entry);
 -			if (!young)
 -				entry = pte_mkold(entry);
 -			if (soft_dirty)
 -				entry = pte_mksoft_dirty(entry);
 -		}
 -		if (dirty)
 -			SetPageDirty(page + i);
 -		pte = pte_offset_map(&_pmd, addr);
 -		BUG_ON(!pte_none(*pte));
 -		set_pte_at(mm, addr, pte, entry);
 -		atomic_inc(&page[i]._mapcount);
 -		pte_unmap(pte);
 -	}
 +/* must be called with anon_vma->root->rwsem held */
 +static void __split_huge_page(struct page *page,
 +			      struct anon_vma *anon_vma,
 +			      struct list_head *list)
 +{
 +	int mapcount, mapcount2;
 +	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
 +	struct anon_vma_chain *avc;
  
 -	/*
 -	 * Set PG_double_map before dropping compound_mapcount to avoid
 -	 * false-negative page_mapped().
 -	 */
 -	if (compound_mapcount(page) > 1 && !TestSetPageDoubleMap(page)) {
 -		for (i = 0; i < HPAGE_PMD_NR; i++)
 -			atomic_inc(&page[i]._mapcount);
 -	}
 -
 -	if (atomic_add_negative(-1, compound_mapcount_ptr(page))) {
 -		/* Last compound_mapcount is gone. */
 -		__dec_node_page_state(page, NR_ANON_THPS);
 -		if (TestClearPageDoubleMap(page)) {
 -			/* No need in mapcount reference anymore */
 -			for (i = 0; i < HPAGE_PMD_NR; i++)
 -				atomic_dec(&page[i]._mapcount);
 -		}
 -	}
 +	BUG_ON(!PageHead(page));
 +	BUG_ON(PageTail(page));
  
 -	smp_wmb(); /* make pte visible before pmd */
 +	mapcount = 0;
 +	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
 +		struct vm_area_struct *vma = avc->vma;
 +		unsigned long addr = vma_address(page, vma);
 +		BUG_ON(is_vma_temporary_stack(vma));
 +		mapcount += __split_huge_page_splitting(page, vma, addr);
 +	}
  	/*
 -	 * Up to this point the pmd is present and huge and userland has the
 -	 * whole access to the hugepage during the split (which happens in
 -	 * place). If we overwrite the pmd with the not-huge version pointing
 -	 * to the pte here (which of course we could if all CPUs were bug
 -	 * free), userland could trigger a small page size TLB miss on the
 -	 * small sized TLB while the hugepage TLB entry is still established in
 -	 * the huge TLB. Some CPU doesn't like that.
 -	 * See http://support.amd.com/us/Processor_TechDocs/41322.pdf, Erratum
 -	 * 383 on page 93. Intel should be safe but is also warns that it's
 -	 * only safe if the permission and cache attributes of the two entries
 -	 * loaded in the two TLB is identical (which should be the case here).
 -	 * But it is generally safer to never allow small and huge TLB entries
 -	 * for the same virtual address to be loaded simultaneously. So instead
 -	 * of doing "pmd_populate(); flush_pmd_tlb_range();" we first mark the
 -	 * current pmd notpresent (atomically because here the pmd_trans_huge
 -	 * and pmd_trans_splitting must remain set at all times on the pmd
 -	 * until the split is complete for this pmd), then we flush the SMP TLB
 -	 * and finally we write the non-huge version of the pmd entry with
 -	 * pmd_populate.
 +	 * It is critical that new vmas are added to the tail of the
 +	 * anon_vma list. This guarantes that if copy_huge_pmd() runs
 +	 * and establishes a child pmd before
 +	 * __split_huge_page_splitting() freezes the parent pmd (so if
 +	 * we fail to prevent copy_huge_pmd() from running until the
 +	 * whole __split_huge_page() is complete), we will still see
 +	 * the newly established pmd of the child later during the
 +	 * walk, to be able to set it as pmd_trans_splitting too.
  	 */
 -	pmdp_invalidate(vma, haddr, pmd);
 -	pmd_populate(mm, pmd, pgtable);
 +	if (mapcount != page_mapcount(page))
 +		printk(KERN_ERR "mapcount %d page_mapcount %d\n",
 +		       mapcount, page_mapcount(page));
 +	BUG_ON(mapcount != page_mapcount(page));
  
 -	if (freeze) {
 -		for (i = 0; i < HPAGE_PMD_NR; i++) {
 -			page_remove_rmap(page + i, false);
 -			put_page(page + i);
 -		}
 +	__split_huge_page_refcount(page, list);
 +
 +	mapcount2 = 0;
 +	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
 +		struct vm_area_struct *vma = avc->vma;
 +		unsigned long addr = vma_address(page, vma);
 +		BUG_ON(is_vma_temporary_stack(vma));
 +		mapcount2 += __split_huge_page_map(page, vma, addr);
  	}
 +	if (mapcount != mapcount2)
 +		printk(KERN_ERR "mapcount %d mapcount2 %d page_mapcount %d\n",
 +		       mapcount, mapcount2, page_mapcount(page));
 +	BUG_ON(mapcount != mapcount2);
  }
  
 -void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 -		unsigned long address, bool freeze, struct page *page)
 +/*
 + * Split a hugepage into normal pages. This doesn't change the position of head
 + * page. If @list is null, tail pages will be added to LRU list, otherwise, to
 + * @list. Both head page and tail pages will inherit mapping, flags, and so on
 + * from the hugepage.
 + * Return 0 if the hugepage is split successfully otherwise return 1.
 + */
 +int split_huge_page_to_list(struct page *page, struct list_head *list)
  {
 -	spinlock_t *ptl;
 -	struct mm_struct *mm = vma->vm_mm;
 -	unsigned long haddr = address & HPAGE_PMD_MASK;
 +	struct anon_vma *anon_vma;
 +	int ret = 1;
  
 -	mmu_notifier_invalidate_range_start(mm, haddr, haddr + HPAGE_PMD_SIZE);
 -	ptl = pmd_lock(mm, pmd);
 +	BUG_ON(is_huge_zero_page(page));
 +	BUG_ON(!PageAnon(page));
  
  	/*
 -	 * If caller asks to setup a migration entries, we need a page to check
 -	 * pmd against. Otherwise we can end up replacing wrong page.
 +	 * The caller does not necessarily hold an mmap_sem that would prevent
 +	 * the anon_vma disappearing so we first we take a reference to it
 +	 * and then lock the anon_vma for write. This is similar to
 +	 * page_lock_anon_vma_read except the write lock is taken to serialise
 +	 * against parallel split or collapse operations.
  	 */
 -	VM_BUG_ON(freeze && !page);
 -	if (page && page != pmd_page(*pmd))
 -	        goto out;
 -
 -	if (pmd_trans_huge(*pmd)) {
 -		page = pmd_page(*pmd);
 -		if (PageMlocked(page))
 -			clear_page_mlock(page);
 -	} else if (!pmd_devmap(*pmd))
 +	anon_vma = page_get_anon_vma(page);
 +	if (!anon_vma)
  		goto out;
 -	__split_huge_pmd_locked(vma, pmd, haddr, freeze);
 -out:
 -	spin_unlock(ptl);
 -	mmu_notifier_invalidate_range_end(mm, haddr, haddr + HPAGE_PMD_SIZE);
 -}
 +	anon_vma_lock_write(anon_vma);
  
 -void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,
 -		bool freeze, struct page *page)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 +	ret = 0;
 +	if (!PageCompound(page))
 +		goto out_unlock;
  
 -	pgd = pgd_offset(vma->vm_mm, address);
 -	if (!pgd_present(*pgd))
 -		return;
 +	BUG_ON(!PageSwapBacked(page));
 +	__split_huge_page(page, anon_vma, list);
 +	count_vm_event(THP_SPLIT);
  
 -	pud = pud_offset(pgd, address);
 -	if (!pud_present(*pud))
 -		return;
 +	BUG_ON(PageCompound(page));
 +out_unlock:
 +	anon_vma_unlock_write(anon_vma);
 +	put_anon_vma(anon_vma);
 +out:
 +	return ret;
 +}
  
 -	pmd = pmd_offset(pud, address);
 +#define VM_NO_THP (VM_SPECIAL|VM_MIXEDMAP|VM_HUGETLB|VM_SHARED|VM_MAYSHARE)
 +
 +int hugepage_madvise(struct vm_area_struct *vma,
 +		     unsigned long *vm_flags, int advice)
 +{
 +	switch (advice) {
 +	case MADV_HUGEPAGE:
 +#ifdef CONFIG_S390
 +		/*
 +		 * qemu blindly sets MADV_HUGEPAGE on all allocations, but s390
 +		 * can't handle this properly after s390_enable_sie, so we simply
 +		 * ignore the madvise to prevent qemu from causing a SIGSEGV.
 +		 */
 +		if (mm_has_pgste(vma->vm_mm))
 +			return 0;
 +#endif
 +		/*
 +		 * Be somewhat over-protective like KSM for now!
 +		 */
 +		if (*vm_flags & (VM_HUGEPAGE | VM_NO_THP))
 +			return -EINVAL;
 +		*vm_flags &= ~VM_NOHUGEPAGE;
 +		*vm_flags |= VM_HUGEPAGE;
 +		/*
 +		 * If the vma become good for khugepaged to scan,
 +		 * register it here without waiting a page fault that
 +		 * may not happen any time soon.
 +		 */
 +		if (unlikely(khugepaged_enter_vma_merge(vma)))
 +			return -ENOMEM;
 +		break;
 +	case MADV_NOHUGEPAGE:
 +		/*
 +		 * Be somewhat over-protective like KSM for now!
 +		 */
 +		if (*vm_flags & (VM_NOHUGEPAGE | VM_NO_THP))
 +			return -EINVAL;
 +		*vm_flags &= ~VM_HUGEPAGE;
 +		*vm_flags |= VM_NOHUGEPAGE;
 +		/*
 +		 * Setting VM_NOHUGEPAGE will prevent khugepaged from scanning
 +		 * this vma even if we leave the mm registered in khugepaged if
 +		 * it got registered before VM_NOHUGEPAGE was set.
 +		 */
 +		break;
 +	}
  
 -	__split_huge_pmd(vma, pmd, address, freeze, page);
 +	return 0;
  }
  
 -void vma_adjust_trans_huge(struct vm_area_struct *vma,
 -			     unsigned long start,
 -			     unsigned long end,
 -			     long adjust_next)
 +static int __init khugepaged_slab_init(void)
  {
 -	/*
 -	 * If the new start address isn't hpage aligned and it could
 -	 * previously contain an hugepage: check if we need to split
 -	 * an huge pmd.
 -	 */
 -	if (start & ~HPAGE_PMD_MASK &&
 -	    (start & HPAGE_PMD_MASK) >= vma->vm_start &&
 -	    (start & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)
 -		split_huge_pmd_address(vma, start, false, NULL);
 -
 -	/*
 -	 * If the new end address isn't hpage aligned and it could
 -	 * previously contain an hugepage: check if we need to split
 -	 * an huge pmd.
 -	 */
 -	if (end & ~HPAGE_PMD_MASK &&
 -	    (end & HPAGE_PMD_MASK) >= vma->vm_start &&
 -	    (end & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)
 -		split_huge_pmd_address(vma, end, false, NULL);
 +	mm_slot_cache = kmem_cache_create("khugepaged_mm_slot",
 +					  sizeof(struct mm_slot),
 +					  __alignof__(struct mm_slot), 0, NULL);
 +	if (!mm_slot_cache)
 +		return -ENOMEM;
  
 -	/*
 -	 * If we're also updating the vma->vm_next->vm_start, if the new
 -	 * vm_next->vm_start isn't page aligned and it could previously
 -	 * contain an hugepage: check if we need to split an huge pmd.
 -	 */
 -	if (adjust_next > 0) {
 -		struct vm_area_struct *next = vma->vm_next;
 -		unsigned long nstart = next->vm_start;
 -		nstart += adjust_next << PAGE_SHIFT;
 -		if (nstart & ~HPAGE_PMD_MASK &&
 -		    (nstart & HPAGE_PMD_MASK) >= next->vm_start &&
 -		    (nstart & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= next->vm_end)
 -			split_huge_pmd_address(next, nstart, false, NULL);
 -	}
 +	return 0;
  }
  
 -static void freeze_page(struct page *page)
 +static inline struct mm_slot *alloc_mm_slot(void)
  {
 -	enum ttu_flags ttu_flags = TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS |
 -		TTU_RMAP_LOCKED;
 -	int i, ret;
 +	if (!mm_slot_cache)	/* initialization failed */
 +		return NULL;
 +	return kmem_cache_zalloc(mm_slot_cache, GFP_KERNEL);
 +}
  
 -	VM_BUG_ON_PAGE(!PageHead(page), page);
 +static inline void free_mm_slot(struct mm_slot *mm_slot)
 +{
 +	kmem_cache_free(mm_slot_cache, mm_slot);
 +}
  
 -	if (PageAnon(page))
 -		ttu_flags |= TTU_MIGRATION;
 +static struct mm_slot *get_mm_slot(struct mm_struct *mm)
 +{
 +	struct mm_slot *mm_slot;
  
 -	/* We only need TTU_SPLIT_HUGE_PMD once */
 -	ret = try_to_unmap(page, ttu_flags | TTU_SPLIT_HUGE_PMD);
 -	for (i = 1; !ret && i < HPAGE_PMD_NR; i++) {
 -		/* Cut short if the page is unmapped */
 -		if (page_count(page) == 1)
 -			return;
 +	hash_for_each_possible(mm_slots_hash, mm_slot, hash, (unsigned long)mm)
 +		if (mm == mm_slot->mm)
 +			return mm_slot;
  
 -		ret = try_to_unmap(page + i, ttu_flags);
 -	}
 -	VM_BUG_ON_PAGE(ret, page + i - 1);
 +	return NULL;
  }
  
 -static void unfreeze_page(struct page *page)
 +static void insert_to_mm_slots_hash(struct mm_struct *mm,
 +				    struct mm_slot *mm_slot)
  {
 -	int i;
 +	mm_slot->mm = mm;
 +	hash_add(mm_slots_hash, &mm_slot->hash, (long)mm);
 +}
  
 -	for (i = 0; i < HPAGE_PMD_NR; i++)
 -		remove_migration_ptes(page + i, page + i, true);
 +static inline int khugepaged_test_exit(struct mm_struct *mm)
 +{
 +	return atomic_read(&mm->mm_users) == 0;
  }
  
 -static void __split_huge_page_tail(struct page *head, int tail,
 -		struct lruvec *lruvec, struct list_head *list)
 +int __khugepaged_enter(struct mm_struct *mm)
  {
 -	struct page *page_tail = head + tail;
 +	struct mm_slot *mm_slot;
 +	int wakeup;
  
 -	VM_BUG_ON_PAGE(atomic_read(&page_tail->_mapcount) != -1, page_tail);
 -	VM_BUG_ON_PAGE(page_ref_count(page_tail) != 0, page_tail);
 +	mm_slot = alloc_mm_slot();
 +	if (!mm_slot)
 +		return -ENOMEM;
  
 -	/*
 -	 * tail_page->_refcount is zero and not changing from under us. But
 -	 * get_page_unless_zero() may be running from under us on the
 -	 * tail_page. If we used atomic_set() below instead of atomic_inc() or
 -	 * atomic_add(), we would then run atomic_set() concurrently with
 -	 * get_page_unless_zero(), and atomic_set() is implemented in C not
 -	 * using locked ops. spin_unlock on x86 sometime uses locked ops
 -	 * because of PPro errata 66, 92, so unless somebody can guarantee
 -	 * atomic_set() here would be safe on all archs (and not only on x86),
 -	 * it's safer to use atomic_inc()/atomic_add().
 -	 */
 -	if (PageAnon(head)) {
 -		page_ref_inc(page_tail);
 -	} else {
 -		/* Additional pin to radix tree */
 -		page_ref_add(page_tail, 2);
 +	/* __khugepaged_exit() must not run from under us */
 +	VM_BUG_ON(khugepaged_test_exit(mm));
 +	if (unlikely(test_and_set_bit(MMF_VM_HUGEPAGE, &mm->flags))) {
 +		free_mm_slot(mm_slot);
 +		return 0;
  	}
  
 -	page_tail->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
 -	page_tail->flags |= (head->flags &
 -			((1L << PG_referenced) |
 -			 (1L << PG_swapbacked) |
 -			 (1L << PG_mlocked) |
 -			 (1L << PG_uptodate) |
 -			 (1L << PG_active) |
 -			 (1L << PG_locked) |
 -			 (1L << PG_unevictable) |
 -			 (1L << PG_dirty)));
 -
 +	spin_lock(&khugepaged_mm_lock);
 +	insert_to_mm_slots_hash(mm, mm_slot);
  	/*
 -	 * After clearing PageTail the gup refcount can be released.
 -	 * Page flags also must be visible before we make the page non-compound.
 +	 * Insert just behind the scanning cursor, to let the area settle
 +	 * down a little.
  	 */
 -	smp_wmb();
 +	wakeup = list_empty(&khugepaged_scan.mm_head);
 +	list_add_tail(&mm_slot->mm_node, &khugepaged_scan.mm_head);
 +	spin_unlock(&khugepaged_mm_lock);
  
 -	clear_compound_head(page_tail);
 +	atomic_inc(&mm->mm_count);
 +	if (wakeup)
 +		wake_up_interruptible(&khugepaged_wait);
  
 -	if (page_is_young(head))
 -		set_page_young(page_tail);
 -	if (page_is_idle(head))
 -		set_page_idle(page_tail);
 -
 -	/* ->mapping in first tail page is compound_mapcount */
 -	VM_BUG_ON_PAGE(tail > 2 && page_tail->mapping != TAIL_MAPPING,
 -			page_tail);
 -	page_tail->mapping = head->mapping;
 -
 -	page_tail->index = head->index + tail;
 -	page_cpupid_xchg_last(page_tail, page_cpupid_last(head));
 -	lru_add_page_tail(head, page_tail, lruvec, list);
 +	return 0;
  }
  
 -static void __split_huge_page(struct page *page, struct list_head *list,
 -		unsigned long flags)
 +int khugepaged_enter_vma_merge(struct vm_area_struct *vma)
  {
 -	struct page *head = compound_head(page);
 -	struct zone *zone = page_zone(head);
 -	struct lruvec *lruvec;
 -	pgoff_t end = -1;
 -	int i;
 -
 -	lruvec = mem_cgroup_page_lruvec(head, zone->zone_pgdat);
 +	unsigned long hstart, hend;
 +	if (!vma->anon_vma)
 +		/*
 +		 * Not yet faulted in so we will register later in the
 +		 * page fault if needed.
 +		 */
 +		return 0;
 +	if (vma->vm_ops)
 +		/* khugepaged not yet working on file or special mappings */
 +		return 0;
 +	VM_BUG_ON(vma->vm_flags & VM_NO_THP);
 +	hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
 +	hend = vma->vm_end & HPAGE_PMD_MASK;
 +	if (hstart < hend)
 +		return khugepaged_enter(vma);
 +	return 0;
 +}
  
 -	/* complete memcg works before add pages to LRU */
 -	mem_cgroup_split_huge_fixup(head);
 +void __khugepaged_exit(struct mm_struct *mm)
 +{
 +	struct mm_slot *mm_slot;
 +	int free = 0;
  
 -	if (!PageAnon(page))
 -		end = DIV_ROUND_UP(i_size_read(head->mapping->host), PAGE_SIZE);
 +	spin_lock(&khugepaged_mm_lock);
 +	mm_slot = get_mm_slot(mm);
 +	if (mm_slot && khugepaged_scan.mm_slot != mm_slot) {
 +		hash_del(&mm_slot->hash);
 +		list_del(&mm_slot->mm_node);
 +		free = 1;
 +	}
 +	spin_unlock(&khugepaged_mm_lock);
  
 -	for (i = HPAGE_PMD_NR - 1; i >= 1; i--) {
 -		__split_huge_page_tail(head, i, lruvec, list);
 -		/* Some pages can be beyond i_size: drop them from page cache */
 -		if (head[i].index >= end) {
 -			__ClearPageDirty(head + i);
 -			__delete_from_page_cache(head + i, NULL);
 -			if (IS_ENABLED(CONFIG_SHMEM) && PageSwapBacked(head))
 -				shmem_uncharge(head->mapping->host, 1);
 -			put_page(head + i);
 -		}
 +	if (free) {
 +		clear_bit(MMF_VM_HUGEPAGE, &mm->flags);
 +		free_mm_slot(mm_slot);
 +		mmdrop(mm);
 +	} else if (mm_slot) {
 +		/*
 +		 * This is required to serialize against
 +		 * khugepaged_test_exit() (which is guaranteed to run
 +		 * under mmap sem read mode). Stop here (after we
 +		 * return all pagetables will be destroyed) until
 +		 * khugepaged has finished working on the pagetables
 +		 * under the mmap_sem.
 +		 */
 +		down_write(&mm->mmap_sem);
 +		up_write(&mm->mmap_sem);
  	}
 +}
  
 -	ClearPageCompound(head);
 -	/* See comment in __split_huge_page_tail() */
 -	if (PageAnon(head)) {
 -		page_ref_inc(head);
 -	} else {
 -		/* Additional pin to radix tree */
 -		page_ref_add(head, 2);
 -		spin_unlock(&head->mapping->tree_lock);
 -	}
 +static void release_pte_page(struct page *page)
 +{
 +	/* 0 stands for page_is_file_cache(page) == false */
 +	dec_zone_page_state(page, NR_ISOLATED_ANON + 0);
 +	unlock_page(page);
 +	putback_lru_page(page);
 +}
  
 -	spin_unlock_irqrestore(zone_lru_lock(page_zone(head)), flags);
 +static void release_pte_pages(pte_t *pte, pte_t *_pte)
 +{
 +	while (--_pte >= pte) {
 +		pte_t pteval = *_pte;
 +		if (!pte_none(pteval))
 +			release_pte_page(pte_page(pteval));
 +	}
 +}
  
 -	unfreeze_page(head);
 +static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 +					unsigned long address,
 +					pte_t *pte)
 +{
 +	struct page *page;
 +	pte_t *_pte;
 +	int referenced = 0, none = 0;
 +	for (_pte = pte; _pte < pte+HPAGE_PMD_NR;
 +	     _pte++, address += PAGE_SIZE) {
 +		pte_t pteval = *_pte;
 +		if (pte_none(pteval)) {
 +			if (!userfaultfd_armed(vma) &&
 +			    ++none <= khugepaged_max_ptes_none)
 +				continue;
 +			else
 +				goto out;
 +		}
 +		if (!pte_present(pteval) || !pte_write(pteval))
 +			goto out;
 +		page = vm_normal_page(vma, address, pteval);
 +		if (unlikely(!page))
 +			goto out;
  
 -	for (i = 0; i < HPAGE_PMD_NR; i++) {
 -		struct page *subpage = head + i;
 -		if (subpage == page)
 -			continue;
 -		unlock_page(subpage);
 +		VM_BUG_ON_PAGE(PageCompound(page), page);
 +		VM_BUG_ON_PAGE(!PageAnon(page), page);
 +		VM_BUG_ON_PAGE(!PageSwapBacked(page), page);
  
 +		/* cannot use mapcount: can't collapse if there's a gup pin */
 +		if (page_count(page) != 1)
 +			goto out;
  		/*
 -		 * Subpages may be freed if there wasn't any mapping
 -		 * like if add_to_swap() is running on a lru page that
 -		 * had its mapping zapped. And freeing these pages
 -		 * requires taking the lru_lock so we do the put_page
 -		 * of the tail pages after the split is complete.
 +		 * We can do it before isolate_lru_page because the
 +		 * page can't be freed from under us. NOTE: PG_lock
 +		 * is needed to serialize against split_huge_page
 +		 * when invoked from the VM.
  		 */
 -		put_page(subpage);
 +		if (!trylock_page(page))
 +			goto out;
 +		/*
 +		 * Isolate the page to avoid collapsing an hugepage
 +		 * currently in use by the VM.
 +		 */
 +		if (isolate_lru_page(page)) {
 +			unlock_page(page);
 +			goto out;
 +		}
 +		/* 0 stands for page_is_file_cache(page) == false */
 +		inc_zone_page_state(page, NR_ISOLATED_ANON + 0);
 +		VM_BUG_ON_PAGE(!PageLocked(page), page);
 +		VM_BUG_ON_PAGE(PageLRU(page), page);
 +
 +		/* If there is no mapped pte young don't collapse the page */
 +		if (pte_young(pteval) || PageReferenced(page) ||
 +		    mmu_notifier_test_young(vma->vm_mm, address))
 +			referenced = 1;
 +	}
 +	if (likely(referenced))
 +		return 1;
 +out:
 +	release_pte_pages(pte, _pte);
 +	return 0;
 +}
 +
 +static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 +				      struct vm_area_struct *vma,
 +				      unsigned long address,
 +				      spinlock_t *ptl)
 +{
 +	pte_t *_pte;
 +	for (_pte = pte; _pte < pte+HPAGE_PMD_NR; _pte++) {
 +		pte_t pteval = *_pte;
 +		struct page *src_page;
 +
 +		if (pte_none(pteval)) {
 +			clear_user_highpage(page, address);
 +			add_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);
 +		} else {
 +			src_page = pte_page(pteval);
 +			copy_user_highpage(page, src_page, address, vma);
 +			VM_BUG_ON_PAGE(page_mapcount(src_page) != 1, src_page);
 +			release_pte_page(src_page);
 +			/*
 +			 * ptl mostly unnecessary, but preempt has to
 +			 * be disabled to update the per-cpu stats
 +			 * inside page_remove_rmap().
 +			 */
 +			spin_lock(ptl);
 +			/*
 +			 * paravirt calls inside pte_clear here are
 +			 * superfluous.
 +			 */
 +			pte_clear(vma->vm_mm, address, _pte);
 +			page_remove_rmap(src_page);
 +			spin_unlock(ptl);
 +			free_page_and_swap_cache(src_page);
 +		}
 +
 +		address += PAGE_SIZE;
 +		page++;
  	}
  }
  
@@@ -2685,373 -2478,153 +2927,452 @@@ out
  	return ret;
  }
  
 -void free_transhuge_page(struct page *page)
 +static void collect_mm_slot(struct mm_slot *mm_slot)
  {
 -	struct pglist_data *pgdata = NODE_DATA(page_to_nid(page));
 -	unsigned long flags;
 +	struct mm_struct *mm = mm_slot->mm;
 +
 +	VM_BUG_ON(NR_CPUS != 1 && !spin_is_locked(&khugepaged_mm_lock));
 +
 +	if (khugepaged_test_exit(mm)) {
 +		/* free mm_slot */
 +		hash_del(&mm_slot->hash);
 +		list_del(&mm_slot->mm_node);
 +
 +		/*
 +		 * Not strictly needed because the mm exited already.
 +		 *
 +		 * clear_bit(MMF_VM_HUGEPAGE, &mm->flags);
 +		 */
  
 -	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
 -	if (!list_empty(page_deferred_list(page))) {
 -		pgdata->split_queue_len--;
 -		list_del(page_deferred_list(page));
 +		/* khugepaged_mm_lock actually not necessary for the below */
 +		free_mm_slot(mm_slot);
 +		mmdrop(mm);
  	}
 -	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
 -	free_compound_page(page);
  }
  
 -void deferred_split_huge_page(struct page *page)
 +static unsigned int khugepaged_scan_mm_slot(unsigned int pages,
 +					    struct page **hpage)
 +	__releases(&khugepaged_mm_lock)
 +	__acquires(&khugepaged_mm_lock)
  {
 -	struct pglist_data *pgdata = NODE_DATA(page_to_nid(page));
 -	unsigned long flags;
 +	struct mm_slot *mm_slot;
 +	struct mm_struct *mm;
 +	struct vm_area_struct *vma;
 +	int progress = 0;
 +
 +	VM_BUG_ON(!pages);
 +	VM_BUG_ON(NR_CPUS != 1 && !spin_is_locked(&khugepaged_mm_lock));
 +
 +	if (khugepaged_scan.mm_slot)
 +		mm_slot = khugepaged_scan.mm_slot;
 +	else {
 +		mm_slot = list_entry(khugepaged_scan.mm_head.next,
 +				     struct mm_slot, mm_node);
 +		khugepaged_scan.address = 0;
 +		khugepaged_scan.mm_slot = mm_slot;
 +	}
 +	spin_unlock(&khugepaged_mm_lock);
 +
 +	mm = mm_slot->mm;
 +	down_read(&mm->mmap_sem);
 +	if (unlikely(khugepaged_test_exit(mm)))
 +		vma = NULL;
 +	else
 +		vma = find_vma(mm, khugepaged_scan.address);
 +
 +	progress++;
 +	for (; vma; vma = vma->vm_next) {
 +		unsigned long hstart, hend;
 +
 +		cond_resched();
 +		if (unlikely(khugepaged_test_exit(mm))) {
 +			progress++;
 +			break;
 +		}
 +		if (!hugepage_vma_check(vma)) {
 +skip:
 +			progress++;
 +			continue;
 +		}
 +		hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
 +		hend = vma->vm_end & HPAGE_PMD_MASK;
 +		if (hstart >= hend)
 +			goto skip;
 +		if (khugepaged_scan.address > hend)
 +			goto skip;
 +		if (khugepaged_scan.address < hstart)
 +			khugepaged_scan.address = hstart;
 +		VM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);
 +
 +		while (khugepaged_scan.address < hend) {
 +			int ret;
 +			cond_resched();
 +			if (unlikely(khugepaged_test_exit(mm)))
 +				goto breakouterloop;
 +
 +			VM_BUG_ON(khugepaged_scan.address < hstart ||
 +				  khugepaged_scan.address + HPAGE_PMD_SIZE >
 +				  hend);
 +			ret = khugepaged_scan_pmd(mm, vma,
 +						  khugepaged_scan.address,
 +						  hpage);
 +			/* move to next address */
 +			khugepaged_scan.address += HPAGE_PMD_SIZE;
 +			progress += HPAGE_PMD_NR;
 +			if (ret)
 +				/* we released mmap_sem so break loop */
 +				goto breakouterloop_mmap_sem;
 +			if (progress >= pages)
 +				goto breakouterloop;
 +		}
 +	}
 +breakouterloop:
 +	up_read(&mm->mmap_sem); /* exit_mmap will destroy ptes after this */
 +breakouterloop_mmap_sem:
  
 -	VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 +	spin_lock(&khugepaged_mm_lock);
 +	VM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);
 +	/*
 +	 * Release the current mm_slot if this mm is about to die, or
 +	 * if we scanned all vmas of this mm.
 +	 */
 +	if (khugepaged_test_exit(mm) || !vma) {
 +		/*
 +		 * Make sure that if mm_users is reaching zero while
 +		 * khugepaged runs here, khugepaged_exit will find
 +		 * mm_slot not pointing to the exiting mm.
 +		 */
 +		if (mm_slot->mm_node.next != &khugepaged_scan.mm_head) {
 +			khugepaged_scan.mm_slot = list_entry(
 +				mm_slot->mm_node.next,
 +				struct mm_slot, mm_node);
 +			khugepaged_scan.address = 0;
 +		} else {
 +			khugepaged_scan.mm_slot = NULL;
 +			khugepaged_full_scans++;
 +		}
  
 -	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
 -	if (list_empty(page_deferred_list(page))) {
 -		count_vm_event(THP_DEFERRED_SPLIT_PAGE);
 -		list_add_tail(page_deferred_list(page), &pgdata->split_queue);
 -		pgdata->split_queue_len++;
 +		collect_mm_slot(mm_slot);
  	}
 -	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
 +
 +	return progress;
  }
  
 -static unsigned long deferred_split_count(struct shrinker *shrink,
 -		struct shrink_control *sc)
 +static int khugepaged_has_work(void)
  {
 -	struct pglist_data *pgdata = NODE_DATA(sc->nid);
 -	return ACCESS_ONCE(pgdata->split_queue_len);
 +	return !list_empty(&khugepaged_scan.mm_head) &&
 +		khugepaged_enabled();
  }
  
 -static unsigned long deferred_split_scan(struct shrinker *shrink,
 -		struct shrink_control *sc)
 +static int khugepaged_wait_event(void)
  {
 -	struct pglist_data *pgdata = NODE_DATA(sc->nid);
 -	unsigned long flags;
 -	LIST_HEAD(list), *pos, *next;
 -	struct page *page;
 -	int split = 0;
 -
 -	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
 -	/* Take pin on all head pages to avoid freeing them under us */
 -	list_for_each_safe(pos, next, &pgdata->split_queue) {
 -		page = list_entry((void *)pos, struct page, mapping);
 -		page = compound_head(page);
 -		if (get_page_unless_zero(page)) {
 -			list_move(page_deferred_list(page), &list);
 -		} else {
 -			/* We lost race with put_compound_page() */
 -			list_del_init(page_deferred_list(page));
 -			pgdata->split_queue_len--;
 -		}
 -		if (!--sc->nr_to_scan)
 +	return !list_empty(&khugepaged_scan.mm_head) ||
 +		kthread_should_stop();
 +}
 +
 +static void khugepaged_do_scan(void)
 +{
 +	struct page *hpage = NULL;
 +	unsigned int progress = 0, pass_through_head = 0;
 +	unsigned int pages = khugepaged_pages_to_scan;
 +	bool wait = true;
 +
 +	barrier(); /* write khugepaged_pages_to_scan to local stack */
 +
 +	while (progress < pages) {
 +		if (!khugepaged_prealloc_page(&hpage, &wait))
 +			break;
 +
 +		cond_resched();
 +
 +		if (unlikely(kthread_should_stop() || freezing(current)))
  			break;
 +
 +		spin_lock(&khugepaged_mm_lock);
 +		if (!khugepaged_scan.mm_slot)
 +			pass_through_head++;
 +		if (khugepaged_has_work() &&
 +		    pass_through_head < 2)
 +			progress += khugepaged_scan_mm_slot(pages - progress,
 +							    &hpage);
 +		else
 +			progress = pages;
 +		spin_unlock(&khugepaged_mm_lock);
  	}
 -	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
  
 -	list_for_each_safe(pos, next, &list) {
 -		page = list_entry((void *)pos, struct page, mapping);
 -		lock_page(page);
 -		/* split_huge_page() removes page from list on success */
 -		if (!split_huge_page(page))
 -			split++;
 -		unlock_page(page);
 -		put_page(page);
 +	if (!IS_ERR_OR_NULL(hpage))
 +		put_page(hpage);
 +}
 +
 +static void khugepaged_wait_work(void)
 +{
 +	try_to_freeze();
 +
 +	if (khugepaged_has_work()) {
 +		if (!khugepaged_scan_sleep_millisecs)
 +			return;
 +
 +		wait_event_freezable_timeout(khugepaged_wait,
 +					     kthread_should_stop(),
 +			msecs_to_jiffies(khugepaged_scan_sleep_millisecs));
 +		return;
  	}
  
 -	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
 -	list_splice_tail(&list, &pgdata->split_queue);
 -	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
 +	if (khugepaged_enabled())
 +		wait_event_freezable(khugepaged_wait, khugepaged_wait_event());
 +}
  
 +static int khugepaged(void *none)
 +{
 +	struct mm_slot *mm_slot;
 +
 +	set_freezable();
 +	set_user_nice(current, 19);
 +
 +	while (!kthread_should_stop()) {
 +		khugepaged_do_scan();
 +		khugepaged_wait_work();
 +	}
 +
 +	spin_lock(&khugepaged_mm_lock);
 +	mm_slot = khugepaged_scan.mm_slot;
 +	khugepaged_scan.mm_slot = NULL;
 +	if (mm_slot)
 +		collect_mm_slot(mm_slot);
 +	spin_unlock(&khugepaged_mm_lock);
 +	return 0;
 +}
++=======
++/*
++ * Returns true if a given pud maps a thp, false otherwise.
++ *
++ * Note that if it returns true, this routine returns without unlocking page
++ * table lock. So callers must unlock it.
++ */
++spinlock_t *__pud_trans_huge_lock(pud_t *pud, struct vm_area_struct *vma)
++{
++	spinlock_t *ptl;
++
++	ptl = pud_lock(vma->vm_mm, pud);
++	if (likely(pud_trans_huge(*pud) || pud_devmap(*pud)))
++		return ptl;
++	spin_unlock(ptl);
++	return NULL;
++}
++
++#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
++int zap_huge_pud(struct mmu_gather *tlb, struct vm_area_struct *vma,
++		 pud_t *pud, unsigned long addr)
++{
++	pud_t orig_pud;
++	spinlock_t *ptl;
++
++	ptl = __pud_trans_huge_lock(pud, vma);
++	if (!ptl)
++		return 0;
+ 	/*
 -	 * Stop shrinker if we didn't split any page, but the queue is empty.
 -	 * This can happen if pages were freed under us.
++	 * For architectures like ppc64 we look at deposited pgtable
++	 * when calling pudp_huge_get_and_clear. So do the
++	 * pgtable_trans_huge_withdraw after finishing pudp related
++	 * operations.
+ 	 */
 -	if (!split && list_empty(&pgdata->split_queue))
 -		return SHRINK_STOP;
 -	return split;
++	orig_pud = pudp_huge_get_and_clear_full(tlb->mm, addr, pud,
++			tlb->fullmm);
++	tlb_remove_pud_tlb_entry(tlb, pud, addr);
++	if (vma_is_dax(vma)) {
++		spin_unlock(ptl);
++		/* No zero page support yet */
++	} else {
++		/* No support for anonymous PUD pages yet */
++		BUG();
++	}
++	return 1;
+ }
+ 
 -static struct shrinker deferred_split_shrinker = {
 -	.count_objects = deferred_split_count,
 -	.scan_objects = deferred_split_scan,
 -	.seeks = DEFAULT_SEEKS,
 -	.flags = SHRINKER_NUMA_AWARE,
 -};
++static void __split_huge_pud_locked(struct vm_area_struct *vma, pud_t *pud,
++		unsigned long haddr)
++{
++	VM_BUG_ON(haddr & ~HPAGE_PUD_MASK);
++	VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
++	VM_BUG_ON_VMA(vma->vm_end < haddr + HPAGE_PUD_SIZE, vma);
++	VM_BUG_ON(!pud_trans_huge(*pud) && !pud_devmap(*pud));
++
++	count_vm_event(THP_SPLIT_PMD);
+ 
 -#ifdef CONFIG_DEBUG_FS
 -static int split_huge_pages_set(void *data, u64 val)
++	pudp_huge_clear_flush_notify(vma, haddr, pud);
++}
++
++void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,
++		unsigned long address)
+ {
 -	struct zone *zone;
 -	struct page *page;
 -	unsigned long pfn, max_zone_pfn;
 -	unsigned long total = 0, split = 0;
++	spinlock_t *ptl;
++	struct mm_struct *mm = vma->vm_mm;
++	unsigned long haddr = address & HPAGE_PUD_MASK;
+ 
 -	if (val != 1)
 -		return -EINVAL;
++	mmu_notifier_invalidate_range_start(mm, haddr, haddr + HPAGE_PUD_SIZE);
++	ptl = pud_lock(mm, pud);
++	if (unlikely(!pud_trans_huge(*pud) && !pud_devmap(*pud)))
++		goto out;
++	__split_huge_pud_locked(vma, pud, haddr);
+ 
 -	for_each_populated_zone(zone) {
 -		max_zone_pfn = zone_end_pfn(zone);
 -		for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++) {
 -			if (!pfn_valid(pfn))
 -				continue;
++out:
++	spin_unlock(ptl);
++	mmu_notifier_invalidate_range_end(mm, haddr, haddr + HPAGE_PUD_SIZE);
++}
++#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
 -			page = pfn_to_page(pfn);
 -			if (!get_page_unless_zero(page))
 -				continue;
 +static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 +		unsigned long haddr, pmd_t *pmd)
 +{
 +	struct mm_struct *mm = vma->vm_mm;
 +	pgtable_t pgtable;
 +	pmd_t _pmd;
 +	int i;
 +
 +	pmdp_clear_flush_notify(vma, haddr, pmd);
 +	/* leave pmd empty until pte is filled */
  
 -			if (zone != page_zone(page))
 -				goto next;
 +	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 +	pmd_populate(mm, &_pmd, pgtable);
  
 -			if (!PageHead(page) || PageHuge(page) || !PageLRU(page))
 -				goto next;
 +	for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
 +		pte_t *pte, entry;
 +		entry = pfn_pte(my_zero_pfn(haddr), vma->vm_page_prot);
 +		entry = pte_mkspecial(entry);
 +		pte = pte_offset_map(&_pmd, haddr);
 +		VM_BUG_ON(!pte_none(*pte));
 +		set_pte_at(mm, haddr, pte, entry);
 +		pte_unmap(pte);
 +	}
 +	smp_wmb(); /* make pte visible before pmd */
 +	pmd_populate(mm, pmd, pgtable);
 +	put_huge_zero_page();
 +}
  
 -			total++;
 -			lock_page(page);
 -			if (!split_huge_page(page))
 -				split++;
 -			unlock_page(page);
 -next:
 -			put_page(page);
 -		}
 +void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,
 +		pmd_t *pmd)
 +{
 +	spinlock_t *ptl;
 +	struct page *page = NULL;
 +	struct mm_struct *mm = vma->vm_mm;
 +	unsigned long haddr = address & HPAGE_PMD_MASK;
 +	unsigned long mmun_start;	/* For mmu_notifiers */
 +	unsigned long mmun_end;		/* For mmu_notifiers */
 +
 +	BUG_ON(vma->vm_start > haddr || vma->vm_end < haddr + HPAGE_PMD_SIZE);
 +
 +	mmun_start = haddr;
 +	mmun_end   = haddr + HPAGE_PMD_SIZE;
 +again:
 +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 +	ptl = pmd_lock(mm, pmd);
 +	if (unlikely(!pmd_trans_huge(*pmd)))
 +		goto unlock;
 +	if (vma_is_dax(vma)) {
 +		pmd_t _pmd = pmdp_clear_flush_notify(vma, haddr, pmd);
 +		if (is_huge_zero_pmd(_pmd))
 +			put_huge_zero_page();
 +	} else if (is_huge_zero_pmd(*pmd)) {
 +		__split_huge_zero_page_pmd(vma, haddr, pmd);
 +	} else {
 +		page = pmd_page(*pmd);
 +		VM_BUG_ON_PAGE(!page_count(page), page);
 +		get_page(page);
  	}
 + unlock:
 +	spin_unlock(ptl);
 +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
  
 -	pr_info("%lu of %lu THP split\n", split, total);
 +	if (!page)
 +		return;
  
 -	return 0;
 +	split_huge_page(page);
 +	put_page(page);
 +
 +	/*
 +	 * We don't always have down_write of mmap_sem here: a racing
 +	 * do_huge_pmd_wp_page() might have copied-on-write to another
 +	 * huge page before our split_huge_page() got the anon_vma lock.
 +	 */
 +	if (unlikely(pmd_trans_huge(*pmd)))
 +		goto again;
  }
 -DEFINE_SIMPLE_ATTRIBUTE(split_huge_pages_fops, NULL, split_huge_pages_set,
 -		"%llu\n");
  
 -static int __init split_huge_pages_debugfs(void)
 +void split_huge_page_pmd_mm(struct mm_struct *mm, unsigned long address,
 +		pmd_t *pmd)
  {
 -	void *ret;
 +	struct vm_area_struct *vma;
  
 -	ret = debugfs_create_file("split_huge_pages", 0200, NULL, NULL,
 -			&split_huge_pages_fops);
 -	if (!ret)
 -		pr_warn("Failed to create split_huge_pages in debugfs");
 -	return 0;
 +	vma = find_vma(mm, address);
 +	BUG_ON(vma == NULL);
 +	split_huge_page_pmd(vma, address, pmd);
 +}
 +
 +static void split_huge_page_address(struct mm_struct *mm,
 +				    unsigned long address)
 +{
 +	pgd_t *pgd;
 +	pud_t *pud;
 +	pmd_t *pmd;
 +
 +	VM_BUG_ON(!(address & ~HPAGE_PMD_MASK));
 +
 +	pgd = pgd_offset(mm, address);
 +	if (!pgd_present(*pgd))
 +		return;
 +
 +	pud = pud_offset(pgd, address);
 +	if (!pud_present(*pud))
 +		return;
 +
 +	pmd = pmd_offset(pud, address);
 +	if (!pmd_present(*pmd))
 +		return;
 +	/*
 +	 * Caller holds the mmap_sem write mode, so a huge pmd cannot
 +	 * materialize from under us.
 +	 */
 +	split_huge_page_pmd_mm(mm, address, pmd);
 +}
 +
 +void vma_adjust_trans_huge(struct vm_area_struct *vma,
 +			     unsigned long start,
 +			     unsigned long end,
 +			     long adjust_next)
 +{
 +	/*
 +	 * If the new start address isn't hpage aligned and it could
 +	 * previously contain an hugepage: check if we need to split
 +	 * an huge pmd.
 +	 */
 +	if (start & ~HPAGE_PMD_MASK &&
 +	    (start & HPAGE_PMD_MASK) >= vma->vm_start &&
 +	    (start & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)
 +		split_huge_page_address(vma->vm_mm, start);
 +
 +	/*
 +	 * If the new end address isn't hpage aligned and it could
 +	 * previously contain an hugepage: check if we need to split
 +	 * an huge pmd.
 +	 */
 +	if (end & ~HPAGE_PMD_MASK &&
 +	    (end & HPAGE_PMD_MASK) >= vma->vm_start &&
 +	    (end & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)
 +		split_huge_page_address(vma->vm_mm, end);
 +
 +	/*
 +	 * If we're also updating the vma->vm_next->vm_start, if the new
 +	 * vm_next->vm_start isn't page aligned and it could previously
 +	 * contain an hugepage: check if we need to split an huge pmd.
 +	 */
 +	if (adjust_next > 0) {
 +		struct vm_area_struct *next = vma->vm_next;
 +		unsigned long nstart = next->vm_start;
 +		nstart += adjust_next << PAGE_SHIFT;
 +		if (nstart & ~HPAGE_PMD_MASK &&
 +		    (nstart & HPAGE_PMD_MASK) >= next->vm_start &&
 +		    (nstart & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= next->vm_end)
 +			split_huge_page_address(next->vm_mm, nstart);
 +	}
  }
 -late_initcall(split_huge_pages_debugfs);
 -#endif
diff --cc mm/memory.c
index 14270187456b,41e2a2d4b2a6..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -1004,9 -999,9 +1004,9 @@@ static inline int copy_pmd_range(struc
  	src_pmd = pmd_offset(src_pud, addr);
  	do {
  		next = pmd_addr_end(addr, end);
 -		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {
 +		if (pmd_trans_huge(*src_pmd)) {
  			int err;
- 			VM_BUG_ON(next-addr != HPAGE_PMD_SIZE);
+ 			VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);
  			err = copy_huge_pmd(dst_mm, src_mm,
  					    dst_pmd, src_pmd, addr, vma);
  			if (err == -ENOMEM)
@@@ -3194,6 -3507,35 +3216,38 @@@ static int wp_huge_pmd(struct mm_struc
  	return VM_FAULT_FALLBACK;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool vma_is_accessible(struct vm_area_struct *vma)
+ {
+ 	return vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE);
+ }
+ 
+ static int create_huge_pud(struct vm_fault *vmf)
+ {
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	/* No support for anonymous transparent PUD pages yet */
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return VM_FAULT_FALLBACK;
+ 	if (vmf->vma->vm_ops->huge_fault)
+ 		return vmf->vma->vm_ops->huge_fault(vmf);
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
+ {
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	/* No support for anonymous transparent PUD pages yet */
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return VM_FAULT_FALLBACK;
+ 	if (vmf->vma->vm_ops->huge_fault)
+ 		return vmf->vma->vm_ops->huge_fault(vmf);
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  /*
   * These routines also need to handle stuff like marking pages dirty
   * and/or accessed for architectures that don't do it in hardware (most
@@@ -3263,57 -3635,84 +3317,94 @@@ unlock
  
  /*
   * By the time we get here, we already hold the mm semaphore
 - *
 - * The mmap_sem may have been released depending on flags and our
 - * return value.  See filemap_fault() and __lock_page_or_retry().
   */
 -static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 -		unsigned int flags)
 +static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 +			     unsigned long address, unsigned int flags)
  {
 -	struct vm_fault vmf = {
 -		.vma = vma,
 -		.address = address & PAGE_MASK,
 -		.flags = flags,
 -		.pgoff = linear_page_index(vma, address),
 -		.gfp_mask = __get_fault_gfp_mask(vma),
 -	};
 -	struct mm_struct *mm = vma->vm_mm;
  	pgd_t *pgd;
++<<<<<<< HEAD
 +	pud_t *pud;
 +	pmd_t *pmd;
 +	pte_t *pte;
 +
 +	if (unlikely(is_vm_hugetlb_page(vma)))
 +		return hugetlb_fault(mm, vma, address, flags);
++=======
+ 	int ret;
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  
  	pgd = pgd_offset(mm, address);
- 	pud = pud_alloc(mm, pgd, address);
- 	if (!pud)
+ 
+ 	vmf.pud = pud_alloc(mm, pgd, address);
+ 	if (!vmf.pud)
  		return VM_FAULT_OOM;
++<<<<<<< HEAD
 +	pmd = pmd_alloc(mm, pud, address);
 +	if (!pmd)
++=======
+ 	if (pud_none(*vmf.pud) && transparent_hugepage_enabled(vma)) {
+ 		vmf.flags |= FAULT_FLAG_SIZE_PUD;
+ 		ret = create_huge_pud(&vmf);
+ 		if (!(ret & VM_FAULT_FALLBACK))
+ 			return ret;
+ 	} else {
+ 		pud_t orig_pud = *vmf.pud;
+ 
+ 		barrier();
+ 		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {
+ 			unsigned int dirty = flags & FAULT_FLAG_WRITE;
+ 
+ 			vmf.flags |= FAULT_FLAG_SIZE_PUD;
+ 
+ 			/* NUMA case for anonymous PUDs would go here */
+ 
+ 			if (dirty && !pud_write(orig_pud)) {
+ 				ret = wp_huge_pud(&vmf, orig_pud);
+ 				if (!(ret & VM_FAULT_FALLBACK))
+ 					return ret;
+ 			} else {
+ 				huge_pud_set_accessed(&vmf, orig_pud);
+ 				return 0;
+ 			}
+ 		}
+ 	}
+ 
+ 	vmf.pmd = pmd_alloc(mm, vmf.pud, address);
+ 	if (!vmf.pmd)
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  		return VM_FAULT_OOM;
 -	if (pmd_none(*vmf.pmd) && transparent_hugepage_enabled(vma)) {
 -		vmf.flags |= FAULT_FLAG_SIZE_PMD;
 -		ret = create_huge_pmd(&vmf);
 +	if (pmd_none(*pmd) && transparent_hugepage_enabled(vma)) {
 +		int ret = create_huge_pmd(mm, vma, address, pmd, flags);
  		if (!(ret & VM_FAULT_FALLBACK))
  			return ret;
 -		/* fall through path, remove PMD flag */
 -		vmf.flags &= ~FAULT_FLAG_SIZE_PMD;
  	} else {
 -		pmd_t orig_pmd = *vmf.pmd;
 +		pmd_t orig_pmd = *pmd;
 +		int ret;
  
  		barrier();
 -		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
 -			vmf.flags |= FAULT_FLAG_SIZE_PMD;
 -			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
 -				return do_huge_pmd_numa_page(&vmf, orig_pmd);
 -
 -			if ((vmf.flags & FAULT_FLAG_WRITE) &&
 -					!pmd_write(orig_pmd)) {
 -				ret = wp_huge_pmd(&vmf, orig_pmd);
 +		if (pmd_trans_huge(orig_pmd)) {
 +			unsigned int dirty = flags & FAULT_FLAG_WRITE;
 +
 +			/*
 +			 * If the pmd is splitting, return and retry the
 +			 * the fault.  Alternative: wait until the split
 +			 * is done, and goto retry.
 +			 */
 +			if (pmd_trans_splitting(orig_pmd))
 +				return 0;
 +
 +			if (pmd_numa(orig_pmd))
 +				return do_huge_pmd_numa_page(mm, vma, address,
 +							     orig_pmd, pmd);
 +
 +			if (dirty && !pmd_write(orig_pmd)) {
 +				ret = wp_huge_pmd(mm, vma, address, pmd,
 +							orig_pmd, flags);
  				if (!(ret & VM_FAULT_FALLBACK))
  					return ret;
 -				/* fall through path, remove PUD flag */
 -				vmf.flags &= ~FAULT_FLAG_SIZE_PUD;
  			} else {
 -				huge_pmd_set_accessed(&vmf, orig_pmd);
 +				huge_pmd_set_accessed(mm, vma, address, pmd,
 +						      orig_pmd, dirty);
  				return 0;
  			}
  		}
@@@ -3425,19 -3823,21 +3517,19 @@@ int __pmd_alloc(struct mm_struct *mm, p
  
  	smp_wmb(); /* See comment in __pte_alloc */
  
- 	spin_lock(&mm->page_table_lock);
+ 	ptl = pud_lock(mm, pud);
  #ifndef __ARCH_HAS_4LEVEL_HACK
 -	if (!pud_present(*pud)) {
 -		mm_inc_nr_pmds(mm);
 -		pud_populate(mm, pud, new);
 -	} else	/* Another has populated it */
 +	if (pud_present(*pud))		/* Another has populated it */
  		pmd_free(mm, new);
 +	else
 +		pud_populate(mm, pud, new);
  #else
 -	if (!pgd_present(*pud)) {
 -		mm_inc_nr_pmds(mm);
 -		pgd_populate(mm, pud, new);
 -	} else /* Another has populated it */
 +	if (pgd_present(*pud))		/* Another has populated it */
  		pmd_free(mm, new);
 +	else
 +		pgd_populate(mm, pud, new);
  #endif /* __ARCH_HAS_4LEVEL_HACK */
- 	spin_unlock(&mm->page_table_lock);
+ 	spin_unlock(ptl);
  	return 0;
  }
  #endif /* __PAGETABLE_PMD_FOLDED */
diff --cc mm/pagewalk.c
index 2beeabf502c5,03761577ae86..000000000000
--- a/mm/pagewalk.c
+++ b/mm/pagewalk.c
@@@ -86,9 -87,24 +87,30 @@@ static int walk_pud_range(pgd_t *pgd, u
  				break;
  			continue;
  		}
++<<<<<<< HEAD
 +		if (walk->pud_entry)
 +			err = walk->pud_entry(pud, addr, next, walk);
 +		if (!err && (walk->pmd_entry || walk->pte_entry))
++=======
+ 
+ 		if (walk->pud_entry) {
+ 			spinlock_t *ptl = pud_trans_huge_lock(pud, walk->vma);
+ 
+ 			if (ptl) {
+ 				err = walk->pud_entry(pud, addr, next, walk);
+ 				spin_unlock(ptl);
+ 				if (err)
+ 					break;
+ 				continue;
+ 			}
+ 		}
+ 
+ 		split_huge_pud(walk->vma, pud, addr);
+ 		if (pud_none(*pud))
+ 			goto again;
+ 
+ 		if (walk->pmd_entry || walk->pte_entry)
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  			err = walk_pmd_range(pud, addr, next, walk);
  		if (err)
  			break;
diff --cc mm/pgtable-generic.c
index d0b6afa621ef,4ed5908c65b0..000000000000
--- a/mm/pgtable-generic.c
+++ b/mm/pgtable-generic.c
@@@ -95,32 -82,61 +95,49 @@@ pte_t ptep_clear_flush(struct vm_area_s
  }
  #endif
  
 +#ifndef __HAVE_ARCH_PMDP_CLEAR_FLUSH
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -
 -#ifndef __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS
 -int pmdp_set_access_flags(struct vm_area_struct *vma,
 -			  unsigned long address, pmd_t *pmdp,
 -			  pmd_t entry, int dirty)
 +pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,
 +		       pmd_t *pmdp)
  {
 -	int changed = !pmd_same(*pmdp, entry);
 -	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 -	if (changed) {
 -		set_pmd_at(vma->vm_mm, address, pmdp, entry);
 -		flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 -	}
 -	return changed;
 -}
 -#endif
 -
 -#ifndef __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH
 -int pmdp_clear_flush_young(struct vm_area_struct *vma,
 -			   unsigned long address, pmd_t *pmdp)
 -{
 -	int young;
 +	pmd_t pmd;
  	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 -	young = pmdp_test_and_clear_young(vma, address, pmdp);
 -	if (young)
 -		flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 -	return young;
 +	pmd = pmdp_get_and_clear(vma->vm_mm, address, pmdp);
 +	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 +	return pmd;
  }
++<<<<<<< HEAD
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  #endif
  
 -#ifndef __HAVE_ARCH_PMDP_HUGE_CLEAR_FLUSH
 -pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,
 -			    pmd_t *pmdp)
 +#ifndef __HAVE_ARCH_PMDP_SPLITTING_FLUSH
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
 +			  pmd_t *pmdp)
  {
 -	pmd_t pmd;
 +	pmd_t pmd = pmd_mksplitting(*pmdp);
  	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 -	VM_BUG_ON(!pmd_trans_huge(*pmdp) && !pmd_devmap(*pmdp));
 -	pmd = pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);
 -	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 -	return pmd;
 +	set_pmd_at(vma->vm_mm, address, pmdp, pmd);
 +	/* tlb flush only to serialize against gup-fast */
 +	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
  }
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++=======
+ 
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ pud_t pudp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,
+ 			    pud_t *pudp)
+ {
+ 	pud_t pud;
+ 
+ 	VM_BUG_ON(address & ~HPAGE_PUD_MASK);
+ 	VM_BUG_ON(!pud_trans_huge(*pudp) && !pud_devmap(*pudp));
+ 	pud = pudp_huge_get_and_clear(vma->vm_mm, address, pudp);
+ 	flush_pud_tlb_range(vma, address, address + HPAGE_PUD_SIZE);
+ 	return pud;
+ }
+ #endif
++>>>>>>> a00cc7d9dd93 (mm, x86: add support for PUD-sized transparent hugepages)
  #endif
  
  #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
* Unmerged path arch/Kconfig
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h
index ddc1e7942cf2..c3d616d31a0b 100644
--- a/arch/x86/include/asm/paravirt.h
+++ b/arch/x86/include/asm/paravirt.h
@@ -502,6 +502,17 @@ static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
 			    native_pmd_val(pmd));
 }
 
+static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,
+			      pud_t *pudp, pud_t pud)
+{
+	if (sizeof(pudval_t) > sizeof(long))
+		/* 5 arg words */
+		pv_mmu_ops.set_pud_at(mm, addr, pudp, pud);
+	else
+		PVOP_VCALL4(pv_mmu_ops.set_pud_at, mm, addr, pudp,
+			    native_pud_val(pud));
+}
+
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
 	pmdval_t val = native_pmd_val(pmd);
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index bd191f0b88c0..703c3c55179f 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -280,6 +280,8 @@ struct pv_mmu_ops {
 	void (*set_pmd)(pmd_t *pmdp, pmd_t pmdval);
 	void (*set_pmd_at)(struct mm_struct *mm, unsigned long addr,
 			   pmd_t *pmdp, pmd_t pmdval);
+	void (*set_pud_at)(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pudval);
 	void (*pte_update)(struct mm_struct *mm, unsigned long addr,
 			   pte_t *ptep);
 	void (*pte_update_defer)(struct mm_struct *mm,
* Unmerged path arch/x86/include/asm/pgtable-2level.h
* Unmerged path arch/x86/include/asm/pgtable-3level.h
* Unmerged path arch/x86/include/asm/pgtable.h
diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
index e3f551b239b4..a3c56b4d68b3 100644
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@ -113,6 +113,21 @@ static inline void native_pud_clear(pud_t *pud)
 	native_set_pud(pud, native_make_pud(0));
 }
 
+static inline pud_t native_pudp_get_and_clear(pud_t *xp)
+{
+#ifdef CONFIG_SMP
+	return native_make_pud(xchg(&xp->pud, 0));
+#else
+	/* native_local_pudp_get_and_clear,
+	 * but duplicated because of cyclic dependency
+	 */
+	pud_t ret = *xp;
+
+	native_pud_clear(xp);
+	return ret;
+#endif
+}
+
 static inline void native_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
 	mm_track_pgd(pgdp);
diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c
index 0e8d1e742550..4df6da213340 100644
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@ -456,6 +456,7 @@ struct pv_mmu_ops pv_mmu_ops = {
 	.pmd_clear = native_pmd_clear,
 #endif
 	.set_pud = native_set_pud,
+	.set_pud_at = native_set_pud_at,
 
 	.pmd_val = PTE_IDENT,
 	.make_pmd = PTE_IDENT,
diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
index 78194902ac3a..c3c8b86660cc 100644
--- a/arch/x86/mm/pgtable.c
+++ b/arch/x86/mm/pgtable.c
@@ -365,6 +365,26 @@ int pmdp_set_access_flags(struct vm_area_struct *vma,
 
 	return changed;
 }
+
+int pudp_set_access_flags(struct vm_area_struct *vma, unsigned long address,
+			  pud_t *pudp, pud_t entry, int dirty)
+{
+	int changed = !pud_same(*pudp, entry);
+
+	VM_BUG_ON(address & ~HPAGE_PUD_MASK);
+
+	if (changed && dirty) {
+		*pudp = entry;
+		/*
+		 * We had a write-protection fault here and changed the pud
+		 * to to more permissive. No need to flush the TLB for that,
+		 * #PF is architecturally guaranteed to do that and in the
+		 * worst-case we'll generate a spurious fault.
+		 */
+	}
+
+	return changed;
+}
 #endif
 
 int ptep_test_and_clear_young(struct vm_area_struct *vma,
@@ -397,6 +417,17 @@ int pmdp_test_and_clear_young(struct vm_area_struct *vma,
 
 	return ret;
 }
+int pudp_test_and_clear_young(struct vm_area_struct *vma,
+			      unsigned long addr, pud_t *pudp)
+{
+	int ret = 0;
+
+	if (pud_young(*pudp))
+		ret = test_and_clear_bit(_PAGE_BIT_ACCESSED,
+					 (unsigned long *)pudp);
+
+	return ret;
+}
 #endif
 
 int ptep_clear_flush_young(struct vm_area_struct *vma,
* Unmerged path include/asm-generic/pgtable.h
* Unmerged path include/asm-generic/tlb.h
* Unmerged path include/linux/huge_mm.h
* Unmerged path include/linux/mm.h
* Unmerged path include/linux/mmu_notifier.h
diff --git a/include/linux/pfn_t.h b/include/linux/pfn_t.h
index a3d90b9da18d..91dad1588606 100644
--- a/include/linux/pfn_t.h
+++ b/include/linux/pfn_t.h
@@ -84,6 +84,13 @@ static inline pmd_t pfn_t_pmd(pfn_t pfn, pgprot_t pgprot)
 {
 	return pfn_pmd(pfn_t_to_pfn(pfn), pgprot);
 }
+
+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+static inline pud_t pfn_t_pud(pfn_t pfn, pgprot_t pgprot)
+{
+	return pfn_pud(pfn_t_to_pfn(pfn), pgprot);
+}
+#endif
 #endif
 
 #ifdef __HAVE_ARCH_PTE_DEVMAP
@@ -100,5 +107,10 @@ static inline bool pfn_t_devmap(pfn_t pfn)
 }
 pte_t pte_mkdevmap(pte_t pte);
 pmd_t pmd_mkdevmap(pmd_t pmd);
+#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && \
+	defined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)
+pud_t pud_mkdevmap(pud_t pud);
 #endif
+#endif /* __HAVE_ARCH_PTE_DEVMAP */
+
 #endif /* _LINUX_PFN_T_H_ */
* Unmerged path mm/gup.c
* Unmerged path mm/huge_memory.c
* Unmerged path mm/memory.c
* Unmerged path mm/pagewalk.c
* Unmerged path mm/pgtable-generic.c
