Revert "block: use DAX for partition table reads"

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit a41fe02b6bba853a29c864d00fd161bbe6cfc715
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/a41fe02b.failed

commit d1a5f2b4d8a1 ("block: use DAX for partition table reads") was
part of a stalled effort to allow dax mappings of block devices. Since
then the device-dax mechanism has filled the role of dax-mapping static
device ranges.

Now that we are moving ->direct_access() from a block_device operation
to a dax_inode operation we would need block devices to map and carry
their own dax_inode reference.

Unless / until we decide to revive dax mapping of raw block devices
through the dax_inode scheme, there is no need to carry
read_dax_sector(). Its removal in turn allows for the removal of
bdev_direct_access() and should have been included in commit
223757016837 ("block_dev: remove DAX leftovers").

	Cc: Jeff Moyer <jmoyer@redhat.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit a41fe02b6bba853a29c864d00fd161bbe6cfc715)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/partition-generic.c
#	fs/dax.c
#	include/linux/dax.h
diff --cc block/partition-generic.c
index 66f35dbbdcf2,5dfac337b0f2..000000000000
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@@ -550,24 -630,12 +549,23 @@@ int invalidate_partitions(struct gendis
  	return 0;
  }
  
++<<<<<<< HEAD
 +static struct page *read_pagecache_sector(struct block_device *bdev, sector_t n)
 +{
 +	struct address_space *mapping = bdev->bd_inode->i_mapping;
 +
 +	return read_mapping_page(mapping, (pgoff_t)(n >> (PAGE_CACHE_SHIFT-9)),
 +			NULL);
 +}
 +
++=======
++>>>>>>> a41fe02b6bba (Revert "block: use DAX for partition table reads")
  unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p)
  {
+ 	struct address_space *mapping = bdev->bd_inode->i_mapping;
  	struct page *page;
  
- 	/* don't populate page cache for dax capable devices */
- 	if (IS_DAX(bdev->bd_inode))
- 		page = read_dax_sector(bdev, n);
- 	else
- 		page = read_pagecache_sector(bdev, n);
- 
+ 	page = read_mapping_page(mapping, (pgoff_t)(n >> (PAGE_SHIFT-9)), NULL);
  	if (!IS_ERR(page)) {
  		if (PageError(page))
  			goto fail;
diff --cc fs/dax.c
index 1dfecdfb6245,b78a6947c4f5..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -75,209 -81,26 +75,232 @@@ static void dax_unmap_atomic(struct blo
  	blk_queue_exit(bdev->bd_queue);
  }
  
++<<<<<<< HEAD
 +struct page *read_dax_sector(struct block_device *bdev, sector_t n)
 +{
 +	struct page *page = alloc_pages(GFP_KERNEL, 0);
 +	struct blk_dax_ctl dax = {
 +		.size = PAGE_SIZE,
 +		.sector = n & ~((((int) PAGE_SIZE) / 512) - 1),
 +	};
 +	long rc;
 +
 +	if (!page)
 +		return ERR_PTR(-ENOMEM);
 +
 +	rc = dax_map_atomic(bdev, &dax);
 +	if (rc < 0)
 +		return ERR_PTR(rc);
 +	memcpy_from_pmem(page_address(page), dax.addr, PAGE_SIZE);
 +	dax_unmap_atomic(bdev, &dax);
 +	return page;
 +}
 +
 +static bool buffer_written(struct buffer_head *bh)
 +{
 +	return buffer_mapped(bh) && !buffer_unwritten(bh);
 +}
 +
 +static int zero_toiovecend_partial(const struct iovec *iov, int offset, int len)
 +{
 +	int zero, orig_len = len;
 +	for (; len > 0; ++iov) {
 +		/* Skip over the finished iovecs */
 +		if (unlikely(offset >= iov->iov_len)) {
 +			offset -= iov->iov_len;
 +			continue;
 +		}
 +		zero = min_t(unsigned int, iov->iov_len - offset, len);
 +		if (clear_user(iov->iov_base + offset, zero))
 +			return orig_len - len;
 +		offset = 0;
 +		len -= zero;
 +	}
 +
 +	return orig_len - len;
 +}
 +
 +
 +static sector_t to_sector(const struct buffer_head *bh,
 +		const struct inode *inode)
 +{
 +	sector_t sector = bh->b_blocknr << (inode->i_blkbits - 9);
 +
 +	return sector;
 +}
 +
 +static ssize_t dax_io(int rw, struct inode *inode, const struct iovec *iov,
 +			loff_t start, loff_t end, get_block_t get_block,
 +			struct buffer_head *bh)
 +{
 +	loff_t pos = start, max = start, bh_max = start;
 +	bool hole = false;
 +	struct block_device *bdev = NULL;
 +	int rc;
 +	long map_len = 0;
 +	struct blk_dax_ctl dax = {
 +		.addr = ERR_PTR(-EIO),
 +	};
 +	unsigned blkbits = inode->i_blkbits;
 +	sector_t file_blks = (i_size_read(inode) + (1 << blkbits) - 1)
 +								>> blkbits;
 +
 +	rw &= RW_MASK;
 +	if (rw == READ)
 +		end = min(end, i_size_read(inode));
 +
 +	while (pos < end) {
 +		size_t len;
 +		if (pos == max) {
 +			long page = pos >> PAGE_SHIFT;
 +			sector_t block = page << (PAGE_SHIFT - blkbits);
 +			unsigned first = pos - (block << blkbits);
 +			long size;
 +
 +			if (pos == bh_max) {
 +				bh->b_size = PAGE_ALIGN(end - pos);
 +				bh->b_state = 0;
 +				rc = get_block(inode, block, bh, rw == WRITE);
 +				if (rc)
 +					break;
 +				bh_max = pos - first + bh->b_size;
 +				bdev = bh->b_bdev;
 +				/*
 +				 * We allow uninitialized buffers for writes
 +				 * beyond EOF as those cannot race with faults
 +				 */
 +				WARN_ON_ONCE(
 +					(buffer_new(bh) && block < file_blks) ||
 +					(rw == WRITE && buffer_unwritten(bh)));
 +			} else {
 +				unsigned done = bh->b_size -
 +						(bh_max - (pos - first));
 +				bh->b_blocknr += done >> blkbits;
 +				bh->b_size -= done;
 +			}
 +
 +			hole = (rw == READ) && !buffer_written(bh);
 +			if (hole) {
 +				size = bh->b_size - first;
 +			} else {
 +				dax_unmap_atomic(bdev, &dax);
 +				dax.sector = to_sector(bh, inode);
 +				dax.size = bh->b_size;
 +				map_len = dax_map_atomic(bdev, &dax);
 +				if (map_len < 0) {
 +					rc = map_len;
 +					break;
 +				}
 +				dax.addr += first;
 +				size = map_len - first;
 +			}
 +			/*
 +			 * pos + size is one past the last offset for IO,
 +			 * so pos + size can overflow loff_t at extreme offsets.
 +			 * Cast to u64 to catch this and get the true minimum.
 +			 */
 +			max = min_t(u64, pos + size, end);
 +		}
 +
 +		if (rw == WRITE)
 +			len = memcpy_fromiovecend_partial_nocache(
 +				(void __force *)dax.addr, iov, pos - start,
 +				max - pos);
 +		else if (!hole)
 +			len = memcpy_toiovecend_partial(iov,
 +				(void __force *)dax.addr, pos - start,
 +				max - pos);
 +		else
 +			len = zero_toiovecend_partial(iov, pos - start,
 +				max - pos);
 +
 +		if (!len) {
 +			rc = -EFAULT;
 +			break;
 +		}
 +
 +		pos += len;
 +		if (!IS_ERR(dax.addr))
 +			dax.addr += len;
 +	}
 +
 +	dax_unmap_atomic(bdev, &dax);
 +
 +	return (pos == start) ? rc : pos - start;
 +}
 +
 +/**
 + * dax_do_io - Perform I/O to a DAX file
 + * @rw: READ to read or WRITE to write
 + * @iocb: The control block for this I/O
 + * @inode: The file which the I/O is directed at
 + * @iter: The addresses to do I/O from or to
 + * @pos: The file offset where the I/O starts
 + * @get_block: The filesystem method used to translate file offsets to blocks
 + * @end_io: A filesystem callback for I/O completion
 + * @flags: See below
 + *
 + * This function uses the same locking scheme as do_blockdev_direct_IO:
 + * If @flags has DIO_LOCKING set, we assume that the i_mutex is held by the
 + * caller for writes.  For reads, we take and release the i_mutex ourselves.
 + * If DIO_LOCKING is not set, the filesystem takes care of its own locking.
 + * As with do_blockdev_direct_IO(), we increment i_dio_count while the I/O
 + * is in progress.
 + */
 +ssize_t dax_do_io(int rw, struct kiocb *iocb, struct inode *inode,
 +		  const struct iovec *iov, loff_t pos, unsigned long nr_segs,
 +		  get_block_t get_block, dio_iodone_t end_io, int flags)
 +{
 +	struct buffer_head bh;
 +	ssize_t retval = -EINVAL;
 +	loff_t end = pos + iov_length(iov, nr_segs);
 +
 +	memset(&bh, 0, sizeof(bh));
 +	bh.b_bdev = inode->i_sb->s_bdev;
 +
 +	if ((flags & DIO_LOCKING) && (rw == READ))
 +		mutex_lock(&inode->i_mutex);
 +
 +	/* Protects against truncate */
 +	if (!(flags & DIO_SKIP_DIO_COUNT))
 +		inode_dio_begin(inode);
 +
 +	retval = dax_io(rw, inode, iov, pos, end, get_block, &bh);
 +
 +	if ((flags & DIO_LOCKING) && (rw == READ))
 +		mutex_unlock(&inode->i_mutex);
 +
 +	if ((retval > 0) && end_io)
 +		end_io(iocb, pos, retval, bh.b_private, retval, false);
 +
 +	if (!(flags & DIO_SKIP_DIO_COUNT))
 +		inode_dio_end(inode);
 +	return retval;
 +}
 +EXPORT_SYMBOL_GPL(dax_do_io);
 +
++=======
+ static int dax_is_pmd_entry(void *entry)
+ {
+ 	return (unsigned long)entry & RADIX_DAX_PMD;
+ }
+ 
+ static int dax_is_pte_entry(void *entry)
+ {
+ 	return !((unsigned long)entry & RADIX_DAX_PMD);
+ }
+ 
+ static int dax_is_zero_entry(void *entry)
+ {
+ 	return (unsigned long)entry & RADIX_DAX_HZP;
+ }
+ 
+ static int dax_is_empty_entry(void *entry)
+ {
+ 	return (unsigned long)entry & RADIX_DAX_EMPTY;
+ }
+ 
++>>>>>>> a41fe02b6bba (Revert "block: use DAX for partition table reads")
  /*
   * DAX radix tree locking
   */
diff --cc include/linux/dax.h
index 8937c7aed5cb,0d0d890f9186..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -33,22 -70,9 +33,28 @@@ void dax_wake_mapping_entry_waiter(stru
  		pgoff_t index, void *entry, bool wake_all);
  
  #ifdef CONFIG_FS_DAX
++<<<<<<< HEAD
 +struct page *read_dax_sector(struct block_device *bdev, sector_t n);
 +void dax_unlock_mapping_entry(struct address_space *mapping, pgoff_t index);
  int __dax_zero_page_range(struct block_device *bdev, sector_t sector,
  		unsigned int offset, unsigned int length);
  #else
 +static inline struct page *read_dax_sector(struct block_device *bdev,
 +		sector_t n)
 +{
 +	return ERR_PTR(-ENXIO);
 +}
 +/* Shouldn't ever be called when dax is disabled. */
 +static inline void dax_unlock_mapping_entry(struct address_space *mapping,
 +					    pgoff_t index)
 +{
 +	BUG();
 +}
++=======
++int __dax_zero_page_range(struct block_device *bdev, sector_t sector,
++		unsigned int offset, unsigned int length);
++#else
++>>>>>>> a41fe02b6bba (Revert "block: use DAX for partition table reads")
  static inline int __dax_zero_page_range(struct block_device *bdev,
  		sector_t sector, unsigned int offset, unsigned int length)
  {
* Unmerged path block/partition-generic.c
* Unmerged path fs/dax.c
* Unmerged path include/linux/dax.h
