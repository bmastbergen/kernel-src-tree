net/mlx5: Allocate FTE object without lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5: Allocate FTE object without lock (Kamal Heib) [1456687 1456694]
Rebuild_FUZZ: 95.00%
commit-author Maor Gottlieb <maorg@mellanox.com>
commit f5c2ff179f51101893e42e78683b23a487929d6c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f5c2ff17.failed

Allocation of new FTE is a massive operation, part of
it could be done without taking the flow group write lock.
Split the FTE allocation to two functions of actions which
need to be under lock and action which don't have.

	Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit f5c2ff179f51101893e42e78683b23a487929d6c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
index f60baa93f1dc,bc4bbb72fa86..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
@@@ -451,18 -520,59 +451,42 @@@ static void del_flow_group(struct fs_no
  	fs_get_obj(fg, node);
  	fs_get_obj(ft, fg->node.parent);
  	dev = get_dev(&ft->node);
 -	trace_mlx5_fs_del_fg(fg);
 -
 -	if (fg->node.active && mlx5_cmd_destroy_flow_group(dev, ft, fg->id))
 -		mlx5_core_warn(dev, "flow steering can't destroy fg %d of ft %d\n",
 -			       fg->id, ft->id);
 -}
  
 -static void del_sw_flow_group(struct fs_node *node)
 -{
 -	struct mlx5_flow_group *fg;
 -	struct mlx5_flow_table *ft;
 -	int err;
 -
 -	fs_get_obj(fg, node);
 -	fs_get_obj(ft, fg->node.parent);
 -
 -	rhashtable_destroy(&fg->ftes_hash);
 -	ida_destroy(&fg->fte_allocator);
  	if (ft->autogroup.active)
  		ft->autogroup.num_groups--;
 -	err = rhltable_remove(&ft->fgs_hash,
 -			      &fg->hash,
 -			      rhash_fg);
 -	WARN_ON(err);
 +
 +	if (mlx5_cmd_destroy_flow_group(dev, ft, fg->id))
 +		mlx5_core_warn(dev, "flow steering can't destroy fg %d of ft %d\n",
 +			       fg->id, ft->id);
  }
  
- static struct fs_fte *alloc_fte(struct mlx5_flow_act *flow_act,
- 				u32 *match_value,
- 				unsigned int index)
+ static int insert_fte(struct mlx5_flow_group *fg, struct fs_fte *fte)
+ {
+ 	int index;
+ 	int ret;
+ 
+ 	index = ida_simple_get(&fg->fte_allocator, 0, fg->max_ftes, GFP_KERNEL);
+ 	if (index < 0)
+ 		return index;
+ 
+ 	fte->index = index + fg->start_index;
+ 	ret = rhashtable_insert_fast(&fg->ftes_hash,
+ 				     &fte->hash,
+ 				     rhash_fte);
+ 	if (ret)
+ 		goto err_ida_remove;
+ 
+ 	tree_add_node(&fte->node, &fg->node);
+ 	list_add_tail(&fte->node.list, &fg->node.children);
+ 	return 0;
+ 
+ err_ida_remove:
+ 	ida_simple_remove(&fg->fte_allocator, index);
+ 	return ret;
+ }
+ 
+ static struct fs_fte *alloc_fte(u32 *match_value,
+ 				struct mlx5_flow_act *flow_act)
  {
  	struct fs_fte *fte;
  
@@@ -478,17 -587,25 +501,35 @@@
  	fte->encap_id = flow_act->encap_id;
  	fte->modify_id = flow_act->modify_id;
  
++<<<<<<< HEAD
 +	return fte;
 +}
 +
 +static struct mlx5_flow_group *alloc_flow_group(u32 *create_fg_in)
++=======
+ 	tree_init_node(&fte->node, del_hw_fte, del_sw_fte);
+ 
+ 	return fte;
+ }
+ 
+ static void dealloc_flow_group(struct mlx5_flow_group *fg)
+ {
+ 	rhashtable_destroy(&fg->ftes_hash);
+ 	kfree(fg);
+ }
+ 
+ static struct mlx5_flow_group *alloc_flow_group(u8 match_criteria_enable,
+ 						void *match_criteria,
+ 						int start_index,
+ 						int end_index)
++>>>>>>> f5c2ff179f51 (net/mlx5: Allocate FTE object without lock)
  {
  	struct mlx5_flow_group *fg;
 -	int ret;
 -
 +	void *match_criteria = MLX5_ADDR_OF(create_flow_group_in,
 +					    create_fg_in, match_criteria);
 +	u8 match_criteria_enable = MLX5_GET(create_flow_group_in,
 +					    create_fg_in,
 +					    match_criteria_enable);
  	fg = kzalloc(sizeof(*fg), GFP_KERNEL);
  	if (!fg)
  		return ERR_PTR(-ENOMEM);
@@@ -1349,6 -1481,199 +1390,202 @@@ static bool dest_is_valid(struct mlx5_f
  	return true;
  }
  
++<<<<<<< HEAD
++=======
+ struct match_list {
+ 	struct list_head	list;
+ 	struct mlx5_flow_group *g;
+ };
+ 
+ struct match_list_head {
+ 	struct list_head  list;
+ 	struct match_list first;
+ };
+ 
+ static void free_match_list(struct match_list_head *head)
+ {
+ 	if (!list_empty(&head->list)) {
+ 		struct match_list *iter, *match_tmp;
+ 
+ 		list_del(&head->first.list);
+ 		tree_put_node(&head->first.g->node);
+ 		list_for_each_entry_safe(iter, match_tmp, &head->list,
+ 					 list) {
+ 			tree_put_node(&iter->g->node);
+ 			list_del(&iter->list);
+ 			kfree(iter);
+ 		}
+ 	}
+ }
+ 
+ static int build_match_list(struct match_list_head *match_head,
+ 			    struct mlx5_flow_table *ft,
+ 			    struct mlx5_flow_spec *spec)
+ {
+ 	struct rhlist_head *tmp, *list;
+ 	struct mlx5_flow_group *g;
+ 	int err = 0;
+ 
+ 	rcu_read_lock();
+ 	INIT_LIST_HEAD(&match_head->list);
+ 	/* Collect all fgs which has a matching match_criteria */
+ 	list = rhltable_lookup(&ft->fgs_hash, spec, rhash_fg);
+ 	/* RCU is atomic, we can't execute FW commands here */
+ 	rhl_for_each_entry_rcu(g, tmp, list, hash) {
+ 		struct match_list *curr_match;
+ 
+ 		if (likely(list_empty(&match_head->list))) {
+ 			if (!tree_get_node(&g->node))
+ 				continue;
+ 			match_head->first.g = g;
+ 			list_add_tail(&match_head->first.list,
+ 				      &match_head->list);
+ 			continue;
+ 		}
+ 
+ 		curr_match = kmalloc(sizeof(*curr_match), GFP_ATOMIC);
+ 		if (!curr_match) {
+ 			free_match_list(match_head);
+ 			err = -ENOMEM;
+ 			goto out;
+ 		}
+ 		if (!tree_get_node(&g->node)) {
+ 			kfree(curr_match);
+ 			continue;
+ 		}
+ 		curr_match->g = g;
+ 		list_add_tail(&curr_match->list, &match_head->list);
+ 	}
+ out:
+ 	rcu_read_unlock();
+ 	return err;
+ }
+ 
+ static u64 matched_fgs_get_version(struct list_head *match_head)
+ {
+ 	struct match_list *iter;
+ 	u64 version = 0;
+ 
+ 	list_for_each_entry(iter, match_head, list)
+ 		version += (u64)atomic_read(&iter->g->node.version);
+ 	return version;
+ }
+ 
+ static struct mlx5_flow_handle *
+ try_add_to_existing_fg(struct mlx5_flow_table *ft,
+ 		       struct list_head *match_head,
+ 		       struct mlx5_flow_spec *spec,
+ 		       struct mlx5_flow_act *flow_act,
+ 		       struct mlx5_flow_destination *dest,
+ 		       int dest_num,
+ 		       int ft_version)
+ {
+ 	struct mlx5_flow_group *g;
+ 	struct mlx5_flow_handle *rule;
+ 	struct match_list *iter;
+ 	bool take_write = false;
+ 	struct fs_fte *fte;
+ 	u64  version;
+ 	int err;
+ 
+ 	fte = alloc_fte(spec->match_value, flow_act);
+ 	if (IS_ERR(fte))
+ 		return  ERR_PTR(-ENOMEM);
+ 
+ 	list_for_each_entry(iter, match_head, list) {
+ 		nested_down_read_ref_node(&iter->g->node, FS_LOCK_PARENT);
+ 		ida_pre_get(&iter->g->fte_allocator, GFP_KERNEL);
+ 	}
+ 
+ search_again_locked:
+ 	version = matched_fgs_get_version(match_head);
+ 	/* Try to find a fg that already contains a matching fte */
+ 	list_for_each_entry(iter, match_head, list) {
+ 		struct fs_fte *fte_tmp;
+ 
+ 		g = iter->g;
+ 		fte_tmp = rhashtable_lookup_fast(&g->ftes_hash, spec->match_value,
+ 						 rhash_fte);
+ 		if (!fte_tmp || !tree_get_node(&fte_tmp->node))
+ 			continue;
+ 
+ 		nested_down_write_ref_node(&fte_tmp->node, FS_LOCK_CHILD);
+ 		if (!take_write) {
+ 			list_for_each_entry(iter, match_head, list)
+ 				up_read_ref_node(&iter->g->node);
+ 		} else {
+ 			list_for_each_entry(iter, match_head, list)
+ 				up_write_ref_node(&iter->g->node);
+ 		}
+ 
+ 		rule = add_rule_fg(g, spec->match_value,
+ 				   flow_act, dest, dest_num, fte_tmp);
+ 		up_write_ref_node(&fte_tmp->node);
+ 		tree_put_node(&fte_tmp->node);
+ 		kfree(fte);
+ 		return rule;
+ 	}
+ 
+ 	/* No group with matching fte found. Try to add a new fte to any
+ 	 * matching fg.
+ 	 */
+ 
+ 	if (!take_write) {
+ 		list_for_each_entry(iter, match_head, list)
+ 			up_read_ref_node(&iter->g->node);
+ 		list_for_each_entry(iter, match_head, list)
+ 			nested_down_write_ref_node(&iter->g->node,
+ 						   FS_LOCK_PARENT);
+ 		take_write = true;
+ 	}
+ 
+ 	/* Check the ft version, for case that new flow group
+ 	 * was added while the fgs weren't locked
+ 	 */
+ 	if (atomic_read(&ft->node.version) != ft_version) {
+ 		rule = ERR_PTR(-EAGAIN);
+ 		goto out;
+ 	}
+ 
+ 	/* Check the fgs version, for case the new FTE with the
+ 	 * same values was added while the fgs weren't locked
+ 	 */
+ 	if (version != matched_fgs_get_version(match_head))
+ 		goto search_again_locked;
+ 
+ 	list_for_each_entry(iter, match_head, list) {
+ 		g = iter->g;
+ 
+ 		if (!g->node.active)
+ 			continue;
+ 		err = insert_fte(g, fte);
+ 		if (err) {
+ 			if (err == -ENOSPC)
+ 				continue;
+ 			list_for_each_entry(iter, match_head, list)
+ 				up_write_ref_node(&iter->g->node);
+ 			kfree(fte);
+ 			return ERR_PTR(err);
+ 		}
+ 
+ 		nested_down_write_ref_node(&fte->node, FS_LOCK_CHILD);
+ 		list_for_each_entry(iter, match_head, list)
+ 			up_write_ref_node(&iter->g->node);
+ 		rule = add_rule_fg(g, spec->match_value,
+ 				   flow_act, dest, dest_num, fte);
+ 		up_write_ref_node(&fte->node);
+ 		tree_put_node(&fte->node);
+ 		return rule;
+ 	}
+ 	rule = ERR_PTR(-ENOENT);
+ out:
+ 	list_for_each_entry(iter, match_head, list)
+ 		up_write_ref_node(&iter->g->node);
+ 	kfree(fte);
+ 	return rule;
+ }
+ 
++>>>>>>> f5c2ff179f51 (net/mlx5: Allocate FTE object without lock)
  static struct mlx5_flow_handle *
  _mlx5_add_flow_rules(struct mlx5_flow_table *ft,
  		     struct mlx5_flow_spec *spec,
@@@ -1365,39 -1698,73 +1602,69 @@@
  		if (!dest_is_valid(&dest[i], flow_act->action, ft))
  			return ERR_PTR(-EINVAL);
  	}
 -	nested_down_read_ref_node(&ft->node, FS_LOCK_GRANDPARENT);
 -search_again_locked:
 -	version = atomic_read(&ft->node.version);
 -
 -	/* Collect all fgs which has a matching match_criteria */
 -	err = build_match_list(&match_head, ft, spec);
 -	if (err)
 -		return ERR_PTR(err);
 -
 -	if (!take_write)
 -		up_read_ref_node(&ft->node);
 -
 -	rule = try_add_to_existing_fg(ft, &match_head.list, spec, flow_act, dest,
 -				      dest_num, version);
 -	free_match_list(&match_head);
 -	if (!IS_ERR(rule) ||
 -	    (PTR_ERR(rule) != -ENOENT && PTR_ERR(rule) != -EAGAIN))
 -		return rule;
  
 -	if (!take_write) {
 -		nested_down_write_ref_node(&ft->node, FS_LOCK_GRANDPARENT);
 -		take_write = true;
 -	}
 -
 -	if (PTR_ERR(rule) == -EAGAIN ||
 -	    version != atomic_read(&ft->node.version))
 -		goto search_again_locked;
 +	nested_lock_ref_node(&ft->node, FS_MUTEX_GRANDPARENT);
 +	fs_for_each_fg(g, ft)
 +		if (compare_match_criteria(g->mask.match_criteria_enable,
 +					   spec->match_criteria_enable,
 +					   g->mask.match_criteria,
 +					   spec->match_criteria)) {
 +			rule = add_rule_fg(g, spec->match_value,
 +					   flow_act, dest, dest_num);
 +			if (!IS_ERR(rule) || PTR_ERR(rule) != -ENOSPC)
 +				goto unlock;
 +		}
  
 -	g = alloc_auto_flow_group(ft, spec);
 +	g = create_autogroup(ft, spec->match_criteria_enable,
 +			     spec->match_criteria);
  	if (IS_ERR(g)) {
  		rule = (void *)g;
 -		up_write_ref_node(&ft->node);
 +		goto unlock;
 +	}
 +
 +	rule = add_rule_fg(g, spec->match_value, flow_act, dest, dest_num);
 +	if (IS_ERR(rule)) {
 +		/* Remove assumes refcount > 0 and autogroup creates a group
 +		 * with a refcount = 0.
 +		 */
 +		unlock_ref_node(&ft->node);
 +		tree_get_node(&g->node);
 +		tree_remove_node(&g->node);
  		return rule;
  	}
++<<<<<<< HEAD
 +unlock:
 +	unlock_ref_node(&ft->node);
++=======
+ 
+ 	nested_down_write_ref_node(&g->node, FS_LOCK_PARENT);
+ 	up_write_ref_node(&ft->node);
+ 
+ 	err = create_auto_flow_group(ft, g);
+ 	if (err)
+ 		goto err_release_fg;
+ 
+ 	fte = alloc_fte(spec->match_value, flow_act);
+ 	if (IS_ERR(fte)) {
+ 		err = PTR_ERR(fte);
+ 		goto err_release_fg;
+ 	}
+ 
+ 	err = insert_fte(g, fte);
+ 	if (err) {
+ 		kfree(fte);
+ 		goto err_release_fg;
+ 	}
+ 
+ 	nested_down_write_ref_node(&fte->node, FS_LOCK_CHILD);
+ 	up_write_ref_node(&g->node);
+ 	rule = add_rule_fg(g, spec->match_value, flow_act, dest,
+ 			   dest_num, fte);
+ 	up_write_ref_node(&fte->node);
+ 	tree_put_node(&fte->node);
+ 	tree_put_node(&g->node);
++>>>>>>> f5c2ff179f51 (net/mlx5: Allocate FTE object without lock)
  	return rule;
 -
 -err_release_fg:
 -	up_write_ref_node(&g->node);
 -	tree_put_node(&g->node);
 -	return ERR_PTR(err);
  }
  
  static bool fwd_next_prio_supported(struct mlx5_flow_table *ft)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
