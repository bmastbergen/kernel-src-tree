scsi: lpfc: Fix MRQ > 1 context list handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] lpfc: Fix MRQ > 1 context list handling (Dick Kennedy) [1385844 1461977 1387768]
Rebuild_FUZZ: 92.86%
commit-author Dick Kennedy <dick.kennedy@broadcom.com>
commit 66d7ce93a0f5b991d6bf068f797dec49eb8e5c57
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/66d7ce93.failed

Various oops including cpu LOCKUPs were seen.

For asynchronously received ius where the driver must assign exchange
resources, the resources were on a single get (free) list and put list
(finished, waiting to be put on get list). As all cpus are sharing the
lists, an interrupt for a receive frame may have to wait for all the
other cpus to place their done work onto the put list before it can
acquire the lock to pull from the list.

Fix by breaking the resource lists into per-cpu lists or at least more
than 1 list with cpu's sharing the lists). A cpu would allocate from the
free list for its own cpu, and put its done work on the its own put list
- avoiding the contention. As cpu load may vary, when empty, a cpu may
grab from another cpu, thereby changing resource distribution.  But
searching for a resource only occurs on 1 or a few cpus until a single
resource can be allocated. if the condition reoccurs, it starts looking
at a different cpu.

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 66d7ce93a0f5b991d6bf068f797dec49eb8e5c57)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_attr.c
#	drivers/scsi/lpfc/lpfc_crtn.h
#	drivers/scsi/lpfc/lpfc_debugfs.c
#	drivers/scsi/lpfc/lpfc_init.c
#	drivers/scsi/lpfc/lpfc_nvmet.c
#	drivers/scsi/lpfc/lpfc_nvmet.h
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/lpfc/lpfc_sli4.h
diff --cc drivers/scsi/lpfc/lpfc_attr.c
index e030a290e43b,d3d01ff44423..000000000000
--- a/drivers/scsi/lpfc/lpfc_attr.c
+++ b/drivers/scsi/lpfc/lpfc_attr.c
@@@ -130,6 -140,230 +130,233 @@@ lpfc_enable_fip_show(struct device *dev
  }
  
  static ssize_t
++<<<<<<< HEAD
++=======
+ lpfc_nvme_info_show(struct device *dev, struct device_attribute *attr,
+ 		    char *buf)
+ {
+ 	struct Scsi_Host *shost = class_to_shost(dev);
+ 	struct lpfc_vport *vport = shost_priv(shost);
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	struct nvme_fc_local_port *localport;
+ 	struct lpfc_nodelist *ndlp;
+ 	struct nvme_fc_remote_port *nrport;
+ 	uint64_t data1, data2, data3, tot;
+ 	char *statep;
+ 	int len = 0;
+ 
+ 	if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME)) {
+ 		len += snprintf(buf, PAGE_SIZE, "NVME Disabled\n");
+ 		return len;
+ 	}
+ 	if (phba->nvmet_support) {
+ 		if (!phba->targetport) {
+ 			len = snprintf(buf, PAGE_SIZE,
+ 					"NVME Target: x%llx is not allocated\n",
+ 					wwn_to_u64(vport->fc_portname.u.wwn));
+ 			return len;
+ 		}
+ 		/* Port state is only one of two values for now. */
+ 		if (phba->targetport->port_id)
+ 			statep = "REGISTERED";
+ 		else
+ 			statep = "INIT";
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"NVME Target Enabled  State %s\n",
+ 				statep);
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"%s%d WWPN x%llx WWNN x%llx DID x%06x\n",
+ 				"NVME Target: lpfc",
+ 				phba->brd_no,
+ 				wwn_to_u64(vport->fc_portname.u.wwn),
+ 				wwn_to_u64(vport->fc_nodename.u.wwn),
+ 				phba->targetport->port_id);
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"\nNVME Target: Statistics\n");
+ 		tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"LS: Rcv %08x Drop %08x Abort %08x\n",
+ 				atomic_read(&tgtp->rcv_ls_req_in),
+ 				atomic_read(&tgtp->rcv_ls_req_drop),
+ 				atomic_read(&tgtp->xmt_ls_abort));
+ 		if (atomic_read(&tgtp->rcv_ls_req_in) !=
+ 		    atomic_read(&tgtp->rcv_ls_req_out)) {
+ 			len += snprintf(buf+len, PAGE_SIZE-len,
+ 					"Rcv LS: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_ls_req_in),
+ 					atomic_read(&tgtp->rcv_ls_req_out));
+ 		}
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"LS: Xmt %08x Drop %08x Cmpl %08x Err %08x\n",
+ 				atomic_read(&tgtp->xmt_ls_rsp),
+ 				atomic_read(&tgtp->xmt_ls_drop),
+ 				atomic_read(&tgtp->xmt_ls_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_ls_rsp_error));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP: Rcv %08x Release %08x Drop %08x\n",
+ 				atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 				atomic_read(&tgtp->xmt_fcp_release),
+ 				atomic_read(&tgtp->rcv_fcp_cmd_drop));
+ 
+ 		if (atomic_read(&tgtp->rcv_fcp_cmd_in) !=
+ 		    atomic_read(&tgtp->rcv_fcp_cmd_out)) {
+ 			len += snprintf(buf+len, PAGE_SIZE-len,
+ 					"Rcv FCP: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 					atomic_read(&tgtp->rcv_fcp_cmd_out));
+ 		}
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP Rsp: RD %08x rsp %08x WR %08x rsp %08x "
+ 				"drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_read),
+ 				atomic_read(&tgtp->xmt_fcp_read_rsp),
+ 				atomic_read(&tgtp->xmt_fcp_write),
+ 				atomic_read(&tgtp->xmt_fcp_rsp),
+ 				atomic_read(&tgtp->xmt_fcp_drop));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP Rsp Cmpl: %08x err %08x drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_error),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_drop));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"ABORT: Xmt %08x Cmpl %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_abort),
+ 				atomic_read(&tgtp->xmt_fcp_abort_cmpl));
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"ABORT: Sol %08x  Usol %08x Err %08x Cmpl %08x",
+ 				atomic_read(&tgtp->xmt_abort_sol),
+ 				atomic_read(&tgtp->xmt_abort_unsol),
+ 				atomic_read(&tgtp->xmt_abort_rsp),
+ 				atomic_read(&tgtp->xmt_abort_rsp_error));
+ 
+ 		/* Calculate outstanding IOs */
+ 		tot = atomic_read(&tgtp->rcv_fcp_cmd_drop);
+ 		tot += atomic_read(&tgtp->xmt_fcp_release);
+ 		tot = atomic_read(&tgtp->rcv_fcp_cmd_in) - tot;
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"IO_CTX: %08x  WAIT: cur %08x tot %08x\n"
+ 				"CTX Outstanding %08llx\n",
+ 				phba->sli4_hba.nvmet_xri_cnt,
+ 				phba->sli4_hba.nvmet_io_wait_cnt,
+ 				phba->sli4_hba.nvmet_io_wait_total,
+ 				tot);
+ 
+ 		len +=  snprintf(buf+len, PAGE_SIZE-len, "\n");
+ 		return len;
+ 	}
+ 
+ 	localport = vport->localport;
+ 	if (!localport) {
+ 		len = snprintf(buf, PAGE_SIZE,
+ 				"NVME Initiator x%llx is not allocated\n",
+ 				wwn_to_u64(vport->fc_portname.u.wwn));
+ 		return len;
+ 	}
+ 	len = snprintf(buf, PAGE_SIZE, "NVME Initiator Enabled\n");
+ 
+ 	spin_lock_irq(shost->host_lock);
+ 
+ 	/* Port state is only one of two values for now. */
+ 	if (localport->port_id)
+ 		statep = "ONLINE";
+ 	else
+ 		statep = "UNKNOWN ";
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"%s%d WWPN x%llx WWNN x%llx DID x%06x %s\n",
+ 			"NVME LPORT lpfc",
+ 			phba->brd_no,
+ 			wwn_to_u64(vport->fc_portname.u.wwn),
+ 			wwn_to_u64(vport->fc_nodename.u.wwn),
+ 			localport->port_id, statep);
+ 
+ 	list_for_each_entry(ndlp, &vport->fc_nodes, nlp_listp) {
+ 		if (!ndlp->nrport)
+ 			continue;
+ 
+ 		/* local short-hand pointer. */
+ 		nrport = ndlp->nrport->remoteport;
+ 
+ 		/* Port state is only one of two values for now. */
+ 		switch (nrport->port_state) {
+ 		case FC_OBJSTATE_ONLINE:
+ 			statep = "ONLINE";
+ 			break;
+ 		case FC_OBJSTATE_UNKNOWN:
+ 			statep = "UNKNOWN ";
+ 			break;
+ 		default:
+ 			statep = "UNSUPPORTED";
+ 			break;
+ 		}
+ 
+ 		/* Tab in to show lport ownership. */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"NVME RPORT       ");
+ 		if (phba->brd_no >= 10)
+ 			len += snprintf(buf + len, PAGE_SIZE - len, " ");
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "WWPN x%llx ",
+ 				nrport->port_name);
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "WWNN x%llx ",
+ 				nrport->node_name);
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "DID x%06x ",
+ 				nrport->port_id);
+ 
+ 		/* An NVME rport can have multiple roles. */
+ 		if (nrport->port_role & FC_PORT_ROLE_NVME_INITIATOR)
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "INITIATOR ");
+ 		if (nrport->port_role & FC_PORT_ROLE_NVME_TARGET)
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "TARGET ");
+ 		if (nrport->port_role & FC_PORT_ROLE_NVME_DISCOVERY)
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "DISCSRVC ");
+ 		if (nrport->port_role & ~(FC_PORT_ROLE_NVME_INITIATOR |
+ 					  FC_PORT_ROLE_NVME_TARGET |
+ 					  FC_PORT_ROLE_NVME_DISCOVERY))
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "UNKNOWN ROLE x%x",
+ 					 nrport->port_role);
+ 
+ 		len +=  snprintf(buf + len, PAGE_SIZE - len, "%s  ", statep);
+ 		/* Terminate the string. */
+ 		len +=  snprintf(buf + len, PAGE_SIZE - len, "\n");
+ 	}
+ 	spin_unlock_irq(shost->host_lock);
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE - len, "\nNVME Statistics\n");
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"LS: Xmt %016x Cmpl %016x\n",
+ 			atomic_read(&phba->fc4NvmeLsRequests),
+ 			atomic_read(&phba->fc4NvmeLsCmpls));
+ 
+ 	tot = atomic_read(&phba->fc4NvmeIoCmpls);
+ 	data1 = atomic_read(&phba->fc4NvmeInputRequests);
+ 	data2 = atomic_read(&phba->fc4NvmeOutputRequests);
+ 	data3 = atomic_read(&phba->fc4NvmeControlRequests);
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"FCP: Rd %016llx Wr %016llx IO %016llx\n",
+ 			data1, data2, data3);
+ 
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"    Cmpl %016llx Outstanding %016llx\n",
+ 			tot, (data1 + data2 + data3) - tot);
+ 	return len;
+ }
+ 
+ static ssize_t
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  lpfc_bg_info_show(struct device *dev, struct device_attribute *attr,
  		  char *buf)
  {
diff --cc drivers/scsi/lpfc/lpfc_crtn.h
index 5c660beb66e2,7e300734b345..000000000000
--- a/drivers/scsi/lpfc/lpfc_crtn.h
+++ b/drivers/scsi/lpfc/lpfc_crtn.h
@@@ -499,5 -542,27 +499,30 @@@ bool lpfc_find_next_oas_lun(struct lpfc
  			    uint32_t *, uint32_t *);
  int lpfc_sli4_dump_page_a0(struct lpfc_hba *phba, struct lpfcMboxq *mbox);
  void lpfc_mbx_cmpl_rdp_page_a0(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmb);
++<<<<<<< HEAD
++=======
+ 
+ /* NVME interfaces. */
+ void lpfc_nvme_unregister_port(struct lpfc_vport *vport,
+ 			struct lpfc_nodelist *ndlp);
+ int lpfc_nvme_register_port(struct lpfc_vport *vport,
+ 			struct lpfc_nodelist *ndlp);
+ int lpfc_nvme_create_localport(struct lpfc_vport *vport);
+ void lpfc_nvme_destroy_localport(struct lpfc_vport *vport);
+ void lpfc_nvme_update_localport(struct lpfc_vport *vport);
+ int lpfc_nvmet_create_targetport(struct lpfc_hba *phba);
+ int lpfc_nvmet_update_targetport(struct lpfc_hba *phba);
+ void lpfc_nvmet_destroy_targetport(struct lpfc_hba *phba);
+ void lpfc_nvmet_unsol_ls_event(struct lpfc_hba *phba,
+ 			struct lpfc_sli_ring *pring, struct lpfc_iocbq *piocb);
+ void lpfc_nvmet_unsol_fcp_event(struct lpfc_hba *phba, uint32_t idx,
+ 				struct rqb_dmabuf *nvmebuf, uint64_t isr_ts);
+ void lpfc_nvme_mod_param_dep(struct lpfc_hba *phba);
+ void lpfc_nvme_abort_fcreq_cmpl(struct lpfc_hba *phba,
+ 				struct lpfc_iocbq *cmdiocb,
+ 				struct lpfc_wcqe_complete *abts_cmpl);
+ extern int lpfc_enable_nvmet_cnt;
+ extern unsigned long long lpfc_enable_nvmet[];
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  extern int lpfc_no_hba_reset_cnt;
  extern unsigned long lpfc_no_hba_reset[];
diff --cc drivers/scsi/lpfc/lpfc_debugfs.c
index 389b2bc6c406,c292264aa687..000000000000
--- a/drivers/scsi/lpfc/lpfc_debugfs.c
+++ b/drivers/scsi/lpfc/lpfc_debugfs.c
@@@ -611,8 -635,639 +611,549 @@@ lpfc_debugfs_nodelist_data(struct lpfc_
  		len +=  snprintf(buf+len, size-len, "\n");
  	}
  	spin_unlock_irq(shost->host_lock);
 -
 -	if (phba->nvmet_support && phba->targetport && (vport == phba->pport)) {
 -		tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
 -		len += snprintf(buf + len, size - len,
 -				"\nNVME Targetport Entry ...\n");
 -
 -		/* Port state is only one of two values for now. */
 -		if (phba->targetport->port_id)
 -			statep = "REGISTERED";
 -		else
 -			statep = "INIT";
 -		len += snprintf(buf + len, size - len,
 -				"TGT WWNN x%llx WWPN x%llx State %s\n",
 -				wwn_to_u64(vport->fc_nodename.u.wwn),
 -				wwn_to_u64(vport->fc_portname.u.wwn),
 -				statep);
 -		len += snprintf(buf + len, size - len,
 -				"    Targetport DID x%06x\n",
 -				phba->targetport->port_id);
 -		goto out_exit;
 -	}
 -
 -	len += snprintf(buf + len, size - len,
 -				"\nNVME Lport/Rport Entries ...\n");
 -
 -	localport = vport->localport;
 -	if (!localport)
 -		goto out_exit;
 -
 -	spin_lock_irq(shost->host_lock);
 -
 -	/* Port state is only one of two values for now. */
 -	if (localport->port_id)
 -		statep = "ONLINE";
 -	else
 -		statep = "UNKNOWN ";
 -
 -	len += snprintf(buf + len, size - len,
 -			"Lport DID x%06x PortState %s\n",
 -			localport->port_id, statep);
 -
 -	len += snprintf(buf + len, size - len, "\tRport List:\n");
 -	list_for_each_entry(ndlp, &vport->fc_nodes, nlp_listp) {
 -		/* local short-hand pointer. */
 -		if (!ndlp->nrport)
 -			continue;
 -
 -		nrport = ndlp->nrport->remoteport;
 -
 -		/* Port state is only one of two values for now. */
 -		switch (nrport->port_state) {
 -		case FC_OBJSTATE_ONLINE:
 -			statep = "ONLINE";
 -			break;
 -		case FC_OBJSTATE_UNKNOWN:
 -			statep = "UNKNOWN ";
 -			break;
 -		default:
 -			statep = "UNSUPPORTED";
 -			break;
 -		}
 -
 -		/* Tab in to show lport ownership. */
 -		len += snprintf(buf + len, size - len,
 -				"\t%s Port ID:x%06x ",
 -				statep, nrport->port_id);
 -		len += snprintf(buf + len, size - len, "WWPN x%llx ",
 -				nrport->port_name);
 -		len += snprintf(buf + len, size - len, "WWNN x%llx ",
 -				nrport->node_name);
 -
 -		/* An NVME rport can have multiple roles. */
 -		if (nrport->port_role & FC_PORT_ROLE_NVME_INITIATOR)
 -			len +=  snprintf(buf + len, size - len,
 -					 "INITIATOR ");
 -		if (nrport->port_role & FC_PORT_ROLE_NVME_TARGET)
 -			len +=  snprintf(buf + len, size - len,
 -					 "TARGET ");
 -		if (nrport->port_role & FC_PORT_ROLE_NVME_DISCOVERY)
 -			len +=  snprintf(buf + len, size - len,
 -					 "DISCSRVC ");
 -		if (nrport->port_role & ~(FC_PORT_ROLE_NVME_INITIATOR |
 -					  FC_PORT_ROLE_NVME_TARGET |
 -					  FC_PORT_ROLE_NVME_DISCOVERY))
 -			len +=  snprintf(buf + len, size - len,
 -					 "UNKNOWN ROLE x%x",
 -					 nrport->port_role);
 -		/* Terminate the string. */
 -		len +=  snprintf(buf + len, size - len, "\n");
 -	}
 -
 -	spin_unlock_irq(shost->host_lock);
 - out_exit:
  	return len;
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * lpfc_debugfs_nvmestat_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmestat_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	struct lpfc_nvmet_rcv_ctx *ctxp, *next_ctxp;
+ 	uint64_t tot, data1, data2, data3;
+ 	int len = 0;
+ 	int cnt;
+ 
+ 	if (phba->nvmet_support) {
+ 		if (!phba->targetport)
+ 			return len;
+ 		tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
+ 		len += snprintf(buf + len, size - len,
+ 				"\nNVME Targetport Statistics\n");
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"LS: Rcv %08x Drop %08x Abort %08x\n",
+ 				atomic_read(&tgtp->rcv_ls_req_in),
+ 				atomic_read(&tgtp->rcv_ls_req_drop),
+ 				atomic_read(&tgtp->xmt_ls_abort));
+ 		if (atomic_read(&tgtp->rcv_ls_req_in) !=
+ 		    atomic_read(&tgtp->rcv_ls_req_out)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Rcv LS: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_ls_req_in),
+ 					atomic_read(&tgtp->rcv_ls_req_out));
+ 		}
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"LS: Xmt %08x Drop %08x Cmpl %08x Err %08x\n",
+ 				atomic_read(&tgtp->xmt_ls_rsp),
+ 				atomic_read(&tgtp->xmt_ls_drop),
+ 				atomic_read(&tgtp->xmt_ls_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_ls_rsp_error));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP: Rcv %08x Drop %08x\n",
+ 				atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 				atomic_read(&tgtp->rcv_fcp_cmd_drop));
+ 
+ 		if (atomic_read(&tgtp->rcv_fcp_cmd_in) !=
+ 		    atomic_read(&tgtp->rcv_fcp_cmd_out)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Rcv FCP: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 					atomic_read(&tgtp->rcv_fcp_cmd_out));
+ 		}
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP Rsp: read %08x readrsp %08x "
+ 				"write %08x rsp %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_read),
+ 				atomic_read(&tgtp->xmt_fcp_read_rsp),
+ 				atomic_read(&tgtp->xmt_fcp_write),
+ 				atomic_read(&tgtp->xmt_fcp_rsp));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP Rsp Cmpl: %08x err %08x drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_error),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_drop));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"ABORT: Xmt %08x Cmpl %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_abort),
+ 				atomic_read(&tgtp->xmt_fcp_abort_cmpl));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"ABORT: Sol %08x  Usol %08x Err %08x Cmpl %08x",
+ 				atomic_read(&tgtp->xmt_abort_sol),
+ 				atomic_read(&tgtp->xmt_abort_unsol),
+ 				atomic_read(&tgtp->xmt_abort_rsp),
+ 				atomic_read(&tgtp->xmt_abort_rsp_error));
+ 
+ 		len +=  snprintf(buf + len, size - len, "\n");
+ 
+ 		cnt = 0;
+ 		spin_lock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		list_for_each_entry_safe(ctxp, next_ctxp,
+ 				&phba->sli4_hba.lpfc_abts_nvmet_ctx_list,
+ 				list) {
+ 			cnt++;
+ 		}
+ 		spin_unlock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		if (cnt) {
+ 			len += snprintf(buf + len, size - len,
+ 					"ABORT: %d ctx entries\n", cnt);
+ 			spin_lock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 			list_for_each_entry_safe(ctxp, next_ctxp,
+ 				    &phba->sli4_hba.lpfc_abts_nvmet_ctx_list,
+ 				    list) {
+ 				if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ))
+ 					break;
+ 				len += snprintf(buf + len, size - len,
+ 						"Entry: oxid %x state %x "
+ 						"flag %x\n",
+ 						ctxp->oxid, ctxp->state,
+ 						ctxp->flag);
+ 			}
+ 			spin_unlock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		}
+ 
+ 		/* Calculate outstanding IOs */
+ 		tot = atomic_read(&tgtp->rcv_fcp_cmd_drop);
+ 		tot += atomic_read(&tgtp->xmt_fcp_release);
+ 		tot = atomic_read(&tgtp->rcv_fcp_cmd_in) - tot;
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"IO_CTX: %08x  WAIT: cur %08x tot %08x\n"
+ 				"CTX Outstanding %08llx\n",
+ 				phba->sli4_hba.nvmet_xri_cnt,
+ 				phba->sli4_hba.nvmet_io_wait_cnt,
+ 				phba->sli4_hba.nvmet_io_wait_total,
+ 				tot);
+ 	} else {
+ 		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME))
+ 			return len;
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"\nNVME Lport Statistics\n");
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"LS: Xmt %016x Cmpl %016x\n",
+ 				atomic_read(&phba->fc4NvmeLsRequests),
+ 				atomic_read(&phba->fc4NvmeLsCmpls));
+ 
+ 		tot = atomic_read(&phba->fc4NvmeIoCmpls);
+ 		data1 = atomic_read(&phba->fc4NvmeInputRequests);
+ 		data2 = atomic_read(&phba->fc4NvmeOutputRequests);
+ 		data3 = atomic_read(&phba->fc4NvmeControlRequests);
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP: Rd %016llx Wr %016llx IO %016llx\n",
+ 				data1, data2, data3);
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"    Cmpl %016llx Outstanding %016llx\n",
+ 				tot, (data1 + data2 + data3) - tot);
+ 	}
+ 
+ 	return len;
+ }
+ 
+ 
+ /**
+  * lpfc_debugfs_nvmektime_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmektime_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	int len = 0;
+ 
+ 	if (phba->nvmet_support == 0) {
+ 		/* NVME Initiator */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"ktime %s: Total Samples: %lld\n",
+ 				(phba->ktime_on ?  "Enabled" : "Disabled"),
+ 				phba->ktime_data_samples);
+ 		if (phba->ktime_data_samples == 0)
+ 			return len;
+ 
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 1: Last NVME Cmd cmpl "
+ 			"done -to- Start of next NVME cnd (in driver)\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg1_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg1_min,
+ 			phba->ktime_seg1_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 2: Driver start of NVME cmd "
+ 			"-to- Firmware WQ doorbell\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg2_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg2_min,
+ 			phba->ktime_seg2_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 3: Firmware WQ doorbell -to- "
+ 			"MSI-X ISR cmpl\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg3_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg3_min,
+ 			phba->ktime_seg3_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 4: MSI-X ISR cmpl -to- "
+ 			"NVME cmpl done\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg4_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg4_min,
+ 			phba->ktime_seg4_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Total IO avg time: %08lld\n",
+ 			div_u64(phba->ktime_seg1_total +
+ 			phba->ktime_seg2_total  +
+ 			phba->ktime_seg3_total +
+ 			phba->ktime_seg4_total,
+ 			phba->ktime_data_samples));
+ 		return len;
+ 	}
+ 
+ 	/* NVME Target */
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"ktime %s: Total Samples: %lld %lld\n",
+ 			(phba->ktime_on ? "Enabled" : "Disabled"),
+ 			phba->ktime_data_samples,
+ 			phba->ktime_status_samples);
+ 	if (phba->ktime_data_samples == 0)
+ 		return len;
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 1: MSI-X ISR Rcv cmd -to- "
+ 			"cmd pass to NVME Layer\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg1_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg1_min,
+ 			phba->ktime_seg1_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 2: cmd pass to NVME Layer- "
+ 			"-to- Driver rcv cmd OP (action)\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg2_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg2_min,
+ 			phba->ktime_seg2_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 3: Driver rcv cmd OP -to- "
+ 			"Firmware WQ doorbell: cmd\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg3_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg3_min,
+ 			phba->ktime_seg3_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 4: Firmware WQ doorbell: cmd "
+ 			"-to- MSI-X ISR for cmd cmpl\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg4_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg4_min,
+ 			phba->ktime_seg4_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 5: MSI-X ISR for cmd cmpl "
+ 			"-to- NVME layer passed cmd done\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg5_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg5_min,
+ 			phba->ktime_seg5_max);
+ 
+ 	if (phba->ktime_status_samples == 0) {
+ 		len += snprintf(buf + len, PAGE_SIZE-len,
+ 				"Total: cmd received by MSI-X ISR "
+ 				"-to- cmd completed on wire\n");
+ 		len += snprintf(buf + len, PAGE_SIZE-len,
+ 				"avg:%08lld min:%08lld "
+ 				"max %08lld\n",
+ 				div_u64(phba->ktime_seg10_total,
+ 					phba->ktime_data_samples),
+ 				phba->ktime_seg10_min,
+ 				phba->ktime_seg10_max);
+ 		return len;
+ 	}
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 6: NVME layer passed cmd done "
+ 			"-to- Driver rcv rsp status OP\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg6_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg6_min,
+ 			phba->ktime_seg6_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 7: Driver rcv rsp status OP "
+ 			"-to- Firmware WQ doorbell: status\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg7_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg7_min,
+ 			phba->ktime_seg7_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 8: Firmware WQ doorbell: status"
+ 			" -to- MSI-X ISR for status cmpl\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg8_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg8_min,
+ 			phba->ktime_seg8_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 9: MSI-X ISR for status cmpl  "
+ 			"-to- NVME layer passed status done\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg9_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg9_min,
+ 			phba->ktime_seg9_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Total: cmd received by MSI-X ISR -to- "
+ 			"cmd completed on wire\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg10_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg10_min,
+ 			phba->ktime_seg10_max);
+ 	return len;
+ }
+ 
+ /**
+  * lpfc_debugfs_nvmeio_trc_data - Dump NVME IO trace list to a buffer
+  * @phba: The phba to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME IO trace associated with @phba
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmeio_trc_data(struct lpfc_hba *phba, char *buf, int size)
+ {
+ 	struct lpfc_debugfs_nvmeio_trc *dtp;
+ 	int i, state, index, skip;
+ 	int len = 0;
+ 
+ 	state = phba->nvmeio_trc_on;
+ 
+ 	index = (atomic_read(&phba->nvmeio_trc_cnt) + 1) &
+ 		(phba->nvmeio_trc_size - 1);
+ 	skip = phba->nvmeio_trc_output_idx;
+ 
+ 	len += snprintf(buf + len, size - len,
+ 			"%s IO Trace %s: next_idx %d skip %d size %d\n",
+ 			(phba->nvmet_support ? "NVME" : "NVMET"),
+ 			(state ? "Enabled" : "Disabled"),
+ 			index, skip, phba->nvmeio_trc_size);
+ 
+ 	if (!phba->nvmeio_trc || state)
+ 		return len;
+ 
+ 	/* trace MUST bhe off to continue */
+ 
+ 	for (i = index; i < phba->nvmeio_trc_size; i++) {
+ 		if (skip) {
+ 			skip--;
+ 			continue;
+ 		}
+ 		dtp = phba->nvmeio_trc + i;
+ 		phba->nvmeio_trc_output_idx++;
+ 
+ 		if (!dtp->fmt)
+ 			continue;
+ 
+ 		len +=  snprintf(buf + len, size - len, dtp->fmt,
+ 			dtp->data1, dtp->data2, dtp->data3);
+ 
+ 		if (phba->nvmeio_trc_output_idx >= phba->nvmeio_trc_size) {
+ 			phba->nvmeio_trc_output_idx = 0;
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Complete\n");
+ 			goto out;
+ 		}
+ 
+ 		if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Continue (%d of %d)\n",
+ 					phba->nvmeio_trc_output_idx,
+ 					phba->nvmeio_trc_size);
+ 			goto out;
+ 		}
+ 	}
+ 	for (i = 0; i < index; i++) {
+ 		if (skip) {
+ 			skip--;
+ 			continue;
+ 		}
+ 		dtp = phba->nvmeio_trc + i;
+ 		phba->nvmeio_trc_output_idx++;
+ 
+ 		if (!dtp->fmt)
+ 			continue;
+ 
+ 		len +=  snprintf(buf + len, size - len, dtp->fmt,
+ 			dtp->data1, dtp->data2, dtp->data3);
+ 
+ 		if (phba->nvmeio_trc_output_idx >= phba->nvmeio_trc_size) {
+ 			phba->nvmeio_trc_output_idx = 0;
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Complete\n");
+ 			goto out;
+ 		}
+ 
+ 		if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Continue (%d of %d)\n",
+ 					phba->nvmeio_trc_output_idx,
+ 					phba->nvmeio_trc_size);
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	len += snprintf(buf + len, size - len,
+ 			"Trace Done\n");
+ out:
+ 	return len;
+ }
+ 
+ /**
+  * lpfc_debugfs_cpucheck_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_cpucheck_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	int i;
+ 	int len = 0;
+ 	uint32_t tot_xmt = 0;
+ 	uint32_t tot_rcv = 0;
+ 	uint32_t tot_cmpl = 0;
+ 	uint32_t tot_ccmpl = 0;
+ 
+ 	if (phba->nvmet_support == 0) {
+ 		/* NVME Initiator */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"CPUcheck %s\n",
+ 				(phba->cpucheck_on & LPFC_CHECK_NVME_IO ?
+ 					"Enabled" : "Disabled"));
+ 		for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
+ 			if (i >= LPFC_CHECK_CPU_CNT)
+ 				break;
+ 			len += snprintf(buf + len, PAGE_SIZE - len,
+ 					"%02d: xmit x%08x cmpl x%08x\n",
+ 					i, phba->cpucheck_xmt_io[i],
+ 					phba->cpucheck_cmpl_io[i]);
+ 			tot_xmt += phba->cpucheck_xmt_io[i];
+ 			tot_cmpl += phba->cpucheck_cmpl_io[i];
+ 		}
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"tot:xmit x%08x cmpl x%08x\n",
+ 				tot_xmt, tot_cmpl);
+ 		return len;
+ 	}
+ 
+ 	/* NVME Target */
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"CPUcheck %s ",
+ 			(phba->cpucheck_on & LPFC_CHECK_NVMET_IO ?
+ 				"IO Enabled - " : "IO Disabled - "));
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"%s\n",
+ 			(phba->cpucheck_on & LPFC_CHECK_NVMET_RCV ?
+ 				"Rcv Enabled\n" : "Rcv Disabled\n"));
+ 	for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
+ 		if (i >= LPFC_CHECK_CPU_CNT)
+ 			break;
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"%02d: xmit x%08x ccmpl x%08x "
+ 				"cmpl x%08x rcv x%08x\n",
+ 				i, phba->cpucheck_xmt_io[i],
+ 				phba->cpucheck_ccmpl_io[i],
+ 				phba->cpucheck_cmpl_io[i],
+ 				phba->cpucheck_rcv_io[i]);
+ 		tot_xmt += phba->cpucheck_xmt_io[i];
+ 		tot_rcv += phba->cpucheck_rcv_io[i];
+ 		tot_cmpl += phba->cpucheck_cmpl_io[i];
+ 		tot_ccmpl += phba->cpucheck_ccmpl_io[i];
+ 	}
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"tot:xmit x%08x ccmpl x%08x cmpl x%08x rcv x%08x\n",
+ 			tot_xmt, tot_ccmpl, tot_cmpl, tot_rcv);
+ 	return len;
+ }
+ 
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  #endif
  
  /**
diff --cc drivers/scsi/lpfc/lpfc_init.c
index bad9d528b08b,c22b88a08c1b..000000000000
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@@ -1201,6 -1249,13 +1201,16 @@@ lpfc_hb_timeout_handler(struct lpfc_hb
  	int retval, i;
  	struct lpfc_sli *psli = &phba->sli;
  	LIST_HEAD(completions);
++<<<<<<< HEAD
++=======
+ 	struct lpfc_queue *qp;
+ 	unsigned long time_elapsed;
+ 	uint32_t tick_cqe, max_cqe, val;
+ 	uint64_t tot, data1, data2, data3;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	struct lpfc_register reg_data;
+ 	void __iomem *eqdreg = phba->sli4_hba.u.if_type2.EQDregaddr;
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  
  	vports = lpfc_create_vport_work_array(phba);
  	if (vports != NULL)
@@@ -1215,6 -1270,96 +1225,99 @@@
  		(phba->pport->fc_flag & FC_OFFLINE_MODE))
  		return;
  
++<<<<<<< HEAD
++=======
+ 	if (phba->cfg_auto_imax) {
+ 		if (!phba->last_eqdelay_time) {
+ 			phba->last_eqdelay_time = jiffies;
+ 			goto skip_eqdelay;
+ 		}
+ 		time_elapsed = jiffies - phba->last_eqdelay_time;
+ 		phba->last_eqdelay_time = jiffies;
+ 
+ 		tot = 0xffff;
+ 		/* Check outstanding IO count */
+ 		if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {
+ 			if (phba->nvmet_support) {
+ 				tgtp = phba->targetport->private;
+ 				/* Calculate outstanding IOs */
+ 				tot = atomic_read(&tgtp->rcv_fcp_cmd_drop);
+ 				tot += atomic_read(&tgtp->xmt_fcp_release);
+ 				tot = atomic_read(&tgtp->rcv_fcp_cmd_in) - tot;
+ 			} else {
+ 				tot = atomic_read(&phba->fc4NvmeIoCmpls);
+ 				data1 = atomic_read(
+ 					&phba->fc4NvmeInputRequests);
+ 				data2 = atomic_read(
+ 					&phba->fc4NvmeOutputRequests);
+ 				data3 = atomic_read(
+ 					&phba->fc4NvmeControlRequests);
+ 				tot =  (data1 + data2 + data3) - tot;
+ 			}
+ 		}
+ 
+ 		/* Interrupts per sec per EQ */
+ 		val = phba->cfg_fcp_imax / phba->io_channel_irqs;
+ 		tick_cqe = val / CONFIG_HZ; /* Per tick per EQ */
+ 
+ 		/* Assume 1 CQE/ISR, calc max CQEs allowed for time duration */
+ 		max_cqe = time_elapsed * tick_cqe;
+ 
+ 		for (i = 0; i < phba->io_channel_irqs; i++) {
+ 			/* Fast-path EQ */
+ 			qp = phba->sli4_hba.hba_eq[i];
+ 			if (!qp)
+ 				continue;
+ 
+ 			/* Use no EQ delay if we don't have many outstanding
+ 			 * IOs, or if we are only processing 1 CQE/ISR or less.
+ 			 * Otherwise, assume we can process up to lpfc_fcp_imax
+ 			 * interrupts per HBA.
+ 			 */
+ 			if (tot < LPFC_NODELAY_MAX_IO ||
+ 			    qp->EQ_cqe_cnt <= max_cqe)
+ 				val = 0;
+ 			else
+ 				val = phba->cfg_fcp_imax;
+ 
+ 			if (phba->sli.sli_flag & LPFC_SLI_USE_EQDR) {
+ 				/* Use EQ Delay Register method */
+ 
+ 				/* Convert for EQ Delay register */
+ 				if (val) {
+ 					/* First, interrupts per sec per EQ */
+ 					val = phba->cfg_fcp_imax /
+ 						phba->io_channel_irqs;
+ 
+ 					/* us delay between each interrupt */
+ 					val = LPFC_SEC_TO_USEC / val;
+ 				}
+ 				if (val != qp->q_mode) {
+ 					reg_data.word0 = 0;
+ 					bf_set(lpfc_sliport_eqdelay_id,
+ 					       &reg_data, qp->queue_id);
+ 					bf_set(lpfc_sliport_eqdelay_delay,
+ 					       &reg_data, val);
+ 					writel(reg_data.word0, eqdreg);
+ 				}
+ 			} else {
+ 				/* Use mbox command method */
+ 				if (val != qp->q_mode)
+ 					lpfc_modify_hba_eq_delay(phba, i,
+ 								 1, val);
+ 			}
+ 
+ 			/*
+ 			 * val is cfg_fcp_imax or 0 for mbox delay or us delay
+ 			 * between interrupts for EQDR.
+ 			 */
+ 			qp->q_mode = val;
+ 			qp->EQ_cqe_cnt = 0;
+ 		}
+ 	}
+ 
+ skip_eqdelay:
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  	spin_lock_irq(&phba->pport->work_port_lock);
  
  	if (time_after(phba->last_completion_time +
@@@ -5436,11 -5925,26 +5539,34 @@@ lpfc_sli4_driver_resource_setup(struct 
  	/*
  	 * Initialize the SLI Layer to run with lpfc SLI4 HBAs.
  	 */
++<<<<<<< HEAD
 +	/* Initialize the Abort scsi buffer list used by driver */
 +	spin_lock_init(&phba->sli4_hba.abts_scsi_buf_list_lock);
 +	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
 +	/* This abort list used by worker thread */
 +	spin_lock_init(&phba->sli4_hba.abts_sgl_list_lock);
++=======
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
+ 		/* Initialize the Abort scsi buffer list used by driver */
+ 		spin_lock_init(&phba->sli4_hba.abts_scsi_buf_list_lock);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
+ 	}
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {
+ 		/* Initialize the Abort nvme buffer list used by driver */
+ 		spin_lock_init(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_nvme_buf_list);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_nvmet_ctx_list);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_nvmet_io_wait_list);
+ 
+ 		/* Fast-path XRI aborted CQ Event work queue list */
+ 		INIT_LIST_HEAD(&phba->sli4_hba.sp_nvme_xri_aborted_work_queue);
+ 	}
+ 
+ 	/* This abort list used by worker thread */
+ 	spin_lock_init(&phba->sli4_hba.sgl_list_lock);
+ 	spin_lock_init(&phba->sli4_hba.nvmet_io_wait_lock);
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  
  	/*
  	 * Initialize driver internal slow-path work queues
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index afe166ddbf5a,8b119f87b51d..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -12447,7 -13215,114 +12447,118 @@@ lpfc_sli4_fp_handle_rel_wcqe(struct lpf
  }
  
  /**
++<<<<<<< HEAD
 + * lpfc_sli4_fp_handle_wcqe - Process fast-path work queue completion entry
++=======
+  * lpfc_sli4_nvmet_handle_rcqe - Process a receive-queue completion queue entry
+  * @phba: Pointer to HBA context object.
+  * @rcqe: Pointer to receive-queue completion queue entry.
+  *
+  * This routine process a receive-queue completion queue entry.
+  *
+  * Return: true if work posted to worker thread, otherwise false.
+  **/
+ static bool
+ lpfc_sli4_nvmet_handle_rcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,
+ 			    struct lpfc_rcqe *rcqe)
+ {
+ 	bool workposted = false;
+ 	struct lpfc_queue *hrq;
+ 	struct lpfc_queue *drq;
+ 	struct rqb_dmabuf *dma_buf;
+ 	struct fc_frame_header *fc_hdr;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	uint32_t status, rq_id;
+ 	unsigned long iflags;
+ 	uint32_t fctl, idx;
+ 
+ 	if ((phba->nvmet_support == 0) ||
+ 	    (phba->sli4_hba.nvmet_cqset == NULL))
+ 		return workposted;
+ 
+ 	idx = cq->queue_id - phba->sli4_hba.nvmet_cqset[0]->queue_id;
+ 	hrq = phba->sli4_hba.nvmet_mrq_hdr[idx];
+ 	drq = phba->sli4_hba.nvmet_mrq_data[idx];
+ 
+ 	/* sanity check on queue memory */
+ 	if (unlikely(!hrq) || unlikely(!drq))
+ 		return workposted;
+ 
+ 	if (bf_get(lpfc_cqe_code, rcqe) == CQE_CODE_RECEIVE_V1)
+ 		rq_id = bf_get(lpfc_rcqe_rq_id_v1, rcqe);
+ 	else
+ 		rq_id = bf_get(lpfc_rcqe_rq_id, rcqe);
+ 
+ 	if ((phba->nvmet_support == 0) ||
+ 	    (rq_id != hrq->queue_id))
+ 		return workposted;
+ 
+ 	status = bf_get(lpfc_rcqe_status, rcqe);
+ 	switch (status) {
+ 	case FC_STATUS_RQ_BUF_LEN_EXCEEDED:
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 				"6126 Receive Frame Truncated!!\n");
+ 		/* Drop thru */
+ 	case FC_STATUS_RQ_SUCCESS:
+ 		lpfc_sli4_rq_release(hrq, drq);
+ 		spin_lock_irqsave(&phba->hbalock, iflags);
+ 		dma_buf = lpfc_sli_rqbuf_get(phba, hrq);
+ 		if (!dma_buf) {
+ 			hrq->RQ_no_buf_found++;
+ 			spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 			goto out;
+ 		}
+ 		spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 		hrq->RQ_rcv_buf++;
+ 		hrq->RQ_buf_posted--;
+ 		fc_hdr = (struct fc_frame_header *)dma_buf->hbuf.virt;
+ 
+ 		/* Just some basic sanity checks on FCP Command frame */
+ 		fctl = (fc_hdr->fh_f_ctl[0] << 16 |
+ 		fc_hdr->fh_f_ctl[1] << 8 |
+ 		fc_hdr->fh_f_ctl[2]);
+ 		if (((fctl &
+ 		    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) !=
+ 		    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) ||
+ 		    (fc_hdr->fh_seq_cnt != 0)) /* 0 byte swapped is still 0 */
+ 			goto drop;
+ 
+ 		if (fc_hdr->fh_type == FC_TYPE_FCP) {
+ 			dma_buf->bytes_recv = bf_get(lpfc_rcqe_length,  rcqe);
+ 			lpfc_nvmet_unsol_fcp_event(
+ 				phba, idx, dma_buf,
+ 				cq->assoc_qp->isr_timestamp);
+ 			return false;
+ 		}
+ drop:
+ 		lpfc_in_buf_free(phba, &dma_buf->dbuf);
+ 		break;
+ 	case FC_STATUS_INSUFF_BUF_FRM_DISC:
+ 		if (phba->nvmet_support) {
+ 			tgtp = phba->targetport->private;
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI | LOG_NVME,
+ 					"6401 RQE Error x%x, posted %d err_cnt "
+ 					"%d: %x %x %x\n",
+ 					status, hrq->RQ_buf_posted,
+ 					hrq->RQ_no_posted_buf,
+ 					atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 					atomic_read(&tgtp->rcv_fcp_cmd_out),
+ 					atomic_read(&tgtp->xmt_fcp_release));
+ 		}
+ 		/* fallthrough */
+ 
+ 	case FC_STATUS_INSUFF_BUF_NEED_BUF:
+ 		hrq->RQ_no_posted_buf++;
+ 		/* Post more buffers if possible */
+ 		break;
+ 	}
+ out:
+ 	return workposted;
+ }
+ 
+ /**
+  * lpfc_sli4_fp_handle_cqe - Process fast-path work queue completion entry
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
   * @cq: Pointer to the completion queue.
   * @eqe: Pointer to fast-path completion queue entry.
   *
diff --cc drivers/scsi/lpfc/lpfc_sli4.h
index 10078254ebc7,7dc8c73b5903..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@@ -568,14 -614,24 +568,27 @@@ struct lpfc_sli4_hba 
  	uint16_t rpi_hdrs_in_use; /* must post rpi hdrs if set. */
  	uint16_t next_xri; /* last_xri - max_cfg_param.xri_base = used */
  	uint16_t next_rpi;
 -	uint16_t nvme_xri_max;
 -	uint16_t nvme_xri_cnt;
 -	uint16_t nvme_xri_start;
  	uint16_t scsi_xri_max;
  	uint16_t scsi_xri_cnt;
 -	uint16_t scsi_xri_start;
  	uint16_t els_xri_cnt;
++<<<<<<< HEAD
 +	uint16_t scsi_xri_start;
 +	struct list_head lpfc_free_sgl_list;
 +	struct list_head lpfc_sgl_list;
++=======
+ 	uint16_t nvmet_xri_cnt;
+ 	uint16_t nvmet_io_wait_cnt;
+ 	uint16_t nvmet_io_wait_total;
+ 	struct list_head lpfc_els_sgl_list;
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  	struct list_head lpfc_abts_els_sgl_list;
 -	struct list_head lpfc_nvmet_sgl_list;
 -	struct list_head lpfc_abts_nvmet_ctx_list;
  	struct list_head lpfc_abts_scsi_buf_list;
++<<<<<<< HEAD
++=======
+ 	struct list_head lpfc_abts_nvme_buf_list;
+ 	struct list_head lpfc_nvmet_io_wait_list;
+ 	struct lpfc_nvmet_ctx_info *nvmet_ctx_info;
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  	struct lpfc_sglq **lpfc_sglq_active_list;
  	struct list_head lpfc_rpi_hdr_list;
  	unsigned long *rpi_bmask;
@@@ -602,8 -659,10 +615,13 @@@
  #define LPFC_SLI4_PPNAME_NON	0
  #define LPFC_SLI4_PPNAME_GET	1
  	struct lpfc_iov iov;
 -	spinlock_t abts_nvme_buf_list_lock; /* list of aborted SCSI IOs */
  	spinlock_t abts_scsi_buf_list_lock; /* list of aborted SCSI IOs */
++<<<<<<< HEAD
 +	spinlock_t abts_sgl_list_lock; /* list of aborted els IOs */
++=======
+ 	spinlock_t sgl_list_lock; /* list of aborted els IOs */
+ 	spinlock_t nvmet_io_wait_lock; /* IOs waiting for ctx resources */
++>>>>>>> 66d7ce93a0f5 (scsi: lpfc: Fix MRQ > 1 context list handling)
  	uint32_t physical_port;
  
  	/* CPU to vector mapping information */
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc_attr.c
* Unmerged path drivers/scsi/lpfc/lpfc_crtn.h
* Unmerged path drivers/scsi/lpfc/lpfc_debugfs.c
* Unmerged path drivers/scsi/lpfc/lpfc_init.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli4.h
