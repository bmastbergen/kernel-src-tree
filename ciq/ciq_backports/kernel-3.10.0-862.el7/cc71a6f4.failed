blk-mq: abstract out helpers for allocating/freeing tag maps

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit cc71a6f43886a8af57dbbce2a45b4b2aaf570fe6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/cc71a6f4.failed

Prep patch for adding an extra tag map for scheduler requests.

	Signed-off-by: Jens Axboe <axboe@fb.com>
	Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Reviewed-by: Omar Sandoval <osandov@fb.com>
(cherry picked from commit cc71a6f43886a8af57dbbce2a45b4b2aaf570fe6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	block/blk-mq.h
diff --cc block/blk-mq.c
index 5b92b7659b74,fcdeadc55753..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1545,19 -1550,11 +1545,24 @@@ run_queue
  	}
  
  	blk_mq_put_ctx(data.ctx);
 -	return cookie;
  }
  
++<<<<<<< HEAD
 +/*
 + * Default mapping to a software queue, since we use one per CPU.
 + */
 +struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q, const int cpu)
 +{
 +	return q->queue_hw_ctx[q->mq_map[cpu]];
 +}
 +EXPORT_SYMBOL(blk_mq_map_queue);
 +
 +static void blk_mq_free_rq_map(struct blk_mq_tag_set *set,
 +		struct blk_mq_tags *tags, unsigned int hctx_idx)
++=======
+ void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+ 		     unsigned int hctx_idx)
++>>>>>>> cc71a6f43886 (blk-mq: abstract out helpers for allocating/freeing tag maps)
  {
  	struct page *page;
  
@@@ -1589,20 -1590,16 +1598,31 @@@ void blk_mq_free_rq_map(struct blk_mq_t
  	blk_mq_free_tags(tags);
  }
  
++<<<<<<< HEAD
 +static size_t order_to_size(unsigned int order)
 +{
 +	return (size_t)PAGE_SIZE << order;
 +}
 +
 +static struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
 +		unsigned int hctx_idx)
++=======
+ struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
+ 					unsigned int hctx_idx,
+ 					unsigned int nr_tags,
+ 					unsigned int reserved_tags)
++>>>>>>> cc71a6f43886 (blk-mq: abstract out helpers for allocating/freeing tag maps)
  {
  	struct blk_mq_tags *tags;
- 	unsigned int i, j, entries_per_page, max_order = 4;
- 	size_t rq_size, left;
  
++<<<<<<< HEAD
 +	tags = blk_mq_init_tags(set->queue_depth, set->reserved_tags,
 +				set->numa_node);
++=======
+ 	tags = blk_mq_init_tags(nr_tags, reserved_tags,
+ 				set->numa_node,
+ 				BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
++>>>>>>> cc71a6f43886 (blk-mq: abstract out helpers for allocating/freeing tag maps)
  	if (!tags)
  		return NULL;
  
@@@ -1675,39 -1686,13 +1709,39 @@@ int blk_mq_alloc_rqs(struct blk_mq_tag_
  			i++;
  		}
  	}
- 	return tags;
+ 	return 0;
  
  fail:
- 	blk_mq_free_rq_map(set, tags, hctx_idx);
- 	return NULL;
+ 	blk_mq_free_rqs(set, tags, hctx_idx);
+ 	return -ENOMEM;
  }
  
 +static void blk_mq_free_bitmap(struct blk_mq_ctxmap *bitmap)
 +{
 +	kfree(bitmap->map);
 +}
 +
 +static int blk_mq_alloc_bitmap(struct blk_mq_ctxmap *bitmap, int node)
 +{
 +	unsigned int bpw = 8, total, num_maps, i;
 +
 +	bitmap->bits_per_word = bpw;
 +
 +	num_maps = ALIGN(nr_cpu_ids, bpw) / bpw;
 +	bitmap->map = kzalloc_node(num_maps * sizeof(struct blk_align_bitmap),
 +					GFP_KERNEL, node);
 +	if (!bitmap->map)
 +		return -ENOMEM;
 +
 +	total = nr_cpu_ids;
 +	for (i = 0; i < num_maps; i++) {
 +		bitmap->map[i].depth = min(total, bitmap->bits_per_word);
 +		total -= bitmap->map[i].depth;
 +	}
 +
 +	return 0;
 +}
 +
  /*
   * 'cpu' is going away. splice any existing rq_list entries from this
   * software queue to the hw queue dispatch list, and ensure that it
diff --cc block/blk-mq.h
index 2d50f02667c4,1b279b02d0f6..000000000000
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@@ -34,7 -32,26 +34,30 @@@ void blk_mq_free_queue(struct request_q
  int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
  void blk_mq_wake_waiters(struct request_queue *q);
  bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
++<<<<<<< HEAD
 +
++=======
+ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
+ 
+ /*
+  * Internal helpers for allocating/freeing the request map
+  */
+ void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+ 		     unsigned int hctx_idx);
+ void blk_mq_free_rq_map(struct blk_mq_tags *tags);
+ struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
+ 					unsigned int hctx_idx,
+ 					unsigned int nr_tags,
+ 					unsigned int reserved_tags);
+ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+ 		     unsigned int hctx_idx, unsigned int depth);
+ 
+ /*
+  * Internal helpers for request insertion into sw queues
+  */
+ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
+ 				bool at_head);
++>>>>>>> cc71a6f43886 (blk-mq: abstract out helpers for allocating/freeing tag maps)
  /*
   * CPU hotplug helpers
   */
* Unmerged path block/blk-mq.c
* Unmerged path block/blk-mq.h
