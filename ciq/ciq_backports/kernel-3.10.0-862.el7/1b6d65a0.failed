block: Introduce BLK_MQ_REQ_PREEMPT

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [block] Introduce BLK_MQ_REQ_PREEMPT (Ming Lei) [1491296]
Rebuild_FUZZ: 88.89%
commit-author Bart Van Assche <bart.vanassche@wdc.com>
commit 1b6d65a0bfb5df2a6029c1430e99fcc5d96bb59a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1b6d65a0.failed

Set RQF_PREEMPT if BLK_MQ_REQ_PREEMPT is passed to
blk_get_request_flags().

	Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Tested-by: Martin Steigerwald <martin@lichtvoll.de>
	Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Ming Lei <ming.lei@redhat.com>
	Cc: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 1b6d65a0bfb5df2a6029c1430e99fcc5d96bb59a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	block/blk-mq.c
#	include/linux/blk-mq.h
diff --cc block/blk-core.c
index df55e6267498,17eed16a6e04..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -1138,10 -1261,13 +1138,17 @@@ static struct request *__get_request(st
  
  	blk_rq_init(q, rq);
  	blk_rq_set_rl(rq, rl);
++<<<<<<< HEAD
 +	rq->cmd_flags = rw_flags | REQ_ALLOCED;
++=======
+ 	rq->cmd_flags = op;
+ 	rq->rq_flags = rq_flags;
+ 	if (flags & BLK_MQ_REQ_PREEMPT)
+ 		rq->rq_flags |= RQF_PREEMPT;
++>>>>>>> 1b6d65a0bfb5 (block: Introduce BLK_MQ_REQ_PREEMPT)
  
  	/* init elvpriv */
 -	if (rq_flags & RQF_ELVPRIV) {
 +	if (rw_flags & REQ_ELVPRIV) {
  		if (unlikely(et->icq_cache && !icq)) {
  			if (ioc)
  				icq = ioc_create_icq(ioc, q, gfp_mask);
@@@ -1286,14 -1434,39 +1293,41 @@@ static struct request *blk_old_get_requ
  	return rq;
  }
  
 -/**
 - * blk_get_request_flags - allocate a request
 - * @q: request queue to allocate a request for
 - * @op: operation (REQ_OP_*) and REQ_* flags, e.g. REQ_SYNC.
 - * @flags: BLK_MQ_REQ_* flags, e.g. BLK_MQ_REQ_NOWAIT.
 - */
 -struct request *blk_get_request_flags(struct request_queue *q, unsigned int op,
 -				      unsigned int flags)
 +struct request *blk_get_request(struct request_queue *q, int rw, gfp_t gfp_mask)
  {
++<<<<<<< HEAD
 +	if (q->mq_ops)
 +		return blk_mq_alloc_request(q, rw,
 +			(gfp_mask & __GFP_WAIT) ?
 +				0 : BLK_MQ_REQ_NOWAIT);
 +	else
 +		return blk_old_get_request(q, rw, gfp_mask);
++=======
+ 	struct request *req;
+ 
+ 	WARN_ON_ONCE(op & REQ_NOWAIT);
+ 	WARN_ON_ONCE(flags & ~(BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_PREEMPT));
+ 
+ 	if (q->mq_ops) {
+ 		req = blk_mq_alloc_request(q, op, flags);
+ 		if (!IS_ERR(req) && q->mq_ops->initialize_rq_fn)
+ 			q->mq_ops->initialize_rq_fn(req);
+ 	} else {
+ 		req = blk_old_get_request(q, op, flags);
+ 		if (!IS_ERR(req) && q->initialize_rq_fn)
+ 			q->initialize_rq_fn(req);
+ 	}
+ 
+ 	return req;
+ }
+ EXPORT_SYMBOL(blk_get_request_flags);
+ 
+ struct request *blk_get_request(struct request_queue *q, unsigned int op,
+ 				gfp_t gfp_mask)
+ {
+ 	return blk_get_request_flags(q, op, gfp_mask & __GFP_DIRECT_RECLAIM ?
+ 				     0 : BLK_MQ_REQ_NOWAIT);
++>>>>>>> 1b6d65a0bfb5 (block: Introduce BLK_MQ_REQ_PREEMPT)
  }
  EXPORT_SYMBOL(blk_get_request);
  
diff --cc block/blk-mq.c
index d9bfe0c6bc0e,e21876778cec..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -203,9 -288,13 +203,19 @@@ static void blk_mq_rq_ctx_init(struct r
  
  	INIT_LIST_HEAD(&rq->queuelist);
  	/* csd/requeue_work/fifo_time is initialized before use */
++<<<<<<< HEAD
 +	rq->q = q;
 +	rq->mq_ctx = ctx;
 +	rq->cmd_flags |= rw_flags;
++=======
+ 	rq->q = data->q;
+ 	rq->mq_ctx = data->ctx;
+ 	rq->cmd_flags = op;
+ 	if (data->flags & BLK_MQ_REQ_PREEMPT)
+ 		rq->rq_flags |= RQF_PREEMPT;
+ 	if (blk_queue_io_stat(data->q))
+ 		rq->rq_flags |= RQF_IO_STAT;
++>>>>>>> 1b6d65a0bfb5 (block: Introduce BLK_MQ_REQ_PREEMPT)
  	/* do not touch atomic flags, it needs atomic ops against the timer */
  	rq->cpu = -1;
  	INIT_HLIST_NODE(&rq->hash);
diff --cc include/linux/blk-mq.h
index ab31251b7413,82b56609736a..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -232,14 -211,15 +232,19 @@@ bool blk_mq_can_queue(struct blk_mq_hw_
  enum {
  	BLK_MQ_REQ_NOWAIT	= (1 << 0), /* return when out of requests */
  	BLK_MQ_REQ_RESERVED	= (1 << 1), /* allocate from reserved pool */
++<<<<<<< HEAD
++=======
+ 	BLK_MQ_REQ_INTERNAL	= (1 << 2), /* allocate internal/sched tag */
+ 	BLK_MQ_REQ_PREEMPT	= (1 << 3), /* set RQF_PREEMPT */
++>>>>>>> 1b6d65a0bfb5 (block: Introduce BLK_MQ_REQ_PREEMPT)
  };
  
 -struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 +struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
  		unsigned int flags);
 -struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 -		unsigned int op, unsigned int flags, unsigned int hctx_idx);
 +struct request *blk_mq_alloc_request_hctx(struct request_queue *q, int op,
 +		unsigned int flags, unsigned int hctx_idx);
  struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 +struct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags);
  
  enum {
  	BLK_MQ_UNIQUE_TAG_BITS = 16,
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blk-mq.h
