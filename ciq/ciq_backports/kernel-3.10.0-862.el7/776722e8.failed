ext4: DAX iomap write support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jan Kara <jack@suse.cz>
commit 776722e85d3b0936253ecc3d14db4fba37f191ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/776722e8.failed

Implement DAX writes using the new iomap infrastructure instead of
overloading the direct IO path.

	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Theodore Ts'o <tytso@mit.edu>
(cherry picked from commit 776722e85d3b0936253ecc3d14db4fba37f191ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/file.c
#	fs/ext4/inode.c
diff --cc fs/ext4/file.c
index db1bf9992414,1953fe34f9fe..000000000000
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@@ -177,27 -159,98 +177,94 @@@ ext4_file_write(struct kiocb *iocb, con
  	 * If we have encountered a bitmap-format file, the size limit
  	 * is smaller than s_maxbytes, which is for extent-mapped files.
  	 */
 +
  	if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {
  		struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
 +		size_t length = iov_length(iov, nr_segs);
  
 -		if (iocb->ki_pos >= sbi->s_bitmap_maxbytes)
 +		if ((pos > sbi->s_bitmap_maxbytes ||
 +		    (pos == sbi->s_bitmap_maxbytes && length > 0)))
  			return -EFBIG;
 -		iov_iter_truncate(from, sbi->s_bitmap_maxbytes - iocb->ki_pos);
 -	}
 -	return iov_iter_count(from);
 -}
  
++<<<<<<< HEAD
 +		if (pos + length > sbi->s_bitmap_maxbytes) {
 +			nr_segs = iov_shorten((struct iovec *)iov, nr_segs,
 +					      sbi->s_bitmap_maxbytes - pos);
 +		}
++=======
+ #ifdef CONFIG_FS_DAX
+ static ssize_t
+ ext4_dax_write_iter(struct kiocb *iocb, struct iov_iter *from)
+ {
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 	ssize_t ret;
+ 	bool overwrite = false;
+ 
+ 	inode_lock(inode);
+ 	ret = ext4_write_checks(iocb, from);
+ 	if (ret <= 0)
+ 		goto out;
+ 	ret = file_remove_privs(iocb->ki_filp);
+ 	if (ret)
+ 		goto out;
+ 	ret = file_update_time(iocb->ki_filp);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (ext4_overwrite_io(inode, iocb->ki_pos, iov_iter_count(from))) {
+ 		overwrite = true;
+ 		downgrade_write(&inode->i_rwsem);
+ 	}
+ 	ret = dax_iomap_rw(iocb, from, &ext4_iomap_ops);
+ out:
+ 	if (!overwrite)
+ 		inode_unlock(inode);
+ 	else
+ 		inode_unlock_shared(inode);
+ 	if (ret > 0)
+ 		ret = generic_write_sync(iocb, ret);
+ 	return ret;
+ }
+ #endif
+ 
+ static ssize_t
+ ext4_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
+ {
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 	int o_direct = iocb->ki_flags & IOCB_DIRECT;
+ 	int unaligned_aio = 0;
+ 	int overwrite = 0;
+ 	ssize_t ret;
+ 
+ #ifdef CONFIG_FS_DAX
+ 	if (IS_DAX(inode))
+ 		return ext4_dax_write_iter(iocb, from);
+ #endif
+ 
+ 	inode_lock(inode);
+ 	ret = ext4_write_checks(iocb, from);
+ 	if (ret <= 0)
+ 		goto out;
+ 
+ 	/*
+ 	 * Unaligned direct AIO must be serialized among each other as zeroing
+ 	 * of partial blocks of two competing unaligned AIOs can result in data
+ 	 * corruption.
+ 	 */
+ 	if (o_direct && ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS) &&
+ 	    !is_sync_kiocb(iocb) &&
+ 	    ext4_unaligned_aio(inode, from, iocb->ki_pos)) {
+ 		unaligned_aio = 1;
+ 		ext4_unwritten_wait(inode);
++>>>>>>> 776722e85d3b (ext4: DAX iomap write support)
  	}
  
 -	iocb->private = &overwrite;
 -	/* Check whether we do a DIO overwrite or not */
 -	if (o_direct && ext4_should_dioread_nolock(inode) && !unaligned_aio &&
 -	    ext4_overwrite_io(inode, iocb->ki_pos, iov_iter_count(from)))
 -		overwrite = 1;
 -
 -	ret = __generic_file_write_iter(iocb, from);
 -	inode_unlock(inode);
 -
 -	if (ret > 0)
 -		ret = generic_write_sync(iocb, ret);
 -
 -	return ret;
 +	iocb->private = &overwrite; /* RHEL7 only - prevent DIO race */
 +	if (unlikely(io_is_direct(iocb->ki_filp)))
 +		ret = ext4_file_dio_write(iocb, iov, nr_segs, pos);
 +	else
 +		ret = generic_file_aio_write(iocb, iov, nr_segs, pos);
  
 -out:
 -	inode_unlock(inode);
  	return ret;
  }
  
diff --cc fs/ext4/inode.c
index f49ba18669c7,3941cee21e4c..000000000000
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@@ -3064,55 -3288,204 +3064,227 @@@ static int ext4_get_block_overwrite(str
  int ext4_dax_get_block(struct inode *inode, sector_t iblock,
  		       struct buffer_head *bh_result, int create)
  {
 -	int ret;
 +	int ret, err;
 +	int credits;
 +	struct ext4_map_blocks map;
 +	handle_t *handle = NULL;
 +	int retries = 0;
 +	int flags = 0;
  
  	ext4_debug("inode %lu, create flag %d\n", inode->i_ino, create);
 -	if (!create)
 -		return _ext4_get_block(inode, iblock, bh_result, 0);
 -
 -	ret = ext4_get_block_trans(inode, iblock, bh_result,
 -				   EXT4_GET_BLOCKS_PRE_IO |
 -				   EXT4_GET_BLOCKS_CREATE_ZERO);
 -	if (ret < 0)
 -		return ret;
 -
 -	if (buffer_unwritten(bh_result)) {
 -		/*
 -		 * We are protected by i_mmap_sem or i_mutex so we know block
 -		 * cannot go away from under us even though we dropped
 -		 * i_data_sem. Convert extent to written and write zeros there.
 -		 */
 -		ret = ext4_get_block_trans(inode, iblock, bh_result,
 -					   EXT4_GET_BLOCKS_CONVERT |
 -					   EXT4_GET_BLOCKS_CREATE_ZERO);
 -		if (ret < 0)
 +	map.m_lblk = iblock;
 +	map.m_len = bh_result->b_size >> inode->i_blkbits;
 +	credits = ext4_chunk_trans_blocks(inode, map.m_len);
 +retry:
 +	if (create) {
 +		flags |= EXT4_GET_BLOCKS_CREATE_ZERO;
 +		handle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS, credits);
 +		if (IS_ERR(handle)) {
 +			ret = PTR_ERR(handle);
  			return ret;
++<<<<<<< HEAD
++=======
+ 	}
+ 	/*
+ 	 * At least for now we have to clear BH_New so that DAX code
+ 	 * doesn't attempt to zero blocks again in a racy way.
+ 	 */
+ 	clear_buffer_new(bh_result);
+ 	return 0;
+ }
+ 
+ static int ext4_iomap_begin(struct inode *inode, loff_t offset, loff_t length,
+ 			    unsigned flags, struct iomap *iomap)
+ {
+ 	unsigned int blkbits = inode->i_blkbits;
+ 	unsigned long first_block = offset >> blkbits;
+ 	unsigned long last_block = (offset + length - 1) >> blkbits;
+ 	struct ext4_map_blocks map;
+ 	int ret;
+ 
+ 	if (WARN_ON_ONCE(ext4_has_inline_data(inode)))
+ 		return -ERANGE;
+ 
+ 	map.m_lblk = first_block;
+ 	map.m_len = last_block - first_block + 1;
+ 
+ 	if (!(flags & IOMAP_WRITE)) {
+ 		ret = ext4_map_blocks(NULL, inode, &map, 0);
+ 	} else {
+ 		int dio_credits;
+ 		handle_t *handle;
+ 		int retries = 0;
+ 
+ 		/* Trim mapping request to maximum we can map at once for DIO */
+ 		if (map.m_len > DIO_MAX_BLOCKS)
+ 			map.m_len = DIO_MAX_BLOCKS;
+ 		dio_credits = ext4_chunk_trans_blocks(inode, map.m_len);
+ retry:
+ 		/*
+ 		 * Either we allocate blocks and then we don't get unwritten
+ 		 * extent so we have reserved enough credits, or the blocks
+ 		 * are already allocated and unwritten and in that case
+ 		 * extent conversion fits in the credits as well.
+ 		 */
+ 		handle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,
+ 					    dio_credits);
+ 		if (IS_ERR(handle))
+ 			return PTR_ERR(handle);
+ 
+ 		ret = ext4_map_blocks(handle, inode, &map,
+ 				      EXT4_GET_BLOCKS_PRE_IO |
+ 				      EXT4_GET_BLOCKS_CREATE_ZERO);
+ 		if (ret < 0) {
+ 			ext4_journal_stop(handle);
+ 			if (ret == -ENOSPC &&
+ 			    ext4_should_retry_alloc(inode->i_sb, &retries))
+ 				goto retry;
+ 			return ret;
+ 		}
+ 		/* For DAX writes we need to zero out unwritten extents */
+ 		if (map.m_flags & EXT4_MAP_UNWRITTEN) {
+ 			/*
+ 			 * We are protected by i_mmap_sem or i_rwsem so we know
+ 			 * block cannot go away from under us even though we
+ 			 * dropped i_data_sem. Convert extent to written and
+ 			 * write zeros there.
+ 			 */
+ 			ret = ext4_map_blocks(handle, inode, &map,
+ 					      EXT4_GET_BLOCKS_CONVERT |
+ 					      EXT4_GET_BLOCKS_CREATE_ZERO);
+ 			if (ret < 0) {
+ 				ext4_journal_stop(handle);
+ 				return ret;
+ 			}
+ 		}
+ 
+ 		/*
+ 		 * If we added blocks beyond i_size we need to make sure they
+ 		 * will get truncated if we crash before updating i_size in
+ 		 * ext4_iomap_end().
+ 		 */
+ 		if (first_block + map.m_len >
+ 		    (inode->i_size + (1 << blkbits) - 1) >> blkbits) {
+ 			int err;
+ 
+ 			err = ext4_orphan_add(handle, inode);
+ 			if (err < 0) {
+ 				ext4_journal_stop(handle);
+ 				return err;
+ 			}
+ 		}
+ 		ext4_journal_stop(handle);
+ 	}
+ 
+ 	iomap->flags = 0;
+ 	iomap->bdev = inode->i_sb->s_bdev;
+ 	iomap->offset = first_block << blkbits;
+ 
+ 	if (ret == 0) {
+ 		iomap->type = IOMAP_HOLE;
+ 		iomap->blkno = IOMAP_NULL_BLOCK;
+ 		iomap->length = (u64)map.m_len << blkbits;
+ 	} else {
+ 		if (map.m_flags & EXT4_MAP_MAPPED) {
+ 			iomap->type = IOMAP_MAPPED;
+ 		} else if (map.m_flags & EXT4_MAP_UNWRITTEN) {
+ 			iomap->type = IOMAP_UNWRITTEN;
+ 		} else {
+ 			WARN_ON_ONCE(1);
+ 			return -EIO;
++>>>>>>> 776722e85d3b (ext4: DAX iomap write support)
  		}
 -		iomap->blkno = (sector_t)map.m_pblk << (blkbits - 9);
 -		iomap->length = (u64)map.m_len << blkbits;
  	}
  
 -	if (map.m_flags & EXT4_MAP_NEW)
 -		iomap->flags |= IOMAP_F_NEW;
 -	return 0;
 +	ret = ext4_map_blocks(handle, inode, &map, flags);
 +	if (create) {
 +		err = ext4_journal_stop(handle);
 +		if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))
 +			goto retry;
 +		if (ret >= 0 && err < 0)
 +			ret = err;
 +	}
 +	if (ret <= 0)
 +		goto out;
 +out:
 +	WARN_ON_ONCE(ret == 0 && create);
 +	if (ret > 0) {
 +		map_bh(bh_result, inode->i_sb, map.m_pblk);
 +		/*
 +		 * At least for now we have to clear BH_New so that DAX code
 +		 * doesn't attempt to zero blocks again in a racy way.
 +		 */
 +		map.m_flags &= ~EXT4_MAP_NEW;
 +		ext4_update_bh_state(bh_result, map.m_flags);
 +		bh_result->b_size = map.m_len << inode->i_blkbits;
 +		ret = 0;
 +	} else if (ret == 0) {
 +		/* hole case, need to fill in bh->b_size */
 +		bh_result->b_size = map.m_len << inode->i_blkbits;
 +	}
 +	return ret;
  }
++<<<<<<< HEAD
++=======
+ 
+ static int ext4_iomap_end(struct inode *inode, loff_t offset, loff_t length,
+ 			  ssize_t written, unsigned flags, struct iomap *iomap)
+ {
+ 	int ret = 0;
+ 	handle_t *handle;
+ 	int blkbits = inode->i_blkbits;
+ 	bool truncate = false;
+ 
+ 	if (!(flags & IOMAP_WRITE))
+ 		return 0;
+ 
+ 	handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
+ 	if (IS_ERR(handle)) {
+ 		ret = PTR_ERR(handle);
+ 		goto orphan_del;
+ 	}
+ 	if (ext4_update_inode_size(inode, offset + written))
+ 		ext4_mark_inode_dirty(handle, inode);
+ 	/*
+ 	 * We may need to truncate allocated but not written blocks beyond EOF.
+ 	 */
+ 	if (iomap->offset + iomap->length > 
+ 	    ALIGN(inode->i_size, 1 << blkbits)) {
+ 		ext4_lblk_t written_blk, end_blk;
+ 
+ 		written_blk = (offset + written) >> blkbits;
+ 		end_blk = (offset + length) >> blkbits;
+ 		if (written_blk < end_blk && ext4_can_truncate(inode))
+ 			truncate = true;
+ 	}
+ 	/*
+ 	 * Remove inode from orphan list if we were extending a inode and
+ 	 * everything went fine.
+ 	 */
+ 	if (!truncate && inode->i_nlink &&
+ 	    !list_empty(&EXT4_I(inode)->i_orphan))
+ 		ext4_orphan_del(handle, inode);
+ 	ext4_journal_stop(handle);
+ 	if (truncate) {
+ 		ext4_truncate_failed_write(inode);
+ orphan_del:
+ 		/*
+ 		 * If truncate failed early the inode might still be on the
+ 		 * orphan list; we need to make sure the inode is removed from
+ 		 * the orphan list in that case.
+ 		 */
+ 		if (inode->i_nlink)
+ 			ext4_orphan_del(NULL, inode);
+ 	}
+ 	return ret;
+ }
+ 
+ struct iomap_ops ext4_iomap_ops = {
+ 	.iomap_begin		= ext4_iomap_begin,
+ 	.iomap_end		= ext4_iomap_end,
+ };
+ 
++>>>>>>> 776722e85d3b (ext4: DAX iomap write support)
  #else
  /* Just define empty function, it will never get called. */
  int ext4_dax_get_block(struct inode *inode, sector_t iblock,
* Unmerged path fs/ext4/file.c
* Unmerged path fs/ext4/inode.c
