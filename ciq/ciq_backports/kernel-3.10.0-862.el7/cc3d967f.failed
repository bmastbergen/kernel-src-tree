KVM: SVM: detect opening of SMI window using STGI intercept

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ladi Prosek <lprosek@redhat.com>
commit cc3d967f7e32ceeb9b78dc962126ebcf1a2b24b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/cc3d967f.failed

Commit 05cade71cf3b ("KVM: nSVM: fix SMI injection in guest mode") made
KVM mask SMI if GIF=0 but it didn't do anything to unmask it when GIF is
enabled.

The issue manifests for me as a significantly longer boot time of Windows
guests when running with SMM-enabled OVMF.

This commit fixes it by intercepting STGI instead of requesting immediate
exit if the reason why SMM was masked is GIF.

Fixes: 05cade71cf3b ("KVM: nSVM: fix SMI injection in guest mode")
	Signed-off-by: Ladi Prosek <lprosek@redhat.com>
	Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>
(cherry picked from commit cc3d967f7e32ceeb9b78dc962126ebcf1a2b24b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/include/asm/kvm_host.h
index ab2d8132f390,7233445a20bd..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -966,7 -1057,15 +966,15 @@@ struct kvm_x86_ops 
  			      uint32_t guest_irq, bool set);
  	void (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);
  
 -	int (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc);
 -	void (*cancel_hv_timer)(struct kvm_vcpu *vcpu);
 -
  	void (*setup_mce)(struct kvm_vcpu *vcpu);
++<<<<<<< HEAD
++=======
+ 
+ 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
+ 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
+ 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
+ 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
++>>>>>>> cc3d967f7e32 (KVM: SVM: detect opening of SMI window using STGI intercept)
  };
  
  struct kvm_arch_async_pf {
diff --cc arch/x86/kvm/svm.c
index 5d58287b60ba,b71daed3cca2..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -3054,8 -3183,17 +3054,18 @@@ static int stgi_interception(struct vcp
  	if (nested_svm_check_permissions(svm))
  		return 1;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * If VGIF is enabled, the STGI intercept is only added to
+ 	 * detect the opening of the SMI/NMI window; remove it now.
+ 	 */
+ 	if (vgif_enabled(svm))
+ 		clr_intercept(svm, INTERCEPT_STGI);
+ 
++>>>>>>> cc3d967f7e32 (KVM: SVM: detect opening of SMI window using STGI intercept)
  	svm->next_rip = kvm_rip_read(&svm->vcpu) + 3;
 -	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 +	skip_emulated_instruction(&svm->vcpu);
  	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
  
  	enable_gif(svm);
@@@ -5230,7 -5401,95 +5240,99 @@@ static inline void avic_post_state_rest
  	avic_handle_ldr_update(vcpu);
  }
  
++<<<<<<< HEAD
 +static struct kvm_x86_ops svm_x86_ops = {
++=======
+ static void svm_setup_mce(struct kvm_vcpu *vcpu)
+ {
+ 	/* [63:9] are reserved. */
+ 	vcpu->arch.mcg_cap &= 0x1ff;
+ }
+ 
+ static int svm_smi_allowed(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
+ 	/* Per APM Vol.2 15.22.2 "Response to SMI" */
+ 	if (!gif_set(svm))
+ 		return 0;
+ 
+ 	if (is_guest_mode(&svm->vcpu) &&
+ 	    svm->nested.intercept & (1ULL << INTERCEPT_SMI)) {
+ 		/* TODO: Might need to set exit_info_1 and exit_info_2 here */
+ 		svm->vmcb->control.exit_code = SVM_EXIT_SMI;
+ 		svm->nested.exit_required = true;
+ 		return 0;
+ 	}
+ 
+ 	return 1;
+ }
+ 
+ static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 	int ret;
+ 
+ 	if (is_guest_mode(vcpu)) {
+ 		/* FED8h - SVM Guest */
+ 		put_smstate(u64, smstate, 0x7ed8, 1);
+ 		/* FEE0h - SVM Guest VMCB Physical Address */
+ 		put_smstate(u64, smstate, 0x7ee0, svm->nested.vmcb);
+ 
+ 		svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
+ 		svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
+ 		svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
+ 
+ 		ret = nested_svm_vmexit(svm);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 	return 0;
+ }
+ 
+ static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 	struct vmcb *nested_vmcb;
+ 	struct page *page;
+ 	struct {
+ 		u64 guest;
+ 		u64 vmcb;
+ 	} svm_state_save;
+ 	int ret;
+ 
+ 	ret = kvm_vcpu_read_guest(vcpu, smbase + 0xfed8, &svm_state_save,
+ 				  sizeof(svm_state_save));
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (svm_state_save.guest) {
+ 		vcpu->arch.hflags &= ~HF_SMM_MASK;
+ 		nested_vmcb = nested_svm_map(svm, svm_state_save.vmcb, &page);
+ 		if (nested_vmcb)
+ 			enter_svm_guest_mode(svm, svm_state_save.vmcb, nested_vmcb, page);
+ 		else
+ 			ret = 1;
+ 		vcpu->arch.hflags |= HF_SMM_MASK;
+ 	}
+ 	return ret;
+ }
+ 
+ static int enable_smi_window(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
+ 	if (!gif_set(svm)) {
+ 		if (vgif_enabled(svm))
+ 			set_intercept(svm, INTERCEPT_STGI);
+ 		/* STGI will cause a vm exit */
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
++>>>>>>> cc3d967f7e32 (KVM: SVM: detect opening of SMI window using STGI intercept)
  	.cpu_has_kvm_support = has_svm,
  	.disabled_by_bios = is_disabled,
  	.hardware_setup = svm_hardware_setup,
@@@ -5344,6 -5598,12 +5446,15 @@@
  	.pmu_ops = &amd_pmu_ops,
  	.deliver_posted_interrupt = svm_deliver_avic_intr,
  	.update_pi_irte = svm_update_pi_irte,
++<<<<<<< HEAD
++=======
+ 	.setup_mce = svm_setup_mce,
+ 
+ 	.smi_allowed = svm_smi_allowed,
+ 	.pre_enter_smm = svm_pre_enter_smm,
+ 	.pre_leave_smm = svm_pre_leave_smm,
+ 	.enable_smi_window = enable_smi_window,
++>>>>>>> cc3d967f7e32 (KVM: SVM: detect opening of SMI window using STGI intercept)
  };
  
  static int __init svm_init(void)
diff --cc arch/x86/kvm/vmx.c
index 92bd20f62d5c,69d45734091f..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -11016,7 -11930,55 +11016,59 @@@ static void vmx_setup_mce(struct kvm_vc
  			~FEATURE_CONTROL_LMCE;
  }
  
++<<<<<<< HEAD
 +static struct kvm_x86_ops vmx_x86_ops = {
++=======
+ static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
+ {
+ 	/* we need a nested vmexit to enter SMM, postpone if run is pending */
+ 	if (to_vmx(vcpu)->nested.nested_run_pending)
+ 		return 0;
+ 	return 1;
+ }
+ 
+ static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 
+ 	vmx->nested.smm.guest_mode = is_guest_mode(vcpu);
+ 	if (vmx->nested.smm.guest_mode)
+ 		nested_vmx_vmexit(vcpu, -1, 0, 0);
+ 
+ 	vmx->nested.smm.vmxon = vmx->nested.vmxon;
+ 	vmx->nested.vmxon = false;
+ 	return 0;
+ }
+ 
+ static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 	int ret;
+ 
+ 	if (vmx->nested.smm.vmxon) {
+ 		vmx->nested.vmxon = true;
+ 		vmx->nested.smm.vmxon = false;
+ 	}
+ 
+ 	if (vmx->nested.smm.guest_mode) {
+ 		vcpu->arch.hflags &= ~HF_SMM_MASK;
+ 		ret = enter_vmx_non_root_mode(vcpu, false);
+ 		vcpu->arch.hflags |= HF_SMM_MASK;
+ 		if (ret)
+ 			return ret;
+ 
+ 		vmx->nested.smm.guest_mode = false;
+ 	}
+ 	return 0;
+ }
+ 
+ static int enable_smi_window(struct kvm_vcpu *vcpu)
+ {
+ 	return 0;
+ }
+ 
+ static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
++>>>>>>> cc3d967f7e32 (KVM: SVM: detect opening of SMI window using STGI intercept)
  	.cpu_has_kvm_support = cpu_has_kvm_support,
  	.disabled_by_bios = vmx_disabled_by_bios,
  	.hardware_setup = hardware_setup,
@@@ -11138,7 -12097,17 +11190,15 @@@
  
  	.update_pi_irte = vmx_update_pi_irte,
  
 -#ifdef CONFIG_X86_64
 -	.set_hv_timer = vmx_set_hv_timer,
 -	.cancel_hv_timer = vmx_cancel_hv_timer,
 -#endif
 -
  	.setup_mce = vmx_setup_mce,
++<<<<<<< HEAD
++=======
+ 
+ 	.smi_allowed = vmx_smi_allowed,
+ 	.pre_enter_smm = vmx_pre_enter_smm,
+ 	.pre_leave_smm = vmx_pre_leave_smm,
+ 	.enable_smi_window = enable_smi_window,
++>>>>>>> cc3d967f7e32 (KVM: SVM: detect opening of SMI window using STGI intercept)
  };
  
  static int __init vmx_init(void)
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 99e230533b87..b3f436b039a0 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -6638,17 +6638,23 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (inject_pending_event(vcpu, req_int_win) != 0)
 			req_immediate_exit = true;
 		else {
-			/* Enable NMI/IRQ window open exits if needed.
+			/* Enable SMI/NMI/IRQ window open exits if needed.
 			 *
-			 * SMIs have two cases: 1) they can be nested, and
-			 * then there is nothing to do here because RSM will
-			 * cause a vmexit anyway; 2) or the SMI can be pending
-			 * because inject_pending_event has completed the
-			 * injection of an IRQ or NMI from the previous vmexit,
-			 * and then we request an immediate exit to inject the SMI.
+			 * SMIs have three cases:
+			 * 1) They can be nested, and then there is nothing to
+			 *    do here because RSM will cause a vmexit anyway.
+			 * 2) There is an ISA-specific reason why SMI cannot be
+			 *    injected, and the moment when this changes can be
+			 *    intercepted.
+			 * 3) Or the SMI can be pending because
+			 *    inject_pending_event has completed the injection
+			 *    of an IRQ or NMI from the previous vmexit, and
+			 *    then we request an immediate exit to inject the
+			 *    SMI.
 			 */
 			if (vcpu->arch.smi_pending && !is_smm(vcpu))
-				req_immediate_exit = true;
+				if (!kvm_x86_ops->enable_smi_window(vcpu))
+					req_immediate_exit = true;
 			if (vcpu->arch.nmi_pending)
 				kvm_x86_ops->enable_nmi_window(vcpu);
 			if (kvm_cpu_has_injectable_intr(vcpu) || req_int_win)
