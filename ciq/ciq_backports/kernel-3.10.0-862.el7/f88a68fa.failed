x86/mm: Extend early_memremap() support with additional attrs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] mm: Extend early_memremap() support with additional attrs (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 96.61%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit f88a68facd9a15b94f8c195d9d2c0b30c76c595a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f88a68fa.failed

Add early_memremap() support to be able to specify encrypted and
decrypted mappings with and without write-protection. The use of
write-protection is necessary when encrypting data "in place". The
write-protect attribute is considered cacheable for loads, but not
stores. This implies that the hardware will never give the core a
dirty line with this memtype.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brijesh Singh <brijesh.singh@amd.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Larry Woodman <lwoodman@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Toshimitsu Kani <toshi.kani@hpe.com>
	Cc: kasan-dev@googlegroups.com
	Cc: kvm@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
	Cc: linux-doc@vger.kernel.org
	Cc: linux-efi@vger.kernel.org
	Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/479b5832c30fae3efa7932e48f81794e86397229.1500319216.git.thomas.lendacky@amd.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit f88a68facd9a15b94f8c195d9d2c0b30c76c595a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/fixmap.h
#	arch/x86/include/asm/pgtable_types.h
#	arch/x86/mm/ioremap.c
#	include/asm-generic/early_ioremap.h
#	mm/early_ioremap.c
diff --cc arch/x86/include/asm/fixmap.h
index 9d7d36c82fc2,dcd9fb55e679..000000000000
--- a/arch/x86/include/asm/fixmap.h
+++ b/arch/x86/include/asm/fixmap.h
@@@ -175,64 -157,33 +175,81 @@@ static inline void __set_fixmap(enum fi
  }
  #endif
  
 +#define set_fixmap(idx, phys)				\
 +	__set_fixmap(idx, phys, PAGE_KERNEL)
 +
  /*
 - * FIXMAP_PAGE_NOCACHE is used for MMIO. Memory encryption is not
 - * supported for MMIO addresses, so make sure that the memory encryption
 - * mask is not part of the page attributes.
 + * Some hardware wants to get fixmapped without caching.
   */
 -#define FIXMAP_PAGE_NOCACHE PAGE_KERNEL_IO_NOCACHE
 +#define set_fixmap_nocache(idx, phys)			\
 +	__set_fixmap(idx, phys, PAGE_KERNEL_NOCACHE)
  
++<<<<<<< HEAD
 +#define clear_fixmap(idx)			\
 +	__set_fixmap(idx, 0, __pgprot(0))
++=======
+ /*
+  * Early memremap routines used for in-place encryption. The mappings created
+  * by these routines are intended to be used as temporary mappings.
+  */
+ void __init *early_memremap_encrypted(resource_size_t phys_addr,
+ 				      unsigned long size);
+ void __init *early_memremap_encrypted_wp(resource_size_t phys_addr,
+ 					 unsigned long size);
+ void __init *early_memremap_decrypted(resource_size_t phys_addr,
+ 				      unsigned long size);
+ void __init *early_memremap_decrypted_wp(resource_size_t phys_addr,
+ 					 unsigned long size);
+ 
+ #include <asm-generic/fixmap.h>
++>>>>>>> f88a68facd9a (x86/mm: Extend early_memremap() support with additional attrs)
 +
 +#define __fix_to_virt(x)	(FIXADDR_TOP - ((x) << PAGE_SHIFT))
 +#define __virt_to_fix(x)	((FIXADDR_TOP - ((x)&PAGE_MASK)) >> PAGE_SHIFT)
 +
 +extern void __this_fixmap_does_not_exist(void);
 +
 +/*
 + * 'index to address' translation. If anyone tries to use the idx
 + * directly without translation, we catch the bug with a NULL-deference
 + * kernel oops. Illegal ranges of incoming indices are caught too.
 + */
 +static __always_inline unsigned long fix_to_virt(const unsigned int idx)
 +{
 +	/*
 +	 * this branch gets completely eliminated after inlining,
 +	 * except when someone tries to use fixaddr indices in an
 +	 * illegal way. (such as mixing up address types or using
 +	 * out-of-range indices).
 +	 *
 +	 * If it doesn't get removed, the linker will complain
 +	 * loudly with a reasonably clear error message..
 +	 */
 +	if (idx >= __end_of_fixed_addresses)
 +		__this_fixmap_does_not_exist();
 +
 +	return __fix_to_virt(idx);
 +}
 +
 +static inline unsigned long virt_to_fix(const unsigned long vaddr)
 +{
 +	BUG_ON(vaddr >= FIXADDR_TOP || vaddr < FIXADDR_START);
 +	return __virt_to_fix(vaddr);
 +}
 +
 +/* Return an pointer with offset calculated */
 +static __always_inline unsigned long
 +__set_fixmap_offset(enum fixed_addresses idx, phys_addr_t phys, pgprot_t flags)
 +{
 +	__set_fixmap(idx, phys, flags);
 +	return fix_to_virt(idx) + (phys & (PAGE_SIZE - 1));
 +}
  
 -#define __late_set_fixmap(idx, phys, flags) __set_fixmap(idx, phys, flags)
 -#define __late_clear_fixmap(idx) __set_fixmap(idx, 0, __pgprot(0))
 +#define set_fixmap_offset(idx, phys)			\
 +	__set_fixmap_offset(idx, phys, PAGE_KERNEL)
  
 -void __early_set_fixmap(enum fixed_addresses idx,
 -			phys_addr_t phys, pgprot_t flags);
 +#define set_fixmap_offset_nocache(idx, phys)			\
 +	__set_fixmap_offset(idx, phys, PAGE_KERNEL_NOCACHE)
  
  #endif /* !__ASSEMBLY__ */
  #endif /* _ASM_X86_FIXMAP_H */
diff --cc arch/x86/include/asm/pgtable_types.h
index 5e5b6dc9e568,32095af0fefb..000000000000
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@@ -220,41 -185,45 +221,72 @@@ enum page_cache_mode 
  
  #define __PAGE_KERNEL_RO		(__PAGE_KERNEL & ~_PAGE_RW)
  #define __PAGE_KERNEL_RX		(__PAGE_KERNEL_EXEC & ~_PAGE_RW)
 -#define __PAGE_KERNEL_NOCACHE		(__PAGE_KERNEL | _PAGE_NOCACHE)
 +#define __PAGE_KERNEL_EXEC_NOCACHE	(__PAGE_KERNEL_EXEC | _PAGE_PCD | _PAGE_PWT)
 +#define __PAGE_KERNEL_WC		(__PAGE_KERNEL | _PAGE_CACHE_WC)
 +#define __PAGE_KERNEL_NOCACHE		(__PAGE_KERNEL | _PAGE_PCD | _PAGE_PWT)
 +#define __PAGE_KERNEL_UC_MINUS		(__PAGE_KERNEL | _PAGE_PCD)
  #define __PAGE_KERNEL_VSYSCALL		(__PAGE_KERNEL_RX | _PAGE_USER)
  #define __PAGE_KERNEL_VVAR		(__PAGE_KERNEL_RO | _PAGE_USER)
 +#define __PAGE_KERNEL_VVAR_NOCACHE	(__PAGE_KERNEL_VVAR | _PAGE_PCD | _PAGE_PWT)
  #define __PAGE_KERNEL_LARGE		(__PAGE_KERNEL | _PAGE_PSE)
 +#define __PAGE_KERNEL_LARGE_NOCACHE	(__PAGE_KERNEL | _PAGE_CACHE_UC | _PAGE_PSE)
  #define __PAGE_KERNEL_LARGE_EXEC	(__PAGE_KERNEL_EXEC | _PAGE_PSE)
+ #define __PAGE_KERNEL_WP		(__PAGE_KERNEL | _PAGE_CACHE_WP)
  
 -#define __PAGE_KERNEL_IO		(__PAGE_KERNEL)
 -#define __PAGE_KERNEL_IO_NOCACHE	(__PAGE_KERNEL_NOCACHE)
 -
 -#ifndef __ASSEMBLY__
 -
 +#define __PAGE_KERNEL_IO		(__PAGE_KERNEL | _PAGE_IOMAP)
 +#define __PAGE_KERNEL_IO_NOCACHE	(__PAGE_KERNEL_NOCACHE | _PAGE_IOMAP)
 +#define __PAGE_KERNEL_IO_UC_MINUS	(__PAGE_KERNEL_UC_MINUS | _PAGE_IOMAP)
 +#define __PAGE_KERNEL_IO_WC		(__PAGE_KERNEL_WC | _PAGE_IOMAP)
 +
 +#define PAGE_KERNEL			__pgprot(__PAGE_KERNEL)
 +#define PAGE_KERNEL_RO			__pgprot(__PAGE_KERNEL_RO)
 +#define PAGE_KERNEL_EXEC		__pgprot(__PAGE_KERNEL_EXEC)
 +#define PAGE_KERNEL_RX			__pgprot(__PAGE_KERNEL_RX)
 +#define PAGE_KERNEL_WC			__pgprot(__PAGE_KERNEL_WC)
 +#define PAGE_KERNEL_NOCACHE		__pgprot(__PAGE_KERNEL_NOCACHE)
 +#define PAGE_KERNEL_UC_MINUS		__pgprot(__PAGE_KERNEL_UC_MINUS)
 +#define PAGE_KERNEL_EXEC_NOCACHE	__pgprot(__PAGE_KERNEL_EXEC_NOCACHE)
 +#define PAGE_KERNEL_LARGE		__pgprot(__PAGE_KERNEL_LARGE)
 +#define PAGE_KERNEL_LARGE_NOCACHE	__pgprot(__PAGE_KERNEL_LARGE_NOCACHE)
 +#define PAGE_KERNEL_LARGE_EXEC		__pgprot(__PAGE_KERNEL_LARGE_EXEC)
 +#define PAGE_KERNEL_VSYSCALL		__pgprot(__PAGE_KERNEL_VSYSCALL)
 +#define PAGE_KERNEL_VVAR		__pgprot(__PAGE_KERNEL_VVAR)
 +#define PAGE_KERNEL_VVAR_NOCACHE	__pgprot(__PAGE_KERNEL_VVAR_NOCACHE)
 +
++<<<<<<< HEAD
 +#define PAGE_KERNEL_IO			__pgprot(__PAGE_KERNEL_IO)
 +#define PAGE_KERNEL_IO_NOCACHE		__pgprot(__PAGE_KERNEL_IO_NOCACHE)
 +#define PAGE_KERNEL_IO_UC_MINUS		__pgprot(__PAGE_KERNEL_IO_UC_MINUS)
 +#define PAGE_KERNEL_IO_WC		__pgprot(__PAGE_KERNEL_IO_WC)
++=======
+ #define _PAGE_ENC	(_AT(pteval_t, sme_me_mask))
+ 
+ #define _PAGE_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |	\
+ 			 _PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_ENC)
+ #define _KERNPG_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED |	\
+ 			 _PAGE_DIRTY | _PAGE_ENC)
+ 
+ #define __PAGE_KERNEL_ENC	(__PAGE_KERNEL | _PAGE_ENC)
+ #define __PAGE_KERNEL_ENC_WP	(__PAGE_KERNEL_WP | _PAGE_ENC)
+ 
+ #define __PAGE_KERNEL_NOENC	(__PAGE_KERNEL)
+ #define __PAGE_KERNEL_NOENC_WP	(__PAGE_KERNEL_WP)
+ 
+ #define PAGE_KERNEL		__pgprot(__PAGE_KERNEL | _PAGE_ENC)
+ #define PAGE_KERNEL_RO		__pgprot(__PAGE_KERNEL_RO | _PAGE_ENC)
+ #define PAGE_KERNEL_EXEC	__pgprot(__PAGE_KERNEL_EXEC | _PAGE_ENC)
+ #define PAGE_KERNEL_RX		__pgprot(__PAGE_KERNEL_RX | _PAGE_ENC)
+ #define PAGE_KERNEL_NOCACHE	__pgprot(__PAGE_KERNEL_NOCACHE | _PAGE_ENC)
+ #define PAGE_KERNEL_LARGE	__pgprot(__PAGE_KERNEL_LARGE | _PAGE_ENC)
+ #define PAGE_KERNEL_LARGE_EXEC	__pgprot(__PAGE_KERNEL_LARGE_EXEC | _PAGE_ENC)
+ #define PAGE_KERNEL_VSYSCALL	__pgprot(__PAGE_KERNEL_VSYSCALL | _PAGE_ENC)
+ #define PAGE_KERNEL_VVAR	__pgprot(__PAGE_KERNEL_VVAR | _PAGE_ENC)
+ 
+ #define PAGE_KERNEL_IO		__pgprot(__PAGE_KERNEL_IO)
+ #define PAGE_KERNEL_IO_NOCACHE	__pgprot(__PAGE_KERNEL_IO_NOCACHE)
+ 
+ #endif	/* __ASSEMBLY__ */
++>>>>>>> f88a68facd9a (x86/mm: Extend early_memremap() support with additional attrs)
  
  /*         xwr */
  #define __P000	PAGE_NONE
diff --cc arch/x86/mm/ioremap.c
index bb79b07b43a9,570201bbf442..000000000000
--- a/arch/x86/mm/ioremap.c
+++ b/arch/x86/mm/ioremap.c
@@@ -361,20 -415,52 +361,67 @@@ void unxlate_dev_mem_ptr(unsigned long 
  		return;
  
  	iounmap((void __iomem *)((unsigned long)addr & PAGE_MASK));
 +	return;
 +}
 +
++<<<<<<< HEAD
 +static int __initdata early_ioremap_debug;
 +
 +static int __init early_ioremap_debug_setup(char *str)
 +{
 +	early_ioremap_debug = 1;
 +
 +	return 0;
  }
 +early_param("early_ioremap_debug", early_ioremap_debug_setup);
  
 +static __initdata int after_paging_init;
++=======
+ #ifdef CONFIG_ARCH_USE_MEMREMAP_PROT
+ /* Remap memory with encryption */
+ void __init *early_memremap_encrypted(resource_size_t phys_addr,
+ 				      unsigned long size)
+ {
+ 	return early_memremap_prot(phys_addr, size, __PAGE_KERNEL_ENC);
+ }
+ 
+ /*
+  * Remap memory with encryption and write-protected - cannot be called
+  * before pat_init() is called
+  */
+ void __init *early_memremap_encrypted_wp(resource_size_t phys_addr,
+ 					 unsigned long size)
+ {
+ 	/* Be sure the write-protect PAT entry is set for write-protect */
+ 	if (__pte2cachemode_tbl[_PAGE_CACHE_MODE_WP] != _PAGE_CACHE_MODE_WP)
+ 		return NULL;
+ 
+ 	return early_memremap_prot(phys_addr, size, __PAGE_KERNEL_ENC_WP);
+ }
+ 
+ /* Remap memory without encryption */
+ void __init *early_memremap_decrypted(resource_size_t phys_addr,
+ 				      unsigned long size)
+ {
+ 	return early_memremap_prot(phys_addr, size, __PAGE_KERNEL_NOENC);
+ }
+ 
+ /*
+  * Remap memory without encryption and write-protected - cannot be called
+  * before pat_init() is called
+  */
+ void __init *early_memremap_decrypted_wp(resource_size_t phys_addr,
+ 					 unsigned long size)
+ {
+ 	/* Be sure the write-protect PAT entry is set for write-protect */
+ 	if (__pte2cachemode_tbl[_PAGE_CACHE_MODE_WP] != _PAGE_CACHE_MODE_WP)
+ 		return NULL;
+ 
+ 	return early_memremap_prot(phys_addr, size, __PAGE_KERNEL_NOENC_WP);
+ }
+ #endif	/* CONFIG_ARCH_USE_MEMREMAP_PROT */
+ 
++>>>>>>> f88a68facd9a (x86/mm: Extend early_memremap() support with additional attrs)
  static pte_t bm_pte[PAGE_SIZE/sizeof(pte_t)] __page_aligned_bss;
  
  static inline pmd_t * __init early_ioremap_pmd(unsigned long addr)
* Unmerged path include/asm-generic/early_ioremap.h
* Unmerged path mm/early_ioremap.c
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 0912e61d8b57..b4573d7b2797 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1305,6 +1305,10 @@ config TRACK_DIRTY_PAGES
 	  live copying of memory and system state to another system.
 	  Most users will say n here.
 
+config ARCH_USE_MEMREMAP_PROT
+	def_bool y
+	depends on AMD_MEM_ENCRYPT
+
 # Common NUMA Features
 config NUMA
 	bool "Numa Memory Allocation and Scheduler Support"
* Unmerged path arch/x86/include/asm/fixmap.h
* Unmerged path arch/x86/include/asm/pgtable_types.h
* Unmerged path arch/x86/mm/ioremap.c
* Unmerged path include/asm-generic/early_ioremap.h
* Unmerged path mm/early_ioremap.c
