nvmet_fc: add defer_req callback for deferment of cmd buffer return

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [nvme] fc: add defer_req callback for deferment of cmd buffer return (David Milburn) [1457880 1456486 1454365]
Rebuild_FUZZ: 95.31%
commit-author James Smart <jsmart2021@gmail.com>
commit 0fb228d30b8d72bfee51f57e638d412324d44a11
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/0fb228d3.failed

At queue creation, the transport allocates a local job struct
(struct nvmet_fc_fcp_iod) for each possible element of the queue.
When a new CMD is received from the wire, a jobs struct is allocated
from the queue and then used for the duration of the command.
The job struct contains buffer space for the wire command iu. Thus,
upon allocation of the job struct, the cmd iu buffer is copied to
the job struct and the LLDD may immediately free/reuse the CMD IU
buffer passed in the call.

However, in some circumstances, due to the packetized nature of FC
and the api of the FC LLDD which may issue a hw command to send the
wire response, but the LLDD may not get the hw completion for the
command and upcall the nvmet_fc layer before a new command may be
asynchronously received on the wire. In other words, its possible
for the initiator to get the response from the wire, thus believe a
command slot free, and send a new command iu. The new command iu
may be received by the LLDD and passed to the transport before the
LLDD had serviced the hw completion and made the teardown calls for
the original job struct. As such, there is no available job struct
available for the new io. E.g. it appears like the host sent more
queue elements than the queue size. It didn't based on it's
understanding.

Rather than treat this as a hard connection failure queue the new
request until the job struct does free up. As the buffer isn't
copied as there's no job struct, a special return value must be
returned to the LLDD to signify to hold off on recycling the cmd
iu buffer.  And later, when a job struct is allocated and the
buffer copied, a new LLDD callback is introduced to notify the
LLDD and allow it to recycle it's command iu buffer.

	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 0fb228d30b8d72bfee51f57e638d412324d44a11)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/target/fc.c
#	include/linux/nvme-fc-driver.h
diff --cc drivers/nvme/target/fc.c
index 58773994b97a,1b7f2520a20d..000000000000
--- a/drivers/nvme/target/fc.c
+++ b/drivers/nvme/target/fc.c
@@@ -479,17 -514,66 +507,80 @@@ static voi
  nvmet_fc_free_fcp_iod(struct nvmet_fc_tgt_queue *queue,
  			struct nvmet_fc_fcp_iod *fod)
  {
++<<<<<<< HEAD
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&queue->qlock, flags);
 +	list_add_tail(&fod->fcp_list, &fod->queue->fod_list);
 +	fod->active = false;
 +	spin_unlock_irqrestore(&queue->qlock, flags);
 +
 +	/*
 +	 * release the reference taken at queue lookup and fod allocation
 +	 */
 +	nvmet_fc_tgt_q_put(queue);
++=======
+ 	struct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;
+ 	struct nvmet_fc_tgtport *tgtport = fod->tgtport;
+ 	struct nvmet_fc_defer_fcp_req *deferfcp;
+ 	unsigned long flags;
+ 
+ 	fc_dma_sync_single_for_cpu(tgtport->dev, fod->rspdma,
+ 				sizeof(fod->rspiubuf), DMA_TO_DEVICE);
+ 
+ 	fcpreq->nvmet_fc_private = NULL;
+ 
+ 	fod->active = false;
+ 	fod->abort = false;
+ 	fod->aborted = false;
+ 	fod->writedataactive = false;
+ 	fod->fcpreq = NULL;
+ 
+ 	tgtport->ops->fcp_req_release(&tgtport->fc_target_port, fcpreq);
+ 
+ 	spin_lock_irqsave(&queue->qlock, flags);
+ 	deferfcp = list_first_entry_or_null(&queue->pending_cmd_list,
+ 				struct nvmet_fc_defer_fcp_req, req_list);
+ 	if (!deferfcp) {
+ 		list_add_tail(&fod->fcp_list, &fod->queue->fod_list);
+ 		spin_unlock_irqrestore(&queue->qlock, flags);
+ 
+ 		/* Release reference taken at queue lookup and fod allocation */
+ 		nvmet_fc_tgt_q_put(queue);
+ 		return;
+ 	}
+ 
+ 	/* Re-use the fod for the next pending cmd that was deferred */
+ 	list_del(&deferfcp->req_list);
+ 
+ 	fcpreq = deferfcp->fcp_req;
+ 
+ 	/* deferfcp can be reused for another IO at a later date */
+ 	list_add_tail(&deferfcp->req_list, &queue->avail_defer_list);
+ 
+ 	spin_unlock_irqrestore(&queue->qlock, flags);
+ 
+ 	/* Save NVME CMD IO in fod */
+ 	memcpy(&fod->cmdiubuf, fcpreq->rspaddr, fcpreq->rsplen);
+ 
+ 	/* Setup new fcpreq to be processed */
+ 	fcpreq->rspaddr = NULL;
+ 	fcpreq->rsplen  = 0;
+ 	fcpreq->nvmet_fc_private = fod;
+ 	fod->fcpreq = fcpreq;
+ 	fod->active = true;
+ 
+ 	/* inform LLDD IO is now being processed */
+ 	tgtport->ops->defer_rcv(&tgtport->fc_target_port, fcpreq);
+ 
+ 	/* Submit deferred IO for processing */
+ 	nvmet_fc_queue_fcp_req(tgtport, queue, fcpreq);
+ 
+ 	/*
+ 	 * Leave the queue lookup get reference taken when
+ 	 * fod was originally allocated.
+ 	 */
++>>>>>>> 0fb228d30b8d (nvmet_fc: add defer_req callback for deferment of cmd buffer return)
  }
  
  static int
@@@ -639,9 -702,11 +732,10 @@@ nvmet_fc_abort_op(struct nvmet_fc_tgtpo
  static void
  nvmet_fc_delete_target_queue(struct nvmet_fc_tgt_queue *queue)
  {
 -	struct nvmet_fc_tgtport *tgtport = queue->assoc->tgtport;
  	struct nvmet_fc_fcp_iod *fod = queue->fod;
+ 	struct nvmet_fc_defer_fcp_req *deferfcp;
  	unsigned long flags;
 -	int i, writedataactive;
 +	int i;
  	bool disconnect;
  
  	disconnect = atomic_xchg(&queue->connected, 0);
@@@ -652,9 -717,51 +746,38 @@@
  		if (fod->active) {
  			spin_lock(&fod->flock);
  			fod->abort = true;
 -			writedataactive = fod->writedataactive;
  			spin_unlock(&fod->flock);
 -			/*
 -			 * only call lldd abort routine if waiting for
 -			 * writedata. other outstanding ops should finish
 -			 * on their own.
 -			 */
 -			if (writedataactive) {
 -				spin_lock(&fod->flock);
 -				fod->aborted = true;
 -				spin_unlock(&fod->flock);
 -				tgtport->ops->fcp_abort(
 -					&tgtport->fc_target_port, fod->fcpreq);
 -			}
  		}
  	}
+ 
+ 	/* Cleanup defer'ed IOs in queue */
+ 	list_for_each_entry(deferfcp, &queue->avail_defer_list, req_list) {
+ 		list_del(&deferfcp->req_list);
+ 		kfree(deferfcp);
+ 	}
+ 
+ 	for (;;) {
+ 		deferfcp = list_first_entry_or_null(&queue->pending_cmd_list,
+ 				struct nvmet_fc_defer_fcp_req, req_list);
+ 		if (!deferfcp)
+ 			break;
+ 
+ 		list_del(&deferfcp->req_list);
+ 		spin_unlock_irqrestore(&queue->qlock, flags);
+ 
+ 		tgtport->ops->defer_rcv(&tgtport->fc_target_port,
+ 				deferfcp->fcp_req);
+ 
+ 		tgtport->ops->fcp_abort(&tgtport->fc_target_port,
+ 				deferfcp->fcp_req);
+ 
+ 		tgtport->ops->fcp_req_release(&tgtport->fc_target_port,
+ 				deferfcp->fcp_req);
+ 
+ 		kfree(deferfcp);
+ 
+ 		spin_lock_irqsave(&queue->qlock, flags);
+ 	}
  	spin_unlock_irqrestore(&queue->qlock, flags);
  
  	flush_workqueue(queue->work_q);
@@@ -2090,14 -2268,41 +2213,41 @@@ nvmet_fc_handle_fcp_rqst_work(struct wo
   * Pass a FC-NVME FCP CMD IU received from the FC link to the nvmet-fc
   * layer for processing.
   *
-  * The nvmet-fc layer will copy cmd payload to an internal structure for
-  * processing.  As such, upon completion of the routine, the LLDD may
-  * immediately free/reuse the CMD IU buffer passed in the call.
+  * The nvmet_fc layer allocates a local job structure (struct
+  * nvmet_fc_fcp_iod) from the queue for the io and copies the
+  * CMD IU buffer to the job structure. As such, on a successful
+  * completion (returns 0), the LLDD may immediately free/reuse
+  * the CMD IU buffer passed in the call.
   *
-  * If this routine returns error, the lldd should abort the exchange.
+  * However, in some circumstances, due to the packetized nature of FC
+  * and the api of the FC LLDD which may issue a hw command to send the
+  * response, but the LLDD may not get the hw completion for that command
+  * and upcall the nvmet_fc layer before a new command may be
+  * asynchronously received - its possible for a command to be received
+  * before the LLDD and nvmet_fc have recycled the job structure. It gives
+  * the appearance of more commands received than fits in the sq.
+  * To alleviate this scenario, a temporary queue is maintained in the
+  * transport for pending LLDD requests waiting for a queue job structure.
+  * In these "overrun" cases, a temporary queue element is allocated
+  * the LLDD request and CMD iu buffer information remembered, and the
+  * routine returns a -EOVERFLOW status. Subsequently, when a queue job
+  * structure is freed, it is immediately reallocated for anything on the
+  * pending request list. The LLDDs defer_rcv() callback is called,
+  * informing the LLDD that it may reuse the CMD IU buffer, and the io
+  * is then started normally with the transport.
+  *
+  * The LLDD, when receiving an -EOVERFLOW completion status, is to treat
+  * the completion as successful but must not reuse the CMD IU buffer
+  * until the LLDD's defer_rcv() callback has been called for the
+  * corresponding struct nvmefc_tgt_fcp_req pointer.
+  *
+  * If there is any other condition in which an error occurs, the
+  * transport will return a non-zero status indicating the error.
+  * In all cases other than -EOVERFLOW, the transport has not accepted the
+  * request and the LLDD should abort the exchange.
   *
   * @target_port: pointer to the (registered) target port the FCP CMD IU
 - *              was received on.
 + *              was receive on.
   * @fcpreq:     pointer to a fcpreq request structure to be used to reference
   *              the exchange corresponding to the FCP Exchange.
   * @cmdiubuf:   pointer to the buffer containing the FCP CMD IU
@@@ -2140,19 -2362,37 +2308,41 @@@ nvmet_fc_rcv_fcp_req(struct nvmet_fc_ta
  		return -ENOENT;
  	}
  
- 	fcpreq->nvmet_fc_private = fod;
- 	fod->fcpreq = fcpreq;
- 	/*
- 	 * put all admin cmds on hw queue id 0. All io commands go to
- 	 * the respective hw queue based on a modulo basis
- 	 */
- 	fcpreq->hwqid = queue->qid ?
- 			((queue->qid - 1) % tgtport->ops->max_hw_queues) : 0;
- 	memcpy(&fod->cmdiubuf, cmdiubuf, cmdiubuf_len);
+ 	deferfcp = list_first_entry_or_null(&queue->avail_defer_list,
+ 			struct nvmet_fc_defer_fcp_req, req_list);
+ 	if (deferfcp) {
+ 		/* Just re-use one that was previously allocated */
+ 		list_del(&deferfcp->req_list);
+ 	} else {
+ 		spin_unlock_irqrestore(&queue->qlock, flags);
  
++<<<<<<< HEAD
 +	queue_work_on(queue->cpu, queue->work_q, &fod->work);
++=======
+ 		/* Now we need to dynamically allocate one */
+ 		deferfcp = kmalloc(sizeof(*deferfcp), GFP_KERNEL);
+ 		if (!deferfcp) {
+ 			/* release the queue lookup reference */
+ 			nvmet_fc_tgt_q_put(queue);
+ 			return -ENOMEM;
+ 		}
+ 		spin_lock_irqsave(&queue->qlock, flags);
+ 	}
++>>>>>>> 0fb228d30b8d (nvmet_fc: add defer_req callback for deferment of cmd buffer return)
  
- 	return 0;
+ 	/* For now, use rspaddr / rsplen to save payload information */
+ 	fcpreq->rspaddr = cmdiubuf;
+ 	fcpreq->rsplen  = cmdiubuf_len;
+ 	deferfcp->fcp_req = fcpreq;
+ 
+ 	/* defer processing till a fod becomes available */
+ 	list_add_tail(&deferfcp->req_list, &queue->pending_cmd_list);
+ 
+ 	/* NOTE: the queue lookup reference is still valid */
+ 
+ 	spin_unlock_irqrestore(&queue->qlock, flags);
+ 
+ 	return -EOVERFLOW;
  }
  EXPORT_SYMBOL_GPL(nvmet_fc_rcv_fcp_req);
  
diff --cc include/linux/nvme-fc-driver.h
index 977e25414837,2591878c1d48..000000000000
--- a/include/linux/nvme-fc-driver.h
+++ b/include/linux/nvme-fc-driver.h
@@@ -820,7 -846,13 +825,17 @@@ struct nvmet_fc_target_template 
  	int (*xmt_ls_rsp)(struct nvmet_fc_target_port *tgtport,
  				struct nvmefc_tgt_ls_req *tls_req);
  	int (*fcp_op)(struct nvmet_fc_target_port *tgtport,
++<<<<<<< HEAD
 +				struct nvmefc_tgt_fcp_req *);
++=======
+ 				struct nvmefc_tgt_fcp_req *fcpreq);
+ 	void (*fcp_abort)(struct nvmet_fc_target_port *tgtport,
+ 				struct nvmefc_tgt_fcp_req *fcpreq);
+ 	void (*fcp_req_release)(struct nvmet_fc_target_port *tgtport,
+ 				struct nvmefc_tgt_fcp_req *fcpreq);
+ 	void (*defer_rcv)(struct nvmet_fc_target_port *tgtport,
+ 				struct nvmefc_tgt_fcp_req *fcpreq);
++>>>>>>> 0fb228d30b8d (nvmet_fc: add defer_req callback for deferment of cmd buffer return)
  
  	u32	max_hw_queues;
  	u16	max_sgl_segments;
* Unmerged path drivers/nvme/target/fc.c
* Unmerged path include/linux/nvme-fc-driver.h
