mm: fix cache mode of dax pmd mappings

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [kernel] fix cache mode of dax pmd mappings (Larry Woodman) [1457572 1457561]
Rebuild_FUZZ: 94.44%
commit-author Dan Williams <dan.j.williams@intel.com>
commit 9049771f7d5490a302589976984810064c83ab40
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/9049771f.failed

track_pfn_insert() in vmf_insert_pfn_pmd() is marking dax mappings as
uncacheable rendering them impractical for application usage.  DAX-pte
mappings are cached and the goal of establishing DAX-pmd mappings is to
attain more performance, not dramatically less (3 orders of magnitude).

track_pfn_insert() relies on a previous call to reserve_memtype() to
establish the expected page_cache_mode for the range.  While memremap()
arranges for reserve_memtype() to be called, devm_memremap_pages() does
not.  So, teach track_pfn_insert() and untrack_pfn() how to handle
tracking without a vma, and arrange for devm_memremap_pages() to
establish the write-back-cache reservation in the memtype tree.

	Cc: <stable@vger.kernel.org>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Nilesh Choudhury <nilesh.choudhury@oracle.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Reported-by: Toshi Kani <toshi.kani@hpe.com>
	Reported-by: Kai Zhang <kai.ka.zhang@oracle.com>
	Acked-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 9049771f7d5490a302589976984810064c83ab40)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/memremap.c
diff --cc kernel/memremap.c
index 6504d174d101,b501e390bb34..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -272,10 -246,8 +272,14 @@@ static void devm_memremap_pages_release
  	/* pages are dead and unused, undo the arch mapping */
  	align_start = res->start & ~(SECTION_SIZE - 1);
  	align_size = ALIGN(resource_size(res), SECTION_SIZE);
 +
 +	mem_hotplug_begin();
  	arch_remove_memory(align_start, align_size);
++<<<<<<< HEAD
 +	mem_hotplug_done();
++=======
+ 	untrack_pfn(NULL, PHYS_PFN(align_start), align_size);
++>>>>>>> 9049771f7d54 (mm: fix cache mode of dax pmd mappings)
  	pgmap_radix_release(res);
  	dev_WARN_ONCE(dev, pgmap->altmap && pgmap->altmap->alloc,
  			"%s: failed to free all reserved pages\n", __func__);
@@@ -389,9 -353,12 +394,17 @@@ void *devm_memremap_pages(struct devic
  	if (nid < 0)
  		nid = numa_mem_id();
  
++<<<<<<< HEAD
 +	mem_hotplug_begin();
++=======
+ 	error = track_pfn_remap(NULL, &pgprot, PHYS_PFN(align_start), 0,
+ 			align_size);
+ 	if (error)
+ 		goto err_pfn_remap;
+ 
++>>>>>>> 9049771f7d54 (mm: fix cache mode of dax pmd mappings)
  	error = arch_add_memory(nid, align_start, align_size, true);
 +	mem_hotplug_done();
  	if (error)
  		goto err_add_memory;
  
diff --git a/arch/x86/mm/pat.c b/arch/x86/mm/pat.c
index 59c76de65d5c..98f8228803f3 100644
--- a/arch/x86/mm/pat.c
+++ b/arch/x86/mm/pat.c
@@ -702,9 +702,10 @@ int track_pfn_copy(struct vm_area_struct *vma)
 }
 
 /*
- * prot is passed in as a parameter for the new mapping. If the vma has a
- * linear pfn mapping for the entire range reserve the entire vma range with
- * single reserve_pfn_range call.
+ * prot is passed in as a parameter for the new mapping. If the vma has
+ * a linear pfn mapping for the entire range, or no vma is provided,
+ * reserve the entire pfn + size range with single reserve_pfn_range
+ * call.
  */
 int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,
 		    unsigned long pfn, unsigned long addr, unsigned long size)
@@ -713,11 +714,12 @@ int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,
 	unsigned long flags;
 
 	/* reserve the whole chunk starting from paddr */
-	if (addr == vma->vm_start && size == (vma->vm_end - vma->vm_start)) {
+	if (!vma || (addr == vma->vm_start
+				&& size == (vma->vm_end - vma->vm_start))) {
 		int ret;
 
 		ret = reserve_pfn_range(paddr, size, prot, 0);
-		if (!ret)
+		if (ret == 0 && vma)
 			vma->vm_flags |= VM_PAT;
 		return ret;
 	}
@@ -772,7 +774,7 @@ void untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,
 	resource_size_t paddr;
 	unsigned long prot;
 
-	if (!(vma->vm_flags & VM_PAT))
+	if (vma && !(vma->vm_flags & VM_PAT))
 		return;
 
 	/* free the chunk starting from pfn or the whole chunk */
@@ -786,7 +788,8 @@ void untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,
 		size = vma->vm_end - vma->vm_start;
 	}
 	free_pfn_range(paddr, size);
-	vma->vm_flags &= ~VM_PAT;
+	if (vma)
+		vma->vm_flags &= ~VM_PAT;
 }
 
 /*
* Unmerged path kernel/memremap.c
