blk-stat: use READ and WRITE instead of BLK_STAT_{READ,WRITE}

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [block] blk-stat: use READ and WRITE instead of BLK_STAT_{READ, WRITE} (Ming Lei) [1458104]
Rebuild_FUZZ: 99.19%
commit-author Omar Sandoval <osandov@fb.com>
commit fa2e39cb9ee78f440d99a1bcfa47462c48a6fc11
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/fa2e39cb.failed

The stats buckets will become generic soon, so make the existing users
use the common READ and WRITE definitions instead of one internal to
blk-stat.

	Signed-off-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit fa2e39cb9ee78f440d99a1bcfa47462c48a6fc11)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-debugfs.c
#	block/blk-mq.c
#	block/blk-stat.c
#	block/blk-stat.h
#	block/blk-sysfs.c
#	block/blk-wbt.c
diff --cc block/blk-mq.c
index 1b06c94aa73d,559e5363bb2c..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1902,6 -2040,8 +1902,11 @@@ static void blk_mq_init_cpu_queues(stru
  		spin_lock_init(&__ctx->lock);
  		INIT_LIST_HEAD(&__ctx->rq_list);
  		__ctx->queue = q;
++<<<<<<< HEAD
++=======
+ 		blk_stat_init(&__ctx->stat[READ]);
+ 		blk_stat_init(&__ctx->stat[WRITE]);
++>>>>>>> fa2e39cb9ee7 (blk-stat: use READ and WRITE instead of BLK_STAT_{READ,WRITE})
  
  		/* If the cpu isn't online, the cpu is mapped to first hctx */
  		if (!cpu_online(i))
@@@ -2563,6 -2740,168 +2568,171 @@@ void blk_mq_update_nr_hw_queues(struct 
  }
  EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
  
++<<<<<<< HEAD
++=======
+ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
+ 				       struct blk_mq_hw_ctx *hctx,
+ 				       struct request *rq)
+ {
+ 	struct blk_rq_stat stat[2];
+ 	unsigned long ret = 0;
+ 
+ 	/*
+ 	 * If stats collection isn't on, don't sleep but turn it on for
+ 	 * future users
+ 	 */
+ 	if (!blk_stat_enable(q))
+ 		return 0;
+ 
+ 	/*
+ 	 * We don't have to do this once per IO, should optimize this
+ 	 * to just use the current window of stats until it changes
+ 	 */
+ 	memset(&stat, 0, sizeof(stat));
+ 	blk_hctx_stat_get(hctx, stat);
+ 
+ 	/*
+ 	 * As an optimistic guess, use half of the mean service time
+ 	 * for this type of request. We can (and should) make this smarter.
+ 	 * For instance, if the completion latencies are tight, we can
+ 	 * get closer than just half the mean. This is especially
+ 	 * important on devices where the completion latencies are longer
+ 	 * than ~10 usec.
+ 	 */
+ 	if (req_op(rq) == REQ_OP_READ && stat[READ].nr_samples)
+ 		ret = (stat[READ].mean + 1) / 2;
+ 	else if (req_op(rq) == REQ_OP_WRITE && stat[WRITE].nr_samples)
+ 		ret = (stat[WRITE].mean + 1) / 2;
+ 
+ 	return ret;
+ }
+ 
+ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
+ 				     struct blk_mq_hw_ctx *hctx,
+ 				     struct request *rq)
+ {
+ 	struct hrtimer_sleeper hs;
+ 	enum hrtimer_mode mode;
+ 	unsigned int nsecs;
+ 	ktime_t kt;
+ 
+ 	if (test_bit(REQ_ATOM_POLL_SLEPT, &rq->atomic_flags))
+ 		return false;
+ 
+ 	/*
+ 	 * poll_nsec can be:
+ 	 *
+ 	 * -1:	don't ever hybrid sleep
+ 	 *  0:	use half of prev avg
+ 	 * >0:	use this specific value
+ 	 */
+ 	if (q->poll_nsec == -1)
+ 		return false;
+ 	else if (q->poll_nsec > 0)
+ 		nsecs = q->poll_nsec;
+ 	else
+ 		nsecs = blk_mq_poll_nsecs(q, hctx, rq);
+ 
+ 	if (!nsecs)
+ 		return false;
+ 
+ 	set_bit(REQ_ATOM_POLL_SLEPT, &rq->atomic_flags);
+ 
+ 	/*
+ 	 * This will be replaced with the stats tracking code, using
+ 	 * 'avg_completion_time / 2' as the pre-sleep target.
+ 	 */
+ 	kt = nsecs;
+ 
+ 	mode = HRTIMER_MODE_REL;
+ 	hrtimer_init_on_stack(&hs.timer, CLOCK_MONOTONIC, mode);
+ 	hrtimer_set_expires(&hs.timer, kt);
+ 
+ 	hrtimer_init_sleeper(&hs, current);
+ 	do {
+ 		if (test_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags))
+ 			break;
+ 		set_current_state(TASK_UNINTERRUPTIBLE);
+ 		hrtimer_start_expires(&hs.timer, mode);
+ 		if (hs.task)
+ 			io_schedule();
+ 		hrtimer_cancel(&hs.timer);
+ 		mode = HRTIMER_MODE_ABS;
+ 	} while (hs.task && !signal_pending(current));
+ 
+ 	__set_current_state(TASK_RUNNING);
+ 	destroy_hrtimer_on_stack(&hs.timer);
+ 	return true;
+ }
+ 
+ static bool __blk_mq_poll(struct blk_mq_hw_ctx *hctx, struct request *rq)
+ {
+ 	struct request_queue *q = hctx->queue;
+ 	long state;
+ 
+ 	/*
+ 	 * If we sleep, have the caller restart the poll loop to reset
+ 	 * the state. Like for the other success return cases, the
+ 	 * caller is responsible for checking if the IO completed. If
+ 	 * the IO isn't complete, we'll get called again and will go
+ 	 * straight to the busy poll loop.
+ 	 */
+ 	if (blk_mq_poll_hybrid_sleep(q, hctx, rq))
+ 		return true;
+ 
+ 	hctx->poll_considered++;
+ 
+ 	state = current->state;
+ 	while (!need_resched()) {
+ 		int ret;
+ 
+ 		hctx->poll_invoked++;
+ 
+ 		ret = q->mq_ops->poll(hctx, rq->tag);
+ 		if (ret > 0) {
+ 			hctx->poll_success++;
+ 			set_current_state(TASK_RUNNING);
+ 			return true;
+ 		}
+ 
+ 		if (signal_pending_state(state, current))
+ 			set_current_state(TASK_RUNNING);
+ 
+ 		if (current->state == TASK_RUNNING)
+ 			return true;
+ 		if (ret < 0)
+ 			break;
+ 		cpu_relax();
+ 	}
+ 
+ 	return false;
+ }
+ 
+ bool blk_mq_poll(struct request_queue *q, blk_qc_t cookie)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	struct blk_plug *plug;
+ 	struct request *rq;
+ 
+ 	if (!q->mq_ops || !q->mq_ops->poll || !blk_qc_t_valid(cookie) ||
+ 	    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ 		return false;
+ 
+ 	plug = current->plug;
+ 	if (plug)
+ 		blk_flush_plug_list(plug, false);
+ 
+ 	hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
+ 	if (!blk_qc_t_is_internal(cookie))
+ 		rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
+ 	else
+ 		rq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));
+ 
+ 	return __blk_mq_poll(hctx, rq);
+ }
+ EXPORT_SYMBOL_GPL(blk_mq_poll);
+ 
++>>>>>>> fa2e39cb9ee7 (blk-stat: use READ and WRITE instead of BLK_STAT_{READ,WRITE})
  void blk_mq_disable_hotplug(void)
  {
  	mutex_lock(&all_q_mutex);
diff --cc block/blk-sysfs.c
index 91f42f273aad,fdb45fd0db0b..000000000000
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@@ -317,6 -362,167 +317,170 @@@ queue_rq_affinity_store(struct request_
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t queue_poll_delay_show(struct request_queue *q, char *page)
+ {
+ 	int val;
+ 
+ 	if (q->poll_nsec == -1)
+ 		val = -1;
+ 	else
+ 		val = q->poll_nsec / 1000;
+ 
+ 	return sprintf(page, "%d\n", val);
+ }
+ 
+ static ssize_t queue_poll_delay_store(struct request_queue *q, const char *page,
+ 				size_t count)
+ {
+ 	int err, val;
+ 
+ 	if (!q->mq_ops || !q->mq_ops->poll)
+ 		return -EINVAL;
+ 
+ 	err = kstrtoint(page, 10, &val);
+ 	if (err < 0)
+ 		return err;
+ 
+ 	if (val == -1)
+ 		q->poll_nsec = -1;
+ 	else
+ 		q->poll_nsec = val * 1000;
+ 
+ 	return count;
+ }
+ 
+ static ssize_t queue_poll_show(struct request_queue *q, char *page)
+ {
+ 	return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+ }
+ 
+ static ssize_t queue_poll_store(struct request_queue *q, const char *page,
+ 				size_t count)
+ {
+ 	unsigned long poll_on;
+ 	ssize_t ret;
+ 
+ 	if (!q->mq_ops || !q->mq_ops->poll)
+ 		return -EINVAL;
+ 
+ 	ret = queue_var_store(&poll_on, page, count);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	spin_lock_irq(q->queue_lock);
+ 	if (poll_on)
+ 		queue_flag_set(QUEUE_FLAG_POLL, q);
+ 	else
+ 		queue_flag_clear(QUEUE_FLAG_POLL, q);
+ 	spin_unlock_irq(q->queue_lock);
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t queue_wb_lat_show(struct request_queue *q, char *page)
+ {
+ 	if (!q->rq_wb)
+ 		return -EINVAL;
+ 
+ 	return sprintf(page, "%llu\n", div_u64(q->rq_wb->min_lat_nsec, 1000));
+ }
+ 
+ static ssize_t queue_wb_lat_store(struct request_queue *q, const char *page,
+ 				  size_t count)
+ {
+ 	struct rq_wb *rwb;
+ 	ssize_t ret;
+ 	s64 val;
+ 
+ 	ret = queue_var_store64(&val, page);
+ 	if (ret < 0)
+ 		return ret;
+ 	if (val < -1)
+ 		return -EINVAL;
+ 
+ 	rwb = q->rq_wb;
+ 	if (!rwb) {
+ 		ret = wbt_init(q);
+ 		if (ret)
+ 			return ret;
+ 
+ 		rwb = q->rq_wb;
+ 		if (!rwb)
+ 			return -EINVAL;
+ 	}
+ 
+ 	if (val == -1)
+ 		rwb->min_lat_nsec = wbt_default_latency_nsec(q);
+ 	else if (val >= 0)
+ 		rwb->min_lat_nsec = val * 1000ULL;
+ 
+ 	if (rwb->enable_state == WBT_STATE_ON_DEFAULT)
+ 		rwb->enable_state = WBT_STATE_ON_MANUAL;
+ 
+ 	wbt_update_limits(rwb);
+ 	return count;
+ }
+ 
+ static ssize_t queue_wc_show(struct request_queue *q, char *page)
+ {
+ 	if (test_bit(QUEUE_FLAG_WC, &q->queue_flags))
+ 		return sprintf(page, "write back\n");
+ 
+ 	return sprintf(page, "write through\n");
+ }
+ 
+ static ssize_t queue_wc_store(struct request_queue *q, const char *page,
+ 			      size_t count)
+ {
+ 	int set = -1;
+ 
+ 	if (!strncmp(page, "write back", 10))
+ 		set = 1;
+ 	else if (!strncmp(page, "write through", 13) ||
+ 		 !strncmp(page, "none", 4))
+ 		set = 0;
+ 
+ 	if (set == -1)
+ 		return -EINVAL;
+ 
+ 	spin_lock_irq(q->queue_lock);
+ 	if (set)
+ 		queue_flag_set(QUEUE_FLAG_WC, q);
+ 	else
+ 		queue_flag_clear(QUEUE_FLAG_WC, q);
+ 	spin_unlock_irq(q->queue_lock);
+ 
+ 	return count;
+ }
+ 
+ static ssize_t queue_dax_show(struct request_queue *q, char *page)
+ {
+ 	return queue_var_show(blk_queue_dax(q), page);
+ }
+ 
+ static ssize_t print_stat(char *page, struct blk_rq_stat *stat, const char *pre)
+ {
+ 	return sprintf(page, "%s samples=%llu, mean=%lld, min=%lld, max=%lld\n",
+ 			pre, (long long) stat->nr_samples,
+ 			(long long) stat->mean, (long long) stat->min,
+ 			(long long) stat->max);
+ }
+ 
+ static ssize_t queue_stats_show(struct request_queue *q, char *page)
+ {
+ 	struct blk_rq_stat stat[2];
+ 	ssize_t ret;
+ 
+ 	blk_queue_stat_get(q, stat);
+ 
+ 	ret = print_stat(page, &stat[READ], "read :");
+ 	ret += print_stat(page + ret, &stat[WRITE], "write:");
+ 	return ret;
+ }
+ 
++>>>>>>> fa2e39cb9ee7 (blk-stat: use READ and WRITE instead of BLK_STAT_{READ,WRITE})
  static struct queue_sysfs_entry queue_requests_entry = {
  	.attr = {.name = "nr_requests", .mode = S_IRUGO | S_IWUSR },
  	.show = queue_requests_show,
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path block/blk-stat.c
* Unmerged path block/blk-stat.h
* Unmerged path block/blk-wbt.c
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path block/blk-mq.c
* Unmerged path block/blk-stat.c
* Unmerged path block/blk-stat.h
* Unmerged path block/blk-sysfs.c
* Unmerged path block/blk-wbt.c
