net/mlx5e: IPSec, Add Innova IPSec offload RX data path

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: IPSec, Add Innova IPSec offload RX data path (Kamal Heib) [1456677 1456694]
Rebuild_FUZZ: 96.23%
commit-author Ilan Tayari <ilant@mellanox.com>
commit 899a59d301bc0ccd312dd23d58899cfbbc94bead
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/899a59d3.failed

In RX data path, the hardware prepends a special metadata ethertype
which indicates that the packet underwent decryption, and the result of
the authentication check.

Communicate this to the stack in skb->sp.

Make wqe_size large enough to account for the injected metadata.

Support only Linked-list RQ type.

IPSec offload RX packets may have useful CHECKSUM_COMPLETE information,
which the stack may not be able to use yet.

	Signed-off-by: Ilan Tayari <ilant@mellanox.com>
	Signed-off-by: Yossi Kuperman <yossiku@mellanox.com>
	Signed-off-by: Yevgeny Kliteynik <kliteyn@mellanox.com>
	Signed-off-by: Boris Pismenny <borisp@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 899a59d301bc0ccd312dd23d58899cfbbc94bead)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/ipath/ipath_wc_ppc64.c
#	drivers/infiniband/hw/ipath/ipsec_rxtx.c
#	drivers/infiniband/hw/ipath/ipsec_rxtx.h
#	drivers/net/ethernet/mellanox/mlx5/core/Makefile
#	drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/infiniband/hw/ipath/ipath_wc_ppc64.c
index 1d7bd82a1fb1,4d745d3dd4b1..000000000000
--- a/drivers/infiniband/hw/ipath/ipath_wc_ppc64.c
+++ b/drivers/infiniband/hw/ipath/ipath_wc_ppc64.c
@@@ -28,22 -28,48 +28,59 @@@
   * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
   * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
   * SOFTWARE.
 - *
   */
  
 -#ifndef __MLX5E_IPSEC_H__
 -#define __MLX5E_IPSEC_H__
 +/*
 + * This file is conditionally built on PowerPC only.  Otherwise weak symbol
 + * versions of the functions exported from here are used.
 + */
  
 -#ifdef CONFIG_MLX5_EN_IPSEC
 +#include "ipath_kernel.h"
  
++<<<<<<< HEAD:drivers/infiniband/hw/ipath/ipath_wc_ppc64.c
 +/**
 + * ipath_enable_wc - enable write combining for MMIO writes to the device
 + * @dd: infinipath device
 + *
 + * Nothing to do on PowerPC, so just return without error.
 + */
 +int ipath_enable_wc(struct ipath_devdata *dd)
++=======
+ #include <linux/mlx5/device.h>
+ #include <net/xfrm.h>
+ #include <linux/idr.h>
+ 
+ #define MLX5E_IPSEC_SADB_RX_BITS 10
+ #define MLX5E_METADATA_ETHER_TYPE (0x8CE4)
+ #define MLX5E_METADATA_ETHER_LEN 8
+ 
+ struct mlx5e_priv;
+ 
+ struct mlx5e_ipsec_sw_stats {
+ 	atomic64_t ipsec_rx_drop_sp_alloc;
+ 	atomic64_t ipsec_rx_drop_sadb_miss;
+ 	atomic64_t ipsec_rx_drop_syndrome;
+ };
+ 
+ struct mlx5e_ipsec {
+ 	struct mlx5e_priv *en_priv;
+ 	DECLARE_HASHTABLE(sadb_rx, MLX5E_IPSEC_SADB_RX_BITS);
+ 	spinlock_t sadb_rx_lock; /* Protects sadb_rx and halloc */
+ 	struct ida halloc;
+ 	struct mlx5e_ipsec_sw_stats sw_stats;
+ };
+ 
+ int mlx5e_ipsec_init(struct mlx5e_priv *priv);
+ void mlx5e_ipsec_cleanup(struct mlx5e_priv *priv);
+ void mlx5e_ipsec_build_netdev(struct mlx5e_priv *priv);
+ 
+ struct xfrm_state *mlx5e_ipsec_sadb_rx_lookup(struct mlx5e_ipsec *dev,
+ 					      unsigned int handle);
+ 
+ #else
+ 
+ static inline int mlx5e_ipsec_init(struct mlx5e_priv *priv)
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path):drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.h
  {
  	return 0;
  }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/Makefile
index 9e644615f07a,23cb8ba91e6f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/Makefile
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Makefile
@@@ -12,4 -18,6 +12,10 @@@ mlx5_core-$(CONFIG_MLX5_CORE_EN) += wq.
  
  mlx5_core-$(CONFIG_MLX5_CORE_EN_DCB) +=  en_dcbnl.o
  
++<<<<<<< HEAD
 +mlx5_core-$(CONFIG_MLX5_CORE_IPOIB) += ipoib.o
++=======
+ mlx5_core-$(CONFIG_MLX5_CORE_IPOIB) += ipoib/ipoib.o ipoib/ethtool.o
+ 
+ mlx5_core-$(CONFIG_MLX5_EN_IPSEC) += en_accel/ipsec.o en_accel/ipsec_rxtx.o
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 5a367b6ed375,170a9378d1f7..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -34,9 -34,14 +34,16 @@@
  #include <net/pkt_cls.h>
  #include <linux/mlx5/fs.h>
  #include <net/vxlan.h>
 -#include <linux/bpf.h>
 -#include "eswitch.h"
  #include "en.h"
  #include "en_tc.h"
++<<<<<<< HEAD
 +#include "eswitch.h"
++=======
+ #include "en_rep.h"
+ #include "en_accel/ipsec.h"
+ #include "en_accel/ipsec_rxtx.h"
+ #include "accel/ipsec.h"
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path)
  #include "vxlan.h"
  
  struct mlx5e_rq_param {
@@@ -76,44 -78,50 +83,49 @@@ static bool mlx5e_check_fragmented_stri
  		MLX5_CAP_ETH(mdev, reg_umr_sq);
  }
  
 -void mlx5e_set_rq_type_params(struct mlx5_core_dev *mdev,
 -			      struct mlx5e_params *params, u8 rq_type)
 +void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type)
  {
 -	params->rq_wq_type = rq_type;
 -	params->lro_wqe_sz = MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ;
 -	switch (params->rq_wq_type) {
 +	priv->params.rq_wq_type = rq_type;
 +	priv->params.lro_wqe_sz = MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ;
 +	switch (priv->params.rq_wq_type) {
  	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
 -		params->log_rq_size = is_kdump_kernel() ?
 -			MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW :
 -			MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW;
 -		params->mpwqe_log_stride_sz =
 -			MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS) ?
 -			MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(mdev) :
 -			MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev);
 -		params->mpwqe_log_num_strides = MLX5_MPWRQ_LOG_WQE_SZ -
 -			params->mpwqe_log_stride_sz;
 +		priv->params.log_rq_size = MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW;
 +		priv->params.mpwqe_log_stride_sz =
 +			MLX5E_GET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS) ?
 +			MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(priv->mdev) :
 +			MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(priv->mdev);
 +		priv->params.mpwqe_log_num_strides = MLX5_MPWRQ_LOG_WQE_SZ -
 +			priv->params.mpwqe_log_stride_sz;
  		break;
  	default: /* MLX5_WQ_TYPE_LINKED_LIST */
 -		params->log_rq_size = is_kdump_kernel() ?
 -			MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE :
 -			MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
 -		params->rq_headroom = params->xdp_prog ?
 -			XDP_PACKET_HEADROOM : MLX5_RX_HEADROOM;
 -		params->rq_headroom += NET_IP_ALIGN;
 +		priv->params.log_rq_size = MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
  
  		/* Extra room needed for build_skb */
 -		params->lro_wqe_sz -= params->rq_headroom +
 +		priv->params.lro_wqe_sz -= MLX5_RX_HEADROOM +
  			SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
  	}
 +	priv->params.min_rx_wqes = mlx5_min_rx_wqes(priv->params.rq_wq_type,
 +					       BIT(priv->params.log_rq_size));
  
 -	mlx5_core_info(mdev, "MLX5E: StrdRq(%d) RqSz(%ld) StrdSz(%ld) RxCqeCmprss(%d)\n",
 -		       params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ,
 -		       BIT(params->log_rq_size),
 -		       BIT(params->mpwqe_log_stride_sz),
 -		       MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS));
 +	mlx5_core_info(priv->mdev,
 +		       "MLX5E: StrdRq(%d) RqSz(%ld) StrdSz(%ld) RxCqeCmprss(%d)\n",
 +		       priv->params.rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ,
 +		       BIT(priv->params.log_rq_size),
 +		       BIT(priv->params.mpwqe_log_stride_sz),
 +		       MLX5E_GET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS));
  }
  
 -static void mlx5e_set_rq_params(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
 +static void mlx5e_set_rq_priv_params(struct mlx5e_priv *priv)
  {
++<<<<<<< HEAD
 +	u8 rq_type = mlx5e_check_fragmented_striding_rq_cap(priv->mdev) ?
++=======
+ 	u8 rq_type = mlx5e_check_fragmented_striding_rq_cap(mdev) &&
+ 		    !params->xdp_prog && !MLX5_IPSEC_DEV(mdev) ?
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path)
  		    MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ :
  		    MLX5_WQ_TYPE_LINKED_LIST;
 -	mlx5e_set_rq_type_params(mdev, params, rq_type);
 +	mlx5e_set_rq_type_params(priv, rq_type);
  }
  
  static void mlx5e_update_carrier(struct mlx5e_priv *priv)
@@@ -557,18 -570,41 +569,34 @@@ static int mlx5e_create_rq(struct mlx5e
  
  	wq_sz = mlx5_wq_ll_get_size(&rq->wq);
  
 -	rq->wq_type = params->rq_wq_type;
 +	rq->wq_type = priv->params.rq_wq_type;
  	rq->pdev    = c->pdev;
  	rq->netdev  = c->netdev;
 -	rq->tstamp  = c->tstamp;
 +	rq->tstamp  = &priv->tstamp;
  	rq->channel = c;
  	rq->ix      = c->ix;
 -	rq->mdev    = mdev;
 -
 -	rq->xdp_prog = params->xdp_prog ? bpf_prog_inc(params->xdp_prog) : NULL;
 -	if (IS_ERR(rq->xdp_prog)) {
 -		err = PTR_ERR(rq->xdp_prog);
 -		rq->xdp_prog = NULL;
 -		goto err_rq_wq_destroy;
 -	}
 +	rq->priv    = c->priv;
  
 -	rq->buff.map_dir = rq->xdp_prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
 -	rq->rx_headroom = params->rq_headroom;
 -
 -	switch (rq->wq_type) {
 +	switch (priv->params.rq_wq_type) {
  	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
++<<<<<<< HEAD
 +		if (mlx5e_is_vf_vport_rep(priv)) {
++=======
+ 
+ 		rq->alloc_wqe = mlx5e_alloc_rx_mpwqe;
+ 		rq->dealloc_wqe = mlx5e_dealloc_rx_mpwqe;
+ 
+ 		rq->handle_rx_cqe = c->priv->profile->rx_handlers.handle_rx_cqe_mpwqe;
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 		if (MLX5_IPSEC_DEV(mdev)) {
+ 			err = -EINVAL;
+ 			netdev_err(c->netdev, "MPWQE RQ with IPSec offload not supported\n");
+ 			goto err_rq_wq_destroy;
+ 		}
+ #endif
+ 		if (!rq->handle_rx_cqe) {
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path)
  			err = -EINVAL;
 -			netdev_err(c->netdev, "RX handler of MPWQE RQ is not set, err %d\n", err);
  			goto err_rq_wq_destroy;
  		}
  
@@@ -607,9 -634,27 +635,33 @@@
  		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
  		rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
  
++<<<<<<< HEAD
 +		rq->buff.wqe_sz = (priv->params.lro_en) ?
 +				priv->params.lro_wqe_sz :
 +				MLX5E_SW2HW_MTU(priv->netdev->mtu);
++=======
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 		if (c->priv->ipsec)
+ 			rq->handle_rx_cqe = mlx5e_ipsec_handle_rx_cqe;
+ 		else
+ #endif
+ 			rq->handle_rx_cqe = c->priv->profile->rx_handlers.handle_rx_cqe;
+ 		if (!rq->handle_rx_cqe) {
+ 			kfree(rq->wqe.frag_info);
+ 			err = -EINVAL;
+ 			netdev_err(c->netdev, "RX handler of RQ is not set, err %d\n", err);
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		rq->buff.wqe_sz = params->lro_en  ?
+ 				params->lro_wqe_sz :
+ 				MLX5E_SW2HW_MTU(c->priv, c->netdev->mtu);
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 		if (MLX5_IPSEC_DEV(mdev))
+ 			rq->buff.wqe_sz += MLX5E_METADATA_ETHER_LEN;
+ #endif
+ 		rq->wqe.page_reuse = !params->xdp_prog && !params->lro_en;
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path)
  		byte_count = rq->buff.wqe_sz;
  
  		/* calc the required page order */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 8b4b5a3808c1,325b2c8c1c6d..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -38,7 -39,9 +38,13 @@@
  #include "en.h"
  #include "en_tc.h"
  #include "eswitch.h"
++<<<<<<< HEAD
 +#include "ipoib.h"
++=======
+ #include "en_rep.h"
+ #include "ipoib/ipoib.h"
+ #include "en_accel/ipsec_rxtx.h"
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path)
  
  static inline bool mlx5e_rx_hw_stamp(struct mlx5e_tstamp *tstamp)
  {
@@@ -853,3 -1026,201 +859,204 @@@ int mlx5e_poll_rx_cq(struct mlx5e_cq *c
  
  	return work_done;
  }
++<<<<<<< HEAD
++=======
+ 
+ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)
+ {
+ 	struct mlx5e_xdpsq *sq;
+ 	struct mlx5e_rq *rq;
+ 	u16 sqcc;
+ 	int i;
+ 
+ 	sq = container_of(cq, struct mlx5e_xdpsq, cq);
+ 
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
+ 		return false;
+ 
+ 	rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 
+ 	/* sq->cc must be updated only after mlx5_cqwq_update_db_record(),
+ 	 * otherwise a cq overrun may occur
+ 	 */
+ 	sqcc = sq->cc;
+ 
+ 	for (i = 0; i < MLX5E_TX_CQ_POLL_BUDGET; i++) {
+ 		struct mlx5_cqe64 *cqe;
+ 		u16 wqe_counter;
+ 		bool last_wqe;
+ 
+ 		cqe = mlx5_cqwq_get_cqe(&cq->wq);
+ 		if (!cqe)
+ 			break;
+ 
+ 		mlx5_cqwq_pop(&cq->wq);
+ 
+ 		wqe_counter = be16_to_cpu(cqe->wqe_counter);
+ 
+ 		do {
+ 			struct mlx5e_dma_info *di;
+ 			u16 ci;
+ 
+ 			last_wqe = (sqcc == wqe_counter);
+ 
+ 			ci = sqcc & sq->wq.sz_m1;
+ 			di = &sq->db.di[ci];
+ 
+ 			sqcc++;
+ 			/* Recycle RX page */
+ 			mlx5e_page_release(rq, di, true);
+ 		} while (!last_wqe);
+ 	}
+ 
+ 	mlx5_cqwq_update_db_record(&cq->wq);
+ 
+ 	/* ensure cq space is freed before enabling more cqes */
+ 	wmb();
+ 
+ 	sq->cc = sqcc;
+ 	return (i == MLX5E_TX_CQ_POLL_BUDGET);
+ }
+ 
+ void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq)
+ {
+ 	struct mlx5e_rq *rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 	struct mlx5e_dma_info *di;
+ 	u16 ci;
+ 
+ 	while (sq->cc != sq->pc) {
+ 		ci = sq->cc & sq->wq.sz_m1;
+ 		di = &sq->db.di[ci];
+ 		sq->cc++;
+ 
+ 		mlx5e_page_release(rq, di, false);
+ 	}
+ }
+ 
+ #ifdef CONFIG_MLX5_CORE_IPOIB
+ 
+ #define MLX5_IB_GRH_DGID_OFFSET 24
+ #define MLX5_GID_SIZE           16
+ 
+ static inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,
+ 					 struct mlx5_cqe64 *cqe,
+ 					 u32 cqe_bcnt,
+ 					 struct sk_buff *skb)
+ {
+ 	struct net_device *netdev = rq->netdev;
+ 	struct mlx5e_tstamp *tstamp = rq->tstamp;
+ 	char *pseudo_header;
+ 	u8 *dgid;
+ 	u8 g;
+ 
+ 	g = (be32_to_cpu(cqe->flags_rqpn) >> 28) & 3;
+ 	dgid = skb->data + MLX5_IB_GRH_DGID_OFFSET;
+ 	if ((!g) || dgid[0] != 0xff)
+ 		skb->pkt_type = PACKET_HOST;
+ 	else if (memcmp(dgid, netdev->broadcast + 4, MLX5_GID_SIZE) == 0)
+ 		skb->pkt_type = PACKET_BROADCAST;
+ 	else
+ 		skb->pkt_type = PACKET_MULTICAST;
+ 
+ 	/* TODO: IB/ipoib: Allow mcast packets from other VFs
+ 	 * 68996a6e760e5c74654723eeb57bf65628ae87f4
+ 	 */
+ 
+ 	skb_pull(skb, MLX5_IB_GRH_BYTES);
+ 
+ 	skb->protocol = *((__be16 *)(skb->data));
+ 
+ 	skb->ip_summed = CHECKSUM_COMPLETE;
+ 	skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
+ 
+ 	if (unlikely(mlx5e_rx_hw_stamp(tstamp)))
+ 		mlx5e_fill_hwstamp(tstamp, get_cqe_ts(cqe), skb_hwtstamps(skb));
+ 
+ 	skb_record_rx_queue(skb, rq->ix);
+ 
+ 	if (likely(netdev->features & NETIF_F_RXHASH))
+ 		mlx5e_skb_set_hash(cqe, skb);
+ 
+ 	/* 20 bytes of ipoib header and 4 for encap existing */
+ 	pseudo_header = skb_push(skb, MLX5_IPOIB_PSEUDO_LEN);
+ 	memset(pseudo_header, 0, MLX5_IPOIB_PSEUDO_LEN);
+ 	skb_reset_mac_header(skb);
+ 	skb_pull(skb, MLX5_IPOIB_HARD_LEN);
+ 
+ 	skb->dev = netdev;
+ 
+ 	rq->stats.csum_complete++;
+ 	rq->stats.packets++;
+ 	rq->stats.bytes += cqe_bcnt;
+ }
+ 
+ void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_wqe_frag_info *wi;
+ 	struct mlx5e_rx_wqe *wqe;
+ 	__be16 wqe_counter_be;
+ 	struct sk_buff *skb;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	wi             = &rq->wqe.frag_info[wqe_counter];
+ 	cqe_bcnt       = be32_to_cpu(cqe->byte_cnt);
+ 
+ 	skb = skb_from_cqe(rq, cqe, wi, cqe_bcnt);
+ 	if (!skb)
+ 		goto wq_free_wqe;
+ 
+ 	mlx5i_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ 
+ wq_free_wqe:
+ 	mlx5e_free_rx_wqe_reuse(rq, wi);
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ #endif /* CONFIG_MLX5_CORE_IPOIB */
+ 
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 
+ void mlx5e_ipsec_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_wqe_frag_info *wi;
+ 	struct mlx5e_rx_wqe *wqe;
+ 	__be16 wqe_counter_be;
+ 	struct sk_buff *skb;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	wi             = &rq->wqe.frag_info[wqe_counter];
+ 	cqe_bcnt       = be32_to_cpu(cqe->byte_cnt);
+ 
+ 	skb = skb_from_cqe(rq, cqe, wi, cqe_bcnt);
+ 	if (unlikely(!skb)) {
+ 		/* a DROP, save the page-reuse checks */
+ 		mlx5e_free_rx_wqe(rq, wi);
+ 		goto wq_ll_pop;
+ 	}
+ 	skb = mlx5e_ipsec_handle_rx_skb(rq->netdev, skb);
+ 	if (unlikely(!skb)) {
+ 		mlx5e_free_rx_wqe(rq, wi);
+ 		goto wq_ll_pop;
+ 	}
+ 
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ 
+ 	mlx5e_free_rx_wqe_reuse(rq, wi);
+ wq_ll_pop:
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ #endif /* CONFIG_MLX5_EN_IPSEC */
++>>>>>>> 899a59d301bc (net/mlx5e: IPSec, Add Innova IPSec offload RX data path)
* Unmerged path drivers/infiniband/hw/ipath/ipsec_rxtx.c
* Unmerged path drivers/infiniband/hw/ipath/ipsec_rxtx.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.c
* Unmerged path drivers/infiniband/hw/ipath/ipath_wc_ppc64.c
* Unmerged path drivers/infiniband/hw/ipath/ipsec_rxtx.c
* Unmerged path drivers/infiniband/hw/ipath/ipsec_rxtx.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/Makefile
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
