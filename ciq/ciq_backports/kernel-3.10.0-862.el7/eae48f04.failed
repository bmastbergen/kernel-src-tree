cpufreq: intel_pstate: Per CPU P-State limits

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Per CPU P-State limits (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 88.89%
commit-author Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
commit eae48f046ffa117afb782cd9b3ae5469df0042e2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/eae48f04.failed

Intel P-State offers two interface to set performance limits:
- Intel P-State sysfs
	/sys/devices/system/cpu/intel_pstate/max_perf_pct
	/sys/devices/system/cpu/intel_pstate/min_perf_pct
- cpufreq
	/sys/devices/system/cpu/cpu*/cpufreq/scaling_max_freq
	/sys/devices/system/cpu/cpu*/cpufreq/scaling_min_freq

In the current implementation both of the above methods, change limits
to every CPU in the system. Moreover the limits placed using cpufreq
policy interface also presented in the Intel P-State sysfs via modified
max_perf_pct and min_per_pct during sysfs reads. This allows to check
percent of reduced/increased performance, irrespective of method used to
limit.

There are some new generations of processors, where it is possible to
have limits placed on individual CPU cores. Using cpufreq interface it
is possible to set limits on each CPU. But the current processing will
use last limits placed on all CPUs. So the per core limit feature of
CPUs can't be used.

This change brings in capability to set P-States limits for each CPU,
with some limitations. In this case what should be the read of
max_perf_pct and min_perf_pct? It can be most restrictive limits placed
on any CPU or max possible performance on any given CPU on which no
limits are placed. In either case someone will have issue.

So the consensus is, we can't have both sysfs controls present when user
wants to use limit per core limits.
- By default per-core-control feature is not enabled. So no one will
notice any difference.
- The way to enable is by kernel command line
intel_pstate=per_cpu_perf_limits
- When the per-core-controls are enabled there is no display of for both
read and write on
	/sys/devices/system/cpu/intel_pstate/max_perf_pct
	/sys/devices/system/cpu/intel_pstate/min_perf_pct
- User can change limits using
	/sys/devices/system/cpu/cpu*/cpufreq/scaling_max_freq
	/sys/devices/system/cpu/cpu*/cpufreq/scaling_min_freq
	/sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
- User can still observe turbo percent and number of P-States from
	/sys/devices/system/cpu/intel_pstate/turbo_pct
	/sys/devices/system/cpu/intel_pstate/num_pstates
- User can read write system wide turbo status
	/sys/devices/system/cpu/no_turbo

While changing this BUG_ON is changed to WARN_ON, as they are not fatal
errors for the system.

	Signed-off-by: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit eae48f046ffa117afb782cd9b3ae5469df0042e2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index 2849fe3729b8,b6e9b49bf151..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -1406,40 -1495,59 +1453,44 @@@ static int intel_pstate_init_cpu(unsign
  
  static unsigned int intel_pstate_get(unsigned int cpu_num)
  {
 -	struct cpudata *cpu = all_cpu_data[cpu_num];
 -
 -	return cpu ? get_avg_frequency(cpu) : 0;
 -}
 -
 -static void intel_pstate_set_update_util_hook(unsigned int cpu_num)
 -{
 -	struct cpudata *cpu = all_cpu_data[cpu_num];
 -
 -	if (cpu->update_util_set)
 -		return;
 +	struct sample *sample;
 +	struct cpudata *cpu;
  
 -	/* Prevent intel_pstate_update_util() from using stale data. */
 -	cpu->sample.time = 0;
 -	cpufreq_add_update_util_hook(cpu_num, &cpu->update_util,
 -				     intel_pstate_update_util);
 -	cpu->update_util_set = true;
 +	cpu = all_cpu_data[cpu_num];
 +	if (!cpu)
 +		return 0;
 +	sample = &cpu->sample;
 +	return sample->freq;
  }
  
- static int intel_pstate_set_policy(struct cpufreq_policy *policy)
 -static void intel_pstate_clear_update_util_hook(unsigned int cpu)
++static void intel_pstate_update_perf_limits(struct cpufreq_policy *policy,
++					    struct perf_limits *limits)
  {
 -	struct cpudata *cpu_data = all_cpu_data[cpu];
 -
 -	if (!cpu_data->update_util_set)
 -		return;
++<<<<<<< HEAD
 +	if (!policy->cpuinfo.max_freq)
 +		return -ENODEV;
  
 -	cpufreq_remove_update_util_hook(cpu);
 -	cpu_data->update_util_set = false;
 -	synchronize_sched();
 -}
 +	pr_debug("set_policy cpuinfo.max %u policy->max %u\n",
 +		 policy->cpuinfo.max_freq, policy->max);
  
 -static void intel_pstate_set_performance_limits(struct perf_limits *limits)
 -{
 -	limits->no_turbo = 0;
 -	limits->turbo_disabled = 0;
 -	limits->max_perf_pct = 100;
 -	limits->max_perf = int_tofp(1);
 -	limits->min_perf_pct = 100;
 -	limits->min_perf = int_tofp(1);
 -	limits->max_policy_pct = 100;
 -	limits->max_sysfs_pct = 100;
 -	limits->min_policy_pct = 0;
 -	limits->min_sysfs_pct = 0;
 -}
 +	if (policy->policy == CPUFREQ_POLICY_PERFORMANCE &&
 +	    policy->max >= policy->cpuinfo.max_freq) {
 +		pr_debug("intel_pstate: set performance\n");
 +		limits = &performance_limits;
 +		if (hwp_active)
 +			intel_pstate_hwp_set(policy->cpus);
 +		return 0;
 +	}
  
 -static void intel_pstate_update_perf_limits(struct cpufreq_policy *policy,
 -					    struct perf_limits *limits)
 -{
 +	pr_debug("intel_pstate: set powersave\n");
 +	limits = &powersave_limits;
++=======
++>>>>>>> eae48f046ffa (cpufreq: intel_pstate: Per CPU P-State limits)
  	limits->min_policy_pct = (policy->min * 100) / policy->cpuinfo.max_freq;
- 	limits->min_policy_pct = clamp_t(int, limits->min_policy_pct, 0 , 100);
+ 	limits->min_policy_pct = clamp_t(int, limits->min_policy_pct, 0, 100);
  	limits->max_policy_pct = DIV_ROUND_UP(policy->max * 100,
  					      policy->cpuinfo.max_freq);
- 	limits->max_policy_pct = clamp_t(int, limits->max_policy_pct, 0 , 100);
+ 	limits->max_policy_pct = clamp_t(int, limits->max_policy_pct, 0, 100);
  
  	/* Normalize user input to [min_policy_pct, max_policy_pct] */
  	limits->min_perf_pct = max(limits->min_policy_pct,
@@@ -1454,14 -1562,71 +1505,78 @@@
  	/* Make sure min_perf_pct <= max_perf_pct */
  	limits->min_perf_pct = min(limits->max_perf_pct, limits->min_perf_pct);
  
 -	limits->min_perf = div_fp(limits->min_perf_pct, 100);
 -	limits->max_perf = div_fp(limits->max_perf_pct, 100);
 +	limits->min_perf = div_fp(int_tofp(limits->min_perf_pct),
 +				  int_tofp(100));
 +	limits->max_perf = div_fp(int_tofp(limits->max_perf_pct),
 +				  int_tofp(100));
  	limits->max_perf = round_up(limits->max_perf, FRAC_BITS);
  
++<<<<<<< HEAD
 +	if (hwp_active)
 +		intel_pstate_hwp_set(policy->cpus);
++=======
+ 	pr_debug("cpu:%d max_perf_pct:%d min_perf_pct:%d\n", policy->cpu,
+ 		 limits->max_perf_pct, limits->min_perf_pct);
+ }
+ 
+ static int intel_pstate_set_policy(struct cpufreq_policy *policy)
+ {
+ 	struct cpudata *cpu;
+ 	struct perf_limits *perf_limits = NULL;
+ 
+ 	if (!policy->cpuinfo.max_freq)
+ 		return -ENODEV;
+ 
+ 	pr_debug("set_policy cpuinfo.max %u policy->max %u\n",
+ 		 policy->cpuinfo.max_freq, policy->max);
+ 
+ 	cpu = all_cpu_data[policy->cpu];
+ 	cpu->policy = policy->policy;
+ 
+ 	if (cpu->pstate.max_pstate_physical > cpu->pstate.max_pstate &&
+ 	    policy->max < policy->cpuinfo.max_freq &&
+ 	    policy->max > cpu->pstate.max_pstate * cpu->pstate.scaling) {
+ 		pr_debug("policy->max > max non turbo frequency\n");
+ 		policy->max = policy->cpuinfo.max_freq;
+ 	}
+ 
+ 	if (per_cpu_limits)
+ 		perf_limits = cpu->perf_limits;
+ 
+ 	if (policy->policy == CPUFREQ_POLICY_PERFORMANCE) {
+ 		if (!perf_limits) {
+ 			limits = &performance_limits;
+ 			perf_limits = limits;
+ 		}
+ 		if (policy->max >= policy->cpuinfo.max_freq) {
+ 			pr_debug("set performance\n");
+ 			intel_pstate_set_performance_limits(perf_limits);
+ 			goto out;
+ 		}
+ 	} else {
+ 		pr_debug("set powersave\n");
+ 		if (!perf_limits) {
+ 			limits = &powersave_limits;
+ 			perf_limits = limits;
+ 		}
+ 
+ 	}
+ 
+ 	intel_pstate_update_perf_limits(policy, perf_limits);
+  out:
+ 	if (cpu->policy == CPUFREQ_POLICY_PERFORMANCE) {
+ 		/*
+ 		 * NOHZ_FULL CPUs need this as the governor callback may not
+ 		 * be invoked on them.
+ 		 */
+ 		intel_pstate_clear_update_util_hook(policy->cpu);
+ 		intel_pstate_max_within_limits(cpu);
+ 	}
+ 
+ 	intel_pstate_set_update_util_hook(policy->cpu);
+ 
+ 	intel_pstate_hwp_set_policy(policy);
++>>>>>>> eae48f046ffa (cpufreq: intel_pstate: Per CPU P-State limits)
  
  	return 0;
  }
* Unmerged path drivers/cpufreq/intel_pstate.c
