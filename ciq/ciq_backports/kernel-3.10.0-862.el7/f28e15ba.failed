netfilter: x_tables: pass xt_counters struct to counter allocator

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Florian Westphal <fw@strlen.de>
commit f28e15bacedd444608e25421c72eb2cf4527c9ca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f28e15ba.failed

Keeps some noise away from a followup patch.

	Signed-off-by: Florian Westphal <fw@strlen.de>
	Acked-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
(cherry picked from commit f28e15bacedd444608e25421c72eb2cf4527c9ca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/netfilter/x_tables.h
#	net/ipv4/netfilter/arp_tables.c
#	net/ipv4/netfilter/ip_tables.c
#	net/ipv6/netfilter/ip6_tables.c
#	net/netfilter/x_tables.c
diff --cc include/linux/netfilter/x_tables.h
index d08f0ed1fa64,05a94bd32c55..000000000000
--- a/include/linux/netfilter/x_tables.h
+++ b/include/linux/netfilter/x_tables.h
@@@ -369,37 -404,8 +369,42 @@@ static inline unsigned long ifname_comp
  }
  
  
++<<<<<<< HEAD
 +/* On SMP, ip(6)t_entry->counters.pcnt holds address of the
 + * real (percpu) counter.  On !SMP, its just the packet count,
 + * so nothing needs to be done there.
 + *
 + * xt_percpu_counter_alloc returns the address of the percpu
 + * counter, or 0 on !SMP. We force an alignment of 16 bytes
 + * so that bytes/packets share a common cache line.
 + *
 + * Hence caller must use IS_ERR_VALUE to check for error, this
 + * allows us to return 0 for single core systems without forcing
 + * callers to deal with SMP vs. NONSMP issues.
 + */
 +static inline u64 xt_percpu_counter_alloc(void)
 +{
 +	if (nr_cpu_ids > 1) {
 +		void __percpu *res = __alloc_percpu(sizeof(struct xt_counters),
 +						    sizeof(struct xt_counters));
 +
 +		if (res == NULL)
 +			return (u64) -ENOMEM;
 +
 +		return (__force u64) res;
 +	}
 +
 +	return 0;
 +}
 +static inline void xt_percpu_counter_free(u64 pcnt)
 +{
 +	if (nr_cpu_ids > 1)
 +		free_percpu((void __percpu *) pcnt);
 +}
++=======
+ bool xt_percpu_counter_alloc(struct xt_counters *counters);
+ void xt_percpu_counter_free(struct xt_counters *cnt);
++>>>>>>> f28e15bacedd (netfilter: x_tables: pass xt_counters struct to counter allocator)
  
  static inline struct xt_counters *
  xt_get_this_cpu_counter(struct xt_counters *cnt)
diff --cc net/ipv4/netfilter/arp_tables.c
index 19bc9997cf1c,808deb275ceb..000000000000
--- a/net/ipv4/netfilter/arp_tables.c
+++ b/net/ipv4/netfilter/arp_tables.c
@@@ -517,8 -417,7 +517,12 @@@ find_check_entry(struct arpt_entry *e, 
  	struct xt_target *target;
  	int ret;
  
++<<<<<<< HEAD
 +	e->counters.pcnt = xt_percpu_counter_alloc();
 +	if (IS_ERR_VALUE(e->counters.pcnt))
++=======
+ 	if (!xt_percpu_counter_alloc(&e->counters))
++>>>>>>> f28e15bacedd (netfilter: x_tables: pass xt_counters struct to counter allocator)
  		return -ENOMEM;
  
  	t = arpt_get_target(e);
diff --cc net/ipv4/netfilter/ip_tables.c
index 4bc05ad14eb5,a48430d3420f..000000000000
--- a/net/ipv4/netfilter/ip_tables.c
+++ b/net/ipv4/netfilter/ip_tables.c
@@@ -649,8 -540,7 +649,12 @@@ find_check_entry(struct ipt_entry *e, s
  	struct xt_mtchk_param mtpar;
  	struct xt_entry_match *ematch;
  
++<<<<<<< HEAD
 +	e->counters.pcnt = xt_percpu_counter_alloc();
 +	if (IS_ERR_VALUE(e->counters.pcnt))
++=======
+ 	if (!xt_percpu_counter_alloc(&e->counters))
++>>>>>>> f28e15bacedd (netfilter: x_tables: pass xt_counters struct to counter allocator)
  		return -ENOMEM;
  
  	j = 0;
diff --cc net/ipv6/netfilter/ip6_tables.c
index 66312651ddc5,a5a92083fd62..000000000000
--- a/net/ipv6/netfilter/ip6_tables.c
+++ b/net/ipv6/netfilter/ip6_tables.c
@@@ -660,8 -571,7 +660,12 @@@ find_check_entry(struct ip6t_entry *e, 
  	struct xt_mtchk_param mtpar;
  	struct xt_entry_match *ematch;
  
++<<<<<<< HEAD
 +	e->counters.pcnt = xt_percpu_counter_alloc();
 +	if (IS_ERR_VALUE(e->counters.pcnt))
++=======
+ 	if (!xt_percpu_counter_alloc(&e->counters))
++>>>>>>> f28e15bacedd (netfilter: x_tables: pass xt_counters struct to counter allocator)
  		return -ENOMEM;
  
  	j = 0;
diff --cc net/netfilter/x_tables.c
index 551407ae5911,be5e83047594..000000000000
--- a/net/netfilter/x_tables.c
+++ b/net/netfilter/x_tables.c
@@@ -1625,6 -1615,45 +1625,48 @@@ void xt_proto_fini(struct net *net, u_i
  }
  EXPORT_SYMBOL_GPL(xt_proto_fini);
  
++<<<<<<< HEAD
++=======
+ /**
+  * xt_percpu_counter_alloc - allocate x_tables rule counter
+  *
+  * @counter: pointer to counter struct inside the ip(6)/arpt_entry struct
+  *
+  * On SMP, the packet counter [ ip(6)t_entry->counters.pcnt ] will then
+  * contain the address of the real (percpu) counter.
+  *
+  * Rule evaluation needs to use xt_get_this_cpu_counter() helper
+  * to fetch the real percpu counter.
+  *
+  * returns false on error.
+  */
+ bool xt_percpu_counter_alloc(struct xt_counters *counter)
+ {
+ 	void __percpu *res;
+ 
+ 	if (nr_cpu_ids <= 1)
+ 		return true;
+ 
+ 	res = __alloc_percpu(sizeof(struct xt_counters),
+ 			     sizeof(struct xt_counters));
+ 	if (!res)
+ 		return false;
+ 
+ 	counter->pcnt = (__force unsigned long)res;
+ 	return true;
+ }
+ EXPORT_SYMBOL_GPL(xt_percpu_counter_alloc);
+ 
+ void xt_percpu_counter_free(struct xt_counters *counters)
+ {
+ 	unsigned long pcnt = counters->pcnt;
+ 
+ 	if (nr_cpu_ids > 1)
+ 		free_percpu((void __percpu *)pcnt);
+ }
+ EXPORT_SYMBOL_GPL(xt_percpu_counter_free);
+ 
++>>>>>>> f28e15bacedd (netfilter: x_tables: pass xt_counters struct to counter allocator)
  static int __net_init xt_net_init(struct net *net)
  {
  	int i;
* Unmerged path include/linux/netfilter/x_tables.h
* Unmerged path net/ipv4/netfilter/arp_tables.c
* Unmerged path net/ipv4/netfilter/ip_tables.c
* Unmerged path net/ipv6/netfilter/ip6_tables.c
* Unmerged path net/netfilter/x_tables.c
