blk-mq-sched: fix starvation for multiple hardware queues and shared tags

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 50e1dab86aa2c10cbca2f754aae9542169403141
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/50e1dab8.failed

If we have both multiple hardware queues and shared tag map between
devices, we need to ensure that we propagate the hardware queue
restart bit higher up. This is because we can get into a situation
where we don't have any IO pending on a hardware queue, yet we fail
getting a tag to start new IO. If that happens, it's not enough to
mark the hardware queue as needing a restart, we need to bubble
that up to the higher level queue as well.

	Signed-off-by: Jens Axboe <axboe@fb.com>
	Reviewed-by: Omar Sandoval <osandov@fb.com>
	Tested-by: Hannes Reinecke <hare@suse.com>
(cherry picked from commit 50e1dab86aa2c10cbca2f754aae9542169403141)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-sched.c
#	block/blk-mq-sched.h
#	block/blk-mq.c
#	block/blk-mq.h
#	include/linux/blkdev.h
diff --cc block/blk-mq.c
index 3b21482b7f01,21795c6575bc..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -38,26 -40,13 +38,26 @@@ static void __blk_mq_run_hw_queue(struc
  /*
   * Check if any of the ctx's have pending work in this hardware queue
   */
- static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
+ bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
  {
 -	return sbitmap_any_bit_set(&hctx->ctx_map) ||
 -			!list_empty_careful(&hctx->dispatch) ||
 -			blk_mq_sched_has_work(hctx);
 +	unsigned int i;
 +
 +	for (i = 0; i < hctx->ctx_map.size; i++)
 +		if (hctx->ctx_map.map[i].word)
 +			return true;
 +
 +	return false;
  }
  
 +static inline struct blk_align_bitmap *get_bm(struct blk_mq_hw_ctx *hctx,
 +					      struct blk_mq_ctx *ctx)
 +{
 +	return &hctx->ctx_map.map[ctx->index_hw / hctx->ctx_map.bits_per_word];
 +}
 +
 +#define CTX_TO_BIT(hctx, ctx)	\
 +	((ctx)->index_hw & ((hctx)->ctx_map.bits_per_word - 1))
 +
  /*
   * Mark this ctx as having pending work in this hardware queue
   */
@@@ -354,18 -327,25 +354,27 @@@ out_queue_exit
  }
  EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
  
 -void __blk_mq_finish_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 -			     struct request *rq)
 +static void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx,
 +				  struct blk_mq_ctx *ctx, struct request *rq)
  {
 -	const int sched_tag = rq->internal_tag;
 +	const int tag = rq->tag;
  	struct request_queue *q = rq->q;
  
 -	if (rq->rq_flags & RQF_MQ_INFLIGHT)
 +	if (rq->cmd_flags & REQ_MQ_INFLIGHT)
  		atomic_dec(&hctx->nr_active);
 -
 -	wbt_done(q->rq_wb, &rq->issue_stat);
 -	rq->rq_flags = 0;
 +	rq->cmd_flags = 0;
  
  	clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags);
++<<<<<<< HEAD
 +	blk_mq_put_tag(hctx, tag, &ctx->last_tag);
++=======
+ 	clear_bit(REQ_ATOM_POLL_SLEPT, &rq->atomic_flags);
+ 	if (rq->tag != -1)
+ 		blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
+ 	if (sched_tag != -1)
+ 		blk_mq_sched_completed_request(hctx, rq);
+ 	blk_mq_sched_restart_queues(hctx);
++>>>>>>> 50e1dab86aa2 (blk-mq-sched: fix starvation for multiple hardware queues and shared tags)
  	blk_queue_exit(q);
  }
  
diff --cc block/blk-mq.h
index 2d50f02667c4,077a4003f1fd..000000000000
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@@ -34,7 -32,29 +34,12 @@@ void blk_mq_free_queue(struct request_q
  int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
  void blk_mq_wake_waiters(struct request_queue *q);
  bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
++<<<<<<< HEAD
++=======
+ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
+ bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
++>>>>>>> 50e1dab86aa2 (blk-mq-sched: fix starvation for multiple hardware queues and shared tags)
  
 -/*
 - * Internal helpers for allocating/freeing the request map
 - */
 -void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 -		     unsigned int hctx_idx);
 -void blk_mq_free_rq_map(struct blk_mq_tags *tags);
 -struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 -					unsigned int hctx_idx,
 -					unsigned int nr_tags,
 -					unsigned int reserved_tags);
 -int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 -		     unsigned int hctx_idx, unsigned int depth);
 -
 -/*
 - * Internal helpers for request insertion into sw queues
 - */
 -void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 -				bool at_head);
 -void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 -				struct list_head *list);
  /*
   * CPU hotplug helpers
   */
diff --cc include/linux/blkdev.h
index 5e42b39c9d84,883b8abe4305..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -567,10 -600,14 +567,21 @@@ struct request_queue 
  #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
  #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
  #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
++<<<<<<< HEAD
 +#define QUEUE_FLAG_UNPRIV_SGIO 21	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NO_SG_MERGE 22	/* don't attempt to merge SG segments*/
 +#define QUEUE_FLAG_SG_GAPS     23	/* queue doesn't support SG gaps */
 +#define QUEUE_FLAG_DAX         24	/* device supports DAX */
++=======
+ #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
+ #define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
+ #define QUEUE_FLAG_WC	       23	/* Write back caching */
+ #define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
+ #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
+ #define QUEUE_FLAG_DAX         26	/* device supports DAX */
+ #define QUEUE_FLAG_STATS       27	/* track rq completion times */
+ #define QUEUE_FLAG_RESTART     28	/* queue needs restart at completion */
++>>>>>>> 50e1dab86aa2 (blk-mq-sched: fix starvation for multiple hardware queues and shared tags)
  
  #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
  				 (1 << QUEUE_FLAG_STACKABLE)	|	\
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-sched.h
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-sched.h
* Unmerged path block/blk-mq.c
* Unmerged path block/blk-mq.h
* Unmerged path include/linux/blkdev.h
