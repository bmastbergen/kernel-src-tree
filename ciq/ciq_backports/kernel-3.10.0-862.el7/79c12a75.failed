nfp: separate data path information from the reset of adapter structure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 79c12a752cea61d41fd2f95600eaaaaafb99fe9e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/79c12a75.failed

Move all data path information into a separate structure.  This way
we will be able to allocate new data path with all new rings etc.
and swap it in easily.

No functional changes.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 79c12a752cea61d41fd2f95600eaaaaafb99fe9e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/nfp_net.h
#	drivers/net/ethernet/netronome/nfp/nfp_net_common.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_debugfs.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_main.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_offload.c
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net.h
index 1826ee93d1da,7d2c38604372..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net.h
@@@ -417,31 -428,82 +417,101 @@@ static inline bool nfp_net_fw_ver_eq(st
  	       fw_ver->minor == minor;
  }
  
 -struct nfp_stat_pair {
 -	u64 pkts;
 -	u64 bytes;
 -};
 -
  /**
++<<<<<<< HEAD
 + * struct nfp_net - NFP network device structure
 + * @pdev:               Backpointer to PCI device
 + * @netdev:             Backpointer to net_device structure
 + * @is_vf:              Is the driver attached to a VF?
 + * @fw_loaded:          Is the firmware loaded?
 + * @ctrl:               Local copy of the control register/word.
 + * @fl_bufsz:           Currently configured size of the freelist buffers
 + * @rx_offset:		Offset in the RX buffers where packet data starts
 + * @fw_ver:             Firmware version
++=======
+  * struct nfp_net_dp - NFP network device datapath data structure
+  * @dev:		Backpointer to struct device
+  * @netdev:		Backpointer to net_device structure
+  * @is_vf:		Is the driver attached to a VF?
+  * @bpf_offload_skip_sw:  Offloaded BPF program will not be rerun by cls_bpf
+  * @bpf_offload_xdp:	Offloaded BPF program is XDP
+  * @chained_metadata_format:  Firemware will use new metadata format
+  * @ctrl:		Local copy of the control register/word.
+  * @fl_bufsz:		Currently configured size of the freelist buffers
+  * @rx_offset:		Offset in the RX buffers where packet data starts
+  * @xdp_prog:		Installed XDP program
+  * @tx_rings:		Array of pre-allocated TX ring structures
+  * @rx_rings:		Array of pre-allocated RX ring structures
+  *
+  * @txd_cnt:		Size of the TX ring in number of descriptors
+  * @rxd_cnt:		Size of the RX ring in number of descriptors
+  * @num_r_vecs:		Number of used ring vectors
+  * @num_tx_rings:	Currently configured number of TX rings
+  * @num_stack_tx_rings:	Number of TX rings used by the stack (not XDP)
+  * @num_rx_rings:	Currently configured number of RX rings
+  */
+ struct nfp_net_dp {
+ 	struct device *dev;
+ 	struct net_device *netdev;
+ 
+ 	unsigned is_vf:1;
+ 	unsigned bpf_offload_skip_sw:1;
+ 	unsigned bpf_offload_xdp:1;
+ 	unsigned chained_metadata_format:1;
+ 
+ 	u32 ctrl;
+ 	u32 fl_bufsz;
+ 
+ 	u32 rx_offset;
+ 
+ 	struct bpf_prog *xdp_prog;
+ 
+ 	struct nfp_net_tx_ring *tx_rings;
+ 	struct nfp_net_rx_ring *rx_rings;
+ 
+ 	/* Cold data follows */
+ 
+ 	unsigned int txd_cnt;
+ 	unsigned int rxd_cnt;
+ 
+ 	unsigned int num_r_vecs;
+ 
+ 	unsigned int num_tx_rings;
+ 	unsigned int num_stack_tx_rings;
+ 	unsigned int num_rx_rings;
+ };
+ 
+ /**
+  * struct nfp_net - NFP network device structure
+  * @dp:			Datapath structure
+  * @fw_ver:		Firmware version
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
   * @cap:                Capabilities advertised by the Firmware
   * @max_mtu:            Maximum support MTU advertised by the Firmware
 - * @rss_hfunc:		RSS selected hash function
   * @rss_cfg:            RSS configuration
   * @rss_key:            RSS secret key
   * @rss_itbl:           RSS indirection table
++<<<<<<< HEAD
 + * @max_tx_rings:       Maximum number of TX rings supported by the Firmware
 + * @max_rx_rings:       Maximum number of RX rings supported by the Firmware
 + * @num_tx_rings:       Currently configured number of TX rings
 + * @num_rx_rings:       Currently configured number of RX rings
 + * @txd_cnt:            Size of the TX ring in number of descriptors
 + * @rxd_cnt:            Size of the RX ring in number of descriptors
 + * @tx_rings:           Array of pre-allocated TX ring structures
 + * @rx_rings:           Array of pre-allocated RX ring structures
 + * @max_r_vecs:	        Number of allocated interrupt vectors for RX/TX
 + * @num_r_vecs:         Number of used ring vectors
++=======
+  * @rx_filter:		Filter offload statistics - dropped packets/bytes
+  * @rx_filter_prev:	Filter offload statistics - values from previous update
+  * @rx_filter_change:	Jiffies when statistics last changed
+  * @rx_filter_stats_timer:  Timer for polling filter offload statistics
+  * @rx_filter_lock:	Lock protecting timer state changes (teardown)
+  * @max_r_vecs:		Number of allocated interrupt vectors for RX/TX
+  * @max_tx_rings:       Maximum number of TX rings supported by the Firmware
+  * @max_rx_rings:       Maximum number of RX rings supported by the Firmware
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
   * @r_vecs:             Pre-allocated array of ring vectors
   * @irq_entries:        Pre-allocated array of MSI-X entries
   * @lsc_handler:        Handler for Link State Change interrupt
@@@ -471,24 -533,15 +541,29 @@@
   * @debugfs_dir:	Device directory in debugfs
   * @ethtool_dump_flag:	Ethtool dump flag
   * @port_list:		Entry on device port list
 - * @pdev:		Backpointer to PCI device
   * @cpp:		CPP device handle if available
 - * @eth_port:		Translated ETH Table port entry
   */
  struct nfp_net {
++<<<<<<< HEAD
 +	struct pci_dev *pdev;
 +	struct net_device *netdev;
 +
 +	unsigned is_vf:1;
 +	unsigned fw_loaded:1;
 +
 +	u32 ctrl;
 +	u32 fl_bufsz;
 +
 +	u32 rx_offset;
 +
 +	struct nfp_net_tx_ring *tx_rings;
 +	struct nfp_net_rx_ring *rx_rings;
++=======
+ 	struct nfp_net_dp dp;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  	struct nfp_net_fw_version fw_ver;
+ 
  	u32 cap;
  	u32 max_mtu;
  
@@@ -499,9 -553,11 +574,12 @@@
  	unsigned int max_tx_rings;
  	unsigned int max_rx_rings;
  
++<<<<<<< HEAD
 +	unsigned int num_tx_rings;
 +	unsigned int num_rx_rings;
 +
++=======
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	int stride_tx;
  	int stride_rx;
  
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index e0a7eb1db7a9,951d511643f1..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@@ -81,18 -86,18 +81,26 @@@ void nfp_net_get_fw_version(struct nfp_
  }
  
  static dma_addr_t
- nfp_net_dma_map_rx(struct nfp_net *nn, void *frag, unsigned int bufsz,
+ nfp_net_dma_map_rx(struct nfp_net_dp *dp, void *frag, unsigned int bufsz,
  		   int direction)
  {
++<<<<<<< HEAD
 +	return dma_map_single(&nn->pdev->dev, frag + NFP_NET_RX_BUF_HEADROOM,
++=======
+ 	return dma_map_single(dp->dev, frag + NFP_NET_RX_BUF_HEADROOM,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			      bufsz - NFP_NET_RX_BUF_NON_DATA, direction);
  }
  
  static void
- nfp_net_dma_unmap_rx(struct nfp_net *nn, dma_addr_t dma_addr,
+ nfp_net_dma_unmap_rx(struct nfp_net_dp *dp, dma_addr_t dma_addr,
  		     unsigned int bufsz, int direction)
  {
++<<<<<<< HEAD
 +	dma_unmap_single(&nn->pdev->dev, dma_addr,
++=======
+ 	dma_unmap_single(dp->dev, dma_addr,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			 bufsz - NFP_NET_RX_BUF_NON_DATA, direction);
  }
  
@@@ -330,13 -336,15 +340,20 @@@ nfp_net_irqs_assign(struct nfp_net *nn
  
  	memcpy(nn->irq_entries, irq_entries, sizeof(*irq_entries) * n);
  
- 	if (nn->num_rx_rings > nn->num_r_vecs ||
- 	    nn->num_tx_rings > nn->num_r_vecs)
+ 	if (dp->num_rx_rings > dp->num_r_vecs ||
+ 	    dp->num_tx_rings > dp->num_r_vecs)
  		nn_warn(nn, "More rings (%d,%d) than vectors (%d).\n",
- 			nn->num_rx_rings, nn->num_tx_rings, nn->num_r_vecs);
+ 			dp->num_rx_rings, dp->num_tx_rings,
+ 			dp->num_r_vecs);
  
++<<<<<<< HEAD
 +	nn->num_rx_rings = min(nn->num_r_vecs, nn->num_rx_rings);
 +	nn->num_tx_rings = min(nn->num_r_vecs, nn->num_tx_rings);
++=======
+ 	dp->num_rx_rings = min(dp->num_r_vecs, dp->num_rx_rings);
+ 	dp->num_tx_rings = min(dp->num_r_vecs, dp->num_tx_rings);
+ 	dp->num_stack_tx_rings = dp->num_tx_rings;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  }
  
  /**
@@@ -756,9 -771,9 +773,15 @@@ static int nfp_net_tx(struct sk_buff *s
  	}
  
  	/* Start with the head skbuf */
++<<<<<<< HEAD
 +	dma_addr = dma_map_single(&nn->pdev->dev, skb->data, skb_headlen(skb),
 +				  DMA_TO_DEVICE);
 +	if (dma_mapping_error(&nn->pdev->dev, dma_addr))
++=======
+ 	dma_addr = dma_map_single(dp->dev, skb->data, skb_headlen(skb),
+ 				  DMA_TO_DEVICE);
+ 	if (dma_mapping_error(dp->dev, dma_addr))
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		goto err_free;
  
  	wr_idx = tx_ring->wr_p & (tx_ring->cnt - 1);
@@@ -800,9 -815,9 +823,15 @@@
  			frag = &skb_shinfo(skb)->frags[f];
  			fsize = skb_frag_size(frag);
  
++<<<<<<< HEAD
 +			dma_addr = skb_frag_dma_map(&nn->pdev->dev, frag, 0,
 +						    fsize, DMA_TO_DEVICE);
 +			if (dma_mapping_error(&nn->pdev->dev, dma_addr))
++=======
+ 			dma_addr = skb_frag_dma_map(dp->dev, frag, 0,
+ 						    fsize, DMA_TO_DEVICE);
+ 			if (dma_mapping_error(dp->dev, dma_addr))
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  				goto err_unmap;
  
  			wr_idx = (wr_idx + 1) & (tx_ring->cnt - 1);
@@@ -845,8 -856,7 +874,12 @@@ err_unmap
  	--f;
  	while (f >= 0) {
  		frag = &skb_shinfo(skb)->frags[f];
++<<<<<<< HEAD
 +		dma_unmap_page(&nn->pdev->dev,
 +			       tx_ring->txbufs[wr_idx].dma_addr,
++=======
+ 		dma_unmap_page(dp->dev, tx_ring->txbufs[wr_idx].dma_addr,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			       skb_frag_size(frag), DMA_TO_DEVICE);
  		tx_ring->txbufs[wr_idx].skb = NULL;
  		tx_ring->txbufs[wr_idx].dma_addr = 0;
@@@ -855,7 -865,7 +888,11 @@@
  		if (wr_idx < 0)
  			wr_idx += tx_ring->cnt;
  	}
++<<<<<<< HEAD
 +	dma_unmap_single(&nn->pdev->dev, tx_ring->txbufs[wr_idx].dma_addr,
++=======
+ 	dma_unmap_single(dp->dev, tx_ring->txbufs[wr_idx].dma_addr,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			 skb_headlen(skb), DMA_TO_DEVICE);
  	tx_ring->txbufs[wr_idx].skb = NULL;
  	tx_ring->txbufs[wr_idx].dma_addr = 0;
@@@ -912,8 -922,7 +949,12 @@@ static void nfp_net_tx_complete(struct 
  
  		if (fidx == -1) {
  			/* unmap head */
++<<<<<<< HEAD
 +			dma_unmap_single(&nn->pdev->dev,
 +					 tx_ring->txbufs[idx].dma_addr,
++=======
+ 			dma_unmap_single(dp->dev, tx_ring->txbufs[idx].dma_addr,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  					 skb_headlen(skb), DMA_TO_DEVICE);
  
  			done_pkts += tx_ring->txbufs[idx].pkt_cnt;
@@@ -921,8 -930,7 +962,12 @@@
  		} else {
  			/* unmap fragment */
  			frag = &skb_shinfo(skb)->frags[fidx];
++<<<<<<< HEAD
 +			dma_unmap_page(&nn->pdev->dev,
 +				       tx_ring->txbufs[idx].dma_addr,
++=======
+ 			dma_unmap_page(dp->dev, tx_ring->txbufs[idx].dma_addr,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  				       skb_frag_size(frag), DMA_TO_DEVICE);
  		}
  
@@@ -957,49 -965,106 +1002,129 @@@
  		  tx_ring->rd_p, tx_ring->wr_p, tx_ring->cnt);
  }
  
++<<<<<<< HEAD
++=======
+ static void nfp_net_xdp_complete(struct nfp_net_tx_ring *tx_ring)
+ {
+ 	struct nfp_net_r_vector *r_vec = tx_ring->r_vec;
+ 	struct nfp_net_dp *dp = &r_vec->nfp_net->dp;
+ 	u32 done_pkts = 0, done_bytes = 0;
+ 	int idx, todo;
+ 	u32 qcp_rd_p;
+ 
+ 	/* Work out how many descriptors have been transmitted */
+ 	qcp_rd_p = nfp_qcp_rd_ptr_read(tx_ring->qcp_q);
+ 
+ 	if (qcp_rd_p == tx_ring->qcp_rd_p)
+ 		return;
+ 
+ 	if (qcp_rd_p > tx_ring->qcp_rd_p)
+ 		todo = qcp_rd_p - tx_ring->qcp_rd_p;
+ 	else
+ 		todo = qcp_rd_p + tx_ring->cnt - tx_ring->qcp_rd_p;
+ 
+ 	while (todo--) {
+ 		idx = tx_ring->rd_p & (tx_ring->cnt - 1);
+ 		tx_ring->rd_p++;
+ 
+ 		if (!tx_ring->txbufs[idx].frag)
+ 			continue;
+ 
+ 		nfp_net_dma_unmap_rx(dp, tx_ring->txbufs[idx].dma_addr,
+ 				     dp->fl_bufsz, DMA_BIDIRECTIONAL);
+ 		__free_page(virt_to_page(tx_ring->txbufs[idx].frag));
+ 
+ 		done_pkts++;
+ 		done_bytes += tx_ring->txbufs[idx].real_len;
+ 
+ 		tx_ring->txbufs[idx].dma_addr = 0;
+ 		tx_ring->txbufs[idx].frag = NULL;
+ 		tx_ring->txbufs[idx].fidx = -2;
+ 	}
+ 
+ 	tx_ring->qcp_rd_p = qcp_rd_p;
+ 
+ 	u64_stats_update_begin(&r_vec->tx_sync);
+ 	r_vec->tx_bytes += done_bytes;
+ 	r_vec->tx_pkts += done_pkts;
+ 	u64_stats_update_end(&r_vec->tx_sync);
+ 
+ 	WARN_ONCE(tx_ring->wr_p - tx_ring->rd_p > tx_ring->cnt,
+ 		  "TX ring corruption rd_p=%u wr_p=%u cnt=%u\n",
+ 		  tx_ring->rd_p, tx_ring->wr_p, tx_ring->cnt);
+ }
+ 
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  /**
   * nfp_net_tx_ring_reset() - Free any untransmitted buffers and reset pointers
-  * @nn:		NFP Net device
+  * @dp:		NFP Net data path struct
   * @tx_ring:	TX ring structure
   *
   * Assumes that the device is stopped
   */
  static void
- nfp_net_tx_ring_reset(struct nfp_net *nn, struct nfp_net_tx_ring *tx_ring)
+ nfp_net_tx_ring_reset(struct nfp_net_dp *dp, struct nfp_net_tx_ring *tx_ring)
  {
 -	struct nfp_net_r_vector *r_vec = tx_ring->r_vec;
  	const struct skb_frag_struct *frag;
  	struct netdev_queue *nd_q;
 +	struct pci_dev *pdev = nn->pdev;
  
  	while (tx_ring->rd_p != tx_ring->wr_p) {
 -		struct nfp_net_tx_buf *tx_buf;
 -		int idx;
 +		int nr_frags, fidx, idx;
 +		struct sk_buff *skb;
  
  		idx = tx_ring->rd_p & (tx_ring->cnt - 1);
 -		tx_buf = &tx_ring->txbufs[idx];
 +		skb = tx_ring->txbufs[idx].skb;
 +		nr_frags = skb_shinfo(skb)->nr_frags;
 +		fidx = tx_ring->txbufs[idx].fidx;
  
++<<<<<<< HEAD
 +		if (fidx == -1) {
 +			/* unmap head */
 +			dma_unmap_single(&pdev->dev,
 +					 tx_ring->txbufs[idx].dma_addr,
 +					 skb_headlen(skb), DMA_TO_DEVICE);
 +		} else {
 +			/* unmap fragment */
 +			frag = &skb_shinfo(skb)->frags[fidx];
 +			dma_unmap_page(&pdev->dev,
 +				       tx_ring->txbufs[idx].dma_addr,
 +				       skb_frag_size(frag), DMA_TO_DEVICE);
++=======
+ 		if (tx_ring == r_vec->xdp_ring) {
+ 			nfp_net_dma_unmap_rx(dp, tx_buf->dma_addr,
+ 					     dp->fl_bufsz, DMA_BIDIRECTIONAL);
+ 			__free_page(virt_to_page(tx_ring->txbufs[idx].frag));
+ 		} else {
+ 			struct sk_buff *skb = tx_ring->txbufs[idx].skb;
+ 			int nr_frags = skb_shinfo(skb)->nr_frags;
+ 
+ 			if (tx_buf->fidx == -1) {
+ 				/* unmap head */
+ 				dma_unmap_single(dp->dev, tx_buf->dma_addr,
+ 						 skb_headlen(skb),
+ 						 DMA_TO_DEVICE);
+ 			} else {
+ 				/* unmap fragment */
+ 				frag = &skb_shinfo(skb)->frags[tx_buf->fidx];
+ 				dma_unmap_page(dp->dev, tx_buf->dma_addr,
+ 					       skb_frag_size(frag),
+ 					       DMA_TO_DEVICE);
+ 			}
+ 
+ 			/* check for last gather fragment */
+ 			if (tx_buf->fidx == nr_frags - 1)
+ 				dev_kfree_skb_any(skb);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		}
  
 -		tx_buf->dma_addr = 0;
 -		tx_buf->skb = NULL;
 -		tx_buf->fidx = -2;
 +		/* check for last gather fragment */
 +		if (fidx == nr_frags - 1)
 +			dev_kfree_skb_any(skb);
 +
 +		tx_ring->txbufs[idx].dma_addr = 0;
 +		tx_ring->txbufs[idx].skb = NULL;
 +		tx_ring->txbufs[idx].fidx = -2;
  
  		tx_ring->qcp_rd_p++;
  		tx_ring->rd_p++;
@@@ -1011,7 -1076,10 +1136,14 @@@
  	tx_ring->qcp_rd_p = 0;
  	tx_ring->wr_ptr_add = 0;
  
++<<<<<<< HEAD
 +	nd_q = netdev_get_tx_queue(nn->netdev, tx_ring->idx);
++=======
+ 	if (tx_ring == r_vec->xdp_ring)
+ 		return;
+ 
+ 	nd_q = netdev_get_tx_queue(dp->netdev, tx_ring->idx);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	netdev_tx_reset_queue(nd_q);
  }
  
@@@ -1020,7 -1088,7 +1152,11 @@@ static void nfp_net_tx_timeout(struct n
  	struct nfp_net *nn = netdev_priv(netdev);
  	int i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < nn->num_tx_rings; i++) {
++=======
+ 	for (i = 0; i < nn->dp.netdev->real_num_tx_queues; i++) {
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		if (!netif_tx_queue_stopped(netdev_get_tx_queue(netdev, i)))
  			continue;
  		nn_warn(nn, "TX timeout on ring: %d\n", i);
@@@ -1060,41 -1138,52 +1196,75 @@@ nfp_net_calc_fl_bufsz(struct nfp_net_d
   */
  static void *
  nfp_net_rx_alloc_one(struct nfp_net_rx_ring *rx_ring, dma_addr_t *dma_addr,
 -		     unsigned int fl_bufsz, bool xdp)
 +		     unsigned int fl_bufsz)
  {
++<<<<<<< HEAD
 +	struct nfp_net *nn = rx_ring->r_vec->nfp_net;
++=======
+ 	struct nfp_net_dp *dp = &rx_ring->r_vec->nfp_net->dp;
+ 	int direction;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	void *frag;
  
 -	if (!xdp)
 -		frag = netdev_alloc_frag(fl_bufsz);
 -	else
 -		frag = page_address(alloc_page(GFP_KERNEL | __GFP_COLD));
 +	frag = netdev_alloc_frag(fl_bufsz);
  	if (!frag) {
- 		nn_warn_ratelimit(nn, "Failed to alloc receive page frag\n");
+ 		nn_dp_warn(dp, "Failed to alloc receive page frag\n");
  		return NULL;
  	}
  
++<<<<<<< HEAD
 +	*dma_addr = nfp_net_dma_map_rx(nn, frag, fl_bufsz, DMA_FROM_DEVICE);
 +	if (dma_mapping_error(&nn->pdev->dev, *dma_addr)) {
 +		skb_free_frag(frag);
 +		nn_warn_ratelimit(nn, "Failed to map DMA RX buffer\n");
++=======
+ 	direction = xdp ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
+ 
+ 	*dma_addr = nfp_net_dma_map_rx(dp, frag, fl_bufsz, direction);
+ 	if (dma_mapping_error(dp->dev, *dma_addr)) {
+ 		nfp_net_free_frag(frag, xdp);
+ 		nn_dp_warn(dp, "Failed to map DMA RX buffer\n");
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		return NULL;
  	}
  
  	return frag;
  }
  
++<<<<<<< HEAD
 +static void *nfp_net_napi_alloc_one(struct nfp_net *nn, dma_addr_t *dma_addr)
 +{
 +	void *frag;
 +
 +	frag = napi_alloc_frag(nn->fl_bufsz);
++=======
+ static void *
+ nfp_net_napi_alloc_one(struct nfp_net_dp *dp, int direction,
+ 		       dma_addr_t *dma_addr)
+ {
+ 	void *frag;
+ 
+ 	if (!dp->xdp_prog)
+ 		frag = napi_alloc_frag(dp->fl_bufsz);
+ 	else
+ 		frag = page_address(alloc_page(GFP_ATOMIC | __GFP_COLD));
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	if (!frag) {
- 		nn_warn_ratelimit(nn, "Failed to alloc receive page frag\n");
+ 		nn_dp_warn(dp, "Failed to alloc receive page frag\n");
  		return NULL;
  	}
  
++<<<<<<< HEAD
 +	*dma_addr = nfp_net_dma_map_rx(nn, frag, nn->fl_bufsz, DMA_FROM_DEVICE);
 +	if (dma_mapping_error(&nn->pdev->dev, *dma_addr)) {
 +		skb_free_frag(frag);
 +		nn_warn_ratelimit(nn, "Failed to map DMA RX buffer\n");
++=======
+ 	*dma_addr = nfp_net_dma_map_rx(dp, frag, dp->fl_bufsz, direction);
+ 	if (dma_mapping_error(dp->dev, *dma_addr)) {
+ 		nfp_net_free_frag(frag, dp->xdp_prog);
+ 		nn_dp_warn(dp, "Failed to map DMA RX buffer\n");
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		return NULL;
  	}
  
@@@ -1162,16 -1251,19 +1332,21 @@@ static void nfp_net_rx_ring_reset(struc
  
  /**
   * nfp_net_rx_ring_bufs_free() - Free any buffers currently on the RX ring
-  * @nn:		NFP Net device
+  * @dp:		NFP Net data path struct
   * @rx_ring:	RX ring to remove buffers from
 - * @xdp:	Whether XDP is enabled
   *
   * Assumes that the device is stopped and buffers are in [0, ring->cnt - 1)
   * entries.  After device is disabled nfp_net_rx_ring_reset() must be called
   * to restore required ring geometry.
   */
  static void
++<<<<<<< HEAD
 +nfp_net_rx_ring_bufs_free(struct nfp_net *nn, struct nfp_net_rx_ring *rx_ring)
++=======
+ nfp_net_rx_ring_bufs_free(struct nfp_net_dp *dp,
+ 			  struct nfp_net_rx_ring *rx_ring, bool xdp)
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  {
 -	int direction = xdp ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
  	unsigned int i;
  
  	for (i = 0; i < rx_ring->cnt - 1; i++) {
@@@ -1182,9 -1274,9 +1357,15 @@@
  		if (!rx_ring->rxbufs[i].frag)
  			continue;
  
++<<<<<<< HEAD
 +		nfp_net_dma_unmap_rx(nn, rx_ring->rxbufs[i].dma_addr,
 +				     rx_ring->bufsz, DMA_FROM_DEVICE);
 +		skb_free_frag(rx_ring->rxbufs[i].frag);
++=======
+ 		nfp_net_dma_unmap_rx(dp, rx_ring->rxbufs[i].dma_addr,
+ 				     rx_ring->bufsz, direction);
+ 		nfp_net_free_frag(rx_ring->rxbufs[i].frag, xdp);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		rx_ring->rxbufs[i].dma_addr = 0;
  		rx_ring->rxbufs[i].frag = NULL;
  	}
@@@ -1192,11 -1284,13 +1373,16 @@@
  
  /**
   * nfp_net_rx_ring_bufs_alloc() - Fill RX ring with buffers (don't give to FW)
-  * @nn:		NFP Net device
+  * @dp:		NFP Net data path struct
   * @rx_ring:	RX ring to remove buffers from
 - * @xdp:	Whether XDP is enabled
   */
  static int
++<<<<<<< HEAD
 +nfp_net_rx_ring_bufs_alloc(struct nfp_net *nn, struct nfp_net_rx_ring *rx_ring)
++=======
+ nfp_net_rx_ring_bufs_alloc(struct nfp_net_dp *dp,
+ 			   struct nfp_net_rx_ring *rx_ring, bool xdp)
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  {
  	struct nfp_net_rx_buf *rxbufs;
  	unsigned int i;
@@@ -1206,9 -1300,9 +1392,13 @@@
  	for (i = 0; i < rx_ring->cnt - 1; i++) {
  		rxbufs[i].frag =
  			nfp_net_rx_alloc_one(rx_ring, &rxbufs[i].dma_addr,
 -					     rx_ring->bufsz, xdp);
 +					     rx_ring->bufsz);
  		if (!rxbufs[i].frag) {
++<<<<<<< HEAD
 +			nfp_net_rx_ring_bufs_free(nn, rx_ring);
++=======
+ 			nfp_net_rx_ring_bufs_free(dp, rx_ring, xdp);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			return -ENOMEM;
  		}
  	}
@@@ -1351,6 -1466,69 +1542,72 @@@ nfp_net_rx_drop(struct nfp_net_r_vecto
  		dev_kfree_skb_any(skb);
  }
  
++<<<<<<< HEAD
++=======
+ static bool
+ nfp_net_tx_xdp_buf(struct nfp_net_dp *dp, struct nfp_net_rx_ring *rx_ring,
+ 		   struct nfp_net_tx_ring *tx_ring,
+ 		   struct nfp_net_rx_buf *rxbuf, unsigned int pkt_off,
+ 		   unsigned int pkt_len)
+ {
+ 	struct nfp_net_tx_buf *txbuf;
+ 	struct nfp_net_tx_desc *txd;
+ 	dma_addr_t new_dma_addr;
+ 	void *new_frag;
+ 	int wr_idx;
+ 
+ 	if (unlikely(nfp_net_tx_full(tx_ring, 1))) {
+ 		nfp_net_rx_drop(rx_ring->r_vec, rx_ring, rxbuf, NULL);
+ 		return false;
+ 	}
+ 
+ 	new_frag = nfp_net_napi_alloc_one(dp, DMA_BIDIRECTIONAL, &new_dma_addr);
+ 	if (unlikely(!new_frag)) {
+ 		nfp_net_rx_drop(rx_ring->r_vec, rx_ring, rxbuf, NULL);
+ 		return false;
+ 	}
+ 	nfp_net_rx_give_one(rx_ring, new_frag, new_dma_addr);
+ 
+ 	wr_idx = tx_ring->wr_p & (tx_ring->cnt - 1);
+ 
+ 	/* Stash the soft descriptor of the head then initialize it */
+ 	txbuf = &tx_ring->txbufs[wr_idx];
+ 	txbuf->frag = rxbuf->frag;
+ 	txbuf->dma_addr = rxbuf->dma_addr;
+ 	txbuf->fidx = -1;
+ 	txbuf->pkt_cnt = 1;
+ 	txbuf->real_len = pkt_len;
+ 
+ 	dma_sync_single_for_device(dp->dev, rxbuf->dma_addr + pkt_off,
+ 				   pkt_len, DMA_BIDIRECTIONAL);
+ 
+ 	/* Build TX descriptor */
+ 	txd = &tx_ring->txds[wr_idx];
+ 	txd->offset_eop = PCIE_DESC_TX_EOP;
+ 	txd->dma_len = cpu_to_le16(pkt_len);
+ 	nfp_desc_set_dma_addr(txd, rxbuf->dma_addr + pkt_off);
+ 	txd->data_len = cpu_to_le16(pkt_len);
+ 
+ 	txd->flags = 0;
+ 	txd->mss = 0;
+ 	txd->l4_offset = 0;
+ 
+ 	tx_ring->wr_p++;
+ 	tx_ring->wr_ptr_add++;
+ 	return true;
+ }
+ 
+ static int nfp_net_run_xdp(struct bpf_prog *prog, void *data, unsigned int len)
+ {
+ 	struct xdp_buff xdp;
+ 
+ 	xdp.data = data;
+ 	xdp.data_end = data + len;
+ 
+ 	return bpf_prog_run_xdp(prog, &xdp);
+ }
+ 
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  /**
   * nfp_net_rx() - receive up to @budget packets on @rx_ring
   * @rx_ring:   RX ring to receive from
@@@ -1365,11 -1543,21 +1622,27 @@@
  static int nfp_net_rx(struct nfp_net_rx_ring *rx_ring, int budget)
  {
  	struct nfp_net_r_vector *r_vec = rx_ring->r_vec;
++<<<<<<< HEAD
 +	struct nfp_net *nn = r_vec->nfp_net;
++=======
+ 	struct nfp_net_dp *dp = &r_vec->nfp_net->dp;
+ 	struct nfp_net_tx_ring *tx_ring;
+ 	struct bpf_prog *xdp_prog;
+ 	unsigned int true_bufsz;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	struct sk_buff *skb;
  	int pkts_polled = 0;
 -	int rx_dma_map_dir;
  	int idx;
  
++<<<<<<< HEAD
++=======
+ 	rcu_read_lock();
+ 	xdp_prog = READ_ONCE(dp->xdp_prog);
+ 	rx_dma_map_dir = xdp_prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
+ 	true_bufsz = xdp_prog ? PAGE_SIZE : dp->fl_bufsz;
+ 	tx_ring = r_vec->xdp_ring;
+ 
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	while (pkts_polled < budget) {
  		unsigned int meta_len, data_len, data_off, pkt_len, pkt_off;
  		struct nfp_net_rx_buf *rxbuf;
@@@ -1420,34 -1608,74 +1693,94 @@@
  		r_vec->rx_bytes += pkt_len;
  		u64_stats_update_end(&r_vec->rx_sync);
  
++<<<<<<< HEAD
 +		skb = build_skb(rxbuf->frag, nn->fl_bufsz);
++=======
+ 		if (xdp_prog && !(rxd->rxd.flags & PCIE_DESC_RX_BPF &&
+ 				  dp->bpf_offload_xdp)) {
+ 			int act;
+ 
+ 			dma_sync_single_for_cpu(dp->dev,
+ 						rxbuf->dma_addr + pkt_off,
+ 						pkt_len, DMA_BIDIRECTIONAL);
+ 			act = nfp_net_run_xdp(xdp_prog, rxbuf->frag + data_off,
+ 					      pkt_len);
+ 			switch (act) {
+ 			case XDP_PASS:
+ 				break;
+ 			case XDP_TX:
+ 				if (unlikely(!nfp_net_tx_xdp_buf(dp, rx_ring,
+ 								 tx_ring, rxbuf,
+ 								 pkt_off,
+ 								 pkt_len)))
+ 					trace_xdp_exception(dp->netdev,
+ 							    xdp_prog, act);
+ 				continue;
+ 			default:
+ 				bpf_warn_invalid_xdp_action(act);
+ 			case XDP_ABORTED:
+ 				trace_xdp_exception(dp->netdev, xdp_prog, act);
+ 			case XDP_DROP:
+ 				nfp_net_rx_give_one(rx_ring, rxbuf->frag,
+ 						    rxbuf->dma_addr);
+ 				continue;
+ 			}
+ 		}
+ 
+ 		skb = build_skb(rxbuf->frag, true_bufsz);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		if (unlikely(!skb)) {
  			nfp_net_rx_drop(r_vec, rx_ring, rxbuf, NULL);
  			continue;
  		}
++<<<<<<< HEAD
 +
 +		nfp_net_set_hash(nn->netdev, skb, rxd);
 +
 +		new_frag = nfp_net_napi_alloc_one(nn, &new_dma_addr);
++=======
+ 		new_frag = nfp_net_napi_alloc_one(dp, rx_dma_map_dir,
+ 						  &new_dma_addr);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		if (unlikely(!new_frag)) {
  			nfp_net_rx_drop(r_vec, rx_ring, rxbuf, skb);
  			continue;
  		}
  
++<<<<<<< HEAD
 +		nfp_net_dma_unmap_rx(nn, rx_ring->rxbufs[idx].dma_addr,
 +				     nn->fl_bufsz, DMA_FROM_DEVICE);
++=======
+ 		nfp_net_dma_unmap_rx(dp, rxbuf->dma_addr, dp->fl_bufsz,
+ 				     rx_dma_map_dir);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  		nfp_net_rx_give_one(rx_ring, new_frag, new_dma_addr);
  
  		skb_reserve(skb, data_off);
  		skb_put(skb, pkt_len);
  
++<<<<<<< HEAD
 +		nfp_net_set_hash_desc(nn->netdev, skb, rxd);
++=======
+ 		if (!dp->chained_metadata_format) {
+ 			nfp_net_set_hash_desc(dp->netdev, skb, rxd);
+ 		} else if (meta_len) {
+ 			void *end;
+ 
+ 			end = nfp_net_parse_meta(dp->netdev, skb, meta_len);
+ 			if (unlikely(end != skb->data)) {
+ 				nn_dp_warn(dp, "invalid RX packet metadata\n");
+ 				nfp_net_rx_drop(r_vec, rx_ring, NULL, skb);
+ 				continue;
+ 			}
+ 		}
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  		skb_record_rx_queue(skb, rx_ring->idx);
- 		skb->protocol = eth_type_trans(skb, nn->netdev);
+ 		skb->protocol = eth_type_trans(skb, dp->netdev);
  
- 		nfp_net_rx_csum(nn, r_vec, rxd, skb);
+ 		nfp_net_rx_csum(dp, r_vec, rxd, skb);
  
  		if (rxd->rxd.flags & PCIE_DESC_RX_VLAN)
  			__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
@@@ -1494,13 -1729,12 +1827,21 @@@ static int nfp_net_poll(struct napi_str
  static void nfp_net_tx_ring_free(struct nfp_net_tx_ring *tx_ring)
  {
  	struct nfp_net_r_vector *r_vec = tx_ring->r_vec;
++<<<<<<< HEAD
 +	struct nfp_net *nn = r_vec->nfp_net;
 +	struct pci_dev *pdev = nn->pdev;
++=======
+ 	struct nfp_net_dp *dp = &r_vec->nfp_net->dp;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  	kfree(tx_ring->txbufs);
  
  	if (tx_ring->txds)
++<<<<<<< HEAD
 +		dma_free_coherent(&pdev->dev, tx_ring->size,
++=======
+ 		dma_free_coherent(dp->dev, tx_ring->size,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  				  tx_ring->txds, tx_ring->dma);
  
  	tx_ring->cnt = 0;
@@@ -1517,17 -1751,18 +1858,25 @@@
   *
   * Return: 0 on success, negative errno otherwise.
   */
 -static int
 -nfp_net_tx_ring_alloc(struct nfp_net_tx_ring *tx_ring, u32 cnt, bool is_xdp)
 +static int nfp_net_tx_ring_alloc(struct nfp_net_tx_ring *tx_ring, u32 cnt)
  {
  	struct nfp_net_r_vector *r_vec = tx_ring->r_vec;
++<<<<<<< HEAD
 +	struct nfp_net *nn = r_vec->nfp_net;
 +	struct pci_dev *pdev = nn->pdev;
++=======
+ 	struct nfp_net_dp *dp = &r_vec->nfp_net->dp;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	int sz;
  
  	tx_ring->cnt = cnt;
  
  	tx_ring->size = sizeof(*tx_ring->txds) * tx_ring->cnt;
++<<<<<<< HEAD
 +	tx_ring->txds = dma_zalloc_coherent(&pdev->dev, tx_ring->size,
++=======
+ 	tx_ring->txds = dma_zalloc_coherent(dp->dev, tx_ring->size,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  					    &tx_ring->dma, GFP_KERNEL);
  	if (!tx_ring->txds)
  		goto err_alloc;
@@@ -1537,11 -1772,9 +1886,17 @@@
  	if (!tx_ring->txbufs)
  		goto err_alloc;
  
++<<<<<<< HEAD
 +	netif_set_xps_queue(nn->netdev, &r_vec->affinity_mask, tx_ring->idx);
 +
 +	nn_dbg(nn, "TxQ%02d: QCidx=%02d cnt=%d dma=%#llx host=%p\n",
 +	       tx_ring->idx, tx_ring->qcidx,
 +	       tx_ring->cnt, (unsigned long long)tx_ring->dma, tx_ring->txds);
++=======
+ 	if (!is_xdp)
+ 		netif_set_xps_queue(dp->netdev, &r_vec->affinity_mask,
+ 				    tx_ring->idx);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  	return 0;
  
@@@ -1609,13 -1847,12 +1963,21 @@@ static void nfp_net_tx_ring_set_free(st
  static void nfp_net_rx_ring_free(struct nfp_net_rx_ring *rx_ring)
  {
  	struct nfp_net_r_vector *r_vec = rx_ring->r_vec;
++<<<<<<< HEAD
 +	struct nfp_net *nn = r_vec->nfp_net;
 +	struct pci_dev *pdev = nn->pdev;
++=======
+ 	struct nfp_net_dp *dp = &r_vec->nfp_net->dp;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  	kfree(rx_ring->rxbufs);
  
  	if (rx_ring->rxds)
++<<<<<<< HEAD
 +		dma_free_coherent(&pdev->dev, rx_ring->size,
++=======
+ 		dma_free_coherent(dp->dev, rx_ring->size,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  				  rx_ring->rxds, rx_ring->dma);
  
  	rx_ring->cnt = 0;
@@@ -1638,15 -1875,14 +2000,23 @@@ nfp_net_rx_ring_alloc(struct nfp_net_rx
  		      u32 cnt)
  {
  	struct nfp_net_r_vector *r_vec = rx_ring->r_vec;
++<<<<<<< HEAD
 +	struct nfp_net *nn = r_vec->nfp_net;
 +	struct pci_dev *pdev = nn->pdev;
++=======
+ 	struct nfp_net_dp *dp = &r_vec->nfp_net->dp;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	int sz;
  
  	rx_ring->cnt = cnt;
  	rx_ring->bufsz = fl_bufsz;
  
  	rx_ring->size = sizeof(*rx_ring->rxds) * rx_ring->cnt;
++<<<<<<< HEAD
 +	rx_ring->rxds = dma_zalloc_coherent(&pdev->dev, rx_ring->size,
++=======
+ 	rx_ring->rxds = dma_zalloc_coherent(dp->dev, rx_ring->size,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  					    &rx_ring->dma, GFP_KERNEL);
  	if (!rx_ring->rxds)
  		goto err_alloc;
@@@ -1668,9 -1900,10 +2038,9 @@@ err_alloc
  }
  
  static struct nfp_net_rx_ring *
 -nfp_net_rx_ring_set_prepare(struct nfp_net *nn, struct nfp_net_ring_set *s,
 -			    bool xdp)
 +nfp_net_rx_ring_set_prepare(struct nfp_net *nn, struct nfp_net_ring_set *s)
  {
- 	unsigned int fl_bufsz =	nfp_net_calc_fl_bufsz(nn, s->mtu);
+ 	unsigned int fl_bufsz =	nfp_net_calc_fl_bufsz(&nn->dp, s->mtu);
  	struct nfp_net_rx_ring *rings;
  	unsigned int r;
  
@@@ -1684,7 -1917,7 +2054,11 @@@
  		if (nfp_net_rx_ring_alloc(&rings[r], fl_bufsz, s->dcnt))
  			goto err_free_prev;
  
++<<<<<<< HEAD
 +		if (nfp_net_rx_ring_bufs_alloc(nn, &rings[r]))
++=======
+ 		if (nfp_net_rx_ring_bufs_alloc(&nn->dp, &rings[r], xdp))
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			goto err_free_ring;
  	}
  
@@@ -1692,7 -1925,7 +2066,11 @@@
  
  err_free_prev:
  	while (r--) {
++<<<<<<< HEAD
 +		nfp_net_rx_ring_bufs_free(nn, &rings[r]);
++=======
+ 		nfp_net_rx_ring_bufs_free(&nn->dp, &rings[r], xdp);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  err_free_ring:
  		nfp_net_rx_ring_free(&rings[r]);
  	}
@@@ -1718,13 -1951,14 +2096,22 @@@ nfp_net_rx_ring_set_swap(struct nfp_ne
  }
  
  static void
++<<<<<<< HEAD
 +nfp_net_rx_ring_set_free(struct nfp_net *nn, struct nfp_net_ring_set *s)
++=======
+ nfp_net_rx_ring_set_free(struct nfp_net_dp *dp, struct nfp_net_ring_set *s,
+ 			 bool xdp)
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  {
  	struct nfp_net_rx_ring *rings = s->rings;
  	unsigned int r;
  
  	for (r = 0; r < s->n_rings; r++) {
++<<<<<<< HEAD
 +		nfp_net_rx_ring_bufs_free(nn, &rings[r]);
++=======
+ 		nfp_net_rx_ring_bufs_free(dp, &rings[r], xdp);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		nfp_net_rx_ring_free(&rings[r]);
  	}
  
@@@ -1732,11 -1966,15 +2119,20 @@@
  }
  
  static void
- nfp_net_vector_assign_rings(struct nfp_net *nn, struct nfp_net_r_vector *r_vec,
- 			    int idx)
+ nfp_net_vector_assign_rings(struct nfp_net_dp *dp,
+ 			    struct nfp_net_r_vector *r_vec, int idx)
  {
++<<<<<<< HEAD
 +	r_vec->rx_ring = idx < nn->num_rx_rings ? &nn->rx_rings[idx] : NULL;
 +	r_vec->tx_ring = idx < nn->num_tx_rings ? &nn->tx_rings[idx] : NULL;
++=======
+ 	r_vec->rx_ring = idx < dp->num_rx_rings ? &dp->rx_rings[idx] : NULL;
+ 	r_vec->tx_ring =
+ 		idx < dp->num_stack_tx_rings ? &dp->tx_rings[idx] : NULL;
+ 
+ 	r_vec->xdp_ring = idx < dp->num_tx_rings - dp->num_stack_tx_rings ?
+ 		&dp->tx_rings[dp->num_stack_tx_rings + idx] : NULL;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  }
  
  static int
@@@ -2057,22 -2295,23 +2453,37 @@@ static int nfp_net_netdev_open(struct n
  			goto err_cleanup_vec_p;
  	}
  
++<<<<<<< HEAD
 +	nn->rx_rings = nfp_net_rx_ring_set_prepare(nn, &rx);
 +	if (!nn->rx_rings) {
++=======
+ 	nn->dp.rx_rings = nfp_net_rx_ring_set_prepare(nn, &rx, nn->dp.xdp_prog);
+ 	if (!nn->dp.rx_rings) {
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		err = -ENOMEM;
  		goto err_cleanup_vec;
  	}
  
++<<<<<<< HEAD
 +	nn->tx_rings = nfp_net_tx_ring_set_prepare(nn, &tx);
 +	if (!nn->tx_rings) {
++=======
+ 	nn->dp.tx_rings = nfp_net_tx_ring_set_prepare(nn, &tx,
+ 						   nn->dp.num_stack_tx_rings);
+ 	if (!nn->dp.tx_rings) {
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		err = -ENOMEM;
  		goto err_free_rx_rings;
  	}
  
  	for (r = 0; r < nn->max_r_vecs; r++)
- 		nfp_net_vector_assign_rings(nn, &nn->r_vecs[r], r);
+ 		nfp_net_vector_assign_rings(&nn->dp, &nn->r_vecs[r], r);
  
++<<<<<<< HEAD
 +	err = netif_set_real_num_tx_queues(netdev, nn->num_tx_rings);
++=======
+ 	err = netif_set_real_num_tx_queues(netdev, nn->dp.num_stack_tx_rings);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	if (err)
  		goto err_free_rings;
  
@@@ -2102,11 -2341,11 +2513,15 @@@
  	return 0;
  
  err_free_rings:
- 	nfp_net_tx_ring_set_free(nn, &tx);
+ 	nfp_net_tx_ring_set_free(&tx);
  err_free_rx_rings:
++<<<<<<< HEAD
 +	nfp_net_rx_ring_set_free(nn, &rx);
++=======
+ 	nfp_net_rx_ring_set_free(&nn->dp, &rx, nn->dp.xdp_prog);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  err_cleanup_vec:
- 	r = nn->num_r_vecs;
+ 	r = nn->dp.num_r_vecs;
  err_cleanup_vec_p:
  	while (r--)
  		nfp_net_cleanup_vector(nn, &nn->r_vecs[r]);
@@@ -2144,17 -2383,18 +2559,24 @@@ static void nfp_net_close_free_all(stru
  {
  	unsigned int r;
  
++<<<<<<< HEAD
 +	for (r = 0; r < nn->num_rx_rings; r++) {
 +		nfp_net_rx_ring_bufs_free(nn, &nn->rx_rings[r]);
 +		nfp_net_rx_ring_free(&nn->rx_rings[r]);
- 	}
- 	for (r = 0; r < nn->num_tx_rings; r++)
- 		nfp_net_tx_ring_free(&nn->tx_rings[r]);
- 	for (r = 0; r < nn->num_r_vecs; r++)
++=======
+ 	for (r = 0; r < nn->dp.num_rx_rings; r++) {
+ 		nfp_net_rx_ring_bufs_free(&nn->dp, &nn->dp.rx_rings[r],
+ 					  nn->dp.xdp_prog);
+ 		nfp_net_rx_ring_free(&nn->dp.rx_rings[r]);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
+ 	}
+ 	for (r = 0; r < nn->dp.num_tx_rings; r++)
+ 		nfp_net_tx_ring_free(&nn->dp.tx_rings[r]);
+ 	for (r = 0; r < nn->dp.num_r_vecs; r++)
  		nfp_net_cleanup_vector(nn, &nn->r_vecs[r]);
  
- 	kfree(nn->rx_rings);
- 	kfree(nn->tx_rings);
+ 	kfree(nn->dp.rx_rings);
+ 	kfree(nn->dp.tx_rings);
  
  	nfp_net_aux_irq_free(nn, NFP_NET_CFG_LSC, NFP_NET_IRQ_LSC_IDX);
  	nfp_net_aux_irq_free(nn, NFP_NET_CFG_EXN, NFP_NET_IRQ_EXN_IDX);
@@@ -2236,22 -2478,23 +2658,33 @@@ nfp_net_ring_swap_enable(struct nfp_ne
  	if (tx)
  		nfp_net_tx_ring_set_swap(nn, tx);
  
++<<<<<<< HEAD
 +	swap(*num_vecs, nn->num_r_vecs);
++=======
+ 	swap(*num_vecs, nn->dp.num_r_vecs);
+ 	swap(*stack_tx_rings, nn->dp.num_stack_tx_rings);
+ 	*xdp_prog = xchg(&nn->dp.xdp_prog, *xdp_prog);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  	for (r = 0; r <	nn->max_r_vecs; r++)
- 		nfp_net_vector_assign_rings(nn, &nn->r_vecs[r], r);
+ 		nfp_net_vector_assign_rings(&nn->dp, &nn->r_vecs[r], r);
  
- 	if (!netif_is_rxfh_configured(nn->netdev))
+ 	if (!netif_is_rxfh_configured(nn->dp.netdev))
  		nfp_net_rss_init_itbl(nn);
  
- 	err = netif_set_real_num_rx_queues(nn->netdev,
- 					   nn->num_rx_rings);
+ 	err = netif_set_real_num_rx_queues(nn->dp.netdev, nn->dp.num_rx_rings);
  	if (err)
  		return err;
  
++<<<<<<< HEAD
 +	if (nn->netdev->real_num_tx_queues != nn->num_tx_rings) {
 +		err = netif_set_real_num_tx_queues(nn->netdev,
 +						   nn->num_tx_rings);
++=======
+ 	if (nn->dp.netdev->real_num_tx_queues != nn->dp.num_stack_tx_rings) {
+ 		err = netif_set_real_num_tx_queues(nn->dp.netdev,
+ 						   nn->dp.num_stack_tx_rings);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		if (err)
  			return err;
  	}
@@@ -2259,36 -2502,65 +2692,86 @@@
  	return __nfp_net_set_config_and_enable(nn);
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ nfp_net_check_config(struct nfp_net *nn, struct bpf_prog *xdp_prog,
+ 		     struct nfp_net_ring_set *rx, struct nfp_net_ring_set *tx)
+ {
+ 	/* XDP-enabled tests */
+ 	if (!xdp_prog)
+ 		return 0;
+ 	if (rx && nfp_net_calc_fl_bufsz(&nn->dp, rx->mtu) > PAGE_SIZE) {
+ 		nn_warn(nn, "MTU too large w/ XDP enabled\n");
+ 		return -EINVAL;
+ 	}
+ 	if (tx && tx->n_rings > nn->max_tx_rings) {
+ 		nn_warn(nn, "Insufficient number of TX rings w/ XDP enabled\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  static void
 -nfp_net_ring_reconfig_down(struct nfp_net *nn, struct bpf_prog **xdp_prog,
 +nfp_net_ring_reconfig_down(struct nfp_net *nn,
  			   struct nfp_net_ring_set *rx,
  			   struct nfp_net_ring_set *tx,
 -			   unsigned int stack_tx_rings, unsigned int num_vecs)
 -{
 +			   unsigned int num_vecs)
 +{
++<<<<<<< HEAD
 +	nn->netdev->mtu = rx ? rx->mtu : nn->netdev->mtu;
 +	nn->fl_bufsz = nfp_net_calc_fl_bufsz(nn, nn->netdev->mtu);
 +	nn->rxd_cnt = rx ? rx->dcnt : nn->rxd_cnt;
 +	nn->txd_cnt = tx ? tx->dcnt : nn->txd_cnt;
 +	nn->num_rx_rings = rx ? rx->n_rings : nn->num_rx_rings;
 +	nn->num_tx_rings = tx ? tx->n_rings : nn->num_tx_rings;
 +	nn->num_r_vecs = num_vecs;
- 
- 	if (!netif_is_rxfh_configured(nn->netdev))
++=======
+ 	nn->dp.netdev->mtu = rx ? rx->mtu : nn->dp.netdev->mtu;
+ 	nn->dp.fl_bufsz = nfp_net_calc_fl_bufsz(&nn->dp, nn->dp.netdev->mtu);
+ 	nn->dp.rxd_cnt = rx ? rx->dcnt : nn->dp.rxd_cnt;
+ 	nn->dp.txd_cnt = tx ? tx->dcnt : nn->dp.txd_cnt;
+ 	nn->dp.num_rx_rings = rx ? rx->n_rings : nn->dp.num_rx_rings;
+ 	nn->dp.num_tx_rings = tx ? tx->n_rings : nn->dp.num_tx_rings;
+ 	nn->dp.num_stack_tx_rings = stack_tx_rings;
+ 	nn->dp.num_r_vecs = num_vecs;
+ 	*xdp_prog = xchg(&nn->dp.xdp_prog, *xdp_prog);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
+ 
+ 	if (!netif_is_rxfh_configured(nn->dp.netdev))
  		nfp_net_rss_init_itbl(nn);
  }
  
  int
 -nfp_net_ring_reconfig(struct nfp_net *nn, struct bpf_prog **xdp_prog,
 -		      struct nfp_net_ring_set *rx, struct nfp_net_ring_set *tx)
 +nfp_net_ring_reconfig(struct nfp_net *nn, struct nfp_net_ring_set *rx,
 +		      struct nfp_net_ring_set *tx)
  {
 -	unsigned int stack_tx_rings, num_vecs, r;
 +	unsigned int num_vecs, r;
  	int err;
  
++<<<<<<< HEAD
 +	num_vecs = max(rx ? rx->n_rings : nn->num_rx_rings,
 +		       tx ? tx->n_rings : nn->num_tx_rings);
 +
 +	if (!netif_running(nn->netdev)) {
 +		nfp_net_ring_reconfig_down(nn, rx, tx, num_vecs);
++=======
+ 	stack_tx_rings = tx ? tx->n_rings : nn->dp.num_tx_rings;
+ 	if (*xdp_prog)
+ 		stack_tx_rings -= rx ? rx->n_rings : nn->dp.num_rx_rings;
+ 
+ 	num_vecs = max(rx ? rx->n_rings : nn->dp.num_rx_rings, stack_tx_rings);
+ 
+ 	err = nfp_net_check_config(nn, *xdp_prog, rx, tx);
+ 	if (err)
+ 		return err;
+ 
+ 	if (!netif_running(nn->dp.netdev)) {
+ 		nfp_net_ring_reconfig_down(nn, xdp_prog, rx, tx,
+ 					   stack_tx_rings, num_vecs);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		return 0;
  	}
  
@@@ -2333,9 -2607,9 +2816,13 @@@
  		nfp_net_cleanup_vector(nn, &nn->r_vecs[r]);
  
  	if (rx)
++<<<<<<< HEAD
 +		nfp_net_rx_ring_set_free(nn, rx);
++=======
+ 		nfp_net_rx_ring_set_free(&nn->dp, rx, *xdp_prog);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	if (tx)
- 		nfp_net_tx_ring_set_free(nn, tx);
+ 		nfp_net_tx_ring_set_free(tx);
  
  	nfp_net_open_stack(nn);
  
@@@ -2343,9 -2617,9 +2830,13 @@@
  
  err_free_rx:
  	if (rx)
++<<<<<<< HEAD
 +		nfp_net_rx_ring_set_free(nn, rx);
++=======
+ 		nfp_net_rx_ring_set_free(&nn->dp, rx, *xdp_prog);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  err_cleanup_vecs:
- 	for (r = num_vecs - 1; r >= nn->num_r_vecs; r--)
+ 	for (r = num_vecs - 1; r >= nn->dp.num_r_vecs; r--)
  		nfp_net_cleanup_vector(nn, &nn->r_vecs[r]);
  	return err;
  }
@@@ -2354,16 -2628,16 +2845,20 @@@ static int nfp_net_change_mtu(struct ne
  {
  	struct nfp_net *nn = netdev_priv(netdev);
  	struct nfp_net_ring_set rx = {
- 		.n_rings = nn->num_rx_rings,
+ 		.n_rings = nn->dp.num_rx_rings,
  		.mtu = new_mtu,
- 		.dcnt = nn->rxd_cnt,
+ 		.dcnt = nn->dp.rxd_cnt,
  	};
  
++<<<<<<< HEAD
 +	return nfp_net_ring_reconfig(nn, &rx, NULL);
++=======
+ 	return nfp_net_ring_reconfig(nn, &nn->dp.xdp_prog, &rx, NULL);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  }
  
 -static void nfp_net_stat64(struct net_device *netdev,
 -			   struct rtnl_link_stats64 *stats)
 +static struct rtnl_link_stats64 *nfp_net_stat64(struct net_device *netdev,
 +						struct rtnl_link_stats64 *stats)
  {
  	struct nfp_net *nn = netdev_priv(netdev);
  	int r;
@@@ -2393,8 -2667,35 +2888,38 @@@
  		stats->tx_bytes += data[1];
  		stats->tx_errors += data[2];
  	}
 -}
  
++<<<<<<< HEAD
 +	return stats;
++=======
+ static bool nfp_net_ebpf_capable(struct nfp_net *nn)
+ {
+ 	if (nn->cap & NFP_NET_CFG_CTRL_BPF &&
+ 	    nn_readb(nn, NFP_NET_CFG_BPF_ABI) == NFP_NET_BPF_ABI)
+ 		return true;
+ 	return false;
+ }
+ 
+ static int
+ nfp_net_setup_tc(struct net_device *netdev, u32 handle, __be16 proto,
+ 		 struct tc_to_netdev *tc)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 
+ 	if (TC_H_MAJ(handle) != TC_H_MAJ(TC_H_INGRESS))
+ 		return -ENOTSUPP;
+ 	if (proto != htons(ETH_P_ALL))
+ 		return -ENOTSUPP;
+ 
+ 	if (tc->type == TC_SETUP_CLSBPF && nfp_net_ebpf_capable(nn)) {
+ 		if (!nn->dp.bpf_offload_xdp)
+ 			return nfp_net_bpf_offload(nn, tc->cls_bpf);
+ 		else
+ 			return -EBUSY;
+ 	}
+ 
+ 	return -EINVAL;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  }
  
  static int nfp_net_set_features(struct net_device *netdev,
@@@ -2451,6 -2752,11 +2976,14 @@@
  			new_ctrl &= ~NFP_NET_CFG_CTRL_GATHER;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (changed & NETIF_F_HW_TC && nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF) {
+ 		nn_err(nn, "Cannot disable HW TC offload while in use\n");
+ 		return -EBUSY;
+ 	}
+ 
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	nn_dbg(nn, "Feature change 0x%llx -> 0x%llx (changed=0x%llx)\n",
  	       netdev->features, features, changed);
  
@@@ -2595,8 -2921,92 +3128,96 @@@ static void nfp_net_del_vxlan_port(stru
  		nfp_net_set_vxlan_port(nn, idx, 0);
  }
  
++<<<<<<< HEAD
++=======
+ static int nfp_net_xdp_offload(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct tc_cls_bpf_offload cmd = {
+ 		.prog = prog,
+ 	};
+ 	int ret;
+ 
+ 	if (!nfp_net_ebpf_capable(nn))
+ 		return -EINVAL;
+ 
+ 	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF) {
+ 		if (!nn->dp.bpf_offload_xdp)
+ 			return prog ? -EBUSY : 0;
+ 		cmd.command = prog ? TC_CLSBPF_REPLACE : TC_CLSBPF_DESTROY;
+ 	} else {
+ 		if (!prog)
+ 			return 0;
+ 		cmd.command = TC_CLSBPF_ADD;
+ 	}
+ 
+ 	ret = nfp_net_bpf_offload(nn, &cmd);
+ 	/* Stop offload if replace not possible */
+ 	if (ret && cmd.command == TC_CLSBPF_REPLACE)
+ 		nfp_net_xdp_offload(nn, NULL);
+ 	nn->dp.bpf_offload_xdp = prog && !ret;
+ 	return ret;
+ }
+ 
+ static int nfp_net_xdp_setup(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_net_ring_set rx = {
+ 		.n_rings = nn->dp.num_rx_rings,
+ 		.mtu = nn->dp.netdev->mtu,
+ 		.dcnt = nn->dp.rxd_cnt,
+ 	};
+ 	struct nfp_net_ring_set tx = {
+ 		.n_rings = nn->dp.num_tx_rings,
+ 		.dcnt = nn->dp.txd_cnt,
+ 	};
+ 	int err;
+ 
+ 	if (prog && prog->xdp_adjust_head) {
+ 		nn_err(nn, "Does not support bpf_xdp_adjust_head()\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 	if (!prog && !nn->dp.xdp_prog)
+ 		return 0;
+ 	if (prog && nn->dp.xdp_prog) {
+ 		prog = xchg(&nn->dp.xdp_prog, prog);
+ 		bpf_prog_put(prog);
+ 		nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 		return 0;
+ 	}
+ 
+ 	tx.n_rings += prog ? nn->dp.num_rx_rings : -nn->dp.num_rx_rings;
+ 
+ 	/* We need RX reconfig to remap the buffers (BIDIR vs FROM_DEV) */
+ 	err = nfp_net_ring_reconfig(nn, &prog, &rx, &tx);
+ 	if (err)
+ 		return err;
+ 
+ 	/* @prog got swapped and is now the old one */
+ 	if (prog)
+ 		bpf_prog_put(prog);
+ 
+ 	nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_net_xdp(struct net_device *netdev, struct netdev_xdp *xdp)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 
+ 	switch (xdp->command) {
+ 	case XDP_SETUP_PROG:
+ 		return nfp_net_xdp_setup(nn, xdp->prog);
+ 	case XDP_QUERY_PROG:
+ 		xdp->prog_attached = !!nn->dp.xdp_prog;
+ 		return 0;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  static const struct net_device_ops nfp_net_netdev_ops = {
 +	.ndo_size		= sizeof(struct net_device_ops),
  	.ndo_open		= nfp_net_netdev_open,
  	.ndo_stop		= nfp_net_netdev_close,
  	.ndo_start_xmit		= nfp_net_tx,
@@@ -2671,23 -3085,27 +3292,30 @@@ struct nfp_net *nfp_net_netdev_alloc(st
  	SET_NETDEV_DEV(netdev, &pdev->dev);
  	nn = netdev_priv(netdev);
  
++<<<<<<< HEAD
 +	nn->netdev = netdev;
++=======
+ 	nn->dp.netdev = netdev;
+ 	nn->dp.dev = &pdev->dev;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	nn->pdev = pdev;
  
  	nn->max_tx_rings = max_tx_rings;
  	nn->max_rx_rings = max_rx_rings;
  
- 	nn->num_tx_rings = min_t(unsigned int, max_tx_rings, num_online_cpus());
- 	nn->num_rx_rings = min_t(unsigned int, max_rx_rings,
+ 	nn->dp.num_tx_rings = min_t(unsigned int,
+ 				    max_tx_rings, num_online_cpus());
+ 	nn->dp.num_rx_rings = min_t(unsigned int, max_rx_rings,
  				 netif_get_num_default_rss_queues());
  
- 	nn->num_r_vecs = max(nn->num_tx_rings, nn->num_rx_rings);
- 	nn->num_r_vecs = min_t(unsigned int, nn->num_r_vecs, num_online_cpus());
+ 	nn->dp.num_r_vecs = max(nn->dp.num_tx_rings, nn->dp.num_rx_rings);
+ 	nn->dp.num_r_vecs = min_t(unsigned int,
+ 				  nn->dp.num_r_vecs, num_online_cpus());
  
- 	nn->txd_cnt = NFP_NET_TX_DESCS_DEFAULT;
- 	nn->rxd_cnt = NFP_NET_RX_DESCS_DEFAULT;
+ 	nn->dp.txd_cnt = NFP_NET_TX_DESCS_DEFAULT;
+ 	nn->dp.rxd_cnt = NFP_NET_RX_DESCS_DEFAULT;
  
  	spin_lock_init(&nn->reconfig_lock);
 -	spin_lock_init(&nn->rx_filter_lock);
  	spin_lock_init(&nn->link_status_lock);
  
  	setup_timer(&nn->reconfig_timer,
@@@ -2702,16 -3122,55 +3330,38 @@@
   */
  void nfp_net_netdev_free(struct nfp_net *nn)
  {
- 	free_netdev(nn->netdev);
+ 	free_netdev(nn->dp.netdev);
  }
  
 -/**
 - * nfp_net_rss_key_sz() - Get current size of the RSS key
 - * @nn:		NFP Net device instance
 - *
 - * Return: size of the RSS key for currently selected hash function.
 - */
 -unsigned int nfp_net_rss_key_sz(struct nfp_net *nn)
 -{
 -	switch (nn->rss_hfunc) {
 -	case ETH_RSS_HASH_TOP:
 -		return NFP_NET_CFG_RSS_KEY_SZ;
 -	case ETH_RSS_HASH_XOR:
 -		return 0;
 -	case ETH_RSS_HASH_CRC32:
 -		return 4;
 -	}
 -
 -	nn_warn(nn, "Unknown hash function: %u\n", nn->rss_hfunc);
 -	return 0;
 -}
 -
  /**
   * nfp_net_rss_init() - Set the initial RSS parameters
   * @nn:	     NFP Net device to reconfigure
   */
  static void nfp_net_rss_init(struct nfp_net *nn)
  {
++<<<<<<< HEAD
 +	netdev_rss_key_fill(nn->rss_key, NFP_NET_CFG_RSS_KEY_SZ);
++=======
+ 	unsigned long func_bit, rss_cap_hfunc;
+ 	u32 reg;
+ 
+ 	/* Read the RSS function capability and select first supported func */
+ 	reg = nn_readl(nn, NFP_NET_CFG_RSS_CAP);
+ 	rss_cap_hfunc =	FIELD_GET(NFP_NET_CFG_RSS_CAP_HFUNC, reg);
+ 	if (!rss_cap_hfunc)
+ 		rss_cap_hfunc =	FIELD_GET(NFP_NET_CFG_RSS_CAP_HFUNC,
+ 					  NFP_NET_CFG_RSS_TOEPLITZ);
+ 
+ 	func_bit = find_first_bit(&rss_cap_hfunc, NFP_NET_CFG_RSS_HFUNCS);
+ 	if (func_bit == NFP_NET_CFG_RSS_HFUNCS) {
+ 		dev_warn(nn->dp.dev,
+ 			 "Bad RSS config, defaulting to Toeplitz hash\n");
+ 		func_bit = ETH_RSS_HASH_TOP_BIT;
+ 	}
+ 	nn->rss_hfunc = 1 << func_bit;
+ 
+ 	netdev_rss_key_fill(nn->rss_key, nfp_net_rss_key_sz(nn));
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  	nfp_net_rss_init_itbl(nn);
  
@@@ -2745,6 -3204,8 +3395,11 @@@ int nfp_net_netdev_init(struct net_devi
  	struct nfp_net *nn = netdev_priv(netdev);
  	int err;
  
++<<<<<<< HEAD
++=======
+ 	nn->dp.chained_metadata_format = nn->fw_ver.major > 3;
+ 
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	/* Get some of the read-only fields from the BAR */
  	nn->cap = nn_readl(nn, NFP_NET_CFG_CAP);
  	nn->max_mtu = nn_readl(nn, NFP_NET_CFG_MAX_MTU);
@@@ -2859,5 -3328,11 +3514,15 @@@
   */
  void nfp_net_netdev_clean(struct net_device *netdev)
  {
++<<<<<<< HEAD
 +	unregister_netdev(netdev);
++=======
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 
+ 	if (nn->dp.xdp_prog)
+ 		bpf_prog_put(nn->dp.xdp_prog);
+ 	if (nn->dp.bpf_offload_xdp)
+ 		nfp_net_xdp_offload(nn, NULL);
+ 	unregister_netdev(nn->dp.netdev);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  }
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_debugfs.c
index fddf57c8c945,74125584260b..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_debugfs.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_debugfs.c
@@@ -126,11 -138,14 +126,15 @@@ static int nfp_net_debugfs_tx_q_read(st
  
  	rtnl_lock();
  
 -	if (debugfs_real_fops(file->file) == &nfp_tx_q_fops)
 -		tx_ring = r_vec->tx_ring;
 -	else
 -		tx_ring = r_vec->xdp_ring;
 -	if (!r_vec->nfp_net || !tx_ring)
 +	if (!r_vec->nfp_net || !r_vec->tx_ring)
  		goto out;
  	nn = r_vec->nfp_net;
++<<<<<<< HEAD
 +	tx_ring = r_vec->tx_ring;
 +	if (!netif_running(nn->netdev))
++=======
+ 	if (!netif_running(nn->dp.netdev))
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		goto out;
  
  	txd_cnt = tx_ring->cnt;
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
index dfbf6b94ff5b,4620c1bba96e..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
@@@ -184,12 -197,13 +184,17 @@@ static int nfp_net_set_ring_size(struc
  		.dcnt = txd_cnt,
  	};
  
- 	if (nn->rxd_cnt != rxd_cnt)
+ 	if (nn->dp.rxd_cnt != rxd_cnt)
  		reconfig_rx = &rx;
- 	if (nn->txd_cnt != txd_cnt)
+ 	if (nn->dp.txd_cnt != txd_cnt)
  		reconfig_tx = &tx;
  
++<<<<<<< HEAD
 +	return nfp_net_ring_reconfig(nn, reconfig_rx, reconfig_tx);
++=======
+ 	return nfp_net_ring_reconfig(nn, &nn->dp.xdp_prog,
+ 				     reconfig_rx, reconfig_tx);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  }
  
  static int nfp_net_set_ringparam(struct net_device *netdev,
@@@ -721,14 -743,19 +726,28 @@@ static void nfp_net_get_channels(struc
  				 struct ethtool_channels *channel)
  {
  	struct nfp_net *nn = netdev_priv(netdev);
++<<<<<<< HEAD
++=======
+ 	unsigned int num_tx_rings;
+ 
+ 	num_tx_rings = nn->dp.num_tx_rings;
+ 	if (nn->dp.xdp_prog)
+ 		num_tx_rings -= nn->dp.num_rx_rings;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  
  	channel->max_rx = min(nn->max_rx_rings, nn->max_r_vecs);
  	channel->max_tx = min(nn->max_tx_rings, nn->max_r_vecs);
  	channel->max_combined = min(channel->max_rx, channel->max_tx);
  	channel->max_other = NFP_NET_NON_Q_VECTORS;
++<<<<<<< HEAD
 +	channel->combined_count = min(nn->num_rx_rings, nn->num_tx_rings);
 +	channel->rx_count = nn->num_rx_rings - channel->combined_count;
 +	channel->tx_count = nn->num_tx_rings - channel->combined_count;
++=======
+ 	channel->combined_count = min(nn->dp.num_rx_rings, num_tx_rings);
+ 	channel->rx_count = nn->dp.num_rx_rings - channel->combined_count;
+ 	channel->tx_count = num_tx_rings - channel->combined_count;
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  	channel->other_count = NFP_NET_NON_Q_VECTORS;
  }
  
@@@ -743,15 -770,21 +762,28 @@@ static int nfp_net_set_num_rings(struc
  	};
  	struct nfp_net_ring_set tx = {
  		.n_rings = total_tx,
- 		.dcnt = nn->txd_cnt,
+ 		.dcnt = nn->dp.txd_cnt,
  	};
  
- 	if (nn->num_rx_rings != total_rx)
+ 	if (nn->dp.num_rx_rings != total_rx)
  		reconfig_rx = &rx;
++<<<<<<< HEAD
 +	if (nn->num_tx_rings != total_tx)
 +		reconfig_tx = &tx;
 +
 +	return nfp_net_ring_reconfig(nn, reconfig_rx, reconfig_tx);
++=======
+ 	if (nn->dp.num_stack_tx_rings != total_tx ||
+ 	    (nn->dp.xdp_prog && reconfig_rx))
+ 		reconfig_tx = &tx;
+ 
+ 	/* nfp_net_check_config() will catch tx.n_rings > nn->max_tx_rings */
+ 	if (nn->dp.xdp_prog)
+ 		tx.n_rings += total_rx;
+ 
+ 	return nfp_net_ring_reconfig(nn, &nn->dp.xdp_prog,
+ 				     reconfig_rx, reconfig_tx);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  }
  
  static int nfp_net_set_channels(struct net_device *netdev,
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_main.c
index 3afcdc11480c,3935d19a273d..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_main.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_main.c
@@@ -141,18 -141,17 +141,27 @@@ nfp_net_get_mac_addr_hwinfo(struct nfp_
  
  	mac_str = nfp_hwinfo_lookup(cpp, name);
  	if (!mac_str) {
++<<<<<<< HEAD
 +		dev_warn(&nn->pdev->dev,
 +			 "Can't lookup MAC address. Generate\n");
 +		eth_hw_addr_random(nn->netdev);
++=======
+ 		dev_warn(dp->dev, "Can't lookup MAC address. Generate\n");
+ 		eth_hw_addr_random(dp->netdev);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  		return;
  	}
  
  	if (sscanf(mac_str, "%02hhx:%02hhx:%02hhx:%02hhx:%02hhx:%02hhx",
  		   &mac_addr[0], &mac_addr[1], &mac_addr[2],
  		   &mac_addr[3], &mac_addr[4], &mac_addr[5]) != 6) {
++<<<<<<< HEAD
 +		dev_warn(&nn->pdev->dev,
++=======
+ 		dev_warn(dp->dev,
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			 "Can't parse MAC address (%s). Generate.\n", mac_str);
- 		eth_hw_addr_random(nn->netdev);
+ 		eth_hw_addr_random(dp->netdev);
  		return;
  	}
  
@@@ -178,8 -177,10 +187,15 @@@ nfp_net_get_mac_addr(struct nfp_net *nn
  		if (pf->eth_tbl->ports[i].eth_index == id) {
  			const u8 *mac_addr = pf->eth_tbl->ports[i].mac_addr;
  
++<<<<<<< HEAD
 +			ether_addr_copy(nn->netdev->dev_addr, mac_addr);
 +			ether_addr_copy(nn->netdev->perm_addr, mac_addr);
++=======
+ 			nn->eth_port = &pf->eth_tbl->ports[i];
+ 
+ 			ether_addr_copy(nn->dp.netdev->dev_addr, mac_addr);
+ 			ether_addr_copy(nn->dp.netdev->perm_addr, mac_addr);
++>>>>>>> 79c12a752cea (nfp: separate data path information from the reset of adapter structure)
  			return;
  		}
  
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_offload.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net.h
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_common.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_debugfs.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_ethtool.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_main.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_offload.c
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_netvf_main.c b/drivers/net/ethernet/netronome/nfp/nfp_netvf_main.c
index bd3cf87f2365..8f24391f1059 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_netvf_main.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_netvf_main.c
@@ -83,12 +83,12 @@ static void nfp_netvf_get_mac_addr(struct nfp_net *nn)
 	put_unaligned_be16(nn_readw(nn, NFP_NET_CFG_MACADDR + 6), &mac_addr[4]);
 
 	if (!is_valid_ether_addr(mac_addr)) {
-		eth_hw_addr_random(nn->netdev);
+		eth_hw_addr_random(nn->dp.netdev);
 		return;
 	}
 
-	ether_addr_copy(nn->netdev->dev_addr, mac_addr);
-	ether_addr_copy(nn->netdev->perm_addr, mac_addr);
+	ether_addr_copy(nn->dp.netdev->dev_addr, mac_addr);
+	ether_addr_copy(nn->dp.netdev->perm_addr, mac_addr);
 }
 
 static int nfp_netvf_pci_probe(struct pci_dev *pdev,
@@ -210,7 +210,7 @@ static int nfp_netvf_pci_probe(struct pci_dev *pdev,
 
 	nn->fw_ver = fw_ver;
 	nn->ctrl_bar = ctrl_bar;
-	nn->is_vf = 1;
+	nn->dp.is_vf = 1;
 	nn->stride_tx = stride;
 	nn->stride_rx = stride;
 
@@ -267,7 +267,8 @@ static int nfp_netvf_pci_probe(struct pci_dev *pdev,
 
 	num_irqs = nfp_net_irqs_alloc(pdev, vf->irq_entries,
 				      NFP_NET_MIN_PORT_IRQS,
-				      NFP_NET_NON_Q_VECTORS + nn->num_r_vecs);
+				      NFP_NET_NON_Q_VECTORS +
+				      nn->dp.num_r_vecs);
 	if (!num_irqs) {
 		nn_warn(nn, "Unable to allocate MSI-X Vectors. Exiting\n");
 		err = -EIO;
@@ -281,7 +282,7 @@ static int nfp_netvf_pci_probe(struct pci_dev *pdev,
 	 */
 	nn->me_freq_mhz = 1200;
 
-	err = nfp_net_netdev_init(nn->netdev);
+	err = nfp_net_netdev_init(nn->dp.netdev);
 	if (err)
 		goto err_irqs_disable;
 
@@ -326,7 +327,7 @@ static void nfp_netvf_pci_remove(struct pci_dev *pdev)
 	nfp_net_debugfs_dir_clean(&nn->debugfs_dir);
 	nfp_net_debugfs_dir_clean(&vf->ddir);
 
-	nfp_net_netdev_clean(nn->netdev);
+	nfp_net_netdev_clean(nn->dp.netdev);
 
 	nfp_net_irqs_disable(pdev);
 
