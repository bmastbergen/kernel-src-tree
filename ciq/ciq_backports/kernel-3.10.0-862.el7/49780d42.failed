IB/mlx5: Expose MR cache for mlx5_ib

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Artemy Kovalyov <artemyko@mellanox.com>
commit 49780d42dfc9ec0f4090c32ca59688449da1a1cd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/49780d42.failed

Allow other parts of mlx5_ib to use MR cache mechanism.
* Add new functions mlx5_mr_cache_alloc and mlx5_mr_cache_free
* Traditional MTT MKey buckets are limited by MAX_UMR_CACHE_ENTRY
  Additinal buckets may be added above.

	Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 49780d42dfc9ec0f4090c32ca59688449da1a1cd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
#	include/linux/mlx5/driver.h
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 47de9e74f883,8f5b94d483e4..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -46,14 -46,10 +46,19 @@@ enum 
  };
  
  #define MLX5_UMR_ALIGN 2048
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +static __be64 mlx5_ib_update_mtt_emergency_buffer[
 +		MLX5_UMR_MTT_MIN_CHUNK_SIZE/sizeof(__be64)]
 +	__aligned(MLX5_UMR_ALIGN);
 +static DEFINE_MUTEX(mlx5_ib_update_mtt_emergency_buffer_mutex);
 +#endif
  
  static int clean_mr(struct mlx5_ib_mr *mr);
++<<<<<<< HEAD
++=======
+ static int use_umr(struct mlx5_ib_dev *dev, int order);
+ static int unreg_umr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr);
++>>>>>>> 49780d42dfc9 (IB/mlx5: Expose MR cache for mlx5_ib)
  
  static int destroy_mkey(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
  {
@@@ -628,17 -662,28 +671,37 @@@ int mlx5_mr_cache_init(struct mlx5_ib_d
  		spin_lock_init(&ent->lock);
  		ent->order = i + 2;
  		ent->dev = dev;
+ 		ent->limit = 0;
  
++<<<<<<< HEAD
 +		if ((dev->mdev->profile->mask & MLX5_PROF_MASK_MR_CACHE) &&
 +		    (mlx5_core_is_pf(dev->mdev)))
 +			limit = dev->mdev->profile->mr_cache[i].limit;
 +		else
 +			limit = 0;
 +
++=======
+ 		init_completion(&ent->compl);
++>>>>>>> 49780d42dfc9 (IB/mlx5: Expose MR cache for mlx5_ib)
  		INIT_WORK(&ent->work, cache_work_func);
  		INIT_DELAYED_WORK(&ent->dwork, delayed_cache_work_func);
- 		ent->limit = limit;
  		queue_work(cache->wq, &ent->work);
+ 
+ 		if (i > MAX_UMR_CACHE_ENTRY)
+ 			continue;
+ 
+ 		if (!use_umr(dev, ent->order))
+ 			continue;
+ 
+ 		ent->page = PAGE_SHIFT;
+ 		ent->xlt = (1 << ent->order) * sizeof(struct mlx5_mtt) /
+ 			   MLX5_IB_UMR_OCTOWORD;
+ 		ent->access_mode = MLX5_MKC_ACCESS_MODE_MTT;
+ 		if ((dev->mdev->profile->mask & MLX5_PROF_MASK_MR_CACHE) &&
+ 		    mlx5_core_is_pf(dev->mdev))
+ 			ent->limit = dev->mdev->profile->mr_cache[i].limit;
+ 		else
+ 			ent->limit = 0;
  	}
  
  	err = mlx5_mr_cache_debugfs_init(dev);
@@@ -759,8 -804,10 +822,13 @@@ static int get_octo_len(u64 addr, u64 l
  	return (npages + 1) / 2;
  }
  
 -static int use_umr(struct mlx5_ib_dev *dev, int order)
 +static int use_umr(int order)
  {
++<<<<<<< HEAD
++=======
+ 	if (MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset))
+ 		return order <= MAX_UMR_CACHE_ENTRY + 2;
++>>>>>>> 49780d42dfc9 (IB/mlx5: Expose MR cache for mlx5_ib)
  	return order <= MLX5_MAX_UMR_SHIFT;
  }
  
@@@ -958,17 -916,11 +1026,17 @@@ static struct mlx5_ib_mr *reg_umr(struc
  	mr->mmkey.size = len;
  	mr->mmkey.pd = to_mpd(pd)->pdn;
  
 -	err = mlx5_ib_update_xlt(mr, 0, npages, page_shift,
 -				 MLX5_IB_UPD_XLT_ENABLE);
 +	mr->live = 1;
 +
 +unmap_dma:
 +	up(&umrc->sem);
 +	dma_unmap_single(ddev, dma, size, DMA_TO_DEVICE);
  
 +	kfree(mr_pas);
 +
 +free_mr:
  	if (err) {
- 		free_cached_mr(dev, mr);
+ 		mlx5_mr_cache_free(dev, mr);
  		return ERR_PTR(err);
  	}
  
diff --cc include/linux/mlx5/driver.h
index 61d8bec93f9f,2534b8a0fd7b..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -980,7 -1052,8 +980,12 @@@ enum 
  };
  
  enum {
++<<<<<<< HEAD
 +	MAX_MR_CACHE_ENTRIES    = 16,
++=======
+ 	MAX_UMR_CACHE_ENTRY = 20,
+ 	MAX_MR_CACHE_ENTRIES
++>>>>>>> 49780d42dfc9 (IB/mlx5: Expose MR cache for mlx5_ib)
  };
  
  enum {
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6922362fef46..d3faf9cf3093 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -561,6 +561,10 @@ struct mlx5_cache_ent {
 	struct dentry	       *dir;
 	char                    name[4];
 	u32                     order;
+	u32			xlt;
+	u32			access_mode;
+	u32			page;
+
 	u32			size;
 	u32                     cur;
 	u32                     miss;
@@ -575,6 +579,7 @@ struct mlx5_cache_ent {
 	struct work_struct	work;
 	struct delayed_work	dwork;
 	int			pending;
+	struct completion	compl;
 };
 
 struct mlx5_mr_cache {
@@ -849,7 +854,9 @@ void mlx5_ib_copy_pas(u64 *old, u64 *new, int step, int num);
 int mlx5_ib_get_cqe_size(struct mlx5_ib_dev *dev, struct ib_cq *ibcq);
 int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
 int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_mr_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift);
+
+struct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev, int entry);
+void mlx5_mr_cache_free(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr);
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path include/linux/mlx5/driver.h
