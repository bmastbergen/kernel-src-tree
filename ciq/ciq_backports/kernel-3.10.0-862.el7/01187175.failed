net/mlx5: Add interface to get reference to a UAR

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [infiniband] lx5: Add interface to get reference to a UAR (Don Dutile) [1385309 1385649 1386645 1409099 1456667 1456687]
Rebuild_FUZZ: 94.62%
commit-author Eli Cohen <eli@mellanox.com>
commit 0118717583cda6f4f36092853ad0345e8150b286
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/01187175.failed

A reference to a UAR is required to generate CQ or EQ doorbells. Since
CQ or EQ doorbells can all be generated using the same UAR area without
any effect on performance, we are just getting a reference to any
available UAR, If one is not available we allocate it but we don't waste
the blue flame registers it can provide and we will use them for
subsequent allocations.
We get a reference to such UAR and put in mlx5_priv so any kernel
consumer can make use of it.

	Signed-off-by: Eli Cohen <eli@mellanox.com>
	Reviewed-by: Matan Barak <matanb@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 0118717583cda6f4f36092853ad0345e8150b286)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/main.c
#	drivers/net/ethernet/mellanox/mlx5/core/uar.c
#	include/linux/mlx5/driver.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index 634e6e3dfa90,5130d65dd41a..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -351,7 -511,8 +351,12 @@@ static void init_eq_buf(struct mlx5_eq 
  }
  
  int mlx5_create_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq, u8 vecidx,
++<<<<<<< HEAD
 +		       int nent, u64 mask, const char *name, struct mlx5_uar *uar)
++=======
+ 		       int nent, u64 mask, const char *name,
+ 		       enum mlx5_eq_type type)
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  {
  	u32 out[MLX5_ST_SZ_DW(create_eq_out)] = {0};
  	struct mlx5_priv *priv = &dev->priv;
@@@ -401,8 -571,8 +406,13 @@@
  	eq->eqn = MLX5_GET(create_eq_out, out, eq_number);
  	eq->irqn = priv->msix_arr[vecidx].vector;
  	eq->dev = dev;
++<<<<<<< HEAD
 +	eq->doorbell = uar->map + MLX5_EQ_DOORBEL_OFFSET;
 +	err = request_irq(eq->irqn, mlx5_msix_handler, 0,
++=======
+ 	eq->doorbell = priv->uar->map + MLX5_EQ_DOORBEL_OFFSET;
+ 	err = request_irq(eq->irqn, handler, 0,
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  			  priv->irq_info[vecidx].name, eq);
  	if (err)
  		goto err_eq;
@@@ -498,12 -684,9 +508,16 @@@ int mlx5_start_eqs(struct mlx5_core_de
  	else
  		mlx5_core_dbg(dev, "port_module_event is not set\n");
  
 +	if (MLX5_CAP_GEN(dev, pps))
 +		async_event_mask |= (1ull << MLX5_EVENT_TYPE_PPS_EVENT);
 +
  	err = mlx5_create_map_eq(dev, &table->cmd_eq, MLX5_EQ_VEC_CMD,
  				 MLX5_NUM_CMD_EQE, 1ull << MLX5_EVENT_TYPE_CMD,
++<<<<<<< HEAD
 +				 "mlx5_cmd_eq", &dev->priv.uuari.uars[0]);
++=======
+ 				 "mlx5_cmd_eq", MLX5_EQ_TYPE_ASYNC);
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  	if (err) {
  		mlx5_core_warn(dev, "failed to create cmd EQ %d\n", err);
  		return err;
@@@ -513,7 -696,7 +527,11 @@@
  
  	err = mlx5_create_map_eq(dev, &table->async_eq, MLX5_EQ_VEC_ASYNC,
  				 MLX5_NUM_ASYNC_EQE, async_event_mask,
++<<<<<<< HEAD
 +				 "mlx5_async_eq", &dev->priv.uuari.uars[0]);
++=======
+ 				 "mlx5_async_eq", MLX5_EQ_TYPE_ASYNC);
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  	if (err) {
  		mlx5_core_warn(dev, "failed to create async EQ %d\n", err);
  		goto err1;
@@@ -523,13 -706,33 +541,35 @@@
  				 MLX5_EQ_VEC_PAGES,
  				 /* TODO: sriov max_vf + */ 1,
  				 1 << MLX5_EVENT_TYPE_PAGE_REQUEST, "mlx5_pages_eq",
++<<<<<<< HEAD
 +				 &dev->priv.uuari.uars[0]);
++=======
+ 				 MLX5_EQ_TYPE_ASYNC);
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  	if (err) {
  		mlx5_core_warn(dev, "failed to create pages EQ %d\n", err);
  		goto err2;
  	}
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	if (MLX5_CAP_GEN(dev, pg)) {
+ 		err = mlx5_create_map_eq(dev, &table->pfault_eq,
+ 					 MLX5_EQ_VEC_PFAULT,
+ 					 MLX5_NUM_ASYNC_EQE,
+ 					 1 << MLX5_EVENT_TYPE_PAGE_FAULT,
+ 					 "mlx5_page_fault_eq",
+ 					 MLX5_EQ_TYPE_PF);
+ 		if (err) {
+ 			mlx5_core_warn(dev, "failed to create page fault EQ %d\n",
+ 				       err);
+ 			goto err3;
+ 		}
+ 	}
+ 
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  	return err;
 -err3:
 -	mlx5_destroy_unmap_eq(dev, &table->pages_eq);
 -#else
 -	return err;
 -#endif
  
  err2:
  	mlx5_destroy_unmap_eq(dev, &table->async_eq);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/main.c
index ddd26354b516,2882d0483ed8..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@@ -732,7 -753,7 +732,11 @@@ static int alloc_comp_eqs(struct mlx5_c
  		snprintf(name, MLX5_MAX_IRQ_NAME, "mlx5_comp%d", i);
  		err = mlx5_create_map_eq(dev, eq,
  					 i + MLX5_EQ_VEC_COMP_BASE, nent, 0,
++<<<<<<< HEAD
 +					 name, &dev->priv.uuari.uars[0]);
++=======
+ 					 name, MLX5_EQ_TYPE_COMP);
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  		if (err) {
  			kfree(eq);
  			goto clean;
@@@ -1072,8 -1093,8 +1076,13 @@@ static int mlx5_load_one(struct mlx5_co
  		goto err_cleanup_once;
  	}
  
++<<<<<<< HEAD
 +	err = mlx5_alloc_uuars(dev, &priv->uuari);
 +	if (err) {
++=======
+ 	dev->priv.uar = mlx5_get_uars_page(dev);
+ 	if (!dev->priv.uar) {
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  		dev_err(&pdev->dev, "Failed allocating uar, aborting\n");
  		goto err_disable_msix;
  	}
@@@ -1148,8 -1175,11 +1163,11 @@@ err_stop_eqs
  	mlx5_stop_eqs(dev);
  
  err_free_uar:
 -	mlx5_free_bfregs(dev, &priv->bfregi);
 +	mlx5_free_uuars(dev, &priv->uuari);
  
+ err_uar_cleanup:
+ 	mlx5_put_uars_page(dev, priv->uar);
+ 
  err_disable_msix:
  	mlx5_disable_msix(dev);
  
@@@ -1211,7 -1238,8 +1229,12 @@@ static int mlx5_unload_one(struct mlx5_
  	mlx5_irq_clear_affinity_hints(dev);
  	free_comp_eqs(dev);
  	mlx5_stop_eqs(dev);
++<<<<<<< HEAD
 +	mlx5_free_uuars(dev, &priv->uuari);
++=======
+ 	mlx5_free_bfregs(dev, &priv->bfregi);
+ 	mlx5_put_uars_page(dev, priv->uar);
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  	mlx5_disable_msix(dev);
  	if (cleanup)
  		mlx5_cleanup_once(dev);
@@@ -1277,6 -1305,20 +1300,23 @@@ static int init_one(struct pci_dev *pde
  	spin_lock_init(&priv->ctx_lock);
  	mutex_init(&dev->pci_status_mutex);
  	mutex_init(&dev->intf_state_mutex);
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	err = init_srcu_struct(&priv->pfault_srcu);
+ 	if (err) {
+ 		dev_err(&pdev->dev, "init_srcu_struct failed with error code %d\n",
+ 			err);
+ 		goto clean_dev;
+ 	}
+ #endif
+ 	mutex_init(&priv->bfregs.reg_head.lock);
+ 	mutex_init(&priv->bfregs.wc_head.lock);
+ 	INIT_LIST_HEAD(&priv->bfregs.reg_head.list);
+ 	INIT_LIST_HEAD(&priv->bfregs.wc_head.list);
+ 
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  	err = mlx5_pci_init(dev, priv);
  	if (err) {
  		dev_err(&pdev->dev, "mlx5_pci_init failed with error code %d\n", err);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/uar.c
index ab0b896621a0,fcc0270ea72f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/uar.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/uar.c
@@@ -231,3 -231,270 +231,273 @@@ void mlx5_unmap_free_uar(struct mlx5_co
  	mlx5_cmd_free_uar(mdev, uar->index);
  }
  EXPORT_SYMBOL(mlx5_unmap_free_uar);
++<<<<<<< HEAD
++=======
+ 
+ static int uars_per_sys_page(struct mlx5_core_dev *mdev)
+ {
+ 	if (MLX5_CAP_GEN(mdev, uar_4k))
+ 		return MLX5_CAP_GEN(mdev, num_of_uars_per_page);
+ 
+ 	return 1;
+ }
+ 
+ static u64 uar2pfn(struct mlx5_core_dev *mdev, u32 index)
+ {
+ 	u32 system_page_index;
+ 
+ 	if (MLX5_CAP_GEN(mdev, uar_4k))
+ 		system_page_index = index >> (PAGE_SHIFT - MLX5_ADAPTER_PAGE_SHIFT);
+ 	else
+ 		system_page_index = index;
+ 
+ 	return (pci_resource_start(mdev->pdev, 0) >> PAGE_SHIFT) + system_page_index;
+ }
+ 
+ static void up_rel_func(struct kref *kref)
+ {
+ 	struct mlx5_uars_page *up = container_of(kref, struct mlx5_uars_page, ref_count);
+ 
+ 	list_del(&up->list);
+ 	if (mlx5_cmd_free_uar(up->mdev, up->index))
+ 		mlx5_core_warn(up->mdev, "failed to free uar index %d\n", up->index);
+ 	kfree(up->reg_bitmap);
+ 	kfree(up->fp_bitmap);
+ 	kfree(up);
+ }
+ 
+ static struct mlx5_uars_page *alloc_uars_page(struct mlx5_core_dev *mdev,
+ 					      bool map_wc)
+ {
+ 	struct mlx5_uars_page *up;
+ 	int err = -ENOMEM;
+ 	phys_addr_t pfn;
+ 	int bfregs;
+ 	int i;
+ 
+ 	bfregs = uars_per_sys_page(mdev) * MLX5_BFREGS_PER_UAR;
+ 	up = kzalloc(sizeof(*up), GFP_KERNEL);
+ 	if (!up)
+ 		return ERR_PTR(err);
+ 
+ 	up->mdev = mdev;
+ 	up->reg_bitmap = kcalloc(BITS_TO_LONGS(bfregs), sizeof(unsigned long), GFP_KERNEL);
+ 	if (!up->reg_bitmap)
+ 		goto error1;
+ 
+ 	up->fp_bitmap = kcalloc(BITS_TO_LONGS(bfregs), sizeof(unsigned long), GFP_KERNEL);
+ 	if (!up->fp_bitmap)
+ 		goto error1;
+ 
+ 	for (i = 0; i < bfregs; i++)
+ 		if ((i % MLX5_BFREGS_PER_UAR) < MLX5_NON_FP_BFREGS_PER_UAR)
+ 			set_bit(i, up->reg_bitmap);
+ 		else
+ 			set_bit(i, up->fp_bitmap);
+ 
+ 	up->bfregs = bfregs;
+ 	up->fp_avail = bfregs * MLX5_FP_BFREGS_PER_UAR / MLX5_BFREGS_PER_UAR;
+ 	up->reg_avail = bfregs * MLX5_NON_FP_BFREGS_PER_UAR / MLX5_BFREGS_PER_UAR;
+ 
+ 	err = mlx5_cmd_alloc_uar(mdev, &up->index);
+ 	if (err) {
+ 		mlx5_core_warn(mdev, "mlx5_cmd_alloc_uar() failed, %d\n", err);
+ 		goto error1;
+ 	}
+ 
+ 	pfn = uar2pfn(mdev, up->index);
+ 	if (map_wc) {
+ 		up->map = ioremap_wc(pfn << PAGE_SHIFT, PAGE_SIZE);
+ 		if (!up->map) {
+ 			err = -EAGAIN;
+ 			goto error2;
+ 		}
+ 	} else {
+ 		up->map = ioremap(pfn << PAGE_SHIFT, PAGE_SIZE);
+ 		if (!up->map) {
+ 			err = -ENOMEM;
+ 			goto error2;
+ 		}
+ 	}
+ 	kref_init(&up->ref_count);
+ 	mlx5_core_dbg(mdev, "allocated UAR page: index %d, total bfregs %d\n",
+ 		      up->index, up->bfregs);
+ 	return up;
+ 
+ error2:
+ 	if (mlx5_cmd_free_uar(mdev, up->index))
+ 		mlx5_core_warn(mdev, "failed to free uar index %d\n", up->index);
+ error1:
+ 	kfree(up->fp_bitmap);
+ 	kfree(up->reg_bitmap);
+ 	kfree(up);
+ 	return ERR_PTR(err);
+ }
+ 
+ struct mlx5_uars_page *mlx5_get_uars_page(struct mlx5_core_dev *mdev)
+ {
+ 	struct mlx5_uars_page *ret;
+ 
+ 	mutex_lock(&mdev->priv.bfregs.reg_head.lock);
+ 	if (list_empty(&mdev->priv.bfregs.reg_head.list)) {
+ 		ret = alloc_uars_page(mdev, false);
+ 		if (IS_ERR(ret)) {
+ 			ret = NULL;
+ 			goto out;
+ 		}
+ 		list_add(&ret->list, &mdev->priv.bfregs.reg_head.list);
+ 	} else {
+ 		ret = list_first_entry(&mdev->priv.bfregs.reg_head.list,
+ 				       struct mlx5_uars_page, list);
+ 		kref_get(&ret->ref_count);
+ 	}
+ out:
+ 	mutex_unlock(&mdev->priv.bfregs.reg_head.lock);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL(mlx5_get_uars_page);
+ 
+ void mlx5_put_uars_page(struct mlx5_core_dev *mdev, struct mlx5_uars_page *up)
+ {
+ 	mutex_lock(&mdev->priv.bfregs.reg_head.lock);
+ 	kref_put(&up->ref_count, up_rel_func);
+ 	mutex_unlock(&mdev->priv.bfregs.reg_head.lock);
+ }
+ EXPORT_SYMBOL(mlx5_put_uars_page);
+ 
+ static unsigned long map_offset(struct mlx5_core_dev *mdev, int dbi)
+ {
+ 	/* return the offset in bytes from the start of the page to the
+ 	 * blue flame area of the UAR
+ 	 */
+ 	return dbi / MLX5_BFREGS_PER_UAR * MLX5_ADAPTER_PAGE_SIZE +
+ 	       (dbi % MLX5_BFREGS_PER_UAR) *
+ 	       (1 << MLX5_CAP_GEN(mdev, log_bf_reg_size)) + MLX5_BF_OFFSET;
+ }
+ 
+ static int alloc_bfreg(struct mlx5_core_dev *mdev, struct mlx5_sq_bfreg *bfreg,
+ 		       bool map_wc, bool fast_path)
+ {
+ 	struct mlx5_bfreg_data *bfregs;
+ 	struct mlx5_uars_page *up;
+ 	struct list_head *head;
+ 	unsigned long *bitmap;
+ 	unsigned int *avail;
+ 	struct mutex *lock;  /* pointer to right mutex */
+ 	int dbi;
+ 
+ 	bfregs = &mdev->priv.bfregs;
+ 	if (map_wc) {
+ 		head = &bfregs->wc_head.list;
+ 		lock = &bfregs->wc_head.lock;
+ 	} else {
+ 		head = &bfregs->reg_head.list;
+ 		lock = &bfregs->reg_head.lock;
+ 	}
+ 	mutex_lock(lock);
+ 	if (list_empty(head)) {
+ 		up = alloc_uars_page(mdev, map_wc);
+ 		if (IS_ERR(up)) {
+ 			mutex_unlock(lock);
+ 			return PTR_ERR(up);
+ 		}
+ 		list_add(&up->list, head);
+ 	} else {
+ 		up = list_entry(head->next, struct mlx5_uars_page, list);
+ 		kref_get(&up->ref_count);
+ 	}
+ 	if (fast_path) {
+ 		bitmap = up->fp_bitmap;
+ 		avail = &up->fp_avail;
+ 	} else {
+ 		bitmap = up->reg_bitmap;
+ 		avail = &up->reg_avail;
+ 	}
+ 	dbi = find_first_bit(bitmap, up->bfregs);
+ 	clear_bit(dbi, bitmap);
+ 	(*avail)--;
+ 	if (!(*avail))
+ 		list_del(&up->list);
+ 
+ 	bfreg->map = up->map + map_offset(mdev, dbi);
+ 	bfreg->up = up;
+ 	bfreg->wc = map_wc;
+ 	bfreg->index = up->index + dbi / MLX5_BFREGS_PER_UAR;
+ 	mutex_unlock(lock);
+ 
+ 	return 0;
+ }
+ 
+ int mlx5_alloc_bfreg(struct mlx5_core_dev *mdev, struct mlx5_sq_bfreg *bfreg,
+ 		     bool map_wc, bool fast_path)
+ {
+ 	int err;
+ 
+ 	err = alloc_bfreg(mdev, bfreg, map_wc, fast_path);
+ 	if (!err)
+ 		return 0;
+ 
+ 	if (err == -EAGAIN && map_wc)
+ 		return alloc_bfreg(mdev, bfreg, false, fast_path);
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_alloc_bfreg);
+ 
+ static unsigned int addr_to_dbi_in_syspage(struct mlx5_core_dev *dev,
+ 					   struct mlx5_uars_page *up,
+ 					   struct mlx5_sq_bfreg *bfreg)
+ {
+ 	unsigned int uar_idx;
+ 	unsigned int bfreg_idx;
+ 	unsigned int bf_reg_size;
+ 
+ 	bf_reg_size = 1 << MLX5_CAP_GEN(dev, log_bf_reg_size);
+ 
+ 	uar_idx = (bfreg->map - up->map) >> MLX5_ADAPTER_PAGE_SHIFT;
+ 	bfreg_idx = (((uintptr_t)bfreg->map % MLX5_ADAPTER_PAGE_SIZE) - MLX5_BF_OFFSET) / bf_reg_size;
+ 
+ 	return uar_idx * MLX5_BFREGS_PER_UAR + bfreg_idx;
+ }
+ 
+ void mlx5_free_bfreg(struct mlx5_core_dev *mdev, struct mlx5_sq_bfreg *bfreg)
+ {
+ 	struct mlx5_bfreg_data *bfregs;
+ 	struct mlx5_uars_page *up;
+ 	struct mutex *lock; /* pointer to right mutex */
+ 	unsigned int dbi;
+ 	bool fp;
+ 	unsigned int *avail;
+ 	unsigned long *bitmap;
+ 	struct list_head *head;
+ 
+ 	bfregs = &mdev->priv.bfregs;
+ 	if (bfreg->wc) {
+ 		head = &bfregs->wc_head.list;
+ 		lock = &bfregs->wc_head.lock;
+ 	} else {
+ 		head = &bfregs->reg_head.list;
+ 		lock = &bfregs->reg_head.lock;
+ 	}
+ 	up = bfreg->up;
+ 	dbi = addr_to_dbi_in_syspage(mdev, up, bfreg);
+ 	fp = (dbi % MLX5_BFREGS_PER_UAR) >= MLX5_NON_FP_BFREGS_PER_UAR;
+ 	if (fp) {
+ 		avail = &up->fp_avail;
+ 		bitmap = up->fp_bitmap;
+ 	} else {
+ 		avail = &up->reg_avail;
+ 		bitmap = up->reg_bitmap;
+ 	}
+ 	mutex_lock(lock);
+ 	(*avail)++;
+ 	set_bit(dbi, bitmap);
+ 	if (*avail == 1)
+ 		list_add_tail(&up->list, head);
+ 
+ 	kref_put(&up->ref_count, up_rel_func);
+ 	mutex_unlock(lock);
+ }
+ EXPORT_SYMBOL(mlx5_free_bfreg);
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
diff --cc include/linux/mlx5/driver.h
index 1fa4d48e24be,9a3a0954855b..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -612,6 -670,16 +612,19 @@@ struct mlx5_priv 
  	struct mlx5_rl_table            rl_table;
  
  	struct mlx5_port_module_event_stats  pme_stats;
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	void		      (*pfault)(struct mlx5_core_dev *dev,
+ 					void *context,
+ 					struct mlx5_pagefault *pfault);
+ 	void		       *pfault_ctx;
+ 	struct srcu_struct      pfault_srcu;
+ #endif
+ 	struct mlx5_bfreg_data		bfregs;
+ 	struct mlx5_uars_page	       *uar;
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  };
  
  enum mlx5_device_state {
@@@ -899,15 -1002,13 +912,20 @@@ void mlx5_fill_page_array(struct mlx5_b
  void mlx5_fill_page_frag_array(struct mlx5_frag_buf *frag_buf, __be64 *pas);
  void mlx5_cq_completion(struct mlx5_core_dev *dev, u32 cqn);
  void mlx5_rsc_event(struct mlx5_core_dev *dev, u32 rsn, int event_type);
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +void mlx5_eq_pagefault(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe);
 +#endif
  void mlx5_srq_event(struct mlx5_core_dev *dev, u32 srqn, int event_type);
  struct mlx5_core_srq *mlx5_core_get_srq(struct mlx5_core_dev *dev, u32 srqn);
 -void mlx5_cmd_comp_handler(struct mlx5_core_dev *dev, u64 vec);
 +void mlx5_cmd_comp_handler(struct mlx5_core_dev *dev, u64 vec, bool forced);
  void mlx5_cq_event(struct mlx5_core_dev *dev, u32 cqn, int event_type);
  int mlx5_create_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq, u8 vecidx,
++<<<<<<< HEAD
 +		       int nent, u64 mask, const char *name, struct mlx5_uar *uar);
++=======
+ 		       int nent, u64 mask, const char *name,
+ 		       enum mlx5_eq_type type);
++>>>>>>> 0118717583cd (net/mlx5: Add interface to get reference to a UAR)
  int mlx5_destroy_unmap_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
  int mlx5_start_eqs(struct mlx5_core_dev *dev);
  int mlx5_stop_eqs(struct mlx5_core_dev *dev);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/uar.c
* Unmerged path include/linux/mlx5/driver.h
