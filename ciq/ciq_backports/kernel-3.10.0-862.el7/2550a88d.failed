RDMA/cxgb4: Limit MRs to < 8GB for T4/T5 devices

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Hariprasad Shenai <hariprasad@chelsio.com>
commit 2550a88d956fb77c34d71b46a0a8e9ebf1c5b4a3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/2550a88d.failed

T4/T5 hardware can't handle MRs >= 8GB due to a hardware bug.  So limit
registrations to < 8GB for thse devices.

Based on original work by Steve Wise <swise@opengridcomputing.com>.

	Signed-off-by: Steve Wise <swise@opengridcomputing.com>
	Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
	Signed-off-by: Roland Dreier <roland@purestorage.com>
(cherry picked from commit 2550a88d956fb77c34d71b46a0a8e9ebf1c5b4a3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/cxgb4/mem.c
diff --cc drivers/infiniband/hw/cxgb4/mem.c
index 39cac5db937e,cb43c2299ac0..000000000000
--- a/drivers/infiniband/hw/cxgb4/mem.c
+++ b/drivers/infiniband/hw/cxgb4/mem.c
@@@ -51,10 -50,17 +51,17 @@@ static int inline_threshold = C4IW_INLI
  module_param(inline_threshold, int, 0644);
  MODULE_PARM_DESC(inline_threshold, "inline vs dsgl threshold (default=128)");
  
+ static int mr_exceeds_hw_limits(struct c4iw_dev *dev, u64 length)
+ {
+ 	return (is_t4(dev->rdev.lldi.adapter_type) ||
+ 		is_t5(dev->rdev.lldi.adapter_type)) &&
+ 		length >= 8*1024*1024*1024ULL;
+ }
+ 
  static int _c4iw_write_mem_dma_aligned(struct c4iw_rdev *rdev, u32 addr,
 -				       u32 len, dma_addr_t data, int wait)
 +				       u32 len, dma_addr_t data,
 +				       int wait, struct sk_buff *skb)
  {
 -	struct sk_buff *skb;
  	struct ulp_mem_io *req;
  	struct ulptx_sgl *sgl;
  	u8 wr_len;
@@@ -411,6 -431,228 +418,231 @@@ static int alloc_pbl(struct c4iw_mr *mh
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int build_phys_page_list(struct ib_phys_buf *buffer_list,
+ 				int num_phys_buf, u64 *iova_start,
+ 				u64 *total_size, int *npages,
+ 				int *shift, __be64 **page_list)
+ {
+ 	u64 mask;
+ 	int i, j, n;
+ 
+ 	mask = 0;
+ 	*total_size = 0;
+ 	for (i = 0; i < num_phys_buf; ++i) {
+ 		if (i != 0 && buffer_list[i].addr & ~PAGE_MASK)
+ 			return -EINVAL;
+ 		if (i != 0 && i != num_phys_buf - 1 &&
+ 		    (buffer_list[i].size & ~PAGE_MASK))
+ 			return -EINVAL;
+ 		*total_size += buffer_list[i].size;
+ 		if (i > 0)
+ 			mask |= buffer_list[i].addr;
+ 		else
+ 			mask |= buffer_list[i].addr & PAGE_MASK;
+ 		if (i != num_phys_buf - 1)
+ 			mask |= buffer_list[i].addr + buffer_list[i].size;
+ 		else
+ 			mask |= (buffer_list[i].addr + buffer_list[i].size +
+ 				PAGE_SIZE - 1) & PAGE_MASK;
+ 	}
+ 
+ 	if (*total_size > 0xFFFFFFFFULL)
+ 		return -ENOMEM;
+ 
+ 	/* Find largest page shift we can use to cover buffers */
+ 	for (*shift = PAGE_SHIFT; *shift < 27; ++(*shift))
+ 		if ((1ULL << *shift) & mask)
+ 			break;
+ 
+ 	buffer_list[0].size += buffer_list[0].addr & ((1ULL << *shift) - 1);
+ 	buffer_list[0].addr &= ~0ull << *shift;
+ 
+ 	*npages = 0;
+ 	for (i = 0; i < num_phys_buf; ++i)
+ 		*npages += (buffer_list[i].size +
+ 			(1ULL << *shift) - 1) >> *shift;
+ 
+ 	if (!*npages)
+ 		return -EINVAL;
+ 
+ 	*page_list = kmalloc(sizeof(u64) * *npages, GFP_KERNEL);
+ 	if (!*page_list)
+ 		return -ENOMEM;
+ 
+ 	n = 0;
+ 	for (i = 0; i < num_phys_buf; ++i)
+ 		for (j = 0;
+ 		     j < (buffer_list[i].size + (1ULL << *shift) - 1) >> *shift;
+ 		     ++j)
+ 			(*page_list)[n++] = cpu_to_be64(buffer_list[i].addr +
+ 			    ((u64) j << *shift));
+ 
+ 	PDBG("%s va 0x%llx mask 0x%llx shift %d len %lld pbl_size %d\n",
+ 	     __func__, (unsigned long long)*iova_start,
+ 	     (unsigned long long)mask, *shift, (unsigned long long)*total_size,
+ 	     *npages);
+ 
+ 	return 0;
+ 
+ }
+ 
+ int c4iw_reregister_phys_mem(struct ib_mr *mr, int mr_rereg_mask,
+ 			     struct ib_pd *pd, struct ib_phys_buf *buffer_list,
+ 			     int num_phys_buf, int acc, u64 *iova_start)
+ {
+ 
+ 	struct c4iw_mr mh, *mhp;
+ 	struct c4iw_pd *php;
+ 	struct c4iw_dev *rhp;
+ 	__be64 *page_list = NULL;
+ 	int shift = 0;
+ 	u64 total_size;
+ 	int npages;
+ 	int ret;
+ 
+ 	PDBG("%s ib_mr %p ib_pd %p\n", __func__, mr, pd);
+ 
+ 	/* There can be no memory windows */
+ 	if (atomic_read(&mr->usecnt))
+ 		return -EINVAL;
+ 
+ 	mhp = to_c4iw_mr(mr);
+ 	rhp = mhp->rhp;
+ 	php = to_c4iw_pd(mr->pd);
+ 
+ 	/* make sure we are on the same adapter */
+ 	if (rhp != php->rhp)
+ 		return -EINVAL;
+ 
+ 	memcpy(&mh, mhp, sizeof *mhp);
+ 
+ 	if (mr_rereg_mask & IB_MR_REREG_PD)
+ 		php = to_c4iw_pd(pd);
+ 	if (mr_rereg_mask & IB_MR_REREG_ACCESS) {
+ 		mh.attr.perms = c4iw_ib_to_tpt_access(acc);
+ 		mh.attr.mw_bind_enable = (acc & IB_ACCESS_MW_BIND) ==
+ 					 IB_ACCESS_MW_BIND;
+ 	}
+ 	if (mr_rereg_mask & IB_MR_REREG_TRANS) {
+ 		ret = build_phys_page_list(buffer_list, num_phys_buf,
+ 						iova_start,
+ 						&total_size, &npages,
+ 						&shift, &page_list);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	if (mr_exceeds_hw_limits(rhp, total_size)) {
+ 		kfree(page_list);
+ 		return -EINVAL;
+ 	}
+ 
+ 	ret = reregister_mem(rhp, php, &mh, shift, npages);
+ 	kfree(page_list);
+ 	if (ret)
+ 		return ret;
+ 	if (mr_rereg_mask & IB_MR_REREG_PD)
+ 		mhp->attr.pdid = php->pdid;
+ 	if (mr_rereg_mask & IB_MR_REREG_ACCESS)
+ 		mhp->attr.perms = c4iw_ib_to_tpt_access(acc);
+ 	if (mr_rereg_mask & IB_MR_REREG_TRANS) {
+ 		mhp->attr.zbva = 0;
+ 		mhp->attr.va_fbo = *iova_start;
+ 		mhp->attr.page_size = shift - 12;
+ 		mhp->attr.len = (u32) total_size;
+ 		mhp->attr.pbl_size = npages;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ struct ib_mr *c4iw_register_phys_mem(struct ib_pd *pd,
+ 				     struct ib_phys_buf *buffer_list,
+ 				     int num_phys_buf, int acc, u64 *iova_start)
+ {
+ 	__be64 *page_list;
+ 	int shift;
+ 	u64 total_size;
+ 	int npages;
+ 	struct c4iw_dev *rhp;
+ 	struct c4iw_pd *php;
+ 	struct c4iw_mr *mhp;
+ 	int ret;
+ 
+ 	PDBG("%s ib_pd %p\n", __func__, pd);
+ 	php = to_c4iw_pd(pd);
+ 	rhp = php->rhp;
+ 
+ 	mhp = kzalloc(sizeof(*mhp), GFP_KERNEL);
+ 	if (!mhp)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mhp->rhp = rhp;
+ 
+ 	/* First check that we have enough alignment */
+ 	if ((*iova_start & ~PAGE_MASK) != (buffer_list[0].addr & ~PAGE_MASK)) {
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	if (num_phys_buf > 1 &&
+ 	    ((buffer_list[0].addr + buffer_list[0].size) & ~PAGE_MASK)) {
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	ret = build_phys_page_list(buffer_list, num_phys_buf, iova_start,
+ 					&total_size, &npages, &shift,
+ 					&page_list);
+ 	if (ret)
+ 		goto err;
+ 
+ 	if (mr_exceeds_hw_limits(rhp, total_size)) {
+ 		kfree(page_list);
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 	ret = alloc_pbl(mhp, npages);
+ 	if (ret) {
+ 		kfree(page_list);
+ 		goto err;
+ 	}
+ 
+ 	ret = write_pbl(&mhp->rhp->rdev, page_list, mhp->attr.pbl_addr,
+ 			     npages);
+ 	kfree(page_list);
+ 	if (ret)
+ 		goto err_pbl;
+ 
+ 	mhp->attr.pdid = php->pdid;
+ 	mhp->attr.zbva = 0;
+ 
+ 	mhp->attr.perms = c4iw_ib_to_tpt_access(acc);
+ 	mhp->attr.va_fbo = *iova_start;
+ 	mhp->attr.page_size = shift - 12;
+ 
+ 	mhp->attr.len = (u32) total_size;
+ 	mhp->attr.pbl_size = npages;
+ 	ret = register_mem(rhp, php, mhp, shift);
+ 	if (ret)
+ 		goto err_pbl;
+ 
+ 	return &mhp->ibmr;
+ 
+ err_pbl:
+ 	c4iw_pblpool_free(&mhp->rhp->rdev, mhp->attr.pbl_addr,
+ 			      mhp->attr.pbl_size << 3);
+ 
+ err:
+ 	kfree(mhp);
+ 	return ERR_PTR(ret);
+ 
+ }
+ 
++>>>>>>> 2550a88d956f (RDMA/cxgb4: Limit MRs to < 8GB for T4/T5 devices)
  struct ib_mr *c4iw_get_dma_mr(struct ib_pd *pd, int acc)
  {
  	struct c4iw_dev *rhp;
* Unmerged path drivers/infiniband/hw/cxgb4/mem.c
