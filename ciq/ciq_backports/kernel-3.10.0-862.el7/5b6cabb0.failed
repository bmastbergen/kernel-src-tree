IB/hfi1: Add 16B RC/UC support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Don Hiatt <don.hiatt@intel.com>
commit 5b6cabb0db772042906cdc0fc235fe2a4f5a6000
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/5b6cabb0.failed

Add 16B bypass packet support for RC/UC traffic types.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Don Hiatt <don.hiatt@intel.com>
	Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 5b6cabb0db772042906cdc0fc235fe2a4f5a6000)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/hfi.h
#	drivers/infiniband/hw/hfi1/rc.c
#	drivers/infiniband/hw/hfi1/ruc.c
#	drivers/infiniband/hw/hfi1/uc.c
diff --cc drivers/infiniband/hw/hfi1/hfi.h
index 9719cf207532,1dfbf16c2ca9..000000000000
--- a/drivers/infiniband/hw/hfi1/hfi.h
+++ b/drivers/infiniband/hw/hfi1/hfi.h
@@@ -372,10 -344,133 +372,130 @@@ struct hfi1_packet 
  	u8 numpkt;
  	u8 rsize;
  	u8 updegr;
 +	u8 rcv_flags;
  	u8 etype;
 -	u8 extra_byte;
 -	u8 pad;
 -	u8 sc;
 -	u8 sl;
 -	u8 opcode;
 -	bool becn;
 -	bool fecn;
  };
  
++<<<<<<< HEAD
++=======
+ /* Packet types */
+ #define HFI1_PKT_TYPE_9B  0
+ #define HFI1_PKT_TYPE_16B 1
+ 
+ /*
+  * OPA 16B Header
+  */
+ #define OPA_16B_L4_MASK		0xFFull
+ #define OPA_16B_SC_MASK		0x1F00000ull
+ #define OPA_16B_SC_SHIFT	20
+ #define OPA_16B_LID_MASK	0xFFFFFull
+ #define OPA_16B_DLID_MASK	0xF000ull
+ #define OPA_16B_DLID_SHIFT	20
+ #define OPA_16B_DLID_HIGH_SHIFT	12
+ #define OPA_16B_SLID_MASK	0xF00ull
+ #define OPA_16B_SLID_SHIFT	20
+ #define OPA_16B_SLID_HIGH_SHIFT	8
+ #define OPA_16B_BECN_MASK       0x80000000ull
+ #define OPA_16B_BECN_SHIFT      31
+ #define OPA_16B_FECN_MASK       0x10000000ull
+ #define OPA_16B_FECN_SHIFT      28
+ #define OPA_16B_L2_MASK		0x60000000ull
+ #define OPA_16B_L2_SHIFT	29
+ #define OPA_16B_PKEY_MASK	0xFFFF0000ull
+ #define OPA_16B_PKEY_SHIFT	16
+ #define OPA_16B_LEN_MASK	0x7FF00000ull
+ #define OPA_16B_LEN_SHIFT	20
+ #define OPA_16B_RC_MASK		0xE000000ull
+ #define OPA_16B_RC_SHIFT	25
+ #define OPA_16B_AGE_MASK	0xFF0000ull
+ #define OPA_16B_AGE_SHIFT	16
+ #define OPA_16B_ENTROPY_MASK	0xFFFFull
+ 
+ /*
+  * OPA 16B L2/L4 Encodings
+  */
+ #define OPA_16B_L2_TYPE		0x02
+ #define OPA_16B_L4_IB_LOCAL	0x09
+ #define OPA_16B_L4_IB_GLOBAL	0x0A
+ #define OPA_16B_L4_ETHR		OPA_VNIC_L4_ETHR
+ 
+ static inline u8 hfi1_16B_get_l4(struct hfi1_16b_header *hdr)
+ {
+ 	return (u8)(hdr->lrh[2] & OPA_16B_L4_MASK);
+ }
+ 
+ static inline u8 hfi1_16B_get_sc(struct hfi1_16b_header *hdr)
+ {
+ 	return (u8)((hdr->lrh[1] & OPA_16B_SC_MASK) >> OPA_16B_SC_SHIFT);
+ }
+ 
+ static inline u32 hfi1_16B_get_dlid(struct hfi1_16b_header *hdr)
+ {
+ 	return (u32)((hdr->lrh[1] & OPA_16B_LID_MASK) |
+ 		     (((hdr->lrh[2] & OPA_16B_DLID_MASK) >>
+ 		     OPA_16B_DLID_HIGH_SHIFT) << OPA_16B_DLID_SHIFT));
+ }
+ 
+ static inline u32 hfi1_16B_get_slid(struct hfi1_16b_header *hdr)
+ {
+ 	return (u32)((hdr->lrh[0] & OPA_16B_LID_MASK) |
+ 		     (((hdr->lrh[2] & OPA_16B_SLID_MASK) >>
+ 		     OPA_16B_SLID_HIGH_SHIFT) << OPA_16B_SLID_SHIFT));
+ }
+ 
+ static inline u8 hfi1_16B_get_becn(struct hfi1_16b_header *hdr)
+ {
+ 	return (u8)((hdr->lrh[0] & OPA_16B_BECN_MASK) >> OPA_16B_BECN_SHIFT);
+ }
+ 
+ static inline u8 hfi1_16B_get_fecn(struct hfi1_16b_header *hdr)
+ {
+ 	return (u8)((hdr->lrh[1] & OPA_16B_FECN_MASK) >> OPA_16B_FECN_SHIFT);
+ }
+ 
+ static inline u8 hfi1_16B_get_l2(struct hfi1_16b_header *hdr)
+ {
+ 	return (u8)((hdr->lrh[1] & OPA_16B_L2_MASK) >> OPA_16B_L2_SHIFT);
+ }
+ 
+ static inline u16 hfi1_16B_get_pkey(struct hfi1_16b_header *hdr)
+ {
+ 	return (u16)((hdr->lrh[2] & OPA_16B_PKEY_MASK) >> OPA_16B_PKEY_SHIFT);
+ }
+ 
+ static inline u8 hfi1_16B_get_rc(struct hfi1_16b_header *hdr)
+ {
+ 	return (u8)((hdr->lrh[1] & OPA_16B_RC_MASK) >> OPA_16B_RC_SHIFT);
+ }
+ 
+ static inline u8 hfi1_16B_get_age(struct hfi1_16b_header *hdr)
+ {
+ 	return (u8)((hdr->lrh[3] & OPA_16B_AGE_MASK) >> OPA_16B_AGE_SHIFT);
+ }
+ 
+ static inline u16 hfi1_16B_get_len(struct hfi1_16b_header *hdr)
+ {
+ 	return (u16)((hdr->lrh[0] & OPA_16B_LEN_MASK) >> OPA_16B_LEN_SHIFT);
+ }
+ 
+ static inline u16 hfi1_16B_get_entropy(struct hfi1_16b_header *hdr)
+ {
+ 	return (u16)(hdr->lrh[3] & OPA_16B_ENTROPY_MASK);
+ }
+ 
+ #define OPA_16B_MAKE_QW(low_dw, high_dw) (((u64)(high_dw) << 32) | (low_dw))
+ 
+ /*
+  * BTH
+  */
+ #define OPA_16B_BTH_PAD_MASK	7
+ static inline u8 hfi1_16B_bth_get_pad(struct ib_other_headers *ohdr)
+ {
+ 	return (u8)((be32_to_cpu(ohdr->bth[0]) >> IB_BTH_PAD_SHIFT) &
+ 		   OPA_16B_BTH_PAD_MASK);
+ }
+ 
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  struct rvt_sge_state;
  
  /*
diff --cc drivers/infiniband/hw/hfi1/rc.c
index be9bcccf13a9,99defcc0ce45..000000000000
--- a/drivers/infiniband/hw/hfi1/rc.c
+++ b/drivers/infiniband/hw/hfi1/rc.c
@@@ -273,9 -276,23 +276,29 @@@ int hfi1_make_rc_req(struct rvt_qp *qp
  	if (IS_ERR(ps->s_txreq))
  		goto bail_no_tx;
  
++<<<<<<< HEAD
 +	ohdr = &ps->s_txreq->phdr.hdr.u.oth;
 +	if (qp->remote_ah_attr.ah_flags & IB_AH_GRH)
 +		ohdr = &ps->s_txreq->phdr.hdr.u.l.oth;
++=======
+ 	ps->s_txreq->phdr.hdr.hdr_type = priv->hdr_type;
+ 	if (priv->hdr_type == HFI1_PKT_TYPE_9B) {
+ 		/* header size in 32-bit words LRH+BTH = (8+12)/4. */
+ 		hwords = 5;
+ 		if (rdma_ah_get_ah_flags(&qp->remote_ah_attr) & IB_AH_GRH)
+ 			ohdr = &ps->s_txreq->phdr.hdr.ibh.u.l.oth;
+ 		else
+ 			ohdr = &ps->s_txreq->phdr.hdr.ibh.u.oth;
+ 	} else {
+ 		/* header size in 32-bit words 16B LRH+BTH = (16+12)/4. */
+ 		hwords = 7;
+ 		if ((rdma_ah_get_ah_flags(&qp->remote_ah_attr) & IB_AH_GRH) &&
+ 		    (hfi1_check_mcast(rdma_ah_get_dlid(&qp->remote_ah_attr))))
+ 			ohdr = &ps->s_txreq->phdr.hdr.opah.u.l.oth;
+ 		else
+ 			ohdr = &ps->s_txreq->phdr.hdr.opah.u.oth;
+ 	}
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  
  	/* Sending responses has higher priority over sending requests. */
  	if ((qp->s_flags & RVT_S_RESP_PENDING) &&
@@@ -703,107 -720,27 +726,111 @@@ bail_no_tx
  	return 0;
  }
  
- /**
-  * hfi1_send_rc_ack - Construct an ACK packet and send it
-  * @qp: a pointer to the QP
-  *
-  * This is called from hfi1_rc_rcv() and handle_receive_interrupt().
-  * Note that RDMA reads and atomics are handled in the
-  * send side QP state and send engine.
-  */
- void hfi1_send_rc_ack(struct hfi1_ctxtdata *rcd, struct rvt_qp *qp,
- 		      int is_fecn)
+ static inline void hfi1_make_bth_aeth(struct rvt_qp *qp,
+ 				      struct ib_other_headers *ohdr,
+ 				      u32 bth0, u32 bth1)
  {
++<<<<<<< HEAD
 +	struct hfi1_ibport *ibp = rcd_to_iport(rcd);
 +	struct hfi1_pportdata *ppd = ppd_from_ibp(ibp);
 +	u64 pbc, pbc_flags = 0;
 +	u16 lrh0;
 +	u16 sc5;
 +	u32 bth0;
 +	u32 hwords;
 +	u32 vl, plen;
 +	struct send_context *sc;
 +	struct pio_buf *pbuf;
 +	struct ib_header hdr;
 +	struct ib_other_headers *ohdr;
 +	unsigned long flags;
 +
 +	/* clear the defer count */
 +	qp->r_adefered = 0;
 +
 +	/* Don't send ACK or NAK if a RDMA read or atomic is pending. */
 +	if (qp->s_flags & RVT_S_RESP_PENDING)
 +		goto queue_ack;
 +
 +	/* Ensure s_rdma_ack_cnt changes are committed */
 +	smp_read_barrier_depends();
 +	if (qp->s_rdma_ack_cnt)
 +		goto queue_ack;
 +
 +	/* Construct the header */
 +	/* header size in 32-bit words LRH+BTH+AETH = (8+12+4)/4 */
 +	hwords = 6;
 +	if (unlikely(qp->remote_ah_attr.ah_flags & IB_AH_GRH)) {
 +		hwords += hfi1_make_grh(ibp, &hdr.u.l.grh,
 +				       &qp->remote_ah_attr.grh, hwords, 0);
 +		ohdr = &hdr.u.l.oth;
 +		lrh0 = HFI1_LRH_GRH;
 +	} else {
 +		ohdr = &hdr.u.oth;
 +		lrh0 = HFI1_LRH_BTH;
 +	}
 +	/* read pkey_index w/o lock (its atomic) */
 +	bth0 = hfi1_get_pkey(ibp, qp->s_pkey_index) | (OP(ACKNOWLEDGE) << 24);
 +	if (qp->s_mig_state == IB_MIG_MIGRATED)
 +		bth0 |= IB_BTH_MIG_REQ;
++=======
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  	if (qp->r_nak_state)
  		ohdr->u.aeth = cpu_to_be32((qp->r_msn & IB_MSN_MASK) |
  					    (qp->r_nak_state <<
  					     IB_AETH_CREDIT_SHIFT));
  	else
  		ohdr->u.aeth = rvt_compute_aeth(qp);
 -
++<<<<<<< HEAD
 +	sc5 = ibp->sl_to_sc[qp->remote_ah_attr.sl];
 +	/* set PBC_DC_INFO bit (aka SC[4]) in pbc_flags */
 +	pbc_flags |= ((!!(sc5 & 0x10)) << PBC_DC_INFO_SHIFT);
 +	lrh0 |= (sc5 & 0xf) << 12 | (qp->remote_ah_attr.sl & 0xf) << 4;
 +	hdr.lrh[0] = cpu_to_be16(lrh0);
 +	hdr.lrh[1] = cpu_to_be16(qp->remote_ah_attr.dlid);
 +	hdr.lrh[2] = cpu_to_be16(hwords + SIZE_OF_CRC);
 +	hdr.lrh[3] = cpu_to_be16(ppd->lid | qp->remote_ah_attr.src_path_bits);
++=======
++
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  	ohdr->bth[0] = cpu_to_be32(bth0);
- 	ohdr->bth[1] = cpu_to_be32(qp->remote_qpn);
- 	ohdr->bth[1] |= cpu_to_be32((!!is_fecn) << IB_BECN_SHIFT);
+ 	ohdr->bth[1] = cpu_to_be32(bth1 | qp->remote_qpn);
  	ohdr->bth[2] = cpu_to_be32(mask_psn(qp->r_ack_psn));
+ }
  
- 	/* Don't try to send ACKs if the link isn't ACTIVE */
- 	if (driver_lstate(ppd) != IB_PORT_ACTIVE)
- 		return;
+ static inline void hfi1_queue_rc_ack(struct rvt_qp *qp, bool is_fecn)
+ {
+ 	struct hfi1_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);
+ 	unsigned long flags;
  
++<<<<<<< HEAD
 +	sc = rcd->sc;
 +	plen = 2 /* PBC */ + hwords;
 +	vl = sc_to_vlt(ppd->dd, sc5);
 +	pbc = create_pbc(ppd, pbc_flags, qp->srate_mbps, vl, plen);
 +
 +	pbuf = sc_buffer_alloc(sc, plen, NULL, NULL);
 +	if (!pbuf) {
 +		/*
 +		 * We have no room to send at the moment.  Pass
 +		 * responsibility for sending the ACK to the send engine
 +		 * so that when enough buffer space becomes available,
 +		 * the ACK is sent ahead of other outgoing packets.
 +		 */
 +		goto queue_ack;
 +	}
 +
 +	trace_ack_output_ibhdr(dd_from_ibdev(qp->ibqp.device),
 +			       &hdr, ib_is_sc5(sc5));
 +
 +	/* write the pbc and data */
 +	ppd->dd->pio_inline_send(ppd->dd, pbuf, pbc, &hdr, hwords);
 +
 +	return;
 +
 +queue_ack:
++=======
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  	spin_lock_irqsave(&qp->s_lock, flags);
  	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK))
  		goto unlock;
@@@ -982,10 -1101,13 +1191,16 @@@ static void reset_sending_psn(struct rv
  /*
   * This should be called with the QP s_lock held and interrupts disabled.
   */
 -void hfi1_rc_send_complete(struct rvt_qp *qp, struct hfi1_opa_header *opah)
 +void hfi1_rc_send_complete(struct rvt_qp *qp, struct ib_header *hdr)
  {
  	struct ib_other_headers *ohdr;
++<<<<<<< HEAD
++=======
+ 	struct hfi1_qp_priv *priv = qp->priv;
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  	struct rvt_swqe *wqe;
+ 	struct ib_header *hdr = NULL;
+ 	struct hfi1_16b_header *hdr_16b = NULL;
  	u32 opcode;
  	u32 psn;
  
@@@ -1919,10 -2041,11 +2135,18 @@@ void hfi1_rc_rcv(struct hfi1_packet *pa
  	struct rvt_qp *qp = packet->qp;
  	struct hfi1_ibport *ibp = rcd_to_iport(rcd);
  	struct ib_other_headers *ohdr = packet->ohdr;
++<<<<<<< HEAD
 +	u32 bth0, opcode;
 +	u32 hdrsize = packet->hlen;
 +	u32 psn;
 +	u32 pad;
++=======
+ 	u32 bth0 = be32_to_cpu(ohdr->bth[0]);
+ 	u32 opcode = packet->opcode;
+ 	u32 hdrsize = packet->hlen;
+ 	u32 psn = ib_bth_get_psn(packet->ohdr);
+ 	u32 pad = packet->pad;
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  	struct ib_wc wc;
  	u32 pmtu = qp->pmtu;
  	int diff;
@@@ -1932,16 -2055,14 +2156,25 @@@
  	bool is_fecn = false;
  	bool copy_last = false;
  	u32 rkey;
+ 	u8 extra_bytes = pad + packet->extra_byte + (SIZE_OF_CRC << 2);
  
  	lockdep_assert_held(&qp->r_lock);
++<<<<<<< HEAD
 +	bth0 = be32_to_cpu(ohdr->bth[0]);
 +	if (hfi1_ruc_check_hdr(ibp, hdr, rcv_flags & HFI1_HAS_GRH, qp, bth0))
 +		return;
 +
 +	is_fecn = process_ecn(qp, packet, false);
 +
 +	psn = be32_to_cpu(ohdr->bth[2]);
 +	opcode = ib_bth_get_opcode(ohdr);
++=======
+ 
+ 	if (hfi1_ruc_check_hdr(ibp, packet))
+ 		return;
+ 
+ 	is_fecn = process_ecn(qp, packet, false);
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  
  	/*
  	 * Process responses (ACKs) before anything else.  Note that the
@@@ -2071,14 -2196,12 +2308,14 @@@ no_immediate_data
  		wc.wc_flags = 0;
  		wc.ex.imm_data = 0;
  send_last:
 +		/* Get the number of bytes the message was padded by. */
 +		pad = ib_bth_get_pad(ohdr);
  		/* Check for invalid length. */
  		/* LAST len should be >= 1 */
- 		if (unlikely(tlen < (hdrsize + pad + 4)))
+ 		if (unlikely(tlen < (hdrsize + extra_bytes)))
  			goto nack_inv;
- 		/* Don't count the CRC. */
- 		tlen -= (hdrsize + pad + 4);
+ 		/* Don't count the CRC(and padding and LT byte for 16B). */
+ 		tlen -= (hdrsize + extra_bytes);
  		wc.byte_len = tlen + qp->r_rcv_len;
  		if (unlikely(wc.byte_len > qp->r_len))
  			goto nack_inv;
diff --cc drivers/infiniband/hw/hfi1/ruc.c
index 675f4ef89dc3,b3291f0fde9a..000000000000
--- a/drivers/infiniband/hw/hfi1/ruc.c
+++ b/drivers/infiniband/hw/hfi1/ruc.c
@@@ -723,25 -897,7 +880,23 @@@ void hfi1_make_ruc_header(struct rvt_q
  			  struct hfi1_pkt_state *ps)
  {
  	struct hfi1_qp_priv *priv = qp->priv;
- 	struct hfi1_ibport *ibp = ps->ibp;
- 	u16 lrh0;
- 	u32 nwords;
- 	u32 extra_bytes;
- 	u32 bth1;
  
++<<<<<<< HEAD
 +	/* Construct the header. */
 +	extra_bytes = -ps->s_txreq->s_cur_size & 3;
 +	nwords = (ps->s_txreq->s_cur_size + extra_bytes) >> 2;
 +	lrh0 = HFI1_LRH_BTH;
 +	if (unlikely(qp->remote_ah_attr.ah_flags & IB_AH_GRH)) {
 +		qp->s_hdrwords += hfi1_make_grh(ibp,
 +						&ps->s_txreq->phdr.hdr.u.l.grh,
 +						&qp->remote_ah_attr.grh,
 +						qp->s_hdrwords, nwords);
 +		lrh0 = HFI1_LRH_GRH;
 +		middle = 0;
 +	}
 +	lrh0 |= (priv->s_sc & 0xf) << 12 | (qp->remote_ah_attr.sl & 0xf) << 4;
++=======
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  	/*
  	 * reset s_ahg/AHG fields
  	 *
@@@ -756,31 -912,9 +911,37 @@@
  	priv->s_ahg->tx_flags = 0;
  	priv->s_ahg->ahgcount = 0;
  	priv->s_ahg->ahgidx = 0;
++<<<<<<< HEAD
 +	if (qp->s_mig_state == IB_MIG_MIGRATED)
 +		bth0 |= IB_BTH_MIG_REQ;
 +	else
 +		middle = 0;
 +	if (middle)
 +		build_ahg(qp, bth2);
 +	else
 +		qp->s_flags &= ~RVT_S_AHG_VALID;
 +	ps->s_txreq->phdr.hdr.lrh[0] = cpu_to_be16(lrh0);
 +	ps->s_txreq->phdr.hdr.lrh[1] = cpu_to_be16(qp->remote_ah_attr.dlid);
 +	ps->s_txreq->phdr.hdr.lrh[2] =
 +		cpu_to_be16(qp->s_hdrwords + nwords + SIZE_OF_CRC);
 +	ps->s_txreq->phdr.hdr.lrh[3] = cpu_to_be16(ppd_from_ibp(ibp)->lid |
 +				       qp->remote_ah_attr.src_path_bits);
 +	bth0 |= hfi1_get_pkey(ibp, qp->s_pkey_index);
 +	bth0 |= extra_bytes << 20;
 +	ohdr->bth[0] = cpu_to_be32(bth0);
 +	bth1 = qp->remote_qpn;
 +	if (qp->s_flags & RVT_S_ECN) {
 +		qp->s_flags &= ~RVT_S_ECN;
 +		/* we recently received a FECN, so return a BECN */
 +		bth1 |= (IB_BECN_MASK << IB_BECN_SHIFT);
 +	}
 +	ohdr->bth[1] = cpu_to_be32(bth1);
 +	ohdr->bth[2] = cpu_to_be32(bth2);
++=======
+ 
+ 	/* Make the appropriate header */
+ 	hfi1_ruc_header_tbl[priv->hdr_type](qp, ohdr, bth0, bth2, middle, ps);
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  }
  
  /* when sending, force a reschedule every one of these periods */
diff --cc drivers/infiniband/hw/hfi1/uc.c
index f0bdb100e005,0b646173ca22..000000000000
--- a/drivers/infiniband/hw/hfi1/uc.c
+++ b/drivers/infiniband/hw/hfi1/uc.c
@@@ -93,9 -93,23 +93,29 @@@ int hfi1_make_uc_req(struct rvt_qp *qp
  		goto done_free_tx;
  	}
  
++<<<<<<< HEAD
 +	ohdr = &ps->s_txreq->phdr.hdr.u.oth;
 +	if (qp->remote_ah_attr.ah_flags & IB_AH_GRH)
 +		ohdr = &ps->s_txreq->phdr.hdr.u.l.oth;
++=======
+ 	ps->s_txreq->phdr.hdr.hdr_type = priv->hdr_type;
+ 	if (priv->hdr_type == HFI1_PKT_TYPE_9B) {
+ 		/* header size in 32-bit words LRH+BTH = (8+12)/4. */
+ 		hwords = 5;
+ 		if (rdma_ah_get_ah_flags(&qp->remote_ah_attr) & IB_AH_GRH)
+ 			ohdr = &ps->s_txreq->phdr.hdr.ibh.u.l.oth;
+ 		else
+ 			ohdr = &ps->s_txreq->phdr.hdr.ibh.u.oth;
+ 	} else {
+ 		/* header size in 32-bit words 16B LRH+BTH = (16+12)/4. */
+ 		hwords = 7;
+ 		if ((rdma_ah_get_ah_flags(&qp->remote_ah_attr) & IB_AH_GRH) &&
+ 		    (hfi1_check_mcast(rdma_ah_get_dlid(&qp->remote_ah_attr))))
+ 			ohdr = &ps->s_txreq->phdr.hdr.opah.u.l.oth;
+ 		else
+ 			ohdr = &ps->s_txreq->phdr.hdr.opah.u.oth;
+ 	}
++>>>>>>> 5b6cabb0db77 (IB/hfi1: Add 16B RC/UC support)
  
  	/* Get the next send request. */
  	wqe = rvt_get_swqe_ptr(qp, qp->s_cur);
@@@ -310,11 -322,10 +330,12 @@@ void hfi1_uc_rcv(struct hfi1_packet *pa
  	struct ib_wc wc;
  	u32 pmtu = qp->pmtu;
  	struct ib_reth *reth;
 +	int has_grh = rcv_flags & HFI1_HAS_GRH;
  	int ret;
+ 	u8 extra_bytes = pad + packet->extra_byte + (SIZE_OF_CRC << 2);
  
 -	if (hfi1_ruc_check_hdr(ibp, packet))
 +	bth0 = be32_to_cpu(ohdr->bth[0]);
 +	if (hfi1_ruc_check_hdr(ibp, hdr, has_grh, qp, bth0))
  		return;
  
  	process_ecn(qp, packet, true);
@@@ -432,14 -446,12 +458,14 @@@ no_immediate_data
  		wc.ex.imm_data = 0;
  		wc.wc_flags = 0;
  send_last:
 +		/* Get the number of bytes the message was padded by. */
 +		pad = ib_bth_get_pad(ohdr);
  		/* Check for invalid length. */
  		/* LAST len should be >= 1 */
- 		if (unlikely(tlen < (hdrsize + pad + 4)))
+ 		if (unlikely(tlen < (hdrsize + extra_bytes)))
  			goto rewind;
  		/* Don't count the CRC. */
- 		tlen -= (hdrsize + pad + 4);
+ 		tlen -= (hdrsize + extra_bytes);
  		wc.byte_len = tlen + qp->r_rcv_len;
  		if (unlikely(wc.byte_len > qp->r_len))
  			goto rewind;
* Unmerged path drivers/infiniband/hw/hfi1/hfi.h
* Unmerged path drivers/infiniband/hw/hfi1/rc.c
* Unmerged path drivers/infiniband/hw/hfi1/ruc.c
* Unmerged path drivers/infiniband/hw/hfi1/uc.c
diff --git a/drivers/infiniband/hw/hfi1/verbs.h b/drivers/infiniband/hw/hfi1/verbs.h
index 76081f770f70..dd7ea8676c84 100644
--- a/drivers/infiniband/hw/hfi1/verbs.h
+++ b/drivers/infiniband/hw/hfi1/verbs.h
@@ -365,7 +365,8 @@ void hfi1_do_send(struct rvt_qp *qp, bool in_thread);
 void hfi1_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,
 			enum ib_wc_status status);
 
-void hfi1_send_rc_ack(struct hfi1_ctxtdata *, struct rvt_qp *qp, int is_fecn);
+void hfi1_send_rc_ack(struct hfi1_ctxtdata *rcd, struct rvt_qp *qp,
+		      bool is_fecn);
 
 int hfi1_make_rc_req(struct rvt_qp *qp, struct hfi1_pkt_state *ps);
 
