treewide: Consolidate get_dma_ops() implementations

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Bart Van Assche <bart.vanassche@sandisk.com>
commit 815dd18788fe0d41899f51b91d0560279cf16b0d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/815dd187.failed

Introduce a new architecture-specific get_arch_dma_ops() function
that takes a struct bus_type * argument. Add get_dma_ops() in
<linux/dma-mapping.h>.

	Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Cc: David Woodhouse <dwmw2@infradead.org>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: linux-arch@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: x86@kernel.org
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 815dd18788fe0d41899f51b91d0560279cf16b0d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/alpha/include/asm/dma-mapping.h
#	arch/arc/include/asm/dma-mapping.h
#	arch/arm/include/asm/dma-mapping.h
#	arch/arm64/include/asm/dma-mapping.h
#	arch/avr32/include/asm/dma-mapping.h
#	arch/blackfin/include/asm/dma-mapping.h
#	arch/c6x/include/asm/dma-mapping.h
#	arch/cris/include/asm/dma-mapping.h
#	arch/frv/include/asm/dma-mapping.h
#	arch/h8300/include/asm/dma-mapping.h
#	arch/hexagon/include/asm/dma-mapping.h
#	arch/ia64/include/asm/dma-mapping.h
#	arch/m32r/include/asm/dma-mapping.h
#	arch/m68k/include/asm/dma-mapping.h
#	arch/metag/include/asm/dma-mapping.h
#	arch/microblaze/include/asm/dma-mapping.h
#	arch/mips/include/asm/dma-mapping.h
#	arch/mn10300/include/asm/dma-mapping.h
#	arch/nios2/include/asm/dma-mapping.h
#	arch/openrisc/include/asm/dma-mapping.h
#	arch/parisc/include/asm/dma-mapping.h
#	arch/powerpc/include/asm/dma-mapping.h
#	arch/s390/include/asm/dma-mapping.h
#	arch/sh/include/asm/dma-mapping.h
#	arch/sparc/include/asm/dma-mapping.h
#	arch/tile/include/asm/dma-mapping.h
#	arch/unicore32/include/asm/dma-mapping.h
#	arch/x86/include/asm/dma-mapping.h
#	arch/xtensa/include/asm/dma-mapping.h
#	include/linux/dma-mapping.h
diff --cc arch/alpha/include/asm/dma-mapping.h
index dfa32f061320,5d53666935e6..000000000000
--- a/arch/alpha/include/asm/dma-mapping.h
+++ b/arch/alpha/include/asm/dma-mapping.h
@@@ -1,11 -1,9 +1,15 @@@
  #ifndef _ALPHA_DMA_MAPPING_H
  #define _ALPHA_DMA_MAPPING_H
  
 -extern const struct dma_map_ops *dma_ops;
 +#include <linux/dma-attrs.h>
  
++<<<<<<< HEAD
 +extern struct dma_map_ops *dma_ops;
 +
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
  	return dma_ops;
  }
diff --cc arch/arc/include/asm/dma-mapping.h
index 45b8e0cea176,94285031c4fb..000000000000
--- a/arch/arc/include/asm/dma-mapping.h
+++ b/arch/arc/include/asm/dma-mapping.h
@@@ -11,211 -11,18 +11,215 @@@
  #ifndef ASM_ARC_DMA_MAPPING_H
  #define ASM_ARC_DMA_MAPPING_H
  
 -#ifndef CONFIG_ARC_PLAT_NEEDS_PHYS_TO_DMA
 -#define plat_dma_to_phys(dev, dma_handle) ((phys_addr_t)(dma_handle))
 -#define plat_phys_to_dma(dev, paddr) ((dma_addr_t)(paddr))
 +#include <asm-generic/dma-coherent.h>
 +#include <asm/cacheflush.h>
 +
 +#ifndef CONFIG_ARC_PLAT_NEEDS_CPU_TO_DMA
 +/*
 + * dma_map_* API take cpu addresses, which is kernel logical address in the
 + * untranslated address space (0x8000_0000) based. The dma address (bus addr)
 + * ideally needs to be 0x0000_0000 based hence these glue routines.
 + * However given that intermediate bus bridges can ignore the high bit, we can
 + * do with these routines being no-ops.
 + * If a platform/device comes up which sriclty requires 0 based bus addr
 + * (e.g. AHB-PCI bridge on Angel4 board), then it can provide it's own versions
 + */
 +#define plat_dma_addr_to_kernel(dev, addr) ((unsigned long)(addr))
 +#define plat_kernel_addr_to_dma(dev, ptr) ((dma_addr_t)(ptr))
 +
  #else
 -#include <plat/dma.h>
 +#include <plat/dma_addr.h>
  #endif
  
 -extern const struct dma_map_ops arc_dma_ops;
 +void *dma_alloc_noncoherent(struct device *dev, size_t size,
 +			    dma_addr_t *dma_handle, gfp_t gfp);
 +
++<<<<<<< HEAD
 +void dma_free_noncoherent(struct device *dev, size_t size, void *vaddr,
 +			  dma_addr_t dma_handle);
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			 dma_addr_t *dma_handle, gfp_t gfp);
 +
 +void dma_free_coherent(struct device *dev, size_t size, void *kvaddr,
 +		       dma_addr_t dma_handle);
 +
 +/* drivers/base/dma-mapping.c */
 +extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +extern int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size);
 +
 +#define dma_mmap_coherent(d, v, c, h, s) dma_common_mmap(d, v, c, h, s)
 +#define dma_get_sgtable(d, t, v, h, s) dma_common_get_sgtable(d, t, v, h, s)
  
 +/*
 + * streaming DMA Mapping API...
 + * CPU accesses page via normal paddr, thus needs to explicitly made
 + * consistent before each use
 + */
 +
 +static inline void __inline_dma_cache_sync(unsigned long paddr, size_t size,
 +					   enum dma_data_direction dir)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	switch (dir) {
 +	case DMA_FROM_DEVICE:
 +		dma_cache_inv(paddr, size);
 +		break;
 +	case DMA_TO_DEVICE:
 +		dma_cache_wback(paddr, size);
 +		break;
 +	case DMA_BIDIRECTIONAL:
 +		dma_cache_wback_inv(paddr, size);
 +		break;
 +	default:
 +		pr_err("Invalid DMA dir [%d] for OP @ %lx\n", dir, paddr);
 +	}
 +}
 +
 +void __arc_dma_cache_sync(unsigned long paddr, size_t size,
 +			  enum dma_data_direction dir);
 +
 +#define _dma_cache_sync(addr, sz, dir)			\
 +do {							\
 +	if (__builtin_constant_p(dir))			\
 +		__inline_dma_cache_sync(addr, sz, dir);	\
 +	else						\
 +		__arc_dma_cache_sync(addr, sz, dir);	\
 +}							\
 +while (0);
 +
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 +	       enum dma_data_direction dir)
 +{
 +	_dma_cache_sync((unsigned long)cpu_addr, size, dir);
 +	return plat_kernel_addr_to_dma(dev, cpu_addr);
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr,
 +		 size_t size, enum dma_data_direction dir)
 +{
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page,
 +	     unsigned long offset, size_t size,
 +	     enum dma_data_direction dir)
 +{
 +	unsigned long paddr = page_to_phys(page) + offset;
 +	return dma_map_single(dev, (void *)paddr, size, dir);
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_handle,
 +	       size_t size, enum dma_data_direction dir)
 +{
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg,
 +	   int nents, enum dma_data_direction dir)
 +{
 +	struct scatterlist *s;
 +	int i;
 +
 +	for_each_sg(sg, s, nents, i)
 +		s->dma_address = dma_map_page(dev, sg_page(s), s->offset,
 +					       s->length, dir);
 +
 +	return nents;
 +}
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg,
 +	     int nents, enum dma_data_direction dir)
 +{
 +	struct scatterlist *s;
 +	int i;
 +
 +	for_each_sg(sg, s, nents, i)
 +		dma_unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			size_t size, enum dma_data_direction dir)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle), size,
 +			DMA_FROM_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +			   size_t size, enum dma_data_direction dir)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle), size,
 +			DMA_TO_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle) + offset,
 +			size, DMA_FROM_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle) + offset,
 +			size, DMA_TO_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction dir)
  {
 -	return &arc_dma_ops;
 +	int i;
 +
 +	for (i = 0; i < nelems; i++, sg++)
 +		_dma_cache_sync((unsigned int)sg_virt(sg), sg->length, dir);
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		       enum dma_data_direction dir)
 +{
 +	int i;
 +
 +	for (i = 0; i < nelems; i++, sg++)
 +		_dma_cache_sync((unsigned int)sg_virt(sg), sg->length, dir);
 +}
 +
 +static inline int dma_supported(struct device *dev, u64 dma_mask)
 +{
 +	/* Support 32 bit DMA mask exclusively */
 +	return dma_mask == DMA_BIT_MASK(32);
 +}
 +
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = dma_mask;
 +
 +	return 0;
  }
  
  #endif
diff --cc arch/arm/include/asm/dma-mapping.h
index 5b579b951503,716656925975..000000000000
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@@ -22,18 -23,16 +22,27 @@@ static inline struct dma_map_ops *get_d
  	return &arm_dma_ops;
  }
  
++<<<<<<< HEAD
 +static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 +{
 +	BUG_ON(!dev);
 +	dev->archdata.dma_ops = ops;
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
+ 	if (xen_initial_domain())
+ 		return xen_dma_ops;
+ 	else
+ 		return __generic_dma_ops(NULL);
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  }
  
 -#define HAVE_ARCH_DMA_SUPPORTED 1
 -extern int dma_supported(struct device *dev, u64 mask);
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	return get_dma_ops(dev)->set_dma_mask(dev, mask);
 +}
  
  #ifdef __arch_page_to_dma
  #error Please update to __arch_pfn_to_dma
diff --cc arch/arm64/include/asm/dma-mapping.h
index 994776894198,505756cdc67a..000000000000
--- a/arch/arm64/include/asm/dma-mapping.h
+++ b/arch/arm64/include/asm/dma-mapping.h
@@@ -21,52 -21,61 +21,71 @@@
  #include <linux/types.h>
  #include <linux/vmalloc.h>
  
 -#include <xen/xen.h>
 -#include <asm/xen/hypervisor.h>
 +#include <asm-generic/dma-coherent.h>
  
 -#define DMA_ERROR_CODE	(~(dma_addr_t)0)
 -extern const struct dma_map_ops dummy_dma_ops;
 +#define ARCH_HAS_DMA_GET_REQUIRED_MASK
  
 -static inline const struct dma_map_ops *__generic_dma_ops(struct device *dev)
 +extern struct dma_map_ops *dma_ops;
 +
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
  {
++<<<<<<< HEAD
 +	if (unlikely(!dev) || !dev->archdata.dma_ops)
 +		return dma_ops;
 +	else
 +		return dev->archdata.dma_ops;
++=======
+ 	if (dev && dev->dma_ops)
+ 		return dev->dma_ops;
+ 
+ 	/*
+ 	 * We expect no ISA devices, and all other DMA masters are expected to
+ 	 * have someone call arch_setup_dma_ops at device creation time.
+ 	 */
+ 	return &dummy_dma_ops;
+ }
+ 
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
+ 	if (xen_initial_domain())
+ 		return xen_dma_ops;
+ 	else
+ 		return __generic_dma_ops(NULL);
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  }
  
 -void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
 -			const struct iommu_ops *iommu, bool coherent);
 -#define arch_setup_dma_ops	arch_setup_dma_ops
 +#include <asm-generic/dma-mapping-common.h>
  
 -#ifdef CONFIG_IOMMU_DMA
 -void arch_teardown_dma_ops(struct device *dev);
 -#define arch_teardown_dma_ops	arch_teardown_dma_ops
 -#endif
 +static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 +{
 +	return (dma_addr_t)paddr;
 +}
  
 -/* do not use this function in a driver */
 -static inline bool is_device_dma_coherent(struct device *dev)
 +static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
  {
 -	if (!dev)
 -		return false;
 -	return dev->archdata.dma_coherent;
 +	return (phys_addr_t)dev_addr;
  }
  
 -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dev_addr)
  {
 -	dma_addr_t dev_addr = (dma_addr_t)paddr;
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	debug_dma_mapping_error(dev, dev_addr);
 +	return ops->mapping_error(dev, dev_addr);
 +}
  
 -	return dev_addr - ((dma_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	return ops->dma_supported(dev, mask);
  }
  
 -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
 +static inline int dma_set_mask(struct device *dev, u64 mask)
  {
 -	phys_addr_t paddr = (phys_addr_t)dev_addr;
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +	*dev->dma_mask = mask;
  
 -	return paddr + ((phys_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
 +	return 0;
  }
  
  static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
diff --cc arch/avr32/include/asm/dma-mapping.h
index b3d18f9f3e8d,7388451f9905..000000000000
--- a/arch/avr32/include/asm/dma-mapping.h
+++ b/arch/avr32/include/asm/dma-mapping.h
@@@ -11,339 -4,11 +11,345 @@@
  extern void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
  	int direction);
  
++<<<<<<< HEAD
 +/*
 + * Return whether the given device DMA address mask can be supported
 + * properly.  For example, if your device can only drive the low 24-bits
 + * during bus mastering, then you would pass 0x00ffffff as the mask
 + * to this function.
 + */
 +static inline int dma_supported(struct device *dev, u64 mask)
++=======
+ extern const struct dma_map_ops avr32_dma_ops;
+ 
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	/* Fix when needed. I really don't know of any limitations */
 +	return 1;
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = dma_mask;
 +	return 0;
 +}
 +
 +/*
 + * dma_map_single can't fail as it is implemented now.
 + */
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t addr)
 +{
 +	return 0;
 +}
 +
 +/**
 + * dma_alloc_coherent - allocate consistent memory for DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: required memory size
 + * @handle: bus-specific DMA address
 + *
 + * Allocate some uncached, unbuffered memory for a device for
 + * performing DMA.  This function allocates pages, and will
 + * return the CPU-viewed address, and sets @handle to be the
 + * device-viewed address.
 + */
 +extern void *dma_alloc_coherent(struct device *dev, size_t size,
 +				dma_addr_t *handle, gfp_t gfp);
 +
 +/**
 + * dma_free_coherent - free memory allocated by dma_alloc_coherent
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: size of memory originally requested in dma_alloc_coherent
 + * @cpu_addr: CPU-view address returned from dma_alloc_coherent
 + * @handle: device-view address returned from dma_alloc_coherent
 + *
 + * Free (and unmap) a DMA buffer previously allocated by
 + * dma_alloc_coherent().
 + *
 + * References to memory and mappings associated with cpu_addr/handle
 + * during and after this call executing are illegal.
 + */
 +extern void dma_free_coherent(struct device *dev, size_t size,
 +			      void *cpu_addr, dma_addr_t handle);
 +
 +/**
 + * dma_alloc_writecombine - allocate write-combining memory for DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: required memory size
 + * @handle: bus-specific DMA address
 + *
 + * Allocate some uncached, buffered memory for a device for
 + * performing DMA.  This function allocates pages, and will
 + * return the CPU-viewed address, and sets @handle to be the
 + * device-viewed address.
 + */
 +extern void *dma_alloc_writecombine(struct device *dev, size_t size,
 +				    dma_addr_t *handle, gfp_t gfp);
 +
 +/**
 + * dma_free_coherent - free memory allocated by dma_alloc_writecombine
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: size of memory originally requested in dma_alloc_writecombine
 + * @cpu_addr: CPU-view address returned from dma_alloc_writecombine
 + * @handle: device-view address returned from dma_alloc_writecombine
 + *
 + * Free (and unmap) a DMA buffer previously allocated by
 + * dma_alloc_writecombine().
 + *
 + * References to memory and mappings associated with cpu_addr/handle
 + * during and after this call executing are illegal.
 + */
 +extern void dma_free_writecombine(struct device *dev, size_t size,
 +				  void *cpu_addr, dma_addr_t handle);
 +
 +/**
 + * dma_map_single - map a single buffer for streaming DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @cpu_addr: CPU direct mapped address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Ensure that any data held in the cache is appropriately discarded
 + * or written back.
 + *
 + * The device owns this memory once this call has completed.  The CPU
 + * can regain ownership by calling dma_unmap_single() or dma_sync_single().
 + */
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	dma_cache_sync(dev, cpu_addr, size, direction);
 +	return virt_to_bus(cpu_addr);
 +}
 +
 +/**
 + * dma_unmap_single - unmap a single buffer previously mapped
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @handle: DMA address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Unmap a single streaming mode DMA translation.  The handle and size
 + * must match what was provided in the previous dma_map_single() call.
 + * All other usages are undefined.
 + *
 + * After this call, reads by the CPU to the buffer are guaranteed to see
 + * whatever the device wrote there.
 + */
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +
 +}
 +
 +/**
 + * dma_map_page - map a portion of a page for streaming DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @page: page that buffer resides in
 + * @offset: offset into page for start of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Ensure that any data held in the cache is appropriately discarded
 + * or written back.
 + *
 + * The device owns this memory once this call has completed.  The CPU
 + * can regain ownership by calling dma_unmap_page() or dma_sync_single().
 + */
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page,
 +	     unsigned long offset, size_t size,
 +	     enum dma_data_direction direction)
 +{
 +	return dma_map_single(dev, page_address(page) + offset,
 +			      size, direction);
 +}
 +
 +/**
 + * dma_unmap_page - unmap a buffer previously mapped through dma_map_page()
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @handle: DMA address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Unmap a single streaming mode DMA translation.  The handle and size
 + * must match what was provided in the previous dma_map_single() call.
 + * All other usages are undefined.
 + *
 + * After this call, reads by the CPU to the buffer are guaranteed to see
 + * whatever the device wrote there.
 + */
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	dma_unmap_single(dev, dma_address, size, direction);
 +}
 +
 +/**
 + * dma_map_sg - map a set of SG buffers for streaming mode DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @sg: list of buffers
 + * @nents: number of buffers to map
 + * @dir: DMA transfer direction
 + *
 + * Map a set of buffers described by scatterlist in streaming
 + * mode for DMA.  This is the scatter-gather version of the
 + * above pci_map_single interface.  Here the scatter gather list
 + * elements are each tagged with the appropriate dma address
 + * and length.  They are obtained via sg_dma_{address,length}(SG).
 + *
 + * NOTE: An implementation may be able to use a smaller number of
 + *       DMA address/length pairs than there are SG table elements.
 + *       (for example via virtual mapping capabilities)
 + *       The routine returns the number of addr/length pairs actually
 + *       used, at most nents.
 + *
 + * Device ownership issues as mentioned above for pci_map_single are
 + * the same here.
 + */
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	for (i = 0; i < nents; i++) {
 +		char *virt;
 +
 +		sg[i].dma_address = page_to_bus(sg_page(&sg[i])) + sg[i].offset;
 +		virt = sg_virt(&sg[i]);
 +		dma_cache_sync(dev, virt, sg[i].length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +/**
 + * dma_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @sg: list of buffers
 + * @nents: number of buffers to map
 + * @dir: DMA transfer direction
 + *
 + * Unmap a set of streaming mode DMA translations.
 + * Again, CPU read rules concerning calls here are the same as for
 + * pci_unmap_single() above.
 + */
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +
 +}
 +
 +/**
 + * dma_sync_single_for_cpu
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @handle: DMA address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Make physical memory consistent for a single streaming mode DMA
 + * translation after a transfer.
 + *
 + * If you perform a dma_map_single() but wish to interrogate the
 + * buffer using the cpu, yet do not wish to teardown the DMA mapping,
 + * you must call this function before doing so.  At the next point you
 + * give the DMA address back to the card, you must first perform a
 + * dma_sync_single_for_device, and then the device again owns the
 + * buffer.
 + */
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			size_t size, enum dma_data_direction direction)
 +{
 +	/*
 +	 * No need to do anything since the CPU isn't supposed to
 +	 * touch this memory after we flushed it at mapping- or
 +	 * sync-for-device time.
 +	 */
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +			   size_t size, enum dma_data_direction direction)
 +{
 +	dma_cache_sync(dev, bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
  {
 -	return &avr32_dma_ops;
 +	/* just sync everything, that's all the pci API can do */
 +	dma_sync_single_for_cpu(dev, dma_handle, offset+size, direction);
  }
  
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +	/* just sync everything, that's all the pci API can do */
 +	dma_sync_single_for_device(dev, dma_handle, offset+size, direction);
 +}
 +
 +/**
 + * dma_sync_sg_for_cpu
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @sg: list of buffers
 + * @nents: number of buffers to map
 + * @dir: DMA transfer direction
 + *
 + * Make physical memory consistent for a set of streaming
 + * mode DMA translations after a transfer.
 + *
 + * The same as dma_sync_single_for_* but for a scatter-gather list,
 + * same rules and usage.
 + */
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 +		    int nents, enum dma_data_direction direction)
 +{
 +	/*
 +	 * No need to do anything since the CPU isn't supposed to
 +	 * touch this memory after we flushed it at mapping- or
 +	 * sync-for-device time.
 +	 */
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 +		       int nents, enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	for (i = 0; i < nents; i++) {
 +		dma_cache_sync(dev, sg_virt(&sg[i]), sg[i].length, direction);
 +	}
 +}
 +
 +/* Now for the API extensions over the pci_ one */
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +/* drivers/base/dma-mapping.c */
 +extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +extern int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size);
 +
 +#define dma_mmap_coherent(d, v, c, h, s) dma_common_mmap(d, v, c, h, s)
 +#define dma_get_sgtable(d, t, v, h, s) dma_common_get_sgtable(d, t, v, h, s)
 +
  #endif /* __ASM_AVR32_DMA_MAPPING_H */
diff --cc arch/blackfin/include/asm/dma-mapping.h
index 054d9ec57d9d,04254ac36bed..000000000000
--- a/arch/blackfin/include/asm/dma-mapping.h
+++ b/arch/blackfin/include/asm/dma-mapping.h
@@@ -66,102 -36,11 +66,108 @@@ _dma_sync(dma_addr_t addr, size_t size
  		__dma_sync(addr, size, dir);
  }
  
++<<<<<<< HEAD
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction dir)
++=======
+ extern const struct dma_map_ops bfin_dma_ops;
+ 
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	_dma_sync((dma_addr_t)ptr, size, dir);
 +	return (dma_addr_t) ptr;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page,
 +	     unsigned long offset, size_t size,
 +	     enum dma_data_direction dir)
 +{
 +	return dma_map_single(dev, page_address(page) + offset, size, dir);
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction dir)
 +{
 +	BUG_ON(!valid_dma_direction(dir));
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 +	       enum dma_data_direction dir)
 +{
 +	dma_unmap_single(dev, dma_addr, size, dir);
 +}
 +
 +extern int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +		      enum dma_data_direction dir);
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg,
 +	     int nhwentries, enum dma_data_direction dir)
  {
 -	return &bfin_dma_ops;
 +	BUG_ON(!valid_dma_direction(dir));
  }
  
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction dir)
 +{
 +	BUG_ON(!valid_dma_direction(dir));
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction dir)
 +{
 +	_dma_sync(handle + offset, size, dir);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
 +			enum dma_data_direction dir)
 +{
 +	dma_sync_single_range_for_cpu(dev, handle, 0, size, dir);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
 +			   enum dma_data_direction dir)
 +{
 +	dma_sync_single_range_for_device(dev, handle, 0, size, dir);
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
 +		    enum dma_data_direction dir)
 +{
 +	BUG_ON(!valid_dma_direction(dir));
 +}
 +
 +extern void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 +		       int nents, enum dma_data_direction dir);
 +
 +static inline void
 +dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 +	       enum dma_data_direction dir)
 +{
 +	_dma_sync((dma_addr_t)vaddr, size, dir);
 +}
 +
 +/* drivers/base/dma-mapping.c */
 +extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +extern int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size);
 +
 +#define dma_mmap_coherent(d, v, c, h, s) dma_common_mmap(d, v, c, h, s)
 +#define dma_get_sgtable(d, t, v, h, s) dma_common_get_sgtable(d, t, v, h, s)
 +
  #endif				/* _BLACKFIN_DMA_MAPPING_H */
diff --cc arch/c6x/include/asm/dma-mapping.h
index 88bd0d899bdb,aca9f755e4f8..000000000000
--- a/arch/c6x/include/asm/dma-mapping.h
+++ b/arch/c6x/include/asm/dma-mapping.h
@@@ -30,78 -15,19 +30,86 @@@ static inline int dma_set_mask(struct d
  /*
   * DMA errors are defined by all-bits-set in the DMA address.
   */
++<<<<<<< HEAD
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
++=======
+ #define DMA_ERROR_CODE ~0
+ 
+ extern const struct dma_map_ops c6x_dma_ops;
+ 
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	debug_dma_mapping_error(dev, dma_addr);
 +	return dma_addr == ~0;
 +}
 +
 +extern dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 +				 size_t size, enum dma_data_direction dir);
 +
 +extern void dma_unmap_single(struct device *dev, dma_addr_t handle,
 +			     size_t size, enum dma_data_direction dir);
 +
 +extern int dma_map_sg(struct device *dev, struct scatterlist *sglist,
 +		      int nents, enum dma_data_direction direction);
 +
 +extern void dma_unmap_sg(struct device *dev, struct scatterlist *sglist,
 +			 int nents, enum dma_data_direction direction);
 +
 +static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 +				      unsigned long offset, size_t size,
 +				      enum dma_data_direction dir)
 +{
 +	dma_addr_t handle;
 +
 +	handle = dma_map_single(dev, page_address(page) + offset, size, dir);
 +
 +	debug_dma_map_page(dev, page, offset, size, dir, handle, false);
 +
 +	return handle;
 +}
 +
 +static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
 +		size_t size, enum dma_data_direction dir)
  {
 -	return &c6x_dma_ops;
 +	dma_unmap_single(dev, handle, size, dir);
 +
 +	debug_dma_unmap_page(dev, handle, size, dir, false);
  }
  
 +extern void dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,
 +				    size_t size, enum dma_data_direction dir);
 +
 +extern void dma_sync_single_for_device(struct device *dev, dma_addr_t handle,
 +				       size_t size,
 +				       enum dma_data_direction dir);
 +
 +extern void dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 +				int nents, enum dma_data_direction dir);
 +
 +extern void dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 +				   int nents, enum dma_data_direction dir);
 +
  extern void coherent_mem_init(u32 start, u32 size);
 -void *c6x_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 -		gfp_t gfp, unsigned long attrs);
 -void c6x_dma_free(struct device *dev, size_t size, void *vaddr,
 -		dma_addr_t dma_handle, unsigned long attrs);
 +extern void *dma_alloc_coherent(struct device *, size_t, dma_addr_t *, gfp_t);
 +extern void dma_free_coherent(struct device *, size_t, void *, dma_addr_t);
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent((d), (s), (h), (f))
 +#define dma_free_noncoherent(d, s, v, h)  dma_free_coherent((d), (s), (v), (h))
 +
 +/* Not supported for now */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
 +{
 +	return -EINVAL;
 +}
 +
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
 +{
 +	return -EINVAL;
 +}
  
  #endif	/* _ASM_C6X_DMA_MAPPING_H */
diff --cc arch/cris/include/asm/dma-mapping.h
index 2f0f654f1b44,256169de3743..000000000000
--- a/arch/cris/include/asm/dma-mapping.h
+++ b/arch/cris/include/asm/dma-mapping.h
@@@ -3,154 -1,20 +3,163 @@@
  #ifndef _ASM_CRIS_DMA_MAPPING_H
  #define _ASM_CRIS_DMA_MAPPING_H
  
 +#include <linux/mm.h>
 +#include <linux/kernel.h>
 +
++<<<<<<< HEAD
 +#include <asm/cache.h>
 +#include <asm/io.h>
 +#include <asm/scatterlist.h>
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
  #ifdef CONFIG_PCI
 -extern const struct dma_map_ops v32_dma_ops;
 +#include <asm-generic/dma-coherent.h>
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			   dma_addr_t *dma_handle, gfp_t flag);
  
 +void dma_free_coherent(struct device *dev, size_t size,
 +			 void *vaddr, dma_addr_t dma_handle);
 +#else
 +static inline void *
 +dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,
 +                   gfp_t flag)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
+ 	return &v32_dma_ops;
+ }
+ #else
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +        BUG();
 +        return NULL;
 +}
 +
 +static inline void
 +dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
 +                    dma_addr_t dma_handle)
  {
 -	BUG();
 -	return NULL;
 +        BUG();
  }
  #endif
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return virt_to_phys(ptr);
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	printk("Map sg\n");
 +	return nents;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return page_to_phys(page) + offset;
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +			enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle, size_t size,
 +			enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction direction)
 +{
 +}
 +
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int
 +dma_supported(struct device *dev, u64 mask)
 +{
 +        /*
 +         * we fall back to GFP_DMA when the mask isn't all 1s,
 +         * so we can't guarantee allocations that must be
 +         * within a tighter range than GFP_DMA..
 +         */
 +        if(mask < 0x00ffffff)
 +                return 0;
 +
 +	return 1;
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if(!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
  
  static inline void
  dma_cache_sync(struct device *dev, void *vaddr, size_t size,
diff --cc arch/frv/include/asm/dma-mapping.h
index 1746a2b8e6e7,354900917585..000000000000
--- a/arch/frv/include/asm/dma-mapping.h
+++ b/arch/frv/include/asm/dma-mapping.h
@@@ -18,111 -7,11 +18,115 @@@
  extern unsigned long __nongprelbss dma_coherent_mem_start;
  extern unsigned long __nongprelbss dma_coherent_mem_end;
  
 -extern const struct dma_map_ops frv_dma_ops;
 +void *dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle, gfp_t gfp);
 +void dma_free_coherent(struct device *dev, size_t size, void *vaddr, dma_addr_t dma_handle);
  
++<<<<<<< HEAD
 +extern dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
 +				 enum dma_data_direction direction);
 +
 +static inline
 +void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		      enum dma_data_direction direction)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +extern int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +		      enum dma_data_direction direction);
 +
 +static inline
 +void dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +extern
 +dma_addr_t dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +			size_t size, enum dma_data_direction direction);
 +
 +static inline
 +void dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +		    enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +
 +static inline
 +void dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +			     enum dma_data_direction direction)
 +{
 +}
 +
 +static inline
 +void dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle, size_t size,
 +				enum dma_data_direction direction)
 +{
 +	flush_write_buffers();
 +}
 +
 +static inline
 +void dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +				   unsigned long offset, size_t size,
 +				   enum dma_data_direction direction)
 +{
 +}
 +
 +static inline
 +void dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				      unsigned long offset, size_t size,
 +				      enum dma_data_direction direction)
 +{
 +	flush_write_buffers();
 +}
 +
 +static inline
 +void dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +			 enum dma_data_direction direction)
 +{
 +}
 +
 +static inline
 +void dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +			    enum dma_data_direction direction)
 +{
 +	flush_write_buffers();
 +}
 +
 +static inline
 +int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline
 +int dma_supported(struct device *dev, u64 mask)
 +{
 +        /*
 +         * we fall back to GFP_DMA when the mask isn't all 1s,
 +         * so we can't guarantee allocations that must be
 +         * within a tighter range than GFP_DMA..
 +         */
 +        if (mask < 0x00ffffff)
 +                return 0;
 +
 +	return 1;
 +}
 +
 +static inline
 +int dma_set_mask(struct device *dev, u64 mask)
  {
 -	return &frv_dma_ops;
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
  }
  
  static inline
diff --cc arch/hexagon/include/asm/dma-mapping.h
index 85e9935660cb,d3a87bd9b686..000000000000
--- a/arch/hexagon/include/asm/dma-mapping.h
+++ b/arch/hexagon/include/asm/dma-mapping.h
@@@ -32,17 -30,12 +32,18 @@@
  
  struct device;
  extern int bad_dma_address;
 -#define DMA_ERROR_CODE bad_dma_address
  
 -extern const struct dma_map_ops *dma_ops;
 +extern struct dma_map_ops *dma_ops;
  
++<<<<<<< HEAD
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
- 	if (unlikely(dev == NULL))
- 		return NULL;
- 
  	return dma_ops;
  }
  
diff --cc arch/ia64/include/asm/dma-mapping.h
index cf3ab7e784b5,73ec3c6f4cfe..000000000000
--- a/arch/ia64/include/asm/dma-mapping.h
+++ b/arch/ia64/include/asm/dma-mapping.h
@@@ -23,59 -23,10 +23,66 @@@ extern void machvec_dma_sync_single(str
  extern void machvec_dma_sync_sg(struct device *, struct scatterlist *, int,
  				enum dma_data_direction);
  
++<<<<<<< HEAD
 +#define dma_alloc_coherent(d,s,h,f)	dma_alloc_attrs(d,s,h,f,NULL)
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				    dma_addr_t *daddr, gfp_t gfp,
 +				    struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	void *caddr;
 +
 +	caddr = ops->alloc(dev, size, daddr, gfp, attrs);
 +	debug_dma_alloc_coherent(dev, size, *daddr, caddr);
 +	return caddr;
 +}
 +
 +#define dma_free_coherent(d,s,c,h) dma_free_attrs(d,s,c,h,NULL)
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				  void *caddr, dma_addr_t daddr,
 +				  struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	debug_dma_free_coherent(dev, size, caddr, daddr);
 +	ops->free(dev, size, caddr, daddr, attrs);
 +}
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +#define get_dma_ops(dev) platform_dma_get_ops(dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
+ 	return platform_dma_get_ops(NULL);
+ }
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t daddr)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	debug_dma_mapping_error(dev, daddr);
 +	return ops->mapping_error(dev, daddr);
 +}
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	return ops->dma_supported(dev, mask);
 +}
 +
 +static inline int
 +dma_set_mask (struct device *dev, u64 mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +	*dev->dma_mask = mask;
 +	return 0;
 +}
  
  static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
  {
diff --cc arch/m68k/include/asm/dma-mapping.h
index 05aa53594d49,9210e470771b..000000000000
--- a/arch/m68k/include/asm/dma-mapping.h
+++ b/arch/m68k/include/asm/dma-mapping.h
@@@ -1,51 -1,13 +1,55 @@@
  #ifndef _M68K_DMA_MAPPING_H
  #define _M68K_DMA_MAPPING_H
  
 -extern const struct dma_map_ops m68k_dma_ops;
 +#include <asm/cache.h>
  
++<<<<<<< HEAD
 +struct scatterlist;
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	return 1;
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	return 0;
 +}
 +
 +extern void *dma_alloc_coherent(struct device *, size_t,
 +				dma_addr_t *, gfp_t);
 +extern void dma_free_coherent(struct device *, size_t,
 +			      void *, dma_addr_t);
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				    dma_addr_t *dma_handle, gfp_t flag,
 +				    struct dma_attrs *attrs)
 +{
 +	/* attrs is not supported and ignored */
 +	return dma_alloc_coherent(dev, size, dma_handle, flag);
 +}
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				  void *cpu_addr, dma_addr_t dma_handle,
 +				  struct dma_attrs *attrs)
  {
 -        return &m68k_dma_ops;
 +	/* attrs is not supported and ignored */
 +	dma_free_coherent(dev, size, cpu_addr, dma_handle);
  }
  
 +static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
 +					  dma_addr_t *handle, gfp_t flag)
 +{
 +	return dma_alloc_coherent(dev, size, handle, flag);
 +}
 +static inline void dma_free_noncoherent(struct device *dev, size_t size,
 +					void *addr, dma_addr_t handle)
 +{
 +	dma_free_coherent(dev, size, addr, handle);
 +}
  static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
  				  enum dma_data_direction dir)
  {
diff --cc arch/metag/include/asm/dma-mapping.h
index 14b23efd9b7a,fad3dc3cb210..000000000000
--- a/arch/metag/include/asm/dma-mapping.h
+++ b/arch/metag/include/asm/dma-mapping.h
@@@ -1,173 -1,11 +1,177 @@@
  #ifndef _ASM_METAG_DMA_MAPPING_H
  #define _ASM_METAG_DMA_MAPPING_H
  
 -extern const struct dma_map_ops metag_dma_ops;
 +#include <linux/mm.h>
  
++<<<<<<< HEAD
 +#include <asm/cache.h>
 +#include <asm/io.h>
 +#include <linux/scatterlist.h>
 +#include <asm/bug.h>
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			 dma_addr_t *dma_handle, gfp_t flag);
 +
 +void dma_free_coherent(struct device *dev, size_t size,
 +		       void *vaddr, dma_addr_t dma_handle);
 +
 +void dma_sync_for_device(void *vaddr, size_t size, int dma_direction);
 +void dma_sync_for_cpu(void *vaddr, size_t size, int dma_direction);
 +
 +int dma_mmap_coherent(struct device *dev, struct vm_area_struct *vma,
 +		      void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +
 +int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
 +			  void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	WARN_ON(size == 0);
 +	dma_sync_for_device(ptr, size, direction);
 +	return virt_to_phys(ptr);
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	dma_sync_for_cpu(phys_to_virt(dma_addr), size, direction);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sglist, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	BUG_ON(!valid_dma_direction(direction));
 +	WARN_ON(nents == 0 || sglist[0].length == 0);
 +
 +	for_each_sg(sglist, sg, nents, i) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +		dma_sync_for_device(sg_virt(sg), sg->length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	dma_sync_for_device((void *)(page_to_phys(page) + offset), size,
 +			    direction);
 +	return page_to_phys(page) + offset;
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	dma_sync_for_cpu(phys_to_virt(dma_address), size, direction);
 +}
 +
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sglist, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	BUG_ON(!valid_dma_direction(direction));
 +	WARN_ON(nhwentries == 0 || sglist[0].length == 0);
 +
 +	for_each_sg(sglist, sg, nhwentries, i) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +		dma_sync_for_cpu(sg_virt(sg), sg->length, direction);
 +	}
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +			enum dma_data_direction direction)
 +{
 +	dma_sync_for_cpu(phys_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +			   size_t size, enum dma_data_direction direction)
 +{
 +	dma_sync_for_device(phys_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
  {
 -	return &metag_dma_ops;
 +	dma_sync_for_cpu(phys_to_virt(dma_handle)+offset, size,
 +			 direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +	dma_sync_for_device(phys_to_virt(dma_handle)+offset, size,
 +			    direction);
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction direction)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		dma_sync_for_cpu(sg_virt(sg), sg->length, direction);
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		       enum dma_data_direction direction)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		dma_sync_for_device(sg_virt(sg), sg->length, direction);
 +}
 +
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +#define dma_supported(dev, mask)        (1)
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
  }
  
  /*
diff --cc arch/microblaze/include/asm/dma-mapping.h
index 46460f1c49c4,3fad5e722a66..000000000000
--- a/arch/microblaze/include/asm/dma-mapping.h
+++ b/arch/microblaze/include/asm/dma-mapping.h
@@@ -48,53 -36,13 +48,57 @@@ extern struct dma_map_ops *dma_ops
  /*
   * Available generic sets of operations
   */
 -extern const struct dma_map_ops dma_direct_ops;
 +extern struct dma_map_ops dma_direct_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	/* We don't handle the NULL dev case for ISA for now. We could
 +	 * do it via an out of line call but it is not needed for now. The
 +	 * only ISA DMA device we support is the floppy and we have a hack
 +	 * in the floppy driver directly to get a device for us.
 +	 */
 +	if (unlikely(!dev) || !dev->archdata.dma_ops)
 +		return NULL;
 +
 +	return dev->archdata.dma_ops;
 +}
 +
 +static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 +{
 +	dev->archdata.dma_ops = ops;
 +}
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
  {
 -	return &dma_direct_ops;
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	if (unlikely(!ops))
 +		return 0;
 +	if (!ops->dma_supported)
 +		return 1;
 +	return ops->dma_supported(dev, mask);
  }
  
 +static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	if (unlikely(ops == NULL))
 +		return -EIO;
 +	if (ops->set_dma_mask)
 +		return ops->set_dma_mask(dev, dma_mask);
 +	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 +		return -EIO;
 +	*dev->dma_mask = dma_mask;
 +	return 0;
 +}
 +
 +#include <asm-generic/dma-mapping-common.h>
 +
  static inline void __dma_sync(unsigned long paddr,
  			      size_t size, enum dma_data_direction direction)
  {
diff --cc arch/mips/include/asm/dma-mapping.h
index 84238c574d5e,aba71385f9d1..000000000000
--- a/arch/mips/include/asm/dma-mapping.h
+++ b/arch/mips/include/asm/dma-mapping.h
@@@ -10,14 -9,11 +10,20 @@@
  #include <dma-coherence.h>
  #endif
  
 -extern const struct dma_map_ops *mips_dma_map_ops;
 +extern struct dma_map_ops *mips_dma_map_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +{
 +	if (dev && dev->archdata.dma_ops)
 +		return dev->archdata.dma_ops;
 +	else
 +		return mips_dma_map_ops;
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
+ 	return mips_dma_map_ops;
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  }
  
  static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
diff --cc arch/mn10300/include/asm/dma-mapping.h
index a18abfc558eb,737ef574b3ea..000000000000
--- a/arch/mn10300/include/asm/dma-mapping.h
+++ b/arch/mn10300/include/asm/dma-mapping.h
@@@ -17,148 -14,11 +17,152 @@@
  #include <asm/cache.h>
  #include <asm/io.h>
  
 -extern const struct dma_map_ops mn10300_dma_ops;
 +/*
 + * See Documentation/DMA-API.txt for the description of how the
 + * following DMA API should work.
 + */
 +
++<<<<<<< HEAD
 +extern void *dma_alloc_coherent(struct device *dev, size_t size,
 +				dma_addr_t *dma_handle, int flag);
 +
 +extern void dma_free_coherent(struct device *dev, size_t size,
 +			      void *vaddr, dma_addr_t dma_handle);
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent((d), (s), (h), (f))
 +#define dma_free_noncoherent(d, s, v, h)  dma_free_coherent((d), (s), (v), (h))
  
 +static inline
 +dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
 +			  enum dma_data_direction direction)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	mn10300_dcache_flush_inv();
 +	return virt_to_bus(ptr);
 +}
 +
 +static inline
 +void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline
 +int dma_map_sg(struct device *dev, struct scatterlist *sglist, int nents,
 +	       enum dma_data_direction direction)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	BUG_ON(!valid_dma_direction(direction));
 +	WARN_ON(nents == 0 || sglist[0].length == 0);
 +
 +	for_each_sg(sglist, sg, nents, i) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +	}
 +
 +	mn10300_dcache_flush_inv();
 +	return nents;
 +}
 +
 +static inline
 +void dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +		  enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +}
 +
 +static inline
 +dma_addr_t dma_map_page(struct device *dev, struct page *page,
 +			unsigned long offset, size_t size,
 +			enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return page_to_bus(page) + offset;
 +}
 +
 +static inline
 +void dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +		    enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline
 +void dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			     size_t size, enum dma_data_direction direction)
  {
 -	return &mn10300_dma_ops;
 +}
 +
 +static inline
 +void dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +				size_t size, enum dma_data_direction direction)
 +{
 +	mn10300_dcache_flush_inv();
 +}
 +
 +static inline
 +void dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +				   unsigned long offset, size_t size,
 +				   enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +	mn10300_dcache_flush_inv();
 +}
 +
 +
 +static inline
 +void dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 +			 int nelems, enum dma_data_direction direction)
 +{
 +}
 +
 +static inline
 +void dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 +			    int nelems, enum dma_data_direction direction)
 +{
 +	mn10300_dcache_flush_inv();
 +}
 +
 +static inline
 +int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline
 +int dma_supported(struct device *dev, u64 mask)
 +{
 +	/*
 +	 * we fall back to GFP_DMA when the mask isn't all 1s, so we can't
 +	 * guarantee allocations that must be within a tighter range than
 +	 * GFP_DMA
 +	 */
 +	if (mask < 0x00ffffff)
 +		return 0;
 +	return 1;
 +}
 +
 +static inline
 +int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +	return 0;
  }
  
  static inline
diff --cc arch/openrisc/include/asm/dma-mapping.h
index fab8628e1b6e,0c0075f17145..000000000000
--- a/arch/openrisc/include/asm/dma-mapping.h
+++ b/arch/openrisc/include/asm/dma-mapping.h
@@@ -29,9 -28,9 +29,13 @@@
  
  #define DMA_ERROR_CODE		(~(dma_addr_t)0x0)
  
 -extern const struct dma_map_ops or1k_dma_map_ops;
 +extern struct dma_map_ops or1k_dma_map_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
  	return &or1k_dma_map_ops;
  }
diff --cc arch/parisc/include/asm/dma-mapping.h
index d0eae5f2bd87,5404c6a726b2..000000000000
--- a/arch/parisc/include/asm/dma-mapping.h
+++ b/arch/parisc/include/asm/dma-mapping.h
@@@ -40,151 -21,15 +40,155 @@@ struct hppa_dma_ops 
  */
  
  #ifdef CONFIG_PA11
 -extern const struct dma_map_ops pcxl_dma_ops;
 -extern const struct dma_map_ops pcx_dma_ops;
 +extern struct hppa_dma_ops pcxl_dma_ops;
 +extern struct hppa_dma_ops pcx_dma_ops;
  #endif
  
 -extern const struct dma_map_ops *hppa_dma_ops;
 +extern struct hppa_dma_ops *hppa_dma_ops;
 +
++<<<<<<< HEAD
 +#define dma_alloc_attrs(d, s, h, f, a) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_attrs(d, s, h, f, a) dma_free_coherent(d, s, h, f)
  
 +static inline void *
 +dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,
 +		   gfp_t flag)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +{
 +	return hppa_dma_ops->alloc_consistent(dev, size, dma_handle, flag);
 +}
 +
 +static inline void *
 +dma_alloc_noncoherent(struct device *dev, size_t size, dma_addr_t *dma_handle,
 +		      gfp_t flag)
 +{
 +	return hppa_dma_ops->alloc_noncoherent(dev, size, dma_handle, flag);
 +}
 +
 +static inline void
 +dma_free_coherent(struct device *dev, size_t size, 
 +		    void *vaddr, dma_addr_t dma_handle)
 +{
 +	hppa_dma_ops->free_consistent(dev, size, vaddr, dma_handle);
 +}
 +
 +static inline void
 +dma_free_noncoherent(struct device *dev, size_t size, 
 +		    void *vaddr, dma_addr_t dma_handle)
 +{
 +	hppa_dma_ops->free_consistent(dev, size, vaddr, dma_handle);
 +}
 +
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
  {
 -	return hppa_dma_ops;
 +	return hppa_dma_ops->map_single(dev, ptr, size, direction);
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +	hppa_dma_ops->unmap_single(dev, dma_addr, size, direction);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	return hppa_dma_ops->map_sg(dev, sg, nents, direction);
 +}
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	hppa_dma_ops->unmap_sg(dev, sg, nhwentries, direction);
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	return dma_map_single(dev, (page_address(page) + (offset)), size, direction);
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	dma_unmap_single(dev, dma_address, size, direction);
 +}
 +
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +		enum dma_data_direction direction)
 +{
 +	if(hppa_dma_ops->dma_sync_single_for_cpu)
 +		hppa_dma_ops->dma_sync_single_for_cpu(dev, dma_handle, 0, size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle, size_t size,
 +		enum dma_data_direction direction)
 +{
 +	if(hppa_dma_ops->dma_sync_single_for_device)
 +		hppa_dma_ops->dma_sync_single_for_device(dev, dma_handle, 0, size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +	if(hppa_dma_ops->dma_sync_single_for_cpu)
 +		hppa_dma_ops->dma_sync_single_for_cpu(dev, dma_handle, offset, size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +	if(hppa_dma_ops->dma_sync_single_for_device)
 +		hppa_dma_ops->dma_sync_single_for_device(dev, dma_handle, offset, size, direction);
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction direction)
 +{
 +	if(hppa_dma_ops->dma_sync_sg_for_cpu)
 +		hppa_dma_ops->dma_sync_sg_for_cpu(dev, sg, nelems, direction);
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction direction)
 +{
 +	if(hppa_dma_ops->dma_sync_sg_for_device)
 +		hppa_dma_ops->dma_sync_sg_for_device(dev, sg, nelems, direction);
 +}
 +
 +static inline int
 +dma_supported(struct device *dev, u64 mask)
 +{
 +	return hppa_dma_ops->dma_supported(dev, mask);
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if(!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
  }
  
  static inline void
diff --cc arch/powerpc/include/asm/dma-mapping.h
index c6f39b7c9f40,181a095468e4..000000000000
--- a/arch/powerpc/include/asm/dma-mapping.h
+++ b/arch/powerpc/include/asm/dma-mapping.h
@@@ -75,24 -76,16 +75,32 @@@ static inline unsigned long device_to_m
  #ifdef CONFIG_PPC64
  extern struct dma_map_ops dma_iommu_ops;
  #endif
 -extern const struct dma_map_ops dma_direct_ops;
 +extern struct dma_map_ops dma_direct_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
  	/* We don't handle the NULL dev case for ISA for now. We could
  	 * do it via an out of line call but it is not needed for now. The
  	 * only ISA DMA device we support is the floppy and we have a hack
  	 * in the floppy driver directly to get a device for us.
  	 */
++<<<<<<< HEAD
 +	if (unlikely(dev == NULL))
 +		return NULL;
 +
 +	return dev->archdata.dma_ops;
 +}
 +
 +static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 +{
 +	dev->archdata.dma_ops = ops;
++=======
+ 	return NULL;
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  }
  
  /*
diff --cc arch/s390/include/asm/dma-mapping.h
index 3f2d5669375a,3108b8dbe266..000000000000
--- a/arch/s390/include/asm/dma-mapping.h
+++ b/arch/s390/include/asm/dma-mapping.h
@@@ -11,12 -10,10 +11,17 @@@
  
  #define DMA_ERROR_CODE		(~(dma_addr_t) 0x0)
  
 -extern const struct dma_map_ops s390_pci_dma_ops;
 +extern struct dma_map_ops s390_pci_dma_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +{
 +	if (dev && dev->device_rh->dma_ops)
 +		return dev->device_rh->dma_ops;
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  	return &dma_noop_ops;
  }
  
diff --cc arch/sh/include/asm/dma-mapping.h
index b437f2c780b8,d99008af5f73..000000000000
--- a/arch/sh/include/asm/dma-mapping.h
+++ b/arch/sh/include/asm/dma-mapping.h
@@@ -1,10 -1,10 +1,14 @@@
  #ifndef __ASM_SH_DMA_MAPPING_H
  #define __ASM_SH_DMA_MAPPING_H
  
 -extern const struct dma_map_ops *dma_ops;
 +extern struct dma_map_ops *dma_ops;
  extern void no_iommu_init(void);
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
  	return dma_ops;
  }
diff --cc arch/sparc/include/asm/dma-mapping.h
index 05fe53f5346e,69cc627779f2..000000000000
--- a/arch/sparc/include/asm/dma-mapping.h
+++ b/arch/sparc/include/asm/dma-mapping.h
@@@ -18,12 -24,14 +18,22 @@@ extern struct dma_map_ops pci32_dma_ops
  
  extern struct bus_type pci_bus_type;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
 -#ifdef CONFIG_SPARC_LEON
 +#if defined(CONFIG_SPARC32) && defined(CONFIG_PCI)
  	if (sparc_cpu_model == sparc_leon)
  		return leon_dma_ops;
++<<<<<<< HEAD
 +	else if (dev->bus == &pci_bus_type)
++=======
+ #endif
+ #if defined(CONFIG_SPARC32) && defined(CONFIG_PCI)
+ 	if (bus == &pci_bus_type)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  		return &pci32_dma_ops;
  #endif
  	return dma_ops;
diff --cc arch/tile/include/asm/dma-mapping.h
index f2ff191376b4,bbc71a29b2c6..000000000000
--- a/arch/tile/include/asm/dma-mapping.h
+++ b/arch/tile/include/asm/dma-mapping.h
@@@ -20,16 -20,18 +20,27 @@@
  #include <linux/cache.h>
  #include <linux/io.h>
  
 -#ifdef __tilegx__
 -#define ARCH_HAS_DMA_GET_REQUIRED_MASK
 -#endif
 +extern struct dma_map_ops *tile_dma_map_ops;
 +extern struct dma_map_ops *gx_pci_dma_map_ops;
 +extern struct dma_map_ops *gx_legacy_pci_dma_map_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +{
 +	if (dev && dev->archdata.dma_ops)
 +		return dev->archdata.dma_ops;
 +	else
 +		return tile_dma_map_ops;
++=======
+ extern const struct dma_map_ops *tile_dma_map_ops;
+ extern const struct dma_map_ops *gx_pci_dma_map_ops;
+ extern const struct dma_map_ops *gx_legacy_pci_dma_map_ops;
+ extern const struct dma_map_ops *gx_hybrid_pci_dma_map_ops;
+ 
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
+ 	return tile_dma_map_ops;
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  }
  
  static inline dma_addr_t get_dma_offset(struct device *dev)
diff --cc arch/unicore32/include/asm/dma-mapping.h
index 366460a81796,518ba5848dd6..000000000000
--- a/arch/unicore32/include/asm/dma-mapping.h
+++ b/arch/unicore32/include/asm/dma-mapping.h
@@@ -23,9 -21,9 +23,13 @@@
  #include <asm/memory.h>
  #include <asm/cacheflush.h>
  
 -extern const struct dma_map_ops swiotlb_dma_map_ops;
 +extern struct dma_map_ops swiotlb_dma_map_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
  	return &swiotlb_dma_map_ops;
  }
diff --cc arch/x86/include/asm/dma-mapping.h
index 1f5b7287d1ad,08a0838b83fb..000000000000
--- a/arch/x86/include/asm/dma-mapping.h
+++ b/arch/x86/include/asm/dma-mapping.h
@@@ -27,38 -25,18 +27,44 @@@ extern int iommu_merge
  extern struct device x86_dma_fallback_dev;
  extern int panic_on_overflow;
  
 -extern const struct dma_map_ops *dma_ops;
 +extern struct dma_map_ops *dma_ops;
  
++<<<<<<< HEAD
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  {
- #ifndef CONFIG_X86_DEV_DMA_OPS
  	return dma_ops;
++<<<<<<< HEAD
 +#else
 +	if (unlikely(!dev) || !dev->archdata.dma_ops)
 +		return dma_ops;
 +	else
 +		return dev->archdata.dma_ops;
 +#endif
++=======
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  }
  
 -bool arch_dma_alloc_attrs(struct device **dev, gfp_t *gfp);
 -#define arch_dma_alloc_attrs arch_dma_alloc_attrs
 +#include <asm-generic/dma-mapping-common.h>
 +
 +/* Make sure we keep the same behaviour */
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	debug_dma_mapping_error(dev, dma_addr);
 +	if (ops->mapping_error)
 +		return ops->mapping_error(dev, dma_addr);
 +
 +	return (dma_addr == DMA_ERROR_CODE);
 +}
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
  
 -#define HAVE_ARCH_DMA_SUPPORTED 1
  extern int dma_supported(struct device *hwdev, u64 mask);
 +extern int dma_set_mask(struct device *dev, u64 mask);
  
  extern void *dma_generic_alloc_coherent(struct device *dev, size_t size,
  					dma_addr_t *dma_addr, gfp_t flag,
diff --cc arch/xtensa/include/asm/dma-mapping.h
index 172a02a6ad14,c6140fa8c0be..000000000000
--- a/arch/xtensa/include/asm/dma-mapping.h
+++ b/arch/xtensa/include/asm/dma-mapping.h
@@@ -18,171 -18,24 +18,177 @@@
  
  #define DMA_ERROR_CODE		(~(dma_addr_t)0x0)
  
 -extern const struct dma_map_ops xtensa_dma_map_ops;
 +/*
 + * DMA-consistent mapping functions.
 + */
 +
++<<<<<<< HEAD
 +extern void *consistent_alloc(int, size_t, dma_addr_t, unsigned long);
 +extern void consistent_free(void*, size_t, dma_addr_t);
 +extern void consistent_sync(void*, size_t, int);
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			   dma_addr_t *dma_handle, gfp_t flag);
 +
 +void dma_free_coherent(struct device *dev, size_t size,
 +			 void *vaddr, dma_addr_t dma_handle);
  
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	consistent_sync(ptr, size, direction);
 +	return virt_to_phys(ptr);
++=======
+ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
+ {
+ 	return &xtensa_dma_map_ops;
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	BUG_ON(direction == DMA_NONE);
 +
 +	for (i = 0; i < nents; i++, sg++ ) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +		consistent_sync(sg_virt(sg), sg->length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return (dma_addr_t)(page_to_pfn(page)) * PAGE_SIZE + offset;
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +		enum dma_data_direction direction)
 +{
 +	consistent_sync((void *)bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +		           size_t size, enum dma_data_direction direction)
 +{
 +	consistent_sync((void *)bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +
 +	consistent_sync((void *)bus_to_virt(dma_handle)+offset,size,direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +
 +	consistent_sync((void *)bus_to_virt(dma_handle)+offset,size,direction);
 +}
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction dir)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		consistent_sync(sg_virt(sg), sg->length, dir);
  }
  
 -void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 -		    enum dma_data_direction direction);
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction dir)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		consistent_sync(sg_virt(sg), sg->length, dir);
 +}
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int
 +dma_supported(struct device *dev, u64 mask)
 +{
 +	return 1;
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if(!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
 +
 +static inline void
 +dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	consistent_sync(vaddr, size, direction);
 +}
  
 -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 +/* Not supported for now */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
  {
 -	return (dma_addr_t)paddr;
 +	return -EINVAL;
  }
  
 -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
  {
 -	return (phys_addr_t)daddr;
 +	return -EINVAL;
  }
  
  #endif	/* _XTENSA_DMA_MAPPING_H */
diff --cc include/linux/dma-mapping.h
index fe4bb5f6dd29,ab8710888ddf..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -85,10 -145,433 +85,25 @@@ static inline int is_device_dma_capable
  	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
  }
  
 -#ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
 -/*
 - * These three functions are only for dma allocator.
 - * Don't use them in device drivers.
 - */
 -int dma_alloc_from_coherent(struct device *dev, ssize_t size,
 -				       dma_addr_t *dma_handle, void **ret);
 -int dma_release_from_coherent(struct device *dev, int order, void *vaddr);
 -
 -int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
 -			    void *cpu_addr, size_t size, int *ret);
 -#else
 -#define dma_alloc_from_coherent(dev, size, handle, ret) (0)
 -#define dma_release_from_coherent(dev, order, vaddr) (0)
 -#define dma_mmap_from_coherent(dev, vma, vaddr, order, ret) (0)
 -#endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 -
  #ifdef CONFIG_HAS_DMA
  #include <asm/dma-mapping.h>
++<<<<<<< HEAD
++=======
+ static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
+ {
+ 	if (dev && dev->dma_ops)
+ 		return dev->dma_ops;
+ 	return get_arch_dma_ops(dev ? dev->bus : NULL);
+ }
+ 
+ static inline void set_dma_ops(struct device *dev,
+ 			       const struct dma_map_ops *dma_ops)
+ {
+ 	dev->dma_ops = dma_ops;
+ }
++>>>>>>> 815dd18788fe (treewide: Consolidate get_dma_ops() implementations)
  #else
 -/*
 - * Define the dma api to allow compilation but not linking of
 - * dma dependent code.  Code that depends on the dma-mapping
 - * API needs to set 'depends on HAS_DMA' in its Kconfig
 - */
 -extern const struct dma_map_ops bad_dma_ops;
 -static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 -{
 -	return &bad_dma_ops;
 -}
 -#endif
 -
 -static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 -					      size_t size,
 -					      enum dma_data_direction dir,
 -					      unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -	dma_addr_t addr;
 -
 -	kmemcheck_mark_initialized(ptr, size);
 -	BUG_ON(!valid_dma_direction(dir));
 -	addr = ops->map_page(dev, virt_to_page(ptr),
 -			     offset_in_page(ptr), size,
 -			     dir, attrs);
 -	debug_dma_map_page(dev, virt_to_page(ptr),
 -			   offset_in_page(ptr), size,
 -			   dir, addr, true);
 -	return addr;
 -}
 -
 -static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 -					  size_t size,
 -					  enum dma_data_direction dir,
 -					  unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->unmap_page)
 -		ops->unmap_page(dev, addr, size, dir, attrs);
 -	debug_dma_unmap_page(dev, addr, size, dir, true);
 -}
 -
 -/*
 - * dma_maps_sg_attrs returns 0 on error and > 0 on success.
 - * It should never return a value < 0.
 - */
 -static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 -				   int nents, enum dma_data_direction dir,
 -				   unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -	int i, ents;
 -	struct scatterlist *s;
 -
 -	for_each_sg(sg, s, nents, i)
 -		kmemcheck_mark_initialized(sg_virt(s), s->length);
 -	BUG_ON(!valid_dma_direction(dir));
 -	ents = ops->map_sg(dev, sg, nents, dir, attrs);
 -	BUG_ON(ents < 0);
 -	debug_dma_map_sg(dev, sg, nents, ents, dir);
 -
 -	return ents;
 -}
 -
 -static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,
 -				      int nents, enum dma_data_direction dir,
 -				      unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	debug_dma_unmap_sg(dev, sg, nents, dir);
 -	if (ops->unmap_sg)
 -		ops->unmap_sg(dev, sg, nents, dir, attrs);
 -}
 -
 -static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 -					    struct page *page,
 -					    size_t offset, size_t size,
 -					    enum dma_data_direction dir,
 -					    unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -	dma_addr_t addr;
 -
 -	kmemcheck_mark_initialized(page_address(page) + offset, size);
 -	BUG_ON(!valid_dma_direction(dir));
 -	addr = ops->map_page(dev, page, offset, size, dir, attrs);
 -	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
 -
 -	return addr;
 -}
 -
 -static inline void dma_unmap_page_attrs(struct device *dev,
 -					dma_addr_t addr, size_t size,
 -					enum dma_data_direction dir,
 -					unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->unmap_page)
 -		ops->unmap_page(dev, addr, size, dir, attrs);
 -	debug_dma_unmap_page(dev, addr, size, dir, false);
 -}
 -
 -static inline dma_addr_t dma_map_resource(struct device *dev,
 -					  phys_addr_t phys_addr,
 -					  size_t size,
 -					  enum dma_data_direction dir,
 -					  unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -	dma_addr_t addr;
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -
 -	/* Don't allow RAM to be mapped */
 -	BUG_ON(pfn_valid(PHYS_PFN(phys_addr)));
 -
 -	addr = phys_addr;
 -	if (ops->map_resource)
 -		addr = ops->map_resource(dev, phys_addr, size, dir, attrs);
 -
 -	debug_dma_map_resource(dev, phys_addr, size, dir, addr);
 -
 -	return addr;
 -}
 -
 -static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
 -				      size_t size, enum dma_data_direction dir,
 -				      unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->unmap_resource)
 -		ops->unmap_resource(dev, addr, size, dir, attrs);
 -	debug_dma_unmap_resource(dev, addr, size, dir);
 -}
 -
 -static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
 -					   size_t size,
 -					   enum dma_data_direction dir)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->sync_single_for_cpu)
 -		ops->sync_single_for_cpu(dev, addr, size, dir);
 -	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
 -}
 -
 -static inline void dma_sync_single_for_device(struct device *dev,
 -					      dma_addr_t addr, size_t size,
 -					      enum dma_data_direction dir)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->sync_single_for_device)
 -		ops->sync_single_for_device(dev, addr, size, dir);
 -	debug_dma_sync_single_for_device(dev, addr, size, dir);
 -}
 -
 -static inline void dma_sync_single_range_for_cpu(struct device *dev,
 -						 dma_addr_t addr,
 -						 unsigned long offset,
 -						 size_t size,
 -						 enum dma_data_direction dir)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->sync_single_for_cpu)
 -		ops->sync_single_for_cpu(dev, addr + offset, size, dir);
 -	debug_dma_sync_single_range_for_cpu(dev, addr, offset, size, dir);
 -}
 -
 -static inline void dma_sync_single_range_for_device(struct device *dev,
 -						    dma_addr_t addr,
 -						    unsigned long offset,
 -						    size_t size,
 -						    enum dma_data_direction dir)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->sync_single_for_device)
 -		ops->sync_single_for_device(dev, addr + offset, size, dir);
 -	debug_dma_sync_single_range_for_device(dev, addr, offset, size, dir);
 -}
 -
 -static inline void
 -dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 -		    int nelems, enum dma_data_direction dir)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->sync_sg_for_cpu)
 -		ops->sync_sg_for_cpu(dev, sg, nelems, dir);
 -	debug_dma_sync_sg_for_cpu(dev, sg, nelems, dir);
 -}
 -
 -static inline void
 -dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 -		       int nelems, enum dma_data_direction dir)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!valid_dma_direction(dir));
 -	if (ops->sync_sg_for_device)
 -		ops->sync_sg_for_device(dev, sg, nelems, dir);
 -	debug_dma_sync_sg_for_device(dev, sg, nelems, dir);
 -
 -}
 -
 -#define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
 -#define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
 -#define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
 -#define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, 0)
 -#define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
 -#define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
 -
 -extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 -			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 -
 -void *dma_common_contiguous_remap(struct page *page, size_t size,
 -			unsigned long vm_flags,
 -			pgprot_t prot, const void *caller);
 -
 -void *dma_common_pages_remap(struct page **pages, size_t size,
 -			unsigned long vm_flags, pgprot_t prot,
 -			const void *caller);
 -void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
 -
 -/**
 - * dma_mmap_attrs - map a coherent DMA allocation into user space
 - * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 - * @vma: vm_area_struct describing requested user mapping
 - * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
 - * @handle: device-view address returned from dma_alloc_attrs
 - * @size: size of memory originally requested in dma_alloc_attrs
 - * @attrs: attributes of mapping properties requested in dma_alloc_attrs
 - *
 - * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
 - * into user space.  The coherent DMA buffer must not be freed by the
 - * driver until the user space mapping has been released.
 - */
 -static inline int
 -dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
 -	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -	BUG_ON(!ops);
 -	if (ops->mmap)
 -		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 -	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);
 -}
 -
 -#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
 -
 -int
 -dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 -		       void *cpu_addr, dma_addr_t dma_addr, size_t size);
 -
 -static inline int
 -dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 -		      dma_addr_t dma_addr, size_t size,
 -		      unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -	BUG_ON(!ops);
 -	if (ops->get_sgtable)
 -		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
 -					attrs);
 -	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size);
 -}
 -
 -#define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
 -
 -#ifndef arch_dma_alloc_attrs
 -#define arch_dma_alloc_attrs(dev, flag)	(true)
 -#endif
 -
 -static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 -				       dma_addr_t *dma_handle, gfp_t flag,
 -				       unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -	void *cpu_addr;
 -
 -	BUG_ON(!ops);
 -
 -	if (dma_alloc_from_coherent(dev, size, dma_handle, &cpu_addr))
 -		return cpu_addr;
 -
 -	if (!arch_dma_alloc_attrs(&dev, &flag))
 -		return NULL;
 -	if (!ops->alloc)
 -		return NULL;
 -
 -	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
 -	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
 -	return cpu_addr;
 -}
 -
 -static inline void dma_free_attrs(struct device *dev, size_t size,
 -				     void *cpu_addr, dma_addr_t dma_handle,
 -				     unsigned long attrs)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	BUG_ON(!ops);
 -	WARN_ON(irqs_disabled());
 -
 -	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))
 -		return;
 -
 -	if (!ops->free || !cpu_addr)
 -		return;
 -
 -	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
 -	ops->free(dev, size, cpu_addr, dma_handle, attrs);
 -}
 -
 -static inline void *dma_alloc_coherent(struct device *dev, size_t size,
 -		dma_addr_t *dma_handle, gfp_t flag)
 -{
 -	return dma_alloc_attrs(dev, size, dma_handle, flag, 0);
 -}
 -
 -static inline void dma_free_coherent(struct device *dev, size_t size,
 -		void *cpu_addr, dma_addr_t dma_handle)
 -{
 -	return dma_free_attrs(dev, size, cpu_addr, dma_handle, 0);
 -}
 -
 -static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
 -		dma_addr_t *dma_handle, gfp_t gfp)
 -{
 -	return dma_alloc_attrs(dev, size, dma_handle, gfp,
 -			       DMA_ATTR_NON_CONSISTENT);
 -}
 -
 -static inline void dma_free_noncoherent(struct device *dev, size_t size,
 -		void *cpu_addr, dma_addr_t dma_handle)
 -{
 -	dma_free_attrs(dev, size, cpu_addr, dma_handle,
 -		       DMA_ATTR_NON_CONSISTENT);
 -}
 -
 -static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 -{
 -	debug_dma_mapping_error(dev, dma_addr);
 -
 -	if (get_dma_ops(dev)->mapping_error)
 -		return get_dma_ops(dev)->mapping_error(dev, dma_addr);
 -
 -#ifdef DMA_ERROR_CODE
 -	return dma_addr == DMA_ERROR_CODE;
 -#else
 -	return 0;
 -#endif
 -}
 -
 -#ifndef HAVE_ARCH_DMA_SUPPORTED
 -static inline int dma_supported(struct device *dev, u64 mask)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	if (!ops)
 -		return 0;
 -	if (!ops->dma_supported)
 -		return 1;
 -	return ops->dma_supported(dev, mask);
 -}
 -#endif
 -
 -#ifndef HAVE_ARCH_DMA_SET_MASK
 -static inline int dma_set_mask(struct device *dev, u64 mask)
 -{
 -	const struct dma_map_ops *ops = get_dma_ops(dev);
 -
 -	if (ops->set_dma_mask)
 -		return ops->set_dma_mask(dev, mask);
 -
 -	if (!dev->dma_mask || !dma_supported(dev, mask))
 -		return -EIO;
 -	*dev->dma_mask = mask;
 -	return 0;
 -}
 +#include <asm-generic/dma-mapping-broken.h>
  #endif
  
  static inline u64 dma_get_mask(struct device *dev)
* Unmerged path arch/h8300/include/asm/dma-mapping.h
* Unmerged path arch/m32r/include/asm/dma-mapping.h
* Unmerged path arch/nios2/include/asm/dma-mapping.h
* Unmerged path arch/alpha/include/asm/dma-mapping.h
* Unmerged path arch/arc/include/asm/dma-mapping.h
* Unmerged path arch/arm/include/asm/dma-mapping.h
* Unmerged path arch/arm64/include/asm/dma-mapping.h
* Unmerged path arch/avr32/include/asm/dma-mapping.h
* Unmerged path arch/blackfin/include/asm/dma-mapping.h
* Unmerged path arch/c6x/include/asm/dma-mapping.h
* Unmerged path arch/cris/include/asm/dma-mapping.h
* Unmerged path arch/frv/include/asm/dma-mapping.h
* Unmerged path arch/h8300/include/asm/dma-mapping.h
* Unmerged path arch/hexagon/include/asm/dma-mapping.h
* Unmerged path arch/ia64/include/asm/dma-mapping.h
* Unmerged path arch/m32r/include/asm/dma-mapping.h
* Unmerged path arch/m68k/include/asm/dma-mapping.h
* Unmerged path arch/metag/include/asm/dma-mapping.h
* Unmerged path arch/microblaze/include/asm/dma-mapping.h
* Unmerged path arch/mips/include/asm/dma-mapping.h
* Unmerged path arch/mn10300/include/asm/dma-mapping.h
* Unmerged path arch/nios2/include/asm/dma-mapping.h
* Unmerged path arch/openrisc/include/asm/dma-mapping.h
* Unmerged path arch/parisc/include/asm/dma-mapping.h
* Unmerged path arch/powerpc/include/asm/dma-mapping.h
diff --git a/arch/powerpc/include/asm/ps3.h b/arch/powerpc/include/asm/ps3.h
index a1bc7e758422..dd137adc0a22 100644
--- a/arch/powerpc/include/asm/ps3.h
+++ b/arch/powerpc/include/asm/ps3.h
@@ -435,7 +435,7 @@ static inline void *ps3_system_bus_get_drvdata(
 	return dev_get_drvdata(&dev->core);
 }
 
-/* These two need global scope for get_dma_ops(). */
+/* These two need global scope for get_arch_dma_ops(). */
 
 extern struct bus_type ps3_system_bus_type;
 
* Unmerged path arch/s390/include/asm/dma-mapping.h
* Unmerged path arch/sh/include/asm/dma-mapping.h
* Unmerged path arch/sparc/include/asm/dma-mapping.h
* Unmerged path arch/tile/include/asm/dma-mapping.h
* Unmerged path arch/unicore32/include/asm/dma-mapping.h
* Unmerged path arch/x86/include/asm/dma-mapping.h
* Unmerged path arch/xtensa/include/asm/dma-mapping.h
* Unmerged path include/linux/dma-mapping.h
