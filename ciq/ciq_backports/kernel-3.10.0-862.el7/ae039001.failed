xen/vcpu: Handle xen_vcpu_setup() failure at boot

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ankur Arora <ankur.a.arora@oracle.com>
commit ae039001054b34c4a624539b32a8b6ff3403aaf9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ae039001.failed

On PVH, PVHVM, at failure in the VCPUOP_register_vcpu_info hypercall
we limit the number of cpus to to MAX_VIRT_CPUS. However, if this
failure had occurred for a cpu beyond MAX_VIRT_CPUS, we continue
to function with > MAX_VIRT_CPUS.

This leads to problems at the next save/restore cycle when there
are > MAX_VIRT_CPUS threads going into stop_machine() but coming
back up there's valid state for only the first MAX_VIRT_CPUS.

This patch pulls the excess CPUs down via cpu_down().

	Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Signed-off-by: Ankur Arora <ankur.a.arora@oracle.com>
	Signed-off-by: Juergen Gross <jgross@suse.com>
(cherry picked from commit ae039001054b34c4a624539b32a8b6ff3403aaf9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/xen/smp.c
#	arch/x86/xen/smp.h
#	arch/x86/xen/smp_hvm.c
#	arch/x86/xen/smp_pv.c
diff --cc arch/x86/xen/smp.c
index 51edb98ff27d,e7f02eb73727..000000000000
--- a/arch/x86/xen/smp.c
+++ b/arch/x86/xen/smp.c
@@@ -1,37 -1,9 +1,42 @@@
++<<<<<<< HEAD
 +/*
 + * Xen SMP support
 + *
 + * This file implements the Xen versions of smp_ops.  SMP under Xen is
 + * very straightforward.  Bringing a CPU up is simply a matter of
 + * loading its initial context and setting it running.
 + *
 + * IPIs are handled through the Xen event mechanism.
 + *
 + * Because virtual CPUs can be scheduled onto any real CPU, there's no
 + * useful topology information for the kernel to make use of.  As a
 + * result, all CPUs are treated as if they're single-core and
 + * single-threaded.
 + */
 +#include <linux/sched.h>
 +#include <linux/err.h>
++=======
+ #include <linux/smp.h>
+ #include <linux/cpu.h>
++>>>>>>> ae039001054b (xen/vcpu: Handle xen_vcpu_setup() failure at boot)
  #include <linux/slab.h>
 -#include <linux/cpumask.h>
 -#include <linux/percpu.h>
 +#include <linux/smp.h>
 +#include <linux/irq_work.h>
 +#include <linux/tick.h>
 +
 +#include <asm/paravirt.h>
 +#include <asm/desc.h>
 +#include <asm/pgtable.h>
 +#include <asm/cpu.h>
 +
 +#include <xen/interface/xen.h>
 +#include <xen/interface/vcpu.h>
 +
 +#include <asm/xen/interface.h>
 +#include <asm/xen/hypercall.h>
  
 +#include <xen/xen.h>
 +#include <xen/page.h>
  #include <xen/events.h>
  
  #include <xen/hvc-console.h>
@@@ -216,333 -115,37 +221,367 @@@ static int xen_smp_intr_init(unsigned i
  	return rc;
  }
  
++<<<<<<< HEAD
 +static void __init xen_fill_possible_map(void)
 +{
 +	int i, rc;
 +
 +	if (xen_initial_domain())
 +		return;
 +
 +	for (i = 0; i < nr_cpu_ids; i++) {
 +		rc = HYPERVISOR_vcpu_op(VCPUOP_is_up, i, NULL);
 +		if (rc >= 0) {
 +			num_processors++;
 +			set_cpu_possible(i, true);
 +		}
 +	}
 +}
 +
 +static void __init xen_filter_cpu_maps(void)
 +{
 +	int i, rc;
 +	unsigned int subtract = 0;
 +
 +	if (!xen_initial_domain())
 +		return;
 +
 +	num_processors = 0;
 +	disabled_cpus = 0;
 +	for (i = 0; i < nr_cpu_ids; i++) {
 +		rc = HYPERVISOR_vcpu_op(VCPUOP_is_up, i, NULL);
 +		if (rc >= 0) {
 +			num_processors++;
 +			set_cpu_possible(i, true);
 +		} else {
 +			set_cpu_possible(i, false);
 +			set_cpu_present(i, false);
 +			subtract++;
 +		}
 +	}
 +#ifdef CONFIG_HOTPLUG_CPU
 +	/* This is akin to using 'nr_cpus' on the Linux command line.
 +	 * Which is OK as when we use 'dom0_max_vcpus=X' we can only
 +	 * have up to X, while nr_cpu_ids is greater than X. This
 +	 * normally is not a problem, except when CPU hotplugging
 +	 * is involved and then there might be more than X CPUs
 +	 * in the guest - which will not work as there is no
 +	 * hypercall to expand the max number of VCPUs an already
 +	 * running guest has. So cap it up to X. */
 +	if (subtract)
 +		nr_cpu_ids = nr_cpu_ids - subtract;
 +#endif
 +
 +}
 +
 +static void __init xen_smp_prepare_boot_cpu(void)
 +{
 +	BUG_ON(smp_processor_id() != 0);
 +	native_smp_prepare_boot_cpu();
 +
 +	if (xen_pv_domain()) {
 +		/* We've switched to the "real" per-cpu gdt, so make sure the
 +		   old memory can be recycled */
 +		make_lowmem_page_readwrite(xen_initial_gdt);
 +
 +		xen_filter_cpu_maps();
 +		xen_setup_vcpu_info_placement();
 +	}
 +
 +	/*
 +	 * Setup vcpu_info for boot CPU.
 +	 */
 +	if (xen_hvm_domain())
 +		xen_vcpu_setup(0);
 +
 +	/*
 +	 * The alternative logic (which patches the unlock/lock) runs before
 +	 * the smp bootup up code is activated. Hence we need to set this up
 +	 * the core kernel is being patched. Otherwise we will have only
 +	 * modules patched but not core code.
 +	 */
 +	xen_init_spinlocks();
 +}
 +
 +static void __init xen_smp_prepare_cpus(unsigned int max_cpus)
 +{
 +	unsigned cpu;
 +	unsigned int i;
 +
 +	if (skip_ioapic_setup) {
 +		char *m = (max_cpus == 0) ?
 +			"The nosmp parameter is incompatible with Xen; " \
 +			"use Xen dom0_max_vcpus=1 parameter" :
 +			"The noapic parameter is incompatible with Xen";
 +
 +		xen_raw_printk(m);
 +		panic(m);
 +	}
 +	xen_init_lock_cpu(0);
 +
 +	smp_store_boot_cpu_info();
 +	cpu_data(0).x86_max_cores = 1;
 +
 +	for_each_possible_cpu(i) {
 +		zalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);
 +		zalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);
 +		zalloc_cpumask_var(&per_cpu(cpu_llc_shared_map, i), GFP_KERNEL);
 +	}
 +	set_cpu_sibling_map(0);
 +
 +	if (xen_smp_intr_init(0))
 +		BUG();
 +
 +	if (!alloc_cpumask_var(&xen_cpu_initialized_map, GFP_KERNEL))
 +		panic("could not allocate xen_cpu_initialized_map\n");
 +
 +	cpumask_copy(xen_cpu_initialized_map, cpumask_of(0));
 +
 +	/* Restrict the possible_map according to max_cpus. */
 +	while ((num_possible_cpus() > 1) && (num_possible_cpus() > max_cpus)) {
 +		for (cpu = nr_cpu_ids - 1; !cpu_possible(cpu); cpu--)
 +			continue;
 +		set_cpu_possible(cpu, false);
 +	}
 +
 +	for_each_possible_cpu(cpu)
 +		set_cpu_present(cpu, true);
 +}
 +
 +static int
 +cpu_initialize_context(unsigned int cpu, struct task_struct *idle)
 +{
 +	struct vcpu_guest_context *ctxt;
 +	struct desc_struct *gdt;
 +	unsigned long gdt_mfn;
 +
 +	/* used to tell cpu_init() that it can proceed with initialization */
 +	cpumask_set_cpu(cpu, cpu_callout_mask);
 +	if (cpumask_test_and_set_cpu(cpu, xen_cpu_initialized_map))
 +		return 0;
 +
 +	ctxt = kzalloc(sizeof(*ctxt), GFP_KERNEL);
 +	if (ctxt == NULL)
 +		return -ENOMEM;
 +
 +	gdt = get_cpu_gdt_table(cpu);
 +
 +	ctxt->flags = VGCF_IN_KERNEL;
 +	ctxt->user_regs.ss = __KERNEL_DS;
 +#ifdef CONFIG_X86_32
 +	ctxt->user_regs.fs = __KERNEL_PERCPU;
 +	ctxt->user_regs.gs = __KERNEL_STACK_CANARY;
 +#else
 +	ctxt->gs_base_kernel = per_cpu_offset(cpu);
 +#endif
 +	ctxt->user_regs.eip = (unsigned long)cpu_bringup_and_idle;
 +
 +	memset(&ctxt->fpu_ctxt, 0, sizeof(ctxt->fpu_ctxt));
 +
 +	{
 +		ctxt->user_regs.eflags = 0x1000; /* IOPL_RING1 */
 +		ctxt->user_regs.ds = __USER_DS;
 +		ctxt->user_regs.es = __USER_DS;
 +
 +		xen_copy_trap_info(ctxt->trap_ctxt);
 +
 +		ctxt->ldt_ents = 0;
 +
 +		BUG_ON((unsigned long)gdt & ~PAGE_MASK);
 +
 +		gdt_mfn = arbitrary_virt_to_mfn(gdt);
 +		make_lowmem_page_readonly(gdt);
 +		make_lowmem_page_readonly(mfn_to_virt(gdt_mfn));
 +
 +		ctxt->gdt_frames[0] = gdt_mfn;
 +		ctxt->gdt_ents      = GDT_ENTRIES;
 +
 +		ctxt->kernel_ss = __KERNEL_DS;
 +		ctxt->kernel_sp = idle->thread.sp0;
 +
 +#ifdef CONFIG_X86_32
 +		ctxt->event_callback_cs     = __KERNEL_CS;
 +		ctxt->failsafe_callback_cs  = __KERNEL_CS;
 +#endif
 +		ctxt->event_callback_eip    =
 +					(unsigned long)xen_hypervisor_callback;
 +		ctxt->failsafe_callback_eip =
 +					(unsigned long)xen_failsafe_callback;
 +	}
 +	ctxt->user_regs.cs = __KERNEL_CS;
 +	ctxt->user_regs.esp = idle->thread.sp0 - sizeof(struct pt_regs);
 +
 +	per_cpu(xen_cr3, cpu) = __pa(swapper_pg_dir);
 +	ctxt->ctrlreg[3] = xen_pfn_to_cr3(virt_to_mfn(swapper_pg_dir));
 +
 +	if (HYPERVISOR_vcpu_op(VCPUOP_initialise, xen_vcpu_nr(cpu), ctxt))
 +		BUG();
 +
 +	kfree(ctxt);
 +	return 0;
 +}
 +
 +static int xen_cpu_up(unsigned int cpu, struct task_struct *idle)
 +{
 +	int rc;
 +
 +	per_cpu(current_task, cpu) = idle;
 +#ifdef CONFIG_X86_32
 +	irq_ctx_init(cpu);
 +#else
 +	clear_tsk_thread_flag(idle, TIF_FORK);
 +	per_cpu(kernel_stack, cpu) =
 +		(unsigned long)task_stack_page(idle) -
 +		KERNEL_STACK_OFFSET + THREAD_SIZE;
 +	per_cpu(__kernel_stack_70__, cpu) =
 +		(unsigned long)task_stack_page(idle) -
 +		KERNEL_STACK_OFFSET + THREAD_SIZE - 8192;
 +#endif
 +	xen_setup_runstate_info(cpu);
 +	xen_setup_timer(cpu);
 +	xen_init_lock_cpu(cpu);
 +
 +	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
 +
 +	/* make sure interrupts start blocked */
 +	per_cpu(xen_vcpu, cpu)->evtchn_upcall_mask = 1;
 +
 +	rc = cpu_initialize_context(cpu, idle);
 +	if (rc)
 +		return rc;
 +
 +	if (num_online_cpus() == 1)
 +		/* Just in case we booted with a single CPU. */
 +		alternatives_enable_smp();
 +
 +	rc = xen_smp_intr_init(cpu);
 +	if (rc)
 +		return rc;
 +
 +	rc = HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL);
 +	BUG_ON(rc);
 +
 +	while(per_cpu(cpu_state, cpu) != CPU_ONLINE) {
 +		HYPERVISOR_sched_op(SCHEDOP_yield, NULL);
 +		barrier();
 +	}
 +
 +	return 0;
 +}
 +
 +static void xen_smp_cpus_done(unsigned int max_cpus)
 +{
 +}
 +
 +#ifdef CONFIG_HOTPLUG_CPU
 +static int xen_cpu_disable(void)
 +{
 +	unsigned int cpu = smp_processor_id();
 +	if (cpu == 0)
 +		return -EBUSY;
 +
 +	cpu_disable_common();
 +
 +	load_cr3(swapper_pg_dir);
 +	return 0;
 +}
 +
 +static void xen_cpu_die(unsigned int cpu)
 +{
 +	while (xen_pv_domain() && HYPERVISOR_vcpu_op(VCPUOP_is_up,
 +						     xen_vcpu_nr(cpu), NULL)) {
 +		current->state = TASK_UNINTERRUPTIBLE;
 +		schedule_timeout(HZ/10);
 +	}
 +	xen_smp_intr_free(cpu);
 +	xen_uninit_lock_cpu(cpu);
 +	xen_teardown_timer(cpu);
 +}
 +
 +static void xen_play_dead(void) /* used only with HOTPLUG_CPU */
 +{
 +	play_dead_common();
 +	HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(smp_processor_id()), NULL);
 +	cpu_bringup();
 +	/*
 +	 * commit 4b0c0f294 (tick: Cleanup NOHZ per cpu data on cpu down)
 +	 * clears certain data that the cpu_idle loop (which called us
 +	 * and that we return from) expects. The only way to get that
 +	 * data back is to call:
 +	 */
 +	tick_nohz_idle_enter();
 +}
 +
 +#else /* !CONFIG_HOTPLUG_CPU */
 +static int xen_cpu_disable(void)
 +{
 +	return -ENOSYS;
 +}
 +
 +static void xen_cpu_die(unsigned int cpu)
 +{
 +	BUG();
 +}
 +
 +static void xen_play_dead(void)
 +{
 +	BUG();
 +}
 +
 +#endif
 +static void stop_self(void *v)
 +{
 +	int cpu = smp_processor_id();
 +
 +	/* make sure we're not pinning something down */
 +	load_cr3(swapper_pg_dir);
 +	/* should set up a minimal gdt */
 +
 +	set_cpu_online(cpu, false);
 +
 +	HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(cpu), NULL);
 +	BUG();
 +}
 +
 +static void xen_stop_other_cpus(int wait)
 +{
 +	smp_call_function(stop_self, NULL, wait);
 +}
 +
 +static void xen_smp_send_reschedule(int cpu)
++=======
+ void __init xen_smp_cpus_done(unsigned int max_cpus)
+ {
+ 	int cpu, rc, count = 0;
+ 
+ 	if (xen_hvm_domain())
+ 		native_smp_cpus_done(max_cpus);
+ 
+ 	if (xen_have_vcpu_info_placement)
+ 		return;
+ 
+ 	for_each_online_cpu(cpu) {
+ 		if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS)
+ 			continue;
+ 
+ 		rc = cpu_down(cpu);
+ 
+ 		if (rc == 0) {
+ 			/*
+ 			 * Reset vcpu_info so this cpu cannot be onlined again.
+ 			 */
+ 			xen_vcpu_info_reset(cpu);
+ 			count++;
+ 		} else {
+ 			pr_warn("%s: failed to bring CPU %d down, error %d\n",
+ 				__func__, cpu, rc);
+ 		}
+ 	}
+ 	WARN(count, "%s: brought %d CPUs offline\n", __func__, count);
+ }
+ 
+ void xen_smp_send_reschedule(int cpu)
++>>>>>>> ae039001054b (xen/vcpu: Handle xen_vcpu_setup() failure at boot)
  {
  	xen_send_IPI_one(cpu, XEN_RESCHEDULE_VECTOR);
  }
diff --cc arch/x86/xen/smp.h
index c7c2d89efd76,87d3c76cba37..000000000000
--- a/arch/x86/xen/smp.h
+++ b/arch/x86/xen/smp.h
@@@ -8,4 -9,34 +8,37 @@@ extern void xen_send_IPI_allbutself(in
  extern void xen_send_IPI_all(int vector);
  extern void xen_send_IPI_self(int vector);
  
++<<<<<<< HEAD
++=======
+ extern int xen_smp_intr_init(unsigned int cpu);
+ extern void xen_smp_intr_free(unsigned int cpu);
+ int xen_smp_intr_init_pv(unsigned int cpu);
+ void xen_smp_intr_free_pv(unsigned int cpu);
+ 
+ void xen_smp_cpus_done(unsigned int max_cpus);
+ 
+ void xen_smp_send_reschedule(int cpu);
+ void xen_smp_send_call_function_ipi(const struct cpumask *mask);
+ void xen_smp_send_call_function_single_ipi(int cpu);
+ 
+ struct xen_common_irq {
+ 	int irq;
+ 	char *name;
+ };
+ #else /* CONFIG_SMP */
+ 
+ static inline int xen_smp_intr_init(unsigned int cpu)
+ {
+ 	return 0;
+ }
+ static inline void xen_smp_intr_free(unsigned int cpu) {}
+ 
+ static inline int xen_smp_intr_init_pv(unsigned int cpu)
+ {
+ 	return 0;
+ }
+ static inline void xen_smp_intr_free_pv(unsigned int cpu) {}
+ #endif /* CONFIG_SMP */
+ 
++>>>>>>> ae039001054b (xen/vcpu: Handle xen_vcpu_setup() failure at boot)
  #endif
* Unmerged path arch/x86/xen/smp_hvm.c
* Unmerged path arch/x86/xen/smp_pv.c
* Unmerged path arch/x86/xen/smp.c
* Unmerged path arch/x86/xen/smp.h
* Unmerged path arch/x86/xen/smp_hvm.c
* Unmerged path arch/x86/xen/smp_pv.c
