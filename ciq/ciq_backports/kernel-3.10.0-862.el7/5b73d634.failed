KVM: PPC: Book3S HV: Prevent double-free on HPT resize commit path

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author David Gibson <david@gibson.dropbear.id.au>
commit 5b73d6347eb82cd2a26698fc339607e25e0ad917
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/5b73d634.failed

resize_hpt_release(), called once the HPT resize of a KVM guest is
completed (successfully or unsuccessfully) frees the state structure for
the resize.  It is currently not safe to call with a NULL pointer.

However, one of the error paths in kvm_vm_ioctl_resize_hpt_commit() can
invoke it with a NULL pointer.  This will occur if userspace improperly
invokes KVM_PPC_RESIZE_HPT_COMMIT without previously calling
KVM_PPC_RESIZE_HPT_PREPARE, or if it calls COMMIT twice without an
intervening PREPARE.

To fix this potential crash bug - and maybe others like it, make it safe
(and a no-op) to call resize_hpt_release() with a NULL resize pointer.

Found by Dan Carpenter with a static checker.

	Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
	Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
(cherry picked from commit 5b73d6347eb82cd2a26698fc339607e25e0ad917)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_64_mmu_hv.c
diff --cc arch/powerpc/kvm/book3s_64_mmu_hv.c
index 283e37e10f56,72ccac26e464..000000000000
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@@ -1119,6 -1209,357 +1119,360 @@@ void kvmppc_unpin_guest_page(struct kv
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * HPT resizing
+  */
+ static int resize_hpt_allocate(struct kvm_resize_hpt *resize)
+ {
+ 	int rc;
+ 
+ 	rc = kvmppc_allocate_hpt(&resize->hpt, resize->order);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_allocate(): HPT @ 0x%lx\n",
+ 			 resize->hpt.virt);
+ 
+ 	return 0;
+ }
+ 
+ static unsigned long resize_hpt_rehash_hpte(struct kvm_resize_hpt *resize,
+ 					    unsigned long idx)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	struct kvm_hpt_info *old = &kvm->arch.hpt;
+ 	struct kvm_hpt_info *new = &resize->hpt;
+ 	unsigned long old_hash_mask = (1ULL << (old->order - 7)) - 1;
+ 	unsigned long new_hash_mask = (1ULL << (new->order - 7)) - 1;
+ 	__be64 *hptep, *new_hptep;
+ 	unsigned long vpte, rpte, guest_rpte;
+ 	int ret;
+ 	struct revmap_entry *rev;
+ 	unsigned long apsize, psize, avpn, pteg, hash;
+ 	unsigned long new_idx, new_pteg, replace_vpte;
+ 
+ 	hptep = (__be64 *)(old->virt + (idx << 4));
+ 
+ 	/* Guest is stopped, so new HPTEs can't be added or faulted
+ 	 * in, only unmapped or altered by host actions.  So, it's
+ 	 * safe to check this before we take the HPTE lock */
+ 	vpte = be64_to_cpu(hptep[0]);
+ 	if (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))
+ 		return 0; /* nothing to do */
+ 
+ 	while (!try_lock_hpte(hptep, HPTE_V_HVLOCK))
+ 		cpu_relax();
+ 
+ 	vpte = be64_to_cpu(hptep[0]);
+ 
+ 	ret = 0;
+ 	if (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))
+ 		/* Nothing to do */
+ 		goto out;
+ 
+ 	/* Unmap */
+ 	rev = &old->rev[idx];
+ 	guest_rpte = rev->guest_rpte;
+ 
+ 	ret = -EIO;
+ 	apsize = hpte_page_size(vpte, guest_rpte);
+ 	if (!apsize)
+ 		goto out;
+ 
+ 	if (vpte & HPTE_V_VALID) {
+ 		unsigned long gfn = hpte_rpn(guest_rpte, apsize);
+ 		int srcu_idx = srcu_read_lock(&kvm->srcu);
+ 		struct kvm_memory_slot *memslot =
+ 			__gfn_to_memslot(kvm_memslots(kvm), gfn);
+ 
+ 		if (memslot) {
+ 			unsigned long *rmapp;
+ 			rmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];
+ 
+ 			lock_rmap(rmapp);
+ 			kvmppc_unmap_hpte(kvm, idx, rmapp, gfn);
+ 			unlock_rmap(rmapp);
+ 		}
+ 
+ 		srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 	}
+ 
+ 	/* Reload PTE after unmap */
+ 	vpte = be64_to_cpu(hptep[0]);
+ 
+ 	BUG_ON(vpte & HPTE_V_VALID);
+ 	BUG_ON(!(vpte & HPTE_V_ABSENT));
+ 
+ 	ret = 0;
+ 	if (!(vpte & HPTE_V_BOLTED))
+ 		goto out;
+ 
+ 	rpte = be64_to_cpu(hptep[1]);
+ 	psize = hpte_base_page_size(vpte, rpte);
+ 	avpn = HPTE_V_AVPN_VAL(vpte) & ~((psize - 1) >> 23);
+ 	pteg = idx / HPTES_PER_GROUP;
+ 	if (vpte & HPTE_V_SECONDARY)
+ 		pteg = ~pteg;
+ 
+ 	if (!(vpte & HPTE_V_1TB_SEG)) {
+ 		unsigned long offset, vsid;
+ 
+ 		/* We only have 28 - 23 bits of offset in avpn */
+ 		offset = (avpn & 0x1f) << 23;
+ 		vsid = avpn >> 5;
+ 		/* We can find more bits from the pteg value */
+ 		if (psize < (1ULL << 23))
+ 			offset |= ((vsid ^ pteg) & old_hash_mask) * psize;
+ 
+ 		hash = vsid ^ (offset / psize);
+ 	} else {
+ 		unsigned long offset, vsid;
+ 
+ 		/* We only have 40 - 23 bits of seg_off in avpn */
+ 		offset = (avpn & 0x1ffff) << 23;
+ 		vsid = avpn >> 17;
+ 		if (psize < (1ULL << 23))
+ 			offset |= ((vsid ^ (vsid << 25) ^ pteg) & old_hash_mask) * psize;
+ 
+ 		hash = vsid ^ (vsid << 25) ^ (offset / psize);
+ 	}
+ 
+ 	new_pteg = hash & new_hash_mask;
+ 	if (vpte & HPTE_V_SECONDARY) {
+ 		BUG_ON(~pteg != (hash & old_hash_mask));
+ 		new_pteg = ~new_pteg;
+ 	} else {
+ 		BUG_ON(pteg != (hash & old_hash_mask));
+ 	}
+ 
+ 	new_idx = new_pteg * HPTES_PER_GROUP + (idx % HPTES_PER_GROUP);
+ 	new_hptep = (__be64 *)(new->virt + (new_idx << 4));
+ 
+ 	replace_vpte = be64_to_cpu(new_hptep[0]);
+ 
+ 	if (replace_vpte & (HPTE_V_VALID | HPTE_V_ABSENT)) {
+ 		BUG_ON(new->order >= old->order);
+ 
+ 		if (replace_vpte & HPTE_V_BOLTED) {
+ 			if (vpte & HPTE_V_BOLTED)
+ 				/* Bolted collision, nothing we can do */
+ 				ret = -ENOSPC;
+ 			/* Discard the new HPTE */
+ 			goto out;
+ 		}
+ 
+ 		/* Discard the previous HPTE */
+ 	}
+ 
+ 	new_hptep[1] = cpu_to_be64(rpte);
+ 	new->rev[new_idx].guest_rpte = guest_rpte;
+ 	/* No need for a barrier, since new HPT isn't active */
+ 	new_hptep[0] = cpu_to_be64(vpte);
+ 	unlock_hpte(new_hptep, vpte);
+ 
+ out:
+ 	unlock_hpte(hptep, vpte);
+ 	return ret;
+ }
+ 
+ static int resize_hpt_rehash(struct kvm_resize_hpt *resize)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	unsigned  long i;
+ 	int rc;
+ 
+ 	for (i = 0; i < kvmppc_hpt_npte(&kvm->arch.hpt); i++) {
+ 		rc = resize_hpt_rehash_hpte(resize, i);
+ 		if (rc != 0)
+ 			return rc;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void resize_hpt_pivot(struct kvm_resize_hpt *resize)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	struct kvm_hpt_info hpt_tmp;
+ 
+ 	/* Exchange the pending tables in the resize structure with
+ 	 * the active tables */
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_pivot()\n");
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	asm volatile("ptesync" : : : "memory");
+ 
+ 	hpt_tmp = kvm->arch.hpt;
+ 	kvmppc_set_hpt(kvm, &resize->hpt);
+ 	resize->hpt = hpt_tmp;
+ 
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	synchronize_srcu_expedited(&kvm->srcu);
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_pivot() done\n");
+ }
+ 
+ static void resize_hpt_release(struct kvm *kvm, struct kvm_resize_hpt *resize)
+ {
+ 	BUG_ON(kvm->arch.resize_hpt != resize);
+ 
+ 	if (!resize)
+ 		return;
+ 
+ 	if (resize->hpt.virt)
+ 		kvmppc_free_hpt(&resize->hpt);
+ 
+ 	kvm->arch.resize_hpt = NULL;
+ 	kfree(resize);
+ }
+ 
+ static void resize_hpt_prepare_work(struct work_struct *work)
+ {
+ 	struct kvm_resize_hpt *resize = container_of(work,
+ 						     struct kvm_resize_hpt,
+ 						     work);
+ 	struct kvm *kvm = resize->kvm;
+ 	int err;
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_prepare_work(): order = %d\n",
+ 			 resize->order);
+ 
+ 	err = resize_hpt_allocate(resize);
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize->error = err;
+ 	resize->prepare_done = true;
+ 
+ 	mutex_unlock(&kvm->lock);
+ }
+ 
+ long kvm_vm_ioctl_resize_hpt_prepare(struct kvm *kvm,
+ 				     struct kvm_ppc_resize_hpt *rhpt)
+ {
+ 	unsigned long flags = rhpt->flags;
+ 	unsigned long shift = rhpt->shift;
+ 	struct kvm_resize_hpt *resize;
+ 	int ret;
+ 
+ 	if (flags != 0)
+ 		return -EINVAL;
+ 
+ 	if (shift && ((shift < 18) || (shift > 46)))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize = kvm->arch.resize_hpt;
+ 
+ 	if (resize) {
+ 		if (resize->order == shift) {
+ 			/* Suitable resize in progress */
+ 			if (resize->prepare_done) {
+ 				ret = resize->error;
+ 				if (ret != 0)
+ 					resize_hpt_release(kvm, resize);
+ 			} else {
+ 				ret = 100; /* estimated time in ms */
+ 			}
+ 
+ 			goto out;
+ 		}
+ 
+ 		/* not suitable, cancel it */
+ 		resize_hpt_release(kvm, resize);
+ 	}
+ 
+ 	ret = 0;
+ 	if (!shift)
+ 		goto out; /* nothing to do */
+ 
+ 	/* start new resize */
+ 
+ 	resize = kzalloc(sizeof(*resize), GFP_KERNEL);
+ 	resize->order = shift;
+ 	resize->kvm = kvm;
+ 	INIT_WORK(&resize->work, resize_hpt_prepare_work);
+ 	kvm->arch.resize_hpt = resize;
+ 
+ 	schedule_work(&resize->work);
+ 
+ 	ret = 100; /* estimated time in ms */
+ 
+ out:
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ static void resize_hpt_boot_vcpu(void *opaque)
+ {
+ 	/* Nothing to do, just force a KVM exit */
+ }
+ 
+ long kvm_vm_ioctl_resize_hpt_commit(struct kvm *kvm,
+ 				    struct kvm_ppc_resize_hpt *rhpt)
+ {
+ 	unsigned long flags = rhpt->flags;
+ 	unsigned long shift = rhpt->shift;
+ 	struct kvm_resize_hpt *resize;
+ 	long ret;
+ 
+ 	if (flags != 0)
+ 		return -EINVAL;
+ 
+ 	if (shift && ((shift < 18) || (shift > 46)))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize = kvm->arch.resize_hpt;
+ 
+ 	/* This shouldn't be possible */
+ 	ret = -EIO;
+ 	if (WARN_ON(!kvm->arch.hpte_setup_done))
+ 		goto out_no_hpt;
+ 
+ 	/* Stop VCPUs from running while we mess with the HPT */
+ 	kvm->arch.hpte_setup_done = 0;
+ 	smp_mb();
+ 
+ 	/* Boot all CPUs out of the guest so they re-read
+ 	 * hpte_setup_done */
+ 	on_each_cpu(resize_hpt_boot_vcpu, NULL, 1);
+ 
+ 	ret = -ENXIO;
+ 	if (!resize || (resize->order != shift))
+ 		goto out;
+ 
+ 	ret = -EBUSY;
+ 	if (!resize->prepare_done)
+ 		goto out;
+ 
+ 	ret = resize->error;
+ 	if (ret != 0)
+ 		goto out;
+ 
+ 	ret = resize_hpt_rehash(resize);
+ 	if (ret != 0)
+ 		goto out;
+ 
+ 	resize_hpt_pivot(resize);
+ 
+ out:
+ 	/* Let VCPUs run again */
+ 	kvm->arch.hpte_setup_done = 1;
+ 	smp_mb();
+ out_no_hpt:
+ 	resize_hpt_release(kvm, resize);
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ /*
++>>>>>>> 5b73d6347eb8 (KVM: PPC: Book3S HV: Prevent double-free on HPT resize commit path)
   * Functions for reading and writing the hash table via reads and
   * writes on a file descriptor.
   *
* Unmerged path arch/powerpc/kvm/book3s_64_mmu_hv.c
