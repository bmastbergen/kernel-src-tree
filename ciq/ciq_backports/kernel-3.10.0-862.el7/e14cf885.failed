cpufreq: intel_pstate: Eliminate struct perf_limits

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Eliminate struct perf_limits (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 90.32%
commit-author Rafael J. Wysocki <rafael.j.wysocki@intel.com>
commit e14cf8857ebd7486a4e30fa7dad06ba187e6cb04
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e14cf885.failed

After recent changes the purpose of struct perf_limits is not
particularly clear any more and the code may be made somewhat
easier to follow by eliminating it, so go for that.

	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit e14cf8857ebd7486a4e30fa7dad06ba187e6cb04)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index 9b85edc571c0,a7ed42d6f366..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -162,9 -186,32 +162,31 @@@ struct _pid 
  };
  
  /**
++<<<<<<< HEAD
++=======
+  * struct global_params - Global parameters, mostly tunable via sysfs.
+  * @no_turbo:		Whether or not to use turbo P-states.
+  * @turbo_disabled:	Whethet or not turbo P-states are available at all,
+  *			based on the MSR_IA32_MISC_ENABLE value and whether or
+  *			not the maximum reported turbo P-state is different from
+  *			the maximum reported non-turbo one.
+  * @min_perf_pct:	Minimum capacity limit in percent of the maximum turbo
+  *			P-state capacity.
+  * @max_perf_pct:	Maximum capacity limit in percent of the maximum turbo
+  *			P-state capacity.
+  */
+ struct global_params {
+ 	bool no_turbo;
+ 	bool turbo_disabled;
+ 	int max_perf_pct;
+ 	int min_perf_pct;
+ };
+ 
+ /**
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
   * struct cpudata -	Per CPU instance data storage
   * @cpu:		CPU number for this instance data
 - * @policy:		CPUFreq policy value
   * @update_util:	CPUFreq utility callback information
 - * @update_util_set:	CPUFreq utility callback is set
 - * @iowait_boost:	iowait-related boost fraction
 - * @last_update:	Time of the last update.
   * @pstate:		Stores P state limits for this CPU
   * @vid:		Stores VID limits for this CPU
   * @pid:		Stores PID parameters for this CPU
@@@ -175,8 -222,20 +197,15 @@@
   * @prev_cummulative_iowait: IO Wait time difference from last and
   *			current sample
   * @sample:		Storage for storing last Sample data
++<<<<<<< HEAD
++=======
+  * @min_perf:		Minimum capacity limit as a fraction of the maximum
+  *			turbo P-state capacity.
+  * @max_perf:		Maximum capacity limit as a fraction of the maximum
+  *			turbo P-state capacity.
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
   * @acpi_perf_data:	Stores ACPI perf information read from _PSS
   * @valid_pss_table:	Set to true for valid ACPI _PSS entries found
 - * @epp_powersave:	Last saved HWP energy performance preference
 - *			(EPP) or energy performance bias (EPB),
 - *			when policy switched to performance
 - * @epp_policy:		Last saved policy used to set EPP/EPB
 - * @epp_default:	Power on default HWP energy performance
 - *			preference/bias
 - * @epp_saved:		Saved EPP/EPB during system suspend or CPU offline
 - *			operation
   *
   * This structure stores per CPU instance data for all CPUs.
   */
@@@ -195,6 -257,8 +224,11 @@@ struct cpudata 
  	u64	prev_tsc;
  	u64	prev_cummulative_iowait;
  	struct sample sample;
++<<<<<<< HEAD
++=======
+ 	int32_t	min_perf;
+ 	int32_t	max_perf;
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
  #ifdef CONFIG_ACPI
  	struct acpi_processor_performance acpi_perf_data;
  	bool valid_pss_table;
@@@ -551,23 -591,255 +585,39 @@@ static inline void update_turbo_state(v
  		 cpu->pstate.max_pstate == cpu->pstate.turbo_pstate);
  }
  
 -static int min_perf_pct_min(void)
 -{
 -	struct cpudata *cpu = all_cpu_data[0];
 -
 -	return DIV_ROUND_UP(cpu->pstate.min_pstate * 100,
 -			    cpu->pstate.turbo_pstate);
 -}
 -
 -static s16 intel_pstate_get_epb(struct cpudata *cpu_data)
 -{
 -	u64 epb;
 -	int ret;
 -
 -	if (!static_cpu_has(X86_FEATURE_EPB))
 -		return -ENXIO;
 -
 -	ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
 -	if (ret)
 -		return (s16)ret;
 -
 -	return (s16)(epb & 0x0f);
 -}
 -
 -static s16 intel_pstate_get_epp(struct cpudata *cpu_data, u64 hwp_req_data)
 -{
 -	s16 epp;
 -
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		/*
 -		 * When hwp_req_data is 0, means that caller didn't read
 -		 * MSR_HWP_REQUEST, so need to read and get EPP.
 -		 */
 -		if (!hwp_req_data) {
 -			epp = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST,
 -					    &hwp_req_data);
 -			if (epp)
 -				return epp;
 -		}
 -		epp = (hwp_req_data >> 24) & 0xff;
 -	} else {
 -		/* When there is no EPP present, HWP uses EPB settings */
 -		epp = intel_pstate_get_epb(cpu_data);
 -	}
 -
 -	return epp;
 -}
 -
 -static int intel_pstate_set_epb(int cpu, s16 pref)
 -{
 -	u64 epb;
 -	int ret;
 -
 -	if (!static_cpu_has(X86_FEATURE_EPB))
 -		return -ENXIO;
 -
 -	ret = rdmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
 -	if (ret)
 -		return ret;
 -
 -	epb = (epb & ~0x0f) | pref;
 -	wrmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, epb);
 -
 -	return 0;
 -}
 -
 -/*
 - * EPP/EPB display strings corresponding to EPP index in the
 - * energy_perf_strings[]
 - *	index		String
 - *-------------------------------------
 - *	0		default
 - *	1		performance
 - *	2		balance_performance
 - *	3		balance_power
 - *	4		power
 - */
 -static const char * const energy_perf_strings[] = {
 -	"default",
 -	"performance",
 -	"balance_performance",
 -	"balance_power",
 -	"power",
 -	NULL
 -};
 -
 -static int intel_pstate_get_energy_pref_index(struct cpudata *cpu_data)
 -{
 -	s16 epp;
 -	int index = -EINVAL;
 -
 -	epp = intel_pstate_get_epp(cpu_data, 0);
 -	if (epp < 0)
 -		return epp;
 -
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		/*
 -		 * Range:
 -		 *	0x00-0x3F	:	Performance
 -		 *	0x40-0x7F	:	Balance performance
 -		 *	0x80-0xBF	:	Balance power
 -		 *	0xC0-0xFF	:	Power
 -		 * The EPP is a 8 bit value, but our ranges restrict the
 -		 * value which can be set. Here only using top two bits
 -		 * effectively.
 -		 */
 -		index = (epp >> 6) + 1;
 -	} else if (static_cpu_has(X86_FEATURE_EPB)) {
 -		/*
 -		 * Range:
 -		 *	0x00-0x03	:	Performance
 -		 *	0x04-0x07	:	Balance performance
 -		 *	0x08-0x0B	:	Balance power
 -		 *	0x0C-0x0F	:	Power
 -		 * The EPB is a 4 bit value, but our ranges restrict the
 -		 * value which can be set. Here only using top two bits
 -		 * effectively.
 -		 */
 -		index = (epp >> 2) + 1;
 -	}
 -
 -	return index;
 -}
 -
 -static int intel_pstate_set_energy_pref_index(struct cpudata *cpu_data,
 -					      int pref_index)
 -{
 -	int epp = -EINVAL;
 -	int ret;
 -
 -	if (!pref_index)
 -		epp = cpu_data->epp_default;
 -
 -	mutex_lock(&intel_pstate_limits_lock);
 -
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		u64 value;
 -
 -		ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, &value);
 -		if (ret)
 -			goto return_pref;
 -
 -		value &= ~GENMASK_ULL(31, 24);
 -
 -		/*
 -		 * If epp is not default, convert from index into
 -		 * energy_perf_strings to epp value, by shifting 6
 -		 * bits left to use only top two bits in epp.
 -		 * The resultant epp need to shifted by 24 bits to
 -		 * epp position in MSR_HWP_REQUEST.
 -		 */
 -		if (epp == -EINVAL)
 -			epp = (pref_index - 1) << 6;
 -
 -		value |= (u64)epp << 24;
 -		ret = wrmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, value);
 -	} else {
 -		if (epp == -EINVAL)
 -			epp = (pref_index - 1) << 2;
 -		ret = intel_pstate_set_epb(cpu_data->cpu, epp);
 -	}
 -return_pref:
 -	mutex_unlock(&intel_pstate_limits_lock);
 -
 -	return ret;
 -}
 -
 -static ssize_t show_energy_performance_available_preferences(
 -				struct cpufreq_policy *policy, char *buf)
 -{
 -	int i = 0;
 -	int ret = 0;
 -
 -	while (energy_perf_strings[i] != NULL)
 -		ret += sprintf(&buf[ret], "%s ", energy_perf_strings[i++]);
 -
 -	ret += sprintf(&buf[ret], "\n");
 -
 -	return ret;
 -}
 -
 -cpufreq_freq_attr_ro(energy_performance_available_preferences);
 -
 -static ssize_t store_energy_performance_preference(
 -		struct cpufreq_policy *policy, const char *buf, size_t count)
 -{
 -	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
 -	char str_preference[21];
 -	int ret, i = 0;
 -
 -	ret = sscanf(buf, "%20s", str_preference);
 -	if (ret != 1)
 -		return -EINVAL;
 -
 -	while (energy_perf_strings[i] != NULL) {
 -		if (!strcmp(str_preference, energy_perf_strings[i])) {
 -			intel_pstate_set_energy_pref_index(cpu_data, i);
 -			return count;
 -		}
 -		++i;
 -	}
 -
 -	return -EINVAL;
 -}
 -
 -static ssize_t show_energy_performance_preference(
 -				struct cpufreq_policy *policy, char *buf)
 -{
 -	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
 -	int preference;
 -
 -	preference = intel_pstate_get_energy_pref_index(cpu_data);
 -	if (preference < 0)
 -		return preference;
 -
 -	return  sprintf(buf, "%s\n", energy_perf_strings[preference]);
 -}
 -
 -cpufreq_freq_attr_rw(energy_performance_preference);
 -
 -static struct freq_attr *hwp_cpufreq_attrs[] = {
 -	&energy_performance_preference,
 -	&energy_performance_available_preferences,
 -	NULL,
 -};
 -
 -static void intel_pstate_hwp_set(struct cpufreq_policy *policy)
 +static void intel_pstate_hwp_set(const struct cpumask *cpumask)
  {
 -	int min, hw_min, max, hw_max, cpu;
 +	int min, hw_min, max, hw_max, cpu, range, adj_range;
  	u64 value, cap;
  
++<<<<<<< HEAD
 +	for_each_cpu(cpu, cpumask) {
++=======
+ 	for_each_cpu(cpu, policy->cpus) {
+ 		struct cpudata *cpu_data = all_cpu_data[cpu];
+ 		s16 epp;
+ 
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
  		rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
  		hw_min = HWP_LOWEST_PERF(cap);
 -		if (global.no_turbo)
 +		if (limits->no_turbo)
  			hw_max = HWP_GUARANTEED_PERF(cap);
  		else
  			hw_max = HWP_HIGHEST_PERF(cap);
++<<<<<<< HEAD
 +		range = hw_max - hw_min;
++=======
+ 
+ 		max = fp_ext_toint(hw_max * cpu_data->max_perf);
+ 		if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE)
+ 			min = max;
+ 		else
+ 			min = fp_ext_toint(hw_max * cpu_data->min_perf);
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
  
  		rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
 -
 +		adj_range = limits->min_perf_pct * range / 100;
 +		min = hw_min + adj_range;
  		value &= ~HWP_MIN_PERF(~0L);
  		value |= HWP_MIN_PERF(min);
  
@@@ -1104,11 -1635,11 +1154,19 @@@ static void intel_pstate_get_min_max(st
  	 * policy, or by cpu specific default values determined through
  	 * experimentation.
  	 */
++<<<<<<< HEAD
 +	max_perf_adj = fp_toint(max_perf * limits->max_perf);
 +	*max = clamp_t(int, max_perf_adj,
 +			cpu->pstate.min_pstate, cpu->pstate.turbo_pstate);
 +
 +	min_perf = fp_toint(max_perf * limits->min_perf);
++=======
+ 	max_perf_adj = fp_ext_toint(max_perf * cpu->max_perf);
+ 	*max = clamp_t(int, max_perf_adj,
+ 			cpu->pstate.min_pstate, cpu->pstate.turbo_pstate);
+ 
+ 	min_perf = fp_ext_toint(max_perf * cpu->min_perf);
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
  	*min = clamp_t(int, min_perf, cpu->pstate.min_pstate, max_perf);
  }
  
@@@ -1424,14 -1978,94 +1482,103 @@@ static int intel_pstate_init_cpu(unsign
  
  static unsigned int intel_pstate_get(unsigned int cpu_num)
  {
 -	struct cpudata *cpu = all_cpu_data[cpu_num];
 +	struct sample *sample;
 +	struct cpudata *cpu;
  
++<<<<<<< HEAD
 +	cpu = all_cpu_data[cpu_num];
 +	if (!cpu)
 +		return 0;
 +	sample = &cpu->sample;
 +	return sample->freq;
++=======
+ 	return cpu ? get_avg_frequency(cpu) : 0;
+ }
+ 
+ static void intel_pstate_set_update_util_hook(unsigned int cpu_num)
+ {
+ 	struct cpudata *cpu = all_cpu_data[cpu_num];
+ 
+ 	if (cpu->update_util_set)
+ 		return;
+ 
+ 	/* Prevent intel_pstate_update_util() from using stale data. */
+ 	cpu->sample.time = 0;
+ 	cpufreq_add_update_util_hook(cpu_num, &cpu->update_util,
+ 				     intel_pstate_update_util);
+ 	cpu->update_util_set = true;
+ }
+ 
+ static void intel_pstate_clear_update_util_hook(unsigned int cpu)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[cpu];
+ 
+ 	if (!cpu_data->update_util_set)
+ 		return;
+ 
+ 	cpufreq_remove_update_util_hook(cpu);
+ 	cpu_data->update_util_set = false;
+ 	synchronize_sched();
+ }
+ 
+ static int intel_pstate_get_max_freq(struct cpudata *cpu)
+ {
+ 	return global.turbo_disabled || global.no_turbo ?
+ 			cpu->pstate.max_freq : cpu->pstate.turbo_freq;
+ }
+ 
+ static void intel_pstate_update_perf_limits(struct cpufreq_policy *policy,
+ 					    struct cpudata *cpu)
+ {
+ 	int max_freq = intel_pstate_get_max_freq(cpu);
+ 	int32_t max_policy_perf, min_policy_perf;
+ 
+ 	max_policy_perf = div_ext_fp(policy->max, max_freq);
+ 	max_policy_perf = clamp_t(int32_t, max_policy_perf, 0, int_ext_tofp(1));
+ 	if (policy->max == policy->min) {
+ 		min_policy_perf = max_policy_perf;
+ 	} else {
+ 		min_policy_perf = div_ext_fp(policy->min, max_freq);
+ 		min_policy_perf = clamp_t(int32_t, min_policy_perf,
+ 					  0, max_policy_perf);
+ 	}
+ 
+ 	/* Normalize user input to [min_perf, max_perf] */
+ 	if (per_cpu_limits) {
+ 		cpu->min_perf = min_policy_perf;
+ 		cpu->max_perf = max_policy_perf;
+ 	} else {
+ 		int32_t global_min, global_max;
+ 
+ 		/* Global limits are in percent of the maximum turbo P-state. */
+ 		global_max = percent_ext_fp(global.max_perf_pct);
+ 		global_min = percent_ext_fp(global.min_perf_pct);
+ 		if (max_freq != cpu->pstate.turbo_freq) {
+ 			int32_t turbo_factor;
+ 
+ 			turbo_factor = div_ext_fp(cpu->pstate.turbo_pstate,
+ 						  cpu->pstate.max_pstate);
+ 			global_min = mul_ext_fp(global_min, turbo_factor);
+ 			global_max = mul_ext_fp(global_max, turbo_factor);
+ 		}
+ 		global_min = clamp_t(int32_t, global_min, 0, global_max);
+ 
+ 		cpu->min_perf = max(min_policy_perf, global_min);
+ 		cpu->min_perf = min(cpu->min_perf, max_policy_perf);
+ 		cpu->max_perf = min(max_policy_perf, global_max);
+ 		cpu->max_perf = max(min_policy_perf, cpu->max_perf);
+ 
+ 		/* Make sure min_perf <= max_perf */
+ 		cpu->min_perf = min(cpu->min_perf, cpu->max_perf);
+ 	}
+ 
+ 	cpu->max_perf = round_up(cpu->max_perf, EXT_FRAC_BITS);
+ 	cpu->min_perf = round_up(cpu->min_perf, EXT_FRAC_BITS);
+ 
+ 	pr_debug("cpu:%d max_perf_pct:%d min_perf_pct:%d\n", policy->cpu,
+ 		 fp_ext_toint(cpu->max_perf * 100),
+ 		 fp_ext_toint(cpu->min_perf * 100));
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
  }
  
  static int intel_pstate_set_policy(struct cpufreq_policy *policy)
@@@ -1534,10 -2168,8 +1681,15 @@@ static int intel_pstate_cpu_init(struc
  
  	cpu = all_cpu_data[policy->cpu];
  
++<<<<<<< HEAD
 +	if (limits->min_perf_pct == 100 && limits->max_perf_pct == 100)
 +		policy->policy = CPUFREQ_POLICY_PERFORMANCE;
 +	else
 +		policy->policy = CPUFREQ_POLICY_POWERSAVE;
++=======
+ 	cpu->max_perf = int_ext_tofp(1);
+ 	cpu->min_perf = 0;
++>>>>>>> e14cf8857ebd (cpufreq: intel_pstate: Eliminate struct perf_limits)
  
  	policy->min = cpu->pstate.min_pstate * cpu->pstate.scaling;
  	policy->max = cpu->pstate.turbo_pstate * cpu->pstate.scaling;
* Unmerged path drivers/cpufreq/intel_pstate.c
