mm/slab: clean up DEBUG_PAGEALLOC processing code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] slab: clean up DEBUG_PAGEALLOC processing code (Waiman Long) [1481847]
Rebuild_FUZZ: 96.84%
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit 40b44137971c2e5865a78f9f7de274449983ccb5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/40b44137.failed

Currently, open code for checking DEBUG_PAGEALLOC cache is spread to
some sites.  It makes code unreadable and hard to change.

This patch cleans up this code.  The following patch will change the
criteria for DEBUG_PAGEALLOC cache so this clean-up will help it, too.

[akpm@linux-foundation.org: fix build with CONFIG_DEBUG_PAGEALLOC=n]
	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Jesper Dangaard Brouer <brouer@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 40b44137971c2e5865a78f9f7de274449983ccb5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slab.c
diff --cc mm/slab.c
index a0e6bca09324,3142ec3965cf..000000000000
--- a/mm/slab.c
+++ b/mm/slab.c
@@@ -1999,19 -1862,11 +2027,22 @@@ static void slab_destroy_debugcheck(str
  {
  	int i;
  	for (i = 0; i < cachep->num; i++) {
 -		void *objp = index_to_obj(cachep, page, i);
 +		void *objp = index_to_obj(cachep, slabp, i);
  
  		if (cachep->flags & SLAB_POISON) {
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_PAGEALLOC
 +			if (cachep->size % PAGE_SIZE == 0 &&
 +					OFF_SLAB(cachep))
 +				kernel_map_pages(virt_to_page(objp),
 +					cachep->size / PAGE_SIZE, 1);
 +			else
 +				check_poison_obj(cachep, objp);
 +#else
++=======
++>>>>>>> 40b44137971c (mm/slab: clean up DEBUG_PAGEALLOC processing code)
  			check_poison_obj(cachep, objp);
- #endif
+ 			slab_kernel_map(cachep, objp, 1, 0);
  		}
  		if (cachep->flags & SLAB_RED_ZONE) {
  			if (*dbg_redzone1(cachep, objp) != RED_INACTIVE)
@@@ -2347,17 -2244,7 +2378,21 @@@ __kmem_cache_create (struct kmem_cache 
  
  	if (flags & CFLGS_OFF_SLAB) {
  		/* really off slab. No need for manual alignment */
++<<<<<<< HEAD
 +		slab_size =
 +		    cachep->num * sizeof(kmem_bufctl_t) + sizeof(struct slab);
 +
 +#ifdef CONFIG_PAGE_POISONING
 +		/* If we're going to use the generic kernel_map_pages()
 +		 * poisoning, then it's going to smash the contents of
 +		 * the redzone and userword anyhow, so switch them off.
 +		 */
 +		if (size % PAGE_SIZE == 0 && flags & SLAB_POISON)
 +			flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);
 +#endif
++=======
+ 		freelist_size = calculate_freelist_size(cachep->num, 0);
++>>>>>>> 40b44137971c (mm/slab: clean up DEBUG_PAGEALLOC processing code)
  	}
  
  	cachep->colour_off = cache_line_size();
@@@ -2373,16 -2260,28 +2408,33 @@@
  	cachep->size = size;
  	cachep->reciprocal_buffer_size = reciprocal_value(size);
  
++<<<<<<< HEAD
 +	if (flags & CFLGS_OFF_SLAB) {
 +		cachep->slabp_cache = kmalloc_slab(slab_size, 0u);
++=======
+ #if DEBUG
+ 	/*
+ 	 * If we're going to use the generic kernel_map_pages()
+ 	 * poisoning, then it's going to smash the contents of
+ 	 * the redzone and userword anyhow, so switch them off.
+ 	 */
+ 	if (IS_ENABLED(CONFIG_PAGE_POISONING) &&
+ 		(cachep->flags & SLAB_POISON) &&
+ 		is_debug_pagealloc_cache(cachep))
+ 		cachep->flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);
+ #endif
+ 
+ 	if (OFF_SLAB(cachep)) {
+ 		cachep->freelist_cache = kmalloc_slab(freelist_size, 0u);
++>>>>>>> 40b44137971c (mm/slab: clean up DEBUG_PAGEALLOC processing code)
  		/*
 -		 * This is a possibility for one of the kmalloc_{dma,}_caches.
 +		 * This is a possibility for one of the malloc_sizes caches.
  		 * But since we go off slab only for object size greater than
 -		 * OFF_SLAB_MIN_SIZE, and kmalloc_{dma,}_caches get created
 -		 * in ascending order,this should not happen at all.
 +		 * PAGE_SIZE/8, and malloc_sizes gets created in ascending order,
 +		 * this should not happen at all.
  		 * But leave a BUG_ON for some lucky dude.
  		 */
 -		BUG_ON(ZERO_OR_NULL_PTR(cachep->freelist_cache));
 +		BUG_ON(ZERO_OR_NULL_PTR(cachep->slabp_cache));
  	}
  
  	err = setup_cpu_cache(cachep, gfp);
@@@ -2639,11 -2494,8 +2691,8 @@@ static void cache_init_objs(struct kmem
  	int i;
  
  	for (i = 0; i < cachep->num; i++) {
 -		void *objp = index_to_obj(cachep, page, i);
 +		void *objp = index_to_obj(cachep, slabp, i);
  #if DEBUG
- 		/* need to poison the objs? */
- 		if (cachep->flags & SLAB_POISON)
- 			poison_obj(cachep, objp, POISON_FREE);
  		if (cachep->flags & SLAB_STORE_USER)
  			*dbg_userword(cachep, objp) = NULL;
  
@@@ -2894,26 -2728,15 +2944,29 @@@ static void *cache_free_debugcheck(stru
  	if (cachep->flags & SLAB_STORE_USER)
  		*dbg_userword(cachep, objp) = (void *)caller;
  
 -	objnr = obj_to_index(cachep, page, objp);
 +	objnr = obj_to_index(cachep, slabp, objp);
  
  	BUG_ON(objnr >= cachep->num);
 -	BUG_ON(objp != index_to_obj(cachep, page, objnr));
 +	BUG_ON(objp != index_to_obj(cachep, slabp, objnr));
  
 -	set_obj_status(page, objnr, OBJECT_FREE);
 +#ifdef CONFIG_DEBUG_SLAB_LEAK
 +	slab_bufctl(slabp)[objnr] = BUFCTL_FREE;
 +#endif
  	if (cachep->flags & SLAB_POISON) {
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_PAGEALLOC
 +		if ((cachep->size % PAGE_SIZE)==0 && OFF_SLAB(cachep)) {
 +			store_stackinfo(cachep, objp, caller);
 +			kernel_map_pages(virt_to_page(objp),
 +					 cachep->size / PAGE_SIZE, 0);
 +		} else {
 +			poison_obj(cachep, objp, POISON_FREE);
 +		}
 +#else
++=======
++>>>>>>> 40b44137971c (mm/slab: clean up DEBUG_PAGEALLOC processing code)
  		poison_obj(cachep, objp, POISON_FREE);
- #endif
+ 		slab_kernel_map(cachep, objp, 0, caller);
  	}
  	return objp;
  }
@@@ -3064,15 -2869,10 +3117,18 @@@ static void *cache_alloc_debugcheck_aft
  	if (!objp)
  		return objp;
  	if (cachep->flags & SLAB_POISON) {
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_PAGEALLOC
 +		if ((cachep->size % PAGE_SIZE) == 0 && OFF_SLAB(cachep))
 +			kernel_map_pages(virt_to_page(objp),
 +					 cachep->size / PAGE_SIZE, 1);
 +		else
 +			check_poison_obj(cachep, objp);
 +#else
++=======
++>>>>>>> 40b44137971c (mm/slab: clean up DEBUG_PAGEALLOC processing code)
  		check_poison_obj(cachep, objp);
- #endif
+ 		slab_kernel_map(cachep, objp, 1, 0);
  		poison_obj(cachep, objp, POISON_INUSE);
  	}
  	if (cachep->flags & SLAB_STORE_USER)
diff --git a/include/linux/mm.h b/include/linux/mm.h
index a0514d1e5d91..57bec228df9a 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2166,14 +2166,18 @@ static inline void vm_stat_account(struct mm_struct *mm,
 extern void kernel_map_pages(struct page *page, int numpages, int enable);
 #ifdef CONFIG_HIBERNATION
 extern bool kernel_page_present(struct page *page);
-#endif /* CONFIG_HIBERNATION */
-#else
+#endif	/* CONFIG_HIBERNATION */
+#else	/* CONFIG_DEBUG_PAGEALLOC */
 static inline void
 kernel_map_pages(struct page *page, int numpages, int enable) {}
 #ifdef CONFIG_HIBERNATION
 static inline bool kernel_page_present(struct page *page) { return true; }
-#endif /* CONFIG_HIBERNATION */
-#endif
+#endif	/* CONFIG_HIBERNATION */
+static inline bool debug_pagealloc_enabled(void)
+{
+	return false;
+}
+#endif	/* CONFIG_DEBUG_PAGEALLOC */
 
 extern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);
 #ifdef	__HAVE_ARCH_GATE_AREA
* Unmerged path mm/slab.c
