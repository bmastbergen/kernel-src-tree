ibmvnic: Clean RX pool buffers during device close

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Thomas Falcon <tlfalcon@linux.vnet.ibm.com>
commit d0869c0071e40c4407d1a4d7c9497653cf47253b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d0869c00.failed

During device close or reset, there were some cases of outstanding
RX socket buffers not being freed. Include a function similar to the
one that already exists to clean TX socket buffers in this case.

	Signed-off-by: Thomas Falcon <tlfalcon@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit d0869c0071e40c4407d1a4d7c9497653cf47253b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/ibm/ibmvnic.c
diff --cc drivers/net/ethernet/ibm/ibmvnic.c
index 2e03bb665f25,996f47568f9e..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.c
+++ b/drivers/net/ethernet/ibm/ibmvnic.c
@@@ -395,161 -1045,144 +395,230 @@@ static void free_rx_pool(struct ibmvnic
  static int ibmvnic_open(struct net_device *netdev)
  {
  	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
++<<<<<<< HEAD
 +	struct device *dev = &adapter->vdev->dev;
++=======
+ 	int rc;
+ 
+ 	mutex_lock(&adapter->reset_lock);
+ 
+ 	if (adapter->state != VNIC_CLOSED) {
+ 		rc = ibmvnic_login(netdev);
+ 		if (rc) {
+ 			mutex_unlock(&adapter->reset_lock);
+ 			return rc;
+ 		}
+ 
+ 		rc = init_resources(adapter);
+ 		if (rc) {
+ 			netdev_err(netdev, "failed to initialize resources\n");
+ 			release_resources(adapter);
+ 			mutex_unlock(&adapter->reset_lock);
+ 			return rc;
+ 		}
+ 	}
+ 
+ 	rc = __ibmvnic_open(netdev);
+ 	netif_carrier_on(netdev);
+ 
+ 	mutex_unlock(&adapter->reset_lock);
+ 
+ 	return rc;
+ }
+ 
+ static void clean_rx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_rx_pool *rx_pool;
+ 	u64 rx_entries;
+ 	int rx_scrqs;
+ 	int i, j;
+ 
+ 	if (!adapter->rx_pool)
+ 		return;
+ 
+ 	rx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	rx_entries = adapter->req_rx_add_entries_per_subcrq;
+ 
+ 	/* Free any remaining skbs in the rx buffer pools */
+ 	for (i = 0; i < rx_scrqs; i++) {
+ 		rx_pool = &adapter->rx_pool[i];
+ 		if (!rx_pool)
+ 			continue;
+ 
+ 		netdev_dbg(adapter->netdev, "Cleaning rx_pool[%d]\n", i);
+ 		for (j = 0; j < rx_entries; j++) {
+ 			if (rx_pool->rx_buff[j].skb) {
+ 				dev_kfree_skb_any(rx_pool->rx_buff[j].skb);
+ 				rx_pool->rx_buff[j].skb = NULL;
+ 			}
+ 		}
+ 	}
+ }
+ 
+ static void clean_tx_pools(struct ibmvnic_adapter *adapter)
+ {
++>>>>>>> d0869c0071e4 (ibmvnic: Clean RX pool buffers during device close)
  	struct ibmvnic_tx_pool *tx_pool;
 -	u64 tx_entries;
 -	int tx_scrqs;
 +	union ibmvnic_crq crq;
 +	int rxadd_subcrqs;
 +	u64 *size_array;
 +	int tx_subcrqs;
  	int i, j;
  
 -	if (!adapter->tx_pool)
 -		return;
 +	rxadd_subcrqs =
 +	    be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
 +	tx_subcrqs =
 +	    be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
 +	size_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +
 +				  be32_to_cpu(adapter->login_rsp_buf->
 +					      off_rxadd_buff_size));
 +	adapter->map_id = 1;
 +	adapter->napi = kcalloc(adapter->req_rx_queues,
 +				sizeof(struct napi_struct), GFP_KERNEL);
 +	if (!adapter->napi)
 +		goto alloc_napi_failed;
 +	for (i = 0; i < adapter->req_rx_queues; i++) {
 +		netif_napi_add(netdev, &adapter->napi[i], ibmvnic_poll,
 +			       NAPI_POLL_WEIGHT);
 +		napi_enable(&adapter->napi[i]);
 +	}
 +	adapter->rx_pool =
 +	    kcalloc(rxadd_subcrqs, sizeof(struct ibmvnic_rx_pool), GFP_KERNEL);
 +
 +	if (!adapter->rx_pool)
 +		goto rx_pool_arr_alloc_failed;
 +	send_map_query(adapter);
 +	for (i = 0; i < rxadd_subcrqs; i++) {
 +		init_rx_pool(adapter, &adapter->rx_pool[i],
 +			     adapter->req_rx_add_entries_per_subcrq, i,
 +			     be64_to_cpu(size_array[i]), 1);
 +		if (alloc_rx_pool(adapter, &adapter->rx_pool[i])) {
 +			dev_err(dev, "Couldn't alloc rx pool\n");
 +			goto rx_pool_alloc_failed;
 +		}
 +	}
 +	adapter->tx_pool =
 +	    kcalloc(tx_subcrqs, sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);
 +
 +	if (!adapter->tx_pool)
 +		goto tx_pool_arr_alloc_failed;
 +	for (i = 0; i < tx_subcrqs; i++) {
 +		tx_pool = &adapter->tx_pool[i];
 +		tx_pool->tx_buff =
 +		    kcalloc(adapter->req_tx_entries_per_subcrq,
 +			    sizeof(struct ibmvnic_tx_buff), GFP_KERNEL);
 +		if (!tx_pool->tx_buff)
 +			goto tx_pool_alloc_failed;
 +
 +		if (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,
 +					 adapter->req_tx_entries_per_subcrq *
 +					 adapter->req_mtu))
 +			goto tx_ltb_alloc_failed;
 +
 +		tx_pool->free_map =
 +		    kcalloc(adapter->req_tx_entries_per_subcrq,
 +			    sizeof(int), GFP_KERNEL);
 +		if (!tx_pool->free_map)
 +			goto tx_fm_alloc_failed;
 +
 +		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
 +			tx_pool->free_map[j] = j;
 +
 +		tx_pool->consumer_index = 0;
 +		tx_pool->producer_index = 0;
 +	}
 +	adapter->bounce_buffer_size =
 +	    (netdev->mtu + ETH_HLEN - 1) / PAGE_SIZE + 1;
 +	adapter->bounce_buffer = kmalloc(adapter->bounce_buffer_size,
 +					 GFP_KERNEL);
 +	if (!adapter->bounce_buffer)
 +		goto bounce_alloc_failed;
 +
 +	adapter->bounce_buffer_dma = dma_map_single(dev, adapter->bounce_buffer,
 +						    adapter->bounce_buffer_size,
 +						    DMA_TO_DEVICE);
 +	if (dma_mapping_error(dev, adapter->bounce_buffer_dma)) {
 +		dev_err(dev, "Couldn't map tx bounce buffer\n");
 +		goto bounce_map_failed;
 +	}
 +	replenish_pools(adapter);
 +
 +	/* We're ready to receive frames, enable the sub-crq interrupts and
 +	 * set the logical link state to up
 +	 */
 +	for (i = 0; i < adapter->req_rx_queues; i++)
 +		enable_scrq_irq(adapter, adapter->rx_scrq[i]);
 +
 +	for (i = 0; i < adapter->req_tx_queues; i++)
 +		enable_scrq_irq(adapter, adapter->tx_scrq[i]);
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.logical_link_state.first = IBMVNIC_CRQ_CMD;
 +	crq.logical_link_state.cmd = LOGICAL_LINK_STATE;
 +	crq.logical_link_state.link_state = IBMVNIC_LOGICAL_LNK_UP;
 +	ibmvnic_send_crq(adapter, &crq);
  
 -	tx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
 -	tx_entries = adapter->req_tx_entries_per_subcrq;
 +	netif_tx_start_all_queues(netdev);
  
 -	/* Free any remaining skbs in the tx buffer pools */
 -	for (i = 0; i < tx_scrqs; i++) {
 -		tx_pool = &adapter->tx_pool[i];
 -		if (!tx_pool)
 -			continue;
 +	return 0;
  
 -		netdev_dbg(adapter->netdev, "Cleaning tx_pool[%d]\n", i);
 -		for (j = 0; j < tx_entries; j++) {
 -			if (tx_pool->tx_buff[j].skb) {
 -				dev_kfree_skb_any(tx_pool->tx_buff[j].skb);
 -				tx_pool->tx_buff[j].skb = NULL;
 -			}
 -		}
 +bounce_map_failed:
 +	kfree(adapter->bounce_buffer);
 +bounce_alloc_failed:
 +	i = tx_subcrqs - 1;
 +	kfree(adapter->tx_pool[i].free_map);
 +tx_fm_alloc_failed:
 +	free_long_term_buff(adapter, &adapter->tx_pool[i].long_term_buff);
 +tx_ltb_alloc_failed:
 +	kfree(adapter->tx_pool[i].tx_buff);
 +tx_pool_alloc_failed:
 +	for (j = 0; j < i; j++) {
 +		kfree(adapter->tx_pool[j].tx_buff);
 +		free_long_term_buff(adapter,
 +				    &adapter->tx_pool[j].long_term_buff);
 +		kfree(adapter->tx_pool[j].free_map);
  	}
 +	kfree(adapter->tx_pool);
 +	adapter->tx_pool = NULL;
 +tx_pool_arr_alloc_failed:
 +	i = rxadd_subcrqs;
 +rx_pool_alloc_failed:
 +	for (j = 0; j < i; j++) {
 +		free_rx_pool(adapter, &adapter->rx_pool[j]);
 +		free_long_term_buff(adapter,
 +				    &adapter->rx_pool[j].long_term_buff);
 +	}
 +	kfree(adapter->rx_pool);
 +	adapter->rx_pool = NULL;
 +rx_pool_arr_alloc_failed:
 +	for (i = 0; i < adapter->req_rx_queues; i++)
 +		napi_disable(&adapter->napi[i]);
 +alloc_napi_failed:
 +	return -ENOMEM;
  }
  
 -static int __ibmvnic_close(struct net_device *netdev)
 +static void disable_sub_crqs(struct ibmvnic_adapter *adapter)
  {
 -	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 -	int rc = 0;
  	int i;
  
 -	adapter->state = VNIC_CLOSING;
 -
 -	/* ensure that transmissions are stopped if called by do_reset */
 -	if (adapter->resetting)
 -		netif_tx_disable(netdev);
 -	else
 -		netif_tx_stop_all_queues(netdev);
 -
 -	ibmvnic_napi_disable(adapter);
 -
  	if (adapter->tx_scrq) {
  		for (i = 0; i < adapter->req_tx_queues; i++)
 -			if (adapter->tx_scrq[i]->irq) {
 -				netdev_dbg(adapter->netdev,
 -					   "Disabling tx_scrq[%d] irq\n", i);
 +			if (adapter->tx_scrq[i])
  				disable_irq(adapter->tx_scrq[i]->irq);
 -			}
  	}
  
 -	rc = set_link_state(adapter, IBMVNIC_LOGICAL_LNK_DN);
 -	if (rc)
 -		return rc;
 -
  	if (adapter->rx_scrq) {
 -		for (i = 0; i < adapter->req_rx_queues; i++) {
 -			int retries = 10;
 -
 -			while (pending_scrq(adapter, adapter->rx_scrq[i])) {
 -				retries--;
 -				mdelay(100);
 -
 -				if (retries == 0)
 -					break;
 -			}
 -
 -			if (adapter->rx_scrq[i]->irq) {
 -				netdev_dbg(adapter->netdev,
 -					   "Disabling rx_scrq[%d] irq\n", i);
 +		for (i = 0; i < adapter->req_rx_queues; i++)
 +			if (adapter->rx_scrq[i])
  				disable_irq(adapter->rx_scrq[i]->irq);
 -			}
 -		}
  	}
++<<<<<<< HEAD
++=======
+ 	clean_rx_pools(adapter);
+ 	clean_tx_pools(adapter);
+ 	adapter->state = VNIC_CLOSED;
+ 	return rc;
++>>>>>>> d0869c0071e4 (ibmvnic: Clean RX pool buffers during device close)
  }
  
  static int ibmvnic_close(struct net_device *netdev)
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.c
