mm/core: Do not enforce PKEY permissions on remote mm access

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] core: Do not enforce PKEY permissions on remote mm access (Rui Wang) [1272615]
Rebuild_FUZZ: 97.44%
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit 1b2ee1266ea647713dbaf44825967c180dfc8d76
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1b2ee126.failed

We try to enforce protection keys in software the same way that we
do in hardware.  (See long example below).

But, we only want to do this when accessing our *own* process's
memory.  If GDB set PKRU[6].AD=1 (disable access to PKEY 6), then
tried to PTRACE_POKE a target process which just happened to have
some mprotect_pkey(pkey=6) memory, we do *not* want to deny the
debugger access to that memory.  PKRU is fundamentally a
thread-local structure and we do not want to enforce it on access
to _another_ thread's data.

This gets especially tricky when we have workqueues or other
delayed-work mechanisms that might run in a random process's context.
We can check that we only enforce pkeys when operating on our *own* mm,
but delayed work gets performed when a random user context is active.
We might end up with a situation where a delayed-work gup fails when
running randomly under its "own" task but succeeds when running under
another process.  We want to avoid that.

To avoid that, we use the new GUP flag: FOLL_REMOTE and add a
fault flag: FAULT_FLAG_REMOTE.  They indicate that we are
walking an mm which is not guranteed to be the same as
current->mm and should not be subject to protection key
enforcement.

Thanks to Jerome Glisse for pointing out this scenario.

	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Alexey Kardashevskiy <aik@ozlabs.ru>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Boaz Harrosh <boaz@plexistor.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Chinner <dchinner@redhat.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: David Gibson <david@gibson.dropbear.id.au>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
	Cc: Dominik Vogt <vogt@linux.vnet.ibm.com>
	Cc: Eric B Munson <emunson@akamai.com>
	Cc: Geliang Tang <geliangtang@163.com>
	Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jan Kara <jack@suse.cz>
	Cc: Jason Low <jason.low2@hp.com>
	Cc: Jerome Marchand <jmarchan@redhat.com>
	Cc: Joerg Roedel <joro@8bytes.org>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Konstantin Khlebnikov <koct9i@gmail.com>
	Cc: Laurent Dufour <ldufour@linux.vnet.ibm.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Matthew Wilcox <willy@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Mikulas Patocka <mpatocka@redhat.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Cc: Shachar Raindel <raindel@mellanox.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Xie XiuQi <xiexiuqi@huawei.com>
	Cc: iommu@lists.linux-foundation.org
	Cc: linux-arch@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Cc: linux-mm@kvack.org
	Cc: linux-s390@vger.kernel.org
	Cc: linuxppc-dev@lists.ozlabs.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 1b2ee1266ea647713dbaf44825967c180dfc8d76)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/mmu_context.h
#	arch/s390/include/asm/mmu_context.h
#	arch/unicore32/include/asm/mmu_context.h
#	arch/x86/include/asm/mmu_context.h
#	drivers/iommu/amd_iommu_v2.c
#	include/asm-generic/mm_hooks.h
#	include/linux/mm.h
#	mm/gup.c
#	mm/memory.c
diff --cc arch/powerpc/include/asm/mmu_context.h
index b2294769341b,df9bf3ed025b..000000000000
--- a/arch/powerpc/include/asm/mmu_context.h
+++ b/arch/powerpc/include/asm/mmu_context.h
@@@ -165,5 -148,17 +165,20 @@@ static inline void arch_bprm_mm_init(st
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool foreign)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  #endif /* __KERNEL__ */
  #endif /* __ASM_POWERPC_MMU_CONTEXT_H */
diff --cc arch/s390/include/asm/mmu_context.h
index eccdb1327fc0,8906600922ce..000000000000
--- a/arch/s390/include/asm/mmu_context.h
+++ b/arch/s390/include/asm/mmu_context.h
@@@ -108,5 -130,16 +108,14 @@@ static inline void arch_bprm_mm_init(st
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool foreign)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  
 -static inline bool arch_pte_access_permitted(pte_t pte, bool write)
 -{
 -	/* by default, allow everything */
 -	return true;
 -}
  #endif /* __S390_MMU_CONTEXT_H */
diff --cc arch/unicore32/include/asm/mmu_context.h
index fb5e4c658f7a,e35632ef23c7..000000000000
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@@ -84,4 -86,27 +84,30 @@@ static inline void arch_dup_mmap(struc
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline void arch_unmap(struct mm_struct *mm,
+ 			struct vm_area_struct *vma,
+ 			unsigned long start, unsigned long end)
+ {
+ }
+ 
+ static inline void arch_bprm_mm_init(struct mm_struct *mm,
+ 				     struct vm_area_struct *vma)
+ {
+ }
+ 
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool foreign)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  #endif
diff --cc arch/x86/include/asm/mmu_context.h
index a34c7d411865,b4d939a17e60..000000000000
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@@ -150,7 -254,86 +150,90 @@@ static inline void arch_bprm_mm_init(st
  static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
  			      unsigned long start, unsigned long end)
  {
++<<<<<<< HEAD
 +	mpx_notify_unmap(mm, vma, start, end);
++=======
+ 	/*
+ 	 * mpx_notify_unmap() goes and reads a rarely-hot
+ 	 * cacheline in the mm_struct.  That can be expensive
+ 	 * enough to be seen in profiles.
+ 	 *
+ 	 * The mpx_notify_unmap() call and its contents have been
+ 	 * observed to affect munmap() performance on hardware
+ 	 * where MPX is not present.
+ 	 *
+ 	 * The unlikely() optimizes for the fast case: no MPX
+ 	 * in the CPU, or no MPX use in the process.  Even if
+ 	 * we get this wrong (in the unlikely event that MPX
+ 	 * is widely enabled on some system) the overhead of
+ 	 * MPX itself (reading bounds tables) is expected to
+ 	 * overwhelm the overhead of getting this unlikely()
+ 	 * consistently wrong.
+ 	 */
+ 	if (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))
+ 		mpx_notify_unmap(mm, vma, start, end);
+ }
+ 
+ static inline int vma_pkey(struct vm_area_struct *vma)
+ {
+ 	u16 pkey = 0;
+ #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
+ 	unsigned long vma_pkey_mask = VM_PKEY_BIT0 | VM_PKEY_BIT1 |
+ 				      VM_PKEY_BIT2 | VM_PKEY_BIT3;
+ 	pkey = (vma->vm_flags & vma_pkey_mask) >> VM_PKEY_SHIFT;
+ #endif
+ 	return pkey;
+ }
+ 
+ static inline bool __pkru_allows_pkey(u16 pkey, bool write)
+ {
+ 	u32 pkru = read_pkru();
+ 
+ 	if (!__pkru_allows_read(pkru, pkey))
+ 		return false;
+ 	if (write && !__pkru_allows_write(pkru, pkey))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * We only want to enforce protection keys on the current process
+  * because we effectively have no access to PKRU for other
+  * processes or any way to tell *which * PKRU in a threaded
+  * process we could use.
+  *
+  * So do not enforce things if the VMA is not from the current
+  * mm, or if we are in a kernel thread.
+  */
+ static inline bool vma_is_foreign(struct vm_area_struct *vma)
+ {
+ 	if (!current->mm)
+ 		return true;
+ 	/*
+ 	 * Should PKRU be enforced on the access to this VMA?  If
+ 	 * the VMA is from another process, then PKRU has no
+ 	 * relevance and should not be enforced.
+ 	 */
+ 	if (current->mm != vma->vm_mm)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool foreign)
+ {
+ 	/* allow access if the VMA is not one from this process */
+ 	if (foreign || vma_is_foreign(vma))
+ 		return true;
+ 	return __pkru_allows_pkey(vma_pkey(vma), write);
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	return __pkru_allows_pkey(pte_flags_pkey(pte_flags(pte)), write);
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  }
  
  #endif /* _ASM_X86_MMU_CONTEXT_H */
diff --cc drivers/iommu/amd_iommu_v2.c
index 80e04d5cacfc,56999d2fac07..000000000000
--- a/drivers/iommu/amd_iommu_v2.c
+++ b/drivers/iommu/amd_iommu_v2.c
@@@ -506,26 -522,31 +506,35 @@@ static void do_fault(struct work_struc
  	mm = fault->state->mm;
  	address = fault->address;
  
++<<<<<<< HEAD
++=======
+ 	if (fault->flags & PPR_FAULT_USER)
+ 		flags |= FAULT_FLAG_USER;
+ 	if (fault->flags & PPR_FAULT_WRITE)
+ 		flags |= FAULT_FLAG_WRITE;
+ 	flags |= FAULT_FLAG_REMOTE;
+ 
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  	down_read(&mm->mmap_sem);
  	vma = find_extend_vma(mm, address);
 -	if (!vma || address < vma->vm_start)
 +	if (!vma || address < vma->vm_start) {
  		/* failed to get a vma in the right range */
 +		up_read(&mm->mmap_sem);
 +		handle_fault_error(fault);
  		goto out;
 +	}
  
 -	/* Check if we have the right permissions on the vma */
 -	if (access_error(vma, fault))
 +	ret = handle_mm_fault(mm, vma, address, write);
 +	if (ret & VM_FAULT_ERROR) {
 +		/* failed to service fault */
 +		up_read(&mm->mmap_sem);
 +		handle_fault_error(fault);
  		goto out;
 +	}
  
 -	ret = handle_mm_fault(mm, vma, address, flags);
 -
 -out:
  	up_read(&mm->mmap_sem);
  
 -	if (ret & VM_FAULT_ERROR)
 -		/* failed to service fault */
 -		handle_fault_error(fault);
 -
 +out:
  	finish_pri_tag(fault->dev_state, fault->state, fault->tag);
  
  	put_pasid_state(fault->state);
diff --cc include/asm-generic/mm_hooks.h
index 866aa461efa5,d5c9633bd955..000000000000
--- a/include/asm-generic/mm_hooks.h
+++ b/include/asm-generic/mm_hooks.h
@@@ -26,4 -26,16 +26,19 @@@ static inline void arch_bprm_mm_init(st
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool foreign)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  #endif	/* _ASM_GENERIC_MM_HOOKS_H */
diff --cc include/linux/mm.h
index a0514d1e5d91,2aaa0f0d67ea..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -218,13 -245,13 +218,23 @@@ extern unsigned int kobjsize(const voi
  extern pgprot_t protection_map[16];
  
  #define FAULT_FLAG_WRITE	0x01	/* Fault was a write access */
++<<<<<<< HEAD
 +#define FAULT_FLAG_NONLINEAR	0x02	/* Fault was via a nonlinear mapping */
 +#define FAULT_FLAG_MKWRITE	0x04	/* Fault was mkwrite of existing pte */
 +#define FAULT_FLAG_ALLOW_RETRY	0x08	/* Retry fault if blocking */
 +#define FAULT_FLAG_RETRY_NOWAIT	0x10	/* Don't drop mmap_sem and wait when retrying */
 +#define FAULT_FLAG_KILLABLE	0x20	/* The fault task is in SIGKILL killable region */
 +#define FAULT_FLAG_TRIED	0x40	/* second try */
 +#define FAULT_FLAG_USER		0x80	/* The fault originated in userspace */
++=======
+ #define FAULT_FLAG_MKWRITE	0x02	/* Fault was mkwrite of existing pte */
+ #define FAULT_FLAG_ALLOW_RETRY	0x04	/* Retry fault if blocking */
+ #define FAULT_FLAG_RETRY_NOWAIT	0x08	/* Don't drop mmap_sem and wait when retrying */
+ #define FAULT_FLAG_KILLABLE	0x10	/* The fault task is in SIGKILL killable region */
+ #define FAULT_FLAG_TRIED	0x20	/* Second try */
+ #define FAULT_FLAG_USER		0x40	/* The fault originated in userspace */
+ #define FAULT_FLAG_REMOTE	0x80	/* faulting for non current tsk/mm */
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  
  /*
   * vm_fault is filled by the the pagefault handler and passed to the vma's
diff --cc mm/gup.c
index 3166366affd5,d276760163b3..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -254,21 -195,263 +254,240 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -/**
 - * follow_page_mask - look up a page descriptor from a user-virtual address
 - * @vma: vm_area_struct mapping @address
 - * @address: virtual address to look up
 - * @flags: flags modifying lookup behaviour
 - * @page_mask: on output, *page_mask is set according to the size of the page
 - *
 - * @flags can have FOLL_ flags set, defined in <linux/mm.h>
 - *
 - * Returns the mapped (struct page *), %NULL if no mapping exists, or
 - * an error pointer if there is a mapping to something not represented
 - * by a page descriptor (see also vm_normal_page()).
 - */
 -struct page *follow_page_mask(struct vm_area_struct *vma,
 -			      unsigned long address, unsigned int flags,
 -			      unsigned int *page_mask)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	*page_mask = 0;
 -
 -	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 -	if (!IS_ERR(page)) {
 -		BUG_ON(flags & FOLL_GET);
  		return page;
++<<<<<<< HEAD
++=======
+ 	}
+ 
+ 	pgd = pgd_offset(mm, address);
+ 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pud = pud_offset(pgd, address);
+ 	if (pud_none(*pud))
+ 		return no_page_table(vma, flags);
+ 	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pud(mm, address, pud, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (unlikely(pud_bad(*pud)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pmd(mm, address, pmd, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_devmap(*pmd)) {
+ 		ptl = pmd_lock(mm, pmd);
+ 		page = follow_devmap_pmd(vma, address, pmd, flags);
+ 		spin_unlock(ptl);
+ 		if (page)
+ 			return page;
+ 	}
+ 	if (likely(!pmd_trans_huge(*pmd)))
+ 		return follow_page_pte(vma, address, pmd, flags);
+ 
+ 	ptl = pmd_lock(mm, pmd);
+ 	if (unlikely(!pmd_trans_huge(*pmd))) {
+ 		spin_unlock(ptl);
+ 		return follow_page_pte(vma, address, pmd, flags);
+ 	}
+ 	if (flags & FOLL_SPLIT) {
+ 		int ret;
+ 		page = pmd_page(*pmd);
+ 		if (is_huge_zero_page(page)) {
+ 			spin_unlock(ptl);
+ 			ret = 0;
+ 			split_huge_pmd(vma, pmd, address);
+ 		} else {
+ 			get_page(page);
+ 			spin_unlock(ptl);
+ 			lock_page(page);
+ 			ret = split_huge_page(page);
+ 			unlock_page(page);
+ 			put_page(page);
+ 		}
+ 
+ 		return ret ? ERR_PTR(ret) :
+ 			follow_page_pte(vma, address, pmd, flags);
+ 	}
+ 
+ 	page = follow_trans_huge_pmd(vma, address, pmd, flags);
+ 	spin_unlock(ptl);
+ 	*page_mask = HPAGE_PMD_NR - 1;
+ 	return page;
+ }
+ 
+ static int get_gate_page(struct mm_struct *mm, unsigned long address,
+ 		unsigned int gup_flags, struct vm_area_struct **vma,
+ 		struct page **page)
+ {
+ 	pgd_t *pgd;
+ 	pud_t *pud;
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 	int ret = -EFAULT;
+ 
+ 	/* user gate pages are read-only */
+ 	if (gup_flags & FOLL_WRITE)
+ 		return -EFAULT;
+ 	if (address > TASK_SIZE)
+ 		pgd = pgd_offset_k(address);
+ 	else
+ 		pgd = pgd_offset_gate(mm, address);
+ 	BUG_ON(pgd_none(*pgd));
+ 	pud = pud_offset(pgd, address);
+ 	BUG_ON(pud_none(*pud));
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return -EFAULT;
+ 	VM_BUG_ON(pmd_trans_huge(*pmd));
+ 	pte = pte_offset_map(pmd, address);
+ 	if (pte_none(*pte))
+ 		goto unmap;
+ 	*vma = get_gate_vma(mm);
+ 	if (!page)
+ 		goto out;
+ 	*page = vm_normal_page(*vma, address, *pte);
+ 	if (!*page) {
+ 		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
+ 			goto unmap;
+ 		*page = pte_page(*pte);
+ 	}
+ 	get_page(*page);
+ out:
+ 	ret = 0;
+ unmap:
+ 	pte_unmap(pte);
+ 	return ret;
+ }
+ 
+ /*
+  * mmap_sem must be held on entry.  If @nonblocking != NULL and
+  * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.
+  * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
+  */
+ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
+ 		unsigned long address, unsigned int *flags, int *nonblocking)
+ {
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned int fault_flags = 0;
+ 	int ret;
+ 
+ 	/* mlock all present pages, but do not fault in new pages */
+ 	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
+ 		return -ENOENT;
+ 	/* For mm_populate(), just skip the stack guard page. */
+ 	if ((*flags & FOLL_POPULATE) &&
+ 			(stack_guard_page_start(vma, address) ||
+ 			 stack_guard_page_end(vma, address + PAGE_SIZE)))
+ 		return -ENOENT;
+ 	if (*flags & FOLL_WRITE)
+ 		fault_flags |= FAULT_FLAG_WRITE;
+ 	if (*flags & FOLL_REMOTE)
+ 		fault_flags |= FAULT_FLAG_REMOTE;
+ 	if (nonblocking)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
+ 	if (*flags & FOLL_NOWAIT)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
+ 	if (*flags & FOLL_TRIED) {
+ 		VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
+ 		fault_flags |= FAULT_FLAG_TRIED;
+ 	}
+ 
+ 	ret = handle_mm_fault(mm, vma, address, fault_flags);
+ 	if (ret & VM_FAULT_ERROR) {
+ 		if (ret & VM_FAULT_OOM)
+ 			return -ENOMEM;
+ 		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
+ 			return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;
+ 		if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
+ 			return -EFAULT;
+ 		BUG();
+ 	}
+ 
+ 	if (tsk) {
+ 		if (ret & VM_FAULT_MAJOR)
+ 			tsk->maj_flt++;
+ 		else
+ 			tsk->min_flt++;
+ 	}
+ 
+ 	if (ret & VM_FAULT_RETRY) {
+ 		if (nonblocking)
+ 			*nonblocking = 0;
+ 		return -EBUSY;
+ 	}
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  
 +no_page_table:
  	/*
 -	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
 -	 * necessary, even if maybe_mkwrite decided not to set pte_write. We
 -	 * can thus safely do subsequent page lookups as if they were reads.
 -	 * But only do so when looping for pte_write is futile: in some cases
 -	 * userspace may also be wanting to write to the gotten user page,
 -	 * which a read fault here might prevent (a readonly page might get
 -	 * reCOWed by userspace write).
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
  	 */
++<<<<<<< HEAD
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
 +	return page;
++=======
+ 	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
+ 		*flags &= ~FOLL_WRITE;
+ 	return 0;
+ }
+ 
+ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
+ {
+ 	vm_flags_t vm_flags = vma->vm_flags;
+ 	int write = (gup_flags & FOLL_WRITE);
+ 	int foreign = (gup_flags & FOLL_REMOTE);
+ 
+ 	if (vm_flags & (VM_IO | VM_PFNMAP))
+ 		return -EFAULT;
+ 
+ 	if (write) {
+ 		if (!(vm_flags & VM_WRITE)) {
+ 			if (!(gup_flags & FOLL_FORCE))
+ 				return -EFAULT;
+ 			/*
+ 			 * We used to let the write,force case do COW in a
+ 			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
+ 			 * set a breakpoint in a read-only mapping of an
+ 			 * executable, without corrupting the file (yet only
+ 			 * when that file had been opened for writing!).
+ 			 * Anon pages in shared mappings are surprising: now
+ 			 * just reject it.
+ 			 */
+ 			if (!is_cow_mapping(vm_flags))
+ 				return -EFAULT;
+ 		}
+ 	} else if (!(vm_flags & VM_READ)) {
+ 		if (!(gup_flags & FOLL_FORCE))
+ 			return -EFAULT;
+ 		/*
+ 		 * Is there actually any vma we can reach here which does not
+ 		 * have VM_MAYREAD set?
+ 		 */
+ 		if (!(vm_flags & VM_MAYREAD))
+ 			return -EFAULT;
+ 	}
+ 	if (!arch_vma_access_permitted(vma, write, foreign))
+ 		return -EFAULT;
+ 	return 0;
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  }
  
  /**
@@@ -535,6 -617,25 +754,28 @@@ next_page
  }
  EXPORT_SYMBOL(__get_user_pages);
  
++<<<<<<< HEAD
++=======
+ bool vma_permits_fault(struct vm_area_struct *vma, unsigned int fault_flags)
+ {
+ 	bool write   = !!(fault_flags & FAULT_FLAG_WRITE);
+ 	bool foreign = !!(fault_flags & FAULT_FLAG_REMOTE);
+ 	vm_flags_t vm_flags = write ? VM_WRITE : VM_READ;
+ 
+ 	if (!(vm_flags & vma->vm_flags))
+ 		return false;
+ 
+ 	/*
+ 	 * The architecture might have a hardware protection
+ 	 * mechanism other than read/write that can deny access.
+ 	 */
+ 	if (!arch_vma_access_permitted(vma, write, foreign))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  /*
   * fixup_user_fault() - manually resolve a user page fault
   * @tsk:	the task_struct to use for page fault accounting, or
diff --cc mm/memory.c
index 2fc5b28b6782,76c44e5dffa2..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3272,6 -3379,10 +3272,13 @@@ static int __handle_mm_fault(struct mm_
  	pmd_t *pmd;
  	pte_t *pte;
  
++<<<<<<< HEAD
++=======
+ 	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
+ 					    flags & FAULT_FLAG_REMOTE))
+ 		return VM_FAULT_SIGSEGV;
+ 
++>>>>>>> 1b2ee1266ea6 (mm/core: Do not enforce PKEY permissions on remote mm access)
  	if (unlikely(is_vm_hugetlb_page(vma)))
  		return hugetlb_fault(mm, vma, address, flags);
  
* Unmerged path arch/powerpc/include/asm/mmu_context.h
* Unmerged path arch/s390/include/asm/mmu_context.h
* Unmerged path arch/unicore32/include/asm/mmu_context.h
* Unmerged path arch/x86/include/asm/mmu_context.h
* Unmerged path drivers/iommu/amd_iommu_v2.c
* Unmerged path include/asm-generic/mm_hooks.h
* Unmerged path include/linux/mm.h
* Unmerged path mm/gup.c
diff --git a/mm/ksm.c b/mm/ksm.c
index feae14491122..6ba13a9e3959 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -429,6 +429,10 @@ static inline bool ksm_test_exit(struct mm_struct *mm)
  * in case the application has unmapped and remapped mm,addr meanwhile.
  * Could a ksm page appear anywhere else?  Actually yes, in a VM_PFNMAP
  * mmap of /dev/mem or /dev/kmem, where we would not want to touch it.
+ *
+ * FAULT_FLAG/FOLL_REMOTE are because we do this outside the context
+ * of the process that owns 'vma'.  We also do not want to enforce
+ * protection keys here anyway.
  */
 static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 {
@@ -437,12 +441,14 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 
 	do {
 		cond_resched();
-		page = follow_page(vma, addr, FOLL_GET | FOLL_MIGRATION);
+		page = follow_page(vma, addr,
+				FOLL_GET | FOLL_MIGRATION | FOLL_REMOTE);
 		if (IS_ERR_OR_NULL(page))
 			break;
 		if (PageKsm(page))
 			ret = handle_mm_fault(vma->vm_mm, vma, addr,
-							FAULT_FLAG_WRITE);
+							FAULT_FLAG_WRITE |
+							FAULT_FLAG_REMOTE);
 		else
 			ret = VM_FAULT_WRITE;
 		put_page(page);
* Unmerged path mm/memory.c
