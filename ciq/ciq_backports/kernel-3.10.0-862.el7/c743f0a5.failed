sched/fair, cpumask: Export for_each_cpu_wrap()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit c743f0a5c50f2fcbc628526279cfa24f3dabe182
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c743f0a5.failed

More users for for_each_cpu_wrap() have appeared. Promote the construct
to generic cpumask interface.

The implementation is slightly modified to reduce arguments.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Lauro Ramos Venancio <lvenanci@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: lwang@redhat.com
Link: http://lkml.kernel.org/r/20170414122005.o35me2h5nowqkxbv@hirez.programming.kicks-ass.net
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit c743f0a5c50f2fcbc628526279cfa24f3dabe182)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 38afc41c3538,f80c825e2b43..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -4419,17 -5637,183 +4419,191 @@@ find_idlest_cpu(struct sched_group *gro
  		}
  	}
  
 -	return shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;
 +	return idlest;
  }
  
++<<<<<<< HEAD
 +/*
 + * Try and locate an idle CPU in the sched_domain.
 + */
 +static int select_idle_sibling(struct task_struct *p, int target)
++=======
+ #ifdef CONFIG_SCHED_SMT
+ 
+ static inline void set_idle_cores(int cpu, int val)
+ {
+ 	struct sched_domain_shared *sds;
+ 
+ 	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
+ 	if (sds)
+ 		WRITE_ONCE(sds->has_idle_cores, val);
+ }
+ 
+ static inline bool test_idle_cores(int cpu, bool def)
+ {
+ 	struct sched_domain_shared *sds;
+ 
+ 	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
+ 	if (sds)
+ 		return READ_ONCE(sds->has_idle_cores);
+ 
+ 	return def;
+ }
+ 
+ /*
+  * Scans the local SMT mask to see if the entire core is idle, and records this
+  * information in sd_llc_shared->has_idle_cores.
+  *
+  * Since SMT siblings share all cache levels, inspecting this limited remote
+  * state should be fairly cheap.
+  */
+ void __update_idle_core(struct rq *rq)
+ {
+ 	int core = cpu_of(rq);
+ 	int cpu;
+ 
+ 	rcu_read_lock();
+ 	if (test_idle_cores(core, true))
+ 		goto unlock;
+ 
+ 	for_each_cpu(cpu, cpu_smt_mask(core)) {
+ 		if (cpu == core)
+ 			continue;
+ 
+ 		if (!idle_cpu(cpu))
+ 			goto unlock;
+ 	}
+ 
+ 	set_idle_cores(core, 1);
+ unlock:
+ 	rcu_read_unlock();
+ }
+ 
+ /*
+  * Scan the entire LLC domain for idle cores; this dynamically switches off if
+  * there are no idle cores left in the system; tracked through
+  * sd_llc->shared->has_idle_cores and enabled through update_idle_core() above.
+  */
+ static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)
+ {
+ 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);
+ 	int core, cpu;
+ 
+ 	if (!static_branch_likely(&sched_smt_present))
+ 		return -1;
+ 
+ 	if (!test_idle_cores(target, false))
+ 		return -1;
+ 
+ 	cpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);
+ 
+ 	for_each_cpu_wrap(core, cpus, target) {
+ 		bool idle = true;
+ 
+ 		for_each_cpu(cpu, cpu_smt_mask(core)) {
+ 			cpumask_clear_cpu(cpu, cpus);
+ 			if (!idle_cpu(cpu))
+ 				idle = false;
+ 		}
+ 
+ 		if (idle)
+ 			return core;
+ 	}
+ 
+ 	/*
+ 	 * Failed to find an idle core; stop looking for one.
+ 	 */
+ 	set_idle_cores(target, 0);
+ 
+ 	return -1;
+ }
+ 
+ /*
+  * Scan the local SMT mask for idle CPUs.
+  */
+ static int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)
+ {
+ 	int cpu;
+ 
+ 	if (!static_branch_likely(&sched_smt_present))
+ 		return -1;
+ 
+ 	for_each_cpu(cpu, cpu_smt_mask(target)) {
+ 		if (!cpumask_test_cpu(cpu, &p->cpus_allowed))
+ 			continue;
+ 		if (idle_cpu(cpu))
+ 			return cpu;
+ 	}
+ 
+ 	return -1;
+ }
+ 
+ #else /* CONFIG_SCHED_SMT */
+ 
+ static inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)
+ {
+ 	return -1;
+ }
+ 
+ static inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)
+ {
+ 	return -1;
+ }
+ 
+ #endif /* CONFIG_SCHED_SMT */
+ 
+ /*
+  * Scan the LLC domain for idle CPUs; this is dynamically regulated by
+  * comparing the average scan cost (tracked in sd->avg_scan_cost) against the
+  * average idle time for this rq (as found in rq->avg_idle).
+  */
+ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)
+ {
+ 	struct sched_domain *this_sd;
+ 	u64 avg_cost, avg_idle = this_rq()->avg_idle;
+ 	u64 time, cost;
+ 	s64 delta;
+ 	int cpu;
+ 
+ 	this_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));
+ 	if (!this_sd)
+ 		return -1;
+ 
+ 	avg_cost = this_sd->avg_scan_cost;
+ 
+ 	/*
+ 	 * Due to large variance we need a large fuzz factor; hackbench in
+ 	 * particularly is sensitive here.
+ 	 */
+ 	if (sched_feat(SIS_AVG_CPU) && (avg_idle / 512) < avg_cost)
+ 		return -1;
+ 
+ 	time = local_clock();
+ 
+ 	for_each_cpu_wrap(cpu, sched_domain_span(sd), target) {
+ 		if (!cpumask_test_cpu(cpu, &p->cpus_allowed))
+ 			continue;
+ 		if (idle_cpu(cpu))
+ 			break;
+ 	}
+ 
+ 	time = local_clock() - time;
+ 	cost = this_sd->avg_scan_cost;
+ 	delta = (s64)(time - cost) / 8;
+ 	this_sd->avg_scan_cost += delta;
+ 
+ 	return cpu;
+ }
+ 
+ /*
+  * Try and locate an idle core/thread in the LLC cache domain.
+  */
+ static int select_idle_sibling(struct task_struct *p, int prev, int target)
++>>>>>>> c743f0a5c50f (sched/fair, cpumask: Export for_each_cpu_wrap())
  {
  	struct sched_domain *sd;
 -	int i;
 +	struct sched_group *sg;
 +	int i = task_cpu(p);
  
  	if (idle_cpu(target))
  		return target;
diff --git a/include/linux/cpumask.h b/include/linux/cpumask.h
index ca6c07782bfa..14b49721096a 100644
--- a/include/linux/cpumask.h
+++ b/include/linux/cpumask.h
@@ -231,6 +231,23 @@ unsigned int cpumask_local_spread(unsigned int i, int node);
 		(cpu) = cpumask_next_zero((cpu), (mask)),	\
 		(cpu) < nr_cpu_ids;)
 
+extern int cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap);
+
+/**
+ * for_each_cpu_wrap - iterate over every cpu in a mask, starting at a specified location
+ * @cpu: the (optionally unsigned) integer iterator
+ * @mask: the cpumask poiter
+ * @start: the start location
+ *
+ * The implementation does not assume any bit in @mask is set (including @start).
+ *
+ * After the loop, cpu is >= nr_cpu_ids.
+ */
+#define for_each_cpu_wrap(cpu, mask, start)					\
+	for ((cpu) = cpumask_next_wrap((start)-1, (mask), (start), false);	\
+	     (cpu) < nr_cpumask_bits;						\
+	     (cpu) = cpumask_next_wrap((cpu), (mask), (start), true))
+
 /**
  * for_each_cpu_and - iterate over every cpu in both masks
  * @cpu: the (optionally unsigned) integer iterator
* Unmerged path kernel/sched/fair.c
diff --git a/lib/cpumask.c b/lib/cpumask.c
index 018f4aa3feed..ebcdc99b059a 100644
--- a/lib/cpumask.c
+++ b/lib/cpumask.c
@@ -63,6 +63,38 @@ int cpumask_any_but(const struct cpumask *mask, unsigned int cpu)
 	return i;
 }
 
+/**
+ * cpumask_next_wrap - helper to implement for_each_cpu_wrap
+ * @n: the cpu prior to the place to search
+ * @mask: the cpumask pointer
+ * @start: the start point of the iteration
+ * @wrap: assume @n crossing @start terminates the iteration
+ *
+ * Returns >= nr_cpu_ids on completion
+ *
+ * Note: the @wrap argument is required for the start condition when
+ * we cannot assume @start is set in @mask.
+ */
+int cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap)
+{
+	int next;
+
+again:
+	next = cpumask_next(n, mask);
+
+	if (wrap && n < start && next >= start) {
+		return nr_cpumask_bits;
+
+	} else if (next >= nr_cpumask_bits) {
+		wrap = true;
+		n = -1;
+		goto again;
+	}
+
+	return next;
+}
+EXPORT_SYMBOL(cpumask_next_wrap);
+
 /* These are not inline because of header tangles. */
 #ifdef CONFIG_CPUMASK_OFFSTACK
 /**
