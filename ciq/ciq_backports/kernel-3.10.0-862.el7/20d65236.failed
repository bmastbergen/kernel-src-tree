kvm: x86: mmu: Update comment in mark_spte_for_access_track

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Junaid Shahid <junaids@google.com>
commit 20d65236d01cdbe14a88f0e2c0f985669f8c41fc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/20d65236.failed

Reword the comment to hopefully make it more clear.

	Signed-off-by: Junaid Shahid <junaids@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 20d65236d01cdbe14a88f0e2c0f985669f8c41fc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 7460413d4173,e13041ac7cdf..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -649,6 -699,61 +649,64 @@@ static u64 mmu_spte_get_lockless(u64 *s
  	return __get_spte_lockless(sptep);
  }
  
++<<<<<<< HEAD
++=======
+ static u64 mark_spte_for_access_track(u64 spte)
+ {
+ 	if (shadow_accessed_mask != 0)
+ 		return spte & ~shadow_accessed_mask;
+ 
+ 	if (shadow_acc_track_mask == 0 || is_access_track_spte(spte))
+ 		return spte;
+ 
+ 	/*
+ 	 * Making an Access Tracking PTE will result in removal of write access
+ 	 * from the PTE. So, verify that we will be able to restore the write
+ 	 * access in the fast page fault path later on.
+ 	 */
+ 	WARN_ONCE((spte & PT_WRITABLE_MASK) &&
+ 		  !spte_can_locklessly_be_made_writable(spte),
+ 		  "kvm: Writable SPTE is not locklessly dirty-trackable\n");
+ 
+ 	WARN_ONCE(spte & (shadow_acc_track_saved_bits_mask <<
+ 			  shadow_acc_track_saved_bits_shift),
+ 		  "kvm: Access Tracking saved bit locations are not zero\n");
+ 
+ 	spte |= (spte & shadow_acc_track_saved_bits_mask) <<
+ 		shadow_acc_track_saved_bits_shift;
+ 	spte &= ~shadow_acc_track_mask;
+ 	spte |= shadow_acc_track_value;
+ 
+ 	return spte;
+ }
+ 
+ /* Returns the Accessed status of the PTE and resets it at the same time. */
+ static bool mmu_spte_age(u64 *sptep)
+ {
+ 	u64 spte = mmu_spte_get_lockless(sptep);
+ 
+ 	if (!is_accessed_spte(spte))
+ 		return false;
+ 
+ 	if (shadow_accessed_mask) {
+ 		clear_bit((ffs(shadow_accessed_mask) - 1),
+ 			  (unsigned long *)sptep);
+ 	} else {
+ 		/*
+ 		 * Capture the dirty status of the page, so that it doesn't get
+ 		 * lost when the SPTE is marked for access tracking.
+ 		 */
+ 		if (is_writable_pte(spte))
+ 			kvm_set_pfn_dirty(spte_to_pfn(spte));
+ 
+ 		spte = mark_spte_for_access_track(spte);
+ 		mmu_spte_update_no_track(sptep, spte);
+ 	}
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 20d65236d01c (kvm: x86: mmu: Update comment in mark_spte_for_access_track)
  static void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)
  {
  	/*
* Unmerged path arch/x86/kvm/mmu.c
