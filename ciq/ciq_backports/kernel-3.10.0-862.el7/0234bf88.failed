KVM: x86: introduce ISA specific SMM entry/exit callbacks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ladi Prosek <lprosek@redhat.com>
commit 0234bf885236a41ef05376039f2a8ebe7028a388
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/0234bf88.failed

Entering and exiting SMM may require ISA specific handling under certain
circumstances. This commit adds two new callbacks with empty implementations.
Actual functionality will be added in following commits.

* pre_enter_smm() is to be called when injecting an SMM, before any
  SMM related vcpu state has been changed
* pre_leave_smm() is to be called when emulating the RSM instruction,
  when the vcpu is in real mode and before any SMM related vcpu state
  has been restored

	Signed-off-by: Ladi Prosek <lprosek@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 0234bf885236a41ef05376039f2a8ebe7028a388)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/svm.c
index ba38eb017cb6,c4e9b99d48d8..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -5224,7 -5395,25 +5224,29 @@@ static inline void avic_post_state_rest
  	avic_handle_ldr_update(vcpu);
  }
  
++<<<<<<< HEAD
 +static struct kvm_x86_ops svm_x86_ops = {
++=======
+ static void svm_setup_mce(struct kvm_vcpu *vcpu)
+ {
+ 	/* [63:9] are reserved. */
+ 	vcpu->arch.mcg_cap &= 0x1ff;
+ }
+ 
+ static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
++>>>>>>> 0234bf885236 (KVM: x86: introduce ISA specific SMM entry/exit callbacks)
  	.cpu_has_kvm_support = has_svm,
  	.disabled_by_bios = is_disabled,
  	.hardware_setup = svm_hardware_setup,
@@@ -5338,6 -5522,10 +5360,13 @@@
  	.pmu_ops = &amd_pmu_ops,
  	.deliver_posted_interrupt = svm_deliver_avic_intr,
  	.update_pi_irte = svm_update_pi_irte,
++<<<<<<< HEAD
++=======
+ 	.setup_mce = svm_setup_mce,
+ 
+ 	.pre_enter_smm = svm_pre_enter_smm,
+ 	.pre_leave_smm = svm_pre_leave_smm,
++>>>>>>> 0234bf885236 (KVM: x86: introduce ISA specific SMM entry/exit callbacks)
  };
  
  static int __init svm_init(void)
diff --cc arch/x86/kvm/vmx.c
index 72c1c1c3db0f,1305bb65688b..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -11014,7 -11916,19 +11014,23 @@@ static void vmx_setup_mce(struct kvm_vc
  			~FEATURE_CONTROL_LMCE;
  }
  
++<<<<<<< HEAD
 +static struct kvm_x86_ops vmx_x86_ops = {
++=======
+ static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
++>>>>>>> 0234bf885236 (KVM: x86: introduce ISA specific SMM entry/exit callbacks)
  	.cpu_has_kvm_support = cpu_has_kvm_support,
  	.disabled_by_bios = vmx_disabled_by_bios,
  	.hardware_setup = hardware_setup,
@@@ -11136,7 -12047,15 +11152,10 @@@
  
  	.update_pi_irte = vmx_update_pi_irte,
  
 -#ifdef CONFIG_X86_64
 -	.set_hv_timer = vmx_set_hv_timer,
 -	.cancel_hv_timer = vmx_cancel_hv_timer,
 -#endif
 -
  	.setup_mce = vmx_setup_mce,
+ 
+ 	.pre_enter_smm = vmx_pre_enter_smm,
+ 	.pre_leave_smm = vmx_pre_leave_smm,
  };
  
  static int __init vmx_init(void)
diff --git a/arch/x86/include/asm/kvm_emulate.h b/arch/x86/include/asm/kvm_emulate.h
index 19d14ac23ef9..f94b015830b4 100644
--- a/arch/x86/include/asm/kvm_emulate.h
+++ b/arch/x86/include/asm/kvm_emulate.h
@@ -224,6 +224,8 @@ struct x86_emulate_ops {
 
 	unsigned (*get_hflags)(struct x86_emulate_ctxt *ctxt);
 	void (*set_hflags)(struct x86_emulate_ctxt *ctxt, unsigned hflags);
+	int (*pre_leave_smm)(struct x86_emulate_ctxt *ctxt, u64 smbase);
+
 };
 
 typedef u32 __attribute__((vector_size(16))) sse128_t;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ab2d8132f390..66d5cca5b86e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -967,6 +967,9 @@ struct kvm_x86_ops {
 	void (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);
 
 	void (*setup_mce)(struct kvm_vcpu *vcpu);
+
+	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
+	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
 };
 
 struct kvm_arch_async_pf {
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 645645864f85..1ecda429ad14 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -2606,6 +2606,15 @@ static int em_rsm(struct x86_emulate_ctxt *ctxt)
 	ctxt->ops->set_msr(ctxt, MSR_EFER, efer);
 
 	smbase = ctxt->ops->get_smbase(ctxt);
+
+	/*
+	 * Give pre_leave_smm() a chance to make ISA-specific changes to the
+	 * vCPU state (e.g. enter guest mode) before loading state from the SMM
+	 * state-save area.
+	 */
+	if (ctxt->ops->pre_leave_smm(ctxt, smbase))
+		return X86EMUL_UNHANDLEABLE;
+
 	if (emulator_has_longmode(ctxt))
 		ret = rsm_load_state_64(ctxt, smbase + 0x8000);
 	else
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 99e230533b87..fa27546c62cf 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5082,6 +5082,11 @@ static void emulator_set_hflags(struct x86_emulate_ctxt *ctxt, unsigned emul_fla
 	kvm_set_hflags(emul_to_vcpu(ctxt), emul_flags);
 }
 
+static int emulator_pre_leave_smm(struct x86_emulate_ctxt *ctxt, u64 smbase)
+{
+	return kvm_x86_ops->pre_leave_smm(emul_to_vcpu(ctxt), smbase);
+}
+
 static const struct x86_emulate_ops emulate_ops = {
 	.read_gpr            = emulator_read_gpr,
 	.write_gpr           = emulator_write_gpr,
@@ -5123,6 +5128,7 @@ static const struct x86_emulate_ops emulate_ops = {
 	.set_nmi_mask        = emulator_set_nmi_mask,
 	.get_hflags          = emulator_get_hflags,
 	.set_hflags          = emulator_set_hflags,
+	.pre_leave_smm       = emulator_pre_leave_smm,
 };
 
 static void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask)
@@ -6395,13 +6401,20 @@ static void enter_smm(struct kvm_vcpu *vcpu)
 	u32 cr0;
 
 	trace_kvm_enter_smm(vcpu->vcpu_id, vcpu->arch.smbase, true);
-	vcpu->arch.hflags |= HF_SMM_MASK;
 	memset(buf, 0, 512);
 	if (guest_cpuid_has_longmode(vcpu))
 		enter_smm_save_state_64(vcpu, buf);
 	else
 		enter_smm_save_state_32(vcpu, buf);
 
+	/*
+	 * Give pre_enter_smm() a chance to make ISA-specific changes to the
+	 * vCPU state (e.g. leave guest mode) after we've saved the state into
+	 * the SMM state-save area.
+	 */
+	kvm_x86_ops->pre_enter_smm(vcpu, buf);
+
+	vcpu->arch.hflags |= HF_SMM_MASK;
 	kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, buf, sizeof(buf));
 
 	if (kvm_x86_ops->get_nmi_mask(vcpu))
