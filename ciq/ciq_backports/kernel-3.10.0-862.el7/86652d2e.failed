libnvdimm, btt: clean up warning and error messages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Vishal Verma <vishal.l.verma@intel.com>
commit 86652d2eb347080a991968c9d68708dc010ac56c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/86652d2e.failed

Convert all WARN* style messages to dev_WARN, and for errors in the IO
paths, use dev_err_ratelimited. Also remove some BUG_ONs in the IO path
and replace them with the above - no need to crash the machine in case
of an unaligned IO.

	Cc: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Vishal Verma <vishal.l.verma@intel.com>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 86652d2eb347080a991968c9d68708dc010ac56c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvdimm/btt.c
diff --cc drivers/nvdimm/btt.c
index 73d03e1bfbd1,130193a9cd8c..000000000000
--- a/drivers/nvdimm/btt.c
+++ b/drivers/nvdimm/btt.c
@@@ -31,8 -31,18 +31,21 @@@ enum log_ent_request 
  	LOG_OLD_ENT
  };
  
++<<<<<<< HEAD
++=======
+ static struct device *to_dev(struct arena_info *arena)
+ {
+ 	return &arena->nd_btt->dev;
+ }
+ 
+ static u64 adjust_initial_offset(struct nd_btt *nd_btt, u64 offset)
+ {
+ 	return offset + nd_btt->initial_offset;
+ }
+ 
++>>>>>>> 86652d2eb347 (libnvdimm, btt: clean up warning and error messages)
  static int arena_read_bytes(struct arena_info *arena, resource_size_t offset,
 -		void *buf, size_t n, unsigned long flags)
 +		void *buf, size_t n)
  {
  	struct nd_btt *nd_btt = arena->nd_btt;
  	struct nd_namespace_common *ndns = nd_btt->ndns;
@@@ -57,8 -67,18 +70,21 @@@ static int btt_info_write(struct arena_
  {
  	int ret;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * infooff and info2off should always be at least 512B aligned.
+ 	 * We rely on that to make sure rw_bytes does error clearing
+ 	 * correctly, so make sure that is the case.
+ 	 */
+ 	dev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->infooff, 512),
+ 		"arena->infooff: %#llx is unaligned\n", arena->infooff);
+ 	dev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->info2off, 512),
+ 		"arena->info2off: %#llx is unaligned\n", arena->info2off);
+ 
++>>>>>>> 86652d2eb347 (libnvdimm, btt: clean up warning and error messages)
  	ret = arena_write_bytes(arena, arena->info2off, super,
 -			sizeof(struct btt_sb), 0);
 +			sizeof(struct btt_sb));
  	if (ret)
  		return ret;
  
@@@ -68,9 -88,8 +94,8 @@@
  
  static int btt_info_read(struct arena_info *arena, struct btt_sb *super)
  {
- 	WARN_ON(!super);
  	return arena_read_bytes(arena, arena->infooff, super,
 -			sizeof(struct btt_sb), 0);
 +			sizeof(struct btt_sb));
  }
  
  /*
@@@ -83,8 -103,11 +108,16 @@@ static int __btt_map_write(struct arena
  {
  	u64 ns_off = arena->mapoff + (lba * MAP_ENT_SIZE);
  
++<<<<<<< HEAD
 +	WARN_ON(lba >= arena->external_nlba);
 +	return arena_write_bytes(arena, ns_off, &mapping, MAP_ENT_SIZE);
++=======
+ 	if (unlikely(lba >= arena->external_nlba))
+ 		dev_err_ratelimited(to_dev(arena),
+ 			"%s: lba %#x out of range (max: %#x)\n",
+ 			__func__, lba, arena->external_nlba);
+ 	return arena_write_bytes(arena, ns_off, &mapping, MAP_ENT_SIZE, flags);
++>>>>>>> 86652d2eb347 (libnvdimm, btt: clean up warning and error messages)
  }
  
  static int btt_map_write(struct arena_info *arena, u32 lba, u32 mapping,
@@@ -138,9 -162,12 +172,12 @@@ static int btt_map_read(struct arena_in
  	u32 raw_mapping, postmap, ze, z_flag, e_flag;
  	u64 ns_off = arena->mapoff + (lba * MAP_ENT_SIZE);
  
- 	WARN_ON(lba >= arena->external_nlba);
+ 	if (unlikely(lba >= arena->external_nlba))
+ 		dev_err_ratelimited(to_dev(arena),
+ 			"%s: lba %#x out of range (max: %#x)\n",
+ 			__func__, lba, arena->external_nlba);
  
 -	ret = arena_read_bytes(arena, ns_off, &in, MAP_ENT_SIZE, rwb_flags);
 +	ret = arena_read_bytes(arena, ns_off, &in, MAP_ENT_SIZE);
  	if (ret)
  		return ret;
  
@@@ -186,10 -213,9 +223,9 @@@
  static int btt_log_read_pair(struct arena_info *arena, u32 lane,
  			struct log_entry *ent)
  {
- 	WARN_ON(!ent);
  	return arena_read_bytes(arena,
  			arena->logoff + (2 * lane * LOG_ENT_SIZE), ent,
 -			2 * LOG_ENT_SIZE, 0);
 +			2 * LOG_ENT_SIZE);
  }
  
  static struct dentry *debugfs_root;
@@@ -393,11 -416,21 +424,27 @@@ static int btt_map_init(struct arena_in
  	if (!zerobuf)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	while (mapsize) {
 +		size_t size = min(mapsize, chunk_size);
 +
++=======
+ 	/*
+ 	 * mapoff should always be at least 512B  aligned. We rely on that to
+ 	 * make sure rw_bytes does error clearing correctly, so make sure that
+ 	 * is the case.
+ 	 */
+ 	dev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->mapoff, 512),
+ 		"arena->mapoff: %#llx is unaligned\n", arena->mapoff);
+ 
+ 	while (mapsize) {
+ 		size_t size = min(mapsize, chunk_size);
+ 
+ 		dev_WARN_ONCE(to_dev(arena), size < 512,
+ 			"chunk size: %#lx is unaligned\n", size);
++>>>>>>> 86652d2eb347 (libnvdimm, btt: clean up warning and error messages)
  		ret = arena_write_bytes(arena, arena->mapoff + offset, zerobuf,
 -				size, 0);
 +				size);
  		if (ret)
  			goto free;
  
@@@ -417,11 -450,38 +464,39 @@@
   */
  static int btt_log_init(struct arena_info *arena)
  {
 -	size_t logsize = arena->info2off - arena->logoff;
 -	size_t chunk_size = SZ_4K, offset = 0;
 -	struct log_entry log;
 -	void *zerobuf;
  	int ret;
  	u32 i;
 +	struct log_entry log, zerolog;
  
++<<<<<<< HEAD
 +	memset(&zerolog, 0, sizeof(zerolog));
++=======
+ 	zerobuf = kzalloc(chunk_size, GFP_KERNEL);
+ 	if (!zerobuf)
+ 		return -ENOMEM;
+ 	/*
+ 	 * logoff should always be at least 512B  aligned. We rely on that to
+ 	 * make sure rw_bytes does error clearing correctly, so make sure that
+ 	 * is the case.
+ 	 */
+ 	dev_WARN_ONCE(to_dev(arena), !IS_ALIGNED(arena->logoff, 512),
+ 		"arena->logoff: %#llx is unaligned\n", arena->logoff);
+ 
+ 	while (logsize) {
+ 		size_t size = min(logsize, chunk_size);
+ 
+ 		dev_WARN_ONCE(to_dev(arena), size < 512,
+ 			"chunk size: %#lx is unaligned\n", size);
+ 		ret = arena_write_bytes(arena, arena->logoff + offset, zerobuf,
+ 				size, 0);
+ 		if (ret)
+ 			goto free;
+ 
+ 		offset += size;
+ 		logsize -= size;
+ 		cond_resched();
+ 	}
++>>>>>>> 86652d2eb347 (libnvdimm, btt: clean up warning and error messages)
  
  	for (i = 0; i < arena->nfree; i++) {
  		log.lba = cpu_to_le32(i);
@@@ -464,6 -557,17 +539,20 @@@ static int btt_freelist_init(struct are
  		arena->freelist[i].seq = nd_inc_seq(le32_to_cpu(log_new.seq));
  		arena->freelist[i].block = le32_to_cpu(log_new.old_map);
  
++<<<<<<< HEAD
++=======
+ 		/*
+ 		 * FIXME: if error clearing fails during init, we want to make
+ 		 * the BTT read-only
+ 		 */
+ 		if (ent_e_flag(log_new.old_map)) {
+ 			ret = arena_clear_freelist_error(arena, i);
+ 			if (ret)
+ 				dev_err_ratelimited(to_dev(arena),
+ 					"Unable to clear known errors\n");
+ 		}
+ 
++>>>>>>> 86652d2eb347 (libnvdimm, btt: clean up warning and error messages)
  		/* This implies a newly created or untouched flog entry */
  		if (log_new.old_map == log_new.new_map)
  			continue;
@@@ -1145,40 -1304,33 +1234,42 @@@ static int btt_do_bvec(struct btt *btt
  	return ret;
  }
  
 -static blk_qc_t btt_make_request(struct request_queue *q, struct bio *bio)
 +static void btt_make_request(struct request_queue *q, struct bio *bio)
  {
 -	struct bio_integrity_payload *bip = bio_integrity(bio);
 +	struct bio_integrity_payload *bip = bio->bi_integrity;
  	struct btt *btt = q->queuedata;
 -	struct bvec_iter iter;
 +	struct bio_vec *bvec;
  	unsigned long start;
 -	struct bio_vec bvec;
 -	int err = 0;
 +	int err = 0, rw, i;
  	bool do_acct;
 +	sector_t sector = bio->bi_sector;
  
 -	if (!bio_integrity_prep(bio))
 -		return BLK_QC_T_NONE;
 +	/*
 +	 * bio_integrity_enabled also checks if the bio already has an
 +	 * integrity payload attached. If it does, we *don't* do a
 +	 * bio_integrity_prep here - the payload has been generated by
 +	 * another kernel subsystem, and we just pass it through.
 +	 */
 +	if (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {
 +		err = -EIO;
 +		goto out;
 +	}
  
  	do_acct = nd_iostat_start(bio, &start);
 -	bio_for_each_segment(bvec, bio, iter) {
 -		unsigned int len = bvec.bv_len;
 +	rw = bio_data_dir(bio);
 +	bio_for_each_segment(bvec, bio, i) {
 +		unsigned int len = bvec->bv_len;
  
- 		BUG_ON(len > PAGE_SIZE);
- 		/* Make sure len is in multiples of sector size. */
- 		/* XXX is this right? */
- 		BUG_ON(len < btt->sector_size);
- 		BUG_ON(len % btt->sector_size);
+ 		if (len > PAGE_SIZE || len < btt->sector_size ||
+ 				len % btt->sector_size) {
+ 			dev_err_ratelimited(&btt->nd_btt->dev,
+ 				"unaligned bio segment (len: %d)\n", len);
+ 			bio->bi_status = BLK_STS_IOERR;
+ 			break;
+ 		}
  
 -		err = btt_do_bvec(btt, bip, bvec.bv_page, len, bvec.bv_offset,
 -				  op_is_write(bio_op(bio)), iter.bi_sector);
 +		err = btt_do_bvec(btt, bip, bvec->bv_page, len, bvec->bv_offset,
 +				rw, sector);
  		if (err) {
  			dev_err(&btt->nd_btt->dev,
  					"io error in %s sector %lld, len %d,\n",
* Unmerged path drivers/nvdimm/btt.c
