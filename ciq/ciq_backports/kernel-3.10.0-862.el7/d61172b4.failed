mm/core, x86/mm/pkeys: Differentiate instruction fetches

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] core, x86/mm/pkeys: Differentiate instruction fetches (Rui Wang) [1272615]
Rebuild_FUZZ: 97.25%
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit d61172b4b695b821388cdb6088a41d431bcbb93b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d61172b4.failed

As discussed earlier, we attempt to enforce protection keys in
software.

However, the code checks all faults to ensure that they are not
violating protection key permissions.  It was assumed that all
faults are either write faults where we check PKRU[key].WD (write
disable) or read faults where we check the AD (access disable)
bit.

But, there is a third category of faults for protection keys:
instruction faults.  Instruction faults never run afoul of
protection keys because they do not affect instruction fetches.

So, plumb the PF_INSTR bit down in to the
arch_vma_access_permitted() function where we do the protection
key checks.

We also add a new FAULT_FLAG_INSTRUCTION.  This is because
handle_mm_fault() is not passed the architecture-specific
error_code where we keep PF_INSTR, so we need to encode the
instruction fetch information in to the arch-generic fault
flags.

	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Dave Hansen <dave@sr71.net>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/20160212210224.96928009@viggo.jf.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit d61172b4b695b821388cdb6088a41d431bcbb93b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/mmu_context.h
#	arch/s390/include/asm/mmu_context.h
#	arch/x86/include/asm/mmu_context.h
#	arch/x86/mm/fault.c
#	include/asm-generic/mm_hooks.h
#	include/linux/mm.h
#	mm/gup.c
#	mm/memory.c
diff --cc arch/powerpc/include/asm/mmu_context.h
index b2294769341b,4eaab40e3ade..000000000000
--- a/arch/powerpc/include/asm/mmu_context.h
+++ b/arch/powerpc/include/asm/mmu_context.h
@@@ -165,5 -148,17 +165,20 @@@ static inline void arch_bprm_mm_init(st
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool execute, bool foreign)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  #endif /* __KERNEL__ */
  #endif /* __ASM_POWERPC_MMU_CONTEXT_H */
diff --cc arch/s390/include/asm/mmu_context.h
index eccdb1327fc0,fa66b6dfa97a..000000000000
--- a/arch/s390/include/asm/mmu_context.h
+++ b/arch/s390/include/asm/mmu_context.h
@@@ -108,5 -130,16 +108,14 @@@ static inline void arch_bprm_mm_init(st
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool execute, bool foreign)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  
 -static inline bool arch_pte_access_permitted(pte_t pte, bool write)
 -{
 -	/* by default, allow everything */
 -	return true;
 -}
  #endif /* __S390_MMU_CONTEXT_H */
diff --cc arch/x86/include/asm/mmu_context.h
index a34c7d411865,6572b949cbca..000000000000
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@@ -150,7 -254,89 +150,93 @@@ static inline void arch_bprm_mm_init(st
  static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
  			      unsigned long start, unsigned long end)
  {
++<<<<<<< HEAD
 +	mpx_notify_unmap(mm, vma, start, end);
++=======
+ 	/*
+ 	 * mpx_notify_unmap() goes and reads a rarely-hot
+ 	 * cacheline in the mm_struct.  That can be expensive
+ 	 * enough to be seen in profiles.
+ 	 *
+ 	 * The mpx_notify_unmap() call and its contents have been
+ 	 * observed to affect munmap() performance on hardware
+ 	 * where MPX is not present.
+ 	 *
+ 	 * The unlikely() optimizes for the fast case: no MPX
+ 	 * in the CPU, or no MPX use in the process.  Even if
+ 	 * we get this wrong (in the unlikely event that MPX
+ 	 * is widely enabled on some system) the overhead of
+ 	 * MPX itself (reading bounds tables) is expected to
+ 	 * overwhelm the overhead of getting this unlikely()
+ 	 * consistently wrong.
+ 	 */
+ 	if (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))
+ 		mpx_notify_unmap(mm, vma, start, end);
+ }
+ 
+ static inline int vma_pkey(struct vm_area_struct *vma)
+ {
+ 	u16 pkey = 0;
+ #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
+ 	unsigned long vma_pkey_mask = VM_PKEY_BIT0 | VM_PKEY_BIT1 |
+ 				      VM_PKEY_BIT2 | VM_PKEY_BIT3;
+ 	pkey = (vma->vm_flags & vma_pkey_mask) >> VM_PKEY_SHIFT;
+ #endif
+ 	return pkey;
+ }
+ 
+ static inline bool __pkru_allows_pkey(u16 pkey, bool write)
+ {
+ 	u32 pkru = read_pkru();
+ 
+ 	if (!__pkru_allows_read(pkru, pkey))
+ 		return false;
+ 	if (write && !__pkru_allows_write(pkru, pkey))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * We only want to enforce protection keys on the current process
+  * because we effectively have no access to PKRU for other
+  * processes or any way to tell *which * PKRU in a threaded
+  * process we could use.
+  *
+  * So do not enforce things if the VMA is not from the current
+  * mm, or if we are in a kernel thread.
+  */
+ static inline bool vma_is_foreign(struct vm_area_struct *vma)
+ {
+ 	if (!current->mm)
+ 		return true;
+ 	/*
+ 	 * Should PKRU be enforced on the access to this VMA?  If
+ 	 * the VMA is from another process, then PKRU has no
+ 	 * relevance and should not be enforced.
+ 	 */
+ 	if (current->mm != vma->vm_mm)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool execute, bool foreign)
+ {
+ 	/* pkeys never affect instruction fetches */
+ 	if (execute)
+ 		return true;
+ 	/* allow access if the VMA is not one from this process */
+ 	if (foreign || vma_is_foreign(vma))
+ 		return true;
+ 	return __pkru_allows_pkey(vma_pkey(vma), write);
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	return __pkru_allows_pkey(pte_flags_pkey(pte_flags(pte)), write);
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  }
  
  #endif /* _ASM_X86_MMU_CONTEXT_H */
diff --cc arch/x86/mm/fault.c
index 1977abf5754c,d81744e6f39f..000000000000
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@@ -809,7 -894,24 +809,28 @@@ __bad_area(struct pt_regs *regs, unsign
  static noinline void
  bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)
  {
++<<<<<<< HEAD
 +	__bad_area(regs, error_code, address, SEGV_MAPERR);
++=======
+ 	__bad_area(regs, error_code, address, NULL, SEGV_MAPERR);
+ }
+ 
+ static inline bool bad_area_access_from_pkeys(unsigned long error_code,
+ 		struct vm_area_struct *vma)
+ {
+ 	/* This code is always called on the current mm */
+ 	bool foreign = false;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
+ 		return false;
+ 	if (error_code & PF_PK)
+ 		return true;
+ 	/* this checks permission keys on the VMA: */
+ 	if (!arch_vma_access_permitted(vma, (error_code & PF_WRITE),
+ 				(error_code & PF_INSTR), foreign))
+ 		return true;
+ 	return false;
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  }
  
  static noinline void
@@@ -966,6 -1098,25 +987,28 @@@ int show_unhandled_signals = 1
  static inline int
  access_error(unsigned long error_code, struct vm_area_struct *vma)
  {
++<<<<<<< HEAD
++=======
+ 	/* This is only called for the current mm, so: */
+ 	bool foreign = false;
+ 	/*
+ 	 * Access or read was blocked by protection keys. We do
+ 	 * this check before any others because we do not want
+ 	 * to, for instance, confuse a protection-key-denied
+ 	 * write with one for which we should do a COW.
+ 	 */
+ 	if (error_code & PF_PK)
+ 		return 1;
+ 	/*
+ 	 * Make sure to check the VMA so that we do not perform
+ 	 * faults just to hit a PF_PK as soon as we fill in a
+ 	 * page.
+ 	 */
+ 	if (!arch_vma_access_permitted(vma, (error_code & PF_WRITE),
+ 				(error_code & PF_INSTR), foreign))
+ 		return 1;
+ 
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  	if (error_code & PF_WRITE) {
  		/* write, present and write, not present: */
  		if (unlikely(!(vma->vm_flags & VM_WRITE)))
diff --cc include/asm-generic/mm_hooks.h
index 866aa461efa5,cc5d9a1405df..000000000000
--- a/include/asm-generic/mm_hooks.h
+++ b/include/asm-generic/mm_hooks.h
@@@ -26,4 -26,16 +26,19 @@@ static inline void arch_bprm_mm_init(st
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool execute, bool foreign)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  #endif	/* _ASM_GENERIC_MM_HOOKS_H */
diff --cc include/linux/mm.h
index a0514d1e5d91,7955c3eb83db..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -218,13 -245,14 +218,24 @@@ extern unsigned int kobjsize(const voi
  extern pgprot_t protection_map[16];
  
  #define FAULT_FLAG_WRITE	0x01	/* Fault was a write access */
++<<<<<<< HEAD
 +#define FAULT_FLAG_NONLINEAR	0x02	/* Fault was via a nonlinear mapping */
 +#define FAULT_FLAG_MKWRITE	0x04	/* Fault was mkwrite of existing pte */
 +#define FAULT_FLAG_ALLOW_RETRY	0x08	/* Retry fault if blocking */
 +#define FAULT_FLAG_RETRY_NOWAIT	0x10	/* Don't drop mmap_sem and wait when retrying */
 +#define FAULT_FLAG_KILLABLE	0x20	/* The fault task is in SIGKILL killable region */
 +#define FAULT_FLAG_TRIED	0x40	/* second try */
 +#define FAULT_FLAG_USER		0x80	/* The fault originated in userspace */
++=======
+ #define FAULT_FLAG_MKWRITE	0x02	/* Fault was mkwrite of existing pte */
+ #define FAULT_FLAG_ALLOW_RETRY	0x04	/* Retry fault if blocking */
+ #define FAULT_FLAG_RETRY_NOWAIT	0x08	/* Don't drop mmap_sem and wait when retrying */
+ #define FAULT_FLAG_KILLABLE	0x10	/* The fault task is in SIGKILL killable region */
+ #define FAULT_FLAG_TRIED	0x20	/* Second try */
+ #define FAULT_FLAG_USER		0x40	/* The fault originated in userspace */
+ #define FAULT_FLAG_REMOTE	0x80	/* faulting for non current tsk/mm */
+ #define FAULT_FLAG_INSTRUCTION  0x100	/* The fault was during an instruction fetch */
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  
  /*
   * vm_fault is filled by the the pagefault handler and passed to the vma's
diff --cc mm/gup.c
index 3166366affd5,7f1c4fb77cfa..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -254,21 -195,267 +254,71 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -/**
 - * follow_page_mask - look up a page descriptor from a user-virtual address
 - * @vma: vm_area_struct mapping @address
 - * @address: virtual address to look up
 - * @flags: flags modifying lookup behaviour
 - * @page_mask: on output, *page_mask is set according to the size of the page
 - *
 - * @flags can have FOLL_ flags set, defined in <linux/mm.h>
 - *
 - * Returns the mapped (struct page *), %NULL if no mapping exists, or
 - * an error pointer if there is a mapping to something not represented
 - * by a page descriptor (see also vm_normal_page()).
 - */
 -struct page *follow_page_mask(struct vm_area_struct *vma,
 -			      unsigned long address, unsigned int flags,
 -			      unsigned int *page_mask)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	*page_mask = 0;
 -
 -	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 -	if (!IS_ERR(page)) {
 -		BUG_ON(flags & FOLL_GET);
  		return page;
 -	}
 -
 -	pgd = pgd_offset(mm, address);
 -	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 -		return no_page_table(vma, flags);
 -
 -	pud = pud_offset(pgd, address);
 -	if (pud_none(*pud))
 -		return no_page_table(vma, flags);
 -	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
 -		page = follow_huge_pud(mm, address, pud, flags);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if (unlikely(pud_bad(*pud)))
 -		return no_page_table(vma, flags);
 -
 -	pmd = pmd_offset(pud, address);
 -	if (pmd_none(*pmd))
 -		return no_page_table(vma, flags);
 -	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
 -		page = follow_huge_pmd(mm, address, pmd, flags);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
 -		return no_page_table(vma, flags);
 -	if (pmd_devmap(*pmd)) {
 -		ptl = pmd_lock(mm, pmd);
 -		page = follow_devmap_pmd(vma, address, pmd, flags);
 -		spin_unlock(ptl);
 -		if (page)
 -			return page;
 -	}
 -	if (likely(!pmd_trans_huge(*pmd)))
 -		return follow_page_pte(vma, address, pmd, flags);
 -
 -	ptl = pmd_lock(mm, pmd);
 -	if (unlikely(!pmd_trans_huge(*pmd))) {
 -		spin_unlock(ptl);
 -		return follow_page_pte(vma, address, pmd, flags);
 -	}
 -	if (flags & FOLL_SPLIT) {
 -		int ret;
 -		page = pmd_page(*pmd);
 -		if (is_huge_zero_page(page)) {
 -			spin_unlock(ptl);
 -			ret = 0;
 -			split_huge_pmd(vma, pmd, address);
 -		} else {
 -			get_page(page);
 -			spin_unlock(ptl);
 -			lock_page(page);
 -			ret = split_huge_page(page);
 -			unlock_page(page);
 -			put_page(page);
 -		}
 -
 -		return ret ? ERR_PTR(ret) :
 -			follow_page_pte(vma, address, pmd, flags);
 -	}
 -
 -	page = follow_trans_huge_pmd(vma, address, pmd, flags);
 -	spin_unlock(ptl);
 -	*page_mask = HPAGE_PMD_NR - 1;
 -	return page;
 -}
 -
 -static int get_gate_page(struct mm_struct *mm, unsigned long address,
 -		unsigned int gup_flags, struct vm_area_struct **vma,
 -		struct page **page)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	pte_t *pte;
 -	int ret = -EFAULT;
 -
 -	/* user gate pages are read-only */
 -	if (gup_flags & FOLL_WRITE)
 -		return -EFAULT;
 -	if (address > TASK_SIZE)
 -		pgd = pgd_offset_k(address);
 -	else
 -		pgd = pgd_offset_gate(mm, address);
 -	BUG_ON(pgd_none(*pgd));
 -	pud = pud_offset(pgd, address);
 -	BUG_ON(pud_none(*pud));
 -	pmd = pmd_offset(pud, address);
 -	if (pmd_none(*pmd))
 -		return -EFAULT;
 -	VM_BUG_ON(pmd_trans_huge(*pmd));
 -	pte = pte_offset_map(pmd, address);
 -	if (pte_none(*pte))
 -		goto unmap;
 -	*vma = get_gate_vma(mm);
 -	if (!page)
 -		goto out;
 -	*page = vm_normal_page(*vma, address, *pte);
 -	if (!*page) {
 -		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
 -			goto unmap;
 -		*page = pte_page(*pte);
 -	}
 -	get_page(*page);
 -out:
 -	ret = 0;
 -unmap:
 -	pte_unmap(pte);
 -	return ret;
 -}
 -
 -/*
 - * mmap_sem must be held on entry.  If @nonblocking != NULL and
 - * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.
 - * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
 - */
 -static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
 -		unsigned long address, unsigned int *flags, int *nonblocking)
 -{
 -	struct mm_struct *mm = vma->vm_mm;
 -	unsigned int fault_flags = 0;
 -	int ret;
 -
 -	/* mlock all present pages, but do not fault in new pages */
 -	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
 -		return -ENOENT;
 -	/* For mm_populate(), just skip the stack guard page. */
 -	if ((*flags & FOLL_POPULATE) &&
 -			(stack_guard_page_start(vma, address) ||
 -			 stack_guard_page_end(vma, address + PAGE_SIZE)))
 -		return -ENOENT;
 -	if (*flags & FOLL_WRITE)
 -		fault_flags |= FAULT_FLAG_WRITE;
 -	if (*flags & FOLL_REMOTE)
 -		fault_flags |= FAULT_FLAG_REMOTE;
 -	if (nonblocking)
 -		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
 -	if (*flags & FOLL_NOWAIT)
 -		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
 -	if (*flags & FOLL_TRIED) {
 -		VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
 -		fault_flags |= FAULT_FLAG_TRIED;
 -	}
 -
 -	ret = handle_mm_fault(mm, vma, address, fault_flags);
 -	if (ret & VM_FAULT_ERROR) {
 -		if (ret & VM_FAULT_OOM)
 -			return -ENOMEM;
 -		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
 -			return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;
 -		if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
 -			return -EFAULT;
 -		BUG();
 -	}
 -
 -	if (tsk) {
 -		if (ret & VM_FAULT_MAJOR)
 -			tsk->maj_flt++;
 -		else
 -			tsk->min_flt++;
 -	}
 -
 -	if (ret & VM_FAULT_RETRY) {
 -		if (nonblocking)
 -			*nonblocking = 0;
 -		return -EBUSY;
 -	}
  
 +no_page_table:
  	/*
 -	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
 -	 * necessary, even if maybe_mkwrite decided not to set pte_write. We
 -	 * can thus safely do subsequent page lookups as if they were reads.
 -	 * But only do so when looping for pte_write is futile: in some cases
 -	 * userspace may also be wanting to write to the gotten user page,
 -	 * which a read fault here might prevent (a readonly page might get
 -	 * reCOWed by userspace write).
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
  	 */
++<<<<<<< HEAD
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
 +	return page;
++=======
+ 	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
+ 		*flags &= ~FOLL_WRITE;
+ 	return 0;
+ }
+ 
+ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
+ {
+ 	vm_flags_t vm_flags = vma->vm_flags;
+ 	int write = (gup_flags & FOLL_WRITE);
+ 	int foreign = (gup_flags & FOLL_REMOTE);
+ 
+ 	if (vm_flags & (VM_IO | VM_PFNMAP))
+ 		return -EFAULT;
+ 
+ 	if (write) {
+ 		if (!(vm_flags & VM_WRITE)) {
+ 			if (!(gup_flags & FOLL_FORCE))
+ 				return -EFAULT;
+ 			/*
+ 			 * We used to let the write,force case do COW in a
+ 			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
+ 			 * set a breakpoint in a read-only mapping of an
+ 			 * executable, without corrupting the file (yet only
+ 			 * when that file had been opened for writing!).
+ 			 * Anon pages in shared mappings are surprising: now
+ 			 * just reject it.
+ 			 */
+ 			if (!is_cow_mapping(vm_flags))
+ 				return -EFAULT;
+ 		}
+ 	} else if (!(vm_flags & VM_READ)) {
+ 		if (!(gup_flags & FOLL_FORCE))
+ 			return -EFAULT;
+ 		/*
+ 		 * Is there actually any vma we can reach here which does not
+ 		 * have VM_MAYREAD set?
+ 		 */
+ 		if (!(vm_flags & VM_MAYREAD))
+ 			return -EFAULT;
+ 	}
+ 	/*
+ 	 * gups are always data accesses, not instruction
+ 	 * fetches, so execute=false here
+ 	 */
+ 	if (!arch_vma_access_permitted(vma, write, false, foreign))
+ 		return -EFAULT;
+ 	return 0;
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  }
  
  /**
@@@ -535,6 -621,28 +585,31 @@@ next_page
  }
  EXPORT_SYMBOL(__get_user_pages);
  
++<<<<<<< HEAD
++=======
+ bool vma_permits_fault(struct vm_area_struct *vma, unsigned int fault_flags)
+ {
+ 	bool write   = !!(fault_flags & FAULT_FLAG_WRITE);
+ 	bool foreign = !!(fault_flags & FAULT_FLAG_REMOTE);
+ 	vm_flags_t vm_flags = write ? VM_WRITE : VM_READ;
+ 
+ 	if (!(vm_flags & vma->vm_flags))
+ 		return false;
+ 
+ 	/*
+ 	 * The architecture might have a hardware protection
+ 	 * mechanism other than read/write that can deny access.
+ 	 *
+ 	 * gup always represents data access, not instruction
+ 	 * fetches, so execute=false here:
+ 	 */
+ 	if (!arch_vma_access_permitted(vma, write, false, foreign))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  /*
   * fixup_user_fault() - manually resolve a user page fault
   * @tsk:	the task_struct to use for page fault accounting, or
diff --cc mm/memory.c
index 2fc5b28b6782,99e9f928264a..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3272,6 -3379,11 +3272,14 @@@ static int __handle_mm_fault(struct mm_
  	pmd_t *pmd;
  	pte_t *pte;
  
++<<<<<<< HEAD
++=======
+ 	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
+ 					    flags & FAULT_FLAG_INSTRUCTION,
+ 					    flags & FAULT_FLAG_REMOTE))
+ 		return VM_FAULT_SIGSEGV;
+ 
++>>>>>>> d61172b4b695 (mm/core, x86/mm/pkeys: Differentiate instruction fetches)
  	if (unlikely(is_vm_hugetlb_page(vma)))
  		return hugetlb_fault(mm, vma, address, flags);
  
* Unmerged path arch/powerpc/include/asm/mmu_context.h
* Unmerged path arch/s390/include/asm/mmu_context.h
* Unmerged path arch/x86/include/asm/mmu_context.h
* Unmerged path arch/x86/mm/fault.c
* Unmerged path include/asm-generic/mm_hooks.h
* Unmerged path include/linux/mm.h
* Unmerged path mm/gup.c
* Unmerged path mm/memory.c
