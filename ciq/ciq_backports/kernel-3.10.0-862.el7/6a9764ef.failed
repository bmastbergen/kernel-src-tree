net/mlx5e: Isolate open_channels from priv->params

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Isolate open_channels from priv->params (Don Dutile) [1456659 1499362]
Rebuild_FUZZ: 95.83%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 6a9764efb255f49a91e229799c38d5c1c9361987
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6a9764ef.failed

In order to have a clean separation between channels resources creation
flows and current active mlx5e netdev parameters, make sure each
resource creation function do not access priv->params, and only works
with on a new fresh set of parameters.

For this we add "new" mlx5e_params field to mlx5e_channels structure
and use it down the road to mlx5e_open_{cq,rq,sq} and so on.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
(cherry picked from commit 6a9764efb255f49a91e229799c38d5c1c9361987)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 16c2c2d53ebb,007f91f54fda..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -286,13 -293,140 +286,144 @@@ struct mlx5e_cq 
  	struct mlx5_frag_wq_ctrl   wq_ctrl;
  } ____cacheline_aligned_in_smp;
  
 -struct mlx5e_tx_wqe_info {
 -	u32 num_bytes;
 -	u8  num_wqebbs;
 -	u8  num_dma;
 -};
 -
 +struct mlx5e_rq;
 +typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
 +				       struct mlx5_cqe64 *cqe);
 +typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
 +				  u16 ix);
 +
++<<<<<<< HEAD
 +typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq *rq, u16 ix);
++=======
+ enum mlx5e_dma_map_type {
+ 	MLX5E_DMA_MAP_SINGLE,
+ 	MLX5E_DMA_MAP_PAGE
+ };
+ 
+ struct mlx5e_sq_dma {
+ 	dma_addr_t              addr;
+ 	u32                     size;
+ 	enum mlx5e_dma_map_type type;
+ };
+ 
+ enum {
+ 	MLX5E_SQ_STATE_ENABLED,
+ };
+ 
+ struct mlx5e_sq_wqe_info {
+ 	u8  opcode;
+ 	u8  num_wqebbs;
+ };
+ 
+ struct mlx5e_txqsq {
+ 	/* data path */
+ 
+ 	/* dirtied @completion */
+ 	u16                        cc;
+ 	u32                        dma_fifo_cc;
+ 
+ 	/* dirtied @xmit */
+ 	u16                        pc ____cacheline_aligned_in_smp;
+ 	u32                        dma_fifo_pc;
+ 	struct mlx5e_sq_stats      stats;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct sk_buff           **skb;
+ 		struct mlx5e_sq_dma       *dma_fifo;
+ 		struct mlx5e_tx_wqe_info  *wqe_info;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	u32                        dma_fifo_mask;
+ 	void __iomem              *uar_map;
+ 	struct netdev_queue       *txq;
+ 	u32                        sqn;
+ 	u16                        max_inline;
+ 	u8                         min_inline_mode;
+ 	u16                        edge;
+ 	struct device             *pdev;
+ 	struct mlx5e_tstamp       *tstamp;
+ 	__be32                     mkey_be;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ 	int                        txq_ix;
+ 	u32                        rate_limit;
+ } ____cacheline_aligned_in_smp;
+ 
+ struct mlx5e_xdpsq {
+ 	/* data path */
+ 
+ 	/* dirtied @rx completion */
+ 	u16                        cc;
+ 	u16                        pc;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct mlx5e_dma_info     *di;
+ 		bool                       doorbell;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	void __iomem              *uar_map;
+ 	u32                        sqn;
+ 	struct device             *pdev;
+ 	__be32                     mkey_be;
+ 	u8                         min_inline_mode;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ } ____cacheline_aligned_in_smp;
+ 
+ struct mlx5e_icosq {
+ 	/* data path */
+ 
+ 	/* dirtied @completion */
+ 	u16                        cc;
+ 
+ 	/* dirtied @xmit */
+ 	u16                        pc ____cacheline_aligned_in_smp;
+ 	u32                        dma_fifo_pc;
+ 	u16                        prev_cc;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct mlx5e_sq_wqe_info *ico_wqe;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	void __iomem              *uar_map;
+ 	u32                        sqn;
+ 	u16                        edge;
+ 	struct device             *pdev;
+ 	__be32                     mkey_be;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ } ____cacheline_aligned_in_smp;
+ 
+ static inline bool
+ mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
+ {
+ 	return (((wq->sz_m1 & (cc - pc)) >= n) || (cc == pc));
+ }
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
  struct mlx5e_dma_info {
  	struct page	*page;
@@@ -503,6 -560,12 +634,15 @@@ struct mlx5e_channel 
  	int                        cpu;
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5e_channels {
+ 	struct mlx5e_channel **c;
+ 	unsigned int           num;
+ 	struct mlx5e_params    params;
+ };
+ 
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  enum mlx5e_traffic_types {
  	MLX5E_TT_IPV4_TCP,
  	MLX5E_TT_IPV6_TCP,
@@@ -670,8 -733,8 +810,13 @@@ struct mlx5e_profile 
  
  struct mlx5e_priv {
  	/* priv data path fields - start */
++<<<<<<< HEAD
 +	struct mlx5e_sq            **txq_to_sq_map;
 +	int channeltc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
++=======
+ 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
+ 	int channel_tc2txq[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	/* priv data path fields - end */
  
  	unsigned long              state;
@@@ -777,11 -840,24 +921,30 @@@ int mlx5e_vlan_rx_kill_vid(struct net_d
  void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
  void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
  
 -int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd);
 +int mlx5e_modify_rqs_vsd(struct mlx5e_priv *priv, bool vsd);
  
++<<<<<<< HEAD
 +int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz, int ix);
 +void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_priv *priv, void *tirc,
 +				    enum mlx5e_traffic_types tt);
++=======
+ struct mlx5e_redirect_rqt_param {
+ 	bool is_rss;
+ 	union {
+ 		u32 rqn; /* Direct RQN (Non-RSS) */
+ 		struct {
+ 			u8 hfunc;
+ 			struct mlx5e_channels *channels;
+ 		} rss; /* RSS data */
+ 	};
+ };
+ 
+ int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
+ 		       struct mlx5e_redirect_rqt_param rrp);
+ void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_params *params,
+ 				    enum mlx5e_traffic_types tt,
+ 				    void *tirc);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
  int mlx5e_open_locked(struct net_device *netdev);
  int mlx5e_close_locked(struct net_device *netdev);
@@@ -792,13 -868,32 +955,14 @@@ int mlx5e_get_max_linkspeed(struct mlx5
  
  void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
  				 u8 cq_period_mode);
- void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type);
+ void mlx5e_set_rq_type_params(struct mlx5_core_dev *mdev,
+ 			      struct mlx5e_params *params, u8 rq_type);
  
 -static inline
 -struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)
 +static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 +				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)
  {
 -	u16                         pi   = *pc & wq->sz_m1;
 -	struct mlx5e_tx_wqe        *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 -	struct mlx5_wqe_ctrl_seg   *cseg = &wqe->ctrl;
 -
 -	memset(cseg, 0, sizeof(*cseg));
 -
 -	cseg->opmod_idx_opcode = cpu_to_be32((*pc << 8) | MLX5_OPCODE_NOP);
 -	cseg->qpn_ds           = cpu_to_be32((sqn << 8) | 0x01);
 -
 -	(*pc)++;
 +	u16 ofst = MLX5_BF_OFFSET + sq->bf_offset;
  
 -	return wqe;
 -}
 -
 -static inline
 -void mlx5e_notify_hw(struct mlx5_wq_cyc *wq, u16 pc,
 -		     void __iomem *uar_map,
 -		     struct mlx5_wqe_ctrl_seg *ctrl)
 -{
 -	ctrl->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
  	/* ensure wqe is visible to device before updating doorbell record */
  	dma_wmb();
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
index 35c9cc1953cf,b2cd0ef7921e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
@@@ -152,12 -152,9 +152,16 @@@ static bool mlx5e_query_global_pause_co
  }
  
  #define MLX5E_NUM_Q_CNTRS(priv) (NUM_Q_COUNTERS * (!!priv->q_counter))
 -#define MLX5E_NUM_RQ_STATS(priv) (NUM_RQ_STATS * (priv)->channels.num)
 +#define MLX5E_NUM_RQ_STATS(priv) \
 +	(NUM_RQ_STATS * priv->params.num_channels * \
 +	 test_bit(MLX5E_STATE_OPENED, &priv->state))
  #define MLX5E_NUM_SQ_STATS(priv) \
++<<<<<<< HEAD
 +	(NUM_SQ_STATS * priv->params.num_channels * priv->params.num_tc * \
 +	 test_bit(MLX5E_STATE_OPENED, &priv->state))
++=======
+ 	(NUM_SQ_STATS * (priv)->channels.num * (priv)->channels.params.num_tc)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  #define MLX5E_NUM_PFC_COUNTERS(priv) \
  	((mlx5e_query_global_pause_combined(priv) + hweight8(mlx5e_query_pfc_combined(priv))) * \
  	  NUM_PPORT_PER_PRIO_PFC_COUNTERS)
@@@ -267,8 -264,8 +271,13 @@@ static void mlx5e_fill_stats_strings(st
  			sprintf(data + (idx++) * ETH_GSTRING_LEN,
  				rq_stats_desc[j].format, i);
  
++<<<<<<< HEAD
 +	for (tc = 0; tc < priv->params.num_tc; tc++)
 +		for (i = 0; i < priv->params.num_channels; i++)
++=======
+ 	for (tc = 0; tc < priv->channels.params.num_tc; tc++)
+ 		for (i = 0; i < priv->channels.num; i++)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  			for (j = 0; j < NUM_SQ_STATS; j++)
  				sprintf(data + (idx++) * ETH_GSTRING_LEN,
  					sq_stats_desc[j].format,
@@@ -382,16 -381,16 +391,21 @@@ static void mlx5e_get_ethtool_stats(str
  		return;
  
  	/* per channel counters */
 -	for (i = 0; i < channels->num; i++)
 +	for (i = 0; i < priv->params.num_channels; i++)
  		for (j = 0; j < NUM_RQ_STATS; j++)
  			data[idx++] =
 -			       MLX5E_READ_CTR64_CPU(&channels->c[i]->rq.stats,
 +			       MLX5E_READ_CTR64_CPU(&priv->channel[i]->rq.stats,
  						    rq_stats_desc, j);
  
++<<<<<<< HEAD
 +	for (tc = 0; tc < priv->params.num_tc; tc++)
 +		for (i = 0; i < priv->params.num_channels; i++)
++=======
+ 	for (tc = 0; tc < priv->channels.params.num_tc; tc++)
+ 		for (i = 0; i < channels->num; i++)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  			for (j = 0; j < NUM_SQ_STATS; j++)
 -				data[idx++] = MLX5E_READ_CTR64_CPU(&channels->c[i]->sq[tc].stats,
 +				data[idx++] = MLX5E_READ_CTR64_CPU(&priv->channel[i]->sq[tc].stats,
  								   sq_stats_desc, j);
  }
  
@@@ -552,8 -547,8 +562,13 @@@ static void mlx5e_get_channels(struct n
  {
  	struct mlx5e_priv *priv = netdev_priv(dev);
  
++<<<<<<< HEAD
 +	ch->max_combined   = mlx5e_get_max_num_channels(priv->mdev);
 +	ch->combined_count = priv->params.num_channels;
++=======
+ 	ch->max_combined   = priv->profile->max_nch(priv->mdev);
+ 	ch->combined_count = priv->channels.params.num_channels;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  }
  
  static int mlx5e_set_channels(struct net_device *dev,
@@@ -628,9 -623,8 +643,9 @@@ static int mlx5e_set_coalesce(struct ne
  {
  	struct mlx5e_priv *priv    = netdev_priv(netdev);
  	struct mlx5_core_dev *mdev = priv->mdev;
 +	struct mlx5e_channel *c;
  	bool restart =
- 		!!coal->use_adaptive_rx_coalesce != priv->params.rx_am_enabled;
+ 		!!coal->use_adaptive_rx_coalesce != priv->channels.params.rx_am_enabled;
  	bool was_opened;
  	int err = 0;
  	int tc;
@@@ -1030,25 -1023,33 +1045,44 @@@ static int mlx5e_set_rxfh(struct net_de
  
  	mutex_lock(&priv->state_lock);
  
 +	if (indir) {
 +		u32 rqtn = priv->indir_rqt.rqtn;
 +
 +		memcpy(priv->params.indirection_rqt, indir,
 +		       sizeof(priv->params.indirection_rqt));
 +		mlx5e_redirect_rqt(priv, rqtn, MLX5E_INDIR_RQT_SIZE, 0);
 +	}
 +
  	if (hfunc != ETH_RSS_HASH_NO_CHANGE &&
- 	    hfunc != priv->params.rss_hfunc) {
- 		priv->params.rss_hfunc = hfunc;
+ 	    hfunc != priv->channels.params.rss_hfunc) {
+ 		priv->channels.params.rss_hfunc = hfunc;
  		hash_changed = true;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (indir) {
+ 		memcpy(priv->channels.params.indirection_rqt, indir,
+ 		       sizeof(priv->channels.params.indirection_rqt));
+ 
+ 		if (test_bit(MLX5E_STATE_OPENED, &priv->state)) {
+ 			u32 rqtn = priv->indir_rqt.rqtn;
+ 			struct mlx5e_redirect_rqt_param rrp = {
+ 				.is_rss = true,
+ 				.rss.hfunc = priv->channels.params.rss_hfunc,
+ 				.rss.channels  = &priv->channels
+ 			};
+ 
+ 			mlx5e_redirect_rqt(priv, rqtn, MLX5E_INDIR_RQT_SIZE, rrp);
+ 		}
+ 	}
+ 
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	if (key) {
- 		memcpy(priv->params.toeplitz_hash_key, key,
- 		       sizeof(priv->params.toeplitz_hash_key));
+ 		memcpy(priv->channels.params.toeplitz_hash_key, key,
+ 		       sizeof(priv->channels.params.toeplitz_hash_key));
  		hash_changed = hash_changed ||
- 			       priv->params.rss_hfunc == ETH_RSS_HASH_TOP;
+ 			       priv->channels.params.rss_hfunc == ETH_RSS_HASH_TOP;
  	}
  
  	if (hash_changed)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index fc92406a15c4,cf8df1d3275e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -48,9 -49,6 +47,12 @@@ struct mlx5e_rq_param 
  struct mlx5e_sq_param {
  	u32                        sqc[MLX5_ST_SZ_DW(sqc)];
  	struct mlx5_wq_param       wq;
++<<<<<<< HEAD
 +	u16                        max_inline;
 +	u8                         min_inline_mode;
 +	enum mlx5e_sq_type         type;
++=======
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  };
  
  struct mlx5e_cq_param {
@@@ -76,44 -75,47 +78,64 @@@ static bool mlx5e_check_fragmented_stri
  		MLX5_CAP_ETH(mdev, reg_umr_sq);
  }
  
- void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type)
+ void mlx5e_set_rq_type_params(struct mlx5_core_dev *mdev,
+ 			      struct mlx5e_params *params, u8 rq_type)
  {
- 	priv->params.rq_wq_type = rq_type;
- 	priv->params.lro_wqe_sz = MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ;
- 	switch (priv->params.rq_wq_type) {
+ 	params->rq_wq_type = rq_type;
+ 	params->lro_wqe_sz = MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ;
+ 	switch (params->rq_wq_type) {
  	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
++<<<<<<< HEAD
 +		priv->params.log_rq_size = MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW;
 +		priv->params.mpwqe_log_stride_sz =
 +			MLX5E_GET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS) ?
 +			MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(priv->mdev) :
 +			MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(priv->mdev);
 +		priv->params.mpwqe_log_num_strides = MLX5_MPWRQ_LOG_WQE_SZ -
 +			priv->params.mpwqe_log_stride_sz;
 +		break;
 +	default: /* MLX5_WQ_TYPE_LINKED_LIST */
 +		priv->params.log_rq_size = MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
++=======
+ 		params->log_rq_size = is_kdump_kernel() ?
+ 			MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW :
+ 			MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW;
+ 		params->mpwqe_log_stride_sz =
+ 			MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS) ?
+ 			MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(mdev) :
+ 			MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev);
+ 		params->mpwqe_log_num_strides = MLX5_MPWRQ_LOG_WQE_SZ -
+ 			params->mpwqe_log_stride_sz;
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_LINKED_LIST */
+ 		params->log_rq_size = is_kdump_kernel() ?
+ 			MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE :
+ 			MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
  		/* Extra room needed for build_skb */
- 		priv->params.lro_wqe_sz -= MLX5_RX_HEADROOM +
+ 		params->lro_wqe_sz -= MLX5_RX_HEADROOM +
  			SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
  	}
- 	priv->params.min_rx_wqes = mlx5_min_rx_wqes(priv->params.rq_wq_type,
- 					       BIT(priv->params.log_rq_size));
  
- 	mlx5_core_info(priv->mdev,
- 		       "MLX5E: StrdRq(%d) RqSz(%ld) StrdSz(%ld) RxCqeCmprss(%d)\n",
- 		       priv->params.rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ,
- 		       BIT(priv->params.log_rq_size),
- 		       BIT(priv->params.mpwqe_log_stride_sz),
- 		       MLX5E_GET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS));
+ 	mlx5_core_info(mdev, "MLX5E: StrdRq(%d) RqSz(%ld) StrdSz(%ld) RxCqeCmprss(%d)\n",
+ 		       params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ,
+ 		       BIT(params->log_rq_size),
+ 		       BIT(params->mpwqe_log_stride_sz),
+ 		       MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS));
  }
  
- static void mlx5e_set_rq_priv_params(struct mlx5e_priv *priv)
+ static void mlx5e_set_rq_params(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
  {
++<<<<<<< HEAD
 +	u8 rq_type = mlx5e_check_fragmented_striding_rq_cap(priv->mdev) ?
++=======
+ 	u8 rq_type = mlx5e_check_fragmented_striding_rq_cap(mdev) &&
+ 		    !params->xdp_prog ?
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  		    MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ :
  		    MLX5_WQ_TYPE_LINKED_LIST;
- 	mlx5e_set_rq_type_params(priv, rq_type);
+ 	mlx5e_set_rq_type_params(mdev, params, rq_type);
  }
  
  static void mlx5e_update_carrier(struct mlx5e_priv *priv)
@@@ -193,8 -200,8 +215,13 @@@ static void mlx5e_update_sw_counters(st
  		s->rx_cache_empty += rq_stats->cache_empty;
  		s->rx_cache_busy  += rq_stats->cache_busy;
  
++<<<<<<< HEAD
 +		for (j = 0; j < priv->params.num_tc; j++) {
 +			sq_stats = &priv->channel[i]->sq[j].stats;
++=======
+ 		for (j = 0; j < priv->channels.params.num_tc; j++) {
+ 			sq_stats = &c->sq[j].stats;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
  			s->tx_packets		+= sq_stats->packets;
  			s->tx_bytes		+= sq_stats->bytes;
@@@ -531,9 -537,10 +558,16 @@@ static int mlx5e_create_rq_umr_mkey(str
  	return mlx5e_create_umr_mkey(priv, num_mtts, PAGE_SHIFT, &rq->umr_mkey);
  }
  
++<<<<<<< HEAD
 +static int mlx5e_create_rq(struct mlx5e_channel *c,
 +			   struct mlx5e_rq_param *param,
 +			   struct mlx5e_rq *rq)
++=======
+ static int mlx5e_alloc_rq(struct mlx5e_channel *c,
+ 			  struct mlx5e_params *params,
+ 			  struct mlx5e_rq_param *rqp,
+ 			  struct mlx5e_rq *rq)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  {
  	struct mlx5e_priv *priv = c->priv;
  	struct mlx5_core_dev *mdev = priv->mdev;
@@@ -565,7 -572,22 +599,26 @@@
  	rq->ix      = c->ix;
  	rq->priv    = c->priv;
  
++<<<<<<< HEAD
 +	switch (priv->params.rq_wq_type) {
++=======
+ 	rq->xdp_prog = params->xdp_prog ? bpf_prog_inc(params->xdp_prog) : NULL;
+ 	if (IS_ERR(rq->xdp_prog)) {
+ 		err = PTR_ERR(rq->xdp_prog);
+ 		rq->xdp_prog = NULL;
+ 		goto err_rq_wq_destroy;
+ 	}
+ 
+ 	if (rq->xdp_prog) {
+ 		rq->buff.map_dir = DMA_BIDIRECTIONAL;
+ 		rq->rx_headroom = XDP_PACKET_HEADROOM;
+ 	} else {
+ 		rq->buff.map_dir = DMA_FROM_DEVICE;
+ 		rq->rx_headroom = MLX5_RX_HEADROOM;
+ 	}
+ 
+ 	switch (rq->wq_type) {
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
  		if (mlx5e_is_vf_vport_rep(priv)) {
  			err = -EINVAL;
@@@ -671,7 -697,8 +723,12 @@@ static void mlx5e_destroy_rq(struct mlx
  	mlx5_wq_destroy(&rq->wq_ctrl);
  }
  
++<<<<<<< HEAD
 +static int mlx5e_enable_rq(struct mlx5e_rq *rq, struct mlx5e_rq_param *param)
++=======
+ static int mlx5e_create_rq(struct mlx5e_rq *rq,
+ 			   struct mlx5e_rq_param *param)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  {
  	struct mlx5e_priv *priv = rq->priv;
  	struct mlx5_core_dev *mdev = priv->mdev;
@@@ -789,6 -816,8 +846,11 @@@ static int mlx5e_wait_for_min_rx_wqes(s
  		msleep(20);
  	}
  
++<<<<<<< HEAD
++=======
+ 	netdev_warn(priv->netdev, "Failed to get min RX wqes on RQN[0x%x] wq cur_sz(%d) min_rx_wqes(%d)\n",
+ 		    rq->rqn, wq->cur_sz, min_wqes);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	return -ETIMEDOUT;
  }
  
@@@ -817,37 -847,29 +880,45 @@@ static int mlx5e_open_rq(struct mlx5e_c
  			 struct mlx5e_rq_param *param,
  			 struct mlx5e_rq *rq)
  {
 +	struct mlx5e_sq *sq = &c->icosq;
 +	u16 pi = sq->pc & sq->wq.sz_m1;
  	int err;
  
++<<<<<<< HEAD
 +	err = mlx5e_create_rq(c, param, rq);
++=======
+ 	err = mlx5e_alloc_rq(c, params, param, rq);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	if (err)
  		return err;
  
 -	err = mlx5e_create_rq(rq, param);
 +	err = mlx5e_enable_rq(rq, param);
  	if (err)
 -		goto err_free_rq;
 +		goto err_destroy_rq;
  
++<<<<<<< HEAD
 +	set_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);
  	err = mlx5e_modify_rq_state(rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);
  	if (err)
 -		goto err_destroy_rq;
 +		goto err_disable_rq;
  
 +	if (param->am_enabled)
++=======
+ 	if (params->rx_am_enabled)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  		set_bit(MLX5E_RQ_STATE_AM, &c->rq.state);
  
 +	sq->db.ico_wqe[pi].opcode     = MLX5_OPCODE_NOP;
 +	sq->db.ico_wqe[pi].num_wqebbs = 1;
 +	mlx5e_send_nop(sq, true); /* trigger mlx5e_post_rx_wqes() */
 +
  	return 0;
  
 +err_disable_rq:
 +	clear_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);
 +	mlx5e_disable_rq(rq);
  err_destroy_rq:
  	mlx5e_destroy_rq(rq);
 -err_free_rq:
 -	mlx5e_free_rq(rq);
  
  	return err;
  }
@@@ -856,14 -891,76 +927,77 @@@ static void mlx5e_close_rq(struct mlx5e
  {
  	clear_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);
  	napi_synchronize(&rq->channel->napi); /* prevent mlx5e_post_rx_wqes */
 -}
 -
 -static void mlx5e_close_rq(struct mlx5e_rq *rq)
 -{
  	cancel_work_sync(&rq->am.work);
 -	mlx5e_destroy_rq(rq);
 +
 +	mlx5e_disable_rq(rq);
  	mlx5e_free_rx_descs(rq);
 -	mlx5e_free_rq(rq);
 +	mlx5e_destroy_rq(rq);
  }
  
++<<<<<<< HEAD
 +static void mlx5e_free_sq_ico_db(struct mlx5e_sq *sq)
++=======
+ static void mlx5e_free_xdpsq_db(struct mlx5e_xdpsq *sq)
+ {
+ 	kfree(sq->db.di);
+ }
+ 
+ static int mlx5e_alloc_xdpsq_db(struct mlx5e_xdpsq *sq, int numa)
+ {
+ 	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
+ 
+ 	sq->db.di = kzalloc_node(sizeof(*sq->db.di) * wq_sz,
+ 				     GFP_KERNEL, numa);
+ 	if (!sq->db.di) {
+ 		mlx5e_free_xdpsq_db(sq);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5e_alloc_xdpsq(struct mlx5e_channel *c,
+ 			     struct mlx5e_params *params,
+ 			     struct mlx5e_sq_param *param,
+ 			     struct mlx5e_xdpsq *sq)
+ {
+ 	void *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);
+ 	struct mlx5e_priv *priv    = c->priv;
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	int err;
+ 
+ 	sq->pdev      = c->pdev;
+ 	sq->mkey_be   = c->mkey_be;
+ 	sq->channel   = c;
+ 	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
+ 	sq->min_inline_mode = params->tx_min_inline_mode;
+ 
+ 	param->wq.db_numa_node = cpu_to_node(c->cpu);
+ 	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq, &sq->wq_ctrl);
+ 	if (err)
+ 		return err;
+ 	sq->wq.db = &sq->wq.db[MLX5_SND_DBR];
+ 
+ 	err = mlx5e_alloc_xdpsq_db(sq, cpu_to_node(c->cpu));
+ 	if (err)
+ 		goto err_sq_wq_destroy;
+ 
+ 	return 0;
+ 
+ err_sq_wq_destroy:
+ 	mlx5_wq_destroy(&sq->wq_ctrl);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_free_xdpsq(struct mlx5e_xdpsq *sq)
+ {
+ 	mlx5e_free_xdpsq_db(sq);
+ 	mlx5_wq_destroy(&sq->wq_ctrl);
+ }
+ 
+ static void mlx5e_free_icosq_db(struct mlx5e_icosq *sq)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  {
  	kfree(sq->db.ico_wqe);
  }
@@@ -880,14 -977,54 +1014,20 @@@ static int mlx5e_alloc_sq_ico_db(struc
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void mlx5e_free_sq_txq_db(struct mlx5e_sq *sq)
++=======
+ static int mlx5e_alloc_icosq(struct mlx5e_channel *c,
+ 			     struct mlx5e_sq_param *param,
+ 			     struct mlx5e_icosq *sq)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  {
 -	void *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);
 -	struct mlx5e_priv *priv    = c->priv;
 -	struct mlx5_core_dev *mdev = priv->mdev;
 -	int err;
 -
 -	sq->pdev      = c->pdev;
 -	sq->mkey_be   = c->mkey_be;
 -	sq->channel   = c;
 -	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
 -
 -	param->wq.db_numa_node = cpu_to_node(c->cpu);
 -	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq, &sq->wq_ctrl);
 -	if (err)
 -		return err;
 -	sq->wq.db = &sq->wq.db[MLX5_SND_DBR];
 -
 -	err = mlx5e_alloc_icosq_db(sq, cpu_to_node(c->cpu));
 -	if (err)
 -		goto err_sq_wq_destroy;
 -
 -	sq->edge = (sq->wq.sz_m1 + 1) - MLX5E_ICOSQ_MAX_WQEBBS;
 -
 -	return 0;
 -
 -err_sq_wq_destroy:
 -	mlx5_wq_destroy(&sq->wq_ctrl);
 -
 -	return err;
 -}
 -
 -static void mlx5e_free_icosq(struct mlx5e_icosq *sq)
 -{
 -	mlx5e_free_icosq_db(sq);
 -	mlx5_wq_destroy(&sq->wq_ctrl);
 -}
 -
 -static void mlx5e_free_txqsq_db(struct mlx5e_txqsq *sq)
 -{
 -	kfree(sq->db.wqe_info);
 -	kfree(sq->db.dma_fifo);
 -	kfree(sq->db.skb);
 +	kfree(sq->db.txq.wqe_info);
 +	kfree(sq->db.txq.dma_fifo);
 +	kfree(sq->db.txq.skb);
  }
  
 -static int mlx5e_alloc_txqsq_db(struct mlx5e_txqsq *sq, int numa)
 +static int mlx5e_alloc_sq_txq_db(struct mlx5e_sq *sq, int numa)
  {
  	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
  	int df_sz = wq_sz * MLX5_SEND_WQEBB_NUM_DS;
@@@ -908,76 -1045,33 +1048,91 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void mlx5e_free_sq_db(struct mlx5e_sq *sq)
++=======
+ static int mlx5e_alloc_txqsq(struct mlx5e_channel *c,
+ 			     int txq_ix,
+ 			     struct mlx5e_params *params,
+ 			     struct mlx5e_sq_param *param,
+ 			     struct mlx5e_txqsq *sq)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  {
 -	void *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);
 -	struct mlx5e_priv *priv    = c->priv;
 +	switch (sq->type) {
 +	case MLX5E_SQ_TXQ:
 +		mlx5e_free_sq_txq_db(sq);
 +		break;
 +	case MLX5E_SQ_ICO:
 +		mlx5e_free_sq_ico_db(sq);
 +		break;
 +	}
 +}
 +
 +static int mlx5e_alloc_sq_db(struct mlx5e_sq *sq, int numa)
 +{
 +	switch (sq->type) {
 +	case MLX5E_SQ_TXQ:
 +		return mlx5e_alloc_sq_txq_db(sq, numa);
 +	case MLX5E_SQ_ICO:
 +		return mlx5e_alloc_sq_ico_db(sq, numa);
 +	}
 +
 +	return 0;
 +}
 +
 +static int mlx5e_create_sq(struct mlx5e_channel *c,
 +			   int tc,
 +			   struct mlx5e_sq_param *param,
 +			   struct mlx5e_sq *sq)
 +{
 +	struct mlx5e_priv *priv = c->priv;
  	struct mlx5_core_dev *mdev = priv->mdev;
 +
 +	void *sqc = param->sqc;
 +	void *sqc_wq = MLX5_ADDR_OF(sqc, sqc, wq);
 +	u16 sq_max_wqebbs;
  	int err;
  
 +	sq->type      = param->type;
  	sq->pdev      = c->pdev;
  	sq->tstamp    = &priv->tstamp;
  	sq->mkey_be   = c->mkey_be;
  	sq->channel   = c;
++<<<<<<< HEAD
 +	sq->tc        = tc;
++=======
+ 	sq->txq_ix    = txq_ix;
+ 	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
+ 	sq->max_inline      = params->tx_max_inline;
+ 	sq->min_inline_mode = params->tx_min_inline_mode;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
 -	param->wq.db_numa_node = cpu_to_node(c->cpu);
 -	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq, &sq->wq_ctrl);
 +	err = mlx5_alloc_map_uar(mdev, &sq->uar, !!MLX5_CAP_GEN(mdev, bf));
  	if (err)
  		return err;
 -	sq->wq.db    = &sq->wq.db[MLX5_SND_DBR];
  
 -	err = mlx5e_alloc_txqsq_db(sq, cpu_to_node(c->cpu));
 +	sq->uar_map = sq->bfreg.map;
 +	param->wq.db_numa_node = cpu_to_node(c->cpu);
 +
 +	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq,
 +				 &sq->wq_ctrl);
 +	if (err)
 +		goto err_unmap_free_uar;
 +
 +	sq->wq.db       = &sq->wq.db[MLX5_SND_DBR];
 +	if (sq->uar.bf_map) {
 +		set_bit(MLX5E_SQ_STATE_BF_ENABLE, &sq->state);
 +		sq->uar_map = sq->uar.bf_map;
 +	} else {
 +		sq->uar_map = sq->uar.map;
 +	}
 +	sq->bf_buf_size = (1 << MLX5_CAP_GEN(mdev, log_bf_reg_size)) / 2;
 +	sq->max_inline  = param->max_inline;
 +	sq->min_inline_mode =
 +		MLX5_CAP_ETH(mdev, wqe_inline_mode) == MLX5_CAP_INLINE_MODE_VPORT_CONTEXT ?
 +		param->min_inline_mode : 0;
 +
 +	err = mlx5e_alloc_sq_db(sq, cpu_to_node(c->cpu));
  	if (err)
  		goto err_sq_wq_destroy;
  
@@@ -1096,42 -1185,63 +1251,93 @@@ static int mlx5e_modify_sq(struct mlx5e
  	return err;
  }
  
 -static void mlx5e_destroy_sq(struct mlx5e_priv *priv, u32 sqn)
 +static void mlx5e_disable_sq(struct mlx5e_sq *sq)
  {
++<<<<<<< HEAD
 +	struct mlx5e_channel *c = sq->channel;
++=======
+ 	mlx5_core_destroy_sq(priv->mdev, sqn);
+ }
+ 
+ static int mlx5e_create_sq_rdy(struct mlx5e_priv *priv,
+ 			       struct mlx5e_sq_param *param,
+ 			       struct mlx5e_create_sq_param *csp,
+ 			       u32 *sqn)
+ {
+ 	struct mlx5e_modify_sq_param msp = {0};
+ 	int err;
+ 
+ 	err = mlx5e_create_sq(priv, param, csp, sqn);
+ 	if (err)
+ 		return err;
+ 
+ 	msp.curr_state = MLX5_SQC_STATE_RST;
+ 	msp.next_state = MLX5_SQC_STATE_RDY;
+ 	err = mlx5e_modify_sq(priv, *sqn, &msp);
+ 	if (err)
+ 		mlx5e_destroy_sq(priv, *sqn);
+ 
+ 	return err;
+ }
+ 
+ static int mlx5e_set_sq_maxrate(struct net_device *dev,
+ 				struct mlx5e_txqsq *sq, u32 rate);
+ 
+ static int mlx5e_open_txqsq(struct mlx5e_channel *c,
+ 			    int tc,
+ 			    int txq_ix,
+ 			    struct mlx5e_params *params,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_txqsq *sq)
+ {
+ 	struct mlx5e_create_sq_param csp = {};
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	struct mlx5e_priv *priv = c->priv;
 -	u32 tx_rate;
 +	struct mlx5_core_dev *mdev = priv->mdev;
 +
 +	mlx5_core_destroy_sq(mdev, sq->sqn);
 +	if (sq->rate_limit)
 +		mlx5_rl_remove_rate(mdev, sq->rate_limit);
 +}
 +
 +static int mlx5e_open_sq(struct mlx5e_channel *c,
 +			 int tc,
 +			 struct mlx5e_sq_param *param,
 +			 struct mlx5e_sq *sq)
 +{
  	int err;
  
++<<<<<<< HEAD
 +	err = mlx5e_create_sq(c, tc, param, sq);
 +	if (err)
 +		return err;
 +
 +	err = mlx5e_enable_sq(sq, param);
++=======
+ 	err = mlx5e_alloc_txqsq(c, txq_ix, params, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.tisn            = priv->tisn[tc];
+ 	csp.tis_lst_sz      = 1;
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = sq->min_inline_mode;
+ 	err = mlx5e_create_sq_rdy(c->priv, param, &csp, &sq->sqn);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
 +	if (err)
 +		goto err_destroy_sq;
 +
 +	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 +	err = mlx5e_modify_sq(sq, MLX5_SQC_STATE_RST, MLX5_SQC_STATE_RDY,
 +			      false, 0);
  	if (err)
 -		goto err_free_txqsq;
 +		goto err_disable_sq;
  
 -	tx_rate = priv->tx_rates[sq->txq_ix];
 -	if (tx_rate)
 -		mlx5e_set_sq_maxrate(priv->netdev, sq, tx_rate);
 +	if (sq->txq) {
 +		netdev_tx_reset_queue(sq->txq);
 +		netif_tx_start_queue(sq->txq);
 +	}
  
  	return 0;
  
@@@ -1151,30 -1269,148 +1357,125 @@@ static inline void netif_tx_disable_que
  	__netif_tx_unlock_bh(txq);
  }
  
 -static void mlx5e_deactivate_txqsq(struct mlx5e_txqsq *sq)
 +static void mlx5e_close_sq(struct mlx5e_sq *sq)
  {
 -	struct mlx5e_channel *c = sq->channel;
 -
  	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
  	/* prevent netif_tx_wake_queue */
 -	napi_synchronize(&c->napi);
 +	napi_synchronize(&sq->channel->napi);
  
 -	netif_tx_disable_queue(sq->txq);
 +	if (sq->txq) {
 +		netif_tx_disable_queue(sq->txq);
  
++<<<<<<< HEAD
 +		/* last doorbell out, godspeed .. */
 +		if (mlx5e_sq_has_room_for(sq, 1)) {
 +			sq->db.txq.skb[(sq->pc & sq->wq.sz_m1)] = NULL;
 +			mlx5e_send_nop(sq, true);
 +		}
++=======
+ 	/* last doorbell out, godspeed .. */
+ 	if (mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, 1)) {
+ 		struct mlx5e_tx_wqe *nop;
+ 
+ 		sq->db.skb[(sq->pc & sq->wq.sz_m1)] = NULL;
+ 		nop = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+ 		mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &nop->ctrl);
+ 	}
+ }
+ 
+ static void mlx5e_close_txqsq(struct mlx5e_txqsq *sq)
+ {
+ 	struct mlx5e_channel *c = sq->channel;
+ 	struct mlx5e_priv *priv = c->priv;
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 
+ 	mlx5e_destroy_sq(priv, sq->sqn);
+ 	if (sq->rate_limit)
+ 		mlx5_rl_remove_rate(mdev, sq->rate_limit);
+ 	mlx5e_free_txqsq_descs(sq);
+ 	mlx5e_free_txqsq(sq);
+ }
+ 
+ static int mlx5e_open_icosq(struct mlx5e_channel *c,
+ 			    struct mlx5e_params *params,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_icosq *sq)
+ {
+ 	struct mlx5e_create_sq_param csp = {};
+ 	int err;
+ 
+ 	err = mlx5e_alloc_icosq(c, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = params->tx_min_inline_mode;
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_create_sq_rdy(c->priv, param, &csp, &sq->sqn);
+ 	if (err)
+ 		goto err_free_icosq;
+ 
+ 	return 0;
+ 
+ err_free_icosq:
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	mlx5e_free_icosq(sq);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_close_icosq(struct mlx5e_icosq *sq)
+ {
+ 	struct mlx5e_channel *c = sq->channel;
+ 
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	napi_synchronize(&c->napi);
+ 
+ 	mlx5e_destroy_sq(c->priv, sq->sqn);
+ 	mlx5e_free_icosq(sq);
+ }
+ 
+ static int mlx5e_open_xdpsq(struct mlx5e_channel *c,
+ 			    struct mlx5e_params *params,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_xdpsq *sq)
+ {
+ 	unsigned int ds_cnt = MLX5E_XDP_TX_DS_COUNT;
+ 	struct mlx5e_create_sq_param csp = {};
+ 	struct mlx5e_priv *priv = c->priv;
+ 	unsigned int inline_hdr_sz = 0;
+ 	int err;
+ 	int i;
+ 
+ 	err = mlx5e_alloc_xdpsq(c, params, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.tis_lst_sz      = 1;
+ 	csp.tisn            = priv->tisn[0]; /* tc = 0 */
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = sq->min_inline_mode;
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_create_sq_rdy(c->priv, param, &csp, &sq->sqn);
+ 	if (err)
+ 		goto err_free_xdpsq;
+ 
+ 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+ 		inline_hdr_sz = MLX5E_XDP_MIN_INLINE;
+ 		ds_cnt++;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	}
  
 -	/* Pre initialize fixed WQE fields */
 -	for (i = 0; i < mlx5_wq_cyc_get_size(&sq->wq); i++) {
 -		struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(&sq->wq, i);
 -		struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
 -		struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
 -		struct mlx5_wqe_data_seg *dseg;
 -
 -		cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_cnt);
 -		eseg->inline_hdr.sz = cpu_to_be16(inline_hdr_sz);
 -
 -		dseg = (struct mlx5_wqe_data_seg *)cseg + (ds_cnt - 1);
 -		dseg->lkey = sq->mkey_be;
 -	}
 -
 -	return 0;
 -
 -err_free_xdpsq:
 -	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 -	mlx5e_free_xdpsq(sq);
 -
 -	return err;
 +	mlx5e_disable_sq(sq);
 +	mlx5e_free_tx_descs(sq);
 +	mlx5e_destroy_sq(sq);
  }
  
 -static void mlx5e_close_xdpsq(struct mlx5e_xdpsq *sq)
 -{
 -	struct mlx5e_channel *c = sq->channel;
 -
 -	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 -	napi_synchronize(&c->napi);
 -
 -	mlx5e_destroy_sq(c->priv, sq->sqn);
 -	mlx5e_free_xdpsq_descs(sq);
 -	mlx5e_free_xdpsq(sq);
 -}
 -
 -static int mlx5e_alloc_cq(struct mlx5e_channel *c,
 -			  struct mlx5e_cq_param *param,
 -			  struct mlx5e_cq *cq)
 +static int mlx5e_create_cq(struct mlx5e_channel *c,
 +			   struct mlx5e_cq_param *param,
 +			   struct mlx5e_cq *cq)
  {
  	struct mlx5e_priv *priv = c->priv;
  	struct mlx5_core_dev *mdev = priv->mdev;
@@@ -1293,18 -1528,16 +1594,16 @@@ static int mlx5e_open_cq(struct mlx5e_c
  	if (err)
  		return err;
  
 -	err = mlx5e_create_cq(cq, param);
 +	err = mlx5e_enable_cq(cq, param);
  	if (err)
 -		goto err_free_cq;
 +		goto err_destroy_cq;
  
  	if (MLX5_CAP_GEN(mdev, cq_moderation))
- 		mlx5_core_modify_cq_moderation(mdev, &cq->mcq,
- 					       moderation.usec,
- 					       moderation.pkts);
+ 		mlx5_core_modify_cq_moderation(mdev, &cq->mcq, moder.usec, moder.pkts);
  	return 0;
  
 -err_free_cq:
 -	mlx5e_free_cq(cq);
 +err_destroy_cq:
 +	mlx5e_destroy_cq(cq);
  
  	return err;
  }
@@@ -1357,8 -1591,10 +1657,15 @@@ static int mlx5e_open_sqs(struct mlx5e_
  	int err;
  	int tc;
  
++<<<<<<< HEAD
 +	for (tc = 0; tc < c->num_tc; tc++) {
 +		err = mlx5e_open_sq(c, tc, &cparam->sq, &c->sq[tc]);
++=======
+ 	for (tc = 0; tc < params->num_tc; tc++) {
+ 		int txq_ix = c->ix + tc * params->num_channels;
+ 
+ 		err = mlx5e_open_txqsq(c, tc, txq_ix, params, &cparam->sq, &c->sq[tc]);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  		if (err)
  			goto err_close_sqs;
  	}
@@@ -1462,18 -1693,24 +1769,18 @@@ static int mlx5e_set_tx_maxrate(struct 
  	return err;
  }
  
 -static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 -{
 -	return is_kdump_kernel() ?
 -		MLX5E_MIN_NUM_CHANNELS :
 -		min_t(int, mdev->priv.eq_table.num_comp_vectors,
 -		      MLX5E_MAX_NUM_CHANNELS);
 -}
 -
  static int mlx5e_open_channel(struct mlx5e_priv *priv, int ix,
+ 			      struct mlx5e_params *params,
  			      struct mlx5e_channel_param *cparam,
  			      struct mlx5e_channel **cp)
  {
- 	struct mlx5e_cq_moder icosq_cq_moder = {0, 0};
+ 	struct mlx5e_cq_moder icocq_moder = {0, 0};
  	struct net_device *netdev = priv->netdev;
- 	struct mlx5e_cq_moder rx_cq_profile;
  	int cpu = mlx5e_get_cpu(priv, ix);
  	struct mlx5e_channel *c;
 +	struct mlx5e_sq *sq;
  	int err;
 +	int i;
  
  	c = kzalloc_node(sizeof(*c), GFP_KERNEL, cpu_to_node(cpu));
  	if (!c)
@@@ -1485,14 -1722,8 +1792,19 @@@
  	c->pdev     = &priv->mdev->pdev->dev;
  	c->netdev   = priv->netdev;
  	c->mkey_be  = cpu_to_be32(priv->mdev->mlx5e_res.mkey.key);
++<<<<<<< HEAD
 +	c->num_tc   = priv->params.num_tc;
 +
 +	if (priv->params.rx_am_enabled)
 +		rx_cq_profile = mlx5e_am_get_def_profile(priv->params.rx_cq_period_mode);
 +	else
 +		rx_cq_profile = priv->params.rx_cq_moderation;
++=======
+ 	c->num_tc   = params->num_tc;
+ 	c->xdp      = !!params->xdp_prog;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
 +
 +	mlx5e_build_channeltc_to_txq_map(priv, ix);
  
  	netif_napi_add(netdev, &c->napi, mlx5e_napi_poll, 64);
  
@@@ -1509,9 -1739,15 +1820,21 @@@
  	if (err)
  		goto err_close_tx_cqs;
  
++<<<<<<< HEAD
 +	napi_enable(&c->napi);
 +
 +	err = mlx5e_open_sq(c, 0, &cparam->icosq, &c->icosq);
++=======
+ 	/* XDP SQ CQ params are same as normal TXQ sq CQ params */
+ 	err = c->xdp ? mlx5e_open_cq(c, params->tx_cq_moderation,
+ 				     &cparam->tx_cq, &c->rq.xdpsq.cq) : 0;
+ 	if (err)
+ 		goto err_close_rx_cq;
+ 
+ 	napi_enable(&c->napi);
+ 
+ 	err = mlx5e_open_icosq(c, params, &cparam->icosq, &c->icosq);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	if (err)
  		goto err_disable_napi;
  
@@@ -1519,21 -1755,14 +1842,27 @@@
  	if (err)
  		goto err_close_icosq;
  
++<<<<<<< HEAD
 +	for (i = 0; i < priv->params.num_tc; i++) {
 +		u32 txq_ix = priv->channeltc_to_txq_map[ix][i];
 +
 +		if (priv->tx_rates[txq_ix]) {
 +			sq = priv->txq_to_sq_map[txq_ix];
 +			mlx5e_set_sq_maxrate(priv->netdev, sq,
 +					     priv->tx_rates[txq_ix]);
 +		}
 +	}
++=======
+ 	err = c->xdp ? mlx5e_open_xdpsq(c, params, &cparam->xdp_sq, &c->rq.xdpsq) : 0;
+ 	if (err)
+ 		goto err_close_sqs;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
- 	err = mlx5e_open_rq(c, &cparam->rq, &c->rq);
+ 	err = mlx5e_open_rq(c, params, &cparam->rq, &c->rq);
  	if (err)
 -		goto err_close_xdp_sq;
 +		goto err_close_sqs;
  
 +	netif_set_xps_queue(netdev, get_cpu_mask(c->cpu), ix);
  	*cp = c;
  
  	return 0;
@@@ -1633,11 -1891,7 +1961,15 @@@ static void mlx5e_build_sq_param(struc
  	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
  
  	mlx5e_build_sq_param_common(priv, param);
++<<<<<<< HEAD
 +	MLX5_SET(wq, wq, log_wq_sz,     priv->params.log_sq_size);
 +
 +	param->max_inline = priv->params.tx_max_inline;
 +	param->min_inline_mode = priv->params.tx_min_inline_mode;
 +	param->type = MLX5E_SQ_TXQ;
++=======
+ 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  }
  
  static void mlx5e_build_common_cq_param(struct mlx5e_priv *priv,
@@@ -1710,51 -1967,51 +2045,89 @@@ static void mlx5e_build_icosq_param(str
  
  	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
  	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(priv->mdev, reg_umr_sq));
 -}
  
++<<<<<<< HEAD
 +	param->type = MLX5E_SQ_ICO;
++=======
+ static void mlx5e_build_xdpsq_param(struct mlx5e_priv *priv,
+ 				    struct mlx5e_params *params,
+ 				    struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	mlx5e_build_sq_param_common(priv, param);
+ 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  }
  
- static void mlx5e_build_channel_param(struct mlx5e_priv *priv, struct mlx5e_channel_param *cparam)
+ static void mlx5e_build_channel_param(struct mlx5e_priv *priv,
+ 				      struct mlx5e_params *params,
+ 				      struct mlx5e_channel_param *cparam)
  {
  	u8 icosq_log_wq_sz = MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
  
++<<<<<<< HEAD
 +	mlx5e_build_rq_param(priv, &cparam->rq);
 +	mlx5e_build_sq_param(priv, &cparam->sq);
 +	mlx5e_build_icosq_param(priv, &cparam->icosq, icosq_log_wq_sz);
 +	mlx5e_build_rx_cq_param(priv, &cparam->rx_cq);
 +	mlx5e_build_tx_cq_param(priv, &cparam->tx_cq);
 +	mlx5e_build_ico_cq_param(priv, &cparam->icosq_cq, icosq_log_wq_sz);
 +}
 +
 +static int mlx5e_open_channels(struct mlx5e_priv *priv)
++=======
+ 	mlx5e_build_rq_param(priv, params, &cparam->rq);
+ 	mlx5e_build_sq_param(priv, params, &cparam->sq);
+ 	mlx5e_build_xdpsq_param(priv, params, &cparam->xdp_sq);
+ 	mlx5e_build_icosq_param(priv, icosq_log_wq_sz, &cparam->icosq);
+ 	mlx5e_build_rx_cq_param(priv, params, &cparam->rx_cq);
+ 	mlx5e_build_tx_cq_param(priv, params, &cparam->tx_cq);
+ 	mlx5e_build_ico_cq_param(priv, icosq_log_wq_sz, &cparam->icosq_cq);
+ }
+ 
+ static int mlx5e_open_channels(struct mlx5e_priv *priv,
+ 			       struct mlx5e_channels *chs)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  {
  	struct mlx5e_channel_param *cparam;
 +	int nch = priv->params.num_channels;
  	int err = -ENOMEM;
  	int i;
 +	int j;
 +
++<<<<<<< HEAD
 +	priv->channel = kcalloc(nch, sizeof(struct mlx5e_channel *),
 +				GFP_KERNEL);
  
 +	priv->txq_to_sq_map = kcalloc(nch * priv->params.num_tc,
 +				      sizeof(struct mlx5e_sq *), GFP_KERNEL);
++=======
+ 	chs->num = chs->params.num_channels;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
 -	chs->c = kcalloc(chs->num, sizeof(struct mlx5e_channel *), GFP_KERNEL);
  	cparam = kzalloc(sizeof(struct mlx5e_channel_param), GFP_KERNEL);
 -	if (!chs->c || !cparam)
 -		goto err_free;
  
 +	if (!priv->channel || !priv->txq_to_sq_map || !cparam)
 +		goto err_free_txq_to_sq_map;
 +
++<<<<<<< HEAD
 +	mlx5e_build_channel_param(priv, cparam);
 +
 +	for (i = 0; i < nch; i++) {
 +		err = mlx5e_open_channel(priv, i, cparam, &priv->channel[i]);
++=======
+ 	mlx5e_build_channel_param(priv, &chs->params, cparam);
+ 	for (i = 0; i < chs->num; i++) {
+ 		err = mlx5e_open_channel(priv, i, &chs->params, cparam, &chs->c[i]);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
 +		if (err)
 +			goto err_close_channels;
 +	}
 +
 +	for (j = 0; j < nch; j++) {
 +		err = mlx5e_wait_for_min_rx_wqes(&priv->channel[j]->rq);
  		if (err)
  			goto err_close_channels;
  	}
@@@ -1818,33 -2056,23 +2191,44 @@@ static void mlx5e_fill_indir_rqt_rqns(s
  {
  	int i;
  
 -	for (i = 0; i < chs->num; i++)
 -		mlx5e_deactivate_channel(chs->c[i]);
 +	for (i = 0; i < MLX5E_INDIR_RQT_SIZE; i++) {
 +		int ix = i;
 +		u32 rqn;
 +
 +		if (priv->params.rss_hfunc == ETH_RSS_HASH_XOR)
 +			ix = mlx5e_bits_invert(i, MLX5E_LOG_INDIR_RQT_SIZE);
 +
++<<<<<<< HEAD
 +		ix = priv->params.indirection_rqt[ix];
 +		rqn = test_bit(MLX5E_STATE_OPENED, &priv->state) ?
 +				priv->channel[ix]->rq.rqn :
 +				priv->drop_rq.rqn;
++=======
++			if (rrp.rss.hfunc == ETH_RSS_HASH_XOR)
++				ix = mlx5e_bits_invert(i, ilog2(sz));
++
++			ix = priv->channels.params.indirection_rqt[ix];
++			rqn = rrp.rss.channels->c[ix]->rq.rqn;
++		} else {
++			rqn = rrp.rqn;
++		}
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
 +		MLX5_SET(rqtc, rqtc, rq_num[i], rqn);
 +	}
  }
  
 -static void mlx5e_close_channels(struct mlx5e_channels *chs)
 +static void mlx5e_fill_direct_rqt_rqn(struct mlx5e_priv *priv, void *rqtc,
 +				      int ix)
  {
 -	int i;
 +	u32 rqn = test_bit(MLX5E_STATE_OPENED, &priv->state) ?
 +			priv->channel[ix]->rq.rqn :
 +			priv->drop_rq.rqn;
  
 -	for (i = 0; i < chs->num; i++)
 -		mlx5e_close_channel(chs->c[i]);
 -
 -	kfree(chs->c);
 -	chs->num = 0;
 +	MLX5_SET(rqtc, rqtc, rq_num[0], rqn);
  }
  
 -static int
 -mlx5e_create_rqt(struct mlx5e_priv *priv, int sz, struct mlx5e_rqt *rqt)
 +static int mlx5e_create_rqt(struct mlx5e_priv *priv, int sz,
 +			    int ix, struct mlx5e_rqt *rqt)
  {
  	struct mlx5_core_dev *mdev = priv->mdev;
  	void *rqtc;
@@@ -1958,9 -2242,31 +2342,35 @@@ static void mlx5e_redirect_rqts(struct 
  	}
  }
  
++<<<<<<< HEAD
 +static void mlx5e_build_tir_ctx_lro(void *tirc, struct mlx5e_priv *priv)
++=======
+ static void mlx5e_redirect_rqts_to_channels(struct mlx5e_priv *priv,
+ 					    struct mlx5e_channels *chs)
  {
- 	if (!priv->params.lro_en)
+ 	struct mlx5e_redirect_rqt_param rrp = {
+ 		.is_rss        = true,
+ 		.rss.channels  = chs,
+ 		.rss.hfunc     = chs->params.rss_hfunc
+ 	};
+ 
+ 	mlx5e_redirect_rqts(priv, rrp);
+ }
+ 
+ static void mlx5e_redirect_rqts_to_drop(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5e_redirect_rqt_param drop_rrp = {
+ 		.is_rss = false,
+ 		.rqn = priv->drop_rq.rqn
+ 	};
+ 
+ 	mlx5e_redirect_rqts(priv, drop_rrp);
+ }
+ 
+ static void mlx5e_build_tir_ctx_lro(struct mlx5e_params *params, void *tirc)
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
+ {
+ 	if (!params->lro_en)
  		return;
  
  #define ROUGH_MAX_L2_L3_HDR_SZ 256
@@@ -2206,25 -2548,17 +2615,25 @@@ int mlx5e_open_locked(struct net_devic
  
  	mlx5e_netdev_set_tcs(netdev);
  
- 	num_txqs = priv->params.num_channels * priv->params.num_tc;
+ 	num_txqs = priv->channels.params.num_channels * priv->channels.params.num_tc;
  	netif_set_real_num_tx_queues(netdev, num_txqs);
- 	netif_set_real_num_rx_queues(netdev, priv->params.num_channels);
+ 	netif_set_real_num_rx_queues(netdev, priv->channels.params.num_channels);
  
 -	err = mlx5e_open_channels(priv, &priv->channels);
 -	if (err)
 +	err = mlx5e_open_channels(priv);
 +	if (err) {
 +		netdev_err(netdev, "%s: mlx5e_open_channels failed, %d\n",
 +			   __func__, err);
  		goto err_clear_state_opened_flag;
 +	}
 +
 +	err = mlx5e_refresh_tirs_self_loopback(priv->mdev, false);
 +	if (err) {
 +		netdev_err(netdev, "%s: mlx5e_refresh_tirs_self_loopback_enable failed, %d\n",
 +			   __func__, err);
 +		goto err_close_channels;
 +	}
  
 -	mlx5e_refresh_tirs(priv, false);
 -	mlx5e_activate_priv_channels(priv);
 -	mlx5e_redirect_rqts_to_channels(priv, &priv->channels);
 +	mlx5e_redirect_rqts(priv);
  	mlx5e_update_carrier(priv);
  	mlx5e_timestamp_init(priv);
  
@@@ -2782,11 -3113,15 +3190,19 @@@ static int set_feature_rx_vlan(struct n
  
  	mutex_lock(&priv->state_lock);
  
++<<<<<<< HEAD
 +	priv->params.vlan_strip_disable = !enable;
 +	err = mlx5e_modify_rqs_vsd(priv, !enable);
++=======
+ 	priv->channels.params.vlan_strip_disable = !enable;
+ 	if (!test_bit(MLX5E_STATE_OPENED, &priv->state))
+ 		goto unlock;
+ 
+ 	err = mlx5e_modify_channels_vsd(&priv->channels, !enable);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	if (err)
- 		priv->params.vlan_strip_disable = enable;
+ 		priv->channels.params.vlan_strip_disable = enable;
  
 -unlock:
  	mutex_unlock(&priv->state_lock);
  
  	return err;
@@@ -2867,22 -3196,10 +3283,22 @@@ static int mlx5e_change_mtu(struct net_
  	int err = 0;
  	bool reset;
  
 +	mlx5_query_port_max_mtu(mdev, &max_mtu, 1);
 +
 +	max_mtu = MLX5E_HW2SW_MTU(max_mtu);
 +	min_mtu = MLX5E_HW2SW_MTU(MXL5E_MIN_MTU);
 +
 +	if (new_mtu > max_mtu || new_mtu < min_mtu) {
 +		netdev_err(netdev,
 +			   "%s: Bad MTU (%d), valid range is: [%d..%d]\n",
 +			   __func__, new_mtu, min_mtu, max_mtu);
 +		return -EINVAL;
 +	}
 +
  	mutex_lock(&priv->state_lock);
  
- 	reset = !priv->params.lro_en &&
- 		(priv->params.rq_wq_type !=
+ 	reset = !priv->channels.params.lro_en &&
+ 		(priv->channels.params.rq_wq_type !=
  		 MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ);
  
  	was_opened = test_bit(MLX5E_STATE_OPENED, &priv->state);
@@@ -3101,8 -3418,8 +3517,13 @@@ static void mlx5e_tx_timeout(struct net
  
  	netdev_err(dev, "TX timeout detected\n");
  
++<<<<<<< HEAD
 +	for (i = 0; i < priv->params.num_channels * priv->params.num_tc; i++) {
 +		struct mlx5e_sq *sq = priv->txq_to_sq_map[i];
++=======
+ 	for (i = 0; i < priv->channels.num * priv->channels.params.num_tc; i++) {
+ 		struct mlx5e_txqsq *sq = priv->txq2sq[i];
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
  		if (!netif_xmit_stopped(netdev_get_tx_queue(dev, i)))
  			continue;
@@@ -3116,6 -3433,101 +3537,104 @@@
  		schedule_work(&priv->tx_timeout_work);
  }
  
++<<<<<<< HEAD
++=======
+ static int mlx5e_xdp_set(struct net_device *netdev, struct bpf_prog *prog)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(netdev);
+ 	struct bpf_prog *old_prog;
+ 	int err = 0;
+ 	bool reset, was_opened;
+ 	int i;
+ 
+ 	mutex_lock(&priv->state_lock);
+ 
+ 	if ((netdev->features & NETIF_F_LRO) && prog) {
+ 		netdev_warn(netdev, "can't set XDP while LRO is on, disable LRO first\n");
+ 		err = -EINVAL;
+ 		goto unlock;
+ 	}
+ 
+ 	was_opened = test_bit(MLX5E_STATE_OPENED, &priv->state);
+ 	/* no need for full reset when exchanging programs */
+ 	reset = (!priv->channels.params.xdp_prog || !prog);
+ 
+ 	if (was_opened && reset)
+ 		mlx5e_close_locked(netdev);
+ 	if (was_opened && !reset) {
+ 		/* num_channels is invariant here, so we can take the
+ 		 * batched reference right upfront.
+ 		 */
+ 		prog = bpf_prog_add(prog, priv->channels.num);
+ 		if (IS_ERR(prog)) {
+ 			err = PTR_ERR(prog);
+ 			goto unlock;
+ 		}
+ 	}
+ 
+ 	/* exchange programs, extra prog reference we got from caller
+ 	 * as long as we don't fail from this point onwards.
+ 	 */
+ 	old_prog = xchg(&priv->channels.params.xdp_prog, prog);
+ 	if (old_prog)
+ 		bpf_prog_put(old_prog);
+ 
+ 	if (reset) /* change RQ type according to priv->xdp_prog */
+ 		mlx5e_set_rq_params(priv->mdev, &priv->channels.params);
+ 
+ 	if (was_opened && reset)
+ 		mlx5e_open_locked(netdev);
+ 
+ 	if (!test_bit(MLX5E_STATE_OPENED, &priv->state) || reset)
+ 		goto unlock;
+ 
+ 	/* exchanging programs w/o reset, we update ref counts on behalf
+ 	 * of the channels RQs here.
+ 	 */
+ 	for (i = 0; i < priv->channels.num; i++) {
+ 		struct mlx5e_channel *c = priv->channels.c[i];
+ 
+ 		clear_bit(MLX5E_RQ_STATE_ENABLED, &c->rq.state);
+ 		napi_synchronize(&c->napi);
+ 		/* prevent mlx5e_poll_rx_cq from accessing rq->xdp_prog */
+ 
+ 		old_prog = xchg(&c->rq.xdp_prog, prog);
+ 
+ 		set_bit(MLX5E_RQ_STATE_ENABLED, &c->rq.state);
+ 		/* napi_schedule in case we have missed anything */
+ 		set_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags);
+ 		napi_schedule(&c->napi);
+ 
+ 		if (old_prog)
+ 			bpf_prog_put(old_prog);
+ 	}
+ 
+ unlock:
+ 	mutex_unlock(&priv->state_lock);
+ 	return err;
+ }
+ 
+ static bool mlx5e_xdp_attached(struct net_device *dev)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 
+ 	return !!priv->channels.params.xdp_prog;
+ }
+ 
+ static int mlx5e_xdp(struct net_device *dev, struct netdev_xdp *xdp)
+ {
+ 	switch (xdp->command) {
+ 	case XDP_SETUP_PROG:
+ 		return mlx5e_xdp_set(dev, xdp->prog);
+ 	case XDP_QUERY_PROG:
+ 		xdp->prog_attached = mlx5e_xdp_attached(dev);
+ 		return 0;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  #ifdef CONFIG_NET_POLL_CONTROLLER
  /* Fake "interrupt" called by netpoll (eg netconsole) to send skbs without
   * reenabling interrupts.
@@@ -3315,63 -3792,13 +3897,61 @@@ static void mlx5e_build_nic_netdev_priv
  					void *ppriv)
  {
  	struct mlx5e_priv *priv = netdev_priv(netdev);
- 	u32 link_speed = 0;
- 	u32 pci_bw = 0;
- 	u8 cq_period_mode = MLX5_CAP_GEN(mdev, cq_period_start_from_cqe) ?
- 					 MLX5_CQ_PERIOD_MODE_START_FROM_CQE :
- 					 MLX5_CQ_PERIOD_MODE_START_FROM_EQE;
  
- 	priv->mdev                         = mdev;
- 	priv->netdev                       = netdev;
- 	priv->params.num_channels          = profile->max_nch(mdev);
- 	priv->profile                      = profile;
- 	priv->ppriv                        = ppriv;
+ 	priv->mdev        = mdev;
+ 	priv->netdev      = netdev;
+ 	priv->profile     = profile;
+ 	priv->ppriv       = ppriv;
  
++<<<<<<< HEAD
 +	priv->params.lro_timeout =
 +		mlx5e_choose_lro_timeout(mdev, MLX5E_DEFAULT_LRO_TIMEOUT);
 +
 +	priv->params.log_sq_size = MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
 +
 +	/* set CQE compression */
 +	priv->params.rx_cqe_compress_def = false;
 +	if (MLX5_CAP_GEN(mdev, cqe_compression) &&
 +	    MLX5_CAP_GEN(mdev, vport_group_manager)) {
 +		mlx5e_get_max_linkspeed(mdev, &link_speed);
 +		mlx5e_get_pci_bw(mdev, &pci_bw);
 +		mlx5_core_dbg(mdev, "Max link speed = %d, PCI BW = %d\n",
 +			      link_speed, pci_bw);
 +		priv->params.rx_cqe_compress_def =
 +			cqe_compress_heuristic(link_speed, pci_bw);
 +	}
 +
 +	MLX5E_SET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS,
 +			priv->params.rx_cqe_compress_def);
 +
 +	mlx5e_set_rq_priv_params(priv);
 +	if (priv->params.rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ)
 +		priv->params.lro_en = true;
 +
 +	priv->params.rx_am_enabled = MLX5_CAP_GEN(mdev, cq_moderation);
 +	mlx5e_set_rx_cq_mode_params(&priv->params, cq_period_mode);
 +
 +	priv->params.tx_cq_moderation.usec =
 +		MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC;
 +	priv->params.tx_cq_moderation.pkts =
 +		MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS;
 +	priv->params.tx_max_inline         = mlx5e_get_max_inline_cap(mdev);
 +	mlx5_query_min_inline(mdev, &priv->params.tx_min_inline_mode);
 +	priv->params.num_tc                = 1;
 +	priv->params.rss_hfunc             = ETH_RSS_HASH_XOR;
 +
 +	netdev_rss_key_fill(priv->params.toeplitz_hash_key,
 +			    sizeof(priv->params.toeplitz_hash_key));
 +
 +	mlx5e_build_default_indir_rqt(mdev, priv->params.indirection_rqt,
 +				      MLX5E_INDIR_RQT_SIZE, profile->max_nch(mdev));
 +
 +	/* Initialize pflags */
 +	MLX5E_SET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_BASED_MODER,
 +			priv->params.rx_cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE);
++=======
+ 	mlx5e_build_nic_params(mdev, &priv->channels.params, profile->max_nch(mdev));
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
  	mutex_init(&priv->state_lock);
  
@@@ -3522,6 -3949,8 +4102,11 @@@ static void mlx5e_nic_cleanup(struct ml
  {
  	mlx5e_vxlan_cleanup(priv);
  
++<<<<<<< HEAD
++=======
+ 	if (priv->channels.params.xdp_prog)
+ 		bpf_prog_put(priv->channels.params.xdp_prog);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  }
  
  static int mlx5e_init_nic_rx(struct mlx5e_priv *priv)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index abcb1976163d,d277c1979b2a..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@@ -108,8 -110,8 +108,13 @@@ static void mlx5e_rep_update_sw_counter
  		s->rx_packets	+= rq_stats->packets;
  		s->rx_bytes	+= rq_stats->bytes;
  
++<<<<<<< HEAD
 +		for (j = 0; j < priv->params.num_tc; j++) {
 +			sq_stats = &priv->channel[i]->sq[j].stats;
++=======
+ 		for (j = 0; j < priv->channels.params.num_tc; j++) {
+ 			sq_stats = &c->sq[j].stats;
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  
  			s->tx_packets		+= sq_stats->packets;
  			s->tx_bytes		+= sq_stats->bytes;
@@@ -190,7 -192,7 +195,11 @@@ int mlx5e_add_sqs_fwd_rules(struct mlx5
  	int n, tc, err, num_sqs = 0;
  	u16 *sqs;
  
++<<<<<<< HEAD
 +	sqs = kcalloc(priv->params.num_channels * priv->params.num_tc, sizeof(u16), GFP_KERNEL);
++=======
+ 	sqs = kcalloc(priv->channels.num * priv->channels.params.num_tc, sizeof(u16), GFP_KERNEL);
++>>>>>>> 6a9764efb255 (net/mlx5e: Isolate open_channels from priv->params)
  	if (!sqs)
  		return -ENOMEM;
  
@@@ -392,21 -392,16 +401,18 @@@ static const struct net_device_ops mlx5
  	.ndo_open                = mlx5e_rep_open,
  	.ndo_stop                = mlx5e_rep_close,
  	.ndo_start_xmit          = mlx5e_xmit,
 -	.ndo_get_phys_port_name  = mlx5e_rep_get_phys_port_name,
 +	.extended.ndo_get_phys_port_name  = mlx5e_rep_get_phys_port_name,
  	.ndo_setup_tc            = mlx5e_rep_ndo_setup_tc,
  	.ndo_get_stats64         = mlx5e_rep_get_stats,
 -	.ndo_has_offload_stats	 = mlx5e_has_offload_stats,
 -	.ndo_get_offload_stats	 = mlx5e_get_offload_stats,
 +	.extended.ndo_udp_tunnel_add      = mlx5e_add_vxlan_port,
 +	.extended.ndo_udp_tunnel_del      = mlx5e_del_vxlan_port,
 +	.extended.ndo_has_offload_stats	 = mlx5e_has_offload_stats,
 +	.extended.ndo_get_offload_stats	 = mlx5e_get_offload_stats,
  };
  
- static void mlx5e_build_rep_netdev_priv(struct mlx5_core_dev *mdev,
- 					struct net_device *netdev,
- 					const struct mlx5e_profile *profile,
- 					void *ppriv)
+ static void mlx5e_build_rep_params(struct mlx5_core_dev *mdev,
+ 				   struct mlx5e_params *params)
  {
- 	struct mlx5e_priv *priv = netdev_priv(netdev);
  	u8 cq_period_mode = MLX5_CAP_GEN(mdev, cq_period_start_from_cqe) ?
  					 MLX5_CQ_PERIOD_MODE_START_FROM_CQE :
  					 MLX5_CQ_PERIOD_MODE_START_FROM_EQE;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c b/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
index 4a787dea75a4..48d7d6868317 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
@@ -111,7 +111,7 @@ int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr)
 	switch (config.rx_filter) {
 	case HWTSTAMP_FILTER_NONE:
 		/* Reset CQE compression to Admin default */
-		mlx5e_modify_rx_cqe_compression_locked(priv, priv->params.rx_cqe_compress_def);
+		mlx5e_modify_rx_cqe_compression_locked(priv, priv->channels.params.rx_cqe_compress_def);
 		break;
 	case HWTSTAMP_FILTER_ALL:
 	case HWTSTAMP_FILTER_SOME:
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_fs_ethtool.c b/drivers/net/ethernet/mellanox/mlx5/core/en_fs_ethtool.c
index 26fc77e80f7b..85bf4a389295 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_fs_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_fs_ethtool.c
@@ -390,7 +390,7 @@ static int validate_flow(struct mlx5e_priv *priv,
 	if (fs->location >= MAX_NUM_OF_ETHTOOL_RULES)
 		return -EINVAL;
 
-	if (fs->ring_cookie >= priv->params.num_channels &&
+	if (fs->ring_cookie >= priv->channels.params.num_channels &&
 	    fs->ring_cookie != RX_CLS_FLOW_DISC)
 		return -EINVAL;
 
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index e8c9b2d23033..e78e1f7eb102 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -162,19 +162,19 @@ void mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val)
 	if (!MLX5_CAP_GEN(priv->mdev, cqe_compression))
 		return;
 
-	if (MLX5E_GET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS) == val)
+	if (MLX5E_GET_PFLAG(&priv->channels.params, MLX5E_PFLAG_RX_CQE_COMPRESS) == val)
 		return;
 
 	was_opened = test_bit(MLX5E_STATE_OPENED, &priv->state);
 	if (was_opened)
 		mlx5e_close_locked(priv->netdev);
 
-	MLX5E_SET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS, val);
-	mlx5e_set_rq_type_params(priv, priv->params.rq_wq_type);
+	MLX5E_SET_PFLAG(&priv->channels.params, MLX5E_PFLAG_RX_CQE_COMPRESS, val);
+	mlx5e_set_rq_type_params(priv->mdev, &priv->channels.params,
+				 priv->channels.params.rq_wq_type);
 
 	if (was_opened)
 		mlx5e_open_locked(priv->netdev);
-
 }
 
 #define RQ_PAGE_SIZE(rq) ((1 << rq->buff.page_order) << PAGE_SHIFT)
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 2a270903b57d..ddcdcda3e4f2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -111,6 +111,7 @@ u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int channel_ix = fallback(dev, skb);
+	u16 num_channels;
 	int up = 0;
 
 	if (!netdev_get_num_tc(dev))
@@ -122,9 +123,9 @@ u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 	/* channel_ix can be larger than num_channels since
 	 * dev->num_real_tx_queues = num_channels * num_tc
 	 */
-	if (channel_ix >= priv->params.num_channels)
-		channel_ix = reciprocal_scale(channel_ix,
-					      priv->params.num_channels);
+	num_channels = priv->channels.params.num_channels;
+	if (channel_ix >= num_channels)
+		channel_ix = reciprocal_scale(channel_ix, num_channels);
 
 	return priv->channeltc_to_txq_map[channel_ix][up];
 }
