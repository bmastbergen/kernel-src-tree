ibmvnic: Fix error handling when registering long-term-mapped buffers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Thomas Falcon <tlfalcon@linux.vnet.ibm.com>
commit f3be0cbc722c8de2f45c5d9f71f1b21da85554fd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f3be0cbc.failed

The patch stores the return code of the REQUEST_MAP_RSP sub-CRQ command
in the private data structure, where it can be later checked during
device open or a reset.

In the case of a reset, the mapping request to the vNIC Server may fail,
especially in the case of a partition migration. The driver attempts to
handle this by re-allocating the buffer and re-sending the mapping request.

The original error handling implementation was removed. The separate
function handling the REQUEST_MAP response message was also removed,
since it is now simple enough to be handled in the ibmvnic_handle_crq
function.

	Signed-off-by: Thomas Falcon <tlfalcon@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit f3be0cbc722c8de2f45c5d9f71f1b21da85554fd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/ibm/ibmvnic.c
#	drivers/net/ethernet/ibm/ibmvnic.h
diff --cc drivers/net/ethernet/ibm/ibmvnic.c
index 0396d573827d,aab69dd018d4..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.c
+++ b/drivers/net/ethernet/ibm/ibmvnic.c
@@@ -160,21 -163,6 +160,24 @@@ static long h_reg_sub_crq(unsigned lon
  	return rc;
  }
  
++<<<<<<< HEAD
 +/* net_device_ops functions */
 +
 +static void init_rx_pool(struct ibmvnic_adapter *adapter,
 +			 struct ibmvnic_rx_pool *rx_pool, int num, int index,
 +			 int buff_size, int active)
 +{
 +	netdev_dbg(adapter->netdev,
 +		   "Initializing rx_pool %d, %d buffs, %d bytes each\n",
 +		   index, num, buff_size);
 +	rx_pool->size = num;
 +	rx_pool->index = index;
 +	rx_pool->buff_size = buff_size;
 +	rx_pool->active = active;
 +}
 +
++=======
++>>>>>>> f3be0cbc722c (ibmvnic: Fix error handling when registering long-term-mapped buffers)
  static int alloc_long_term_buff(struct ibmvnic_adapter *adapter,
  				struct ibmvnic_long_term_buff *ltb, int size)
  {
@@@ -203,47 -197,33 +212,65 @@@ static void free_long_term_buff(struct 
  {
  	struct device *dev = &adapter->vdev->dev;
  
 -	if (!ltb->buff)
 -		return;
 -
 -	if (adapter->reset_reason != VNIC_RESET_FAILOVER &&
 -	    adapter->reset_reason != VNIC_RESET_MOBILITY)
 -		send_request_unmap(adapter, ltb->map_id);
  	dma_free_coherent(dev, ltb->size, ltb->buff, ltb->addr);
 +	if (!adapter->failover)
 +		send_request_unmap(adapter, ltb->map_id);
 +}
 +
 +static int alloc_rx_pool(struct ibmvnic_adapter *adapter,
 +			 struct ibmvnic_rx_pool *pool)
 +{
 +	struct device *dev = &adapter->vdev->dev;
 +	int i;
 +
 +	pool->free_map = kcalloc(pool->size, sizeof(int), GFP_KERNEL);
 +	if (!pool->free_map)
 +		return -ENOMEM;
 +
 +	pool->rx_buff = kcalloc(pool->size, sizeof(struct ibmvnic_rx_buff),
 +				GFP_KERNEL);
 +
 +	if (!pool->rx_buff) {
 +		dev_err(dev, "Couldn't alloc rx buffers\n");
 +		kfree(pool->free_map);
 +		return -ENOMEM;
 +	}
 +
 +	if (alloc_long_term_buff(adapter, &pool->long_term_buff,
 +				 pool->size * pool->buff_size)) {
 +		kfree(pool->free_map);
 +		kfree(pool->rx_buff);
 +		return -ENOMEM;
 +	}
 +
 +	for (i = 0; i < pool->size; ++i)
 +		pool->free_map[i] = i;
 +
 +	atomic_set(&pool->available, 0);
 +	pool->next_alloc = 0;
 +	pool->next_free = 0;
 +
 +	return 0;
  }
  
+ static int reset_long_term_buff(struct ibmvnic_adapter *adapter,
+ 				struct ibmvnic_long_term_buff *ltb)
+ {
+ 	memset(ltb->buff, 0, ltb->size);
+ 
+ 	init_completion(&adapter->fw_done);
+ 	send_request_map(adapter, ltb->addr, ltb->size, ltb->map_id);
+ 	wait_for_completion(&adapter->fw_done);
+ 
+ 	if (adapter->fw_done_rc) {
+ 		dev_info(&adapter->vdev->dev,
+ 			 "Reset failed, attempting to free and reallocate buffer\n");
+ 		free_long_term_buff(adapter, ltb);
+ 		return alloc_long_term_buff(adapter, ltb, ltb->size);
+ 	}
+ 	return 0;
+ }
+ 
  static void deactivate_rx_pools(struct ibmvnic_adapter *adapter)
  {
  	int i;
@@@ -369,8 -346,281 +396,286 @@@ static void replenish_pools(struct ibmv
  	}
  }
  
++<<<<<<< HEAD
 +static void free_rx_pool(struct ibmvnic_adapter *adapter,
 +			 struct ibmvnic_rx_pool *pool)
++=======
+ static void release_stats_token(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 
+ 	if (!adapter->stats_token)
+ 		return;
+ 
+ 	dma_unmap_single(dev, adapter->stats_token,
+ 			 sizeof(struct ibmvnic_statistics),
+ 			 DMA_FROM_DEVICE);
+ 	adapter->stats_token = 0;
+ }
+ 
+ static int init_stats_token(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 	dma_addr_t stok;
+ 
+ 	stok = dma_map_single(dev, &adapter->stats,
+ 			      sizeof(struct ibmvnic_statistics),
+ 			      DMA_FROM_DEVICE);
+ 	if (dma_mapping_error(dev, stok)) {
+ 		dev_err(dev, "Couldn't map stats buffer\n");
+ 		return -1;
+ 	}
+ 
+ 	adapter->stats_token = stok;
+ 	return 0;
+ }
+ 
+ static int reset_rx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_rx_pool *rx_pool;
+ 	int rx_scrqs;
+ 	int i, j, rc;
+ 
+ 	rx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	for (i = 0; i < rx_scrqs; i++) {
+ 		rx_pool = &adapter->rx_pool[i];
+ 
+ 		rc = reset_long_term_buff(adapter, &rx_pool->long_term_buff);
+ 		if (rc)
+ 			return rc;
+ 
+ 		for (j = 0; j < rx_pool->size; j++)
+ 			rx_pool->free_map[j] = j;
+ 
+ 		memset(rx_pool->rx_buff, 0,
+ 		       rx_pool->size * sizeof(struct ibmvnic_rx_buff));
+ 
+ 		atomic_set(&rx_pool->available, 0);
+ 		rx_pool->next_alloc = 0;
+ 		rx_pool->next_free = 0;
+ 		rx_pool->active = 1;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void release_rx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_rx_pool *rx_pool;
+ 	int rx_scrqs;
+ 	int i, j;
+ 
+ 	if (!adapter->rx_pool)
+ 		return;
+ 
+ 	rx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	for (i = 0; i < rx_scrqs; i++) {
+ 		rx_pool = &adapter->rx_pool[i];
+ 
+ 		kfree(rx_pool->free_map);
+ 		free_long_term_buff(adapter, &rx_pool->long_term_buff);
+ 
+ 		if (!rx_pool->rx_buff)
+ 			continue;
+ 
+ 		for (j = 0; j < rx_pool->size; j++) {
+ 			if (rx_pool->rx_buff[j].skb) {
+ 				dev_kfree_skb_any(rx_pool->rx_buff[i].skb);
+ 				rx_pool->rx_buff[i].skb = NULL;
+ 			}
+ 		}
+ 
+ 		kfree(rx_pool->rx_buff);
+ 	}
+ 
+ 	kfree(adapter->rx_pool);
+ 	adapter->rx_pool = NULL;
+ }
+ 
+ static int init_rx_pools(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	struct device *dev = &adapter->vdev->dev;
+ 	struct ibmvnic_rx_pool *rx_pool;
+ 	int rxadd_subcrqs;
+ 	u64 *size_array;
+ 	int i, j;
+ 
+ 	rxadd_subcrqs =
+ 		be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	size_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +
+ 		be32_to_cpu(adapter->login_rsp_buf->off_rxadd_buff_size));
+ 
+ 	adapter->rx_pool = kcalloc(rxadd_subcrqs,
+ 				   sizeof(struct ibmvnic_rx_pool),
+ 				   GFP_KERNEL);
+ 	if (!adapter->rx_pool) {
+ 		dev_err(dev, "Failed to allocate rx pools\n");
+ 		return -1;
+ 	}
+ 
+ 	for (i = 0; i < rxadd_subcrqs; i++) {
+ 		rx_pool = &adapter->rx_pool[i];
+ 
+ 		netdev_dbg(adapter->netdev,
+ 			   "Initializing rx_pool %d, %lld buffs, %lld bytes each\n",
+ 			   i, adapter->req_rx_add_entries_per_subcrq,
+ 			   be64_to_cpu(size_array[i]));
+ 
+ 		rx_pool->size = adapter->req_rx_add_entries_per_subcrq;
+ 		rx_pool->index = i;
+ 		rx_pool->buff_size = be64_to_cpu(size_array[i]);
+ 		rx_pool->active = 1;
+ 
+ 		rx_pool->free_map = kcalloc(rx_pool->size, sizeof(int),
+ 					    GFP_KERNEL);
+ 		if (!rx_pool->free_map) {
+ 			release_rx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		rx_pool->rx_buff = kcalloc(rx_pool->size,
+ 					   sizeof(struct ibmvnic_rx_buff),
+ 					   GFP_KERNEL);
+ 		if (!rx_pool->rx_buff) {
+ 			dev_err(dev, "Couldn't alloc rx buffers\n");
+ 			release_rx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		if (alloc_long_term_buff(adapter, &rx_pool->long_term_buff,
+ 					 rx_pool->size * rx_pool->buff_size)) {
+ 			release_rx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		for (j = 0; j < rx_pool->size; ++j)
+ 			rx_pool->free_map[j] = j;
+ 
+ 		atomic_set(&rx_pool->available, 0);
+ 		rx_pool->next_alloc = 0;
+ 		rx_pool->next_free = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int reset_tx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_tx_pool *tx_pool;
+ 	int tx_scrqs;
+ 	int i, j, rc;
+ 
+ 	tx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	for (i = 0; i < tx_scrqs; i++) {
+ 		tx_pool = &adapter->tx_pool[i];
+ 
+ 		rc = reset_long_term_buff(adapter, &tx_pool->long_term_buff);
+ 		if (rc)
+ 			return rc;
+ 
+ 		memset(tx_pool->tx_buff, 0,
+ 		       adapter->req_tx_entries_per_subcrq *
+ 		       sizeof(struct ibmvnic_tx_buff));
+ 
+ 		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
+ 			tx_pool->free_map[j] = j;
+ 
+ 		tx_pool->consumer_index = 0;
+ 		tx_pool->producer_index = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void release_tx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_tx_pool *tx_pool;
+ 	int i, tx_scrqs;
+ 
+ 	if (!adapter->tx_pool)
+ 		return;
+ 
+ 	tx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	for (i = 0; i < tx_scrqs; i++) {
+ 		tx_pool = &adapter->tx_pool[i];
+ 		kfree(tx_pool->tx_buff);
+ 		free_long_term_buff(adapter, &tx_pool->long_term_buff);
+ 		kfree(tx_pool->free_map);
+ 	}
+ 
+ 	kfree(adapter->tx_pool);
+ 	adapter->tx_pool = NULL;
+ }
+ 
+ static int init_tx_pools(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	struct device *dev = &adapter->vdev->dev;
+ 	struct ibmvnic_tx_pool *tx_pool;
+ 	int tx_subcrqs;
+ 	int i, j;
+ 
+ 	tx_subcrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	adapter->tx_pool = kcalloc(tx_subcrqs,
+ 				   sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);
+ 	if (!adapter->tx_pool)
+ 		return -1;
+ 
+ 	for (i = 0; i < tx_subcrqs; i++) {
+ 		tx_pool = &adapter->tx_pool[i];
+ 		tx_pool->tx_buff = kcalloc(adapter->req_tx_entries_per_subcrq,
+ 					   sizeof(struct ibmvnic_tx_buff),
+ 					   GFP_KERNEL);
+ 		if (!tx_pool->tx_buff) {
+ 			dev_err(dev, "tx pool buffer allocation failed\n");
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		if (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,
+ 					 adapter->req_tx_entries_per_subcrq *
+ 					 adapter->req_mtu)) {
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		tx_pool->free_map = kcalloc(adapter->req_tx_entries_per_subcrq,
+ 					    sizeof(int), GFP_KERNEL);
+ 		if (!tx_pool->free_map) {
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
+ 			tx_pool->free_map[j] = j;
+ 
+ 		tx_pool->consumer_index = 0;
+ 		tx_pool->producer_index = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void release_error_buffers(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 	struct ibmvnic_error_buff *error_buff, *tmp;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&adapter->error_list_lock, flags);
+ 	list_for_each_entry_safe(error_buff, tmp, &adapter->errors, list) {
+ 		list_del(&error_buff->list);
+ 		dma_unmap_single(dev, error_buff->dma, error_buff->len,
+ 				 DMA_FROM_DEVICE);
+ 		kfree(error_buff->buff);
+ 		kfree(error_buff);
+ 	}
+ 	spin_unlock_irqrestore(&adapter->error_list_lock, flags);
+ }
+ 
+ static void ibmvnic_napi_enable(struct ibmvnic_adapter *adapter)
++>>>>>>> f3be0cbc722c (ibmvnic: Fix error handling when registering long-term-mapped buffers)
  {
  	int i;
  
diff --cc drivers/net/ethernet/ibm/ibmvnic.h
index 082a339df814,8eff6e15f4bb..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.h
+++ b/drivers/net/ethernet/ibm/ibmvnic.h
@@@ -996,22 -987,8 +996,26 @@@ struct ibmvnic_adapter 
  	struct list_head errors;
  	spinlock_t error_list_lock;
  
 +	/* debugfs */
 +	struct dentry *debugfs_dir;
 +	struct dentry *debugfs_dump;
  	struct completion fw_done;
++<<<<<<< HEAD
 +	char *dump_data;
 +	dma_addr_t dump_data_token;
 +	int dump_data_size;
 +	int ras_comp_num;
 +	struct ibmvnic_fw_component *ras_comps;
 +	struct ibmvnic_fw_comp_internal *ras_comp_int;
 +	dma_addr_t ras_comps_tok;
 +	struct dentry *ras_comps_ent;
 +
 +	/* in-flight commands that allocate and/or map memory*/
 +	struct list_head inflight;
 +	spinlock_t inflight_lock;
++=======
+ 	int fw_done_rc;
++>>>>>>> f3be0cbc722c (ibmvnic: Fix error handling when registering long-term-mapped buffers)
  
  	/* partner capabilities */
  	u64 min_tx_queues;
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.c
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.h
