cpuset: apply cs->effective_{cpus,mems}

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Li Zefan <lizefan@huawei.com>
commit ae1c802382f7af60aa54879fb4f5920a9df1ff48
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ae1c8023.failed

Now we can use cs->effective_{cpus,mems} as effective masks. It's
used whenever:

- we update tasks' cpus_allowed/mems_allowed,
- we want to retrieve tasks_cs(tsk)'s cpus_allowed/mems_allowed.

They actually replace effective_{cpu,node}mask_cpuset().

effective_mask == configured_mask & parent effective_mask except when
the reault is empty, in which case it inherits parent effective_mask.
The result equals the mask computed from effective_{cpu,node}mask_cpuset().

This won't affect the original legacy hierarchy, because in this case we
make sure the effective masks are always the same with user-configured
masks.

	Signed-off-by: Li Zefan <lizefan@huawei.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit ae1c802382f7af60aa54879fb4f5920a9df1ff48)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cpuset.c
diff --cc kernel/cpuset.c
index 650413f2caa8,820870a715f8..000000000000
--- a/kernel/cpuset.c
+++ b/kernel/cpuset.c
@@@ -320,12 -311,11 +320,12 @@@ static struct file_system_type cpuset_f
   *
   * Call with callback_mutex held.
   */
 -static void guarantee_online_cpus(struct cpuset *cs, struct cpumask *pmask)
 +static void guarantee_online_cpus(const struct cpuset *cs,
 +				  struct cpumask *pmask)
  {
- 	while (!cpumask_intersects(cs->cpus_allowed, cpu_online_mask))
+ 	while (!cpumask_intersects(cs->effective_cpus, cpu_online_mask))
  		cs = parent_cs(cs);
- 	cpumask_and(pmask, cs->cpus_allowed, cpu_online_mask);
+ 	cpumask_and(pmask, cs->effective_cpus, cpu_online_mask);
  }
  
  /*
@@@ -339,11 -329,11 +339,11 @@@
   *
   * Call with callback_mutex held.
   */
 -static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)
 +static void guarantee_online_mems(const struct cpuset *cs, nodemask_t *pmask)
  {
- 	while (!nodes_intersects(cs->mems_allowed, node_states[N_MEMORY]))
+ 	while (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))
  		cs = parent_cs(cs);
- 	nodes_and(*pmask, cs->mems_allowed, node_states[N_MEMORY]);
+ 	nodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);
  }
  
  /*
@@@ -814,45 -795,23 +814,58 @@@ void rebuild_sched_domains(void
  	mutex_unlock(&cpuset_mutex);
  }
  
++<<<<<<< HEAD
 +/**
 + * cpuset_change_cpumask - make a task's cpus_allowed the same as its cpuset's
 + * @tsk: task to test
 + * @scan: struct cgroup_scanner containing the cgroup of the task
 + *
 + * Called by cgroup_scan_tasks() for each task in a cgroup whose
 + * cpus_allowed mask needs to be changed.
 + *
 + * We don't need to re-check for the cgroup/cpuset membership, since we're
 + * holding cpuset_mutex at this point.
 + */
 +static void cpuset_change_cpumask(struct task_struct *tsk,
 +				  struct cgroup_scanner *scan)
 +{
 +	set_cpus_allowed_ptr(tsk, ((cgroup_cs(scan->cg))->cpus_allowed));
 +}
 +
++=======
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  /**
   * update_tasks_cpumask - Update the cpumasks of tasks in the cpuset.
   * @cs: the cpuset in which each task's cpus_allowed mask needs to be changed
 + * @heap: if NULL, defer allocating heap memory to cgroup_scan_tasks()
 + *
 + * Called with cpuset_mutex held
 + *
 + * The cgroup_scan_tasks() function will scan all the tasks in a cgroup,
 + * calling callback functions for each.
   *
 - * Iterate through each task of @cs updating its cpus_allowed to the
 - * effective cpuset's.  As this function is called with cpuset_mutex held,
 - * cpuset membership stays stable.
 + * No return value. It's guaranteed that cgroup_scan_tasks() always returns 0
 + * if @heap != NULL.
   */
 -static void update_tasks_cpumask(struct cpuset *cs)
 +static void update_tasks_cpumask(struct cpuset *cs, struct ptr_heap *heap)
  {
++<<<<<<< HEAD
 +	struct cgroup_scanner scan;
 +
 +	scan.cg = cs->css.cgroup;
 +	scan.test_task = NULL;
 +	scan.process_task = cpuset_change_cpumask;
 +	scan.heap = heap;
 +	cgroup_scan_tasks(&scan);
++=======
+ 	struct css_task_iter it;
+ 	struct task_struct *task;
+ 
+ 	css_task_iter_start(&cs->css, &it);
+ 	while ((task = css_task_iter_next(&it)))
+ 		set_cpus_allowed_ptr(task, cs->effective_cpus);
+ 	css_task_iter_end(&it);
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  }
  
  /*
@@@ -978,7 -954,7 +991,11 @@@ static void cpuset_migrate_mm(struct mm
  	do_migrate_pages(mm, from, to, MPOL_MF_MOVE_ALL);
  
  	rcu_read_lock();
++<<<<<<< HEAD
 +	guarantee_online_mems(task_cs(tsk),&tsk->mems_allowed);
++=======
+ 	guarantee_online_mems(task_cs(tsk), &tsk->mems_allowed);
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  	rcu_read_unlock();
  }
  
@@@ -1066,26 -1015,20 +1083,34 @@@ static void *cpuset_being_rebound
  /**
   * update_tasks_nodemask - Update the nodemasks of tasks in the cpuset.
   * @cs: the cpuset in which each task's mems_allowed mask needs to be changed
 + * @heap: if NULL, defer allocating heap memory to cgroup_scan_tasks()
   *
 - * Iterate through each task of @cs updating its mems_allowed to the
 - * effective cpuset's.  As this function is called with cpuset_mutex held,
 - * cpuset membership stays stable.
 + * Called with cpuset_mutex held
 + * No return value. It's guaranteed that cgroup_scan_tasks() always returns 0
 + * if @heap != NULL.
   */
 -static void update_tasks_nodemask(struct cpuset *cs)
 +static void update_tasks_nodemask(struct cpuset *cs, struct ptr_heap *heap)
  {
  	static nodemask_t newmems;	/* protected by cpuset_mutex */
++<<<<<<< HEAD
 +	struct cgroup_scanner scan;
++=======
+ 	struct css_task_iter it;
+ 	struct task_struct *task;
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  
  	cpuset_being_rebound = cs;		/* causes mpol_dup() rebind */
  
  	guarantee_online_mems(cs, &newmems);
++<<<<<<< HEAD
 +
 +	scan.cg = cs->css.cgroup;
 +	scan.test_task = NULL;
 +	scan.process_task = cpuset_change_nodemask;
 +	scan.heap = heap;
 +	scan.data = &newmems;
++=======
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  
  	/*
  	 * The mpol_rebind_mm() call takes mmap_sem, which we couldn't
@@@ -1501,9 -1452,8 +1526,14 @@@ static void cpuset_attach(struct cgrou
  	struct mm_struct *mm;
  	struct task_struct *task;
  	struct task_struct *leader = cgroup_taskset_first(tset);
++<<<<<<< HEAD
 +	struct cgroup *oldcgrp = cgroup_taskset_cur_cgroup(tset);
 +	struct cpuset *cs = cgroup_cs(cgrp);
 +	struct cpuset *oldcs = cgroup_cs(oldcgrp);
++=======
+ 	struct cpuset *cs = css_cs(css);
+ 	struct cpuset *oldcs = cpuset_attach_old_cs;
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  
  	mutex_lock(&cpuset_mutex);
  
@@@ -1534,9 -1484,18 +1564,22 @@@
  	mm = get_task_mm(leader);
  	if (mm) {
  		mpol_rebind_mm(mm, &cpuset_attach_nodemask_to);
++<<<<<<< HEAD
 +		if (is_memory_migrate(cs))
 +			cpuset_migrate_mm(mm, &oldcs->mems_allowed,
++=======
+ 
+ 		/*
+ 		 * old_mems_allowed is the same with mems_allowed here, except
+ 		 * if this task is being moved automatically due to hotplug.
+ 		 * In that case @mems_allowed has been updated and is empty,
+ 		 * so @old_mems_allowed is the right nodesets that we migrate
+ 		 * mm from.
+ 		 */
+ 		if (is_memory_migrate(cs)) {
+ 			cpuset_migrate_mm(mm, &oldcs->old_mems_allowed,
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  					  &cpuset_attach_nodemask_to);
 -		}
  		mmput(mm);
  	}
  
@@@ -2297,19 -2285,16 +2340,30 @@@ void __init cpuset_init_smp(void
  void cpuset_cpus_allowed(struct task_struct *tsk, struct cpumask *pmask)
  {
  	mutex_lock(&callback_mutex);
++<<<<<<< HEAD
 +	task_lock(tsk);
 +	guarantee_online_cpus(task_cs(tsk), pmask);
 +	task_unlock(tsk);
++=======
+ 	rcu_read_lock();
+ 	guarantee_online_cpus(task_cs(tsk), pmask);
+ 	rcu_read_unlock();
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  	mutex_unlock(&callback_mutex);
  }
  
  void cpuset_cpus_allowed_fallback(struct task_struct *tsk)
  {
++<<<<<<< HEAD
 +	const struct cpuset *cs;
 +
 +	rcu_read_lock();
 +	cs = task_cs(tsk);
 +	do_set_cpus_allowed(tsk, cs->cpus_allowed);
++=======
+ 	rcu_read_lock();
+ 	do_set_cpus_allowed(tsk, task_cs(tsk)->effective_cpus);
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  	rcu_read_unlock();
  
  	/*
@@@ -2351,9 -2336,9 +2405,15 @@@ nodemask_t cpuset_mems_allowed(struct t
  	nodemask_t mask;
  
  	mutex_lock(&callback_mutex);
++<<<<<<< HEAD
 +	task_lock(tsk);
 +	guarantee_online_mems(task_cs(tsk), &mask);
 +	task_unlock(tsk);
++=======
+ 	rcu_read_lock();
+ 	guarantee_online_mems(task_cs(tsk), &mask);
+ 	rcu_read_unlock();
++>>>>>>> ae1c802382f7 (cpuset: apply cs->effective_{cpus,mems})
  	mutex_unlock(&callback_mutex);
  
  	return mask;
* Unmerged path kernel/cpuset.c
