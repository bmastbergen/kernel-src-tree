crypto: chcr - Use cipher instead of Block Cipher in gcm setkey

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [crypto] chcr - Use cipher instead of Block Cipher in gcm setkey (Arjun Vynipadath) [1458315]
Rebuild_FUZZ: 93.22%
commit-author Harsh Jain <harsh@chelsio.com>
commit 8356ea515ba1396d6a24dd1e80f101ee9a20ff3c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/8356ea51.failed

1 Block of encrption can be done with aes-generic. no need of
cbc(aes). This patch replaces cbc(aes-generic) with aes-generic.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 8356ea515ba1396d6a24dd1e80f101ee9a20ff3c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
diff --cc drivers/crypto/chelsio/chcr_algo.c
index bda117371c9f,d3359439bcd3..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -1260,6 -1321,1168 +1260,1171 @@@ static void chcr_hmac_cra_exit(struct c
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int chcr_copy_assoc(struct aead_request *req,
+ 				struct chcr_aead_ctx *ctx)
+ {
+ 	SKCIPHER_REQUEST_ON_STACK(skreq, ctx->null);
+ 
+ 	skcipher_request_set_tfm(skreq, ctx->null);
+ 	skcipher_request_set_callback(skreq, aead_request_flags(req),
+ 			NULL, NULL);
+ 	skcipher_request_set_crypt(skreq, req->src, req->dst, req->assoclen,
+ 			NULL);
+ 
+ 	return crypto_skcipher_encrypt(skreq);
+ }
+ 
+ static unsigned char get_hmac(unsigned int authsize)
+ {
+ 	switch (authsize) {
+ 	case ICV_8:
+ 		return CHCR_SCMD_HMAC_CTRL_PL1;
+ 	case ICV_10:
+ 		return CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 	case ICV_12:
+ 		return CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 	}
+ 	return CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ }
+ 
+ 
+ static struct sk_buff *create_authenc_wr(struct aead_request *req,
+ 					 unsigned short qid,
+ 					 int size,
+ 					 unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = crypto_aead_ivsize(tfm), dst_size = 0;
+ 	unsigned int   kctx_len = 0;
+ 	unsigned short stop_offset = 0;
+ 	unsigned int  assoclen = req->assoclen;
+ 	unsigned int  authsize = crypto_aead_authsize(tfm);
+ 	int err = 0;
+ 	int null = 0;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 	if (aeadctx->enckey_len == 0 || (req->cryptlen == 0))
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 
+ 	if (sg_nents_for_len(req->src, req->assoclen + req->cryptlen) < 0)
+ 		goto err;
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		err = chcr_copy_assoc(req, aeadctx);
+ 		if (err)
+ 			return ERR_PTR(err);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {
+ 		null = 1;
+ 		assoclen = 0;
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents <= 0) {
+ 		pr_err("AUTHENC:Invalid Destination sg entries\n");
+ 		goto err;
+ 	}
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)
+ 		- sizeof(chcr_req->key_ctx);
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb)
+ 		goto err;
+ 
+ 	/* LLD is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	/* Write WR */
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	stop_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 
+ 	/*
+ 	 * Input order	is AAD,IV and Payload. where IV should be included as
+ 	 * the part of authdata. All other fields should be filled according
+ 	 * to the hardware spec
+ 	 */
+ 	chcr_req->sec_cpl.op_ivinsrtofst =
+ 		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,
+ 				       (ivsize ? (assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1,
+ 					(stop_offset & 0x1F0) >> 4);
+ 	chcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(
+ 					stop_offset & 0xF,
+ 					null ? 0 : assoclen + ivsize + 1,
+ 					stop_offset, stop_offset);
+ 	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_CBC,
+ 					actx->auth_mode, aeadctx->hmac_ctrl,
+ 					ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					 0, 1, dst_size);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	if (op_type == CHCR_ENCRYPT_OP)
+ 		memcpy(chcr_req->key_ctx.key, aeadctx->key,
+ 		       aeadctx->enckey_len);
+ 	else
+ 		memcpy(chcr_req->key_ctx.key, actx->dec_rrkey,
+ 		       aeadctx->enckey_len);
+ 
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<
+ 					4), actx->h_iopad, kctx_len -
+ 				(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	sg_param.align = 0;
+ 	if (map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl, reqctx->dst,
+ 				  &sg_param))
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 
+ 	if (assoclen) {
+ 		/* AAD buffer in */
+ 		write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 
+ 	}
+ 	write_buffer_to_skb(skb, &frags, req->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	create_wreq(ctx, chcr_req, req, skb, kctx_len, size, 1,
+ 		   sizeof(struct cpl_rx_phys_dsgl) + dst_size);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 
+ 	return skb;
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(-EINVAL);
+ }
+ 
+ static void aes_gcm_empty_pld_pad(struct scatterlist *sg,
+ 				  unsigned short offset)
+ {
+ 	struct page *spage;
+ 	unsigned char *addr;
+ 
+ 	spage = sg_page(sg);
+ 	get_page(spage); /* so that it is not freed by NIC */
+ #ifdef KMAP_ATOMIC_ARGS
+ 	addr = kmap_atomic(spage, KM_SOFTIRQ0);
+ #else
+ 	addr = kmap_atomic(spage);
+ #endif
+ 	memset(addr + sg->offset, 0, offset + 1);
+ 
+ 	kunmap_atomic(addr);
+ }
+ 
+ static int set_msg_len(u8 *block, unsigned int msglen, int csize)
+ {
+ 	__be32 data;
+ 
+ 	memset(block, 0, csize);
+ 	block += csize;
+ 
+ 	if (csize >= 4)
+ 		csize = 4;
+ 	else if (msglen > (unsigned int)(1 << (8 * csize)))
+ 		return -EOVERFLOW;
+ 
+ 	data = cpu_to_be32(msglen);
+ 	memcpy(block - csize, (u8 *)&data + 4 - csize, csize);
+ 
+ 	return 0;
+ }
+ 
+ static void generate_b0(struct aead_request *req,
+ 			struct chcr_aead_ctx *aeadctx,
+ 			unsigned short op_type)
+ {
+ 	unsigned int l, lp, m;
+ 	int rc;
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	u8 *b0 = reqctx->scratch_pad;
+ 
+ 	m = crypto_aead_authsize(aead);
+ 
+ 	memcpy(b0, reqctx->iv, 16);
+ 
+ 	lp = b0[0];
+ 	l = lp + 1;
+ 
+ 	/* set m, bits 3-5 */
+ 	*b0 |= (8 * ((m - 2) / 2));
+ 
+ 	/* set adata, bit 6, if associated data is used */
+ 	if (req->assoclen)
+ 		*b0 |= 64;
+ 	rc = set_msg_len(b0 + 16 - l,
+ 			 (op_type == CHCR_DECRYPT_OP) ?
+ 			 req->cryptlen - m : req->cryptlen, l);
+ }
+ 
+ static inline int crypto_ccm_check_iv(const u8 *iv)
+ {
+ 	/* 2 <= L <= 8, so 1 <= L' <= 7. */
+ 	if (iv[0] < 1 || iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int ccm_format_packet(struct aead_request *req,
+ 			     struct chcr_aead_ctx *aeadctx,
+ 			     unsigned int sub_type,
+ 			     unsigned short op_type)
+ {
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int rc = 0;
+ 
+ 	if (req->assoclen > T5_MAX_AAD_SIZE) {
+ 		pr_err("CCM: Unsupported AAD data. It should be < %d\n",
+ 		       T5_MAX_AAD_SIZE);
+ 		return -EINVAL;
+ 	}
+ 	if (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		reqctx->iv[0] = 3;
+ 		memcpy(reqctx->iv + 1, &aeadctx->salt[0], 3);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 		memset(reqctx->iv + 12, 0, 4);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen - 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 16);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen);
+ 	}
+ 	generate_b0(req, aeadctx, op_type);
+ 	/* zero the ctr value */
+ 	memset(reqctx->iv + 15 - reqctx->iv[0], 0, reqctx->iv[0] + 1);
+ 	return rc;
+ }
+ 
+ static void fill_sec_cpl_for_aead(struct cpl_tx_sec_pdu *sec_cpl,
+ 				  unsigned int dst_size,
+ 				  struct aead_request *req,
+ 				  unsigned short op_type,
+ 					  struct chcr_context *chcrctx)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int cipher_mode = CHCR_SCMD_CIPHER_MODE_AES_CCM;
+ 	unsigned int mac_mode = CHCR_SCMD_AUTH_MODE_CBCMAC;
+ 	unsigned int c_id = chcrctx->dev->rx_channel_id;
+ 	unsigned int ccm_xtra;
+ 	unsigned char tag_offset = 0, auth_offset = 0;
+ 	unsigned char hmac_ctrl = get_hmac(crypto_aead_authsize(tfm));
+ 	unsigned int assoclen;
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 		assoclen = req->assoclen - 8;
+ 	else
+ 		assoclen = req->assoclen;
+ 	ccm_xtra = CCM_B0_SIZE +
+ 		((assoclen) ? CCM_AAD_FIELD_SIZE : 0);
+ 
+ 	auth_offset = req->cryptlen ?
+ 		(assoclen + ivsize + 1 + ccm_xtra) : 0;
+ 	if (op_type == CHCR_DECRYPT_OP) {
+ 		if (crypto_aead_authsize(tfm) != req->cryptlen)
+ 			tag_offset = crypto_aead_authsize(tfm);
+ 		else
+ 			auth_offset = 0;
+ 	}
+ 
+ 
+ 	sec_cpl->op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(c_id,
+ 					 2, (ivsize ?  (assoclen + 1) :  0) +
+ 					 ccm_xtra);
+ 	sec_cpl->pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen + ccm_xtra);
+ 	/* For CCM there wil be b0 always. So AAD start will be 1 always */
+ 	sec_cpl->aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					1, assoclen + ccm_xtra, assoclen
+ 					+ ivsize + 1 + ccm_xtra, 0);
+ 
+ 	sec_cpl->cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(0,
+ 					auth_offset, tag_offset,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 :
+ 					crypto_aead_authsize(tfm));
+ 	sec_cpl->seqno_numivs =  FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 : 1,
+ 					cipher_mode, mac_mode, hmac_ctrl,
+ 					ivsize >> 1);
+ 
+ 	sec_cpl->ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1, 0,
+ 					1, dst_size);
+ }
+ 
+ int aead_ccm_validate_input(unsigned short op_type,
+ 			    struct aead_request *req,
+ 			    struct chcr_aead_ctx *aeadctx,
+ 			    unsigned int sub_type)
+ {
+ 	if (sub_type != CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		if (crypto_ccm_check_iv(req->iv)) {
+ 			pr_err("CCM: IV check fails\n");
+ 			return -EINVAL;
+ 		}
+ 	} else {
+ 		if (req->assoclen != 16 && req->assoclen != 20) {
+ 			pr_err("RFC4309: Invalid AAD length %d\n",
+ 			       req->assoclen);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	if (aeadctx->enckey_len == 0) {
+ 		pr_err("CCM: Encryption key not set\n");
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ unsigned int fill_aead_req_fields(struct sk_buff *skb,
+ 				  struct aead_request *req,
+ 				  struct scatterlist *src,
+ 				  unsigned int ivsize,
+ 				  struct chcr_aead_ctx *aeadctx)
+ {
+ 	unsigned int frags = 0;
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	/* b0 and aad length(if available) */
+ 
+ 	write_buffer_to_skb(skb, &frags, reqctx->scratch_pad, CCM_B0_SIZE +
+ 				(req->assoclen ?  CCM_AAD_FIELD_SIZE : 0));
+ 	if (req->assoclen) {
+ 		if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 			write_sg_to_skb(skb, &frags, req->src,
+ 					req->assoclen - 8);
+ 		else
+ 			write_sg_to_skb(skb, &frags, req->src, req->assoclen);
+ 	}
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	if (req->cryptlen)
+ 		write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 
+ 	return frags;
+ }
+ 
+ static struct sk_buff *create_aead_ccm_wr(struct aead_request *req,
+ 					  unsigned short qid,
+ 					  int size,
+ 					  unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len, ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len;
+ 	unsigned int sub_type;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int err = 0;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 
+ 	if (sg_nents_for_len(req->src, req->assoclen + req->cryptlen) < 0)
+ 		goto err;
+ 	sub_type = get_aead_subtype(tfm);
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		err = chcr_copy_assoc(req, aeadctx);
+ 		if (err) {
+ 			pr_err("AAD copy to destination buffer fails\n");
+ 			return ERR_PTR(err);
+ 		}
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents <= 0) {
+ 		pr_err("CCM:Invalid Destination sg entries\n");
+ 		goto err;
+ 	}
+ 
+ 
+ 	if (aead_ccm_validate_input(op_type, req, aeadctx, sub_type))
+ 		goto err;
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) * 2;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)),  flags);
+ 
+ 	if (!skb)
+ 		goto err;
+ 
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	fill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, op_type, ctx);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 					16), aeadctx->key, aeadctx->enckey_len);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	if (ccm_format_packet(req, aeadctx, sub_type, op_type))
+ 		goto dstmap_fail;
+ 
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	sg_param.align = 0;
+ 	if (map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl, reqctx->dst,
+ 				  &sg_param))
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	frags = fill_aead_req_fields(skb, req, src, ivsize, aeadctx);
+ 	create_wreq(ctx, chcr_req, req, skb, kctx_len, 0, 1,
+ 		    sizeof(struct cpl_rx_phys_dsgl) + dst_size);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ dstmap_fail:
+ 	kfree_skb(skb);
+ 	skb = NULL;
+ err:
+ 	return ERR_PTR(-EINVAL);
+ }
+ 
+ static struct sk_buff *create_gcm_wr(struct aead_request *req,
+ 				     unsigned short qid,
+ 				     int size,
+ 				     unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len;
+ 	unsigned char tag_offset = 0;
+ 	unsigned int crypt_len = 0;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	unsigned char hmac_ctrl = get_hmac(authsize);
+ 	int err = 0;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 	/* validate key size */
+ 	if (aeadctx->enckey_len == 0)
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 
+ 	if (sg_nents_for_len(req->src, req->assoclen + req->cryptlen) < 0)
+ 		goto err;
+ 
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 	if (req->src != req->dst) {
+ 		err = chcr_copy_assoc(req, aeadctx);
+ 		if (err)
+ 			return	ERR_PTR(err);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 
+ 	if (!req->cryptlen)
+ 		/* null-payload is not supported in the hardware.
+ 		 * software is sending block size
+ 		 */
+ 		crypt_len = AES_BLOCK_SIZE;
+ 	else
+ 		crypt_len = req->cryptlen;
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents <= 0) {
+ 		pr_err("GCM:Invalid Destination sg entries\n");
+ 		goto err;
+ 	}
+ 
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb)
+ 		goto err;
+ 
+ 	/* NIC driver is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)
+ 		req->assoclen -= 8;
+ 
+ 	tag_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 	chcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(
+ 					ctx->dev->rx_channel_id, 2, (ivsize ?
+ 					(req->assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen = htonl(req->assoclen + ivsize + crypt_len);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					req->assoclen ? 1 : 0, req->assoclen,
+ 					req->assoclen + ivsize + 1, 0);
+ 	if (req->cryptlen) {
+ 		chcr_req->sec_cpl.cipherstop_lo_authinsert =
+ 			FILL_SEC_CPL_AUTHINSERT(0, req->assoclen + ivsize + 1,
+ 						tag_offset, tag_offset);
+ 		chcr_req->sec_cpl.seqno_numivs =
+ 			FILL_SEC_CPL_SCMD0_SEQNO(op_type, (op_type ==
+ 					CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_GCM,
+ 					CHCR_SCMD_AUTH_MODE_GHASH, hmac_ctrl,
+ 					ivsize >> 1);
+ 	} else {
+ 		chcr_req->sec_cpl.cipherstop_lo_authinsert =
+ 			FILL_SEC_CPL_AUTHINSERT(0, 0, 0, 0);
+ 		chcr_req->sec_cpl.seqno_numivs =
+ 			FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type ==  CHCR_ENCRYPT_OP) ?
+ 					1 : 0, CHCR_SCMD_CIPHER_MODE_AES_CBC,
+ 					0, 0, ivsize >> 1);
+ 	}
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					0, 1, dst_size);
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 				16), GCM_CTX(aeadctx)->ghash_h, AEAD_H_SIZE);
+ 
+ 	/* prepare a 16 byte iv */
+ 	/* S   A   L  T |  IV | 0x00000001 */
+ 	if (get_aead_subtype(tfm) ==
+ 	    CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) {
+ 		memcpy(reqctx->iv, aeadctx->salt, 4);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 12);
+ 	}
+ 	*((unsigned int *)(reqctx->iv + 12)) = htonl(0x01);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	sg_param.align = 0;
+ 	if (map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl, reqctx->dst,
+ 				  &sg_param))
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 
+ 	write_sg_to_skb(skb, &frags, req->src, req->assoclen);
+ 
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 
+ 	if (req->cryptlen) {
+ 		write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	} else {
+ 		aes_gcm_empty_pld_pad(req->dst, authsize - 1);
+ 		write_sg_to_skb(skb, &frags, reqctx->dst, crypt_len);
+ 
+ 	}
+ 
+ 	create_wreq(ctx, chcr_req, req, skb, kctx_len, size, 1,
+ 			sizeof(struct cpl_rx_phys_dsgl) + dst_size);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ 
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ 	skb = NULL;
+ err:
+ 	return skb;
+ }
+ 
+ 
+ 
+ static int chcr_aead_cra_init(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 
+ 	crypto_aead_set_reqsize(tfm, sizeof(struct chcr_aead_reqctx));
+ 	aeadctx->null = crypto_get_default_null_skcipher();
+ 	if (IS_ERR(aeadctx->null))
+ 		return PTR_ERR(aeadctx->null);
+ 	return chcr_device_init(ctx);
+ }
+ 
+ static void chcr_aead_cra_exit(struct crypto_aead *tfm)
+ {
+ 	crypto_put_default_null_skcipher();
+ }
+ 
+ static int chcr_authenc_null_setauthsize(struct crypto_aead *tfm,
+ 					unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NOP;
+ 	aeadctx->mayverify = VERIFY_HW;
+ 	return 0;
+ }
+ static int chcr_authenc_setauthsize(struct crypto_aead *tfm,
+ 				    unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	u32 maxauth = crypto_aead_maxauthsize(tfm);
+ 
+ 	/*SHA1 authsize in ipsec is 12 instead of 10 i.e maxauthsize / 2 is not
+ 	 * true for sha1. authsize == 12 condition should be before
+ 	 * authsize == (maxauth >> 1)
+ 	 */
+ 	if (authsize == ICV_4) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_6) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_10) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_12) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_14) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == (maxauth >> 1)) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == maxauth) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 	}
+ 	return 0;
+ }
+ 
+ 
+ static int chcr_gcm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_13:
+ 	case ICV_15:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 		break;
+ 	default:
+ 
+ 		  crypto_tfm_set_flags((struct crypto_tfm *) tfm,
+ 			CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ static int chcr_4106_4309_setauthsize(struct crypto_aead *tfm,
+ 					  unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ static int chcr_ccm_setauthsize(struct crypto_aead *tfm,
+ 				unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_6:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_10:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ static int chcr_aead_ccm_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	unsigned char ck_size, mk_size;
+ 	int key_ctx_size = 0;
+ 
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4)  * 2;
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		mk_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, mk_size, 0, 0,
+ 						key_ctx_size >> 4);
+ 	return 0;
+ }
+ 
+ static int chcr_aead_rfc4309_setkey(struct crypto_aead *aead, const u8 *key,
+ 				    unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	 struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 
+ 	if (keylen < 3) {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	keylen -= 3;
+ 	memcpy(aeadctx->salt, key + keylen, 3);
+ 	return chcr_aead_ccm_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_gcm_setkey(struct crypto_aead *aead, const u8 *key,
+ 			   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_gcm_ctx *gctx = GCM_CTX(aeadctx);
+ 	struct crypto_cipher *cipher;
+ 	unsigned int ck_size;
+ 	int ret = 0, key_ctx_size = 0;
+ 
+ 	if (get_aead_subtype(aead) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&
+ 	    keylen > 3) {
+ 		keylen -= 4;  /* nonce/salt is present in the last 4 bytes */
+ 		memcpy(aeadctx->salt, key + keylen, 4);
+ 	}
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		pr_err("GCM: Invalid key length %d", keylen);
+ 		ret = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,
+ 						CHCR_KEYCTX_MAC_KEY_SIZE_128,
+ 						0, 0,
+ 						key_ctx_size >> 4);
+ 	/* Calculate the H = CIPH(K, 0 repeated 16 times).
+ 	 * It will go in key context
+ 	 */
+ 	cipher = crypto_alloc_cipher("aes-generic", 0, 0);
+ 	if (IS_ERR(cipher)) {
+ 		aeadctx->enckey_len = 0;
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	ret = crypto_cipher_setkey(cipher, key, keylen);
+ 	if (ret) {
+ 		aeadctx->enckey_len = 0;
+ 		goto out1;
+ 	}
+ 	memset(gctx->ghash_h, 0, AEAD_H_SIZE);
+ 	crypto_cipher_encrypt_one(cipher, gctx->ghash_h, gctx->ghash_h);
+ 
+ out1:
+ 	crypto_free_cipher(cipher);
+ out:
+ 	return ret;
+ }
+ 
+ static int chcr_authenc_setkey(struct crypto_aead *authenc, const u8 *key,
+ 				   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	/* it contains auth and cipher key both*/
+ 	struct crypto_authenc_keys keys;
+ 	unsigned int bs;
+ 	unsigned int max_authsize = crypto_aead_alg(authenc)->maxauthsize;
+ 	int err = 0, i, key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 	unsigned char pad[CHCR_HASH_MAX_BLOCK_SIZE_128] = { 0 };
+ 	struct crypto_shash *base_hash = NULL;
+ 	struct algo_param param;
+ 	int align;
+ 	u8 *o_ptr = NULL;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 
+ 	if (get_alg_config(&param, max_authsize)) {
+ 		pr_err("chcr : Unsupported digest size\n");
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 
+ 	/* Copy only encryption key. We use authkey to generate h(ipad) and
+ 	 * h(opad) so authkey is not needed again. authkeylen size have the
+ 	 * size of the hash digest size.
+ 	 */
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 			    aeadctx->enckey_len << 3);
+ 
+ 	base_hash  = chcr_alloc_shash(max_authsize);
+ 	if (IS_ERR(base_hash)) {
+ 		pr_err("chcr : Base driver cannot be loaded\n");
+ 		goto out;
+ 	}
+ 	{
+ 		SHASH_DESC_ON_STACK(shash, base_hash);
+ 		shash->tfm = base_hash;
+ 		shash->flags = crypto_shash_get_flags(base_hash);
+ 		bs = crypto_shash_blocksize(base_hash);
+ 		align = KEYCTX_ALIGN_PAD(max_authsize);
+ 		o_ptr =  actx->h_iopad + param.result_size + align;
+ 
+ 		if (keys.authkeylen > bs) {
+ 			err = crypto_shash_digest(shash, keys.authkey,
+ 						  keys.authkeylen,
+ 						  o_ptr);
+ 			if (err) {
+ 				pr_err("chcr : Base driver cannot be loaded\n");
+ 				goto out;
+ 			}
+ 			keys.authkeylen = max_authsize;
+ 		} else
+ 			memcpy(o_ptr, keys.authkey, keys.authkeylen);
+ 
+ 		/* Compute the ipad-digest*/
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= IPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, actx->h_iopad,
+ 					      max_authsize))
+ 			goto out;
+ 		/* Compute the opad-digest */
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= OPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, o_ptr, max_authsize))
+ 			goto out;
+ 
+ 		/* convert the ipad and opad digest to network order */
+ 		chcr_change_order(actx->h_iopad, param.result_size);
+ 		chcr_change_order(o_ptr, param.result_size);
+ 		key_ctx_len = sizeof(struct _key_ctx) +
+ 			((DIV_ROUND_UP(keys.enckeylen, 16)) << 4) +
+ 			(param.result_size + align) * 2;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, param.mk_size,
+ 						0, 1, key_ctx_len >> 4);
+ 		actx->auth_mode = param.auth_mode;
+ 		chcr_free_shash(base_hash);
+ 
+ 		return 0;
+ 	}
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	if (base_hash)
+ 		chcr_free_shash(base_hash);
+ 	return -EINVAL;
+ }
+ 
+ static int chcr_aead_digest_null_setkey(struct crypto_aead *authenc,
+ 					const u8 *key, unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct crypto_authenc_keys keys;
+ 
+ 	/* it contains auth and cipher key both*/
+ 	int key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 				    aeadctx->enckey_len << 3);
+ 	key_ctx_len =  sizeof(struct _key_ctx)
+ 		+ ((DIV_ROUND_UP(keys.enckeylen, 16)) << 4);
+ 
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY, 0,
+ 						0, key_ctx_len >> 4);
+ 	actx->auth_mode = CHCR_SCMD_AUTH_MODE_NOP;
+ 	return 0;
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	return -EINVAL;
+ }
+ static int chcr_aead_encrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 
+ 	reqctx->verify = VERIFY_HW;
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int size;
+ 
+ 	if (aeadctx->mayverify == VERIFY_SW) {
+ 		size = crypto_aead_maxauthsize(tfm);
+ 		reqctx->verify = VERIFY_SW;
+ 	} else {
+ 		size = 0;
+ 		reqctx->verify = VERIFY_HW;
+ 	}
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_op(struct aead_request *req,
+ 			  unsigned short op_type,
+ 			  int size,
+ 			  create_wr_t create_wr_fn)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct sk_buff *skb;
+ 
+ 	if (ctx && !ctx->dev) {
+ 		pr_err("chcr : %s : No crypto device.\n", __func__);
+ 		return -ENXIO;
+ 	}
+ 	if (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 				   ctx->tx_channel_id)) {
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
+ 			return -EBUSY;
+ 	}
+ 
+ 	/* Form a WR from req */
+ 	skb = create_wr_fn(req, u_ctx->lldi.rxq_ids[ctx->tx_channel_id], size,
+ 			   op_type);
+ 
+ 	if (IS_ERR(skb) || skb == NULL) {
+ 		pr_err("chcr : %s : failed to form WR. No memory\n", __func__);
+ 		return PTR_ERR(skb);
+ 	}
+ 
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_channel_id);
+ 	chcr_send_wr(skb);
+ 	return -EINPROGRESS;
+ }
++>>>>>>> 8356ea515ba1 (crypto: chcr - Use cipher instead of Block Cipher in gcm setkey)
  static struct chcr_alg_template driver_algs[] = {
  	/* AES-CBC */
  	{
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
