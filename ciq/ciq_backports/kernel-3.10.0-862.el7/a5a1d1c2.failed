clocksource: Use a plain u64 instead of cycle_t

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/a5a1d1c2.failed

There is no point in having an extra type for extra confusion. u64 is
unambiguous.

Conversion was done with the following coccinelle script:

@rem@
@@
-typedef u64 cycle_t;

@fix@
typedef cycle_t;
@@
-cycle_t
+u64

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: John Stultz <john.stultz@linaro.org>
(cherry picked from commit a5a1d1c2914b5316924c7893eb683a5420ebd3be)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/alpha/kernel/time.c
#	arch/arm/include/asm/kvm_arch_timer.h
#	arch/arm/mach-at91/at91rm9200_time.c
#	arch/arm/mach-ep93xx/timer-ep93xx.c
#	arch/ia64/kernel/time.c
#	arch/microblaze/kernel/timer.c
#	arch/mips/kernel/csrc-sb1250.c
#	arch/mips/loongson32/common/time.c
#	arch/mips/loongson64/loongson-3/hpet.c
#	arch/mips/mti-malta/malta-time.c
#	arch/nios2/kernel/time.c
#	arch/parisc/kernel/time.c
#	arch/powerpc/kernel/time.c
#	arch/um/kernel/time.c
#	arch/x86/entry/vdso/vclock_gettime.c
#	arch/x86/include/asm/vgtod.h
#	drivers/clocksource/arc_timer.c
#	drivers/clocksource/arm_global_timer.c
#	drivers/clocksource/cadence_ttc_timer.c
#	drivers/clocksource/clksrc-dbx500-prcmu.c
#	drivers/clocksource/exynos_mct.c
#	drivers/clocksource/h8300_timer16.c
#	drivers/clocksource/h8300_tpu.c
#	drivers/clocksource/jcore-pit.c
#	drivers/clocksource/metag_generic.c
#	drivers/clocksource/mips-gic-timer.c
#	drivers/clocksource/mmio.c
#	drivers/clocksource/qcom-timer.c
#	drivers/clocksource/samsung_pwm_timer.c
#	drivers/clocksource/time-pistachio.c
#	drivers/clocksource/timer-atmel-pit.c
#	drivers/clocksource/timer-nps.c
#	drivers/clocksource/timer-prima2.c
#	drivers/clocksource/timer-sun5i.c
#	drivers/clocksource/timer-ti-32k.c
#	drivers/irqchip/irq-mips-gic.c
#	include/linux/clocksource.h
#	include/linux/dw_apb_timer.h
#	include/linux/irqchip/mips-gic.h
#	include/linux/timekeeper_internal.h
#	include/linux/types.h
#	kernel/time/clocksource.c
#	kernel/time/timekeeping.c
#	kernel/trace/ftrace.c
#	kernel/trace/trace.c
#	kernel/trace/trace_irqsoff.c
#	kernel/trace/trace_sched_wakeup.c
#	virt/kvm/arm/arch_timer.c
diff --cc arch/alpha/kernel/time.c
index e336694ca042,3bfe058d75d9..000000000000
--- a/arch/alpha/kernel/time.c
+++ b/arch/alpha/kernel/time.c
@@@ -200,6 -104,101 +200,104 @@@ irqreturn_t timer_interrupt(int irq, vo
  	return IRQ_HANDLED;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ rtc_ce_set_next_event(unsigned long evt, struct clock_event_device *ce)
+ {
+ 	/* This hook is for oneshot mode, which we don't support.  */
+ 	return -EINVAL;
+ }
+ 
+ static void __init
+ init_rtc_clockevent(void)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct clock_event_device *ce = &per_cpu(cpu_ce, cpu);
+ 
+ 	*ce = (struct clock_event_device){
+ 		.name = "rtc",
+ 		.features = CLOCK_EVT_FEAT_PERIODIC,
+ 		.rating = 100,
+ 		.cpumask = cpumask_of(cpu),
+ 		.set_next_event = rtc_ce_set_next_event,
+ 	};
+ 
+ 	clockevents_config_and_register(ce, CONFIG_HZ, 0, 0);
+ }
+ 
+ 
+ /*
+  * The QEMU clock as a clocksource primitive.
+  */
+ 
+ static u64
+ qemu_cs_read(struct clocksource *cs)
+ {
+ 	return qemu_get_vmtime();
+ }
+ 
+ static struct clocksource qemu_cs = {
+ 	.name                   = "qemu",
+ 	.rating                 = 400,
+ 	.read                   = qemu_cs_read,
+ 	.mask                   = CLOCKSOURCE_MASK(64),
+ 	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS,
+ 	.max_idle_ns		= LONG_MAX
+ };
+ 
+ 
+ /*
+  * The QEMU alarm as a clock_event_device primitive.
+  */
+ 
+ static int qemu_ce_shutdown(struct clock_event_device *ce)
+ {
+ 	/* The mode member of CE is updated for us in generic code.
+ 	   Just make sure that the event is disabled.  */
+ 	qemu_set_alarm_abs(0);
+ 	return 0;
+ }
+ 
+ static int
+ qemu_ce_set_next_event(unsigned long evt, struct clock_event_device *ce)
+ {
+ 	qemu_set_alarm_rel(evt);
+ 	return 0;
+ }
+ 
+ static irqreturn_t
+ qemu_timer_interrupt(int irq, void *dev)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct clock_event_device *ce = &per_cpu(cpu_ce, cpu);
+ 
+ 	ce->event_handler(ce);
+ 	return IRQ_HANDLED;
+ }
+ 
+ static void __init
+ init_qemu_clockevent(void)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct clock_event_device *ce = &per_cpu(cpu_ce, cpu);
+ 
+ 	*ce = (struct clock_event_device){
+ 		.name = "qemu",
+ 		.features = CLOCK_EVT_FEAT_ONESHOT,
+ 		.rating = 400,
+ 		.cpumask = cpumask_of(cpu),
+ 		.set_state_shutdown = qemu_ce_shutdown,
+ 		.set_state_oneshot = qemu_ce_shutdown,
+ 		.tick_resume = qemu_ce_shutdown,
+ 		.set_next_event = qemu_ce_set_next_event,
+ 	};
+ 
+ 	clockevents_config_and_register(ce, NSEC_PER_SEC, 1000, LONG_MAX);
+ }
+ 
+ 
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  void __init
  common_init_rtc(void)
  {
@@@ -235,16 -244,37 +333,36 @@@
  	init_rtc_irq();
  }
  
++<<<<<<< HEAD
 +unsigned int common_get_rtc_time(struct rtc_time *time)
++=======
+ 
+ #ifndef CONFIG_ALPHA_WTINT
+ /*
+  * The RPCC as a clocksource primitive.
+  *
+  * While we have free-running timecounters running on all CPUs, and we make
+  * a half-hearted attempt in init_rtc_rpcc_info to sync the timecounter
+  * with the wall clock, that initialization isn't kept up-to-date across
+  * different time counters in SMP mode.  Therefore we can only use this
+  * method when there's only one CPU enabled.
+  *
+  * When using the WTINT PALcall, the RPCC may shift to a lower frequency,
+  * or stop altogether, while waiting for the interrupt.  Therefore we cannot
+  * use this method when WTINT is in use.
+  */
+ 
+ static u64 read_rpcc(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
 -	return rpcc();
 +	return __get_rtc_time(time);
  }
  
 -static struct clocksource clocksource_rpcc = {
 -	.name                   = "rpcc",
 -	.rating                 = 300,
 -	.read                   = read_rpcc,
 -	.mask                   = CLOCKSOURCE_MASK(32),
 -	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS
 -};
 -#endif /* ALPHA_WTINT */
 +int common_set_rtc_time(struct rtc_time *time)
 +{
 +	return __set_rtc_time(time);
 +}
  
 -
  /* Validate a computed cycle counter result against the known bounds for
     the given processor core.  There's too much brokenness in the way of
     timing hardware for any one method to work everywhere.  :-(
diff --cc arch/arm/include/asm/kvm_arch_timer.h
index 68cb9e1dfb81,b717ed9d2b75..000000000000
--- a/arch/arm/include/asm/kvm_arch_timer.h
+++ b/arch/arm/include/asm/kvm_arch_timer.h
@@@ -24,20 -24,14 +24,24 @@@
  #include <linux/workqueue.h>
  
  struct arch_timer_kvm {
 +#ifdef CONFIG_KVM_ARM_TIMER
 +	/* Is the timer enabled */
 +	bool			enabled;
 +
  	/* Virtual offset */
++<<<<<<< HEAD:arch/arm/include/asm/kvm_arch_timer.h
 +	cycle_t			cntvoff;
 +#endif
++=======
+ 	u64			cntvoff;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t):include/kvm/arm_arch_timer.h
  };
  
  struct arch_timer_cpu {
 +#ifdef CONFIG_KVM_ARM_TIMER
  	/* Registers: control register, timer value */
  	u32				cntv_ctl;	/* Saved/restored */
- 	cycle_t				cntv_cval;	/* Saved/restored */
+ 	u64				cntv_cval;	/* Saved/restored */
  
  	/*
  	 * Anything that is not used directly from assembly code goes
diff --cc arch/arm/mach-at91/at91rm9200_time.c
index 180b3024bec3,be4ac7604136..000000000000
--- a/arch/arm/mach-at91/at91rm9200_time.c
+++ b/arch/arm/mach-at91/at91rm9200_time.c
@@@ -91,14 -92,7 +91,18 @@@ static irqreturn_t at91rm9200_timer_int
  	return IRQ_NONE;
  }
  
++<<<<<<< HEAD:arch/arm/mach-at91/at91rm9200_time.c
 +static struct irqaction at91rm9200_timer_irq = {
 +	.name		= "at91_tick",
 +	.flags		= IRQF_SHARED | IRQF_DISABLED | IRQF_TIMER | IRQF_IRQPOLL,
 +	.handler	= at91rm9200_timer_interrupt,
 +	.irq		= NR_IRQS_LEGACY + AT91_ID_SYS,
 +};
 +
 +static cycle_t read_clk32k(struct clocksource *cs)
++=======
+ static u64 read_clk32k(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t):drivers/clocksource/timer-atmel-st.c
  {
  	return read_CRTR();
  }
diff --cc arch/ia64/kernel/time.c
index fbaac1afb844,71775b95d6cc..000000000000
--- a/arch/ia64/kernel/time.c
+++ b/arch/ia64/kernel/time.c
@@@ -441,7 -397,7 +441,11 @@@ void update_vsyscall_tz(void
  }
  
  void update_vsyscall_old(struct timespec *wall, struct timespec *wtm,
++<<<<<<< HEAD
 +			struct clocksource *c, u32 mult)
++=======
+ 			 struct clocksource *c, u32 mult, u64 cycle_last)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	write_seqcount_begin(&fsyscall_gtod_data.seq);
  
diff --cc arch/microblaze/kernel/timer.c
index aec5020a6e31,1d6fad50fa76..000000000000
--- a/arch/microblaze/kernel/timer.c
+++ b/arch/microblaze/kernel/timer.c
@@@ -162,41 -165,48 +162,57 @@@ static irqreturn_t timer_interrupt(int 
  
  static struct irqaction timer_irqaction = {
  	.handler = timer_interrupt,
 -	.flags = IRQF_TIMER,
 +	.flags = IRQF_DISABLED | IRQF_TIMER,
  	.name = "timer",
 -	.dev_id = &clockevent_xilinx_timer,
 +	.dev_id = &clockevent_microblaze_timer,
  };
  
 -static __init int xilinx_clockevent_init(void)
 +static __init void microblaze_clockevent_init(void)
  {
 -	clockevent_xilinx_timer.mult =
 +	clockevent_microblaze_timer.mult =
  		div_sc(timer_clock_freq, NSEC_PER_SEC,
 -				clockevent_xilinx_timer.shift);
 -	clockevent_xilinx_timer.max_delta_ns =
 -		clockevent_delta2ns((u32)~0, &clockevent_xilinx_timer);
 -	clockevent_xilinx_timer.min_delta_ns =
 -		clockevent_delta2ns(1, &clockevent_xilinx_timer);
 -	clockevent_xilinx_timer.cpumask = cpumask_of(0);
 -	clockevents_register_device(&clockevent_xilinx_timer);
 -
 -	return 0;
 +				clockevent_microblaze_timer.shift);
 +	clockevent_microblaze_timer.max_delta_ns =
 +		clockevent_delta2ns((u32)~0, &clockevent_microblaze_timer);
 +	clockevent_microblaze_timer.min_delta_ns =
 +		clockevent_delta2ns(1, &clockevent_microblaze_timer);
 +	clockevent_microblaze_timer.cpumask = cpumask_of(0);
 +	clockevents_register_device(&clockevent_microblaze_timer);
  }
  
++<<<<<<< HEAD
 +static cycle_t microblaze_read(struct clocksource *cs)
 +{
 +	/* reading actual value of timer 1 */
 +	return (cycle_t) (in_be32(TIMER_BASE + TCR1));
++=======
+ static u64 xilinx_clock_read(void)
+ {
+ 	return read_fn(timer_baseaddr + TCR1);
+ }
+ 
+ static u64 xilinx_read(struct clocksource *cs)
+ {
+ 	/* reading actual value of timer 1 */
+ 	return (u64)xilinx_clock_read();
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  }
  
 -static struct timecounter xilinx_tc = {
 +static struct timecounter microblaze_tc = {
  	.cc = NULL,
  };
  
++<<<<<<< HEAD
 +static cycle_t microblaze_cc_read(const struct cyclecounter *cc)
++=======
+ static u64 xilinx_cc_read(const struct cyclecounter *cc)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
 -	return xilinx_read(NULL);
 +	return microblaze_read(NULL);
  }
  
 -static struct cyclecounter xilinx_cc = {
 -	.read = xilinx_cc_read,
 +static struct cyclecounter microblaze_cc = {
 +	.read = microblaze_cc_read,
  	.mask = CLOCKSOURCE_MASK(32),
  	.shift = 8,
  };
diff --cc arch/mips/kernel/csrc-sb1250.c
index 6ecb77d82063,b07b7310d3f4..000000000000
--- a/arch/mips/kernel/csrc-sb1250.c
+++ b/arch/mips/kernel/csrc-sb1250.c
@@@ -33,13 -30,20 +33,28 @@@
   * The HPT is free running from SB1250_HPT_VALUE down to 0 then starts over
   * again.
   */
++<<<<<<< HEAD
 +static cycle_t sb1250_hpt_read(struct clocksource *cs)
++=======
+ static inline u64 sb1250_hpt_get_cycles(void)
+ {
+ 	unsigned int count;
+ 	void __iomem *addr;
+ 
+ 	addr = IOADDR(A_SCD_TIMER_REGISTER(SB1250_HPT_NUM, R_SCD_TIMER_CNT));
+ 	count = G_SCD_TIMER_CNT(__raw_readq(addr));
+ 
+ 	return SB1250_HPT_VALUE - count;
+ }
+ 
+ static u64 sb1250_hpt_read(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
 -	return sb1250_hpt_get_cycles();
 +	unsigned int count;
 +
 +	count = G_SCD_TIMER_CNT(__raw_readq(IOADDR(A_SCD_TIMER_REGISTER(SB1250_HPT_NUM, R_SCD_TIMER_CNT))));
 +
 +	return SB1250_HPT_VALUE - count;
  }
  
  struct clocksource bcm1250_clocksource = {
diff --cc arch/mips/mti-malta/malta-time.c
index 0ad305f75802,1829a9031eec..000000000000
--- a/arch/mips/mti-malta/malta-time.c
+++ b/arch/mips/mti-malta/malta-time.c
@@@ -71,22 -73,12 +71,28 @@@ static void __init estimate_frequencies
  {
  	unsigned long flags;
  	unsigned int count, start;
++<<<<<<< HEAD
 +#ifdef CONFIG_IRQ_GIC
 +	unsigned int giccount = 0, gicstart = 0;
 +#endif
++=======
+ 	unsigned char secs1, secs2, ctrl;
+ 	int secs;
+ 	u64 giccount = 0, gicstart = 0;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
 +
 +#if defined (CONFIG_KVM_GUEST) && defined (CONFIG_KVM_HOST_FREQ)
 +	unsigned int prid = read_c0_prid() & 0xffff00;
 +
 +	/*
 +	 * XXXKYMA: hardwire the CPU frequency to Host Freq/4
 +	 */
 +	count = (CONFIG_KVM_HOST_FREQ * 1000000) >> 3;
 +	if ((prid != (PRID_COMP_MIPS | PRID_IMP_20KC)) &&
 +	    (prid != (PRID_COMP_MIPS | PRID_IMP_25KF)))
 +		count *= 2;
  
 -#if defined(CONFIG_KVM_GUEST) && CONFIG_KVM_GUEST_TIMER_FREQ
 -	mips_hpt_frequency = CONFIG_KVM_GUEST_TIMER_FREQ * 1000000;
 +	mips_hpt_frequency = count;
  	return;
  #endif
  
diff --cc arch/parisc/kernel/time.c
index 70e105d62423,da0d9cb63403..000000000000
--- a/arch/parisc/kernel/time.c
+++ b/arch/parisc/kernel/time.c
@@@ -189,7 -137,7 +189,11 @@@ EXPORT_SYMBOL(profile_pc)
  
  /* clock source code */
  
++<<<<<<< HEAD
 +static cycle_t read_cr16(struct clocksource *cs)
++=======
+ static u64 notrace read_cr16(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	return get_cycles();
  }
diff --cc arch/powerpc/kernel/time.c
index 859343e21b75,bc2e08d415fa..000000000000
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@@ -779,7 -813,7 +779,11 @@@ static u64 timebase_read(struct clockso
  }
  
  void update_vsyscall_old(struct timespec *wall_time, struct timespec *wtm,
++<<<<<<< HEAD
 +			struct clocksource *clock, u32 mult)
++=======
+ 			 struct clocksource *clock, u32 mult, u64 cycle_last)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	u64 new_tb_to_xs, new_stamp_xsec;
  	u32 frac_sec;
diff --cc arch/um/kernel/time.c
index 117568d4f64a,ba87a27d6715..000000000000
--- a/arch/um/kernel/time.c
+++ b/arch/um/kernel/time.c
@@@ -65,15 -83,15 +65,19 @@@ static irqreturn_t um_timer(int irq, vo
  	return IRQ_HANDLED;
  }
  
++<<<<<<< HEAD
 +static cycle_t itimer_read(struct clocksource *cs)
++=======
+ static u64 timer_read(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
 -	return os_nsecs() / TIMER_MULTIPLIER;
 +	return os_nsecs() / 1000;
  }
  
 -static struct clocksource timer_clocksource = {
 -	.name		= "timer",
 +static struct clocksource itimer_clocksource = {
 +	.name		= "itimer",
  	.rating		= 300,
 -	.read		= timer_read,
 +	.read		= itimer_read,
  	.mask		= CLOCKSOURCE_MASK(64),
  	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
  };
diff --cc arch/x86/include/asm/vgtod.h
index 46e24d36b7da,022e59714562..000000000000
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@@ -1,29 -1,39 +1,37 @@@
  #ifndef _ASM_X86_VGTOD_H
  #define _ASM_X86_VGTOD_H
  
 -#include <linux/compiler.h>
 +#include <asm/vsyscall.h>
  #include <linux/clocksource.h>
  
 -#ifdef BUILD_VDSO32_64
 -typedef u64 gtod_long_t;
 -#else
 -typedef unsigned long gtod_long_t;
 -#endif
 -/*
 - * vsyscall_gtod_data will be accessed by 32 and 64 bit code at the same time
 - * so be carefull by modifying this structure.
 - */
  struct vsyscall_gtod_data {
 -	unsigned seq;
 -
 +	seqcount_t	seq;
 +
++<<<<<<< HEAD
 +	struct { /* extract of a clocksource struct */
 +		int vclock_mode;
 +		cycle_t	cycle_last;
 +		cycle_t	mask;
 +		u32	mult;
 +		u32	shift;
 +	} clock;
++=======
+ 	int vclock_mode;
+ 	u64	cycle_last;
+ 	u64	mask;
+ 	u32	mult;
+ 	u32	shift;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  
  	/* open coded 'struct timespec' */
 +	time_t		wall_time_sec;
  	u64		wall_time_snsec;
 -	gtod_long_t	wall_time_sec;
 -	gtod_long_t	monotonic_time_sec;
  	u64		monotonic_time_snsec;
 -	gtod_long_t	wall_time_coarse_sec;
 -	gtod_long_t	wall_time_coarse_nsec;
 -	gtod_long_t	monotonic_time_coarse_sec;
 -	gtod_long_t	monotonic_time_coarse_nsec;
 +	time_t		monotonic_time_sec;
  
 -	int		tz_minuteswest;
 -	int		tz_dsttime;
 +	struct timezone sys_tz;
 +	struct timespec wall_time_coarse;
 +	struct timespec monotonic_time_coarse;
  };
  extern struct vsyscall_gtod_data vsyscall_gtod_data;
  
diff --cc drivers/clocksource/cadence_ttc_timer.c
index 685bc60e210a,44e5e951583b..000000000000
--- a/drivers/clocksource/cadence_ttc_timer.c
+++ b/drivers/clocksource/cadence_ttc_timer.c
@@@ -150,7 -162,7 +150,11 @@@ static u64 __ttc_clocksource_read(struc
  {
  	struct ttc_timer *timer = &to_ttc_timer_clksrc(cs)->ttc;
  
++<<<<<<< HEAD
 +	return (cycle_t)__raw_readl(timer->base_addr +
++=======
+ 	return (u64)readl_relaxed(timer->base_addr +
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  				TTC_COUNT_VAL_OFFSET);
  }
  
diff --cc drivers/clocksource/clksrc-dbx500-prcmu.c
index 54f3d119d99c,c69e2772658d..000000000000
--- a/drivers/clocksource/clksrc-dbx500-prcmu.c
+++ b/drivers/clocksource/clksrc-dbx500-prcmu.c
@@@ -30,8 -30,9 +30,12 @@@
  
  static void __iomem *clksrc_dbx500_timer_base;
  
++<<<<<<< HEAD
 +static cycle_t clksrc_dbx500_prcmu_read(struct clocksource *cs)
++=======
+ static u64 notrace clksrc_dbx500_prcmu_read(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
 -	void __iomem *base = clksrc_dbx500_timer_base;
  	u32 count, count2;
  
  	do {
diff --cc drivers/clocksource/exynos_mct.c
index fd71b56c98f2,c8b9f834f4de..000000000000
--- a/drivers/clocksource/exynos_mct.c
+++ b/drivers/clocksource/exynos_mct.c
@@@ -169,19 -179,37 +169,40 @@@ static cycle_t exynos4_frc_read(struct 
  
  	do {
  		hi = hi2;
 -		lo = readl_relaxed(reg_base + EXYNOS4_MCT_G_CNT_L);
 -		hi2 = readl_relaxed(reg_base + EXYNOS4_MCT_G_CNT_U);
 +		lo = __raw_readl(reg_base + EXYNOS4_MCT_G_CNT_L);
 +		hi2 = __raw_readl(reg_base + EXYNOS4_MCT_G_CNT_U);
  	} while (hi != hi2);
  
- 	return ((cycle_t)hi << 32) | lo;
+ 	return ((u64)hi << 32) | lo;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * exynos4_read_count_32 - Read the lower 32-bits of the global counter
+  *
+  * This will read just the lower 32-bits of the global counter.  This is marked
+  * as notrace so it can be used by the scheduler clock.
+  *
+  * Returns the number of cycles in the global counter (lower 32 bits).
+  */
+ static u32 notrace exynos4_read_count_32(void)
+ {
+ 	return readl_relaxed(reg_base + EXYNOS4_MCT_G_CNT_L);
+ }
+ 
+ static u64 exynos4_frc_read(struct clocksource *cs)
+ {
+ 	return exynos4_read_count_32();
+ }
+ 
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  static void exynos4_frc_resume(struct clocksource *cs)
  {
 -	exynos4_mct_frc_start();
 +	exynos4_mct_frc_start(0, 0);
  }
  
 -static struct clocksource mct_frc = {
 +struct clocksource mct_frc = {
  	.name		= "mct-frc",
  	.rating		= 400,
  	.read		= exynos4_frc_read,
@@@ -209,15 -263,14 +230,15 @@@ static void exynos4_mct_comp0_stop(void
  	exynos4_mct_write(0, EXYNOS4_MCT_G_INT_ENB);
  }
  
 -static void exynos4_mct_comp0_start(bool periodic, unsigned long cycles)
 +static void exynos4_mct_comp0_start(enum clock_event_mode mode,
 +				    unsigned long cycles)
  {
  	unsigned int tcon;
- 	cycle_t comp_cycle;
+ 	u64 comp_cycle;
  
 -	tcon = readl_relaxed(reg_base + EXYNOS4_MCT_G_TCON);
 +	tcon = __raw_readl(reg_base + EXYNOS4_MCT_G_TCON);
  
 -	if (periodic) {
 +	if (mode == CLOCK_EVT_MODE_PERIODIC) {
  		tcon |= MCT_G_TCON_COMP0_AUTO_INC;
  		exynos4_mct_write(cycles, EXYNOS4_MCT_G_COMP0_ADD_INCR);
  	}
diff --cc drivers/clocksource/metag_generic.c
index 05ccb99acf3e,8d06a0f7ff26..000000000000
--- a/drivers/clocksource/metag_generic.c
+++ b/drivers/clocksource/metag_generic.c
@@@ -56,26 -56,7 +56,30 @@@ static int metag_timer_set_next_event(u
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void metag_timer_set_mode(enum clock_event_mode mode,
 +				 struct clock_event_device *evt)
 +{
 +	switch (mode) {
 +	case CLOCK_EVT_MODE_ONESHOT:
 +	case CLOCK_EVT_MODE_RESUME:
 +		break;
 +
 +	case CLOCK_EVT_MODE_SHUTDOWN:
 +		/* We should disable the IRQ here */
 +		break;
 +
 +	case CLOCK_EVT_MODE_PERIODIC:
 +	case CLOCK_EVT_MODE_UNUSED:
 +		WARN_ON(1);
 +		break;
 +	};
 +}
 +
 +static cycle_t metag_clocksource_read(struct clocksource *cs)
++=======
+ static u64 metag_clocksource_read(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	return __core_reg_get(TXTIMER);
  }
diff --cc drivers/clocksource/mmio.c
index c0e25125a55e,4c4df981d8cc..000000000000
--- a/drivers/clocksource/mmio.c
+++ b/drivers/clocksource/mmio.c
@@@ -20,24 -20,24 +20,40 @@@ static inline struct clocksource_mmio *
  	return container_of(c, struct clocksource_mmio, clksrc);
  }
  
- cycle_t clocksource_mmio_readl_up(struct clocksource *c)
+ u64 clocksource_mmio_readl_up(struct clocksource *c)
  {
++<<<<<<< HEAD
 +	return readl_relaxed(to_mmio_clksrc(c)->reg);
++=======
+ 	return (u64)readl_relaxed(to_mmio_clksrc(c)->reg);
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  }
  
- cycle_t clocksource_mmio_readl_down(struct clocksource *c)
+ u64 clocksource_mmio_readl_down(struct clocksource *c)
  {
++<<<<<<< HEAD
 +	return ~readl_relaxed(to_mmio_clksrc(c)->reg);
++=======
+ 	return ~(u64)readl_relaxed(to_mmio_clksrc(c)->reg) & c->mask;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  }
  
- cycle_t clocksource_mmio_readw_up(struct clocksource *c)
+ u64 clocksource_mmio_readw_up(struct clocksource *c)
  {
++<<<<<<< HEAD
 +	return readw_relaxed(to_mmio_clksrc(c)->reg);
++=======
+ 	return (u64)readw_relaxed(to_mmio_clksrc(c)->reg);
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  }
  
- cycle_t clocksource_mmio_readw_down(struct clocksource *c)
+ u64 clocksource_mmio_readw_down(struct clocksource *c)
  {
++<<<<<<< HEAD
 +	return ~(unsigned)readw_relaxed(to_mmio_clksrc(c)->reg);
++=======
+ 	return ~(u64)readw_relaxed(to_mmio_clksrc(c)->reg) & c->mask;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  }
  
  /**
diff --cc drivers/clocksource/samsung_pwm_timer.c
index 0234c8d2c8f2,0093ece661fe..000000000000
--- a/drivers/clocksource/samsung_pwm_timer.c
+++ b/drivers/clocksource/samsung_pwm_timer.c
@@@ -286,23 -293,34 +286,49 @@@ static void __init samsung_clockevent_i
  	}
  }
  
 -static void samsung_clocksource_suspend(struct clocksource *cs)
 +static void __iomem *samsung_timer_reg(void)
  {
 -	samsung_time_stop(pwm.source_id);
 +	switch (pwm.source_id) {
 +	case 0:
 +	case 1:
 +	case 2:
 +	case 3:
 +		return pwm.base + pwm.source_id * 0x0c + 0x14;
 +
 +	case 4:
 +		return pwm.base + 0x40;
 +
 +	default:
 +		BUG();
 +	}
  }
  
++<<<<<<< HEAD
++=======
+ static void samsung_clocksource_resume(struct clocksource *cs)
+ {
+ 	samsung_timer_set_prescale(pwm.source_id, pwm.tscaler_div);
+ 	samsung_timer_set_divisor(pwm.source_id, pwm.tdiv);
+ 
+ 	samsung_time_setup(pwm.source_id, pwm.tcnt_max);
+ 	samsung_time_start(pwm.source_id, true);
+ }
+ 
+ static u64 notrace samsung_clocksource_read(struct clocksource *c)
+ {
+ 	return ~readl_relaxed(pwm.source_reg);
+ }
+ 
+ static struct clocksource samsung_clocksource = {
+ 	.name		= "samsung_clocksource_timer",
+ 	.rating		= 250,
+ 	.read		= samsung_clocksource_read,
+ 	.suspend	= samsung_clocksource_suspend,
+ 	.resume		= samsung_clocksource_resume,
+ 	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
+ };
+ 
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  /*
   * Override the global weak sched_clock symbol with this
   * local implementation which uses the clocksource to get some
diff --cc drivers/clocksource/timer-prima2.c
index 760882665d7a,bfa981ac1eaf..000000000000
--- a/drivers/clocksource/timer-prima2.c
+++ b/drivers/clocksource/timer-prima2.c
@@@ -70,7 -72,7 +70,11 @@@ static irqreturn_t sirfsoc_timer_interr
  }
  
  /* read 64-bit timer counter */
++<<<<<<< HEAD
 +static cycle_t sirfsoc_timer_read(struct clocksource *cs)
++=======
+ static u64 notrace sirfsoc_timer_read(struct clocksource *cs)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	u64 cycles;
  
diff --cc include/linux/clocksource.h
index 9a44d85904f0,e315d04a2fd9..000000000000
--- a/include/linux/clocksource.h
+++ b/include/linux/clocksource.h
@@@ -60,17 -62,21 +60,22 @@@ struct module
   * @archdata:		arch-specific data
   * @suspend:		suspend function for the clocksource, if necessary
   * @resume:		resume function for the clocksource, if necessary
 + * @cycle_last:		most recent cycle counter value seen by ::read()
   * @owner:		module reference, must be set by clocksource in modules
 - *
 - * Note: This struct is not used in hotpathes of the timekeeping code
 - * because the timekeeper caches the hot path fields in its own data
 - * structure, so no line cache alignment is required,
 - *
 - * The pointer to the clocksource itself is handed to the read
 - * callback. If you need extra information there you can wrap struct
 - * clocksource into your own struct. Depending on the amount of
 - * information you need you should consider to cache line align that
 - * structure.
   */
  struct clocksource {
++<<<<<<< HEAD
 +	/*
 +	 * Hotpath data, fits in a single cache line when the
 +	 * clocksource itself is cacheline aligned.
 +	 */
 +	cycle_t (*read)(struct clocksource *cs);
 +	cycle_t cycle_last;
 +	cycle_t mask;
++=======
+ 	u64 (*read)(struct clocksource *cs);
+ 	u64 mask;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  	u32 mult;
  	u32 shift;
  	u64 max_idle_ns;
@@@ -92,11 -98,11 +97,11 @@@
  #ifdef CONFIG_CLOCKSOURCE_WATCHDOG
  	/* Watchdog related data, used by the framework */
  	struct list_head wd_list;
- 	cycle_t cs_last;
- 	cycle_t wd_last;
+ 	u64 cs_last;
+ 	u64 wd_last;
  #endif
  	struct module *owner;
 -};
 +} ____cacheline_aligned;
  
  /*
   * Clock source flags bits::
@@@ -111,8 -117,25 +116,8 @@@
  #define CLOCK_SOURCE_RESELECT			0x100
  
  /* simplify initialization of mask field */
- #define CLOCKSOURCE_MASK(bits) (cycle_t)((bits) < 64 ? ((1ULL<<(bits))-1) : -1)
+ #define CLOCKSOURCE_MASK(bits) (u64)((bits) < 64 ? ((1ULL<<(bits))-1) : -1)
  
 -static inline u32 clocksource_freq2mult(u32 freq, u32 shift_constant, u64 from)
 -{
 -	/*  freq = cyc/from
 -	 *  mult/2^shift  = ns/cyc
 -	 *  mult = ns/cyc * 2^shift
 -	 *  mult = from/freq * 2^shift
 -	 *  mult = from * 2^shift / freq
 -	 *  mult = (from<<shift) / freq
 -	 */
 -	u64 tmp = ((u64)from) << shift_constant;
 -
 -	tmp += freq/2; /* round for do_div */
 -	do_div(tmp, freq);
 -
 -	return (u32)tmp;
 -}
 -
  /**
   * clocksource_khz2mult - calculates mult from khz and shift
   * @khz:		Clocksource frequency in KHz
diff --cc include/linux/dw_apb_timer.h
index dd755ce2a5eb,4334106f44c3..000000000000
--- a/include/linux/dw_apb_timer.h
+++ b/include/linux/dw_apb_timer.h
@@@ -50,8 -50,6 +50,12 @@@ dw_apb_clocksource_init(unsigned rating
  			unsigned long freq);
  void dw_apb_clocksource_register(struct dw_apb_clocksource *dw_cs);
  void dw_apb_clocksource_start(struct dw_apb_clocksource *dw_cs);
++<<<<<<< HEAD
 +cycle_t dw_apb_clocksource_read(struct dw_apb_clocksource *dw_cs);
 +void dw_apb_clocksource_unregister(struct dw_apb_clocksource *dw_cs);
++=======
+ u64 dw_apb_clocksource_read(struct dw_apb_clocksource *dw_cs);
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  
 +extern void dw_apb_timer_init(void);
  #endif /* __DW_APB_TIMER_H__ */
diff --cc include/linux/timekeeper_internal.h
index 3e4944d6e94c,110f4532188c..000000000000
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@@ -27,7 -29,9 +27,13 @@@
   */
  struct tk_read_base {
  	struct clocksource	*clock;
++<<<<<<< HEAD
 +	cycle_t			cycle_last;
++=======
+ 	u64			(*read)(struct clocksource *cs);
+ 	u64			mask;
+ 	u64			cycle_last;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  	u32			mult;
  	u32			shift;
  	u64			xtime_nsec;
@@@ -86,10 -94,10 +92,14 @@@ struct timekeeper 
  	struct timespec64	raw_time;
  
  	/* The following members are for timekeeping internal use */
++<<<<<<< HEAD
 +	unsigned int		clock_was_set_seq;
 +	u8			cs_was_changed_seq;
 +	ktime_t			next_leap_ktime;
 +	cycle_t			cycle_interval;
++=======
+ 	u64			cycle_interval;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  	u64			xtime_interval;
  	s64			xtime_remainder;
  	u32			raw_interval;
@@@ -125,18 -135,10 +135,23 @@@ extern void update_vsyscall_tz(void)
  #elif defined(CONFIG_GENERIC_TIME_VSYSCALL_OLD)
  
  extern void update_vsyscall_old(struct timespec *ts, struct timespec *wtm,
++<<<<<<< HEAD
 +				struct clocksource *c, u32 mult);
++=======
+ 				struct clocksource *c, u32 mult,
+ 				u64 cycle_last);
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  extern void update_vsyscall_tz(void);
  
 +static inline void update_vsyscall(struct timekeeper *tk)
 +{
 +	struct timespec xt;
 +
 +	xt = tk_xtime(tk);
 +	update_vsyscall_old(&xt, &tk->wall_to_monotonic, tk->tkr_mono.clock,
 +			    tk->tkr_mono.mult);
 +}
 +
  #else
  
  static inline void update_vsyscall(struct timekeeper *tk)
diff --cc include/linux/types.h
index 2533a70a926b,1e7bd24848fc..000000000000
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@@ -218,11 -222,11 +218,16 @@@ struct ustat 
  struct callback_head {
  	struct callback_head *next;
  	void (*func)(struct callback_head *head);
 -} __attribute__((aligned(sizeof(void *))));
 +};
  #define rcu_head callback_head
  
++<<<<<<< HEAD
 +/* clocksource cycle base type */
 +typedef u64 cycle_t;
++=======
+ typedef void (*rcu_callback_t)(struct rcu_head *head);
+ typedef void (*call_rcu_func_t)(struct rcu_head *head, rcu_callback_t func);
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  
  #endif /*  __ASSEMBLY__ */
  #endif /* _LINUX_TYPES_H */
diff --cc kernel/time/clocksource.c
index b85dc1d4ff0c,665985b0a89a..000000000000
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@@ -174,7 -170,7 +174,11 @@@ void clocksource_mark_unstable(struct c
  static void clocksource_watchdog(unsigned long data)
  {
  	struct clocksource *cs;
++<<<<<<< HEAD
 +	cycle_t csnow, wdnow, delta;
++=======
+ 	u64 csnow, wdnow, cslast, wdlast, delta;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  	int64_t wd_nsec, cs_nsec;
  	int next_cpu, reset_pending;
  
diff --cc kernel/time/timekeeping.c
index e79a23a1bd03,f4152a69277f..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -98,18 -111,115 +98,121 @@@ static void tk_set_wall_to_mono(struct 
  	tk->offs_tai = ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0));
  }
  
 -static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)
 +static void tk_set_sleep_time(struct timekeeper *tk, struct timespec64 t)
  {
 -	tk->offs_boot = ktime_add(tk->offs_boot, delta);
 +	/* Verify consistency before modifying */
 +	WARN_ON_ONCE(tk->offs_boot.tv64 != timespec64_to_ktime(tk->total_sleep_time).tv64);
 +
 +	tk->total_sleep_time	= t;
 +	tk->offs_boot		= timespec64_to_ktime(t);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DEBUG_TIMEKEEPING
+ #define WARNING_FREQ (HZ*300) /* 5 minute rate-limiting */
+ 
+ static void timekeeping_check_update(struct timekeeper *tk, u64 offset)
+ {
+ 
+ 	u64 max_cycles = tk->tkr_mono.clock->max_cycles;
+ 	const char *name = tk->tkr_mono.clock->name;
+ 
+ 	if (offset > max_cycles) {
+ 		printk_deferred("WARNING: timekeeping: Cycle offset (%lld) is larger than allowed by the '%s' clock's max_cycles value (%lld): time overflow danger\n",
+ 				offset, name, max_cycles);
+ 		printk_deferred("         timekeeping: Your kernel is sick, but tries to cope by capping time updates\n");
+ 	} else {
+ 		if (offset > (max_cycles >> 1)) {
+ 			printk_deferred("INFO: timekeeping: Cycle offset (%lld) is larger than the '%s' clock's 50%% safety margin (%lld)\n",
+ 					offset, name, max_cycles >> 1);
+ 			printk_deferred("      timekeeping: Your kernel is still fine, but is feeling a bit nervous\n");
+ 		}
+ 	}
+ 
+ 	if (tk->underflow_seen) {
+ 		if (jiffies - tk->last_warning > WARNING_FREQ) {
+ 			printk_deferred("WARNING: Underflow in clocksource '%s' observed, time update ignored.\n", name);
+ 			printk_deferred("         Please report this, consider using a different clocksource, if possible.\n");
+ 			printk_deferred("         Your kernel is probably still fine.\n");
+ 			tk->last_warning = jiffies;
+ 		}
+ 		tk->underflow_seen = 0;
+ 	}
+ 
+ 	if (tk->overflow_seen) {
+ 		if (jiffies - tk->last_warning > WARNING_FREQ) {
+ 			printk_deferred("WARNING: Overflow in clocksource '%s' observed, time update capped.\n", name);
+ 			printk_deferred("         Please report this, consider using a different clocksource, if possible.\n");
+ 			printk_deferred("         Your kernel is probably still fine.\n");
+ 			tk->last_warning = jiffies;
+ 		}
+ 		tk->overflow_seen = 0;
+ 	}
+ }
+ 
+ static inline u64 timekeeping_get_delta(struct tk_read_base *tkr)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	u64 now, last, mask, max, delta;
+ 	unsigned int seq;
+ 
+ 	/*
+ 	 * Since we're called holding a seqlock, the data may shift
+ 	 * under us while we're doing the calculation. This can cause
+ 	 * false positives, since we'd note a problem but throw the
+ 	 * results away. So nest another seqlock here to atomically
+ 	 * grab the points we are checking with.
+ 	 */
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		now = tkr->read(tkr->clock);
+ 		last = tkr->cycle_last;
+ 		mask = tkr->mask;
+ 		max = tkr->clock->max_cycles;
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	delta = clocksource_delta(now, last, mask);
+ 
+ 	/*
+ 	 * Try to catch underflows by checking if we are seeing small
+ 	 * mask-relative negative values.
+ 	 */
+ 	if (unlikely((~delta & mask) < (mask >> 3))) {
+ 		tk->underflow_seen = 1;
+ 		delta = 0;
+ 	}
+ 
+ 	/* Cap delta value to the max_cycles values to avoid mult overflows */
+ 	if (unlikely(delta > max)) {
+ 		tk->overflow_seen = 1;
+ 		delta = tkr->clock->max_cycles;
+ 	}
+ 
+ 	return delta;
+ }
+ #else
+ static inline void timekeeping_check_update(struct timekeeper *tk, u64 offset)
+ {
+ }
+ static inline u64 timekeeping_get_delta(struct tk_read_base *tkr)
+ {
+ 	u64 cycle_now, delta;
+ 
+ 	/* read clocksource */
+ 	cycle_now = tkr->read(tkr->clock);
+ 
+ 	/* calculate the delta since the last update_wall_time */
+ 	delta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);
+ 
+ 	return delta;
+ }
+ #endif
+ 
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  /**
 - * tk_setup_internals - Set up internals to use clocksource clock.
 + * timekeeper_setup_internals - Set up internals to use clocksource clock.
   *
 - * @tk:		The target timekeeper to setup.
   * @clock:		Pointer to clocksource.
   *
   * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment
@@@ -187,25 -298,9 +290,29 @@@ u32 (*arch_gettimeoffset)(void) = defau
  static inline u32 arch_gettimeoffset(void) { return 0; }
  #endif
  
++<<<<<<< HEAD
 +static inline cycle_t timekeeping_get_delta(struct tk_read_base *tkr)
 +{
 +	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
 +
 +	/* read clocksource: */
 +	clock = tkr->clock;
 +	cycle_now = tkr->clock->read(clock);
 +
 +	/* calculate the delta since the last update_wall_time */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
 +
 +	return delta;
 +}
 +
 +static inline s64 timekeeping_delta_to_ns(struct tk_read_base *tkr,
 +					  cycle_t delta)
++=======
+ static inline u64 timekeeping_delta_to_ns(struct tk_read_base *tkr, u64 delta)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
 -	u64 nsec;
 +	s64 nsec;
  
  	nsec = delta * tkr->mult + tkr->xtime_nsec;
  	nsec >>= tkr->shift;
@@@ -214,14 -309,23 +321,26 @@@
  	return nsec + arch_gettimeoffset();
  }
  
 -static inline u64 timekeeping_get_ns(struct tk_read_base *tkr)
 +static inline s64 timekeeping_get_ns(struct tk_read_base *tkr)
  {
- 	cycle_t delta;
+ 	u64 delta;
  
  	delta = timekeeping_get_delta(tkr);
  	return timekeeping_delta_to_ns(tkr, delta);
  }
  
++<<<<<<< HEAD
++=======
+ static inline u64 timekeeping_cycles_to_ns(struct tk_read_base *tkr, u64 cycles)
+ {
+ 	u64 delta;
+ 
+ 	/* calculate the delta since the last update_wall_time */
+ 	delta = clocksource_delta(cycles, tkr->cycle_last, tkr->mask);
+ 	return timekeeping_delta_to_ns(tkr, delta);
+ }
+ 
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  /**
   * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.
   * @tkr: Timekeeping readout base from which we take the update
@@@ -337,16 -422,106 +456,92 @@@ u64 ktime_get_raw_fast_ns(void
  }
  EXPORT_SYMBOL_GPL(ktime_get_raw_fast_ns);
  
 -/**
 - * ktime_get_boot_fast_ns - NMI safe and fast access to boot clock.
 - *
 - * To keep it NMI safe since we're accessing from tracing, we're not using a
 - * separate timekeeper with updates to monotonic clock and boot offset
 - * protected with seqlocks. This has the following minor side effects:
 - *
 - * (1) Its possible that a timestamp be taken after the boot offset is updated
 - * but before the timekeeper is updated. If this happens, the new boot offset
 - * is added to the old timekeeping making the clock appear to update slightly
 - * earlier:
 - *    CPU 0                                        CPU 1
 - *    timekeeping_inject_sleeptime64()
 - *    __timekeeping_inject_sleeptime(tk, delta);
 - *                                                 timestamp();
 - *    timekeeping_update(tk, TK_CLEAR_NTP...);
 - *
 - * (2) On 32-bit systems, the 64-bit boot offset (tk->offs_boot) may be
 - * partially updated.  Since the tk->offs_boot update is a rare event, this
 - * should be a rare occurrence which postprocessing should be able to handle.
 - */
 -u64 notrace ktime_get_boot_fast_ns(void)
 +static inline s64 timekeeping_cycles_to_ns(struct tk_read_base *tkr,
 +					    cycle_t cycles)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	cycle_t delta;
  
 -	return (ktime_get_mono_fast_ns() + ktime_to_ns(tk->offs_boot));
 +	/* calculate the delta since the last update_wall_time */
 +	delta = clocksource_delta(cycles, tkr->clock->cycle_last,
 +				  tkr->clock->mask);
 +	return timekeeping_delta_to_ns(tkr, delta);
  }
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL_GPL(ktime_get_boot_fast_ns);
+ 
+ /* Suspend-time cycles value for halted fast timekeeper. */
+ static u64 cycles_at_suspend;
+ 
+ static u64 dummy_clock_read(struct clocksource *cs)
+ {
+ 	return cycles_at_suspend;
+ }
+ 
+ /**
+  * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.
+  * @tk: Timekeeper to snapshot.
+  *
+  * It generally is unsafe to access the clocksource after timekeeping has been
+  * suspended, so take a snapshot of the readout base of @tk and use it as the
+  * fast timekeeper's readout base while suspended.  It will return the same
+  * number of cycles every time until timekeeping is resumed at which time the
+  * proper readout base for the fast timekeeper will be restored automatically.
+  */
+ static void halt_fast_timekeeper(struct timekeeper *tk)
+ {
+ 	static struct tk_read_base tkr_dummy;
+ 	struct tk_read_base *tkr = &tk->tkr_mono;
+ 
+ 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
+ 	cycles_at_suspend = tkr->read(tkr->clock);
+ 	tkr_dummy.read = dummy_clock_read;
+ 	update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);
+ 
+ 	tkr = &tk->tkr_raw;
+ 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
+ 	tkr_dummy.read = dummy_clock_read;
+ 	update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
+ }
+ 
+ #ifdef CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 
+ static inline void update_vsyscall(struct timekeeper *tk)
+ {
+ 	struct timespec xt, wm;
+ 
+ 	xt = timespec64_to_timespec(tk_xtime(tk));
+ 	wm = timespec64_to_timespec(tk->wall_to_monotonic);
+ 	update_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,
+ 			    tk->tkr_mono.cycle_last);
+ }
+ 
+ static inline void old_vsyscall_fixup(struct timekeeper *tk)
+ {
+ 	s64 remainder;
+ 
+ 	/*
+ 	* Store only full nanoseconds into xtime_nsec after rounding
+ 	* it up and add the remainder to the error difference.
+ 	* XXX - This is necessary to avoid small 1ns inconsistnecies caused
+ 	* by truncating the remainder in vsyscalls. However, it causes
+ 	* additional work to be done in timekeeping_adjust(). Once
+ 	* the vsyscall implementations are converted to use xtime_nsec
+ 	* (shifted nanoseconds), and CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 	* users are removed, this can be killed.
+ 	*/
+ 	remainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);
+ 	if (remainder != 0) {
+ 		tk->tkr_mono.xtime_nsec -= remainder;
+ 		tk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;
+ 		tk->ntp_error += remainder << tk->ntp_error_shift;
+ 		tk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;
+ 	}
+ }
+ #else
+ #define old_vsyscall_fixup(tk)
+ #endif
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  
  static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
  
@@@ -457,14 -647,13 +652,20 @@@ static void timekeeping_update(struct t
   */
  static void timekeeping_forward_now(struct timekeeper *tk)
  {
++<<<<<<< HEAD
 +	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
 +	s64 nsec;
++=======
+ 	struct clocksource *clock = tk->tkr_mono.clock;
+ 	u64 cycle_now, delta;
+ 	u64 nsec;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  
 -	cycle_now = tk->tkr_mono.read(clock);
 -	delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
 -	tk->tkr_mono.cycle_last = cycle_now;
 +	clock = tk->tkr_mono.clock;
 +	cycle_now = clock->read(clock);
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
 +	tk->tkr_mono.cycle_last = clock->cycle_last = cycle_now;
  	tk->tkr_raw.cycle_last  = cycle_now;
  
  	tk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;
@@@ -599,51 -849,100 +800,89 @@@ void ktime_get_ts64(struct timespec64 *
  }
  EXPORT_SYMBOL_GPL(ktime_get_ts64);
  
 +
  /**
 - * ktime_get_seconds - Get the seconds portion of CLOCK_MONOTONIC
 + * timekeeping_clocktai - Returns the TAI time of day in a timespec
 + * @ts:		pointer to the timespec to be set
   *
 - * Returns the seconds portion of CLOCK_MONOTONIC with a single non
 - * serialized read. tk->ktime_sec is of type 'unsigned long' so this
 - * works on both 32 and 64 bit systems. On 32 bit systems the readout
 - * covers ~136 years of uptime which should be enough to prevent
 - * premature wrap arounds.
 + * Returns the time of day in a timespec.
   */
 -time64_t ktime_get_seconds(void)
 +void timekeeping_clocktai(struct timespec *ts)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	struct timekeeper *tk = &timekeeper;
 +	struct timespec64 ts64;
 +	unsigned long seq;
 +	u64 nsecs;
  
  	WARN_ON(timekeeping_suspended);
 -	return tk->ktime_sec;
 -}
 -EXPORT_SYMBOL_GPL(ktime_get_seconds);
  
 -/**
 - * ktime_get_real_seconds - Get the seconds portion of CLOCK_REALTIME
 - *
 - * Returns the wall clock seconds since 1970. This replaces the
 - * get_seconds() interface which is not y2038 safe on 32bit systems.
 - *
 - * For 64bit systems the fast access to tk->xtime_sec is preserved. On
 - * 32bit systems the access must be protected with the sequence
 - * counter to provide "atomic" access to the 64bit tk->xtime_sec
 - * value.
 - */
 -time64_t ktime_get_real_seconds(void)
 -{
 -	struct timekeeper *tk = &tk_core.timekeeper;
 -	time64_t seconds;
 -	unsigned int seq;
 +	do {
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
 -	if (IS_ENABLED(CONFIG_64BIT))
 -		return tk->xtime_sec;
 +		ts64.tv_sec = tk->xtime_sec + tk->tai_offset;
 +		nsecs = timekeeping_get_ns(&tk->tkr_mono);
  
 -	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 -		seconds = tk->xtime_sec;
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	ts64.tv_nsec = 0;
 +	timespec64_add_ns(&ts64, nsecs);
 +	*ts = timespec64_to_timespec(ts64);
  
 -	return seconds;
  }
 -EXPORT_SYMBOL_GPL(ktime_get_real_seconds);
 +EXPORT_SYMBOL(timekeeping_clocktai);
 +
  
  /**
 - * __ktime_get_real_seconds - The same as ktime_get_real_seconds
 - * but without the sequence counter protect. This internal function
 - * is called just when timekeeping lock is already held.
 + * ktime_get_clocktai - Returns the TAI time of day in a ktime
 + *
 + * Returns the time of day in a ktime.
   */
 -time64_t __ktime_get_real_seconds(void)
 +ktime_t ktime_get_clocktai(void)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	struct timespec ts;
  
 -	return tk->xtime_sec;
 +	timekeeping_clocktai(&ts);
 +	return timespec_to_ktime(ts);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(ktime_get_clocktai);
++=======
+ 
+ /**
+  * ktime_get_snapshot - snapshots the realtime/monotonic raw clocks with counter
+  * @systime_snapshot:	pointer to struct receiving the system time snapshot
+  */
+ void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned long seq;
+ 	ktime_t base_raw;
+ 	ktime_t base_real;
+ 	u64 nsec_raw;
+ 	u64 nsec_real;
+ 	u64 now;
+ 
+ 	WARN_ON_ONCE(timekeeping_suspended);
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 
+ 		now = tk->tkr_mono.read(tk->tkr_mono.clock);
+ 		systime_snapshot->cs_was_changed_seq = tk->cs_was_changed_seq;
+ 		systime_snapshot->clock_was_set_seq = tk->clock_was_set_seq;
+ 		base_real = ktime_add(tk->tkr_mono.base,
+ 				      tk_core.timekeeper.offs_real);
+ 		base_raw = tk->tkr_raw.base;
+ 		nsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono, now);
+ 		nsec_raw  = timekeeping_cycles_to_ns(&tk->tkr_raw, now);
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	systime_snapshot->cycles = now;
+ 	systime_snapshot->real = ktime_add_ns(base_real, nsec_real);
+ 	systime_snapshot->raw = ktime_add_ns(base_raw, nsec_raw);
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_snapshot);
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  
  /* Scale base by mult/div checking for overflow */
  static int scale64_check_overflow(u64 mult, u64 div, u64 *base)
@@@ -775,11 -1074,11 +1014,17 @@@ int get_device_system_crosststamp(int (
  				  struct system_device_crosststamp *xtstamp)
  {
  	struct system_counterval_t system_counterval;
++<<<<<<< HEAD
 +	struct timekeeper *tk = &timekeeper;
 +	cycle_t cycles, now, interval_start;
 +	unsigned int clock_was_set_seq;
++=======
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	u64 cycles, now, interval_start;
+ 	unsigned int clock_was_set_seq = 0;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  	ktime_t base_real, base_raw;
 -	u64 nsec_real, nsec_raw;
 +	s64 nsec_real, nsec_raw;
  	u8 cs_was_changed_seq;
  	unsigned long seq;
  	bool do_interp;
@@@ -1352,12 -1642,10 +1597,16 @@@ static void timekeeping_resume(void
  	struct clocksource *clock = tk->tkr_mono.clock;
  	unsigned long flags;
  	struct timespec64 ts_new, ts_delta;
++<<<<<<< HEAD
 +	struct timespec tmp;
 +	cycle_t cycle_now, cycle_delta;
 +	bool suspendtime_found = false;
++=======
+ 	u64 cycle_now;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  
 -	sleeptime_injected = false;
 -	read_persistent_clock64(&ts_new);
 +	read_persistent_clock(&tmp);
 +	ts_new = timespec_to_timespec64(tmp);
  
  	clockevents_resume();
  	clocksource_resume();
@@@ -1730,14 -2008,13 +1979,13 @@@ static inline unsigned int accumulate_n
   *
   * Returns the unconsumed cycles.
   */
- static cycle_t logarithmic_accumulation(struct timekeeper *tk, cycle_t offset,
- 						u32 shift,
- 						unsigned int *clock_set)
+ static u64 logarithmic_accumulation(struct timekeeper *tk, u64 offset,
+ 				    u32 shift, unsigned int *clock_set)
  {
- 	cycle_t interval = tk->cycle_interval << shift;
+ 	u64 interval = tk->cycle_interval << shift;
  	u64 raw_nsecs;
  
 -	/* If the offset is smaller than a shifted interval, do nothing */
 +	/* If the offset is smaller then a shifted interval, do nothing */
  	if (offset < interval)
  		return offset;
  
@@@ -1800,10 -2050,9 +2048,10 @@@ static inline void old_vsyscall_fixup(s
   */
  void update_wall_time(void)
  {
 -	struct timekeeper *real_tk = &tk_core.timekeeper;
 +	struct clocksource *clock;
 +	struct timekeeper *real_tk = &timekeeper;
  	struct timekeeper *tk = &shadow_timekeeper;
- 	cycle_t offset;
+ 	u64 offset;
  	int shift = 0, maxshift;
  	unsigned int clock_set = 0;
  	unsigned long flags;
diff --cc kernel/trace/ftrace.c
index 3800d970e761,eb230f06ba41..000000000000
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@@ -2353,12 -2839,15 +2353,16 @@@ static void ftrace_shutdown_sysctl(void
  		return;
  
  	/* ftrace_start_up is true if ftrace is running */
 -	if (ftrace_start_up) {
 -		command = FTRACE_DISABLE_CALLS;
 -		if (ftrace_graph_active)
 -			command |= FTRACE_STOP_FUNC_RET;
 -		ftrace_run_update_code(command);
 -	}
 +	if (ftrace_start_up)
 +		ftrace_run_update_code(FTRACE_DISABLE_CALLS);
  }
  
++<<<<<<< HEAD
 +static cycle_t		ftrace_update_time;
 +static unsigned long	ftrace_update_cnt;
++=======
+ static u64		ftrace_update_time;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  unsigned long		ftrace_update_tot_cnt;
  
  static inline int ops_traces_mod(struct ftrace_ops *ops)
@@@ -2418,38 -2894,30 +2422,44 @@@ static int ftrace_update_code(struct mo
  {
  	struct ftrace_page *pg;
  	struct dyn_ftrace *p;
++<<<<<<< HEAD
 +	cycle_t start, stop;
 +	unsigned long ref = 0;
 +	bool test = false;
++=======
+ 	u64 start, stop;
+ 	unsigned long update_cnt = 0;
+ 	unsigned long rec_flags = 0;
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  	int i;
  
 -	start = ftrace_now(raw_smp_processor_id());
 -
  	/*
 -	 * When a module is loaded, this function is called to convert
 -	 * the calls to mcount in its text to nops, and also to create
 -	 * an entry in the ftrace data. Now, if ftrace is activated
 -	 * after this call, but before the module sets its text to
 -	 * read-only, the modification of enabling ftrace can fail if
 -	 * the read-only is done while ftrace is converting the calls.
 -	 * To prevent this, the module's records are set as disabled
 -	 * and will be enabled after the call to set the module's text
 -	 * to read-only.
 +	 * When adding a module, we need to check if tracers are
 +	 * currently enabled and if they are set to trace all functions.
 +	 * If they are, we need to enable the module functions as well
 +	 * as update the reference counts for those function records.
  	 */
 -	if (mod)
 -		rec_flags |= FTRACE_FL_DISABLED;
 +	if (mod) {
 +		struct ftrace_ops *ops;
 +
 +		for (ops = ftrace_ops_list;
 +		     ops != &ftrace_list_end; ops = ops->next) {
 +			if (ops->flags & FTRACE_OPS_FL_ENABLED) {
 +				if (ops_traces_mod(ops))
 +					ref++;
 +				else
 +					test = true;
 +			}
 +		}
 +	}
  
 -	for (pg = new_pgs; pg; pg = pg->next) {
 +	start = ftrace_now(raw_smp_processor_id());
 +	ftrace_update_cnt = 0;
 +
 +	for (pg = ftrace_new_pgs; pg; pg = pg->next) {
  
  		for (i = 0; i < pg->index; i++) {
 +			int cnt = ref;
  
  			/* If something went wrong, bail without enabling anything */
  			if (unlikely(ftrace_disabled))
diff --cc kernel/trace/trace.c
index a14bc0a4b959,d7449783987a..000000000000
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@@ -182,8 -217,26 +182,8 @@@ static int __init set_trace_boot_option
  }
  __setup("trace_options=", set_trace_boot_options);
  
 -static char trace_boot_clock_buf[MAX_TRACER_SIZE] __initdata;
 -static char *trace_boot_clock __initdata;
 -
 -static int __init set_trace_boot_clock(char *str)
 -{
 -	strlcpy(trace_boot_clock_buf, str, MAX_TRACER_SIZE);
 -	trace_boot_clock = trace_boot_clock_buf;
 -	return 0;
 -}
 -__setup("trace_clock=", set_trace_boot_clock);
 -
 -static int __init set_tracepoint_printk(char *str)
 -{
 -	if ((strcmp(str, "=0") != 0 && strcmp(str, "=off") != 0))
 -		tracepoint_printk = 1;
 -	return 1;
 -}
 -__setup("tp_printk", set_tracepoint_printk);
  
- unsigned long long ns2usecs(cycle_t nsec)
+ unsigned long long ns2usecs(u64 nsec)
  {
  	nsec += 500;
  	do_div(nsec, 1000);
@@@ -237,15 -308,272 +237,271 @@@ void trace_array_put(struct trace_arra
  	mutex_unlock(&trace_types_lock);
  }
  
 -int call_filter_check_discard(struct trace_event_call *call, void *rec,
 -			      struct ring_buffer *buffer,
 -			      struct ring_buffer_event *event)
 +int filter_current_check_discard(struct ring_buffer *buffer,
 +				 struct ftrace_event_call *call, void *rec,
 +				 struct ring_buffer_event *event)
  {
 -	if (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&
 -	    !filter_match_preds(call->filter, rec)) {
 -		__trace_event_discard_commit(buffer, event);
 -		return 1;
 -	}
 -
 -	return 0;
 +	return filter_check_discard(call, rec, buffer, event);
  }
 +EXPORT_SYMBOL_GPL(filter_current_check_discard);
  
++<<<<<<< HEAD
 +cycle_t buffer_ftrace_now(struct trace_buffer *buf, int cpu)
++=======
+ void trace_free_pid_list(struct trace_pid_list *pid_list)
+ {
+ 	vfree(pid_list->pids);
+ 	kfree(pid_list);
+ }
+ 
+ /**
+  * trace_find_filtered_pid - check if a pid exists in a filtered_pid list
+  * @filtered_pids: The list of pids to check
+  * @search_pid: The PID to find in @filtered_pids
+  *
+  * Returns true if @search_pid is fonud in @filtered_pids, and false otherwis.
+  */
+ bool
+ trace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)
+ {
+ 	/*
+ 	 * If pid_max changed after filtered_pids was created, we
+ 	 * by default ignore all pids greater than the previous pid_max.
+ 	 */
+ 	if (search_pid >= filtered_pids->pid_max)
+ 		return false;
+ 
+ 	return test_bit(search_pid, filtered_pids->pids);
+ }
+ 
+ /**
+  * trace_ignore_this_task - should a task be ignored for tracing
+  * @filtered_pids: The list of pids to check
+  * @task: The task that should be ignored if not filtered
+  *
+  * Checks if @task should be traced or not from @filtered_pids.
+  * Returns true if @task should *NOT* be traced.
+  * Returns false if @task should be traced.
+  */
+ bool
+ trace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
+ {
+ 	/*
+ 	 * Return false, because if filtered_pids does not exist,
+ 	 * all pids are good to trace.
+ 	 */
+ 	if (!filtered_pids)
+ 		return false;
+ 
+ 	return !trace_find_filtered_pid(filtered_pids, task->pid);
+ }
+ 
+ /**
+  * trace_pid_filter_add_remove - Add or remove a task from a pid_list
+  * @pid_list: The list to modify
+  * @self: The current task for fork or NULL for exit
+  * @task: The task to add or remove
+  *
+  * If adding a task, if @self is defined, the task is only added if @self
+  * is also included in @pid_list. This happens on fork and tasks should
+  * only be added when the parent is listed. If @self is NULL, then the
+  * @task pid will be removed from the list, which would happen on exit
+  * of a task.
+  */
+ void trace_filter_add_remove_task(struct trace_pid_list *pid_list,
+ 				  struct task_struct *self,
+ 				  struct task_struct *task)
+ {
+ 	if (!pid_list)
+ 		return;
+ 
+ 	/* For forks, we only add if the forking task is listed */
+ 	if (self) {
+ 		if (!trace_find_filtered_pid(pid_list, self->pid))
+ 			return;
+ 	}
+ 
+ 	/* Sorry, but we don't support pid_max changing after setting */
+ 	if (task->pid >= pid_list->pid_max)
+ 		return;
+ 
+ 	/* "self" is set for forks, and NULL for exits */
+ 	if (self)
+ 		set_bit(task->pid, pid_list->pids);
+ 	else
+ 		clear_bit(task->pid, pid_list->pids);
+ }
+ 
+ /**
+  * trace_pid_next - Used for seq_file to get to the next pid of a pid_list
+  * @pid_list: The pid list to show
+  * @v: The last pid that was shown (+1 the actual pid to let zero be displayed)
+  * @pos: The position of the file
+  *
+  * This is used by the seq_file "next" operation to iterate the pids
+  * listed in a trace_pid_list structure.
+  *
+  * Returns the pid+1 as we want to display pid of zero, but NULL would
+  * stop the iteration.
+  */
+ void *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos)
+ {
+ 	unsigned long pid = (unsigned long)v;
+ 
+ 	(*pos)++;
+ 
+ 	/* pid already is +1 of the actual prevous bit */
+ 	pid = find_next_bit(pid_list->pids, pid_list->pid_max, pid);
+ 
+ 	/* Return pid + 1 to allow zero to be represented */
+ 	if (pid < pid_list->pid_max)
+ 		return (void *)(pid + 1);
+ 
+ 	return NULL;
+ }
+ 
+ /**
+  * trace_pid_start - Used for seq_file to start reading pid lists
+  * @pid_list: The pid list to show
+  * @pos: The position of the file
+  *
+  * This is used by seq_file "start" operation to start the iteration
+  * of listing pids.
+  *
+  * Returns the pid+1 as we want to display pid of zero, but NULL would
+  * stop the iteration.
+  */
+ void *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos)
+ {
+ 	unsigned long pid;
+ 	loff_t l = 0;
+ 
+ 	pid = find_first_bit(pid_list->pids, pid_list->pid_max);
+ 	if (pid >= pid_list->pid_max)
+ 		return NULL;
+ 
+ 	/* Return pid + 1 so that zero can be the exit value */
+ 	for (pid++; pid && l < *pos;
+ 	     pid = (unsigned long)trace_pid_next(pid_list, (void *)pid, &l))
+ 		;
+ 	return (void *)pid;
+ }
+ 
+ /**
+  * trace_pid_show - show the current pid in seq_file processing
+  * @m: The seq_file structure to write into
+  * @v: A void pointer of the pid (+1) value to display
+  *
+  * Can be directly used by seq_file operations to display the current
+  * pid value.
+  */
+ int trace_pid_show(struct seq_file *m, void *v)
+ {
+ 	unsigned long pid = (unsigned long)v - 1;
+ 
+ 	seq_printf(m, "%lu\n", pid);
+ 	return 0;
+ }
+ 
+ /* 128 should be much more than enough */
+ #define PID_BUF_SIZE		127
+ 
+ int trace_pid_write(struct trace_pid_list *filtered_pids,
+ 		    struct trace_pid_list **new_pid_list,
+ 		    const char __user *ubuf, size_t cnt)
+ {
+ 	struct trace_pid_list *pid_list;
+ 	struct trace_parser parser;
+ 	unsigned long val;
+ 	int nr_pids = 0;
+ 	ssize_t read = 0;
+ 	ssize_t ret = 0;
+ 	loff_t pos;
+ 	pid_t pid;
+ 
+ 	if (trace_parser_get_init(&parser, PID_BUF_SIZE + 1))
+ 		return -ENOMEM;
+ 
+ 	/*
+ 	 * Always recreate a new array. The write is an all or nothing
+ 	 * operation. Always create a new array when adding new pids by
+ 	 * the user. If the operation fails, then the current list is
+ 	 * not modified.
+ 	 */
+ 	pid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);
+ 	if (!pid_list)
+ 		return -ENOMEM;
+ 
+ 	pid_list->pid_max = READ_ONCE(pid_max);
+ 
+ 	/* Only truncating will shrink pid_max */
+ 	if (filtered_pids && filtered_pids->pid_max > pid_list->pid_max)
+ 		pid_list->pid_max = filtered_pids->pid_max;
+ 
+ 	pid_list->pids = vzalloc((pid_list->pid_max + 7) >> 3);
+ 	if (!pid_list->pids) {
+ 		kfree(pid_list);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	if (filtered_pids) {
+ 		/* copy the current bits to the new max */
+ 		for_each_set_bit(pid, filtered_pids->pids,
+ 				 filtered_pids->pid_max) {
+ 			set_bit(pid, pid_list->pids);
+ 			nr_pids++;
+ 		}
+ 	}
+ 
+ 	while (cnt > 0) {
+ 
+ 		pos = 0;
+ 
+ 		ret = trace_get_user(&parser, ubuf, cnt, &pos);
+ 		if (ret < 0 || !trace_parser_loaded(&parser))
+ 			break;
+ 
+ 		read += ret;
+ 		ubuf += ret;
+ 		cnt -= ret;
+ 
+ 		parser.buffer[parser.idx] = 0;
+ 
+ 		ret = -EINVAL;
+ 		if (kstrtoul(parser.buffer, 0, &val))
+ 			break;
+ 		if (val >= pid_list->pid_max)
+ 			break;
+ 
+ 		pid = (pid_t)val;
+ 
+ 		set_bit(pid, pid_list->pids);
+ 		nr_pids++;
+ 
+ 		trace_parser_clear(&parser);
+ 		ret = 0;
+ 	}
+ 	trace_parser_put(&parser);
+ 
+ 	if (ret < 0) {
+ 		trace_free_pid_list(pid_list);
+ 		return ret;
+ 	}
+ 
+ 	if (!nr_pids) {
+ 		/* Cleared the list of pids */
+ 		trace_free_pid_list(pid_list);
+ 		read = ret;
+ 		pid_list = NULL;
+ 	}
+ 
+ 	*new_pid_list = pid_list;
+ 
+ 	return read;
+ }
+ 
+ static u64 buffer_ftrace_now(struct trace_buffer *buf, int cpu)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	u64 ts;
  
diff --cc kernel/trace/trace_irqsoff.c
index c042017b2251,7758bc0617cb..000000000000
--- a/kernel/trace/trace_irqsoff.c
+++ b/kernel/trace/trace_irqsoff.c
@@@ -303,7 -298,7 +303,11 @@@ static void irqsoff_print_header(struc
  /*
   * Should this new latency be reported/recorded?
   */
++<<<<<<< HEAD
 +static int report_latency(struct trace_array *tr, cycle_t delta)
++=======
+ static bool report_latency(struct trace_array *tr, u64 delta)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	if (tracing_thresh) {
  		if (delta < tracing_thresh)
diff --cc kernel/trace/trace_sched_wakeup.c
index 42c29e436ee5,ddec53b67646..000000000000
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@@ -350,7 -358,7 +350,11 @@@ static void wakeup_print_header(struct 
  /*
   * Should this new latency be reported/recorded?
   */
++<<<<<<< HEAD
 +static int report_latency(struct trace_array *tr, cycle_t delta)
++=======
+ static bool report_latency(struct trace_array *tr, u64 delta)
++>>>>>>> a5a1d1c2914b (clocksource: Use a plain u64 instead of cycle_t)
  {
  	if (tracing_thresh) {
  		if (delta < tracing_thresh)
* Unmerged path arch/arm/mach-ep93xx/timer-ep93xx.c
* Unmerged path arch/mips/loongson32/common/time.c
* Unmerged path arch/mips/loongson64/loongson-3/hpet.c
* Unmerged path arch/nios2/kernel/time.c
* Unmerged path arch/x86/entry/vdso/vclock_gettime.c
* Unmerged path drivers/clocksource/arc_timer.c
* Unmerged path drivers/clocksource/arm_global_timer.c
* Unmerged path drivers/clocksource/h8300_timer16.c
* Unmerged path drivers/clocksource/h8300_tpu.c
* Unmerged path drivers/clocksource/jcore-pit.c
* Unmerged path drivers/clocksource/mips-gic-timer.c
* Unmerged path drivers/clocksource/qcom-timer.c
* Unmerged path drivers/clocksource/time-pistachio.c
* Unmerged path drivers/clocksource/timer-atmel-pit.c
* Unmerged path drivers/clocksource/timer-nps.c
* Unmerged path drivers/clocksource/timer-sun5i.c
* Unmerged path drivers/clocksource/timer-ti-32k.c
* Unmerged path drivers/irqchip/irq-mips-gic.c
* Unmerged path include/linux/irqchip/mips-gic.h
* Unmerged path virt/kvm/arm/arch_timer.c
* Unmerged path arch/alpha/kernel/time.c
* Unmerged path arch/arm/include/asm/kvm_arch_timer.h
* Unmerged path arch/arm/mach-at91/at91rm9200_time.c
diff --git a/arch/arm/mach-davinci/time.c b/arch/arm/mach-davinci/time.c
index bad361ec1666..a84e0d346aae 100644
--- a/arch/arm/mach-davinci/time.c
+++ b/arch/arm/mach-davinci/time.c
@@ -268,7 +268,7 @@ static void __init timer_init(void)
 /*
  * clocksource
  */
-static cycle_t read_cycles(struct clocksource *cs)
+static u64 read_cycles(struct clocksource *cs)
 {
 	struct timer_s *t = &timers[TID_CLOCKSOURCE];
 
* Unmerged path arch/arm/mach-ep93xx/timer-ep93xx.c
diff --git a/arch/arm/mach-footbridge/dc21285-timer.c b/arch/arm/mach-footbridge/dc21285-timer.c
index 9ee78f7b4990..1cd4510d40cb 100644
--- a/arch/arm/mach-footbridge/dc21285-timer.c
+++ b/arch/arm/mach-footbridge/dc21285-timer.c
@@ -18,7 +18,7 @@
 
 #include "common.h"
 
-static cycle_t cksrc_dc21285_read(struct clocksource *cs)
+static u64 cksrc_dc21285_read(struct clocksource *cs)
 {
 	return cs->mask - *CSR_TIMER2_VALUE;
 }
diff --git a/arch/arm/mach-ixp4xx/common.c b/arch/arm/mach-ixp4xx/common.c
index 6600cff6bd92..7656a9c385cb 100644
--- a/arch/arm/mach-ixp4xx/common.c
+++ b/arch/arm/mach-ixp4xx/common.c
@@ -457,7 +457,7 @@ static u32 notrace ixp4xx_read_sched_clock(void)
  * clocksource
  */
 
-static cycle_t ixp4xx_clocksource_read(struct clocksource *c)
+static u64 ixp4xx_clocksource_read(struct clocksource *c)
 {
 	return *IXP4XX_OSTS;
 }
diff --git a/arch/arm/mach-mmp/time.c b/arch/arm/mach-mmp/time.c
index 86a18b3d252e..ad356e9fe5f5 100644
--- a/arch/arm/mach-mmp/time.c
+++ b/arch/arm/mach-mmp/time.c
@@ -146,7 +146,7 @@ static struct clock_event_device ckevt = {
 	.set_mode	= timer_set_mode,
 };
 
-static cycle_t clksrc_read(struct clocksource *cs)
+static u64 clksrc_read(struct clocksource *cs)
 {
 	return timer_read();
 }
diff --git a/arch/arm/mach-omap2/timer.c b/arch/arm/mach-omap2/timer.c
index f8b23b8040d9..a0baff4f1aaf 100644
--- a/arch/arm/mach-omap2/timer.c
+++ b/arch/arm/mach-omap2/timer.c
@@ -356,9 +356,9 @@ static bool use_gptimer_clksrc;
 /*
  * clocksource
  */
-static cycle_t clocksource_read_cycles(struct clocksource *cs)
+static u64 clocksource_read_cycles(struct clocksource *cs)
 {
-	return (cycle_t)__omap_dm_timer_read_counter(&clksrc,
+	return (u64)__omap_dm_timer_read_counter(&clksrc,
 						     OMAP_TIMER_NONPOSTED);
 }
 
diff --git a/arch/arm/plat-iop/time.c b/arch/arm/plat-iop/time.c
index 837a2d52e9db..f73b7235e0b2 100644
--- a/arch/arm/plat-iop/time.c
+++ b/arch/arm/plat-iop/time.c
@@ -38,7 +38,7 @@
 /*
  * IOP clocksource (free-running timer 1).
  */
-static cycle_t notrace iop_clocksource_read(struct clocksource *unused)
+static u64 notrace iop_clocksource_read(struct clocksource *unused)
 {
 	return 0xffffffffu - read_tcr1();
 }
diff --git a/arch/avr32/kernel/time.c b/arch/avr32/kernel/time.c
index 12f828ad5058..a67f3f8c97d0 100644
--- a/arch/avr32/kernel/time.c
+++ b/arch/avr32/kernel/time.c
@@ -19,9 +19,9 @@
 #include <mach/pm.h>
 
 
-static cycle_t read_cycle_count(struct clocksource *cs)
+static u64 read_cycle_count(struct clocksource *cs)
 {
-	return (cycle_t)sysreg_read(COUNT);
+	return (u64)sysreg_read(COUNT);
 }
 
 /*
diff --git a/arch/blackfin/kernel/time-ts.c b/arch/blackfin/kernel/time-ts.c
index cb0a4845339e..ba9097c86c5a 100644
--- a/arch/blackfin/kernel/time-ts.c
+++ b/arch/blackfin/kernel/time-ts.c
@@ -26,7 +26,7 @@
 
 #if defined(CONFIG_CYCLES_CLOCKSOURCE)
 
-static notrace cycle_t bfin_read_cycles(struct clocksource *cs)
+static notrace u64 bfin_read_cycles(struct clocksource *cs)
 {
 #ifdef CONFIG_CPU_FREQ
 	return __bfin_cycles_off + (get_cycles() << __bfin_cycles_mod);
@@ -80,7 +80,7 @@ void __init setup_gptimer0(void)
 	enable_gptimers(TIMER0bit);
 }
 
-static cycle_t bfin_read_gptimer0(struct clocksource *cs)
+static u64 bfin_read_gptimer0(struct clocksource *cs)
 {
 	return bfin_read_TIMER0_COUNTER();
 }
diff --git a/arch/c6x/kernel/time.c b/arch/c6x/kernel/time.c
index 356ee84cad95..4979a9a53741 100644
--- a/arch/c6x/kernel/time.c
+++ b/arch/c6x/kernel/time.c
@@ -26,7 +26,7 @@
 static u32 sched_clock_multiplier;
 #define SCHED_CLOCK_SHIFT 16
 
-static cycle_t tsc_read(struct clocksource *cs)
+static u64 tsc_read(struct clocksource *cs)
 {
 	return get_cycles();
 }
diff --git a/arch/hexagon/kernel/time.c b/arch/hexagon/kernel/time.c
index 9903fad997f3..d1cef03189e8 100644
--- a/arch/hexagon/kernel/time.c
+++ b/arch/hexagon/kernel/time.c
@@ -72,9 +72,9 @@ struct adsp_hw_timer_struct {
 /*  Look for "TCX0" for related constants.  */
 static __iomem struct adsp_hw_timer_struct *rtos_timer;
 
-static cycle_t timer_get_cycles(struct clocksource *cs)
+static u64 timer_get_cycles(struct clocksource *cs)
 {
-	return (cycle_t) __vmgettime();
+	return (u64) __vmgettime();
 }
 
 static struct clocksource hexagon_clocksource = {
diff --git a/arch/ia64/kernel/cyclone.c b/arch/ia64/kernel/cyclone.c
index 4826ff957a3d..3d2e8c6dd591 100644
--- a/arch/ia64/kernel/cyclone.c
+++ b/arch/ia64/kernel/cyclone.c
@@ -21,9 +21,9 @@ void __init cyclone_setup(void)
 
 static void __iomem *cyclone_mc;
 
-static cycle_t read_cyclone(struct clocksource *cs)
+static u64 read_cyclone(struct clocksource *cs)
 {
-	return (cycle_t)readq((void __iomem *)cyclone_mc);
+	return (u64)readq((void __iomem *)cyclone_mc);
 }
 
 static struct clocksource clocksource_cyclone = {
diff --git a/arch/ia64/kernel/fsyscall_gtod_data.h b/arch/ia64/kernel/fsyscall_gtod_data.h
index 146b15b5fec3..dcc514917731 100644
--- a/arch/ia64/kernel/fsyscall_gtod_data.h
+++ b/arch/ia64/kernel/fsyscall_gtod_data.h
@@ -9,15 +9,15 @@ struct fsyscall_gtod_data_t {
 	seqcount_t	seq;
 	struct timespec	wall_time;
 	struct timespec monotonic_time;
-	cycle_t		clk_mask;
+	u64		clk_mask;
 	u32		clk_mult;
 	u32		clk_shift;
 	void		*clk_fsys_mmio;
-	cycle_t		clk_cycle_last;
+	u64		clk_cycle_last;
 } ____cacheline_aligned;
 
 struct itc_jitter_data_t {
 	int		itc_jitter;
-	cycle_t		itc_lastcycle;
+	u64		itc_lastcycle;
 } ____cacheline_aligned;
 
* Unmerged path arch/ia64/kernel/time.c
diff --git a/arch/ia64/sn/kernel/sn2/timer.c b/arch/ia64/sn/kernel/sn2/timer.c
index abab8f99e913..66edc36426ed 100644
--- a/arch/ia64/sn/kernel/sn2/timer.c
+++ b/arch/ia64/sn/kernel/sn2/timer.c
@@ -22,9 +22,9 @@
 
 extern unsigned long sn_rtc_cycles_per_second;
 
-static cycle_t read_sn2(struct clocksource *cs)
+static u64 read_sn2(struct clocksource *cs)
 {
-	return (cycle_t)readq(RTC_COUNTER_ADDR);
+	return (u64)readq(RTC_COUNTER_ADDR);
 }
 
 static struct clocksource clocksource_sn2 = {
diff --git a/arch/m68k/platform/68000/timers.c b/arch/m68k/platform/68000/timers.c
index ec30acbfe6db..b01a8bca347e 100644
--- a/arch/m68k/platform/68000/timers.c
+++ b/arch/m68k/platform/68000/timers.c
@@ -76,7 +76,7 @@ static struct irqaction m68328_timer_irq = {
 
 /***************************************************************************/
 
-static cycle_t m68328_read_clk(struct clocksource *cs)
+static u64 m68328_read_clk(struct clocksource *cs)
 {
 	unsigned long flags;
 	u32 cycles;
diff --git a/arch/m68k/platform/coldfire/dma_timer.c b/arch/m68k/platform/coldfire/dma_timer.c
index 235ad57c4707..8273eea57874 100644
--- a/arch/m68k/platform/coldfire/dma_timer.c
+++ b/arch/m68k/platform/coldfire/dma_timer.c
@@ -34,7 +34,7 @@
 #define DMA_DTMR_CLK_DIV_16	(2 << 1)
 #define DMA_DTMR_ENABLE		(1 << 0)
 
-static cycle_t cf_dt_get_cycles(struct clocksource *cs)
+static u64 cf_dt_get_cycles(struct clocksource *cs)
 {
 	return __raw_readl(DTCN0);
 }
diff --git a/arch/m68k/platform/coldfire/pit.c b/arch/m68k/platform/coldfire/pit.c
index e8f3b97b0f77..b9a6074c6549 100644
--- a/arch/m68k/platform/coldfire/pit.c
+++ b/arch/m68k/platform/coldfire/pit.c
@@ -124,7 +124,7 @@ static struct irqaction pit_irq = {
 
 /***************************************************************************/
 
-static cycle_t pit_read_clk(struct clocksource *cs)
+static u64 pit_read_clk(struct clocksource *cs)
 {
 	unsigned long flags;
 	u32 cycles;
diff --git a/arch/m68k/platform/coldfire/sltimers.c b/arch/m68k/platform/coldfire/sltimers.c
index bb5a25ada848..ce006b349e32 100644
--- a/arch/m68k/platform/coldfire/sltimers.c
+++ b/arch/m68k/platform/coldfire/sltimers.c
@@ -97,7 +97,7 @@ static struct irqaction mcfslt_timer_irq = {
 	.handler = mcfslt_tick,
 };
 
-static cycle_t mcfslt_read_clk(struct clocksource *cs)
+static u64 mcfslt_read_clk(struct clocksource *cs)
 {
 	unsigned long flags;
 	u32 cycles, scnt;
diff --git a/arch/m68k/platform/coldfire/timers.c b/arch/m68k/platform/coldfire/timers.c
index d06068e45764..71c9c9e15049 100644
--- a/arch/m68k/platform/coldfire/timers.c
+++ b/arch/m68k/platform/coldfire/timers.c
@@ -89,7 +89,7 @@ static struct irqaction mcftmr_timer_irq = {
 
 /***************************************************************************/
 
-static cycle_t mcftmr_read_clk(struct clocksource *cs)
+static u64 mcftmr_read_clk(struct clocksource *cs)
 {
 	unsigned long flags;
 	u32 cycles;
* Unmerged path arch/microblaze/kernel/timer.c
diff --git a/arch/mips/alchemy/common/time.c b/arch/mips/alchemy/common/time.c
index 93fa586d52e2..8fd727bb51d7 100644
--- a/arch/mips/alchemy/common/time.c
+++ b/arch/mips/alchemy/common/time.c
@@ -44,7 +44,7 @@
 /* 32kHz clock enabled and detected */
 #define CNTR_OK (SYS_CNTRL_E0 | SYS_CNTRL_32S)
 
-static cycle_t au1x_counter1_read(struct clocksource *cs)
+static u64 au1x_counter1_read(struct clocksource *cs)
 {
 	return au_readl(SYS_RTCREAD);
 }
diff --git a/arch/mips/cavium-octeon/csrc-octeon.c b/arch/mips/cavium-octeon/csrc-octeon.c
index 02193953eb9e..62458c1772ad 100644
--- a/arch/mips/cavium-octeon/csrc-octeon.c
+++ b/arch/mips/cavium-octeon/csrc-octeon.c
@@ -83,7 +83,7 @@ void octeon_init_cvmcount(void)
 	local_irq_restore(flags);
 }
 
-static cycle_t octeon_cvmcount_read(struct clocksource *cs)
+static u64 octeon_cvmcount_read(struct clocksource *cs)
 {
 	return read_c0_cvmcount();
 }
diff --git a/arch/mips/jz4740/time.c b/arch/mips/jz4740/time.c
index 5e430ce9ac7e..b0039188098b 100644
--- a/arch/mips/jz4740/time.c
+++ b/arch/mips/jz4740/time.c
@@ -30,7 +30,7 @@
 
 static uint16_t jz4740_jiffies_per_tick;
 
-static cycle_t jz4740_clocksource_read(struct clocksource *cs)
+static u64 jz4740_clocksource_read(struct clocksource *cs)
 {
 	return jz4740_timer_get_count(TIMER_CLOCKSOURCE);
 }
diff --git a/arch/mips/kernel/cevt-txx9.c b/arch/mips/kernel/cevt-txx9.c
index 2ae08462e46e..2832a2b5d6df 100644
--- a/arch/mips/kernel/cevt-txx9.c
+++ b/arch/mips/kernel/cevt-txx9.c
@@ -26,7 +26,7 @@ struct txx9_clocksource {
 	struct txx9_tmr_reg __iomem *tmrptr;
 };
 
-static cycle_t txx9_cs_read(struct clocksource *cs)
+static u64 txx9_cs_read(struct clocksource *cs)
 {
 	struct txx9_clocksource *txx9_cs =
 		container_of(cs, struct txx9_clocksource, cs);
diff --git a/arch/mips/kernel/csrc-bcm1480.c b/arch/mips/kernel/csrc-bcm1480.c
index 468f3eba4132..94c39274a801 100644
--- a/arch/mips/kernel/csrc-bcm1480.c
+++ b/arch/mips/kernel/csrc-bcm1480.c
@@ -28,9 +28,9 @@
 
 #include <asm/sibyte/sb1250.h>
 
-static cycle_t bcm1480_hpt_read(struct clocksource *cs)
+static u64 bcm1480_hpt_read(struct clocksource *cs)
 {
-	return (cycle_t) __raw_readq(IOADDR(A_SCD_ZBBUS_CYCLE_COUNT));
+	return (u64) __raw_readq(IOADDR(A_SCD_ZBBUS_CYCLE_COUNT));
 }
 
 struct clocksource bcm1480_clocksource = {
diff --git a/arch/mips/kernel/csrc-ioasic.c b/arch/mips/kernel/csrc-ioasic.c
index 0654bff9b69c..eb24b12538d4 100644
--- a/arch/mips/kernel/csrc-ioasic.c
+++ b/arch/mips/kernel/csrc-ioasic.c
@@ -25,7 +25,7 @@
 #include <asm/dec/ioasic.h>
 #include <asm/dec/ioasic_addrs.h>
 
-static cycle_t dec_ioasic_hpt_read(struct clocksource *cs)
+static u64 dec_ioasic_hpt_read(struct clocksource *cs)
 {
 	return ioasic_read(IO_REG_FCTR);
 }
diff --git a/arch/mips/kernel/csrc-r4k.c b/arch/mips/kernel/csrc-r4k.c
index decd1fa38d55..c09f242ee130 100644
--- a/arch/mips/kernel/csrc-r4k.c
+++ b/arch/mips/kernel/csrc-r4k.c
@@ -10,7 +10,7 @@
 
 #include <asm/time.h>
 
-static cycle_t c0_hpt_read(struct clocksource *cs)
+static u64 c0_hpt_read(struct clocksource *cs)
 {
 	return read_c0_count();
 }
* Unmerged path arch/mips/kernel/csrc-sb1250.c
diff --git a/arch/mips/loongson/common/cs5536/cs5536_mfgpt.c b/arch/mips/loongson/common/cs5536/cs5536_mfgpt.c
index c639b9db0012..b4cc73d5198e 100644
--- a/arch/mips/loongson/common/cs5536/cs5536_mfgpt.c
+++ b/arch/mips/loongson/common/cs5536/cs5536_mfgpt.c
@@ -149,7 +149,7 @@ void __init setup_mfgpt0_timer(void)
  * to just read by itself. So use jiffies to emulate a free
  * running counter:
  */
-static cycle_t mfgpt_read(struct clocksource *cs)
+static u64 mfgpt_read(struct clocksource *cs)
 {
 	unsigned long flags;
 	int count;
@@ -193,7 +193,7 @@ static cycle_t mfgpt_read(struct clocksource *cs)
 
 	spin_unlock_irqrestore(&mfgpt_lock, flags);
 
-	return (cycle_t) (jifs * COMPARE) + count;
+	return (u64) (jifs * COMPARE) + count;
 }
 
 static struct clocksource clocksource_mfgpt = {
* Unmerged path arch/mips/loongson32/common/time.c
* Unmerged path arch/mips/loongson64/loongson-3/hpet.c
* Unmerged path arch/mips/mti-malta/malta-time.c
diff --git a/arch/mips/netlogic/common/time.c b/arch/mips/netlogic/common/time.c
index 5c56555380bb..d84a64f6b86b 100644
--- a/arch/mips/netlogic/common/time.c
+++ b/arch/mips/netlogic/common/time.c
@@ -59,14 +59,14 @@ unsigned int __cpuinit get_c0_compare_int(void)
 	return IRQ_TIMER;
 }
 
-static cycle_t nlm_get_pic_timer(struct clocksource *cs)
+static u64 nlm_get_pic_timer(struct clocksource *cs)
 {
 	uint64_t picbase = nlm_get_node(0)->picbase;
 
 	return ~nlm_pic_read_timer(picbase, PIC_CLOCK_TIMER);
 }
 
-static cycle_t nlm_get_pic_timer32(struct clocksource *cs)
+static u64 nlm_get_pic_timer32(struct clocksource *cs)
 {
 	uint64_t picbase = nlm_get_node(0)->picbase;
 
diff --git a/arch/mips/sgi-ip27/ip27-timer.c b/arch/mips/sgi-ip27/ip27-timer.c
index 2e21b761cb9c..c16d87496a20 100644
--- a/arch/mips/sgi-ip27/ip27-timer.c
+++ b/arch/mips/sgi-ip27/ip27-timer.c
@@ -146,7 +146,7 @@ static void __init hub_rt_clock_event_global_init(void)
 	setup_irq(irq, &hub_rt_irqaction);
 }
 
-static cycle_t hub_rt_read(struct clocksource *cs)
+static u64 hub_rt_read(struct clocksource *cs)
 {
 	return REMOTE_HUB_L(cputonasid(0), PI_RT_COUNT);
 }
diff --git a/arch/mn10300/kernel/csrc-mn10300.c b/arch/mn10300/kernel/csrc-mn10300.c
index 45644cf18c41..6b74df3661f2 100644
--- a/arch/mn10300/kernel/csrc-mn10300.c
+++ b/arch/mn10300/kernel/csrc-mn10300.c
@@ -13,7 +13,7 @@
 #include <asm/timex.h>
 #include "internal.h"
 
-static cycle_t mn10300_read(struct clocksource *cs)
+static u64 mn10300_read(struct clocksource *cs)
 {
 	return read_timestamp_counter();
 }
* Unmerged path arch/nios2/kernel/time.c
diff --git a/arch/openrisc/kernel/time.c b/arch/openrisc/kernel/time.c
index 7c52e9494a8d..f2324521f8de 100644
--- a/arch/openrisc/kernel/time.c
+++ b/arch/openrisc/kernel/time.c
@@ -141,9 +141,9 @@ static __init void openrisc_clockevent_init(void)
  * is 32 bits wide and runs at the CPU clock frequency.
  */
 
-static cycle_t openrisc_timer_read(struct clocksource *cs)
+static u64 openrisc_timer_read(struct clocksource *cs)
 {
-	return (cycle_t) mfspr(SPR_TTCR);
+	return (u64) mfspr(SPR_TTCR);
 }
 
 static struct clocksource openrisc_timer = {
* Unmerged path arch/parisc/kernel/time.c
* Unmerged path arch/powerpc/kernel/time.c
diff --git a/arch/s390/kernel/time.c b/arch/s390/kernel/time.c
index 232a1e9ed542..acae4315adb1 100644
--- a/arch/s390/kernel/time.c
+++ b/arch/s390/kernel/time.c
@@ -210,7 +210,7 @@ void read_boot_clock(struct timespec *ts)
 	tod_to_timeval(sched_clock_base_cc - TOD_UNIX_EPOCH, ts);
 }
 
-static cycle_t read_tod_clock(struct clocksource *cs)
+static u64 read_tod_clock(struct clocksource *cs)
 {
 	return get_tod_clock();
 }
diff --git a/arch/sparc/kernel/time_32.c b/arch/sparc/kernel/time_32.c
index c4c27b0f9063..cc5bd0cc1332 100644
--- a/arch/sparc/kernel/time_32.c
+++ b/arch/sparc/kernel/time_32.c
@@ -155,7 +155,7 @@ static unsigned int sbus_cycles_offset(void)
 	return offset;
 }
 
-static cycle_t timer_cs_read(struct clocksource *cs)
+static u64 timer_cs_read(struct clocksource *cs)
 {
 	unsigned int seq, offset;
 	u64 cycles;
diff --git a/arch/sparc/kernel/time_64.c b/arch/sparc/kernel/time_64.c
index c3d82b5f54ca..17c0a65e0b7c 100644
--- a/arch/sparc/kernel/time_64.c
+++ b/arch/sparc/kernel/time_64.c
@@ -801,7 +801,7 @@ void udelay(unsigned long usecs)
 }
 EXPORT_SYMBOL(udelay);
 
-static cycle_t clocksource_tick_read(struct clocksource *cs)
+static u64 clocksource_tick_read(struct clocksource *cs)
 {
 	return tick_ops->get_tick();
 }
* Unmerged path arch/um/kernel/time.c
diff --git a/arch/unicore32/kernel/time.c b/arch/unicore32/kernel/time.c
index d3824b2ff644..d673ba349d01 100644
--- a/arch/unicore32/kernel/time.c
+++ b/arch/unicore32/kernel/time.c
@@ -71,7 +71,7 @@ static struct clock_event_device ckevt_puv3_osmr0 = {
 	.set_mode	= puv3_osmr0_set_mode,
 };
 
-static cycle_t puv3_read_oscr(struct clocksource *cs)
+static u64 puv3_read_oscr(struct clocksource *cs)
 {
 	return readl(OST_OSCR);
 }
* Unmerged path arch/x86/entry/vdso/vclock_gettime.c
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 296c78b05cc2..fae23e88f069 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -690,7 +690,7 @@ struct kvm_arch {
 	spinlock_t pvclock_gtod_sync_lock;
 	bool use_master_clock;
 	u64 master_kernel_ns;
-	cycle_t master_cycle_now;
+	u64 master_cycle_now;
 	struct delayed_work kvmclock_update_work;
 	struct delayed_work kvmclock_sync_work;
 
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 2cea01128985..f28bf874cd16 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -14,7 +14,7 @@ static inline struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)
 #endif
 
 /* some helper functions for xen and kvm pv clock sources */
-cycle_t pvclock_clocksource_read(struct pvclock_vcpu_time_info *src);
+u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src);
 u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src);
 void pvclock_set_flags(u8 flags);
 unsigned long pvclock_tsc_khz(struct pvclock_vcpu_time_info *src);
@@ -89,11 +89,10 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 }
 
 static __always_inline
-cycle_t __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src,
-			      u64 tsc)
+u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
 	u64 delta = tsc - src->tsc_timestamp;
-	cycle_t offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
+	u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
 					     src->tsc_shift);
 	return src->system_time + offset;
 }
diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index b2c4704ceccd..03e9e2ac76d7 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -29,7 +29,7 @@ static inline cycles_t get_cycles(void)
 	return rdtsc();
 }
 
-extern struct system_counterval_t convert_art_to_tsc(cycle_t art);
+extern struct system_counterval_t convert_art_to_tsc(u64 art);
 
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
* Unmerged path arch/x86/include/asm/vgtod.h
diff --git a/arch/x86/kernel/apb_timer.c b/arch/x86/kernel/apb_timer.c
index ff05b68d4bb1..46122632b9b5 100644
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@ -270,7 +270,7 @@ void apbt_setup_secondary_clock(void) {}
 static int apbt_clocksource_register(void)
 {
 	u64 start, now;
-	cycle_t t1;
+	u64 t1;
 
 	/* Start the counter, use timer 2 as source, timer 0/1 for event */
 	dw_apb_clocksource_start(clocksource_apbt);
@@ -378,7 +378,7 @@ unsigned long apbt_quick_calibrate(void)
 {
 	int i, scale;
 	u64 old, new;
-	cycle_t t1, t2;
+	u64 t1, t2;
 	unsigned long khz = 0;
 	u32 loop, shift;
 
diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c
index 7dd31a5b9370..fc11a84a2778 100644
--- a/arch/x86/kernel/cpu/mshyperv.c
+++ b/arch/x86/kernel/cpu/mshyperv.c
@@ -133,9 +133,9 @@ static uint32_t  __init ms_hyperv_platform(void)
 	return 0;
 }
 
-static cycle_t read_hv_clock(struct clocksource *arg)
+static u64 read_hv_clock(struct clocksource *arg)
 {
-	cycle_t current_tick;
+	u64 current_tick;
 	/*
 	 * Read the partition counter to get the current tick count. This count
 	 * is set to 0 when the partition is created and is incremented in
diff --git a/arch/x86/kernel/hpet.c b/arch/x86/kernel/hpet.c
index 27bcf0a7c61c..000a0ede894a 100644
--- a/arch/x86/kernel/hpet.c
+++ b/arch/x86/kernel/hpet.c
@@ -774,7 +774,7 @@ static union hpet_lock hpet __cacheline_aligned = {
 	{ .lock = __ARCH_SPIN_LOCK_UNLOCKED, },
 };
 
-static cycle_t read_hpet(struct clocksource *cs)
+static u64 read_hpet(struct clocksource *cs)
 {
 	unsigned long flags;
 	union hpet_lock old, new;
@@ -785,7 +785,7 @@ static cycle_t read_hpet(struct clocksource *cs)
 	 * Read HPET directly if in NMI.
 	 */
 	if (in_nmi())
-		return (cycle_t)hpet_readl(HPET_COUNTER);
+		return (u64)hpet_readl(HPET_COUNTER);
 
 	/*
 	 * Read the current state of the lock and HPET value atomically.
@@ -804,7 +804,7 @@ static cycle_t read_hpet(struct clocksource *cs)
 		WRITE_ONCE(hpet.value, new.value);
 		arch_spin_unlock(&hpet.lock);
 		local_irq_restore(flags);
-		return (cycle_t)new.value;
+		return (u64)new.value;
 	}
 	local_irq_restore(flags);
 
@@ -826,15 +826,15 @@ contended:
 		new.lockval = READ_ONCE(hpet.lockval);
 	} while ((new.value == old.value) && arch_spin_is_locked(&new.lock));
 
-	return (cycle_t)new.value;
+	return (u64)new.value;
 }
 #else
 /*
  * For UP or 32-bit.
  */
-static cycle_t read_hpet(struct clocksource *cs)
+static u64 read_hpet(struct clocksource *cs)
 {
-	return (cycle_t)hpet_readl(HPET_COUNTER);
+	return (u64)hpet_readl(HPET_COUNTER);
 }
 #endif
 
@@ -853,7 +853,7 @@ static struct clocksource clocksource_hpet = {
 static int hpet_clocksource_register(void)
 {
 	u64 start, now;
-	cycle_t t1;
+	u64 t1;
 
 	/* Start the counter */
 	hpet_restart_counter();
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index d4dcab1c784a..90a1bab755d7 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -33,7 +33,7 @@
 static int kvmclock = 1;
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
-static cycle_t kvm_sched_clock_offset;
+static u64 kvm_sched_clock_offset;
 
 static int parse_no_kvmclock(char *arg)
 {
@@ -82,10 +82,10 @@ static int kvm_set_wallclock(const struct timespec *now)
 	return -1;
 }
 
-static cycle_t kvm_clock_read(void)
+static u64 kvm_clock_read(void)
 {
 	struct pvclock_vcpu_time_info *src;
-	cycle_t ret;
+	u64 ret;
 	int cpu;
 
 	preempt_disable_notrace();
@@ -96,12 +96,12 @@ static cycle_t kvm_clock_read(void)
 	return ret;
 }
 
-static cycle_t kvm_clock_get_cycles(struct clocksource *cs)
+static u64 kvm_clock_get_cycles(struct clocksource *cs)
 {
 	return kvm_clock_read();
 }
 
-static cycle_t kvm_sched_clock_read(void)
+static u64 kvm_sched_clock_read(void)
 {
 	return kvm_clock_read() - kvm_sched_clock_offset;
 }
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index 0b347ed56b0b..03a932c4d363 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -71,10 +71,10 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
-cycle_t pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
+u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 {
 	unsigned version;
-	cycle_t ret;
+	u64 ret;
 	u64 last;
 	u8 flags;
 
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 65f4ed2c438a..e60cc80e8d80 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1105,9 +1105,9 @@ static struct clocksource clocksource_tsc;
  * checking the result of read_tsc() - cycle_last for being negative.
  * That works because CLOCKSOURCE_MASK(64) does not mask out any bit.
  */
-static cycle_t read_tsc(struct clocksource *cs)
+static u64 read_tsc(struct clocksource *cs)
 {
-	return (cycle_t)rdtsc_ordered();
+	return (u64)rdtsc_ordered();
 }
 
 /*
@@ -1195,7 +1195,7 @@ int unsynchronized_tsc(void)
 /*
  * Convert ART to TSC given numerator/denominator found in detect_art()
  */
-struct system_counterval_t convert_art_to_tsc(cycle_t art)
+struct system_counterval_t convert_art_to_tsc(u64 art)
 {
 	u64 tmp, res, rem;
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 08257aba05f8..d1191caa22e0 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1106,8 +1106,8 @@ struct pvclock_gtod_data {
 
 	struct { /* extract of a clocksource struct */
 		int vclock_mode;
-		cycle_t	cycle_last;
-		cycle_t	mask;
+		u64	cycle_last;
+		u64	mask;
 		u32	mult;
 		u32	shift;
 	} clock;
@@ -1549,9 +1549,9 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
-static cycle_t read_tsc(void)
+static u64 read_tsc(void)
 {
-	cycle_t ret = (cycle_t)rdtsc_ordered();
+	u64 ret = (u64)rdtsc_ordered();
 	u64 last = pvclock_gtod_data.clock.cycle_last;
 
 	if (likely(ret >= last))
@@ -1569,7 +1569,7 @@ static cycle_t read_tsc(void)
 	return last;
 }
 
-static inline u64 vgettsc(cycle_t *cycle_now)
+static inline u64 vgettsc(u64 *cycle_now)
 {
 	long v;
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -1580,7 +1580,7 @@ static inline u64 vgettsc(cycle_t *cycle_now)
 	return v * gtod->clock.mult;
 }
 
-static int do_monotonic_boot(s64 *t, cycle_t *cycle_now)
+static int do_monotonic_boot(s64 *t, u64 *cycle_now)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 	unsigned long seq;
@@ -1623,7 +1623,7 @@ static int do_realtime(struct timespec *ts, u64 *cycle_now)
 }
 
 /* returns true if host is using tsc clocksource */
-static bool kvm_get_time_and_clockread(s64 *kernel_ns, cycle_t *cycle_now)
+static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *cycle_now)
 {
 	/* checked again under seqlock below */
 	if (pvclock_gtod_data.clock.vclock_mode != VCLOCK_TSC)
diff --git a/arch/x86/lguest/boot.c b/arch/x86/lguest/boot.c
index 8424d5adcfa2..fffa82f111f1 100644
--- a/arch/x86/lguest/boot.c
+++ b/arch/x86/lguest/boot.c
@@ -902,7 +902,7 @@ static unsigned long lguest_tsc_khz(void)
  * If we can't use the TSC, the kernel falls back to our lower-priority
  * "lguest_clock", where we read the time value given to us by the Host.
  */
-static cycle_t lguest_clock_read(struct clocksource *cs)
+static u64 lguest_clock_read(struct clocksource *cs)
 {
 	unsigned long sec, nsec;
 
diff --git a/arch/x86/platform/uv/uv_time.c b/arch/x86/platform/uv/uv_time.c
index 10047a3e9332..362c149c799c 100644
--- a/arch/x86/platform/uv/uv_time.c
+++ b/arch/x86/platform/uv/uv_time.c
@@ -30,7 +30,7 @@
 
 #define RTC_NAME		"sgi_rtc"
 
-static cycle_t uv_read_rtc(struct clocksource *cs);
+static u64 uv_read_rtc(struct clocksource *cs);
 static int uv_rtc_next_event(unsigned long, struct clock_event_device *);
 static void uv_rtc_timer_setup(enum clock_event_mode,
 				struct clock_event_device *);
@@ -39,7 +39,7 @@ static struct clocksource clocksource_uv = {
 	.name		= RTC_NAME,
 	.rating		= 299,
 	.read		= uv_read_rtc,
-	.mask		= (cycle_t)UVH_RTC_REAL_TIME_CLOCK_MASK,
+	.mask		= (u64)UVH_RTC_REAL_TIME_CLOCK_MASK,
 	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
 };
 
@@ -297,7 +297,7 @@ static int uv_rtc_unset_timer(int cpu, int force)
  * cachelines of it's own page.  This allows faster simultaneous reads
  * from a given socket.
  */
-static cycle_t uv_read_rtc(struct clocksource *cs)
+static u64 uv_read_rtc(struct clocksource *cs)
 {
 	unsigned long offset;
 
@@ -306,7 +306,7 @@ static cycle_t uv_read_rtc(struct clocksource *cs)
 	else
 		offset = (uv_blade_processor_id() * L1_CACHE_BYTES) % PAGE_SIZE;
 
-	return (cycle_t)uv_read_local_mmr(UVH_RTC | offset);
+	return (u64)uv_read_local_mmr(UVH_RTC | offset);
 }
 
 /*
diff --git a/arch/x86/xen/time.c b/arch/x86/xen/time.c
index 330e18ae4cb5..e82ce4184bad 100644
--- a/arch/x86/xen/time.c
+++ b/arch/x86/xen/time.c
@@ -152,10 +152,10 @@ static unsigned long xen_tsc_khz(void)
 	return pvclock_tsc_khz(info);
 }
 
-cycle_t xen_clocksource_read(void)
+u64 xen_clocksource_read(void)
 {
         struct pvclock_vcpu_time_info *src;
-	cycle_t ret;
+	u64 ret;
 
 	preempt_disable_notrace();
 	src = &__get_cpu_var(xen_vcpu)->time;
@@ -164,7 +164,7 @@ cycle_t xen_clocksource_read(void)
 	return ret;
 }
 
-static cycle_t xen_clocksource_get_cycles(struct clocksource *cs)
+static u64 xen_clocksource_get_cycles(struct clocksource *cs)
 {
 	return xen_clocksource_read();
 }
diff --git a/arch/x86/xen/xen-ops.h b/arch/x86/xen/xen-ops.h
index 774e39171c8c..c1ed840f6736 100644
--- a/arch/x86/xen/xen-ops.h
+++ b/arch/x86/xen/xen-ops.h
@@ -50,7 +50,7 @@ void xen_init_irq_ops(void);
 void xen_setup_timer(int cpu);
 void xen_setup_runstate_info(int cpu);
 void xen_teardown_timer(int cpu);
-cycle_t xen_clocksource_read(void);
+u64 xen_clocksource_read(void);
 void xen_setup_cpu_clockevents(void);
 void __init xen_init_time_ops(void);
 void __init xen_hvm_init_time_ops(void);
diff --git a/arch/xtensa/kernel/time.c b/arch/xtensa/kernel/time.c
index ffb474104311..89734a53d821 100644
--- a/arch/xtensa/kernel/time.c
+++ b/arch/xtensa/kernel/time.c
@@ -32,9 +32,9 @@ unsigned long ccount_per_jiffy;		/* per 1/HZ */
 unsigned long nsec_per_ccount;		/* nsec per ccount increment */
 #endif
 
-static cycle_t ccount_read(struct clocksource *cs)
+static u64 ccount_read(struct clocksource *cs)
 {
-	return (cycle_t)get_ccount();
+	return (u64)get_ccount();
 }
 
 static struct clocksource ccount_clocksource = {
diff --git a/drivers/char/hpet.c b/drivers/char/hpet.c
index d6c6f31c6c95..e506617e2f50 100644
--- a/drivers/char/hpet.c
+++ b/drivers/char/hpet.c
@@ -73,9 +73,9 @@ static u32 hpet_nhpet, hpet_max_freq = HPET_USER_FREQ;
 #ifdef CONFIG_IA64
 static void __iomem *hpet_mctr;
 
-static cycle_t read_hpet(struct clocksource *cs)
+static u64 read_hpet(struct clocksource *cs)
 {
-	return (cycle_t)read_counter((void __iomem *)hpet_mctr);
+	return (u64)read_counter((void __iomem *)hpet_mctr);
 }
 
 static struct clocksource clocksource_hpet = {
diff --git a/drivers/clocksource/acpi_pm.c b/drivers/clocksource/acpi_pm.c
index 6efe4d1ab3aa..4a0738f4e292 100644
--- a/drivers/clocksource/acpi_pm.c
+++ b/drivers/clocksource/acpi_pm.c
@@ -58,16 +58,16 @@ u32 acpi_pm_read_verified(void)
 	return v2;
 }
 
-static cycle_t acpi_pm_read(struct clocksource *cs)
+static u64 acpi_pm_read(struct clocksource *cs)
 {
-	return (cycle_t)read_pmtmr();
+	return (u64)read_pmtmr();
 }
 
 static struct clocksource clocksource_acpi_pm = {
 	.name		= "acpi_pm",
 	.rating		= 200,
 	.read		= acpi_pm_read,
-	.mask		= (cycle_t)ACPI_PM_MASK,
+	.mask		= (u64)ACPI_PM_MASK,
 	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
 };
 
@@ -81,9 +81,9 @@ static int __init acpi_pm_good_setup(char *__str)
 }
 __setup("acpi_pm_good", acpi_pm_good_setup);
 
-static cycle_t acpi_pm_read_slow(struct clocksource *cs)
+static u64 acpi_pm_read_slow(struct clocksource *cs)
 {
-	return (cycle_t)acpi_pm_read_verified();
+	return (u64)acpi_pm_read_verified();
 }
 
 static inline void acpi_pm_need_workaround(void)
@@ -150,7 +150,7 @@ DECLARE_PCI_FIXUP_EARLY(PCI_VENDOR_ID_SERVERWORKS, PCI_DEVICE_ID_SERVERWORKS_LE,
  */
 static int verify_pmtmr_rate(void)
 {
-	cycle_t value1, value2;
+	u64 value1, value2;
 	unsigned long count, delta;
 
 	mach_prepare_counter();
@@ -181,7 +181,7 @@ static int verify_pmtmr_rate(void)
 
 static int __init init_acpi_pm_clocksource(void)
 {
-	cycle_t value1, value2;
+	u64 value1, value2;
 	unsigned int i, j = 0;
 
 	if (!pmtmr_ioport)
* Unmerged path drivers/clocksource/arc_timer.c
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index 960fd03a2cb0..2027908cc087 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -199,12 +199,12 @@ static u64 arch_timer_read_zero(void)
 
 u64 (*arch_timer_read_counter)(void) = arch_timer_read_zero;
 
-static cycle_t arch_counter_read(struct clocksource *cs)
+static u64 arch_counter_read(struct clocksource *cs)
 {
 	return arch_timer_read_counter();
 }
 
-static cycle_t arch_counter_read_cc(const struct cyclecounter *cc)
+static u64 arch_counter_read_cc(const struct cyclecounter *cc)
 {
 	return arch_timer_read_counter();
 }
* Unmerged path drivers/clocksource/arm_global_timer.c
* Unmerged path drivers/clocksource/cadence_ttc_timer.c
* Unmerged path drivers/clocksource/clksrc-dbx500-prcmu.c
diff --git a/drivers/clocksource/dw_apb_timer.c b/drivers/clocksource/dw_apb_timer.c
index 8c2a35f26d9b..01e547699f9d 100644
--- a/drivers/clocksource/dw_apb_timer.c
+++ b/drivers/clocksource/dw_apb_timer.c
@@ -316,7 +316,7 @@ void dw_apb_clocksource_start(struct dw_apb_clocksource *dw_cs)
 	dw_apb_clocksource_read(dw_cs);
 }
 
-static cycle_t __apbt_read_clocksource(struct clocksource *cs)
+static u64 __apbt_read_clocksource(struct clocksource *cs)
 {
 	unsigned long current_count;
 	struct dw_apb_clocksource *dw_cs =
@@ -324,7 +324,7 @@ static cycle_t __apbt_read_clocksource(struct clocksource *cs)
 
 	current_count = apbt_readl(&dw_cs->timer, APBTMR_N_CURRENT_VALUE);
 
-	return (cycle_t)~current_count;
+	return (u64)~current_count;
 }
 
 static void apbt_restart_clocksource(struct clocksource *cs)
@@ -383,9 +383,9 @@ void dw_apb_clocksource_register(struct dw_apb_clocksource *dw_cs)
  *
  * @dw_cs:	The clocksource to read.
  */
-cycle_t dw_apb_clocksource_read(struct dw_apb_clocksource *dw_cs)
+u64 dw_apb_clocksource_read(struct dw_apb_clocksource *dw_cs)
 {
-	return (cycle_t)~apbt_readl(&dw_cs->timer, APBTMR_N_CURRENT_VALUE);
+	return (u64)~apbt_readl(&dw_cs->timer, APBTMR_N_CURRENT_VALUE);
 }
 
 /**
diff --git a/drivers/clocksource/em_sti.c b/drivers/clocksource/em_sti.c
index 4329a29a5310..ce7625151950 100644
--- a/drivers/clocksource/em_sti.c
+++ b/drivers/clocksource/em_sti.c
@@ -110,9 +110,9 @@ static void em_sti_disable(struct em_sti_priv *p)
 	clk_disable(p->clk);
 }
 
-static cycle_t em_sti_count(struct em_sti_priv *p)
+static u64 em_sti_count(struct em_sti_priv *p)
 {
-	cycle_t ticks;
+	u64 ticks;
 	unsigned long flags;
 
 	/* the STI hardware buffers the 48-bit count, but to
@@ -121,14 +121,14 @@ static cycle_t em_sti_count(struct em_sti_priv *p)
 	 * Always read STI_COUNT_H before STI_COUNT_L.
 	 */
 	raw_spin_lock_irqsave(&p->lock, flags);
-	ticks = (cycle_t)(em_sti_read(p, STI_COUNT_H) & 0xffff) << 32;
+	ticks = (u64)(em_sti_read(p, STI_COUNT_H) & 0xffff) << 32;
 	ticks |= em_sti_read(p, STI_COUNT_L);
 	raw_spin_unlock_irqrestore(&p->lock, flags);
 
 	return ticks;
 }
 
-static cycle_t em_sti_set_next(struct em_sti_priv *p, cycle_t next)
+static u64 em_sti_set_next(struct em_sti_priv *p, u64 next)
 {
 	unsigned long flags;
 
@@ -198,7 +198,7 @@ static struct em_sti_priv *cs_to_em_sti(struct clocksource *cs)
 	return container_of(cs, struct em_sti_priv, cs);
 }
 
-static cycle_t em_sti_clocksource_read(struct clocksource *cs)
+static u64 em_sti_clocksource_read(struct clocksource *cs)
 {
 	return em_sti_count(cs_to_em_sti(cs));
 }
@@ -284,7 +284,7 @@ static int em_sti_clock_event_next(unsigned long delta,
 				   struct clock_event_device *ced)
 {
 	struct em_sti_priv *p = ced_to_em_sti(ced);
-	cycle_t next;
+	u64 next;
 	int safe;
 
 	next = em_sti_set_next(p, em_sti_count(p) + delta);
* Unmerged path drivers/clocksource/exynos_mct.c
* Unmerged path drivers/clocksource/h8300_timer16.c
* Unmerged path drivers/clocksource/h8300_tpu.c
diff --git a/drivers/clocksource/i8253.c b/drivers/clocksource/i8253.c
index 14ee3efcc404..ef4cd0ccedc3 100644
--- a/drivers/clocksource/i8253.c
+++ b/drivers/clocksource/i8253.c
@@ -25,7 +25,7 @@ EXPORT_SYMBOL(i8253_lock);
  * to just read by itself. So use jiffies to emulate a free
  * running counter:
  */
-static cycle_t i8253_read(struct clocksource *cs)
+static u64 i8253_read(struct clocksource *cs)
 {
 	static int old_count;
 	static u32 old_jifs;
@@ -83,7 +83,7 @@ static cycle_t i8253_read(struct clocksource *cs)
 
 	count = (PIT_LATCH - 1) - count;
 
-	return (cycle_t)(jifs * PIT_LATCH) + count;
+	return (u64)(jifs * PIT_LATCH) + count;
 }
 
 static struct clocksource i8253_cs = {
* Unmerged path drivers/clocksource/jcore-pit.c
* Unmerged path drivers/clocksource/metag_generic.c
* Unmerged path drivers/clocksource/mips-gic-timer.c
* Unmerged path drivers/clocksource/mmio.c
diff --git a/drivers/clocksource/mxs_timer.c b/drivers/clocksource/mxs_timer.c
index 02af4204af86..39d448d2975a 100644
--- a/drivers/clocksource/mxs_timer.c
+++ b/drivers/clocksource/mxs_timer.c
@@ -100,7 +100,7 @@ static void timrot_irq_acknowledge(void)
 		     HW_TIMROT_TIMCTRLn(0) + STMP_OFFSET_REG_CLR);
 }
 
-static cycle_t timrotv1_get_cycles(struct clocksource *cs)
+static u64 timrotv1_get_cycles(struct clocksource *cs)
 {
 	return ~((__raw_readl(mxs_timrot_base + HW_TIMROT_TIMCOUNTn(1))
 			& 0xffff0000) >> 16);
* Unmerged path drivers/clocksource/qcom-timer.c
* Unmerged path drivers/clocksource/samsung_pwm_timer.c
diff --git a/drivers/clocksource/scx200_hrt.c b/drivers/clocksource/scx200_hrt.c
index 64f9e8294434..a46660bf6588 100644
--- a/drivers/clocksource/scx200_hrt.c
+++ b/drivers/clocksource/scx200_hrt.c
@@ -43,10 +43,10 @@ MODULE_PARM_DESC(ppm, "+-adjust to actual XO freq (ppm)");
 /* The base timer frequency, * 27 if selected */
 #define HRT_FREQ   1000000
 
-static cycle_t read_hrt(struct clocksource *cs)
+static u64 read_hrt(struct clocksource *cs)
 {
 	/* Read the timer value */
-	return (cycle_t) inl(scx200_cb_base + SCx200_TIMER_OFFSET);
+	return (u64) inl(scx200_cb_base + SCx200_TIMER_OFFSET);
 }
 
 static struct clocksource cs_hrt = {
diff --git a/drivers/clocksource/sh_cmt.c b/drivers/clocksource/sh_cmt.c
index 08d0c418c94a..f1979aa6a4bd 100644
--- a/drivers/clocksource/sh_cmt.c
+++ b/drivers/clocksource/sh_cmt.c
@@ -475,7 +475,7 @@ static struct sh_cmt_priv *cs_to_sh_cmt(struct clocksource *cs)
 	return container_of(cs, struct sh_cmt_priv, cs);
 }
 
-static cycle_t sh_cmt_clocksource_read(struct clocksource *cs)
+static u64 sh_cmt_clocksource_read(struct clocksource *cs)
 {
 	struct sh_cmt_priv *p = cs_to_sh_cmt(cs);
 	unsigned long flags, raw;
diff --git a/drivers/clocksource/sh_tmu.c b/drivers/clocksource/sh_tmu.c
index 78b8dae49628..57aefd7b9459 100644
--- a/drivers/clocksource/sh_tmu.c
+++ b/drivers/clocksource/sh_tmu.c
@@ -219,7 +219,7 @@ static struct sh_tmu_priv *cs_to_sh_tmu(struct clocksource *cs)
 	return container_of(cs, struct sh_tmu_priv, cs);
 }
 
-static cycle_t sh_tmu_clocksource_read(struct clocksource *cs)
+static u64 sh_tmu_clocksource_read(struct clocksource *cs)
 {
 	struct sh_tmu_priv *p = cs_to_sh_tmu(cs);
 
diff --git a/drivers/clocksource/tcb_clksrc.c b/drivers/clocksource/tcb_clksrc.c
index 8a6187225dd0..707994f42b04 100644
--- a/drivers/clocksource/tcb_clksrc.c
+++ b/drivers/clocksource/tcb_clksrc.c
@@ -41,7 +41,7 @@
 
 static void __iomem *tcaddr;
 
-static cycle_t tc_get_cycles(struct clocksource *cs)
+static u64 tc_get_cycles(struct clocksource *cs)
 {
 	unsigned long	flags;
 	u32		lower, upper;
@@ -56,7 +56,7 @@ static cycle_t tc_get_cycles(struct clocksource *cs)
 	return (upper << 16) | lower;
 }
 
-static cycle_t tc_get_cycles32(struct clocksource *cs)
+static u64 tc_get_cycles32(struct clocksource *cs)
 {
 	return __raw_readl(tcaddr + ATMEL_TC_REG(0, CV));
 }
* Unmerged path drivers/clocksource/time-pistachio.c
* Unmerged path drivers/clocksource/timer-atmel-pit.c
diff --git a/drivers/clocksource/timer-marco.c b/drivers/clocksource/timer-marco.c
index 15d2faa28b9b..712fc7b48786 100644
--- a/drivers/clocksource/timer-marco.c
+++ b/drivers/clocksource/timer-marco.c
@@ -84,7 +84,7 @@ static irqreturn_t sirfsoc_timer_interrupt(int irq, void *dev_id)
 }
 
 /* read 64-bit timer counter */
-static cycle_t sirfsoc_timer_read(struct clocksource *cs)
+static u64 sirfsoc_timer_read(struct clocksource *cs)
 {
 	u64 cycles;
 
* Unmerged path drivers/clocksource/timer-nps.c
* Unmerged path drivers/clocksource/timer-prima2.c
* Unmerged path drivers/clocksource/timer-sun5i.c
* Unmerged path drivers/clocksource/timer-ti-32k.c
diff --git a/drivers/clocksource/vt8500_timer.c b/drivers/clocksource/vt8500_timer.c
index 64f553f04fa4..5f324d035931 100644
--- a/drivers/clocksource/vt8500_timer.c
+++ b/drivers/clocksource/vt8500_timer.c
@@ -52,7 +52,7 @@
 
 static void __iomem *regbase;
 
-static cycle_t vt8500_timer_read(struct clocksource *cs)
+static u64 vt8500_timer_read(struct clocksource *cs)
 {
 	int loops = msecs_to_loops(10);
 	writel(3, regbase + TIMER_CTRL_VAL);
@@ -74,7 +74,7 @@ static int vt8500_timer_set_next_event(unsigned long cycles,
 				    struct clock_event_device *evt)
 {
 	int loops = msecs_to_loops(10);
-	cycle_t alarm = clocksource.read(&clocksource) + cycles;
+	u64 alarm = clocksource.read(&clocksource) + cycles;
 	while ((readl(regbase + TIMER_AS_VAL) & TIMER_MATCH_W_ACTIVE)
 						&& --loops)
 		cpu_relax();
diff --git a/drivers/hv/hv.c b/drivers/hv/hv.c
index d326dfc3f40e..57fef8dc0ecc 100644
--- a/drivers/hv/hv.c
+++ b/drivers/hv/hv.c
@@ -135,9 +135,9 @@ u64 hv_do_hypercall(u64 control, void *input, void *output)
 EXPORT_SYMBOL_GPL(hv_do_hypercall);
 
 #ifdef CONFIG_X86_64
-static cycle_t read_hv_clock_tsc(struct clocksource *arg)
+static u64 read_hv_clock_tsc(struct clocksource *arg)
 {
-	cycle_t current_tick;
+	u64 current_tick;
 	struct ms_hyperv_tsc_page *tsc_pg = hv_context.tsc_page;
 
 	if (tsc_pg->tsc_sequence != 0) {
@@ -146,7 +146,7 @@ static cycle_t read_hv_clock_tsc(struct clocksource *arg)
 		 */
 
 		while (1) {
-			cycle_t tmp;
+			u64 tmp;
 			u32 sequence = tsc_pg->tsc_sequence;
 			u64 cur_tsc;
 			u64 scale = tsc_pg->tsc_scale;
@@ -356,7 +356,7 @@ int hv_post_message(union hv_connection_id connection_id,
 static int hv_ce_set_next_event(unsigned long delta,
 				struct clock_event_device *evt)
 {
-	cycle_t current_tick;
+	u64 current_tick;
 
 	WARN_ON(evt->mode != CLOCK_EVT_MODE_ONESHOT);
 
* Unmerged path drivers/irqchip/irq-mips-gic.c
diff --git a/drivers/net/ethernet/amd/xgbe/xgbe-ptp.c b/drivers/net/ethernet/amd/xgbe/xgbe-ptp.c
index b03e4f58d02e..a533a6cc2d53 100644
--- a/drivers/net/ethernet/amd/xgbe/xgbe-ptp.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-ptp.c
@@ -122,7 +122,7 @@
 #include "xgbe.h"
 #include "xgbe-common.h"
 
-static cycle_t xgbe_cc_read(const struct cyclecounter *cc)
+static u64 xgbe_cc_read(const struct cyclecounter *cc)
 {
 	struct xgbe_prv_data *pdata = container_of(cc,
 						   struct xgbe_prv_data,
diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
index c48d896f6711..98263fbe5859 100644
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
@@ -15253,7 +15253,7 @@ void bnx2x_set_rx_ts(struct bnx2x *bp, struct sk_buff *skb)
 }
 
 /* Read the PHC */
-static cycle_t bnx2x_cyclecounter_read(const struct cyclecounter *cc)
+static u64 bnx2x_cyclecounter_read(const struct cyclecounter *cc)
 {
 	struct bnx2x *bp = container_of(cc, struct bnx2x, cyclecounter);
 	int port = BP_PORT(bp);
diff --git a/drivers/net/ethernet/freescale/fec_ptp.c b/drivers/net/ethernet/freescale/fec_ptp.c
index 25fc960cbf0e..57b194c78224 100644
--- a/drivers/net/ethernet/freescale/fec_ptp.c
+++ b/drivers/net/ethernet/freescale/fec_ptp.c
@@ -79,7 +79,7 @@
  * cyclecounter structure used to construct a ns counter from the
  * arbitrary fixed point registers
  */
-static cycle_t fec_ptp_read(const struct cyclecounter *cc)
+static u64 fec_ptp_read(const struct cyclecounter *cc)
 {
 	struct fec_enet_private *fep =
 		container_of(cc, struct fec_enet_private, cc);
diff --git a/drivers/net/ethernet/intel/e1000e/netdev.c b/drivers/net/ethernet/intel/e1000e/netdev.c
index f7604e894ff1..754998454fec 100644
--- a/drivers/net/ethernet/intel/e1000e/netdev.c
+++ b/drivers/net/ethernet/intel/e1000e/netdev.c
@@ -4319,24 +4319,24 @@ void e1000e_reinit_locked(struct e1000_adapter *adapter)
 /**
  * e1000e_sanitize_systim - sanitize raw cycle counter reads
  * @hw: pointer to the HW structure
- * @systim: cycle_t value read, sanitized and returned
+ * @systim: time value read, sanitized and returned
  *
  * Errata for 82574/82583 possible bad bits read from SYSTIMH/L:
  * check to see that the time is incrementing at a reasonable
  * rate and is a multiple of incvalue.
  **/
-static cycle_t e1000e_sanitize_systim(struct e1000_hw *hw, cycle_t systim)
+static u64 e1000e_sanitize_systim(struct e1000_hw *hw, u64 systim)
 {
 	u64 time_delta, rem, temp;
-	cycle_t systim_next;
+	u64 systim_next;
 	u32 incvalue;
 	int i;
 
 	incvalue = er32(TIMINCA) & E1000_TIMINCA_INCVALUE_MASK;
 	for (i = 0; i < E1000_MAX_82574_SYSTIM_REREADS; i++) {
 		/* latch SYSTIMH on read of SYSTIML */
-		systim_next = (cycle_t)er32(SYSTIML);
-		systim_next |= (cycle_t)er32(SYSTIMH) << 32;
+		systim_next = (u64)er32(SYSTIML);
+		systim_next |= (u64)er32(SYSTIMH) << 32;
 
 		time_delta = systim_next - systim;
 		temp = time_delta;
@@ -4356,13 +4356,13 @@ static cycle_t e1000e_sanitize_systim(struct e1000_hw *hw, cycle_t systim)
  * e1000e_cyclecounter_read - read raw cycle counter (used by time counter)
  * @cc: cyclecounter structure
  **/
-static cycle_t e1000e_cyclecounter_read(const struct cyclecounter *cc)
+static u64 e1000e_cyclecounter_read(const struct cyclecounter *cc)
 {
 	struct e1000_adapter *adapter = container_of(cc, struct e1000_adapter,
 						     cc);
 	struct e1000_hw *hw = &adapter->hw;
 	u32 systimel, systimeh;
-	cycle_t systim;
+	u64 systim;
 	/* SYSTIMH latching upon SYSTIML read does not work well.
 	 * This means that if SYSTIML overflows after we read it but before
 	 * we read SYSTIMH, the value of SYSTIMH has been incremented and we
@@ -4382,8 +4382,8 @@ static cycle_t e1000e_cyclecounter_read(const struct cyclecounter *cc)
 			systimel = systimel_2;
 		}
 	}
-	systim = (cycle_t)systimel;
-	systim |= (cycle_t)systimeh << 32;
+	systim = (u64)systimel;
+	systim |= (u64)systimeh << 32;
 
 	if (adapter->flags2 & FLAG2_CHECK_SYSTIM_OVERFLOW)
 		systim = e1000e_sanitize_systim(hw, systim);
diff --git a/drivers/net/ethernet/intel/e1000e/ptp.c b/drivers/net/ethernet/intel/e1000e/ptp.c
index ede84e47be0e..1b1eca2a24c3 100644
--- a/drivers/net/ethernet/intel/e1000e/ptp.c
+++ b/drivers/net/ethernet/intel/e1000e/ptp.c
@@ -127,8 +127,8 @@ static int e1000e_phc_get_syncdevicetime(ktime_t *device,
 	unsigned long flags;
 	int i;
 	u32 tsync_ctrl;
-	cycle_t dev_cycles;
-	cycle_t sys_cycles;
+	u64 dev_cycles;
+	u64 sys_cycles;
 
 	tsync_ctrl = er32(TSYNCTXCTL);
 	tsync_ctrl |= E1000_TSYNCTXCTL_START_SYNC |
diff --git a/drivers/net/ethernet/intel/igb/igb_ptp.c b/drivers/net/ethernet/intel/igb/igb_ptp.c
index a7895c4cbcc3..0f17b05d2e1a 100644
--- a/drivers/net/ethernet/intel/igb/igb_ptp.c
+++ b/drivers/net/ethernet/intel/igb/igb_ptp.c
@@ -77,7 +77,7 @@
 static void igb_ptp_tx_hwtstamp(struct igb_adapter *adapter);
 
 /* SYSTIM read access for the 82576 */
-static cycle_t igb_ptp_read_82576(const struct cyclecounter *cc)
+static u64 igb_ptp_read_82576(const struct cyclecounter *cc)
 {
 	struct igb_adapter *igb = container_of(cc, struct igb_adapter, cc);
 	struct e1000_hw *hw = &igb->hw;
@@ -94,7 +94,7 @@ static cycle_t igb_ptp_read_82576(const struct cyclecounter *cc)
 }
 
 /* SYSTIM read access for the 82580 */
-static cycle_t igb_ptp_read_82580(const struct cyclecounter *cc)
+static u64 igb_ptp_read_82580(const struct cyclecounter *cc)
 {
 	struct igb_adapter *igb = container_of(cc, struct igb_adapter, cc);
 	struct e1000_hw *hw = &igb->hw;
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c
index 55999072ba90..ef0635e0918c 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c
@@ -245,7 +245,7 @@ static void ixgbe_ptp_setup_sdp_x540(struct ixgbe_adapter *adapter)
  * result of SYSTIME is 32bits of "billions of cycles" and 32 bits of
  * "cycles", rather than seconds and nanoseconds.
  */
-static cycle_t ixgbe_ptp_read_X550(const struct cyclecounter *hw_cc)
+static u64 ixgbe_ptp_read_X550(const struct cyclecounter *hw_cc)
 {
 	struct ixgbe_adapter *adapter =
 			container_of(hw_cc, struct ixgbe_adapter, hw_cc);
@@ -282,7 +282,7 @@ static cycle_t ixgbe_ptp_read_X550(const struct cyclecounter *hw_cc)
  * cyclecounter structure used to construct a ns counter from the
  * arbitrary fixed point registers
  */
-static cycle_t ixgbe_ptp_read_82599(const struct cyclecounter *cc)
+static u64 ixgbe_ptp_read_82599(const struct cyclecounter *cc)
 {
 	struct ixgbe_adapter *adapter =
 		container_of(cc, struct ixgbe_adapter, hw_cc);
diff --git a/drivers/net/ethernet/mellanox/mlx4/en_clock.c b/drivers/net/ethernet/mellanox/mlx4/en_clock.c
index 35738b3736a4..e9e9b47c93bd 100644
--- a/drivers/net/ethernet/mellanox/mlx4/en_clock.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_clock.c
@@ -38,7 +38,7 @@
 
 /* mlx4_en_read_clock - read raw cycle counter (to be used by time counter)
  */
-static cycle_t mlx4_en_read_clock(const struct cyclecounter *tc)
+static u64 mlx4_en_read_clock(const struct cyclecounter *tc)
 {
 	struct mlx4_en_dev *mdev =
 		container_of(tc, struct mlx4_en_dev, cycles);
diff --git a/drivers/net/ethernet/mellanox/mlx4/main.c b/drivers/net/ethernet/mellanox/mlx4/main.c
index fe07e430aab2..33c2281caa6e 100644
--- a/drivers/net/ethernet/mellanox/mlx4/main.c
+++ b/drivers/net/ethernet/mellanox/mlx4/main.c
@@ -1839,10 +1839,10 @@ static void unmap_bf_area(struct mlx4_dev *dev)
 		io_mapping_free(mlx4_priv(dev)->bf_mapping);
 }
 
-cycle_t mlx4_read_clock(struct mlx4_dev *dev)
+u64 mlx4_read_clock(struct mlx4_dev *dev)
 {
 	u32 clockhi, clocklo, clockhi1;
-	cycle_t cycles;
+	u64 cycles;
 	int i;
 	struct mlx4_priv *priv = mlx4_priv(dev);
 
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c b/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
index 0349c0e8dfb6..349dc72cd2b6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
@@ -65,7 +65,7 @@ void mlx5e_fill_hwstamp(struct mlx5e_tstamp *tstamp, u64 timestamp,
 	hwts->hwtstamp = ns_to_ktime(nsec);
 }
 
-static cycle_t mlx5e_read_internal_timer(const struct cyclecounter *cc)
+static u64 mlx5e_read_internal_timer(const struct cyclecounter *cc)
 {
 	struct mlx5e_tstamp *tstamp = container_of(cc, struct mlx5e_tstamp,
 						   cycles);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/main.c b/drivers/net/ethernet/mellanox/mlx5/core/main.c
index ddd26354b516..bc249f4c0728 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -564,7 +564,7 @@ int mlx5_core_disable_hca(struct mlx5_core_dev *dev, u16 func_id)
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
 }
 
-cycle_t mlx5_read_internal_timer(struct mlx5_core_dev *dev)
+u64 mlx5_read_internal_timer(struct mlx5_core_dev *dev)
 {
 	u32 timer_h, timer_h1, timer_l;
 
@@ -574,7 +574,7 @@ cycle_t mlx5_read_internal_timer(struct mlx5_core_dev *dev)
 	if (timer_h != timer_h1) /* wrap around */
 		timer_l = ioread32be(&dev->iseg->internal_timer_l);
 
-	return (cycle_t)timer_l | (cycle_t)timer_h1 << 32;
+	return (u64)timer_l | (u64)timer_h1 << 32;
 }
 
 static int mlx5_irq_set_affinity_hint(struct mlx5_core_dev *mdev, int i)
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
index 1a71674063d3..ab6faa22dc95 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -106,7 +106,7 @@ int mlx5_modify_scheduling_element_cmd(struct mlx5_core_dev *dev, u8 hierarchy,
 int mlx5_destroy_scheduling_element_cmd(struct mlx5_core_dev *dev, u8 hierarchy,
 					u32 element_id);
 int mlx5_wait_for_vf_pages(struct mlx5_core_dev *dev);
-cycle_t mlx5_read_internal_timer(struct mlx5_core_dev *dev);
+u64 mlx5_read_internal_timer(struct mlx5_core_dev *dev);
 u32 mlx5_get_msix_vec(struct mlx5_core_dev *dev, int vecidx);
 struct mlx5_eq *mlx5_eqn2eq(struct mlx5_core_dev *dev, int eqn);
 void mlx5_cq_tasklet_cb(unsigned long data);
diff --git a/drivers/net/ethernet/ti/cpts.c b/drivers/net/ethernet/ti/cpts.c
index 8c351f100aca..c753deb8f074 100644
--- a/drivers/net/ethernet/ti/cpts.c
+++ b/drivers/net/ethernet/ti/cpts.c
@@ -103,7 +103,7 @@ static int cpts_fifo_read(struct cpts *cpts, int match)
 	return type == match ? 0 : -1;
 }
 
-static cycle_t cpts_systim_read(const struct cyclecounter *cc)
+static u64 cpts_systim_read(const struct cyclecounter *cc)
 {
 	u64 val = 0;
 	struct cpts_event *event;
* Unmerged path include/linux/clocksource.h
* Unmerged path include/linux/dw_apb_timer.h
* Unmerged path include/linux/irqchip/mips-gic.h
diff --git a/include/linux/mlx4/device.h b/include/linux/mlx4/device.h
index 363dcb904bd2..add8155006bb 100644
--- a/include/linux/mlx4/device.h
+++ b/include/linux/mlx4/device.h
@@ -1463,7 +1463,7 @@ int mlx4_get_roce_gid_from_slave(struct mlx4_dev *dev, int port, int slave_id,
 int mlx4_FLOW_STEERING_IB_UC_QP_RANGE(struct mlx4_dev *dev, u32 min_range_qpn,
 				      u32 max_range_qpn);
 
-cycle_t mlx4_read_clock(struct mlx4_dev *dev);
+u64 mlx4_read_clock(struct mlx4_dev *dev);
 
 struct mlx4_active_ports {
 	DECLARE_BITMAP(ports, MLX4_MAX_PORTS);
diff --git a/include/linux/timecounter.h b/include/linux/timecounter.h
index 2e4bb1e78b77..3d19ad72938c 100644
--- a/include/linux/timecounter.h
+++ b/include/linux/timecounter.h
@@ -21,7 +21,7 @@
 #include <linux/clocksource.h>
 
 /* simplify initialization of mask field */
-#define CYCLECOUNTER_MASK(bits) (cycle_t)((bits) < 64 ? ((1ULL<<(bits))-1) : -1)
+#define CYCLECOUNTER_MASK(bits) (u64)((bits) < 64 ? ((1ULL<<(bits))-1) : -1)
 
 /**
  * struct cyclecounter - hardware abstraction for a free running counter
@@ -38,8 +38,8 @@
  * @shift:		cycle to nanosecond divisor (power of two)
  */
 struct cyclecounter {
-	cycle_t (*read)(const struct cyclecounter *cc);
-	cycle_t mask;
+	u64 (*read)(const struct cyclecounter *cc);
+	u64 mask;
 	u32 mult;
 	u32 shift;
 };
@@ -64,7 +64,7 @@ struct cyclecounter {
  */
 struct timecounter {
 	const struct cyclecounter *cc;
-	cycle_t cycle_last;
+	u64 cycle_last;
 	u64 nsec;
 	u64 mask;
 	u64 frac;
@@ -78,7 +78,7 @@ struct timecounter {
  * @frac:	pointer to storage for the fractional nanoseconds.
  */
 static inline u64 cyclecounter_cyc2ns(const struct cyclecounter *cc,
-				      cycle_t cycles, u64 mask, u64 *frac)
+				      u64 cycles, u64 mask, u64 *frac)
 {
 	u64 ns = (u64) cycles;
 
@@ -135,6 +135,6 @@ extern u64 timecounter_read(struct timecounter *tc);
  * in the past.
  */
 extern u64 timecounter_cyc2time(struct timecounter *tc,
-				cycle_t cycle_tstamp);
+				u64 cycle_tstamp);
 
 #endif
* Unmerged path include/linux/timekeeper_internal.h
diff --git a/include/linux/timekeeping.h b/include/linux/timekeeping.h
index f5c843066dde..e4ad8a06e6be 100644
--- a/include/linux/timekeeping.h
+++ b/include/linux/timekeeping.h
@@ -135,7 +135,7 @@ static inline u64 ktime_get_raw_ns(void)
  * @cs_was_changed_seq:	The sequence number of clocksource change events
  */
 struct system_time_snapshot {
-	cycle_t		cycles;
+	u64		cycles;
 	ktime_t		real;
 	ktime_t		raw;
 	unsigned int	clock_was_set_seq;
@@ -163,7 +163,7 @@ struct system_device_crosststamp {
  *	timekeeping code to verify comparibility of two cycle values
  */
 struct system_counterval_t {
-	cycle_t			cycles;
+	u64			cycles;
 	struct clocksource	*cs;
 };
 
* Unmerged path include/linux/types.h
* Unmerged path kernel/time/clocksource.c
diff --git a/kernel/time/jiffies.c b/kernel/time/jiffies.c
index 7a925ba456fb..68c0a364c82b 100644
--- a/kernel/time/jiffies.c
+++ b/kernel/time/jiffies.c
@@ -53,9 +53,9 @@
  */
 #define JIFFIES_SHIFT	8
 
-static cycle_t jiffies_read(struct clocksource *cs)
+static u64 jiffies_read(struct clocksource *cs)
 {
-	return (cycle_t) jiffies;
+	return (u64) jiffies;
 }
 
 static struct clocksource clocksource_jiffies = {
diff --git a/kernel/time/timecounter.c b/kernel/time/timecounter.c
index 4687b3104bae..8afd78932bdf 100644
--- a/kernel/time/timecounter.c
+++ b/kernel/time/timecounter.c
@@ -43,7 +43,7 @@ EXPORT_SYMBOL_GPL(timecounter_init);
  */
 static u64 timecounter_read_delta(struct timecounter *tc)
 {
-	cycle_t cycle_now, cycle_delta;
+	u64 cycle_now, cycle_delta;
 	u64 ns_offset;
 
 	/* read cycle counter: */
@@ -80,7 +80,7 @@ EXPORT_SYMBOL_GPL(timecounter_read);
  * time previous to the time stored in the cycle counter.
  */
 static u64 cc_cyc2ns_backwards(const struct cyclecounter *cc,
-			       cycle_t cycles, u64 mask, u64 frac)
+			       u64 cycles, u64 mask, u64 frac)
 {
 	u64 ns = (u64) cycles;
 
@@ -90,7 +90,7 @@ static u64 cc_cyc2ns_backwards(const struct cyclecounter *cc,
 }
 
 u64 timecounter_cyc2time(struct timecounter *tc,
-			 cycle_t cycle_tstamp)
+			 u64 cycle_tstamp)
 {
 	u64 delta = (cycle_tstamp - tc->cycle_last) & tc->cc->mask;
 	u64 nsec = tc->nsec, frac = tc->frac;
* Unmerged path kernel/time/timekeeping.c
diff --git a/kernel/time/timekeeping_internal.h b/kernel/time/timekeeping_internal.h
index 676947b97c42..efb9bb602871 100644
--- a/kernel/time/timekeeping_internal.h
+++ b/kernel/time/timekeeping_internal.h
@@ -7,14 +7,14 @@
 #include <linux/clocksource.h>
 
 #ifdef CONFIG_CLOCKSOURCE_VALIDATE_LAST_CYCLE
-static inline cycle_t clocksource_delta(cycle_t now, cycle_t last, cycle_t mask)
+static inline u64 clocksource_delta(u64 now, u64 last, u64 mask)
 {
-	cycle_t ret = (now - last) & mask;
+	u64 ret = (now - last) & mask;
 
 	return (s64) ret > 0 ? ret : 0;
 }
 #else
-static inline cycle_t clocksource_delta(cycle_t now, cycle_t last, cycle_t mask)
+static inline u64 clocksource_delta(u64 now, u64 last, u64 mask)
 {
 	return (now - last) & mask;
 }
* Unmerged path kernel/trace/ftrace.c
* Unmerged path kernel/trace/trace.c
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d1e2cc07e8dc..2100e1c2eec7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -157,7 +157,7 @@ struct trace_array_cpu {
 	unsigned long		policy;
 	unsigned long		rt_priority;
 	unsigned long		skipped_entries;
-	cycle_t			preempt_timestamp;
+	u64			preempt_timestamp;
 	pid_t			pid;
 	kuid_t			uid;
 	char			comm[TASK_COMM_LEN];
@@ -169,7 +169,7 @@ struct trace_buffer {
 	struct trace_array		*tr;
 	struct ring_buffer		*buffer;
 	struct trace_array_cpu __percpu	*data;
-	cycle_t				time_start;
+	u64				time_start;
 	int				cpu;
 };
 
@@ -658,7 +658,7 @@ static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
 }
 #endif /* CONFIG_STACKTRACE */
 
-extern cycle_t ftrace_now(int cpu);
+extern u64 ftrace_now(int cpu);
 
 extern void trace_find_cmdline(int pid, char comm[]);
 
@@ -696,7 +696,7 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);
-extern unsigned long long ns2usecs(cycle_t nsec);
+extern unsigned long long ns2usecs(u64 nsec);
 extern int
 trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
 extern int
* Unmerged path kernel/trace/trace_irqsoff.c
* Unmerged path kernel/trace/trace_sched_wakeup.c
diff --git a/sound/hda/hdac_stream.c b/sound/hda/hdac_stream.c
index e5d16a0f9502..e312f0962aad 100644
--- a/sound/hda/hdac_stream.c
+++ b/sound/hda/hdac_stream.c
@@ -464,7 +464,7 @@ int snd_hdac_stream_set_params(struct hdac_stream *azx_dev,
 }
 EXPORT_SYMBOL_GPL(snd_hdac_stream_set_params);
 
-static cycle_t azx_cc_read(const struct cyclecounter *cc)
+static u64 azx_cc_read(const struct cyclecounter *cc)
 {
 	struct hdac_stream *azx_dev = container_of(cc, struct hdac_stream, cc);
 
@@ -472,7 +472,7 @@ static cycle_t azx_cc_read(const struct cyclecounter *cc)
 }
 
 static void azx_timecounter_init(struct hdac_stream *azx_dev,
-				 bool force, cycle_t last)
+				 bool force, u64 last)
 {
 	struct timecounter *tc = &azx_dev->tc;
 	struct cyclecounter *cc = &azx_dev->cc;
@@ -522,7 +522,7 @@ void snd_hdac_stream_timecounter_init(struct hdac_stream *azx_dev,
 	struct snd_pcm_runtime *runtime = azx_dev->substream->runtime;
 	struct hdac_stream *s;
 	bool inited = false;
-	cycle_t cycle_last = 0;
+	u64 cycle_last = 0;
 	int i = 0;
 
 	list_for_each_entry(s, &bus->stream_list, list) {
* Unmerged path virt/kvm/arm/arch_timer.c
