scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] qla2xxx: Add FC-NVMe F/W initialization and transport registration (Himanshu Madhani) [1316281]
Rebuild_FUZZ: 95.65%
commit-author Duane Grigsby <duane.grigsby@cavium.com>
commit e84067d7430107a982858f11c5239542b56a8449
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e84067d7.failed

This code provides the interfaces to register remote and local ports of
FC4 type 0x28 with the FC-NVMe transport and transports the requests
(FC-NVMe FC link services and FC-NVMe commands IUs) to the fabric. It
also provides the support for allocating h/w queues and aborting FC-NVMe
FC requests.

	Signed-off-by: Darren Trapp <darren.trapp@cavium.com>
	Signed-off-by: Duane Grigsby <duane.grigsby@cavium.com>
	Signed-off-by: Anil Gurumurthy <anil.gurumurhty@cavium.com>
	Signed-off-by: Giridhar Malavali <giridhar.malavali@cavium.com>
	Signed-off-by: Himanshu Madhani <himanshu.madhani@cavium.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit e84067d7430107a982858f11c5239542b56a8449)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/qla2xxx/qla_dbg.c
#	drivers/scsi/qla2xxx/qla_def.h
#	drivers/scsi/qla2xxx/qla_gbl.h
#	drivers/scsi/qla2xxx/qla_init.c
#	drivers/scsi/qla2xxx/qla_iocb.c
#	drivers/scsi/qla2xxx/qla_isr.c
#	drivers/scsi/qla2xxx/qla_mbx.c
#	drivers/scsi/qla2xxx/qla_os.c
diff --cc drivers/scsi/qla2xxx/qla_dbg.c
index d4184772f017,26751d34bcf2..000000000000
--- a/drivers/scsi/qla2xxx/qla_dbg.c
+++ b/drivers/scsi/qla2xxx/qla_dbg.c
@@@ -12,16 -12,14 +12,27 @@@
   * |             Level            |   Last Value Used  |     Holes	|
   * ----------------------------------------------------------------------
   * | Module Init and Probe        |       0x0193       | 0x0146         |
++<<<<<<< HEAD
 + * | Mailbox commands             |       0x1199       | 0x111a-0x111b  |
 + * |                              |                    | 0x1155-0x1158  |
 + * |                              |                    | 0x1018-0x1019  |
 + * |                              |                    | 0x1115-0x1116  |
 + * |                              |                    | 0x10ca,0x1193  |
 + * | Device Discovery             |       0x2095       | 0x2016         |
 + * |                              |                    | 0x2020-0x2022, |
 + * |                              |                    | 0x2011-0x2012, |
 + * |                              |                    | 0x2099-0x20a4  |
 + * | Queue Command and IO tracing |       0x3075       | 0x300b         |
++=======
+  * |                              |                    | 0x015b-0x0160	|
+  * |                              |                    | 0x016e		|
+  * | Mailbox commands             |       0x1199       | 0x1193		|
+  * | Device Discovery             |       0x2134       | 0x210e-0x2116  |
+  * |				  | 		       | 0x211a         |
+  * |                              |                    | 0x211c-0x2128  |
+  * |                              |                    | 0x212a-0x2130  |
+  * | Queue Command and IO tracing |       0x3074       | 0x300b         |
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
   * |                              |                    | 0x3027-0x3028  |
   * |                              |                    | 0x303d-0x3041  |
   * |                              |                    | 0x302d,0x3033  |
diff --cc drivers/scsi/qla2xxx/qla_def.h
index 3f28536dc812,0dbcb84011b0..000000000000
--- a/drivers/scsi/qla2xxx/qla_def.h
+++ b/drivers/scsi/qla2xxx/qla_def.h
@@@ -363,6 -399,35 +364,38 @@@ struct srb_iocb 
  			__le16 comp_status;
  			struct completion comp;
  		} abt;
++<<<<<<< HEAD
++=======
+ 		struct ct_arg ctarg;
+ #define MAX_IOCB_MB_REG 28
+ #define SIZEOF_IOCB_MB_REG (MAX_IOCB_MB_REG * sizeof(uint16_t))
+ 		struct {
+ 			__le16 in_mb[MAX_IOCB_MB_REG];	/* from FW */
+ 			__le16 out_mb[MAX_IOCB_MB_REG];	/* to FW */
+ 			void *out, *in;
+ 			dma_addr_t out_dma, in_dma;
+ 			struct completion comp;
+ 			int rc;
+ 		} mbx;
+ 		struct {
+ 			struct imm_ntfy_from_isp *ntfy;
+ 		} nack;
+ 		struct {
+ 			__le16 comp_status;
+ 			uint16_t rsp_pyld_len;
+ 			uint8_t	aen_op;
+ 			void *desc;
+ 
+ 			/* These are only used with ls4 requests */
+ 			int cmd_len;
+ 			int rsp_len;
+ 			dma_addr_t cmd_dma;
+ 			dma_addr_t rsp_dma;
+ 			enum nvmefc_fcp_datadir dir;
+ 			uint32_t dl;
+ 			uint32_t timeout_sec;
+ 		} nvme;
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  	} u;
  
  	struct timer_list timer;
@@@ -382,11 -447,32 +415,24 @@@
  #define SRB_FXIOCB_DCMD	10
  #define SRB_FXIOCB_BCMD	11
  #define SRB_ABT_CMD	12
++<<<<<<< HEAD
++=======
+ #define SRB_ELS_DCMD	13
+ #define SRB_MB_IOCB	14
+ #define SRB_CT_PTHRU_CMD 15
+ #define SRB_NACK_PLOGI	16
+ #define SRB_NACK_PRLI	17
+ #define SRB_NACK_LOGO	18
+ #define SRB_NVME_CMD	19
+ #define SRB_NVME_LS	20
+ #define SRB_PRLI_CMD	21
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  
 -enum {
 -	TYPE_SRB,
 -	TYPE_TGT_CMD,
 -};
  
  typedef struct srb {
 -	/*
 -	 * Do not move cmd_type field, it needs to
 -	 * line up with qla_tgt_cmd->cmd_type
 -	 */
 -	uint8_t cmd_type;
 -	uint8_t pad[3];
  	atomic_t ref_count;
+ 	wait_queue_head_t nvme_ls_waitQ;
  	struct fc_port *fcport;
 -	struct scsi_qla_host *vha;
  	uint32_t handle;
  	uint16_t flags;
  	uint16_t type;
@@@ -2027,6 -2290,40 +2073,43 @@@ typedef struct fc_port 
  	uint16_t loop_id;
  	uint16_t old_loop_id;
  
++<<<<<<< HEAD
++=======
+ 	unsigned int conf_compl_supported:1;
+ 	unsigned int deleted:2;
+ 	unsigned int local:1;
+ 	unsigned int logout_on_delete:1;
+ 	unsigned int logo_ack_needed:1;
+ 	unsigned int keep_nport_handle:1;
+ 	unsigned int send_els_logo:1;
+ 	unsigned int login_pause:1;
+ 	unsigned int login_succ:1;
+ 
+ 	struct work_struct nvme_del_work;
+ 	atomic_t nvme_ref_count;
+ 	wait_queue_head_t nvme_waitQ;
+ 	uint32_t nvme_prli_service_param;
+ #define NVME_PRLI_SP_CONF       BIT_7
+ #define NVME_PRLI_SP_INITIATOR  BIT_5
+ #define NVME_PRLI_SP_TARGET     BIT_4
+ #define NVME_PRLI_SP_DISCOVERY  BIT_3
+ 	uint8_t nvme_flag;
+ #define NVME_FLAG_REGISTERED 4
+ 
+ 	struct fc_port *conflict;
+ 	unsigned char logout_completed;
+ 	int generation;
+ 
+ 	struct se_session *se_sess;
+ 	struct kref sess_kref;
+ 	struct qla_tgt *tgt;
+ 	unsigned long expires;
+ 	struct list_head del_list_entry;
+ 	struct work_struct free_work;
+ 
+ 	struct qlt_plogi_ack_t *plogi_link[QLT_PLOGI_LINK_MAX];
+ 
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  	uint16_t tgt_id;
  	uint16_t old_tgt_id;
  
@@@ -3654,6 -4127,13 +3737,16 @@@ typedef struct scsi_qla_host 
  	uint8_t		port_name[WWN_SIZE];
  	uint8_t		fabric_node_name[WWN_SIZE];
  
++<<<<<<< HEAD
++=======
+ 	struct		nvme_fc_local_port *nvme_local_port;
+ 	atomic_t	nvme_ref_count;
+ 	wait_queue_head_t nvme_waitQ;
+ 	struct list_head nvme_rport_list;
+ 	atomic_t 	nvme_active_aen_cnt;
+ 	uint16_t	nvme_last_rptd_aen;
+ 
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  	uint16_t	fcoe_vlan_id;
  	uint16_t	fcoe_fcf_idx;
  	uint8_t		fcoe_vn_port_mac[6];
diff --cc drivers/scsi/qla2xxx/qla_gbl.h
index c4f068539cd6,c6af45f7d5d6..000000000000
--- a/drivers/scsi/qla2xxx/qla_gbl.h
+++ b/drivers/scsi/qla2xxx/qla_gbl.h
@@@ -118,10 -142,16 +128,16 @@@ extern int ql2xenabledif
  extern int ql2xenablehba_err_chk;
  extern int ql2xtargetreset;
  extern int ql2xdontresethba;
 -extern uint64_t ql2xmaxlun;
 +extern unsigned int ql2xmaxlun;
  extern int ql2xmdcapmask;
  extern int ql2xmdenable;
 -extern int ql2xexlogins;
 -extern int ql2xexchoffld;
 -extern int ql2xiniexchg;
  extern int ql2xfwholdabts;
++<<<<<<< HEAD
++=======
+ extern int ql2xmvasynctoatio;
+ extern int ql2xuctrlirq;
+ extern int ql2xnvmeenable;
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  
  extern int qla2x00_loop_reset(scsi_qla_host_t *);
  extern void qla2x00_abort_all_cmds(scsi_qla_host_t *, int);
diff --cc drivers/scsi/qla2xxx/qla_init.c
index 998b579c233c,d06c3c758322..000000000000
--- a/drivers/scsi/qla2xxx/qla_init.c
+++ b/drivers/scsi/qla2xxx/qla_init.c
@@@ -3382,7 -4516,15 +3382,12 @@@ qla2x00_update_fcport(scsi_qla_host_t *
  	}
  	fcport->login_retry = 0;
  	fcport->flags &= ~(FCF_LOGIN_NEEDED | FCF_ASYNC_SENT);
 -	fcport->disc_state = DSC_LOGIN_COMPLETE;
 -	fcport->deleted = 0;
 -	fcport->logout_on_delete = 1;
  
+ 	if (fcport->fc4f_nvme) {
+ 		qla_nvme_register_remote(vha, fcport);
+ 		return;
+ 	}
+ 
  	qla2x00_set_fcport_state(fcport, FCS_ONLINE);
  	qla2x00_iidma_fcport(vha, fcport);
  	qla24xx_update_fcport_fcp_prio(vha, fcport);
@@@ -3481,106 -4661,25 +3486,113 @@@ qla2x00_configure_fabric(scsi_qla_host_
  			fcport->scan_state = QLA_FCPORT_SCAN;
  		}
  
 -		/* Mark the time right before querying FW for connected ports.
 -		 * This process is long, asynchronous and by the time it's done,
 -		 * collected information might not be accurate anymore. E.g.
 -		 * disconnected port might have re-connected and a brand new
 -		 * session has been created. In this case session's generation
 -		 * will be newer than discovery_gen. */
 -		qlt_do_generation_tick(vha, &discovery_gen);
 -
 -		rval = qla2x00_find_all_fabric_devs(vha);
 +		rval = qla2x00_find_all_fabric_devs(vha, &new_fcports);
  		if (rval != QLA_SUCCESS)
  			break;
 +
 +		/*
 +		 * Logout all previous fabric devices marked lost, except
 +		 * FCP2 devices.
 +		 */
 +		list_for_each_entry(fcport, &vha->vp_fcports, list) {
 +			if (test_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags))
 +				break;
 +
 +			if ((fcport->flags & FCF_FABRIC_DEVICE) == 0)
 +				continue;
 +
 +			if (fcport->scan_state == QLA_FCPORT_SCAN &&
 +			    atomic_read(&fcport->state) == FCS_ONLINE) {
 +				qla2x00_mark_device_lost(vha, fcport,
 +				    ql2xplogiabsentdevice, 0);
 +				if (fcport->loop_id != FC_NO_LOOP_ID &&
 +				    (fcport->flags & FCF_FCP2_DEVICE) == 0 &&
 +				    fcport->port_type != FCT_INITIATOR &&
 +				    fcport->port_type != FCT_BROADCAST) {
 +					ha->isp_ops->fabric_logout(vha,
 +					    fcport->loop_id,
 +					    fcport->d_id.b.domain,
 +					    fcport->d_id.b.area,
 +					    fcport->d_id.b.al_pa);
 +					qla2x00_clear_loop_id(fcport);
 +				}
 +			}
 +		}
 +
 +		/* Starting free loop ID. */
 +		next_loopid = ha->min_external_loopid;
 +
 +		/*
 +		 * Scan through our port list and login entries that need to be
 +		 * logged in.
 +		 */
 +		list_for_each_entry(fcport, &vha->vp_fcports, list) {
 +			if (atomic_read(&vha->loop_down_timer) ||
 +			    test_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags))
 +				break;
 +
 +			if ((fcport->flags & FCF_FABRIC_DEVICE) == 0 ||
 +			    (fcport->flags & FCF_LOGIN_NEEDED) == 0)
 +				continue;
 +
 +			if (fcport->loop_id == FC_NO_LOOP_ID) {
 +				fcport->loop_id = next_loopid;
 +				rval = qla2x00_find_new_loop_id(
 +				    base_vha, fcport);
 +				if (rval != QLA_SUCCESS) {
 +					/* Ran out of IDs to use */
 +					break;
 +				}
 +			}
 +			/* Login and update database */
 +			qla2x00_fabric_dev_login(vha, fcport, &next_loopid);
 +		}
 +
 +		/* Exit if out of loop IDs. */
 +		if (rval != QLA_SUCCESS) {
 +			break;
 +		}
 +
 +		/*
 +		 * Login and add the new devices to our port list.
 +		 */
 +		list_for_each_entry_safe(fcport, fcptemp, &new_fcports, list) {
 +			if (atomic_read(&vha->loop_down_timer) ||
 +			    test_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags))
 +				break;
 +
 +			/* Find a new loop ID to use. */
 +			fcport->loop_id = next_loopid;
 +			rval = qla2x00_find_new_loop_id(base_vha, fcport);
 +			if (rval != QLA_SUCCESS) {
 +				/* Ran out of IDs to use */
 +				break;
 +			}
 +
 +			/* Login and update database */
 +			qla2x00_fabric_dev_login(vha, fcport, &next_loopid);
 +
 +			list_move_tail(&fcport->list, &vha->vp_fcports);
 +		}
  	} while (0);
  
++<<<<<<< HEAD
 +	/* Free all new device structures not processed. */
 +	list_for_each_entry_safe(fcport, fcptemp, &new_fcports, list) {
 +		list_del(&fcport->list);
 +		kfree(fcport);
 +	}
 +
 +	if (rval) {
++=======
+ 	if (!vha->nvme_local_port && vha->flags.nvme_enabled)
+ 		qla_nvme_register_hba(vha);
+ 
+ 	if (rval)
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  		ql_dbg(ql_dbg_disc, vha, 0x2068,
  		    "Configure fabric error exit rval=%d.\n", rval);
 +	}
  
  	return (rval);
  }
diff --cc drivers/scsi/qla2xxx/qla_iocb.c
index ff2ecaf39d3d,d40fa000615c..000000000000
--- a/drivers/scsi/qla2xxx/qla_iocb.c
+++ b/drivers/scsi/qla2xxx/qla_iocb.c
@@@ -2893,6 -3106,88 +2893,91 @@@ qla24xx_abort_iocb(srb_t *sp, struct ab
  	wmb();
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ qla2x00_mb_iocb(srb_t *sp, struct mbx_24xx_entry *mbx)
+ {
+ 	int i, sz;
+ 
+ 	mbx->entry_type = MBX_IOCB_TYPE;
+ 	mbx->handle = sp->handle;
+ 	sz = min(ARRAY_SIZE(mbx->mb), ARRAY_SIZE(sp->u.iocb_cmd.u.mbx.out_mb));
+ 
+ 	for (i = 0; i < sz; i++)
+ 		mbx->mb[i] = cpu_to_le16(sp->u.iocb_cmd.u.mbx.out_mb[i]);
+ }
+ 
+ static void
+ qla2x00_ctpthru_cmd_iocb(srb_t *sp, struct ct_entry_24xx *ct_pkt)
+ {
+ 	sp->u.iocb_cmd.u.ctarg.iocb = ct_pkt;
+ 	qla24xx_prep_ms_iocb(sp->vha, &sp->u.iocb_cmd.u.ctarg);
+ 	ct_pkt->handle = sp->handle;
+ }
+ 
+ static void qla2x00_send_notify_ack_iocb(srb_t *sp,
+ 	struct nack_to_isp *nack)
+ {
+ 	struct imm_ntfy_from_isp *ntfy = sp->u.iocb_cmd.u.nack.ntfy;
+ 
+ 	nack->entry_type = NOTIFY_ACK_TYPE;
+ 	nack->entry_count = 1;
+ 	nack->ox_id = ntfy->ox_id;
+ 
+ 	nack->u.isp24.handle = sp->handle;
+ 	nack->u.isp24.nport_handle = ntfy->u.isp24.nport_handle;
+ 	if (le16_to_cpu(ntfy->u.isp24.status) == IMM_NTFY_ELS) {
+ 		nack->u.isp24.flags = ntfy->u.isp24.flags &
+ 			cpu_to_le32(NOTIFY24XX_FLAGS_PUREX_IOCB);
+ 	}
+ 	nack->u.isp24.srr_rx_id = ntfy->u.isp24.srr_rx_id;
+ 	nack->u.isp24.status = ntfy->u.isp24.status;
+ 	nack->u.isp24.status_subcode = ntfy->u.isp24.status_subcode;
+ 	nack->u.isp24.fw_handle = ntfy->u.isp24.fw_handle;
+ 	nack->u.isp24.exchange_address = ntfy->u.isp24.exchange_address;
+ 	nack->u.isp24.srr_rel_offs = ntfy->u.isp24.srr_rel_offs;
+ 	nack->u.isp24.srr_ui = ntfy->u.isp24.srr_ui;
+ 	nack->u.isp24.srr_flags = 0;
+ 	nack->u.isp24.srr_reject_code = 0;
+ 	nack->u.isp24.srr_reject_code_expl = 0;
+ 	nack->u.isp24.vp_index = ntfy->u.isp24.vp_index;
+ }
+ 
+ /*
+  * Build NVME LS request
+  */
+ static int
+ qla_nvme_ls(srb_t *sp, struct pt_ls4_request *cmd_pkt)
+ {
+ 	struct srb_iocb *nvme;
+ 	int     rval = QLA_SUCCESS;
+ 
+ 	nvme = &sp->u.iocb_cmd;
+ 	cmd_pkt->entry_type = PT_LS4_REQUEST;
+ 	cmd_pkt->entry_count = 1;
+ 	cmd_pkt->control_flags = CF_LS4_ORIGINATOR << CF_LS4_SHIFT;
+ 
+ 	cmd_pkt->timeout = cpu_to_le16(nvme->u.nvme.timeout_sec);
+ 	cmd_pkt->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+ 	cmd_pkt->vp_index = sp->fcport->vha->vp_idx;
+ 
+ 	cmd_pkt->tx_dseg_count = 1;
+ 	cmd_pkt->tx_byte_count = nvme->u.nvme.cmd_len;
+ 	cmd_pkt->dseg0_len = nvme->u.nvme.cmd_len;
+ 	cmd_pkt->dseg0_address[0] = cpu_to_le32(LSD(nvme->u.nvme.cmd_dma));
+ 	cmd_pkt->dseg0_address[1] = cpu_to_le32(MSD(nvme->u.nvme.cmd_dma));
+ 
+ 	cmd_pkt->rx_dseg_count = 1;
+ 	cmd_pkt->rx_byte_count = nvme->u.nvme.rsp_len;
+ 	cmd_pkt->dseg1_len  = nvme->u.nvme.rsp_len;
+ 	cmd_pkt->dseg1_address[0] =  cpu_to_le32(LSD(nvme->u.nvme.rsp_dma));
+ 	cmd_pkt->dseg1_address[1] =  cpu_to_le32(MSD(nvme->u.nvme.rsp_dma));
+ 
+ 	return rval;
+ }
+ 
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  int
  qla2x00_start_sp(srb_t *sp)
  {
diff --cc drivers/scsi/qla2xxx/qla_isr.c
index 3d56972c1852,011faa1dc618..000000000000
--- a/drivers/scsi/qla2xxx/qla_isr.c
+++ b/drivers/scsi/qla2xxx/qla_isr.c
@@@ -2546,10 -2824,25 +2546,25 @@@ qla24xx_abort_iocb_entry(scsi_qla_host_
  		return;
  
  	abt = &sp->u.iocb_cmd;
 -	abt->u.abt.comp_status = le16_to_cpu(pkt->nport_handle);
 -	sp->done(sp, 0);
 +	abt->u.abt.comp_status = le32_to_cpu(pkt->nport_handle);
 +	sp->done(vha, sp, 0);
  }
  
+ void qla24xx_nvme_ls4_iocb(scsi_qla_host_t *vha, struct pt_ls4_request *pkt,
+     struct req_que *req)
+ {
+ 	srb_t *sp;
+ 	const char func[] = "LS4_IOCB";
+ 	uint16_t comp_status;
+ 
+ 	sp = qla2x00_get_sp_from_handle(vha, func, req, pkt);
+ 	if (!sp)
+ 		return;
+ 
+ 	comp_status = le16_to_cpu(pkt->status);
+ 	sp->done(sp, comp_status);
+ }
+ 
  /**
   * qla24xx_process_response_queue() - Process response queue entries.
   * @ha: SCSI driver HA context
@@@ -2614,12 -2902,31 +2629,22 @@@ void qla24xx_process_response_queue(str
  			qla24xx_els_ct_entry(vha, rsp->req, pkt, ELS_IOCB_TYPE);
  			break;
  		case ABTS_RECV_24XX:
 -			if (IS_QLA83XX(ha) || IS_QLA27XX(ha)) {
 -				/* ensure that the ATIO queue is empty */
 -				qlt_handle_abts_recv(vha, rsp,
 -				    (response_t *)pkt);
 -				break;
 -			} else {
 -				/* drop through */
 -				qlt_24xx_process_atio_queue(vha, 1);
 -			}
 +			/* ensure that the ATIO queue is empty */
 +			qlt_24xx_process_atio_queue(vha);
  		case ABTS_RESP_24XX:
  		case CTIO_TYPE7:
++<<<<<<< HEAD
++=======
+ 		case CTIO_CRC2:
+ 			qlt_response_pkt_all_vps(vha, rsp, (response_t *)pkt);
+ 			break;
+ 		case PT_LS4_REQUEST:
+ 			qla24xx_nvme_ls4_iocb(vha, (struct pt_ls4_request *)pkt,
+ 			    rsp->req);
+ 			break;
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  		case NOTIFY_ACK_TYPE:
 -			if (pkt->handle == QLA_TGT_SKIP_HANDLE)
 -				qlt_response_pkt_all_vps(vha, rsp,
 -				    (response_t *)pkt);
 -			else
 -				qla24xxx_nack_iocb_entry(vha, rsp->req,
 -					(struct nack_to_isp *)pkt);
 +			qlt_response_pkt_all_vps(vha, (response_t *)pkt);
  			break;
  		case MARKER_TYPE:
  			/* Do nothing in this case, this check is to prevent it
diff --cc drivers/scsi/qla2xxx/qla_mbx.c
index 11982e7bd50f,0764b6172ed1..000000000000
--- a/drivers/scsi/qla2xxx/qla_mbx.c
+++ b/drivers/scsi/qla2xxx/qla_mbx.c
@@@ -584,6 -602,16 +586,19 @@@ qla2x00_execute_fw(scsi_qla_host_t *vha
  			    EXTENDED_BB_CREDITS);
  		} else
  			mcp->mb[4] = 0;
++<<<<<<< HEAD
++=======
+ 
+ 		if (ql2xnvmeenable && IS_QLA27XX(ha))
+ 			mcp->mb[4] |= NVME_ENABLE_FLAG;
+ 
+ 		if (ha->flags.exlogins_enabled)
+ 			mcp->mb[4] |= ENABLE_EXTENDED_LOGIN;
+ 
+ 		if (ha->flags.exchoffld_enabled)
+ 			mcp->mb[4] |= ENABLE_EXCHANGE_OFFLD;
+ 
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  		mcp->out_mb |= MBX_4|MBX_3|MBX_2|MBX_1;
  		mcp->in_mb |= MBX_1;
  	} else {
@@@ -690,6 -936,32 +705,35 @@@ qla2x00_get_fw_version(scsi_qla_host_t 
  		ql_dbg(ql_dbg_mbx + ql_dbg_verbose, vha, 0x112f,
  		    "%s: Ext_FwAttributes Upper: 0x%x, Lower: 0x%x.\n",
  		    __func__, mcp->mb[17], mcp->mb[16]);
++<<<<<<< HEAD
++=======
+ 
+ 		if (ha->fw_attributes_h & 0x4)
+ 			ql_dbg(ql_dbg_mbx + ql_dbg_verbose, vha, 0x118d,
+ 			    "%s: Firmware supports Extended Login 0x%x\n",
+ 			    __func__, ha->fw_attributes_h);
+ 
+ 		if (ha->fw_attributes_h & 0x8)
+ 			ql_dbg(ql_dbg_mbx + ql_dbg_verbose, vha, 0x1191,
+ 			    "%s: Firmware supports Exchange Offload 0x%x\n",
+ 			    __func__, ha->fw_attributes_h);
+ 
+ 		/* bit 26 of fw_attributes */
+ 		if ((ha->fw_attributes_h & 0x400) && ql2xnvmeenable) {
+ 			struct init_cb_24xx *icb;
+ 
+ 			icb = (struct init_cb_24xx *)ha->init_cb;
+ 			/*
+ 			 * fw supports nvme and driver load
+ 			 * parameter requested nvme
+ 			 */
+ 			vha->flags.nvme_enabled = 1;
+ 			icb->firmware_options_2 &= cpu_to_le32(~0xf);
+ 			ha->zio_mode = 0;
+ 			ha->zio_timer = 0;
+ 		}
+ 
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  	}
  
  	if (IS_QLA27XX(ha)) {
diff --cc drivers/scsi/qla2xxx/qla_os.c
index c724f888aca6,df57655779ed..000000000000
--- a/drivers/scsi/qla2xxx/qla_os.c
+++ b/drivers/scsi/qla2xxx/qla_os.c
@@@ -117,9 -118,13 +117,13 @@@ static int ql2xmaxqdepth = MAX_Q_DEPTH
  module_param(ql2xmaxqdepth, int, S_IRUGO|S_IWUSR);
  MODULE_PARM_DESC(ql2xmaxqdepth,
  		"Maximum queue depth to set for each LUN. "
 -		"Default is 32.");
 +		"Default is 64.");
  
+ #if (IS_ENABLED(CONFIG_NVME_FC))
+ int ql2xenabledif;
+ #else
  int ql2xenabledif = 2;
+ #endif
  module_param(ql2xenabledif, int, S_IRUGO);
  MODULE_PARM_DESC(ql2xenabledif,
  		" Enable T10-CRC-DIF:\n"
@@@ -244,6 -280,8 +258,11 @@@ static int qla2x00_change_queue_type(st
  static void qla2x00_clear_drv_active(struct qla_hw_data *);
  static void qla2x00_free_device(scsi_qla_host_t *);
  static void qla83xx_disable_laser(scsi_qla_host_t *vha);
++<<<<<<< HEAD
++=======
+ static int qla2xxx_map_queues(struct Scsi_Host *shost);
+ static void qla2x00_destroy_deferred_work(struct qla_hw_data *);
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  
  struct scsi_host_template qla2xxx_driver_template = {
  	.module			= THIS_MODULE,
@@@ -584,7 -664,83 +603,87 @@@ qla24xx_fw_version_str(struct scsi_qla_
  }
  
  void
++<<<<<<< HEAD
 +qla2x00_sp_free_dma(void *vha, void *ptr)
++=======
+ qla2x00_sp_free_dma(void *ptr)
+ {
+ 	srb_t *sp = ptr;
+ 	struct qla_hw_data *ha = sp->vha->hw;
+ 	struct scsi_cmnd *cmd = GET_CMD_SP(sp);
+ 	void *ctx = GET_CMD_CTX_SP(sp);
+ 
+ 	if (sp->flags & SRB_DMA_VALID) {
+ 		scsi_dma_unmap(cmd);
+ 		sp->flags &= ~SRB_DMA_VALID;
+ 	}
+ 
+ 	if (sp->flags & SRB_CRC_PROT_DMA_VALID) {
+ 		dma_unmap_sg(&ha->pdev->dev, scsi_prot_sglist(cmd),
+ 		    scsi_prot_sg_count(cmd), cmd->sc_data_direction);
+ 		sp->flags &= ~SRB_CRC_PROT_DMA_VALID;
+ 	}
+ 
+ 	if (!ctx)
+ 		goto end;
+ 
+ 	if (sp->flags & SRB_CRC_CTX_DSD_VALID) {
+ 		/* List assured to be having elements */
+ 		qla2x00_clean_dsd_pool(ha, ctx);
+ 		sp->flags &= ~SRB_CRC_CTX_DSD_VALID;
+ 	}
+ 
+ 	if (sp->flags & SRB_CRC_CTX_DMA_VALID) {
+ 		struct crc_context *ctx0 = ctx;
+ 
+ 		dma_pool_free(ha->dl_dma_pool, ctx0, ctx0->crc_ctx_dma);
+ 		sp->flags &= ~SRB_CRC_CTX_DMA_VALID;
+ 	}
+ 
+ 	if (sp->flags & SRB_FCP_CMND_DMA_VALID) {
+ 		struct ct6_dsd *ctx1 = ctx;
+ 
+ 		dma_pool_free(ha->fcp_cmnd_dma_pool, ctx1->fcp_cmnd,
+ 		    ctx1->fcp_cmnd_dma);
+ 		list_splice(&ctx1->dsd_list, &ha->gbl_dsd_list);
+ 		ha->gbl_dsd_inuse -= ctx1->dsd_use_cnt;
+ 		ha->gbl_dsd_avail += ctx1->dsd_use_cnt;
+ 		mempool_free(ctx1, ha->ctx_mempool);
+ 	}
+ 
+ end:
+ 	if ((sp->type != SRB_NVME_CMD) && (sp->type != SRB_NVME_LS)) {
+ 		CMD_SP(cmd) = NULL;
+ 		qla2x00_rel_sp(sp);
+ 	}
+ }
+ 
+ void
+ qla2x00_sp_compl(void *ptr, int res)
+ {
+ 	srb_t *sp = ptr;
+ 	struct scsi_cmnd *cmd = GET_CMD_SP(sp);
+ 
+ 	cmd->result = res;
+ 
+ 	if (atomic_read(&sp->ref_count) == 0) {
+ 		ql_dbg(ql_dbg_io, sp->vha, 0x3015,
+ 		    "SP reference-count to ZERO -- sp=%p cmd=%p.\n",
+ 		    sp, GET_CMD_SP(sp));
+ 		if (ql2xextended_error_logging & ql_dbg_io)
+ 			WARN_ON(atomic_read(&sp->ref_count) == 0);
+ 		return;
+ 	}
+ 	if (!atomic_dec_and_test(&sp->ref_count))
+ 		return;
+ 
+ 	qla2x00_sp_free_dma(sp);
+ 	cmd->scsi_done(cmd);
+ }
+ 
+ void
+ qla2xxx_qpair_sp_free_dma(void *ptr)
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  {
  	srb_t *sp = (srb_t *)ptr;
  	struct scsi_cmnd *cmd = GET_CMD_SP(sp);
@@@ -1590,27 -1713,65 +1689,88 @@@ qla2x00_abort_all_cmds(scsi_qla_host_t 
  		for (cnt = 1; cnt < req->num_outstanding_cmds; cnt++) {
  			sp = req->outstanding_cmds[cnt];
  			if (sp) {
++<<<<<<< HEAD
 +				/* Don't abort commands in adapter during EEH
 +				 * recovery as it's not accessible/responding.
 +				 */
 +				if (GET_CMD_SP(sp) && !ha->flags.eeh_busy &&
 +				    (sp->type == SRB_SCSI_CMD)) {
 +					/* Get a reference to the sp and drop the lock.
 +					 * The reference ensures this sp->done() call
 +					 * - and not the call in qla2xxx_eh_abort() -
 +					 * ends the SCSI command (with result 'res').
 +					 */
 +					sp_get(sp);
 +					spin_unlock_irqrestore(&ha->hardware_lock, flags);
 +					status = qla2xxx_eh_abort(GET_CMD_SP(sp));
 +					spin_lock_irqsave(&ha->hardware_lock, flags);
 +					/* Get rid of extra reference if immediate exit
 +					 * from ql2xxx_eh_abort */
 +					if (status == FAILED && (qla2x00_isp_reg_stat(ha)))
 +						atomic_dec(&sp->ref_count);
++=======
+ 				req->outstanding_cmds[cnt] = NULL;
+ 				if (sp->cmd_type == TYPE_SRB) {
+ 					if ((sp->type == SRB_NVME_CMD) ||
+ 					    (sp->type == SRB_NVME_LS)) {
+ 						sp_get(sp);
+ 						spin_unlock_irqrestore(
+ 						    &ha->hardware_lock, flags);
+ 						qla_nvme_abort(ha, sp);
+ 						spin_lock_irqsave(
+ 						    &ha->hardware_lock, flags);
+ 					} else if (GET_CMD_SP(sp) &&
+ 					    !ha->flags.eeh_busy &&
+ 					    (sp->type == SRB_SCSI_CMD)) {
+ 						/*
+ 						 * Don't abort commands in
+ 						 * adapter during EEH
+ 						 * recovery as it's not
+ 						 * accessible/responding.
+ 						 *
+ 						 * Get a reference to the sp
+ 						 * and drop the lock. The
+ 						 * reference ensures this
+ 						 * sp->done() call and not the
+ 						 * call in qla2xxx_eh_abort()
+ 						 * ends the SCSI command (with
+ 						 * result 'res').
+ 						 */
+ 						sp_get(sp);
+ 						spin_unlock_irqrestore(
+ 						    &ha->hardware_lock, flags);
+ 						status = qla2xxx_eh_abort(
+ 						    GET_CMD_SP(sp));
+ 						spin_lock_irqsave(
+ 						    &ha->hardware_lock, flags);
+ 						/*
+ 						 * Get rid of extra reference
+ 						 * if immediate exit from
+ 						 * ql2xxx_eh_abort
+ 						 */
+ 						if (status == FAILED &&
+ 						    (qla2x00_isp_reg_stat(ha)))
+ 							atomic_dec(
+ 							    &sp->ref_count);
+ 					}
+ 					sp->done(sp, res);
+ 				} else {
+ 					if (!vha->hw->tgt.tgt_ops || !tgt ||
+ 					    qla_ini_mode_enabled(vha)) {
+ 						if (!trace)
+ 							ql_dbg(ql_dbg_tgt_mgt,
+ 							    vha, 0xf003,
+ 							    "HOST-ABORT-HNDLR: dpc_flags=%lx. Target mode disabled\n",
+ 							    vha->dpc_flags);
+ 						continue;
+ 					}
+ 					cmd = (struct qla_tgt_cmd *)sp;
+ 					qlt_abort_cmd_on_host_reset(cmd->vha,
+ 					    cmd);
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  				}
 +				req->outstanding_cmds[cnt] = NULL;
 +				sp->done(vha, sp, res);
  			}
  		}
  	}
@@@ -3446,6 -3557,11 +3606,14 @@@ qla2x00_remove_one(struct pci_dev *pdev
  		return;
  
  	set_bit(UNLOADING, &base_vha->dpc_flags);
++<<<<<<< HEAD
++=======
+ 
+ 	qla_nvme_delete(base_vha);
+ 
+ 	dma_free_coherent(&ha->pdev->dev,
+ 		base_vha->gnl.size, base_vha->gnl.l, base_vha->gnl.ldma);
++>>>>>>> e84067d74301 (scsi: qla2xxx: Add FC-NVMe F/W initialization and transport registration)
  
  	if (IS_QLAFX00(ha))
  		qlafx00_driver_shutdown(base_vha, 20);
diff --git a/drivers/scsi/qla2xxx/Makefile b/drivers/scsi/qla2xxx/Makefile
index 44def6bb4bb0..0b767a0bb308 100644
--- a/drivers/scsi/qla2xxx/Makefile
+++ b/drivers/scsi/qla2xxx/Makefile
@@ -1,6 +1,6 @@
 qla2xxx-y := qla_os.o qla_init.o qla_mbx.o qla_iocb.o qla_isr.o qla_gs.o \
 		qla_dbg.o qla_sup.o qla_attr.o qla_mid.o qla_dfs.o qla_bsg.o \
-		qla_nx.o qla_mr.o qla_nx2.o qla_target.o qla_tmpl.o
+		qla_nx.o qla_mr.o qla_nx2.o qla_target.o qla_tmpl.o qla_nvme.o
 
 obj-$(CONFIG_SCSI_QLA_FC) += qla2xxx.o
 obj-$(CONFIG_TCM_QLA2XXX) += tcm_qla2xxx.o
* Unmerged path drivers/scsi/qla2xxx/qla_dbg.c
* Unmerged path drivers/scsi/qla2xxx/qla_def.h
* Unmerged path drivers/scsi/qla2xxx/qla_gbl.h
* Unmerged path drivers/scsi/qla2xxx/qla_init.c
* Unmerged path drivers/scsi/qla2xxx/qla_iocb.c
* Unmerged path drivers/scsi/qla2xxx/qla_isr.c
* Unmerged path drivers/scsi/qla2xxx/qla_mbx.c
diff --git a/drivers/scsi/qla2xxx/qla_nvme.c b/drivers/scsi/qla2xxx/qla_nvme.c
new file mode 100644
index 000000000000..1da8fa8f641d
--- /dev/null
+++ b/drivers/scsi/qla2xxx/qla_nvme.c
@@ -0,0 +1,756 @@
+/*
+ * QLogic Fibre Channel HBA Driver
+ * Copyright (c)  2003-2017 QLogic Corporation
+ *
+ * See LICENSE.qla2xxx for copyright and licensing details.
+ */
+#include "qla_nvme.h"
+#include "qla_def.h"
+#include <linux/scatterlist.h>
+#include <linux/delay.h>
+#include <linux/nvme.h>
+#include <linux/nvme-fc.h>
+
+static struct nvme_fc_port_template qla_nvme_fc_transport;
+
+static void qla_nvme_unregister_remote_port(struct work_struct *);
+
+int qla_nvme_register_remote(scsi_qla_host_t *vha, fc_port_t *fcport)
+{
+#if (IS_ENABLED(CONFIG_NVME_FC))
+	struct nvme_rport *rport;
+	int ret;
+
+	if (fcport->nvme_flag & NVME_FLAG_REGISTERED)
+		return 0;
+
+	if (!vha->flags.nvme_enabled) {
+		ql_log(ql_log_info, vha, 0x2100,
+		    "%s: Not registering target since Host NVME is not enabled\n",
+		    __func__);
+		return 0;
+	}
+
+	if (!(fcport->nvme_prli_service_param &
+	    (NVME_PRLI_SP_TARGET | NVME_PRLI_SP_DISCOVERY)))
+		return 0;
+
+	INIT_WORK(&fcport->nvme_del_work, qla_nvme_unregister_remote_port);
+	rport = kzalloc(sizeof(*rport), GFP_KERNEL);
+	if (!rport) {
+		ql_log(ql_log_warn, vha, 0x2101,
+		    "%s: unable to alloc memory\n", __func__);
+		return -ENOMEM;
+	}
+
+	rport->req.port_name = wwn_to_u64(fcport->port_name);
+	rport->req.node_name = wwn_to_u64(fcport->node_name);
+	rport->req.port_role = 0;
+
+	if (fcport->nvme_prli_service_param & NVME_PRLI_SP_INITIATOR)
+		rport->req.port_role = FC_PORT_ROLE_NVME_INITIATOR;
+
+	if (fcport->nvme_prli_service_param & NVME_PRLI_SP_TARGET)
+		rport->req.port_role |= FC_PORT_ROLE_NVME_TARGET;
+
+	if (fcport->nvme_prli_service_param & NVME_PRLI_SP_DISCOVERY)
+		rport->req.port_role |= FC_PORT_ROLE_NVME_DISCOVERY;
+
+	rport->req.port_id = fcport->d_id.b24;
+
+	ql_log(ql_log_info, vha, 0x2102,
+	    "%s: traddr=pn-0x%016llx:nn-0x%016llx PortID:%06x\n",
+	    __func__, rport->req.port_name, rport->req.node_name,
+	    rport->req.port_id);
+
+	ret = nvme_fc_register_remoteport(vha->nvme_local_port, &rport->req,
+	    &fcport->nvme_remote_port);
+	if (ret) {
+		ql_log(ql_log_warn, vha, 0x212e,
+		    "Failed to register remote port. Transport returned %d\n",
+		    ret);
+		return ret;
+	}
+
+	fcport->nvme_remote_port->private = fcport;
+	fcport->nvme_flag |= NVME_FLAG_REGISTERED;
+	atomic_set(&fcport->nvme_ref_count, 1);
+	init_waitqueue_head(&fcport->nvme_waitQ);
+	rport->fcport = fcport;
+	list_add_tail(&rport->list, &vha->nvme_rport_list);
+#endif
+	return 0;
+}
+
+/* Allocate a queue for NVMe traffic */
+static int qla_nvme_alloc_queue(struct nvme_fc_local_port *lport, unsigned int qidx,
+    u16 qsize, void **handle)
+{
+	struct scsi_qla_host *vha;
+	struct qla_hw_data *ha;
+	struct qla_qpair *qpair;
+
+	if (!qidx)
+		qidx++;
+
+	vha = (struct scsi_qla_host *)lport->private;
+	ha = vha->hw;
+
+	ql_log(ql_log_info, vha, 0x2104,
+	    "%s: handle %p, idx =%d, qsize %d\n",
+	    __func__, handle, qidx, qsize);
+
+	if (qidx > qla_nvme_fc_transport.max_hw_queues) {
+		ql_log(ql_log_warn, vha, 0x212f,
+		    "%s: Illegal qidx=%d. Max=%d\n",
+		    __func__, qidx, qla_nvme_fc_transport.max_hw_queues);
+		return -EINVAL;
+	}
+
+	if (ha->queue_pair_map[qidx]) {
+		*handle = ha->queue_pair_map[qidx];
+		ql_log(ql_log_info, vha, 0x2121,
+		    "Returning existing qpair of %p for idx=%x\n",
+		    *handle, qidx);
+		return 0;
+	}
+
+	ql_log(ql_log_warn, vha, 0xffff,
+	    "allocating q for idx=%x w/o cpu mask\n", qidx);
+	qpair = qla2xxx_create_qpair(vha, 5, vha->vp_idx, true);
+	if (qpair == NULL) {
+		ql_log(ql_log_warn, vha, 0x2122,
+		    "Failed to allocate qpair\n");
+		return -EINVAL;
+	}
+	*handle = qpair;
+
+	return 0;
+}
+
+static void qla_nvme_sp_ls_done(void *ptr, int res)
+{
+	srb_t *sp = ptr;
+	struct srb_iocb *nvme;
+	struct nvmefc_ls_req   *fd;
+	struct nvme_private *priv;
+
+	if (atomic_read(&sp->ref_count) == 0) {
+		ql_log(ql_log_warn, sp->fcport->vha, 0x2123,
+		    "SP reference-count to ZERO on LS_done -- sp=%p.\n", sp);
+		return;
+	}
+
+	if (!atomic_dec_and_test(&sp->ref_count))
+		return;
+
+	if (res)
+		res = -EINVAL;
+
+	nvme = &sp->u.iocb_cmd;
+	fd = nvme->u.nvme.desc;
+	priv = fd->private;
+	priv->comp_status = res;
+	schedule_work(&priv->ls_work);
+	/* work schedule doesn't need the sp */
+	qla2x00_rel_sp(sp);
+}
+
+static void qla_nvme_sp_done(void *ptr, int res)
+{
+	srb_t *sp = ptr;
+	struct srb_iocb *nvme;
+	struct nvmefc_fcp_req *fd;
+
+	nvme = &sp->u.iocb_cmd;
+	fd = nvme->u.nvme.desc;
+
+	if (!atomic_dec_and_test(&sp->ref_count))
+		return;
+
+	if (!(sp->fcport->nvme_flag & NVME_FLAG_REGISTERED))
+		goto rel;
+
+	if (unlikely(nvme->u.nvme.comp_status || res))
+		fd->status = -EINVAL;
+	else
+		fd->status = 0;
+
+	fd->rcv_rsplen = nvme->u.nvme.rsp_pyld_len;
+	fd->done(fd);
+rel:
+	qla2xxx_rel_qpair_sp(sp->qpair, sp);
+}
+
+static void qla_nvme_ls_abort(struct nvme_fc_local_port *lport,
+    struct nvme_fc_remote_port *rport, struct nvmefc_ls_req *fd)
+{
+	struct nvme_private *priv = fd->private;
+	fc_port_t *fcport = rport->private;
+	srb_t *sp = priv->sp;
+	int rval;
+	struct qla_hw_data *ha = fcport->vha->hw;
+
+	rval = ha->isp_ops->abort_command(sp);
+	if (rval != QLA_SUCCESS)
+		ql_log(ql_log_warn, fcport->vha, 0x2125,
+		    "%s: failed to abort LS command for SP:%p rval=%x\n",
+		    __func__, sp, rval);
+
+	ql_dbg(ql_dbg_io, fcport->vha, 0x212b,
+	    "%s: aborted sp:%p on fcport:%p\n", __func__, sp, fcport);
+}
+
+static void qla_nvme_ls_complete(struct work_struct *work)
+{
+	struct nvme_private *priv =
+	    container_of(work, struct nvme_private, ls_work);
+	struct nvmefc_ls_req *fd = priv->fd;
+
+	fd->done(fd, priv->comp_status);
+}
+
+static int qla_nvme_ls_req(struct nvme_fc_local_port *lport,
+    struct nvme_fc_remote_port *rport, struct nvmefc_ls_req *fd)
+{
+	fc_port_t *fcport = (fc_port_t *)rport->private;
+	struct srb_iocb   *nvme;
+	struct nvme_private *priv = fd->private;
+	struct scsi_qla_host *vha;
+	int     rval = QLA_FUNCTION_FAILED;
+	struct qla_hw_data *ha;
+	srb_t           *sp;
+
+	if (!(fcport->nvme_flag & NVME_FLAG_REGISTERED))
+		return rval;
+
+	vha = fcport->vha;
+	ha = vha->hw;
+	/* Alloc SRB structure */
+	sp = qla2x00_get_sp(vha, fcport, GFP_ATOMIC);
+	if (!sp)
+		return rval;
+
+	sp->type = SRB_NVME_LS;
+	sp->name = "nvme_ls";
+	sp->done = qla_nvme_sp_ls_done;
+	atomic_set(&sp->ref_count, 1);
+	init_waitqueue_head(&sp->nvme_ls_waitQ);
+	nvme = &sp->u.iocb_cmd;
+	priv->sp = sp;
+	priv->fd = fd;
+	INIT_WORK(&priv->ls_work, qla_nvme_ls_complete);
+	nvme->u.nvme.desc = fd;
+	nvme->u.nvme.dir = 0;
+	nvme->u.nvme.dl = 0;
+	nvme->u.nvme.cmd_len = fd->rqstlen;
+	nvme->u.nvme.rsp_len = fd->rsplen;
+	nvme->u.nvme.rsp_dma = fd->rspdma;
+	nvme->u.nvme.timeout_sec = fd->timeout;
+	nvme->u.nvme.cmd_dma = dma_map_single(&ha->pdev->dev, fd->rqstaddr,
+	    fd->rqstlen, DMA_TO_DEVICE);
+	dma_sync_single_for_device(&ha->pdev->dev, nvme->u.nvme.cmd_dma,
+	    fd->rqstlen, DMA_TO_DEVICE);
+
+	rval = qla2x00_start_sp(sp);
+	if (rval != QLA_SUCCESS) {
+		ql_log(ql_log_warn, vha, 0x700e,
+		    "qla2x00_start_sp failed = %d\n", rval);
+		atomic_dec(&sp->ref_count);
+		wake_up(&sp->nvme_ls_waitQ);
+		return rval;
+	}
+
+	return rval;
+}
+
+static void qla_nvme_fcp_abort(struct nvme_fc_local_port *lport,
+    struct nvme_fc_remote_port *rport, void *hw_queue_handle,
+    struct nvmefc_fcp_req *fd)
+{
+	struct nvme_private *priv = fd->private;
+	srb_t *sp = priv->sp;
+	int rval;
+	fc_port_t *fcport = rport->private;
+	struct qla_hw_data *ha = fcport->vha->hw;
+
+	rval = ha->isp_ops->abort_command(sp);
+	if (!rval)
+		ql_log(ql_log_warn, fcport->vha, 0x2127,
+		    "%s: failed to abort command for SP:%p rval=%x\n",
+		    __func__, sp, rval);
+
+	ql_dbg(ql_dbg_io, fcport->vha, 0x2126,
+	    "%s: aborted sp:%p on fcport:%p\n", __func__, sp, fcport);
+}
+
+static void qla_nvme_poll(struct nvme_fc_local_port *lport, void *hw_queue_handle)
+{
+	struct scsi_qla_host *vha = lport->private;
+	unsigned long flags;
+	struct qla_qpair *qpair = (struct qla_qpair *)hw_queue_handle;
+
+	/* Acquire ring specific lock */
+	spin_lock_irqsave(&qpair->qp_lock, flags);
+	qla24xx_process_response_queue(vha, qpair->rsp);
+	spin_unlock_irqrestore(&qpair->qp_lock, flags);
+}
+
+static int qla2x00_start_nvme_mq(srb_t *sp)
+{
+	unsigned long   flags;
+	uint32_t        *clr_ptr;
+	uint32_t        index;
+	uint32_t        handle;
+	struct cmd_nvme *cmd_pkt;
+	uint16_t        cnt, i;
+	uint16_t        req_cnt;
+	uint16_t        tot_dsds;
+	uint16_t	avail_dsds;
+	uint32_t	*cur_dsd;
+	struct req_que *req = NULL;
+	struct scsi_qla_host *vha = sp->fcport->vha;
+	struct qla_hw_data *ha = vha->hw;
+	struct qla_qpair *qpair = sp->qpair;
+	struct srb_iocb *nvme = &sp->u.iocb_cmd;
+	struct scatterlist *sgl, *sg;
+	struct nvmefc_fcp_req *fd = nvme->u.nvme.desc;
+	uint32_t        rval = QLA_SUCCESS;
+
+	/* Setup qpair pointers */
+	req = qpair->req;
+	tot_dsds = fd->sg_cnt;
+
+	/* Acquire qpair specific lock */
+	spin_lock_irqsave(&qpair->qp_lock, flags);
+
+	/* Check for room in outstanding command list. */
+	handle = req->current_outstanding_cmd;
+	for (index = 1; index < req->num_outstanding_cmds; index++) {
+		handle++;
+		if (handle == req->num_outstanding_cmds)
+			handle = 1;
+		if (!req->outstanding_cmds[handle])
+			break;
+	}
+
+	if (index == req->num_outstanding_cmds) {
+		rval = -1;
+		goto queuing_error;
+	}
+	req_cnt = qla24xx_calc_iocbs(vha, tot_dsds);
+	if (req->cnt < (req_cnt + 2)) {
+		cnt = IS_SHADOW_REG_CAPABLE(ha) ? *req->out_ptr :
+		    RD_REG_DWORD_RELAXED(req->req_q_out);
+
+		if (req->ring_index < cnt)
+			req->cnt = cnt - req->ring_index;
+		else
+			req->cnt = req->length - (req->ring_index - cnt);
+
+		if (req->cnt < (req_cnt + 2)){
+			rval = -1;
+			goto queuing_error;
+		}
+	}
+
+	if (unlikely(!fd->sqid)) {
+		struct nvme_fc_cmd_iu *cmd = fd->cmdaddr;
+		if (cmd->sqe.common.opcode == nvme_admin_async_event) {
+			nvme->u.nvme.aen_op = 1;
+			atomic_inc(&vha->nvme_active_aen_cnt);
+		}
+	}
+
+	/* Build command packet. */
+	req->current_outstanding_cmd = handle;
+	req->outstanding_cmds[handle] = sp;
+	sp->handle = handle;
+	req->cnt -= req_cnt;
+
+	cmd_pkt = (struct cmd_nvme *)req->ring_ptr;
+	cmd_pkt->handle = MAKE_HANDLE(req->id, handle);
+
+	/* Zero out remaining portion of packet. */
+	clr_ptr = (uint32_t *)cmd_pkt + 2;
+	memset(clr_ptr, 0, REQUEST_ENTRY_SIZE - 8);
+
+	cmd_pkt->entry_status = 0;
+
+	/* Update entry type to indicate Command NVME IOCB */
+	cmd_pkt->entry_type = COMMAND_NVME;
+
+	/* No data transfer how do we check buffer len == 0?? */
+	if (fd->io_dir == NVMEFC_FCP_READ) {
+		cmd_pkt->control_flags =
+		    cpu_to_le16(CF_READ_DATA | CF_NVME_ENABLE);
+		vha->qla_stats.input_bytes += fd->payload_length;
+		vha->qla_stats.input_requests++;
+	} else if (fd->io_dir == NVMEFC_FCP_WRITE) {
+		cmd_pkt->control_flags =
+		    cpu_to_le16(CF_WRITE_DATA | CF_NVME_ENABLE);
+		vha->qla_stats.output_bytes += fd->payload_length;
+		vha->qla_stats.output_requests++;
+	} else if (fd->io_dir == 0) {
+		cmd_pkt->control_flags = cpu_to_le16(CF_NVME_ENABLE);
+	}
+
+	/* Set NPORT-ID */
+	cmd_pkt->nport_handle = cpu_to_le16(sp->fcport->loop_id);
+	cmd_pkt->port_id[0] = sp->fcport->d_id.b.al_pa;
+	cmd_pkt->port_id[1] = sp->fcport->d_id.b.area;
+	cmd_pkt->port_id[2] = sp->fcport->d_id.b.domain;
+	cmd_pkt->vp_index = sp->fcport->vha->vp_idx;
+
+	/* NVME RSP IU */
+	cmd_pkt->nvme_rsp_dsd_len = cpu_to_le16(fd->rsplen);
+	cmd_pkt->nvme_rsp_dseg_address[0] = cpu_to_le32(LSD(fd->rspdma));
+	cmd_pkt->nvme_rsp_dseg_address[1] = cpu_to_le32(MSD(fd->rspdma));
+
+	/* NVME CNMD IU */
+	cmd_pkt->nvme_cmnd_dseg_len = cpu_to_le16(fd->cmdlen);
+	cmd_pkt->nvme_cmnd_dseg_address[0] = cpu_to_le32(LSD(fd->cmddma));
+	cmd_pkt->nvme_cmnd_dseg_address[1] = cpu_to_le32(MSD(fd->cmddma));
+
+	cmd_pkt->dseg_count = cpu_to_le16(tot_dsds);
+	cmd_pkt->byte_count = cpu_to_le32(fd->payload_length);
+
+	/* One DSD is available in the Command Type NVME IOCB */
+	avail_dsds = 1;
+	cur_dsd = (uint32_t *)&cmd_pkt->nvme_data_dseg_address[0];
+	sgl = fd->first_sgl;
+
+	/* Load data segments */
+	for_each_sg(sgl, sg, tot_dsds, i) {
+		dma_addr_t      sle_dma;
+		cont_a64_entry_t *cont_pkt;
+
+		/* Allocate additional continuation packets? */
+		if (avail_dsds == 0) {
+			/*
+			 * Five DSDs are available in the Continuation
+			 * Type 1 IOCB.
+			 */
+
+			/* Adjust ring index */
+			req->ring_index++;
+			if (req->ring_index == req->length) {
+				req->ring_index = 0;
+				req->ring_ptr = req->ring;
+			} else {
+				req->ring_ptr++;
+			}
+			cont_pkt = (cont_a64_entry_t *)req->ring_ptr;
+			cont_pkt->entry_type = cpu_to_le32(CONTINUE_A64_TYPE);
+
+			cur_dsd = (uint32_t *)cont_pkt->dseg_0_address;
+			avail_dsds = 5;
+		}
+
+		sle_dma = sg_dma_address(sg);
+		*cur_dsd++ = cpu_to_le32(LSD(sle_dma));
+		*cur_dsd++ = cpu_to_le32(MSD(sle_dma));
+		*cur_dsd++ = cpu_to_le32(sg_dma_len(sg));
+		avail_dsds--;
+	}
+
+	/* Set total entry count. */
+	cmd_pkt->entry_count = (uint8_t)req_cnt;
+	wmb();
+
+	/* Adjust ring index. */
+	req->ring_index++;
+	if (req->ring_index == req->length) {
+		req->ring_index = 0;
+		req->ring_ptr = req->ring;
+	} else {
+		req->ring_ptr++;
+	}
+
+	/* Set chip new ring index. */
+	WRT_REG_DWORD(req->req_q_in, req->ring_index);
+
+queuing_error:
+	spin_unlock_irqrestore(&qpair->qp_lock, flags);
+	return rval;
+}
+
+/* Post a command */
+static int qla_nvme_post_cmd(struct nvme_fc_local_port *lport,
+    struct nvme_fc_remote_port *rport, void *hw_queue_handle,
+    struct nvmefc_fcp_req *fd)
+{
+	fc_port_t *fcport;
+	struct srb_iocb *nvme;
+	struct scsi_qla_host *vha;
+	int rval = QLA_FUNCTION_FAILED;
+	srb_t *sp;
+	struct qla_qpair *qpair = (struct qla_qpair *)hw_queue_handle;
+	struct nvme_private *priv;
+
+	if (!fd) {
+		ql_log(ql_log_warn, NULL, 0x2134, "NO NVMe FCP reqeust\n");
+		return rval;
+	}
+
+	priv = fd->private;
+	fcport = (fc_port_t *)rport->private;
+	if (!fcport) {
+		ql_log(ql_log_warn, NULL, 0x210e, "No fcport ptr\n");
+		return rval;
+	}
+
+	vha = fcport->vha;
+	if ((!qpair) || (!(fcport->nvme_flag & NVME_FLAG_REGISTERED)))
+		return -EBUSY;
+
+	/* Alloc SRB structure */
+	sp = qla2xxx_get_qpair_sp(qpair, fcport, GFP_ATOMIC);
+	if (!sp)
+		return -EIO;
+
+	atomic_set(&sp->ref_count, 1);
+	init_waitqueue_head(&sp->nvme_ls_waitQ);
+	priv->sp = sp;
+	sp->type = SRB_NVME_CMD;
+	sp->name = "nvme_cmd";
+	sp->done = qla_nvme_sp_done;
+	sp->qpair = qpair;
+	nvme = &sp->u.iocb_cmd;
+	nvme->u.nvme.desc = fd;
+
+	rval = qla2x00_start_nvme_mq(sp);
+	if (rval != QLA_SUCCESS) {
+		ql_log(ql_log_warn, vha, 0x212d,
+		    "qla2x00_start_nvme_mq failed = %d\n", rval);
+		atomic_dec(&sp->ref_count);
+		wake_up(&sp->nvme_ls_waitQ);
+		return -EIO;
+	}
+
+	return rval;
+}
+
+static void qla_nvme_localport_delete(struct nvme_fc_local_port *lport)
+{
+	struct scsi_qla_host *vha = lport->private;
+
+	atomic_dec(&vha->nvme_ref_count);
+	wake_up_all(&vha->nvme_waitQ);
+
+	ql_log(ql_log_info, vha, 0x210f,
+	    "localport delete of %p completed.\n", vha->nvme_local_port);
+	vha->nvme_local_port = NULL;
+}
+
+static void qla_nvme_remoteport_delete(struct nvme_fc_remote_port *rport)
+{
+	fc_port_t *fcport;
+	struct nvme_rport *r_port, *trport;
+
+	fcport = (fc_port_t *)rport->private;
+	fcport->nvme_remote_port = NULL;
+	fcport->nvme_flag &= ~NVME_FLAG_REGISTERED;
+	atomic_dec(&fcport->nvme_ref_count);
+	wake_up_all(&fcport->nvme_waitQ);
+
+	list_for_each_entry_safe(r_port, trport,
+	    &fcport->vha->nvme_rport_list, list) {
+		if (r_port->fcport == fcport) {
+			list_del(&r_port->list);
+			break;
+		}
+	}
+	kfree(r_port);
+
+	ql_log(ql_log_info, fcport->vha, 0x2110,
+	    "remoteport_delete of %p completed.\n", fcport);
+}
+
+static struct nvme_fc_port_template qla_nvme_fc_transport = {
+	.localport_delete = qla_nvme_localport_delete,
+	.remoteport_delete = qla_nvme_remoteport_delete,
+	.create_queue   = qla_nvme_alloc_queue,
+	.delete_queue 	= NULL,
+	.ls_req		= qla_nvme_ls_req,
+	.ls_abort	= qla_nvme_ls_abort,
+	.fcp_io		= qla_nvme_post_cmd,
+	.fcp_abort	= qla_nvme_fcp_abort,
+	.poll_queue	= qla_nvme_poll,
+	.max_hw_queues  = 8,
+	.max_sgl_segments = 128,
+	.max_dif_sgl_segments = 64,
+	.dma_boundary = 0xFFFFFFFF,
+	.local_priv_sz  = 8,
+	.remote_priv_sz = 0,
+	.lsrqst_priv_sz = sizeof(struct nvme_private),
+	.fcprqst_priv_sz = sizeof(struct nvme_private),
+};
+
+#define NVME_ABORT_POLLING_PERIOD    2
+static int qla_nvme_wait_on_command(srb_t *sp)
+{
+	int ret = QLA_SUCCESS;
+
+	wait_event_timeout(sp->nvme_ls_waitQ, (atomic_read(&sp->ref_count) > 1),
+	    NVME_ABORT_POLLING_PERIOD*HZ);
+
+	if (atomic_read(&sp->ref_count) > 1)
+		ret = QLA_FUNCTION_FAILED;
+
+	return ret;
+}
+
+static int qla_nvme_wait_on_rport_del(fc_port_t *fcport)
+{
+	int ret = QLA_SUCCESS;
+
+	wait_event_timeout(fcport->nvme_waitQ,
+	    atomic_read(&fcport->nvme_ref_count),
+	    NVME_ABORT_POLLING_PERIOD*HZ);
+
+	if (atomic_read(&fcport->nvme_ref_count)) {
+		ret = QLA_FUNCTION_FAILED;
+		ql_log(ql_log_info, fcport->vha, 0x2111,
+		    "timed out waiting for fcport=%p to delete\n", fcport);
+	}
+
+	return ret;
+}
+
+void qla_nvme_abort(struct qla_hw_data *ha, srb_t *sp)
+{
+	int rval;
+
+	rval = ha->isp_ops->abort_command(sp);
+	if (!rval) {
+		if (!qla_nvme_wait_on_command(sp))
+			ql_log(ql_log_warn, NULL, 0x2112,
+			    "nvme_wait_on_comand timed out waiting on sp=%p\n",
+			    sp);
+	}
+}
+
+static void qla_nvme_abort_all(fc_port_t *fcport)
+{
+	int que, cnt;
+	unsigned long flags;
+	srb_t *sp;
+	struct qla_hw_data *ha = fcport->vha->hw;
+	struct req_que *req;
+
+	spin_lock_irqsave(&ha->hardware_lock, flags);
+	for (que = 0; que < ha->max_req_queues; que++) {
+		req = ha->req_q_map[que];
+		if (!req)
+			continue;
+		if (!req->outstanding_cmds)
+			continue;
+		for (cnt = 1; cnt < req->num_outstanding_cmds; cnt++) {
+			sp = req->outstanding_cmds[cnt];
+			if ((sp) && ((sp->type == SRB_NVME_CMD) ||
+			    (sp->type == SRB_NVME_LS)) &&
+				(sp->fcport == fcport)) {
+				atomic_inc(&sp->ref_count);
+				spin_unlock_irqrestore(&ha->hardware_lock,
+				    flags);
+				qla_nvme_abort(ha, sp);
+				spin_lock_irqsave(&ha->hardware_lock, flags);
+				req->outstanding_cmds[cnt] = NULL;
+				sp->done(sp, 1);
+			}
+		}
+	}
+	spin_unlock_irqrestore(&ha->hardware_lock, flags);
+}
+
+static void qla_nvme_unregister_remote_port(struct work_struct *work)
+{
+#if (IS_ENABLED(CONFIG_NVME_FC))
+	struct fc_port *fcport = container_of(work, struct fc_port,
+	    nvme_del_work);
+	struct nvme_rport *rport, *trport;
+
+	list_for_each_entry_safe(rport, trport,
+	    &fcport->vha->nvme_rport_list, list) {
+		if (rport->fcport == fcport) {
+			ql_log(ql_log_info, fcport->vha, 0x2113,
+			    "%s: fcport=%p\n", __func__, fcport);
+			nvme_fc_unregister_remoteport(
+			    fcport->nvme_remote_port);
+		}
+	}
+#endif
+}
+
+void qla_nvme_delete(scsi_qla_host_t *vha)
+{
+#if (IS_ENABLED(CONFIG_NVME_FC))
+	struct nvme_rport *rport, *trport;
+	fc_port_t *fcport;
+	int nv_ret;
+
+	list_for_each_entry_safe(rport, trport, &vha->nvme_rport_list, list) {
+		fcport = rport->fcport;
+
+		ql_log(ql_log_info, fcport->vha, 0x2114, "%s: fcport=%p\n",
+		    __func__, fcport);
+
+		nvme_fc_unregister_remoteport(fcport->nvme_remote_port);
+		qla_nvme_wait_on_rport_del(fcport);
+		qla_nvme_abort_all(fcport);
+	}
+
+	if (vha->nvme_local_port) {
+		nv_ret = nvme_fc_unregister_localport(vha->nvme_local_port);
+		if (nv_ret == 0)
+			ql_log(ql_log_info, vha, 0x2116,
+			    "unregistered localport=%p\n",
+			    vha->nvme_local_port);
+		else
+			ql_log(ql_log_info, vha, 0x2115,
+			    "Unregister of localport failed\n");
+	}
+#endif
+}
+
+void qla_nvme_register_hba(scsi_qla_host_t *vha)
+{
+#if (IS_ENABLED(CONFIG_NVME_FC))
+	struct nvme_fc_port_template *tmpl;
+	struct qla_hw_data *ha;
+	struct nvme_fc_port_info pinfo;
+	int ret;
+
+	ha = vha->hw;
+	tmpl = &qla_nvme_fc_transport;
+
+	WARN_ON(vha->nvme_local_port);
+	WARN_ON(ha->max_req_queues < 3);
+
+	qla_nvme_fc_transport.max_hw_queues =
+	    min((uint8_t)(qla_nvme_fc_transport.max_hw_queues),
+		(uint8_t)(ha->max_req_queues - 2));
+
+	pinfo.node_name = wwn_to_u64(vha->node_name);
+	pinfo.port_name = wwn_to_u64(vha->port_name);
+	pinfo.port_role = FC_PORT_ROLE_NVME_INITIATOR;
+	pinfo.port_id = vha->d_id.b24;
+
+	ql_log(ql_log_info, vha, 0xffff,
+	    "register_localport: host-traddr=pn-0x%llx:nn-0x%llx on portID:%x\n",
+	    pinfo.port_name, pinfo.node_name, pinfo.port_id);
+	qla_nvme_fc_transport.dma_boundary = vha->host->dma_boundary;
+
+	ret = nvme_fc_register_localport(&pinfo, tmpl,
+	    get_device(&ha->pdev->dev), &vha->nvme_local_port);
+	if (ret) {
+		ql_log(ql_log_warn, vha, 0xffff,
+		    "register_localport failed: ret=%x\n", ret);
+		return;
+	}
+	atomic_set(&vha->nvme_ref_count, 1);
+	vha->nvme_local_port->private = vha;
+	init_waitqueue_head(&vha->nvme_waitQ);
+#endif
+}
diff --git a/drivers/scsi/qla2xxx/qla_nvme.h b/drivers/scsi/qla2xxx/qla_nvme.h
new file mode 100644
index 000000000000..dfe56f207b28
--- /dev/null
+++ b/drivers/scsi/qla2xxx/qla_nvme.h
@@ -0,0 +1,132 @@
+/*
+ * QLogic Fibre Channel HBA Driver
+ * Copyright (c)  2003-2017 QLogic Corporation
+ *
+ * See LICENSE.qla2xxx for copyright and licensing details.
+ */
+#ifndef __QLA_NVME_H
+#define __QLA_NVME_H
+
+#include <linux/blk-mq.h>
+#include <uapi/scsi/fc/fc_fs.h>
+#include <uapi/scsi/fc/fc_els.h>
+#include <linux/nvme-fc-driver.h>
+
+#define NVME_ATIO_CMD_OFF 32
+#define NVME_FIRST_PACKET_CMDLEN (64 - NVME_ATIO_CMD_OFF)
+#define Q2T_NVME_NUM_TAGS 2048
+#define QLA_MAX_FC_SEGMENTS 64
+
+struct srb;
+struct nvme_private {
+	struct srb	*sp;
+	struct nvmefc_ls_req *fd;
+	struct work_struct ls_work;
+	int comp_status;
+};
+
+struct nvme_rport {
+	struct nvme_fc_port_info req;
+	struct list_head list;
+	struct fc_port *fcport;
+};
+
+#define COMMAND_NVME    0x88            /* Command Type FC-NVMe IOCB */
+struct cmd_nvme {
+	uint8_t entry_type;             /* Entry type. */
+	uint8_t entry_count;            /* Entry count. */
+	uint8_t sys_define;             /* System defined. */
+	uint8_t entry_status;           /* Entry Status. */
+
+	uint32_t handle;                /* System handle. */
+	uint16_t nport_handle;          /* N_PORT handle. */
+	uint16_t timeout;               /* Command timeout. */
+
+	uint16_t dseg_count;            /* Data segment count. */
+	uint16_t nvme_rsp_dsd_len;      /* NVMe RSP DSD length */
+
+	uint64_t rsvd;
+
+	uint16_t control_flags;         /* Control Flags */
+#define CF_NVME_ENABLE                  BIT_9
+#define CF_DIF_SEG_DESCR_ENABLE         BIT_3
+#define CF_DATA_SEG_DESCR_ENABLE        BIT_2
+#define CF_READ_DATA                    BIT_1
+#define CF_WRITE_DATA                   BIT_0
+
+	uint16_t nvme_cmnd_dseg_len;             /* Data segment length. */
+	uint32_t nvme_cmnd_dseg_address[2];      /* Data segment address. */
+	uint32_t nvme_rsp_dseg_address[2];       /* Data segment address. */
+
+	uint32_t byte_count;            /* Total byte count. */
+
+	uint8_t port_id[3];             /* PortID of destination port. */
+	uint8_t vp_index;
+
+	uint32_t nvme_data_dseg_address[2];      /* Data segment address. */
+	uint32_t nvme_data_dseg_len;             /* Data segment length. */
+};
+
+#define PT_LS4_REQUEST 0x89	/* Link Service pass-through IOCB (request) */
+struct pt_ls4_request {
+	uint8_t entry_type;
+	uint8_t entry_count;
+	uint8_t sys_define;
+	uint8_t entry_status;
+	uint32_t handle;
+	uint16_t status;
+	uint16_t nport_handle;
+	uint16_t tx_dseg_count;
+	uint8_t  vp_index;
+	uint8_t  rsvd;
+	uint16_t timeout;
+	uint16_t control_flags;
+#define CF_LS4_SHIFT		13
+#define CF_LS4_ORIGINATOR	0
+#define CF_LS4_RESPONDER	1
+#define CF_LS4_RESPONDER_TERM	2
+
+	uint16_t rx_dseg_count;
+	uint16_t rsvd2;
+	uint32_t exchange_address;
+	uint32_t rsvd3;
+	uint32_t rx_byte_count;
+	uint32_t tx_byte_count;
+	uint32_t dseg0_address[2];
+	uint32_t dseg0_len;
+	uint32_t dseg1_address[2];
+	uint32_t dseg1_len;
+};
+
+#define PT_LS4_UNSOL 0x56	/* pass-up unsolicited rec FC-NVMe request */
+struct pt_ls4_rx_unsol {
+	uint8_t entry_type;
+	uint8_t entry_count;
+	uint16_t rsvd0;
+	uint16_t rsvd1;
+	uint8_t vp_index;
+	uint8_t rsvd2;
+	uint16_t rsvd3;
+	uint16_t nport_handle;
+	uint16_t frame_size;
+	uint16_t rsvd4;
+	uint32_t exchange_address;
+	uint8_t d_id[3];
+	uint8_t r_ctl;
+	uint8_t s_id[3];
+	uint8_t cs_ctl;
+	uint8_t f_ctl[3];
+	uint8_t type;
+	uint16_t seq_cnt;
+	uint8_t df_ctl;
+	uint8_t seq_id;
+	uint16_t rx_id;
+	uint16_t ox_id;
+	uint32_t param;
+	uint32_t desc0;
+#define PT_LS4_PAYLOAD_OFFSET 0x2c
+#define PT_LS4_FIRST_PACKET_LEN 20
+	uint32_t desc_len;
+	uint32_t payload[3];
+};
+#endif
* Unmerged path drivers/scsi/qla2xxx/qla_os.c
