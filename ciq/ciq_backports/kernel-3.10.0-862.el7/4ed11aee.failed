KVM: PPC: Book3S HV: Fix use after free in case of multiple resize requests

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Serhii Popovych <spopovyc@redhat.com>
commit 4ed11aeefda439c76ddae3ceebcfa4fad111f149
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/4ed11aee.failed

When serving multiple resize requests following could happen:

    CPU0                                    CPU1
    ----                                    ----
    kvm_vm_ioctl_resize_hpt_prepare(1);
      -> schedule_work()
                                            /* system_rq might be busy: delay */
    kvm_vm_ioctl_resize_hpt_prepare(2);
      mutex_lock();
      if (resize) {
         ...
         release_hpt_resize();
      }
      ...                                   resize_hpt_prepare_work()
      -> schedule_work()                    {
      mutex_unlock()                           /* resize->kvm could be wrong */
                                               struct kvm *kvm = resize->kvm;

                                               mutex_lock(&kvm->lock);   <<<< UAF
                                               ...
                                            }

i.e. a second resize request with different order could be started by
kvm_vm_ioctl_resize_hpt_prepare(), causing the previous request to be
free()d when there's still an active worker thread which will try to
access it.  This leads to a use after free in point marked with UAF on
the diagram above.

To prevent this from happening, instead of unconditionally releasing a
pre-existing resize structure from the prepare ioctl(), we check if
the existing structure has an in-progress worker.  We do that by
checking if the resize->error == -EBUSY, which is safe because the
resize->error field is protected by the kvm->lock.  If there is an
active worker, instead of releasing, we mark the structure as stale by
unlinking it from kvm_struct.

In the worker thread we check for a stale structure (with kvm->lock
held), and in that case abort, releasing the stale structure ourself.
We make the check both before and the actual allocation.  Strictly,
only the check afterwards is needed, the check before is an
optimization: if the structure happens to become stale before the
worker thread is dispatched, rather than during the allocation, it
means we can avoid allocating then immediately freeing a potentially
substantial amount of memory.

This fixes following or similar host kernel crash message:

[  635.277361] Unable to handle kernel paging request for data at address 0x00000000
[  635.277438] Faulting instruction address: 0xc00000000052f568
[  635.277446] Oops: Kernel access of bad area, sig: 11 [#1]
[  635.277451] SMP NR_CPUS=2048 NUMA PowerNV
[  635.277470] Modules linked in: xt_CHECKSUM iptable_mangle ipt_MASQUERADE
nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4
nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT nf_reject_ipv4 tun bridge stp llc
ebtable_filter ebtables ip6table_filter ip6_tables iptable_filter nfsv3 nfs_acl nfs
lockd grace fscache kvm_hv kvm rpcrdma sunrpc ib_isert iscsi_target_mod ib_iser libiscsi
scsi_transport_iscsi ib_srpt target_core_mod ext4 ib_srp scsi_transport_srp
ib_ipoib mbcache jbd2 rdma_ucm ib_ucm ib_uverbs ib_umad rdma_cm ib_cm iw_cm ocrdma(T)
ib_core ses enclosure scsi_transport_sas sg shpchp leds_powernv ibmpowernv i2c_opal
i2c_core powernv_rng ipmi_powernv ipmi_devintf ipmi_msghandler ip_tables xfs
libcrc32c sr_mod sd_mod cdrom lpfc nvme_fc(T) nvme_fabrics nvme_core ipr nvmet_fc(T)
tg3 nvmet libata be2net crc_t10dif crct10dif_generic scsi_transport_fc ptp scsi_tgt
pps_core crct10dif_common dm_mirror dm_region_hash dm_log dm_mod
[  635.278687] CPU: 40 PID: 749 Comm: kworker/40:1 Tainted: G
------------ T 3.10.0.bz1510771+ #1
[  635.278782] Workqueue: events resize_hpt_prepare_work [kvm_hv]
[  635.278851] task: c0000007e6840000 ti: c0000007e9180000 task.ti: c0000007e9180000
[  635.278919] NIP: c00000000052f568 LR: c0000000009ea310 CTR: c0000000009ea4f0
[  635.278988] REGS: c0000007e91837f0 TRAP: 0300   Tainted: G
------------ T  (3.10.0.bz1510771+)
[  635.279077] MSR: 9000000100009033 <SF,HV,EE,ME,IR,DR,RI,LE>  CR: 24002022  XER:
00000000
[  635.279248] CFAR: c000000000009368 DAR: 0000000000000000 DSISR: 40000000 SOFTE: 1
GPR00: c0000000009ea310 c0000007e9183a70 c000000001250b00 c0000007e9183b10
GPR04: 0000000000000000 0000000000000000 c0000007e9183650 0000000000000000
GPR08: c0000007ffff7b80 00000000ffffffff 0000000080000028 d00000000d2529a0
GPR12: 0000000000002200 c000000007b56800 c000000000120028 c0000007f135bb40
GPR16: 0000000000000000 c000000005c1e018 c000000005c1e018 0000000000000000
GPR20: 0000000000000001 c0000000011bf778 0000000000000001 fffffffffffffef7
GPR24: 0000000000000000 c000000f1e262e50 0000000000000002 c0000007e9180000
GPR28: c000000f1e262e4c c000000f1e262e50 0000000000000000 c0000007e9183b10
[  635.280149] NIP [c00000000052f568] __list_add+0x38/0x110
[  635.280197] LR [c0000000009ea310] __mutex_lock_slowpath+0xe0/0x2c0
[  635.280253] Call Trace:
[  635.280277] [c0000007e9183af0] [c0000000009ea310] __mutex_lock_slowpath+0xe0/0x2c0
[  635.280356] [c0000007e9183b70] [c0000000009ea554] mutex_lock+0x64/0x70
[  635.280426] [c0000007e9183ba0] [d00000000d24da04]
resize_hpt_prepare_work+0xe4/0x1c0 [kvm_hv]
[  635.280507] [c0000007e9183c40] [c000000000113c0c] process_one_work+0x1dc/0x680
[  635.280587] [c0000007e9183ce0] [c000000000114250] worker_thread+0x1a0/0x520
[  635.280655] [c0000007e9183d80] [c00000000012010c] kthread+0xec/0x100
[  635.280724] [c0000007e9183e30] [c00000000000a4b8] ret_from_kernel_thread+0x5c/0xa4
[  635.280814] Instruction dump:
[  635.280880] 7c0802a6 fba1ffe8 fbc1fff0 7cbd2b78 fbe1fff8 7c9e2378 7c7f1b78
f8010010
[  635.281099] f821ff81 e8a50008 7fa52040 40de00b8 <e8be0000> 7fbd2840 40de008c
7fbff040
[  635.281324] ---[ end trace b628b73449719b9d ]---

	Cc: stable@vger.kernel.org # v4.10+
Fixes: b5baa6877315 ("KVM: PPC: Book3S HV: KVM-HV HPT resizing implementation")
	Signed-off-by: Serhii Popovych <spopovyc@redhat.com>
[dwg: Replaced BUG_ON()s with WARN_ONs() and reworded commit message
 for clarity]
	Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
(cherry picked from commit 4ed11aeefda439c76ddae3ceebcfa4fad111f149)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_64_mmu_hv.c
diff --cc arch/powerpc/kvm/book3s_64_mmu_hv.c
index 283e37e10f56,8355398f0bb6..000000000000
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@@ -1119,6 -1216,392 +1119,395 @@@ void kvmppc_unpin_guest_page(struct kv
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * HPT resizing
+  */
+ static int resize_hpt_allocate(struct kvm_resize_hpt *resize)
+ {
+ 	int rc;
+ 
+ 	rc = kvmppc_allocate_hpt(&resize->hpt, resize->order);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_allocate(): HPT @ 0x%lx\n",
+ 			 resize->hpt.virt);
+ 
+ 	return 0;
+ }
+ 
+ static unsigned long resize_hpt_rehash_hpte(struct kvm_resize_hpt *resize,
+ 					    unsigned long idx)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	struct kvm_hpt_info *old = &kvm->arch.hpt;
+ 	struct kvm_hpt_info *new = &resize->hpt;
+ 	unsigned long old_hash_mask = (1ULL << (old->order - 7)) - 1;
+ 	unsigned long new_hash_mask = (1ULL << (new->order - 7)) - 1;
+ 	__be64 *hptep, *new_hptep;
+ 	unsigned long vpte, rpte, guest_rpte;
+ 	int ret;
+ 	struct revmap_entry *rev;
+ 	unsigned long apsize, avpn, pteg, hash;
+ 	unsigned long new_idx, new_pteg, replace_vpte;
+ 	int pshift;
+ 
+ 	hptep = (__be64 *)(old->virt + (idx << 4));
+ 
+ 	/* Guest is stopped, so new HPTEs can't be added or faulted
+ 	 * in, only unmapped or altered by host actions.  So, it's
+ 	 * safe to check this before we take the HPTE lock */
+ 	vpte = be64_to_cpu(hptep[0]);
+ 	if (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))
+ 		return 0; /* nothing to do */
+ 
+ 	while (!try_lock_hpte(hptep, HPTE_V_HVLOCK))
+ 		cpu_relax();
+ 
+ 	vpte = be64_to_cpu(hptep[0]);
+ 
+ 	ret = 0;
+ 	if (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))
+ 		/* Nothing to do */
+ 		goto out;
+ 
+ 	/* Unmap */
+ 	rev = &old->rev[idx];
+ 	guest_rpte = rev->guest_rpte;
+ 
+ 	ret = -EIO;
+ 	apsize = kvmppc_actual_pgsz(vpte, guest_rpte);
+ 	if (!apsize)
+ 		goto out;
+ 
+ 	if (vpte & HPTE_V_VALID) {
+ 		unsigned long gfn = hpte_rpn(guest_rpte, apsize);
+ 		int srcu_idx = srcu_read_lock(&kvm->srcu);
+ 		struct kvm_memory_slot *memslot =
+ 			__gfn_to_memslot(kvm_memslots(kvm), gfn);
+ 
+ 		if (memslot) {
+ 			unsigned long *rmapp;
+ 			rmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];
+ 
+ 			lock_rmap(rmapp);
+ 			kvmppc_unmap_hpte(kvm, idx, memslot, rmapp, gfn);
+ 			unlock_rmap(rmapp);
+ 		}
+ 
+ 		srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 	}
+ 
+ 	/* Reload PTE after unmap */
+ 	vpte = be64_to_cpu(hptep[0]);
+ 
+ 	BUG_ON(vpte & HPTE_V_VALID);
+ 	BUG_ON(!(vpte & HPTE_V_ABSENT));
+ 
+ 	ret = 0;
+ 	if (!(vpte & HPTE_V_BOLTED))
+ 		goto out;
+ 
+ 	rpte = be64_to_cpu(hptep[1]);
+ 	pshift = kvmppc_hpte_base_page_shift(vpte, rpte);
+ 	avpn = HPTE_V_AVPN_VAL(vpte) & ~(((1ul << pshift) - 1) >> 23);
+ 	pteg = idx / HPTES_PER_GROUP;
+ 	if (vpte & HPTE_V_SECONDARY)
+ 		pteg = ~pteg;
+ 
+ 	if (!(vpte & HPTE_V_1TB_SEG)) {
+ 		unsigned long offset, vsid;
+ 
+ 		/* We only have 28 - 23 bits of offset in avpn */
+ 		offset = (avpn & 0x1f) << 23;
+ 		vsid = avpn >> 5;
+ 		/* We can find more bits from the pteg value */
+ 		if (pshift < 23)
+ 			offset |= ((vsid ^ pteg) & old_hash_mask) << pshift;
+ 
+ 		hash = vsid ^ (offset >> pshift);
+ 	} else {
+ 		unsigned long offset, vsid;
+ 
+ 		/* We only have 40 - 23 bits of seg_off in avpn */
+ 		offset = (avpn & 0x1ffff) << 23;
+ 		vsid = avpn >> 17;
+ 		if (pshift < 23)
+ 			offset |= ((vsid ^ (vsid << 25) ^ pteg) & old_hash_mask) << pshift;
+ 
+ 		hash = vsid ^ (vsid << 25) ^ (offset >> pshift);
+ 	}
+ 
+ 	new_pteg = hash & new_hash_mask;
+ 	if (vpte & HPTE_V_SECONDARY) {
+ 		BUG_ON(~pteg != (hash & old_hash_mask));
+ 		new_pteg = ~new_pteg;
+ 	} else {
+ 		BUG_ON(pteg != (hash & old_hash_mask));
+ 	}
+ 
+ 	new_idx = new_pteg * HPTES_PER_GROUP + (idx % HPTES_PER_GROUP);
+ 	new_hptep = (__be64 *)(new->virt + (new_idx << 4));
+ 
+ 	replace_vpte = be64_to_cpu(new_hptep[0]);
+ 
+ 	if (replace_vpte & (HPTE_V_VALID | HPTE_V_ABSENT)) {
+ 		BUG_ON(new->order >= old->order);
+ 
+ 		if (replace_vpte & HPTE_V_BOLTED) {
+ 			if (vpte & HPTE_V_BOLTED)
+ 				/* Bolted collision, nothing we can do */
+ 				ret = -ENOSPC;
+ 			/* Discard the new HPTE */
+ 			goto out;
+ 		}
+ 
+ 		/* Discard the previous HPTE */
+ 	}
+ 
+ 	new_hptep[1] = cpu_to_be64(rpte);
+ 	new->rev[new_idx].guest_rpte = guest_rpte;
+ 	/* No need for a barrier, since new HPT isn't active */
+ 	new_hptep[0] = cpu_to_be64(vpte);
+ 	unlock_hpte(new_hptep, vpte);
+ 
+ out:
+ 	unlock_hpte(hptep, vpte);
+ 	return ret;
+ }
+ 
+ static int resize_hpt_rehash(struct kvm_resize_hpt *resize)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	unsigned  long i;
+ 	int rc;
+ 
+ 	/*
+ 	 * resize_hpt_rehash_hpte() doesn't handle the new-format HPTEs
+ 	 * that POWER9 uses, and could well hit a BUG_ON on POWER9.
+ 	 */
+ 	if (cpu_has_feature(CPU_FTR_ARCH_300))
+ 		return -EIO;
+ 	for (i = 0; i < kvmppc_hpt_npte(&kvm->arch.hpt); i++) {
+ 		rc = resize_hpt_rehash_hpte(resize, i);
+ 		if (rc != 0)
+ 			return rc;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void resize_hpt_pivot(struct kvm_resize_hpt *resize)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	struct kvm_hpt_info hpt_tmp;
+ 
+ 	/* Exchange the pending tables in the resize structure with
+ 	 * the active tables */
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_pivot()\n");
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	asm volatile("ptesync" : : : "memory");
+ 
+ 	hpt_tmp = kvm->arch.hpt;
+ 	kvmppc_set_hpt(kvm, &resize->hpt);
+ 	resize->hpt = hpt_tmp;
+ 
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	synchronize_srcu_expedited(&kvm->srcu);
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_pivot() done\n");
+ }
+ 
+ static void resize_hpt_release(struct kvm *kvm, struct kvm_resize_hpt *resize)
+ {
+ 	if (WARN_ON(!mutex_is_locked(&kvm->lock)))
+ 		return;
+ 
+ 	if (!resize)
+ 		return;
+ 
+ 	if (resize->error != -EBUSY) {
+ 		if (resize->hpt.virt)
+ 			kvmppc_free_hpt(&resize->hpt);
+ 		kfree(resize);
+ 	}
+ 
+ 	if (kvm->arch.resize_hpt == resize)
+ 		kvm->arch.resize_hpt = NULL;
+ }
+ 
+ static void resize_hpt_prepare_work(struct work_struct *work)
+ {
+ 	struct kvm_resize_hpt *resize = container_of(work,
+ 						     struct kvm_resize_hpt,
+ 						     work);
+ 	struct kvm *kvm = resize->kvm;
+ 	int err = 0;
+ 
+ 	if (WARN_ON(resize->error != -EBUSY))
+ 		return;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	/* Request is still current? */
+ 	if (kvm->arch.resize_hpt == resize) {
+ 		/* We may request large allocations here:
+ 		 * do not sleep with kvm->lock held for a while.
+ 		 */
+ 		mutex_unlock(&kvm->lock);
+ 
+ 		resize_hpt_debug(resize, "resize_hpt_prepare_work(): order = %d\n",
+ 				 resize->order);
+ 
+ 		err = resize_hpt_allocate(resize);
+ 
+ 		/* We have strict assumption about -EBUSY
+ 		 * when preparing for HPT resize.
+ 		 */
+ 		if (WARN_ON(err == -EBUSY))
+ 			err = -EINPROGRESS;
+ 
+ 		mutex_lock(&kvm->lock);
+ 		/* It is possible that kvm->arch.resize_hpt != resize
+ 		 * after we grab kvm->lock again.
+ 		 */
+ 	}
+ 
+ 	resize->error = err;
+ 
+ 	if (kvm->arch.resize_hpt != resize)
+ 		resize_hpt_release(kvm, resize);
+ 
+ 	mutex_unlock(&kvm->lock);
+ }
+ 
+ long kvm_vm_ioctl_resize_hpt_prepare(struct kvm *kvm,
+ 				     struct kvm_ppc_resize_hpt *rhpt)
+ {
+ 	unsigned long flags = rhpt->flags;
+ 	unsigned long shift = rhpt->shift;
+ 	struct kvm_resize_hpt *resize;
+ 	int ret;
+ 
+ 	if (flags != 0 || kvm_is_radix(kvm))
+ 		return -EINVAL;
+ 
+ 	if (shift && ((shift < 18) || (shift > 46)))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize = kvm->arch.resize_hpt;
+ 
+ 	if (resize) {
+ 		if (resize->order == shift) {
+ 			/* Suitable resize in progress? */
+ 			ret = resize->error;
+ 			if (ret == -EBUSY)
+ 				ret = 100; /* estimated time in ms */
+ 			else if (ret)
+ 				resize_hpt_release(kvm, resize);
+ 
+ 			goto out;
+ 		}
+ 
+ 		/* not suitable, cancel it */
+ 		resize_hpt_release(kvm, resize);
+ 	}
+ 
+ 	ret = 0;
+ 	if (!shift)
+ 		goto out; /* nothing to do */
+ 
+ 	/* start new resize */
+ 
+ 	resize = kzalloc(sizeof(*resize), GFP_KERNEL);
+ 	if (!resize) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	resize->error = -EBUSY;
+ 	resize->order = shift;
+ 	resize->kvm = kvm;
+ 	INIT_WORK(&resize->work, resize_hpt_prepare_work);
+ 	kvm->arch.resize_hpt = resize;
+ 
+ 	schedule_work(&resize->work);
+ 
+ 	ret = 100; /* estimated time in ms */
+ 
+ out:
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ static void resize_hpt_boot_vcpu(void *opaque)
+ {
+ 	/* Nothing to do, just force a KVM exit */
+ }
+ 
+ long kvm_vm_ioctl_resize_hpt_commit(struct kvm *kvm,
+ 				    struct kvm_ppc_resize_hpt *rhpt)
+ {
+ 	unsigned long flags = rhpt->flags;
+ 	unsigned long shift = rhpt->shift;
+ 	struct kvm_resize_hpt *resize;
+ 	long ret;
+ 
+ 	if (flags != 0 || kvm_is_radix(kvm))
+ 		return -EINVAL;
+ 
+ 	if (shift && ((shift < 18) || (shift > 46)))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize = kvm->arch.resize_hpt;
+ 
+ 	/* This shouldn't be possible */
+ 	ret = -EIO;
+ 	if (WARN_ON(!kvm->arch.mmu_ready))
+ 		goto out_no_hpt;
+ 
+ 	/* Stop VCPUs from running while we mess with the HPT */
+ 	kvm->arch.mmu_ready = 0;
+ 	smp_mb();
+ 
+ 	/* Boot all CPUs out of the guest so they re-read
+ 	 * mmu_ready */
+ 	on_each_cpu(resize_hpt_boot_vcpu, NULL, 1);
+ 
+ 	ret = -ENXIO;
+ 	if (!resize || (resize->order != shift))
+ 		goto out;
+ 
+ 	ret = resize->error;
+ 	if (ret)
+ 		goto out;
+ 
+ 	ret = resize_hpt_rehash(resize);
+ 	if (ret)
+ 		goto out;
+ 
+ 	resize_hpt_pivot(resize);
+ 
+ out:
+ 	/* Let VCPUs run again */
+ 	kvm->arch.mmu_ready = 1;
+ 	smp_mb();
+ out_no_hpt:
+ 	resize_hpt_release(kvm, resize);
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ /*
++>>>>>>> 4ed11aeefda4 (KVM: PPC: Book3S HV: Fix use after free in case of multiple resize requests)
   * Functions for reading and writing the hash table via reads and
   * writes on a file descriptor.
   *
* Unmerged path arch/powerpc/kvm/book3s_64_mmu_hv.c
