scsi: lpfc: Fix nvmet RQ resource needs for large block writes.

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] lpfc: Fix nvmet RQ resource needs for large block writes (Dick Kennedy) [1385844 1461977 1387768]
Rebuild_FUZZ: 94.12%
commit-author James Smart <jsmart2021@gmail.com>
commit 61f3d4bf4f8f062cf6be143c9b7adbc3a017ea6e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/61f3d4bf.failed

Large block writes to the nvme target were failing because the default
number of RQs posted was insufficient.

Expand the NVMET RQs to 2048 RQEs and ensure a minimum of 512 RQEs are
posted, no matter how many MRQs are configured.

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.de>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 61f3d4bf4f8f062cf6be143c9b7adbc3a017ea6e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_attr.c
#	drivers/scsi/lpfc/lpfc_init.c
#	drivers/scsi/lpfc/lpfc_nvmet.c
#	drivers/scsi/lpfc/lpfc_nvmet.h
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/lpfc/lpfc_sli4.h
diff --cc drivers/scsi/lpfc/lpfc_attr.c
index b0e0bd1cf345,129d6cd7635b..000000000000
--- a/drivers/scsi/lpfc/lpfc_attr.c
+++ b/drivers/scsi/lpfc/lpfc_attr.c
@@@ -50,9 -56,13 +50,19 @@@
  #include "lpfc_vport.h"
  #include "lpfc_attr.h"
  
++<<<<<<< HEAD
 +#define LPFC_DEF_DEVLOSS_TMO 30
 +#define LPFC_MIN_DEVLOSS_TMO 1
 +#define LPFC_MAX_DEVLOSS_TMO 255
++=======
+ #define LPFC_DEF_DEVLOSS_TMO	30
+ #define LPFC_MIN_DEVLOSS_TMO	1
+ #define LPFC_MAX_DEVLOSS_TMO	255
+ 
+ #define LPFC_DEF_MRQ_POST	512
+ #define LPFC_MIN_MRQ_POST	512
+ #define LPFC_MAX_MRQ_POST	2048
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
  
  /*
   * Write key size should be multiple of 4. If write key is changed
diff --cc drivers/scsi/lpfc/lpfc_init.c
index 4d8c754a14fe,5f62e3a1dff6..000000000000
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@@ -3271,6 -3357,172 +3271,175 @@@ lpfc_sli4_xri_sgl_update(struct lpfc_hb
  		sglq_entry->sli4_lxritag = lxri;
  		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
  	}
++<<<<<<< HEAD
++=======
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_els_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_nvmet_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_sglq *sglq_entry = NULL, *sglq_entry_next = NULL;
+ 	uint16_t i, lxri, xri_cnt, els_xri_cnt;
+ 	uint16_t nvmet_xri_cnt, tot_cnt;
+ 	LIST_HEAD(nvmet_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's nvmet xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	nvmet_xri_cnt = phba->cfg_nvmet_mrq * phba->cfg_nvmet_mrq_post;
+ 
+ 	/* Ensure we at least meet the minimun for the system */
+ 	if (nvmet_xri_cnt < LPFC_NVMET_RQE_DEF_COUNT)
+ 		nvmet_xri_cnt = LPFC_NVMET_RQE_DEF_COUNT;
+ 
+ 	tot_cnt = phba->sli4_hba.max_cfg_param.max_xri - els_xri_cnt;
+ 	if (nvmet_xri_cnt > tot_cnt) {
+ 		phba->cfg_nvmet_mrq_post = tot_cnt / phba->cfg_nvmet_mrq;
+ 		nvmet_xri_cnt = phba->cfg_nvmet_mrq * phba->cfg_nvmet_mrq_post;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6301 NVMET post-sgl count changed to %d\n",
+ 				phba->cfg_nvmet_mrq_post);
+ 	}
+ 
+ 	if (nvmet_xri_cnt > phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* els xri-sgl expanded */
+ 		xri_cnt = nvmet_xri_cnt - phba->sli4_hba.nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6302 NVMET xri-sgl cnt grew from %d to %d\n",
+ 				phba->sli4_hba.nvmet_xri_cnt, nvmet_xri_cnt);
+ 		/* allocate the additional nvmet sgls */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			sglq_entry = kzalloc(sizeof(struct lpfc_sglq),
+ 					     GFP_KERNEL);
+ 			if (sglq_entry == NULL) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6303 Failure to allocate an "
+ 						"NVMET sgl entry:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->buff_type = NVMET_BUFF_TYPE;
+ 			sglq_entry->virt = lpfc_nvmet_buf_alloc(phba, 0,
+ 							   &sglq_entry->phys);
+ 			if (sglq_entry->virt == NULL) {
+ 				kfree(sglq_entry);
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6304 Failure to allocate an "
+ 						"NVMET buf:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->sgl = sglq_entry->virt;
+ 			memset(sglq_entry->sgl, 0,
+ 			       phba->cfg_sg_dma_buf_size);
+ 			sglq_entry->state = SGL_FREED;
+ 			list_add_tail(&sglq_entry->list, &nvmet_sgl_list);
+ 		}
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else if (nvmet_xri_cnt < phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* nvmet xri-sgl shrunk */
+ 		xri_cnt = phba->sli4_hba.nvmet_xri_cnt - nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6305 NVMET xri-sgl count decreased from "
+ 				"%d to %d\n", phba->sli4_hba.nvmet_xri_cnt,
+ 				nvmet_xri_cnt);
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&phba->sli4_hba.lpfc_nvmet_sgl_list,
+ 				 &nvmet_sgl_list);
+ 		/* release extra nvmet sgls from list */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			list_remove_head(&nvmet_sgl_list,
+ 					 sglq_entry, struct lpfc_sglq, list);
+ 			if (sglq_entry) {
+ 				lpfc_nvmet_buf_free(phba, sglq_entry->virt,
+ 						    sglq_entry->phys);
+ 				kfree(sglq_entry);
+ 			}
+ 		}
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6306 NVMET xri-sgl count unchanged: %d\n",
+ 				nvmet_xri_cnt);
+ 	phba->sli4_hba.nvmet_xri_cnt = nvmet_xri_cnt;
+ 
+ 	/* update xris to nvmet sgls on the list */
+ 	sglq_entry = NULL;
+ 	sglq_entry_next = NULL;
+ 	list_for_each_entry_safe(sglq_entry, sglq_entry_next,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list, list) {
+ 		lxri = lpfc_sli4_next_xritag(phba);
+ 		if (lxri == NO_XRI) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 					"6307 Failed to allocate xri for "
+ 					"NVMET sgl\n");
+ 			rc = -ENOMEM;
+ 			goto out_free_mem;
+ 		}
+ 		sglq_entry->sli4_lxritag = lxri;
+ 		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
+ 	}
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_nvmet_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_scsi_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_scsi_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_scsi_buf *psb, *psb_next;
+ 	uint16_t i, lxri, els_xri_cnt, scsi_xri_cnt;
+ 	LIST_HEAD(scsi_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's els xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	phba->total_scsi_bufs = 0;
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
  
  	/*
  	 * update on pci function's allocated scsi xri-sgl list
@@@ -7586,6 -8158,44 +7755,47 @@@ lpfc_sli4_queue_create(struct lpfc_hba 
  	}
  	phba->sli4_hba.dat_rq = qdesc;
  
++<<<<<<< HEAD
++=======
+ 	if (phba->nvmet_support) {
+ 		for (idx = 0; idx < phba->cfg_nvmet_mrq; idx++) {
+ 			/* Create NVMET Receive Queue for header */
+ 			qdesc = lpfc_sli4_queue_alloc(phba,
+ 						      phba->sli4_hba.rq_esize,
+ 						      LPFC_NVMET_RQE_DEF_COUNT);
+ 			if (!qdesc) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"3146 Failed allocate "
+ 						"receive HRQ\n");
+ 				goto out_error;
+ 			}
+ 			phba->sli4_hba.nvmet_mrq_hdr[idx] = qdesc;
+ 
+ 			/* Only needed for header of RQ pair */
+ 			qdesc->rqbp = kzalloc(sizeof(struct lpfc_rqb),
+ 					      GFP_KERNEL);
+ 			if (qdesc->rqbp == NULL) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"6131 Failed allocate "
+ 						"Header RQBP\n");
+ 				goto out_error;
+ 			}
+ 
+ 			/* Create NVMET Receive Queue for data */
+ 			qdesc = lpfc_sli4_queue_alloc(phba,
+ 						      phba->sli4_hba.rq_esize,
+ 						      LPFC_NVMET_RQE_DEF_COUNT);
+ 			if (!qdesc) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"3156 Failed allocate "
+ 						"receive DRQ\n");
+ 				goto out_error;
+ 			}
+ 			phba->sli4_hba.nvmet_mrq_data[idx] = qdesc;
+ 		}
+ 	}
+ 
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
  	/* Create the Queues needed for Flash Optimized Fabric operations */
  	if (phba->cfg_fof)
  		lpfc_fof_queue_create(phba);
@@@ -7984,12 -8772,9 +8194,9 @@@ lpfc_sli4_queue_setup(struct lpfc_hba *
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
  				"0540 Receive Queue not allocated\n");
  		rc = -ENOMEM;
 -		goto out_destroy;
 +		goto out_destroy_els_wq;
  	}
  
- 	lpfc_rq_adjust_repost(phba, phba->sli4_hba.hdr_rq, LPFC_ELS_HBQ);
- 	lpfc_rq_adjust_repost(phba, phba->sli4_hba.dat_rq, LPFC_ELS_HBQ);
- 
  	rc = lpfc_rq_create(phba, phba->sli4_hba.hdr_rq, phba->sli4_hba.dat_rq,
  			    phba->sli4_hba.els_cq, LPFC_USOL);
  	if (rc) {
@@@ -10559,9 -11098,8 +10766,13 @@@ lpfc_pci_probe_one_s4(struct pci_dev *p
  	struct lpfc_hba   *phba;
  	struct lpfc_vport *vport = NULL;
  	struct Scsi_Host  *shost = NULL;
++<<<<<<< HEAD
 +	int error;
++=======
+ 	int error, cnt, num;
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
  	uint32_t cfg_mode, intr_mode;
 +	int adjusted_fcp_io_channel;
  
  	/* Allocate memory for HBA structure */
  	phba = lpfc_hba_alloc(pdev);
@@@ -10602,12 -11132,20 +10813,24 @@@
  		goto out_unset_pci_mem_s4;
  	}
  
++<<<<<<< HEAD
++=======
+ 	cnt = phba->cfg_iocb_cnt * 1024;
+ 	if (phba->nvmet_support) {
+ 		/* Ensure we at least meet the minimun for the system */
+ 		num = (phba->cfg_nvmet_mrq_post * phba->cfg_nvmet_mrq);
+ 		if (num < LPFC_NVMET_RQE_DEF_COUNT)
+ 			num = LPFC_NVMET_RQE_DEF_COUNT;
+ 		cnt += num;
+ 	}
+ 
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
  	/* Initialize and populate the iocb list per host */
 +
  	lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -			"2821 initialize iocb list %d total %d\n",
 -			phba->cfg_iocb_cnt, cnt);
 -	error = lpfc_init_iocb_list(phba, cnt);
 +			"2821 initialize iocb list %d.\n",
 +			phba->cfg_iocb_cnt*1024);
 +	error = lpfc_init_iocb_list(phba, phba->cfg_iocb_cnt*1024);
  
  	if (error) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index c54385fd9058,f344abce4949..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -6611,19 -6849,100 +6612,68 @@@ lpfc_sli4_hba_setup(struct lpfc_hba *ph
  				"0582 Error %d during els sgl post "
  				"operation\n", rc);
  		rc = -ENODEV;
 -		goto out_destroy_queue;
 -	}
 -	phba->sli4_hba.els_xri_cnt = rc;
 -
 -	if (phba->nvmet_support) {
 -		/* update host nvmet xri-sgl sizes and mappings */
 -		rc = lpfc_sli4_nvmet_sgl_update(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"6308 Failed to update nvmet-sgl size "
 -					"and mapping: %d\n", rc);
 -			goto out_destroy_queue;
 -		}
 -
 -		/* register the nvmet sgl pool to the port */
 -		rc = lpfc_sli4_repost_sgl_list(
 -			phba,
 -			&phba->sli4_hba.lpfc_nvmet_sgl_list,
 -			phba->sli4_hba.nvmet_xri_cnt);
 -		if (unlikely(rc < 0)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"3117 Error %d during nvmet "
 -					"sgl post\n", rc);
 -			rc = -ENODEV;
 -			goto out_destroy_queue;
 -		}
 -		phba->sli4_hba.nvmet_xri_cnt = rc;
 -		lpfc_nvmet_create_targetport(phba);
 -	} else {
 -		/* update host scsi xri-sgl sizes and mappings */
 -		rc = lpfc_sli4_scsi_sgl_update(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"6309 Failed to update scsi-sgl size "
 -					"and mapping: %d\n", rc);
 -			goto out_destroy_queue;
 -		}
 -
 -		/* update host nvme xri-sgl sizes and mappings */
 -		rc = lpfc_sli4_nvme_sgl_update(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"6082 Failed to update nvme-sgl size "
 -					"and mapping: %d\n", rc);
 -			goto out_destroy_queue;
 -		}
 +		goto out_free_mbox;
  	}
  
++<<<<<<< HEAD
 +	/* register the allocated scsi sgl pool to the port */
 +	rc = lpfc_sli4_repost_scsi_sgl_list(phba);
 +	if (unlikely(rc)) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 +				"0383 Error %d during scsi sgl post "
 +				"operation\n", rc);
 +		/* Some Scsi buffers were moved to the abort scsi list */
 +		/* A pci function reset will repost them */
 +		rc = -ENODEV;
 +		goto out_free_mbox;
++=======
+ 	if (phba->nvmet_support && phba->cfg_nvmet_mrq) {
+ 
+ 		/* Post initial buffers to all RQs created */
+ 		for (i = 0; i < phba->cfg_nvmet_mrq; i++) {
+ 			rqbp = phba->sli4_hba.nvmet_mrq_hdr[i]->rqbp;
+ 			INIT_LIST_HEAD(&rqbp->rqb_buffer_list);
+ 			rqbp->rqb_alloc_buffer = lpfc_sli4_nvmet_alloc;
+ 			rqbp->rqb_free_buffer = lpfc_sli4_nvmet_free;
+ 			rqbp->entry_count = LPFC_NVMET_RQE_DEF_COUNT;
+ 			rqbp->buffer_count = 0;
+ 
+ 			lpfc_post_rq_buffer(
+ 				phba, phba->sli4_hba.nvmet_mrq_hdr[i],
+ 				phba->sli4_hba.nvmet_mrq_data[i],
+ 				phba->cfg_nvmet_mrq_post);
+ 		}
+ 	}
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
+ 		/* register the allocated scsi sgl pool to the port */
+ 		rc = lpfc_sli4_repost_scsi_sgl_list(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"0383 Error %d during scsi sgl post "
+ 					"operation\n", rc);
+ 			/* Some Scsi buffers were moved to abort scsi list */
+ 			/* A pci function reset will repost them */
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
+ 	}
+ 
+ 	if ((phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) &&
+ 	    (phba->nvmet_support == 0)) {
+ 
+ 		/* register the allocated nvme sgl pool to the port */
+ 		rc = lpfc_repost_nvme_sgl_list(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6116 Error %d during nvme sgl post "
+ 					"operation\n", rc);
+ 			/* Some NVME buffers were moved to abort nvme list */
+ 			/* A pci function reset will repost them */
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
  	}
  
  	/* Post the rpi header region to the device. */
@@@ -14107,6 -15166,199 +14131,202 @@@ out
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_mrq_create - Create MRQ Receive Queues on the HBA
+  * @phba: HBA structure that indicates port to create a queue on.
+  * @hrqp: The queue structure array to use to create the header receive queues.
+  * @drqp: The queue structure array to use to create the data receive queues.
+  * @cqp: The completion queue array to bind these receive queues to.
+  *
+  * This function creates a receive buffer queue pair , as detailed in @hrq and
+  * @drq, on a port, described by @phba by sending a RQ_CREATE mailbox command
+  * to the HBA.
+  *
+  * The @phba struct is used to send mailbox command to HBA. The @drq and @hrq
+  * struct is used to get the entry count that is necessary to determine the
+  * number of pages to use for this queue. The @cq is used to indicate which
+  * completion queue to bind received buffers that are posted to these queues to.
+  * This function will send the RQ_CREATE mailbox command to the HBA to setup the
+  * receive queue pair. This function is asynchronous and will wait for the
+  * mailbox command to finish before continuing.
+  *
+  * On success this function will return a zero. If unable to allocate enough
+  * memory this function will return -ENOMEM. If the queue create mailbox command
+  * fails this function will return -ENXIO.
+  **/
+ int
+ lpfc_mrq_create(struct lpfc_hba *phba, struct lpfc_queue **hrqp,
+ 		struct lpfc_queue **drqp, struct lpfc_queue **cqp,
+ 		uint32_t subtype)
+ {
+ 	struct lpfc_queue *hrq, *drq, *cq;
+ 	struct lpfc_mbx_rq_create_v2 *rq_create;
+ 	struct lpfc_dmabuf *dmabuf;
+ 	LPFC_MBOXQ_t *mbox;
+ 	int rc, length, alloclen, status = 0;
+ 	int cnt, idx, numrq, page_idx = 0;
+ 	uint32_t shdr_status, shdr_add_status;
+ 	union lpfc_sli4_cfg_shdr *shdr;
+ 	uint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;
+ 
+ 	numrq = phba->cfg_nvmet_mrq;
+ 	/* sanity check on array memory */
+ 	if (!hrqp || !drqp || !cqp || !numrq)
+ 		return -ENODEV;
+ 	if (!phba->sli4_hba.pc_sli4_params.supported)
+ 		hw_page_size = SLI4_PAGE_SIZE;
+ 
+ 	mbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);
+ 	if (!mbox)
+ 		return -ENOMEM;
+ 
+ 	length = sizeof(struct lpfc_mbx_rq_create_v2);
+ 	length += ((2 * numrq * hrqp[0]->page_count) *
+ 		   sizeof(struct dma_address));
+ 
+ 	alloclen = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,
+ 				    LPFC_MBOX_OPCODE_FCOE_RQ_CREATE, length,
+ 				    LPFC_SLI4_MBX_NEMBED);
+ 	if (alloclen < length) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 				"3099 Allocated DMA memory size (%d) is "
+ 				"less than the requested DMA memory size "
+ 				"(%d)\n", alloclen, length);
+ 		status = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 
+ 
+ 	rq_create = mbox->sge_array->addr[0];
+ 	shdr = (union lpfc_sli4_cfg_shdr *)&rq_create->cfg_shdr;
+ 
+ 	bf_set(lpfc_mbox_hdr_version, &shdr->request, LPFC_Q_CREATE_VERSION_2);
+ 	cnt = 0;
+ 
+ 	for (idx = 0; idx < numrq; idx++) {
+ 		hrq = hrqp[idx];
+ 		drq = drqp[idx];
+ 		cq  = cqp[idx];
+ 
+ 		/* sanity check on queue memory */
+ 		if (!hrq || !drq || !cq) {
+ 			status = -ENODEV;
+ 			goto out;
+ 		}
+ 
+ 		if (hrq->entry_count != drq->entry_count) {
+ 			status = -EINVAL;
+ 			goto out;
+ 		}
+ 
+ 		if (idx == 0) {
+ 			bf_set(lpfc_mbx_rq_create_num_pages,
+ 			       &rq_create->u.request,
+ 			       hrq->page_count);
+ 			bf_set(lpfc_mbx_rq_create_rq_cnt,
+ 			       &rq_create->u.request, (numrq * 2));
+ 			bf_set(lpfc_mbx_rq_create_dnb, &rq_create->u.request,
+ 			       1);
+ 			bf_set(lpfc_rq_context_base_cq,
+ 			       &rq_create->u.request.context,
+ 			       cq->queue_id);
+ 			bf_set(lpfc_rq_context_data_size,
+ 			       &rq_create->u.request.context,
+ 			       LPFC_DATA_BUF_SIZE);
+ 			bf_set(lpfc_rq_context_hdr_size,
+ 			       &rq_create->u.request.context,
+ 			       LPFC_HDR_BUF_SIZE);
+ 			bf_set(lpfc_rq_context_rqe_count_1,
+ 			       &rq_create->u.request.context,
+ 			       hrq->entry_count);
+ 			bf_set(lpfc_rq_context_rqe_size,
+ 			       &rq_create->u.request.context,
+ 			       LPFC_RQE_SIZE_8);
+ 			bf_set(lpfc_rq_context_page_size,
+ 			       &rq_create->u.request.context,
+ 			       (PAGE_SIZE/SLI4_PAGE_SIZE));
+ 		}
+ 		rc = 0;
+ 		list_for_each_entry(dmabuf, &hrq->page_list, list) {
+ 			memset(dmabuf->virt, 0, hw_page_size);
+ 			cnt = page_idx + dmabuf->buffer_tag;
+ 			rq_create->u.request.page[cnt].addr_lo =
+ 					putPaddrLow(dmabuf->phys);
+ 			rq_create->u.request.page[cnt].addr_hi =
+ 					putPaddrHigh(dmabuf->phys);
+ 			rc++;
+ 		}
+ 		page_idx += rc;
+ 
+ 		rc = 0;
+ 		list_for_each_entry(dmabuf, &drq->page_list, list) {
+ 			memset(dmabuf->virt, 0, hw_page_size);
+ 			cnt = page_idx + dmabuf->buffer_tag;
+ 			rq_create->u.request.page[cnt].addr_lo =
+ 					putPaddrLow(dmabuf->phys);
+ 			rq_create->u.request.page[cnt].addr_hi =
+ 					putPaddrHigh(dmabuf->phys);
+ 			rc++;
+ 		}
+ 		page_idx += rc;
+ 
+ 		hrq->db_format = LPFC_DB_RING_FORMAT;
+ 		hrq->db_regaddr = phba->sli4_hba.RQDBregaddr;
+ 		hrq->type = LPFC_HRQ;
+ 		hrq->assoc_qid = cq->queue_id;
+ 		hrq->subtype = subtype;
+ 		hrq->host_index = 0;
+ 		hrq->hba_index = 0;
+ 		hrq->entry_repost = LPFC_RQ_REPOST;
+ 
+ 		drq->db_format = LPFC_DB_RING_FORMAT;
+ 		drq->db_regaddr = phba->sli4_hba.RQDBregaddr;
+ 		drq->type = LPFC_DRQ;
+ 		drq->assoc_qid = cq->queue_id;
+ 		drq->subtype = subtype;
+ 		drq->host_index = 0;
+ 		drq->hba_index = 0;
+ 		drq->entry_repost = LPFC_RQ_REPOST;
+ 
+ 		list_add_tail(&hrq->list, &cq->child_list);
+ 		list_add_tail(&drq->list, &cq->child_list);
+ 	}
+ 
+ 	rc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);
+ 	/* The IOCTL status is embedded in the mailbox subheader. */
+ 	shdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);
+ 	shdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);
+ 	if (shdr_status || shdr_add_status || rc) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"3120 RQ_CREATE mailbox failed with "
+ 				"status x%x add_status x%x, mbx status x%x\n",
+ 				shdr_status, shdr_add_status, rc);
+ 		status = -ENXIO;
+ 		goto out;
+ 	}
+ 	rc = bf_get(lpfc_mbx_rq_create_q_id, &rq_create->u.response);
+ 	if (rc == 0xFFFF) {
+ 		status = -ENXIO;
+ 		goto out;
+ 	}
+ 
+ 	/* Initialize all RQs with associated queue id */
+ 	for (idx = 0; idx < numrq; idx++) {
+ 		hrq = hrqp[idx];
+ 		hrq->queue_id = rc + (2 * idx);
+ 		drq = drqp[idx];
+ 		drq->queue_id = rc + (2 * idx) + 1;
+ 	}
+ 
+ out:
+ 	lpfc_sli4_mbox_cmd_free(phba, mbox);
+ 	return status;
+ }
+ 
+ /**
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
   * lpfc_eq_destroy - Destroy an event Queue on the HBA
   * @eq: The queue structure associated with the queue to destroy.
   *
diff --cc drivers/scsi/lpfc/lpfc_sli4.h
index 10078254ebc7,422bde85c9f1..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@@ -135,9 -156,9 +135,10 @@@ struct lpfc_queue 
  	uint32_t entry_size;	/* Size of each queue entry. */
  	uint32_t entry_repost;	/* Count of entries before doorbell is rung */
  #define LPFC_QUEUE_MIN_REPOST	8
+ #define LPFC_RQ_REPOST		64
  	uint32_t queue_id;	/* Queue ID assigned by the hardware */
  	uint32_t assoc_qid;     /* Queue ID associated with, for CQ/WQ/MQ */
 +	struct list_head page_list;
  	uint32_t page_count;	/* Number of pages allocated for this queue */
  	uint32_t host_index;	/* The host's index for putting or getting */
  	uint32_t hba_index;	/* The last known hba index for get or put */
@@@ -704,7 -761,9 +705,13 @@@ int lpfc_wq_create(struct lpfc_hba *, s
  			struct lpfc_queue *, uint32_t);
  int lpfc_rq_create(struct lpfc_hba *, struct lpfc_queue *,
  			struct lpfc_queue *, struct lpfc_queue *, uint32_t);
++<<<<<<< HEAD
 +void lpfc_rq_adjust_repost(struct lpfc_hba *, struct lpfc_queue *, int);
++=======
+ int lpfc_mrq_create(struct lpfc_hba *phba, struct lpfc_queue **hrqp,
+ 			struct lpfc_queue **drqp, struct lpfc_queue **cqp,
+ 			uint32_t subtype);
++>>>>>>> 61f3d4bf4f8f (scsi: lpfc: Fix nvmet RQ resource needs for large block writes.)
  int lpfc_eq_destroy(struct lpfc_hba *, struct lpfc_queue *);
  int lpfc_cq_destroy(struct lpfc_hba *, struct lpfc_queue *);
  int lpfc_mq_destroy(struct lpfc_hba *, struct lpfc_queue *);
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc_attr.c
* Unmerged path drivers/scsi/lpfc/lpfc_init.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli4.h
