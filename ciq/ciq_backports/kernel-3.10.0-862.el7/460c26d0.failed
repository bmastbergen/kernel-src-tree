iommu/amd: Rip out old queue flushing code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] amd: Rip out old queue flushing code (Suravee Suthikulpanit) [1508644]
Rebuild_FUZZ: 92.31%
commit-author Joerg Roedel <jroedel@suse.de>
commit 460c26d05bf7357a4d5b41b3d7a97727f5bdffe1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/460c26d0.failed

The queue flushing is pretty inefficient when it flushes the
queues for all cpus at once. Further it flushes all domains
from all IOMMUs for all CPUs, which is overkill as well.

Rip it out to make room for something more efficient.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 460c26d05bf7357a4d5b41b3d7a97727f5bdffe1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index e80343c1de99,ec9da25c06a1..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -2626,7 -2380,16 +2626,20 @@@ static void __unmap_single(struct dma_o
  		start += PAGE_SIZE;
  	}
  
++<<<<<<< HEAD
 +	dma_ops_free_addresses(dma_dom, dma_addr, pages);
++=======
+ 	if (amd_iommu_unmap_flush) {
+ 		dma_ops_free_iova(dma_dom, dma_addr, pages);
+ 		domain_flush_tlb(&dma_dom->domain);
+ 		domain_flush_complete(&dma_dom->domain);
+ 	} else {
+ 		/* Keep the if() around, we need it later again */
+ 		dma_ops_free_iova(dma_dom, dma_addr, pages);
+ 		domain_flush_tlb(&dma_dom->domain);
+ 		domain_flush_complete(&dma_dom->domain);
+ 	}
++>>>>>>> 460c26d05bf7 (iommu/amd: Rip out old queue flushing code)
  }
  
  /*
@@@ -2854,20 -2721,31 +2867,46 @@@ static int set_dma_mask(struct device *
  	return 0;
  }
  
 +static struct dma_map_ops amd_iommu_dma_ops = {
 +	.alloc		= alloc_coherent,
 +	.free		= free_coherent,
 +	.map_page	= map_page,
 +	.unmap_page	= unmap_page,
 +	.map_sg		= map_sg,
 +	.unmap_sg	= unmap_sg,
 +	.dma_supported	= amd_iommu_dma_supported,
 +	.set_dma_mask	= set_dma_mask,
 +};
 +
  int __init amd_iommu_init_api(void)
  {
++<<<<<<< HEAD
 +	return bus_set_iommu(&pci_bus_type, &amd_iommu_ops);
++=======
+ 	int ret, err = 0;
+ 
+ 	ret = iova_cache_get();
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = init_reserved_iova_ranges();
+ 	if (ret)
+ 		return ret;
+ 
+ 	err = bus_set_iommu(&pci_bus_type, &amd_iommu_ops);
+ 	if (err)
+ 		return err;
+ #ifdef CONFIG_ARM_AMBA
+ 	err = bus_set_iommu(&amba_bustype, &amd_iommu_ops);
+ 	if (err)
+ 		return err;
+ #endif
+ 	err = bus_set_iommu(&platform_bus_type, &amd_iommu_ops);
+ 	if (err)
+ 		return err;
+ 
+ 	return 0;
++>>>>>>> 460c26d05bf7 (iommu/amd: Rip out old queue flushing code)
  }
  
  int __init amd_iommu_init_dma_ops(void)
@@@ -3026,7 -2905,8 +3065,12 @@@ static void amd_iommu_domain_free(struc
  
  	switch (dom->type) {
  	case IOMMU_DOMAIN_DMA:
++<<<<<<< HEAD
 +		dma_dom = domain->priv;
++=======
+ 		/* Now release the domain */
+ 		dma_dom = to_dma_ops_domain(domain);
++>>>>>>> 460c26d05bf7 (iommu/amd: Rip out old queue flushing code)
  		dma_ops_domain_free(dma_dom);
  		break;
  	default:
* Unmerged path drivers/iommu/amd_iommu.c
