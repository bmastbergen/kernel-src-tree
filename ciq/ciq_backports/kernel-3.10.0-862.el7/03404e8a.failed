IB/mlx5: Add support to dropless RQ

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Maor Gottlieb <maorg@mellanox.com>
commit 03404e8ae652e02a5e3388224836cef53d7a0988
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/03404e8a.failed

RQs that were configured for "delay drop" will prevent packet drops
when their WQEs are depleted.
Marking an RQ to be drop-less is done by setting delay_drop_en in RQ
context using CREATE_RQ command.

Since this feature is globally activated/deactivated by using the
SET_DELAY_DROP command on all the marked RQs, we activated/deactivated
it according to the number of RQs with 'delay_drop' enabled.

When timeout is expired, then the feature is deactivated. Therefore
the driver handles the delay drop timeout event and reactivate it.

	Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
	Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 03404e8ae652e02a5e3388224836cef53d7a0988)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --cc drivers/infiniband/hw/mlx5/main.c
index 6fac05a1178c,ad4b12decc23..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -3270,6 -3524,153 +3298,156 @@@ dealloc_counters
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static struct rdma_hw_stats *mlx5_ib_alloc_hw_stats(struct ib_device *ibdev,
+ 						    u8 port_num)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx5_ib_port *port = &dev->port[port_num - 1];
+ 
+ 	/* We support only per port stats */
+ 	if (port_num == 0)
+ 		return NULL;
+ 
+ 	return rdma_alloc_hw_stats_struct(port->cnts.names,
+ 					  port->cnts.num_q_counters +
+ 					  port->cnts.num_cong_counters,
+ 					  RDMA_HW_STATS_DEFAULT_LIFESPAN);
+ }
+ 
+ static int mlx5_ib_query_q_counters(struct mlx5_ib_dev *dev,
+ 				    struct mlx5_ib_port *port,
+ 				    struct rdma_hw_stats *stats)
+ {
+ 	int outlen = MLX5_ST_SZ_BYTES(query_q_counter_out);
+ 	void *out;
+ 	__be32 val;
+ 	int ret, i;
+ 
+ 	out = kvzalloc(outlen, GFP_KERNEL);
+ 	if (!out)
+ 		return -ENOMEM;
+ 
+ 	ret = mlx5_core_query_q_counter(dev->mdev,
+ 					port->cnts.set_id, 0,
+ 					out, outlen);
+ 	if (ret)
+ 		goto free;
+ 
+ 	for (i = 0; i < port->cnts.num_q_counters; i++) {
+ 		val = *(__be32 *)(out + port->cnts.offsets[i]);
+ 		stats->value[i] = (u64)be32_to_cpu(val);
+ 	}
+ 
+ free:
+ 	kvfree(out);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_query_cong_counters(struct mlx5_ib_dev *dev,
+ 				       struct mlx5_ib_port *port,
+ 				       struct rdma_hw_stats *stats)
+ {
+ 	int outlen = MLX5_ST_SZ_BYTES(query_cong_statistics_out);
+ 	void *out;
+ 	int ret, i;
+ 	int offset = port->cnts.num_q_counters;
+ 
+ 	out = kvzalloc(outlen, GFP_KERNEL);
+ 	if (!out)
+ 		return -ENOMEM;
+ 
+ 	ret = mlx5_cmd_query_cong_counter(dev->mdev, false, out, outlen);
+ 	if (ret)
+ 		goto free;
+ 
+ 	for (i = 0; i < port->cnts.num_cong_counters; i++) {
+ 		stats->value[i + offset] =
+ 			be64_to_cpup((__be64 *)(out +
+ 				     port->cnts.offsets[i + offset]));
+ 	}
+ 
+ free:
+ 	kvfree(out);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_get_hw_stats(struct ib_device *ibdev,
+ 				struct rdma_hw_stats *stats,
+ 				u8 port_num, int index)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx5_ib_port *port = &dev->port[port_num - 1];
+ 	int ret, num_counters;
+ 
+ 	if (!stats)
+ 		return -EINVAL;
+ 
+ 	ret = mlx5_ib_query_q_counters(dev, port, stats);
+ 	if (ret)
+ 		return ret;
+ 	num_counters = port->cnts.num_q_counters;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, cc_query_allowed)) {
+ 		ret = mlx5_ib_query_cong_counters(dev, port, stats);
+ 		if (ret)
+ 			return ret;
+ 		num_counters += port->cnts.num_cong_counters;
+ 	}
+ 
+ 	return num_counters;
+ }
+ 
+ static void mlx5_ib_free_rdma_netdev(struct net_device *netdev)
+ {
+ 	return mlx5_rdma_netdev_free(netdev);
+ }
+ 
+ static struct net_device*
+ mlx5_ib_alloc_rdma_netdev(struct ib_device *hca,
+ 			  u8 port_num,
+ 			  enum rdma_netdev_t type,
+ 			  const char *name,
+ 			  unsigned char name_assign_type,
+ 			  void (*setup)(struct net_device *))
+ {
+ 	struct net_device *netdev;
+ 	struct rdma_netdev *rn;
+ 
+ 	if (type != RDMA_NETDEV_IPOIB)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	netdev = mlx5_rdma_netdev_alloc(to_mdev(hca)->mdev, hca,
+ 					name, setup);
+ 	if (likely(!IS_ERR_OR_NULL(netdev))) {
+ 		rn = netdev_priv(netdev);
+ 		rn->free_rdma_netdev = mlx5_ib_free_rdma_netdev;
+ 	}
+ 	return netdev;
+ }
+ 
+ static void cancel_delay_drop(struct mlx5_ib_dev *dev)
+ {
+ 	if (!(dev->ib_dev.attrs.raw_packet_caps & IB_RAW_PACKET_CAP_DELAY_DROP))
+ 		return;
+ 
+ 	cancel_work_sync(&dev->delay_drop.delay_drop_work);
+ }
+ 
+ static void init_delay_drop(struct mlx5_ib_dev *dev)
+ {
+ 	if (!(dev->ib_dev.attrs.raw_packet_caps & IB_RAW_PACKET_CAP_DELAY_DROP))
+ 		return;
+ 
+ 	mutex_init(&dev->delay_drop.lock);
+ 	dev->delay_drop.dev = dev;
+ 	dev->delay_drop.activate = false;
+ 	dev->delay_drop.timeout = MLX5_MAX_DELAY_DROP_TIMEOUT_MS * 1000;
+ 	INIT_WORK(&dev->delay_drop.delay_drop_work, delay_drop_handler);
+ }
+ 
++>>>>>>> 03404e8ae652 (IB/mlx5: Add support to dropless RQ)
  static void *mlx5_ib_add(struct mlx5_core_dev *mdev)
  {
  	struct mlx5_ib_dev *dev;
@@@ -3494,9 -3916,13 +3674,9 @@@
  		err = device_create_file(&dev->ib_dev.dev,
  					 mlx5_class_attributes[i]);
  		if (err)
- 			goto err_umrc;
+ 			goto err_delay_drop;
  	}
  
 -	if ((MLX5_CAP_GEN(mdev, port_type) == MLX5_CAP_PORT_TYPE_ETH) &&
 -	    MLX5_CAP_GEN(mdev, disable_local_lb))
 -		mutex_init(&dev->lb_mutex);
 -
  	dev->ib_active = true;
  
  	return dev;
@@@ -3537,10 -3975,15 +3718,11 @@@ static void mlx5_ib_remove(struct mlx5_
  	struct mlx5_ib_dev *dev = context;
  	enum rdma_link_layer ll = mlx5_ib_port_link_layer(&dev->ib_dev, 1);
  
+ 	cancel_delay_drop(dev);
  	mlx5_remove_netdev_notifier(dev);
  	ib_unregister_device(&dev->ib_dev);
 -	mlx5_free_bfreg(dev->mdev, &dev->fp_bfreg);
 -	mlx5_free_bfreg(dev->mdev, &dev->bfreg);
 -	mlx5_put_uars_page(dev->mdev, mdev->priv.uar);
 -	mlx5_ib_cleanup_cong_debugfs(dev);
  	if (MLX5_CAP_GEN(dev->mdev, max_qp_cnt))
 -		mlx5_ib_dealloc_counters(dev);
 +		mlx5_ib_dealloc_q_counters(dev);
  	destroy_umrc_res(dev);
  	mlx5_ib_odp_remove_one(dev);
  	destroy_dev_resources(&dev->devr);
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 83418c5208ab,097f12dc65a6..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -620,6 -624,52 +625,55 @@@ struct mlx5_roce 
  	enum ib_port_state last_port_state;
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5_ib_dbg_param {
+ 	int			offset;
+ 	struct mlx5_ib_dev	*dev;
+ 	struct dentry		*dentry;
+ };
+ 
+ enum mlx5_ib_dbg_cc_types {
+ 	MLX5_IB_DBG_CC_RP_CLAMP_TGT_RATE,
+ 	MLX5_IB_DBG_CC_RP_CLAMP_TGT_RATE_ATI,
+ 	MLX5_IB_DBG_CC_RP_TIME_RESET,
+ 	MLX5_IB_DBG_CC_RP_BYTE_RESET,
+ 	MLX5_IB_DBG_CC_RP_THRESHOLD,
+ 	MLX5_IB_DBG_CC_RP_AI_RATE,
+ 	MLX5_IB_DBG_CC_RP_HAI_RATE,
+ 	MLX5_IB_DBG_CC_RP_MIN_DEC_FAC,
+ 	MLX5_IB_DBG_CC_RP_MIN_RATE,
+ 	MLX5_IB_DBG_CC_RP_RATE_TO_SET_ON_FIRST_CNP,
+ 	MLX5_IB_DBG_CC_RP_DCE_TCP_G,
+ 	MLX5_IB_DBG_CC_RP_DCE_TCP_RTT,
+ 	MLX5_IB_DBG_CC_RP_RATE_REDUCE_MONITOR_PERIOD,
+ 	MLX5_IB_DBG_CC_RP_INITIAL_ALPHA_VALUE,
+ 	MLX5_IB_DBG_CC_RP_GD,
+ 	MLX5_IB_DBG_CC_NP_CNP_DSCP,
+ 	MLX5_IB_DBG_CC_NP_CNP_PRIO_MODE,
+ 	MLX5_IB_DBG_CC_NP_CNP_PRIO,
+ 	MLX5_IB_DBG_CC_MAX,
+ };
+ 
+ struct mlx5_ib_dbg_cc_params {
+ 	struct dentry			*root;
+ 	struct mlx5_ib_dbg_param	params[MLX5_IB_DBG_CC_MAX];
+ };
+ 
+ enum {
+ 	MLX5_MAX_DELAY_DROP_TIMEOUT_MS = 100,
+ };
+ 
+ struct mlx5_ib_delay_drop {
+ 	struct mlx5_ib_dev     *dev;
+ 	struct work_struct	delay_drop_work;
+ 	/* serialize setting of delay drop */
+ 	struct mutex		lock;
+ 	u32			timeout;
+ 	bool			activate;
+ };
+ 
++>>>>>>> 03404e8ae652 (IB/mlx5: Add support to dropless RQ)
  struct mlx5_ib_dev {
  	struct ib_device		ib_dev;
  	struct mlx5_core_dev		*mdev;
@@@ -654,6 -704,15 +708,18 @@@
  	struct list_head	qp_list;
  	/* Array with num_ports elements */
  	struct mlx5_ib_port	*port;
++<<<<<<< HEAD
++=======
+ 	struct mlx5_sq_bfreg	bfreg;
+ 	struct mlx5_sq_bfreg	fp_bfreg;
+ 	struct mlx5_ib_delay_drop	delay_drop;
+ 	struct mlx5_ib_dbg_cc_params	*dbg_cc_params;
+ 
+ 	/* protect the user_td */
+ 	struct mutex		lb_mutex;
+ 	u32			user_td;
+ 	u8			umr_fence;
++>>>>>>> 03404e8ae652 (IB/mlx5: Add support to dropless RQ)
  };
  
  static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --git a/drivers/infiniband/hw/mlx5/qp.c b/drivers/infiniband/hw/mlx5/qp.c
index 50a7f77d9dee..3f8d2d514fab 100644
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -4688,6 +4688,24 @@ static void mlx5_ib_wq_event(struct mlx5_core_qp *core_qp, int type)
 	}
 }
 
+static int set_delay_drop(struct mlx5_ib_dev *dev)
+{
+	int err = 0;
+
+	mutex_lock(&dev->delay_drop.lock);
+	if (dev->delay_drop.activate)
+		goto out;
+
+	err = mlx5_core_set_delay_drop(dev->mdev, dev->delay_drop.timeout);
+	if (err)
+		goto out;
+
+	dev->delay_drop.activate = true;
+out:
+	mutex_unlock(&dev->delay_drop.lock);
+	return err;
+}
+
 static int  create_rq(struct mlx5_ib_rwq *rwq, struct ib_pd *pd,
 		      struct ib_wq_init_attr *init_attr)
 {
@@ -4742,9 +4760,28 @@ static int  create_rq(struct mlx5_ib_rwq *rwq, struct ib_pd *pd,
 		}
 		MLX5_SET(rqc, rqc, scatter_fcs, 1);
 	}
+	if (init_attr->create_flags & IB_WQ_FLAGS_DELAY_DROP) {
+		if (!(dev->ib_dev.attrs.raw_packet_caps &
+		      IB_RAW_PACKET_CAP_DELAY_DROP)) {
+			mlx5_ib_dbg(dev, "Delay drop is not supported\n");
+			err = -EOPNOTSUPP;
+			goto out;
+		}
+		MLX5_SET(rqc, rqc, delay_drop_en, 1);
+	}
 	rq_pas0 = (__be64 *)MLX5_ADDR_OF(wq, wq, pas);
 	mlx5_ib_populate_pas(dev, rwq->umem, rwq->page_shift, rq_pas0, 0);
 	err = mlx5_core_create_rq_tracked(dev->mdev, in, inlen, &rwq->core_qp);
+	if (!err && init_attr->create_flags & IB_WQ_FLAGS_DELAY_DROP) {
+		err = set_delay_drop(dev);
+		if (err) {
+			mlx5_ib_warn(dev, "Failed to enable delay drop err=%d\n",
+				     err);
+			mlx5_core_destroy_rq_tracked(dev->mdev, &rwq->core_qp);
+		} else {
+			rwq->create_flags |= MLX5_IB_WQ_FLAGS_DELAY_DROP;
+		}
+	}
 out:
 	kvfree(in);
 	return err;
diff --git a/include/linux/mlx5/mlx5_ifc.h b/include/linux/mlx5/mlx5_ifc.h
index 7814f59a455b..cec2adf9c6a9 100644
--- a/include/linux/mlx5/mlx5_ifc.h
+++ b/include/linux/mlx5/mlx5_ifc.h
@@ -2497,7 +2497,7 @@ enum {
 
 struct mlx5_ifc_rqc_bits {
 	u8         rlky[0x1];
-	u8         reserved_at_1[0x1];
+	u8	   delay_drop_en[0x1];
 	u8         scatter_fcs[0x1];
 	u8         vsd[0x1];
 	u8         mem_rq_type[0x4];
