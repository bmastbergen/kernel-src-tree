iommu/iova: Disable preemption around use of this_cpu_ptr()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] iova: Disable preemption around use of this_cpu_ptr() (Jerry Snitselaar) [1499325]
Rebuild_FUZZ: 94.64%
commit-author Chris Wilson <chris@chris-wilson.co.uk>
commit 583248e6620a4726093295e2d6785fcbc2e86428
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/583248e6.failed

Between acquiring the this_cpu_ptr() and using it, ideally we don't want
to be preempted and work on another CPU's private data. this_cpu_ptr()
checks whether or not preemption is disable, and get_cpu_ptr() provides
a convenient wrapper for operating on the cpu ptr inside a preemption
disabled critical section (which currently is provided by the
spinlock).

[  167.997877] BUG: using smp_processor_id() in preemptible [00000000] code: usb-storage/216
[  167.997940] caller is debug_smp_processor_id+0x17/0x20
[  167.997945] CPU: 7 PID: 216 Comm: usb-storage Tainted: G     U          4.7.0-rc1-gfxbench-RO_Patchwork_1057+ #1
[  167.997948] Hardware name: Hewlett-Packard HP Pro 3500 Series/2ABF, BIOS 8.11 10/24/2012
[  167.997951]  0000000000000000 ffff880118b7f9c8 ffffffff8140dca5 0000000000000007
[  167.997958]  ffffffff81a3a7e9 ffff880118b7f9f8 ffffffff8142a927 0000000000000000
[  167.997965]  ffff8800d499ed58 0000000000000001 00000000000fffff ffff880118b7fa08
[  167.997971] Call Trace:
[  167.997977]  [<ffffffff8140dca5>] dump_stack+0x67/0x92
[  167.997981]  [<ffffffff8142a927>] check_preemption_disabled+0xd7/0xe0
[  167.997985]  [<ffffffff8142a947>] debug_smp_processor_id+0x17/0x20
[  167.997990]  [<ffffffff81507e17>] alloc_iova_fast+0xb7/0x210
[  167.997994]  [<ffffffff8150c55f>] intel_alloc_iova+0x7f/0xd0
[  167.997998]  [<ffffffff8151021d>] intel_map_sg+0xbd/0x240
[  167.998002]  [<ffffffff810e5efd>] ? debug_lockdep_rcu_enabled+0x1d/0x20
[  167.998009]  [<ffffffff81596059>] usb_hcd_map_urb_for_dma+0x4b9/0x5a0
[  167.998013]  [<ffffffff81596d19>] usb_hcd_submit_urb+0xe9/0xaa0
[  167.998017]  [<ffffffff810cff2f>] ? mark_held_locks+0x6f/0xa0
[  167.998022]  [<ffffffff810d525c>] ? __raw_spin_lock_init+0x1c/0x50
[  167.998025]  [<ffffffff810e5efd>] ? debug_lockdep_rcu_enabled+0x1d/0x20
[  167.998028]  [<ffffffff815988f3>] usb_submit_urb+0x3f3/0x5a0
[  167.998032]  [<ffffffff810d0082>] ? trace_hardirqs_on_caller+0x122/0x1b0
[  167.998035]  [<ffffffff81599ae7>] usb_sg_wait+0x67/0x150
[  167.998039]  [<ffffffff815dc202>] usb_stor_bulk_transfer_sglist.part.3+0x82/0xd0
[  167.998042]  [<ffffffff815dc29c>] usb_stor_bulk_srb+0x4c/0x60
[  167.998045]  [<ffffffff815dc42e>] usb_stor_Bulk_transport+0x17e/0x420
[  167.998049]  [<ffffffff815dcf32>] usb_stor_invoke_transport+0x242/0x540
[  167.998052]  [<ffffffff810e5efd>] ? debug_lockdep_rcu_enabled+0x1d/0x20
[  167.998058]  [<ffffffff815dba19>] usb_stor_transparent_scsi_command+0x9/0x10
[  167.998061]  [<ffffffff815de518>] usb_stor_control_thread+0x158/0x260
[  167.998064]  [<ffffffff815de3c0>] ? fill_inquiry_response+0x20/0x20
[  167.998067]  [<ffffffff815de3c0>] ? fill_inquiry_response+0x20/0x20
[  167.998071]  [<ffffffff8109ddfa>] kthread+0xea/0x100
[  167.998078]  [<ffffffff817ac6af>] ret_from_fork+0x1f/0x40
[  167.998081]  [<ffffffff8109dd10>] ? kthread_create_on_node+0x1f0/0x1f0

Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=96293
	Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
	Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
	Cc: Joerg Roedel <joro@8bytes.org>
	Cc: iommu@lists.linux-foundation.org
	Cc: linux-kernel@vger.kernel.org
Fixes: 9257b4a206fc ('iommu/iova: introduce per-cpu caching to iova allocation')
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 583248e6620a4726093295e2d6785fcbc2e86428)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
diff --cc drivers/iommu/iova.c
index d4833981e720,e23001bfcfee..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -361,6 -388,66 +361,69 @@@ free_iova(struct iova_domain *iovad, un
  		__free_iova(iovad, iova);
  
  }
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL_GPL(free_iova);
+ 
+ /**
+  * alloc_iova_fast - allocates an iova from rcache
+  * @iovad: - iova domain in question
+  * @size: - size of page frames to allocate
+  * @limit_pfn: - max limit address
+  * This function tries to satisfy an iova allocation from the rcache,
+  * and falls back to regular allocation on failure.
+ */
+ unsigned long
+ alloc_iova_fast(struct iova_domain *iovad, unsigned long size,
+ 		unsigned long limit_pfn)
+ {
+ 	bool flushed_rcache = false;
+ 	unsigned long iova_pfn;
+ 	struct iova *new_iova;
+ 
+ 	iova_pfn = iova_rcache_get(iovad, size, limit_pfn);
+ 	if (iova_pfn)
+ 		return iova_pfn;
+ 
+ retry:
+ 	new_iova = alloc_iova(iovad, size, limit_pfn, true);
+ 	if (!new_iova) {
+ 		unsigned int cpu;
+ 
+ 		if (flushed_rcache)
+ 			return 0;
+ 
+ 		/* Try replenishing IOVAs by flushing rcache. */
+ 		flushed_rcache = true;
+ 		preempt_disable();
+ 		for_each_online_cpu(cpu)
+ 			free_cpu_cached_iovas(cpu, iovad);
+ 		preempt_enable();
+ 		goto retry;
+ 	}
+ 
+ 	return new_iova->pfn_lo;
+ }
+ EXPORT_SYMBOL_GPL(alloc_iova_fast);
+ 
+ /**
+  * free_iova_fast - free iova pfn range into rcache
+  * @iovad: - iova domain in question.
+  * @pfn: - pfn that is allocated previously
+  * @size: - # of pages in range
+  * This functions frees an iova range by trying to put it into the rcache,
+  * falling back to regular iova deallocation via free_iova() if this fails.
+  */
+ void
+ free_iova_fast(struct iova_domain *iovad, unsigned long pfn, unsigned long size)
+ {
+ 	if (iova_rcache_insert(iovad, pfn, size))
+ 		return;
+ 
+ 	free_iova(iovad, pfn);
+ }
+ EXPORT_SYMBOL_GPL(free_iova_fast);
++>>>>>>> 583248e6620a (iommu/iova: Disable preemption around use of this_cpu_ptr())
  
  /**
   * put_iova_domain - destroys the iova doamin
@@@ -540,5 -631,297 +603,300 @@@ error
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Magazine caches for IOVA ranges.  For an introduction to magazines,
+  * see the USENIX 2001 paper "Magazines and Vmem: Extending the Slab
+  * Allocator to Many CPUs and Arbitrary Resources" by Bonwick and Adams.
+  * For simplicity, we use a static magazine size and don't implement the
+  * dynamic size tuning described in the paper.
+  */
+ 
+ #define IOVA_MAG_SIZE 128
+ 
+ struct iova_magazine {
+ 	unsigned long size;
+ 	unsigned long pfns[IOVA_MAG_SIZE];
+ };
+ 
+ struct iova_cpu_rcache {
+ 	spinlock_t lock;
+ 	struct iova_magazine *loaded;
+ 	struct iova_magazine *prev;
+ };
+ 
+ static struct iova_magazine *iova_magazine_alloc(gfp_t flags)
+ {
+ 	return kzalloc(sizeof(struct iova_magazine), flags);
+ }
+ 
+ static void iova_magazine_free(struct iova_magazine *mag)
+ {
+ 	kfree(mag);
+ }
+ 
+ static void
+ iova_magazine_free_pfns(struct iova_magazine *mag, struct iova_domain *iovad)
+ {
+ 	unsigned long flags;
+ 	int i;
+ 
+ 	if (!mag)
+ 		return;
+ 
+ 	spin_lock_irqsave(&iovad->iova_rbtree_lock, flags);
+ 
+ 	for (i = 0 ; i < mag->size; ++i) {
+ 		struct iova *iova = private_find_iova(iovad, mag->pfns[i]);
+ 
+ 		BUG_ON(!iova);
+ 		private_free_iova(iovad, iova);
+ 	}
+ 
+ 	spin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);
+ 
+ 	mag->size = 0;
+ }
+ 
+ static bool iova_magazine_full(struct iova_magazine *mag)
+ {
+ 	return (mag && mag->size == IOVA_MAG_SIZE);
+ }
+ 
+ static bool iova_magazine_empty(struct iova_magazine *mag)
+ {
+ 	return (!mag || mag->size == 0);
+ }
+ 
+ static unsigned long iova_magazine_pop(struct iova_magazine *mag,
+ 				       unsigned long limit_pfn)
+ {
+ 	BUG_ON(iova_magazine_empty(mag));
+ 
+ 	if (mag->pfns[mag->size - 1] >= limit_pfn)
+ 		return 0;
+ 
+ 	return mag->pfns[--mag->size];
+ }
+ 
+ static void iova_magazine_push(struct iova_magazine *mag, unsigned long pfn)
+ {
+ 	BUG_ON(iova_magazine_full(mag));
+ 
+ 	mag->pfns[mag->size++] = pfn;
+ }
+ 
+ static void init_iova_rcaches(struct iova_domain *iovad)
+ {
+ 	struct iova_cpu_rcache *cpu_rcache;
+ 	struct iova_rcache *rcache;
+ 	unsigned int cpu;
+ 	int i;
+ 
+ 	for (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {
+ 		rcache = &iovad->rcaches[i];
+ 		spin_lock_init(&rcache->lock);
+ 		rcache->depot_size = 0;
+ 		rcache->cpu_rcaches = __alloc_percpu(sizeof(*cpu_rcache), cache_line_size());
+ 		if (WARN_ON(!rcache->cpu_rcaches))
+ 			continue;
+ 		for_each_possible_cpu(cpu) {
+ 			cpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);
+ 			spin_lock_init(&cpu_rcache->lock);
+ 			cpu_rcache->loaded = iova_magazine_alloc(GFP_KERNEL);
+ 			cpu_rcache->prev = iova_magazine_alloc(GFP_KERNEL);
+ 		}
+ 	}
+ }
+ 
+ /*
+  * Try inserting IOVA range starting with 'iova_pfn' into 'rcache', and
+  * return true on success.  Can fail if rcache is full and we can't free
+  * space, and free_iova() (our only caller) will then return the IOVA
+  * range to the rbtree instead.
+  */
+ static bool __iova_rcache_insert(struct iova_domain *iovad,
+ 				 struct iova_rcache *rcache,
+ 				 unsigned long iova_pfn)
+ {
+ 	struct iova_magazine *mag_to_free = NULL;
+ 	struct iova_cpu_rcache *cpu_rcache;
+ 	bool can_insert = false;
+ 	unsigned long flags;
+ 
+ 	cpu_rcache = get_cpu_ptr(rcache->cpu_rcaches);
+ 	spin_lock_irqsave(&cpu_rcache->lock, flags);
+ 
+ 	if (!iova_magazine_full(cpu_rcache->loaded)) {
+ 		can_insert = true;
+ 	} else if (!iova_magazine_full(cpu_rcache->prev)) {
+ 		swap(cpu_rcache->prev, cpu_rcache->loaded);
+ 		can_insert = true;
+ 	} else {
+ 		struct iova_magazine *new_mag = iova_magazine_alloc(GFP_ATOMIC);
+ 
+ 		if (new_mag) {
+ 			spin_lock(&rcache->lock);
+ 			if (rcache->depot_size < MAX_GLOBAL_MAGS) {
+ 				rcache->depot[rcache->depot_size++] =
+ 						cpu_rcache->loaded;
+ 			} else {
+ 				mag_to_free = cpu_rcache->loaded;
+ 			}
+ 			spin_unlock(&rcache->lock);
+ 
+ 			cpu_rcache->loaded = new_mag;
+ 			can_insert = true;
+ 		}
+ 	}
+ 
+ 	if (can_insert)
+ 		iova_magazine_push(cpu_rcache->loaded, iova_pfn);
+ 
+ 	spin_unlock_irqrestore(&cpu_rcache->lock, flags);
+ 	put_cpu_ptr(rcache->cpu_rcaches);
+ 
+ 	if (mag_to_free) {
+ 		iova_magazine_free_pfns(mag_to_free, iovad);
+ 		iova_magazine_free(mag_to_free);
+ 	}
+ 
+ 	return can_insert;
+ }
+ 
+ static bool iova_rcache_insert(struct iova_domain *iovad, unsigned long pfn,
+ 			       unsigned long size)
+ {
+ 	unsigned int log_size = order_base_2(size);
+ 
+ 	if (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)
+ 		return false;
+ 
+ 	return __iova_rcache_insert(iovad, &iovad->rcaches[log_size], pfn);
+ }
+ 
+ /*
+  * Caller wants to allocate a new IOVA range from 'rcache'.  If we can
+  * satisfy the request, return a matching non-NULL range and remove
+  * it from the 'rcache'.
+  */
+ static unsigned long __iova_rcache_get(struct iova_rcache *rcache,
+ 				       unsigned long limit_pfn)
+ {
+ 	struct iova_cpu_rcache *cpu_rcache;
+ 	unsigned long iova_pfn = 0;
+ 	bool has_pfn = false;
+ 	unsigned long flags;
+ 
+ 	cpu_rcache = get_cpu_ptr(rcache->cpu_rcaches);
+ 	spin_lock_irqsave(&cpu_rcache->lock, flags);
+ 
+ 	if (!iova_magazine_empty(cpu_rcache->loaded)) {
+ 		has_pfn = true;
+ 	} else if (!iova_magazine_empty(cpu_rcache->prev)) {
+ 		swap(cpu_rcache->prev, cpu_rcache->loaded);
+ 		has_pfn = true;
+ 	} else {
+ 		spin_lock(&rcache->lock);
+ 		if (rcache->depot_size > 0) {
+ 			iova_magazine_free(cpu_rcache->loaded);
+ 			cpu_rcache->loaded = rcache->depot[--rcache->depot_size];
+ 			has_pfn = true;
+ 		}
+ 		spin_unlock(&rcache->lock);
+ 	}
+ 
+ 	if (has_pfn)
+ 		iova_pfn = iova_magazine_pop(cpu_rcache->loaded, limit_pfn);
+ 
+ 	spin_unlock_irqrestore(&cpu_rcache->lock, flags);
+ 	put_cpu_ptr(rcache->cpu_rcaches);
+ 
+ 	return iova_pfn;
+ }
+ 
+ /*
+  * Try to satisfy IOVA allocation range from rcache.  Fail if requested
+  * size is too big or the DMA limit we are given isn't satisfied by the
+  * top element in the magazine.
+  */
+ static unsigned long iova_rcache_get(struct iova_domain *iovad,
+ 				     unsigned long size,
+ 				     unsigned long limit_pfn)
+ {
+ 	unsigned int log_size = order_base_2(size);
+ 
+ 	if (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)
+ 		return 0;
+ 
+ 	return __iova_rcache_get(&iovad->rcaches[log_size], limit_pfn);
+ }
+ 
+ /*
+  * Free a cpu's rcache.
+  */
+ static void free_cpu_iova_rcache(unsigned int cpu, struct iova_domain *iovad,
+ 				 struct iova_rcache *rcache)
+ {
+ 	struct iova_cpu_rcache *cpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&cpu_rcache->lock, flags);
+ 
+ 	iova_magazine_free_pfns(cpu_rcache->loaded, iovad);
+ 	iova_magazine_free(cpu_rcache->loaded);
+ 
+ 	iova_magazine_free_pfns(cpu_rcache->prev, iovad);
+ 	iova_magazine_free(cpu_rcache->prev);
+ 
+ 	spin_unlock_irqrestore(&cpu_rcache->lock, flags);
+ }
+ 
+ /*
+  * free rcache data structures.
+  */
+ static void free_iova_rcaches(struct iova_domain *iovad)
+ {
+ 	struct iova_rcache *rcache;
+ 	unsigned long flags;
+ 	unsigned int cpu;
+ 	int i, j;
+ 
+ 	for (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {
+ 		rcache = &iovad->rcaches[i];
+ 		for_each_possible_cpu(cpu)
+ 			free_cpu_iova_rcache(cpu, iovad, rcache);
+ 		spin_lock_irqsave(&rcache->lock, flags);
+ 		free_percpu(rcache->cpu_rcaches);
+ 		for (j = 0; j < rcache->depot_size; ++j) {
+ 			iova_magazine_free_pfns(rcache->depot[j], iovad);
+ 			iova_magazine_free(rcache->depot[j]);
+ 		}
+ 		spin_unlock_irqrestore(&rcache->lock, flags);
+ 	}
+ }
+ 
+ /*
+  * free all the IOVA ranges cached by a cpu (used when cpu is unplugged)
+  */
+ void free_cpu_cached_iovas(unsigned int cpu, struct iova_domain *iovad)
+ {
+ 	struct iova_cpu_rcache *cpu_rcache;
+ 	struct iova_rcache *rcache;
+ 	unsigned long flags;
+ 	int i;
+ 
+ 	for (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {
+ 		rcache = &iovad->rcaches[i];
+ 		cpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);
+ 		spin_lock_irqsave(&cpu_rcache->lock, flags);
+ 		iova_magazine_free_pfns(cpu_rcache->loaded, iovad);
+ 		iova_magazine_free_pfns(cpu_rcache->prev, iovad);
+ 		spin_unlock_irqrestore(&cpu_rcache->lock, flags);
+ 	}
+ }
+ 
++>>>>>>> 583248e6620a (iommu/iova: Disable preemption around use of this_cpu_ptr())
  MODULE_AUTHOR("Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>");
  MODULE_LICENSE("GPL");
* Unmerged path drivers/iommu/iova.c
