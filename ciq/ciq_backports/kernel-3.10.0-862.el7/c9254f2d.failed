block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [block] Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag (Ming Lei) [1491296]
Rebuild_FUZZ: 93.46%
commit-author Bart Van Assche <bart.vanassche@wdc.com>
commit c9254f2ddb19387ea9714a57ea48463c20333b92
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c9254f2d.failed

This flag will be used in the next patch to let the block layer
core know whether or not a SCSI request queue has been quiesced.
A quiesced SCSI queue namely only processes RQF_PREEMPT requests.

	Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Tested-by: Martin Steigerwald <martin@lichtvoll.de>
	Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
	Cc: Ming Lei <ming.lei@redhat.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit c9254f2ddb19387ea9714a57ea48463c20333b92)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-debugfs.c
#	include/linux/blkdev.h
diff --cc include/linux/blkdev.h
index 2d7fe01dd7d2,2147e2381a22..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -527,54 -580,61 +527,77 @@@ struct request_queue 
  #endif
  	struct rcu_head		rcu_head;
  	wait_queue_head_t	mq_freeze_wq;
 -	struct percpu_ref	q_usage_counter;
 +	RH_KABI_DEPRECATE(struct percpu_counter, mq_usage_counter)
  	struct list_head	all_q_node;
  
 -	struct blk_mq_tag_set	*tag_set;
 -	struct list_head	tag_set_list;
 -	struct bio_set		*bio_split;
 -
 -#ifdef CONFIG_BLK_DEBUG_FS
 -	struct dentry		*debugfs_dir;
 -	struct dentry		*sched_debugfs_dir;
 -#endif
 -
 -	bool			mq_sysfs_init_done;
 -
 -	size_t			cmd_size;
 -	void			*rq_alloc_data;
 -
 -	struct work_struct	release_work;
 -
 -#define BLK_MAX_WRITE_HINTS	5
 -	u64			write_hints[BLK_MAX_WRITE_HINTS];
 +	RH_KABI_EXTEND(unprep_rq_fn		*unprep_rq_fn)
 +
 +	RH_KABI_EXTEND(struct blk_mq_tag_set	*tag_set)
 +	RH_KABI_EXTEND(struct list_head		tag_set_list)
 +
 +	RH_KABI_EXTEND(struct list_head		requeue_list)
 +	RH_KABI_EXTEND(spinlock_t			requeue_lock)
 +	/* requeue_work's type is changed from 'work_struct' to 'delayed_work' below */
 +	RH_KABI_EXTEND(struct work_struct	rh_reserved_requeue_work)
 +	RH_KABI_EXTEND(atomic_t				mq_freeze_depth)
 +	RH_KABI_EXTEND(struct blk_flush_queue   *fq)
 +	RH_KABI_EXTEND(struct percpu_ref	q_usage_counter)
 +	RH_KABI_EXTEND(bool			mq_sysfs_init_done)
 +	RH_KABI_EXTEND(struct work_struct	timeout_work)
 +	RH_KABI_EXTEND(struct delayed_work	requeue_work)
  };
  
 -#define QUEUE_FLAG_QUEUED	0	/* uses generic tag queueing */
 -#define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
 -#define QUEUE_FLAG_DYING	2	/* queue being torn down */
 -#define QUEUE_FLAG_BYPASS	3	/* act as dumb FIFO queue */
 -#define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
 -#define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
 -#define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
 -#define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
 -#define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
 +#define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
 +#define QUEUE_FLAG_STOPPED	2	/* queue is stopped */
 +#define	QUEUE_FLAG_SYNCFULL	3	/* read queue has been filled */
 +#define QUEUE_FLAG_ASYNCFULL	4	/* write queue has been filled */
 +#define QUEUE_FLAG_DYING	5	/* queue being torn down */
 +#define QUEUE_FLAG_BYPASS	6	/* act as dumb FIFO queue */
 +#define QUEUE_FLAG_BIDI		7	/* queue supports bidi requests */
 +#define QUEUE_FLAG_NOMERGES     8	/* disable merge attempts */
 +#define QUEUE_FLAG_SAME_COMP	9	/* complete on same CPU-group */
 +#define QUEUE_FLAG_FAIL_IO     10	/* fake timeout */
 +#define QUEUE_FLAG_STACKABLE   11	/* supports request stacking */
 +#define QUEUE_FLAG_NONROT      12	/* non-rotational device (SSD) */
  #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
++<<<<<<< HEAD
 +#define QUEUE_FLAG_IO_STAT     13	/* do IO stats */
 +#define QUEUE_FLAG_DISCARD     14	/* supports DISCARD */
 +#define QUEUE_FLAG_NOXMERGES   15	/* No extended merges */
 +#define QUEUE_FLAG_ADD_RANDOM  16	/* Contributes to random pool */
 +#define QUEUE_FLAG_SECDISCARD  17	/* supports SECDISCARD */
 +#define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
 +#define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
 +#define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
 +#define QUEUE_FLAG_UNPRIV_SGIO 21	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NO_SG_MERGE 22	/* don't attempt to merge SG segments*/
 +#define QUEUE_FLAG_SG_GAPS     23	/* queue doesn't support SG gaps */
 +#define QUEUE_FLAG_DAX         24	/* device supports DAX */
++=======
+ #define QUEUE_FLAG_IO_STAT     10	/* do IO stats */
+ #define QUEUE_FLAG_DISCARD     11	/* supports DISCARD */
+ #define QUEUE_FLAG_NOXMERGES   12	/* No extended merges */
+ #define QUEUE_FLAG_ADD_RANDOM  13	/* Contributes to random pool */
+ #define QUEUE_FLAG_SECERASE    14	/* supports secure erase */
+ #define QUEUE_FLAG_SAME_FORCE  15	/* force complete on same CPU */
+ #define QUEUE_FLAG_DEAD        16	/* queue tear-down finished */
+ #define QUEUE_FLAG_INIT_DONE   17	/* queue is initialized */
+ #define QUEUE_FLAG_NO_SG_MERGE 18	/* don't attempt to merge SG segments*/
+ #define QUEUE_FLAG_POLL	       19	/* IO polling enabled if set */
+ #define QUEUE_FLAG_WC	       20	/* Write back caching */
+ #define QUEUE_FLAG_FUA	       21	/* device supports FUA writes */
+ #define QUEUE_FLAG_FLUSH_NQ    22	/* flush not queueuable */
+ #define QUEUE_FLAG_DAX         23	/* device supports DAX */
+ #define QUEUE_FLAG_STATS       24	/* track rq completion times */
+ #define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
+ #define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
+ #define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
+ #define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
+ #define QUEUE_FLAG_PREEMPT_ONLY	29	/* only process REQ_PREEMPT requests */
++>>>>>>> c9254f2ddb19 (block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag)
  
  #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 +				 (1 << QUEUE_FLAG_STACKABLE)	|	\
  				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
  				 (1 << QUEUE_FLAG_ADD_RANDOM))
  
@@@ -667,14 -732,17 +690,23 @@@ static inline void queue_flag_clear(uns
  #define blk_noretry_request(rq) \
  	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
  			     REQ_FAILFAST_DRIVER))
++<<<<<<< HEAD
++=======
+ #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+ #define blk_queue_preempt_only(q)				\
+ 	test_bit(QUEUE_FLAG_PREEMPT_ONLY, &(q)->queue_flags)
+ 
+ extern int blk_set_preempt_only(struct request_queue *q);
+ extern void blk_clear_preempt_only(struct request_queue *q);
++>>>>>>> c9254f2ddb19 (block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag)
  
 -static inline bool blk_account_rq(struct request *rq)
 -{
 -	return (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);
 -}
 +#define blk_account_rq(rq) \
 +	(((rq)->cmd_flags & REQ_STARTED) && \
 +	 ((rq)->cmd_type == REQ_TYPE_FS))
 +
 +#define blk_pm_request(rq)	\
 +	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND || \
 +	 (rq)->cmd_type == REQ_TYPE_PM_RESUME)
  
  #define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
  #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
* Unmerged path block/blk-mq-debugfs.c
diff --git a/block/blk-core.c b/block/blk-core.c
index df55e6267498..aabb04349019 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -259,6 +259,36 @@ void blk_sync_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_sync_queue);
 
+/**
+ * blk_set_preempt_only - set QUEUE_FLAG_PREEMPT_ONLY
+ * @q: request queue pointer
+ *
+ * Returns the previous value of the PREEMPT_ONLY flag - 0 if the flag was not
+ * set and 1 if the flag was already set.
+ */
+int blk_set_preempt_only(struct request_queue *q)
+{
+	unsigned long flags;
+	int res;
+
+	spin_lock_irqsave(q->queue_lock, flags);
+	res = queue_flag_test_and_set(QUEUE_FLAG_PREEMPT_ONLY, q);
+	spin_unlock_irqrestore(q->queue_lock, flags);
+
+	return res;
+}
+EXPORT_SYMBOL_GPL(blk_set_preempt_only);
+
+void blk_clear_preempt_only(struct request_queue *q)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(q->queue_lock, flags);
+	queue_flag_clear(QUEUE_FLAG_PREEMPT_ONLY, q);
+	spin_unlock_irqrestore(q->queue_lock, flags);
+}
+EXPORT_SYMBOL_GPL(blk_clear_preempt_only);
+
 /**
  * __blk_run_queue_uncond - run a queue whether or not it has been stopped
  * @q:	The queue to run
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path include/linux/blkdev.h
