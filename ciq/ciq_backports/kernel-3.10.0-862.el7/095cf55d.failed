KVM: x86: Hyper-V tsc page setup

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 095cf55df715d14d5dad75326faf5984e7fc0b3a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/095cf55d.failed

Lately tsc page was implemented but filled with empty
values. This patch setup tsc page scale and offset based
on vcpu tsc, tsc_khz and  HV_X64_MSR_TIME_REF_COUNT value.

The valid tsc page drops HV_X64_MSR_TIME_REF_COUNT msr
reads count to zero which potentially improves performance.

	Signed-off-by: Andrey Smetanin <asmetanin@virtuozzo.com>
	Reviewed-by: Peter Hornyack <peterhornyack@google.com>
	Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
CC: Paolo Bonzini <pbonzini@redhat.com>
CC: Roman Kagan <rkagan@virtuozzo.com>
CC: Denis V. Lunev <den@openvz.org>
[Computation of TSC page parameters rewritten to use the Linux timekeeper
 parameters. - Paolo]
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 095cf55df715d14d5dad75326faf5984e7fc0b3a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/hyperv.c
#	arch/x86/kvm/hyperv.h
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/hyperv.c
index 61529ab2e8e3,42b1c83741c8..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -30,6 -33,669 +30,672 @@@
  
  #include "trace.h"
  
++<<<<<<< HEAD
++=======
+ static inline u64 synic_read_sint(struct kvm_vcpu_hv_synic *synic, int sint)
+ {
+ 	return atomic64_read(&synic->sint[sint]);
+ }
+ 
+ static inline int synic_get_sint_vector(u64 sint_value)
+ {
+ 	if (sint_value & HV_SYNIC_SINT_MASKED)
+ 		return -1;
+ 	return sint_value & HV_SYNIC_SINT_VECTOR_MASK;
+ }
+ 
+ static bool synic_has_vector_connected(struct kvm_vcpu_hv_synic *synic,
+ 				      int vector)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static bool synic_has_vector_auto_eoi(struct kvm_vcpu_hv_synic *synic,
+ 				     int vector)
+ {
+ 	int i;
+ 	u64 sint_value;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		sint_value = synic_read_sint(synic, i);
+ 		if (synic_get_sint_vector(sint_value) == vector &&
+ 		    sint_value & HV_SYNIC_SINT_AUTO_EOI)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static int synic_set_sint(struct kvm_vcpu_hv_synic *synic, int sint,
+ 			  u64 data, bool host)
+ {
+ 	int vector;
+ 
+ 	vector = data & HV_SYNIC_SINT_VECTOR_MASK;
+ 	if (vector < 16 && !host)
+ 		return 1;
+ 	/*
+ 	 * Guest may configure multiple SINTs to use the same vector, so
+ 	 * we maintain a bitmap of vectors handled by synic, and a
+ 	 * bitmap of vectors with auto-eoi behavior.  The bitmaps are
+ 	 * updated here, and atomically queried on fast paths.
+ 	 */
+ 
+ 	atomic64_set(&synic->sint[sint], data);
+ 
+ 	if (synic_has_vector_connected(synic, vector))
+ 		__set_bit(vector, synic->vec_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->vec_bitmap);
+ 
+ 	if (synic_has_vector_auto_eoi(synic, vector))
+ 		__set_bit(vector, synic->auto_eoi_bitmap);
+ 	else
+ 		__clear_bit(vector, synic->auto_eoi_bitmap);
+ 
+ 	/* Load SynIC vectors into EOI exit bitmap */
+ 	kvm_make_request(KVM_REQ_SCAN_IOAPIC, synic_to_vcpu(synic));
+ 	return 0;
+ }
+ 
+ static struct kvm_vcpu_hv_synic *synic_get(struct kvm *kvm, u32 vcpu_id)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	if (vcpu_id >= atomic_read(&kvm->online_vcpus))
+ 		return NULL;
+ 	vcpu = kvm_get_vcpu(kvm, vcpu_id);
+ 	if (!vcpu)
+ 		return NULL;
+ 	synic = vcpu_to_synic(vcpu);
+ 	return (synic->active) ? synic : NULL;
+ }
+ 
+ static void synic_clear_sint_msg_pending(struct kvm_vcpu_hv_synic *synic,
+ 					u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct page *page;
+ 	gpa_t gpa;
+ 	struct hv_message *msg;
+ 	struct hv_message_page *msg_page;
+ 
+ 	gpa = synic->msg_page & PAGE_MASK;
+ 	page = kvm_vcpu_gfn_to_page(vcpu, gpa >> PAGE_SHIFT);
+ 	if (is_error_page(page)) {
+ 		vcpu_err(vcpu, "Hyper-V SynIC can't get msg page, gpa 0x%llx\n",
+ 			 gpa);
+ 		return;
+ 	}
+ 	msg_page = kmap_atomic(page);
+ 
+ 	msg = &msg_page->sint_message[sint];
+ 	msg->header.message_flags.msg_pending = 0;
+ 
+ 	kunmap_atomic(msg_page);
+ 	kvm_release_page_dirty(page);
+ 	kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ }
+ 
+ static void kvm_hv_notify_acked_sint(struct kvm_vcpu *vcpu, u32 sint)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 	int gsi, idx, stimers_pending;
+ 
+ 	trace_kvm_hv_notify_acked_sint(vcpu->vcpu_id, sint);
+ 
+ 	if (synic->msg_page & HV_SYNIC_SIMP_ENABLE)
+ 		synic_clear_sint_msg_pending(synic, sint);
+ 
+ 	/* Try to deliver pending Hyper-V SynIC timers messages */
+ 	stimers_pending = 0;
+ 	for (idx = 0; idx < ARRAY_SIZE(hv_vcpu->stimer); idx++) {
+ 		stimer = &hv_vcpu->stimer[idx];
+ 		if (stimer->msg_pending &&
+ 		    (stimer->config & HV_STIMER_ENABLE) &&
+ 		    HV_STIMER_SINT(stimer->config) == sint) {
+ 			set_bit(stimer->index,
+ 				hv_vcpu->stimer_pending_bitmap);
+ 			stimers_pending++;
+ 		}
+ 	}
+ 	if (stimers_pending)
+ 		kvm_make_request(KVM_REQ_HV_STIMER, vcpu);
+ 
+ 	idx = srcu_read_lock(&kvm->irq_srcu);
+ 	gsi = atomic_read(&synic->sint_to_gsi[sint]);
+ 	if (gsi != -1)
+ 		kvm_notify_acked_gsi(kvm, gsi);
+ 	srcu_read_unlock(&kvm->irq_srcu, idx);
+ }
+ 
+ static void synic_exit(struct kvm_vcpu_hv_synic *synic, u32 msr)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_vcpu_hv *hv_vcpu = &vcpu->arch.hyperv;
+ 
+ 	hv_vcpu->exit.type = KVM_EXIT_HYPERV_SYNIC;
+ 	hv_vcpu->exit.u.synic.msr = msr;
+ 	hv_vcpu->exit.u.synic.control = synic->control;
+ 	hv_vcpu->exit.u.synic.evt_page = synic->evt_page;
+ 	hv_vcpu->exit.u.synic.msg_page = synic->msg_page;
+ 
+ 	kvm_make_request(KVM_REQ_HV_EXIT, vcpu);
+ }
+ 
+ static int synic_set_msr(struct kvm_vcpu_hv_synic *synic,
+ 			 u32 msr, u64 data, bool host)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	int ret;
+ 
+ 	if (!synic->active)
+ 		return 1;
+ 
+ 	trace_kvm_hv_synic_set_msr(vcpu->vcpu_id, msr, data, host);
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		synic->control = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		if (!host) {
+ 			ret = 1;
+ 			break;
+ 		}
+ 		synic->version = data;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		if (data & HV_SYNIC_SIEFP_ENABLE)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->evt_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		if (data & HV_SYNIC_SIMP_ENABLE)
+ 			if (kvm_clear_guest(vcpu->kvm,
+ 					    data & PAGE_MASK, PAGE_SIZE)) {
+ 				ret = 1;
+ 				break;
+ 			}
+ 		synic->msg_page = data;
+ 		if (!host)
+ 			synic_exit(synic, msr);
+ 		break;
+ 	case HV_X64_MSR_EOM: {
+ 		int i;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ 		break;
+ 	}
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		ret = synic_set_sint(synic, msr - HV_X64_MSR_SINT0, data, host);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int synic_get_msr(struct kvm_vcpu_hv_synic *synic, u32 msr, u64 *pdata)
+ {
+ 	int ret;
+ 
+ 	if (!synic->active)
+ 		return 1;
+ 
+ 	ret = 0;
+ 	switch (msr) {
+ 	case HV_X64_MSR_SCONTROL:
+ 		*pdata = synic->control;
+ 		break;
+ 	case HV_X64_MSR_SVERSION:
+ 		*pdata = synic->version;
+ 		break;
+ 	case HV_X64_MSR_SIEFP:
+ 		*pdata = synic->evt_page;
+ 		break;
+ 	case HV_X64_MSR_SIMP:
+ 		*pdata = synic->msg_page;
+ 		break;
+ 	case HV_X64_MSR_EOM:
+ 		*pdata = 0;
+ 		break;
+ 	case HV_X64_MSR_SINT0 ... HV_X64_MSR_SINT15:
+ 		*pdata = atomic64_read(&synic->sint[msr - HV_X64_MSR_SINT0]);
+ 		break;
+ 	default:
+ 		ret = 1;
+ 		break;
+ 	}
+ 	return ret;
+ }
+ 
+ int synic_set_irq(struct kvm_vcpu_hv_synic *synic, u32 sint)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct kvm_lapic_irq irq;
+ 	int ret, vector;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint))
+ 		return -EINVAL;
+ 
+ 	vector = synic_get_sint_vector(synic_read_sint(synic, sint));
+ 	if (vector < 0)
+ 		return -ENOENT;
+ 
+ 	memset(&irq, 0, sizeof(irq));
+ 	irq.dest_id = kvm_apic_id(vcpu->arch.apic);
+ 	irq.dest_mode = APIC_DEST_PHYSICAL;
+ 	irq.delivery_mode = APIC_DM_FIXED;
+ 	irq.vector = vector;
+ 	irq.level = 1;
+ 
+ 	ret = kvm_irq_delivery_to_apic(vcpu->kvm, NULL, &irq, NULL);
+ 	trace_kvm_hv_synic_set_irq(vcpu->vcpu_id, sint, irq.vector, ret);
+ 	return ret;
+ }
+ 
+ int kvm_hv_synic_set_irq(struct kvm *kvm, u32 vcpu_id, u32 sint)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vcpu_id);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	return synic_set_irq(synic, sint);
+ }
+ 
+ void kvm_hv_synic_send_eoi(struct kvm_vcpu *vcpu, int vector)
+ {
+ 	struct kvm_vcpu_hv_synic *synic = vcpu_to_synic(vcpu);
+ 	int i;
+ 
+ 	trace_kvm_hv_synic_send_eoi(vcpu->vcpu_id, vector);
+ 
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++)
+ 		if (synic_get_sint_vector(synic_read_sint(synic, i)) == vector)
+ 			kvm_hv_notify_acked_sint(vcpu, i);
+ }
+ 
+ static int kvm_hv_set_sint_gsi(struct kvm *kvm, u32 vcpu_id, u32 sint, int gsi)
+ {
+ 	struct kvm_vcpu_hv_synic *synic;
+ 
+ 	synic = synic_get(kvm, vcpu_id);
+ 	if (!synic)
+ 		return -EINVAL;
+ 
+ 	if (sint >= ARRAY_SIZE(synic->sint_to_gsi))
+ 		return -EINVAL;
+ 
+ 	atomic_set(&synic->sint_to_gsi[sint], gsi);
+ 	return 0;
+ }
+ 
+ void kvm_hv_irq_routing_update(struct kvm *kvm)
+ {
+ 	struct kvm_irq_routing_table *irq_rt;
+ 	struct kvm_kernel_irq_routing_entry *e;
+ 	u32 gsi;
+ 
+ 	irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+ 					lockdep_is_held(&kvm->irq_lock));
+ 
+ 	for (gsi = 0; gsi < irq_rt->nr_rt_entries; gsi++) {
+ 		hlist_for_each_entry(e, &irq_rt->map[gsi], link) {
+ 			if (e->type == KVM_IRQ_ROUTING_HV_SINT)
+ 				kvm_hv_set_sint_gsi(kvm, e->hv_sint.vcpu,
+ 						    e->hv_sint.sint, gsi);
+ 		}
+ 	}
+ }
+ 
+ static void synic_init(struct kvm_vcpu_hv_synic *synic)
+ {
+ 	int i;
+ 
+ 	memset(synic, 0, sizeof(*synic));
+ 	synic->version = HV_SYNIC_VERSION_1;
+ 	for (i = 0; i < ARRAY_SIZE(synic->sint); i++) {
+ 		atomic64_set(&synic->sint[i], HV_SYNIC_SINT_MASKED);
+ 		atomic_set(&synic->sint_to_gsi[i], -1);
+ 	}
+ }
+ 
+ static u64 get_time_ref_counter(struct kvm *kvm)
+ {
+ 	struct kvm_hv *hv = &kvm->arch.hyperv;
+ 	struct kvm_vcpu *vcpu;
+ 	u64 tsc;
+ 
+ 	/*
+ 	 * The guest has not set up the TSC page or the clock isn't
+ 	 * stable, fall back to get_kvmclock_ns.
+ 	 */
+ 	if (!hv->tsc_ref.tsc_sequence)
+ 		return div_u64(get_kvmclock_ns(kvm), 100);
+ 
+ 	vcpu = kvm_get_vcpu(kvm, 0);
+ 	tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ 	return mul_u64_u64_shr(tsc, hv->tsc_ref.tsc_scale, 64)
+ 		+ hv->tsc_ref.tsc_offset;
+ }
+ 
+ static void stimer_mark_pending(struct kvm_vcpu_hv_stimer *stimer,
+ 				bool vcpu_kick)
+ {
+ 	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
+ 
+ 	set_bit(stimer->index,
+ 		vcpu_to_hv_vcpu(vcpu)->stimer_pending_bitmap);
+ 	kvm_make_request(KVM_REQ_HV_STIMER, vcpu);
+ 	if (vcpu_kick)
+ 		kvm_vcpu_kick(vcpu);
+ }
+ 
+ static void stimer_cleanup(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
+ 
+ 	trace_kvm_hv_stimer_cleanup(stimer_to_vcpu(stimer)->vcpu_id,
+ 				    stimer->index);
+ 
+ 	hrtimer_cancel(&stimer->timer);
+ 	clear_bit(stimer->index,
+ 		  vcpu_to_hv_vcpu(vcpu)->stimer_pending_bitmap);
+ 	stimer->msg_pending = false;
+ 	stimer->exp_time = 0;
+ }
+ 
+ static enum hrtimer_restart stimer_timer_callback(struct hrtimer *timer)
+ {
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 
+ 	stimer = container_of(timer, struct kvm_vcpu_hv_stimer, timer);
+ 	trace_kvm_hv_stimer_callback(stimer_to_vcpu(stimer)->vcpu_id,
+ 				     stimer->index);
+ 	stimer_mark_pending(stimer, true);
+ 
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ /*
+  * stimer_start() assumptions:
+  * a) stimer->count is not equal to 0
+  * b) stimer->config has HV_STIMER_ENABLE flag
+  */
+ static int stimer_start(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	u64 time_now;
+ 	ktime_t ktime_now;
+ 
+ 	time_now = get_time_ref_counter(stimer_to_vcpu(stimer)->kvm);
+ 	ktime_now = ktime_get();
+ 
+ 	if (stimer->config & HV_STIMER_PERIODIC) {
+ 		if (stimer->exp_time) {
+ 			if (time_now >= stimer->exp_time) {
+ 				u64 remainder;
+ 
+ 				div64_u64_rem(time_now - stimer->exp_time,
+ 					      stimer->count, &remainder);
+ 				stimer->exp_time =
+ 					time_now + (stimer->count - remainder);
+ 			}
+ 		} else
+ 			stimer->exp_time = time_now + stimer->count;
+ 
+ 		trace_kvm_hv_stimer_start_periodic(
+ 					stimer_to_vcpu(stimer)->vcpu_id,
+ 					stimer->index,
+ 					time_now, stimer->exp_time);
+ 
+ 		hrtimer_start(&stimer->timer,
+ 			      ktime_add_ns(ktime_now,
+ 					   100 * (stimer->exp_time - time_now)),
+ 			      HRTIMER_MODE_ABS);
+ 		return 0;
+ 	}
+ 	stimer->exp_time = stimer->count;
+ 	if (time_now >= stimer->count) {
+ 		/*
+ 		 * Expire timer according to Hypervisor Top-Level Functional
+ 		 * specification v4(15.3.1):
+ 		 * "If a one shot is enabled and the specified count is in
+ 		 * the past, it will expire immediately."
+ 		 */
+ 		stimer_mark_pending(stimer, false);
+ 		return 0;
+ 	}
+ 
+ 	trace_kvm_hv_stimer_start_one_shot(stimer_to_vcpu(stimer)->vcpu_id,
+ 					   stimer->index,
+ 					   time_now, stimer->count);
+ 
+ 	hrtimer_start(&stimer->timer,
+ 		      ktime_add_ns(ktime_now, 100 * (stimer->count - time_now)),
+ 		      HRTIMER_MODE_ABS);
+ 	return 0;
+ }
+ 
+ static int stimer_set_config(struct kvm_vcpu_hv_stimer *stimer, u64 config,
+ 			     bool host)
+ {
+ 	trace_kvm_hv_stimer_set_config(stimer_to_vcpu(stimer)->vcpu_id,
+ 				       stimer->index, config, host);
+ 
+ 	stimer_cleanup(stimer);
+ 	if ((stimer->config & HV_STIMER_ENABLE) && HV_STIMER_SINT(config) == 0)
+ 		config &= ~HV_STIMER_ENABLE;
+ 	stimer->config = config;
+ 	stimer_mark_pending(stimer, false);
+ 	return 0;
+ }
+ 
+ static int stimer_set_count(struct kvm_vcpu_hv_stimer *stimer, u64 count,
+ 			    bool host)
+ {
+ 	trace_kvm_hv_stimer_set_count(stimer_to_vcpu(stimer)->vcpu_id,
+ 				      stimer->index, count, host);
+ 
+ 	stimer_cleanup(stimer);
+ 	stimer->count = count;
+ 	if (stimer->count == 0)
+ 		stimer->config &= ~HV_STIMER_ENABLE;
+ 	else if (stimer->config & HV_STIMER_AUTOENABLE)
+ 		stimer->config |= HV_STIMER_ENABLE;
+ 	stimer_mark_pending(stimer, false);
+ 	return 0;
+ }
+ 
+ static int stimer_get_config(struct kvm_vcpu_hv_stimer *stimer, u64 *pconfig)
+ {
+ 	*pconfig = stimer->config;
+ 	return 0;
+ }
+ 
+ static int stimer_get_count(struct kvm_vcpu_hv_stimer *stimer, u64 *pcount)
+ {
+ 	*pcount = stimer->count;
+ 	return 0;
+ }
+ 
+ static int synic_deliver_msg(struct kvm_vcpu_hv_synic *synic, u32 sint,
+ 			     struct hv_message *src_msg)
+ {
+ 	struct kvm_vcpu *vcpu = synic_to_vcpu(synic);
+ 	struct page *page;
+ 	gpa_t gpa;
+ 	struct hv_message *dst_msg;
+ 	int r;
+ 	struct hv_message_page *msg_page;
+ 
+ 	if (!(synic->msg_page & HV_SYNIC_SIMP_ENABLE))
+ 		return -ENOENT;
+ 
+ 	gpa = synic->msg_page & PAGE_MASK;
+ 	page = kvm_vcpu_gfn_to_page(vcpu, gpa >> PAGE_SHIFT);
+ 	if (is_error_page(page))
+ 		return -EFAULT;
+ 
+ 	msg_page = kmap_atomic(page);
+ 	dst_msg = &msg_page->sint_message[sint];
+ 	if (sync_cmpxchg(&dst_msg->header.message_type, HVMSG_NONE,
+ 			 src_msg->header.message_type) != HVMSG_NONE) {
+ 		dst_msg->header.message_flags.msg_pending = 1;
+ 		r = -EAGAIN;
+ 	} else {
+ 		memcpy(&dst_msg->u.payload, &src_msg->u.payload,
+ 		       src_msg->header.payload_size);
+ 		dst_msg->header.message_type = src_msg->header.message_type;
+ 		dst_msg->header.payload_size = src_msg->header.payload_size;
+ 		r = synic_set_irq(synic, sint);
+ 		if (r >= 1)
+ 			r = 0;
+ 		else if (r == 0)
+ 			r = -EFAULT;
+ 	}
+ 	kunmap_atomic(msg_page);
+ 	kvm_release_page_dirty(page);
+ 	kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ 	return r;
+ }
+ 
+ static int stimer_send_msg(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct kvm_vcpu *vcpu = stimer_to_vcpu(stimer);
+ 	struct hv_message *msg = &stimer->msg;
+ 	struct hv_timer_message_payload *payload =
+ 			(struct hv_timer_message_payload *)&msg->u.payload;
+ 
+ 	payload->expiration_time = stimer->exp_time;
+ 	payload->delivery_time = get_time_ref_counter(vcpu->kvm);
+ 	return synic_deliver_msg(vcpu_to_synic(vcpu),
+ 				 HV_STIMER_SINT(stimer->config), msg);
+ }
+ 
+ static void stimer_expiration(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	int r;
+ 
+ 	stimer->msg_pending = true;
+ 	r = stimer_send_msg(stimer);
+ 	trace_kvm_hv_stimer_expiration(stimer_to_vcpu(stimer)->vcpu_id,
+ 				       stimer->index, r);
+ 	if (!r) {
+ 		stimer->msg_pending = false;
+ 		if (!(stimer->config & HV_STIMER_PERIODIC))
+ 			stimer->config &= ~HV_STIMER_ENABLE;
+ 	}
+ }
+ 
+ void kvm_hv_process_stimers(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	struct kvm_vcpu_hv_stimer *stimer;
+ 	u64 time_now, exp_time;
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
+ 		if (test_and_clear_bit(i, hv_vcpu->stimer_pending_bitmap)) {
+ 			stimer = &hv_vcpu->stimer[i];
+ 			if (stimer->config & HV_STIMER_ENABLE) {
+ 				exp_time = stimer->exp_time;
+ 
+ 				if (exp_time) {
+ 					time_now =
+ 						get_time_ref_counter(vcpu->kvm);
+ 					if (time_now >= exp_time)
+ 						stimer_expiration(stimer);
+ 				}
+ 
+ 				if ((stimer->config & HV_STIMER_ENABLE) &&
+ 				    stimer->count)
+ 					stimer_start(stimer);
+ 				else
+ 					stimer_cleanup(stimer);
+ 			}
+ 		}
+ }
+ 
+ void kvm_hv_vcpu_uninit(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
+ 		stimer_cleanup(&hv_vcpu->stimer[i]);
+ }
+ 
+ static void stimer_prepare_msg(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct hv_message *msg = &stimer->msg;
+ 	struct hv_timer_message_payload *payload =
+ 			(struct hv_timer_message_payload *)&msg->u.payload;
+ 
+ 	memset(&msg->header, 0, sizeof(msg->header));
+ 	msg->header.message_type = HVMSG_TIMER_EXPIRED;
+ 	msg->header.payload_size = sizeof(*payload);
+ 
+ 	payload->timer_index = stimer->index;
+ 	payload->expiration_time = 0;
+ 	payload->delivery_time = 0;
+ }
+ 
+ static void stimer_init(struct kvm_vcpu_hv_stimer *stimer, int timer_index)
+ {
+ 	memset(stimer, 0, sizeof(*stimer));
+ 	stimer->index = timer_index;
+ 	hrtimer_init(&stimer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+ 	stimer->timer.function = stimer_timer_callback;
+ 	stimer_prepare_msg(stimer);
+ }
+ 
+ void kvm_hv_vcpu_init(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu = vcpu_to_hv_vcpu(vcpu);
+ 	int i;
+ 
+ 	synic_init(&hv_vcpu->synic);
+ 
+ 	bitmap_zero(hv_vcpu->stimer_pending_bitmap, HV_SYNIC_STIMER_COUNT);
+ 	for (i = 0; i < ARRAY_SIZE(hv_vcpu->stimer); i++)
+ 		stimer_init(&hv_vcpu->stimer[i], i);
+ }
+ 
+ int kvm_hv_activate_synic(struct kvm_vcpu *vcpu)
+ {
+ 	/*
+ 	 * Hyper-V SynIC auto EOI SINT's are
+ 	 * not compatible with APICV, so deactivate APICV
+ 	 */
+ 	kvm_vcpu_deactivate_apicv(vcpu);
+ 	vcpu_to_synic(vcpu)->active = true;
+ 	return 0;
+ }
+ 
++>>>>>>> 095cf55df715 (KVM: x86: Hyper-V tsc page setup)
  static bool kvm_hv_msr_partition_wide(u32 msr)
  {
  	bool r = false;
diff --cc arch/x86/kvm/hyperv.h
index c7bce559f67b,cd1119538add..000000000000
--- a/arch/x86/kvm/hyperv.h
+++ b/arch/x86/kvm/hyperv.h
@@@ -29,4 -52,39 +29,41 @@@ int kvm_hv_get_msr_common(struct kvm_vc
  bool kvm_hv_hypercall_enabled(struct kvm *kvm);
  int kvm_hv_hypercall(struct kvm_vcpu *vcpu);
  
++<<<<<<< HEAD
++=======
+ void kvm_hv_irq_routing_update(struct kvm *kvm);
+ int kvm_hv_synic_set_irq(struct kvm *kvm, u32 vcpu_id, u32 sint);
+ void kvm_hv_synic_send_eoi(struct kvm_vcpu *vcpu, int vector);
+ int kvm_hv_activate_synic(struct kvm_vcpu *vcpu);
+ 
+ void kvm_hv_vcpu_init(struct kvm_vcpu *vcpu);
+ void kvm_hv_vcpu_uninit(struct kvm_vcpu *vcpu);
+ 
+ static inline struct kvm_vcpu_hv_stimer *vcpu_to_stimer(struct kvm_vcpu *vcpu,
+ 							int timer_index)
+ {
+ 	return &vcpu_to_hv_vcpu(vcpu)->stimer[timer_index];
+ }
+ 
+ static inline struct kvm_vcpu *stimer_to_vcpu(struct kvm_vcpu_hv_stimer *stimer)
+ {
+ 	struct kvm_vcpu_hv *hv_vcpu;
+ 
+ 	hv_vcpu = container_of(stimer - stimer->index, struct kvm_vcpu_hv,
+ 			       stimer[0]);
+ 	return hv_vcpu_to_vcpu(hv_vcpu);
+ }
+ 
+ static inline bool kvm_hv_has_stimer_pending(struct kvm_vcpu *vcpu)
+ {
+ 	return !bitmap_empty(vcpu->arch.hyperv.stimer_pending_bitmap,
+ 			     HV_SYNIC_STIMER_COUNT);
+ }
+ 
+ void kvm_hv_process_stimers(struct kvm_vcpu *vcpu);
+ 
+ void kvm_hv_setup_tsc_page(struct kvm *kvm,
+ 			   struct pvclock_vcpu_time_info *hv_clock);
+ 
++>>>>>>> 095cf55df715 (KVM: x86: Hyper-V tsc page setup)
  #endif
diff --cc arch/x86/kvm/x86.c
index 99e230533b87,3ee8a91a78c3..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1882,18 -1887,10 +1882,25 @@@ static int kvm_guest_time_update(struc
  
  	vcpu->hv_clock.flags = pvclock_flags;
  
++<<<<<<< HEAD
 +	trace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);
 +
 +	kvm_write_guest_cached(v->kvm, &vcpu->pv_time,
 +				&vcpu->hv_clock,
 +				sizeof(vcpu->hv_clock));
 +
 +	smp_wmb();
 +
 +	vcpu->hv_clock.version++;
 +	kvm_write_guest_cached(v->kvm, &vcpu->pv_time,
 +				&vcpu->hv_clock,
 +				sizeof(vcpu->hv_clock.version));
++=======
+ 	if (vcpu->pv_time_enabled)
+ 		kvm_setup_pvclock_page(v);
+ 	if (v == kvm_get_vcpu(v->kvm, 0))
+ 		kvm_hv_setup_tsc_page(v->kvm, &vcpu->hv_clock);
++>>>>>>> 095cf55df715 (KVM: x86: Hyper-V tsc page setup)
  	return 0;
  }
  
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ab2d8132f390..0b238f10e8d6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -653,6 +653,8 @@ struct kvm_hv {
 	/* Hyper-v based guest crash (NT kernel bugcheck) parameters */
 	u64 hv_crash_param[HV_X64_MSR_CRASH_PARAMS];
 	u64 hv_crash_ctl;
+
+	HV_REFERENCE_TSC_PAGE tsc_ref;
 };
 
 struct kvm_arch {
* Unmerged path arch/x86/kvm/hyperv.c
* Unmerged path arch/x86/kvm/hyperv.h
* Unmerged path arch/x86/kvm/x86.c
