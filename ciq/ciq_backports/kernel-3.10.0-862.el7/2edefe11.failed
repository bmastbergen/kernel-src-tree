memcg, slab: fix races in per-memcg cache creation/destruction

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Vladimir Davydov <vdavydov@parallels.com>
commit 2edefe1155b3ad3dc92065f6e1018d363525296e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/2edefe11.failed

We obtain a per-memcg cache from a root kmem_cache by dereferencing an
entry of the root cache's memcg_params::memcg_caches array.  If we find
no cache for a memcg there on allocation, we initiate the memcg cache
creation (see memcg_kmem_get_cache()).  The cache creation proceeds
asynchronously in memcg_create_kmem_cache() in order to avoid lock
clashes, so there can be several threads trying to create the same
kmem_cache concurrently, but only one of them may succeed.  However, due
to a race in the code, it is not always true.  The point is that the
memcg_caches array can be relocated when we activate kmem accounting for
a memcg (see memcg_update_all_caches(), memcg_update_cache_size()).  If
memcg_update_cache_size() and memcg_create_kmem_cache() proceed
concurrently as described below, we can leak a kmem_cache.

Asume two threads schedule creation of the same kmem_cache.  One of them
successfully creates it.  Another one should fail then, but if
memcg_create_kmem_cache() interleaves with memcg_update_cache_size() as
follows, it won't:

  memcg_create_kmem_cache()             memcg_update_cache_size()
  (called w/o mutexes held)             (called with slab_mutex,
                                         set_limit_mutex held)
  -------------------------             -------------------------

  mutex_lock(&memcg_cache_mutex)

                                        s->memcg_params=kzalloc(...)

  new_cachep=cache_from_memcg_idx(cachep,idx)
  // new_cachep==NULL => proceed to creation

                                        s->memcg_params->memcg_caches[i]
                                            =cur_params->memcg_caches[i]

  // kmem_cache_create_memcg takes slab_mutex
  // so we will hang around until
  // memcg_update_cache_size finishes, but
  // nothing will prevent it from succeeding so
  // memcg_caches[idx] will be overwritten in
  // memcg_register_cache!

  new_cachep = kmem_cache_create_memcg(...)
  mutex_unlock(&memcg_cache_mutex)

Let's fix this by moving the check for existence of the memcg cache to
kmem_cache_create_memcg() to be called under the slab_mutex and make it
return NULL if so.

A similar race is possible when destroying a memcg cache (see
kmem_cache_destroy()).  Since memcg_unregister_cache(), which clears the
pointer in the memcg_caches array, is called w/o protection, we can race
with memcg_update_cache_size() and omit clearing the pointer.  Therefore
memcg_unregister_cache() should be moved before we release the
slab_mutex.

	Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Glauber Costa <glommer@gmail.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: Christoph Lameter <cl@linux.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 2edefe1155b3ad3dc92065f6e1018d363525296e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
#	mm/slab_common.c
diff --cc mm/memcontrol.c
index 23e6528af2de,d2da65c4cd84..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -3233,29 -3261,74 +3233,91 @@@ void memcg_release_cache(struct kmem_ca
  	struct mem_cgroup *memcg;
  	int id;
  
 -	if (is_root_cache(s))
 +	/*
 +	 * This happens, for instance, when a root cache goes away before we
 +	 * add any memcg.
 +	 */
 +	if (!s->memcg_params)
  		return;
  
++<<<<<<< HEAD
 +	if (s->memcg_params->is_root_cache)
 +		goto out;
 +
 +	memcg = s->memcg_params->memcg;
 +	id  = memcg_cache_id(memcg);
++=======
+ 	/*
+ 	 * Holding the slab_mutex assures nobody will touch the memcg_caches
+ 	 * array while we are modifying it.
+ 	 */
+ 	lockdep_assert_held(&slab_mutex);
+ 
+ 	root = s->memcg_params->root_cache;
+ 	memcg = s->memcg_params->memcg;
+ 	id = memcg_cache_id(memcg);
+ 
+ 	css_get(&memcg->css);
+ 
+ 
+ 	/*
+ 	 * Since readers won't lock (see cache_from_memcg_idx()), we need a
+ 	 * barrier here to ensure nobody will see the kmem_cache partially
+ 	 * initialized.
+ 	 */
+ 	smp_wmb();
+ 
+ 	/*
+ 	 * Initialize the pointer to this cache in its parent's memcg_params
+ 	 * before adding it to the memcg_slab_caches list, otherwise we can
+ 	 * fail to convert memcg_params_to_cache() while traversing the list.
+ 	 */
+ 	VM_BUG_ON(root->memcg_params->memcg_caches[id]);
+ 	root->memcg_params->memcg_caches[id] = s;
+ 
+ 	mutex_lock(&memcg->slab_caches_mutex);
+ 	list_add(&s->memcg_params->list, &memcg->memcg_slab_caches);
+ 	mutex_unlock(&memcg->slab_caches_mutex);
+ }
+ 
+ void memcg_unregister_cache(struct kmem_cache *s)
+ {
+ 	struct kmem_cache *root;
+ 	struct mem_cgroup *memcg;
+ 	int id;
+ 
+ 	if (is_root_cache(s))
+ 		return;
++>>>>>>> 2edefe1155b3 (memcg, slab: fix races in per-memcg cache creation/destruction)
+ 
+ 	/*
+ 	 * Holding the slab_mutex assures nobody will touch the memcg_caches
+ 	 * array while we are modifying it.
+ 	 */
+ 	lockdep_assert_held(&slab_mutex);
  
  	root = s->memcg_params->root_cache;
 -	memcg = s->memcg_params->memcg;
 -	id = memcg_cache_id(memcg);
 +	root->memcg_params->memcg_caches[id] = NULL;
  
  	mutex_lock(&memcg->slab_caches_mutex);
  	list_del(&s->memcg_params->list);
  	mutex_unlock(&memcg->slab_caches_mutex);
  
++<<<<<<< HEAD
 +	mem_cgroup_put(memcg);
 +out:
 +	kfree(s->memcg_params);
++=======
+ 	/*
+ 	 * Clear the pointer to this cache in its parent's memcg_params only
+ 	 * after removing it from the memcg_slab_caches list, otherwise we can
+ 	 * fail to convert memcg_params_to_cache() while traversing the list.
+ 	 */
+ 	VM_BUG_ON(!root->memcg_params->memcg_caches[id]);
+ 	root->memcg_params->memcg_caches[id] = NULL;
+ 
+ 	css_put(&memcg->css);
++>>>>>>> 2edefe1155b3 (memcg, slab: fix races in per-memcg cache creation/destruction)
  }
  
  /*
@@@ -3408,30 -3481,10 +3470,33 @@@ static struct kmem_cache *memcg_create_
  
  	BUG_ON(!memcg_can_account_kmem(memcg));
  
- 	idx = memcg_cache_id(memcg);
- 
  	mutex_lock(&memcg_cache_mutex);
++<<<<<<< HEAD
 +	new_cachep = cachep->memcg_params->memcg_caches[idx];
 +	if (new_cachep)
 +		goto out;
 +
++=======
++>>>>>>> 2edefe1155b3 (memcg, slab: fix races in per-memcg cache creation/destruction)
  	new_cachep = kmem_cache_dup(memcg, cachep);
 -	if (new_cachep == NULL)
 +	if (new_cachep == NULL) {
  		new_cachep = cachep;
++<<<<<<< HEAD
 +		goto out;
 +	}
 +
 +	mem_cgroup_get(memcg);
 +	atomic_set(&new_cachep->memcg_params->nr_pages , 0);
 +
 +	cachep->memcg_params->memcg_caches[idx] = new_cachep;
 +	/*
 +	 * the readers won't lock, make sure everybody sees the updated value,
 +	 * so they won't put stuff in the queue again for no reason
 +	 */
 +	wmb();
 +out:
++=======
++>>>>>>> 2edefe1155b3 (memcg, slab: fix races in per-memcg cache creation/destruction)
  	mutex_unlock(&memcg_cache_mutex);
  	return new_cachep;
  }
diff --cc mm/slab_common.c
index e13d227ed0ab,f34707eeacc7..000000000000
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@@ -196,9 -176,22 +196,21 @@@ kmem_cache_create_memcg(struct mem_cgro
  	get_online_cpus();
  	mutex_lock(&slab_mutex);
  
 -	err = kmem_cache_sanity_check(memcg, name, size);
 -	if (err)
 -		goto out_unlock;
 +	if (!kmem_cache_sanity_check(memcg, name, size) == 0)
 +		goto out_locked;
  
+ 	if (memcg) {
+ 		/*
+ 		 * Since per-memcg caches are created asynchronously on first
+ 		 * allocation (see memcg_kmem_get_cache()), several threads can
+ 		 * try to create the same cache, but only one of them may
+ 		 * succeed. Therefore if we get here and see the cache has
+ 		 * already been created, we silently return NULL.
+ 		 */
+ 		if (cache_from_memcg_idx(parent_cache, memcg_cache_id(memcg)))
+ 			goto out_unlock;
+ 	}
+ 
  	/*
  	 * Some allocators will constraint the set of valid flags to a subset
  	 * of all flags. We expect them to define CACHE_CREATE_MASK in this
@@@ -290,7 -278,7 +303,11 @@@ void kmem_cache_destroy(struct kmem_cac
  			if (s->flags & SLAB_DESTROY_BY_RCU)
  				rcu_barrier();
  
++<<<<<<< HEAD
 +			memcg_release_cache(s);
++=======
+ 			memcg_free_cache_params(s);
++>>>>>>> 2edefe1155b3 (memcg, slab: fix races in per-memcg cache creation/destruction)
  			kfree(s->name);
  			kmem_cache_free(kmem_cache, s);
  		} else {
* Unmerged path mm/memcontrol.c
* Unmerged path mm/slab_common.c
