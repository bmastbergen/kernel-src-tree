nvme-pci: set cqe_seen on polled completions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [nvme] pci: set cqe_seen on polled completions (David Milburn) [1457880 1456486 1454365]
Rebuild_FUZZ: 93.98%
commit-author Keith Busch <keith.busch@intel.com>
commit e9d8a0fdeacd843c85dcef480cdb2ab76bcdb6e4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e9d8a0fd.failed

Fixes: 920d13a884 ("nvme-pci: factor out the cqe reading mechanics from __nvme_process_cq")
	Reported-by: Jens Axboe <axboe@kernel.dk>
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit e9d8a0fdeacd843c85dcef480cdb2ab76bcdb6e4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/pci.c
diff --cc drivers/nvme/host/pci.c
index 74de1d1461c5,925467b31a33..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -591,58 -765,74 +591,100 @@@ static inline bool nvme_cqe_valid(struc
  	return (le16_to_cpu(nvmeq->cqes[head].status) & 1) == phase;
  }
  
 -static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
 +static int nvme_process_cq(struct nvme_queue *nvmeq)
  {
 -	u16 head = nvmeq->cq_head;
 +	u16 head, phase;
  
 -	if (likely(nvmeq->cq_vector >= 0)) {
 -		if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
 -						      nvmeq->dbbuf_cq_ei))
 -			writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
 -	}
 -}
 +	head = nvmeq->cq_head;
 +	phase = nvmeq->cq_phase;
  
 -static inline void nvme_handle_cqe(struct nvme_queue *nvmeq,
 -		struct nvme_completion *cqe)
 -{
 -	struct request *req;
 +	while (nvme_cqe_valid(nvmeq, head, phase)) {
 +		struct nvme_completion cqe = nvmeq->cqes[head];
 +		struct request *req;
  
++<<<<<<< HEAD
 +		if (++head == nvmeq->q_depth) {
 +			head = 0;
 +			phase = !phase;
++=======
+ 	if (unlikely(cqe->command_id >= nvmeq->q_depth)) {
+ 		dev_warn(nvmeq->dev->ctrl.device,
+ 			"invalid id %d completed on queue %d\n",
+ 			cqe->command_id, le16_to_cpu(cqe->sq_id));
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * AEN requests are special as they don't time out and can
+ 	 * survive any kind of queue freeze and often don't respond to
+ 	 * aborts.  We don't even bother to allocate a struct request
+ 	 * for them but rather special case them here.
+ 	 */
+ 	if (unlikely(nvmeq->qid == 0 &&
+ 			cqe->command_id >= NVME_AQ_BLKMQ_DEPTH)) {
+ 		nvme_complete_async_event(&nvmeq->dev->ctrl,
+ 				cqe->status, &cqe->result);
+ 		return;
+ 	}
+ 
+ 	nvmeq->cqe_seen = 1;
+ 	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+ 	nvme_end_request(req, cqe->status, cqe->result);
+ }
+ 
+ static inline bool nvme_read_cqe(struct nvme_queue *nvmeq,
+ 		struct nvme_completion *cqe)
+ {
+ 	if (nvme_cqe_valid(nvmeq, nvmeq->cq_head, nvmeq->cq_phase)) {
+ 		*cqe = nvmeq->cqes[nvmeq->cq_head];
+ 
+ 		if (++nvmeq->cq_head == nvmeq->q_depth) {
+ 			nvmeq->cq_head = 0;
+ 			nvmeq->cq_phase = !nvmeq->cq_phase;
++>>>>>>> e9d8a0fdeacd (nvme-pci: set cqe_seen on polled completions)
  		}
 -		return true;
 -	}
 -	return false;
 -}
  
 -static void nvme_process_cq(struct nvme_queue *nvmeq)
 -{
 -	struct nvme_completion cqe;
 -	int consumed = 0;
  
 -	while (nvme_read_cqe(nvmeq, &cqe)) {
 -		nvme_handle_cqe(nvmeq, &cqe);
 -		consumed++;
 +		if (unlikely(cqe.command_id >= nvmeq->q_depth)) {
 +			dev_warn(nvmeq->dev->ctrl.device,
 +				"invalid id %d completed on queue %d\n",
 +				cqe.command_id, le16_to_cpu(cqe.sq_id));
 +			continue;
 +		}
 +
 +		/*
 +		 * AEN requests are special as they don't time out and can
 +		 * survive any kind of queue freeze and often don't respond to
 +		 * aborts.  We don't even bother to allocate a struct request
 +		 * for them but rather special case them here.
 +		 */
 +		if (unlikely(nvmeq->qid == 0 &&
 +				cqe.command_id >= NVME_AQ_BLKMQ_DEPTH)) {
 +			nvme_complete_async_event(&nvmeq->dev->ctrl,
 +					cqe.status, &cqe.result);
 +			continue;
 +		}
 +
 +		req = blk_mq_tag_to_rq(*nvmeq->tags, cqe.command_id);
 +		nvme_req(req)->result = cqe.result;
 +		blk_mq_complete_request(req, le16_to_cpu(cqe.status) >> 1);
  	}
  
++<<<<<<< HEAD
 +	if (head == nvmeq->cq_head && phase == nvmeq->cq_phase)
 +		return 0;
 +
 +	if (likely(nvmeq->cq_vector >= 0))
 +		writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
 +	nvmeq->cq_head = head;
 +	nvmeq->cq_phase = phase;
 +
 +	nvmeq->cqe_seen = 1;
 +	return 1;
++=======
+ 	if (consumed)
+ 		nvme_ring_cq_doorbell(nvmeq);
++>>>>>>> e9d8a0fdeacd (nvme-pci: set cqe_seen on polled completions)
  }
  
  static irqreturn_t nvme_irq(int irq, void *data)
* Unmerged path drivers/nvme/host/pci.c
