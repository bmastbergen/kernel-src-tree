mm/zsmalloc: simplify zs_max_alloc_size handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jerome Marchand <jmarchan@redhat.com>
commit cf8e0fedf0784ef4bc1889380b09eda295e3d109
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/cf8e0fed.failed

Commit 40f9fb8cffc6 ("mm/zsmalloc: support allocating obj with size of
ZS_MAX_ALLOC_SIZE") fixes a size calculation error that prevented
zsmalloc to allocate an object of the maximal size (ZS_MAX_ALLOC_SIZE).
I think however the fix is unneededly complicated.

This patch replaces the dynamic calculation of zs_size_classes at init
time by a compile time calculation that uses the DIV_ROUND_UP() macro
already used in get_size_class_index().

[akpm@linux-foundation.org: use min_t]
Link: http://lkml.kernel.org/r/20170630114859.1979-1-jmarchan@redhat.com
	Signed-off-by: Jerome Marchand <jmarchan@redhat.com>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
	Cc: Mahendran Ganesh <opensource.ganesh@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit cf8e0fedf0784ef4bc1889380b09eda295e3d109)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/zsmalloc.c
diff --cc mm/zsmalloc.c
index b919973220e7,013eea76685e..000000000000
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@@ -133,9 -94,33 +133,14 @@@
  #endif
  #endif
  #define _PFN_BITS		(MAX_PHYSMEM_BITS - PAGE_SHIFT)
 -
 -/*
 - * Memory for allocating for handle keeps object position by
 - * encoding <page, obj_idx> and the encoded value has a room
 - * in least bit(ie, look at obj_to_location).
 - * We use the bit to synchronize between object access by
 - * user and migration.
 - */
 -#define HANDLE_PIN_BIT	0
 -
 -/*
 - * Head in allocated object should have OBJ_ALLOCATED_TAG
 - * to identify the object was allocated or not.
 - * It's okay to add the status bit in the least bit because
 - * header keeps handle which is 4byte-aligned address so we
 - * have room for two bit at least.
 - */
 -#define OBJ_ALLOCATED_TAG 1
 -#define OBJ_TAG_BITS 1
 -#define OBJ_INDEX_BITS	(BITS_PER_LONG - _PFN_BITS - OBJ_TAG_BITS)
 +#define OBJ_INDEX_BITS	(BITS_PER_LONG - _PFN_BITS)
  #define OBJ_INDEX_MASK	((_AC(1, UL) << OBJ_INDEX_BITS) - 1)
  
+ #define FULLNESS_BITS	2
+ #define CLASS_BITS	8
+ #define ISOLATED_BITS	3
+ #define MAGIC_VAL_BITS	8
+ 
  #define MAX(a, b) ((a) >= (b) ? (a) : (b))
  /* ZS_MIN_ALLOC_SIZE must be multiple of ZS_ALIGN */
  #define ZS_MIN_ALLOC_SIZE \
@@@ -155,23 -141,41 +160,61 @@@
   *  ZS_MIN_ALLOC_SIZE and ZS_SIZE_CLASS_DELTA must be multiple of ZS_ALIGN
   *  (reason above)
   */
++<<<<<<< HEAD
 +#define ZS_SIZE_CLASS_DELTA	(PAGE_SIZE >> 8)
 +#define ZS_SIZE_CLASSES		((ZS_MAX_ALLOC_SIZE - ZS_MIN_ALLOC_SIZE) / \
 +					ZS_SIZE_CLASS_DELTA + 1)
 +
 +/*
 + * We do not maintain any list for completely empty or full pages
 + */
 +enum fullness_group {
 +	ZS_ALMOST_FULL,
 +	ZS_ALMOST_EMPTY,
 +	_ZS_NR_FULLNESS_GROUPS,
 +
 +	ZS_EMPTY,
 +	ZS_FULL
 +};
 +
 +/*
++=======
+ #define ZS_SIZE_CLASS_DELTA	(PAGE_SIZE >> CLASS_BITS)
+ #define ZS_SIZE_CLASSES	(DIV_ROUND_UP(ZS_MAX_ALLOC_SIZE - ZS_MIN_ALLOC_SIZE, \
+ 				      ZS_SIZE_CLASS_DELTA) + 1)
+ 
+ enum fullness_group {
+ 	ZS_EMPTY,
+ 	ZS_ALMOST_EMPTY,
+ 	ZS_ALMOST_FULL,
+ 	ZS_FULL,
+ 	NR_ZS_FULLNESS,
+ };
+ 
+ enum zs_stat_type {
+ 	CLASS_EMPTY,
+ 	CLASS_ALMOST_EMPTY,
+ 	CLASS_ALMOST_FULL,
+ 	CLASS_FULL,
+ 	OBJ_ALLOCATED,
+ 	OBJ_USED,
+ 	NR_ZS_STAT_TYPE,
+ };
+ 
+ struct zs_size_stat {
+ 	unsigned long objs[NR_ZS_STAT_TYPE];
+ };
+ 
+ #ifdef CONFIG_ZSMALLOC_STAT
+ static struct dentry *zs_stat_root;
+ #endif
+ 
+ #ifdef CONFIG_COMPACTION
+ static struct vfsmount *zsmalloc_mnt;
+ #endif
+ 
+ /*
++>>>>>>> cf8e0fedf078 (mm/zsmalloc: simplify zs_max_alloc_size handling)
   * We assign a page to ZS_ALMOST_EMPTY fullness group when:
   *	n <= N / f, where
   * n = number of allocated objects
@@@ -215,20 -244,47 +258,46 @@@ struct link_free 
  };
  
  struct zs_pool {
++<<<<<<< HEAD
 +	struct size_class size_class[ZS_SIZE_CLASSES];
++=======
+ 	const char *name;
+ 
+ 	struct size_class *size_class[ZS_SIZE_CLASSES];
+ 	struct kmem_cache *handle_cachep;
+ 	struct kmem_cache *zspage_cachep;
++>>>>>>> cf8e0fedf078 (mm/zsmalloc: simplify zs_max_alloc_size handling)
  
 +	gfp_t flags;	/* allocation flags used when growing pool */
  	atomic_long_t pages_allocated;
 -
 -	struct zs_pool_stats stats;
 -
 -	/* Compact classes */
 -	struct shrinker shrinker;
 -	/*
 -	 * To signify that register_shrinker() was successful
 -	 * and unregister_shrinker() will not Oops.
 -	 */
 -	bool shrinker_enabled;
 -#ifdef CONFIG_ZSMALLOC_STAT
 -	struct dentry *stat_dentry;
 -#endif
 -#ifdef CONFIG_COMPACTION
 -	struct inode *inode;
 -	struct work_struct free_work;
 -#endif
  };
  
++<<<<<<< HEAD
 +/*
 + * A zspage's class index and fullness group
 + * are encoded in its (first)page->mapping
 + */
 +#define CLASS_IDX_BITS	28
 +#define FULLNESS_BITS	4
 +#define CLASS_IDX_MASK	((1 << CLASS_IDX_BITS) - 1)
 +#define FULLNESS_MASK	((1 << FULLNESS_BITS) - 1)
++=======
+ struct zspage {
+ 	struct {
+ 		unsigned int fullness:FULLNESS_BITS;
+ 		unsigned int class:CLASS_BITS + 1;
+ 		unsigned int isolated:ISOLATED_BITS;
+ 		unsigned int magic:MAGIC_VAL_BITS;
+ 	};
+ 	unsigned int inuse;
+ 	unsigned int freeobj;
+ 	struct page *first_page;
+ 	struct list_head list; /* fullness list */
+ #ifdef CONFIG_COMPACTION
+ 	rwlock_t lock;
+ #endif
+ };
++>>>>>>> cf8e0fedf078 (mm/zsmalloc: simplify zs_max_alloc_size handling)
  
  struct mapping_area {
  #ifdef CONFIG_PGTABLE_MAPPING
@@@ -367,9 -548,168 +436,174 @@@ static int get_size_class_index(int siz
  		idx = DIV_ROUND_UP(size - ZS_MIN_ALLOC_SIZE,
  				ZS_SIZE_CLASS_DELTA);
  
++<<<<<<< HEAD
 +	return idx;
 +}
 +
++=======
+ 	return min_t(int, ZS_SIZE_CLASSES - 1, idx);
+ }
+ 
+ static inline void zs_stat_inc(struct size_class *class,
+ 				enum zs_stat_type type, unsigned long cnt)
+ {
+ 	class->stats.objs[type] += cnt;
+ }
+ 
+ static inline void zs_stat_dec(struct size_class *class,
+ 				enum zs_stat_type type, unsigned long cnt)
+ {
+ 	class->stats.objs[type] -= cnt;
+ }
+ 
+ static inline unsigned long zs_stat_get(struct size_class *class,
+ 				enum zs_stat_type type)
+ {
+ 	return class->stats.objs[type];
+ }
+ 
+ #ifdef CONFIG_ZSMALLOC_STAT
+ 
+ static void __init zs_stat_init(void)
+ {
+ 	if (!debugfs_initialized()) {
+ 		pr_warn("debugfs not available, stat dir not created\n");
+ 		return;
+ 	}
+ 
+ 	zs_stat_root = debugfs_create_dir("zsmalloc", NULL);
+ 	if (!zs_stat_root)
+ 		pr_warn("debugfs 'zsmalloc' stat dir creation failed\n");
+ }
+ 
+ static void __exit zs_stat_exit(void)
+ {
+ 	debugfs_remove_recursive(zs_stat_root);
+ }
+ 
+ static unsigned long zs_can_compact(struct size_class *class);
+ 
+ static int zs_stats_size_show(struct seq_file *s, void *v)
+ {
+ 	int i;
+ 	struct zs_pool *pool = s->private;
+ 	struct size_class *class;
+ 	int objs_per_zspage;
+ 	unsigned long class_almost_full, class_almost_empty;
+ 	unsigned long obj_allocated, obj_used, pages_used, freeable;
+ 	unsigned long total_class_almost_full = 0, total_class_almost_empty = 0;
+ 	unsigned long total_objs = 0, total_used_objs = 0, total_pages = 0;
+ 	unsigned long total_freeable = 0;
+ 
+ 	seq_printf(s, " %5s %5s %11s %12s %13s %10s %10s %16s %8s\n",
+ 			"class", "size", "almost_full", "almost_empty",
+ 			"obj_allocated", "obj_used", "pages_used",
+ 			"pages_per_zspage", "freeable");
+ 
+ 	for (i = 0; i < ZS_SIZE_CLASSES; i++) {
+ 		class = pool->size_class[i];
+ 
+ 		if (class->index != i)
+ 			continue;
+ 
+ 		spin_lock(&class->lock);
+ 		class_almost_full = zs_stat_get(class, CLASS_ALMOST_FULL);
+ 		class_almost_empty = zs_stat_get(class, CLASS_ALMOST_EMPTY);
+ 		obj_allocated = zs_stat_get(class, OBJ_ALLOCATED);
+ 		obj_used = zs_stat_get(class, OBJ_USED);
+ 		freeable = zs_can_compact(class);
+ 		spin_unlock(&class->lock);
+ 
+ 		objs_per_zspage = class->objs_per_zspage;
+ 		pages_used = obj_allocated / objs_per_zspage *
+ 				class->pages_per_zspage;
+ 
+ 		seq_printf(s, " %5u %5u %11lu %12lu %13lu"
+ 				" %10lu %10lu %16d %8lu\n",
+ 			i, class->size, class_almost_full, class_almost_empty,
+ 			obj_allocated, obj_used, pages_used,
+ 			class->pages_per_zspage, freeable);
+ 
+ 		total_class_almost_full += class_almost_full;
+ 		total_class_almost_empty += class_almost_empty;
+ 		total_objs += obj_allocated;
+ 		total_used_objs += obj_used;
+ 		total_pages += pages_used;
+ 		total_freeable += freeable;
+ 	}
+ 
+ 	seq_puts(s, "\n");
+ 	seq_printf(s, " %5s %5s %11lu %12lu %13lu %10lu %10lu %16s %8lu\n",
+ 			"Total", "", total_class_almost_full,
+ 			total_class_almost_empty, total_objs,
+ 			total_used_objs, total_pages, "", total_freeable);
+ 
+ 	return 0;
+ }
+ 
+ static int zs_stats_size_open(struct inode *inode, struct file *file)
+ {
+ 	return single_open(file, zs_stats_size_show, inode->i_private);
+ }
+ 
+ static const struct file_operations zs_stat_size_ops = {
+ 	.open           = zs_stats_size_open,
+ 	.read           = seq_read,
+ 	.llseek         = seq_lseek,
+ 	.release        = single_release,
+ };
+ 
+ static void zs_pool_stat_create(struct zs_pool *pool, const char *name)
+ {
+ 	struct dentry *entry;
+ 
+ 	if (!zs_stat_root) {
+ 		pr_warn("no root stat dir, not creating <%s> stat dir\n", name);
+ 		return;
+ 	}
+ 
+ 	entry = debugfs_create_dir(name, zs_stat_root);
+ 	if (!entry) {
+ 		pr_warn("debugfs dir <%s> creation failed\n", name);
+ 		return;
+ 	}
+ 	pool->stat_dentry = entry;
+ 
+ 	entry = debugfs_create_file("classes", S_IFREG | S_IRUGO,
+ 			pool->stat_dentry, pool, &zs_stat_size_ops);
+ 	if (!entry) {
+ 		pr_warn("%s: debugfs file entry <%s> creation failed\n",
+ 				name, "classes");
+ 		debugfs_remove_recursive(pool->stat_dentry);
+ 		pool->stat_dentry = NULL;
+ 	}
+ }
+ 
+ static void zs_pool_stat_destroy(struct zs_pool *pool)
+ {
+ 	debugfs_remove_recursive(pool->stat_dentry);
+ }
+ 
+ #else /* CONFIG_ZSMALLOC_STAT */
+ static void __init zs_stat_init(void)
+ {
+ }
+ 
+ static void __exit zs_stat_exit(void)
+ {
+ }
+ 
+ static inline void zs_pool_stat_create(struct zs_pool *pool, const char *name)
+ {
+ }
+ 
+ static inline void zs_pool_stat_destroy(struct zs_pool *pool)
+ {
+ }
+ #endif
+ 
+ 
++>>>>>>> cf8e0fedf078 (mm/zsmalloc: simplify zs_max_alloc_size handling)
  /*
   * For each size class, zspages are divided into different groups
   * depending on how "full" they are. This was done so that we could
@@@ -855,238 -1274,43 +1089,243 @@@ out
  
  #endif /* CONFIG_PGTABLE_MAPPING */
  
 -static int zs_cpu_prepare(unsigned int cpu)
 +static int zs_cpu_notifier(struct notifier_block *nb, unsigned long action,
 +				void *pcpu)
  {
 +	int ret, cpu = (long)pcpu;
  	struct mapping_area *area;
  
 -	area = &per_cpu(zs_map_area, cpu);
 -	return __zs_cpu_up(area);
 +	switch (action) {
 +	case CPU_UP_PREPARE:
 +		area = &per_cpu(zs_map_area, cpu);
 +		ret = __zs_cpu_up(area);
 +		if (ret)
 +			return notifier_from_errno(ret);
 +		break;
 +	case CPU_DEAD:
 +	case CPU_UP_CANCELED:
 +		area = &per_cpu(zs_map_area, cpu);
 +		__zs_cpu_down(area);
 +		break;
 +	}
 +
 +	return NOTIFY_OK;
 +}
 +
 +static struct notifier_block zs_cpu_nb = {
 +	.notifier_call = zs_cpu_notifier
 +};
 +
 +static void zs_exit(void)
 +{
 +	int cpu;
 +
 +#ifdef CONFIG_ZPOOL
 +	zpool_unregister_driver(&zs_zpool_driver);
 +#endif
 +
 +	cpu_notifier_register_begin();
 +
 +	for_each_online_cpu(cpu)
 +		zs_cpu_notifier(NULL, CPU_DEAD, (void *)(long)cpu);
 +	__unregister_cpu_notifier(&zs_cpu_nb);
 +
 +	cpu_notifier_register_done();
  }
  
 -static int zs_cpu_dead(unsigned int cpu)
 +static int zs_init(void)
  {
 -	struct mapping_area *area;
 +	int cpu, ret;
 +
 +	cpu_notifier_register_begin();
 +
 +	__register_cpu_notifier(&zs_cpu_nb);
 +	for_each_online_cpu(cpu) {
 +		ret = zs_cpu_notifier(NULL, CPU_UP_PREPARE, (void *)(long)cpu);
 +		if (notifier_to_errno(ret)) {
 +			cpu_notifier_register_done();
 +			goto fail;
 +		}
 +	}
 +
 +	cpu_notifier_register_done();
 +
 +#ifdef CONFIG_ZPOOL
 +	zpool_register_driver(&zs_zpool_driver);
 +#endif
  
 -	area = &per_cpu(zs_map_area, cpu);
 -	__zs_cpu_down(area);
  	return 0;
 +fail:
 +	zs_exit();
 +	return notifier_to_errno(ret);
  }
  
++<<<<<<< HEAD
 +/**
 + * zs_create_pool - Creates an allocation pool to work from.
 + * @flags: allocation flags used to allocate pool metadata
 + *
 + * This function must be called before anything when using
 + * the zsmalloc allocator.
 + *
 + * On success, a pointer to the newly created pool is returned,
 + * otherwise NULL.
 + */
 +struct zs_pool *zs_create_pool(char *name, gfp_t flags)
 +{
 +	int i, ovhd_size;
 +	struct zs_pool *pool;
 +
 +	ovhd_size = roundup(sizeof(*pool), PAGE_SIZE);
 +	pool = kzalloc(ovhd_size, GFP_KERNEL);
 +	if (!pool)
 +		return NULL;
 +
 +	for (i = 0; i < ZS_SIZE_CLASSES; i++) {
 +		int size;
 +		struct size_class *class;
 +
 +		size = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;
 +		if (size > ZS_MAX_ALLOC_SIZE)
 +			size = ZS_MAX_ALLOC_SIZE;
 +
 +		class = &pool->size_class[i];
 +		class->size = size;
 +		class->index = i;
 +		spin_lock_init(&class->lock);
 +		class->pages_per_zspage = get_pages_per_zspage(size);
 +
 +	}
 +
 +	pool->flags = flags;
 +
 +	return pool;
 +}
 +EXPORT_SYMBOL_GPL(zs_create_pool);
 +
 +void zs_destroy_pool(struct zs_pool *pool)
++=======
+ static bool can_merge(struct size_class *prev, int pages_per_zspage,
+ 					int objs_per_zspage)
++>>>>>>> cf8e0fedf078 (mm/zsmalloc: simplify zs_max_alloc_size handling)
  {
 -	if (prev->pages_per_zspage == pages_per_zspage &&
 -		prev->objs_per_zspage == objs_per_zspage)
 -		return true;
 +	int i;
  
 -	return false;
 +	for (i = 0; i < ZS_SIZE_CLASSES; i++) {
 +		int fg;
 +		struct size_class *class = &pool->size_class[i];
 +
 +		for (fg = 0; fg < _ZS_NR_FULLNESS_GROUPS; fg++) {
 +			if (class->fullness_list[fg]) {
 +				pr_info("Freeing non-empty class with size %db, fullness group %d\n",
 +					class->size, fg);
 +			}
 +		}
 +	}
 +	kfree(pool);
  }
 +EXPORT_SYMBOL_GPL(zs_destroy_pool);
  
 -static bool zspage_full(struct size_class *class, struct zspage *zspage)
 +/**
 + * zs_malloc - Allocate block of given size from pool.
 + * @pool: pool to allocate from
 + * @size: size of block to allocate
 + *
 + * On success, handle to the allocated object is returned,
 + * otherwise 0.
 + * Allocation requests with size > ZS_MAX_ALLOC_SIZE will fail.
 + */
 +unsigned long zs_malloc(struct zs_pool *pool, size_t size)
  {
 -	return get_zspage_inuse(zspage) == class->objs_per_zspage;
 +	unsigned long obj;
 +	struct link_free *link;
 +	int class_idx;
 +	struct size_class *class;
 +
 +	struct page *first_page, *m_page;
 +	unsigned long m_objidx, m_offset;
 +
 +	if (unlikely(!size || size > ZS_MAX_ALLOC_SIZE))
 +		return 0;
 +
 +	class_idx = get_size_class_index(size);
 +	class = &pool->size_class[class_idx];
 +	BUG_ON(class_idx != class->index);
 +
 +	spin_lock(&class->lock);
 +	first_page = find_get_zspage(class);
 +
 +	if (!first_page) {
 +		spin_unlock(&class->lock);
 +		first_page = alloc_zspage(class, pool->flags);
 +		if (unlikely(!first_page))
 +			return 0;
 +
 +		set_zspage_mapping(first_page, class->index, ZS_EMPTY);
 +		atomic_long_add(class->pages_per_zspage,
 +					&pool->pages_allocated);
 +		spin_lock(&class->lock);
 +	}
 +
 +	obj = (unsigned long)first_page->freelist;
 +	obj_handle_to_location(obj, &m_page, &m_objidx);
 +	m_offset = obj_idx_to_offset(m_page, m_objidx, class->size);
 +
 +	link = (struct link_free *)kmap_atomic(m_page) +
 +					m_offset / sizeof(*link);
 +	first_page->freelist = link->next;
 +	memset(link, POISON_INUSE, sizeof(*link));
 +	kunmap_atomic(link);
 +
 +	first_page->inuse++;
 +	/* Now move the zspage to another fullness group, if required */
 +	fix_fullness_group(pool, first_page);
 +	spin_unlock(&class->lock);
 +
 +	return obj;
  }
 +EXPORT_SYMBOL_GPL(zs_malloc);
  
 -unsigned long zs_get_total_pages(struct zs_pool *pool)
 +void zs_free(struct zs_pool *pool, unsigned long obj)
  {
 -	return atomic_long_read(&pool->pages_allocated);
 +	struct link_free *link;
 +	struct page *first_page, *f_page;
 +	unsigned long f_objidx, f_offset;
 +
 +	int class_idx;
 +	struct size_class *class;
 +	enum fullness_group fullness;
 +
 +	if (unlikely(!obj))
 +		return;
 +
 +	obj_handle_to_location(obj, &f_page, &f_objidx);
 +	first_page = get_first_page(f_page);
 +
 +	get_zspage_mapping(first_page, &class_idx, &fullness);
 +	class = &pool->size_class[class_idx];
 +	f_offset = obj_idx_to_offset(f_page, f_objidx, class->size);
 +
 +	spin_lock(&class->lock);
 +
 +	/* Insert this object in containing zspage's freelist */
 +	link = (struct link_free *)((unsigned char *)kmap_atomic(f_page)
 +							+ f_offset);
 +	link->next = first_page->freelist;
 +	kunmap_atomic(link);
 +	first_page->freelist = (void *)obj;
 +
 +	first_page->inuse--;
 +	fullness = fix_fullness_group(pool, first_page);
 +	spin_unlock(&class->lock);
 +
 +	if (fullness == ZS_EMPTY) {
 +		atomic_long_sub(class->pages_per_zspage,
 +				&pool->pages_allocated);
 +		free_zspage(first_page);
 +	}
  }
 -EXPORT_SYMBOL_GPL(zs_get_total_pages);
 +EXPORT_SYMBOL_GPL(zs_free);
  
  /**
   * zs_map_object - get address of allocated object from handle.
@@@ -1178,11 -1420,1082 +1417,1083 @@@ void zs_unmap_object(struct zs_pool *po
  }
  EXPORT_SYMBOL_GPL(zs_unmap_object);
  
 -static unsigned long obj_malloc(struct size_class *class,
 -				struct zspage *zspage, unsigned long handle)
 +unsigned long zs_get_total_pages(struct zs_pool *pool)
  {
++<<<<<<< HEAD
 +	return atomic_long_read(&pool->pages_allocated);
++=======
+ 	int i, nr_page, offset;
+ 	unsigned long obj;
+ 	struct link_free *link;
+ 
+ 	struct page *m_page;
+ 	unsigned long m_offset;
+ 	void *vaddr;
+ 
+ 	handle |= OBJ_ALLOCATED_TAG;
+ 	obj = get_freeobj(zspage);
+ 
+ 	offset = obj * class->size;
+ 	nr_page = offset >> PAGE_SHIFT;
+ 	m_offset = offset & ~PAGE_MASK;
+ 	m_page = get_first_page(zspage);
+ 
+ 	for (i = 0; i < nr_page; i++)
+ 		m_page = get_next_page(m_page);
+ 
+ 	vaddr = kmap_atomic(m_page);
+ 	link = (struct link_free *)vaddr + m_offset / sizeof(*link);
+ 	set_freeobj(zspage, link->next >> OBJ_TAG_BITS);
+ 	if (likely(!PageHugeObject(m_page)))
+ 		/* record handle in the header of allocated chunk */
+ 		link->handle = handle;
+ 	else
+ 		/* record handle to page->index */
+ 		zspage->first_page->index = handle;
+ 
+ 	kunmap_atomic(vaddr);
+ 	mod_zspage_inuse(zspage, 1);
+ 	zs_stat_inc(class, OBJ_USED, 1);
+ 
+ 	obj = location_to_obj(m_page, obj);
+ 
+ 	return obj;
+ }
+ 
+ 
+ /**
+  * zs_malloc - Allocate block of given size from pool.
+  * @pool: pool to allocate from
+  * @size: size of block to allocate
+  * @gfp: gfp flags when allocating object
+  *
+  * On success, handle to the allocated object is returned,
+  * otherwise 0.
+  * Allocation requests with size > ZS_MAX_ALLOC_SIZE will fail.
+  */
+ unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)
+ {
+ 	unsigned long handle, obj;
+ 	struct size_class *class;
+ 	enum fullness_group newfg;
+ 	struct zspage *zspage;
+ 
+ 	if (unlikely(!size || size > ZS_MAX_ALLOC_SIZE))
+ 		return 0;
+ 
+ 	handle = cache_alloc_handle(pool, gfp);
+ 	if (!handle)
+ 		return 0;
+ 
+ 	/* extra space in chunk to keep the handle */
+ 	size += ZS_HANDLE_SIZE;
+ 	class = pool->size_class[get_size_class_index(size)];
+ 
+ 	spin_lock(&class->lock);
+ 	zspage = find_get_zspage(class);
+ 	if (likely(zspage)) {
+ 		obj = obj_malloc(class, zspage, handle);
+ 		/* Now move the zspage to another fullness group, if required */
+ 		fix_fullness_group(class, zspage);
+ 		record_obj(handle, obj);
+ 		spin_unlock(&class->lock);
+ 
+ 		return handle;
+ 	}
+ 
+ 	spin_unlock(&class->lock);
+ 
+ 	zspage = alloc_zspage(pool, class, gfp);
+ 	if (!zspage) {
+ 		cache_free_handle(pool, handle);
+ 		return 0;
+ 	}
+ 
+ 	spin_lock(&class->lock);
+ 	obj = obj_malloc(class, zspage, handle);
+ 	newfg = get_fullness_group(class, zspage);
+ 	insert_zspage(class, zspage, newfg);
+ 	set_zspage_mapping(zspage, class->index, newfg);
+ 	record_obj(handle, obj);
+ 	atomic_long_add(class->pages_per_zspage,
+ 				&pool->pages_allocated);
+ 	zs_stat_inc(class, OBJ_ALLOCATED, class->objs_per_zspage);
+ 
+ 	/* We completely set up zspage so mark them as movable */
+ 	SetZsPageMovable(pool, zspage);
+ 	spin_unlock(&class->lock);
+ 
+ 	return handle;
+ }
+ EXPORT_SYMBOL_GPL(zs_malloc);
+ 
+ static void obj_free(struct size_class *class, unsigned long obj)
+ {
+ 	struct link_free *link;
+ 	struct zspage *zspage;
+ 	struct page *f_page;
+ 	unsigned long f_offset;
+ 	unsigned int f_objidx;
+ 	void *vaddr;
+ 
+ 	obj &= ~OBJ_ALLOCATED_TAG;
+ 	obj_to_location(obj, &f_page, &f_objidx);
+ 	f_offset = (class->size * f_objidx) & ~PAGE_MASK;
+ 	zspage = get_zspage(f_page);
+ 
+ 	vaddr = kmap_atomic(f_page);
+ 
+ 	/* Insert this object in containing zspage's freelist */
+ 	link = (struct link_free *)(vaddr + f_offset);
+ 	link->next = get_freeobj(zspage) << OBJ_TAG_BITS;
+ 	kunmap_atomic(vaddr);
+ 	set_freeobj(zspage, f_objidx);
+ 	mod_zspage_inuse(zspage, -1);
+ 	zs_stat_dec(class, OBJ_USED, 1);
+ }
+ 
+ void zs_free(struct zs_pool *pool, unsigned long handle)
+ {
+ 	struct zspage *zspage;
+ 	struct page *f_page;
+ 	unsigned long obj;
+ 	unsigned int f_objidx;
+ 	int class_idx;
+ 	struct size_class *class;
+ 	enum fullness_group fullness;
+ 	bool isolated;
+ 
+ 	if (unlikely(!handle))
+ 		return;
+ 
+ 	pin_tag(handle);
+ 	obj = handle_to_obj(handle);
+ 	obj_to_location(obj, &f_page, &f_objidx);
+ 	zspage = get_zspage(f_page);
+ 
+ 	migrate_read_lock(zspage);
+ 
+ 	get_zspage_mapping(zspage, &class_idx, &fullness);
+ 	class = pool->size_class[class_idx];
+ 
+ 	spin_lock(&class->lock);
+ 	obj_free(class, obj);
+ 	fullness = fix_fullness_group(class, zspage);
+ 	if (fullness != ZS_EMPTY) {
+ 		migrate_read_unlock(zspage);
+ 		goto out;
+ 	}
+ 
+ 	isolated = is_zspage_isolated(zspage);
+ 	migrate_read_unlock(zspage);
+ 	/* If zspage is isolated, zs_page_putback will free the zspage */
+ 	if (likely(!isolated))
+ 		free_zspage(pool, class, zspage);
+ out:
+ 
+ 	spin_unlock(&class->lock);
+ 	unpin_tag(handle);
+ 	cache_free_handle(pool, handle);
+ }
+ EXPORT_SYMBOL_GPL(zs_free);
+ 
+ static void zs_object_copy(struct size_class *class, unsigned long dst,
+ 				unsigned long src)
+ {
+ 	struct page *s_page, *d_page;
+ 	unsigned int s_objidx, d_objidx;
+ 	unsigned long s_off, d_off;
+ 	void *s_addr, *d_addr;
+ 	int s_size, d_size, size;
+ 	int written = 0;
+ 
+ 	s_size = d_size = class->size;
+ 
+ 	obj_to_location(src, &s_page, &s_objidx);
+ 	obj_to_location(dst, &d_page, &d_objidx);
+ 
+ 	s_off = (class->size * s_objidx) & ~PAGE_MASK;
+ 	d_off = (class->size * d_objidx) & ~PAGE_MASK;
+ 
+ 	if (s_off + class->size > PAGE_SIZE)
+ 		s_size = PAGE_SIZE - s_off;
+ 
+ 	if (d_off + class->size > PAGE_SIZE)
+ 		d_size = PAGE_SIZE - d_off;
+ 
+ 	s_addr = kmap_atomic(s_page);
+ 	d_addr = kmap_atomic(d_page);
+ 
+ 	while (1) {
+ 		size = min(s_size, d_size);
+ 		memcpy(d_addr + d_off, s_addr + s_off, size);
+ 		written += size;
+ 
+ 		if (written == class->size)
+ 			break;
+ 
+ 		s_off += size;
+ 		s_size -= size;
+ 		d_off += size;
+ 		d_size -= size;
+ 
+ 		if (s_off >= PAGE_SIZE) {
+ 			kunmap_atomic(d_addr);
+ 			kunmap_atomic(s_addr);
+ 			s_page = get_next_page(s_page);
+ 			s_addr = kmap_atomic(s_page);
+ 			d_addr = kmap_atomic(d_page);
+ 			s_size = class->size - written;
+ 			s_off = 0;
+ 		}
+ 
+ 		if (d_off >= PAGE_SIZE) {
+ 			kunmap_atomic(d_addr);
+ 			d_page = get_next_page(d_page);
+ 			d_addr = kmap_atomic(d_page);
+ 			d_size = class->size - written;
+ 			d_off = 0;
+ 		}
+ 	}
+ 
+ 	kunmap_atomic(d_addr);
+ 	kunmap_atomic(s_addr);
+ }
+ 
+ /*
+  * Find alloced object in zspage from index object and
+  * return handle.
+  */
+ static unsigned long find_alloced_obj(struct size_class *class,
+ 					struct page *page, int *obj_idx)
+ {
+ 	unsigned long head;
+ 	int offset = 0;
+ 	int index = *obj_idx;
+ 	unsigned long handle = 0;
+ 	void *addr = kmap_atomic(page);
+ 
+ 	offset = get_first_obj_offset(page);
+ 	offset += class->size * index;
+ 
+ 	while (offset < PAGE_SIZE) {
+ 		head = obj_to_head(page, addr + offset);
+ 		if (head & OBJ_ALLOCATED_TAG) {
+ 			handle = head & ~OBJ_ALLOCATED_TAG;
+ 			if (trypin_tag(handle))
+ 				break;
+ 			handle = 0;
+ 		}
+ 
+ 		offset += class->size;
+ 		index++;
+ 	}
+ 
+ 	kunmap_atomic(addr);
+ 
+ 	*obj_idx = index;
+ 
+ 	return handle;
+ }
+ 
+ struct zs_compact_control {
+ 	/* Source spage for migration which could be a subpage of zspage */
+ 	struct page *s_page;
+ 	/* Destination page for migration which should be a first page
+ 	 * of zspage. */
+ 	struct page *d_page;
+ 	 /* Starting object index within @s_page which used for live object
+ 	  * in the subpage. */
+ 	int obj_idx;
+ };
+ 
+ static int migrate_zspage(struct zs_pool *pool, struct size_class *class,
+ 				struct zs_compact_control *cc)
+ {
+ 	unsigned long used_obj, free_obj;
+ 	unsigned long handle;
+ 	struct page *s_page = cc->s_page;
+ 	struct page *d_page = cc->d_page;
+ 	int obj_idx = cc->obj_idx;
+ 	int ret = 0;
+ 
+ 	while (1) {
+ 		handle = find_alloced_obj(class, s_page, &obj_idx);
+ 		if (!handle) {
+ 			s_page = get_next_page(s_page);
+ 			if (!s_page)
+ 				break;
+ 			obj_idx = 0;
+ 			continue;
+ 		}
+ 
+ 		/* Stop if there is no more space */
+ 		if (zspage_full(class, get_zspage(d_page))) {
+ 			unpin_tag(handle);
+ 			ret = -ENOMEM;
+ 			break;
+ 		}
+ 
+ 		used_obj = handle_to_obj(handle);
+ 		free_obj = obj_malloc(class, get_zspage(d_page), handle);
+ 		zs_object_copy(class, free_obj, used_obj);
+ 		obj_idx++;
+ 		/*
+ 		 * record_obj updates handle's value to free_obj and it will
+ 		 * invalidate lock bit(ie, HANDLE_PIN_BIT) of handle, which
+ 		 * breaks synchronization using pin_tag(e,g, zs_free) so
+ 		 * let's keep the lock bit.
+ 		 */
+ 		free_obj |= BIT(HANDLE_PIN_BIT);
+ 		record_obj(handle, free_obj);
+ 		unpin_tag(handle);
+ 		obj_free(class, used_obj);
+ 	}
+ 
+ 	/* Remember last position in this iteration */
+ 	cc->s_page = s_page;
+ 	cc->obj_idx = obj_idx;
+ 
+ 	return ret;
+ }
+ 
+ static struct zspage *isolate_zspage(struct size_class *class, bool source)
+ {
+ 	int i;
+ 	struct zspage *zspage;
+ 	enum fullness_group fg[2] = {ZS_ALMOST_EMPTY, ZS_ALMOST_FULL};
+ 
+ 	if (!source) {
+ 		fg[0] = ZS_ALMOST_FULL;
+ 		fg[1] = ZS_ALMOST_EMPTY;
+ 	}
+ 
+ 	for (i = 0; i < 2; i++) {
+ 		zspage = list_first_entry_or_null(&class->fullness_list[fg[i]],
+ 							struct zspage, list);
+ 		if (zspage) {
+ 			VM_BUG_ON(is_zspage_isolated(zspage));
+ 			remove_zspage(class, zspage, fg[i]);
+ 			return zspage;
+ 		}
+ 	}
+ 
+ 	return zspage;
+ }
+ 
+ /*
+  * putback_zspage - add @zspage into right class's fullness list
+  * @class: destination class
+  * @zspage: target page
+  *
+  * Return @zspage's fullness_group
+  */
+ static enum fullness_group putback_zspage(struct size_class *class,
+ 			struct zspage *zspage)
+ {
+ 	enum fullness_group fullness;
+ 
+ 	VM_BUG_ON(is_zspage_isolated(zspage));
+ 
+ 	fullness = get_fullness_group(class, zspage);
+ 	insert_zspage(class, zspage, fullness);
+ 	set_zspage_mapping(zspage, class->index, fullness);
+ 
+ 	return fullness;
+ }
+ 
+ #ifdef CONFIG_COMPACTION
+ static struct dentry *zs_mount(struct file_system_type *fs_type,
+ 				int flags, const char *dev_name, void *data)
+ {
+ 	static const struct dentry_operations ops = {
+ 		.d_dname = simple_dname,
+ 	};
+ 
+ 	return mount_pseudo(fs_type, "zsmalloc:", NULL, &ops, ZSMALLOC_MAGIC);
+ }
+ 
+ static struct file_system_type zsmalloc_fs = {
+ 	.name		= "zsmalloc",
+ 	.mount		= zs_mount,
+ 	.kill_sb	= kill_anon_super,
+ };
+ 
+ static int zsmalloc_mount(void)
+ {
+ 	int ret = 0;
+ 
+ 	zsmalloc_mnt = kern_mount(&zsmalloc_fs);
+ 	if (IS_ERR(zsmalloc_mnt))
+ 		ret = PTR_ERR(zsmalloc_mnt);
+ 
+ 	return ret;
+ }
+ 
+ static void zsmalloc_unmount(void)
+ {
+ 	kern_unmount(zsmalloc_mnt);
+ }
+ 
+ static void migrate_lock_init(struct zspage *zspage)
+ {
+ 	rwlock_init(&zspage->lock);
+ }
+ 
+ static void migrate_read_lock(struct zspage *zspage)
+ {
+ 	read_lock(&zspage->lock);
+ }
+ 
+ static void migrate_read_unlock(struct zspage *zspage)
+ {
+ 	read_unlock(&zspage->lock);
+ }
+ 
+ static void migrate_write_lock(struct zspage *zspage)
+ {
+ 	write_lock(&zspage->lock);
+ }
+ 
+ static void migrate_write_unlock(struct zspage *zspage)
+ {
+ 	write_unlock(&zspage->lock);
+ }
+ 
+ /* Number of isolated subpage for *page migration* in this zspage */
+ static void inc_zspage_isolation(struct zspage *zspage)
+ {
+ 	zspage->isolated++;
+ }
+ 
+ static void dec_zspage_isolation(struct zspage *zspage)
+ {
+ 	zspage->isolated--;
+ }
+ 
+ static void replace_sub_page(struct size_class *class, struct zspage *zspage,
+ 				struct page *newpage, struct page *oldpage)
+ {
+ 	struct page *page;
+ 	struct page *pages[ZS_MAX_PAGES_PER_ZSPAGE] = {NULL, };
+ 	int idx = 0;
+ 
+ 	page = get_first_page(zspage);
+ 	do {
+ 		if (page == oldpage)
+ 			pages[idx] = newpage;
+ 		else
+ 			pages[idx] = page;
+ 		idx++;
+ 	} while ((page = get_next_page(page)) != NULL);
+ 
+ 	create_page_chain(class, zspage, pages);
+ 	set_first_obj_offset(newpage, get_first_obj_offset(oldpage));
+ 	if (unlikely(PageHugeObject(oldpage)))
+ 		newpage->index = oldpage->index;
+ 	__SetPageMovable(newpage, page_mapping(oldpage));
+ }
+ 
+ bool zs_page_isolate(struct page *page, isolate_mode_t mode)
+ {
+ 	struct zs_pool *pool;
+ 	struct size_class *class;
+ 	int class_idx;
+ 	enum fullness_group fullness;
+ 	struct zspage *zspage;
+ 	struct address_space *mapping;
+ 
+ 	/*
+ 	 * Page is locked so zspage couldn't be destroyed. For detail, look at
+ 	 * lock_zspage in free_zspage.
+ 	 */
+ 	VM_BUG_ON_PAGE(!PageMovable(page), page);
+ 	VM_BUG_ON_PAGE(PageIsolated(page), page);
+ 
+ 	zspage = get_zspage(page);
+ 
+ 	/*
+ 	 * Without class lock, fullness could be stale while class_idx is okay
+ 	 * because class_idx is constant unless page is freed so we should get
+ 	 * fullness again under class lock.
+ 	 */
+ 	get_zspage_mapping(zspage, &class_idx, &fullness);
+ 	mapping = page_mapping(page);
+ 	pool = mapping->private_data;
+ 	class = pool->size_class[class_idx];
+ 
+ 	spin_lock(&class->lock);
+ 	if (get_zspage_inuse(zspage) == 0) {
+ 		spin_unlock(&class->lock);
+ 		return false;
+ 	}
+ 
+ 	/* zspage is isolated for object migration */
+ 	if (list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {
+ 		spin_unlock(&class->lock);
+ 		return false;
+ 	}
+ 
+ 	/*
+ 	 * If this is first time isolation for the zspage, isolate zspage from
+ 	 * size_class to prevent further object allocation from the zspage.
+ 	 */
+ 	if (!list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {
+ 		get_zspage_mapping(zspage, &class_idx, &fullness);
+ 		remove_zspage(class, zspage, fullness);
+ 	}
+ 
+ 	inc_zspage_isolation(zspage);
+ 	spin_unlock(&class->lock);
+ 
+ 	return true;
+ }
+ 
+ int zs_page_migrate(struct address_space *mapping, struct page *newpage,
+ 		struct page *page, enum migrate_mode mode)
+ {
+ 	struct zs_pool *pool;
+ 	struct size_class *class;
+ 	int class_idx;
+ 	enum fullness_group fullness;
+ 	struct zspage *zspage;
+ 	struct page *dummy;
+ 	void *s_addr, *d_addr, *addr;
+ 	int offset, pos;
+ 	unsigned long handle, head;
+ 	unsigned long old_obj, new_obj;
+ 	unsigned int obj_idx;
+ 	int ret = -EAGAIN;
+ 
+ 	VM_BUG_ON_PAGE(!PageMovable(page), page);
+ 	VM_BUG_ON_PAGE(!PageIsolated(page), page);
+ 
+ 	zspage = get_zspage(page);
+ 
+ 	/* Concurrent compactor cannot migrate any subpage in zspage */
+ 	migrate_write_lock(zspage);
+ 	get_zspage_mapping(zspage, &class_idx, &fullness);
+ 	pool = mapping->private_data;
+ 	class = pool->size_class[class_idx];
+ 	offset = get_first_obj_offset(page);
+ 
+ 	spin_lock(&class->lock);
+ 	if (!get_zspage_inuse(zspage)) {
+ 		ret = -EBUSY;
+ 		goto unlock_class;
+ 	}
+ 
+ 	pos = offset;
+ 	s_addr = kmap_atomic(page);
+ 	while (pos < PAGE_SIZE) {
+ 		head = obj_to_head(page, s_addr + pos);
+ 		if (head & OBJ_ALLOCATED_TAG) {
+ 			handle = head & ~OBJ_ALLOCATED_TAG;
+ 			if (!trypin_tag(handle))
+ 				goto unpin_objects;
+ 		}
+ 		pos += class->size;
+ 	}
+ 
+ 	/*
+ 	 * Here, any user cannot access all objects in the zspage so let's move.
+ 	 */
+ 	d_addr = kmap_atomic(newpage);
+ 	memcpy(d_addr, s_addr, PAGE_SIZE);
+ 	kunmap_atomic(d_addr);
+ 
+ 	for (addr = s_addr + offset; addr < s_addr + pos;
+ 					addr += class->size) {
+ 		head = obj_to_head(page, addr);
+ 		if (head & OBJ_ALLOCATED_TAG) {
+ 			handle = head & ~OBJ_ALLOCATED_TAG;
+ 			if (!testpin_tag(handle))
+ 				BUG();
+ 
+ 			old_obj = handle_to_obj(handle);
+ 			obj_to_location(old_obj, &dummy, &obj_idx);
+ 			new_obj = (unsigned long)location_to_obj(newpage,
+ 								obj_idx);
+ 			new_obj |= BIT(HANDLE_PIN_BIT);
+ 			record_obj(handle, new_obj);
+ 		}
+ 	}
+ 
+ 	replace_sub_page(class, zspage, newpage, page);
+ 	get_page(newpage);
+ 
+ 	dec_zspage_isolation(zspage);
+ 
+ 	/*
+ 	 * Page migration is done so let's putback isolated zspage to
+ 	 * the list if @page is final isolated subpage in the zspage.
+ 	 */
+ 	if (!is_zspage_isolated(zspage))
+ 		putback_zspage(class, zspage);
+ 
+ 	reset_page(page);
+ 	put_page(page);
+ 	page = newpage;
+ 
+ 	ret = MIGRATEPAGE_SUCCESS;
+ unpin_objects:
+ 	for (addr = s_addr + offset; addr < s_addr + pos;
+ 						addr += class->size) {
+ 		head = obj_to_head(page, addr);
+ 		if (head & OBJ_ALLOCATED_TAG) {
+ 			handle = head & ~OBJ_ALLOCATED_TAG;
+ 			if (!testpin_tag(handle))
+ 				BUG();
+ 			unpin_tag(handle);
+ 		}
+ 	}
+ 	kunmap_atomic(s_addr);
+ unlock_class:
+ 	spin_unlock(&class->lock);
+ 	migrate_write_unlock(zspage);
+ 
+ 	return ret;
+ }
+ 
+ void zs_page_putback(struct page *page)
+ {
+ 	struct zs_pool *pool;
+ 	struct size_class *class;
+ 	int class_idx;
+ 	enum fullness_group fg;
+ 	struct address_space *mapping;
+ 	struct zspage *zspage;
+ 
+ 	VM_BUG_ON_PAGE(!PageMovable(page), page);
+ 	VM_BUG_ON_PAGE(!PageIsolated(page), page);
+ 
+ 	zspage = get_zspage(page);
+ 	get_zspage_mapping(zspage, &class_idx, &fg);
+ 	mapping = page_mapping(page);
+ 	pool = mapping->private_data;
+ 	class = pool->size_class[class_idx];
+ 
+ 	spin_lock(&class->lock);
+ 	dec_zspage_isolation(zspage);
+ 	if (!is_zspage_isolated(zspage)) {
+ 		fg = putback_zspage(class, zspage);
+ 		/*
+ 		 * Due to page_lock, we cannot free zspage immediately
+ 		 * so let's defer.
+ 		 */
+ 		if (fg == ZS_EMPTY)
+ 			schedule_work(&pool->free_work);
+ 	}
+ 	spin_unlock(&class->lock);
+ }
+ 
+ const struct address_space_operations zsmalloc_aops = {
+ 	.isolate_page = zs_page_isolate,
+ 	.migratepage = zs_page_migrate,
+ 	.putback_page = zs_page_putback,
+ };
+ 
+ static int zs_register_migration(struct zs_pool *pool)
+ {
+ 	pool->inode = alloc_anon_inode(zsmalloc_mnt->mnt_sb);
+ 	if (IS_ERR(pool->inode)) {
+ 		pool->inode = NULL;
+ 		return 1;
+ 	}
+ 
+ 	pool->inode->i_mapping->private_data = pool;
+ 	pool->inode->i_mapping->a_ops = &zsmalloc_aops;
+ 	return 0;
+ }
+ 
+ static void zs_unregister_migration(struct zs_pool *pool)
+ {
+ 	flush_work(&pool->free_work);
+ 	iput(pool->inode);
+ }
+ 
+ /*
+  * Caller should hold page_lock of all pages in the zspage
+  * In here, we cannot use zspage meta data.
+  */
+ static void async_free_zspage(struct work_struct *work)
+ {
+ 	int i;
+ 	struct size_class *class;
+ 	unsigned int class_idx;
+ 	enum fullness_group fullness;
+ 	struct zspage *zspage, *tmp;
+ 	LIST_HEAD(free_pages);
+ 	struct zs_pool *pool = container_of(work, struct zs_pool,
+ 					free_work);
+ 
+ 	for (i = 0; i < ZS_SIZE_CLASSES; i++) {
+ 		class = pool->size_class[i];
+ 		if (class->index != i)
+ 			continue;
+ 
+ 		spin_lock(&class->lock);
+ 		list_splice_init(&class->fullness_list[ZS_EMPTY], &free_pages);
+ 		spin_unlock(&class->lock);
+ 	}
+ 
+ 
+ 	list_for_each_entry_safe(zspage, tmp, &free_pages, list) {
+ 		list_del(&zspage->list);
+ 		lock_zspage(zspage);
+ 
+ 		get_zspage_mapping(zspage, &class_idx, &fullness);
+ 		VM_BUG_ON(fullness != ZS_EMPTY);
+ 		class = pool->size_class[class_idx];
+ 		spin_lock(&class->lock);
+ 		__free_zspage(pool, pool->size_class[class_idx], zspage);
+ 		spin_unlock(&class->lock);
+ 	}
+ };
+ 
+ static void kick_deferred_free(struct zs_pool *pool)
+ {
+ 	schedule_work(&pool->free_work);
+ }
+ 
+ static void init_deferred_free(struct zs_pool *pool)
+ {
+ 	INIT_WORK(&pool->free_work, async_free_zspage);
+ }
+ 
+ static void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage)
+ {
+ 	struct page *page = get_first_page(zspage);
+ 
+ 	do {
+ 		WARN_ON(!trylock_page(page));
+ 		__SetPageMovable(page, pool->inode->i_mapping);
+ 		unlock_page(page);
+ 	} while ((page = get_next_page(page)) != NULL);
+ }
+ #endif
+ 
+ /*
+  *
+  * Based on the number of unused allocated objects calculate
+  * and return the number of pages that we can free.
+  */
+ static unsigned long zs_can_compact(struct size_class *class)
+ {
+ 	unsigned long obj_wasted;
+ 	unsigned long obj_allocated = zs_stat_get(class, OBJ_ALLOCATED);
+ 	unsigned long obj_used = zs_stat_get(class, OBJ_USED);
+ 
+ 	if (obj_allocated <= obj_used)
+ 		return 0;
+ 
+ 	obj_wasted = obj_allocated - obj_used;
+ 	obj_wasted /= class->objs_per_zspage;
+ 
+ 	return obj_wasted * class->pages_per_zspage;
+ }
+ 
+ static void __zs_compact(struct zs_pool *pool, struct size_class *class)
+ {
+ 	struct zs_compact_control cc;
+ 	struct zspage *src_zspage;
+ 	struct zspage *dst_zspage = NULL;
+ 
+ 	spin_lock(&class->lock);
+ 	while ((src_zspage = isolate_zspage(class, true))) {
+ 
+ 		if (!zs_can_compact(class))
+ 			break;
+ 
+ 		cc.obj_idx = 0;
+ 		cc.s_page = get_first_page(src_zspage);
+ 
+ 		while ((dst_zspage = isolate_zspage(class, false))) {
+ 			cc.d_page = get_first_page(dst_zspage);
+ 			/*
+ 			 * If there is no more space in dst_page, resched
+ 			 * and see if anyone had allocated another zspage.
+ 			 */
+ 			if (!migrate_zspage(pool, class, &cc))
+ 				break;
+ 
+ 			putback_zspage(class, dst_zspage);
+ 		}
+ 
+ 		/* Stop if we couldn't find slot */
+ 		if (dst_zspage == NULL)
+ 			break;
+ 
+ 		putback_zspage(class, dst_zspage);
+ 		if (putback_zspage(class, src_zspage) == ZS_EMPTY) {
+ 			free_zspage(pool, class, src_zspage);
+ 			pool->stats.pages_compacted += class->pages_per_zspage;
+ 		}
+ 		spin_unlock(&class->lock);
+ 		cond_resched();
+ 		spin_lock(&class->lock);
+ 	}
+ 
+ 	if (src_zspage)
+ 		putback_zspage(class, src_zspage);
+ 
+ 	spin_unlock(&class->lock);
+ }
+ 
+ unsigned long zs_compact(struct zs_pool *pool)
+ {
+ 	int i;
+ 	struct size_class *class;
+ 
+ 	for (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {
+ 		class = pool->size_class[i];
+ 		if (!class)
+ 			continue;
+ 		if (class->index != i)
+ 			continue;
+ 		__zs_compact(pool, class);
+ 	}
+ 
+ 	return pool->stats.pages_compacted;
+ }
+ EXPORT_SYMBOL_GPL(zs_compact);
+ 
+ void zs_pool_stats(struct zs_pool *pool, struct zs_pool_stats *stats)
+ {
+ 	memcpy(stats, &pool->stats, sizeof(struct zs_pool_stats));
+ }
+ EXPORT_SYMBOL_GPL(zs_pool_stats);
+ 
+ static unsigned long zs_shrinker_scan(struct shrinker *shrinker,
+ 		struct shrink_control *sc)
+ {
+ 	unsigned long pages_freed;
+ 	struct zs_pool *pool = container_of(shrinker, struct zs_pool,
+ 			shrinker);
+ 
+ 	pages_freed = pool->stats.pages_compacted;
+ 	/*
+ 	 * Compact classes and calculate compaction delta.
+ 	 * Can run concurrently with a manually triggered
+ 	 * (by user) compaction.
+ 	 */
+ 	pages_freed = zs_compact(pool) - pages_freed;
+ 
+ 	return pages_freed ? pages_freed : SHRINK_STOP;
+ }
+ 
+ static unsigned long zs_shrinker_count(struct shrinker *shrinker,
+ 		struct shrink_control *sc)
+ {
+ 	int i;
+ 	struct size_class *class;
+ 	unsigned long pages_to_free = 0;
+ 	struct zs_pool *pool = container_of(shrinker, struct zs_pool,
+ 			shrinker);
+ 
+ 	for (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {
+ 		class = pool->size_class[i];
+ 		if (!class)
+ 			continue;
+ 		if (class->index != i)
+ 			continue;
+ 
+ 		pages_to_free += zs_can_compact(class);
+ 	}
+ 
+ 	return pages_to_free;
+ }
+ 
+ static void zs_unregister_shrinker(struct zs_pool *pool)
+ {
+ 	if (pool->shrinker_enabled) {
+ 		unregister_shrinker(&pool->shrinker);
+ 		pool->shrinker_enabled = false;
+ 	}
+ }
+ 
+ static int zs_register_shrinker(struct zs_pool *pool)
+ {
+ 	pool->shrinker.scan_objects = zs_shrinker_scan;
+ 	pool->shrinker.count_objects = zs_shrinker_count;
+ 	pool->shrinker.batch = 0;
+ 	pool->shrinker.seeks = DEFAULT_SEEKS;
+ 
+ 	return register_shrinker(&pool->shrinker);
+ }
+ 
+ /**
+  * zs_create_pool - Creates an allocation pool to work from.
+  * @name: pool name to be created
+  *
+  * This function must be called before anything when using
+  * the zsmalloc allocator.
+  *
+  * On success, a pointer to the newly created pool is returned,
+  * otherwise NULL.
+  */
+ struct zs_pool *zs_create_pool(const char *name)
+ {
+ 	int i;
+ 	struct zs_pool *pool;
+ 	struct size_class *prev_class = NULL;
+ 
+ 	pool = kzalloc(sizeof(*pool), GFP_KERNEL);
+ 	if (!pool)
+ 		return NULL;
+ 
+ 	init_deferred_free(pool);
+ 
+ 	pool->name = kstrdup(name, GFP_KERNEL);
+ 	if (!pool->name)
+ 		goto err;
+ 
+ 	if (create_cache(pool))
+ 		goto err;
+ 
+ 	/*
+ 	 * Iterate reversely, because, size of size_class that we want to use
+ 	 * for merging should be larger or equal to current size.
+ 	 */
+ 	for (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {
+ 		int size;
+ 		int pages_per_zspage;
+ 		int objs_per_zspage;
+ 		struct size_class *class;
+ 		int fullness = 0;
+ 
+ 		size = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;
+ 		if (size > ZS_MAX_ALLOC_SIZE)
+ 			size = ZS_MAX_ALLOC_SIZE;
+ 		pages_per_zspage = get_pages_per_zspage(size);
+ 		objs_per_zspage = pages_per_zspage * PAGE_SIZE / size;
+ 
+ 		/*
+ 		 * size_class is used for normal zsmalloc operation such
+ 		 * as alloc/free for that size. Although it is natural that we
+ 		 * have one size_class for each size, there is a chance that we
+ 		 * can get more memory utilization if we use one size_class for
+ 		 * many different sizes whose size_class have same
+ 		 * characteristics. So, we makes size_class point to
+ 		 * previous size_class if possible.
+ 		 */
+ 		if (prev_class) {
+ 			if (can_merge(prev_class, pages_per_zspage, objs_per_zspage)) {
+ 				pool->size_class[i] = prev_class;
+ 				continue;
+ 			}
+ 		}
+ 
+ 		class = kzalloc(sizeof(struct size_class), GFP_KERNEL);
+ 		if (!class)
+ 			goto err;
+ 
+ 		class->size = size;
+ 		class->index = i;
+ 		class->pages_per_zspage = pages_per_zspage;
+ 		class->objs_per_zspage = objs_per_zspage;
+ 		spin_lock_init(&class->lock);
+ 		pool->size_class[i] = class;
+ 		for (fullness = ZS_EMPTY; fullness < NR_ZS_FULLNESS;
+ 							fullness++)
+ 			INIT_LIST_HEAD(&class->fullness_list[fullness]);
+ 
+ 		prev_class = class;
+ 	}
+ 
+ 	/* debug only, don't abort if it fails */
+ 	zs_pool_stat_create(pool, name);
+ 
+ 	if (zs_register_migration(pool))
+ 		goto err;
+ 
+ 	/*
+ 	 * Not critical, we still can use the pool
+ 	 * and user can trigger compaction manually.
+ 	 */
+ 	if (zs_register_shrinker(pool) == 0)
+ 		pool->shrinker_enabled = true;
+ 	return pool;
+ 
+ err:
+ 	zs_destroy_pool(pool);
+ 	return NULL;
+ }
+ EXPORT_SYMBOL_GPL(zs_create_pool);
+ 
+ void zs_destroy_pool(struct zs_pool *pool)
+ {
+ 	int i;
+ 
+ 	zs_unregister_shrinker(pool);
+ 	zs_unregister_migration(pool);
+ 	zs_pool_stat_destroy(pool);
+ 
+ 	for (i = 0; i < ZS_SIZE_CLASSES; i++) {
+ 		int fg;
+ 		struct size_class *class = pool->size_class[i];
+ 
+ 		if (!class)
+ 			continue;
+ 
+ 		if (class->index != i)
+ 			continue;
+ 
+ 		for (fg = ZS_EMPTY; fg < NR_ZS_FULLNESS; fg++) {
+ 			if (!list_empty(&class->fullness_list[fg])) {
+ 				pr_info("Freeing non-empty class with size %db, fullness group %d\n",
+ 					class->size, fg);
+ 			}
+ 		}
+ 		kfree(class);
+ 	}
+ 
+ 	destroy_cache(pool);
+ 	kfree(pool->size_class);
+ 	kfree(pool->name);
+ 	kfree(pool);
+ }
+ EXPORT_SYMBOL_GPL(zs_destroy_pool);
+ 
+ static int __init zs_init(void)
+ {
+ 	int ret;
+ 
+ 	ret = zsmalloc_mount();
+ 	if (ret)
+ 		goto out;
+ 
+ 	ret = cpuhp_setup_state(CPUHP_MM_ZS_PREPARE, "mm/zsmalloc:prepare",
+ 				zs_cpu_prepare, zs_cpu_dead);
+ 	if (ret)
+ 		goto hp_setup_fail;
+ 
+ #ifdef CONFIG_ZPOOL
+ 	zpool_register_driver(&zs_zpool_driver);
+ #endif
+ 
+ 	zs_stat_init();
+ 
+ 	return 0;
+ 
+ hp_setup_fail:
+ 	zsmalloc_unmount();
+ out:
+ 	return ret;
+ }
+ 
+ static void __exit zs_exit(void)
+ {
+ #ifdef CONFIG_ZPOOL
+ 	zpool_unregister_driver(&zs_zpool_driver);
+ #endif
+ 	zsmalloc_unmount();
+ 	cpuhp_remove_state(CPUHP_MM_ZS_PREPARE);
+ 
+ 	zs_stat_exit();
++>>>>>>> cf8e0fedf078 (mm/zsmalloc: simplify zs_max_alloc_size handling)
  }
 +EXPORT_SYMBOL_GPL(zs_get_total_pages);
  
  module_init(zs_init);
  module_exit(zs_exit);
* Unmerged path mm/zsmalloc.c
