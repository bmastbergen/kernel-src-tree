dax: make cache flushing protected by entry lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jan Kara <jack@suse.cz>
commit a6abc2c0e77b16480f4d2c1eb7925e5287ae1526
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/a6abc2c0.failed

Currently, flushing of caches for DAX mappings was ignoring entry lock.
So far this was ok (modulo a bug that a difference in entry lock could
cause cache flushing to be mistakenly skipped) but in the following
patches we will write-protect PTEs on cache flushing and clear dirty
tags.  For that we will need more exclusion.  So do cache flushing under
an entry lock.  This allows us to remove one lock-unlock pair of
mapping->tree_lock as a bonus.

Link: http://lkml.kernel.org/r/1479460644-25076-19-git-send-email-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a6abc2c0e77b16480f4d2c1eb7925e5287ae1526)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 1dfecdfb6245,df5c0daba698..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -683,35 -618,59 +683,74 @@@ static int dax_writeback_one(struct blo
  		struct address_space *mapping, pgoff_t index, void *entry)
  {
  	struct radix_tree_root *page_tree = &mapping->page_tree;
++<<<<<<< HEAD
 +	int type = RADIX_DAX_TYPE(entry);
 +	struct radix_tree_node *node;
++=======
++>>>>>>> a6abc2c0e77b (dax: make cache flushing protected by entry lock)
  	struct blk_dax_ctl dax;
- 	void **slot;
+ 	void *entry2, **slot;
  	int ret = 0;
  
- 	spin_lock_irq(&mapping->tree_lock);
  	/*
- 	 * Regular page slots are stabilized by the page lock even
- 	 * without the tree itself locked.  These unlocked entries
- 	 * need verification under the tree lock.
+ 	 * A page got tagged dirty in DAX mapping? Something is seriously
+ 	 * wrong.
  	 */
- 	if (!__radix_tree_lookup(page_tree, index, &node, &slot))
- 		goto unlock;
- 	if (*slot != entry)
- 		goto unlock;
- 
- 	/* another fsync thread may have already written back this entry */
- 	if (!radix_tree_tag_get(page_tree, index, PAGECACHE_TAG_TOWRITE))
- 		goto unlock;
+ 	if (WARN_ON(!radix_tree_exceptional_entry(entry)))
+ 		return -EIO;
  
++<<<<<<< HEAD
 +	if (WARN_ON_ONCE(type != RADIX_DAX_PTE && type != RADIX_DAX_PMD)) {
++=======
+ 	spin_lock_irq(&mapping->tree_lock);
+ 	entry2 = get_unlocked_mapping_entry(mapping, index, &slot);
+ 	/* Entry got punched out / reallocated? */
+ 	if (!entry2 || !radix_tree_exceptional_entry(entry2))
+ 		goto put_unlocked;
+ 	/*
+ 	 * Entry got reallocated elsewhere? No need to writeback. We have to
+ 	 * compare sectors as we must not bail out due to difference in lockbit
+ 	 * or entry type.
+ 	 */
+ 	if (dax_radix_sector(entry2) != dax_radix_sector(entry))
+ 		goto put_unlocked;
+ 	if (WARN_ON_ONCE(dax_is_empty_entry(entry) ||
+ 				dax_is_zero_entry(entry))) {
++>>>>>>> a6abc2c0e77b (dax: make cache flushing protected by entry lock)
  		ret = -EIO;
- 		goto unlock;
+ 		goto put_unlocked;
  	}
  
++<<<<<<< HEAD
 +	dax.sector = RADIX_DAX_SECTOR(entry);
 +	dax.size = (type == RADIX_DAX_PMD ? PMD_SIZE : PAGE_SIZE);
 +	spin_unlock_irq(&mapping->tree_lock);
++=======
+ 	/* Another fsync thread may have already written back this entry */
+ 	if (!radix_tree_tag_get(page_tree, index, PAGECACHE_TAG_TOWRITE))
+ 		goto put_unlocked;
+ 	/* Lock the entry to serialize with page faults */
+ 	entry = lock_slot(mapping, slot);
+ 	/*
+ 	 * We can clear the tag now but we have to be careful so that concurrent
+ 	 * dax_writeback_one() calls for the same index cannot finish before we
+ 	 * actually flush the caches. This is achieved as the calls will look
+ 	 * at the entry only under tree_lock and once they do that they will
+ 	 * see the entry locked and wait for it to unlock.
+ 	 */
+ 	radix_tree_tag_clear(page_tree, index, PAGECACHE_TAG_TOWRITE);
+ 	spin_unlock_irq(&mapping->tree_lock);
+ 
+ 	/*
+ 	 * Even if dax_writeback_mapping_range() was given a wbc->range_start
+ 	 * in the middle of a PMD, the 'index' we are given will be aligned to
+ 	 * the start index of the PMD, as will the sector we pull from
+ 	 * 'entry'.  This allows us to flush for PMD_SIZE and not have to
+ 	 * worry about partial PMD writebacks.
+ 	 */
+ 	dax.sector = dax_radix_sector(entry);
+ 	dax.size = PAGE_SIZE << dax_radix_order(entry);
++>>>>>>> a6abc2c0e77b (dax: make cache flushing protected by entry lock)
  
  	/*
  	 * We cannot hold tree_lock while calling dax_map_atomic() because it
* Unmerged path fs/dax.c
