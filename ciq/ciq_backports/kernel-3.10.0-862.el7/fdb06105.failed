ibmvnic: Enable TSO support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Thomas Falcon <tlfalcon@linux.vnet.ibm.com>
commit fdb061056f57e849a05cac072a4998c7f33d797e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/fdb06105.failed

This patch enables TSO support. It includes additional
buffers reserved exclusively for large packets. Throughput
is greatly increased with TSO enabled, from about 1 Gb/s to
9 Gb/s on our test systems.

	Signed-off-by: Thomas Falcon <tlfalcon@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit fdb061056f57e849a05cac072a4998c7f33d797e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/ibm/ibmvnic.c
diff --cc drivers/net/ethernet/ibm/ibmvnic.c
index 372983d733ad,aedb81c230a6..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.c
+++ b/drivers/net/ethernet/ibm/ibmvnic.c
@@@ -369,8 -347,335 +369,340 @@@ static void replenish_pools(struct ibmv
  	}
  }
  
++<<<<<<< HEAD
 +static void free_rx_pool(struct ibmvnic_adapter *adapter,
 +			 struct ibmvnic_rx_pool *pool)
++=======
+ static void release_stats_buffers(struct ibmvnic_adapter *adapter)
+ {
+ 	kfree(adapter->tx_stats_buffers);
+ 	kfree(adapter->rx_stats_buffers);
+ }
+ 
+ static int init_stats_buffers(struct ibmvnic_adapter *adapter)
+ {
+ 	adapter->tx_stats_buffers =
+ 				kcalloc(adapter->req_tx_queues,
+ 					sizeof(struct ibmvnic_tx_queue_stats),
+ 					GFP_KERNEL);
+ 	if (!adapter->tx_stats_buffers)
+ 		return -ENOMEM;
+ 
+ 	adapter->rx_stats_buffers =
+ 				kcalloc(adapter->req_rx_queues,
+ 					sizeof(struct ibmvnic_rx_queue_stats),
+ 					GFP_KERNEL);
+ 	if (!adapter->rx_stats_buffers)
+ 		return -ENOMEM;
+ 
+ 	return 0;
+ }
+ 
+ static void release_stats_token(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 
+ 	if (!adapter->stats_token)
+ 		return;
+ 
+ 	dma_unmap_single(dev, adapter->stats_token,
+ 			 sizeof(struct ibmvnic_statistics),
+ 			 DMA_FROM_DEVICE);
+ 	adapter->stats_token = 0;
+ }
+ 
+ static int init_stats_token(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 	dma_addr_t stok;
+ 
+ 	stok = dma_map_single(dev, &adapter->stats,
+ 			      sizeof(struct ibmvnic_statistics),
+ 			      DMA_FROM_DEVICE);
+ 	if (dma_mapping_error(dev, stok)) {
+ 		dev_err(dev, "Couldn't map stats buffer\n");
+ 		return -1;
+ 	}
+ 
+ 	adapter->stats_token = stok;
+ 	netdev_dbg(adapter->netdev, "Stats token initialized (%llx)\n", stok);
+ 	return 0;
+ }
+ 
+ static int reset_rx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_rx_pool *rx_pool;
+ 	int rx_scrqs;
+ 	int i, j, rc;
+ 
+ 	rx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	for (i = 0; i < rx_scrqs; i++) {
+ 		rx_pool = &adapter->rx_pool[i];
+ 
+ 		netdev_dbg(adapter->netdev, "Re-setting rx_pool[%d]\n", i);
+ 
+ 		rc = reset_long_term_buff(adapter, &rx_pool->long_term_buff);
+ 		if (rc)
+ 			return rc;
+ 
+ 		for (j = 0; j < rx_pool->size; j++)
+ 			rx_pool->free_map[j] = j;
+ 
+ 		memset(rx_pool->rx_buff, 0,
+ 		       rx_pool->size * sizeof(struct ibmvnic_rx_buff));
+ 
+ 		atomic_set(&rx_pool->available, 0);
+ 		rx_pool->next_alloc = 0;
+ 		rx_pool->next_free = 0;
+ 		rx_pool->active = 1;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void release_rx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_rx_pool *rx_pool;
+ 	int rx_scrqs;
+ 	int i, j;
+ 
+ 	if (!adapter->rx_pool)
+ 		return;
+ 
+ 	rx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	for (i = 0; i < rx_scrqs; i++) {
+ 		rx_pool = &adapter->rx_pool[i];
+ 
+ 		netdev_dbg(adapter->netdev, "Releasing rx_pool[%d]\n", i);
+ 
+ 		kfree(rx_pool->free_map);
+ 		free_long_term_buff(adapter, &rx_pool->long_term_buff);
+ 
+ 		if (!rx_pool->rx_buff)
+ 			continue;
+ 
+ 		for (j = 0; j < rx_pool->size; j++) {
+ 			if (rx_pool->rx_buff[j].skb) {
+ 				dev_kfree_skb_any(rx_pool->rx_buff[i].skb);
+ 				rx_pool->rx_buff[i].skb = NULL;
+ 			}
+ 		}
+ 
+ 		kfree(rx_pool->rx_buff);
+ 	}
+ 
+ 	kfree(adapter->rx_pool);
+ 	adapter->rx_pool = NULL;
+ }
+ 
+ static int init_rx_pools(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	struct device *dev = &adapter->vdev->dev;
+ 	struct ibmvnic_rx_pool *rx_pool;
+ 	int rxadd_subcrqs;
+ 	u64 *size_array;
+ 	int i, j;
+ 
+ 	rxadd_subcrqs =
+ 		be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	size_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +
+ 		be32_to_cpu(adapter->login_rsp_buf->off_rxadd_buff_size));
+ 
+ 	adapter->rx_pool = kcalloc(rxadd_subcrqs,
+ 				   sizeof(struct ibmvnic_rx_pool),
+ 				   GFP_KERNEL);
+ 	if (!adapter->rx_pool) {
+ 		dev_err(dev, "Failed to allocate rx pools\n");
+ 		return -1;
+ 	}
+ 
+ 	for (i = 0; i < rxadd_subcrqs; i++) {
+ 		rx_pool = &adapter->rx_pool[i];
+ 
+ 		netdev_dbg(adapter->netdev,
+ 			   "Initializing rx_pool[%d], %lld buffs, %lld bytes each\n",
+ 			   i, adapter->req_rx_add_entries_per_subcrq,
+ 			   be64_to_cpu(size_array[i]));
+ 
+ 		rx_pool->size = adapter->req_rx_add_entries_per_subcrq;
+ 		rx_pool->index = i;
+ 		rx_pool->buff_size = be64_to_cpu(size_array[i]);
+ 		rx_pool->active = 1;
+ 
+ 		rx_pool->free_map = kcalloc(rx_pool->size, sizeof(int),
+ 					    GFP_KERNEL);
+ 		if (!rx_pool->free_map) {
+ 			release_rx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		rx_pool->rx_buff = kcalloc(rx_pool->size,
+ 					   sizeof(struct ibmvnic_rx_buff),
+ 					   GFP_KERNEL);
+ 		if (!rx_pool->rx_buff) {
+ 			dev_err(dev, "Couldn't alloc rx buffers\n");
+ 			release_rx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		if (alloc_long_term_buff(adapter, &rx_pool->long_term_buff,
+ 					 rx_pool->size * rx_pool->buff_size)) {
+ 			release_rx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		for (j = 0; j < rx_pool->size; ++j)
+ 			rx_pool->free_map[j] = j;
+ 
+ 		atomic_set(&rx_pool->available, 0);
+ 		rx_pool->next_alloc = 0;
+ 		rx_pool->next_free = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int reset_tx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_tx_pool *tx_pool;
+ 	int tx_scrqs;
+ 	int i, j, rc;
+ 
+ 	tx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	for (i = 0; i < tx_scrqs; i++) {
+ 		netdev_dbg(adapter->netdev, "Re-setting tx_pool[%d]\n", i);
+ 
+ 		tx_pool = &adapter->tx_pool[i];
+ 
+ 		rc = reset_long_term_buff(adapter, &tx_pool->long_term_buff);
+ 		if (rc)
+ 			return rc;
+ 
+ 		rc = reset_long_term_buff(adapter, &tx_pool->tso_ltb);
+ 		if (rc)
+ 			return rc;
+ 
+ 		memset(tx_pool->tx_buff, 0,
+ 		       adapter->req_tx_entries_per_subcrq *
+ 		       sizeof(struct ibmvnic_tx_buff));
+ 
+ 		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
+ 			tx_pool->free_map[j] = j;
+ 
+ 		tx_pool->consumer_index = 0;
+ 		tx_pool->producer_index = 0;
+ 		tx_pool->tso_index = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void release_tx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_tx_pool *tx_pool;
+ 	int i, tx_scrqs;
+ 
+ 	if (!adapter->tx_pool)
+ 		return;
+ 
+ 	tx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	for (i = 0; i < tx_scrqs; i++) {
+ 		netdev_dbg(adapter->netdev, "Releasing tx_pool[%d]\n", i);
+ 		tx_pool = &adapter->tx_pool[i];
+ 		kfree(tx_pool->tx_buff);
+ 		free_long_term_buff(adapter, &tx_pool->long_term_buff);
+ 		free_long_term_buff(adapter, &tx_pool->tso_ltb);
+ 		kfree(tx_pool->free_map);
+ 	}
+ 
+ 	kfree(adapter->tx_pool);
+ 	adapter->tx_pool = NULL;
+ }
+ 
+ static int init_tx_pools(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	struct device *dev = &adapter->vdev->dev;
+ 	struct ibmvnic_tx_pool *tx_pool;
+ 	int tx_subcrqs;
+ 	int i, j;
+ 
+ 	tx_subcrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	adapter->tx_pool = kcalloc(tx_subcrqs,
+ 				   sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);
+ 	if (!adapter->tx_pool)
+ 		return -1;
+ 
+ 	for (i = 0; i < tx_subcrqs; i++) {
+ 		tx_pool = &adapter->tx_pool[i];
+ 
+ 		netdev_dbg(adapter->netdev,
+ 			   "Initializing tx_pool[%d], %lld buffs\n",
+ 			   i, adapter->req_tx_entries_per_subcrq);
+ 
+ 		tx_pool->tx_buff = kcalloc(adapter->req_tx_entries_per_subcrq,
+ 					   sizeof(struct ibmvnic_tx_buff),
+ 					   GFP_KERNEL);
+ 		if (!tx_pool->tx_buff) {
+ 			dev_err(dev, "tx pool buffer allocation failed\n");
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		if (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,
+ 					 adapter->req_tx_entries_per_subcrq *
+ 					 adapter->req_mtu)) {
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		/* alloc TSO ltb */
+ 		if (alloc_long_term_buff(adapter, &tx_pool->tso_ltb,
+ 					 IBMVNIC_TSO_BUFS *
+ 					 IBMVNIC_TSO_BUF_SZ)) {
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		tx_pool->tso_index = 0;
+ 
+ 		tx_pool->free_map = kcalloc(adapter->req_tx_entries_per_subcrq,
+ 					    sizeof(int), GFP_KERNEL);
+ 		if (!tx_pool->free_map) {
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
+ 			tx_pool->free_map[j] = j;
+ 
+ 		tx_pool->consumer_index = 0;
+ 		tx_pool->producer_index = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void release_error_buffers(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 	struct ibmvnic_error_buff *error_buff, *tmp;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&adapter->error_list_lock, flags);
+ 	list_for_each_entry_safe(error_buff, tmp, &adapter->errors, list) {
+ 		list_del(&error_buff->list);
+ 		dma_unmap_single(dev, error_buff->dma, error_buff->len,
+ 				 DMA_FROM_DEVICE);
+ 		kfree(error_buff->buff);
+ 		kfree(error_buff);
+ 	}
+ 	spin_unlock_irqrestore(&adapter->error_list_lock, flags);
+ }
+ 
+ static void ibmvnic_napi_enable(struct ibmvnic_adapter *adapter)
++>>>>>>> fdb061056f57 (ibmvnic: Enable TSO support)
  {
  	int i;
  
@@@ -783,11 -1210,28 +1115,22 @@@ static int ibmvnic_xmit(struct sk_buff 
  		goto out;
  	}
  
 -	tx_pool = &adapter->tx_pool[queue_num];
 -	tx_scrq = adapter->tx_scrq[queue_num];
 -	txq = netdev_get_tx_queue(netdev, skb_get_queue_mapping(skb));
 -	handle_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +
 -		be32_to_cpu(adapter->login_rsp_buf->off_txsubm_subcrqs));
 -
  	index = tx_pool->free_map[tx_pool->consumer_index];
- 	offset = index * adapter->req_mtu;
- 	dst = tx_pool->long_term_buff.buff + offset;
- 	memset(dst, 0, adapter->req_mtu);
- 	data_dma_addr = tx_pool->long_term_buff.addr + offset;
+ 
+ 	if (skb_is_gso(skb)) {
+ 		offset = tx_pool->tso_index * IBMVNIC_TSO_BUF_SZ;
+ 		dst = tx_pool->tso_ltb.buff + offset;
+ 		memset(dst, 0, IBMVNIC_TSO_BUF_SZ);
+ 		data_dma_addr = tx_pool->tso_ltb.addr + offset;
+ 		tx_pool->tso_index++;
+ 		if (tx_pool->tso_index == IBMVNIC_TSO_BUFS)
+ 			tx_pool->tso_index = 0;
+ 	} else {
+ 		offset = index * adapter->req_mtu;
+ 		dst = tx_pool->long_term_buff.buff + offset;
+ 		memset(dst, 0, adapter->req_mtu);
+ 		data_dma_addr = tx_pool->long_term_buff.addr + offset;
+ 	}
  
  	if (skb_shinfo(skb)->nr_frags) {
  		int cur, i;
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.c
diff --git a/drivers/net/ethernet/ibm/ibmvnic.h b/drivers/net/ethernet/ibm/ibmvnic.h
index 082a339df814..f758320e6a28 100644
--- a/drivers/net/ethernet/ibm/ibmvnic.h
+++ b/drivers/net/ethernet/ibm/ibmvnic.h
@@ -39,6 +39,9 @@
 #define IBMVNIC_BUFFS_PER_POOL	100
 #define IBMVNIC_MAX_TX_QUEUES	5
 
+#define IBMVNIC_TSO_BUF_SZ	65536
+#define IBMVNIC_TSO_BUFS	64
+
 struct ibmvnic_login_buffer {
 	__be32 len;
 	__be32 version;
@@ -895,6 +898,8 @@ struct ibmvnic_tx_pool {
 	wait_queue_head_t ibmvnic_tx_comp_q;
 	struct task_struct *work_thread;
 	struct ibmvnic_long_term_buff long_term_buff;
+	struct ibmvnic_long_term_buff tso_ltb;
+	int tso_index;
 };
 
 struct ibmvnic_rx_buff {
