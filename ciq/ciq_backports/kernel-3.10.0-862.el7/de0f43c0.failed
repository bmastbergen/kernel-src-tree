mlxsw: spectrum_router: Track RIF of IPIP next hops

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Petr Machata <petrm@mellanox.com>
commit de0f43c01a4b5d408a5c087c8a92ac1739938f8b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/de0f43c0.failed

When considering whether to set RTNH_F_OFFLOAD flag on an IPv6 route,
mlxsw_sp_fib6_entry_offload_set() looks up the mlxsw_sp_nexthop
corresponding to a given route, and decides based on whether the next
hop's offloaded flag was set. When looking for the matching next hop, it
also takes into account the device of the route, which must match next
hop's RIF.

IPIP next hops however hitherto didn't set the RIF. As a result, IPv6
routes forwarding traffic to IP-in-IP netdevices are never marked as
offloaded, even when they actually are.

Thus track RIF of IPIP next hops the same way as that of ETHERNET next
hops.

Fixes: 8f28a3097645 ("mlxsw: spectrum_router: Support IPv6 overlay encap")
	Signed-off-by: Petr Machata <petrm@mellanox.com>
	Reviewed-by: Ido Schimmel <idosch@mellanox.com>
	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit de0f43c01a4b5d408a5c087c8a92ac1739938f8b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
index 1b11e7dc65c3,032089efc1a0..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
@@@ -1515,199 -1845,327 +1515,324 @@@ mlxsw_sp_nexthop_neigh_update(struct ml
  	}
  }
  
 -void
 -mlxsw_sp_neigh_entry_counter_update(struct mlxsw_sp *mlxsw_sp,
 -				    struct mlxsw_sp_neigh_entry *neigh_entry,
 -				    bool adding)
 +static void mlxsw_sp_nexthop_rif_init(struct mlxsw_sp_nexthop *nh,
 +				      struct mlxsw_sp_rif *r)
  {
 -	if (adding)
 -		mlxsw_sp_neigh_counter_alloc(mlxsw_sp, neigh_entry);
 -	else
 -		mlxsw_sp_neigh_counter_free(mlxsw_sp, neigh_entry);
 -	mlxsw_sp_neigh_entry_update(mlxsw_sp, neigh_entry, true);
 +	if (nh->r)
 +		return;
 +
 +	nh->r = r;
 +	list_add(&nh->rif_list_node, &r->nexthop_list);
  }
  
 -struct mlxsw_sp_neigh_event_work {
 -	struct work_struct work;
 -	struct mlxsw_sp *mlxsw_sp;
 -	struct neighbour *n;
 -};
 +static void mlxsw_sp_nexthop_rif_fini(struct mlxsw_sp_nexthop *nh)
 +{
 +	if (!nh->r)
 +		return;
  
 -static void mlxsw_sp_router_neigh_event_work(struct work_struct *work)
 +	list_del(&nh->rif_list_node);
 +	nh->r = NULL;
 +}
 +
 +static int mlxsw_sp_nexthop_neigh_init(struct mlxsw_sp *mlxsw_sp,
 +				       struct mlxsw_sp_nexthop *nh)
  {
 -	struct mlxsw_sp_neigh_event_work *neigh_work =
 -		container_of(work, struct mlxsw_sp_neigh_event_work, work);
 -	struct mlxsw_sp *mlxsw_sp = neigh_work->mlxsw_sp;
  	struct mlxsw_sp_neigh_entry *neigh_entry;
 -	struct neighbour *n = neigh_work->n;
 -	unsigned char ha[ETH_ALEN];
 -	bool entry_connected;
 +	struct fib_nh *fib_nh = nh->key.fib_nh;
 +	struct neighbour *n;
  	u8 nud_state, dead;
 +	int err;
  
 -	/* If these parameters are changed after we release the lock,
 -	 * then we are guaranteed to receive another event letting us
 -	 * know about it.
 -	 */
 -	read_lock_bh(&n->lock);
 -	memcpy(ha, n->ha, ETH_ALEN);
 -	nud_state = n->nud_state;
 -	dead = n->dead;
 -	read_unlock_bh(&n->lock);
 +	if (!nh->nh_grp->gateway || nh->neigh_entry)
 +		return 0;
  
 -	rtnl_lock();
 -	entry_connected = nud_state & NUD_VALID && !dead;
 +	/* Take a reference of neigh here ensuring that neigh would
 +	 * not be destructed before the nexthop entry is finished.
 +	 * The reference is taken either in neigh_lookup() or
 +	 * in neigh_create() in case n is not found.
 +	 */
 +	n = neigh_lookup(&arp_tbl, &fib_nh->nh_gw, fib_nh->nh_dev);
 +	if (!n) {
 +		n = neigh_create(&arp_tbl, &fib_nh->nh_gw, fib_nh->nh_dev);
 +		if (IS_ERR(n))
 +			return PTR_ERR(n);
 +		neigh_event_send(n, NULL);
 +	}
  	neigh_entry = mlxsw_sp_neigh_entry_lookup(mlxsw_sp, n);
 -	if (!entry_connected && !neigh_entry)
 -		goto out;
  	if (!neigh_entry) {
  		neigh_entry = mlxsw_sp_neigh_entry_create(mlxsw_sp, n);
 -		if (IS_ERR(neigh_entry))
 -			goto out;
 +		if (IS_ERR(neigh_entry)) {
 +			err = -EINVAL;
 +			goto err_neigh_entry_create;
 +		}
  	}
  
 -	memcpy(neigh_entry->ha, ha, ETH_ALEN);
 -	mlxsw_sp_neigh_entry_update(mlxsw_sp, neigh_entry, entry_connected);
 -	mlxsw_sp_nexthop_neigh_update(mlxsw_sp, neigh_entry, !entry_connected);
 +	/* If that is the first nexthop connected to that neigh, add to
 +	 * nexthop_neighs_list
 +	 */
 +	if (list_empty(&neigh_entry->nexthop_list))
 +		list_add_tail(&neigh_entry->nexthop_neighs_list_node,
 +			      &mlxsw_sp->router.nexthop_neighs_list);
  
 -	if (!neigh_entry->connected && list_empty(&neigh_entry->nexthop_list))
 -		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
 +	nh->neigh_entry = neigh_entry;
 +	list_add_tail(&nh->neigh_list_node, &neigh_entry->nexthop_list);
 +	read_lock_bh(&n->lock);
 +	nud_state = n->nud_state;
 +	dead = n->dead;
 +	read_unlock_bh(&n->lock);
 +	__mlxsw_sp_nexthop_neigh_update(nh, !(nud_state & NUD_VALID && !dead));
  
 -out:
 -	rtnl_unlock();
 +	return 0;
 +
 +err_neigh_entry_create:
  	neigh_release(n);
 -	kfree(neigh_work);
 +	return err;
  }
  
 -int mlxsw_sp_router_netevent_event(struct notifier_block *unused,
 -				   unsigned long event, void *ptr)
 +static void mlxsw_sp_nexthop_neigh_fini(struct mlxsw_sp *mlxsw_sp,
 +					struct mlxsw_sp_nexthop *nh)
  {
 -	struct mlxsw_sp_neigh_event_work *neigh_work;
 -	struct mlxsw_sp_port *mlxsw_sp_port;
 -	struct mlxsw_sp *mlxsw_sp;
 -	unsigned long interval;
 -	struct neigh_parms *p;
 +	struct mlxsw_sp_neigh_entry *neigh_entry = nh->neigh_entry;
  	struct neighbour *n;
  
 -	switch (event) {
 -	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
 -		p = ptr;
 -
 -		/* We don't care about changes in the default table. */
 -		if (!p->dev || (p->tbl->family != AF_INET &&
 -				p->tbl->family != AF_INET6))
 -			return NOTIFY_DONE;
 +	if (!neigh_entry)
 +		return;
 +	n = neigh_entry->key.n;
  
 -		/* We are in atomic context and can't take RTNL mutex,
 -		 * so use RCU variant to walk the device chain.
 -		 */
 -		mlxsw_sp_port = mlxsw_sp_port_lower_dev_hold(p->dev);
 -		if (!mlxsw_sp_port)
 -			return NOTIFY_DONE;
 +	__mlxsw_sp_nexthop_neigh_update(nh, true);
 +	list_del(&nh->neigh_list_node);
 +	nh->neigh_entry = NULL;
  
 -		mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 -		interval = jiffies_to_msecs(NEIGH_VAR(p, DELAY_PROBE_TIME));
 -		mlxsw_sp->router->neighs_update.interval = interval;
 +	/* If that is the last nexthop connected to that neigh, remove from
 +	 * nexthop_neighs_list
 +	 */
 +	if (list_empty(&neigh_entry->nexthop_list))
 +		list_del(&neigh_entry->nexthop_neighs_list_node);
  
 -		mlxsw_sp_port_dev_put(mlxsw_sp_port);
 -		break;
 -	case NETEVENT_NEIGH_UPDATE:
 -		n = ptr;
 +	if (!neigh_entry->connected && list_empty(&neigh_entry->nexthop_list))
 +		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
  
 -		if (n->tbl->family != AF_INET && n->tbl->family != AF_INET6)
 -			return NOTIFY_DONE;
 +	neigh_release(n);
 +}
  
 -		mlxsw_sp_port = mlxsw_sp_port_lower_dev_hold(n->dev);
 -		if (!mlxsw_sp_port)
 -			return NOTIFY_DONE;
++<<<<<<< HEAD
 +static int mlxsw_sp_nexthop_init(struct mlxsw_sp *mlxsw_sp,
 +				 struct mlxsw_sp_nexthop_group *nh_grp,
 +				 struct mlxsw_sp_nexthop *nh,
 +				 struct fib_nh *fib_nh)
++=======
++static bool mlxsw_sp_netdev_ipip_type(const struct mlxsw_sp *mlxsw_sp,
++				      const struct net_device *dev,
++				      enum mlxsw_sp_ipip_type *p_type)
++{
++	struct mlxsw_sp_router *router = mlxsw_sp->router;
++	const struct mlxsw_sp_ipip_ops *ipip_ops;
++	enum mlxsw_sp_ipip_type ipipt;
+ 
 -		neigh_work = kzalloc(sizeof(*neigh_work), GFP_ATOMIC);
 -		if (!neigh_work) {
 -			mlxsw_sp_port_dev_put(mlxsw_sp_port);
 -			return NOTIFY_BAD;
++	for (ipipt = 0; ipipt < MLXSW_SP_IPIP_TYPE_MAX; ++ipipt) {
++		ipip_ops = router->ipip_ops_arr[ipipt];
++		if (dev->type == ipip_ops->dev_type) {
++			if (p_type)
++				*p_type = ipipt;
++			return true;
+ 		}
++	}
++	return false;
++}
+ 
 -		INIT_WORK(&neigh_work->work, mlxsw_sp_router_neigh_event_work);
 -		neigh_work->mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 -		neigh_work->n = n;
++static int mlxsw_sp_nexthop_ipip_init(struct mlxsw_sp *mlxsw_sp,
++				      enum mlxsw_sp_ipip_type ipipt,
++				      struct mlxsw_sp_nexthop *nh,
++				      struct net_device *ol_dev)
++{
++	if (!nh->nh_grp->gateway || nh->ipip_entry)
++		return 0;
+ 
 -		/* Take a reference to ensure the neighbour won't be
 -		 * destructed until we drop the reference in delayed
 -		 * work.
 -		 */
 -		neigh_clone(n);
 -		mlxsw_core_schedule_work(&neigh_work->work);
 -		mlxsw_sp_port_dev_put(mlxsw_sp_port);
 -		break;
 -	}
++	nh->ipip_entry = mlxsw_sp_ipip_entry_get(mlxsw_sp, ipipt, ol_dev);
++	if (IS_ERR(nh->ipip_entry))
++		return PTR_ERR(nh->ipip_entry);
+ 
 -	return NOTIFY_DONE;
++	__mlxsw_sp_nexthop_neigh_update(nh, false);
++	return 0;
+ }
+ 
 -static int mlxsw_sp_neigh_init(struct mlxsw_sp *mlxsw_sp)
++static void mlxsw_sp_nexthop_ipip_fini(struct mlxsw_sp *mlxsw_sp,
++				       struct mlxsw_sp_nexthop *nh)
+ {
 -	int err;
++	struct mlxsw_sp_ipip_entry *ipip_entry = nh->ipip_entry;
+ 
 -	err = rhashtable_init(&mlxsw_sp->router->neigh_ht,
 -			      &mlxsw_sp_neigh_ht_params);
 -	if (err)
 -		return err;
++	if (!ipip_entry)
++		return;
+ 
 -	/* Initialize the polling interval according to the default
 -	 * table.
 -	 */
 -	mlxsw_sp_router_neighs_update_interval_init(mlxsw_sp);
++	__mlxsw_sp_nexthop_neigh_update(nh, true);
++	mlxsw_sp_ipip_entry_put(mlxsw_sp, ipip_entry);
++	nh->ipip_entry = NULL;
++}
+ 
 -	/* Create the delayed works for the activity_update */
 -	INIT_DELAYED_WORK(&mlxsw_sp->router->neighs_update.dw,
 -			  mlxsw_sp_router_neighs_update_work);
 -	INIT_DELAYED_WORK(&mlxsw_sp->router->nexthop_probe_dw,
 -			  mlxsw_sp_router_probe_unresolved_nexthops);
 -	mlxsw_core_schedule_dw(&mlxsw_sp->router->neighs_update.dw, 0);
 -	mlxsw_core_schedule_dw(&mlxsw_sp->router->nexthop_probe_dw, 0);
 -	return 0;
++static bool mlxsw_sp_nexthop4_ipip_type(const struct mlxsw_sp *mlxsw_sp,
++					const struct fib_nh *fib_nh,
++					enum mlxsw_sp_ipip_type *p_ipipt)
++{
++	struct net_device *dev = fib_nh->nh_dev;
++
++	return dev &&
++	       fib_nh->nh_parent->fib_type == RTN_UNICAST &&
++	       mlxsw_sp_netdev_ipip_type(mlxsw_sp, dev, p_ipipt);
+ }
+ 
 -static void mlxsw_sp_neigh_fini(struct mlxsw_sp *mlxsw_sp)
++static void mlxsw_sp_nexthop_type_fini(struct mlxsw_sp *mlxsw_sp,
++				       struct mlxsw_sp_nexthop *nh)
+ {
 -	cancel_delayed_work_sync(&mlxsw_sp->router->neighs_update.dw);
 -	cancel_delayed_work_sync(&mlxsw_sp->router->nexthop_probe_dw);
 -	rhashtable_destroy(&mlxsw_sp->router->neigh_ht);
++	switch (nh->type) {
++	case MLXSW_SP_NEXTHOP_TYPE_ETH:
++		mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
++		mlxsw_sp_nexthop_rif_fini(nh);
++		break;
++	case MLXSW_SP_NEXTHOP_TYPE_IPIP:
++		mlxsw_sp_nexthop_rif_fini(nh);
++		mlxsw_sp_nexthop_ipip_fini(mlxsw_sp, nh);
++		break;
++	}
+ }
+ 
 -static void mlxsw_sp_neigh_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 -					 struct mlxsw_sp_rif *rif)
++static int mlxsw_sp_nexthop4_type_init(struct mlxsw_sp *mlxsw_sp,
++				       struct mlxsw_sp_nexthop *nh,
++				       struct fib_nh *fib_nh)
+ {
 -	struct mlxsw_sp_neigh_entry *neigh_entry, *tmp;
++	struct mlxsw_sp_router *router = mlxsw_sp->router;
++	struct net_device *dev = fib_nh->nh_dev;
++	enum mlxsw_sp_ipip_type ipipt;
++	struct mlxsw_sp_rif *rif;
++	int err;
+ 
 -	list_for_each_entry_safe(neigh_entry, tmp, &rif->neigh_list,
 -				 rif_list_node) {
 -		mlxsw_sp_neigh_entry_update(mlxsw_sp, neigh_entry, false);
 -		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
++	if (mlxsw_sp_nexthop4_ipip_type(mlxsw_sp, fib_nh, &ipipt) &&
++	    router->ipip_ops_arr[ipipt]->can_offload(mlxsw_sp, dev,
++						     MLXSW_SP_L3_PROTO_IPV4)) {
++		nh->type = MLXSW_SP_NEXTHOP_TYPE_IPIP;
++		err = mlxsw_sp_nexthop_ipip_init(mlxsw_sp, ipipt, nh, dev);
++		if (err)
++			return err;
++		mlxsw_sp_nexthop_rif_init(nh, &nh->ipip_entry->ol_lb->common);
++		return 0;
+ 	}
 -}
+ 
 -enum mlxsw_sp_nexthop_type {
 -	MLXSW_SP_NEXTHOP_TYPE_ETH,
 -	MLXSW_SP_NEXTHOP_TYPE_IPIP,
 -};
++	nh->type = MLXSW_SP_NEXTHOP_TYPE_ETH;
++	rif = mlxsw_sp_rif_find_by_dev(mlxsw_sp, dev);
++	if (!rif)
++		return 0;
+ 
 -struct mlxsw_sp_nexthop_key {
 -	struct fib_nh *fib_nh;
 -};
++	mlxsw_sp_nexthop_rif_init(nh, rif);
++	err = mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
++	if (err)
++		goto err_neigh_init;
+ 
 -struct mlxsw_sp_nexthop {
 -	struct list_head neigh_list_node; /* member of neigh entry list */
 -	struct list_head rif_list_node;
 -	struct mlxsw_sp_nexthop_group *nh_grp; /* pointer back to the group
 -						* this belongs to
 -						*/
 -	struct rhash_head ht_node;
 -	struct mlxsw_sp_nexthop_key key;
 -	unsigned char gw_addr[sizeof(struct in6_addr)];
 -	int ifindex;
 -	struct mlxsw_sp_rif *rif;
 -	u8 should_offload:1, /* set indicates this neigh is connected and
 -			      * should be put to KVD linear area of this group.
 -			      */
 -	   offloaded:1, /* set in case the neigh is actually put into
 -			 * KVD linear area of this group.
 -			 */
 -	   update:1; /* set indicates that MAC of this neigh should be
 -		      * updated in HW
 -		      */
 -	enum mlxsw_sp_nexthop_type type;
 -	union {
 -		struct mlxsw_sp_neigh_entry *neigh_entry;
 -		struct mlxsw_sp_ipip_entry *ipip_entry;
 -	};
 -};
++	return 0;
+ 
 -struct mlxsw_sp_nexthop_group {
 -	void *priv;
 -	struct rhash_head ht_node;
 -	struct list_head fib_list; /* list of fib entries that use this group */
 -	struct neigh_table *neigh_tbl;
 -	u8 adj_index_valid:1,
 -	   gateway:1; /* routes using the group use a gateway */
 -	u32 adj_index;
 -	u16 ecmp_size;
 -	u16 count;
 -	struct mlxsw_sp_nexthop nexthops[0];
 -#define nh_rif	nexthops[0].rif
 -};
++err_neigh_init:
++	mlxsw_sp_nexthop_rif_fini(nh);
++	return err;
++}
+ 
 -static struct fib_info *
 -mlxsw_sp_nexthop4_group_fi(const struct mlxsw_sp_nexthop_group *nh_grp)
++static void mlxsw_sp_nexthop4_type_fini(struct mlxsw_sp *mlxsw_sp,
++					struct mlxsw_sp_nexthop *nh)
+ {
 -	return nh_grp->priv;
++	mlxsw_sp_nexthop_type_fini(mlxsw_sp, nh);
+ }
+ 
 -struct mlxsw_sp_nexthop_group_cmp_arg {
 -	enum mlxsw_sp_l3proto proto;
 -	union {
 -		struct fib_info *fi;
 -		struct mlxsw_sp_fib6_entry *fib6_entry;
 -	};
 -};
 -
 -static bool
 -mlxsw_sp_nexthop6_group_has_nexthop(const struct mlxsw_sp_nexthop_group *nh_grp,
 -				    const struct in6_addr *gw, int ifindex)
++static int mlxsw_sp_nexthop4_init(struct mlxsw_sp *mlxsw_sp,
++				  struct mlxsw_sp_nexthop_group *nh_grp,
++				  struct mlxsw_sp_nexthop *nh,
++				  struct fib_nh *fib_nh)
++>>>>>>> de0f43c01a4b (mlxsw: spectrum_router: Track RIF of IPIP next hops)
  {
 -	int i;
 -
 -	for (i = 0; i < nh_grp->count; i++) {
 -		const struct mlxsw_sp_nexthop *nh;
 +	struct net_device *dev = fib_nh->nh_dev;
 +	struct in_device *in_dev;
 +	struct mlxsw_sp_rif *r;
 +	int err;
  
 -		nh = &nh_grp->nexthops[i];
 -		if (nh->ifindex == ifindex &&
 -		    ipv6_addr_equal(gw, (struct in6_addr *) nh->gw_addr))
 -			return true;
 -	}
 +	nh->nh_grp = nh_grp;
 +	nh->key.fib_nh = fib_nh;
 +	err = mlxsw_sp_nexthop_insert(mlxsw_sp, nh);
 +	if (err)
 +		return err;
  
 -	return false;
 -}
 +	if (!dev)
 +		return 0;
  
 -static bool
 -mlxsw_sp_nexthop6_group_cmp(const struct mlxsw_sp_nexthop_group *nh_grp,
 -			    const struct mlxsw_sp_fib6_entry *fib6_entry)
 -{
 -	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
 +	in_dev = __in_dev_get_rtnl(dev);
 +#if 0 /* RHEL does not support LINKDOWN yet - the condition is always false */
 +	if (in_dev && IN_DEV_IGNORE_ROUTES_WITH_LINKDOWN(in_dev) &&
 +	    fib_nh->nh_flags & RTNH_F_LINKDOWN)
 +		return 0;
 +#endif
  
 -	if (nh_grp->count != fib6_entry->nrt6)
 -		return false;
 +	r = mlxsw_sp_rif_find_by_dev(mlxsw_sp, dev);
 +	if (!r)
 +		return 0;
 +	mlxsw_sp_nexthop_rif_init(nh, r);
  
 -	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
 -		struct in6_addr *gw;
 -		int ifindex;
 +	err = mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
 +	if (err)
 +		goto err_nexthop_neigh_init;
  
 -		ifindex = mlxsw_sp_rt6->rt->dst.dev->ifindex;
 -		gw = &mlxsw_sp_rt6->rt->rt6i_gateway;
 -		if (!mlxsw_sp_nexthop6_group_has_nexthop(nh_grp, gw, ifindex))
 -			return false;
 -	}
 +	return 0;
  
 -	return true;
 +err_nexthop_neigh_init:
 +	mlxsw_sp_nexthop_rif_fini(nh);
 +	mlxsw_sp_nexthop_remove(mlxsw_sp, nh);
 +	return err;
  }
  
 -static int
 -mlxsw_sp_nexthop_group_cmp(struct rhashtable_compare_arg *arg, const void *ptr)
 +static void mlxsw_sp_nexthop_fini(struct mlxsw_sp *mlxsw_sp,
 +				  struct mlxsw_sp_nexthop *nh)
  {
 -	const struct mlxsw_sp_nexthop_group_cmp_arg *cmp_arg = arg->key;
 -	const struct mlxsw_sp_nexthop_group *nh_grp = ptr;
 +	mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
 +	mlxsw_sp_nexthop_rif_fini(nh);
 +	mlxsw_sp_nexthop_remove(mlxsw_sp, nh);
 +}
  
 -	switch (cmp_arg->proto) {
 -	case MLXSW_SP_L3_PROTO_IPV4:
 -		return cmp_arg->fi != mlxsw_sp_nexthop4_group_fi(nh_grp);
 -	case MLXSW_SP_L3_PROTO_IPV6:
 -		return !mlxsw_sp_nexthop6_group_cmp(nh_grp,
 -						    cmp_arg->fib6_entry);
 -	default:
 -		WARN_ON(1);
 -		return 1;
 +static void mlxsw_sp_nexthop_event(struct mlxsw_sp *mlxsw_sp,
 +				   unsigned long event, struct fib_nh *fib_nh)
 +{
 +	struct mlxsw_sp_nexthop_key key;
 +	struct mlxsw_sp_nexthop *nh;
 +	struct mlxsw_sp_rif *r;
 +
 +	if (mlxsw_sp->router.aborted)
 +		return;
 +
 +	key.fib_nh = fib_nh;
 +	nh = mlxsw_sp_nexthop_lookup(mlxsw_sp, key);
 +	if (WARN_ON_ONCE(!nh))
 +		return;
 +
 +	r = mlxsw_sp_rif_find_by_dev(mlxsw_sp, fib_nh->nh_dev);
 +	if (!r)
 +		return;
 +
 +	switch (event) {
 +	case FIB_EVENT_NH_ADD:
 +		mlxsw_sp_nexthop_rif_init(nh, r);
 +		mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
 +		break;
 +	case FIB_EVENT_NH_DEL:
 +		mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
 +		mlxsw_sp_nexthop_rif_fini(nh);
 +		break;
  	}
 -}
  
 -static int
 -mlxsw_sp_nexthop_group_type(const struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	return nh_grp->neigh_tbl->family;
 +	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh->nh_grp);
  }
  
 -static u32 mlxsw_sp_nexthop_group_hash_obj(const void *data, u32 len, u32 seed)
 +static void mlxsw_sp_nexthop_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 +					   struct mlxsw_sp_rif *r)
  {
 -	const struct mlxsw_sp_nexthop_group *nh_grp = data;
 -	const struct mlxsw_sp_nexthop *nh;
 -	struct fib_info *fi;
 -	unsigned int val;
 -	int i;
 +	struct mlxsw_sp_nexthop *nh, *tmp;
  
 -	switch (mlxsw_sp_nexthop_group_type(nh_grp)) {
 -	case AF_INET:
 -		fi = mlxsw_sp_nexthop4_group_fi(nh_grp);
 -		return jhash(&fi, sizeof(fi), seed);
 -	case AF_INET6:
 -		val = nh_grp->count;
 -		for (i = 0; i < nh_grp->count; i++) {
 -			nh = &nh_grp->nexthops[i];
 -			val ^= nh->ifindex;
 -		}
 -		return jhash(&val, sizeof(val), seed);
 -	default:
 -		WARN_ON(1);
 -		return 0;
 +	list_for_each_entry_safe(nh, tmp, &r->nexthop_list, rif_list_node) {
 +		mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
 +		mlxsw_sp_nexthop_rif_fini(nh);
 +		mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh->nh_grp);
  	}
  }
  
@@@ -2597,233 -4543,710 +2722,668 @@@ err_fib4_entry_create
  	return err;
  }
  
 -static void mlxsw_sp_router_fib6_del(struct mlxsw_sp *mlxsw_sp,
 -				     struct rt6_info *rt)
 +static void mlxsw_sp_router_fib4_del(struct mlxsw_sp *mlxsw_sp,
 +				     struct fib_entry_notifier_info *fen_info)
  {
 -	struct mlxsw_sp_fib6_entry *fib6_entry;
++<<<<<<< HEAD
++=======
++	struct mlxsw_sp_fib4_entry *fib4_entry;
+ 	struct mlxsw_sp_fib_node *fib_node;
+ 
+ 	if (mlxsw_sp->router->aborted)
+ 		return;
+ 
 -	if (mlxsw_sp_fib6_rt_should_ignore(rt))
++	fib4_entry = mlxsw_sp_fib4_entry_lookup(mlxsw_sp, fen_info);
++	if (WARN_ON(!fib4_entry))
+ 		return;
++	fib_node = fib4_entry->common.fib_node;
+ 
 -	fib6_entry = mlxsw_sp_fib6_entry_lookup(mlxsw_sp, rt);
 -	if (WARN_ON(!fib6_entry))
 -		return;
++	mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib4_entry);
++	mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib4_entry);
++	mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
++}
+ 
 -	/* If route is part of a multipath entry, but not the last one
 -	 * removed, then only reduce its nexthop group.
++static bool mlxsw_sp_fib6_rt_should_ignore(const struct rt6_info *rt)
++{
++	/* Packets with link-local destination IP arriving to the router
++	 * are trapped to the CPU, so no need to program specific routes
++	 * for them.
+ 	 */
 -	if (!list_is_singular(&fib6_entry->rt6_list)) {
 -		mlxsw_sp_fib6_entry_nexthop_del(mlxsw_sp, fib6_entry, rt);
 -		return;
 -	}
++	if (ipv6_addr_type(&rt->rt6i_dst.addr) & IPV6_ADDR_LINKLOCAL)
++		return true;
+ 
 -	fib_node = fib6_entry->common.fib_node;
++	/* Multicast routes aren't supported, so ignore them. Neighbour
++	 * Discovery packets are specifically trapped.
++	 */
++	if (ipv6_addr_type(&rt->rt6i_dst.addr) & IPV6_ADDR_MULTICAST)
++		return true;
+ 
 -	mlxsw_sp_fib6_node_entry_unlink(mlxsw_sp, fib6_entry);
 -	mlxsw_sp_fib6_entry_destroy(mlxsw_sp, fib6_entry);
 -	mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
++	/* Cloned routes are irrelevant in the forwarding path. */
++	if (rt->rt6i_flags & RTF_CACHE)
++		return true;
++
++	return false;
+ }
+ 
 -static int __mlxsw_sp_router_set_abort_trap(struct mlxsw_sp *mlxsw_sp,
 -					    enum mlxsw_reg_ralxx_protocol proto,
 -					    u8 tree_id)
++static struct mlxsw_sp_rt6 *mlxsw_sp_rt6_create(struct rt6_info *rt)
+ {
 -	char ralta_pl[MLXSW_REG_RALTA_LEN];
 -	char ralst_pl[MLXSW_REG_RALST_LEN];
 -	int i, err;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
+ 
 -	mlxsw_reg_ralta_pack(ralta_pl, true, proto, tree_id);
 -	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralta), ralta_pl);
 -	if (err)
 -		return err;
++	mlxsw_sp_rt6 = kzalloc(sizeof(*mlxsw_sp_rt6), GFP_KERNEL);
++	if (!mlxsw_sp_rt6)
++		return ERR_PTR(-ENOMEM);
+ 
 -	mlxsw_reg_ralst_pack(ralst_pl, 0xff, tree_id);
 -	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralst), ralst_pl);
 -	if (err)
 -		return err;
++	/* In case of route replace, replaced route is deleted with
++	 * no notification. Take reference to prevent accessing freed
++	 * memory.
++	 */
++	mlxsw_sp_rt6->rt = rt;
++	rt6_hold(rt);
+ 
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 -		struct mlxsw_sp_vr *vr = &mlxsw_sp->router->vrs[i];
 -		char raltb_pl[MLXSW_REG_RALTB_LEN];
 -		char ralue_pl[MLXSW_REG_RALUE_LEN];
++	return mlxsw_sp_rt6;
++}
+ 
 -		mlxsw_reg_raltb_pack(raltb_pl, vr->id, proto, tree_id);
 -		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(raltb),
 -				      raltb_pl);
 -		if (err)
 -			return err;
++#if IS_ENABLED(CONFIG_IPV6)
++static void mlxsw_sp_rt6_release(struct rt6_info *rt)
++{
++	rt6_release(rt);
++}
++#else
++static void mlxsw_sp_rt6_release(struct rt6_info *rt)
++{
++}
++#endif
+ 
 -		mlxsw_reg_ralue_pack(ralue_pl, proto,
 -				     MLXSW_REG_RALUE_OP_WRITE_WRITE, vr->id, 0);
 -		mlxsw_reg_ralue_act_ip2me_pack(ralue_pl);
 -		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralue),
 -				      ralue_pl);
 -		if (err)
 -			return err;
++static void mlxsw_sp_rt6_destroy(struct mlxsw_sp_rt6 *mlxsw_sp_rt6)
++{
++	mlxsw_sp_rt6_release(mlxsw_sp_rt6->rt);
++	kfree(mlxsw_sp_rt6);
++}
++
++static bool mlxsw_sp_fib6_rt_can_mp(const struct rt6_info *rt)
++{
++	/* RTF_CACHE routes are ignored */
++	return (rt->rt6i_flags & (RTF_GATEWAY | RTF_ADDRCONF)) == RTF_GATEWAY;
++}
++
++static struct rt6_info *
++mlxsw_sp_fib6_entry_rt(const struct mlxsw_sp_fib6_entry *fib6_entry)
++{
++	return list_first_entry(&fib6_entry->rt6_list, struct mlxsw_sp_rt6,
++				list)->rt;
++}
++
++static struct mlxsw_sp_fib6_entry *
++mlxsw_sp_fib6_node_mp_entry_find(const struct mlxsw_sp_fib_node *fib_node,
++				 const struct rt6_info *nrt, bool replace)
++{
++	struct mlxsw_sp_fib6_entry *fib6_entry;
++
++	if (!mlxsw_sp_fib6_rt_can_mp(nrt) || replace)
++		return NULL;
++
++	list_for_each_entry(fib6_entry, &fib_node->entry_list, common.list) {
++		struct rt6_info *rt = mlxsw_sp_fib6_entry_rt(fib6_entry);
++
++		/* RT6_TABLE_LOCAL and RT6_TABLE_MAIN share the same
++		 * virtual router.
++		 */
++		if (rt->rt6i_table->tb6_id > nrt->rt6i_table->tb6_id)
++			continue;
++		if (rt->rt6i_table->tb6_id != nrt->rt6i_table->tb6_id)
++			break;
++		if (rt->rt6i_metric < nrt->rt6i_metric)
++			continue;
++		if (rt->rt6i_metric == nrt->rt6i_metric &&
++		    mlxsw_sp_fib6_rt_can_mp(rt))
++			return fib6_entry;
++		if (rt->rt6i_metric > nrt->rt6i_metric)
++			break;
+ 	}
+ 
 -	return 0;
++	return NULL;
+ }
+ 
 -static int mlxsw_sp_router_set_abort_trap(struct mlxsw_sp *mlxsw_sp)
++static struct mlxsw_sp_rt6 *
++mlxsw_sp_fib6_entry_rt_find(const struct mlxsw_sp_fib6_entry *fib6_entry,
++			    const struct rt6_info *rt)
+ {
 -	enum mlxsw_reg_ralxx_protocol proto = MLXSW_REG_RALXX_PROTOCOL_IPV4;
 -	int err;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
+ 
 -	err = __mlxsw_sp_router_set_abort_trap(mlxsw_sp, proto,
 -					       MLXSW_SP_LPM_TREE_MIN);
 -	if (err)
 -		return err;
++	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
++		if (mlxsw_sp_rt6->rt == rt)
++			return mlxsw_sp_rt6;
++	}
+ 
 -	proto = MLXSW_REG_RALXX_PROTOCOL_IPV6;
 -	return __mlxsw_sp_router_set_abort_trap(mlxsw_sp, proto,
 -						MLXSW_SP_LPM_TREE_MIN + 1);
++	return NULL;
+ }
+ 
 -static void mlxsw_sp_fib4_node_flush(struct mlxsw_sp *mlxsw_sp,
 -				     struct mlxsw_sp_fib_node *fib_node)
++static bool mlxsw_sp_nexthop6_ipip_type(const struct mlxsw_sp *mlxsw_sp,
++					const struct rt6_info *rt,
++					enum mlxsw_sp_ipip_type *ret)
+ {
 -	struct mlxsw_sp_fib4_entry *fib4_entry, *tmp;
++	return rt->dst.dev &&
++	       mlxsw_sp_netdev_ipip_type(mlxsw_sp, rt->dst.dev, ret);
++}
+ 
 -	list_for_each_entry_safe(fib4_entry, tmp, &fib_node->entry_list,
 -				 common.list) {
 -		bool do_break = &tmp->common.list == &fib_node->entry_list;
++static int mlxsw_sp_nexthop6_type_init(struct mlxsw_sp *mlxsw_sp,
++				       struct mlxsw_sp_nexthop_group *nh_grp,
++				       struct mlxsw_sp_nexthop *nh,
++				       const struct rt6_info *rt)
++{
++	struct mlxsw_sp_router *router = mlxsw_sp->router;
++	struct net_device *dev = rt->dst.dev;
++	enum mlxsw_sp_ipip_type ipipt;
++	struct mlxsw_sp_rif *rif;
++	int err;
+ 
 -		mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib4_entry);
 -		mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib4_entry);
 -		mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
 -		/* Break when entry list is empty and node was freed.
 -		 * Otherwise, we'll access freed memory in the next
 -		 * iteration.
 -		 */
 -		if (do_break)
 -			break;
++	if (mlxsw_sp_nexthop6_ipip_type(mlxsw_sp, rt, &ipipt) &&
++	    router->ipip_ops_arr[ipipt]->can_offload(mlxsw_sp, dev,
++						     MLXSW_SP_L3_PROTO_IPV6)) {
++		nh->type = MLXSW_SP_NEXTHOP_TYPE_IPIP;
++		err = mlxsw_sp_nexthop_ipip_init(mlxsw_sp, ipipt, nh, dev);
++		if (err)
++			return err;
++		mlxsw_sp_nexthop_rif_init(nh, &nh->ipip_entry->ol_lb->common);
++		return 0;
+ 	}
 -}
+ 
 -static void mlxsw_sp_fib6_node_flush(struct mlxsw_sp *mlxsw_sp,
 -				     struct mlxsw_sp_fib_node *fib_node)
 -{
 -	struct mlxsw_sp_fib6_entry *fib6_entry, *tmp;
++	nh->type = MLXSW_SP_NEXTHOP_TYPE_ETH;
++	rif = mlxsw_sp_rif_find_by_dev(mlxsw_sp, dev);
++	if (!rif)
++		return 0;
++	mlxsw_sp_nexthop_rif_init(nh, rif);
+ 
 -	list_for_each_entry_safe(fib6_entry, tmp, &fib_node->entry_list,
 -				 common.list) {
 -		bool do_break = &tmp->common.list == &fib_node->entry_list;
++	err = mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
++	if (err)
++		goto err_nexthop_neigh_init;
+ 
 -		mlxsw_sp_fib6_node_entry_unlink(mlxsw_sp, fib6_entry);
 -		mlxsw_sp_fib6_entry_destroy(mlxsw_sp, fib6_entry);
 -		mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
 -		if (do_break)
 -			break;
 -	}
 -}
++	return 0;
+ 
 -static void mlxsw_sp_fib_node_flush(struct mlxsw_sp *mlxsw_sp,
 -				    struct mlxsw_sp_fib_node *fib_node)
 -{
 -	switch (fib_node->fib->proto) {
 -	case MLXSW_SP_L3_PROTO_IPV4:
 -		mlxsw_sp_fib4_node_flush(mlxsw_sp, fib_node);
 -		break;
 -	case MLXSW_SP_L3_PROTO_IPV6:
 -		mlxsw_sp_fib6_node_flush(mlxsw_sp, fib_node);
 -		break;
 -	}
++err_nexthop_neigh_init:
++	mlxsw_sp_nexthop_rif_fini(nh);
++	return err;
+ }
+ 
 -static void mlxsw_sp_vr_fib_flush(struct mlxsw_sp *mlxsw_sp,
 -				  struct mlxsw_sp_vr *vr,
 -				  enum mlxsw_sp_l3proto proto)
++static void mlxsw_sp_nexthop6_type_fini(struct mlxsw_sp *mlxsw_sp,
++					struct mlxsw_sp_nexthop *nh)
+ {
 -	struct mlxsw_sp_fib *fib = mlxsw_sp_vr_fib(vr, proto);
 -	struct mlxsw_sp_fib_node *fib_node, *tmp;
 -
 -	list_for_each_entry_safe(fib_node, tmp, &fib->node_list, list) {
 -		bool do_break = &tmp->list == &fib->node_list;
 -
 -		mlxsw_sp_fib_node_flush(mlxsw_sp, fib_node);
 -		if (do_break)
 -			break;
 -	}
++	mlxsw_sp_nexthop_type_fini(mlxsw_sp, nh);
+ }
+ 
 -static void mlxsw_sp_router_fib_flush(struct mlxsw_sp *mlxsw_sp)
++static int mlxsw_sp_nexthop6_init(struct mlxsw_sp *mlxsw_sp,
++				  struct mlxsw_sp_nexthop_group *nh_grp,
++				  struct mlxsw_sp_nexthop *nh,
++				  const struct rt6_info *rt)
+ {
 -	int i;
++	struct net_device *dev = rt->dst.dev;
+ 
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 -		struct mlxsw_sp_vr *vr = &mlxsw_sp->router->vrs[i];
++	nh->nh_grp = nh_grp;
++	memcpy(&nh->gw_addr, &rt->rt6i_gateway, sizeof(nh->gw_addr));
+ 
 -		if (!mlxsw_sp_vr_is_used(vr))
 -			continue;
 -		mlxsw_sp_vr_fib_flush(mlxsw_sp, vr, MLXSW_SP_L3_PROTO_IPV4);
++	if (!dev)
++		return 0;
++	nh->ifindex = dev->ifindex;
+ 
 -		/* If virtual router was only used for IPv4, then it's no
 -		 * longer used.
 -		 */
 -		if (!mlxsw_sp_vr_is_used(vr))
 -			continue;
 -		mlxsw_sp_vr_fib_flush(mlxsw_sp, vr, MLXSW_SP_L3_PROTO_IPV6);
 -	}
++	return mlxsw_sp_nexthop6_type_init(mlxsw_sp, nh_grp, nh, rt);
+ }
+ 
 -static void mlxsw_sp_router_fib_abort(struct mlxsw_sp *mlxsw_sp)
++static void mlxsw_sp_nexthop6_fini(struct mlxsw_sp *mlxsw_sp,
++				   struct mlxsw_sp_nexthop *nh)
+ {
 -	int err;
 -
 -	if (mlxsw_sp->router->aborted)
 -		return;
 -	dev_warn(mlxsw_sp->bus_info->dev, "FIB abort triggered. Note that FIB entries are no longer being offloaded to this device.\n");
 -	mlxsw_sp_router_fib_flush(mlxsw_sp);
 -	mlxsw_sp->router->aborted = true;
 -	err = mlxsw_sp_router_set_abort_trap(mlxsw_sp);
 -	if (err)
 -		dev_warn(mlxsw_sp->bus_info->dev, "Failed to set abort trap.\n");
++	mlxsw_sp_nexthop6_type_fini(mlxsw_sp, nh);
+ }
+ 
 -struct mlxsw_sp_fib_event_work {
 -	struct work_struct work;
 -	union {
 -		struct fib6_entry_notifier_info fen6_info;
 -		struct fib_entry_notifier_info fen_info;
 -		struct fib_rule_notifier_info fr_info;
 -		struct fib_nh_notifier_info fnh_info;
 -	};
 -	struct mlxsw_sp *mlxsw_sp;
 -	unsigned long event;
 -};
 -
 -static void mlxsw_sp_router_fib4_event_work(struct work_struct *work)
++static bool mlxsw_sp_rt6_is_gateway(const struct mlxsw_sp *mlxsw_sp,
++				    const struct rt6_info *rt)
+ {
 -	struct mlxsw_sp_fib_event_work *fib_work =
 -		container_of(work, struct mlxsw_sp_fib_event_work, work);
 -	struct mlxsw_sp *mlxsw_sp = fib_work->mlxsw_sp;
 -	struct fib_rule *rule;
 -	bool replace, append;
 -	int err;
 -
 -	/* Protect internal structures from changes */
 -	rtnl_lock();
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD:
 -		replace = fib_work->event == FIB_EVENT_ENTRY_REPLACE;
 -		append = fib_work->event == FIB_EVENT_ENTRY_APPEND;
 -		err = mlxsw_sp_router_fib4_add(mlxsw_sp, &fib_work->fen_info,
 -					       replace, append);
 -		if (err)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		fib_info_put(fib_work->fen_info.fi);
 -		break;
 -	case FIB_EVENT_ENTRY_DEL:
 -		mlxsw_sp_router_fib4_del(mlxsw_sp, &fib_work->fen_info);
 -		fib_info_put(fib_work->fen_info.fi);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		rule = fib_work->fr_info.rule;
 -		if (!fib4_rule_default(rule) && !rule->l3mdev)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		fib_rule_put(rule);
 -		break;
 -	case FIB_EVENT_NH_ADD: /* fall through */
 -	case FIB_EVENT_NH_DEL:
 -		mlxsw_sp_nexthop4_event(mlxsw_sp, fib_work->event,
 -					fib_work->fnh_info.fib_nh);
 -		fib_info_put(fib_work->fnh_info.fib_nh->nh_parent);
 -		break;
 -	}
 -	rtnl_unlock();
 -	kfree(fib_work);
++	return rt->rt6i_flags & RTF_GATEWAY ||
++	       mlxsw_sp_nexthop6_ipip_type(mlxsw_sp, rt, NULL);
+ }
+ 
 -static void mlxsw_sp_router_fib6_event_work(struct work_struct *work)
++static struct mlxsw_sp_nexthop_group *
++mlxsw_sp_nexthop6_group_create(struct mlxsw_sp *mlxsw_sp,
++			       struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	struct mlxsw_sp_fib_event_work *fib_work =
 -		container_of(work, struct mlxsw_sp_fib_event_work, work);
 -	struct mlxsw_sp *mlxsw_sp = fib_work->mlxsw_sp;
 -	struct fib_rule *rule;
 -	bool replace;
++	struct mlxsw_sp_nexthop_group *nh_grp;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
++	struct mlxsw_sp_nexthop *nh;
++	size_t alloc_size;
++	int i = 0;
+ 	int err;
+ 
 -	rtnl_lock();
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD:
 -		replace = fib_work->event == FIB_EVENT_ENTRY_REPLACE;
 -		err = mlxsw_sp_router_fib6_add(mlxsw_sp,
 -					       fib_work->fen6_info.rt, replace);
++	alloc_size = sizeof(*nh_grp) +
++		     fib6_entry->nrt6 * sizeof(struct mlxsw_sp_nexthop);
++	nh_grp = kzalloc(alloc_size, GFP_KERNEL);
++	if (!nh_grp)
++		return ERR_PTR(-ENOMEM);
++	INIT_LIST_HEAD(&nh_grp->fib_list);
++#if IS_ENABLED(CONFIG_IPV6)
++	nh_grp->neigh_tbl = &nd_tbl;
++#endif
++	mlxsw_sp_rt6 = list_first_entry(&fib6_entry->rt6_list,
++					struct mlxsw_sp_rt6, list);
++	nh_grp->gateway = mlxsw_sp_rt6_is_gateway(mlxsw_sp, mlxsw_sp_rt6->rt);
++	nh_grp->count = fib6_entry->nrt6;
++	for (i = 0; i < nh_grp->count; i++) {
++		struct rt6_info *rt = mlxsw_sp_rt6->rt;
++
++		nh = &nh_grp->nexthops[i];
++		err = mlxsw_sp_nexthop6_init(mlxsw_sp, nh_grp, nh, rt);
+ 		if (err)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		mlxsw_sp_rt6_release(fib_work->fen6_info.rt);
 -		break;
 -	case FIB_EVENT_ENTRY_DEL:
 -		mlxsw_sp_router_fib6_del(mlxsw_sp, fib_work->fen6_info.rt);
 -		mlxsw_sp_rt6_release(fib_work->fen6_info.rt);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		rule = fib_work->fr_info.rule;
 -		if (!fib6_rule_default(rule) && !rule->l3mdev)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		fib_rule_put(rule);
 -		break;
++			goto err_nexthop6_init;
++		mlxsw_sp_rt6 = list_next_entry(mlxsw_sp_rt6, list);
+ 	}
 -	rtnl_unlock();
 -	kfree(fib_work);
 -}
+ 
 -static void mlxsw_sp_router_fib4_event(struct mlxsw_sp_fib_event_work *fib_work,
 -				       struct fib_notifier_info *info)
 -{
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD: /* fall through */
 -	case FIB_EVENT_ENTRY_DEL:
 -		memcpy(&fib_work->fen_info, info, sizeof(fib_work->fen_info));
 -		/* Take referece on fib_info to prevent it from being
 -		 * freed while work is queued. Release it afterwards.
 -		 */
 -		fib_info_hold(fib_work->fen_info.fi);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		memcpy(&fib_work->fr_info, info, sizeof(fib_work->fr_info));
 -		fib_rule_get(fib_work->fr_info.rule);
 -		break;
 -	case FIB_EVENT_NH_ADD: /* fall through */
 -	case FIB_EVENT_NH_DEL:
 -		memcpy(&fib_work->fnh_info, info, sizeof(fib_work->fnh_info));
 -		fib_info_hold(fib_work->fnh_info.fib_nh->nh_parent);
 -		break;
++	err = mlxsw_sp_nexthop_group_insert(mlxsw_sp, nh_grp);
++	if (err)
++		goto err_nexthop_group_insert;
++
++	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
++	return nh_grp;
++
++err_nexthop_group_insert:
++err_nexthop6_init:
++	for (i--; i >= 0; i--) {
++		nh = &nh_grp->nexthops[i];
++		mlxsw_sp_nexthop6_fini(mlxsw_sp, nh);
+ 	}
++	kfree(nh_grp);
++	return ERR_PTR(err);
+ }
+ 
 -static void mlxsw_sp_router_fib6_event(struct mlxsw_sp_fib_event_work *fib_work,
 -				       struct fib_notifier_info *info)
++static void
++mlxsw_sp_nexthop6_group_destroy(struct mlxsw_sp *mlxsw_sp,
++				struct mlxsw_sp_nexthop_group *nh_grp)
+ {
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD: /* fall through */
 -	case FIB_EVENT_ENTRY_DEL:
 -		memcpy(&fib_work->fen6_info, info, sizeof(fib_work->fen6_info));
 -		rt6_hold(fib_work->fen6_info.rt);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		memcpy(&fib_work->fr_info, info, sizeof(fib_work->fr_info));
 -		fib_rule_get(fib_work->fr_info.rule);
 -		break;
++	struct mlxsw_sp_nexthop *nh;
++	int i = nh_grp->count;
++
++	mlxsw_sp_nexthop_group_remove(mlxsw_sp, nh_grp);
++	for (i--; i >= 0; i--) {
++		nh = &nh_grp->nexthops[i];
++		mlxsw_sp_nexthop6_fini(mlxsw_sp, nh);
+ 	}
++	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
++	WARN_ON(nh_grp->adj_index_valid);
++	kfree(nh_grp);
+ }
+ 
 -/* Called with rcu_read_lock() */
 -static int mlxsw_sp_router_fib_event(struct notifier_block *nb,
 -				     unsigned long event, void *ptr)
++static int mlxsw_sp_nexthop6_group_get(struct mlxsw_sp *mlxsw_sp,
++				       struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	struct mlxsw_sp_fib_event_work *fib_work;
 -	struct fib_notifier_info *info = ptr;
 -	struct mlxsw_sp_router *router;
 -
 -	if (!net_eq(info->net, &init_net) ||
 -	    (info->family != AF_INET && info->family != AF_INET6))
 -		return NOTIFY_DONE;
 -
 -	fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
 -	if (WARN_ON(!fib_work))
 -		return NOTIFY_BAD;
 -
 -	router = container_of(nb, struct mlxsw_sp_router, fib_nb);
 -	fib_work->mlxsw_sp = router->mlxsw_sp;
 -	fib_work->event = event;
++	struct mlxsw_sp_nexthop_group *nh_grp;
+ 
 -	switch (info->family) {
 -	case AF_INET:
 -		INIT_WORK(&fib_work->work, mlxsw_sp_router_fib4_event_work);
 -		mlxsw_sp_router_fib4_event(fib_work, info);
 -		break;
 -	case AF_INET6:
 -		INIT_WORK(&fib_work->work, mlxsw_sp_router_fib6_event_work);
 -		mlxsw_sp_router_fib6_event(fib_work, info);
 -		break;
++	nh_grp = mlxsw_sp_nexthop6_group_lookup(mlxsw_sp, fib6_entry);
++	if (!nh_grp) {
++		nh_grp = mlxsw_sp_nexthop6_group_create(mlxsw_sp, fib6_entry);
++		if (IS_ERR(nh_grp))
++			return PTR_ERR(nh_grp);
+ 	}
+ 
 -	mlxsw_core_schedule_work(&fib_work->work);
++	list_add_tail(&fib6_entry->common.nexthop_group_node,
++		      &nh_grp->fib_list);
++	fib6_entry->common.nh_group = nh_grp;
+ 
 -	return NOTIFY_DONE;
++	return 0;
+ }
+ 
 -static struct mlxsw_sp_rif *
 -mlxsw_sp_rif_find_by_dev(const struct mlxsw_sp *mlxsw_sp,
 -			 const struct net_device *dev)
++static void mlxsw_sp_nexthop6_group_put(struct mlxsw_sp *mlxsw_sp,
++					struct mlxsw_sp_fib_entry *fib_entry)
+ {
 -	int i;
 -
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_RIFS); i++)
 -		if (mlxsw_sp->router->rifs[i] &&
 -		    mlxsw_sp->router->rifs[i]->dev == dev)
 -			return mlxsw_sp->router->rifs[i];
++	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
+ 
 -	return NULL;
++	list_del(&fib_entry->nexthop_group_node);
++	if (!list_empty(&nh_grp->fib_list))
++		return;
++	mlxsw_sp_nexthop6_group_destroy(mlxsw_sp, nh_grp);
+ }
+ 
 -static int mlxsw_sp_router_rif_disable(struct mlxsw_sp *mlxsw_sp, u16 rif)
++static int
++mlxsw_sp_nexthop6_group_update(struct mlxsw_sp *mlxsw_sp,
++			       struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	char ritr_pl[MLXSW_REG_RITR_LEN];
++	struct mlxsw_sp_nexthop_group *old_nh_grp = fib6_entry->common.nh_group;
+ 	int err;
+ 
 -	mlxsw_reg_ritr_rif_pack(ritr_pl, rif);
 -	err = mlxsw_reg_query(mlxsw_sp->core, MLXSW_REG(ritr), ritr_pl);
 -	if (WARN_ON_ONCE(err))
 -		return err;
 -
 -	mlxsw_reg_ritr_enable_set(ritr_pl, false);
 -	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ritr), ritr_pl);
 -}
 -
 -static void mlxsw_sp_router_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 -					  struct mlxsw_sp_rif *rif)
 -{
 -	mlxsw_sp_router_rif_disable(mlxsw_sp, rif->rif_index);
 -	mlxsw_sp_nexthop_rif_gone_sync(mlxsw_sp, rif);
 -	mlxsw_sp_neigh_rif_gone_sync(mlxsw_sp, rif);
 -}
++	fib6_entry->common.nh_group = NULL;
++	list_del(&fib6_entry->common.nexthop_group_node);
+ 
 -static bool
 -mlxsw_sp_rif_should_config(struct mlxsw_sp_rif *rif, struct net_device *dev,
 -			   unsigned long event)
 -{
 -	struct inet6_dev *inet6_dev;
 -	bool addr_list_empty = true;
 -	struct in_device *idev;
++	err = mlxsw_sp_nexthop6_group_get(mlxsw_sp, fib6_entry);
++	if (err)
++		goto err_nexthop6_group_get;
+ 
 -	switch (event) {
 -	case NETDEV_UP:
 -		return rif == NULL;
 -	case NETDEV_DOWN:
 -		idev = __in_dev_get_rtnl(dev);
 -		if (idev && idev->ifa_list)
 -			addr_list_empty = false;
++	/* In case this entry is offloaded, then the adjacency index
++	 * currently associated with it in the device's table is that
++	 * of the old group. Start using the new one instead.
++	 */
++	err = mlxsw_sp_fib_node_entry_add(mlxsw_sp, &fib6_entry->common);
++	if (err)
++		goto err_fib_node_entry_add;
+ 
 -		inet6_dev = __in6_dev_get(dev);
 -		if (addr_list_empty && inet6_dev &&
 -		    !list_empty(&inet6_dev->addr_list))
 -			addr_list_empty = false;
++	if (list_empty(&old_nh_grp->fib_list))
++		mlxsw_sp_nexthop6_group_destroy(mlxsw_sp, old_nh_grp);
+ 
 -		if (rif && addr_list_empty &&
 -		    !netif_is_l3_slave(rif->dev))
 -			return true;
 -		/* It is possible we already removed the RIF ourselves
 -		 * if it was assigned to a netdev that is now a bridge
 -		 * or LAG slave.
 -		 */
 -		return false;
 -	}
++	return 0;
+ 
 -	return false;
++err_fib_node_entry_add:
++	mlxsw_sp_nexthop6_group_put(mlxsw_sp, &fib6_entry->common);
++err_nexthop6_group_get:
++	list_add_tail(&fib6_entry->common.nexthop_group_node,
++		      &old_nh_grp->fib_list);
++	fib6_entry->common.nh_group = old_nh_grp;
++	return err;
+ }
+ 
 -static enum mlxsw_sp_rif_type
 -mlxsw_sp_dev_rif_type(const struct mlxsw_sp *mlxsw_sp,
 -		      const struct net_device *dev)
++static int
++mlxsw_sp_fib6_entry_nexthop_add(struct mlxsw_sp *mlxsw_sp,
++				struct mlxsw_sp_fib6_entry *fib6_entry,
++				struct rt6_info *rt)
+ {
 -	enum mlxsw_sp_fid_type type;
 -
 -	if (mlxsw_sp_netdev_ipip_type(mlxsw_sp, dev, NULL))
 -		return MLXSW_SP_RIF_TYPE_IPIP_LB;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
++	int err;
+ 
 -	/* Otherwise RIF type is derived from the type of the underlying FID. */
 -	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
 -		type = MLXSW_SP_FID_TYPE_8021Q;
 -	else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev))
 -		type = MLXSW_SP_FID_TYPE_8021Q;
 -	else if (netif_is_bridge_master(dev))
 -		type = MLXSW_SP_FID_TYPE_8021D;
 -	else
 -		type = MLXSW_SP_FID_TYPE_RFID;
++	mlxsw_sp_rt6 = mlxsw_sp_rt6_create(rt);
++	if (IS_ERR(mlxsw_sp_rt6))
++		return PTR_ERR(mlxsw_sp_rt6);
+ 
 -	return mlxsw_sp_fid_type_rif_type(mlxsw_sp, type);
 -}
++	list_add_tail(&mlxsw_sp_rt6->list, &fib6_entry->rt6_list);
++	fib6_entry->nrt6++;
+ 
 -static int mlxsw_sp_rif_index_alloc(struct mlxsw_sp *mlxsw_sp, u16 *p_rif_index)
 -{
 -	int i;
++	err = mlxsw_sp_nexthop6_group_update(mlxsw_sp, fib6_entry);
++	if (err)
++		goto err_nexthop6_group_update;
+ 
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_RIFS); i++) {
 -		if (!mlxsw_sp->router->rifs[i]) {
 -			*p_rif_index = i;
 -			return 0;
 -		}
 -	}
++	return 0;
+ 
 -	return -ENOBUFS;
++err_nexthop6_group_update:
++	fib6_entry->nrt6--;
++	list_del(&mlxsw_sp_rt6->list);
++	mlxsw_sp_rt6_destroy(mlxsw_sp_rt6);
++	return err;
+ }
+ 
 -static struct mlxsw_sp_rif *mlxsw_sp_rif_alloc(size_t rif_size, u16 rif_index,
 -					       u16 vr_id,
 -					       struct net_device *l3_dev)
++static void
++mlxsw_sp_fib6_entry_nexthop_del(struct mlxsw_sp *mlxsw_sp,
++				struct mlxsw_sp_fib6_entry *fib6_entry,
++				struct rt6_info *rt)
+ {
 -	struct mlxsw_sp_rif *rif;
 -
 -	rif = kzalloc(rif_size, GFP_KERNEL);
 -	if (!rif)
 -		return NULL;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
+ 
 -	INIT_LIST_HEAD(&rif->nexthop_list);
 -	INIT_LIST_HEAD(&rif->neigh_list);
 -	ether_addr_copy(rif->addr, l3_dev->dev_addr);
 -	rif->mtu = l3_dev->mtu;
 -	rif->vr_id = vr_id;
 -	rif->dev = l3_dev;
 -	rif->rif_index = rif_index;
++	mlxsw_sp_rt6 = mlxsw_sp_fib6_entry_rt_find(fib6_entry, rt);
++	if (WARN_ON(!mlxsw_sp_rt6))
++		return;
+ 
 -	return rif;
++	fib6_entry->nrt6--;
++	list_del(&mlxsw_sp_rt6->list);
++	mlxsw_sp_nexthop6_group_update(mlxsw_sp, fib6_entry);
++	mlxsw_sp_rt6_destroy(mlxsw_sp_rt6);
+ }
+ 
 -struct mlxsw_sp_rif *mlxsw_sp_rif_by_index(const struct mlxsw_sp *mlxsw_sp,
 -					   u16 rif_index)
++static void mlxsw_sp_fib6_entry_type_set(struct mlxsw_sp *mlxsw_sp,
++					 struct mlxsw_sp_fib_entry *fib_entry,
++					 const struct rt6_info *rt)
+ {
 -	return mlxsw_sp->router->rifs[rif_index];
++	/* Packets hitting RTF_REJECT routes need to be discarded by the
++	 * stack. We can rely on their destination device not having a
++	 * RIF (it's the loopback device) and can thus use action type
++	 * local, which will cause them to be trapped with a lower
++	 * priority than packets that need to be locally received.
++	 */
++	if (rt->rt6i_flags & (RTF_LOCAL | RTF_ANYCAST))
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_TRAP;
++	else if (rt->rt6i_flags & RTF_REJECT)
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_LOCAL;
++	else if (mlxsw_sp_rt6_is_gateway(mlxsw_sp, rt))
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_REMOTE;
++	else
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_LOCAL;
+ }
+ 
 -u16 mlxsw_sp_rif_index(const struct mlxsw_sp_rif *rif)
++static void
++mlxsw_sp_fib6_entry_rt_destroy_all(struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	return rif->rif_index;
 -}
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6, *tmp;
+ 
 -u16 mlxsw_sp_ipip_lb_rif_index(const struct mlxsw_sp_rif_ipip_lb *lb_rif)
 -{
 -	return lb_rif->common.rif_index;
++	list_for_each_entry_safe(mlxsw_sp_rt6, tmp, &fib6_entry->rt6_list,
++				 list) {
++		fib6_entry->nrt6--;
++		list_del(&mlxsw_sp_rt6->list);
++		mlxsw_sp_rt6_destroy(mlxsw_sp_rt6);
++	}
+ }
+ 
 -u16 mlxsw_sp_ipip_lb_ul_vr_id(const struct mlxsw_sp_rif_ipip_lb *lb_rif)
++static struct mlxsw_sp_fib6_entry *
++mlxsw_sp_fib6_entry_create(struct mlxsw_sp *mlxsw_sp,
++			   struct mlxsw_sp_fib_node *fib_node,
++			   struct rt6_info *rt)
+ {
 -	return lb_rif->ul_vr_id;
 -}
++	struct mlxsw_sp_fib6_entry *fib6_entry;
++>>>>>>> de0f43c01a4b (mlxsw: spectrum_router: Track RIF of IPIP next hops)
 +	struct mlxsw_sp_fib_entry *fib_entry;
 +	struct mlxsw_sp_fib_node *fib_node;
  
 -int mlxsw_sp_rif_dev_ifindex(const struct mlxsw_sp_rif *rif)
 -{
 -	return rif->dev->ifindex;
 -}
 +	if (mlxsw_sp->router.aborted)
 +		return;
  
 -static struct mlxsw_sp_rif *
 -mlxsw_sp_rif_create(struct mlxsw_sp *mlxsw_sp,
 -		    const struct mlxsw_sp_rif_params *params)
 -{
 -	u32 tb_id = l3mdev_fib_table(params->dev);
 -	const struct mlxsw_sp_rif_ops *ops;
 -	struct mlxsw_sp_fid *fid = NULL;
 -	enum mlxsw_sp_rif_type type;
 -	struct mlxsw_sp_rif *rif;
 -	struct mlxsw_sp_vr *vr;
 -	u16 rif_index;
 -	int err;
 +	fib_entry = mlxsw_sp_fib4_entry_lookup(mlxsw_sp, fen_info);
 +	if (WARN_ON(!fib_entry))
 +		return;
 +	fib_node = fib_entry->fib_node;
  
 -	type = mlxsw_sp_dev_rif_type(mlxsw_sp, params->dev);
 -	ops = mlxsw_sp->router->rif_ops_arr[type];
 +	mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib_entry);
 +	mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib_entry);
 +	mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
 +}
  
 -	vr = mlxsw_sp_vr_get(mlxsw_sp, tb_id ? : RT_TABLE_MAIN);
 -	if (IS_ERR(vr))
 -		return ERR_CAST(vr);
 -	vr->rif_count++;
 +static int mlxsw_sp_router_set_abort_trap(struct mlxsw_sp *mlxsw_sp)
 +{
 +	char ralta_pl[MLXSW_REG_RALTA_LEN];
 +	char ralst_pl[MLXSW_REG_RALST_LEN];
 +	int i, err;
  
 -	err = mlxsw_sp_rif_index_alloc(mlxsw_sp, &rif_index);
 +	mlxsw_reg_ralta_pack(ralta_pl, true, MLXSW_REG_RALXX_PROTOCOL_IPV4,
 +			     MLXSW_SP_LPM_TREE_MIN);
 +	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralta), ralta_pl);
  	if (err)
 -		goto err_rif_index_alloc;
 +		return err;
  
 -	rif = mlxsw_sp_rif_alloc(ops->rif_size, rif_index, vr->id, params->dev);
 -	if (!rif) {
 -		err = -ENOMEM;
 -		goto err_rif_alloc;
 -	}
 -	rif->mlxsw_sp = mlxsw_sp;
 -	rif->ops = ops;
 -
 -	if (ops->fid_get) {
 -		fid = ops->fid_get(rif);
 -		if (IS_ERR(fid)) {
 -			err = PTR_ERR(fid);
 -			goto err_fid_get;
 -		}
 -		rif->fid = fid;
 -	}
 +	mlxsw_reg_ralst_pack(ralst_pl, 0xff, MLXSW_SP_LPM_TREE_MIN);
 +	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralst), ralst_pl);
 +	if (err)
 +		return err;
  
 -	if (ops->setup)
 -		ops->setup(rif, params);
 +	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 +		struct mlxsw_sp_vr *vr = &mlxsw_sp->router.vrs[i];
 +		char raltb_pl[MLXSW_REG_RALTB_LEN];
 +		char ralue_pl[MLXSW_REG_RALUE_LEN];
  
 -	err = ops->configure(rif);
 -	if (err)
 -		goto err_configure;
 +		if (!mlxsw_sp_vr_is_used(vr))
 +			continue;
  
 -	mlxsw_sp_rif_counters_alloc(rif);
 -	mlxsw_sp->router->rifs[rif_index] = rif;
 +		mlxsw_reg_raltb_pack(raltb_pl, vr->id,
 +				     MLXSW_REG_RALXX_PROTOCOL_IPV4,
 +				     MLXSW_SP_LPM_TREE_MIN);
 +		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(raltb),
 +				      raltb_pl);
 +		if (err)
 +			return err;
  
 -	return rif;
 +		mlxsw_reg_ralue_pack4(ralue_pl, MLXSW_SP_L3_PROTO_IPV4,
 +				      MLXSW_REG_RALUE_OP_WRITE_WRITE, vr->id, 0,
 +				      0);
 +		mlxsw_reg_ralue_act_ip2me_pack(ralue_pl);
 +		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralue),
 +				      ralue_pl);
 +		if (err)
 +			return err;
 +	}
  
 -err_configure:
 -	if (fid)
 -		mlxsw_sp_fid_put(fid);
 -err_fid_get:
 -	kfree(rif);
 -err_rif_alloc:
 -err_rif_index_alloc:
 -	vr->rif_count--;
 -	mlxsw_sp_vr_put(vr);
 -	return ERR_PTR(err);
 +	return 0;
  }
  
 -void mlxsw_sp_rif_destroy(struct mlxsw_sp_rif *rif)
 +static void mlxsw_sp_fib4_node_flush(struct mlxsw_sp *mlxsw_sp,
 +				     struct mlxsw_sp_fib_node *fib_node)
  {
 -	const struct mlxsw_sp_rif_ops *ops = rif->ops;
 -	struct mlxsw_sp *mlxsw_sp = rif->mlxsw_sp;
 -	struct mlxsw_sp_fid *fid = rif->fid;
 -	struct mlxsw_sp_vr *vr;
 +	struct mlxsw_sp_fib_entry *fib_entry, *tmp;
  
 -	mlxsw_sp_router_rif_gone_sync(mlxsw_sp, rif);
 -	vr = &mlxsw_sp->router->vrs[rif->vr_id];
 +	list_for_each_entry_safe(fib_entry, tmp, &fib_node->entry_list, list) {
 +		bool do_break = &tmp->list == &fib_node->entry_list;
  
 -	mlxsw_sp->router->rifs[rif->rif_index] = NULL;
 -	mlxsw_sp_rif_counters_free(rif);
 -	ops->deconfigure(rif);
 -	if (fid)
 -		/* Loopback RIFs are not associated with a FID. */
 -		mlxsw_sp_fid_put(fid);
 -	kfree(rif);
 -	vr->rif_count--;
 -	mlxsw_sp_vr_put(vr);
 +		mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib_entry);
 +		mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib_entry);
 +		mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
 +		/* Break when entry list is empty and node was freed.
 +		 * Otherwise, we'll access freed memory in the next
 +		 * iteration.
 +		 */
 +		if (do_break)
 +			break;
 +	}
  }
  
 -static void
 -mlxsw_sp_rif_subport_params_init(struct mlxsw_sp_rif_params *params,
 -				 struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan)
 +static void mlxsw_sp_fib_node_flush(struct mlxsw_sp *mlxsw_sp,
 +				    struct mlxsw_sp_fib_node *fib_node)
  {
 -	struct mlxsw_sp_port *mlxsw_sp_port = mlxsw_sp_port_vlan->mlxsw_sp_port;
 -
 -	params->vid = mlxsw_sp_port_vlan->vid;
 -	params->lag = mlxsw_sp_port->lagged;
 -	if (params->lag)
 -		params->lag_id = mlxsw_sp_port->lag_id;
 -	else
 -		params->system_port = mlxsw_sp_port->local_port;
 +	switch (fib_node->fib->proto) {
 +	case MLXSW_SP_L3_PROTO_IPV4:
 +		mlxsw_sp_fib4_node_flush(mlxsw_sp, fib_node);
 +		break;
 +	case MLXSW_SP_L3_PROTO_IPV6:
 +		WARN_ON_ONCE(1);
 +		break;
 +	}
  }
  
 -static int
 -mlxsw_sp_port_vlan_router_join(struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan,
 -			       struct net_device *l3_dev)
 +static void mlxsw_sp_vr_fib_flush(struct mlxsw_sp *mlxsw_sp,
 +				  struct mlxsw_sp_vr *vr,
 +				  enum mlxsw_sp_l3proto proto)
  {
 -	struct mlxsw_sp_port *mlxsw_sp_port = mlxsw_sp_port_vlan->mlxsw_sp_port;
 -	struct mlxsw_sp *mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 -	u16 vid = mlxsw_sp_port_vlan->vid;
 -	struct mlxsw_sp_rif *rif;
 -	struct mlxsw_sp_fid *fid;
 -	int err;
 +	struct mlxsw_sp_fib *fib = mlxsw_sp_vr_fib(vr, proto);
 +	struct mlxsw_sp_fib_node *fib_node, *tmp;
  
 -	rif = mlxsw_sp_rif_find_by_dev(mlxsw_sp, l3_dev);
 -	if (!rif) {
 -		struct mlxsw_sp_rif_params params = {
 -			.dev = l3_dev,
 -		};
 +	list_for_each_entry_safe(fib_node, tmp, &fib->node_list, list) {
 +		bool do_break = &tmp->list == &fib->node_list;
  
 -		mlxsw_sp_rif_subport_params_init(&params, mlxsw_sp_port_vlan);
 -		rif = mlxsw_sp_rif_create(mlxsw_sp, &params);
 -		if (IS_ERR(rif))
 -			return PTR_ERR(rif);
 +		mlxsw_sp_fib_node_flush(mlxsw_sp, fib_node);
 +		if (do_break)
 +			break;
  	}
 +}
  
 -	/* FID was already created, just take a reference */
 -	fid = rif->ops->fid_get(rif);
 -	err = mlxsw_sp_fid_port_vid_map(fid, mlxsw_sp_port, vid);
 -	if (err)
 -		goto err_fid_port_vid_map;
 +static void mlxsw_sp_router_fib_flush(struct mlxsw_sp *mlxsw_sp)
 +{
 +	int i;
 +
 +	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 +		struct mlxsw_sp_vr *vr = &mlxsw_sp->router.vrs[i];
 +
 +		if (!mlxsw_sp_vr_is_used(vr))
 +			continue;
 +		mlxsw_sp_vr_fib_flush(mlxsw_sp, vr, MLXSW_SP_L3_PROTO_IPV4);
 +	}
 +}
  
 -	err = mlxsw_sp_port_vid_learning_set(mlxsw_sp_port, vid, false);
 -	if (err)
 -		goto err_port_vid_learning_set;
 +static void mlxsw_sp_router_fib4_abort(struct mlxsw_sp *mlxsw_sp)
 +{
 +	int err;
  
 -	err = mlxsw_sp_port_vid_stp_set(mlxsw_sp_port, vid,
 -					BR_STATE_FORWARDING);
 +	if (mlxsw_sp->router.aborted)
 +		return;
 +	dev_warn(mlxsw_sp->bus_info->dev, "FIB abort triggered. Note that FIB entries are no longer being offloaded to this device.\n");
 +	mlxsw_sp_router_fib_flush(mlxsw_sp);
 +	mlxsw_sp->router.aborted = true;
 +	err = mlxsw_sp_router_set_abort_trap(mlxsw_sp);
  	if (err)
 -		goto err_port_vid_stp_set;
 +		dev_warn(mlxsw_sp->bus_info->dev, "Failed to set abort trap.\n");
 +}
  
 -	mlxsw_sp_port_vlan->fid = fid;
 +struct mlxsw_sp_fib_event_work {
 +	struct work_struct work;
 +	union {
 +		struct fib_entry_notifier_info fen_info;
 +		struct fib_nh_notifier_info fnh_info;
 +	};
 +	struct mlxsw_sp *mlxsw_sp;
 +	unsigned long event;
 +};
  
 -	return 0;
 +static void mlxsw_sp_router_fib_event_work(struct work_struct *work)
 +{
 +	struct mlxsw_sp_fib_event_work *fib_work =
 +		container_of(work, struct mlxsw_sp_fib_event_work, work);
 +	struct mlxsw_sp *mlxsw_sp = fib_work->mlxsw_sp;
 +	bool replace, append;
 +	int err;
  
 -err_port_vid_stp_set:
 -	mlxsw_sp_port_vid_learning_set(mlxsw_sp_port, vid, true);
 -err_port_vid_learning_set:
 -	mlxsw_sp_fid_port_vid_unmap(fid, mlxsw_sp_port, vid);
 -err_fid_port_vid_map:
 -	mlxsw_sp_fid_put(fid);
 -	return err;
 +	/* Protect internal structures from changes */
 +	rtnl_lock();
 +	switch (fib_work->event) {
 +	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 +	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 +	case FIB_EVENT_ENTRY_ADD:
 +		replace = fib_work->event == FIB_EVENT_ENTRY_REPLACE;
 +		append = fib_work->event == FIB_EVENT_ENTRY_APPEND;
 +		err = mlxsw_sp_router_fib4_add(mlxsw_sp, &fib_work->fen_info,
 +					       replace, append);
 +		if (err)
 +			mlxsw_sp_router_fib4_abort(mlxsw_sp);
 +		fib_info_put(fib_work->fen_info.fi);
 +		break;
 +	case FIB_EVENT_ENTRY_DEL:
 +		mlxsw_sp_router_fib4_del(mlxsw_sp, &fib_work->fen_info);
 +		fib_info_put(fib_work->fen_info.fi);
 +		break;
 +	case FIB_EVENT_RULE_ADD: /* fall through */
 +	case FIB_EVENT_RULE_DEL:
 +		mlxsw_sp_router_fib4_abort(mlxsw_sp);
 +		break;
 +	case FIB_EVENT_NH_ADD: /* fall through */
 +	case FIB_EVENT_NH_DEL:
 +		mlxsw_sp_nexthop_event(mlxsw_sp, fib_work->event,
 +				       fib_work->fnh_info.fib_nh);
 +		fib_info_put(fib_work->fnh_info.fib_nh->nh_parent);
 +		break;
 +	}
 +	rtnl_unlock();
 +	kfree(fib_work);
  }
  
 -void
 -mlxsw_sp_port_vlan_router_leave(struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan)
 +/* Called with rcu_read_lock() */
 +static int mlxsw_sp_router_fib_event(struct notifier_block *nb,
 +				     unsigned long event, void *ptr)
  {
 -	struct mlxsw_sp_port *mlxsw_sp_port = mlxsw_sp_port_vlan->mlxsw_sp_port;
 -	struct mlxsw_sp_fid *fid = mlxsw_sp_port_vlan->fid;
 -	u16 vid = mlxsw_sp_port_vlan->vid;
 -
 -	if (WARN_ON(mlxsw_sp_fid_type(fid) != MLXSW_SP_FID_TYPE_RFID))
 -		return;
 +	struct mlxsw_sp *mlxsw_sp = container_of(nb, struct mlxsw_sp, fib_nb);
 +	struct mlxsw_sp_fib_event_work *fib_work;
 +	struct fib_notifier_info *info = ptr;
  
 -	mlxsw_sp_port_vlan->fid = NULL;
 -	mlxsw_sp_port_vid_stp_set(mlxsw_sp_port, vid, BR_STATE_BLOCKING);
 -	mlxsw_sp_port_vid_learning_set(mlxsw_sp_port, vid, true);
 -	mlxsw_sp_fid_port_vid_unmap(fid, mlxsw_sp_port, vid);
 -	/* If router port holds the last reference on the rFID, then the
 -	 * associated Sub-port RIF will be destroyed.
 -	 */
 -	mlxsw_sp_fid_put(fid);
 -}
 +	if (!net_eq(info->net, &init_net) ||
 +	    (info->family != AF_INET && info->family != AF_INET6))
 +		return NOTIFY_DONE;
  
 -static int mlxsw_sp_inetaddr_port_vlan_event(struct net_device *l3_dev,
 -					     struct net_device *port_dev,
 -					     unsigned long event, u16 vid)
 -{
 -	struct mlxsw_sp_port *mlxsw_sp_port = netdev_priv(port_dev);
 -	struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan;
 +	fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
 +	if (WARN_ON(!fib_work))
 +		return NOTIFY_BAD;
  
 -	mlxsw_sp_port_vlan = mlxsw_sp_port_vlan_find_by_vid(mlxsw_sp_port, vid);
 -	if (WARN_ON(!mlxsw_sp_port_vlan))
 -		return -EINVAL;
 +	INIT_WORK(&fib_work->work, mlxsw_sp_router_fib_event_work);
 +	fib_work->mlxsw_sp = mlxsw_sp;
 +	fib_work->event = event;
  
  	switch (event) {
 -	case NETDEV_UP:
 -		return mlxsw_sp_port_vlan_router_join(mlxsw_sp_port_vlan,
 -						      l3_dev);
 -	case NETDEV_DOWN:
 -		mlxsw_sp_port_vlan_router_leave(mlxsw_sp_port_vlan);
 +	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 +	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 +	case FIB_EVENT_ENTRY_ADD: /* fall through */
 +	case FIB_EVENT_ENTRY_DEL:
 +		memcpy(&fib_work->fen_info, ptr, sizeof(fib_work->fen_info));
 +		/* Take referece on fib_info to prevent it from being
 +		 * freed while work is queued. Release it afterwards.
 +		 */
 +		fib_info_hold(fib_work->fen_info.fi);
 +		break;
 +	case FIB_EVENT_NH_ADD: /* fall through */
 +	case FIB_EVENT_NH_DEL:
 +		memcpy(&fib_work->fnh_info, ptr, sizeof(fib_work->fnh_info));
 +		fib_info_hold(fib_work->fnh_info.fib_nh->nh_parent);
  		break;
  	}
  
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
