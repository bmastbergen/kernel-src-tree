net: Fix skb consume leak in sch_handle_egress

jira LE-1907
Rebuild_History Non-Buildable kernel-5.14.0-427.18.1.el9_4
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 28d18b673ffa2d13112ddb6e4c32c60d9b0cda50
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-427.18.1.el9_4/28d18b67.failed

Fix a memory leak for the tc egress path with TC_ACT_{STOLEN,QUEUED,TRAP}:

  [...]
  unreferenced object 0xffff88818bcb4f00 (size 232):
  comm "softirq", pid 0, jiffies 4299085078 (age 134.028s)
  hex dump (first 32 bytes):
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
    00 80 70 61 81 88 ff ff 00 41 31 14 81 88 ff ff  ..pa.....A1.....
  backtrace:
    [<ffffffff9991b938>] kmem_cache_alloc_node+0x268/0x400
    [<ffffffff9b3d9231>] __alloc_skb+0x211/0x2c0
    [<ffffffff9b3f0c7e>] alloc_skb_with_frags+0xbe/0x6b0
    [<ffffffff9b3bf9a9>] sock_alloc_send_pskb+0x6a9/0x870
    [<ffffffff9b6b3f00>] __ip_append_data+0x14d0/0x3bf0
    [<ffffffff9b6ba24e>] ip_append_data+0xee/0x190
    [<ffffffff9b7e1496>] icmp_push_reply+0xa6/0x470
    [<ffffffff9b7e4030>] icmp_reply+0x900/0xa00
    [<ffffffff9b7e42e3>] icmp_echo.part.0+0x1a3/0x230
    [<ffffffff9b7e444d>] icmp_echo+0xcd/0x190
    [<ffffffff9b7e9566>] icmp_rcv+0x806/0xe10
    [<ffffffff9b699bd1>] ip_protocol_deliver_rcu+0x351/0x3d0
    [<ffffffff9b699f14>] ip_local_deliver_finish+0x2b4/0x450
    [<ffffffff9b69a234>] ip_local_deliver+0x174/0x1f0
    [<ffffffff9b69a4b2>] ip_sublist_rcv_finish+0x1f2/0x420
    [<ffffffff9b69ab56>] ip_sublist_rcv+0x466/0x920
  [...]

I was able to reproduce this via:

  ip link add dev dummy0 type dummy
  ip link set dev dummy0 up
  tc qdisc add dev eth0 clsact
  tc filter add dev eth0 egress protocol ip prio 1 u32 match ip protocol 1 0xff action mirred egress redirect dev dummy0
  ping 1.1.1.1
  <stolen>

After the fix, there are no kmemleak reports with the reproducer. This is
in line with what is also done on the ingress side, and from debugging the
skb_unref(skb) on dummy xmit and sch_handle_egress() side, it is visible
that these are two different skbs with both skb_unref(skb) as true. The two
seen skbs are due to mirred doing a skb_clone() internally as use_reinsert
is false in tcf_mirred_act() for egress. This was initially reported by Gal.

Fixes: e420bed02507 ("bpf: Add fd-based tcx multi-prog infra with link support")
	Reported-by: Gal Pressman <gal@nvidia.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
Link: https://lore.kernel.org/bpf/bdfc2640-8f65-5b56-4472-db8e2b161aab@nvidia.com
	Reviewed-by: Simon Horman <horms@kernel.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 28d18b673ffa2d13112ddb6e4c32c60d9b0cda50)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/dev.c
diff --cc net/core/dev.c
index 6439f9b64980,9f6ed6d97f89..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -3897,6 -3909,180 +3897,183 @@@ void netdev_xmit_skip_txqueue(bool skip
  EXPORT_SYMBOL_GPL(netdev_xmit_skip_txqueue);
  #endif /* CONFIG_NET_EGRESS */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_XGRESS
+ static int tc_run(struct tcx_entry *entry, struct sk_buff *skb)
+ {
+ 	int ret = TC_ACT_UNSPEC;
+ #ifdef CONFIG_NET_CLS_ACT
+ 	struct mini_Qdisc *miniq = rcu_dereference_bh(entry->miniq);
+ 	struct tcf_result res;
+ 
+ 	if (!miniq)
+ 		return ret;
+ 
+ 	tc_skb_cb(skb)->mru = 0;
+ 	tc_skb_cb(skb)->post_ct = false;
+ 
+ 	mini_qdisc_bstats_cpu_update(miniq, skb);
+ 	ret = tcf_classify(skb, miniq->block, miniq->filter_list, &res, false);
+ 	/* Only tcf related quirks below. */
+ 	switch (ret) {
+ 	case TC_ACT_SHOT:
+ 		mini_qdisc_qstats_cpu_drop(miniq);
+ 		break;
+ 	case TC_ACT_OK:
+ 	case TC_ACT_RECLASSIFY:
+ 		skb->tc_index = TC_H_MIN(res.classid);
+ 		break;
+ 	}
+ #endif /* CONFIG_NET_CLS_ACT */
+ 	return ret;
+ }
+ 
+ static DEFINE_STATIC_KEY_FALSE(tcx_needed_key);
+ 
+ void tcx_inc(void)
+ {
+ 	static_branch_inc(&tcx_needed_key);
+ }
+ 
+ void tcx_dec(void)
+ {
+ 	static_branch_dec(&tcx_needed_key);
+ }
+ 
+ static __always_inline enum tcx_action_base
+ tcx_run(const struct bpf_mprog_entry *entry, struct sk_buff *skb,
+ 	const bool needs_mac)
+ {
+ 	const struct bpf_mprog_fp *fp;
+ 	const struct bpf_prog *prog;
+ 	int ret = TCX_NEXT;
+ 
+ 	if (needs_mac)
+ 		__skb_push(skb, skb->mac_len);
+ 	bpf_mprog_foreach_prog(entry, fp, prog) {
+ 		bpf_compute_data_pointers(skb);
+ 		ret = bpf_prog_run(prog, skb);
+ 		if (ret != TCX_NEXT)
+ 			break;
+ 	}
+ 	if (needs_mac)
+ 		__skb_pull(skb, skb->mac_len);
+ 	return tcx_action_code(skb, ret);
+ }
+ 
+ static __always_inline struct sk_buff *
+ sch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,
+ 		   struct net_device *orig_dev, bool *another)
+ {
+ 	struct bpf_mprog_entry *entry = rcu_dereference_bh(skb->dev->tcx_ingress);
+ 	int sch_ret;
+ 
+ 	if (!entry)
+ 		return skb;
+ 	if (*pt_prev) {
+ 		*ret = deliver_skb(skb, *pt_prev, orig_dev);
+ 		*pt_prev = NULL;
+ 	}
+ 
+ 	qdisc_skb_cb(skb)->pkt_len = skb->len;
+ 	tcx_set_ingress(skb, true);
+ 
+ 	if (static_branch_unlikely(&tcx_needed_key)) {
+ 		sch_ret = tcx_run(entry, skb, true);
+ 		if (sch_ret != TC_ACT_UNSPEC)
+ 			goto ingress_verdict;
+ 	}
+ 	sch_ret = tc_run(tcx_entry(entry), skb);
+ ingress_verdict:
+ 	switch (sch_ret) {
+ 	case TC_ACT_REDIRECT:
+ 		/* skb_mac_header check was done by BPF, so we can safely
+ 		 * push the L2 header back before redirecting to another
+ 		 * netdev.
+ 		 */
+ 		__skb_push(skb, skb->mac_len);
+ 		if (skb_do_redirect(skb) == -EAGAIN) {
+ 			__skb_pull(skb, skb->mac_len);
+ 			*another = true;
+ 			break;
+ 		}
+ 		*ret = NET_RX_SUCCESS;
+ 		return NULL;
+ 	case TC_ACT_SHOT:
+ 		kfree_skb_reason(skb, SKB_DROP_REASON_TC_INGRESS);
+ 		*ret = NET_RX_DROP;
+ 		return NULL;
+ 	/* used by tc_run */
+ 	case TC_ACT_STOLEN:
+ 	case TC_ACT_QUEUED:
+ 	case TC_ACT_TRAP:
+ 		consume_skb(skb);
+ 		fallthrough;
+ 	case TC_ACT_CONSUMED:
+ 		*ret = NET_RX_SUCCESS;
+ 		return NULL;
+ 	}
+ 
+ 	return skb;
+ }
+ 
+ static __always_inline struct sk_buff *
+ sch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)
+ {
+ 	struct bpf_mprog_entry *entry = rcu_dereference_bh(dev->tcx_egress);
+ 	int sch_ret;
+ 
+ 	if (!entry)
+ 		return skb;
+ 
+ 	/* qdisc_skb_cb(skb)->pkt_len & tcx_set_ingress() was
+ 	 * already set by the caller.
+ 	 */
+ 	if (static_branch_unlikely(&tcx_needed_key)) {
+ 		sch_ret = tcx_run(entry, skb, false);
+ 		if (sch_ret != TC_ACT_UNSPEC)
+ 			goto egress_verdict;
+ 	}
+ 	sch_ret = tc_run(tcx_entry(entry), skb);
+ egress_verdict:
+ 	switch (sch_ret) {
+ 	case TC_ACT_REDIRECT:
+ 		/* No need to push/pop skb's mac_header here on egress! */
+ 		skb_do_redirect(skb);
+ 		*ret = NET_XMIT_SUCCESS;
+ 		return NULL;
+ 	case TC_ACT_SHOT:
+ 		kfree_skb_reason(skb, SKB_DROP_REASON_TC_EGRESS);
+ 		*ret = NET_XMIT_DROP;
+ 		return NULL;
+ 	/* used by tc_run */
+ 	case TC_ACT_STOLEN:
+ 	case TC_ACT_QUEUED:
+ 	case TC_ACT_TRAP:
+ 		consume_skb(skb);
+ 		*ret = NET_XMIT_SUCCESS;
+ 		return NULL;
+ 	}
+ 
+ 	return skb;
+ }
+ #else
+ static __always_inline struct sk_buff *
+ sch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,
+ 		   struct net_device *orig_dev, bool *another)
+ {
+ 	return skb;
+ }
+ 
+ static __always_inline struct sk_buff *
+ sch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)
+ {
+ 	return skb;
+ }
+ #endif /* CONFIG_NET_XGRESS */
+ 
++>>>>>>> 28d18b673ffa (net: Fix skb consume leak in sch_handle_egress)
  #ifdef CONFIG_XPS
  static int __get_xps_queue_idx(struct net_device *dev, struct sk_buff *skb,
  			       struct xps_dev_maps *dev_maps, unsigned int tci)
* Unmerged path net/core/dev.c
