io_uring: Ensure mask is initialized in io_arm_poll_handler

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Nathan Chancellor <natechancellor@gmail.com>
commit 8755d97a09fed0de206772bcad1838301293c4d8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/8755d97a.failed

Clang warns:

fs/io_uring.c:4178:6: warning: variable 'mask' is used uninitialized
whenever 'if' condition is false [-Wsometimes-uninitialized]
        if (def->pollin)
            ^~~~~~~~~~~
fs/io_uring.c:4182:2: note: uninitialized use occurs here
        mask |= POLLERR | POLLPRI;
        ^~~~
fs/io_uring.c:4178:2: note: remove the 'if' if its condition is always
true
        if (def->pollin)
        ^~~~~~~~~~~~~~~~
fs/io_uring.c:4154:15: note: initialize the variable 'mask' to silence
this warning
        __poll_t mask, ret;
                     ^
                      = 0
1 warning generated.

io_op_defs has many definitions where pollin is not set so mask indeed
might be uninitialized. Initialize it to zero and change the next
assignment to |=, in case further masks are added in the future to avoid
missing changing the assignment then.

Fixes: d7718a9d25a6 ("io_uring: use poll driven retry for files that support it")
Link: https://github.com/ClangBuiltLinux/linux/issues/916
	Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 8755d97a09fed0de206772bcad1838301293c4d8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,e92b88455e5e..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1758,105 -3568,871 +1758,461 @@@ struct io_poll_table 
  	int error;
  };
  
++<<<<<<< HEAD
++=======
+ static void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,
+ 			    struct wait_queue_head *head)
+ {
+ 	if (unlikely(poll->head)) {
+ 		pt->error = -EINVAL;
+ 		return;
+ 	}
+ 
+ 	pt->error = 0;
+ 	poll->head = head;
+ 	add_wait_queue(head, &poll->wait);
+ }
+ 
+ static void io_async_queue_proc(struct file *file, struct wait_queue_head *head,
+ 			       struct poll_table_struct *p)
+ {
+ 	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
+ 
+ 	__io_queue_proc(&pt->req->apoll->poll, pt, head);
+ }
+ 
+ static int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,
+ 			   __poll_t mask, task_work_func_t func)
+ {
+ 	struct task_struct *tsk;
+ 
+ 	/* for instances that support it check for an event match first: */
+ 	if (mask && !(mask & poll->events))
+ 		return 0;
+ 
+ 	trace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);
+ 
+ 	list_del_init(&poll->wait.entry);
+ 
+ 	tsk = req->task;
+ 	req->result = mask;
+ 	init_task_work(&req->task_work, func);
+ 	/*
+ 	 * If this fails, then the task is exiting. If that is the case, then
+ 	 * the exit check will ultimately cancel these work items. Hence we
+ 	 * don't need to check here and handle it specifically.
+ 	 */
+ 	task_work_add(tsk, &req->task_work, true);
+ 	wake_up_process(tsk);
+ 	return 1;
+ }
+ 
+ static void io_async_task_func(struct callback_head *cb)
+ {
+ 	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
+ 	struct async_poll *apoll = req->apoll;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	trace_io_uring_task_run(req->ctx, req->opcode, req->user_data);
+ 
+ 	WARN_ON_ONCE(!list_empty(&req->apoll->poll.wait.entry));
+ 
+ 	if (hash_hashed(&req->hash_node)) {
+ 		spin_lock_irq(&ctx->completion_lock);
+ 		hash_del(&req->hash_node);
+ 		spin_unlock_irq(&ctx->completion_lock);
+ 	}
+ 
+ 	/* restore ->work in case we need to retry again */
+ 	memcpy(&req->work, &apoll->work, sizeof(req->work));
+ 
+ 	__set_current_state(TASK_RUNNING);
+ 	mutex_lock(&ctx->uring_lock);
+ 	__io_queue_sqe(req, NULL);
+ 	mutex_unlock(&ctx->uring_lock);
+ 
+ 	kfree(apoll);
+ }
+ 
+ static int io_async_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
+ 			void *key)
+ {
+ 	struct io_kiocb *req = wait->private;
+ 	struct io_poll_iocb *poll = &req->apoll->poll;
+ 
+ 	trace_io_uring_poll_wake(req->ctx, req->opcode, req->user_data,
+ 					key_to_poll(key));
+ 
+ 	return __io_async_wake(req, poll, key_to_poll(key), io_async_task_func);
+ }
+ 
+ static void io_poll_req_insert(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct hlist_head *list;
+ 
+ 	list = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];
+ 	hlist_add_head(&req->hash_node, list);
+ }
+ 
+ static __poll_t __io_arm_poll_handler(struct io_kiocb *req,
+ 				      struct io_poll_iocb *poll,
+ 				      struct io_poll_table *ipt, __poll_t mask,
+ 				      wait_queue_func_t wake_func)
+ 	__acquires(&ctx->completion_lock)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	bool cancel = false;
+ 
+ 	poll->file = req->file;
+ 	poll->head = NULL;
+ 	poll->done = poll->canceled = false;
+ 	poll->events = mask;
+ 
+ 	ipt->pt._key = mask;
+ 	ipt->req = req;
+ 	ipt->error = -EINVAL;
+ 
+ 	INIT_LIST_HEAD(&poll->wait.entry);
+ 	init_waitqueue_func_entry(&poll->wait, wake_func);
+ 	poll->wait.private = req;
+ 
+ 	mask = vfs_poll(req->file, &ipt->pt) & poll->events;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (likely(poll->head)) {
+ 		spin_lock(&poll->head->lock);
+ 		if (unlikely(list_empty(&poll->wait.entry))) {
+ 			if (ipt->error)
+ 				cancel = true;
+ 			ipt->error = 0;
+ 			mask = 0;
+ 		}
+ 		if (mask || ipt->error)
+ 			list_del_init(&poll->wait.entry);
+ 		else if (cancel)
+ 			WRITE_ONCE(poll->canceled, true);
+ 		else if (!poll->done) /* actually waiting for an event */
+ 			io_poll_req_insert(req);
+ 		spin_unlock(&poll->head->lock);
+ 	}
+ 
+ 	return mask;
+ }
+ 
+ static bool io_arm_poll_handler(struct io_kiocb *req)
+ {
+ 	const struct io_op_def *def = &io_op_defs[req->opcode];
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct async_poll *apoll;
+ 	struct io_poll_table ipt;
+ 	__poll_t mask, ret;
+ 
+ 	if (!req->file || !file_can_poll(req->file))
+ 		return false;
+ 	if (req->flags & (REQ_F_MUST_PUNT | REQ_F_POLLED))
+ 		return false;
+ 	if (!def->pollin && !def->pollout)
+ 		return false;
+ 
+ 	apoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);
+ 	if (unlikely(!apoll))
+ 		return false;
+ 
+ 	req->flags |= REQ_F_POLLED;
+ 	memcpy(&apoll->work, &req->work, sizeof(req->work));
+ 
+ 	/*
+ 	 * Don't need a reference here, as we're adding it to the task
+ 	 * task_works list. If the task exits, the list is pruned.
+ 	 */
+ 	req->task = current;
+ 	req->apoll = apoll;
+ 	INIT_HLIST_NODE(&req->hash_node);
+ 
+ 	mask = 0;
+ 	if (def->pollin)
+ 		mask |= POLLIN | POLLRDNORM;
+ 	if (def->pollout)
+ 		mask |= POLLOUT | POLLWRNORM;
+ 	mask |= POLLERR | POLLPRI;
+ 
+ 	ipt.pt._qproc = io_async_queue_proc;
+ 
+ 	ret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,
+ 					io_async_wake);
+ 	if (ret) {
+ 		ipt.error = 0;
+ 		apoll->poll.done = true;
+ 		spin_unlock_irq(&ctx->completion_lock);
+ 		memcpy(&req->work, &apoll->work, sizeof(req->work));
+ 		kfree(apoll);
+ 		return false;
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	trace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,
+ 					apoll->poll.events);
+ 	return true;
+ }
+ 
+ static bool __io_poll_remove_one(struct io_kiocb *req,
+ 				 struct io_poll_iocb *poll)
+ {
+ 	bool do_complete = false;
+ 
+ 	spin_lock(&poll->head->lock);
+ 	WRITE_ONCE(poll->canceled, true);
+ 	if (!list_empty(&poll->wait.entry)) {
+ 		list_del_init(&poll->wait.entry);
+ 		do_complete = true;
+ 	}
+ 	spin_unlock(&poll->head->lock);
+ 	return do_complete;
+ }
+ 
+ static bool io_poll_remove_one(struct io_kiocb *req)
+ {
+ 	bool do_complete;
+ 
+ 	if (req->opcode == IORING_OP_POLL_ADD) {
+ 		do_complete = __io_poll_remove_one(req, &req->poll);
+ 	} else {
+ 		/* non-poll requests have submit ref still */
+ 		do_complete = __io_poll_remove_one(req, &req->apoll->poll);
+ 		if (do_complete)
+ 			io_put_req(req);
+ 	}
+ 
+ 	hash_del(&req->hash_node);
+ 
+ 	if (do_complete) {
+ 		io_cqring_fill_event(req, -ECANCELED);
+ 		io_commit_cqring(req->ctx);
+ 		req->flags |= REQ_F_COMP_LOCKED;
+ 		io_put_req(req);
+ 	}
+ 
+ 	return do_complete;
+ }
+ 
+ static void io_poll_remove_all(struct io_ring_ctx *ctx)
+ {
+ 	struct hlist_node *tmp;
+ 	struct io_kiocb *req;
+ 	int i;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	for (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {
+ 		struct hlist_head *list;
+ 
+ 		list = &ctx->cancel_hash[i];
+ 		hlist_for_each_entry_safe(req, tmp, list, hash_node)
+ 			io_poll_remove_one(req);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	io_cqring_ev_posted(ctx);
+ }
+ 
+ static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)
+ {
+ 	struct hlist_head *list;
+ 	struct io_kiocb *req;
+ 
+ 	list = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];
+ 	hlist_for_each_entry(req, list, hash_node) {
+ 		if (sqe_addr != req->user_data)
+ 			continue;
+ 		if (io_poll_remove_one(req))
+ 			return 0;
+ 		return -EALREADY;
+ 	}
+ 
+ 	return -ENOENT;
+ }
+ 
+ static int io_poll_remove_prep(struct io_kiocb *req,
+ 			       const struct io_uring_sqe *sqe)
+ {
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
+ 	    sqe->poll_events)
+ 		return -EINVAL;
+ 
+ 	req->poll.addr = READ_ONCE(sqe->addr);
+ 	return 0;
+ }
+ 
+ /*
+  * Find a running poll command that matches one specified in sqe->addr,
+  * and remove it if found.
+  */
+ static int io_poll_remove(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	u64 addr;
+ 	int ret;
+ 
+ 	addr = req->poll.addr;
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	ret = io_poll_cancel(ctx, addr);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static void io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	req->poll.done = true;
+ 	io_cqring_fill_event(req, error ? error : mangle_poll(mask));
+ 	io_commit_cqring(ctx);
+ }
+ 
 -static void io_poll_task_handler(struct io_kiocb *req, struct io_kiocb **nxt)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	hash_del(&req->hash_node);
 -	io_poll_complete(req, req->result, 0);
 -	req->flags |= REQ_F_COMP_LOCKED;
 -	io_put_req_find_next(req, nxt);
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	io_cqring_ev_posted(ctx);
 -}
 -
 -static void io_poll_task_func(struct callback_head *cb)
 -{
 -	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
 -	struct io_kiocb *nxt = NULL;
 -
 -	io_poll_task_handler(req, &nxt);
 -	if (nxt) {
 -		struct io_ring_ctx *ctx = nxt->ctx;
 -
 -		mutex_lock(&ctx->uring_lock);
 -		__io_queue_sqe(nxt, NULL);
 -		mutex_unlock(&ctx->uring_lock);
 -	}
 -}
 -
 -static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 -			void *key)
 -{
 -	struct io_kiocb *req = wait->private;
 -	struct io_poll_iocb *poll = &req->poll;
 -
 -	return __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);
 -}
 -
 -static void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,
 -			       struct poll_table_struct *p)
 -{
 -	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
 -
 -	__io_queue_proc(&pt->req->poll, pt, head);
 -}
 -
 -static int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	struct io_poll_iocb *poll = &req->poll;
 -	u16 events;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -	if (!poll->file)
 -		return -EBADF;
 -
 -	events = READ_ONCE(sqe->poll_events);
 -	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
 -
 -	/*
 -	 * Don't need a reference here, as we're adding it to the task
 -	 * task_works list. If the task exits, the list is pruned.
 -	 */
 -	req->task = current;
 -	return 0;
 -}
 -
 -static int io_poll_add(struct io_kiocb *req, struct io_kiocb **nxt)
 -{
 -	struct io_poll_iocb *poll = &req->poll;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_poll_table ipt;
 -	__poll_t mask;
 -
 -	INIT_HLIST_NODE(&req->hash_node);
 -	INIT_LIST_HEAD(&req->list);
 -	ipt.pt._qproc = io_poll_queue_proc;
 -
 -	mask = __io_arm_poll_handler(req, &req->poll, &ipt, poll->events,
 -					io_poll_wake);
 -
 -	if (mask) { /* no async, we'd stolen it */
 -		ipt.error = 0;
 -		io_poll_complete(req, mask, 0);
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	if (mask) {
 -		io_cqring_ev_posted(ctx);
 -		io_put_req_find_next(req, nxt);
 -	}
 -	return ipt.error;
 -}
 -
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 -{
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 -
 -	atomic_inc(&ctx->cq_timeouts);
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -	/*
 -	 * We could be racing with timeout deletion. If the list is empty,
 -	 * then timeout lookup already found it and will be handling it.
 -	 */
 -	if (!list_empty(&req->list)) {
 -		struct io_kiocb *prev;
 -
 -		/*
 -		 * Adjust the reqs sequence before the current one because it
 -		 * will consume a slot in the cq_ring and the cq_tail
 -		 * pointer will be increased, otherwise other timeout reqs may
 -		 * return in advance without waiting for enough wait_nr.
 -		 */
 -		prev = req;
 -		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
 -			prev->sequence++;
 -		list_del_init(&req->list);
 -	}
 -
 -	io_cqring_fill_event(req, -ETIME);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -
 -	io_cqring_ev_posted(ctx);
 -	req_set_fail_links(req);
 -	io_put_req(req);
 -	return HRTIMER_NORESTART;
 -}
 -
 -static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
 -{
 -	struct io_kiocb *req;
 -	int ret = -ENOENT;
 -
 -	list_for_each_entry(req, &ctx->timeout_list, list) {
 -		if (user_data == req->user_data) {
 -			list_del_init(&req->list);
 -			ret = 0;
 -			break;
 -		}
 -	}
 -
 -	if (ret == -ENOENT)
 -		return ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
 -	if (ret == -1)
 -		return -EALREADY;
 -
 -	req_set_fail_links(req);
 -	io_cqring_fill_event(req, -ECANCELED);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_timeout_remove_prep(struct io_kiocb *req,
 -				  const struct io_uring_sqe *sqe)
 -{
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
 -		return -EINVAL;
 -
 -	req->timeout.addr = READ_ONCE(sqe->addr);
 -	req->timeout.flags = READ_ONCE(sqe->timeout_flags);
 -	if (req->timeout.flags)
 -		return -EINVAL;
 -
 -	return 0;
 -}
 -
 -/*
 - * Remove or update an existing timeout command
 - */
 -static int io_timeout_remove(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	ret = io_timeout_cancel(ctx, req->timeout.addr);
 -
 -	io_cqring_fill_event(req, ret);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irq(&ctx->completion_lock);
 -	io_cqring_ev_posted(ctx);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			   bool is_timeout_link)
 -{
 -	struct io_timeout_data *data;
 -	unsigned flags;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
 -		return -EINVAL;
 -	if (sqe->off && is_timeout_link)
 -		return -EINVAL;
 -	flags = READ_ONCE(sqe->timeout_flags);
 -	if (flags & ~IORING_TIMEOUT_ABS)
 -		return -EINVAL;
 -
 -	req->timeout.count = READ_ONCE(sqe->off);
 -
 -	if (!req->io && io_alloc_async_ctx(req))
 -		return -ENOMEM;
 -
 -	data = &req->io->timeout;
 -	data->req = req;
 -	req->flags |= REQ_F_TIMEOUT;
 -
 -	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
 -		return -EFAULT;
 -
 -	if (flags & IORING_TIMEOUT_ABS)
 -		data->mode = HRTIMER_MODE_ABS;
 -	else
 -		data->mode = HRTIMER_MODE_REL;
 -
 -	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
 -	return 0;
 -}
 -
 -static int io_timeout(struct io_kiocb *req)
 -{
 -	unsigned count;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_timeout_data *data;
 -	struct list_head *entry;
 -	unsigned span = 0;
 -
 -	data = &req->io->timeout;
 -
 -	/*
 -	 * sqe->off holds how many events that need to occur for this
 -	 * timeout event to be satisfied. If it isn't set, then this is
 -	 * a pure timeout request, sequence isn't used.
 -	 */
 -	count = req->timeout.count;
 -	if (!count) {
 -		req->flags |= REQ_F_TIMEOUT_NOSEQ;
 -		spin_lock_irq(&ctx->completion_lock);
 -		entry = ctx->timeout_list.prev;
 -		goto add;
 -	}
 -
 -	req->sequence = ctx->cached_sq_head + count - 1;
 -	data->seq_offset = count;
 -
 -	/*
 -	 * Insertion sort, ensuring the first entry in the list is always
 -	 * the one we need first.
 -	 */
 -	spin_lock_irq(&ctx->completion_lock);
 -	list_for_each_prev(entry, &ctx->timeout_list) {
 -		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
 -		unsigned nxt_sq_head;
 -		long long tmp, tmp_nxt;
 -		u32 nxt_offset = nxt->io->timeout.seq_offset;
 -
 -		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
 -			continue;
 -
 -		/*
 -		 * Since cached_sq_head + count - 1 can overflow, use type long
 -		 * long to store it.
 -		 */
 -		tmp = (long long)ctx->cached_sq_head + count - 1;
 -		nxt_sq_head = nxt->sequence - nxt_offset + 1;
 -		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
 -
 -		/*
 -		 * cached_sq_head may overflow, and it will never overflow twice
 -		 * once there is some timeout req still be valid.
 -		 */
 -		if (ctx->cached_sq_head < nxt_sq_head)
 -			tmp += UINT_MAX;
 -
 -		if (tmp > tmp_nxt)
 -			break;
 -
 -		/*
 -		 * Sequence of reqs after the insert one and itself should
 -		 * be adjusted because each timeout req consumes a slot.
 -		 */
 -		span++;
 -		nxt->sequence++;
 -	}
 -	req->sequence -= span;
 -add:
 -	list_add(&req->list, entry);
 -	data->timer.function = io_timeout_fn;
 -	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
 -	spin_unlock_irq(&ctx->completion_lock);
 -	return 0;
 -}
 -
 -static bool io_cancel_cb(struct io_wq_work *work, void *data)
 -{
 -	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 -
 -	return req->user_data == (unsigned long) data;
 -}
 -
 -static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
 -{
 -	enum io_wq_cancel cancel_ret;
 -	int ret = 0;
 -
 -	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
 -	switch (cancel_ret) {
 -	case IO_WQ_CANCEL_OK:
 -		ret = 0;
 -		break;
 -	case IO_WQ_CANCEL_RUNNING:
 -		ret = -EALREADY;
 -		break;
 -	case IO_WQ_CANCEL_NOTFOUND:
 -		ret = -ENOENT;
 -		break;
 -	}
 -
 -	return ret;
 -}
 -
 -static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
 -				     struct io_kiocb *req, __u64 sqe_addr,
 -				     struct io_kiocb **nxt, int success_ret)
++static void io_poll_task_handler(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
 -	unsigned long flags;
 -	int ret;
++	struct io_ring_ctx *ctx = req->ctx;
+ 
 -	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
 -	if (ret != -ENOENT) {
 -		spin_lock_irqsave(&ctx->completion_lock, flags);
 -		goto done;
 -	}
++	spin_lock_irq(&ctx->completion_lock);
++	hash_del(&req->hash_node);
++	io_poll_complete(req, req->result, 0);
++	req->flags |= REQ_F_COMP_LOCKED;
++	io_put_req_find_next(req, nxt);
++	spin_unlock_irq(&ctx->completion_lock);
+ 
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -	ret = io_timeout_cancel(ctx, sqe_addr);
 -	if (ret != -ENOENT)
 -		goto done;
 -	ret = io_poll_cancel(ctx, sqe_addr);
 -done:
 -	if (!ret)
 -		ret = success_ret;
 -	io_cqring_fill_event(req, ret);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
 -
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req_find_next(req, nxt);
+ }
+ 
 -static int io_async_cancel_prep(struct io_kiocb *req,
 -				const struct io_uring_sqe *sqe)
++static void io_poll_task_func(struct callback_head *cb)
+ {
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
 -	    sqe->cancel_flags)
 -		return -EINVAL;
++	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
++	struct io_kiocb *nxt = NULL;
+ 
 -	req->cancel.addr = READ_ONCE(sqe->addr);
 -	return 0;
++	io_poll_task_handler(req, &nxt);
++	if (nxt) {
++		struct io_ring_ctx *ctx = nxt->ctx;
++
++		mutex_lock(&ctx->uring_lock);
++		__io_queue_sqe(nxt, NULL);
++		mutex_unlock(&ctx->uring_lock);
++	}
+ }
+ 
 -static int io_async_cancel(struct io_kiocb *req, struct io_kiocb **nxt)
++static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
++			void *key)
+ {
 -	struct io_ring_ctx *ctx = req->ctx;
++	struct io_kiocb *req = wait->private;
++	struct io_poll_iocb *poll = &req->poll;
+ 
 -	io_async_find_and_cancel(ctx, req, req->cancel.addr, nxt, 0);
 -	return 0;
++	return __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);
+ }
+ 
 -static int io_files_update_prep(struct io_kiocb *req,
 -				const struct io_uring_sqe *sqe)
++>>>>>>> 8755d97a09fe (io_uring: Ensure mask is initialized in io_arm_poll_handler)
 +static void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,
 +			       struct poll_table_struct *p)
  {
 -	if (sqe->flags || sqe->ioprio || sqe->rw_flags)
 -		return -EINVAL;
 +	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
  
 -	req->files_update.offset = READ_ONCE(sqe->off);
 -	req->files_update.nr_args = READ_ONCE(sqe->len);
 -	if (!req->files_update.nr_args)
 -		return -EINVAL;
 -	req->files_update.arg = READ_ONCE(sqe->addr);
 -	return 0;
 +	if (unlikely(pt->req->poll.head)) {
 +		pt->error = -EINVAL;
 +		return;
 +	}
 +
 +	pt->error = 0;
 +	pt->req->poll.head = head;
 +	add_wait_queue(head, &pt->req->poll.wait);
  }
  
 -static int io_files_update(struct io_kiocb *req, bool force_nonblock)
 +static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 +	struct io_poll_iocb *poll = &req->poll;
  	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_uring_files_update up;
 -	int ret;
 +	struct io_poll_table ipt;
 +	bool cancel = false;
 +	__poll_t mask;
 +	u16 events;
  
 -	if (force_nonblock)
 -		return -EAGAIN;
 +	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 +		return -EINVAL;
 +	if (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)
 +		return -EINVAL;
 +	if (!poll->file)
 +		return -EBADF;
  
 -	up.offset = req->files_update.offset;
 -	up.fds = req->files_update.arg;
 +	req->submit.sqe = NULL;
 +	INIT_WORK(&req->work, io_poll_complete_work);
 +	events = READ_ONCE(sqe->poll_events);
 +	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
  
 -	mutex_lock(&ctx->uring_lock);
 -	ret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);
 -	mutex_unlock(&ctx->uring_lock);
 +	poll->head = NULL;
 +	poll->done = false;
 +	poll->canceled = false;
  
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 +	ipt.pt._qproc = io_poll_queue_proc;
 +	ipt.pt._key = poll->events;
 +	ipt.req = req;
 +	ipt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */
  
 -static int io_req_defer_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -	ssize_t ret = 0;
 +	/* initialized the list so that we can do list_empty checks */
 +	INIT_LIST_HEAD(&poll->wait.entry);
 +	init_waitqueue_func_entry(&poll->wait, io_poll_wake);
  
 -	if (io_op_defs[req->opcode].file_table) {
 -		ret = io_grab_files(req);
 -		if (unlikely(ret))
 -			return ret;
 -	}
 +	INIT_LIST_HEAD(&req->list);
  
 -	io_req_work_grab_env(req, &io_op_defs[req->opcode]);
 +	mask = vfs_poll(poll->file, &ipt.pt) & poll->events;
  
 -	switch (req->opcode) {
 -	case IORING_OP_NOP:
 -		break;
 -	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -	case IORING_OP_READ:
 -		ret = io_read_prep(req, sqe, true);
 -		break;
 -	case IORING_OP_WRITEV:
 -	case IORING_OP_WRITE_FIXED:
 -	case IORING_OP_WRITE:
 -		ret = io_write_prep(req, sqe, true);
 -		break;
 -	case IORING_OP_POLL_ADD:
 -		ret = io_poll_add_prep(req, sqe);
 -		break;
 -	case IORING_OP_POLL_REMOVE:
 -		ret = io_poll_remove_prep(req, sqe);
 -		break;
 -	case IORING_OP_FSYNC:
 -		ret = io_prep_fsync(req, sqe);
 -		break;
 -	case IORING_OP_SYNC_FILE_RANGE:
 -		ret = io_prep_sfr(req, sqe);
 -		break;
 -	case IORING_OP_SENDMSG:
 -	case IORING_OP_SEND:
 -		ret = io_sendmsg_prep(req, sqe);
 -		break;
 -	case IORING_OP_RECVMSG:
 -	case IORING_OP_RECV:
 -		ret = io_recvmsg_prep(req, sqe);
 -		break;
 -	case IORING_OP_CONNECT:
 -		ret = io_connect_prep(req, sqe);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		ret = io_timeout_prep(req, sqe, false);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		ret = io_timeout_remove_prep(req, sqe);
 -		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		ret = io_async_cancel_prep(req, sqe);
 -		break;
 -	case IORING_OP_LINK_TIMEOUT:
 -		ret = io_timeout_prep(req, sqe, true);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		ret = io_accept_prep(req, sqe);
 -		break;
 -	case IORING_OP_FALLOCATE:
 -		ret = io_fallocate_prep(req, sqe);
 -		break;
 -	case IORING_OP_OPENAT:
 -		ret = io_openat_prep(req, sqe);
 -		break;
 -	case IORING_OP_CLOSE:
 -		ret = io_close_prep(req, sqe);
 -		break;
 -	case IORING_OP_FILES_UPDATE:
 -		ret = io_files_update_prep(req, sqe);
 -		break;
 -	case IORING_OP_STATX:
 -		ret = io_statx_prep(req, sqe);
 -		break;
 -	case IORING_OP_FADVISE:
 -		ret = io_fadvise_prep(req, sqe);
 -		break;
 -	case IORING_OP_MADVISE:
 -		ret = io_madvise_prep(req, sqe);
 -		break;
 -	case IORING_OP_OPENAT2:
 -		ret = io_openat2_prep(req, sqe);
 -		break;
 -	case IORING_OP_EPOLL_CTL:
 -		ret = io_epoll_ctl_prep(req, sqe);
 -		break;
 -	case IORING_OP_SPLICE:
 -		ret = io_splice_prep(req, sqe);
 -		break;
 -	default:
 -		printk_once(KERN_WARNING "io_uring: unhandled opcode %d\n",
 -				req->opcode);
 -		ret = -EINVAL;
 -		break;
 +	spin_lock_irq(&ctx->completion_lock);
 +	if (likely(poll->head)) {
 +		spin_lock(&poll->head->lock);
 +		if (unlikely(list_empty(&poll->wait.entry))) {
 +			if (ipt.error)
 +				cancel = true;
 +			ipt.error = 0;
 +			mask = 0;
 +		}
 +		if (mask || ipt.error)
 +			list_del_init(&poll->wait.entry);
 +		else if (cancel)
 +			WRITE_ONCE(poll->canceled, true);
 +		else if (!poll->done) /* actually waiting for an event */
 +			list_add_tail(&req->list, &ctx->cancel_list);
 +		spin_unlock(&poll->head->lock);
 +	}
 +	if (mask) { /* no async, we'd stolen it */
 +		ipt.error = 0;
 +		io_poll_complete(ctx, req, mask);
  	}
 +	spin_unlock_irq(&ctx->completion_lock);
  
 -	return ret;
 +	if (mask) {
 +		io_cqring_ev_posted(ctx);
 +		io_put_req(req);
 +	}
 +	return ipt.error;
  }
  
 -static int io_req_defer(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 +	struct io_uring_sqe *sqe_copy;
  
 -	/* Still need defer if there is pending req in defer list. */
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
  		return 0;
  
 -	if (!req->io && io_alloc_async_ctx(req))
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
  		return -EAGAIN;
  
 -	ret = io_req_defer_prep(req, sqe);
 -	if (ret < 0)
 -		return ret;
 -
  	spin_lock_irq(&ctx->completion_lock);
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list)) {
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list)) {
  		spin_unlock_irq(&ctx->completion_lock);
 +		kfree(sqe_copy);
  		return 0;
  	}
  
* Unmerged path fs/io_uring.c
