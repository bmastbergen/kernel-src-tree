io_uring: fix io_sq_thread no schedule when busy

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Xuan Zhuo <xuanzhuo@linux.alibaba.com>
commit b772f07add1c0b22e02c0f1e96f647560679d3a9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b772f07a.failed

When the user consumes and generates sqe at a fast rate,
io_sqring_entries can always get sqe, and ret will not be equal to -EBUSY,
so that io_sq_thread will never call cond_resched or schedule, and then
we will get the following system error prompt:

rcu: INFO: rcu_sched self-detected stall on CPU
or
watchdog: BUG: soft lockup-CPU#23 stuck for 112s! [io_uring-sq:1863]

This patch checks whether need to call cond_resched() by checking
the need_resched() function every cycle.

	Suggested-by: Jens Axboe <axboe@kernel.dk>
	Signed-off-by: Xuan Zhuo <xuanzhuo@linux.alibaba.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit b772f07add1c0b22e02c0f1e96f647560679d3a9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 0b681a205810,9de9db70b928..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2496,56 -5980,38 +2496,66 @@@ out
  
  static int io_sq_thread(void *data)
  {
 +	struct sqe_submit sqes[IO_IOPOLL_BATCH];
  	struct io_ring_ctx *ctx = data;
 -	const struct cred *old_cred;
 +	struct mm_struct *cur_mm = NULL;
 +	mm_segment_t old_fs;
  	DEFINE_WAIT(wait);
 +	unsigned inflight;
  	unsigned long timeout;
 -	int ret = 0;
  
 -	complete(&ctx->sq_thread_comp);
 +	complete(&ctx->sqo_thread_started);
  
 -	old_cred = override_creds(ctx->creds);
 +	old_fs = get_fs();
 +	set_fs(USER_DS);
  
 -	timeout = jiffies + ctx->sq_thread_idle;
 +	timeout = inflight = 0;
  	while (!kthread_should_park()) {
 -		unsigned int to_submit;
 +		bool all_fixed, mm_fault = false;
 +		int i;
  
 -		if (!list_empty(&ctx->poll_list)) {
 +		if (inflight) {
  			unsigned nr_events = 0;
  
 -			mutex_lock(&ctx->uring_lock);
 -			if (!list_empty(&ctx->poll_list))
 -				io_iopoll_getevents(ctx, &nr_events, 0);
 -			else
 +			if (ctx->flags & IORING_SETUP_IOPOLL) {
 +				/*
 +				 * inflight is the count of the maximum possible
 +				 * entries we submitted, but it can be smaller
 +				 * if we dropped some of them. If we don't have
 +				 * poll entries available, then we know that we
 +				 * have nothing left to poll for. Reset the
 +				 * inflight count to zero in that case.
 +				 */
 +				mutex_lock(&ctx->uring_lock);
 +				if (!list_empty(&ctx->poll_list))
 +					io_iopoll_getevents(ctx, &nr_events, 0);
 +				else
 +					inflight = 0;
 +				mutex_unlock(&ctx->uring_lock);
 +			} else {
 +				/*
 +				 * Normal IO, just pretend everything completed.
 +				 * We don't have to poll completions for that.
 +				 */
 +				nr_events = inflight;
 +			}
 +
 +			inflight -= nr_events;
 +			if (!inflight)
  				timeout = jiffies + ctx->sq_thread_idle;
 -			mutex_unlock(&ctx->uring_lock);
  		}
  
++<<<<<<< HEAD
 +		if (!io_get_sqring(ctx, &sqes[0])) {
++=======
+ 		to_submit = io_sqring_entries(ctx);
+ 
+ 		/*
+ 		 * If submit got -EBUSY, flag us as needing the application
+ 		 * to enter the kernel to reap and flush events.
+ 		 */
+ 		if (!to_submit || ret == -EBUSY || need_resched()) {
++>>>>>>> b772f07add1c (io_uring: fix io_sq_thread no schedule when busy)
  			/*
  			 * Drop cur_mm before scheduling, we can't hold it for
  			 * long periods (or over schedule()). Do this before
@@@ -2561,9 -6023,15 +2571,17 @@@
  			/*
  			 * We're polling. If we're within the defined idle
  			 * period, then let us spin without work before going
 -			 * to sleep. The exception is if we got EBUSY doing
 -			 * more IO, we should wait for the application to
 -			 * reap events and wake us up.
 +			 * to sleep.
  			 */
++<<<<<<< HEAD
 +			if (inflight || !time_after(jiffies, timeout)) {
++=======
+ 			if (!list_empty(&ctx->poll_list) || need_resched() ||
+ 			    (!time_after(jiffies, timeout) && ret != -EBUSY &&
+ 			    !percpu_ref_is_dying(&ctx->refs))) {
+ 				if (current->task_works)
+ 					task_work_run();
++>>>>>>> b772f07add1c (io_uring: fix io_sq_thread no schedule when busy)
  				cond_resched();
  				continue;
  			}
* Unmerged path fs/io_uring.c
