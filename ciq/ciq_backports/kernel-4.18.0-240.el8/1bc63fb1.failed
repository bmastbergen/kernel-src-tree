mm, memcg: make scan aggression always exclude protection

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Chris Down <chris@chrisdown.name>
commit 1bc63fb1272be0773e925f78c0fbd06c89701d55
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/1bc63fb1.failed

This patch is an incremental improvement on the existing
memory.{low,min} relative reclaim work to base its scan pressure
calculations on how much protection is available compared to the current
usage, rather than how much the current usage is over some protection
threshold.

This change doesn't change the experience for the user in the normal
case too much.  One benefit is that it replaces the (somewhat arbitrary)
100% cutoff with an indefinite slope, which makes it easier to ballpark
a memory.low value.

As well as this, the old methodology doesn't quite apply generically to
machines with varying amounts of physical memory.  Let's say we have a
top level cgroup, workload.slice, and another top level cgroup,
system-management.slice.  We want to roughly give 12G to
system-management.slice, so on a 32GB machine we set memory.low to 20GB
in workload.slice, and on a 64GB machine we set memory.low to 52GB.
However, because these are relative amounts to the total machine size,
while the amount of memory we want to generally be willing to yield to
system.slice is absolute (12G), we end up putting more pressure on
system.slice just because we have a larger machine and a larger workload
to fill it, which seems fairly unintuitive.  With this new behaviour, we
don't end up with this unintended side effect.

Previously the way that memory.low protection works is that if you are
50% over a certain baseline, you get 50% of your normal scan pressure.
This is certainly better than the previous cliff-edge behaviour, but it
can be improved even further by always considering memory under the
currently enforced protection threshold to be out of bounds.  This means
that we can set relatively low memory.low thresholds for variable or
bursty workloads while still getting a reasonable level of protection,
whereas with the previous version we may still trivially hit the 100%
clamp.  The previous 100% clamp is also somewhat arbitrary, whereas this
one is more concretely based on the currently enforced protection
threshold, which is likely easier to reason about.

There is also a subtle issue with the way that proportional reclaim
worked previously -- it promotes having no memory.low, since it makes
pressure higher during low reclaim.  This happens because we base our
scan pressure modulation on how far memory.current is between memory.min
and memory.low, but if memory.low is unset, we only use the overage
method.  In most cromulent configurations, this then means that we end
up with *more* pressure than with no memory.low at all when we're in low
reclaim, which is not really very usable or expected.

With this patch, memory.low and memory.min affect reclaim pressure in a
more understandable and composable way.  For example, from a user
standpoint, "protected" memory now remains untouchable from a reclaim
aggression standpoint, and users can also have more confidence that
bursty workloads will still receive some amount of guaranteed
protection.

Link: http://lkml.kernel.org/r/20190322160307.GA3316@chrisdown.name
	Signed-off-by: Chris Down <chris@chrisdown.name>
	Reviewed-by: Roman Gushchin <guro@fb.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Acked-by: Michal Hocko <mhocko@kernel.org>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Dennis Zhou <dennis@kernel.org>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1bc63fb1272be0773e925f78c0fbd06c89701d55)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	mm/vmscan.c
diff --cc include/linux/memcontrol.h
index edfd7c1e1c27,ae703ea3ef48..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -339,6 -356,19 +339,22 @@@ static inline bool mem_cgroup_disabled(
  	return !cgroup_subsys_enabled(memory_cgrp_subsys);
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long mem_cgroup_protection(struct mem_cgroup *memcg,
+ 						  bool in_low_reclaim)
+ {
+ 	if (mem_cgroup_disabled())
+ 		return 0;
+ 
+ 	if (in_low_reclaim)
+ 		return READ_ONCE(memcg->memory.emin);
+ 
+ 	return max(READ_ONCE(memcg->memory.emin),
+ 		   READ_ONCE(memcg->memory.elow));
+ }
+ 
++>>>>>>> 1bc63fb1272b (mm, memcg: make scan aggression always exclude protection)
  enum mem_cgroup_protection mem_cgroup_protected(struct mem_cgroup *root,
  						struct mem_cgroup *memcg);
  
@@@ -823,6 -844,12 +839,15 @@@ static inline void memcg_memory_event_m
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long mem_cgroup_protection(struct mem_cgroup *memcg,
+ 						  bool in_low_reclaim)
+ {
+ 	return 0;
+ }
+ 
++>>>>>>> 1bc63fb1272b (mm, memcg: make scan aggression always exclude protection)
  static inline enum mem_cgroup_protection mem_cgroup_protected(
  	struct mem_cgroup *root, struct mem_cgroup *memcg)
  {
diff --cc mm/vmscan.c
index 52e40eb6d6d4,c6659bb758a4..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -2423,11 -2459,64 +2423,69 @@@ out
  	*lru_pages = 0;
  	for_each_evictable_lru(lru) {
  		int file = is_file_lru(lru);
 -		unsigned long lruvec_size;
 +		unsigned long size;
  		unsigned long scan;
++<<<<<<< HEAD
++=======
+ 		unsigned long protection;
+ 
+ 		lruvec_size = lruvec_lru_size(lruvec, lru, sc->reclaim_idx);
+ 		protection = mem_cgroup_protection(memcg,
+ 						   sc->memcg_low_reclaim);
+ 
+ 		if (protection) {
+ 			/*
+ 			 * Scale a cgroup's reclaim pressure by proportioning
+ 			 * its current usage to its memory.low or memory.min
+ 			 * setting.
+ 			 *
+ 			 * This is important, as otherwise scanning aggression
+ 			 * becomes extremely binary -- from nothing as we
+ 			 * approach the memory protection threshold, to totally
+ 			 * nominal as we exceed it.  This results in requiring
+ 			 * setting extremely liberal protection thresholds. It
+ 			 * also means we simply get no protection at all if we
+ 			 * set it too low, which is not ideal.
+ 			 *
+ 			 * If there is any protection in place, we reduce scan
+ 			 * pressure by how much of the total memory used is
+ 			 * within protection thresholds.
+ 			 *
+ 			 * There is one special case: in the first reclaim pass,
+ 			 * we skip over all groups that are within their low
+ 			 * protection. If that fails to reclaim enough pages to
+ 			 * satisfy the reclaim goal, we come back and override
+ 			 * the best-effort low protection. However, we still
+ 			 * ideally want to honor how well-behaved groups are in
+ 			 * that case instead of simply punishing them all
+ 			 * equally. As such, we reclaim them based on how much
+ 			 * memory they are using, reducing the scan pressure
+ 			 * again by how much of the total memory used is under
+ 			 * hard protection.
+ 			 */
+ 			unsigned long cgroup_size = mem_cgroup_size(memcg);
+ 
+ 			/* Avoid TOCTOU with earlier protection check */
+ 			cgroup_size = max(cgroup_size, protection);
+ 
+ 			scan = lruvec_size - lruvec_size * protection /
+ 				cgroup_size;
+ 
+ 			/*
+ 			 * Minimally target SWAP_CLUSTER_MAX pages to keep
+ 			 * reclaim moving forwards, avoiding decremeting
+ 			 * sc->priority further than desirable.
+ 			 */
+ 			scan = max(scan, SWAP_CLUSTER_MAX);
+ 		} else {
+ 			scan = lruvec_size;
+ 		}
+ 
+ 		scan >>= sc->priority;
++>>>>>>> 1bc63fb1272b (mm, memcg: make scan aggression always exclude protection)
  
 +		size = lruvec_lru_size(lruvec, lru, sc->reclaim_idx);
 +		scan = size >> sc->priority;
  		/*
  		 * If the cgroup's already been deleted, make sure to
  		 * scrape out the remaining cache.
* Unmerged path include/linux/memcontrol.h
* Unmerged path mm/vmscan.c
