RDMA/mlx5: Lift implicit_mr_alloc() into the two routines that call it

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit c2edcd69351f681594a30b17b7fbc5259a038fb0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/c2edcd69.failed

This makes the routines easier to understand, particularly with respect
the locking requirements of the entire sequence. The implicit_mr_alloc()
had a lot of ifs specializing it to each of the callers, and only a very
small amount of code was actually shared.

Following patches will cause the flow in the two functions to diverge
further.

Link: https://lore.kernel.org/r/20191009160934.3143-7-jgg@ziepe.ca
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit c2edcd69351f681594a30b17b7fbc5259a038fb0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/odp.c
index df3038ca913b,dfaa39fc4d3f..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -421,134 -416,126 +421,200 @@@ static void mlx5_ib_page_fault_resume(s
  			    wq_num, err);
  }
  
++<<<<<<< HEAD
 +static struct mlx5_ib_mr *implicit_mr_alloc(struct ib_pd *pd,
 +					    struct ib_umem *umem,
 +					    bool ksm, int access_flags)
 +{
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct mlx5_ib_mr *mr;
 +	int err;
 +
 +	mr = mlx5_mr_cache_alloc(dev, ksm ? MLX5_IMR_KSM_CACHE_ENTRY :
 +					    MLX5_IMR_MTT_CACHE_ENTRY);
 +
 +	if (IS_ERR(mr))
 +		return mr;
 +
 +	mr->ibmr.pd = pd;
 +
 +	mr->dev = dev;
 +	mr->access_flags = access_flags;
 +	mr->mmkey.iova = 0;
 +	mr->umem = umem;
 +
 +	if (ksm) {
 +		err = mlx5_ib_update_xlt(mr, 0,
 +					 mlx5_imr_ksm_entries,
 +					 MLX5_KSM_PAGE_SHIFT,
 +					 MLX5_IB_UPD_XLT_INDIRECT |
 +					 MLX5_IB_UPD_XLT_ZAP |
 +					 MLX5_IB_UPD_XLT_ENABLE);
 +
 +	} else {
 +		err = mlx5_ib_update_xlt(mr, 0,
 +					 MLX5_IMR_MTT_ENTRIES,
 +					 PAGE_SHIFT,
 +					 MLX5_IB_UPD_XLT_ZAP |
 +					 MLX5_IB_UPD_XLT_ENABLE |
 +					 MLX5_IB_UPD_XLT_ATOMIC);
 +	}
 +
 +	if (err)
 +		goto fail;
 +
 +	mr->ibmr.lkey = mr->mmkey.key;
 +	mr->ibmr.rkey = mr->mmkey.key;
 +
 +	mr->live = 1;
 +
 +	mlx5_ib_dbg(dev, "key %x dev %p mr %p\n",
 +		    mr->mmkey.key, dev->mdev, mr);
 +
 +	return mr;
 +
 +fail:
 +	mlx5_ib_err(dev, "Failed to register MKEY %d\n", err);
 +	mlx5_mr_cache_free(dev, mr);
 +
 +	return ERR_PTR(err);
 +}
 +
 +static struct ib_umem_odp *implicit_mr_get_data(struct mlx5_ib_mr *mr,
++=======
+ static struct mlx5_ib_mr *implicit_get_child_mr(struct mlx5_ib_mr *imr,
+ 						unsigned long idx)
+ {
+ 	struct ib_umem_odp *odp;
+ 	struct mlx5_ib_mr *mr;
+ 	struct mlx5_ib_mr *ret;
+ 	int err;
+ 
+ 	odp = ib_umem_odp_alloc_child(to_ib_umem_odp(imr->umem),
+ 				      idx * MLX5_IMR_MTT_SIZE,
+ 				      MLX5_IMR_MTT_SIZE);
+ 	if (IS_ERR(odp))
+ 		return ERR_CAST(odp);
+ 
+ 	ret = mr = mlx5_mr_cache_alloc(imr->dev, MLX5_IMR_MTT_CACHE_ENTRY);
+ 	if (IS_ERR(mr))
+ 		goto out_umem;
+ 
+ 	err = xa_reserve(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),
+ 			 GFP_KERNEL);
+ 	if (err) {
+ 		ret = ERR_PTR(err);
+ 		goto out_mr;
+ 	}
+ 
+ 	mr->ibmr.pd = imr->ibmr.pd;
+ 	mr->access_flags = imr->access_flags;
+ 	mr->umem = &odp->umem;
+ 	mr->ibmr.lkey = mr->mmkey.key;
+ 	mr->ibmr.rkey = mr->mmkey.key;
+ 	mr->mmkey.iova = 0;
+ 	mr->parent = imr;
+ 	odp->private = mr;
+ 	INIT_WORK(&odp->work, mr_leaf_free_action);
+ 
+ 	err = mlx5_ib_update_xlt(mr, 0,
+ 				 MLX5_IMR_MTT_ENTRIES,
+ 				 PAGE_SHIFT,
+ 				 MLX5_IB_UPD_XLT_ZAP |
+ 				 MLX5_IB_UPD_XLT_ENABLE |
+ 				 MLX5_IB_UPD_XLT_ATOMIC);
+ 	if (err) {
+ 		ret = ERR_PTR(err);
+ 		goto out_release;
+ 	}
+ 
+ 	mr->mmkey.iova = idx * MLX5_IMR_MTT_SIZE;
+ 	xa_store(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),
+ 		 &mr->mmkey, GFP_ATOMIC);
+ 
+ 	mlx5_ib_dbg(imr->dev, "key %x mr %p\n", mr->mmkey.key, mr);
+ 	return mr;
+ 
+ out_release:
+ 	xa_release(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
+ out_mr:
+ 	mlx5_mr_cache_free(imr->dev, mr);
+ out_umem:
+ 	ib_umem_odp_release(odp);
+ 	return ret;
+ }
+ 
+ static struct ib_umem_odp *implicit_mr_get_data(struct mlx5_ib_mr *imr,
++>>>>>>> c2edcd69351f (RDMA/mlx5: Lift implicit_mr_alloc() into the two routines that call it)
  						u64 io_virt, size_t bcnt)
  {
 -	struct ib_umem_odp *odp_imr = to_ib_umem_odp(imr->umem);
 -	unsigned long end_idx = (io_virt + bcnt - 1) >> MLX5_IMR_MTT_SHIFT;
 -	unsigned long idx = io_virt >> MLX5_IMR_MTT_SHIFT;
 -	unsigned long inv_start_idx = end_idx + 1;
 -	unsigned long inv_len = 0;
 -	struct ib_umem_odp *result = NULL;
 -	struct ib_umem_odp *odp;
 -	int ret;
 +	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.pd->device);
 +	struct ib_umem_odp *odp, *result = NULL;
 +	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
 +	u64 addr = io_virt & MLX5_IMR_MTT_MASK;
 +	int nentries = 0, start_idx = 0, ret;
 +	struct mlx5_ib_mr *mtt;
  
 -	mutex_lock(&odp_imr->umem_mutex);
 -	odp = odp_lookup(idx * MLX5_IMR_MTT_SIZE, 1, imr);
 -	for (idx = idx; idx <= end_idx; idx++) {
 -		if (unlikely(!odp)) {
 -			struct mlx5_ib_mr *mtt;
 +	mutex_lock(&odp_mr->umem_mutex);
 +	odp = odp_lookup(addr, 1, mr);
  
 -			mtt = implicit_get_child_mr(imr, idx);
 -			if (IS_ERR(mtt)) {
 -				result = ERR_CAST(mtt);
 -				goto out;
 -			}
 -			odp = to_ib_umem_odp(mtt->umem);
 -			inv_start_idx = min(inv_start_idx, idx);
 -			inv_len = idx - inv_start_idx + 1;
 +	mlx5_ib_dbg(dev, "io_virt:%llx bcnt:%zx addr:%llx odp:%p\n",
 +		    io_virt, bcnt, addr, odp);
 +
 +next_mr:
 +	if (likely(odp)) {
 +		if (nentries)
 +			nentries++;
 +	} else {
 +		odp = ib_alloc_odp_umem(odp_mr, addr,
 +					MLX5_IMR_MTT_SIZE);
 +		if (IS_ERR(odp)) {
 +			mutex_unlock(&odp_mr->umem_mutex);
 +			return ERR_CAST(odp);
  		}
  
 -		/* Return first odp if region not covered by single one */
 -		if (likely(!result))
 -			result = odp;
 +		mtt = implicit_mr_alloc(mr->ibmr.pd, &odp->umem, 0,
 +					mr->access_flags);
 +		if (IS_ERR(mtt)) {
 +			mutex_unlock(&odp_mr->umem_mutex);
 +			ib_umem_release(&odp->umem);
 +			return ERR_CAST(mtt);
 +		}
 +
 +		odp->private = mtt;
 +		mtt->umem = &odp->umem;
 +		mtt->mmkey.iova = addr;
 +		mtt->parent = mr;
 +		INIT_WORK(&odp->work, mr_leaf_free_action);
 +
 +		if (!nentries)
 +			start_idx = addr >> MLX5_IMR_MTT_SHIFT;
 +		nentries++;
 +	}
 +
 +	/* Return first odp if region not covered by single one */
 +	if (likely(!result))
 +		result = odp;
  
 +	addr += MLX5_IMR_MTT_SIZE;
 +	if (unlikely(addr < io_virt + bcnt)) {
  		odp = odp_next(odp);
 -		if (odp && ib_umem_start(odp) != idx * MLX5_IMR_MTT_SIZE)
 +		if (odp && ib_umem_start(odp) != addr)
  			odp = NULL;
 +		goto next_mr;
  	}
  
 -	/*
 -	 * Any time the children in the interval tree are changed we must
 -	 * perform an update of the xlt before exiting to ensure the HW and
 -	 * the tree remains synchronized.
 -	 */
 -out:
 -	if (likely(!inv_len))
 -		goto out_unlock;
 -
 -	ret = mlx5_ib_update_xlt(imr, inv_start_idx, inv_len, 0,
 -				 MLX5_IB_UPD_XLT_INDIRECT |
 +	if (unlikely(nentries)) {
 +		ret = mlx5_ib_update_xlt(mr, start_idx, nentries, 0,
 +					 MLX5_IB_UPD_XLT_INDIRECT |
  					 MLX5_IB_UPD_XLT_ATOMIC);
 -	if (ret) {
 -		mlx5_ib_err(to_mdev(imr->ibmr.pd->device),
 -			    "Failed to update PAS\n");
 -		result = ERR_PTR(ret);
 -		goto out_unlock;
 +		if (ret) {
 +			mlx5_ib_err(dev, "Failed to update PAS\n");
 +			result = ERR_PTR(ret);
 +		}
  	}
  
 -out_unlock:
 -	mutex_unlock(&odp_imr->umem_mutex);
 +	mutex_unlock(&odp_mr->umem_mutex);
  	return result;
  }
  
@@@ -556,57 -543,85 +622,104 @@@ struct mlx5_ib_mr *mlx5_ib_alloc_implic
  					     struct ib_udata *udata,
  					     int access_flags)
  {
++<<<<<<< HEAD
 +	struct mlx5_ib_mr *imr;
 +	struct ib_umem *umem;
++=======
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->ibpd.device);
+ 	struct ib_umem_odp *umem_odp;
+ 	struct mlx5_ib_mr *imr;
+ 	int err;
++>>>>>>> c2edcd69351f (RDMA/mlx5: Lift implicit_mr_alloc() into the two routines that call it)
 +
 +	umem = ib_umem_get(udata, 0, 0, access_flags, 0);
 +	if (IS_ERR(umem))
 +		return ERR_CAST(umem);
  
 -	umem_odp = ib_umem_odp_alloc_implicit(udata, access_flags);
 -	if (IS_ERR(umem_odp))
 -		return ERR_CAST(umem_odp);
++<<<<<<< HEAD
 +	imr = implicit_mr_alloc(&pd->ibpd, umem, 1, access_flags);
 +	if (IS_ERR(imr)) {
 +		ib_umem_release(umem);
 +		return ERR_CAST(imr);
 +	}
  
 +	imr->umem = umem;
 +	init_waitqueue_head(&imr->q_leaf_free);
 +	atomic_set(&imr->num_leaf_free, 0);
 +	atomic_set(&imr->num_pending_prefetch, 0);
 +
 +	imr->is_odp_implicit = true;
++=======
+ 	imr = mlx5_mr_cache_alloc(dev, MLX5_IMR_KSM_CACHE_ENTRY);
+ 	if (IS_ERR(imr)) {
+ 		err = PTR_ERR(imr);
+ 		goto out_umem;
+ 	}
+ 
+ 	imr->ibmr.pd = &pd->ibpd;
+ 	imr->access_flags = access_flags;
+ 	imr->mmkey.iova = 0;
+ 	imr->umem = &umem_odp->umem;
+ 	imr->ibmr.lkey = imr->mmkey.key;
+ 	imr->ibmr.rkey = imr->mmkey.key;
+ 	imr->umem = &umem_odp->umem;
+ 	init_waitqueue_head(&imr->q_leaf_free);
+ 	atomic_set(&imr->num_leaf_free, 0);
+ 	atomic_set(&imr->num_pending_prefetch, 0);
++>>>>>>> c2edcd69351f (RDMA/mlx5: Lift implicit_mr_alloc() into the two routines that call it)
+ 
+ 	err = mlx5_ib_update_xlt(imr, 0,
+ 				 mlx5_imr_ksm_entries,
+ 				 MLX5_KSM_PAGE_SHIFT,
+ 				 MLX5_IB_UPD_XLT_INDIRECT |
+ 				 MLX5_IB_UPD_XLT_ZAP |
+ 				 MLX5_IB_UPD_XLT_ENABLE);
+ 	if (err)
+ 		goto out_mr;
  
+ 	err = xa_err(xa_store(&dev->odp_mkeys, mlx5_base_mkey(imr->mmkey.key),
+ 			      &imr->mmkey, GFP_KERNEL));
+ 	if (err)
+ 		goto out_mr;
+ 
+ 	mlx5_ib_dbg(dev, "key %x mr %p\n", imr->mmkey.key, imr);
  	return imr;
+ out_mr:
+ 	mlx5_ib_err(dev, "Failed to register MKEY %d\n", err);
+ 	mlx5_mr_cache_free(dev, imr);
+ out_umem:
+ 	ib_umem_odp_release(umem_odp);
+ 	return ERR_PTR(err);
  }
  
 -void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
 +static int mr_leaf_free(struct ib_umem_odp *umem_odp, u64 start, u64 end,
 +			void *cookie)
  {
 -	struct ib_ucontext_per_mm *per_mm = mr_to_per_mm(imr);
 -	struct rb_node *node;
 +	struct mlx5_ib_mr *mr = umem_odp->private, *imr = cookie;
  
 -	down_read(&per_mm->umem_rwsem);
 -	for (node = rb_first_cached(&per_mm->umem_tree); node;
 -	     node = rb_next(node)) {
 -		struct ib_umem_odp *umem_odp =
 -			rb_entry(node, struct ib_umem_odp, interval_tree.rb);
 -		struct mlx5_ib_mr *mr = umem_odp->private;
 +	if (mr->parent != imr)
 +		return 0;
  
 -		if (mr->parent != imr)
 -			continue;
 +	ib_umem_odp_unmap_dma_pages(umem_odp, ib_umem_start(umem_odp),
 +				    ib_umem_end(umem_odp));
  
 -		mutex_lock(&umem_odp->umem_mutex);
 -		ib_umem_odp_unmap_dma_pages(umem_odp, ib_umem_start(umem_odp),
 -					    ib_umem_end(umem_odp));
 +	if (umem_odp->dying)
 +		return 0;
  
 -		if (umem_odp->dying) {
 -			mutex_unlock(&umem_odp->umem_mutex);
 -			continue;
 -		}
 +	WRITE_ONCE(umem_odp->dying, 1);
 +	atomic_inc(&imr->num_leaf_free);
 +	schedule_work(&umem_odp->work);
  
 -		umem_odp->dying = 1;
 -		atomic_inc(&imr->num_leaf_free);
 -		schedule_work(&umem_odp->work);
 -		mutex_unlock(&umem_odp->umem_mutex);
 -	}
 +	return 0;
 +}
 +
 +void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
 +{
 +	struct ib_ucontext_per_mm *per_mm = mr_to_per_mm(imr);
 +
 +	down_read(&per_mm->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, 0, ULLONG_MAX,
 +				      mr_leaf_free, imr);
  	up_read(&per_mm->umem_rwsem);
  
  	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
