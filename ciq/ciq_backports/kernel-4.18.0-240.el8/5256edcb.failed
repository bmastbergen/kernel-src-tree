RDMA/mlx5: Rework implicit ODP destroy

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 5256edcb98a14b11409a2d323f56a70a8b366363
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5256edcb.failed

Use SRCU in a sensible way by removing all MRs in the implicit tree from
the two xarrays (the update operation), then a synchronize, followed by a
normal single threaded teardown.

This is only a little unusual from the normal pattern as there can still
be some work pending in the unbound wq that may also require a workqueue
flush. This is tracked with a single atomic, consolidating the redundant
existing atomics and wait queue.

For understand-ability the entire ODP implicit create/destroy flow now
largely exists in a single pair of functions within odp.c, with a few
support functions for tearing down an unused child.

Link: https://lore.kernel.org/r/20191009160934.3143-13-jgg@ziepe.ca
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 5256edcb98a14b11409a2d323f56a70a8b366363)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index ca3faf45438a,add24b628900..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -6141,10 -6145,8 +6141,15 @@@ static struct ib_counters *mlx5_ib_crea
  static void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
  {
  	mlx5_ib_cleanup_multiport_master(dev);
++<<<<<<< HEAD
 +	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {
 +		srcu_barrier(&dev->mr_srcu);
 +		cleanup_srcu_struct(&dev->mr_srcu);
 +	}
++=======
+ 	WARN_ON(!xa_empty(&dev->odp_mkeys));
+ 	cleanup_srcu_struct(&dev->odp_srcu);
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  
  	WARN_ON(!xa_empty(&dev->sig_mrs));
  	WARN_ON(!bitmap_empty(dev->dm.memic_alloc_pages, MLX5_MAX_MEMIC_PAGES));
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index e2bebcd6069c,b8c958f62628..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -616,12 -617,16 +616,24 @@@ struct mlx5_ib_mr 
  	u64			data_iova;
  	u64			pi_iova;
  
++<<<<<<< HEAD
 +	atomic_t		num_leaf_free;
 +	wait_queue_head_t       q_leaf_free;
++=======
+ 	/* For ODP and implicit */
+ 	atomic_t		num_deferred_work;
+ 	struct xarray		implicit_children;
+ 	union {
+ 		struct rcu_head rcu;
+ 		struct list_head elm;
+ 		struct work_struct work;
+ 	} odp_destroy;
+ 
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  	struct mlx5_async_work  cb_work;
 +	atomic_t		num_pending_prefetch;
 +	struct ib_odp_counters	odp_stats;
 +	bool			is_odp_implicit;
  };
  
  static inline bool is_odp_mr(struct mlx5_ib_mr *mr)
diff --cc drivers/infiniband/hw/mlx5/mr.c
index c7b8ab228778,1e91f61efa8a..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -1334,9 -1315,16 +1334,22 @@@ struct ib_mr *mlx5_ib_reg_user_mr(struc
  		}
  	}
  
++<<<<<<< HEAD
 +	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING)) {
 +		mr->live = 1;
 +		atomic_set(&mr->num_pending_prefetch, 0);
++=======
+ 	if (is_odp_mr(mr)) {
+ 		to_ib_umem_odp(mr->umem)->private = mr;
+ 		atomic_set(&mr->num_deferred_work, 0);
+ 		err = xa_err(xa_store(&dev->odp_mkeys,
+ 				      mlx5_base_mkey(mr->mmkey.key), &mr->mmkey,
+ 				      GFP_KERNEL));
+ 		if (err) {
+ 			dereg_mr(dev, mr);
+ 			return ERR_PTR(err);
+ 		}
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  	}
  
  	return &mr->ibmr;
@@@ -1579,23 -1567,21 +1592,30 @@@ static void dereg_mr(struct mlx5_ib_de
  		/* Prevent new page faults and
  		 * prefetch requests from succeeding
  		 */
 -		xa_erase(&dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
 +		mr->live = 0;
  
  		/* Wait for all running page-fault handlers to finish. */
 -		synchronize_srcu(&dev->odp_srcu);
 +		synchronize_srcu(&dev->mr_srcu);
  
  		/* dequeue pending prefetch requests for the mr */
- 		if (atomic_read(&mr->num_pending_prefetch))
+ 		if (atomic_read(&mr->num_deferred_work)) {
  			flush_workqueue(system_unbound_wq);
- 		WARN_ON(atomic_read(&mr->num_pending_prefetch));
+ 			WARN_ON(atomic_read(&mr->num_deferred_work));
+ 		}
  
  		/* Destroy all page mappings */
++<<<<<<< HEAD
 +		if (umem_odp->page_list)
 +			mlx5_ib_invalidate_range(umem_odp,
 +						 ib_umem_start(umem_odp),
 +						 ib_umem_end(umem_odp));
 +		else
 +			mlx5_ib_free_implicit_mr(mr);
++=======
+ 		mlx5_ib_invalidate_range(umem_odp, ib_umem_start(umem_odp),
+ 					 ib_umem_end(umem_odp));
+ 
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  		/*
  		 * We kill the umem before the MR for ODP,
  		 * so that there will not be any invalidations in
diff --cc drivers/infiniband/hw/mlx5/odp.c
index df3038ca913b,5cf93d8129a9..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -220,31 -144,79 +220,92 @@@ void mlx5_odp_populate_klm(struct mlx5_
  	}
  }
  
- static void mr_leaf_free_action(struct work_struct *work)
+ /*
+  * This must be called after the mr has been removed from implicit_children
+  * and odp_mkeys and the SRCU synchronized.  NOTE: The MR does not necessarily
+  * have to be empty here, parallel page faults could have raced with the free
+  * process and added pages to it.
+  */
+ static void free_implicit_child_mr(struct mlx5_ib_mr *mr, bool need_imr_xlt)
  {
- 	struct ib_umem_odp *odp = container_of(work, struct ib_umem_odp, work);
- 	int idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
- 	struct mlx5_ib_mr *mr = odp->private, *imr = mr->parent;
+ 	struct mlx5_ib_mr *imr = mr->parent;
  	struct ib_umem_odp *odp_imr = to_ib_umem_odp(imr->umem);
+ 	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
+ 	unsigned long idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
  	int srcu_key;
  
++<<<<<<< HEAD
 +	mr->parent = NULL;
 +	synchronize_srcu(&mr->dev->mr_srcu);
 +
 +	if (imr->live) {
 +		srcu_key = srcu_read_lock(&mr->dev->mr_srcu);
++=======
+ 	/* implicit_child_mr's are not allowed to have deferred work */
+ 	WARN_ON(atomic_read(&mr->num_deferred_work));
+ 
+ 	if (need_imr_xlt) {
+ 		srcu_key = srcu_read_lock(&mr->dev->odp_srcu);
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  		mutex_lock(&odp_imr->umem_mutex);
- 		mlx5_ib_update_xlt(imr, idx, 1, 0,
+ 		mlx5_ib_update_xlt(mr->parent, idx, 1, 0,
  				   MLX5_IB_UPD_XLT_INDIRECT |
  				   MLX5_IB_UPD_XLT_ATOMIC);
  		mutex_unlock(&odp_imr->umem_mutex);
 -		srcu_read_unlock(&mr->dev->odp_srcu, srcu_key);
 +		srcu_read_unlock(&mr->dev->mr_srcu, srcu_key);
  	}
++<<<<<<< HEAD
 +	ib_umem_release(&odp->umem);
 +	mlx5_mr_cache_free(mr->dev, mr);
++=======
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
+ 
+ 	mr->parent = NULL;
+ 	mlx5_mr_cache_free(mr->dev, mr);
+ 	ib_umem_odp_release(odp);
+ 	atomic_dec(&imr->num_deferred_work);
+ }
+ 
+ static void free_implicit_child_mr_work(struct work_struct *work)
+ {
+ 	struct mlx5_ib_mr *mr =
+ 		container_of(work, struct mlx5_ib_mr, odp_destroy.work);
+ 
+ 	free_implicit_child_mr(mr, true);
+ }
  
- 	if (atomic_dec_and_test(&imr->num_leaf_free))
- 		wake_up(&imr->q_leaf_free);
+ static void free_implicit_child_mr_rcu(struct rcu_head *head)
+ {
+ 	struct mlx5_ib_mr *mr =
+ 		container_of(head, struct mlx5_ib_mr, odp_destroy.rcu);
+ 
+ 	/* Freeing a MR is a sleeping operation, so bounce to a work queue */
+ 	INIT_WORK(&mr->odp_destroy.work, free_implicit_child_mr_work);
+ 	queue_work(system_unbound_wq, &mr->odp_destroy.work);
+ }
+ 
+ static void destroy_unused_implicit_child_mr(struct mlx5_ib_mr *mr)
+ {
+ 	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
+ 	unsigned long idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
+ 	struct mlx5_ib_mr *imr = mr->parent;
+ 
+ 	xa_lock(&imr->implicit_children);
+ 	/*
+ 	 * This can race with mlx5_ib_free_implicit_mr(), the first one to
+ 	 * reach the xa lock wins the race and destroys the MR.
+ 	 */
+ 	if (__xa_cmpxchg(&imr->implicit_children, idx, mr, NULL, GFP_ATOMIC) !=
+ 	    mr)
+ 		goto out_unlock;
+ 
+ 	__xa_erase(&mr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
+ 	atomic_inc(&imr->num_deferred_work);
+ 	call_srcu(&mr->dev->odp_srcu, &mr->odp_destroy.rcu,
+ 		  free_implicit_child_mr_rcu);
+ 
+ out_unlock:
+ 	xa_unlock(&imr->implicit_children);
  }
  
  void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
@@@ -324,13 -288,9 +385,19 @@@
  
  	ib_umem_odp_unmap_dma_pages(umem_odp, start, end);
  
++<<<<<<< HEAD
 +
 +	if (unlikely(!umem_odp->npages && mr->parent &&
 +		     !umem_odp->dying)) {
 +		WRITE_ONCE(umem_odp->dying, 1);
 +		atomic_inc(&mr->parent->num_leaf_free);
 +		schedule_work(&umem_odp->work);
 +	}
++=======
+ 	if (unlikely(!umem_odp->npages && mr->parent))
+ 		destroy_unused_implicit_child_mr(mr);
+ 	mutex_unlock(&umem_odp->umem_mutex);
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  }
  
  void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
@@@ -421,224 -383,192 +488,313 @@@ static void mlx5_ib_page_fault_resume(s
  			    wq_num, err);
  }
  
 -static struct mlx5_ib_mr *implicit_get_child_mr(struct mlx5_ib_mr *imr,
 -						unsigned long idx)
 +static struct mlx5_ib_mr *implicit_mr_alloc(struct ib_pd *pd,
 +					    struct ib_umem *umem,
 +					    bool ksm, int access_flags)
  {
 -	struct ib_umem_odp *odp;
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
  	struct mlx5_ib_mr *mr;
 -	struct mlx5_ib_mr *ret;
  	int err;
  
 -	odp = ib_umem_odp_alloc_child(to_ib_umem_odp(imr->umem),
 -				      idx * MLX5_IMR_MTT_SIZE,
 -				      MLX5_IMR_MTT_SIZE);
 -	if (IS_ERR(odp))
 -		return ERR_CAST(odp);
 +	mr = mlx5_mr_cache_alloc(dev, ksm ? MLX5_IMR_KSM_CACHE_ENTRY :
 +					    MLX5_IMR_MTT_CACHE_ENTRY);
  
 -	ret = mr = mlx5_mr_cache_alloc(imr->dev, MLX5_IMR_MTT_CACHE_ENTRY);
  	if (IS_ERR(mr))
 -		goto out_umem;
 +		return mr;
  
 -	err = xa_reserve(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),
 -			 GFP_KERNEL);
 -	if (err) {
 -		ret = ERR_PTR(err);
 -		goto out_mr;
 +	mr->ibmr.pd = pd;
 +
 +	mr->dev = dev;
 +	mr->access_flags = access_flags;
 +	mr->mmkey.iova = 0;
 +	mr->umem = umem;
 +
 +	if (ksm) {
 +		err = mlx5_ib_update_xlt(mr, 0,
 +					 mlx5_imr_ksm_entries,
 +					 MLX5_KSM_PAGE_SHIFT,
 +					 MLX5_IB_UPD_XLT_INDIRECT |
 +					 MLX5_IB_UPD_XLT_ZAP |
 +					 MLX5_IB_UPD_XLT_ENABLE);
 +
 +	} else {
 +		err = mlx5_ib_update_xlt(mr, 0,
 +					 MLX5_IMR_MTT_ENTRIES,
 +					 PAGE_SHIFT,
 +					 MLX5_IB_UPD_XLT_ZAP |
 +					 MLX5_IB_UPD_XLT_ENABLE |
 +					 MLX5_IB_UPD_XLT_ATOMIC);
  	}
  
 -	mr->ibmr.pd = imr->ibmr.pd;
 -	mr->access_flags = imr->access_flags;
 -	mr->umem = &odp->umem;
 +	if (err)
 +		goto fail;
 +
  	mr->ibmr.lkey = mr->mmkey.key;
  	mr->ibmr.rkey = mr->mmkey.key;
++<<<<<<< HEAD
++=======
+ 	mr->mmkey.iova = idx * MLX5_IMR_MTT_SIZE;
+ 	mr->parent = imr;
+ 	odp->private = mr;
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  
 -	err = mlx5_ib_update_xlt(mr, 0,
 -				 MLX5_IMR_MTT_ENTRIES,
 -				 PAGE_SHIFT,
 -				 MLX5_IB_UPD_XLT_ZAP |
 -				 MLX5_IB_UPD_XLT_ENABLE);
 -	if (err) {
 -		ret = ERR_PTR(err);
 -		goto out_release;
 -	}
 +	mr->live = 1;
 +
++<<<<<<< HEAD
 +	mlx5_ib_dbg(dev, "key %x dev %p mr %p\n",
 +		    mr->mmkey.key, dev->mdev, mr);
  
++=======
+ 	/*
+ 	 * Once the store to either xarray completes any error unwind has to
+ 	 * use synchronize_srcu(). Avoid this with xa_reserve()
+ 	 */
+ 	ret = xa_cmpxchg(&imr->implicit_children, idx, NULL, mr,
+ 			 GFP_KERNEL);
+ 	if (likely(!ret))
+ 		xa_store(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),
+ 			 &mr->mmkey, GFP_ATOMIC);
+ 	if (unlikely(ret)) {
+ 		if (xa_is_err(ret)) {
+ 			ret = ERR_PTR(xa_err(ret));
+ 			goto out_release;
+ 		}
+ 		/*
+ 		 * Another thread beat us to creating the child mr, use
+ 		 * theirs.
+ 		 */
+ 		goto out_release;
+ 	}
+ 
+ 	mlx5_ib_dbg(imr->dev, "key %x mr %p\n", mr->mmkey.key, mr);
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  	return mr;
  
 -out_release:
 -	xa_release(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
 -out_mr:
 -	mlx5_mr_cache_free(imr->dev, mr);
 -out_umem:
 -	ib_umem_odp_release(odp);
 -	return ret;
 +fail:
 +	mlx5_ib_err(dev, "Failed to register MKEY %d\n", err);
 +	mlx5_mr_cache_free(dev, mr);
 +
 +	return ERR_PTR(err);
 +}
 +
 +static struct ib_umem_odp *implicit_mr_get_data(struct mlx5_ib_mr *mr,
 +						u64 io_virt, size_t bcnt)
 +{
 +	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.pd->device);
 +	struct ib_umem_odp *odp, *result = NULL;
 +	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
 +	u64 addr = io_virt & MLX5_IMR_MTT_MASK;
 +	int nentries = 0, start_idx = 0, ret;
 +	struct mlx5_ib_mr *mtt;
 +
 +	mutex_lock(&odp_mr->umem_mutex);
 +	odp = odp_lookup(addr, 1, mr);
 +
 +	mlx5_ib_dbg(dev, "io_virt:%llx bcnt:%zx addr:%llx odp:%p\n",
 +		    io_virt, bcnt, addr, odp);
 +
 +next_mr:
 +	if (likely(odp)) {
 +		if (nentries)
 +			nentries++;
 +	} else {
 +		odp = ib_alloc_odp_umem(odp_mr, addr,
 +					MLX5_IMR_MTT_SIZE);
 +		if (IS_ERR(odp)) {
 +			mutex_unlock(&odp_mr->umem_mutex);
 +			return ERR_CAST(odp);
 +		}
 +
 +		mtt = implicit_mr_alloc(mr->ibmr.pd, &odp->umem, 0,
 +					mr->access_flags);
 +		if (IS_ERR(mtt)) {
 +			mutex_unlock(&odp_mr->umem_mutex);
 +			ib_umem_release(&odp->umem);
 +			return ERR_CAST(mtt);
 +		}
 +
 +		odp->private = mtt;
 +		mtt->umem = &odp->umem;
 +		mtt->mmkey.iova = addr;
 +		mtt->parent = mr;
 +		INIT_WORK(&odp->work, mr_leaf_free_action);
 +
 +		if (!nentries)
 +			start_idx = addr >> MLX5_IMR_MTT_SHIFT;
 +		nentries++;
 +	}
 +
 +	/* Return first odp if region not covered by single one */
 +	if (likely(!result))
 +		result = odp;
 +
 +	addr += MLX5_IMR_MTT_SIZE;
 +	if (unlikely(addr < io_virt + bcnt)) {
 +		odp = odp_next(odp);
 +		if (odp && ib_umem_start(odp) != addr)
 +			odp = NULL;
 +		goto next_mr;
 +	}
 +
 +	if (unlikely(nentries)) {
 +		ret = mlx5_ib_update_xlt(mr, start_idx, nentries, 0,
 +					 MLX5_IB_UPD_XLT_INDIRECT |
 +					 MLX5_IB_UPD_XLT_ATOMIC);
 +		if (ret) {
 +			mlx5_ib_err(dev, "Failed to update PAS\n");
 +			result = ERR_PTR(ret);
 +		}
 +	}
 +
 +	mutex_unlock(&odp_mr->umem_mutex);
 +	return result;
  }
  
  struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
  					     struct ib_udata *udata,
  					     int access_flags)
  {
 -	struct mlx5_ib_dev *dev = to_mdev(pd->ibpd.device);
 -	struct ib_umem_odp *umem_odp;
  	struct mlx5_ib_mr *imr;
 -	int err;
 +	struct ib_umem *umem;
  
 -	umem_odp = ib_umem_odp_alloc_implicit(udata, access_flags);
 -	if (IS_ERR(umem_odp))
 -		return ERR_CAST(umem_odp);
 +	umem = ib_umem_get(udata, 0, 0, access_flags, 0);
 +	if (IS_ERR(umem))
 +		return ERR_CAST(umem);
  
 -	imr = mlx5_mr_cache_alloc(dev, MLX5_IMR_KSM_CACHE_ENTRY);
 +	imr = implicit_mr_alloc(&pd->ibpd, umem, 1, access_flags);
  	if (IS_ERR(imr)) {
 -		err = PTR_ERR(imr);
 -		goto out_umem;
 +		ib_umem_release(umem);
 +		return ERR_CAST(imr);
  	}
  
++<<<<<<< HEAD
 +	imr->umem = umem;
 +	init_waitqueue_head(&imr->q_leaf_free);
 +	atomic_set(&imr->num_leaf_free, 0);
 +	atomic_set(&imr->num_pending_prefetch, 0);
++=======
+ 	imr->ibmr.pd = &pd->ibpd;
+ 	imr->access_flags = access_flags;
+ 	imr->mmkey.iova = 0;
+ 	imr->umem = &umem_odp->umem;
+ 	imr->ibmr.lkey = imr->mmkey.key;
+ 	imr->ibmr.rkey = imr->mmkey.key;
+ 	imr->umem = &umem_odp->umem;
+ 	atomic_set(&imr->num_deferred_work, 0);
+ 	xa_init(&imr->implicit_children);
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  
 -	err = mlx5_ib_update_xlt(imr, 0,
 -				 mlx5_imr_ksm_entries,
 -				 MLX5_KSM_PAGE_SHIFT,
 -				 MLX5_IB_UPD_XLT_INDIRECT |
 -				 MLX5_IB_UPD_XLT_ZAP |
 -				 MLX5_IB_UPD_XLT_ENABLE);
 -	if (err)
 -		goto out_mr;
 -
 -	err = xa_err(xa_store(&dev->odp_mkeys, mlx5_base_mkey(imr->mmkey.key),
 -			      &imr->mmkey, GFP_KERNEL));
 -	if (err)
 -		goto out_mr;
 +	imr->is_odp_implicit = true;
  
 -	mlx5_ib_dbg(dev, "key %x mr %p\n", imr->mmkey.key, imr);
  	return imr;
 -out_mr:
 -	mlx5_ib_err(dev, "Failed to register MKEY %d\n", err);
 -	mlx5_mr_cache_free(dev, imr);
 -out_umem:
 -	ib_umem_odp_release(umem_odp);
 -	return ERR_PTR(err);
 +}
 +
 +static int mr_leaf_free(struct ib_umem_odp *umem_odp, u64 start, u64 end,
 +			void *cookie)
 +{
 +	struct mlx5_ib_mr *mr = umem_odp->private, *imr = cookie;
 +
 +	if (mr->parent != imr)
 +		return 0;
 +
 +	ib_umem_odp_unmap_dma_pages(umem_odp, ib_umem_start(umem_odp),
 +				    ib_umem_end(umem_odp));
 +
 +	if (umem_odp->dying)
 +		return 0;
 +
 +	WRITE_ONCE(umem_odp->dying, 1);
 +	atomic_inc(&imr->num_leaf_free);
 +	schedule_work(&umem_odp->work);
 +
 +	return 0;
  }
  
  void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
  {
++<<<<<<< HEAD
 +	struct ib_ucontext_per_mm *per_mm = mr_to_per_mm(imr);
 +
 +	down_read(&per_mm->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, 0, ULLONG_MAX,
 +				      mr_leaf_free, imr);
 +	up_read(&per_mm->umem_rwsem);
 +
 +	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
++=======
+ 	struct ib_umem_odp *odp_imr = to_ib_umem_odp(imr->umem);
+ 	struct mlx5_ib_dev *dev = imr->dev;
+ 	struct list_head destroy_list;
+ 	struct mlx5_ib_mr *mtt;
+ 	struct mlx5_ib_mr *tmp;
+ 	unsigned long idx;
+ 
+ 	INIT_LIST_HEAD(&destroy_list);
+ 
+ 	xa_erase(&dev->odp_mkeys, mlx5_base_mkey(imr->mmkey.key));
+ 	/*
+ 	 * This stops the SRCU protected page fault path from touching either
+ 	 * the imr or any children. The page fault path can only reach the
+ 	 * children xarray via the imr.
+ 	 */
+ 	synchronize_srcu(&dev->odp_srcu);
+ 
+ 	xa_lock(&imr->implicit_children);
+ 	xa_for_each (&imr->implicit_children, idx, mtt) {
+ 		__xa_erase(&imr->implicit_children, idx);
+ 		__xa_erase(&dev->odp_mkeys, mlx5_base_mkey(mtt->mmkey.key));
+ 		list_add(&mtt->odp_destroy.elm, &destroy_list);
+ 	}
+ 	xa_unlock(&imr->implicit_children);
+ 
+ 	/* Fence access to the child pointers via the pagefault thread */
+ 	synchronize_srcu(&dev->odp_srcu);
+ 
+ 	/*
+ 	 * num_deferred_work can only be incremented inside the odp_srcu, or
+ 	 * under xa_lock while the child is in the xarray. Thus at this point
+ 	 * it is only decreasing, and all work holding it is now on the wq.
+ 	 */
+ 	if (atomic_read(&imr->num_deferred_work)) {
+ 		flush_workqueue(system_unbound_wq);
+ 		WARN_ON(atomic_read(&imr->num_deferred_work));
+ 	}
+ 
+ 	list_for_each_entry_safe (mtt, tmp, &destroy_list, odp_destroy.elm)
+ 		free_implicit_child_mr(mtt, false);
+ 
+ 	mlx5_mr_cache_free(dev, imr);
+ 	ib_umem_odp_release(odp_imr);
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  }
  
 +#define MLX5_PF_FLAGS_PREFETCH  BIT(0)
  #define MLX5_PF_FLAGS_DOWNGRADE BIT(1)
 -static int pagefault_real_mr(struct mlx5_ib_mr *mr, struct ib_umem_odp *odp,
 -			     u64 user_va, size_t bcnt, u32 *bytes_mapped,
 -			     u32 flags)
 +static int pagefault_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
 +			u64 io_virt, size_t bcnt, u32 *bytes_mapped,
 +			u32 flags)
  {
 -	int current_seq, page_shift, ret, np;
 +	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
  	bool downgrade = flags & MLX5_PF_FLAGS_DOWNGRADE;
 +	bool prefetch = flags & MLX5_PF_FLAGS_PREFETCH;
 +	int npages = 0, current_seq, page_shift, ret, np;
  	u64 access_mask;
  	u64 start_idx, page_mask;
 +	struct ib_umem_odp *odp;
 +	size_t size;
 +
 +	if (!odp_mr->page_list) {
 +		odp = implicit_mr_get_data(mr, io_virt, bcnt);
 +
 +		if (IS_ERR(odp))
 +			return PTR_ERR(odp);
 +		mr = odp->private;
 +	} else {
 +		odp = odp_mr;
 +	}
 +
 +next_mr:
 +	size = min_t(size_t, bcnt, ib_umem_end(odp) - io_virt);
  
  	page_shift = odp->page_shift;
  	page_mask = ~(BIT(page_shift) - 1);
@@@ -1636,75 -1617,134 +1792,172 @@@ int mlx5_ib_odp_init(void
  
  struct prefetch_mr_work {
  	struct work_struct work;
 +	struct ib_pd *pd;
  	u32 pf_flags;
  	u32 num_sge;
 -	struct {
 -		u64 io_virt;
 -		struct mlx5_ib_mr *mr;
 -		size_t length;
 -	} frags[];
 +	struct ib_sge sg_list[0];
  };
  
 -static void destroy_prefetch_work(struct prefetch_mr_work *work)
 +static void num_pending_prefetch_dec(struct mlx5_ib_dev *dev,
 +				     struct ib_sge *sg_list, u32 num_sge,
 +				     u32 from)
  {
  	u32 i;
++<<<<<<< HEAD
++=======
+ 
+ 	for (i = 0; i < work->num_sge; ++i)
+ 		atomic_dec(&work->frags[i].mr->num_deferred_work);
+ 	kvfree(work);
+ }
+ 
+ static struct mlx5_ib_mr *
+ get_prefetchable_mr(struct ib_pd *pd, enum ib_uverbs_advise_mr_advice advice,
+ 		    u32 lkey)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct mlx5_core_mkey *mmkey;
+ 	struct ib_umem_odp *odp;
+ 	struct mlx5_ib_mr *mr;
+ 
+ 	lockdep_assert_held(&dev->odp_srcu);
+ 
+ 	mmkey = xa_load(&dev->odp_mkeys, mlx5_base_mkey(lkey));
+ 	if (!mmkey || mmkey->key != lkey || mmkey->type != MLX5_MKEY_MR)
+ 		return NULL;
+ 
+ 	mr = container_of(mmkey, struct mlx5_ib_mr, mmkey);
+ 
+ 	if (mr->ibmr.pd != pd)
+ 		return NULL;
+ 
+ 	/*
+ 	 * Implicit child MRs are internal and userspace should not refer to
+ 	 * them.
+ 	 */
+ 	if (mr->parent)
+ 		return NULL;
+ 
+ 	odp = to_ib_umem_odp(mr->umem);
+ 
+ 	/* prefetch with write-access must be supported by the MR */
+ 	if (advice == IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH_WRITE &&
+ 	    !odp->umem.writable)
+ 		return NULL;
+ 
+ 	return mr;
+ }
+ 
+ static void mlx5_ib_prefetch_mr_work(struct work_struct *w)
+ {
+ 	struct prefetch_mr_work *work =
+ 		container_of(w, struct prefetch_mr_work, work);
+ 	u32 bytes_mapped = 0;
+ 	u32 i;
+ 
+ 	for (i = 0; i < work->num_sge; ++i)
+ 		pagefault_mr(work->frags[i].mr, work->frags[i].io_virt,
+ 			     work->frags[i].length, &bytes_mapped,
+ 			     work->pf_flags);
+ 
+ 	destroy_prefetch_work(work);
+ }
+ 
+ static bool init_prefetch_work(struct ib_pd *pd,
+ 			       enum ib_uverbs_advise_mr_advice advice,
+ 			       u32 pf_flags, struct prefetch_mr_work *work,
+ 			       struct ib_sge *sg_list, u32 num_sge)
+ {
+ 	u32 i;
+ 
+ 	INIT_WORK(&work->work, mlx5_ib_prefetch_mr_work);
+ 	work->pf_flags = pf_flags;
+ 
+ 	for (i = 0; i < num_sge; ++i) {
+ 		work->frags[i].io_virt = sg_list[i].addr;
+ 		work->frags[i].length = sg_list[i].length;
+ 		work->frags[i].mr =
+ 			get_prefetchable_mr(pd, advice, sg_list[i].lkey);
+ 		if (!work->frags[i].mr) {
+ 			work->num_sge = i - 1;
+ 			if (i)
+ 				destroy_prefetch_work(work);
+ 			return false;
+ 		}
+ 
+ 		/* Keep the MR pointer will valid outside the SRCU */
+ 		atomic_inc(&work->frags[i].mr->num_deferred_work);
+ 	}
+ 	work->num_sge = num_sge;
+ 	return true;
+ }
+ 
+ static int mlx5_ib_prefetch_sg_list(struct ib_pd *pd,
+ 				    enum ib_uverbs_advise_mr_advice advice,
+ 				    u32 pf_flags, struct ib_sge *sg_list,
+ 				    u32 num_sge)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	u32 bytes_mapped = 0;
++>>>>>>> 5256edcb98a1 (RDMA/mlx5: Rework implicit ODP destroy)
  	int srcu_key;
 -	int ret = 0;
 +
 +	srcu_key = srcu_read_lock(&dev->mr_srcu);
 +
 +	for (i = from; i < num_sge; ++i) {
 +		struct mlx5_core_mkey *mmkey;
 +		struct mlx5_ib_mr *mr;
 +
 +		mmkey = xa_load(&dev->mdev->priv.mkey_table,
 +				mlx5_base_mkey(sg_list[i].lkey));
 +		mr = container_of(mmkey, struct mlx5_ib_mr, mmkey);
 +		atomic_dec(&mr->num_pending_prefetch);
 +	}
 +
 +	srcu_read_unlock(&dev->mr_srcu, srcu_key);
 +}
 +
 +static bool num_pending_prefetch_inc(struct ib_pd *pd,
 +				     struct ib_sge *sg_list, u32 num_sge)
 +{
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	bool ret = true;
  	u32 i;
  
 -	srcu_key = srcu_read_lock(&dev->odp_srcu);
  	for (i = 0; i < num_sge; ++i) {
 +		struct mlx5_core_mkey *mmkey;
  		struct mlx5_ib_mr *mr;
  
 -		mr = get_prefetchable_mr(pd, advice, sg_list[i].lkey);
 -		if (!mr) {
 -			ret = -ENOENT;
 -			goto out;
 +		mmkey = xa_load(&dev->mdev->priv.mkey_table,
 +				mlx5_base_mkey(sg_list[i].lkey));
 +		if (!mmkey || mmkey->key != sg_list[i].lkey) {
 +			ret = false;
 +			break;
  		}
 -		ret = pagefault_mr(mr, sg_list[i].addr, sg_list[i].length,
 -				   &bytes_mapped, pf_flags);
 -		if (ret < 0)
 -			goto out;
 +
 +		if (mmkey->type != MLX5_MKEY_MR) {
 +			ret = false;
 +			break;
 +		}
 +
 +		mr = container_of(mmkey, struct mlx5_ib_mr, mmkey);
 +
 +		if (mr->ibmr.pd != pd) {
 +			ret = false;
 +			break;
 +		}
 +
 +		if (!mr->live) {
 +			ret = false;
 +			break;
 +		}
 +
 +		atomic_inc(&mr->num_pending_prefetch);
  	}
 -	ret = 0;
  
 -out:
 -	srcu_read_unlock(&dev->odp_srcu, srcu_key);
 +	if (!ret)
 +		num_pending_prefetch_dec(dev, sg_list, i, 0);
 +
  	return ret;
  }
  
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
diff --git a/include/rdma/ib_umem_odp.h b/include/rdma/ib_umem_odp.h
index 8811ea99d902..657285908e0c 100644
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@ -75,9 +75,7 @@ struct ib_umem_odp {
 	struct umem_odp_node	interval_tree;
 
 	struct completion	notifier_completion;
-	int			dying;
 	unsigned int		page_shift;
-	struct work_struct	work;
 };
 
 static inline struct ib_umem_odp *to_ib_umem_odp(struct ib_umem *umem)
