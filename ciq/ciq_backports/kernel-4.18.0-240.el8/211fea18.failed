io_uring: remove redundant variable pointer nxt and io_wq_assign_next call

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Colin Ian King <colin.king@canonical.com>
commit 211fea18a7bb9b8d51cb5d2b9cbe5583af256609
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/211fea18.failed

An earlier commit "io_uring: remove @nxt from handlers" removed the
setting of pointer nxt and now it is always null, hence the non-null
check and call to io_wq_assign_next is redundant and can be removed.

Addresses-Coverity: ("'Constant' variable guard")
	Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
	Signed-off-by: Colin Ian King <colin.king@canonical.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 211fea18a7bb9b8d51cb5d2b9cbe5583af256609)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 2afa3b27779e,20662bbc0507..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1555,152 -2836,213 +1555,168 @@@ static int io_prep_sfr(struct io_kiocb 
  	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
  		return -EINVAL;
  
 -	req->sync.flags = READ_ONCE(sqe->fsync_flags);
 -	if (unlikely(req->sync.flags & ~IORING_FSYNC_DATASYNC))
 -		return -EINVAL;
 -
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->len);
 -	return 0;
 -}
 -
 -static bool io_req_cancelled(struct io_kiocb *req)
 -{
 -	if (req->work.flags & IO_WQ_WORK_CANCEL) {
 -		req_set_fail_links(req);
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_put_req(req);
 -		return true;
 -	}
 -
 -	return false;
 +	return ret;
  }
  
 -static void __io_fsync(struct io_kiocb *req)
 +static int io_sync_file_range(struct io_kiocb *req,
 +			      const struct io_uring_sqe *sqe,
 +			      bool force_nonblock)
  {
 -	loff_t end = req->sync.off + req->sync.len;
 +	loff_t sqe_off;
 +	loff_t sqe_len;
 +	unsigned flags;
  	int ret;
  
 -	ret = vfs_fsync_range(req->file, req->sync.off,
 -				end > 0 ? end : LLONG_MAX,
 -				req->sync.flags & IORING_FSYNC_DATASYNC);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -}
 +	ret = io_prep_sfr(req, sqe);
 +	if (ret)
 +		return ret;
  
 -static void io_fsync_finish(struct io_wq_work **workptr)
++<<<<<<< HEAD
++=======
++
++static void io_sync_file_range_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
 -	__io_fsync(req);
 -	io_steal_work(req, workptr);
++	__io_sync_file_range(req);
++	io_put_req(req); /* put submission ref */
+ }
+ 
 -static int io_fsync(struct io_kiocb *req, bool force_nonblock)
++static int io_sync_file_range(struct io_kiocb *req, bool force_nonblock)
+ {
 -	/* fsync always requires a blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_fsync_finish;
++>>>>>>> 211fea18a7bb (io_uring: remove redundant variable pointer nxt and io_wq_assign_next call)
 +	/* sync_file_range always requires a blocking context */
 +	if (force_nonblock)
  		return -EAGAIN;
 -	}
 -	__io_fsync(req);
 -	return 0;
 -}
  
 -static void __io_fallocate(struct io_kiocb *req)
 -{
 -	int ret;
 +	sqe_off = READ_ONCE(sqe->off);
 +	sqe_len = READ_ONCE(sqe->len);
 +	flags = READ_ONCE(sqe->sync_range_flags);
  
 -	current->signal->rlim[RLIMIT_FSIZE].rlim_cur = req->fsize;
 -	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
 -				req->sync.len);
 -	current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 +	ret = sync_file_range(req->rw.ki_filp, sqe_off, sqe_len, flags);
 +
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
  	io_put_req(req);
 +	return 0;
  }
  
 -static void io_fallocate_finish(struct io_wq_work **workptr)
 +#if defined(CONFIG_NET)
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
  {
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_fallocate(req);
 -	io_steal_work(req, workptr);
 -}
 +	struct socket *sock;
 +	int ret;
  
 -static int io_fallocate_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
 +	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
  		return -EINVAL;
  
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->addr);
 -	req->sync.mode = READ_ONCE(sqe->len);
 -	req->fsize = rlimit(RLIMIT_FSIZE);
 -	return 0;
 -}
 +	sock = sock_from_file(req->file, &ret);
 +	if (sock) {
 +		struct user_msghdr __user *msg;
 +		unsigned flags;
  
 -static int io_fallocate(struct io_kiocb *req, bool force_nonblock)
 -{
 -	/* fallocate always requiring blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_fallocate_finish;
 -		return -EAGAIN;
 +		flags = READ_ONCE(sqe->msg_flags);
 +		if (flags & MSG_DONTWAIT)
 +			req->flags |= REQ_F_NOWAIT;
 +		else if (force_nonblock)
 +			flags |= MSG_DONTWAIT;
 +
 +		msg = (struct user_msghdr __user *) (unsigned long)
 +			READ_ONCE(sqe->addr);
 +
 +		ret = fn(sock, msg, flags);
 +		if (force_nonblock && ret == -EAGAIN)
 +			return ret;
 +		if (ret == -ERESTARTSYS)
 +			ret = -EINTR;
  	}
  
 -	__io_fallocate(req);
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
  	return 0;
  }
 +#endif
  
 -static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_sendmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		      bool force_nonblock)
  {
 -	const char __user *fname;
 -	int ret;
 +#if defined(CONFIG_NET)
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_sendmsg_sock);
 +#else
 +	return -EOPNOTSUPP;
 +#endif
 +}
  
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 +static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		      bool force_nonblock)
 +{
 +#if defined(CONFIG_NET)
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_recvmsg_sock);
 +#else
 +	return -EOPNOTSUPP;
 +#endif
 +}
  
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	req->open.how.mode = READ_ONCE(sqe->len);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	req->open.how.flags = READ_ONCE(sqe->open_flags);
 +static void io_poll_remove_one(struct io_kiocb *req)
 +{
 +	struct io_poll_iocb *poll = &req->poll;
  
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 +	spin_lock(&poll->head->lock);
 +	WRITE_ONCE(poll->canceled, true);
 +	if (!list_empty(&poll->wait.entry)) {
 +		list_del_init(&poll->wait.entry);
 +		io_queue_async_work(req->ctx, req);
  	}
 +	spin_unlock(&poll->head->lock);
  
 -	req->open.nofile = rlimit(RLIMIT_NOFILE);
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	return 0;
 +	list_del_init(&req->list);
  }
  
 -static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static void io_poll_remove_all(struct io_ring_ctx *ctx)
  {
 -	struct open_how __user *how;
 -	const char __user *fname;
 -	size_t len;
 -	int ret;
 -
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	len = READ_ONCE(sqe->len);
 -
 -	if (len < OPEN_HOW_SIZE_VER0)
 -		return -EINVAL;
 -
 -	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
 -					len);
 -	if (ret)
 -		return ret;
 -
 -	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
 -		req->open.how.flags |= O_LARGEFILE;
 +	struct io_kiocb *req;
  
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 +	spin_lock_irq(&ctx->completion_lock);
 +	while (!list_empty(&ctx->cancel_list)) {
 +		req = list_first_entry(&ctx->cancel_list, struct io_kiocb,list);
 +		io_poll_remove_one(req);
  	}
 -
 -	req->open.nofile = rlimit(RLIMIT_NOFILE);
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	return 0;
 +	spin_unlock_irq(&ctx->completion_lock);
  }
  
 -static int io_openat2(struct io_kiocb *req, bool force_nonblock)
 +/*
 + * Find a running poll command that matches one specified in sqe->addr,
 + * and remove it if found.
 + */
 +static int io_poll_remove(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 -	struct open_flags op;
 -	struct file *file;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 -
 -	ret = build_open_flags(&req->open.how, &op);
 -	if (ret)
 -		goto err;
 +	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_kiocb *poll_req, *next;
 +	int ret = -ENOENT;
  
 -	ret = __get_unused_fd_flags(req->open.how.flags, req->open.nofile);
 -	if (ret < 0)
 -		goto err;
 +	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 +		return -EINVAL;
 +	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 +	    sqe->poll_events)
 +		return -EINVAL;
  
 -	file = do_filp_open(req->open.dfd, req->open.filename, &op);
 -	if (IS_ERR(file)) {
 -		put_unused_fd(ret);
 -		ret = PTR_ERR(file);
 -	} else {
 -		fsnotify_open(file);
 -		fd_install(ret, file);
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_for_each_entry_safe(poll_req, next, &ctx->cancel_list, list) {
 +		if (READ_ONCE(sqe->addr) == poll_req->user_data) {
 +			io_poll_remove_one(poll_req);
 +			ret = 0;
 +			break;
 +		}
  	}
 -err:
 -	putname(req->open.filename);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
  	io_put_req(req);
  	return 0;
  }
* Unmerged path fs/io_uring.c
