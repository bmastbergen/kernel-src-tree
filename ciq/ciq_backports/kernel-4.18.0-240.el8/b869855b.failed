KVM: x86/mmu: Move fast_cr3_switch() side effects to __kvm_mmu_new_cr3()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit b869855badd1387bd12415e4d5571e931825b546
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b869855b.failed

Handle the side effects of a fast CR3 (PGD) switch up a level in
__kvm_mmu_new_cr3(), which is the only caller of fast_cr3_switch().

This consolidates handling all side effects in __kvm_mmu_new_cr3()
(where freeing the current root when KVM can't do a fast switch is
already handled), and ameliorates the pain of adding a second boolean in
a future patch to provide a separate "skip" override for the MMU sync.

	Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Message-Id: <20200320212833.3507-31-sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit b869855badd1387bd12415e4d5571e931825b546)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index dbd97923dc2c,6a939204d467..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -4357,39 -4294,9 +4356,45 @@@ static bool fast_cr3_switch(struct kvm_
  	 * later if necessary.
  	 */
  	if (mmu->shadow_root_level >= PT64_ROOT_4LEVEL &&
++<<<<<<< HEAD
 +	    mmu->root_level >= PT64_ROOT_4LEVEL) {
 +		if (mmu_check_root(vcpu, new_cr3 >> PAGE_SHIFT))
 +			return false;
 +
 +		if (cached_root_available(vcpu, new_cr3, new_role)) {
 +			/*
 +			 * It is possible that the cached previous root page is
 +			 * obsolete because of a change in the MMU generation
 +			 * number. However, changing the generation number is
 +			 * accompanied by KVM_REQ_MMU_RELOAD, which will free
 +			 * the root set here and allocate a new one.
 +			 */
 +			kvm_make_request(KVM_REQ_LOAD_CR3, vcpu);
 +			if (!skip_tlb_flush) {
 +				kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);
 +				kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 +			}
 +
 +			/*
 +			 * The last MMIO access's GVA and GPA are cached in the
 +			 * VCPU. When switching to a new CR3, that GVA->GPA
 +			 * mapping may no longer be valid. So clear any cached
 +			 * MMIO info even when we don't need to sync the shadow
 +			 * page tables.
 +			 */
 +			vcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);
 +
 +			__clear_sp_write_flooding_count(
 +				page_header(mmu->root_hpa));
 +
 +			return true;
 +		}
 +	}
++=======
+ 	    mmu->root_level >= PT64_ROOT_4LEVEL)
+ 		return !mmu_check_root(vcpu, new_cr3 >> PAGE_SHIFT) &&
+ 		       cached_root_available(vcpu, new_cr3, new_role);
++>>>>>>> b869855badd1 (KVM: x86/mmu: Move fast_cr3_switch() side effects to __kvm_mmu_new_cr3())
  
  	return false;
  }
* Unmerged path arch/x86/kvm/mmu/mmu.c
