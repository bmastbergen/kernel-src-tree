hugetlbfs: add arch_hugetlb_valid_size

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mike Kravetz <mike.kravetz@oracle.com>
commit ae94da898133947c2d1f005da10838478e4548db
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ae94da89.failed

Patch series "Clean up hugetlb boot command line processing", v4.

Longpeng(Mike) reported a weird message from hugetlb command line
processing and proposed a solution [1].  While the proposed patch does
address the specific issue, there are other related issues in command line
processing.  As hugetlbfs evolved, updates to command line processing have
been made to meet immediate needs and not necessarily in a coordinated
manner.  The result is that some processing is done in arch specific code,
some is done in arch independent code and coordination is problematic.
Semantics can vary between architectures.

The patch series does the following:
- Define arch specific arch_hugetlb_valid_size routine used to validate
  passed huge page sizes.
- Move hugepagesz= command line parsing out of arch specific code and into
  an arch independent routine.
- Clean up command line processing to follow desired semantics and
  document those semantics.

[1] https://lore.kernel.org/linux-mm/20200305033014.1152-1-longpeng2@huawei.com

This patch (of 3):

The architecture independent routine hugetlb_default_setup sets up the
default huge pages size.  It has no way to verify if the passed value is
valid, so it accepts it and attempts to validate at a later time.  This
requires undocumented cooperation between the arch specific and arch
independent code.

For architectures that support more than one huge page size, provide a
routine arch_hugetlb_valid_size to validate a huge page size.
hugetlb_default_setup can use this to validate passed values.

arch_hugetlb_valid_size will also be used in a subsequent patch to move
processing of the "hugepagesz=" in arch specific code to a common routine
in arch independent code.

	Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>	[s390]
	Acked-by: Will Deacon <will@kernel.org>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Paul Walmsley <paul.walmsley@sifive.com>
	Cc: Palmer Dabbelt <palmer@dabbelt.com>
	Cc: Albert Ou <aou@eecs.berkeley.edu>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Vasily Gorbik <gor@linux.ibm.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: David S. Miller <davem@davemloft.net>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Longpeng <longpeng2@huawei.com>
	Cc: Christophe Leroy <christophe.leroy@c-s.fr>
	Cc: Randy Dunlap <rdunlap@infradead.org>
	Cc: Mina Almasry <almasrymina@google.com>
	Cc: Peter Xu <peterx@redhat.com>
	Cc: Nitesh Narayan Lal <nitesh@redhat.com>
	Cc: Anders Roxell <anders.roxell@linaro.org>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
	Cc: Qian Cai <cai@lca.pw>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
Link: http://lkml.kernel.org/r/20200428205614.246260-1-mike.kravetz@oracle.com
Link: http://lkml.kernel.org/r/20200428205614.246260-2-mike.kravetz@oracle.com
Link: http://lkml.kernel.org/r/20200417185049.275845-1-mike.kravetz@oracle.com
Link: http://lkml.kernel.org/r/20200417185049.275845-2-mike.kravetz@oracle.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ae94da898133947c2d1f005da10838478e4548db)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/mm/hugetlbpage.c
#	arch/riscv/mm/hugetlbpage.c
diff --cc arch/powerpc/mm/hugetlbpage.c
index 8110b9827c69,de54d2a37830..000000000000
--- a/arch/powerpc/mm/hugetlbpage.c
+++ b/arch/powerpc/mm/hugetlbpage.c
@@@ -588,50 -558,19 +588,63 @@@ unsigned long vma_mmu_pagesize(struct v
  	return vma_kernel_pagesize(vma);
  }
  
++<<<<<<< HEAD
 +static inline bool is_power_of_4(unsigned long x)
 +{
 +	if (is_power_of_2(x))
 +		return (__ilog2(x) % 2) ? false : true;
 +	return false;
 +}
 +
 +static int __init add_huge_page_size(unsigned long long size)
++=======
+ bool __init arch_hugetlb_valid_size(unsigned long size)
++>>>>>>> ae94da898133 (hugetlbfs: add arch_hugetlb_valid_size)
  {
  	int shift = __ffs(size);
  	int mmu_psize;
  
  	/* Check that it is a page size supported by the hardware and
  	 * that it fits within pagetable and slice limits. */
++<<<<<<< HEAD
 +	if (size <= PAGE_SIZE)
 +		return -EINVAL;
 +#if defined(CONFIG_PPC_FSL_BOOK3E)
 +	if (!is_power_of_4(size))
 +		return -EINVAL;
 +#elif !defined(CONFIG_PPC_8xx)
 +	if (!is_power_of_2(size) || (shift > SLICE_HIGH_SHIFT))
 +		return -EINVAL;
 +#endif
 +
 +	if ((mmu_psize = shift_to_mmu_psize(shift)) < 0)
 +		return -EINVAL;
 +
 +#ifdef CONFIG_PPC_BOOK3S_64
 +	/*
 +	 * We need to make sure that for different page sizes reported by
 +	 * firmware we only add hugetlb support for page sizes that can be
 +	 * supported by linux page table layout.
 +	 * For now we have
 +	 * Radix: 2M and 1G
 +	 * Hash: 16M and 16G
 +	 */
 +	if (radix_enabled()) {
 +		if (mmu_psize != MMU_PAGE_2M && mmu_psize != MMU_PAGE_1G)
 +			return -EINVAL;
 +	} else {
 +		if (mmu_psize != MMU_PAGE_16M && mmu_psize != MMU_PAGE_16G)
 +			return -EINVAL;
 +	}
 +#endif
++=======
+ 	if (size <= PAGE_SIZE || !is_power_of_2(size))
+ 		return false;
+ 
+ 	mmu_psize = check_and_get_huge_psize(shift);
+ 	if (mmu_psize < 0)
+ 		return false;
++>>>>>>> ae94da898133 (hugetlbfs: add arch_hugetlb_valid_size)
  
  	BUG_ON(mmu_psize_defs[mmu_psize].shift != shift);
  
* Unmerged path arch/riscv/mm/hugetlbpage.c
diff --git a/arch/arm64/mm/hugetlbpage.c b/arch/arm64/mm/hugetlbpage.c
index 2e6c432a5f0c..0f937f5c8f5a 100644
--- a/arch/arm64/mm/hugetlbpage.c
+++ b/arch/arm64/mm/hugetlbpage.c
@@ -450,17 +450,26 @@ static int __init hugetlbpage_init(void)
 }
 arch_initcall(hugetlbpage_init);
 
-static __init int setup_hugepagesz(char *opt)
+bool __init arch_hugetlb_valid_size(unsigned long size)
 {
-	unsigned long ps = memparse(opt, &opt);
-
-	switch (ps) {
+	switch (size) {
 #ifdef CONFIG_ARM64_4K_PAGES
 	case PUD_SIZE:
 #endif
 	case CONT_PMD_SIZE:
 	case PMD_SIZE:
 	case CONT_PTE_SIZE:
+		return true;
+	}
+
+	return false;
+}
+
+static __init int setup_hugepagesz(char *opt)
+{
+	unsigned long ps = memparse(opt, &opt);
+
+	if (arch_hugetlb_valid_size(ps)) {
 		add_huge_page_size(ps);
 		return 1;
 	}
* Unmerged path arch/powerpc/mm/hugetlbpage.c
* Unmerged path arch/riscv/mm/hugetlbpage.c
diff --git a/arch/s390/mm/hugetlbpage.c b/arch/s390/mm/hugetlbpage.c
index 5674710a4841..9fb34a810769 100644
--- a/arch/s390/mm/hugetlbpage.c
+++ b/arch/s390/mm/hugetlbpage.c
@@ -251,16 +251,24 @@ follow_huge_pud(struct mm_struct *mm, unsigned long address,
 	return pud_page(*pud) + ((address & ~PUD_MASK) >> PAGE_SHIFT);
 }
 
+bool __init arch_hugetlb_valid_size(unsigned long size)
+{
+	if (MACHINE_HAS_EDAT1 && size == PMD_SIZE)
+		return true;
+	else if (MACHINE_HAS_EDAT2 && size == PUD_SIZE)
+		return true;
+	else
+		return false;
+}
+
 static __init int setup_hugepagesz(char *opt)
 {
 	unsigned long size;
 	char *string = opt;
 
 	size = memparse(opt, &opt);
-	if (MACHINE_HAS_EDAT1 && size == PMD_SIZE) {
-		hugetlb_add_hstate(PMD_SHIFT - PAGE_SHIFT);
-	} else if (MACHINE_HAS_EDAT2 && size == PUD_SIZE) {
-		hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
+	if (arch_hugetlb_valid_size(size)) {
+		hugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);
 	} else {
 		hugetlb_bad_size();
 		pr_err("hugepagesz= specifies an unsupported page size %s\n",
diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c
index 578ec3da410a..662daeaa9140 100644
--- a/arch/sparc/mm/init_64.c
+++ b/arch/sparc/mm/init_64.c
@@ -361,16 +361,11 @@ static void __init pud_huge_patch(void)
 	__asm__ __volatile__("flush %0" : : "r" (addr));
 }
 
-static int __init setup_hugepagesz(char *string)
+bool __init arch_hugetlb_valid_size(unsigned long size)
 {
-	unsigned long long hugepage_size;
-	unsigned int hugepage_shift;
+	unsigned int hugepage_shift = ilog2(size);
 	unsigned short hv_pgsz_idx;
 	unsigned int hv_pgsz_mask;
-	int rc = 0;
-
-	hugepage_size = memparse(string, &string);
-	hugepage_shift = ilog2(hugepage_size);
 
 	switch (hugepage_shift) {
 	case HPAGE_16GB_SHIFT:
@@ -398,7 +393,20 @@ static int __init setup_hugepagesz(char *string)
 		hv_pgsz_mask = 0;
 	}
 
-	if ((hv_pgsz_mask & cpu_pgsz_mask) == 0U) {
+	if ((hv_pgsz_mask & cpu_pgsz_mask) == 0U)
+		return false;
+
+	return true;
+}
+
+static int __init setup_hugepagesz(char *string)
+{
+	unsigned long long hugepage_size;
+	int rc = 0;
+
+	hugepage_size = memparse(string, &string);
+
+	if (!arch_hugetlb_valid_size((unsigned long)hugepage_size)) {
 		hugetlb_bad_size();
 		pr_err("hugepagesz=%llu not supported by MMU.\n",
 			hugepage_size);
diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c
index 00b296617ca4..1f9a589b1bd2 100644
--- a/arch/x86/mm/hugetlbpage.c
+++ b/arch/x86/mm/hugetlbpage.c
@@ -186,13 +186,22 @@ hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
 #endif /* CONFIG_HUGETLB_PAGE */
 
 #ifdef CONFIG_X86_64
+bool __init arch_hugetlb_valid_size(unsigned long size)
+{
+	if (size == PMD_SIZE)
+		return true;
+	else if (size == PUD_SIZE && boot_cpu_has(X86_FEATURE_GBPAGES))
+		return true;
+	else
+		return false;
+}
+
 static __init int setup_hugepagesz(char *opt)
 {
 	unsigned long ps = memparse(opt, &opt);
-	if (ps == PMD_SIZE) {
-		hugetlb_add_hstate(PMD_SHIFT - PAGE_SHIFT);
-	} else if (ps == PUD_SIZE && boot_cpu_has(X86_FEATURE_GBPAGES)) {
-		hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
+
+	if (arch_hugetlb_valid_size(ps)) {
+		hugetlb_add_hstate(ilog2(ps) - PAGE_SHIFT);
 	} else {
 		hugetlb_bad_size();
 		printk(KERN_ERR "hugepagesz: Unsupported page size %lu M\n",
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 75b7d65145a4..88d686f7f113 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -384,6 +384,7 @@ int __init alloc_bootmem_huge_page(struct hstate *h);
 
 void __init hugetlb_bad_size(void);
 void __init hugetlb_add_hstate(unsigned order);
+bool __init arch_hugetlb_valid_size(unsigned long size);
 struct hstate *size_to_hstate(unsigned long size);
 
 #ifndef HUGE_MAX_HSTATE
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 449fb9efecb9..0cea0c4c4d00 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -2869,6 +2869,12 @@ static int __init hugetlb_init(void)
 }
 subsys_initcall(hugetlb_init);
 
+/* Overwritten by architectures with more huge page sizes */
+bool __init __attribute((weak)) arch_hugetlb_valid_size(unsigned long size)
+{
+	return size == HPAGE_SIZE;
+}
+
 /* Should be called on processing a hugepagesz=... option */
 void __init hugetlb_bad_size(void)
 {
@@ -2944,12 +2950,21 @@ static int __init hugetlb_nrpages_setup(char *s)
 }
 __setup("hugepages=", hugetlb_nrpages_setup);
 
-static int __init hugetlb_default_setup(char *s)
+static int __init default_hugepagesz_setup(char *s)
 {
-	default_hstate_size = memparse(s, &s);
+	unsigned long size;
+
+	size = (unsigned long)memparse(s, NULL);
+
+	if (!arch_hugetlb_valid_size(size)) {
+		pr_err("HugeTLB: unsupported default_hugepagesz %s\n", s);
+		return 0;
+	}
+
+	default_hstate_size = size;
 	return 1;
 }
-__setup("default_hugepagesz=", hugetlb_default_setup);
+__setup("default_hugepagesz=", default_hugepagesz_setup);
 
 static unsigned int cpuset_mems_nr(unsigned int *array)
 {
