crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Eric Biggers <ebiggers@google.com>
commit 674f368a952c48ede71784935a799a5205b92b6c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/674f368a.failed

The CRYPTO_TFM_RES_BAD_KEY_LEN flag was apparently meant as a way to
make the ->setkey() functions provide more information about errors.

However, no one actually checks for this flag, which makes it pointless.

Also, many algorithms fail to set this flag when given a bad length key.
Reviewing just the generic implementations, this is the case for
aes-fixed-time, cbcmac, echainiv, nhpoly1305, pcrypt, rfc3686, rfc4309,
rfc7539, rfc7539esp, salsa20, seqiv, and xcbc.  But there are probably
many more in arch/*/crypto/ and drivers/crypto/.

Some algorithms can even set this flag when the key is the correct
length.  For example, authenc and authencesn set it when the key payload
is malformed in any way (not just a bad length), the atmel-sha and ccree
drivers can set it if a memory allocation fails, and the chelsio driver
sets it for bad auth tag lengths, not just bad key lengths.

So even if someone actually wanted to start checking this flag (which
seems unlikely, since it's been unused for a long time), there would be
a lot of work needed to get it working correctly.  But it would probably
be much better to go back to the drawing board and just define different
return values, like -EINVAL if the key is invalid for the algorithm vs.
-EKEYREJECTED if the key was rejected by a policy like "no weak keys".
That would be much simpler, less error-prone, and easier to test.

So just remove this flag.

	Signed-off-by: Eric Biggers <ebiggers@google.com>
	Reviewed-by: Horia GeantÄƒ <horia.geanta@nxp.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 674f368a952c48ede71784935a799a5205b92b6c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/crypto/aes-glue.c
#	arch/arm64/crypto/ghash-ce-glue.c
#	arch/powerpc/crypto/aes-spe-glue.c
#	arch/s390/crypto/aes_s390.c
#	arch/s390/crypto/paes_s390.c
#	arch/x86/crypto/blake2s-glue.c
#	crypto/aes_generic.c
#	crypto/blake2b_generic.c
#	crypto/blake2s_generic.c
#	crypto/cipher.c
#	crypto/essiv.c
#	crypto/xxhash_generic.c
#	drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c
#	drivers/crypto/allwinner/sun8i-ss/sun8i-ss-cipher.c
#	drivers/crypto/amlogic/amlogic-gxl-cipher.c
#	drivers/crypto/atmel-aes.c
#	drivers/crypto/axis/artpec6_crypto.c
#	drivers/crypto/bcm/cipher.c
#	drivers/crypto/caam/caamalg.c
#	drivers/crypto/caam/caamalg_qi.c
#	drivers/crypto/caam/caamalg_qi2.c
#	drivers/crypto/caam/caamhash.c
#	drivers/crypto/cavium/cpt/cptvf_algs.c
#	drivers/crypto/cavium/nitrox/nitrox_aead.c
#	drivers/crypto/cavium/nitrox/nitrox_skcipher.c
#	drivers/crypto/ccp/ccp-crypto-aes.c
#	drivers/crypto/ccree/cc_aead.c
#	drivers/crypto/ccree/cc_cipher.c
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/geode-aes.c
#	drivers/crypto/inside-secure/safexcel_cipher.c
#	drivers/crypto/inside-secure/safexcel_hash.c
#	drivers/crypto/marvell/cipher.c
#	drivers/crypto/mediatek/mtk-aes.c
#	drivers/crypto/n2_core.c
#	drivers/crypto/padlock-aes.c
#	drivers/crypto/picoxcell_crypto.c
#	drivers/crypto/rockchip/rk3288_crypto_skcipher.c
#	drivers/crypto/sunxi-ss/sun4i-ss-cipher.c
#	drivers/crypto/talitos.c
#	drivers/crypto/ux500/cryp/cryp_core.c
#	drivers/crypto/virtio/virtio_crypto_algs.c
#	include/crypto/internal/des.h
#	include/linux/crypto.h
diff --cc arch/arm64/crypto/aes-glue.c
index 33579a5b636c,ed5409c6abf4..000000000000
--- a/arch/arm64/crypto/aes-glue.c
+++ b/arch/arm64/crypto/aes-glue.c
@@@ -121,11 -131,13 +121,17 @@@ struct mac_desc_ctx 
  static int skcipher_aes_setkey(struct crypto_skcipher *tfm, const u8 *in_key,
  			       unsigned int key_len)
  {
++<<<<<<< HEAD
 +	return aes_setkey(crypto_skcipher_tfm(tfm), in_key, key_len);
++=======
+ 	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
+ 
+ 	return aes_expandkey(ctx, in_key, key_len);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  
 -static int __maybe_unused xts_set_key(struct crypto_skcipher *tfm,
 -				      const u8 *in_key, unsigned int key_len)
 +static int xts_set_key(struct crypto_skcipher *tfm, const u8 *in_key,
 +		       unsigned int key_len)
  {
  	struct crypto_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
  	int ret;
@@@ -138,14 -150,29 +144,33 @@@
  	if (!ret)
  		ret = aes_expandkey(&ctx->key2, &in_key[key_len / 2],
  				    key_len / 2);
- 	if (!ret)
- 		return 0;
- 
- 	crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
- 	return -EINVAL;
+ 	return ret;
  }
  
++<<<<<<< HEAD
 +static int ecb_encrypt(struct skcipher_request *req)
++=======
+ static int __maybe_unused essiv_cbc_set_key(struct crypto_skcipher *tfm,
+ 					    const u8 *in_key,
+ 					    unsigned int key_len)
+ {
+ 	struct crypto_aes_essiv_cbc_ctx *ctx = crypto_skcipher_ctx(tfm);
+ 	SHASH_DESC_ON_STACK(desc, ctx->hash);
+ 	u8 digest[SHA256_DIGEST_SIZE];
+ 	int ret;
+ 
+ 	ret = aes_expandkey(&ctx->key1, in_key, key_len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	desc->tfm = ctx->hash;
+ 	crypto_shash_digest(desc, in_key, key_len, digest);
+ 
+ 	return aes_expandkey(&ctx->key2, digest, sizeof(digest));
+ }
+ 
+ static int __maybe_unused ecb_encrypt(struct skcipher_request *req)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  {
  	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
  	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
diff --cc arch/arm64/crypto/ghash-ce-glue.c
index fd6f2177e835,22831d3b7f62..000000000000
--- a/arch/arm64/crypto/ghash-ce-glue.c
+++ b/arch/arm64/crypto/ghash-ce-glue.c
@@@ -312,14 -303,11 +310,18 @@@ static int gcm_setkey(struct crypto_aea
  	u8 key[GHASH_BLOCK_SIZE];
  	int ret;
  
++<<<<<<< HEAD
 +	ret = crypto_aes_expand_key(&ctx->aes_key, inkey, keylen);
 +	if (ret) {
 +		tfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
++=======
+ 	ret = aes_expandkey(&ctx->aes_key, inkey, keylen);
+ 	if (ret)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
- 	}
  
 -	aes_encrypt(&ctx->aes_key, key, (u8[AES_BLOCK_SIZE]){});
 +	__aes_arm64_encrypt(ctx->aes_key.key_enc, key, (u8[AES_BLOCK_SIZE]){},
 +			    num_rounds(&ctx->aes_key));
  
  	return __ghash_setkey(&ctx->ghash_key, key, sizeof(be128));
  }
diff --cc arch/powerpc/crypto/aes-spe-glue.c
index 748fc00c5e19,c2b23b69d7b1..000000000000
--- a/arch/powerpc/crypto/aes-spe-glue.c
+++ b/arch/powerpc/crypto/aes-spe-glue.c
@@@ -135,13 -134,6 +130,16 @@@ static int ppc_xts_setkey(struct crypto
  
  	key_len >>= 1;
  
++<<<<<<< HEAD
 +	if (key_len != AES_KEYSIZE_128 &&
 +	    key_len != AES_KEYSIZE_192 &&
 +	    key_len != AES_KEYSIZE_256) {
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 +		return -EINVAL;
 +	}
 +
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  	switch (key_len) {
  	case AES_KEYSIZE_128:
  		ctx->rounds = 4;
diff --cc arch/s390/crypto/aes_s390.c
index 6eab374f5a60,2db167e5871c..000000000000
--- a/arch/s390/crypto/aes_s390.c
+++ b/arch/s390/crypto/aes_s390.c
@@@ -516,10 -414,8 +516,13 @@@ static int xts_aes_set_key(struct crypt
  		return err;
  
  	/* In fips mode only 128 bit or 256 bit keys are valid */
++<<<<<<< HEAD
 +	if (fips_enabled && key_len != 32 && key_len != 64) {
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
++=======
+ 	if (fips_enabled && key_len != 32 && key_len != 64)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
- 	}
  
  	/* Pick the correct function code based on the key length */
  	fc = (key_len == 32) ? CPACF_KM_XTS_128 :
diff --cc arch/s390/crypto/paes_s390.c
index 6184dceed340,e2a85783f804..000000000000
--- a/arch/s390/crypto/paes_s390.c
+++ b/arch/s390/crypto/paes_s390.c
@@@ -150,18 -151,14 +150,22 @@@ static int ecb_paes_set_key(struct cryp
  	if (rc)
  		return rc;
  
++<<<<<<< HEAD
 +	if (__paes_set_key(ctx)) {
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 +		return -EINVAL;
 +	}
 +	return 0;
++=======
+ 	return __paes_set_key(ctx);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  
 -static int ecb_paes_crypt(struct skcipher_request *req, unsigned long modifier)
 +static int ecb_paes_crypt(struct blkcipher_desc *desc,
 +			  unsigned long modifier,
 +			  struct blkcipher_walk *walk)
  {
 -	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 -	struct s390_paes_ctx *ctx = crypto_skcipher_ctx(tfm);
 -	struct skcipher_walk walk;
 +	struct s390_paes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);
  	unsigned int nbytes, n, k;
  	int ret;
  
@@@ -269,17 -250,14 +273,21 @@@ static int cbc_paes_set_key(struct cryp
  	if (rc)
  		return rc;
  
++<<<<<<< HEAD
 +	if (__cbc_paes_set_key(ctx)) {
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 +		return -EINVAL;
 +	}
 +	return 0;
++=======
+ 	return __cbc_paes_set_key(ctx);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  
 -static int cbc_paes_crypt(struct skcipher_request *req, unsigned long modifier)
 +static int cbc_paes_crypt(struct blkcipher_desc *desc, unsigned long modifier,
 +			  struct blkcipher_walk *walk)
  {
 -	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
 -	struct s390_paes_ctx *ctx = crypto_skcipher_ctx(tfm);
 -	struct skcipher_walk walk;
 +	struct s390_paes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);
  	unsigned int nbytes, n, k;
  	int ret;
  	struct {
@@@ -413,10 -378,9 +421,16 @@@ static int xts_paes_set_key(struct cryp
  	if (rc)
  		return rc;
  
++<<<<<<< HEAD
 +	if (__xts_paes_set_key(ctx)) {
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 +		return -EINVAL;
 +	}
++=======
+ 	rc = __xts_paes_set_key(ctx);
+ 	if (rc)
+ 		return rc;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	/*
  	 * xts_check_key verifies the key length is not odd and makes
@@@ -566,11 -517,7 +580,15 @@@ static int ctr_paes_set_key(struct cryp
  	if (rc)
  		return rc;
  
++<<<<<<< HEAD
 +	if (__ctr_paes_set_key(ctx)) {
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 +		return -EINVAL;
 +	}
 +	return 0;
++=======
+ 	return __ctr_paes_set_key(ctx);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  
  static unsigned int __ctrblk_init(u8 *ctrptr, u8 *iv, unsigned int nbytes)
diff --cc crypto/aes_generic.c
index ca554d57d01e,27ab27931813..000000000000
--- a/crypto/aes_generic.c
+++ b/crypto/aes_generic.c
@@@ -1279,24 -1127,18 +1279,34 @@@ EXPORT_SYMBOL_GPL(crypto_aes_expand_key
   * @in_key:	The input key.
   * @key_len:	The size of the key.
   *
++<<<<<<< HEAD
 + * Returns 0 on success, on failure the %CRYPTO_TFM_RES_BAD_KEY_LEN flag in tfm
 + * is set. The function uses crypto_aes_expand_key() to expand the key.
 + * &crypto_aes_ctx _must_ be the private data embedded in @tfm which is
 + * retrieved with crypto_tfm_ctx().
++=======
+  * This function uses aes_expand_key() to expand the key.  &crypto_aes_ctx
+  * _must_ be the private data embedded in @tfm which is retrieved with
+  * crypto_tfm_ctx().
+  *
+  * Return: 0 on success; -EINVAL on failure (only happens for bad key lengths)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
   */
  int crypto_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,
  		unsigned int key_len)
  {
  	struct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);
- 	u32 *flags = &tfm->crt_flags;
- 	int ret;
  
++<<<<<<< HEAD
 +	ret = crypto_aes_expand_key(ctx, in_key, key_len);
 +	if (!ret)
 +		return 0;
 +
 +	*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 +	return -EINVAL;
++=======
+ 	return aes_expandkey(ctx, in_key, key_len);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  EXPORT_SYMBOL_GPL(crypto_aes_set_key);
  
diff --cc crypto/cipher.c
index 57836c30a49a,0fb7042a709d..000000000000
--- a/crypto/cipher.c
+++ b/crypto/cipher.c
@@@ -44,16 -39,15 +44,20 @@@ static int setkey_unaligned(struct cryp
  
  }
  
 -int crypto_cipher_setkey(struct crypto_cipher *tfm,
 -			 const u8 *key, unsigned int keylen)
 +static int setkey(struct crypto_tfm *tfm, const u8 *key, unsigned int keylen)
  {
 -	struct cipher_alg *cia = crypto_cipher_alg(tfm);
 -	unsigned long alignmask = crypto_cipher_alignmask(tfm);
 +	struct cipher_alg *cia = &tfm->__crt_alg->cra_cipher;
 +	unsigned long alignmask = crypto_tfm_alg_alignmask(tfm);
  
++<<<<<<< HEAD
 +	tfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;
 +	if (keylen < cia->cia_min_keysize || keylen > cia->cia_max_keysize) {
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
++=======
+ 	crypto_cipher_clear_flags(tfm, CRYPTO_TFM_RES_MASK);
+ 	if (keylen < cia->cia_min_keysize || keylen > cia->cia_max_keysize)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
- 	}
  
  	if ((unsigned long)key & alignmask)
  		return setkey_unaligned(tfm, key, keylen);
diff --cc drivers/crypto/atmel-aes.c
index 801aeab5ab1e,898f66cb2eb2..000000000000
--- a/drivers/crypto/atmel-aes.c
+++ b/drivers/crypto/atmel-aes.c
@@@ -1144,10 -1140,8 +1144,13 @@@ static int atmel_aes_setkey(struct cryp
  
  	if (keylen != AES_KEYSIZE_128 &&
  	    keylen != AES_KEYSIZE_192 &&
++<<<<<<< HEAD
 +	    keylen != AES_KEYSIZE_256) {
 +		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
+ 	    keylen != AES_KEYSIZE_256)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
- 	}
  
  	memcpy(ctx->key, key, keylen);
  	ctx->keylen = keylen;
diff --cc drivers/crypto/axis/artpec6_crypto.c
index 0fb8bbf41a8d,fcf1effc7661..000000000000
--- a/drivers/crypto/axis/artpec6_crypto.c
+++ b/drivers/crypto/axis/artpec6_crypto.c
@@@ -1253,10 -1249,8 +1253,15 @@@ static int artpec6_crypto_aead_set_key(
  {
  	struct artpec6_cryptotfm_context *ctx = crypto_tfm_ctx(&tfm->base);
  
++<<<<<<< HEAD
 +	if (len != 16 && len != 24 && len != 32) {
 +		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 +		return -1;
 +	}
++=======
+ 	if (len != 16 && len != 24 && len != 32)
+ 		return -EINVAL;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	ctx->key_length = len;
  
diff --cc drivers/crypto/bcm/cipher.c
index 281af5b4cad8,184a3e1245cf..000000000000
--- a/drivers/crypto/bcm/cipher.c
+++ b/drivers/crypto/bcm/cipher.c
@@@ -1882,7 -1846,6 +1882,10 @@@ static int aes_setkey(struct crypto_abl
  		ctx->cipher_type = CIPHER_TYPE_AES256;
  		break;
  	default:
++<<<<<<< HEAD
 +		crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
  	}
  	WARN_ON((ctx->max_payload != SPU_MAX_PAYLOAD_INF) &&
diff --cc drivers/crypto/caam/caamalg.c
index 00f298134128,ef1a65f4fc92..000000000000
--- a/drivers/crypto/caam/caamalg.c
+++ b/drivers/crypto/caam/caamalg.c
@@@ -510,6 -504,59 +510,62 @@@ static int rfc4543_setauthsize(struct c
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int chachapoly_set_sh_desc(struct crypto_aead *aead)
+ {
+ 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+ 	struct device *jrdev = ctx->jrdev;
+ 	unsigned int ivsize = crypto_aead_ivsize(aead);
+ 	u32 *desc;
+ 
+ 	if (!ctx->cdata.keylen || !ctx->authsize)
+ 		return 0;
+ 
+ 	desc = ctx->sh_desc_enc;
+ 	cnstr_shdsc_chachapoly(desc, &ctx->cdata, &ctx->adata, ivsize,
+ 			       ctx->authsize, true, false);
+ 	dma_sync_single_for_device(jrdev, ctx->sh_desc_enc_dma,
+ 				   desc_bytes(desc), ctx->dir);
+ 
+ 	desc = ctx->sh_desc_dec;
+ 	cnstr_shdsc_chachapoly(desc, &ctx->cdata, &ctx->adata, ivsize,
+ 			       ctx->authsize, false, false);
+ 	dma_sync_single_for_device(jrdev, ctx->sh_desc_dec_dma,
+ 				   desc_bytes(desc), ctx->dir);
+ 
+ 	return 0;
+ }
+ 
+ static int chachapoly_setauthsize(struct crypto_aead *aead,
+ 				  unsigned int authsize)
+ {
+ 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+ 
+ 	if (authsize != POLY1305_DIGEST_SIZE)
+ 		return -EINVAL;
+ 
+ 	ctx->authsize = authsize;
+ 	return chachapoly_set_sh_desc(aead);
+ }
+ 
+ static int chachapoly_setkey(struct crypto_aead *aead, const u8 *key,
+ 			     unsigned int keylen)
+ {
+ 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+ 	unsigned int ivsize = crypto_aead_ivsize(aead);
+ 	unsigned int saltlen = CHACHAPOLY_IV_SIZE - ivsize;
+ 
+ 	if (keylen != CHACHA_KEY_SIZE + saltlen)
+ 		return -EINVAL;
+ 
+ 	ctx->cdata.key_virt = key;
+ 	ctx->cdata.keylen = keylen - saltlen;
+ 
+ 	return chachapoly_set_sh_desc(aead);
+ }
+ 
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  static int aead_setkey(struct crypto_aead *aead,
  			       const u8 *key, unsigned int keylen)
  {
@@@ -616,11 -643,14 +671,20 @@@ static int gcm_setkey(struct crypto_aea
  {
  	struct caam_ctx *ctx = crypto_aead_ctx(aead);
  	struct device *jrdev = ctx->jrdev;
 -	int err;
  
++<<<<<<< HEAD
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
++=======
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
+ 
+ 	print_hex_dump_debug("key in @"__stringify(__LINE__)": ",
+ 			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	memcpy(ctx->key, key, keylen);
  	dma_sync_single_for_device(jrdev, ctx->key_dma, keylen, ctx->dir);
@@@ -634,14 -664,14 +698,20 @@@ static int rfc4106_setkey(struct crypto
  {
  	struct caam_ctx *ctx = crypto_aead_ctx(aead);
  	struct device *jrdev = ctx->jrdev;
 -	int err;
  
++<<<<<<< HEAD
 +	if (keylen < 4)
 +		return -EINVAL;
++=======
+ 	err = aes_check_keylen(keylen - 4);
+ 	if (err)
+ 		return err;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
 -	print_hex_dump_debug("key in @"__stringify(__LINE__)": ",
 -			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
  
  	memcpy(ctx->key, key, keylen);
  
@@@ -660,14 -690,14 +730,20 @@@ static int rfc4543_setkey(struct crypto
  {
  	struct caam_ctx *ctx = crypto_aead_ctx(aead);
  	struct device *jrdev = ctx->jrdev;
 -	int err;
  
++<<<<<<< HEAD
 +	if (keylen < 4)
 +		return -EINVAL;
++=======
+ 	err = aes_check_keylen(keylen - 4);
+ 	if (err)
+ 		return err;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
 -	print_hex_dump_debug("key in @"__stringify(__LINE__)": ",
 -			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
  
  	memcpy(ctx->key, key, keylen);
  
@@@ -681,81 -711,123 +757,181 @@@
  	return rfc4543_set_sh_desc(aead);
  }
  
 -static int skcipher_setkey(struct crypto_skcipher *skcipher, const u8 *key,
 -			   unsigned int keylen, const u32 ctx1_iv_off)
 +static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 +			     const u8 *key, unsigned int keylen)
  {
 -	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
 -	struct caam_skcipher_alg *alg =
 -		container_of(crypto_skcipher_alg(skcipher), typeof(*alg),
 -			     skcipher);
 +	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
 +	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(ablkcipher);
 +	const char *alg_name = crypto_tfm_alg_name(tfm);
  	struct device *jrdev = ctx->jrdev;
 -	unsigned int ivsize = crypto_skcipher_ivsize(skcipher);
 +	unsigned int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
  	u32 *desc;
 -	const bool is_rfc3686 = alg->caam.rfc3686;
 -
 +	u32 ctx1_iv_off = 0;
 +	const bool ctr_mode = ((ctx->cdata.algtype & OP_ALG_AAI_MASK) ==
 +			       OP_ALG_AAI_CTR_MOD128);
 +	const bool is_rfc3686 = (ctr_mode &&
 +				 (strstr(alg_name, "rfc3686") != NULL));
 +
++<<<<<<< HEAD
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
 +	/*
 +	 * AES-CTR needs to load IV in CONTEXT1 reg
 +	 * at an offset of 128bits (16bytes)
 +	 * CONTEXT1[255:128] = IV
 +	 */
 +	if (ctr_mode)
 +		ctx1_iv_off = 16;
++=======
+ 	print_hex_dump_debug("key in @"__stringify(__LINE__)": ",
+ 			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+ 
+ 	ctx->cdata.keylen = keylen;
+ 	ctx->cdata.key_virt = key;
+ 	ctx->cdata.key_inline = true;
+ 
+ 	/* skcipher_encrypt shared descriptor */
+ 	desc = ctx->sh_desc_enc;
+ 	cnstr_shdsc_skcipher_encap(desc, &ctx->cdata, ivsize, is_rfc3686,
+ 				   ctx1_iv_off);
+ 	dma_sync_single_for_device(jrdev, ctx->sh_desc_enc_dma,
+ 				   desc_bytes(desc), ctx->dir);
+ 
+ 	/* skcipher_decrypt shared descriptor */
+ 	desc = ctx->sh_desc_dec;
+ 	cnstr_shdsc_skcipher_decap(desc, &ctx->cdata, ivsize, is_rfc3686,
+ 				   ctx1_iv_off);
+ 	dma_sync_single_for_device(jrdev, ctx->sh_desc_dec_dma,
+ 				   desc_bytes(desc), ctx->dir);
+ 
+ 	return 0;
+ }
+ 
+ static int aes_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 			       const u8 *key, unsigned int keylen)
+ {
+ 	int err;
+ 
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
+ 
+ 	return skcipher_setkey(skcipher, key, keylen, 0);
+ }
+ 
+ static int rfc3686_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 				   const u8 *key, unsigned int keylen)
+ {
+ 	u32 ctx1_iv_off;
+ 	int err;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	/*
  	 * RFC3686 specific:
  	 *	| CONTEXT1[255:128] = {NONCE, IV, COUNTER}
  	 *	| *key = {KEY, NONCE}
  	 */
++<<<<<<< HEAD
 +	if (is_rfc3686) {
 +		ctx1_iv_off = 16 + CTR_RFC3686_NONCE_SIZE;
 +		keylen -= CTR_RFC3686_NONCE_SIZE;
 +	}
++=======
+ 	ctx1_iv_off = 16 + CTR_RFC3686_NONCE_SIZE;
+ 	keylen -= CTR_RFC3686_NONCE_SIZE;
+ 
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
 -	return skcipher_setkey(skcipher, key, keylen, ctx1_iv_off);
 +	ctx->cdata.keylen = keylen;
 +	ctx->cdata.key_virt = key;
 +	ctx->cdata.key_inline = true;
 +
 +	/* ablkcipher_encrypt shared descriptor */
 +	desc = ctx->sh_desc_enc;
 +	cnstr_shdsc_ablkcipher_encap(desc, &ctx->cdata, ivsize, is_rfc3686,
 +				     ctx1_iv_off);
 +	dma_sync_single_for_device(jrdev, ctx->sh_desc_enc_dma,
 +				   desc_bytes(desc), ctx->dir);
 +
 +	/* ablkcipher_decrypt shared descriptor */
 +	desc = ctx->sh_desc_dec;
 +	cnstr_shdsc_ablkcipher_decap(desc, &ctx->cdata, ivsize, is_rfc3686,
 +				     ctx1_iv_off);
 +	dma_sync_single_for_device(jrdev, ctx->sh_desc_dec_dma,
 +				   desc_bytes(desc), ctx->dir);
 +
 +	/* ablkcipher_givencrypt shared descriptor */
 +	desc = ctx->sh_desc_givenc;
 +	cnstr_shdsc_ablkcipher_givencap(desc, &ctx->cdata, ivsize, is_rfc3686,
 +					ctx1_iv_off);
 +	dma_sync_single_for_device(jrdev, ctx->sh_desc_givenc_dma,
 +				   desc_bytes(desc), ctx->dir);
 +
 +	return 0;
  }
  
 -static int ctr_skcipher_setkey(struct crypto_skcipher *skcipher,
 -			       const u8 *key, unsigned int keylen)
 +static int xts_ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 +				 const u8 *key, unsigned int keylen)
  {
++<<<<<<< HEAD
 +	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
++=======
+ 	u32 ctx1_iv_off;
+ 	int err;
+ 
+ 	/*
+ 	 * AES-CTR needs to load IV in CONTEXT1 reg
+ 	 * at an offset of 128bits (16bytes)
+ 	 * CONTEXT1[255:128] = IV
+ 	 */
+ 	ctx1_iv_off = 16;
+ 
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
+ 
+ 	return skcipher_setkey(skcipher, key, keylen, ctx1_iv_off);
+ }
+ 
+ static int arc4_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 				const u8 *key, unsigned int keylen)
+ {
+ 	return skcipher_setkey(skcipher, key, keylen, 0);
+ }
+ 
+ static int des_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 			       const u8 *key, unsigned int keylen)
+ {
+ 	return verify_skcipher_des_key(skcipher, key) ?:
+ 	       skcipher_setkey(skcipher, key, keylen, 0);
+ }
+ 
+ static int des3_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 				const u8 *key, unsigned int keylen)
+ {
+ 	return verify_skcipher_des3_key(skcipher, key) ?:
+ 	       skcipher_setkey(skcipher, key, keylen, 0);
+ }
+ 
+ static int xts_skcipher_setkey(struct crypto_skcipher *skcipher, const u8 *key,
+ 			       unsigned int keylen)
+ {
+ 	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  	struct device *jrdev = ctx->jrdev;
  	u32 *desc;
  
  	if (keylen != 2 * AES_MIN_KEY_SIZE  && keylen != 2 * AES_MAX_KEY_SIZE) {
++<<<<<<< HEAD
 +		crypto_ablkcipher_set_flags(ablkcipher,
 +					    CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		dev_err(jrdev, "key size mismatch\n");
  		return -EINVAL;
  	}
diff --cc drivers/crypto/caam/caamalg_qi.c
index 21727553740d,4a29e0ef9d63..000000000000
--- a/drivers/crypto/caam/caamalg_qi.c
+++ b/drivers/crypto/caam/caamalg_qi.c
@@@ -379,13 -354,16 +378,22 @@@ static int gcm_setkey(struct crypto_aea
  	struct device *jrdev = ctx->jrdev;
  	int ret;
  
++<<<<<<< HEAD
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @" __stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
++=======
+ 	ret = aes_check_keylen(keylen);
+ 	if (ret)
+ 		return ret;
+ 
+ 	print_hex_dump_debug("key in @" __stringify(__LINE__)": ",
+ 			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	memcpy(ctx->key, key, keylen);
 -	dma_sync_single_for_device(jrdev->parent, ctx->key_dma, keylen,
 -				   ctx->dir);
 +	dma_sync_single_for_device(jrdev, ctx->key_dma, keylen, ctx->dir);
  	ctx->cdata.keylen = keylen;
  
  	ret = gcm_set_sh_desc(aead);
@@@ -475,13 -458,12 +483,19 @@@ static int rfc4106_setkey(struct crypto
  	struct device *jrdev = ctx->jrdev;
  	int ret;
  
++<<<<<<< HEAD
 +	if (keylen < 4)
 +		return -EINVAL;
++=======
+ 	ret = aes_check_keylen(keylen - 4);
+ 	if (ret)
+ 		return ret;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
 -	print_hex_dump_debug("key in @" __stringify(__LINE__)": ",
 -			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @" __stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
  
  	memcpy(ctx->key, key, keylen);
  	/*
@@@ -579,13 -564,12 +593,19 @@@ static int rfc4543_setkey(struct crypto
  	struct device *jrdev = ctx->jrdev;
  	int ret;
  
++<<<<<<< HEAD
 +	if (keylen < 4)
 +		return -EINVAL;
++=======
+ 	ret = aes_check_keylen(keylen - 4);
+ 	if (ret)
+ 		return ret;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
 -	print_hex_dump_debug("key in @" __stringify(__LINE__)": ",
 -			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @" __stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
  
  	memcpy(ctx->key, key, keylen);
  	/*
@@@ -622,107 -606,130 +642,219 @@@
  	return 0;
  }
  
 -static int skcipher_setkey(struct crypto_skcipher *skcipher, const u8 *key,
 -			   unsigned int keylen, const u32 ctx1_iv_off)
 +static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 +			     const u8 *key, unsigned int keylen)
  {
 -	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
 -	struct caam_skcipher_alg *alg =
 -		container_of(crypto_skcipher_alg(skcipher), typeof(*alg),
 -			     skcipher);
 +	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
 +	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(ablkcipher);
 +	const char *alg_name = crypto_tfm_alg_name(tfm);
  	struct device *jrdev = ctx->jrdev;
 -	unsigned int ivsize = crypto_skcipher_ivsize(skcipher);
 -	const bool is_rfc3686 = alg->caam.rfc3686;
 +	unsigned int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
 +	u32 ctx1_iv_off = 0;
 +	const bool ctr_mode = ((ctx->cdata.algtype & OP_ALG_AAI_MASK) ==
 +			       OP_ALG_AAI_CTR_MOD128);
 +	const bool is_rfc3686 = (ctr_mode && strstr(alg_name, "rfc3686"));
  	int ret = 0;
  
++<<<<<<< HEAD
 +#ifdef DEBUG
 +	print_hex_dump(KERN_ERR, "key in @" __stringify(__LINE__)": ",
 +		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 +#endif
 +	/*
 +	 * AES-CTR needs to load IV in CONTEXT1 reg
 +	 * at an offset of 128bits (16bytes)
 +	 * CONTEXT1[255:128] = IV
 +	 */
 +	if (ctr_mode)
 +		ctx1_iv_off = 16;
++=======
+ 	print_hex_dump_debug("key in @" __stringify(__LINE__)": ",
+ 			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+ 
+ 	ctx->cdata.keylen = keylen;
+ 	ctx->cdata.key_virt = key;
+ 	ctx->cdata.key_inline = true;
+ 
+ 	/* skcipher encrypt, decrypt shared descriptors */
+ 	cnstr_shdsc_skcipher_encap(ctx->sh_desc_enc, &ctx->cdata, ivsize,
+ 				   is_rfc3686, ctx1_iv_off);
+ 	cnstr_shdsc_skcipher_decap(ctx->sh_desc_dec, &ctx->cdata, ivsize,
+ 				   is_rfc3686, ctx1_iv_off);
+ 
+ 	/* Now update the driver contexts with the new shared descriptor */
+ 	if (ctx->drv_ctx[ENCRYPT]) {
+ 		ret = caam_drv_ctx_update(ctx->drv_ctx[ENCRYPT],
+ 					  ctx->sh_desc_enc);
+ 		if (ret) {
+ 			dev_err(jrdev, "driver enc context update failed\n");
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	if (ctx->drv_ctx[DECRYPT]) {
+ 		ret = caam_drv_ctx_update(ctx->drv_ctx[DECRYPT],
+ 					  ctx->sh_desc_dec);
+ 		if (ret) {
+ 			dev_err(jrdev, "driver dec context update failed\n");
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int aes_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 			       const u8 *key, unsigned int keylen)
+ {
+ 	int err;
+ 
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
+ 
+ 	return skcipher_setkey(skcipher, key, keylen, 0);
+ }
+ 
+ static int rfc3686_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 				   const u8 *key, unsigned int keylen)
+ {
+ 	u32 ctx1_iv_off;
+ 	int err;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	/*
  	 * RFC3686 specific:
  	 *	| CONTEXT1[255:128] = {NONCE, IV, COUNTER}
  	 *	| *key = {KEY, NONCE}
  	 */
++<<<<<<< HEAD
 +	if (is_rfc3686) {
 +		ctx1_iv_off = 16 + CTR_RFC3686_NONCE_SIZE;
 +		keylen -= CTR_RFC3686_NONCE_SIZE;
++=======
+ 	ctx1_iv_off = 16 + CTR_RFC3686_NONCE_SIZE;
+ 	keylen -= CTR_RFC3686_NONCE_SIZE;
+ 
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
+ 
+ 	return skcipher_setkey(skcipher, key, keylen, ctx1_iv_off);
+ }
+ 
+ static int ctr_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 			       const u8 *key, unsigned int keylen)
+ {
+ 	u32 ctx1_iv_off;
+ 	int err;
+ 
+ 	/*
+ 	 * AES-CTR needs to load IV in CONTEXT1 reg
+ 	 * at an offset of 128bits (16bytes)
+ 	 * CONTEXT1[255:128] = IV
+ 	 */
+ 	ctx1_iv_off = 16;
+ 
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
+ 
+ 	return skcipher_setkey(skcipher, key, keylen, ctx1_iv_off);
+ }
+ 
+ static int des3_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 				const u8 *key, unsigned int keylen)
+ {
+ 	return verify_skcipher_des3_key(skcipher, key) ?:
+ 	       skcipher_setkey(skcipher, key, keylen, 0);
+ }
+ 
+ static int des_skcipher_setkey(struct crypto_skcipher *skcipher,
+ 			       const u8 *key, unsigned int keylen)
+ {
+ 	return verify_skcipher_des_key(skcipher, key) ?:
+ 	       skcipher_setkey(skcipher, key, keylen, 0);
+ }
+ 
+ static int xts_skcipher_setkey(struct crypto_skcipher *skcipher, const u8 *key,
+ 			       unsigned int keylen)
+ {
+ 	struct caam_ctx *ctx = crypto_skcipher_ctx(skcipher);
+ 	struct device *jrdev = ctx->jrdev;
+ 	int ret = 0;
+ 
+ 	if (keylen != 2 * AES_MIN_KEY_SIZE  && keylen != 2 * AES_MAX_KEY_SIZE) {
+ 		dev_err(jrdev, "key size mismatch\n");
+ 		return -EINVAL;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
 +	}
 +
 +	ctx->cdata.keylen = keylen;
 +	ctx->cdata.key_virt = key;
 +	ctx->cdata.key_inline = true;
 +
 +	/* ablkcipher encrypt, decrypt, givencrypt shared descriptors */
 +	cnstr_shdsc_ablkcipher_encap(ctx->sh_desc_enc, &ctx->cdata, ivsize,
 +				     is_rfc3686, ctx1_iv_off);
 +	cnstr_shdsc_ablkcipher_decap(ctx->sh_desc_dec, &ctx->cdata, ivsize,
 +				     is_rfc3686, ctx1_iv_off);
 +	cnstr_shdsc_ablkcipher_givencap(ctx->sh_desc_givenc, &ctx->cdata,
 +					ivsize, is_rfc3686, ctx1_iv_off);
 +
 +	/* Now update the driver contexts with the new shared descriptor */
 +	if (ctx->drv_ctx[ENCRYPT]) {
 +		ret = caam_drv_ctx_update(ctx->drv_ctx[ENCRYPT],
 +					  ctx->sh_desc_enc);
 +		if (ret) {
 +			dev_err(jrdev, "driver enc context update failed\n");
 +			goto badkey;
 +		}
 +	}
 +
 +	if (ctx->drv_ctx[DECRYPT]) {
 +		ret = caam_drv_ctx_update(ctx->drv_ctx[DECRYPT],
 +					  ctx->sh_desc_dec);
 +		if (ret) {
 +			dev_err(jrdev, "driver dec context update failed\n");
 +			goto badkey;
 +		}
 +	}
 +
 +	if (ctx->drv_ctx[GIVENCRYPT]) {
 +		ret = caam_drv_ctx_update(ctx->drv_ctx[GIVENCRYPT],
 +					  ctx->sh_desc_givenc);
 +		if (ret) {
 +			dev_err(jrdev, "driver givenc context update failed\n");
 +			goto badkey;
 +		}
 +	}
 +
 +	return ret;
 +badkey:
 +	crypto_ablkcipher_set_flags(ablkcipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
 +	return -EINVAL;
 +}
 +
 +static int des3_ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 +				  const u8 *key, unsigned int keylen)
 +{
 +	return unlikely(des3_verify_key(skcipher, key)) ?:
 +	       ablkcipher_setkey(ablkcipher, key, keylen);
 +}
 +
 +static int xts_ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 +				 const u8 *key, unsigned int keylen)
 +{
 +	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
 +	struct device *jrdev = ctx->jrdev;
 +	int ret = 0;
 +
 +	if (keylen != 2 * AES_MIN_KEY_SIZE  && keylen != 2 * AES_MAX_KEY_SIZE) {
 +		crypto_ablkcipher_set_flags(ablkcipher,
 +					    CRYPTO_TFM_RES_BAD_KEY_LEN);
 +		dev_err(jrdev, "key size mismatch\n");
 +		return -EINVAL;
  	}
  
  	ctx->cdata.keylen = keylen;
@@@ -753,9 -760,6 +885,12 @@@
  	}
  
  	return ret;
++<<<<<<< HEAD
 +badkey:
 +	crypto_ablkcipher_set_flags(ablkcipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
 +	return 0;
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  
  /*
diff --cc drivers/crypto/caam/caamhash.c
index 0beb28196e20,8d9143407fc5..000000000000
--- a/drivers/crypto/caam/caamhash.c
+++ b/drivers/crypto/caam/caamhash.c
@@@ -482,10 -476,48 +482,52 @@@ static int ahash_setkey(struct crypto_a
  	return -EINVAL;
  }
  
++<<<<<<< HEAD
++=======
+ static int axcbc_setkey(struct crypto_ahash *ahash, const u8 *key,
+ 			unsigned int keylen)
+ {
+ 	struct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);
+ 	struct device *jrdev = ctx->jrdev;
+ 
+ 	if (keylen != AES_KEYSIZE_128)
+ 		return -EINVAL;
+ 
+ 	memcpy(ctx->key, key, keylen);
+ 	dma_sync_single_for_device(jrdev, ctx->adata.key_dma, keylen,
+ 				   DMA_TO_DEVICE);
+ 	ctx->adata.keylen = keylen;
+ 
+ 	print_hex_dump_debug("axcbc ctx.key@" __stringify(__LINE__)" : ",
+ 			     DUMP_PREFIX_ADDRESS, 16, 4, ctx->key, keylen, 1);
+ 
+ 	return axcbc_set_sh_desc(ahash);
+ }
+ 
+ static int acmac_setkey(struct crypto_ahash *ahash, const u8 *key,
+ 			unsigned int keylen)
+ {
+ 	struct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);
+ 	int err;
+ 
+ 	err = aes_check_keylen(keylen);
+ 	if (err)
+ 		return err;
+ 
+ 	/* key is immediate data for all cmac shared descriptors */
+ 	ctx->adata.key_virt = key;
+ 	ctx->adata.keylen = keylen;
+ 
+ 	print_hex_dump_debug("acmac ctx.key@" __stringify(__LINE__)" : ",
+ 			     DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+ 
+ 	return acmac_set_sh_desc(ahash);
+ }
+ 
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  /*
   * ahash_edesc - s/w-extended ahash descriptor
 + * @dst_dma: physical mapped address of req->result
   * @sec4_sg_dma: physical mapped address of h/w link table
   * @src_nents: number of segments in input scatterlist
   * @sec4_sg_bytes: length of dma mapped sec4_sg space
diff --cc drivers/crypto/cavium/cpt/cptvf_algs.c
index 486cbc6b27aa,1be1adffff1d..000000000000
--- a/drivers/crypto/cavium/cpt/cptvf_algs.c
+++ b/drivers/crypto/cavium/cpt/cptvf_algs.c
@@@ -300,8 -295,6 +300,11 @@@ static int cvm_setkey(struct crypto_abl
  		memcpy(ctx->enc_key, key, keylen);
  		return 0;
  	} else {
++<<<<<<< HEAD
 +		crypto_ablkcipher_set_flags(cipher,
 +					    CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
  	}
  }
diff --cc drivers/crypto/ccp/ccp-crypto-aes.c
index 114fa0587d72,51e12fbd1159..000000000000
--- a/drivers/crypto/ccp/ccp-crypto-aes.c
+++ b/drivers/crypto/ccp/ccp-crypto-aes.c
@@@ -52,7 -51,6 +52,10 @@@ static int ccp_aes_setkey(struct crypto
  		ctx->u.aes.type = CCP_AES_TYPE_256;
  		break;
  	default:
++<<<<<<< HEAD
 +		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
  	}
  	ctx->u.aes.mode = alg->mode;
diff --cc drivers/crypto/ccree/cc_aead.c
index f71917756a7f,d014c8e063a7..000000000000
--- a/drivers/crypto/ccree/cc_aead.c
+++ b/drivers/crypto/ccree/cc_aead.c
@@@ -554,19 -558,15 +554,31 @@@ static int cc_aead_setkey(struct crypto
  	/* STAT_PHASE_0: Init and sanity checks */
  
  	if (ctx->auth_mode != DRV_HASH_NULL) { /* authenc() alg. */
++<<<<<<< HEAD
 +		if (!RTA_OK(rta, keylen))
 +			goto badkey;
 +		if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)
 +			goto badkey;
 +		if (RTA_PAYLOAD(rta) < sizeof(*param))
 +			goto badkey;
 +		param = RTA_DATA(rta);
 +		ctx->enc_keylen = be32_to_cpu(param->enckeylen);
 +		key += RTA_ALIGN(rta->rta_len);
 +		keylen -= RTA_ALIGN(rta->rta_len);
 +		if (keylen < ctx->enc_keylen)
 +			goto badkey;
 +		ctx->auth_keylen = keylen - ctx->enc_keylen;
++=======
+ 		struct crypto_authenc_keys keys;
+ 
+ 		rc = crypto_authenc_extractkeys(&keys, key, keylen);
+ 		if (rc)
+ 			return rc;
+ 		enckey = keys.enckey;
+ 		authkey = keys.authkey;
+ 		ctx->enc_keylen = keys.enckeylen;
+ 		ctx->auth_keylen = keys.authkeylen;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  		if (ctx->cipher_mode == DRV_CIPHER_CTR) {
  			/* the nonce is stored in bytes at end of key */
@@@ -598,11 -599,12 +610,11 @@@
  	if (ctx->enc_keylen == 24)
  		memset(ctx->enckey + 24, 0, CC_AES_KEY_SIZE_MAX - 24);
  	if (ctx->auth_mode == DRV_HASH_XCBC_MAC) {
 -		memcpy(ctx->auth_state.xcbc.xcbc_keys, authkey,
 -		       ctx->auth_keylen);
 +		memcpy(ctx->auth_state.xcbc.xcbc_keys, key, ctx->auth_keylen);
  	} else if (ctx->auth_mode != DRV_HASH_NULL) { /* HMAC */
 -		rc = cc_get_plain_hmac_key(tfm, authkey, ctx->auth_keylen);
 +		rc = cc_get_plain_hmac_key(tfm, key, ctx->auth_keylen);
  		if (rc)
- 			goto badkey;
+ 			return rc;
  	}
  
  	/* STAT_PHASE_2: Create sequence */
diff --cc drivers/crypto/ccree/cc_cipher.c
index d2810c183b73,7493a32f12b9..000000000000
--- a/drivers/crypto/ccree/cc_cipher.c
+++ b/drivers/crypto/ccree/cc_cipher.c
@@@ -248,15 -288,9 +248,19 @@@ static int cc_cipher_sethkey(struct cry
  
  	/* STAT_PHASE_0: Init and sanity checks */
  
 -	/* This check the size of the protected key token */
 +	/* This check the size of the hardware key token */
  	if (keylen != sizeof(hki)) {
++<<<<<<< HEAD
 +		dev_err(dev, "Unsupported HW key size %d.\n", keylen);
 +		crypto_tfm_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
+ 		dev_err(dev, "Unsupported protected key size %d.\n", keylen);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
 +		return -EINVAL;
 +	}
 +
 +	if (ctx_p->flow_mode != S_DIN_to_AES) {
 +		dev_err(dev, "HW key not supported for non-AES flows\n");
  		return -EINVAL;
  	}
  
diff --cc drivers/crypto/chelsio/chcr_algo.c
index 221e80097813,720b2ff55464..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -911,7 -912,6 +911,10 @@@ static int chcr_aes_cbc_setkey(struct c
  	ablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CBC;
  	return 0;
  badkey_err:
++<<<<<<< HEAD
 +	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  	ablkctx->enckey_len = 0;
  
  	return err;
@@@ -942,7 -942,6 +945,10 @@@ static int chcr_aes_ctr_setkey(struct c
  
  	return 0;
  badkey_err:
++<<<<<<< HEAD
 +	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  	ablkctx->enckey_len = 0;
  
  	return err;
@@@ -980,7 -979,6 +986,10 @@@ static int chcr_aes_rfc3686_setkey(stru
  
  	return 0;
  badkey_err:
++<<<<<<< HEAD
 +	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  	ablkctx->enckey_len = 0;
  
  	return err;
@@@ -2184,7 -2171,6 +2193,10 @@@ static int chcr_aes_xts_setkey(struct c
  	ablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_XTS;
  	return 0;
  badkey_err:
++<<<<<<< HEAD
 +	crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  	ablkctx->enckey_len = 0;
  
  	return err;
@@@ -3301,8 -3280,6 +3313,11 @@@ static int chcr_ccm_common_setkey(struc
  		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
  		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;
  	} else {
++<<<<<<< HEAD
 +		crypto_tfm_set_flags((struct crypto_tfm *)aead,
 +				     CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		aeadctx->enckey_len = 0;
  		return	-EINVAL;
  	}
@@@ -3340,8 -3317,6 +3355,11 @@@ static int chcr_aead_rfc4309_setkey(str
  	int error;
  
  	if (keylen < 3) {
++<<<<<<< HEAD
 +		crypto_tfm_set_flags((struct crypto_tfm *)aead,
 +				     CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		aeadctx->enckey_len = 0;
  		return	-EINVAL;
  	}
@@@ -3391,8 -3366,6 +3409,11 @@@ static int chcr_gcm_setkey(struct crypt
  	} else if (keylen == AES_KEYSIZE_256) {
  		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
  	} else {
++<<<<<<< HEAD
 +		crypto_tfm_set_flags((struct crypto_tfm *)aead,
 +				     CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		pr_err("GCM: Invalid key length %d\n", keylen);
  		ret = -EINVAL;
  		goto out;
diff --cc drivers/crypto/geode-aes.c
index eb2a0a73cbed,eb6e6b618361..000000000000
--- a/drivers/crypto/geode-aes.c
+++ b/drivers/crypto/geode-aes.c
@@@ -161,11 -152,9 +159,13 @@@ static int geode_setkey_blk(struct cryp
  		return 0;
  	}
  
- 	if (len != AES_KEYSIZE_192 && len != AES_KEYSIZE_256) {
+ 	if (len != AES_KEYSIZE_192 && len != AES_KEYSIZE_256)
  		/* not supported at all */
++<<<<<<< HEAD
 +		tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
- 	}
  
  	/*
  	 * The requested key size is not supported by HW, do a fallback
diff --cc drivers/crypto/inside-secure/safexcel_cipher.c
index 6bb60fda2043,5ee66532f336..000000000000
--- a/drivers/crypto/inside-secure/safexcel_cipher.c
+++ b/drivers/crypto/inside-secure/safexcel_cipher.c
@@@ -139,15 -379,13 +139,19 @@@ static int safexcel_skcipher_aes_setkey
  	struct crypto_aes_ctx aes;
  	int ret, i;
  
++<<<<<<< HEAD
 +	ret = crypto_aes_expand_key(&aes, key, len);
 +	if (ret) {
 +		crypto_skcipher_set_flags(ctfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return ret;
- 	}
  
 -	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
 +	if (priv->version == EIP197 && ctx->base.ctxr_dma) {
  		for (i = 0; i < len / sizeof(u32); i++) {
 -			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
 +			if (ctx->key[i] != cpu_to_le32(aes.key_enc[i])) {
  				ctx->base.needs_inv = true;
  				break;
  			}
@@@ -171,20 -409,61 +175,57 @@@ static int safexcel_aead_aes_setkey(str
  	struct safexcel_ahash_export_state istate, ostate;
  	struct safexcel_crypto_priv *priv = ctx->priv;
  	struct crypto_authenc_keys keys;
 -	struct crypto_aes_ctx aes;
 -	int err = -EINVAL, i;
  
 -	if (unlikely(crypto_authenc_extractkeys(&keys, key, len)))
 +	if (crypto_authenc_extractkeys(&keys, key, len) != 0)
  		goto badkey;
  
 -	if (ctx->mode == CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD) {
 -		/* Must have at least space for the nonce here */
 -		if (unlikely(keys.enckeylen < CTR_RFC3686_NONCE_SIZE))
 -			goto badkey;
 -		/* last 4 bytes of key are the nonce! */
 -		ctx->nonce = *(u32 *)(keys.enckey + keys.enckeylen -
 -				      CTR_RFC3686_NONCE_SIZE);
 -		/* exclude the nonce here */
 -		keys.enckeylen -= CTR_RFC3686_NONCE_SIZE;
 -	}
 +	if (keys.enckeylen > sizeof(ctx->key))
 +		goto badkey;
  
  	/* Encryption key */
++<<<<<<< HEAD
 +	if (priv->version == EIP197 && ctx->base.ctxr_dma &&
 +	    memcmp(ctx->key, keys.enckey, keys.enckeylen))
 +		ctx->base.needs_inv = true;
++=======
+ 	switch (ctx->alg) {
+ 	case SAFEXCEL_DES:
+ 		err = verify_aead_des_key(ctfm, keys.enckey, keys.enckeylen);
+ 		if (unlikely(err))
+ 			goto badkey;
+ 		break;
+ 	case SAFEXCEL_3DES:
+ 		err = verify_aead_des3_key(ctfm, keys.enckey, keys.enckeylen);
+ 		if (unlikely(err))
+ 			goto badkey;
+ 		break;
+ 	case SAFEXCEL_AES:
+ 		err = aes_expandkey(&aes, keys.enckey, keys.enckeylen);
+ 		if (unlikely(err))
+ 			goto badkey;
+ 		break;
+ 	case SAFEXCEL_SM4:
+ 		if (unlikely(keys.enckeylen != SM4_KEY_SIZE))
+ 			goto badkey;
+ 		break;
+ 	default:
+ 		dev_err(priv->dev, "aead: unsupported cipher algorithm\n");
+ 		goto badkey;
+ 	}
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < keys.enckeylen / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) !=
+ 			    ((u32 *)keys.enckey)[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	/* Auth key */
 -	switch (ctx->hash_alg) {
 +	switch (ctx->alg) {
  	case CONTEXT_CONTROL_CRYPTO_ALG_SHA1:
  		if (safexcel_hmac_setkey("safexcel-sha1", keys.authkey,
  					 keys.authkeylen, &istate, &ostate))
@@@ -224,9 -519,8 +265,12 @@@
  	return 0;
  
  badkey:
++<<<<<<< HEAD
 +	crypto_aead_set_flags(ctfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  	memzero_explicit(&keys, sizeof(keys));
 -	return err;
 +	return -EINVAL;
  }
  
  static int safexcel_context_control(struct safexcel_cipher_ctx *ctx,
@@@ -880,7 -1344,349 +924,353 @@@ struct safexcel_alg_template safexcel_a
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
  			.cra_alignmask = 0,
++<<<<<<< HEAD
 +			.cra_init = safexcel_skcipher_cra_init,
++=======
+ 			.cra_init = safexcel_skcipher_aes_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aes_cfb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cfb_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aes_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = AES_MIN_KEY_SIZE,
+ 		.max_keysize = AES_MAX_KEY_SIZE,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cfb(aes)",
+ 			.cra_driver_name = "safexcel-cfb-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_cfb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aes_ofb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_OFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ofb_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aes_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = AES_MIN_KEY_SIZE,
+ 		.max_keysize = AES_MAX_KEY_SIZE,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "ofb(aes)",
+ 			.cra_driver_name = "safexcel-ofb-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_ofb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aesctr_setkey(struct crypto_skcipher *ctfm,
+ 					   const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 	unsigned int keylen;
+ 
+ 	/* last 4 bytes of key are the nonce! */
+ 	ctx->nonce = *(u32 *)(key + len - CTR_RFC3686_NONCE_SIZE);
+ 	/* exclude the nonce here */
+ 	keylen = len - CTR_RFC3686_NONCE_SIZE;
+ 	ret = aes_expandkey(&aes, key, keylen);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < keylen / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < keylen / sizeof(u32); i++)
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 
+ 	ctx->key_len = keylen;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_aes_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aesctr_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		/* Add nonce size */
+ 		.min_keysize = AES_MIN_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.max_keysize = AES_MAX_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc3686(ctr(aes))",
+ 			.cra_driver_name = "safexcel-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_ctr_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_des_setkey(struct crypto_skcipher *ctfm, const u8 *key,
+ 			       unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_skcipher_ctx(ctfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	int ret;
+ 
+ 	ret = verify_skcipher_des_key(ctfm, key);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* if context exits and key changed, need to invalidate it */
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, len))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, len);
+ 	ctx->key_len = len;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_des_cbc_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_DES;
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CBC;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES_KEY_SIZE,
+ 		.max_keysize = DES_KEY_SIZE,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cbc(des)",
+ 			.cra_driver_name = "safexcel-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_des_ecb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_DES;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_ECB;
+ 	ctx->blocksz = 0;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ecb_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES_KEY_SIZE,
+ 		.max_keysize = DES_KEY_SIZE,
+ 		.base = {
+ 			.cra_name = "ecb(des)",
+ 			.cra_driver_name = "safexcel-ecb-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des_ecb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_des3_ede_setkey(struct crypto_skcipher *ctfm,
+ 				   const u8 *key, unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_skcipher_ctx(ctfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	int err;
+ 
+ 	err = verify_skcipher_des3_key(ctfm, key);
+ 	if (err)
+ 		return err;
+ 
+ 	/* if context exits and key changed, need to invalidate it */
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, len))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, len);
+ 	ctx->key_len = len;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_des3_cbc_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_3DES;
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CBC;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des3_ede_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES3_EDE_KEY_SIZE,
+ 		.max_keysize = DES3_EDE_KEY_SIZE,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cbc(des3_ede)",
+ 			.cra_driver_name = "safexcel-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des3_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_des3_ecb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_3DES;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_ECB;
+ 	ctx->blocksz = 0;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ecb_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des3_ede_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES3_EDE_KEY_SIZE,
+ 		.max_keysize = DES3_EDE_KEY_SIZE,
+ 		.base = {
+ 			.cra_name = "ecb(des3_ede)",
+ 			.cra_driver_name = "safexcel-ecb-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des3_ecb_cra_init,
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  			.cra_exit = safexcel_skcipher_cra_exit,
  			.cra_module = THIS_MODULE,
  		},
@@@ -1022,3 -1834,1897 +1412,1900 @@@ struct safexcel_alg_template safexcel_a
  		},
  	},
  };
++<<<<<<< HEAD
++=======
+ 
+ static int safexcel_aead_sha512_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;
+ 	ctx->state_sz = SHA512_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_cbc_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),cbc(aes))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-cbc-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = AES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;
+ 	ctx->state_sz = SHA512_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_cbc_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),cbc(aes))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-cbc-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = AES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha1_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha1_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha1_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha256_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha256_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha256_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA256_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha256),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha256-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha256_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha224_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha224_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha224_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA224_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha224),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha224-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha224_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha512_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha512_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha384_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha1_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha1_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha1_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha256_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha256_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha256_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA256_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha256),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha256-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha256_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha224_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha224_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha224_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA224_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha224),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha224-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha224_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha512_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha512_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha384_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha1_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha1_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha1_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha256_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha256_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha256_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA256_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha256),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha256-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha256_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha224_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha224_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha224_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA224_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha224),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha224-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha224_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha512_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha512_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha384_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aesxts_setkey(struct crypto_skcipher *ctfm,
+ 					   const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 	unsigned int keylen;
+ 
+ 	/* Check for illegal XTS keys */
+ 	ret = xts_verify_key(ctfm, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Only half of the key data is cipher key */
+ 	keylen = (len >> 1);
+ 	ret = aes_expandkey(&aes, key, keylen);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < keylen / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < keylen / sizeof(u32); i++)
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 
+ 	/* The other half is the tweak key */
+ 	ret = aes_expandkey(&aes, (u8 *)(key + keylen), keylen);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < keylen / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i + keylen / sizeof(u32)]) !=
+ 			    aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < keylen / sizeof(u32); i++)
+ 		ctx->key[i + keylen / sizeof(u32)] =
+ 			cpu_to_le32(aes.key_enc[i]);
+ 
+ 	ctx->key_len = keylen << 1;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_aes_xts_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->xts  = 1;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_XTS;
+ 	return 0;
+ }
+ 
+ static int safexcel_encrypt_xts(struct skcipher_request *req)
+ {
+ 	if (req->cryptlen < XTS_BLOCK_SIZE)
+ 		return -EINVAL;
+ 	return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 				  SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_decrypt_xts(struct skcipher_request *req)
+ {
+ 	if (req->cryptlen < XTS_BLOCK_SIZE)
+ 		return -EINVAL;
+ 	return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 				  SAFEXCEL_DECRYPT);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_xts_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_AES_XTS,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aesxts_setkey,
+ 		.encrypt = safexcel_encrypt_xts,
+ 		.decrypt = safexcel_decrypt_xts,
+ 		/* XTS actually uses 2 AES keys glued together */
+ 		.min_keysize = AES_MIN_KEY_SIZE * 2,
+ 		.max_keysize = AES_MAX_KEY_SIZE * 2,
+ 		.ivsize = XTS_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "xts(aes)",
+ 			.cra_driver_name = "safexcel-xts-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = XTS_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_xts_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_gcm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				    unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	u32 hashkey[AES_BLOCK_SIZE >> 2];
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret) {
+ 		memzero_explicit(&aes, sizeof(aes));
+ 		return ret;
+ 	}
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < len / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < len / sizeof(u32); i++)
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 
+ 	ctx->key_len = len;
+ 
+ 	/* Compute hash key by encrypting zeroes with cipher key */
+ 	crypto_cipher_clear_flags(ctx->hkaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->hkaes, crypto_aead_get_flags(ctfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->hkaes, key, len);
+ 	crypto_aead_set_flags(ctfm, crypto_cipher_get_flags(ctx->hkaes) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		return ret;
+ 
+ 	memset(hashkey, 0, AES_BLOCK_SIZE);
+ 	crypto_cipher_encrypt_one(ctx->hkaes, (u8 *)hashkey, (u8 *)hashkey);
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < AES_BLOCK_SIZE / sizeof(u32); i++) {
+ 			if (be32_to_cpu(ctx->ipad[i]) != hashkey[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < AES_BLOCK_SIZE / sizeof(u32); i++)
+ 		ctx->ipad[i] = cpu_to_be32(hashkey[i]);
+ 
+ 	memzero_explicit(hashkey, AES_BLOCK_SIZE);
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_gcm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_GHASH;
+ 	ctx->state_sz = GHASH_BLOCK_SIZE;
+ 	ctx->xcm = EIP197_XCM_MODE_GCM;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_XCM; /* override default */
+ 
+ 	ctx->hkaes = crypto_alloc_cipher("aes", 0, 0);
+ 	return PTR_ERR_OR_ZERO(ctx->hkaes);
+ }
+ 
+ static void safexcel_aead_gcm_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_cipher(ctx->hkaes);
+ 	safexcel_aead_cra_exit(tfm);
+ }
+ 
+ static int safexcel_aead_gcm_setauthsize(struct crypto_aead *tfm,
+ 					 unsigned int authsize)
+ {
+ 	return crypto_gcm_check_authsize(authsize);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_gcm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_GHASH,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_gcm_setkey,
+ 		.setauthsize = safexcel_aead_gcm_setauthsize,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = GCM_AES_IV_SIZE,
+ 		.maxauthsize = GHASH_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "gcm(aes)",
+ 			.cra_driver_name = "safexcel-gcm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_gcm_cra_init,
+ 			.cra_exit = safexcel_aead_gcm_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_ccm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				    unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret) {
+ 		memzero_explicit(&aes, sizeof(aes));
+ 		return ret;
+ 	}
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < len / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < len / sizeof(u32); i++) {
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 		ctx->ipad[i + 2 * AES_BLOCK_SIZE / sizeof(u32)] =
+ 			cpu_to_be32(aes.key_enc[i]);
+ 	}
+ 
+ 	ctx->key_len = len;
+ 	ctx->state_sz = 2 * AES_BLOCK_SIZE + len;
+ 
+ 	if (len == AES_KEYSIZE_192)
+ 		ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;
+ 	else if (len == AES_KEYSIZE_256)
+ 		ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;
+ 	else
+ 		ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_ccm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 	ctx->state_sz = 3 * AES_BLOCK_SIZE;
+ 	ctx->xcm = EIP197_XCM_MODE_CCM;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_XCM; /* override default */
+ 	ctx->ctrinit = 0;
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_ccm_setauthsize(struct crypto_aead *tfm,
+ 					 unsigned int authsize)
+ {
+ 	/* Borrowed from crypto/ccm.c */
+ 	switch (authsize) {
+ 	case 4:
+ 	case 6:
+ 	case 8:
+ 	case 10:
+ 	case 12:
+ 	case 14:
+ 	case 16:
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_ccm_encrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	if (req->iv[0] < 1 || req->iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_ccm_decrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	if (req->iv[0] < 1 || req->iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_DECRYPT);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ccm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_CBC_MAC_ALL,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_ccm_setkey,
+ 		.setauthsize = safexcel_aead_ccm_setauthsize,
+ 		.encrypt = safexcel_ccm_encrypt,
+ 		.decrypt = safexcel_ccm_decrypt,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.maxauthsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "ccm(aes)",
+ 			.cra_driver_name = "safexcel-ccm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_ccm_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static void safexcel_chacha20_setkey(struct safexcel_cipher_ctx *ctx,
+ 				     const u8 *key)
+ {
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, CHACHA_KEY_SIZE))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, CHACHA_KEY_SIZE);
+ 	ctx->key_len = CHACHA_KEY_SIZE;
+ }
+ 
+ static int safexcel_skcipher_chacha20_setkey(struct crypto_skcipher *ctfm,
+ 					     const u8 *key, unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_skcipher_ctx(ctfm);
+ 
+ 	if (len != CHACHA_KEY_SIZE)
+ 		return -EINVAL;
+ 
+ 	safexcel_chacha20_setkey(ctx, key);
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_chacha20_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_CHACHA20;
+ 	ctx->ctrinit = 0;
+ 	ctx->mode = CONTEXT_CONTROL_CHACHA20_MODE_256_32;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_chacha20 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_CHACHA20,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_chacha20_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = CHACHA_KEY_SIZE,
+ 		.max_keysize = CHACHA_KEY_SIZE,
+ 		.ivsize = CHACHA_IV_SIZE,
+ 		.base = {
+ 			.cra_name = "chacha20",
+ 			.cra_driver_name = "safexcel-chacha20",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_chacha20_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_chachapoly_setkey(struct crypto_aead *ctfm,
+ 				    const u8 *key, unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_aead_ctx(ctfm);
+ 
+ 	if (ctx->aead  == EIP197_AEAD_TYPE_IPSEC_ESP &&
+ 	    len > EIP197_AEAD_IPSEC_NONCE_SIZE) {
+ 		/* ESP variant has nonce appended to key */
+ 		len -= EIP197_AEAD_IPSEC_NONCE_SIZE;
+ 		ctx->nonce = *(u32 *)(key + len);
+ 	}
+ 	if (len != CHACHA_KEY_SIZE)
+ 		return -EINVAL;
+ 
+ 	safexcel_chacha20_setkey(ctx, key);
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_chachapoly_setauthsize(struct crypto_aead *tfm,
+ 					 unsigned int authsize)
+ {
+ 	if (authsize != POLY1305_DIGEST_SIZE)
+ 		return -EINVAL;
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_chachapoly_crypt(struct aead_request *req,
+ 					  enum safexcel_cipher_direction dir)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(aead);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 	u32 key[CHACHA_KEY_SIZE / sizeof(u32) + 1];
+ 	int ret = 0;
+ 
+ 	/*
+ 	 * Instead of wasting time detecting umpteen silly corner cases,
+ 	 * just dump all "small" requests to the fallback implementation.
+ 	 * HW would not be faster on such small requests anyway.
+ 	 */
+ 	if (likely((ctx->aead != EIP197_AEAD_TYPE_IPSEC_ESP ||
+ 		    req->assoclen >= EIP197_AEAD_IPSEC_IV_SIZE) &&
+ 		   req->cryptlen > POLY1305_DIGEST_SIZE)) {
+ 		return safexcel_queue_req(&req->base, creq, dir);
+ 	}
+ 
+ 	/* HW cannot do full (AAD+payload) zero length, use fallback */
+ 	memcpy(key, ctx->key, CHACHA_KEY_SIZE);
+ 	if (ctx->aead == EIP197_AEAD_TYPE_IPSEC_ESP) {
+ 		/* ESP variant has nonce appended to the key */
+ 		key[CHACHA_KEY_SIZE / sizeof(u32)] = ctx->nonce;
+ 		ret = crypto_aead_setkey(ctx->fback, (u8 *)key,
+ 					 CHACHA_KEY_SIZE +
+ 					 EIP197_AEAD_IPSEC_NONCE_SIZE);
+ 	} else {
+ 		ret = crypto_aead_setkey(ctx->fback, (u8 *)key,
+ 					 CHACHA_KEY_SIZE);
+ 	}
+ 	if (ret) {
+ 		crypto_aead_clear_flags(aead, CRYPTO_TFM_REQ_MASK);
+ 		crypto_aead_set_flags(aead, crypto_aead_get_flags(ctx->fback) &
+ 					    CRYPTO_TFM_REQ_MASK);
+ 		return ret;
+ 	}
+ 
+ 	aead_request_set_tfm(subreq, ctx->fback);
+ 	aead_request_set_callback(subreq, req->base.flags, req->base.complete,
+ 				  req->base.data);
+ 	aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 			       req->iv);
+ 	aead_request_set_ad(subreq, req->assoclen);
+ 
+ 	return (dir ==  SAFEXCEL_ENCRYPT) ?
+ 		crypto_aead_encrypt(subreq) :
+ 		crypto_aead_decrypt(subreq);
+ }
+ 
+ static int safexcel_aead_chachapoly_encrypt(struct aead_request *req)
+ {
+ 	return safexcel_aead_chachapoly_crypt(req, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_aead_chachapoly_decrypt(struct aead_request *req)
+ {
+ 	return safexcel_aead_chachapoly_crypt(req, SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_aead_fallback_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct crypto_aead *aead = __crypto_aead_cast(tfm);
+ 	struct aead_alg *alg = crypto_aead_alg(aead);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 
+ 	/* Allocate fallback implementation */
+ 	ctx->fback = crypto_alloc_aead(alg->base.cra_name, 0,
+ 				       CRYPTO_ALG_ASYNC |
+ 				       CRYPTO_ALG_NEED_FALLBACK);
+ 	if (IS_ERR(ctx->fback))
+ 		return PTR_ERR(ctx->fback);
+ 
+ 	crypto_aead_set_reqsize(aead, max(sizeof(struct safexcel_cipher_req),
+ 					  sizeof(struct aead_request) +
+ 					  crypto_aead_reqsize(ctx->fback)));
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_chachapoly_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_fallback_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_CHACHA20;
+ 	ctx->mode = CONTEXT_CONTROL_CHACHA20_MODE_256_32 |
+ 		    CONTEXT_CONTROL_CHACHA20_MODE_CALC_OTK;
+ 	ctx->ctrinit = 0;
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_POLY1305;
+ 	ctx->state_sz = 0; /* Precomputed by HW */
+ 	return 0;
+ }
+ 
+ static void safexcel_aead_fallback_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_aead(ctx->fback);
+ 	safexcel_aead_cra_exit(tfm);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_chachapoly = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_CHACHA20 | SAFEXCEL_ALG_POLY1305,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_chachapoly_setkey,
+ 		.setauthsize = safexcel_aead_chachapoly_setauthsize,
+ 		.encrypt = safexcel_aead_chachapoly_encrypt,
+ 		.decrypt = safexcel_aead_chachapoly_decrypt,
+ 		.ivsize = CHACHAPOLY_IV_SIZE,
+ 		.maxauthsize = POLY1305_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc7539(chacha20,poly1305)",
+ 			.cra_driver_name = "safexcel-chacha20-poly1305",
+ 			/* +1 to put it above HW chacha + SW poly */
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY + 1,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				     CRYPTO_ALG_NEED_FALLBACK,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_chachapoly_cra_init,
+ 			.cra_exit = safexcel_aead_fallback_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_chachapolyesp_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_chachapoly_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP;
+ 	ctx->aadskip = EIP197_AEAD_IPSEC_IV_SIZE;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_chachapoly_esp = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_CHACHA20 | SAFEXCEL_ALG_POLY1305,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_chachapoly_setkey,
+ 		.setauthsize = safexcel_aead_chachapoly_setauthsize,
+ 		.encrypt = safexcel_aead_chachapoly_encrypt,
+ 		.decrypt = safexcel_aead_chachapoly_decrypt,
+ 		.ivsize = CHACHAPOLY_IV_SIZE - EIP197_AEAD_IPSEC_NONCE_SIZE,
+ 		.maxauthsize = POLY1305_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc7539esp(chacha20,poly1305)",
+ 			.cra_driver_name = "safexcel-chacha20-poly1305-esp",
+ 			/* +1 to put it above HW chacha + SW poly */
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY + 1,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				     CRYPTO_ALG_NEED_FALLBACK,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_chachapolyesp_cra_init,
+ 			.cra_exit = safexcel_aead_fallback_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_setkey(struct crypto_skcipher *ctfm,
+ 					const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 
+ 	if (len != SM4_KEY_SIZE)
+ 		return -EINVAL;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, SM4_KEY_SIZE))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, SM4_KEY_SIZE);
+ 	ctx->key_len = SM4_KEY_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sm4_blk_encrypt(struct skcipher_request *req)
+ {
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else
+ 		return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 					  SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_sm4_blk_decrypt(struct skcipher_request *req)
+ {
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else
+ 		return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 					  SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_skcipher_sm4_ecb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_ECB;
+ 	ctx->blocksz = 0;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ecb_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_sm4_blk_encrypt,
+ 		.decrypt = safexcel_sm4_blk_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.base = {
+ 			.cra_name = "ecb(sm4)",
+ 			.cra_driver_name = "safexcel-ecb-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_ecb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_cbc_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CBC;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbc_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_sm4_blk_encrypt,
+ 		.decrypt = safexcel_sm4_blk_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cbc(sm4)",
+ 			.cra_driver_name = "safexcel-cbc-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_ofb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_OFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ofb_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "ofb(sm4)",
+ 			.cra_driver_name = "safexcel-ofb-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_ofb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_cfb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cfb_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cfb(sm4)",
+ 			.cra_driver_name = "safexcel-cfb-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_cfb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4ctr_setkey(struct crypto_skcipher *ctfm,
+ 					   const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* last 4 bytes of key are the nonce! */
+ 	ctx->nonce = *(u32 *)(key + len - CTR_RFC3686_NONCE_SIZE);
+ 	/* exclude the nonce here */
+ 	len -= CTR_RFC3686_NONCE_SIZE;
+ 
+ 	return safexcel_skcipher_sm4_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_skcipher_sm4_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ctr_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4ctr_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		/* Add nonce size */
+ 		.min_keysize = SM4_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc3686(ctr(sm4))",
+ 			.cra_driver_name = "safexcel-ctr-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_ctr_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sm4_blk_encrypt(struct aead_request *req)
+ {
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, aead_request_ctx(req),
+ 				  SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_aead_sm4_blk_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if ((req->cryptlen - crypto_aead_authsize(tfm)) & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, aead_request_ctx(req),
+ 				  SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sha1_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA1;
+ 	ctx->state_sz = SHA1_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_cbc_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_sm4_blk_encrypt,
+ 		.decrypt = safexcel_aead_sm4_blk_decrypt,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),cbc(sm4))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-cbc-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4cbc_sha1_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_fallback_setkey(struct crypto_aead *ctfm,
+ 					 const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* Keep fallback cipher synchronized */
+ 	return crypto_aead_setkey(ctx->fback, (u8 *)key, len) ?:
+ 	       safexcel_aead_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_aead_fallback_setauthsize(struct crypto_aead *ctfm,
+ 					      unsigned int authsize)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* Keep fallback cipher synchronized */
+ 	return crypto_aead_setauthsize(ctx->fback, authsize);
+ }
+ 
+ static int safexcel_aead_fallback_crypt(struct aead_request *req,
+ 					enum safexcel_cipher_direction dir)
+ {
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(aead);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 
+ 	aead_request_set_tfm(subreq, ctx->fback);
+ 	aead_request_set_callback(subreq, req->base.flags, req->base.complete,
+ 				  req->base.data);
+ 	aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 			       req->iv);
+ 	aead_request_set_ad(subreq, req->assoclen);
+ 
+ 	return (dir ==  SAFEXCEL_ENCRYPT) ?
+ 		crypto_aead_encrypt(subreq) :
+ 		crypto_aead_decrypt(subreq);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sm3_encrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else if (req->cryptlen || req->assoclen) /* If input length > 0 only */
+ 		return safexcel_queue_req(&req->base, creq, SAFEXCEL_ENCRYPT);
+ 
+ 	/* HW cannot do full (AAD+payload) zero length, use fallback */
+ 	return safexcel_aead_fallback_crypt(req, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sm3_decrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if ((req->cryptlen - crypto_aead_authsize(tfm)) & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else if (req->cryptlen > crypto_aead_authsize(tfm) || req->assoclen)
+ 		/* If input length > 0 only */
+ 		return safexcel_queue_req(&req->base, creq, SAFEXCEL_DECRYPT);
+ 
+ 	/* HW cannot do full (AAD+payload) zero length, use fallback */
+ 	return safexcel_aead_fallback_crypt(req, SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sm3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_fallback_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;
+ 	ctx->state_sz = SM3_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sm3_cbc_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SM3,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_fallback_setkey,
+ 		.setauthsize = safexcel_aead_fallback_setauthsize,
+ 		.encrypt = safexcel_aead_sm4cbc_sm3_encrypt,
+ 		.decrypt = safexcel_aead_sm4cbc_sm3_decrypt,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.maxauthsize = SM3_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sm3),cbc(sm4))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sm3-cbc-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				     CRYPTO_ALG_NEED_FALLBACK,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4cbc_sm3_cra_init,
+ 			.cra_exit = safexcel_aead_fallback_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sm4ctr_sha1_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sm4cbc_sha1_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_ctr_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),rfc3686(ctr(sm4)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-ctr-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4ctr_sha1_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sm4ctr_sm3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sm4cbc_sm3_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sm3_ctr_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SM3,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SM3_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sm3),rfc3686(ctr(sm4)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sm3-ctr-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4ctr_sm3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_rfc4106_gcm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				       unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* last 4 bytes of key are the nonce! */
+ 	ctx->nonce = *(u32 *)(key + len - CTR_RFC3686_NONCE_SIZE);
+ 
+ 	len -= CTR_RFC3686_NONCE_SIZE;
+ 	return safexcel_aead_gcm_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_rfc4106_gcm_setauthsize(struct crypto_aead *tfm,
+ 					    unsigned int authsize)
+ {
+ 	return crypto_rfc4106_check_authsize(authsize);
+ }
+ 
+ static int safexcel_rfc4106_encrypt(struct aead_request *req)
+ {
+ 	return crypto_ipsec_check_assoclen(req->assoclen) ?:
+ 	       safexcel_aead_encrypt(req);
+ }
+ 
+ static int safexcel_rfc4106_decrypt(struct aead_request *req)
+ {
+ 	return crypto_ipsec_check_assoclen(req->assoclen) ?:
+ 	       safexcel_aead_decrypt(req);
+ }
+ 
+ static int safexcel_rfc4106_gcm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_gcm_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP;
+ 	ctx->aadskip = EIP197_AEAD_IPSEC_IV_SIZE;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_rfc4106_gcm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_GHASH,
+ 	.alg.aead = {
+ 		.setkey = safexcel_rfc4106_gcm_setkey,
+ 		.setauthsize = safexcel_rfc4106_gcm_setauthsize,
+ 		.encrypt = safexcel_rfc4106_encrypt,
+ 		.decrypt = safexcel_rfc4106_decrypt,
+ 		.ivsize = GCM_RFC4106_IV_SIZE,
+ 		.maxauthsize = GHASH_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc4106(gcm(aes))",
+ 			.cra_driver_name = "safexcel-rfc4106-gcm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_rfc4106_gcm_cra_init,
+ 			.cra_exit = safexcel_aead_gcm_cra_exit,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_rfc4543_gcm_setauthsize(struct crypto_aead *tfm,
+ 					    unsigned int authsize)
+ {
+ 	if (authsize != GHASH_DIGEST_SIZE)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_rfc4543_gcm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_gcm_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP_GMAC;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_rfc4543_gcm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_GHASH,
+ 	.alg.aead = {
+ 		.setkey = safexcel_rfc4106_gcm_setkey,
+ 		.setauthsize = safexcel_rfc4543_gcm_setauthsize,
+ 		.encrypt = safexcel_rfc4106_encrypt,
+ 		.decrypt = safexcel_rfc4106_decrypt,
+ 		.ivsize = GCM_RFC4543_IV_SIZE,
+ 		.maxauthsize = GHASH_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc4543(gcm(aes))",
+ 			.cra_driver_name = "safexcel-rfc4543-gcm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_rfc4543_gcm_cra_init,
+ 			.cra_exit = safexcel_aead_gcm_cra_exit,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_rfc4309_ccm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				       unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* First byte of the nonce = L = always 3 for RFC4309 (4 byte ctr) */
+ 	*(u8 *)&ctx->nonce = EIP197_AEAD_IPSEC_COUNTER_SIZE - 1;
+ 	/* last 3 bytes of key are the nonce! */
+ 	memcpy((u8 *)&ctx->nonce + 1, key + len -
+ 	       EIP197_AEAD_IPSEC_CCM_NONCE_SIZE,
+ 	       EIP197_AEAD_IPSEC_CCM_NONCE_SIZE);
+ 
+ 	len -= EIP197_AEAD_IPSEC_CCM_NONCE_SIZE;
+ 	return safexcel_aead_ccm_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_rfc4309_ccm_setauthsize(struct crypto_aead *tfm,
+ 					    unsigned int authsize)
+ {
+ 	/* Borrowed from crypto/ccm.c */
+ 	switch (authsize) {
+ 	case 8:
+ 	case 12:
+ 	case 16:
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_rfc4309_ccm_encrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	/* Borrowed from crypto/ccm.c */
+ 	if (req->assoclen != 16 && req->assoclen != 20)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_rfc4309_ccm_decrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	/* Borrowed from crypto/ccm.c */
+ 	if (req->assoclen != 16 && req->assoclen != 20)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_rfc4309_ccm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_ccm_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP;
+ 	ctx->aadskip = EIP197_AEAD_IPSEC_IV_SIZE;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_rfc4309_ccm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_CBC_MAC_ALL,
+ 	.alg.aead = {
+ 		.setkey = safexcel_rfc4309_ccm_setkey,
+ 		.setauthsize = safexcel_rfc4309_ccm_setauthsize,
+ 		.encrypt = safexcel_rfc4309_ccm_encrypt,
+ 		.decrypt = safexcel_rfc4309_ccm_decrypt,
+ 		.ivsize = EIP197_AEAD_IPSEC_IV_SIZE,
+ 		.maxauthsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc4309(ccm(aes))",
+ 			.cra_driver_name = "safexcel-rfc4309-ccm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_rfc4309_ccm_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
diff --cc drivers/crypto/inside-secure/safexcel_hash.c
index c77b0e1655a8,088d7f8aab5e..000000000000
--- a/drivers/crypto/inside-secure/safexcel_hash.c
+++ b/drivers/crypto/inside-secure/safexcel_hash.c
@@@ -1251,3 -1499,1621 +1251,1624 @@@ struct safexcel_alg_template safexcel_a
  		},
  	},
  };
++<<<<<<< HEAD
++=======
+ 
+ static int safexcel_sha512_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sha512_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_sha512_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha512_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_sha512_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha512",
+ 				.cra_driver_name = "safexcel-sha512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha384_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sha384_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_sha384_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha384_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_sha384_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha384",
+ 				.cra_driver_name = "safexcel-sha384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha512_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				       unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sha512",
+ 					SHA512_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_sha512_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, SHA512_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= SHA512_BLOCK_SIZE;
+ 	req->processed	= SHA512_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha512_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_sha512_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha512_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_sha512_digest,
+ 		.setkey = safexcel_hmac_sha512_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha512)",
+ 				.cra_driver_name = "safexcel-hmac-sha512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha384_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				       unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sha384",
+ 					SHA512_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_sha384_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, SHA512_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= SHA512_BLOCK_SIZE;
+ 	req->processed	= SHA512_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha384_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_sha384_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha384_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_sha384_digest,
+ 		.setkey = safexcel_hmac_sha384_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha384)",
+ 				.cra_driver_name = "safexcel-hmac-sha384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_md5_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_MD5;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = MD5_DIGEST_SIZE;
+ 	req->digest_sz = MD5_DIGEST_SIZE;
+ 	req->block_sz = MD5_HMAC_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_md5_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_md5_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_md5 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_MD5,
+ 	.alg.ahash = {
+ 		.init = safexcel_md5_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_md5_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = MD5_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "md5",
+ 				.cra_driver_name = "safexcel-md5",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_md5_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, MD5_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= MD5_HMAC_BLOCK_SIZE;
+ 	req->processed	= MD5_HMAC_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_MD5;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = MD5_DIGEST_SIZE;
+ 	req->digest_sz = MD5_DIGEST_SIZE;
+ 	req->block_sz = MD5_HMAC_BLOCK_SIZE;
+ 	req->len_is_le = true; /* MD5 is little endian! ... */
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_md5_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				     unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-md5",
+ 					MD5_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_md5_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_md5_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_md5 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_MD5,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_md5_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_md5_digest,
+ 		.setkey = safexcel_hmac_md5_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = MD5_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(md5)",
+ 				.cra_driver_name = "safexcel-hmac-md5",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_crc32_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret = safexcel_ahash_cra_init(tfm);
+ 
+ 	/* Default 'key' is all zeroes */
+ 	memset(ctx->ipad, 0, sizeof(u32));
+ 	return ret;
+ }
+ 
+ static int safexcel_crc32_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from loaded key */
+ 	req->state[0]	= (__force __le32)le32_to_cpu(~ctx->ipad[0]);
+ 	/* Set processed to non-zero to enable invalidation detection */
+ 	req->len	= sizeof(u32);
+ 	req->processed	= sizeof(u32);
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_CRC32;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_XCM;
+ 	req->state_sz = sizeof(u32);
+ 	req->digest_sz = sizeof(u32);
+ 	req->block_sz = sizeof(u32);
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_crc32_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				 unsigned int keylen)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 
+ 	if (keylen != sizeof(u32))
+ 		return -EINVAL;
+ 
+ 	memcpy(ctx->ipad, key, sizeof(u32));
+ 	return 0;
+ }
+ 
+ static int safexcel_crc32_digest(struct ahash_request *areq)
+ {
+ 	return safexcel_crc32_init(areq) ?: safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_crc32 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_crc32_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_crc32_digest,
+ 		.setkey = safexcel_crc32_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = sizeof(u32),
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "crc32",
+ 				.cra_driver_name = "safexcel-crc32",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_OPTIONAL_KEY |
+ 					     CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = 1,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_crc32_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_cbcmac_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from loaded keys */
+ 	memcpy(req->state, ctx->ipad, ctx->key_sz);
+ 	/* Set processed to non-zero to enable invalidation detection */
+ 	req->len	= AES_BLOCK_SIZE;
+ 	req->processed	= AES_BLOCK_SIZE;
+ 
+ 	req->digest   = CONTEXT_CONTROL_DIGEST_XCM;
+ 	req->state_sz = ctx->key_sz;
+ 	req->digest_sz = AES_BLOCK_SIZE;
+ 	req->block_sz = AES_BLOCK_SIZE;
+ 	req->xcbcmac  = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_cbcmac_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				 unsigned int len)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	memset(ctx->ipad, 0, 2 * AES_BLOCK_SIZE);
+ 	for (i = 0; i < len / sizeof(u32); i++)
+ 		ctx->ipad[i + 8] = (__force __le32)cpu_to_be32(aes.key_enc[i]);
+ 
+ 	if (len == AES_KEYSIZE_192) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else if (len == AES_KEYSIZE_256) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 		ctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	}
+ 	ctx->cbcmac  = true;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_cbcmac_digest(struct ahash_request *areq)
+ {
+ 	return safexcel_cbcmac_init(areq) ?: safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbcmac = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_cbcmac_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_cbcmac_digest,
+ 		.setkey = safexcel_cbcmac_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = AES_BLOCK_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "cbcmac(aes)",
+ 				.cra_driver_name = "safexcel-cbcmac-aes",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = 1,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_xcbcmac_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				 unsigned int len)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 	struct crypto_aes_ctx aes;
+ 	u32 key_tmp[3 * AES_BLOCK_SIZE / sizeof(u32)];
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* precompute the XCBC key material */
+ 	crypto_cipher_clear_flags(ctx->kaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->kaes, crypto_ahash_get_flags(tfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->kaes, key, len);
+ 	crypto_ahash_set_flags(tfm, crypto_cipher_get_flags(ctx->kaes) &
+ 			       CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		return ret;
+ 
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)key_tmp + 2 * AES_BLOCK_SIZE,
+ 		"\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1");
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)key_tmp,
+ 		"\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2");
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)key_tmp + AES_BLOCK_SIZE,
+ 		"\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3");
+ 	for (i = 0; i < 3 * AES_BLOCK_SIZE / sizeof(u32); i++)
+ 		ctx->ipad[i] =
+ 			cpu_to_le32((__force u32)cpu_to_be32(key_tmp[i]));
+ 
+ 	crypto_cipher_clear_flags(ctx->kaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->kaes, crypto_ahash_get_flags(tfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->kaes,
+ 				   (u8 *)key_tmp + 2 * AES_BLOCK_SIZE,
+ 				   AES_MIN_KEY_SIZE);
+ 	crypto_ahash_set_flags(tfm, crypto_cipher_get_flags(ctx->kaes) &
+ 			       CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 	ctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	ctx->cbcmac = false;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_xcbcmac_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_ahash_cra_init(tfm);
+ 	ctx->kaes = crypto_alloc_cipher("aes", 0, 0);
+ 	return PTR_ERR_OR_ZERO(ctx->kaes);
+ }
+ 
+ static void safexcel_xcbcmac_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_cipher(ctx->kaes);
+ 	safexcel_ahash_cra_exit(tfm);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_xcbcmac = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_cbcmac_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_cbcmac_digest,
+ 		.setkey = safexcel_xcbcmac_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = AES_BLOCK_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "xcbc(aes)",
+ 				.cra_driver_name = "safexcel-xcbc-aes",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = AES_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_xcbcmac_cra_init,
+ 				.cra_exit = safexcel_xcbcmac_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_cmac_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				unsigned int len)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 	struct crypto_aes_ctx aes;
+ 	__be64 consts[4];
+ 	u64 _const[2];
+ 	u8 msb_mask, gfmask;
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	for (i = 0; i < len / sizeof(u32); i++)
+ 		ctx->ipad[i + 8] =
+ 			cpu_to_le32((__force u32)cpu_to_be32(aes.key_enc[i]));
+ 
+ 	/* precompute the CMAC key material */
+ 	crypto_cipher_clear_flags(ctx->kaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->kaes, crypto_ahash_get_flags(tfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->kaes, key, len);
+ 	crypto_ahash_set_flags(tfm, crypto_cipher_get_flags(ctx->kaes) &
+ 			       CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* code below borrowed from crypto/cmac.c */
+ 	/* encrypt the zero block */
+ 	memset(consts, 0, AES_BLOCK_SIZE);
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)consts, (u8 *)consts);
+ 
+ 	gfmask = 0x87;
+ 	_const[0] = be64_to_cpu(consts[1]);
+ 	_const[1] = be64_to_cpu(consts[0]);
+ 
+ 	/* gf(2^128) multiply zero-ciphertext with u and u^2 */
+ 	for (i = 0; i < 4; i += 2) {
+ 		msb_mask = ((s64)_const[1] >> 63) & gfmask;
+ 		_const[1] = (_const[1] << 1) | (_const[0] >> 63);
+ 		_const[0] = (_const[0] << 1) ^ msb_mask;
+ 
+ 		consts[i + 0] = cpu_to_be64(_const[1]);
+ 		consts[i + 1] = cpu_to_be64(_const[0]);
+ 	}
+ 	/* end of code borrowed from crypto/cmac.c */
+ 
+ 	for (i = 0; i < 2 * AES_BLOCK_SIZE / sizeof(u32); i++)
+ 		ctx->ipad[i] = (__force __le32)cpu_to_be32(((u32 *)consts)[i]);
+ 
+ 	if (len == AES_KEYSIZE_192) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else if (len == AES_KEYSIZE_256) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 		ctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	}
+ 	ctx->cbcmac = false;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cmac = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_cbcmac_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_cbcmac_digest,
+ 		.setkey = safexcel_cmac_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = AES_BLOCK_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "cmac(aes)",
+ 				.cra_driver_name = "safexcel-cmac-aes",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = AES_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_xcbcmac_cra_init,
+ 				.cra_exit = safexcel_xcbcmac_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sm3_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SM3_DIGEST_SIZE;
+ 	req->digest_sz = SM3_DIGEST_SIZE;
+ 	req->block_sz = SM3_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sm3_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_sm3_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sm3 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SM3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sm3_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_sm3_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SM3_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sm3",
+ 				.cra_driver_name = "safexcel-sm3",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SM3_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sm3_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				    unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sm3",
+ 					SM3_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_sm3_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, SM3_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= SM3_BLOCK_SIZE;
+ 	req->processed	= SM3_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SM3_DIGEST_SIZE;
+ 	req->digest_sz = SM3_DIGEST_SIZE;
+ 	req->block_sz = SM3_BLOCK_SIZE;
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sm3_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_sm3_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sm3 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SM3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sm3_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_sm3_digest,
+ 		.setkey = safexcel_hmac_sm3_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SM3_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sm3)",
+ 				.cra_driver_name = "safexcel-hmac-sm3",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SM3_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_224_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_224;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_224_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_224_DIGEST_SIZE;
+ 	req->block_sz = SHA3_224_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_fbcheck(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 	int ret = 0;
+ 
+ 	if (ctx->do_fallback) {
+ 		ahash_request_set_tfm(subreq, ctx->fback);
+ 		ahash_request_set_callback(subreq, req->base.flags,
+ 					   req->base.complete, req->base.data);
+ 		ahash_request_set_crypt(subreq, req->src, req->result,
+ 					req->nbytes);
+ 		if (!ctx->fb_init_done) {
+ 			if (ctx->fb_do_setkey) {
+ 				/* Set fallback cipher HMAC key */
+ 				u8 key[SHA3_224_BLOCK_SIZE];
+ 
+ 				memcpy(key, ctx->ipad,
+ 				       crypto_ahash_blocksize(ctx->fback) / 2);
+ 				memcpy(key +
+ 				       crypto_ahash_blocksize(ctx->fback) / 2,
+ 				       ctx->opad,
+ 				       crypto_ahash_blocksize(ctx->fback) / 2);
+ 				ret = crypto_ahash_setkey(ctx->fback, key,
+ 					crypto_ahash_blocksize(ctx->fback));
+ 				memzero_explicit(key,
+ 					crypto_ahash_blocksize(ctx->fback));
+ 				ctx->fb_do_setkey = false;
+ 			}
+ 			ret = ret ?: crypto_ahash_init(subreq);
+ 			ctx->fb_init_done = true;
+ 		}
+ 	}
+ 	return ret;
+ }
+ 
+ static int safexcel_sha3_update(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_update(subreq);
+ }
+ 
+ static int safexcel_sha3_final(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_final(subreq);
+ }
+ 
+ static int safexcel_sha3_finup(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback |= !req->nbytes;
+ 	if (ctx->do_fallback)
+ 		/* Update or ex/import happened or len 0, cannot use the HW */
+ 		return safexcel_sha3_fbcheck(req) ?:
+ 		       crypto_ahash_finup(subreq);
+ 	else
+ 		return safexcel_ahash_finup(req);
+ }
+ 
+ static int safexcel_sha3_digest_fallback(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	ctx->fb_init_done = false;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_finup(subreq);
+ }
+ 
+ static int safexcel_sha3_224_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_224_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_sha3_export(struct ahash_request *req, void *out)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_export(subreq, out);
+ }
+ 
+ static int safexcel_sha3_import(struct ahash_request *req, const void *in)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_import(subreq, in);
+ 	// return safexcel_ahash_import(req, in);
+ }
+ 
+ static int safexcel_sha3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct crypto_ahash *ahash = __crypto_ahash_cast(tfm);
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_ahash_cra_init(tfm);
+ 
+ 	/* Allocate fallback implementation */
+ 	ctx->fback = crypto_alloc_ahash(crypto_tfm_alg_name(tfm), 0,
+ 					CRYPTO_ALG_ASYNC |
+ 					CRYPTO_ALG_NEED_FALLBACK);
+ 	if (IS_ERR(ctx->fback))
+ 		return PTR_ERR(ctx->fback);
+ 
+ 	/* Update statesize from fallback algorithm! */
+ 	crypto_hash_alg_common(ahash)->statesize =
+ 		crypto_ahash_statesize(ctx->fback);
+ 	crypto_ahash_set_reqsize(ahash, max(sizeof(struct safexcel_ahash_req),
+ 					    sizeof(struct ahash_request) +
+ 					    crypto_ahash_reqsize(ctx->fback)));
+ 	return 0;
+ }
+ 
+ static void safexcel_sha3_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_ahash(ctx->fback);
+ 	safexcel_ahash_cra_exit(tfm);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_224 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_224_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_224_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_224_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-224",
+ 				.cra_driver_name = "safexcel-sha3-224",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_224_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_256_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_256;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_256_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_256_DIGEST_SIZE;
+ 	req->block_sz = SHA3_256_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_256_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_256_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_256 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_256_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_256_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_256_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-256",
+ 				.cra_driver_name = "safexcel-sha3-256",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_256_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_384_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_384_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_384_DIGEST_SIZE;
+ 	req->block_sz = SHA3_384_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_384_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_384_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_384_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_384_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-384",
+ 				.cra_driver_name = "safexcel-sha3-384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_512_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_512_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_512_DIGEST_SIZE;
+ 	req->block_sz = SHA3_512_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_512_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_512_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_512_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_512_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-512",
+ 				.cra_driver_name = "safexcel-sha3-512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_cra_init(struct crypto_tfm *tfm, const char *alg)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_sha3_cra_init(tfm);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Allocate precalc basic digest implementation */
+ 	ctx->shpre = crypto_alloc_shash(alg, 0, CRYPTO_ALG_NEED_FALLBACK);
+ 	if (IS_ERR(ctx->shpre))
+ 		return PTR_ERR(ctx->shpre);
+ 
+ 	ctx->shdesc = kmalloc(sizeof(*ctx->shdesc) +
+ 			      crypto_shash_descsize(ctx->shpre), GFP_KERNEL);
+ 	if (!ctx->shdesc) {
+ 		crypto_free_shash(ctx->shpre);
+ 		return -ENOMEM;
+ 	}
+ 	ctx->shdesc->tfm = ctx->shpre;
+ 	return 0;
+ }
+ 
+ static void safexcel_hmac_sha3_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_ahash(ctx->fback);
+ 	crypto_free_shash(ctx->shpre);
+ 	kfree(ctx->shdesc);
+ 	safexcel_ahash_cra_exit(tfm);
+ }
+ 
+ static int safexcel_hmac_sha3_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				     unsigned int keylen)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	int ret = 0;
+ 
+ 	if (keylen > crypto_ahash_blocksize(tfm)) {
+ 		/*
+ 		 * If the key is larger than the blocksize, then hash it
+ 		 * first using our fallback cipher
+ 		 */
+ 		ret = crypto_shash_digest(ctx->shdesc, key, keylen,
+ 					  (u8 *)ctx->ipad);
+ 		keylen = crypto_shash_digestsize(ctx->shpre);
+ 
+ 		/*
+ 		 * If the digest is larger than half the blocksize, we need to
+ 		 * move the rest to opad due to the way our HMAC infra works.
+ 		 */
+ 		if (keylen > crypto_ahash_blocksize(tfm) / 2)
+ 			/* Buffers overlap, need to use memmove iso memcpy! */
+ 			memmove(ctx->opad,
+ 				(u8 *)ctx->ipad +
+ 					crypto_ahash_blocksize(tfm) / 2,
+ 				keylen - crypto_ahash_blocksize(tfm) / 2);
+ 	} else {
+ 		/*
+ 		 * Copy the key to our ipad & opad buffers
+ 		 * Note that ipad and opad each contain one half of the key,
+ 		 * to match the existing HMAC driver infrastructure.
+ 		 */
+ 		if (keylen <= crypto_ahash_blocksize(tfm) / 2) {
+ 			memcpy(ctx->ipad, key, keylen);
+ 		} else {
+ 			memcpy(ctx->ipad, key,
+ 			       crypto_ahash_blocksize(tfm) / 2);
+ 			memcpy(ctx->opad,
+ 			       key + crypto_ahash_blocksize(tfm) / 2,
+ 			       keylen - crypto_ahash_blocksize(tfm) / 2);
+ 		}
+ 	}
+ 
+ 	/* Pad key with zeroes */
+ 	if (keylen <= crypto_ahash_blocksize(tfm) / 2) {
+ 		memset((u8 *)ctx->ipad + keylen, 0,
+ 		       crypto_ahash_blocksize(tfm) / 2 - keylen);
+ 		memset(ctx->opad, 0, crypto_ahash_blocksize(tfm) / 2);
+ 	} else {
+ 		memset((u8 *)ctx->opad + keylen -
+ 		       crypto_ahash_blocksize(tfm) / 2, 0,
+ 		       crypto_ahash_blocksize(tfm) - keylen);
+ 	}
+ 
+ 	/* If doing fallback, still need to set the new key! */
+ 	ctx->fb_do_setkey = true;
+ 	return ret;
+ }
+ 
+ static int safexcel_hmac_sha3_224_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_224_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_224_BLOCK_SIZE;
+ 	req->processed	= SHA3_224_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_224;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_224_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_224_DIGEST_SIZE;
+ 	req->block_sz = SHA3_224_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_224_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_224_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_224_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-224");
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_224 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_224_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_224_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_224_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-224)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-224",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_224_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_224_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_256_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_256_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_256_BLOCK_SIZE;
+ 	req->processed	= SHA3_256_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_256;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_256_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_256_DIGEST_SIZE;
+ 	req->block_sz = SHA3_256_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_256_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_256_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_256_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-256");
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_256 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_256_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_256_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_256_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-256)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-256",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_256_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_256_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_384_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_384_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_384_BLOCK_SIZE;
+ 	req->processed	= SHA3_384_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_384_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_384_DIGEST_SIZE;
+ 	req->block_sz = SHA3_384_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_384_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_384_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_384_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-384");
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_384_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_384_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-384)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_384_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_512_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_512_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_512_BLOCK_SIZE;
+ 	req->processed	= SHA3_512_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_512_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_512_DIGEST_SIZE;
+ 	req->block_sz = SHA3_512_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_512_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_512_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_512_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-512");
+ }
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_512_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_512_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-512)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_512_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
diff --cc drivers/crypto/marvell/cipher.c
index 177a27b6e4db,c24f34a48cef..000000000000
--- a/drivers/crypto/marvell/cipher.c
+++ b/drivers/crypto/marvell/cipher.c
@@@ -257,11 -254,9 +257,15 @@@ static int mv_cesa_aes_setkey(struct cr
  	int ret;
  	int i;
  
++<<<<<<< HEAD
 +	ret = crypto_aes_expand_key(&ctx->aes, key, len);
 +	if (ret) {
 +		crypto_skcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
+ 	ret = aes_expandkey(&ctx->aes, key, len);
+ 	if (ret)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return ret;
- 	}
  
  	remaining = (ctx->aes.key_length - 16) / 4;
  	offset = ctx->aes.key_length + 24 - remaining;
diff --cc drivers/crypto/mediatek/mtk-aes.c
index c2058cf59f57,00e580bf8536..000000000000
--- a/drivers/crypto/mediatek/mtk-aes.c
+++ b/drivers/crypto/mediatek/mtk-aes.c
@@@ -642,7 -652,6 +642,10 @@@ static int mtk_aes_setkey(struct crypto
  		break;
  
  	default:
++<<<<<<< HEAD
 +		crypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
  	}
  
diff --cc drivers/crypto/n2_core.c
index 8030bb60fb1c,f5c468f2cc82..000000000000
--- a/drivers/crypto/n2_core.c
+++ b/drivers/crypto/n2_core.c
@@@ -746,7 -746,6 +746,10 @@@ static int n2_aes_setkey(struct crypto_
  		ctx->enc_type |= ENC_TYPE_ALG_AES256;
  		break;
  	default:
++<<<<<<< HEAD
 +		crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
  	}
  
diff --cc drivers/crypto/padlock-aes.c
index 09d823d36d3a,594d6b1695d5..000000000000
--- a/drivers/crypto/padlock-aes.c
+++ b/drivers/crypto/padlock-aes.c
@@@ -144,10 -143,8 +141,13 @@@ static int aes_set_key(struct crypto_tf
  	ctx->cword.encrypt.keygen = 1;
  	ctx->cword.decrypt.keygen = 1;
  
++<<<<<<< HEAD
 +	if (crypto_aes_expand_key(&gen_aes, in_key, key_len)) {
 +		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
++=======
+ 	if (aes_expandkey(&gen_aes, in_key, key_len))
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
- 	}
  
  	memcpy(ctx->E, gen_aes.key_enc, AES_MAX_KEYLENGTH);
  	memcpy(ctx->D, gen_aes.key_dec, AES_MAX_KEYLENGTH);
diff --cc drivers/crypto/picoxcell_crypto.c
index 6a0a275f56f6,ced4cbed9ea0..000000000000
--- a/drivers/crypto/picoxcell_crypto.c
+++ b/drivers/crypto/picoxcell_crypto.c
@@@ -800,10 -779,8 +799,13 @@@ static int spacc_aes_setkey(struct cryp
  	struct spacc_ablk_ctx *ctx = crypto_tfm_ctx(tfm);
  	int err = 0;
  
++<<<<<<< HEAD
 +	if (len > AES_MAX_KEY_SIZE) {
 +		crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
+ 	if (len > AES_MAX_KEY_SIZE)
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		return -EINVAL;
- 	}
  
  	/*
  	 * IPSec engine only supports 128 and 256 bit AES keys. If we get a
@@@ -850,7 -827,6 +852,10 @@@ static int spacc_kasumi_f8_setkey(struc
  	int err = 0;
  
  	if (len > AES_MAX_KEY_SIZE) {
++<<<<<<< HEAD
 +		crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  		err = -EINVAL;
  		goto out;
  	}
diff --cc drivers/crypto/sunxi-ss/sun4i-ss-cipher.c
index 41721aa243d5,7f22d305178e..000000000000
--- a/drivers/crypto/sunxi-ss/sun4i-ss-cipher.c
+++ b/drivers/crypto/sunxi-ss/sun4i-ss-cipher.c
@@@ -489,8 -540,7 +489,12 @@@ int sun4i_ss_aes_setkey(struct crypto_s
  		op->keymode = SS_AES_256BITS;
  		break;
  	default:
++<<<<<<< HEAD:drivers/crypto/sunxi-ss/sun4i-ss-cipher.c
 +		dev_err(ss->dev, "ERROR: Invalid keylen %u\n", keylen);
 +		crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
++=======
+ 		dev_dbg(ss->dev, "ERROR: Invalid keylen %u\n", keylen);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN):drivers/crypto/allwinner/sun4i-ss/sun4i-ss-cipher.c
  		return -EINVAL;
  	}
  	op->keylen = keylen;
diff --cc drivers/crypto/talitos.c
index 001a50ae58dd,9c6db7f698c4..000000000000
--- a/drivers/crypto/talitos.c
+++ b/drivers/crypto/talitos.c
@@@ -928,17 -932,11 +927,17 @@@ static int aead_des3_setkey(struct cryp
  
  	err = -EINVAL;
  	if (keys.authkeylen + keys.enckeylen > TALITOS_MAX_KEY_SIZE)
- 		goto badkey;
+ 		goto out;
  
 -	err = verify_aead_des3_key(authenc, keys.enckey, keys.enckeylen);
 -	if (err)
 +	if (keys.enckeylen != DES3_EDE_KEY_SIZE)
 +		goto badkey;
 +
 +	flags = crypto_aead_get_flags(authenc);
 +	err = __des3_verify_key(&flags, keys.enckey);
 +	if (unlikely(err)) {
 +		crypto_aead_set_flags(authenc, flags);
  		goto out;
 +	}
  
  	if (ctx->keylen)
  		dma_unmap_single(dev, ctx->dma_key, ctx->keylen, DMA_TO_DEVICE);
@@@ -955,42 -953,8 +954,38 @@@
  out:
  	memzero_explicit(&keys, sizeof(keys));
  	return err;
- 
- badkey:
- 	crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
- 	goto out;
  }
  
 +/*
 + * talitos_edesc - s/w-extended descriptor
 + * @src_nents: number of segments in input scatterlist
 + * @dst_nents: number of segments in output scatterlist
 + * @icv_ool: whether ICV is out-of-line
 + * @iv_dma: dma address of iv for checking continuity and link table
 + * @dma_len: length of dma mapped link_tbl space
 + * @dma_link_tbl: bus physical address of link_tbl/buf
 + * @desc: h/w descriptor
 + * @link_tbl: input and output h/w link tables (if {src,dst}_nents > 1) (SEC2)
 + * @buf: input and output buffeur (if {src,dst}_nents > 1) (SEC1)
 + *
 + * if decrypting (with authcheck), or either one of src_nents or dst_nents
 + * is greater than 1, an integrity check value is concatenated to the end
 + * of link_tbl data
 + */
 +struct talitos_edesc {
 +	int src_nents;
 +	int dst_nents;
 +	bool icv_ool;
 +	dma_addr_t iv_dma;
 +	int dma_len;
 +	dma_addr_t dma_link_tbl;
 +	struct talitos_desc desc;
 +	union {
 +		struct talitos_ptr link_tbl[0];
 +		u8 buf[0];
 +	};
 +};
 +
  static void talitos_sg_unmap(struct device *dev,
  			     struct talitos_edesc *edesc,
  			     struct scatterlist *src,
@@@ -1595,35 -1502,28 +1590,39 @@@ static int ablkcipher_setkey(struct cry
  	return 0;
  }
  
 -static int skcipher_des_setkey(struct crypto_skcipher *cipher,
 +static int ablkcipher_des_setkey(struct crypto_ablkcipher *cipher,
  				 const u8 *key, unsigned int keylen)
  {
 -	return verify_skcipher_des_key(cipher, key) ?:
 -	       skcipher_setkey(cipher, key, keylen);
 -}
 +	u32 tmp[DES_EXPKEY_WORDS];
  
 -static int skcipher_des3_setkey(struct crypto_skcipher *cipher,
 -				  const u8 *key, unsigned int keylen)
 -{
 -	return verify_skcipher_des3_key(cipher, key) ?:
 -	       skcipher_setkey(cipher, key, keylen);
 +	if (unlikely(crypto_ablkcipher_get_flags(cipher) &
 +		     CRYPTO_TFM_REQ_WEAK_KEY) &&
 +	    !des_ekey(tmp, key)) {
 +		crypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_WEAK_KEY);
 +		return -EINVAL;
 +	}
 +
 +	return ablkcipher_setkey(cipher, key, keylen);
  }
  
 -static int skcipher_aes_setkey(struct crypto_skcipher *cipher,
 +static int ablkcipher_des3_setkey(struct crypto_ablkcipher *cipher,
  				  const u8 *key, unsigned int keylen)
  {
 -	if (keylen == AES_KEYSIZE_128 || keylen == AES_KEYSIZE_192 ||
 -	    keylen == AES_KEYSIZE_256)
 -		return skcipher_setkey(cipher, key, keylen);
 +	u32 flags;
 +	int err;
  
 +	flags = crypto_ablkcipher_get_flags(cipher);
 +	err = __des3_verify_key(&flags, key);
 +	if (unlikely(err)) {
 +		crypto_ablkcipher_set_flags(cipher, flags);
 +		return err;
 +	}
 +
++<<<<<<< HEAD
 +	return ablkcipher_setkey(cipher, key, keylen);
++=======
+ 	return -EINVAL;
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  
  static void common_nonsnoop_unmap(struct device *dev,
diff --cc drivers/crypto/ux500/cryp/cryp_core.c
index 5b5ad7bcd15f,800dfc4d16c4..000000000000
--- a/drivers/crypto/ux500/cryp/cryp_core.c
+++ b/drivers/crypto/ux500/cryp/cryp_core.c
@@@ -941,11 -947,10 +941,15 @@@ out
  	return ret;
  }
  
 -static int aes_skcipher_setkey(struct crypto_skcipher *cipher,
 +static int aes_ablkcipher_setkey(struct crypto_ablkcipher *cipher,
  				 const u8 *key, unsigned int keylen)
  {
++<<<<<<< HEAD
 +	struct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);
 +	u32 *flags = &cipher->base.crt_flags;
++=======
+ 	struct cryp_ctx *ctx = crypto_skcipher_ctx(cipher);
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  	pr_debug(DEV_DBG_NAME " [%s]", __func__);
  
diff --cc drivers/crypto/virtio/virtio_crypto_algs.c
index af6a908dfa7a,fd045e64972a..000000000000
--- a/drivers/crypto/virtio/virtio_crypto_algs.c
+++ b/drivers/crypto/virtio/virtio_crypto_algs.c
@@@ -285,10 -276,10 +285,10 @@@ static int virtio_crypto_alg_ablkcipher
  	}
  
  	if (virtio_crypto_alg_validate_key(keylen, &alg))
- 		goto bad_key;
+ 		return -EINVAL;
  
  	/* Create encryption session */
 -	ret = virtio_crypto_alg_skcipher_init_session(ctx,
 +	ret = virtio_crypto_alg_ablkcipher_init_session(ctx,
  			alg, key, keylen, 1);
  	if (ret)
  		return ret;
@@@ -300,10 -291,6 +300,13 @@@
  		return ret;
  	}
  	return 0;
++<<<<<<< HEAD
 +
 +bad_key:
 +	crypto_tfm_set_flags(ctx->tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 +	return -EINVAL;
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  }
  
  /* Note: kernel crypto API realization */
diff --cc include/linux/crypto.h
index 6eb06101089f,61fccc7d0efb..000000000000
--- a/include/linux/crypto.h
+++ b/include/linux/crypto.h
@@@ -124,10 -113,6 +124,13 @@@
  #define CRYPTO_TFM_REQ_MAY_SLEEP	0x00000200
  #define CRYPTO_TFM_REQ_MAY_BACKLOG	0x00000400
  #define CRYPTO_TFM_RES_WEAK_KEY		0x00100000
++<<<<<<< HEAD
 +#define CRYPTO_TFM_RES_BAD_KEY_LEN   	0x00200000
 +#define CRYPTO_TFM_RES_BAD_KEY_SCHED 	0x00400000
 +#define CRYPTO_TFM_RES_BAD_BLOCK_LEN 	0x00800000
 +#define CRYPTO_TFM_RES_BAD_FLAGS 	0x01000000
++=======
++>>>>>>> 674f368a952c (crypto: remove CRYPTO_TFM_RES_BAD_KEY_LEN)
  
  /*
   * Miscellaneous stuff.
* Unmerged path arch/x86/crypto/blake2s-glue.c
* Unmerged path crypto/blake2b_generic.c
* Unmerged path crypto/blake2s_generic.c
* Unmerged path crypto/essiv.c
* Unmerged path crypto/xxhash_generic.c
* Unmerged path drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c
* Unmerged path drivers/crypto/allwinner/sun8i-ss/sun8i-ss-cipher.c
* Unmerged path drivers/crypto/amlogic/amlogic-gxl-cipher.c
* Unmerged path drivers/crypto/caam/caamalg_qi2.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_aead.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_skcipher.c
* Unmerged path drivers/crypto/rockchip/rk3288_crypto_skcipher.c
* Unmerged path include/crypto/internal/des.h
diff --git a/arch/arm/crypto/aes-ce-glue.c b/arch/arm/crypto/aes-ce-glue.c
index d0a9cec73707..434110f3d453 100644
--- a/arch/arm/crypto/aes-ce-glue.c
+++ b/arch/arm/crypto/aes-ce-glue.c
@@ -138,14 +138,8 @@ static int ce_aes_setkey(struct crypto_skcipher *tfm, const u8 *in_key,
 			 unsigned int key_len)
 {
 	struct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);
-	int ret;
-
-	ret = ce_aes_expandkey(ctx, in_key, key_len);
-	if (!ret)
-		return 0;
 
-	crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
-	return -EINVAL;
+	return ce_aes_expandkey(ctx, in_key, key_len);
 }
 
 struct crypto_aes_xts_ctx {
@@ -167,11 +161,7 @@ static int xts_set_key(struct crypto_skcipher *tfm, const u8 *in_key,
 	if (!ret)
 		ret = ce_aes_expandkey(&ctx->key2, &in_key[key_len / 2],
 				       key_len / 2);
-	if (!ret)
-		return 0;
-
-	crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
-	return -EINVAL;
+	return ret;
 }
 
 static int ecb_encrypt(struct skcipher_request *req)
diff --git a/arch/arm/crypto/crc32-ce-glue.c b/arch/arm/crypto/crc32-ce-glue.c
index 96e62ec105d0..48109d7e964b 100644
--- a/arch/arm/crypto/crc32-ce-glue.c
+++ b/arch/arm/crypto/crc32-ce-glue.c
@@ -56,10 +56,8 @@ static int crc32_setkey(struct crypto_shash *hash, const u8 *key,
 {
 	u32 *mctx = crypto_shash_ctx(hash);
 
-	if (keylen != sizeof(u32)) {
-		crypto_shash_set_flags(hash, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(u32))
 		return -EINVAL;
-	}
 	*mctx = le32_to_cpup((__le32 *)key);
 	return 0;
 }
diff --git a/arch/arm/crypto/ghash-ce-glue.c b/arch/arm/crypto/ghash-ce-glue.c
index d9bb52cae2ac..77c6af92cb72 100644
--- a/arch/arm/crypto/ghash-ce-glue.c
+++ b/arch/arm/crypto/ghash-ce-glue.c
@@ -123,10 +123,8 @@ static int ghash_setkey(struct crypto_shash *tfm,
 	struct ghash_key *key = crypto_shash_ctx(tfm);
 	u64 a, b;
 
-	if (keylen != GHASH_BLOCK_SIZE) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != GHASH_BLOCK_SIZE)
 		return -EINVAL;
-	}
 
 	/* perform multiplication by 'x' in GF(2^128) */
 	b = get_unaligned_be64(inkey);
diff --git a/arch/arm64/crypto/aes-ce-ccm-glue.c b/arch/arm64/crypto/aes-ce-ccm-glue.c
index 5fc6f51908fd..a92757fb0844 100644
--- a/arch/arm64/crypto/aes-ce-ccm-glue.c
+++ b/arch/arm64/crypto/aes-ce-ccm-glue.c
@@ -51,14 +51,8 @@ static int ccm_setkey(struct crypto_aead *tfm, const u8 *in_key,
 		      unsigned int key_len)
 {
 	struct crypto_aes_ctx *ctx = crypto_aead_ctx(tfm);
-	int ret;
 
-	ret = ce_aes_expandkey(ctx, in_key, key_len);
-	if (!ret)
-		return 0;
-
-	tfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
-	return -EINVAL;
+	return ce_aes_expandkey(ctx, in_key, key_len);
 }
 
 static int ccm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
diff --git a/arch/arm64/crypto/aes-ce-glue.c b/arch/arm64/crypto/aes-ce-glue.c
index e6b3227bbf57..7e7bd0f1ac37 100644
--- a/arch/arm64/crypto/aes-ce-glue.c
+++ b/arch/arm64/crypto/aes-ce-glue.c
@@ -148,14 +148,8 @@ int ce_aes_setkey(struct crypto_tfm *tfm, const u8 *in_key,
 		  unsigned int key_len)
 {
 	struct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);
-	int ret;
 
-	ret = ce_aes_expandkey(ctx, in_key, key_len);
-	if (!ret)
-		return 0;
-
-	tfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
-	return -EINVAL;
+	return ce_aes_expandkey(ctx, in_key, key_len);
 }
 EXPORT_SYMBOL(ce_aes_setkey);
 
* Unmerged path arch/arm64/crypto/aes-glue.c
* Unmerged path arch/arm64/crypto/ghash-ce-glue.c
diff --git a/arch/mips/crypto/crc32-mips.c b/arch/mips/crypto/crc32-mips.c
index 7d1d2425746f..faa88a6a74c0 100644
--- a/arch/mips/crypto/crc32-mips.c
+++ b/arch/mips/crypto/crc32-mips.c
@@ -177,10 +177,8 @@ static int chksum_setkey(struct crypto_shash *tfm, const u8 *key,
 {
 	struct chksum_ctx *mctx = crypto_shash_ctx(tfm);
 
-	if (keylen != sizeof(mctx->key)) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(mctx->key))
 		return -EINVAL;
-	}
 	mctx->key = get_unaligned_le32(key);
 	return 0;
 }
* Unmerged path arch/powerpc/crypto/aes-spe-glue.c
diff --git a/arch/powerpc/crypto/crc32c-vpmsum_glue.c b/arch/powerpc/crypto/crc32c-vpmsum_glue.c
index fd1d6c83f0c0..a38f99f3b788 100644
--- a/arch/powerpc/crypto/crc32c-vpmsum_glue.c
+++ b/arch/powerpc/crypto/crc32c-vpmsum_glue.c
@@ -70,10 +70,8 @@ static int crc32c_vpmsum_setkey(struct crypto_shash *hash, const u8 *key,
 {
 	u32 *mctx = crypto_shash_ctx(hash);
 
-	if (keylen != sizeof(u32)) {
-		crypto_shash_set_flags(hash, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(u32))
 		return -EINVAL;
-	}
 	*mctx = le32_to_cpup((__le32 *)key);
 	return 0;
 }
* Unmerged path arch/s390/crypto/aes_s390.c
diff --git a/arch/s390/crypto/crc32-vx.c b/arch/s390/crypto/crc32-vx.c
index 423ee05887e6..fafecad20752 100644
--- a/arch/s390/crypto/crc32-vx.c
+++ b/arch/s390/crypto/crc32-vx.c
@@ -111,10 +111,8 @@ static int crc32_vx_setkey(struct crypto_shash *tfm, const u8 *newkey,
 {
 	struct crc_ctx *mctx = crypto_shash_ctx(tfm);
 
-	if (newkeylen != sizeof(mctx->key)) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (newkeylen != sizeof(mctx->key))
 		return -EINVAL;
-	}
 	mctx->key = le32_to_cpu(*(__le32 *)newkey);
 	return 0;
 }
@@ -124,10 +122,8 @@ static int crc32be_vx_setkey(struct crypto_shash *tfm, const u8 *newkey,
 {
 	struct crc_ctx *mctx = crypto_shash_ctx(tfm);
 
-	if (newkeylen != sizeof(mctx->key)) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (newkeylen != sizeof(mctx->key))
 		return -EINVAL;
-	}
 	mctx->key = be32_to_cpu(*(__be32 *)newkey);
 	return 0;
 }
diff --git a/arch/s390/crypto/ghash_s390.c b/arch/s390/crypto/ghash_s390.c
index 2792e2d522bf..b5d603c49f5d 100644
--- a/arch/s390/crypto/ghash_s390.c
+++ b/arch/s390/crypto/ghash_s390.c
@@ -43,10 +43,8 @@ static int ghash_setkey(struct crypto_shash *tfm,
 {
 	struct ghash_ctx *ctx = crypto_shash_ctx(tfm);
 
-	if (keylen != GHASH_BLOCK_SIZE) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != GHASH_BLOCK_SIZE)
 		return -EINVAL;
-	}
 
 	memcpy(ctx->key, key, GHASH_BLOCK_SIZE);
 
* Unmerged path arch/s390/crypto/paes_s390.c
diff --git a/arch/sparc/crypto/aes_glue.c b/arch/sparc/crypto/aes_glue.c
index 3cd4f6b198b6..3198ba5d5368 100644
--- a/arch/sparc/crypto/aes_glue.c
+++ b/arch/sparc/crypto/aes_glue.c
@@ -167,7 +167,6 @@ static int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,
 		       unsigned int key_len)
 {
 	struct crypto_sparc64_aes_ctx *ctx = crypto_tfm_ctx(tfm);
-	u32 *flags = &tfm->crt_flags;
 
 	switch (key_len) {
 	case AES_KEYSIZE_128:
@@ -186,7 +185,6 @@ static int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,
 		break;
 
 	default:
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 		return -EINVAL;
 	}
 
diff --git a/arch/sparc/crypto/camellia_glue.c b/arch/sparc/crypto/camellia_glue.c
index 561a84d93cf6..69de3c450f1c 100644
--- a/arch/sparc/crypto/camellia_glue.c
+++ b/arch/sparc/crypto/camellia_glue.c
@@ -37,12 +37,9 @@ static int camellia_set_key(struct crypto_tfm *tfm, const u8 *_in_key,
 {
 	struct camellia_sparc64_ctx *ctx = crypto_tfm_ctx(tfm);
 	const u32 *in_key = (const u32 *) _in_key;
-	u32 *flags = &tfm->crt_flags;
 
-	if (key_len != 16 && key_len != 24 && key_len != 32) {
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+	if (key_len != 16 && key_len != 24 && key_len != 32)
 		return -EINVAL;
-	}
 
 	ctx->key_len = key_len;
 
diff --git a/arch/sparc/crypto/crc32c_glue.c b/arch/sparc/crypto/crc32c_glue.c
index 8aa664638c3c..1d31b922a8a2 100644
--- a/arch/sparc/crypto/crc32c_glue.c
+++ b/arch/sparc/crypto/crc32c_glue.c
@@ -32,10 +32,8 @@ static int crc32c_sparc64_setkey(struct crypto_shash *hash, const u8 *key,
 {
 	u32 *mctx = crypto_shash_ctx(hash);
 
-	if (keylen != sizeof(u32)) {
-		crypto_shash_set_flags(hash, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(u32))
 		return -EINVAL;
-	}
 	*(__le32 *)mctx = le32_to_cpup((__le32 *)key);
 	return 0;
 }
diff --git a/arch/x86/crypto/aegis128-aesni-glue.c b/arch/x86/crypto/aegis128-aesni-glue.c
index acd11b3bf639..d0001ecb7b70 100644
--- a/arch/x86/crypto/aegis128-aesni-glue.c
+++ b/arch/x86/crypto/aegis128-aesni-glue.c
@@ -159,10 +159,8 @@ static int crypto_aegis128_aesni_setkey(struct crypto_aead *aead, const u8 *key,
 {
 	struct aegis_ctx *ctx = crypto_aegis128_aesni_ctx(aead);
 
-	if (keylen != AEGIS128_KEY_SIZE) {
-		crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != AEGIS128_KEY_SIZE)
 		return -EINVAL;
-	}
 
 	memcpy(ctx->key.bytes, key, AEGIS128_KEY_SIZE);
 
diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
index acbe7e8336d8..d5808e5511f6 100644
--- a/arch/x86/crypto/aesni-intel_glue.c
+++ b/arch/x86/crypto/aesni-intel_glue.c
@@ -351,14 +351,11 @@ static int aes_set_key_common(struct crypto_tfm *tfm, void *raw_ctx,
 			      const u8 *in_key, unsigned int key_len)
 {
 	struct crypto_aes_ctx *ctx = aes_ctx(raw_ctx);
-	u32 *flags = &tfm->crt_flags;
 	int err;
 
 	if (key_len != AES_KEYSIZE_128 && key_len != AES_KEYSIZE_192 &&
-	    key_len != AES_KEYSIZE_256) {
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+	    key_len != AES_KEYSIZE_256)
 		return -EINVAL;
-	}
 
 	if (!irq_fpu_usable())
 		err = crypto_aes_expand_key(ctx, in_key, key_len);
@@ -723,10 +720,9 @@ static int common_rfc4106_set_key(struct crypto_aead *aead, const u8 *key,
 {
 	struct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(aead);
 
-	if (key_len < 4) {
-		crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (key_len < 4)
 		return -EINVAL;
-	}
+
 	/*Account for 4 byte nonce at the end.*/
 	key_len -= 4;
 
* Unmerged path arch/x86/crypto/blake2s-glue.c
diff --git a/arch/x86/crypto/camellia_aesni_avx2_glue.c b/arch/x86/crypto/camellia_aesni_avx2_glue.c
index d4992e458f92..7bec0cc2e2e9 100644
--- a/arch/x86/crypto/camellia_aesni_avx2_glue.c
+++ b/arch/x86/crypto/camellia_aesni_avx2_glue.c
@@ -150,8 +150,7 @@ static const struct common_glue_ctx camellia_dec_xts = {
 static int camellia_setkey(struct crypto_skcipher *tfm, const u8 *key,
 			   unsigned int keylen)
 {
-	return __camellia_setkey(crypto_skcipher_ctx(tfm), key, keylen,
-				 &tfm->base.crt_flags);
+	return __camellia_setkey(crypto_skcipher_ctx(tfm), key, keylen);
 }
 
 static int ecb_encrypt(struct skcipher_request *req)
diff --git a/arch/x86/crypto/camellia_aesni_avx_glue.c b/arch/x86/crypto/camellia_aesni_avx_glue.c
index d09f6521466a..9d8762b4e808 100644
--- a/arch/x86/crypto/camellia_aesni_avx_glue.c
+++ b/arch/x86/crypto/camellia_aesni_avx_glue.c
@@ -154,8 +154,7 @@ static const struct common_glue_ctx camellia_dec_xts = {
 static int camellia_setkey(struct crypto_skcipher *tfm, const u8 *key,
 			   unsigned int keylen)
 {
-	return __camellia_setkey(crypto_skcipher_ctx(tfm), key, keylen,
-				 &tfm->base.crt_flags);
+	return __camellia_setkey(crypto_skcipher_ctx(tfm), key, keylen);
 }
 
 static int ecb_encrypt(struct skcipher_request *req)
@@ -188,7 +187,6 @@ int xts_camellia_setkey(struct crypto_skcipher *tfm, const u8 *key,
 			unsigned int keylen)
 {
 	struct camellia_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
-	u32 *flags = &tfm->base.crt_flags;
 	int err;
 
 	err = xts_verify_key(tfm, key, keylen);
@@ -196,13 +194,12 @@ int xts_camellia_setkey(struct crypto_skcipher *tfm, const u8 *key,
 		return err;
 
 	/* first half of xts-key is for crypt */
-	err = __camellia_setkey(&ctx->crypt_ctx, key, keylen / 2, flags);
+	err = __camellia_setkey(&ctx->crypt_ctx, key, keylen / 2);
 	if (err)
 		return err;
 
 	/* second half of xts-key is for tweak */
-	return __camellia_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2,
-				flags);
+	return __camellia_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2);
 }
 EXPORT_SYMBOL_GPL(xts_camellia_setkey);
 
diff --git a/arch/x86/crypto/camellia_glue.c b/arch/x86/crypto/camellia_glue.c
index dcd5e0f71b00..610e8263dd51 100644
--- a/arch/x86/crypto/camellia_glue.c
+++ b/arch/x86/crypto/camellia_glue.c
@@ -1244,12 +1244,10 @@ static void camellia_setup192(const unsigned char *key, u64 *subkey)
 }
 
 int __camellia_setkey(struct camellia_ctx *cctx, const unsigned char *key,
-		      unsigned int key_len, u32 *flags)
+		      unsigned int key_len)
 {
-	if (key_len != 16 && key_len != 24 && key_len != 32) {
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+	if (key_len != 16 && key_len != 24 && key_len != 32)
 		return -EINVAL;
-	}
 
 	cctx->key_length = key_len;
 
@@ -1272,8 +1270,7 @@ EXPORT_SYMBOL_GPL(__camellia_setkey);
 static int camellia_setkey(struct crypto_tfm *tfm, const u8 *key,
 			   unsigned int key_len)
 {
-	return __camellia_setkey(crypto_tfm_ctx(tfm), key, key_len,
-				 &tfm->crt_flags);
+	return __camellia_setkey(crypto_tfm_ctx(tfm), key, key_len);
 }
 
 static int camellia_setkey_skcipher(struct crypto_skcipher *tfm, const u8 *key,
diff --git a/arch/x86/crypto/cast6_avx_glue.c b/arch/x86/crypto/cast6_avx_glue.c
index 18965c39305e..a6e5c476dc5b 100644
--- a/arch/x86/crypto/cast6_avx_glue.c
+++ b/arch/x86/crypto/cast6_avx_glue.c
@@ -192,7 +192,6 @@ static int xts_cast6_setkey(struct crypto_skcipher *tfm, const u8 *key,
 			    unsigned int keylen)
 {
 	struct cast6_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
-	u32 *flags = &tfm->base.crt_flags;
 	int err;
 
 	err = xts_verify_key(tfm, key, keylen);
@@ -200,13 +199,12 @@ static int xts_cast6_setkey(struct crypto_skcipher *tfm, const u8 *key,
 		return err;
 
 	/* first half of xts-key is for crypt */
-	err = __cast6_setkey(&ctx->crypt_ctx, key, keylen / 2, flags);
+	err = __cast6_setkey(&ctx->crypt_ctx, key, keylen / 2);
 	if (err)
 		return err;
 
 	/* second half of xts-key is for tweak */
-	return __cast6_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2,
-			      flags);
+	return __cast6_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2);
 }
 
 static int xts_encrypt(struct skcipher_request *req)
diff --git a/arch/x86/crypto/crc32-pclmul_glue.c b/arch/x86/crypto/crc32-pclmul_glue.c
index c8d9cdacbf10..c423b281719b 100644
--- a/arch/x86/crypto/crc32-pclmul_glue.c
+++ b/arch/x86/crypto/crc32-pclmul_glue.c
@@ -93,10 +93,8 @@ static int crc32_pclmul_setkey(struct crypto_shash *hash, const u8 *key,
 {
 	u32 *mctx = crypto_shash_ctx(hash);
 
-	if (keylen != sizeof(u32)) {
-		crypto_shash_set_flags(hash, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(u32))
 		return -EINVAL;
-	}
 	*mctx = le32_to_cpup((__le32 *)key);
 	return 0;
 }
diff --git a/arch/x86/crypto/crc32c-intel_glue.c b/arch/x86/crypto/crc32c-intel_glue.c
index 5773e1161072..0534cd55d37f 100644
--- a/arch/x86/crypto/crc32c-intel_glue.c
+++ b/arch/x86/crypto/crc32c-intel_glue.c
@@ -103,10 +103,8 @@ static int crc32c_intel_setkey(struct crypto_shash *hash, const u8 *key,
 {
 	u32 *mctx = crypto_shash_ctx(hash);
 
-	if (keylen != sizeof(u32)) {
-		crypto_shash_set_flags(hash, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(u32))
 		return -EINVAL;
-	}
 	*mctx = le32_to_cpup((__le32 *)key);
 	return 0;
 }
diff --git a/arch/x86/crypto/ghash-clmulni-intel_glue.c b/arch/x86/crypto/ghash-clmulni-intel_glue.c
index 2ddbe3a1868b..b825c6330fc6 100644
--- a/arch/x86/crypto/ghash-clmulni-intel_glue.c
+++ b/arch/x86/crypto/ghash-clmulni-intel_glue.c
@@ -59,10 +59,8 @@ static int ghash_setkey(struct crypto_shash *tfm,
 	be128 *x = (be128 *)key;
 	u64 a, b;
 
-	if (keylen != GHASH_BLOCK_SIZE) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != GHASH_BLOCK_SIZE)
 		return -EINVAL;
-	}
 
 	/* perform multiplication by 'x' in GF(2^128) */
 	a = be64_to_cpu(x->a);
diff --git a/arch/x86/crypto/twofish_avx_glue.c b/arch/x86/crypto/twofish_avx_glue.c
index 66d989230d10..4a17b10c6867 100644
--- a/arch/x86/crypto/twofish_avx_glue.c
+++ b/arch/x86/crypto/twofish_avx_glue.c
@@ -85,7 +85,6 @@ static int xts_twofish_setkey(struct crypto_skcipher *tfm, const u8 *key,
 			      unsigned int keylen)
 {
 	struct twofish_xts_ctx *ctx = crypto_skcipher_ctx(tfm);
-	u32 *flags = &tfm->base.crt_flags;
 	int err;
 
 	err = xts_verify_key(tfm, key, keylen);
@@ -93,13 +92,12 @@ static int xts_twofish_setkey(struct crypto_skcipher *tfm, const u8 *key,
 		return err;
 
 	/* first half of xts-key is for crypt */
-	err = __twofish_setkey(&ctx->crypt_ctx, key, keylen / 2, flags);
+	err = __twofish_setkey(&ctx->crypt_ctx, key, keylen / 2);
 	if (err)
 		return err;
 
 	/* second half of xts-key is for tweak */
-	return __twofish_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2,
-				flags);
+	return __twofish_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2);
 }
 
 static const struct common_glue_ctx twofish_enc = {
diff --git a/arch/x86/include/asm/crypto/camellia.h b/arch/x86/include/asm/crypto/camellia.h
index a5d86fc0593f..b35cc16f26cd 100644
--- a/arch/x86/include/asm/crypto/camellia.h
+++ b/arch/x86/include/asm/crypto/camellia.h
@@ -26,7 +26,7 @@ struct camellia_xts_ctx {
 
 extern int __camellia_setkey(struct camellia_ctx *cctx,
 			     const unsigned char *key,
-			     unsigned int key_len, u32 *flags);
+			     unsigned int key_len);
 
 extern int xts_camellia_setkey(struct crypto_skcipher *tfm, const u8 *key,
 			       unsigned int keylen);
diff --git a/crypto/aegis128.c b/crypto/aegis128.c
index 38271303ce16..398718ce6d44 100644
--- a/crypto/aegis128.c
+++ b/crypto/aegis128.c
@@ -333,10 +333,8 @@ static int crypto_aegis128_setkey(struct crypto_aead *aead, const u8 *key,
 {
 	struct aegis_ctx *ctx = crypto_aead_ctx(aead);
 
-	if (keylen != AEGIS128_KEY_SIZE) {
-		crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != AEGIS128_KEY_SIZE)
 		return -EINVAL;
-	}
 
 	memcpy(ctx->key.bytes, key, AEGIS128_KEY_SIZE);
 	return 0;
* Unmerged path crypto/aes_generic.c
diff --git a/crypto/anubis.c b/crypto/anubis.c
index 4bb187c2a902..2cde2eef8baf 100644
--- a/crypto/anubis.c
+++ b/crypto/anubis.c
@@ -464,7 +464,6 @@ static int anubis_setkey(struct crypto_tfm *tfm, const u8 *in_key,
 {
 	struct anubis_ctx *ctx = crypto_tfm_ctx(tfm);
 	const __be32 *key = (const __be32 *)in_key;
-	u32 *flags = &tfm->crt_flags;
 	int N, R, i, r;
 	u32 kappa[ANUBIS_MAX_N];
 	u32 inter[ANUBIS_MAX_N];
@@ -474,7 +473,6 @@ static int anubis_setkey(struct crypto_tfm *tfm, const u8 *in_key,
 		case 32: case 36: case 40:
 			break;
 		default:
-			*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 			return -EINVAL;
 	}
 
diff --git a/crypto/authenc.c b/crypto/authenc.c
index 3ee10fc25aff..a7422968c763 100644
--- a/crypto/authenc.c
+++ b/crypto/authenc.c
@@ -96,7 +96,7 @@ static int crypto_authenc_setkey(struct crypto_aead *authenc, const u8 *key,
 	int err = -EINVAL;
 
 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0)
-		goto badkey;
+		goto out;
 
 	crypto_ahash_clear_flags(auth, CRYPTO_TFM_REQ_MASK);
 	crypto_ahash_set_flags(auth, crypto_aead_get_flags(authenc) &
@@ -118,10 +118,6 @@ static int crypto_authenc_setkey(struct crypto_aead *authenc, const u8 *key,
 out:
 	memzero_explicit(&keys, sizeof(keys));
 	return err;
-
-badkey:
-	crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
-	goto out;
 }
 
 static void authenc_geniv_ahash_done(struct crypto_async_request *areq, int err)
diff --git a/crypto/authencesn.c b/crypto/authencesn.c
index 50b804747e20..b37a356786ca 100644
--- a/crypto/authencesn.c
+++ b/crypto/authencesn.c
@@ -70,7 +70,7 @@ static int crypto_authenc_esn_setkey(struct crypto_aead *authenc_esn, const u8 *
 	int err = -EINVAL;
 
 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0)
-		goto badkey;
+		goto out;
 
 	crypto_ahash_clear_flags(auth, CRYPTO_TFM_REQ_MASK);
 	crypto_ahash_set_flags(auth, crypto_aead_get_flags(authenc_esn) &
@@ -92,10 +92,6 @@ static int crypto_authenc_esn_setkey(struct crypto_aead *authenc_esn, const u8 *
 out:
 	memzero_explicit(&keys, sizeof(keys));
 	return err;
-
-badkey:
-	crypto_aead_set_flags(authenc_esn, CRYPTO_TFM_RES_BAD_KEY_LEN);
-	goto out;
 }
 
 static int crypto_authenc_esn_genicv_tail(struct aead_request *req,
* Unmerged path crypto/blake2b_generic.c
* Unmerged path crypto/blake2s_generic.c
diff --git a/crypto/camellia_generic.c b/crypto/camellia_generic.c
index 32ddd4836ff5..2e4f7be001f4 100644
--- a/crypto/camellia_generic.c
+++ b/crypto/camellia_generic.c
@@ -982,12 +982,9 @@ camellia_set_key(struct crypto_tfm *tfm, const u8 *in_key,
 {
 	struct camellia_ctx *cctx = crypto_tfm_ctx(tfm);
 	const unsigned char *key = (const unsigned char *)in_key;
-	u32 *flags = &tfm->crt_flags;
 
-	if (key_len != 16 && key_len != 24 && key_len != 32) {
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+	if (key_len != 16 && key_len != 24 && key_len != 32)
 		return -EINVAL;
-	}
 
 	cctx->key_length = key_len;
 
diff --git a/crypto/cast6_generic.c b/crypto/cast6_generic.c
index c8e5ec69790e..adab9479cc58 100644
--- a/crypto/cast6_generic.c
+++ b/crypto/cast6_generic.c
@@ -110,17 +110,14 @@ static inline void W(u32 *key, unsigned int i)
 	key[7] ^= F2(key[0], Tr[i % 4][7], Tm[i][7]);
 }
 
-int __cast6_setkey(struct cast6_ctx *c, const u8 *in_key,
-		   unsigned key_len, u32 *flags)
+int __cast6_setkey(struct cast6_ctx *c, const u8 *in_key, unsigned int key_len)
 {
 	int i;
 	u32 key[8];
 	__be32 p_key[8]; /* padded key */
 
-	if (key_len % 4 != 0) {
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+	if (key_len % 4 != 0)
 		return -EINVAL;
-	}
 
 	memset(p_key, 0, 32);
 	memcpy(p_key, in_key, key_len);
@@ -155,8 +152,7 @@ EXPORT_SYMBOL_GPL(__cast6_setkey);
 
 int cast6_setkey(struct crypto_tfm *tfm, const u8 *key, unsigned int keylen)
 {
-	return __cast6_setkey(crypto_tfm_ctx(tfm), key, keylen,
-			      &tfm->crt_flags);
+	return __cast6_setkey(crypto_tfm_ctx(tfm), key, keylen);
 }
 EXPORT_SYMBOL_GPL(cast6_setkey);
 
* Unmerged path crypto/cipher.c
diff --git a/crypto/crc32_generic.c b/crypto/crc32_generic.c
index 00facd27bcc2..78d05ac82f6d 100644
--- a/crypto/crc32_generic.c
+++ b/crypto/crc32_generic.c
@@ -60,10 +60,8 @@ static int crc32_setkey(struct crypto_shash *hash, const u8 *key,
 {
 	u32 *mctx = crypto_shash_ctx(hash);
 
-	if (keylen != sizeof(u32)) {
-		crypto_shash_set_flags(hash, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(u32))
 		return -EINVAL;
-	}
 	*mctx = get_unaligned_le32(key);
 	return 0;
 }
diff --git a/crypto/crc32c_generic.c b/crypto/crc32c_generic.c
index 7283066ecc98..4a0de75f554c 100644
--- a/crypto/crc32c_generic.c
+++ b/crypto/crc32c_generic.c
@@ -79,10 +79,8 @@ static int chksum_setkey(struct crypto_shash *tfm, const u8 *key,
 {
 	struct chksum_ctx *mctx = crypto_shash_ctx(tfm);
 
-	if (keylen != sizeof(mctx->key)) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(mctx->key))
 		return -EINVAL;
-	}
 	mctx->key = get_unaligned_le32(key);
 	return 0;
 }
* Unmerged path crypto/essiv.c
diff --git a/crypto/ghash-generic.c b/crypto/ghash-generic.c
index 1bffb3f712dd..f30e394b7317 100644
--- a/crypto/ghash-generic.c
+++ b/crypto/ghash-generic.c
@@ -35,10 +35,8 @@ static int ghash_setkey(struct crypto_shash *tfm,
 {
 	struct ghash_ctx *ctx = crypto_shash_ctx(tfm);
 
-	if (keylen != GHASH_BLOCK_SIZE) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != GHASH_BLOCK_SIZE)
 		return -EINVAL;
-	}
 
 	if (ctx->gf128)
 		gf128mul_free_4k(ctx->gf128);
diff --git a/crypto/michael_mic.c b/crypto/michael_mic.c
index 46195e0d0f4d..3fd3be524039 100644
--- a/crypto/michael_mic.c
+++ b/crypto/michael_mic.c
@@ -140,10 +140,8 @@ static int michael_setkey(struct crypto_shash *tfm, const u8 *key,
 
 	const __le32 *data = (const __le32 *)key;
 
-	if (keylen != 8) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != 8)
 		return -EINVAL;
-	}
 
 	mctx->l = le32_to_cpu(data[0]);
 	mctx->r = le32_to_cpu(data[1]);
diff --git a/crypto/skcipher.c b/crypto/skcipher.c
index 0f834eeafd61..9e5b12dd0cdd 100644
--- a/crypto/skcipher.c
+++ b/crypto/skcipher.c
@@ -810,10 +810,8 @@ static int skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key,
 	unsigned long alignmask = crypto_skcipher_alignmask(tfm);
 	int err;
 
-	if (keylen < cipher->min_keysize || keylen > cipher->max_keysize) {
-		crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen < cipher->min_keysize || keylen > cipher->max_keysize)
 		return -EINVAL;
-	}
 
 	if ((unsigned long)key & alignmask)
 		err = skcipher_setkey_unaligned(tfm, key, keylen);
diff --git a/crypto/sm4_generic.c b/crypto/sm4_generic.c
index c18eebfd5edd..4138370b17e3 100644
--- a/crypto/sm4_generic.c
+++ b/crypto/sm4_generic.c
@@ -143,29 +143,23 @@ int crypto_sm4_expand_key(struct crypto_sm4_ctx *ctx, const u8 *in_key,
 EXPORT_SYMBOL_GPL(crypto_sm4_expand_key);
 
 /**
- * crypto_sm4_set_key - Set the AES key.
+ * crypto_sm4_set_key - Set the SM4 key.
  * @tfm:	The %crypto_tfm that is used in the context.
  * @in_key:	The input key.
  * @key_len:	The size of the key.
  *
- * Returns 0 on success, on failure the %CRYPTO_TFM_RES_BAD_KEY_LEN flag in tfm
- * is set. The function uses crypto_sm4_expand_key() to expand the key.
+ * This function uses crypto_sm4_expand_key() to expand the key.
  * &crypto_sm4_ctx _must_ be the private data embedded in @tfm which is
  * retrieved with crypto_tfm_ctx().
+ *
+ * Return: 0 on success; -EINVAL on failure (only happens for bad key lengths)
  */
 int crypto_sm4_set_key(struct crypto_tfm *tfm, const u8 *in_key,
 		       unsigned int key_len)
 {
 	struct crypto_sm4_ctx *ctx = crypto_tfm_ctx(tfm);
-	u32 *flags = &tfm->crt_flags;
-	int ret;
-
-	ret = crypto_sm4_expand_key(ctx, in_key, key_len);
-	if (!ret)
-		return 0;
 
-	*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
-	return -EINVAL;
+	return crypto_sm4_expand_key(ctx, in_key, key_len);
 }
 EXPORT_SYMBOL_GPL(crypto_sm4_set_key);
 
diff --git a/crypto/twofish_common.c b/crypto/twofish_common.c
index f3a0dd25f871..1293d0e0f751 100644
--- a/crypto/twofish_common.c
+++ b/crypto/twofish_common.c
@@ -580,7 +580,7 @@ static const u8 calc_sb_tbl[512] = {
 
 /* Perform the key setup. */
 int __twofish_setkey(struct twofish_ctx *ctx, const u8 *key,
-		     unsigned int key_len, u32 *flags)
+		     unsigned int key_len)
 {
 	int i, j, k;
 
@@ -597,10 +597,7 @@ int __twofish_setkey(struct twofish_ctx *ctx, const u8 *key,
 
 	/* Check key length. */
 	if (key_len % 8)
-	{
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 		return -EINVAL; /* unsupported key length */
-	}
 
 	/* Compute the first two words of the S vector.  The magic numbers are
 	 * the entries of the RS matrix, preprocessed through poly_to_exp. The
@@ -701,8 +698,7 @@ EXPORT_SYMBOL_GPL(__twofish_setkey);
 
 int twofish_setkey(struct crypto_tfm *tfm, const u8 *key, unsigned int key_len)
 {
-	return __twofish_setkey(crypto_tfm_ctx(tfm), key, key_len,
-				&tfm->crt_flags);
+	return __twofish_setkey(crypto_tfm_ctx(tfm), key, key_len);
 }
 EXPORT_SYMBOL_GPL(twofish_setkey);
 
diff --git a/crypto/vmac.c b/crypto/vmac.c
index 156dd252fac3..b1f1d736953a 100644
--- a/crypto/vmac.c
+++ b/crypto/vmac.c
@@ -429,10 +429,8 @@ static int vmac_setkey(struct crypto_shash *tfm,
 	unsigned int i;
 	int err;
 
-	if (keylen != VMAC_KEY_LEN) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != VMAC_KEY_LEN)
 		return -EINVAL;
-	}
 
 	err = crypto_cipher_setkey(tctx->cipher, key, keylen);
 	if (err)
* Unmerged path crypto/xxhash_generic.c
* Unmerged path drivers/crypto/allwinner/sun8i-ce/sun8i-ce-cipher.c
* Unmerged path drivers/crypto/allwinner/sun8i-ss/sun8i-ss-cipher.c
diff --git a/drivers/crypto/amcc/crypto4xx_alg.c b/drivers/crypto/amcc/crypto4xx_alg.c
index f5c07498ea4f..22a71e3ddd75 100644
--- a/drivers/crypto/amcc/crypto4xx_alg.c
+++ b/drivers/crypto/amcc/crypto4xx_alg.c
@@ -123,12 +123,9 @@ static int crypto4xx_setkey_aes(struct crypto_skcipher *cipher,
 	struct dynamic_sa_ctl *sa;
 	int    rc;
 
-	if (keylen != AES_KEYSIZE_256 &&
-		keylen != AES_KEYSIZE_192 && keylen != AES_KEYSIZE_128) {
-		crypto_skcipher_set_flags(cipher,
-				CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != AES_KEYSIZE_256 && keylen != AES_KEYSIZE_192 &&
+	    keylen != AES_KEYSIZE_128)
 		return -EINVAL;
-	}
 
 	/* Create SA */
 	if (ctx->sa_in || ctx->sa_out)
@@ -549,10 +546,8 @@ int crypto4xx_setkey_aes_gcm(struct crypto_aead *cipher,
 	struct dynamic_sa_ctl *sa;
 	int    rc = 0;
 
-	if (crypto4xx_aes_gcm_validate_keylen(keylen) != 0) {
-		crypto_aead_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (crypto4xx_aes_gcm_validate_keylen(keylen) != 0)
 		return -EINVAL;
-	}
 
 	rc = crypto4xx_aead_setup_fallback(ctx, cipher, key, keylen);
 	if (rc)
* Unmerged path drivers/crypto/amlogic/amlogic-gxl-cipher.c
* Unmerged path drivers/crypto/atmel-aes.c
* Unmerged path drivers/crypto/axis/artpec6_crypto.c
* Unmerged path drivers/crypto/bcm/cipher.c
* Unmerged path drivers/crypto/caam/caamalg.c
* Unmerged path drivers/crypto/caam/caamalg_qi.c
* Unmerged path drivers/crypto/caam/caamalg_qi2.c
* Unmerged path drivers/crypto/caam/caamhash.c
* Unmerged path drivers/crypto/cavium/cpt/cptvf_algs.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_aead.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_skcipher.c
diff --git a/drivers/crypto/ccp/ccp-crypto-aes-cmac.c b/drivers/crypto/ccp/ccp-crypto-aes-cmac.c
index 4ff8dab8789b..46e416f8266a 100644
--- a/drivers/crypto/ccp/ccp-crypto-aes-cmac.c
+++ b/drivers/crypto/ccp/ccp-crypto-aes-cmac.c
@@ -275,7 +275,6 @@ static int ccp_aes_cmac_setkey(struct crypto_ahash *tfm, const u8 *key,
 		ctx->u.aes.type = CCP_AES_TYPE_256;
 		break;
 	default:
-		crypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 		return -EINVAL;
 	}
 	ctx->u.aes.mode = alg->mode;
diff --git a/drivers/crypto/ccp/ccp-crypto-aes-galois.c b/drivers/crypto/ccp/ccp-crypto-aes-galois.c
index 02eba84028b3..4062c800b0eb 100644
--- a/drivers/crypto/ccp/ccp-crypto-aes-galois.c
+++ b/drivers/crypto/ccp/ccp-crypto-aes-galois.c
@@ -42,7 +42,6 @@ static int ccp_aes_gcm_setkey(struct crypto_aead *tfm, const u8 *key,
 		ctx->u.aes.type = CCP_AES_TYPE_256;
 		break;
 	default:
-		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 		return -EINVAL;
 	}
 
* Unmerged path drivers/crypto/ccp/ccp-crypto-aes.c
diff --git a/drivers/crypto/ccp/ccp-crypto-sha.c b/drivers/crypto/ccp/ccp-crypto-sha.c
index 268132dc0d84..c2a143b51d41 100644
--- a/drivers/crypto/ccp/ccp-crypto-sha.c
+++ b/drivers/crypto/ccp/ccp-crypto-sha.c
@@ -295,10 +295,8 @@ static int ccp_sha_setkey(struct crypto_ahash *tfm, const u8 *key,
 
 		ret = crypto_shash_digest(sdesc, key, key_len,
 					  ctx->u.sha.key);
-		if (ret) {
-			crypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		if (ret)
 			return -EINVAL;
-		}
 
 		key_len = digest_size;
 	} else {
* Unmerged path drivers/crypto/ccree/cc_aead.c
* Unmerged path drivers/crypto/ccree/cc_cipher.c
diff --git a/drivers/crypto/ccree/cc_hash.c b/drivers/crypto/ccree/cc_hash.c
index 96ff777474d7..2ee583ad9b2a 100644
--- a/drivers/crypto/ccree/cc_hash.c
+++ b/drivers/crypto/ccree/cc_hash.c
@@ -921,9 +921,6 @@ static int cc_hash_setkey(struct crypto_ahash *ahash, const u8 *key,
 	rc = cc_send_sync_request(ctx->drvdata, &cc_req, desc, idx);
 
 out:
-	if (rc)
-		crypto_ahash_set_flags(ahash, CRYPTO_TFM_RES_BAD_KEY_LEN);
-
 	if (ctx->key_params.key_dma_addr) {
 		dma_unmap_single(dev, ctx->key_params.key_dma_addr,
 				 ctx->key_params.keylen, DMA_TO_DEVICE);
@@ -1004,9 +1001,6 @@ static int cc_xcbc_setkey(struct crypto_ahash *ahash,
 
 	rc = cc_send_sync_request(ctx->drvdata, &cc_req, desc, idx);
 
-	if (rc)
-		crypto_ahash_set_flags(ahash, CRYPTO_TFM_RES_BAD_KEY_LEN);
-
 	dma_unmap_single(dev, ctx->key_params.key_dma_addr,
 			 ctx->key_params.keylen, DMA_TO_DEVICE);
 	dev_dbg(dev, "Unmapped key-buffer: key_dma_addr=%pad keylen=%u\n",
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/geode-aes.c
* Unmerged path drivers/crypto/inside-secure/safexcel_cipher.c
* Unmerged path drivers/crypto/inside-secure/safexcel_hash.c
diff --git a/drivers/crypto/ixp4xx_crypto.c b/drivers/crypto/ixp4xx_crypto.c
index ea0ea400cf82..c1a0a4e15df5 100644
--- a/drivers/crypto/ixp4xx_crypto.c
+++ b/drivers/crypto/ixp4xx_crypto.c
@@ -737,7 +737,6 @@ static int setup_cipher(struct crypto_tfm *tfm, int encrypt,
 	u32 keylen_cfg = 0;
 	struct ix_sa_dir *dir;
 	struct ixp_ctx *ctx = crypto_tfm_ctx(tfm);
-	u32 *flags = &tfm->crt_flags;
 
 	dir = encrypt ? &ctx->encrypt : &ctx->decrypt;
 	cinfo = dir->npe_ctx;
@@ -754,7 +753,6 @@ static int setup_cipher(struct crypto_tfm *tfm, int encrypt,
 		case 24: keylen_cfg = MOD_AES192; break;
 		case 32: keylen_cfg = MOD_AES256; break;
 		default:
-			*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 			return -EINVAL;
 		}
 		cipher_cfg |= keylen_cfg;
@@ -1175,7 +1173,6 @@ static int aead_setkey(struct crypto_aead *tfm, const u8 *key,
 	memzero_explicit(&keys, sizeof(keys));
 	return aead_setup(tfm, crypto_aead_authsize(tfm));
 badkey:
-	crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 	memzero_explicit(&keys, sizeof(keys));
 	return -EINVAL;
 }
* Unmerged path drivers/crypto/marvell/cipher.c
* Unmerged path drivers/crypto/mediatek/mtk-aes.c
* Unmerged path drivers/crypto/n2_core.c
* Unmerged path drivers/crypto/padlock-aes.c
* Unmerged path drivers/crypto/picoxcell_crypto.c
diff --git a/drivers/crypto/qat/qat_common/qat_algs.c b/drivers/crypto/qat/qat_common/qat_algs.c
index 35bca76b640f..833bb1d3a11b 100644
--- a/drivers/crypto/qat/qat_common/qat_algs.c
+++ b/drivers/crypto/qat/qat_common/qat_algs.c
@@ -570,7 +570,6 @@ static int qat_alg_aead_init_sessions(struct crypto_aead *tfm, const u8 *key,
 	memzero_explicit(&keys, sizeof(keys));
 	return 0;
 bad_key:
-	crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 	memzero_explicit(&keys, sizeof(keys));
 	return -EINVAL;
 error:
@@ -586,14 +585,11 @@ static int qat_alg_skcipher_init_sessions(struct qat_alg_skcipher_ctx *ctx,
 	int alg;
 
 	if (qat_alg_validate_key(keylen, &alg, mode))
-		goto bad_key;
+		return -EINVAL;
 
 	qat_alg_skcipher_init_enc(ctx, alg, key, keylen, mode);
 	qat_alg_skcipher_init_dec(ctx, alg, key, keylen, mode);
 	return 0;
-bad_key:
-	crypto_skcipher_set_flags(ctx->tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
-	return -EINVAL;
 }
 
 static int qat_alg_aead_rekey(struct crypto_aead *tfm, const uint8_t *key,
diff --git a/drivers/crypto/qce/sha.c b/drivers/crypto/qce/sha.c
index 53227d70d397..a91ceb140ab0 100644
--- a/drivers/crypto/qce/sha.c
+++ b/drivers/crypto/qce/sha.c
@@ -405,8 +405,6 @@ static int qce_ahash_hmac_setkey(struct crypto_ahash *tfm, const u8 *key,
 	ahash_request_set_crypt(req, &sg, ctx->authkey, keylen);
 
 	ret = crypto_wait_req(crypto_ahash_digest(req), &wait);
-	if (ret)
-		crypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 
 	kfree(buf);
 err_free_req:
* Unmerged path drivers/crypto/rockchip/rk3288_crypto_skcipher.c
diff --git a/drivers/crypto/stm32/stm32_crc32.c b/drivers/crypto/stm32/stm32_crc32.c
index 8f09b8430893..ec0c211afc1c 100644
--- a/drivers/crypto/stm32/stm32_crc32.c
+++ b/drivers/crypto/stm32/stm32_crc32.c
@@ -84,10 +84,8 @@ static int stm32_crc_setkey(struct crypto_shash *tfm, const u8 *key,
 {
 	struct stm32_crc_ctx *mctx = crypto_shash_ctx(tfm);
 
-	if (keylen != sizeof(u32)) {
-		crypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen != sizeof(u32))
 		return -EINVAL;
-	}
 
 	mctx->key = get_unaligned_le32(key);
 	return 0;
* Unmerged path drivers/crypto/sunxi-ss/sun4i-ss-cipher.c
* Unmerged path drivers/crypto/talitos.c
* Unmerged path drivers/crypto/ux500/cryp/cryp_core.c
* Unmerged path drivers/crypto/virtio/virtio_crypto_algs.c
diff --git a/include/crypto/cast6.h b/include/crypto/cast6.h
index c71f6ef47f0f..651f22f004c9 100644
--- a/include/crypto/cast6.h
+++ b/include/crypto/cast6.h
@@ -15,8 +15,7 @@ struct cast6_ctx {
 	u8 Kr[12][4];
 };
 
-int __cast6_setkey(struct cast6_ctx *ctx, const u8 *key,
-		   unsigned int keylen, u32 *flags);
+int __cast6_setkey(struct cast6_ctx *ctx, const u8 *key, unsigned int keylen);
 int cast6_setkey(struct crypto_tfm *tfm, const u8 *key, unsigned int keylen);
 
 void __cast6_encrypt(struct cast6_ctx *ctx, u8 *dst, const u8 *src);
* Unmerged path include/crypto/internal/des.h
diff --git a/include/crypto/twofish.h b/include/crypto/twofish.h
index 2e2c09673d88..f6b307a58554 100644
--- a/include/crypto/twofish.h
+++ b/include/crypto/twofish.h
@@ -19,7 +19,7 @@ struct twofish_ctx {
 };
 
 int __twofish_setkey(struct twofish_ctx *ctx, const u8 *key,
-		     unsigned int key_len, u32 *flags);
+		     unsigned int key_len);
 int twofish_setkey(struct crypto_tfm *tfm, const u8 *key, unsigned int key_len);
 
 #endif
diff --git a/include/crypto/xts.h b/include/crypto/xts.h
index 34d94c95445a..56dba302b183 100644
--- a/include/crypto/xts.h
+++ b/include/crypto/xts.h
@@ -19,10 +19,8 @@ static inline int xts_check_key(struct crypto_tfm *tfm,
 	 * key consists of keys of equal size concatenated, therefore
 	 * the length must be even.
 	 */
-	if (keylen % 2) {
-		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+	if (keylen % 2)
 		return -EINVAL;
-	}
 
 	/* ensure that the AES and tweak key are not identical */
 	if (fips_enabled &&
@@ -41,10 +39,8 @@ static inline int xts_verify_key(struct crypto_skcipher *tfm,
 	 * key consists of keys of equal size concatenated, therefore
 	 * the length must be even.
 	 */
-	if (keylen % 2) {
-		crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	if (keylen % 2)
 		return -EINVAL;
-	}
 
 	/* ensure that the AES and tweak key are not identical */
 	if ((fips_enabled || crypto_skcipher_get_flags(tfm) &
* Unmerged path include/linux/crypto.h
