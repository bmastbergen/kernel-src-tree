io_uring: refactor file register/unregister/update handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>
commit 0558955373023b08f638c9ede36741b0e4200f58
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/05589553.failed

While diving into io_uring fileset register/unregister/update codes, we
found one bug in the fileset update handling. io_uring fileset update
use a percpu_ref variable to check whether we can put the previously
registered file, only when the refcnt of the perfcpu_ref variable
reaches zero, can we safely put these files. But this doesn't work so
well. If applications always issue requests continually, this
perfcpu_ref will never have an chance to reach zero, and it'll always be
in atomic mode, also will defeat the gains introduced by fileset
register/unresiger/update feature, which are used to reduce the atomic
operation overhead of fput/fget.

To fix this issue, while applications do IORING_REGISTER_FILES or
IORING_REGISTER_FILES_UPDATE operations, we allocate a new percpu_ref
and kill the old percpu_ref, new requests will use the new percpu_ref.
Once all previous old requests complete, old percpu_refs will be dropped
and registered files will be put safely.

Link: https://lore.kernel.org/io-uring/5a8dac33-4ca2-4847-b091-f7dcd3ad0ff3@linux.alibaba.com/T/#t
	Signed-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 0558955373023b08f638c9ede36741b0e4200f58)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,7b5087904640..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -197,14 -182,34 +197,41 @@@ struct io_mapped_ubuf 
  	unsigned int	nr_bvecs;
  };
  
 -struct fixed_file_table {
 -	struct file		**files;
 -};
 +struct async_list {
 +	spinlock_t		lock;
 +	atomic_t		cnt;
 +	struct list_head	list;
  
++<<<<<<< HEAD
 +	struct file		*file;
 +	off_t			io_start;
 +	size_t			io_len;
++=======
+ struct fixed_file_ref_node {
+ 	struct percpu_ref		refs;
+ 	struct list_head		node;
+ 	struct list_head		file_list;
+ 	struct fixed_file_data		*file_data;
+ 	struct work_struct		work;
+ };
+ 
+ struct fixed_file_data {
+ 	struct fixed_file_table		*table;
+ 	struct io_ring_ctx		*ctx;
+ 
+ 	struct percpu_ref		*cur_refs;
+ 	struct percpu_ref		refs;
+ 	struct completion		done;
+ 	struct list_head		ref_list;
+ 	spinlock_t			lock;
+ };
+ 
+ struct io_buffer {
+ 	struct list_head list;
+ 	__u64 addr;
+ 	__s32 len;
+ 	__u16 bid;
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  };
  
  struct io_ring_ctx {
@@@ -341,7 -623,27 +368,31 @@@ struct io_kiocb 
  	u32			result;
  	u32			sequence;
  
++<<<<<<< HEAD
 +	struct work_struct	work;
++=======
+ 	struct list_head	link_list;
+ 
+ 	struct list_head	inflight_entry;
+ 
+ 	struct percpu_ref	*fixed_file_refs;
+ 
+ 	union {
+ 		/*
+ 		 * Only commands that never go async can use the below fields,
+ 		 * obviously. Right now only IORING_OP_POLL_ADD uses them, and
+ 		 * async armed poll handlers for regular commands. The latter
+ 		 * restore the work, if needed.
+ 		 */
+ 		struct {
+ 			struct callback_head	task_work;
+ 			struct hlist_node	hash_node;
+ 			struct async_poll	*apoll;
+ 			int			cflags;
+ 		};
+ 		struct io_wq_work	work;
+ 	};
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  };
  
  #define IO_PLUG_THRESHOLD		2
@@@ -366,8 -668,202 +417,207 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ 	/* needs ->fs */
+ 	unsigned		needs_fs : 1;
+ 	/* set if opcode supports polled "wait" */
+ 	unsigned		pollin : 1;
+ 	unsigned		pollout : 1;
+ 	/* op supports buffer selection */
+ 	unsigned		buffer_select : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_SPLICE] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_PROVIDE_BUFFERS] = {},
+ 	[IORING_OP_REMOVE_BUFFERS] = {},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_cleanup_req(struct io_kiocb *req);
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 		       int fd, struct file **out_file, bool fixed);
+ static void __io_queue_sqe(struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe);
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  
  static struct kmem_cache *req_cachep;
  
@@@ -612,26 -1347,128 +862,122 @@@ out
  	return NULL;
  }
  
 -static inline void io_put_file(struct io_kiocb *req, struct file *file,
 -			  bool fixed)
 +static void io_free_req_many(struct io_ring_ctx *ctx, void **reqs, int *nr)
  {
++<<<<<<< HEAD
 +	if (*nr) {
 +		kmem_cache_free_bulk(req_cachep, *nr, reqs);
 +		percpu_ref_put_many(&ctx->refs, *nr);
 +		*nr = 0;
 +	}
++=======
+ 	if (fixed)
+ 		percpu_ref_put(req->fixed_file_refs);
+ 	else
+ 		fput(file);
+ }
+ 
+ static void __io_req_do_free(struct io_kiocb *req)
+ {
+ 	if (likely(!io_is_fallback_req(req)))
+ 		kmem_cache_free(req_cachep, req);
+ 	else
+ 		clear_bit_unlock(0, (unsigned long *) req->ctx->fallback_req);
+ }
+ 
+ static void __io_req_aux_free(struct io_kiocb *req)
+ {
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		io_cleanup_req(req);
+ 
+ 	kfree(req->io);
+ 	if (req->file)
+ 		io_put_file(req, req->file, (req->flags & REQ_F_FIXED_FILE));
+ 
+ 	io_req_work_drop_env(req);
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  }
  
  static void __io_free_req(struct io_kiocb *req)
  {
 -	__io_req_aux_free(req);
 -
 -	if (req->flags & REQ_F_INFLIGHT) {
 -		struct io_ring_ctx *ctx = req->ctx;
 -		unsigned long flags;
 -
 -		spin_lock_irqsave(&ctx->inflight_lock, flags);
 -		list_del(&req->inflight_entry);
 -		if (waitqueue_active(&ctx->inflight_wait))
 -			wake_up(&ctx->inflight_wait);
 -		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
 -	}
 -
 +	if (req->file && !(req->flags & REQ_F_FIXED_FILE))
 +		fput(req->file);
  	percpu_ref_put(&req->ctx->refs);
 -	__io_req_do_free(req);
 +	kmem_cache_free(req_cachep, req);
  }
  
 -struct req_batch {
 -	void *reqs[IO_IOPOLL_BATCH];
 -	int to_free;
 -	int need_iter;
 -};
 -
 -static void io_free_req_many(struct io_ring_ctx *ctx, struct req_batch *rb)
 +static void io_req_link_next(struct io_kiocb *req)
  {
++<<<<<<< HEAD
 +	struct io_kiocb *nxt;
++=======
+ 	if (!rb->to_free)
+ 		return;
+ 	if (rb->need_iter) {
+ 		int i, inflight = 0;
+ 		unsigned long flags;
+ 
+ 		for (i = 0; i < rb->to_free; i++) {
+ 			struct io_kiocb *req = rb->reqs[i];
+ 
+ 			if (req->flags & REQ_F_FIXED_FILE) {
+ 				req->file = NULL;
+ 				percpu_ref_put(req->fixed_file_refs);
+ 			}
+ 			if (req->flags & REQ_F_INFLIGHT)
+ 				inflight++;
+ 			__io_req_aux_free(req);
+ 		}
+ 		if (!inflight)
+ 			goto do_free;
+ 
+ 		spin_lock_irqsave(&ctx->inflight_lock, flags);
+ 		for (i = 0; i < rb->to_free; i++) {
+ 			struct io_kiocb *req = rb->reqs[i];
+ 
+ 			if (req->flags & REQ_F_INFLIGHT) {
+ 				list_del(&req->inflight_entry);
+ 				if (!--inflight)
+ 					break;
+ 			}
+ 		}
+ 		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
+ 
+ 		if (waitqueue_active(&ctx->inflight_wait))
+ 			wake_up(&ctx->inflight_wait);
+ 	}
+ do_free:
+ 	kmem_cache_free_bulk(req_cachep, rb->to_free, rb->reqs);
+ 	percpu_ref_put_many(&ctx->refs, rb->to_free);
+ 	rb->to_free = rb->need_iter = 0;
+ }
+ 
+ static bool io_link_cancel_timeout(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
+ 	if (ret != -1) {
+ 		io_cqring_fill_event(req, -ECANCELED);
+ 		io_commit_cqring(ctx);
+ 		req->flags &= ~REQ_F_LINK;
+ 		io_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_req_link_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	bool wake_ev = false;
+ 
+ 	/* Already got next link */
+ 	if (req->flags & REQ_F_LINK_NEXT)
+ 		return;
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  
  	/*
  	 * The list should never be empty when we are called here. But could
@@@ -1938,176 -5269,133 +2284,194 @@@ static int __io_submit_sqe(struct io_ri
  	return 0;
  }
  
 -static void io_wq_submit_work(struct io_wq_work **workptr)
 +static struct async_list *io_async_list_from_sqe(struct io_ring_ctx *ctx,
 +						 const struct io_uring_sqe *sqe)
 +{
 +	switch (sqe->opcode) {
 +	case IORING_OP_READV:
 +	case IORING_OP_READ_FIXED:
 +		return &ctx->pending_async[READ];
 +	case IORING_OP_WRITEV:
 +	case IORING_OP_WRITE_FIXED:
 +		return &ctx->pending_async[WRITE];
 +	default:
 +		return NULL;
 +	}
 +}
 +
 +static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 +{
 +	u8 opcode = READ_ONCE(sqe->opcode);
 +
 +	return !(opcode == IORING_OP_READ_FIXED ||
 +		 opcode == IORING_OP_WRITE_FIXED);
 +}
 +
 +static void io_sq_wq_submit_work(struct work_struct *work)
  {
 -	struct io_wq_work *work = *workptr;
  	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 -	int ret = 0;
 +	struct io_ring_ctx *ctx = req->ctx;
 +	struct mm_struct *cur_mm = NULL;
 +	struct async_list *async_list;
 +	LIST_HEAD(req_list);
 +	mm_segment_t old_fs;
 +	int ret;
  
 -	/* if NO_CANCEL is set, we must still run the work */
 -	if ((work->flags & (IO_WQ_WORK_CANCEL|IO_WQ_WORK_NO_CANCEL)) ==
 -				IO_WQ_WORK_CANCEL) {
 -		ret = -ECANCELED;
 -	}
++<<<<<<< HEAD
 +	async_list = io_async_list_from_sqe(ctx, req->submit.sqe);
 +restart:
 +	do {
 +		struct sqe_submit *s = &req->submit;
 +		const struct io_uring_sqe *sqe = s->sqe;
 +		unsigned int flags = req->flags;
  
 -	if (!ret) {
 -		do {
 -			ret = io_issue_sqe(req, NULL, false);
 -			/*
 -			 * We can get EAGAIN for polled IO even though we're
 -			 * forcing a sync submission from here, since we can't
 -			 * wait for request slots on the block side.
 -			 */
 -			if (ret != -EAGAIN)
 -				break;
 -			cond_resched();
 -		} while (1);
 -	}
 +		/* Ensure we clear previously set non-block flag */
 +		req->rw.ki_flags &= ~IOCB_NOWAIT;
  
 -	if (ret) {
 -		req_set_fail_links(req);
 -		io_cqring_add_event(req, ret);
 +		ret = 0;
 +		if (io_sqe_needs_user(sqe) && !cur_mm) {
 +			if (!mmget_not_zero(ctx->sqo_mm)) {
 +				ret = -EFAULT;
 +			} else {
 +				cur_mm = ctx->sqo_mm;
 +				use_mm(cur_mm);
 +				old_fs = get_fs();
 +				set_fs(USER_DS);
 +			}
 +		}
 +
 +		if (!ret) {
 +			s->has_user = cur_mm != NULL;
 +			s->needs_lock = true;
 +			do {
 +				ret = __io_submit_sqe(ctx, req, s, false);
 +				/*
 +				 * We can get EAGAIN for polled IO even though
 +				 * we're forcing a sync submission from here,
 +				 * since we can't wait for request slots on the
 +				 * block side.
 +				 */
 +				if (ret != -EAGAIN)
 +					break;
 +				cond_resched();
 +			} while (1);
 +		}
 +
 +		/* drop submission reference */
  		io_put_req(req);
 -	}
  
 -	io_steal_work(req, workptr);
 -}
 +		if (ret) {
 +			io_cqring_add_event(ctx, sqe->user_data, ret);
 +			io_put_req(req);
 +		}
  
 -static int io_req_needs_file(struct io_kiocb *req, int fd)
 -{
 -	if (!io_op_defs[req->opcode].needs_file)
 -		return 0;
 -	if ((fd == -1 || fd == AT_FDCWD) && io_op_defs[req->opcode].fd_non_neg)
 -		return 0;
 -	return 1;
 -}
 +		/* async context always use a copy of the sqe */
 +		kfree(sqe);
  
 -static inline struct file *io_file_from_index(struct io_ring_ctx *ctx,
 -					      int index)
 -{
 -	struct fixed_file_table *table;
 +		/* req from defer and link list needn't decrease async cnt */
 +		if (flags & (REQ_F_IO_DRAINED | REQ_F_LINK_DONE))
 +			goto out;
  
 -	table = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];
 -	return table->files[index & IORING_FILE_TABLE_MASK];;
 -}
 +		if (!async_list)
 +			break;
 +		if (!list_empty(&req_list)) {
 +			req = list_first_entry(&req_list, struct io_kiocb,
 +						list);
 +			list_del(&req->list);
 +			continue;
 +		}
 +		if (list_empty(&async_list->list))
 +			break;
  
 -static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
 -			int fd, struct file **out_file, bool fixed)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct file *file;
 +		req = NULL;
 +		spin_lock(&async_list->lock);
 +		if (list_empty(&async_list->list)) {
 +			spin_unlock(&async_list->lock);
 +			break;
 +		}
 +		list_splice_init(&async_list->list, &req_list);
 +		spin_unlock(&async_list->lock);
 +
 +		req = list_first_entry(&req_list, struct io_kiocb, list);
 +		list_del(&req->list);
 +	} while (req);
  
 +	/*
 +	 * Rare case of racing with a submitter. If we find the count has
 +	 * dropped to zero AND we have pending work items, then restart
 +	 * the processing. This is a tiny race window.
 +	 */
 +	if (async_list) {
 +		ret = atomic_dec_return(&async_list->cnt);
 +		while (!ret && !list_empty(&async_list->list)) {
 +			spin_lock(&async_list->lock);
 +			atomic_inc(&async_list->cnt);
 +			list_splice_init(&async_list->list, &req_list);
 +			spin_unlock(&async_list->lock);
 +
 +			if (!list_empty(&req_list)) {
 +				req = list_first_entry(&req_list,
 +							struct io_kiocb, list);
 +				list_del(&req->list);
 +				goto restart;
 +			}
 +			ret = atomic_dec_return(&async_list->cnt);
 +		}
++=======
+ 	if (fixed) {
+ 		if (unlikely(!ctx->file_data ||
+ 		    (unsigned) fd >= ctx->nr_user_files))
+ 			return -EBADF;
+ 		fd = array_index_nospec(fd, ctx->nr_user_files);
+ 		file = io_file_from_index(ctx, fd);
+ 		if (!file)
+ 			return -EBADF;
+ 		req->fixed_file_refs = ctx->file_data->cur_refs;
+ 		percpu_ref_get(req->fixed_file_refs);
+ 	} else {
+ 		trace_io_uring_file_get(ctx, fd);
+ 		file = __io_file_get(state, fd);
+ 		if (unlikely(!file))
+ 			return -EBADF;
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  	}
  
 -	*out_file = file;
 -	return 0;
 -}
 -
 -static int io_req_set_file(struct io_submit_state *state, struct io_kiocb *req,
 -			   const struct io_uring_sqe *sqe)
 -{
 -	unsigned flags;
 -	int fd;
 -	bool fixed;
 -
 -	flags = READ_ONCE(sqe->flags);
 -	fd = READ_ONCE(sqe->fd);
 -
 -	if (!io_req_needs_file(req, fd))
 -		return 0;
 -
 -	fixed = (flags & IOSQE_FIXED_FILE);
 -	if (unlikely(!fixed && req->needs_fixed_file))
 -		return -EBADF;
 -
 -	return io_file_get(state, req, fd, &req->file, fixed);
 +out:
 +	if (cur_mm) {
 +		set_fs(old_fs);
 +		unuse_mm(cur_mm);
 +		mmput(cur_mm);
 +	}
  }
  
 -static int io_grab_files(struct io_kiocb *req)
 +/*
 + * See if we can piggy back onto previously submitted work, that is still
 + * running. We currently only allow this if the new request is sequential
 + * to the previous one we punted.
 + */
 +static bool io_add_to_prev_work(struct async_list *list, struct io_kiocb *req)
  {
 -	int ret = -EBADF;
 -	struct io_ring_ctx *ctx = req->ctx;
 +	bool ret;
  
 -	if (req->work.files)
 -		return 0;
 -	if (!ctx->ring_file)
 -		return -EBADF;
 +	if (!list)
 +		return false;
 +	if (!(req->flags & REQ_F_SEQ_PREV))
 +		return false;
 +	if (!atomic_read(&list->cnt))
 +		return false;
  
 -	rcu_read_lock();
 -	spin_lock_irq(&ctx->inflight_lock);
 +	ret = true;
 +	spin_lock(&list->lock);
 +	list_add_tail(&req->list, &list->list);
  	/*
 -	 * We use the f_ops->flush() handler to ensure that we can flush
 -	 * out work accessing these files if the fd is closed. Check if
 -	 * the fd has changed since we started down this path, and disallow
 -	 * this operation if it has.
 +	 * Ensure we see a simultaneous modification from io_sq_wq_submit_work()
  	 */
 -	if (fcheck(ctx->ring_fd) == ctx->ring_file) {
 -		list_add(&req->inflight_entry, &ctx->inflight_list);
 -		req->flags |= REQ_F_INFLIGHT;
 -		req->work.files = current->files;
 -		ret = 0;
 +	smp_mb();
 +	if (!atomic_read(&list->cnt)) {
 +		list_del_init(&req->list);
 +		ret = false;
  	}
 -	spin_unlock_irq(&ctx->inflight_lock);
 -	rcu_read_unlock();
 -
 +	spin_unlock(&list->lock);
  	return ret;
  }
  
@@@ -2740,14 -6122,45 +3104,56 @@@ static void __io_sqe_files_unregister(s
  #endif
  }
  
++<<<<<<< HEAD
 +static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
 +{
 +	if (!ctx->user_files)
 +		return -ENXIO;
 +
 +	__io_sqe_files_unregister(ctx);
 +	kfree(ctx->user_files);
 +	ctx->user_files = NULL;
++=======
+ static void io_file_ref_kill(struct percpu_ref *ref)
+ {
+ 	struct fixed_file_data *data;
+ 
+ 	data = container_of(ref, struct fixed_file_data, refs);
+ 	complete(&data->done);
+ }
+ 
+ static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
+ {
+ 	struct fixed_file_data *data = ctx->file_data;
+ 	struct fixed_file_ref_node *ref_node = NULL;
+ 	unsigned nr_tables, i;
+ 	unsigned long flags;
+ 
+ 	if (!data)
+ 		return -ENXIO;
+ 
+ 	spin_lock_irqsave(&data->lock, flags);
+ 	if (!list_empty(&data->ref_list))
+ 		ref_node = list_first_entry(&data->ref_list,
+ 				struct fixed_file_ref_node, node);
+ 	spin_unlock_irqrestore(&data->lock, flags);
+ 	if (ref_node)
+ 		percpu_ref_kill(&ref_node->refs);
+ 
+ 	percpu_ref_kill(&data->refs);
+ 
+ 	/* wait for all refs nodes to complete */
+ 	wait_for_completion(&data->done);
+ 
+ 	__io_sqe_files_unregister(ctx);
+ 	nr_tables = DIV_ROUND_UP(ctx->nr_user_files, IORING_MAX_FILES_TABLE);
+ 	for (i = 0; i < nr_tables; i++)
+ 		kfree(data->table[i].files);
+ 	kfree(data->table);
+ 	percpu_ref_exit(&data->refs);
+ 	kfree(data);
+ 	ctx->file_data = NULL;
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  	ctx->nr_user_files = 0;
  	return 0;
  }
@@@ -3021,6 -6383,198 +3427,201 @@@ static void io_sqe_file_unregister(stru
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ struct io_file_put {
+ 	struct list_head list;
+ 	struct file *file;
+ };
+ 
+ static void io_file_put_work(struct work_struct *work)
+ {
+ 	struct fixed_file_ref_node *ref_node;
+ 	struct fixed_file_data *file_data;
+ 	struct io_ring_ctx *ctx;
+ 	struct io_file_put *pfile, *tmp;
+ 	unsigned long flags;
+ 
+ 	ref_node = container_of(work, struct fixed_file_ref_node, work);
+ 	file_data = ref_node->file_data;
+ 	ctx = file_data->ctx;
+ 
+ 	list_for_each_entry_safe(pfile, tmp, &ref_node->file_list, list) {
+ 		list_del_init(&pfile->list);
+ 		io_ring_file_put(ctx, pfile->file);
+ 		kfree(pfile);
+ 	}
+ 
+ 	spin_lock_irqsave(&file_data->lock, flags);
+ 	list_del_init(&ref_node->node);
+ 	spin_unlock_irqrestore(&file_data->lock, flags);
+ 
+ 	percpu_ref_exit(&ref_node->refs);
+ 	kfree(ref_node);
+ 	percpu_ref_put(&file_data->refs);
+ }
+ 
+ static void io_file_data_ref_zero(struct percpu_ref *ref)
+ {
+ 	struct fixed_file_ref_node *ref_node;
+ 
+ 	ref_node = container_of(ref, struct fixed_file_ref_node, refs);
+ 
+ 	queue_work(system_wq, &ref_node->work);
+ }
+ 
+ static struct fixed_file_ref_node *alloc_fixed_file_ref_node(
+ 			struct io_ring_ctx *ctx)
+ {
+ 	struct fixed_file_ref_node *ref_node;
+ 
+ 	ref_node = kzalloc(sizeof(*ref_node), GFP_KERNEL);
+ 	if (!ref_node)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	if (percpu_ref_init(&ref_node->refs, io_file_data_ref_zero,
+ 			    0, GFP_KERNEL)) {
+ 		kfree(ref_node);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 	INIT_LIST_HEAD(&ref_node->node);
+ 	INIT_LIST_HEAD(&ref_node->file_list);
+ 	INIT_WORK(&ref_node->work, io_file_put_work);
+ 	ref_node->file_data = ctx->file_data;
+ 	return ref_node;
+ 
+ }
+ 
+ static void destroy_fixed_file_ref_node(struct fixed_file_ref_node *ref_node)
+ {
+ 	percpu_ref_exit(&ref_node->refs);
+ 	kfree(ref_node);
+ }
+ 
+ static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
+ 				 unsigned nr_args)
+ {
+ 	__s32 __user *fds = (__s32 __user *) arg;
+ 	unsigned nr_tables;
+ 	struct file *file;
+ 	int fd, ret = 0;
+ 	unsigned i;
+ 	struct fixed_file_ref_node *ref_node;
+ 	unsigned long flags;
+ 
+ 	if (ctx->file_data)
+ 		return -EBUSY;
+ 	if (!nr_args)
+ 		return -EINVAL;
+ 	if (nr_args > IORING_MAX_FIXED_FILES)
+ 		return -EMFILE;
+ 
+ 	ctx->file_data = kzalloc(sizeof(*ctx->file_data), GFP_KERNEL);
+ 	if (!ctx->file_data)
+ 		return -ENOMEM;
+ 	ctx->file_data->ctx = ctx;
+ 	init_completion(&ctx->file_data->done);
+ 	INIT_LIST_HEAD(&ctx->file_data->ref_list);
+ 
+ 	nr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);
+ 	ctx->file_data->table = kcalloc(nr_tables,
+ 					sizeof(struct fixed_file_table),
+ 					GFP_KERNEL);
+ 	if (!ctx->file_data->table) {
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	if (percpu_ref_init(&ctx->file_data->refs, io_file_ref_kill,
+ 				PERCPU_REF_ALLOW_REINIT, GFP_KERNEL)) {
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	if (io_sqe_alloc_file_tables(ctx, nr_tables, nr_args)) {
+ 		percpu_ref_exit(&ctx->file_data->refs);
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for (i = 0; i < nr_args; i++, ctx->nr_user_files++) {
+ 		struct fixed_file_table *table;
+ 		unsigned index;
+ 
+ 		ret = -EFAULT;
+ 		if (copy_from_user(&fd, &fds[i], sizeof(fd)))
+ 			break;
+ 		/* allow sparse sets */
+ 		if (fd == -1) {
+ 			ret = 0;
+ 			continue;
+ 		}
+ 
+ 		table = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];
+ 		index = i & IORING_FILE_TABLE_MASK;
+ 		file = fget(fd);
+ 
+ 		ret = -EBADF;
+ 		if (!file)
+ 			break;
+ 
+ 		/*
+ 		 * Don't allow io_uring instances to be registered. If UNIX
+ 		 * isn't enabled, then this causes a reference cycle and this
+ 		 * instance can never get freed. If UNIX is enabled we'll
+ 		 * handle it just fine, but there's still no point in allowing
+ 		 * a ring fd as it doesn't support regular read/write anyway.
+ 		 */
+ 		if (file->f_op == &io_uring_fops) {
+ 			fput(file);
+ 			break;
+ 		}
+ 		ret = 0;
+ 		table->files[index] = file;
+ 	}
+ 
+ 	if (ret) {
+ 		for (i = 0; i < ctx->nr_user_files; i++) {
+ 			file = io_file_from_index(ctx, i);
+ 			if (file)
+ 				fput(file);
+ 		}
+ 		for (i = 0; i < nr_tables; i++)
+ 			kfree(ctx->file_data->table[i].files);
+ 
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		ctx->nr_user_files = 0;
+ 		return ret;
+ 	}
+ 
+ 	ret = io_sqe_files_scm(ctx);
+ 	if (ret) {
+ 		io_sqe_files_unregister(ctx);
+ 		return ret;
+ 	}
+ 
+ 	ref_node = alloc_fixed_file_ref_node(ctx);
+ 	if (IS_ERR(ref_node)) {
+ 		io_sqe_files_unregister(ctx);
+ 		return PTR_ERR(ref_node);
+ 	}
+ 
+ 	ctx->file_data->cur_refs = &ref_node->refs;
+ 	spin_lock_irqsave(&ctx->file_data->lock, flags);
+ 	list_add(&ref_node->node, &ctx->file_data->ref_list);
+ 	spin_unlock_irqrestore(&ctx->file_data->lock, flags);
+ 	percpu_ref_get(&ctx->file_data->refs);
+ 	return ret;
+ }
+ 
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  static int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,
  				int index)
  {
@@@ -3064,43 -6618,69 +3665,90 @@@
  #endif
  }
  
++<<<<<<< HEAD
 +static int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,
 +			       unsigned nr_args)
 +{
 +	struct io_uring_files_update up;
++=======
+ static int io_queue_file_removal(struct fixed_file_data *data,
+ 				 struct file *file)
+ {
+ 	struct io_file_put *pfile;
+ 	struct percpu_ref *refs = data->cur_refs;
+ 	struct fixed_file_ref_node *ref_node;
+ 
+ 	pfile = kzalloc(sizeof(*pfile), GFP_KERNEL);
+ 	if (!pfile)
+ 		return -ENOMEM;
+ 
+ 	ref_node = container_of(refs, struct fixed_file_ref_node, refs);
+ 	pfile->file = file;
+ 	list_add(&pfile->list, &ref_node->file_list);
+ 
+ 	return 0;
+ }
+ 
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *up,
+ 				 unsigned nr_args)
+ {
+ 	struct fixed_file_data *data = ctx->file_data;
+ 	struct fixed_file_ref_node *ref_node;
+ 	struct file *file;
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  	__s32 __user *fds;
  	int fd, i, err;
  	__u32 done;
+ 	unsigned long flags;
+ 	bool needs_switch = false;
  
 -	if (check_add_overflow(up->offset, nr_args, &done))
 +	if (!ctx->user_files)
 +		return -ENXIO;
 +	if (!nr_args)
 +		return -EINVAL;
 +	if (copy_from_user(&up, arg, sizeof(up)))
 +		return -EFAULT;
 +	if (up.resv)
 +		return -EINVAL;
 +	if (check_add_overflow(up.offset, nr_args, &done))
  		return -EOVERFLOW;
  	if (done > ctx->nr_user_files)
  		return -EINVAL;
  
+ 	ref_node = alloc_fixed_file_ref_node(ctx);
+ 	if (IS_ERR(ref_node))
+ 		return PTR_ERR(ref_node);
+ 
  	done = 0;
 -	fds = u64_to_user_ptr(up->fds);
 +	fds = u64_to_user_ptr(up.fds);
  	while (nr_args) {
 -		struct fixed_file_table *table;
 -		unsigned index;
 -
  		err = 0;
  		if (copy_from_user(&fd, &fds[done], sizeof(fd))) {
  			err = -EFAULT;
  			break;
  		}
++<<<<<<< HEAD
 +		i = array_index_nospec(up.offset, ctx->nr_user_files);
 +		if (ctx->user_files[i]) {
 +			io_sqe_file_unregister(ctx, i);
 +			ctx->user_files[i] = NULL;
++=======
+ 		i = array_index_nospec(up->offset, ctx->nr_user_files);
+ 		table = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];
+ 		index = i & IORING_FILE_TABLE_MASK;
+ 		if (table->files[index]) {
+ 			file = io_file_from_index(ctx, index);
+ 			err = io_queue_file_removal(data, file);
+ 			if (err)
+ 				break;
+ 			table->files[index] = NULL;
+ 			needs_switch = true;
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  		}
  		if (fd != -1) {
 +			struct file *file;
 +
  			file = fget(fd);
  			if (!file) {
  				err = -EBADF;
@@@ -3126,11 -6706,92 +3774,97 @@@
  		}
  		nr_args--;
  		done++;
 -		up->offset++;
 +		up.offset++;
  	}
  
++<<<<<<< HEAD
 +	return done ? done : err;
 +}
++=======
+ 	if (needs_switch) {
+ 		percpu_ref_kill(data->cur_refs);
+ 		spin_lock_irqsave(&data->lock, flags);
+ 		list_add(&ref_node->node, &data->ref_list);
+ 		data->cur_refs = &ref_node->refs;
+ 		spin_unlock_irqrestore(&data->lock, flags);
+ 		percpu_ref_get(&ctx->file_data->refs);
+ 	} else
+ 		destroy_fixed_file_ref_node(ref_node);
+ 
+ 	return done ? done : err;
+ }
+ 
+ static int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,
+ 			       unsigned nr_args)
+ {
+ 	struct io_uring_files_update up;
+ 
+ 	if (!ctx->file_data)
+ 		return -ENXIO;
+ 	if (!nr_args)
+ 		return -EINVAL;
+ 	if (copy_from_user(&up, arg, sizeof(up)))
+ 		return -EFAULT;
+ 	if (up.resv)
+ 		return -EINVAL;
+ 
+ 	return __io_sqe_files_update(ctx, &up, nr_args);
+ }
+ 
+ static void io_free_work(struct io_wq_work *work)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	/* Consider that io_steal_work() relies on this ref */
+ 	io_put_req(req);
+ }
+ 
+ static int io_init_wq_offload(struct io_ring_ctx *ctx,
+ 			      struct io_uring_params *p)
+ {
+ 	struct io_wq_data data;
+ 	struct fd f;
+ 	struct io_ring_ctx *ctx_attach;
+ 	unsigned int concurrency;
+ 	int ret = 0;
+ 
+ 	data.user = ctx->user;
+ 	data.free_work = io_free_work;
+ 
+ 	if (!(p->flags & IORING_SETUP_ATTACH_WQ)) {
+ 		/* Do QD, or 4 * CPUS, whatever is smallest */
+ 		concurrency = min(ctx->sq_entries, 4 * num_online_cpus());
+ 
+ 		ctx->io_wq = io_wq_create(concurrency, &data);
+ 		if (IS_ERR(ctx->io_wq)) {
+ 			ret = PTR_ERR(ctx->io_wq);
+ 			ctx->io_wq = NULL;
+ 		}
+ 		return ret;
+ 	}
+ 
+ 	f = fdget(p->wq_fd);
+ 	if (!f.file)
+ 		return -EBADF;
+ 
+ 	if (f.file->f_op != &io_uring_fops) {
+ 		ret = -EINVAL;
+ 		goto out_fput;
+ 	}
+ 
+ 	ctx_attach = f.file->private_data;
+ 	/* @io_wq is protected by holding the fd */
+ 	if (!io_wq_get(ctx_attach->io_wq, &data)) {
+ 		ret = -EINVAL;
+ 		goto out_fput;
+ 	}
+ 
+ 	ctx->io_wq = ctx_attach->io_wq;
+ out_fput:
+ 	fdput(f);
+ 	return ret;
+ }
++>>>>>>> 055895537302 (io_uring: refactor file register/unregister/update handling)
  
  static int io_sq_offload_start(struct io_ring_ctx *ctx,
  			       struct io_uring_params *p)
* Unmerged path fs/io_uring.c
