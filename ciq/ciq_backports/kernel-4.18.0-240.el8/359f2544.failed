hugetlbfs: move hugepagesz= parsing to arch independent code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mike Kravetz <mike.kravetz@oracle.com>
commit 359f25443a8dada0fb709dd044a422017031790f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/359f2544.failed

Now that architectures provide arch_hugetlb_valid_size(), parsing of
"hugepagesz=" can be done in architecture independent code.  Create a
single routine to handle hugepagesz= parsing and remove all arch specific
routines.  We can also remove the interface hugetlb_bad_size() as this is
no longer used outside arch independent code.

This also provides consistent behavior of hugetlbfs command line options.
The hugepagesz= option should only be specified once for a specific size,
but some architectures allow multiple instances.  This appears to be more
of an oversight when code was added by some architectures to set up ALL
huge pages sizes.

	Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Tested-by: Sandipan Das <sandipan@linux.ibm.com>
	Reviewed-by: Peter Xu <peterx@redhat.com>
	Acked-by: Mina Almasry <almasrymina@google.com>
	Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>	[s390]
	Acked-by: Will Deacon <will@kernel.org>
	Cc: Albert Ou <aou@eecs.berkeley.edu>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: Christophe Leroy <christophe.leroy@c-s.fr>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: David S. Miller <davem@davemloft.net>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Longpeng <longpeng2@huawei.com>
	Cc: Nitesh Narayan Lal <nitesh@redhat.com>
	Cc: Palmer Dabbelt <palmer@dabbelt.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Paul Walmsley <paul.walmsley@sifive.com>
	Cc: Randy Dunlap <rdunlap@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vasily Gorbik <gor@linux.ibm.com>
	Cc: Anders Roxell <anders.roxell@linaro.org>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
	Cc: Qian Cai <cai@lca.pw>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
Link: http://lkml.kernel.org/r/20200417185049.275845-3-mike.kravetz@oracle.com
Link: http://lkml.kernel.org/r/20200428205614.246260-3-mike.kravetz@oracle.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 359f25443a8dada0fb709dd044a422017031790f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/mm/hugetlbpage.c
#	arch/powerpc/mm/hugetlbpage.c
#	arch/riscv/mm/hugetlbpage.c
#	arch/s390/mm/hugetlbpage.c
#	arch/sparc/mm/init_64.c
#	arch/x86/mm/hugetlbpage.c
#	mm/hugetlb.c
diff --cc arch/arm64/mm/hugetlbpage.c
index 2e6c432a5f0c,d6cb9fe71b44..000000000000
--- a/arch/arm64/mm/hugetlbpage.c
+++ b/arch/arm64/mm/hugetlbpage.c
@@@ -461,12 -473,8 +461,20 @@@ static __init int setup_hugepagesz(cha
  	case CONT_PMD_SIZE:
  	case PMD_SIZE:
  	case CONT_PTE_SIZE:
++<<<<<<< HEAD
 +		add_huge_page_size(ps);
 +		return 1;
 +	}
 +
 +	hugetlb_bad_size();
 +	pr_err("hugepagesz: Unsupported page size %lu K\n", ps >> 10);
 +	return 0;
 +}
 +__setup("hugepagesz=", setup_hugepagesz);
++=======
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
++>>>>>>> 359f25443a8d (hugetlbfs: move hugepagesz= parsing to arch independent code)
diff --cc arch/powerpc/mm/hugetlbpage.c
index 8110b9827c69,2c3fa0a7787b..000000000000
--- a/arch/powerpc/mm/hugetlbpage.c
+++ b/arch/powerpc/mm/hugetlbpage.c
@@@ -635,33 -574,24 +635,36 @@@ static int __init add_huge_page_size(un
  
  	BUG_ON(mmu_psize_defs[mmu_psize].shift != shift);
  
 -	return true;
 +	/* Return if huge page size has already been setup */
 +	if (size_to_hstate(size))
 +		return 0;
 +
 +	hugetlb_add_hstate(shift - PAGE_SHIFT);
 +
 +	return 0;
  }
  
 -static int __init add_huge_page_size(unsigned long long size)
++<<<<<<< HEAD
 +static int __init hugepage_setup_sz(char *str)
  {
 -	int shift = __ffs(size);
 +	unsigned long long size;
  
 -	if (!arch_hugetlb_valid_size((unsigned long)size))
 -		return -EINVAL;
 +	size = memparse(str, &str);
  
 -	if (!size_to_hstate(size))
 -		hugetlb_add_hstate(shift - PAGE_SHIFT);
 -	return 0;
 +	if (add_huge_page_size(size) != 0) {
 +		hugetlb_bad_size();
 +		pr_err("Invalid huge page size specified(%llu)\n", size);
 +	}
 +
 +	return 1;
  }
 +__setup("hugepagesz=", hugepage_setup_sz);
  
 +struct kmem_cache *hugepte_cache;
++=======
++>>>>>>> 359f25443a8d (hugetlbfs: move hugepagesz= parsing to arch independent code)
  static int __init hugetlbpage_init(void)
  {
 -	bool configured = false;
  	int psize;
  
  	if (hugetlb_disabled) {
diff --cc arch/s390/mm/hugetlbpage.c
index 5674710a4841,82df06d720e8..000000000000
--- a/arch/s390/mm/hugetlbpage.c
+++ b/arch/s390/mm/hugetlbpage.c
@@@ -251,25 -254,15 +251,37 @@@ follow_huge_pud(struct mm_struct *mm, u
  	return pud_page(*pud) + ((address & ~PUD_MASK) >> PAGE_SHIFT);
  }
  
++<<<<<<< HEAD
 +static __init int setup_hugepagesz(char *opt)
 +{
 +	unsigned long size;
 +	char *string = opt;
 +
 +	size = memparse(opt, &opt);
 +	if (MACHINE_HAS_EDAT1 && size == PMD_SIZE) {
 +		hugetlb_add_hstate(PMD_SHIFT - PAGE_SHIFT);
 +	} else if (MACHINE_HAS_EDAT2 && size == PUD_SIZE) {
 +		hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
 +	} else {
 +		hugetlb_bad_size();
 +		pr_err("hugepagesz= specifies an unsupported page size %s\n",
 +			string);
 +		return 0;
 +	}
 +	return 1;
 +}
 +__setup("hugepagesz=", setup_hugepagesz);
++=======
+ bool __init arch_hugetlb_valid_size(unsigned long size)
+ {
+ 	if (MACHINE_HAS_EDAT1 && size == PMD_SIZE)
+ 		return true;
+ 	else if (MACHINE_HAS_EDAT2 && size == PUD_SIZE)
+ 		return true;
+ 	else
+ 		return false;
+ }
++>>>>>>> 359f25443a8d (hugetlbfs: move hugepagesz= parsing to arch independent code)
  
  static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,
  		unsigned long addr, unsigned long len,
diff --cc arch/sparc/mm/init_64.c
index 578ec3da410a,1b1f1ac1869e..000000000000
--- a/arch/sparc/mm/init_64.c
+++ b/arch/sparc/mm/init_64.c
@@@ -398,20 -392,11 +398,28 @@@ static int __init setup_hugepagesz(cha
  		hv_pgsz_mask = 0;
  	}
  
++<<<<<<< HEAD
 +	if ((hv_pgsz_mask & cpu_pgsz_mask) == 0U) {
 +		hugetlb_bad_size();
 +		pr_err("hugepagesz=%llu not supported by MMU.\n",
 +			hugepage_size);
 +		goto out;
 +	}
 +
 +	add_huge_page_size(hugepage_size);
 +	rc = 1;
 +
 +out:
 +	return rc;
 +}
 +__setup("hugepagesz=", setup_hugepagesz);
++=======
+ 	if ((hv_pgsz_mask & cpu_pgsz_mask) == 0U)
+ 		return false;
+ 
+ 	return true;
+ }
++>>>>>>> 359f25443a8d (hugetlbfs: move hugepagesz= parsing to arch independent code)
  #endif	/* CONFIG_HUGETLB_PAGE */
  
  void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep)
diff --cc arch/x86/mm/hugetlbpage.c
index 00b296617ca4,937d640a89e3..000000000000
--- a/arch/x86/mm/hugetlbpage.c
+++ b/arch/x86/mm/hugetlbpage.c
@@@ -186,24 -181,17 +186,38 @@@ get_unmapped_area
  #endif /* CONFIG_HUGETLB_PAGE */
  
  #ifdef CONFIG_X86_64
++<<<<<<< HEAD
 +static __init int setup_hugepagesz(char *opt)
 +{
 +	unsigned long ps = memparse(opt, &opt);
 +	if (ps == PMD_SIZE) {
 +		hugetlb_add_hstate(PMD_SHIFT - PAGE_SHIFT);
 +	} else if (ps == PUD_SIZE && boot_cpu_has(X86_FEATURE_GBPAGES)) {
 +		hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
 +	} else {
 +		hugetlb_bad_size();
 +		printk(KERN_ERR "hugepagesz: Unsupported page size %lu M\n",
 +			ps >> 20);
 +		return 0;
 +	}
 +	return 1;
 +}
 +__setup("hugepagesz=", setup_hugepagesz);
 +
 +#if (defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || defined(CONFIG_CMA)
++=======
+ bool __init arch_hugetlb_valid_size(unsigned long size)
+ {
+ 	if (size == PMD_SIZE)
+ 		return true;
+ 	else if (size == PUD_SIZE && boot_cpu_has(X86_FEATURE_GBPAGES))
+ 		return true;
+ 	else
+ 		return false;
+ }
+ 
+ #ifdef CONFIG_CONTIG_ALLOC
++>>>>>>> 359f25443a8d (hugetlbfs: move hugepagesz= parsing to arch independent code)
  static __init int gigantic_pages_init(void)
  {
  	/* With compaction or CMA we can allocate gigantic pages at runtime */
diff --cc mm/hugetlb.c
index 449fb9efecb9,6a8454bc2917..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -2869,10 -3256,10 +2869,17 @@@ static int __init hugetlb_init(void
  }
  subsys_initcall(hugetlb_init);
  
++<<<<<<< HEAD
 +/* Should be called on processing a hugepagesz=... option */
 +void __init hugetlb_bad_size(void)
 +{
 +	parsed_valid_hugepagesz = false;
++=======
+ /* Overwritten by architectures with more huge page sizes */
+ bool __init __attribute((weak)) arch_hugetlb_valid_size(unsigned long size)
+ {
+ 	return size == HPAGE_SIZE;
++>>>>>>> 359f25443a8d (hugetlbfs: move hugepagesz= parsing to arch independent code)
  }
  
  void __init hugetlb_add_hstate(unsigned int order)
@@@ -2944,12 -3331,38 +2951,33 @@@ static int __init hugetlb_nrpages_setup
  }
  __setup("hugepages=", hugetlb_nrpages_setup);
  
++<<<<<<< HEAD
 +static int __init hugetlb_default_setup(char *s)
++=======
+ static int __init hugepagesz_setup(char *s)
+ {
+ 	unsigned long size;
+ 
+ 	size = (unsigned long)memparse(s, NULL);
+ 
+ 	if (!arch_hugetlb_valid_size(size)) {
+ 		parsed_valid_hugepagesz = false;
+ 		pr_err("HugeTLB: unsupported hugepagesz %s\n", s);
+ 		return 0;
+ 	}
+ 
+ 	hugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);
+ 	return 1;
+ }
+ __setup("hugepagesz=", hugepagesz_setup);
+ 
+ static int __init default_hugepagesz_setup(char *s)
++>>>>>>> 359f25443a8d (hugetlbfs: move hugepagesz= parsing to arch independent code)
  {
 -	unsigned long size;
 -
 -	size = (unsigned long)memparse(s, NULL);
 -
 -	if (!arch_hugetlb_valid_size(size)) {
 -		pr_err("HugeTLB: unsupported default_hugepagesz %s\n", s);
 -		return 0;
 -	}
 -
 -	default_hstate_size = size;
 +	default_hstate_size = memparse(s, &s);
  	return 1;
  }
 -__setup("default_hugepagesz=", default_hugepagesz_setup);
 +__setup("default_hugepagesz=", hugetlb_default_setup);
  
  static unsigned int cpuset_mems_nr(unsigned int *array)
  {
* Unmerged path arch/riscv/mm/hugetlbpage.c
* Unmerged path arch/arm64/mm/hugetlbpage.c
* Unmerged path arch/powerpc/mm/hugetlbpage.c
* Unmerged path arch/riscv/mm/hugetlbpage.c
* Unmerged path arch/s390/mm/hugetlbpage.c
* Unmerged path arch/sparc/mm/init_64.c
* Unmerged path arch/x86/mm/hugetlbpage.c
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 75b7d65145a4..5004a8b25ce6 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -382,7 +382,6 @@ int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 int __init __alloc_bootmem_huge_page(struct hstate *h);
 int __init alloc_bootmem_huge_page(struct hstate *h);
 
-void __init hugetlb_bad_size(void);
 void __init hugetlb_add_hstate(unsigned order);
 struct hstate *size_to_hstate(unsigned long size);
 
* Unmerged path mm/hugetlb.c
