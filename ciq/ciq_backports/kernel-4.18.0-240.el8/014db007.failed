io_uring: remove @nxt from handlers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 014db0073cc6a12e1f421b9231d6f3aa35735823
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/014db007.failed

There will be no use for @nxt in the handlers, and it's doesn't work
anyway, so purge it

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 014db0073cc6a12e1f421b9231d6f3aa35735823)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,f4faaa2a9a3f..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1089,11 -1998,23 +1089,31 @@@ static inline void io_rw_done(struct ki
  	}
  }
  
++<<<<<<< HEAD
 +static int io_import_fixed(struct io_ring_ctx *ctx, int rw,
 +			   const struct io_uring_sqe *sqe,
 +			   struct iov_iter *iter)
 +{
 +	size_t len = READ_ONCE(sqe->len);
++=======
+ static void kiocb_done(struct kiocb *kiocb, ssize_t ret)
+ {
+ 	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);
+ 
+ 	if (req->flags & REQ_F_CUR_POS)
+ 		req->file->f_pos = kiocb->ki_pos;
+ 	if (ret >= 0 && kiocb->ki_complete == io_complete_rw)
+ 		io_complete_rw(kiocb, ret, 0);
+ 	else
+ 		io_rw_done(kiocb, ret);
+ }
+ 
+ static ssize_t io_import_fixed(struct io_kiocb *req, int rw,
+ 			       struct iov_iter *iter)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	size_t len = req->rw.len;
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  	struct io_mapped_ubuf *imu;
  	unsigned index, buf_index;
  	size_t offset;
@@@ -1318,105 -2185,199 +1338,216 @@@ static ssize_t loop_rw_iter(int rw, str
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
 +		   bool force_nonblock)
++=======
+ static void io_req_map_rw(struct io_kiocb *req, ssize_t io_size,
+ 			  struct iovec *iovec, struct iovec *fast_iov,
+ 			  struct iov_iter *iter)
+ {
+ 	req->io->rw.nr_segs = iter->nr_segs;
+ 	req->io->rw.size = io_size;
+ 	req->io->rw.iov = iovec;
+ 	if (!req->io->rw.iov) {
+ 		req->io->rw.iov = req->io->rw.fast_iov;
+ 		memcpy(req->io->rw.iov, fast_iov,
+ 			sizeof(struct iovec) * iter->nr_segs);
+ 	} else {
+ 		req->flags |= REQ_F_NEED_CLEANUP;
+ 	}
+ }
+ 
+ static int io_alloc_async_ctx(struct io_kiocb *req)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 	req->io = kmalloc(sizeof(*req->io), GFP_KERNEL);
+ 	return req->io == NULL;
+ }
+ 
+ static int io_setup_async_rw(struct io_kiocb *req, ssize_t io_size,
+ 			     struct iovec *iovec, struct iovec *fast_iov,
+ 			     struct iov_iter *iter)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 	if (!req->io) {
+ 		if (io_alloc_async_ctx(req))
+ 			return -ENOMEM;
+ 
+ 		io_req_map_rw(req, io_size, iovec, fast_iov, iter);
+ 	}
+ 	return 0;
+ }
+ 
+ static int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			bool force_nonblock)
+ {
+ 	struct io_async_ctx *io;
+ 	struct iov_iter iter;
+ 	ssize_t ret;
+ 
+ 	ret = io_prep_rw(req, sqe, force_nonblock);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (unlikely(!(req->file->f_mode & FMODE_READ)))
+ 		return -EBADF;
+ 
+ 	/* either don't need iovec imported or already have it */
+ 	if (!req->io || req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	io = req->io;
+ 	io->rw.iov = io->rw.fast_iov;
+ 	req->io = NULL;
+ 	ret = io_import_iovec(READ, req, &io->rw.iov, &iter);
+ 	req->io = io;
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
+ 	return 0;
+ }
+ 
+ static int io_read(struct io_kiocb *req, bool force_nonblock)
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 -	struct kiocb *kiocb = &req->rw.kiocb;
 +	struct kiocb *kiocb = &req->rw;
  	struct iov_iter iter;
 +	struct file *file;
  	size_t iov_count;
 -	ssize_t io_size, ret;
 +	ssize_t read_size, ret;
  
 -	ret = io_import_iovec(READ, req, &iovec, &iter);
 -	if (ret < 0)
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
  		return ret;
 +	file = kiocb->ki_filp;
  
 -	/* Ensure we clear previously set non-block flag */
 -	if (!force_nonblock)
 -		kiocb->ki_flags &= ~IOCB_NOWAIT;
 +	if (unlikely(!(file->f_mode & FMODE_READ)))
 +		return -EBADF;
  
 -	req->result = 0;
 -	io_size = ret;
 -	if (req->flags & REQ_F_LINK)
 -		req->result = io_size;
 +	ret = io_import_iovec(req->ctx, READ, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 -	/*
 -	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
 -	 * we know to async punt it even if it was opened O_NONBLOCK
 -	 */
 -	if (force_nonblock && !io_file_supports_async(req->file))
 -		goto copy_iov;
 +	read_size = ret;
 +	if (req->flags & REQ_F_LINK)
 +		req->result = read_size;
  
  	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(READ, req->file, &kiocb->ki_pos, iov_count);
 +	ret = rw_verify_area(READ, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
  
 -		if (req->file->f_op->read_iter)
 -			ret2 = call_read_iter(req->file, kiocb, &iter);
 +		if (file->f_op->read_iter)
 +			ret2 = call_read_iter(file, kiocb, &iter);
  		else
 -			ret2 = loop_rw_iter(READ, req->file, kiocb, &iter);
 +			ret2 = loop_rw_iter(READ, file, kiocb, &iter);
  
 +		/*
 +		 * In case of a short read, punt to async. This can happen
 +		 * if we have data partially cached. Alternatively we can
 +		 * return the short read, in which case the application will
 +		 * need to issue another SQE and wait for it. That SQE will
 +		 * need async punt anyway, so it's more efficient to do it
 +		 * here.
 +		 */
 +		if (force_nonblock && ret2 > 0 && ret2 < read_size)
 +			ret2 = -EAGAIN;
  		/* Catch -EAGAIN return for forced non-blocking submission */
  		if (!force_nonblock || ret2 != -EAGAIN) {
++<<<<<<< HEAD
 +			io_rw_done(kiocb, ret2);
++=======
+ 			kiocb_done(kiocb, ret2);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  		} else {
 -copy_iov:
 -			ret = io_setup_async_rw(req, io_size, iovec,
 -						inline_vecs, &iter);
 -			if (ret)
 -				goto out_free;
 -			/* any defer here is final, must blocking retry */
 -			if (!(req->flags & REQ_F_NOWAIT))
 -				req->flags |= REQ_F_MUST_PUNT;
 -			return -EAGAIN;
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(READ, req, iov_count);
 +			ret = -EAGAIN;
  		}
  	}
 -out_free:
  	kfree(iovec);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_write(struct io_kiocb *req, const struct sqe_submit *s,
 +		    bool force_nonblock)
++=======
+ static int io_write_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			 bool force_nonblock)
+ {
+ 	struct io_async_ctx *io;
+ 	struct iov_iter iter;
+ 	ssize_t ret;
+ 
+ 	ret = io_prep_rw(req, sqe, force_nonblock);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (unlikely(!(req->file->f_mode & FMODE_WRITE)))
+ 		return -EBADF;
+ 
+ 	/* either don't need iovec imported or already have it */
+ 	if (!req->io || req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	io = req->io;
+ 	io->rw.iov = io->rw.fast_iov;
+ 	req->io = NULL;
+ 	ret = io_import_iovec(WRITE, req, &io->rw.iov, &iter);
+ 	req->io = io;
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
+ 	return 0;
+ }
+ 
+ static int io_write(struct io_kiocb *req, bool force_nonblock)
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 -	struct kiocb *kiocb = &req->rw.kiocb;
 +	struct kiocb *kiocb = &req->rw;
  	struct iov_iter iter;
 +	struct file *file;
  	size_t iov_count;
 -	ssize_t ret, io_size;
 +	ssize_t ret;
  
 -	ret = io_import_iovec(WRITE, req, &iovec, &iter);
 -	if (ret < 0)
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
  		return ret;
  
 -	/* Ensure we clear previously set non-block flag */
 -	if (!force_nonblock)
 -		req->rw.kiocb.ki_flags &= ~IOCB_NOWAIT;
 +	file = kiocb->ki_filp;
 +	if (unlikely(!(file->f_mode & FMODE_WRITE)))
 +		return -EBADF;
 +
 +	ret = io_import_iovec(req->ctx, WRITE, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 -	req->result = 0;
 -	io_size = ret;
  	if (req->flags & REQ_F_LINK)
 -		req->result = io_size;
 +		req->result = ret;
  
 -	/*
 -	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
 -	 * we know to async punt it even if it was opened O_NONBLOCK
 -	 */
 -	if (force_nonblock && !io_file_supports_async(req->file))
 -		goto copy_iov;
 +	iov_count = iov_iter_count(&iter);
  
 -	/* file path doesn't support NOWAIT for non-direct_IO */
 -	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&
 -	    (req->flags & REQ_F_ISREG))
 -		goto copy_iov;
 +	ret = -EAGAIN;
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
 +		goto out_free;
 +	}
  
 -	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(WRITE, req->file, &kiocb->ki_pos, iov_count);
 +	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
  
@@@ -1435,20 -2396,27 +1566,24 @@@
  		}
  		kiocb->ki_flags |= IOCB_WRITE;
  
 -		if (req->file->f_op->write_iter)
 -			ret2 = call_write_iter(req->file, kiocb, &iter);
 +		if (file->f_op->write_iter)
 +			ret2 = call_write_iter(file, kiocb, &iter);
  		else
 -			ret2 = loop_rw_iter(WRITE, req->file, kiocb, &iter);
 -		/*
 -		 * Raw bdev writes will -EOPNOTSUPP for IOCB_NOWAIT. Just
 -		 * retry them without IOCB_NOWAIT.
 -		 */
 -		if (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT))
 -			ret2 = -EAGAIN;
 +			ret2 = loop_rw_iter(WRITE, file, kiocb, &iter);
  		if (!force_nonblock || ret2 != -EAGAIN) {
++<<<<<<< HEAD
 +			io_rw_done(kiocb, ret2);
++=======
+ 			kiocb_done(kiocb, ret2);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  		} else {
 -copy_iov:
 -			ret = io_setup_async_rw(req, io_size, iovec,
 -						inline_vecs, &iter);
 -			if (ret)
 -				goto out_free;
 -			/* any defer here is final, must blocking retry */
 -			req->flags |= REQ_F_MUST_PUNT;
 -			return -EAGAIN;
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(WRITE, req, iov_count);
 +			ret = -EAGAIN;
  		}
  	}
  out_free:
@@@ -1456,6 -2425,76 +1591,79 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int io_splice_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_splice* sp = &req->splice;
+ 	unsigned int valid_flags = SPLICE_F_FD_IN_FIXED | SPLICE_F_ALL;
+ 	int ret;
+ 
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	sp->file_in = NULL;
+ 	sp->off_in = READ_ONCE(sqe->splice_off_in);
+ 	sp->off_out = READ_ONCE(sqe->off);
+ 	sp->len = READ_ONCE(sqe->len);
+ 	sp->flags = READ_ONCE(sqe->splice_flags);
+ 
+ 	if (unlikely(sp->flags & ~valid_flags))
+ 		return -EINVAL;
+ 
+ 	ret = io_file_get(NULL, req, READ_ONCE(sqe->splice_fd_in), &sp->file_in,
+ 			  (sp->flags & SPLICE_F_FD_IN_FIXED));
+ 	if (ret)
+ 		return ret;
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 
+ 	if (!S_ISREG(file_inode(sp->file_in)->i_mode))
+ 		req->work.flags |= IO_WQ_WORK_UNBOUND;
+ 
+ 	return 0;
+ }
+ 
+ static bool io_splice_punt(struct file *file)
+ {
+ 	if (get_pipe_info(file))
+ 		return false;
+ 	if (!io_file_supports_async(file))
+ 		return true;
+ 	return !(file->f_mode & O_NONBLOCK);
+ }
+ 
+ static int io_splice(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_splice *sp = &req->splice;
+ 	struct file *in = sp->file_in;
+ 	struct file *out = sp->file_out;
+ 	unsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;
+ 	loff_t *poff_in, *poff_out;
+ 	long ret;
+ 
+ 	if (force_nonblock) {
+ 		if (io_splice_punt(in) || io_splice_punt(out))
+ 			return -EAGAIN;
+ 		flags |= SPLICE_F_NONBLOCK;
+ 	}
+ 
+ 	poff_in = (sp->off_in == -1) ? NULL : &sp->off_in;
+ 	poff_out = (sp->off_out == -1) ? NULL : &sp->off_out;
+ 	ret = do_splice(in, poff_in, out, poff_out, sp->len, flags);
+ 	if (force_nonblock && ret == -EAGAIN)
+ 		return -EAGAIN;
+ 
+ 	io_put_file(req, in, (sp->flags & SPLICE_F_FD_IN_FIXED));
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 
+ 	io_cqring_add_event(req, ret);
+ 	if (ret != sp->len)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  /*
   * IORING_OP_NOP just posts a completion event, nothing else.
   */
@@@ -1487,81 -2525,233 +1695,255 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		    bool force_nonblock)
++=======
+ static bool io_req_cancelled(struct io_kiocb *req)
+ {
+ 	if (req->work.flags & IO_WQ_WORK_CANCEL) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, -ECANCELED);
+ 		io_double_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_link_work_cb(struct io_wq_work **workptr)
+ {
+ 	struct io_wq_work *work = *workptr;
+ 	struct io_kiocb *link = work->data;
+ 
+ 	io_queue_linked_timeout(link);
+ 	io_wq_submit_work(workptr);
+ }
+ 
+ static void io_wq_assign_next(struct io_wq_work **workptr, struct io_kiocb *nxt)
+ {
+ 	struct io_kiocb *link;
+ 
+ 	*workptr = &nxt->work;
+ 	link = io_prep_linked_timeout(nxt);
+ 	if (link) {
+ 		nxt->work.func = io_link_work_cb;
+ 		nxt->work.data = link;
+ 	}
+ }
+ 
+ static void __io_fsync(struct io_kiocb *req)
+ {
+ 	loff_t end = req->sync.off + req->sync.len;
+ 	int ret;
+ 
+ 	ret = vfs_fsync_range(req->file, req->sync.off,
+ 				end > 0 ? end : LLONG_MAX,
+ 				req->sync.flags & IORING_FSYNC_DATASYNC);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ }
+ 
+ static void io_fsync_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fsync(req);
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_fsync(struct io_kiocb *req, bool force_nonblock)
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  {
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
 +	int ret;
 +
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
 +
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
 +
  	/* fsync always requires a blocking context */
- 	if (force_nonblock)
++<<<<<<< HEAD
++=======
+ 	if (force_nonblock) {
+ 		req->work.func = io_fsync_finish;
  		return -EAGAIN;
+ 	}
+ 	__io_fsync(req);
+ 	return 0;
+ }
  
- 	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
- 				end > 0 ? end : LLONG_MAX,
- 				fsync_flags & IORING_FSYNC_DATASYNC);
+ static void __io_fallocate(struct io_kiocb *req)
+ {
+ 	int ret;
  
- 	if (ret < 0 && (req->flags & REQ_F_LINK))
- 		req->flags |= REQ_F_FAIL_LINK;
- 	io_cqring_add_event(req->ctx, sqe->user_data, ret);
+ 	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
+ 				req->sync.len);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
  	io_put_req(req);
- 	return 0;
  }
  
- static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ static void io_fallocate_finish(struct io_wq_work **workptr)
  {
- 	struct io_ring_ctx *ctx = req->ctx;
- 	int ret = 0;
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
  
- 	if (!req->file)
- 		return -EBADF;
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fallocate(req);
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
  
- 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ static int io_fallocate_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
  		return -EINVAL;
- 	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
+ 
+ 	req->sync.off = READ_ONCE(sqe->off);
+ 	req->sync.len = READ_ONCE(sqe->addr);
+ 	req->sync.mode = READ_ONCE(sqe->len);
+ 	return 0;
+ }
+ 
+ static int io_fallocate(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	/* fallocate always requiring blocking context */
+ 	if (force_nonblock) {
+ 		req->work.func = io_fallocate_finish;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	__io_fallocate(req);
+ 	return 0;
+ }
+ 
+ static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
  		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
  
- 	return ret;
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.how.mode = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.how.flags = READ_ONCE(sqe->open_flags);
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
  }
  
- static int io_sync_file_range(struct io_kiocb *req,
- 			      const struct io_uring_sqe *sqe,
- 			      bool force_nonblock)
+ static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
- 	loff_t sqe_off;
- 	loff_t sqe_len;
- 	unsigned flags;
+ 	struct open_how __user *how;
+ 	const char __user *fname;
+ 	size_t len;
  	int ret;
  
- 	ret = io_prep_sfr(req, sqe);
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	len = READ_ONCE(sqe->len);
+ 
+ 	if (len < OPEN_HOW_SIZE_VER0)
+ 		return -EINVAL;
+ 
+ 	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
+ 					len);
  	if (ret)
  		return ret;
  
- 	/* sync_file_range always requires a blocking context */
+ 	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
+ 		req->open.how.flags |= O_LARGEFILE;
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_openat2(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct open_flags op;
+ 	struct file *file;
+ 	int ret;
+ 
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  	if (force_nonblock)
  		return -EAGAIN;
  
- 	sqe_off = READ_ONCE(sqe->off);
- 	sqe_len = READ_ONCE(sqe->len);
- 	flags = READ_ONCE(sqe->sync_range_flags);
- 
- 	ret = sync_file_range(req->rw.ki_filp, sqe_off, sqe_len, flags);
 -	ret = build_open_flags(&req->open.how, &op);
 -	if (ret)
 -		goto err;
++	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
++				end > 0 ? end : LLONG_MAX,
++				fsync_flags & IORING_FSYNC_DATASYNC);
  
++<<<<<<< HEAD
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
++=======
+ 	ret = get_unused_fd_flags(req->open.how.flags);
+ 	if (ret < 0)
+ 		goto err;
+ 
+ 	file = do_filp_open(req->open.dfd, req->open.filename, &op);
+ 	if (IS_ERR(file)) {
+ 		put_unused_fd(ret);
+ 		ret = PTR_ERR(file);
+ 	} else {
+ 		fsnotify_open(file);
+ 		fd_install(ret, file);
+ 	}
+ err:
+ 	putname(req->open.filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
  	return 0;
  }
  
@@@ -1614,70 -2806,1040 +1998,674 @@@ static int io_epoll_ctl(struct io_kioc
  #endif
  }
  
- static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
- 		      bool force_nonblock)
+ static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
- #if defined(CONFIG_NET)
- 	return io_send_recvmsg(req, sqe, force_nonblock, __sys_recvmsg_sock);
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	if (sqe->ioprio || sqe->buf_index || sqe->off)
+ 		return -EINVAL;
+ 
+ 	req->madvise.addr = READ_ONCE(sqe->addr);
+ 	req->madvise.len = READ_ONCE(sqe->len);
+ 	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	struct io_madvise *ma = &req->madvise;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	ret = do_madvise(ma->addr, ma->len, ma->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->addr)
+ 		return -EINVAL;
+ 
+ 	req->fadvise.offset = READ_ONCE(sqe->off);
+ 	req->fadvise.len = READ_ONCE(sqe->len);
+ 	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ }
+ 
+ static int io_fadvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_fadvise *fa = &req->fadvise;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		switch (fa->advice) {
+ 		case POSIX_FADV_NORMAL:
+ 		case POSIX_FADV_RANDOM:
+ 		case POSIX_FADV_SEQUENTIAL:
+ 			break;
+ 		default:
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	unsigned lookup_flags;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.mask = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	req->open.how.flags = READ_ONCE(sqe->statx_flags);
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
+ 		return -EINVAL;
+ 
+ 	req->open.filename = getname_flags(fname, lookup_flags, NULL);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_statx(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_open *ctx = &req->open;
+ 	unsigned lookup_flags;
+ 	struct path path;
+ 	struct kstat stat;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
+ 		return -EINVAL;
+ 
+ retry:
+ 	/* filename_lookup() drops it, keep a reference */
+ 	ctx->filename->refcnt++;
+ 
+ 	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
+ 				NULL);
+ 	if (ret)
+ 		goto err;
+ 
+ 	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
+ 	path_put(&path);
+ 	if (retry_estale(ret, lookup_flags)) {
+ 		lookup_flags |= LOOKUP_REVAL;
+ 		goto retry;
+ 	}
+ 	if (!ret)
+ 		ret = cp_statx(&stat, ctx->buffer);
+ err:
+ 	putname(ctx->filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * If we queue this for async, it must not be cancellable. That would
+ 	 * leave the 'file' in an undeterminate state.
+ 	 */
+ 	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
+ 
+ 	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
+ 	    sqe->rw_flags || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->close.fd = READ_ONCE(sqe->fd);
+ 	if (req->file->f_op == &io_uring_fops ||
+ 	    req->close.fd == req->ctx->ring_fd)
+ 		return -EBADF;
+ 
+ 	return 0;
+ }
+ 
+ /* only called when __close_fd_get_file() is done */
+ static void __io_close_finish(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = filp_close(req->close.put_file, req->work.files);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	fput(req->close.put_file);
+ 	io_put_req(req);
+ }
+ 
+ static void io_close_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	/* not cancellable, don't do io_req_cancelled() */
+ 	__io_close_finish(req);
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_close(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	int ret;
+ 
+ 	req->close.put_file = NULL;
+ 	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* if the file has a flush method, be safe and punt to async */
+ 	if (req->close.put_file->f_op->flush && force_nonblock) {
+ 		/* submission ref will be dropped, take it for async */
+ 		refcount_inc(&req->refs);
+ 
+ 		req->work.func = io_close_finish;
+ 		/*
+ 		 * Do manual async queue here to avoid grabbing files - we don't
+ 		 * need the files, and it'll cause io_close_finish() to close
+ 		 * the file again and cause a double CQE entry for this request
+ 		 */
+ 		io_queue_async_work(req);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * No ->flush(), safely close from here and just punt the
+ 	 * fput() to async context.
+ 	 */
+ 	__io_close_finish(req);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
+ 	return 0;
+ }
+ 
+ static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
++	int ret = 0;
+ 
+ 	if (!req->file)
+ 		return -EBADF;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
+ 		return -EINVAL;
+ 
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->len);
 -	req->sync.flags = READ_ONCE(sqe->sync_range_flags);
 -	return 0;
++	return ret;
+ }
+ 
++<<<<<<< HEAD
++static int io_sync_file_range(struct io_kiocb *req,
++			      const struct io_uring_sqe *sqe,
++			      bool force_nonblock)
++=======
+ static void __io_sync_file_range(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = sync_file_range(req->file, req->sync.off, req->sync.len,
+ 				req->sync.flags);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ }
+ 
+ 
+ static void io_sync_file_range_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_sync_file_range(req);
+ 	io_put_req(req); /* put submission ref */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_sync_file_range(struct io_kiocb *req, bool force_nonblock)
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
+ {
++	loff_t sqe_off;
++	loff_t sqe_len;
++	unsigned flags;
++	int ret;
++
++	ret = io_prep_sfr(req, sqe);
++	if (ret)
++		return ret;
++
+ 	/* sync_file_range always requires a blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_sync_file_range_finish;
++	if (force_nonblock)
+ 		return -EAGAIN;
 -	}
+ 
++<<<<<<< HEAD
++	sqe_off = READ_ONCE(sqe->off);
++	sqe_len = READ_ONCE(sqe->len);
++	flags = READ_ONCE(sqe->sync_range_flags);
++
++	ret = sync_file_range(req->rw.ki_filp, sqe_off, sqe_len, flags);
++
++	if (ret < 0 && (req->flags & REQ_F_LINK))
++		req->flags |= REQ_F_FAIL_LINK;
++	io_cqring_add_event(req->ctx, sqe->user_data, ret);
++	io_put_req(req);
++=======
+ 	__io_sync_file_range(req);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
+ 	return 0;
+ }
+ 
 -static int io_setup_async_msg(struct io_kiocb *req,
 -			      struct io_async_msghdr *kmsg)
 -{
 -	if (req->io)
 -		return -EAGAIN;
 -	if (io_alloc_async_ctx(req)) {
 -		if (kmsg->iov != kmsg->fast_iov)
 -			kfree(kmsg->iov);
 -		return -ENOMEM;
 -	}
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	memcpy(&req->io->msg, kmsg, sizeof(*kmsg));
 -	return -EAGAIN;
 -}
 -
 -static int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
+ #if defined(CONFIG_NET)
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct io_async_ctx *io = req->io;
++static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++			   bool force_nonblock,
++		   long (*fn)(struct socket *, struct user_msghdr __user *,
++				unsigned int))
++{
++	struct socket *sock;
+ 	int ret;
+ 
 -	sr->msg_flags = READ_ONCE(sqe->msg_flags);
 -	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	sr->len = READ_ONCE(sqe->len);
++	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
++		return -EINVAL;
+ 
 -#ifdef CONFIG_COMPAT
 -	if (req->ctx->compat)
 -		sr->msg_flags |= MSG_CMSG_COMPAT;
 -#endif
++	sock = sock_from_file(req->file, &ret);
++	if (sock) {
++		struct user_msghdr __user *msg;
++		unsigned flags;
+ 
 -	if (!io || req->opcode == IORING_OP_SEND)
 -		return 0;
 -	/* iovec is already imported */
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
++		flags = READ_ONCE(sqe->msg_flags);
++		if (flags & MSG_DONTWAIT)
++			req->flags |= REQ_F_NOWAIT;
++		else if (force_nonblock)
++			flags |= MSG_DONTWAIT;
+ 
 -	io->msg.iov = io->msg.fast_iov;
 -	ret = sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
 -					&io->msg.iov);
 -	if (!ret)
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -	return ret;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
++		msg = (struct user_msghdr __user *) (unsigned long)
++			READ_ONCE(sqe->addr);
++
++		ret = fn(sock, msg, flags);
++		if (force_nonblock && ret == -EAGAIN)
++			return ret;
++		if (ret == -ERESTARTSYS)
++			ret = -EINTR;
++	}
++
++	io_cqring_add_event(req->ctx, sqe->user_data, ret);
++	io_put_req(req);
++	return 0;
+ }
++#endif
+ 
++<<<<<<< HEAD
++static int io_sendmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++		      bool force_nonblock)
++{
++#if defined(CONFIG_NET)
++	return io_send_recvmsg(req, sqe, force_nonblock, __sys_sendmsg_sock);
++=======
+ static int io_sendmsg(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_async_msghdr *kmsg = NULL;
+ 	struct socket *sock;
+ 	int ret;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_async_ctx io;
+ 		unsigned flags;
+ 
+ 		if (req->io) {
+ 			kmsg = &req->io->msg;
+ 			kmsg->msg.msg_name = &req->io->msg.addr;
+ 			/* if iov is set, it's allocated already */
+ 			if (!kmsg->iov)
+ 				kmsg->iov = kmsg->fast_iov;
+ 			kmsg->msg.msg_iter.iov = kmsg->iov;
+ 		} else {
+ 			struct io_sr_msg *sr = &req->sr_msg;
+ 
+ 			kmsg = &io.msg;
+ 			kmsg->msg.msg_name = &io.msg.addr;
+ 
+ 			io.msg.iov = io.msg.fast_iov;
+ 			ret = sendmsg_copy_msghdr(&io.msg.msg, sr->msg,
+ 					sr->msg_flags, &io.msg.iov);
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
+ 		ret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);
+ 		if (force_nonblock && ret == -EAGAIN)
+ 			return io_setup_async_msg(req, kmsg);
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
+ 	}
+ 
+ 	if (kmsg && kmsg->iov != kmsg->fast_iov)
+ 		kfree(kmsg->iov);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
++<<<<<<< HEAD
++static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++		      bool force_nonblock)
++=======
+ static int io_send(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct socket *sock;
+ 	int ret;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_sr_msg *sr = &req->sr_msg;
+ 		struct msghdr msg;
+ 		struct iovec iov;
+ 		unsigned flags;
+ 
+ 		ret = import_single_range(WRITE, sr->buf, sr->len, &iov,
+ 						&msg.msg_iter);
+ 		if (ret)
+ 			return ret;
+ 
+ 		msg.msg_name = NULL;
+ 		msg.msg_control = NULL;
+ 		msg.msg_controllen = 0;
+ 		msg.msg_namelen = 0;
+ 
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
+ 		msg.msg_flags = flags;
+ 		ret = sock_sendmsg(sock, &msg);
+ 		if (force_nonblock && ret == -EAGAIN)
+ 			return -EAGAIN;
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
+ 	}
+ 
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_recvmsg_prep(struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_sr_msg *sr = &req->sr_msg;
+ 	struct io_async_ctx *io = req->io;
+ 	int ret;
+ 
+ 	sr->msg_flags = READ_ONCE(sqe->msg_flags);
+ 	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	sr->len = READ_ONCE(sqe->len);
+ 
+ #ifdef CONFIG_COMPAT
+ 	if (req->ctx->compat)
+ 		sr->msg_flags |= MSG_CMSG_COMPAT;
+ #endif
+ 
+ 	if (!io || req->opcode == IORING_OP_RECV)
+ 		return 0;
+ 	/* iovec is already imported */
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	io->msg.iov = io->msg.fast_iov;
+ 	ret = recvmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
+ 					&io->msg.uaddr, &io->msg.iov);
+ 	if (!ret)
+ 		req->flags |= REQ_F_NEED_CLEANUP;
+ 	return ret;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_recvmsg(struct io_kiocb *req, bool force_nonblock)
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
+ {
+ #if defined(CONFIG_NET)
 -	struct io_async_msghdr *kmsg = NULL;
 -	struct socket *sock;
 -	int ret;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct io_async_ctx io;
 -		unsigned flags;
++	return io_send_recvmsg(req, sqe, force_nonblock, __sys_recvmsg_sock);
 +#else
 +	return -EOPNOTSUPP;
 +#endif
 +}
  
 -		if (req->io) {
 -			kmsg = &req->io->msg;
 -			kmsg->msg.msg_name = &req->io->msg.addr;
 -			/* if iov is set, it's allocated already */
 -			if (!kmsg->iov)
 -				kmsg->iov = kmsg->fast_iov;
 -			kmsg->msg.msg_iter.iov = kmsg->iov;
 -		} else {
 -			struct io_sr_msg *sr = &req->sr_msg;
 +static void io_poll_remove_one(struct io_kiocb *req)
 +{
 +	struct io_poll_iocb *poll = &req->poll;
  
 -			kmsg = &io.msg;
 -			kmsg->msg.msg_name = &io.msg.addr;
 +	spin_lock(&poll->head->lock);
 +	WRITE_ONCE(poll->canceled, true);
 +	if (!list_empty(&poll->wait.entry)) {
 +		list_del_init(&poll->wait.entry);
 +		io_queue_async_work(req->ctx, req);
 +	}
 +	spin_unlock(&poll->head->lock);
  
 -			io.msg.iov = io.msg.fast_iov;
 -			ret = recvmsg_copy_msghdr(&io.msg.msg, sr->msg,
 -					sr->msg_flags, &io.msg.uaddr,
 -					&io.msg.iov);
 -			if (ret)
 -				return ret;
 -		}
 +	list_del_init(&req->list);
 +}
  
 -		flags = req->sr_msg.msg_flags;
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 +static void io_poll_remove_all(struct io_ring_ctx *ctx)
 +{
 +	struct io_kiocb *req;
  
 -		ret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.msg,
 -						kmsg->uaddr, flags);
 -		if (force_nonblock && ret == -EAGAIN)
 -			return io_setup_async_msg(req, kmsg);
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 +	spin_lock_irq(&ctx->completion_lock);
 +	while (!list_empty(&ctx->cancel_list)) {
 +		req = list_first_entry(&ctx->cancel_list, struct io_kiocb,list);
 +		io_poll_remove_one(req);
  	}
 -
 -	if (kmsg && kmsg->iov != kmsg->fast_iov)
 -		kfree(kmsg->iov);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 +	spin_unlock_irq(&ctx->completion_lock);
  }
  
 -static int io_recv(struct io_kiocb *req, bool force_nonblock)
 +/*
 + * Find a running poll command that matches one specified in sqe->addr,
 + * and remove it if found.
 + */
 +static int io_poll_remove(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 -#if defined(CONFIG_NET)
 -	struct socket *sock;
 -	int ret;
 +	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_kiocb *poll_req, *next;
 +	int ret = -ENOENT;
  
  	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
  		return -EINVAL;
 +	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 +	    sqe->poll_events)
 +		return -EINVAL;
  
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct io_sr_msg *sr = &req->sr_msg;
 -		struct msghdr msg;
 -		struct iovec iov;
 -		unsigned flags;
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_for_each_entry_safe(poll_req, next, &ctx->cancel_list, list) {
 +		if (READ_ONCE(sqe->addr) == poll_req->user_data) {
 +			io_poll_remove_one(poll_req);
 +			ret = 0;
 +			break;
 +		}
 +	}
 +	spin_unlock_irq(&ctx->completion_lock);
  
- 	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 -		ret = import_single_range(READ, sr->buf, sr->len, &iov,
 -						&msg.msg_iter);
 -		if (ret)
 -			return ret;
 -
 -		msg.msg_name = NULL;
 -		msg.msg_control = NULL;
 -		msg.msg_controllen = 0;
 -		msg.msg_namelen = 0;
 -		msg.msg_iocb = NULL;
 -		msg.msg_flags = 0;
 -
 -		flags = req->sr_msg.msg_flags;
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 -
 -		ret = sock_recvmsg(sock, &msg, flags);
 -		if (force_nonblock && ret == -EAGAIN)
 -			return -EAGAIN;
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 -	}
 -
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -
 -static int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_accept *accept = &req->accept;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -
 -	accept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	accept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	accept->flags = READ_ONCE(sqe->accept_flags);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -#if defined(CONFIG_NET)
 -static int __io_accept(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct io_accept *accept = &req->accept;
 -	unsigned file_flags;
 -	int ret;
 -
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -	ret = __sys_accept4_file(req->file, file_flags, accept->addr,
 -					accept->addr_len, accept->flags);
 -	if (ret == -EAGAIN && force_nonblock)
 -		return -EAGAIN;
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -	if (ret < 0)
 -		req_set_fail_links(req);
++<<<<<<< HEAD
++	io_cqring_add_event(req->ctx, sqe->user_data, ret);
++=======
++	if (kmsg && kmsg->iov != kmsg->fast_iov)
++		kfree(kmsg->iov);
++	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static void io_accept_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_accept(req, false);
 -	io_put_req(req); /* drop submission reference */
 -	if (nxt)
 -		io_wq_assign_next(workptr, nxt);
 -}
 -#endif
 -
 -static int io_accept(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	int ret;
 -
 -	ret = __io_accept(req, force_nonblock);
 -	if (ret == -EAGAIN && force_nonblock) {
 -		req->work.func = io_accept_finish;
 -		return -EAGAIN;
 -	}
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_connect *conn = &req->connect;
 -	struct io_async_ctx *io = req->io;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 -
 -	conn->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	conn->addr_len =  READ_ONCE(sqe->addr2);
 -
 -	if (!io)
 -		return 0;
 -
 -	return move_addr_to_kernel(conn->addr, conn->addr_len,
 -					&io->connect.address);
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_connect(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_ctx __io, *io;
 -	unsigned file_flags;
 -	int ret;
 -
 -	if (req->io) {
 -		io = req->io;
 -	} else {
 -		ret = move_addr_to_kernel(req->connect.addr,
 -						req->connect.addr_len,
 -						&__io.connect.address);
 -		if (ret)
 -			goto out;
 -		io = &__io;
 -	}
 -
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	ret = __sys_connect_file(req->file, &io->connect.address,
 -					req->connect.addr_len, file_flags);
 -	if ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {
 -		if (req->io)
 -			return -EAGAIN;
 -		if (io_alloc_async_ctx(req)) {
 -			ret = -ENOMEM;
 -			goto out;
 -		}
 -		memcpy(&req->io->connect, &__io.connect, sizeof(__io.connect));
 -		return -EAGAIN;
 -	}
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -out:
+ 	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -struct io_poll_table {
 -	struct poll_table_struct pt;
 -	struct io_kiocb *req;
 -	int error;
 -};
 -
 -static void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,
 -			    struct wait_queue_head *head)
 -{
 -	if (unlikely(poll->head)) {
 -		pt->error = -EINVAL;
 -		return;
 -	}
 -
 -	pt->error = 0;
 -	poll->head = head;
 -	add_wait_queue(head, &poll->wait);
 -}
 -
 -static void io_async_queue_proc(struct file *file, struct wait_queue_head *head,
 -			       struct poll_table_struct *p)
 -{
 -	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
 -
 -	__io_queue_proc(&pt->req->apoll->poll, pt, head);
 -}
 -
 -static int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,
 -			   __poll_t mask, task_work_func_t func)
 -{
 -	struct task_struct *tsk;
 -
 -	/* for instances that support it check for an event match first: */
 -	if (mask && !(mask & poll->events))
 -		return 0;
 -
 -	trace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);
 -
 -	list_del_init(&poll->wait.entry);
 -
 -	tsk = req->task;
 -	req->result = mask;
 -	init_task_work(&req->task_work, func);
 -	/*
 -	 * If this fails, then the task is exiting. If that is the case, then
 -	 * the exit check will ultimately cancel these work items. Hence we
 -	 * don't need to check here and handle it specifically.
 -	 */
 -	task_work_add(tsk, &req->task_work, true);
 -	wake_up_process(tsk);
 -	return 1;
 -}
 -
 -static void io_async_task_func(struct callback_head *cb)
 -{
 -	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
 -	struct async_poll *apoll = req->apoll;
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	trace_io_uring_task_run(req->ctx, req->opcode, req->user_data);
 -
 -	WARN_ON_ONCE(!list_empty(&req->apoll->poll.wait.entry));
 -
 -	if (hash_hashed(&req->hash_node)) {
 -		spin_lock_irq(&ctx->completion_lock);
 -		hash_del(&req->hash_node);
 -		spin_unlock_irq(&ctx->completion_lock);
 -	}
 -
 -	/* restore ->work in case we need to retry again */
 -	memcpy(&req->work, &apoll->work, sizeof(req->work));
 -
 -	__set_current_state(TASK_RUNNING);
 -	mutex_lock(&ctx->uring_lock);
 -	__io_queue_sqe(req, NULL);
 -	mutex_unlock(&ctx->uring_lock);
 -
 -	kfree(apoll);
 -}
 -
 -static int io_async_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 -			void *key)
 -{
 -	struct io_kiocb *req = wait->private;
 -	struct io_poll_iocb *poll = &req->apoll->poll;
 -
 -	trace_io_uring_poll_wake(req->ctx, req->opcode, req->user_data,
 -					key_to_poll(key));
 -
 -	return __io_async_wake(req, poll, key_to_poll(key), io_async_task_func);
 -}
 -
 -static void io_poll_req_insert(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct hlist_head *list;
 -
 -	list = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];
 -	hlist_add_head(&req->hash_node, list);
 -}
 -
 -static __poll_t __io_arm_poll_handler(struct io_kiocb *req,
 -				      struct io_poll_iocb *poll,
 -				      struct io_poll_table *ipt, __poll_t mask,
 -				      wait_queue_func_t wake_func)
 -	__acquires(&ctx->completion_lock)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	bool cancel = false;
 -
 -	poll->file = req->file;
 -	poll->head = NULL;
 -	poll->done = poll->canceled = false;
 -	poll->events = mask;
 -
 -	ipt->pt._key = mask;
 -	ipt->req = req;
 -	ipt->error = -EINVAL;
 -
 -	INIT_LIST_HEAD(&poll->wait.entry);
 -	init_waitqueue_func_entry(&poll->wait, wake_func);
 -	poll->wait.private = req;
 -
 -	mask = vfs_poll(req->file, &ipt->pt) & poll->events;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (likely(poll->head)) {
 -		spin_lock(&poll->head->lock);
 -		if (unlikely(list_empty(&poll->wait.entry))) {
 -			if (ipt->error)
 -				cancel = true;
 -			ipt->error = 0;
 -			mask = 0;
 -		}
 -		if (mask || ipt->error)
 -			list_del_init(&poll->wait.entry);
 -		else if (cancel)
 -			WRITE_ONCE(poll->canceled, true);
 -		else if (!poll->done) /* actually waiting for an event */
 -			io_poll_req_insert(req);
 -		spin_unlock(&poll->head->lock);
 -	}
 -
 -	return mask;
 -}
 -
 -static bool io_arm_poll_handler(struct io_kiocb *req)
 -{
 -	const struct io_op_def *def = &io_op_defs[req->opcode];
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct async_poll *apoll;
 -	struct io_poll_table ipt;
 -	__poll_t mask, ret;
 -
 -	if (!req->file || !file_can_poll(req->file))
 -		return false;
 -	if (req->flags & (REQ_F_MUST_PUNT | REQ_F_POLLED))
 -		return false;
 -	if (!def->pollin && !def->pollout)
 -		return false;
 -
 -	apoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);
 -	if (unlikely(!apoll))
 -		return false;
 -
 -	req->flags |= REQ_F_POLLED;
 -	memcpy(&apoll->work, &req->work, sizeof(req->work));
 -
 -	/*
 -	 * Don't need a reference here, as we're adding it to the task
 -	 * task_works list. If the task exits, the list is pruned.
 -	 */
 -	req->task = current;
 -	req->apoll = apoll;
 -	INIT_HLIST_NODE(&req->hash_node);
 -
 -	mask = 0;
 -	if (def->pollin)
 -		mask |= POLLIN | POLLRDNORM;
 -	if (def->pollout)
 -		mask |= POLLOUT | POLLWRNORM;
 -	mask |= POLLERR | POLLPRI;
 -
 -	ipt.pt._qproc = io_async_queue_proc;
 -
 -	ret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,
 -					io_async_wake);
 -	if (ret) {
 -		ipt.error = 0;
 -		apoll->poll.done = true;
 -		spin_unlock_irq(&ctx->completion_lock);
 -		memcpy(&req->work, &apoll->work, sizeof(req->work));
 -		kfree(apoll);
 -		return false;
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -	trace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,
 -					apoll->poll.events);
 -	return true;
 -}
 -
 -static bool __io_poll_remove_one(struct io_kiocb *req,
 -				 struct io_poll_iocb *poll)
 -{
 -	bool do_complete = false;
 -
 -	spin_lock(&poll->head->lock);
 -	WRITE_ONCE(poll->canceled, true);
 -	if (!list_empty(&poll->wait.entry)) {
 -		list_del_init(&poll->wait.entry);
 -		do_complete = true;
 -	}
 -	spin_unlock(&poll->head->lock);
 -	return do_complete;
 -}
 -
 -static bool io_poll_remove_one(struct io_kiocb *req)
 -{
 -	bool do_complete;
 -
 -	if (req->opcode == IORING_OP_POLL_ADD) {
 -		do_complete = __io_poll_remove_one(req, &req->poll);
 -	} else {
 -		/* non-poll requests have submit ref still */
 -		do_complete = __io_poll_remove_one(req, &req->apoll->poll);
 -		if (do_complete)
 -			io_put_req(req);
 -	}
 -
 -	hash_del(&req->hash_node);
 -
 -	if (do_complete) {
 -		io_cqring_fill_event(req, -ECANCELED);
 -		io_commit_cqring(req->ctx);
 -		req->flags |= REQ_F_COMP_LOCKED;
 -		io_put_req(req);
 -	}
 -
 -	return do_complete;
++		req_set_fail_links(req);
++	io_put_req(req);
++	return 0;
++#else
++	return -EOPNOTSUPP;
++#endif
+ }
+ 
 -static void io_poll_remove_all(struct io_ring_ctx *ctx)
++static int io_recv(struct io_kiocb *req, bool force_nonblock)
+ {
 -	struct hlist_node *tmp;
 -	struct io_kiocb *req;
 -	int i;
++#if defined(CONFIG_NET)
++	struct socket *sock;
++	int ret;
+ 
 -	spin_lock_irq(&ctx->completion_lock);
 -	for (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {
 -		struct hlist_head *list;
++	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
++		return -EINVAL;
+ 
 -		list = &ctx->cancel_hash[i];
 -		hlist_for_each_entry_safe(req, tmp, list, hash_node)
 -			io_poll_remove_one(req);
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
++	sock = sock_from_file(req->file, &ret);
++	if (sock) {
++		struct io_sr_msg *sr = &req->sr_msg;
++		struct msghdr msg;
++		struct iovec iov;
++		unsigned flags;
+ 
 -	io_cqring_ev_posted(ctx);
 -}
++		ret = import_single_range(READ, sr->buf, sr->len, &iov,
++						&msg.msg_iter);
++		if (ret)
++			return ret;
+ 
 -static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)
 -{
 -	struct hlist_head *list;
 -	struct io_kiocb *req;
++		msg.msg_name = NULL;
++		msg.msg_control = NULL;
++		msg.msg_controllen = 0;
++		msg.msg_namelen = 0;
++		msg.msg_iocb = NULL;
++		msg.msg_flags = 0;
+ 
 -	list = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];
 -	hlist_for_each_entry(req, list, hash_node) {
 -		if (sqe_addr != req->user_data)
 -			continue;
 -		if (io_poll_remove_one(req))
 -			return 0;
 -		return -EALREADY;
++		flags = req->sr_msg.msg_flags;
++		if (flags & MSG_DONTWAIT)
++			req->flags |= REQ_F_NOWAIT;
++		else if (force_nonblock)
++			flags |= MSG_DONTWAIT;
++
++		ret = sock_recvmsg(sock, &msg, flags);
++		if (force_nonblock && ret == -EAGAIN)
++			return -EAGAIN;
++		if (ret == -ERESTARTSYS)
++			ret = -EINTR;
+ 	}
+ 
 -	return -ENOENT;
++	io_cqring_add_event(req, ret);
++	if (ret < 0)
++		req_set_fail_links(req);
++	io_put_req(req);
++	return 0;
++#else
++	return -EOPNOTSUPP;
++#endif
+ }
+ 
 -static int io_poll_remove_prep(struct io_kiocb *req,
 -			       const struct io_uring_sqe *sqe)
++
++static int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
++#if defined(CONFIG_NET)
++	struct io_accept *accept = &req->accept;
++
++	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
+ 		return -EINVAL;
 -	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 -	    sqe->poll_events)
++	if (sqe->ioprio || sqe->len || sqe->buf_index)
+ 		return -EINVAL;
+ 
 -	req->poll.addr = READ_ONCE(sqe->addr);
++	accept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
++	accept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));
++	accept->flags = READ_ONCE(sqe->accept_flags);
+ 	return 0;
++#else
++	return -EOPNOTSUPP;
++#endif
+ }
+ 
 -/*
 - * Find a running poll command that matches one specified in sqe->addr,
 - * and remove it if found.
 - */
 -static int io_poll_remove(struct io_kiocb *req)
++#if defined(CONFIG_NET)
++static int __io_accept(struct io_kiocb *req, bool force_nonblock)
+ {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	u64 addr;
++	struct io_accept *accept = &req->accept;
++	unsigned file_flags;
+ 	int ret;
+ 
 -	addr = req->poll.addr;
 -	spin_lock_irq(&ctx->completion_lock);
 -	ret = io_poll_cancel(ctx, addr);
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	io_cqring_add_event(req, ret);
++	file_flags = force_nonblock ? O_NONBLOCK : 0;
++	ret = __sys_accept4_file(req->file, file_flags, accept->addr,
++					accept->addr_len, accept->flags);
++	if (ret == -EAGAIN && force_nonblock)
++		return -EAGAIN;
++	if (ret == -ERESTARTSYS)
++		ret = -EINTR;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
++	io_cqring_add_event(req, ret);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  	io_put_req(req);
  	return 0;
  }
@@@ -1690,74 -3853,44 +2678,130 @@@ static void io_poll_complete(struct io_
  	io_commit_cqring(ctx);
  }
  
 -static void io_poll_task_handler(struct io_kiocb *req, struct io_kiocb **nxt)
 +static void io_poll_complete_work(struct work_struct *work)
  {
 +	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 +	struct io_poll_iocb *poll = &req->poll;
 +	struct poll_table_struct pt = { ._key = poll->events };
  	struct io_ring_ctx *ctx = req->ctx;
 +	__poll_t mask = 0;
  
 -	spin_lock_irq(&ctx->completion_lock);
 -	hash_del(&req->hash_node);
 -	io_poll_complete(req, req->result, 0);
 -	req->flags |= REQ_F_COMP_LOCKED;
 -	io_put_req_find_next(req, nxt);
 -	spin_unlock_irq(&ctx->completion_lock);
 +	if (!READ_ONCE(poll->canceled))
 +		mask = vfs_poll(poll->file, &pt) & poll->events;
  
 -	io_cqring_ev_posted(ctx);
 +	/*
 +	 * Note that ->ki_cancel callers also delete iocb from active_reqs after
 +	 * calling ->ki_cancel.  We need the ctx_lock roundtrip here to
 +	 * synchronize with them.  In the cancellation case the list_del_init
 +	 * itself is not actually needed, but harmless so we keep it in to
 +	 * avoid further branches in the fast path.
 +	 */
 +	spin_lock_irq(&ctx->completion_lock);
 +	if (!mask && !READ_ONCE(poll->canceled)) {
 +		add_wait_queue(poll->head, &poll->wait);
 +		spin_unlock_irq(&ctx->completion_lock);
 +		return;
++<<<<<<< HEAD
++=======
++	__io_accept(req, false);
++	io_put_req(req); /* drop submission reference */
++	if (nxt)
++		io_wq_assign_next(workptr, nxt);
+ }
++#endif
+ 
 -static void io_poll_task_func(struct callback_head *cb)
++static int io_accept(struct io_kiocb *req, bool force_nonblock)
+ {
 -	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
 -	struct io_kiocb *nxt = NULL;
 -
 -	io_poll_task_handler(req, &nxt);
 -	if (nxt) {
 -		struct io_ring_ctx *ctx = nxt->ctx;
++#if defined(CONFIG_NET)
++	int ret;
+ 
 -		mutex_lock(&ctx->uring_lock);
 -		__io_queue_sqe(nxt, NULL);
 -		mutex_unlock(&ctx->uring_lock);
++	ret = __io_accept(req, force_nonblock);
++	if (ret == -EAGAIN && force_nonblock) {
++		req->work.func = io_accept_finish;
++		return -EAGAIN;
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  	}
 +	list_del_init(&req->list);
 +	io_poll_complete(ctx, req, mask);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	io_cqring_ev_posted(ctx);
 +	io_put_req(req);
  }
  
  static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
  			void *key)
  {
 -	struct io_kiocb *req = wait->private;
 -	struct io_poll_iocb *poll = &req->poll;
 +	struct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,
 +							wait);
 +	struct io_kiocb *req = container_of(poll, struct io_kiocb, poll);
 +	struct io_ring_ctx *ctx = req->ctx;
 +	__poll_t mask = key_to_poll(key);
 +	unsigned long flags;
 +
 +	/* for instances that support it check for an event match first: */
 +	if (mask && !(mask & poll->events))
 +		return 0;
 +
 +	list_del_init(&poll->wait.entry);
 +
++<<<<<<< HEAD
 +	if (mask && spin_trylock_irqsave(&ctx->completion_lock, flags)) {
 +		list_del(&req->list);
 +		io_poll_complete(ctx, req, mask);
 +		spin_unlock_irqrestore(&ctx->completion_lock, flags);
++=======
++static int io_connect(struct io_kiocb *req, bool force_nonblock)
++{
++#if defined(CONFIG_NET)
++	struct io_async_ctx __io, *io;
++	unsigned file_flags;
++	int ret;
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
 +
 +		io_cqring_ev_posted(ctx);
 +		io_put_req(req);
 +	} else {
 +		io_queue_async_work(ctx, req);
 +	}
 +
++<<<<<<< HEAD
 +	return 1;
++=======
++	file_flags = force_nonblock ? O_NONBLOCK : 0;
+ 
 -	return __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);
++	ret = __sys_connect_file(req->file, &io->connect.address,
++					req->connect.addr_len, file_flags);
++	if ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {
++		if (req->io)
++			return -EAGAIN;
++		if (io_alloc_async_ctx(req)) {
++			ret = -ENOMEM;
++			goto out;
++		}
++		memcpy(&req->io->connect, &__io.connect, sizeof(__io.connect));
++		return -EAGAIN;
++	}
++	if (ret == -ERESTARTSYS)
++		ret = -EINTR;
++out:
++	if (ret < 0)
++		req_set_fail_links(req);
++	io_cqring_add_event(req, ret);
++	io_put_req(req);
++	return 0;
++#else
++	return -EOPNOTSUPP;
++#endif
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  }
  
 +struct io_poll_table {
 +	struct poll_table_struct pt;
 +	struct io_kiocb *req;
 +	int error;
 +};
 +
  static void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,
  			       struct poll_table_struct *p)
  {
@@@ -1794,126 -3914,742 +2838,778 @@@ static int io_poll_add(struct io_kiocb 
  	events = READ_ONCE(sqe->poll_events);
  	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
  
- 	poll->head = NULL;
- 	poll->done = false;
- 	poll->canceled = false;
 -	/*
 -	 * Don't need a reference here, as we're adding it to the task
 -	 * task_works list. If the task exits, the list is pruned.
 -	 */
 -	req->task = current;
 -	return 0;
 -}
++	poll->head = NULL;
++	poll->done = false;
++	poll->canceled = false;
+ 
++<<<<<<< HEAD
++=======
+ static int io_poll_add(struct io_kiocb *req)
+ {
+ 	struct io_poll_iocb *poll = &req->poll;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_poll_table ipt;
+ 	__poll_t mask;
+ 
+ 	INIT_HLIST_NODE(&req->hash_node);
+ 	INIT_LIST_HEAD(&req->list);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
+ 	ipt.pt._qproc = io_poll_queue_proc;
++	ipt.pt._key = poll->events;
++	ipt.req = req;
++	ipt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */
+ 
 -	mask = __io_arm_poll_handler(req, &req->poll, &ipt, poll->events,
 -					io_poll_wake);
++	/* initialized the list so that we can do list_empty checks */
++	INIT_LIST_HEAD(&poll->wait.entry);
++	init_waitqueue_func_entry(&poll->wait, io_poll_wake);
++
++	INIT_LIST_HEAD(&req->list);
++
++	mask = vfs_poll(poll->file, &ipt.pt) & poll->events;
+ 
++	spin_lock_irq(&ctx->completion_lock);
++	if (likely(poll->head)) {
++		spin_lock(&poll->head->lock);
++		if (unlikely(list_empty(&poll->wait.entry))) {
++			if (ipt.error)
++				cancel = true;
++			ipt.error = 0;
++			mask = 0;
++		}
++		if (mask || ipt.error)
++			list_del_init(&poll->wait.entry);
++		else if (cancel)
++			WRITE_ONCE(poll->canceled, true);
++		else if (!poll->done) /* actually waiting for an event */
++			list_add_tail(&req->list, &ctx->cancel_list);
++		spin_unlock(&poll->head->lock);
++	}
+ 	if (mask) { /* no async, we'd stolen it */
+ 		ipt.error = 0;
 -		io_poll_complete(req, mask, 0);
++		io_poll_complete(ctx, req, mask);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	if (mask) {
+ 		io_cqring_ev_posted(ctx);
+ 		io_put_req(req);
+ 	}
+ 	return ipt.error;
+ }
+ 
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
++static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
++			const struct io_uring_sqe *sqe)
+ {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
++	struct io_uring_sqe *sqe_copy;
+ 
++<<<<<<< HEAD
++	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
++=======
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * We could be racing with timeout deletion. If the list is empty,
+ 	 * then timeout lookup already found it and will be handling it.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		struct io_kiocb *prev;
+ 
+ 		/*
+ 		 * Adjust the reqs sequence before the current one because it
+ 		 * will consume a slot in the cq_ring and the cq_tail
+ 		 * pointer will be increased, otherwise other timeout reqs may
+ 		 * return in advance without waiting for enough wait_nr.
+ 		 */
+ 		prev = req;
+ 		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 			prev->sequence++;
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	io_cqring_fill_event(req, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
+ {
+ 	struct io_kiocb *req;
+ 	int ret = -ENOENT;
+ 
+ 	list_for_each_entry(req, &ctx->timeout_list, list) {
+ 		if (user_data == req->user_data) {
+ 			list_del_init(&req->list);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (ret == -ENOENT)
+ 		return ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
+ 	if (ret == -1)
+ 		return -EALREADY;
+ 
+ 	req_set_fail_links(req);
+ 	io_cqring_fill_event(req, -ECANCELED);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_remove_prep(struct io_kiocb *req,
+ 				  const struct io_uring_sqe *sqe)
+ {
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
+ 		return -EINVAL;
+ 
+ 	req->timeout.addr = READ_ONCE(sqe->addr);
+ 	req->timeout.flags = READ_ONCE(sqe->timeout_flags);
+ 	if (req->timeout.flags)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Remove or update an existing timeout command
+  */
+ static int io_timeout_remove(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	ret = io_timeout_cancel(ctx, req->timeout.addr);
+ 
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	io_cqring_ev_posted(ctx);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			   bool is_timeout_link)
+ {
+ 	struct io_timeout_data *data;
+ 	unsigned flags;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
+ 		return -EINVAL;
+ 	if (sqe->off && is_timeout_link)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 
+ 	req->timeout.count = READ_ONCE(sqe->off);
+ 
+ 	if (!req->io && io_alloc_async_ctx(req))
+ 		return -ENOMEM;
+ 
+ 	data = &req->io->timeout;
+ 	data->req = req;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	if (flags & IORING_TIMEOUT_ABS)
+ 		data->mode = HRTIMER_MODE_ABS;
+ 	else
+ 		data->mode = HRTIMER_MODE_REL;
+ 
+ 	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
+ 	return 0;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_timeout_data *data;
+ 	struct list_head *entry;
+ 	unsigned span = 0;
+ 
+ 	data = &req->io->timeout;
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied. If it isn't set, then this is
+ 	 * a pure timeout request, sequence isn't used.
+ 	 */
+ 	count = req->timeout.count;
+ 	if (!count) {
+ 		req->flags |= REQ_F_TIMEOUT_NOSEQ;
+ 		spin_lock_irq(&ctx->completion_lock);
+ 		entry = ctx->timeout_list.prev;
+ 		goto add;
+ 	}
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	data->seq_offset = count;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 		u32 nxt_offset = nxt->io->timeout.seq_offset;
+ 
+ 		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
+ 			continue;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt_offset + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ add:
+ 	list_add(&req->list, entry);
+ 	data->timer.function = io_timeout_fn;
+ 	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	return 0;
+ }
+ 
+ static bool io_cancel_cb(struct io_wq_work *work, void *data)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	return req->user_data == (unsigned long) data;
+ }
+ 
+ static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
+ {
+ 	enum io_wq_cancel cancel_ret;
+ 	int ret = 0;
+ 
+ 	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
+ 	switch (cancel_ret) {
+ 	case IO_WQ_CANCEL_OK:
+ 		ret = 0;
+ 		break;
+ 	case IO_WQ_CANCEL_RUNNING:
+ 		ret = -EALREADY;
+ 		break;
+ 	case IO_WQ_CANCEL_NOTFOUND:
+ 		ret = -ENOENT;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
+ 				     struct io_kiocb *req, __u64 sqe_addr,
+ 				     int success_ret)
+ {
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
+ 	if (ret != -ENOENT) {
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		goto done;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	ret = io_timeout_cancel(ctx, sqe_addr);
+ 	if (ret != -ENOENT)
+ 		goto done;
+ 	ret = io_poll_cancel(ctx, sqe_addr);
+ done:
+ 	if (!ret)
+ 		ret = success_ret;
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ }
+ 
+ static int io_async_cancel_prep(struct io_kiocb *req,
+ 				const struct io_uring_sqe *sqe)
+ {
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
+ 	    sqe->cancel_flags)
+ 		return -EINVAL;
+ 
+ 	req->cancel.addr = READ_ONCE(sqe->addr);
+ 	return 0;
+ }
+ 
+ static int io_async_cancel(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	io_async_find_and_cancel(ctx, req, req->cancel.addr, 0);
+ 	return 0;
+ }
+ 
+ static int io_files_update_prep(struct io_kiocb *req,
+ 				const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->flags || sqe->ioprio || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	req->files_update.offset = READ_ONCE(sqe->off);
+ 	req->files_update.nr_args = READ_ONCE(sqe->len);
+ 	if (!req->files_update.nr_args)
+ 		return -EINVAL;
+ 	req->files_update.arg = READ_ONCE(sqe->addr);
+ 	return 0;
+ }
+ 
+ static int io_files_update(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_uring_files_update up;
+ 	int ret;
  
- 	ipt.pt._qproc = io_poll_queue_proc;
- 	ipt.pt._key = poll->events;
- 	ipt.req = req;
- 	ipt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */
+ 	if (force_nonblock)
+ 		return -EAGAIN;
  
- 	/* initialized the list so that we can do list_empty checks */
- 	INIT_LIST_HEAD(&poll->wait.entry);
- 	init_waitqueue_func_entry(&poll->wait, io_poll_wake);
+ 	up.offset = req->files_update.offset;
+ 	up.fds = req->files_update.arg;
  
- 	INIT_LIST_HEAD(&req->list);
+ 	mutex_lock(&ctx->uring_lock);
+ 	ret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);
+ 	mutex_unlock(&ctx->uring_lock);
  
- 	mask = vfs_poll(poll->file, &ipt.pt) & poll->events;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
  
- 	spin_lock_irq(&ctx->completion_lock);
- 	if (likely(poll->head)) {
- 		spin_lock(&poll->head->lock);
- 		if (unlikely(list_empty(&poll->wait.entry))) {
- 			if (ipt.error)
- 				cancel = true;
- 			ipt.error = 0;
- 			mask = 0;
- 		}
- 		if (mask || ipt.error)
- 			list_del_init(&poll->wait.entry);
- 		else if (cancel)
- 			WRITE_ONCE(poll->canceled, true);
- 		else if (!poll->done) /* actually waiting for an event */
- 			list_add_tail(&req->list, &ctx->cancel_list);
- 		spin_unlock(&poll->head->lock);
- 	}
- 	if (mask) { /* no async, we'd stolen it */
- 		ipt.error = 0;
- 		io_poll_complete(ctx, req, mask);
+ static int io_req_defer_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	ssize_t ret = 0;
+ 
+ 	if (io_op_defs[req->opcode].file_table) {
+ 		ret = io_grab_files(req);
+ 		if (unlikely(ret))
+ 			return ret;
  	}
- 	spin_unlock_irq(&ctx->completion_lock);
  
- 	if (mask) {
- 		io_cqring_ev_posted(ctx);
- 		io_put_req(req);
+ 	io_req_work_grab_env(req, &io_op_defs[req->opcode]);
+ 
+ 	switch (req->opcode) {
+ 	case IORING_OP_NOP:
+ 		break;
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 		ret = io_read_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 	case IORING_OP_WRITE:
+ 		ret = io_write_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_POLL_ADD:
+ 		ret = io_poll_add_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_POLL_REMOVE:
+ 		ret = io_poll_remove_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FSYNC:
+ 		ret = io_prep_fsync(req, sqe);
+ 		break;
+ 	case IORING_OP_SYNC_FILE_RANGE:
+ 		ret = io_prep_sfr(req, sqe);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 	case IORING_OP_SEND:
+ 		ret = io_sendmsg_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_RECVMSG:
+ 	case IORING_OP_RECV:
+ 		ret = io_recvmsg_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_CONNECT:
+ 		ret = io_connect_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_TIMEOUT:
+ 		ret = io_timeout_prep(req, sqe, false);
+ 		break;
+ 	case IORING_OP_TIMEOUT_REMOVE:
+ 		ret = io_timeout_remove_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_ASYNC_CANCEL:
+ 		ret = io_async_cancel_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_LINK_TIMEOUT:
+ 		ret = io_timeout_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_ACCEPT:
+ 		ret = io_accept_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FALLOCATE:
+ 		ret = io_fallocate_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_OPENAT:
+ 		ret = io_openat_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_CLOSE:
+ 		ret = io_close_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FILES_UPDATE:
+ 		ret = io_files_update_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_STATX:
+ 		ret = io_statx_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FADVISE:
+ 		ret = io_fadvise_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_MADVISE:
+ 		ret = io_madvise_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_OPENAT2:
+ 		ret = io_openat2_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_EPOLL_CTL:
+ 		ret = io_epoll_ctl_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_SPLICE:
+ 		ret = io_splice_prep(req, sqe);
+ 		break;
+ 	default:
+ 		printk_once(KERN_WARNING "io_uring: unhandled opcode %d\n",
+ 				req->opcode);
+ 		ret = -EINVAL;
+ 		break;
  	}
- 	return ipt.error;
+ 
+ 	return ret;
  }
  
- static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
- 			const struct io_uring_sqe *sqe)
+ static int io_req_defer(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
- 	struct io_uring_sqe *sqe_copy;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
  
- 	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
+ 	/* Still need defer if there is pending req in defer list. */
+ 	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  		return 0;
  
 -	if (!req->io && io_alloc_async_ctx(req))
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
  		return -EAGAIN;
  
 -	ret = io_req_defer_prep(req, sqe);
 -	if (ret < 0)
 -		return ret;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list)) {
 -		spin_unlock_irq(&ctx->completion_lock);
 -		return 0;
 -	}
 -
 -	trace_io_uring_defer(ctx, req, req->user_data);
 -	list_add_tail(&req->list, &ctx->defer_list);
 -	spin_unlock_irq(&ctx->completion_lock);
 -	return -EIOCBQUEUED;
 -}
 -
 -static void io_cleanup_req(struct io_kiocb *req)
 -{
 -	struct io_async_ctx *io = req->io;
 -
 -	switch (req->opcode) {
 -	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -	case IORING_OP_READ:
 -	case IORING_OP_WRITEV:
 -	case IORING_OP_WRITE_FIXED:
 -	case IORING_OP_WRITE:
 -		if (io->rw.iov != io->rw.fast_iov)
 -			kfree(io->rw.iov);
 -		break;
 -	case IORING_OP_SENDMSG:
 -	case IORING_OP_RECVMSG:
 -		if (io->msg.iov != io->msg.fast_iov)
 -			kfree(io->msg.iov);
 -		break;
 -	case IORING_OP_OPENAT:
 -	case IORING_OP_OPENAT2:
 -	case IORING_OP_STATX:
 -		putname(req->open.filename);
 -		break;
 -	case IORING_OP_SPLICE:
 -		io_put_file(req, req->splice.file_in,
 -			    (req->splice.flags & SPLICE_F_FD_IN_FIXED));
 -		break;
 +	spin_lock_irq(&ctx->completion_lock);
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list)) {
 +		spin_unlock_irq(&ctx->completion_lock);
 +		kfree(sqe_copy);
 +		return 0;
  	}
  
 +	memcpy(sqe_copy, sqe, sizeof(*sqe_copy));
 +	req->submit.sqe = sqe_copy;
 +
 +	INIT_WORK(&req->work, io_sq_wq_submit_work);
 +	list_add_tail(&req->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +	return -EIOCBQUEUED;
 +}
 +
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
 +{
 +	int ret, opcode;
 +
 +	req->user_data = READ_ONCE(s->sqe->user_data);
 +
++<<<<<<< HEAD
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
++=======
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ }
+ 
+ static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			bool force_nonblock)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	switch (req->opcode) {
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  	case IORING_OP_NOP:
 -		ret = io_nop(req);
 +		ret = io_nop(req, req->user_data);
  		break;
  	case IORING_OP_READV:
++<<<<<<< HEAD
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
++=======
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 		if (sqe) {
+ 			ret = io_read_prep(req, sqe, force_nonblock);
+ 			if (ret < 0)
+ 				break;
+ 		}
+ 		ret = io_read(req, force_nonblock);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  		break;
  	case IORING_OP_WRITEV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_READ_FIXED:
 +		ret = io_read(req, s, force_nonblock);
 +		break;
  	case IORING_OP_WRITE_FIXED:
++<<<<<<< HEAD
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_FSYNC:
 +		ret = io_fsync(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_POLL_ADD:
 +		ret = io_poll_add(req, s->sqe);
++=======
+ 	case IORING_OP_WRITE:
+ 		if (sqe) {
+ 			ret = io_write_prep(req, sqe, force_nonblock);
+ 			if (ret < 0)
+ 				break;
+ 		}
+ 		ret = io_write(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_FSYNC:
+ 		if (sqe) {
+ 			ret = io_prep_fsync(req, sqe);
+ 			if (ret < 0)
+ 				break;
+ 		}
+ 		ret = io_fsync(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_POLL_ADD:
+ 		if (sqe) {
+ 			ret = io_poll_add_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_poll_add(req);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  		break;
  	case IORING_OP_POLL_REMOVE:
 -		if (sqe) {
 -			ret = io_poll_remove_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_poll_remove(req);
 +		ret = io_poll_remove(req, s->sqe);
  		break;
  	case IORING_OP_SYNC_FILE_RANGE:
++<<<<<<< HEAD
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_SENDMSG:
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_RECVMSG:
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
++=======
+ 		if (sqe) {
+ 			ret = io_prep_sfr(req, sqe);
+ 			if (ret < 0)
+ 				break;
+ 		}
+ 		ret = io_sync_file_range(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 	case IORING_OP_SEND:
+ 		if (sqe) {
+ 			ret = io_sendmsg_prep(req, sqe);
+ 			if (ret < 0)
+ 				break;
+ 		}
+ 		if (req->opcode == IORING_OP_SENDMSG)
+ 			ret = io_sendmsg(req, force_nonblock);
+ 		else
+ 			ret = io_send(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_RECVMSG:
+ 	case IORING_OP_RECV:
+ 		if (sqe) {
+ 			ret = io_recvmsg_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		if (req->opcode == IORING_OP_RECVMSG)
+ 			ret = io_recvmsg(req, force_nonblock);
+ 		else
+ 			ret = io_recv(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_TIMEOUT:
+ 		if (sqe) {
+ 			ret = io_timeout_prep(req, sqe, false);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_timeout(req);
+ 		break;
+ 	case IORING_OP_TIMEOUT_REMOVE:
+ 		if (sqe) {
+ 			ret = io_timeout_remove_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_timeout_remove(req);
+ 		break;
+ 	case IORING_OP_ACCEPT:
+ 		if (sqe) {
+ 			ret = io_accept_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_accept(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_CONNECT:
+ 		if (sqe) {
+ 			ret = io_connect_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_connect(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_ASYNC_CANCEL:
+ 		if (sqe) {
+ 			ret = io_async_cancel_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_async_cancel(req);
+ 		break;
+ 	case IORING_OP_FALLOCATE:
+ 		if (sqe) {
+ 			ret = io_fallocate_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_fallocate(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_OPENAT:
+ 		if (sqe) {
+ 			ret = io_openat_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_openat(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_CLOSE:
+ 		if (sqe) {
+ 			ret = io_close_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_close(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_FILES_UPDATE:
+ 		if (sqe) {
+ 			ret = io_files_update_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_files_update(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_STATX:
+ 		if (sqe) {
+ 			ret = io_statx_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_statx(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_FADVISE:
+ 		if (sqe) {
+ 			ret = io_fadvise_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_fadvise(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_MADVISE:
+ 		if (sqe) {
+ 			ret = io_madvise_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_madvise(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_OPENAT2:
+ 		if (sqe) {
+ 			ret = io_openat2_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_openat2(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_EPOLL_CTL:
+ 		if (sqe) {
+ 			ret = io_epoll_ctl_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_epoll_ctl(req, force_nonblock);
+ 		break;
+ 	case IORING_OP_SPLICE:
+ 		if (sqe) {
+ 			ret = io_splice_prep(req, sqe);
+ 			if (ret < 0)
+ 				break;
+ 		}
+ 		ret = io_splice(req, force_nonblock);
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  		break;
  	default:
  		ret = -EINVAL;
@@@ -1938,195 -4678,90 +3634,253 @@@
  	return 0;
  }
  
 -static void io_wq_submit_work(struct io_wq_work **workptr)
 +static struct async_list *io_async_list_from_sqe(struct io_ring_ctx *ctx,
 +						 const struct io_uring_sqe *sqe)
 +{
 +	switch (sqe->opcode) {
 +	case IORING_OP_READV:
 +	case IORING_OP_READ_FIXED:
 +		return &ctx->pending_async[READ];
 +	case IORING_OP_WRITEV:
 +	case IORING_OP_WRITE_FIXED:
 +		return &ctx->pending_async[WRITE];
 +	default:
 +		return NULL;
 +	}
 +}
 +
 +static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 +{
 +	u8 opcode = READ_ONCE(sqe->opcode);
 +
 +	return !(opcode == IORING_OP_READ_FIXED ||
 +		 opcode == IORING_OP_WRITE_FIXED);
 +}
 +
 +static void io_sq_wq_submit_work(struct work_struct *work)
  {
 -	struct io_wq_work *work = *workptr;
  	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
++<<<<<<< HEAD
++=======
+ 	struct io_kiocb *nxt = NULL;
+ 	int ret = 0;
+ 
+ 	/* if NO_CANCEL is set, we must still run the work */
+ 	if ((work->flags & (IO_WQ_WORK_CANCEL|IO_WQ_WORK_NO_CANCEL)) ==
+ 				IO_WQ_WORK_CANCEL) {
+ 		ret = -ECANCELED;
+ 	}
+ 
+ 	if (!ret) {
+ 		do {
+ 			ret = io_issue_sqe(req, NULL, false);
+ 			/*
+ 			 * We can get EAGAIN for polled IO even though we're
+ 			 * forcing a sync submission from here, since we can't
+ 			 * wait for request slots on the block side.
+ 			 */
+ 			if (ret != -EAGAIN)
+ 				break;
+ 			cond_resched();
+ 		} while (1);
+ 	}
+ 
+ 	if (ret) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, ret);
+ 		io_put_req(req);
+ 	}
+ 
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_req_needs_file(struct io_kiocb *req, int fd)
+ {
+ 	if (!io_op_defs[req->opcode].needs_file)
+ 		return 0;
+ 	if ((fd == -1 || fd == AT_FDCWD) && io_op_defs[req->opcode].fd_non_neg)
+ 		return 0;
+ 	return 1;
+ }
+ 
+ static inline struct file *io_file_from_index(struct io_ring_ctx *ctx,
+ 					      int index)
+ {
+ 	struct fixed_file_table *table;
+ 
+ 	table = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];
+ 	return table->files[index & IORING_FILE_TABLE_MASK];;
+ }
+ 
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 			int fd, struct file **out_file, bool fixed)
+ {
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  	struct io_ring_ctx *ctx = req->ctx;
 -	struct file *file;
 +	struct mm_struct *cur_mm = NULL;
 +	struct async_list *async_list;
 +	LIST_HEAD(req_list);
 +	mm_segment_t old_fs;
 +	int ret;
  
 -	if (fixed) {
 -		if (unlikely(!ctx->file_data ||
 -		    (unsigned) fd >= ctx->nr_user_files))
 -			return -EBADF;
 -		fd = array_index_nospec(fd, ctx->nr_user_files);
 -		file = io_file_from_index(ctx, fd);
 -		if (!file)
 -			return -EBADF;
 -		percpu_ref_get(&ctx->file_data->refs);
 -	} else {
 -		trace_io_uring_file_get(ctx, fd);
 -		file = __io_file_get(state, fd);
 -		if (unlikely(!file))
 -			return -EBADF;
 +	async_list = io_async_list_from_sqe(ctx, req->submit.sqe);
 +restart:
 +	do {
 +		struct sqe_submit *s = &req->submit;
 +		const struct io_uring_sqe *sqe = s->sqe;
 +		unsigned int flags = req->flags;
 +
 +		/* Ensure we clear previously set non-block flag */
 +		req->rw.ki_flags &= ~IOCB_NOWAIT;
 +
 +		ret = 0;
 +		if (io_sqe_needs_user(sqe) && !cur_mm) {
 +			if (!mmget_not_zero(ctx->sqo_mm)) {
 +				ret = -EFAULT;
 +			} else {
 +				cur_mm = ctx->sqo_mm;
 +				use_mm(cur_mm);
 +				old_fs = get_fs();
 +				set_fs(USER_DS);
 +			}
 +		}
 +
 +		if (!ret) {
 +			s->has_user = cur_mm != NULL;
 +			s->needs_lock = true;
 +			do {
 +				ret = __io_submit_sqe(ctx, req, s, false);
 +				/*
 +				 * We can get EAGAIN for polled IO even though
 +				 * we're forcing a sync submission from here,
 +				 * since we can't wait for request slots on the
 +				 * block side.
 +				 */
 +				if (ret != -EAGAIN)
 +					break;
 +				cond_resched();
 +			} while (1);
 +		}
 +
 +		/* drop submission reference */
 +		io_put_req(req);
 +
 +		if (ret) {
 +			io_cqring_add_event(ctx, sqe->user_data, ret);
 +			io_put_req(req);
 +		}
 +
 +		/* async context always use a copy of the sqe */
 +		kfree(sqe);
 +
 +		/* req from defer and link list needn't decrease async cnt */
 +		if (flags & (REQ_F_IO_DRAINED | REQ_F_LINK_DONE))
 +			goto out;
 +
 +		if (!async_list)
 +			break;
 +		if (!list_empty(&req_list)) {
 +			req = list_first_entry(&req_list, struct io_kiocb,
 +						list);
 +			list_del(&req->list);
 +			continue;
 +		}
 +		if (list_empty(&async_list->list))
 +			break;
 +
 +		req = NULL;
 +		spin_lock(&async_list->lock);
 +		if (list_empty(&async_list->list)) {
 +			spin_unlock(&async_list->lock);
 +			break;
 +		}
 +		list_splice_init(&async_list->list, &req_list);
 +		spin_unlock(&async_list->lock);
 +
 +		req = list_first_entry(&req_list, struct io_kiocb, list);
 +		list_del(&req->list);
 +	} while (req);
 +
 +	/*
 +	 * Rare case of racing with a submitter. If we find the count has
 +	 * dropped to zero AND we have pending work items, then restart
 +	 * the processing. This is a tiny race window.
 +	 */
 +	if (async_list) {
 +		ret = atomic_dec_return(&async_list->cnt);
 +		while (!ret && !list_empty(&async_list->list)) {
 +			spin_lock(&async_list->lock);
 +			atomic_inc(&async_list->cnt);
 +			list_splice_init(&async_list->list, &req_list);
 +			spin_unlock(&async_list->lock);
 +
 +			if (!list_empty(&req_list)) {
 +				req = list_first_entry(&req_list,
 +							struct io_kiocb, list);
 +				list_del(&req->list);
 +				goto restart;
 +			}
 +			ret = atomic_dec_return(&async_list->cnt);
 +		}
  	}
  
 -	*out_file = file;
 -	return 0;
 +out:
 +	if (cur_mm) {
 +		set_fs(old_fs);
 +		unuse_mm(cur_mm);
 +		mmput(cur_mm);
 +	}
  }
  
 -static int io_req_set_file(struct io_submit_state *state, struct io_kiocb *req,
 -			   const struct io_uring_sqe *sqe)
 +/*
 + * See if we can piggy back onto previously submitted work, that is still
 + * running. We currently only allow this if the new request is sequential
 + * to the previous one we punted.
 + */
 +static bool io_add_to_prev_work(struct async_list *list, struct io_kiocb *req)
 +{
 +	bool ret;
 +
 +	if (!list)
 +		return false;
 +	if (!(req->flags & REQ_F_SEQ_PREV))
 +		return false;
 +	if (!atomic_read(&list->cnt))
 +		return false;
 +
 +	ret = true;
 +	spin_lock(&list->lock);
 +	list_add_tail(&req->list, &list->list);
 +	/*
 +	 * Ensure we see a simultaneous modification from io_sq_wq_submit_work()
 +	 */
 +	smp_mb();
 +	if (!atomic_read(&list->cnt)) {
 +		list_del_init(&req->list);
 +		ret = false;
 +	}
 +	spin_unlock(&list->lock);
 +	return ret;
 +}
 +
 +static bool io_op_needs_file(const struct io_uring_sqe *sqe)
 +{
 +	int op = READ_ONCE(sqe->opcode);
 +
 +	switch (op) {
 +	case IORING_OP_NOP:
 +	case IORING_OP_POLL_REMOVE:
 +	case IORING_OP_TIMEOUT:
 +		return false;
 +	default:
 +		return true;
 +	}
 +}
 +
 +static int io_req_set_file(struct io_ring_ctx *ctx, const struct sqe_submit *s,
 +			   struct io_submit_state *state, struct io_kiocb *req)
  {
  	unsigned flags;
  	int fd;
@@@ -2211,8 -4810,163 +3965,168 @@@ static int __io_queue_sqe(struct io_rin
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
++=======
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_timeout_data *data = container_of(timer,
+ 						struct io_timeout_data, timer);
+ 	struct io_kiocb *req = data->req;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->link_list)) {
+ 		prev = list_entry(req->link_list.prev, struct io_kiocb,
+ 				  link_list);
+ 		if (refcount_inc_not_zero(&prev->refs)) {
+ 			list_del_init(&req->link_list);
+ 			prev->flags &= ~REQ_F_LINK_TIMEOUT;
+ 		} else
+ 			prev = NULL;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		req_set_fail_links(prev);
+ 		io_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);
+ 		io_put_req(prev);
+ 	} else {
+ 		io_cqring_add_event(req, -ETIME);
+ 		io_put_req(req);
+ 	}
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static void io_queue_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	/*
+ 	 * If the list is now empty, then our linked request finished before
+ 	 * we got a chance to setup the timer
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (!list_empty(&req->link_list)) {
+ 		struct io_timeout_data *data = &req->io->timeout;
+ 
+ 		data->timer.function = io_link_timeout_fn;
+ 		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
+ 				data->mode);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	/* drop submission reference */
+ 	io_put_req(req);
+ }
+ 
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 	/* for polled retry, if flag is set, we already went through here */
+ 	if (req->flags & REQ_F_POLLED)
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb,
+ 					link_list);
+ 	if (!nxt || nxt->opcode != IORING_OP_LINK_TIMEOUT)
+ 		return NULL;
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 	return nxt;
+ }
+ 
+ static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_kiocb *linked_timeout;
+ 	struct io_kiocb *nxt;
+ 	const struct cred *old_creds = NULL;
+ 	int ret;
+ 
+ again:
+ 	linked_timeout = io_prep_linked_timeout(req);
+ 
+ 	if (req->work.creds && req->work.creds != current_cred()) {
+ 		if (old_creds)
+ 			revert_creds(old_creds);
+ 		if (old_creds == req->work.creds)
+ 			old_creds = NULL; /* restored original creds */
+ 		else
+ 			old_creds = override_creds(req->work.creds);
+ 	}
+ 
+ 	ret = io_issue_sqe(req, sqe, true);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		if (io_arm_poll_handler(req)) {
+ 			if (linked_timeout)
+ 				io_queue_linked_timeout(linked_timeout);
+ 			goto exit;
+ 		}
+ punt:
+ 		if (io_op_defs[req->opcode].file_table) {
+ 			ret = io_grab_files(req);
+ 			if (ret)
+ 				goto err;
+ 		}
+ 
+ 		/*
+ 		 * Queued up for async execution, worker will release
+ 		 * submit reference when the iocb is actually submitted.
+ 		 */
+ 		io_queue_async_work(req);
+ 		goto exit;
+ 	}
+ 
+ err:
+ 	nxt = NULL;
+ 	/* drop submission reference */
+ 	io_put_req_find_next(req, &nxt);
+ 
+ 	if (linked_timeout) {
+ 		if (!ret)
+ 			io_queue_linked_timeout(linked_timeout);
+ 		else
+ 			io_put_req(linked_timeout);
+ 	}
+ 
+ 	/* and drop final reference, if we failed */
+ 	if (ret) {
+ 		io_cqring_add_event(req, ret);
+ 		req_set_fail_links(req);
+ 		io_put_req(req);
+ 	}
+ 	if (nxt) {
+ 		req = nxt;
+ 
+ 		if (req->flags & REQ_F_FORCE_ASYNC)
+ 			goto punt;
+ 		goto again;
+ 	}
+ exit:
+ 	if (old_creds)
+ 		revert_creds(old_creds);
+ }
+ 
+ static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++>>>>>>> 014db0073cc6 (io_uring: remove @nxt from handlers)
  {
  	int ret;
  
* Unmerged path fs/io_uring.c
