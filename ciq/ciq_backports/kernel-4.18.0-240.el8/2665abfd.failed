io_uring: add support for linked SQE timeouts

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 2665abfd757fb35a241c6f0b1ebf620e3ffb36fb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/2665abfd.failed

While we have support for generic timeouts, we don't have a way to tie
a timeout to a specific SQE. The generic timeouts simply trigger wakeups
on the CQ ring.

This adds support for IORING_OP_LINK_TIMEOUT. This command is only valid
as a link to a previous command. The timeout specific can be either
relative or absolute, following the same rules as IORING_OP_TIMEOUT. If
the timeout triggers before the dependent command completes, it will
attempt to cancel that command. Likewise, if the dependent command
completes before the timeout triggers, it will cancel the timeout.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 2665abfd757fb35a241c6f0b1ebf620e3ffb36fb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index fca9cdc96d77,eadd19ab39a8..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -334,9 -329,13 +334,13 @@@ struct io_kiocb 
  #define REQ_F_IO_DRAIN		16	/* drain existing IO first */
  #define REQ_F_IO_DRAINED	32	/* drain done */
  #define REQ_F_LINK		64	/* linked sqes */
++<<<<<<< HEAD
 +#define REQ_F_LINK_DONE		128	/* linked sqes done */
++=======
+ #define REQ_F_LINK_TIMEOUT	128	/* has linked timeout */
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
  #define REQ_F_FAIL_LINK		256	/* fail rest of links */
  #define REQ_F_SHADOW_DRAIN	512	/* link-drain shadow req */
 -#define REQ_F_TIMEOUT		1024	/* timeout request */
 -#define REQ_F_ISREG		2048	/* regular file */
 -#define REQ_F_MUST_PUNT		4096	/* must be punted even for NONBLOCK */
 -#define REQ_F_INFLIGHT		8192	/* on inflight list */
  	u64			user_data;
  	u32			result;
  	u32			sequence;
@@@ -367,8 -368,11 +371,9 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
 -static void io_wq_submit_work(struct io_wq_work **workptr);
 -static void io_cqring_fill_event(struct io_ring_ctx *ctx, u64 ki_user_data,
 -				 long res);
 +static void io_sq_wq_submit_work(struct work_struct *work);
  static void __io_free_req(struct io_kiocb *req);
+ static void io_put_req(struct io_kiocb *req, struct io_kiocb **nxtptr);
  
  static struct kmem_cache *req_cachep;
  
@@@ -632,9 -714,28 +637,32 @@@ static void __io_free_req(struct io_kio
  	kmem_cache_free(req_cachep, req);
  }
  
++<<<<<<< HEAD
 +static void io_req_link_next(struct io_kiocb *req)
++=======
+ static bool io_link_cancel_timeout(struct io_ring_ctx *ctx,
+ 				   struct io_kiocb *req)
  {
+ 	int ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->timeout.timer);
+ 	if (ret != -1) {
+ 		io_cqring_fill_event(ctx, req->user_data, -ECANCELED);
+ 		io_commit_cqring(ctx);
+ 		req->flags &= ~REQ_F_LINK;
+ 		__io_free_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_req_link_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
  	struct io_kiocb *nxt;
+ 	bool wake_ev = false;
  
  	/*
  	 * The list should never be empty when we are called here. But could
@@@ -650,10 -751,27 +678,33 @@@
  			nxt->flags |= REQ_F_LINK;
  		}
  
++<<<<<<< HEAD
 +		nxt->flags |= REQ_F_LINK_DONE;
 +		INIT_WORK(&nxt->work, io_sq_wq_submit_work);
 +		io_queue_async_work(req->ctx, nxt);
++=======
+ 		/*
+ 		 * If we're in async work, we can continue processing the chain
+ 		 * in this context instead of having to queue up new async work.
+ 		 */
+ 		if (req->flags & REQ_F_LINK_TIMEOUT) {
+ 			wake_ev = io_link_cancel_timeout(ctx, nxt);
+ 
+ 			/* we dropped this link, get next */
+ 			nxt = list_first_entry_or_null(&req->link_list,
+ 							struct io_kiocb, list);
+ 		} else if (nxtptr && current_work()) {
+ 			*nxtptr = nxt;
+ 			break;
+ 		} else {
+ 			io_queue_async_work(req->ctx, nxt);
+ 			break;
+ 		}
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
  	}
+ 
+ 	if (wake_ev)
+ 		io_cqring_ev_posted(ctx);
  }
  
  /*
@@@ -665,26 -787,53 +720,66 @@@ static void io_fail_links(struct io_kio
  
  	while (!list_empty(&req->link_list)) {
  		link = list_first_entry(&req->link_list, struct io_kiocb, list);
- 		list_del(&link->list);
+ 		list_del_init(&link->list);
  
++<<<<<<< HEAD
 +		io_cqring_add_event(req->ctx, link->user_data, -ECANCELED);
 +		__io_free_req(link);
++=======
+ 		trace_io_uring_fail_link(req, link);
+ 
+ 		if ((req->flags & REQ_F_LINK_TIMEOUT) &&
+ 		    link->submit.sqe->opcode == IORING_OP_LINK_TIMEOUT) {
+ 			io_link_cancel_timeout(ctx, link);
+ 		} else {
+ 			io_cqring_fill_event(ctx, link->user_data, -ECANCELED);
+ 			__io_free_req(link);
+ 		}
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
  	}
+ 
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
  }
  
 -static void io_free_req(struct io_kiocb *req, struct io_kiocb **nxt)
 +static void io_free_req(struct io_kiocb *req)
  {
+ 	if (likely(!(req->flags & REQ_F_LINK))) {
+ 		__io_free_req(req);
+ 		return;
+ 	}
+ 
  	/*
  	 * If LINK is set, we have dependent requests in this chain. If we
  	 * didn't fail this request, queue the first one up, moving any other
  	 * dependencies to the next request. In case of failure, fail the rest
  	 * of the chain.
  	 */
++<<<<<<< HEAD
 +	if (req->flags & REQ_F_LINK) {
 +		if (req->flags & REQ_F_FAIL_LINK)
 +			io_fail_links(req);
 +		else
 +			io_req_link_next(req);
++=======
+ 	if (req->flags & REQ_F_FAIL_LINK) {
+ 		io_fail_links(req);
+ 	} else if (req->flags & REQ_F_LINK_TIMEOUT) {
+ 		struct io_ring_ctx *ctx = req->ctx;
+ 		unsigned long flags;
+ 
+ 		/*
+ 		 * If this is a timeout link, we could be racing with the
+ 		 * timeout timer. Grab the completion lock for this case to
+ 		 * protection against that.
+ 		 */
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		io_req_link_next(req, nxt);
+ 		spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	} else {
+ 		io_req_link_next(req, nxt);
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
  	}
  
  	__io_free_req(req);
@@@ -2162,13 -2486,145 +2257,155 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
++=======
+ static int io_grab_files(struct io_ring_ctx *ctx, struct io_kiocb *req)
+ {
+ 	int ret = -EBADF;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irq(&ctx->inflight_lock);
+ 	/*
+ 	 * We use the f_ops->flush() handler to ensure that we can flush
+ 	 * out work accessing these files if the fd is closed. Check if
+ 	 * the fd has changed since we started down this path, and disallow
+ 	 * this operation if it has.
+ 	 */
+ 	if (fcheck(req->submit.ring_fd) == req->submit.ring_file) {
+ 		list_add(&req->inflight_entry, &ctx->inflight_list);
+ 		req->flags |= REQ_F_INFLIGHT;
+ 		req->work.files = current->files;
+ 		ret = 0;
+ 	}
+ 	spin_unlock_irq(&ctx->inflight_lock);
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_kiocb *req = container_of(timer, struct io_kiocb,
+ 						timeout.timer);
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 	int ret = -ETIME;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		prev = list_entry(req->list.prev, struct io_kiocb, link_list);
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		void *user_data = (void *) (unsigned long) prev->user_data;
+ 		ret = io_async_cancel_one(ctx, user_data);
+ 	}
+ 
+ 	io_cqring_add_event(ctx, req->user_data, ret);
+ 	io_put_req(req, NULL);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_queue_linked_timeout(struct io_kiocb *req, struct io_kiocb *nxt)
+ {
+ 	const struct io_uring_sqe *sqe = nxt->submit.sqe;
+ 	enum hrtimer_mode mode;
+ 	struct timespec64 ts;
+ 	int ret = -EINVAL;
+ 
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1 || sqe->off)
+ 		goto err;
+ 	if (sqe->timeout_flags & ~IORING_TIMEOUT_ABS)
+ 		goto err;
+ 	if (get_timespec64(&ts, u64_to_user_ptr(sqe->addr))) {
+ 		ret = -EFAULT;
+ 		goto err;
+ 	}
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 
+ 	if (sqe->timeout_flags & IORING_TIMEOUT_ABS)
+ 		mode = HRTIMER_MODE_ABS;
+ 	else
+ 		mode = HRTIMER_MODE_REL;
+ 	hrtimer_init(&nxt->timeout.timer, CLOCK_MONOTONIC, mode);
+ 	nxt->timeout.timer.function = io_link_timeout_fn;
+ 	hrtimer_start(&nxt->timeout.timer, timespec64_to_ktime(ts), mode);
+ 	ret = 0;
+ err:
+ 	/* drop submission reference */
+ 	io_put_req(nxt, NULL);
+ 
+ 	if (ret) {
+ 		struct io_ring_ctx *ctx = req->ctx;
+ 
+ 		/*
+ 		 * Break the link and fail linked timeout, parent will get
+ 		 * failed by the regular submission path.
+ 		 */
+ 		list_del(&nxt->list);
+ 		io_cqring_fill_event(ctx, nxt->user_data, ret);
+ 		trace_io_uring_fail_link(req, nxt);
+ 		io_commit_cqring(ctx);
+ 		io_put_req(nxt, NULL);
+ 		ret = -ECANCELED;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static inline struct io_kiocb *io_get_linked_timeout(struct io_kiocb *req)
  {
+ 	struct io_kiocb *nxt;
+ 
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
+ 	if (nxt && nxt->submit.sqe->opcode == IORING_OP_LINK_TIMEOUT)
+ 		return nxt;
+ 
+ 	return NULL;
+ }
+ 
+ static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req)
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
+ {
+ 	struct io_kiocb *nxt;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
++=======
+ 	nxt = io_get_linked_timeout(req);
+ 	if (unlikely(nxt)) {
+ 		ret = io_queue_linked_timeout(req, nxt);
+ 		if (ret)
+ 			goto err;
+ 	}
+ 
+ 	ret = __io_submit_sqe(ctx, req, NULL, true);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		struct sqe_submit *s = &req->submit;
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
  		struct io_uring_sqe *sqe_copy;
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
@@@ -2322,11 -2768,14 +2559,15 @@@ err
  	} else if (s->sqe->flags & IOSQE_IO_LINK) {
  		req->flags |= REQ_F_LINK;
  
 +		memcpy(&req->submit, s, sizeof(*s));
  		INIT_LIST_HEAD(&req->link_list);
  		*link = req;
+ 	} else if (READ_ONCE(s->sqe->opcode) == IORING_OP_LINK_TIMEOUT) {
+ 		/* Only valid as a linked SQE */
+ 		ret = -EINVAL;
+ 		goto err_req;
  	} else {
 -		io_queue_sqe(ctx, req);
 +		io_queue_sqe(ctx, req, s, force_nonblock);
  	}
  }
  
diff --cc include/uapi/linux/io_uring.h
index ee8693aec163,f1a118b01d18..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -62,6 -68,11 +62,14 @@@ struct io_uring_sqe 
  #define IORING_OP_SYNC_FILE_RANGE	8
  #define IORING_OP_SENDMSG	9
  #define IORING_OP_RECVMSG	10
++<<<<<<< HEAD
++=======
+ #define IORING_OP_TIMEOUT	11
+ #define IORING_OP_TIMEOUT_REMOVE	12
+ #define IORING_OP_ACCEPT	13
+ #define IORING_OP_ASYNC_CANCEL	14
+ #define IORING_OP_LINK_TIMEOUT	15
++>>>>>>> 2665abfd757f (io_uring: add support for linked SQE timeouts)
  
  /*
   * sqe->fsync_flags
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
