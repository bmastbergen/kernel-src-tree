io_uring: fix refcounting with batched allocations at OOM

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 9466f43741bc08edd7b1bee642dd6f5561091634
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/9466f437.failed

In case of out of memory the second argument of percpu_ref_put_many() in
io_submit_sqes() may evaluate into "nr - (-EAGAIN)", that is clearly
wrong.

Fixes: 2b85edfc0c90 ("io_uring: batch getting pcpu references")
	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 9466f43741bc08edd7b1bee642dd6f5561091634)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index e4dddc0b25db,1dd20305c664..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2438,45 -4786,57 +2438,53 @@@ static int io_submit_sqes(struct io_rin
  		statep = &state;
  	}
  
 -	ctx->ring_fd = ring_fd;
 -	ctx->ring_file = ring_file;
 -
  	for (i = 0; i < nr; i++) {
 -		const struct io_uring_sqe *sqe;
 -		struct io_kiocb *req;
 -
 -		req = io_get_req(ctx, statep);
 -		if (unlikely(!req)) {
 -			if (!submitted)
 -				submitted = -EAGAIN;
 -			break;
 -		}
 -		if (!io_get_sqring(ctx, req, &sqe)) {
 -			__io_req_do_free(req);
 -			break;
 -		}
 -
 -		/* will complete beyond this point, count as submitted */
 -		submitted++;
 -
 -		if (unlikely(req->opcode >= IORING_OP_LAST)) {
 -			io_cqring_add_event(req, -EINVAL);
 -			io_double_put_req(req);
 -			break;
 -		}
 -
 -		if (io_op_defs[req->opcode].needs_mm && !*mm) {
 -			mm_fault = mm_fault || !mmget_not_zero(ctx->sqo_mm);
 -			if (!mm_fault) {
 -				use_mm(ctx->sqo_mm);
 -				*mm = ctx->sqo_mm;
 +		/*
 +		 * If previous wasn't linked and we have a linked command,
 +		 * that's the end of the chain. Submit the previous link.
 +		 */
 +		if (!prev_was_link && link) {
 +			io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +						true);
 +			link = NULL;
 +			shadow_req = NULL;
 +		}
 +		prev_was_link = (sqes[i].sqe->flags & IOSQE_IO_LINK) != 0;
 +
 +		if (link && (sqes[i].sqe->flags & IOSQE_IO_DRAIN)) {
 +			if (!shadow_req) {
 +				shadow_req = io_get_req(ctx, NULL);
 +				if (unlikely(!shadow_req))
 +					goto out;
 +				shadow_req->flags |= (REQ_F_IO_DRAIN | REQ_F_SHADOW_DRAIN);
 +				refcount_dec(&shadow_req->refs);
  			}
 +			shadow_req->sequence = sqes[i].sequence;
  		}
  
 -		req->has_user = *mm != NULL;
 -		req->in_async = async;
 -		req->needs_fixed_file = async;
 -		trace_io_uring_submit_sqe(ctx, req->opcode, req->user_data,
 -						true, async);
 -		if (!io_submit_sqe(req, sqe, statep, &link))
 -			break;
 +out:
 +		if (unlikely(mm_fault)) {
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
 +						-EFAULT);
 +		} else {
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
 +			submitted++;
 +		}
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (unlikely(submitted != nr)) {
+ 		int ref_used = (submitted == -EAGAIN) ? 0 : submitted;
+ 
+ 		percpu_ref_put_many(&ctx->refs, nr - ref_used);
+ 	}
++>>>>>>> 9466f43741bc (io_uring: fix refcounting with batched allocations at OOM)
  	if (link)
 -		io_queue_link_head(link);
 +		io_queue_link_head(ctx, link, &link->submit, shadow_req, true);
  	if (statep)
  		io_submit_state_end(&state);
  
* Unmerged path fs/io_uring.c
