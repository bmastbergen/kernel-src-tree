io_uring: keep all sqe->flags in req->flags

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit dea3b49c7fb09b4f6b6a574c0485ffeb9df7b69c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/dea3b49c.failed

It's a good idea to not read sqe->flags twice, as it's prone to security
bugs. Instead of passing it around, embeed them in req->flags. It's
already so except for IOSQE_IO_LINK.
1. rename former REQ_F_LINK -> REQ_F_LINK_HEAD
2. introduce and copy REQ_F_LINK, which mimics IO_IOSQE_LINK

And leave req_set_fail_links() using new REQ_F_LINK, because it's more
sensible.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit dea3b49c7fb09b4f6b6a574c0485ffeb9df7b69c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 2afa3b27779e,b0e1bdfe0a43..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -308,6 -346,245 +308,248 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_close {
+ 	struct file			*file;
+ 	struct file			*put_file;
+ 	int				fd;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ 	unsigned long			nofile;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ 	int				mode;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	unsigned			count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	union {
+ 		struct user_msghdr __user *msg;
+ 		void __user		*buf;
+ 	};
+ 	int				msg_flags;
+ 	int				bgid;
+ 	size_t				len;
+ 	struct io_buffer		*kbuf;
+ };
+ 
+ struct io_open {
+ 	struct file			*file;
+ 	int				dfd;
+ 	union {
+ 		unsigned		mask;
+ 	};
+ 	struct filename			*filename;
+ 	struct statx __user		*buffer;
+ 	struct open_how			how;
+ 	unsigned long			nofile;
+ };
+ 
+ struct io_files_update {
+ 	struct file			*file;
+ 	u64				arg;
+ 	u32				nr_args;
+ 	u32				offset;
+ };
+ 
+ struct io_fadvise {
+ 	struct file			*file;
+ 	u64				offset;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_madvise {
+ 	struct file			*file;
+ 	u64				addr;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_epoll {
+ 	struct file			*file;
+ 	int				epfd;
+ 	int				op;
+ 	int				fd;
+ 	struct epoll_event		event;
+ };
+ 
+ struct io_splice {
+ 	struct file			*file_out;
+ 	struct file			*file_in;
+ 	loff_t				off_out;
+ 	loff_t				off_in;
+ 	u64				len;
+ 	unsigned int			flags;
+ };
+ 
+ struct io_provide_buf {
+ 	struct file			*file;
+ 	__u64				addr;
+ 	__s32				len;
+ 	__u32				bgid;
+ 	__u16				nbufs;
+ 	__u16				bid;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ 	struct sockaddr_storage		addr;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
+ };
+ 
+ enum {
+ 	REQ_F_FIXED_FILE_BIT	= IOSQE_FIXED_FILE_BIT,
+ 	REQ_F_IO_DRAIN_BIT	= IOSQE_IO_DRAIN_BIT,
+ 	REQ_F_LINK_BIT		= IOSQE_IO_LINK_BIT,
+ 	REQ_F_HARDLINK_BIT	= IOSQE_IO_HARDLINK_BIT,
+ 	REQ_F_FORCE_ASYNC_BIT	= IOSQE_ASYNC_BIT,
+ 	REQ_F_BUFFER_SELECT_BIT	= IOSQE_BUFFER_SELECT_BIT,
+ 
+ 	REQ_F_LINK_HEAD_BIT,
+ 	REQ_F_LINK_NEXT_BIT,
+ 	REQ_F_FAIL_LINK_BIT,
+ 	REQ_F_INFLIGHT_BIT,
+ 	REQ_F_CUR_POS_BIT,
+ 	REQ_F_NOWAIT_BIT,
+ 	REQ_F_IOPOLL_COMPLETED_BIT,
+ 	REQ_F_LINK_TIMEOUT_BIT,
+ 	REQ_F_TIMEOUT_BIT,
+ 	REQ_F_ISREG_BIT,
+ 	REQ_F_MUST_PUNT_BIT,
+ 	REQ_F_TIMEOUT_NOSEQ_BIT,
+ 	REQ_F_COMP_LOCKED_BIT,
+ 	REQ_F_NEED_CLEANUP_BIT,
+ 	REQ_F_OVERFLOW_BIT,
+ 	REQ_F_POLLED_BIT,
+ 	REQ_F_BUFFER_SELECTED_BIT,
+ 
+ 	/* not a real bit, just to check we're not overflowing the space */
+ 	__REQ_F_LAST_BIT,
+ };
+ 
+ enum {
+ 	/* ctx owns file */
+ 	REQ_F_FIXED_FILE	= BIT(REQ_F_FIXED_FILE_BIT),
+ 	/* drain existing IO first */
+ 	REQ_F_IO_DRAIN		= BIT(REQ_F_IO_DRAIN_BIT),
+ 	/* linked sqes */
+ 	REQ_F_LINK		= BIT(REQ_F_LINK_BIT),
+ 	/* doesn't sever on completion < 0 */
+ 	REQ_F_HARDLINK		= BIT(REQ_F_HARDLINK_BIT),
+ 	/* IOSQE_ASYNC */
+ 	REQ_F_FORCE_ASYNC	= BIT(REQ_F_FORCE_ASYNC_BIT),
+ 	/* IOSQE_BUFFER_SELECT */
+ 	REQ_F_BUFFER_SELECT	= BIT(REQ_F_BUFFER_SELECT_BIT),
+ 
+ 	/* head of a link */
+ 	REQ_F_LINK_HEAD		= BIT(REQ_F_LINK_HEAD_BIT),
+ 	/* already grabbed next link */
+ 	REQ_F_LINK_NEXT		= BIT(REQ_F_LINK_NEXT_BIT),
+ 	/* fail rest of links */
+ 	REQ_F_FAIL_LINK		= BIT(REQ_F_FAIL_LINK_BIT),
+ 	/* on inflight list */
+ 	REQ_F_INFLIGHT		= BIT(REQ_F_INFLIGHT_BIT),
+ 	/* read/write uses file position */
+ 	REQ_F_CUR_POS		= BIT(REQ_F_CUR_POS_BIT),
+ 	/* must not punt to workers */
+ 	REQ_F_NOWAIT		= BIT(REQ_F_NOWAIT_BIT),
+ 	/* polled IO has completed */
+ 	REQ_F_IOPOLL_COMPLETED	= BIT(REQ_F_IOPOLL_COMPLETED_BIT),
+ 	/* has linked timeout */
+ 	REQ_F_LINK_TIMEOUT	= BIT(REQ_F_LINK_TIMEOUT_BIT),
+ 	/* timeout request */
+ 	REQ_F_TIMEOUT		= BIT(REQ_F_TIMEOUT_BIT),
+ 	/* regular file */
+ 	REQ_F_ISREG		= BIT(REQ_F_ISREG_BIT),
+ 	/* must be punted even for NONBLOCK */
+ 	REQ_F_MUST_PUNT		= BIT(REQ_F_MUST_PUNT_BIT),
+ 	/* no timeout sequence */
+ 	REQ_F_TIMEOUT_NOSEQ	= BIT(REQ_F_TIMEOUT_NOSEQ_BIT),
+ 	/* completion under lock */
+ 	REQ_F_COMP_LOCKED	= BIT(REQ_F_COMP_LOCKED_BIT),
+ 	/* needs cleanup */
+ 	REQ_F_NEED_CLEANUP	= BIT(REQ_F_NEED_CLEANUP_BIT),
+ 	/* in overflow list */
+ 	REQ_F_OVERFLOW		= BIT(REQ_F_OVERFLOW_BIT),
+ 	/* already went through poll handler */
+ 	REQ_F_POLLED		= BIT(REQ_F_POLLED_BIT),
+ 	/* buffer already selected */
+ 	REQ_F_BUFFER_SELECTED	= BIT(REQ_F_BUFFER_SELECTED_BIT),
+ };
+ 
+ struct async_poll {
+ 	struct io_poll_iocb	poll;
+ 	struct io_wq_work	work;
+ };
+ 
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -623,34 -1362,128 +865,111 @@@ static void io_free_req_many(struct io_
  
  static void __io_free_req(struct io_kiocb *req)
  {
 -	__io_req_aux_free(req);
 -
 -	if (req->flags & REQ_F_INFLIGHT) {
 -		struct io_ring_ctx *ctx = req->ctx;
 -		unsigned long flags;
 -
 -		spin_lock_irqsave(&ctx->inflight_lock, flags);
 -		list_del(&req->inflight_entry);
 -		if (waitqueue_active(&ctx->inflight_wait))
 -			wake_up(&ctx->inflight_wait);
 -		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
 -	}
 -
 +	if (req->file && !(req->flags & REQ_F_FIXED_FILE))
 +		fput(req->file);
  	percpu_ref_put(&req->ctx->refs);
 -	if (likely(!io_is_fallback_req(req)))
 -		kmem_cache_free(req_cachep, req);
 -	else
 -		clear_bit_unlock(0, (unsigned long *) req->ctx->fallback_req);
 +	kmem_cache_free(req_cachep, req);
  }
  
 -struct req_batch {
 -	void *reqs[IO_IOPOLL_BATCH];
 -	int to_free;
 -	int need_iter;
 -};
 -
 -static void io_free_req_many(struct io_ring_ctx *ctx, struct req_batch *rb)
 +static void io_req_link_next(struct io_kiocb *req)
  {
++<<<<<<< HEAD
 +	struct io_kiocb *nxt;
++=======
+ 	if (!rb->to_free)
+ 		return;
+ 	if (rb->need_iter) {
+ 		int i, inflight = 0;
+ 		unsigned long flags;
+ 
+ 		for (i = 0; i < rb->to_free; i++) {
+ 			struct io_kiocb *req = rb->reqs[i];
+ 
+ 			if (req->flags & REQ_F_FIXED_FILE) {
+ 				req->file = NULL;
+ 				percpu_ref_put(req->fixed_file_refs);
+ 			}
+ 			if (req->flags & REQ_F_INFLIGHT)
+ 				inflight++;
+ 			__io_req_aux_free(req);
+ 		}
+ 		if (!inflight)
+ 			goto do_free;
+ 
+ 		spin_lock_irqsave(&ctx->inflight_lock, flags);
+ 		for (i = 0; i < rb->to_free; i++) {
+ 			struct io_kiocb *req = rb->reqs[i];
+ 
+ 			if (req->flags & REQ_F_INFLIGHT) {
+ 				list_del(&req->inflight_entry);
+ 				if (!--inflight)
+ 					break;
+ 			}
+ 		}
+ 		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
+ 
+ 		if (waitqueue_active(&ctx->inflight_wait))
+ 			wake_up(&ctx->inflight_wait);
+ 	}
+ do_free:
+ 	kmem_cache_free_bulk(req_cachep, rb->to_free, rb->reqs);
+ 	percpu_ref_put_many(&ctx->refs, rb->to_free);
+ 	rb->to_free = rb->need_iter = 0;
+ }
+ 
+ static bool io_link_cancel_timeout(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
+ 	if (ret != -1) {
+ 		io_cqring_fill_event(req, -ECANCELED);
+ 		io_commit_cqring(ctx);
+ 		req->flags &= ~REQ_F_LINK_HEAD;
+ 		io_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_req_link_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	bool wake_ev = false;
+ 
+ 	/* Already got next link */
+ 	if (req->flags & REQ_F_LINK_NEXT)
+ 		return;
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  
  	/*
  	 * The list should never be empty when we are called here. But could
  	 * potentially happen if the chain is messed up, check to be on the
  	 * safe side.
  	 */
 -	while (!list_empty(&req->link_list)) {
 -		struct io_kiocb *nxt = list_first_entry(&req->link_list,
 -						struct io_kiocb, link_list);
 -
 -		if (unlikely((req->flags & REQ_F_LINK_TIMEOUT) &&
 -			     (nxt->flags & REQ_F_TIMEOUT))) {
 -			list_del_init(&nxt->link_list);
 -			wake_ev |= io_link_cancel_timeout(nxt);
 -			req->flags &= ~REQ_F_LINK_TIMEOUT;
 -			continue;
 +	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
 +	if (nxt) {
 +		list_del(&nxt->list);
 +		if (!list_empty(&req->link_list)) {
 +			INIT_LIST_HEAD(&nxt->link_list);
 +			list_splice(&req->link_list, &nxt->link_list);
 +			nxt->flags |= REQ_F_LINK;
  		}
  
++<<<<<<< HEAD
 +		nxt->flags |= REQ_F_LINK_DONE;
 +		INIT_WORK(&nxt->work, io_sq_wq_submit_work);
 +		io_queue_async_work(req->ctx, nxt);
++=======
+ 		list_del_init(&req->link_list);
+ 		if (!list_empty(&nxt->link_list))
+ 			nxt->flags |= REQ_F_LINK_HEAD;
+ 		*nxtptr = nxt;
+ 		break;
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  	}
 -
 -	req->flags |= REQ_F_LINK_NEXT;
 -	if (wake_ev)
 -		io_cqring_ev_posted(ctx);
  }
  
  /*
@@@ -658,19 -1491,38 +977,25 @@@
   */
  static void io_fail_links(struct io_kiocb *req)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 +	struct io_kiocb *link;
  
  	while (!list_empty(&req->link_list)) {
 -		struct io_kiocb *link = list_first_entry(&req->link_list,
 -						struct io_kiocb, link_list);
 -
 -		list_del_init(&link->link_list);
 -		trace_io_uring_fail_link(req, link);
 +		link = list_first_entry(&req->link_list, struct io_kiocb, list);
 +		list_del(&link->list);
  
 -		if ((req->flags & REQ_F_LINK_TIMEOUT) &&
 -		    link->opcode == IORING_OP_LINK_TIMEOUT) {
 -			io_link_cancel_timeout(link);
 -		} else {
 -			io_cqring_fill_event(link, -ECANCELED);
 -			__io_double_put_req(link);
 -		}
 -		req->flags &= ~REQ_F_LINK_TIMEOUT;
 +		io_cqring_add_event(req->ctx, link->user_data, -ECANCELED);
 +		__io_free_req(link);
  	}
 -
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -	io_cqring_ev_posted(ctx);
  }
  
 -static void io_req_find_next(struct io_kiocb *req, struct io_kiocb **nxt)
 +static void io_free_req(struct io_kiocb *req)
  {
++<<<<<<< HEAD
++=======
+ 	if (likely(!(req->flags & REQ_F_LINK_HEAD)))
+ 		return;
+ 
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  	/*
  	 * If LINK is set, we have dependent requests in this chain. If we
  	 * didn't fail this request, queue the first one up, moving any other
@@@ -693,11 -1603,98 +1018,50 @@@ static void io_put_req(struct io_kiocb 
  		io_free_req(req);
  }
  
 -static void io_steal_work(struct io_kiocb *req,
 -			  struct io_wq_work **workptr)
 -{
 -	/*
 -	 * It's in an io-wq worker, so there always should be at least
 -	 * one reference, which will be dropped in io_put_work() just
 -	 * after the current handler returns.
 -	 *
 -	 * It also means, that if the counter dropped to 1, then there is
 -	 * no asynchronous users left, so it's safe to steal the next work.
 -	 */
 -	if (refcount_read(&req->refs) == 1) {
 -		struct io_kiocb *nxt = NULL;
 -
 -		io_req_find_next(req, &nxt);
 -		if (nxt)
 -			io_wq_assign_next(workptr, nxt);
 -	}
 -}
 -
 -/*
 - * Must only be used if we don't need to care about links, usually from
 - * within the completion handling itself.
 - */
 -static void __io_double_put_req(struct io_kiocb *req)
 -{
 -	/* drop both submit and complete references */
 -	if (refcount_sub_and_test(2, &req->refs))
 -		__io_free_req(req);
 -}
 -
 -static void io_double_put_req(struct io_kiocb *req)
 -{
 -	/* drop both submit and complete references */
 -	if (refcount_sub_and_test(2, &req->refs))
 -		io_free_req(req);
 -}
 -
 -static unsigned io_cqring_events(struct io_ring_ctx *ctx, bool noflush)
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
  {
 -	struct io_rings *rings = ctx->rings;
 -
 -	if (test_bit(0, &ctx->cq_check_overflow)) {
 -		/*
 -		 * noflush == true is from the waitqueue handler, just ensure
 -		 * we wake up the task, and the next invocation will flush the
 -		 * entries. We cannot safely to it from here.
 -		 */
 -		if (noflush && !list_empty(&ctx->cq_overflow_list))
 -			return -1U;
 -
 -		io_cqring_overflow_flush(ctx, false);
 -	}
 -
  	/* See comment at the top of this file */
  	smp_rmb();
++<<<<<<< HEAD
 +	return READ_ONCE(ring->r.tail) - READ_ONCE(ring->r.head);
++=======
+ 	return ctx->cached_cq_tail - READ_ONCE(rings->cq.head);
+ }
+ 
+ static inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)
+ {
+ 	struct io_rings *rings = ctx->rings;
+ 
+ 	/* make sure SQ entry isn't read before tail */
+ 	return smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;
+ }
+ 
+ static inline bool io_req_multi_free(struct req_batch *rb, struct io_kiocb *req)
+ {
+ 	if ((req->flags & REQ_F_LINK_HEAD) || io_is_fallback_req(req))
+ 		return false;
+ 
+ 	if (!(req->flags & REQ_F_FIXED_FILE) || req->io)
+ 		rb->need_iter++;
+ 
+ 	rb->reqs[rb->to_free++] = req;
+ 	if (unlikely(rb->to_free == ARRAY_SIZE(rb->reqs)))
+ 		io_free_req_many(req->ctx, rb);
+ 	return true;
+ }
+ 
+ static int io_put_kbuf(struct io_kiocb *req)
+ {
+ 	struct io_buffer *kbuf;
+ 	int cflags;
+ 
+ 	kbuf = (struct io_buffer *) (unsigned long) req->rw.addr;
+ 	cflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;
+ 	cflags |= IORING_CQE_F_BUFFER;
+ 	req->rw.addr = 0;
+ 	kfree(kbuf);
+ 	return cflags;
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  }
  
  /*
@@@ -1363,12 -2559,24 +1727,30 @@@ static int io_read(struct io_kiocb *req
  	if (ret < 0)
  		return ret;
  
++<<<<<<< HEAD
 +	read_size = ret;
 +	if (req->flags & REQ_F_LINK)
 +		req->result = read_size;
++=======
+ 	/* Ensure we clear previously set non-block flag */
+ 	if (!force_nonblock)
+ 		kiocb->ki_flags &= ~IOCB_NOWAIT;
+ 
+ 	req->result = 0;
+ 	io_size = ret;
+ 	if (req->flags & REQ_F_LINK_HEAD)
+ 		req->result = io_size;
+ 
+ 	/*
+ 	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
+ 	 * we know to async punt it even if it was opened O_NONBLOCK
+ 	 */
+ 	if (force_nonblock && !io_file_supports_async(req->file))
+ 		goto copy_iov;
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  
  	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(READ, req->file, &kiocb->ki_pos, iov_count);
 +	ret = rw_verify_area(READ, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
  
@@@ -1426,20 -2634,45 +1808,59 @@@ static int io_write(struct io_kiocb *re
  	if (ret < 0)
  		return ret;
  
++<<<<<<< HEAD
 +	if (req->flags & REQ_F_LINK)
 +		req->result = ret;
++=======
+ 	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
+ 	return 0;
+ }
+ 
+ static int io_write(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
+ 	struct kiocb *kiocb = &req->rw.kiocb;
+ 	struct iov_iter iter;
+ 	size_t iov_count;
+ 	ssize_t ret, io_size;
+ 
+ 	ret = io_import_iovec(WRITE, req, &iovec, &iter, !force_nonblock);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* Ensure we clear previously set non-block flag */
+ 	if (!force_nonblock)
+ 		req->rw.kiocb.ki_flags &= ~IOCB_NOWAIT;
+ 
+ 	req->result = 0;
+ 	io_size = ret;
+ 	if (req->flags & REQ_F_LINK_HEAD)
+ 		req->result = io_size;
+ 
+ 	/*
+ 	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
+ 	 * we know to async punt it even if it was opened O_NONBLOCK
+ 	 */
+ 	if (force_nonblock && !io_file_supports_async(req->file))
+ 		goto copy_iov;
+ 
+ 	/* file path doesn't support NOWAIT for non-direct_IO */
+ 	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&
+ 	    (req->flags & REQ_F_ISREG))
+ 		goto copy_iov;
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  
  	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(WRITE, req->file, &kiocb->ki_pos, iov_count);
 +
 +	ret = -EAGAIN;
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
 +		goto out_free;
 +	}
 +
 +	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
  
@@@ -2234,98 -5415,236 +2655,265 @@@ static int __io_queue_sqe(struct io_rin
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
++=======
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_timeout_data *data = container_of(timer,
+ 						struct io_timeout_data, timer);
+ 	struct io_kiocb *req = data->req;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->link_list)) {
+ 		prev = list_entry(req->link_list.prev, struct io_kiocb,
+ 				  link_list);
+ 		if (refcount_inc_not_zero(&prev->refs)) {
+ 			list_del_init(&req->link_list);
+ 			prev->flags &= ~REQ_F_LINK_TIMEOUT;
+ 		} else
+ 			prev = NULL;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		req_set_fail_links(prev);
+ 		io_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);
+ 		io_put_req(prev);
+ 	} else {
+ 		io_cqring_add_event(req, -ETIME);
+ 		io_put_req(req);
+ 	}
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static void io_queue_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	/*
+ 	 * If the list is now empty, then our linked request finished before
+ 	 * we got a chance to setup the timer
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (!list_empty(&req->link_list)) {
+ 		struct io_timeout_data *data = &req->io->timeout;
+ 
+ 		data->timer.function = io_link_timeout_fn;
+ 		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
+ 				data->mode);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	/* drop submission reference */
+ 	io_put_req(req);
+ }
+ 
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	if (!(req->flags & REQ_F_LINK_HEAD))
+ 		return NULL;
+ 	/* for polled retry, if flag is set, we already went through here */
+ 	if (req->flags & REQ_F_POLLED)
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb,
+ 					link_list);
+ 	if (!nxt || nxt->opcode != IORING_OP_LINK_TIMEOUT)
+ 		return NULL;
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 	return nxt;
+ }
+ 
+ static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_kiocb *linked_timeout;
+ 	struct io_kiocb *nxt;
+ 	const struct cred *old_creds = NULL;
+ 	int ret;
+ 
+ again:
+ 	linked_timeout = io_prep_linked_timeout(req);
+ 
+ 	if (req->work.creds && req->work.creds != current_cred()) {
+ 		if (old_creds)
+ 			revert_creds(old_creds);
+ 		if (old_creds == req->work.creds)
+ 			old_creds = NULL; /* restored original creds */
+ 		else
+ 			old_creds = override_creds(req->work.creds);
+ 	}
+ 
+ 	ret = io_issue_sqe(req, sqe, true);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		if (io_arm_poll_handler(req)) {
+ 			if (linked_timeout)
+ 				io_queue_linked_timeout(linked_timeout);
+ 			goto exit;
+ 		}
+ punt:
+ 		if (io_op_defs[req->opcode].file_table) {
+ 			ret = io_grab_files(req);
+ 			if (ret)
+ 				goto err;
+ 		}
+ 
+ 		/*
+ 		 * Queued up for async execution, worker will release
+ 		 * submit reference when the iocb is actually submitted.
+ 		 */
+ 		io_queue_async_work(req);
+ 		goto exit;
+ 	}
+ 
+ err:
+ 	nxt = NULL;
+ 	/* drop submission reference */
+ 	io_put_req_find_next(req, &nxt);
+ 
+ 	if (linked_timeout) {
+ 		if (!ret)
+ 			io_queue_linked_timeout(linked_timeout);
+ 		else
+ 			io_put_req(linked_timeout);
+ 	}
+ 
+ 	/* and drop final reference, if we failed */
+ 	if (ret) {
+ 		io_cqring_add_event(req, ret);
+ 		req_set_fail_links(req);
+ 		io_put_req(req);
+ 	}
+ 	if (nxt) {
+ 		req = nxt;
+ 
+ 		if (req->flags & REQ_F_FORCE_ASYNC)
+ 			goto punt;
+ 		goto again;
+ 	}
+ exit:
+ 	if (old_creds)
+ 		revert_creds(old_creds);
+ }
+ 
+ static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  {
  	int ret;
  
 -	ret = io_req_defer(req, sqe);
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -fail_req:
 -			io_cqring_add_event(req, ret);
 -			req_set_fail_links(req);
 -			io_double_put_req(req);
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
  		}
 -	} else if (req->flags & REQ_F_FORCE_ASYNC) {
 -		ret = io_req_defer_prep(req, sqe);
 -		if (unlikely(ret < 0))
 -			goto fail_req;
 -		/*
 -		 * Never try inline submit of IOSQE_ASYNC is set, go straight
 -		 * to async execution.
 -		 */
 -		req->work.flags |= IO_WQ_WORK_CONCURRENT;
 -		io_queue_async_work(req);
 -	} else {
 -		__io_queue_sqe(req, sqe);
 +		return 0;
  	}
 +
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
  }
  
 -static inline void io_queue_link_head(struct io_kiocb *req)
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
  {
 -	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_double_put_req(req);
 -	} else
 -		io_queue_sqe(req, NULL);
 -}
 +	int ret;
 +	int need_submit = false;
  
 -#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|	\
 -				IOSQE_IO_HARDLINK | IOSQE_ASYNC | \
 -				IOSQE_BUFFER_SELECT)
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
  
 -static int io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			  struct io_submit_state *state, struct io_kiocb **link)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned int sqe_flags;
 -	int ret, id, fd;
 +	/*
 +	 * Mark the first IO in link list as DRAIN, let all the following
 +	 * IOs enter the defer list. all IO needs to be completed before link
 +	 * list.
 +	 */
 +	req->flags |= REQ_F_IO_DRAIN;
 +	ret = io_req_defer(ctx, req, s->sqe);
 +	if (ret) {
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
 +		}
 +	} else {
 +		/*
 +		 * If ret == 0 means that all IOs in front of link io are
 +		 * running done. let's queue link head.
 +		 */
 +		need_submit = true;
 +	}
 +
 +	/* Insert shadow req to defer_list, blocking next IOs */
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_add_tail(&shadow->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
  
 -	sqe_flags = READ_ONCE(sqe->flags);
 +	if (need_submit)
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
  
 -	/* enforce forwards compatibility on users */
 -	if (unlikely(sqe_flags & ~SQE_VALID_FLAGS))
 -		return -EINVAL;
 +	return 0;
 +}
  
 -	if ((sqe_flags & IOSQE_BUFFER_SELECT) &&
 -	    !io_op_defs[req->opcode].buffer_select)
 -		return -EOPNOTSUPP;
 +#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
  
 -	id = READ_ONCE(sqe->personality);
 -	if (id) {
 -		req->work.creds = idr_find(&ctx->personality_idr, id);
 -		if (unlikely(!req->work.creds))
 -			return -EINVAL;
 -		get_cred(req->work.creds);
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
 +{
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
 +	int ret;
 +
 +	/* enforce forwards compatibility on users */
 +	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
 +		ret = -EINVAL;
 +		goto err;
  	}
  
++<<<<<<< HEAD
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
 +	}
++=======
+ 	/* same numerical values with corresponding REQ_F_*, safe to copy */
+ 	req->flags |= sqe_flags & (IOSQE_IO_DRAIN | IOSQE_IO_HARDLINK |
+ 					IOSQE_ASYNC | IOSQE_FIXED_FILE |
+ 					IOSQE_BUFFER_SELECT | IOSQE_IO_LINK);
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  
 -	fd = READ_ONCE(sqe->fd);
 -	ret = io_req_set_file(state, req, fd, sqe_flags);
 -	if (unlikely(ret))
 -		return ret;
 +	ret = io_req_set_file(ctx, s, state, req);
 +	if (unlikely(ret)) {
 +err_req:
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		return;
 +	}
 +
 +	req->user_data = s->sqe->user_data;
  
  	/*
  	 * If we already have a head request, queue this one for async
@@@ -2335,26 -5654,58 +2923,47 @@@
  	 * conditions are true (normal request), then just queue it.
  	 */
  	if (*link) {
 -		struct io_kiocb *head = *link;
 +		struct io_kiocb *prev = *link;
  
 -		/*
 -		 * Taking sequential execution of a link, draining both sides
 -		 * of the link also fullfils IOSQE_IO_DRAIN semantics for all
 -		 * requests in the link. So, it drains the head and the
 -		 * next after the link request. The last one is done via
 -		 * drain_next flag to persist the effect across calls.
 -		 */
 -		if (sqe_flags & IOSQE_IO_DRAIN) {
 -			head->flags |= REQ_F_IO_DRAIN;
 -			ctx->drain_next = 1;
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (!sqe_copy) {
 +			ret = -EAGAIN;
 +			goto err_req;
  		}
 -		if (io_alloc_async_ctx(req))
 -			return -EAGAIN;
  
 -		ret = io_req_defer_prep(req, sqe);
 -		if (ret) {
 -			/* fail even hard links since we don't submit */
 -			head->flags |= REQ_F_FAIL_LINK;
 -			return ret;
 -		}
 -		trace_io_uring_link(ctx, req, head);
 -		list_add_tail(&req->link_list, &head->link_list);
 +		s->sqe = sqe_copy;
 +		memcpy(&req->submit, s, sizeof(*s));
 +		list_add_tail(&req->list, &prev->link_list);
 +	} else if (s->sqe->flags & IOSQE_IO_LINK) {
 +		req->flags |= REQ_F_LINK;
  
 -		/* last request of a link, enqueue the link */
 -		if (!(sqe_flags & (IOSQE_IO_LINK|IOSQE_IO_HARDLINK))) {
 -			io_queue_link_head(head);
 -			*link = NULL;
 -		}
 +		memcpy(&req->submit, s, sizeof(*s));
 +		INIT_LIST_HEAD(&req->link_list);
 +		*link = req;
  	} else {
++<<<<<<< HEAD
 +		io_queue_sqe(ctx, req, s, force_nonblock);
++=======
+ 		if (unlikely(ctx->drain_next)) {
+ 			req->flags |= REQ_F_IO_DRAIN;
+ 			req->ctx->drain_next = 0;
+ 		}
+ 		if (sqe_flags & (IOSQE_IO_LINK|IOSQE_IO_HARDLINK)) {
+ 			req->flags |= REQ_F_LINK_HEAD;
+ 			INIT_LIST_HEAD(&req->link_list);
+ 
+ 			if (io_alloc_async_ctx(req))
+ 				return -EAGAIN;
+ 
+ 			ret = io_req_defer_prep(req, sqe);
+ 			if (ret)
+ 				req->flags |= REQ_F_FAIL_LINK;
+ 			*link = req;
+ 		} else {
+ 			io_queue_sqe(req, sqe);
+ 		}
++>>>>>>> dea3b49c7fb0 (io_uring: keep all sqe->flags in req->flags)
  	}
 -
 -	return 0;
  }
  
  /*
* Unmerged path fs/io_uring.c
