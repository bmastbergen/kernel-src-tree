io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 52de1fe122408d7a62b6cff9ed3895ebb882d71f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/52de1fe1.failed

Like IORING_OP_READV, this is limited to supporting just a single
segment in the iovec passed in.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 52de1fe122408d7a62b6cff9ed3895ebb882d71f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,455d53fd840f..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -44,8 -44,10 +44,9 @@@
  #include <linux/errno.h>
  #include <linux/syscalls.h>
  #include <linux/compat.h>
+ #include <net/compat.h>
  #include <linux/refcount.h>
  #include <linux/uio.h>
 -#include <linux/bits.h>
  
  #include <linux/sched/signal.h>
  #include <linux/fs.h>
@@@ -366,8 -649,202 +367,207 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ 	/* needs ->fs */
+ 	unsigned		needs_fs : 1;
+ 	/* set if opcode supports polled "wait" */
+ 	unsigned		pollin : 1;
+ 	unsigned		pollout : 1;
+ 	/* op supports buffer selection */
+ 	unsigned		buffer_select : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_SPLICE] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_PROVIDE_BUFFERS] = {},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_ring_file_ref_flush(struct fixed_file_data *data);
+ static void io_cleanup_req(struct io_kiocb *req);
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 		       int fd, struct file **out_file, bool fixed);
+ static void __io_queue_sqe(struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe);
++>>>>>>> 52de1fe12240 (io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG)
  
  static struct kmem_cache *req_cachep;
  
@@@ -1565,119 -2913,1381 +1765,427 @@@ static int io_sync_file_range(struct io
  	return 0;
  }
  
 -static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +#if defined(CONFIG_NET)
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
  {
 -	struct open_how __user *how;
 -	const char __user *fname;
 -	size_t len;
 +	struct socket *sock;
  	int ret;
  
 -	if (sqe->ioprio || sqe->buf_index)
 +	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
  		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
  
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	len = READ_ONCE(sqe->len);
 -
 -	if (len < OPEN_HOW_SIZE_VER0)
 -		return -EINVAL;
 +	sock = sock_from_file(req->file, &ret);
 +	if (sock) {
 +		struct user_msghdr __user *msg;
 +		unsigned flags;
  
 -	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
 -					len);
 -	if (ret)
 -		return ret;
 +		flags = READ_ONCE(sqe->msg_flags);
 +		if (flags & MSG_DONTWAIT)
 +			req->flags |= REQ_F_NOWAIT;
 +		else if (force_nonblock)
 +			flags |= MSG_DONTWAIT;
  
 -	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
 -		req->open.how.flags |= O_LARGEFILE;
 +		msg = (struct user_msghdr __user *) (unsigned long)
 +			READ_ONCE(sqe->addr);
  
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 +		ret = fn(sock, msg, flags);
 +		if (force_nonblock && ret == -EAGAIN)
 +			return ret;
 +		if (ret == -ERESTARTSYS)
 +			ret = -EINTR;
  	}
  
 -	req->flags |= REQ_F_NEED_CLEANUP;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
  	return 0;
  }
 +#endif
  
 -static int io_openat2(struct io_kiocb *req, bool force_nonblock)
 +static int io_sendmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		      bool force_nonblock)
  {
 -	struct open_flags op;
 -	struct file *file;
 -	int ret;
 +#if defined(CONFIG_NET)
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_sendmsg_sock);
 +#else
 +	return -EOPNOTSUPP;
 +#endif
 +}
  
 -	if (force_nonblock)
 -		return -EAGAIN;
++<<<<<<< HEAD
 +static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		      bool force_nonblock)
 +{
 +#if defined(CONFIG_NET)
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_recvmsg_sock);
++=======
++static int __io_recvmsg_copy_hdr(struct io_kiocb *req, struct io_async_ctx *io)
++{
++	struct io_sr_msg *sr = &req->sr_msg;
++	struct iovec __user *uiov;
++	size_t iov_len;
++	int ret;
+ 
 -	ret = build_open_flags(&req->open.how, &op);
++	ret = __copy_msghdr_from_user(&io->msg.msg, sr->msg, &io->msg.uaddr,
++					&uiov, &iov_len);
+ 	if (ret)
 -		goto err;
 -
 -	ret = get_unused_fd_flags(req->open.how.flags);
 -	if (ret < 0)
 -		goto err;
++		return ret;
+ 
 -	file = do_filp_open(req->open.dfd, req->open.filename, &op);
 -	if (IS_ERR(file)) {
 -		put_unused_fd(ret);
 -		ret = PTR_ERR(file);
++	if (req->flags & REQ_F_BUFFER_SELECT) {
++		if (iov_len > 1)
++			return -EINVAL;
++		if (copy_from_user(io->msg.iov, uiov, sizeof(*uiov)))
++			return -EFAULT;
++		sr->len = io->msg.iov[0].iov_len;
++		iov_iter_init(&io->msg.msg.msg_iter, READ, io->msg.iov, 1,
++				sr->len);
++		io->msg.iov = NULL;
+ 	} else {
 -		fsnotify_open(file);
 -		fd_install(ret, file);
++		ret = import_iovec(READ, uiov, iov_len, UIO_FASTIOV,
++					&io->msg.iov, &io->msg.msg.msg_iter);
++		if (ret > 0)
++			ret = 0;
+ 	}
 -err:
 -	putname(req->open.filename);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
+ 
 -static int io_openat(struct io_kiocb *req, bool force_nonblock)
 -{
 -	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
 -	return io_openat2(req, force_nonblock);
++	return ret;
+ }
+ 
 -static int io_provide_buffers_prep(struct io_kiocb *req,
 -				   const struct io_uring_sqe *sqe)
++#ifdef CONFIG_COMPAT
++static int __io_compat_recvmsg_copy_hdr(struct io_kiocb *req,
++					struct io_async_ctx *io)
+ {
 -	struct io_provide_buf *p = &req->pbuf;
 -	u64 tmp;
++	struct compat_msghdr __user *msg_compat;
++	struct io_sr_msg *sr = &req->sr_msg;
++	struct compat_iovec __user *uiov;
++	compat_uptr_t ptr;
++	compat_size_t len;
++	int ret;
+ 
 -	if (sqe->ioprio || sqe->rw_flags)
 -		return -EINVAL;
++	msg_compat = (struct compat_msghdr __user *) sr->msg;
++	ret = __get_compat_msghdr(&io->msg.msg, msg_compat, &io->msg.uaddr,
++					&ptr, &len);
++	if (ret)
++		return ret;
+ 
 -	tmp = READ_ONCE(sqe->fd);
 -	if (!tmp || tmp > USHRT_MAX)
 -		return -E2BIG;
 -	p->nbufs = tmp;
 -	p->addr = READ_ONCE(sqe->addr);
 -	p->len = READ_ONCE(sqe->len);
++	uiov = compat_ptr(ptr);
++	if (req->flags & REQ_F_BUFFER_SELECT) {
++		compat_ssize_t clen;
+ 
 -	if (!access_ok(u64_to_user_ptr(p->addr), p->len))
 -		return -EFAULT;
++		if (len > 1)
++			return -EINVAL;
++		if (!access_ok(uiov, sizeof(*uiov)))
++			return -EFAULT;
++		if (__get_user(clen, &uiov->iov_len))
++			return -EFAULT;
++		if (clen < 0)
++			return -EINVAL;
++		sr->len = io->msg.iov[0].iov_len;
++		io->msg.iov = NULL;
++	} else {
++		ret = compat_import_iovec(READ, uiov, len, UIO_FASTIOV,
++						&io->msg.iov,
++						&io->msg.msg.msg_iter);
++		if (ret < 0)
++			return ret;
++	}
+ 
 -	p->bgid = READ_ONCE(sqe->buf_group);
 -	tmp = READ_ONCE(sqe->off);
 -	if (tmp > USHRT_MAX)
 -		return -E2BIG;
 -	p->bid = tmp;
+ 	return 0;
+ }
++#endif
+ 
 -static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)
++static int io_recvmsg_copy_hdr(struct io_kiocb *req, struct io_async_ctx *io)
+ {
 -	struct io_buffer *buf;
 -	u64 addr = pbuf->addr;
 -	int i, bid = pbuf->bid;
 -
 -	for (i = 0; i < pbuf->nbufs; i++) {
 -		buf = kmalloc(sizeof(*buf), GFP_KERNEL);
 -		if (!buf)
 -			break;
++	io->msg.iov = io->msg.fast_iov;
+ 
 -		buf->addr = addr;
 -		buf->len = pbuf->len;
 -		buf->bid = bid;
 -		addr += pbuf->len;
 -		bid++;
 -		if (!*head) {
 -			INIT_LIST_HEAD(&buf->list);
 -			*head = buf;
 -		} else {
 -			list_add_tail(&buf->list, &(*head)->list);
 -		}
 -	}
++#ifdef CONFIG_COMPAT
++	if (req->ctx->compat)
++		return __io_compat_recvmsg_copy_hdr(req, io);
++#endif
+ 
 -	return i ? i : -ENOMEM;
++	return __io_recvmsg_copy_hdr(req, io);
+ }
+ 
 -static int io_provide_buffers(struct io_kiocb *req, bool force_nonblock)
++static struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,
++					       int *cflags, bool needs_lock)
+ {
 -	struct io_provide_buf *p = &req->pbuf;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_buffer *head, *list;
 -	int ret = 0;
 -
 -	io_ring_submit_lock(ctx, !force_nonblock);
++	struct io_sr_msg *sr = &req->sr_msg;
++	struct io_buffer *kbuf;
+ 
 -	lockdep_assert_held(&ctx->uring_lock);
++	if (!(req->flags & REQ_F_BUFFER_SELECT))
++		return NULL;
+ 
 -	list = head = idr_find(&ctx->io_buffer_idr, p->bgid);
++	kbuf = io_buffer_select(req, &sr->len, sr->bgid, sr->kbuf, needs_lock);
++	if (IS_ERR(kbuf))
++		return kbuf;
+ 
 -	ret = io_add_buffers(p, &head);
 -	if (ret < 0)
 -		goto out;
++	sr->kbuf = kbuf;
++	req->flags |= REQ_F_BUFFER_SELECTED;
+ 
 -	if (!list) {
 -		ret = idr_alloc(&ctx->io_buffer_idr, head, p->bgid, p->bgid + 1,
 -					GFP_KERNEL);
 -		if (ret < 0) {
 -			while (!list_empty(&head->list)) {
 -				struct io_buffer *buf;
 -
 -				buf = list_first_entry(&head->list,
 -							struct io_buffer, list);
 -				list_del(&buf->list);
 -				kfree(buf);
 -			}
 -			kfree(head);
 -			goto out;
 -		}
 -	}
 -out:
 -	io_ring_submit_unlock(ctx, !force_nonblock);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
++	*cflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;
++	*cflags |= IORING_CQE_F_BUFFER;
++	return kbuf;
+ }
+ 
 -static int io_epoll_ctl_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
++static int io_recvmsg_prep(struct io_kiocb *req,
++			   const struct io_uring_sqe *sqe)
+ {
 -#if defined(CONFIG_EPOLL)
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
++#if defined(CONFIG_NET)
++	struct io_sr_msg *sr = &req->sr_msg;
++	struct io_async_ctx *io = req->io;
++	int ret;
+ 
 -	req->epoll.epfd = READ_ONCE(sqe->fd);
 -	req->epoll.op = READ_ONCE(sqe->len);
 -	req->epoll.fd = READ_ONCE(sqe->off);
++	sr->msg_flags = READ_ONCE(sqe->msg_flags);
++	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
++	sr->len = READ_ONCE(sqe->len);
++	sr->bgid = READ_ONCE(sqe->buf_group);
+ 
 -	if (ep_op_has_event(req->epoll.op)) {
 -		struct epoll_event __user *ev;
++#ifdef CONFIG_COMPAT
++	if (req->ctx->compat)
++		sr->msg_flags |= MSG_CMSG_COMPAT;
++#endif
+ 
 -		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
 -			return -EFAULT;
 -	}
++	if (!io || req->opcode == IORING_OP_RECV)
++		return 0;
++	/* iovec is already imported */
++	if (req->flags & REQ_F_NEED_CLEANUP)
++		return 0;
+ 
 -	return 0;
++	ret = io_recvmsg_copy_hdr(req, io);
++	if (!ret)
++		req->flags |= REQ_F_NEED_CLEANUP;
++	return ret;
++>>>>>>> 52de1fe12240 (io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG)
  #else
  	return -EOPNOTSUPP;
  #endif
  }
  
 -static int io_epoll_ctl(struct io_kiocb *req, bool force_nonblock)
 +static void io_poll_remove_one(struct io_kiocb *req)
  {
 -#if defined(CONFIG_EPOLL)
 -	struct io_epoll *ie = &req->epoll;
 -	int ret;
++<<<<<<< HEAD
 +	struct io_poll_iocb *poll = &req->poll;
  
 -	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
 -	if (force_nonblock && ret == -EAGAIN)
 -		return -EAGAIN;
 +	spin_lock(&poll->head->lock);
 +	WRITE_ONCE(poll->canceled, true);
 +	if (!list_empty(&poll->wait.entry)) {
 +		list_del_init(&poll->wait.entry);
 +		io_queue_async_work(req->ctx, req);
 +	}
 +	spin_unlock(&poll->head->lock);
  
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 +	list_del_init(&req->list);
  }
  
 -static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static void io_poll_remove_all(struct io_ring_ctx *ctx)
  {
 -#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
 -	if (sqe->ioprio || sqe->buf_index || sqe->off)
 -		return -EINVAL;
 +	struct io_kiocb *req;
  
 -	req->madvise.addr = READ_ONCE(sqe->addr);
 -	req->madvise.len = READ_ONCE(sqe->len);
 -	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 +	spin_lock_irq(&ctx->completion_lock);
 +	while (!list_empty(&ctx->cancel_list)) {
 +		req = list_first_entry(&ctx->cancel_list, struct io_kiocb,list);
 +		io_poll_remove_one(req);
 +	}
 +	spin_unlock_irq(&ctx->completion_lock);
  }
  
 -static int io_madvise(struct io_kiocb *req, bool force_nonblock)
 +/*
 + * Find a running poll command that matches one specified in sqe->addr,
 + * and remove it if found.
 + */
 +static int io_poll_remove(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 -#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
 -	struct io_madvise *ma = &req->madvise;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 +	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_kiocb *poll_req, *next;
 +	int ret = -ENOENT;
++=======
++#if defined(CONFIG_NET)
++	struct io_async_msghdr *kmsg = NULL;
++	struct socket *sock;
++	int ret, cflags = 0;
++>>>>>>> 52de1fe12240 (io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG)
  
 -	ret = do_madvise(ma->addr, ma->len, ma->advice);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->ioprio || sqe->buf_index || sqe->addr)
 +	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 +		return -EINVAL;
 +	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 +	    sqe->poll_events)
  		return -EINVAL;
  
 -	req->fadvise.offset = READ_ONCE(sqe->off);
 -	req->fadvise.len = READ_ONCE(sqe->len);
 -	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
 -	return 0;
 -}
 -
 -static int io_fadvise(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct io_fadvise *fa = &req->fadvise;
 -	int ret;
 -
 -	if (force_nonblock) {
 -		switch (fa->advice) {
 -		case POSIX_FADV_NORMAL:
 -		case POSIX_FADV_RANDOM:
 -		case POSIX_FADV_SEQUENTIAL:
++<<<<<<< HEAD
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_for_each_entry_safe(poll_req, next, &ctx->cancel_list, list) {
 +		if (READ_ONCE(sqe->addr) == poll_req->user_data) {
 +			io_poll_remove_one(poll_req);
 +			ret = 0;
  			break;
 -		default:
 -			return -EAGAIN;
  		}
 -	}
 -
 -	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	const char __user *fname;
 -	unsigned lookup_flags;
 -	int ret;
 -
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	req->open.mask = READ_ONCE(sqe->len);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	req->open.how.flags = READ_ONCE(sqe->statx_flags);
 -
 -	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
 -		return -EINVAL;
 -
 -	req->open.filename = getname_flags(fname, lookup_flags, NULL);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 -	}
 -
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	return 0;
 -}
 -
 -static int io_statx(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct io_open *ctx = &req->open;
 -	unsigned lookup_flags;
 -	struct path path;
 -	struct kstat stat;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 -
 -	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
 -		return -EINVAL;
 -
 -retry:
 -	/* filename_lookup() drops it, keep a reference */
 -	ctx->filename->refcnt++;
 -
 -	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
 -				NULL);
 -	if (ret)
 -		goto err;
 -
 -	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
 -	path_put(&path);
 -	if (retry_estale(ret, lookup_flags)) {
 -		lookup_flags |= LOOKUP_REVAL;
 -		goto retry;
 -	}
 -	if (!ret)
 -		ret = cp_statx(&stat, ctx->buffer);
 -err:
 -	putname(ctx->filename);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	/*
 -	 * If we queue this for async, it must not be cancellable. That would
 -	 * leave the 'file' in an undeterminate state.
 -	 */
 -	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
 -
 -	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
 -	    sqe->rw_flags || sqe->buf_index)
 -		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -
 -	req->close.fd = READ_ONCE(sqe->fd);
 -	if (req->file->f_op == &io_uring_fops ||
 -	    req->close.fd == req->ctx->ring_fd)
 -		return -EBADF;
 -
 -	return 0;
 -}
 -
 -/* only called when __close_fd_get_file() is done */
 -static void __io_close_finish(struct io_kiocb *req)
 -{
 -	int ret;
 -
 -	ret = filp_close(req->close.put_file, req->work.files);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	fput(req->close.put_file);
 -	io_put_req(req);
 -}
 -
 -static void io_close_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -
 -	/* not cancellable, don't do io_req_cancelled() */
 -	__io_close_finish(req);
 -	io_steal_work(req, workptr);
 -}
 -
 -static int io_close(struct io_kiocb *req, bool force_nonblock)
 -{
 -	int ret;
 -
 -	req->close.put_file = NULL;
 -	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
 -	if (ret < 0)
 -		return ret;
 -
 -	/* if the file has a flush method, be safe and punt to async */
 -	if (req->close.put_file->f_op->flush && force_nonblock) {
 -		/* submission ref will be dropped, take it for async */
 -		refcount_inc(&req->refs);
 -
 -		req->work.func = io_close_finish;
 -		/*
 -		 * Do manual async queue here to avoid grabbing files - we don't
 -		 * need the files, and it'll cause io_close_finish() to close
 -		 * the file again and cause a double CQE entry for this request
 -		 */
 -		io_queue_async_work(req);
 -		return 0;
 -	}
 -
 -	/*
 -	 * No ->flush(), safely close from here and just punt the
 -	 * fput() to async context.
 -	 */
 -	__io_close_finish(req);
 -	return 0;
 -}
 -
 -static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	if (!req->file)
 -		return -EBADF;
 -
 -	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
 -		return -EINVAL;
 -
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->len);
 -	req->sync.flags = READ_ONCE(sqe->sync_range_flags);
 -	return 0;
 -}
 -
 -static void __io_sync_file_range(struct io_kiocb *req)
 -{
 -	int ret;
 -
 -	ret = sync_file_range(req->file, req->sync.off, req->sync.len,
 -				req->sync.flags);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -}
 -
 -
 -static void io_sync_file_range_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_sync_file_range(req);
 -	io_put_req(req); /* put submission ref */
 -	if (nxt)
 -		io_wq_assign_next(workptr, nxt);
 -}
 -
 -static int io_sync_file_range(struct io_kiocb *req, bool force_nonblock)
 -{
 -	/* sync_file_range always requires a blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_sync_file_range_finish;
 -		return -EAGAIN;
 -	}
 -
 -	__io_sync_file_range(req);
 -	return 0;
 -}
 -
 -static int io_setup_async_msg(struct io_kiocb *req,
 -			      struct io_async_msghdr *kmsg)
 -{
 -	if (req->io)
 -		return -EAGAIN;
 -	if (io_alloc_async_ctx(req)) {
 -		if (kmsg->iov != kmsg->fast_iov)
 -			kfree(kmsg->iov);
 -		return -ENOMEM;
 -	}
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	memcpy(&req->io->msg, kmsg, sizeof(*kmsg));
 -	return -EAGAIN;
 -}
 -
 -static int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct io_async_ctx *io = req->io;
 -	int ret;
 -
 -	sr->msg_flags = READ_ONCE(sqe->msg_flags);
 -	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	sr->len = READ_ONCE(sqe->len);
 -
 -#ifdef CONFIG_COMPAT
 -	if (req->ctx->compat)
 -		sr->msg_flags |= MSG_CMSG_COMPAT;
 -#endif
 -
 -	if (!io || req->opcode == IORING_OP_SEND)
 -		return 0;
 -	/* iovec is already imported */
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -
 -	io->msg.iov = io->msg.fast_iov;
 -	ret = sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
 -					&io->msg.iov);
 -	if (!ret)
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -	return ret;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_sendmsg(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_msghdr *kmsg = NULL;
 -	struct socket *sock;
 -	int ret;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -
++=======
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
++		struct io_buffer *kbuf;
+ 		struct io_async_ctx io;
+ 		unsigned flags;
+ 
+ 		if (req->io) {
+ 			kmsg = &req->io->msg;
+ 			kmsg->msg.msg_name = &req->io->msg.addr;
+ 			/* if iov is set, it's allocated already */
+ 			if (!kmsg->iov)
+ 				kmsg->iov = kmsg->fast_iov;
+ 			kmsg->msg.msg_iter.iov = kmsg->iov;
+ 		} else {
 -			struct io_sr_msg *sr = &req->sr_msg;
 -
+ 			kmsg = &io.msg;
+ 			kmsg->msg.msg_name = &io.msg.addr;
+ 
 -			io.msg.iov = io.msg.fast_iov;
 -			ret = sendmsg_copy_msghdr(&io.msg.msg, sr->msg,
 -					sr->msg_flags, &io.msg.iov);
++			ret = io_recvmsg_copy_hdr(req, &io);
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
++		kbuf = io_recv_buffer_select(req, &cflags, !force_nonblock);
++		if (IS_ERR(kbuf)) {
++			return PTR_ERR(kbuf);
++		} else if (kbuf) {
++			kmsg->fast_iov[0].iov_base = u64_to_user_ptr(kbuf->addr);
++			iov_iter_init(&kmsg->msg.msg_iter, READ, kmsg->iov,
++					1, req->sr_msg.len);
++		}
++
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
 -		ret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);
++		ret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.msg,
++						kmsg->uaddr, flags);
+ 		if (force_nonblock && ret == -EAGAIN)
+ 			return io_setup_async_msg(req, kmsg);
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
++>>>>>>> 52de1fe12240 (io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG)
  	}
 +	spin_unlock_irq(&ctx->completion_lock);
  
++<<<<<<< HEAD
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
++=======
+ 	if (kmsg && kmsg->iov != kmsg->fast_iov)
+ 		kfree(kmsg->iov);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	io_cqring_add_event(req, ret);
++	__io_cqring_add_event(req, ret, cflags);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
 -static int io_send(struct io_kiocb *req, bool force_nonblock)
++static int io_recv(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
++	struct io_buffer *kbuf = NULL;
+ 	struct socket *sock;
 -	int ret;
++	int ret, cflags = 0;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_sr_msg *sr = &req->sr_msg;
++		void __user *buf = sr->buf;
+ 		struct msghdr msg;
+ 		struct iovec iov;
+ 		unsigned flags;
+ 
 -		ret = import_single_range(WRITE, sr->buf, sr->len, &iov,
++		kbuf = io_recv_buffer_select(req, &cflags, !force_nonblock);
++		if (IS_ERR(kbuf))
++			return PTR_ERR(kbuf);
++		else if (kbuf)
++			buf = u64_to_user_ptr(kbuf->addr);
++
++		ret = import_single_range(READ, buf, sr->len, &iov,
+ 						&msg.msg_iter);
 -		if (ret)
++		if (ret) {
++			kfree(kbuf);
+ 			return ret;
++		}
+ 
++		req->flags |= REQ_F_NEED_CLEANUP;
+ 		msg.msg_name = NULL;
+ 		msg.msg_control = NULL;
+ 		msg.msg_controllen = 0;
+ 		msg.msg_namelen = 0;
++		msg.msg_iocb = NULL;
++		msg.msg_flags = 0;
+ 
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
 -		msg.msg_flags = flags;
 -		ret = sock_sendmsg(sock, &msg);
++		ret = sock_recvmsg(sock, &msg, flags);
+ 		if (force_nonblock && ret == -EAGAIN)
+ 			return -EAGAIN;
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
+ 	}
+ 
 -	io_cqring_add_event(req, ret);
++	kfree(kbuf);
++	req->flags &= ~REQ_F_NEED_CLEANUP;
++	__io_cqring_add_event(req, ret, cflags);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
 -static int __io_recvmsg_copy_hdr(struct io_kiocb *req, struct io_async_ctx *io)
++
++static int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct iovec __user *uiov;
 -	size_t iov_len;
 -	int ret;
++#if defined(CONFIG_NET)
++	struct io_accept *accept = &req->accept;
+ 
 -	ret = __copy_msghdr_from_user(&io->msg.msg, sr->msg, &io->msg.uaddr,
 -					&uiov, &iov_len);
 -	if (ret)
 -		return ret;
++	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
++		return -EINVAL;
++	if (sqe->ioprio || sqe->len || sqe->buf_index)
++		return -EINVAL;
+ 
 -	if (req->flags & REQ_F_BUFFER_SELECT) {
 -		if (iov_len > 1)
 -			return -EINVAL;
 -		if (copy_from_user(io->msg.iov, uiov, sizeof(*uiov)))
 -			return -EFAULT;
 -		sr->len = io->msg.iov[0].iov_len;
 -		iov_iter_init(&io->msg.msg.msg_iter, READ, io->msg.iov, 1,
 -				sr->len);
 -		io->msg.iov = NULL;
 -	} else {
 -		ret = import_iovec(READ, uiov, iov_len, UIO_FASTIOV,
 -					&io->msg.iov, &io->msg.msg.msg_iter);
 -		if (ret > 0)
 -			ret = 0;
 -	}
 -
 -	return ret;
 -}
 -
 -#ifdef CONFIG_COMPAT
 -static int __io_compat_recvmsg_copy_hdr(struct io_kiocb *req,
 -					struct io_async_ctx *io)
 -{
 -	struct compat_msghdr __user *msg_compat;
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct compat_iovec __user *uiov;
 -	compat_uptr_t ptr;
 -	compat_size_t len;
 -	int ret;
 -
 -	msg_compat = (struct compat_msghdr __user *) sr->msg;
 -	ret = __get_compat_msghdr(&io->msg.msg, msg_compat, &io->msg.uaddr,
 -					&ptr, &len);
 -	if (ret)
 -		return ret;
 -
 -	uiov = compat_ptr(ptr);
 -	if (req->flags & REQ_F_BUFFER_SELECT) {
 -		compat_ssize_t clen;
 -
 -		if (len > 1)
 -			return -EINVAL;
 -		if (!access_ok(uiov, sizeof(*uiov)))
 -			return -EFAULT;
 -		if (__get_user(clen, &uiov->iov_len))
 -			return -EFAULT;
 -		if (clen < 0)
 -			return -EINVAL;
 -		sr->len = io->msg.iov[0].iov_len;
 -		io->msg.iov = NULL;
 -	} else {
 -		ret = compat_import_iovec(READ, uiov, len, UIO_FASTIOV,
 -						&io->msg.iov,
 -						&io->msg.msg.msg_iter);
 -		if (ret < 0)
 -			return ret;
 -	}
 -
 -	return 0;
 -}
 -#endif
 -
 -static int io_recvmsg_copy_hdr(struct io_kiocb *req, struct io_async_ctx *io)
 -{
 -	io->msg.iov = io->msg.fast_iov;
 -
 -#ifdef CONFIG_COMPAT
 -	if (req->ctx->compat)
 -		return __io_compat_recvmsg_copy_hdr(req, io);
 -#endif
 -
 -	return __io_recvmsg_copy_hdr(req, io);
 -}
 -
 -static struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,
 -					       int *cflags, bool needs_lock)
 -{
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct io_buffer *kbuf;
 -
 -	if (!(req->flags & REQ_F_BUFFER_SELECT))
 -		return NULL;
 -
 -	kbuf = io_buffer_select(req, &sr->len, sr->bgid, sr->kbuf, needs_lock);
 -	if (IS_ERR(kbuf))
 -		return kbuf;
 -
 -	sr->kbuf = kbuf;
 -	req->flags |= REQ_F_BUFFER_SELECTED;
 -
 -	*cflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;
 -	*cflags |= IORING_CQE_F_BUFFER;
 -	return kbuf;
 -}
 -
 -static int io_recvmsg_prep(struct io_kiocb *req,
 -			   const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct io_async_ctx *io = req->io;
 -	int ret;
 -
 -	sr->msg_flags = READ_ONCE(sqe->msg_flags);
 -	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	sr->len = READ_ONCE(sqe->len);
 -	sr->bgid = READ_ONCE(sqe->buf_group);
 -
 -#ifdef CONFIG_COMPAT
 -	if (req->ctx->compat)
 -		sr->msg_flags |= MSG_CMSG_COMPAT;
 -#endif
 -
 -	if (!io || req->opcode == IORING_OP_RECV)
 -		return 0;
 -	/* iovec is already imported */
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -
 -	ret = io_recvmsg_copy_hdr(req, io);
 -	if (!ret)
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -	return ret;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_recvmsg(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_msghdr *kmsg = NULL;
 -	struct socket *sock;
 -	int ret, cflags = 0;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct io_buffer *kbuf;
 -		struct io_async_ctx io;
 -		unsigned flags;
 -
 -		if (req->io) {
 -			kmsg = &req->io->msg;
 -			kmsg->msg.msg_name = &req->io->msg.addr;
 -			/* if iov is set, it's allocated already */
 -			if (!kmsg->iov)
 -				kmsg->iov = kmsg->fast_iov;
 -			kmsg->msg.msg_iter.iov = kmsg->iov;
 -		} else {
 -			kmsg = &io.msg;
 -			kmsg->msg.msg_name = &io.msg.addr;
 -
 -			ret = io_recvmsg_copy_hdr(req, &io);
 -			if (ret)
 -				return ret;
 -		}
 -
 -		kbuf = io_recv_buffer_select(req, &cflags, !force_nonblock);
 -		if (IS_ERR(kbuf)) {
 -			return PTR_ERR(kbuf);
 -		} else if (kbuf) {
 -			kmsg->fast_iov[0].iov_base = u64_to_user_ptr(kbuf->addr);
 -			iov_iter_init(&kmsg->msg.msg_iter, READ, kmsg->iov,
 -					1, req->sr_msg.len);
 -		}
 -
 -		flags = req->sr_msg.msg_flags;
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 -
 -		ret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.msg,
 -						kmsg->uaddr, flags);
 -		if (force_nonblock && ret == -EAGAIN)
 -			return io_setup_async_msg(req, kmsg);
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 -	}
 -
 -	if (kmsg && kmsg->iov != kmsg->fast_iov)
 -		kfree(kmsg->iov);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	__io_cqring_add_event(req, ret, cflags);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_recv(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_buffer *kbuf = NULL;
 -	struct socket *sock;
 -	int ret, cflags = 0;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct io_sr_msg *sr = &req->sr_msg;
 -		void __user *buf = sr->buf;
 -		struct msghdr msg;
 -		struct iovec iov;
 -		unsigned flags;
 -
 -		kbuf = io_recv_buffer_select(req, &cflags, !force_nonblock);
 -		if (IS_ERR(kbuf))
 -			return PTR_ERR(kbuf);
 -		else if (kbuf)
 -			buf = u64_to_user_ptr(kbuf->addr);
 -
 -		ret = import_single_range(READ, buf, sr->len, &iov,
 -						&msg.msg_iter);
 -		if (ret) {
 -			kfree(kbuf);
 -			return ret;
 -		}
 -
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -		msg.msg_name = NULL;
 -		msg.msg_control = NULL;
 -		msg.msg_controllen = 0;
 -		msg.msg_namelen = 0;
 -		msg.msg_iocb = NULL;
 -		msg.msg_flags = 0;
 -
 -		flags = req->sr_msg.msg_flags;
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 -
 -		ret = sock_recvmsg(sock, &msg, flags);
 -		if (force_nonblock && ret == -EAGAIN)
 -			return -EAGAIN;
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 -	}
 -
 -	kfree(kbuf);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	__io_cqring_add_event(req, ret, cflags);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -
 -static int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_accept *accept = &req->accept;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -
 -	accept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	accept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	accept->flags = READ_ONCE(sqe->accept_flags);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -#if defined(CONFIG_NET)
 -static int __io_accept(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct io_accept *accept = &req->accept;
 -	unsigned file_flags;
 -	int ret;
 -
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -	ret = __sys_accept4_file(req->file, file_flags, accept->addr,
 -					accept->addr_len, accept->flags);
 -	if (ret == -EAGAIN && force_nonblock)
 -		return -EAGAIN;
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static void io_accept_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_accept(req, false);
 -	io_steal_work(req, workptr);
 -}
 -#endif
 -
 -static int io_accept(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	int ret;
 -
 -	ret = __io_accept(req, force_nonblock);
 -	if (ret == -EAGAIN && force_nonblock) {
 -		req->work.func = io_accept_finish;
 -		return -EAGAIN;
 -	}
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_connect *conn = &req->connect;
 -	struct io_async_ctx *io = req->io;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 -
 -	conn->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	conn->addr_len =  READ_ONCE(sqe->addr2);
 -
 -	if (!io)
 -		return 0;
 -
 -	return move_addr_to_kernel(conn->addr, conn->addr_len,
 -					&io->connect.address);
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_connect(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_ctx __io, *io;
 -	unsigned file_flags;
 -	int ret;
 -
 -	if (req->io) {
 -		io = req->io;
 -	} else {
 -		ret = move_addr_to_kernel(req->connect.addr,
 -						req->connect.addr_len,
 -						&__io.connect.address);
 -		if (ret)
 -			goto out;
 -		io = &__io;
 -	}
 -
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	ret = __sys_connect_file(req->file, &io->connect.address,
 -					req->connect.addr_len, file_flags);
 -	if ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {
 -		if (req->io)
 -			return -EAGAIN;
 -		if (io_alloc_async_ctx(req)) {
 -			ret = -ENOMEM;
 -			goto out;
 -		}
 -		memcpy(&req->io->connect, &__io.connect, sizeof(__io.connect));
 -		return -EAGAIN;
 -	}
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -out:
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -struct io_poll_table {
 -	struct poll_table_struct pt;
 -	struct io_kiocb *req;
 -	int error;
 -};
 -
 -static void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,
 -			    struct wait_queue_head *head)
 -{
 -	if (unlikely(poll->head)) {
 -		pt->error = -EINVAL;
 -		return;
 -	}
 -
 -	pt->error = 0;
 -	poll->head = head;
 -	add_wait_queue(head, &poll->wait);
 -}
 -
 -static void io_async_queue_proc(struct file *file, struct wait_queue_head *head,
 -			       struct poll_table_struct *p)
 -{
 -	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
 -
 -	__io_queue_proc(&pt->req->apoll->poll, pt, head);
 -}
 -
 -static int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,
 -			   __poll_t mask, task_work_func_t func)
 -{
 -	struct task_struct *tsk;
 -
 -	/* for instances that support it check for an event match first: */
 -	if (mask && !(mask & poll->events))
 -		return 0;
 -
 -	trace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);
 -
 -	list_del_init(&poll->wait.entry);
 -
 -	tsk = req->task;
 -	req->result = mask;
 -	init_task_work(&req->task_work, func);
 -	/*
 -	 * If this fails, then the task is exiting. If that is the case, then
 -	 * the exit check will ultimately cancel these work items. Hence we
 -	 * don't need to check here and handle it specifically.
 -	 */
 -	task_work_add(tsk, &req->task_work, true);
 -	wake_up_process(tsk);
 -	return 1;
 -}
 -
 -static void io_async_task_func(struct callback_head *cb)
 -{
 -	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
 -	struct async_poll *apoll = req->apoll;
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	trace_io_uring_task_run(req->ctx, req->opcode, req->user_data);
 -
 -	WARN_ON_ONCE(!list_empty(&req->apoll->poll.wait.entry));
 -
 -	if (hash_hashed(&req->hash_node)) {
 -		spin_lock_irq(&ctx->completion_lock);
 -		hash_del(&req->hash_node);
 -		spin_unlock_irq(&ctx->completion_lock);
 -	}
 -
 -	/* restore ->work in case we need to retry again */
 -	memcpy(&req->work, &apoll->work, sizeof(req->work));
 -
 -	__set_current_state(TASK_RUNNING);
 -	mutex_lock(&ctx->uring_lock);
 -	__io_queue_sqe(req, NULL);
 -	mutex_unlock(&ctx->uring_lock);
 -
 -	kfree(apoll);
 -}
 -
 -static int io_async_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 -			void *key)
 -{
 -	struct io_kiocb *req = wait->private;
 -	struct io_poll_iocb *poll = &req->apoll->poll;
 -
 -	trace_io_uring_poll_wake(req->ctx, req->opcode, req->user_data,
 -					key_to_poll(key));
 -
 -	return __io_async_wake(req, poll, key_to_poll(key), io_async_task_func);
 -}
 -
 -static void io_poll_req_insert(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct hlist_head *list;
 -
 -	list = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];
 -	hlist_add_head(&req->hash_node, list);
 -}
 -
 -static __poll_t __io_arm_poll_handler(struct io_kiocb *req,
 -				      struct io_poll_iocb *poll,
 -				      struct io_poll_table *ipt, __poll_t mask,
 -				      wait_queue_func_t wake_func)
 -	__acquires(&ctx->completion_lock)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	bool cancel = false;
 -
 -	poll->file = req->file;
 -	poll->head = NULL;
 -	poll->done = poll->canceled = false;
 -	poll->events = mask;
 -
 -	ipt->pt._key = mask;
 -	ipt->req = req;
 -	ipt->error = -EINVAL;
 -
 -	INIT_LIST_HEAD(&poll->wait.entry);
 -	init_waitqueue_func_entry(&poll->wait, wake_func);
 -	poll->wait.private = req;
 -
 -	mask = vfs_poll(req->file, &ipt->pt) & poll->events;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (likely(poll->head)) {
 -		spin_lock(&poll->head->lock);
 -		if (unlikely(list_empty(&poll->wait.entry))) {
 -			if (ipt->error)
 -				cancel = true;
 -			ipt->error = 0;
 -			mask = 0;
 -		}
 -		if (mask || ipt->error)
 -			list_del_init(&poll->wait.entry);
 -		else if (cancel)
 -			WRITE_ONCE(poll->canceled, true);
 -		else if (!poll->done) /* actually waiting for an event */
 -			io_poll_req_insert(req);
 -		spin_unlock(&poll->head->lock);
 -	}
 -
 -	return mask;
 -}
 -
 -static bool io_arm_poll_handler(struct io_kiocb *req)
 -{
 -	const struct io_op_def *def = &io_op_defs[req->opcode];
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct async_poll *apoll;
 -	struct io_poll_table ipt;
 -	__poll_t mask, ret;
 -
 -	if (!req->file || !file_can_poll(req->file))
 -		return false;
 -	if (req->flags & (REQ_F_MUST_PUNT | REQ_F_POLLED))
 -		return false;
 -	if (!def->pollin && !def->pollout)
 -		return false;
 -
 -	apoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);
 -	if (unlikely(!apoll))
 -		return false;
 -
 -	req->flags |= REQ_F_POLLED;
 -	memcpy(&apoll->work, &req->work, sizeof(req->work));
 -
 -	/*
 -	 * Don't need a reference here, as we're adding it to the task
 -	 * task_works list. If the task exits, the list is pruned.
 -	 */
 -	req->task = current;
 -	req->apoll = apoll;
 -	INIT_HLIST_NODE(&req->hash_node);
 -
 -	mask = 0;
 -	if (def->pollin)
 -		mask |= POLLIN | POLLRDNORM;
 -	if (def->pollout)
 -		mask |= POLLOUT | POLLWRNORM;
 -	mask |= POLLERR | POLLPRI;
 -
 -	ipt.pt._qproc = io_async_queue_proc;
 -
 -	ret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,
 -					io_async_wake);
 -	if (ret) {
 -		ipt.error = 0;
 -		apoll->poll.done = true;
 -		spin_unlock_irq(&ctx->completion_lock);
 -		memcpy(&req->work, &apoll->work, sizeof(req->work));
 -		kfree(apoll);
 -		return false;
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -	trace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,
 -					apoll->poll.events);
 -	return true;
 -}
 -
 -static bool __io_poll_remove_one(struct io_kiocb *req,
 -				 struct io_poll_iocb *poll)
 -{
 -	bool do_complete = false;
 -
 -	spin_lock(&poll->head->lock);
 -	WRITE_ONCE(poll->canceled, true);
 -	if (!list_empty(&poll->wait.entry)) {
 -		list_del_init(&poll->wait.entry);
 -		do_complete = true;
 -	}
 -	spin_unlock(&poll->head->lock);
 -	return do_complete;
 -}
 -
 -static bool io_poll_remove_one(struct io_kiocb *req)
 -{
 -	bool do_complete;
 -
 -	if (req->opcode == IORING_OP_POLL_ADD) {
 -		do_complete = __io_poll_remove_one(req, &req->poll);
 -	} else {
 -		/* non-poll requests have submit ref still */
 -		do_complete = __io_poll_remove_one(req, &req->apoll->poll);
 -		if (do_complete)
 -			io_put_req(req);
 -	}
 -
 -	hash_del(&req->hash_node);
 -
 -	if (do_complete) {
 -		io_cqring_fill_event(req, -ECANCELED);
 -		io_commit_cqring(req->ctx);
 -		req->flags |= REQ_F_COMP_LOCKED;
 -		io_put_req(req);
 -	}
 -
 -	return do_complete;
 -}
 -
 -static void io_poll_remove_all(struct io_ring_ctx *ctx)
 -{
 -	struct hlist_node *tmp;
 -	struct io_kiocb *req;
 -	int i;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	for (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {
 -		struct hlist_head *list;
 -
 -		list = &ctx->cancel_hash[i];
 -		hlist_for_each_entry_safe(req, tmp, list, hash_node)
 -			io_poll_remove_one(req);
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	io_cqring_ev_posted(ctx);
 -}
 -
 -static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)
 -{
 -	struct hlist_head *list;
 -	struct io_kiocb *req;
 -
 -	list = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];
 -	hlist_for_each_entry(req, list, hash_node) {
 -		if (sqe_addr != req->user_data)
 -			continue;
 -		if (io_poll_remove_one(req))
 -			return 0;
 -		return -EALREADY;
 -	}
 -
 -	return -ENOENT;
 -}
 -
 -static int io_poll_remove_prep(struct io_kiocb *req,
 -			       const struct io_uring_sqe *sqe)
 -{
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 -	    sqe->poll_events)
 -		return -EINVAL;
 -
 -	req->poll.addr = READ_ONCE(sqe->addr);
++	accept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
++	accept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));
++	accept->flags = READ_ONCE(sqe->accept_flags);
+ 	return 0;
++#else
++	return -EOPNOTSUPP;
++#endif
+ }
+ 
 -/*
 - * Find a running poll command that matches one specified in sqe->addr,
 - * and remove it if found.
 - */
 -static int io_poll_remove(struct io_kiocb *req)
++#if defined(CONFIG_NET)
++static int __io_accept(struct io_kiocb *req, bool force_nonblock)
+ {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	u64 addr;
++	struct io_accept *accept = &req->accept;
++	unsigned file_flags;
+ 	int ret;
+ 
 -	addr = req->poll.addr;
 -	spin_lock_irq(&ctx->completion_lock);
 -	ret = io_poll_cancel(ctx, addr);
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	io_cqring_add_event(req, ret);
++	file_flags = force_nonblock ? O_NONBLOCK : 0;
++	ret = __sys_accept4_file(req->file, file_flags, accept->addr,
++					accept->addr_len, accept->flags);
++	if (ret == -EAGAIN && force_nonblock)
++		return -EAGAIN;
++	if (ret == -ERESTARTSYS)
++		ret = -EINTR;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
++	io_cqring_add_event(req, ret);
++>>>>>>> 52de1fe12240 (io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG)
  	io_put_req(req);
  	return 0;
  }
@@@ -1869,51 -4863,262 +2377,89 @@@ static int io_req_defer(struct io_ring_
  	return -EIOCBQUEUED;
  }
  
 -static void io_cleanup_req(struct io_kiocb *req)
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_async_ctx *io = req->io;
 +	int ret, opcode;
  
++<<<<<<< HEAD
 +	req->user_data = READ_ONCE(s->sqe->user_data);
++=======
+ 	switch (req->opcode) {
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 		if (req->flags & REQ_F_BUFFER_SELECTED)
+ 			kfree((void *)(unsigned long)req->rw.addr);
+ 		/* fallthrough */
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 	case IORING_OP_WRITE:
+ 		if (io->rw.iov != io->rw.fast_iov)
+ 			kfree(io->rw.iov);
+ 		break;
+ 	case IORING_OP_RECVMSG:
+ 		if (req->flags & REQ_F_BUFFER_SELECTED)
+ 			kfree(req->sr_msg.kbuf);
+ 		/* fallthrough */
+ 	case IORING_OP_SENDMSG:
+ 		if (io->msg.iov != io->msg.fast_iov)
+ 			kfree(io->msg.iov);
+ 		break;
+ 	case IORING_OP_RECV:
+ 		if (req->flags & REQ_F_BUFFER_SELECTED)
+ 			kfree(req->sr_msg.kbuf);
+ 		break;
+ 	case IORING_OP_OPENAT:
+ 	case IORING_OP_OPENAT2:
+ 	case IORING_OP_STATX:
+ 		putname(req->open.filename);
+ 		break;
+ 	case IORING_OP_SPLICE:
+ 		io_put_file(req, req->splice.file_in,
+ 			    (req->splice.flags & SPLICE_F_FD_IN_FIXED));
+ 		break;
+ 	}
++>>>>>>> 52de1fe12240 (io_uring: add IOSQE_BUFFER_SELECT support for IORING_OP_RECVMSG)
  
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -}
 -
 -static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			bool force_nonblock)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	switch (req->opcode) {
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
  	case IORING_OP_NOP:
 -		ret = io_nop(req);
 +		ret = io_nop(req, req->user_data);
  		break;
  	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -	case IORING_OP_READ:
 -		if (sqe) {
 -			ret = io_read_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_read(req, force_nonblock);
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
  		break;
  	case IORING_OP_WRITEV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_READ_FIXED:
 +		ret = io_read(req, s, force_nonblock);
 +		break;
  	case IORING_OP_WRITE_FIXED:
 -	case IORING_OP_WRITE:
 -		if (sqe) {
 -			ret = io_write_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_write(req, force_nonblock);
 +		ret = io_write(req, s, force_nonblock);
  		break;
  	case IORING_OP_FSYNC:
 -		if (sqe) {
 -			ret = io_prep_fsync(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_fsync(req, force_nonblock);
 +		ret = io_fsync(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_POLL_ADD:
 -		if (sqe) {
 -			ret = io_poll_add_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_poll_add(req);
 +		ret = io_poll_add(req, s->sqe);
  		break;
  	case IORING_OP_POLL_REMOVE:
 -		if (sqe) {
 -			ret = io_poll_remove_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_poll_remove(req);
 +		ret = io_poll_remove(req, s->sqe);
  		break;
  	case IORING_OP_SYNC_FILE_RANGE:
 -		if (sqe) {
 -			ret = io_prep_sfr(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_sync_file_range(req, force_nonblock);
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_SENDMSG:
 -	case IORING_OP_SEND:
 -		if (sqe) {
 -			ret = io_sendmsg_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_SENDMSG)
 -			ret = io_sendmsg(req, force_nonblock);
 -		else
 -			ret = io_send(req, force_nonblock);
 -		break;
 -	case IORING_OP_RECVMSG:
 -	case IORING_OP_RECV:
 -		if (sqe) {
 -			ret = io_recvmsg_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_RECVMSG)
 -			ret = io_recvmsg(req, force_nonblock);
 -		else
 -			ret = io_recv(req, force_nonblock);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		if (sqe) {
 -			ret = io_timeout_prep(req, sqe, false);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout(req);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		if (sqe) {
 -			ret = io_timeout_remove_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout_remove(req);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		if (sqe) {
 -			ret = io_accept_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_accept(req, force_nonblock);
 -		break;
 -	case IORING_OP_CONNECT:
 -		if (sqe) {
 -			ret = io_connect_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_connect(req, force_nonblock);
 -		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		if (sqe) {
 -			ret = io_async_cancel_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_async_cancel(req);
 -		break;
 -	case IORING_OP_FALLOCATE:
 -		if (sqe) {
 -			ret = io_fallocate_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fallocate(req, force_nonblock);
 -		break;
 -	case IORING_OP_OPENAT:
 -		if (sqe) {
 -			ret = io_openat_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat(req, force_nonblock);
 -		break;
 -	case IORING_OP_CLOSE:
 -		if (sqe) {
 -			ret = io_close_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_close(req, force_nonblock);
 -		break;
 -	case IORING_OP_FILES_UPDATE:
 -		if (sqe) {
 -			ret = io_files_update_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_files_update(req, force_nonblock);
 -		break;
 -	case IORING_OP_STATX:
 -		if (sqe) {
 -			ret = io_statx_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_statx(req, force_nonblock);
 -		break;
 -	case IORING_OP_FADVISE:
 -		if (sqe) {
 -			ret = io_fadvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fadvise(req, force_nonblock);
 -		break;
 -	case IORING_OP_MADVISE:
 -		if (sqe) {
 -			ret = io_madvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_madvise(req, force_nonblock);
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
  		break;
 -	case IORING_OP_OPENAT2:
 -		if (sqe) {
 -			ret = io_openat2_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat2(req, force_nonblock);
 -		break;
 -	case IORING_OP_EPOLL_CTL:
 -		if (sqe) {
 -			ret = io_epoll_ctl_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_epoll_ctl(req, force_nonblock);
 -		break;
 -	case IORING_OP_SPLICE:
 -		if (sqe) {
 -			ret = io_splice_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_splice(req, force_nonblock);
 -		break;
 -	case IORING_OP_PROVIDE_BUFFERS:
 -		if (sqe) {
 -			ret = io_provide_buffers_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_provide_buffers(req, force_nonblock);
 +	case IORING_OP_RECVMSG:
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
  		break;
  	default:
  		ret = -EINVAL;
* Unmerged path fs/io_uring.c
