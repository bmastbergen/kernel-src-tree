bpf: Fix memory leaks in generic update/delete batch ops

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Brian Vazquez <brianvv@google.com>
commit 2e3a94aa2bfc6de95a0700f0a868c6f5db3a9592
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/2e3a94aa.failed

Generic update/delete batch ops functions were using __bpf_copy_key
without properly freeing the memory. Handle the memory allocation and
copy_from_user separately.

Fixes: aa2e93b8e58e ("bpf: Add generic support for update and delete batch ops")
	Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
	Signed-off-by: Brian Vazquez <brianvv@google.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Yonghong Song <yhs@fb.com>
Link: https://lore.kernel.org/bpf/20200119194040.128369-1-brianvv@google.com
(cherry picked from commit 2e3a94aa2bfc6de95a0700f0a868c6f5db3a9592)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/syscall.c
diff --cc kernel/bpf/syscall.c
index 4aa6a47aa4b8,9a840c57f6df..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -1124,6 -1218,220 +1124,223 @@@ err_put
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ int generic_map_delete_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 cp, max_count;
+ 	int err = 0;
+ 	void *key;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!key)
+ 		return -ENOMEM;
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		err = -EFAULT;
+ 		if (copy_from_user(key, keys + cp * map->key_size,
+ 				   map->key_size))
+ 			break;
+ 
+ 		if (bpf_map_is_dev_bound(map)) {
+ 			err = bpf_map_offload_delete_elem(map, key);
+ 			break;
+ 		}
+ 
+ 		preempt_disable();
+ 		__this_cpu_inc(bpf_prog_active);
+ 		rcu_read_lock();
+ 		err = map->ops->map_delete_elem(map, key);
+ 		rcu_read_unlock();
+ 		__this_cpu_dec(bpf_prog_active);
+ 		preempt_enable();
+ 		maybe_wait_bpf_programs(map);
+ 		if (err)
+ 			break;
+ 	}
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 
+ 	kfree(key);
+ 	return err;
+ }
+ 
+ int generic_map_update_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 value_size, cp, max_count;
+ 	int ufd = attr->map_fd;
+ 	void *key, *value;
+ 	struct fd f;
+ 	int err = 0;
+ 
+ 	f = fdget(ufd);
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!key)
+ 		return -ENOMEM;
+ 
+ 	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!value) {
+ 		kfree(key);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		err = -EFAULT;
+ 		if (copy_from_user(key, keys + cp * map->key_size,
+ 		    map->key_size) ||
+ 		    copy_from_user(value, values + cp * value_size, value_size))
+ 			break;
+ 
+ 		err = bpf_map_update_value(map, f, key, value,
+ 					   attr->batch.elem_flags);
+ 
+ 		if (err)
+ 			break;
+ 	}
+ 
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 
+ 	kfree(value);
+ 	kfree(key);
+ 	return err;
+ }
+ 
+ #define MAP_LOOKUP_RETRIES 3
+ 
+ int generic_map_lookup_batch(struct bpf_map *map,
+ 				    const union bpf_attr *attr,
+ 				    union bpf_attr __user *uattr)
+ {
+ 	void __user *uobatch = u64_to_user_ptr(attr->batch.out_batch);
+ 	void __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	void *buf, *buf_prevkey, *prev_key, *key, *value;
+ 	int err, retry = MAP_LOOKUP_RETRIES;
+ 	u32 value_size, cp, max_count;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map))
+ 		return -EINVAL;
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	if (put_user(0, &uattr->batch.count))
+ 		return -EFAULT;
+ 
+ 	buf_prevkey = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf_prevkey)
+ 		return -ENOMEM;
+ 
+ 	buf = kmalloc(map->key_size + value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf) {
+ 		kvfree(buf_prevkey);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	err = -EFAULT;
+ 	prev_key = NULL;
+ 	if (ubatch && copy_from_user(buf_prevkey, ubatch, map->key_size))
+ 		goto free_buf;
+ 	key = buf;
+ 	value = key + map->key_size;
+ 	if (ubatch)
+ 		prev_key = buf_prevkey;
+ 
+ 	for (cp = 0; cp < max_count;) {
+ 		rcu_read_lock();
+ 		err = map->ops->map_get_next_key(map, prev_key, key);
+ 		rcu_read_unlock();
+ 		if (err)
+ 			break;
+ 		err = bpf_map_copy_value(map, key, value,
+ 					 attr->batch.elem_flags);
+ 
+ 		if (err == -ENOENT) {
+ 			if (retry) {
+ 				retry--;
+ 				continue;
+ 			}
+ 			err = -EINTR;
+ 			break;
+ 		}
+ 
+ 		if (err)
+ 			goto free_buf;
+ 
+ 		if (copy_to_user(keys + cp * map->key_size, key,
+ 				 map->key_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 		if (copy_to_user(values + cp * value_size, value, value_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 
+ 		if (!prev_key)
+ 			prev_key = buf_prevkey;
+ 
+ 		swap(prev_key, key);
+ 		retry = MAP_LOOKUP_RETRIES;
+ 		cp++;
+ 	}
+ 
+ 	if (err == -EFAULT)
+ 		goto free_buf;
+ 
+ 	if ((copy_to_user(&uattr->batch.count, &cp, sizeof(cp)) ||
+ 		    (cp && copy_to_user(uobatch, prev_key, map->key_size))))
+ 		err = -EFAULT;
+ 
+ free_buf:
+ 	kfree(buf_prevkey);
+ 	kfree(buf);
+ 	return err;
+ }
+ 
++>>>>>>> 2e3a94aa2bfc (bpf: Fix memory leaks in generic update/delete batch ops)
  #define BPF_MAP_LOOKUP_AND_DELETE_ELEM_LAST_FIELD value
  
  static int map_lookup_and_delete_elem(union bpf_attr *attr)
* Unmerged path kernel/bpf/syscall.c
