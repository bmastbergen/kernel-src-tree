io_uring: remove io_prep_next_work()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 3b17cf5a58f2a38e23ee980b5dece717d0464fb7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/3b17cf5a.failed

io-wq cares about IO_WQ_WORK_UNBOUND flag only while enqueueing, so
it's useless setting it for a next req of a link. Thus, removed it
from io_prep_linked_timeout(), and inline the function.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 3b17cf5a58f2a38e23ee980b5dece717d0464fb7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,fb8fe0bd5e18..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -451,36 -924,143 +451,51 @@@ static struct io_kiocb *io_get_deferred
  	return NULL;
  }
  
 -static struct io_kiocb *io_get_timeout_req(struct io_ring_ctx *ctx)
 -{
 -	struct io_kiocb *req;
 -
 -	req = list_first_entry_or_null(&ctx->timeout_list, struct io_kiocb, list);
 -	if (req) {
 -		if (req->flags & REQ_F_TIMEOUT_NOSEQ)
 -			return NULL;
 -		if (!__req_need_defer(req)) {
 -			list_del_init(&req->list);
 -			return req;
 -		}
 -	}
 -
 -	return NULL;
 -}
 -
  static void __io_commit_cqring(struct io_ring_ctx *ctx)
  {
 -	struct io_rings *rings = ctx->rings;
 -
 -	/* order cqe stores with ring update */
 -	smp_store_release(&rings->cq.tail, ctx->cached_cq_tail);
 +	struct io_cq_ring *ring = ctx->cq_ring;
  
 -	if (wq_has_sleeper(&ctx->cq_wait)) {
 -		wake_up_interruptible(&ctx->cq_wait);
 -		kill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);
 -	}
 -}
 +	if (ctx->cached_cq_tail != READ_ONCE(ring->r.tail)) {
 +		/* order cqe stores with ring update */
 +		smp_store_release(&ring->r.tail, ctx->cached_cq_tail);
  
 -static inline void io_req_work_grab_env(struct io_kiocb *req,
 -					const struct io_op_def *def)
 -{
 -	if (!req->work.mm && def->needs_mm) {
 -		mmgrab(current->mm);
 -		req->work.mm = current->mm;
 -	}
 -	if (!req->work.creds)
 -		req->work.creds = get_current_cred();
 -	if (!req->work.fs && def->needs_fs) {
 -		spin_lock(&current->fs->lock);
 -		if (!current->fs->in_exec) {
 -			req->work.fs = current->fs;
 -			req->work.fs->users++;
 -		} else {
 -			req->work.flags |= IO_WQ_WORK_CANCEL;
 +		if (wq_has_sleeper(&ctx->cq_wait)) {
 +			wake_up_interruptible(&ctx->cq_wait);
 +			kill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);
  		}
 -		spin_unlock(&current->fs->lock);
  	}
 -	if (!req->work.task_pid)
 -		req->work.task_pid = task_pid_vnr(current);
  }
  
 -static inline void io_req_work_drop_env(struct io_kiocb *req)
++<<<<<<< HEAD
 +static inline void io_queue_async_work(struct io_ring_ctx *ctx,
 +				       struct io_kiocb *req)
  {
 -	if (req->work.mm) {
 -		mmdrop(req->work.mm);
 -		req->work.mm = NULL;
 -	}
 -	if (req->work.creds) {
 -		put_cred(req->work.creds);
 -		req->work.creds = NULL;
 -	}
 -	if (req->work.fs) {
 -		struct fs_struct *fs = req->work.fs;
 -
 -		spin_lock(&req->work.fs->lock);
 -		if (--fs->users)
 -			fs = NULL;
 -		spin_unlock(&req->work.fs->lock);
 -		if (fs)
 -			free_fs_struct(fs);
 -	}
 -}
 +	int rw = 0;
  
 +	if (req->submit.sqe) {
 +		switch (req->submit.sqe->opcode) {
 +		case IORING_OP_WRITEV:
 +		case IORING_OP_WRITE_FIXED:
 +			rw = !(req->rw.ki_flags & IOCB_DIRECT);
 +			break;
 +		}
++=======
+ static inline bool io_prep_async_work(struct io_kiocb *req,
+ 				      struct io_kiocb **link)
+ {
+ 	const struct io_op_def *def = &io_op_defs[req->opcode];
+ 	bool do_hashed = false;
+ 
+ 	if (req->flags & REQ_F_ISREG) {
+ 		if (def->hash_reg_file)
+ 			do_hashed = true;
+ 	} else {
+ 		if (def->unbound_nonreg_file)
+ 			req->work.flags |= IO_WQ_WORK_UNBOUND;
++>>>>>>> 3b17cf5a58f2 (io_uring: remove io_prep_next_work())
  	}
  
 -	io_req_work_grab_env(req, def);
 -
 -	*link = io_prep_linked_timeout(req);
 -	return do_hashed;
 -}
 -
 -static inline void io_queue_async_work(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_kiocb *link;
 -	bool do_hashed;
 -
 -	do_hashed = io_prep_async_work(req, &link);
 -
 -	trace_io_uring_queue_async_work(ctx, do_hashed, req, &req->work,
 -					req->flags);
 -	if (!do_hashed) {
 -		io_wq_enqueue(ctx->io_wq, &req->work);
 -	} else {
 -		io_wq_enqueue_hashed(ctx->io_wq, &req->work,
 -					file_inode(req->file));
 -	}
 -
 -	if (link)
 -		io_queue_linked_timeout(link);
 -}
 -
 -static void io_kill_timeout(struct io_kiocb *req)
 -{
 -	int ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
 -	if (ret != -1) {
 -		atomic_inc(&req->ctx->cq_timeouts);
 -		list_del_init(&req->list);
 -		io_cqring_fill_event(req, 0);
 -		io_put_req(req);
 -	}
 -}
 -
 -static void io_kill_timeouts(struct io_ring_ctx *ctx)
 -{
 -	struct io_kiocb *req, *tmp;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	list_for_each_entry_safe(req, tmp, &ctx->timeout_list, list)
 -		io_kill_timeout(req);
 -	spin_unlock_irq(&ctx->completion_lock);
 +	queue_work(ctx->sqo_wq[rw], &req->work);
  }
  
  static void io_commit_cqring(struct io_ring_ctx *ctx)
@@@ -1487,35 -2539,118 +1502,98 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++=======
+ static bool io_req_cancelled(struct io_kiocb *req)
+ {
+ 	if (req->work.flags & IO_WQ_WORK_CANCEL) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, -ECANCELED);
+ 		io_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_link_work_cb(struct io_wq_work **workptr)
+ {
+ 	struct io_wq_work *work = *workptr;
+ 	struct io_kiocb *link = work->data;
+ 
+ 	io_queue_linked_timeout(link);
+ 	io_wq_submit_work(workptr);
+ }
+ 
+ static void io_wq_assign_next(struct io_wq_work **workptr, struct io_kiocb *nxt)
+ {
+ 	struct io_kiocb *link;
+ 
+ 	*workptr = &nxt->work;
+ 	link = io_prep_linked_timeout(nxt);
+ 	if (link) {
+ 		nxt->work.func = io_link_work_cb;
+ 		nxt->work.data = link;
+ 	}
+ }
+ 
+ static void __io_fsync(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	loff_t end = req->sync.off + req->sync.len;
+ 	int ret;
+ 
+ 	ret = vfs_fsync_range(req->file, req->sync.off,
+ 				end > 0 ? end : LLONG_MAX,
+ 				req->sync.flags & IORING_FSYNC_DATASYNC);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_fsync_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fsync(req, &nxt);
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_fsync(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 3b17cf5a58f2 (io_uring: remove io_prep_next_work())
  		    bool force_nonblock)
  {
 -	/* fsync always requires a blocking context */
 -	if (force_nonblock) {
 -		io_put_req(req);
 -		req->work.func = io_fsync_finish;
 -		return -EAGAIN;
 -	}
 -	__io_fsync(req, nxt);
 -	return 0;
 -}
 -
 -static void __io_fallocate(struct io_kiocb *req, struct io_kiocb **nxt)
 -{
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
  	int ret;
  
 -	if (io_req_cancelled(req))
 -		return;
 -
 -	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
 -				req->sync.len);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 -}
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
  
 -static void io_fallocate_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
  
 -	__io_fallocate(req, &nxt);
 -	if (nxt)
 -		io_wq_assign_next(workptr, nxt);
 -}
 +	/* fsync always requires a blocking context */
 +	if (force_nonblock)
 +		return -EAGAIN;
  
 -static int io_fallocate_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 +	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
 +				end > 0 ? end : LLONG_MAX,
 +				fsync_flags & IORING_FSYNC_DATASYNC);
  
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->addr);
 -	req->sync.mode = READ_ONCE(sqe->len);
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
  	return 0;
  }
  
* Unmerged path fs/io_uring.c
