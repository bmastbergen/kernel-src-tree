bpf: Add bpf_xdp_output() helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Eelco Chaudron <echaudro@redhat.com>
commit d831ee84bfc9173eecf30dbbc2553ae81b996c60
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/d831ee84.failed

Introduce new helper that reuses existing xdp perf_event output
implementation, but can be called from raw_tracepoint programs
that receive 'struct xdp_buff *' as a tracepoint argument.

	Signed-off-by: Eelco Chaudron <echaudro@redhat.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: John Fastabend <john.fastabend@gmail.com>
	Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
Link: https://lore.kernel.org/bpf/158348514556.2239.11050972434793741444.stgit@xdp-tutorial
(cherry picked from commit d831ee84bfc9173eecf30dbbc2553ae81b996c60)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	kernel/bpf/verifier.c
#	kernel/trace/bpf_trace.c
#	tools/include/uapi/linux/bpf.h
diff --cc include/uapi/linux/bpf.h
index c9871d53e313,5d01c5c7e598..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -2757,6 -2778,178 +2757,181 @@@ union bpf_attr 
   *		**-EOPNOTSUPP** kernel configuration does not enable SYN cookies
   *
   *		**-EPROTONOSUPPORT** IP packet version is not 4 or 6
++<<<<<<< HEAD
++=======
+  *
+  * int bpf_skb_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  * 	Description
+  * 		Write raw *data* blob into a special BPF perf event held by
+  * 		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  * 		event must have the following attributes: **PERF_SAMPLE_RAW**
+  * 		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  * 		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  * 		The *flags* are used to indicate the index in *map* for which
+  * 		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  * 		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  * 		to indicate that the index of the current CPU core should be
+  * 		used.
+  *
+  * 		The value to write, of *size*, is passed through eBPF stack and
+  * 		pointed by *data*.
+  *
+  * 		*ctx* is a pointer to in-kernel struct sk_buff.
+  *
+  * 		This helper is similar to **bpf_perf_event_output**\ () but
+  * 		restricted to raw_tracepoint bpf programs.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from user space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_kernel(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from kernel space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe user address
+  * 		*unsafe_ptr* to *dst*. The *size* should include the
+  * 		terminating NUL byte. In case the string length is smaller than
+  * 		*size*, the target is not padded with further NUL bytes. If the
+  * 		string length is larger than *size*, just *size*-1 bytes are
+  * 		copied and the last byte is set to NUL.
+  *
+  * 		On success, the length of the copied string is returned. This
+  * 		makes this helper useful in tracing programs for reading
+  * 		strings, and more importantly to get its length at runtime. See
+  * 		the following snippet:
+  *
+  * 		::
+  *
+  * 			SEC("kprobe/sys_open")
+  * 			void bpf_sys_open(struct pt_regs *ctx)
+  * 			{
+  * 			        char buf[PATHLEN]; // PATHLEN is defined to 256
+  * 			        int res = bpf_probe_read_user_str(buf, sizeof(buf),
+  * 				                                  ctx->di);
+  *
+  * 				// Consume buf, for example push it to
+  * 				// userspace via bpf_perf_event_output(); we
+  * 				// can use res (the string length) as event
+  * 				// size, after checking its boundaries.
+  * 			}
+  *
+  * 		In comparison, using **bpf_probe_read_user()** helper here
+  * 		instead to read the string would require to estimate the length
+  * 		at compile time, and would often result in copying more memory
+  * 		than necessary.
+  *
+  * 		Another useful use case is when parsing individual process
+  * 		arguments or individual environment variables navigating
+  * 		*current*\ **->mm->arg_start** and *current*\
+  * 		**->mm->env_start**: using this helper and the return value,
+  * 		one can quickly iterate at the right offset of the memory area.
+  * 	Return
+  * 		On success, the strictly positive length of the string,
+  * 		including the trailing NUL character. On error, a negative
+  * 		value.
+  *
+  * int bpf_probe_read_kernel_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe kernel address *unsafe_ptr*
+  * 		to *dst*. Same semantics as with bpf_probe_read_user_str() apply.
+  * 	Return
+  * 		On success, the strictly positive length of the string,	including
+  * 		the trailing NUL character. On error, a negative value.
+  *
+  * int bpf_tcp_send_ack(void *tp, u32 rcv_nxt)
+  *	Description
+  *		Send out a tcp-ack. *tp* is the in-kernel struct tcp_sock.
+  *		*rcv_nxt* is the ack_seq to be sent out.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_send_signal_thread(u32 sig)
+  *	Description
+  *		Send signal *sig* to the thread corresponding to the current task.
+  *	Return
+  *		0 on success or successfully queued.
+  *
+  *		**-EBUSY** if work queue under nmi is full.
+  *
+  *		**-EINVAL** if *sig* is invalid.
+  *
+  *		**-EPERM** if no permission to send the *sig*.
+  *
+  *		**-EAGAIN** if bpf program can try again.
+  *
+  * u64 bpf_jiffies64(void)
+  *	Description
+  *		Obtain the 64bit jiffies
+  *	Return
+  *		The 64 bit jiffies
+  *
+  * int bpf_read_branch_records(struct bpf_perf_event_data *ctx, void *buf, u32 size, u64 flags)
+  *	Description
+  *		For an eBPF program attached to a perf event, retrieve the
+  *		branch records (struct perf_branch_entry) associated to *ctx*
+  *		and store it in	the buffer pointed by *buf* up to size
+  *		*size* bytes.
+  *	Return
+  *		On success, number of bytes written to *buf*. On error, a
+  *		negative value.
+  *
+  *		The *flags* can be set to **BPF_F_GET_BRANCH_RECORDS_SIZE** to
+  *		instead	return the number of bytes required to store all the
+  *		branch entries. If this flag is set, *buf* may be NULL.
+  *
+  *		**-EINVAL** if arguments invalid or **size** not a multiple
+  *		of sizeof(struct perf_branch_entry).
+  *
+  *		**-ENOENT** if architecture does not support branch records.
+  *
+  * int bpf_get_ns_current_pid_tgid(u64 dev, u64 ino, struct bpf_pidns_info *nsdata, u32 size)
+  *	Description
+  *		Returns 0 on success, values for *pid* and *tgid* as seen from the current
+  *		*namespace* will be returned in *nsdata*.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** if dev and inum supplied don't match dev_t and inode number
+  *              with nsfs of current task, or if dev conversion to dev_t lost high bits.
+  *
+  *		**-ENOENT** if pidns does not exists for the current task.
+  *
+  * int bpf_xdp_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  *	Description
+  *		Write raw *data* blob into a special BPF perf event held by
+  *		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  *		event must have the following attributes: **PERF_SAMPLE_RAW**
+  *		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  *		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  *		The *flags* are used to indicate the index in *map* for which
+  *		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  *		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  *		to indicate that the index of the current CPU core should be
+  *		used.
+  *
+  *		The value to write, of *size*, is passed through eBPF stack and
+  *		pointed by *data*.
+  *
+  *		*ctx* is a pointer to in-kernel struct xdp_buff.
+  *
+  *		This helper is similar to **bpf_perf_eventoutput**\ () but
+  *		restricted to raw_tracepoint bpf programs.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -2869,7 -3062,18 +3044,22 @@@
  	FN(sk_storage_get),		\
  	FN(sk_storage_delete),		\
  	FN(send_signal),		\
++<<<<<<< HEAD
 +	FN(tcp_gen_syncookie),
++=======
+ 	FN(tcp_gen_syncookie),		\
+ 	FN(skb_output),			\
+ 	FN(probe_read_user),		\
+ 	FN(probe_read_kernel),		\
+ 	FN(probe_read_user_str),	\
+ 	FN(probe_read_kernel_str),	\
+ 	FN(tcp_send_ack),		\
+ 	FN(send_signal_thread),		\
+ 	FN(jiffies64),			\
+ 	FN(read_branch_records),	\
+ 	FN(get_ns_current_pid_tgid),	\
+ 	FN(xdp_output),
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --cc kernel/bpf/verifier.c
index 1889f4bbcf69,745f3cfdf3b2..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -3622,7 -3649,9 +3622,13 @@@ static int check_map_func_compatibility
  	case BPF_MAP_TYPE_PERF_EVENT_ARRAY:
  		if (func_id != BPF_FUNC_perf_event_read &&
  		    func_id != BPF_FUNC_perf_event_output &&
++<<<<<<< HEAD
 +		    func_id != BPF_FUNC_perf_event_read_value)
++=======
+ 		    func_id != BPF_FUNC_skb_output &&
+ 		    func_id != BPF_FUNC_perf_event_read_value &&
+ 		    func_id != BPF_FUNC_xdp_output)
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
  			goto error;
  		break;
  	case BPF_MAP_TYPE_STACK_TRACE:
@@@ -3710,6 -3740,8 +3716,11 @@@
  	case BPF_FUNC_perf_event_read:
  	case BPF_FUNC_perf_event_output:
  	case BPF_FUNC_perf_event_read_value:
++<<<<<<< HEAD
++=======
+ 	case BPF_FUNC_skb_output:
+ 	case BPF_FUNC_xdp_output:
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
  		if (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)
  			goto error;
  		break;
diff --cc kernel/trace/bpf_trace.c
index 2425aaf4e9d9,e619eedb5919..000000000000
--- a/kernel/trace/bpf_trace.c
+++ b/kernel/trace/bpf_trace.c
@@@ -1014,6 -1144,9 +1014,12 @@@ static const struct bpf_func_proto bpf_
  	.arg5_type	= ARG_CONST_SIZE_OR_ZERO,
  };
  
++<<<<<<< HEAD
++=======
+ extern const struct bpf_func_proto bpf_skb_output_proto;
+ extern const struct bpf_func_proto bpf_xdp_output_proto;
+ 
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
  BPF_CALL_3(bpf_get_stackid_raw_tp, struct bpf_raw_tracepoint_args *, args,
  	   struct bpf_map *, map, u64, flags)
  {
@@@ -1081,6 -1214,21 +1087,24 @@@ raw_tp_prog_func_proto(enum bpf_func_i
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static const struct bpf_func_proto *
+ tracing_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
+ {
+ 	switch (func_id) {
+ #ifdef CONFIG_NET
+ 	case BPF_FUNC_skb_output:
+ 		return &bpf_skb_output_proto;
+ 	case BPF_FUNC_xdp_output:
+ 		return &bpf_xdp_output_proto;
+ #endif
+ 	default:
+ 		return raw_tp_prog_func_proto(func_id, prog);
+ 	}
+ }
+ 
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
  static bool raw_tp_prog_is_valid_access(int off, int size,
  					enum bpf_access_type type,
  					const struct bpf_prog *prog,
diff --cc tools/include/uapi/linux/bpf.h
index 2bc35095902a,5d01c5c7e598..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -2718,6 -2751,205 +2718,208 @@@ union bpf_attr 
   *		**-EPERM** if no permission to send the *sig*.
   *
   *		**-EAGAIN** if bpf program can try again.
++<<<<<<< HEAD
++=======
+  *
+  * s64 bpf_tcp_gen_syncookie(struct bpf_sock *sk, void *iph, u32 iph_len, struct tcphdr *th, u32 th_len)
+  *	Description
+  *		Try to issue a SYN cookie for the packet with corresponding
+  *		IP/TCP headers, *iph* and *th*, on the listening socket in *sk*.
+  *
+  *		*iph* points to the start of the IPv4 or IPv6 header, while
+  *		*iph_len* contains **sizeof**\ (**struct iphdr**) or
+  *		**sizeof**\ (**struct ip6hdr**).
+  *
+  *		*th* points to the start of the TCP header, while *th_len*
+  *		contains the length of the TCP header.
+  *
+  *	Return
+  *		On success, lower 32 bits hold the generated SYN cookie in
+  *		followed by 16 bits which hold the MSS value for that cookie,
+  *		and the top 16 bits are unused.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** SYN cookie cannot be issued due to error
+  *
+  *		**-ENOENT** SYN cookie should not be issued (no SYN flood)
+  *
+  *		**-EOPNOTSUPP** kernel configuration does not enable SYN cookies
+  *
+  *		**-EPROTONOSUPPORT** IP packet version is not 4 or 6
+  *
+  * int bpf_skb_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  * 	Description
+  * 		Write raw *data* blob into a special BPF perf event held by
+  * 		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  * 		event must have the following attributes: **PERF_SAMPLE_RAW**
+  * 		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  * 		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  * 		The *flags* are used to indicate the index in *map* for which
+  * 		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  * 		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  * 		to indicate that the index of the current CPU core should be
+  * 		used.
+  *
+  * 		The value to write, of *size*, is passed through eBPF stack and
+  * 		pointed by *data*.
+  *
+  * 		*ctx* is a pointer to in-kernel struct sk_buff.
+  *
+  * 		This helper is similar to **bpf_perf_event_output**\ () but
+  * 		restricted to raw_tracepoint bpf programs.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from user space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_kernel(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from kernel space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe user address
+  * 		*unsafe_ptr* to *dst*. The *size* should include the
+  * 		terminating NUL byte. In case the string length is smaller than
+  * 		*size*, the target is not padded with further NUL bytes. If the
+  * 		string length is larger than *size*, just *size*-1 bytes are
+  * 		copied and the last byte is set to NUL.
+  *
+  * 		On success, the length of the copied string is returned. This
+  * 		makes this helper useful in tracing programs for reading
+  * 		strings, and more importantly to get its length at runtime. See
+  * 		the following snippet:
+  *
+  * 		::
+  *
+  * 			SEC("kprobe/sys_open")
+  * 			void bpf_sys_open(struct pt_regs *ctx)
+  * 			{
+  * 			        char buf[PATHLEN]; // PATHLEN is defined to 256
+  * 			        int res = bpf_probe_read_user_str(buf, sizeof(buf),
+  * 				                                  ctx->di);
+  *
+  * 				// Consume buf, for example push it to
+  * 				// userspace via bpf_perf_event_output(); we
+  * 				// can use res (the string length) as event
+  * 				// size, after checking its boundaries.
+  * 			}
+  *
+  * 		In comparison, using **bpf_probe_read_user()** helper here
+  * 		instead to read the string would require to estimate the length
+  * 		at compile time, and would often result in copying more memory
+  * 		than necessary.
+  *
+  * 		Another useful use case is when parsing individual process
+  * 		arguments or individual environment variables navigating
+  * 		*current*\ **->mm->arg_start** and *current*\
+  * 		**->mm->env_start**: using this helper and the return value,
+  * 		one can quickly iterate at the right offset of the memory area.
+  * 	Return
+  * 		On success, the strictly positive length of the string,
+  * 		including the trailing NUL character. On error, a negative
+  * 		value.
+  *
+  * int bpf_probe_read_kernel_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe kernel address *unsafe_ptr*
+  * 		to *dst*. Same semantics as with bpf_probe_read_user_str() apply.
+  * 	Return
+  * 		On success, the strictly positive length of the string,	including
+  * 		the trailing NUL character. On error, a negative value.
+  *
+  * int bpf_tcp_send_ack(void *tp, u32 rcv_nxt)
+  *	Description
+  *		Send out a tcp-ack. *tp* is the in-kernel struct tcp_sock.
+  *		*rcv_nxt* is the ack_seq to be sent out.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_send_signal_thread(u32 sig)
+  *	Description
+  *		Send signal *sig* to the thread corresponding to the current task.
+  *	Return
+  *		0 on success or successfully queued.
+  *
+  *		**-EBUSY** if work queue under nmi is full.
+  *
+  *		**-EINVAL** if *sig* is invalid.
+  *
+  *		**-EPERM** if no permission to send the *sig*.
+  *
+  *		**-EAGAIN** if bpf program can try again.
+  *
+  * u64 bpf_jiffies64(void)
+  *	Description
+  *		Obtain the 64bit jiffies
+  *	Return
+  *		The 64 bit jiffies
+  *
+  * int bpf_read_branch_records(struct bpf_perf_event_data *ctx, void *buf, u32 size, u64 flags)
+  *	Description
+  *		For an eBPF program attached to a perf event, retrieve the
+  *		branch records (struct perf_branch_entry) associated to *ctx*
+  *		and store it in	the buffer pointed by *buf* up to size
+  *		*size* bytes.
+  *	Return
+  *		On success, number of bytes written to *buf*. On error, a
+  *		negative value.
+  *
+  *		The *flags* can be set to **BPF_F_GET_BRANCH_RECORDS_SIZE** to
+  *		instead	return the number of bytes required to store all the
+  *		branch entries. If this flag is set, *buf* may be NULL.
+  *
+  *		**-EINVAL** if arguments invalid or **size** not a multiple
+  *		of sizeof(struct perf_branch_entry).
+  *
+  *		**-ENOENT** if architecture does not support branch records.
+  *
+  * int bpf_get_ns_current_pid_tgid(u64 dev, u64 ino, struct bpf_pidns_info *nsdata, u32 size)
+  *	Description
+  *		Returns 0 on success, values for *pid* and *tgid* as seen from the current
+  *		*namespace* will be returned in *nsdata*.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** if dev and inum supplied don't match dev_t and inode number
+  *              with nsfs of current task, or if dev conversion to dev_t lost high bits.
+  *
+  *		**-ENOENT** if pidns does not exists for the current task.
+  *
+  * int bpf_xdp_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  *	Description
+  *		Write raw *data* blob into a special BPF perf event held by
+  *		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  *		event must have the following attributes: **PERF_SAMPLE_RAW**
+  *		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  *		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  *		The *flags* are used to indicate the index in *map* for which
+  *		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  *		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  *		to indicate that the index of the current CPU core should be
+  *		used.
+  *
+  *		The value to write, of *size*, is passed through eBPF stack and
+  *		pointed by *data*.
+  *
+  *		*ctx* is a pointer to in-kernel struct xdp_buff.
+  *
+  *		This helper is similar to **bpf_perf_eventoutput**\ () but
+  *		restricted to raw_tracepoint bpf programs.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -2829,7 -3061,19 +3031,23 @@@
  	FN(strtoul),			\
  	FN(sk_storage_get),		\
  	FN(sk_storage_delete),		\
++<<<<<<< HEAD
 +	FN(send_signal),
++=======
+ 	FN(send_signal),		\
+ 	FN(tcp_gen_syncookie),		\
+ 	FN(skb_output),			\
+ 	FN(probe_read_user),		\
+ 	FN(probe_read_kernel),		\
+ 	FN(probe_read_user_str),	\
+ 	FN(probe_read_kernel_str),	\
+ 	FN(tcp_send_ack),		\
+ 	FN(send_signal_thread),		\
+ 	FN(jiffies64),			\
+ 	FN(read_branch_records),	\
+ 	FN(get_ns_current_pid_tgid),	\
+ 	FN(xdp_output),
++>>>>>>> d831ee84bfc9 (bpf: Add bpf_xdp_output() helper)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/bpf/verifier.c
* Unmerged path kernel/trace/bpf_trace.c
diff --git a/net/core/filter.c b/net/core/filter.c
index bc63dd60788c..6b2d0f55367e 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -4148,7 +4148,8 @@ BPF_CALL_5(bpf_xdp_event_output, struct xdp_buff *, xdp, struct bpf_map *, map,
 
 	if (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))
 		return -EINVAL;
-	if (unlikely(xdp_size > (unsigned long)(xdp->data_end - xdp->data)))
+	if (unlikely(!xdp ||
+		     xdp_size > (unsigned long)(xdp->data_end - xdp->data)))
 		return -EFAULT;
 
 	return bpf_event_output(map, flags, meta, meta_size, xdp->data,
@@ -4166,6 +4167,19 @@ static const struct bpf_func_proto bpf_xdp_event_output_proto = {
 	.arg5_type	= ARG_CONST_SIZE_OR_ZERO,
 };
 
+static int bpf_xdp_output_btf_ids[5];
+const struct bpf_func_proto bpf_xdp_output_proto = {
+	.func		= bpf_xdp_event_output,
+	.gpl_only	= true,
+	.ret_type	= RET_INTEGER,
+	.arg1_type	= ARG_PTR_TO_BTF_ID,
+	.arg2_type	= ARG_CONST_MAP_PTR,
+	.arg3_type	= ARG_ANYTHING,
+	.arg4_type	= ARG_PTR_TO_MEM,
+	.arg5_type	= ARG_CONST_SIZE_OR_ZERO,
+	.btf_id		= bpf_xdp_output_btf_ids,
+};
+
 BPF_CALL_1(bpf_get_socket_cookie, struct sk_buff *, skb)
 {
 	return skb->sk ? sock_gen_cookie(skb->sk) : 0;
* Unmerged path tools/include/uapi/linux/bpf.h
diff --git a/tools/testing/selftests/bpf/prog_tests/xdp_bpf2bpf.c b/tools/testing/selftests/bpf/prog_tests/xdp_bpf2bpf.c
index 4ba011031d4c..a0f688c37023 100644
--- a/tools/testing/selftests/bpf/prog_tests/xdp_bpf2bpf.c
+++ b/tools/testing/selftests/bpf/prog_tests/xdp_bpf2bpf.c
@@ -4,17 +4,51 @@
 #include "test_xdp.skel.h"
 #include "test_xdp_bpf2bpf.skel.h"
 
+struct meta {
+	int ifindex;
+	int pkt_len;
+};
+
+static void on_sample(void *ctx, int cpu, void *data, __u32 size)
+{
+	int duration = 0;
+	struct meta *meta = (struct meta *)data;
+	struct ipv4_packet *trace_pkt_v4 = data + sizeof(*meta);
+
+	if (CHECK(size < sizeof(pkt_v4) + sizeof(*meta),
+		  "check_size", "size %u < %zu\n",
+		  size, sizeof(pkt_v4) + sizeof(*meta)))
+		return;
+
+	if (CHECK(meta->ifindex != if_nametoindex("lo"), "check_meta_ifindex",
+		  "meta->ifindex = %d\n", meta->ifindex))
+		return;
+
+	if (CHECK(meta->pkt_len != sizeof(pkt_v4), "check_meta_pkt_len",
+		  "meta->pkt_len = %zd\n", sizeof(pkt_v4)))
+		return;
+
+	if (CHECK(memcmp(trace_pkt_v4, &pkt_v4, sizeof(pkt_v4)),
+		  "check_packet_content", "content not the same\n"))
+		return;
+
+	*(bool *)ctx = true;
+}
+
 void test_xdp_bpf2bpf(void)
 {
 	__u32 duration = 0, retval, size;
 	char buf[128];
 	int err, pkt_fd, map_fd;
+	bool passed = false;
 	struct iphdr *iph = (void *)buf + sizeof(struct ethhdr);
 	struct iptnl_info value4 = {.family = AF_INET};
 	struct test_xdp *pkt_skel = NULL;
 	struct test_xdp_bpf2bpf *ftrace_skel = NULL;
 	struct vip key4 = {.protocol = 6, .family = AF_INET};
 	struct bpf_program *prog;
+	struct perf_buffer *pb = NULL;
+	struct perf_buffer_opts pb_opts = {};
 
 	/* Load XDP program to introspect */
 	pkt_skel = test_xdp__open_and_load();
@@ -50,6 +84,14 @@ void test_xdp_bpf2bpf(void)
 	if (CHECK(err, "ftrace_attach", "ftrace attach failed: %d\n", err))
 		goto out;
 
+	/* Set up perf buffer */
+	pb_opts.sample_cb = on_sample;
+	pb_opts.ctx = &passed;
+	pb = perf_buffer__new(bpf_map__fd(ftrace_skel->maps.perf_buf_map),
+			      1, &pb_opts);
+	if (CHECK(IS_ERR(pb), "perf_buf__new", "err %ld\n", PTR_ERR(pb)))
+		goto out;
+
 	/* Run test program */
 	err = bpf_prog_test_run(pkt_fd, 1, &pkt_v4, sizeof(pkt_v4),
 				buf, &size, &retval, &duration);
@@ -60,6 +102,15 @@ void test_xdp_bpf2bpf(void)
 		  err, errno, retval, size))
 		goto out;
 
+	/* Make sure bpf_xdp_output() was triggered and it sent the expected
+	 * data to the perf ring buffer.
+	 */
+	err = perf_buffer__poll(pb, 100);
+	if (CHECK(err < 0, "perf_buffer__poll", "err %d\n", err))
+		goto out;
+
+	CHECK_FAIL(!passed);
+
 	/* Verify test results */
 	if (CHECK(ftrace_skel->bss->test_result_fentry != if_nametoindex("lo"),
 		  "result", "fentry failed err %llu\n",
@@ -70,6 +121,8 @@ void test_xdp_bpf2bpf(void)
 	      "fexit failed err %llu\n", ftrace_skel->bss->test_result_fexit);
 
 out:
+	if (pb)
+		perf_buffer__free(pb);
 	test_xdp__destroy(pkt_skel);
 	test_xdp_bpf2bpf__destroy(ftrace_skel);
 }
diff --git a/tools/testing/selftests/bpf/progs/test_xdp_bpf2bpf.c b/tools/testing/selftests/bpf/progs/test_xdp_bpf2bpf.c
index c0f6d0831c5d..6efe6394675f 100644
--- a/tools/testing/selftests/bpf/progs/test_xdp_bpf2bpf.c
+++ b/tools/testing/selftests/bpf/progs/test_xdp_bpf2bpf.c
@@ -3,6 +3,8 @@
 #include "bpf_helpers.h"
 #include "bpf_trace_helpers.h"
 
+char _license[] SEC("license") = "GPL";
+
 struct net_device {
 	/* Structure does not need to contain all entries,
 	 * as "preserve_access_index" will use BTF to fix this...
@@ -27,10 +29,32 @@ struct xdp_buff {
 	struct xdp_rxq_info *rxq;
 } __attribute__((preserve_access_index));
 
+struct meta {
+	int ifindex;
+	int pkt_len;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY);
+	__uint(key_size, sizeof(int));
+	__uint(value_size, sizeof(int));
+} perf_buf_map SEC(".maps");
+
 __u64 test_result_fentry = 0;
 SEC("fentry/FUNC")
 int BPF_PROG(trace_on_entry, struct xdp_buff *xdp)
 {
+	struct meta meta;
+	void *data_end = (void *)(long)xdp->data_end;
+	void *data = (void *)(long)xdp->data;
+
+	meta.ifindex = xdp->rxq->dev->ifindex;
+	meta.pkt_len = data_end - data;
+	bpf_xdp_output(xdp, &perf_buf_map,
+		       ((__u64) meta.pkt_len << 32) |
+		       BPF_F_CURRENT_CPU,
+		       &meta, sizeof(meta));
+
 	test_result_fentry = xdp->rxq->dev->ifindex;
 	return 0;
 }
