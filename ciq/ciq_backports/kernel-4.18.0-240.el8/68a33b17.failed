dma-direct: exclude dma_direct_map_resource from the min_low_pfn check

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 68a33b1794665ba8a1d1ef1d3bfcc7c587d380a6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/68a33b17.failed

The valid memory address check in dma_capable only makes sense when mapping
normal memory, not when using dma_map_resource to map a device resource.
Add a new boolean argument to dma_capable to exclude that check for the
dma_map_resource case.

Fixes: b12d66278dd6 ("dma-direct: check for overflows on 32 bit DMA addresses")
	Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
	Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
(cherry picked from commit 68a33b1794665ba8a1d1ef1d3bfcc7c587d380a6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/xen/swiotlb-xen.c
#	include/linux/dma-direct.h
#	kernel/dma/direct.c
#	kernel/dma/swiotlb.c
diff --cc drivers/xen/swiotlb-xen.c
index e920c075fbde,b6d27762c6f8..000000000000
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@@ -399,8 -397,8 +399,13 @@@ static dma_addr_t xen_swiotlb_map_page(
  	/*
  	 * Ensure that the address returned is DMA'ble
  	 */
++<<<<<<< HEAD
 +	if (unlikely(!dma_capable(dev, dev_addr, size))) {
 +		swiotlb_tbl_unmap_single(dev, map, size, dir,
++=======
+ 	if (unlikely(!dma_capable(dev, dev_addr, size, true))) {
+ 		swiotlb_tbl_unmap_single(dev, map, size, size, dir,
++>>>>>>> 68a33b179466 (dma-direct: exclude dma_direct_map_resource from the min_low_pfn check)
  				attrs | DMA_ATTR_SKIP_CPU_SYNC);
  		return DMA_MAPPING_ERROR;
  	}
diff --cc include/linux/dma-direct.h
index 3238177e65ad,99b77dd5f79b..000000000000
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@@ -65,6 -49,21 +65,24 @@@ static inline phys_addr_t dma_to_phys(s
  	return __sme_clr(__dma_to_phys(dev, daddr));
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size,
+ 		bool is_ram)
+ {
+ 	dma_addr_t end = addr + size - 1;
+ 
+ 	if (!dev->dma_mask)
+ 		return false;
+ 
+ 	if (is_ram && !IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT) &&
+ 	    min(addr, end) < phys_to_dma(dev, PFN_PHYS(min_low_pfn)))
+ 		return false;
+ 
+ 	return end <= min_not_zero(*dev->dma_mask, dev->bus_dma_mask);
+ }
+ 
++>>>>>>> 68a33b179466 (dma-direct: exclude dma_direct_map_resource from the min_low_pfn check)
  u64 dma_direct_get_required_mask(struct device *dev);
  void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
  		gfp_t gfp, unsigned long attrs);
diff --cc kernel/dma/direct.c
index f4581748eeeb,40f1f0aac4b1..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -365,6 -407,73 +365,76 @@@ out_unmap
  }
  EXPORT_SYMBOL(dma_direct_map_sg);
  
++<<<<<<< HEAD
++=======
+ dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	dma_addr_t dma_addr = paddr;
+ 
+ 	if (unlikely(!dma_capable(dev, dma_addr, size, false))) {
+ 		report_addr(dev, dma_addr, size);
+ 		return DMA_MAPPING_ERROR;
+ 	}
+ 
+ 	return dma_addr;
+ }
+ EXPORT_SYMBOL(dma_direct_map_resource);
+ 
+ int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	struct page *page = dma_direct_to_page(dev, dma_addr);
+ 	int ret;
+ 
+ 	ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+ 	if (!ret)
+ 		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_MMU
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return dev_is_dma_coherent(dev) ||
+ 		IS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	unsigned long user_count = vma_pages(vma);
+ 	unsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	unsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));
+ 	int ret = -ENXIO;
+ 
+ 	vma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);
+ 
+ 	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
+ 		return ret;
+ 
+ 	if (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)
+ 		return -ENXIO;
+ 	return remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,
+ 			user_count << PAGE_SHIFT, vma->vm_page_prot);
+ }
+ #else /* CONFIG_MMU */
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return false;
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ #endif /* CONFIG_MMU */
+ 
++>>>>>>> 68a33b179466 (dma-direct: exclude dma_direct_map_resource from the min_low_pfn check)
  /*
   * Because 32-bit DMA masks are so common we expect every architecture to be
   * able to satisfy them - either by not supporting more physical memory, or by
diff --cc kernel/dma/swiotlb.c
index ab8254d12114,9280d6f8271e..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -669,8 -678,8 +669,13 @@@ bool swiotlb_map(struct device *dev, ph
  
  	/* Ensure that the address returned is DMA'ble */
  	*dma_addr = __phys_to_dma(dev, *phys);
++<<<<<<< HEAD
 +	if (unlikely(!dma_capable(dev, *dma_addr, size))) {
 +		swiotlb_tbl_unmap_single(dev, *phys, size, dir,
++=======
+ 	if (unlikely(!dma_capable(dev, *dma_addr, size, true))) {
+ 		swiotlb_tbl_unmap_single(dev, *phys, size, size, dir,
++>>>>>>> 68a33b179466 (dma-direct: exclude dma_direct_map_resource from the min_low_pfn check)
  			attrs | DMA_ATTR_SKIP_CPU_SYNC);
  		return false;
  	}
diff --git a/arch/x86/kernel/amd_gart_64.c b/arch/x86/kernel/amd_gart_64.c
index adcd3345a7f0..e066a4e04c02 100644
--- a/arch/x86/kernel/amd_gart_64.c
+++ b/arch/x86/kernel/amd_gart_64.c
@@ -188,13 +188,13 @@ static void iommu_full(struct device *dev, size_t size, int dir)
 static inline int
 need_iommu(struct device *dev, unsigned long addr, size_t size)
 {
-	return force_iommu || !dma_capable(dev, addr, size);
+	return force_iommu || !dma_capable(dev, addr, size, true);
 }
 
 static inline int
 nonforced_iommu(struct device *dev, unsigned long addr, size_t size)
 {
-	return !dma_capable(dev, addr, size);
+	return !dma_capable(dev, addr, size, true);
 }
 
 /* Map a single continuous physical area into the IOMMU.
* Unmerged path drivers/xen/swiotlb-xen.c
* Unmerged path include/linux/dma-direct.h
* Unmerged path kernel/dma/direct.c
* Unmerged path kernel/dma/swiotlb.c
