mm: add account_locked_vm utility function

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [mm] add account_locked_vm utility function (David Gibson) [1814624]
Rebuild_FUZZ: 95.00%
commit-author Daniel Jordan <daniel.m.jordan@oracle.com>
commit 79eb597cba06c435b72f220e9d426ae413fc2579
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/79eb597c.failed

locked_vm accounting is done roughly the same way in five places, so
unify them in a helper.

Include the helper's caller in the debug print to distinguish between
callsites.

Error codes stay the same, so user-visible behavior does too.  The one
exception is that the -EPERM case in tce_account_locked_vm is removed
because Alexey has never seen it triggered.

[daniel.m.jordan@oracle.com: v3]
  Link: http://lkml.kernel.org/r/20190529205019.20927-1-daniel.m.jordan@oracle.com
[sfr@canb.auug.org.au: fix mm/util.c]
Link: http://lkml.kernel.org/r/20190524175045.26897-1-daniel.m.jordan@oracle.com
	Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
	Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
	Tested-by: Alexey Kardashevskiy <aik@ozlabs.ru>
	Acked-by: Alex Williamson <alex.williamson@redhat.com>
	Cc: Alan Tull <atull@kernel.org>
	Cc: Alex Williamson <alex.williamson@redhat.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Christophe Leroy <christophe.leroy@c-s.fr>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Jason Gunthorpe <jgg@mellanox.com>
	Cc: Mark Rutland <mark.rutland@arm.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Moritz Fischer <mdf@kernel.org>
	Cc: Paul Mackerras <paulus@ozlabs.org>
	Cc: Steve Sistare <steven.sistare@oracle.com>
	Cc: Wu Hao <hao.wu@intel.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 79eb597cba06c435b72f220e9d426ae413fc2579)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/fpga/dfl-afu-dma-region.c
#	mm/util.c
diff --cc mm/util.c
index 6fc256ddf4aa,e6351a80f248..000000000000
--- a/mm/util.c
+++ b/mm/util.c
@@@ -287,52 -301,79 +288,128 @@@ void arch_pick_mmap_layout(struct mm_st
  }
  #endif
  
++<<<<<<< HEAD
 +/*
 + * Like get_user_pages_fast() except its IRQ-safe in that it won't fall
 + * back to the regular GUP.
 + * Note a difference with get_user_pages_fast: this always returns the
 + * number of pages pinned, 0 if no pages were pinned.
 + * If the architecture does not support this function, simply return with no
 + * pages pinned.
 + */
 +int __weak __get_user_pages_fast(unsigned long start,
 +				 int nr_pages, int write, struct page **pages)
 +{
 +	return 0;
 +}
 +EXPORT_SYMBOL_GPL(__get_user_pages_fast);
 +
 +/**
 + * get_user_pages_fast() - pin user pages in memory
 + * @start:	starting user address
 + * @nr_pages:	number of pages from start to pin
 + * @gup_flags:	flags modifying pin behaviour
 + * @pages:	array that receives pointers to the pages pinned.
 + *		Should be at least nr_pages long.
 + *
 + * Returns number of pages pinned. This may be fewer than the number
 + * requested. If nr_pages is 0 or negative, returns 0. If no pages
 + * were pinned, returns -errno.
 + *
 + * get_user_pages_fast provides equivalent functionality to get_user_pages,
 + * operating on current and current->mm, with force=0 and vma=NULL. However
 + * unlike get_user_pages, it must be called without mmap_sem held.
 + *
 + * get_user_pages_fast may take mmap_sem and page table locks, so no
 + * assumptions can be made about lack of locking. get_user_pages_fast is to be
 + * implemented in a way that is advantageous (vs get_user_pages()) when the
 + * user memory area is already faulted in and present in ptes. However if the
 + * pages have to be faulted in, it may turn out to be slightly slower so
 + * callers need to carefully consider what to use. On many architectures,
 + * get_user_pages_fast simply falls back to get_user_pages.
 + */
 +int __weak get_user_pages_fast(unsigned long start,
 +				int nr_pages, unsigned int gup_flags,
 +				struct page **pages)
 +{
 +	return get_user_pages_unlocked(start, nr_pages, pages, gup_flags);
 +}
 +EXPORT_SYMBOL_GPL(get_user_pages_fast);
++=======
+ /**
+  * __account_locked_vm - account locked pages to an mm's locked_vm
+  * @mm:          mm to account against
+  * @pages:       number of pages to account
+  * @inc:         %true if @pages should be considered positive, %false if not
+  * @task:        task used to check RLIMIT_MEMLOCK
+  * @bypass_rlim: %true if checking RLIMIT_MEMLOCK should be skipped
+  *
+  * Assumes @task and @mm are valid (i.e. at least one reference on each), and
+  * that mmap_sem is held as writer.
+  *
+  * Return:
+  * * 0       on success
+  * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.
+  */
+ int __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,
+ 			struct task_struct *task, bool bypass_rlim)
+ {
+ 	unsigned long locked_vm, limit;
+ 	int ret = 0;
+ 
+ 	lockdep_assert_held_write(&mm->mmap_sem);
+ 
+ 	locked_vm = mm->locked_vm;
+ 	if (inc) {
+ 		if (!bypass_rlim) {
+ 			limit = task_rlimit(task, RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+ 			if (locked_vm + pages > limit)
+ 				ret = -ENOMEM;
+ 		}
+ 		if (!ret)
+ 			mm->locked_vm = locked_vm + pages;
+ 	} else {
+ 		WARN_ON_ONCE(pages > locked_vm);
+ 		mm->locked_vm = locked_vm - pages;
+ 	}
+ 
+ 	pr_debug("%s: [%d] caller %ps %c%lu %lu/%lu%s\n", __func__, task->pid,
+ 		 (void *)_RET_IP_, (inc) ? '+' : '-', pages << PAGE_SHIFT,
+ 		 locked_vm << PAGE_SHIFT, task_rlimit(task, RLIMIT_MEMLOCK),
+ 		 ret ? " - exceeded" : "");
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(__account_locked_vm);
+ 
+ /**
+  * account_locked_vm - account locked pages to an mm's locked_vm
+  * @mm:          mm to account against, may be NULL
+  * @pages:       number of pages to account
+  * @inc:         %true if @pages should be considered positive, %false if not
+  *
+  * Assumes a non-NULL @mm is valid (i.e. at least one reference on it).
+  *
+  * Return:
+  * * 0       on success, or if mm is NULL
+  * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.
+  */
+ int account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc)
+ {
+ 	int ret;
+ 
+ 	if (pages == 0 || !mm)
+ 		return 0;
+ 
+ 	down_write(&mm->mmap_sem);
+ 	ret = __account_locked_vm(mm, pages, inc, current,
+ 				  capable(CAP_IPC_LOCK));
+ 	up_write(&mm->mmap_sem);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(account_locked_vm);
++>>>>>>> 79eb597cba06 (mm: add account_locked_vm utility function)
  
  unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
  	unsigned long len, unsigned long prot,
* Unmerged path drivers/fpga/dfl-afu-dma-region.c
diff --git a/arch/powerpc/kvm/book3s_64_vio.c b/arch/powerpc/kvm/book3s_64_vio.c
index 824f460b2f4f..4fe4cda1c0d8 100644
--- a/arch/powerpc/kvm/book3s_64_vio.c
+++ b/arch/powerpc/kvm/book3s_64_vio.c
@@ -30,6 +30,7 @@
 #include <linux/anon_inodes.h>
 #include <linux/iommu.h>
 #include <linux/file.h>
+#include <linux/mm.h>
 
 #include <asm/tlbflush.h>
 #include <asm/kvm_ppc.h>
@@ -57,43 +58,6 @@ static unsigned long kvmppc_stt_pages(unsigned long tce_pages)
 	return tce_pages + ALIGN(stt_bytes, PAGE_SIZE) / PAGE_SIZE;
 }
 
-static long kvmppc_account_memlimit(unsigned long stt_pages, bool inc)
-{
-	long ret = 0;
-
-	if (!current || !current->mm)
-		return ret; /* process exited */
-
-	down_write(&current->mm->mmap_sem);
-
-	if (inc) {
-		unsigned long locked, lock_limit;
-
-		locked = current->mm->locked_vm + stt_pages;
-		lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
-		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
-			ret = -ENOMEM;
-		else
-			current->mm->locked_vm += stt_pages;
-	} else {
-		if (WARN_ON_ONCE(stt_pages > current->mm->locked_vm))
-			stt_pages = current->mm->locked_vm;
-
-		current->mm->locked_vm -= stt_pages;
-	}
-
-	pr_debug("[%d] RLIMIT_MEMLOCK KVM %c%ld %ld/%ld%s\n", current->pid,
-			inc ? '+' : '-',
-			stt_pages << PAGE_SHIFT,
-			current->mm->locked_vm << PAGE_SHIFT,
-			rlimit(RLIMIT_MEMLOCK),
-			ret ? " - exceeded" : "");
-
-	up_write(&current->mm->mmap_sem);
-
-	return ret;
-}
-
 static void kvm_spapr_tce_iommu_table_free(struct rcu_head *head)
 {
 	struct kvmppc_spapr_tce_iommu_table *stit = container_of(head,
@@ -303,7 +267,7 @@ static int kvm_spapr_tce_release(struct inode *inode, struct file *filp)
 
 	kvm_put_kvm(stt->kvm);
 
-	kvmppc_account_memlimit(
+	account_locked_vm(current->mm,
 		kvmppc_stt_pages(kvmppc_tce_pages(stt->size)), false);
 	call_rcu(&stt->rcu, release_spapr_tce_table);
 
@@ -328,7 +292,7 @@ long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 		return -EINVAL;
 
 	npages = kvmppc_tce_pages(size);
-	ret = kvmppc_account_memlimit(kvmppc_stt_pages(npages), true);
+	ret = account_locked_vm(current->mm, kvmppc_stt_pages(npages), true);
 	if (ret)
 		return ret;
 
@@ -374,7 +338,7 @@ long kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,
 
 	kfree(stt);
  fail_acct:
-	kvmppc_account_memlimit(kvmppc_stt_pages(npages), false);
+	account_locked_vm(current->mm, kvmppc_stt_pages(npages), false);
 	return ret;
 }
 
diff --git a/arch/powerpc/mm/book3s64/iommu_api.c b/arch/powerpc/mm/book3s64/iommu_api.c
index 5c521f3924a5..18d22eec0ebd 100644
--- a/arch/powerpc/mm/book3s64/iommu_api.c
+++ b/arch/powerpc/mm/book3s64/iommu_api.c
@@ -19,6 +19,7 @@
 #include <linux/hugetlb.h>
 #include <linux/swap.h>
 #include <linux/sizes.h>
+#include <linux/mm.h>
 #include <asm/mmu_context.h>
 #include <asm/pte-walk.h>
 #include <linux/mm_inline.h>
@@ -51,40 +52,6 @@ struct mm_iommu_table_group_mem_t {
 	u64 dev_hpa;		/* Device memory base address */
 };
 
-static long mm_iommu_adjust_locked_vm(struct mm_struct *mm,
-		unsigned long npages, bool incr)
-{
-	long ret = 0, locked, lock_limit;
-
-	if (!npages)
-		return 0;
-
-	down_write(&mm->mmap_sem);
-
-	if (incr) {
-		locked = mm->locked_vm + npages;
-		lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
-		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
-			ret = -ENOMEM;
-		else
-			mm->locked_vm += npages;
-	} else {
-		if (WARN_ON_ONCE(npages > mm->locked_vm))
-			npages = mm->locked_vm;
-		mm->locked_vm -= npages;
-	}
-
-	pr_debug("[%d] RLIMIT_MEMLOCK HASH64 %c%ld %ld/%ld\n",
-			current ? current->pid : 0,
-			incr ? '+' : '-',
-			npages << PAGE_SHIFT,
-			mm->locked_vm << PAGE_SHIFT,
-			rlimit(RLIMIT_MEMLOCK));
-	up_write(&mm->mmap_sem);
-
-	return ret;
-}
-
 bool mm_iommu_preregistered(struct mm_struct *mm)
 {
 	return !list_empty(&mm->context.iommu_group_mem_list);
@@ -101,7 +68,7 @@ static long mm_iommu_do_alloc(struct mm_struct *mm, unsigned long ua,
 	unsigned long entry, chunk;
 
 	if (dev_hpa == MM_IOMMU_TABLE_INVALID_HPA) {
-		ret = mm_iommu_adjust_locked_vm(mm, entries, true);
+		ret = account_locked_vm(mm, entries, true);
 		if (ret)
 			return ret;
 
@@ -216,7 +183,7 @@ static long mm_iommu_do_alloc(struct mm_struct *mm, unsigned long ua,
 	kfree(mem);
 
 unlock_exit:
-	mm_iommu_adjust_locked_vm(mm, locked_entries, false);
+	account_locked_vm(mm, locked_entries, false);
 
 	return ret;
 }
@@ -316,7 +283,7 @@ long mm_iommu_put(struct mm_struct *mm, struct mm_iommu_table_group_mem_t *mem)
 unlock_exit:
 	mutex_unlock(&mem_list_mutex);
 
-	mm_iommu_adjust_locked_vm(mm, unlock_entries, false);
+	account_locked_vm(mm, unlock_entries, false);
 
 	return ret;
 }
* Unmerged path drivers/fpga/dfl-afu-dma-region.c
diff --git a/drivers/vfio/vfio_iommu_spapr_tce.c b/drivers/vfio/vfio_iommu_spapr_tce.c
index a428c1b5dffd..78cc6ffe9461 100644
--- a/drivers/vfio/vfio_iommu_spapr_tce.c
+++ b/drivers/vfio/vfio_iommu_spapr_tce.c
@@ -22,6 +22,7 @@
 #include <linux/vmalloc.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/signal.h>
+#include <linux/mm.h>
 
 #include <asm/iommu.h>
 #include <asm/tce.h>
@@ -34,51 +35,6 @@
 static void tce_iommu_detach_group(void *iommu_data,
 		struct iommu_group *iommu_group);
 
-static long try_increment_locked_vm(struct mm_struct *mm, long npages)
-{
-	long ret = 0, locked, lock_limit;
-
-	if (WARN_ON_ONCE(!mm))
-		return -EPERM;
-
-	if (!npages)
-		return 0;
-
-	down_write(&mm->mmap_sem);
-	locked = mm->locked_vm + npages;
-	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
-	if (locked > lock_limit && !capable(CAP_IPC_LOCK))
-		ret = -ENOMEM;
-	else
-		mm->locked_vm += npages;
-
-	pr_debug("[%d] RLIMIT_MEMLOCK +%ld %ld/%ld%s\n", current->pid,
-			npages << PAGE_SHIFT,
-			mm->locked_vm << PAGE_SHIFT,
-			rlimit(RLIMIT_MEMLOCK),
-			ret ? " - exceeded" : "");
-
-	up_write(&mm->mmap_sem);
-
-	return ret;
-}
-
-static void decrement_locked_vm(struct mm_struct *mm, long npages)
-{
-	if (!mm || !npages)
-		return;
-
-	down_write(&mm->mmap_sem);
-	if (WARN_ON_ONCE(npages > mm->locked_vm))
-		npages = mm->locked_vm;
-	mm->locked_vm -= npages;
-	pr_debug("[%d] RLIMIT_MEMLOCK -%ld %ld/%ld\n", current->pid,
-			npages << PAGE_SHIFT,
-			mm->locked_vm << PAGE_SHIFT,
-			rlimit(RLIMIT_MEMLOCK));
-	up_write(&mm->mmap_sem);
-}
-
 /*
  * VFIO IOMMU fd for SPAPR_TCE IOMMU implementation
  *
@@ -336,7 +292,7 @@ static int tce_iommu_enable(struct tce_container *container)
 		return ret;
 
 	locked = table_group->tce32_size >> PAGE_SHIFT;
-	ret = try_increment_locked_vm(container->mm, locked);
+	ret = account_locked_vm(container->mm, locked, true);
 	if (ret)
 		return ret;
 
@@ -355,7 +311,7 @@ static void tce_iommu_disable(struct tce_container *container)
 	container->enabled = false;
 
 	BUG_ON(!container->mm);
-	decrement_locked_vm(container->mm, container->locked_pages);
+	account_locked_vm(container->mm, container->locked_pages, false);
 }
 
 static void *tce_iommu_open(unsigned long arg)
@@ -663,7 +619,7 @@ static long tce_iommu_create_table(struct tce_container *container,
 	if (!table_size)
 		return -EINVAL;
 
-	ret = try_increment_locked_vm(container->mm, table_size >> PAGE_SHIFT);
+	ret = account_locked_vm(container->mm, table_size >> PAGE_SHIFT, true);
 	if (ret)
 		return ret;
 
@@ -682,7 +638,7 @@ static void tce_iommu_free_table(struct tce_container *container,
 	unsigned long pages = tbl->it_allocated_size >> PAGE_SHIFT;
 
 	iommu_tce_table_put(tbl);
-	decrement_locked_vm(container->mm, pages);
+	account_locked_vm(container->mm, pages, false);
 }
 
 static long tce_iommu_create_window(struct tce_container *container,
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 253b526308f2..f0dc8da1752b 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -282,21 +282,8 @@ static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 
 	ret = down_write_killable(&mm->mmap_sem);
 	if (!ret) {
-		if (npage > 0) {
-			if (!dma->lock_cap) {
-				unsigned long limit;
-
-				limit = task_rlimit(dma->task,
-						RLIMIT_MEMLOCK) >> PAGE_SHIFT;
-
-				if (mm->locked_vm + npage > limit)
-					ret = -ENOMEM;
-			}
-		}
-
-		if (!ret)
-			mm->locked_vm += npage;
-
+		ret = __account_locked_vm(mm, abs(npage), npage > 0, dma->task,
+					  dma->lock_cap);
 		up_write(&mm->mmap_sem);
 	}
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 7814b1c09166..05ad11109416 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1477,6 +1477,10 @@ long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
 int get_user_pages_fast(unsigned long start, int nr_pages,
 			unsigned int gup_flags, struct page **pages);
 
+int account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc);
+int __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,
+			struct task_struct *task, bool bypass_rlim);
+
 /* Container for pinned pfns / pages */
 struct frame_vector {
 	unsigned int nr_allocated;	/* Number of frames we have space for */
* Unmerged path mm/util.c
