mm: remove the unused MIGRATE_PFN_DEVICE flag

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [mm] remove the unused MIGRATE_PFN_DEVICE flag (Michael Roth) [1851259]
Rebuild_FUZZ: 95.35%
commit-author Christoph Hellwig <hch@lst.de>
commit 06d462beb470d361ffa8bd7b3d865509a8606987
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/06d462be.failed

No one ever checks this flag, and we could easily get that information
from the page if needed.

Link: https://lore.kernel.org/r/20190814075928.23766-10-hch@lst.de
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Tested-by: Ralph Campbell <rcampbell@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 06d462beb470d361ffa8bd7b3d865509a8606987)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/nouveau/nouveau_dmem.c
#	include/linux/migrate.h
diff --cc drivers/gpu/drm/nouveau/nouveau_dmem.c
index 98c1d07bfd14,fa1439941596..000000000000
--- a/drivers/gpu/drm/nouveau/nouveau_dmem.c
+++ b/drivers/gpu/drm/nouveau/nouveau_dmem.c
@@@ -642,144 -557,65 +642,148 @@@ out_free
  	drm->dmem = NULL;
  }
  
 -static unsigned long nouveau_dmem_migrate_copy_one(struct nouveau_drm *drm,
 -		unsigned long src, dma_addr_t *dma_addr)
 +static void
 +nouveau_dmem_migrate_alloc_and_copy(struct vm_area_struct *vma,
 +				    const unsigned long *src_pfns,
 +				    unsigned long *dst_pfns,
 +				    unsigned long start,
 +				    unsigned long end,
 +				    struct nouveau_migrate *migrate)
  {
 +	struct nouveau_drm *drm = migrate->drm;
  	struct device *dev = drm->dev->dev;
 -	struct page *dpage, *spage;
 +	unsigned long addr, i, npages = 0;
 +	nouveau_migrate_copy_t copy;
 +	int ret;
  
 -	spage = migrate_pfn_to_page(src);
 -	if (!spage || !(src & MIGRATE_PFN_MIGRATE))
 -		goto out;
 +	/* First allocate new memory */
 +	for (addr = start, i = 0; addr < end; addr += PAGE_SIZE, i++) {
 +		struct page *dpage, *spage;
  
 -	dpage = nouveau_dmem_page_alloc_locked(drm);
 -	if (!dpage)
 -		return 0;
 +		dst_pfns[i] = 0;
 +		spage = migrate_pfn_to_page(src_pfns[i]);
 +		if (!spage || !(src_pfns[i] & MIGRATE_PFN_MIGRATE))
 +			continue;
  
 -	*dma_addr = dma_map_page(dev, spage, 0, PAGE_SIZE, DMA_BIDIRECTIONAL);
 -	if (dma_mapping_error(dev, *dma_addr))
 -		goto out_free_page;
 +		dpage = nouveau_dmem_page_alloc_locked(drm);
 +		if (!dpage)
 +			continue;
  
 -	if (drm->dmem->migrate.copy_func(drm, 1, NOUVEAU_APER_VRAM,
 -			nouveau_dmem_page_addr(dpage), NOUVEAU_APER_HOST,
 -			*dma_addr))
 -		goto out_dma_unmap;
 +		dst_pfns[i] = migrate_pfn(page_to_pfn(dpage)) |
 +			      MIGRATE_PFN_LOCKED |
 +			      MIGRATE_PFN_DEVICE;
 +		npages++;
 +	}
  
++<<<<<<< HEAD
 +	if (!npages)
 +		return;
++=======
+ 	return migrate_pfn(page_to_pfn(dpage)) | MIGRATE_PFN_LOCKED;
++>>>>>>> 06d462beb470 (mm: remove the unused MIGRATE_PFN_DEVICE flag)
  
 -out_dma_unmap:
 -	dma_unmap_page(dev, *dma_addr, PAGE_SIZE, DMA_BIDIRECTIONAL);
 -out_free_page:
 -	nouveau_dmem_page_free_locked(drm, dpage);
 -out:
 -	return 0;
 +	/* Allocate storage for DMA addresses, so we can unmap later. */
 +	migrate->dma = kmalloc(sizeof(*migrate->dma) * npages, GFP_KERNEL);
 +	if (!migrate->dma)
 +		goto error;
 +
 +	/* Copy things over */
 +	copy = drm->dmem->migrate.copy_func;
 +	for (addr = start, i = 0; addr < end; addr += PAGE_SIZE, i++) {
 +		struct nouveau_dmem_chunk *chunk;
 +		struct page *spage, *dpage;
 +		u64 src_addr, dst_addr;
 +
 +		dpage = migrate_pfn_to_page(dst_pfns[i]);
 +		if (!dpage || dst_pfns[i] == MIGRATE_PFN_ERROR)
 +			continue;
 +
 +		chunk = dpage->zone_device_data;
 +		dst_addr = page_to_pfn(dpage) - chunk->pfn_first;
 +		dst_addr = (dst_addr << PAGE_SHIFT) + chunk->bo->bo.offset;
 +
 +		spage = migrate_pfn_to_page(src_pfns[i]);
 +		if (!spage || !(src_pfns[i] & MIGRATE_PFN_MIGRATE)) {
 +			nouveau_dmem_page_free_locked(drm, dpage);
 +			dst_pfns[i] = 0;
 +			continue;
 +		}
 +
 +		migrate->dma[migrate->dma_nr] =
 +			dma_map_page_attrs(dev, spage, 0, PAGE_SIZE,
 +					   PCI_DMA_BIDIRECTIONAL,
 +					   DMA_ATTR_SKIP_CPU_SYNC);
 +		if (dma_mapping_error(dev, migrate->dma[migrate->dma_nr])) {
 +			nouveau_dmem_page_free_locked(drm, dpage);
 +			dst_pfns[i] = 0;
 +			continue;
 +		}
 +
 +		src_addr = migrate->dma[migrate->dma_nr++];
 +
 +		ret = copy(drm, 1, NOUVEAU_APER_VRAM, dst_addr,
 +				   NOUVEAU_APER_HOST, src_addr);
 +		if (ret) {
 +			nouveau_dmem_page_free_locked(drm, dpage);
 +			dst_pfns[i] = 0;
 +			continue;
 +		}
 +	}
 +
 +	nouveau_fence_new(drm->dmem->migrate.chan, false, &migrate->fence);
 +
 +	return;
 +
 +error:
 +	for (addr = start, i = 0; addr < end; addr += PAGE_SIZE, ++i) {
 +		struct page *page;
 +
 +		if (!dst_pfns[i] || dst_pfns[i] == MIGRATE_PFN_ERROR)
 +			continue;
 +
 +		page = migrate_pfn_to_page(dst_pfns[i]);
 +		dst_pfns[i] = MIGRATE_PFN_ERROR;
 +		if (page == NULL)
 +			continue;
 +
 +		__free_page(page);
 +	}
  }
  
 -static void nouveau_dmem_migrate_chunk(struct nouveau_drm *drm,
 -		struct migrate_vma *args, dma_addr_t *dma_addrs)
 +static void
 +nouveau_dmem_migrate_finalize_and_map(struct nouveau_migrate *migrate)
  {
 -	struct nouveau_fence *fence;
 -	unsigned long addr = args->start, nr_dma = 0, i;
 -
 -	for (i = 0; addr < args->end; i++) {
 -		args->dst[i] = nouveau_dmem_migrate_copy_one(drm, args->src[i],
 -				dma_addrs + nr_dma);
 -		if (args->dst[i])
 -			nr_dma++;
 -		addr += PAGE_SIZE;
 -	}
 +	struct nouveau_drm *drm = migrate->drm;
  
 -	nouveau_fence_new(drm->dmem->migrate.chan, false, &fence);
 -	migrate_vma_pages(args);
 -	nouveau_dmem_fence_done(&fence);
 +	if (migrate->fence) {
 +		nouveau_fence_wait(migrate->fence, true, false);
 +		nouveau_fence_unref(&migrate->fence);
 +	} else {
 +		/*
 +		 * FIXME wait for channel to be IDLE before finalizing
 +		 * the hmem object below (nouveau_migrate_hmem_fini()) ?
 +		 */
 +	}
  
 -	while (nr_dma--) {
 -		dma_unmap_page(drm->dev->dev, dma_addrs[nr_dma], PAGE_SIZE,
 -				DMA_BIDIRECTIONAL);
 +	while (migrate->dma_nr--) {
 +		dma_unmap_page(drm->dev->dev, migrate->dma[migrate->dma_nr],
 +			       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
  	}
 +	kfree(migrate->dma);
 +
  	/*
 -	 * FIXME optimization: update GPU page table to point to newly migrated
 -	 * memory.
 +	 * FIXME optimization: update GPU page table to point to newly
 +	 * migrated memory.
  	 */
 +}
 +
 +static void nouveau_dmem_migrate_chunk(struct migrate_vma *args,
 +		struct nouveau_migrate *migrate)
 +{
 +	nouveau_dmem_migrate_alloc_and_copy(args->vma, args->src, args->dst,
 +			args->start, args->end, migrate);
 +	migrate_vma_pages(args);
 +	nouveau_dmem_migrate_finalize_and_map(migrate);
  	migrate_vma_finalize(args);
  }
  
diff --cc include/linux/migrate.h
index a0724547188c,72120061b7d4..000000000000
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@@ -168,8 -166,6 +168,11 @@@ static inline int migrate_misplaced_tra
  #define MIGRATE_PFN_MIGRATE	(1UL << 1)
  #define MIGRATE_PFN_LOCKED	(1UL << 2)
  #define MIGRATE_PFN_WRITE	(1UL << 3)
++<<<<<<< HEAD
 +#define MIGRATE_PFN_DEVICE	(1UL << 4)
 +#define MIGRATE_PFN_ERROR	(1UL << 5)
++=======
++>>>>>>> 06d462beb470 (mm: remove the unused MIGRATE_PFN_DEVICE flag)
  #define MIGRATE_PFN_SHIFT	6
  
  static inline struct page *migrate_pfn_to_page(unsigned long mpfn)
* Unmerged path drivers/gpu/drm/nouveau/nouveau_dmem.c
* Unmerged path include/linux/migrate.h
diff --git a/mm/migrate.c b/mm/migrate.c
index 88c7080dca57..9fd865c403f9 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -2242,8 +2242,8 @@ static int migrate_vma_collect_pmd(pmd_t *pmdp,
 				goto next;
 
 			page = device_private_entry_to_page(entry);
-			mpfn = migrate_pfn(page_to_pfn(page))|
-				MIGRATE_PFN_DEVICE | MIGRATE_PFN_MIGRATE;
+			mpfn = migrate_pfn(page_to_pfn(page)) |
+					MIGRATE_PFN_MIGRATE;
 			if (is_write_device_private_entry(entry))
 				mpfn |= MIGRATE_PFN_WRITE;
 		} else {
