net/sched: act_ct: fix lockdep splat in tcf_ct_flow_table_get

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [net] sched: act_ct: fix lockdep splat in tcf_ct_flow_table_get (Ivan Vecera) [1824071]
Rebuild_FUZZ: 96.61%
commit-author Eric Dumazet <edumazet@google.com>
commit 138470a9b2cc2e26e6018300394afc3858a54e6a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/138470a9.failed

Convert zones_lock spinlock to zones_mutex mutex,
and struct (tcf_ct_flow_table)->ref to a refcount,
so that control path can use regular GFP_KERNEL allocations
from standard process context. This is more robust
in case of memory pressure.

The refcount is needed because tcf_ct_flow_table_put() can
be called from RCU callback, thus in BH context.

The issue was spotted by syzbot, as rhashtable_init()
was called with a spinlock held, which is bad since GFP_KERNEL
allocations can sleep.

Note to developers : Please make sure your patches are tested
with CONFIG_DEBUG_ATOMIC_SLEEP=y

BUG: sleeping function called from invalid context at mm/slab.h:565
in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 9582, name: syz-executor610
2 locks held by syz-executor610/9582:
 #0: ffffffff8a34eb80 (rtnl_mutex){+.+.}, at: rtnl_lock net/core/rtnetlink.c:72 [inline]
 #0: ffffffff8a34eb80 (rtnl_mutex){+.+.}, at: rtnetlink_rcv_msg+0x3f9/0xad0 net/core/rtnetlink.c:5437
 #1: ffffffff8a3961b8 (zones_lock){+...}, at: spin_lock_bh include/linux/spinlock.h:343 [inline]
 #1: ffffffff8a3961b8 (zones_lock){+...}, at: tcf_ct_flow_table_get+0xa3/0x1700 net/sched/act_ct.c:67
Preemption disabled at:
[<0000000000000000>] 0x0
CPU: 0 PID: 9582 Comm: syz-executor610 Not tainted 5.6.0-rc3-syzkaller #0
Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
Call Trace:
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x188/0x20d lib/dump_stack.c:118
 ___might_sleep.cold+0x1f4/0x23d kernel/sched/core.c:6798
 slab_pre_alloc_hook mm/slab.h:565 [inline]
 slab_alloc_node mm/slab.c:3227 [inline]
 kmem_cache_alloc_node_trace+0x272/0x790 mm/slab.c:3593
 __do_kmalloc_node mm/slab.c:3615 [inline]
 __kmalloc_node+0x38/0x60 mm/slab.c:3623
 kmalloc_node include/linux/slab.h:578 [inline]
 kvmalloc_node+0x61/0xf0 mm/util.c:574
 kvmalloc include/linux/mm.h:645 [inline]
 kvzalloc include/linux/mm.h:653 [inline]
 bucket_table_alloc+0x8b/0x480 lib/rhashtable.c:175
 rhashtable_init+0x3d2/0x750 lib/rhashtable.c:1054
 nf_flow_table_init+0x16d/0x310 net/netfilter/nf_flow_table_core.c:498
 tcf_ct_flow_table_get+0xe33/0x1700 net/sched/act_ct.c:82
 tcf_ct_init+0xba4/0x18a6 net/sched/act_ct.c:1050
 tcf_action_init_1+0x697/0xa20 net/sched/act_api.c:945
 tcf_action_init+0x1e9/0x2f0 net/sched/act_api.c:1001
 tcf_action_add+0xdb/0x370 net/sched/act_api.c:1411
 tc_ctl_action+0x366/0x456 net/sched/act_api.c:1466
 rtnetlink_rcv_msg+0x44e/0xad0 net/core/rtnetlink.c:5440
 netlink_rcv_skb+0x15a/0x410 net/netlink/af_netlink.c:2478
 netlink_unicast_kernel net/netlink/af_netlink.c:1303 [inline]
 netlink_unicast+0x537/0x740 net/netlink/af_netlink.c:1329
 netlink_sendmsg+0x882/0xe10 net/netlink/af_netlink.c:1918
 sock_sendmsg_nosec net/socket.c:652 [inline]
 sock_sendmsg+0xcf/0x120 net/socket.c:672
 ____sys_sendmsg+0x6b9/0x7d0 net/socket.c:2343
 ___sys_sendmsg+0x100/0x170 net/socket.c:2397
 __sys_sendmsg+0xec/0x1b0 net/socket.c:2430
 do_syscall_64+0xf6/0x790 arch/x86/entry/common.c:294
 entry_SYSCALL_64_after_hwframe+0x49/0xbe
RIP: 0033:0x4403d9
Code: 18 89 d0 c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 fb 13 fc ff c3 66 2e 0f 1f 84 00 00 00 00
RSP: 002b:00007ffd719af218 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
RAX: ffffffffffffffda RBX: 00000000004002c8 RCX: 00000000004403d9
RDX: 0000000000000000 RSI: 0000000020000300 RDI: 0000000000000003
RBP: 00000000006ca018 R08: 0000000000000005 R09: 00000000004002c8
R10: 0000000000000008 R11: 00000000000

Fixes: c34b961a2492 ("net/sched: act_ct: Create nf flow table per zone")
	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Paul Blakey <paulb@mellanox.com>
	Cc: Jiri Pirko <jiri@mellanox.com>
	Reported-by: syzbot <syzkaller@googlegroups.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 138470a9b2cc2e26e6018300394afc3858a54e6a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/act_ct.c
diff --cc net/sched/act_ct.c
index f67211a791ce,3d9e678d7d53..000000000000
--- a/net/sched/act_ct.c
+++ b/net/sched/act_ct.c
@@@ -30,6 -31,309 +30,312 @@@
  #include <net/netfilter/nf_conntrack_zones.h>
  #include <net/netfilter/nf_conntrack_helper.h>
  #include <net/netfilter/ipv6/nf_defrag_ipv6.h>
++<<<<<<< HEAD
++=======
+ #include <uapi/linux/netfilter/nf_nat.h>
+ 
+ static struct workqueue_struct *act_ct_wq;
+ static struct rhashtable zones_ht;
+ static DEFINE_MUTEX(zones_mutex);
+ 
+ struct tcf_ct_flow_table {
+ 	struct rhash_head node; /* In zones tables */
+ 
+ 	struct rcu_work rwork;
+ 	struct nf_flowtable nf_ft;
+ 	refcount_t ref;
+ 	u16 zone;
+ 
+ 	bool dying;
+ };
+ 
+ static const struct rhashtable_params zones_params = {
+ 	.head_offset = offsetof(struct tcf_ct_flow_table, node),
+ 	.key_offset = offsetof(struct tcf_ct_flow_table, zone),
+ 	.key_len = sizeof_field(struct tcf_ct_flow_table, zone),
+ 	.automatic_shrinking = true,
+ };
+ 
+ static struct nf_flowtable_type flowtable_ct = {
+ 	.owner		= THIS_MODULE,
+ };
+ 
+ static int tcf_ct_flow_table_get(struct tcf_ct_params *params)
+ {
+ 	struct tcf_ct_flow_table *ct_ft;
+ 	int err = -ENOMEM;
+ 
+ 	mutex_lock(&zones_mutex);
+ 	ct_ft = rhashtable_lookup_fast(&zones_ht, &params->zone, zones_params);
+ 	if (ct_ft && refcount_inc_not_zero(&ct_ft->ref))
+ 		goto out_unlock;
+ 
+ 	ct_ft = kzalloc(sizeof(*ct_ft), GFP_KERNEL);
+ 	if (!ct_ft)
+ 		goto err_alloc;
+ 	refcount_set(&ct_ft->ref, 1);
+ 
+ 	ct_ft->zone = params->zone;
+ 	err = rhashtable_insert_fast(&zones_ht, &ct_ft->node, zones_params);
+ 	if (err)
+ 		goto err_insert;
+ 
+ 	ct_ft->nf_ft.type = &flowtable_ct;
+ 	err = nf_flow_table_init(&ct_ft->nf_ft);
+ 	if (err)
+ 		goto err_init;
+ 
+ 	__module_get(THIS_MODULE);
+ out_unlock:
+ 	params->ct_ft = ct_ft;
+ 	mutex_unlock(&zones_mutex);
+ 
+ 	return 0;
+ 
+ err_init:
+ 	rhashtable_remove_fast(&zones_ht, &ct_ft->node, zones_params);
+ err_insert:
+ 	kfree(ct_ft);
+ err_alloc:
+ 	mutex_unlock(&zones_mutex);
+ 	return err;
+ }
+ 
+ static void tcf_ct_flow_table_cleanup_work(struct work_struct *work)
+ {
+ 	struct tcf_ct_flow_table *ct_ft;
+ 
+ 	ct_ft = container_of(to_rcu_work(work), struct tcf_ct_flow_table,
+ 			     rwork);
+ 	nf_flow_table_free(&ct_ft->nf_ft);
+ 	kfree(ct_ft);
+ 
+ 	module_put(THIS_MODULE);
+ }
+ 
+ static void tcf_ct_flow_table_put(struct tcf_ct_params *params)
+ {
+ 	struct tcf_ct_flow_table *ct_ft = params->ct_ft;
+ 
+ 	if (refcount_dec_and_test(&params->ct_ft->ref)) {
+ 		rhashtable_remove_fast(&zones_ht, &ct_ft->node, zones_params);
+ 		INIT_RCU_WORK(&ct_ft->rwork, tcf_ct_flow_table_cleanup_work);
+ 		queue_rcu_work(act_ct_wq, &ct_ft->rwork);
+ 	}
+ }
+ 
+ static void tcf_ct_flow_table_add(struct tcf_ct_flow_table *ct_ft,
+ 				  struct nf_conn *ct,
+ 				  bool tcp)
+ {
+ 	struct flow_offload *entry;
+ 	int err;
+ 
+ 	if (test_and_set_bit(IPS_OFFLOAD_BIT, &ct->status))
+ 		return;
+ 
+ 	entry = flow_offload_alloc(ct);
+ 	if (!entry) {
+ 		WARN_ON_ONCE(1);
+ 		goto err_alloc;
+ 	}
+ 
+ 	if (tcp) {
+ 		ct->proto.tcp.seen[0].flags |= IP_CT_TCP_FLAG_BE_LIBERAL;
+ 		ct->proto.tcp.seen[1].flags |= IP_CT_TCP_FLAG_BE_LIBERAL;
+ 	}
+ 
+ 	err = flow_offload_add(&ct_ft->nf_ft, entry);
+ 	if (err)
+ 		goto err_add;
+ 
+ 	return;
+ 
+ err_add:
+ 	flow_offload_free(entry);
+ err_alloc:
+ 	clear_bit(IPS_OFFLOAD_BIT, &ct->status);
+ }
+ 
+ static void tcf_ct_flow_table_process_conn(struct tcf_ct_flow_table *ct_ft,
+ 					   struct nf_conn *ct,
+ 					   enum ip_conntrack_info ctinfo)
+ {
+ 	bool tcp = false;
+ 
+ 	if (ctinfo != IP_CT_ESTABLISHED && ctinfo != IP_CT_ESTABLISHED_REPLY)
+ 		return;
+ 
+ 	switch (nf_ct_protonum(ct)) {
+ 	case IPPROTO_TCP:
+ 		tcp = true;
+ 		if (ct->proto.tcp.state != TCP_CONNTRACK_ESTABLISHED)
+ 			return;
+ 		break;
+ 	case IPPROTO_UDP:
+ 		break;
+ 	default:
+ 		return;
+ 	}
+ 
+ 	if (nf_ct_ext_exist(ct, NF_CT_EXT_HELPER) ||
+ 	    ct->status & IPS_SEQ_ADJUST)
+ 		return;
+ 
+ 	tcf_ct_flow_table_add(ct_ft, ct, tcp);
+ }
+ 
+ static bool
+ tcf_ct_flow_table_fill_tuple_ipv4(struct sk_buff *skb,
+ 				  struct flow_offload_tuple *tuple,
+ 				  struct tcphdr **tcph)
+ {
+ 	struct flow_ports *ports;
+ 	unsigned int thoff;
+ 	struct iphdr *iph;
+ 
+ 	if (!pskb_network_may_pull(skb, sizeof(*iph)))
+ 		return false;
+ 
+ 	iph = ip_hdr(skb);
+ 	thoff = iph->ihl * 4;
+ 
+ 	if (ip_is_fragment(iph) ||
+ 	    unlikely(thoff != sizeof(struct iphdr)))
+ 		return false;
+ 
+ 	if (iph->protocol != IPPROTO_TCP &&
+ 	    iph->protocol != IPPROTO_UDP)
+ 		return false;
+ 
+ 	if (iph->ttl <= 1)
+ 		return false;
+ 
+ 	if (!pskb_network_may_pull(skb, iph->protocol == IPPROTO_TCP ?
+ 					thoff + sizeof(struct tcphdr) :
+ 					thoff + sizeof(*ports)))
+ 		return false;
+ 
+ 	iph = ip_hdr(skb);
+ 	if (iph->protocol == IPPROTO_TCP)
+ 		*tcph = (void *)(skb_network_header(skb) + thoff);
+ 
+ 	ports = (struct flow_ports *)(skb_network_header(skb) + thoff);
+ 	tuple->src_v4.s_addr = iph->saddr;
+ 	tuple->dst_v4.s_addr = iph->daddr;
+ 	tuple->src_port = ports->source;
+ 	tuple->dst_port = ports->dest;
+ 	tuple->l3proto = AF_INET;
+ 	tuple->l4proto = iph->protocol;
+ 
+ 	return true;
+ }
+ 
+ static bool
+ tcf_ct_flow_table_fill_tuple_ipv6(struct sk_buff *skb,
+ 				  struct flow_offload_tuple *tuple,
+ 				  struct tcphdr **tcph)
+ {
+ 	struct flow_ports *ports;
+ 	struct ipv6hdr *ip6h;
+ 	unsigned int thoff;
+ 
+ 	if (!pskb_network_may_pull(skb, sizeof(*ip6h)))
+ 		return false;
+ 
+ 	ip6h = ipv6_hdr(skb);
+ 
+ 	if (ip6h->nexthdr != IPPROTO_TCP &&
+ 	    ip6h->nexthdr != IPPROTO_UDP)
+ 		return false;
+ 
+ 	if (ip6h->hop_limit <= 1)
+ 		return false;
+ 
+ 	thoff = sizeof(*ip6h);
+ 	if (!pskb_network_may_pull(skb, ip6h->nexthdr == IPPROTO_TCP ?
+ 					thoff + sizeof(struct tcphdr) :
+ 					thoff + sizeof(*ports)))
+ 		return false;
+ 
+ 	ip6h = ipv6_hdr(skb);
+ 	if (ip6h->nexthdr == IPPROTO_TCP)
+ 		*tcph = (void *)(skb_network_header(skb) + thoff);
+ 
+ 	ports = (struct flow_ports *)(skb_network_header(skb) + thoff);
+ 	tuple->src_v6 = ip6h->saddr;
+ 	tuple->dst_v6 = ip6h->daddr;
+ 	tuple->src_port = ports->source;
+ 	tuple->dst_port = ports->dest;
+ 	tuple->l3proto = AF_INET6;
+ 	tuple->l4proto = ip6h->nexthdr;
+ 
+ 	return true;
+ }
+ 
+ static bool tcf_ct_flow_table_lookup(struct tcf_ct_params *p,
+ 				     struct sk_buff *skb,
+ 				     u8 family)
+ {
+ 	struct nf_flowtable *nf_ft = &p->ct_ft->nf_ft;
+ 	struct flow_offload_tuple_rhash *tuplehash;
+ 	struct flow_offload_tuple tuple = {};
+ 	enum ip_conntrack_info ctinfo;
+ 	struct tcphdr *tcph = NULL;
+ 	struct flow_offload *flow;
+ 	struct nf_conn *ct;
+ 	u8 dir;
+ 
+ 	/* Previously seen or loopback */
+ 	ct = nf_ct_get(skb, &ctinfo);
+ 	if ((ct && !nf_ct_is_template(ct)) || ctinfo == IP_CT_UNTRACKED)
+ 		return false;
+ 
+ 	switch (family) {
+ 	case NFPROTO_IPV4:
+ 		if (!tcf_ct_flow_table_fill_tuple_ipv4(skb, &tuple, &tcph))
+ 			return false;
+ 		break;
+ 	case NFPROTO_IPV6:
+ 		if (!tcf_ct_flow_table_fill_tuple_ipv6(skb, &tuple, &tcph))
+ 			return false;
+ 		break;
+ 	default:
+ 		return false;
+ 	}
+ 
+ 	tuplehash = flow_offload_lookup(nf_ft, &tuple);
+ 	if (!tuplehash)
+ 		return false;
+ 
+ 	dir = tuplehash->tuple.dir;
+ 	flow = container_of(tuplehash, struct flow_offload, tuplehash[dir]);
+ 	ct = flow->ct;
+ 
+ 	if (tcph && (unlikely(tcph->fin || tcph->rst))) {
+ 		flow_offload_teardown(flow);
+ 		return false;
+ 	}
+ 
+ 	ctinfo = dir == FLOW_OFFLOAD_DIR_ORIGINAL ? IP_CT_ESTABLISHED :
+ 						    IP_CT_ESTABLISHED_REPLY;
+ 
+ 	nf_conntrack_get(&ct->ct_general);
+ 	nf_ct_set(skb, ct, ctinfo);
+ 
+ 	return true;
+ }
+ 
+ static int tcf_ct_flow_tables_init(void)
+ {
+ 	return rhashtable_init(&zones_ht, &zones_params);
+ }
+ 
+ static void tcf_ct_flow_tables_uninit(void)
+ {
+ 	rhashtable_destroy(&zones_ht);
+ }
++>>>>>>> 138470a9b2cc (net/sched: act_ct: fix lockdep splat in tcf_ct_flow_table_get)
  
  static struct tc_action_ops act_ct_ops;
  static unsigned int ct_net_id;
* Unmerged path net/sched/act_ct.c
