io_uring: replace workqueue usage with io-wq

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 561fb04a6a2257716738dac2ed812f377c2634c2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/561fb04a.failed

Drop various work-arounds we have for workqueues:

- We no longer need the async_list for tracking sequential IO.

- We don't have to maintain our own mm tracking/setting.

- We don't need a separate workqueue for buffered writes. This didn't
  even work that well to begin with, as it was suboptimal for multiple
  buffered writers on multiple files.

- We can properly cancel pending interruptible work. This fixes
  deadlocks with particularly socket IO, where we cannot cancel them
  when the io_uring is closed. Hence the ring will wait forever for
  these requests to complete, which may never happen. This is different
  from disk IO where we know requests will complete in a finite amount
  of time.

- Due to being able to cancel work interruptible work that is already
  running, we can implement file table support for work. We need that
  for supporting system calls that add to a process file table.

- It gets us one step closer to adding async support for any system
  call.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 561fb04a6a2257716738dac2ed812f377c2634c2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/trace/events/io_uring.h
#	init/Kconfig
diff --cc fs/io_uring.c
index 7f974c7eddcc,d94bd4e3a60e..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -367,7 -347,9 +355,13 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
++=======
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_ring_ctx *ctx, u64 ki_user_data,
+ 				 long res);
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  static void __io_free_req(struct io_kiocb *req);
  
  static struct kmem_cache *req_cachep;
@@@ -476,12 -477,52 +471,33 @@@ static inline bool io_prep_async_work(s
  		switch (req->submit.sqe->opcode) {
  		case IORING_OP_WRITEV:
  		case IORING_OP_WRITE_FIXED:
- 			rw = !(req->rw.ki_flags & IOCB_DIRECT);
+ 			do_hashed = true;
  			break;
  		}
+ 		if (io_sqe_needs_user(req->submit.sqe))
+ 			req->work.flags |= IO_WQ_WORK_NEEDS_USER;
  	}
  
++<<<<<<< HEAD
 +	queue_work(ctx->sqo_wq[rw], &req->work);
++=======
+ 	return do_hashed;
+ }
+ 
+ static inline void io_queue_async_work(struct io_ring_ctx *ctx,
+ 				       struct io_kiocb *req)
+ {
+ 	bool do_hashed = io_prep_async_work(req);
+ 
+ 	trace_io_uring_queue_async_work(ctx, do_hashed, req, &req->work,
+ 					req->flags);
+ 	if (!do_hashed) {
+ 		io_wq_enqueue(ctx->io_wq, &req->work);
+ 	} else {
+ 		io_wq_enqueue_hashed(ctx->io_wq, &req->work,
+ 					file_inode(req->file));
+ 	}
 -}
 -
 -static void io_kill_timeout(struct io_kiocb *req)
 -{
 -	int ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->timeout.timer);
 -	if (ret != -1) {
 -		atomic_inc(&req->ctx->cq_timeouts);
 -		list_del(&req->list);
 -		io_cqring_fill_event(req->ctx, req->user_data, 0);
 -		__io_free_req(req);
 -	}
 -}
 -
 -static void io_kill_timeouts(struct io_ring_ctx *ctx)
 -{
 -	struct io_kiocb *req, *tmp;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	list_for_each_entry_safe(req, tmp, &ctx->timeout_list, list)
 -		io_kill_timeout(req);
 -	spin_unlock_irq(&ctx->completion_lock);
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  }
  
  static void io_commit_cqring(struct io_ring_ctx *ctx)
@@@ -651,8 -695,14 +668,19 @@@ static void io_req_link_next(struct io_
  		}
  
  		nxt->flags |= REQ_F_LINK_DONE;
++<<<<<<< HEAD
 +		INIT_WORK(&nxt->work, io_sq_wq_submit_work);
 +		io_queue_async_work(req->ctx, nxt);
++=======
+ 		/*
+ 		 * If we're in async work, we can continue processing the chain
+ 		 * in this context instead of having to queue up new async work.
+ 		 */
+ 		if (nxtptr && current_work())
+ 			*nxtptr = nxt;
+ 		else
+ 			io_queue_async_work(req->ctx, nxt);
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  	}
  }
  
@@@ -690,13 -741,34 +718,30 @@@ static void io_free_req(struct io_kioc
  	__io_free_req(req);
  }
  
 -/*
 - * Drop reference to request, return next in chain (if there is one) if this
 - * was the last reference to this request.
 - */
 -static struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)
 +static void io_put_req(struct io_kiocb *req)
  {
 -	struct io_kiocb *nxt = NULL;
 -
  	if (refcount_dec_and_test(&req->refs))
 -		io_free_req(req, &nxt);
 -
 -	return nxt;
 +		io_free_req(req);
  }
  
++<<<<<<< HEAD
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
++=======
+ static void io_put_req(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	nxt = io_put_req_find_next(req);
+ 	if (nxt) {
+ 		if (nxtptr)
+ 			*nxtptr = nxt;
+ 		else
+ 			io_queue_async_work(nxt->ctx, nxt);
+ 	}
+ }
+ 
+ static unsigned io_cqring_events(struct io_rings *rings)
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  {
  	/* See comment at the top of this file */
  	smp_rmb();
@@@ -1360,20 -1415,15 +1346,25 @@@ static int io_read(struct io_kiocb *req
  		 * need async punt anyway, so it's more efficient to do it
  		 * here.
  		 */
 -		if (force_nonblock && !(req->flags & REQ_F_NOWAIT) &&
 -		    (req->flags & REQ_F_ISREG) &&
 -		    ret2 > 0 && ret2 < read_size)
 +		if (force_nonblock && ret2 > 0 && ret2 < read_size)
  			ret2 = -EAGAIN;
  		/* Catch -EAGAIN return for forced non-blocking submission */
++<<<<<<< HEAD
 +		if (!force_nonblock || ret2 != -EAGAIN) {
 +			io_rw_done(kiocb, ret2);
 +		} else {
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(READ, req, iov_count);
++=======
+ 		if (!force_nonblock || ret2 != -EAGAIN)
+ 			kiocb_done(kiocb, ret2, nxt, s->in_async);
+ 		else
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  			ret = -EAGAIN;
- 		}
  	}
  	kfree(iovec);
  	return ret;
@@@ -1407,12 -1457,8 +1398,15 @@@ static int io_write(struct io_kiocb *re
  	iov_count = iov_iter_count(&iter);
  
  	ret = -EAGAIN;
++<<<<<<< HEAD
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
++=======
+ 	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT))
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  		goto out_free;
- 	}
  
  	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
@@@ -1437,17 -1483,10 +1431,22 @@@
  			ret2 = call_write_iter(file, kiocb, &iter);
  		else
  			ret2 = loop_rw_iter(WRITE, file, kiocb, &iter);
++<<<<<<< HEAD
 +		if (!force_nonblock || ret2 != -EAGAIN) {
 +			io_rw_done(kiocb, ret2);
 +		} else {
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(WRITE, req, iov_count);
++=======
+ 		if (!force_nonblock || ret2 != -EAGAIN)
+ 			kiocb_done(kiocb, ret2, nxt, s->in_async);
+ 		else
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  			ret = -EAGAIN;
- 		}
  	}
  out_free:
  	kfree(iovec);
@@@ -1859,7 -2090,7 +1862,11 @@@ static int io_req_defer(struct io_ring_
  	memcpy(sqe_copy, sqe, sizeof(*sqe_copy));
  	req->submit.sqe = sqe_copy;
  
++<<<<<<< HEAD
 +	INIT_WORK(&req->work, io_sq_wq_submit_work);
++=======
+ 	trace_io_uring_defer(ctx, req, false);
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  	list_add_tail(&req->list, &ctx->defer_list);
  	spin_unlock_irq(&ctx->completion_lock);
  	return -EIOCBQUEUED;
@@@ -1934,179 -2172,56 +1941,179 @@@ static int __io_submit_sqe(struct io_ri
  	return 0;
  }
  
- static struct async_list *io_async_list_from_sqe(struct io_ring_ctx *ctx,
- 						 const struct io_uring_sqe *sqe)
- {
- 	switch (sqe->opcode) {
- 	case IORING_OP_READV:
- 	case IORING_OP_READ_FIXED:
- 		return &ctx->pending_async[READ];
- 	case IORING_OP_WRITEV:
- 	case IORING_OP_WRITE_FIXED:
- 		return &ctx->pending_async[WRITE];
- 	default:
- 		return NULL;
- 	}
- }
- 
- static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
- {
- 	u8 opcode = READ_ONCE(sqe->opcode);
- 
- 	return !(opcode == IORING_OP_READ_FIXED ||
- 		 opcode == IORING_OP_WRITE_FIXED);
- }
- 
- static void io_sq_wq_submit_work(struct work_struct *work)
+ static void io_wq_submit_work(struct io_wq_work **workptr)
  {
+ 	struct io_wq_work *work = *workptr;
  	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
  	struct io_ring_ctx *ctx = req->ctx;
- 	struct mm_struct *cur_mm = NULL;
- 	struct async_list *async_list;
- 	LIST_HEAD(req_list);
- 	mm_segment_t old_fs;
- 	int ret;
+ 	struct sqe_submit *s = &req->submit;
+ 	const struct io_uring_sqe *sqe = s->sqe;
+ 	struct io_kiocb *nxt = NULL;
+ 	int ret = 0;
  
++<<<<<<< HEAD
 +	async_list = io_async_list_from_sqe(ctx, req->submit.sqe);
 +restart:
 +	do {
 +		struct sqe_submit *s = &req->submit;
 +		const struct io_uring_sqe *sqe = s->sqe;
 +		unsigned int flags = req->flags;
++=======
+ 	/* Ensure we clear previously set non-block flag */
+ 	req->rw.ki_flags &= ~IOCB_NOWAIT;
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  
- 		/* Ensure we clear previously set non-block flag */
- 		req->rw.ki_flags &= ~IOCB_NOWAIT;
+ 	if (work->flags & IO_WQ_WORK_CANCEL)
+ 		ret = -ECANCELED;
  
- 		ret = 0;
- 		if (io_sqe_needs_user(sqe) && !cur_mm) {
- 			if (!mmget_not_zero(ctx->sqo_mm)) {
- 				ret = -EFAULT;
- 			} else {
- 				cur_mm = ctx->sqo_mm;
- 				use_mm(cur_mm);
- 				old_fs = get_fs();
- 				set_fs(USER_DS);
- 			}
- 		}
+ 	if (!ret) {
+ 		s->has_user = (work->flags & IO_WQ_WORK_HAS_MM) != 0;
+ 		s->in_async = true;
+ 		do {
+ 			ret = __io_submit_sqe(ctx, req, s, &nxt, false);
+ 			/*
+ 			 * We can get EAGAIN for polled IO even though we're
+ 			 * forcing a sync submission from here, since we can't
+ 			 * wait for request slots on the block side.
+ 			 */
+ 			if (ret != -EAGAIN)
+ 				break;
+ 			cond_resched();
+ 		} while (1);
+ 	}
  
++<<<<<<< HEAD
 +		if (!ret) {
 +			s->has_user = cur_mm != NULL;
 +			s->needs_lock = true;
 +			do {
 +				ret = __io_submit_sqe(ctx, req, s, false);
 +				/*
 +				 * We can get EAGAIN for polled IO even though
 +				 * we're forcing a sync submission from here,
 +				 * since we can't wait for request slots on the
 +				 * block side.
 +				 */
 +				if (ret != -EAGAIN)
 +					break;
 +				cond_resched();
 +			} while (1);
 +		}
 +
 +		/* drop submission reference */
 +		io_put_req(req);
 +
 +		if (ret) {
 +			io_cqring_add_event(ctx, sqe->user_data, ret);
 +			io_put_req(req);
 +		}
 +
 +		/* async context always use a copy of the sqe */
 +		kfree(sqe);
 +
 +		/* req from defer and link list needn't decrease async cnt */
 +		if (flags & (REQ_F_IO_DRAINED | REQ_F_LINK_DONE))
 +			goto out;
 +
 +		if (!async_list)
 +			break;
 +		if (!list_empty(&req_list)) {
 +			req = list_first_entry(&req_list, struct io_kiocb,
 +						list);
 +			list_del(&req->list);
 +			continue;
 +		}
 +		if (list_empty(&async_list->list))
 +			break;
 +
 +		req = NULL;
 +		spin_lock(&async_list->lock);
 +		if (list_empty(&async_list->list)) {
 +			spin_unlock(&async_list->lock);
 +			break;
 +		}
 +		list_splice_init(&async_list->list, &req_list);
 +		spin_unlock(&async_list->lock);
 +
 +		req = list_first_entry(&req_list, struct io_kiocb, list);
 +		list_del(&req->list);
 +	} while (req);
 +
 +	/*
 +	 * Rare case of racing with a submitter. If we find the count has
 +	 * dropped to zero AND we have pending work items, then restart
 +	 * the processing. This is a tiny race window.
 +	 */
 +	if (async_list) {
 +		ret = atomic_dec_return(&async_list->cnt);
 +		while (!ret && !list_empty(&async_list->list)) {
 +			spin_lock(&async_list->lock);
 +			atomic_inc(&async_list->cnt);
 +			list_splice_init(&async_list->list, &req_list);
 +			spin_unlock(&async_list->lock);
 +
 +			if (!list_empty(&req_list)) {
 +				req = list_first_entry(&req_list,
 +							struct io_kiocb, list);
 +				list_del(&req->list);
 +				goto restart;
 +			}
 +			ret = atomic_dec_return(&async_list->cnt);
 +		}
++=======
+ 	/* drop submission reference */
+ 	io_put_req(req, NULL);
+ 
+ 	if (ret) {
+ 		io_cqring_add_event(ctx, sqe->user_data, ret);
+ 		io_put_req(req, NULL);
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  	}
  
- out:
- 	if (cur_mm) {
- 		set_fs(old_fs);
- 		unuse_mm(cur_mm);
- 		mmput(cur_mm);
+ 	/* async context always use a copy of the sqe */
+ 	kfree(sqe);
+ 
+ 	/* if a dependent link is ready, pass it back */
+ 	if (!ret && nxt) {
+ 		io_prep_async_work(nxt);
+ 		*workptr = &nxt->work;
  	}
  }
  
++<<<<<<< HEAD
 +/*
 + * See if we can piggy back onto previously submitted work, that is still
 + * running. We currently only allow this if the new request is sequential
 + * to the previous one we punted.
 + */
 +static bool io_add_to_prev_work(struct async_list *list, struct io_kiocb *req)
 +{
 +	bool ret;
 +
 +	if (!list)
 +		return false;
 +	if (!(req->flags & REQ_F_SEQ_PREV))
 +		return false;
 +	if (!atomic_read(&list->cnt))
 +		return false;
 +
 +	ret = true;
 +	spin_lock(&list->lock);
 +	list_add_tail(&req->list, &list->list);
 +	/*
 +	 * Ensure we see a simultaneous modification from io_sq_wq_submit_work()
 +	 */
 +	smp_mb();
 +	if (!atomic_read(&list->cnt)) {
 +		list_del_init(&req->list);
 +		ret = false;
 +	}
 +	spin_unlock(&list->lock);
 +	return ret;
 +}
 +
++=======
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  static bool io_op_needs_file(const struct io_uring_sqe *sqe)
  {
  	int op = READ_ONCE(sqe->opcode);
@@@ -3547,7 -3694,12 +3526,11 @@@ static void io_ring_ctx_wait_and_kill(s
  	percpu_ref_kill(&ctx->refs);
  	mutex_unlock(&ctx->uring_lock);
  
 -	io_kill_timeouts(ctx);
  	io_poll_remove_all(ctx);
+ 
+ 	if (ctx->io_wq)
+ 		io_wq_cancel_all(ctx->io_wq);
+ 
  	io_iopoll_reap_events(ctx);
  	wait_for_completion(&ctx->ctx_done);
  	io_ring_ctx_free(ctx);
diff --cc init/Kconfig
index 3b1ea0bbdb4e,4d8d145c41d2..000000000000
--- a/init/Kconfig
+++ b/init/Kconfig
@@@ -1390,7 -1548,8 +1390,12 @@@ config AI
  config IO_URING
  	bool "Enable IO uring support" if EXPERT
  	select ANON_INODES
++<<<<<<< HEAD
 +	default n
++=======
+ 	select IO_WQ
+ 	default y
++>>>>>>> 561fb04a6a22 (io_uring: replace workqueue usage with io-wq)
  	help
  	  This option enables support for the io_uring interface, enabling
  	  applications to submit and complete IO through submission and
* Unmerged path include/trace/events/io_uring.h
* Unmerged path fs/io_uring.c
* Unmerged path include/trace/events/io_uring.h
* Unmerged path init/Kconfig
