libperf: Move mask setup to perf_evlist__mmap_ops()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jiri Olsa <jolsa@kernel.org>
commit b6cd35e4e09c12f9478ed98cb015d4352fa98056
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b6cd35e4.failed

Move the mask setup to perf_evlist__mmap_ops(), because it's the same on
both perf and libperf path.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Jin Yao <yao.jin@linux.intel.com>
	Cc: Michael Petlan <mpetlan@redhat.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
Link: http://lore.kernel.org/lkml/20191017105918.20873-4-jolsa@kernel.org
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit b6cd35e4e09c12f9478ed98cb015d4352fa98056)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/lib/evlist.c
#	tools/perf/util/evlist.c
diff --cc tools/perf/util/evlist.c
index 29a998d183ce,fdce590d2278..000000000000
--- a/tools/perf/util/evlist.c
+++ b/tools/perf/util/evlist.c
@@@ -1023,47 -799,37 +1023,52 @@@ int perf_evlist__mmap_ex(struct perf_ev
  	 * Its value is decided by evsel's write_backward.
  	 * So &mp should not be passed through const pointer.
  	 */
 -	struct mmap_params mp = {
 -		.nr_cblocks	= nr_cblocks,
 -		.affinity	= affinity,
 -		.flush		= flush,
 -		.comp_level	= comp_level
 -	};
 -	struct perf_evlist_mmap_ops ops = {
 -		.idx  = perf_evlist__mmap_cb_idx,
 -		.get  = perf_evlist__mmap_cb_get,
 -		.mmap = perf_evlist__mmap_cb_mmap,
 -	};
 +	struct mmap_params mp = { .nr_cblocks = nr_cblocks, .affinity = affinity, .flush = flush,
 +				  .comp_level = comp_level };
  
++<<<<<<< HEAD
 +	if (!evlist->mmap)
 +		evlist->mmap = perf_evlist__alloc_mmap(evlist, false);
 +	if (!evlist->mmap)
 +		return -ENOMEM;
++=======
+ 	evlist->core.mmap_len = evlist__mmap_size(pages);
+ 	pr_debug("mmap size %zuB\n", evlist->core.mmap_len);
++>>>>>>> b6cd35e4e09c (libperf: Move mask setup to perf_evlist__mmap_ops())
 +
 +	if (evlist->pollfd.entries == NULL && perf_evlist__alloc_pollfd(evlist) < 0)
 +		return -ENOMEM;
 +
 +	evlist->mmap_len = perf_evlist__mmap_size(pages);
 +	pr_debug("mmap size %zuB\n", evlist->mmap_len);
 +	mp.mask = evlist->mmap_len - page_size - 1;
  
 -	auxtrace_mmap_params__init(&mp.auxtrace_mp, evlist->core.mmap_len,
 +	auxtrace_mmap_params__init(&mp.auxtrace_mp, evlist->mmap_len,
  				   auxtrace_pages, auxtrace_overwrite);
  
 -	return perf_evlist__mmap_ops(&evlist->core, &ops, &mp.core);
 +	evlist__for_each_entry(evlist, evsel) {
 +		if ((evsel->attr.read_format & PERF_FORMAT_ID) &&
 +		    evsel->sample_id == NULL &&
 +		    perf_evsel__alloc_id(evsel, cpu_map__nr(cpus), threads->nr) < 0)
 +			return -ENOMEM;
 +	}
 +
 +	if (cpu_map__empty(cpus))
 +		return perf_evlist__mmap_per_thread(evlist, &mp);
 +
 +	return perf_evlist__mmap_per_cpu(evlist, &mp);
  }
  
 -int evlist__mmap(struct evlist *evlist, unsigned int pages)
 +int perf_evlist__mmap(struct perf_evlist *evlist, unsigned int pages)
  {
 -	return evlist__mmap_ex(evlist, pages, 0, false, 0, PERF_AFFINITY_SYS, 1, 0);
 +	return perf_evlist__mmap_ex(evlist, pages, 0, false, 0, PERF_AFFINITY_SYS, 1, 0);
  }
  
 -int perf_evlist__create_maps(struct evlist *evlist, struct target *target)
 +int perf_evlist__create_maps(struct perf_evlist *evlist, struct target *target)
  {
  	bool all_threads = (target->per_thread && target->system_wide);
 -	struct perf_cpu_map *cpus;
 -	struct perf_thread_map *threads;
 +	struct cpu_map *cpus;
 +	struct thread_map *threads;
  
  	/*
  	 * If specify '-a' and '--per-thread' to perf record, perf record
* Unmerged path tools/perf/lib/evlist.c
* Unmerged path tools/perf/lib/evlist.c
* Unmerged path tools/perf/util/evlist.c
