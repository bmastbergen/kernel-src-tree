KVM: x86: Dynamically allocate per-vCPU emulation context

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit c9b8b07cded58c55ad2bf67e68b9bfae96092293
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/c9b8b07c.failed

Allocate the emulation context instead of embedding it in struct
kvm_vcpu_arch.

Dynamic allocation provides several benefits:

  - Shrinks the size x86 vcpus by ~2.5k bytes, dropping them back below
    the PAGE_ALLOC_COSTLY_ORDER threshold.
  - Allows for dropping the include of kvm_emulate.h from asm/kvm_host.h
    and moving kvm_emulate.h into KVM's private directory.
  - Allows a reducing KVM's attack surface by shrinking the amount of
    vCPU data that is exposed to usercopy.
  - Allows a future patch to disable the emulator entirely, which may or
    may not be a realistic endeavor.

Mark the entire struct as valid for usercopy to maintain existing
behavior with respect to hardened usercopy.  Future patches can shrink
the usercopy range to cover only what is necessary.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c9b8b07cded58c55ad2bf67e68b9bfae96092293)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/x86.c
index f8b28602562f,4465975f034b..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -9136,36 -9251,93 +9174,111 @@@ static void fx_init(struct kvm_vcpu *vc
  	vcpu->arch.cr0 |= X86_CR0_ET;
  }
  
 -int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 +void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
  {
 -	if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
 -		pr_warn_once("kvm: SMP vm created on host with unstable TSC; "
 -			     "guest TSC will not be reliable\n");
 +	void *wbinvd_dirty_mask = vcpu->arch.wbinvd_dirty_mask;
 +	struct gfn_to_pfn_cache *cache = &vcpu->arch.st.cache;
  
 -	return 0;
 +	kvm_release_pfn(cache->pfn, cache->dirty, cache);
 +
 +	kvmclock_reset(vcpu);
 +
 +	kvm_x86_ops->vcpu_free(vcpu);
 +	free_cpumask_var(wbinvd_dirty_mask);
  }
  
 -int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 +struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 +						unsigned int id)
  {
 -	struct page *page;
 -	int r;
 +	struct kvm_vcpu *vcpu;
  
++<<<<<<< HEAD
 +	if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
 +		printk_once(KERN_WARNING
 +		"kvm: SMP vm created on host with unstable TSC; "
 +		"guest TSC will not be reliable\n");
++=======
+ 	if (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu))
+ 		vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+ 	else
+ 		vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
++>>>>>>> c9b8b07cded5 (KVM: x86: Dynamically allocate per-vCPU emulation context)
  
 -	kvm_set_tsc_khz(vcpu, max_tsc_khz);
 +	vcpu = kvm_x86_ops->vcpu_create(kvm, id);
  
++<<<<<<< HEAD
 +	return vcpu;
 +}
++=======
+ 	r = kvm_mmu_create(vcpu);
+ 	if (r < 0)
+ 		return r;
+ 
+ 	if (irqchip_in_kernel(vcpu->kvm)) {
+ 		r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ 		if (r < 0)
+ 			goto fail_mmu_destroy;
+ 		if (kvm_apicv_activated(vcpu->kvm))
+ 			vcpu->arch.apicv_active = true;
+ 	} else
+ 		static_key_slow_inc(&kvm_no_apic_vcpu);
+ 
+ 	r = -ENOMEM;
+ 
+ 	page = alloc_page(GFP_KERNEL | __GFP_ZERO);
+ 	if (!page)
+ 		goto fail_free_lapic;
+ 	vcpu->arch.pio_data = page_address(page);
+ 
+ 	vcpu->arch.mce_banks = kzalloc(KVM_MAX_MCE_BANKS * sizeof(u64) * 4,
+ 				       GFP_KERNEL_ACCOUNT);
+ 	if (!vcpu->arch.mce_banks)
+ 		goto fail_free_pio_data;
+ 	vcpu->arch.mcg_cap = KVM_MAX_MCE_BANKS;
+ 
+ 	if (!zalloc_cpumask_var(&vcpu->arch.wbinvd_dirty_mask,
+ 				GFP_KERNEL_ACCOUNT))
+ 		goto fail_free_mce_banks;
+ 
+ 	if (!alloc_emulate_ctxt(vcpu))
+ 		goto free_wbinvd_dirty_mask;
+ 
+ 	vcpu->arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
+ 						GFP_KERNEL_ACCOUNT);
+ 	if (!vcpu->arch.user_fpu) {
+ 		pr_err("kvm: failed to allocate userspace's fpu\n");
+ 		goto free_emulate_ctxt;
+ 	}
+ 
+ 	vcpu->arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
+ 						 GFP_KERNEL_ACCOUNT);
+ 	if (!vcpu->arch.guest_fpu) {
+ 		pr_err("kvm: failed to allocate vcpu's fpu\n");
+ 		goto free_user_fpu;
+ 	}
+ 	fx_init(vcpu);
+ 
+ 	vcpu->arch.guest_xstate_size = XSAVE_HDR_SIZE + XSAVE_HDR_OFFSET;
+ 
+ 	vcpu->arch.maxphyaddr = cpuid_query_maxphyaddr(vcpu);
+ 
+ 	vcpu->arch.pat = MSR_IA32_CR_PAT_DEFAULT;
+ 
+ 	kvm_async_pf_hash_reset(vcpu);
+ 	kvm_pmu_init(vcpu);
+ 
+ 	vcpu->arch.pending_external_vector = -1;
+ 	vcpu->arch.preempted_in_kernel = false;
+ 
+ 	kvm_hv_vcpu_init(vcpu);
+ 
+ 	r = kvm_x86_ops->vcpu_create(vcpu);
+ 	if (r)
+ 		goto free_guest_fpu;
++>>>>>>> c9b8b07cded5 (KVM: x86: Dynamically allocate per-vCPU emulation context)
  
 +int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 +{
  	vcpu->arch.arch_capabilities = kvm_get_arch_capabilities();
  	vcpu->arch.msr_platform_info = MSR_PLATFORM_INFO_CPUID_FAULT;
  	kvm_vcpu_mtrr_init(vcpu);
@@@ -9174,6 -9346,24 +9287,27 @@@
  	kvm_init_mmu(vcpu, false);
  	vcpu_put(vcpu);
  	return 0;
++<<<<<<< HEAD
++=======
+ 
+ free_guest_fpu:
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
+ free_user_fpu:
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
+ free_emulate_ctxt:
+ 	kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+ free_wbinvd_dirty_mask:
+ 	free_cpumask_var(vcpu->arch.wbinvd_dirty_mask);
+ fail_free_mce_banks:
+ 	kfree(vcpu->arch.mce_banks);
+ fail_free_pio_data:
+ 	free_page((unsigned long)vcpu->arch.pio_data);
+ fail_free_lapic:
+ 	kvm_free_lapic(vcpu);
+ fail_mmu_destroy:
+ 	kvm_mmu_destroy(vcpu);
+ 	return r;
++>>>>>>> c9b8b07cded5 (KVM: x86: Dynamically allocate per-vCPU emulation context)
  }
  
  void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
@@@ -9204,7 -9394,30 +9338,34 @@@
  
  void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	kvm_arch_vcpu_free(vcpu);
++=======
+ 	struct gfn_to_pfn_cache *cache = &vcpu->arch.st.cache;
+ 	int idx;
+ 
+ 	kvm_release_pfn(cache->pfn, cache->dirty, cache);
+ 
+ 	kvmclock_reset(vcpu);
+ 
+ 	kvm_x86_ops->vcpu_free(vcpu);
+ 
+ 	kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+ 	free_cpumask_var(vcpu->arch.wbinvd_dirty_mask);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
+ 
+ 	kvm_hv_vcpu_uninit(vcpu);
+ 	kvm_pmu_destroy(vcpu);
+ 	kfree(vcpu->arch.mce_banks);
+ 	kvm_free_lapic(vcpu);
+ 	idx = srcu_read_lock(&vcpu->kvm->srcu);
+ 	kvm_mmu_destroy(vcpu);
+ 	srcu_read_unlock(&vcpu->kvm->srcu, idx);
+ 	free_page((unsigned long)vcpu->arch.pio_data);
+ 	if (!lapic_in_kernel(vcpu))
+ 		static_key_slow_dec(&kvm_no_apic_vcpu);
++>>>>>>> c9b8b07cded5 (KVM: x86: Dynamically allocate per-vCPU emulation context)
  }
  
  void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
diff --git a/arch/x86/include/asm/kvm_emulate.h b/arch/x86/include/asm/kvm_emulate.h
index dcdb44a5fb11..d7c53868f581 100644
--- a/arch/x86/include/asm/kvm_emulate.h
+++ b/arch/x86/include/asm/kvm_emulate.h
@@ -301,6 +301,7 @@ struct fastop;
 typedef void (*fastop_t)(struct fastop *);
 
 struct x86_emulate_ctxt {
+	void *vcpu;
 	const struct x86_emulate_ops *ops;
 
 	/* Register state before/after emulation. */
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5dfa212d1e49..5b0caf33fb80 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -681,7 +681,7 @@ struct kvm_vcpu_arch {
 
 	/* emulate context */
 
-	struct x86_emulate_ctxt emulate_ctxt;
+	struct x86_emulate_ctxt *emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
 	int (*complete_userspace_io)(struct kvm_vcpu *vcpu);
diff --git a/arch/x86/kvm/trace.h b/arch/x86/kvm/trace.h
index 7c741a0c5f80..aed5a759a51e 100644
--- a/arch/x86/kvm/trace.h
+++ b/arch/x86/kvm/trace.h
@@ -745,13 +745,13 @@ TRACE_EVENT(kvm_emulate_insn,
 
 	TP_fast_assign(
 		__entry->csbase = kvm_x86_ops->get_segment_base(vcpu, VCPU_SREG_CS);
-		__entry->len = vcpu->arch.emulate_ctxt.fetch.ptr
-			       - vcpu->arch.emulate_ctxt.fetch.data;
-		__entry->rip = vcpu->arch.emulate_ctxt._eip - __entry->len;
+		__entry->len = vcpu->arch.emulate_ctxt->fetch.ptr
+			       - vcpu->arch.emulate_ctxt->fetch.data;
+		__entry->rip = vcpu->arch.emulate_ctxt->_eip - __entry->len;
 		memcpy(__entry->insn,
-		       vcpu->arch.emulate_ctxt.fetch.data,
+		       vcpu->arch.emulate_ctxt->fetch.data,
 		       15);
-		__entry->flags = kei_decode_mode(vcpu->arch.emulate_ctxt.mode);
+		__entry->flags = kei_decode_mode(vcpu->arch.emulate_ctxt->mode);
 		__entry->failed = failed;
 		),
 
* Unmerged path arch/x86/kvm/x86.c
