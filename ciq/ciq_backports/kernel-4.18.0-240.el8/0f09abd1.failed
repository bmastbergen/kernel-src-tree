bpf: Enable bpf cgroup hooks to retrieve cgroup v2 and ancestor id

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 0f09abd105da6c37713d2b253730a86cb45e127a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/0f09abd1.failed

Enable the bpf_get_current_cgroup_id() helper for connect(), sendmsg(),
recvmsg() and bind-related hooks in order to retrieve the cgroup v2
context which can then be used as part of the key for BPF map lookups,
for example. Given these hooks operate in process context 'current' is
always valid and pointing to the app that is performing mentioned
syscalls if it's subject to a v2 cgroup. Also with same motivation of
commit 7723628101aa ("bpf: Introduce bpf_skb_ancestor_cgroup_id helper")
enable retrieval of ancestor from current so the cgroup id can be used
for policy lookups which can then forbid connect() / bind(), for example.

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/d2a7ef42530ad299e3cbb245e6c12374b72145ef.1585323121.git.daniel@iogearbox.net
(cherry picked from commit 0f09abd105da6c37713d2b253730a86cb45e127a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	tools/include/uapi/linux/bpf.h
diff --cc include/uapi/linux/bpf.h
index c9871d53e313,222ba11966e3..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -2757,6 -2778,209 +2757,212 @@@ union bpf_attr 
   *		**-EOPNOTSUPP** kernel configuration does not enable SYN cookies
   *
   *		**-EPROTONOSUPPORT** IP packet version is not 4 or 6
++<<<<<<< HEAD
++=======
+  *
+  * int bpf_skb_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  * 	Description
+  * 		Write raw *data* blob into a special BPF perf event held by
+  * 		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  * 		event must have the following attributes: **PERF_SAMPLE_RAW**
+  * 		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  * 		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  * 		The *flags* are used to indicate the index in *map* for which
+  * 		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  * 		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  * 		to indicate that the index of the current CPU core should be
+  * 		used.
+  *
+  * 		The value to write, of *size*, is passed through eBPF stack and
+  * 		pointed by *data*.
+  *
+  * 		*ctx* is a pointer to in-kernel struct sk_buff.
+  *
+  * 		This helper is similar to **bpf_perf_event_output**\ () but
+  * 		restricted to raw_tracepoint bpf programs.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from user space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_kernel(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from kernel space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe user address
+  * 		*unsafe_ptr* to *dst*. The *size* should include the
+  * 		terminating NUL byte. In case the string length is smaller than
+  * 		*size*, the target is not padded with further NUL bytes. If the
+  * 		string length is larger than *size*, just *size*-1 bytes are
+  * 		copied and the last byte is set to NUL.
+  *
+  * 		On success, the length of the copied string is returned. This
+  * 		makes this helper useful in tracing programs for reading
+  * 		strings, and more importantly to get its length at runtime. See
+  * 		the following snippet:
+  *
+  * 		::
+  *
+  * 			SEC("kprobe/sys_open")
+  * 			void bpf_sys_open(struct pt_regs *ctx)
+  * 			{
+  * 			        char buf[PATHLEN]; // PATHLEN is defined to 256
+  * 			        int res = bpf_probe_read_user_str(buf, sizeof(buf),
+  * 				                                  ctx->di);
+  *
+  * 				// Consume buf, for example push it to
+  * 				// userspace via bpf_perf_event_output(); we
+  * 				// can use res (the string length) as event
+  * 				// size, after checking its boundaries.
+  * 			}
+  *
+  * 		In comparison, using **bpf_probe_read_user()** helper here
+  * 		instead to read the string would require to estimate the length
+  * 		at compile time, and would often result in copying more memory
+  * 		than necessary.
+  *
+  * 		Another useful use case is when parsing individual process
+  * 		arguments or individual environment variables navigating
+  * 		*current*\ **->mm->arg_start** and *current*\
+  * 		**->mm->env_start**: using this helper and the return value,
+  * 		one can quickly iterate at the right offset of the memory area.
+  * 	Return
+  * 		On success, the strictly positive length of the string,
+  * 		including the trailing NUL character. On error, a negative
+  * 		value.
+  *
+  * int bpf_probe_read_kernel_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe kernel address *unsafe_ptr*
+  * 		to *dst*. Same semantics as with bpf_probe_read_user_str() apply.
+  * 	Return
+  * 		On success, the strictly positive length of the string,	including
+  * 		the trailing NUL character. On error, a negative value.
+  *
+  * int bpf_tcp_send_ack(void *tp, u32 rcv_nxt)
+  *	Description
+  *		Send out a tcp-ack. *tp* is the in-kernel struct tcp_sock.
+  *		*rcv_nxt* is the ack_seq to be sent out.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_send_signal_thread(u32 sig)
+  *	Description
+  *		Send signal *sig* to the thread corresponding to the current task.
+  *	Return
+  *		0 on success or successfully queued.
+  *
+  *		**-EBUSY** if work queue under nmi is full.
+  *
+  *		**-EINVAL** if *sig* is invalid.
+  *
+  *		**-EPERM** if no permission to send the *sig*.
+  *
+  *		**-EAGAIN** if bpf program can try again.
+  *
+  * u64 bpf_jiffies64(void)
+  *	Description
+  *		Obtain the 64bit jiffies
+  *	Return
+  *		The 64 bit jiffies
+  *
+  * int bpf_read_branch_records(struct bpf_perf_event_data *ctx, void *buf, u32 size, u64 flags)
+  *	Description
+  *		For an eBPF program attached to a perf event, retrieve the
+  *		branch records (struct perf_branch_entry) associated to *ctx*
+  *		and store it in	the buffer pointed by *buf* up to size
+  *		*size* bytes.
+  *	Return
+  *		On success, number of bytes written to *buf*. On error, a
+  *		negative value.
+  *
+  *		The *flags* can be set to **BPF_F_GET_BRANCH_RECORDS_SIZE** to
+  *		instead	return the number of bytes required to store all the
+  *		branch entries. If this flag is set, *buf* may be NULL.
+  *
+  *		**-EINVAL** if arguments invalid or **size** not a multiple
+  *		of sizeof(struct perf_branch_entry).
+  *
+  *		**-ENOENT** if architecture does not support branch records.
+  *
+  * int bpf_get_ns_current_pid_tgid(u64 dev, u64 ino, struct bpf_pidns_info *nsdata, u32 size)
+  *	Description
+  *		Returns 0 on success, values for *pid* and *tgid* as seen from the current
+  *		*namespace* will be returned in *nsdata*.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** if dev and inum supplied don't match dev_t and inode number
+  *              with nsfs of current task, or if dev conversion to dev_t lost high bits.
+  *
+  *		**-ENOENT** if pidns does not exists for the current task.
+  *
+  * int bpf_xdp_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  *	Description
+  *		Write raw *data* blob into a special BPF perf event held by
+  *		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  *		event must have the following attributes: **PERF_SAMPLE_RAW**
+  *		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  *		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  *		The *flags* are used to indicate the index in *map* for which
+  *		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  *		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  *		to indicate that the index of the current CPU core should be
+  *		used.
+  *
+  *		The value to write, of *size*, is passed through eBPF stack and
+  *		pointed by *data*.
+  *
+  *		*ctx* is a pointer to in-kernel struct xdp_buff.
+  *
+  *		This helper is similar to **bpf_perf_eventoutput**\ () but
+  *		restricted to raw_tracepoint bpf programs.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * u64 bpf_get_netns_cookie(void *ctx)
+  * 	Description
+  * 		Retrieve the cookie (generated by the kernel) of the network
+  * 		namespace the input *ctx* is associated with. The network
+  * 		namespace cookie remains stable for its lifetime and provides
+  * 		a global identifier that can be assumed unique. If *ctx* is
+  * 		NULL, then the helper returns the cookie for the initial
+  * 		network namespace. The cookie itself is very similar to that
+  * 		of bpf_get_socket_cookie() helper, but for network namespaces
+  * 		instead of sockets.
+  * 	Return
+  * 		A 8-byte long opaque number.
+  *
+  * u64 bpf_get_current_ancestor_cgroup_id(int ancestor_level)
+  * 	Description
+  * 		Return id of cgroup v2 that is ancestor of the cgroup associated
+  * 		with the current task at the *ancestor_level*. The root cgroup
+  * 		is at *ancestor_level* zero and each step down the hierarchy
+  * 		increments the level. If *ancestor_level* == level of cgroup
+  * 		associated with the current task, then return value will be the
+  * 		same as that of **bpf_get_current_cgroup_id**\ ().
+  *
+  * 		The helper is useful to implement policies based on cgroups
+  * 		that are upper in hierarchy than immediate cgroup associated
+  * 		with the current task.
+  *
+  * 		The format of returned id and helper limitations are same as in
+  * 		**bpf_get_current_cgroup_id**\ ().
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
++>>>>>>> 0f09abd105da (bpf: Enable bpf cgroup hooks to retrieve cgroup v2 and ancestor id)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -2869,7 -3093,20 +3075,24 @@@
  	FN(sk_storage_get),		\
  	FN(sk_storage_delete),		\
  	FN(send_signal),		\
++<<<<<<< HEAD
 +	FN(tcp_gen_syncookie),
++=======
+ 	FN(tcp_gen_syncookie),		\
+ 	FN(skb_output),			\
+ 	FN(probe_read_user),		\
+ 	FN(probe_read_kernel),		\
+ 	FN(probe_read_user_str),	\
+ 	FN(probe_read_kernel_str),	\
+ 	FN(tcp_send_ack),		\
+ 	FN(send_signal_thread),		\
+ 	FN(jiffies64),			\
+ 	FN(read_branch_records),	\
+ 	FN(get_ns_current_pid_tgid),	\
+ 	FN(xdp_output),			\
+ 	FN(get_netns_cookie),		\
+ 	FN(get_current_ancestor_cgroup_id),
++>>>>>>> 0f09abd105da (bpf: Enable bpf cgroup hooks to retrieve cgroup v2 and ancestor id)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --cc tools/include/uapi/linux/bpf.h
index 2bc35095902a,222ba11966e3..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -2718,6 -2751,236 +2718,239 @@@ union bpf_attr 
   *		**-EPERM** if no permission to send the *sig*.
   *
   *		**-EAGAIN** if bpf program can try again.
++<<<<<<< HEAD
++=======
+  *
+  * s64 bpf_tcp_gen_syncookie(struct bpf_sock *sk, void *iph, u32 iph_len, struct tcphdr *th, u32 th_len)
+  *	Description
+  *		Try to issue a SYN cookie for the packet with corresponding
+  *		IP/TCP headers, *iph* and *th*, on the listening socket in *sk*.
+  *
+  *		*iph* points to the start of the IPv4 or IPv6 header, while
+  *		*iph_len* contains **sizeof**\ (**struct iphdr**) or
+  *		**sizeof**\ (**struct ip6hdr**).
+  *
+  *		*th* points to the start of the TCP header, while *th_len*
+  *		contains the length of the TCP header.
+  *
+  *	Return
+  *		On success, lower 32 bits hold the generated SYN cookie in
+  *		followed by 16 bits which hold the MSS value for that cookie,
+  *		and the top 16 bits are unused.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** SYN cookie cannot be issued due to error
+  *
+  *		**-ENOENT** SYN cookie should not be issued (no SYN flood)
+  *
+  *		**-EOPNOTSUPP** kernel configuration does not enable SYN cookies
+  *
+  *		**-EPROTONOSUPPORT** IP packet version is not 4 or 6
+  *
+  * int bpf_skb_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  * 	Description
+  * 		Write raw *data* blob into a special BPF perf event held by
+  * 		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  * 		event must have the following attributes: **PERF_SAMPLE_RAW**
+  * 		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  * 		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  * 		The *flags* are used to indicate the index in *map* for which
+  * 		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  * 		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  * 		to indicate that the index of the current CPU core should be
+  * 		used.
+  *
+  * 		The value to write, of *size*, is passed through eBPF stack and
+  * 		pointed by *data*.
+  *
+  * 		*ctx* is a pointer to in-kernel struct sk_buff.
+  *
+  * 		This helper is similar to **bpf_perf_event_output**\ () but
+  * 		restricted to raw_tracepoint bpf programs.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from user space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_kernel(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from kernel space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe user address
+  * 		*unsafe_ptr* to *dst*. The *size* should include the
+  * 		terminating NUL byte. In case the string length is smaller than
+  * 		*size*, the target is not padded with further NUL bytes. If the
+  * 		string length is larger than *size*, just *size*-1 bytes are
+  * 		copied and the last byte is set to NUL.
+  *
+  * 		On success, the length of the copied string is returned. This
+  * 		makes this helper useful in tracing programs for reading
+  * 		strings, and more importantly to get its length at runtime. See
+  * 		the following snippet:
+  *
+  * 		::
+  *
+  * 			SEC("kprobe/sys_open")
+  * 			void bpf_sys_open(struct pt_regs *ctx)
+  * 			{
+  * 			        char buf[PATHLEN]; // PATHLEN is defined to 256
+  * 			        int res = bpf_probe_read_user_str(buf, sizeof(buf),
+  * 				                                  ctx->di);
+  *
+  * 				// Consume buf, for example push it to
+  * 				// userspace via bpf_perf_event_output(); we
+  * 				// can use res (the string length) as event
+  * 				// size, after checking its boundaries.
+  * 			}
+  *
+  * 		In comparison, using **bpf_probe_read_user()** helper here
+  * 		instead to read the string would require to estimate the length
+  * 		at compile time, and would often result in copying more memory
+  * 		than necessary.
+  *
+  * 		Another useful use case is when parsing individual process
+  * 		arguments or individual environment variables navigating
+  * 		*current*\ **->mm->arg_start** and *current*\
+  * 		**->mm->env_start**: using this helper and the return value,
+  * 		one can quickly iterate at the right offset of the memory area.
+  * 	Return
+  * 		On success, the strictly positive length of the string,
+  * 		including the trailing NUL character. On error, a negative
+  * 		value.
+  *
+  * int bpf_probe_read_kernel_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe kernel address *unsafe_ptr*
+  * 		to *dst*. Same semantics as with bpf_probe_read_user_str() apply.
+  * 	Return
+  * 		On success, the strictly positive length of the string,	including
+  * 		the trailing NUL character. On error, a negative value.
+  *
+  * int bpf_tcp_send_ack(void *tp, u32 rcv_nxt)
+  *	Description
+  *		Send out a tcp-ack. *tp* is the in-kernel struct tcp_sock.
+  *		*rcv_nxt* is the ack_seq to be sent out.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_send_signal_thread(u32 sig)
+  *	Description
+  *		Send signal *sig* to the thread corresponding to the current task.
+  *	Return
+  *		0 on success or successfully queued.
+  *
+  *		**-EBUSY** if work queue under nmi is full.
+  *
+  *		**-EINVAL** if *sig* is invalid.
+  *
+  *		**-EPERM** if no permission to send the *sig*.
+  *
+  *		**-EAGAIN** if bpf program can try again.
+  *
+  * u64 bpf_jiffies64(void)
+  *	Description
+  *		Obtain the 64bit jiffies
+  *	Return
+  *		The 64 bit jiffies
+  *
+  * int bpf_read_branch_records(struct bpf_perf_event_data *ctx, void *buf, u32 size, u64 flags)
+  *	Description
+  *		For an eBPF program attached to a perf event, retrieve the
+  *		branch records (struct perf_branch_entry) associated to *ctx*
+  *		and store it in	the buffer pointed by *buf* up to size
+  *		*size* bytes.
+  *	Return
+  *		On success, number of bytes written to *buf*. On error, a
+  *		negative value.
+  *
+  *		The *flags* can be set to **BPF_F_GET_BRANCH_RECORDS_SIZE** to
+  *		instead	return the number of bytes required to store all the
+  *		branch entries. If this flag is set, *buf* may be NULL.
+  *
+  *		**-EINVAL** if arguments invalid or **size** not a multiple
+  *		of sizeof(struct perf_branch_entry).
+  *
+  *		**-ENOENT** if architecture does not support branch records.
+  *
+  * int bpf_get_ns_current_pid_tgid(u64 dev, u64 ino, struct bpf_pidns_info *nsdata, u32 size)
+  *	Description
+  *		Returns 0 on success, values for *pid* and *tgid* as seen from the current
+  *		*namespace* will be returned in *nsdata*.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** if dev and inum supplied don't match dev_t and inode number
+  *              with nsfs of current task, or if dev conversion to dev_t lost high bits.
+  *
+  *		**-ENOENT** if pidns does not exists for the current task.
+  *
+  * int bpf_xdp_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  *	Description
+  *		Write raw *data* blob into a special BPF perf event held by
+  *		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  *		event must have the following attributes: **PERF_SAMPLE_RAW**
+  *		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  *		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  *		The *flags* are used to indicate the index in *map* for which
+  *		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  *		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  *		to indicate that the index of the current CPU core should be
+  *		used.
+  *
+  *		The value to write, of *size*, is passed through eBPF stack and
+  *		pointed by *data*.
+  *
+  *		*ctx* is a pointer to in-kernel struct xdp_buff.
+  *
+  *		This helper is similar to **bpf_perf_eventoutput**\ () but
+  *		restricted to raw_tracepoint bpf programs.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * u64 bpf_get_netns_cookie(void *ctx)
+  * 	Description
+  * 		Retrieve the cookie (generated by the kernel) of the network
+  * 		namespace the input *ctx* is associated with. The network
+  * 		namespace cookie remains stable for its lifetime and provides
+  * 		a global identifier that can be assumed unique. If *ctx* is
+  * 		NULL, then the helper returns the cookie for the initial
+  * 		network namespace. The cookie itself is very similar to that
+  * 		of bpf_get_socket_cookie() helper, but for network namespaces
+  * 		instead of sockets.
+  * 	Return
+  * 		A 8-byte long opaque number.
+  *
+  * u64 bpf_get_current_ancestor_cgroup_id(int ancestor_level)
+  * 	Description
+  * 		Return id of cgroup v2 that is ancestor of the cgroup associated
+  * 		with the current task at the *ancestor_level*. The root cgroup
+  * 		is at *ancestor_level* zero and each step down the hierarchy
+  * 		increments the level. If *ancestor_level* == level of cgroup
+  * 		associated with the current task, then return value will be the
+  * 		same as that of **bpf_get_current_cgroup_id**\ ().
+  *
+  * 		The helper is useful to implement policies based on cgroups
+  * 		that are upper in hierarchy than immediate cgroup associated
+  * 		with the current task.
+  *
+  * 		The format of returned id and helper limitations are same as in
+  * 		**bpf_get_current_cgroup_id**\ ().
+  * 	Return
+  * 		The id is returned or 0 in case the id could not be retrieved.
++>>>>>>> 0f09abd105da (bpf: Enable bpf cgroup hooks to retrieve cgroup v2 and ancestor id)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -2829,7 -3092,21 +3062,25 @@@
  	FN(strtoul),			\
  	FN(sk_storage_get),		\
  	FN(sk_storage_delete),		\
++<<<<<<< HEAD
 +	FN(send_signal),
++=======
+ 	FN(send_signal),		\
+ 	FN(tcp_gen_syncookie),		\
+ 	FN(skb_output),			\
+ 	FN(probe_read_user),		\
+ 	FN(probe_read_kernel),		\
+ 	FN(probe_read_user_str),	\
+ 	FN(probe_read_kernel_str),	\
+ 	FN(tcp_send_ack),		\
+ 	FN(send_signal_thread),		\
+ 	FN(jiffies64),			\
+ 	FN(read_branch_records),	\
+ 	FN(get_ns_current_pid_tgid),	\
+ 	FN(xdp_output),			\
+ 	FN(get_netns_cookie),		\
+ 	FN(get_current_ancestor_cgroup_id),
++>>>>>>> 0f09abd105da (bpf: Enable bpf cgroup hooks to retrieve cgroup v2 and ancestor id)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --git a/include/linux/bpf.h b/include/linux/bpf.h
index c1c99fdb999a..10aec9a1ef10 100644
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -1088,6 +1088,7 @@ extern const struct bpf_func_proto bpf_get_stack_proto;
 extern const struct bpf_func_proto bpf_sock_map_update_proto;
 extern const struct bpf_func_proto bpf_sock_hash_update_proto;
 extern const struct bpf_func_proto bpf_get_current_cgroup_id_proto;
+extern const struct bpf_func_proto bpf_get_current_ancestor_cgroup_id_proto;
 extern const struct bpf_func_proto bpf_msg_redirect_hash_proto;
 extern const struct bpf_func_proto bpf_msg_redirect_map_proto;
 extern const struct bpf_func_proto bpf_sk_redirect_hash_proto;
* Unmerged path include/uapi/linux/bpf.h
diff --git a/kernel/bpf/core.c b/kernel/bpf/core.c
index 5381636b23b4..b88c5e1edcde 100644
--- a/kernel/bpf/core.c
+++ b/kernel/bpf/core.c
@@ -2083,6 +2083,7 @@ const struct bpf_func_proto bpf_get_current_pid_tgid_proto __weak;
 const struct bpf_func_proto bpf_get_current_uid_gid_proto __weak;
 const struct bpf_func_proto bpf_get_current_comm_proto __weak;
 const struct bpf_func_proto bpf_get_current_cgroup_id_proto __weak;
+const struct bpf_func_proto bpf_get_current_ancestor_cgroup_id_proto __weak;
 const struct bpf_func_proto bpf_get_local_storage_proto __weak;
 
 const struct bpf_func_proto * __weak bpf_get_trace_printk_proto(void)
diff --git a/kernel/bpf/helpers.c b/kernel/bpf/helpers.c
index 4266ffde07ca..eaf51312eecf 100644
--- a/kernel/bpf/helpers.c
+++ b/kernel/bpf/helpers.c
@@ -334,6 +334,24 @@ const struct bpf_func_proto bpf_get_current_cgroup_id_proto = {
 	.ret_type	= RET_INTEGER,
 };
 
+BPF_CALL_1(bpf_get_current_ancestor_cgroup_id, int, ancestor_level)
+{
+	struct cgroup *cgrp = task_dfl_cgroup(current);
+	struct cgroup *ancestor;
+
+	ancestor = cgroup_ancestor(cgrp, ancestor_level);
+	if (!ancestor)
+		return 0;
+	return cgroup_id(ancestor);
+}
+
+const struct bpf_func_proto bpf_get_current_ancestor_cgroup_id_proto = {
+	.func		= bpf_get_current_ancestor_cgroup_id,
+	.gpl_only	= false,
+	.ret_type	= RET_INTEGER,
+	.arg1_type	= ARG_ANYTHING,
+};
+
 #ifdef CONFIG_CGROUP_BPF
 DECLARE_PER_CPU(struct bpf_cgroup_storage*,
 		bpf_cgroup_storage[MAX_BPF_CGROUP_STORAGE_TYPE]);
diff --git a/net/core/filter.c b/net/core/filter.c
index f77b9440c7a9..5b19bfb7b49b 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -6042,6 +6042,12 @@ sock_filter_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
 		return &bpf_get_socket_cookie_sock_proto;
 	case BPF_FUNC_perf_event_output:
 		return &bpf_event_output_data_proto;
+#ifdef CONFIG_CGROUPS
+	case BPF_FUNC_get_current_cgroup_id:
+		return &bpf_get_current_cgroup_id_proto;
+	case BPF_FUNC_get_current_ancestor_cgroup_id:
+		return &bpf_get_current_ancestor_cgroup_id_proto;
+#endif
 #ifdef CONFIG_CGROUP_NET_CLASSID
 	case BPF_FUNC_get_cgroup_classid:
 		return &bpf_get_cgroup_classid_curr_proto;
@@ -6074,6 +6080,12 @@ sock_addr_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
 		return &bpf_get_local_storage_proto;
 	case BPF_FUNC_perf_event_output:
 		return &bpf_event_output_data_proto;
+#ifdef CONFIG_CGROUPS
+	case BPF_FUNC_get_current_cgroup_id:
+		return &bpf_get_current_cgroup_id_proto;
+	case BPF_FUNC_get_current_ancestor_cgroup_id:
+		return &bpf_get_current_ancestor_cgroup_id_proto;
+#endif
 #ifdef CONFIG_CGROUP_NET_CLASSID
 	case BPF_FUNC_get_cgroup_classid:
 		return &bpf_get_cgroup_classid_curr_proto;
* Unmerged path tools/include/uapi/linux/bpf.h
