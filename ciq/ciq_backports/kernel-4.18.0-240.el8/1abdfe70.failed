lib: Restrict cpumask_local_spread to houskeeping CPUs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Alex Belits <abelits@marvell.com>
commit 1abdfe706a579a702799fce465bceb9fb01d407c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/1abdfe70.failed

The current implementation of cpumask_local_spread() does not respect the
isolated CPUs, i.e., even if a CPU has been isolated for Real-Time task,
it will return it to the caller for pinning of its IRQ threads. Having
these unwanted IRQ threads on an isolated CPU adds up to a latency
overhead.

Restrict the CPUs that are returned for spreading IRQs only to the
available housekeeping CPUs.

	Signed-off-by: Alex Belits <abelits@marvell.com>
	Signed-off-by: Nitesh Narayan Lal <nitesh@redhat.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20200625223443.2684-2-nitesh@redhat.com
(cherry picked from commit 1abdfe706a579a702799fce465bceb9fb01d407c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/cpumask.c
diff --cc lib/cpumask.c
index beca6244671a,85da6ab4fbb5..000000000000
--- a/lib/cpumask.c
+++ b/lib/cpumask.c
@@@ -4,7 -4,9 +4,13 @@@
  #include <linux/bitops.h>
  #include <linux/cpumask.h>
  #include <linux/export.h>
++<<<<<<< HEAD
 +#include <linux/bootmem.h>
++=======
+ #include <linux/memblock.h>
+ #include <linux/numa.h>
+ #include <linux/sched/isolation.h>
++>>>>>>> 1abdfe706a57 (lib: Restrict cpumask_local_spread to houskeeping CPUs)
  
  /**
   * cpumask_next - get the next cpu in a cpumask
@@@ -201,22 -206,27 +207,32 @@@ void __init free_bootmem_cpumask_var(cp
   */
  unsigned int cpumask_local_spread(unsigned int i, int node)
  {
- 	int cpu;
+ 	int cpu, hk_flags;
+ 	const struct cpumask *mask;
  
+ 	hk_flags = HK_FLAG_DOMAIN | HK_FLAG_MANAGED_IRQ;
+ 	mask = housekeeping_cpumask(hk_flags);
  	/* Wrap: we always want a cpu. */
- 	i %= num_online_cpus();
+ 	i %= cpumask_weight(mask);
  
++<<<<<<< HEAD
 +	if (node == -1) {
 +		for_each_cpu(cpu, cpu_online_mask)
++=======
+ 	if (node == NUMA_NO_NODE) {
+ 		for_each_cpu(cpu, mask) {
++>>>>>>> 1abdfe706a57 (lib: Restrict cpumask_local_spread to houskeeping CPUs)
  			if (i-- == 0)
  				return cpu;
+ 		}
  	} else {
  		/* NUMA first. */
- 		for_each_cpu_and(cpu, cpumask_of_node(node), cpu_online_mask)
+ 		for_each_cpu_and(cpu, cpumask_of_node(node), mask) {
  			if (i-- == 0)
  				return cpu;
+ 		}
  
- 		for_each_cpu(cpu, cpu_online_mask) {
+ 		for_each_cpu(cpu, mask) {
  			/* Skip NUMA nodes, done above. */
  			if (cpumask_test_cpu(cpu, cpumask_of_node(node)))
  				continue;
* Unmerged path lib/cpumask.c
