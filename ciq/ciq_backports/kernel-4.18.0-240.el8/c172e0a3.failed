sched/membarrier: Return -ENOMEM to userspace on memory allocation failure

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
commit c172e0a3e8e65a4c6fffec5bc4d6de08d6f894f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/c172e0a3.failed

Remove the IPI fallback code from membarrier to deal with very
infrequent cpumask memory allocation failure. Use GFP_KERNEL rather
than GFP_NOWAIT, and relax the blocking guarantees for the expedited
membarrier system call commands, allowing it to block if waiting for
memory to be made available.

In addition, now -ENOMEM can be returned to user-space if the cpumask
memory allocation fails.

	Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Chris Metcalf <cmetcalf@ezchip.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Eric W. Biederman <ebiederm@xmission.com>
	Cc: Kirill Tkhai <tkhai@yandex.ru>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Paul E. McKenney <paulmck@linux.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Russell King - ARM Linux admin <linux@armlinux.org.uk>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20190919173705.2181-8-mathieu.desnoyers@efficios.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit c172e0a3e8e65a4c6fffec5bc4d6de08d6f894f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/membarrier.c
diff --cc kernel/sched/membarrier.c
index a50cf26f944f,a39bed2c784f..000000000000
--- a/kernel/sched/membarrier.c
+++ b/kernel/sched/membarrier.c
@@@ -54,17 -77,11 +53,10 @@@ static int membarrier_global_expedited(
  	 */
  	smp_mb();	/* system call entry is not a mb. */
  
- 	/*
- 	 * Expedited membarrier commands guarantee that they won't
- 	 * block, hence the GFP_NOWAIT allocation flag and fallback
- 	 * implementation.
- 	 */
- 	if (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {
- 		/* Fallback for OOM. */
- 		fallback = true;
- 	}
+ 	if (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
+ 		return -ENOMEM;
  
  	cpus_read_lock();
 -	rcu_read_lock();
  	for_each_online_cpu(cpu) {
  		struct task_struct *p;
  
@@@ -79,23 -96,28 +71,38 @@@
  		if (cpu == raw_smp_processor_id())
  			continue;
  
 -		if (!(READ_ONCE(cpu_rq(cpu)->membarrier_state) &
 -		    MEMBARRIER_STATE_GLOBAL_EXPEDITED))
 -			continue;
 -
 -		/*
 -		 * Skip the CPU if it runs a kernel thread. The scheduler
 -		 * leaves the prior task mm in place as an optimization when
 -		 * scheduling a kthread.
 -		 */
 +		rcu_read_lock();
  		p = rcu_dereference(cpu_rq(cpu)->curr);
++<<<<<<< HEAD
 +		if (p && p->mm && (atomic_read(&p->mm->membarrier_state) &
 +				   MEMBARRIER_STATE_GLOBAL_EXPEDITED)) {
 +			if (!fallback)
 +				__cpumask_set_cpu(cpu, tmpmask);
 +			else
 +				smp_call_function_single(cpu, ipi_mb, NULL, 1);
 +		}
 +		rcu_read_unlock();
 +	}
 +	if (!fallback) {
 +		preempt_disable();
 +		smp_call_function_many(tmpmask, ipi_mb, NULL, 1);
 +		preempt_enable();
 +		free_cpumask_var(tmpmask);
 +	}
++=======
+ 		if (p->flags & PF_KTHREAD)
+ 			continue;
+ 
+ 		__cpumask_set_cpu(cpu, tmpmask);
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	preempt_disable();
+ 	smp_call_function_many(tmpmask, ipi_mb, NULL, 1);
+ 	preempt_enable();
+ 
+ 	free_cpumask_var(tmpmask);
++>>>>>>> c172e0a3e8e6 (sched/membarrier: Return -ENOMEM to userspace on memory allocation failure)
  	cpus_read_unlock();
  
  	/*
@@@ -135,17 -156,11 +141,10 @@@ static int membarrier_private_expedited
  	 */
  	smp_mb();	/* system call entry is not a mb. */
  
- 	/*
- 	 * Expedited membarrier commands guarantee that they won't
- 	 * block, hence the GFP_NOWAIT allocation flag and fallback
- 	 * implementation.
- 	 */
- 	if (!zalloc_cpumask_var(&tmpmask, GFP_NOWAIT)) {
- 		/* Fallback for OOM. */
- 		fallback = true;
- 	}
+ 	if (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
+ 		return -ENOMEM;
  
  	cpus_read_lock();
 -	rcu_read_lock();
  	for_each_online_cpu(cpu) {
  		struct task_struct *p;
  
@@@ -161,20 -176,16 +160,33 @@@
  			continue;
  		rcu_read_lock();
  		p = rcu_dereference(cpu_rq(cpu)->curr);
++<<<<<<< HEAD
 +		if (p && p->mm == mm) {
 +			if (!fallback)
 +				__cpumask_set_cpu(cpu, tmpmask);
 +			else
 +				smp_call_function_single(cpu, ipi_mb, NULL, 1);
 +		}
 +		rcu_read_unlock();
 +	}
 +	if (!fallback) {
 +		preempt_disable();
 +		smp_call_function_many(tmpmask, ipi_mb, NULL, 1);
 +		preempt_enable();
 +		free_cpumask_var(tmpmask);
 +	}
++=======
+ 		if (p && p->mm == mm)
+ 			__cpumask_set_cpu(cpu, tmpmask);
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	preempt_disable();
+ 	smp_call_function_many(tmpmask, ipi_mb, NULL, 1);
+ 	preempt_enable();
+ 
+ 	free_cpumask_var(tmpmask);
++>>>>>>> c172e0a3e8e6 (sched/membarrier: Return -ENOMEM to userspace on memory allocation failure)
  	cpus_read_unlock();
  
  	/*
@@@ -187,6 -198,65 +199,68 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int sync_runqueues_membarrier_state(struct mm_struct *mm)
+ {
+ 	int membarrier_state = atomic_read(&mm->membarrier_state);
+ 	cpumask_var_t tmpmask;
+ 	int cpu;
+ 
+ 	if (atomic_read(&mm->mm_users) == 1 || num_online_cpus() == 1) {
+ 		this_cpu_write(runqueues.membarrier_state, membarrier_state);
+ 
+ 		/*
+ 		 * For single mm user, we can simply issue a memory barrier
+ 		 * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the
+ 		 * mm and in the current runqueue to guarantee that no memory
+ 		 * access following registration is reordered before
+ 		 * registration.
+ 		 */
+ 		smp_mb();
+ 		return 0;
+ 	}
+ 
+ 	if (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
+ 		return -ENOMEM;
+ 
+ 	/*
+ 	 * For mm with multiple users, we need to ensure all future
+ 	 * scheduler executions will observe @mm's new membarrier
+ 	 * state.
+ 	 */
+ 	synchronize_rcu();
+ 
+ 	/*
+ 	 * For each cpu runqueue, if the task's mm match @mm, ensure that all
+ 	 * @mm's membarrier state set bits are also set in in the runqueue's
+ 	 * membarrier state. This ensures that a runqueue scheduling
+ 	 * between threads which are users of @mm has its membarrier state
+ 	 * updated.
+ 	 */
+ 	cpus_read_lock();
+ 	rcu_read_lock();
+ 	for_each_online_cpu(cpu) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 		struct task_struct *p;
+ 
+ 		p = rcu_dereference(rq->curr);
+ 		if (p && p->mm == mm)
+ 			__cpumask_set_cpu(cpu, tmpmask);
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	preempt_disable();
+ 	smp_call_function_many(tmpmask, ipi_sync_rq_state, mm, 1);
+ 	preempt_enable();
+ 
+ 	free_cpumask_var(tmpmask);
+ 	cpus_read_unlock();
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> c172e0a3e8e6 (sched/membarrier: Return -ENOMEM to userspace on memory allocation failure)
  static int membarrier_register_global_expedited(void)
  {
  	struct task_struct *p = current;
* Unmerged path kernel/sched/membarrier.c
