KVM: nSVM: split kvm_init_shadow_npt_mmu() from kvm_init_shadow_mmu()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 0f04a2ac4fe96bbf05b7ac7d3e94598db550d6b8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/0f04a2ac.failed

As a preparatory change for moving kvm_mmu_new_pgd() from
nested_prepare_vmcb_save() to nested_svm_init_mmu_context() split
kvm_init_shadow_npt_mmu() from kvm_init_shadow_mmu(). This also makes
the code look more like nVMX (kvm_init_shadow_ept_mmu()).

No functional change intended.

	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
Message-Id: <20200710141157.1640173-2-vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 0f04a2ac4fe96bbf05b7ac7d3e94598db550d6b8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.h
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/svm/nested.c
diff --cc arch/x86/kvm/mmu.h
index 3165671cd7d6,75125ae57e50..000000000000
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@@ -57,7 -57,8 +57,12 @@@ voi
  reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu, struct kvm_mmu *context);
  
  void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots);
++<<<<<<< HEAD
 +void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu);
++=======
+ void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, u32 cr0, u32 cr4, u32 efer,
+ 			     gpa_t nested_cr3);
++>>>>>>> 0f04a2ac4fe9 (KVM: nSVM: split kvm_init_shadow_npt_mmu() from kvm_init_shadow_mmu())
  void kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,
  			     bool accessed_dirty, gpa_t new_eptp);
  bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu);
diff --cc arch/x86/kvm/mmu/mmu.c
index 78a7db187b32,f6e032c000ac..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -5034,20 -4918,16 +5034,20 @@@ kvm_calc_shadow_mmu_root_page_role(stru
  	return role;
  }
  
++<<<<<<< HEAD
 +void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
++=======
+ static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, u32 cr0, u32 cr4,
+ 				    u32 efer, union kvm_mmu_role new_role)
++>>>>>>> 0f04a2ac4fe9 (KVM: nSVM: split kvm_init_shadow_npt_mmu() from kvm_init_shadow_mmu())
  {
  	struct kvm_mmu *context = vcpu->arch.mmu;
- 	union kvm_mmu_role new_role =
- 		kvm_calc_shadow_mmu_root_page_role(vcpu, false);
- 
- 	if (new_role.as_u64 == context->mmu_role.as_u64)
- 		return;
  
 -	if (!(cr0 & X86_CR0_PG))
 +	if (!is_paging(vcpu))
  		nonpaging_init_context(vcpu, context);
 -	else if (efer & EFER_LMA)
 +	else if (is_long_mode(vcpu))
  		paging64_init_context(vcpu, context);
 -	else if (cr4 & X86_CR4_PAE)
 +	else if (is_pae(vcpu))
  		paging32E_init_context(vcpu, context);
  	else
  		paging32_init_context(vcpu, context);
* Unmerged path arch/x86/kvm/svm/nested.c
* Unmerged path arch/x86/kvm/mmu.h
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/svm/nested.c
