libperf: Centralize map refcnt setting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jiri Olsa <jolsa@kernel.org>
commit 285aaeac8c5d537b56b70169e21ac29ae5caa8e1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/285aaeac.failed

Currently when a new map is mmapped we set its refcnt to 2 in the
perf_evlist_mmap_ops::mmap callback.

Every mmap gets its refcnt set to 2 when it's first mmaped:

  - 1 for the current user, which will be taken out by a call to
    perf_evlist__munmap_filtered(), where we find out there's
    no more data comming from kernel to this mmap.

  - 1 for the drain code where in perf_mmap__consume() the mmap
    is released if it is empty.

Move this common setup into libperf's generic code before the mmap
callback is called.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Michael Petlan <mpetlan@redhat.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
Link: http://lore.kernel.org/lkml/20191007125344.14268-23-jolsa@kernel.org
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit 285aaeac8c5d537b56b70169e21ac29ae5caa8e1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/lib/evlist.c
#	tools/perf/util/mmap.c
diff --cc tools/perf/util/mmap.c
index 850493205040,063d1b93c53d..000000000000
--- a/tools/perf/util/mmap.c
+++ b/tools/perf/util/mmap.c
@@@ -344,39 -236,18 +344,43 @@@ static void perf_mmap__setup_affinity_m
  {
  	CPU_ZERO(&map->affinity_mask);
  	if (mp->affinity == PERF_AFFINITY_NODE && cpu__max_node() > 1)
 -		build_node_mask(cpu__get_node(map->core.cpu), &map->affinity_mask);
 +		build_node_mask(cpu__get_node(map->cpu), &map->affinity_mask);
  	else if (mp->affinity == PERF_AFFINITY_CPU)
 -		CPU_SET(map->core.cpu, &map->affinity_mask);
 +		CPU_SET(map->cpu, &map->affinity_mask);
  }
  
 -int mmap__mmap(struct mmap *map, struct mmap_params *mp, int fd, int cpu)
 +int perf_mmap__mmap(struct perf_mmap *map, struct mmap_params *mp, int fd, int cpu)
  {
++<<<<<<< HEAD
 +	/*
 +	 * The last one will be done at perf_mmap__consume(), so that we
 +	 * make sure we don't prevent tools from consuming every last event in
 +	 * the ring buffer.
 +	 *
 +	 * I.e. we can get the POLLHUP meaning that the fd doesn't exist
 +	 * anymore, but the last events for it are still in the ring buffer,
 +	 * waiting to be consumed.
 +	 *
 +	 * Tools can chose to ignore this at their own discretion, but the
 +	 * evlist layer can't just drop it when filtering events in
 +	 * perf_evlist__filter_pollfd().
 +	 */
 +	refcount_set(&map->refcnt, 2);
 +	map->prev = 0;
 +	map->mask = mp->mask;
 +	map->base = mmap(NULL, perf_mmap__mmap_len(map), mp->prot,
 +			 MAP_SHARED, fd, 0);
 +	if (map->base == MAP_FAILED) {
++=======
+ 	if (perf_mmap__mmap(&map->core, &mp->core, fd, cpu)) {
++>>>>>>> 285aaeac8c5d (libperf: Centralize map refcnt setting)
  		pr_debug2("failed to mmap perf event ring buffer, error %d\n",
  			  errno);
 +		map->base = NULL;
  		return -1;
  	}
 +	map->fd = fd;
 +	map->cpu = cpu;
  
  	perf_mmap__setup_affinity_mask(map, mp);
  
* Unmerged path tools/perf/lib/evlist.c
* Unmerged path tools/perf/lib/evlist.c
* Unmerged path tools/perf/util/mmap.c
