bpf: Add generic support for update and delete batch ops

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Brian Vazquez <brianvv@google.com>
commit aa2e93b8e58e18442edfb2427446732415bc215e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/aa2e93b8.failed

This commit adds generic support for update and delete batch ops that
can be used for almost all the bpf maps. These commands share the same
UAPI attr that lookup and lookup_and_delete batch ops use and the
syscall commands are:

  BPF_MAP_UPDATE_BATCH
  BPF_MAP_DELETE_BATCH

The main difference between update/delete and lookup batch ops is that
for update/delete keys/values must be specified for userspace and
because of that, neither in_batch nor out_batch are used.

	Suggested-by: Stanislav Fomichev <sdf@google.com>
	Signed-off-by: Brian Vazquez <brianvv@google.com>
	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200115184308.162644-4-brianvv@google.com
(cherry picked from commit aa2e93b8e58e18442edfb2427446732415bc215e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	include/uapi/linux/bpf.h
#	kernel/bpf/syscall.c
diff --cc include/linux/bpf.h
index 602dd6841705,05466ad6cf1c..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -41,7 -43,13 +41,17 @@@ struct bpf_map_ops 
  	void (*map_free)(struct bpf_map *map);
  	int (*map_get_next_key)(struct bpf_map *map, void *key, void *next_key);
  	void (*map_release_uref)(struct bpf_map *map);
++<<<<<<< HEAD
 +	RH_KABI_BROKEN_INSERT(void *(*map_lookup_elem_sys_only)(struct bpf_map *map, void *key))
++=======
+ 	void *(*map_lookup_elem_sys_only)(struct bpf_map *map, void *key);
+ 	int (*map_lookup_batch)(struct bpf_map *map, const union bpf_attr *attr,
+ 				union bpf_attr __user *uattr);
+ 	int (*map_update_batch)(struct bpf_map *map, const union bpf_attr *attr,
+ 				union bpf_attr __user *uattr);
+ 	int (*map_delete_batch)(struct bpf_map *map, const union bpf_attr *attr,
+ 				union bpf_attr __user *uattr);
++>>>>>>> aa2e93b8e58e (bpf: Add generic support for update and delete batch ops)
  
  	/* funcs callable from userspace and from eBPF programs */
  	void *(*map_lookup_elem)(struct bpf_map *map, void *key);
@@@ -696,8 -985,18 +706,20 @@@ void bpf_map_charge_finish(struct bpf_m
  void bpf_map_charge_move(struct bpf_map_memory *dst,
  			 struct bpf_map_memory *src);
  void *bpf_map_area_alloc(u64 size, int numa_node);
 -void *bpf_map_area_mmapable_alloc(u64 size, int numa_node);
  void bpf_map_area_free(void *base);
  void bpf_map_init_from_attr(struct bpf_map *map, union bpf_attr *attr);
++<<<<<<< HEAD
++=======
+ int  generic_map_lookup_batch(struct bpf_map *map,
+ 			      const union bpf_attr *attr,
+ 			      union bpf_attr __user *uattr);
+ int  generic_map_update_batch(struct bpf_map *map,
+ 			      const union bpf_attr *attr,
+ 			      union bpf_attr __user *uattr);
+ int  generic_map_delete_batch(struct bpf_map *map,
+ 			      const union bpf_attr *attr,
+ 			      union bpf_attr __user *uattr);
++>>>>>>> aa2e93b8e58e (bpf: Add generic support for update and delete batch ops)
  
  extern int sysctl_unprivileged_bpf_disabled;
  
diff --cc include/uapi/linux/bpf.h
index f26f93a554f1,d5320b0be459..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -107,6 -107,9 +107,12 @@@ enum bpf_cmd 
  	BPF_MAP_LOOKUP_AND_DELETE_ELEM,
  	BPF_MAP_FREEZE,
  	BPF_BTF_GET_NEXT_ID,
++<<<<<<< HEAD
++=======
+ 	BPF_MAP_LOOKUP_BATCH,
+ 	BPF_MAP_UPDATE_BATCH,
+ 	BPF_MAP_DELETE_BATCH,
++>>>>>>> aa2e93b8e58e (bpf: Add generic support for update and delete batch ops)
  };
  
  enum bpf_map_type {
diff --cc kernel/bpf/syscall.c
index 4aa6a47aa4b8,ce8244d1ba99..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -1124,6 -1218,214 +1124,217 @@@ err_put
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ int generic_map_delete_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 cp, max_count;
+ 	int err = 0;
+ 	void *key;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		key = __bpf_copy_key(keys + cp * map->key_size, map->key_size);
+ 		if (IS_ERR(key)) {
+ 			err = PTR_ERR(key);
+ 			break;
+ 		}
+ 
+ 		if (bpf_map_is_dev_bound(map)) {
+ 			err = bpf_map_offload_delete_elem(map, key);
+ 			break;
+ 		}
+ 
+ 		preempt_disable();
+ 		__this_cpu_inc(bpf_prog_active);
+ 		rcu_read_lock();
+ 		err = map->ops->map_delete_elem(map, key);
+ 		rcu_read_unlock();
+ 		__this_cpu_dec(bpf_prog_active);
+ 		preempt_enable();
+ 		maybe_wait_bpf_programs(map);
+ 		if (err)
+ 			break;
+ 	}
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 	return err;
+ }
+ 
+ int generic_map_update_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 value_size, cp, max_count;
+ 	int ufd = attr->map_fd;
+ 	void *key, *value;
+ 	struct fd f;
+ 	int err = 0;
+ 
+ 	f = fdget(ufd);
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!value)
+ 		return -ENOMEM;
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		key = __bpf_copy_key(keys + cp * map->key_size, map->key_size);
+ 		if (IS_ERR(key)) {
+ 			err = PTR_ERR(key);
+ 			break;
+ 		}
+ 		err = -EFAULT;
+ 		if (copy_from_user(value, values + cp * value_size, value_size))
+ 			break;
+ 
+ 		err = bpf_map_update_value(map, f, key, value,
+ 					   attr->batch.elem_flags);
+ 
+ 		if (err)
+ 			break;
+ 	}
+ 
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 
+ 	kfree(value);
+ 	kfree(key);
+ 	return err;
+ }
+ 
+ #define MAP_LOOKUP_RETRIES 3
+ 
+ int generic_map_lookup_batch(struct bpf_map *map,
+ 				    const union bpf_attr *attr,
+ 				    union bpf_attr __user *uattr)
+ {
+ 	void __user *uobatch = u64_to_user_ptr(attr->batch.out_batch);
+ 	void __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	void *buf, *buf_prevkey, *prev_key, *key, *value;
+ 	int err, retry = MAP_LOOKUP_RETRIES;
+ 	u32 value_size, cp, max_count;
+ 	bool first_key = false;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map))
+ 		return -EINVAL;
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	if (put_user(0, &uattr->batch.count))
+ 		return -EFAULT;
+ 
+ 	buf_prevkey = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf_prevkey)
+ 		return -ENOMEM;
+ 
+ 	buf = kmalloc(map->key_size + value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf) {
+ 		kvfree(buf_prevkey);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	err = -EFAULT;
+ 	first_key = false;
+ 	prev_key = NULL;
+ 	if (ubatch && copy_from_user(buf_prevkey, ubatch, map->key_size))
+ 		goto free_buf;
+ 	key = buf;
+ 	value = key + map->key_size;
+ 	if (ubatch)
+ 		prev_key = buf_prevkey;
+ 
+ 	for (cp = 0; cp < max_count;) {
+ 		rcu_read_lock();
+ 		err = map->ops->map_get_next_key(map, prev_key, key);
+ 		rcu_read_unlock();
+ 		if (err)
+ 			break;
+ 		err = bpf_map_copy_value(map, key, value,
+ 					 attr->batch.elem_flags);
+ 
+ 		if (err == -ENOENT) {
+ 			if (retry) {
+ 				retry--;
+ 				continue;
+ 			}
+ 			err = -EINTR;
+ 			break;
+ 		}
+ 
+ 		if (err)
+ 			goto free_buf;
+ 
+ 		if (copy_to_user(keys + cp * map->key_size, key,
+ 				 map->key_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 		if (copy_to_user(values + cp * value_size, value, value_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 
+ 		if (!prev_key)
+ 			prev_key = buf_prevkey;
+ 
+ 		swap(prev_key, key);
+ 		retry = MAP_LOOKUP_RETRIES;
+ 		cp++;
+ 	}
+ 
+ 	if (err == -EFAULT)
+ 		goto free_buf;
+ 
+ 	if ((copy_to_user(&uattr->batch.count, &cp, sizeof(cp)) ||
+ 		    (cp && copy_to_user(uobatch, prev_key, map->key_size))))
+ 		err = -EFAULT;
+ 
+ free_buf:
+ 	kfree(buf_prevkey);
+ 	kfree(buf);
+ 	return err;
+ }
+ 
++>>>>>>> aa2e93b8e58e (bpf: Add generic support for update and delete batch ops)
  #define BPF_MAP_LOOKUP_AND_DELETE_ELEM_LAST_FIELD value
  
  static int map_lookup_and_delete_elem(union bpf_attr *attr)
@@@ -2906,6 -3282,58 +3117,61 @@@ out
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ #define BPF_MAP_BATCH_LAST_FIELD batch.flags
+ 
+ #define BPF_DO_BATCH(fn)			\
+ 	do {					\
+ 		if (!fn) {			\
+ 			err = -ENOTSUPP;	\
+ 			goto err_put;		\
+ 		}				\
+ 		err = fn(map, attr, uattr);	\
+ 	} while (0)
+ 
+ static int bpf_map_do_batch(const union bpf_attr *attr,
+ 			    union bpf_attr __user *uattr,
+ 			    int cmd)
+ {
+ 	struct bpf_map *map;
+ 	int err, ufd;
+ 	struct fd f;
+ 
+ 	if (CHECK_ATTR(BPF_MAP_BATCH))
+ 		return -EINVAL;
+ 
+ 	ufd = attr->batch.map_fd;
+ 	f = fdget(ufd);
+ 	map = __bpf_map_get(f);
+ 	if (IS_ERR(map))
+ 		return PTR_ERR(map);
+ 
+ 	if (cmd == BPF_MAP_LOOKUP_BATCH &&
+ 	    !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {
+ 		err = -EPERM;
+ 		goto err_put;
+ 	}
+ 
+ 	if (cmd != BPF_MAP_LOOKUP_BATCH &&
+ 	    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
+ 		err = -EPERM;
+ 		goto err_put;
+ 	}
+ 
+ 	if (cmd == BPF_MAP_LOOKUP_BATCH)
+ 		BPF_DO_BATCH(map->ops->map_lookup_batch);
+ 	else if (cmd == BPF_MAP_UPDATE_BATCH)
+ 		BPF_DO_BATCH(map->ops->map_update_batch);
+ 	else
+ 		BPF_DO_BATCH(map->ops->map_delete_batch);
+ 
+ err_put:
+ 	fdput(f);
+ 	return err;
+ }
+ 
++>>>>>>> aa2e93b8e58e (bpf: Add generic support for update and delete batch ops)
  SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
  {
  	union bpf_attr attr = {};
@@@ -3003,6 -3431,15 +3269,18 @@@
  	case BPF_MAP_LOOKUP_AND_DELETE_ELEM:
  		err = map_lookup_and_delete_elem(&attr);
  		break;
++<<<<<<< HEAD
++=======
+ 	case BPF_MAP_LOOKUP_BATCH:
+ 		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_LOOKUP_BATCH);
+ 		break;
+ 	case BPF_MAP_UPDATE_BATCH:
+ 		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_UPDATE_BATCH);
+ 		break;
+ 	case BPF_MAP_DELETE_BATCH:
+ 		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_DELETE_BATCH);
+ 		break;
++>>>>>>> aa2e93b8e58e (bpf: Add generic support for update and delete batch ops)
  	default:
  		err = -EINVAL;
  		break;
* Unmerged path include/linux/bpf.h
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/bpf/syscall.c
