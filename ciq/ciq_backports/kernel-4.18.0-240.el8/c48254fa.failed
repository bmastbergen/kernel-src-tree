net/smc: move add link processing for new device into llc layer

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Karsten Graul <kgraul@linux.ibm.com>
commit c48254fa48e5bad589fbbf1578dae960cedcafcf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/c48254fa.failed

When a new ib device is up smc will send an add link invitation to the
peer if needed. This is currently done with rudimentary flow control.
Under high workload these add link invitations can disturb other llc
flows because they arrive unexpected. Fix this by integrating the
invitations into the normal llc event flow and handle them as a flow.
While at it, check for already assigned requests in the flow before
the new add link request is assigned.

	Reviewed-by: Ursula Braun <ubraun@linux.ibm.com>
Fixes: 1f90a05d9ff9 ("net/smc: add smcr_port_add() and smcr_link_up() processing")
	Signed-off-by: Karsten Graul <kgraul@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c48254fa48e5bad589fbbf1578dae960cedcafcf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/smc/smc_core.c
#	net/smc/smc_llc.c
#	net/smc/smc_llc.h
diff --cc net/smc/smc_core.c
index 399bc3ffb64e,2e965de7412d..000000000000
--- a/net/smc/smc_core.c
+++ b/net/smc/smc_core.c
@@@ -43,17 -45,11 +43,14 @@@ static struct smc_lgr_list smc_lgr_lis
  static atomic_t lgr_cnt = ATOMIC_INIT(0); /* number of existing link groups */
  static DECLARE_WAIT_QUEUE_HEAD(lgrs_deleted);
  
- struct smc_ib_up_work {
- 	struct work_struct	work;
- 	struct smc_link_group	*lgr;
- 	struct smc_ib_device	*smcibdev;
- 	u8			ibport;
- };
- 
  static void smc_buf_free(struct smc_link_group *lgr, bool is_rmb,
  			 struct smc_buf_desc *buf_desc);
 -static void __smc_lgr_terminate(struct smc_link_group *lgr, bool soft);
  
++<<<<<<< HEAD
 +static void smc_link_up_work(struct work_struct *work);
++=======
+ static void smc_link_down_work(struct work_struct *work);
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
  
  /* return head of link group list and its lock for a given link group */
  static inline struct list_head *smc_lgr_list_head(struct smc_link_group *lgr,
@@@ -921,50 -1098,8 +918,52 @@@ static void smc_conn_abort_work(struct 
  	sock_put(&smc->sk); /* sock_hold done by schedulers of abort_work */
  }
  
++<<<<<<< HEAD
 +/* link is up - establish alternate link if applicable */
 +static void smcr_link_up(struct smc_link_group *lgr,
 +			 struct smc_ib_device *smcibdev, u8 ibport)
 +{
 +	struct smc_link *link = NULL;
 +
 +	if (list_empty(&lgr->list) ||
 +	    lgr->type == SMC_LGR_SYMMETRIC ||
 +	    lgr->type == SMC_LGR_ASYMMETRIC_PEER)
 +		return;
 +
 +	if (lgr->role == SMC_SERV) {
 +		/* trigger local add link processing */
 +		link = smc_llc_usable_link(lgr);
 +		if (!link)
 +			return;
 +		/* tbd: call smc_llc_srv_add_link_local(link); */
 +	} else {
 +		/* invite server to start add link processing */
 +		u8 gid[SMC_GID_SIZE];
 +
 +		if (smc_ib_determine_gid(smcibdev, ibport, lgr->vlan_id, gid,
 +					 NULL))
 +			return;
 +		if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
 +			/* some other llc task is ongoing */
 +			wait_event_interruptible_timeout(lgr->llc_waiter,
 +				(lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE),
 +				SMC_LLC_WAIT_TIME);
 +		}
 +		if (list_empty(&lgr->list) ||
 +		    !smc_ib_port_active(smcibdev, ibport))
 +			return; /* lgr or device no longer active */
 +		link = smc_llc_usable_link(lgr);
 +		if (!link)
 +			return;
 +		smc_llc_send_add_link(link, smcibdev->mac[ibport - 1], gid,
 +				      NULL, SMC_LLC_REQ);
 +	}
 +}
 +
++=======
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
  void smcr_port_add(struct smc_ib_device *smcibdev, u8 ibport)
  {
- 	struct smc_ib_up_work *ib_work;
  	struct smc_link_group *lgr, *n;
  
  	list_for_each_entry_safe(lgr, n, &smc_lgr_list.list, list) {
@@@ -984,18 -1118,97 +982,112 @@@
  	}
  }
  
++<<<<<<< HEAD
 +static void smc_link_up_work(struct work_struct *work)
 +{
 +	struct smc_ib_up_work *ib_work = container_of(work,
 +						      struct smc_ib_up_work,
 +						      work);
 +	struct smc_link_group *lgr = ib_work->lgr;
 +
 +	if (list_empty(&lgr->list))
 +		goto out;
 +	smcr_link_up(lgr, ib_work->smcibdev, ib_work->ibport);
 +out:
 +	kfree(ib_work);
++=======
+ /* link is down - switch connections to alternate link,
+  * must be called under lgr->llc_conf_mutex lock
+  */
+ static void smcr_link_down(struct smc_link *lnk)
+ {
+ 	struct smc_link_group *lgr = lnk->lgr;
+ 	struct smc_link *to_lnk;
+ 	int del_link_id;
+ 
+ 	if (!lgr || lnk->state == SMC_LNK_UNUSED || list_empty(&lgr->list))
+ 		return;
+ 
+ 	smc_ib_modify_qp_reset(lnk);
+ 	to_lnk = smc_switch_conns(lgr, lnk, true);
+ 	if (!to_lnk) { /* no backup link available */
+ 		smcr_link_clear(lnk, true);
+ 		return;
+ 	}
+ 	smcr_lgr_set_type(lgr, SMC_LGR_SINGLE);
+ 	del_link_id = lnk->link_id;
+ 
+ 	if (lgr->role == SMC_SERV) {
+ 		/* trigger local delete link processing */
+ 		smc_llc_srv_delete_link_local(to_lnk, del_link_id);
+ 	} else {
+ 		if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
+ 			/* another llc task is ongoing */
+ 			mutex_unlock(&lgr->llc_conf_mutex);
+ 			wait_event_timeout(lgr->llc_flow_waiter,
+ 				(list_empty(&lgr->list) ||
+ 				 lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE),
+ 				SMC_LLC_WAIT_TIME);
+ 			mutex_lock(&lgr->llc_conf_mutex);
+ 		}
+ 		if (!list_empty(&lgr->list)) {
+ 			smc_llc_send_delete_link(to_lnk, del_link_id,
+ 						 SMC_LLC_REQ, true,
+ 						 SMC_LLC_DEL_LOST_PATH);
+ 			smcr_link_clear(lnk, true);
+ 		}
+ 		wake_up(&lgr->llc_flow_waiter);	/* wake up next waiter */
+ 	}
+ }
+ 
+ /* must be called under lgr->llc_conf_mutex lock */
+ void smcr_link_down_cond(struct smc_link *lnk)
+ {
+ 	if (smc_link_downing(&lnk->state))
+ 		smcr_link_down(lnk);
+ }
+ 
+ /* will get the lgr->llc_conf_mutex lock */
+ void smcr_link_down_cond_sched(struct smc_link *lnk)
+ {
+ 	if (smc_link_downing(&lnk->state))
+ 		schedule_work(&lnk->link_down_wrk);
+ }
+ 
+ void smcr_port_err(struct smc_ib_device *smcibdev, u8 ibport)
+ {
+ 	struct smc_link_group *lgr, *n;
+ 	int i;
+ 
+ 	list_for_each_entry_safe(lgr, n, &smc_lgr_list.list, list) {
+ 		if (strncmp(smcibdev->pnetid[ibport - 1], lgr->pnet_id,
+ 			    SMC_MAX_PNETID_LEN))
+ 			continue; /* lgr is not affected */
+ 		if (list_empty(&lgr->list))
+ 			continue;
+ 		for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 			struct smc_link *lnk = &lgr->lnk[i];
+ 
+ 			if (smc_link_usable(lnk) &&
+ 			    lnk->smcibdev == smcibdev && lnk->ibport == ibport)
+ 				smcr_link_down_cond_sched(lnk);
+ 		}
+ 	}
+ }
+ 
+ static void smc_link_down_work(struct work_struct *work)
+ {
+ 	struct smc_link *link = container_of(work, struct smc_link,
+ 					     link_down_wrk);
+ 	struct smc_link_group *lgr = link->lgr;
+ 
+ 	if (list_empty(&lgr->list))
+ 		return;
+ 	wake_up_all(&lgr->llc_msg_waiter);
+ 	mutex_lock(&lgr->llc_conf_mutex);
+ 	smcr_link_down(link);
+ 	mutex_unlock(&lgr->llc_conf_mutex);
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
  }
  
  /* Determine vlan of internal TCP socket.
diff --cc net/smc/smc_llc.c
index 4119cdb6b6bf,30da040ab5b6..000000000000
--- a/net/smc/smc_llc.c
+++ b/net/smc/smc_llc.c
@@@ -398,196 -821,893 +398,713 @@@ static int smc_llc_send_message(struct 
  	return 0;
  }
  
 -static void smc_llc_save_add_link_info(struct smc_link *link,
 -				       struct smc_llc_msg_add_link *add_llc)
 -{
 -	link->peer_qpn = ntoh24(add_llc->sender_qp_num);
 -	memcpy(link->peer_gid, add_llc->sender_gid, SMC_GID_SIZE);
 -	memcpy(link->peer_mac, add_llc->sender_mac, ETH_ALEN);
 -	link->peer_psn = ntoh24(add_llc->initial_psn);
 -	link->peer_mtu = add_llc->qp_mtu;
 -}
 +/********************************* receive ***********************************/
  
 -/* as an SMC client, process an add link request */
 -int smc_llc_cli_add_link(struct smc_link *link, struct smc_llc_qentry *qentry)
 +static void smc_llc_rx_confirm_link(struct smc_link *link,
 +				    struct smc_llc_msg_confirm_link *llc)
  {
 -	struct smc_llc_msg_add_link *llc = &qentry->msg.add_link;
 -	enum smc_lgr_type lgr_new_t = SMC_LGR_SYMMETRIC;
  	struct smc_link_group *lgr = smc_get_lgr(link);
 -	struct smc_link *lnk_new = NULL;
 -	struct smc_init_info ini;
 -	int lnk_idx, rc = 0;
 -
 -	ini.vlan_id = lgr->vlan_id;
 -	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
 -	if (!memcmp(llc->sender_gid, link->peer_gid, SMC_GID_SIZE) &&
 -	    !memcmp(llc->sender_mac, link->peer_mac, ETH_ALEN)) {
 -		if (!ini.ib_dev)
 -			goto out_reject;
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_PEER;
 -	}
 -	if (!ini.ib_dev) {
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_LOCAL;
 -		ini.ib_dev = link->smcibdev;
 -		ini.ib_port = link->ibport;
 -	}
 -	lnk_idx = smc_llc_alloc_alt_link(lgr, lgr_new_t);
 -	if (lnk_idx < 0)
 -		goto out_reject;
 -	lnk_new = &lgr->lnk[lnk_idx];
 -	rc = smcr_link_init(lgr, lnk_new, lnk_idx, &ini);
 -	if (rc)
 -		goto out_reject;
 -	smc_llc_save_add_link_info(lnk_new, llc);
 -	lnk_new->link_id = llc->link_num;	/* SMC server assigns link id */
 -	smc_llc_link_set_uid(lnk_new);
 +	int conf_rc;
  
 +	/* RMBE eyecatchers are not supported */
 +	if (llc->hd.flags & SMC_LLC_FLAG_NO_RMBE_EYEC)
 +		conf_rc = 0;
 +	else
 +		conf_rc = ENOTSUPP;
 +
++<<<<<<< HEAD
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (lgr->role == SMC_SERV &&
 +		    link->state == SMC_LNK_ACTIVATING) {
 +			link->llc_confirm_resp_rc = conf_rc;
 +			complete(&link->llc_confirm_resp);
++=======
+ 	rc = smc_ib_ready_link(lnk_new);
+ 	if (rc)
+ 		goto out_clear_lnk;
+ 
+ 	rc = smcr_buf_map_lgr(lnk_new);
+ 	if (rc)
+ 		goto out_clear_lnk;
+ 
+ 	rc = smc_llc_send_add_link(link,
+ 				   lnk_new->smcibdev->mac[ini.ib_port - 1],
+ 				   lnk_new->gid, lnk_new, SMC_LLC_RESP);
+ 	if (rc)
+ 		goto out_clear_lnk;
+ 	rc = smc_llc_cli_rkey_exchange(link, lnk_new);
+ 	if (rc) {
+ 		rc = 0;
+ 		goto out_clear_lnk;
+ 	}
+ 	rc = smc_llc_cli_conf_link(link, &ini, lnk_new, lgr_new_t);
+ 	if (!rc)
+ 		goto out;
+ out_clear_lnk:
+ 	smcr_link_clear(lnk_new, false);
+ out_reject:
+ 	smc_llc_cli_add_link_reject(qentry);
+ out:
+ 	kfree(qentry);
+ 	return rc;
+ }
+ 
+ /* as an SMC client, invite server to start the add_link processing */
+ static void smc_llc_cli_add_link_invite(struct smc_link *link,
+ 					struct smc_llc_qentry *qentry)
+ {
+ 	struct smc_link_group *lgr = smc_get_lgr(link);
+ 	struct smc_init_info ini;
+ 
+ 	if (lgr->type == SMC_LGR_SYMMETRIC ||
+ 	    lgr->type == SMC_LGR_ASYMMETRIC_PEER)
+ 		goto out;
+ 
+ 	ini.vlan_id = lgr->vlan_id;
+ 	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
+ 	if (!ini.ib_dev)
+ 		goto out;
+ 
+ 	smc_llc_send_add_link(link, ini.ib_dev->mac[ini.ib_port - 1],
+ 			      ini.ib_gid, NULL, SMC_LLC_REQ);
+ out:
+ 	kfree(qentry);
+ }
+ 
+ static bool smc_llc_is_local_add_link(union smc_llc_msg *llc)
+ {
+ 	if (llc->raw.hdr.common.type == SMC_LLC_ADD_LINK &&
+ 	    !llc->add_link.qp_mtu && !llc->add_link.link_num)
+ 		return true;
+ 	return false;
+ }
+ 
+ static void smc_llc_process_cli_add_link(struct smc_link_group *lgr)
+ {
+ 	struct smc_llc_qentry *qentry;
+ 
+ 	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
+ 
+ 	mutex_lock(&lgr->llc_conf_mutex);
+ 	if (smc_llc_is_local_add_link(&qentry->msg))
+ 		smc_llc_cli_add_link_invite(qentry->link, qentry);
+ 	else
+ 		smc_llc_cli_add_link(qentry->link, qentry);
+ 	mutex_unlock(&lgr->llc_conf_mutex);
+ }
+ 
+ static int smc_llc_active_link_count(struct smc_link_group *lgr)
+ {
+ 	int i, link_count = 0;
+ 
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		if (!smc_link_usable(&lgr->lnk[i]))
+ 			continue;
+ 		link_count++;
+ 	}
+ 	return link_count;
+ }
+ 
+ /* find the asymmetric link when 3 links are established  */
+ static struct smc_link *smc_llc_find_asym_link(struct smc_link_group *lgr)
+ {
+ 	int asym_idx = -ENOENT;
+ 	int i, j, k;
+ 	bool found;
+ 
+ 	/* determine asymmetric link */
+ 	found = false;
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		for (j = i + 1; j < SMC_LINKS_PER_LGR_MAX; j++) {
+ 			if (!smc_link_usable(&lgr->lnk[i]) ||
+ 			    !smc_link_usable(&lgr->lnk[j]))
+ 				continue;
+ 			if (!memcmp(lgr->lnk[i].gid, lgr->lnk[j].gid,
+ 				    SMC_GID_SIZE)) {
+ 				found = true;	/* asym_lnk is i or j */
+ 				break;
+ 			}
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
 +		}
 +	} else {
 +		if (lgr->role == SMC_CLNT &&
 +		    link->state == SMC_LNK_ACTIVATING) {
 +			link->llc_confirm_rc = conf_rc;
 +			link->link_id = llc->link_num;
 +			complete(&link->llc_confirm);
  		}
 -		if (found)
 -			break;
  	}
 -	if (!found)
 -		goto out; /* no asymmetric link */
 -	for (k = 0; k < SMC_LINKS_PER_LGR_MAX; k++) {
 -		if (!smc_link_usable(&lgr->lnk[k]))
 -			continue;
 -		if (k != i &&
 -		    !memcmp(lgr->lnk[i].peer_gid, lgr->lnk[k].peer_gid,
 -			    SMC_GID_SIZE)) {
 -			asym_idx = i;
 -			break;
 +}
 +
 +static void smc_llc_rx_add_link(struct smc_link *link,
 +				struct smc_llc_msg_add_link *llc)
 +{
 +	struct smc_link_group *lgr = smc_get_lgr(link);
 +
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (link->state == SMC_LNK_ACTIVATING)
 +			complete(&link->llc_add_resp);
 +	} else {
 +		if (link->state == SMC_LNK_ACTIVATING) {
 +			complete(&link->llc_add);
 +			return;
  		}
 -		if (k != j &&
 -		    !memcmp(lgr->lnk[j].peer_gid, lgr->lnk[k].peer_gid,
 -			    SMC_GID_SIZE)) {
 -			asym_idx = j;
 -			break;
 +
 +		if (lgr->role == SMC_SERV) {
 +			smc_llc_prep_add_link(llc, link,
 +					link->smcibdev->mac[link->ibport - 1],
 +					link->gid, SMC_LLC_REQ);
 +
 +		} else {
 +			smc_llc_prep_add_link(llc, link,
 +					link->smcibdev->mac[link->ibport - 1],
 +					link->gid, SMC_LLC_RESP);
  		}
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -out:
 -	return (asym_idx < 0) ? NULL : &lgr->lnk[asym_idx];
  }
  
 -static void smc_llc_delete_asym_link(struct smc_link_group *lgr)
 +static void smc_llc_rx_delete_link(struct smc_link *link,
 +				   struct smc_llc_msg_del_link *llc)
  {
 -	struct smc_link *lnk_new = NULL, *lnk_asym;
 -	struct smc_llc_qentry *qentry;
 -	int rc;
 +	struct smc_link_group *lgr = smc_get_lgr(link);
  
 -	lnk_asym = smc_llc_find_asym_link(lgr);
 -	if (!lnk_asym)
 -		return; /* no asymmetric link */
 -	if (!smc_link_downing(&lnk_asym->state))
 -		return;
 -	lnk_new = smc_switch_conns(lgr, lnk_asym, false);
 -	smc_wr_tx_wait_no_pending_sends(lnk_asym);
 -	if (!lnk_new)
 -		goto out_free;
 -	/* change flow type from ADD_LINK into DEL_LINK */
 -	lgr->llc_flow_lcl.type = SMC_LLC_FLOW_DEL_LINK;
 -	rc = smc_llc_send_delete_link(lnk_new, lnk_asym->link_id, SMC_LLC_REQ,
 -				      true, SMC_LLC_DEL_NO_ASYM_NEEDED);
 -	if (rc) {
 -		smcr_link_down_cond(lnk_new);
 -		goto out_free;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (lgr->role == SMC_SERV)
 +			smc_lgr_schedule_free_work_fast(lgr);
 +	} else {
 +		smc_lgr_forget(lgr);
 +		smc_llc_link_deleting(link);
 +		if (lgr->role == SMC_SERV) {
 +			/* client asks to delete this link, send request */
 +			smc_llc_prep_delete_link(llc, link, SMC_LLC_REQ, true);
 +		} else {
 +			/* server requests to delete this link, send response */
 +			smc_llc_prep_delete_link(llc, link, SMC_LLC_RESP, true);
 +		}
 +		smc_llc_send_message(link, llc, sizeof(*llc));
 +		smc_lgr_terminate_sched(lgr);
  	}
 -	qentry = smc_llc_wait(lgr, lnk_new, SMC_LLC_WAIT_TIME,
 -			      SMC_LLC_DELETE_LINK);
 -	if (!qentry) {
 -		smcr_link_down_cond(lnk_new);
 -		goto out_free;
 +}
 +
 +static void smc_llc_rx_test_link(struct smc_link *link,
 +				 struct smc_llc_msg_test_link *llc)
 +{
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (link->state == SMC_LNK_ACTIVE)
 +			complete(&link->llc_testlink_resp);
 +	} else {
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -out_free:
 -	smcr_link_clear(lnk_asym, true);
  }
  
 -static int smc_llc_srv_rkey_exchange(struct smc_link *link,
 -				     struct smc_link *link_new)
 +static void smc_llc_rx_confirm_rkey(struct smc_link *link,
 +				    struct smc_llc_msg_confirm_rkey *llc)
  {
 -	struct smc_llc_msg_add_link_cont *addc_llc;
 -	struct smc_link_group *lgr = link->lgr;
 -	u8 max, num_rkeys_send, num_rkeys_recv;
 -	struct smc_llc_qentry *qentry = NULL;
 -	struct smc_buf_desc *buf_pos;
 -	int buf_lst;
 -	int rc = 0;
 -	int i;
 +	int rc;
  
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		link->llc_confirm_rkey_rc = llc->hd.flags &
 +					    SMC_LLC_FLAG_RKEY_NEG;
 +		complete(&link->llc_confirm_rkey);
 +	} else {
 +		rc = smc_rtoken_add(link,
 +				    llc->rtoken[0].rmb_vaddr,
 +				    llc->rtoken[0].rmb_key);
 +
 +		/* ignore rtokens for other links, we have only one link */
 +
++<<<<<<< HEAD
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		if (rc < 0)
 +			llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
++=======
+ 	mutex_lock(&lgr->rmbs_lock);
+ 	num_rkeys_send = lgr->conns_num;
+ 	buf_pos = smc_llc_get_first_rmb(lgr, &buf_lst);
+ 	do {
+ 		smc_llc_add_link_cont(link, link_new, &num_rkeys_send,
+ 				      &buf_lst, &buf_pos);
+ 		qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_TIME,
+ 				      SMC_LLC_ADD_LINK_CONT);
+ 		if (!qentry) {
+ 			rc = -ETIMEDOUT;
+ 			goto out;
+ 		}
+ 		addc_llc = &qentry->msg.add_link_cont;
+ 		num_rkeys_recv = addc_llc->num_rkeys;
+ 		max = min_t(u8, num_rkeys_recv, SMC_LLC_RKEYS_PER_CONT_MSG);
+ 		for (i = 0; i < max; i++) {
+ 			smc_rtoken_set(lgr, link->link_idx, link_new->link_idx,
+ 				       addc_llc->rt[i].rmb_key,
+ 				       addc_llc->rt[i].rmb_vaddr_new,
+ 				       addc_llc->rt[i].rmb_key_new);
+ 			num_rkeys_recv--;
+ 		}
+ 		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 	} while (num_rkeys_send || num_rkeys_recv);
+ out:
+ 	mutex_unlock(&lgr->rmbs_lock);
+ 	return rc;
+ }
+ 
+ static int smc_llc_srv_conf_link(struct smc_link *link,
+ 				 struct smc_link *link_new,
+ 				 enum smc_lgr_type lgr_new_t)
+ {
+ 	struct smc_link_group *lgr = link->lgr;
+ 	struct smc_llc_qentry *qentry = NULL;
+ 	int rc;
+ 
+ 	/* send CONFIRM LINK request over the RoCE fabric */
+ 	rc = smc_llc_send_confirm_link(link_new, SMC_LLC_REQ);
+ 	if (rc)
+ 		return -ENOLINK;
+ 	/* receive CONFIRM LINK response over the RoCE fabric */
+ 	qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_FIRST_TIME, 0);
+ 	if (!qentry ||
+ 	    qentry->msg.raw.hdr.common.type != SMC_LLC_CONFIRM_LINK) {
+ 		/* send DELETE LINK */
+ 		smc_llc_send_delete_link(link, link_new->link_id, SMC_LLC_REQ,
+ 					 false, SMC_LLC_DEL_LOST_PATH);
+ 		if (qentry)
+ 			smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 		return -ENOLINK;
+ 	}
+ 	smc_llc_save_peer_uid(qentry);
+ 	smc_llc_link_active(link_new);
+ 	if (lgr_new_t == SMC_LGR_ASYMMETRIC_LOCAL ||
+ 	    lgr_new_t == SMC_LGR_ASYMMETRIC_PEER)
+ 		smcr_lgr_set_type_asym(lgr, lgr_new_t, link_new->link_idx);
+ 	else
+ 		smcr_lgr_set_type(lgr, lgr_new_t);
+ 	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 	return 0;
+ }
+ 
+ int smc_llc_srv_add_link(struct smc_link *link)
+ {
+ 	enum smc_lgr_type lgr_new_t = SMC_LGR_SYMMETRIC;
+ 	struct smc_link_group *lgr = link->lgr;
+ 	struct smc_llc_msg_add_link *add_llc;
+ 	struct smc_llc_qentry *qentry = NULL;
+ 	struct smc_link *link_new;
+ 	struct smc_init_info ini;
+ 	int lnk_idx, rc = 0;
+ 
+ 	/* ignore client add link recommendation, start new flow */
+ 	ini.vlan_id = lgr->vlan_id;
+ 	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
+ 	if (!ini.ib_dev) {
+ 		lgr_new_t = SMC_LGR_ASYMMETRIC_LOCAL;
+ 		ini.ib_dev = link->smcibdev;
+ 		ini.ib_port = link->ibport;
+ 	}
+ 	lnk_idx = smc_llc_alloc_alt_link(lgr, lgr_new_t);
+ 	if (lnk_idx < 0)
+ 		return 0;
+ 
+ 	rc = smcr_link_init(lgr, &lgr->lnk[lnk_idx], lnk_idx, &ini);
+ 	if (rc)
+ 		return rc;
+ 	link_new = &lgr->lnk[lnk_idx];
+ 	rc = smc_llc_send_add_link(link,
+ 				   link_new->smcibdev->mac[ini.ib_port - 1],
+ 				   link_new->gid, link_new, SMC_LLC_REQ);
+ 	if (rc)
+ 		goto out_err;
+ 	/* receive ADD LINK response over the RoCE fabric */
+ 	qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_TIME, SMC_LLC_ADD_LINK);
+ 	if (!qentry) {
+ 		rc = -ETIMEDOUT;
+ 		goto out_err;
+ 	}
+ 	add_llc = &qentry->msg.add_link;
+ 	if (add_llc->hd.flags & SMC_LLC_FLAG_ADD_LNK_REJ) {
+ 		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 		rc = -ENOLINK;
+ 		goto out_err;
+ 	}
+ 	if (lgr->type == SMC_LGR_SINGLE &&
+ 	    (!memcmp(add_llc->sender_gid, link->peer_gid, SMC_GID_SIZE) &&
+ 	     !memcmp(add_llc->sender_mac, link->peer_mac, ETH_ALEN))) {
+ 		lgr_new_t = SMC_LGR_ASYMMETRIC_PEER;
+ 	}
+ 	smc_llc_save_add_link_info(link_new, add_llc);
+ 	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 
+ 	rc = smc_ib_ready_link(link_new);
+ 	if (rc)
+ 		goto out_err;
+ 	rc = smcr_buf_map_lgr(link_new);
+ 	if (rc)
+ 		goto out_err;
+ 	rc = smcr_buf_reg_lgr(link_new);
+ 	if (rc)
+ 		goto out_err;
+ 	rc = smc_llc_srv_rkey_exchange(link, link_new);
+ 	if (rc)
+ 		goto out_err;
+ 	rc = smc_llc_srv_conf_link(link, link_new, lgr_new_t);
+ 	if (rc)
+ 		goto out_err;
+ 	return 0;
+ out_err:
+ 	smcr_link_clear(link_new, false);
+ 	return rc;
+ }
+ 
+ static void smc_llc_process_srv_add_link(struct smc_link_group *lgr)
+ {
+ 	struct smc_link *link = lgr->llc_flow_lcl.qentry->link;
+ 	int rc;
+ 
+ 	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 
+ 	mutex_lock(&lgr->llc_conf_mutex);
+ 	rc = smc_llc_srv_add_link(link);
+ 	if (!rc && lgr->type == SMC_LGR_SYMMETRIC) {
+ 		/* delete any asymmetric link */
+ 		smc_llc_delete_asym_link(lgr);
+ 	}
+ 	mutex_unlock(&lgr->llc_conf_mutex);
+ }
+ 
+ /* enqueue a local add_link req to trigger a new add_link flow */
+ void smc_llc_add_link_local(struct smc_link *link)
+ {
+ 	struct smc_llc_msg_add_link add_llc = {0};
+ 
+ 	add_llc.hd.length = sizeof(add_llc);
+ 	add_llc.hd.common.type = SMC_LLC_ADD_LINK;
+ 	/* no dev and port needed */
+ 	smc_llc_enqueue(link, (union smc_llc_msg *)&add_llc);
+ }
+ 
+ /* worker to process an add link message */
+ static void smc_llc_add_link_work(struct work_struct *work)
+ {
+ 	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
+ 						  llc_add_link_work);
+ 
+ 	if (list_empty(&lgr->list)) {
+ 		/* link group is terminating */
+ 		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 		goto out;
+ 	}
+ 
+ 	if (lgr->role == SMC_CLNT)
+ 		smc_llc_process_cli_add_link(lgr);
+ 	else
+ 		smc_llc_process_srv_add_link(lgr);
+ out:
+ 	smc_llc_flow_stop(lgr, &lgr->llc_flow_lcl);
+ }
+ 
+ /* enqueue a local del_link msg to trigger a new del_link flow,
+  * called only for role SMC_SERV
+  */
+ void smc_llc_srv_delete_link_local(struct smc_link *link, u8 del_link_id)
+ {
+ 	struct smc_llc_msg_del_link del_llc = {0};
+ 
+ 	del_llc.hd.length = sizeof(del_llc);
+ 	del_llc.hd.common.type = SMC_LLC_DELETE_LINK;
+ 	del_llc.link_num = del_link_id;
+ 	del_llc.reason = htonl(SMC_LLC_DEL_LOST_PATH);
+ 	del_llc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ORDERLY;
+ 	smc_llc_enqueue(link, (union smc_llc_msg *)&del_llc);
+ }
+ 
+ static void smc_llc_process_cli_delete_link(struct smc_link_group *lgr)
+ {
+ 	struct smc_link *lnk_del = NULL, *lnk_asym, *lnk;
+ 	struct smc_llc_msg_del_link *del_llc;
+ 	struct smc_llc_qentry *qentry;
+ 	int active_links;
+ 	int lnk_idx;
+ 
+ 	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
+ 	lnk = qentry->link;
+ 	del_llc = &qentry->msg.delete_link;
+ 
+ 	if (del_llc->hd.flags & SMC_LLC_FLAG_DEL_LINK_ALL) {
+ 		smc_lgr_terminate_sched(lgr);
+ 		goto out;
+ 	}
+ 	mutex_lock(&lgr->llc_conf_mutex);
+ 	/* delete single link */
+ 	for (lnk_idx = 0; lnk_idx < SMC_LINKS_PER_LGR_MAX; lnk_idx++) {
+ 		if (lgr->lnk[lnk_idx].link_id != del_llc->link_num)
+ 			continue;
+ 		lnk_del = &lgr->lnk[lnk_idx];
+ 		break;
+ 	}
+ 	del_llc->hd.flags |= SMC_LLC_FLAG_RESP;
+ 	if (!lnk_del) {
+ 		/* link was not found */
+ 		del_llc->reason = htonl(SMC_LLC_DEL_NOLNK);
+ 		smc_llc_send_message(lnk, &qentry->msg);
+ 		goto out_unlock;
+ 	}
+ 	lnk_asym = smc_llc_find_asym_link(lgr);
+ 
+ 	del_llc->reason = 0;
+ 	smc_llc_send_message(lnk, &qentry->msg); /* response */
+ 
+ 	if (smc_link_downing(&lnk_del->state)) {
+ 		if (smc_switch_conns(lgr, lnk_del, false))
+ 			smc_wr_tx_wait_no_pending_sends(lnk_del);
+ 	}
+ 	smcr_link_clear(lnk_del, true);
+ 
+ 	active_links = smc_llc_active_link_count(lgr);
+ 	if (lnk_del == lnk_asym) {
+ 		/* expected deletion of asym link, don't change lgr state */
+ 	} else if (active_links == 1) {
+ 		smcr_lgr_set_type(lgr, SMC_LGR_SINGLE);
+ 	} else if (!active_links) {
+ 		smcr_lgr_set_type(lgr, SMC_LGR_NONE);
+ 		smc_lgr_terminate_sched(lgr);
+ 	}
+ out_unlock:
+ 	mutex_unlock(&lgr->llc_conf_mutex);
+ out:
+ 	kfree(qentry);
+ }
+ 
+ /* try to send a DELETE LINK ALL request on any active link,
+  * waiting for send completion
+  */
+ void smc_llc_send_link_delete_all(struct smc_link_group *lgr, bool ord, u32 rsn)
+ {
+ 	struct smc_llc_msg_del_link delllc = {0};
+ 	int i;
+ 
+ 	delllc.hd.common.type = SMC_LLC_DELETE_LINK;
+ 	delllc.hd.length = sizeof(delllc);
+ 	if (ord)
+ 		delllc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ORDERLY;
+ 	delllc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ALL;
+ 	delllc.reason = htonl(rsn);
+ 
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		if (!smc_link_usable(&lgr->lnk[i]))
+ 			continue;
+ 		if (!smc_llc_send_message_wait(&lgr->lnk[i], &delllc))
+ 			break;
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
  	}
  }
  
 -static void smc_llc_process_srv_delete_link(struct smc_link_group *lgr)
 +static void smc_llc_rx_confirm_rkey_cont(struct smc_link *link,
 +				      struct smc_llc_msg_confirm_rkey_cont *llc)
  {
 -	struct smc_llc_msg_del_link *del_llc;
 -	struct smc_link *lnk, *lnk_del;
 -	struct smc_llc_qentry *qentry;
 -	int active_links;
 -	int i;
 -
 -	mutex_lock(&lgr->llc_conf_mutex);
 -	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
 -	lnk = qentry->link;
 -	del_llc = &qentry->msg.delete_link;
 -
 -	if (qentry->msg.delete_link.hd.flags & SMC_LLC_FLAG_DEL_LINK_ALL) {
 -		/* delete entire lgr */
 -		smc_llc_send_link_delete_all(lgr, true, ntohl(
 -					      qentry->msg.delete_link.reason));
 -		smc_lgr_terminate_sched(lgr);
 -		goto out;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		/* unused as long as we don't send this type of msg */
 +	} else {
 +		/* ignore rtokens for other links, we have only one link */
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
++<<<<<<< HEAD
++=======
+ 	/* delete single link */
+ 	lnk_del = NULL;
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		if (lgr->lnk[i].link_id == del_llc->link_num) {
+ 			lnk_del = &lgr->lnk[i];
+ 			break;
+ 		}
+ 	}
+ 	if (!lnk_del)
+ 		goto out; /* asymmetric link already deleted */
+ 
+ 	if (smc_link_downing(&lnk_del->state)) {
+ 		if (smc_switch_conns(lgr, lnk_del, false))
+ 			smc_wr_tx_wait_no_pending_sends(lnk_del);
+ 	}
+ 	if (!list_empty(&lgr->list)) {
+ 		/* qentry is either a request from peer (send it back to
+ 		 * initiate the DELETE_LINK processing), or a locally
+ 		 * enqueued DELETE_LINK request (forward it)
+ 		 */
+ 		if (!smc_llc_send_message(lnk, &qentry->msg)) {
+ 			struct smc_llc_qentry *qentry2;
+ 
+ 			qentry2 = smc_llc_wait(lgr, lnk, SMC_LLC_WAIT_TIME,
+ 					       SMC_LLC_DELETE_LINK);
+ 			if (qentry2)
+ 				smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
+ 		}
+ 	}
+ 	smcr_link_clear(lnk_del, true);
+ 
+ 	active_links = smc_llc_active_link_count(lgr);
+ 	if (active_links == 1) {
+ 		smcr_lgr_set_type(lgr, SMC_LGR_SINGLE);
+ 	} else if (!active_links) {
+ 		smcr_lgr_set_type(lgr, SMC_LGR_NONE);
+ 		smc_lgr_terminate_sched(lgr);
+ 	}
+ 
+ 	if (lgr->type == SMC_LGR_SINGLE && !list_empty(&lgr->list)) {
+ 		/* trigger setup of asymm alt link */
+ 		smc_llc_add_link_local(lnk);
+ 	}
+ out:
+ 	mutex_unlock(&lgr->llc_conf_mutex);
+ 	kfree(qentry);
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
  }
  
 -static void smc_llc_delete_link_work(struct work_struct *work)
 -{
 -	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
 -						  llc_del_link_work);
 -
 -	if (list_empty(&lgr->list)) {
 -		/* link group is terminating */
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		goto out;
 -	}
 -
 -	if (lgr->role == SMC_CLNT)
 -		smc_llc_process_cli_delete_link(lgr);
 -	else
 -		smc_llc_process_srv_delete_link(lgr);
 -out:
 -	smc_llc_flow_stop(lgr, &lgr->llc_flow_lcl);
 -}
 -
 -/* process a confirm_rkey request from peer, remote flow */
 -static void smc_llc_rmt_conf_rkey(struct smc_link_group *lgr)
 -{
 -	struct smc_llc_msg_confirm_rkey *llc;
 -	struct smc_llc_qentry *qentry;
 -	struct smc_link *link;
 -	int num_entries;
 -	int rk_idx;
 -	int i;
 -
 -	qentry = lgr->llc_flow_rmt.qentry;
 -	llc = &qentry->msg.confirm_rkey;
 -	link = qentry->link;
 -
 -	num_entries = llc->rtoken[0].num_rkeys;
 -	/* first rkey entry is for receiving link */
 -	rk_idx = smc_rtoken_add(link,
 -				llc->rtoken[0].rmb_vaddr,
 -				llc->rtoken[0].rmb_key);
 -	if (rk_idx < 0)
 -		goto out_err;
 -
 -	for (i = 1; i <= min_t(u8, num_entries, SMC_LLC_RKEYS_PER_MSG - 1); i++)
 -		smc_rtoken_set2(lgr, rk_idx, llc->rtoken[i].link_id,
 -				llc->rtoken[i].rmb_vaddr,
 -				llc->rtoken[i].rmb_key);
 -	/* max links is 3 so there is no need to support conf_rkey_cont msgs */
 -	goto out;
 -out_err:
 -	llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 -	llc->hd.flags |= SMC_LLC_FLAG_RKEY_RETRY;
 -out:
 -	llc->hd.flags |= SMC_LLC_FLAG_RESP;
 -	smc_llc_send_message(link, &qentry->msg);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_rmt);
 -}
 -
 -/* process a delete_rkey request from peer, remote flow */
 -static void smc_llc_rmt_delete_rkey(struct smc_link_group *lgr)
 +static void smc_llc_rx_delete_rkey(struct smc_link *link,
 +				   struct smc_llc_msg_delete_rkey *llc)
  {
 -	struct smc_llc_msg_delete_rkey *llc;
 -	struct smc_llc_qentry *qentry;
 -	struct smc_link *link;
  	u8 err_mask = 0;
  	int i, max;
  
++<<<<<<< HEAD
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		link->llc_delete_rkey_rc = llc->hd.flags &
 +					    SMC_LLC_FLAG_RKEY_NEG;
 +		complete(&link->llc_delete_rkey);
 +	} else {
 +		max = min_t(u8, llc->num_rkeys, SMC_LLC_DEL_RKEY_MAX);
 +		for (i = 0; i < max; i++) {
 +			if (smc_rtoken_delete(link, llc->rkey[i]))
 +				err_mask |= 1 << (SMC_LLC_DEL_RKEY_MAX - 1 - i);
++=======
+ 	qentry = lgr->llc_flow_rmt.qentry;
+ 	llc = &qentry->msg.delete_rkey;
+ 	link = qentry->link;
+ 
+ 	max = min_t(u8, llc->num_rkeys, SMC_LLC_DEL_RKEY_MAX);
+ 	for (i = 0; i < max; i++) {
+ 		if (smc_rtoken_delete(link, llc->rkey[i]))
+ 			err_mask |= 1 << (SMC_LLC_DEL_RKEY_MAX - 1 - i);
+ 	}
+ 	if (err_mask) {
+ 		llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
+ 		llc->err_mask = err_mask;
+ 	}
+ 	llc->hd.flags |= SMC_LLC_FLAG_RESP;
+ 	smc_llc_send_message(link, &qentry->msg);
+ 	smc_llc_flow_qentry_del(&lgr->llc_flow_rmt);
+ }
+ 
+ static void smc_llc_protocol_violation(struct smc_link_group *lgr, u8 type)
+ {
+ 	pr_warn_ratelimited("smc: SMC-R lg %*phN LLC protocol violation: "
+ 			    "llc_type %d\n", SMC_LGR_ID_SIZE, &lgr->id, type);
+ 	smc_llc_set_termination_rsn(lgr, SMC_LLC_DEL_PROT_VIOL);
+ 	smc_lgr_terminate_sched(lgr);
+ }
+ 
+ /* flush the llc event queue */
+ static void smc_llc_event_flush(struct smc_link_group *lgr)
+ {
+ 	struct smc_llc_qentry *qentry, *q;
+ 
+ 	spin_lock_bh(&lgr->llc_event_q_lock);
+ 	list_for_each_entry_safe(qentry, q, &lgr->llc_event_q, list) {
+ 		list_del_init(&qentry->list);
+ 		kfree(qentry);
+ 	}
+ 	spin_unlock_bh(&lgr->llc_event_q_lock);
+ }
+ 
+ static void smc_llc_event_handler(struct smc_llc_qentry *qentry)
+ {
+ 	union smc_llc_msg *llc = &qentry->msg;
+ 	struct smc_link *link = qentry->link;
+ 	struct smc_link_group *lgr = link->lgr;
+ 
+ 	if (!smc_link_usable(link))
+ 		goto out;
+ 
+ 	switch (llc->raw.hdr.common.type) {
+ 	case SMC_LLC_TEST_LINK:
+ 		llc->test_link.hd.flags |= SMC_LLC_FLAG_RESP;
+ 		smc_llc_send_message(link, llc);
+ 		break;
+ 	case SMC_LLC_ADD_LINK:
+ 		if (list_empty(&lgr->list))
+ 			goto out;	/* lgr is terminating */
+ 		if (lgr->role == SMC_CLNT) {
+ 			if (smc_llc_is_local_add_link(llc)) {
+ 				if (lgr->llc_flow_lcl.type ==
+ 				    SMC_LLC_FLOW_ADD_LINK)
+ 					break;	/* add_link in progress */
+ 				if (smc_llc_flow_start(&lgr->llc_flow_lcl,
+ 						       qentry)) {
+ 					schedule_work(&lgr->llc_add_link_work);
+ 				}
+ 				return;
+ 			}
+ 			if (lgr->llc_flow_lcl.type == SMC_LLC_FLOW_ADD_LINK &&
+ 			    !lgr->llc_flow_lcl.qentry) {
+ 				/* a flow is waiting for this message */
+ 				smc_llc_flow_qentry_set(&lgr->llc_flow_lcl,
+ 							qentry);
+ 				wake_up(&lgr->llc_msg_waiter);
+ 			} else if (smc_llc_flow_start(&lgr->llc_flow_lcl,
+ 						      qentry)) {
+ 				schedule_work(&lgr->llc_add_link_work);
+ 			}
+ 		} else if (smc_llc_flow_start(&lgr->llc_flow_lcl, qentry)) {
+ 			/* as smc server, handle client suggestion */
+ 			schedule_work(&lgr->llc_add_link_work);
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
  		}
 -		return;
 -	case SMC_LLC_CONFIRM_LINK:
 -	case SMC_LLC_ADD_LINK_CONT:
 -		if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
 -			/* a flow is waiting for this message */
 -			smc_llc_flow_qentry_set(&lgr->llc_flow_lcl, qentry);
 -			wake_up(&lgr->llc_msg_waiter);
 -			return;
 -		}
 -		break;
 -	case SMC_LLC_DELETE_LINK:
 -		if (lgr->role == SMC_CLNT) {
 -			/* server requests to delete this link, send response */
 -			if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
 -				/* DEL LINK REQ during ADD LINK SEQ */
 -				smc_llc_flow_qentry_set(&lgr->llc_flow_lcl,
 -							qentry);
 -				wake_up(&lgr->llc_msg_waiter);
 -			} else if (smc_llc_flow_start(&lgr->llc_flow_lcl,
 -						      qentry)) {
 -				schedule_work(&lgr->llc_del_link_work);
 -			}
 -		} else {
 -			if (lgr->llc_flow_lcl.type == SMC_LLC_FLOW_ADD_LINK &&
 -			    !lgr->llc_flow_lcl.qentry) {
 -				/* DEL LINK REQ during ADD LINK SEQ */
 -				smc_llc_flow_qentry_set(&lgr->llc_flow_lcl,
 -							qentry);
 -				wake_up(&lgr->llc_msg_waiter);
 -			} else if (smc_llc_flow_start(&lgr->llc_flow_lcl,
 -						      qentry)) {
 -				schedule_work(&lgr->llc_del_link_work);
 -			}
 -		}
 -		return;
 -	case SMC_LLC_CONFIRM_RKEY:
 -		/* new request from remote, assign to remote flow */
 -		if (smc_llc_flow_start(&lgr->llc_flow_rmt, qentry)) {
 -			/* process here, does not wait for more llc msgs */
 -			smc_llc_rmt_conf_rkey(lgr);
 -			smc_llc_flow_stop(lgr, &lgr->llc_flow_rmt);
 -		}
 -		return;
 -	case SMC_LLC_CONFIRM_RKEY_CONT:
 -		/* not used because max links is 3, and 3 rkeys fit into
 -		 * one CONFIRM_RKEY message
 -		 */
 -		break;
 -	case SMC_LLC_DELETE_RKEY:
 -		/* new request from remote, assign to remote flow */
 -		if (smc_llc_flow_start(&lgr->llc_flow_rmt, qentry)) {
 -			/* process here, does not wait for more llc msgs */
 -			smc_llc_rmt_delete_rkey(lgr);
 -			smc_llc_flow_stop(lgr, &lgr->llc_flow_rmt);
 -		}
 -		return;
 -	default:
 -		smc_llc_protocol_violation(lgr, llc->raw.hdr.common.type);
 -		break;
 -	}
 -out:
 -	kfree(qentry);
 -}
 -
 -/* worker to process llc messages on the event queue */
 -static void smc_llc_event_work(struct work_struct *work)
 -{
 -	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
 -						  llc_event_work);
 -	struct smc_llc_qentry *qentry;
  
 -	if (!lgr->llc_flow_lcl.type && lgr->delayed_event) {
 -		if (smc_link_usable(lgr->delayed_event->link)) {
 -			smc_llc_event_handler(lgr->delayed_event);
 -		} else {
 -			qentry = lgr->delayed_event;
 -			lgr->delayed_event = NULL;
 -			kfree(qentry);
 +		if (err_mask) {
 +			llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 +			llc->err_mask = err_mask;
  		}
 -	}
  
 -again:
 -	spin_lock_bh(&lgr->llc_event_q_lock);
 -	if (!list_empty(&lgr->llc_event_q)) {
 -		qentry = list_first_entry(&lgr->llc_event_q,
 -					  struct smc_llc_qentry, list);
 -		list_del_init(&qentry->list);
 -		spin_unlock_bh(&lgr->llc_event_q_lock);
 -		smc_llc_event_handler(qentry);
 -		goto again;
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -	spin_unlock_bh(&lgr->llc_event_q_lock);
  }
  
 -/* process llc responses in tasklet context */
 -static void smc_llc_rx_response(struct smc_link *link,
 -				struct smc_llc_qentry *qentry)
 +static void smc_llc_rx_handler(struct ib_wc *wc, void *buf)
  {
 -	enum smc_llc_flowtype flowtype = link->lgr->llc_flow_lcl.type;
 -	struct smc_llc_flow *flow = &link->lgr->llc_flow_lcl;
 -	u8 llc_type = qentry->msg.raw.hdr.common.type;
 +	struct smc_link *link = (struct smc_link *)wc->qp->qp_context;
 +	union smc_llc_msg *llc = buf;
 +
 +	if (wc->byte_len < sizeof(*llc))
 +		return; /* short message */
 +	if (llc->raw.hdr.length != sizeof(*llc))
 +		return; /* invalid message */
 +	if (link->state == SMC_LNK_INACTIVE)
 +		return; /* link not active, drop msg */
  
 -	switch (llc_type) {
 +	switch (llc->raw.hdr.common.type) {
  	case SMC_LLC_TEST_LINK:
 -		if (link->state == SMC_LNK_ACTIVE)
 -			complete(&link->llc_testlink_resp);
 +		smc_llc_rx_test_link(link, &llc->test_link);
  		break;
 -	case SMC_LLC_ADD_LINK:
 -	case SMC_LLC_ADD_LINK_CONT:
  	case SMC_LLC_CONFIRM_LINK:
 -		if (flowtype != SMC_LLC_FLOW_ADD_LINK || flow->qentry)
 -			break;	/* drop out-of-flow response */
 -		goto assign;
 +		smc_llc_rx_confirm_link(link, &llc->confirm_link);
 +		break;
 +	case SMC_LLC_ADD_LINK:
 +		smc_llc_rx_add_link(link, &llc->add_link);
 +		break;
  	case SMC_LLC_DELETE_LINK:
 -		if (flowtype != SMC_LLC_FLOW_DEL_LINK || flow->qentry)
 -			break;	/* drop out-of-flow response */
 -		goto assign;
 +		smc_llc_rx_delete_link(link, &llc->delete_link);
 +		break;
  	case SMC_LLC_CONFIRM_RKEY:
 -	case SMC_LLC_DELETE_RKEY:
 -		if (flowtype != SMC_LLC_FLOW_RKEY || flow->qentry)
 -			break;	/* drop out-of-flow response */
 -		goto assign;
 +		smc_llc_rx_confirm_rkey(link, &llc->confirm_rkey);
 +		break;
  	case SMC_LLC_CONFIRM_RKEY_CONT:
 -		/* not used because max links is 3 */
 +		smc_llc_rx_confirm_rkey_cont(link, &llc->confirm_rkey_cont);
  		break;
 -	default:
 -		smc_llc_protocol_violation(link->lgr, llc_type);
 +	case SMC_LLC_DELETE_RKEY:
 +		smc_llc_rx_delete_rkey(link, &llc->delete_rkey);
  		break;
  	}
 -	kfree(qentry);
 -	return;
 -assign:
 -	/* assign responses to the local flow, we requested them */
 -	smc_llc_flow_qentry_set(&link->lgr->llc_flow_lcl, qentry);
 -	wake_up(&link->lgr->llc_msg_waiter);
 -}
 -
 -static void smc_llc_enqueue(struct smc_link *link, union smc_llc_msg *llc)
 -{
 -	struct smc_link_group *lgr = link->lgr;
 -	struct smc_llc_qentry *qentry;
 -	unsigned long flags;
 -
 -	qentry = kmalloc(sizeof(*qentry), GFP_ATOMIC);
 -	if (!qentry)
 -		return;
 -	qentry->link = link;
 -	INIT_LIST_HEAD(&qentry->list);
 -	memcpy(&qentry->msg, llc, sizeof(union smc_llc_msg));
 -
 -	/* process responses immediately */
 -	if (llc->raw.hdr.flags & SMC_LLC_FLAG_RESP) {
 -		smc_llc_rx_response(link, qentry);
 -		return;
 -	}
 -
 -	/* add requests to event queue */
 -	spin_lock_irqsave(&lgr->llc_event_q_lock, flags);
 -	list_add_tail(&qentry->list, &lgr->llc_event_q);
 -	spin_unlock_irqrestore(&lgr->llc_event_q_lock, flags);
 -	schedule_work(&lgr->llc_event_work);
 -}
 -
 -/* copy received msg and add it to the event queue */
 -static void smc_llc_rx_handler(struct ib_wc *wc, void *buf)
 -{
 -	struct smc_link *link = (struct smc_link *)wc->qp->qp_context;
 -	union smc_llc_msg *llc = buf;
 -
 -	if (wc->byte_len < sizeof(*llc))
 -		return; /* short message */
 -	if (llc->raw.hdr.length != sizeof(*llc))
 -		return; /* invalid message */
 -
 -	smc_llc_enqueue(link, llc);
  }
  
  /***************************** worker, utils *********************************/
diff --cc net/smc/smc_llc.h
index 461c0c3ef76e,cc00a2ec4e92..000000000000
--- a/net/smc/smc_llc.h
+++ b/net/smc/smc_llc.h
@@@ -39,18 -72,38 +39,38 @@@ enum smc_llc_msg_type 
  int smc_llc_send_confirm_link(struct smc_link *lnk,
  			      enum smc_llc_reqresp reqresp);
  int smc_llc_send_add_link(struct smc_link *link, u8 mac[], u8 gid[],
 -			  struct smc_link *link_new,
  			  enum smc_llc_reqresp reqresp);
 -int smc_llc_send_delete_link(struct smc_link *link, u8 link_del_id,
 -			     enum smc_llc_reqresp reqresp, bool orderly,
 -			     u32 reason);
 -void smc_llc_srv_delete_link_local(struct smc_link *link, u8 del_link_id);
 -void smc_llc_lgr_init(struct smc_link_group *lgr, struct smc_sock *smc);
 -void smc_llc_lgr_clear(struct smc_link_group *lgr);
 +int smc_llc_send_delete_link(struct smc_link *link,
 +			     enum smc_llc_reqresp reqresp, bool orderly);
  int smc_llc_link_init(struct smc_link *link);
 -void smc_llc_link_active(struct smc_link *link);
 -void smc_llc_link_clear(struct smc_link *link, bool log);
 -int smc_llc_do_confirm_rkey(struct smc_link *send_link,
 +void smc_llc_link_active(struct smc_link *link, int testlink_time);
 +void smc_llc_link_deleting(struct smc_link *link);
 +void smc_llc_link_inactive(struct smc_link *link);
 +void smc_llc_link_clear(struct smc_link *link);
 +int smc_llc_do_confirm_rkey(struct smc_link *link,
  			    struct smc_buf_desc *rmb_desc);
 -int smc_llc_do_delete_rkey(struct smc_link_group *lgr,
 +int smc_llc_do_delete_rkey(struct smc_link *link,
  			   struct smc_buf_desc *rmb_desc);
++<<<<<<< HEAD
++=======
+ int smc_llc_flow_initiate(struct smc_link_group *lgr,
+ 			  enum smc_llc_flowtype type);
+ void smc_llc_flow_stop(struct smc_link_group *lgr, struct smc_llc_flow *flow);
+ int smc_llc_eval_conf_link(struct smc_llc_qentry *qentry,
+ 			   enum smc_llc_reqresp type);
+ void smc_llc_link_set_uid(struct smc_link *link);
+ void smc_llc_save_peer_uid(struct smc_llc_qentry *qentry);
+ struct smc_llc_qentry *smc_llc_wait(struct smc_link_group *lgr,
+ 				    struct smc_link *lnk,
+ 				    int time_out, u8 exp_msg);
+ struct smc_llc_qentry *smc_llc_flow_qentry_clr(struct smc_llc_flow *flow);
+ void smc_llc_flow_qentry_del(struct smc_llc_flow *flow);
+ void smc_llc_send_link_delete_all(struct smc_link_group *lgr, bool ord,
+ 				  u32 rsn);
+ int smc_llc_cli_add_link(struct smc_link *link, struct smc_llc_qentry *qentry);
+ int smc_llc_srv_add_link(struct smc_link *link);
+ void smc_llc_add_link_local(struct smc_link *link);
++>>>>>>> c48254fa48e5 (net/smc: move add link processing for new device into llc layer)
  int smc_llc_init(void) __init;
  
  #endif /* SMC_LLC_H */
* Unmerged path net/smc/smc_core.c
* Unmerged path net/smc/smc_llc.c
* Unmerged path net/smc/smc_llc.h
