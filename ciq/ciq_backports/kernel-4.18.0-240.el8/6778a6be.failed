net/smc: separate LLC wait queues for flow and messages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Karsten Graul <kgraul@linux.ibm.com>
commit 6778a6bed09b58beca936a675e9dd195c0986580
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6778a6be.failed

There might be races in scenarios where both SMC link groups are on the
same system. Prevent that by creating separate wait queues for LLC flows
and messages. Switch to non-interruptable versions of wait_event() and
wake_up() for the llc flow waiter to make sure the waiters get control
sequentially. Fine tune the llc_flow_lock to include the assignment of
the message. Write to system log when an unexpected message was
dropped. And remove an extra indirection and use the existing local
variable lgr in smc_llc_enqueue().

Fixes: 555da9af827d ("net/smc: add event-based llc_flow framework")
	Reviewed-by: Ursula Braun <ubraun@linux.ibm.com>
	Signed-off-by: Karsten Graul <kgraul@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 6778a6bed09b58beca936a675e9dd195c0986580)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/smc/smc_core.c
#	net/smc/smc_core.h
#	net/smc/smc_llc.c
diff --cc net/smc/smc_core.c
index 399bc3ffb64e,d695ce71837e..000000000000
--- a/net/smc/smc_core.c
+++ b/net/smc/smc_core.c
@@@ -226,18 -237,18 +226,23 @@@ void smc_lgr_cleanup_early(struct smc_c
  	smc_lgr_schedule_free_work_fast(lgr);
  }
  
 -static void smcr_lgr_link_deactivate_all(struct smc_link_group *lgr)
 +/* Send delete link, either as client to request the initiation
 + * of the DELETE LINK sequence from server; or as server to
 + * initiate the delete processing. See smc_llc_rx_delete_link().
 + */
 +static int smcr_link_send_delete(struct smc_link *lnk, bool orderly)
  {
 -	int i;
 -
 -	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
 -		struct smc_link *lnk = &lgr->lnk[i];
 -
 -		if (smc_link_usable(lnk))
 -			lnk->state = SMC_LNK_INACTIVE;
 +	if (lnk->state == SMC_LNK_ACTIVE &&
 +	    !smc_llc_send_delete_link(lnk, SMC_LLC_REQ, orderly)) {
 +		smc_llc_link_deleting(lnk);
 +		return 0;
  	}
++<<<<<<< HEAD
 +	return -ENOTCONN;
++=======
+ 	wake_up_all(&lgr->llc_msg_waiter);
+ 	wake_up_all(&lgr->llc_flow_waiter);
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
  }
  
  static void smc_lgr_free(struct smc_link_group *lgr);
@@@ -984,6 -1169,83 +990,86 @@@ void smcr_port_add(struct smc_ib_devic
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /* link is down - switch connections to alternate link,
+  * must be called under lgr->llc_conf_mutex lock
+  */
+ static void smcr_link_down(struct smc_link *lnk)
+ {
+ 	struct smc_link_group *lgr = lnk->lgr;
+ 	struct smc_link *to_lnk;
+ 	int del_link_id;
+ 
+ 	if (!lgr || lnk->state == SMC_LNK_UNUSED || list_empty(&lgr->list))
+ 		return;
+ 
+ 	smc_ib_modify_qp_reset(lnk);
+ 	to_lnk = smc_switch_conns(lgr, lnk, true);
+ 	if (!to_lnk) { /* no backup link available */
+ 		smcr_link_clear(lnk, true);
+ 		return;
+ 	}
+ 	smcr_lgr_set_type(lgr, SMC_LGR_SINGLE);
+ 	del_link_id = lnk->link_id;
+ 
+ 	if (lgr->role == SMC_SERV) {
+ 		/* trigger local delete link processing */
+ 		smc_llc_srv_delete_link_local(to_lnk, del_link_id);
+ 	} else {
+ 		if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
+ 			/* another llc task is ongoing */
+ 			mutex_unlock(&lgr->llc_conf_mutex);
+ 			wait_event_timeout(lgr->llc_flow_waiter,
+ 				(list_empty(&lgr->list) ||
+ 				 lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE),
+ 				SMC_LLC_WAIT_TIME);
+ 			mutex_lock(&lgr->llc_conf_mutex);
+ 		}
+ 		if (!list_empty(&lgr->list))
+ 			smc_llc_send_delete_link(to_lnk, del_link_id,
+ 						 SMC_LLC_REQ, true,
+ 						 SMC_LLC_DEL_LOST_PATH);
+ 		wake_up(&lgr->llc_flow_waiter);	/* wake up next waiter */
+ 	}
+ }
+ 
+ /* must be called under lgr->llc_conf_mutex lock */
+ void smcr_link_down_cond(struct smc_link *lnk)
+ {
+ 	if (smc_link_downing(&lnk->state))
+ 		smcr_link_down(lnk);
+ }
+ 
+ /* will get the lgr->llc_conf_mutex lock */
+ void smcr_link_down_cond_sched(struct smc_link *lnk)
+ {
+ 	if (smc_link_downing(&lnk->state))
+ 		schedule_work(&lnk->link_down_wrk);
+ }
+ 
+ void smcr_port_err(struct smc_ib_device *smcibdev, u8 ibport)
+ {
+ 	struct smc_link_group *lgr, *n;
+ 	int i;
+ 
+ 	list_for_each_entry_safe(lgr, n, &smc_lgr_list.list, list) {
+ 		if (strncmp(smcibdev->pnetid[ibport - 1], lgr->pnet_id,
+ 			    SMC_MAX_PNETID_LEN))
+ 			continue; /* lgr is not affected */
+ 		if (list_empty(&lgr->list))
+ 			continue;
+ 		for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 			struct smc_link *lnk = &lgr->lnk[i];
+ 
+ 			if (smc_link_usable(lnk) &&
+ 			    lnk->smcibdev == smcibdev && lnk->ibport == ibport)
+ 				smcr_link_down_cond_sched(lnk);
+ 		}
+ 	}
+ }
+ 
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
  static void smc_link_up_work(struct work_struct *work)
  {
  	struct smc_ib_up_work *ib_work = container_of(work,
@@@ -998,6 -1260,20 +1084,23 @@@ out
  	kfree(ib_work);
  }
  
++<<<<<<< HEAD
++=======
+ static void smc_link_down_work(struct work_struct *work)
+ {
+ 	struct smc_link *link = container_of(work, struct smc_link,
+ 					     link_down_wrk);
+ 	struct smc_link_group *lgr = link->lgr;
+ 
+ 	if (list_empty(&lgr->list))
+ 		return;
+ 	wake_up_all(&lgr->llc_msg_waiter);
+ 	mutex_lock(&lgr->llc_conf_mutex);
+ 	smcr_link_down(link);
+ 	mutex_unlock(&lgr->llc_conf_mutex);
+ }
+ 
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
  /* Determine vlan of internal TCP socket.
   * @vlan_id: address to store the determined vlan id into
   */
diff --cc net/smc/smc_core.h
index dc42fae85777,c3ff512fd891..000000000000
--- a/net/smc/smc_core.h
+++ b/net/smc/smc_core.h
@@@ -233,6 -248,36 +233,39 @@@ struct smc_link_group 
  			DECLARE_BITMAP(rtokens_used_mask, SMC_RMBS_PER_LGR_MAX);
  						/* used rtoken elements */
  			u8			next_link_id;
++<<<<<<< HEAD
++=======
+ 			enum smc_lgr_type	type;
+ 						/* redundancy state */
+ 			u8			pnet_id[SMC_MAX_PNETID_LEN + 1];
+ 						/* pnet id of this lgr */
+ 			struct list_head	llc_event_q;
+ 						/* queue for llc events */
+ 			spinlock_t		llc_event_q_lock;
+ 						/* protects llc_event_q */
+ 			struct mutex		llc_conf_mutex;
+ 						/* protects lgr reconfig. */
+ 			struct work_struct	llc_add_link_work;
+ 			struct work_struct	llc_del_link_work;
+ 			struct work_struct	llc_event_work;
+ 						/* llc event worker */
+ 			wait_queue_head_t	llc_flow_waiter;
+ 						/* w4 next llc event */
+ 			wait_queue_head_t	llc_msg_waiter;
+ 						/* w4 next llc msg */
+ 			struct smc_llc_flow	llc_flow_lcl;
+ 						/* llc local control field */
+ 			struct smc_llc_flow	llc_flow_rmt;
+ 						/* llc remote control field */
+ 			struct smc_llc_qentry	*delayed_event;
+ 						/* arrived when flow active */
+ 			spinlock_t		llc_flow_lock;
+ 						/* protects llc flow */
+ 			int			llc_testlink_time;
+ 						/* link keep alive time */
+ 			u32			llc_termination_rsn;
+ 						/* rsn code for termination */
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
  		};
  		struct { /* SMC-D */
  			u64			peer_gid;
diff --cc net/smc/smc_llc.c
index 4119cdb6b6bf,df164232574b..000000000000
--- a/net/smc/smc_llc.c
+++ b/net/smc/smc_llc.c
@@@ -134,6 -153,181 +134,184 @@@ union smc_llc_msg 
  
  #define SMC_LLC_FLAG_RESP		0x80
  
++<<<<<<< HEAD
++=======
+ struct smc_llc_qentry {
+ 	struct list_head list;
+ 	struct smc_link *link;
+ 	union smc_llc_msg msg;
+ };
+ 
+ static void smc_llc_enqueue(struct smc_link *link, union smc_llc_msg *llc);
+ 
+ struct smc_llc_qentry *smc_llc_flow_qentry_clr(struct smc_llc_flow *flow)
+ {
+ 	struct smc_llc_qentry *qentry = flow->qentry;
+ 
+ 	flow->qentry = NULL;
+ 	return qentry;
+ }
+ 
+ void smc_llc_flow_qentry_del(struct smc_llc_flow *flow)
+ {
+ 	struct smc_llc_qentry *qentry;
+ 
+ 	if (flow->qentry) {
+ 		qentry = flow->qentry;
+ 		flow->qentry = NULL;
+ 		kfree(qentry);
+ 	}
+ }
+ 
+ static inline void smc_llc_flow_qentry_set(struct smc_llc_flow *flow,
+ 					   struct smc_llc_qentry *qentry)
+ {
+ 	flow->qentry = qentry;
+ }
+ 
+ static void smc_llc_flow_parallel(struct smc_link_group *lgr, u8 flow_type,
+ 				  struct smc_llc_qentry *qentry)
+ {
+ 	u8 msg_type = qentry->msg.raw.hdr.common.type;
+ 
+ 	if ((msg_type == SMC_LLC_ADD_LINK || msg_type == SMC_LLC_DELETE_LINK) &&
+ 	    flow_type != msg_type && !lgr->delayed_event) {
+ 		lgr->delayed_event = qentry;
+ 		return;
+ 	}
+ 	/* drop parallel or already-in-progress llc requests */
+ 	if (flow_type != msg_type)
+ 		pr_warn_once("smc: SMC-R lg %*phN dropped parallel "
+ 			     "LLC msg: msg %d flow %d role %d\n",
+ 			     SMC_LGR_ID_SIZE, &lgr->id,
+ 			     qentry->msg.raw.hdr.common.type,
+ 			     flow_type, lgr->role);
+ 	kfree(qentry);
+ }
+ 
+ /* try to start a new llc flow, initiated by an incoming llc msg */
+ static bool smc_llc_flow_start(struct smc_llc_flow *flow,
+ 			       struct smc_llc_qentry *qentry)
+ {
+ 	struct smc_link_group *lgr = qentry->link->lgr;
+ 
+ 	spin_lock_bh(&lgr->llc_flow_lock);
+ 	if (flow->type) {
+ 		/* a flow is already active */
+ 		smc_llc_flow_parallel(lgr, flow->type, qentry);
+ 		spin_unlock_bh(&lgr->llc_flow_lock);
+ 		return false;
+ 	}
+ 	switch (qentry->msg.raw.hdr.common.type) {
+ 	case SMC_LLC_ADD_LINK:
+ 		flow->type = SMC_LLC_FLOW_ADD_LINK;
+ 		break;
+ 	case SMC_LLC_DELETE_LINK:
+ 		flow->type = SMC_LLC_FLOW_DEL_LINK;
+ 		break;
+ 	case SMC_LLC_CONFIRM_RKEY:
+ 	case SMC_LLC_DELETE_RKEY:
+ 		flow->type = SMC_LLC_FLOW_RKEY;
+ 		break;
+ 	default:
+ 		flow->type = SMC_LLC_FLOW_NONE;
+ 	}
+ 	if (qentry == lgr->delayed_event)
+ 		lgr->delayed_event = NULL;
+ 	smc_llc_flow_qentry_set(flow, qentry);
+ 	spin_unlock_bh(&lgr->llc_flow_lock);
+ 	return true;
+ }
+ 
+ /* start a new local llc flow, wait till current flow finished */
+ int smc_llc_flow_initiate(struct smc_link_group *lgr,
+ 			  enum smc_llc_flowtype type)
+ {
+ 	enum smc_llc_flowtype allowed_remote = SMC_LLC_FLOW_NONE;
+ 	int rc;
+ 
+ 	/* all flows except confirm_rkey and delete_rkey are exclusive,
+ 	 * confirm/delete rkey flows can run concurrently (local and remote)
+ 	 */
+ 	if (type == SMC_LLC_FLOW_RKEY)
+ 		allowed_remote = SMC_LLC_FLOW_RKEY;
+ again:
+ 	if (list_empty(&lgr->list))
+ 		return -ENODEV;
+ 	spin_lock_bh(&lgr->llc_flow_lock);
+ 	if (lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE &&
+ 	    (lgr->llc_flow_rmt.type == SMC_LLC_FLOW_NONE ||
+ 	     lgr->llc_flow_rmt.type == allowed_remote)) {
+ 		lgr->llc_flow_lcl.type = type;
+ 		spin_unlock_bh(&lgr->llc_flow_lock);
+ 		return 0;
+ 	}
+ 	spin_unlock_bh(&lgr->llc_flow_lock);
+ 	rc = wait_event_timeout(lgr->llc_flow_waiter, (list_empty(&lgr->list) ||
+ 				(lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE &&
+ 				 (lgr->llc_flow_rmt.type == SMC_LLC_FLOW_NONE ||
+ 				  lgr->llc_flow_rmt.type == allowed_remote))),
+ 				SMC_LLC_WAIT_TIME * 10);
+ 	if (!rc)
+ 		return -ETIMEDOUT;
+ 	goto again;
+ }
+ 
+ /* finish the current llc flow */
+ void smc_llc_flow_stop(struct smc_link_group *lgr, struct smc_llc_flow *flow)
+ {
+ 	spin_lock_bh(&lgr->llc_flow_lock);
+ 	memset(flow, 0, sizeof(*flow));
+ 	flow->type = SMC_LLC_FLOW_NONE;
+ 	spin_unlock_bh(&lgr->llc_flow_lock);
+ 	if (!list_empty(&lgr->list) && lgr->delayed_event &&
+ 	    flow == &lgr->llc_flow_lcl)
+ 		schedule_work(&lgr->llc_event_work);
+ 	else
+ 		wake_up(&lgr->llc_flow_waiter);
+ }
+ 
+ /* lnk is optional and used for early wakeup when link goes down, useful in
+  * cases where we wait for a response on the link after we sent a request
+  */
+ struct smc_llc_qentry *smc_llc_wait(struct smc_link_group *lgr,
+ 				    struct smc_link *lnk,
+ 				    int time_out, u8 exp_msg)
+ {
+ 	struct smc_llc_flow *flow = &lgr->llc_flow_lcl;
+ 	u8 rcv_msg;
+ 
+ 	wait_event_timeout(lgr->llc_msg_waiter,
+ 			   (flow->qentry ||
+ 			    (lnk && !smc_link_usable(lnk)) ||
+ 			    list_empty(&lgr->list)),
+ 			   time_out);
+ 	if (!flow->qentry ||
+ 	    (lnk && !smc_link_usable(lnk)) || list_empty(&lgr->list)) {
+ 		smc_llc_flow_qentry_del(flow);
+ 		goto out;
+ 	}
+ 	rcv_msg = flow->qentry->msg.raw.hdr.common.type;
+ 	if (exp_msg && rcv_msg != exp_msg) {
+ 		if (exp_msg == SMC_LLC_ADD_LINK &&
+ 		    rcv_msg == SMC_LLC_DELETE_LINK) {
+ 			/* flow_start will delay the unexpected msg */
+ 			smc_llc_flow_start(&lgr->llc_flow_lcl,
+ 					   smc_llc_flow_qentry_clr(flow));
+ 			return NULL;
+ 		}
+ 		pr_warn_once("smc: SMC-R lg %*phN dropped unexpected LLC msg: "
+ 			     "msg %d exp %d flow %d role %d flags %x\n",
+ 			     SMC_LGR_ID_SIZE, &lgr->id, rcv_msg, exp_msg,
+ 			     flow->type, lgr->role,
+ 			     flow->qentry->msg.raw.hdr.flags);
+ 		smc_llc_flow_qentry_del(flow);
+ 	}
+ out:
+ 	return flow->qentry;
+ }
+ 
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
  /********************************** send *************************************/
  
  struct smc_llc_tx_pend {
@@@ -372,187 -588,1057 +550,410 @@@ static void smc_llc_send_message_work(s
  	struct smc_wr_buf *wr_buf;
  	int rc;
  
 -	if (!smc_link_usable(link))
 -		return -ENOLINK;
 -	rc = smc_llc_add_pending_send(link, &wr_buf, &pend);
 +	if (llcwrk->link->state == SMC_LNK_INACTIVE)
 +		goto out;
 +	rc = smc_llc_add_pending_send(llcwrk->link, &wr_buf, &pend);
  	if (rc)
 -		return rc;
 -	memcpy(wr_buf, llcbuf, sizeof(union smc_llc_msg));
 -	return smc_wr_tx_send_wait(link, pend, SMC_LLC_WAIT_TIME);
 -}
 -
 -/********************************* receive ***********************************/
 -
 -static int smc_llc_alloc_alt_link(struct smc_link_group *lgr,
 -				  enum smc_lgr_type lgr_new_t)
 -{
 -	int i;
 -
 -	if (lgr->type == SMC_LGR_SYMMETRIC ||
 -	    (lgr->type != SMC_LGR_SINGLE &&
 -	     (lgr_new_t == SMC_LGR_ASYMMETRIC_LOCAL ||
 -	      lgr_new_t == SMC_LGR_ASYMMETRIC_PEER)))
 -		return -EMLINK;
 -
 -	if (lgr_new_t == SMC_LGR_ASYMMETRIC_LOCAL ||
 -	    lgr_new_t == SMC_LGR_ASYMMETRIC_PEER) {
 -		for (i = SMC_LINKS_PER_LGR_MAX - 1; i >= 0; i--)
 -			if (lgr->lnk[i].state == SMC_LNK_UNUSED)
 -				return i;
 -	} else {
 -		for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++)
 -			if (lgr->lnk[i].state == SMC_LNK_UNUSED)
 -				return i;
 -	}
 -	return -EMLINK;
 -}
 -
 -/* return first buffer from any of the next buf lists */
 -static struct smc_buf_desc *_smc_llc_get_next_rmb(struct smc_link_group *lgr,
 -						  int *buf_lst)
 -{
 -	struct smc_buf_desc *buf_pos;
 -
 -	while (*buf_lst < SMC_RMBE_SIZES) {
 -		buf_pos = list_first_entry_or_null(&lgr->rmbs[*buf_lst],
 -						   struct smc_buf_desc, list);
 -		if (buf_pos)
 -			return buf_pos;
 -		(*buf_lst)++;
 -	}
 -	return NULL;
 +		goto out;
 +	memcpy(wr_buf, &llcwrk->llcbuf, llcwrk->llclen);
 +	smc_wr_tx_send(llcwrk->link, pend);
 +out:
 +	kfree(llcwrk);
  }
  
 -/* return next rmb from buffer lists */
 -static struct smc_buf_desc *smc_llc_get_next_rmb(struct smc_link_group *lgr,
 -						 int *buf_lst,
 -						 struct smc_buf_desc *buf_pos)
 +/* copy llcbuf and schedule an llc send on link */
 +static int smc_llc_send_message(struct smc_link *link, void *llcbuf, int llclen)
  {
 -	struct smc_buf_desc *buf_next;
 +	struct smc_llc_send_work *wrk = kmalloc(sizeof(*wrk), GFP_ATOMIC);
  
 -	if (!buf_pos || list_is_last(&buf_pos->list, &lgr->rmbs[*buf_lst])) {
 -		(*buf_lst)++;
 -		return _smc_llc_get_next_rmb(lgr, buf_lst);
 -	}
 -	buf_next = list_next_entry(buf_pos, list);
 -	return buf_next;
 +	if (!wrk)
 +		return -ENOMEM;
 +	INIT_WORK(&wrk->work, smc_llc_send_message_work);
 +	wrk->link = link;
 +	wrk->llclen = llclen;
 +	memcpy(&wrk->llcbuf, llcbuf, llclen);
 +	queue_work(link->llc_wq, &wrk->work);
 +	return 0;
  }
  
 -static struct smc_buf_desc *smc_llc_get_first_rmb(struct smc_link_group *lgr,
 -						  int *buf_lst)
 -{
 -	*buf_lst = 0;
 -	return smc_llc_get_next_rmb(lgr, buf_lst, NULL);
 -}
 +/********************************* receive ***********************************/
  
 -/* send one add_link_continue msg */
 -static int smc_llc_add_link_cont(struct smc_link *link,
 -				 struct smc_link *link_new, u8 *num_rkeys_todo,
 -				 int *buf_lst, struct smc_buf_desc **buf_pos)
 +static void smc_llc_rx_confirm_link(struct smc_link *link,
 +				    struct smc_llc_msg_confirm_link *llc)
  {
 -	struct smc_llc_msg_add_link_cont *addc_llc;
 -	struct smc_link_group *lgr = link->lgr;
 -	int prim_lnk_idx, lnk_idx, i, rc;
 -	struct smc_wr_tx_pend_priv *pend;
 -	struct smc_wr_buf *wr_buf;
 -	struct smc_buf_desc *rmb;
 -	u8 n;
 -
 -	rc = smc_llc_add_pending_send(link, &wr_buf, &pend);
 -	if (rc)
 -		return rc;
 -	addc_llc = (struct smc_llc_msg_add_link_cont *)wr_buf;
 -	memset(addc_llc, 0, sizeof(*addc_llc));
 -
 -	prim_lnk_idx = link->link_idx;
 -	lnk_idx = link_new->link_idx;
 -	addc_llc->link_num = link_new->link_id;
 -	addc_llc->num_rkeys = *num_rkeys_todo;
 -	n = *num_rkeys_todo;
 -	for (i = 0; i < min_t(u8, n, SMC_LLC_RKEYS_PER_CONT_MSG); i++) {
 -		if (!*buf_pos) {
 -			addc_llc->num_rkeys = addc_llc->num_rkeys -
 -					      *num_rkeys_todo;
 -			*num_rkeys_todo = 0;
 -			break;
 -		}
 -		rmb = *buf_pos;
 -
 -		addc_llc->rt[i].rmb_key = htonl(rmb->mr_rx[prim_lnk_idx]->rkey);
 -		addc_llc->rt[i].rmb_key_new = htonl(rmb->mr_rx[lnk_idx]->rkey);
 -		addc_llc->rt[i].rmb_vaddr_new =
 -			cpu_to_be64((u64)sg_dma_address(rmb->sgt[lnk_idx].sgl));
 +	struct smc_link_group *lgr = smc_get_lgr(link);
 +	int conf_rc;
  
 -		(*num_rkeys_todo)--;
 -		*buf_pos = smc_llc_get_next_rmb(lgr, buf_lst, *buf_pos);
 -		while (*buf_pos && !(*buf_pos)->used)
 -			*buf_pos = smc_llc_get_next_rmb(lgr, buf_lst, *buf_pos);
 -	}
 -	addc_llc->hd.common.type = SMC_LLC_ADD_LINK_CONT;
 -	addc_llc->hd.length = sizeof(struct smc_llc_msg_add_link_cont);
 -	if (lgr->role == SMC_CLNT)
 -		addc_llc->hd.flags |= SMC_LLC_FLAG_RESP;
 -	return smc_wr_tx_send(link, pend);
 -}
 +	/* RMBE eyecatchers are not supported */
 +	if (llc->hd.flags & SMC_LLC_FLAG_NO_RMBE_EYEC)
 +		conf_rc = 0;
 +	else
 +		conf_rc = ENOTSUPP;
  
 -static int smc_llc_cli_rkey_exchange(struct smc_link *link,
 -				     struct smc_link *link_new)
 -{
 -	struct smc_llc_msg_add_link_cont *addc_llc;
 -	struct smc_link_group *lgr = link->lgr;
 -	u8 max, num_rkeys_send, num_rkeys_recv;
 -	struct smc_llc_qentry *qentry;
 -	struct smc_buf_desc *buf_pos;
 -	int buf_lst;
 -	int rc = 0;
 -	int i;
 -
 -	mutex_lock(&lgr->rmbs_lock);
 -	num_rkeys_send = lgr->conns_num;
 -	buf_pos = smc_llc_get_first_rmb(lgr, &buf_lst);
 -	do {
 -		qentry = smc_llc_wait(lgr, NULL, SMC_LLC_WAIT_TIME,
 -				      SMC_LLC_ADD_LINK_CONT);
 -		if (!qentry) {
 -			rc = -ETIMEDOUT;
 -			break;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (lgr->role == SMC_SERV &&
 +		    link->state == SMC_LNK_ACTIVATING) {
 +			link->llc_confirm_resp_rc = conf_rc;
 +			complete(&link->llc_confirm_resp);
  		}
 -		addc_llc = &qentry->msg.add_link_cont;
 -		num_rkeys_recv = addc_llc->num_rkeys;
 -		max = min_t(u8, num_rkeys_recv, SMC_LLC_RKEYS_PER_CONT_MSG);
 -		for (i = 0; i < max; i++) {
 -			smc_rtoken_set(lgr, link->link_idx, link_new->link_idx,
 -				       addc_llc->rt[i].rmb_key,
 -				       addc_llc->rt[i].rmb_vaddr_new,
 -				       addc_llc->rt[i].rmb_key_new);
 -			num_rkeys_recv--;
 +	} else {
 +		if (lgr->role == SMC_CLNT &&
 +		    link->state == SMC_LNK_ACTIVATING) {
 +			link->llc_confirm_rc = conf_rc;
 +			link->link_id = llc->link_num;
 +			complete(&link->llc_confirm);
  		}
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		rc = smc_llc_add_link_cont(link, link_new, &num_rkeys_send,
 -					   &buf_lst, &buf_pos);
 -		if (rc)
 -			break;
 -	} while (num_rkeys_send || num_rkeys_recv);
 -
 -	mutex_unlock(&lgr->rmbs_lock);
 -	return rc;
 -}
 -
 -/* prepare and send an add link reject response */
 -static int smc_llc_cli_add_link_reject(struct smc_llc_qentry *qentry)
 -{
 -	qentry->msg.raw.hdr.flags |= SMC_LLC_FLAG_RESP;
 -	qentry->msg.raw.hdr.flags |= SMC_LLC_FLAG_ADD_LNK_REJ;
 -	qentry->msg.raw.hdr.add_link_rej_rsn = SMC_LLC_REJ_RSN_NO_ALT_PATH;
 -	return smc_llc_send_message(qentry->link, &qentry->msg);
 -}
 -
 -static int smc_llc_cli_conf_link(struct smc_link *link,
 -				 struct smc_init_info *ini,
 -				 struct smc_link *link_new,
 -				 enum smc_lgr_type lgr_new_t)
 -{
 -	struct smc_link_group *lgr = link->lgr;
 -	struct smc_llc_qentry *qentry = NULL;
 -	int rc = 0;
 -
 -	/* receive CONFIRM LINK request over RoCE fabric */
 -	qentry = smc_llc_wait(lgr, NULL, SMC_LLC_WAIT_FIRST_TIME, 0);
 -	if (!qentry) {
 -		rc = smc_llc_send_delete_link(link, link_new->link_id,
 -					      SMC_LLC_REQ, false,
 -					      SMC_LLC_DEL_LOST_PATH);
 -		return -ENOLINK;
  	}
 -	if (qentry->msg.raw.hdr.common.type != SMC_LLC_CONFIRM_LINK) {
 -		/* received DELETE_LINK instead */
 -		qentry->msg.raw.hdr.flags |= SMC_LLC_FLAG_RESP;
 -		smc_llc_send_message(link, &qentry->msg);
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		return -ENOLINK;
 -	}
 -	smc_llc_save_peer_uid(qentry);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -
 -	rc = smc_ib_modify_qp_rts(link_new);
 -	if (rc) {
 -		smc_llc_send_delete_link(link, link_new->link_id, SMC_LLC_REQ,
 -					 false, SMC_LLC_DEL_LOST_PATH);
 -		return -ENOLINK;
 -	}
 -	smc_wr_remember_qp_attr(link_new);
 -
 -	rc = smcr_buf_reg_lgr(link_new);
 -	if (rc) {
 -		smc_llc_send_delete_link(link, link_new->link_id, SMC_LLC_REQ,
 -					 false, SMC_LLC_DEL_LOST_PATH);
 -		return -ENOLINK;
 -	}
 -
 -	/* send CONFIRM LINK response over RoCE fabric */
 -	rc = smc_llc_send_confirm_link(link_new, SMC_LLC_RESP);
 -	if (rc) {
 -		smc_llc_send_delete_link(link, link_new->link_id, SMC_LLC_REQ,
 -					 false, SMC_LLC_DEL_LOST_PATH);
 -		return -ENOLINK;
 -	}
 -	smc_llc_link_active(link_new);
 -	if (lgr_new_t == SMC_LGR_ASYMMETRIC_LOCAL ||
 -	    lgr_new_t == SMC_LGR_ASYMMETRIC_PEER)
 -		smcr_lgr_set_type_asym(lgr, lgr_new_t, link_new->link_idx);
 -	else
 -		smcr_lgr_set_type(lgr, lgr_new_t);
 -	return 0;
 -}
 -
 -static void smc_llc_save_add_link_info(struct smc_link *link,
 -				       struct smc_llc_msg_add_link *add_llc)
 -{
 -	link->peer_qpn = ntoh24(add_llc->sender_qp_num);
 -	memcpy(link->peer_gid, add_llc->sender_gid, SMC_GID_SIZE);
 -	memcpy(link->peer_mac, add_llc->sender_mac, ETH_ALEN);
 -	link->peer_psn = ntoh24(add_llc->initial_psn);
 -	link->peer_mtu = add_llc->qp_mtu;
  }
  
 -/* as an SMC client, process an add link request */
 -int smc_llc_cli_add_link(struct smc_link *link, struct smc_llc_qentry *qentry)
 +static void smc_llc_rx_add_link(struct smc_link *link,
 +				struct smc_llc_msg_add_link *llc)
  {
 -	struct smc_llc_msg_add_link *llc = &qentry->msg.add_link;
 -	enum smc_lgr_type lgr_new_t = SMC_LGR_SYMMETRIC;
  	struct smc_link_group *lgr = smc_get_lgr(link);
 -	struct smc_link *lnk_new = NULL;
 -	struct smc_init_info ini;
 -	int lnk_idx, rc = 0;
 -
 -	ini.vlan_id = lgr->vlan_id;
 -	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
 -	if (!memcmp(llc->sender_gid, link->peer_gid, SMC_GID_SIZE) &&
 -	    !memcmp(llc->sender_mac, link->peer_mac, ETH_ALEN)) {
 -		if (!ini.ib_dev)
 -			goto out_reject;
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_PEER;
 -	}
 -	if (!ini.ib_dev) {
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_LOCAL;
 -		ini.ib_dev = link->smcibdev;
 -		ini.ib_port = link->ibport;
 -	}
 -	lnk_idx = smc_llc_alloc_alt_link(lgr, lgr_new_t);
 -	if (lnk_idx < 0)
 -		goto out_reject;
 -	lnk_new = &lgr->lnk[lnk_idx];
 -	rc = smcr_link_init(lgr, lnk_new, lnk_idx, &ini);
 -	if (rc)
 -		goto out_reject;
 -	smc_llc_save_add_link_info(lnk_new, llc);
 -	lnk_new->link_id = llc->link_num;	/* SMC server assigns link id */
 -	smc_llc_link_set_uid(lnk_new);
 -
 -	rc = smc_ib_ready_link(lnk_new);
 -	if (rc)
 -		goto out_clear_lnk;
 -
 -	rc = smcr_buf_map_lgr(lnk_new);
 -	if (rc)
 -		goto out_clear_lnk;
 -
 -	rc = smc_llc_send_add_link(link,
 -				   lnk_new->smcibdev->mac[ini.ib_port - 1],
 -				   lnk_new->gid, lnk_new, SMC_LLC_RESP);
 -	if (rc)
 -		goto out_clear_lnk;
 -	rc = smc_llc_cli_rkey_exchange(link, lnk_new);
 -	if (rc) {
 -		rc = 0;
 -		goto out_clear_lnk;
 -	}
 -	rc = smc_llc_cli_conf_link(link, &ini, lnk_new, lgr_new_t);
 -	if (!rc)
 -		goto out;
 -out_clear_lnk:
 -	smcr_link_clear(lnk_new, false);
 -out_reject:
 -	smc_llc_cli_add_link_reject(qentry);
 -out:
 -	kfree(qentry);
 -	return rc;
 -}
 -
 -static void smc_llc_process_cli_add_link(struct smc_link_group *lgr)
 -{
 -	struct smc_llc_qentry *qentry;
 -
 -	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
  
 -	mutex_lock(&lgr->llc_conf_mutex);
 -	smc_llc_cli_add_link(qentry->link, qentry);
 -	mutex_unlock(&lgr->llc_conf_mutex);
 -}
 -
 -static int smc_llc_active_link_count(struct smc_link_group *lgr)
 -{
 -	int i, link_count = 0;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (link->state == SMC_LNK_ACTIVATING)
 +			complete(&link->llc_add_resp);
 +	} else {
 +		if (link->state == SMC_LNK_ACTIVATING) {
 +			complete(&link->llc_add);
 +			return;
 +		}
  
 -	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
 -		if (!smc_link_usable(&lgr->lnk[i]))
 -			continue;
 -		link_count++;
 -	}
 -	return link_count;
 -}
 +		if (lgr->role == SMC_SERV) {
 +			smc_llc_prep_add_link(llc, link,
 +					link->smcibdev->mac[link->ibport - 1],
 +					link->gid, SMC_LLC_REQ);
  
 -/* find the asymmetric link when 3 links are established  */
 -static struct smc_link *smc_llc_find_asym_link(struct smc_link_group *lgr)
 -{
 -	int asym_idx = -ENOENT;
 -	int i, j, k;
 -	bool found;
 -
 -	/* determine asymmetric link */
 -	found = false;
 -	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
 -		for (j = i + 1; j < SMC_LINKS_PER_LGR_MAX; j++) {
 -			if (!smc_link_usable(&lgr->lnk[i]) ||
 -			    !smc_link_usable(&lgr->lnk[j]))
 -				continue;
 -			if (!memcmp(lgr->lnk[i].gid, lgr->lnk[j].gid,
 -				    SMC_GID_SIZE)) {
 -				found = true;	/* asym_lnk is i or j */
 -				break;
 -			}
 -		}
 -		if (found)
 -			break;
 -	}
 -	if (!found)
 -		goto out; /* no asymmetric link */
 -	for (k = 0; k < SMC_LINKS_PER_LGR_MAX; k++) {
 -		if (!smc_link_usable(&lgr->lnk[k]))
 -			continue;
 -		if (k != i &&
 -		    !memcmp(lgr->lnk[i].peer_gid, lgr->lnk[k].peer_gid,
 -			    SMC_GID_SIZE)) {
 -			asym_idx = i;
 -			break;
 -		}
 -		if (k != j &&
 -		    !memcmp(lgr->lnk[j].peer_gid, lgr->lnk[k].peer_gid,
 -			    SMC_GID_SIZE)) {
 -			asym_idx = j;
 -			break;
 +		} else {
 +			smc_llc_prep_add_link(llc, link,
 +					link->smcibdev->mac[link->ibport - 1],
 +					link->gid, SMC_LLC_RESP);
  		}
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -out:
 -	return (asym_idx < 0) ? NULL : &lgr->lnk[asym_idx];
  }
  
 -static void smc_llc_delete_asym_link(struct smc_link_group *lgr)
 +static void smc_llc_rx_delete_link(struct smc_link *link,
 +				   struct smc_llc_msg_del_link *llc)
  {
 -	struct smc_link *lnk_new = NULL, *lnk_asym;
 -	struct smc_llc_qentry *qentry;
 -	int rc;
 -
 -	lnk_asym = smc_llc_find_asym_link(lgr);
 -	if (!lnk_asym)
 -		return; /* no asymmetric link */
 -	if (!smc_link_downing(&lnk_asym->state))
 -		return;
 -	lnk_new = smc_switch_conns(lgr, lnk_asym, false);
 -	smc_wr_tx_wait_no_pending_sends(lnk_asym);
 -	if (!lnk_new)
 -		goto out_free;
 -	/* change flow type from ADD_LINK into DEL_LINK */
 -	lgr->llc_flow_lcl.type = SMC_LLC_FLOW_DEL_LINK;
 -	rc = smc_llc_send_delete_link(lnk_new, lnk_asym->link_id, SMC_LLC_REQ,
 -				      true, SMC_LLC_DEL_NO_ASYM_NEEDED);
 -	if (rc) {
 -		smcr_link_down_cond(lnk_new);
 -		goto out_free;
 -	}
 -	qentry = smc_llc_wait(lgr, lnk_new, SMC_LLC_WAIT_TIME,
 -			      SMC_LLC_DELETE_LINK);
 -	if (!qentry) {
 -		smcr_link_down_cond(lnk_new);
 -		goto out_free;
 -	}
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -out_free:
 -	smcr_link_clear(lnk_asym, true);
 -}
 +	struct smc_link_group *lgr = smc_get_lgr(link);
  
 -static int smc_llc_srv_rkey_exchange(struct smc_link *link,
 -				     struct smc_link *link_new)
 -{
 -	struct smc_llc_msg_add_link_cont *addc_llc;
 -	struct smc_link_group *lgr = link->lgr;
 -	u8 max, num_rkeys_send, num_rkeys_recv;
 -	struct smc_llc_qentry *qentry = NULL;
 -	struct smc_buf_desc *buf_pos;
 -	int buf_lst;
 -	int rc = 0;
 -	int i;
 -
 -	mutex_lock(&lgr->rmbs_lock);
 -	num_rkeys_send = lgr->conns_num;
 -	buf_pos = smc_llc_get_first_rmb(lgr, &buf_lst);
 -	do {
 -		smc_llc_add_link_cont(link, link_new, &num_rkeys_send,
 -				      &buf_lst, &buf_pos);
 -		qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_TIME,
 -				      SMC_LLC_ADD_LINK_CONT);
 -		if (!qentry) {
 -			rc = -ETIMEDOUT;
 -			goto out;
 -		}
 -		addc_llc = &qentry->msg.add_link_cont;
 -		num_rkeys_recv = addc_llc->num_rkeys;
 -		max = min_t(u8, num_rkeys_recv, SMC_LLC_RKEYS_PER_CONT_MSG);
 -		for (i = 0; i < max; i++) {
 -			smc_rtoken_set(lgr, link->link_idx, link_new->link_idx,
 -				       addc_llc->rt[i].rmb_key,
 -				       addc_llc->rt[i].rmb_vaddr_new,
 -				       addc_llc->rt[i].rmb_key_new);
 -			num_rkeys_recv--;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (lgr->role == SMC_SERV)
 +			smc_lgr_schedule_free_work_fast(lgr);
 +	} else {
 +		smc_lgr_forget(lgr);
 +		smc_llc_link_deleting(link);
 +		if (lgr->role == SMC_SERV) {
 +			/* client asks to delete this link, send request */
 +			smc_llc_prep_delete_link(llc, link, SMC_LLC_REQ, true);
 +		} else {
 +			/* server requests to delete this link, send response */
 +			smc_llc_prep_delete_link(llc, link, SMC_LLC_RESP, true);
  		}
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -	} while (num_rkeys_send || num_rkeys_recv);
 -out:
 -	mutex_unlock(&lgr->rmbs_lock);
 -	return rc;
 -}
 -
 -static int smc_llc_srv_conf_link(struct smc_link *link,
 -				 struct smc_link *link_new,
 -				 enum smc_lgr_type lgr_new_t)
 -{
 -	struct smc_link_group *lgr = link->lgr;
 -	struct smc_llc_qentry *qentry = NULL;
 -	int rc;
 -
 -	/* send CONFIRM LINK request over the RoCE fabric */
 -	rc = smc_llc_send_confirm_link(link_new, SMC_LLC_REQ);
 -	if (rc)
 -		return -ENOLINK;
 -	/* receive CONFIRM LINK response over the RoCE fabric */
 -	qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_FIRST_TIME,
 -			      SMC_LLC_CONFIRM_LINK);
 -	if (!qentry) {
 -		/* send DELETE LINK */
 -		smc_llc_send_delete_link(link, link_new->link_id, SMC_LLC_REQ,
 -					 false, SMC_LLC_DEL_LOST_PATH);
 -		return -ENOLINK;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
 +		smc_lgr_terminate_sched(lgr);
  	}
 -	smc_llc_save_peer_uid(qentry);
 -	smc_llc_link_active(link_new);
 -	if (lgr_new_t == SMC_LGR_ASYMMETRIC_LOCAL ||
 -	    lgr_new_t == SMC_LGR_ASYMMETRIC_PEER)
 -		smcr_lgr_set_type_asym(lgr, lgr_new_t, link_new->link_idx);
 -	else
 -		smcr_lgr_set_type(lgr, lgr_new_t);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -	return 0;
  }
  
 -int smc_llc_srv_add_link(struct smc_link *link)
 +static void smc_llc_rx_test_link(struct smc_link *link,
 +				 struct smc_llc_msg_test_link *llc)
  {
 -	enum smc_lgr_type lgr_new_t = SMC_LGR_SYMMETRIC;
 -	struct smc_link_group *lgr = link->lgr;
 -	struct smc_llc_msg_add_link *add_llc;
 -	struct smc_llc_qentry *qentry = NULL;
 -	struct smc_link *link_new;
 -	struct smc_init_info ini;
 -	int lnk_idx, rc = 0;
 -
 -	/* ignore client add link recommendation, start new flow */
 -	ini.vlan_id = lgr->vlan_id;
 -	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
 -	if (!ini.ib_dev) {
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_LOCAL;
 -		ini.ib_dev = link->smcibdev;
 -		ini.ib_port = link->ibport;
 -	}
 -	lnk_idx = smc_llc_alloc_alt_link(lgr, lgr_new_t);
 -	if (lnk_idx < 0)
 -		return 0;
 -
 -	rc = smcr_link_init(lgr, &lgr->lnk[lnk_idx], lnk_idx, &ini);
 -	if (rc)
 -		return rc;
 -	link_new = &lgr->lnk[lnk_idx];
 -	rc = smc_llc_send_add_link(link,
 -				   link_new->smcibdev->mac[ini.ib_port - 1],
 -				   link_new->gid, link_new, SMC_LLC_REQ);
 -	if (rc)
 -		goto out_err;
 -	/* receive ADD LINK response over the RoCE fabric */
 -	qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_TIME, SMC_LLC_ADD_LINK);
 -	if (!qentry) {
 -		rc = -ETIMEDOUT;
 -		goto out_err;
 -	}
 -	add_llc = &qentry->msg.add_link;
 -	if (add_llc->hd.flags & SMC_LLC_FLAG_ADD_LNK_REJ) {
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		rc = -ENOLINK;
 -		goto out_err;
 -	}
 -	if (lgr->type == SMC_LGR_SINGLE &&
 -	    (!memcmp(add_llc->sender_gid, link->peer_gid, SMC_GID_SIZE) &&
 -	     !memcmp(add_llc->sender_mac, link->peer_mac, ETH_ALEN))) {
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_PEER;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (link->state == SMC_LNK_ACTIVE)
 +			complete(&link->llc_testlink_resp);
 +	} else {
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -	smc_llc_save_add_link_info(link_new, add_llc);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -
 -	rc = smc_ib_ready_link(link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smcr_buf_map_lgr(link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smcr_buf_reg_lgr(link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smc_llc_srv_rkey_exchange(link, link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smc_llc_srv_conf_link(link, link_new, lgr_new_t);
 -	if (rc)
 -		goto out_err;
 -	return 0;
 -out_err:
 -	smcr_link_clear(link_new, false);
 -	return rc;
  }
  
 -static void smc_llc_process_srv_add_link(struct smc_link_group *lgr)
 +static void smc_llc_rx_confirm_rkey(struct smc_link *link,
 +				    struct smc_llc_msg_confirm_rkey *llc)
  {
 -	struct smc_link *link = lgr->llc_flow_lcl.qentry->link;
  	int rc;
  
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -
 -	mutex_lock(&lgr->llc_conf_mutex);
 -	rc = smc_llc_srv_add_link(link);
 -	if (!rc && lgr->type == SMC_LGR_SYMMETRIC) {
 -		/* delete any asymmetric link */
 -		smc_llc_delete_asym_link(lgr);
 -	}
 -	mutex_unlock(&lgr->llc_conf_mutex);
 -}
 -
 -/* enqueue a local add_link req to trigger a new add_link flow, only as SERV */
 -void smc_llc_srv_add_link_local(struct smc_link *link)
 -{
 -	struct smc_llc_msg_add_link add_llc = {0};
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		link->llc_confirm_rkey_rc = llc->hd.flags &
 +					    SMC_LLC_FLAG_RKEY_NEG;
 +		complete(&link->llc_confirm_rkey);
 +	} else {
 +		rc = smc_rtoken_add(link,
 +				    llc->rtoken[0].rmb_vaddr,
 +				    llc->rtoken[0].rmb_key);
  
 -	add_llc.hd.length = sizeof(add_llc);
 -	add_llc.hd.common.type = SMC_LLC_ADD_LINK;
 -	/* no dev and port needed, we as server ignore client data anyway */
 -	smc_llc_enqueue(link, (union smc_llc_msg *)&add_llc);
 -}
 +		/* ignore rtokens for other links, we have only one link */
  
 -/* worker to process an add link message */
 -static void smc_llc_add_link_work(struct work_struct *work)
 -{
 -	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
 -						  llc_add_link_work);
 -
 -	if (list_empty(&lgr->list)) {
 -		/* link group is terminating */
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		goto out;
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		if (rc < 0)
 +			llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -
 -	if (lgr->role == SMC_CLNT)
 -		smc_llc_process_cli_add_link(lgr);
 -	else
 -		smc_llc_process_srv_add_link(lgr);
 -out:
 -	smc_llc_flow_stop(lgr, &lgr->llc_flow_lcl);
  }
  
 -/* enqueue a local del_link msg to trigger a new del_link flow,
 - * called only for role SMC_SERV
 - */
 -void smc_llc_srv_delete_link_local(struct smc_link *link, u8 del_link_id)
 -{
 -	struct smc_llc_msg_del_link del_llc = {0};
 -
 -	del_llc.hd.length = sizeof(del_llc);
 -	del_llc.hd.common.type = SMC_LLC_DELETE_LINK;
 -	del_llc.link_num = del_link_id;
 -	del_llc.reason = htonl(SMC_LLC_DEL_LOST_PATH);
 -	del_llc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ORDERLY;
 -	smc_llc_enqueue(link, (union smc_llc_msg *)&del_llc);
 -}
 -
 -static void smc_llc_process_cli_delete_link(struct smc_link_group *lgr)
 +static void smc_llc_rx_confirm_rkey_cont(struct smc_link *link,
 +				      struct smc_llc_msg_confirm_rkey_cont *llc)
  {
 -	struct smc_link *lnk_del = NULL, *lnk_asym, *lnk;
 -	struct smc_llc_msg_del_link *del_llc;
 -	struct smc_llc_qentry *qentry;
 -	int active_links;
 -	int lnk_idx;
 -
 -	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
 -	lnk = qentry->link;
 -	del_llc = &qentry->msg.delete_link;
 -
 -	if (del_llc->hd.flags & SMC_LLC_FLAG_DEL_LINK_ALL) {
 -		smc_lgr_terminate_sched(lgr);
 -		goto out;
 -	}
 -	mutex_lock(&lgr->llc_conf_mutex);
 -	/* delete single link */
 -	for (lnk_idx = 0; lnk_idx < SMC_LINKS_PER_LGR_MAX; lnk_idx++) {
 -		if (lgr->lnk[lnk_idx].link_id != del_llc->link_num)
 -			continue;
 -		lnk_del = &lgr->lnk[lnk_idx];
 -		break;
 -	}
 -	del_llc->hd.flags |= SMC_LLC_FLAG_RESP;
 -	if (!lnk_del) {
 -		/* link was not found */
 -		del_llc->reason = htonl(SMC_LLC_DEL_NOLNK);
 -		smc_llc_send_message(lnk, &qentry->msg);
 -		goto out_unlock;
 -	}
 -	lnk_asym = smc_llc_find_asym_link(lgr);
 -
 -	del_llc->reason = 0;
 -	smc_llc_send_message(lnk, &qentry->msg); /* response */
 -
 -	if (smc_link_downing(&lnk_del->state)) {
 -		smc_switch_conns(lgr, lnk_del, false);
 -		smc_wr_tx_wait_no_pending_sends(lnk_del);
 -	}
 -	smcr_link_clear(lnk_del, true);
 -
 -	active_links = smc_llc_active_link_count(lgr);
 -	if (lnk_del == lnk_asym) {
 -		/* expected deletion of asym link, don't change lgr state */
 -	} else if (active_links == 1) {
 -		smcr_lgr_set_type(lgr, SMC_LGR_SINGLE);
 -	} else if (!active_links) {
 -		smcr_lgr_set_type(lgr, SMC_LGR_NONE);
 -		smc_lgr_terminate_sched(lgr);
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		/* unused as long as we don't send this type of msg */
 +	} else {
 +		/* ignore rtokens for other links, we have only one link */
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -out_unlock:
 -	mutex_unlock(&lgr->llc_conf_mutex);
 -out:
 -	kfree(qentry);
  }
  
 -/* try to send a DELETE LINK ALL request on any active link,
 - * waiting for send completion
 - */
 -void smc_llc_send_link_delete_all(struct smc_link_group *lgr, bool ord, u32 rsn)
 +static void smc_llc_rx_delete_rkey(struct smc_link *link,
 +				   struct smc_llc_msg_delete_rkey *llc)
  {
 -	struct smc_llc_msg_del_link delllc = {0};
 -	int i;
 -
 -	delllc.hd.common.type = SMC_LLC_DELETE_LINK;
 -	delllc.hd.length = sizeof(delllc);
 -	if (ord)
 -		delllc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ORDERLY;
 -	delllc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ALL;
 -	delllc.reason = htonl(rsn);
 -
 -	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
 -		if (!smc_link_usable(&lgr->lnk[i]))
 -			continue;
 -		if (!smc_llc_send_message_wait(&lgr->lnk[i], &delllc))
 -			break;
 -	}
 -}
 +	u8 err_mask = 0;
 +	int i, max;
  
 -static void smc_llc_process_srv_delete_link(struct smc_link_group *lgr)
 -{
 -	struct smc_llc_msg_del_link *del_llc;
 -	struct smc_link *lnk, *lnk_del;
 -	struct smc_llc_qentry *qentry;
 -	int active_links;
 -	int i;
 -
 -	mutex_lock(&lgr->llc_conf_mutex);
 -	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
 -	lnk = qentry->link;
 -	del_llc = &qentry->msg.delete_link;
 -
 -	if (qentry->msg.delete_link.hd.flags & SMC_LLC_FLAG_DEL_LINK_ALL) {
 -		/* delete entire lgr */
 -		smc_llc_send_link_delete_all(lgr, true, ntohl(
 -					      qentry->msg.delete_link.reason));
 -		smc_lgr_terminate_sched(lgr);
 -		goto out;
 -	}
 -	/* delete single link */
 -	lnk_del = NULL;
 -	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
 -		if (lgr->lnk[i].link_id == del_llc->link_num) {
 -			lnk_del = &lgr->lnk[i];
 -			break;
++<<<<<<< HEAD
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		link->llc_delete_rkey_rc = llc->hd.flags &
 +					    SMC_LLC_FLAG_RKEY_NEG;
 +		complete(&link->llc_delete_rkey);
 +	} else {
 +		max = min_t(u8, llc->num_rkeys, SMC_LLC_DEL_RKEY_MAX);
 +		for (i = 0; i < max; i++) {
 +			if (smc_rtoken_delete(link, llc->rkey[i]))
 +				err_mask |= 1 << (SMC_LLC_DEL_RKEY_MAX - 1 - i);
  		}
 -	}
 -	if (!lnk_del)
 -		goto out; /* asymmetric link already deleted */
  
 -	if (smc_link_downing(&lnk_del->state)) {
 -		smc_switch_conns(lgr, lnk_del, false);
 -		smc_wr_tx_wait_no_pending_sends(lnk_del);
 -	}
 -	if (!list_empty(&lgr->list)) {
 -		/* qentry is either a request from peer (send it back to
 -		 * initiate the DELETE_LINK processing), or a locally
 -		 * enqueued DELETE_LINK request (forward it)
 -		 */
 -		if (!smc_llc_send_message(lnk, &qentry->msg)) {
 -			struct smc_llc_qentry *qentry2;
 -
 -			qentry2 = smc_llc_wait(lgr, lnk, SMC_LLC_WAIT_TIME,
 -					       SMC_LLC_DELETE_LINK);
 -			if (qentry2)
 -				smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 +		if (err_mask) {
 +			llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 +			llc->err_mask = err_mask;
  		}
 -	}
 -	smcr_link_clear(lnk_del, true);
 -
 -	active_links = smc_llc_active_link_count(lgr);
 -	if (active_links == 1) {
 -		smcr_lgr_set_type(lgr, SMC_LGR_SINGLE);
 -	} else if (!active_links) {
 -		smcr_lgr_set_type(lgr, SMC_LGR_NONE);
 -		smc_lgr_terminate_sched(lgr);
 -	}
 -
 -	if (lgr->type == SMC_LGR_SINGLE && !list_empty(&lgr->list)) {
 -		/* trigger setup of asymm alt link */
 -		smc_llc_srv_add_link_local(lnk);
 -	}
 -out:
 -	mutex_unlock(&lgr->llc_conf_mutex);
 -	kfree(qentry);
 -}
 -
 -static void smc_llc_delete_link_work(struct work_struct *work)
 -{
 -	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
 -						  llc_del_link_work);
 -
 -	if (list_empty(&lgr->list)) {
 -		/* link group is terminating */
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		goto out;
 -	}
 -
 -	if (lgr->role == SMC_CLNT)
 -		smc_llc_process_cli_delete_link(lgr);
 -	else
 -		smc_llc_process_srv_delete_link(lgr);
 -out:
 -	smc_llc_flow_stop(lgr, &lgr->llc_flow_lcl);
 -}
 -
 -/* process a confirm_rkey request from peer, remote flow */
 -static void smc_llc_rmt_conf_rkey(struct smc_link_group *lgr)
 -{
 -	struct smc_llc_msg_confirm_rkey *llc;
 -	struct smc_llc_qentry *qentry;
 -	struct smc_link *link;
 -	int num_entries;
 -	int rk_idx;
 -	int i;
 -
 -	qentry = lgr->llc_flow_rmt.qentry;
 -	llc = &qentry->msg.confirm_rkey;
 -	link = qentry->link;
 -
 -	num_entries = llc->rtoken[0].num_rkeys;
 -	/* first rkey entry is for receiving link */
 -	rk_idx = smc_rtoken_add(link,
 -				llc->rtoken[0].rmb_vaddr,
 -				llc->rtoken[0].rmb_key);
 -	if (rk_idx < 0)
 -		goto out_err;
 -
 -	for (i = 1; i <= min_t(u8, num_entries, SMC_LLC_RKEYS_PER_MSG - 1); i++)
 -		smc_rtoken_set2(lgr, rk_idx, llc->rtoken[i].link_id,
 -				llc->rtoken[i].rmb_vaddr,
 -				llc->rtoken[i].rmb_key);
 -	/* max links is 3 so there is no need to support conf_rkey_cont msgs */
 -	goto out;
 -out_err:
 -	llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 -	llc->hd.flags |= SMC_LLC_FLAG_RKEY_RETRY;
 -out:
 -	llc->hd.flags |= SMC_LLC_FLAG_RESP;
 -	smc_llc_send_message(link, &qentry->msg);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_rmt);
 -}
 -
 -/* process a delete_rkey request from peer, remote flow */
 -static void smc_llc_rmt_delete_rkey(struct smc_link_group *lgr)
 -{
 -	struct smc_llc_msg_delete_rkey *llc;
 -	struct smc_llc_qentry *qentry;
 -	struct smc_link *link;
 -	u8 err_mask = 0;
 -	int i, max;
  
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
++=======
+ 	qentry = lgr->llc_flow_rmt.qentry;
+ 	llc = &qentry->msg.delete_rkey;
+ 	link = qentry->link;
+ 
+ 	max = min_t(u8, llc->num_rkeys, SMC_LLC_DEL_RKEY_MAX);
+ 	for (i = 0; i < max; i++) {
+ 		if (smc_rtoken_delete(link, llc->rkey[i]))
+ 			err_mask |= 1 << (SMC_LLC_DEL_RKEY_MAX - 1 - i);
+ 	}
+ 	if (err_mask) {
+ 		llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
+ 		llc->err_mask = err_mask;
+ 	}
+ 	llc->hd.flags |= SMC_LLC_FLAG_RESP;
+ 	smc_llc_send_message(link, &qentry->msg);
+ 	smc_llc_flow_qentry_del(&lgr->llc_flow_rmt);
+ }
+ 
+ static void smc_llc_protocol_violation(struct smc_link_group *lgr, u8 type)
+ {
+ 	pr_warn_ratelimited("smc: SMC-R lg %*phN LLC protocol violation: "
+ 			    "llc_type %d\n", SMC_LGR_ID_SIZE, &lgr->id, type);
+ 	smc_llc_set_termination_rsn(lgr, SMC_LLC_DEL_PROT_VIOL);
+ 	smc_lgr_terminate_sched(lgr);
+ }
+ 
+ /* flush the llc event queue */
+ static void smc_llc_event_flush(struct smc_link_group *lgr)
+ {
+ 	struct smc_llc_qentry *qentry, *q;
+ 
+ 	spin_lock_bh(&lgr->llc_event_q_lock);
+ 	list_for_each_entry_safe(qentry, q, &lgr->llc_event_q, list) {
+ 		list_del_init(&qentry->list);
+ 		kfree(qentry);
+ 	}
+ 	spin_unlock_bh(&lgr->llc_event_q_lock);
+ }
+ 
+ static void smc_llc_event_handler(struct smc_llc_qentry *qentry)
+ {
+ 	union smc_llc_msg *llc = &qentry->msg;
+ 	struct smc_link *link = qentry->link;
+ 	struct smc_link_group *lgr = link->lgr;
+ 
+ 	if (!smc_link_usable(link))
+ 		goto out;
+ 
+ 	switch (llc->raw.hdr.common.type) {
+ 	case SMC_LLC_TEST_LINK:
+ 		llc->test_link.hd.flags |= SMC_LLC_FLAG_RESP;
+ 		smc_llc_send_message(link, llc);
+ 		break;
+ 	case SMC_LLC_ADD_LINK:
+ 		if (list_empty(&lgr->list))
+ 			goto out;	/* lgr is terminating */
+ 		if (lgr->role == SMC_CLNT) {
+ 			if (lgr->llc_flow_lcl.type == SMC_LLC_FLOW_ADD_LINK) {
+ 				/* a flow is waiting for this message */
+ 				smc_llc_flow_qentry_set(&lgr->llc_flow_lcl,
+ 							qentry);
+ 				wake_up(&lgr->llc_msg_waiter);
+ 			} else if (smc_llc_flow_start(&lgr->llc_flow_lcl,
+ 						      qentry)) {
+ 				schedule_work(&lgr->llc_add_link_work);
+ 			}
+ 		} else if (smc_llc_flow_start(&lgr->llc_flow_lcl, qentry)) {
+ 			/* as smc server, handle client suggestion */
+ 			schedule_work(&lgr->llc_add_link_work);
+ 		}
+ 		return;
+ 	case SMC_LLC_CONFIRM_LINK:
+ 	case SMC_LLC_ADD_LINK_CONT:
+ 		if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
+ 			/* a flow is waiting for this message */
+ 			smc_llc_flow_qentry_set(&lgr->llc_flow_lcl, qentry);
+ 			wake_up(&lgr->llc_msg_waiter);
+ 			return;
+ 		}
+ 		break;
+ 	case SMC_LLC_DELETE_LINK:
+ 		if (lgr->role == SMC_CLNT) {
+ 			/* server requests to delete this link, send response */
+ 			if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
+ 				/* DEL LINK REQ during ADD LINK SEQ */
+ 				smc_llc_flow_qentry_set(&lgr->llc_flow_lcl,
+ 							qentry);
+ 				wake_up(&lgr->llc_msg_waiter);
+ 			} else if (smc_llc_flow_start(&lgr->llc_flow_lcl,
+ 						      qentry)) {
+ 				schedule_work(&lgr->llc_del_link_work);
+ 			}
+ 		} else {
+ 			if (lgr->llc_flow_lcl.type == SMC_LLC_FLOW_ADD_LINK &&
+ 			    !lgr->llc_flow_lcl.qentry) {
+ 				/* DEL LINK REQ during ADD LINK SEQ */
+ 				smc_llc_flow_qentry_set(&lgr->llc_flow_lcl,
+ 							qentry);
+ 				wake_up(&lgr->llc_msg_waiter);
+ 			} else if (smc_llc_flow_start(&lgr->llc_flow_lcl,
+ 						      qentry)) {
+ 				schedule_work(&lgr->llc_del_link_work);
+ 			}
+ 		}
+ 		return;
+ 	case SMC_LLC_CONFIRM_RKEY:
+ 		/* new request from remote, assign to remote flow */
+ 		if (smc_llc_flow_start(&lgr->llc_flow_rmt, qentry)) {
+ 			/* process here, does not wait for more llc msgs */
+ 			smc_llc_rmt_conf_rkey(lgr);
+ 			smc_llc_flow_stop(lgr, &lgr->llc_flow_rmt);
+ 		}
+ 		return;
+ 	case SMC_LLC_CONFIRM_RKEY_CONT:
+ 		/* not used because max links is 3, and 3 rkeys fit into
+ 		 * one CONFIRM_RKEY message
+ 		 */
+ 		break;
+ 	case SMC_LLC_DELETE_RKEY:
+ 		/* new request from remote, assign to remote flow */
+ 		if (smc_llc_flow_start(&lgr->llc_flow_rmt, qentry)) {
+ 			/* process here, does not wait for more llc msgs */
+ 			smc_llc_rmt_delete_rkey(lgr);
+ 			smc_llc_flow_stop(lgr, &lgr->llc_flow_rmt);
+ 		}
+ 		return;
+ 	default:
+ 		smc_llc_protocol_violation(lgr, llc->raw.hdr.common.type);
+ 		break;
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
+ 	}
 -out:
 -	kfree(qentry);
+ }
+ 
++<<<<<<< HEAD
++=======
+ /* worker to process llc messages on the event queue */
+ static void smc_llc_event_work(struct work_struct *work)
+ {
+ 	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
+ 						  llc_event_work);
+ 	struct smc_llc_qentry *qentry;
+ 
+ 	if (!lgr->llc_flow_lcl.type && lgr->delayed_event) {
+ 		if (smc_link_usable(lgr->delayed_event->link)) {
+ 			smc_llc_event_handler(lgr->delayed_event);
+ 		} else {
+ 			qentry = lgr->delayed_event;
+ 			lgr->delayed_event = NULL;
+ 			kfree(qentry);
+ 		}
+ 	}
+ 
+ again:
+ 	spin_lock_bh(&lgr->llc_event_q_lock);
+ 	if (!list_empty(&lgr->llc_event_q)) {
+ 		qentry = list_first_entry(&lgr->llc_event_q,
+ 					  struct smc_llc_qentry, list);
+ 		list_del_init(&qentry->list);
+ 		spin_unlock_bh(&lgr->llc_event_q_lock);
+ 		smc_llc_event_handler(qentry);
+ 		goto again;
  	}
+ 	spin_unlock_bh(&lgr->llc_event_q_lock);
  }
  
+ /* process llc responses in tasklet context */
+ static void smc_llc_rx_response(struct smc_link *link,
+ 				struct smc_llc_qentry *qentry)
+ {
+ 	u8 llc_type = qentry->msg.raw.hdr.common.type;
+ 
+ 	switch (llc_type) {
+ 	case SMC_LLC_TEST_LINK:
+ 		if (link->state == SMC_LNK_ACTIVE)
+ 			complete(&link->llc_testlink_resp);
+ 		break;
+ 	case SMC_LLC_ADD_LINK:
+ 	case SMC_LLC_DELETE_LINK:
+ 	case SMC_LLC_CONFIRM_LINK:
+ 	case SMC_LLC_ADD_LINK_CONT:
+ 	case SMC_LLC_CONFIRM_RKEY:
+ 	case SMC_LLC_DELETE_RKEY:
+ 		/* assign responses to the local flow, we requested them */
+ 		smc_llc_flow_qentry_set(&link->lgr->llc_flow_lcl, qentry);
+ 		wake_up(&link->lgr->llc_msg_waiter);
+ 		return;
+ 	case SMC_LLC_CONFIRM_RKEY_CONT:
+ 		/* not used because max links is 3 */
+ 		break;
+ 	default:
+ 		smc_llc_protocol_violation(link->lgr, llc_type);
+ 		break;
+ 	}
+ 	kfree(qentry);
+ }
+ 
+ static void smc_llc_enqueue(struct smc_link *link, union smc_llc_msg *llc)
+ {
+ 	struct smc_link_group *lgr = link->lgr;
+ 	struct smc_llc_qentry *qentry;
+ 	unsigned long flags;
+ 
+ 	qentry = kmalloc(sizeof(*qentry), GFP_ATOMIC);
+ 	if (!qentry)
+ 		return;
+ 	qentry->link = link;
+ 	INIT_LIST_HEAD(&qentry->list);
+ 	memcpy(&qentry->msg, llc, sizeof(union smc_llc_msg));
+ 
+ 	/* process responses immediately */
+ 	if (llc->raw.hdr.flags & SMC_LLC_FLAG_RESP) {
+ 		smc_llc_rx_response(link, qentry);
+ 		return;
+ 	}
+ 
+ 	/* add requests to event queue */
+ 	spin_lock_irqsave(&lgr->llc_event_q_lock, flags);
+ 	list_add_tail(&qentry->list, &lgr->llc_event_q);
+ 	spin_unlock_irqrestore(&lgr->llc_event_q_lock, flags);
+ 	schedule_work(&lgr->llc_event_work);
+ }
+ 
+ /* copy received msg and add it to the event queue */
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
  static void smc_llc_rx_handler(struct ib_wc *wc, void *buf)
  {
  	struct smc_link *link = (struct smc_link *)wc->qp->qp_context;
@@@ -624,21 -1686,39 +1025,55 @@@ out
  	schedule_delayed_work(&link->llc_testlink_wrk, next_interval);
  }
  
++<<<<<<< HEAD
++=======
+ void smc_llc_lgr_init(struct smc_link_group *lgr, struct smc_sock *smc)
+ {
+ 	struct net *net = sock_net(smc->clcsock->sk);
+ 
+ 	INIT_WORK(&lgr->llc_event_work, smc_llc_event_work);
+ 	INIT_WORK(&lgr->llc_add_link_work, smc_llc_add_link_work);
+ 	INIT_WORK(&lgr->llc_del_link_work, smc_llc_delete_link_work);
+ 	INIT_LIST_HEAD(&lgr->llc_event_q);
+ 	spin_lock_init(&lgr->llc_event_q_lock);
+ 	spin_lock_init(&lgr->llc_flow_lock);
+ 	init_waitqueue_head(&lgr->llc_flow_waiter);
+ 	init_waitqueue_head(&lgr->llc_msg_waiter);
+ 	mutex_init(&lgr->llc_conf_mutex);
+ 	lgr->llc_testlink_time = net->ipv4.sysctl_tcp_keepalive_time;
+ }
+ 
+ /* called after lgr was removed from lgr_list */
+ void smc_llc_lgr_clear(struct smc_link_group *lgr)
+ {
+ 	smc_llc_event_flush(lgr);
+ 	wake_up_all(&lgr->llc_flow_waiter);
+ 	wake_up_all(&lgr->llc_msg_waiter);
+ 	cancel_work_sync(&lgr->llc_event_work);
+ 	cancel_work_sync(&lgr->llc_add_link_work);
+ 	cancel_work_sync(&lgr->llc_del_link_work);
+ 	if (lgr->delayed_event) {
+ 		kfree(lgr->delayed_event);
+ 		lgr->delayed_event = NULL;
+ 	}
+ }
+ 
++>>>>>>> 6778a6bed09b (net/smc: separate LLC wait queues for flow and messages)
  int smc_llc_link_init(struct smc_link *link)
  {
 +	struct smc_link_group *lgr = smc_get_lgr(link);
 +	link->llc_wq = alloc_ordered_workqueue("llc_wq-%x:%x)", WQ_MEM_RECLAIM,
 +					       *((u32 *)lgr->id),
 +					       link->link_id);
 +	if (!link->llc_wq)
 +		return -ENOMEM;
 +	init_completion(&link->llc_confirm);
 +	init_completion(&link->llc_confirm_resp);
 +	init_completion(&link->llc_add);
 +	init_completion(&link->llc_add_resp);
 +	init_completion(&link->llc_confirm_rkey);
 +	init_completion(&link->llc_delete_rkey);
 +	mutex_init(&link->llc_delete_rkey_mutex);
  	init_completion(&link->llc_testlink_resp);
  	INIT_DELAYED_WORK(&link->llc_testlink_wrk, smc_llc_testlink_work);
  	return 0;
* Unmerged path net/smc/smc_core.c
* Unmerged path net/smc/smc_core.h
* Unmerged path net/smc/smc_llc.c
