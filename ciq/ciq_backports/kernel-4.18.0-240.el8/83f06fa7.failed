KVM: x86/mmu: Rely on host page tables to find HugeTLB mappings

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 83f06fa7a6fd9d5758e5f8438e2137f25f6f2e6b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/83f06fa7.failed

Remove KVM's HugeTLB specific logic and instead rely on walking the host
page tables (already done for THP) to identify HugeTLB mappings.
Eliminating the HugeTLB-only logic avoids taking mmap_sem and calling
find_vma() for all hugepage compatible page faults, and simplifies KVM's
page fault code by consolidating all hugepage adjustments into a common
helper.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 83f06fa7a6fd9d5758e5f8438e2137f25f6f2e6b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/paging_tmpl.h
diff --cc arch/x86/kvm/mmu/mmu.c
index f41b4d90aa8d,6be0239dcfbf..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -3332,33 -3294,63 +3297,91 @@@ static void direct_pte_prefetch(struct 
  	__direct_pte_prefetch(vcpu, sp, sptep);
  }
  
++<<<<<<< HEAD
 +static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,
 +					gfn_t gfn, kvm_pfn_t *pfnp,
 +					int *levelp)
 +{
 +	kvm_pfn_t pfn = *pfnp;
 +	int level = *levelp;
++=======
+ static int host_pfn_mapping_level(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 				  kvm_pfn_t pfn)
+ {
+ 	struct kvm_memory_slot *slot;
+ 	unsigned long hva;
+ 	pte_t *pte;
+ 	int level;
+ 
+ 	BUILD_BUG_ON(PT_PAGE_TABLE_LEVEL != (int)PG_LEVEL_4K ||
+ 		     PT_DIRECTORY_LEVEL != (int)PG_LEVEL_2M ||
+ 		     PT_PDPE_LEVEL != (int)PG_LEVEL_1G);
+ 
+ 	if (!PageCompound(pfn_to_page(pfn)))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, true);
+ 	if (!slot)
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	hva = __gfn_to_hva_memslot(slot, gfn);
+ 
+ 	pte = lookup_address_in_mm(vcpu->kvm->mm, hva, &level);
+ 	if (unlikely(!pte))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	return level;
+ }
+ 
+ static int kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 				   int max_level, kvm_pfn_t *pfnp)
+ {
+ 	kvm_pfn_t pfn = *pfnp;
+ 	kvm_pfn_t mask;
+ 	int level;
+ 
+ 	if (max_level == PT_PAGE_TABLE_LEVEL)
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	if (is_error_noslot_pfn(pfn) || kvm_is_reserved_pfn(pfn) ||
+ 	    kvm_is_zone_device_pfn(pfn))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	level = host_pfn_mapping_level(vcpu, gfn, pfn);
+ 	if (level == PT_PAGE_TABLE_LEVEL)
+ 		return level;
+ 
+ 	level = min(level, max_level);
++>>>>>>> 83f06fa7a6fd (KVM: x86/mmu: Rely on host page tables to find HugeTLB mappings)
  
  	/*
 -	 * mmu_notifier_retry() was successful and mmu_lock is held, so
 -	 * the pmd can't be split from under us.
 +	 * Check if it's a transparent hugepage. If this would be an
 +	 * hugetlbfs page, level wouldn't be set to
 +	 * PT_PAGE_TABLE_LEVEL and there would be no adjustment done
 +	 * here.
  	 */
++<<<<<<< HEAD
 +	if (!is_error_noslot_pfn(pfn) && !kvm_is_reserved_pfn(pfn) &&
 +	    !kvm_is_zone_device_pfn(pfn) && level == PT_PAGE_TABLE_LEVEL &&
 +	    PageTransCompoundMap(pfn_to_page(pfn))) {
 +		unsigned long mask;
 +
 +		/*
 +		 * mmu_notifier_retry() was successful and mmu_lock is held, so
 +		 * the pmd can't be split from under us.
 +		 */
 +		*levelp = level = PT_DIRECTORY_LEVEL;
 +		mask = KVM_PAGES_PER_HPAGE(level) - 1;
 +		VM_BUG_ON((gfn & mask) != (pfn & mask));
 +		*pfnp = pfn & ~mask;
 +	}
++=======
+ 	mask = KVM_PAGES_PER_HPAGE(level) - 1;
+ 	VM_BUG_ON((gfn & mask) != (pfn & mask));
+ 	*pfnp = pfn & ~mask;
+ 
+ 	return level;
++>>>>>>> 83f06fa7a6fd (KVM: x86/mmu: Rely on host page tables to find HugeTLB mappings)
  }
  
  static void disallowed_hugepage_adjust(struct kvm_shadow_walk_iterator it,
@@@ -3398,8 -3389,7 +3420,12 @@@ static int __direct_map(struct kvm_vcp
  	if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root_hpa)))
  		return RET_PF_RETRY;
  
++<<<<<<< HEAD
 +	if (likely(max_level > PT_PAGE_TABLE_LEVEL))
 +		transparent_hugepage_adjust(vcpu, gfn, &pfn, &level);
++=======
+ 	level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn);
++>>>>>>> 83f06fa7a6fd (KVM: x86/mmu: Rely on host page tables to find HugeTLB mappings)
  
  	trace_kvm_mmu_spte_requested(gpa, level, pfn);
  	for_each_shadow_entry(vcpu, gpa, it) {
diff --cc arch/x86/kvm/mmu/paging_tmpl.h
index 64abb6a4320c,472c32cdf2ff..000000000000
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@@ -691,8 -688,7 +691,12 @@@ static int FNAME(fetch)(struct kvm_vcp
  	gfn = gw->gfn | ((addr & PT_LVL_OFFSET_MASK(gw->level)) >> PAGE_SHIFT);
  	base_gfn = gfn;
  
++<<<<<<< HEAD
 +	if (max_level > PT_PAGE_TABLE_LEVEL)
 +		transparent_hugepage_adjust(vcpu, gw->gfn, &pfn, &hlevel);
++=======
+ 	hlevel = kvm_mmu_hugepage_adjust(vcpu, gw->gfn, max_level, &pfn);
++>>>>>>> 83f06fa7a6fd (KVM: x86/mmu: Rely on host page tables to find HugeTLB mappings)
  
  	trace_kvm_mmu_spte_requested(addr, gw->level, pfn);
  
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/paging_tmpl.h
