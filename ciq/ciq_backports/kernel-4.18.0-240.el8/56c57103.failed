mm/mmu_notifiers: hoist do_mmu_notifier_register down_write to the caller

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 56c57103db17db9ecdad0507a3f0e3eea747fabe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/56c57103.failed

This simplifies the code to not have so many one line functions and extra
logic. __mmu_notifier_register() simply becomes the entry point to
register the notifier, and the other one calls it under lock.

Also add a lockdep_assert to check that the callers are holding the lock
as expected.

Link: https://lore.kernel.org/r/20190806231548.25242-2-jgg@ziepe.ca
	Suggested-by: Christoph Hellwig <hch@infradead.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Tested-by: Ralph Campbell <rcampbell@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 56c57103db17db9ecdad0507a3f0e3eea747fabe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mmu_notifier.c
diff --cc mm/mmu_notifier.c
index e6c07a8c4021,218a6f108bc2..000000000000
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@@ -230,39 -237,10 +230,46 @@@ void __mmu_notifier_invalidate_range(st
  EXPORT_SYMBOL_GPL(__mmu_notifier_invalidate_range);
  
  /*
++<<<<<<< HEAD
 + * Must be called while holding mm->mmap_sem for either read or write.
 + * The result is guaranteed to be valid until mm->mmap_sem is dropped.
 + */
 +bool mm_has_blockable_invalidate_notifiers(struct mm_struct *mm)
 +{
 +	struct mmu_notifier *mn;
 +	int id;
 +	bool ret = false;
 +
 +	WARN_ON_ONCE(!rwsem_is_locked(&mm->mmap_sem));
 +
 +	if (!mm_has_notifiers(mm))
 +		return ret;
 +
 +	id = srcu_read_lock(&srcu);
 +	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
 +		if (!mn->ops->invalidate_range &&
 +		    !mn->ops->invalidate_range_start &&
 +		    !mn->ops->invalidate_range_end)
 +				continue;
 +
 +		if (!(mn->ops->flags & MMU_INVALIDATE_DOES_NOT_BLOCK)) {
 +			ret = true;
 +			break;
 +		}
 +	}
 +	srcu_read_unlock(&srcu, id);
 +	return ret;
 +}
 +
 +static int do_mmu_notifier_register(struct mmu_notifier *mn,
 +				    struct mm_struct *mm,
 +				    int take_mmap_sem)
++=======
+  * Same as mmu_notifier_register but here the caller must hold the
+  * mmap_sem in write mode.
+  */
+ int __mmu_notifier_register(struct mmu_notifier *mn, struct mm_struct *mm)
++>>>>>>> 56c57103db17 (mm/mmu_notifiers: hoist do_mmu_notifier_register down_write to the caller)
  {
  	struct mmu_notifier_mm *mmu_notifier_mm;
  	int ret;
* Unmerged path mm/mmu_notifier.c
