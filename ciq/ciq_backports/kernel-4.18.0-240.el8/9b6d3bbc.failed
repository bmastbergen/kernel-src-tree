RDMA/mlx5: Prevent overflow in mmap offset calculations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit 9b6d3bbc1335404b331f4f11dc896066bdf1c752
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/9b6d3bbc.failed

The cmd and index variables declared as u16 and the result is supposed to
be stored in u64. The C arithmetic rules doesn't promote "(index >> 8) <<
16" to be u64 and leaves the end result to be u16.

Fixes: 7be76bef320b ("IB/mlx5: Introduce VAR object and its alloc/destroy methods")
Link: https://lore.kernel.org/r/20200212072635.682689-10-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 9b6d3bbc1335404b331f4f11dc896066bdf1c752)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index edc0323d5818,987bfdcd12a5..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -2211,25 -2227,67 +2211,49 @@@ free_bfreg
  	return err;
  }
  
 -static int add_dm_mmap_entry(struct ib_ucontext *context,
 -			     struct mlx5_ib_dm *mdm,
 -			     u64 address)
 -{
 -	mdm->mentry.mmap_flag = MLX5_IB_MMAP_TYPE_MEMIC;
 -	mdm->mentry.address = address;
 -	return rdma_user_mmap_entry_insert_range(
 -			context, &mdm->mentry.rdma_entry,
 -			mdm->size,
 -			MLX5_IB_MMAP_DEVICE_MEM << 16,
 -			(MLX5_IB_MMAP_DEVICE_MEM << 16) + (1UL << 16) - 1);
 -}
 -
 -static unsigned long mlx5_vma_to_pgoff(struct vm_area_struct *vma)
 +static int dm_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)
  {
 -	unsigned long idx;
 -	u8 command;
 -
 -	command = get_command(vma->vm_pgoff);
 -	idx = get_extended_index(vma->vm_pgoff);
 -
 -	return (command << 16 | idx);
 -}
 -
 -static int mlx5_ib_mmap_offset(struct mlx5_ib_dev *dev,
 -			       struct vm_area_struct *vma,
 -			       struct ib_ucontext *ucontext)
 -{
 -	struct mlx5_user_mmap_entry *mentry;
 -	struct rdma_user_mmap_entry *entry;
 -	unsigned long pgoff;
 -	pgprot_t prot;
 +	struct mlx5_ib_ucontext *mctx = to_mucontext(context);
 +	struct mlx5_ib_dev *dev = to_mdev(context->device);
 +	u16 page_idx = get_extended_index(vma->vm_pgoff);
 +	size_t map_size = vma->vm_end - vma->vm_start;
 +	u32 npages = map_size >> PAGE_SHIFT;
  	phys_addr_t pfn;
 -	int ret;
  
 -	pgoff = mlx5_vma_to_pgoff(vma);
 -	entry = rdma_user_mmap_entry_get_pgoff(ucontext, pgoff);
 -	if (!entry)
 +	if (find_next_zero_bit(mctx->dm_pages, page_idx + npages, page_idx) !=
 +	    page_idx + npages)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	pfn = ((dev->mdev->bar_addr +
 +	      MLX5_CAP64_DEV_MEM(dev->mdev, memic_bar_start_addr)) >>
 +	      PAGE_SHIFT) +
 +	      page_idx;
 +	return rdma_user_mmap_io(context, vma, pfn, map_size,
 +				 pgprot_writecombine(vma->vm_page_prot));
++=======
+ 	mentry = to_mmmap(entry);
+ 	pfn = (mentry->address >> PAGE_SHIFT);
+ 	if (mentry->mmap_flag == MLX5_IB_MMAP_TYPE_VAR)
+ 		prot = pgprot_noncached(vma->vm_page_prot);
+ 	else
+ 		prot = pgprot_writecombine(vma->vm_page_prot);
+ 	ret = rdma_user_mmap_io(ucontext, vma, pfn,
+ 				entry->npages * PAGE_SIZE,
+ 				prot,
+ 				entry);
+ 	rdma_user_mmap_entry_put(&mentry->rdma_entry);
+ 	return ret;
+ }
+ 
+ static u64 mlx5_entry_to_mmap_offset(struct mlx5_user_mmap_entry *entry)
+ {
+ 	u64 cmd = (entry->rdma_entry.start_pgoff >> 16) & 0xFFFF;
+ 	u64 index = entry->rdma_entry.start_pgoff & 0xFFFF;
+ 
+ 	return (((index >> 8) << 16) | (cmd << MLX5_IB_MMAP_CMD_SHIFT) |
+ 		(index & 0xFF)) << PAGE_SHIFT;
++>>>>>>> 9b6d3bbc1335 (RDMA/mlx5: Prevent overflow in mmap offset calculations)
  }
  
  static int mlx5_ib_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vma)
* Unmerged path drivers/infiniband/hw/mlx5/main.c
