io_uring: make submission ref putting consistent

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 594506fec5faec2b1ec82ad6fb0c8132512fc459
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/594506fe.failed

The rule is simple, any async handler gets a submission ref and should
put it at the end. Make them all follow it, and so more consistent.

This is a preparation patch, and as io_wq_assign_next() currently won't
ever work, this doesn't care to use io_put_req_find_next() instead of
io_put_req().

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>

refcount_inc_not_zero() -> refcount_inc() fix.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 594506fec5faec2b1ec82ad6fb0c8132512fc459)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,0e935d77d8aa..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1487,35 -2539,516 +1487,538 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++=======
+ static bool io_req_cancelled(struct io_kiocb *req)
+ {
+ 	if (req->work.flags & IO_WQ_WORK_CANCEL) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, -ECANCELED);
+ 		io_double_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_link_work_cb(struct io_wq_work **workptr)
+ {
+ 	struct io_wq_work *work = *workptr;
+ 	struct io_kiocb *link = work->data;
+ 
+ 	io_queue_linked_timeout(link);
+ 	io_wq_submit_work(workptr);
+ }
+ 
+ static void io_wq_assign_next(struct io_wq_work **workptr, struct io_kiocb *nxt)
+ {
+ 	struct io_kiocb *link;
+ 
+ 	*workptr = &nxt->work;
+ 	link = io_prep_linked_timeout(nxt);
+ 	if (link) {
+ 		nxt->work.func = io_link_work_cb;
+ 		nxt->work.data = link;
+ 	}
+ }
+ 
+ static void __io_fsync(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	loff_t end = req->sync.off + req->sync.len;
+ 	int ret;
+ 
+ 	ret = vfs_fsync_range(req->file, req->sync.off,
+ 				end > 0 ? end : LLONG_MAX,
+ 				req->sync.flags & IORING_FSYNC_DATASYNC);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_fsync_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fsync(req, &nxt);
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_fsync(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  		    bool force_nonblock)
  {
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
 +	int ret;
 +
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
 +
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
 +
  	/* fsync always requires a blocking context */
++<<<<<<< HEAD
++=======
+ 	if (force_nonblock) {
+ 		req->work.func = io_fsync_finish;
+ 		return -EAGAIN;
+ 	}
+ 	__io_fsync(req, nxt);
+ 	return 0;
+ }
+ 
+ static void __io_fallocate(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	int ret;
+ 
+ 	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
+ 				req->sync.len);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_fallocate_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fallocate(req, &nxt);
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_fallocate_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	req->sync.off = READ_ONCE(sqe->off);
+ 	req->sync.len = READ_ONCE(sqe->addr);
+ 	req->sync.mode = READ_ONCE(sqe->len);
+ 	return 0;
+ }
+ 
+ static int io_fallocate(struct io_kiocb *req, struct io_kiocb **nxt,
+ 			bool force_nonblock)
+ {
+ 	/* fallocate always requiring blocking context */
+ 	if (force_nonblock) {
+ 		req->work.func = io_fallocate_finish;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	__io_fallocate(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.how.mode = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.how.flags = READ_ONCE(sqe->open_flags);
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct open_how __user *how;
+ 	const char __user *fname;
+ 	size_t len;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	len = READ_ONCE(sqe->len);
+ 
+ 	if (len < OPEN_HOW_SIZE_VER0)
+ 		return -EINVAL;
+ 
+ 	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
+ 					len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
+ 		req->open.how.flags |= O_LARGEFILE;
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_openat2(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ 	struct open_flags op;
+ 	struct file *file;
+ 	int ret;
+ 
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  	if (force_nonblock)
  		return -EAGAIN;
  
 -	ret = build_open_flags(&req->open.how, &op);
 -	if (ret)
 -		goto err;
 +	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
 +				end > 0 ? end : LLONG_MAX,
 +				fsync_flags & IORING_FSYNC_DATASYNC);
  
++<<<<<<< HEAD
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
++=======
+ 	ret = get_unused_fd_flags(req->open.how.flags);
+ 	if (ret < 0)
+ 		goto err;
+ 
+ 	file = do_filp_open(req->open.dfd, req->open.filename, &op);
+ 	if (IS_ERR(file)) {
+ 		put_unused_fd(ret);
+ 		ret = PTR_ERR(file);
+ 	} else {
+ 		fsnotify_open(file);
+ 		fd_install(ret, file);
+ 	}
+ err:
+ 	putname(req->open.filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_openat(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		     bool force_nonblock)
+ {
+ 	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
+ 	return io_openat2(req, nxt, force_nonblock);
+ }
+ 
+ static int io_epoll_ctl_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	req->epoll.epfd = READ_ONCE(sqe->fd);
+ 	req->epoll.op = READ_ONCE(sqe->len);
+ 	req->epoll.fd = READ_ONCE(sqe->off);
+ 
+ 	if (ep_op_has_event(req->epoll.op)) {
+ 		struct epoll_event __user *ev;
+ 
+ 		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
+ 			return -EFAULT;
+ 	}
+ 
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_epoll_ctl(struct io_kiocb *req, struct io_kiocb **nxt,
+ 			bool force_nonblock)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	struct io_epoll *ie = &req->epoll;
+ 	int ret;
+ 
+ 	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
+ 	if (force_nonblock && ret == -EAGAIN)
+ 		return -EAGAIN;
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	if (sqe->ioprio || sqe->buf_index || sqe->off)
+ 		return -EINVAL;
+ 
+ 	req->madvise.addr = READ_ONCE(sqe->addr);
+ 	req->madvise.len = READ_ONCE(sqe->len);
+ 	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	struct io_madvise *ma = &req->madvise;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	ret = do_madvise(ma->addr, ma->len, ma->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->addr)
+ 		return -EINVAL;
+ 
+ 	req->fadvise.offset = READ_ONCE(sqe->off);
+ 	req->fadvise.len = READ_ONCE(sqe->len);
+ 	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ }
+ 
+ static int io_fadvise(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ 	struct io_fadvise *fa = &req->fadvise;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		switch (fa->advice) {
+ 		case POSIX_FADV_NORMAL:
+ 		case POSIX_FADV_RANDOM:
+ 		case POSIX_FADV_SEQUENTIAL:
+ 			break;
+ 		default:
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	unsigned lookup_flags;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.mask = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	req->open.how.flags = READ_ONCE(sqe->statx_flags);
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
+ 		return -EINVAL;
+ 
+ 	req->open.filename = getname_flags(fname, lookup_flags, NULL);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_statx(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		    bool force_nonblock)
+ {
+ 	struct io_open *ctx = &req->open;
+ 	unsigned lookup_flags;
+ 	struct path path;
+ 	struct kstat stat;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
+ 		return -EINVAL;
+ 
+ retry:
+ 	/* filename_lookup() drops it, keep a reference */
+ 	ctx->filename->refcnt++;
+ 
+ 	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
+ 				NULL);
+ 	if (ret)
+ 		goto err;
+ 
+ 	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
+ 	path_put(&path);
+ 	if (retry_estale(ret, lookup_flags)) {
+ 		lookup_flags |= LOOKUP_REVAL;
+ 		goto retry;
+ 	}
+ 	if (!ret)
+ 		ret = cp_statx(&stat, ctx->buffer);
+ err:
+ 	putname(ctx->filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * If we queue this for async, it must not be cancellable. That would
+ 	 * leave the 'file' in an undeterminate state.
+ 	 */
+ 	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
+ 
+ 	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
+ 	    sqe->rw_flags || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->close.fd = READ_ONCE(sqe->fd);
+ 	if (req->file->f_op == &io_uring_fops ||
+ 	    req->close.fd == req->ctx->ring_fd)
+ 		return -EBADF;
+ 
+ 	return 0;
+ }
+ 
+ /* only called when __close_fd_get_file() is done */
+ static void __io_close_finish(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	int ret;
+ 
+ 	ret = filp_close(req->close.put_file, req->work.files);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	fput(req->close.put_file);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_close_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	/* not cancellable, don't do io_req_cancelled() */
+ 	__io_close_finish(req, &nxt);
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_close(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		    bool force_nonblock)
+ {
+ 	int ret;
+ 
+ 	req->close.put_file = NULL;
+ 	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* if the file has a flush method, be safe and punt to async */
+ 	if (req->close.put_file->f_op->flush && force_nonblock) {
+ 		/* submission ref will be dropped, take it for async */
+ 		refcount_inc(&req->refs);
+ 
+ 		req->work.func = io_close_finish;
+ 		/*
+ 		 * Do manual async queue here to avoid grabbing files - we don't
+ 		 * need the files, and it'll cause io_close_finish() to close
+ 		 * the file again and cause a double CQE entry for this request
+ 		 */
+ 		io_queue_async_work(req);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * No ->flush(), safely close from here and just punt the
+ 	 * fput() to async context.
+ 	 */
+ 	__io_close_finish(req, nxt);
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  	return 0;
  }
  
@@@ -1532,45 -3064,104 +2035,89 @@@ static int io_prep_sfr(struct io_kiocb 
  	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
  		return -EINVAL;
  
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->len);
 -	req->sync.flags = READ_ONCE(sqe->sync_range_flags);
 -	return 0;
 +	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_sync_file_range(struct io_kiocb *req,
 +			      const struct io_uring_sqe *sqe,
++=======
+ static void __io_sync_file_range(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	int ret;
+ 
+ 	ret = sync_file_range(req->file, req->sync.off, req->sync.len,
+ 				req->sync.flags);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ 
+ static void io_sync_file_range_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_sync_file_range(req, &nxt);
+ 	io_put_req(req); /* put submission ref */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_sync_file_range(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  			      bool force_nonblock)
  {
 +	loff_t sqe_off;
 +	loff_t sqe_len;
 +	unsigned flags;
 +	int ret;
 +
 +	ret = io_prep_sfr(req, sqe);
 +	if (ret)
 +		return ret;
 +
  	/* sync_file_range always requires a blocking context */
++<<<<<<< HEAD
 +	if (force_nonblock)
++=======
+ 	if (force_nonblock) {
+ 		req->work.func = io_sync_file_range_finish;
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  		return -EAGAIN;
 -	}
 -
 -	__io_sync_file_range(req, nxt);
 -	return 0;
 -}
 -
 -static int io_setup_async_msg(struct io_kiocb *req,
 -			      struct io_async_msghdr *kmsg)
 -{
 -	if (req->io)
 -		return -EAGAIN;
 -	if (io_alloc_async_ctx(req)) {
 -		if (kmsg->iov != kmsg->fast_iov)
 -			kfree(kmsg->iov);
 -		return -ENOMEM;
 -	}
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	memcpy(&req->io->msg, kmsg, sizeof(*kmsg));
 -	return -EAGAIN;
 -}
 -
 -static int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct io_async_ctx *io = req->io;
 -	int ret;
 -
 -	sr->msg_flags = READ_ONCE(sqe->msg_flags);
 -	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	sr->len = READ_ONCE(sqe->len);
  
 -#ifdef CONFIG_COMPAT
 -	if (req->ctx->compat)
 -		sr->msg_flags |= MSG_CMSG_COMPAT;
 -#endif
 +	sqe_off = READ_ONCE(sqe->off);
 +	sqe_len = READ_ONCE(sqe->len);
 +	flags = READ_ONCE(sqe->sync_range_flags);
  
 -	if (!io || req->opcode == IORING_OP_SEND)
 -		return 0;
 -	/* iovec is already imported */
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 +	ret = sync_file_range(req->rw.ki_filp, sqe_off, sqe_len, flags);
  
 -	io->msg.iov = io->msg.fast_iov;
 -	ret = sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
 -					&io->msg.iov);
 -	if (!ret)
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -	return ret;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
++<<<<<<< HEAD
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
 +	return 0;
++=======
++	if (io_req_cancelled(req))
++		return;
++	__io_accept(req, &nxt, false);
++	io_put_req(req); /* drop submission reference */
++	if (nxt)
++		io_wq_assign_next(workptr, nxt);
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  }
  
 -static int io_sendmsg(struct io_kiocb *req, struct io_kiocb **nxt,
 -		      bool force_nonblock)
 -{
  #if defined(CONFIG_NET)
 -	struct io_async_msghdr *kmsg = NULL;
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
 +{
  	struct socket *sock;
  	int ret;
  
@@@ -2166,48 -4709,135 +2713,150 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static void io_wq_submit_work(struct io_wq_work **workptr)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_wq_work *work = *workptr;
 -	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -	int ret = 0;
 +	int ret;
  
 -	/* if NO_CANCEL is set, we must still run the work */
 -	if ((work->flags & (IO_WQ_WORK_CANCEL|IO_WQ_WORK_NO_CANCEL)) ==
 -				IO_WQ_WORK_CANCEL) {
 -		ret = -ECANCELED;
 -	}
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
 +		struct io_uring_sqe *sqe_copy;
 +
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (sqe_copy) {
 +			struct async_list *list;
 +
 +			s->sqe = sqe_copy;
 +			memcpy(&req->submit, s, sizeof(*s));
 +			list = io_async_list_from_sqe(ctx, s->sqe);
 +			if (!io_add_to_prev_work(list, req)) {
 +				if (list)
 +					atomic_inc(&list->cnt);
 +				INIT_WORK(&req->work, io_sq_wq_submit_work);
 +				io_queue_async_work(ctx, req);
 +			}
  
 -	if (!ret) {
 -		do {
 -			ret = io_issue_sqe(req, NULL, &nxt, false);
  			/*
 -			 * We can get EAGAIN for polled IO even though we're
 -			 * forcing a sync submission from here, since we can't
 -			 * wait for request slots on the block side.
 +			 * Queued up for async execution, worker will release
 +			 * submit reference when the iocb is actually submitted.
  			 */
 -			if (ret != -EAGAIN)
 -				break;
 -			cond_resched();
 -		} while (1);
 +			return 0;
 +		}
  	}
  
++<<<<<<< HEAD
 +	/* drop submission reference */
 +	io_put_req(req);
 +
 +	/* and drop final reference, if we failed */
++=======
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  	if (ret) {
 -		req_set_fail_links(req);
 -		io_cqring_add_event(req, ret);
 +		io_cqring_add_event(ctx, req->user_data, ret);
 +		if (req->flags & REQ_F_LINK)
 +			req->flags |= REQ_F_FAIL_LINK;
  		io_put_req(req);
  	}
  
++<<<<<<< HEAD
++=======
+ 	io_put_req(req); /* drop submission reference */
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_req_needs_file(struct io_kiocb *req, int fd)
+ {
+ 	if (!io_op_defs[req->opcode].needs_file)
+ 		return 0;
+ 	if ((fd == -1 || fd == AT_FDCWD) && io_op_defs[req->opcode].fd_non_neg)
+ 		return 0;
+ 	return 1;
+ }
+ 
+ static inline struct file *io_file_from_index(struct io_ring_ctx *ctx,
+ 					      int index)
+ {
+ 	struct fixed_file_table *table;
+ 
+ 	table = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];
+ 	return table->files[index & IORING_FILE_TABLE_MASK];;
+ }
+ 
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 			int fd, struct file **out_file, bool fixed)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct file *file;
+ 
+ 	if (fixed) {
+ 		if (unlikely(!ctx->file_data ||
+ 		    (unsigned) fd >= ctx->nr_user_files))
+ 			return -EBADF;
+ 		fd = array_index_nospec(fd, ctx->nr_user_files);
+ 		file = io_file_from_index(ctx, fd);
+ 		if (!file)
+ 			return -EBADF;
+ 		percpu_ref_get(&ctx->file_data->refs);
+ 	} else {
+ 		trace_io_uring_file_get(ctx, fd);
+ 		file = __io_file_get(state, fd);
+ 		if (unlikely(!file))
+ 			return -EBADF;
+ 	}
+ 
+ 	*out_file = file;
+ 	return 0;
+ }
+ 
+ static int io_req_set_file(struct io_submit_state *state, struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe)
+ {
+ 	unsigned flags;
+ 	int fd;
+ 	bool fixed;
+ 
+ 	flags = READ_ONCE(sqe->flags);
+ 	fd = READ_ONCE(sqe->fd);
+ 
+ 	if (!io_req_needs_file(req, fd))
+ 		return 0;
+ 
+ 	fixed = (flags & IOSQE_FIXED_FILE);
+ 	if (unlikely(!fixed && req->needs_fixed_file))
+ 		return -EBADF;
+ 
+ 	return io_file_get(state, req, fd, &req->file, fixed);
+ }
+ 
+ static int io_grab_files(struct io_kiocb *req)
+ {
+ 	int ret = -EBADF;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	if (req->work.files)
+ 		return 0;
+ 	if (!ctx->ring_file)
+ 		return -EBADF;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irq(&ctx->inflight_lock);
+ 	/*
+ 	 * We use the f_ops->flush() handler to ensure that we can flush
+ 	 * out work accessing these files if the fd is closed. Check if
+ 	 * the fd has changed since we started down this path, and disallow
+ 	 * this operation if it has.
+ 	 */
+ 	if (fcheck(ctx->ring_fd) == ctx->ring_file) {
+ 		list_add(&req->inflight_entry, &ctx->inflight_list);
+ 		req->flags |= REQ_F_INFLIGHT;
+ 		req->work.files = current->files;
+ 		ret = 0;
+ 	}
+ 	spin_unlock_irq(&ctx->inflight_lock);
+ 	rcu_read_unlock();
+ 
++>>>>>>> 594506fec5fa (io_uring: make submission ref putting consistent)
  	return ret;
  }
  
* Unmerged path fs/io_uring.c
