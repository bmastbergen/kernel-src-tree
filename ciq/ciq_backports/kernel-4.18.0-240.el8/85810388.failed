xprtrdma: Destroy rpcrdma_rep when Receive is flushed

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 85810388a9ddcc8e82738a3df6d3d7b32a79e0ea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/85810388.failed

This reduces the hardware and memory footprint of an unconnected
transport.

At some point in the future, transport reconnect will allow
resolving the destination IP address through a different device. The
current change enables reps for the new connection to be allocated
on whichever NUMA node the new device affines to after a reconnect.

Note that this does not destroy _all_ the transport's reps... there
will be a few that are still part of a running RPC completion.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 85810388a9ddcc8e82738a3df6d3d7b32a79e0ea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
diff --cc net/sunrpc/xprtrdma/verbs.c
index e03a8d720a48,52481e70891a..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -73,10 -74,16 +73,15 @@@
  /*
   * internal functions
   */
 -static int rpcrdma_sendctxs_create(struct rpcrdma_xprt *r_xprt);
 -static void rpcrdma_sendctxs_destroy(struct rpcrdma_xprt *r_xprt);
 -static void rpcrdma_sendctx_put_locked(struct rpcrdma_xprt *r_xprt,
 -				       struct rpcrdma_sendctx *sc);
 -static int rpcrdma_reqs_setup(struct rpcrdma_xprt *r_xprt);
 +static void rpcrdma_sendctx_put_locked(struct rpcrdma_sendctx *sc);
  static void rpcrdma_reqs_reset(struct rpcrdma_xprt *r_xprt);
++<<<<<<< HEAD
++=======
+ static void rpcrdma_rep_destroy(struct rpcrdma_rep *rep);
+ static void rpcrdma_reps_unmap(struct rpcrdma_xprt *r_xprt);
++>>>>>>> 85810388a9dd (xprtrdma: Destroy rpcrdma_rep when Receive is flushed)
  static void rpcrdma_mrs_create(struct rpcrdma_xprt *r_xprt);
 -static void rpcrdma_mrs_destroy(struct rpcrdma_xprt *r_xprt);
 +static void rpcrdma_mrs_destroy(struct rpcrdma_buffer *buf);
  static struct rpcrdma_regbuf *
  rpcrdma_regbuf_alloc(size_t size, enum dma_data_direction direction,
  		     gfp_t flags);
@@@ -171,14 -178,14 +176,14 @@@ rpcrdma_wc_receive(struct ib_cq *cq, st
  	return;
  
  out_flushed:
- 	rpcrdma_recv_buffer_put(rep);
+ 	rpcrdma_rep_destroy(rep);
  }
  
 -static void rpcrdma_update_cm_private(struct rpcrdma_xprt *r_xprt,
 -				      struct rdma_conn_param *param)
 +static void
 +rpcrdma_update_connect_private(struct rpcrdma_xprt *r_xprt,
 +			       struct rdma_conn_param *param)
  {
  	const struct rpcrdma_connect_private *pmsg = param->private_data;
 -	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
  	unsigned int rsize, wsize;
  
  	/* Default settings for RPC-over-RDMA Version One */
@@@ -1022,13 -1071,45 +1027,16 @@@ static void rpcrdma_reqs_reset(struct r
  {
  	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
  	struct rpcrdma_req *req;
 -	int rc;
  
  	list_for_each_entry(req, &buf->rb_allreqs, rl_all) {
 -		rc = rpcrdma_req_setup(r_xprt, req);
 -		if (rc)
 -			return rc;
 +		/* Credits are valid only for one connection */
 +		req->rl_slot.rq_cong = 0;
  	}
 -	return 0;
 -}
 -
 -static void rpcrdma_req_reset(struct rpcrdma_req *req)
 -{
 -	/* Credits are valid for only one connection */
 -	req->rl_slot.rq_cong = 0;
 -
 -	rpcrdma_regbuf_free(req->rl_rdmabuf);
 -	req->rl_rdmabuf = NULL;
 -
 -	rpcrdma_regbuf_dma_unmap(req->rl_sendbuf);
 -	rpcrdma_regbuf_dma_unmap(req->rl_recvbuf);
 -}
 -
 -/* ASSUMPTION: the rb_allreqs list is stable for the duration,
 - * and thus can be walked without holding rb_lock. Eg. the
 - * caller is holding the transport send lock to exclude
 - * device removal or disconnection.
 - */
 -static void rpcrdma_reqs_reset(struct rpcrdma_xprt *r_xprt)
 -{
 -	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
 -	struct rpcrdma_req *req;
 -
 -	list_for_each_entry(req, &buf->rb_allreqs, rl_all)
 -		rpcrdma_req_reset(req);
  }
  
+ /* No locking needed here. This function is called only by the
+  * Receive completion handler.
+  */
  static struct rpcrdma_rep *rpcrdma_rep_create(struct rpcrdma_xprt *r_xprt,
  					      bool temp)
  {
@@@ -1060,6 -1142,50 +1068,53 @@@ out
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ /* No locking needed here. This function is invoked only by the
+  * Receive completion handler, or during transport shutdown.
+  */
+ static void rpcrdma_rep_destroy(struct rpcrdma_rep *rep)
+ {
+ 	list_del(&rep->rr_all);
+ 	rpcrdma_regbuf_free(rep->rr_rdmabuf);
+ 	kfree(rep);
+ }
+ 
+ static struct rpcrdma_rep *rpcrdma_rep_get_locked(struct rpcrdma_buffer *buf)
+ {
+ 	struct llist_node *node;
+ 
+ 	/* Calls to llist_del_first are required to be serialized */
+ 	node = llist_del_first(&buf->rb_free_reps);
+ 	if (!node)
+ 		return NULL;
+ 	return llist_entry(node, struct rpcrdma_rep, rr_node);
+ }
+ 
+ static void rpcrdma_rep_put(struct rpcrdma_buffer *buf,
+ 			    struct rpcrdma_rep *rep)
+ {
+ 	llist_add(&rep->rr_node, &buf->rb_free_reps);
+ }
+ 
+ static void rpcrdma_reps_unmap(struct rpcrdma_xprt *r_xprt)
+ {
+ 	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
+ 	struct rpcrdma_rep *rep;
+ 
+ 	list_for_each_entry(rep, &buf->rb_all_reps, rr_all)
+ 		rpcrdma_regbuf_dma_unmap(rep->rr_rdmabuf);
+ }
+ 
+ static void rpcrdma_reps_destroy(struct rpcrdma_buffer *buf)
+ {
+ 	struct rpcrdma_rep *rep;
+ 
+ 	while ((rep = rpcrdma_rep_get_locked(buf)) != NULL)
+ 		rpcrdma_rep_destroy(rep);
+ }
+ 
++>>>>>>> 85810388a9dd (xprtrdma: Destroy rpcrdma_rep when Receive is flushed)
  /**
   * rpcrdma_buffer_create - Create initial set of req/rep objects
   * @r_xprt: transport instance to (re)initialize
* Unmerged path net/sunrpc/xprtrdma/verbs.c
