ceph: reorganize __send_cap for less spinlock abuse

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jeff Layton <jlayton@kernel.org>
commit 0a454bdd501ad1aa30bb72e9581efa338ad6ce5c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/0a454bdd.failed

Get rid of the __releases annotation by breaking it up into two
functions: __prep_cap which is done under the spinlock and __send_cap
that is done outside it. Add new fields to cap_msg_args for the wake
boolean and old_xattr_buf pointer.

Nothing checks the return value from __send_cap, so make it void
return.

	Signed-off-by: Jeff Layton <jlayton@kernel.org>
	Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
(cherry picked from commit 0a454bdd501ad1aa30bb72e9581efa338ad6ce5c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ceph/caps.c
diff --cc fs/ceph/caps.c
index 0b723b85ab9b,8b17f4b4ef7c..000000000000
--- a/fs/ceph/caps.c
+++ b/fs/ceph/caps.c
@@@ -1321,19 -1333,9 +1317,15 @@@ static void __prep_cap(struct cap_msg_a
  {
  	struct ceph_inode_info *ci = cap->ci;
  	struct inode *inode = &ci->vfs_inode;
- 	struct ceph_buffer *old_blob = NULL;
- 	struct cap_msg_args arg;
  	int held, revoking;
++<<<<<<< HEAD
 +	int wake = 0;
 +	int delayed = 0;
 +	int ret;
++=======
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  
- 	/* Don't send anything if it's still being created. Return delayed */
- 	if (ci->i_ceph_flags & CEPH_I_ASYNC_CREATE) {
- 		spin_unlock(&ci->i_ceph_lock);
- 		dout("%s async create in flight for %p\n", __func__, inode);
- 		return 1;
- 	}
+ 	lockdep_assert_held(&ci->i_ceph_lock);
  
  	held = cap->issued | cap->implemented;
  	revoking = cap->implemented & ~cap->issued;
@@@ -1345,83 -1347,61 +1337,90 @@@
  	     ceph_cap_string(revoking));
  	BUG_ON((retain & CEPH_CAP_PIN) == 0);
  
 -	ci->i_ceph_flags &= ~CEPH_I_FLUSH;
 +	arg.session = cap->session;
 +
 +	/* don't release wanted unless we've waited a bit. */
 +	if ((ci->i_ceph_flags & CEPH_I_NODELAY) == 0 &&
 +	    time_before(jiffies, ci->i_hold_caps_min)) {
 +		dout(" delaying issued %s -> %s, wanted %s -> %s on send\n",
 +		     ceph_cap_string(cap->issued),
 +		     ceph_cap_string(cap->issued & retain),
 +		     ceph_cap_string(cap->mds_wanted),
 +		     ceph_cap_string(want));
 +		want |= cap->mds_wanted;
 +		retain |= cap->issued;
 +		delayed = 1;
 +	}
 +	ci->i_ceph_flags &= ~(CEPH_I_NODELAY | CEPH_I_FLUSH);
 +	if (want & ~cap->mds_wanted) {
 +		/* user space may open/close single file frequently.
 +		 * This avoids droping mds_wanted immediately after
 +		 * requesting new mds_wanted.
 +		 */
 +		__cap_set_timeouts(mdsc, ci);
 +	}
  
  	cap->issued &= retain;  /* drop bits we don't want */
- 	if (cap->implemented & ~cap->issued) {
- 		/*
- 		 * Wake up any waiters on wanted -> needed transition.
- 		 * This is due to the weird transition from buffered
- 		 * to sync IO... we need to flush dirty pages _before_
- 		 * allowing sync writes to avoid reordering.
- 		 */
- 		wake = 1;
- 	}
+ 	/*
+ 	 * Wake up any waiters on wanted -> needed transition. This is due to
+ 	 * the weird transition from buffered to sync IO... we need to flush
+ 	 * dirty pages _before_ allowing sync writes to avoid reordering.
+ 	 */
+ 	arg->wake = cap->implemented & ~cap->issued;
  	cap->implemented &= cap->issued | used;
  	cap->mds_wanted = want;
  
++<<<<<<< HEAD
 +	arg.ino = ceph_vino(inode).ino;
 +	arg.cid = cap->cap_id;
 +	arg.follows = flushing ? ci->i_head_snapc->seq : 0;
 +	arg.flush_tid = flush_tid;
 +	arg.oldest_flush_tid = oldest_flush_tid;
- 
- 	arg.size = inode->i_size;
- 	ci->i_reported_size = arg.size;
- 	arg.max_size = ci->i_wanted_max_size;
++=======
+ 	arg->session = cap->session;
+ 	arg->ino = ceph_vino(inode).ino;
+ 	arg->cid = cap->cap_id;
+ 	arg->follows = flushing ? ci->i_head_snapc->seq : 0;
+ 	arg->flush_tid = flush_tid;
+ 	arg->oldest_flush_tid = oldest_flush_tid;
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
+ 
+ 	arg->size = inode->i_size;
+ 	ci->i_reported_size = arg->size;
+ 	arg->max_size = ci->i_wanted_max_size;
  	if (cap == ci->i_auth_cap)
- 		ci->i_requested_max_size = arg.max_size;
+ 		ci->i_requested_max_size = arg->max_size;
  
  	if (flushing & CEPH_CAP_XATTR_EXCL) {
- 		old_blob = __ceph_build_xattrs_blob(ci);
- 		arg.xattr_version = ci->i_xattrs.version;
- 		arg.xattr_buf = ci->i_xattrs.blob;
+ 		arg->old_xattr_buf = __ceph_build_xattrs_blob(ci);
+ 		arg->xattr_version = ci->i_xattrs.version;
+ 		arg->xattr_buf = ci->i_xattrs.blob;
  	} else {
- 		arg.xattr_buf = NULL;
+ 		arg->xattr_buf = NULL;
+ 		arg->old_xattr_buf = NULL;
  	}
  
- 	arg.mtime = inode->i_mtime;
- 	arg.atime = inode->i_atime;
- 	arg.ctime = inode->i_ctime;
- 	arg.btime = ci->i_btime;
- 	arg.change_attr = inode_peek_iversion_raw(inode);
+ 	arg->mtime = inode->i_mtime;
+ 	arg->atime = inode->i_atime;
+ 	arg->ctime = inode->i_ctime;
+ 	arg->btime = ci->i_btime;
+ 	arg->change_attr = inode_peek_iversion_raw(inode);
  
- 	arg.op = op;
- 	arg.caps = cap->implemented;
- 	arg.wanted = want;
- 	arg.dirty = flushing;
+ 	arg->op = op;
+ 	arg->caps = cap->implemented;
+ 	arg->wanted = want;
+ 	arg->dirty = flushing;
  
- 	arg.seq = cap->seq;
- 	arg.issue_seq = cap->issue_seq;
- 	arg.mseq = cap->mseq;
- 	arg.time_warp_seq = ci->i_time_warp_seq;
+ 	arg->seq = cap->seq;
+ 	arg->issue_seq = cap->issue_seq;
+ 	arg->mseq = cap->mseq;
+ 	arg->time_warp_seq = ci->i_time_warp_seq;
  
- 	arg.uid = inode->i_uid;
- 	arg.gid = inode->i_gid;
- 	arg.mode = inode->i_mode;
+ 	arg->uid = inode->i_uid;
+ 	arg->gid = inode->i_gid;
+ 	arg->mode = inode->i_mode;
  
- 	arg.inline_data = ci->i_inline_version != CEPH_INLINE_NONE;
+ 	arg->inline_data = ci->i_inline_version != CEPH_INLINE_NONE;
  	if (!(flags & CEPH_CLIENT_CAPS_PENDING_CAPSNAP) &&
  	    !list_empty(&ci->i_cap_snaps)) {
  		struct ceph_cap_snap *capsnap;
@@@ -1434,22 -1414,35 +1433,44 @@@
  			}
  		}
  	}
- 	arg.flags = flags;
- 
- 	spin_unlock(&ci->i_ceph_lock);
+ 	arg->flags = flags;
+ }
  
- 	ceph_buffer_put(old_blob);
+ /*
+  * Send a cap msg on the given inode.
+  *
+  * Caller should hold snap_rwsem (read), s_mutex.
+  */
+ static void __send_cap(struct ceph_mds_client *mdsc, struct cap_msg_args *arg,
+ 		       struct ceph_inode_info *ci)
+ {
+ 	struct inode *inode = &ci->vfs_inode;
+ 	int ret;
  
- 	ret = send_cap_msg(&arg);
+ 	ret = send_cap_msg(arg);
  	if (ret < 0) {
++<<<<<<< HEAD
 +		dout("error sending cap msg, must requeue %p\n", inode);
 +		delayed = 1;
++=======
+ 		pr_err("error sending cap msg, ino (%llx.%llx) "
+ 		       "flushing %s tid %llu, requeue\n",
+ 		       ceph_vinop(inode), ceph_cap_string(arg->dirty),
+ 		       arg->flush_tid);
+ 		spin_lock(&ci->i_ceph_lock);
+ 		__cap_delay_requeue(mdsc, ci);
+ 		spin_unlock(&ci->i_ceph_lock);
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  	}
  
- 	if (wake)
- 		wake_up_all(&ci->i_cap_wq);
+ 	ceph_buffer_put(arg->old_xattr_buf);
  
++<<<<<<< HEAD
 +	return delayed;
++=======
+ 	if (arg->wake)
+ 		wake_up_all(&ci->i_cap_wq);
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  }
  
  static inline int __send_flush_snap(struct inode *inode,
@@@ -2118,12 -2090,13 +2143,21 @@@ ack
  		}
  
  		mds = cap->mds;  /* remember mds, so we don't repeat */
 -
 +		sent++;
 +
++<<<<<<< HEAD
 +		/* __send_cap drops i_ceph_lock */
 +		delayed += __send_cap(mdsc, cap, CEPH_CAP_OP_UPDATE, 0,
 +				cap_used, want, retain, flushing,
 +				flush_tid, oldest_flush_tid);
++=======
+ 		__prep_cap(&arg, cap, CEPH_CAP_OP_UPDATE, 0, cap_used, want,
+ 			   retain, flushing, flush_tid, oldest_flush_tid);
+ 		spin_unlock(&ci->i_ceph_lock);
+ 
+ 		__send_cap(mdsc, &arg, ci);
+ 
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  		goto retry; /* retake i_ceph_lock and restart our cap scan. */
  	}
  
@@@ -2166,7 -2135,7 +2200,11 @@@ retry
  retry_locked:
  	if (ci->i_dirty_caps && ci->i_auth_cap) {
  		struct ceph_cap *cap = ci->i_auth_cap;
++<<<<<<< HEAD
 +		int delayed;
++=======
+ 		struct cap_msg_args arg;
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  
  		if (session != cap->session) {
  			spin_unlock(&ci->i_ceph_lock);
@@@ -2194,19 -2163,13 +2232,29 @@@
  		flush_tid = __mark_caps_flushing(inode, session, true,
  						 &oldest_flush_tid);
  
++<<<<<<< HEAD
 +		/* __send_cap drops i_ceph_lock */
 +		delayed = __send_cap(mdsc, cap, CEPH_CAP_OP_FLUSH,
 +				     CEPH_CLIENT_CAPS_SYNC,
 +				     __ceph_caps_used(ci),
 +				     __ceph_caps_wanted(ci),
 +				     (cap->issued | cap->implemented),
 +				     flushing, flush_tid, oldest_flush_tid);
 +
 +		if (delayed) {
 +			spin_lock(&ci->i_ceph_lock);
 +			__cap_delay_requeue(mdsc, ci, true);
 +			spin_unlock(&ci->i_ceph_lock);
 +		}
++=======
+ 		__prep_cap(&arg, cap, CEPH_CAP_OP_FLUSH, CEPH_CLIENT_CAPS_SYNC,
+ 			   __ceph_caps_used(ci), __ceph_caps_wanted(ci),
+ 			   (cap->issued | cap->implemented),
+ 			   flushing, flush_tid, oldest_flush_tid);
+ 		spin_unlock(&ci->i_ceph_lock);
+ 
+ 		__send_cap(mdsc, &arg, ci);
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  	} else {
  		if (!list_empty(&ci->i_cap_flush_list)) {
  			struct ceph_cap_flush *cf =
@@@ -2408,24 -2371,19 +2456,35 @@@ static void __kick_flushing_caps(struc
  		first_tid = cf->tid + 1;
  
  		if (cf->caps) {
+ 			struct cap_msg_args arg;
+ 
  			dout("kick_flushing_caps %p cap %p tid %llu %s\n",
  			     inode, cap, cf->tid, ceph_cap_string(cf->caps));
++<<<<<<< HEAD
 +			ci->i_ceph_flags |= CEPH_I_NODELAY;
 +
 +			ret = __send_cap(mdsc, cap, CEPH_CAP_OP_FLUSH,
++=======
+ 			__prep_cap(&arg, cap, CEPH_CAP_OP_FLUSH,
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  					 (cf->tid < last_snap_flush ?
  					  CEPH_CLIENT_CAPS_PENDING_CAPSNAP : 0),
  					  __ceph_caps_used(ci),
  					  __ceph_caps_wanted(ci),
  					  (cap->issued | cap->implemented),
  					  cf->caps, cf->tid, oldest_flush_tid);
++<<<<<<< HEAD
 +			if (ret) {
 +				pr_err("kick_flushing_caps: error sending "
 +					"cap flush, ino (%llx.%llx) "
 +					"tid %llu flushing %s\n",
 +					ceph_vinop(inode), cf->tid,
 +					ceph_cap_string(cf->caps));
 +			}
++=======
+ 			spin_unlock(&ci->i_ceph_lock);
+ 			__send_cap(mdsc, &arg, ci);
++>>>>>>> 0a454bdd501a (ceph: reorganize __send_cap for less spinlock abuse)
  		} else {
  			struct ceph_cap_snap *capsnap =
  					container_of(cf, struct ceph_cap_snap,
* Unmerged path fs/ceph/caps.c
