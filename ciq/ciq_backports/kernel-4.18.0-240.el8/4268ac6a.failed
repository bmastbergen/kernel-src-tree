dma-direct: don't check swiotlb=force in dma_direct_map_resource

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 4268ac6ae5870af10a7417b22990d615f72f77e2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/4268ac6a.failed

When mapping resources we can't just use swiotlb ram for bounce
buffering.  Switch to a direct dma_capable check instead.

Fixes: cfced786969c ("dma-mapping: remove the default map_resource implementation")
	Reported-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
	Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
(cherry picked from commit 4268ac6ae5870af10a7417b22990d615f72f77e2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index 9f38de9d2578,a479bd2d1e8b..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -370,6 -407,73 +370,76 @@@ out_unmap
  }
  EXPORT_SYMBOL(dma_direct_map_sg);
  
++<<<<<<< HEAD
++=======
+ dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	dma_addr_t dma_addr = paddr;
+ 
+ 	if (unlikely(!dma_capable(dev, dma_addr, size))) {
+ 		report_addr(dev, dma_addr, size);
+ 		return DMA_MAPPING_ERROR;
+ 	}
+ 
+ 	return dma_addr;
+ }
+ EXPORT_SYMBOL(dma_direct_map_resource);
+ 
+ int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	struct page *page = dma_direct_to_page(dev, dma_addr);
+ 	int ret;
+ 
+ 	ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+ 	if (!ret)
+ 		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_MMU
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return dev_is_dma_coherent(dev) ||
+ 		IS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	unsigned long user_count = vma_pages(vma);
+ 	unsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	unsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));
+ 	int ret = -ENXIO;
+ 
+ 	vma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);
+ 
+ 	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
+ 		return ret;
+ 
+ 	if (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)
+ 		return -ENXIO;
+ 	return remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,
+ 			user_count << PAGE_SHIFT, vma->vm_page_prot);
+ }
+ #else /* CONFIG_MMU */
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return false;
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ #endif /* CONFIG_MMU */
+ 
++>>>>>>> 4268ac6ae587 (dma-direct: don't check swiotlb=force in dma_direct_map_resource)
  /*
   * Because 32-bit DMA masks are so common we expect every architecture to be
   * able to satisfy them - either by not supporting more physical memory, or by
* Unmerged path kernel/dma/direct.c
