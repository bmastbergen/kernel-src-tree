mm: memcg/slab: cache page number in memcg_(un)charge_slab()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Roman Gushchin <guro@fb.com>
commit 9c315e4d7d8c3bddad3893777bbab4164b298818
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/9c315e4d.failed

There are many places in memcg_charge_slab() and memcg_uncharge_slab()
which are calculating the number of pages to charge, css references to
grab etc depending on the order of the slab page.

Let's simplify the code by calculating it once and caching in the local
variable.

	Signed-off-by: Roman Gushchin <guro@fb.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
Link: http://lkml.kernel.org/r/20200109202659.752357-6-guro@fb.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 9c315e4d7d8c3bddad3893777bbab4164b298818)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slab.h
diff --cc mm/slab.h
index b10310f68736,43f8ce4aa325..000000000000
--- a/mm/slab.h
+++ b/mm/slab.h
@@@ -305,12 -370,12 +306,17 @@@ static __always_inline int memcg_charge
  	if (ret)
  		goto out;
  
++<<<<<<< HEAD
 +	lruvec = mem_cgroup_lruvec(page_pgdat(page), memcg);
 +	mod_lruvec_state(lruvec, cache_vmstat_idx(s), 1 << order);
++=======
+ 	lruvec = mem_cgroup_lruvec(memcg, page_pgdat(page));
+ 	mod_lruvec_state(lruvec, cache_vmstat_idx(s), nr_pages);
++>>>>>>> 9c315e4d7d8c (mm: memcg/slab: cache page number in memcg_(un)charge_slab())
  
  	/* transer try_charge() page references to kmem_cache */
- 	percpu_ref_get_many(&s->memcg_params.refcnt, 1 << order);
- 	css_put_many(&memcg->css, 1 << order);
+ 	percpu_ref_get_many(&s->memcg_params.refcnt, nr_pages);
+ 	css_put_many(&memcg->css, nr_pages);
  out:
  	css_put(&memcg->css);
  	return ret;
@@@ -329,12 -395,12 +336,18 @@@ static __always_inline void memcg_uncha
  	rcu_read_lock();
  	memcg = READ_ONCE(s->memcg_params.memcg);
  	if (likely(!mem_cgroup_is_root(memcg))) {
++<<<<<<< HEAD
 +		lruvec = mem_cgroup_lruvec(page_pgdat(page), memcg);
 +		mod_lruvec_state(lruvec, cache_vmstat_idx(s), -(1 << order));
 +		memcg_kmem_uncharge_memcg(memcg, order);
++=======
+ 		lruvec = mem_cgroup_lruvec(memcg, page_pgdat(page));
+ 		mod_lruvec_state(lruvec, cache_vmstat_idx(s), -nr_pages);
+ 		memcg_kmem_uncharge_memcg(memcg, nr_pages);
++>>>>>>> 9c315e4d7d8c (mm: memcg/slab: cache page number in memcg_(un)charge_slab())
  	} else {
  		mod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),
- 				    -(1 << order));
+ 				    -nr_pages);
  	}
  	rcu_read_unlock();
  
* Unmerged path mm/slab.h
