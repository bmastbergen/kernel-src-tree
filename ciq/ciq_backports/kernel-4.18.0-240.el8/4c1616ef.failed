powerpc/mm: move FSL_BOOK3 version of update_mmu_cache()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [powerpc] mm: move FSL_BOOK3 version of update_mmu_cache() (Greg Kurz) [1748772]
Rebuild_FUZZ: 92.31%
commit-author Christophe Leroy <christophe.leroy@c-s.fr>
commit 4c1616ef036ffaaea95a29d7b6abf9d3e8eb9d92
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/4c1616ef.failed

Move FSL_BOOK3E version of update_mmu_cache() at the same
place as book3e_hugetlb_preload() as update_mmu_cache() is
the only user of book3e_hugetlb_preload().

	Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/4d69fdc86df9c74adc71a60331a86f6afb8b5e9e.1565933217.git.christophe.leroy@c-s.fr

(cherry picked from commit 4c1616ef036ffaaea95a29d7b6abf9d3e8eb9d92)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/hugetlb.h
#	arch/powerpc/mm/mem.c
diff --cc arch/powerpc/include/asm/hugetlb.h
index 3225eb6402cc,bd6504c28c2f..000000000000
--- a/arch/powerpc/include/asm/hugetlb.h
+++ b/arch/powerpc/include/asm/hugetlb.h
@@@ -101,18 -31,7 +101,22 @@@ static inline int is_hugepage_only_rang
  	return 0;
  }
  
++<<<<<<< HEAD
 +void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
 +			    pte_t pte);
 +#ifdef CONFIG_PPC_8xx
 +static inline void flush_hugetlb_page(struct vm_area_struct *vma,
 +				      unsigned long vmaddr)
 +{
 +	flush_tlb_page(vma, vmaddr);
 +}
 +#else
 +void flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr);
 +#endif
 +
++=======
+ #define __HAVE_ARCH_HUGETLB_FREE_PGD_RANGE
++>>>>>>> 4c1616ef036f (powerpc/mm: move FSL_BOOK3 version of update_mmu_cache())
  void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
  			    unsigned long end, unsigned long floor,
  			    unsigned long ceiling);
diff --cc arch/powerpc/mm/mem.c
index 54211ad167a4,58d11362eee4..000000000000
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@@ -514,13 -456,8 +514,18 @@@ void update_mmu_cache(struct vm_area_st
  	}
  
  	hash_preload(vma->vm_mm, address, is_exec, trap);
++<<<<<<< HEAD
 +#endif /* CONFIG_PPC_STD_MMU */
 +#if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
 +	&& defined(CONFIG_HUGETLB_PAGE)
 +	if (is_vm_hugetlb_page(vma))
 +		book3e_hugetlb_preload(vma, address, *ptep);
 +#endif
 +}
++=======
+ }
+ #endif /* CONFIG_PPC_BOOK3S */
++>>>>>>> 4c1616ef036f (powerpc/mm: move FSL_BOOK3 version of update_mmu_cache())
  
  /*
   * System memory should not be in /proc/iomem but various tools expect it
* Unmerged path arch/powerpc/include/asm/hugetlb.h
diff --git a/arch/powerpc/mm/hugetlbpage-book3e.c b/arch/powerpc/mm/hugetlbpage-book3e.c
index f84ec46cdb26..336c811f9eb1 100644
--- a/arch/powerpc/mm/hugetlbpage-book3e.c
+++ b/arch/powerpc/mm/hugetlbpage-book3e.c
@@ -131,8 +131,8 @@ static inline int book3e_tlb_exists(unsigned long ea, unsigned long pid)
 	return found;
 }
 
-void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
-			    pte_t pte)
+static void
+book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea, pte_t pte)
 {
 	unsigned long mas1, mas2;
 	u64 mas7_3;
@@ -197,6 +197,18 @@ void book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,
 	local_irq_restore(flags);
 }
 
+/*
+ * This is called at the end of handling a user page fault, when the
+ * fault has been handled by updating a PTE in the linux page tables.
+ *
+ * This must always be called with the pte lock held.
+ */
+void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep)
+{
+	if (is_vm_hugetlb_page(vma))
+		book3e_hugetlb_preload(vma, address, *ptep);
+}
+
 void flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr)
 {
 	struct hstate *hstate = hstate_file(vma->vm_file);
* Unmerged path arch/powerpc/mm/mem.c
