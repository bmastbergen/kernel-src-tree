hugetlbfs: remove hugetlb_add_hstate() warning for existing hstate

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mike Kravetz <mike.kravetz@oracle.com>
commit 38237830882ba8d425a397066982d5e32b4ced21
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/38237830.failed

hugetlb_add_hstate() prints a warning if the hstate already exists.  This
was originally done as part of kernel command line parsing.  If
'hugepagesz=' was specified more than once, the warning

	pr_warn("hugepagesz= specified twice, ignoring\n");

would be printed.

Some architectures want to enable all huge page sizes.  They would call
hugetlb_add_hstate for all supported sizes.  However, this was done after
command line processing and as a result hstates could have already been
created for some sizes.  To make sure no warning were printed, there would
often be code like:

	if (!size_to_hstate(size)
		hugetlb_add_hstate(ilog2(size) - PAGE_SHIFT)

The only time we want to print the warning is as the result of command
line processing.  So, remove the warning from hugetlb_add_hstate and add
it to the single arch independent routine processing "hugepagesz=".  After
this, calls to size_to_hstate() in arch specific code can be removed and
hugetlb_add_hstate can be called without worrying about warning messages.

[mike.kravetz@oracle.com: fix hugetlb initialization]
  Link: http://lkml.kernel.org/r/4c36c6ce-3774-78fa-abc4-b7346bf24348@oracle.com
  Link: http://lkml.kernel.org/r/20200428205614.246260-5-mike.kravetz@oracle.com
	Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Tested-by: Anders Roxell <anders.roxell@linaro.org>
	Acked-by: Mina Almasry <almasrymina@google.com>
	Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>	[s390]
	Acked-by: Will Deacon <will@kernel.org>
	Cc: Albert Ou <aou@eecs.berkeley.edu>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: Christophe Leroy <christophe.leroy@c-s.fr>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: David S. Miller <davem@davemloft.net>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Longpeng <longpeng2@huawei.com>
	Cc: Nitesh Narayan Lal <nitesh@redhat.com>
	Cc: Palmer Dabbelt <palmer@dabbelt.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Paul Walmsley <paul.walmsley@sifive.com>
	Cc: Peter Xu <peterx@redhat.com>
	Cc: Randy Dunlap <rdunlap@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vasily Gorbik <gor@linux.ibm.com>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
	Cc: Qian Cai <cai@lca.pw>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
Link: http://lkml.kernel.org/r/20200417185049.275845-4-mike.kravetz@oracle.com
Link: http://lkml.kernel.org/r/20200428205614.246260-4-mike.kravetz@oracle.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 38237830882ba8d425a397066982d5e32b4ced21)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/mm/hugetlbpage.c
#	arch/riscv/mm/hugetlbpage.c
#	mm/hugetlb.c
diff --cc arch/powerpc/mm/hugetlbpage.c
index 8110b9827c69,4d5ed1093615..000000000000
--- a/arch/powerpc/mm/hugetlbpage.c
+++ b/arch/powerpc/mm/hugetlbpage.c
@@@ -635,12 -574,17 +635,19 @@@ static int __init add_huge_page_size(un
  
  	BUG_ON(mmu_psize_defs[mmu_psize].shift != shift);
  
 -	return true;
 -}
 +	/* Return if huge page size has already been setup */
 +	if (size_to_hstate(size))
 +		return 0;
  
 -static int __init add_huge_page_size(unsigned long long size)
 -{
 -	int shift = __ffs(size);
 +	hugetlb_add_hstate(shift - PAGE_SHIFT);
  
++<<<<<<< HEAD
++=======
+ 	if (!arch_hugetlb_valid_size((unsigned long)size))
+ 		return -EINVAL;
+ 
+ 	hugetlb_add_hstate(shift - PAGE_SHIFT);
++>>>>>>> 38237830882b (hugetlbfs: remove hugetlb_add_hstate() warning for existing hstate)
  	return 0;
  }
  
diff --cc mm/hugetlb.c
index 449fb9efecb9,2ae0e506cfc7..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -2944,12 -3329,43 +2942,31 @@@ static int __init hugetlb_nrpages_setup
  }
  __setup("hugepages=", hugetlb_nrpages_setup);
  
 -static int __init hugepagesz_setup(char *s)
 +static int __init hugetlb_default_setup(char *s)
  {
++<<<<<<< HEAD
 +	default_hstate_size = memparse(s, &s);
++=======
+ 	unsigned long size;
+ 
+ 	size = (unsigned long)memparse(s, NULL);
+ 
+ 	if (!arch_hugetlb_valid_size(size)) {
+ 		parsed_valid_hugepagesz = false;
+ 		pr_err("HugeTLB: unsupported hugepagesz %s\n", s);
+ 		return 0;
+ 	}
+ 
+ 	if (size_to_hstate(size)) {
+ 		pr_warn("HugeTLB: hugepagesz %s specified twice, ignoring\n", s);
+ 		return 0;
+ 	}
+ 
+ 	hugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);
++>>>>>>> 38237830882b (hugetlbfs: remove hugetlb_add_hstate() warning for existing hstate)
  	return 1;
  }
 -__setup("hugepagesz=", hugepagesz_setup);
 -
 -static int __init default_hugepagesz_setup(char *s)
 -{
 -	unsigned long size;
 -
 -	size = (unsigned long)memparse(s, NULL);
 -
 -	if (!arch_hugetlb_valid_size(size)) {
 -		pr_err("HugeTLB: unsupported default_hugepagesz %s\n", s);
 -		return 0;
 -	}
 -
 -	default_hstate_size = size;
 -	return 1;
 -}
 -__setup("default_hugepagesz=", default_hugepagesz_setup);
 +__setup("default_hugepagesz=", hugetlb_default_setup);
  
  static unsigned int cpuset_mems_nr(unsigned int *array)
  {
* Unmerged path arch/riscv/mm/hugetlbpage.c
diff --git a/arch/arm64/mm/hugetlbpage.c b/arch/arm64/mm/hugetlbpage.c
index 2e6c432a5f0c..b041b58df9e6 100644
--- a/arch/arm64/mm/hugetlbpage.c
+++ b/arch/arm64/mm/hugetlbpage.c
@@ -429,22 +429,14 @@ void huge_ptep_clear_flush(struct vm_area_struct *vma,
 	clear_flush(vma->vm_mm, addr, ptep, pgsize, ncontig);
 }
 
-static void __init add_huge_page_size(unsigned long size)
-{
-	if (size_to_hstate(size))
-		return;
-
-	hugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);
-}
-
 static int __init hugetlbpage_init(void)
 {
 #ifdef CONFIG_ARM64_4K_PAGES
-	add_huge_page_size(PUD_SIZE);
+	hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
 #endif
-	add_huge_page_size(CONT_PMD_SIZE);
-	add_huge_page_size(PMD_SIZE);
-	add_huge_page_size(CONT_PTE_SIZE);
+	hugetlb_add_hstate((CONT_PMD_SHIFT + PMD_SHIFT) - PAGE_SHIFT);
+	hugetlb_add_hstate(PMD_SHIFT - PAGE_SHIFT);
+	hugetlb_add_hstate((CONT_PTE_SHIFT + PAGE_SHIFT) - PAGE_SHIFT);
 
 	return 0;
 }
* Unmerged path arch/powerpc/mm/hugetlbpage.c
* Unmerged path arch/riscv/mm/hugetlbpage.c
diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c
index 578ec3da410a..5734aac7672d 100644
--- a/arch/sparc/mm/init_64.c
+++ b/arch/sparc/mm/init_64.c
@@ -326,23 +326,12 @@ static void __update_mmu_tsb_insert(struct mm_struct *mm, unsigned long tsb_inde
 }
 
 #ifdef CONFIG_HUGETLB_PAGE
-static void __init add_huge_page_size(unsigned long size)
-{
-	unsigned int order;
-
-	if (size_to_hstate(size))
-		return;
-
-	order = ilog2(size) - PAGE_SHIFT;
-	hugetlb_add_hstate(order);
-}
-
 static int __init hugetlbpage_init(void)
 {
-	add_huge_page_size(1UL << HPAGE_64K_SHIFT);
-	add_huge_page_size(1UL << HPAGE_SHIFT);
-	add_huge_page_size(1UL << HPAGE_256MB_SHIFT);
-	add_huge_page_size(1UL << HPAGE_2GB_SHIFT);
+	hugetlb_add_hstate(HPAGE_64K_SHIFT - PAGE_SHIFT);
+	hugetlb_add_hstate(HPAGE_SHIFT - PAGE_SHIFT);
+	hugetlb_add_hstate(HPAGE_256MB_SHIFT - PAGE_SHIFT);
+	hugetlb_add_hstate(HPAGE_2GB_SHIFT - PAGE_SHIFT);
 
 	return 0;
 }
diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c
index 00b296617ca4..68f14a467090 100644
--- a/arch/x86/mm/hugetlbpage.c
+++ b/arch/x86/mm/hugetlbpage.c
@@ -207,7 +207,7 @@ __setup("hugepagesz=", setup_hugepagesz);
 static __init int gigantic_pages_init(void)
 {
 	/* With compaction or CMA we can allocate gigantic pages at runtime */
-	if (boot_cpu_has(X86_FEATURE_GBPAGES) && !size_to_hstate(1UL << PUD_SHIFT))
+	if (boot_cpu_has(X86_FEATURE_GBPAGES))
 		hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
 	return 0;
 }
* Unmerged path mm/hugetlb.c
