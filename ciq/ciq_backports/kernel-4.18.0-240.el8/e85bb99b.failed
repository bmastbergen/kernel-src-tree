iommu/vt-d: Add get_domain_info() helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Lu Baolu <baolu.lu@linux.intel.com>
commit e85bb99b79ca5ad2681612a7bb22f94cc2c71866
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e85bb99b.failed

Add a get_domain_info() helper to retrieve the valid per-device
iommu private data.

	Signed-off-by: Lu Baolu <baolu.lu@linux.intel.com>
Link: https://lore.kernel.org/r/20200516062101.29541-10-baolu.lu@linux.intel.com
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit e85bb99b79ca5ad2681612a7bb22f94cc2c71866)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/intel-iommu.c
#	include/linux/intel-iommu.h
diff --cc drivers/iommu/intel-iommu.c
index e339a5e3c53b,a13b723ca38d..000000000000
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@@ -379,7 -365,22 +379,26 @@@ EXPORT_SYMBOL_GPL(intel_iommu_gfx_mappe
  
  #define DUMMY_DEVICE_DOMAIN_INFO ((struct device_domain_info *)(-1))
  #define DEFER_DEVICE_DOMAIN_INFO ((struct device_domain_info *)(-2))
++<<<<<<< HEAD
 +static DEFINE_SPINLOCK(device_domain_lock);
++=======
+ struct device_domain_info *get_domain_info(struct device *dev)
+ {
+ 	struct device_domain_info *info;
+ 
+ 	if (!dev)
+ 		return NULL;
+ 
+ 	info = dev->archdata.iommu;
+ 	if (unlikely(info == DUMMY_DEVICE_DOMAIN_INFO ||
+ 		     info == DEFER_DEVICE_DOMAIN_INFO))
+ 		return NULL;
+ 
+ 	return info;
+ }
+ 
+ DEFINE_SPINLOCK(device_domain_lock);
++>>>>>>> e85bb99b79ca (iommu/vt-d: Add get_domain_info() helper)
  static LIST_HEAD(device_domain_list);
  
  #define device_needs_bounce(d) (!intel_no_bounce && dev_is_pci(d) &&	\
@@@ -2430,8 -2440,11 +2449,8 @@@ static struct dmar_domain *find_domain(
  	if (unlikely(attach_deferred(dev) || iommu_dummy(dev)))
  		return NULL;
  
 -	if (dev_is_pci(dev))
 -		dev = &pci_real_dma_dev(to_pci_dev(dev))->dev;
 -
  	/* No lock here, assumes no domain exit in normal case */
- 	info = dev->archdata.iommu;
+ 	info = get_domain_info(dev);
  	if (likely(info))
  		return info->domain;
  
@@@ -5217,9 -5341,179 +5235,182 @@@ static void intel_iommu_aux_detach_devi
  	aux_domain_remove_dev(to_dmar_domain(domain), dev);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * 2D array for converting and sanitizing IOMMU generic TLB granularity to
+  * VT-d granularity. Invalidation is typically included in the unmap operation
+  * as a result of DMA or VFIO unmap. However, for assigned devices guest
+  * owns the first level page tables. Invalidations of translation caches in the
+  * guest are trapped and passed down to the host.
+  *
+  * vIOMMU in the guest will only expose first level page tables, therefore
+  * we do not support IOTLB granularity for request without PASID (second level).
+  *
+  * For example, to find the VT-d granularity encoding for IOTLB
+  * type and page selective granularity within PASID:
+  * X: indexed by iommu cache type
+  * Y: indexed by enum iommu_inv_granularity
+  * [IOMMU_CACHE_INV_TYPE_IOTLB][IOMMU_INV_GRANU_ADDR]
+  */
+ 
+ const static int
+ inv_type_granu_table[IOMMU_CACHE_INV_TYPE_NR][IOMMU_INV_GRANU_NR] = {
+ 	/*
+ 	 * PASID based IOTLB invalidation: PASID selective (per PASID),
+ 	 * page selective (address granularity)
+ 	 */
+ 	{-EINVAL, QI_GRAN_NONG_PASID, QI_GRAN_PSI_PASID},
+ 	/* PASID based dev TLBs */
+ 	{-EINVAL, -EINVAL, QI_DEV_IOTLB_GRAN_PASID_SEL},
+ 	/* PASID cache */
+ 	{-EINVAL, -EINVAL, -EINVAL}
+ };
+ 
+ static inline int to_vtd_granularity(int type, int granu)
+ {
+ 	return inv_type_granu_table[type][granu];
+ }
+ 
+ static inline u64 to_vtd_size(u64 granu_size, u64 nr_granules)
+ {
+ 	u64 nr_pages = (granu_size * nr_granules) >> VTD_PAGE_SHIFT;
+ 
+ 	/* VT-d size is encoded as 2^size of 4K pages, 0 for 4k, 9 for 2MB, etc.
+ 	 * IOMMU cache invalidate API passes granu_size in bytes, and number of
+ 	 * granu size in contiguous memory.
+ 	 */
+ 	return order_base_2(nr_pages);
+ }
+ 
+ #ifdef CONFIG_INTEL_IOMMU_SVM
+ static int
+ intel_iommu_sva_invalidate(struct iommu_domain *domain, struct device *dev,
+ 			   struct iommu_cache_invalidate_info *inv_info)
+ {
+ 	struct dmar_domain *dmar_domain = to_dmar_domain(domain);
+ 	struct device_domain_info *info;
+ 	struct intel_iommu *iommu;
+ 	unsigned long flags;
+ 	int cache_type;
+ 	u8 bus, devfn;
+ 	u16 did, sid;
+ 	int ret = 0;
+ 	u64 size = 0;
+ 
+ 	if (!inv_info || !dmar_domain ||
+ 	    inv_info->version != IOMMU_CACHE_INVALIDATE_INFO_VERSION_1)
+ 		return -EINVAL;
+ 
+ 	if (!dev || !dev_is_pci(dev))
+ 		return -ENODEV;
+ 
+ 	iommu = device_to_iommu(dev, &bus, &devfn);
+ 	if (!iommu)
+ 		return -ENODEV;
+ 
+ 	if (!(dmar_domain->flags & DOMAIN_FLAG_NESTING_MODE))
+ 		return -EINVAL;
+ 
+ 	spin_lock_irqsave(&device_domain_lock, flags);
+ 	spin_lock(&iommu->lock);
+ 	info = get_domain_info(dev);
+ 	if (!info) {
+ 		ret = -EINVAL;
+ 		goto out_unlock;
+ 	}
+ 	did = dmar_domain->iommu_did[iommu->seq_id];
+ 	sid = PCI_DEVID(bus, devfn);
+ 
+ 	/* Size is only valid in address selective invalidation */
+ 	if (inv_info->granularity != IOMMU_INV_GRANU_PASID)
+ 		size = to_vtd_size(inv_info->addr_info.granule_size,
+ 				   inv_info->addr_info.nb_granules);
+ 
+ 	for_each_set_bit(cache_type,
+ 			 (unsigned long *)&inv_info->cache,
+ 			 IOMMU_CACHE_INV_TYPE_NR) {
+ 		int granu = 0;
+ 		u64 pasid = 0;
+ 
+ 		granu = to_vtd_granularity(cache_type, inv_info->granularity);
+ 		if (granu == -EINVAL) {
+ 			pr_err_ratelimited("Invalid cache type and granu combination %d/%d\n",
+ 					   cache_type, inv_info->granularity);
+ 			break;
+ 		}
+ 
+ 		/*
+ 		 * PASID is stored in different locations based on the
+ 		 * granularity.
+ 		 */
+ 		if (inv_info->granularity == IOMMU_INV_GRANU_PASID &&
+ 		    (inv_info->pasid_info.flags & IOMMU_INV_PASID_FLAGS_PASID))
+ 			pasid = inv_info->pasid_info.pasid;
+ 		else if (inv_info->granularity == IOMMU_INV_GRANU_ADDR &&
+ 			 (inv_info->addr_info.flags & IOMMU_INV_ADDR_FLAGS_PASID))
+ 			pasid = inv_info->addr_info.pasid;
+ 
+ 		switch (BIT(cache_type)) {
+ 		case IOMMU_CACHE_INV_TYPE_IOTLB:
+ 			if (inv_info->granularity == IOMMU_INV_GRANU_ADDR &&
+ 			    size &&
+ 			    (inv_info->addr_info.addr & ((BIT(VTD_PAGE_SHIFT + size)) - 1))) {
+ 				pr_err_ratelimited("Address out of range, 0x%llx, size order %llu\n",
+ 						   inv_info->addr_info.addr, size);
+ 				ret = -ERANGE;
+ 				goto out_unlock;
+ 			}
+ 
+ 			/*
+ 			 * If granu is PASID-selective, address is ignored.
+ 			 * We use npages = -1 to indicate that.
+ 			 */
+ 			qi_flush_piotlb(iommu, did, pasid,
+ 					mm_to_dma_pfn(inv_info->addr_info.addr),
+ 					(granu == QI_GRAN_NONG_PASID) ? -1 : 1 << size,
+ 					inv_info->addr_info.flags & IOMMU_INV_ADDR_FLAGS_LEAF);
+ 
+ 			/*
+ 			 * Always flush device IOTLB if ATS is enabled. vIOMMU
+ 			 * in the guest may assume IOTLB flush is inclusive,
+ 			 * which is more efficient.
+ 			 */
+ 			if (info->ats_enabled)
+ 				qi_flush_dev_iotlb_pasid(iommu, sid,
+ 						info->pfsid, pasid,
+ 						info->ats_qdep,
+ 						inv_info->addr_info.addr,
+ 						size, granu);
+ 			break;
+ 		case IOMMU_CACHE_INV_TYPE_DEV_IOTLB:
+ 			if (info->ats_enabled)
+ 				qi_flush_dev_iotlb_pasid(iommu, sid,
+ 						info->pfsid, pasid,
+ 						info->ats_qdep,
+ 						inv_info->addr_info.addr,
+ 						size, granu);
+ 			else
+ 				pr_warn_ratelimited("Passdown device IOTLB flush w/o ATS!\n");
+ 			break;
+ 		default:
+ 			dev_err_ratelimited(dev, "Unsupported IOMMU invalidation type %d\n",
+ 					    cache_type);
+ 			ret = -EINVAL;
+ 		}
+ 	}
+ out_unlock:
+ 	spin_unlock(&iommu->lock);
+ 	spin_unlock_irqrestore(&device_domain_lock, flags);
+ 
+ 	return ret;
+ }
+ #endif
+ 
++>>>>>>> e85bb99b79ca (iommu/vt-d: Add get_domain_info() helper)
  static int intel_iommu_map(struct iommu_domain *domain,
  			   unsigned long iova, phys_addr_t hpa,
 -			   size_t size, int iommu_prot, gfp_t gfp)
 +			   size_t size, int iommu_prot)
  {
  	struct dmar_domain *dmar_domain = to_dmar_domain(domain);
  	u64 max_addr;
diff --cc include/linux/intel-iommu.h
index 66826a69329c,caa179e806fc..000000000000
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@@ -672,6 -713,8 +672,11 @@@ int for_each_device_domain(int (*fn)(st
  				     void *data), void *data);
  void iommu_flush_write_buffer(struct intel_iommu *iommu);
  int intel_iommu_enable_pasid(struct intel_iommu *iommu, struct device *dev);
++<<<<<<< HEAD
++=======
+ struct dmar_domain *find_domain(struct device *dev);
+ struct device_domain_info *get_domain_info(struct device *dev);
++>>>>>>> e85bb99b79ca (iommu/vt-d: Add get_domain_info() helper)
  
  #ifdef CONFIG_INTEL_IOMMU_SVM
  extern void intel_svm_check(struct intel_iommu *iommu);
* Unmerged path drivers/iommu/intel-iommu.c
diff --git a/drivers/iommu/intel-pasid.c b/drivers/iommu/intel-pasid.c
index 732bfee228df..aa14516ad232 100644
--- a/drivers/iommu/intel-pasid.c
+++ b/drivers/iommu/intel-pasid.c
@@ -130,7 +130,7 @@ int intel_pasid_alloc_table(struct device *dev)
 	int size;
 
 	might_sleep();
-	info = dev->archdata.iommu;
+	info = get_domain_info(dev);
 	if (WARN_ON(!info || !dev_is_pci(dev) || info->pasid_table))
 		return -EINVAL;
 
@@ -177,7 +177,7 @@ void intel_pasid_free_table(struct device *dev)
 	struct pasid_entry *table;
 	int i, max_pde;
 
-	info = dev->archdata.iommu;
+	info = get_domain_info(dev);
 	if (!info || !dev_is_pci(dev) || !info->pasid_table)
 		return;
 
@@ -203,7 +203,7 @@ struct pasid_table *intel_pasid_get_table(struct device *dev)
 {
 	struct device_domain_info *info;
 
-	info = dev->archdata.iommu;
+	info = get_domain_info(dev);
 	if (!info)
 		return NULL;
 
@@ -214,7 +214,7 @@ int intel_pasid_get_dev_max_id(struct device *dev)
 {
 	struct device_domain_info *info;
 
-	info = dev->archdata.iommu;
+	info = get_domain_info(dev);
 	if (!info || !info->pasid_table)
 		return 0;
 
@@ -235,7 +235,7 @@ struct pasid_entry *intel_pasid_get_entry(struct device *dev, int pasid)
 		return NULL;
 
 	dir = pasid_table->table;
-	info = dev->archdata.iommu;
+	info = get_domain_info(dev);
 	dir_index = pasid >> PASID_PDE_SHIFT;
 	index = pasid & PASID_PTE_MASK;
 
@@ -430,7 +430,7 @@ devtlb_invalidation_with_pasid(struct intel_iommu *iommu,
 	struct device_domain_info *info;
 	u16 sid, qdep, pfsid;
 
-	info = dev->archdata.iommu;
+	info = get_domain_info(dev);
 	if (!info || !info->ats_enabled)
 		return;
 
diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c
index 519aa64ec28a..3e1f2c7ff0ba 100644
--- a/drivers/iommu/intel-svm.c
+++ b/drivers/iommu/intel-svm.c
@@ -308,7 +308,7 @@ int intel_svm_bind_mm(struct device *dev, int *pasid, int flags, struct svm_dev_
 		goto out;
 	}
 
-	info = dev->archdata.iommu;
+	info = get_domain_info(dev);
 	if (!info || !info->pasid_supported) {
 		kfree(sdev);
 		goto out;
* Unmerged path include/linux/intel-iommu.h
