io_uring: pass in io_kiocb to fill/add CQ handlers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 78e19bbef38362cebff38aa1ca12e2c82bb72eb8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/78e19bbe.failed

This is in preparation for handling CQ ring overflow a bit smarter. We
should not have any functional changes in this patch. Most of the
changes are fairly straight forward, the only ones that stick out a bit
are the ones that change __io_free_req() to take the reference count
into account. If the request hasn't been submitted yet, we know it's
safe to simply ignore references and free it. But let's clean these up
too, as later patches will depend on the caller doing the right thing if
the completion logging grabs a reference to the request.

	Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 78e19bbef38362cebff38aa1ca12e2c82bb72eb8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index fca9cdc96d77,91103fc9771d..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -367,8 -368,11 +367,16 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void __io_free_req(struct io_kiocb *req);
+ static void io_put_req(struct io_kiocb *req, struct io_kiocb **nxtptr);
+ static void io_double_put_req(struct io_kiocb *req);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  
  static struct kmem_cache *req_cachep;
  
@@@ -476,12 -502,52 +484,54 @@@ static inline void io_queue_async_work(
  		switch (req->submit.sqe->opcode) {
  		case IORING_OP_WRITEV:
  		case IORING_OP_WRITE_FIXED:
 -			do_hashed = true;
 +			rw = !(req->rw.ki_flags & IOCB_DIRECT);
  			break;
  		}
 -		if (io_sqe_needs_user(req->submit.sqe))
 -			req->work.flags |= IO_WQ_WORK_NEEDS_USER;
  	}
  
++<<<<<<< HEAD
 +	queue_work(ctx->sqo_wq[rw], &req->work);
++=======
+ 	return do_hashed;
+ }
+ 
+ static inline void io_queue_async_work(struct io_ring_ctx *ctx,
+ 				       struct io_kiocb *req)
+ {
+ 	bool do_hashed = io_prep_async_work(req);
+ 
+ 	trace_io_uring_queue_async_work(ctx, do_hashed, req, &req->work,
+ 					req->flags);
+ 	if (!do_hashed) {
+ 		io_wq_enqueue(ctx->io_wq, &req->work);
+ 	} else {
+ 		io_wq_enqueue_hashed(ctx->io_wq, &req->work,
+ 					file_inode(req->file));
+ 	}
+ }
+ 
+ static void io_kill_timeout(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->timeout.timer);
+ 	if (ret != -1) {
+ 		atomic_inc(&req->ctx->cq_timeouts);
+ 		list_del_init(&req->list);
+ 		io_cqring_fill_event(req, 0);
+ 		io_put_req(req, NULL);
+ 	}
+ }
+ 
+ static void io_kill_timeouts(struct io_ring_ctx *ctx)
+ {
+ 	struct io_kiocb *req, *tmp;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_entry_safe(req, tmp, &ctx->timeout_list, list)
+ 		io_kill_timeout(req);
+ 	spin_unlock_irq(&ctx->completion_lock);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  }
  
  static void io_commit_cqring(struct io_ring_ctx *ctx)
@@@ -516,14 -585,16 +566,19 @@@ static struct io_uring_cqe *io_get_cqri
  		return NULL;
  
  	ctx->cached_cq_tail++;
 -	return &rings->cqes[tail & ctx->cq_mask];
 +	return &ring->cqes[tail & ctx->cq_mask];
  }
  
- static void io_cqring_fill_event(struct io_ring_ctx *ctx, u64 ki_user_data,
- 				 long res)
+ static void io_cqring_fill_event(struct io_kiocb *req, long res)
  {
+ 	struct io_ring_ctx *ctx = req->ctx;
  	struct io_uring_cqe *cqe;
  
++<<<<<<< HEAD
++=======
+ 	trace_io_uring_complete(ctx, req->user_data, res);
+ 
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	/*
  	 * If we can't get a cq entry, userspace overflowed the
  	 * submission (by quite a lot). Increment the overflow count in
@@@ -632,9 -714,28 +687,29 @@@ static void __io_free_req(struct io_kio
  	kmem_cache_free(req_cachep, req);
  }
  
 -static bool io_link_cancel_timeout(struct io_ring_ctx *ctx,
 -				   struct io_kiocb *req)
 +static void io_req_link_next(struct io_kiocb *req)
  {
++<<<<<<< HEAD
++=======
+ 	int ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->timeout.timer);
+ 	if (ret != -1) {
+ 		io_cqring_fill_event(req, -ECANCELED);
+ 		io_commit_cqring(ctx);
+ 		req->flags &= ~REQ_F_LINK;
+ 		io_put_req(req, NULL);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_req_link_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	struct io_kiocb *nxt;
 -	bool wake_ev = false;
  
  	/*
  	 * The list should never be empty when we are called here. But could
@@@ -665,15 -784,34 +740,27 @@@ static void io_fail_links(struct io_kio
  
  	while (!list_empty(&req->link_list)) {
  		link = list_first_entry(&req->link_list, struct io_kiocb, list);
 -		list_del_init(&link->list);
 +		list_del(&link->list);
  
++<<<<<<< HEAD
 +		io_cqring_add_event(req->ctx, link->user_data, -ECANCELED);
 +		__io_free_req(link);
++=======
+ 		trace_io_uring_fail_link(req, link);
+ 
+ 		if ((req->flags & REQ_F_LINK_TIMEOUT) &&
+ 		    link->submit.sqe->opcode == IORING_OP_LINK_TIMEOUT) {
+ 			io_link_cancel_timeout(ctx, link);
+ 		} else {
+ 			io_cqring_fill_event(link, -ECANCELED);
+ 			io_double_put_req(link);
+ 		}
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	}
 -
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -	io_cqring_ev_posted(ctx);
  }
  
 -static void io_free_req(struct io_kiocb *req, struct io_kiocb **nxt)
 +static void io_free_req(struct io_kiocb *req)
  {
 -	if (likely(!(req->flags & REQ_F_LINK))) {
 -		__io_free_req(req);
 -		return;
 -	}
 -
  	/*
  	 * If LINK is set, we have dependent requests in this chain. If we
  	 * didn't fail this request, queue the first one up, moving any other
@@@ -690,17 -839,55 +777,42 @@@
  	__io_free_req(req);
  }
  
 -/*
 - * Drop reference to request, return next in chain (if there is one) if this
 - * was the last reference to this request.
 - */
 -static struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)
 +static void io_put_req(struct io_kiocb *req)
  {
 -	struct io_kiocb *nxt = NULL;
 -
  	if (refcount_dec_and_test(&req->refs))
 -		io_free_req(req, &nxt);
 -
 -	return nxt;
 +		io_free_req(req);
  }
  
 -static void io_put_req(struct io_kiocb *req, struct io_kiocb **nxtptr)
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
  {
++<<<<<<< HEAD
++=======
+ 	struct io_kiocb *nxt;
+ 
+ 	nxt = io_put_req_find_next(req);
+ 	if (nxt) {
+ 		if (nxtptr)
+ 			*nxtptr = nxt;
+ 		else
+ 			io_queue_async_work(nxt->ctx, nxt);
+ 	}
+ }
+ 
+ static void io_double_put_req(struct io_kiocb *req)
+ {
+ 	/* drop both submit and complete references */
+ 	if (refcount_sub_and_test(2, &req->refs))
+ 		__io_free_req(req);
+ }
+ 
+ static unsigned io_cqring_events(struct io_ring_ctx *ctx)
+ {
+ 	struct io_rings *rings = ctx->rings;
+ 
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	/* See comment at the top of this file */
  	smp_rmb();
 -	return READ_ONCE(rings->cq.tail) - READ_ONCE(rings->cq.head);
 -}
 -
 -static inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)
 -{
 -	struct io_rings *rings = ctx->rings;
 -
 -	/* make sure SQ entry isn't read before tail */
 -	return smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;
 +	return READ_ONCE(ring->r.tail) - READ_ONCE(ring->r.head);
  }
  
  /*
@@@ -891,19 -1078,30 +1003,35 @@@ static int io_iopoll_check(struct io_ri
  	return ret;
  }
  
 -static void kiocb_end_write(struct io_kiocb *req)
 +static void kiocb_end_write(struct kiocb *kiocb)
  {
 -	/*
 -	 * Tell lockdep we inherited freeze protection from submission
 -	 * thread.
 -	 */
 -	if (req->flags & REQ_F_ISREG) {
 -		struct inode *inode = file_inode(req->file);
 +	if (kiocb->ki_flags & IOCB_WRITE) {
 +		struct inode *inode = file_inode(kiocb->ki_filp);
  
 -		__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		/*
 +		 * Tell lockdep we inherited freeze protection from submission
 +		 * thread.
 +		 */
 +		if (S_ISREG(inode->i_mode))
 +			__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		file_end_write(kiocb->ki_filp);
  	}
++<<<<<<< HEAD
++=======
+ 	file_end_write(req->file);
+ }
+ 
+ static void io_complete_rw_common(struct kiocb *kiocb, long res)
+ {
+ 	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw);
+ 
+ 	if (kiocb->ki_flags & IOCB_WRITE)
+ 		kiocb_end_write(req);
+ 
+ 	if ((req->flags & REQ_F_LINK) && res != req->result)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_cqring_add_event(req, res);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  }
  
  static void io_complete_rw(struct kiocb *kiocb, long res, long res2)
@@@ -1466,8 -1609,8 +1593,13 @@@ static int io_nop(struct io_kiocb *req
  	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	io_cqring_add_event(ctx, user_data, err);
 +	io_put_req(req);
++=======
+ 	io_cqring_add_event(req, 0);
+ 	io_put_req(req, NULL);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	return 0;
  }
  
@@@ -1513,8 -1656,8 +1645,13 @@@ static int io_fsync(struct io_kiocb *re
  
  	if (ret < 0 && (req->flags & REQ_F_LINK))
  		req->flags |= REQ_F_FAIL_LINK;
++<<<<<<< HEAD
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
++=======
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req, nxt);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	return 0;
  }
  
@@@ -1559,8 -1703,8 +1696,13 @@@ static int io_sync_file_range(struct io
  
  	if (ret < 0 && (req->flags & REQ_F_LINK))
  		req->flags |= REQ_F_FAIL_LINK;
++<<<<<<< HEAD
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
++=======
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req, nxt);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	return 0;
  }
  
@@@ -1595,8 -1739,10 +1737,15 @@@ static int io_send_recvmsg(struct io_ki
  			return ret;
  	}
  
++<<<<<<< HEAD
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
++=======
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req(req, nxt);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	return 0;
  }
  #endif
@@@ -1612,10 -1759,45 +1761,49 @@@ static int io_sendmsg(struct io_kiocb *
  }
  
  static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		      struct io_kiocb **nxt, bool force_nonblock)
 +		      bool force_nonblock)
  {
  #if defined(CONFIG_NET)
++<<<<<<< HEAD
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_recvmsg_sock);
++=======
+ 	return io_send_recvmsg(req, sqe, nxt, force_nonblock,
+ 				__sys_recvmsg_sock);
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_accept(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 		     struct io_kiocb **nxt, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct sockaddr __user *addr;
+ 	int __user *addr_len;
+ 	unsigned file_flags;
+ 	int flags, ret;
+ 
+ 	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
+ 	addr_len = (int __user *) (unsigned long) READ_ONCE(sqe->addr2);
+ 	flags = READ_ONCE(sqe->accept_flags);
+ 	file_flags = force_nonblock ? O_NONBLOCK : 0;
+ 
+ 	ret = __sys_accept4_file(req->file, file_flags, addr, addr_len, flags);
+ 	if (ret == -EAGAIN && force_nonblock) {
+ 		req->work.flags |= IO_WQ_WORK_NEEDS_FILES;
+ 		return -EAGAIN;
+ 	}
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req, nxt);
+ 	return 0;
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  #else
  	return -EOPNOTSUPP;
  #endif
@@@ -1674,8 -1856,10 +1862,15 @@@ static int io_poll_remove(struct io_kio
  	}
  	spin_unlock_irq(&ctx->completion_lock);
  
++<<<<<<< HEAD
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
++=======
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req(req, NULL);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	return 0;
  }
  
@@@ -1838,9 -2031,245 +2033,249 @@@ static int io_poll_add(struct io_kiocb 
  	return ipt.error;
  }
  
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
++<<<<<<< HEAD
++=======
+ 	struct io_ring_ctx *ctx;
+ 	struct io_kiocb *req;
+ 	unsigned long flags;
+ 
+ 	req = container_of(timer, struct io_kiocb, timeout.timer);
+ 	ctx = req->ctx;
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * We could be racing with timeout deletion. If the list is empty,
+ 	 * then timeout lookup already found it and will be handling it.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		struct io_kiocb *prev;
+ 
+ 		/*
+ 		 * Adjust the reqs sequence before the current one because it
+ 		 * will consume a slot in the cq_ring and the the cq_tail
+ 		 * pointer will be increased, otherwise other timeout reqs may
+ 		 * return in advance without waiting for enough wait_nr.
+ 		 */
+ 		prev = req;
+ 		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 			prev->sequence++;
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	io_cqring_fill_event(req, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	if (req->flags & REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req(req, NULL);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ /*
+  * Remove or update an existing timeout command
+  */
+ static int io_timeout_remove(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *treq;
+ 	int ret = -ENOENT;
+ 	__u64 user_data;
+ 	unsigned flags;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags)
+ 		return -EINVAL;
+ 
+ 	user_data = READ_ONCE(sqe->addr);
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_entry(treq, &ctx->timeout_list, list) {
+ 		if (user_data == treq->user_data) {
+ 			list_del_init(&treq->list);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* didn't find timeout */
+ 	if (ret) {
+ fill_ev:
+ 		io_cqring_fill_event(req, ret);
+ 		io_commit_cqring(ctx);
+ 		spin_unlock_irq(&ctx->completion_lock);
+ 		io_cqring_ev_posted(ctx);
+ 		if (req->flags & REQ_F_LINK)
+ 			req->flags |= REQ_F_FAIL_LINK;
+ 		io_put_req(req, NULL);
+ 		return 0;
+ 	}
+ 
+ 	ret = hrtimer_try_to_cancel(&treq->timeout.timer);
+ 	if (ret == -1) {
+ 		ret = -EBUSY;
+ 		goto fill_ev;
+ 	}
+ 
+ 	io_cqring_fill_event(req, 0);
+ 	io_cqring_fill_event(treq, -ECANCELED);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	io_put_req(treq, NULL);
+ 	io_put_req(req, NULL);
+ 	return 0;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct list_head *entry;
+ 	enum hrtimer_mode mode;
+ 	struct timespec64 ts;
+ 	unsigned span = 0;
+ 	unsigned flags;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len != 1)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 
+ 	if (get_timespec64(&ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	if (flags & IORING_TIMEOUT_ABS)
+ 		mode = HRTIMER_MODE_ABS;
+ 	else
+ 		mode = HRTIMER_MODE_REL;
+ 
+ 	hrtimer_init(&req->timeout.timer, CLOCK_MONOTONIC, mode);
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied.
+ 	 */
+ 	count = READ_ONCE(sqe->off);
+ 	if (!count)
+ 		count = 1;
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	/* reuse it to store the count */
+ 	req->submit.sequence = count;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt->submit.sequence + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt->submit.sequence - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ 	list_add(&req->list, entry);
+ 	req->timeout.timer.function = io_timeout_fn;
+ 	hrtimer_start(&req->timeout.timer, timespec64_to_ktime(ts), mode);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	return 0;
+ }
+ 
+ static bool io_cancel_cb(struct io_wq_work *work, void *data)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	return req->user_data == (unsigned long) data;
+ }
+ 
+ static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
+ {
+ 	enum io_wq_cancel cancel_ret;
+ 	int ret = 0;
+ 
+ 	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
+ 	switch (cancel_ret) {
+ 	case IO_WQ_CANCEL_OK:
+ 		ret = 0;
+ 		break;
+ 	case IO_WQ_CANCEL_RUNNING:
+ 		ret = -EALREADY;
+ 		break;
+ 	case IO_WQ_CANCEL_NOTFOUND:
+ 		ret = -ENOENT;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int io_async_cancel(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			   struct io_kiocb **nxt)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	void *sqe_addr;
+ 	int ret;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
+ 	    sqe->cancel_flags)
+ 		return -EINVAL;
+ 
+ 	sqe_addr = (void *) (unsigned long) READ_ONCE(sqe->addr);
+ 	ret = io_async_cancel_one(ctx, sqe_addr);
+ 
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req)
+ {
+ 	const struct io_uring_sqe *sqe = req->submit.sqe;
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	struct io_uring_sqe *sqe_copy;
  
  	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
@@@ -1867,12 -2296,11 +2302,10 @@@
  }
  
  static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 -			   struct io_kiocb **nxt, bool force_nonblock)
 +			   const struct sqe_submit *s, bool force_nonblock)
  {
  	int ret, opcode;
 -	struct sqe_submit *s = &req->submit;
  
- 	req->user_data = READ_ONCE(s->sqe->user_data);
- 
  	opcode = READ_ONCE(s->sqe->opcode);
  	switch (opcode) {
  	case IORING_OP_NOP:
@@@ -1935,177 -2375,56 +2368,188 @@@
  	return 0;
  }
  
 -static void io_wq_submit_work(struct io_wq_work **workptr)
 +static struct async_list *io_async_list_from_sqe(struct io_ring_ctx *ctx,
 +						 const struct io_uring_sqe *sqe)
 +{
 +	switch (sqe->opcode) {
 +	case IORING_OP_READV:
 +	case IORING_OP_READ_FIXED:
 +		return &ctx->pending_async[READ];
 +	case IORING_OP_WRITEV:
 +	case IORING_OP_WRITE_FIXED:
 +		return &ctx->pending_async[WRITE];
 +	default:
 +		return NULL;
 +	}
 +}
 +
 +static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 +{
 +	u8 opcode = READ_ONCE(sqe->opcode);
 +
 +	return !(opcode == IORING_OP_READ_FIXED ||
 +		 opcode == IORING_OP_WRITE_FIXED);
 +}
 +
 +static void io_sq_wq_submit_work(struct work_struct *work)
  {
 -	struct io_wq_work *work = *workptr;
  	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
  	struct io_ring_ctx *ctx = req->ctx;
 -	struct sqe_submit *s = &req->submit;
 -	const struct io_uring_sqe *sqe = s->sqe;
 -	struct io_kiocb *nxt = NULL;
 -	int ret = 0;
 +	struct mm_struct *cur_mm = NULL;
 +	struct async_list *async_list;
 +	LIST_HEAD(req_list);
 +	mm_segment_t old_fs;
 +	int ret;
  
 -	/* Ensure we clear previously set non-block flag */
 -	req->rw.ki_flags &= ~IOCB_NOWAIT;
 +	async_list = io_async_list_from_sqe(ctx, req->submit.sqe);
 +restart:
 +	do {
 +		struct sqe_submit *s = &req->submit;
 +		const struct io_uring_sqe *sqe = s->sqe;
 +		unsigned int flags = req->flags;
  
 -	if (work->flags & IO_WQ_WORK_CANCEL)
 -		ret = -ECANCELED;
 +		/* Ensure we clear previously set non-block flag */
 +		req->rw.ki_flags &= ~IOCB_NOWAIT;
  
 -	if (!ret) {
 -		s->has_user = (work->flags & IO_WQ_WORK_HAS_MM) != 0;
 -		s->in_async = true;
 -		do {
 -			ret = __io_submit_sqe(ctx, req, &nxt, false);
 -			/*
 -			 * We can get EAGAIN for polled IO even though we're
 -			 * forcing a sync submission from here, since we can't
 -			 * wait for request slots on the block side.
 -			 */
 -			if (ret != -EAGAIN)
 -				break;
 -			cond_resched();
 -		} while (1);
 +		ret = 0;
 +		if (io_sqe_needs_user(sqe) && !cur_mm) {
 +			if (!mmget_not_zero(ctx->sqo_mm)) {
 +				ret = -EFAULT;
 +			} else {
 +				cur_mm = ctx->sqo_mm;
 +				use_mm(cur_mm);
 +				old_fs = get_fs();
 +				set_fs(USER_DS);
 +			}
 +		}
 +
 +		if (!ret) {
 +			s->has_user = cur_mm != NULL;
 +			s->needs_lock = true;
 +			do {
 +				ret = __io_submit_sqe(ctx, req, s, false);
 +				/*
 +				 * We can get EAGAIN for polled IO even though
 +				 * we're forcing a sync submission from here,
 +				 * since we can't wait for request slots on the
 +				 * block side.
 +				 */
 +				if (ret != -EAGAIN)
 +					break;
 +				cond_resched();
 +			} while (1);
 +		}
 +
 +		/* drop submission reference */
 +		io_put_req(req);
 +
 +		if (ret) {
 +			io_cqring_add_event(ctx, sqe->user_data, ret);
 +			io_put_req(req);
 +		}
 +
 +		/* async context always use a copy of the sqe */
 +		kfree(sqe);
 +
 +		/* req from defer and link list needn't decrease async cnt */
 +		if (flags & (REQ_F_IO_DRAINED | REQ_F_LINK_DONE))
 +			goto out;
 +
 +		if (!async_list)
 +			break;
 +		if (!list_empty(&req_list)) {
 +			req = list_first_entry(&req_list, struct io_kiocb,
 +						list);
 +			list_del(&req->list);
 +			continue;
 +		}
 +		if (list_empty(&async_list->list))
 +			break;
 +
 +		req = NULL;
 +		spin_lock(&async_list->lock);
 +		if (list_empty(&async_list->list)) {
 +			spin_unlock(&async_list->lock);
 +			break;
 +		}
 +		list_splice_init(&async_list->list, &req_list);
 +		spin_unlock(&async_list->lock);
 +
 +		req = list_first_entry(&req_list, struct io_kiocb, list);
 +		list_del(&req->list);
 +	} while (req);
 +
 +	/*
 +	 * Rare case of racing with a submitter. If we find the count has
 +	 * dropped to zero AND we have pending work items, then restart
 +	 * the processing. This is a tiny race window.
 +	 */
 +	if (async_list) {
 +		ret = atomic_dec_return(&async_list->cnt);
 +		while (!ret && !list_empty(&async_list->list)) {
 +			spin_lock(&async_list->lock);
 +			atomic_inc(&async_list->cnt);
 +			list_splice_init(&async_list->list, &req_list);
 +			spin_unlock(&async_list->lock);
 +
 +			if (!list_empty(&req_list)) {
 +				req = list_first_entry(&req_list,
 +							struct io_kiocb, list);
 +				list_del(&req->list);
 +				goto restart;
 +			}
 +			ret = atomic_dec_return(&async_list->cnt);
 +		}
  	}
  
++<<<<<<< HEAD
 +out:
 +	if (cur_mm) {
 +		set_fs(old_fs);
 +		unuse_mm(cur_mm);
 +		mmput(cur_mm);
++=======
+ 	/* drop submission reference */
+ 	io_put_req(req, NULL);
+ 
+ 	if (ret) {
+ 		if (req->flags & REQ_F_LINK)
+ 			req->flags |= REQ_F_FAIL_LINK;
+ 		io_cqring_add_event(req, ret);
+ 		io_put_req(req, NULL);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	}
 +}
  
 -	/* async context always use a copy of the sqe */
 -	kfree(sqe);
 +/*
 + * See if we can piggy back onto previously submitted work, that is still
 + * running. We currently only allow this if the new request is sequential
 + * to the previous one we punted.
 + */
 +static bool io_add_to_prev_work(struct async_list *list, struct io_kiocb *req)
 +{
 +	bool ret;
  
 -	/* if a dependent link is ready, pass it back */
 -	if (!ret && nxt) {
 -		io_prep_async_work(nxt);
 -		*workptr = &nxt->work;
 +	if (!list)
 +		return false;
 +	if (!(req->flags & REQ_F_SEQ_PREV))
 +		return false;
 +	if (!atomic_read(&list->cnt))
 +		return false;
 +
 +	ret = true;
 +	spin_lock(&list->lock);
 +	list_add_tail(&req->list, &list->list);
 +	/*
 +	 * Ensure we see a simultaneous modification from io_sq_wq_submit_work()
 +	 */
 +	smp_mb();
 +	if (!atomic_read(&list->cnt)) {
 +		list_del_init(&req->list);
 +		ret = false;
  	}
 +	spin_unlock(&list->lock);
 +	return ret;
  }
  
  static bool io_op_needs_file(const struct io_uring_sqe *sqe)
@@@ -2162,13 -2492,145 +2606,135 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static int io_grab_files(struct io_ring_ctx *ctx, struct io_kiocb *req)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
++<<<<<<< HEAD
++=======
+ 	int ret = -EBADF;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irq(&ctx->inflight_lock);
+ 	/*
+ 	 * We use the f_ops->flush() handler to ensure that we can flush
+ 	 * out work accessing these files if the fd is closed. Check if
+ 	 * the fd has changed since we started down this path, and disallow
+ 	 * this operation if it has.
+ 	 */
+ 	if (fcheck(req->submit.ring_fd) == req->submit.ring_file) {
+ 		list_add(&req->inflight_entry, &ctx->inflight_list);
+ 		req->flags |= REQ_F_INFLIGHT;
+ 		req->work.files = current->files;
+ 		ret = 0;
+ 	}
+ 	spin_unlock_irq(&ctx->inflight_lock);
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_kiocb *req = container_of(timer, struct io_kiocb,
+ 						timeout.timer);
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 	int ret = -ETIME;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		prev = list_entry(req->list.prev, struct io_kiocb, link_list);
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		void *user_data = (void *) (unsigned long) prev->user_data;
+ 		ret = io_async_cancel_one(ctx, user_data);
+ 	}
+ 
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req, NULL);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_queue_linked_timeout(struct io_kiocb *req, struct io_kiocb *nxt)
+ {
+ 	const struct io_uring_sqe *sqe = nxt->submit.sqe;
+ 	enum hrtimer_mode mode;
+ 	struct timespec64 ts;
+ 	int ret = -EINVAL;
+ 
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1 || sqe->off)
+ 		goto err;
+ 	if (sqe->timeout_flags & ~IORING_TIMEOUT_ABS)
+ 		goto err;
+ 	if (get_timespec64(&ts, u64_to_user_ptr(sqe->addr))) {
+ 		ret = -EFAULT;
+ 		goto err;
+ 	}
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 
+ 	if (sqe->timeout_flags & IORING_TIMEOUT_ABS)
+ 		mode = HRTIMER_MODE_ABS;
+ 	else
+ 		mode = HRTIMER_MODE_REL;
+ 	hrtimer_init(&nxt->timeout.timer, CLOCK_MONOTONIC, mode);
+ 	nxt->timeout.timer.function = io_link_timeout_fn;
+ 	hrtimer_start(&nxt->timeout.timer, timespec64_to_ktime(ts), mode);
+ 	ret = 0;
+ err:
+ 	/* drop submission reference */
+ 	io_put_req(nxt, NULL);
+ 
+ 	if (ret) {
+ 		struct io_ring_ctx *ctx = req->ctx;
+ 
+ 		/*
+ 		 * Break the link and fail linked timeout, parent will get
+ 		 * failed by the regular submission path.
+ 		 */
+ 		list_del(&nxt->list);
+ 		io_cqring_fill_event(nxt, ret);
+ 		trace_io_uring_fail_link(req, nxt);
+ 		io_commit_cqring(ctx);
+ 		io_put_req(nxt, NULL);
+ 		ret = -ECANCELED;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static inline struct io_kiocb *io_get_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
+ 	if (nxt && nxt->submit.sqe->opcode == IORING_OP_LINK_TIMEOUT)
+ 		return nxt;
+ 
+ 	return NULL;
+ }
+ 
+ static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt;
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  	int ret;
  
 -	nxt = io_get_linked_timeout(req);
 -	if (unlikely(nxt)) {
 -		ret = io_queue_linked_timeout(req, nxt);
 -		if (ret)
 -			goto err;
 -	}
 -
 -	ret = __io_submit_sqe(ctx, req, NULL, true);
 -
 -	/*
 -	 * We async punt it if the file wasn't marked NOWAIT, or if the file
 -	 * doesn't support non-blocking read/write attempts
 -	 */
 -	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
 -	    (req->flags & REQ_F_MUST_PUNT))) {
 -		struct sqe_submit *s = &req->submit;
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
  		struct io_uring_sqe *sqe_copy;
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
@@@ -2198,10 -2659,10 +2764,10 @@@
  
  	/* and drop final reference, if we failed */
  	if (ret) {
- 		io_cqring_add_event(ctx, req->user_data, ret);
+ 		io_cqring_add_event(req, ret);
  		if (req->flags & REQ_F_LINK)
  			req->flags |= REQ_F_FAIL_LINK;
 -		io_put_req(req, NULL);
 +		io_put_req(req);
  	}
  
  	return ret;
@@@ -2212,11 -2672,11 +2778,16 @@@ static int io_queue_sqe(struct io_ring_
  {
  	int ret;
  
 -	ret = io_req_defer(ctx, req);
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
++<<<<<<< HEAD
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
++=======
+ 			io_cqring_add_event(req, ret);
+ 			io_double_put_req(req);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  		}
  		return 0;
  	}
@@@ -2240,12 -2699,12 +2811,17 @@@ static int io_queue_link_head(struct io
  	 * list.
  	 */
  	req->flags |= REQ_F_IO_DRAIN;
 -	ret = io_req_defer(ctx, req);
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
++<<<<<<< HEAD
 +			io_free_req(req);
++=======
+ 			io_cqring_add_event(req, ret);
+ 			io_double_put_req(req);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
  			return 0;
  		}
  	} else {
@@@ -2269,32 -2729,26 +2845,39 @@@
  
  #define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
  
 -static void io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 -			  struct io_submit_state *state, struct io_kiocb **link)
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
  {
  	struct io_uring_sqe *sqe_copy;
 -	struct sqe_submit *s = &req->submit;
 +	struct io_kiocb *req;
  	int ret;
  
+ 	req->user_data = s->sqe->user_data;
+ 
  	/* enforce forwards compatibility on users */
  	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
  		ret = -EINVAL;
 -		goto err_req;
 +		goto err;
  	}
  
 -	ret = io_req_set_file(ctx, state, req);
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
 +	}
 +
 +	ret = io_req_set_file(ctx, s, state, req);
  	if (unlikely(ret)) {
  err_req:
++<<<<<<< HEAD
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
++=======
+ 		io_cqring_add_event(req, ret);
+ 		io_double_put_req(req);
++>>>>>>> 78e19bbef383 (io_uring: pass in io_kiocb to fill/add CQ handlers)
  		return;
  	}
  
* Unmerged path fs/io_uring.c
