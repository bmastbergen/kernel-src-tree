x86/split_lock: Bits in IA32_CORE_CAPABILITIES are not architectural

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Tony Luck <tony.luck@intel.com>
commit 48fd5b5ee714714f4cf9f9e1cba3b49b1fd40ed6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/48fd5b5e.failed

The Intel Software Developers' Manual erroneously listed bit 5 of the
IA32_CORE_CAPABILITIES register as an architectural feature. It is not.

Features enumerated by IA32_CORE_CAPABILITIES are model specific and
implementation details may vary in different cpu models. Thus it is only
safe to trust features after checking the CPU model.

Icelake client and server models are known to implement the split lock
detect feature even though they don't enumerate IA32_CORE_CAPABILITIES

[ tglx: Use switch() for readability and massage comments ]

Fixes: 6650cdd9a8cc ("x86/split_lock: Enable split lock detection by kernel")
	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20200416205754.21177-3-tony.luck@intel.com

(cherry picked from commit 48fd5b5ee714714f4cf9f9e1cba3b49b1fd40ed6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/intel.c
diff --cc arch/x86/kernel/cpu/intel.c
index ae064a2ca368,c23ad481347e..000000000000
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@@ -1034,3 -967,202 +1034,205 @@@ static const struct cpu_dev intel_cpu_d
  
  cpu_dev_register(intel_cpu_dev);
  
++<<<<<<< HEAD
++=======
+ #undef pr_fmt
+ #define pr_fmt(fmt) "x86/split lock detection: " fmt
+ 
+ static const struct {
+ 	const char			*option;
+ 	enum split_lock_detect_state	state;
+ } sld_options[] __initconst = {
+ 	{ "off",	sld_off   },
+ 	{ "warn",	sld_warn  },
+ 	{ "fatal",	sld_fatal },
+ };
+ 
+ static inline bool match_option(const char *arg, int arglen, const char *opt)
+ {
+ 	int len = strlen(opt);
+ 
+ 	return len == arglen && !strncmp(arg, opt, len);
+ }
+ 
+ static bool split_lock_verify_msr(bool on)
+ {
+ 	u64 ctrl, tmp;
+ 
+ 	if (rdmsrl_safe(MSR_TEST_CTRL, &ctrl))
+ 		return false;
+ 	if (on)
+ 		ctrl |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	else
+ 		ctrl &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	if (wrmsrl_safe(MSR_TEST_CTRL, ctrl))
+ 		return false;
+ 	rdmsrl(MSR_TEST_CTRL, tmp);
+ 	return ctrl == tmp;
+ }
+ 
+ static void __init split_lock_setup(void)
+ {
+ 	enum split_lock_detect_state state = sld_warn;
+ 	char arg[20];
+ 	int i, ret;
+ 
+ 	if (!split_lock_verify_msr(false)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	ret = cmdline_find_option(boot_command_line, "split_lock_detect",
+ 				  arg, sizeof(arg));
+ 	if (ret >= 0) {
+ 		for (i = 0; i < ARRAY_SIZE(sld_options); i++) {
+ 			if (match_option(arg, ret, sld_options[i].option)) {
+ 				state = sld_options[i].state;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	switch (state) {
+ 	case sld_off:
+ 		pr_info("disabled\n");
+ 		return;
+ 	case sld_warn:
+ 		pr_info("warning about user-space split_locks\n");
+ 		break;
+ 	case sld_fatal:
+ 		pr_info("sending SIGBUS on user-space split_locks\n");
+ 		break;
+ 	}
+ 
+ 	rdmsrl(MSR_TEST_CTRL, msr_test_ctrl_cache);
+ 
+ 	if (!split_lock_verify_msr(true)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	sld_state = state;
+ 	setup_force_cpu_cap(X86_FEATURE_SPLIT_LOCK_DETECT);
+ }
+ 
+ /*
+  * MSR_TEST_CTRL is per core, but we treat it like a per CPU MSR. Locking
+  * is not implemented as one thread could undo the setting of the other
+  * thread immediately after dropping the lock anyway.
+  */
+ static void sld_update_msr(bool on)
+ {
+ 	u64 test_ctrl_val = msr_test_ctrl_cache;
+ 
+ 	if (on)
+ 		test_ctrl_val |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 
+ 	wrmsrl(MSR_TEST_CTRL, test_ctrl_val);
+ }
+ 
+ static void split_lock_init(void)
+ {
+ 	split_lock_verify_msr(sld_state != sld_off);
+ }
+ 
+ static void split_lock_warn(unsigned long ip)
+ {
+ 	pr_warn_ratelimited("#AC: %s/%d took a split_lock trap at address: 0x%lx\n",
+ 			    current->comm, current->pid, ip);
+ 
+ 	/*
+ 	 * Disable the split lock detection for this task so it can make
+ 	 * progress and set TIF_SLD so the detection is re-enabled via
+ 	 * switch_to_sld() when the task is scheduled out.
+ 	 */
+ 	sld_update_msr(false);
+ 	set_tsk_thread_flag(current, TIF_SLD);
+ }
+ 
+ bool handle_guest_split_lock(unsigned long ip)
+ {
+ 	if (sld_state == sld_warn) {
+ 		split_lock_warn(ip);
+ 		return true;
+ 	}
+ 
+ 	pr_warn_once("#AC: %s/%d %s split_lock trap at address: 0x%lx\n",
+ 		     current->comm, current->pid,
+ 		     sld_state == sld_fatal ? "fatal" : "bogus", ip);
+ 
+ 	current->thread.error_code = 0;
+ 	current->thread.trap_nr = X86_TRAP_AC;
+ 	force_sig_fault(SIGBUS, BUS_ADRALN, NULL);
+ 	return false;
+ }
+ EXPORT_SYMBOL_GPL(handle_guest_split_lock);
+ 
+ bool handle_user_split_lock(struct pt_regs *regs, long error_code)
+ {
+ 	if ((regs->flags & X86_EFLAGS_AC) || sld_state == sld_fatal)
+ 		return false;
+ 	split_lock_warn(regs->ip);
+ 	return true;
+ }
+ 
+ /*
+  * This function is called only when switching between tasks with
+  * different split-lock detection modes. It sets the MSR for the
+  * mode of the new task. This is right most of the time, but since
+  * the MSR is shared by hyperthreads on a physical core there can
+  * be glitches when the two threads need different modes.
+  */
+ void switch_to_sld(unsigned long tifn)
+ {
+ 	sld_update_msr(!(tifn & _TIF_SLD));
+ }
+ 
+ /*
+  * Bits in the IA32_CORE_CAPABILITIES are not architectural, so they should
+  * only be trusted if it is confirmed that a CPU model implements a
+  * specific feature at a particular bit position.
+  *
+  * The possible driver data field values:
+  *
+  * - 0: CPU models that are known to have the per-core split-lock detection
+  *	feature even though they do not enumerate IA32_CORE_CAPABILITIES.
+  *
+  * - 1: CPU models which may enumerate IA32_CORE_CAPABILITIES and if so use
+  *      bit 5 to enumerate the per-core split-lock detection feature.
+  */
+ static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
+ 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,		0),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_L,		0),
+ 	{}
+ };
+ 
+ void __init cpu_set_core_cap_bits(struct cpuinfo_x86 *c)
+ {
+ 	const struct x86_cpu_id *m;
+ 	u64 ia32_core_caps;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+ 		return;
+ 
+ 	m = x86_match_cpu(split_lock_cpu_ids);
+ 	if (!m)
+ 		return;
+ 
+ 	switch (m->driver_data) {
+ 	case 0:
+ 		break;
+ 	case 1:
+ 		if (!cpu_has(c, X86_FEATURE_CORE_CAPABILITIES))
+ 			return;
+ 		rdmsrl(MSR_IA32_CORE_CAPS, ia32_core_caps);
+ 		if (!(ia32_core_caps & MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT))
+ 			return;
+ 		break;
+ 	default:
+ 		return;
+ 	}
+ 
+ 	split_lock_setup();
+ }
++>>>>>>> 48fd5b5ee714 (x86/split_lock: Bits in IA32_CORE_CAPABILITIES are not architectural)
* Unmerged path arch/x86/kernel/cpu/intel.c
