sched/fair: Move init_numa_balancing() below task_numa_work()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Valentin Schneider <valentin.schneider@arm.com>
commit d35927a144641700c8328d707d1c89d305b4ecb8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/d35927a1.failed

To reference task_numa_work() from within init_numa_balancing(), we
need the former to be declared before the latter. Do just that.

This is a pure code movement.

	Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: mgorman@suse.de
	Cc: riel@surriel.com
Link: https://lkml.kernel.org/r/20190715102508.32434-2-valentin.schneider@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit d35927a144641700c8328d707d1c89d305b4ecb8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 8e9226f8554c,f0c488015649..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -1169,50 -1188,9 +1169,53 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
 +void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 +{
 +	int mm_users = 0;
 +	struct mm_struct *mm = p->mm;
 +
 +	if (mm) {
 +		mm_users = atomic_read(&mm->mm_users);
 +		if (mm_users == 1) {
 +			mm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
 +			mm->numa_scan_seq = 0;
 +		}
 +	}
 +	p->node_stamp			= 0;
 +	p->numa_scan_seq		= mm ? mm->numa_scan_seq : 0;
 +	p->numa_scan_period		= sysctl_numa_balancing_scan_delay;
 +	p->numa_work.next		= &p->numa_work;
 +	p->numa_faults			= NULL;
 +	RCU_INIT_POINTER(p->numa_group, NULL);
 +	p->last_task_numa_placement	= 0;
 +	p->last_sum_exec_runtime	= 0;
 +
 +	/* New address space, reset the preferred nid */
 +	if (!(clone_flags & CLONE_VM)) {
 +		p->numa_preferred_nid = -1;
 +		return;
 +	}
 +
 +	/*
 +	 * New thread, keep existing numa_preferred_nid which should be copied
 +	 * already by arch_dup_task_struct but stagger when scans start.
 +	 */
 +	if (mm) {
 +		unsigned int delay;
 +
 +		delay = min_t(unsigned int, task_scan_max(current),
 +			current->numa_scan_period * mm_users * NSEC_PER_MSEC);
 +		delay += 2 * TICK_NSEC;
 +		p->node_stamp = delay;
 +	}
 +}
 +
++=======
++>>>>>>> d35927a14464 (sched/fair: Move init_numa_balancing() below task_numa_work())
  static void account_numa_enqueue(struct rq *rq, struct task_struct *p)
  {
 -	rq->nr_numa_running += (p->numa_preferred_nid != NUMA_NO_NODE);
 +	rq->nr_numa_running += (p->numa_preferred_nid != -1);
  	rq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));
  }
  
* Unmerged path kernel/sched/fair.c
