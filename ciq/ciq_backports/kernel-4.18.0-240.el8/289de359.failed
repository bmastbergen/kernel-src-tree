sched/fair: Fix statistics for find_idlest_group()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Vincent Guittot <vincent.guittot@linaro.org>
commit 289de35984815576793f579ec27248609e75976e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/289de359.failed

sgs->group_weight is not set while gathering statistics in
update_sg_wakeup_stats(). This means that a group can be classified as
fully busy with 0 running tasks if utilization is high enough.

This path is mainly used for fork and exec.

Fixes: 57abff067a08 ("sched/fair: Rework find_idlest_group()")
	Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
Link: https://lore.kernel.org/r/20200218144534.4564-1-vincent.guittot@linaro.org
(cherry picked from commit 289de35984815576793f579ec27248609e75976e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 0b55f8f47ff7,c1217bfe5e81..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -8119,6 -8246,304 +8119,307 @@@ static inline enum fbq_type fbq_classif
  }
  #endif /* CONFIG_NUMA_BALANCING */
  
++<<<<<<< HEAD
++=======
+ 
+ struct sg_lb_stats;
+ 
+ /*
+  * task_running_on_cpu - return 1 if @p is running on @cpu.
+  */
+ 
+ static unsigned int task_running_on_cpu(int cpu, struct task_struct *p)
+ {
+ 	/* Task has no contribution or is new */
+ 	if (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))
+ 		return 0;
+ 
+ 	if (task_on_rq_queued(p))
+ 		return 1;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * idle_cpu_without - would a given CPU be idle without p ?
+  * @cpu: the processor on which idleness is tested.
+  * @p: task which should be ignored.
+  *
+  * Return: 1 if the CPU would be idle. 0 otherwise.
+  */
+ static int idle_cpu_without(int cpu, struct task_struct *p)
+ {
+ 	struct rq *rq = cpu_rq(cpu);
+ 
+ 	if (rq->curr != rq->idle && rq->curr != p)
+ 		return 0;
+ 
+ 	/*
+ 	 * rq->nr_running can't be used but an updated version without the
+ 	 * impact of p on cpu must be used instead. The updated nr_running
+ 	 * be computed and tested before calling idle_cpu_without().
+ 	 */
+ 
+ #ifdef CONFIG_SMP
+ 	if (!llist_empty(&rq->wake_list))
+ 		return 0;
+ #endif
+ 
+ 	return 1;
+ }
+ 
+ /*
+  * update_sg_wakeup_stats - Update sched_group's statistics for wakeup.
+  * @sd: The sched_domain level to look for idlest group.
+  * @group: sched_group whose statistics are to be updated.
+  * @sgs: variable to hold the statistics for this group.
+  * @p: The task for which we look for the idlest group/CPU.
+  */
+ static inline void update_sg_wakeup_stats(struct sched_domain *sd,
+ 					  struct sched_group *group,
+ 					  struct sg_lb_stats *sgs,
+ 					  struct task_struct *p)
+ {
+ 	int i, nr_running;
+ 
+ 	memset(sgs, 0, sizeof(*sgs));
+ 
+ 	for_each_cpu(i, sched_group_span(group)) {
+ 		struct rq *rq = cpu_rq(i);
+ 		unsigned int local;
+ 
+ 		sgs->group_load += cpu_load_without(rq, p);
+ 		sgs->group_util += cpu_util_without(i, p);
+ 		local = task_running_on_cpu(i, p);
+ 		sgs->sum_h_nr_running += rq->cfs.h_nr_running - local;
+ 
+ 		nr_running = rq->nr_running - local;
+ 		sgs->sum_nr_running += nr_running;
+ 
+ 		/*
+ 		 * No need to call idle_cpu_without() if nr_running is not 0
+ 		 */
+ 		if (!nr_running && idle_cpu_without(i, p))
+ 			sgs->idle_cpus++;
+ 
+ 	}
+ 
+ 	/* Check if task fits in the group */
+ 	if (sd->flags & SD_ASYM_CPUCAPACITY &&
+ 	    !task_fits_capacity(p, group->sgc->max_capacity)) {
+ 		sgs->group_misfit_task_load = 1;
+ 	}
+ 
+ 	sgs->group_capacity = group->sgc->capacity;
+ 
+ 	sgs->group_weight = group->group_weight;
+ 
+ 	sgs->group_type = group_classify(sd->imbalance_pct, group, sgs);
+ 
+ 	/*
+ 	 * Computing avg_load makes sense only when group is fully busy or
+ 	 * overloaded
+ 	 */
+ 	if (sgs->group_type < group_fully_busy)
+ 		sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
+ 				sgs->group_capacity;
+ }
+ 
+ static bool update_pick_idlest(struct sched_group *idlest,
+ 			       struct sg_lb_stats *idlest_sgs,
+ 			       struct sched_group *group,
+ 			       struct sg_lb_stats *sgs)
+ {
+ 	if (sgs->group_type < idlest_sgs->group_type)
+ 		return true;
+ 
+ 	if (sgs->group_type > idlest_sgs->group_type)
+ 		return false;
+ 
+ 	/*
+ 	 * The candidate and the current idlest group are the same type of
+ 	 * group. Let check which one is the idlest according to the type.
+ 	 */
+ 
+ 	switch (sgs->group_type) {
+ 	case group_overloaded:
+ 	case group_fully_busy:
+ 		/* Select the group with lowest avg_load. */
+ 		if (idlest_sgs->avg_load <= sgs->avg_load)
+ 			return false;
+ 		break;
+ 
+ 	case group_imbalanced:
+ 	case group_asym_packing:
+ 		/* Those types are not used in the slow wakeup path */
+ 		return false;
+ 
+ 	case group_misfit_task:
+ 		/* Select group with the highest max capacity */
+ 		if (idlest->sgc->max_capacity >= group->sgc->max_capacity)
+ 			return false;
+ 		break;
+ 
+ 	case group_has_spare:
+ 		/* Select group with most idle CPUs */
+ 		if (idlest_sgs->idle_cpus >= sgs->idle_cpus)
+ 			return false;
+ 		break;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ /*
+  * find_idlest_group() finds and returns the least busy CPU group within the
+  * domain.
+  *
+  * Assumes p is allowed on at least one CPU in sd.
+  */
+ static struct sched_group *
+ find_idlest_group(struct sched_domain *sd, struct task_struct *p,
+ 		  int this_cpu, int sd_flag)
+ {
+ 	struct sched_group *idlest = NULL, *local = NULL, *group = sd->groups;
+ 	struct sg_lb_stats local_sgs, tmp_sgs;
+ 	struct sg_lb_stats *sgs;
+ 	unsigned long imbalance;
+ 	struct sg_lb_stats idlest_sgs = {
+ 			.avg_load = UINT_MAX,
+ 			.group_type = group_overloaded,
+ 	};
+ 
+ 	imbalance = scale_load_down(NICE_0_LOAD) *
+ 				(sd->imbalance_pct-100) / 100;
+ 
+ 	do {
+ 		int local_group;
+ 
+ 		/* Skip over this group if it has no CPUs allowed */
+ 		if (!cpumask_intersects(sched_group_span(group),
+ 					p->cpus_ptr))
+ 			continue;
+ 
+ 		local_group = cpumask_test_cpu(this_cpu,
+ 					       sched_group_span(group));
+ 
+ 		if (local_group) {
+ 			sgs = &local_sgs;
+ 			local = group;
+ 		} else {
+ 			sgs = &tmp_sgs;
+ 		}
+ 
+ 		update_sg_wakeup_stats(sd, group, sgs, p);
+ 
+ 		if (!local_group && update_pick_idlest(idlest, &idlest_sgs, group, sgs)) {
+ 			idlest = group;
+ 			idlest_sgs = *sgs;
+ 		}
+ 
+ 	} while (group = group->next, group != sd->groups);
+ 
+ 
+ 	/* There is no idlest group to push tasks to */
+ 	if (!idlest)
+ 		return NULL;
+ 
+ 	/* The local group has been skipped because of CPU affinity */
+ 	if (!local)
+ 		return idlest;
+ 
+ 	/*
+ 	 * If the local group is idler than the selected idlest group
+ 	 * don't try and push the task.
+ 	 */
+ 	if (local_sgs.group_type < idlest_sgs.group_type)
+ 		return NULL;
+ 
+ 	/*
+ 	 * If the local group is busier than the selected idlest group
+ 	 * try and push the task.
+ 	 */
+ 	if (local_sgs.group_type > idlest_sgs.group_type)
+ 		return idlest;
+ 
+ 	switch (local_sgs.group_type) {
+ 	case group_overloaded:
+ 	case group_fully_busy:
+ 		/*
+ 		 * When comparing groups across NUMA domains, it's possible for
+ 		 * the local domain to be very lightly loaded relative to the
+ 		 * remote domains but "imbalance" skews the comparison making
+ 		 * remote CPUs look much more favourable. When considering
+ 		 * cross-domain, add imbalance to the load on the remote node
+ 		 * and consider staying local.
+ 		 */
+ 
+ 		if ((sd->flags & SD_NUMA) &&
+ 		    ((idlest_sgs.avg_load + imbalance) >= local_sgs.avg_load))
+ 			return NULL;
+ 
+ 		/*
+ 		 * If the local group is less loaded than the selected
+ 		 * idlest group don't try and push any tasks.
+ 		 */
+ 		if (idlest_sgs.avg_load >= (local_sgs.avg_load + imbalance))
+ 			return NULL;
+ 
+ 		if (100 * local_sgs.avg_load <= sd->imbalance_pct * idlest_sgs.avg_load)
+ 			return NULL;
+ 		break;
+ 
+ 	case group_imbalanced:
+ 	case group_asym_packing:
+ 		/* Those type are not used in the slow wakeup path */
+ 		return NULL;
+ 
+ 	case group_misfit_task:
+ 		/* Select group with the highest max capacity */
+ 		if (local->sgc->max_capacity >= idlest->sgc->max_capacity)
+ 			return NULL;
+ 		break;
+ 
+ 	case group_has_spare:
+ 		if (sd->flags & SD_NUMA) {
+ #ifdef CONFIG_NUMA_BALANCING
+ 			int idlest_cpu;
+ 			/*
+ 			 * If there is spare capacity at NUMA, try to select
+ 			 * the preferred node
+ 			 */
+ 			if (cpu_to_node(this_cpu) == p->numa_preferred_nid)
+ 				return NULL;
+ 
+ 			idlest_cpu = cpumask_first(sched_group_span(idlest));
+ 			if (cpu_to_node(idlest_cpu) == p->numa_preferred_nid)
+ 				return idlest;
+ #endif
+ 			/*
+ 			 * Otherwise, keep the task on this node to stay close
+ 			 * its wakeup source and improve locality. If there is
+ 			 * a real need of migration, periodic load balance will
+ 			 * take care of it.
+ 			 */
+ 			if (local_sgs.idle_cpus)
+ 				return NULL;
+ 		}
+ 
+ 		/*
+ 		 * Select group with highest number of idle CPUs. We could also
+ 		 * compare the utilization which is more stable but it can end
+ 		 * up that the group has less spare capacity but finally more
+ 		 * idle CPUs which means more opportunity to run task.
+ 		 */
+ 		if (local_sgs.idle_cpus >= idlest_sgs.idle_cpus)
+ 			return NULL;
+ 		break;
+ 	}
+ 
+ 	return idlest;
+ }
+ 
++>>>>>>> 289de3598481 (sched/fair: Fix statistics for find_idlest_group())
  /**
   * update_sd_lb_stats - Update sched_domain's statistics for load balancing.
   * @env: The load balancing environment.
* Unmerged path kernel/sched/fair.c
