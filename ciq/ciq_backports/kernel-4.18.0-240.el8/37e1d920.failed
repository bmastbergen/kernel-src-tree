bpf: Replace cant_sleep() with cant_migrate()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 37e1d9202225635772b32e340294208367279c2b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/37e1d920.failed

As already discussed in the previous change which introduced
BPF_RUN_PROG_PIN_ON_CPU() BPF only requires to disable migration to
guarantee per CPUness.

If RT substitutes the preempt disable based migration protection then the
cant_sleep() check will obviously trigger as preemption is not disabled.

Replace it by cant_migrate() which maps to cant_sleep() on a non RT kernel
and will verify that migration is disabled on a full RT kernel.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200224145643.583038889@linutronix.de
(cherry picked from commit 37e1d9202225635772b32e340294208367279c2b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/filter.h
diff --cc include/linux/filter.h
index b3373f2e5cac,1982a52eb4c9..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -566,23 -559,48 +566,41 @@@ struct sk_filter 
  
  DECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);
  
++<<<<<<< HEAD
 +#define BPF_PROG_RUN(prog, ctx)	({				\
 +	u32 ret;						\
 +	cant_sleep();						\
 +	if (static_branch_unlikely(&bpf_stats_enabled_key)) {	\
 +		struct bpf_prog_stats *stats;			\
 +		u64 start = sched_clock();			\
 +		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
 +		stats = this_cpu_ptr(prog->aux->stats);		\
 +		u64_stats_update_begin(&stats->syncp);		\
 +		stats->cnt++;					\
 +		stats->nsecs += sched_clock() - start;		\
 +		u64_stats_update_end(&stats->syncp);		\
 +	} else {						\
 +		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
 +	}							\
++=======
+ #define __BPF_PROG_RUN(prog, ctx, dfunc)	({			\
+ 	u32 ret;							\
+ 	cant_migrate();							\
+ 	if (static_branch_unlikely(&bpf_stats_enabled_key)) {		\
+ 		struct bpf_prog_stats *stats;				\
+ 		u64 start = sched_clock();				\
+ 		ret = dfunc(ctx, (prog)->insnsi, (prog)->bpf_func);	\
+ 		stats = this_cpu_ptr(prog->aux->stats);			\
+ 		u64_stats_update_begin(&stats->syncp);			\
+ 		stats->cnt++;						\
+ 		stats->nsecs += sched_clock() - start;			\
+ 		u64_stats_update_end(&stats->syncp);			\
+ 	} else {							\
+ 		ret = dfunc(ctx, (prog)->insnsi, (prog)->bpf_func);	\
+ 	}								\
++>>>>>>> 37e1d9202225 (bpf: Replace cant_sleep() with cant_migrate())
  	ret; })
  
 -#define BPF_PROG_RUN(prog, ctx)						\
 -	__BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nopfunc)
 -
 -/*
 - * Use in preemptible and therefore migratable context to make sure that
 - * the execution of the BPF program runs on one CPU.
 - *
 - * This uses migrate_disable/enable() explicitly to document that the
 - * invocation of a BPF program does not require reentrancy protection
 - * against a BPF program which is invoked from a preempting task.
 - *
 - * For non RT enabled kernels migrate_disable/enable() maps to
 - * preempt_disable/enable(), i.e. it disables also preemption.
 - */
 -static inline u32 bpf_prog_run_pin_on_cpu(const struct bpf_prog *prog,
 -					  const void *ctx)
 -{
 -	u32 ret;
 -
 -	migrate_disable();
 -	ret = __BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nopfunc);
 -	migrate_enable();
 -	return ret;
 -}
 -
  #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
  
  struct bpf_skb_data_end {
* Unmerged path include/linux/filter.h
