KVM: SVM: always update CR3 in VMCB

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 978ce5837c7ed50e4ea30cc0fa20f2f820edf8ea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/978ce583.failed

svm_load_mmu_pgd is delaying the write of GUEST_CR3 to prepare_vmcs02 as
an optimization, but this is only correct before the nested vmentry.
If userspace is modifying CR3 with KVM_SET_SREGS after the VM has
already been put in guest mode, the value of CR3 will not be updated.
Remove the optimization, which almost never triggers anyway.
This was was added in commit 689f3bf21628 ("KVM: x86: unify callbacks
to load paging root", 2020-03-16) just to keep the two vendor-specific
modules closer, but we'll fix VMX too.

Fixes: 689f3bf21628 ("KVM: x86: unify callbacks to load paging root")
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 978ce5837c7ed50e4ea30cc0fa20f2f820edf8ea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/nested.c
#	arch/x86/kvm/svm/svm.c
diff --cc arch/x86/kvm/svm/svm.c
index 2816d8e26433,feb96a410f2d..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -4950,1420 -2763,277 +4950,1434 @@@ static void dump_vmcb(struct kvm_vcpu *
  		return;
  	}
  
 -	pr_err("VMCB Control Area:\n");
 -	pr_err("%-20s%04x\n", "cr_read:", control->intercept_cr & 0xffff);
 -	pr_err("%-20s%04x\n", "cr_write:", control->intercept_cr >> 16);
 -	pr_err("%-20s%04x\n", "dr_read:", control->intercept_dr & 0xffff);
 -	pr_err("%-20s%04x\n", "dr_write:", control->intercept_dr >> 16);
 -	pr_err("%-20s%08x\n", "exceptions:", control->intercept_exceptions);
 -	pr_err("%-20s%016llx\n", "intercepts:", control->intercept);
 -	pr_err("%-20s%d\n", "pause filter count:", control->pause_filter_count);
 -	pr_err("%-20s%d\n", "pause filter threshold:",
 -	       control->pause_filter_thresh);
 -	pr_err("%-20s%016llx\n", "iopm_base_pa:", control->iopm_base_pa);
 -	pr_err("%-20s%016llx\n", "msrpm_base_pa:", control->msrpm_base_pa);
 -	pr_err("%-20s%016llx\n", "tsc_offset:", control->tsc_offset);
 -	pr_err("%-20s%d\n", "asid:", control->asid);
 -	pr_err("%-20s%d\n", "tlb_ctl:", control->tlb_ctl);
 -	pr_err("%-20s%08x\n", "int_ctl:", control->int_ctl);
 -	pr_err("%-20s%08x\n", "int_vector:", control->int_vector);
 -	pr_err("%-20s%08x\n", "int_state:", control->int_state);
 -	pr_err("%-20s%08x\n", "exit_code:", control->exit_code);
 -	pr_err("%-20s%016llx\n", "exit_info1:", control->exit_info_1);
 -	pr_err("%-20s%016llx\n", "exit_info2:", control->exit_info_2);
 -	pr_err("%-20s%08x\n", "exit_int_info:", control->exit_int_info);
 -	pr_err("%-20s%08x\n", "exit_int_info_err:", control->exit_int_info_err);
 -	pr_err("%-20s%lld\n", "nested_ctl:", control->nested_ctl);
 -	pr_err("%-20s%016llx\n", "nested_cr3:", control->nested_cr3);
 -	pr_err("%-20s%016llx\n", "avic_vapic_bar:", control->avic_vapic_bar);
 -	pr_err("%-20s%08x\n", "event_inj:", control->event_inj);
 -	pr_err("%-20s%08x\n", "event_inj_err:", control->event_inj_err);
 -	pr_err("%-20s%lld\n", "virt_ext:", control->virt_ext);
 -	pr_err("%-20s%016llx\n", "next_rip:", control->next_rip);
 -	pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
 -	pr_err("%-20s%016llx\n", "avic_logical_id:", control->avic_logical_id);
 -	pr_err("%-20s%016llx\n", "avic_physical_id:", control->avic_physical_id);
 -	pr_err("VMCB State Save Area:\n");
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "es:",
 -	       save->es.selector, save->es.attrib,
 -	       save->es.limit, save->es.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "cs:",
 -	       save->cs.selector, save->cs.attrib,
 -	       save->cs.limit, save->cs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ss:",
 -	       save->ss.selector, save->ss.attrib,
 -	       save->ss.limit, save->ss.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ds:",
 -	       save->ds.selector, save->ds.attrib,
 -	       save->ds.limit, save->ds.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "fs:",
 -	       save->fs.selector, save->fs.attrib,
 -	       save->fs.limit, save->fs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "gs:",
 -	       save->gs.selector, save->gs.attrib,
 -	       save->gs.limit, save->gs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "gdtr:",
 -	       save->gdtr.selector, save->gdtr.attrib,
 -	       save->gdtr.limit, save->gdtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ldtr:",
 -	       save->ldtr.selector, save->ldtr.attrib,
 -	       save->ldtr.limit, save->ldtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "idtr:",
 -	       save->idtr.selector, save->idtr.attrib,
 -	       save->idtr.limit, save->idtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "tr:",
 -	       save->tr.selector, save->tr.attrib,
 -	       save->tr.limit, save->tr.base);
 -	pr_err("cpl:            %d                efer:         %016llx\n",
 -		save->cpl, save->efer);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cr0:", save->cr0, "cr2:", save->cr2);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cr3:", save->cr3, "cr4:", save->cr4);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "dr6:", save->dr6, "dr7:", save->dr7);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "rip:", save->rip, "rflags:", save->rflags);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "rsp:", save->rsp, "rax:", save->rax);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "star:", save->star, "lstar:", save->lstar);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cstar:", save->cstar, "sfmask:", save->sfmask);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "kernel_gs_base:", save->kernel_gs_base,
 -	       "sysenter_cs:", save->sysenter_cs);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "sysenter_esp:", save->sysenter_esp,
 -	       "sysenter_eip:", save->sysenter_eip);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "gpat:", save->g_pat, "dbgctl:", save->dbgctl);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "br_from:", save->br_from, "br_to:", save->br_to);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "excp_from:", save->last_excp_from,
 -	       "excp_to:", save->last_excp_to);
 +	pr_err("VMCB Control Area:\n");
 +	pr_err("%-20s%04x\n", "cr_read:", control->intercept_cr & 0xffff);
 +	pr_err("%-20s%04x\n", "cr_write:", control->intercept_cr >> 16);
 +	pr_err("%-20s%04x\n", "dr_read:", control->intercept_dr & 0xffff);
 +	pr_err("%-20s%04x\n", "dr_write:", control->intercept_dr >> 16);
 +	pr_err("%-20s%08x\n", "exceptions:", control->intercept_exceptions);
 +	pr_err("%-20s%016llx\n", "intercepts:", control->intercept);
 +	pr_err("%-20s%d\n", "pause filter count:", control->pause_filter_count);
 +	pr_err("%-20s%d\n", "pause filter threshold:",
 +	       control->pause_filter_thresh);
 +	pr_err("%-20s%016llx\n", "iopm_base_pa:", control->iopm_base_pa);
 +	pr_err("%-20s%016llx\n", "msrpm_base_pa:", control->msrpm_base_pa);
 +	pr_err("%-20s%016llx\n", "tsc_offset:", control->tsc_offset);
 +	pr_err("%-20s%d\n", "asid:", control->asid);
 +	pr_err("%-20s%d\n", "tlb_ctl:", control->tlb_ctl);
 +	pr_err("%-20s%08x\n", "int_ctl:", control->int_ctl);
 +	pr_err("%-20s%08x\n", "int_vector:", control->int_vector);
 +	pr_err("%-20s%08x\n", "int_state:", control->int_state);
 +	pr_err("%-20s%08x\n", "exit_code:", control->exit_code);
 +	pr_err("%-20s%016llx\n", "exit_info1:", control->exit_info_1);
 +	pr_err("%-20s%016llx\n", "exit_info2:", control->exit_info_2);
 +	pr_err("%-20s%08x\n", "exit_int_info:", control->exit_int_info);
 +	pr_err("%-20s%08x\n", "exit_int_info_err:", control->exit_int_info_err);
 +	pr_err("%-20s%lld\n", "nested_ctl:", control->nested_ctl);
 +	pr_err("%-20s%016llx\n", "nested_cr3:", control->nested_cr3);
 +	pr_err("%-20s%016llx\n", "avic_vapic_bar:", control->avic_vapic_bar);
 +	pr_err("%-20s%08x\n", "event_inj:", control->event_inj);
 +	pr_err("%-20s%08x\n", "event_inj_err:", control->event_inj_err);
 +	pr_err("%-20s%lld\n", "virt_ext:", control->virt_ext);
 +	pr_err("%-20s%016llx\n", "next_rip:", control->next_rip);
 +	pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
 +	pr_err("%-20s%016llx\n", "avic_logical_id:", control->avic_logical_id);
 +	pr_err("%-20s%016llx\n", "avic_physical_id:", control->avic_physical_id);
 +	pr_err("VMCB State Save Area:\n");
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "es:",
 +	       save->es.selector, save->es.attrib,
 +	       save->es.limit, save->es.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "cs:",
 +	       save->cs.selector, save->cs.attrib,
 +	       save->cs.limit, save->cs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ss:",
 +	       save->ss.selector, save->ss.attrib,
 +	       save->ss.limit, save->ss.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ds:",
 +	       save->ds.selector, save->ds.attrib,
 +	       save->ds.limit, save->ds.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "fs:",
 +	       save->fs.selector, save->fs.attrib,
 +	       save->fs.limit, save->fs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "gs:",
 +	       save->gs.selector, save->gs.attrib,
 +	       save->gs.limit, save->gs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "gdtr:",
 +	       save->gdtr.selector, save->gdtr.attrib,
 +	       save->gdtr.limit, save->gdtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ldtr:",
 +	       save->ldtr.selector, save->ldtr.attrib,
 +	       save->ldtr.limit, save->ldtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "idtr:",
 +	       save->idtr.selector, save->idtr.attrib,
 +	       save->idtr.limit, save->idtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "tr:",
 +	       save->tr.selector, save->tr.attrib,
 +	       save->tr.limit, save->tr.base);
 +	pr_err("cpl:            %d                efer:         %016llx\n",
 +		save->cpl, save->efer);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cr0:", save->cr0, "cr2:", save->cr2);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cr3:", save->cr3, "cr4:", save->cr4);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "dr6:", save->dr6, "dr7:", save->dr7);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "rip:", save->rip, "rflags:", save->rflags);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "rsp:", save->rsp, "rax:", save->rax);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "star:", save->star, "lstar:", save->lstar);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cstar:", save->cstar, "sfmask:", save->sfmask);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "kernel_gs_base:", save->kernel_gs_base,
 +	       "sysenter_cs:", save->sysenter_cs);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "sysenter_esp:", save->sysenter_esp,
 +	       "sysenter_eip:", save->sysenter_eip);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "gpat:", save->g_pat, "dbgctl:", save->dbgctl);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "br_from:", save->br_from, "br_to:", save->br_to);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "excp_from:", save->last_excp_from,
 +	       "excp_to:", save->last_excp_to);
 +}
 +
 +static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +{
 +	struct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;
 +
 +	*info1 = control->exit_info_1;
 +	*info2 = control->exit_info_2;
 +}
 +
 +static int handle_exit(struct kvm_vcpu *vcpu,
 +	enum exit_fastpath_completion exit_fastpath)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct kvm_run *kvm_run = vcpu->run;
 +	u32 exit_code = svm->vmcb->control.exit_code;
 +
 +	trace_kvm_exit(exit_code, vcpu, KVM_ISA_SVM);
 +
 +	if (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))
 +		vcpu->arch.cr0 = svm->vmcb->save.cr0;
 +	if (npt_enabled)
 +		vcpu->arch.cr3 = svm->vmcb->save.cr3;
 +
 +	if (unlikely(svm->nested.exit_required)) {
 +		nested_svm_vmexit(svm);
 +		svm->nested.exit_required = false;
 +
 +		return 1;
 +	}
 +
 +	if (is_guest_mode(vcpu)) {
 +		int vmexit;
 +
 +		trace_kvm_nested_vmexit(svm->vmcb->save.rip, exit_code,
 +					svm->vmcb->control.exit_info_1,
 +					svm->vmcb->control.exit_info_2,
 +					svm->vmcb->control.exit_int_info,
 +					svm->vmcb->control.exit_int_info_err,
 +					KVM_ISA_SVM);
 +
 +		vmexit = nested_svm_exit_special(svm);
 +
 +		if (vmexit == NESTED_EXIT_CONTINUE)
 +			vmexit = nested_svm_exit_handled(svm);
 +
 +		if (vmexit == NESTED_EXIT_DONE)
 +			return 1;
 +	}
 +
 +	svm_complete_interrupts(svm);
 +
 +	if (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {
 +		kvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 +		kvm_run->fail_entry.hardware_entry_failure_reason
 +			= svm->vmcb->control.exit_code;
 +		dump_vmcb(vcpu);
 +		return 0;
 +	}
 +
 +	if (is_external_interrupt(svm->vmcb->control.exit_int_info) &&
 +	    exit_code != SVM_EXIT_EXCP_BASE + PF_VECTOR &&
 +	    exit_code != SVM_EXIT_NPF && exit_code != SVM_EXIT_TASK_SWITCH &&
 +	    exit_code != SVM_EXIT_INTR && exit_code != SVM_EXIT_NMI)
 +		printk(KERN_ERR "%s: unexpected exit_int_info 0x%x "
 +		       "exit_code 0x%x\n",
 +		       __func__, svm->vmcb->control.exit_int_info,
 +		       exit_code);
 +
 +	if (exit_fastpath == EXIT_FASTPATH_SKIP_EMUL_INS) {
 +		kvm_skip_emulated_instruction(vcpu);
 +		return 1;
 +	} else if (exit_code >= ARRAY_SIZE(svm_exit_handlers)
 +	    || !svm_exit_handlers[exit_code]) {
 +		vcpu_unimpl(vcpu, "svm: unexpected exit reason 0x%x\n", exit_code);
 +		dump_vmcb(vcpu);
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror =
 +			KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
 +		vcpu->run->internal.ndata = 1;
 +		vcpu->run->internal.data[0] = exit_code;
 +		return 0;
 +	}
 +
 +#ifdef CONFIG_RETPOLINE
 +	if (exit_code == SVM_EXIT_MSR)
 +		return msr_interception(svm);
 +	else if (exit_code == SVM_EXIT_VINTR)
 +		return interrupt_window_interception(svm);
 +	else if (exit_code == SVM_EXIT_INTR)
 +		return intr_interception(svm);
 +	else if (exit_code == SVM_EXIT_HLT)
 +		return halt_interception(svm);
 +	else if (exit_code == SVM_EXIT_NPF)
 +		return npf_interception(svm);
 +#endif
 +	return svm_exit_handlers[exit_code](svm);
 +}
 +
 +static void reload_tss(struct kvm_vcpu *vcpu)
 +{
 +	int cpu = raw_smp_processor_id();
 +
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +	sd->tss_desc->type = 9; /* available 32/64-bit TSS */
 +	load_TR_desc();
 +}
 +
 +static void pre_sev_run(struct vcpu_svm *svm, int cpu)
 +{
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +	int asid = sev_get_asid(svm->vcpu.kvm);
 +
 +	/* Assign the asid allocated with this SEV guest */
 +	svm->vmcb->control.asid = asid;
 +
 +	/*
 +	 * Flush guest TLB:
 +	 *
 +	 * 1) when different VMCB for the same ASID is to be run on the same host CPU.
 +	 * 2) or this VMCB was executed on different host CPU in previous VMRUNs.
 +	 */
 +	if (sd->sev_vmcbs[asid] == svm->vmcb &&
 +	    svm->last_cpu == cpu)
 +		return;
 +
 +	svm->last_cpu = cpu;
 +	sd->sev_vmcbs[asid] = svm->vmcb;
 +	svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 +	mark_dirty(svm->vmcb, VMCB_ASID);
 +}
 +
 +static void pre_svm_run(struct vcpu_svm *svm)
 +{
 +	int cpu = raw_smp_processor_id();
 +
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +
 +	if (sev_guest(svm->vcpu.kvm))
 +		return pre_sev_run(svm, cpu);
 +
 +	/* FIXME: handle wraparound of asid_generation */
 +	if (svm->asid_generation != sd->asid_generation)
 +		new_asid(svm, sd);
 +}
 +
 +static void svm_inject_nmi(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;
 +	vcpu->arch.hflags |= HF_NMI_MASK;
 +	set_intercept(svm, INTERCEPT_IRET);
 +	++vcpu->stat.nmi_injections;
 +}
 +
 +static void svm_set_irq(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	BUG_ON(!(gif_set(svm)));
 +
 +	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 +	++vcpu->stat.irq_injections;
 +
 +	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 +		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
 +}
 +
 +static inline bool svm_nested_virtualize_tpr(struct kvm_vcpu *vcpu)
 +{
 +	return is_guest_mode(vcpu) && (vcpu->arch.hflags & HF_VINTR_MASK);
 +}
 +
 +static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (svm_nested_virtualize_tpr(vcpu))
 +		return;
 +
 +	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +
 +	if (irr == -1)
 +		return;
 +
 +	if (tpr >= irr)
 +		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +}
 +
 +static void svm_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 +{
 +	return;
 +}
 +
 +static void svm_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 +{
 +}
 +
 +static void svm_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 +{
 +}
 +
 +static int svm_set_pi_irte_mode(struct kvm_vcpu *vcpu, bool activate)
 +{
 +	int ret = 0;
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *ir;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (!kvm_arch_has_assigned_device(vcpu->kvm))
 +		return 0;
 +
 +	/*
 +	 * Here, we go through the per-vcpu ir_list to update all existing
 +	 * interrupt remapping table entry targeting this vcpu.
 +	 */
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +
 +	if (list_empty(&svm->ir_list))
 +		goto out;
 +
 +	list_for_each_entry(ir, &svm->ir_list, node) {
 +		if (activate)
 +			ret = amd_iommu_activate_guest_mode(ir->data);
 +		else
 +			ret = amd_iommu_deactivate_guest_mode(ir->data);
 +		if (ret)
 +			break;
 +	}
 +out:
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +	return ret;
 +}
 +
 +static void svm_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	bool activated = kvm_vcpu_apicv_active(vcpu);
 +
 +	if (!avic)
 +		return;
 +
 +	if (activated) {
 +		/**
 +		 * During AVIC temporary deactivation, guest could update
 +		 * APIC ID, DFR and LDR registers, which would not be trapped
 +		 * by avic_unaccelerated_access_interception(). In this case,
 +		 * we need to check and update the AVIC logical APIC ID table
 +		 * accordingly before re-activating.
 +		 */
 +		avic_post_state_restore(vcpu);
 +		vmcb->control.int_ctl |= AVIC_ENABLE_MASK;
 +	} else {
 +		vmcb->control.int_ctl &= ~AVIC_ENABLE_MASK;
 +	}
 +	mark_dirty(vmcb, VMCB_AVIC);
 +
 +	svm_set_pi_irte_mode(vcpu, activated);
 +}
 +
 +static void svm_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 +{
 +	return;
 +}
 +
 +static int svm_deliver_avic_intr(struct kvm_vcpu *vcpu, int vec)
 +{
 +	if (!vcpu->arch.apicv_active)
 +		return -1;
 +
 +	kvm_lapic_set_irr(vec, vcpu->arch.apic);
 +	smp_mb__after_atomic();
 +
 +	if (avic_vcpu_is_running(vcpu)) {
 +		int cpuid = vcpu->cpu;
 +
 +		if (cpuid != get_cpu())
 +			wrmsrl(SVM_AVIC_DOORBELL, kvm_cpu_get_apicid(cpuid));
 +		put_cpu();
 +	} else
 +		kvm_vcpu_wake_up(vcpu);
 +
 +	return 0;
 +}
 +
 +static bool svm_dy_apicv_has_pending_interrupt(struct kvm_vcpu *vcpu)
 +{
 +	return false;
 +}
 +
 +static void svm_ir_list_del(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 +{
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *cur;
 +
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +	list_for_each_entry(cur, &svm->ir_list, node) {
 +		if (cur->data != pi->ir_data)
 +			continue;
 +		list_del(&cur->node);
 +		kfree(cur);
 +		break;
 +	}
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +}
 +
 +static int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 +{
 +	int ret = 0;
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *ir;
 +
 +	/**
 +	 * In some cases, the existing irte is updaed and re-set,
 +	 * so we need to check here if it's already been * added
 +	 * to the ir_list.
 +	 */
 +	if (pi->ir_data && (pi->prev_ga_tag != 0)) {
 +		struct kvm *kvm = svm->vcpu.kvm;
 +		u32 vcpu_id = AVIC_GATAG_TO_VCPUID(pi->prev_ga_tag);
 +		struct kvm_vcpu *prev_vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
 +		struct vcpu_svm *prev_svm;
 +
 +		if (!prev_vcpu) {
 +			ret = -EINVAL;
 +			goto out;
 +		}
 +
 +		prev_svm = to_svm(prev_vcpu);
 +		svm_ir_list_del(prev_svm, pi);
 +	}
 +
 +	/**
 +	 * Allocating new amd_iommu_pi_data, which will get
 +	 * add to the per-vcpu ir_list.
 +	 */
 +	ir = kzalloc(sizeof(struct amd_svm_iommu_ir), GFP_KERNEL_ACCOUNT);
 +	if (!ir) {
 +		ret = -ENOMEM;
 +		goto out;
 +	}
 +	ir->data = pi->ir_data;
 +
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +	list_add(&ir->node, &svm->ir_list);
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +out:
 +	return ret;
 +}
 +
 +/**
 + * Note:
 + * The HW cannot support posting multicast/broadcast
 + * interrupts to a vCPU. So, we still use legacy interrupt
 + * remapping for these kind of interrupts.
 + *
 + * For lowest-priority interrupts, we only support
 + * those with single CPU as the destination, e.g. user
 + * configures the interrupts via /proc/irq or uses
 + * irqbalance to make the interrupts single-CPU.
 + */
 +static int
 +get_pi_vcpu_info(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 +		 struct vcpu_data *vcpu_info, struct vcpu_svm **svm)
 +{
 +	struct kvm_lapic_irq irq;
 +	struct kvm_vcpu *vcpu = NULL;
 +
 +	kvm_set_msi_irq(kvm, e, &irq);
 +
 +	if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
 +	    !kvm_irq_is_postable(&irq)) {
 +		pr_debug("SVM: %s: use legacy intr remap mode for irq %u\n",
 +			 __func__, irq.vector);
 +		return -1;
 +	}
 +
 +	pr_debug("SVM: %s: use GA mode for irq %u\n", __func__,
 +		 irq.vector);
 +	*svm = to_svm(vcpu);
 +	vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
 +	vcpu_info->vector = irq.vector;
 +
 +	return 0;
 +}
 +
 +/*
 + * svm_update_pi_irte - set IRTE for Posted-Interrupts
 + *
 + * @kvm: kvm
 + * @host_irq: host irq of the interrupt
 + * @guest_irq: gsi of the interrupt
 + * @set: set or unset PI
 + * returns 0 on success, < 0 on failure
 + */
 +static int svm_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 +			      uint32_t guest_irq, bool set)
 +{
 +	struct kvm_kernel_irq_routing_entry *e;
 +	struct kvm_irq_routing_table *irq_rt;
 +	int idx, ret = -EINVAL;
 +
 +	if (!kvm_arch_has_assigned_device(kvm) ||
 +	    !irq_remapping_cap(IRQ_POSTING_CAP))
 +		return 0;
 +
 +	pr_debug("SVM: %s: host_irq=%#x, guest_irq=%#x, set=%#x\n",
 +		 __func__, host_irq, guest_irq, set);
 +
 +	idx = srcu_read_lock(&kvm->irq_srcu);
 +	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
 +	WARN_ON(guest_irq >= irq_rt->nr_rt_entries);
 +
 +	hlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {
 +		struct vcpu_data vcpu_info;
 +		struct vcpu_svm *svm = NULL;
 +
 +		if (e->type != KVM_IRQ_ROUTING_MSI)
 +			continue;
 +
 +		/**
 +		 * Here, we setup with legacy mode in the following cases:
 +		 * 1. When cannot target interrupt to a specific vcpu.
 +		 * 2. Unsetting posted interrupt.
 +		 * 3. APIC virtialization is disabled for the vcpu.
 +		 * 4. IRQ has incompatible delivery mode (SMI, INIT, etc)
 +		 */
 +		if (!get_pi_vcpu_info(kvm, e, &vcpu_info, &svm) && set &&
 +		    kvm_vcpu_apicv_active(&svm->vcpu)) {
 +			struct amd_iommu_pi_data pi;
 +
 +			/* Try to enable guest_mode in IRTE */
 +			pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
 +					    AVIC_HPA_MASK);
 +			pi.ga_tag = AVIC_GATAG(to_kvm_svm(kvm)->avic_vm_id,
 +						     svm->vcpu.vcpu_id);
 +			pi.is_guest_mode = true;
 +			pi.vcpu_data = &vcpu_info;
 +			ret = irq_set_vcpu_affinity(host_irq, &pi);
 +
 +			/**
 +			 * Here, we successfully setting up vcpu affinity in
 +			 * IOMMU guest mode. Now, we need to store the posted
 +			 * interrupt information in a per-vcpu ir_list so that
 +			 * we can reference to them directly when we update vcpu
 +			 * scheduling information in IOMMU irte.
 +			 */
 +			if (!ret && pi.is_guest_mode)
 +				svm_ir_list_add(svm, &pi);
 +		} else {
 +			/* Use legacy mode in IRTE */
 +			struct amd_iommu_pi_data pi;
 +
 +			/**
 +			 * Here, pi is used to:
 +			 * - Tell IOMMU to use legacy mode for this interrupt.
 +			 * - Retrieve ga_tag of prior interrupt remapping data.
 +			 */
 +			pi.is_guest_mode = false;
 +			ret = irq_set_vcpu_affinity(host_irq, &pi);
 +
 +			/**
 +			 * Check if the posted interrupt was previously
 +			 * setup with the guest_mode by checking if the ga_tag
 +			 * was cached. If so, we need to clean up the per-vcpu
 +			 * ir_list.
 +			 */
 +			if (!ret && pi.prev_ga_tag) {
 +				int id = AVIC_GATAG_TO_VCPUID(pi.prev_ga_tag);
 +				struct kvm_vcpu *vcpu;
 +
 +				vcpu = kvm_get_vcpu_by_id(kvm, id);
 +				if (vcpu)
 +					svm_ir_list_del(to_svm(vcpu), &pi);
 +			}
 +		}
 +
 +		if (!ret && svm) {
 +			trace_kvm_pi_irte_update(host_irq, svm->vcpu.vcpu_id,
 +						 e->gsi, vcpu_info.vector,
 +						 vcpu_info.pi_desc_addr, set);
 +		}
 +
 +		if (ret < 0) {
 +			pr_err("%s: failed to update PI IRTE\n", __func__);
 +			goto out;
 +		}
 +	}
 +
 +	ret = 0;
 +out:
 +	srcu_read_unlock(&kvm->irq_srcu, idx);
 +	return ret;
 +}
 +
 +static int svm_nmi_allowed(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	int ret;
 +	ret = !(vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) &&
 +	      !(svm->vcpu.arch.hflags & HF_NMI_MASK);
 +	ret = ret && gif_set(svm) && nested_svm_nmi(svm);
 +
 +	return ret;
 +}
 +
 +static bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	return !!(svm->vcpu.arch.hflags & HF_NMI_MASK);
 +}
 +
 +static void svm_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (masked) {
 +		svm->vcpu.arch.hflags |= HF_NMI_MASK;
 +		set_intercept(svm, INTERCEPT_IRET);
 +	} else {
 +		svm->vcpu.arch.hflags &= ~HF_NMI_MASK;
 +		clr_intercept(svm, INTERCEPT_IRET);
 +	}
 +}
 +
 +static int svm_interrupt_allowed(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	int ret;
 +
 +	if (!gif_set(svm) ||
 +	     (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK))
 +		return 0;
 +
 +	ret = !!(kvm_get_rflags(vcpu) & X86_EFLAGS_IF);
 +
 +	if (is_guest_mode(vcpu))
 +		return ret && !(svm->vcpu.arch.hflags & HF_VINTR_MASK);
 +
 +	return ret;
 +}
 +
 +static void enable_irq_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	/*
 +	 * In case GIF=0 we can't rely on the CPU to tell us when GIF becomes
 +	 * 1, because that's a separate STGI/VMRUN intercept.  The next time we
 +	 * get that intercept, this function will be called again though and
 +	 * we'll get the vintr intercept. However, if the vGIF feature is
 +	 * enabled, the STGI interception will not occur. Enable the irq
 +	 * window under the assumption that the hardware will set the GIF.
 +	 */
 +	if ((vgif_enabled(svm) || gif_set(svm)) && nested_svm_intr(svm)) {
 +		svm_set_vintr(svm);
 +	}
 +}
 +
 +static void enable_nmi_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if ((svm->vcpu.arch.hflags & (HF_NMI_MASK | HF_IRET_MASK))
 +	    == HF_NMI_MASK)
 +		return; /* IRET will cause a vm exit */
 +
 +	if (!gif_set(svm)) {
 +		if (vgif_enabled(svm))
 +			set_intercept(svm, INTERCEPT_STGI);
 +		return; /* STGI will cause a vm exit */
 +	}
 +
 +	if (svm->nested.exit_required)
 +		return; /* we're not going to run the guest yet */
 +
 +	/*
 +	 * Something prevents NMI from been injected. Single step over possible
 +	 * problem (IRET or exception injection or interrupt shadow)
 +	 */
 +	svm->nmi_singlestep_guest_rflags = svm_get_rflags(vcpu);
 +	svm->nmi_singlestep = true;
 +	svm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);
 +}
 +
 +static int svm_set_tss_addr(struct kvm *kvm, unsigned int addr)
 +{
 +	return 0;
 +}
 +
 +static int svm_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 +{
 +	return 0;
 +}
 +
 +static void svm_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	/*
 +	 * Flush only the current ASID even if the TLB flush was invoked via
 +	 * kvm_flush_remote_tlbs().  Although flushing remote TLBs requires all
 +	 * ASIDs to be flushed, KVM uses a single ASID for L1 and L2, and
 +	 * unconditionally does a TLB flush on both nested VM-Enter and nested
 +	 * VM-Exit (via kvm_mmu_reset_context()).
 +	 */
 +	if (static_cpu_has(X86_FEATURE_FLUSHBYASID))
 +		svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 +	else
 +		svm->asid_generation--;
 +}
 +
 +static void svm_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t gva)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	invlpga(gva, svm->vmcb->control.asid);
 +}
 +
 +static void svm_flush_tlb_guest(struct kvm_vcpu *vcpu)
 +{
 +	svm_flush_tlb(vcpu, false);
 +}
 +
 +static void svm_prepare_guest_switch(struct kvm_vcpu *vcpu)
 +{
 +}
 +
 +static inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (svm_nested_virtualize_tpr(vcpu))
 +		return;
 +
 +	if (!is_cr_intercept(svm, INTERCEPT_CR8_WRITE)) {
 +		int cr8 = svm->vmcb->control.int_ctl & V_TPR_MASK;
 +		kvm_set_cr8(vcpu, cr8);
 +	}
 +}
 +
 +static inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u64 cr8;
 +
 +	if (svm_nested_virtualize_tpr(vcpu) ||
 +	    kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	cr8 = kvm_get_cr8(vcpu);
 +	svm->vmcb->control.int_ctl &= ~V_TPR_MASK;
 +	svm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;
 +}
 +
 +static void svm_complete_interrupts(struct vcpu_svm *svm)
 +{
 +	u8 vector;
 +	int type;
 +	u32 exitintinfo = svm->vmcb->control.exit_int_info;
 +	unsigned int3_injected = svm->int3_injected;
 +
 +	svm->int3_injected = 0;
 +
 +	/*
 +	 * If we've made progress since setting HF_IRET_MASK, we've
 +	 * executed an IRET and can allow NMI injection.
 +	 */
 +	if ((svm->vcpu.arch.hflags & HF_IRET_MASK)
 +	    && kvm_rip_read(&svm->vcpu) != svm->nmi_iret_rip) {
 +		svm->vcpu.arch.hflags &= ~(HF_NMI_MASK | HF_IRET_MASK);
 +		kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +	}
 +
 +	svm->vcpu.arch.nmi_injected = false;
 +	kvm_clear_exception_queue(&svm->vcpu);
 +	kvm_clear_interrupt_queue(&svm->vcpu);
 +
 +	if (!(exitintinfo & SVM_EXITINTINFO_VALID))
 +		return;
 +
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +
 +	vector = exitintinfo & SVM_EXITINTINFO_VEC_MASK;
 +	type = exitintinfo & SVM_EXITINTINFO_TYPE_MASK;
 +
 +	switch (type) {
 +	case SVM_EXITINTINFO_TYPE_NMI:
 +		svm->vcpu.arch.nmi_injected = true;
 +		break;
 +	case SVM_EXITINTINFO_TYPE_EXEPT:
 +		/*
 +		 * In case of software exceptions, do not reinject the vector,
 +		 * but re-execute the instruction instead. Rewind RIP first
 +		 * if we emulated INT3 before.
 +		 */
 +		if (kvm_exception_is_soft(vector)) {
 +			if (vector == BP_VECTOR && int3_injected &&
 +			    kvm_is_linear_rip(&svm->vcpu, svm->int3_rip))
 +				kvm_rip_write(&svm->vcpu,
 +					      kvm_rip_read(&svm->vcpu) -
 +					      int3_injected);
 +			break;
 +		}
 +		if (exitintinfo & SVM_EXITINTINFO_VALID_ERR) {
 +			u32 err = svm->vmcb->control.exit_int_info_err;
 +			kvm_requeue_exception_e(&svm->vcpu, vector, err);
 +
 +		} else
 +			kvm_requeue_exception(&svm->vcpu, vector);
 +		break;
 +	case SVM_EXITINTINFO_TYPE_INTR:
 +		kvm_queue_interrupt(&svm->vcpu, vector, false);
 +		break;
 +	default:
 +		break;
 +	}
  }
  
 -static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +static void svm_cancel_injection(struct kvm_vcpu *vcpu)
  {
 -	struct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb_control_area *control = &svm->vmcb->control;
  
 -	*info1 = control->exit_info_1;
 -	*info2 = control->exit_info_2;
 +	control->exit_int_info = control->event_inj;
 +	control->exit_int_info_err = control->event_inj_err;
 +	control->event_inj = 0;
 +	svm_complete_interrupts(svm);
  }
  
 -static int handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 +static void svm_vcpu_run(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct kvm_run *kvm_run = vcpu->run;
 -	u32 exit_code = svm->vmcb->control.exit_code;
  
 -	trace_kvm_exit(exit_code, vcpu, KVM_ISA_SVM);
 +	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 +	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 +	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
  
 -	if (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))
 -		vcpu->arch.cr0 = svm->vmcb->save.cr0;
 -	if (npt_enabled)
 -		vcpu->arch.cr3 = svm->vmcb->save.cr3;
 +	/*
 +	 * A vmexit emulation is required before the vcpu can be executed
 +	 * again.
 +	 */
 +	if (unlikely(svm->nested.exit_required))
 +		return;
  
 -	if (is_guest_mode(vcpu)) {
 -		int vmexit;
 +	/*
 +	 * Disable singlestep if we're injecting an interrupt/exception.
 +	 * We don't want our modified rflags to be pushed on the stack where
 +	 * we might not be able to easily reset them if we disabled NMI
 +	 * singlestep later.
 +	 */
 +	if (svm->nmi_singlestep && svm->vmcb->control.event_inj) {
 +		/*
 +		 * Event injection happens before external interrupts cause a
 +		 * vmexit and interrupts are disabled here, so smp_send_reschedule
 +		 * is enough to force an immediate vmexit.
 +		 */
 +		disable_nmi_singlestep(svm);
 +		smp_send_reschedule(vcpu->cpu);
 +	}
  
 -		trace_kvm_nested_vmexit(svm->vmcb->save.rip, exit_code,
 -					svm->vmcb->control.exit_info_1,
 -					svm->vmcb->control.exit_info_2,
 -					svm->vmcb->control.exit_int_info,
 -					svm->vmcb->control.exit_int_info_err,
 -					KVM_ISA_SVM);
 +	pre_svm_run(svm);
  
 -		vmexit = nested_svm_exit_special(svm);
 +	sync_lapic_to_cr8(vcpu);
  
 -		if (vmexit == NESTED_EXIT_CONTINUE)
 -			vmexit = nested_svm_exit_handled(svm);
 +	svm->vmcb->save.cr2 = vcpu->arch.cr2;
  
 -		if (vmexit == NESTED_EXIT_DONE)
 -			return 1;
 +	clgi();
 +	kvm_load_guest_xsave_state(vcpu);
 +
 +	if (lapic_in_kernel(vcpu) &&
 +		vcpu->arch.apic->lapic_timer.timer_advance_ns)
 +		kvm_wait_lapic_expire(vcpu);
 +
 +	/*
 +	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 +	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 +	 * is no need to worry about the conditional branch over the wrmsr
 +	 * being speculatively taken.
 +	 */
 +	x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
 +
 +	local_irq_enable();
 +
 +	asm volatile (
 +		"push %%" _ASM_BP "; \n\t"
 +		"mov %c[rbx](%[svm]), %%" _ASM_BX " \n\t"
 +		"mov %c[rcx](%[svm]), %%" _ASM_CX " \n\t"
 +		"mov %c[rdx](%[svm]), %%" _ASM_DX " \n\t"
 +		"mov %c[rsi](%[svm]), %%" _ASM_SI " \n\t"
 +		"mov %c[rdi](%[svm]), %%" _ASM_DI " \n\t"
 +		"mov %c[rbp](%[svm]), %%" _ASM_BP " \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %c[r8](%[svm]),  %%r8  \n\t"
 +		"mov %c[r9](%[svm]),  %%r9  \n\t"
 +		"mov %c[r10](%[svm]), %%r10 \n\t"
 +		"mov %c[r11](%[svm]), %%r11 \n\t"
 +		"mov %c[r12](%[svm]), %%r12 \n\t"
 +		"mov %c[r13](%[svm]), %%r13 \n\t"
 +		"mov %c[r14](%[svm]), %%r14 \n\t"
 +		"mov %c[r15](%[svm]), %%r15 \n\t"
 +#endif
 +
 +		/* Enter guest mode */
 +		"push %%" _ASM_AX " \n\t"
 +		"mov %c[vmcb](%[svm]), %%" _ASM_AX " \n\t"
 +		__ex("vmload %%" _ASM_AX) "\n\t"
 +		__ex("vmrun %%" _ASM_AX) "\n\t"
 +		__ex("vmsave %%" _ASM_AX) "\n\t"
 +		"pop %%" _ASM_AX " \n\t"
 +
 +		/* Save guest registers, load host registers */
 +		"mov %%" _ASM_BX ", %c[rbx](%[svm]) \n\t"
 +		"mov %%" _ASM_CX ", %c[rcx](%[svm]) \n\t"
 +		"mov %%" _ASM_DX ", %c[rdx](%[svm]) \n\t"
 +		"mov %%" _ASM_SI ", %c[rsi](%[svm]) \n\t"
 +		"mov %%" _ASM_DI ", %c[rdi](%[svm]) \n\t"
 +		"mov %%" _ASM_BP ", %c[rbp](%[svm]) \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %%r8,  %c[r8](%[svm]) \n\t"
 +		"mov %%r9,  %c[r9](%[svm]) \n\t"
 +		"mov %%r10, %c[r10](%[svm]) \n\t"
 +		"mov %%r11, %c[r11](%[svm]) \n\t"
 +		"mov %%r12, %c[r12](%[svm]) \n\t"
 +		"mov %%r13, %c[r13](%[svm]) \n\t"
 +		"mov %%r14, %c[r14](%[svm]) \n\t"
 +		"mov %%r15, %c[r15](%[svm]) \n\t"
 +		/*
 +		* Clear host registers marked as clobbered to prevent
 +		* speculative use.
 +		*/
 +		"xor %%r8d, %%r8d \n\t"
 +		"xor %%r9d, %%r9d \n\t"
 +		"xor %%r10d, %%r10d \n\t"
 +		"xor %%r11d, %%r11d \n\t"
 +		"xor %%r12d, %%r12d \n\t"
 +		"xor %%r13d, %%r13d \n\t"
 +		"xor %%r14d, %%r14d \n\t"
 +		"xor %%r15d, %%r15d \n\t"
 +#endif
 +		"xor %%ebx, %%ebx \n\t"
 +		"xor %%ecx, %%ecx \n\t"
 +		"xor %%edx, %%edx \n\t"
 +		"xor %%esi, %%esi \n\t"
 +		"xor %%edi, %%edi \n\t"
 +		"pop %%" _ASM_BP
 +		:
 +		: [svm]"a"(svm),
 +		  [vmcb]"i"(offsetof(struct vcpu_svm, vmcb_pa)),
 +		  [rbx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBX])),
 +		  [rcx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RCX])),
 +		  [rdx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDX])),
 +		  [rsi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RSI])),
 +		  [rdi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDI])),
 +		  [rbp]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBP]))
 +#ifdef CONFIG_X86_64
 +		  , [r8]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R8])),
 +		  [r9]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R9])),
 +		  [r10]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R10])),
 +		  [r11]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R11])),
 +		  [r12]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R12])),
 +		  [r13]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R13])),
 +		  [r14]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R14])),
 +		  [r15]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R15]))
 +#endif
 +		: "cc", "memory"
 +#ifdef CONFIG_X86_64
 +		, "rbx", "rcx", "rdx", "rsi", "rdi"
 +		, "r8", "r9", "r10", "r11" , "r12", "r13", "r14", "r15"
 +#else
 +		, "ebx", "ecx", "edx", "esi", "edi"
 +#endif
 +		);
 +
 +	/* Eliminate branch target predictions from guest mode */
 +	vmexit_fill_RSB();
 +
 +#ifdef CONFIG_X86_64
 +	wrmsrl(MSR_GS_BASE, svm->host.gs_base);
 +#else
 +	loadsegment(fs, svm->host.fs);
 +#ifndef CONFIG_X86_32_LAZY_GS
 +	loadsegment(gs, svm->host.gs);
 +#endif
 +#endif
 +
 +	/*
 +	 * We do not use IBRS in the kernel. If this vCPU has used the
 +	 * SPEC_CTRL MSR it may have left it on; save the value and
 +	 * turn it off. This is much more efficient than blindly adding
 +	 * it to the atomic save/restore list. Especially as the former
 +	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 +	 *
 +	 * For non-nested case:
 +	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 *
 +	 * For nested case:
 +	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 */
 +	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 +		svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 +
 +	reload_tss(vcpu);
 +
 +	local_irq_disable();
 +
 +	x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
 +
 +	vcpu->arch.cr2 = svm->vmcb->save.cr2;
 +	vcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;
 +	vcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;
 +	vcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;
 +
 +	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 +		kvm_before_interrupt(&svm->vcpu);
 +
 +	kvm_load_host_xsave_state(vcpu);
 +	stgi();
 +
 +	/* Any pending NMI will happen here */
 +
 +	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 +		kvm_after_interrupt(&svm->vcpu);
 +
 +	sync_cr8_to_lapic(vcpu);
 +
 +	svm->next_rip = 0;
 +
 +	svm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;
 +
 +	/* if exit due to PF check for async PF */
 +	if (svm->vmcb->control.exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR)
 +		svm->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +
 +	if (npt_enabled) {
 +		vcpu->arch.regs_avail &= ~(1 << VCPU_EXREG_PDPTR);
 +		vcpu->arch.regs_dirty &= ~(1 << VCPU_EXREG_PDPTR);
  	}
  
 -	svm_complete_interrupts(svm);
 +	/*
 +	 * We need to handle MC intercepts here before the vcpu has a chance to
 +	 * change the physical cpu
 +	 */
 +	if (unlikely(svm->vmcb->control.exit_code ==
 +		     SVM_EXIT_EXCP_BASE + MC_VECTOR))
 +		svm_handle_mce(svm);
  
 -	if (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {
 -		kvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 -		kvm_run->fail_entry.hardware_entry_failure_reason
 -			= svm->vmcb->control.exit_code;
 -		dump_vmcb(vcpu);
 -		return 0;
 +	mark_all_clean(svm->vmcb);
 +}
 +
 +static void svm_set_cr3(struct kvm_vcpu *vcpu, unsigned long root)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
++<<<<<<< HEAD
++=======
++	unsigned long cr3;
++>>>>>>> 978ce5837c7e (KVM: SVM: always update CR3 in VMCB)
 +
 +	svm->vmcb->save.cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_CR);
 +}
 +
++<<<<<<< HEAD
 +static void set_tdp_cr3(struct kvm_vcpu *vcpu, unsigned long root)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->vmcb->control.nested_cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_NPT);
 +
 +	/* Also sync guest cr3 here in case we live migrate */
 +	svm->vmcb->save.cr3 = kvm_read_cr3(vcpu);
++=======
++		/* Loading L2's CR3 is handled by enter_svm_guest_mode.  */
++		if (!test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))
++			return;
++		cr3 = vcpu->arch.cr3;
+ 	}
+ 
 -	if (is_external_interrupt(svm->vmcb->control.exit_int_info) &&
 -	    exit_code != SVM_EXIT_EXCP_BASE + PF_VECTOR &&
 -	    exit_code != SVM_EXIT_NPF && exit_code != SVM_EXIT_TASK_SWITCH &&
 -	    exit_code != SVM_EXIT_INTR && exit_code != SVM_EXIT_NMI)
 -		printk(KERN_ERR "%s: unexpected exit_int_info 0x%x "
 -		       "exit_code 0x%x\n",
 -		       __func__, svm->vmcb->control.exit_int_info,
 -		       exit_code);
++	svm->vmcb->save.cr3 = cr3;
++>>>>>>> 978ce5837c7e (KVM: SVM: always update CR3 in VMCB)
 +	mark_dirty(svm->vmcb, VMCB_CR);
 +}
 +
 +static int is_disabled(void)
 +{
 +	u64 vm_cr;
  
 -	if (exit_fastpath != EXIT_FASTPATH_NONE)
 +	rdmsrl(MSR_VM_CR, vm_cr);
 +	if (vm_cr & (1 << SVM_VM_CR_SVM_DISABLE))
  		return 1;
  
 -	if (exit_code >= ARRAY_SIZE(svm_exit_handlers)
 -	    || !svm_exit_handlers[exit_code]) {
 -		vcpu_unimpl(vcpu, "svm: unexpected exit reason 0x%x\n", exit_code);
 -		dump_vmcb(vcpu);
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror =
 -			KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
 -		vcpu->run->internal.ndata = 1;
 -		vcpu->run->internal.data[0] = exit_code;
 -		return 0;
 -	}
 +	return 0;
 +}
  
 -#ifdef CONFIG_RETPOLINE
 -	if (exit_code == SVM_EXIT_MSR)
 -		return msr_interception(svm);
 -	else if (exit_code == SVM_EXIT_VINTR)
 -		return interrupt_window_interception(svm);
 -	else if (exit_code == SVM_EXIT_INTR)
 -		return intr_interception(svm);
 -	else if (exit_code == SVM_EXIT_HLT)
 -		return halt_interception(svm);
 -	else if (exit_code == SVM_EXIT_NPF)
 -		return npf_interception(svm);
 -#endif
 -	return svm_exit_handlers[exit_code](svm);
 +static void
 +svm_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +{
 +	/*
 +	 * Patch in the VMMCALL instruction:
 +	 */
 +	hypercall[0] = 0x0f;
 +	hypercall[1] = 0x01;
 +	hypercall[2] = 0xd9;
 +}
 +
 +static int __init svm_check_processor_compat(void)
 +{
 +	return 0;
 +}
 +
 +static bool svm_cpu_has_accelerated_tpr(void)
 +{
 +	return false;
 +}
 +
 +static bool svm_has_emulated_msr(u32 index)
 +{
 +	switch (index) {
 +	case MSR_IA32_MCG_EXT_CTL:
 +	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 +		return false;
 +	default:
 +		break;
 +	}
 +
 +	return true;
  }
  
 -static void reload_tss(struct kvm_vcpu *vcpu)
 +static u64 svm_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
  {
 -	int cpu = raw_smp_processor_id();
 -
 -	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 -	sd->tss_desc->type = 9; /* available 32/64-bit TSS */
 -	load_TR_desc();
 +	return 0;
  }
  
 -static void pre_svm_run(struct vcpu_svm *svm)
 +static void svm_cpuid_update(struct kvm_vcpu *vcpu)
  {
 -	int cpu = raw_smp_processor_id();
 +	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +	vcpu->arch.xsaves_enabled = guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&
 +				    boot_cpu_has(X86_FEATURE_XSAVE) &&
 +				    boot_cpu_has(X86_FEATURE_XSAVES);
  
 -	if (sev_guest(svm->vcpu.kvm))
 -		return pre_sev_run(svm, cpu);
 +	/* Update nrips enabled cache */
 +	svm->nrips_enabled = !!guest_cpuid_has(&svm->vcpu, X86_FEATURE_NRIPS);
  
 -	/* FIXME: handle wraparound of asid_generation */
 -	if (svm->asid_generation != sd->asid_generation)
 -		new_asid(svm, sd);
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	guest_cpuid_clear(vcpu, X86_FEATURE_X2APIC);
  }
  
 -static void svm_inject_nmi(struct kvm_vcpu *vcpu)
 +#define F feature_bit
 +
 +static void svm_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	switch (func) {
 +	case 0x1:
 +		if (avic)
 +			entry->ecx &= ~F(X2APIC);
 +		break;
 +	case 0x80000001:
 +		if (nested)
 +			entry->ecx |= (1 << 2); /* Set SVM bit */
 +		break;
 +	case 0x80000008:
 +		if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) ||
 +		     boot_cpu_has(X86_FEATURE_AMD_SSBD))
 +			entry->ebx |= F(VIRT_SSBD);
 +		break;
 +	case 0x8000000A:
 +		entry->eax = 1; /* SVM revision 1 */
 +		entry->ebx = 8; /* Lets support 8 ASIDs in case we add proper
 +				   ASID emulation to nested SVM */
 +		entry->ecx = 0; /* Reserved */
 +		entry->edx = 0; /* Per default do not support any
 +				   additional features */
 +
 +		/* Support next_rip if host supports it */
 +		if (boot_cpu_has(X86_FEATURE_NRIPS))
 +			entry->edx |= F(NRIPS);
 +
 +		/* Support NPT for the guest if enabled */
 +		if (npt_enabled)
 +			entry->edx |= F(NPT);
  
 -	svm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;
 -	vcpu->arch.hflags |= HF_NMI_MASK;
 -	set_intercept(svm, INTERCEPT_IRET);
 -	++vcpu->stat.nmi_injections;
 +	}
  }
  
 -static void svm_set_irq(struct kvm_vcpu *vcpu)
 +static int svm_get_lpage_level(void)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	return PT_PDPE_LEVEL;
 +}
  
 -	BUG_ON(!(gif_set(svm)));
 +static bool svm_rdtscp_supported(void)
 +{
 +	return boot_cpu_has(X86_FEATURE_RDTSCP);
 +}
  
 -	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 -	++vcpu->stat.irq_injections;
 +static bool svm_invpcid_supported(void)
 +{
 +	return false;
 +}
  
 -	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 -		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
 +static bool svm_mpx_supported(void)
 +{
 +	return false;
  }
  
 -static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +static bool svm_xsaves_supported(void)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	return boot_cpu_has(X86_FEATURE_XSAVES);
 +}
  
 -	if (svm_nested_virtualize_tpr(vcpu))
 -		return;
 +static bool svm_umip_emulated(void)
 +{
 +	return false;
 +}
  
 -	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +static bool svm_pt_supported(void)
 +{
 +	return false;
 +}
  
 -	if (irr == -1)
 -		return;
 +static bool svm_has_wbinvd_exit(void)
 +{
 +	return true;
 +}
  
 -	if (tpr >= irr)
 -		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +static bool svm_pku_supported(void)
 +{
 +	return false;
  }
  
 -bool svm_nmi_blocked(struct kvm_vcpu *vcpu)
 +#define PRE_EX(exit)  { .exit_code = (exit), \
 +			.stage = X86_ICPT_PRE_EXCEPT, }
 +#define POST_EX(exit) { .exit_code = (exit), \
 +			.stage = X86_ICPT_POST_EXCEPT, }
 +#define POST_MEM(exit) { .exit_code = (exit), \
 +			.stage = X86_ICPT_POST_MEMACCESS, }
 +
 +static const struct __x86_intercept {
 +	u32 exit_code;
 +	enum x86_intercept_stage stage;
 +} x86_intercept_map[] = {
 +	[x86_intercept_cr_read]		= POST_EX(SVM_EXIT_READ_CR0),
 +	[x86_intercept_cr_write]	= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_clts]		= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_lmsw]		= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_smsw]		= POST_EX(SVM_EXIT_READ_CR0),
 +	[x86_intercept_dr_read]		= POST_EX(SVM_EXIT_READ_DR0),
 +	[x86_intercept_dr_write]	= POST_EX(SVM_EXIT_WRITE_DR0),
 +	[x86_intercept_sldt]		= POST_EX(SVM_EXIT_LDTR_READ),
 +	[x86_intercept_str]		= POST_EX(SVM_EXIT_TR_READ),
 +	[x86_intercept_lldt]		= POST_EX(SVM_EXIT_LDTR_WRITE),
 +	[x86_intercept_ltr]		= POST_EX(SVM_EXIT_TR_WRITE),
 +	[x86_intercept_sgdt]		= POST_EX(SVM_EXIT_GDTR_READ),
 +	[x86_intercept_sidt]		= POST_EX(SVM_EXIT_IDTR_READ),
 +	[x86_intercept_lgdt]		= POST_EX(SVM_EXIT_GDTR_WRITE),
 +	[x86_intercept_lidt]		= POST_EX(SVM_EXIT_IDTR_WRITE),
 +	[x86_intercept_vmrun]		= POST_EX(SVM_EXIT_VMRUN),
 +	[x86_intercept_vmmcall]		= POST_EX(SVM_EXIT_VMMCALL),
 +	[x86_intercept_vmload]		= POST_EX(SVM_EXIT_VMLOAD),
 +	[x86_intercept_vmsave]		= POST_EX(SVM_EXIT_VMSAVE),
 +	[x86_intercept_stgi]		= POST_EX(SVM_EXIT_STGI),
 +	[x86_intercept_clgi]		= POST_EX(SVM_EXIT_CLGI),
 +	[x86_intercept_skinit]		= POST_EX(SVM_EXIT_SKINIT),
 +	[x86_intercept_invlpga]		= POST_EX(SVM_EXIT_INVLPGA),
 +	[x86_intercept_rdtscp]		= POST_EX(SVM_EXIT_RDTSCP),
 +	[x86_intercept_monitor]		= POST_MEM(SVM_EXIT_MONITOR),
 +	[x86_intercept_mwait]		= POST_EX(SVM_EXIT_MWAIT),
 +	[x86_intercept_invlpg]		= POST_EX(SVM_EXIT_INVLPG),
 +	[x86_intercept_invd]		= POST_EX(SVM_EXIT_INVD),
 +	[x86_intercept_wbinvd]		= POST_EX(SVM_EXIT_WBINVD),
 +	[x86_intercept_wrmsr]		= POST_EX(SVM_EXIT_MSR),
 +	[x86_intercept_rdtsc]		= POST_EX(SVM_EXIT_RDTSC),
 +	[x86_intercept_rdmsr]		= POST_EX(SVM_EXIT_MSR),
 +	[x86_intercept_rdpmc]		= POST_EX(SVM_EXIT_RDPMC),
 +	[x86_intercept_cpuid]		= PRE_EX(SVM_EXIT_CPUID),
 +	[x86_intercept_rsm]		= PRE_EX(SVM_EXIT_RSM),
 +	[x86_intercept_pause]		= PRE_EX(SVM_EXIT_PAUSE),
 +	[x86_intercept_pushf]		= PRE_EX(SVM_EXIT_PUSHF),
 +	[x86_intercept_popf]		= PRE_EX(SVM_EXIT_POPF),
 +	[x86_intercept_intn]		= PRE_EX(SVM_EXIT_SWINT),
 +	[x86_intercept_iret]		= PRE_EX(SVM_EXIT_IRET),
 +	[x86_intercept_icebp]		= PRE_EX(SVM_EXIT_ICEBP),
 +	[x86_intercept_hlt]		= POST_EX(SVM_EXIT_HLT),
 +	[x86_intercept_in]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_ins]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_out]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_outs]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_xsetbv]		= PRE_EX(SVM_EXIT_XSETBV),
 +};
 +
 +#undef PRE_EX
 +#undef POST_EX
 +#undef POST_MEM
 +
 +static int svm_check_intercept(struct kvm_vcpu *vcpu,
 +			       struct x86_instruction_info *info,
 +			       enum x86_intercept_stage stage)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 +	int vmexit, ret = X86EMUL_CONTINUE;
 +	struct __x86_intercept icpt_info;
  	struct vmcb *vmcb = svm->vmcb;
 -	bool ret;
  
 -	if (!gif_set(svm))
 -		return true;
 +	if (info->intercept >= ARRAY_SIZE(x86_intercept_map))
 +		goto out;
  
 -	if (is_guest_mode(vcpu) && nested_exit_on_nmi(svm))
 -		return false;
 +	icpt_info = x86_intercept_map[info->intercept];
 +
 +	if (stage != icpt_info.stage)
 +		goto out;
 +
 +	switch (icpt_info.exit_code) {
 +	case SVM_EXIT_READ_CR0:
 +		if (info->intercept == x86_intercept_cr_read)
 +			icpt_info.exit_code += info->modrm_reg;
 +		break;
 +	case SVM_EXIT_WRITE_CR0: {
 +		unsigned long cr0, val;
 +		u64 intercept;
 +
 +		if (info->intercept == x86_intercept_cr_write)
 +			icpt_info.exit_code += info->modrm_reg;
 +
 +		if (icpt_info.exit_code != SVM_EXIT_WRITE_CR0 ||
 +		    info->intercept == x86_intercept_clts)
 +			break;
 +
 +		intercept = svm->nested.intercept;
 +
 +		if (!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0)))
 +			break;
 +
 +		cr0 = vcpu->arch.cr0 & ~SVM_CR0_SELECTIVE_MASK;
 +		val = info->src_val  & ~SVM_CR0_SELECTIVE_MASK;
 +
 +		if (info->intercept == x86_intercept_lmsw) {
 +			cr0 &= 0xfUL;
 +			val &= 0xfUL;
 +			/* lmsw can't clear PE - catch this here */
 +			if (cr0 & X86_CR0_PE)
 +				val |= X86_CR0_PE;
 +		}
 +
 +		if (cr0 ^ val)
 +			icpt_info.exit_code = SVM_EXIT_CR0_SEL_WRITE;
 +
 +		break;
 +	}
 +	case SVM_EXIT_READ_DR0:
 +	case SVM_EXIT_WRITE_DR0:
 +		icpt_info.exit_code += info->modrm_reg;
 +		break;
 +	case SVM_EXIT_MSR:
 +		if (info->intercept == x86_intercept_wrmsr)
 +			vmcb->control.exit_info_1 = 1;
 +		else
 +			vmcb->control.exit_info_1 = 0;
 +		break;
 +	case SVM_EXIT_PAUSE:
 +		/*
 +		 * We get this for NOP only, but pause
 +		 * is rep not, check this here
 +		 */
 +		if (info->rep_prefix != REPE_PREFIX)
 +			goto out;
 +		break;
 +	case SVM_EXIT_IOIO: {
 +		u64 exit_info;
 +		u32 bytes;
 +
 +		if (info->intercept == x86_intercept_in ||
 +		    info->intercept == x86_intercept_ins) {
 +			exit_info = ((info->src_val & 0xffff) << 16) |
 +				SVM_IOIO_TYPE_MASK;
 +			bytes = info->dst_bytes;
 +		} else {
 +			exit_info = (info->dst_val & 0xffff) << 16;
 +			bytes = info->src_bytes;
 +		}
 +
 +		if (info->intercept == x86_intercept_outs ||
 +		    info->intercept == x86_intercept_ins)
 +			exit_info |= SVM_IOIO_STR_MASK;
 +
 +		if (info->rep_prefix)
 +			exit_info |= SVM_IOIO_REP_MASK;
 +
 +		bytes = min(bytes, 4u);
 +
 +		exit_info |= bytes << SVM_IOIO_SIZE_SHIFT;
 +
 +		exit_info |= (u32)info->ad_bytes << (SVM_IOIO_ASIZE_SHIFT - 1);
 +
 +		vmcb->control.exit_info_1 = exit_info;
 +		vmcb->control.exit_info_2 = info->next_rip;
 +
 +		break;
 +	}
 +	default:
 +		break;
 +	}
 +
 +	/* TODO: Advertise NRIPS to guest hypervisor unconditionally */
 +	if (static_cpu_has(X86_FEATURE_NRIPS))
 +		vmcb->control.next_rip  = info->next_rip;
 +	vmcb->control.exit_code = icpt_info.exit_code;
 +	vmexit = nested_svm_exit_handled(svm);
  
 -	ret = (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) ||
 -	      (svm->vcpu.arch.hflags & HF_NMI_MASK);
 +	ret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED
 +					   : X86EMUL_CONTINUE;
  
 +out:
  	return ret;
  }
  
* Unmerged path arch/x86/kvm/svm/nested.c
* Unmerged path arch/x86/kvm/svm/nested.c
* Unmerged path arch/x86/kvm/svm/svm.c
