io_uring: fix -ENOENT issue with linked timer with short timeout

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 76a46e066e2d93bd333599d1c84c605c2c4cc909
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/76a46e06.failed

If you prep a read (for example) that needs to get punted to async
context with a timer, if the timeout is sufficiently short, the timer
request will get completed with -ENOENT as it could not find the read.

The issue is that we prep and start the timer before we start the read.
Hence the timer can trigger before the read is even started, and the end
result is then that the timer completes with -ENOENT, while the read
starts instead of being cancelled by the timer.

Fix this by splitting the linked timer into two parts:

1) Prep and validate the linked timer
2) Start timer

The read is then started between steps 1 and 2, so we know that the
timer will always have a consistent view of the read request state.

	Reported-by: Hrvoje Zeba <zeba.hrvoje@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 76a46e066e2d93bd333599d1c84c605c2c4cc909)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index fca9cdc96d77,ad7f569319c2..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -642,8 -854,8 +642,13 @@@ static void io_req_link_next(struct io_
  	 * safe side.
  	 */
  	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
++<<<<<<< HEAD
 +	if (nxt) {
 +		list_del(&nxt->list);
++=======
+ 	while (nxt) {
+ 		list_del_init(&nxt->list);
++>>>>>>> 76a46e066e2d (io_uring: fix -ENOENT issue with linked timer with short timeout)
  		if (!list_empty(&req->link_list)) {
  			INIT_LIST_HEAD(&nxt->link_list);
  			list_splice(&req->link_list, &nxt->link_list);
@@@ -2162,13 -2647,154 +2167,162 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static int io_grab_files(struct io_kiocb *req)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
++<<<<<<< HEAD
 +	int ret;
 +
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
++=======
+ 	int ret = -EBADF;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irq(&ctx->inflight_lock);
+ 	/*
+ 	 * We use the f_ops->flush() handler to ensure that we can flush
+ 	 * out work accessing these files if the fd is closed. Check if
+ 	 * the fd has changed since we started down this path, and disallow
+ 	 * this operation if it has.
+ 	 */
+ 	if (fcheck(req->submit.ring_fd) == req->submit.ring_file) {
+ 		list_add(&req->inflight_entry, &ctx->inflight_list);
+ 		req->flags |= REQ_F_INFLIGHT;
+ 		req->work.files = current->files;
+ 		ret = 0;
+ 	}
+ 	spin_unlock_irq(&ctx->inflight_lock);
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_kiocb *req = container_of(timer, struct io_kiocb,
+ 						timeout.timer);
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		prev = list_entry(req->list.prev, struct io_kiocb, link_list);
+ 		if (refcount_inc_not_zero(&prev->refs))
+ 			list_del_init(&req->list);
+ 		else
+ 			prev = NULL;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		io_async_find_and_cancel(ctx, req, prev->user_data, NULL);
+ 		io_put_req(prev);
+ 	} else {
+ 		io_cqring_add_event(req, -ETIME);
+ 		io_put_req(req);
+ 	}
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static void io_queue_linked_timeout(struct io_kiocb *req, struct timespec64 *ts,
+ 				    enum hrtimer_mode *mode)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	/*
+ 	 * If the list is now empty, then our linked request finished before
+ 	 * we got a chance to setup the timer
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (!list_empty(&req->list)) {
+ 		req->timeout.timer.function = io_link_timeout_fn;
+ 		hrtimer_start(&req->timeout.timer, timespec64_to_ktime(*ts),
+ 				*mode);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	/* drop submission reference */
+ 	io_put_req(req);
+ }
+ 
+ static int io_validate_link_timeout(const struct io_uring_sqe *sqe,
+ 				    struct timespec64 *ts)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1 || sqe->off)
+ 		return -EINVAL;
+ 	if (sqe->timeout_flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 	if (get_timespec64(ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	return 0;
+ }
+ 
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req,
+ 					       struct timespec64 *ts,
+ 					       enum hrtimer_mode *mode)
+ {
+ 	struct io_kiocb *nxt;
+ 	int ret;
+ 
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
+ 	if (!nxt || nxt->submit.sqe->opcode != IORING_OP_LINK_TIMEOUT)
+ 		return NULL;
+ 
+ 	ret = io_validate_link_timeout(nxt->submit.sqe, ts);
+ 	if (ret) {
+ 		list_del_init(&nxt->list);
+ 		io_cqring_add_event(nxt, ret);
+ 		io_double_put_req(nxt);
+ 		return ERR_PTR(-ECANCELED);
+ 	}
+ 
+ 	if (nxt->submit.sqe->timeout_flags & IORING_TIMEOUT_ABS)
+ 		*mode = HRTIMER_MODE_ABS;
+ 	else
+ 		*mode = HRTIMER_MODE_REL;
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 	hrtimer_init(&nxt->timeout.timer, CLOCK_MONOTONIC, *mode);
+ 	return nxt;
+ }
+ 
+ static int __io_queue_sqe(struct io_kiocb *req)
+ {
+ 	enum hrtimer_mode mode;
+ 	struct io_kiocb *nxt;
+ 	struct timespec64 ts;
+ 	int ret;
+ 
+ 	nxt = io_prep_linked_timeout(req, &ts, &mode);
+ 	if (IS_ERR(nxt)) {
+ 		ret = PTR_ERR(nxt);
+ 		nxt = NULL;
+ 		goto err;
+ 	}
+ 
+ 	ret = __io_submit_sqe(req, NULL, true);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		struct sqe_submit *s = &req->submit;
++>>>>>>> 76a46e066e2d (io_uring: fix -ENOENT issue with linked timer with short timeout)
  		struct io_uring_sqe *sqe_copy;
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
@@@ -2189,16 -2812,29 +2343,35 @@@
  			 * Queued up for async execution, worker will release
  			 * submit reference when the iocb is actually submitted.
  			 */
++<<<<<<< HEAD
++=======
+ 			io_queue_async_work(req);
+ 
+ 			if (nxt)
+ 				io_queue_linked_timeout(nxt, &ts, &mode);
+ 
++>>>>>>> 76a46e066e2d (io_uring: fix -ENOENT issue with linked timer with short timeout)
  			return 0;
  		}
  	}
  
++<<<<<<< HEAD
++=======
+ err:
++>>>>>>> 76a46e066e2d (io_uring: fix -ENOENT issue with linked timer with short timeout)
  	/* drop submission reference */
  	io_put_req(req);
  
+ 	if (nxt) {
+ 		if (!ret)
+ 			io_queue_linked_timeout(nxt, &ts, &mode);
+ 		else
+ 			io_put_req(nxt);
+ 	}
+ 
  	/* and drop final reference, if we failed */
  	if (ret) {
 -		io_cqring_add_event(req, ret);
 +		io_cqring_add_event(ctx, req->user_data, ret);
  		if (req->flags & REQ_F_LINK)
  			req->flags |= REQ_F_FAIL_LINK;
  		io_put_req(req);
* Unmerged path fs/io_uring.c
