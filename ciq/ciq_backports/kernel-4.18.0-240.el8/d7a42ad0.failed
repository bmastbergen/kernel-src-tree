net/mlx5e: Allow partial data mask for tunnel options

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Roi Dayan <roid@mellanox.com>
commit d7a42ad062cc6b20b2c2a8c09dc61df2d4f5751f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/d7a42ad0.failed

We use mapping to save and restore the tunnel options.
Save also the tunnel options mask.

	Signed-off-by: Roi Dayan <roid@mellanox.com>
	Reviewed-by: Paul Blakey <paulb@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit d7a42ad062cc6b20b2c2a8c09dc61df2d4f5751f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index 130ef1fe0993,7d2b05576f44..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -149,6 -158,116 +149,119 @@@ struct mlx5e_tc_flow_parse_attr 
  #define MLX5E_TC_TABLE_NUM_GROUPS 4
  #define MLX5E_TC_TABLE_MAX_GROUP_SIZE BIT(16)
  
++<<<<<<< HEAD
++=======
+ struct tunnel_match_key {
+ 	struct flow_dissector_key_control enc_control;
+ 	struct flow_dissector_key_keyid enc_key_id;
+ 	struct flow_dissector_key_ports enc_tp;
+ 	struct flow_dissector_key_ip enc_ip;
+ 	union {
+ 		struct flow_dissector_key_ipv4_addrs enc_ipv4;
+ 		struct flow_dissector_key_ipv6_addrs enc_ipv6;
+ 	};
+ 
+ 	int filter_ifindex;
+ };
+ 
+ struct tunnel_match_enc_opts {
+ 	struct flow_dissector_key_enc_opts key;
+ 	struct flow_dissector_key_enc_opts mask;
+ };
+ 
+ /* Tunnel_id mapping is TUNNEL_INFO_BITS + ENC_OPTS_BITS.
+  * Upper TUNNEL_INFO_BITS for general tunnel info.
+  * Lower ENC_OPTS_BITS bits for enc_opts.
+  */
+ #define TUNNEL_INFO_BITS 6
+ #define TUNNEL_INFO_BITS_MASK GENMASK(TUNNEL_INFO_BITS - 1, 0)
+ #define ENC_OPTS_BITS 2
+ #define ENC_OPTS_BITS_MASK GENMASK(ENC_OPTS_BITS - 1, 0)
+ #define TUNNEL_ID_BITS (TUNNEL_INFO_BITS + ENC_OPTS_BITS)
+ #define TUNNEL_ID_MASK GENMASK(TUNNEL_ID_BITS - 1, 0)
+ 
+ struct mlx5e_tc_attr_to_reg_mapping mlx5e_tc_attr_to_reg_mappings[] = {
+ 	[CHAIN_TO_REG] = {
+ 		.mfield = MLX5_ACTION_IN_FIELD_METADATA_REG_C_0,
+ 		.moffset = 0,
+ 		.mlen = 2,
+ 	},
+ 	[TUNNEL_TO_REG] = {
+ 		.mfield = MLX5_ACTION_IN_FIELD_METADATA_REG_C_1,
+ 		.moffset = 3,
+ 		.mlen = 1,
+ 		.soffset = MLX5_BYTE_OFF(fte_match_param,
+ 					 misc_parameters_2.metadata_reg_c_1),
+ 	},
+ 	[ZONE_TO_REG] = zone_to_reg_ct,
+ 	[CTSTATE_TO_REG] = ctstate_to_reg_ct,
+ 	[MARK_TO_REG] = mark_to_reg_ct,
+ 	[LABELS_TO_REG] = labels_to_reg_ct,
+ 	[FTEID_TO_REG] = fteid_to_reg_ct,
+ 	[TUPLEID_TO_REG] = tupleid_to_reg_ct,
+ };
+ 
+ static void mlx5e_put_flow_tunnel_id(struct mlx5e_tc_flow *flow);
+ 
+ void
+ mlx5e_tc_match_to_reg_match(struct mlx5_flow_spec *spec,
+ 			    enum mlx5e_tc_attr_to_reg type,
+ 			    u32 data,
+ 			    u32 mask)
+ {
+ 	int soffset = mlx5e_tc_attr_to_reg_mappings[type].soffset;
+ 	int match_len = mlx5e_tc_attr_to_reg_mappings[type].mlen;
+ 	void *headers_c = spec->match_criteria;
+ 	void *headers_v = spec->match_value;
+ 	void *fmask, *fval;
+ 
+ 	fmask = headers_c + soffset;
+ 	fval = headers_v + soffset;
+ 
+ 	mask = cpu_to_be32(mask) >> (32 - (match_len * 8));
+ 	data = cpu_to_be32(data) >> (32 - (match_len * 8));
+ 
+ 	memcpy(fmask, &mask, match_len);
+ 	memcpy(fval, &data, match_len);
+ 
+ 	spec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS_2;
+ }
+ 
+ int
+ mlx5e_tc_match_to_reg_set(struct mlx5_core_dev *mdev,
+ 			  struct mlx5e_tc_mod_hdr_acts *mod_hdr_acts,
+ 			  enum mlx5e_tc_attr_to_reg type,
+ 			  u32 data)
+ {
+ 	int moffset = mlx5e_tc_attr_to_reg_mappings[type].moffset;
+ 	int mfield = mlx5e_tc_attr_to_reg_mappings[type].mfield;
+ 	int mlen = mlx5e_tc_attr_to_reg_mappings[type].mlen;
+ 	char *modact;
+ 	int err;
+ 
+ 	err = alloc_mod_hdr_actions(mdev, MLX5_FLOW_NAMESPACE_FDB,
+ 				    mod_hdr_acts);
+ 	if (err)
+ 		return err;
+ 
+ 	modact = mod_hdr_acts->actions +
+ 		 (mod_hdr_acts->num_actions * MLX5_MH_ACT_SZ);
+ 
+ 	/* Firmware has 5bit length field and 0 means 32bits */
+ 	if (mlen == 4)
+ 		mlen = 0;
+ 
+ 	MLX5_SET(set_action_in, modact, action_type, MLX5_ACTION_TYPE_SET);
+ 	MLX5_SET(set_action_in, modact, field, mfield);
+ 	MLX5_SET(set_action_in, modact, offset, moffset * 8);
+ 	MLX5_SET(set_action_in, modact, length, mlen * 8);
+ 	MLX5_SET(set_action_in, modact, data, data);
+ 	mod_hdr_acts->num_actions++;
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> d7a42ad062cc (net/mlx5e: Allow partial data mask for tunnel options)
  struct mlx5e_hairpin {
  	struct mlx5_hairpin *pair;
  
@@@ -1658,33 -1791,261 +1771,219 @@@ static void mlx5e_tc_del_flow(struct ml
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int flow_has_tc_fwd_action(struct flow_cls_offload *f)
+ {
+ 	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+ 	struct flow_action *flow_action = &rule->action;
+ 	const struct flow_action_entry *act;
+ 	int i;
+ 
+ 	flow_action_for_each(i, act, flow_action) {
+ 		switch (act->id) {
+ 		case FLOW_ACTION_GOTO:
+ 			return true;
+ 		default:
+ 			continue;
+ 		}
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static int
+ enc_opts_is_dont_care_or_full_match(struct mlx5e_priv *priv,
+ 				    struct flow_dissector_key_enc_opts *opts,
+ 				    struct netlink_ext_ack *extack,
+ 				    bool *dont_care)
+ {
+ 	struct geneve_opt *opt;
+ 	int off = 0;
+ 
+ 	*dont_care = true;
+ 
+ 	while (opts->len > off) {
+ 		opt = (struct geneve_opt *)&opts->data[off];
+ 
+ 		if (!(*dont_care) || opt->opt_class || opt->type ||
+ 		    memchr_inv(opt->opt_data, 0, opt->length * 4)) {
+ 			*dont_care = false;
+ 
+ 			if (opt->opt_class != U16_MAX ||
+ 			    opt->type != U8_MAX) {
+ 				NL_SET_ERR_MSG(extack,
+ 					       "Partial match of tunnel options in chain > 0 isn't supported");
+ 				netdev_warn(priv->netdev,
+ 					    "Partial match of tunnel options in chain > 0 isn't supported");
+ 				return -EOPNOTSUPP;
+ 			}
+ 		}
+ 
+ 		off += sizeof(struct geneve_opt) + opt->length * 4;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ #define COPY_DISSECTOR(rule, diss_key, dst)\
+ ({ \
+ 	struct flow_rule *__rule = (rule);\
+ 	typeof(dst) __dst = dst;\
+ \
+ 	memcpy(__dst,\
+ 	       skb_flow_dissector_target(__rule->match.dissector,\
+ 					 diss_key,\
+ 					 __rule->match.key),\
+ 	       sizeof(*__dst));\
+ })
+ 
+ static int mlx5e_get_flow_tunnel_id(struct mlx5e_priv *priv,
+ 				    struct mlx5e_tc_flow *flow,
+ 				    struct flow_cls_offload *f,
+ 				    struct net_device *filter_dev)
+ {
+ 	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+ 	struct netlink_ext_ack *extack = f->common.extack;
+ 	struct mlx5_esw_flow_attr *attr = flow->esw_attr;
+ 	struct mlx5e_tc_mod_hdr_acts *mod_hdr_acts;
+ 	struct flow_match_enc_opts enc_opts_match;
+ 	struct tunnel_match_enc_opts tun_enc_opts;
+ 	struct mlx5_rep_uplink_priv *uplink_priv;
+ 	struct mlx5e_rep_priv *uplink_rpriv;
+ 	struct tunnel_match_key tunnel_key;
+ 	bool enc_opts_is_dont_care = true;
+ 	u32 tun_id, enc_opts_id = 0;
+ 	struct mlx5_eswitch *esw;
+ 	u32 value, mask;
+ 	int err;
+ 
+ 	esw = priv->mdev->priv.eswitch;
+ 	uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
+ 	uplink_priv = &uplink_rpriv->uplink_priv;
+ 
+ 	memset(&tunnel_key, 0, sizeof(tunnel_key));
+ 	COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_CONTROL,
+ 		       &tunnel_key.enc_control);
+ 	if (tunnel_key.enc_control.addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS)
+ 		COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS,
+ 			       &tunnel_key.enc_ipv4);
+ 	else
+ 		COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS,
+ 			       &tunnel_key.enc_ipv6);
+ 	COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_IP, &tunnel_key.enc_ip);
+ 	COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_PORTS,
+ 		       &tunnel_key.enc_tp);
+ 	COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_KEYID,
+ 		       &tunnel_key.enc_key_id);
+ 	tunnel_key.filter_ifindex = filter_dev->ifindex;
+ 
+ 	err = mapping_add(uplink_priv->tunnel_mapping, &tunnel_key, &tun_id);
+ 	if (err)
+ 		return err;
+ 
+ 	flow_rule_match_enc_opts(rule, &enc_opts_match);
+ 	err = enc_opts_is_dont_care_or_full_match(priv,
+ 						  enc_opts_match.mask,
+ 						  extack,
+ 						  &enc_opts_is_dont_care);
+ 	if (err)
+ 		goto err_enc_opts;
+ 
+ 	if (!enc_opts_is_dont_care) {
+ 		memset(&tun_enc_opts, 0, sizeof(tun_enc_opts));
+ 		memcpy(&tun_enc_opts.key, enc_opts_match.key,
+ 		       sizeof(*enc_opts_match.key));
+ 		memcpy(&tun_enc_opts.mask, enc_opts_match.mask,
+ 		       sizeof(*enc_opts_match.mask));
+ 
+ 		err = mapping_add(uplink_priv->tunnel_enc_opts_mapping,
+ 				  &tun_enc_opts, &enc_opts_id);
+ 		if (err)
+ 			goto err_enc_opts;
+ 	}
+ 
+ 	value = tun_id << ENC_OPTS_BITS | enc_opts_id;
+ 	mask = enc_opts_id ? TUNNEL_ID_MASK :
+ 			     (TUNNEL_ID_MASK & ~ENC_OPTS_BITS_MASK);
+ 
+ 	if (attr->chain) {
+ 		mlx5e_tc_match_to_reg_match(&attr->parse_attr->spec,
+ 					    TUNNEL_TO_REG, value, mask);
+ 	} else {
+ 		mod_hdr_acts = &attr->parse_attr->mod_hdr_acts;
+ 		err = mlx5e_tc_match_to_reg_set(priv->mdev,
+ 						mod_hdr_acts,
+ 						TUNNEL_TO_REG, value);
+ 		if (err)
+ 			goto err_set;
+ 
+ 		attr->action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;
+ 	}
+ 
+ 	flow->tunnel_id = value;
+ 	return 0;
+ 
+ err_set:
+ 	if (enc_opts_id)
+ 		mapping_remove(uplink_priv->tunnel_enc_opts_mapping,
+ 			       enc_opts_id);
+ err_enc_opts:
+ 	mapping_remove(uplink_priv->tunnel_mapping, tun_id);
+ 	return err;
+ }
+ 
+ static void mlx5e_put_flow_tunnel_id(struct mlx5e_tc_flow *flow)
+ {
+ 	u32 enc_opts_id = flow->tunnel_id & ENC_OPTS_BITS_MASK;
+ 	u32 tun_id = flow->tunnel_id >> ENC_OPTS_BITS;
+ 	struct mlx5_rep_uplink_priv *uplink_priv;
+ 	struct mlx5e_rep_priv *uplink_rpriv;
+ 	struct mlx5_eswitch *esw;
+ 
+ 	esw = flow->priv->mdev->priv.eswitch;
+ 	uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
+ 	uplink_priv = &uplink_rpriv->uplink_priv;
+ 
+ 	if (tun_id)
+ 		mapping_remove(uplink_priv->tunnel_mapping, tun_id);
+ 	if (enc_opts_id)
+ 		mapping_remove(uplink_priv->tunnel_enc_opts_mapping,
+ 			       enc_opts_id);
+ }
+ 
+ u32 mlx5e_tc_get_flow_tun_id(struct mlx5e_tc_flow *flow)
+ {
+ 	return flow->tunnel_id;
+ }
++>>>>>>> d7a42ad062cc (net/mlx5e: Allow partial data mask for tunnel options)
  
  static int parse_tunnel_attr(struct mlx5e_priv *priv,
 -			     struct mlx5e_tc_flow *flow,
  			     struct mlx5_flow_spec *spec,
  			     struct flow_cls_offload *f,
 -			     struct net_device *filter_dev,
 -			     u8 *match_level,
 -			     bool *match_inner)
 +			     struct net_device *filter_dev, u8 *match_level)
  {
 -	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  	struct netlink_ext_ack *extack = f->common.extack;
 -	bool needs_mapping, sets_mapping;
  	int err;
  
 -	if (!mlx5e_is_eswitch_flow(flow))
 -		return -EOPNOTSUPP;
 -
 -	needs_mapping = !!flow->esw_attr->chain;
 -	sets_mapping = !flow->esw_attr->chain && flow_has_tc_fwd_action(f);
 -	*match_inner = !needs_mapping;
 -
 -	if ((needs_mapping || sets_mapping) &&
 -	    !mlx5_eswitch_reg_c1_loopback_enabled(esw)) {
 -		NL_SET_ERR_MSG(extack,
 -			       "Chains on tunnel devices isn't supported without register loopback support");
 -		netdev_warn(priv->netdev,
 -			    "Chains on tunnel devices isn't supported without register loopback support");
 -		return -EOPNOTSUPP;
 -	}
 -
 -	if (!flow->esw_attr->chain) {
 -		err = mlx5e_tc_tun_parse(filter_dev, priv, spec, f,
 -					 match_level);
 -		if (err) {
 -			NL_SET_ERR_MSG_MOD(extack,
 -					   "Failed to parse tunnel attributes");
 -			netdev_warn(priv->netdev,
 -				    "Failed to parse tunnel attributes");
 -			return err;
 -		}
 -
 -		flow->esw_attr->action |= MLX5_FLOW_CONTEXT_ACTION_DECAP;
 +	err = mlx5e_tc_tun_parse(filter_dev, priv, spec, f, match_level);
 +	if (err) {
 +		NL_SET_ERR_MSG_MOD(extack,
 +				   "failed to parse tunnel attributes");
 +		return err;
  	}
  
 -	if (!needs_mapping && !sets_mapping)
 -		return 0;
 -
 -	return mlx5e_get_flow_tunnel_id(priv, flow, f, filter_dev);
 -}
 -
 -static void *get_match_inner_headers_criteria(struct mlx5_flow_spec *spec)
 -{
 -	return MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 -			    inner_headers);
 -}
 -
 -static void *get_match_inner_headers_value(struct mlx5_flow_spec *spec)
 -{
 -	return MLX5_ADDR_OF(fte_match_param, spec->match_value,
 -			    inner_headers);
 -}
 -
 -static void *get_match_outer_headers_criteria(struct mlx5_flow_spec *spec)
 -{
 -	return MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 -			    outer_headers);
 +	return 0;
  }
  
 -static void *get_match_outer_headers_value(struct mlx5_flow_spec *spec)
 +static void *get_match_headers_criteria(u32 flags,
 +					struct mlx5_flow_spec *spec)
  {
 -	return MLX5_ADDR_OF(fte_match_param, spec->match_value,
 -			    outer_headers);
 +	return (flags & MLX5_FLOW_CONTEXT_ACTION_DECAP) ?
 +		MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 +			     inner_headers) :
 +		MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 +			     outer_headers);
  }
  
  static void *get_match_headers_value(u32 flags,
@@@ -4191,7 -4717,50 +4490,54 @@@ void mlx5e_tc_nic_cleanup(struct mlx5e_
  
  int mlx5e_tc_esw_init(struct rhashtable *tc_ht)
  {
++<<<<<<< HEAD
 +	return rhashtable_init(tc_ht, &tc_ht_params);
++=======
+ 	const size_t sz_enc_opts = sizeof(struct tunnel_match_enc_opts);
+ 	struct mlx5_rep_uplink_priv *uplink_priv;
+ 	struct mlx5e_rep_priv *priv;
+ 	struct mapping_ctx *mapping;
+ 	int err;
+ 
+ 	uplink_priv = container_of(tc_ht, struct mlx5_rep_uplink_priv, tc_ht);
+ 	priv = container_of(uplink_priv, struct mlx5e_rep_priv, uplink_priv);
+ 
+ 	err = mlx5_tc_ct_init(uplink_priv);
+ 	if (err)
+ 		goto err_ct;
+ 
+ 	mapping = mapping_create(sizeof(struct tunnel_match_key),
+ 				 TUNNEL_INFO_BITS_MASK, true);
+ 	if (IS_ERR(mapping)) {
+ 		err = PTR_ERR(mapping);
+ 		goto err_tun_mapping;
+ 	}
+ 	uplink_priv->tunnel_mapping = mapping;
+ 
+ 	mapping = mapping_create(sz_enc_opts, ENC_OPTS_BITS_MASK, true);
+ 	if (IS_ERR(mapping)) {
+ 		err = PTR_ERR(mapping);
+ 		goto err_enc_opts_mapping;
+ 	}
+ 	uplink_priv->tunnel_enc_opts_mapping = mapping;
+ 
+ 	err = rhashtable_init(tc_ht, &tc_ht_params);
+ 	if (err)
+ 		goto err_ht_init;
+ 
+ 	return err;
+ 
+ err_ht_init:
+ 	mapping_destroy(uplink_priv->tunnel_enc_opts_mapping);
+ err_enc_opts_mapping:
+ 	mapping_destroy(uplink_priv->tunnel_mapping);
+ err_tun_mapping:
+ 	mlx5_tc_ct_clean(uplink_priv);
+ err_ct:
+ 	netdev_warn(priv->netdev,
+ 		    "Failed to initialize tc (eswitch), err: %d", err);
+ 	return err;
++>>>>>>> d7a42ad062cc (net/mlx5e: Allow partial data mask for tunnel options)
  }
  
  void mlx5e_tc_esw_cleanup(struct rhashtable *tc_ht)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
