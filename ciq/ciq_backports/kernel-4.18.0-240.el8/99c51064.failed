devmap: Use bpf_map_area_alloc() for allocating hash buckets

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Toke Høiland-Jørgensen <toke@redhat.com>
commit 99c51064fb06146b3d494b745c947e438a10aaa7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/99c51064.failed

Syzkaller discovered that creating a hash of type devmap_hash with a large
number of entries can hit the memory allocator limit for allocating
contiguous memory regions. There's really no reason to use kmalloc_array()
directly in the devmap code, so just switch it to the existing
bpf_map_area_alloc() function that is used elsewhere.

Fixes: 6f9d451ab1a3 ("xdp: Add devmap_hash map type for looking up devices by hashed index")
	Reported-by: Xiumei Mu <xmu@redhat.com>
	Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: John Fastabend <john.fastabend@gmail.com>
Link: https://lore.kernel.org/bpf/20200616142829.114173-1-toke@redhat.com
(cherry picked from commit 99c51064fb06146b3d494b745c947e438a10aaa7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/devmap.c
diff --cc kernel/bpf/devmap.c
index 3c6d97f77537,5fdbc776a760..000000000000
--- a/kernel/bpf/devmap.c
+++ b/kernel/bpf/devmap.c
@@@ -83,14 -85,41 +83,37 @@@ struct bpf_dtab 
  static DEFINE_SPINLOCK(dev_map_lock);
  static LIST_HEAD(dev_map_list);
  
++<<<<<<< HEAD
++=======
+ static struct hlist_head *dev_map_create_hash(unsigned int entries,
+ 					      int numa_node)
+ {
+ 	int i;
+ 	struct hlist_head *hash;
+ 
+ 	hash = bpf_map_area_alloc(entries * sizeof(*hash), numa_node);
+ 	if (hash != NULL)
+ 		for (i = 0; i < entries; i++)
+ 			INIT_HLIST_HEAD(&hash[i]);
+ 
+ 	return hash;
+ }
+ 
+ static inline struct hlist_head *dev_map_index_hash(struct bpf_dtab *dtab,
+ 						    int idx)
+ {
+ 	return &dtab->dev_index_head[idx & (dtab->n_buckets - 1)];
+ }
+ 
++>>>>>>> 99c51064fb06 (devmap: Use bpf_map_area_alloc() for allocating hash buckets)
  static int dev_map_init_map(struct bpf_dtab *dtab, union bpf_attr *attr)
  {
 -	u32 valsize = attr->value_size;
 -	u64 cost = 0;
 -	int err;
 +	int err, cpu;
 +	u64 cost;
  
 -	/* check sanity of attributes. 2 value sizes supported:
 -	 * 4 bytes: ifindex
 -	 * 8 bytes: ifindex + prog fd
 -	 */
 +	/* check sanity of attributes */
  	if (attr->max_entries == 0 || attr->key_size != 4 ||
 -	    (valsize != offsetofend(struct bpf_devmap_val, ifindex) &&
 -	     valsize != offsetofend(struct bpf_devmap_val, bpf_prog.fd)) ||
 -	    attr->map_flags & ~DEV_CREATE_FLAG_MASK)
 +	    attr->value_size != 4 || attr->map_flags & ~DEV_CREATE_FLAG_MASK)
  		return -EINVAL;
  
  	/* Lookup returns a pointer straight to dev->ifindex, so make sure the
@@@ -110,18 -145,20 +133,26 @@@
  	if (err)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	dtab->flush_list = alloc_percpu(struct list_head);
 +	if (!dtab->flush_list)
 +		goto free_charge;
++=======
+ 	if (attr->map_type == BPF_MAP_TYPE_DEVMAP_HASH) {
+ 		dtab->dev_index_head = dev_map_create_hash(dtab->n_buckets,
+ 							   dtab->map.numa_node);
+ 		if (!dtab->dev_index_head)
+ 			goto free_charge;
++>>>>>>> 99c51064fb06 (devmap: Use bpf_map_area_alloc() for allocating hash buckets)
  
 -		spin_lock_init(&dtab->index_lock);
 -	} else {
 -		dtab->netdev_map = bpf_map_area_alloc(dtab->map.max_entries *
 -						      sizeof(struct bpf_dtab_netdev *),
 -						      dtab->map.numa_node);
 -		if (!dtab->netdev_map)
 -			goto free_charge;
 -	}
 +	for_each_possible_cpu(cpu)
 +		INIT_LIST_HEAD(per_cpu_ptr(dtab->flush_list, cpu));
 +
 +	dtab->netdev_map = bpf_map_area_alloc(dtab->map.max_entries *
 +					      sizeof(struct bpf_dtab_netdev *),
 +					      dtab->map.numa_node);
 +	if (!dtab->netdev_map)
 +		goto free_percpu;
  
  	return 0;
  
@@@ -182,32 -217,41 +213,62 @@@ static void dev_map_free(struct bpf_ma
  	/* Make sure prior __dev_map_entry_free() have completed. */
  	rcu_barrier();
  
 -	if (dtab->map.map_type == BPF_MAP_TYPE_DEVMAP_HASH) {
 -		for (i = 0; i < dtab->n_buckets; i++) {
 -			struct bpf_dtab_netdev *dev;
 -			struct hlist_head *head;
 -			struct hlist_node *next;
 +	/* To ensure all pending flush operations have completed wait for flush
 +	 * list to empty on _all_ cpus.
 +	 * Because the above synchronize_rcu() ensures the map is disconnected
 +	 * from the program we can assume no new items will be added.
 +	 */
 +	for_each_online_cpu(cpu) {
 +		struct list_head *flush_list = per_cpu_ptr(dtab->flush_list, cpu);
  
++<<<<<<< HEAD
 +		while (!list_empty(flush_list))
 +			cond_resched();
++=======
+ 			head = dev_map_index_hash(dtab, i);
+ 
+ 			hlist_for_each_entry_safe(dev, next, head, index_hlist) {
+ 				hlist_del_rcu(&dev->index_hlist);
+ 				if (dev->xdp_prog)
+ 					bpf_prog_put(dev->xdp_prog);
+ 				dev_put(dev->dev);
+ 				kfree(dev);
+ 			}
+ 		}
+ 
+ 		bpf_map_area_free(dtab->dev_index_head);
+ 	} else {
+ 		for (i = 0; i < dtab->map.max_entries; i++) {
+ 			struct bpf_dtab_netdev *dev;
+ 
+ 			dev = dtab->netdev_map[i];
+ 			if (!dev)
+ 				continue;
+ 
+ 			if (dev->xdp_prog)
+ 				bpf_prog_put(dev->xdp_prog);
+ 			dev_put(dev->dev);
+ 			kfree(dev);
+ 		}
+ 
+ 		bpf_map_area_free(dtab->netdev_map);
++>>>>>>> 99c51064fb06 (devmap: Use bpf_map_area_alloc() for allocating hash buckets)
  	}
  
 +	for (i = 0; i < dtab->map.max_entries; i++) {
 +		struct bpf_dtab_netdev *dev;
 +
 +		dev = dtab->netdev_map[i];
 +		if (!dev)
 +			continue;
 +
 +		free_percpu(dev->bulkq);
 +		dev_put(dev->dev);
 +		kfree(dev);
 +	}
 +
 +	free_percpu(dtab->flush_list);
 +	bpf_map_area_free(dtab->netdev_map);
  	kfree(dtab);
  }
  
* Unmerged path kernel/bpf/devmap.c
