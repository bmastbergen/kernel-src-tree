iommu/amd: Do not flush Device Table in iommu_map_page()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Joerg Roedel <jroedel@suse.de>
commit 119b2b2c3e256f4bcff7439acc25ac1e9f1aaa4e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/119b2b2c.failed

The flush of the Device Table Entries for the domain has already
happened in increase_address_space(), if necessary. Do no flush them
again in iommu_map_page().

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
	Tested-by: Qian Cai <cai@lca.pw>
Link: https://lore.kernel.org/r/20200504125413.16798-6-joro@8bytes.org
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 119b2b2c3e256f4bcff7439acc25ac1e9f1aaa4e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index 96814d8279f3,1dc3718560d0..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -1480,14 -1444,20 +1480,29 @@@ static bool increase_address_space(stru
  				   unsigned long address,
  				   gfp_t gfp)
  {
 -	struct domain_pgtable pgtable;
  	unsigned long flags;
++<<<<<<< HEAD
 +	bool ret = false;
 +	u64 *pte;
 +
 +	spin_lock_irqsave(&domain->lock, flags);
 +
 +	if (address <= PM_LEVEL_SIZE(domain->mode) ||
 +	    WARN_ON_ONCE(domain->mode == PAGE_MODE_6_LEVEL))
++=======
+ 	bool ret = true;
+ 	u64 *pte, root;
+ 
+ 	spin_lock_irqsave(&domain->lock, flags);
+ 
+ 	amd_iommu_domain_get_pgtable(domain, &pgtable);
+ 
+ 	if (address <= PM_LEVEL_SIZE(pgtable.mode))
+ 		goto out;
+ 
+ 	ret = false;
+ 	if (WARN_ON_ONCE(pgtable.mode == PAGE_MODE_6_LEVEL))
++>>>>>>> 119b2b2c3e25 (iommu/amd: Do not flush Device Table in iommu_map_page())
  		goto out;
  
  	pte = (void *)get_zeroed_page(gfp);
@@@ -1519,11 -1499,23 +1534,29 @@@ static u64 *alloc_pte(struct protection
  
  	BUG_ON(!is_power_of_2(page_size));
  
 -	amd_iommu_domain_get_pgtable(domain, &pgtable);
 +	while (address > PM_LEVEL_SIZE(domain->mode))
 +		*updated = increase_address_space(domain, address, gfp) || *updated;
  
++<<<<<<< HEAD
 +	level   = domain->mode - 1;
 +	pte     = &domain->pt_root[PM_LEVEL_INDEX(level, address)];
++=======
+ 	while (address > PM_LEVEL_SIZE(pgtable.mode)) {
+ 		/*
+ 		 * Return an error if there is no memory to update the
+ 		 * page-table.
+ 		 */
+ 		if (!increase_address_space(domain, address, gfp))
+ 			return NULL;
+ 
+ 		/* Read new values to check if update was successful */
+ 		amd_iommu_domain_get_pgtable(domain, &pgtable);
+ 	}
+ 
+ 
+ 	level   = pgtable.mode - 1;
+ 	pte     = &pgtable.root[PM_LEVEL_INDEX(level, address)];
++>>>>>>> 119b2b2c3e25 (iommu/amd: Do not flush Device Table in iommu_map_page())
  	address = PAGE_SIZE_ALIGN(address, page_size);
  	end_lvl = PAGE_SIZE_LEVEL(page_size);
  
* Unmerged path drivers/iommu/amd_iommu.c
