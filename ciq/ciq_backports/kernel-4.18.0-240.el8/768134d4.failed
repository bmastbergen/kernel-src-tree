io_uring: don't do flush cancel under inflight_lock

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 768134d4f48109b90f4248feecbeeb7d684e410c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/768134d4.failed

We can't safely cancel under the inflight lock. If the work hasn't been
started yet, then io_wq_cancel_work() simply marks the work as cancelled
and invokes the work handler. But if the work completion needs to grab
the inflight lock because it's grabbing user files, then we'll deadlock
trying to finish the work as we already hold that lock.

Instead grab a reference to the request, if it isn't already zero. If
it's zero, then we know it's going through completion anyway, and we
can safely ignore it. If it's not zero, then we can drop the lock and
attempt to cancel from there.

This also fixes a missing finish_wait() at the end of
io_uring_cancel_files().

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 768134d4f48109b90f4248feecbeeb7d684e410c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index fca9cdc96d77,dcb0602c9fd2..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -3564,6 -4249,56 +3564,59 @@@ static int io_uring_release(struct inod
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static void io_uring_cancel_files(struct io_ring_ctx *ctx,
+ 				  struct files_struct *files)
+ {
+ 	struct io_kiocb *req;
+ 	DEFINE_WAIT(wait);
+ 
+ 	while (!list_empty_careful(&ctx->inflight_list)) {
+ 		enum io_wq_cancel ret = IO_WQ_CANCEL_NOTFOUND;
+ 		struct io_kiocb *cancel_req = NULL;
+ 
+ 		spin_lock_irq(&ctx->inflight_lock);
+ 		list_for_each_entry(req, &ctx->inflight_list, inflight_entry) {
+ 			if (req->work.files != files)
+ 				continue;
+ 			/* req is being completed, ignore */
+ 			if (!refcount_inc_not_zero(&req->refs))
+ 				continue;
+ 			cancel_req = req;
+ 			break;
+ 		}
+ 		if (cancel_req)
+ 			prepare_to_wait(&ctx->inflight_wait, &wait,
+ 						TASK_UNINTERRUPTIBLE);
+ 		spin_unlock_irq(&ctx->inflight_lock);
+ 
+ 		if (cancel_req) {
+ 			ret = io_wq_cancel_work(ctx->io_wq, &cancel_req->work);
+ 			io_put_req(cancel_req);
+ 		}
+ 
+ 		/* We need to keep going until we don't find a matching req */
+ 		if (!cancel_req)
+ 			break;
+ 		schedule();
+ 	}
+ 	finish_wait(&ctx->inflight_wait, &wait);
+ }
+ 
+ static int io_uring_flush(struct file *file, void *data)
+ {
+ 	struct io_ring_ctx *ctx = file->private_data;
+ 
+ 	io_uring_cancel_files(ctx, data);
+ 	if (fatal_signal_pending(current) || (current->flags & PF_EXITING)) {
+ 		io_cqring_overflow_flush(ctx, true);
+ 		io_wq_cancel_all(ctx->io_wq);
+ 	}
+ 	return 0;
+ }
+ 
++>>>>>>> 768134d4f481 (io_uring: don't do flush cancel under inflight_lock)
  static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
  {
  	loff_t offset = (loff_t) vma->vm_pgoff << PAGE_SHIFT;
* Unmerged path fs/io_uring.c
