locking/percpu-rwsem: Fold __percpu_up_read()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Davidlohr Bueso <dave@stgolabs.net>
commit ac8dec420970f5cbaf2f6eda39153a60ec5b257b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ac8dec42.failed

Now that __percpu_up_read() is only ever used from percpu_up_read()
merge them, it's a small function.

	Signed-off-by: Davidlohr Bueso <dave@stgolabs.net>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Acked-by: Will Deacon <will@kernel.org>
	Acked-by: Waiman Long <longman@redhat.com>
Link: https://lkml.kernel.org/r/20200131151540.212415454@infradead.org
(cherry picked from commit ac8dec420970f5cbaf2f6eda39153a60ec5b257b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/percpu-rwsem.h
#	kernel/locking/percpu-rwsem.c
diff --cc include/linux/percpu-rwsem.h
index 1b3a8d20c1ca,5e033fe1ff4e..000000000000
--- a/include/linux/percpu-rwsem.h
+++ b/include/linux/percpu-rwsem.h
@@@ -12,22 -12,37 +12,31 @@@
  struct percpu_rw_semaphore {
  	struct rcu_sync		rss;
  	unsigned int __percpu	*read_count;
 -	struct rcuwait		writer;
 -	wait_queue_head_t	waiters;
 -	atomic_t		block;
 -#ifdef CONFIG_DEBUG_LOCK_ALLOC
 -	struct lockdep_map	dep_map;
 -#endif
 +	struct rw_semaphore	rw_sem; /* slowpath */
 +	struct rcuwait          writer; /* blocked writer */
 +	int			readers_block;
  };
  
 -#ifdef CONFIG_DEBUG_LOCK_ALLOC
 -#define __PERCPU_RWSEM_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname },
 -#else
 -#define __PERCPU_RWSEM_DEP_MAP_INIT(lockname)
 -#endif
 -
 -#define __DEFINE_PERCPU_RWSEM(name, is_static)				\
 +#define DEFINE_STATIC_PERCPU_RWSEM(name)				\
  static DEFINE_PER_CPU(unsigned int, __percpu_rwsem_rc_##name);		\
 -is_static struct percpu_rw_semaphore name = {				\
 -	.rss = __RCU_SYNC_INITIALIZER(name.rss),			\
 +static struct percpu_rw_semaphore name = {				\
 +	.rss = __RCU_SYNC_INITIALIZER(name.rss, RCU_SCHED_SYNC),	\
  	.read_count = &__percpu_rwsem_rc_##name,			\
 +	.rw_sem = __RWSEM_INITIALIZER(name.rw_sem),			\
  	.writer = __RCUWAIT_INITIALIZER(name.writer),			\
 -	.waiters = __WAIT_QUEUE_HEAD_INITIALIZER(name.waiters),		\
 -	.block = ATOMIC_INIT(0),					\
 -	__PERCPU_RWSEM_DEP_MAP_INIT(name)				\
  }
  
++<<<<<<< HEAD
 +extern int __percpu_down_read(struct percpu_rw_semaphore *, int);
 +extern void __percpu_up_read(struct percpu_rw_semaphore *);
++=======
+ #define DEFINE_PERCPU_RWSEM(name)		\
+ 	__DEFINE_PERCPU_RWSEM(name, /* not static */)
+ #define DEFINE_STATIC_PERCPU_RWSEM(name)	\
+ 	__DEFINE_PERCPU_RWSEM(name, static)
+ 
+ extern bool __percpu_down_read(struct percpu_rw_semaphore *, bool);
++>>>>>>> ac8dec420970 (locking/percpu-rwsem: Fold __percpu_up_read())
  
  static inline void percpu_down_read(struct percpu_rw_semaphore *sem)
  {
@@@ -85,13 -102,23 +94,25 @@@ static inline void percpu_up_read(struc
  	/*
  	 * Same as in percpu_down_read().
  	 */
- 	if (likely(rcu_sync_is_idle(&sem->rss)))
+ 	if (likely(rcu_sync_is_idle(&sem->rss))) {
  		__this_cpu_dec(*sem->read_count);
- 	else
- 		__percpu_up_read(sem); /* Unconditional memory barrier */
+ 	} else {
+ 		/*
+ 		 * slowpath; reader will only ever wake a single blocked
+ 		 * writer.
+ 		 */
+ 		smp_mb(); /* B matches C */
+ 		/*
+ 		 * In other words, if they see our decrement (presumably to
+ 		 * aggregate zero, as that is the only time it matters) they
+ 		 * will also see our critical section.
+ 		 */
+ 		__this_cpu_dec(*sem->read_count);
+ 		rcuwait_wake_up(&sem->writer);
+ 	}
  	preempt_enable();
 +
 +	rwsem_release(&sem->rw_sem.dep_map, 1, _RET_IP_);
  }
  
  extern void percpu_down_write(struct percpu_rw_semaphore *);
diff --cc kernel/locking/percpu-rwsem.c
index eb5134b6ac21,8048a9a255d5..000000000000
--- a/kernel/locking/percpu-rwsem.c
+++ b/kernel/locking/percpu-rwsem.c
@@@ -62,54 -65,118 +62,57 @@@ int __percpu_down_read(struct percpu_rw
  	smp_mb(); /* A matches D */
  
  	/*
 -	 * If !sem->block the critical section starts here, matched by the
 +	 * If !readers_block the critical section starts here, matched by the
  	 * release in percpu_up_write().
  	 */
 -	if (likely(!atomic_read_acquire(&sem->block)))
 -		return true;
 -
 -	__this_cpu_dec(*sem->read_count);
 -
 -	/* Prod writer to re-evaluate readers_active_check() */
 -	rcuwait_wake_up(&sem->writer);
 -
 -	return false;
 -}
 -
 -static inline bool __percpu_down_write_trylock(struct percpu_rw_semaphore *sem)
 -{
 -	if (atomic_read(&sem->block))
 -		return false;
 -
 -	return atomic_xchg(&sem->block, 1) == 0;
 -}
 -
 -static bool __percpu_rwsem_trylock(struct percpu_rw_semaphore *sem, bool reader)
 -{
 -	if (reader) {
 -		bool ret;
 -
 -		preempt_disable();
 -		ret = __percpu_down_read_trylock(sem);
 -		preempt_enable();
 -
 -		return ret;
 -	}
 -	return __percpu_down_write_trylock(sem);
 -}
 -
 -/*
 - * The return value of wait_queue_entry::func means:
 - *
 - *  <0 - error, wakeup is terminated and the error is returned
 - *   0 - no wakeup, a next waiter is tried
 - *  >0 - woken, if EXCLUSIVE, counted towards @nr_exclusive.
 - *
 - * We use EXCLUSIVE for both readers and writers to preserve FIFO order,
 - * and play games with the return value to allow waking multiple readers.
 - *
 - * Specifically, we wake readers until we've woken a single writer, or until a
 - * trylock fails.
 - */
 -static int percpu_rwsem_wake_function(struct wait_queue_entry *wq_entry,
 -				      unsigned int mode, int wake_flags,
 -				      void *key)
 -{
 -	struct task_struct *p = get_task_struct(wq_entry->private);
 -	bool reader = wq_entry->flags & WQ_FLAG_CUSTOM;
 -	struct percpu_rw_semaphore *sem = key;
 -
 -	/* concurrent against percpu_down_write(), can get stolen */
 -	if (!__percpu_rwsem_trylock(sem, reader))
 +	if (likely(!smp_load_acquire(&sem->readers_block)))
  		return 1;
  
 -	list_del_init(&wq_entry->entry);
 -	smp_store_release(&wq_entry->private, NULL);
 -
 -	wake_up_process(p);
 -	put_task_struct(p);
 +	/*
 +	 * Per the above comment; we still have preemption disabled and
 +	 * will thus decrement on the same CPU as we incremented.
 +	 */
 +	__percpu_up_read(sem);
  
 -	return !reader; /* wake (readers until) 1 writer */
 -}
 +	if (try)
 +		return 0;
  
 -static void percpu_rwsem_wait(struct percpu_rw_semaphore *sem, bool reader)
 -{
 -	DEFINE_WAIT_FUNC(wq_entry, percpu_rwsem_wake_function);
 -	bool wait;
 +	/*
 +	 * We either call schedule() in the wait, or we'll fall through
 +	 * and reschedule on the preempt_enable() in percpu_down_read().
 +	 */
 +	preempt_enable_no_resched();
  
 -	spin_lock_irq(&sem->waiters.lock);
  	/*
 -	 * Serialize against the wakeup in percpu_up_write(), if we fail
 -	 * the trylock, the wakeup must see us on the list.
 +	 * Avoid lockdep for the down/up_read() we already have them.
  	 */
 -	wait = !__percpu_rwsem_trylock(sem, reader);
 -	if (wait) {
 -		wq_entry.flags |= WQ_FLAG_EXCLUSIVE | reader * WQ_FLAG_CUSTOM;
 -		__add_wait_queue_entry_tail(&sem->waiters, &wq_entry);
 -	}
 -	spin_unlock_irq(&sem->waiters.lock);
 +	__down_read(&sem->rw_sem);
 +	this_cpu_inc(*sem->read_count);
 +	__up_read(&sem->rw_sem);
  
 -	while (wait) {
 -		set_current_state(TASK_UNINTERRUPTIBLE);
 -		if (!smp_load_acquire(&wq_entry.private))
 -			break;
 -		schedule();
 -	}
 -	__set_current_state(TASK_RUNNING);
 +	preempt_disable();
 +	return 1;
  }
 +EXPORT_SYMBOL_GPL(__percpu_down_read);
  
 -bool __percpu_down_read(struct percpu_rw_semaphore *sem, bool try)
++<<<<<<< HEAD
 +void __percpu_up_read(struct percpu_rw_semaphore *sem)
  {
 -	if (__percpu_down_read_trylock(sem))
 -		return true;
 -
 -	if (try)
 -		return false;
 -
 -	preempt_enable();
 -	percpu_rwsem_wait(sem, /* .reader = */ true);
 -	preempt_disable();
 +	smp_mb(); /* B matches C */
 +	/*
 +	 * In other words, if they see our decrement (presumably to aggregate
 +	 * zero, as that is the only time it matters) they will also see our
 +	 * critical section.
 +	 */
 +	__this_cpu_dec(*sem->read_count);
  
 -	return true;
 +	/* Prod writer to recheck readers_active */
 +	rcuwait_wake_up(&sem->writer);
  }
 -EXPORT_SYMBOL_GPL(__percpu_down_read);
 +EXPORT_SYMBOL_GPL(__percpu_up_read);
  
++=======
++>>>>>>> ac8dec420970 (locking/percpu-rwsem: Fold __percpu_up_read())
  #define per_cpu_sum(var)						\
  ({									\
  	typeof(var) __sum = 0;						\
* Unmerged path include/linux/percpu-rwsem.h
diff --git a/kernel/exit.c b/kernel/exit.c
index 3ff1a3721602..148f0892e9a6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -256,6 +256,7 @@ void rcuwait_wake_up(struct rcuwait *w)
 		wake_up_process(task);
 	rcu_read_unlock();
 }
+EXPORT_SYMBOL_GPL(rcuwait_wake_up);
 
 /*
  * Determine if a process group is "orphaned", according to the POSIX
* Unmerged path kernel/locking/percpu-rwsem.c
