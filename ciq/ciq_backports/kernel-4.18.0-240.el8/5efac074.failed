KVM: x86: introduce kvm_mmu_invalidate_gva

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 5efac0741ce238e0844d3f7af00198f81e84926a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5efac074.failed

Wrap the combination of mmu->invlpg and kvm_x86_ops->tlb_flush_gva
into a new function.  This function also lets us specify the host PGD to
invalidate and also the MMU, both of which will be useful in fixing and
simplifying kvm_inject_emulated_page_fault.

A nested guest's MMU however has g_context->invlpg == NULL.  Instead of
setting it to nonpaging_invlpg, make kvm_mmu_invalidate_gva the only
entry point to mmu->invlpg and make a NULL invlpg pointer equivalent
to nonpaging_invlpg, saving a retpoline.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 5efac0741ce238e0844d3f7af00198f81e84926a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 7b990f5b5c82,84eeaa4ea149..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -4985,11 -4924,10 +4981,11 @@@ static void init_kvm_tdp_mmu(struct kvm
  	context->mmu_role.as_u64 = new_role.as_u64;
  	context->page_fault = kvm_tdp_page_fault;
  	context->sync_page = nonpaging_sync_page;
- 	context->invlpg = nonpaging_invlpg;
+ 	context->invlpg = NULL;
  	context->update_pte = nonpaging_update_pte;
 -	context->shadow_root_level = kvm_x86_ops.get_tdp_level(vcpu);
 +	context->shadow_root_level = kvm_x86_ops->get_tdp_level(vcpu);
  	context->direct_map = true;
 +	context->set_cr3 = kvm_x86_ops->set_tdp_cr3;
  	context->get_guest_pgd = get_cr3;
  	context->get_pdptr = kvm_pdptr_read;
  	context->inject_page_fault = kvm_inject_page_fault;
@@@ -5556,33 -5499,49 +5558,53 @@@ emulate
  }
  EXPORT_SYMBOL_GPL(kvm_mmu_page_fault);
  
- void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)
+ void kvm_mmu_invalidate_gva(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
+ 			    gva_t gva, hpa_t root_hpa)
  {
- 	struct kvm_mmu *mmu = vcpu->arch.mmu;
  	int i;
  
- 	/* INVLPG on a * non-canonical address is a NOP according to the SDM.  */
- 	if (is_noncanonical_address(gva, vcpu))
+ 	/* It's actually a GPA for vcpu->arch.guest_mmu.  */
+ 	if (mmu != &vcpu->arch.guest_mmu) {
+ 		/* INVLPG on a non-canonical address is a NOP according to the SDM.  */
+ 		if (is_noncanonical_address(gva, vcpu))
+ 			return;
+ 
+ 		kvm_x86_ops.tlb_flush_gva(vcpu, gva);
+ 	}
+ 
+ 	if (!mmu->invlpg)
  		return;
  
- 	mmu->invlpg(vcpu, gva, mmu->root_hpa);
+ 	if (root_hpa == INVALID_PAGE) {
+ 		mmu->invlpg(vcpu, gva, mmu->root_hpa);
  
- 	/*
- 	 * INVLPG is required to invalidate any global mappings for the VA,
- 	 * irrespective of PCID. Since it would take us roughly similar amount
- 	 * of work to determine whether any of the prev_root mappings of the VA
- 	 * is marked global, or to just sync it blindly, so we might as well
- 	 * just always sync it.
- 	 *
- 	 * Mappings not reachable via the current cr3 or the prev_roots will be
- 	 * synced when switching to that cr3, so nothing needs to be done here
- 	 * for them.
- 	 */
- 	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
- 		if (VALID_PAGE(mmu->prev_roots[i].hpa))
- 			mmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);
+ 		/*
+ 		 * INVLPG is required to invalidate any global mappings for the VA,
+ 		 * irrespective of PCID. Since it would take us roughly similar amount
+ 		 * of work to determine whether any of the prev_root mappings of the VA
+ 		 * is marked global, or to just sync it blindly, so we might as well
+ 		 * just always sync it.
+ 		 *
+ 		 * Mappings not reachable via the current cr3 or the prev_roots will be
+ 		 * synced when switching to that cr3, so nothing needs to be done here
+ 		 * for them.
+ 		 */
+ 		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 			if (VALID_PAGE(mmu->prev_roots[i].hpa))
+ 				mmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);
+ 	} else {
+ 		mmu->invlpg(vcpu, gva, root_hpa);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_invalidate_gva);
  
++<<<<<<< HEAD
 +	kvm_x86_ops->tlb_flush_gva(vcpu, gva);
++=======
+ void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)
+ {
+ 	kvm_mmu_invalidate_gva(vcpu, vcpu->arch.mmu, gva, INVALID_PAGE);
++>>>>>>> 5efac0741ce2 (KVM: x86: introduce kvm_mmu_invalidate_gva)
  	++vcpu->stat.invlpg;
  }
  EXPORT_SYMBOL_GPL(kvm_mmu_invlpg);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 30b85c04e379..71a94eee7383 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1506,6 +1506,8 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len);
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
+void kvm_mmu_invalidate_gva(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
+			    gva_t gva, hpa_t root_hpa);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush);
 
* Unmerged path arch/x86/kvm/mmu/mmu.c
