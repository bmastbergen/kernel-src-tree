KVM: x86/mmu: Walk host page tables to find THP mappings

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit db5432165e9b51d2a36572be38d078e79f8df0d8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/db543216.failed

Explicitly walk the host page tables to identify THP mappings instead
of relying solely on the metadata in struct page.  This sets the stage
for using a common method of identifying huge mappings regardless of the
underlying implementation (HugeTLB vs THB vs DAX), and hopefully avoids
the pitfalls of relying on metadata to identify THP mappings, e.g. see
commit 169226f7e0d2 ("mm: thp: handle page cache THP correctly in
PageTransCompoundMap") and the need for KVM to explicitly check for a
THP compound page.  KVM will also naturally work with 1gb THP pages, if
they are ever supported.

Walking the tables for THP mappings is likely marginally slower than
querying metadata, but a future patch will reuse the walk to identify
HugeTLB mappings, at which point eliminating the existing VMA lookup for
HugeTLB will make this a net positive.

	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Barret Rhoden <brho@google.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit db5432165e9b51d2a36572be38d078e79f8df0d8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 8246e5e487a1,3acaadb7acb8..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -3332,33 -3329,63 +3332,83 @@@ static void direct_pte_prefetch(struct 
  	__direct_pte_prefetch(vcpu, sp, sptep);
  }
  
++<<<<<<< HEAD
 +static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,
 +					gfn_t gfn, kvm_pfn_t *pfnp,
++=======
+ static int host_pfn_mapping_level(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 				  kvm_pfn_t pfn)
+ {
+ 	struct kvm_memory_slot *slot;
+ 	unsigned long hva;
+ 	pte_t *pte;
+ 	int level;
+ 
+ 	BUILD_BUG_ON(PT_PAGE_TABLE_LEVEL != (int)PG_LEVEL_4K ||
+ 		     PT_DIRECTORY_LEVEL != (int)PG_LEVEL_2M ||
+ 		     PT_PDPE_LEVEL != (int)PG_LEVEL_1G);
+ 
+ 	if (!PageCompound(pfn_to_page(pfn)))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, true);
+ 	if (!slot)
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	hva = __gfn_to_hva_memslot(slot, gfn);
+ 
+ 	pte = lookup_address_in_mm(vcpu->kvm->mm, hva, &level);
+ 	if (unlikely(!pte))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	return level;
+ }
+ 
+ static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 					int max_level, kvm_pfn_t *pfnp,
++>>>>>>> db5432165e9b (KVM: x86/mmu: Walk host page tables to find THP mappings)
  					int *levelp)
  {
  	kvm_pfn_t pfn = *pfnp;
  	int level = *levelp;
++<<<<<<< HEAD
++=======
+ 	kvm_pfn_t mask;
+ 
+ 	if (max_level == PT_PAGE_TABLE_LEVEL || level > PT_PAGE_TABLE_LEVEL)
+ 		return;
+ 
+ 	if (is_error_noslot_pfn(pfn) || kvm_is_reserved_pfn(pfn) ||
+ 	    kvm_is_zone_device_pfn(pfn))
+ 		return;
+ 
+ 	level = host_pfn_mapping_level(vcpu, gfn, pfn);
+ 	if (level == PT_PAGE_TABLE_LEVEL)
+ 		return;
+ 
+ 	level = min(level, max_level);
++>>>>>>> db5432165e9b (KVM: x86/mmu: Walk host page tables to find THP mappings)
  
  	/*
 -	 * mmu_notifier_retry() was successful and mmu_lock is held, so
 -	 * the pmd can't be split from under us.
 +	 * Check if it's a transparent hugepage. If this would be an
 +	 * hugetlbfs page, level wouldn't be set to
 +	 * PT_PAGE_TABLE_LEVEL and there would be no adjustment done
 +	 * here.
  	 */
 -	*levelp = level;
 -	mask = KVM_PAGES_PER_HPAGE(level) - 1;
 -	VM_BUG_ON((gfn & mask) != (pfn & mask));
 -	*pfnp = pfn & ~mask;
 +	if (!is_error_noslot_pfn(pfn) && !kvm_is_reserved_pfn(pfn) &&
 +	    !kvm_is_zone_device_pfn(pfn) && level == PT_PAGE_TABLE_LEVEL &&
 +	    PageTransCompoundMap(pfn_to_page(pfn))) {
 +		unsigned long mask;
 +
 +		/*
 +		 * mmu_notifier_retry() was successful and mmu_lock is held, so
 +		 * the pmd can't be split from under us.
 +		 */
 +		*levelp = level = PT_DIRECTORY_LEVEL;
 +		mask = KVM_PAGES_PER_HPAGE(level) - 1;
 +		VM_BUG_ON((gfn & mask) != (pfn & mask));
 +		*pfnp = pfn & ~mask;
 +	}
  }
  
  static void disallowed_hugepage_adjust(struct kvm_shadow_walk_iterator it,
* Unmerged path arch/x86/kvm/mmu/mmu.c
