io_uring: add splice(2) support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 7d67af2c013402537385dae343a2d0f6a4cb3bfd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/7d67af2c.failed

Add support for splice(2).

- output file is specified as sqe->fd, so it's handled by generic code
- hash_reg_file handled by generic code as well
- len is 32bit, but should be fine
- the fd_in is registered file, when SPLICE_F_FD_IN_FIXED is set, which
is a splice flag (i.e. sqe->splice_flags).

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 7d67af2c013402537385dae343a2d0f6a4cb3bfd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index 7842c6de7135,1ef20a2af10b..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -71,6 -71,15 +71,18 @@@
  #include <linux/sizes.h>
  #include <linux/hugetlb.h>
  #include <linux/highmem.h>
++<<<<<<< HEAD
++=======
+ #include <linux/namei.h>
+ #include <linux/fsnotify.h>
+ #include <linux/fadvise.h>
+ #include <linux/eventpoll.h>
+ #include <linux/fs_struct.h>
+ #include <linux/splice.h>
+ 
+ #define CREATE_TRACE_POINTS
+ #include <trace/events/io_uring.h>
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  
  #include <uapi/linux/io_uring.h>
  
@@@ -308,6 -325,212 +320,215 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_close {
+ 	struct file			*file;
+ 	struct file			*put_file;
+ 	int				fd;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ 	int				mode;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	unsigned			count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	union {
+ 		struct user_msghdr __user *msg;
+ 		void __user		*buf;
+ 	};
+ 	int				msg_flags;
+ 	size_t				len;
+ };
+ 
+ struct io_open {
+ 	struct file			*file;
+ 	int				dfd;
+ 	union {
+ 		unsigned		mask;
+ 	};
+ 	struct filename			*filename;
+ 	struct statx __user		*buffer;
+ 	struct open_how			how;
+ };
+ 
+ struct io_files_update {
+ 	struct file			*file;
+ 	u64				arg;
+ 	u32				nr_args;
+ 	u32				offset;
+ };
+ 
+ struct io_fadvise {
+ 	struct file			*file;
+ 	u64				offset;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_madvise {
+ 	struct file			*file;
+ 	u64				addr;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_epoll {
+ 	struct file			*file;
+ 	int				epfd;
+ 	int				op;
+ 	int				fd;
+ 	struct epoll_event		event;
+ };
+ 
+ struct io_splice {
+ 	struct file			*file_out;
+ 	struct file			*file_in;
+ 	loff_t				off_out;
+ 	loff_t				off_in;
+ 	u64				len;
+ 	unsigned int			flags;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ 	struct sockaddr_storage		addr;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
+ };
+ 
+ enum {
+ 	REQ_F_FIXED_FILE_BIT	= IOSQE_FIXED_FILE_BIT,
+ 	REQ_F_IO_DRAIN_BIT	= IOSQE_IO_DRAIN_BIT,
+ 	REQ_F_LINK_BIT		= IOSQE_IO_LINK_BIT,
+ 	REQ_F_HARDLINK_BIT	= IOSQE_IO_HARDLINK_BIT,
+ 	REQ_F_FORCE_ASYNC_BIT	= IOSQE_ASYNC_BIT,
+ 
+ 	REQ_F_LINK_NEXT_BIT,
+ 	REQ_F_FAIL_LINK_BIT,
+ 	REQ_F_INFLIGHT_BIT,
+ 	REQ_F_CUR_POS_BIT,
+ 	REQ_F_NOWAIT_BIT,
+ 	REQ_F_IOPOLL_COMPLETED_BIT,
+ 	REQ_F_LINK_TIMEOUT_BIT,
+ 	REQ_F_TIMEOUT_BIT,
+ 	REQ_F_ISREG_BIT,
+ 	REQ_F_MUST_PUNT_BIT,
+ 	REQ_F_TIMEOUT_NOSEQ_BIT,
+ 	REQ_F_COMP_LOCKED_BIT,
+ 	REQ_F_NEED_CLEANUP_BIT,
+ 	REQ_F_OVERFLOW_BIT,
+ };
+ 
+ enum {
+ 	/* ctx owns file */
+ 	REQ_F_FIXED_FILE	= BIT(REQ_F_FIXED_FILE_BIT),
+ 	/* drain existing IO first */
+ 	REQ_F_IO_DRAIN		= BIT(REQ_F_IO_DRAIN_BIT),
+ 	/* linked sqes */
+ 	REQ_F_LINK		= BIT(REQ_F_LINK_BIT),
+ 	/* doesn't sever on completion < 0 */
+ 	REQ_F_HARDLINK		= BIT(REQ_F_HARDLINK_BIT),
+ 	/* IOSQE_ASYNC */
+ 	REQ_F_FORCE_ASYNC	= BIT(REQ_F_FORCE_ASYNC_BIT),
+ 
+ 	/* already grabbed next link */
+ 	REQ_F_LINK_NEXT		= BIT(REQ_F_LINK_NEXT_BIT),
+ 	/* fail rest of links */
+ 	REQ_F_FAIL_LINK		= BIT(REQ_F_FAIL_LINK_BIT),
+ 	/* on inflight list */
+ 	REQ_F_INFLIGHT		= BIT(REQ_F_INFLIGHT_BIT),
+ 	/* read/write uses file position */
+ 	REQ_F_CUR_POS		= BIT(REQ_F_CUR_POS_BIT),
+ 	/* must not punt to workers */
+ 	REQ_F_NOWAIT		= BIT(REQ_F_NOWAIT_BIT),
+ 	/* polled IO has completed */
+ 	REQ_F_IOPOLL_COMPLETED	= BIT(REQ_F_IOPOLL_COMPLETED_BIT),
+ 	/* has linked timeout */
+ 	REQ_F_LINK_TIMEOUT	= BIT(REQ_F_LINK_TIMEOUT_BIT),
+ 	/* timeout request */
+ 	REQ_F_TIMEOUT		= BIT(REQ_F_TIMEOUT_BIT),
+ 	/* regular file */
+ 	REQ_F_ISREG		= BIT(REQ_F_ISREG_BIT),
+ 	/* must be punted even for NONBLOCK */
+ 	REQ_F_MUST_PUNT		= BIT(REQ_F_MUST_PUNT_BIT),
+ 	/* no timeout sequence */
+ 	REQ_F_TIMEOUT_NOSEQ	= BIT(REQ_F_TIMEOUT_NOSEQ_BIT),
+ 	/* completion under lock */
+ 	REQ_F_COMP_LOCKED	= BIT(REQ_F_COMP_LOCKED_BIT),
+ 	/* needs cleanup */
+ 	REQ_F_NEED_CLEANUP	= BIT(REQ_F_NEED_CLEANUP_BIT),
+ 	/* in overflow list */
+ 	REQ_F_OVERFLOW		= BIT(REQ_F_OVERFLOW_BIT),
+ };
+ 
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -317,14 -540,36 +538,30 @@@
  struct io_kiocb {
  	union {
  		struct file		*file;
 -		struct io_rw		rw;
 +		struct kiocb		rw;
  		struct io_poll_iocb	poll;
++<<<<<<< HEAD
++=======
+ 		struct io_accept	accept;
+ 		struct io_sync		sync;
+ 		struct io_cancel	cancel;
+ 		struct io_timeout	timeout;
+ 		struct io_connect	connect;
+ 		struct io_sr_msg	sr_msg;
+ 		struct io_open		open;
+ 		struct io_close		close;
+ 		struct io_files_update	files_update;
+ 		struct io_fadvise	fadvise;
+ 		struct io_madvise	madvise;
+ 		struct io_epoll		epoll;
+ 		struct io_splice	splice;
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  	};
  
 -	struct io_async_ctx		*io;
 -	/*
 -	 * llist_node is only used for poll deferred completions
 -	 */
 -	struct llist_node		llist_node;
 -	bool				needs_fixed_file;
 -	u8				opcode;
 +	struct sqe_submit	submit;
  
  	struct io_ring_ctx	*ctx;
 -	union {
 -		struct list_head	list;
 -		struct hlist_node	hash_node;
 -	};
 +	struct list_head	list;
  	struct list_head	link_list;
  	unsigned int		flags;
  	refcount_t		refs;
@@@ -366,8 -604,180 +603,185 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ 	/* needs ->fs */
+ 	unsigned		needs_fs : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_SPLICE] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	}
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_ring_file_ref_flush(struct fixed_file_data *data);
+ static void io_cleanup_req(struct io_kiocb *req);
+ static int io_file_get(struct io_submit_state *state,
+ 		       struct io_kiocb *req,
+ 		       int fd, struct file **out_file,
+ 		       bool fixed);
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  
  static struct kmem_cache *req_cachep;
  
@@@ -1841,22 -3899,460 +2326,459 @@@ static int io_poll_add(struct io_kiocb 
  	return ipt.error;
  }
  
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 +	struct io_uring_sqe *sqe_copy;
  
++<<<<<<< HEAD
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
++=======
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * We could be racing with timeout deletion. If the list is empty,
+ 	 * then timeout lookup already found it and will be handling it.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		struct io_kiocb *prev;
+ 
+ 		/*
+ 		 * Adjust the reqs sequence before the current one because it
+ 		 * will consume a slot in the cq_ring and the cq_tail
+ 		 * pointer will be increased, otherwise other timeout reqs may
+ 		 * return in advance without waiting for enough wait_nr.
+ 		 */
+ 		prev = req;
+ 		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 			prev->sequence++;
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	io_cqring_fill_event(req, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
+ {
+ 	struct io_kiocb *req;
+ 	int ret = -ENOENT;
+ 
+ 	list_for_each_entry(req, &ctx->timeout_list, list) {
+ 		if (user_data == req->user_data) {
+ 			list_del_init(&req->list);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (ret == -ENOENT)
+ 		return ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
+ 	if (ret == -1)
+ 		return -EALREADY;
+ 
+ 	req_set_fail_links(req);
+ 	io_cqring_fill_event(req, -ECANCELED);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_remove_prep(struct io_kiocb *req,
+ 				  const struct io_uring_sqe *sqe)
+ {
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
+ 		return -EINVAL;
+ 
+ 	req->timeout.addr = READ_ONCE(sqe->addr);
+ 	req->timeout.flags = READ_ONCE(sqe->timeout_flags);
+ 	if (req->timeout.flags)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Remove or update an existing timeout command
+  */
+ static int io_timeout_remove(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	ret = io_timeout_cancel(ctx, req->timeout.addr);
+ 
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	io_cqring_ev_posted(ctx);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			   bool is_timeout_link)
+ {
+ 	struct io_timeout_data *data;
+ 	unsigned flags;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
+ 		return -EINVAL;
+ 	if (sqe->off && is_timeout_link)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 
+ 	req->timeout.count = READ_ONCE(sqe->off);
+ 
+ 	if (!req->io && io_alloc_async_ctx(req))
+ 		return -ENOMEM;
+ 
+ 	data = &req->io->timeout;
+ 	data->req = req;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	if (flags & IORING_TIMEOUT_ABS)
+ 		data->mode = HRTIMER_MODE_ABS;
+ 	else
+ 		data->mode = HRTIMER_MODE_REL;
+ 
+ 	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
+ 	return 0;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_timeout_data *data;
+ 	struct list_head *entry;
+ 	unsigned span = 0;
+ 
+ 	data = &req->io->timeout;
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied. If it isn't set, then this is
+ 	 * a pure timeout request, sequence isn't used.
+ 	 */
+ 	count = req->timeout.count;
+ 	if (!count) {
+ 		req->flags |= REQ_F_TIMEOUT_NOSEQ;
+ 		spin_lock_irq(&ctx->completion_lock);
+ 		entry = ctx->timeout_list.prev;
+ 		goto add;
+ 	}
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	data->seq_offset = count;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 		u32 nxt_offset = nxt->io->timeout.seq_offset;
+ 
+ 		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
+ 			continue;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt_offset + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ add:
+ 	list_add(&req->list, entry);
+ 	data->timer.function = io_timeout_fn;
+ 	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	return 0;
+ }
+ 
+ static bool io_cancel_cb(struct io_wq_work *work, void *data)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	return req->user_data == (unsigned long) data;
+ }
+ 
+ static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
+ {
+ 	enum io_wq_cancel cancel_ret;
+ 	int ret = 0;
+ 
+ 	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
+ 	switch (cancel_ret) {
+ 	case IO_WQ_CANCEL_OK:
+ 		ret = 0;
+ 		break;
+ 	case IO_WQ_CANCEL_RUNNING:
+ 		ret = -EALREADY;
+ 		break;
+ 	case IO_WQ_CANCEL_NOTFOUND:
+ 		ret = -ENOENT;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
+ 				     struct io_kiocb *req, __u64 sqe_addr,
+ 				     struct io_kiocb **nxt, int success_ret)
+ {
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
+ 	if (ret != -ENOENT) {
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		goto done;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	ret = io_timeout_cancel(ctx, sqe_addr);
+ 	if (ret != -ENOENT)
+ 		goto done;
+ 	ret = io_poll_cancel(ctx, sqe_addr);
+ done:
+ 	if (!ret)
+ 		ret = success_ret;
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static int io_async_cancel_prep(struct io_kiocb *req,
+ 				const struct io_uring_sqe *sqe)
+ {
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
+ 	    sqe->cancel_flags)
+ 		return -EINVAL;
+ 
+ 	req->cancel.addr = READ_ONCE(sqe->addr);
+ 	return 0;
+ }
+ 
+ static int io_async_cancel(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	io_async_find_and_cancel(ctx, req, req->cancel.addr, nxt, 0);
+ 	return 0;
+ }
+ 
+ static int io_files_update_prep(struct io_kiocb *req,
+ 				const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->flags || sqe->ioprio || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	req->files_update.offset = READ_ONCE(sqe->off);
+ 	req->files_update.nr_args = READ_ONCE(sqe->len);
+ 	if (!req->files_update.nr_args)
+ 		return -EINVAL;
+ 	req->files_update.arg = READ_ONCE(sqe->addr);
+ 	return 0;
+ }
+ 
+ static int io_files_update(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_uring_files_update up;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	up.offset = req->files_update.offset;
+ 	up.fds = req->files_update.arg;
+ 
+ 	mutex_lock(&ctx->uring_lock);
+ 	ret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);
+ 	mutex_unlock(&ctx->uring_lock);
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_req_defer_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	ssize_t ret = 0;
+ 
+ 	if (io_op_defs[req->opcode].file_table) {
+ 		ret = io_grab_files(req);
+ 		if (unlikely(ret))
+ 			return ret;
+ 	}
+ 
+ 	io_req_work_grab_env(req, &io_op_defs[req->opcode]);
+ 
+ 	switch (req->opcode) {
+ 	case IORING_OP_NOP:
+ 		break;
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 		ret = io_read_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 	case IORING_OP_WRITE:
+ 		ret = io_write_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_POLL_ADD:
+ 		ret = io_poll_add_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_POLL_REMOVE:
+ 		ret = io_poll_remove_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FSYNC:
+ 		ret = io_prep_fsync(req, sqe);
+ 		break;
+ 	case IORING_OP_SYNC_FILE_RANGE:
+ 		ret = io_prep_sfr(req, sqe);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 	case IORING_OP_SEND:
+ 		ret = io_sendmsg_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_RECVMSG:
+ 	case IORING_OP_RECV:
+ 		ret = io_recvmsg_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_CONNECT:
+ 		ret = io_connect_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_TIMEOUT:
+ 		ret = io_timeout_prep(req, sqe, false);
+ 		break;
+ 	case IORING_OP_TIMEOUT_REMOVE:
+ 		ret = io_timeout_remove_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_ASYNC_CANCEL:
+ 		ret = io_async_cancel_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_LINK_TIMEOUT:
+ 		ret = io_timeout_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_ACCEPT:
+ 		ret = io_accept_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FALLOCATE:
+ 		ret = io_fallocate_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_OPENAT:
+ 		ret = io_openat_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_CLOSE:
+ 		ret = io_close_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FILES_UPDATE:
+ 		ret = io_files_update_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_STATX:
+ 		ret = io_statx_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FADVISE:
+ 		ret = io_fadvise_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_MADVISE:
+ 		ret = io_madvise_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_OPENAT2:
+ 		ret = io_openat2_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_EPOLL_CTL:
+ 		ret = io_epoll_ctl_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_SPLICE:
+ 		ret = io_splice_prep(req, sqe);
+ 		break;
+ 	default:
+ 		printk_once(KERN_WARNING "io_uring: unhandled opcode %d\n",
+ 				req->opcode);
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int io_req_defer(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	/* Still need defer if there is pending req in defer list. */
+ 	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  		return 0;
  
 -	if (!req->io && io_alloc_async_ctx(req))
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
  		return -EAGAIN;
  
 -	ret = io_req_defer_prep(req, sqe);
 -	if (ret < 0)
 -		return ret;
 -
  	spin_lock_irq(&ctx->completion_lock);
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list)) {
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list)) {
  		spin_unlock_irq(&ctx->completion_lock);
 +		kfree(sqe_copy);
  		return 0;
  	}
  
@@@ -1869,52 -4362,245 +2791,88 @@@
  	return -EIOCBQUEUED;
  }
  
 -static void io_cleanup_req(struct io_kiocb *req)
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_async_ctx *io = req->io;
 +	int ret, opcode;
  
++<<<<<<< HEAD
 +	req->user_data = READ_ONCE(s->sqe->user_data);
++=======
+ 	switch (req->opcode) {
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 	case IORING_OP_WRITE:
+ 		if (io->rw.iov != io->rw.fast_iov)
+ 			kfree(io->rw.iov);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 	case IORING_OP_RECVMSG:
+ 		if (io->msg.iov != io->msg.fast_iov)
+ 			kfree(io->msg.iov);
+ 		break;
+ 	case IORING_OP_OPENAT:
+ 	case IORING_OP_OPENAT2:
+ 	case IORING_OP_STATX:
+ 		putname(req->open.filename);
+ 		break;
+ 	case IORING_OP_SPLICE:
+ 		io_put_file(req, req->splice.file_in,
+ 			    (req->splice.flags & SPLICE_F_FD_IN_FIXED));
+ 		break;
+ 	}
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -}
 -
 -static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			struct io_kiocb **nxt, bool force_nonblock)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	switch (req->opcode) {
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
  	case IORING_OP_NOP:
 -		ret = io_nop(req);
 +		ret = io_nop(req, req->user_data);
  		break;
  	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -	case IORING_OP_READ:
 -		if (sqe) {
 -			ret = io_read_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_read(req, nxt, force_nonblock);
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
  		break;
  	case IORING_OP_WRITEV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_READ_FIXED:
 +		ret = io_read(req, s, force_nonblock);
 +		break;
  	case IORING_OP_WRITE_FIXED:
 -	case IORING_OP_WRITE:
 -		if (sqe) {
 -			ret = io_write_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_write(req, nxt, force_nonblock);
 +		ret = io_write(req, s, force_nonblock);
  		break;
  	case IORING_OP_FSYNC:
 -		if (sqe) {
 -			ret = io_prep_fsync(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_fsync(req, nxt, force_nonblock);
 +		ret = io_fsync(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_POLL_ADD:
 -		if (sqe) {
 -			ret = io_poll_add_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_poll_add(req, nxt);
 +		ret = io_poll_add(req, s->sqe);
  		break;
  	case IORING_OP_POLL_REMOVE:
 -		if (sqe) {
 -			ret = io_poll_remove_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_poll_remove(req);
 -		break;
 -	case IORING_OP_SYNC_FILE_RANGE:
 -		if (sqe) {
 -			ret = io_prep_sfr(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_sync_file_range(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_SENDMSG:
 -	case IORING_OP_SEND:
 -		if (sqe) {
 -			ret = io_sendmsg_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_SENDMSG)
 -			ret = io_sendmsg(req, nxt, force_nonblock);
 -		else
 -			ret = io_send(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_RECVMSG:
 -	case IORING_OP_RECV:
 -		if (sqe) {
 -			ret = io_recvmsg_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_RECVMSG)
 -			ret = io_recvmsg(req, nxt, force_nonblock);
 -		else
 -			ret = io_recv(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		if (sqe) {
 -			ret = io_timeout_prep(req, sqe, false);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout(req);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		if (sqe) {
 -			ret = io_timeout_remove_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout_remove(req);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		if (sqe) {
 -			ret = io_accept_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_accept(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_CONNECT:
 -		if (sqe) {
 -			ret = io_connect_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_connect(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		if (sqe) {
 -			ret = io_async_cancel_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_async_cancel(req, nxt);
 -		break;
 -	case IORING_OP_FALLOCATE:
 -		if (sqe) {
 -			ret = io_fallocate_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fallocate(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_OPENAT:
 -		if (sqe) {
 -			ret = io_openat_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_CLOSE:
 -		if (sqe) {
 -			ret = io_close_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_close(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_FILES_UPDATE:
 -		if (sqe) {
 -			ret = io_files_update_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_files_update(req, force_nonblock);
 -		break;
 -	case IORING_OP_STATX:
 -		if (sqe) {
 -			ret = io_statx_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_statx(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_FADVISE:
 -		if (sqe) {
 -			ret = io_fadvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fadvise(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_MADVISE:
 -		if (sqe) {
 -			ret = io_madvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_madvise(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_OPENAT2:
 -		if (sqe) {
 -			ret = io_openat2_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat2(req, nxt, force_nonblock);
 +		ret = io_poll_remove(req, s->sqe);
  		break;
 -	case IORING_OP_EPOLL_CTL:
 -		if (sqe) {
 -			ret = io_epoll_ctl_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_epoll_ctl(req, nxt, force_nonblock);
 +	case IORING_OP_SYNC_FILE_RANGE:
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_SENDMSG:
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_RECVMSG:
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
  		break;
+ 	case IORING_OP_SPLICE:
+ 		if (sqe) {
+ 			ret = io_splice_prep(req, sqe);
+ 			if (ret < 0)
+ 				break;
+ 		}
+ 		ret = io_splice(req, nxt, force_nonblock);
+ 		break;
  	default:
  		ret = -EINVAL;
  		break;
@@@ -4088,6 -7321,43 +5046,46 @@@ out_fput
  
  static int __init io_uring_init(void)
  {
++<<<<<<< HEAD
++=======
+ #define __BUILD_BUG_VERIFY_ELEMENT(stype, eoffset, etype, ename) do { \
+ 	BUILD_BUG_ON(offsetof(stype, ename) != eoffset); \
+ 	BUILD_BUG_ON(sizeof(etype) != sizeof_field(stype, ename)); \
+ } while (0)
+ 
+ #define BUILD_BUG_SQE_ELEM(eoffset, etype, ename) \
+ 	__BUILD_BUG_VERIFY_ELEMENT(struct io_uring_sqe, eoffset, etype, ename)
+ 	BUILD_BUG_ON(sizeof(struct io_uring_sqe) != 64);
+ 	BUILD_BUG_SQE_ELEM(0,  __u8,   opcode);
+ 	BUILD_BUG_SQE_ELEM(1,  __u8,   flags);
+ 	BUILD_BUG_SQE_ELEM(2,  __u16,  ioprio);
+ 	BUILD_BUG_SQE_ELEM(4,  __s32,  fd);
+ 	BUILD_BUG_SQE_ELEM(8,  __u64,  off);
+ 	BUILD_BUG_SQE_ELEM(8,  __u64,  addr2);
+ 	BUILD_BUG_SQE_ELEM(16, __u64,  addr);
+ 	BUILD_BUG_SQE_ELEM(16, __u64,  splice_off_in);
+ 	BUILD_BUG_SQE_ELEM(24, __u32,  len);
+ 	BUILD_BUG_SQE_ELEM(28,     __kernel_rwf_t, rw_flags);
+ 	BUILD_BUG_SQE_ELEM(28, /* compat */   int, rw_flags);
+ 	BUILD_BUG_SQE_ELEM(28, /* compat */ __u32, rw_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  fsync_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u16,  poll_events);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  sync_range_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  msg_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  timeout_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  accept_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  cancel_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  open_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  statx_flags);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  fadvise_advice);
+ 	BUILD_BUG_SQE_ELEM(28, __u32,  splice_flags);
+ 	BUILD_BUG_SQE_ELEM(32, __u64,  user_data);
+ 	BUILD_BUG_SQE_ELEM(40, __u16,  buf_index);
+ 	BUILD_BUG_SQE_ELEM(42, __u16,  personality);
+ 	BUILD_BUG_SQE_ELEM(44, __s32,  splice_fd_in);
+ 
+ 	BUILD_BUG_ON(ARRAY_SIZE(io_op_defs) != IORING_OP_LAST);
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  	req_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);
  	return 0;
  };
diff --cc include/uapi/linux/io_uring.h
index dd4a49ec83b7,08891cc1c1e7..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -19,8 -19,14 +19,19 @@@ struct io_uring_sqe 
  	__u8	flags;		/* IOSQE_ flags */
  	__u16	ioprio;		/* ioprio for the request */
  	__s32	fd;		/* file descriptor to do IO on */
++<<<<<<< HEAD
 +	__u64	off;		/* offset into file */
 +	__u64	addr;		/* pointer to buffer or iovecs */
++=======
+ 	union {
+ 		__u64	off;	/* offset into file */
+ 		__u64	addr2;
+ 	};
+ 	union {
+ 		__u64	addr;	/* pointer to buffer or iovecs */
+ 		__u64	splice_off_in;
+ 	};
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  	__u32	len;		/* buffer size or number of iovecs */
  	union {
  		__kernel_rwf_t	rw_flags;
@@@ -28,10 -34,23 +39,30 @@@
  		__u16		poll_events;
  		__u32		sync_range_flags;
  		__u32		msg_flags;
++<<<<<<< HEAD
 +	};
 +	__u64	user_data;	/* data to be passed back at completion time */
 +	union {
 +		__u16	buf_index;	/* index into fixed buffers, if used */
++=======
+ 		__u32		timeout_flags;
+ 		__u32		accept_flags;
+ 		__u32		cancel_flags;
+ 		__u32		open_flags;
+ 		__u32		statx_flags;
+ 		__u32		fadvise_advice;
+ 		__u32		splice_flags;
+ 	};
+ 	__u64	user_data;	/* data to be passed back at completion time */
+ 	union {
+ 		struct {
+ 			/* index into fixed buffers, if used */
+ 			__u16	buf_index;
+ 			/* personality to use, if used */
+ 			__u16	personality;
+ 			__s32	splice_fd_in;
+ 		};
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  		__u64	__pad2[3];
  	};
  };
@@@ -51,18 -85,44 +82,57 @@@
  #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
  #define IORING_SETUP_CQSIZE	(1U << 3)	/* app defines CQ size */
  #define IORING_SETUP_CLAMP	(1U << 4)	/* clamp SQ/CQ ring sizes */
 -#define IORING_SETUP_ATTACH_WQ	(1U << 5)	/* attach to existing wq */
  
++<<<<<<< HEAD
 +#define IORING_OP_NOP		0
 +#define IORING_OP_READV		1
 +#define IORING_OP_WRITEV	2
 +#define IORING_OP_FSYNC		3
 +#define IORING_OP_READ_FIXED	4
 +#define IORING_OP_WRITE_FIXED	5
 +#define IORING_OP_POLL_ADD	6
 +#define IORING_OP_POLL_REMOVE	7
 +#define IORING_OP_SYNC_FILE_RANGE	8
 +#define IORING_OP_SENDMSG	9
 +#define IORING_OP_RECVMSG	10
++=======
+ enum {
+ 	IORING_OP_NOP,
+ 	IORING_OP_READV,
+ 	IORING_OP_WRITEV,
+ 	IORING_OP_FSYNC,
+ 	IORING_OP_READ_FIXED,
+ 	IORING_OP_WRITE_FIXED,
+ 	IORING_OP_POLL_ADD,
+ 	IORING_OP_POLL_REMOVE,
+ 	IORING_OP_SYNC_FILE_RANGE,
+ 	IORING_OP_SENDMSG,
+ 	IORING_OP_RECVMSG,
+ 	IORING_OP_TIMEOUT,
+ 	IORING_OP_TIMEOUT_REMOVE,
+ 	IORING_OP_ACCEPT,
+ 	IORING_OP_ASYNC_CANCEL,
+ 	IORING_OP_LINK_TIMEOUT,
+ 	IORING_OP_CONNECT,
+ 	IORING_OP_FALLOCATE,
+ 	IORING_OP_OPENAT,
+ 	IORING_OP_CLOSE,
+ 	IORING_OP_FILES_UPDATE,
+ 	IORING_OP_STATX,
+ 	IORING_OP_READ,
+ 	IORING_OP_WRITE,
+ 	IORING_OP_FADVISE,
+ 	IORING_OP_MADVISE,
+ 	IORING_OP_SEND,
+ 	IORING_OP_RECV,
+ 	IORING_OP_OPENAT2,
+ 	IORING_OP_EPOLL_CTL,
+ 	IORING_OP_SPLICE,
+ 
+ 	/* this goes last, obviously */
+ 	IORING_OP_LAST,
+ };
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
  
  /*
   * sqe->fsync_flags
@@@ -70,6 -130,17 +140,20 @@@
  #define IORING_FSYNC_DATASYNC	(1U << 0)
  
  /*
++<<<<<<< HEAD
++=======
+  * sqe->timeout_flags
+  */
+ #define IORING_TIMEOUT_ABS	(1U << 0)
+ 
+ /*
+  * sqe->splice_flags
+  * extends splice(2) flags
+  */
+ #define SPLICE_F_FD_IN_FIXED	(1U << 31) /* the last bit of __u32 */
+ 
+ /*
++>>>>>>> 7d67af2c0134 (io_uring: add splice(2) support)
   * IO completion data structure (Completion Queue Entry)
   */
  struct io_uring_cqe {
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
