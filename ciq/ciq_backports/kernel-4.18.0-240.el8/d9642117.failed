powerpc/mm: define empty update_mmu_cache() as static inline

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [powerpc] mm: define empty update_mmu_cache() as static inline (Greg Kurz) [1748772]
Rebuild_FUZZ: 92.86%
commit-author Christophe Leroy <christophe.leroy@c-s.fr>
commit d9642117914c9d3f800b3bacc19d7e388b04edb4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/d9642117.failed

Only BOOK3S and FSL_BOOK3E have a usefull update_mmu_cache().

For the others, just define it static inline.

In the meantime, simplify the FSL_BOOK3E related ifdef as
book3e_hugetlb_preload() only exists when CONFIG_PPC_FSL_BOOK3E
is selected.

	Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/668aba4db6b9af6d8a151174e11a4289f1a6bbcd.1565933217.git.christophe.leroy@c-s.fr

(cherry picked from commit d9642117914c9d3f800b3bacc19d7e388b04edb4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/pgtable.h
#	arch/powerpc/mm/mem.c
diff --cc arch/powerpc/include/asm/pgtable.h
index 3bbbd9c542d6,c70916a7865a..000000000000
--- a/arch/powerpc/include/asm/pgtable.h
+++ b/arch/powerpc/include/asm/pgtable.h
@@@ -48,21 -77,6 +48,24 @@@ extern void paging_init(void)
  
  #include <asm-generic/pgtable.h>
  
++<<<<<<< HEAD
 +
 +/*
 + * This gets called at the end of handling a page fault, when
 + * the kernel has put a new PTE into the page table for the process.
 + * We use it to ensure coherency between the i-cache and d-cache
 + * for the page which has just been mapped in.
 + * On machines which use an MMU hash table, we use this to put a
 + * corresponding HPTE into the hash table ahead of time, instead of
 + * waiting for the inevitable extra hash-table miss exception.
 + */
 +extern void update_mmu_cache(struct vm_area_struct *, unsigned long, pte_t *);
 +
 +extern int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,
 +		       unsigned long end, int write,
 +		       struct page **pages, int *nr);
++=======
++>>>>>>> d9642117914c (powerpc/mm: define empty update_mmu_cache() as static inline)
  #ifndef CONFIG_TRANSPARENT_HUGEPAGE
  #define pmd_large(pmd)		0
  #endif
diff --cc arch/powerpc/mm/mem.c
index 54211ad167a4,ee8225cd2cd2..000000000000
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@@ -476,7 -419,6 +477,10 @@@ EXPORT_SYMBOL(flush_icache_user_range)
  void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
  		      pte_t *ptep)
  {
++<<<<<<< HEAD
 +#ifdef CONFIG_PPC_STD_MMU
++=======
++>>>>>>> d9642117914c (powerpc/mm: define empty update_mmu_cache() as static inline)
  	/*
  	 * We don't need to worry about _PAGE_PRESENT here because we are
  	 * called with either mm->page_table_lock held or ptl lock held
@@@ -514,13 -456,16 +518,22 @@@
  	}
  
  	hash_preload(vma->vm_mm, address, is_exec, trap);
++<<<<<<< HEAD
 +#endif /* CONFIG_PPC_STD_MMU */
 +#if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
 +	&& defined(CONFIG_HUGETLB_PAGE)
++=======
+ }
+ #endif /* CONFIG_PPC_BOOK3S */
+ #if defined(CONFIG_PPC_FSL_BOOK3E) && defined(CONFIG_HUGETLB_PAGE)
+ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
+ 		      pte_t *ptep)
+ {
++>>>>>>> d9642117914c (powerpc/mm: define empty update_mmu_cache() as static inline)
  	if (is_vm_hugetlb_page(vma))
  		book3e_hugetlb_preload(vma, address, *ptep);
- #endif
  }
+ #endif
  
  /*
   * System memory should not be in /proc/iomem but various tools expect it
diff --git a/arch/powerpc/include/asm/book3s/pgtable.h b/arch/powerpc/include/asm/book3s/pgtable.h
index 6436b65ac7bc..0e1263455d73 100644
--- a/arch/powerpc/include/asm/book3s/pgtable.h
+++ b/arch/powerpc/include/asm/book3s/pgtable.h
@@ -26,5 +26,16 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 				     unsigned long size, pgprot_t vma_prot);
 #define __HAVE_PHYS_MEM_ACCESS_PROT
 
+/*
+ * This gets called at the end of handling a page fault, when
+ * the kernel has put a new PTE into the page table for the process.
+ * We use it to ensure coherency between the i-cache and d-cache
+ * for the page which has just been mapped in.
+ * On machines which use an MMU hash table, we use this to put a
+ * corresponding HPTE into the hash table ahead of time, instead of
+ * waiting for the inevitable extra hash-table miss exception.
+ */
+void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep);
+
 #endif /* __ASSEMBLY__ */
 #endif
diff --git a/arch/powerpc/include/asm/nohash/pgtable.h b/arch/powerpc/include/asm/nohash/pgtable.h
index 95771329c591..c3d4e255903f 100644
--- a/arch/powerpc/include/asm/nohash/pgtable.h
+++ b/arch/powerpc/include/asm/nohash/pgtable.h
@@ -244,5 +244,18 @@ static inline int pgd_huge(pgd_t pgd)
 #define is_hugepd(hpd)		(hugepd_ok(hpd))
 #endif
 
+/*
+ * This gets called at the end of handling a page fault, when
+ * the kernel has put a new PTE into the page table for the process.
+ * We use it to ensure coherency between the i-cache and d-cache
+ * for the page which has just been mapped in.
+ */
+#if defined(CONFIG_PPC_FSL_BOOK3E) && defined(CONFIG_HUGETLB_PAGE)
+void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep);
+#else
+static inline
+void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep) {}
+#endif
+
 #endif /* __ASSEMBLY__ */
 #endif
* Unmerged path arch/powerpc/include/asm/pgtable.h
* Unmerged path arch/powerpc/mm/mem.c
