IB/core: Fix ODP with IB_ACCESS_HUGETLB handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Yishai Hadas <yishaih@mellanox.com>
commit 9ff1b6466a291a33389c4a9c7f3f9b64d62df40a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/9ff1b646.failed

As VMAs for a given range might not be available as part of the
registration phase in ODP.

ib_init_umem_odp() considered the expected page shift value that was
previously set and initializes its internals accordingly.

If memory isn't backed by physical contiguous pages aligned to a hugepage
boundary an error will be set as part of the page fault flow and come back
to the user as some failed RDMA operation.

Fixes: 0008b84ea9af ("IB/umem: Add support to huge ODP")
Link: https://lore.kernel.org/r/20191222124649.52300-4-leon@kernel.org
	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 9ff1b6466a291a33389c4a9c7f3f9b64d62df40a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem_odp.c
diff --cc drivers/infiniband/core/umem_odp.c
index e93b673104dc,f42fa31c24a2..000000000000
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@@ -339,112 -179,85 +339,126 @@@ struct ib_umem_odp *ib_alloc_odp_umem(s
  	if (!odp_data)
  		return ERR_PTR(-ENOMEM);
  	umem = &odp_data->umem;
 -	umem->ibdev = root->umem.ibdev;
 +	umem->context    = ctx;
  	umem->length     = size;
  	umem->address    = addr;
 -	umem->writable   = root->umem.writable;
 -	umem->owning_mm  = root->umem.owning_mm;
  	odp_data->page_shift = PAGE_SHIFT;
 -	odp_data->notifier.ops = ops;
 +	umem->writable   = root->umem.writable;
 +	umem->is_odp = 1;
 +	odp_data->per_mm = per_mm;
 +	umem->owning_mm  = per_mm->mm;
 +	mmgrab(umem->owning_mm);
 +
 +	mutex_init(&odp_data->umem_mutex);
 +	init_completion(&odp_data->notifier_completion);
 +
 +	odp_data->page_list =
 +		vzalloc(array_size(pages, sizeof(*odp_data->page_list)));
 +	if (!odp_data->page_list) {
 +		ret = -ENOMEM;
 +		goto out_odp_data;
 +	}
  
 -	odp_data->tgid = get_pid(root->tgid);
 -	ret = ib_init_umem_odp(odp_data, ops);
 -	if (ret) {
 -		put_pid(odp_data->tgid);
 -		kfree(odp_data);
 -		return ERR_PTR(ret);
 +	odp_data->dma_list =
 +		vzalloc(array_size(pages, sizeof(*odp_data->dma_list)));
 +	if (!odp_data->dma_list) {
 +		ret = -ENOMEM;
 +		goto out_page_list;
  	}
 +
 +	/*
 +	 * Caller must ensure that the umem_odp that the per_mm came from
 +	 * cannot be freed during the call to ib_alloc_odp_umem.
 +	 */
 +	mutex_lock(&ctx->per_mm_list_lock);
 +	per_mm->odp_mrs_count++;
 +	mutex_unlock(&ctx->per_mm_list_lock);
 +	add_umem_to_per_mm(odp_data);
 +
  	return odp_data;
 +
 +out_page_list:
 +	vfree(odp_data->page_list);
 +out_odp_data:
 +	mmdrop(umem->owning_mm);
 +	kfree(odp_data);
 +	return ERR_PTR(ret);
  }
 -EXPORT_SYMBOL(ib_umem_odp_alloc_child);
 +EXPORT_SYMBOL(ib_alloc_odp_umem);
  
 -/**
 - * ib_umem_odp_get - Create a umem_odp for a userspace va
 - *
 - * @udata: userspace context to pin memory for
 - * @addr: userspace virtual address to start at
 - * @size: length of region to pin
 - * @access: IB_ACCESS_xxx flags for memory being pinned
 - *
 - * The driver should use when the access flags indicate ODP memory. It avoids
 - * pinning, instead, stores the mm for future page fault handling in
 - * conjunction with MMU notifiers.
 - */
 -struct ib_umem_odp *ib_umem_odp_get(struct ib_udata *udata, unsigned long addr,
 -				    size_t size, int access,
 -				    const struct mmu_interval_notifier_ops *ops)
 +int ib_umem_odp_get(struct ib_umem_odp *umem_odp, int access)
  {
 -	struct ib_umem_odp *umem_odp;
 -	struct ib_ucontext *context;
 -	struct mm_struct *mm;
 -	int ret;
 +	struct ib_umem *umem = &umem_odp->umem;
 +	/*
 +	 * NOTE: This must called in a process context where umem->owning_mm
 +	 * == current->mm
 +	 */
 +	struct mm_struct *mm = umem->owning_mm;
 +	int ret_val;
  
 -	if (!udata)
 -		return ERR_PTR(-EIO);
++<<<<<<< HEAD
 +	umem_odp->page_shift = PAGE_SHIFT;
 +	if (access & IB_ACCESS_HUGETLB) {
 +		struct vm_area_struct *vma;
 +		struct hstate *h;
 +
 +		down_read(&mm->mmap_sem);
 +		vma = find_vma(mm, ib_umem_start(umem_odp));
 +		if (!vma || !is_vm_hugetlb_page(vma)) {
 +			up_read(&mm->mmap_sem);
 +			return -EINVAL;
 +		}
 +		h = hstate_vma(vma);
 +		umem_odp->page_shift = huge_page_shift(h);
 +		up_read(&mm->mmap_sem);
 +	}
++=======
++	if (access & IB_ACCESS_HUGETLB)
++		umem_odp->page_shift = HPAGE_SHIFT;
++	else
++		umem_odp->page_shift = PAGE_SHIFT;
++>>>>>>> 9ff1b6466a29 (IB/core: Fix ODP with IB_ACCESS_HUGETLB handling)
  
 -	context = container_of(udata, struct uverbs_attr_bundle, driver_udata)
 -			  ->context;
 -	if (!context)
 -		return ERR_PTR(-EIO);
 +	mutex_init(&umem_odp->umem_mutex);
  
 -	if (WARN_ON_ONCE(!(access & IB_ACCESS_ON_DEMAND)))
 -		return ERR_PTR(-EINVAL);
++<<<<<<< HEAD
 +	init_completion(&umem_odp->notifier_completion);
  
 -	umem_odp = kzalloc(sizeof(struct ib_umem_odp), GFP_KERNEL);
 -	if (!umem_odp)
 -		return ERR_PTR(-ENOMEM);
 +	if (ib_umem_odp_num_pages(umem_odp)) {
 +		umem_odp->page_list =
 +			vzalloc(array_size(sizeof(*umem_odp->page_list),
 +					   ib_umem_odp_num_pages(umem_odp)));
 +		if (!umem_odp->page_list)
 +			return -ENOMEM;
  
 -	umem_odp->umem.ibdev = context->device;
 -	umem_odp->umem.length = size;
 -	umem_odp->umem.address = addr;
 -	umem_odp->umem.writable = ib_access_writable(access);
 -	umem_odp->umem.owning_mm = mm = current->mm;
 -	umem_odp->notifier.ops = ops;
 +		umem_odp->dma_list =
 +			vzalloc(array_size(sizeof(*umem_odp->dma_list),
 +					   ib_umem_odp_num_pages(umem_odp)));
 +		if (!umem_odp->dma_list) {
 +			ret_val = -ENOMEM;
 +			goto out_page_list;
 +		}
 +	}
  
 -	if (access & IB_ACCESS_HUGETLB)
 -		umem_odp->page_shift = HPAGE_SHIFT;
 -	else
 -		umem_odp->page_shift = PAGE_SHIFT;
 +	ret_val = get_per_mm(umem_odp);
 +	if (ret_val)
 +		goto out_dma_list;
 +	add_umem_to_per_mm(umem_odp);
  
 -	umem_odp->tgid = get_task_pid(current->group_leader, PIDTYPE_PID);
 -	ret = ib_init_umem_odp(umem_odp, ops);
 -	if (ret)
 -		goto err_put_pid;
 -	return umem_odp;
 +	return 0;
  
 +out_dma_list:
 +	vfree(umem_odp->dma_list);
 +out_page_list:
 +	vfree(umem_odp->page_list);
 +	return ret_val;
++=======
+ err_put_pid:
+ 	put_pid(umem_odp->tgid);
+ 	kfree(umem_odp);
+ 	return ERR_PTR(ret);
++>>>>>>> 9ff1b6466a29 (IB/core: Fix ODP with IB_ACCESS_HUGETLB handling)
  }
 -EXPORT_SYMBOL(ib_umem_odp_get);
  
  void ib_umem_odp_release(struct ib_umem_odp *umem_odp)
  {
* Unmerged path drivers/infiniband/core/umem_odp.c
