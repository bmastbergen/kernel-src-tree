bpf: Refactor trampoline update code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author KP Singh <kpsingh@google.com>
commit 88fd9e5352fe05f7fe57778293aebd4cd106960b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/88fd9e53.failed

As we need to introduce a third type of attachment for trampolines, the
flattened signature of arch_prepare_bpf_trampoline gets even more
complicated.

Refactor the prog and count argument to arch_prepare_bpf_trampoline to
use bpf_tramp_progs to simplify the addition and accounting for new
attachment types.

	Signed-off-by: KP Singh <kpsingh@google.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
Link: https://lore.kernel.org/bpf/20200304191853.1529-2-kpsingh@chromium.org
(cherry picked from commit 88fd9e5352fe05f7fe57778293aebd4cd106960b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
#	include/linux/bpf.h
#	kernel/bpf/bpf_struct_ops.c
#	kernel/bpf/trampoline.c
diff --cc arch/x86/net/bpf_jit_comp.c
index 490d8e35c7c9,15c7d28bc05c..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -1242,6 -1328,357 +1242,360 @@@ emit_jmp
  	return proglen;
  }
  
++<<<<<<< HEAD
++=======
+ static void save_regs(const struct btf_func_model *m, u8 **prog, int nr_args,
+ 		      int stack_size)
+ {
+ 	int i;
+ 	/* Store function arguments to stack.
+ 	 * For a function that accepts two pointers the sequence will be:
+ 	 * mov QWORD PTR [rbp-0x10],rdi
+ 	 * mov QWORD PTR [rbp-0x8],rsi
+ 	 */
+ 	for (i = 0; i < min(nr_args, 6); i++)
+ 		emit_stx(prog, bytes_to_bpf_size(m->arg_size[i]),
+ 			 BPF_REG_FP,
+ 			 i == 5 ? X86_REG_R9 : BPF_REG_1 + i,
+ 			 -(stack_size - i * 8));
+ }
+ 
+ static void restore_regs(const struct btf_func_model *m, u8 **prog, int nr_args,
+ 			 int stack_size)
+ {
+ 	int i;
+ 
+ 	/* Restore function arguments from stack.
+ 	 * For a function that accepts two pointers the sequence will be:
+ 	 * EMIT4(0x48, 0x8B, 0x7D, 0xF0); mov rdi,QWORD PTR [rbp-0x10]
+ 	 * EMIT4(0x48, 0x8B, 0x75, 0xF8); mov rsi,QWORD PTR [rbp-0x8]
+ 	 */
+ 	for (i = 0; i < min(nr_args, 6); i++)
+ 		emit_ldx(prog, bytes_to_bpf_size(m->arg_size[i]),
+ 			 i == 5 ? X86_REG_R9 : BPF_REG_1 + i,
+ 			 BPF_REG_FP,
+ 			 -(stack_size - i * 8));
+ }
+ 
+ static int invoke_bpf(const struct btf_func_model *m, u8 **pprog,
+ 		      struct bpf_tramp_progs *tp, int stack_size)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0, i;
+ 
+ 	for (i = 0; i < tp->nr_progs; i++) {
+ 		if (emit_call(&prog, __bpf_prog_enter, prog))
+ 			return -EINVAL;
+ 		/* remember prog start time returned by __bpf_prog_enter */
+ 		emit_mov_reg(&prog, true, BPF_REG_6, BPF_REG_0);
+ 
+ 		/* arg1: lea rdi, [rbp - stack_size] */
+ 		EMIT4(0x48, 0x8D, 0x7D, -stack_size);
+ 		/* arg2: progs[i]->insnsi for interpreter */
+ 		if (!tp->progs[i]->jited)
+ 			emit_mov_imm64(&prog, BPF_REG_2,
+ 				       (long) tp->progs[i]->insnsi >> 32,
+ 				       (u32) (long) tp->progs[i]->insnsi);
+ 		/* call JITed bpf program or interpreter */
+ 		if (emit_call(&prog, tp->progs[i]->bpf_func, prog))
+ 			return -EINVAL;
+ 
+ 		/* arg1: mov rdi, progs[i] */
+ 		emit_mov_imm64(&prog, BPF_REG_1, (long) tp->progs[i] >> 32,
+ 			       (u32) (long) tp->progs[i]);
+ 		/* arg2: mov rsi, rbx <- start time in nsec */
+ 		emit_mov_reg(&prog, true, BPF_REG_2, BPF_REG_6);
+ 		if (emit_call(&prog, __bpf_prog_exit, prog))
+ 			return -EINVAL;
+ 	}
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ /* Example:
+  * __be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev);
+  * its 'struct btf_func_model' will be nr_args=2
+  * The assembly code when eth_type_trans is executing after trampoline:
+  *
+  * push rbp
+  * mov rbp, rsp
+  * sub rsp, 16                     // space for skb and dev
+  * push rbx                        // temp regs to pass start time
+  * mov qword ptr [rbp - 16], rdi   // save skb pointer to stack
+  * mov qword ptr [rbp - 8], rsi    // save dev pointer to stack
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time in bpf stats are enabled
+  * lea rdi, [rbp - 16]             // R1==ctx of bpf prog
+  * call addr_of_jited_FENTRY_prog
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rdi, qword ptr [rbp - 16]   // restore skb pointer from stack
+  * mov rsi, qword ptr [rbp - 8]    // restore dev pointer from stack
+  * pop rbx
+  * leave
+  * ret
+  *
+  * eth_type_trans has 5 byte nop at the beginning. These 5 bytes will be
+  * replaced with 'call generated_bpf_trampoline'. When it returns
+  * eth_type_trans will continue executing with original skb and dev pointers.
+  *
+  * The assembly code when eth_type_trans is called from trampoline:
+  *
+  * push rbp
+  * mov rbp, rsp
+  * sub rsp, 24                     // space for skb, dev, return value
+  * push rbx                        // temp regs to pass start time
+  * mov qword ptr [rbp - 24], rdi   // save skb pointer to stack
+  * mov qword ptr [rbp - 16], rsi   // save dev pointer to stack
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time if bpf stats are enabled
+  * lea rdi, [rbp - 24]             // R1==ctx of bpf prog
+  * call addr_of_jited_FENTRY_prog  // bpf prog can access skb and dev
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rdi, qword ptr [rbp - 24]   // restore skb pointer from stack
+  * mov rsi, qword ptr [rbp - 16]   // restore dev pointer from stack
+  * call eth_type_trans+5           // execute body of eth_type_trans
+  * mov qword ptr [rbp - 8], rax    // save return value
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time in bpf stats are enabled
+  * lea rdi, [rbp - 24]             // R1==ctx of bpf prog
+  * call addr_of_jited_FEXIT_prog   // bpf prog can access skb, dev, return value
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rax, qword ptr [rbp - 8]    // restore eth_type_trans's return value
+  * pop rbx
+  * leave
+  * add rsp, 8                      // skip eth_type_trans's frame
+  * ret                             // return to its caller
+  */
+ int arch_prepare_bpf_trampoline(void *image, void *image_end,
+ 				const struct btf_func_model *m, u32 flags,
+ 				struct bpf_tramp_progs *tprogs,
+ 				void *orig_call)
+ {
+ 	int cnt = 0, nr_args = m->nr_args;
+ 	int stack_size = nr_args * 8;
+ 	struct bpf_tramp_progs *fentry = &tprogs[BPF_TRAMP_FENTRY];
+ 	struct bpf_tramp_progs *fexit = &tprogs[BPF_TRAMP_FEXIT];
+ 	u8 *prog;
+ 
+ 	/* x86-64 supports up to 6 arguments. 7+ can be added in the future */
+ 	if (nr_args > 6)
+ 		return -ENOTSUPP;
+ 
+ 	if ((flags & BPF_TRAMP_F_RESTORE_REGS) &&
+ 	    (flags & BPF_TRAMP_F_SKIP_FRAME))
+ 		return -EINVAL;
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG)
+ 		stack_size += 8; /* room for return value of orig_call */
+ 
+ 	if (flags & BPF_TRAMP_F_SKIP_FRAME)
+ 		/* skip patched call instruction and point orig_call to actual
+ 		 * body of the kernel function.
+ 		 */
+ 		orig_call += X86_PATCH_SIZE;
+ 
+ 	prog = image;
+ 
+ 	EMIT1(0x55);		 /* push rbp */
+ 	EMIT3(0x48, 0x89, 0xE5); /* mov rbp, rsp */
+ 	EMIT4(0x48, 0x83, 0xEC, stack_size); /* sub rsp, stack_size */
+ 	EMIT1(0x53);		 /* push rbx */
+ 
+ 	save_regs(m, &prog, nr_args, stack_size);
+ 
+ 	if (fentry->nr_progs)
+ 		if (invoke_bpf(m, &prog, fentry, stack_size))
+ 			return -EINVAL;
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG) {
+ 		if (fentry->nr_progs)
+ 			restore_regs(m, &prog, nr_args, stack_size);
+ 
+ 		/* call original function */
+ 		if (emit_call(&prog, orig_call, prog))
+ 			return -EINVAL;
+ 		/* remember return value in a stack for bpf prog to access */
+ 		emit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);
+ 	}
+ 
+ 	if (fexit->nr_progs)
+ 		if (invoke_bpf(m, &prog, fexit, stack_size))
+ 			return -EINVAL;
+ 
+ 	if (flags & BPF_TRAMP_F_RESTORE_REGS)
+ 		restore_regs(m, &prog, nr_args, stack_size);
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG)
+ 		/* restore original return value back into RAX */
+ 		emit_ldx(&prog, BPF_DW, BPF_REG_0, BPF_REG_FP, -8);
+ 
+ 	EMIT1(0x5B); /* pop rbx */
+ 	EMIT1(0xC9); /* leave */
+ 	if (flags & BPF_TRAMP_F_SKIP_FRAME)
+ 		/* skip our return address and return to parent */
+ 		EMIT4(0x48, 0x83, 0xC4, 8); /* add rsp, 8 */
+ 	EMIT1(0xC3); /* ret */
+ 	/* Make sure the trampoline generation logic doesn't overflow */
+ 	if (WARN_ON_ONCE(prog > (u8 *)image_end - BPF_INSN_SAFETY))
+ 		return -EFAULT;
+ 	return prog - (u8 *)image;
+ }
+ 
+ static int emit_cond_near_jump(u8 **pprog, void *func, void *ip, u8 jmp_cond)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 	s64 offset;
+ 
+ 	offset = func - (ip + 2 + 4);
+ 	if (!is_simm32(offset)) {
+ 		pr_err("Target %p is out of range\n", func);
+ 		return -EINVAL;
+ 	}
+ 	EMIT2_off32(0x0F, jmp_cond + 0x10, offset);
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static void emit_nops(u8 **pprog, unsigned int len)
+ {
+ 	unsigned int i, noplen;
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	while (len > 0) {
+ 		noplen = len;
+ 
+ 		if (noplen > ASM_NOP_MAX)
+ 			noplen = ASM_NOP_MAX;
+ 
+ 		for (i = 0; i < noplen; i++)
+ 			EMIT1(ideal_nops[noplen][i]);
+ 		len -= noplen;
+ 	}
+ 
+ 	*pprog = prog;
+ }
+ 
+ static int emit_fallback_jump(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int err = 0;
+ 
+ #ifdef CONFIG_RETPOLINE
+ 	/* Note that this assumes the the compiler uses external
+ 	 * thunks for indirect calls. Both clang and GCC use the same
+ 	 * naming convention for external thunks.
+ 	 */
+ 	err = emit_jump(&prog, __x86_indirect_thunk_rdx, prog);
+ #else
+ 	int cnt = 0;
+ 
+ 	EMIT2(0xFF, 0xE2);	/* jmp rdx */
+ #endif
+ 	*pprog = prog;
+ 	return err;
+ }
+ 
+ static int emit_bpf_dispatcher(u8 **pprog, int a, int b, s64 *progs)
+ {
+ 	u8 *jg_reloc, *jg_target, *prog = *pprog;
+ 	int pivot, err, jg_bytes = 1, cnt = 0;
+ 	s64 jg_offset;
+ 
+ 	if (a == b) {
+ 		/* Leaf node of recursion, i.e. not a range of indices
+ 		 * anymore.
+ 		 */
+ 		EMIT1(add_1mod(0x48, BPF_REG_3));	/* cmp rdx,func */
+ 		if (!is_simm32(progs[a]))
+ 			return -1;
+ 		EMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3),
+ 			    progs[a]);
+ 		err = emit_cond_near_jump(&prog,	/* je func */
+ 					  (void *)progs[a], prog,
+ 					  X86_JE);
+ 		if (err)
+ 			return err;
+ 
+ 		err = emit_fallback_jump(&prog);	/* jmp thunk/indirect */
+ 		if (err)
+ 			return err;
+ 
+ 		*pprog = prog;
+ 		return 0;
+ 	}
+ 
+ 	/* Not a leaf node, so we pivot, and recursively descend into
+ 	 * the lower and upper ranges.
+ 	 */
+ 	pivot = (b - a) / 2;
+ 	EMIT1(add_1mod(0x48, BPF_REG_3));		/* cmp rdx,func */
+ 	if (!is_simm32(progs[a + pivot]))
+ 		return -1;
+ 	EMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3), progs[a + pivot]);
+ 
+ 	if (pivot > 2) {				/* jg upper_part */
+ 		/* Require near jump. */
+ 		jg_bytes = 4;
+ 		EMIT2_off32(0x0F, X86_JG + 0x10, 0);
+ 	} else {
+ 		EMIT2(X86_JG, 0);
+ 	}
+ 	jg_reloc = prog;
+ 
+ 	err = emit_bpf_dispatcher(&prog, a, a + pivot,	/* emit lower_part */
+ 				  progs);
+ 	if (err)
+ 		return err;
+ 
+ 	/* From Intel 64 and IA-32 Architectures Optimization
+ 	 * Reference Manual, 3.4.1.4 Code Alignment, Assembly/Compiler
+ 	 * Coding Rule 11: All branch targets should be 16-byte
+ 	 * aligned.
+ 	 */
+ 	jg_target = PTR_ALIGN(prog, 16);
+ 	if (jg_target != prog)
+ 		emit_nops(&prog, jg_target - prog);
+ 	jg_offset = prog - jg_reloc;
+ 	emit_code(jg_reloc - jg_bytes, jg_offset, jg_bytes);
+ 
+ 	err = emit_bpf_dispatcher(&prog, a + pivot + 1,	/* emit upper_part */
+ 				  b, progs);
+ 	if (err)
+ 		return err;
+ 
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static int cmp_ips(const void *a, const void *b)
+ {
+ 	const s64 *ipa = a;
+ 	const s64 *ipb = b;
+ 
+ 	if (*ipa > *ipb)
+ 		return 1;
+ 	if (*ipa < *ipb)
+ 		return -1;
+ 	return 0;
+ }
+ 
+ int arch_prepare_bpf_dispatcher(void *image, s64 *funcs, int num_funcs)
+ {
+ 	u8 *prog = image;
+ 
+ 	sort(funcs, num_funcs, sizeof(funcs[0]), cmp_ips, NULL);
+ 	return emit_bpf_dispatcher(&prog, 0, num_funcs - 1, funcs);
+ }
+ 
++>>>>>>> 88fd9e5352fe (bpf: Refactor trampoline update code)
  struct x64_jit_data {
  	struct bpf_binary_header *header;
  	int *addrs;
diff --cc include/linux/bpf.h
index 609313a9dbb2,98ec10b23dbb..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -389,13 -413,221 +389,225 @@@ struct bpf_prog_stats 
  	struct u64_stats_sync syncp;
  } __aligned(2 * sizeof(u64));
  
++<<<<<<< HEAD
++=======
+ struct btf_func_model {
+ 	u8 ret_size;
+ 	u8 nr_args;
+ 	u8 arg_size[MAX_BPF_FUNC_ARGS];
+ };
+ 
+ /* Restore arguments before returning from trampoline to let original function
+  * continue executing. This flag is used for fentry progs when there are no
+  * fexit progs.
+  */
+ #define BPF_TRAMP_F_RESTORE_REGS	BIT(0)
+ /* Call original function after fentry progs, but before fexit progs.
+  * Makes sense for fentry/fexit, normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_CALL_ORIG		BIT(1)
+ /* Skip current frame and return to parent.  Makes sense for fentry/fexit
+  * programs only. Should not be used with normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_SKIP_FRAME		BIT(2)
+ 
+ /* Each call __bpf_prog_enter + call bpf_func + call __bpf_prog_exit is ~50
+  * bytes on x86.  Pick a number to fit into BPF_IMAGE_SIZE / 2
+  */
+ #define BPF_MAX_TRAMP_PROGS 40
+ 
+ struct bpf_tramp_progs {
+ 	struct bpf_prog *progs[BPF_MAX_TRAMP_PROGS];
+ 	int nr_progs;
+ };
+ 
+ /* Different use cases for BPF trampoline:
+  * 1. replace nop at the function entry (kprobe equivalent)
+  *    flags = BPF_TRAMP_F_RESTORE_REGS
+  *    fentry = a set of programs to run before returning from trampoline
+  *
+  * 2. replace nop at the function entry (kprobe + kretprobe equivalent)
+  *    flags = BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_SKIP_FRAME
+  *    orig_call = fentry_ip + MCOUNT_INSN_SIZE
+  *    fentry = a set of program to run before calling original function
+  *    fexit = a set of program to run after original function
+  *
+  * 3. replace direct call instruction anywhere in the function body
+  *    or assign a function pointer for indirect call (like tcp_congestion_ops->cong_avoid)
+  *    With flags = 0
+  *      fentry = a set of programs to run before returning from trampoline
+  *    With flags = BPF_TRAMP_F_CALL_ORIG
+  *      orig_call = original callback addr or direct function addr
+  *      fentry = a set of program to run before calling original function
+  *      fexit = a set of program to run after original function
+  */
+ int arch_prepare_bpf_trampoline(void *image, void *image_end,
+ 				const struct btf_func_model *m, u32 flags,
+ 				struct bpf_tramp_progs *tprogs,
+ 				void *orig_call);
+ /* these two functions are called from generated trampoline */
+ u64 notrace __bpf_prog_enter(void);
+ void notrace __bpf_prog_exit(struct bpf_prog *prog, u64 start);
+ 
+ enum bpf_tramp_prog_type {
+ 	BPF_TRAMP_FENTRY,
+ 	BPF_TRAMP_FEXIT,
+ 	BPF_TRAMP_MAX,
+ 	BPF_TRAMP_REPLACE, /* more than MAX */
+ };
+ 
+ struct bpf_trampoline {
+ 	/* hlist for trampoline_table */
+ 	struct hlist_node hlist;
+ 	/* serializes access to fields of this trampoline */
+ 	struct mutex mutex;
+ 	refcount_t refcnt;
+ 	u64 key;
+ 	struct {
+ 		struct btf_func_model model;
+ 		void *addr;
+ 		bool ftrace_managed;
+ 	} func;
+ 	/* if !NULL this is BPF_PROG_TYPE_EXT program that extends another BPF
+ 	 * program by replacing one of its functions. func.addr is the address
+ 	 * of the function it replaced.
+ 	 */
+ 	struct bpf_prog *extension_prog;
+ 	/* list of BPF programs using this trampoline */
+ 	struct hlist_head progs_hlist[BPF_TRAMP_MAX];
+ 	/* Number of attached programs. A counter per kind. */
+ 	int progs_cnt[BPF_TRAMP_MAX];
+ 	/* Executable image of trampoline */
+ 	void *image;
+ 	u64 selector;
+ };
+ 
+ #define BPF_DISPATCHER_MAX 48 /* Fits in 2048B */
+ 
+ struct bpf_dispatcher_prog {
+ 	struct bpf_prog *prog;
+ 	refcount_t users;
+ };
+ 
+ struct bpf_dispatcher {
+ 	/* dispatcher mutex */
+ 	struct mutex mutex;
+ 	void *func;
+ 	struct bpf_dispatcher_prog progs[BPF_DISPATCHER_MAX];
+ 	int num_progs;
+ 	void *image;
+ 	u32 image_off;
+ };
+ 
+ static __always_inline unsigned int bpf_dispatcher_nopfunc(
+ 	const void *ctx,
+ 	const struct bpf_insn *insnsi,
+ 	unsigned int (*bpf_func)(const void *,
+ 				 const struct bpf_insn *))
+ {
+ 	return bpf_func(ctx, insnsi);
+ }
+ #ifdef CONFIG_BPF_JIT
+ struct bpf_trampoline *bpf_trampoline_lookup(u64 key);
+ int bpf_trampoline_link_prog(struct bpf_prog *prog);
+ int bpf_trampoline_unlink_prog(struct bpf_prog *prog);
+ void bpf_trampoline_put(struct bpf_trampoline *tr);
+ #define BPF_DISPATCHER_INIT(name) {			\
+ 	.mutex = __MUTEX_INITIALIZER(name.mutex),	\
+ 	.func = &name##func,				\
+ 	.progs = {},					\
+ 	.num_progs = 0,					\
+ 	.image = NULL,					\
+ 	.image_off = 0					\
+ }
+ 
+ #define DEFINE_BPF_DISPATCHER(name)					\
+ 	noinline unsigned int name##func(				\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *))	\
+ 	{								\
+ 		return bpf_func(ctx, insnsi);				\
+ 	}								\
+ 	EXPORT_SYMBOL(name##func);			\
+ 	struct bpf_dispatcher name = BPF_DISPATCHER_INIT(name);
+ #define DECLARE_BPF_DISPATCHER(name)					\
+ 	unsigned int name##func(					\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *));	\
+ 	extern struct bpf_dispatcher name;
+ #define BPF_DISPATCHER_FUNC(name) name##func
+ #define BPF_DISPATCHER_PTR(name) (&name)
+ void bpf_dispatcher_change_prog(struct bpf_dispatcher *d, struct bpf_prog *from,
+ 				struct bpf_prog *to);
+ struct bpf_image {
+ 	struct latch_tree_node tnode;
+ 	unsigned char data[];
+ };
+ #define BPF_IMAGE_SIZE (PAGE_SIZE - sizeof(struct bpf_image))
+ bool is_bpf_image_address(unsigned long address);
+ void *bpf_image_alloc(void);
+ #else
+ static inline struct bpf_trampoline *bpf_trampoline_lookup(u64 key)
+ {
+ 	return NULL;
+ }
+ static inline int bpf_trampoline_link_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline int bpf_trampoline_unlink_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline void bpf_trampoline_put(struct bpf_trampoline *tr) {}
+ #define DEFINE_BPF_DISPATCHER(name)
+ #define DECLARE_BPF_DISPATCHER(name)
+ #define BPF_DISPATCHER_FUNC(name) bpf_dispatcher_nopfunc
+ #define BPF_DISPATCHER_PTR(name) NULL
+ static inline void bpf_dispatcher_change_prog(struct bpf_dispatcher *d,
+ 					      struct bpf_prog *from,
+ 					      struct bpf_prog *to) {}
+ static inline bool is_bpf_image_address(unsigned long address)
+ {
+ 	return false;
+ }
+ #endif
+ 
+ struct bpf_func_info_aux {
+ 	u16 linkage;
+ 	bool unreliable;
+ };
+ 
+ enum bpf_jit_poke_reason {
+ 	BPF_POKE_REASON_TAIL_CALL,
+ };
+ 
+ /* Descriptor of pokes pointing /into/ the JITed image. */
+ struct bpf_jit_poke_descriptor {
+ 	void *ip;
+ 	union {
+ 		struct {
+ 			struct bpf_map *map;
+ 			u32 key;
+ 		} tail_call;
+ 	};
+ 	bool ip_stable;
+ 	u8 adj_off;
+ 	u16 reason;
+ };
+ 
++>>>>>>> 88fd9e5352fe (bpf: Refactor trampoline update code)
  struct bpf_prog_aux {
 -	atomic64_t refcnt;
 +	atomic_t refcnt;
  	u32 used_map_cnt;
  	u32 max_ctx_offset;
 -	u32 max_pkt_offset;
 -	u32 max_tp_access;
 +	/* not protected by KABI, safe to extend in the middle */
 +	RH_KABI_BROKEN_INSERT(u32 max_pkt_offset)
 +	RH_KABI_BROKEN_INSERT(u32 max_tp_access)
  	u32 stack_depth;
  	u32 id;
  	u32 func_cnt; /* used by non-func prog as the number of func progs */
* Unmerged path kernel/bpf/bpf_struct_ops.c
* Unmerged path kernel/bpf/trampoline.c
* Unmerged path arch/x86/net/bpf_jit_comp.c
* Unmerged path include/linux/bpf.h
* Unmerged path kernel/bpf/bpf_struct_ops.c
* Unmerged path kernel/bpf/trampoline.c
