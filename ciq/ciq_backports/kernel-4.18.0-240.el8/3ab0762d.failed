x86/split_lock: Update to use X86_MATCH_INTEL_FAM6_MODEL()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Tony Luck <tony.luck@intel.com>
commit 3ab0762d1edfda6ccbc08f636acab42c103c299f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/3ab0762d.failed

The SPLIT_LOCK_CPU() macro escaped the tree-wide sweep for old-style
initialization. Update to use X86_MATCH_INTEL_FAM6_MODEL().

Fixes: 6650cdd9a8cc ("x86/split_lock: Enable split lock detection by kernel")
	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20200416205754.21177-2-tony.luck@intel.com

(cherry picked from commit 3ab0762d1edfda6ccbc08f636acab42c103c299f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/intel.c
diff --cc arch/x86/kernel/cpu/intel.c
index ae064a2ca368,ec0d8c74932f..000000000000
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@@ -1034,3 -967,185 +1034,188 @@@ static const struct cpu_dev intel_cpu_d
  
  cpu_dev_register(intel_cpu_dev);
  
++<<<<<<< HEAD
++=======
+ #undef pr_fmt
+ #define pr_fmt(fmt) "x86/split lock detection: " fmt
+ 
+ static const struct {
+ 	const char			*option;
+ 	enum split_lock_detect_state	state;
+ } sld_options[] __initconst = {
+ 	{ "off",	sld_off   },
+ 	{ "warn",	sld_warn  },
+ 	{ "fatal",	sld_fatal },
+ };
+ 
+ static inline bool match_option(const char *arg, int arglen, const char *opt)
+ {
+ 	int len = strlen(opt);
+ 
+ 	return len == arglen && !strncmp(arg, opt, len);
+ }
+ 
+ static bool split_lock_verify_msr(bool on)
+ {
+ 	u64 ctrl, tmp;
+ 
+ 	if (rdmsrl_safe(MSR_TEST_CTRL, &ctrl))
+ 		return false;
+ 	if (on)
+ 		ctrl |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	else
+ 		ctrl &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	if (wrmsrl_safe(MSR_TEST_CTRL, ctrl))
+ 		return false;
+ 	rdmsrl(MSR_TEST_CTRL, tmp);
+ 	return ctrl == tmp;
+ }
+ 
+ static void __init split_lock_setup(void)
+ {
+ 	enum split_lock_detect_state state = sld_warn;
+ 	char arg[20];
+ 	int i, ret;
+ 
+ 	if (!split_lock_verify_msr(false)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	ret = cmdline_find_option(boot_command_line, "split_lock_detect",
+ 				  arg, sizeof(arg));
+ 	if (ret >= 0) {
+ 		for (i = 0; i < ARRAY_SIZE(sld_options); i++) {
+ 			if (match_option(arg, ret, sld_options[i].option)) {
+ 				state = sld_options[i].state;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	switch (state) {
+ 	case sld_off:
+ 		pr_info("disabled\n");
+ 		return;
+ 	case sld_warn:
+ 		pr_info("warning about user-space split_locks\n");
+ 		break;
+ 	case sld_fatal:
+ 		pr_info("sending SIGBUS on user-space split_locks\n");
+ 		break;
+ 	}
+ 
+ 	rdmsrl(MSR_TEST_CTRL, msr_test_ctrl_cache);
+ 
+ 	if (!split_lock_verify_msr(true)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	sld_state = state;
+ 	setup_force_cpu_cap(X86_FEATURE_SPLIT_LOCK_DETECT);
+ }
+ 
+ /*
+  * MSR_TEST_CTRL is per core, but we treat it like a per CPU MSR. Locking
+  * is not implemented as one thread could undo the setting of the other
+  * thread immediately after dropping the lock anyway.
+  */
+ static void sld_update_msr(bool on)
+ {
+ 	u64 test_ctrl_val = msr_test_ctrl_cache;
+ 
+ 	if (on)
+ 		test_ctrl_val |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 
+ 	wrmsrl(MSR_TEST_CTRL, test_ctrl_val);
+ }
+ 
+ static void split_lock_init(void)
+ {
+ 	split_lock_verify_msr(sld_state != sld_off);
+ }
+ 
+ static void split_lock_warn(unsigned long ip)
+ {
+ 	pr_warn_ratelimited("#AC: %s/%d took a split_lock trap at address: 0x%lx\n",
+ 			    current->comm, current->pid, ip);
+ 
+ 	/*
+ 	 * Disable the split lock detection for this task so it can make
+ 	 * progress and set TIF_SLD so the detection is re-enabled via
+ 	 * switch_to_sld() when the task is scheduled out.
+ 	 */
+ 	sld_update_msr(false);
+ 	set_tsk_thread_flag(current, TIF_SLD);
+ }
+ 
+ bool handle_guest_split_lock(unsigned long ip)
+ {
+ 	if (sld_state == sld_warn) {
+ 		split_lock_warn(ip);
+ 		return true;
+ 	}
+ 
+ 	pr_warn_once("#AC: %s/%d %s split_lock trap at address: 0x%lx\n",
+ 		     current->comm, current->pid,
+ 		     sld_state == sld_fatal ? "fatal" : "bogus", ip);
+ 
+ 	current->thread.error_code = 0;
+ 	current->thread.trap_nr = X86_TRAP_AC;
+ 	force_sig_fault(SIGBUS, BUS_ADRALN, NULL);
+ 	return false;
+ }
+ EXPORT_SYMBOL_GPL(handle_guest_split_lock);
+ 
+ bool handle_user_split_lock(struct pt_regs *regs, long error_code)
+ {
+ 	if ((regs->flags & X86_EFLAGS_AC) || sld_state == sld_fatal)
+ 		return false;
+ 	split_lock_warn(regs->ip);
+ 	return true;
+ }
+ 
+ /*
+  * This function is called only when switching between tasks with
+  * different split-lock detection modes. It sets the MSR for the
+  * mode of the new task. This is right most of the time, but since
+  * the MSR is shared by hyperthreads on a physical core there can
+  * be glitches when the two threads need different modes.
+  */
+ void switch_to_sld(unsigned long tifn)
+ {
+ 	sld_update_msr(!(tifn & _TIF_SLD));
+ }
+ 
+ /*
+  * The following processors have the split lock detection feature. But
+  * since they don't have the IA32_CORE_CAPABILITIES MSR, the feature cannot
+  * be enumerated. Enable it by family and model matching on these
+  * processors.
+  */
+ static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
+ 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,		0),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_L,		0),
+ 	{}
+ };
+ 
+ void __init cpu_set_core_cap_bits(struct cpuinfo_x86 *c)
+ {
+ 	u64 ia32_core_caps = 0;
+ 
+ 	if (c->x86_vendor != X86_VENDOR_INTEL)
+ 		return;
+ 	if (cpu_has(c, X86_FEATURE_CORE_CAPABILITIES)) {
+ 		/* Enumerate features reported in IA32_CORE_CAPABILITIES MSR. */
+ 		rdmsrl(MSR_IA32_CORE_CAPS, ia32_core_caps);
+ 	} else if (!boot_cpu_has(X86_FEATURE_HYPERVISOR)) {
+ 		/* Enumerate split lock detection by family and model. */
+ 		if (x86_match_cpu(split_lock_cpu_ids))
+ 			ia32_core_caps |= MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT;
+ 	}
+ 
+ 	if (ia32_core_caps & MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT)
+ 		split_lock_setup();
+ }
++>>>>>>> 3ab0762d1edf (x86/split_lock: Update to use X86_MATCH_INTEL_FAM6_MODEL())
* Unmerged path arch/x86/kernel/cpu/intel.c
