libbpf: Add setter for initial value for internal maps

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Toke Høiland-Jørgensen <toke@redhat.com>
commit e2842be53d4f31962a9992eab39391cdf637fa2e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e2842be5.failed

For internal maps (most notably the maps backing global variables), libbpf
uses an internal mmaped area to store the data after opening the object.
This data is subsequently copied into the kernel map when the object is
loaded.

This adds a function to set a new value for that data, which can be used to
before it is loaded into the kernel. This is especially relevant for RODATA
maps, since those are frozen on load.

	Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
Link: https://lore.kernel.org/bpf/20200329132253.232541-1-toke@redhat.com
(cherry picked from commit e2842be53d4f31962a9992eab39391cdf637fa2e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/lib/bpf/libbpf.map
diff --cc tools/lib/bpf/libbpf.map
index 3ca7228af37e,159826b36b38..000000000000
--- a/tools/lib/bpf/libbpf.map
+++ b/tools/lib/bpf/libbpf.map
@@@ -188,9 -197,53 +188,54 @@@ LIBBPF_0.0.4 
  		bpf_map__get_pin_path;
  		bpf_map__is_pinned;
  		bpf_map__set_pin_path;
 -		bpf_object__open_file;
 -		bpf_object__open_mem;
 -		bpf_program__attach_trace;
  		bpf_program__get_expected_attach_type;
  		bpf_program__get_type;
 -		bpf_program__is_tracing;
 -		bpf_program__set_tracing;
 +		bpf_get_link_xdp_info;
  		bpf_program__size;
++<<<<<<< HEAD
 +		bpf_btf_get_next_id;
 +} LIBBPF_0.0.3;
++=======
+ 		btf__find_by_name_kind;
+ 		libbpf_find_vmlinux_btf_id;
+ } LIBBPF_0.0.5;
+ 
+ LIBBPF_0.0.7 {
+ 	global:
+ 		btf_dump__emit_type_decl;
+ 		bpf_link__disconnect;
+ 		bpf_map__attach_struct_ops;
+ 		bpf_map_delete_batch;
+ 		bpf_map_lookup_and_delete_batch;
+ 		bpf_map_lookup_batch;
+ 		bpf_map_update_batch;
+ 		bpf_object__find_program_by_name;
+ 		bpf_object__attach_skeleton;
+ 		bpf_object__destroy_skeleton;
+ 		bpf_object__detach_skeleton;
+ 		bpf_object__load_skeleton;
+ 		bpf_object__open_skeleton;
+ 		bpf_probe_large_insn_limit;
+ 		bpf_prog_attach_xattr;
+ 		bpf_program__attach;
+ 		bpf_program__name;
+ 		bpf_program__is_extension;
+ 		bpf_program__is_struct_ops;
+ 		bpf_program__set_extension;
+ 		bpf_program__set_struct_ops;
+ 		btf__align_of;
+ 		libbpf_find_kernel_btf;
+ } LIBBPF_0.0.6;
+ 
+ LIBBPF_0.0.8 {
+ 	global:
+ 		bpf_link__fd;
+ 		bpf_link__open;
+ 		bpf_link__pin;
+ 		bpf_link__pin_path;
+ 		bpf_link__unpin;
+ 		bpf_map__set_initial_value;
+ 		bpf_program__set_attach_target;
+ 		bpf_set_link_xdp_fd_opts;
+ } LIBBPF_0.0.7;
++>>>>>>> e2842be53d4f (libbpf: Add setter for initial value for internal maps)
diff --git a/tools/lib/bpf/libbpf.c b/tools/lib/bpf/libbpf.c
index 7277de638c6d..a869d74074b3 100644
--- a/tools/lib/bpf/libbpf.c
+++ b/tools/lib/bpf/libbpf.c
@@ -5281,6 +5281,17 @@ void *bpf_map__priv(const struct bpf_map *map)
 	return map ? map->priv : ERR_PTR(-EINVAL);
 }
 
+int bpf_map__set_initial_value(struct bpf_map *map,
+			       const void *data, size_t size)
+{
+	if (!map->mmaped || map->libbpf_type == LIBBPF_MAP_KCONFIG ||
+	    size != map->def.value_size || map->fd >= 0)
+		return -EINVAL;
+
+	memcpy(map->mmaped, data, size);
+	return 0;
+}
+
 bool bpf_map__is_offload_neutral(const struct bpf_map *map)
 {
 	return map->def.type == BPF_MAP_TYPE_PERF_EVENT_ARRAY;
diff --git a/tools/lib/bpf/libbpf.h b/tools/lib/bpf/libbpf.h
index e454ceec9fa2..625cf0955a1a 100644
--- a/tools/lib/bpf/libbpf.h
+++ b/tools/lib/bpf/libbpf.h
@@ -370,6 +370,8 @@ typedef void (*bpf_map_clear_priv_t)(struct bpf_map *, void *);
 LIBBPF_API int bpf_map__set_priv(struct bpf_map *map, void *priv,
 				 bpf_map_clear_priv_t clear_priv);
 LIBBPF_API void *bpf_map__priv(const struct bpf_map *map);
+LIBBPF_API int bpf_map__set_initial_value(struct bpf_map *map,
+					  const void *data, size_t size);
 LIBBPF_API int bpf_map__reuse_fd(struct bpf_map *map, int fd);
 LIBBPF_API int bpf_map__resize(struct bpf_map *map, __u32 max_entries);
 LIBBPF_API bool bpf_map__is_offload_neutral(const struct bpf_map *map);
* Unmerged path tools/lib/bpf/libbpf.map
