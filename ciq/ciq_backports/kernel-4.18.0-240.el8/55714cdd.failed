KVM: nSVM: Move SMI vmexit handling to svm_check_nested_events()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 55714cddbf1028bbfa19fd7d69182de3f135ce99
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/55714cdd.failed

Unlike VMX, SVM allows a hypervisor to take a SMI vmexit without having
any special SMM-monitor enablement sequence.  Therefore, it has to be
handled like interrupts and NMIs.  Check for an unblocked SMI in
svm_check_nested_events() so that pending SMIs are correctly prioritized
over IRQs and NMIs when the latter events will trigger VM-Exit.

Note that there is no need to test explicitly for SMI vmexits, because
guests always runs outside SMM and therefore can never get an SMI while
they are blocked.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 55714cddbf1028bbfa19fd7d69182de3f135ce99)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/nested.c
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/svm/svm.h
diff --cc arch/x86/kvm/svm/svm.c
index a95f04022d02,83b8bc305fe1..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -6409,64 -3040,48 +6409,68 @@@ static int svm_smi_allowed(struct kvm_v
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	BUG_ON(!(gif_set(svm)));
 +	/* Per APM Vol.2 15.22.2 "Response to SMI" */
 +	if (!gif_set(svm))
 +		return 0;
  
 -	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 -	++vcpu->stat.irq_injections;
++<<<<<<< HEAD
 +	if (is_guest_mode(&svm->vcpu) &&
 +	    svm->nested.intercept & (1ULL << INTERCEPT_SMI)) {
 +		/* TODO: Might need to set exit_info_1 and exit_info_2 here */
 +		svm->vmcb->control.exit_code = SVM_EXIT_SMI;
 +		svm->nested.exit_required = true;
 +		return 0;
 +	}
  
 -	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 -		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
 +	return 1;
++=======
++	return !is_smm(vcpu);
++>>>>>>> 55714cddbf10 (KVM: nSVM: Move SMI vmexit handling to svm_check_nested_events())
  }
  
 -static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 +	int ret;
  
 -	if (svm_nested_virtualize_tpr(vcpu))
 -		return;
 -
 -	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +	if (is_guest_mode(vcpu)) {
 +		/* FED8h - SVM Guest */
 +		put_smstate(u64, smstate, 0x7ed8, 1);
 +		/* FEE0h - SVM Guest VMCB Physical Address */
 +		put_smstate(u64, smstate, 0x7ee0, svm->nested.vmcb);
  
 -	if (irr == -1)
 -		return;
 +		svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 +		svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 +		svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
  
 -	if (tpr >= irr)
 -		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +		ret = nested_svm_vmexit(svm);
 +		if (ret)
 +			return ret;
 +	}
 +	return 0;
  }
  
 -static bool svm_nmi_allowed(struct kvm_vcpu *vcpu)
 +static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, const char *smstate)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb *vmcb = svm->vmcb;
 -	bool ret;
 -
 -	if (is_guest_mode(vcpu) && nested_exit_on_nmi(svm))
 -		return true;
 +	struct vmcb *nested_vmcb;
 +	struct kvm_host_map map;
 +	u64 guest;
 +	u64 vmcb;
  
 -	ret = !(vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) &&
 -	      !(svm->vcpu.arch.hflags & HF_NMI_MASK);
 -	ret = ret && gif_set(svm);
 +	guest = GET_SMSTATE(u64, smstate, 0x7ed8);
 +	vmcb = GET_SMSTATE(u64, smstate, 0x7ee0);
  
 -	return ret;
 +	if (guest) {
 +		if (kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb), &map) == -EINVAL)
 +			return 1;
 +		nested_vmcb = map.hva;
 +		enter_svm_guest_mode(svm, vmcb, nested_vmcb, &map);
 +	}
 +	return 0;
  }
  
 -static bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)
 +static int enable_smi_window(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
  
* Unmerged path arch/x86/kvm/svm/nested.c
* Unmerged path arch/x86/kvm/svm/svm.h
* Unmerged path arch/x86/kvm/svm/nested.c
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/svm/svm.h
