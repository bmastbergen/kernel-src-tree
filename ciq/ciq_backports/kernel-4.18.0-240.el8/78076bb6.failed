io_uring: use hash table for poll command lookups

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 78076bb64aa8ba5b7207c38b2660a9e10ffa8cc7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/78076bb6.failed

We recently changed this from a single list to an rbtree, but for some
real life workloads, the rbtree slows down the submission/insertion
case enough so that it's the top cycle consumer on the io_uring side.
In testing, using a hash table is a more well rounded compromise. It
is fast for insertion, and as long as it's sized appropriately, it
works well for the cancellation case as well. Running TAO with a lot
of network sockets, this removes io_poll_req_insert() from spending
2% of the CPU cycles.

	Reported-by: Dan Melnic <dmm@fb.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 78076bb64aa8ba5b7207c38b2660a9e10ffa8cc7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 8c2ecae32a7b,8fa6b190a238..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -277,22 -275,12 +277,30 @@@ struct io_ring_ctx 
  		 * manipulate the list, hence no extra locking is needed there.
  		 */
  		struct list_head	poll_list;
++<<<<<<< HEAD
 +		struct list_head	cancel_list;
++=======
+ 		struct hlist_head	*cancel_hash;
+ 		unsigned		cancel_hash_bits;
+ 
+ 		spinlock_t		inflight_lock;
+ 		struct list_head	inflight_list;
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	} ____cacheline_aligned_in_smp;
 +
 +	struct async_list	pending_async[2];
 +
 +#if defined(CONFIG_UNIX)
 +	struct socket		*ring_sock;
 +#endif
 +};
 +
 +struct sqe_submit {
 +	const struct io_uring_sqe	*sqe;
 +	u32				sequence;
 +	bool				has_user;
 +	bool				needs_lock;
 +	bool				needs_fixed_file;
  };
  
  /*
@@@ -321,10 -345,19 +329,17 @@@ struct io_kiocb 
  		struct io_poll_iocb	poll;
  	};
  
 -	const struct io_uring_sqe	*sqe;
 -	struct io_async_ctx		*io;
 -	struct file			*ring_file;
 -	int				ring_fd;
 -	bool				has_user;
 -	bool				in_async;
 -	bool				needs_fixed_file;
 +	struct sqe_submit	submit;
  
  	struct io_ring_ctx	*ctx;
++<<<<<<< HEAD
 +	struct list_head	list;
++=======
+ 	union {
+ 		struct list_head	list;
+ 		struct hlist_node	hash_node;
+ 	};
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	struct list_head	link_list;
  	unsigned int		flags;
  	refcount_t		refs;
@@@ -397,43 -445,77 +412,86 @@@ static void io_ring_ctx_ref_free(struc
  static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
  {
  	struct io_ring_ctx *ctx;
++<<<<<<< HEAD
 +	int i;
++=======
+ 	int hash_bits;
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  
  	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
  	if (!ctx)
  		return NULL;
  
++<<<<<<< HEAD
++=======
+ 	ctx->fallback_req = kmem_cache_alloc(req_cachep, GFP_KERNEL);
+ 	if (!ctx->fallback_req)
+ 		goto err;
+ 
+ 	ctx->completions = kmalloc(2 * sizeof(struct completion), GFP_KERNEL);
+ 	if (!ctx->completions)
+ 		goto err;
+ 
+ 	/*
+ 	 * Use 5 bits less than the max cq entries, that should give us around
+ 	 * 32 entries per hash list if totally full and uniformly spread.
+ 	 */
+ 	hash_bits = ilog2(p->cq_entries);
+ 	hash_bits -= 5;
+ 	if (hash_bits <= 0)
+ 		hash_bits = 1;
+ 	ctx->cancel_hash_bits = hash_bits;
+ 	ctx->cancel_hash = kmalloc((1U << hash_bits) * sizeof(struct hlist_head),
+ 					GFP_KERNEL);
+ 	if (!ctx->cancel_hash)
+ 		goto err;
+ 	__hash_init(ctx->cancel_hash, 1U << hash_bits);
+ 
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	if (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,
 -			    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))
 -		goto err;
 +			    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL)) {
 +		kfree(ctx);
 +		return NULL;
 +	}
  
  	ctx->flags = p->flags;
  	init_waitqueue_head(&ctx->cq_wait);
 -	INIT_LIST_HEAD(&ctx->cq_overflow_list);
 -	init_completion(&ctx->completions[0]);
 -	init_completion(&ctx->completions[1]);
 +	init_completion(&ctx->ctx_done);
 +	init_completion(&ctx->sqo_thread_started);
  	mutex_init(&ctx->uring_lock);
  	init_waitqueue_head(&ctx->wait);
 +	for (i = 0; i < ARRAY_SIZE(ctx->pending_async); i++) {
 +		spin_lock_init(&ctx->pending_async[i].lock);
 +		INIT_LIST_HEAD(&ctx->pending_async[i].list);
 +		atomic_set(&ctx->pending_async[i].cnt, 0);
 +	}
  	spin_lock_init(&ctx->completion_lock);
  	INIT_LIST_HEAD(&ctx->poll_list);
++<<<<<<< HEAD
 +	INIT_LIST_HEAD(&ctx->cancel_list);
++=======
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	INIT_LIST_HEAD(&ctx->defer_list);
 -	INIT_LIST_HEAD(&ctx->timeout_list);
 -	init_waitqueue_head(&ctx->inflight_wait);
 -	spin_lock_init(&ctx->inflight_lock);
 -	INIT_LIST_HEAD(&ctx->inflight_list);
  	return ctx;
++<<<<<<< HEAD
++=======
+ err:
+ 	if (ctx->fallback_req)
+ 		kmem_cache_free(req_cachep, ctx->fallback_req);
+ 	kfree(ctx->completions);
+ 	kfree(ctx->cancel_hash);
+ 	kfree(ctx);
+ 	return NULL;
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  }
  
 -static inline bool __req_need_defer(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	return req->sequence != ctx->cached_cq_tail + ctx->cached_sq_dropped
 -					+ atomic_read(&ctx->cached_cq_overflow);
 -}
 -
 -static inline bool req_need_defer(struct io_kiocb *req)
 +static inline bool io_sequence_defer(struct io_ring_ctx *ctx,
 +				     struct io_kiocb *req)
  {
 -	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) == REQ_F_IO_DRAIN)
 -		return __req_need_defer(req);
 +	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) != REQ_F_IO_DRAIN)
 +		return false;
  
 -	return false;
 +	return req->sequence != ctx->cached_cq_tail + ctx->sq_ring->dropped;
  }
  
  static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
@@@ -1636,33 -2092,238 +1694,140 @@@ static int io_recvmsg(struct io_kiocb *
  #endif
  }
  
 -static int io_recvmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
++<<<<<<< HEAD
++=======
++static int io_connect_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
 -	struct user_msghdr __user *msg;
 -	unsigned flags;
++	struct sockaddr __user *addr;
++	int addr_len;
+ 
 -	flags = READ_ONCE(sqe->msg_flags);
 -	msg = (struct user_msghdr __user *)(unsigned long) READ_ONCE(sqe->addr);
 -	return recvmsg_copy_msghdr(&io->msg.msg, msg, flags, &io->msg.uaddr,
 -					&io->msg.iov);
++	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
++	addr_len = READ_ONCE(sqe->addr2);
++	return move_addr_to_kernel(addr, addr_len, &io->connect.address);
+ #else
+ 	return 0;
+ #endif
+ }
+ 
 -static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++static int io_connect(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 		      struct io_kiocb **nxt, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
 -	struct socket *sock;
 -	int ret;
++	struct io_async_ctx __io, *io;
++	unsigned file_flags;
++	int addr_len, ret;
+ 
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
++	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
++		return -EINVAL;
++	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
+ 		return -EINVAL;
+ 
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct user_msghdr __user *msg;
 -		struct io_async_ctx io, *copy;
 -		struct sockaddr_storage addr;
 -		struct msghdr *kmsg;
 -		unsigned flags;
++	addr_len = READ_ONCE(sqe->addr2);
++	file_flags = force_nonblock ? O_NONBLOCK : 0;
+ 
 -		flags = READ_ONCE(sqe->msg_flags);
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 -
 -		msg = (struct user_msghdr __user *) (unsigned long)
 -			READ_ONCE(sqe->addr);
 -		if (req->io) {
 -			kmsg = &req->io->msg.msg;
 -			kmsg->msg_name = &addr;
 -		} else {
 -			kmsg = &io.msg.msg;
 -			kmsg->msg_name = &addr;
 -			io.msg.iov = io.msg.fast_iov;
 -			ret = io_recvmsg_prep(req, &io);
 -			if (ret)
 -				goto out;
 -		}
 -
 -		ret = __sys_recvmsg_sock(sock, kmsg, msg, io.msg.uaddr, flags);
 -		if (force_nonblock && ret == -EAGAIN) {
 -			copy = kmalloc(sizeof(*copy), GFP_KERNEL);
 -			if (!copy) {
 -				ret = -ENOMEM;
 -				goto out;
 -			}
 -			memcpy(copy, &io, sizeof(*copy));
 -			req->io = copy;
 -			memcpy(&req->io->sqe, req->sqe, sizeof(*req->sqe));
 -			req->sqe = &req->io->sqe;
 -			return ret;
 -		}
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 -	}
 -
 -out:
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0 && (req->flags & REQ_F_LINK))
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_put_req_find_next(req, nxt);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_accept(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		     struct io_kiocb **nxt, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct sockaddr __user *addr;
 -	int __user *addr_len;
 -	unsigned file_flags;
 -	int flags, ret;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -
 -	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
 -	addr_len = (int __user *) (unsigned long) READ_ONCE(sqe->addr2);
 -	flags = READ_ONCE(sqe->accept_flags);
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	ret = __sys_accept4_file(req->file, file_flags, addr, addr_len, flags);
 -	if (ret == -EAGAIN && force_nonblock) {
 -		req->work.flags |= IO_WQ_WORK_NEEDS_FILES;
 -		return -EAGAIN;
 -	}
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -	if (ret < 0 && (req->flags & REQ_F_LINK))
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_connect_prep(struct io_kiocb *req, struct io_async_ctx *io)
 -{
 -#if defined(CONFIG_NET)
 -	const struct io_uring_sqe *sqe = req->sqe;
 -	struct sockaddr __user *addr;
 -	int addr_len;
 -
 -	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
 -	addr_len = READ_ONCE(sqe->addr2);
 -	return move_addr_to_kernel(addr, addr_len, &io->connect.address);
 -#else
 -	return 0;
 -#endif
 -}
 -
 -static int io_connect(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		      struct io_kiocb **nxt, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_ctx __io, *io;
 -	unsigned file_flags;
 -	int addr_len, ret;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 -
 -	addr_len = READ_ONCE(sqe->addr2);
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	if (req->io) {
 -		io = req->io;
 -	} else {
 -		ret = io_connect_prep(req, &__io);
 -		if (ret)
 -			goto out;
 -		io = &__io;
 -	}
++	if (req->io) {
++		io = req->io;
++	} else {
++		ret = io_connect_prep(req, &__io);
++		if (ret)
++			goto out;
++		io = &__io;
++	}
+ 
+ 	ret = __sys_connect_file(req->file, &io->connect.address, addr_len,
+ 					file_flags);
+ 	if ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {
+ 		io = kmalloc(sizeof(*io), GFP_KERNEL);
+ 		if (!io) {
+ 			ret = -ENOMEM;
+ 			goto out;
+ 		}
+ 		memcpy(&io->connect, &__io.connect, sizeof(io->connect));
+ 		req->io = io;
+ 		memcpy(&io->sqe, req->sqe, sizeof(*req->sqe));
+ 		req->sqe = &io->sqe;
+ 		return -EAGAIN;
+ 	}
+ 	if (ret == -ERESTARTSYS)
+ 		ret = -EINTR;
+ out:
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  static void io_poll_remove_one(struct io_kiocb *req)
  {
  	struct io_poll_iocb *poll = &req->poll;
  
  	spin_lock(&poll->head->lock);
  	WRITE_ONCE(poll->canceled, true);
 -	if (!list_empty(&poll->wait->entry)) {
 -		list_del_init(&poll->wait->entry);
 -		io_queue_async_work(req);
 +	if (!list_empty(&poll->wait.entry)) {
 +		list_del_init(&poll->wait.entry);
 +		io_queue_async_work(req->ctx, req);
  	}
  	spin_unlock(&poll->head->lock);
++<<<<<<< HEAD
 +
 +	list_del_init(&req->list);
++=======
+ 	hash_del(&req->hash_node);
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  }
  
  static void io_poll_remove_all(struct io_ring_ctx *ctx)
  {
++<<<<<<< HEAD
++=======
+ 	struct hlist_node *tmp;
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	struct io_kiocb *req;
+ 	int i;
  
  	spin_lock_irq(&ctx->completion_lock);
++<<<<<<< HEAD
 +	while (!list_empty(&ctx->cancel_list)) {
 +		req = list_first_entry(&ctx->cancel_list, struct io_kiocb,list);
 +		io_poll_remove_one(req);
++=======
+ 	for (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {
+ 		struct hlist_head *list;
+ 
+ 		list = &ctx->cancel_hash[i];
+ 		hlist_for_each_entry_safe(req, tmp, list, hash_node)
+ 			io_poll_remove_one(req);
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	}
  	spin_unlock_irq(&ctx->completion_lock);
  }
  
++<<<<<<< HEAD
++=======
+ static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)
+ {
+ 	struct hlist_head *list;
+ 	struct io_kiocb *req;
+ 
+ 	list = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];
+ 	hlist_for_each_entry(req, list, hash_node) {
+ 		if (sqe_addr == req->user_data) {
+ 			io_poll_remove_one(req);
+ 			return 0;
+ 		}
+ 	}
+ 
+ 	return -ENOENT;
+ }
+ 
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  /*
   * Find a running poll command that matches one specified in sqe->addr,
   * and remove it if found.
@@@ -1726,8 -2397,8 +1891,13 @@@ static void io_poll_complete_work(struc
  		spin_unlock_irq(&ctx->completion_lock);
  		return;
  	}
++<<<<<<< HEAD
 +	list_del_init(&req->list);
 +	io_poll_complete(ctx, req, mask);
++=======
+ 	hash_del(&req->hash_node);
+ 	io_poll_complete(req, mask, ret);
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	spin_unlock_irq(&ctx->completion_lock);
  
  	io_cqring_ev_posted(ctx);
@@@ -1748,11 -2423,19 +1918,18 @@@ static int io_poll_wake(struct wait_que
  	if (mask && !(mask & poll->events))
  		return 0;
  
 -	list_del_init(&poll->wait->entry);
 +	list_del_init(&poll->wait.entry);
  
 -	/*
 -	 * Run completion inline if we can. We're using trylock here because
 -	 * we are violating the completion_lock -> poll wq lock ordering.
 -	 * If we have a link timeout we're going to need the completion_lock
 -	 * for finalizing the request, mark us as having grabbed that already.
 -	 */
  	if (mask && spin_trylock_irqsave(&ctx->completion_lock, flags)) {
++<<<<<<< HEAD
 +		list_del(&req->list);
 +		io_poll_complete(ctx, req, mask);
++=======
+ 		hash_del(&req->hash_node);
+ 		io_poll_complete(req, mask, 0);
+ 		req->flags |= REQ_F_COMP_LOCKED;
+ 		io_put_req(req);
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  		spin_unlock_irqrestore(&ctx->completion_lock, flags);
  
  		io_cqring_ev_posted(ctx);
@@@ -1782,10 -2464,20 +1959,24 @@@ static void io_poll_queue_proc(struct f
  
  	pt->error = 0;
  	pt->req->poll.head = head;
 -	add_wait_queue(head, pt->req->poll.wait);
 +	add_wait_queue(head, &pt->req->poll.wait);
  }
  
++<<<<<<< HEAD
 +static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++=======
+ static void io_poll_req_insert(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct hlist_head *list;
+ 
+ 	list = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];
+ 	hlist_add_head(&req->hash_node, list);
+ }
+ 
+ static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 		       struct io_kiocb **nxt)
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  {
  	struct io_poll_iocb *poll = &req->poll;
  	struct io_ring_ctx *ctx = req->ctx;
@@@ -1801,10 -2493,15 +1992,14 @@@
  	if (!poll->file)
  		return -EBADF;
  
 -	poll->wait = kmalloc(sizeof(*poll->wait), GFP_KERNEL);
 -	if (!poll->wait)
 -		return -ENOMEM;
 -
 -	req->io = NULL;
 -	INIT_IO_WORK(&req->work, io_poll_complete_work);
 +	req->submit.sqe = NULL;
 +	INIT_WORK(&req->work, io_poll_complete_work);
  	events = READ_ONCE(sqe->poll_events);
  	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
++<<<<<<< HEAD
++=======
+ 	INIT_HLIST_NODE(&req->hash_node);
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  
  	poll->head = NULL;
  	poll->done = false;
@@@ -3529,6 -4639,10 +3724,13 @@@ static void io_ring_ctx_free(struct io_
  		io_unaccount_mem(ctx->user,
  				ring_pages(ctx->sq_entries, ctx->cq_entries));
  	free_uid(ctx->user);
++<<<<<<< HEAD
++=======
+ 	put_cred(ctx->creds);
+ 	kfree(ctx->completions);
+ 	kfree(ctx->cancel_hash);
+ 	kmem_cache_free(req_cachep, ctx->fallback_req);
++>>>>>>> 78076bb64aa8 (io_uring: use hash table for poll command lookups)
  	kfree(ctx);
  }
  
* Unmerged path fs/io_uring.c
