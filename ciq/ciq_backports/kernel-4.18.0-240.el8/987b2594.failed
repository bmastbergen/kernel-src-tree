KVM: x86: Move kvm_vcpu_init() invocation to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 987b2594ed5d128c95c5255a9c7755f7480bf407
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/987b2594.failed

Move the kvm_cpu_{un}init() calls to common x86 code as an intermediate
step to removing kvm_cpu_{un}init() altogether.

Note, VMX'x alloc_apic_access_page() and init_rmode_identity_map() are
per-VM allocations and are intentionally kept if vCPU creation fails.
They are freed by kvm_arch_destroy_vm().

No functional change intended.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 987b2594ed5d128c95c5255a9c7755f7480bf407)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/include/asm/kvm_host.h
index b0a84267e07c,fff9ed6956b5..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1053,7 -1050,7 +1053,11 @@@ struct kvm_x86_ops 
  	void (*vm_destroy)(struct kvm *kvm);
  
  	/* Create, but do not attach this VCPU */
++<<<<<<< HEAD
 +	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);
++=======
+ 	int (*vcpu_create)(struct kvm_vcpu *vcpu);
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  	void (*vcpu_free)(struct kvm_vcpu *vcpu);
  	void (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);
  
diff --cc arch/x86/kvm/svm.c
index 4b247e55061a,83257a7a2e37..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -2201,7 -2187,7 +2201,11 @@@ static int avic_init_vcpu(struct vcpu_s
  	return ret;
  }
  
++<<<<<<< HEAD
 +static struct kvm_vcpu *svm_create_vcpu(struct kvm *kvm, unsigned int id)
++=======
+ static int svm_create_vcpu(struct kvm_vcpu *vcpu)
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  {
  	struct vcpu_svm *svm;
  	struct page *page;
@@@ -2210,35 -2196,9 +2214,38 @@@
  	struct page *nested_msrpm_pages;
  	int err;
  
 -	BUILD_BUG_ON(offsetof(struct vcpu_svm, vcpu) != 0);
 -	svm = to_svm(vcpu);
 +	BUILD_BUG_ON_MSG(offsetof(struct vcpu_svm, vcpu) != 0,
 +		"struct kvm_vcpu must be at offset 0 for arch usercopy region");
 +
++<<<<<<< HEAD
 +	svm = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL_ACCOUNT);
 +	if (!svm) {
 +		err = -ENOMEM;
 +		goto out;
 +	}
 +
 +	svm->vcpu.arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +						     GFP_KERNEL_ACCOUNT);
 +	if (!svm->vcpu.arch.user_fpu) {
 +		printk(KERN_ERR "kvm: failed to allocate kvm userspace's fpu\n");
 +		err = -ENOMEM;
 +		goto free_partial_svm;
 +	}
  
 +	svm->vcpu.arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +						     GFP_KERNEL_ACCOUNT);
 +	if (!svm->vcpu.arch.guest_fpu) {
 +		printk(KERN_ERR "kvm: failed to allocate vcpu's fpu\n");
 +		err = -ENOMEM;
 +		goto free_user_fpu;
 +	}
 +
 +	err = kvm_vcpu_init(&svm->vcpu, kvm, id);
 +	if (err)
 +		goto free_svm;
 +
++=======
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  	err = -ENOMEM;
  	page = alloc_page(GFP_KERNEL_ACCOUNT);
  	if (!page)
@@@ -2292,16 -2251,8 +2299,21 @@@ free_page2
  	__free_pages(msrpm_pages, MSRPM_ALLOC_ORDER);
  free_page1:
  	__free_page(page);
++<<<<<<< HEAD
 +uninit:
 +	kvm_vcpu_uninit(&svm->vcpu);
 +free_svm:
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.guest_fpu);
 +free_user_fpu:
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.user_fpu);
 +free_partial_svm:
 +	kmem_cache_free(kvm_vcpu_cache, svm);
 +out:
 +	return ERR_PTR(err);
++=======
+ out:
+ 	return err;
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  }
  
  static void svm_clear_current_vmcb(struct vmcb *vmcb)
@@@ -2327,10 -2278,6 +2339,13 @@@ static void svm_free_vcpu(struct kvm_vc
  	__free_pages(virt_to_page(svm->msrpm), MSRPM_ALLOC_ORDER);
  	__free_page(virt_to_page(svm->nested.hsave));
  	__free_pages(virt_to_page(svm->nested.msrpm), MSRPM_ALLOC_ORDER);
++<<<<<<< HEAD
 +	kvm_vcpu_uninit(vcpu);
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.user_fpu);
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.guest_fpu);
 +	kmem_cache_free(kvm_vcpu_cache, svm);
++=======
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  }
  
  static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
diff --cc arch/x86/kvm/vmx/vmx.c
index aace07a1fbd9,2134726b0442..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -6752,46 -6681,17 +6752,55 @@@ static void vmx_free_vcpu(struct kvm_vc
  	free_vpid(vmx->vpid);
  	nested_vmx_free_vcpu(vcpu);
  	free_loaded_vmcs(vmx->loaded_vmcs);
++<<<<<<< HEAD
 +	kvm_vcpu_uninit(vcpu);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.user_fpu);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
 +}
 +
 +static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
++=======
+ }
+ 
+ static int vmx_create_vcpu(struct kvm_vcpu *vcpu)
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  {
 +	int err;
  	struct vcpu_vmx *vmx;
  	unsigned long *msr_bitmap;
 -	int i, cpu, err;
 +	int cpu;
  
 -	BUILD_BUG_ON(offsetof(struct vcpu_vmx, vcpu) != 0);
 -	vmx = to_vmx(vcpu);
 +	BUILD_BUG_ON_MSG(offsetof(struct vcpu_vmx, vcpu) != 0,
 +		"struct kvm_vcpu must be at offset 0 for arch usercopy region");
 +
++<<<<<<< HEAD
 +	vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL_ACCOUNT);
 +	if (!vmx)
 +		return ERR_PTR(-ENOMEM);
 +
 +	vmx->vcpu.arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +			GFP_KERNEL_ACCOUNT);
 +	if (!vmx->vcpu.arch.user_fpu) {
 +		printk(KERN_ERR "kvm: failed to allocate kvm userspace's fpu\n");
 +		err = -ENOMEM;
 +		goto free_partial_vcpu;
 +	}
 +
 +	vmx->vcpu.arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +			GFP_KERNEL_ACCOUNT);
 +	if (!vmx->vcpu.arch.guest_fpu) {
 +		printk(KERN_ERR "kvm: failed to allocate vcpu's fpu\n");
 +		err = -ENOMEM;
 +		goto free_user_fpu;
 +	}
  
 +	err = kvm_vcpu_init(&vmx->vcpu, kvm, id);
 +	if (err)
 +		goto free_vcpu;
 +
++=======
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  	err = -ENOMEM;
  
  	vmx->vpid = allocate_vpid();
@@@ -6832,13 -6760,13 +6841,18 @@@
  
  	vmx->loaded_vmcs = &vmx->vmcs01;
  	cpu = get_cpu();
 -	vmx_vcpu_load(vcpu, cpu);
 -	vcpu->cpu = cpu;
 +	vmx_vcpu_load(&vmx->vcpu, cpu);
 +	vmx->vcpu.cpu = cpu;
  	init_vmcs(vmx);
 -	vmx_vcpu_put(vcpu);
 +	vmx_vcpu_put(&vmx->vcpu);
  	put_cpu();
++<<<<<<< HEAD
 +	if (cpu_need_virtualize_apic_accesses(&vmx->vcpu)) {
 +		err = alloc_apic_access_page(kvm);
++=======
+ 	if (cpu_need_virtualize_apic_accesses(vcpu)) {
+ 		err = alloc_apic_access_page(vcpu->kvm);
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  		if (err)
  			goto free_vmcs;
  	}
@@@ -6877,16 -6804,9 +6891,20 @@@ free_vmcs
  	free_loaded_vmcs(vmx->loaded_vmcs);
  free_pml:
  	vmx_destroy_pml_buffer(vmx);
++<<<<<<< HEAD
 +uninit_vcpu:
 +	kvm_vcpu_uninit(&vmx->vcpu);
++=======
+ free_vpid:
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  	free_vpid(vmx->vpid);
 -	return err;
 +free_vcpu:
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
 +free_user_fpu:
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.user_fpu);
 +free_partial_vcpu:
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
 +	return ERR_PTR(err);
  }
  
  #define L1TF_MSG_SMT "L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n"
diff --cc arch/x86/kvm/x86.c
index 28c9e8821b43,51292843afcb..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -9155,7 -9175,13 +9155,17 @@@ void kvm_arch_vcpu_free(struct kvm_vcp
  	kvmclock_reset(vcpu);
  
  	kvm_x86_ops->vcpu_free(vcpu);
++<<<<<<< HEAD
 +	free_cpumask_var(wbinvd_dirty_mask);
++=======
+ 
+ 	kvm_vcpu_uninit(vcpu);
+ 
+ 	free_cpumask_var(vcpu->arch.wbinvd_dirty_mask);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
+ 	kmem_cache_free(kvm_vcpu_cache, vcpu);
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  }
  
  struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
@@@ -9168,9 -9195,24 +9178,25 @@@
  		"kvm: SMP vm created on host with unstable TSC; "
  		"guest TSC will not be reliable\n");
  
 -	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL_ACCOUNT);
 -	if (!vcpu)
 -		return ERR_PTR(-ENOMEM);
 +	vcpu = kvm_x86_ops->vcpu_create(kvm, id);
  
++<<<<<<< HEAD
++=======
+ 	r = kvm_vcpu_init(vcpu, kvm, id);
+ 	if (r)
+ 		goto free_vcpu;
+ 
+ 	r = kvm_x86_ops->vcpu_create(vcpu);
+ 	if (r)
+ 		goto uninit_vcpu;
++>>>>>>> 987b2594ed5d (KVM: x86: Move kvm_vcpu_init() invocation to common code)
  	return vcpu;
+ 
+ uninit_vcpu:
+ 	kvm_vcpu_uninit(vcpu);
+ free_vcpu:
+ 	kmem_cache_free(kvm_vcpu_cache, vcpu);
+ 	return ERR_PTR(r);
  }
  
  int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/x86.c
