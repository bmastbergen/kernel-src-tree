RDMA/mlx5: Split implicit handling from pagefault_mr

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 54375e7382952daded7002d1618eadaae859cecb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/54375e73.failed

The single routine has a very confusing scheme to advance to the next
child MR when working on an implicit parent. This scheme can only be used
when working with an implicit parent and must not be triggered when
working on a normal MR.

Re-arrange things by directly putting all the single-MR stuff into one
function and calling it in a loop for the implicit case. Simplify some of
the error handling in the new pagefault_real_mr() to remove unneeded gotos.

Link: https://lore.kernel.org/r/20191009160934.3143-9-jgg@ziepe.ca
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 54375e7382952daded7002d1618eadaae859cecb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/odp.c
index df3038ca913b,8f43af6580ce..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -612,47 -628,21 +612,58 @@@ void mlx5_ib_free_implicit_mr(struct ml
  	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
  }
  
 +#define MLX5_PF_FLAGS_PREFETCH  BIT(0)
  #define MLX5_PF_FLAGS_DOWNGRADE BIT(1)
++<<<<<<< HEAD
 +static int pagefault_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
 +			u64 io_virt, size_t bcnt, u32 *bytes_mapped,
 +			u32 flags)
 +{
 +	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
++=======
+ static int pagefault_real_mr(struct mlx5_ib_mr *mr, struct ib_umem_odp *odp,
+ 			     u64 user_va, size_t bcnt, u32 *bytes_mapped,
+ 			     u32 flags)
+ {
+ 	int current_seq, page_shift, ret, np;
++>>>>>>> 54375e738295 (RDMA/mlx5: Split implicit handling from pagefault_mr)
  	bool downgrade = flags & MLX5_PF_FLAGS_DOWNGRADE;
 +	bool prefetch = flags & MLX5_PF_FLAGS_PREFETCH;
 +	int npages = 0, current_seq, page_shift, ret, np;
  	u64 access_mask;
  	u64 start_idx, page_mask;
++<<<<<<< HEAD
 +	struct ib_umem_odp *odp;
 +	size_t size;
 +
 +	if (!odp_mr->page_list) {
 +		odp = implicit_mr_get_data(mr, io_virt, bcnt);
 +
 +		if (IS_ERR(odp))
 +			return PTR_ERR(odp);
 +		mr = odp->private;
 +	} else {
 +		odp = odp_mr;
 +	}
 +
 +next_mr:
 +	size = min_t(size_t, bcnt, ib_umem_end(odp) - io_virt);
++=======
++>>>>>>> 54375e738295 (RDMA/mlx5: Split implicit handling from pagefault_mr)
  
  	page_shift = odp->page_shift;
  	page_mask = ~(BIT(page_shift) - 1);
- 	start_idx = (io_virt - (mr->mmkey.iova & page_mask)) >> page_shift;
+ 	start_idx = (user_va - (mr->mmkey.iova & page_mask)) >> page_shift;
  	access_mask = ODP_READ_ALLOWED_BIT;
  
 +	if (prefetch && !downgrade && !odp->umem.writable) {
 +		/* prefetch with write-access must
 +		 * be supported by the MR
 +		 */
 +		ret = -EINVAL;
 +		goto out;
 +	}
 +
  	if (odp->umem.writable && !downgrade)
  		access_mask |= ODP_WRITE_ALLOWED_BIT;
  
@@@ -693,29 -681,12 +701,34 @@@
  
  	if (bytes_mapped) {
  		u32 new_mappings = (np << page_shift) -
- 			(io_virt - round_down(io_virt, 1 << page_shift));
- 		*bytes_mapped += min_t(u32, new_mappings, size);
+ 			(user_va - round_down(user_va, 1 << page_shift));
+ 
+ 		*bytes_mapped += min_t(u32, new_mappings, bcnt);
  	}
  
++<<<<<<< HEAD
 +	npages += np << (page_shift - PAGE_SHIFT);
 +	bcnt -= size;
 +
 +	if (unlikely(bcnt)) {
 +		struct ib_umem_odp *next;
 +
 +		io_virt += size;
 +		next = odp_next(odp);
 +		if (unlikely(!next || ib_umem_start(next) != io_virt)) {
 +			mlx5_ib_dbg(dev, "next implicit leaf removed at 0x%llx. got %p\n",
 +				    io_virt, next);
 +			return -EAGAIN;
 +		}
 +		odp = next;
 +		mr = odp->private;
 +		goto next_mr;
 +	}
 +
 +	return npages;
++=======
+ 	return np << (page_shift - PAGE_SHIFT);
++>>>>>>> 54375e738295 (RDMA/mlx5: Split implicit handling from pagefault_mr)
  
  out:
  	if (ret == -EAGAIN) {
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
