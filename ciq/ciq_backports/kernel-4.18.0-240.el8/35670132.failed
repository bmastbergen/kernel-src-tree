atomics/treewide: Make atomic64_fetch_add_unless() optional

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit 356701329fb391184618eda7b7fb68cb35271506
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/35670132.failed

Architectures with atomic64_fetch_add_unless() provide a preprocessor
symbol if they do so, and all other architectures have trivial C
implementations of atomic64_add_unless() which are near-identical.

Let's unify the trivial definitions of atomic64_fetch_add_unless() in
<linux/atomic.h>, so that we always have both
atomic64_fetch_add_unless() and atomic64_add_unless() with less
boilerplate code.

This means that atomic64_add_unless() is always implemented in core
code, and the instrumented atomics are updated accordingly.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Reviewed-by: Will Deacon <will.deacon@arm.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Boqun Feng <boqun.feng@gmail.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: https://lore.kernel.org/lkml/20180621121321.4761-15-mark.rutland@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 356701329fb391184618eda7b7fb68cb35271506)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/ia64/include/asm/atomic.h
#	arch/mips/include/asm/atomic.h
#	arch/parisc/include/asm/atomic.h
#	arch/sparc/include/asm/atomic_64.h
#	arch/x86/include/asm/atomic64_64.h
#	include/asm-generic/atomic-instrumented.h
#	include/linux/atomic.h
diff --cc arch/ia64/include/asm/atomic.h
index 2524fb60fbc2,0f80a3eafaba..000000000000
--- a/arch/ia64/include/asm/atomic.h
+++ b/arch/ia64/include/asm/atomic.h
@@@ -215,39 -215,6 +215,42 @@@ ATOMIC64_FETCH_OP(xor, ^
  	(cmpxchg(&((v)->counter), old, new))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
 +
 +static __inline__ long atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	long c, old;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic64_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != (u);
 +}
 +
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
++=======
++>>>>>>> 356701329fb3 (atomics/treewide: Make atomic64_fetch_add_unless() optional)
  static __inline__ long atomic64_dec_if_positive(atomic64_t *v)
  {
  	long c, old, dec;
diff --cc arch/mips/include/asm/atomic.h
index 0ab176bdb8e8,d42b27df1548..000000000000
--- a/arch/mips/include/asm/atomic.h
+++ b/arch/mips/include/asm/atomic.h
@@@ -620,32 -596,6 +620,35 @@@ static __inline__ long atomic64_sub_if_
  	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), (new)))
  
++<<<<<<< HEAD
 +/**
 + * atomic64_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic64_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns true iff @v was not @u.
 + */
 +static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	long c, old;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic64_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != (u);
 +}
 +
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
++=======
++>>>>>>> 356701329fb3 (atomics/treewide: Make atomic64_fetch_add_unless() optional)
  #define atomic64_dec_return(v) atomic64_sub_return(1, (v))
  #define atomic64_inc_return(v) atomic64_add_return(1, (v))
  
diff --cc arch/parisc/include/asm/atomic.h
index 88bae6676c9b,f53ba2d6ff67..000000000000
--- a/arch/parisc/include/asm/atomic.h
+++ b/arch/parisc/include/asm/atomic.h
@@@ -281,32 -257,6 +281,35 @@@ atomic64_read(const atomic64_t *v
  	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +/**
 + * atomic64_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic64_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	long c, old;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic64_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != (u);
 +}
 +
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
++=======
++>>>>>>> 356701329fb3 (atomics/treewide: Make atomic64_fetch_add_unless() optional)
  /*
   * atomic64_dec_if_positive - decrement by 1 if old value positive
   * @v: pointer of type atomic_t
diff --cc arch/sparc/include/asm/atomic_64.h
index 28db058d471b,458783e99997..000000000000
--- a/arch/sparc/include/asm/atomic_64.h
+++ b/arch/sparc/include/asm/atomic_64.h
@@@ -108,23 -93,6 +108,26 @@@ static inline int __atomic_add_unless(a
  	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +static inline long atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	long c, old;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic64_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != (u);
 +}
 +
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
++=======
++>>>>>>> 356701329fb3 (atomics/treewide: Make atomic64_fetch_add_unless() optional)
  long atomic64_dec_if_positive(atomic64_t *v);
  
  #endif /* !(__ARCH_SPARC64_ATOMIC__) */
diff --cc arch/x86/include/asm/atomic64_64.h
index c99f33271b13,7e04b294e6eb..000000000000
--- a/arch/x86/include/asm/atomic64_64.h
+++ b/arch/x86/include/asm/atomic64_64.h
@@@ -188,27 -188,6 +188,30 @@@ static inline long arch_atomic64_xchg(a
  	return xchg(&v->counter, new);
  }
  
++<<<<<<< HEAD
 +/**
 + * arch_atomic64_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic64_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static inline bool arch_atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	s64 c = arch_atomic64_read(v);
 +	do {
 +		if (unlikely(c == u))
 +			return false;
 +	} while (!arch_atomic64_try_cmpxchg(v, &c, c + a));
 +	return true;
 +}
 +
 +#define arch_atomic64_inc_not_zero(v) arch_atomic64_add_unless((v), 1, 0)
 +
++=======
++>>>>>>> 356701329fb3 (atomics/treewide: Make atomic64_fetch_add_unless() optional)
  /*
   * arch_atomic64_dec_if_positive - decrement by 1 if old value positive
   * @v: pointer of type atomic_t
diff --cc include/asm-generic/atomic-instrumented.h
index cfee349ddd5a,2b487f28ef35..000000000000
--- a/include/asm-generic/atomic-instrumented.h
+++ b/include/asm-generic/atomic-instrumented.h
@@@ -84,18 -84,23 +84,29 @@@ static __always_inline bool atomic64_tr
  }
  #endif
  
 -#ifdef arch_atomic_fetch_add_unless
 -#define atomic_fetch_add_unless atomic_fetch_add_unless
 -static __always_inline int atomic_fetch_add_unless(atomic_t *v, int a, int u)
 +static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)
  {
  	kasan_check_write(v, sizeof(*v));
 -	return arch_atomic_fetch_add_unless(v, a, u);
 +	return __arch_atomic_add_unless(v, a, u);
  }
 -#endif
  
++<<<<<<< HEAD
 +
 +static __always_inline bool atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
 +{
 +	kasan_check_write(v, sizeof(*v));
 +	return arch_atomic64_add_unless(v, a, u);
 +}
++=======
+ #ifdef arch_atomic64_fetch_add_unless
+ #define atomic64_fetch_add_unless atomic64_fetch_add_unless
+ static __always_inline s64 atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
+ {
+ 	kasan_check_write(v, sizeof(*v));
+ 	return arch_atomic64_fetch_add_unless(v, a, u);
+ }
+ #endif
++>>>>>>> 356701329fb3 (atomics/treewide: Make atomic64_fetch_add_unless() optional)
  
  static __always_inline void atomic_inc(atomic_t *v)
  {
diff --cc include/linux/atomic.h
index 6ebab115d8ad,530562ac7909..000000000000
--- a/include/linux/atomic.h
+++ b/include/linux/atomic.h
@@@ -1019,6 -1042,55 +1019,58 @@@ static inline int atomic_dec_if_positiv
  #define atomic64_try_cmpxchg_release	atomic64_try_cmpxchg
  #endif /* atomic64_try_cmpxchg */
  
++<<<<<<< HEAD
++=======
+ /**
+  * atomic64_fetch_add_unless - add unless the number is already a given value
+  * @v: pointer of type atomic64_t
+  * @a: the amount to add to v...
+  * @u: ...unless v is equal to u.
+  *
+  * Atomically adds @a to @v, if @v was not already @u.
+  * Returns the original value of @v.
+  */
+ #ifndef atomic64_fetch_add_unless
+ static inline long long atomic64_fetch_add_unless(atomic64_t *v, long long a,
+ 						  long long u)
+ {
+ 	long long c = atomic64_read(v);
+ 
+ 	do {
+ 		if (unlikely(c == u))
+ 			break;
+ 	} while (!atomic64_try_cmpxchg(v, &c, c + a));
+ 
+ 	return c;
+ }
+ #endif
+ 
+ /**
+  * atomic64_add_unless - add unless the number is already a given value
+  * @v: pointer of type atomic_t
+  * @a: the amount to add to v...
+  * @u: ...unless v is equal to u.
+  *
+  * Atomically adds @a to @v, if @v was not already @u.
+  * Returns true if the addition was done.
+  */
+ static inline bool atomic64_add_unless(atomic64_t *v, long long a, long long u)
+ {
+ 	return atomic64_fetch_add_unless(v, a, u) != u;
+ }
+ 
+ /**
+  * atomic64_inc_not_zero - increment unless the number is zero
+  * @v: pointer of type atomic64_t
+  *
+  * Atomically increments @v by 1, if @v is non-zero.
+  * Returns true if the increment was done.
+  */
+ #ifndef atomic64_inc_not_zero
+ #define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1, 0)
+ #endif
+ 
++>>>>>>> 356701329fb3 (atomics/treewide: Make atomic64_fetch_add_unless() optional)
  #ifndef atomic64_andnot
  static inline void atomic64_andnot(long long i, atomic64_t *v)
  {
diff --git a/arch/arm64/include/asm/atomic.h b/arch/arm64/include/asm/atomic.h
index c0235e0ff849..f3d064e617d6 100644
--- a/arch/arm64/include/asm/atomic.h
+++ b/arch/arm64/include/asm/atomic.h
@@ -40,17 +40,6 @@
 
 #include <asm/cmpxchg.h>
 
-#define ___atomic_add_unless(v, a, u, sfx)				\
-({									\
-	typeof((v)->counter) c, old;					\
-									\
-	c = atomic##sfx##_read(v);					\
-	while (c != (u) &&						\
-	      (old = atomic##sfx##_cmpxchg((v), c, c + (a))) != c)	\
-		c = old;						\
-	c;								\
- })
-
 #define ATOMIC_INIT(i)	{ (i) }
 
 #define atomic_read(v)			READ_ONCE((v)->counter)
@@ -201,7 +190,6 @@
 #define atomic64_dec_and_test(v)	(atomic64_dec_return(v) == 0)
 #define atomic64_sub_and_test(i, v)	(atomic64_sub_return((i), (v)) == 0)
 #define atomic64_add_negative(i, v)	(atomic64_add_return((i), (v)) < 0)
-#define atomic64_add_unless(v, a, u)	(___atomic_add_unless(v, a, u, 64) != u)
 #define atomic64_andnot			atomic64_andnot
 
 #define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1, 0)
* Unmerged path arch/ia64/include/asm/atomic.h
* Unmerged path arch/mips/include/asm/atomic.h
* Unmerged path arch/parisc/include/asm/atomic.h
diff --git a/arch/s390/include/asm/atomic.h b/arch/s390/include/asm/atomic.h
index 4b55532f15c4..0547a6d41d28 100644
--- a/arch/s390/include/asm/atomic.h
+++ b/arch/s390/include/asm/atomic.h
@@ -168,22 +168,6 @@ ATOMIC64_OPS(xor)
 
 #undef ATOMIC64_OPS
 
-static inline int atomic64_add_unless(atomic64_t *v, long i, long u)
-{
-	long c, old;
-
-	c = atomic64_read(v);
-	for (;;) {
-		if (unlikely(c == u))
-			break;
-		old = atomic64_cmpxchg(v, c, c + i);
-		if (likely(old == c))
-			break;
-		c = old;
-	}
-	return c != u;
-}
-
 static inline long atomic64_dec_if_positive(atomic64_t *v)
 {
 	long c, old, dec;
* Unmerged path arch/sparc/include/asm/atomic_64.h
* Unmerged path arch/x86/include/asm/atomic64_64.h
* Unmerged path include/asm-generic/atomic-instrumented.h
* Unmerged path include/linux/atomic.h
