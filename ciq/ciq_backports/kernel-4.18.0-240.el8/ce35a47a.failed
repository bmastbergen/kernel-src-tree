io_uring: add IOSQE_ASYNC

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit ce35a47a3a0208a77b4d31b7f2e8ed57d624093d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ce35a47a.failed

io_uring defaults to always doing inline submissions, if at all
possible. But for larger copies, even if the data is fully cached, that
can take a long time. Add an IOSQE_ASYNC flag that the application can
set on the SQE - if set, it'll ensure that we always go async for those
kinds of requests. Use the io-wq IO_WQ_WORK_CONCURRENT flag to ensure we
get the concurrency we desire for this case.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ce35a47a3a0208a77b4d31b7f2e8ed57d624093d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index eb3b77d5111e,9ffcfdc6382b..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -334,9 -472,18 +334,21 @@@ struct io_kiocb 
  #define REQ_F_IO_DRAIN		16	/* drain existing IO first */
  #define REQ_F_IO_DRAINED	32	/* drain done */
  #define REQ_F_LINK		64	/* linked sqes */
 -#define REQ_F_LINK_TIMEOUT	128	/* has linked timeout */
 +#define REQ_F_LINK_DONE		128	/* linked sqes done */
  #define REQ_F_FAIL_LINK		256	/* fail rest of links */
++<<<<<<< HEAD
 +#define REQ_F_SHADOW_DRAIN	512	/* link-drain shadow req */
++=======
+ #define REQ_F_DRAIN_LINK	512	/* link should be fully drained */
+ #define REQ_F_TIMEOUT		1024	/* timeout request */
+ #define REQ_F_ISREG		2048	/* regular file */
+ #define REQ_F_MUST_PUNT		4096	/* must be punted even for NONBLOCK */
+ #define REQ_F_TIMEOUT_NOSEQ	8192	/* no timeout sequence */
+ #define REQ_F_INFLIGHT		16384	/* on inflight list */
+ #define REQ_F_COMP_LOCKED	32768	/* completion under lock */
+ #define REQ_F_HARDLINK		65536	/* doesn't sever on completion < 0 */
+ #define REQ_F_FORCE_ASYNC	131072	/* IOSQE_ASYNC */
++>>>>>>> ce35a47a3a02 (io_uring: add IOSQE_ASYNC)
  	u64			user_data;
  	u32			result;
  	u32			sequence;
@@@ -2228,94 -4005,66 +2240,115 @@@ static int io_queue_sqe(struct io_ring_
  {
  	int ret;
  
 -	if (unlikely(req->ctx->drain_next)) {
 -		req->flags |= REQ_F_IO_DRAIN;
 -		req->ctx->drain_next = false;
 -	}
 -	req->ctx->drain_next = (req->flags & REQ_F_DRAIN_LINK);
 -
 -	ret = io_req_defer(req, sqe);
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -			io_cqring_add_event(req, ret);
 -			req_set_fail_links(req);
 -			io_double_put_req(req);
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
  		}
++<<<<<<< HEAD
 +		return 0;
 +	}
 +
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
++=======
+ 	} else if ((req->flags & REQ_F_FORCE_ASYNC) &&
+ 		   !io_wq_current_is_worker()) {
+ 		/*
+ 		 * Never try inline submit of IOSQE_ASYNC is set, go straight
+ 		 * to async execution.
+ 		 */
+ 		req->work.flags |= IO_WQ_WORK_CONCURRENT;
+ 		io_queue_async_work(req);
+ 	} else {
+ 		__io_queue_sqe(req, sqe);
+ 	}
++>>>>>>> ce35a47a3a02 (io_uring: add IOSQE_ASYNC)
  }
  
 -static inline void io_queue_link_head(struct io_kiocb *req)
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
  {
 -	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_double_put_req(req);
 -	} else
 -		io_queue_sqe(req, NULL);
 +	int ret;
 +	int need_submit = false;
 +
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	/*
 +	 * Mark the first IO in link list as DRAIN, let all the following
 +	 * IOs enter the defer list. all IO needs to be completed before link
 +	 * list.
 +	 */
 +	req->flags |= REQ_F_IO_DRAIN;
 +	ret = io_req_defer(ctx, req, s->sqe);
 +	if (ret) {
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
 +		}
 +	} else {
 +		/*
 +		 * If ret == 0 means that all IOs in front of link io are
 +		 * running done. let's queue link head.
 +		 */
 +		need_submit = true;
 +	}
 +
 +	/* Insert shadow req to defer_list, blocking next IOs */
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_add_tail(&shadow->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	if (need_submit)
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	return 0;
  }
  
++<<<<<<< HEAD
 +#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
++=======
+ #define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|	\
+ 				IOSQE_IO_HARDLINK | IOSQE_ASYNC)
++>>>>>>> ce35a47a3a02 (io_uring: add IOSQE_ASYNC)
  
 -static bool io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			  struct io_submit_state *state, struct io_kiocb **link)
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
  	int ret;
  
  	/* enforce forwards compatibility on users */
 -	if (unlikely(sqe->flags & ~SQE_VALID_FLAGS)) {
 +	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
  		ret = -EINVAL;
 -		goto err_req;
 +		goto err;
  	}
+ 	if (sqe->flags & IOSQE_ASYNC)
+ 		req->flags |= REQ_F_FORCE_ASYNC;
  
 -	ret = io_req_set_file(state, req, sqe);
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
 +	}
 +
 +	ret = io_req_set_file(ctx, s, state, req);
  	if (unlikely(ret)) {
  err_req:
 -		io_cqring_add_event(req, ret);
 -		io_double_put_req(req);
 -		return false;
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		return;
  	}
  
 +	req->user_data = s->sqe->user_data;
 +
  	/*
  	 * If we already have a head request, queue this one for async
  	 * submittal once the head completes. If we don't have a head but
diff --cc include/uapi/linux/io_uring.h
index 22b1c5919fbd,d7ec50247a3a..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -42,6 -50,8 +42,11 @@@ struct io_uring_sqe 
  #define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
  #define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
  #define IOSQE_IO_LINK		(1U << 2)	/* links next sqe */
++<<<<<<< HEAD
++=======
+ #define IOSQE_IO_HARDLINK	(1U << 3)	/* like LINK, but stronger */
+ #define IOSQE_ASYNC		(1U << 4)	/* always go async */
++>>>>>>> ce35a47a3a02 (io_uring: add IOSQE_ASYNC)
  
  /*
   * io_uring_setup() flags
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
