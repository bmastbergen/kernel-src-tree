io_uring: move all prep state for IORING_OP_{SEND,RECV}_MGS to prep handler

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [fs] io_uring: move all prep state for IORING_OP_{SEND, RECV}_MGS to prep handler (Jeff Moyer) [1784478]
Rebuild_FUZZ: 99.34%
commit-author Jens Axboe <axboe@kernel.dk>
commit e47293fdf98998292a89d516c8f7b8b9eb5c5213
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e47293fd.failed

Add struct io_sr_msg in our io_kiocb per-command union, and ensure that
the send/recvmsg prep handlers have grabbed what they need from the SQE
by the time prep is done.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit e47293fdf98998292a89d516c8f7b8b9eb5c5213)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index ab99aea677bc,89e5b19044cc..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -308,6 -299,86 +308,89 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	struct user_msghdr __user	*msg;
+ 	int				msg_flags;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	struct io_uring_sqe		sqe;
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
+ };
+ 
++>>>>>>> e47293fdf989 (io_uring: move all prep state for IORING_OP_{SEND,RECV}_MGS to prep handler)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -317,14 -388,30 +400,23 @@@
  struct io_kiocb {
  	union {
  		struct file		*file;
 -		struct io_rw		rw;
 +		struct kiocb		rw;
  		struct io_poll_iocb	poll;
++<<<<<<< HEAD
++=======
+ 		struct io_accept	accept;
+ 		struct io_sync		sync;
+ 		struct io_cancel	cancel;
+ 		struct io_timeout	timeout;
+ 		struct io_connect	connect;
+ 		struct io_sr_msg	sr_msg;
++>>>>>>> e47293fdf989 (io_uring: move all prep state for IORING_OP_{SEND,RECV}_MGS to prep handler)
  	};
  
 -	const struct io_uring_sqe	*sqe;
 -	struct io_async_ctx		*io;
 -	struct file			*ring_file;
 -	int				ring_fd;
 -	bool				has_user;
 -	bool				in_async;
 -	bool				needs_fixed_file;
 -	u8				opcode;
 +	struct sqe_submit	submit;
  
  	struct io_ring_ctx	*ctx;
 -	union {
 -		struct list_head	list;
 -		struct hlist_node	hash_node;
 -	};
 +	struct list_head	list;
  	struct list_head	link_list;
  	unsigned int		flags;
  	refcount_t		refs;
@@@ -1578,11 -2155,119 +1670,125 @@@ static int io_sync_file_range(struct io
  }
  
  #if defined(CONFIG_NET)
 -static void io_sendrecv_async(struct io_wq_work **workptr)
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
  {
++<<<<<<< HEAD
++=======
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct iovec *iov = NULL;
+ 
+ 	if (req->io->rw.iov != req->io->rw.fast_iov)
+ 		iov = req->io->msg.iov;
+ 	io_wq_submit_work(workptr);
+ 	kfree(iov);
+ }
+ #endif
+ 
+ static int io_sendmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct io_sr_msg *sr = &req->sr_msg;
+ 
+ 	sr->msg_flags = READ_ONCE(sqe->msg_flags);
+ 	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	io->msg.iov = io->msg.fast_iov;
+ 	return sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
+ 					&io->msg.iov);
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_sendmsg(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_async_msghdr *kmsg = NULL;
+ 	struct socket *sock;
+ 	int ret;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_async_ctx io;
+ 		struct sockaddr_storage addr;
+ 		unsigned flags;
+ 
+ 		if (req->io) {
+ 			kmsg = &req->io->msg;
+ 			kmsg->msg.msg_name = &addr;
+ 			/* if iov is set, it's allocated already */
+ 			if (!kmsg->iov)
+ 				kmsg->iov = kmsg->fast_iov;
+ 			kmsg->msg.msg_iter.iov = kmsg->iov;
+ 		} else {
+ 			kmsg = &io.msg;
+ 			kmsg->msg.msg_name = &addr;
+ 			ret = io_sendmsg_prep(req, &io);
+ 			if (ret)
+ 				goto out;
+ 		}
+ 
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
+ 		ret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);
+ 		if (force_nonblock && ret == -EAGAIN) {
+ 			if (req->io)
+ 				return -EAGAIN;
+ 			if (io_alloc_async_ctx(req))
+ 				return -ENOMEM;
+ 			memcpy(&req->io->msg, &io.msg, sizeof(io.msg));
+ 			req->work.func = io_sendrecv_async;
+ 			return -EAGAIN;
+ 		}
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
+ 	}
+ 
+ out:
+ 	if (!io_wq_current_is_worker() && kmsg && kmsg->iov != kmsg->fast_iov)
+ 		kfree(kmsg->iov);
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_recvmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_sr_msg *sr = &req->sr_msg;
+ 
+ 	sr->msg_flags = READ_ONCE(req->sqe->msg_flags);
+ 	sr->msg = u64_to_user_ptr(READ_ONCE(req->sqe->addr));
+ 	io->msg.iov = io->msg.fast_iov;
+ 	return recvmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
+ 					&io->msg.uaddr, &io->msg.iov);
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_recvmsg(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_async_msghdr *kmsg = NULL;
++>>>>>>> e47293fdf989 (io_uring: move all prep state for IORING_OP_{SEND,RECV}_MGS to prep handler)
  	struct socket *sock;
  	int ret;
  
@@@ -1591,21 -2276,42 +1797,60 @@@
  
  	sock = sock_from_file(req->file, &ret);
  	if (sock) {
 -		struct io_async_ctx io;
 -		struct sockaddr_storage addr;
++<<<<<<< HEAD
 +		struct user_msghdr __user *msg;
  		unsigned flags;
  
 -		if (req->io) {
 +		flags = READ_ONCE(sqe->msg_flags);
 +		if (flags & MSG_DONTWAIT)
 +			req->flags |= REQ_F_NOWAIT;
 +		else if (force_nonblock)
 +			flags |= MSG_DONTWAIT;
 +
 +		msg = (struct user_msghdr __user *) (unsigned long)
 +			READ_ONCE(sqe->addr);
 +
 +		ret = fn(sock, msg, flags);
 +		if (force_nonblock && ret == -EAGAIN)
 +			return ret;
++=======
++		struct io_async_ctx io;
++		struct sockaddr_storage addr;
++		unsigned flags;
++
++		if (req->io) {
+ 			kmsg = &req->io->msg;
+ 			kmsg->msg.msg_name = &addr;
+ 			/* if iov is set, it's allocated already */
+ 			if (!kmsg->iov)
+ 				kmsg->iov = kmsg->fast_iov;
+ 			kmsg->msg.msg_iter.iov = kmsg->iov;
+ 		} else {
+ 			kmsg = &io.msg;
+ 			kmsg->msg.msg_name = &addr;
+ 			ret = io_recvmsg_prep(req, &io);
+ 			if (ret)
+ 				goto out;
+ 		}
+ 
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
+ 		ret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.msg,
+ 						kmsg->uaddr, flags);
+ 		if (force_nonblock && ret == -EAGAIN) {
+ 			if (req->io)
+ 				return -EAGAIN;
+ 			if (io_alloc_async_ctx(req))
+ 				return -ENOMEM;
+ 			memcpy(&req->io->msg, &io.msg, sizeof(io.msg));
+ 			req->work.func = io_sendrecv_async;
+ 			return -EAGAIN;
+ 		}
++>>>>>>> e47293fdf989 (io_uring: move all prep state for IORING_OP_{SEND,RECV}_MGS to prep handler)
  		if (ret == -ERESTARTSYS)
  			ret = -EINTR;
  	}
* Unmerged path fs/io_uring.c
