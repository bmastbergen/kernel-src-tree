futex: Prevent robust futex exit race

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Yang Tao <yang.tao172@zte.com.cn>
commit ca16d5bee59807bf04deaab0a8eccecd5061528c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ca16d5be.failed

Robust futexes utilize the robust_list mechanism to allow the kernel to
release futexes which are held when a task exits. The exit can be voluntary
or caused by a signal or fault. This prevents that waiters block forever.

The futex operations in user space store a pointer to the futex they are
either locking or unlocking in the op_pending member of the per task robust
list.

After a lock operation has succeeded the futex is queued in the robust list
linked list and the op_pending pointer is cleared.

After an unlock operation has succeeded the futex is removed from the
robust list linked list and the op_pending pointer is cleared.

The robust list exit code checks for the pending operation and any futex
which is queued in the linked list. It carefully checks whether the futex
value is the TID of the exiting task. If so, it sets the OWNER_DIED bit and
tries to wake up a potential waiter.

This is race free for the lock operation but unlock has two race scenarios
where waiters might not be woken up. These issues can be observed with
regular robust pthread mutexes. PI aware pthread mutexes are not affected.

(1) Unlocking task is killed after unlocking the futex value in user space
    before being able to wake a waiter.

        pthread_mutex_unlock()
                |
                V
        atomic_exchange_rel (&mutex->__data.__lock, 0)
                        <------------------------killed
            lll_futex_wake ()                   |
                                                |
                                                |(__lock = 0)
                                                |(enter kernel)
                                                |
                                                V
                                            do_exit()
                                            exit_mm()
                                          mm_release()
                                        exit_robust_list()
                                        handle_futex_death()
                                                |
                                                |(__lock = 0)
                                                |(uval = 0)
                                                |
                                                V
        if ((uval & FUTEX_TID_MASK) != task_pid_vnr(curr))
                return 0;

    The sanity check which ensures that the user space futex is owned by
    the exiting task prevents the wakeup of waiters which in consequence
    block infinitely.

(2) Waiting task is killed after a wakeup and before it can acquire the
    futex in user space.

        OWNER                         WAITER
				futex_wait()      		
   pthread_mutex_unlock()               |
                |                       |
                |(__lock = 0)           |
                |                       |
                V                       |
         futex_wake() ------------>  wakeup()
                                        |
                                        |(return to userspace)
                                        |(__lock = 0)
                                        |
                                        V
                        oldval = mutex->__data.__lock
                                          <-----------------killed
    atomic_compare_and_exchange_val_acq (&mutex->__data.__lock,  |
                        id | assume_other_futex_waiters, 0)      |
                                                                 |
                                                                 |
                                                   (enter kernel)|
                                                                 |
                                                                 V
                                                         do_exit()
                                                        |
                                                        |
                                                        V
                                        handle_futex_death()
                                        |
                                        |(__lock = 0)
                                        |(uval = 0)
                                        |
                                        V
        if ((uval & FUTEX_TID_MASK) != task_pid_vnr(curr))
                return 0;

    The sanity check which ensures that the user space futex is owned
    by the exiting task prevents the wakeup of waiters, which seems to
    be correct as the exiting task does not own the futex value, but
    the consequence is that other waiters wont be woken up and block
    infinitely.

In both scenarios the following conditions are true:

   - task->robust_list->list_op_pending != NULL
   - user space futex value == 0
   - Regular futex (not PI)

If these conditions are met then it is reasonably safe to wake up a
potential waiter in order to prevent the above problems.

As this might be a false positive it can cause spurious wakeups, but the
waiter side has to handle other types of unrelated wakeups, e.g. signals
gracefully anyway. So such a spurious wakeup will not affect the
correctness of these operations.

This workaround must not touch the user space futex value and cannot set
the OWNER_DIED bit because the lock value is 0, i.e. uncontended. Setting
OWNER_DIED in this case would result in inconsistent state and subsequently
in malfunction of the owner died handling in user space.

The rest of the user space state is still consistent as no other task can
observe the list_op_pending entry in the exiting tasks robust list.

The eventually woken up waiter will observe the uncontended lock value and
take it over.

[ tglx: Massaged changelog and comment. Made the return explicit and not
  	depend on the subsequent check and added constants to hand into
  	handle_futex_death() instead of plain numbers. Fixed a few coding
	style issues. ]

Fixes: 0771dfefc9e5 ("[PATCH] lightweight robust futexes: core")
	Signed-off-by: Yang Tao <yang.tao172@zte.com.cn>
	Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Ingo Molnar <mingo@kernel.org>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: stable@vger.kernel.org
Link: https://lkml.kernel.org/r/1573010582-35297-1-git-send-email-wang.yi59@zte.com.cn
Link: https://lkml.kernel.org/r/20191106224555.943191378@linutronix.de

(cherry picked from commit ca16d5bee59807bf04deaab0a8eccecd5061528c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/futex.c
diff --cc kernel/futex.c
index ec3cc9521f31,49eaf5be851a..000000000000
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@@ -3459,7 -3460,8 +3463,12 @@@ err_unlock
   * Process a futex-list entry, check whether it's owned by the
   * dying task, and do notification if so:
   */
++<<<<<<< HEAD
 +int handle_futex_death(u32 __user *uaddr, struct task_struct *curr, int pi)
++=======
+ static int handle_futex_death(u32 __user *uaddr, struct task_struct *curr,
+ 			      bool pi, bool pending_op)
++>>>>>>> ca16d5bee598 (futex: Prevent robust futex exit race)
  {
  	u32 uval, uninitialized_var(nval), mval;
  	int err;
@@@ -3708,6 -3748,195 +3755,198 @@@ SYSCALL_DEFINE6(futex, u32 __user *, ua
  	return do_futex(uaddr, op, val, tp, uaddr2, val2, val3);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_COMPAT
+ /*
+  * Fetch a robust-list pointer. Bit 0 signals PI futexes:
+  */
+ static inline int
+ compat_fetch_robust_entry(compat_uptr_t *uentry, struct robust_list __user **entry,
+ 		   compat_uptr_t __user *head, unsigned int *pi)
+ {
+ 	if (get_user(*uentry, head))
+ 		return -EFAULT;
+ 
+ 	*entry = compat_ptr((*uentry) & ~1);
+ 	*pi = (unsigned int)(*uentry) & 1;
+ 
+ 	return 0;
+ }
+ 
+ static void __user *futex_uaddr(struct robust_list __user *entry,
+ 				compat_long_t futex_offset)
+ {
+ 	compat_uptr_t base = ptr_to_compat(entry);
+ 	void __user *uaddr = compat_ptr(base + futex_offset);
+ 
+ 	return uaddr;
+ }
+ 
+ /*
+  * Walk curr->robust_list (very carefully, it's a userspace list!)
+  * and mark any locks found there dead, and notify any waiters.
+  *
+  * We silently return on any sign of list-walking problem.
+  */
+ void compat_exit_robust_list(struct task_struct *curr)
+ {
+ 	struct compat_robust_list_head __user *head = curr->compat_robust_list;
+ 	struct robust_list __user *entry, *next_entry, *pending;
+ 	unsigned int limit = ROBUST_LIST_LIMIT, pi, pip;
+ 	unsigned int uninitialized_var(next_pi);
+ 	compat_uptr_t uentry, next_uentry, upending;
+ 	compat_long_t futex_offset;
+ 	int rc;
+ 
+ 	if (!futex_cmpxchg_enabled)
+ 		return;
+ 
+ 	/*
+ 	 * Fetch the list head (which was registered earlier, via
+ 	 * sys_set_robust_list()):
+ 	 */
+ 	if (compat_fetch_robust_entry(&uentry, &entry, &head->list.next, &pi))
+ 		return;
+ 	/*
+ 	 * Fetch the relative futex offset:
+ 	 */
+ 	if (get_user(futex_offset, &head->futex_offset))
+ 		return;
+ 	/*
+ 	 * Fetch any possibly pending lock-add first, and handle it
+ 	 * if it exists:
+ 	 */
+ 	if (compat_fetch_robust_entry(&upending, &pending,
+ 			       &head->list_op_pending, &pip))
+ 		return;
+ 
+ 	next_entry = NULL;	/* avoid warning with gcc */
+ 	while (entry != (struct robust_list __user *) &head->list) {
+ 		/*
+ 		 * Fetch the next entry in the list before calling
+ 		 * handle_futex_death:
+ 		 */
+ 		rc = compat_fetch_robust_entry(&next_uentry, &next_entry,
+ 			(compat_uptr_t __user *)&entry->next, &next_pi);
+ 		/*
+ 		 * A pending lock might already be on the list, so
+ 		 * dont process it twice:
+ 		 */
+ 		if (entry != pending) {
+ 			void __user *uaddr = futex_uaddr(entry, futex_offset);
+ 
+ 			if (handle_futex_death(uaddr, curr, pi,
+ 					       HANDLE_DEATH_LIST))
+ 				return;
+ 		}
+ 		if (rc)
+ 			return;
+ 		uentry = next_uentry;
+ 		entry = next_entry;
+ 		pi = next_pi;
+ 		/*
+ 		 * Avoid excessively long or circular lists:
+ 		 */
+ 		if (!--limit)
+ 			break;
+ 
+ 		cond_resched();
+ 	}
+ 	if (pending) {
+ 		void __user *uaddr = futex_uaddr(pending, futex_offset);
+ 
+ 		handle_futex_death(uaddr, curr, pip, HANDLE_DEATH_PENDING);
+ 	}
+ }
+ 
+ COMPAT_SYSCALL_DEFINE2(set_robust_list,
+ 		struct compat_robust_list_head __user *, head,
+ 		compat_size_t, len)
+ {
+ 	if (!futex_cmpxchg_enabled)
+ 		return -ENOSYS;
+ 
+ 	if (unlikely(len != sizeof(*head)))
+ 		return -EINVAL;
+ 
+ 	current->compat_robust_list = head;
+ 
+ 	return 0;
+ }
+ 
+ COMPAT_SYSCALL_DEFINE3(get_robust_list, int, pid,
+ 			compat_uptr_t __user *, head_ptr,
+ 			compat_size_t __user *, len_ptr)
+ {
+ 	struct compat_robust_list_head __user *head;
+ 	unsigned long ret;
+ 	struct task_struct *p;
+ 
+ 	if (!futex_cmpxchg_enabled)
+ 		return -ENOSYS;
+ 
+ 	rcu_read_lock();
+ 
+ 	ret = -ESRCH;
+ 	if (!pid)
+ 		p = current;
+ 	else {
+ 		p = find_task_by_vpid(pid);
+ 		if (!p)
+ 			goto err_unlock;
+ 	}
+ 
+ 	ret = -EPERM;
+ 	if (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS))
+ 		goto err_unlock;
+ 
+ 	head = p->compat_robust_list;
+ 	rcu_read_unlock();
+ 
+ 	if (put_user(sizeof(*head), len_ptr))
+ 		return -EFAULT;
+ 	return put_user(ptr_to_compat(head), head_ptr);
+ 
+ err_unlock:
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ #endif /* CONFIG_COMPAT */
+ 
+ #ifdef CONFIG_COMPAT_32BIT_TIME
+ SYSCALL_DEFINE6(futex_time32, u32 __user *, uaddr, int, op, u32, val,
+ 		struct old_timespec32 __user *, utime, u32 __user *, uaddr2,
+ 		u32, val3)
+ {
+ 	struct timespec64 ts;
+ 	ktime_t t, *tp = NULL;
+ 	int val2 = 0;
+ 	int cmd = op & FUTEX_CMD_MASK;
+ 
+ 	if (utime && (cmd == FUTEX_WAIT || cmd == FUTEX_LOCK_PI ||
+ 		      cmd == FUTEX_WAIT_BITSET ||
+ 		      cmd == FUTEX_WAIT_REQUEUE_PI)) {
+ 		if (get_old_timespec32(&ts, utime))
+ 			return -EFAULT;
+ 		if (!timespec64_valid(&ts))
+ 			return -EINVAL;
+ 
+ 		t = timespec64_to_ktime(ts);
+ 		if (cmd == FUTEX_WAIT)
+ 			t = ktime_add_safe(ktime_get(), t);
+ 		tp = &t;
+ 	}
+ 	if (cmd == FUTEX_REQUEUE || cmd == FUTEX_CMP_REQUEUE ||
+ 	    cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)
+ 		val2 = (int) (unsigned long) utime;
+ 
+ 	return do_futex(uaddr, op, val, tp, uaddr2, val2, val3);
+ }
+ #endif /* CONFIG_COMPAT_32BIT_TIME */
+ 
++>>>>>>> ca16d5bee598 (futex: Prevent robust futex exit race)
  static void __init futex_detect_cmpxchg(void)
  {
  #ifndef CONFIG_HAVE_FUTEX_CMPXCHG
* Unmerged path kernel/futex.c
