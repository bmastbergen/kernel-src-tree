mm/khugepaged: fix crashes due to misaccounted holes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Hugh Dickins <hughd@google.com>
commit aaa52e340073b7f4593b3c4ddafcafa70cf838b5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/aaa52e34.failed

Huge tmpfs testing on a shortish file mapped into a pmd-rounded extent
hit shmem_evict_inode()'s WARN_ON(inode->i_blocks) followed by
clear_inode()'s BUG_ON(inode->i_data.nrpages) when the file was later
closed and unlinked.

khugepaged's collapse_shmem() was forgetting to update mapping->nrpages
on the rollback path, after it had added but then needs to undo some
holes.

There is indeed an irritating asymmetry between shmem_charge(), whose
callers want it to increment nrpages after successfully accounting
blocks, and shmem_uncharge(), when __delete_from_page_cache() already
decremented nrpages itself: oh well, just add a comment on that to them
both.

And shmem_recalc_inode() is supposed to be called when the accounting is
expected to be in balance (so it can deduce from imbalance that reclaim
discarded some pages): so change shmem_charge() to update nrpages
earlier (though it's rare for the difference to matter at all).

Link: http://lkml.kernel.org/r/alpine.LSU.2.11.1811261523450.2275@eggly.anvils
Fixes: 800d8c63b2e98 ("shmem: add huge pages support")
Fixes: f3f0e1d2150b2 ("khugepaged: add support of collapse for tmpfs/shmem pages")
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: <stable@vger.kernel.org>	[4.8+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit aaa52e340073b7f4593b3c4ddafcafa70cf838b5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
diff --cc mm/khugepaged.c
index abf64094dd2b,65e82f665c7c..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1521,16 -1502,21 +1521,28 @@@ tree_unlocked
  		unlock_page(new_page);
  
  		*hpage = NULL;
 -
 -		khugepaged_pages_collapsed++;
  	} else {
++<<<<<<< HEAD
 +		/* Something went wrong: rollback changes to the radix-tree */
 +		shmem_uncharge(mapping->host, nr_none);
 +		xa_lock_irq(&mapping->i_pages);
 +		radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start) {
 +			if (iter.index >= end)
 +				break;
++=======
+ 		struct page *page;
+ 
+ 		/* Something went wrong: roll back page cache changes */
+ 		xas_lock_irq(&xas);
+ 		mapping->nrpages -= nr_none;
+ 		shmem_uncharge(mapping->host, nr_none);
+ 
+ 		xas_set(&xas, start);
+ 		xas_for_each(&xas, page, end - 1) {
++>>>>>>> aaa52e340073 (mm/khugepaged: fix crashes due to misaccounted holes)
  			page = list_first_entry_or_null(&pagelist,
  					struct page, lru);
 -			if (!page || xas.xa_index < page->index) {
 +			if (!page || iter.index < page->index) {
  				if (!nr_none)
  					break;
  				nr_none--;
* Unmerged path mm/khugepaged.c
diff --git a/mm/shmem.c b/mm/shmem.c
index 3056a28d5af1..ac78b5ec6241 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -296,12 +296,14 @@ bool shmem_charge(struct inode *inode, long pages)
 	if (!shmem_inode_acct_block(inode, pages))
 		return false;
 
+	/* nrpages adjustment first, then shmem_recalc_inode() when balanced */
+	inode->i_mapping->nrpages += pages;
+
 	spin_lock_irqsave(&info->lock, flags);
 	info->alloced += pages;
 	inode->i_blocks += pages * BLOCKS_PER_PAGE;
 	shmem_recalc_inode(inode);
 	spin_unlock_irqrestore(&info->lock, flags);
-	inode->i_mapping->nrpages += pages;
 
 	return true;
 }
@@ -311,6 +313,8 @@ void shmem_uncharge(struct inode *inode, long pages)
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	unsigned long flags;
 
+	/* nrpages adjustment done by __delete_from_page_cache() or caller */
+
 	spin_lock_irqsave(&info->lock, flags);
 	info->alloced -= pages;
 	inode->i_blocks -= pages * BLOCKS_PER_PAGE;
