io_uring: io_queue_link*() right after submit

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit e5eb6366ac2d1df8ad5b010718ac1997ceae45be
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e5eb6366.failed

After a call to io_submit_sqe(), it's already known whether it needs
to queue a link or not. Do it there, as it's simplier and doesn't keep
an extra variable across the loop.

	Reviewed-byï¼šBob Liu <bob.liu@oracle.com>
	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit e5eb6366ac2d1df8ad5b010718ac1997ceae45be)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index fca9cdc96d77,6524898831e0..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2414,8 -2700,8 +2414,7 @@@ static int io_submit_sqes(struct io_rin
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
  	struct io_kiocb *shadow_req = NULL;
- 	bool prev_was_link = false;
  	int i, submitted = 0;
 -	bool mm_fault = false;
  
  	if (nr > IO_PLUG_THRESHOLD) {
  		io_submit_state_start(&state, ctx, nr);
@@@ -2423,19 -2709,20 +2422,36 @@@
  	}
  
  	for (i = 0; i < nr; i++) {
++<<<<<<< HEAD
 +		/*
 +		 * If previous wasn't linked and we have a linked command,
 +		 * that's the end of the chain. Submit the previous link.
 +		 */
 +		if (!prev_was_link && link) {
 +			io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +						true);
 +			link = NULL;
 +			shadow_req = NULL;
 +		}
 +		prev_was_link = (sqes[i].sqe->flags & IOSQE_IO_LINK) != 0;
 +
 +		if (link && (sqes[i].sqe->flags & IOSQE_IO_DRAIN)) {
++=======
+ 		struct sqe_submit s;
+ 
+ 		if (!io_get_sqring(ctx, &s))
+ 			break;
+ 
+ 		if (io_sqe_needs_user(s.sqe) && !*mm) {
+ 			mm_fault = mm_fault || !mmget_not_zero(ctx->sqo_mm);
+ 			if (!mm_fault) {
+ 				use_mm(ctx->sqo_mm);
+ 				*mm = ctx->sqo_mm;
+ 			}
+ 		}
+ 
+ 		if (link && (s.sqe->flags & IOSQE_IO_DRAIN)) {
++>>>>>>> e5eb6366ac2d (io_uring: io_queue_link*() right after submit)
  			if (!shadow_req) {
  				shadow_req = io_get_req(ctx, NULL);
  				if (unlikely(!shadow_req))
@@@ -2447,15 -2734,23 +2463,35 @@@
  		}
  
  out:
++<<<<<<< HEAD
 +		if (unlikely(mm_fault)) {
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
 +						-EFAULT);
 +		} else {
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
 +			submitted++;
++=======
+ 		s.ring_file = ring_file;
+ 		s.ring_fd = ring_fd;
+ 		s.has_user = *mm != NULL;
+ 		s.in_async = async;
+ 		s.needs_fixed_file = async;
+ 		trace_io_uring_submit_sqe(ctx, s.sqe->user_data, true, async);
+ 		io_submit_sqe(ctx, &s, statep, &link);
+ 		submitted++;
+ 
+ 		/*
+ 		 * If previous wasn't linked and we have a linked command,
+ 		 * that's the end of the chain. Submit the previous link.
+ 		 */
+ 		if (!(s.sqe->flags & IOSQE_IO_LINK) && link) {
+ 			io_queue_link_head(ctx, link, &link->submit, shadow_req);
+ 			link = NULL;
+ 			shadow_req = NULL;
++>>>>>>> e5eb6366ac2d (io_uring: io_queue_link*() right after submit)
  		}
  	}
  
* Unmerged path fs/io_uring.c
