x86/split_lock: Rework the initialization flow of split lock detection

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Xiaoyao Li <xiaoyao.li@intel.com>
commit dbaba47085b0c2aa793ce849750164bd3765e163
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/dbaba470.failed

Current initialization flow of split lock detection has following issues:

1. It assumes the initial value of MSR_TEST_CTRL.SPLIT_LOCK_DETECT to be
   zero. However, it's possible that BIOS/firmware has set it.

2. X86_FEATURE_SPLIT_LOCK_DETECT flag is unconditionally set even if
   there is a virtualization flaw that FMS indicates the existence while
   it's actually not supported.

Rework the initialization flow to solve above issues. In detail, explicitly
clear and set split_lock_detect bit to verify MSR_TEST_CTRL can be
accessed, and rdmsr after wrmsr to ensure bit is cleared/set successfully.

X86_FEATURE_SPLIT_LOCK_DETECT flag is set only when the feature does exist
and the feature is not disabled with kernel param "split_lock_detect=off"

On each processor, explicitly updating the SPLIT_LOCK_DETECT bit based on
sld_sate in split_lock_init() since BIOS/firmware may touch it.

Originally-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Xiaoyao Li <xiaoyao.li@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20200325030924.132881-2-xiaoyao.li@intel.com

(cherry picked from commit dbaba47085b0c2aa793ce849750164bd3765e163)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/intel.c
diff --cc arch/x86/kernel/cpu/intel.c
index ae064a2ca368,0c859c91d008..000000000000
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@@ -31,38 -33,42 +31,42 @@@
  #include <asm/apic.h>
  #endif
  
 -enum split_lock_detect_state {
 -	sld_off = 0,
 -	sld_warn,
 -	sld_fatal,
 -};
 -
  /*
 - * Default to sld_off because most systems do not support split lock detection
 - * split_lock_setup() will switch this to sld_warn on systems that support
 - * split lock detect, unless there is a command line override.
 + * Just in case our CPU detection goes bad, or you have a weird system,
 + * allow a way to override the automatic disabling of MPX.
   */
++<<<<<<< HEAD
 +static int forcempx;
++=======
+ static enum split_lock_detect_state sld_state __ro_after_init = sld_off;
++>>>>>>> dbaba47085b0 (x86/split_lock: Rework the initialization flow of split lock detection)
  
 -/*
 - * Processors which have self-snooping capability can handle conflicting
 - * memory type across CPUs by snooping its own cache. However, there exists
 - * CPU models in which having conflicting memory types still leads to
 - * unpredictable behavior, machine check errors, or hangs. Clear this
 - * feature to prevent its use on machines with known erratas.
 - */
 -static void check_memory_type_self_snoop_errata(struct cpuinfo_x86 *c)
 +static int __init forcempx_setup(char *__unused)
  {
 -	switch (c->x86_model) {
 -	case INTEL_FAM6_CORE_YONAH:
 -	case INTEL_FAM6_CORE2_MEROM:
 -	case INTEL_FAM6_CORE2_MEROM_L:
 -	case INTEL_FAM6_CORE2_PENRYN:
 -	case INTEL_FAM6_CORE2_DUNNINGTON:
 -	case INTEL_FAM6_NEHALEM:
 -	case INTEL_FAM6_NEHALEM_G:
 -	case INTEL_FAM6_NEHALEM_EP:
 -	case INTEL_FAM6_NEHALEM_EX:
 -	case INTEL_FAM6_WESTMERE:
 -	case INTEL_FAM6_WESTMERE_EP:
 -	case INTEL_FAM6_SANDYBRIDGE:
 -		setup_clear_cpu_cap(X86_FEATURE_SELFSNOOP);
 +	forcempx = 1;
 +
 +	return 1;
 +}
 +__setup("intel-skd-046-workaround=disable", forcempx_setup);
 +
 +void check_mpx_erratum(struct cpuinfo_x86 *c)
 +{
 +	if (forcempx)
 +		return;
 +	/*
 +	 * Turn off the MPX feature on CPUs where SMEP is not
 +	 * available or disabled.
 +	 *
 +	 * Works around Intel Erratum SKD046: "Branch Instructions
 +	 * May Initialize MPX Bound Registers Incorrectly".
 +	 *
 +	 * This might falsely disable MPX on systems without
 +	 * SMEP, like Atom processors without SMEP.  But there
 +	 * is no such hardware known at the moment.
 +	 */
 +	if (cpu_has(c, X86_FEATURE_MPX) && !cpu_has(c, X86_FEATURE_SMEP)) {
 +		setup_clear_cpu_cap(X86_FEATURE_MPX);
 +		pr_warn("x86/mpx: Disabling MPX since SMEP not present\n");
  	}
  }
  
@@@ -1034,3 -965,167 +1038,170 @@@ static const struct cpu_dev intel_cpu_d
  
  cpu_dev_register(intel_cpu_dev);
  
++<<<<<<< HEAD
++=======
+ #undef pr_fmt
+ #define pr_fmt(fmt) "x86/split lock detection: " fmt
+ 
+ static const struct {
+ 	const char			*option;
+ 	enum split_lock_detect_state	state;
+ } sld_options[] __initconst = {
+ 	{ "off",	sld_off   },
+ 	{ "warn",	sld_warn  },
+ 	{ "fatal",	sld_fatal },
+ };
+ 
+ static inline bool match_option(const char *arg, int arglen, const char *opt)
+ {
+ 	int len = strlen(opt);
+ 
+ 	return len == arglen && !strncmp(arg, opt, len);
+ }
+ 
+ static bool split_lock_verify_msr(bool on)
+ {
+ 	u64 ctrl, tmp;
+ 
+ 	if (rdmsrl_safe(MSR_TEST_CTRL, &ctrl))
+ 		return false;
+ 	if (on)
+ 		ctrl |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	else
+ 		ctrl &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	if (wrmsrl_safe(MSR_TEST_CTRL, ctrl))
+ 		return false;
+ 	rdmsrl(MSR_TEST_CTRL, tmp);
+ 	return ctrl == tmp;
+ }
+ 
+ static void __init split_lock_setup(void)
+ {
+ 	enum split_lock_detect_state state = sld_warn;
+ 	char arg[20];
+ 	int i, ret;
+ 
+ 	if (!split_lock_verify_msr(false)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	ret = cmdline_find_option(boot_command_line, "split_lock_detect",
+ 				  arg, sizeof(arg));
+ 	if (ret >= 0) {
+ 		for (i = 0; i < ARRAY_SIZE(sld_options); i++) {
+ 			if (match_option(arg, ret, sld_options[i].option)) {
+ 				state = sld_options[i].state;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	switch (state) {
+ 	case sld_off:
+ 		pr_info("disabled\n");
+ 		return;
+ 	case sld_warn:
+ 		pr_info("warning about user-space split_locks\n");
+ 		break;
+ 	case sld_fatal:
+ 		pr_info("sending SIGBUS on user-space split_locks\n");
+ 		break;
+ 	}
+ 
+ 	if (!split_lock_verify_msr(true)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	sld_state = state;
+ 	setup_force_cpu_cap(X86_FEATURE_SPLIT_LOCK_DETECT);
+ }
+ 
+ /*
+  * MSR_TEST_CTRL is per core, but we treat it like a per CPU MSR. Locking
+  * is not implemented as one thread could undo the setting of the other
+  * thread immediately after dropping the lock anyway.
+  */
+ static void sld_update_msr(bool on)
+ {
+ 	u64 test_ctrl_val;
+ 
+ 	rdmsrl(MSR_TEST_CTRL, test_ctrl_val);
+ 
+ 	if (on)
+ 		test_ctrl_val |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	else
+ 		test_ctrl_val &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 
+ 	wrmsrl(MSR_TEST_CTRL, test_ctrl_val);
+ }
+ 
+ static void split_lock_init(void)
+ {
+ 	split_lock_verify_msr(sld_state != sld_off);
+ }
+ 
+ bool handle_user_split_lock(struct pt_regs *regs, long error_code)
+ {
+ 	if ((regs->flags & X86_EFLAGS_AC) || sld_state == sld_fatal)
+ 		return false;
+ 
+ 	pr_warn_ratelimited("#AC: %s/%d took a split_lock trap at address: 0x%lx\n",
+ 			    current->comm, current->pid, regs->ip);
+ 
+ 	/*
+ 	 * Disable the split lock detection for this task so it can make
+ 	 * progress and set TIF_SLD so the detection is re-enabled via
+ 	 * switch_to_sld() when the task is scheduled out.
+ 	 */
+ 	sld_update_msr(false);
+ 	set_tsk_thread_flag(current, TIF_SLD);
+ 	return true;
+ }
+ 
+ /*
+  * This function is called only when switching between tasks with
+  * different split-lock detection modes. It sets the MSR for the
+  * mode of the new task. This is right most of the time, but since
+  * the MSR is shared by hyperthreads on a physical core there can
+  * be glitches when the two threads need different modes.
+  */
+ void switch_to_sld(unsigned long tifn)
+ {
+ 	sld_update_msr(!(tifn & _TIF_SLD));
+ }
+ 
+ #define SPLIT_LOCK_CPU(model) {X86_VENDOR_INTEL, 6, model, X86_FEATURE_ANY}
+ 
+ /*
+  * The following processors have the split lock detection feature. But
+  * since they don't have the IA32_CORE_CAPABILITIES MSR, the feature cannot
+  * be enumerated. Enable it by family and model matching on these
+  * processors.
+  */
+ static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
+ 	SPLIT_LOCK_CPU(INTEL_FAM6_ICELAKE_X),
+ 	SPLIT_LOCK_CPU(INTEL_FAM6_ICELAKE_L),
+ 	{}
+ };
+ 
+ void __init cpu_set_core_cap_bits(struct cpuinfo_x86 *c)
+ {
+ 	u64 ia32_core_caps = 0;
+ 
+ 	if (c->x86_vendor != X86_VENDOR_INTEL)
+ 		return;
+ 	if (cpu_has(c, X86_FEATURE_CORE_CAPABILITIES)) {
+ 		/* Enumerate features reported in IA32_CORE_CAPABILITIES MSR. */
+ 		rdmsrl(MSR_IA32_CORE_CAPS, ia32_core_caps);
+ 	} else if (!boot_cpu_has(X86_FEATURE_HYPERVISOR)) {
+ 		/* Enumerate split lock detection by family and model. */
+ 		if (x86_match_cpu(split_lock_cpu_ids))
+ 			ia32_core_caps |= MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT;
+ 	}
+ 
+ 	if (ia32_core_caps & MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT)
+ 		split_lock_setup();
+ }
++>>>>>>> dbaba47085b0 (x86/split_lock: Rework the initialization flow of split lock detection)
* Unmerged path arch/x86/kernel/cpu/intel.c
