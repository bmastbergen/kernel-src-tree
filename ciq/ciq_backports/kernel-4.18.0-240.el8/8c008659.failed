KVM: MMU: stop dereferencing vcpu->arch.mmu to get the context for MMU init

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 8c008659aa43be97c60c1633074b8f52f9f4445c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/8c008659.failed

kvm_init_shadow_mmu() was actually the only function that could be called
with different vcpu->arch.mmu values.  Now that kvm_init_shadow_npt_mmu()
is separated from kvm_init_shadow_mmu(), we always know the MMU context
we need to use and there is no need to dereference vcpu->arch.mmu pointer.

Based on a patch by Vitaly Kuznetsov <vkuznets@redhat.com>.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
Message-Id: <20200710141157.1640173-3-vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 8c008659aa43be97c60c1633074b8f52f9f4445c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 78a7db187b32,78c88e8aecfa..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -5034,20 -4918,15 +5034,28 @@@ kvm_calc_shadow_mmu_root_page_role(stru
  	return role;
  }
  
++<<<<<<< HEAD
 +void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_mmu *context = vcpu->arch.mmu;
 +	union kvm_mmu_role new_role =
 +		kvm_calc_shadow_mmu_root_page_role(vcpu, false);
 +
 +	if (new_role.as_u64 == context->mmu_role.as_u64)
 +		return;
 +
 +	if (!is_paging(vcpu))
++=======
+ static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
+ 				    u32 cr0, u32 cr4, u32 efer,
+ 				    union kvm_mmu_role new_role)
+ {
+ 	if (!(cr0 & X86_CR0_PG))
++>>>>>>> 8c008659aa43 (KVM: MMU: stop dereferencing vcpu->arch.mmu to get the context for MMU init)
  		nonpaging_init_context(vcpu, context);
 -	else if (efer & EFER_LMA)
 +	else if (is_long_mode(vcpu))
  		paging64_init_context(vcpu, context);
 -	else if (cr4 & X86_CR4_PAE)
 +	else if (is_pae(vcpu))
  		paging32E_init_context(vcpu, context);
  	else
  		paging32_init_context(vcpu, context);
@@@ -5055,7 -4934,28 +5063,32 @@@
  	context->mmu_role.as_u64 = new_role.as_u64;
  	reset_shadow_zero_bits_mask(vcpu, context);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(kvm_init_shadow_mmu);
++=======
+ 
+ static void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu, u32 cr0, u32 cr4, u32 efer)
+ {
+ 	struct kvm_mmu *context = &vcpu->arch.root_mmu;
+ 	union kvm_mmu_role new_role =
+ 		kvm_calc_shadow_mmu_root_page_role(vcpu, false);
+ 
+ 	if (new_role.as_u64 != context->mmu_role.as_u64)
+ 		shadow_mmu_init_context(vcpu, context, cr0, cr4, efer, new_role);
+ }
+ 
+ void kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, u32 cr0, u32 cr4, u32 efer,
+ 			     gpa_t nested_cr3)
+ {
+ 	struct kvm_mmu *context = &vcpu->arch.guest_mmu;
+ 	union kvm_mmu_role new_role =
+ 		kvm_calc_shadow_mmu_root_page_role(vcpu, false);
+ 
+ 	if (new_role.as_u64 != context->mmu_role.as_u64)
+ 		shadow_mmu_init_context(vcpu, context, cr0, cr4, efer, new_role);
+ }
+ EXPORT_SYMBOL_GPL(kvm_init_shadow_npt_mmu);
++>>>>>>> 8c008659aa43 (KVM: MMU: stop dereferencing vcpu->arch.mmu to get the context for MMU init)
  
  static union kvm_mmu_role
  kvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,
@@@ -5123,10 -5023,13 +5156,10 @@@ EXPORT_SYMBOL_GPL(kvm_init_shadow_ept_m
  
  static void init_kvm_softmmu(struct kvm_vcpu *vcpu)
  {
- 	struct kvm_mmu *context = vcpu->arch.mmu;
+ 	struct kvm_mmu *context = &vcpu->arch.root_mmu;
  
 -	kvm_init_shadow_mmu(vcpu,
 -			    kvm_read_cr0_bits(vcpu, X86_CR0_PG),
 -			    kvm_read_cr4_bits(vcpu, X86_CR4_PAE),
 -			    vcpu->arch.efer);
 -
 +	kvm_init_shadow_mmu(vcpu);
 +	context->set_cr3           = kvm_x86_ops->set_cr3;
  	context->get_guest_pgd     = get_cr3;
  	context->get_pdptr         = kvm_pdptr_read;
  	context->inject_page_fault = kvm_inject_page_fault;
* Unmerged path arch/x86/kvm/mmu/mmu.c
