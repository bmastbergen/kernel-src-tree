RDMA/mlx5: Fix handling of IOVA != user_va in ODP paths

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 8ffc32485158528f870b62707077ab494ba31deb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/8ffc3248.failed

Till recently it was not possible for userspace to specify a different
IOVA, but with the new ibv_reg_mr_iova() library call this can be done.

To compute the user_va we must compute:
  user_va = (iova - iova_start) + user_va_start

while being cautious of overflow and other math problems.

The iova is not reliably stored in the mmkey when the MR is created. Only
the cached creation path (the common one) set it, so it must also be set
when creating uncached MRs.

Fix the weird use of iova when computing the starting page index in the
MR. In the normal case, when iova == umem.address:
  iova & (~(BIT(page_shift) - 1)) ==
  ALIGN_DOWN(umem.address, odp->page_size) ==
  ib_umem_start(odp)

And when iova is different using it in math with a user_va is wrong.

Finally, do not allow an implicit ODP to be created with a non-zero IOVA
as we have no support for that.

Fixes: 7bdf65d411c1 ("IB/mlx5: Handle page faults")
	Signed-off-by: Moni Shoua <monis@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit 8ffc32485158528f870b62707077ab494ba31deb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/odp.c
index aea828fa7445,0afb0042bd53..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -556,103 -492,144 +556,110 @@@ struct mlx5_ib_mr *mlx5_ib_alloc_implic
  					     struct ib_udata *udata,
  					     int access_flags)
  {
 -	struct mlx5_ib_dev *dev = to_mdev(pd->ibpd.device);
 -	struct ib_umem_odp *umem_odp;
  	struct mlx5_ib_mr *imr;
 -	int err;
 +	struct ib_umem *umem;
  
 -	umem_odp = ib_umem_odp_alloc_implicit(&dev->ib_dev, access_flags);
 -	if (IS_ERR(umem_odp))
 -		return ERR_CAST(umem_odp);
 +	umem = ib_umem_get(udata, 0, 0, access_flags, 0);
 +	if (IS_ERR(umem))
 +		return ERR_CAST(umem);
  
 -	imr = mlx5_mr_cache_alloc(dev, MLX5_IMR_KSM_CACHE_ENTRY);
 +	imr = implicit_mr_alloc(&pd->ibpd, umem, 1, access_flags);
  	if (IS_ERR(imr)) {
 -		err = PTR_ERR(imr);
 -		goto out_umem;
 +		ib_umem_release(umem);
 +		return ERR_CAST(imr);
  	}
  
 -	imr->ibmr.pd = &pd->ibpd;
 -	imr->access_flags = access_flags;
 -	imr->mmkey.iova = 0;
 -	imr->umem = &umem_odp->umem;
 -	imr->ibmr.lkey = imr->mmkey.key;
 -	imr->ibmr.rkey = imr->mmkey.key;
 -	imr->umem = &umem_odp->umem;
 -	imr->is_odp_implicit = true;
 -	atomic_set(&imr->num_deferred_work, 0);
 -	xa_init(&imr->implicit_children);
 -
 -	err = mlx5_ib_update_xlt(imr, 0,
 -				 mlx5_imr_ksm_entries,
 -				 MLX5_KSM_PAGE_SHIFT,
 -				 MLX5_IB_UPD_XLT_INDIRECT |
 -				 MLX5_IB_UPD_XLT_ZAP |
 -				 MLX5_IB_UPD_XLT_ENABLE);
 -	if (err)
 -		goto out_mr;
 +	imr->umem = umem;
 +	init_waitqueue_head(&imr->q_leaf_free);
 +	atomic_set(&imr->num_leaf_free, 0);
 +	atomic_set(&imr->num_pending_prefetch, 0);
  
 -	err = xa_err(xa_store(&dev->odp_mkeys, mlx5_base_mkey(imr->mmkey.key),
 -			      &imr->mmkey, GFP_KERNEL));
 -	if (err)
 -		goto out_mr;
 +	imr->is_odp_implicit = true;
  
 -	mlx5_ib_dbg(dev, "key %x mr %p\n", imr->mmkey.key, imr);
  	return imr;
 -out_mr:
 -	mlx5_ib_err(dev, "Failed to register MKEY %d\n", err);
 -	mlx5_mr_cache_free(dev, imr);
 -out_umem:
 -	ib_umem_odp_release(umem_odp);
 -	return ERR_PTR(err);
  }
  
 -void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
 +static int mr_leaf_free(struct ib_umem_odp *umem_odp, u64 start, u64 end,
 +			void *cookie)
  {
 -	struct ib_umem_odp *odp_imr = to_ib_umem_odp(imr->umem);
 -	struct mlx5_ib_dev *dev = imr->dev;
 -	struct list_head destroy_list;
 -	struct mlx5_ib_mr *mtt;
 -	struct mlx5_ib_mr *tmp;
 -	unsigned long idx;
 +	struct mlx5_ib_mr *mr = umem_odp->private, *imr = cookie;
  
 -	INIT_LIST_HEAD(&destroy_list);
 +	if (mr->parent != imr)
 +		return 0;
  
 -	xa_erase(&dev->odp_mkeys, mlx5_base_mkey(imr->mmkey.key));
 -	/*
 -	 * This stops the SRCU protected page fault path from touching either
 -	 * the imr or any children. The page fault path can only reach the
 -	 * children xarray via the imr.
 -	 */
 -	synchronize_srcu(&dev->odp_srcu);
 +	ib_umem_odp_unmap_dma_pages(umem_odp, ib_umem_start(umem_odp),
 +				    ib_umem_end(umem_odp));
  
 -	xa_lock(&imr->implicit_children);
 -	xa_for_each (&imr->implicit_children, idx, mtt) {
 -		__xa_erase(&imr->implicit_children, idx);
 -		list_add(&mtt->odp_destroy.elm, &destroy_list);
 -	}
 -	xa_unlock(&imr->implicit_children);
 +	if (umem_odp->dying)
 +		return 0;
  
 -	/*
 -	 * num_deferred_work can only be incremented inside the odp_srcu, or
 -	 * under xa_lock while the child is in the xarray. Thus at this point
 -	 * it is only decreasing, and all work holding it is now on the wq.
 -	 */
 -	if (atomic_read(&imr->num_deferred_work)) {
 -		flush_workqueue(system_unbound_wq);
 -		WARN_ON(atomic_read(&imr->num_deferred_work));
 -	}
 -
 -	/*
 -	 * Fence the imr before we destroy the children. This allows us to
 -	 * skip updating the XLT of the imr during destroy of the child mkey
 -	 * the imr points to.
 -	 */
 -	mlx5_mr_cache_invalidate(imr);
 +	WRITE_ONCE(umem_odp->dying, 1);
 +	atomic_inc(&imr->num_leaf_free);
 +	schedule_work(&umem_odp->work);
  
 -	list_for_each_entry_safe (mtt, tmp, &destroy_list, odp_destroy.elm)
 -		free_implicit_child_mr(mtt, false);
 -
 -	mlx5_mr_cache_free(dev, imr);
 -	ib_umem_odp_release(odp_imr);
 +	return 0;
  }
  
 -/**
 - * mlx5_ib_fence_odp_mr - Stop all access to the ODP MR
 - * @mr: to fence
 - *
 - * On return no parallel threads will be touching this MR and no DMA will be
 - * active.
 - */
 -void mlx5_ib_fence_odp_mr(struct mlx5_ib_mr *mr)
 +void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
  {
 -	/* Prevent new page faults and prefetch requests from succeeding */
 -	xa_erase(&mr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
 +	struct ib_ucontext_per_mm *per_mm = mr_to_per_mm(imr);
  
 -	/* Wait for all running page-fault handlers to finish. */
 -	synchronize_srcu(&mr->dev->odp_srcu);
 +	down_read(&per_mm->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, 0, ULLONG_MAX,
 +				      mr_leaf_free, imr);
 +	up_read(&per_mm->umem_rwsem);
  
 -	if (atomic_read(&mr->num_deferred_work)) {
 -		flush_workqueue(system_unbound_wq);
 -		WARN_ON(atomic_read(&mr->num_deferred_work));
 -	}
 -
 -	dma_fence_odp_mr(mr);
 +	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
  }
  
 +#define MLX5_PF_FLAGS_PREFETCH  BIT(0)
  #define MLX5_PF_FLAGS_DOWNGRADE BIT(1)
 -static int pagefault_real_mr(struct mlx5_ib_mr *mr, struct ib_umem_odp *odp,
 -			     u64 user_va, size_t bcnt, u32 *bytes_mapped,
 -			     u32 flags)
 +static int pagefault_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
 +			u64 io_virt, size_t bcnt, u32 *bytes_mapped,
 +			u32 flags)
  {
 -	int page_shift, ret, np;
 +	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
  	bool downgrade = flags & MLX5_PF_FLAGS_DOWNGRADE;
 -	unsigned long current_seq;
 +	bool prefetch = flags & MLX5_PF_FLAGS_PREFETCH;
 +	int npages = 0, current_seq, page_shift, ret, np;
  	u64 access_mask;
++<<<<<<< HEAD
 +	u64 start_idx, page_mask;
 +	struct ib_umem_odp *odp;
 +	size_t size;
 +
 +	if (!odp_mr->page_list) {
 +		odp = implicit_mr_get_data(mr, io_virt, bcnt);
 +
 +		if (IS_ERR(odp))
 +			return PTR_ERR(odp);
 +		mr = odp->private;
 +	} else {
 +		odp = odp_mr;
 +	}
 +
 +next_mr:
 +	size = min_t(size_t, bcnt, ib_umem_end(odp) - io_virt);
 +
 +	page_shift = odp->page_shift;
 +	page_mask = ~(BIT(page_shift) - 1);
 +	start_idx = (io_virt - (mr->mmkey.iova & page_mask)) >> page_shift;
++=======
+ 	u64 start_idx;
+ 
+ 	page_shift = odp->page_shift;
+ 	start_idx = (user_va - ib_umem_start(odp)) >> page_shift;
++>>>>>>> 8ffc32485158 (RDMA/mlx5: Fix handling of IOVA != user_va in ODP paths)
  	access_mask = ODP_READ_ALLOWED_BIT;
  
 +	if (prefetch && !downgrade && !odp->umem.writable) {
 +		/* prefetch with write-access must
 +		 * be supported by the MR
 +		 */
 +		ret = -EINVAL;
 +		goto out;
 +	}
 +
  	if (odp->umem.writable && !downgrade)
  		access_mask |= ODP_WRITE_ALLOWED_BIT;
  
@@@ -734,6 -752,39 +741,42 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Returns:
+  *  -EFAULT: The io_virt->bcnt is not within the MR, it covers pages that are
+  *           not accessible, or the MR is no longer valid.
+  *  -EAGAIN/-ENOMEM: The operation should be retried
+  *
+  *  -EINVAL/others: General internal malfunction
+  *  >0: Number of pages mapped
+  */
+ static int pagefault_mr(struct mlx5_ib_mr *mr, u64 io_virt, size_t bcnt,
+ 			u32 *bytes_mapped, u32 flags)
+ {
+ 	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
+ 
+ 	if (unlikely(io_virt < mr->mmkey.iova))
+ 		return -EFAULT;
+ 
+ 	if (!odp->is_implicit_odp) {
+ 		u64 user_va;
+ 
+ 		if (check_add_overflow(io_virt - mr->mmkey.iova,
+ 				       (u64)odp->umem.address, &user_va))
+ 			return -EFAULT;
+ 		if (unlikely(user_va >= ib_umem_end(odp) ||
+ 			     ib_umem_end(odp) - user_va < bcnt))
+ 			return -EFAULT;
+ 		return pagefault_real_mr(mr, odp, user_va, bcnt, bytes_mapped,
+ 					 flags);
+ 	}
+ 	return pagefault_implicit_mr(mr, odp, io_virt, bcnt, bytes_mapped,
+ 				     flags);
+ }
+ 
++>>>>>>> 8ffc32485158 (RDMA/mlx5: Fix handling of IOVA != user_va in ODP paths)
  struct pf_frame {
  	struct pf_frame *next;
  	u32 key;
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index 4538dfcf3120..3e532946aec9 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -1266,6 +1266,8 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 
 	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) && !start &&
 	    length == U64_MAX) {
+		if (virt_addr != start)
+			return ERR_PTR(-EINVAL);
 		if (!(access_flags & IB_ACCESS_ON_DEMAND) ||
 		    !(dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
 			return ERR_PTR(-EINVAL);
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
