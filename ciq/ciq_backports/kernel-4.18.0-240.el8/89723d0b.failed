io_uring: enable optimized link handling for IORING_OP_POLL_ADD

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 89723d0bd6c77540c01ce7db2cd6f8c3be2fd958
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/89723d0b.failed

As introduced by commit:

ba816ad61fdf ("io_uring: run dependent links inline if possible")

enable inline dependent link running for poll commands.
io_poll_complete_work() is the most important change, as it allows a
linked sequence of { POLL, READ } (for example) to proceed inline
instead of needing to get punted to another async context. The
submission side only potentially matters for sqthread, but may as well
include that bit.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 89723d0bd6c77540c01ce7db2cd6f8c3be2fd958)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index fca9cdc96d77,bda27b52fd5b..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1693,8 -1806,12 +1693,9 @@@ static void io_poll_complete_work(struc
  	struct io_poll_iocb *poll = &req->poll;
  	struct poll_table_struct pt = { ._key = poll->events };
  	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *nxt = NULL;
  	__poll_t mask = 0;
  
 -	if (work->flags & IO_WQ_WORK_CANCEL)
 -		WRITE_ONCE(poll->canceled, true);
 -
  	if (!READ_ONCE(poll->canceled))
  		mask = vfs_poll(poll->file, &pt) & poll->events;
  
@@@ -1716,7 -1833,10 +1717,14 @@@
  	spin_unlock_irq(&ctx->completion_lock);
  
  	io_cqring_ev_posted(ctx);
++<<<<<<< HEAD
 +	io_put_req(req);
++=======
+ 
+ 	io_put_req(req, &nxt);
+ 	if (nxt)
+ 		*workptr = &nxt->work;
++>>>>>>> 89723d0bd6c7 (io_uring: enable optimized link handling for IORING_OP_POLL_ADD)
  }
  
  static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
@@@ -1833,7 -1954,7 +1842,11 @@@ static int io_poll_add(struct io_kiocb 
  
  	if (mask) {
  		io_cqring_ev_posted(ctx);
++<<<<<<< HEAD
 +		io_put_req(req);
++=======
+ 		io_put_req(req, nxt);
++>>>>>>> 89723d0bd6c7 (io_uring: enable optimized link handling for IORING_OP_POLL_ADD)
  	}
  	return ipt.error;
  }
@@@ -1886,19 -2231,19 +1899,19 @@@ static int __io_submit_sqe(struct io_ri
  	case IORING_OP_WRITEV:
  		if (unlikely(s->sqe->buf_index))
  			return -EINVAL;
 -		ret = io_write(req, s, nxt, force_nonblock);
 +		ret = io_write(req, s, force_nonblock);
  		break;
  	case IORING_OP_READ_FIXED:
 -		ret = io_read(req, s, nxt, force_nonblock);
 +		ret = io_read(req, s, force_nonblock);
  		break;
  	case IORING_OP_WRITE_FIXED:
 -		ret = io_write(req, s, nxt, force_nonblock);
 +		ret = io_write(req, s, force_nonblock);
  		break;
  	case IORING_OP_FSYNC:
 -		ret = io_fsync(req, s->sqe, nxt, force_nonblock);
 +		ret = io_fsync(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_POLL_ADD:
- 		ret = io_poll_add(req, s->sqe);
+ 		ret = io_poll_add(req, s->sqe, nxt);
  		break;
  	case IORING_OP_POLL_REMOVE:
  		ret = io_poll_remove(req, s->sqe);
* Unmerged path fs/io_uring.c
