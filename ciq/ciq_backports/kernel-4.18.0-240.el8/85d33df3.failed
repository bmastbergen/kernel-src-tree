bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Martin KaFai Lau <kafai@fb.com>
commit 85d33df357b634649ddbe0a20fd2d0fc5732c3cb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/85d33df3.failed

The patch introduces BPF_MAP_TYPE_STRUCT_OPS.  The map value
is a kernel struct with its func ptr implemented in bpf prog.
This new map is the interface to register/unregister/introspect
a bpf implemented kernel struct.

The kernel struct is actually embedded inside another new struct
(or called the "value" struct in the code).  For example,
"struct tcp_congestion_ops" is embbeded in:
struct bpf_struct_ops_tcp_congestion_ops {
	refcount_t refcnt;
	enum bpf_struct_ops_state state;
	struct tcp_congestion_ops data;  /* <-- kernel subsystem struct here */
}
The map value is "struct bpf_struct_ops_tcp_congestion_ops".
The "bpftool map dump" will then be able to show the
state ("inuse"/"tobefree") and the number of subsystem's refcnt (e.g.
number of tcp_sock in the tcp_congestion_ops case).  This "value" struct
is created automatically by a macro.  Having a separate "value" struct
will also make extending "struct bpf_struct_ops_XYZ" easier (e.g. adding
"void (*init)(void)" to "struct bpf_struct_ops_XYZ" to do some
initialization works before registering the struct_ops to the kernel
subsystem).  The libbpf will take care of finding and populating the
"struct bpf_struct_ops_XYZ" from "struct XYZ".

Register a struct_ops to a kernel subsystem:
1. Load all needed BPF_PROG_TYPE_STRUCT_OPS prog(s)
2. Create a BPF_MAP_TYPE_STRUCT_OPS with attr->btf_vmlinux_value_type_id
   set to the btf id "struct bpf_struct_ops_tcp_congestion_ops" of the
   running kernel.
   Instead of reusing the attr->btf_value_type_id,
   btf_vmlinux_value_type_id s added such that attr->btf_fd can still be
   used as the "user" btf which could store other useful sysadmin/debug
   info that may be introduced in the furture,
   e.g. creation-date/compiler-details/map-creator...etc.
3. Create a "struct bpf_struct_ops_tcp_congestion_ops" object as described
   in the running kernel btf.  Populate the value of this object.
   The function ptr should be populated with the prog fds.
4. Call BPF_MAP_UPDATE with the object created in (3) as
   the map value.  The key is always "0".

During BPF_MAP_UPDATE, the code that saves the kernel-func-ptr's
args as an array of u64 is generated.  BPF_MAP_UPDATE also allows
the specific struct_ops to do some final checks in "st_ops->init_member()"
(e.g. ensure all mandatory func ptrs are implemented).
If everything looks good, it will register this kernel struct
to the kernel subsystem.  The map will not allow further update
from this point.

Unregister a struct_ops from the kernel subsystem:
BPF_MAP_DELETE with key "0".

Introspect a struct_ops:
BPF_MAP_LOOKUP_ELEM with key "0".  The map value returned will
have the prog _id_ populated as the func ptr.

The map value state (enum bpf_struct_ops_state) will transit from:
INIT (map created) =>
INUSE (map updated, i.e. reg) =>
TOBEFREE (map value deleted, i.e. unreg)

The kernel subsystem needs to call bpf_struct_ops_get() and
bpf_struct_ops_put() to manage the "refcnt" in the
"struct bpf_struct_ops_XYZ".  This patch uses a separate refcnt
for the purose of tracking the subsystem usage.  Another approach
is to reuse the map->refcnt and then "show" (i.e. during map_lookup)
the subsystem's usage by doing map->refcnt - map->usercnt to filter out
the map-fd/pinned-map usage.  However, that will also tie down the
future semantics of map->refcnt and map->usercnt.

The very first subsystem's refcnt (during reg()) holds one
count to map->refcnt.  When the very last subsystem's refcnt
is gone, it will also release the map->refcnt.  All bpf_prog will be
freed when the map->refcnt reaches 0 (i.e. during map_free()).

Here is how the bpftool map command will look like:
[root@arch-fb-vm1 bpf]# bpftool map show
6: struct_ops  name dctcp  flags 0x0
	key 4B  value 256B  max_entries 1  memlock 4096B
	btf_id 6
[root@arch-fb-vm1 bpf]# bpftool map dump id 6
[{
        "value": {
            "refcnt": {
                "refs": {
                    "counter": 1
                }
            },
            "state": 1,
            "data": {
                "list": {
                    "next": 0,
                    "prev": 0
                },
                "key": 0,
                "flags": 2,
                "init": 24,
                "release": 0,
                "ssthresh": 25,
                "cong_avoid": 30,
                "set_state": 27,
                "cwnd_event": 28,
                "in_ack_event": 26,
                "undo_cwnd": 29,
                "pkts_acked": 0,
                "min_tso_segs": 0,
                "sndbuf_expand": 0,
                "cong_control": 0,
                "get_info": 0,
                "name": [98,112,102,95,100,99,116,99,112,0,0,0,0,0,0,0
                ],
                "owner": 0
            }
        }
    }
]

Misc Notes:
* bpf_struct_ops_map_sys_lookup_elem() is added for syscall lookup.
  It does an inplace update on "*value" instead returning a pointer
  to syscall.c.  Otherwise, it needs a separate copy of "zero" value
  for the BPF_STRUCT_OPS_STATE_INIT to avoid races.

* The bpf_struct_ops_map_delete_elem() is also called without
  preempt_disable() from map_delete_elem().  It is because
  the "->unreg()" may requires sleepable context, e.g.
  the "tcp_unregister_congestion_control()".

* "const" is added to some of the existing "struct btf_func_model *"
  function arg to avoid a compiler warning caused by this patch.

	Signed-off-by: Martin KaFai Lau <kafai@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
	Acked-by: Yonghong Song <yhs@fb.com>
Link: https://lore.kernel.org/bpf/20200109003505.3855919-1-kafai@fb.com
(cherry picked from commit 85d33df357b634649ddbe0a20fd2d0fc5732c3cb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
#	include/linux/bpf.h
#	include/linux/btf.h
#	include/uapi/linux/bpf.h
#	kernel/bpf/bpf_struct_ops.c
#	kernel/bpf/btf.c
#	kernel/bpf/syscall.c
#	kernel/bpf/trampoline.c
diff --cc arch/x86/net/bpf_jit_comp.c
index 490d8e35c7c9,9ba08e9abc09..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -1242,6 -1328,356 +1242,359 @@@ emit_jmp
  	return proglen;
  }
  
++<<<<<<< HEAD
++=======
+ static void save_regs(const struct btf_func_model *m, u8 **prog, int nr_args,
+ 		      int stack_size)
+ {
+ 	int i;
+ 	/* Store function arguments to stack.
+ 	 * For a function that accepts two pointers the sequence will be:
+ 	 * mov QWORD PTR [rbp-0x10],rdi
+ 	 * mov QWORD PTR [rbp-0x8],rsi
+ 	 */
+ 	for (i = 0; i < min(nr_args, 6); i++)
+ 		emit_stx(prog, bytes_to_bpf_size(m->arg_size[i]),
+ 			 BPF_REG_FP,
+ 			 i == 5 ? X86_REG_R9 : BPF_REG_1 + i,
+ 			 -(stack_size - i * 8));
+ }
+ 
+ static void restore_regs(const struct btf_func_model *m, u8 **prog, int nr_args,
+ 			 int stack_size)
+ {
+ 	int i;
+ 
+ 	/* Restore function arguments from stack.
+ 	 * For a function that accepts two pointers the sequence will be:
+ 	 * EMIT4(0x48, 0x8B, 0x7D, 0xF0); mov rdi,QWORD PTR [rbp-0x10]
+ 	 * EMIT4(0x48, 0x8B, 0x75, 0xF8); mov rsi,QWORD PTR [rbp-0x8]
+ 	 */
+ 	for (i = 0; i < min(nr_args, 6); i++)
+ 		emit_ldx(prog, bytes_to_bpf_size(m->arg_size[i]),
+ 			 i == 5 ? X86_REG_R9 : BPF_REG_1 + i,
+ 			 BPF_REG_FP,
+ 			 -(stack_size - i * 8));
+ }
+ 
+ static int invoke_bpf(const struct btf_func_model *m, u8 **pprog,
+ 		      struct bpf_prog **progs, int prog_cnt, int stack_size)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0, i;
+ 
+ 	for (i = 0; i < prog_cnt; i++) {
+ 		if (emit_call(&prog, __bpf_prog_enter, prog))
+ 			return -EINVAL;
+ 		/* remember prog start time returned by __bpf_prog_enter */
+ 		emit_mov_reg(&prog, true, BPF_REG_6, BPF_REG_0);
+ 
+ 		/* arg1: lea rdi, [rbp - stack_size] */
+ 		EMIT4(0x48, 0x8D, 0x7D, -stack_size);
+ 		/* arg2: progs[i]->insnsi for interpreter */
+ 		if (!progs[i]->jited)
+ 			emit_mov_imm64(&prog, BPF_REG_2,
+ 				       (long) progs[i]->insnsi >> 32,
+ 				       (u32) (long) progs[i]->insnsi);
+ 		/* call JITed bpf program or interpreter */
+ 		if (emit_call(&prog, progs[i]->bpf_func, prog))
+ 			return -EINVAL;
+ 
+ 		/* arg1: mov rdi, progs[i] */
+ 		emit_mov_imm64(&prog, BPF_REG_1, (long) progs[i] >> 32,
+ 			       (u32) (long) progs[i]);
+ 		/* arg2: mov rsi, rbx <- start time in nsec */
+ 		emit_mov_reg(&prog, true, BPF_REG_2, BPF_REG_6);
+ 		if (emit_call(&prog, __bpf_prog_exit, prog))
+ 			return -EINVAL;
+ 	}
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ /* Example:
+  * __be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev);
+  * its 'struct btf_func_model' will be nr_args=2
+  * The assembly code when eth_type_trans is executing after trampoline:
+  *
+  * push rbp
+  * mov rbp, rsp
+  * sub rsp, 16                     // space for skb and dev
+  * push rbx                        // temp regs to pass start time
+  * mov qword ptr [rbp - 16], rdi   // save skb pointer to stack
+  * mov qword ptr [rbp - 8], rsi    // save dev pointer to stack
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time in bpf stats are enabled
+  * lea rdi, [rbp - 16]             // R1==ctx of bpf prog
+  * call addr_of_jited_FENTRY_prog
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rdi, qword ptr [rbp - 16]   // restore skb pointer from stack
+  * mov rsi, qword ptr [rbp - 8]    // restore dev pointer from stack
+  * pop rbx
+  * leave
+  * ret
+  *
+  * eth_type_trans has 5 byte nop at the beginning. These 5 bytes will be
+  * replaced with 'call generated_bpf_trampoline'. When it returns
+  * eth_type_trans will continue executing with original skb and dev pointers.
+  *
+  * The assembly code when eth_type_trans is called from trampoline:
+  *
+  * push rbp
+  * mov rbp, rsp
+  * sub rsp, 24                     // space for skb, dev, return value
+  * push rbx                        // temp regs to pass start time
+  * mov qword ptr [rbp - 24], rdi   // save skb pointer to stack
+  * mov qword ptr [rbp - 16], rsi   // save dev pointer to stack
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time if bpf stats are enabled
+  * lea rdi, [rbp - 24]             // R1==ctx of bpf prog
+  * call addr_of_jited_FENTRY_prog  // bpf prog can access skb and dev
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rdi, qword ptr [rbp - 24]   // restore skb pointer from stack
+  * mov rsi, qword ptr [rbp - 16]   // restore dev pointer from stack
+  * call eth_type_trans+5           // execute body of eth_type_trans
+  * mov qword ptr [rbp - 8], rax    // save return value
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time in bpf stats are enabled
+  * lea rdi, [rbp - 24]             // R1==ctx of bpf prog
+  * call addr_of_jited_FEXIT_prog   // bpf prog can access skb, dev, return value
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rax, qword ptr [rbp - 8]    // restore eth_type_trans's return value
+  * pop rbx
+  * leave
+  * add rsp, 8                      // skip eth_type_trans's frame
+  * ret                             // return to its caller
+  */
+ int arch_prepare_bpf_trampoline(void *image, void *image_end,
+ 				const struct btf_func_model *m, u32 flags,
+ 				struct bpf_prog **fentry_progs, int fentry_cnt,
+ 				struct bpf_prog **fexit_progs, int fexit_cnt,
+ 				void *orig_call)
+ {
+ 	int cnt = 0, nr_args = m->nr_args;
+ 	int stack_size = nr_args * 8;
+ 	u8 *prog;
+ 
+ 	/* x86-64 supports up to 6 arguments. 7+ can be added in the future */
+ 	if (nr_args > 6)
+ 		return -ENOTSUPP;
+ 
+ 	if ((flags & BPF_TRAMP_F_RESTORE_REGS) &&
+ 	    (flags & BPF_TRAMP_F_SKIP_FRAME))
+ 		return -EINVAL;
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG)
+ 		stack_size += 8; /* room for return value of orig_call */
+ 
+ 	if (flags & BPF_TRAMP_F_SKIP_FRAME)
+ 		/* skip patched call instruction and point orig_call to actual
+ 		 * body of the kernel function.
+ 		 */
+ 		orig_call += X86_PATCH_SIZE;
+ 
+ 	prog = image;
+ 
+ 	EMIT1(0x55);		 /* push rbp */
+ 	EMIT3(0x48, 0x89, 0xE5); /* mov rbp, rsp */
+ 	EMIT4(0x48, 0x83, 0xEC, stack_size); /* sub rsp, stack_size */
+ 	EMIT1(0x53);		 /* push rbx */
+ 
+ 	save_regs(m, &prog, nr_args, stack_size);
+ 
+ 	if (fentry_cnt)
+ 		if (invoke_bpf(m, &prog, fentry_progs, fentry_cnt, stack_size))
+ 			return -EINVAL;
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG) {
+ 		if (fentry_cnt)
+ 			restore_regs(m, &prog, nr_args, stack_size);
+ 
+ 		/* call original function */
+ 		if (emit_call(&prog, orig_call, prog))
+ 			return -EINVAL;
+ 		/* remember return value in a stack for bpf prog to access */
+ 		emit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);
+ 	}
+ 
+ 	if (fexit_cnt)
+ 		if (invoke_bpf(m, &prog, fexit_progs, fexit_cnt, stack_size))
+ 			return -EINVAL;
+ 
+ 	if (flags & BPF_TRAMP_F_RESTORE_REGS)
+ 		restore_regs(m, &prog, nr_args, stack_size);
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG)
+ 		/* restore original return value back into RAX */
+ 		emit_ldx(&prog, BPF_DW, BPF_REG_0, BPF_REG_FP, -8);
+ 
+ 	EMIT1(0x5B); /* pop rbx */
+ 	EMIT1(0xC9); /* leave */
+ 	if (flags & BPF_TRAMP_F_SKIP_FRAME)
+ 		/* skip our return address and return to parent */
+ 		EMIT4(0x48, 0x83, 0xC4, 8); /* add rsp, 8 */
+ 	EMIT1(0xC3); /* ret */
+ 	/* Make sure the trampoline generation logic doesn't overflow */
+ 	if (WARN_ON_ONCE(prog > (u8 *)image_end - BPF_INSN_SAFETY))
+ 		return -EFAULT;
+ 	return prog - (u8 *)image;
+ }
+ 
+ static int emit_cond_near_jump(u8 **pprog, void *func, void *ip, u8 jmp_cond)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 	s64 offset;
+ 
+ 	offset = func - (ip + 2 + 4);
+ 	if (!is_simm32(offset)) {
+ 		pr_err("Target %p is out of range\n", func);
+ 		return -EINVAL;
+ 	}
+ 	EMIT2_off32(0x0F, jmp_cond + 0x10, offset);
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static void emit_nops(u8 **pprog, unsigned int len)
+ {
+ 	unsigned int i, noplen;
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	while (len > 0) {
+ 		noplen = len;
+ 
+ 		if (noplen > ASM_NOP_MAX)
+ 			noplen = ASM_NOP_MAX;
+ 
+ 		for (i = 0; i < noplen; i++)
+ 			EMIT1(ideal_nops[noplen][i]);
+ 		len -= noplen;
+ 	}
+ 
+ 	*pprog = prog;
+ }
+ 
+ static int emit_fallback_jump(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int err = 0;
+ 
+ #ifdef CONFIG_RETPOLINE
+ 	/* Note that this assumes the the compiler uses external
+ 	 * thunks for indirect calls. Both clang and GCC use the same
+ 	 * naming convention for external thunks.
+ 	 */
+ 	err = emit_jump(&prog, __x86_indirect_thunk_rdx, prog);
+ #else
+ 	int cnt = 0;
+ 
+ 	EMIT2(0xFF, 0xE2);	/* jmp rdx */
+ #endif
+ 	*pprog = prog;
+ 	return err;
+ }
+ 
+ static int emit_bpf_dispatcher(u8 **pprog, int a, int b, s64 *progs)
+ {
+ 	u8 *jg_reloc, *jg_target, *prog = *pprog;
+ 	int pivot, err, jg_bytes = 1, cnt = 0;
+ 	s64 jg_offset;
+ 
+ 	if (a == b) {
+ 		/* Leaf node of recursion, i.e. not a range of indices
+ 		 * anymore.
+ 		 */
+ 		EMIT1(add_1mod(0x48, BPF_REG_3));	/* cmp rdx,func */
+ 		if (!is_simm32(progs[a]))
+ 			return -1;
+ 		EMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3),
+ 			    progs[a]);
+ 		err = emit_cond_near_jump(&prog,	/* je func */
+ 					  (void *)progs[a], prog,
+ 					  X86_JE);
+ 		if (err)
+ 			return err;
+ 
+ 		err = emit_fallback_jump(&prog);	/* jmp thunk/indirect */
+ 		if (err)
+ 			return err;
+ 
+ 		*pprog = prog;
+ 		return 0;
+ 	}
+ 
+ 	/* Not a leaf node, so we pivot, and recursively descend into
+ 	 * the lower and upper ranges.
+ 	 */
+ 	pivot = (b - a) / 2;
+ 	EMIT1(add_1mod(0x48, BPF_REG_3));		/* cmp rdx,func */
+ 	if (!is_simm32(progs[a + pivot]))
+ 		return -1;
+ 	EMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3), progs[a + pivot]);
+ 
+ 	if (pivot > 2) {				/* jg upper_part */
+ 		/* Require near jump. */
+ 		jg_bytes = 4;
+ 		EMIT2_off32(0x0F, X86_JG + 0x10, 0);
+ 	} else {
+ 		EMIT2(X86_JG, 0);
+ 	}
+ 	jg_reloc = prog;
+ 
+ 	err = emit_bpf_dispatcher(&prog, a, a + pivot,	/* emit lower_part */
+ 				  progs);
+ 	if (err)
+ 		return err;
+ 
+ 	/* From Intel 64 and IA-32 Architectures Optimization
+ 	 * Reference Manual, 3.4.1.4 Code Alignment, Assembly/Compiler
+ 	 * Coding Rule 11: All branch targets should be 16-byte
+ 	 * aligned.
+ 	 */
+ 	jg_target = PTR_ALIGN(prog, 16);
+ 	if (jg_target != prog)
+ 		emit_nops(&prog, jg_target - prog);
+ 	jg_offset = prog - jg_reloc;
+ 	emit_code(jg_reloc - jg_bytes, jg_offset, jg_bytes);
+ 
+ 	err = emit_bpf_dispatcher(&prog, a + pivot + 1,	/* emit upper_part */
+ 				  b, progs);
+ 	if (err)
+ 		return err;
+ 
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static int cmp_ips(const void *a, const void *b)
+ {
+ 	const s64 *ipa = a;
+ 	const s64 *ipb = b;
+ 
+ 	if (*ipa > *ipb)
+ 		return 1;
+ 	if (*ipa < *ipb)
+ 		return -1;
+ 	return 0;
+ }
+ 
+ int arch_prepare_bpf_dispatcher(void *image, s64 *funcs, int num_funcs)
+ {
+ 	u8 *prog = image;
+ 
+ 	sort(funcs, num_funcs, sizeof(funcs[0]), cmp_ips, NULL);
+ 	return emit_bpf_dispatcher(&prog, 0, num_funcs - 1, funcs);
+ }
+ 
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  struct x64_jit_data {
  	struct bpf_binary_header *header;
  	int *addrs;
diff --cc include/linux/bpf.h
index 602dd6841705,a7bfe8a388c6..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -15,8 -12,12 +15,14 @@@
  #include <linux/err.h>
  #include <linux/rbtree_latch.h>
  #include <linux/numa.h>
 -#include <linux/mm_types.h>
  #include <linux/wait.h>
  #include <linux/u64_stats_sync.h>
++<<<<<<< HEAD
++=======
+ #include <linux/refcount.h>
+ #include <linux/mutex.h>
+ #include <linux/module.h>
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  
  struct bpf_verifier_env;
  struct bpf_verifier_log;
@@@ -103,10 -105,12 +109,16 @@@ struct bpf_map 
  	u32 btf_key_type_id;
  	u32 btf_value_type_id;
  	struct btf *btf;
++<<<<<<< HEAD
 +	RH_KABI_BROKEN_INSERT(struct bpf_map_memory memory)
++=======
+ 	struct bpf_map_memory memory;
+ 	char name[BPF_OBJ_NAME_LEN];
+ 	u32 btf_vmlinux_value_type_id;
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  	bool unpriv_array;
 -	bool frozen; /* write-once; write-protected by freeze_mutex */
 -	/* 22 bytes hole */
 +	RH_KABI_FILL_HOLE(bool frozen) /* write-once */
 +	/* 48 bytes hole */
  
  	/* The 3rd and 4th cacheline with misc members to avoid false sharing
  	 * particularly with refcounting.
@@@ -389,13 -404,195 +402,199 @@@ struct bpf_prog_stats 
  	struct u64_stats_sync syncp;
  } __aligned(2 * sizeof(u64));
  
++<<<<<<< HEAD
++=======
+ struct btf_func_model {
+ 	u8 ret_size;
+ 	u8 nr_args;
+ 	u8 arg_size[MAX_BPF_FUNC_ARGS];
+ };
+ 
+ /* Restore arguments before returning from trampoline to let original function
+  * continue executing. This flag is used for fentry progs when there are no
+  * fexit progs.
+  */
+ #define BPF_TRAMP_F_RESTORE_REGS	BIT(0)
+ /* Call original function after fentry progs, but before fexit progs.
+  * Makes sense for fentry/fexit, normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_CALL_ORIG		BIT(1)
+ /* Skip current frame and return to parent.  Makes sense for fentry/fexit
+  * programs only. Should not be used with normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_SKIP_FRAME		BIT(2)
+ 
+ /* Different use cases for BPF trampoline:
+  * 1. replace nop at the function entry (kprobe equivalent)
+  *    flags = BPF_TRAMP_F_RESTORE_REGS
+  *    fentry = a set of programs to run before returning from trampoline
+  *
+  * 2. replace nop at the function entry (kprobe + kretprobe equivalent)
+  *    flags = BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_SKIP_FRAME
+  *    orig_call = fentry_ip + MCOUNT_INSN_SIZE
+  *    fentry = a set of program to run before calling original function
+  *    fexit = a set of program to run after original function
+  *
+  * 3. replace direct call instruction anywhere in the function body
+  *    or assign a function pointer for indirect call (like tcp_congestion_ops->cong_avoid)
+  *    With flags = 0
+  *      fentry = a set of programs to run before returning from trampoline
+  *    With flags = BPF_TRAMP_F_CALL_ORIG
+  *      orig_call = original callback addr or direct function addr
+  *      fentry = a set of program to run before calling original function
+  *      fexit = a set of program to run after original function
+  */
+ int arch_prepare_bpf_trampoline(void *image, void *image_end,
+ 				const struct btf_func_model *m, u32 flags,
+ 				struct bpf_prog **fentry_progs, int fentry_cnt,
+ 				struct bpf_prog **fexit_progs, int fexit_cnt,
+ 				void *orig_call);
+ /* these two functions are called from generated trampoline */
+ u64 notrace __bpf_prog_enter(void);
+ void notrace __bpf_prog_exit(struct bpf_prog *prog, u64 start);
+ 
+ enum bpf_tramp_prog_type {
+ 	BPF_TRAMP_FENTRY,
+ 	BPF_TRAMP_FEXIT,
+ 	BPF_TRAMP_MAX
+ };
+ 
+ struct bpf_trampoline {
+ 	/* hlist for trampoline_table */
+ 	struct hlist_node hlist;
+ 	/* serializes access to fields of this trampoline */
+ 	struct mutex mutex;
+ 	refcount_t refcnt;
+ 	u64 key;
+ 	struct {
+ 		struct btf_func_model model;
+ 		void *addr;
+ 		bool ftrace_managed;
+ 	} func;
+ 	/* list of BPF programs using this trampoline */
+ 	struct hlist_head progs_hlist[BPF_TRAMP_MAX];
+ 	/* Number of attached programs. A counter per kind. */
+ 	int progs_cnt[BPF_TRAMP_MAX];
+ 	/* Executable image of trampoline */
+ 	void *image;
+ 	u64 selector;
+ };
+ 
+ #define BPF_DISPATCHER_MAX 48 /* Fits in 2048B */
+ 
+ struct bpf_dispatcher_prog {
+ 	struct bpf_prog *prog;
+ 	refcount_t users;
+ };
+ 
+ struct bpf_dispatcher {
+ 	/* dispatcher mutex */
+ 	struct mutex mutex;
+ 	void *func;
+ 	struct bpf_dispatcher_prog progs[BPF_DISPATCHER_MAX];
+ 	int num_progs;
+ 	void *image;
+ 	u32 image_off;
+ };
+ 
+ static __always_inline unsigned int bpf_dispatcher_nopfunc(
+ 	const void *ctx,
+ 	const struct bpf_insn *insnsi,
+ 	unsigned int (*bpf_func)(const void *,
+ 				 const struct bpf_insn *))
+ {
+ 	return bpf_func(ctx, insnsi);
+ }
+ #ifdef CONFIG_BPF_JIT
+ struct bpf_trampoline *bpf_trampoline_lookup(u64 key);
+ int bpf_trampoline_link_prog(struct bpf_prog *prog);
+ int bpf_trampoline_unlink_prog(struct bpf_prog *prog);
+ void bpf_trampoline_put(struct bpf_trampoline *tr);
+ void *bpf_jit_alloc_exec_page(void);
+ #define BPF_DISPATCHER_INIT(name) {			\
+ 	.mutex = __MUTEX_INITIALIZER(name.mutex),	\
+ 	.func = &name##func,				\
+ 	.progs = {},					\
+ 	.num_progs = 0,					\
+ 	.image = NULL,					\
+ 	.image_off = 0					\
+ }
+ 
+ #define DEFINE_BPF_DISPATCHER(name)					\
+ 	noinline unsigned int name##func(				\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *))	\
+ 	{								\
+ 		return bpf_func(ctx, insnsi);				\
+ 	}								\
+ 	EXPORT_SYMBOL(name##func);			\
+ 	struct bpf_dispatcher name = BPF_DISPATCHER_INIT(name);
+ #define DECLARE_BPF_DISPATCHER(name)					\
+ 	unsigned int name##func(					\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *));	\
+ 	extern struct bpf_dispatcher name;
+ #define BPF_DISPATCHER_FUNC(name) name##func
+ #define BPF_DISPATCHER_PTR(name) (&name)
+ void bpf_dispatcher_change_prog(struct bpf_dispatcher *d, struct bpf_prog *from,
+ 				struct bpf_prog *to);
+ #else
+ static inline struct bpf_trampoline *bpf_trampoline_lookup(u64 key)
+ {
+ 	return NULL;
+ }
+ static inline int bpf_trampoline_link_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline int bpf_trampoline_unlink_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline void bpf_trampoline_put(struct bpf_trampoline *tr) {}
+ #define DEFINE_BPF_DISPATCHER(name)
+ #define DECLARE_BPF_DISPATCHER(name)
+ #define BPF_DISPATCHER_FUNC(name) bpf_dispatcher_nopfunc
+ #define BPF_DISPATCHER_PTR(name) NULL
+ static inline void bpf_dispatcher_change_prog(struct bpf_dispatcher *d,
+ 					      struct bpf_prog *from,
+ 					      struct bpf_prog *to) {}
+ #endif
+ 
+ struct bpf_func_info_aux {
+ 	bool unreliable;
+ };
+ 
+ enum bpf_jit_poke_reason {
+ 	BPF_POKE_REASON_TAIL_CALL,
+ };
+ 
+ /* Descriptor of pokes pointing /into/ the JITed image. */
+ struct bpf_jit_poke_descriptor {
+ 	void *ip;
+ 	union {
+ 		struct {
+ 			struct bpf_map *map;
+ 			u32 key;
+ 		} tail_call;
+ 	};
+ 	bool ip_stable;
+ 	u8 adj_off;
+ 	u16 reason;
+ };
+ 
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  struct bpf_prog_aux {
 -	atomic64_t refcnt;
 +	atomic_t refcnt;
  	u32 used_map_cnt;
  	u32 max_ctx_offset;
 -	u32 max_pkt_offset;
 -	u32 max_tp_access;
 +	/* not protected by KABI, safe to extend in the middle */
 +	RH_KABI_BROKEN_INSERT(u32 max_pkt_offset)
 +	RH_KABI_BROKEN_INSERT(u32 max_tp_access)
  	u32 stack_depth;
  	u32 id;
  	u32 func_cnt; /* used by non-func prog as the number of func progs */
@@@ -460,8 -669,80 +659,78 @@@ struct bpf_array_aux 
  	 */
  	enum bpf_prog_type type;
  	bool jited;
 -	/* Programs with direct jumps into programs part of this array. */
 -	struct list_head poke_progs;
 -	struct bpf_map *map;
 -	struct mutex poke_mutex;
 -	struct work_struct work;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_struct_ops_value;
+ struct btf_type;
+ struct btf_member;
+ 
+ #define BPF_STRUCT_OPS_MAX_NR_MEMBERS 64
+ struct bpf_struct_ops {
+ 	const struct bpf_verifier_ops *verifier_ops;
+ 	int (*init)(struct btf *btf);
+ 	int (*check_member)(const struct btf_type *t,
+ 			    const struct btf_member *member);
+ 	int (*init_member)(const struct btf_type *t,
+ 			   const struct btf_member *member,
+ 			   void *kdata, const void *udata);
+ 	int (*reg)(void *kdata);
+ 	void (*unreg)(void *kdata);
+ 	const struct btf_type *type;
+ 	const struct btf_type *value_type;
+ 	const char *name;
+ 	struct btf_func_model func_models[BPF_STRUCT_OPS_MAX_NR_MEMBERS];
+ 	u32 type_id;
+ 	u32 value_id;
+ };
+ 
+ #if defined(CONFIG_BPF_JIT) && defined(CONFIG_BPF_SYSCALL)
+ #define BPF_MODULE_OWNER ((void *)((0xeB9FUL << 2) + POISON_POINTER_DELTA))
+ const struct bpf_struct_ops *bpf_struct_ops_find(u32 type_id);
+ void bpf_struct_ops_init(struct btf *btf);
+ bool bpf_struct_ops_get(const void *kdata);
+ void bpf_struct_ops_put(const void *kdata);
+ int bpf_struct_ops_map_sys_lookup_elem(struct bpf_map *map, void *key,
+ 				       void *value);
+ static inline bool bpf_try_module_get(const void *data, struct module *owner)
+ {
+ 	if (owner == BPF_MODULE_OWNER)
+ 		return bpf_struct_ops_get(data);
+ 	else
+ 		return try_module_get(owner);
+ }
+ static inline void bpf_module_put(const void *data, struct module *owner)
+ {
+ 	if (owner == BPF_MODULE_OWNER)
+ 		bpf_struct_ops_put(data);
+ 	else
+ 		module_put(owner);
+ }
+ #else
+ static inline const struct bpf_struct_ops *bpf_struct_ops_find(u32 type_id)
+ {
+ 	return NULL;
+ }
+ static inline void bpf_struct_ops_init(struct btf *btf) { }
+ static inline bool bpf_try_module_get(const void *data, struct module *owner)
+ {
+ 	return try_module_get(owner);
+ }
+ static inline void bpf_module_put(const void *data, struct module *owner)
+ {
+ 	module_put(owner);
+ }
+ static inline int bpf_struct_ops_map_sys_lookup_elem(struct bpf_map *map,
+ 						     void *key,
+ 						     void *value)
+ {
+ 	return -EINVAL;
+ }
+ #endif
+ 
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  struct bpf_array {
  	struct bpf_map map;
  	u32 elem_size;
diff --cc include/linux/btf.h
index 55d43bc856be,881e9b76ef49..000000000000
--- a/include/linux/btf.h
+++ b/include/linux/btf.h
@@@ -5,7 -5,10 +5,9 @@@
  #define _LINUX_BTF_H 1
  
  #include <linux/types.h>
 -#include <uapi/linux/btf.h>
  
+ #define BTF_TYPE_EMIT(type) ((void)(type *)0)
+ 
  struct btf;
  struct btf_member;
  struct btf_type;
@@@ -52,6 -55,81 +54,84 @@@ bool btf_member_is_reg_int(const struc
  			   u32 expected_offset, u32 expected_size);
  int btf_find_spin_lock(const struct btf *btf, const struct btf_type *t);
  bool btf_type_is_void(const struct btf_type *t);
++<<<<<<< HEAD
++=======
+ s32 btf_find_by_name_kind(const struct btf *btf, const char *name, u8 kind);
+ const struct btf_type *btf_type_skip_modifiers(const struct btf *btf,
+ 					       u32 id, u32 *res_id);
+ const struct btf_type *btf_type_resolve_ptr(const struct btf *btf,
+ 					    u32 id, u32 *res_id);
+ const struct btf_type *btf_type_resolve_func_ptr(const struct btf *btf,
+ 						 u32 id, u32 *res_id);
+ const struct btf_type *
+ btf_resolve_size(const struct btf *btf, const struct btf_type *type,
+ 		 u32 *type_size, const struct btf_type **elem_type,
+ 		 u32 *total_nelems);
+ 
+ #define for_each_member(i, struct_type, member)			\
+ 	for (i = 0, member = btf_type_member(struct_type);	\
+ 	     i < btf_type_vlen(struct_type);			\
+ 	     i++, member++)
+ 
+ static inline bool btf_type_is_ptr(const struct btf_type *t)
+ {
+ 	return BTF_INFO_KIND(t->info) == BTF_KIND_PTR;
+ }
+ 
+ static inline bool btf_type_is_int(const struct btf_type *t)
+ {
+ 	return BTF_INFO_KIND(t->info) == BTF_KIND_INT;
+ }
+ 
+ static inline bool btf_type_is_enum(const struct btf_type *t)
+ {
+ 	return BTF_INFO_KIND(t->info) == BTF_KIND_ENUM;
+ }
+ 
+ static inline bool btf_type_is_typedef(const struct btf_type *t)
+ {
+ 	return BTF_INFO_KIND(t->info) == BTF_KIND_TYPEDEF;
+ }
+ 
+ static inline bool btf_type_is_func(const struct btf_type *t)
+ {
+ 	return BTF_INFO_KIND(t->info) == BTF_KIND_FUNC;
+ }
+ 
+ static inline bool btf_type_is_func_proto(const struct btf_type *t)
+ {
+ 	return BTF_INFO_KIND(t->info) == BTF_KIND_FUNC_PROTO;
+ }
+ 
+ static inline u16 btf_type_vlen(const struct btf_type *t)
+ {
+ 	return BTF_INFO_VLEN(t->info);
+ }
+ 
+ static inline bool btf_type_kflag(const struct btf_type *t)
+ {
+ 	return BTF_INFO_KFLAG(t->info);
+ }
+ 
+ static inline u32 btf_member_bit_offset(const struct btf_type *struct_type,
+ 					const struct btf_member *member)
+ {
+ 	return btf_type_kflag(struct_type) ? BTF_MEMBER_BIT_OFFSET(member->offset)
+ 					   : member->offset;
+ }
+ 
+ static inline u32 btf_member_bitfield_size(const struct btf_type *struct_type,
+ 					   const struct btf_member *member)
+ {
+ 	return btf_type_kflag(struct_type) ? BTF_MEMBER_BITFIELD_SIZE(member->offset)
+ 					   : 0;
+ }
+ 
+ static inline const struct btf_member *btf_type_member(const struct btf_type *t)
+ {
+ 	return (const struct btf_member *)(t + 1);
+ }
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  
  #ifdef CONFIG_BPF_SYSCALL
  const struct btf_type *btf_type_by_id(const struct btf *btf, u32 type_id);
diff --cc include/uapi/linux/bpf.h
index f26f93a554f1,38059880963e..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -136,7 -135,8 +136,12 @@@ enum bpf_map_type 
  	BPF_MAP_TYPE_QUEUE,
  	BPF_MAP_TYPE_STACK,
  	BPF_MAP_TYPE_SK_STORAGE,
++<<<<<<< HEAD
 +#endif /* __GENKSYMS__ */
++=======
+ 	BPF_MAP_TYPE_DEVMAP_HASH,
+ 	BPF_MAP_TYPE_STRUCT_OPS,
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  };
  
  /* Note that tracing related programs such as
diff --cc kernel/bpf/btf.c
index fef5ecb3622e,81d9cf75cacd..000000000000
--- a/kernel/bpf/btf.c
+++ b/kernel/bpf/btf.c
@@@ -461,30 -500,6 +461,33 @@@ static const char *btf_int_encoding_str
  		return "UNKN";
  }
  
++<<<<<<< HEAD
 +static u16 btf_type_vlen(const struct btf_type *t)
 +{
 +	return BTF_INFO_VLEN(t->info);
 +}
 +
 +static bool btf_type_kflag(const struct btf_type *t)
 +{
 +	return BTF_INFO_KFLAG(t->info);
 +}
 +
 +static u32 btf_member_bit_offset(const struct btf_type *struct_type,
 +			     const struct btf_member *member)
 +{
 +	return btf_type_kflag(struct_type) ? BTF_MEMBER_BIT_OFFSET(member->offset)
 +					   : member->offset;
 +}
 +
 +static u32 btf_member_bitfield_size(const struct btf_type *struct_type,
 +				    const struct btf_member *member)
 +{
 +	return btf_type_kflag(struct_type) ? BTF_MEMBER_BITFIELD_SIZE(member->offset)
 +					   : 0;
 +}
 +
++=======
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  static u32 btf_type_int(const struct btf_type *t)
  {
  	return *(u32 *)(t + 1);
diff --cc kernel/bpf/syscall.c
index 4aa6a47aa4b8,f9db72a96ec0..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -603,17 -668,21 +611,20 @@@ static int map_create(union bpf_attr *a
  	if (err)
  		goto free_map;
  
 -	atomic64_set(&map->refcnt, 1);
 -	atomic64_set(&map->usercnt, 1);
 -	mutex_init(&map->freeze_mutex);
 +	atomic_set(&map->refcnt, 1);
 +	atomic_set(&map->usercnt, 1);
  
- 	if (attr->btf_key_type_id || attr->btf_value_type_id) {
+ 	map->spin_lock_off = -EINVAL;
+ 	if (attr->btf_key_type_id || attr->btf_value_type_id ||
+ 	    /* Even the map's value is a kernel's struct,
+ 	     * the bpf_prog.o must have BTF to begin with
+ 	     * to figure out the corresponding kernel's
+ 	     * counter part.  Thus, attr->btf_fd has
+ 	     * to be valid also.
+ 	     */
+ 	    attr->btf_vmlinux_value_type_id) {
  		struct btf *btf;
  
- 		if (!attr->btf_value_type_id) {
- 			err = -EINVAL;
- 			goto free_map;
- 		}
- 
  		btf = btf_get_by_fd(attr->btf_fd);
  		if (IS_ERR(btf)) {
  			err = PTR_ERR(btf);
@@@ -955,9 -1017,14 +969,10 @@@ static int map_update_elem(union bpf_at
  		goto out;
  	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP ||
  		   map->map_type == BPF_MAP_TYPE_SOCKHASH ||
- 		   map->map_type == BPF_MAP_TYPE_SOCKMAP) {
+ 		   map->map_type == BPF_MAP_TYPE_SOCKMAP ||
+ 		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
  		err = map->ops->map_update_elem(map, key, value, attr->flags);
  		goto out;
 -	} else if (IS_FD_PROG_ARRAY(map)) {
 -		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
 -						   attr->flags);
 -		goto out;
  	}
  
  	/* must increment bpf_prog_active to avoid kprobe+bpf triggering from
@@@ -1040,6 -1107,11 +1055,14 @@@ static int map_delete_elem(union bpf_at
  	if (bpf_map_is_dev_bound(map)) {
  		err = bpf_map_offload_delete_elem(map, key);
  		goto out;
++<<<<<<< HEAD
++=======
+ 	} else if (IS_FD_PROG_ARRAY(map) ||
+ 		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+ 		/* These maps require sleepable context */
+ 		err = map->ops->map_delete_elem(map, key);
+ 		goto out;
++>>>>>>> 85d33df357b6 (bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS)
  	}
  
  	preempt_disable();
* Unmerged path kernel/bpf/bpf_struct_ops.c
* Unmerged path kernel/bpf/trampoline.c
* Unmerged path arch/x86/net/bpf_jit_comp.c
* Unmerged path include/linux/bpf.h
diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index eec5aeeeaf92..4c5727bf319e 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -77,3 +77,6 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_REUSEPORT_SOCKARRAY, reuseport_array_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_QUEUE, queue_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_STACK, stack_map_ops)
+#if defined(CONFIG_BPF_JIT)
+BPF_MAP_TYPE(BPF_MAP_TYPE_STRUCT_OPS, bpf_struct_ops_map_ops)
+#endif
* Unmerged path include/linux/btf.h
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/bpf/bpf_struct_ops.c
* Unmerged path kernel/bpf/btf.c
diff --git a/kernel/bpf/map_in_map.c b/kernel/bpf/map_in_map.c
index 464168da2070..30366819bca5 100644
--- a/kernel/bpf/map_in_map.c
+++ b/kernel/bpf/map_in_map.c
@@ -25,7 +25,8 @@ struct bpf_map *bpf_map_meta_alloc(int inner_map_ufd)
 	 */
 	if (inner_map->map_type == BPF_MAP_TYPE_PROG_ARRAY ||
 	    inner_map->map_type == BPF_MAP_TYPE_CGROUP_STORAGE ||
-	    inner_map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
+	    inner_map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE ||
+	    inner_map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
 		fdput(f);
 		return ERR_PTR(-ENOTSUPP);
 	}
* Unmerged path kernel/bpf/syscall.c
* Unmerged path kernel/bpf/trampoline.c
diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c
index a2c1dcade9ad..3f4bfa220be0 100644
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -8155,6 +8155,11 @@ static int check_map_prog_compatibility(struct bpf_verifier_env *env,
 		return -EINVAL;
 	}
 
+	if (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+		verbose(env, "bpf_struct_ops map cannot be used in prog\n");
+		return -EINVAL;
+	}
+
 	return 0;
 }
 
