veth: rely on peer veth_rq for ndo_xdp_xmit accounting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Lorenzo Bianconi <lorenzo@kernel.org>
commit 5fe6e56776ba6979844f9ef5361f43852a7b16a2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5fe6e567.failed

Rely on 'remote' veth_rq to account ndo_xdp_xmit ethtool counters.
Move XDP_TX accounting to veth_xdp_flush_bq routine.
Remove 'rx' prefix in rx xdp ethool counters

	Signed-off-by: Lorenzo Bianconi <lorenzo@kernel.org>
	Acked-by: Toshiaki Makita <toshiaki.makita1@gmail.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 5fe6e56776ba6979844f9ef5361f43852a7b16a2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/veth.c
diff --cc drivers/net/veth.c
index 013947869d96,aece0e5eec8c..000000000000
--- a/drivers/net/veth.c
+++ b/drivers/net/veth.c
@@@ -33,16 -34,23 +33,32 @@@
  #define VETH_RING_SIZE		256
  #define VETH_XDP_HEADROOM	(XDP_PACKET_HEADROOM + NET_IP_ALIGN)
  
 +/* Separating two types of XDP xmit */
 +#define VETH_XDP_TX		BIT(0)
 +#define VETH_XDP_REDIR		BIT(1)
 +
  #define VETH_XDP_TX_BULK_SIZE	16
  
++<<<<<<< HEAD
++=======
+ struct veth_stats {
+ 	u64	rx_drops;
+ 	/* xdp */
+ 	u64	xdp_packets;
+ 	u64	xdp_bytes;
+ 	u64	xdp_redirect;
+ 	u64	xdp_drops;
+ 	u64	xdp_tx;
+ 	u64	xdp_tx_err;
+ 	u64	peer_tq_xdp_xmit;
+ 	u64	peer_tq_xdp_xmit_err;
+ };
+ 
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  struct veth_rq_stats {
 -	struct veth_stats	vs;
 +	u64			xdp_packets;
 +	u64			xdp_bytes;
 +	u64			xdp_drops;
  	struct u64_stats_sync	syncp;
  };
  
@@@ -84,7 -92,11 +100,15 @@@ struct veth_q_stat_desc 
  static const struct veth_q_stat_desc veth_rq_stats_desc[] = {
  	{ "xdp_packets",	VETH_RQ_STAT(xdp_packets) },
  	{ "xdp_bytes",		VETH_RQ_STAT(xdp_bytes) },
++<<<<<<< HEAD
 +	{ "xdp_drops",		VETH_RQ_STAT(xdp_drops) },
++=======
+ 	{ "drops",		VETH_RQ_STAT(rx_drops) },
+ 	{ "xdp_redirect",	VETH_RQ_STAT(xdp_redirect) },
+ 	{ "xdp_drops",		VETH_RQ_STAT(xdp_drops) },
+ 	{ "xdp_tx",		VETH_RQ_STAT(xdp_tx) },
+ 	{ "xdp_tx_errors",	VETH_RQ_STAT(xdp_tx_err) },
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  };
  
  #define VETH_RQ_STATS_LEN	ARRAY_SIZE(veth_rq_stats_desc)
@@@ -280,51 -322,42 +340,82 @@@ drop
  	return NETDEV_TX_OK;
  }
  
 -static u64 veth_stats_tx(struct net_device *dev, u64 *packets, u64 *bytes)
 +static u64 veth_stats_tx(struct pcpu_lstats *result, struct net_device *dev)
  {
  	struct veth_priv *priv = netdev_priv(dev);
 -
 +	int cpu;
 +
++<<<<<<< HEAD
 +	result->packets = 0;
 +	result->bytes = 0;
 +	for_each_possible_cpu(cpu) {
 +		struct pcpu_lstats *stats = per_cpu_ptr(dev->lstats, cpu);
 +		u64 packets, bytes;
++=======
+ 	dev_lstats_read(dev, packets, bytes);
+ 	return atomic64_read(&priv->dropped);
+ }
+ 
+ static void veth_stats_rx(struct veth_stats *result, struct net_device *dev)
+ {
+ 	struct veth_priv *priv = netdev_priv(dev);
+ 	int i;
+ 
+ 	result->peer_tq_xdp_xmit_err = 0;
+ 	result->xdp_packets = 0;
+ 	result->xdp_tx_err = 0;
+ 	result->xdp_bytes = 0;
+ 	result->rx_drops = 0;
+ 	for (i = 0; i < dev->num_rx_queues; i++) {
+ 		u64 packets, bytes, drops, xdp_tx_err, peer_tq_xdp_xmit_err;
+ 		struct veth_rq_stats *stats = &priv->rq[i].stats;
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
 +		unsigned int start;
 +
 +		do {
 +			start = u64_stats_fetch_begin_irq(&stats->syncp);
++<<<<<<< HEAD
 +			packets = stats->packets;
 +			bytes = stats->bytes;
 +		} while (u64_stats_fetch_retry_irq(&stats->syncp, start));
 +		result->packets += packets;
 +		result->bytes += bytes;
 +	}
 +	return atomic64_read(&priv->dropped);
 +}
 +
 +static void veth_stats_rx(struct veth_rq_stats *result, struct net_device *dev)
 +{
 +	struct veth_priv *priv = netdev_priv(dev);
 +	int i;
 +
 +	result->xdp_packets = 0;
 +	result->xdp_bytes = 0;
 +	result->xdp_drops = 0;
 +	for (i = 0; i < dev->num_rx_queues; i++) {
 +		struct veth_rq_stats *stats = &priv->rq[i].stats;
 +		u64 packets, bytes, drops;
  		unsigned int start;
  
  		do {
  			start = u64_stats_fetch_begin_irq(&stats->syncp);
 +			packets = stats->xdp_packets;
 +			bytes = stats->xdp_bytes;
 +			drops = stats->xdp_drops;
 +		} while (u64_stats_fetch_retry_irq(&stats->syncp, start));
++=======
+ 			peer_tq_xdp_xmit_err = stats->vs.peer_tq_xdp_xmit_err;
+ 			xdp_tx_err = stats->vs.xdp_tx_err;
+ 			packets = stats->vs.xdp_packets;
+ 			bytes = stats->vs.xdp_bytes;
+ 			drops = stats->vs.rx_drops;
+ 		} while (u64_stats_fetch_retry_irq(&stats->syncp, start));
+ 		result->peer_tq_xdp_xmit_err += peer_tq_xdp_xmit_err;
+ 		result->xdp_tx_err += xdp_tx_err;
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  		result->xdp_packets += packets;
  		result->xdp_bytes += bytes;
 -		result->rx_drops += drops;
 +		result->xdp_drops += drops;
  	}
  }
  
@@@ -333,26 -366,29 +424,36 @@@ static void veth_get_stats64(struct net
  {
  	struct veth_priv *priv = netdev_priv(dev);
  	struct net_device *peer;
 -	struct veth_stats rx;
 -	u64 packets, bytes;
 +	struct veth_rq_stats rx;
 +	struct pcpu_lstats tx;
  
 -	tot->tx_dropped = veth_stats_tx(dev, &packets, &bytes);
 -	tot->tx_bytes = bytes;
 -	tot->tx_packets = packets;
 +	tot->tx_dropped = veth_stats_tx(&tx, dev);
 +	tot->tx_bytes = tx.bytes;
 +	tot->tx_packets = tx.packets;
  
  	veth_stats_rx(&rx, dev);
++<<<<<<< HEAD
 +	tot->rx_dropped = rx.xdp_drops;
++=======
+ 	tot->tx_dropped += rx.xdp_tx_err;
+ 	tot->rx_dropped = rx.rx_drops + rx.peer_tq_xdp_xmit_err;
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  	tot->rx_bytes = rx.xdp_bytes;
  	tot->rx_packets = rx.xdp_packets;
  
  	rcu_read_lock();
  	peer = rcu_dereference(priv->peer);
  	if (peer) {
 -		veth_stats_tx(peer, &packets, &bytes);
 -		tot->rx_bytes += bytes;
 -		tot->rx_packets += packets;
 +		tot->rx_dropped += veth_stats_tx(&tx, peer);
 +		tot->rx_bytes += tx.bytes;
 +		tot->rx_packets += tx.packets;
  
  		veth_stats_rx(&rx, peer);
++<<<<<<< HEAD
++=======
+ 		tot->tx_dropped += rx.peer_tq_xdp_xmit_err;
+ 		tot->rx_dropped += rx.xdp_tx_err;
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  		tot->tx_bytes += rx.xdp_bytes;
  		tot->tx_packets += rx.xdp_packets;
  	}
@@@ -389,24 -425,22 +490,40 @@@ static int veth_select_rxq(struct net_d
  }
  
  static int veth_xdp_xmit(struct net_device *dev, int n,
 -			 struct xdp_frame **frames,
 -			 u32 flags, bool ndo_xmit)
 +			 struct xdp_frame **frames, u32 flags)
  {
  	struct veth_priv *rcv_priv, *priv = netdev_priv(dev);
++<<<<<<< HEAD
 +	struct net_device *rcv;
 +	int i, ret, drops = n;
 +	unsigned int max_len;
 +	struct veth_rq *rq;
 +
 +	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK)) {
 +		ret = -EINVAL;
 +		goto drop;
 +	}
++=======
+ 	int i, ret = -ENXIO, drops = 0;
+ 	struct net_device *rcv;
+ 	unsigned int max_len;
+ 	struct veth_rq *rq;
  
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
+ 
+ 	rcu_read_lock();
  	rcv = rcu_dereference(priv->peer);
++<<<<<<< HEAD
 +	if (unlikely(!rcv)) {
 +		ret = -ENXIO;
 +		goto drop;
 +	}
++=======
+ 	if (unlikely(!rcv))
+ 		goto out;
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  
  	rcv_priv = netdev_priv(rcv);
  	rq = &rcv_priv->rq[veth_select_rxq(rcv)];
@@@ -438,17 -469,36 +552,48 @@@
  	if (flags & XDP_XMIT_FLUSH)
  		__veth_xdp_flush(rq);
  
 +	if (likely(!drops))
 +		return n;
 +
  	ret = n - drops;
++<<<<<<< HEAD
 +drop:
 +	atomic64_add(drops, &priv->dropped);
++=======
+ 	if (ndo_xmit) {
+ 		u64_stats_update_begin(&rq->stats.syncp);
+ 		rq->stats.vs.peer_tq_xdp_xmit += n - drops;
+ 		rq->stats.vs.peer_tq_xdp_xmit_err += drops;
+ 		u64_stats_update_end(&rq->stats.syncp);
+ 	}
+ 
+ out:
+ 	rcu_read_unlock();
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void veth_xdp_flush_bq(struct net_device *dev, struct veth_xdp_tx_bq *bq)
++=======
+ static int veth_ndo_xdp_xmit(struct net_device *dev, int n,
+ 			     struct xdp_frame **frames, u32 flags)
+ {
+ 	int err;
+ 
+ 	err = veth_xdp_xmit(dev, n, frames, flags, true);
+ 	if (err < 0) {
+ 		struct veth_priv *priv = netdev_priv(dev);
+ 
+ 		atomic64_add(n, &priv->dropped);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static void veth_xdp_flush_bq(struct veth_rq *rq, struct veth_xdp_tx_bq *bq)
++>>>>>>> 5fe6e56776ba (veth: rely on peer veth_rq for ndo_xdp_xmit accounting)
  {
  	int sent, i, err = 0;
  
@@@ -459,8 -509,13 +604,13 @@@
  		for (i = 0; i < bq->count; i++)
  			xdp_return_frame(bq->q[i]);
  	}
 -	trace_xdp_bulk_tx(rq->dev, sent, bq->count - sent, err);
 +	trace_xdp_bulk_tx(dev, sent, bq->count - sent, err);
  
+ 	u64_stats_update_begin(&rq->stats.syncp);
+ 	rq->stats.vs.xdp_tx += sent;
+ 	rq->stats.vs.xdp_tx_err += bq->count - sent;
+ 	u64_stats_update_end(&rq->stats.syncp);
+ 
  	bq->count = 0;
  }
  
* Unmerged path drivers/net/veth.c
