io_uring: fix async close() with f_op->flush()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit a93b33312f63ef6d5997f45d6fdf4de84c5396cc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/a93b3331.failed

First, io_close() misses filp_close() and io_cqring_add_event(), when
f_op->flush is defined. That's because in this case it will
io_queue_async_work() itself not grabbing files, so the corresponding
chunk in io_close_finish() won't be executed.

Second, when submitted through io_wq_submit_work(), it will do
filp_close() and *_add_event() twice: first inline in io_close(),
and the second one in call to io_close_finish() from io_close().
The second one will also fire, because it was submitted async through
generic path, and so have grabbed files.

And the last nice thing is to remove this weird pilgrimage with checking
work/old_work and casting it to nxt. Just use a helper instead.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit a93b33312f63ef6d5997f45d6fdf4de84c5396cc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index ffb8e9d82a6a,759301bdb19b..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1517,14 -2624,303 +1517,310 @@@ static int io_fsync(struct io_kiocb *re
  	if (force_nonblock)
  		return -EAGAIN;
  
 -	ret = build_open_flags(&req->open.how, &op);
 -	if (ret)
 -		goto err;
 +	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
 +				end > 0 ? end : LLONG_MAX,
 +				fsync_flags & IORING_FSYNC_DATASYNC);
  
++<<<<<<< HEAD
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
++=======
+ 	ret = get_unused_fd_flags(req->open.how.flags);
+ 	if (ret < 0)
+ 		goto err;
+ 
+ 	file = do_filp_open(req->open.dfd, req->open.filename, &op);
+ 	if (IS_ERR(file)) {
+ 		put_unused_fd(ret);
+ 		ret = PTR_ERR(file);
+ 	} else {
+ 		fsnotify_open(file);
+ 		fd_install(ret, file);
+ 	}
+ err:
+ 	putname(req->open.filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_openat(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		     bool force_nonblock)
+ {
+ 	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
+ 	return io_openat2(req, nxt, force_nonblock);
+ }
+ 
+ static int io_epoll_ctl_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	req->epoll.epfd = READ_ONCE(sqe->fd);
+ 	req->epoll.op = READ_ONCE(sqe->len);
+ 	req->epoll.fd = READ_ONCE(sqe->off);
+ 
+ 	if (ep_op_has_event(req->epoll.op)) {
+ 		struct epoll_event __user *ev;
+ 
+ 		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
+ 			return -EFAULT;
+ 	}
+ 
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_epoll_ctl(struct io_kiocb *req, struct io_kiocb **nxt,
+ 			bool force_nonblock)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	struct io_epoll *ie = &req->epoll;
+ 	int ret;
+ 
+ 	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
+ 	if (force_nonblock && ret == -EAGAIN)
+ 		return -EAGAIN;
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	if (sqe->ioprio || sqe->buf_index || sqe->off)
+ 		return -EINVAL;
+ 
+ 	req->madvise.addr = READ_ONCE(sqe->addr);
+ 	req->madvise.len = READ_ONCE(sqe->len);
+ 	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	struct io_madvise *ma = &req->madvise;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	ret = do_madvise(ma->addr, ma->len, ma->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->addr)
+ 		return -EINVAL;
+ 
+ 	req->fadvise.offset = READ_ONCE(sqe->off);
+ 	req->fadvise.len = READ_ONCE(sqe->len);
+ 	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ }
+ 
+ static int io_fadvise(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ 	struct io_fadvise *fa = &req->fadvise;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		switch (fa->advice) {
+ 		case POSIX_FADV_NORMAL:
+ 		case POSIX_FADV_RANDOM:
+ 		case POSIX_FADV_SEQUENTIAL:
+ 			break;
+ 		default:
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	unsigned lookup_flags;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.mask = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	req->open.how.flags = READ_ONCE(sqe->statx_flags);
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
+ 		return -EINVAL;
+ 
+ 	req->open.filename = getname_flags(fname, lookup_flags, NULL);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_statx(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		    bool force_nonblock)
+ {
+ 	struct io_open *ctx = &req->open;
+ 	unsigned lookup_flags;
+ 	struct path path;
+ 	struct kstat stat;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
+ 		return -EINVAL;
+ 
+ retry:
+ 	/* filename_lookup() drops it, keep a reference */
+ 	ctx->filename->refcnt++;
+ 
+ 	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
+ 				NULL);
+ 	if (ret)
+ 		goto err;
+ 
+ 	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
+ 	path_put(&path);
+ 	if (retry_estale(ret, lookup_flags)) {
+ 		lookup_flags |= LOOKUP_REVAL;
+ 		goto retry;
+ 	}
+ 	if (!ret)
+ 		ret = cp_statx(&stat, ctx->buffer);
+ err:
+ 	putname(ctx->filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * If we queue this for async, it must not be cancellable. That would
+ 	 * leave the 'file' in an undeterminate state.
+ 	 */
+ 	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
+ 
+ 	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
+ 	    sqe->rw_flags || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->close.fd = READ_ONCE(sqe->fd);
+ 	if (req->file->f_op == &io_uring_fops ||
+ 	    req->close.fd == req->ctx->ring_fd)
+ 		return -EBADF;
+ 
+ 	return 0;
+ }
+ 
+ /* only called when __close_fd_get_file() is done */
+ static void __io_close_finish(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	int ret;
+ 
+ 	ret = filp_close(req->close.put_file, req->work.files);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	fput(req->close.put_file);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_close_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	__io_close_finish(req, &nxt);
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_close(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		    bool force_nonblock)
+ {
+ 	int ret;
+ 
+ 	req->close.put_file = NULL;
+ 	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* if the file has a flush method, be safe and punt to async */
+ 	if (req->close.put_file->f_op->flush && !io_wq_current_is_worker())
+ 		goto eagain;
+ 
+ 	/*
+ 	 * No ->flush(), safely close from here and just punt the
+ 	 * fput() to async context.
+ 	 */
+ 	__io_close_finish(req, nxt);
+ 	return 0;
+ eagain:
+ 	req->work.func = io_close_finish;
+ 	/*
+ 	 * Do manual async queue here to avoid grabbing files - we don't
+ 	 * need the files, and it'll cause io_close_finish() to close
+ 	 * the file again and cause a double CQE entry for this request
+ 	 */
+ 	io_queue_async_work(req);
++>>>>>>> a93b33312f63 (io_uring: fix async close() with f_op->flush())
  	return 0;
  }
  
* Unmerged path fs/io_uring.c
