io_uring: ensure async punted sendmsg/recvmsg requests copy data

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 03b1230ca12a12e045d83b0357792075bf94a1e0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/03b1230c.failed

Just like commit f67676d160c6 for read/write requests, this one ensures
that the msghdr data is fully copied if we need to punt a recvmsg or
sendmsg system call to async context.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 03b1230ca12a12e045d83b0357792075bf94a1e0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 671f4f0982a1,2700382ebcc7..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -305,7 -292,42 +305,46 @@@ struct io_poll_iocb 
  	__poll_t			events;
  	bool				done;
  	bool				canceled;
++<<<<<<< HEAD
 +	struct wait_queue_entry		wait;
++=======
+ 	struct wait_queue_entry		*wait;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	struct io_timeout_data		*data;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	struct io_uring_sqe		sqe;
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 	};
++>>>>>>> 03b1230ca12a (io_uring: ensure async punted sendmsg/recvmsg requests copy data)
  };
  
  /*
@@@ -1577,12 -1999,104 +1616,112 @@@ static int io_sync_file_range(struct io
  	return 0;
  }
  
++<<<<<<< HEAD
 +#if defined(CONFIG_NET)
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
++=======
+ static int io_sendmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
++>>>>>>> 03b1230ca12a (io_uring: ensure async punted sendmsg/recvmsg requests copy data)
  {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct user_msghdr __user *msg;
+ 	unsigned flags;
+ 
+ 	flags = READ_ONCE(sqe->msg_flags);
+ 	msg = (struct user_msghdr __user *)(unsigned long) READ_ONCE(sqe->addr);
+ 	return sendmsg_copy_msghdr(&io->msg.msg, msg, flags, &io->msg.iov);
+ #else
+ 	return 0;
+ #endif
+ }
+ 
+ static int io_sendmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 		      struct io_kiocb **nxt, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct socket *sock;
+ 	int ret;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_async_ctx io, *copy;
+ 		struct sockaddr_storage addr;
+ 		struct msghdr *kmsg;
+ 		unsigned flags;
+ 
+ 		flags = READ_ONCE(sqe->msg_flags);
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
+ 		if (req->io) {
+ 			kmsg = &req->io->msg.msg;
+ 			kmsg->msg_name = &addr;
+ 		} else {
+ 			kmsg = &io.msg.msg;
+ 			kmsg->msg_name = &addr;
+ 			io.msg.iov = io.msg.fast_iov;
+ 			ret = io_sendmsg_prep(req, &io);
+ 			if (ret)
+ 				goto out;
+ 		}
+ 
+ 		ret = __sys_sendmsg_sock(sock, kmsg, flags);
+ 		if (force_nonblock && ret == -EAGAIN) {
+ 			copy = kmalloc(sizeof(*copy), GFP_KERNEL);
+ 			if (!copy) {
+ 				ret = -ENOMEM;
+ 				goto out;
+ 			}
+ 			memcpy(&copy->msg, &io.msg, sizeof(copy->msg));
+ 			req->io = copy;
+ 			memcpy(&req->io->sqe, req->sqe, sizeof(*req->sqe));
+ 			req->sqe = &req->io->sqe;
+ 			return ret;
+ 		}
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
+ 	}
+ 
+ out:
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_recvmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct user_msghdr __user *msg;
+ 	unsigned flags;
+ 
+ 	flags = READ_ONCE(sqe->msg_flags);
+ 	msg = (struct user_msghdr __user *)(unsigned long) READ_ONCE(sqe->addr);
+ 	return recvmsg_copy_msghdr(&io->msg.msg, msg, flags, &io->msg.uaddr,
+ 					&io->msg.iov);
+ #else
+ 	return 0;
+ #endif
+ }
+ 
+ static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 		      struct io_kiocb **nxt, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
  	struct socket *sock;
  	int ret;
  
@@@ -1602,35 -2119,109 +1744,65 @@@
  
  		msg = (struct user_msghdr __user *) (unsigned long)
  			READ_ONCE(sqe->addr);
+ 		if (req->io) {
+ 			kmsg = &req->io->msg.msg;
+ 			kmsg->msg_name = &addr;
+ 		} else {
+ 			kmsg = &io.msg.msg;
+ 			kmsg->msg_name = &addr;
+ 			io.msg.iov = io.msg.fast_iov;
+ 			ret = io_recvmsg_prep(req, &io);
+ 			if (ret)
+ 				goto out;
+ 		}
  
- 		ret = fn(sock, msg, flags);
- 		if (force_nonblock && ret == -EAGAIN)
+ 		ret = __sys_recvmsg_sock(sock, kmsg, msg, io.msg.uaddr, flags);
+ 		if (force_nonblock && ret == -EAGAIN) {
+ 			copy = kmalloc(sizeof(*copy), GFP_KERNEL);
+ 			if (!copy) {
+ 				ret = -ENOMEM;
+ 				goto out;
+ 			}
+ 			memcpy(copy, &io, sizeof(*copy));
+ 			req->io = copy;
+ 			memcpy(&req->io->sqe, req->sqe, sizeof(*req->sqe));
+ 			req->sqe = &req->io->sqe;
  			return ret;
+ 		}
  		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 -	}
 -
 -out:
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0 && (req->flags & REQ_F_LINK))
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_put_req_find_next(req, nxt);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_accept(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		     struct io_kiocb **nxt, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct sockaddr __user *addr;
 -	int __user *addr_len;
 -	unsigned file_flags;
 -	int flags, ret;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -
 -	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
 -	addr_len = (int __user *) (unsigned long) READ_ONCE(sqe->addr2);
 -	flags = READ_ONCE(sqe->accept_flags);
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	ret = __sys_accept4_file(req->file, file_flags, addr, addr_len, flags);
 -	if (ret == -EAGAIN && force_nonblock) {
 -		req->work.flags |= IO_WQ_WORK_NEEDS_FILES;
 -		return -EAGAIN;
 -	}
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -	if (ret < 0 && (req->flags & REQ_F_LINK))
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 +			ret = -EINTR;
 +	}
 +
++<<<<<<< HEAD
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
  	return 0;
 +}
 +#endif
 +
 +static int io_sendmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		      bool force_nonblock)
 +{
 +#if defined(CONFIG_NET)
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_sendmsg_sock);
  #else
  	return -EOPNOTSUPP;
  #endif
  }
  
 -static int io_connect(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		      struct io_kiocb **nxt, bool force_nonblock)
 +static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		      bool force_nonblock)
  {
  #if defined(CONFIG_NET)
 -	struct sockaddr __user *addr;
 -	unsigned file_flags;
 -	int addr_len, ret;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 -
 -	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
 -	addr_len = READ_ONCE(sqe->addr2);
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	ret = __sys_connect_file(req->file, addr, addr_len, file_flags);
 -	if (ret == -EAGAIN && force_nonblock)
 -		return -EAGAIN;
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_recvmsg_sock);
++=======
++out:
++	io_cqring_add_event(req, ret);
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
 -	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
++>>>>>>> 03b1230ca12a (io_uring: ensure async punted sendmsg/recvmsg requests copy data)
  #else
  	return -EOPNOTSUPP;
  #endif
@@@ -1853,16 -2521,342 +2025,343 @@@ static int io_poll_add(struct io_kiocb 
  	return ipt.error;
  }
  
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 +	struct io_uring_sqe *sqe_copy;
  
++<<<<<<< HEAD
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
++=======
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * We could be racing with timeout deletion. If the list is empty,
+ 	 * then timeout lookup already found it and will be handling it.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		struct io_kiocb *prev;
+ 
+ 		/*
+ 		 * Adjust the reqs sequence before the current one because it
+ 		 * will consume a slot in the cq_ring and the the cq_tail
+ 		 * pointer will be increased, otherwise other timeout reqs may
+ 		 * return in advance without waiting for enough wait_nr.
+ 		 */
+ 		prev = req;
+ 		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 			prev->sequence++;
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	io_cqring_fill_event(req, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	if (req->flags & REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req(req);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
+ {
+ 	struct io_kiocb *req;
+ 	int ret = -ENOENT;
+ 
+ 	list_for_each_entry(req, &ctx->timeout_list, list) {
+ 		if (user_data == req->user_data) {
+ 			list_del_init(&req->list);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (ret == -ENOENT)
+ 		return ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->timeout.data->timer);
+ 	if (ret == -1)
+ 		return -EALREADY;
+ 
+ 	if (req->flags & REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_cqring_fill_event(req, -ECANCELED);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ /*
+  * Remove or update an existing timeout command
+  */
+ static int io_timeout_remove(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	unsigned flags;
+ 	int ret;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags)
+ 		return -EINVAL;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	ret = io_timeout_cancel(ctx, READ_ONCE(sqe->addr));
+ 
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	io_cqring_ev_posted(ctx);
+ 	if (ret < 0 && req->flags & REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_setup(struct io_kiocb *req)
+ {
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct io_timeout_data *data;
+ 	unsigned flags;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 
+ 	data = kzalloc(sizeof(struct io_timeout_data), GFP_KERNEL);
+ 	if (!data)
+ 		return -ENOMEM;
+ 	data->req = req;
+ 	req->timeout.data = data;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	if (flags & IORING_TIMEOUT_ABS)
+ 		data->mode = HRTIMER_MODE_ABS;
+ 	else
+ 		data->mode = HRTIMER_MODE_REL;
+ 
+ 	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
+ 	return 0;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_timeout_data *data;
+ 	struct list_head *entry;
+ 	unsigned span = 0;
+ 	int ret;
+ 
+ 	ret = io_timeout_setup(req);
+ 	/* common setup allows flags (like links) set, we don't */
+ 	if (!ret && sqe->flags)
+ 		ret = -EINVAL;
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied. If it isn't set, then this is
+ 	 * a pure timeout request, sequence isn't used.
+ 	 */
+ 	count = READ_ONCE(sqe->off);
+ 	if (!count) {
+ 		req->flags |= REQ_F_TIMEOUT_NOSEQ;
+ 		spin_lock_irq(&ctx->completion_lock);
+ 		entry = ctx->timeout_list.prev;
+ 		goto add;
+ 	}
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	req->timeout.data->seq_offset = count;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 		u32 nxt_offset = nxt->timeout.data->seq_offset;
+ 
+ 		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
+ 			continue;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt_offset + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ add:
+ 	list_add(&req->list, entry);
+ 	data = req->timeout.data;
+ 	data->timer.function = io_timeout_fn;
+ 	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	return 0;
+ }
+ 
+ static bool io_cancel_cb(struct io_wq_work *work, void *data)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	return req->user_data == (unsigned long) data;
+ }
+ 
+ static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
+ {
+ 	enum io_wq_cancel cancel_ret;
+ 	int ret = 0;
+ 
+ 	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
+ 	switch (cancel_ret) {
+ 	case IO_WQ_CANCEL_OK:
+ 		ret = 0;
+ 		break;
+ 	case IO_WQ_CANCEL_RUNNING:
+ 		ret = -EALREADY;
+ 		break;
+ 	case IO_WQ_CANCEL_NOTFOUND:
+ 		ret = -ENOENT;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
+ 				     struct io_kiocb *req, __u64 sqe_addr,
+ 				     struct io_kiocb **nxt, int success_ret)
+ {
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
+ 	if (ret != -ENOENT) {
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		goto done;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	ret = io_timeout_cancel(ctx, sqe_addr);
+ 	if (ret != -ENOENT)
+ 		goto done;
+ 	ret = io_poll_cancel(ctx, sqe_addr);
+ done:
+ 	if (!ret)
+ 		ret = success_ret;
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static int io_async_cancel(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			   struct io_kiocb **nxt)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
+ 	    sqe->cancel_flags)
+ 		return -EINVAL;
+ 
+ 	io_async_find_and_cancel(ctx, req, READ_ONCE(sqe->addr), nxt, 0);
+ 	return 0;
+ }
+ 
+ static int io_req_defer_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
+ 	struct iov_iter iter;
+ 	ssize_t ret;
+ 
+ 	memcpy(&io->sqe, req->sqe, sizeof(io->sqe));
+ 	req->sqe = &io->sqe;
+ 
+ 	switch (io->sqe.opcode) {
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 		ret = io_read_prep(req, &iovec, &iter, true);
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 		ret = io_write_prep(req, &iovec, &iter, true);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 		ret = io_sendmsg_prep(req, io);
+ 		break;
+ 	case IORING_OP_RECVMSG:
+ 		ret = io_recvmsg_prep(req, io);
+ 		break;
+ 	default:
+ 		req->io = io;
+ 		return 0;
+ 	}
+ 
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	req->io = io;
+ 	io_req_map_io(req, ret, iovec, inline_vecs, &iter);
+ 	return 0;
+ }
+ 
+ static int io_req_defer(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_async_ctx *io;
+ 	int ret;
+ 
+ 	/* Still need defer if there is pending req in defer list. */
+ 	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
++>>>>>>> 03b1230ca12a (io_uring: ensure async punted sendmsg/recvmsg requests copy data)
  		return 0;
  
 -	io = kmalloc(sizeof(*io), GFP_KERNEL);
 -	if (!io)
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
  		return -EAGAIN;
  
  	spin_lock_irq(&ctx->completion_lock);
* Unmerged path fs/io_uring.c
diff --git a/include/linux/socket.h b/include/linux/socket.h
index 29e5d23681ed..168dca188ecc 100644
--- a/include/linux/socket.h
+++ b/include/linux/socket.h
@@ -367,12 +367,19 @@ extern int __sys_recvmmsg(int fd, struct mmsghdr __user *mmsg, unsigned int vlen
 extern int __sys_sendmmsg(int fd, struct mmsghdr __user *mmsg,
 			  unsigned int vlen, unsigned int flags,
 			  bool forbid_cmsg_compat);
-extern long __sys_sendmsg_sock(struct socket *sock,
-			       struct user_msghdr __user *msg,
+extern long __sys_sendmsg_sock(struct socket *sock, struct msghdr *msg,
 			       unsigned int flags);
-extern long __sys_recvmsg_sock(struct socket *sock,
-			       struct user_msghdr __user *msg,
+extern long __sys_recvmsg_sock(struct socket *sock, struct msghdr *msg,
+			       struct user_msghdr __user *umsg,
+			       struct sockaddr __user *uaddr,
 			       unsigned int flags);
+extern int sendmsg_copy_msghdr(struct msghdr *msg,
+			       struct user_msghdr __user *umsg, unsigned flags,
+			       struct iovec **iov);
+extern int recvmsg_copy_msghdr(struct msghdr *msg,
+			       struct user_msghdr __user *umsg, unsigned flags,
+			       struct sockaddr __user **uaddr,
+			       struct iovec **iov);
 
 /* helpers which do the actual work for syscalls */
 extern int __sys_recvfrom(int fd, void __user *ubuf, size_t size,
diff --git a/net/socket.c b/net/socket.c
index 6d8a09541194..bfb84054b6f8 100644
--- a/net/socket.c
+++ b/net/socket.c
@@ -2309,9 +2309,9 @@ static int ____sys_sendmsg(struct socket *sock, struct msghdr *msg_sys,
 	return err;
 }
 
-static int sendmsg_copy_msghdr(struct msghdr *msg,
-			       struct user_msghdr __user *umsg, unsigned flags,
-			       struct iovec **iov)
+int sendmsg_copy_msghdr(struct msghdr *msg,
+			struct user_msghdr __user *umsg, unsigned flags,
+			struct iovec **iov)
 {
 	int err;
 
@@ -2353,27 +2353,14 @@ static int ___sys_sendmsg(struct socket *sock, struct user_msghdr __user *msg,
 /*
  *	BSD sendmsg interface
  */
-long __sys_sendmsg_sock(struct socket *sock, struct user_msghdr __user *umsg,
+long __sys_sendmsg_sock(struct socket *sock, struct msghdr *msg,
 			unsigned int flags)
 {
-	struct iovec iovstack[UIO_FASTIOV], *iov = iovstack;
-	struct sockaddr_storage address;
-	struct msghdr msg = { .msg_name = &address };
-	ssize_t err;
-
-	err = sendmsg_copy_msghdr(&msg, umsg, flags, &iov);
-	if (err)
-		return err;
 	/* disallow ancillary data requests from this path */
-	if (msg.msg_control || msg.msg_controllen) {
-		err = -EINVAL;
-		goto out;
-	}
+	if (msg->msg_control || msg->msg_controllen)
+		return -EINVAL;
 
-	err = ____sys_sendmsg(sock, &msg, flags, NULL, 0);
-out:
-	kfree(iov);
-	return err;
+	return ____sys_sendmsg(sock, msg, flags, NULL, 0);
 }
 
 long __sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned int flags,
@@ -2479,10 +2466,10 @@ SYSCALL_DEFINE4(sendmmsg, int, fd, struct mmsghdr __user *, mmsg,
 	return __sys_sendmmsg(fd, mmsg, vlen, flags, true);
 }
 
-static int recvmsg_copy_msghdr(struct msghdr *msg,
-			       struct user_msghdr __user *umsg, unsigned flags,
-			       struct sockaddr __user **uaddr,
-			       struct iovec **iov)
+int recvmsg_copy_msghdr(struct msghdr *msg,
+			struct user_msghdr __user *umsg, unsigned flags,
+			struct sockaddr __user **uaddr,
+			struct iovec **iov)
 {
 	ssize_t err;
 
@@ -2572,28 +2559,15 @@ static int ___sys_recvmsg(struct socket *sock, struct user_msghdr __user *msg,
  *	BSD recvmsg interface
  */
 
-long __sys_recvmsg_sock(struct socket *sock, struct user_msghdr __user *umsg,
-			unsigned int flags)
+long __sys_recvmsg_sock(struct socket *sock, struct msghdr *msg,
+			struct user_msghdr __user *umsg,
+			struct sockaddr __user *uaddr, unsigned int flags)
 {
-	struct iovec iovstack[UIO_FASTIOV], *iov = iovstack;
-	struct sockaddr_storage address;
-	struct msghdr msg = { .msg_name = &address };
-	struct sockaddr __user *uaddr;
-	ssize_t err;
-
-	err = recvmsg_copy_msghdr(&msg, umsg, flags, &uaddr, &iov);
-	if (err)
-		return err;
 	/* disallow ancillary data requests from this path */
-	if (msg.msg_control || msg.msg_controllen) {
-		err = -EINVAL;
-		goto out;
-	}
+	if (msg->msg_control || msg->msg_controllen)
+		return -EINVAL;
 
-	err = ____sys_recvmsg(sock, &msg, umsg, uaddr, flags, 0);
-out:
-	kfree(iov);
-	return err;
+	return ____sys_recvmsg(sock, msg, umsg, uaddr, flags, 0);
 }
 
 long __sys_recvmsg(int fd, struct user_msghdr __user *msg, unsigned int flags,
