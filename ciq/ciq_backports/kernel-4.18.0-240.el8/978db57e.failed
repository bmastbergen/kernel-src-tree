io_uring: make io_double_put_req() use normal completion path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 978db57e2c329fc612ff669cab9bf0007efd3ca3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/978db57e.failed

If we don't use the normal completion path, we may skip killing links
that should be errored and freed. Add __io_double_put_req() for use
within the completion path itself, other calls should just use
io_double_put_req().

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 978db57e2c329fc612ff669cab9bf0007efd3ca3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index f656b9c7fa46,1da103df2bfa..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -367,8 -378,12 +367,14 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
 -static void io_wq_submit_work(struct io_wq_work **workptr);
 -static void io_cqring_fill_event(struct io_kiocb *req, long res);
 +static void io_sq_wq_submit_work(struct work_struct *work);
  static void __io_free_req(struct io_kiocb *req);
++<<<<<<< HEAD
++=======
+ static void io_put_req(struct io_kiocb *req);
+ static void io_double_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
++>>>>>>> 978db57e2c32 (io_uring: make io_double_put_req() use normal completion path)
  
  static struct kmem_cache *req_cachep;
  
@@@ -665,15 -905,34 +671,27 @@@ static void io_fail_links(struct io_kio
  
  	while (!list_empty(&req->link_list)) {
  		link = list_first_entry(&req->link_list, struct io_kiocb, list);
 -		list_del_init(&link->list);
 +		list_del(&link->list);
  
++<<<<<<< HEAD
 +		io_cqring_add_event(req->ctx, link->user_data, -ECANCELED);
 +		__io_free_req(link);
++=======
+ 		trace_io_uring_fail_link(req, link);
+ 
+ 		if ((req->flags & REQ_F_LINK_TIMEOUT) &&
+ 		    link->submit.sqe->opcode == IORING_OP_LINK_TIMEOUT) {
+ 			io_link_cancel_timeout(link);
+ 		} else {
+ 			io_cqring_fill_event(link, -ECANCELED);
+ 			__io_double_put_req(link);
+ 		}
++>>>>>>> 978db57e2c32 (io_uring: make io_double_put_req() use normal completion path)
  	}
 -
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -	io_cqring_ev_posted(ctx);
  }
  
 -static void io_free_req_find_next(struct io_kiocb *req, struct io_kiocb **nxt)
 +static void io_free_req(struct io_kiocb *req)
  {
 -	if (likely(!(req->flags & REQ_F_LINK))) {
 -		__io_free_req(req);
 -		return;
 -	}
 -
  	/*
  	 * If LINK is set, we have dependent requests in this chain. If we
  	 * didn't fail this request, queue the first one up, moving any other
@@@ -696,11 -991,49 +714,46 @@@ static void io_put_req(struct io_kiocb 
  		io_free_req(req);
  }
  
++<<<<<<< HEAD
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
 +{
++=======
+ /*
+  * Must only be used if we don't need to care about links, usually from
+  * within the completion handling itself.
+  */
+ static void __io_double_put_req(struct io_kiocb *req)
+ {
+ 	/* drop both submit and complete references */
+ 	if (refcount_sub_and_test(2, &req->refs))
+ 		__io_free_req(req);
+ }
+ 
+ static void io_double_put_req(struct io_kiocb *req)
+ {
+ 	/* drop both submit and complete references */
+ 	if (refcount_sub_and_test(2, &req->refs))
+ 		io_free_req(req);
+ }
+ 
+ static unsigned io_cqring_events(struct io_ring_ctx *ctx, bool noflush)
+ {
+ 	struct io_rings *rings = ctx->rings;
+ 
+ 	/*
+ 	 * noflush == true is from the waitqueue handler, just ensure we wake
+ 	 * up the task, and the next invocation will flush the entries. We
+ 	 * cannot safely to it from here.
+ 	 */
+ 	if (noflush && !list_empty(&ctx->cq_overflow_list))
+ 		return -1U;
+ 
+ 	io_cqring_overflow_flush(ctx, false);
+ 
++>>>>>>> 978db57e2c32 (io_uring: make io_double_put_req() use normal completion path)
  	/* See comment at the top of this file */
  	smp_rmb();
 -	return READ_ONCE(rings->cq.tail) - READ_ONCE(rings->cq.head);
 -}
 -
 -static inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)
 -{
 -	struct io_rings *rings = ctx->rings;
 -
 -	/* make sure SQ entry isn't read before tail */
 -	return smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;
 +	return READ_ONCE(ring->r.tail) - READ_ONCE(ring->r.head);
  }
  
  /*
* Unmerged path fs/io_uring.c
