io_uring: IORING_OP_TIMEOUT support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 5262f567987d3c30052b22e78c35c2313d07b230
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5262f567.failed

There's been a few requests for functionality similar to io_getevents()
and epoll_wait(), where the user can specify a timeout for waiting on
events. I deliberately did not add support for this through the system
call initially to avoid overloading the args, but I can see that the use
cases for this are valid.

This adds support for IORING_OP_TIMEOUT. If a user wants to get woken
when waiting for events, simply submit one of these timeout commands
with your wait call (or before). This ensures that the application
sleeping on the CQ ring waiting for events will get woken. The timeout
command is passed in as a pointer to a struct timespec. Timeouts are
relative. The timeout command also includes a way to auto-cancel after
N events has passed.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 5262f567987d3c30052b22e78c35c2313d07b230)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 25a5cc4d4a8e,9d8e703bc851..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -242,8 -217,11 +243,9 @@@ struct io_ring_ctx 
  		struct wait_queue_head	cq_wait;
  		struct fasync_struct	*cq_fasync;
  		struct eventfd_ctx	*cq_ev_fd;
+ 		atomic_t		cq_timeouts;
  	} ____cacheline_aligned_in_smp;
  
 -	struct io_rings	*rings;
 -
  	/*
  	 * If used, fixed file set. Writers must ensure that ->refs is dead,
  	 * readers must ensure that ->refs is alive as long as the file* is
@@@ -430,13 -418,16 +442,16 @@@ static struct io_ring_ctx *io_ring_ctx_
  static inline bool io_sequence_defer(struct io_ring_ctx *ctx,
  				     struct io_kiocb *req)
  {
- 	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) != REQ_F_IO_DRAIN)
+ 	/* timeout requests always honor sequence */
+ 	if (!(req->flags & REQ_F_TIMEOUT) &&
+ 	    (req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) != REQ_F_IO_DRAIN)
  		return false;
  
 -	return req->sequence != ctx->cached_cq_tail + ctx->rings->sq_dropped;
 +	return req->sequence != ctx->cached_cq_tail + ctx->sq_ring->dropped;
  }
  
- static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
+ static struct io_kiocb *__io_get_deferred_req(struct io_ring_ctx *ctx,
+ 					      struct list_head *list)
  {
  	struct io_kiocb *req;
  
@@@ -452,13 -443,23 +467,23 @@@
  	return NULL;
  }
  
+ static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
+ {
+ 	return __io_get_deferred_req(ctx, &ctx->defer_list);
+ }
+ 
+ static struct io_kiocb *io_get_timeout_req(struct io_ring_ctx *ctx)
+ {
+ 	return __io_get_deferred_req(ctx, &ctx->timeout_list);
+ }
+ 
  static void __io_commit_cqring(struct io_ring_ctx *ctx)
  {
 -	struct io_rings *rings = ctx->rings;
 +	struct io_cq_ring *ring = ctx->cq_ring;
  
 -	if (ctx->cached_cq_tail != READ_ONCE(rings->cq.tail)) {
 +	if (ctx->cached_cq_tail != READ_ONCE(ring->r.tail)) {
  		/* order cqe stores with ring update */
 -		smp_store_release(&rings->cq.tail, ctx->cached_cq_tail);
 +		smp_store_release(&ring->r.tail, ctx->cached_cq_tail);
  
  		if (wq_has_sleeper(&ctx->cq_wait)) {
  			wake_up_interruptible(&ctx->cq_wait);
@@@ -2622,11 -2727,11 +2751,16 @@@ out
  static int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,
  			  const sigset_t __user *sig, size_t sigsz)
  {
++<<<<<<< HEAD
 +	struct io_cq_ring *ring = ctx->cq_ring;
 +	sigset_t ksigmask, sigsaved;
++=======
+ 	struct io_rings *rings = ctx->rings;
+ 	unsigned nr_timeouts;
++>>>>>>> 5262f567987d (io_uring: IORING_OP_TIMEOUT support)
  	int ret;
  
 -	if (io_cqring_events(rings) >= min_events)
 +	if (io_cqring_events(ring) >= min_events)
  		return 0;
  
  	if (sig) {
@@@ -2643,11 -2747,16 +2777,24 @@@
  			return ret;
  	}
  
++<<<<<<< HEAD
 +	ret = wait_event_interruptible(ctx->wait, io_cqring_events(ring) >= min_events);
 +
 +	if (sig)
 +		restore_user_sigmask(sig, &sigsaved, ret == -ERESTARTSYS);
 +
++=======
+ 	nr_timeouts = atomic_read(&ctx->cq_timeouts);
+ 	/*
+ 	 * Return if we have enough events, or if a timeout occured since
+ 	 * we started waiting. For timeouts, we always want to return to
+ 	 * userspace.
+ 	 */
+ 	ret = wait_event_interruptible(ctx->wait,
+ 				io_cqring_events(rings) >= min_events ||
+ 				atomic_read(&ctx->cq_timeouts) != nr_timeouts);
+ 	restore_saved_sigmask_unless(ret == -ERESTARTSYS);
++>>>>>>> 5262f567987d (io_uring: IORING_OP_TIMEOUT support)
  	if (ret == -ERESTARTSYS)
  		ret = -EINTR;
  
* Unmerged path fs/io_uring.c
diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 1e1652f25cc1..b79254dbfd55 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -28,6 +28,7 @@ struct io_uring_sqe {
 		__u16		poll_events;
 		__u32		sync_range_flags;
 		__u32		msg_flags;
+		__u32		timeout_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -61,6 +62,7 @@ struct io_uring_sqe {
 #define IORING_OP_SYNC_FILE_RANGE	8
 #define IORING_OP_SENDMSG	9
 #define IORING_OP_RECVMSG	10
+#define IORING_OP_TIMEOUT	11
 
 /*
  * sqe->fsync_flags
