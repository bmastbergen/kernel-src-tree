KVM: x86/mmu: Refactor THP adjust to prep for changing query

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 17eff01904f5f2fa12f4a56666637ce69ce5c645
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/17eff019.failed

Refactor transparent_hugepage_adjust() in preparation for walking the
host page tables to identify hugepage mappings, initially for THP pages,
and eventualy for HugeTLB and DAX-backed pages as well.  The latter
cases support 1gb pages, i.e. the adjustment logic needs access to the
max allowed level.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 17eff01904f5f2fa12f4a56666637ce69ce5c645)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 8246e5e487a1,64c28a39d8ef..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -3338,27 -3335,28 +3338,45 @@@ static void transparent_hugepage_adjust
  {
  	kvm_pfn_t pfn = *pfnp;
  	int level = *levelp;
+ 	kvm_pfn_t mask;
+ 
+ 	if (max_level == PT_PAGE_TABLE_LEVEL || level > PT_PAGE_TABLE_LEVEL)
+ 		return;
+ 
+ 	if (is_error_noslot_pfn(pfn) || kvm_is_reserved_pfn(pfn) ||
+ 	    kvm_is_zone_device_pfn(pfn))
+ 		return;
+ 
+ 	if (!kvm_is_transparent_hugepage(pfn))
+ 		return;
+ 
+ 	level = PT_DIRECTORY_LEVEL;
  
  	/*
- 	 * Check if it's a transparent hugepage. If this would be an
- 	 * hugetlbfs page, level wouldn't be set to
- 	 * PT_PAGE_TABLE_LEVEL and there would be no adjustment done
- 	 * here.
+ 	 * mmu_notifier_retry() was successful and mmu_lock is held, so
+ 	 * the pmd can't be split from under us.
  	 */
++<<<<<<< HEAD
 +	if (!is_error_noslot_pfn(pfn) && !kvm_is_reserved_pfn(pfn) &&
 +	    !kvm_is_zone_device_pfn(pfn) && level == PT_PAGE_TABLE_LEVEL &&
 +	    PageTransCompoundMap(pfn_to_page(pfn))) {
 +		unsigned long mask;
 +
 +		/*
 +		 * mmu_notifier_retry() was successful and mmu_lock is held, so
 +		 * the pmd can't be split from under us.
 +		 */
 +		*levelp = level = PT_DIRECTORY_LEVEL;
 +		mask = KVM_PAGES_PER_HPAGE(level) - 1;
 +		VM_BUG_ON((gfn & mask) != (pfn & mask));
 +		*pfnp = pfn & ~mask;
 +	}
++=======
+ 	*levelp = level;
+ 	mask = KVM_PAGES_PER_HPAGE(level) - 1;
+ 	VM_BUG_ON((gfn & mask) != (pfn & mask));
+ 	*pfnp = pfn & ~mask;
++>>>>>>> 17eff01904f5 (KVM: x86/mmu: Refactor THP adjust to prep for changing query)
  }
  
  static void disallowed_hugepage_adjust(struct kvm_shadow_walk_iterator it,
* Unmerged path arch/x86/kvm/mmu/mmu.c
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index 64abb6a4320c..b1d95f7aface 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -691,8 +691,7 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, gpa_t addr,
 	gfn = gw->gfn | ((addr & PT_LVL_OFFSET_MASK(gw->level)) >> PAGE_SHIFT);
 	base_gfn = gfn;
 
-	if (max_level > PT_PAGE_TABLE_LEVEL)
-		transparent_hugepage_adjust(vcpu, gw->gfn, &pfn, &hlevel);
+	transparent_hugepage_adjust(vcpu, gw->gfn, max_level, &pfn, &hlevel);
 
 	trace_kvm_mmu_spte_requested(addr, gw->level, pfn);
 
