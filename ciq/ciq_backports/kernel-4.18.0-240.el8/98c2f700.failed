sched/core: Simplify sched_class::pick_next_task()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 98c2f700edb413e4baa4a0368c5861d96211a775
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/98c2f700.failed

Now that the indirect class call never uses the last two arguments of
pick_next_task(), remove them.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: bsegall@google.com
	Cc: dietmar.eggemann@arm.com
	Cc: juri.lelli@redhat.com
	Cc: ktkhai@virtuozzo.com
	Cc: mgorman@suse.de
	Cc: qais.yousef@arm.com
	Cc: qperret@google.com
	Cc: rostedt@goodmis.org
	Cc: valentin.schneider@arm.com
	Cc: vincent.guittot@linaro.org
Link: https://lkml.kernel.org/r/20191108131909.660595546@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 98c2f700edb413e4baa4a0368c5861d96211a775)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/deadline.c
#	kernel/sched/fair.c
#	kernel/sched/idle.c
#	kernel/sched/rt.c
#	kernel/sched/sched.h
#	kernel/sched/stop_task.c
diff --cc kernel/sched/core.c
index 9079f8ce0c0d,513a4794ff36..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -3339,25 -3917,41 +3339,37 @@@ pick_next_task(struct rq *rq, struct ta
  		    prev->sched_class == &fair_sched_class) &&
  		   rq->nr_running == rq->cfs.h_nr_running)) {
  
 -		p = pick_next_task_fair(rq, prev, rf);
 +		p = fair_sched_class.pick_next_task(rq, prev, rf);
  		if (unlikely(p == RETRY_TASK))
 -			goto restart;
 +			goto again;
  
  		/* Assumes fair_sched_class->next == idle_sched_class */
++<<<<<<< HEAD
 +		if (unlikely(!p))
 +			p = idle_sched_class.pick_next_task(rq, prev, rf);
++=======
+ 		if (!p) {
+ 			put_prev_task(rq, prev);
+ 			p = pick_next_task_idle(rq);
+ 		}
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  
  		return p;
  	}
  
 -restart:
 -#ifdef CONFIG_SMP
 -	/*
 -	 * We must do the balancing pass before put_next_task(), such
 -	 * that when we release the rq->lock the task is in the same
 -	 * state as before we took rq->lock.
 -	 *
 -	 * We can terminate the balance pass as soon as we know there is
 -	 * a runnable task of @class priority or higher.
 -	 */
 -	for_class_range(class, prev->sched_class, &idle_sched_class) {
 -		if (class->balance(rq, prev, rf))
 -			break;
 -	}
 -#endif
 -
 -	put_prev_task(rq, prev);
 -
 +again:
  	for_each_class(class) {
++<<<<<<< HEAD
 +		p = class->pick_next_task(rq, prev, rf);
 +		if (p) {
 +			if (unlikely(p == RETRY_TASK))
 +				goto again;
++=======
+ 		p = class->pick_next_task(rq);
+ 		if (p)
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  			return p;
 +		}
  	}
  
  	/* The idle class should always have a runnable task: */
@@@ -5598,9 -6210,9 +5610,9 @@@ static struct task_struct *__pick_migra
  	struct task_struct *next;
  
  	for_each_class(class) {
- 		next = class->pick_next_task(rq, NULL, NULL);
+ 		next = class->pick_next_task(rq);
  		if (next) {
 -			next->sched_class->put_prev_task(rq, next);
 +			next->sched_class->put_prev_task(rq, next, NULL);
  			return next;
  		}
  	}
diff --cc kernel/sched/deadline.c
index f6520d44abd2,f7fbb4427959..000000000000
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@@ -1754,53 -1770,19 +1754,56 @@@ static struct sched_dl_entity *pick_nex
  	return rb_entry(left, struct sched_dl_entity, rb_node);
  }
  
- static struct task_struct *
- pick_next_task_dl(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+ static struct task_struct *pick_next_task_dl(struct rq *rq)
  {
  	struct sched_dl_entity *dl_se;
 -	struct dl_rq *dl_rq = &rq->dl;
  	struct task_struct *p;
 +	struct dl_rq *dl_rq;
 +
++<<<<<<< HEAD
 +	dl_rq = &rq->dl;
 +
 +	if (need_pull_dl_task(rq, prev)) {
 +		/*
 +		 * This is OK, because current is on_cpu, which avoids it being
 +		 * picked for load-balance and preemption/IRQs are still
 +		 * disabled avoiding further scheduler activity on it and we're
 +		 * being very careful to re-start the picking loop.
 +		 */
 +		rq_unpin_lock(rq, rf);
 +		pull_dl_task(rq);
 +		rq_repin_lock(rq, rf);
 +		/*
 +		 * pull_dl_task() can drop (and re-acquire) rq->lock; this
 +		 * means a stop task can slip in, in which case we need to
 +		 * re-start task selection.
 +		 */
 +		if (rq->stop && task_on_rq_queued(rq->stop))
 +			return RETRY_TASK;
 +	}
  
 +	/*
 +	 * When prev is DL, we may throttle it in put_prev_task().
 +	 * So, we update time before we check for dl_nr_running.
 +	 */
 +	if (prev->sched_class == &dl_sched_class)
 +		update_curr_dl(rq);
 +
 +	if (unlikely(!dl_rq->dl_nr_running))
++=======
+ 	if (!sched_dl_runnable(rq))
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  		return NULL;
  
 +	put_prev_task(rq, prev);
 +
  	dl_se = pick_next_dl_entity(rq, dl_rq);
  	BUG_ON(!dl_se);
 +
  	p = dl_task_of(dl_se);
 +
  	set_next_task_dl(rq, p);
 +
  	return p;
  }
  
diff --cc kernel/sched/fair.c
index 9b5a5e15e495,1789193d9917..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -10333,8 -10627,7 +10338,12 @@@ const struct sched_class fair_sched_cla
  
  	.check_preempt_curr	= check_preempt_wakeup,
  
++<<<<<<< HEAD
 +	.pick_next_task		= pick_next_task_fair,
 +
++=======
+ 	.pick_next_task		= __pick_next_task_fair,
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  	.put_prev_task		= put_prev_task_fair,
  	.set_next_task          = set_next_task_fair,
  
diff --cc kernel/sched/idle.c
index b77157291d47,f88b79eeee1e..000000000000
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@@ -398,12 -391,10 +398,19 @@@ static void set_next_task_idle(struct r
  	schedstat_inc(rq->sched_goidle);
  }
  
++<<<<<<< HEAD
 +static struct task_struct *
 +pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 +{
 +	struct task_struct *next = rq->idle;
 +
 +	put_prev_task(rq, prev);
++=======
+ struct task_struct *pick_next_task_idle(struct rq *rq)
+ {
+ 	struct task_struct *next = rq->idle;
+ 
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  	set_next_task_idle(rq, next);
  
  	return next;
diff --cc kernel/sched/rt.c
index 9a6c4d4cbc91,38027c04b04c..000000000000
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@@ -1547,48 -1564,15 +1547,51 @@@ static struct task_struct *_pick_next_t
  	return rt_task_of(rt_se);
  }
  
- static struct task_struct *
- pick_next_task_rt(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+ static struct task_struct *pick_next_task_rt(struct rq *rq)
  {
  	struct task_struct *p;
 +	struct rt_rq *rt_rq = &rq->rt;
 +
++<<<<<<< HEAD
 +	if (need_pull_rt_task(rq, prev)) {
 +		/*
 +		 * This is OK, because current is on_cpu, which avoids it being
 +		 * picked for load-balance and preemption/IRQs are still
 +		 * disabled avoiding further scheduler activity on it and we're
 +		 * being very careful to re-start the picking loop.
 +		 */
 +		rq_unpin_lock(rq, rf);
 +		pull_rt_task(rq);
 +		rq_repin_lock(rq, rf);
 +		/*
 +		 * pull_rt_task() can drop (and re-acquire) rq->lock; this
 +		 * means a dl or stop task can slip in, in which case we need
 +		 * to re-start task selection.
 +		 */
 +		if (unlikely((rq->stop && task_on_rq_queued(rq->stop)) ||
 +			     rq->dl.dl_nr_running))
 +			return RETRY_TASK;
 +	}
 +
 +	/*
 +	 * We may dequeue prev's rt_rq in put_prev_task().
 +	 * So, we update time before rt_nr_running check.
 +	 */
 +	if (prev->sched_class == &rt_sched_class)
 +		update_curr_rt(rq);
  
 +	if (!rt_rq->rt_queued)
++=======
+ 	if (!sched_rt_runnable(rq))
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  		return NULL;
  
 +	put_prev_task(rq, prev);
 +
  	p = _pick_next_task_rt(rq);
 +
  	set_next_task_rt(rq, p);
 +
  	return p;
  }
  
diff --cc kernel/sched/sched.h
index 53610e62107c,75d96cce1492..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -1676,18 -1713,9 +1676,24 @@@ struct sched_class 
  
  	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);
  
++<<<<<<< HEAD
 +	/*
 +	 * It is the responsibility of the pick_next_task() method that will
 +	 * return the next task to call put_prev_task() on the @prev task or
 +	 * something equivalent.
 +	 *
 +	 * May return RETRY_TASK when it finds a higher prio class has runnable
 +	 * tasks.
 +	 */
 +	struct task_struct * (*pick_next_task)(struct rq *rq,
 +					       struct task_struct *prev,
 +					       struct rq_flags *rf);
 +	void (*put_prev_task)(struct rq *rq, struct task_struct *p, struct rq_flags *rf);
++=======
+ 	struct task_struct *(*pick_next_task)(struct rq *rq);
+ 
+ 	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  	void (*set_next_task)(struct rq *rq, struct task_struct *p);
  
  #ifdef CONFIG_SMP
@@@ -1761,6 -1789,28 +1767,31 @@@ extern const struct sched_class rt_sche
  extern const struct sched_class fair_sched_class;
  extern const struct sched_class idle_sched_class;
  
++<<<<<<< HEAD
++=======
+ static inline bool sched_stop_runnable(struct rq *rq)
+ {
+ 	return rq->stop && task_on_rq_queued(rq->stop);
+ }
+ 
+ static inline bool sched_dl_runnable(struct rq *rq)
+ {
+ 	return rq->dl.dl_nr_running > 0;
+ }
+ 
+ static inline bool sched_rt_runnable(struct rq *rq)
+ {
+ 	return rq->rt.rt_queued > 0;
+ }
+ 
+ static inline bool sched_fair_runnable(struct rq *rq)
+ {
+ 	return rq->cfs.nr_running > 0;
+ }
+ 
+ extern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
+ extern struct task_struct *pick_next_task_idle(struct rq *rq);
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  
  #ifdef CONFIG_SMP
  
diff --cc kernel/sched/stop_task.c
index 8f414018d5e0,0aefdfb79b36..000000000000
--- a/kernel/sched/stop_task.c
+++ b/kernel/sched/stop_task.c
@@@ -28,18 -34,13 +28,21 @@@ static void set_next_task_stop(struct r
  	stop->se.exec_start = rq_clock_task(rq);
  }
  
- static struct task_struct *
- pick_next_task_stop(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+ static struct task_struct *pick_next_task_stop(struct rq *rq)
  {
++<<<<<<< HEAD
 +	struct task_struct *stop = rq->stop;
 +
 +	if (!stop || !task_on_rq_queued(stop))
++=======
+ 	if (!sched_stop_runnable(rq))
++>>>>>>> 98c2f700edb4 (sched/core: Simplify sched_class::pick_next_task())
  		return NULL;
  
 -	set_next_task_stop(rq, rq->stop);
 -	return rq->stop;
 +	put_prev_task(rq, prev);
 +	set_next_task_stop(rq, stop);
 +
 +	return stop;
  }
  
  static void
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/deadline.c
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/idle.c
* Unmerged path kernel/sched/rt.c
* Unmerged path kernel/sched/sched.h
* Unmerged path kernel/sched/stop_task.c
