io_uring: avoid ring quiesce for fixed file set unregister and update

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 05f3fb3c5397524feae2e73ee8e150a9090a7da2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/05f3fb3c.failed

We currently fully quiesce the ring before an unregister or update of
the fixed fileset. This is very expensive, and we can be a bit smarter
about this.

Add a percpu refcount for the file tables as a whole. Grab a percpu ref
when we use a registered file, and put it on completion. This is cheap
to do. Upon removal of a file from a set, switch the ref count to atomic
mode. When we hit zero ref on the completion side, then we know we can
drop the previously registered files. When the old files have been
dropped, switch the ref back to percpu mode for normal operation.

Since there's a period between doing the update and the kernel being
done with it, add a IORING_OP_FILES_UPDATE opcode that can perform the
same action. The application knows the update has completed when it gets
the CQE for it. Between doing the update and receiving this completion,
the application must continue to use the unregistered fd if submitting
IO on this particular file.

This takes the runtime of test/file-register from liburing from 14s to
about 0.7s.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 05f3fb3c5397524feae2e73ee8e150a9090a7da2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index eb3b77d5111e,4325068324b7..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -197,16 -175,25 +197,31 @@@ struct io_mapped_ubuf 
  	unsigned int	nr_bvecs;
  };
  
 -struct fixed_file_table {
 -	struct file		**files;
 +struct async_list {
 +	spinlock_t		lock;
 +	atomic_t		cnt;
 +	struct list_head	list;
 +
 +	struct file		*file;
 +	off_t			io_start;
 +	size_t			io_len;
  };
  
+ enum {
+ 	FFD_F_ATOMIC,
+ };
+ 
+ struct fixed_file_data {
+ 	struct fixed_file_table		*table;
+ 	struct io_ring_ctx		*ctx;
+ 
+ 	struct percpu_ref		refs;
+ 	struct llist_head		put_llist;
+ 	unsigned long			state;
+ 	struct work_struct		ref_work;
+ 	struct completion		done;
+ };
+ 
  struct io_ring_ctx {
  	struct {
  		struct percpu_ref	refs;
@@@ -251,7 -246,7 +266,11 @@@
  	 * readers must ensure that ->refs is alive as long as the file* is
  	 * used. Only updated through io_uring_register(2).
  	 */
++<<<<<<< HEAD
 +	struct file		**user_files;
++=======
+ 	struct fixed_file_data	*file_data;
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  	unsigned		nr_user_files;
  
  	/* if used, fixed mapped user buffers */
@@@ -308,6 -316,114 +327,117 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_close {
+ 	struct file			*file;
+ 	struct file			*put_file;
+ 	int				fd;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ 	int				mode;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	unsigned			count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	struct user_msghdr __user	*msg;
+ 	int				msg_flags;
+ };
+ 
+ struct io_open {
+ 	struct file			*file;
+ 	int				dfd;
+ 	umode_t				mode;
+ 	const char __user		*fname;
+ 	struct filename			*filename;
+ 	int				flags;
+ };
+ 
+ struct io_files_update {
+ 	struct file			*file;
+ 	u64				arg;
+ 	u32				nr_args;
+ 	u32				offset;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_open {
+ 	struct filename			*filename;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 		struct io_async_open	open;
+ 	};
+ };
+ 
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -317,14 -433,32 +447,26 @@@
  struct io_kiocb {
  	union {
  		struct file		*file;
 -		struct io_rw		rw;
 +		struct kiocb		rw;
  		struct io_poll_iocb	poll;
++<<<<<<< HEAD
++=======
+ 		struct io_accept	accept;
+ 		struct io_sync		sync;
+ 		struct io_cancel	cancel;
+ 		struct io_timeout	timeout;
+ 		struct io_connect	connect;
+ 		struct io_sr_msg	sr_msg;
+ 		struct io_open		open;
+ 		struct io_close		close;
+ 		struct io_files_update	files_update;
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  	};
  
 -	struct io_async_ctx		*io;
 -	struct file			*ring_file;
 -	int				ring_fd;
 -	bool				has_user;
 -	bool				in_async;
 -	bool				needs_fixed_file;
 -	u8				opcode;
 +	struct sqe_submit	submit;
  
  	struct io_ring_ctx	*ctx;
 -	union {
 -		struct list_head	list;
 -		struct hlist_node	hash_node;
 -	};
 +	struct list_head	list;
  	struct list_head	link_list;
  	unsigned int		flags;
  	refcount_t		refs;
@@@ -367,8 -511,17 +509,19 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
 -static void io_wq_submit_work(struct io_wq_work **workptr);
 -static void io_cqring_fill_event(struct io_kiocb *req, long res);
 +static void io_sq_wq_submit_work(struct work_struct *work);
  static void __io_free_req(struct io_kiocb *req);
++<<<<<<< HEAD
++=======
+ static void io_put_req(struct io_kiocb *req);
+ static void io_double_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  
  static struct kmem_cache *req_cachep;
  
@@@ -626,15 -978,57 +780,42 @@@ static void io_free_req_many(struct io_
  
  static void __io_free_req(struct io_kiocb *req)
  {
++<<<<<<< HEAD
 +	if (req->file && !(req->flags & REQ_F_FIXED_FILE))
 +		fput(req->file);
 +	percpu_ref_put(&req->ctx->refs);
 +	kmem_cache_free(req_cachep, req);
++=======
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	if (req->io)
+ 		kfree(req->io);
+ 	if (req->file) {
+ 		if (req->flags & REQ_F_FIXED_FILE)
+ 			percpu_ref_put(&ctx->file_data->refs);
+ 		else
+ 			fput(req->file);
+ 	}
+ 	if (req->flags & REQ_F_INFLIGHT) {
+ 		unsigned long flags;
+ 
+ 		spin_lock_irqsave(&ctx->inflight_lock, flags);
+ 		list_del(&req->inflight_entry);
+ 		if (waitqueue_active(&ctx->inflight_wait))
+ 			wake_up(&ctx->inflight_wait);
+ 		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
+ 	}
+ 	percpu_ref_put(&ctx->refs);
+ 	if (likely(!io_is_fallback_req(req)))
+ 		kmem_cache_free(req_cachep, req);
+ 	else
+ 		clear_bit_unlock(0, (unsigned long *) ctx->fallback_req);
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  }
  
 -static bool io_link_cancel_timeout(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
 -	if (ret != -1) {
 -		io_cqring_fill_event(req, -ECANCELED);
 -		io_commit_cqring(ctx);
 -		req->flags &= ~REQ_F_LINK;
 -		io_put_req(req);
 -		return true;
 -	}
 -
 -	return false;
 -}
 -
 -static void io_req_link_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
 +static void io_req_link_next(struct io_kiocb *req)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	bool wake_ev = false;
 -
 -	/* Already got next link */
 -	if (req->flags & REQ_F_LINK_NEXT)
 -		return;
 +	struct io_kiocb *nxt;
  
  	/*
  	 * The list should never be empty when we are called here. But could
@@@ -1853,22 -3030,432 +2034,431 @@@ static int io_poll_add(struct io_kiocb 
  	return ipt.error;
  }
  
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 +	struct io_uring_sqe *sqe_copy;
  
++<<<<<<< HEAD
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
++=======
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * We could be racing with timeout deletion. If the list is empty,
+ 	 * then timeout lookup already found it and will be handling it.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		struct io_kiocb *prev;
+ 
+ 		/*
+ 		 * Adjust the reqs sequence before the current one because it
+ 		 * will consume a slot in the cq_ring and the cq_tail
+ 		 * pointer will be increased, otherwise other timeout reqs may
+ 		 * return in advance without waiting for enough wait_nr.
+ 		 */
+ 		prev = req;
+ 		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 			prev->sequence++;
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	io_cqring_fill_event(req, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
+ {
+ 	struct io_kiocb *req;
+ 	int ret = -ENOENT;
+ 
+ 	list_for_each_entry(req, &ctx->timeout_list, list) {
+ 		if (user_data == req->user_data) {
+ 			list_del_init(&req->list);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (ret == -ENOENT)
+ 		return ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
+ 	if (ret == -1)
+ 		return -EALREADY;
+ 
+ 	req_set_fail_links(req);
+ 	io_cqring_fill_event(req, -ECANCELED);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_remove_prep(struct io_kiocb *req,
+ 				  const struct io_uring_sqe *sqe)
+ {
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
+ 		return -EINVAL;
+ 
+ 	req->timeout.addr = READ_ONCE(sqe->addr);
+ 	req->timeout.flags = READ_ONCE(sqe->timeout_flags);
+ 	if (req->timeout.flags)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Remove or update an existing timeout command
+  */
+ static int io_timeout_remove(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	ret = io_timeout_cancel(ctx, req->timeout.addr);
+ 
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	io_cqring_ev_posted(ctx);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			   bool is_timeout_link)
+ {
+ 	struct io_timeout_data *data;
+ 	unsigned flags;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
+ 		return -EINVAL;
+ 	if (sqe->off && is_timeout_link)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 
+ 	req->timeout.count = READ_ONCE(sqe->off);
+ 
+ 	if (!req->io && io_alloc_async_ctx(req))
+ 		return -ENOMEM;
+ 
+ 	data = &req->io->timeout;
+ 	data->req = req;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	if (flags & IORING_TIMEOUT_ABS)
+ 		data->mode = HRTIMER_MODE_ABS;
+ 	else
+ 		data->mode = HRTIMER_MODE_REL;
+ 
+ 	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
+ 	return 0;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_timeout_data *data;
+ 	struct list_head *entry;
+ 	unsigned span = 0;
+ 
+ 	data = &req->io->timeout;
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied. If it isn't set, then this is
+ 	 * a pure timeout request, sequence isn't used.
+ 	 */
+ 	count = req->timeout.count;
+ 	if (!count) {
+ 		req->flags |= REQ_F_TIMEOUT_NOSEQ;
+ 		spin_lock_irq(&ctx->completion_lock);
+ 		entry = ctx->timeout_list.prev;
+ 		goto add;
+ 	}
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	data->seq_offset = count;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 		u32 nxt_offset = nxt->io->timeout.seq_offset;
+ 
+ 		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
+ 			continue;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt_offset + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ add:
+ 	list_add(&req->list, entry);
+ 	data->timer.function = io_timeout_fn;
+ 	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	return 0;
+ }
+ 
+ static bool io_cancel_cb(struct io_wq_work *work, void *data)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	return req->user_data == (unsigned long) data;
+ }
+ 
+ static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
+ {
+ 	enum io_wq_cancel cancel_ret;
+ 	int ret = 0;
+ 
+ 	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
+ 	switch (cancel_ret) {
+ 	case IO_WQ_CANCEL_OK:
+ 		ret = 0;
+ 		break;
+ 	case IO_WQ_CANCEL_RUNNING:
+ 		ret = -EALREADY;
+ 		break;
+ 	case IO_WQ_CANCEL_NOTFOUND:
+ 		ret = -ENOENT;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
+ 				     struct io_kiocb *req, __u64 sqe_addr,
+ 				     struct io_kiocb **nxt, int success_ret)
+ {
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
+ 	if (ret != -ENOENT) {
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		goto done;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	ret = io_timeout_cancel(ctx, sqe_addr);
+ 	if (ret != -ENOENT)
+ 		goto done;
+ 	ret = io_poll_cancel(ctx, sqe_addr);
+ done:
+ 	if (!ret)
+ 		ret = success_ret;
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static int io_async_cancel_prep(struct io_kiocb *req,
+ 				const struct io_uring_sqe *sqe)
+ {
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
+ 	    sqe->cancel_flags)
+ 		return -EINVAL;
+ 
+ 	req->cancel.addr = READ_ONCE(sqe->addr);
+ 	return 0;
+ }
+ 
+ static int io_async_cancel(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	io_async_find_and_cancel(ctx, req, req->cancel.addr, nxt, 0);
+ 	return 0;
+ }
+ 
+ static int io_files_update_prep(struct io_kiocb *req,
+ 				const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->flags || sqe->ioprio || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	req->files_update.offset = READ_ONCE(sqe->off);
+ 	req->files_update.nr_args = READ_ONCE(sqe->len);
+ 	if (!req->files_update.nr_args)
+ 		return -EINVAL;
+ 	req->files_update.arg = READ_ONCE(sqe->addr);
+ 	return 0;
+ }
+ 
+ static int io_files_update(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_uring_files_update up;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		req->work.flags |= IO_WQ_WORK_NEEDS_FILES;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	up.offset = req->files_update.offset;
+ 	up.fds = req->files_update.arg;
+ 
+ 	mutex_lock(&ctx->uring_lock);
+ 	ret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);
+ 	mutex_unlock(&ctx->uring_lock);
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_req_defer_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	ssize_t ret = 0;
+ 
+ 	switch (req->opcode) {
+ 	case IORING_OP_NOP:
+ 		break;
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 		ret = io_read_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 		ret = io_write_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_POLL_ADD:
+ 		ret = io_poll_add_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_POLL_REMOVE:
+ 		ret = io_poll_remove_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FSYNC:
+ 		ret = io_prep_fsync(req, sqe);
+ 		break;
+ 	case IORING_OP_SYNC_FILE_RANGE:
+ 		ret = io_prep_sfr(req, sqe);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 		ret = io_sendmsg_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_RECVMSG:
+ 		ret = io_recvmsg_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_CONNECT:
+ 		ret = io_connect_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_TIMEOUT:
+ 		ret = io_timeout_prep(req, sqe, false);
+ 		break;
+ 	case IORING_OP_TIMEOUT_REMOVE:
+ 		ret = io_timeout_remove_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_ASYNC_CANCEL:
+ 		ret = io_async_cancel_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_LINK_TIMEOUT:
+ 		ret = io_timeout_prep(req, sqe, true);
+ 		break;
+ 	case IORING_OP_ACCEPT:
+ 		ret = io_accept_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FALLOCATE:
+ 		ret = io_fallocate_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_OPENAT:
+ 		ret = io_openat_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_CLOSE:
+ 		ret = io_close_prep(req, sqe);
+ 		break;
+ 	case IORING_OP_FILES_UPDATE:
+ 		ret = io_files_update_prep(req, sqe);
+ 		break;
+ 	default:
+ 		printk_once(KERN_WARNING "io_uring: unhandled opcode %d\n",
+ 				req->opcode);
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int io_req_defer(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	/* Still need defer if there is pending req in defer list. */
+ 	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  		return 0;
  
 -	if (!req->io && io_alloc_async_ctx(req))
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
  		return -EAGAIN;
  
 -	ret = io_req_defer_prep(req, sqe);
 -	if (ret < 0)
 -		return ret;
 -
  	spin_lock_irq(&ctx->completion_lock);
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list)) {
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list)) {
  		spin_unlock_irq(&ctx->completion_lock);
 +		kfree(sqe_copy);
  		return 0;
  	}
  
@@@ -1881,52 -3465,154 +2471,60 @@@
  	return -EIOCBQUEUED;
  }
  
 -static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			struct io_kiocb **nxt, bool force_nonblock)
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 +	int ret, opcode;
  
 -	switch (req->opcode) {
 +	req->user_data = READ_ONCE(s->sqe->user_data);
 +
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
  	case IORING_OP_NOP:
 -		ret = io_nop(req);
 +		ret = io_nop(req, req->user_data);
  		break;
  	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -		if (sqe) {
 -			ret = io_read_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_read(req, nxt, force_nonblock);
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
  		break;
  	case IORING_OP_WRITEV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_READ_FIXED:
 +		ret = io_read(req, s, force_nonblock);
 +		break;
  	case IORING_OP_WRITE_FIXED:
 -		if (sqe) {
 -			ret = io_write_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_write(req, nxt, force_nonblock);
 +		ret = io_write(req, s, force_nonblock);
  		break;
  	case IORING_OP_FSYNC:
 -		if (sqe) {
 -			ret = io_prep_fsync(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_fsync(req, nxt, force_nonblock);
 +		ret = io_fsync(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_POLL_ADD:
 -		if (sqe) {
 -			ret = io_poll_add_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_poll_add(req, nxt);
 -		break;
 -	case IORING_OP_POLL_REMOVE:
 -		if (sqe) {
 -			ret = io_poll_remove_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_poll_remove(req);
 -		break;
 -	case IORING_OP_SYNC_FILE_RANGE:
 -		if (sqe) {
 -			ret = io_prep_sfr(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_sync_file_range(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_SENDMSG:
 -		if (sqe) {
 -			ret = io_sendmsg_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_sendmsg(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_RECVMSG:
 -		if (sqe) {
 -			ret = io_recvmsg_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_recvmsg(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		if (sqe) {
 -			ret = io_timeout_prep(req, sqe, false);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout(req);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		if (sqe) {
 -			ret = io_timeout_remove_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout_remove(req);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		if (sqe) {
 -			ret = io_accept_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_accept(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_CONNECT:
 -		if (sqe) {
 -			ret = io_connect_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_connect(req, nxt, force_nonblock);
 +		ret = io_poll_add(req, s->sqe);
  		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		if (sqe) {
 -			ret = io_async_cancel_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_async_cancel(req, nxt);
 +	case IORING_OP_POLL_REMOVE:
 +		ret = io_poll_remove(req, s->sqe);
  		break;
 -	case IORING_OP_FALLOCATE:
 -		if (sqe) {
 -			ret = io_fallocate_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fallocate(req, nxt, force_nonblock);
 +	case IORING_OP_SYNC_FILE_RANGE:
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
  		break;
 -	case IORING_OP_OPENAT:
 -		if (sqe) {
 -			ret = io_openat_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat(req, nxt, force_nonblock);
 +	case IORING_OP_SENDMSG:
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
  		break;
 -	case IORING_OP_CLOSE:
 -		if (sqe) {
 -			ret = io_close_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_close(req, nxt, force_nonblock);
 +	case IORING_OP_RECVMSG:
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
  		break;
+ 	case IORING_OP_FILES_UPDATE:
+ 		if (sqe) {
+ 			ret = io_files_update_prep(req, sqe);
+ 			if (ret)
+ 				break;
+ 		}
+ 		ret = io_files_update(req, force_nonblock);
+ 		break;
  	default:
  		ret = -EINVAL;
  		break;
@@@ -2137,39 -3707,46 +2735,57 @@@ static bool io_op_needs_file(const stru
  	}
  }
  
 -static inline struct file *io_file_from_index(struct io_ring_ctx *ctx,
 -					      int index)
 +static int io_req_set_file(struct io_ring_ctx *ctx, const struct sqe_submit *s,
 +			   struct io_submit_state *state, struct io_kiocb *req)
  {
++<<<<<<< HEAD
++=======
+ 	struct fixed_file_table *table;
+ 
+ 	table = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];
+ 	return table->files[index & IORING_FILE_TABLE_MASK];;
+ }
+ 
+ static int io_req_set_file(struct io_submit_state *state, struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  	unsigned flags;
 -	int fd, ret;
 +	int fd;
  
 -	flags = READ_ONCE(sqe->flags);
 -	fd = READ_ONCE(sqe->fd);
 +	flags = READ_ONCE(s->sqe->flags);
 +	fd = READ_ONCE(s->sqe->fd);
  
  	if (flags & IOSQE_IO_DRAIN)
  		req->flags |= REQ_F_IO_DRAIN;
 +	/*
 +	 * All io need record the previous position, if LINK vs DARIN,
 +	 * it can be used to mark the position of the first IO in the
 +	 * link list.
 +	 */
 +	req->sequence = s->sequence;
  
 -	ret = io_req_needs_file(req, fd);
 -	if (ret <= 0)
 -		return ret;
 +	if (!io_op_needs_file(s->sqe))
 +		return 0;
  
  	if (flags & IOSQE_FIXED_FILE) {
++<<<<<<< HEAD
 +		if (unlikely(!ctx->user_files ||
++=======
+ 		if (unlikely(!ctx->file_data ||
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  		    (unsigned) fd >= ctx->nr_user_files))
  			return -EBADF;
  		fd = array_index_nospec(fd, ctx->nr_user_files);
 -		req->file = io_file_from_index(ctx, fd);
 -		if (!req->file)
 +		if (!ctx->user_files[fd])
  			return -EBADF;
 +		req->file = ctx->user_files[fd];
  		req->flags |= REQ_F_FIXED_FILE;
+ 		percpu_ref_get(&ctx->file_data->refs);
  	} else {
 -		if (req->needs_fixed_file)
 +		if (s->needs_fixed_file)
  			return -EBADF;
 -		trace_io_uring_file_get(ctx, fd);
  		req->file = io_file_get(state, fd);
  		if (unlikely(!req->file))
  			return -EBADF;
@@@ -2753,14 -4420,37 +3369,46 @@@ static void __io_sqe_files_unregister(s
  #endif
  }
  
+ static void io_file_ref_kill(struct percpu_ref *ref)
+ {
+ 	struct fixed_file_data *data;
+ 
+ 	data = container_of(ref, struct fixed_file_data, refs);
+ 	complete(&data->done);
+ }
+ 
  static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
  {
++<<<<<<< HEAD
 +	if (!ctx->user_files)
++=======
+ 	struct fixed_file_data *data = ctx->file_data;
+ 	unsigned nr_tables, i;
+ 
+ 	if (!data)
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  		return -ENXIO;
  
+ 	/* protect against inflight atomic switch, which drops the ref */
+ 	flush_work(&data->ref_work);
+ 	percpu_ref_get(&data->refs);
+ 	percpu_ref_kill_and_confirm(&data->refs, io_file_ref_kill);
+ 	wait_for_completion(&data->done);
+ 	percpu_ref_put(&data->refs);
+ 	percpu_ref_exit(&data->refs);
+ 
  	__io_sqe_files_unregister(ctx);
++<<<<<<< HEAD
 +	kfree(ctx->user_files);
 +	ctx->user_files = NULL;
++=======
+ 	nr_tables = DIV_ROUND_UP(ctx->nr_user_files, IORING_MAX_FILES_TABLE);
+ 	for (i = 0; i < nr_tables; i++)
+ 		kfree(data->table[i].files);
+ 	kfree(data->table);
+ 	kfree(data);
+ 	ctx->file_data = NULL;
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  	ctx->nr_user_files = 0;
  	return 0;
  }
@@@ -2795,18 -4481,6 +3443,21 @@@ static void io_finish_async(struct io_r
  }
  
  #if defined(CONFIG_UNIX)
++<<<<<<< HEAD
 +static void io_destruct_skb(struct sk_buff *skb)
 +{
 +	struct io_ring_ctx *ctx = skb->sk->sk_user_data;
 +	int i;
 +
 +	for (i = 0; i < ARRAY_SIZE(ctx->sqo_wq); i++)
 +		if (ctx->sqo_wq[i])
 +			flush_workqueue(ctx->sqo_wq[i]);
 +
 +	unix_destruct_scm(skb);
 +}
 +
++=======
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  /*
   * Ensure the UNIX gc is aware of our file set, so we are certain that
   * the io_uring can be safely unregistered on process exit, even if we have
@@@ -2906,75 -4584,36 +3557,108 @@@ static int io_sqe_files_scm(struct io_r
  }
  #endif
  
++<<<<<<< HEAD
 +static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
 +				 unsigned nr_args)
 +{
 +	__s32 __user *fds = (__s32 __user *) arg;
 +	int fd, ret = 0;
 +	unsigned i;
 +
 +	if (ctx->user_files)
 +		return -EBUSY;
 +	if (!nr_args)
 +		return -EINVAL;
 +	if (nr_args > IORING_MAX_FIXED_FILES)
 +		return -EMFILE;
 +
 +	ctx->user_files = kcalloc(nr_args, sizeof(struct file *), GFP_KERNEL);
 +	if (!ctx->user_files)
 +		return -ENOMEM;
 +
 +	for (i = 0; i < nr_args; i++, ctx->nr_user_files++) {
 +		ret = -EFAULT;
 +		if (copy_from_user(&fd, &fds[i], sizeof(fd)))
 +			break;
 +		/* allow sparse sets */
 +		if (fd == -1) {
 +			ret = 0;
 +			continue;
 +		}
 +
 +		ctx->user_files[i] = fget(fd);
 +
 +		ret = -EBADF;
 +		if (!ctx->user_files[i])
 +			break;
 +		/*
 +		 * Don't allow io_uring instances to be registered. If UNIX
 +		 * isn't enabled, then this causes a reference cycle and this
 +		 * instance can never get freed. If UNIX is enabled we'll
 +		 * handle it just fine, but there's still no point in allowing
 +		 * a ring fd as it doesn't support regular read/write anyway.
 +		 */
 +		if (ctx->user_files[i]->f_op == &io_uring_fops) {
 +			fput(ctx->user_files[i]);
 +			break;
 +		}
 +		ret = 0;
 +	}
 +
 +	if (ret) {
 +		for (i = 0; i < ctx->nr_user_files; i++)
 +			if (ctx->user_files[i])
 +				fput(ctx->user_files[i]);
 +
 +		kfree(ctx->user_files);
 +		ctx->user_files = NULL;
 +		ctx->nr_user_files = 0;
 +		return ret;
 +	}
 +
 +	ret = io_sqe_files_scm(ctx);
 +	if (ret)
 +		io_sqe_files_unregister(ctx);
 +
 +	return ret;
 +}
 +
 +static void io_sqe_file_unregister(struct io_ring_ctx *ctx, int index)
 +{
 +#if defined(CONFIG_UNIX)
 +	struct file *file = ctx->user_files[index];
++=======
+ static int io_sqe_alloc_file_tables(struct io_ring_ctx *ctx, unsigned nr_tables,
+ 				    unsigned nr_files)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < nr_tables; i++) {
+ 		struct fixed_file_table *table = &ctx->file_data->table[i];
+ 		unsigned this_files;
+ 
+ 		this_files = min(nr_files, IORING_MAX_FILES_TABLE);
+ 		table->files = kcalloc(this_files, sizeof(struct file *),
+ 					GFP_KERNEL);
+ 		if (!table->files)
+ 			break;
+ 		nr_files -= this_files;
+ 	}
+ 
+ 	if (i == nr_tables)
+ 		return 0;
+ 
+ 	for (i = 0; i < nr_tables; i++) {
+ 		struct fixed_file_table *table = &ctx->file_data->table[i];
+ 		kfree(table->files);
+ 	}
+ 	return 1;
+ }
+ 
+ static void io_ring_file_put(struct io_ring_ctx *ctx, struct file *file)
+ {
+ #if defined(CONFIG_UNIX)
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  	struct sock *sock = ctx->ring_sock->sk;
  	struct sk_buff_head list, *head = &sock->sk_receive_queue;
  	struct sk_buff *skb;
@@@ -3030,7 -4669,7 +3714,11 @@@
  		spin_unlock_irq(&head->lock);
  	}
  #else
++<<<<<<< HEAD
 +	fput(ctx->user_files[index]);
++=======
+ 	fput(file);
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  #endif
  }
  
@@@ -3085,35 -4915,32 +3964,48 @@@ static int __io_sqe_files_update(struc
  	int fd, i, err;
  	__u32 done;
  
++<<<<<<< HEAD
 +	if (!ctx->user_files)
 +		return -ENXIO;
 +	if (!nr_args)
 +		return -EINVAL;
 +	if (copy_from_user(&up, arg, sizeof(up)))
 +		return -EFAULT;
 +	if (up.resv)
 +		return -EINVAL;
 +	if (check_add_overflow(up.offset, nr_args, &done))
++=======
+ 	if (check_add_overflow(up->offset, nr_args, &done))
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  		return -EOVERFLOW;
  	if (done > ctx->nr_user_files)
  		return -EINVAL;
  
  	done = 0;
- 	fds = u64_to_user_ptr(up.fds);
+ 	fds = u64_to_user_ptr(up->fds);
  	while (nr_args) {
 -		struct fixed_file_table *table;
 -		unsigned index;
 -
  		err = 0;
  		if (copy_from_user(&fd, &fds[done], sizeof(fd))) {
  			err = -EFAULT;
  			break;
  		}
++<<<<<<< HEAD
 +		i = array_index_nospec(up.offset, ctx->nr_user_files);
 +		if (ctx->user_files[i]) {
 +			io_sqe_file_unregister(ctx, i);
 +			ctx->user_files[i] = NULL;
++=======
+ 		i = array_index_nospec(up->offset, ctx->nr_user_files);
+ 		table = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];
+ 		index = i & IORING_FILE_TABLE_MASK;
+ 		if (table->files[index]) {
+ 			file = io_file_from_index(ctx, index);
+ 			table->files[index] = NULL;
+ 			if (io_queue_file_removal(data, file))
+ 				ref_switch = true;
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  		}
  		if (fd != -1) {
- 			struct file *file;
- 
  			file = fget(fd);
  			if (!file) {
  				err = -EBADF;
@@@ -3144,7 -4976,37 +4041,23 @@@
  
  	return done ? done : err;
  }
+ static int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,
+ 			       unsigned nr_args)
+ {
+ 	struct io_uring_files_update up;
+ 
+ 	if (!ctx->file_data)
+ 		return -ENXIO;
+ 	if (!nr_args)
+ 		return -EINVAL;
+ 	if (copy_from_user(&up, arg, sizeof(up)))
+ 		return -EFAULT;
+ 	if (up.resv)
+ 		return -EINVAL;
+ 
+ 	return __io_sqe_files_update(ctx, &up, nr_args);
+ }
  
 -static void io_put_work(struct io_wq_work *work)
 -{
 -	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 -
 -	io_put_req(req);
 -}
 -
 -static void io_get_work(struct io_wq_work *work)
 -{
 -	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 -
 -	refcount_inc(&req->refs);
 -}
 -
  static int io_sq_offload_start(struct io_ring_ctx *ctx,
  			       struct io_uring_params *p)
  {
@@@ -3931,18 -5917,22 +4843,35 @@@ static int __io_uring_register(struct i
  	if (percpu_ref_is_dying(&ctx->refs))
  		return -ENXIO;
  
- 	percpu_ref_kill(&ctx->refs);
+ 	if (opcode != IORING_UNREGISTER_FILES &&
+ 	    opcode != IORING_REGISTER_FILES_UPDATE) {
+ 		percpu_ref_kill(&ctx->refs);
  
++<<<<<<< HEAD
 +	/*
 +	 * Drop uring mutex before waiting for references to exit. If another
 +	 * thread is currently inside io_uring_enter() it might need to grab
 +	 * the uring_lock to make progress. If we hold it here across the drain
 +	 * wait, then we can deadlock. It's safe to drop the mutex here, since
 +	 * no new references will come in after we've killed the percpu ref.
 +	 */
 +	mutex_unlock(&ctx->uring_lock);
 +	wait_for_completion(&ctx->ctx_done);
 +	mutex_lock(&ctx->uring_lock);
++=======
+ 		/*
+ 		 * Drop uring mutex before waiting for references to exit. If
+ 		 * another thread is currently inside io_uring_enter() it might
+ 		 * need to grab the uring_lock to make progress. If we hold it
+ 		 * here across the drain wait, then we can deadlock. It's safe
+ 		 * to drop the mutex here, since no new references will come in
+ 		 * after we've killed the percpu ref.
+ 		 */
+ 		mutex_unlock(&ctx->uring_lock);
+ 		wait_for_completion(&ctx->completions[0]);
+ 		mutex_lock(&ctx->uring_lock);
+ 	}
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  
  	switch (opcode) {
  	case IORING_REGISTER_BUFFERS:
@@@ -3983,9 -5973,13 +4912,19 @@@
  		break;
  	}
  
++<<<<<<< HEAD
 +	/* bring the ctx back to life */
 +	reinit_completion(&ctx->ctx_done);
 +	percpu_ref_reinit(&ctx->refs);
++=======
+ 
+ 	if (opcode != IORING_UNREGISTER_FILES &&
+ 	    opcode != IORING_REGISTER_FILES_UPDATE) {
+ 		/* bring the ctx back to life */
+ 		reinit_completion(&ctx->completions[0]);
+ 		percpu_ref_reinit(&ctx->refs);
+ 	}
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  	return ret;
  }
  
diff --cc include/uapi/linux/io_uring.h
index 22b1c5919fbd,ca436b9d4921..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -51,17 -59,32 +51,46 @@@ struct io_uring_sqe 
  #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
  #define IORING_SETUP_CQSIZE	(1U << 3)	/* app defines CQ size */
  
++<<<<<<< HEAD
 +#define IORING_OP_NOP		0
 +#define IORING_OP_READV		1
 +#define IORING_OP_WRITEV	2
 +#define IORING_OP_FSYNC		3
 +#define IORING_OP_READ_FIXED	4
 +#define IORING_OP_WRITE_FIXED	5
 +#define IORING_OP_POLL_ADD	6
 +#define IORING_OP_POLL_REMOVE	7
 +#define IORING_OP_SYNC_FILE_RANGE	8
 +#define IORING_OP_SENDMSG	9
 +#define IORING_OP_RECVMSG	10
++=======
+ enum {
+ 	IORING_OP_NOP,
+ 	IORING_OP_READV,
+ 	IORING_OP_WRITEV,
+ 	IORING_OP_FSYNC,
+ 	IORING_OP_READ_FIXED,
+ 	IORING_OP_WRITE_FIXED,
+ 	IORING_OP_POLL_ADD,
+ 	IORING_OP_POLL_REMOVE,
+ 	IORING_OP_SYNC_FILE_RANGE,
+ 	IORING_OP_SENDMSG,
+ 	IORING_OP_RECVMSG,
+ 	IORING_OP_TIMEOUT,
+ 	IORING_OP_TIMEOUT_REMOVE,
+ 	IORING_OP_ACCEPT,
+ 	IORING_OP_ASYNC_CANCEL,
+ 	IORING_OP_LINK_TIMEOUT,
+ 	IORING_OP_CONNECT,
+ 	IORING_OP_FALLOCATE,
+ 	IORING_OP_OPENAT,
+ 	IORING_OP_CLOSE,
+ 	IORING_OP_FILES_UPDATE,
+ 
+ 	/* this goes last, obviously */
+ 	IORING_OP_LAST,
+ };
++>>>>>>> 05f3fb3c5397 (io_uring: avoid ring quiesce for fixed file set unregister and update)
  
  /*
   * sqe->fsync_flags
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
