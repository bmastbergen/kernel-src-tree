drm/i915/gt: Incrementally check for rewinding

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Chris Wilson <chris@chris-wilson.co.uk>
commit e36ba817fa966f81fb1c8d16f3721b5a644b2fa9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e36ba817.failed

In commit 5ba32c7be81e ("drm/i915/execlists: Always force a context
reload when rewinding RING_TAIL"), we placed the check for rewinding a
context on actually submitting the next request in that context. This
was so that we only had to check once, and could do so with precision
avoiding as many forced restores as possible. For example, to ensure
that we can resubmit the same request a couple of times, we include a
small wa_tail such that on the next submission, the ring->tail will
appear to move forwards when resubmitting the same request. This is very
common as it will happen for every lite-restore to fill the second port
after a context switch.

However, intel_ring_direction() is limited in precision to movements of
upto half the ring size. The consequence being that if we tried to
unwind many requests, we could exceed half the ring and flip the sense
of the direction, so missing a force restore. As no request can be
greater than half the ring (i.e. 2048 bytes in the smallest case), we
can check for rollback incrementally. As we check against the tail that
would be submitted, we do not lose any sensitivity and allow lite
restores for the simple case. We still need to double check upon
submitting the context, to allow for multiple preemptions and
resubmissions.

Fixes: 5ba32c7be81e ("drm/i915/execlists: Always force a context reload when rewinding RING_TAIL")
	Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
	Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
	Cc: <stable@vger.kernel.org> # v5.4+
	Reviewed-by: Bruce Chang <yu.bruce.chang@intel.com>
	Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
Link: https://patchwork.freedesktop.org/patch/msgid/20200609151723.12971-1-chris@chris-wilson.co.uk
(cherry picked from commit e36ba817fa966f81fb1c8d16f3721b5a644b2fa9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/gt/intel_lrc.c
#	drivers/gpu/drm/i915/gt/intel_ring.c
#	drivers/gpu/drm/i915/gt/selftest_mocs.c
diff --cc drivers/gpu/drm/i915/gt/intel_lrc.c
index f2865f3cc1d5,5ab0ed35af84..000000000000
--- a/drivers/gpu/drm/i915/gt/intel_lrc.c
+++ b/drivers/gpu/drm/i915/gt/intel_lrc.c
@@@ -466,9 -1135,34 +466,21 @@@ __unwind_incomplete_requests(struct int
  			GEM_BUG_ON(RB_EMPTY_ROOT(&engine->execlists.queue.rb_root));
  
  			list_move(&rq->sched.link, pl);
++<<<<<<< HEAD
++=======
+ 			set_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);
+ 
+ 			/* Check in case we rollback so far we wrap [size/2] */
+ 			if (intel_ring_direction(rq->ring,
+ 						 intel_ring_wrap(rq->ring,
+ 								 rq->tail),
+ 						 rq->ring->tail) > 0)
+ 				rq->context->lrc.desc |= CTX_DESC_FORCE_RESTORE;
+ 
++>>>>>>> e36ba817fa96 (drm/i915/gt: Incrementally check for rewinding)
  			active = rq;
  		} else {
 -			struct intel_engine_cs *owner = rq->context->engine;
 -
 -			/*
 -			 * Decouple the virtual breadcrumb before moving it
 -			 * back to the virtual engine -- we don't want the
 -			 * request to complete in the background and try
 -			 * and cancel the breadcrumb on the virtual engine
 -			 * (instead of the old engine where it is linked)!
 -			 */
 -			if (test_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT,
 -				     &rq->fence.flags)) {
 -				spin_lock_nested(&rq->lock,
 -						 SINGLE_DEPTH_NESTING);
 -				i915_request_cancel_breadcrumb(rq);
 -				spin_unlock(&rq->lock);
 -			}
 -			WRITE_ONCE(rq->engine, owner);
 +			rq->engine = owner;
  			owner->submit_request(rq);
  			active = NULL;
  		}
@@@ -555,10 -1491,34 +567,37 @@@ execlists_context_schedule_out(struct i
  
  static u64 execlists_update_context(struct i915_request *rq)
  {
 -	struct intel_context *ce = rq->context;
 -	u64 desc = ce->lrc.desc;
 -	u32 tail, prev;
 +	struct intel_context *ce = rq->hw_context;
  
++<<<<<<< HEAD
 +	ce->lrc_reg_state[CTX_RING_TAIL + 1] =
 +		intel_ring_set_tail(rq->ring, rq->tail);
++=======
+ 	/*
+ 	 * WaIdleLiteRestore:bdw,skl
+ 	 *
+ 	 * We should never submit the context with the same RING_TAIL twice
+ 	 * just in case we submit an empty ring, which confuses the HW.
+ 	 *
+ 	 * We append a couple of NOOPs (gen8_emit_wa_tail) after the end of
+ 	 * the normal request to be able to always advance the RING_TAIL on
+ 	 * subsequent resubmissions (for lite restore). Should that fail us,
+ 	 * and we try and submit the same tail again, force the context
+ 	 * reload.
+ 	 *
+ 	 * If we need to return to a preempted context, we need to skip the
+ 	 * lite-restore and force it to reload the RING_TAIL. Otherwise, the
+ 	 * HW has a tendency to ignore us rewinding the TAIL to the end of
+ 	 * an earlier request.
+ 	 */
+ 	GEM_BUG_ON(ce->lrc_reg_state[CTX_RING_TAIL] != rq->ring->tail);
+ 	prev = rq->ring->tail;
+ 	tail = intel_ring_set_tail(rq->ring, rq->tail);
+ 	if (unlikely(intel_ring_direction(rq->ring, tail, prev) <= 0))
+ 		desc |= CTX_DESC_FORCE_RESTORE;
+ 	ce->lrc_reg_state[CTX_RING_TAIL] = tail;
+ 	rq->tail = rq->wa_tail;
++>>>>>>> e36ba817fa96 (drm/i915/gt: Incrementally check for rewinding)
  
  	/*
  	 * Make sure the context image is complete before we submit it to HW.
@@@ -2508,6 -4552,228 +2547,231 @@@ static int gen8_emit_flush_render(struc
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int gen11_emit_flush_render(struct i915_request *request,
+ 				   u32 mode)
+ {
+ 	if (mode & EMIT_FLUSH) {
+ 		u32 *cs;
+ 		u32 flags = 0;
+ 
+ 		flags |= PIPE_CONTROL_CS_STALL;
+ 
+ 		flags |= PIPE_CONTROL_TILE_CACHE_FLUSH;
+ 		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+ 		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+ 		flags |= PIPE_CONTROL_DC_FLUSH_ENABLE;
+ 		flags |= PIPE_CONTROL_FLUSH_ENABLE;
+ 		flags |= PIPE_CONTROL_QW_WRITE;
+ 		flags |= PIPE_CONTROL_STORE_DATA_INDEX;
+ 
+ 		cs = intel_ring_begin(request, 6);
+ 		if (IS_ERR(cs))
+ 			return PTR_ERR(cs);
+ 
+ 		cs = gen8_emit_pipe_control(cs, flags, LRC_PPHWSP_SCRATCH_ADDR);
+ 		intel_ring_advance(request, cs);
+ 	}
+ 
+ 	if (mode & EMIT_INVALIDATE) {
+ 		u32 *cs;
+ 		u32 flags = 0;
+ 
+ 		flags |= PIPE_CONTROL_CS_STALL;
+ 
+ 		flags |= PIPE_CONTROL_COMMAND_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_TLB_INVALIDATE;
+ 		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_QW_WRITE;
+ 		flags |= PIPE_CONTROL_STORE_DATA_INDEX;
+ 
+ 		cs = intel_ring_begin(request, 6);
+ 		if (IS_ERR(cs))
+ 			return PTR_ERR(cs);
+ 
+ 		cs = gen8_emit_pipe_control(cs, flags, LRC_PPHWSP_SCRATCH_ADDR);
+ 		intel_ring_advance(request, cs);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static u32 preparser_disable(bool state)
+ {
+ 	return MI_ARB_CHECK | 1 << 8 | state;
+ }
+ 
+ static i915_reg_t aux_inv_reg(const struct intel_engine_cs *engine)
+ {
+ 	static const i915_reg_t vd[] = {
+ 		GEN12_VD0_AUX_NV,
+ 		GEN12_VD1_AUX_NV,
+ 		GEN12_VD2_AUX_NV,
+ 		GEN12_VD3_AUX_NV,
+ 	};
+ 
+ 	static const i915_reg_t ve[] = {
+ 		GEN12_VE0_AUX_NV,
+ 		GEN12_VE1_AUX_NV,
+ 	};
+ 
+ 	if (engine->class == VIDEO_DECODE_CLASS)
+ 		return vd[engine->instance];
+ 
+ 	if (engine->class == VIDEO_ENHANCEMENT_CLASS)
+ 		return ve[engine->instance];
+ 
+ 	GEM_BUG_ON("unknown aux_inv_reg\n");
+ 
+ 	return INVALID_MMIO_REG;
+ }
+ 
+ static u32 *
+ gen12_emit_aux_table_inv(const i915_reg_t inv_reg, u32 *cs)
+ {
+ 	*cs++ = MI_LOAD_REGISTER_IMM(1);
+ 	*cs++ = i915_mmio_reg_offset(inv_reg);
+ 	*cs++ = AUX_INV;
+ 	*cs++ = MI_NOOP;
+ 
+ 	return cs;
+ }
+ 
+ static int gen12_emit_flush_render(struct i915_request *request,
+ 				   u32 mode)
+ {
+ 	if (mode & EMIT_FLUSH) {
+ 		u32 flags = 0;
+ 		u32 *cs;
+ 
+ 		flags |= PIPE_CONTROL_TILE_CACHE_FLUSH;
+ 		flags |= PIPE_CONTROL_FLUSH_L3;
+ 		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+ 		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+ 		/* Wa_1409600907:tgl */
+ 		flags |= PIPE_CONTROL_DEPTH_STALL;
+ 		flags |= PIPE_CONTROL_DC_FLUSH_ENABLE;
+ 		flags |= PIPE_CONTROL_FLUSH_ENABLE;
+ 
+ 		flags |= PIPE_CONTROL_STORE_DATA_INDEX;
+ 		flags |= PIPE_CONTROL_QW_WRITE;
+ 
+ 		flags |= PIPE_CONTROL_CS_STALL;
+ 
+ 		cs = intel_ring_begin(request, 6);
+ 		if (IS_ERR(cs))
+ 			return PTR_ERR(cs);
+ 
+ 		cs = gen12_emit_pipe_control(cs,
+ 					     PIPE_CONTROL0_HDC_PIPELINE_FLUSH,
+ 					     flags, LRC_PPHWSP_SCRATCH_ADDR);
+ 		intel_ring_advance(request, cs);
+ 	}
+ 
+ 	if (mode & EMIT_INVALIDATE) {
+ 		u32 flags = 0;
+ 		u32 *cs;
+ 
+ 		flags |= PIPE_CONTROL_COMMAND_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_TLB_INVALIDATE;
+ 		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+ 		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+ 
+ 		flags |= PIPE_CONTROL_STORE_DATA_INDEX;
+ 		flags |= PIPE_CONTROL_QW_WRITE;
+ 
+ 		flags |= PIPE_CONTROL_CS_STALL;
+ 
+ 		cs = intel_ring_begin(request, 8 + 4);
+ 		if (IS_ERR(cs))
+ 			return PTR_ERR(cs);
+ 
+ 		/*
+ 		 * Prevent the pre-parser from skipping past the TLB
+ 		 * invalidate and loading a stale page for the batch
+ 		 * buffer / request payload.
+ 		 */
+ 		*cs++ = preparser_disable(true);
+ 
+ 		cs = gen8_emit_pipe_control(cs, flags, LRC_PPHWSP_SCRATCH_ADDR);
+ 
+ 		/* hsdes: 1809175790 */
+ 		cs = gen12_emit_aux_table_inv(GEN12_GFX_CCS_AUX_NV, cs);
+ 
+ 		*cs++ = preparser_disable(false);
+ 		intel_ring_advance(request, cs);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int gen12_emit_flush(struct i915_request *request, u32 mode)
+ {
+ 	intel_engine_mask_t aux_inv = 0;
+ 	u32 cmd, *cs;
+ 
+ 	if (mode & EMIT_INVALIDATE)
+ 		aux_inv = request->engine->mask & ~BIT(BCS0);
+ 
+ 	cs = intel_ring_begin(request,
+ 			      4 + (aux_inv ? 2 * hweight8(aux_inv) + 2 : 0));
+ 	if (IS_ERR(cs))
+ 		return PTR_ERR(cs);
+ 
+ 	cmd = MI_FLUSH_DW + 1;
+ 
+ 	/* We always require a command barrier so that subsequent
+ 	 * commands, such as breadcrumb interrupts, are strictly ordered
+ 	 * wrt the contents of the write cache being flushed to memory
+ 	 * (and thus being coherent from the CPU).
+ 	 */
+ 	cmd |= MI_FLUSH_DW_STORE_INDEX | MI_FLUSH_DW_OP_STOREDW;
+ 
+ 	if (mode & EMIT_INVALIDATE) {
+ 		cmd |= MI_INVALIDATE_TLB;
+ 		if (request->engine->class == VIDEO_DECODE_CLASS)
+ 			cmd |= MI_INVALIDATE_BSD;
+ 	}
+ 
+ 	*cs++ = cmd;
+ 	*cs++ = LRC_PPHWSP_SCRATCH_ADDR;
+ 	*cs++ = 0; /* upper addr */
+ 	*cs++ = 0; /* value */
+ 
+ 	if (aux_inv) { /* hsdes: 1809175790 */
+ 		struct intel_engine_cs *engine;
+ 		unsigned int tmp;
+ 
+ 		*cs++ = MI_LOAD_REGISTER_IMM(hweight8(aux_inv));
+ 		for_each_engine_masked(engine, request->engine->gt,
+ 				       aux_inv, tmp) {
+ 			*cs++ = i915_mmio_reg_offset(aux_inv_reg(engine));
+ 			*cs++ = AUX_INV;
+ 		}
+ 		*cs++ = MI_NOOP;
+ 	}
+ 	intel_ring_advance(request, cs);
+ 
+ 	return 0;
+ }
+ 
+ static void assert_request_valid(struct i915_request *rq)
+ {
+ 	struct intel_ring *ring __maybe_unused = rq->ring;
+ 
+ 	/* Can we unwind this request without appearing to go forwards? */
+ 	GEM_BUG_ON(intel_ring_direction(ring, rq->wa_tail, rq->head) <= 0);
+ }
+ 
++>>>>>>> e36ba817fa96 (drm/i915/gt: Incrementally check for rewinding)
  /*
   * Reserve space for 2 NOOPs at the end of each request to be
   * used as a workaround for not being allowed to do lite
* Unmerged path drivers/gpu/drm/i915/gt/intel_ring.c
* Unmerged path drivers/gpu/drm/i915/gt/selftest_mocs.c
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_cs.c b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
index f25632c9b292..aee82f4242b5 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_cs.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
@@ -719,7 +719,7 @@ struct measure_breadcrumb {
 	struct i915_request rq;
 	struct i915_timeline timeline;
 	struct intel_ring ring;
-	u32 cs[1024];
+	u32 cs[2048];
 };
 
 static int measure_breadcrumb_dw(struct intel_engine_cs *engine)
@@ -742,6 +742,8 @@ static int measure_breadcrumb_dw(struct intel_engine_cs *engine)
 	frame->ring.timeline = &frame->timeline;
 	frame->ring.vaddr = frame->cs;
 	frame->ring.size = sizeof(frame->cs);
+	frame->ring.wrap =
+		BITS_PER_TYPE(frame->ring.size) - ilog2(frame->ring.size);
 	frame->ring.effective_size = frame->ring.size;
 	intel_ring_update_space(&frame->ring);
 
* Unmerged path drivers/gpu/drm/i915/gt/intel_lrc.c
* Unmerged path drivers/gpu/drm/i915/gt/intel_ring.c
* Unmerged path drivers/gpu/drm/i915/gt/selftest_mocs.c
diff --git a/drivers/gpu/drm/i915/gt/selftest_ring.c b/drivers/gpu/drm/i915/gt/selftest_ring.c
new file mode 100644
index 000000000000..2a8c534dc125
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/selftest_ring.c
@@ -0,0 +1,110 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright © 2020 Intel Corporation
+ */
+
+static struct intel_ring *mock_ring(unsigned long sz)
+{
+	struct intel_ring *ring;
+
+	ring = kzalloc(sizeof(*ring) + sz, GFP_KERNEL);
+	if (!ring)
+		return NULL;
+
+	kref_init(&ring->ref);
+	ring->size = sz;
+	ring->wrap = BITS_PER_TYPE(ring->size) - ilog2(sz);
+	ring->effective_size = sz;
+	ring->vaddr = (void *)(ring + 1);
+	atomic_set(&ring->pin_count, 1);
+
+	intel_ring_update_space(ring);
+
+	return ring;
+}
+
+static void mock_ring_free(struct intel_ring *ring)
+{
+	kfree(ring);
+}
+
+static int check_ring_direction(struct intel_ring *ring,
+				u32 next, u32 prev,
+				int expected)
+{
+	int result;
+
+	result = intel_ring_direction(ring, next, prev);
+	if (result < 0)
+		result = -1;
+	else if (result > 0)
+		result = 1;
+
+	if (result != expected) {
+		pr_err("intel_ring_direction(%u, %u):%d != %d\n",
+		       next, prev, result, expected);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int check_ring_step(struct intel_ring *ring, u32 x, u32 step)
+{
+	u32 prev = x, next = intel_ring_wrap(ring, x + step);
+	int err = 0;
+
+	err |= check_ring_direction(ring, next, next,  0);
+	err |= check_ring_direction(ring, prev, prev,  0);
+	err |= check_ring_direction(ring, next, prev,  1);
+	err |= check_ring_direction(ring, prev, next, -1);
+
+	return err;
+}
+
+static int check_ring_offset(struct intel_ring *ring, u32 x, u32 step)
+{
+	int err = 0;
+
+	err |= check_ring_step(ring, x, step);
+	err |= check_ring_step(ring, intel_ring_wrap(ring, x + 1), step);
+	err |= check_ring_step(ring, intel_ring_wrap(ring, x - 1), step);
+
+	return err;
+}
+
+static int igt_ring_direction(void *dummy)
+{
+	struct intel_ring *ring;
+	unsigned int half = 2048;
+	int step, err = 0;
+
+	ring = mock_ring(2 * half);
+	if (!ring)
+		return -ENOMEM;
+
+	GEM_BUG_ON(ring->size != 2 * half);
+
+	/* Precision of wrap detection is limited to ring->size / 2 */
+	for (step = 1; step < half; step <<= 1) {
+		err |= check_ring_offset(ring, 0, step);
+		err |= check_ring_offset(ring, half, step);
+	}
+	err |= check_ring_step(ring, 0, half - 64);
+
+	/* And check unwrapped handling for good measure */
+	err |= check_ring_offset(ring, 0, 2 * half + 64);
+	err |= check_ring_offset(ring, 3 * half, 1);
+
+	mock_ring_free(ring);
+	return err;
+}
+
+int intel_ring_mock_selftests(void)
+{
+	static const struct i915_subtest tests[] = {
+		SUBTEST(igt_ring_direction),
+	};
+
+	return i915_subtests(tests, NULL);
+}
diff --git a/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h b/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
index 510eb176bb2c..d142dba9a7a7 100644
--- a/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
+++ b/drivers/gpu/drm/i915/selftests/i915_mock_selftests.h
@@ -14,6 +14,7 @@ selftest(fence, i915_sw_fence_mock_selftests)
 selftest(scatterlist, scatterlist_mock_selftests)
 selftest(syncmap, i915_syncmap_mock_selftests)
 selftest(uncore, intel_uncore_mock_selftests)
+selftest(ring, intel_ring_mock_selftests)
 selftest(engine, intel_engine_cs_mock_selftests)
 selftest(timelines, i915_timeline_mock_selftests)
 selftest(requests, i915_request_mock_selftests)
