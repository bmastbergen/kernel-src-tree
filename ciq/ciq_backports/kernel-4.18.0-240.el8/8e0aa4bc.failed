net/mlx5: E-switch, Protect eswitch mode changes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Parav Pandit <parav@mellanox.com>
commit 8e0aa4bc959c98c14ed0aaee522d77ca52690189
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/8e0aa4bc.failed

Currently eswitch mode change is occurring from 2 different execution
contexts as below.
1. sriov sysfs enable/disable
2. devlink eswitch set commands

Both of them need to access eswitch related data structures in
synchronized manner.
Without any synchronization below race condition exist.

SR-IOV enable/disable with devlink eswitch mode change:

          cpu-0                         cpu-1
          -----                         -----
mlx5_device_disable_sriov()        mlx5_devlink_eswitch_mode_set()
  mlx5_eswitch_disable()             esw_offloads_stop()
    esw_offloads_disable()             mlx5_eswitch_disable()
                                         esw_offloads_disable()

Hence, they are synchronized using a new mode_lock.
eswitch's state_lock is not used as it can lead to a deadlock scenario
below and state_lock is only for vport and fdb exclusive access.

ip link set vf <param>
  netlink rcv_msg() - Lock A
    rtnl_lock
    vfinfo()
      esw->state_lock() - Lock B
devlink eswitch_set
   devlink_mutex
     esw->state_lock() - Lock B
       attach_netdev()
         register_netdev()
           rtnl_lock - Lock A

Alternatives considered:
1. Acquiring rtnl lock before taking esw->state_lock to follow similar
locking sequence as ip link flow during eswitch mode set.
rtnl lock is not good idea for two reasons.
(a) Holding rtnl lock for several hundred device commands is not good
    idea.
(b) It leads to below and more similar deadlocks.

devlink eswitch_set
   devlink_mutex
     rtnl_lock - Lock A
       esw->state_lock() - Lock B
         eswitch_disable()
           reload()
             ib_register_device()
               ib_cache_setup_one()
                 rtnl_lock()

2. Exporting devlink lock may lead to undesired use of it in vendor
driver(s) in future.

3. Unloading representors outside of the mode_lock requires
serialization with other process trying to enable the eswitch.

4. Differing the representors life cycle to a different workqueue
requires synchronization with func_change_handler workqueue.

	Reviewed-by: Roi Dayan <roid@mellanox.com>
	Reviewed-by: Bodong Wang <bodong@mellanox.com>
	Reviewed-by: Mark Bloch <markb@mellanox.com>
	Signed-off-by: Parav Pandit <parav@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 8e0aa4bc959c98c14ed0aaee522d77ca52690189)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
#	drivers/net/ethernet/mellanox/mlx5/core/sriov.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
index 6b21d02803c5,7f618a443bfd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
@@@ -1889,19 -2040,81 +1889,84 @@@ mlx5_eswitch_enable_pf_vf_vports(struc
   */
  void mlx5_eswitch_disable_pf_vf_vports(struct mlx5_eswitch *esw)
  {
 -	mlx5_eswitch_unload_vf_vports(esw, esw->esw_funcs.num_vfs);
 -
 -	if (mlx5_ecpf_vport_exists(esw->dev))
 -		mlx5_eswitch_unload_vport(esw, MLX5_VPORT_ECPF);
 +	struct mlx5_vport *vport;
 +	int i;
  
 -	mlx5_eswitch_unload_vport(esw, MLX5_VPORT_PF);
 +	mlx5_esw_for_all_vports_reverse(esw, i, vport)
 +		esw_disable_vport(esw, vport);
  }
  
++<<<<<<< HEAD
 +int mlx5_eswitch_enable(struct mlx5_eswitch *esw, int mode)
++=======
+ static void mlx5_eswitch_get_devlink_param(struct mlx5_eswitch *esw)
  {
+ 	struct devlink *devlink = priv_to_devlink(esw->dev);
+ 	union devlink_param_value val;
  	int err;
  
- 	if (!ESW_ALLOWED(esw) ||
- 	    !MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ft_support)) {
+ 	err = devlink_param_driverinit_value_get(devlink,
+ 						 MLX5_DEVLINK_PARAM_ID_ESW_LARGE_GROUP_NUM,
+ 						 &val);
+ 	if (!err) {
+ 		esw->params.large_group_num = val.vu32;
+ 	} else {
+ 		esw_warn(esw->dev,
+ 			 "Devlink can't get param fdb_large_groups, uses default (%d).\n",
+ 			 ESW_OFFLOADS_DEFAULT_NUM_GROUPS);
+ 		esw->params.large_group_num = ESW_OFFLOADS_DEFAULT_NUM_GROUPS;
+ 	}
+ }
+ 
+ static void
+ mlx5_eswitch_update_num_of_vfs(struct mlx5_eswitch *esw, int num_vfs)
+ {
+ 	const u32 *out;
+ 
+ 	WARN_ON_ONCE(esw->mode != MLX5_ESWITCH_NONE);
+ 
+ 	if (num_vfs < 0)
+ 		return;
+ 
+ 	if (!mlx5_core_is_ecpf_esw_manager(esw->dev)) {
+ 		esw->esw_funcs.num_vfs = num_vfs;
+ 		return;
+ 	}
+ 
+ 	out = mlx5_esw_query_functions(esw->dev);
+ 	if (IS_ERR(out))
+ 		return;
+ 
+ 	esw->esw_funcs.num_vfs = MLX5_GET(query_esw_functions_out, out,
+ 					  host_params_context.host_num_of_vfs);
+ 	kvfree(out);
+ }
+ 
+ /**
+  * mlx5_eswitch_enable_locked - Enable eswitch
+  * @esw:	Pointer to eswitch
+  * @mode:	Eswitch mode to enable
+  * @num_vfs:	Enable eswitch for given number of VFs. This is optional.
+  *		Valid value are 0, > 0 and MLX5_ESWITCH_IGNORE_NUM_VFS.
+  *		Caller should pass num_vfs > 0 when enabling eswitch for
+  *		vf vports. Caller should pass num_vfs = 0, when eswitch
+  *		is enabled without sriov VFs or when caller
+  *		is unaware of the sriov state of the host PF on ECPF based
+  *		eswitch. Caller should pass < 0 when num_vfs should be
+  *		completely ignored. This is typically the case when eswitch
+  *		is enabled without sriov regardless of PF/ECPF system.
+  * mlx5_eswitch_enable_locked() Enables eswitch in either legacy or offloads
+  * mode. If num_vfs >=0 is provided, it setup VF related eswitch vports.
+  * It returns 0 on success or error code on failure.
+  */
+ int mlx5_eswitch_enable_locked(struct mlx5_eswitch *esw, int mode, int num_vfs)
++>>>>>>> 8e0aa4bc959c (net/mlx5: E-switch, Protect eswitch mode changes)
+ {
+ 	int err;
+ 
+ 	lockdep_assert_held(&esw->mode_lock);
+ 
+ 	if (!MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ft_support)) {
  		esw_warn(esw->dev, "FDB is not supported, aborting ...\n");
  		return -EOPNOTSUPP;
  	}
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
index 9c629f913b96,39f42f985fbd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@@ -271,7 -301,11 +276,15 @@@ int mlx5_esw_modify_vport_rate(struct m
  /* E-Switch API */
  int mlx5_eswitch_init(struct mlx5_core_dev *dev);
  void mlx5_eswitch_cleanup(struct mlx5_eswitch *esw);
++<<<<<<< HEAD
 +int mlx5_eswitch_enable(struct mlx5_eswitch *esw, int mode);
++=======
+ 
+ #define MLX5_ESWITCH_IGNORE_NUM_VFS (-1)
+ int mlx5_eswitch_enable_locked(struct mlx5_eswitch *esw, int mode, int num_vfs);
+ int mlx5_eswitch_enable(struct mlx5_eswitch *esw, int num_vfs);
+ void mlx5_eswitch_disable_locked(struct mlx5_eswitch *esw, bool clear_vf);
++>>>>>>> 8e0aa4bc959c (net/mlx5: E-switch, Protect eswitch mode changes)
  void mlx5_eswitch_disable(struct mlx5_eswitch *esw, bool clear_vf);
  int mlx5_eswitch_set_vport_mac(struct mlx5_eswitch *esw,
  			       u16 vport, u8 mac[ETH_ALEN]);
@@@ -613,7 -681,7 +626,11 @@@ void mlx5_eswitch_disable_pf_vf_vports(
  /* eswitch API stubs */
  static inline int  mlx5_eswitch_init(struct mlx5_core_dev *dev) { return 0; }
  static inline void mlx5_eswitch_cleanup(struct mlx5_eswitch *esw) {}
++<<<<<<< HEAD
 +static inline int  mlx5_eswitch_enable(struct mlx5_eswitch *esw, int mode) { return 0; }
++=======
+ static inline int mlx5_eswitch_enable(struct mlx5_eswitch *esw, int num_vfs) { return 0; }
++>>>>>>> 8e0aa4bc959c (net/mlx5: E-switch, Protect eswitch mode changes)
  static inline void mlx5_eswitch_disable(struct mlx5_eswitch *esw, bool clear_vf) {}
  static inline bool mlx5_esw_lag_prereq(struct mlx5_core_dev *dev0, struct mlx5_core_dev *dev1) { return true; }
  static inline bool mlx5_eswitch_is_funcs_handler(struct mlx5_core_dev *dev) { return false; }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index 98ef84b15725,612bc7d1cdcd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@@ -1370,47 -1438,190 +1370,58 @@@ out
  	return flow_rule;
  }
  
 -
 -static int mlx5_eswitch_inline_mode_get(const struct mlx5_eswitch *esw, u8 *mode)
 +static int esw_offloads_start(struct mlx5_eswitch *esw,
 +			      struct netlink_ext_ack *extack)
  {
 -	u8 prev_mlx5_mode, mlx5_mode = MLX5_INLINE_MODE_L2;
 -	struct mlx5_core_dev *dev = esw->dev;
 -	int vport;
 -
 -	if (!MLX5_CAP_GEN(dev, vport_group_manager))
 -		return -EOPNOTSUPP;
 -
 -	if (esw->mode == MLX5_ESWITCH_NONE)
 -		return -EOPNOTSUPP;
 +	int err, err1;
  
 -	switch (MLX5_CAP_ETH(dev, wqe_inline_mode)) {
 -	case MLX5_CAP_INLINE_MODE_NOT_REQUIRED:
 -		mlx5_mode = MLX5_INLINE_MODE_NONE;
 -		goto out;
 -	case MLX5_CAP_INLINE_MODE_L2:
 -		mlx5_mode = MLX5_INLINE_MODE_L2;
 -		goto out;
 -	case MLX5_CAP_INLINE_MODE_VPORT_CONTEXT:
 -		goto query_vports;
 +	if (esw->mode != MLX5_ESWITCH_LEGACY &&
 +	    !mlx5_core_is_ecpf_esw_manager(esw->dev)) {
 +		NL_SET_ERR_MSG_MOD(extack,
 +				   "Can't set offloads mode, SRIOV legacy not enabled");
 +		return -EINVAL;
  	}
  
 -query_vports:
 -	mlx5_query_nic_vport_min_inline(dev, esw->first_host_vport, &prev_mlx5_mode);
 -	mlx5_esw_for_each_host_func_vport(esw, vport, esw->esw_funcs.num_vfs) {
 -		mlx5_query_nic_vport_min_inline(dev, vport, &mlx5_mode);
 -		if (prev_mlx5_mode != mlx5_mode)
 -			return -EINVAL;
 -		prev_mlx5_mode = mlx5_mode;
++<<<<<<< HEAD
 +	mlx5_eswitch_disable(esw, false);
 +	mlx5_eswitch_update_num_of_vfs(esw, esw->dev->priv.sriov.num_vfs);
 +	err = mlx5_eswitch_enable(esw, MLX5_ESWITCH_OFFLOADS);
 +	if (err) {
 +		NL_SET_ERR_MSG_MOD(extack,
 +				   "Failed setting eswitch to offloads");
 +		err1 = mlx5_eswitch_enable(esw, MLX5_ESWITCH_LEGACY);
++=======
++	mlx5_eswitch_disable_locked(esw, false);
++	err = mlx5_eswitch_enable_locked(esw, MLX5_ESWITCH_OFFLOADS,
++					 esw->dev->priv.sriov.num_vfs);
++	if (err) {
++		NL_SET_ERR_MSG_MOD(extack,
++				   "Failed setting eswitch to offloads");
++		err1 = mlx5_eswitch_enable_locked(esw, MLX5_ESWITCH_LEGACY,
++						  MLX5_ESWITCH_IGNORE_NUM_VFS);
++>>>>>>> 8e0aa4bc959c (net/mlx5: E-switch, Protect eswitch mode changes)
 +		if (err1) {
 +			NL_SET_ERR_MSG_MOD(extack,
 +					   "Failed setting eswitch back to legacy");
 +		}
  	}
 +	if (esw->offloads.inline_mode == MLX5_INLINE_MODE_NONE) {
 +		if (mlx5_eswitch_inline_mode_get(esw,
 +						 &esw->offloads.inline_mode)) {
 +			esw->offloads.inline_mode = MLX5_INLINE_MODE_L2;
 +			NL_SET_ERR_MSG_MOD(extack,
 +					   "Inline mode is different between vports");
 +		}
 +	}
 +	return err;
 +}
  
 -out:
 -	*mode = mlx5_mode;
 -	return 0;
 -}       
 -
 -static void esw_destroy_restore_table(struct mlx5_eswitch *esw)
 +void esw_offloads_cleanup_reps(struct mlx5_eswitch *esw)
  {
 -	struct mlx5_esw_offload *offloads = &esw->offloads;
 -
 -	if (!mlx5_eswitch_reg_c1_loopback_supported(esw))
 -		return;
 -
 -	mlx5_modify_header_dealloc(esw->dev, offloads->restore_copy_hdr_id);
 -	mlx5_destroy_flow_group(offloads->restore_group);
 -	mlx5_destroy_flow_table(offloads->ft_offloads_restore);
 +	kfree(esw->offloads.vport_reps);
  }
  
 -static int esw_create_restore_table(struct mlx5_eswitch *esw)
 -{
 -	u8 modact[MLX5_UN_SZ_BYTES(set_action_in_add_action_in_auto)] = {};
 -	int inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);
 -	struct mlx5_flow_table_attr ft_attr = {};
 -	struct mlx5_core_dev *dev = esw->dev;
 -	struct mlx5_flow_namespace *ns;
 -	struct mlx5_modify_hdr *mod_hdr;
 -	void *match_criteria, *misc;
 -	struct mlx5_flow_table *ft;
 -	struct mlx5_flow_group *g;
 -	u32 *flow_group_in;
 -	int err = 0;
 -
 -	if (!mlx5_eswitch_reg_c1_loopback_supported(esw))
 -		return 0;
 -
 -	ns = mlx5_get_flow_namespace(dev, MLX5_FLOW_NAMESPACE_OFFLOADS);
 -	if (!ns) {
 -		esw_warn(esw->dev, "Failed to get offloads flow namespace\n");
 -		return -EOPNOTSUPP;
 -	}
 -
 -	flow_group_in = kvzalloc(inlen, GFP_KERNEL);
 -	if (!flow_group_in) {
 -		err = -ENOMEM;
 -		goto out_free;
 -	}
 -
 -	ft_attr.max_fte = 1 << ESW_CHAIN_TAG_METADATA_BITS;
 -	ft = mlx5_create_flow_table(ns, &ft_attr);
 -	if (IS_ERR(ft)) {
 -		err = PTR_ERR(ft);
 -		esw_warn(esw->dev, "Failed to create restore table, err %d\n",
 -			 err);
 -		goto out_free;
 -	}
 -
 -	memset(flow_group_in, 0, inlen);
 -	match_criteria = MLX5_ADDR_OF(create_flow_group_in, flow_group_in,
 -				      match_criteria);
 -	misc = MLX5_ADDR_OF(fte_match_param, match_criteria,
 -			    misc_parameters_2);
 -
 -	MLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,
 -		 ESW_CHAIN_TAG_METADATA_MASK);
 -	MLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 0);
 -	MLX5_SET(create_flow_group_in, flow_group_in, end_flow_index,
 -		 ft_attr.max_fte - 1);
 -	MLX5_SET(create_flow_group_in, flow_group_in, match_criteria_enable,
 -		 MLX5_MATCH_MISC_PARAMETERS_2);
 -	g = mlx5_create_flow_group(ft, flow_group_in);
 -	if (IS_ERR(g)) {
 -		err = PTR_ERR(g);
 -		esw_warn(dev, "Failed to create restore flow group, err: %d\n",
 -			 err);
 -		goto err_group;
 -	}
 -
 -	MLX5_SET(copy_action_in, modact, action_type, MLX5_ACTION_TYPE_COPY);
 -	MLX5_SET(copy_action_in, modact, src_field,
 -		 MLX5_ACTION_IN_FIELD_METADATA_REG_C_1);
 -	MLX5_SET(copy_action_in, modact, dst_field,
 -		 MLX5_ACTION_IN_FIELD_METADATA_REG_B);
 -	mod_hdr = mlx5_modify_header_alloc(esw->dev,
 -					   MLX5_FLOW_NAMESPACE_KERNEL, 1,
 -					   modact);
 -	if (IS_ERR(mod_hdr)) {
 -		esw_warn(dev, "Failed to create restore mod header, err: %d\n",
 -			 err);
 -		err = PTR_ERR(mod_hdr);
 -		goto err_mod_hdr;
 -	}
 -
 -	esw->offloads.ft_offloads_restore = ft;
 -	esw->offloads.restore_group = g;
 -	esw->offloads.restore_copy_hdr_id = mod_hdr;
 -
 -	kvfree(flow_group_in);
 -
 -	return 0;
 -
 -err_mod_hdr:
 -	mlx5_destroy_flow_group(g);
 -err_group:
 -	mlx5_destroy_flow_table(ft);
 -out_free:
 -	kvfree(flow_group_in);
 -
 -	return err;
 -}
 -
 -static int esw_offloads_start(struct mlx5_eswitch *esw,
 -			      struct netlink_ext_ack *extack)
 -{
 -	int err, err1;
 -
 -	if (esw->mode != MLX5_ESWITCH_LEGACY &&
 -	    !mlx5_core_is_ecpf_esw_manager(esw->dev)) {
 -		NL_SET_ERR_MSG_MOD(extack,
 -				   "Can't set offloads mode, SRIOV legacy not enabled");
 -		return -EINVAL;
 -	}
 -
 -	mlx5_eswitch_disable_locked(esw, false);
 -	err = mlx5_eswitch_enable_locked(esw, MLX5_ESWITCH_OFFLOADS,
 -					 esw->dev->priv.sriov.num_vfs);
 -	if (err) {
 -		NL_SET_ERR_MSG_MOD(extack,
 -				   "Failed setting eswitch to offloads");
 -		err1 = mlx5_eswitch_enable_locked(esw, MLX5_ESWITCH_LEGACY,
 -						  MLX5_ESWITCH_IGNORE_NUM_VFS);
 -		if (err1) {
 -			NL_SET_ERR_MSG_MOD(extack,
 -					   "Failed setting eswitch back to legacy");
 -		}
 -	}
 -	if (esw->offloads.inline_mode == MLX5_INLINE_MODE_NONE) {
 -		if (mlx5_eswitch_inline_mode_get(esw,
 -						 &esw->offloads.inline_mode)) {
 -			esw->offloads.inline_mode = MLX5_INLINE_MODE_L2;
 -			NL_SET_ERR_MSG_MOD(extack,
 -					   "Inline mode is different between vports");
 -		}
 -	}
 -	return err;
 -}
 -
 -void esw_offloads_cleanup_reps(struct mlx5_eswitch *esw)
 -{
 -	kfree(esw->offloads.vport_reps);
 -}
 -
 -int esw_offloads_init_reps(struct mlx5_eswitch *esw)
 +int esw_offloads_init_reps(struct mlx5_eswitch *esw)
  {
  	int total_vports = esw->total_vports;
  	struct mlx5_eswitch_rep *rep;
@@@ -2203,11 -2397,13 +2214,21 @@@ static int esw_offloads_stop(struct mlx
  {
  	int err, err1;
  
++<<<<<<< HEAD
 +	mlx5_eswitch_disable(esw, false);
 +	err = mlx5_eswitch_enable(esw, MLX5_ESWITCH_LEGACY);
 +	if (err) {
 +		NL_SET_ERR_MSG_MOD(extack, "Failed setting eswitch to legacy");
 +		err1 = mlx5_eswitch_enable(esw, MLX5_ESWITCH_OFFLOADS);
++=======
+ 	mlx5_eswitch_disable_locked(esw, false);
+ 	err = mlx5_eswitch_enable_locked(esw, MLX5_ESWITCH_LEGACY,
+ 					 MLX5_ESWITCH_IGNORE_NUM_VFS);
+ 	if (err) {
+ 		NL_SET_ERR_MSG_MOD(extack, "Failed setting eswitch to legacy");
+ 		err1 = mlx5_eswitch_enable_locked(esw, MLX5_ESWITCH_OFFLOADS,
+ 						  MLX5_ESWITCH_IGNORE_NUM_VFS);
++>>>>>>> 8e0aa4bc959c (net/mlx5: E-switch, Protect eswitch mode changes)
  		if (err1) {
  			NL_SET_ERR_MSG_MOD(extack,
  					   "Failed setting eswitch back to offloads");
@@@ -2442,50 -2654,17 +2479,54 @@@ int mlx5_devlink_eswitch_inline_mode_ge
  	if (err)
  		return err;
  
+ 	mutex_lock(&esw->mode_lock);
  	err = eswitch_devlink_esw_mode_check(esw);
  	if (err)
- 		return err;
+ 		goto unlock;
  
- 	return esw_inline_mode_to_devlink(esw->offloads.inline_mode, mode);
+ 	err = esw_inline_mode_to_devlink(esw->offloads.inline_mode, mode);
+ unlock:
+ 	mutex_unlock(&esw->mode_lock);
+ 	return err;
  }
  
 +int mlx5_eswitch_inline_mode_get(struct mlx5_eswitch *esw, u8 *mode)
 +{
 +	u8 prev_mlx5_mode, mlx5_mode = MLX5_INLINE_MODE_L2;
 +	struct mlx5_core_dev *dev = esw->dev;
 +	int vport;
 +
 +	if (!MLX5_CAP_GEN(dev, vport_group_manager))
 +		return -EOPNOTSUPP;
 +
 +	if (esw->mode == MLX5_ESWITCH_NONE)
 +		return -EOPNOTSUPP;
 +
 +	switch (MLX5_CAP_ETH(dev, wqe_inline_mode)) {
 +	case MLX5_CAP_INLINE_MODE_NOT_REQUIRED:
 +		mlx5_mode = MLX5_INLINE_MODE_NONE;
 +		goto out;
 +	case MLX5_CAP_INLINE_MODE_L2:
 +		mlx5_mode = MLX5_INLINE_MODE_L2;
 +		goto out;
 +	case MLX5_CAP_INLINE_MODE_VPORT_CONTEXT:
 +		goto query_vports;
 +	}
 +
 +query_vports:
 +	mlx5_query_nic_vport_min_inline(dev, esw->first_host_vport, &prev_mlx5_mode);
 +	mlx5_esw_for_each_host_func_vport(esw, vport, esw->esw_funcs.num_vfs) {
 +		mlx5_query_nic_vport_min_inline(dev, vport, &mlx5_mode);
 +		if (prev_mlx5_mode != mlx5_mode)
 +			return -EINVAL;
 +		prev_mlx5_mode = mlx5_mode;
 +	}
 +
 +out:
 +	*mode = mlx5_mode;
 +	return 0;
 +}
 +
  int mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink,
  					enum devlink_eswitch_encap_mode encap,
  					struct netlink_ext_ack *extack)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/sriov.c
index 03f037811f1d,3094d20297a9..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
@@@ -77,8 -77,7 +77,12 @@@ static int mlx5_device_enable_sriov(str
  	if (!MLX5_ESWITCH_MANAGER(dev))
  		goto enable_vfs_hca;
  
++<<<<<<< HEAD
 +	mlx5_eswitch_update_num_of_vfs(dev->priv.eswitch, num_vfs);
 +	err = mlx5_eswitch_enable(dev->priv.eswitch, MLX5_ESWITCH_LEGACY);
++=======
+ 	err = mlx5_eswitch_enable(dev->priv.eswitch, num_vfs);
++>>>>>>> 8e0aa4bc959c (net/mlx5: E-switch, Protect eswitch mode changes)
  	if (err) {
  		mlx5_core_warn(dev,
  			       "failed to enable eswitch SRIOV (%d)\n", err);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/sriov.c
