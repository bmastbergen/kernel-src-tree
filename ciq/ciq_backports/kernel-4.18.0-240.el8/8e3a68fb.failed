dma-mapping: make dma_atomic_pool_init self-contained

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 8e3a68fb55e00e0760bd8023883e064f1f93c62d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/8e3a68fb.failed

The memory allocated for the atomic pool needs to have the same
mapping attributes that we use for remapping, so use
pgprot_dmacoherent instead of open coding it.  Also deduct a
suitable zone to allocate the memory from based on the presence
of the DMA zones.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 8e3a68fb55e00e0760bd8023883e064f1f93c62d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/mm/dma.c
#	arch/csky/mm/dma-mapping.c
#	arch/nds32/kernel/dma.c
diff --cc arch/arc/mm/dma.c
index ec47e6079f5d,ff4a5752f8cc..000000000000
--- a/arch/arc/mm/dma.c
+++ b/arch/arc/mm/dma.c
@@@ -185,3 -86,21 +185,24 @@@ void arch_sync_dma_for_cpu(struct devic
  		break;
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Plug in direct dma map ops.
+  */
+ void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+ 			const struct iommu_ops *iommu, bool coherent)
+ {
+ 	/*
+ 	 * IOC hardware snoops all DMA traffic keeping the caches consistent
+ 	 * with memory - eliding need for any explicit cache maintenance of
+ 	 * DMA buffers.
+ 	 */
+ 	if (is_isa_arcv2() && ioc_enable && coherent)
+ 		dev->dma_coherent = true;
+ 
+ 	dev_info(dev, "use %sncoherent DMA ops\n",
+ 		 dev->dma_coherent ? "" : "non");
+ }
++>>>>>>> 8e3a68fb55e0 (dma-mapping: make dma_atomic_pool_init self-contained)
diff --cc arch/nds32/kernel/dma.c
index d0dbd4fe9645,4206d4b6c8ce..000000000000
--- a/arch/nds32/kernel/dma.c
+++ b/arch/nds32/kernel/dma.c
@@@ -389,3 -75,8 +389,11 @@@ void arch_sync_dma_for_cpu(struct devic
  		BUG();
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ void arch_dma_prep_coherent(struct page *page, size_t size)
+ {
+ 	cache_op(page_to_phys(page), size, cpu_dma_wbinval_range);
+ }
++>>>>>>> 8e3a68fb55e0 (dma-mapping: make dma_atomic_pool_init self-contained)
* Unmerged path arch/csky/mm/dma-mapping.c
* Unmerged path arch/arc/mm/dma.c
diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c
index fa63a9f40045..0969153b2be0 100644
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@ -57,12 +57,6 @@ void arch_dma_prep_coherent(struct page *page, size_t size)
 	__dma_flush_area(page_address(page), size);
 }
 
-static int __init arm64_dma_init(void)
-{
-	return dma_atomic_pool_init(GFP_DMA32, __pgprot(PROT_NORMAL_NC));
-}
-arch_initcall(arm64_dma_init);
-
 #ifdef CONFIG_IOMMU_DMA
 void arch_teardown_dma_ops(struct device *dev)
 {
* Unmerged path arch/csky/mm/dma-mapping.c
* Unmerged path arch/nds32/kernel/dma.c
diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f08f222fef10..f84c12163dfd 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -627,7 +627,6 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 			const void *caller);
 void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
 
-int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot);
 bool dma_in_atomic_pool(void *start, size_t size);
 void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
 bool dma_free_from_pool(void *start, size_t size);
diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index ffe78f0b2fe4..838123f79639 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -105,7 +105,16 @@ static int __init early_coherent_pool(char *p)
 }
 early_param("coherent_pool", early_coherent_pool);
 
-int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
+static gfp_t dma_atomic_pool_gfp(void)
+{
+	if (IS_ENABLED(CONFIG_ZONE_DMA))
+		return GFP_DMA;
+	if (IS_ENABLED(CONFIG_ZONE_DMA32))
+		return GFP_DMA32;
+	return GFP_KERNEL;
+}
+
+static int __init dma_atomic_pool_init(void)
 {
 	unsigned int pool_size_order = get_order(atomic_pool_size);
 	unsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;
@@ -117,7 +126,7 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 		page = dma_alloc_from_contiguous(NULL, nr_pages,
 						 pool_size_order, false);
 	else
-		page = alloc_pages(gfp, pool_size_order);
+		page = alloc_pages(dma_atomic_pool_gfp(), pool_size_order);
 	if (!page)
 		goto out;
 
@@ -128,7 +137,8 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 		goto free_page;
 
 	addr = dma_common_contiguous_remap(page, atomic_pool_size, VM_USERMAP,
-					   prot, __builtin_return_address(0));
+					   pgprot_dmacoherent(PAGE_KERNEL),
+					   __builtin_return_address(0));
 	if (!addr)
 		goto destroy_genpool;
 
@@ -155,6 +165,7 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 		atomic_pool_size / 1024);
 	return -ENOMEM;
 }
+postcore_initcall(dma_atomic_pool_init);
 
 bool dma_in_atomic_pool(void *start, size_t size)
 {
