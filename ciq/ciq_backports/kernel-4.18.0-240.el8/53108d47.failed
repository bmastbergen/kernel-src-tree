io_uring: only hash regular files for async work execution

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 53108d476a105ab2597d7a4e6040b127829391b5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/53108d47.failed

We hash regular files to avoid having multiple threads hammer on the
inode mutex, but it should not be needed on other types of files
(like sockets).

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 53108d476a105ab2597d7a4e6040b127829391b5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7e2b8c92aeeb,e54bd469d53a..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -467,21 -564,93 +467,42 @@@ static void __io_commit_cqring(struct i
  	}
  }
  
 -static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 -{
 -	u8 opcode = READ_ONCE(sqe->opcode);
 -
 -	return !(opcode == IORING_OP_READ_FIXED ||
 -		 opcode == IORING_OP_WRITE_FIXED);
 -}
 -
 -static inline bool io_prep_async_work(struct io_kiocb *req,
 -				      struct io_kiocb **link)
 +static inline void io_queue_async_work(struct io_ring_ctx *ctx,
 +				       struct io_kiocb *req)
  {
 -	bool do_hashed = false;
 +	int rw = 0;
  
 -	if (req->sqe) {
 -		switch (req->sqe->opcode) {
 +	if (req->submit.sqe) {
 +		switch (req->submit.sqe->opcode) {
  		case IORING_OP_WRITEV:
  		case IORING_OP_WRITE_FIXED:
++<<<<<<< HEAD
 +			rw = !(req->rw.ki_flags & IOCB_DIRECT);
++=======
+ 			/* only regular files should be hashed for writes */
+ 			if (req->flags & REQ_F_ISREG)
+ 				do_hashed = true;
+ 			/* fall-through */
+ 		case IORING_OP_READV:
+ 		case IORING_OP_READ_FIXED:
+ 		case IORING_OP_SENDMSG:
+ 		case IORING_OP_RECVMSG:
+ 		case IORING_OP_ACCEPT:
+ 		case IORING_OP_POLL_ADD:
+ 		case IORING_OP_CONNECT:
+ 			/*
+ 			 * We know REQ_F_ISREG is not set on some of these
+ 			 * opcodes, but this enables us to keep the check in
+ 			 * just one place.
+ 			 */
+ 			if (!(req->flags & REQ_F_ISREG))
+ 				req->work.flags |= IO_WQ_WORK_UNBOUND;
++>>>>>>> 53108d476a10 (io_uring: only hash regular files for async work execution)
  			break;
  		}
 -		if (io_sqe_needs_user(req->sqe))
 -			req->work.flags |= IO_WQ_WORK_NEEDS_USER;
 -	}
 -
 -	*link = io_prep_linked_timeout(req);
 -	return do_hashed;
 -}
 -
 -static inline void io_queue_async_work(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_kiocb *link;
 -	bool do_hashed;
 -
 -	do_hashed = io_prep_async_work(req, &link);
 -
 -	trace_io_uring_queue_async_work(ctx, do_hashed, req, &req->work,
 -					req->flags);
 -	if (!do_hashed) {
 -		io_wq_enqueue(ctx->io_wq, &req->work);
 -	} else {
 -		io_wq_enqueue_hashed(ctx->io_wq, &req->work,
 -					file_inode(req->file));
 -	}
 -
 -	if (link)
 -		io_queue_linked_timeout(link);
 -}
 -
 -static void io_kill_timeout(struct io_kiocb *req)
 -{
 -	int ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
 -	if (ret != -1) {
 -		atomic_inc(&req->ctx->cq_timeouts);
 -		list_del_init(&req->list);
 -		io_cqring_fill_event(req, 0);
 -		io_put_req(req);
  	}
 -}
 -
 -static void io_kill_timeouts(struct io_ring_ctx *ctx)
 -{
 -	struct io_kiocb *req, *tmp;
  
 -	spin_lock_irq(&ctx->completion_lock);
 -	list_for_each_entry_safe(req, tmp, &ctx->timeout_list, list)
 -		io_kill_timeout(req);
 -	spin_unlock_irq(&ctx->completion_lock);
 +	queue_work(ctx->sqo_wq[rw], &req->work);
  }
  
  static void io_commit_cqring(struct io_ring_ctx *ctx)
* Unmerged path fs/io_uring.c
