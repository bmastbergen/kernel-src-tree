io_uring: return locked and pinned page accounting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit cbcf72148da4af55ea81cfb351ea7c026ff1014f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/cbcf7214.failed

Locked and pinned memory accounting in io_{,un}account_mem() depends on
having ->sqo_mm, which is NULL after a recent change for non SQPOLL'ed
io_ring. That disables the accounting.

Return ->sqo_mm initialisation back, and do __io_sq_thread_acquire_mm()
based on IORING_SETUP_SQPOLL flag.

Fixes: 8eb06d7e8dd85 ("io_uring: fix missing ->mm on exit")
	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit cbcf72148da4af55ea81cfb351ea7c026ff1014f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index bff2058d3f07,680b16f71a03..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -386,6 -947,81 +386,84 @@@ struct sock *io_uring_get_socket(struc
  }
  EXPORT_SYMBOL(io_uring_get_socket);
  
++<<<<<<< HEAD
++=======
+ static void io_get_req_task(struct io_kiocb *req)
+ {
+ 	if (req->flags & REQ_F_TASK_PINNED)
+ 		return;
+ 	get_task_struct(req->task);
+ 	req->flags |= REQ_F_TASK_PINNED;
+ }
+ 
+ static inline void io_clean_op(struct io_kiocb *req)
+ {
+ 	if (req->flags & (REQ_F_NEED_CLEANUP | REQ_F_BUFFER_SELECTED))
+ 		__io_clean_op(req);
+ }
+ 
+ /* not idempotent -- it doesn't clear REQ_F_TASK_PINNED */
+ static void __io_put_req_task(struct io_kiocb *req)
+ {
+ 	if (req->flags & REQ_F_TASK_PINNED)
+ 		put_task_struct(req->task);
+ }
+ 
+ static void io_sq_thread_drop_mm(void)
+ {
+ 	struct mm_struct *mm = current->mm;
+ 
+ 	if (mm) {
+ 		kthread_unuse_mm(mm);
+ 		mmput(mm);
+ 	}
+ }
+ 
+ static int __io_sq_thread_acquire_mm(struct io_ring_ctx *ctx)
+ {
+ 	if (!current->mm) {
+ 		if (unlikely(!(ctx->flags & IORING_SETUP_SQPOLL) ||
+ 			     !mmget_not_zero(ctx->sqo_mm)))
+ 			return -EFAULT;
+ 		kthread_use_mm(ctx->sqo_mm);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int io_sq_thread_acquire_mm(struct io_ring_ctx *ctx,
+ 				   struct io_kiocb *req)
+ {
+ 	if (!io_op_defs[req->opcode].needs_mm)
+ 		return 0;
+ 	return __io_sq_thread_acquire_mm(ctx);
+ }
+ 
+ static inline void req_set_fail_links(struct io_kiocb *req)
+ {
+ 	if ((req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) == REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ }
+ 
+ /*
+  * Note: must call io_req_init_async() for the first time you
+  * touch any members of io_wq_work.
+  */
+ static inline void io_req_init_async(struct io_kiocb *req)
+ {
+ 	if (req->flags & REQ_F_WORK_INITIALIZED)
+ 		return;
+ 
+ 	memset(&req->work, 0, sizeof(req->work));
+ 	req->flags |= REQ_F_WORK_INITIALIZED;
+ }
+ 
+ static inline bool io_async_submit(struct io_ring_ctx *ctx)
+ {
+ 	return ctx->flags & IORING_SETUP_SQPOLL;
+ }
+ 
++>>>>>>> cbcf72148da4 (io_uring: return locked and pinned page accounting)
  static void io_ring_ctx_ref_free(struct percpu_ref *ref)
  {
  	struct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);
* Unmerged path fs/io_uring.c
