IB/mlx5: Unify ODP MR code paths to allow extra flexibility

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Artemy Kovalyov <artemyko@mellanox.com>
commit cbe4b8f0a5766a40563876932cba6c9bf28eb98a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/cbe4b8f0.failed

Building MR translation table in the ODP case requires additional
flexibility, namely random access to DMA addresses. Make both direct and
indirect ODP MR use same code path, separated from the non-ODP MR code
path.

With the restructuring the correct page_shift is now used around
__mlx5_ib_populate_pas().

Fixes: d2183c6f1958 ("RDMA/umem: Move page_shift from ib_umem to ib_odp_umem")
Link: https://lore.kernel.org/r/20191222124649.52300-2-leon@kernel.org
	Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit cbe4b8f0a5766a40563876932cba6c9bf28eb98a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/odp.c
index df3038ca913b,92da6c4f7ddd..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -93,86 -93,15 +93,91 @@@ struct mlx5_pagefault 
  
  static u64 mlx5_imr_ksm_entries;
  
++<<<<<<< HEAD
 +static int check_parent(struct ib_umem_odp *odp,
 +			       struct mlx5_ib_mr *parent)
++=======
+ static void populate_klm(struct mlx5_klm *pklm, size_t idx, size_t nentries,
+ 			struct mlx5_ib_mr *imr, int flags)
++>>>>>>> cbe4b8f0a576 (IB/mlx5: Unify ODP MR code paths to allow extra flexibility)
  {
 -	struct mlx5_klm *end = pklm + nentries;
 +	struct mlx5_ib_mr *mr = odp->private;
 +
 +	return mr && mr->parent == parent && !odp->dying;
 +}
 +
 +static struct ib_ucontext_per_mm *mr_to_per_mm(struct mlx5_ib_mr *mr)
 +{
 +	if (WARN_ON(!mr || !is_odp_mr(mr)))
 +		return NULL;
 +
 +	return to_ib_umem_odp(mr->umem)->per_mm;
 +}
 +
 +static struct ib_umem_odp *odp_next(struct ib_umem_odp *odp)
 +{
 +	struct mlx5_ib_mr *mr = odp->private, *parent = mr->parent;
 +	struct ib_ucontext_per_mm *per_mm = odp->per_mm;
 +	struct rb_node *rb;
 +
 +	down_read(&per_mm->umem_rwsem);
 +	while (1) {
 +		rb = rb_next(&odp->interval_tree.rb);
 +		if (!rb)
 +			goto not_found;
 +		odp = rb_entry(rb, struct ib_umem_odp, interval_tree.rb);
 +		if (check_parent(odp, parent))
 +			goto end;
 +	}
 +not_found:
 +	odp = NULL;
 +end:
 +	up_read(&per_mm->umem_rwsem);
 +	return odp;
 +}
 +
 +static struct ib_umem_odp *odp_lookup(u64 start, u64 length,
 +				      struct mlx5_ib_mr *parent)
 +{
 +	struct ib_ucontext_per_mm *per_mm = mr_to_per_mm(parent);
 +	struct ib_umem_odp *odp;
 +	struct rb_node *rb;
 +
 +	down_read(&per_mm->umem_rwsem);
 +	odp = rbt_ib_umem_lookup(&per_mm->umem_tree, start, length);
 +	if (!odp)
 +		goto end;
 +
 +	while (1) {
 +		if (check_parent(odp, parent))
 +			goto end;
 +		rb = rb_next(&odp->interval_tree.rb);
 +		if (!rb)
 +			goto not_found;
 +		odp = rb_entry(rb, struct ib_umem_odp, interval_tree.rb);
 +		if (ib_umem_start(odp) > start + length)
 +			goto not_found;
 +	}
 +not_found:
 +	odp = NULL;
 +end:
 +	up_read(&per_mm->umem_rwsem);
 +	return odp;
 +}
 +
 +void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 +			   size_t nentries, struct mlx5_ib_mr *mr, int flags)
 +{
 +	struct ib_pd *pd = mr->ibmr.pd;
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct ib_umem_odp *odp;
 +	unsigned long va;
 +	int i;
  
  	if (flags & MLX5_IB_UPD_XLT_ZAP) {
 -		for (; pklm != end; pklm++, idx++) {
 +		for (i = 0; i < nentries; i++, pklm++) {
  			pklm->bcount = cpu_to_be32(MLX5_IMR_MTT_SIZE);
 -			pklm->key = cpu_to_be32(imr->dev->null_mkey);
 +			pklm->key = cpu_to_be32(dev->null_mkey);
  			pklm->va = 0;
  		}
  		return;
@@@ -220,21 -144,86 +225,63 @@@
  	}
  }
  
++<<<<<<< HEAD
 +static void mr_leaf_free_action(struct work_struct *work)
++=======
+ static u64 umem_dma_to_mtt(dma_addr_t umem_dma)
+ {
+ 	u64 mtt_entry = umem_dma & ODP_DMA_ADDR_MASK;
+ 
+ 	if (umem_dma & ODP_READ_ALLOWED_BIT)
+ 		mtt_entry |= MLX5_IB_MTT_READ;
+ 	if (umem_dma & ODP_WRITE_ALLOWED_BIT)
+ 		mtt_entry |= MLX5_IB_MTT_WRITE;
+ 
+ 	return mtt_entry;
+ }
+ 
+ static void populate_mtt(__be64 *pas, size_t idx, size_t nentries,
+ 			 struct mlx5_ib_mr *mr, int flags)
+ {
+ 	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
+ 	dma_addr_t pa;
+ 	size_t i;
+ 
+ 	if (flags & MLX5_IB_UPD_XLT_ZAP)
+ 		return;
+ 
+ 	for (i = 0; i < nentries; i++) {
+ 		pa = odp->dma_list[idx + i];
+ 		pas[i] = cpu_to_be64(umem_dma_to_mtt(pa));
+ 	}
+ }
+ 
+ void mlx5_odp_populate_xlt(void *xlt, size_t idx, size_t nentries,
+ 			   struct mlx5_ib_mr *mr, int flags)
+ {
+ 	if (flags & MLX5_IB_UPD_XLT_INDIRECT) {
+ 		populate_klm(xlt, idx, nentries, mr, flags);
+ 	} else {
+ 		populate_mtt(xlt, idx, nentries, mr, flags);
+ 	}
+ }
+ 
+ static void dma_fence_odp_mr(struct mlx5_ib_mr *mr)
++>>>>>>> cbe4b8f0a576 (IB/mlx5: Unify ODP MR code paths to allow extra flexibility)
  {
 -	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
 -
 -	/* Ensure mlx5_ib_invalidate_range() will not touch the MR any more */
 -	mutex_lock(&odp->umem_mutex);
 -	if (odp->npages) {
 -		mlx5_mr_cache_invalidate(mr);
 -		ib_umem_odp_unmap_dma_pages(odp, ib_umem_start(odp),
 -					    ib_umem_end(odp));
 -		WARN_ON(odp->npages);
 -	}
 -	odp->private = NULL;
 -	mutex_unlock(&odp->umem_mutex);
 -
 -	if (!mr->allocated_from_cache) {
 -		mlx5_core_destroy_mkey(mr->dev->mdev, &mr->mmkey);
 -		WARN_ON(mr->descs);
 -	}
 -}
 -
 -/*
 - * This must be called after the mr has been removed from implicit_children
 - * and the SRCU synchronized.  NOTE: The MR does not necessarily have to be
 - * empty here, parallel page faults could have raced with the free process and
 - * added pages to it.
 - */
 -static void free_implicit_child_mr(struct mlx5_ib_mr *mr, bool need_imr_xlt)
 -{
 -	struct mlx5_ib_mr *imr = mr->parent;
 +	struct ib_umem_odp *odp = container_of(work, struct ib_umem_odp, work);
 +	int idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
 +	struct mlx5_ib_mr *mr = odp->private, *imr = mr->parent;
  	struct ib_umem_odp *odp_imr = to_ib_umem_odp(imr->umem);
 -	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
 -	unsigned long idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
  	int srcu_key;
  
 -	/* implicit_child_mr's are not allowed to have deferred work */
 -	WARN_ON(atomic_read(&mr->num_deferred_work));
 +	mr->parent = NULL;
 +	synchronize_srcu(&mr->dev->mr_srcu);
  
 -	if (need_imr_xlt) {
 -		srcu_key = srcu_read_lock(&mr->dev->odp_srcu);
 +	if (imr->live) {
 +		srcu_key = srcu_read_lock(&mr->dev->mr_srcu);
  		mutex_lock(&odp_imr->umem_mutex);
 -		mlx5_ib_update_xlt(mr->parent, idx, 1, 0,
 +		mlx5_ib_update_xlt(imr, idx, 1, 0,
  				   MLX5_IB_UPD_XLT_INDIRECT |
  				   MLX5_IB_UPD_XLT_ATOMIC);
  		mutex_unlock(&odp_imr->umem_mutex);
diff --git a/drivers/infiniband/hw/mlx5/mem.c b/drivers/infiniband/hw/mlx5/mem.c
index de69e47205e3..e542f37276b0 100644
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@ -114,18 +114,6 @@ void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr,
 	*count = i;
 }
 
-static u64 umem_dma_to_mtt(dma_addr_t umem_dma)
-{
-	u64 mtt_entry = umem_dma & ODP_DMA_ADDR_MASK;
-
-	if (umem_dma & ODP_READ_ALLOWED_BIT)
-		mtt_entry |= MLX5_IB_MTT_READ;
-	if (umem_dma & ODP_WRITE_ALLOWED_BIT)
-		mtt_entry |= MLX5_IB_MTT_WRITE;
-
-	return mtt_entry;
-}
-
 /*
  * Populate the given array with bus addresses from the umem.
  *
@@ -152,19 +140,6 @@ void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 	struct scatterlist *sg;
 	int entry;
 
-	if (umem->is_odp) {
-		WARN_ON(shift != 0);
-		WARN_ON(access_flags != (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE));
-
-		for (i = 0; i < num_pages; ++i) {
-			dma_addr_t pa =
-				to_ib_umem_odp(umem)->dma_list[offset + i];
-
-			pas[i] = cpu_to_be64(umem_dma_to_mtt(pa));
-		}
-		return;
-	}
-
 	i = 0;
 	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {
 		len = sg_dma_len(sg) >> PAGE_SHIFT;
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b39d154065df..1a6075db3992 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1251,8 +1251,8 @@ void mlx5_ib_odp_cleanup(void);
 void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
 			      unsigned long end);
 void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
-void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
-			   size_t nentries, struct mlx5_ib_mr *mr, int flags);
+void mlx5_odp_populate_xlt(void *xlt, size_t idx, size_t nentries,
+			   struct mlx5_ib_mr *mr, int flags);
 
 int mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
 			       enum ib_uverbs_advise_mr_advice advice,
@@ -1268,9 +1268,8 @@ static inline void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *ibdev) {}
 static inline int mlx5_ib_odp_init(void) { return 0; }
 static inline void mlx5_ib_odp_cleanup(void)				    {}
 static inline void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent) {}
-static inline void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
-					 size_t nentries, struct mlx5_ib_mr *mr,
-					 int flags) {}
+static inline void mlx5_odp_populate_xlt(void *xlt, size_t idx, size_t nentries,
+					 struct mlx5_ib_mr *mr, int flags) {}
 
 static inline int
 mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index c7b8ab228778..4a62a06797c0 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -887,36 +887,6 @@ static struct mlx5_ib_mr *alloc_mr_from_cache(
 	return mr;
 }
 
-static inline int populate_xlt(struct mlx5_ib_mr *mr, int idx, int npages,
-			       void *xlt, int page_shift, size_t size,
-			       int flags)
-{
-	struct mlx5_ib_dev *dev = mr->dev;
-	struct ib_umem *umem = mr->umem;
-
-	if (flags & MLX5_IB_UPD_XLT_INDIRECT) {
-		if (!umr_can_use_indirect_mkey(dev))
-			return -EPERM;
-		mlx5_odp_populate_klm(xlt, idx, npages, mr, flags);
-		return npages;
-	}
-
-	npages = min_t(size_t, npages, ib_umem_num_pages(umem) - idx);
-
-	if (!(flags & MLX5_IB_UPD_XLT_ZAP)) {
-		__mlx5_ib_populate_pas(dev, umem, page_shift,
-				       idx, npages, xlt,
-				       MLX5_IB_MTT_PRESENT);
-		/* Clear padding after the pages
-		 * brought from the umem.
-		 */
-		memset(xlt + (npages * sizeof(struct mlx5_mtt)), 0,
-		       size - npages * sizeof(struct mlx5_mtt));
-	}
-
-	return npages;
-}
-
 #define MLX5_MAX_UMR_CHUNK ((1 << (MLX5_MAX_UMR_SHIFT + 4)) - \
 			    MLX5_UMR_MTT_ALIGNMENT)
 #define MLX5_SPARE_UMR_CHUNK 0x10000
@@ -940,6 +910,7 @@ int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
 	size_t pages_mapped = 0;
 	size_t pages_to_map = 0;
 	size_t pages_iter = 0;
+	size_t size_to_map = 0;
 	gfp_t gfp;
 	bool use_emergency_page = false;
 
@@ -986,6 +957,15 @@ int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
 		goto free_xlt;
 	}
 
+	if (mr->umem->is_odp) {
+		if (!(flags & MLX5_IB_UPD_XLT_INDIRECT)) {
+			struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
+			size_t max_pages = ib_umem_odp_num_pages(odp) - idx;
+
+			pages_to_map = min_t(size_t, pages_to_map, max_pages);
+		}
+	}
+
 	sg.addr = dma;
 	sg.lkey = dev->umrc.pd->local_dma_lkey;
 
@@ -1008,14 +988,22 @@ int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
 	     pages_mapped < pages_to_map && !err;
 	     pages_mapped += pages_iter, idx += pages_iter) {
 		npages = min_t(int, pages_iter, pages_to_map - pages_mapped);
+		size_to_map = npages * desc_size;
 		dma_sync_single_for_cpu(ddev, dma, size, DMA_TO_DEVICE);
-		npages = populate_xlt(mr, idx, npages, xlt,
-				      page_shift, size, flags);
-
+		if (mr->umem->is_odp) {
+			mlx5_odp_populate_xlt(xlt, idx, npages, mr, flags);
+		} else {
+			__mlx5_ib_populate_pas(dev, mr->umem, page_shift, idx,
+					       npages, xlt,
+					       MLX5_IB_MTT_PRESENT);
+			/* Clear padding after the pages
+			 * brought from the umem.
+			 */
+			memset(xlt + size_to_map, 0, size - size_to_map);
+		}
 		dma_sync_single_for_device(ddev, dma, size, DMA_TO_DEVICE);
 
-		sg.length = ALIGN(npages * desc_size,
-				  MLX5_UMR_MTT_ALIGNMENT);
+		sg.length = ALIGN(size_to_map, MLX5_UMR_MTT_ALIGNMENT);
 
 		if (pages_mapped + pages_iter >= pages_to_map) {
 			if (flags & MLX5_IB_UPD_XLT_ENABLE)
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
