io_uring: make poll->wait dynamically allocated

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit e944475e69849273ca8f1fe04a3ce81b5901d165
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e944475e.failed

In the quest to bring io_kiocb down to 3 cachelines, this one does
the trick. Make the wait_queue_entry for the poll command come out
of kmalloc instead of embedding it in struct io_poll_iocb, as the
latter is the largest member of io_kiocb. Once we trim this down a
bit, we're back at a healthy 192 bytes for struct io_kiocb.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit e944475e69849273ca8f1fe04a3ce81b5901d165)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 9058714611f6,2c2e8c25da01..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -304,9 -291,22 +304,9 @@@ struct io_poll_iocb 
  	__poll_t			events;
  	bool				done;
  	bool				canceled;
- 	struct wait_queue_entry		wait;
+ 	struct wait_queue_entry		*wait;
  };
  
 -struct io_timeout_data {
 -	struct io_kiocb			*req;
 -	struct hrtimer			timer;
 -	struct timespec64		ts;
 -	enum hrtimer_mode		mode;
 -	u32				seq_offset;
 -};
 -
 -struct io_timeout {
 -	struct file			*file;
 -	struct io_timeout_data		*data;
 -};
 -
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -1639,13 -2030,12 +1639,19 @@@ static void io_poll_remove_one(struct i
  
  	spin_lock(&poll->head->lock);
  	WRITE_ONCE(poll->canceled, true);
++<<<<<<< HEAD
 +	if (!list_empty(&poll->wait.entry)) {
 +		list_del_init(&poll->wait.entry);
 +		io_queue_async_work(req->ctx, req);
++=======
+ 	if (!list_empty(&poll->wait->entry)) {
+ 		list_del_init(&poll->wait->entry);
+ 		io_queue_async_work(req);
++>>>>>>> e944475e6984 (io_uring: make poll->wait dynamically allocated)
  	}
  	spin_unlock(&poll->head->lock);
 -	io_poll_remove_req(req);
 +
 +	list_del_init(&req->list);
  }
  
  static void io_poll_remove_all(struct io_ring_ctx *ctx)
@@@ -1691,11 -2099,16 +1697,19 @@@ static int io_poll_remove(struct io_kio
  	return 0;
  }
  
 -static void io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)
 +static void io_poll_complete(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			     __poll_t mask)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -
  	req->poll.done = true;
++<<<<<<< HEAD
 +	io_cqring_fill_event(ctx, req->user_data, mangle_poll(mask));
++=======
+ 	kfree(req->poll.wait);
+ 	if (error)
+ 		io_cqring_fill_event(req, error);
+ 	else
+ 		io_cqring_fill_event(req, mangle_poll(mask));
++>>>>>>> e944475e6984 (io_uring: make poll->wait dynamically allocated)
  	io_commit_cqring(ctx);
  }
  
@@@ -1718,8 -2141,8 +1732,13 @@@ static void io_poll_complete_work(struc
  	 * avoid further branches in the fast path.
  	 */
  	spin_lock_irq(&ctx->completion_lock);
++<<<<<<< HEAD
 +	if (!mask && !READ_ONCE(poll->canceled)) {
 +		add_wait_queue(poll->head, &poll->wait);
++=======
+ 	if (!mask && ret != -ECANCELED) {
+ 		add_wait_queue(poll->head, poll->wait);
++>>>>>>> e944475e6984 (io_uring: make poll->wait dynamically allocated)
  		spin_unlock_irq(&ctx->completion_lock);
  		return;
  	}
@@@ -1745,11 -2172,19 +1763,11 @@@ static int io_poll_wake(struct wait_que
  	if (mask && !(mask & poll->events))
  		return 0;
  
- 	list_del_init(&poll->wait.entry);
+ 	list_del_init(&poll->wait->entry);
  
 -	/*
 -	 * Run completion inline if we can. We're using trylock here because
 -	 * we are violating the completion_lock -> poll wq lock ordering.
 -	 * If we have a link timeout we're going to need the completion_lock
 -	 * for finalizing the request, mark us as having grabbed that already.
 -	 */
  	if (mask && spin_trylock_irqsave(&ctx->completion_lock, flags)) {
 -		io_poll_remove_req(req);
 -		io_poll_complete(req, mask, 0);
 -		req->flags |= REQ_F_COMP_LOCKED;
 -		io_put_req(req);
 +		list_del(&req->list);
 +		io_poll_complete(ctx, req, mask);
  		spin_unlock_irqrestore(&ctx->completion_lock, flags);
  
  		io_cqring_ev_posted(ctx);
@@@ -1779,10 -2213,30 +1797,10 @@@ static void io_poll_queue_proc(struct f
  
  	pt->error = 0;
  	pt->req->poll.head = head;
- 	add_wait_queue(head, &pt->req->poll.wait);
+ 	add_wait_queue(head, pt->req->poll.wait);
  }
  
 -static void io_poll_req_insert(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct rb_node **p = &ctx->cancel_tree.rb_node;
 -	struct rb_node *parent = NULL;
 -	struct io_kiocb *tmp;
 -
 -	while (*p) {
 -		parent = *p;
 -		tmp = rb_entry(parent, struct io_kiocb, rb_node);
 -		if (req->user_data < tmp->user_data)
 -			p = &(*p)->rb_left;
 -		else
 -			p = &(*p)->rb_right;
 -	}
 -	rb_link_node(&req->rb_node, parent, p);
 -	rb_insert_color(&req->rb_node, &ctx->cancel_tree);
 -}
 -
 -static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		       struct io_kiocb **nxt)
 +static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
  	struct io_poll_iocb *poll = &req->poll;
  	struct io_ring_ctx *ctx = req->ctx;
@@@ -1798,10 -2252,15 +1816,19 @@@
  	if (!poll->file)
  		return -EBADF;
  
++<<<<<<< HEAD
 +	req->submit.sqe = NULL;
 +	INIT_WORK(&req->work, io_poll_complete_work);
++=======
+ 	poll->wait = kmalloc(sizeof(*poll->wait), GFP_KERNEL);
+ 	if (!poll->wait)
+ 		return -ENOMEM;
+ 
+ 	req->sqe = NULL;
+ 	INIT_IO_WORK(&req->work, io_poll_complete_work);
++>>>>>>> e944475e6984 (io_uring: make poll->wait dynamically allocated)
  	events = READ_ONCE(sqe->poll_events);
  	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
 -	RB_CLEAR_NODE(&req->rb_node);
  
  	poll->head = NULL;
  	poll->done = false;
* Unmerged path fs/io_uring.c
