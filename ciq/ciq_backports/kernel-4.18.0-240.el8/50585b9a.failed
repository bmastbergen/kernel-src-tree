io_uring: Use submit info inlined into req

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 50585b9a07367b92382c1e975265344daeba78cd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/50585b9a.failed

Stack allocated struct sqe_submit is passed down to the submission path
along with a request (a.k.a. struct io_kiocb), and will be copied into
req->submit for async requests.

As space for it is already allocated, fill req->submit in the first
place instead of using on-stack one. As a result:

1. sqe->submit is the only place for sqe_submit and is always valid,
so we don't need to track which one to use.
2. don't need to copy in case of async
3. allows to simplify the code by not carrying it as an argument all
the way down
4. allows to reduce number of function arguments / potentially improve
spilling

The downside is that stack is most probably be cached, that's not true
for just allocated memory for a request. Another concern is cache
pollution. Though, a request would be touched and fetched along with
req->submit at some point anyway, so shouldn't be a problem.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 50585b9a07367b92382c1e975265344daeba78cd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index fca9cdc96d77,2b48a79848f2..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2173,16 -2455,13 +2173,25 @@@ static int __io_queue_sqe(struct io_rin
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
  		if (sqe_copy) {
 +			struct async_list *list;
 +
  			s->sqe = sqe_copy;
++<<<<<<< HEAD
 +			memcpy(&req->submit, s, sizeof(*s));
 +			list = io_async_list_from_sqe(ctx, s->sqe);
 +			if (!io_add_to_prev_work(list, req)) {
 +				if (list)
 +					atomic_inc(&list->cnt);
 +				INIT_WORK(&req->work, io_sq_wq_submit_work);
 +				io_queue_async_work(ctx, req);
++=======
+ 			if (req->work.flags & IO_WQ_WORK_NEEDS_FILES) {
+ 				ret = io_grab_files(ctx, req);
+ 				if (ret) {
+ 					kfree(sqe_copy);
+ 					goto err;
+ 				}
++>>>>>>> 50585b9a0736 (io_uring: Use submit info inlined into req)
  			}
  
  			/*
@@@ -2317,7 -2590,7 +2326,11 @@@ err
  		}
  
  		s->sqe = sqe_copy;
++<<<<<<< HEAD
 +		memcpy(&req->submit, s, sizeof(*s));
++=======
+ 		trace_io_uring_link(ctx, req, prev);
++>>>>>>> 50585b9a0736 (io_uring: Use submit info inlined into req)
  		list_add_tail(&req->list, &prev->link_list);
  	} else if (s->sqe->flags & IOSQE_IO_LINK) {
  		req->flags |= REQ_F_LINK;
@@@ -2423,19 -2699,31 +2435,48 @@@ static int io_submit_sqes(struct io_rin
  	}
  
  	for (i = 0; i < nr; i++) {
++<<<<<<< HEAD
 +		/*
 +		 * If previous wasn't linked and we have a linked command,
 +		 * that's the end of the chain. Submit the previous link.
 +		 */
 +		if (!prev_was_link && link) {
 +			io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +						true);
 +			link = NULL;
 +			shadow_req = NULL;
++=======
+ 		struct io_kiocb *req;
+ 		unsigned int sqe_flags;
+ 
+ 		req = io_get_req(ctx, statep);
+ 		if (unlikely(!req)) {
+ 			if (!submitted)
+ 				submitted = -EAGAIN;
+ 			break;
+ 		}
+ 		if (!io_get_sqring(ctx, &req->submit)) {
+ 			__io_free_req(req);
+ 			break;
++>>>>>>> 50585b9a0736 (io_uring: Use submit info inlined into req)
  		}
 +		prev_was_link = (sqes[i].sqe->flags & IOSQE_IO_LINK) != 0;
  
++<<<<<<< HEAD
 +		if (link && (sqes[i].sqe->flags & IOSQE_IO_DRAIN)) {
++=======
+ 		if (io_sqe_needs_user(req->submit.sqe) && !*mm) {
+ 			mm_fault = mm_fault || !mmget_not_zero(ctx->sqo_mm);
+ 			if (!mm_fault) {
+ 				use_mm(ctx->sqo_mm);
+ 				*mm = ctx->sqo_mm;
+ 			}
+ 		}
+ 
+ 		sqe_flags = req->submit.sqe->flags;
+ 
+ 		if (link && (sqe_flags & IOSQE_IO_DRAIN)) {
++>>>>>>> 50585b9a0736 (io_uring: Use submit info inlined into req)
  			if (!shadow_req) {
  				shadow_req = io_get_req(ctx, NULL);
  				if (unlikely(!shadow_req))
@@@ -2443,19 -2731,28 +2484,44 @@@
  				shadow_req->flags |= (REQ_F_IO_DRAIN | REQ_F_SHADOW_DRAIN);
  				refcount_dec(&shadow_req->refs);
  			}
++<<<<<<< HEAD
 +			shadow_req->sequence = sqes[i].sequence;
 +		}
 +
 +out:
 +		if (unlikely(mm_fault)) {
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
 +						-EFAULT);
 +		} else {
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
 +			submitted++;
++=======
+ 			shadow_req->sequence = req->submit.sequence;
+ 		}
+ 
+ out:
+ 		req->submit.ring_file = ring_file;
+ 		req->submit.ring_fd = ring_fd;
+ 		req->submit.has_user = *mm != NULL;
+ 		req->submit.in_async = async;
+ 		req->submit.needs_fixed_file = async;
+ 		trace_io_uring_submit_sqe(ctx, req->submit.sqe->user_data,
+ 					  true, async);
+ 		io_submit_sqe(ctx, req, &req->submit, statep, &link);
+ 		submitted++;
+ 
+ 		/*
+ 		 * If previous wasn't linked and we have a linked command,
+ 		 * that's the end of the chain. Submit the previous link.
+ 		 */
+ 		if (!(sqe_flags & IOSQE_IO_LINK) && link) {
+ 			io_queue_link_head(ctx, link, &link->submit, shadow_req);
+ 			link = NULL;
+ 			shadow_req = NULL;
++>>>>>>> 50585b9a0736 (io_uring: Use submit info inlined into req)
  		}
  	}
  
* Unmerged path fs/io_uring.c
