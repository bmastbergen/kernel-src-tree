io_uring: add general async offload context

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 1a6b74fc87024db59d41cd7346bd437f20fb3e2d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/1a6b74fc.failed

Right now we just copy the sqe for async offload, but we want to store
more context across an async punt. In preparation for doing so, put the
sqe copy inside a structure that we can expand. With this pointer added,
we can get rid of REQ_F_FREE_SQE, as that is now indicated by whether
req->io is NULL or not.

No functional changes in this patch.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 1a6b74fc87024db59d41cd7346bd437f20fb3e2d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 671f4f0982a1,bbbd9f664b1e..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -305,9 -292,26 +305,13 @@@ struct io_poll_iocb 
  	__poll_t			events;
  	bool				done;
  	bool				canceled;
 -	struct wait_queue_entry		*wait;
 -};
 -
 -struct io_timeout_data {
 -	struct io_kiocb			*req;
 -	struct hrtimer			timer;
 -	struct timespec64		ts;
 -	enum hrtimer_mode		mode;
 -	u32				seq_offset;
 -};
 -
 -struct io_timeout {
 -	struct file			*file;
 -	struct io_timeout_data		*data;
 +	struct wait_queue_entry		wait;
  };
  
+ struct io_async_ctx {
+ 	struct io_uring_sqe		sqe;
+ };
+ 
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -319,12 -323,22 +323,22 @@@ struct io_kiocb 
  		struct file		*file;
  		struct kiocb		rw;
  		struct io_poll_iocb	poll;
 -		struct io_timeout	timeout;
  	};
  
++<<<<<<< HEAD
 +	struct sqe_submit	submit;
++=======
+ 	const struct io_uring_sqe	*sqe;
+ 	struct io_async_ctx		*io;
+ 	struct file			*ring_file;
+ 	int				ring_fd;
+ 	bool				has_user;
+ 	bool				in_async;
+ 	bool				needs_fixed_file;
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  
  	struct io_ring_ctx	*ctx;
 -	union {
 -		struct list_head	list;
 -		struct rb_node		rb_node;
 -	};
 +	struct list_head	list;
  	struct list_head	link_list;
  	unsigned int		flags;
  	refcount_t		refs;
@@@ -334,9 -348,16 +348,19 @@@
  #define REQ_F_IO_DRAIN		16	/* drain existing IO first */
  #define REQ_F_IO_DRAINED	32	/* drain done */
  #define REQ_F_LINK		64	/* linked sqes */
 -#define REQ_F_LINK_TIMEOUT	128	/* has linked timeout */
 +#define REQ_F_LINK_DONE		128	/* linked sqes done */
  #define REQ_F_FAIL_LINK		256	/* fail rest of links */
++<<<<<<< HEAD
 +#define REQ_F_SHADOW_DRAIN	512	/* link-drain shadow req */
++=======
+ #define REQ_F_DRAIN_LINK	512	/* link should be fully drained */
+ #define REQ_F_TIMEOUT		1024	/* timeout request */
+ #define REQ_F_ISREG		2048	/* regular file */
+ #define REQ_F_MUST_PUNT		4096	/* must be punted even for NONBLOCK */
+ #define REQ_F_TIMEOUT_NOSEQ	8192	/* no timeout sequence */
+ #define REQ_F_INFLIGHT		16384	/* on inflight list */
+ #define REQ_F_COMP_LOCKED	32768	/* completion under lock */
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  	u64			user_data;
  	u32			result;
  	u32			sequence;
@@@ -603,6 -809,9 +627,12 @@@ static struct io_kiocb *io_get_req(stru
  		state->cur_req++;
  	}
  
++<<<<<<< HEAD
++=======
+ got_it:
+ 	req->io = NULL;
+ 	req->ring_file = NULL;
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  	req->file = NULL;
  	req->ctx = ctx;
  	req->flags = 0;
@@@ -626,15 -839,56 +656,22 @@@ static void io_free_req_many(struct io_
  
  static void __io_free_req(struct io_kiocb *req)
  {
++<<<<<<< HEAD
++=======
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	if (req->io)
+ 		kfree(req->io);
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  	if (req->file && !(req->flags & REQ_F_FIXED_FILE))
  		fput(req->file);
 -	if (req->flags & REQ_F_INFLIGHT) {
 -		unsigned long flags;
 -
 -		spin_lock_irqsave(&ctx->inflight_lock, flags);
 -		list_del(&req->inflight_entry);
 -		if (waitqueue_active(&ctx->inflight_wait))
 -			wake_up(&ctx->inflight_wait);
 -		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
 -	}
 -	if (req->flags & REQ_F_TIMEOUT)
 -		kfree(req->timeout.data);
 -	percpu_ref_put(&ctx->refs);
 -	if (likely(!io_is_fallback_req(req)))
 -		kmem_cache_free(req_cachep, req);
 -	else
 -		clear_bit_unlock(0, (unsigned long *) ctx->fallback_req);
 -}
 -
 -static bool io_link_cancel_timeout(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->timeout.data->timer);
 -	if (ret != -1) {
 -		io_cqring_fill_event(req, -ECANCELED);
 -		io_commit_cqring(ctx);
 -		req->flags &= ~REQ_F_LINK;
 -		io_put_req(req);
 -		return true;
 -	}
 -
 -	return false;
 +	percpu_ref_put(&req->ctx->refs);
 +	kmem_cache_free(req_cachep, req);
  }
  
 -static void io_req_link_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
 +static void io_req_link_next(struct io_kiocb *req)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
  	struct io_kiocb *nxt;
 -	bool wake_ev = false;
 -
 -	/* Already got next link */
 -	if (req->flags & REQ_F_LINK_NEXT)
 -		return;
  
  	/*
  	 * The list should never be empty when we are called here. But could
@@@ -727,8 -1084,9 +764,14 @@@ static void io_iopoll_complete(struct i
  			 * completions for those, only batch free for fixed
  			 * file and non-linked commands.
  			 */
++<<<<<<< HEAD
 +			if ((req->flags & (REQ_F_FIXED_FILE|REQ_F_LINK)) ==
 +			    REQ_F_FIXED_FILE) {
++=======
+ 			if (((req->flags & (REQ_F_FIXED_FILE|REQ_F_LINK)) ==
+ 			    REQ_F_FIXED_FILE) && !io_is_fallback_req(req) &&
+ 			    !req->io) {
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  				reqs[to_free++] = req;
  				if (to_free == ARRAY_SIZE(reqs))
  					io_free_req_many(ctx, reqs, &to_free);
@@@ -1801,10 -2260,15 +1844,19 @@@ static int io_poll_add(struct io_kiocb 
  	if (!poll->file)
  		return -EBADF;
  
++<<<<<<< HEAD
 +	req->submit.sqe = NULL;
 +	INIT_WORK(&req->work, io_poll_complete_work);
++=======
+ 	poll->wait = kmalloc(sizeof(*poll->wait), GFP_KERNEL);
+ 	if (!poll->wait)
+ 		return -ENOMEM;
+ 
+ 	req->io = NULL;
+ 	INIT_IO_WORK(&req->work, io_poll_complete_work);
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  	events = READ_ONCE(sqe->poll_events);
  	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
 -	RB_CLEAR_NODE(&req->rb_node);
  
  	poll->head = NULL;
  	poll->done = false;
@@@ -1853,100 -2318,216 +1905,111 @@@
  	return ipt.error;
  }
  
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
++<<<<<<< HEAD
 +	struct io_uring_sqe *sqe_copy;
++=======
+ 	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
++	struct io_async_ctx *io;
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  
 -	atomic_inc(&ctx->cq_timeouts);
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
 +		return 0;
  
- 	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
- 	if (!sqe_copy)
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -	/*
 -	 * We could be racing with timeout deletion. If the list is empty,
 -	 * then timeout lookup already found it and will be handling it.
 -	 */
 -	if (!list_empty(&req->list)) {
 -		struct io_kiocb *prev;
++	io = kmalloc(sizeof(*io), GFP_KERNEL);
++	if (!io)
 +		return -EAGAIN;
  
 -		/*
 -		 * Adjust the reqs sequence before the current one because it
 -		 * will consume a slot in the cq_ring and the the cq_tail
 -		 * pointer will be increased, otherwise other timeout reqs may
 -		 * return in advance without waiting for enough wait_nr.
 -		 */
 -		prev = req;
 -		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
 -			prev->sequence++;
 -		list_del_init(&req->list);
 +	spin_lock_irq(&ctx->completion_lock);
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list)) {
 +		spin_unlock_irq(&ctx->completion_lock);
- 		kfree(sqe_copy);
++		kfree(io);
 +		return 0;
  	}
  
 -	io_cqring_fill_event(req, -ETIME);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
++<<<<<<< HEAD
 +	memcpy(sqe_copy, sqe, sizeof(*sqe_copy));
 +	req->submit.sqe = sqe_copy;
++=======
++	memcpy(&io->sqe, req->sqe, sizeof(io->sqe));
++	req->sqe = &io->sqe;
++	req->io = io;
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  
 -	io_cqring_ev_posted(ctx);
 -	if (req->flags & REQ_F_LINK)
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_put_req(req);
 -	return HRTIMER_NORESTART;
 +	INIT_WORK(&req->work, io_sq_wq_submit_work);
 +	list_add_tail(&req->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +	return -EIOCBQUEUED;
  }
  
 -static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_kiocb *req;
 -	int ret = -ENOENT;
 +	int ret, opcode;
  
 -	list_for_each_entry(req, &ctx->timeout_list, list) {
 -		if (user_data == req->user_data) {
 -			list_del_init(&req->list);
 -			ret = 0;
 -			break;
 -		}
 +	req->user_data = READ_ONCE(s->sqe->user_data);
 +
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
 +	case IORING_OP_NOP:
 +		ret = io_nop(req, req->user_data);
 +		break;
 +	case IORING_OP_READV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_WRITEV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_READ_FIXED:
 +		ret = io_read(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_WRITE_FIXED:
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_FSYNC:
 +		ret = io_fsync(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_POLL_ADD:
 +		ret = io_poll_add(req, s->sqe);
 +		break;
 +	case IORING_OP_POLL_REMOVE:
 +		ret = io_poll_remove(req, s->sqe);
 +		break;
 +	case IORING_OP_SYNC_FILE_RANGE:
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_SENDMSG:
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_RECVMSG:
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
 +		break;
 +	default:
 +		ret = -EINVAL;
 +		break;
  	}
  
 -	if (ret == -ENOENT)
 +	if (ret)
  		return ret;
  
 -	ret = hrtimer_try_to_cancel(&req->timeout.data->timer);
 -	if (ret == -1)
 -		return -EALREADY;
 +	if (ctx->flags & IORING_SETUP_IOPOLL) {
 +		if (req->result == -EAGAIN)
 +			return -EAGAIN;
  
 -	if (req->flags & REQ_F_LINK)
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_cqring_fill_event(req, -ECANCELED);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -/*
 - * Remove or update an existing timeout command
 - */
 -static int io_timeout_remove(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned flags;
 -	int ret;
 -
 -	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
 -		return -EINVAL;
 -	flags = READ_ONCE(sqe->timeout_flags);
 -	if (flags)
 -		return -EINVAL;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	ret = io_timeout_cancel(ctx, READ_ONCE(sqe->addr));
 -
 -	io_cqring_fill_event(req, ret);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irq(&ctx->completion_lock);
 -	io_cqring_ev_posted(ctx);
 -	if (ret < 0 && req->flags & REQ_F_LINK)
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_timeout_setup(struct io_kiocb *req)
 -{
 -	const struct io_uring_sqe *sqe = req->sqe;
 -	struct io_timeout_data *data;
 -	unsigned flags;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
 -		return -EINVAL;
 -	flags = READ_ONCE(sqe->timeout_flags);
 -	if (flags & ~IORING_TIMEOUT_ABS)
 -		return -EINVAL;
 -
 -	data = kzalloc(sizeof(struct io_timeout_data), GFP_KERNEL);
 -	if (!data)
 -		return -ENOMEM;
 -	data->req = req;
 -	req->timeout.data = data;
 -	req->flags |= REQ_F_TIMEOUT;
 -
 -	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
 -		return -EFAULT;
 -
 -	if (flags & IORING_TIMEOUT_ABS)
 -		data->mode = HRTIMER_MODE_ABS;
 -	else
 -		data->mode = HRTIMER_MODE_REL;
 -
 -	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
 -	return 0;
 -}
 -
 -static int io_timeout(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	unsigned count;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_timeout_data *data;
 -	struct list_head *entry;
 -	unsigned span = 0;
 -	int ret;
 -
 -	ret = io_timeout_setup(req);
 -	/* common setup allows flags (like links) set, we don't */
 -	if (!ret && sqe->flags)
 -		ret = -EINVAL;
 -	if (ret)
 -		return ret;
 -
 -	/*
 -	 * sqe->off holds how many events that need to occur for this
 -	 * timeout event to be satisfied. If it isn't set, then this is
 -	 * a pure timeout request, sequence isn't used.
 -	 */
 -	count = READ_ONCE(sqe->off);
 -	if (!count) {
 -		req->flags |= REQ_F_TIMEOUT_NOSEQ;
 -		spin_lock_irq(&ctx->completion_lock);
 -		entry = ctx->timeout_list.prev;
 -		goto add;
 +		/* workqueue context doesn't hold uring_lock, grab it now */
 +		if (s->needs_lock)
 +			mutex_lock(&ctx->uring_lock);
 +		io_iopoll_req_issued(req);
 +		if (s->needs_lock)
 +			mutex_unlock(&ctx->uring_lock);
  	}
  
 -	req->sequence = ctx->cached_sq_head + count - 1;
 -	req->timeout.data->seq_offset = count;
 -
 -	/*
 -	 * Insertion sort, ensuring the first entry in the list is always
 -	 * the one we need first.
 -	 */
 -	spin_lock_irq(&ctx->completion_lock);
 -	list_for_each_prev(entry, &ctx->timeout_list) {
 -		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
 -		unsigned nxt_sq_head;
 -		long long tmp, tmp_nxt;
 -		u32 nxt_offset = nxt->timeout.data->seq_offset;
 -
 -		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
 -			continue;
 -
 -		/*
 -		 * Since cached_sq_head + count - 1 can overflow, use type long
 -		 * long to store it.
 -		 */
 -		tmp = (long long)ctx->cached_sq_head + count - 1;
 -		nxt_sq_head = nxt->sequence - nxt_offset + 1;
 -		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
 -
 -		/*
 -		 * cached_sq_head may overflow, and it will never overflow twice
 -		 * once there is some timeout req still be valid.
 -		 */
 -		if (ctx->cached_sq_head < nxt_sq_head)
 -			tmp += UINT_MAX;
 -
 -		if (tmp > tmp_nxt)
 -			break;
 -
 -		/*
 -		 * Sequence of reqs after the insert one and itself should
 -		 * be adjusted because each timeout req consumes a slot.
 -		 */
 -		span++;
 -		nxt->sequence++;
 -	}
 -	req->sequence -= span;
 -add:
 -	list_add(&req->list, entry);
 -	data = req->timeout.data;
 -	data->timer.function = io_timeout_fn;
 -	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
 -	spin_unlock_irq(&ctx->completion_lock);
  	return 0;
  }
  
@@@ -2178,37 -2843,149 +2241,60 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static int io_grab_files(struct io_kiocb *req)
 -{
 -	int ret = -EBADF;
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	rcu_read_lock();
 -	spin_lock_irq(&ctx->inflight_lock);
 -	/*
 -	 * We use the f_ops->flush() handler to ensure that we can flush
 -	 * out work accessing these files if the fd is closed. Check if
 -	 * the fd has changed since we started down this path, and disallow
 -	 * this operation if it has.
 -	 */
 -	if (fcheck(req->ring_fd) == req->ring_file) {
 -		list_add(&req->inflight_entry, &ctx->inflight_list);
 -		req->flags |= REQ_F_INFLIGHT;
 -		req->work.files = current->files;
 -		ret = 0;
 -	}
 -	spin_unlock_irq(&ctx->inflight_lock);
 -	rcu_read_unlock();
 -
 -	return ret;
 -}
 -
 -static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
 -{
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_kiocb *prev = NULL;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -
 -	/*
 -	 * We don't expect the list to be empty, that will only happen if we
 -	 * race with the completion of the linked work.
 -	 */
 -	if (!list_empty(&req->list)) {
 -		prev = list_entry(req->list.prev, struct io_kiocb, link_list);
 -		if (refcount_inc_not_zero(&prev->refs)) {
 -			list_del_init(&req->list);
 -			prev->flags &= ~REQ_F_LINK_TIMEOUT;
 -		} else
 -			prev = NULL;
 -	}
 -
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -
 -	if (prev) {
 -		if (prev->flags & REQ_F_LINK)
 -			prev->flags |= REQ_F_FAIL_LINK;
 -		io_async_find_and_cancel(ctx, req, prev->user_data, NULL,
 -						-ETIME);
 -		io_put_req(prev);
 -	} else {
 -		io_cqring_add_event(req, -ETIME);
 -		io_put_req(req);
 -	}
 -	return HRTIMER_NORESTART;
 -}
 -
 -static void io_queue_linked_timeout(struct io_kiocb *req)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	/*
 -	 * If the list is now empty, then our linked request finished before
 -	 * we got a chance to setup the timer
 -	 */
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (!list_empty(&req->list)) {
 -		struct io_timeout_data *data = req->timeout.data;
 -
 -		data->timer.function = io_link_timeout_fn;
 -		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
 -				data->mode);
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	/* drop submission reference */
 -	io_put_req(req);
 -}
 -
 -static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
 -{
 -	struct io_kiocb *nxt;
 -
 -	if (!(req->flags & REQ_F_LINK))
 -		return NULL;
 -
 -	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
 -	if (!nxt || nxt->sqe->opcode != IORING_OP_LINK_TIMEOUT)
 -		return NULL;
 -
 -	req->flags |= REQ_F_LINK_TIMEOUT;
 -	return nxt;
 -}
 -
 -static void __io_queue_sqe(struct io_kiocb *req)
 -{
 -	struct io_kiocb *linked_timeout = io_prep_linked_timeout(req);
 -	struct io_kiocb *nxt = NULL;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
 +		struct io_uring_sqe *sqe_copy;
 +
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (sqe_copy) {
 +			struct async_list *list;
 +
 +			s->sqe = sqe_copy;
 +			memcpy(&req->submit, s, sizeof(*s));
 +			list = io_async_list_from_sqe(ctx, s->sqe);
 +			if (!io_add_to_prev_work(list, req)) {
 +				if (list)
 +					atomic_inc(&list->cnt);
 +				INIT_WORK(&req->work, io_sq_wq_submit_work);
 +				io_queue_async_work(ctx, req);
 +			}
++=======
+ 	ret = io_issue_sqe(req, &nxt, true);
+ 	if (nxt)
+ 		io_queue_async_work(nxt);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		struct io_async_ctx *io;
+ 
+ 		io = kmalloc(sizeof(*io), GFP_KERNEL);
+ 		if (!io)
+ 			goto err;
+ 
+ 		memcpy(&io->sqe, req->sqe, sizeof(io->sqe));
+ 
+ 		req->sqe = &io->sqe;
+ 		req->io = io;
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  
 -		if (req->work.flags & IO_WQ_WORK_NEEDS_FILES) {
 -			ret = io_grab_files(req);
 -			if (ret)
 -				goto err;
 +			/*
 +			 * Queued up for async execution, worker will release
 +			 * submit reference when the iocb is actually submitted.
 +			 */
 +			return 0;
  		}
 -
 -		/*
 -		 * Queued up for async execution, worker will release
 -		 * submit reference when the iocb is actually submitted.
 -		 */
 -		io_queue_async_work(req);
 -		return;
  	}
  
 -err:
  	/* drop submission reference */
  	io_put_req(req);
  
@@@ -2325,20 -3070,36 +2411,48 @@@ err
  	 */
  	if (*link) {
  		struct io_kiocb *prev = *link;
++<<<<<<< HEAD
 +
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (!sqe_copy) {
++=======
+ 		struct io_async_ctx *io;
+ 
+ 		if (req->sqe->flags & IOSQE_IO_DRAIN)
+ 			(*link)->flags |= REQ_F_DRAIN_LINK | REQ_F_IO_DRAIN;
+ 
+ 		if (READ_ONCE(req->sqe->opcode) == IORING_OP_LINK_TIMEOUT) {
+ 			ret = io_timeout_setup(req);
+ 			/* common setup allows offset being set, we don't */
+ 			if (!ret && req->sqe->off)
+ 				ret = -EINVAL;
+ 			if (ret) {
+ 				prev->flags |= REQ_F_FAIL_LINK;
+ 				goto err_req;
+ 			}
+ 		}
+ 
+ 		io = kmalloc(sizeof(*io), GFP_KERNEL);
+ 		if (!io) {
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  			ret = -EAGAIN;
  			goto err_req;
  		}
  
++<<<<<<< HEAD
 +		s->sqe = sqe_copy;
 +		memcpy(&req->submit, s, sizeof(*s));
++=======
+ 		memcpy(&io->sqe, req->sqe, sizeof(io->sqe));
+ 		req->sqe = &io->sqe;
+ 		req->io = io;
+ 		trace_io_uring_link(ctx, req, prev);
++>>>>>>> 1a6b74fc8702 (io_uring: add general async offload context)
  		list_add_tail(&req->list, &prev->link_list);
 -	} else if (req->sqe->flags & IOSQE_IO_LINK) {
 +	} else if (s->sqe->flags & IOSQE_IO_LINK) {
  		req->flags |= REQ_F_LINK;
  
 +		memcpy(&req->submit, s, sizeof(*s));
  		INIT_LIST_HEAD(&req->link_list);
  		*link = req;
  	} else {
* Unmerged path fs/io_uring.c
