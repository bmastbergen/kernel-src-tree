io_uring: handle connect -EINPROGRESS like -EAGAIN

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 87f80d623c6c93c721b2aaead8a45e848bc8ffbf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/87f80d62.failed

Right now we return it to userspace, which means the application has
to poll for the socket to be writeable. Let's just treat it like
-EAGAIN and have io_uring handle it internally, this makes it much
easier to use.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 87f80d623c6c93c721b2aaead8a45e848bc8ffbf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 8c2ecae32a7b,6c22a277904e..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1636,6 -2082,199 +1636,84 @@@ static int io_recvmsg(struct io_kiocb *
  #endif
  }
  
 -static int io_recvmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
++<<<<<<< HEAD
++=======
++static int io_connect_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
 -	struct user_msghdr __user *msg;
 -	unsigned flags;
++	struct sockaddr __user *addr;
++	int addr_len;
+ 
 -	flags = READ_ONCE(sqe->msg_flags);
 -	msg = (struct user_msghdr __user *)(unsigned long) READ_ONCE(sqe->addr);
 -	return recvmsg_copy_msghdr(&io->msg.msg, msg, flags, &io->msg.uaddr,
 -					&io->msg.iov);
++	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
++	addr_len = READ_ONCE(sqe->addr2);
++	return move_addr_to_kernel(addr, addr_len, &io->connect.address);
+ #else
+ 	return 0;
+ #endif
+ }
+ 
 -static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++static int io_connect(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 		      struct io_kiocb **nxt, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
 -	struct socket *sock;
 -	int ret;
++	struct io_async_ctx __io, *io;
++	unsigned file_flags;
++	int addr_len, ret;
+ 
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct user_msghdr __user *msg;
 -		struct io_async_ctx io, *copy;
 -		struct sockaddr_storage addr;
 -		struct msghdr *kmsg;
 -		unsigned flags;
 -
 -		flags = READ_ONCE(sqe->msg_flags);
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 -
 -		msg = (struct user_msghdr __user *) (unsigned long)
 -			READ_ONCE(sqe->addr);
 -		if (req->io) {
 -			kmsg = &req->io->msg.msg;
 -			kmsg->msg_name = &addr;
 -		} else {
 -			kmsg = &io.msg.msg;
 -			kmsg->msg_name = &addr;
 -			io.msg.iov = io.msg.fast_iov;
 -			ret = io_recvmsg_prep(req, &io);
 -			if (ret)
 -				goto out;
 -		}
 -
 -		ret = __sys_recvmsg_sock(sock, kmsg, msg, io.msg.uaddr, flags);
 -		if (force_nonblock && ret == -EAGAIN) {
 -			copy = kmalloc(sizeof(*copy), GFP_KERNEL);
 -			if (!copy) {
 -				ret = -ENOMEM;
 -				goto out;
 -			}
 -			memcpy(copy, &io, sizeof(*copy));
 -			req->io = copy;
 -			memcpy(&req->io->sqe, req->sqe, sizeof(*req->sqe));
 -			req->sqe = &req->io->sqe;
 -			return ret;
 -		}
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 -	}
 -
 -out:
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0 && (req->flags & REQ_F_LINK))
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_put_req_find_next(req, nxt);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_accept(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		     struct io_kiocb **nxt, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct sockaddr __user *addr;
 -	int __user *addr_len;
 -	unsigned file_flags;
 -	int flags, ret;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -
 -	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
 -	addr_len = (int __user *) (unsigned long) READ_ONCE(sqe->addr2);
 -	flags = READ_ONCE(sqe->accept_flags);
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	ret = __sys_accept4_file(req->file, file_flags, addr, addr_len, flags);
 -	if (ret == -EAGAIN && force_nonblock) {
 -		req->work.flags |= IO_WQ_WORK_NEEDS_FILES;
 -		return -EAGAIN;
 -	}
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -	if (ret < 0 && (req->flags & REQ_F_LINK))
 -		req->flags |= REQ_F_FAIL_LINK;
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_connect_prep(struct io_kiocb *req, struct io_async_ctx *io)
 -{
 -#if defined(CONFIG_NET)
 -	const struct io_uring_sqe *sqe = req->sqe;
 -	struct sockaddr __user *addr;
 -	int addr_len;
 -
 -	addr = (struct sockaddr __user *) (unsigned long) READ_ONCE(sqe->addr);
 -	addr_len = READ_ONCE(sqe->addr2);
 -	return move_addr_to_kernel(addr, addr_len, &io->connect.address);
 -#else
 -	return 0;
 -#endif
 -}
 -
 -static int io_connect(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -		      struct io_kiocb **nxt, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_ctx __io, *io;
 -	unsigned file_flags;
 -	int addr_len, ret;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
++	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
++		return -EINVAL;
++	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	addr_len = READ_ONCE(sqe->addr2);
+ 	file_flags = force_nonblock ? O_NONBLOCK : 0;
+ 
+ 	if (req->io) {
+ 		io = req->io;
+ 	} else {
+ 		ret = io_connect_prep(req, &__io);
+ 		if (ret)
+ 			goto out;
+ 		io = &__io;
+ 	}
+ 
+ 	ret = __sys_connect_file(req->file, &io->connect.address, addr_len,
+ 					file_flags);
+ 	if ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {
+ 		io = kmalloc(sizeof(*io), GFP_KERNEL);
+ 		if (!io) {
+ 			ret = -ENOMEM;
+ 			goto out;
+ 		}
+ 		memcpy(&io->connect, &__io.connect, sizeof(io->connect));
+ 		req->io = io;
+ 		memcpy(&io->sqe, req->sqe, sizeof(*req->sqe));
+ 		req->sqe = &io->sqe;
+ 		return -EAGAIN;
+ 	}
+ 	if (ret == -ERESTARTSYS)
+ 		ret = -EINTR;
+ out:
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static inline void io_poll_remove_req(struct io_kiocb *req)
+ {
+ 	if (!RB_EMPTY_NODE(&req->rb_node)) {
+ 		rb_erase(&req->rb_node, &req->ctx->cancel_tree);
+ 		RB_CLEAR_NODE(&req->rb_node);
+ 	}
+ }
+ 
++>>>>>>> 87f80d623c6c (io_uring: handle connect -EINPROGRESS like -EAGAIN)
  static void io_poll_remove_one(struct io_kiocb *req)
  {
  	struct io_poll_iocb *poll = &req->poll;
* Unmerged path fs/io_uring.c
