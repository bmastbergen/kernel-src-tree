io_uring: fix overflowed reqs cancellation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 7b53d59859bc932b37895d2d37388e7fa29af7a5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/7b53d598.failed

Overflowed requests in io_uring_cancel_files() should be shed only of
inflight and overflowed refs. All other left references are owned by
someone else.

If refcount_sub_and_test() fails, it will go further and put put extra
ref, don't do that. Also, don't need to do io_wq_cancel_work()
for overflowed reqs, they will be let go shortly anyway.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 7b53d59859bc932b37895d2d37388e7fa29af7a5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 047c6a5f549f,732ec73ec3c0..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -3586,12 -7401,84 +3586,74 @@@ static int io_uring_release(struct inod
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
++=======
+ static void io_uring_cancel_files(struct io_ring_ctx *ctx,
+ 				  struct files_struct *files)
+ {
+ 	while (!list_empty_careful(&ctx->inflight_list)) {
+ 		struct io_kiocb *cancel_req = NULL, *req;
+ 		DEFINE_WAIT(wait);
+ 
+ 		spin_lock_irq(&ctx->inflight_lock);
+ 		list_for_each_entry(req, &ctx->inflight_list, inflight_entry) {
+ 			if (req->work.files != files)
+ 				continue;
+ 			/* req is being completed, ignore */
+ 			if (!refcount_inc_not_zero(&req->refs))
+ 				continue;
+ 			cancel_req = req;
+ 			break;
+ 		}
+ 		if (cancel_req)
+ 			prepare_to_wait(&ctx->inflight_wait, &wait,
+ 						TASK_UNINTERRUPTIBLE);
+ 		spin_unlock_irq(&ctx->inflight_lock);
+ 
+ 		/* We need to keep going until we don't find a matching req */
+ 		if (!cancel_req)
+ 			break;
+ 
+ 		if (cancel_req->flags & REQ_F_OVERFLOW) {
+ 			spin_lock_irq(&ctx->completion_lock);
+ 			list_del(&cancel_req->list);
+ 			cancel_req->flags &= ~REQ_F_OVERFLOW;
+ 			if (list_empty(&ctx->cq_overflow_list)) {
+ 				clear_bit(0, &ctx->sq_check_overflow);
+ 				clear_bit(0, &ctx->cq_check_overflow);
+ 			}
+ 			spin_unlock_irq(&ctx->completion_lock);
+ 
+ 			WRITE_ONCE(ctx->rings->cq_overflow,
+ 				atomic_inc_return(&ctx->cached_cq_overflow));
+ 
+ 			/*
+ 			 * Put inflight ref and overflow ref. If that's
+ 			 * all we had, then we're done with this request.
+ 			 */
+ 			if (refcount_sub_and_test(2, &cancel_req->refs)) {
+ 				io_free_req(cancel_req);
+ 				finish_wait(&ctx->inflight_wait, &wait);
+ 				continue;
+ 			}
+ 		} else {
+ 			io_wq_cancel_work(ctx->io_wq, &cancel_req->work);
+ 			io_put_req(cancel_req);
+ 		}
+ 
+ 		schedule();
+ 		finish_wait(&ctx->inflight_wait, &wait);
+ 	}
+ }
+ 
+ static int io_uring_flush(struct file *file, void *data)
++>>>>>>> 7b53d59859bc (io_uring: fix overflowed reqs cancellation)
  {
 +	loff_t offset = (loff_t) vma->vm_pgoff << PAGE_SHIFT;
 +	unsigned long sz = vma->vm_end - vma->vm_start;
  	struct io_ring_ctx *ctx = file->private_data;
 -
 -	io_uring_cancel_files(ctx, data);
 -
 -	/*
 -	 * If the task is going away, cancel work it may have pending
 -	 */
 -	if (fatal_signal_pending(current) || (current->flags & PF_EXITING))
 -		io_wq_cancel_pid(ctx->io_wq, task_pid_vnr(current));
 -
 -	return 0;
 -}
 -
 -static void *io_uring_validate_mmap_request(struct file *file,
 -					    loff_t pgoff, size_t sz)
 -{
 -	struct io_ring_ctx *ctx = file->private_data;
 -	loff_t offset = pgoff << PAGE_SHIFT;
 +	unsigned long pfn;
  	struct page *page;
  	void *ptr;
  
* Unmerged path fs/io_uring.c
