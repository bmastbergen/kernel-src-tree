sched/fair: Optimize enqueue_task_fair()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Vincent Guittot <vincent.guittot@linaro.org>
commit 7d148be69e3a0eaa9d029a3c51b545e322116a99
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/7d148be6.failed

enqueue_task_fair jumps to enqueue_throttle label when cfs_rq_of(se) is
throttled which means that se can't be NULL in such case and we can move
the label after the if (!se) statement. Futhermore, the latter can be
removed because se is always NULL when reaching this point.

	Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Phil Auld <pauld@redhat.com>
Link: https://lkml.kernel.org/r/20200513135502.4672-1-vincent.guittot@linaro.org
(cherry picked from commit 7d148be69e3a0eaa9d029a3c51b545e322116a99)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 487d0e3ead80,4e586863827b..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5230,36 -5492,47 +5230,60 @@@ enqueue_task_fair(struct rq *rq, struc
  
  	for_each_sched_entity(se) {
  		cfs_rq = cfs_rq_of(se);
 +		cfs_rq->h_nr_running++;
 +
 +		if (cfs_rq_throttled(cfs_rq))
 +			break;
  
  		update_load_avg(cfs_rq, se, UPDATE_TG);
 -		se_update_runnable(se);
  		update_cfs_group(se);
 +	}
  
 -		cfs_rq->h_nr_running++;
 -		cfs_rq->idle_h_nr_running += idle_h_nr_running;
 -
 -		/* end evaluation on encountering a throttled cfs_rq */
 -		if (cfs_rq_throttled(cfs_rq))
 -			goto enqueue_throttle;
++<<<<<<< HEAD
 +	if (!se) {
 +		add_nr_running(rq, 1);
 +		/*
 +		 * Since new tasks are assigned an initial util_avg equal to
 +		 * half of the spare capacity of their CPU, tiny tasks have the
 +		 * ability to cross the overutilized threshold, which will
 +		 * result in the load balancer ruining all the task placement
 +		 * done by EAS. As a way to mitigate that effect, do not account
 +		 * for the first enqueue operation of new tasks during the
 +		 * overutilized flag detection.
 +		 *
 +		 * A better way of solving this problem would be to wait for
 +		 * the PELT signals of tasks to converge before taking them
 +		 * into account, but that is not straightforward to implement,
 +		 * and the following generally works well enough in practice.
 +		 */
 +		if (flags & ENQUEUE_WAKEUP)
 +			update_overutilized_status(rq);
  
 -               /*
 -                * One parent has been throttled and cfs_rq removed from the
 -                * list. Add it back to not break the leaf list.
 -                */
 -               if (throttled_hierarchy(cfs_rq))
 -                       list_add_leaf_cfs_rq(cfs_rq);
  	}
  
++=======
+ 	/* At this point se is NULL and we are at root level*/
+ 	add_nr_running(rq, 1);
+ 
+ 	/*
+ 	 * Since new tasks are assigned an initial util_avg equal to
+ 	 * half of the spare capacity of their CPU, tiny tasks have the
+ 	 * ability to cross the overutilized threshold, which will
+ 	 * result in the load balancer ruining all the task placement
+ 	 * done by EAS. As a way to mitigate that effect, do not account
+ 	 * for the first enqueue operation of new tasks during the
+ 	 * overutilized flag detection.
+ 	 *
+ 	 * A better way of solving this problem would be to wait for
+ 	 * the PELT signals of tasks to converge before taking them
+ 	 * into account, but that is not straightforward to implement,
+ 	 * and the following generally works well enough in practice.
+ 	 */
+ 	if (flags & ENQUEUE_WAKEUP)
+ 		update_overutilized_status(rq);
+ 
+ enqueue_throttle:
++>>>>>>> 7d148be69e3a (sched/fair: Optimize enqueue_task_fair())
  	if (cfs_bandwidth_used()) {
  		/*
  		 * When bandwidth control is enabled; the cfs_rq_throttled()
* Unmerged path kernel/sched/fair.c
