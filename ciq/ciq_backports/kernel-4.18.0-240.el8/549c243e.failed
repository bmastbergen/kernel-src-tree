net/mlx5e: Extract neigh-specific code from en_rep.c to rep/neigh.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Vlad Buslov <vladbu@mellanox.com>
commit 549c243e4e010067a075e248f4d72e8dda844e12
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/549c243e.failed

As a preparation for introducing new kconfig option that controls
compilation of all TC offloads code in mlx5, extract neigh-specific code
from en_rep.c to standalone file. This allows easily compiling out the code
by only including new source in make file when corresponding kconfig is
enabled instead of adding multiple ifdef blocks to en_rep.

	Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 549c243e4e010067a075e248f4d72e8dda844e12)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/Makefile
#	drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c
#	drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/Makefile
index 8d3b7a3aee92,3c9d78e6695c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/Makefile
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Makefile
@@@ -32,9 -33,12 +32,18 @@@ mlx5_core-$(CONFIG_MLX5_CORE_EN) += en_
  mlx5_core-$(CONFIG_MLX5_EN_ARFS)     += en_arfs.o
  mlx5_core-$(CONFIG_MLX5_EN_RXNFC)    += en_fs_ethtool.o
  mlx5_core-$(CONFIG_MLX5_CORE_EN_DCB) += en_dcbnl.o en/port_buffer.o
++<<<<<<< HEAD
 +mlx5_core-$(CONFIG_MLX5_ESWITCH)     += en_rep.o en_tc.o en/tc_tun.o lib/port_tun.o lag_mp.o \
 +					lib/geneve.o en/tc_tun_vxlan.o en/tc_tun_gre.o \
 +					en/tc_tun_geneve.o
++=======
+ mlx5_core-$(CONFIG_MLX5_ESWITCH)     += en_rep.o en_tc.o en/rep/tc.o en/rep/neigh.o en/tc_tun.o lib/port_tun.o \
+ 					lag_mp.o \
+ 					lib/geneve.o en/mapping.o en/tc_tun_vxlan.o en/tc_tun_gre.o \
+ 					en/tc_tun_geneve.o diag/en_tc_tracepoint.o
+ mlx5_core-$(CONFIG_PCI_HYPERV_INTERFACE) += en/hv_vhca_stats.o
+ mlx5_core-$(CONFIG_MLX5_TC_CT)	     += en/tc_ct.o
++>>>>>>> 549c243e4e01 (net/mlx5e: Extract neigh-specific code from en_rep.c to rep/neigh.c)
  
  #
  # Core extra
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
index 43187d8c2261,9be1fcc269b2..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
@@@ -6,6 -6,8 +6,11 @@@
  #include <net/geneve.h>
  #include "en/tc_tun.h"
  #include "en_tc.h"
++<<<<<<< HEAD
++=======
+ #include "rep/tc.h"
+ #include "rep/neigh.h"
++>>>>>>> 549c243e4e01 (net/mlx5e: Extract neigh-specific code from en_rep.c to rep/neigh.c)
  
  struct mlx5e_tc_tunnel *mlx5e_get_tc_tun(struct net_device *tunnel_dev)
  {
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index c30e19ff93cf,a46405c6d560..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@@ -44,12 -43,16 +43,17 @@@
  #include "en.h"
  #include "en_rep.h"
  #include "en_tc.h"
++<<<<<<< HEAD
 +#include "en/tc_tun.h"
++=======
+ #include "en/rep/tc.h"
+ #include "en/rep/neigh.h"
++>>>>>>> 549c243e4e01 (net/mlx5e: Extract neigh-specific code from en_rep.c to rep/neigh.c)
  #include "fs_core.h"
 -#include "lib/mlx5.h"
 -#define CREATE_TRACE_POINTS
 -#include "diag/en_rep_tracepoint.h"
 +#include "lib/port_tun.h"
  
  #define MLX5E_REP_PARAMS_DEF_LOG_SQ_SIZE \
 -        max(0x7, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE)
 +	max(0x7, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE)
  #define MLX5E_REP_PARAMS_DEF_NUM_CHANNELS 1
  
  static const char mlx5e_rep_driver_name[] = "mlx5e_rep";
@@@ -475,704 -474,6 +479,707 @@@ void mlx5e_remove_sqs_fwd_rules(struct 
  	mlx5e_sqs2vport_stop(esw, rep);
  }
  
++<<<<<<< HEAD
 +static unsigned long mlx5e_rep_ipv6_interval(void)
 +{
 +	if (IS_ENABLED(CONFIG_IPV6) && ipv6_stub->nd_tbl)
 +		return NEIGH_VAR(&ipv6_stub->nd_tbl->parms, DELAY_PROBE_TIME);
 +
 +	return ~0UL;
 +}
 +
 +static void mlx5e_rep_neigh_update_init_interval(struct mlx5e_rep_priv *rpriv)
 +{
 +	unsigned long ipv4_interval = NEIGH_VAR(&arp_tbl.parms, DELAY_PROBE_TIME);
 +	unsigned long ipv6_interval = mlx5e_rep_ipv6_interval();
 +	struct net_device *netdev = rpriv->netdev;
 +	struct mlx5e_priv *priv = netdev_priv(netdev);
 +
 +	rpriv->neigh_update.min_interval = min_t(unsigned long, ipv6_interval, ipv4_interval);
 +	mlx5_fc_update_sampling_interval(priv->mdev, rpriv->neigh_update.min_interval);
 +}
 +
 +void mlx5e_rep_queue_neigh_stats_work(struct mlx5e_priv *priv)
 +{
 +	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 +	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
 +
 +	mlx5_fc_queue_stats_work(priv->mdev,
 +				 &neigh_update->neigh_stats_work,
 +				 neigh_update->min_interval);
 +}
 +
 +static bool mlx5e_rep_neigh_entry_hold(struct mlx5e_neigh_hash_entry *nhe)
 +{
 +	return refcount_inc_not_zero(&nhe->refcnt);
 +}
 +
 +static void mlx5e_rep_neigh_entry_remove(struct mlx5e_neigh_hash_entry *nhe);
 +
 +static void mlx5e_rep_neigh_entry_release(struct mlx5e_neigh_hash_entry *nhe)
 +{
 +	if (refcount_dec_and_test(&nhe->refcnt)) {
 +		mlx5e_rep_neigh_entry_remove(nhe);
 +		kfree_rcu(nhe, rcu);
 +	}
 +}
 +
 +static struct mlx5e_neigh_hash_entry *
 +mlx5e_get_next_nhe(struct mlx5e_rep_priv *rpriv,
 +		   struct mlx5e_neigh_hash_entry *nhe)
 +{
 +	struct mlx5e_neigh_hash_entry *next = NULL;
 +
 +	rcu_read_lock();
 +
 +	for (next = nhe ?
 +		     list_next_or_null_rcu(&rpriv->neigh_update.neigh_list,
 +					   &nhe->neigh_list,
 +					   struct mlx5e_neigh_hash_entry,
 +					   neigh_list) :
 +		     list_first_or_null_rcu(&rpriv->neigh_update.neigh_list,
 +					    struct mlx5e_neigh_hash_entry,
 +					    neigh_list);
 +	     next;
 +	     next = list_next_or_null_rcu(&rpriv->neigh_update.neigh_list,
 +					  &next->neigh_list,
 +					  struct mlx5e_neigh_hash_entry,
 +					  neigh_list))
 +		if (mlx5e_rep_neigh_entry_hold(next))
 +			break;
 +
 +	rcu_read_unlock();
 +
 +	if (nhe)
 +		mlx5e_rep_neigh_entry_release(nhe);
 +
 +	return next;
 +}
 +
 +static void mlx5e_rep_neigh_stats_work(struct work_struct *work)
 +{
 +	struct mlx5e_rep_priv *rpriv = container_of(work, struct mlx5e_rep_priv,
 +						    neigh_update.neigh_stats_work.work);
 +	struct net_device *netdev = rpriv->netdev;
 +	struct mlx5e_priv *priv = netdev_priv(netdev);
 +	struct mlx5e_neigh_hash_entry *nhe = NULL;
 +
 +	rtnl_lock();
 +	if (!list_empty(&rpriv->neigh_update.neigh_list))
 +		mlx5e_rep_queue_neigh_stats_work(priv);
 +
 +	while ((nhe = mlx5e_get_next_nhe(rpriv, nhe)) != NULL)
 +		mlx5e_tc_update_neigh_used_value(nhe);
 +
 +	rtnl_unlock();
 +}
 +
 +static void mlx5e_rep_update_flows(struct mlx5e_priv *priv,
 +				   struct mlx5e_encap_entry *e,
 +				   bool neigh_connected,
 +				   unsigned char ha[ETH_ALEN])
 +{
 +	struct ethhdr *eth = (struct ethhdr *)e->encap_header;
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	bool encap_connected;
 +	LIST_HEAD(flow_list);
 +
 +	ASSERT_RTNL();
 +
 +	/* wait for encap to be fully initialized */
 +	wait_for_completion(&e->res_ready);
 +
 +	mutex_lock(&esw->offloads.encap_tbl_lock);
 +	encap_connected = !!(e->flags & MLX5_ENCAP_ENTRY_VALID);
 +	if (e->compl_result < 0 || (encap_connected == neigh_connected &&
 +				    ether_addr_equal(e->h_dest, ha)))
 +		goto unlock;
 +
 +	mlx5e_take_all_encap_flows(e, &flow_list);
 +
 +	if ((e->flags & MLX5_ENCAP_ENTRY_VALID) &&
 +	    (!neigh_connected || !ether_addr_equal(e->h_dest, ha)))
 +		mlx5e_tc_encap_flows_del(priv, e, &flow_list);
 +
 +	if (neigh_connected && !(e->flags & MLX5_ENCAP_ENTRY_VALID)) {
 +		ether_addr_copy(e->h_dest, ha);
 +		ether_addr_copy(eth->h_dest, ha);
 +		/* Update the encap source mac, in case that we delete
 +		 * the flows when encap source mac changed.
 +		 */
 +		ether_addr_copy(eth->h_source, e->route_dev->dev_addr);
 +
 +		mlx5e_tc_encap_flows_add(priv, e, &flow_list);
 +	}
 +unlock:
 +	mutex_unlock(&esw->offloads.encap_tbl_lock);
 +	mlx5e_put_encap_flow_list(priv, &flow_list);
 +}
 +
 +static void mlx5e_rep_neigh_update(struct work_struct *work)
 +{
 +	struct mlx5e_neigh_hash_entry *nhe =
 +		container_of(work, struct mlx5e_neigh_hash_entry, neigh_update_work);
 +	struct neighbour *n = nhe->n;
 +	struct mlx5e_encap_entry *e;
 +	unsigned char ha[ETH_ALEN];
 +	struct mlx5e_priv *priv;
 +	bool neigh_connected;
 +	u8 nud_state, dead;
 +
 +	rtnl_lock();
 +
 +	/* If these parameters are changed after we release the lock,
 +	 * we'll receive another event letting us know about it.
 +	 * We use this lock to avoid inconsistency between the neigh validity
 +	 * and it's hw address.
 +	 */
 +	read_lock_bh(&n->lock);
 +	memcpy(ha, n->ha, ETH_ALEN);
 +	nud_state = n->nud_state;
 +	dead = n->dead;
 +	read_unlock_bh(&n->lock);
 +
 +	neigh_connected = (nud_state & NUD_VALID) && !dead;
 +
 +	list_for_each_entry(e, &nhe->encap_list, encap_list) {
 +		if (!mlx5e_encap_take(e))
 +			continue;
 +
 +		priv = netdev_priv(e->out_dev);
 +		mlx5e_rep_update_flows(priv, e, neigh_connected, ha);
 +		mlx5e_encap_put(priv, e);
 +	}
 +	mlx5e_rep_neigh_entry_release(nhe);
 +	rtnl_unlock();
 +	neigh_release(n);
 +}
 +
 +static struct mlx5e_rep_indr_block_priv *
 +mlx5e_rep_indr_block_priv_lookup(struct mlx5e_rep_priv *rpriv,
 +				 struct net_device *netdev)
 +{
 +	struct mlx5e_rep_indr_block_priv *cb_priv;
 +
 +	/* All callback list access should be protected by RTNL. */
 +	ASSERT_RTNL();
 +
 +	list_for_each_entry(cb_priv,
 +			    &rpriv->uplink_priv.tc_indr_block_priv_list,
 +			    list)
 +		if (cb_priv->netdev == netdev)
 +			return cb_priv;
 +
 +	return NULL;
 +}
 +
 +static void mlx5e_rep_indr_clean_block_privs(struct mlx5e_rep_priv *rpriv)
 +{
 +	struct mlx5e_rep_indr_block_priv *cb_priv, *temp;
 +	struct list_head *head = &rpriv->uplink_priv.tc_indr_block_priv_list;
 +
 +	list_for_each_entry_safe(cb_priv, temp, head, list) {
 +		mlx5e_rep_indr_unregister_block(rpriv, cb_priv->netdev);
 +		kfree(cb_priv);
 +	}
 +}
 +
 +static int
 +mlx5e_rep_indr_offload(struct net_device *netdev,
 +		       struct flow_cls_offload *flower,
 +		       struct mlx5e_rep_indr_block_priv *indr_priv,
 +		       unsigned long flags)
 +{
 +	struct mlx5e_priv *priv = netdev_priv(indr_priv->rpriv->netdev);
 +	int err = 0;
 +
 +	switch (flower->command) {
 +	case FLOW_CLS_REPLACE:
 +		err = mlx5e_configure_flower(netdev, priv, flower, flags);
 +		break;
 +	case FLOW_CLS_DESTROY:
 +		err = mlx5e_delete_flower(netdev, priv, flower, flags);
 +		break;
 +	case FLOW_CLS_STATS:
 +		err = mlx5e_stats_flower(netdev, priv, flower, flags);
 +		break;
 +	default:
 +		err = -EOPNOTSUPP;
 +	}
 +
 +	return err;
 +}
 +
 +static int mlx5e_rep_indr_setup_tc_cb(enum tc_setup_type type,
 +				      void *type_data, void *indr_priv)
 +{
 +	unsigned long flags = MLX5_TC_FLAG(EGRESS) | MLX5_TC_FLAG(ESW_OFFLOAD);
 +	struct mlx5e_rep_indr_block_priv *priv = indr_priv;
 +
 +	switch (type) {
 +	case TC_SETUP_CLSFLOWER:
 +		return mlx5e_rep_indr_offload(priv->netdev, type_data, priv,
 +					      flags);
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
 +static int mlx5e_rep_indr_setup_ft_cb(enum tc_setup_type type,
 +				      void *type_data, void *indr_priv)
 +{
 +	struct mlx5e_rep_indr_block_priv *priv = indr_priv;
 +	struct flow_cls_offload *f = type_data;
 +	struct flow_cls_offload tmp;
 +	struct mlx5e_priv *mpriv;
 +	struct mlx5_eswitch *esw;
 +	unsigned long flags;
 +	int err;
 +
 +	mpriv = netdev_priv(priv->rpriv->netdev);
 +	esw = mpriv->mdev->priv.eswitch;
 +
 +	flags = MLX5_TC_FLAG(EGRESS) |
 +		MLX5_TC_FLAG(ESW_OFFLOAD) |
 +		MLX5_TC_FLAG(FT_OFFLOAD);
 +
 +	switch (type) {
 +	case TC_SETUP_CLSFLOWER:
 +		memcpy(&tmp, f, sizeof(*f));
 +
 +		/* Re-use tc offload path by moving the ft flow to the
 +		 * reserved ft chain.
 +		 *
 +		 * FT offload can use prio range [0, INT_MAX], so we normalize
 +		 * it to range [1, mlx5_esw_chains_get_prio_range(esw)]
 +		 * as with tc, where prio 0 isn't supported.
 +		 *
 +		 * We only support chain 0 of FT offload.
 +		 */
 +		if (!mlx5_esw_chains_prios_supported(esw) ||
 +		    tmp.common.prio >= mlx5_esw_chains_get_prio_range(esw) ||
 +		    tmp.common.chain_index)
 +			return -EOPNOTSUPP;
 +
 +		tmp.common.chain_index = mlx5_esw_chains_get_ft_chain(esw);
 +		tmp.common.prio++;
 +		err = mlx5e_rep_indr_offload(priv->netdev, &tmp, priv, flags);
 +		memcpy(&f->stats, &tmp.stats, sizeof(f->stats));
 +		return err;
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
 +static void mlx5e_rep_indr_block_unbind(void *cb_priv)
 +{
 +	struct mlx5e_rep_indr_block_priv *indr_priv = cb_priv;
 +
 +	list_del(&indr_priv->list);
 +	kfree(indr_priv);
 +}
 +
 +static LIST_HEAD(mlx5e_block_cb_list);
 +
 +static int
 +mlx5e_rep_indr_setup_block(struct net_device *netdev,
 +			   struct mlx5e_rep_priv *rpriv,
 +			   struct flow_block_offload *f,
 +			   flow_setup_cb_t *setup_cb)
 +{
 +	struct mlx5e_rep_indr_block_priv *indr_priv;
 +	struct flow_block_cb *block_cb;
 +
 +	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
 +		return -EOPNOTSUPP;
 +
 +	f->unlocked_driver_cb = true;
 +	f->driver_block_list = &mlx5e_block_cb_list;
 +
 +	switch (f->command) {
 +	case FLOW_BLOCK_BIND:
 +		indr_priv = mlx5e_rep_indr_block_priv_lookup(rpriv, netdev);
 +		if (indr_priv)
 +			return -EEXIST;
 +
 +		indr_priv = kmalloc(sizeof(*indr_priv), GFP_KERNEL);
 +		if (!indr_priv)
 +			return -ENOMEM;
 +
 +		indr_priv->netdev = netdev;
 +		indr_priv->rpriv = rpriv;
 +		list_add(&indr_priv->list,
 +			 &rpriv->uplink_priv.tc_indr_block_priv_list);
 +
 +		block_cb = flow_block_cb_alloc(setup_cb, indr_priv, indr_priv,
 +					       mlx5e_rep_indr_block_unbind);
 +		if (IS_ERR(block_cb)) {
 +			list_del(&indr_priv->list);
 +			kfree(indr_priv);
 +			return PTR_ERR(block_cb);
 +		}
 +		flow_block_cb_add(block_cb, f);
 +		list_add_tail(&block_cb->driver_list, &mlx5e_block_cb_list);
 +
 +		return 0;
 +	case FLOW_BLOCK_UNBIND:
 +		indr_priv = mlx5e_rep_indr_block_priv_lookup(rpriv, netdev);
 +		if (!indr_priv)
 +			return -ENOENT;
 +
 +		block_cb = flow_block_cb_lookup(f->block, setup_cb, indr_priv);
 +		if (!block_cb)
 +			return -ENOENT;
 +
 +		flow_block_cb_remove(block_cb, f);
 +		list_del(&block_cb->driver_list);
 +		return 0;
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +	return 0;
 +}
 +
 +static
 +int mlx5e_rep_indr_setup_cb(struct net_device *netdev, void *cb_priv,
 +			    enum tc_setup_type type, void *type_data)
 +{
 +	switch (type) {
 +	case TC_SETUP_BLOCK:
 +		return mlx5e_rep_indr_setup_block(netdev, cb_priv, type_data,
 +						  mlx5e_rep_indr_setup_tc_cb);
 +	case TC_SETUP_FT:
 +		return mlx5e_rep_indr_setup_block(netdev, cb_priv, type_data,
 +						  mlx5e_rep_indr_setup_ft_cb);
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
 +static int mlx5e_rep_indr_register_block(struct mlx5e_rep_priv *rpriv,
 +					 struct net_device *netdev)
 +{
 +	int err;
 +
 +	err = __flow_indr_block_cb_register(netdev, rpriv,
 +					    mlx5e_rep_indr_setup_cb,
 +					    rpriv);
 +	if (err) {
 +		struct mlx5e_priv *priv = netdev_priv(rpriv->netdev);
 +
 +		mlx5_core_err(priv->mdev, "Failed to register remote block notifier for %s err=%d\n",
 +			      netdev_name(netdev), err);
 +	}
 +	return err;
 +}
 +
 +static void mlx5e_rep_indr_unregister_block(struct mlx5e_rep_priv *rpriv,
 +					    struct net_device *netdev)
 +{
 +	__flow_indr_block_cb_unregister(netdev, mlx5e_rep_indr_setup_cb,
 +					rpriv);
 +}
 +
 +static int mlx5e_nic_rep_netdevice_event(struct notifier_block *nb,
 +					 unsigned long event, void *ptr)
 +{
 +	struct mlx5e_rep_priv *rpriv = container_of(nb, struct mlx5e_rep_priv,
 +						     uplink_priv.netdevice_nb);
 +	struct mlx5e_priv *priv = netdev_priv(rpriv->netdev);
 +	struct net_device *netdev = netdev_notifier_info_to_dev(ptr);
 +
 +	if (!mlx5e_tc_tun_device_to_offload(priv, netdev) &&
 +	    !(is_vlan_dev(netdev) && vlan_dev_real_dev(netdev) == rpriv->netdev))
 +		return NOTIFY_OK;
 +
 +	switch (event) {
 +	case NETDEV_REGISTER:
 +		mlx5e_rep_indr_register_block(rpriv, netdev);
 +		break;
 +	case NETDEV_UNREGISTER:
 +		mlx5e_rep_indr_unregister_block(rpriv, netdev);
 +		break;
 +	}
 +	return NOTIFY_OK;
 +}
 +
 +static void
 +mlx5e_rep_queue_neigh_update_work(struct mlx5e_priv *priv,
 +				  struct mlx5e_neigh_hash_entry *nhe,
 +				  struct neighbour *n)
 +{
 +	/* Take a reference to ensure the neighbour and mlx5 encap
 +	 * entry won't be destructed until we drop the reference in
 +	 * delayed work.
 +	 */
 +	neigh_hold(n);
 +
 +	/* This assignment is valid as long as the the neigh reference
 +	 * is taken
 +	 */
 +	nhe->n = n;
 +
 +	if (!queue_work(priv->wq, &nhe->neigh_update_work)) {
 +		mlx5e_rep_neigh_entry_release(nhe);
 +		neigh_release(n);
 +	}
 +}
 +
 +static struct mlx5e_neigh_hash_entry *
 +mlx5e_rep_neigh_entry_lookup(struct mlx5e_priv *priv,
 +			     struct mlx5e_neigh *m_neigh);
 +
 +static int mlx5e_rep_netevent_event(struct notifier_block *nb,
 +				    unsigned long event, void *ptr)
 +{
 +	struct mlx5e_rep_priv *rpriv = container_of(nb, struct mlx5e_rep_priv,
 +						    neigh_update.netevent_nb);
 +	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
 +	struct net_device *netdev = rpriv->netdev;
 +	struct mlx5e_priv *priv = netdev_priv(netdev);
 +	struct mlx5e_neigh_hash_entry *nhe = NULL;
 +	struct mlx5e_neigh m_neigh = {};
 +	struct neigh_parms *p;
 +	struct neighbour *n;
 +	bool found = false;
 +
 +	switch (event) {
 +	case NETEVENT_NEIGH_UPDATE:
 +		n = ptr;
 +#if IS_ENABLED(CONFIG_IPV6)
 +		if (n->tbl != ipv6_stub->nd_tbl && n->tbl != &arp_tbl)
 +#else
 +		if (n->tbl != &arp_tbl)
 +#endif
 +			return NOTIFY_DONE;
 +
 +		m_neigh.dev = n->dev;
 +		m_neigh.family = n->ops->family;
 +		memcpy(&m_neigh.dst_ip, n->primary_key, n->tbl->key_len);
 +
 +		rcu_read_lock();
 +		nhe = mlx5e_rep_neigh_entry_lookup(priv, &m_neigh);
 +		rcu_read_unlock();
 +		if (!nhe)
 +			return NOTIFY_DONE;
 +
 +		mlx5e_rep_queue_neigh_update_work(priv, nhe, n);
 +		break;
 +
 +	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
 +		p = ptr;
 +
 +		/* We check the device is present since we don't care about
 +		 * changes in the default table, we only care about changes
 +		 * done per device delay prob time parameter.
 +		 */
 +#if IS_ENABLED(CONFIG_IPV6)
 +		if (!p->dev || (p->tbl != ipv6_stub->nd_tbl && p->tbl != &arp_tbl))
 +#else
 +		if (!p->dev || p->tbl != &arp_tbl)
 +#endif
 +			return NOTIFY_DONE;
 +
 +		rcu_read_lock();
 +		list_for_each_entry_rcu(nhe, &neigh_update->neigh_list,
 +					neigh_list) {
 +			if (p->dev == nhe->m_neigh.dev) {
 +				found = true;
 +				break;
 +			}
 +		}
 +		rcu_read_unlock();
 +		if (!found)
 +			return NOTIFY_DONE;
 +
 +		neigh_update->min_interval = min_t(unsigned long,
 +						   NEIGH_VAR(p, DELAY_PROBE_TIME),
 +						   neigh_update->min_interval);
 +		mlx5_fc_update_sampling_interval(priv->mdev,
 +						 neigh_update->min_interval);
 +		break;
 +	}
 +	return NOTIFY_DONE;
 +}
 +
 +static const struct rhashtable_params mlx5e_neigh_ht_params = {
 +	.head_offset = offsetof(struct mlx5e_neigh_hash_entry, rhash_node),
 +	.key_offset = offsetof(struct mlx5e_neigh_hash_entry, m_neigh),
 +	.key_len = sizeof(struct mlx5e_neigh),
 +	.automatic_shrinking = true,
 +};
 +
 +static int mlx5e_rep_neigh_init(struct mlx5e_rep_priv *rpriv)
 +{
 +	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
 +	int err;
 +
 +	err = rhashtable_init(&neigh_update->neigh_ht, &mlx5e_neigh_ht_params);
 +	if (err)
 +		return err;
 +
 +	INIT_LIST_HEAD(&neigh_update->neigh_list);
 +	mutex_init(&neigh_update->encap_lock);
 +	INIT_DELAYED_WORK(&neigh_update->neigh_stats_work,
 +			  mlx5e_rep_neigh_stats_work);
 +	mlx5e_rep_neigh_update_init_interval(rpriv);
 +
 +	rpriv->neigh_update.netevent_nb.notifier_call = mlx5e_rep_netevent_event;
 +	err = register_netevent_notifier(&rpriv->neigh_update.netevent_nb);
 +	if (err)
 +		goto out_err;
 +	return 0;
 +
 +out_err:
 +	rhashtable_destroy(&neigh_update->neigh_ht);
 +	return err;
 +}
 +
 +static void mlx5e_rep_neigh_cleanup(struct mlx5e_rep_priv *rpriv)
 +{
 +	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
 +	struct mlx5e_priv *priv = netdev_priv(rpriv->netdev);
 +
 +	unregister_netevent_notifier(&neigh_update->netevent_nb);
 +
 +	flush_workqueue(priv->wq); /* flush neigh update works */
 +
 +	cancel_delayed_work_sync(&rpriv->neigh_update.neigh_stats_work);
 +
 +	mutex_destroy(&neigh_update->encap_lock);
 +	rhashtable_destroy(&neigh_update->neigh_ht);
 +}
 +
 +static int mlx5e_rep_neigh_entry_insert(struct mlx5e_priv *priv,
 +					struct mlx5e_neigh_hash_entry *nhe)
 +{
 +	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 +	int err;
 +
 +	err = rhashtable_insert_fast(&rpriv->neigh_update.neigh_ht,
 +				     &nhe->rhash_node,
 +				     mlx5e_neigh_ht_params);
 +	if (err)
 +		return err;
 +
 +	list_add_rcu(&nhe->neigh_list, &rpriv->neigh_update.neigh_list);
 +
 +	return err;
 +}
 +
 +static void mlx5e_rep_neigh_entry_remove(struct mlx5e_neigh_hash_entry *nhe)
 +{
 +	struct mlx5e_rep_priv *rpriv = nhe->priv->ppriv;
 +
 +	mutex_lock(&rpriv->neigh_update.encap_lock);
 +
 +	list_del_rcu(&nhe->neigh_list);
 +
 +	rhashtable_remove_fast(&rpriv->neigh_update.neigh_ht,
 +			       &nhe->rhash_node,
 +			       mlx5e_neigh_ht_params);
 +	mutex_unlock(&rpriv->neigh_update.encap_lock);
 +}
 +
 +/* This function must only be called under the representor's encap_lock or
 + * inside rcu read lock section.
 + */
 +static struct mlx5e_neigh_hash_entry *
 +mlx5e_rep_neigh_entry_lookup(struct mlx5e_priv *priv,
 +			     struct mlx5e_neigh *m_neigh)
 +{
 +	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 +	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
 +	struct mlx5e_neigh_hash_entry *nhe;
 +
 +	nhe = rhashtable_lookup_fast(&neigh_update->neigh_ht, m_neigh,
 +				     mlx5e_neigh_ht_params);
 +	return nhe && mlx5e_rep_neigh_entry_hold(nhe) ? nhe : NULL;
 +}
 +
 +static int mlx5e_rep_neigh_entry_create(struct mlx5e_priv *priv,
 +					struct mlx5e_encap_entry *e,
 +					struct mlx5e_neigh_hash_entry **nhe)
 +{
 +	int err;
 +
 +	*nhe = kzalloc(sizeof(**nhe), GFP_KERNEL);
 +	if (!*nhe)
 +		return -ENOMEM;
 +
 +	(*nhe)->priv = priv;
 +	memcpy(&(*nhe)->m_neigh, &e->m_neigh, sizeof(e->m_neigh));
 +	INIT_WORK(&(*nhe)->neigh_update_work, mlx5e_rep_neigh_update);
 +	spin_lock_init(&(*nhe)->encap_list_lock);
 +	INIT_LIST_HEAD(&(*nhe)->encap_list);
 +	refcount_set(&(*nhe)->refcnt, 1);
 +
 +	err = mlx5e_rep_neigh_entry_insert(priv, *nhe);
 +	if (err)
 +		goto out_free;
 +	return 0;
 +
 +out_free:
 +	kfree(*nhe);
 +	return err;
 +}
 +
 +int mlx5e_rep_encap_entry_attach(struct mlx5e_priv *priv,
 +				 struct mlx5e_encap_entry *e)
 +{
 +	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 +	struct mlx5_rep_uplink_priv *uplink_priv = &rpriv->uplink_priv;
 +	struct mlx5_tun_entropy *tun_entropy = &uplink_priv->tun_entropy;
 +	struct mlx5e_neigh_hash_entry *nhe;
 +	int err;
 +
 +	err = mlx5_tun_entropy_refcount_inc(tun_entropy, e->reformat_type);
 +	if (err)
 +		return err;
 +
 +	mutex_lock(&rpriv->neigh_update.encap_lock);
 +	nhe = mlx5e_rep_neigh_entry_lookup(priv, &e->m_neigh);
 +	if (!nhe) {
 +		err = mlx5e_rep_neigh_entry_create(priv, e, &nhe);
 +		if (err) {
 +			mutex_unlock(&rpriv->neigh_update.encap_lock);
 +			mlx5_tun_entropy_refcount_dec(tun_entropy,
 +						      e->reformat_type);
 +			return err;
 +		}
 +	}
 +
 +	e->nhe = nhe;
 +	spin_lock(&nhe->encap_list_lock);
 +	list_add_rcu(&e->encap_list, &nhe->encap_list);
 +	spin_unlock(&nhe->encap_list_lock);
 +
 +	mutex_unlock(&rpriv->neigh_update.encap_lock);
 +
 +	return 0;
 +}
 +
 +void mlx5e_rep_encap_entry_detach(struct mlx5e_priv *priv,
 +				  struct mlx5e_encap_entry *e)
 +{
 +	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 +	struct mlx5_rep_uplink_priv *uplink_priv = &rpriv->uplink_priv;
 +	struct mlx5_tun_entropy *tun_entropy = &uplink_priv->tun_entropy;
 +
 +	if (!e->nhe)
 +		return;
 +
 +	spin_lock(&e->nhe->encap_list_lock);
 +	list_del_rcu(&e->encap_list);
 +	spin_unlock(&e->nhe->encap_list_lock);
 +
 +	mlx5e_rep_neigh_entry_release(e->nhe);
 +	e->nhe = NULL;
 +	mlx5_tun_entropy_refcount_dec(tun_entropy, e->reformat_type);
 +}
 +
++=======
++>>>>>>> 549c243e4e01 (net/mlx5e: Extract neigh-specific code from en_rep.c to rep/neigh.c)
  static int mlx5e_rep_open(struct net_device *dev)
  {
  	struct mlx5e_priv *priv = netdev_priv(dev);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
index a0bd9f11375b,81ed06e58fea..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
@@@ -195,20 -203,10 +195,23 @@@ void mlx5e_handle_rx_cqe_rep(struct mlx
  void mlx5e_handle_rx_cqe_mpwrq_rep(struct mlx5e_rq *rq,
  				   struct mlx5_cqe64 *cqe);
  
 +int mlx5e_rep_encap_entry_attach(struct mlx5e_priv *priv,
 +				 struct mlx5e_encap_entry *e);
 +void mlx5e_rep_encap_entry_detach(struct mlx5e_priv *priv,
 +				  struct mlx5e_encap_entry *e);
 +
  void mlx5e_rep_queue_neigh_stats_work(struct mlx5e_priv *priv);
  
 -bool mlx5e_eswitch_rep(struct net_device *netdev);
 +bool mlx5e_eswitch_vf_rep(struct net_device *netdev);
  bool mlx5e_eswitch_uplink_rep(struct net_device *netdev);
++<<<<<<< HEAD
 +static inline bool mlx5e_eswitch_rep(struct net_device *netdev)
 +{
 +	return mlx5e_eswitch_vf_rep(netdev) ||
 +	       mlx5e_eswitch_uplink_rep(netdev);
 +}
++=======
++>>>>>>> 549c243e4e01 (net/mlx5e: Extract neigh-specific code from en_rep.c to rep/neigh.c)
  
  #else /* CONFIG_MLX5_ESWITCH */
  static inline bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv) { return false; }
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index d358fdb49b84,749390dc7aaa..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -4224,138 -4777,3 +4224,141 @@@ void mlx5e_tc_reoffload_flows_work(stru
  	}
  	mutex_unlock(&rpriv->unready_flows_lock);
  }
++<<<<<<< HEAD
 +
 +#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)
 +static bool mlx5e_restore_tunnel(struct mlx5e_priv *priv, struct sk_buff *skb,
 +				 struct mlx5e_tc_update_priv *tc_priv,
 +				 u32 tunnel_id)
 +{
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct flow_dissector_key_enc_opts enc_opts = {};
 +	struct mlx5_rep_uplink_priv *uplink_priv;
 +	struct mlx5e_rep_priv *uplink_rpriv;
 +	struct metadata_dst *tun_dst;
 +	struct tunnel_match_key key;
 +	u32 tun_id, enc_opts_id;
 +	struct net_device *dev;
 +	int err;
 +
 +	enc_opts_id = tunnel_id & ENC_OPTS_BITS_MASK;
 +	tun_id = tunnel_id >> ENC_OPTS_BITS;
 +
 +	if (!tun_id)
 +		return true;
 +
 +	uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
 +	uplink_priv = &uplink_rpriv->uplink_priv;
 +
 +	err = mapping_find(uplink_priv->tunnel_mapping, tun_id, &key);
 +	if (err) {
 +		WARN_ON_ONCE(true);
 +		netdev_dbg(priv->netdev,
 +			   "Couldn't find tunnel for tun_id: %d, err: %d\n",
 +			   tun_id, err);
 +		return false;
 +	}
 +
 +	if (enc_opts_id) {
 +		err = mapping_find(uplink_priv->tunnel_enc_opts_mapping,
 +				   enc_opts_id, &enc_opts);
 +		if (err) {
 +			netdev_dbg(priv->netdev,
 +				   "Couldn't find tunnel (opts) for tun_id: %d, err: %d\n",
 +				   enc_opts_id, err);
 +			return false;
 +		}
 +	}
 +
 +	tun_dst = tun_rx_dst(enc_opts.len);
 +	if (!tun_dst) {
 +		WARN_ON_ONCE(true);
 +		return false;
 +	}
 +
 +	ip_tunnel_key_init(&tun_dst->u.tun_info.key,
 +			   key.enc_ipv4.src, key.enc_ipv4.dst,
 +			   key.enc_ip.tos, key.enc_ip.ttl,
 +			   0, /* label */
 +			   key.enc_tp.src, key.enc_tp.dst,
 +			   key32_to_tunnel_id(key.enc_key_id.keyid),
 +			   TUNNEL_KEY);
 +
 +	if (enc_opts.len)
 +		ip_tunnel_info_opts_set(&tun_dst->u.tun_info, enc_opts.data,
 +					enc_opts.len, enc_opts.dst_opt_type);
 +
 +	skb_dst_set(skb, (struct dst_entry *)tun_dst);
 +	dev = dev_get_by_index(&init_net, key.filter_ifindex);
 +	if (!dev) {
 +		netdev_dbg(priv->netdev,
 +			   "Couldn't find tunnel device with ifindex: %d\n",
 +			   key.filter_ifindex);
 +		return false;
 +	}
 +
 +	/* Set tun_dev so we do dev_put() after datapath */
 +	tc_priv->tun_dev = dev;
 +
 +	skb->dev = dev;
 +
 +	return true;
 +}
 +#endif /* CONFIG_NET_TC_SKB_EXT */
 +
 +bool mlx5e_tc_rep_update_skb(struct mlx5_cqe64 *cqe,
 +			     struct sk_buff *skb,
 +			     struct mlx5e_tc_update_priv *tc_priv)
 +{
 +#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)
 +	u32 chain = 0, reg_c0, reg_c1, tunnel_id;
 +	struct tc_skb_ext *tc_skb_ext;
 +	struct mlx5_eswitch *esw;
 +	struct mlx5e_priv *priv;
 +	int tunnel_moffset;
 +	int err;
 +
 +	reg_c0 = (be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK);
 +	if (reg_c0 == MLX5_FS_DEFAULT_FLOW_TAG)
 +		reg_c0 = 0;
 +	reg_c1 = be32_to_cpu(cqe->ft_metadata);
 +
 +	if (!reg_c0)
 +		return true;
 +
 +	priv = netdev_priv(skb->dev);
 +	esw = priv->mdev->priv.eswitch;
 +
 +	err = mlx5_eswitch_get_chain_for_tag(esw, reg_c0, &chain);
 +	if (err) {
 +		netdev_dbg(priv->netdev,
 +			   "Couldn't find chain for chain tag: %d, err: %d\n",
 +			   reg_c0, err);
 +		return false;
 +	}
 +
 +	if (chain) {
 +		tc_skb_ext = skb_ext_add(skb, TC_SKB_EXT);
 +		if (!tc_skb_ext) {
 +			WARN_ON(1);
 +			return false;
 +		}
 +
 +		tc_skb_ext->chain = chain;
 +	}
 +
 +	tunnel_moffset = mlx5e_tc_attr_to_reg_mappings[TUNNEL_TO_REG].moffset;
 +	tunnel_id = reg_c1 >> (8 * tunnel_moffset);
 +	return mlx5e_restore_tunnel(priv, skb, tc_priv, tunnel_id);
 +#endif /* CONFIG_NET_TC_SKB_EXT */
 +
 +	return true;
 +}
 +
 +void mlx5_tc_rep_post_napi_receive(struct mlx5e_tc_update_priv *tc_priv)
 +{
 +	if (tc_priv->tun_dev)
 +		dev_put(tc_priv->tun_dev);
 +}
++=======
++>>>>>>> 549c243e4e01 (net/mlx5e: Extract neigh-specific code from en_rep.c to rep/neigh.c)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/Makefile
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/rep/neigh.c b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/neigh.c
new file mode 100644
index 000000000000..baa162432e75
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/neigh.c
@@ -0,0 +1,368 @@
+// SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB
+/* Copyright (c) 2020 Mellanox Technologies. */
+
+#include <linux/refcount.h>
+#include <linux/list.h>
+#include <linux/rculist.h>
+#include <linux/rtnetlink.h>
+#include <linux/workqueue.h>
+#include <linux/rwlock.h>
+#include <linux/spinlock.h>
+#include <linux/notifier.h>
+#include <net/netevent.h>
+#include "neigh.h"
+#include "tc.h"
+#include "en_rep.h"
+#include "fs_core.h"
+#include "diag/en_rep_tracepoint.h"
+
+static unsigned long mlx5e_rep_ipv6_interval(void)
+{
+	if (IS_ENABLED(CONFIG_IPV6) && ipv6_stub->nd_tbl)
+		return NEIGH_VAR(&ipv6_stub->nd_tbl->parms, DELAY_PROBE_TIME);
+
+	return ~0UL;
+}
+
+static void mlx5e_rep_neigh_update_init_interval(struct mlx5e_rep_priv *rpriv)
+{
+	unsigned long ipv4_interval = NEIGH_VAR(&arp_tbl.parms, DELAY_PROBE_TIME);
+	unsigned long ipv6_interval = mlx5e_rep_ipv6_interval();
+	struct net_device *netdev = rpriv->netdev;
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+
+	rpriv->neigh_update.min_interval = min_t(unsigned long, ipv6_interval, ipv4_interval);
+	mlx5_fc_update_sampling_interval(priv->mdev, rpriv->neigh_update.min_interval);
+}
+
+void mlx5e_rep_queue_neigh_stats_work(struct mlx5e_priv *priv)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+
+	mlx5_fc_queue_stats_work(priv->mdev,
+				 &neigh_update->neigh_stats_work,
+				 neigh_update->min_interval);
+}
+
+static bool mlx5e_rep_neigh_entry_hold(struct mlx5e_neigh_hash_entry *nhe)
+{
+	return refcount_inc_not_zero(&nhe->refcnt);
+}
+
+static void mlx5e_rep_neigh_entry_remove(struct mlx5e_neigh_hash_entry *nhe);
+
+void mlx5e_rep_neigh_entry_release(struct mlx5e_neigh_hash_entry *nhe)
+{
+	if (refcount_dec_and_test(&nhe->refcnt)) {
+		mlx5e_rep_neigh_entry_remove(nhe);
+		kfree_rcu(nhe, rcu);
+	}
+}
+
+static struct mlx5e_neigh_hash_entry *
+mlx5e_get_next_nhe(struct mlx5e_rep_priv *rpriv,
+		   struct mlx5e_neigh_hash_entry *nhe)
+{
+	struct mlx5e_neigh_hash_entry *next = NULL;
+
+	rcu_read_lock();
+
+	for (next = nhe ?
+		     list_next_or_null_rcu(&rpriv->neigh_update.neigh_list,
+					   &nhe->neigh_list,
+					   struct mlx5e_neigh_hash_entry,
+					   neigh_list) :
+		     list_first_or_null_rcu(&rpriv->neigh_update.neigh_list,
+					    struct mlx5e_neigh_hash_entry,
+					    neigh_list);
+	     next;
+	     next = list_next_or_null_rcu(&rpriv->neigh_update.neigh_list,
+					  &next->neigh_list,
+					  struct mlx5e_neigh_hash_entry,
+					  neigh_list))
+		if (mlx5e_rep_neigh_entry_hold(next))
+			break;
+
+	rcu_read_unlock();
+
+	if (nhe)
+		mlx5e_rep_neigh_entry_release(nhe);
+
+	return next;
+}
+
+static void mlx5e_rep_neigh_stats_work(struct work_struct *work)
+{
+	struct mlx5e_rep_priv *rpriv = container_of(work, struct mlx5e_rep_priv,
+						    neigh_update.neigh_stats_work.work);
+	struct net_device *netdev = rpriv->netdev;
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	struct mlx5e_neigh_hash_entry *nhe = NULL;
+
+	rtnl_lock();
+	if (!list_empty(&rpriv->neigh_update.neigh_list))
+		mlx5e_rep_queue_neigh_stats_work(priv);
+
+	while ((nhe = mlx5e_get_next_nhe(rpriv, nhe)) != NULL)
+		mlx5e_tc_update_neigh_used_value(nhe);
+
+	rtnl_unlock();
+}
+
+static void mlx5e_rep_neigh_update(struct work_struct *work)
+{
+	struct mlx5e_neigh_hash_entry *nhe =
+		container_of(work, struct mlx5e_neigh_hash_entry, neigh_update_work);
+	struct neighbour *n = nhe->n;
+	struct mlx5e_encap_entry *e;
+	unsigned char ha[ETH_ALEN];
+	struct mlx5e_priv *priv;
+	bool neigh_connected;
+	u8 nud_state, dead;
+
+	rtnl_lock();
+
+	/* If these parameters are changed after we release the lock,
+	 * we'll receive another event letting us know about it.
+	 * We use this lock to avoid inconsistency between the neigh validity
+	 * and it's hw address.
+	 */
+	read_lock_bh(&n->lock);
+	memcpy(ha, n->ha, ETH_ALEN);
+	nud_state = n->nud_state;
+	dead = n->dead;
+	read_unlock_bh(&n->lock);
+
+	neigh_connected = (nud_state & NUD_VALID) && !dead;
+
+	trace_mlx5e_rep_neigh_update(nhe, ha, neigh_connected);
+
+	list_for_each_entry(e, &nhe->encap_list, encap_list) {
+		if (!mlx5e_encap_take(e))
+			continue;
+
+		priv = netdev_priv(e->out_dev);
+		mlx5e_rep_update_flows(priv, e, neigh_connected, ha);
+		mlx5e_encap_put(priv, e);
+	}
+	mlx5e_rep_neigh_entry_release(nhe);
+	rtnl_unlock();
+	neigh_release(n);
+}
+
+static void mlx5e_rep_queue_neigh_update_work(struct mlx5e_priv *priv,
+					      struct mlx5e_neigh_hash_entry *nhe,
+					      struct neighbour *n)
+{
+	/* Take a reference to ensure the neighbour and mlx5 encap
+	 * entry won't be destructed until we drop the reference in
+	 * delayed work.
+	 */
+	neigh_hold(n);
+
+	/* This assignment is valid as long as the the neigh reference
+	 * is taken
+	 */
+	nhe->n = n;
+
+	if (!queue_work(priv->wq, &nhe->neigh_update_work)) {
+		mlx5e_rep_neigh_entry_release(nhe);
+		neigh_release(n);
+	}
+}
+
+static int mlx5e_rep_netevent_event(struct notifier_block *nb,
+				    unsigned long event, void *ptr)
+{
+	struct mlx5e_rep_priv *rpriv = container_of(nb, struct mlx5e_rep_priv,
+						    neigh_update.netevent_nb);
+	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+	struct net_device *netdev = rpriv->netdev;
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	struct mlx5e_neigh_hash_entry *nhe = NULL;
+	struct mlx5e_neigh m_neigh = {};
+	struct neigh_parms *p;
+	struct neighbour *n;
+	bool found = false;
+
+	switch (event) {
+	case NETEVENT_NEIGH_UPDATE:
+		n = ptr;
+#if IS_ENABLED(CONFIG_IPV6)
+		if (n->tbl != ipv6_stub->nd_tbl && n->tbl != &arp_tbl)
+#else
+		if (n->tbl != &arp_tbl)
+#endif
+			return NOTIFY_DONE;
+
+		m_neigh.dev = n->dev;
+		m_neigh.family = n->ops->family;
+		memcpy(&m_neigh.dst_ip, n->primary_key, n->tbl->key_len);
+
+		rcu_read_lock();
+		nhe = mlx5e_rep_neigh_entry_lookup(priv, &m_neigh);
+		rcu_read_unlock();
+		if (!nhe)
+			return NOTIFY_DONE;
+
+		mlx5e_rep_queue_neigh_update_work(priv, nhe, n);
+		break;
+
+	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
+		p = ptr;
+
+		/* We check the device is present since we don't care about
+		 * changes in the default table, we only care about changes
+		 * done per device delay prob time parameter.
+		 */
+#if IS_ENABLED(CONFIG_IPV6)
+		if (!p->dev || (p->tbl != ipv6_stub->nd_tbl && p->tbl != &arp_tbl))
+#else
+		if (!p->dev || p->tbl != &arp_tbl)
+#endif
+			return NOTIFY_DONE;
+
+		rcu_read_lock();
+		list_for_each_entry_rcu(nhe, &neigh_update->neigh_list,
+					neigh_list) {
+			if (p->dev == nhe->m_neigh.dev) {
+				found = true;
+				break;
+			}
+		}
+		rcu_read_unlock();
+		if (!found)
+			return NOTIFY_DONE;
+
+		neigh_update->min_interval = min_t(unsigned long,
+						   NEIGH_VAR(p, DELAY_PROBE_TIME),
+						   neigh_update->min_interval);
+		mlx5_fc_update_sampling_interval(priv->mdev,
+						 neigh_update->min_interval);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static const struct rhashtable_params mlx5e_neigh_ht_params = {
+	.head_offset = offsetof(struct mlx5e_neigh_hash_entry, rhash_node),
+	.key_offset = offsetof(struct mlx5e_neigh_hash_entry, m_neigh),
+	.key_len = sizeof(struct mlx5e_neigh),
+	.automatic_shrinking = true,
+};
+
+int mlx5e_rep_neigh_init(struct mlx5e_rep_priv *rpriv)
+{
+	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+	int err;
+
+	err = rhashtable_init(&neigh_update->neigh_ht, &mlx5e_neigh_ht_params);
+	if (err)
+		return err;
+
+	INIT_LIST_HEAD(&neigh_update->neigh_list);
+	mutex_init(&neigh_update->encap_lock);
+	INIT_DELAYED_WORK(&neigh_update->neigh_stats_work,
+			  mlx5e_rep_neigh_stats_work);
+	mlx5e_rep_neigh_update_init_interval(rpriv);
+
+	rpriv->neigh_update.netevent_nb.notifier_call = mlx5e_rep_netevent_event;
+	err = register_netevent_notifier(&rpriv->neigh_update.netevent_nb);
+	if (err)
+		goto out_err;
+	return 0;
+
+out_err:
+	rhashtable_destroy(&neigh_update->neigh_ht);
+	return err;
+}
+
+void mlx5e_rep_neigh_cleanup(struct mlx5e_rep_priv *rpriv)
+{
+	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+	struct mlx5e_priv *priv = netdev_priv(rpriv->netdev);
+
+	unregister_netevent_notifier(&neigh_update->netevent_nb);
+
+	flush_workqueue(priv->wq); /* flush neigh update works */
+
+	cancel_delayed_work_sync(&rpriv->neigh_update.neigh_stats_work);
+
+	mutex_destroy(&neigh_update->encap_lock);
+	rhashtable_destroy(&neigh_update->neigh_ht);
+}
+
+static int mlx5e_rep_neigh_entry_insert(struct mlx5e_priv *priv,
+					struct mlx5e_neigh_hash_entry *nhe)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+	int err;
+
+	err = rhashtable_insert_fast(&rpriv->neigh_update.neigh_ht,
+				     &nhe->rhash_node,
+				     mlx5e_neigh_ht_params);
+	if (err)
+		return err;
+
+	list_add_rcu(&nhe->neigh_list, &rpriv->neigh_update.neigh_list);
+
+	return err;
+}
+
+static void mlx5e_rep_neigh_entry_remove(struct mlx5e_neigh_hash_entry *nhe)
+{
+	struct mlx5e_rep_priv *rpriv = nhe->priv->ppriv;
+
+	mutex_lock(&rpriv->neigh_update.encap_lock);
+
+	list_del_rcu(&nhe->neigh_list);
+
+	rhashtable_remove_fast(&rpriv->neigh_update.neigh_ht,
+			       &nhe->rhash_node,
+			       mlx5e_neigh_ht_params);
+	mutex_unlock(&rpriv->neigh_update.encap_lock);
+}
+
+/* This function must only be called under the representor's encap_lock or
+ * inside rcu read lock section.
+ */
+struct mlx5e_neigh_hash_entry *
+mlx5e_rep_neigh_entry_lookup(struct mlx5e_priv *priv,
+			     struct mlx5e_neigh *m_neigh)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+	struct mlx5e_neigh_hash_entry *nhe;
+
+	nhe = rhashtable_lookup_fast(&neigh_update->neigh_ht, m_neigh,
+				     mlx5e_neigh_ht_params);
+	return nhe && mlx5e_rep_neigh_entry_hold(nhe) ? nhe : NULL;
+}
+
+int mlx5e_rep_neigh_entry_create(struct mlx5e_priv *priv,
+				 struct mlx5e_encap_entry *e,
+				 struct mlx5e_neigh_hash_entry **nhe)
+{
+	int err;
+
+	*nhe = kzalloc(sizeof(**nhe), GFP_KERNEL);
+	if (!*nhe)
+		return -ENOMEM;
+
+	(*nhe)->priv = priv;
+	memcpy(&(*nhe)->m_neigh, &e->m_neigh, sizeof(e->m_neigh));
+	INIT_WORK(&(*nhe)->neigh_update_work, mlx5e_rep_neigh_update);
+	spin_lock_init(&(*nhe)->encap_list_lock);
+	INIT_LIST_HEAD(&(*nhe)->encap_list);
+	refcount_set(&(*nhe)->refcnt, 1);
+
+	err = mlx5e_rep_neigh_entry_insert(priv, *nhe);
+	if (err)
+		goto out_free;
+	return 0;
+
+out_free:
+	kfree(*nhe);
+	return err;
+}
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/rep/neigh.h b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/neigh.h
new file mode 100644
index 000000000000..8eddb3ac0d74
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/neigh.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB */
+/* Copyright (c) 2020 Mellanox Technologies. */
+
+#ifndef __MLX5_EN_REP_NEIGH__
+#define __MLX5_EN_REP_NEIGH__
+
+#include "en.h"
+#include "en_rep.h"
+
+int mlx5e_rep_neigh_init(struct mlx5e_rep_priv *rpriv);
+void mlx5e_rep_neigh_cleanup(struct mlx5e_rep_priv *rpriv);
+
+struct mlx5e_neigh_hash_entry *
+mlx5e_rep_neigh_entry_lookup(struct mlx5e_priv *priv,
+			     struct mlx5e_neigh *m_neigh);
+int mlx5e_rep_neigh_entry_create(struct mlx5e_priv *priv,
+				 struct mlx5e_encap_entry *e,
+				 struct mlx5e_neigh_hash_entry **nhe);
+void mlx5e_rep_neigh_entry_release(struct mlx5e_neigh_hash_entry *nhe);
+
+void mlx5e_rep_queue_neigh_stats_work(struct mlx5e_priv *priv);
+
+#endif /* __MLX5_EN_REP_NEIGH__ */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
