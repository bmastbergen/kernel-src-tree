drm/udl: Unmap buffer object after damage update

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Thomas Zimmermann <tzimmermann@suse.de>
commit 6c44e30ae130b740441ac0c65d5fa12dd6586942
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6c44e30a.failed

Udl keeps a BO mapped for its entire lifetime if it has been used in a
damage update at least once. The BO's free callback release the mapping
before it frees the BO.

Change this behaviour to unmap immediately after the damage update, so
SHMEM's implementation of free can be used.

	Signed-off-by: Thomas Zimmermann <tzimmermann@suse.de>
	Acked-by: Sam Ravnborg <sam@ravnborg.org>
Link: https://patchwork.freedesktop.org/patch/msgid/20191114141025.32198-2-tzimmermann@suse.de
(cherry picked from commit 6c44e30ae130b740441ac0c65d5fa12dd6586942)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/udl/udl_fb.c
diff --cc drivers/gpu/drm/udl/udl_fb.c
index e1116bf7b9d7,a9e6ec360b16..000000000000
--- a/drivers/gpu/drm/udl/udl_fb.c
+++ b/drivers/gpu/drm/udl/udl_fb.c
@@@ -95,16 -81,10 +96,23 @@@ int udl_handle_damage(struct udl_frameb
  	if (!fb->active_16)
  		return 0;
  
++<<<<<<< HEAD
 +	if (!fb->obj->vmapping) {
 +		ret = udl_gem_vmap(fb->obj);
 +		if (ret == -ENOMEM) {
 +			DRM_ERROR("failed to vmap fb\n");
 +			return 0;
 +		}
 +		if (!fb->obj->vmapping) {
 +			DRM_ERROR("failed to vmapping\n");
 +			return 0;
 +		}
++=======
+ 	vaddr = drm_gem_shmem_vmap(&fb->shmem->base);
+ 	if (IS_ERR(vaddr)) {
+ 		DRM_ERROR("failed to vmap fb\n");
+ 		return 0;
++>>>>>>> 6c44e30ae130 (drm/udl: Unmap buffer object after damage update)
  	}
  
  	aligned_x = DL_ALIGN_DOWN(x, sizeof(unsigned long));
@@@ -127,8 -109,7 +137,12 @@@
  		const int line_offset = fb->base.pitches[0] * i;
  		const int byte_offset = line_offset + (x << log_bpp);
  		const int dev_byte_offset = (fb->base.width * i + x) << log_bpp;
++<<<<<<< HEAD
 +		if (udl_render_hline(dev, log_bpp, &urb,
 +				     (char *) fb->obj->vmapping,
++=======
+ 		if (udl_render_hline(dev, log_bpp, &urb, (char *)vaddr,
++>>>>>>> 6c44e30ae130 (drm/udl: Unmap buffer object after damage update)
  				     &cmd, byte_offset, dev_byte_offset,
  				     width << log_bpp,
  				     &bytes_identical, &bytes_sent))
@@@ -155,126 -136,16 +169,133 @@@ error
  		    >> 10)), /* Kcycles */
  		   &udl->cpu_kcycles_used);
  
+ out:
+ 	drm_gem_shmem_vunmap(&fb->shmem->base, vaddr);
+ 
  	return 0;
+ 
+ err_drm_gem_shmem_vunmap:
+ 	drm_gem_shmem_vunmap(&fb->shmem->base, vaddr);
+ 	return ret;
  }
  
 +static int udl_fb_mmap(struct fb_info *info, struct vm_area_struct *vma)
 +{
 +	unsigned long start = vma->vm_start;
 +	unsigned long size = vma->vm_end - vma->vm_start;
 +	unsigned long offset;
 +	unsigned long page, pos;
 +
 +	if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT))
 +		return -EINVAL;
 +
 +	offset = vma->vm_pgoff << PAGE_SHIFT;
 +
 +	if (offset > info->fix.smem_len || size > info->fix.smem_len - offset)
 +		return -EINVAL;
 +
 +	pos = (unsigned long)info->fix.smem_start + offset;
 +
 +	pr_debug("mmap() framebuffer addr:%lu size:%lu\n",
 +		  pos, size);
 +
 +	/* We don't want the framebuffer to be mapped encrypted */
 +	vma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);
 +
 +	while (size > 0) {
 +		page = vmalloc_to_pfn((void *)pos);
 +		if (remap_pfn_range(vma, start, page, PAGE_SIZE, PAGE_SHARED))
 +			return -EAGAIN;
 +
 +		start += PAGE_SIZE;
 +		pos += PAGE_SIZE;
 +		if (size > PAGE_SIZE)
 +			size -= PAGE_SIZE;
 +		else
 +			size = 0;
 +	}
 +
 +	/* VM_IO | VM_DONTEXPAND | VM_DONTDUMP are set by remap_pfn_range() */
 +	return 0;
 +}
 +
 +/*
 + * It's common for several clients to have framebuffer open simultaneously.
 + * e.g. both fbcon and X. Makes things interesting.
 + * Assumes caller is holding info->lock (for open and release at least)
 + */
 +static int udl_fb_open(struct fb_info *info, int user)
 +{
 +	struct udl_fbdev *ufbdev = info->par;
 +	struct drm_device *dev = ufbdev->ufb.base.dev;
 +	struct udl_device *udl = to_udl(dev);
 +
 +	/* If the USB device is gone, we don't accept new opens */
 +	if (drm_dev_is_unplugged(&udl->drm))
 +		return -ENODEV;
 +
 +	ufbdev->fb_count++;
 +
 +#ifdef CONFIG_DRM_FBDEV_EMULATION
 +	if (fb_defio && (info->fbdefio == NULL)) {
 +		/* enable defio at last moment if not disabled by client */
 +
 +		struct fb_deferred_io *fbdefio;
 +
 +		fbdefio = kzalloc(sizeof(struct fb_deferred_io), GFP_KERNEL);
 +
 +		if (fbdefio) {
 +			fbdefio->delay = DL_DEFIO_WRITE_DELAY;
 +			fbdefio->deferred_io = drm_fb_helper_deferred_io;
 +		}
 +
 +		info->fbdefio = fbdefio;
 +		fb_deferred_io_init(info);
 +	}
 +#endif
 +
 +	pr_debug("open /dev/fb%d user=%d fb_info=%p count=%d\n",
 +		  info->node, user, info, ufbdev->fb_count);
 +
 +	return 0;
 +}
 +
 +
 +/*
 + * Assumes caller is holding info->lock mutex (for open and release at least)
 + */
 +static int udl_fb_release(struct fb_info *info, int user)
 +{
 +	struct udl_fbdev *ufbdev = info->par;
 +
 +	ufbdev->fb_count--;
 +
 +#ifdef CONFIG_DRM_FBDEV_EMULATION
 +	if ((ufbdev->fb_count == 0) && (info->fbdefio)) {
 +		fb_deferred_io_cleanup(info);
 +		kfree(info->fbdefio);
 +		info->fbdefio = NULL;
 +		info->fbops->fb_mmap = udl_fb_mmap;
 +	}
 +#endif
 +
 +	pr_debug("released /dev/fb%d user=%d count=%d\n",
 +		info->node, user, ufbdev->fb_count);
 +
 +	return 0;
 +}
 +
 +static struct fb_ops udlfb_ops = {
 +	.owner = THIS_MODULE,
 +	DRM_FB_HELPER_DEFAULT_OPS,
 +	.fb_fillrect = drm_fb_helper_sys_fillrect,
 +	.fb_copyarea = drm_fb_helper_sys_copyarea,
 +	.fb_imageblit = drm_fb_helper_sys_imageblit,
 +	.fb_mmap = udl_fb_mmap,
 +	.fb_open = udl_fb_open,
 +	.fb_release = udl_fb_release,
 +};
 +
  static int udl_user_framebuffer_dirty(struct drm_framebuffer *fb,
  				      struct drm_file *file,
  				      unsigned flags, unsigned color,
* Unmerged path drivers/gpu/drm/udl/udl_fb.c
