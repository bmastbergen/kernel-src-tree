io_uring: statx must grab the file table for valid fd

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 5b0bbee4732cbd58aa98213d4c11a366356bba3d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5b0bbee4.failed

Clay reports that OP_STATX fails for a test case with a valid fd
and empty path:

 -- Test 0: statx:fd 3: SUCCEED, file mode 100755
 -- Test 1: statx:path ./uring_statx: SUCCEED, file mode 100755
 -- Test 2: io_uring_statx:fd 3: FAIL, errno 9: Bad file descriptor
 -- Test 3: io_uring_statx:path ./uring_statx: SUCCEED, file mode 100755

This is due to statx not grabbing the process file table, hence we can't
lookup the fd in async context. If the fd is valid, ensure that we grab
the file table so we can grab the file from async context.

	Cc: stable@vger.kernel.org # v5.6
	Reported-by: Clay Harris <bugs@claycon.org>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 5b0bbee4732cbd58aa98213d4c11a366356bba3d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 2afa3b27779e,084dfade5cda..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -308,6 -346,247 +308,250 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_close {
+ 	struct file			*file;
+ 	struct file			*put_file;
+ 	int				fd;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ 	unsigned long			nofile;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ 	int				mode;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	u32				count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	union {
+ 		struct user_msghdr __user *msg;
+ 		void __user		*buf;
+ 	};
+ 	int				msg_flags;
+ 	int				bgid;
+ 	size_t				len;
+ 	struct io_buffer		*kbuf;
+ };
+ 
+ struct io_open {
+ 	struct file			*file;
+ 	int				dfd;
+ 	union {
+ 		unsigned		mask;
+ 	};
+ 	struct filename			*filename;
+ 	struct statx __user		*buffer;
+ 	struct open_how			how;
+ 	unsigned long			nofile;
+ };
+ 
+ struct io_files_update {
+ 	struct file			*file;
+ 	u64				arg;
+ 	u32				nr_args;
+ 	u32				offset;
+ };
+ 
+ struct io_fadvise {
+ 	struct file			*file;
+ 	u64				offset;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_madvise {
+ 	struct file			*file;
+ 	u64				addr;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_epoll {
+ 	struct file			*file;
+ 	int				epfd;
+ 	int				op;
+ 	int				fd;
+ 	struct epoll_event		event;
+ };
+ 
+ struct io_splice {
+ 	struct file			*file_out;
+ 	struct file			*file_in;
+ 	loff_t				off_out;
+ 	loff_t				off_in;
+ 	u64				len;
+ 	unsigned int			flags;
+ };
+ 
+ struct io_provide_buf {
+ 	struct file			*file;
+ 	__u64				addr;
+ 	__s32				len;
+ 	__u32				bgid;
+ 	__u16				nbufs;
+ 	__u16				bid;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ 	struct sockaddr_storage		addr;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
+ };
+ 
+ enum {
+ 	REQ_F_FIXED_FILE_BIT	= IOSQE_FIXED_FILE_BIT,
+ 	REQ_F_IO_DRAIN_BIT	= IOSQE_IO_DRAIN_BIT,
+ 	REQ_F_LINK_BIT		= IOSQE_IO_LINK_BIT,
+ 	REQ_F_HARDLINK_BIT	= IOSQE_IO_HARDLINK_BIT,
+ 	REQ_F_FORCE_ASYNC_BIT	= IOSQE_ASYNC_BIT,
+ 	REQ_F_BUFFER_SELECT_BIT	= IOSQE_BUFFER_SELECT_BIT,
+ 
+ 	REQ_F_LINK_HEAD_BIT,
+ 	REQ_F_LINK_NEXT_BIT,
+ 	REQ_F_FAIL_LINK_BIT,
+ 	REQ_F_INFLIGHT_BIT,
+ 	REQ_F_CUR_POS_BIT,
+ 	REQ_F_NOWAIT_BIT,
+ 	REQ_F_IOPOLL_COMPLETED_BIT,
+ 	REQ_F_LINK_TIMEOUT_BIT,
+ 	REQ_F_TIMEOUT_BIT,
+ 	REQ_F_ISREG_BIT,
+ 	REQ_F_MUST_PUNT_BIT,
+ 	REQ_F_TIMEOUT_NOSEQ_BIT,
+ 	REQ_F_COMP_LOCKED_BIT,
+ 	REQ_F_NEED_CLEANUP_BIT,
+ 	REQ_F_OVERFLOW_BIT,
+ 	REQ_F_POLLED_BIT,
+ 	REQ_F_BUFFER_SELECTED_BIT,
+ 	REQ_F_NO_FILE_TABLE_BIT,
+ 
+ 	/* not a real bit, just to check we're not overflowing the space */
+ 	__REQ_F_LAST_BIT,
+ };
+ 
+ enum {
+ 	/* ctx owns file */
+ 	REQ_F_FIXED_FILE	= BIT(REQ_F_FIXED_FILE_BIT),
+ 	/* drain existing IO first */
+ 	REQ_F_IO_DRAIN		= BIT(REQ_F_IO_DRAIN_BIT),
+ 	/* linked sqes */
+ 	REQ_F_LINK		= BIT(REQ_F_LINK_BIT),
+ 	/* doesn't sever on completion < 0 */
+ 	REQ_F_HARDLINK		= BIT(REQ_F_HARDLINK_BIT),
+ 	/* IOSQE_ASYNC */
+ 	REQ_F_FORCE_ASYNC	= BIT(REQ_F_FORCE_ASYNC_BIT),
+ 	/* IOSQE_BUFFER_SELECT */
+ 	REQ_F_BUFFER_SELECT	= BIT(REQ_F_BUFFER_SELECT_BIT),
+ 
+ 	/* head of a link */
+ 	REQ_F_LINK_HEAD		= BIT(REQ_F_LINK_HEAD_BIT),
+ 	/* already grabbed next link */
+ 	REQ_F_LINK_NEXT		= BIT(REQ_F_LINK_NEXT_BIT),
+ 	/* fail rest of links */
+ 	REQ_F_FAIL_LINK		= BIT(REQ_F_FAIL_LINK_BIT),
+ 	/* on inflight list */
+ 	REQ_F_INFLIGHT		= BIT(REQ_F_INFLIGHT_BIT),
+ 	/* read/write uses file position */
+ 	REQ_F_CUR_POS		= BIT(REQ_F_CUR_POS_BIT),
+ 	/* must not punt to workers */
+ 	REQ_F_NOWAIT		= BIT(REQ_F_NOWAIT_BIT),
+ 	/* polled IO has completed */
+ 	REQ_F_IOPOLL_COMPLETED	= BIT(REQ_F_IOPOLL_COMPLETED_BIT),
+ 	/* has linked timeout */
+ 	REQ_F_LINK_TIMEOUT	= BIT(REQ_F_LINK_TIMEOUT_BIT),
+ 	/* timeout request */
+ 	REQ_F_TIMEOUT		= BIT(REQ_F_TIMEOUT_BIT),
+ 	/* regular file */
+ 	REQ_F_ISREG		= BIT(REQ_F_ISREG_BIT),
+ 	/* must be punted even for NONBLOCK */
+ 	REQ_F_MUST_PUNT		= BIT(REQ_F_MUST_PUNT_BIT),
+ 	/* no timeout sequence */
+ 	REQ_F_TIMEOUT_NOSEQ	= BIT(REQ_F_TIMEOUT_NOSEQ_BIT),
+ 	/* completion under lock */
+ 	REQ_F_COMP_LOCKED	= BIT(REQ_F_COMP_LOCKED_BIT),
+ 	/* needs cleanup */
+ 	REQ_F_NEED_CLEANUP	= BIT(REQ_F_NEED_CLEANUP_BIT),
+ 	/* in overflow list */
+ 	REQ_F_OVERFLOW		= BIT(REQ_F_OVERFLOW_BIT),
+ 	/* already went through poll handler */
+ 	REQ_F_POLLED		= BIT(REQ_F_POLLED_BIT),
+ 	/* buffer already selected */
+ 	REQ_F_BUFFER_SELECTED	= BIT(REQ_F_BUFFER_SELECTED_BIT),
+ 	/* doesn't need file table for this request */
+ 	REQ_F_NO_FILE_TABLE	= BIT(REQ_F_NO_FILE_TABLE_BIT),
+ };
+ 
+ struct async_poll {
+ 	struct io_poll_iocb	poll;
+ 	struct io_wq_work	work;
+ };
+ 
++>>>>>>> 5b0bbee4732c (io_uring: statx must grab the file table for valid fd)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -366,8 -673,203 +610,208 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ 	/* needs ->fs */
+ 	unsigned		needs_fs : 1;
+ 	/* set if opcode supports polled "wait" */
+ 	unsigned		pollin : 1;
+ 	unsigned		pollout : 1;
+ 	/* op supports buffer selection */
+ 	unsigned		buffer_select : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.needs_fs		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_SPLICE] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_PROVIDE_BUFFERS] = {},
+ 	[IORING_OP_REMOVE_BUFFERS] = {},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_cleanup_req(struct io_kiocb *req);
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 		       int fd, struct file **out_file, bool fixed);
+ static void __io_queue_sqe(struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe);
++>>>>>>> 5b0bbee4732c (io_uring: statx must grab the file table for valid fd)
  
  static struct kmem_cache *req_cachep;
  
@@@ -1510,38 -2830,648 +1954,473 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
 -static bool io_req_cancelled(struct io_kiocb *req)
 -{
 -	if (req->work.flags & IO_WQ_WORK_CANCEL) {
 -		req_set_fail_links(req);
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_put_req(req);
 -		return true;
 -	}
 -
 -	return false;
 -}
 -
 -static void __io_fsync(struct io_kiocb *req)
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		    bool force_nonblock)
  {
 -	loff_t end = req->sync.off + req->sync.len;
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
  	int ret;
  
 -	ret = vfs_fsync_range(req->file, req->sync.off,
 -				end > 0 ? end : LLONG_MAX,
 -				req->sync.flags & IORING_FSYNC_DATASYNC);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -}
 -
 -static void io_fsync_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
  
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_fsync(req);
 -	io_steal_work(req, workptr);
 -}
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
  
 -static int io_fsync(struct io_kiocb *req, bool force_nonblock)
 -{
  	/* fsync always requires a blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_fsync_finish;
 +	if (force_nonblock)
  		return -EAGAIN;
 -	}
 -	__io_fsync(req);
 -	return 0;
 -}
  
 -static void __io_fallocate(struct io_kiocb *req)
 -{
 -	int ret;
 +	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
 +				end > 0 ? end : LLONG_MAX,
 +				fsync_flags & IORING_FSYNC_DATASYNC);
  
 -	current->signal->rlim[RLIMIT_FSIZE].rlim_cur = req->fsize;
 -	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
 -				req->sync.len);
 -	current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
  	io_put_req(req);
 +	return 0;
  }
  
 -static void io_fallocate_finish(struct io_wq_work **workptr)
++<<<<<<< HEAD
++=======
++static int io_openat(struct io_kiocb *req, bool force_nonblock)
+ {
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_fallocate(req);
 -	io_steal_work(req, workptr);
++	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
++	return io_openat2(req, force_nonblock);
+ }
+ 
 -static int io_fallocate_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
++static int io_remove_buffers_prep(struct io_kiocb *req,
++				  const struct io_uring_sqe *sqe)
+ {
 -	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
++	struct io_provide_buf *p = &req->pbuf;
++	u64 tmp;
+ 
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->addr);
 -	req->sync.mode = READ_ONCE(sqe->len);
 -	req->fsize = rlimit(RLIMIT_FSIZE);
 -	return 0;
 -}
++	if (sqe->ioprio || sqe->rw_flags || sqe->addr || sqe->len || sqe->off)
++		return -EINVAL;
+ 
 -static int io_fallocate(struct io_kiocb *req, bool force_nonblock)
 -{
 -	/* fallocate always requiring blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_fallocate_finish;
 -		return -EAGAIN;
 -	}
++	tmp = READ_ONCE(sqe->fd);
++	if (!tmp || tmp > USHRT_MAX)
++		return -EINVAL;
+ 
 -	__io_fallocate(req);
++	memset(p, 0, sizeof(*p));
++	p->nbufs = tmp;
++	p->bgid = READ_ONCE(sqe->buf_group);
+ 	return 0;
+ }
+ 
 -static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++static int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,
++			       int bgid, unsigned nbufs)
+ {
 -	const char __user *fname;
 -	int ret;
++	unsigned i = 0;
+ 
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (req->flags & REQ_F_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
++	/* shouldn't happen */
++	if (!nbufs)
+ 		return 0;
+ 
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	req->open.how.mode = READ_ONCE(sqe->len);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	req->open.how.flags = READ_ONCE(sqe->open_flags);
 -	if (force_o_largefile())
 -		req->open.how.flags |= O_LARGEFILE;
++	/* the head kbuf is the list itself */
++	while (!list_empty(&buf->list)) {
++		struct io_buffer *nxt;
+ 
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
++		nxt = list_first_entry(&buf->list, struct io_buffer, list);
++		list_del(&nxt->list);
++		kfree(nxt);
++		if (++i == nbufs)
++			return i;
+ 	}
++	i++;
++	kfree(buf);
++	idr_remove(&ctx->io_buffer_idr, bgid);
+ 
 -	req->open.nofile = rlimit(RLIMIT_NOFILE);
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	return 0;
++	return i;
+ }
+ 
 -static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++static int io_remove_buffers(struct io_kiocb *req, bool force_nonblock)
+ {
 -	struct open_how __user *how;
 -	const char __user *fname;
 -	size_t len;
 -	int ret;
 -
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (req->flags & REQ_F_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	len = READ_ONCE(sqe->len);
 -
 -	if (len < OPEN_HOW_SIZE_VER0)
 -		return -EINVAL;
 -
 -	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
 -					len);
 -	if (ret)
 -		return ret;
 -
 -	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
 -		req->open.how.flags |= O_LARGEFILE;
 -
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 -	}
 -
 -	req->open.nofile = rlimit(RLIMIT_NOFILE);
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	return 0;
 -}
 -
 -static int io_openat2(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct open_flags op;
 -	struct file *file;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 -
 -	ret = build_open_flags(&req->open.how, &op);
 -	if (ret)
 -		goto err;
 -
 -	ret = __get_unused_fd_flags(req->open.how.flags, req->open.nofile);
 -	if (ret < 0)
 -		goto err;
 -
 -	file = do_filp_open(req->open.dfd, req->open.filename, &op);
 -	if (IS_ERR(file)) {
 -		put_unused_fd(ret);
 -		ret = PTR_ERR(file);
 -	} else {
 -		fsnotify_open(file);
 -		fd_install(ret, file);
 -	}
 -err:
 -	putname(req->open.filename);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_openat(struct io_kiocb *req, bool force_nonblock)
 -{
 -	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
 -	return io_openat2(req, force_nonblock);
 -}
 -
 -static int io_remove_buffers_prep(struct io_kiocb *req,
 -				  const struct io_uring_sqe *sqe)
 -{
 -	struct io_provide_buf *p = &req->pbuf;
 -	u64 tmp;
 -
 -	if (sqe->ioprio || sqe->rw_flags || sqe->addr || sqe->len || sqe->off)
 -		return -EINVAL;
 -
 -	tmp = READ_ONCE(sqe->fd);
 -	if (!tmp || tmp > USHRT_MAX)
 -		return -EINVAL;
 -
 -	memset(p, 0, sizeof(*p));
 -	p->nbufs = tmp;
 -	p->bgid = READ_ONCE(sqe->buf_group);
 -	return 0;
 -}
 -
 -static int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,
 -			       int bgid, unsigned nbufs)
 -{
 -	unsigned i = 0;
 -
 -	/* shouldn't happen */
 -	if (!nbufs)
 -		return 0;
 -
 -	/* the head kbuf is the list itself */
 -	while (!list_empty(&buf->list)) {
 -		struct io_buffer *nxt;
 -
 -		nxt = list_first_entry(&buf->list, struct io_buffer, list);
 -		list_del(&nxt->list);
 -		kfree(nxt);
 -		if (++i == nbufs)
 -			return i;
 -	}
 -	i++;
 -	kfree(buf);
 -	idr_remove(&ctx->io_buffer_idr, bgid);
 -
 -	return i;
 -}
 -
 -static int io_remove_buffers(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct io_provide_buf *p = &req->pbuf;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_buffer *head;
 -	int ret = 0;
++	struct io_provide_buf *p = &req->pbuf;
++	struct io_ring_ctx *ctx = req->ctx;
++	struct io_buffer *head;
++	int ret = 0;
+ 
+ 	io_ring_submit_lock(ctx, !force_nonblock);
+ 
+ 	lockdep_assert_held(&ctx->uring_lock);
+ 
+ 	ret = -ENOENT;
+ 	head = idr_find(&ctx->io_buffer_idr, p->bgid);
+ 	if (head)
+ 		ret = __io_remove_buffers(ctx, head, p->bgid, p->nbufs);
+ 
+ 	io_ring_submit_lock(ctx, !force_nonblock);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_provide_buffers_prep(struct io_kiocb *req,
+ 				   const struct io_uring_sqe *sqe)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	u64 tmp;
+ 
+ 	if (sqe->ioprio || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	tmp = READ_ONCE(sqe->fd);
+ 	if (!tmp || tmp > USHRT_MAX)
+ 		return -E2BIG;
+ 	p->nbufs = tmp;
+ 	p->addr = READ_ONCE(sqe->addr);
+ 	p->len = READ_ONCE(sqe->len);
+ 
+ 	if (!access_ok(u64_to_user_ptr(p->addr), p->len))
+ 		return -EFAULT;
+ 
+ 	p->bgid = READ_ONCE(sqe->buf_group);
+ 	tmp = READ_ONCE(sqe->off);
+ 	if (tmp > USHRT_MAX)
+ 		return -E2BIG;
+ 	p->bid = tmp;
+ 	return 0;
+ }
+ 
+ static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)
+ {
+ 	struct io_buffer *buf;
+ 	u64 addr = pbuf->addr;
+ 	int i, bid = pbuf->bid;
+ 
+ 	for (i = 0; i < pbuf->nbufs; i++) {
+ 		buf = kmalloc(sizeof(*buf), GFP_KERNEL);
+ 		if (!buf)
+ 			break;
+ 
+ 		buf->addr = addr;
+ 		buf->len = pbuf->len;
+ 		buf->bid = bid;
+ 		addr += pbuf->len;
+ 		bid++;
+ 		if (!*head) {
+ 			INIT_LIST_HEAD(&buf->list);
+ 			*head = buf;
+ 		} else {
+ 			list_add_tail(&buf->list, &(*head)->list);
+ 		}
+ 	}
+ 
+ 	return i ? i : -ENOMEM;
+ }
+ 
+ static int io_provide_buffers(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_buffer *head, *list;
+ 	int ret = 0;
+ 
+ 	io_ring_submit_lock(ctx, !force_nonblock);
+ 
+ 	lockdep_assert_held(&ctx->uring_lock);
+ 
+ 	list = head = idr_find(&ctx->io_buffer_idr, p->bgid);
+ 
+ 	ret = io_add_buffers(p, &head);
+ 	if (ret < 0)
+ 		goto out;
+ 
+ 	if (!list) {
+ 		ret = idr_alloc(&ctx->io_buffer_idr, head, p->bgid, p->bgid + 1,
+ 					GFP_KERNEL);
+ 		if (ret < 0) {
+ 			__io_remove_buffers(ctx, head, p->bgid, -1U);
+ 			goto out;
+ 		}
+ 	}
+ out:
+ 	io_ring_submit_unlock(ctx, !force_nonblock);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_epoll_ctl_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	req->epoll.epfd = READ_ONCE(sqe->fd);
+ 	req->epoll.op = READ_ONCE(sqe->len);
+ 	req->epoll.fd = READ_ONCE(sqe->off);
+ 
+ 	if (ep_op_has_event(req->epoll.op)) {
+ 		struct epoll_event __user *ev;
+ 
+ 		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
+ 			return -EFAULT;
+ 	}
+ 
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_epoll_ctl(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	struct io_epoll *ie = &req->epoll;
+ 	int ret;
+ 
+ 	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
+ 	if (force_nonblock && ret == -EAGAIN)
+ 		return -EAGAIN;
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	if (sqe->ioprio || sqe->buf_index || sqe->off)
+ 		return -EINVAL;
+ 
+ 	req->madvise.addr = READ_ONCE(sqe->addr);
+ 	req->madvise.len = READ_ONCE(sqe->len);
+ 	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	struct io_madvise *ma = &req->madvise;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	ret = do_madvise(ma->addr, ma->len, ma->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->addr)
+ 		return -EINVAL;
+ 
+ 	req->fadvise.offset = READ_ONCE(sqe->off);
+ 	req->fadvise.len = READ_ONCE(sqe->len);
+ 	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ }
+ 
+ static int io_fadvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_fadvise *fa = &req->fadvise;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		switch (fa->advice) {
+ 		case POSIX_FADV_NORMAL:
+ 		case POSIX_FADV_RANDOM:
+ 		case POSIX_FADV_SEQUENTIAL:
+ 			break;
+ 		default:
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	unsigned lookup_flags;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (req->flags & REQ_F_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.mask = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	req->open.how.flags = READ_ONCE(sqe->statx_flags);
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
+ 		return -EINVAL;
+ 
+ 	req->open.filename = getname_flags(fname, lookup_flags, NULL);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_statx(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_open *ctx = &req->open;
+ 	unsigned lookup_flags;
+ 	struct path path;
+ 	struct kstat stat;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		/* only need file table for an actual valid fd */
+ 		if (ctx->dfd == -1 || ctx->dfd == AT_FDCWD)
+ 			req->flags |= REQ_F_NO_FILE_TABLE;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
+ 		return -EINVAL;
+ 
+ retry:
+ 	/* filename_lookup() drops it, keep a reference */
+ 	ctx->filename->refcnt++;
+ 
+ 	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
+ 				NULL);
+ 	if (ret)
+ 		goto err;
+ 
+ 	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
+ 	path_put(&path);
+ 	if (retry_estale(ret, lookup_flags)) {
+ 		lookup_flags |= LOOKUP_REVAL;
+ 		goto retry;
+ 	}
+ 	if (!ret)
+ 		ret = cp_statx(&stat, ctx->buffer);
+ err:
+ 	putname(ctx->filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * If we queue this for async, it must not be cancellable. That would
+ 	 * leave the 'file' in an undeterminate state.
+ 	 */
+ 	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
+ 
+ 	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
+ 	    sqe->rw_flags || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (req->flags & REQ_F_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->close.fd = READ_ONCE(sqe->fd);
+ 	if (req->file->f_op == &io_uring_fops ||
+ 	    req->close.fd == req->ctx->ring_fd)
+ 		return -EBADF;
+ 
+ 	return 0;
+ }
+ 
+ /* only called when __close_fd_get_file() is done */
+ static void __io_close_finish(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = filp_close(req->close.put_file, req->work.files);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	fput(req->close.put_file);
+ 	io_put_req(req);
+ }
+ 
+ static void io_close_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 
+ 	/* not cancellable, don't do io_req_cancelled() */
+ 	__io_close_finish(req);
+ 	io_steal_work(req, workptr);
+ }
+ 
+ static int io_close(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	int ret;
+ 
+ 	req->close.put_file = NULL;
+ 	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* if the file has a flush method, be safe and punt to async */
+ 	if (req->close.put_file->f_op->flush && force_nonblock) {
+ 		/* submission ref will be dropped, take it for async */
+ 		refcount_inc(&req->refs);
+ 
+ 		req->work.func = io_close_finish;
+ 		/*
+ 		 * Do manual async queue here to avoid grabbing files - we don't
+ 		 * need the files, and it'll cause io_close_finish() to close
+ 		 * the file again and cause a double CQE entry for this request
+ 		 */
+ 		io_queue_async_work(req);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * No ->flush(), safely close from here and just punt the
+ 	 * fput() to async context.
+ 	 */
+ 	__io_close_finish(req);
+ 	return 0;
+ }
+ 
++>>>>>>> 5b0bbee4732c (io_uring: statx must grab the file table for valid fd)
  static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
  	struct io_ring_ctx *ctx = req->ctx;
@@@ -2189,143 -5416,249 +3068,169 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static int io_req_set_file(struct io_submit_state *state, struct io_kiocb *req,
 -			   int fd, unsigned int flags)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	bool fixed;
 +	int ret;
  
 -	if (!io_req_needs_file(req, fd))
 -		return 0;
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
 +		struct io_uring_sqe *sqe_copy;
 +
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (sqe_copy) {
 +			struct async_list *list;
 +
 +			s->sqe = sqe_copy;
 +			memcpy(&req->submit, s, sizeof(*s));
 +			list = io_async_list_from_sqe(ctx, s->sqe);
 +			if (!io_add_to_prev_work(list, req)) {
 +				if (list)
 +					atomic_inc(&list->cnt);
 +				INIT_WORK(&req->work, io_sq_wq_submit_work);
 +				io_queue_async_work(ctx, req);
 +			}
  
 -	fixed = (flags & IOSQE_FIXED_FILE);
 -	if (unlikely(!fixed && req->needs_fixed_file))
 -		return -EBADF;
++<<<<<<< HEAD
 +			/*
 +			 * Queued up for async execution, worker will release
 +			 * submit reference when the iocb is actually submitted.
 +			 */
 +			return 0;
 +		}
 +	}
  
 -	return io_file_get(state, req, fd, &req->file, fixed);
 -}
 +	/* drop submission reference */
 +	io_put_req(req);
  
 +	/* and drop final reference, if we failed */
 +	if (ret) {
 +		io_cqring_add_event(ctx, req->user_data, ret);
 +		if (req->flags & REQ_F_LINK)
 +			req->flags |= REQ_F_FAIL_LINK;
 +		io_put_req(req);
++=======
+ static int io_grab_files(struct io_kiocb *req)
+ {
+ 	int ret = -EBADF;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	if (req->work.files || (req->flags & REQ_F_NO_FILE_TABLE))
+ 		return 0;
+ 	if (!ctx->ring_file)
+ 		return -EBADF;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irq(&ctx->inflight_lock);
+ 	/*
+ 	 * We use the f_ops->flush() handler to ensure that we can flush
+ 	 * out work accessing these files if the fd is closed. Check if
+ 	 * the fd has changed since we started down this path, and disallow
+ 	 * this operation if it has.
+ 	 */
+ 	if (fcheck(ctx->ring_fd) == ctx->ring_file) {
+ 		list_add(&req->inflight_entry, &ctx->inflight_list);
 -		req->flags |= REQ_F_INFLIGHT;
 -		req->work.files = current->files;
 -		ret = 0;
 -	}
 -	spin_unlock_irq(&ctx->inflight_lock);
 -	rcu_read_unlock();
 -
 -	return ret;
 -}
 -
 -static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
 -{
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_kiocb *prev = NULL;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -
 -	/*
 -	 * We don't expect the list to be empty, that will only happen if we
 -	 * race with the completion of the linked work.
 -	 */
 -	if (!list_empty(&req->link_list)) {
 -		prev = list_entry(req->link_list.prev, struct io_kiocb,
 -				  link_list);
 -		if (refcount_inc_not_zero(&prev->refs)) {
 -			list_del_init(&req->link_list);
 -			prev->flags &= ~REQ_F_LINK_TIMEOUT;
 -		} else
 -			prev = NULL;
 -	}
 -
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -
 -	if (prev) {
 -		req_set_fail_links(prev);
 -		io_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);
 -		io_put_req(prev);
 -	} else {
 -		io_cqring_add_event(req, -ETIME);
 -		io_put_req(req);
 -	}
 -	return HRTIMER_NORESTART;
 -}
 -
 -static void io_queue_linked_timeout(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	/*
 -	 * If the list is now empty, then our linked request finished before
 -	 * we got a chance to setup the timer
 -	 */
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (!list_empty(&req->link_list)) {
 -		struct io_timeout_data *data = &req->io->timeout;
 -
 -		data->timer.function = io_link_timeout_fn;
 -		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
 -				data->mode);
++		req->flags |= REQ_F_INFLIGHT;
++		req->work.files = current->files;
++		ret = 0;
++>>>>>>> 5b0bbee4732c (io_uring: statx must grab the file table for valid fd)
  	}
 -	spin_unlock_irq(&ctx->completion_lock);
  
 -	/* drop submission reference */
 -	io_put_req(req);
 +	return ret;
  }
  
 -static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_kiocb *nxt;
 -
 -	if (!(req->flags & REQ_F_LINK_HEAD))
 -		return NULL;
 -	/* for polled retry, if flag is set, we already went through here */
 -	if (req->flags & REQ_F_POLLED)
 -		return NULL;
 +	int ret;
  
 -	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb,
 -					link_list);
 -	if (!nxt || nxt->opcode != IORING_OP_LINK_TIMEOUT)
 -		return NULL;
 +	ret = io_req_defer(ctx, req, s->sqe);
 +	if (ret) {
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		}
 +		return 0;
 +	}
  
 -	req->flags |= REQ_F_LINK_TIMEOUT;
 -	return nxt;
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
  }
  
 -static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
  {
 -	struct io_kiocb *linked_timeout;
 -	struct io_kiocb *nxt;
 -	const struct cred *old_creds = NULL;
  	int ret;
 +	int need_submit = false;
  
 -again:
 -	linked_timeout = io_prep_linked_timeout(req);
 -
 -	if (req->work.creds && req->work.creds != current_cred()) {
 -		if (old_creds)
 -			revert_creds(old_creds);
 -		if (old_creds == req->work.creds)
 -			old_creds = NULL; /* restored original creds */
 -		else
 -			old_creds = override_creds(req->work.creds);
 -	}
 -
 -	ret = io_issue_sqe(req, sqe, true);
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
  
  	/*
 -	 * We async punt it if the file wasn't marked NOWAIT, or if the file
 -	 * doesn't support non-blocking read/write attempts
 +	 * Mark the first IO in link list as DRAIN, let all the following
 +	 * IOs enter the defer list. all IO needs to be completed before link
 +	 * list.
  	 */
 -	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
 -	    (req->flags & REQ_F_MUST_PUNT))) {
 -		if (io_arm_poll_handler(req)) {
 -			if (linked_timeout)
 -				io_queue_linked_timeout(linked_timeout);
 -			goto exit;
 -		}
 -punt:
 -		if (io_op_defs[req->opcode].file_table) {
 -			ret = io_grab_files(req);
 -			if (ret)
 -				goto err;
 +	req->flags |= REQ_F_IO_DRAIN;
 +	ret = io_req_defer(ctx, req, s->sqe);
 +	if (ret) {
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
  		}
 -
 +	} else {
  		/*
 -		 * Queued up for async execution, worker will release
 -		 * submit reference when the iocb is actually submitted.
 +		 * If ret == 0 means that all IOs in front of link io are
 +		 * running done. let's queue link head.
  		 */
 -		io_queue_async_work(req);
 -		goto exit;
 +		need_submit = true;
  	}
  
 -err:
 -	nxt = NULL;
 -	/* drop submission reference */
 -	io_put_req_find_next(req, &nxt);
 -
 -	if (linked_timeout) {
 -		if (!ret)
 -			io_queue_linked_timeout(linked_timeout);
 -		else
 -			io_put_req(linked_timeout);
 -	}
 +	/* Insert shadow req to defer_list, blocking next IOs */
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_add_tail(&shadow->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
  
 -	/* and drop final reference, if we failed */
 -	if (ret) {
 -		io_cqring_add_event(req, ret);
 -		req_set_fail_links(req);
 -		io_put_req(req);
 -	}
 -	if (nxt) {
 -		req = nxt;
 +	if (need_submit)
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
  
 -		if (req->flags & REQ_F_FORCE_ASYNC)
 -			goto punt;
 -		goto again;
 -	}
 -exit:
 -	if (old_creds)
 -		revert_creds(old_creds);
 +	return 0;
  }
  
 -static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
 +
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
  {
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
  	int ret;
  
 -	ret = io_req_defer(req, sqe);
 -	if (ret) {
 -		if (ret != -EIOCBQUEUED) {
 -fail_req:
 -			io_cqring_add_event(req, ret);
 -			req_set_fail_links(req);
 -			io_double_put_req(req);
 -		}
 -	} else if (req->flags & REQ_F_FORCE_ASYNC) {
 -		ret = io_req_defer_prep(req, sqe);
 -		if (unlikely(ret < 0))
 -			goto fail_req;
 -		/*
 -		 * Never try inline submit of IOSQE_ASYNC is set, go straight
 -		 * to async execution.
 -		 */
 -		req->work.flags |= IO_WQ_WORK_CONCURRENT;
 -		io_queue_async_work(req);
 -	} else {
 -		__io_queue_sqe(req, sqe);
 +	/* enforce forwards compatibility on users */
 +	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
 +		ret = -EINVAL;
 +		goto err;
  	}
 -}
  
 -static inline void io_queue_link_head(struct io_kiocb *req)
 -{
 -	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_double_put_req(req);
 -	} else
 -		io_queue_sqe(req, NULL);
 -}
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
 +	}
  
 -static int io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			  struct io_submit_state *state, struct io_kiocb **link)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 +	ret = io_req_set_file(ctx, s, state, req);
 +	if (unlikely(ret)) {
 +err_req:
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		return;
 +	}
 +
 +	req->user_data = s->sqe->user_data;
  
  	/*
  	 * If we already have a head request, queue this one for async
* Unmerged path fs/io_uring.c
