net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [net] rename flow_action_hw_stats_types* -> flow_action_hw_stats* (Ivan Vecera) [1824071]
Rebuild_FUZZ: 95.93%
commit-author Jakub Kicinski <kuba@kernel.org>
commit 53eca1f3479f355ec17b2e86a6b0680510292833
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/53eca1f3.failed

flow_action_hw_stats_types_check() helper takes one of the
FLOW_ACTION_HW_STATS_*_BIT values as input. If we align
the arguments to the opening bracket of the helper there
is no way to call this helper and stay under 80 characters.

Remove the "types" part from the new flow_action helpers
and enum values.

	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 53eca1f3479f355ec17b2e86a6b0680510292833)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/broadcom/bnxt/bnxt_tc.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_tc_flower.c
#	drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_flower.c
#	drivers/net/ethernet/mscc/ocelot_flower.c
#	drivers/net/ethernet/netronome/nfp/flower/action.c
#	drivers/net/ethernet/qlogic/qede/qede_filter.c
#	drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c
#	drivers/net/ethernet/stmicro/stmmac/stmmac_tc.c
#	include/net/flow_offload.h
#	net/dsa/slave.c
diff --cc drivers/net/ethernet/broadcom/bnxt/bnxt_tc.c
index 51891cbb4dc4,b19be7549aad..000000000000
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt_tc.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt_tc.c
@@@ -124,6 -300,9 +124,12 @@@ static int bnxt_tc_parse_actions(struc
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (!flow_action_basic_hw_stats_check(flow_action, extack))
+ 		return -EOPNOTSUPP;
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  	flow_action_for_each(i, act, flow_action) {
  		switch (act->id) {
  		case FLOW_ACTION_DROP:
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_tc_flower.c
index e447976bdd3e,b457f2505f97..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_tc_flower.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_tc_flower.c
@@@ -554,7 -553,10 +554,14 @@@ static int cxgb4_validate_flow_actions(
  	bool act_vlan = false;
  	int i;
  
++<<<<<<< HEAD
 +	flow_action_for_each(i, act, &rule->action) {
++=======
+ 	if (!flow_action_basic_hw_stats_check(actions, extack))
+ 		return -EOPNOTSUPP;
+ 
+ 	flow_action_for_each(i, act, actions) {
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  		switch (act->id) {
  		case FLOW_ACTION_ACCEPT:
  		case FLOW_ACTION_DROP:
diff --cc drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
index 8581d5b17dd5,8972cdd559e8..000000000000
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
@@@ -107,7 -1074,398 +107,402 @@@ void mvpp2_cls_oversize_rxq_set(struct 
  	mvpp2_write(port->priv, MVPP2_CLS_SWFWD_PCTRL_REG, val);
  }
  
++<<<<<<< HEAD
 +void mvpp22_init_rss(struct mvpp2_port *port)
++=======
+ static int mvpp2_port_c2_tcam_rule_add(struct mvpp2_port *port,
+ 				       struct mvpp2_rfs_rule *rule)
+ {
+ 	struct flow_action_entry *act;
+ 	struct mvpp2_cls_c2_entry c2;
+ 	u8 qh, ql, pmap;
+ 	int index, ctx;
+ 
+ 	if (!flow_action_basic_hw_stats_check(&rule->flow->action, NULL))
+ 		return -EOPNOTSUPP;
+ 
+ 	memset(&c2, 0, sizeof(c2));
+ 
+ 	index = mvpp2_cls_c2_port_flow_index(port, rule->loc);
+ 	if (index < 0)
+ 		return -EINVAL;
+ 	c2.index = index;
+ 
+ 	act = &rule->flow->action.entries[0];
+ 
+ 	rule->c2_index = c2.index;
+ 
+ 	c2.tcam[3] = (rule->c2_tcam & 0xffff) |
+ 		     ((rule->c2_tcam_mask & 0xffff) << 16);
+ 	c2.tcam[2] = ((rule->c2_tcam >> 16) & 0xffff) |
+ 		     (((rule->c2_tcam_mask >> 16) & 0xffff) << 16);
+ 	c2.tcam[1] = ((rule->c2_tcam >> 32) & 0xffff) |
+ 		     (((rule->c2_tcam_mask >> 32) & 0xffff) << 16);
+ 	c2.tcam[0] = ((rule->c2_tcam >> 48) & 0xffff) |
+ 		     (((rule->c2_tcam_mask >> 48) & 0xffff) << 16);
+ 
+ 	pmap = BIT(port->id);
+ 	c2.tcam[4] = MVPP22_CLS_C2_PORT_ID(pmap);
+ 	c2.tcam[4] |= MVPP22_CLS_C2_TCAM_EN(MVPP22_CLS_C2_PORT_ID(pmap));
+ 
+ 	/* Match on Lookup Type */
+ 	c2.tcam[4] |= MVPP22_CLS_C2_TCAM_EN(MVPP22_CLS_C2_LU_TYPE(MVPP2_CLS_LU_TYPE_MASK));
+ 	c2.tcam[4] |= MVPP22_CLS_C2_LU_TYPE(rule->loc);
+ 
+ 	if (act->id == FLOW_ACTION_DROP) {
+ 		c2.act = MVPP22_CLS_C2_ACT_COLOR(MVPP22_C2_COL_RED_LOCK);
+ 	} else {
+ 		/* We want to keep the default color derived from the Header
+ 		 * Parser drop entries, for VLAN and MAC filtering. This will
+ 		 * assign a default color of Green or Red, and we want matches
+ 		 * with a non-drop action to keep that color.
+ 		 */
+ 		c2.act = MVPP22_CLS_C2_ACT_COLOR(MVPP22_C2_COL_NO_UPD_LOCK);
+ 
+ 		/* Update RSS status after matching this entry */
+ 		if (act->queue.ctx)
+ 			c2.attr[2] |= MVPP22_CLS_C2_ATTR2_RSS_EN;
+ 
+ 		/* Always lock the RSS_EN decision. We might have high prio
+ 		 * rules steering to an RXQ, and a lower one steering to RSS,
+ 		 * we don't want the low prio RSS rule overwriting this flag.
+ 		 */
+ 		c2.act = MVPP22_CLS_C2_ACT_RSS_EN(MVPP22_C2_UPD_LOCK);
+ 
+ 		/* Mark packet as "forwarded to software", needed for RSS */
+ 		c2.act |= MVPP22_CLS_C2_ACT_FWD(MVPP22_C2_FWD_SW_LOCK);
+ 
+ 		c2.act |= MVPP22_CLS_C2_ACT_QHIGH(MVPP22_C2_UPD_LOCK) |
+ 			   MVPP22_CLS_C2_ACT_QLOW(MVPP22_C2_UPD_LOCK);
+ 
+ 		if (act->queue.ctx) {
+ 			/* Get the global ctx number */
+ 			ctx = mvpp22_rss_ctx(port, act->queue.ctx);
+ 			if (ctx < 0)
+ 				return -EINVAL;
+ 
+ 			qh = (ctx >> 3) & MVPP22_CLS_C2_ATTR0_QHIGH_MASK;
+ 			ql = ctx & MVPP22_CLS_C2_ATTR0_QLOW_MASK;
+ 		} else {
+ 			qh = ((act->queue.index + port->first_rxq) >> 3) &
+ 			      MVPP22_CLS_C2_ATTR0_QHIGH_MASK;
+ 			ql = (act->queue.index + port->first_rxq) &
+ 			      MVPP22_CLS_C2_ATTR0_QLOW_MASK;
+ 		}
+ 
+ 		c2.attr[0] = MVPP22_CLS_C2_ATTR0_QHIGH(qh) |
+ 			      MVPP22_CLS_C2_ATTR0_QLOW(ql);
+ 	}
+ 
+ 	c2.valid = true;
+ 
+ 	mvpp2_cls_c2_write(port->priv, &c2);
+ 
+ 	return 0;
+ }
+ 
+ static int mvpp2_port_c2_rfs_rule_insert(struct mvpp2_port *port,
+ 					 struct mvpp2_rfs_rule *rule)
+ {
+ 	return mvpp2_port_c2_tcam_rule_add(port, rule);
+ }
+ 
+ static int mvpp2_port_cls_rfs_rule_remove(struct mvpp2_port *port,
+ 					  struct mvpp2_rfs_rule *rule)
+ {
+ 	const struct mvpp2_cls_flow *flow;
+ 	struct mvpp2_cls_flow_entry fe;
+ 	int index, i;
+ 
+ 	for_each_cls_flow_id_containing_type(i, rule->flow_type) {
+ 		flow = mvpp2_cls_flow_get(i);
+ 		if (!flow)
+ 			return 0;
+ 
+ 		index = MVPP2_CLS_FLT_C2_RFS(port->id, flow->flow_id, rule->loc);
+ 
+ 		mvpp2_cls_flow_read(port->priv, index, &fe);
+ 		mvpp2_cls_flow_port_remove(&fe, BIT(port->id));
+ 		mvpp2_cls_flow_write(port->priv, &fe);
+ 	}
+ 
+ 	if (rule->c2_index >= 0)
+ 		mvpp22_port_c2_lookup_disable(port, rule->c2_index);
+ 
+ 	return 0;
+ }
+ 
+ static int mvpp2_port_flt_rfs_rule_insert(struct mvpp2_port *port,
+ 					  struct mvpp2_rfs_rule *rule)
+ {
+ 	const struct mvpp2_cls_flow *flow;
+ 	struct mvpp2 *priv = port->priv;
+ 	struct mvpp2_cls_flow_entry fe;
+ 	int index, ret, i;
+ 
+ 	if (rule->engine != MVPP22_CLS_ENGINE_C2)
+ 		return -EOPNOTSUPP;
+ 
+ 	ret = mvpp2_port_c2_rfs_rule_insert(port, rule);
+ 	if (ret)
+ 		return ret;
+ 
+ 	for_each_cls_flow_id_containing_type(i, rule->flow_type) {
+ 		flow = mvpp2_cls_flow_get(i);
+ 		if (!flow)
+ 			return 0;
+ 
+ 		if ((rule->hek_fields & flow->supported_hash_opts) != rule->hek_fields)
+ 			continue;
+ 
+ 		index = MVPP2_CLS_FLT_C2_RFS(port->id, flow->flow_id, rule->loc);
+ 
+ 		mvpp2_cls_flow_read(priv, index, &fe);
+ 		mvpp2_cls_flow_eng_set(&fe, rule->engine);
+ 		mvpp2_cls_flow_port_id_sel(&fe, true);
+ 		mvpp2_flow_set_hek_fields(&fe, rule->hek_fields);
+ 		mvpp2_cls_flow_lu_type_set(&fe, rule->loc);
+ 		mvpp2_cls_flow_port_add(&fe, 0xf);
+ 
+ 		mvpp2_cls_flow_write(priv, &fe);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int mvpp2_cls_c2_build_match(struct mvpp2_rfs_rule *rule)
+ {
+ 	struct flow_rule *flow = rule->flow;
+ 	int offs = 0;
+ 
+ 	/* The order of insertion in C2 tcam must match the order in which
+ 	 * the fields are found in the header
+ 	 */
+ 	if (flow_rule_match_key(flow, FLOW_DISSECTOR_KEY_VLAN)) {
+ 		struct flow_match_vlan match;
+ 
+ 		flow_rule_match_vlan(flow, &match);
+ 		if (match.mask->vlan_id) {
+ 			rule->hek_fields |= MVPP22_CLS_HEK_OPT_VLAN;
+ 
+ 			rule->c2_tcam |= ((u64)match.key->vlan_id) << offs;
+ 			rule->c2_tcam_mask |= ((u64)match.mask->vlan_id) << offs;
+ 
+ 			/* Don't update the offset yet */
+ 		}
+ 
+ 		if (match.mask->vlan_priority) {
+ 			rule->hek_fields |= MVPP22_CLS_HEK_OPT_VLAN_PRI;
+ 
+ 			/* VLAN pri is always at offset 13 relative to the
+ 			 * current offset
+ 			 */
+ 			rule->c2_tcam |= ((u64)match.key->vlan_priority) <<
+ 				(offs + 13);
+ 			rule->c2_tcam_mask |= ((u64)match.mask->vlan_priority) <<
+ 				(offs + 13);
+ 		}
+ 
+ 		if (match.mask->vlan_dei)
+ 			return -EOPNOTSUPP;
+ 
+ 		/* vlan id and prio always seem to take a full 16-bit slot in
+ 		 * the Header Extracted Key.
+ 		 */
+ 		offs += 16;
+ 	}
+ 
+ 	if (flow_rule_match_key(flow, FLOW_DISSECTOR_KEY_PORTS)) {
+ 		struct flow_match_ports match;
+ 
+ 		flow_rule_match_ports(flow, &match);
+ 		if (match.mask->src) {
+ 			rule->hek_fields |= MVPP22_CLS_HEK_OPT_L4SIP;
+ 
+ 			rule->c2_tcam |= ((u64)ntohs(match.key->src)) << offs;
+ 			rule->c2_tcam_mask |= ((u64)ntohs(match.mask->src)) << offs;
+ 			offs += mvpp2_cls_hek_field_size(MVPP22_CLS_HEK_OPT_L4SIP);
+ 		}
+ 
+ 		if (match.mask->dst) {
+ 			rule->hek_fields |= MVPP22_CLS_HEK_OPT_L4DIP;
+ 
+ 			rule->c2_tcam |= ((u64)ntohs(match.key->dst)) << offs;
+ 			rule->c2_tcam_mask |= ((u64)ntohs(match.mask->dst)) << offs;
+ 			offs += mvpp2_cls_hek_field_size(MVPP22_CLS_HEK_OPT_L4DIP);
+ 		}
+ 	}
+ 
+ 	if (hweight16(rule->hek_fields) > MVPP2_FLOW_N_FIELDS)
+ 		return -EOPNOTSUPP;
+ 
+ 	return 0;
+ }
+ 
+ static int mvpp2_cls_rfs_parse_rule(struct mvpp2_rfs_rule *rule)
+ {
+ 	struct flow_rule *flow = rule->flow;
+ 	struct flow_action_entry *act;
+ 
+ 	if (!flow_action_basic_hw_stats_check(&rule->flow->action, NULL))
+ 		return -EOPNOTSUPP;
+ 
+ 	act = &flow->action.entries[0];
+ 	if (act->id != FLOW_ACTION_QUEUE && act->id != FLOW_ACTION_DROP)
+ 		return -EOPNOTSUPP;
+ 
+ 	/* When both an RSS context and an queue index are set, the index
+ 	 * is considered as an offset to be added to the indirection table
+ 	 * entries. We don't support this, so reject this rule.
+ 	 */
+ 	if (act->queue.ctx && act->queue.index)
+ 		return -EOPNOTSUPP;
+ 
+ 	/* For now, only use the C2 engine which has a HEK size limited to 64
+ 	 * bits for TCAM matching.
+ 	 */
+ 	rule->engine = MVPP22_CLS_ENGINE_C2;
+ 
+ 	if (mvpp2_cls_c2_build_match(rule))
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ int mvpp2_ethtool_cls_rule_get(struct mvpp2_port *port,
+ 			       struct ethtool_rxnfc *rxnfc)
+ {
+ 	struct mvpp2_ethtool_fs *efs;
+ 
+ 	if (rxnfc->fs.location >= MVPP2_N_RFS_ENTRIES_PER_FLOW)
+ 		return -EINVAL;
+ 
+ 	efs = port->rfs_rules[rxnfc->fs.location];
+ 	if (!efs)
+ 		return -ENOENT;
+ 
+ 	memcpy(rxnfc, &efs->rxnfc, sizeof(efs->rxnfc));
+ 
+ 	return 0;
+ }
+ 
+ int mvpp2_ethtool_cls_rule_ins(struct mvpp2_port *port,
+ 			       struct ethtool_rxnfc *info)
+ {
+ 	struct ethtool_rx_flow_spec_input input = {};
+ 	struct ethtool_rx_flow_rule *ethtool_rule;
+ 	struct mvpp2_ethtool_fs *efs, *old_efs;
+ 	int ret = 0;
+ 
+ 	if (info->fs.location >= MVPP2_N_RFS_ENTRIES_PER_FLOW)
+ 		return -EINVAL;
+ 
+ 	efs = kzalloc(sizeof(*efs), GFP_KERNEL);
+ 	if (!efs)
+ 		return -ENOMEM;
+ 
+ 	input.fs = &info->fs;
+ 
+ 	/* We need to manually set the rss_ctx, since this info isn't present
+ 	 * in info->fs
+ 	 */
+ 	if (info->fs.flow_type & FLOW_RSS)
+ 		input.rss_ctx = info->rss_context;
+ 
+ 	ethtool_rule = ethtool_rx_flow_rule_create(&input);
+ 	if (IS_ERR(ethtool_rule)) {
+ 		ret = PTR_ERR(ethtool_rule);
+ 		goto clean_rule;
+ 	}
+ 
+ 	efs->rule.flow = ethtool_rule->rule;
+ 	efs->rule.flow_type = mvpp2_cls_ethtool_flow_to_type(info->fs.flow_type);
+ 	if (efs->rule.flow_type < 0) {
+ 		ret = efs->rule.flow_type;
+ 		goto clean_rule;
+ 	}
+ 
+ 	ret = mvpp2_cls_rfs_parse_rule(&efs->rule);
+ 	if (ret)
+ 		goto clean_eth_rule;
+ 
+ 	efs->rule.loc = info->fs.location;
+ 
+ 	/* Replace an already existing rule */
+ 	if (port->rfs_rules[efs->rule.loc]) {
+ 		old_efs = port->rfs_rules[efs->rule.loc];
+ 		ret = mvpp2_port_cls_rfs_rule_remove(port, &old_efs->rule);
+ 		if (ret)
+ 			goto clean_eth_rule;
+ 		kfree(old_efs);
+ 		port->n_rfs_rules--;
+ 	}
+ 
+ 	ret = mvpp2_port_flt_rfs_rule_insert(port, &efs->rule);
+ 	if (ret)
+ 		goto clean_eth_rule;
+ 
+ 	ethtool_rx_flow_rule_destroy(ethtool_rule);
+ 	efs->rule.flow = NULL;
+ 
+ 	memcpy(&efs->rxnfc, info, sizeof(*info));
+ 	port->rfs_rules[efs->rule.loc] = efs;
+ 	port->n_rfs_rules++;
+ 
+ 	return ret;
+ 
+ clean_eth_rule:
+ 	ethtool_rx_flow_rule_destroy(ethtool_rule);
+ clean_rule:
+ 	kfree(efs);
+ 	return ret;
+ }
+ 
+ int mvpp2_ethtool_cls_rule_del(struct mvpp2_port *port,
+ 			       struct ethtool_rxnfc *info)
+ {
+ 	struct mvpp2_ethtool_fs *efs;
+ 	int ret;
+ 
+ 	efs = port->rfs_rules[info->fs.location];
+ 	if (!efs)
+ 		return -EINVAL;
+ 
+ 	/* Remove the rule from the engines. */
+ 	ret = mvpp2_port_cls_rfs_rule_remove(port, &efs->rule);
+ 	if (ret)
+ 		return ret;
+ 
+ 	port->n_rfs_rules--;
+ 	port->rfs_rules[info->fs.location] = NULL;
+ 	kfree(efs);
+ 
+ 	return 0;
+ }
+ 
+ static inline u32 mvpp22_rxfh_indir(struct mvpp2_port *port, u32 rxq)
+ {
+ 	int nrxqs, cpu, cpus = num_possible_cpus();
+ 
+ 	/* Number of RXQs per CPU */
+ 	nrxqs = port->nrxqs / cpus;
+ 
+ 	/* CPU that will handle this rx queue */
+ 	cpu = rxq / nrxqs;
+ 
+ 	if (!cpu_online(cpu))
+ 		return port->first_rxq;
+ 
+ 	/* Indirection to better distribute the paquets on the CPUs when
+ 	 * configuring the RSS queues.
+ 	 */
+ 	return port->first_rxq + ((rxq * nrxqs + rxq / cpus) % port->nrxqs);
+ }
+ 
+ static void mvpp22_rss_fill_table(struct mvpp2_port *port,
+ 				  struct mvpp2_rss_table *table,
+ 				  u32 rss_ctx)
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  {
  	struct mvpp2 *priv = port->priv;
  	int i;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index 123069223147,4a48bcb0a8f6..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -2865,6 -3180,10 +2865,13 @@@ static int parse_tc_nic_actions(struct 
  	if (!flow_action_has_entries(flow_action))
  		return -EINVAL;
  
++<<<<<<< HEAD
++=======
+ 	if (!flow_action_hw_stats_check(flow_action, extack,
+ 					FLOW_ACTION_HW_STATS_DELAYED_BIT))
+ 		return -EOPNOTSUPP;
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  	attr->flow_tag = MLX5_FS_DEFAULT_FLOW_TAG;
  
  	flow_action_for_each(i, act, flow_action) {
@@@ -3270,6 -3675,10 +3277,13 @@@ static int parse_tc_fdb_actions(struct 
  	if (!flow_action_has_entries(flow_action))
  		return -EINVAL;
  
++<<<<<<< HEAD
++=======
+ 	if (!flow_action_hw_stats_check(flow_action, extack,
+ 					FLOW_ACTION_HW_STATS_DELAYED_BIT))
+ 		return -EOPNOTSUPP;
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  	flow_action_for_each(i, act, flow_action) {
  		switch (act->id) {
  		case FLOW_ACTION_DROP:
@@@ -4056,6 -4510,9 +4070,12 @@@ static int scan_tc_matchall_fdb_actions
  		return -EOPNOTSUPP;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (!flow_action_basic_hw_stats_check(flow_action, extack))
+ 		return -EOPNOTSUPP;
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  	flow_action_for_each(i, act, flow_action) {
  		switch (act->id) {
  		case FLOW_ACTION_POLICE:
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum_flower.c
index 95387525a11b,21c4b10d106c..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_flower.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_flower.c
@@@ -35,7 -36,7 +35,11 @@@ static int mlxsw_sp_flower_parse_action
  		err = mlxsw_sp_acl_rulei_act_count(mlxsw_sp, rulei, extack);
  		if (err)
  			return err;
++<<<<<<< HEAD
 +	} else {
++=======
+ 	} else if (act->hw_stats_type != FLOW_ACTION_HW_STATS_DISABLED) {
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  		NL_SET_ERR_MSG_MOD(extack, "Unsupported action HW stats type");
  		return -EOPNOTSUPP;
  	}
diff --cc drivers/net/ethernet/netronome/nfp/flower/action.c
index 1b019fdfcd97,5fb9869f85d7..000000000000
--- a/drivers/net/ethernet/netronome/nfp/flower/action.c
+++ b/drivers/net/ethernet/netronome/nfp/flower/action.c
@@@ -1178,6 -1207,9 +1178,12 @@@ int nfp_flower_compile_action(struct nf
  	bool pkt_host = false;
  	u32 csum_updated = 0;
  
++<<<<<<< HEAD
++=======
+ 	if (!flow_action_basic_hw_stats_check(&flow->rule->action, extack))
+ 		return -EOPNOTSUPP;
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  	memset(nfp_flow->action_data, 0, NFP_FL_MAX_A_SIZ);
  	nfp_flow->meta.act_len = 0;
  	tun_type = NFP_FL_TUNNEL_NONE;
diff --cc drivers/net/ethernet/qlogic/qede/qede_filter.c
index c8bdbf057d5a,fe72bb6c9455..000000000000
--- a/drivers/net/ethernet/qlogic/qede/qede_filter.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_filter.c
@@@ -1756,6 -1757,9 +1756,12 @@@ static int qede_parse_actions(struct qe
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (!flow_action_basic_hw_stats_check(flow_action, extack))
+ 		return -EOPNOTSUPP;
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  	flow_action_for_each(i, act, flow_action) {
  		switch (act->id) {
  		case FLOW_ACTION_DROP:
diff --cc drivers/net/ethernet/stmicro/stmmac/stmmac_tc.c
index 225c833b2291,3d747846f482..000000000000
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_tc.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_tc.c
@@@ -288,6 -306,456 +288,459 @@@ static int tc_init(struct stmmac_priv *
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int tc_setup_cbs(struct stmmac_priv *priv,
+ 			struct tc_cbs_qopt_offload *qopt)
+ {
+ 	u32 tx_queues_count = priv->plat->tx_queues_to_use;
+ 	u32 queue = qopt->queue;
+ 	u32 ptr, speed_div;
+ 	u32 mode_to_use;
+ 	u64 value;
+ 	int ret;
+ 
+ 	/* Queue 0 is not AVB capable */
+ 	if (queue <= 0 || queue >= tx_queues_count)
+ 		return -EINVAL;
+ 	if (!priv->dma_cap.av)
+ 		return -EOPNOTSUPP;
+ 
+ 	mode_to_use = priv->plat->tx_queues_cfg[queue].mode_to_use;
+ 	if (mode_to_use == MTL_QUEUE_DCB && qopt->enable) {
+ 		ret = stmmac_dma_qmode(priv, priv->ioaddr, queue, MTL_QUEUE_AVB);
+ 		if (ret)
+ 			return ret;
+ 
+ 		priv->plat->tx_queues_cfg[queue].mode_to_use = MTL_QUEUE_AVB;
+ 	} else if (!qopt->enable) {
+ 		return stmmac_dma_qmode(priv, priv->ioaddr, queue, MTL_QUEUE_DCB);
+ 	}
+ 
+ 	/* Port Transmit Rate and Speed Divider */
+ 	ptr = (priv->speed == SPEED_100) ? 4 : 8;
+ 	speed_div = (priv->speed == SPEED_100) ? 100000 : 1000000;
+ 
+ 	/* Final adjustments for HW */
+ 	value = div_s64(qopt->idleslope * 1024ll * ptr, speed_div);
+ 	priv->plat->tx_queues_cfg[queue].idle_slope = value & GENMASK(31, 0);
+ 
+ 	value = div_s64(-qopt->sendslope * 1024ll * ptr, speed_div);
+ 	priv->plat->tx_queues_cfg[queue].send_slope = value & GENMASK(31, 0);
+ 
+ 	value = qopt->hicredit * 1024ll * 8;
+ 	priv->plat->tx_queues_cfg[queue].high_credit = value & GENMASK(31, 0);
+ 
+ 	value = qopt->locredit * 1024ll * 8;
+ 	priv->plat->tx_queues_cfg[queue].low_credit = value & GENMASK(31, 0);
+ 
+ 	ret = stmmac_config_cbs(priv, priv->hw,
+ 				priv->plat->tx_queues_cfg[queue].send_slope,
+ 				priv->plat->tx_queues_cfg[queue].idle_slope,
+ 				priv->plat->tx_queues_cfg[queue].high_credit,
+ 				priv->plat->tx_queues_cfg[queue].low_credit,
+ 				queue);
+ 	if (ret)
+ 		return ret;
+ 
+ 	dev_info(priv->device, "CBS queue %d: send %d, idle %d, hi %d, lo %d\n",
+ 			queue, qopt->sendslope, qopt->idleslope,
+ 			qopt->hicredit, qopt->locredit);
+ 	return 0;
+ }
+ 
+ static int tc_parse_flow_actions(struct stmmac_priv *priv,
+ 				 struct flow_action *action,
+ 				 struct stmmac_flow_entry *entry,
+ 				 struct netlink_ext_ack *extack)
+ {
+ 	struct flow_action_entry *act;
+ 	int i;
+ 
+ 	if (!flow_action_has_entries(action))
+ 		return -EINVAL;
+ 
+ 	if (!flow_action_basic_hw_stats_check(action, extack))
+ 		return -EOPNOTSUPP;
+ 
+ 	flow_action_for_each(i, act, action) {
+ 		switch (act->id) {
+ 		case FLOW_ACTION_DROP:
+ 			entry->action |= STMMAC_FLOW_ACTION_DROP;
+ 			return 0;
+ 		default:
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* Nothing to do, maybe inverse filter ? */
+ 	return 0;
+ }
+ 
+ static int tc_add_basic_flow(struct stmmac_priv *priv,
+ 			     struct flow_cls_offload *cls,
+ 			     struct stmmac_flow_entry *entry)
+ {
+ 	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+ 	struct flow_dissector *dissector = rule->match.dissector;
+ 	struct flow_match_basic match;
+ 
+ 	/* Nothing to do here */
+ 	if (!dissector_uses_key(dissector, FLOW_DISSECTOR_KEY_BASIC))
+ 		return -EINVAL;
+ 
+ 	flow_rule_match_basic(rule, &match);
+ 	entry->ip_proto = match.key->ip_proto;
+ 	return 0;
+ }
+ 
+ static int tc_add_ip4_flow(struct stmmac_priv *priv,
+ 			   struct flow_cls_offload *cls,
+ 			   struct stmmac_flow_entry *entry)
+ {
+ 	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+ 	struct flow_dissector *dissector = rule->match.dissector;
+ 	bool inv = entry->action & STMMAC_FLOW_ACTION_DROP;
+ 	struct flow_match_ipv4_addrs match;
+ 	u32 hw_match;
+ 	int ret;
+ 
+ 	/* Nothing to do here */
+ 	if (!dissector_uses_key(dissector, FLOW_DISSECTOR_KEY_IPV4_ADDRS))
+ 		return -EINVAL;
+ 
+ 	flow_rule_match_ipv4_addrs(rule, &match);
+ 	hw_match = ntohl(match.key->src) & ntohl(match.mask->src);
+ 	if (hw_match) {
+ 		ret = stmmac_config_l3_filter(priv, priv->hw, entry->idx, true,
+ 					      false, true, inv, hw_match);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	hw_match = ntohl(match.key->dst) & ntohl(match.mask->dst);
+ 	if (hw_match) {
+ 		ret = stmmac_config_l3_filter(priv, priv->hw, entry->idx, true,
+ 					      false, false, inv, hw_match);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int tc_add_ports_flow(struct stmmac_priv *priv,
+ 			     struct flow_cls_offload *cls,
+ 			     struct stmmac_flow_entry *entry)
+ {
+ 	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+ 	struct flow_dissector *dissector = rule->match.dissector;
+ 	bool inv = entry->action & STMMAC_FLOW_ACTION_DROP;
+ 	struct flow_match_ports match;
+ 	u32 hw_match;
+ 	bool is_udp;
+ 	int ret;
+ 
+ 	/* Nothing to do here */
+ 	if (!dissector_uses_key(dissector, FLOW_DISSECTOR_KEY_PORTS))
+ 		return -EINVAL;
+ 
+ 	switch (entry->ip_proto) {
+ 	case IPPROTO_TCP:
+ 		is_udp = false;
+ 		break;
+ 	case IPPROTO_UDP:
+ 		is_udp = true;
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	flow_rule_match_ports(rule, &match);
+ 
+ 	hw_match = ntohs(match.key->src) & ntohs(match.mask->src);
+ 	if (hw_match) {
+ 		ret = stmmac_config_l4_filter(priv, priv->hw, entry->idx, true,
+ 					      is_udp, true, inv, hw_match);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	hw_match = ntohs(match.key->dst) & ntohs(match.mask->dst);
+ 	if (hw_match) {
+ 		ret = stmmac_config_l4_filter(priv, priv->hw, entry->idx, true,
+ 					      is_udp, false, inv, hw_match);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	entry->is_l4 = true;
+ 	return 0;
+ }
+ 
+ static struct stmmac_flow_entry *tc_find_flow(struct stmmac_priv *priv,
+ 					      struct flow_cls_offload *cls,
+ 					      bool get_free)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < priv->flow_entries_max; i++) {
+ 		struct stmmac_flow_entry *entry = &priv->flow_entries[i];
+ 
+ 		if (entry->cookie == cls->cookie)
+ 			return entry;
+ 		if (get_free && (entry->in_use == false))
+ 			return entry;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static struct {
+ 	int (*fn)(struct stmmac_priv *priv, struct flow_cls_offload *cls,
+ 		  struct stmmac_flow_entry *entry);
+ } tc_flow_parsers[] = {
+ 	{ .fn = tc_add_basic_flow },
+ 	{ .fn = tc_add_ip4_flow },
+ 	{ .fn = tc_add_ports_flow },
+ };
+ 
+ static int tc_add_flow(struct stmmac_priv *priv,
+ 		       struct flow_cls_offload *cls)
+ {
+ 	struct stmmac_flow_entry *entry = tc_find_flow(priv, cls, false);
+ 	struct flow_rule *rule = flow_cls_offload_flow_rule(cls);
+ 	int i, ret;
+ 
+ 	if (!entry) {
+ 		entry = tc_find_flow(priv, cls, true);
+ 		if (!entry)
+ 			return -ENOENT;
+ 	}
+ 
+ 	ret = tc_parse_flow_actions(priv, &rule->action, entry,
+ 				    cls->common.extack);
+ 	if (ret)
+ 		return ret;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(tc_flow_parsers); i++) {
+ 		ret = tc_flow_parsers[i].fn(priv, cls, entry);
+ 		if (!ret) {
+ 			entry->in_use = true;
+ 			continue;
+ 		}
+ 	}
+ 
+ 	if (!entry->in_use)
+ 		return -EINVAL;
+ 
+ 	entry->cookie = cls->cookie;
+ 	return 0;
+ }
+ 
+ static int tc_del_flow(struct stmmac_priv *priv,
+ 		       struct flow_cls_offload *cls)
+ {
+ 	struct stmmac_flow_entry *entry = tc_find_flow(priv, cls, false);
+ 	int ret;
+ 
+ 	if (!entry || !entry->in_use)
+ 		return -ENOENT;
+ 
+ 	if (entry->is_l4) {
+ 		ret = stmmac_config_l4_filter(priv, priv->hw, entry->idx, false,
+ 					      false, false, false, 0);
+ 	} else {
+ 		ret = stmmac_config_l3_filter(priv, priv->hw, entry->idx, false,
+ 					      false, false, false, 0);
+ 	}
+ 
+ 	entry->in_use = false;
+ 	entry->cookie = 0;
+ 	entry->is_l4 = false;
+ 	return ret;
+ }
+ 
+ static int tc_setup_cls(struct stmmac_priv *priv,
+ 			struct flow_cls_offload *cls)
+ {
+ 	int ret = 0;
+ 
+ 	/* When RSS is enabled, the filtering will be bypassed */
+ 	if (priv->rss.enable)
+ 		return -EBUSY;
+ 
+ 	switch (cls->command) {
+ 	case FLOW_CLS_REPLACE:
+ 		ret = tc_add_flow(priv, cls);
+ 		break;
+ 	case FLOW_CLS_DESTROY:
+ 		ret = tc_del_flow(priv, cls);
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int tc_setup_taprio(struct stmmac_priv *priv,
+ 			   struct tc_taprio_qopt_offload *qopt)
+ {
+ 	u32 size, wid = priv->dma_cap.estwid, dep = priv->dma_cap.estdep;
+ 	struct plat_stmmacenet_data *plat = priv->plat;
+ 	struct timespec64 time;
+ 	bool fpe = false;
+ 	int i, ret = 0;
+ 	u64 ctr;
+ 
+ 	if (!priv->dma_cap.estsel)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (wid) {
+ 	case 0x1:
+ 		wid = 16;
+ 		break;
+ 	case 0x2:
+ 		wid = 20;
+ 		break;
+ 	case 0x3:
+ 		wid = 24;
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	switch (dep) {
+ 	case 0x1:
+ 		dep = 64;
+ 		break;
+ 	case 0x2:
+ 		dep = 128;
+ 		break;
+ 	case 0x3:
+ 		dep = 256;
+ 		break;
+ 	case 0x4:
+ 		dep = 512;
+ 		break;
+ 	case 0x5:
+ 		dep = 1024;
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (!qopt->enable)
+ 		goto disable;
+ 	if (qopt->num_entries >= dep)
+ 		return -EINVAL;
+ 	if (!qopt->base_time)
+ 		return -ERANGE;
+ 	if (!qopt->cycle_time)
+ 		return -ERANGE;
+ 
+ 	if (!plat->est) {
+ 		plat->est = devm_kzalloc(priv->device, sizeof(*plat->est),
+ 					 GFP_KERNEL);
+ 		if (!plat->est)
+ 			return -ENOMEM;
+ 	} else {
+ 		memset(plat->est, 0, sizeof(*plat->est));
+ 	}
+ 
+ 	size = qopt->num_entries;
+ 
+ 	priv->plat->est->gcl_size = size;
+ 	priv->plat->est->enable = qopt->enable;
+ 
+ 	for (i = 0; i < size; i++) {
+ 		s64 delta_ns = qopt->entries[i].interval;
+ 		u32 gates = qopt->entries[i].gate_mask;
+ 
+ 		if (delta_ns > GENMASK(wid, 0))
+ 			return -ERANGE;
+ 		if (gates > GENMASK(31 - wid, 0))
+ 			return -ERANGE;
+ 
+ 		switch (qopt->entries[i].command) {
+ 		case TC_TAPRIO_CMD_SET_GATES:
+ 			if (fpe)
+ 				return -EINVAL;
+ 			break;
+ 		case TC_TAPRIO_CMD_SET_AND_HOLD:
+ 			gates |= BIT(0);
+ 			fpe = true;
+ 			break;
+ 		case TC_TAPRIO_CMD_SET_AND_RELEASE:
+ 			gates &= ~BIT(0);
+ 			fpe = true;
+ 			break;
+ 		default:
+ 			return -EOPNOTSUPP;
+ 		}
+ 
+ 		priv->plat->est->gcl[i] = delta_ns | (gates << wid);
+ 	}
+ 
+ 	/* Adjust for real system time */
+ 	time = ktime_to_timespec64(qopt->base_time);
+ 	priv->plat->est->btr[0] = (u32)time.tv_nsec;
+ 	priv->plat->est->btr[1] = (u32)time.tv_sec;
+ 
+ 	ctr = qopt->cycle_time;
+ 	priv->plat->est->ctr[0] = do_div(ctr, NSEC_PER_SEC);
+ 	priv->plat->est->ctr[1] = (u32)ctr;
+ 
+ 	if (fpe && !priv->dma_cap.fpesel)
+ 		return -EOPNOTSUPP;
+ 
+ 	ret = stmmac_fpe_configure(priv, priv->ioaddr,
+ 				   priv->plat->tx_queues_to_use,
+ 				   priv->plat->rx_queues_to_use, fpe);
+ 	if (ret && fpe) {
+ 		netdev_err(priv->dev, "failed to enable Frame Preemption\n");
+ 		return ret;
+ 	}
+ 
+ 	ret = stmmac_est_configure(priv, priv->ioaddr, priv->plat->est,
+ 				   priv->plat->clk_ptp_rate);
+ 	if (ret) {
+ 		netdev_err(priv->dev, "failed to configure EST\n");
+ 		goto disable;
+ 	}
+ 
+ 	netdev_info(priv->dev, "configured EST\n");
+ 	return 0;
+ 
+ disable:
+ 	priv->plat->est->enable = false;
+ 	stmmac_est_configure(priv, priv->ioaddr, priv->plat->est,
+ 			     priv->plat->clk_ptp_rate);
+ 	return ret;
+ }
+ 
+ static int tc_setup_etf(struct stmmac_priv *priv,
+ 			struct tc_etf_qopt_offload *qopt)
+ {
+ 	if (!priv->dma_cap.tbssel)
+ 		return -EOPNOTSUPP;
+ 	if (qopt->queue >= priv->plat->tx_queues_to_use)
+ 		return -EINVAL;
+ 	if (!(priv->tx_queue[qopt->queue].tbs & STMMAC_TBS_AVAIL))
+ 		return -EINVAL;
+ 
+ 	if (qopt->enable)
+ 		priv->tx_queue[qopt->queue].tbs |= STMMAC_TBS_EN;
+ 	else
+ 		priv->tx_queue[qopt->queue].tbs &= ~STMMAC_TBS_EN;
+ 
+ 	netdev_info(priv->dev, "%s ETF for Queue %d\n",
+ 		    qopt->enable ? "enabled" : "disabled", qopt->queue);
+ 	return 0;
+ }
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  const struct stmmac_tc_ops dwmac510_tc_ops = {
  	.init = tc_init,
  	.setup_cls_u32 = tc_setup_cls_u32,
diff --cc include/net/flow_offload.h
index 26b427dff50f,1e30b0d44b61..000000000000
--- a/include/net/flow_offload.h
+++ b/include/net/flow_offload.h
@@@ -159,8 -162,19 +159,24 @@@ enum flow_action_mangle_base 
  	FLOW_ACT_MANGLE_HDR_TYPE_UDP,
  };
  
++<<<<<<< HEAD
 +#define FLOW_ACTION_HW_STATS_TYPE_IMMEDIATE BIT(0)
 +#define FLOW_ACTION_HW_STATS_TYPE_ANY FLOW_ACTION_HW_STATS_TYPE_IMMEDIATE
++=======
+ enum flow_action_hw_stats_type_bit {
+ 	FLOW_ACTION_HW_STATS_IMMEDIATE_BIT,
+ 	FLOW_ACTION_HW_STATS_DELAYED_BIT,
+ };
+ 
+ enum flow_action_hw_stats_type {
+ 	FLOW_ACTION_HW_STATS_DISABLED = 0,
+ 	FLOW_ACTION_HW_STATS_IMMEDIATE =
+ 		BIT(FLOW_ACTION_HW_STATS_IMMEDIATE_BIT),
+ 	FLOW_ACTION_HW_STATS_DELAYED = BIT(FLOW_ACTION_HW_STATS_DELAYED_BIT),
+ 	FLOW_ACTION_HW_STATS_ANY = FLOW_ACTION_HW_STATS_IMMEDIATE |
+ 				   FLOW_ACTION_HW_STATS_DELAYED,
+ };
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  
  typedef void (*action_destr)(void *priv);
  
@@@ -258,7 -278,77 +274,81 @@@ static inline bool flow_offload_has_one
  }
  
  #define flow_action_for_each(__i, __act, __actions)			\
++<<<<<<< HEAD
 +        for (__i = 0, __act = &(__actions)->entries[0]; __i < (__actions)->num_entries; __act = &(__actions)->entries[++__i])
++=======
+         for (__i = 0, __act = &(__actions)->entries[0];			\
+ 	     __i < (__actions)->num_entries;				\
+ 	     __act = &(__actions)->entries[++__i])
+ 
+ static inline bool
+ flow_action_mixed_hw_stats_check(const struct flow_action *action,
+ 				 struct netlink_ext_ack *extack)
+ {
+ 	const struct flow_action_entry *action_entry;
+ 	u8 uninitialized_var(last_hw_stats_type);
+ 	int i;
+ 
+ 	if (flow_offload_has_one_action(action))
+ 		return true;
+ 
+ 	flow_action_for_each(i, action_entry, action) {
+ 		if (i && action_entry->hw_stats_type != last_hw_stats_type) {
+ 			NL_SET_ERR_MSG_MOD(extack, "Mixing HW stats types for actions is not supported");
+ 			return false;
+ 		}
+ 		last_hw_stats_type = action_entry->hw_stats_type;
+ 	}
+ 	return true;
+ }
+ 
+ static inline const struct flow_action_entry *
+ flow_action_first_entry_get(const struct flow_action *action)
+ {
+ 	WARN_ON(!flow_action_has_entries(action));
+ 	return &action->entries[0];
+ }
+ 
+ static inline bool
+ __flow_action_hw_stats_check(const struct flow_action *action,
+ 			     struct netlink_ext_ack *extack,
+ 			     bool check_allow_bit,
+ 			     enum flow_action_hw_stats_type_bit allow_bit)
+ {
+ 	const struct flow_action_entry *action_entry;
+ 
+ 	if (!flow_action_has_entries(action))
+ 		return true;
+ 	if (!flow_action_mixed_hw_stats_check(action, extack))
+ 		return false;
+ 	action_entry = flow_action_first_entry_get(action);
+ 	if (!check_allow_bit &&
+ 	    action_entry->hw_stats_type != FLOW_ACTION_HW_STATS_ANY) {
+ 		NL_SET_ERR_MSG_MOD(extack, "Driver supports only default HW stats type \"any\"");
+ 		return false;
+ 	} else if (check_allow_bit &&
+ 		   !(action_entry->hw_stats_type & BIT(allow_bit))) {
+ 		NL_SET_ERR_MSG_MOD(extack, "Driver does not support selected HW stats type");
+ 		return false;
+ 	}
+ 	return true;
+ }
+ 
+ static inline bool
+ flow_action_hw_stats_check(const struct flow_action *action,
+ 			   struct netlink_ext_ack *extack,
+ 			   enum flow_action_hw_stats_type_bit allow_bit)
+ {
+ 	return __flow_action_hw_stats_check(action, extack, true, allow_bit);
+ }
+ 
+ static inline bool
+ flow_action_basic_hw_stats_check(const struct flow_action *action,
+ 				 struct netlink_ext_ack *extack)
+ {
+ 	return __flow_action_hw_stats_check(action, extack, false, 0);
+ }
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  
  struct flow_rule {
  	struct flow_match	match;
diff --cc net/dsa/slave.c
index fb4f949cd420,5f782fa3029f..000000000000
--- a/net/dsa/slave.c
+++ b/net/dsa/slave.c
@@@ -778,6 -861,10 +778,13 @@@ static int dsa_slave_add_cls_matchall(s
  	if (!flow_offload_has_one_action(&cls->rule->action))
  		return err;
  
++<<<<<<< HEAD
++=======
+ 	if (!flow_action_basic_hw_stats_check(&cls->rule->action,
+ 					      cls->common.extack))
+ 		return err;
+ 
++>>>>>>> 53eca1f3479f (net: rename flow_action_hw_stats_types* -> flow_action_hw_stats*)
  	act = &cls->rule->action.entries[0];
  
  	if (act->id == FLOW_ACTION_MIRRED && protocol == htons(ETH_P_ALL)) {
* Unmerged path drivers/net/ethernet/mscc/ocelot_flower.c
* Unmerged path drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c
* Unmerged path drivers/net/ethernet/broadcom/bnxt/bnxt_tc.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_tc_flower.c
* Unmerged path drivers/net/ethernet/marvell/mvpp2/mvpp2_cls.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_flower.c
* Unmerged path drivers/net/ethernet/mscc/ocelot_flower.c
* Unmerged path drivers/net/ethernet/netronome/nfp/flower/action.c
* Unmerged path drivers/net/ethernet/qlogic/qede/qede_filter.c
* Unmerged path drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c
* Unmerged path drivers/net/ethernet/stmicro/stmmac/stmmac_tc.c
* Unmerged path include/net/flow_offload.h
* Unmerged path net/dsa/slave.c
diff --git a/net/sched/cls_api.c b/net/sched/cls_api.c
index f020be511b31..64765dd61040 100644
--- a/net/sched/cls_api.c
+++ b/net/sched/cls_api.c
@@ -3533,9 +3533,9 @@ int tc_setup_flow_action(struct flow_action *flow_action,
 	struct tc_action *act;
 	int i, j, k, err = 0;
 
-	BUILD_BUG_ON(TCA_ACT_HW_STATS_TYPE_ANY != FLOW_ACTION_HW_STATS_TYPE_ANY);
-	BUILD_BUG_ON(TCA_ACT_HW_STATS_TYPE_IMMEDIATE != FLOW_ACTION_HW_STATS_TYPE_IMMEDIATE);
-	BUILD_BUG_ON(TCA_ACT_HW_STATS_TYPE_DELAYED != FLOW_ACTION_HW_STATS_TYPE_DELAYED);
+	BUILD_BUG_ON(TCA_ACT_HW_STATS_TYPE_ANY != FLOW_ACTION_HW_STATS_ANY);
+	BUILD_BUG_ON(TCA_ACT_HW_STATS_TYPE_IMMEDIATE != FLOW_ACTION_HW_STATS_IMMEDIATE);
+	BUILD_BUG_ON(TCA_ACT_HW_STATS_TYPE_DELAYED != FLOW_ACTION_HW_STATS_DELAYED);
 
 	if (!exts)
 		return 0;
