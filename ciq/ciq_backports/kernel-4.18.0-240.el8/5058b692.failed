KVM: VMX: Move vmx_flush_tlb() to vmx.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 5058b692c69997f9736b94df786509366c32f34d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5058b692.failed

Move vmx_flush_tlb() to vmx.c and make it non-inline static now that all
its callers live in vmx.c.

No functional change intended.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Message-Id: <20200320212833.3507-19-sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 5058b692c69997f9736b94df786509366c32f34d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.h
diff --cc arch/x86/kvm/vmx/vmx.h
index 70790b80a8a1,4c7b0713b438..000000000000
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@@ -503,49 -500,6 +503,52 @@@ static inline struct vmcs *alloc_vmcs(b
  
  u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);
  
++<<<<<<< HEAD
 +static inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,
 +				bool invalidate_gpa)
 +{
 +	if (enable_ept && (invalidate_gpa || !enable_vpid)) {
 +		if (!VALID_PAGE(vcpu->arch.mmu->root_hpa))
 +			return;
 +		ept_sync_context(construct_eptp(vcpu,
 +						vcpu->arch.mmu->root_hpa));
 +	} else {
 +		vpid_sync_context(vpid);
 +	}
 +}
 +
 +static inline void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 +{
 +	struct vcpu_vmx *vmx = to_vmx(vcpu);
 +
 +	/*
 +	 * Flush all EPTP/VPID contexts if the TLB flush _may_ have been
 +	 * invoked via kvm_flush_remote_tlbs(), which always passes %true for
 +	 * @invalidate_gpa.  Flushing remote TLBs requires all contexts to be
 +	 * flushed, not just the active context.
 +	 *
 +	 * Note, this also ensures a deferred TLB flush with VPID enabled and
 +	 * EPT disabled invalidates the "correct" VPID, by nuking both L1 and
 +	 * L2's VPIDs.
 +	 */
 +	if (invalidate_gpa) {
 +		if (enable_ept) {
 +			ept_sync_global();
 +		} else if (enable_vpid) {
 +			if (cpu_has_vmx_invvpid_global()) {
 +				vpid_sync_vcpu_global();
 +			} else {
 +				vpid_sync_vcpu_single(vmx->vpid);
 +				vpid_sync_vcpu_single(vmx->nested.vpid02);
 +			}
 +		}
 +	} else {
 +		__vmx_flush_tlb(vcpu, vmx->vpid, false);
 +	}
 +}
 +
++=======
++>>>>>>> 5058b692c699 (KVM: VMX: Move vmx_flush_tlb() to vmx.c)
  static inline void decache_tsc_multiplier(struct vcpu_vmx *vmx)
  {
  	vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index dbadf7ce3b2f..012dff946267 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -2931,6 +2931,31 @@ static void exit_lmode(struct kvm_vcpu *vcpu)
 
 #endif
 
+static void vmx_flush_tlb(struct kvm_vcpu *vcpu)
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	/*
+	 * Flush all EPTP/VPID contexts, as the TLB flush _may_ have been
+	 * invoked via kvm_flush_remote_tlbs().  Flushing remote TLBs requires
+	 * all contexts to be flushed, not just the active context.
+	 *
+	 * Note, this also ensures a deferred TLB flush with VPID enabled and
+	 * EPT disabled invalidates the "correct" VPID, by nuking both L1 and
+	 * L2's VPIDs.
+	 */
+	if (enable_ept) {
+		ept_sync_global();
+	} else if (enable_vpid) {
+		if (cpu_has_vmx_invvpid_global()) {
+			vpid_sync_vcpu_global();
+		} else {
+			vpid_sync_vcpu_single(vmx->vpid);
+			vpid_sync_vcpu_single(vmx->nested.vpid02);
+		}
+	}
+}
+
 static void vmx_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t addr)
 {
 	/*
* Unmerged path arch/x86/kvm/vmx/vmx.h
