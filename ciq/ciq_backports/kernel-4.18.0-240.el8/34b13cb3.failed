net/mlx5: Accumulate levels for chains prio namespaces

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paul Blakey <paulb@mellanox.com>
commit 34b13cb3eaa5ad205f4497da6420262da4940b9e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/34b13cb3.failed

Tc chains are implemented by creating a chained prio steering type, and
inside it there is a namespace for each chain (FDB_TC_MAX_CHAINS). Each
of those has a list of priorities.

Currently, all namespaces in a prio start at the parent prio level.
But since we can jump from chain (namespace) to another chain in the
same prio, we need the levels for higher chains to be higher as well.
So we created unused prios to account for levels in previous namespaces.

Fix that by accumulating the namespaces levels if we are inside a chained
type prio, and removing the unused prios.

Fixes: 328edb499f99 ('net/mlx5: Split FDB fast path prio to multiple namespaces')
	Signed-off-by: Paul Blakey <paulb@mellanox.com>
	Reviewed-by: Mark Bloch <markb@mellanox.com>
	Acked-by: Pablo Neira Ayuso <pablo@netfilter.org>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 34b13cb3eaa5ad205f4497da6420262da4940b9e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index 00d126fa6e02,60d3d88e406c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@@ -952,7 -953,7 +952,11 @@@ esw_get_prio_table(struct mlx5_eswitch 
  		flags |= (MLX5_FLOW_TABLE_TUNNEL_EN_REFORMAT |
  			  MLX5_FLOW_TABLE_TUNNEL_EN_DECAP);
  
++<<<<<<< HEAD
 +	table_prio = (chain * FDB_MAX_PRIO) + prio - 1;
++=======
+ 	table_prio = prio - 1;
++>>>>>>> 34b13cb3eaa5 (net/mlx5: Accumulate levels for chains prio namespaces)
  
  	/* create earlier levels for correct fs_core lookup when
  	 * connecting tables
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/fs_core.c b/drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
index d64993b85979..6bc7cb3910c3 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/fs_core.c
@@ -2368,9 +2368,17 @@ static void set_prio_attrs_in_prio(struct fs_prio *prio, int acc_level)
 	int acc_level_ns = acc_level;
 
 	prio->start_level = acc_level;
-	fs_for_each_ns(ns, prio)
+	fs_for_each_ns(ns, prio) {
 		/* This updates start_level and num_levels of ns's priority descendants */
 		acc_level_ns = set_prio_attrs_in_ns(ns, acc_level);
+
+		/* If this a prio with chains, and we can jump from one chain
+		 * (namepsace) to another, so we accumulate the levels
+		 */
+		if (prio->node.type == FS_TYPE_PRIO_CHAINS)
+			acc_level = acc_level_ns;
+	}
+
 	if (!prio->num_levels)
 		prio->num_levels = acc_level_ns - prio->start_level;
 	WARN_ON(prio->num_levels < acc_level_ns - prio->start_level);
