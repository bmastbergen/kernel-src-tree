libperf: Add prev/start/end to struct perf_mmap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jiri Olsa <jolsa@kernel.org>
commit ebe4d72bba86a499ea0935c58ba1c2aea5aafb43
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ebe4d72b.failed

Move prev/start/end from tools/perf's mmap to libperf's perf_mmap struct.

Committer notes:

Add linux/types.h as we use u64.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Michael Petlan <mpetlan@redhat.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
Link: http://lore.kernel.org/lkml/20190913132355.21634-16-jolsa@kernel.org
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit ebe4d72bba86a499ea0935c58ba1c2aea5aafb43)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/lib/include/internal/mmap.h
#	tools/perf/util/mmap.c
#	tools/perf/util/mmap.h
diff --cc tools/perf/util/mmap.c
index 850493205040,6ce70ff005cb..000000000000
--- a/tools/perf/util/mmap.c
+++ b/tools/perf/util/mmap.c
@@@ -101,28 -104,28 +101,28 @@@ union perf_event *perf_mmap__read_event
  	return event;
  }
  
 -static bool perf_mmap__empty(struct mmap *map)
 +static bool perf_mmap__empty(struct perf_mmap *map)
  {
- 	return perf_mmap__read_head(map) == map->prev && !map->auxtrace_mmap.base;
+ 	return perf_mmap__read_head(map) == map->core.prev && !map->auxtrace_mmap.base;
  }
  
 -void perf_mmap__get(struct mmap *map)
 +void perf_mmap__get(struct perf_mmap *map)
  {
 -	refcount_inc(&map->core.refcnt);
 +	refcount_inc(&map->refcnt);
  }
  
 -void perf_mmap__put(struct mmap *map)
 +void perf_mmap__put(struct perf_mmap *map)
  {
 -	BUG_ON(map->core.base && refcount_read(&map->core.refcnt) == 0);
 +	BUG_ON(map->base && refcount_read(&map->refcnt) == 0);
  
 -	if (refcount_dec_and_test(&map->core.refcnt))
 +	if (refcount_dec_and_test(&map->refcnt))
  		perf_mmap__munmap(map);
  }
  
 -void perf_mmap__consume(struct mmap *map)
 +void perf_mmap__consume(struct perf_mmap *map)
  {
  	if (!map->overwrite) {
- 		u64 old = map->prev;
+ 		u64 old = map->core.prev;
  
  		perf_mmap__write_tail(map, old);
  	}
@@@ -364,19 -367,19 +364,26 @@@ int perf_mmap__mmap(struct perf_mmap *m
  	 * evlist layer can't just drop it when filtering events in
  	 * perf_evlist__filter_pollfd().
  	 */
++<<<<<<< HEAD
 +	refcount_set(&map->refcnt, 2);
 +	map->prev = 0;
 +	map->mask = mp->mask;
 +	map->base = mmap(NULL, perf_mmap__mmap_len(map), mp->prot,
++=======
+ 	refcount_set(&map->core.refcnt, 2);
+ 	map->core.prev = 0;
+ 	map->core.mask = mp->mask;
+ 	map->core.base = mmap(NULL, perf_mmap__mmap_len(map), mp->prot,
++>>>>>>> ebe4d72bba86 (libperf: Add prev/start/end to struct perf_mmap)
  			 MAP_SHARED, fd, 0);
 -	if (map->core.base == MAP_FAILED) {
 +	if (map->base == MAP_FAILED) {
  		pr_debug2("failed to mmap perf event ring buffer, error %d\n",
  			  errno);
 -		map->core.base = NULL;
 +		map->base = NULL;
  		return -1;
  	}
 -	map->core.fd = fd;
 -	map->core.cpu = cpu;
 +	map->fd = fd;
 +	map->cpu = cpu;
  
  	perf_mmap__setup_affinity_mask(map, mp);
  
@@@ -437,21 -440,21 +444,31 @@@ static int overwrite_rb_find_range(voi
  /*
   * Report the start and end of the available data in ringbuffer
   */
 -static int __perf_mmap__read_init(struct mmap *md)
 +static int __perf_mmap__read_init(struct perf_mmap *md)
  {
  	u64 head = perf_mmap__read_head(md);
++<<<<<<< HEAD
 +	u64 old = md->prev;
 +	unsigned char *data = md->base + page_size;
++=======
+ 	u64 old = md->core.prev;
+ 	unsigned char *data = md->core.base + page_size;
++>>>>>>> ebe4d72bba86 (libperf: Add prev/start/end to struct perf_mmap)
  	unsigned long size;
  
- 	md->start = md->overwrite ? head : old;
- 	md->end = md->overwrite ? old : head;
+ 	md->core.start = md->overwrite ? head : old;
+ 	md->core.end = md->overwrite ? old : head;
  
- 	if ((md->end - md->start) < md->flush)
+ 	if ((md->core.end - md->core.start) < md->flush)
  		return -EAGAIN;
  
++<<<<<<< HEAD
 +	size = md->end - md->start;
 +	if (size > (unsigned long)(md->mask) + 1) {
++=======
+ 	size = md->core.end - md->core.start;
+ 	if (size > (unsigned long)(md->core.mask) + 1) {
++>>>>>>> ebe4d72bba86 (libperf: Add prev/start/end to struct perf_mmap)
  		if (!md->overwrite) {
  			WARN_ONCE(1, "failed to keep up with mmap data. (warn only once)\n");
  
@@@ -464,7 -467,7 +481,11 @@@
  		 * Backward ring buffer is full. We still have a chance to read
  		 * most of data from it.
  		 */
++<<<<<<< HEAD
 +		if (overwrite_rb_find_range(data, md->mask, &md->start, &md->end))
++=======
+ 		if (overwrite_rb_find_range(data, md->core.mask, &md->core.start, &md->core.end))
++>>>>>>> ebe4d72bba86 (libperf: Add prev/start/end to struct perf_mmap)
  			return -EINVAL;
  	}
  
@@@ -495,12 -498,12 +516,19 @@@ int perf_mmap__push(struct perf_mmap *m
  	if (rc < 0)
  		return (rc == -EAGAIN) ? 1 : -1;
  
- 	size = md->end - md->start;
+ 	size = md->core.end - md->core.start;
  
++<<<<<<< HEAD
 +	if ((md->start & md->mask) + size != (md->end & md->mask)) {
 +		buf = &data[md->start & md->mask];
 +		size = md->mask + 1 - (md->start & md->mask);
 +		md->start += size;
++=======
+ 	if ((md->core.start & md->core.mask) + size != (md->core.end & md->core.mask)) {
+ 		buf = &data[md->core.start & md->core.mask];
+ 		size = md->core.mask + 1 - (md->core.start & md->core.mask);
+ 		md->core.start += size;
++>>>>>>> ebe4d72bba86 (libperf: Add prev/start/end to struct perf_mmap)
  
  		if (push(md, to, buf, size) < 0) {
  			rc = -1;
@@@ -508,9 -511,9 +536,15 @@@
  		}
  	}
  
++<<<<<<< HEAD
 +	buf = &data[md->start & md->mask];
 +	size = md->end - md->start;
 +	md->start += size;
++=======
+ 	buf = &data[md->core.start & md->core.mask];
+ 	size = md->core.end - md->core.start;
+ 	md->core.start += size;
++>>>>>>> ebe4d72bba86 (libperf: Add prev/start/end to struct perf_mmap)
  
  	if (push(md, to, buf, size) < 0) {
  		rc = -1;
@@@ -526,16 -529,16 +560,16 @@@ out
  /*
   * Mandatory for overwrite mode
   * The direction of overwrite mode is backward.
-  * The last perf_mmap__read() will set tail to map->prev.
-  * Need to correct the map->prev to head which is the end of next read.
+  * The last perf_mmap__read() will set tail to map->core.prev.
+  * Need to correct the map->core.prev to head which is the end of next read.
   */
 -void perf_mmap__read_done(struct mmap *map)
 +void perf_mmap__read_done(struct perf_mmap *map)
  {
  	/*
  	 * Check if event was unmapped due to a POLLHUP/POLLERR.
  	 */
 -	if (!refcount_read(&map->core.refcnt))
 +	if (!refcount_read(&map->refcnt))
  		return;
  
- 	map->prev = perf_mmap__read_head(map);
+ 	map->core.prev = perf_mmap__read_head(map);
  }
diff --cc tools/perf/util/mmap.h
index 274ce389cd84,a3dd53f2bfb8..000000000000
--- a/tools/perf/util/mmap.h
+++ b/tools/perf/util/mmap.h
@@@ -18,15 -20,8 +18,20 @@@ struct aiocb
   *
   * @refcnt - e.g. code using PERF_EVENT_IOC_SET_OUTPUT to share this
   */
++<<<<<<< HEAD
 +struct perf_mmap {
 +	void		 *base;
 +	int		 mask;
 +	int		 fd;
 +	int		 cpu;
 +	refcount_t	 refcnt;
 +	u64		 prev;
 +	u64		 start;
 +	u64		 end;
++=======
+ struct mmap {
+ 	struct perf_mmap	core;
++>>>>>>> ebe4d72bba86 (libperf: Add prev/start/end to struct perf_mmap)
  	bool		 overwrite;
  	struct auxtrace_mmap auxtrace_mmap;
  	char		 event_copy[PERF_SAMPLE_MAX_SIZE] __aligned(8);
* Unmerged path tools/perf/lib/include/internal/mmap.h
* Unmerged path tools/perf/lib/include/internal/mmap.h
* Unmerged path tools/perf/util/mmap.c
* Unmerged path tools/perf/util/mmap.h
