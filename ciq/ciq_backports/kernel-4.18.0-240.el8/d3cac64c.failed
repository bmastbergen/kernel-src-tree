io_uring: fix NULL-mm for linked reqs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit d3cac64c498c4fb2df46b97ee6f4c7d6d75f5e3d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/d3cac64c.failed

__io_queue_sqe() tries to handle all request of a link,
so it's not enough to grab mm in io_sq_thread_acquire_mm()
based just on the head.

Don't check req->needs_mm and do it always.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
(cherry picked from commit d3cac64c498c4fb2df46b97ee6f4c7d6d75f5e3d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 0b681a205810,72739188b2ff..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -903,19 -1946,147 +903,152 @@@ static int io_iopoll_check(struct io_ri
  	return ret;
  }
  
 -static void kiocb_end_write(struct io_kiocb *req)
 +static void kiocb_end_write(struct kiocb *kiocb)
  {
 -	/*
 -	 * Tell lockdep we inherited freeze protection from submission
 -	 * thread.
 -	 */
 -	if (req->flags & REQ_F_ISREG) {
 -		struct inode *inode = file_inode(req->file);
 +	if (kiocb->ki_flags & IOCB_WRITE) {
 +		struct inode *inode = file_inode(kiocb->ki_filp);
  
 -		__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		/*
 +		 * Tell lockdep we inherited freeze protection from submission
 +		 * thread.
 +		 */
 +		if (S_ISREG(inode->i_mode))
 +			__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		file_end_write(kiocb->ki_filp);
  	}
++<<<<<<< HEAD
++=======
+ 	file_end_write(req->file);
+ }
+ 
+ static inline void req_set_fail_links(struct io_kiocb *req)
+ {
+ 	if ((req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) == REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ }
+ 
+ static void io_complete_rw_common(struct kiocb *kiocb, long res)
+ {
+ 	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);
+ 	int cflags = 0;
+ 
+ 	if (kiocb->ki_flags & IOCB_WRITE)
+ 		kiocb_end_write(req);
+ 
+ 	if (res != req->result)
+ 		req_set_fail_links(req);
+ 	if (req->flags & REQ_F_BUFFER_SELECTED)
+ 		cflags = io_put_kbuf(req);
+ 	__io_cqring_add_event(req, res, cflags);
+ }
+ 
+ static void io_sq_thread_drop_mm(struct io_ring_ctx *ctx)
+ {
+ 	struct mm_struct *mm = current->mm;
+ 
+ 	if (mm) {
+ 		kthread_unuse_mm(mm);
+ 		mmput(mm);
+ 	}
+ }
+ 
+ static int __io_sq_thread_acquire_mm(struct io_ring_ctx *ctx)
+ {
+ 	if (!current->mm) {
+ 		if (unlikely(!mmget_not_zero(ctx->sqo_mm)))
+ 			return -EFAULT;
+ 		kthread_use_mm(ctx->sqo_mm);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int io_sq_thread_acquire_mm(struct io_ring_ctx *ctx,
+ 				   struct io_kiocb *req)
+ {
+ 	if (!io_op_defs[req->opcode].needs_mm)
+ 		return 0;
+ 	return __io_sq_thread_acquire_mm(ctx);
+ }
+ 
+ #ifdef CONFIG_BLOCK
+ static bool io_resubmit_prep(struct io_kiocb *req, int error)
+ {
+ 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
+ 	ssize_t ret = -ECANCELED;
+ 	struct iov_iter iter;
+ 	int rw;
+ 
+ 	if (error) {
+ 		ret = error;
+ 		goto end_req;
+ 	}
+ 
+ 	switch (req->opcode) {
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 		rw = READ;
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 	case IORING_OP_WRITE:
+ 		rw = WRITE;
+ 		break;
+ 	default:
+ 		printk_once(KERN_WARNING "io_uring: bad opcode in resubmit %d\n",
+ 				req->opcode);
+ 		goto end_req;
+ 	}
+ 
+ 	ret = io_import_iovec(rw, req, &iovec, &iter, false);
+ 	if (ret < 0)
+ 		goto end_req;
+ 	ret = io_setup_async_rw(req, ret, iovec, inline_vecs, &iter);
+ 	if (!ret)
+ 		return true;
+ 	kfree(iovec);
+ end_req:
+ 	io_cqring_add_event(req, ret);
+ 	req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return false;
+ }
+ 
+ static void io_rw_resubmit(struct callback_head *cb)
+ {
+ 	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int err;
+ 
+ 	__set_current_state(TASK_RUNNING);
+ 
+ 	err = io_sq_thread_acquire_mm(ctx, req);
+ 
+ 	if (io_resubmit_prep(req, err)) {
+ 		refcount_inc(&req->refs);
+ 		io_queue_async_work(req);
+ 	}
+ }
+ #endif
+ 
+ static bool io_rw_reissue(struct io_kiocb *req, long res)
+ {
+ #ifdef CONFIG_BLOCK
+ 	struct task_struct *tsk;
+ 	int ret;
+ 
+ 	if ((res != -EAGAIN && res != -EOPNOTSUPP) || io_wq_current_is_worker())
+ 		return false;
+ 
+ 	tsk = req->task;
+ 	init_task_work(&req->task_work, io_rw_resubmit);
+ 	ret = task_work_add(tsk, &req->task_work, true);
+ 	if (!ret)
+ 		return true;
+ #endif
+ 	return false;
++>>>>>>> d3cac64c498c (io_uring: fix NULL-mm for linked reqs)
  }
  
  static void io_complete_rw(struct kiocb *kiocb, long res, long res2)
@@@ -1342,66 -2677,263 +1475,266 @@@ static ssize_t loop_rw_iter(int rw, str
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
 +		   bool force_nonblock)
++=======
+ static void io_req_map_rw(struct io_kiocb *req, ssize_t io_size,
+ 			  struct iovec *iovec, struct iovec *fast_iov,
+ 			  struct iov_iter *iter)
+ {
+ 	req->io->rw.nr_segs = iter->nr_segs;
+ 	req->io->rw.size = io_size;
+ 	req->io->rw.iov = iovec;
+ 	if (!req->io->rw.iov) {
+ 		req->io->rw.iov = req->io->rw.fast_iov;
+ 		if (req->io->rw.iov != fast_iov)
+ 			memcpy(req->io->rw.iov, fast_iov,
+ 			       sizeof(struct iovec) * iter->nr_segs);
+ 	} else {
+ 		req->flags |= REQ_F_NEED_CLEANUP;
+ 	}
+ }
+ 
+ static inline int __io_alloc_async_ctx(struct io_kiocb *req)
+ {
+ 	req->io = kmalloc(sizeof(*req->io), GFP_KERNEL);
+ 	return req->io == NULL;
+ }
+ 
+ static int io_alloc_async_ctx(struct io_kiocb *req)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 
+ 	return  __io_alloc_async_ctx(req);
+ }
+ 
+ static int io_setup_async_rw(struct io_kiocb *req, ssize_t io_size,
+ 			     struct iovec *iovec, struct iovec *fast_iov,
+ 			     struct iov_iter *iter)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 	if (!req->io) {
+ 		if (__io_alloc_async_ctx(req))
+ 			return -ENOMEM;
+ 
+ 		io_req_map_rw(req, io_size, iovec, fast_iov, iter);
+ 	}
+ 	return 0;
+ }
+ 
+ static int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			bool force_nonblock)
+ {
+ 	struct io_async_ctx *io;
+ 	struct iov_iter iter;
+ 	ssize_t ret;
+ 
+ 	ret = io_prep_rw(req, sqe, force_nonblock);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (unlikely(!(req->file->f_mode & FMODE_READ)))
+ 		return -EBADF;
+ 
+ 	/* either don't need iovec imported or already have it */
+ 	if (!req->io || req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	io = req->io;
+ 	io->rw.iov = io->rw.fast_iov;
+ 	req->io = NULL;
+ 	ret = io_import_iovec(READ, req, &io->rw.iov, &iter, !force_nonblock);
+ 	req->io = io;
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
+ 	return 0;
+ }
+ 
+ static void __io_async_buf_error(struct io_kiocb *req, int error)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	io_cqring_fill_event(req, error);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	req_set_fail_links(req);
+ 	io_double_put_req(req);
+ }
+ 
+ static void io_async_buf_cancel(struct callback_head *cb)
+ {
+ 	struct io_async_rw *rw;
+ 	struct io_kiocb *req;
+ 
+ 	rw = container_of(cb, struct io_async_rw, task_work);
+ 	req = rw->wpq.wait.private;
+ 	__io_async_buf_error(req, -ECANCELED);
+ }
+ 
+ static void io_async_buf_retry(struct callback_head *cb)
+ {
+ 	struct io_async_rw *rw;
+ 	struct io_ring_ctx *ctx;
+ 	struct io_kiocb *req;
+ 
+ 	rw = container_of(cb, struct io_async_rw, task_work);
+ 	req = rw->wpq.wait.private;
+ 	ctx = req->ctx;
+ 
+ 	__set_current_state(TASK_RUNNING);
+ 	if (!__io_sq_thread_acquire_mm(ctx)) {
+ 		mutex_lock(&ctx->uring_lock);
+ 		__io_queue_sqe(req, NULL);
+ 		mutex_unlock(&ctx->uring_lock);
+ 	} else {
+ 		__io_async_buf_error(req, -EFAULT);
+ 	}
+ }
+ 
+ static int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,
+ 			     int sync, void *arg)
+ {
+ 	struct wait_page_queue *wpq;
+ 	struct io_kiocb *req = wait->private;
+ 	struct io_async_rw *rw = &req->io->rw;
+ 	struct wait_page_key *key = arg;
+ 	struct task_struct *tsk;
+ 	int ret;
+ 
+ 	wpq = container_of(wait, struct wait_page_queue, wait);
+ 
+ 	ret = wake_page_match(wpq, key);
+ 	if (ret != 1)
+ 		return ret;
+ 
+ 	list_del_init(&wait->entry);
+ 
+ 	init_task_work(&rw->task_work, io_async_buf_retry);
+ 	/* submit ref gets dropped, acquire a new one */
+ 	refcount_inc(&req->refs);
+ 	tsk = req->task;
+ 	ret = task_work_add(tsk, &rw->task_work, true);
+ 	if (unlikely(ret)) {
+ 		/* queue just for cancelation */
+ 		init_task_work(&rw->task_work, io_async_buf_cancel);
+ 		tsk = io_wq_get_task(req->ctx->io_wq);
+ 		task_work_add(tsk, &rw->task_work, true);
+ 	}
+ 	wake_up_process(tsk);
+ 	return 1;
+ }
+ 
+ static bool io_rw_should_retry(struct io_kiocb *req)
+ {
+ 	struct kiocb *kiocb = &req->rw.kiocb;
+ 	int ret;
+ 
+ 	/* never retry for NOWAIT, we just complete with -EAGAIN */
+ 	if (req->flags & REQ_F_NOWAIT)
+ 		return false;
+ 
+ 	/* already tried, or we're doing O_DIRECT */
+ 	if (kiocb->ki_flags & (IOCB_DIRECT | IOCB_WAITQ))
+ 		return false;
+ 	/*
+ 	 * just use poll if we can, and don't attempt if the fs doesn't
+ 	 * support callback based unlocks
+ 	 */
+ 	if (file_can_poll(req->file) || !(req->file->f_mode & FMODE_BUF_RASYNC))
+ 		return false;
+ 
+ 	/*
+ 	 * If request type doesn't require req->io to defer in general,
+ 	 * we need to allocate it here
+ 	 */
+ 	if (!req->io && __io_alloc_async_ctx(req))
+ 		return false;
+ 
+ 	ret = kiocb_wait_page_queue_init(kiocb, &req->io->rw.wpq,
+ 						io_async_buf_func, req);
+ 	if (!ret) {
+ 		io_get_req_task(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static int io_iter_do_read(struct io_kiocb *req, struct iov_iter *iter)
+ {
+ 	if (req->file->f_op->read_iter)
+ 		return call_read_iter(req->file, &req->rw.kiocb, iter);
+ 	return loop_rw_iter(READ, req->file, &req->rw.kiocb, iter);
+ }
+ 
+ static int io_read(struct io_kiocb *req, bool force_nonblock)
++>>>>>>> d3cac64c498c (io_uring: fix NULL-mm for linked reqs)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 -	struct kiocb *kiocb = &req->rw.kiocb;
 +	struct kiocb *kiocb = &req->rw;
  	struct iov_iter iter;
 +	struct file *file;
  	size_t iov_count;
 -	ssize_t io_size, ret;
 +	ssize_t read_size, ret;
  
 -	ret = io_import_iovec(READ, req, &iovec, &iter, !force_nonblock);
 -	if (ret < 0)
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
  		return ret;
 +	file = kiocb->ki_filp;
  
 -	/* Ensure we clear previously set non-block flag */
 -	if (!force_nonblock)
 -		kiocb->ki_flags &= ~IOCB_NOWAIT;
 +	if (unlikely(!(file->f_mode & FMODE_READ)))
 +		return -EBADF;
  
 -	req->result = 0;
 -	io_size = ret;
 -	if (req->flags & REQ_F_LINK_HEAD)
 -		req->result = io_size;
 +	ret = io_import_iovec(req->ctx, READ, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 -	/* If the file doesn't support async, just async punt */
 -	if (force_nonblock && !io_file_supports_async(req->file, READ))
 -		goto copy_iov;
 +	read_size = ret;
 +	if (req->flags & REQ_F_LINK)
 +		req->result = read_size;
  
  	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(READ, req->file, &kiocb->ki_pos, iov_count);
 +	ret = rw_verify_area(READ, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
 -		unsigned long nr_segs = iter.nr_segs;
 -		ssize_t ret2 = 0;
 +		ssize_t ret2;
  
 -		ret2 = io_iter_do_read(req, &iter);
 +		if (file->f_op->read_iter)
 +			ret2 = call_read_iter(file, kiocb, &iter);
 +		else
 +			ret2 = loop_rw_iter(READ, file, kiocb, &iter);
  
 +		/*
 +		 * In case of a short read, punt to async. This can happen
 +		 * if we have data partially cached. Alternatively we can
 +		 * return the short read, in which case the application will
 +		 * need to issue another SQE and wait for it. That SQE will
 +		 * need async punt anyway, so it's more efficient to do it
 +		 * here.
 +		 */
 +		if (force_nonblock && ret2 > 0 && ret2 < read_size)
 +			ret2 = -EAGAIN;
  		/* Catch -EAGAIN return for forced non-blocking submission */
 -		if (!force_nonblock || (ret2 != -EAGAIN && ret2 != -EIO)) {
 -			kiocb_done(kiocb, ret2);
 +		if (!force_nonblock || ret2 != -EAGAIN) {
 +			io_rw_done(kiocb, ret2);
  		} else {
 -			iter.count = iov_count;
 -			iter.nr_segs = nr_segs;
 -copy_iov:
 -			ret = io_setup_async_rw(req, io_size, iovec,
 -						inline_vecs, &iter);
 -			if (ret)
 -				goto out_free;
 -			/* if we can retry, do so with the callbacks armed */
 -			if (io_rw_should_retry(req)) {
 -				ret2 = io_iter_do_read(req, &iter);
 -				if (ret2 == -EIOCBQUEUED) {
 -					goto out_free;
 -				} else if (ret2 != -EAGAIN) {
 -					kiocb_done(kiocb, ret2);
 -					goto out_free;
 -				}
 -			}
 -			kiocb->ki_flags &= ~IOCB_WAITQ;
 -			return -EAGAIN;
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(READ, req, iov_count);
 +			ret = -EAGAIN;
  		}
  	}
 -out_free:
 -	if (!(req->flags & REQ_F_NEED_CLEANUP))
 -		kfree(iovec);
 +	kfree(iovec);
  	return ret;
  }
  
* Unmerged path fs/io_uring.c
