io_uring: remove IO_WQ_WORK_CB

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 5eae8619907a1389dbd1b4a1049caf52782c0916
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5eae8619.failed

IO_WQ_WORK_CB is used only for linked timeouts, which will be armed
before the work setup (i.e. mm, override creds, etc). The setup
shouldn't take long, so it's ok to arm it a bit later and get rid
of IO_WQ_WORK_CB.

Make io-wq call work->func() only once, callbacks will handle the rest.
i.e. the linked timeout handler will do the actual issue. And as a
bonus, it removes an extra indirect call.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 5eae8619907a1389dbd1b4a1049caf52782c0916)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io-wq.c
#	fs/io-wq.h
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,f999503854b7..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1487,35 -2525,118 +1487,98 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++=======
+ static bool io_req_cancelled(struct io_kiocb *req)
+ {
+ 	if (req->work.flags & IO_WQ_WORK_CANCEL) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, -ECANCELED);
+ 		io_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_link_work_cb(struct io_wq_work **workptr)
+ {
+ 	struct io_wq_work *work = *workptr;
+ 	struct io_kiocb *link = work->data;
+ 
+ 	io_queue_linked_timeout(link);
+ 	io_wq_submit_work(workptr);
+ }
+ 
+ static void io_wq_assign_next(struct io_wq_work **workptr, struct io_kiocb *nxt)
+ {
+ 	struct io_kiocb *link;
+ 
+ 	io_prep_next_work(nxt, &link);
+ 	*workptr = &nxt->work;
+ 	if (link) {
+ 		nxt->work.func = io_link_work_cb;
+ 		nxt->work.data = link;
+ 	}
+ }
+ 
+ static void __io_fsync(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	loff_t end = req->sync.off + req->sync.len;
+ 	int ret;
+ 
+ 	ret = vfs_fsync_range(req->file, req->sync.off,
+ 				end > 0 ? end : LLONG_MAX,
+ 				req->sync.flags & IORING_FSYNC_DATASYNC);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_fsync_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fsync(req, &nxt);
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_fsync(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 5eae8619907a (io_uring: remove IO_WQ_WORK_CB)
  		    bool force_nonblock)
  {
 -	/* fsync always requires a blocking context */
 -	if (force_nonblock) {
 -		io_put_req(req);
 -		req->work.func = io_fsync_finish;
 -		return -EAGAIN;
 -	}
 -	__io_fsync(req, nxt);
 -	return 0;
 -}
 -
 -static void __io_fallocate(struct io_kiocb *req, struct io_kiocb **nxt)
 -{
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
  	int ret;
  
 -	if (io_req_cancelled(req))
 -		return;
 -
 -	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
 -				req->sync.len);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 -}
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
  
 -static void io_fallocate_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
  
 -	__io_fallocate(req, &nxt);
 -	if (nxt)
 -		io_wq_assign_next(workptr, nxt);
 -}
 +	/* fsync always requires a blocking context */
 +	if (force_nonblock)
 +		return -EAGAIN;
  
 -static int io_fallocate_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 +	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
 +				end > 0 ? end : LLONG_MAX,
 +				fsync_flags & IORING_FSYNC_DATASYNC);
  
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->addr);
 -	req->sync.mode = READ_ONCE(sqe->len);
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
  	return 0;
  }
  
* Unmerged path fs/io-wq.c
* Unmerged path fs/io-wq.h
* Unmerged path fs/io-wq.c
* Unmerged path fs/io-wq.h
* Unmerged path fs/io_uring.c
