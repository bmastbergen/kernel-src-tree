io_uring: allow registering credentials

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 071698e13ac6ba786dfa22349a7b62deb5a9464d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/071698e1.failed

If an application wants to use a ring with different kinds of
credentials, it can register them upfront. We don't lookup credentials,
the credentials of the task calling IORING_REGISTER_PERSONALITY is used.

An 'id' is returned for the application to use in subsequent personality
support.

	Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 071698e13ac6ba786dfa22349a7b62deb5a9464d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index e4dddc0b25db,d74567fc9628..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -260,7 -261,30 +260,34 @@@ struct io_ring_ctx 
  
  	struct user_struct	*user;
  
++<<<<<<< HEAD
 +	struct completion	ctx_done;
++=======
+ 	const struct cred	*creds;
+ 
+ 	/* 0 is for ctx quiesce/reinit/free, 1 is for sqo_thread started */
+ 	struct completion	*completions;
+ 
+ 	/* if all else fails... */
+ 	struct io_kiocb		*fallback_req;
+ 
+ #if defined(CONFIG_UNIX)
+ 	struct socket		*ring_sock;
+ #endif
+ 
+ 	struct idr		personality_idr;
+ 
+ 	struct {
+ 		unsigned		cached_cq_tail;
+ 		unsigned		cq_entries;
+ 		unsigned		cq_mask;
+ 		atomic_t		cq_timeouts;
+ 		unsigned long		cq_check_overflow;
+ 		struct wait_queue_head	cq_wait;
+ 		struct fasync_struct	*cq_fasync;
+ 		struct eventfd_ctx	*cq_ev_fd;
+ 	} ____cacheline_aligned_in_smp;
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  
  	struct {
  		struct mutex		uring_lock;
@@@ -411,29 -795,44 +438,36 @@@ static struct io_ring_ctx *io_ring_ctx_
  
  	ctx->flags = p->flags;
  	init_waitqueue_head(&ctx->cq_wait);
++<<<<<<< HEAD
 +	init_completion(&ctx->ctx_done);
 +	init_completion(&ctx->sqo_thread_started);
++=======
+ 	INIT_LIST_HEAD(&ctx->cq_overflow_list);
+ 	init_completion(&ctx->completions[0]);
+ 	init_completion(&ctx->completions[1]);
+ 	idr_init(&ctx->personality_idr);
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  	mutex_init(&ctx->uring_lock);
  	init_waitqueue_head(&ctx->wait);
 +	for (i = 0; i < ARRAY_SIZE(ctx->pending_async); i++) {
 +		spin_lock_init(&ctx->pending_async[i].lock);
 +		INIT_LIST_HEAD(&ctx->pending_async[i].list);
 +		atomic_set(&ctx->pending_async[i].cnt, 0);
 +	}
  	spin_lock_init(&ctx->completion_lock);
 -	init_llist_head(&ctx->poll_llist);
  	INIT_LIST_HEAD(&ctx->poll_list);
 +	INIT_LIST_HEAD(&ctx->cancel_list);
  	INIT_LIST_HEAD(&ctx->defer_list);
 -	INIT_LIST_HEAD(&ctx->timeout_list);
 -	init_waitqueue_head(&ctx->inflight_wait);
 -	spin_lock_init(&ctx->inflight_lock);
 -	INIT_LIST_HEAD(&ctx->inflight_list);
  	return ctx;
 -err:
 -	if (ctx->fallback_req)
 -		kmem_cache_free(req_cachep, ctx->fallback_req);
 -	kfree(ctx->completions);
 -	kfree(ctx->cancel_hash);
 -	kfree(ctx);
 -	return NULL;
 -}
 -
 -static inline bool __req_need_defer(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	return req->sequence != ctx->cached_cq_tail + ctx->cached_sq_dropped
 -					+ atomic_read(&ctx->cached_cq_overflow);
  }
  
 -static inline bool req_need_defer(struct io_kiocb *req)
 +static inline bool io_sequence_defer(struct io_ring_ctx *ctx,
 +				     struct io_kiocb *req)
  {
 -	if (unlikely(req->flags & REQ_F_IO_DRAIN))
 -		return __req_need_defer(req);
 +	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) != REQ_F_IO_DRAIN)
 +		return false;
  
 -	return false;
 +	return req->sequence != ctx->cached_cq_tail + ctx->sq_ring->dropped;
  }
  
  static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
@@@ -3567,9 -6197,18 +3612,17 @@@ static void io_ring_ctx_wait_and_kill(s
  	percpu_ref_kill(&ctx->refs);
  	mutex_unlock(&ctx->uring_lock);
  
 -	io_kill_timeouts(ctx);
  	io_poll_remove_all(ctx);
 -
 -	if (ctx->io_wq)
 -		io_wq_cancel_all(ctx->io_wq);
 -
  	io_iopoll_reap_events(ctx);
++<<<<<<< HEAD
 +	wait_for_completion(&ctx->ctx_done);
++=======
+ 	/* if we failed setting up the ctx, we might not have any rings */
+ 	if (ctx->rings)
+ 		io_cqring_overflow_flush(ctx, true);
+ 	idr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);
+ 	wait_for_completion(&ctx->completions[0]);
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  	io_ring_ctx_free(ctx);
  }
  
@@@ -3921,6 -6659,84 +3974,87 @@@ SYSCALL_DEFINE2(io_uring_setup, u32, en
  	return io_uring_setup(entries, params);
  }
  
++<<<<<<< HEAD
++=======
+ static int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)
+ {
+ 	struct io_uring_probe *p;
+ 	size_t size;
+ 	int i, ret;
+ 
+ 	size = struct_size(p, ops, nr_args);
+ 	if (size == SIZE_MAX)
+ 		return -EOVERFLOW;
+ 	p = kzalloc(size, GFP_KERNEL);
+ 	if (!p)
+ 		return -ENOMEM;
+ 
+ 	ret = -EFAULT;
+ 	if (copy_from_user(p, arg, size))
+ 		goto out;
+ 	ret = -EINVAL;
+ 	if (memchr_inv(p, 0, size))
+ 		goto out;
+ 
+ 	p->last_op = IORING_OP_LAST - 1;
+ 	if (nr_args > IORING_OP_LAST)
+ 		nr_args = IORING_OP_LAST;
+ 
+ 	for (i = 0; i < nr_args; i++) {
+ 		p->ops[i].op = i;
+ 		if (!io_op_defs[i].not_supported)
+ 			p->ops[i].flags = IO_URING_OP_SUPPORTED;
+ 	}
+ 	p->ops_len = i;
+ 
+ 	ret = 0;
+ 	if (copy_to_user(arg, p, size))
+ 		ret = -EFAULT;
+ out:
+ 	kfree(p);
+ 	return ret;
+ }
+ 
+ static int io_register_personality(struct io_ring_ctx *ctx)
+ {
+ 	const struct cred *creds = get_current_cred();
+ 	int id;
+ 
+ 	id = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,
+ 				USHRT_MAX, GFP_KERNEL);
+ 	if (id < 0)
+ 		put_cred(creds);
+ 	return id;
+ }
+ 
+ static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)
+ {
+ 	const struct cred *old_creds;
+ 
+ 	old_creds = idr_remove(&ctx->personality_idr, id);
+ 	if (old_creds) {
+ 		put_cred(old_creds);
+ 		return 0;
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ static bool io_register_op_must_quiesce(int op)
+ {
+ 	switch (op) {
+ 	case IORING_UNREGISTER_FILES:
+ 	case IORING_REGISTER_FILES_UPDATE:
+ 	case IORING_REGISTER_PROBE:
+ 	case IORING_REGISTER_PERSONALITY:
+ 	case IORING_UNREGISTER_PERSONALITY:
+ 		return false;
+ 	default:
+ 		return true;
+ 	}
+ }
+ 
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
  			       void __user *arg, unsigned nr_args)
  	__releases(ctx->uring_lock)
@@@ -3936,18 -6752,26 +4070,23 @@@
  	if (percpu_ref_is_dying(&ctx->refs))
  		return -ENXIO;
  
++<<<<<<< HEAD
 +	percpu_ref_kill(&ctx->refs);
++=======
+ 	if (io_register_op_must_quiesce(opcode)) {
+ 		percpu_ref_kill(&ctx->refs);
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  
 -		/*
 -		 * Drop uring mutex before waiting for references to exit. If
 -		 * another thread is currently inside io_uring_enter() it might
 -		 * need to grab the uring_lock to make progress. If we hold it
 -		 * here across the drain wait, then we can deadlock. It's safe
 -		 * to drop the mutex here, since no new references will come in
 -		 * after we've killed the percpu ref.
 -		 */
 -		mutex_unlock(&ctx->uring_lock);
 -		ret = wait_for_completion_interruptible(&ctx->completions[0]);
 -		mutex_lock(&ctx->uring_lock);
 -		if (ret) {
 -			percpu_ref_resurrect(&ctx->refs);
 -			ret = -EINTR;
 -			goto out;
 -		}
 -	}
 +	/*
 +	 * Drop uring mutex before waiting for references to exit. If another
 +	 * thread is currently inside io_uring_enter() it might need to grab
 +	 * the uring_lock to make progress. If we hold it here across the drain
 +	 * wait, then we can deadlock. It's safe to drop the mutex here, since
 +	 * no new references will come in after we've killed the percpu ref.
 +	 */
 +	mutex_unlock(&ctx->uring_lock);
 +	wait_for_completion(&ctx->ctx_done);
 +	mutex_lock(&ctx->uring_lock);
  
  	switch (opcode) {
  	case IORING_REGISTER_BUFFERS:
@@@ -3983,14 -6814,35 +4122,44 @@@
  			break;
  		ret = io_eventfd_unregister(ctx);
  		break;
++<<<<<<< HEAD
++=======
+ 	case IORING_REGISTER_PROBE:
+ 		ret = -EINVAL;
+ 		if (!arg || nr_args > 256)
+ 			break;
+ 		ret = io_probe(ctx, arg, nr_args);
+ 		break;
+ 	case IORING_REGISTER_PERSONALITY:
+ 		ret = -EINVAL;
+ 		if (arg || nr_args)
+ 			break;
+ 		ret = io_register_personality(ctx);
+ 		break;
+ 	case IORING_UNREGISTER_PERSONALITY:
+ 		ret = -EINVAL;
+ 		if (arg)
+ 			break;
+ 		ret = io_unregister_personality(ctx, nr_args);
+ 		break;
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  	default:
  		ret = -EINVAL;
  		break;
  	}
  
++<<<<<<< HEAD
 +	/* bring the ctx back to life */
 +	reinit_completion(&ctx->ctx_done);
 +	percpu_ref_reinit(&ctx->refs);
++=======
+ 	if (io_register_op_must_quiesce(opcode)) {
+ 		/* bring the ctx back to life */
+ 		percpu_ref_reinit(&ctx->refs);
+ out:
+ 		reinit_completion(&ctx->completions[0]);
+ 	}
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  	return ret;
  }
  
diff --cc include/uapi/linux/io_uring.h
index dd4a49ec83b7,b4ccf31db2d1..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -145,6 -209,10 +145,13 @@@ struct io_uring_params 
  #define IORING_REGISTER_EVENTFD		4
  #define IORING_UNREGISTER_EVENTFD	5
  #define IORING_REGISTER_FILES_UPDATE	6
++<<<<<<< HEAD
++=======
+ #define IORING_REGISTER_EVENTFD_ASYNC	7
+ #define IORING_REGISTER_PROBE		8
+ #define IORING_REGISTER_PERSONALITY	9
+ #define IORING_UNREGISTER_PERSONALITY	10
++>>>>>>> 071698e13ac6 (io_uring: allow registering credentials)
  
  struct io_uring_files_update {
  	__u32 offset;
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
