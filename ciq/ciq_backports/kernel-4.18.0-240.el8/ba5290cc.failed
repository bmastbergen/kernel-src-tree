io_uring: replace s->needs_lock with s->in_async

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jackie Liu <liuyun01@kylinos.cn>
commit ba5290ccb6b57fc5e274ae46d051fba1f0ece262
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ba5290cc.failed

There is no function change, just to clean up the code, use s->in_async
to make the code know where it is.

	Signed-off-by: Jackie Liu <liuyun01@kylinos.cn>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ba5290ccb6b57fc5e274ae46d051fba1f0ece262)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 8b8da0089cd2,6bbca3d58941..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1365,13 -1474,9 +1365,13 @@@ static int io_read(struct io_kiocb *req
  			ret2 = -EAGAIN;
  		/* Catch -EAGAIN return for forced non-blocking submission */
  		if (!force_nonblock || ret2 != -EAGAIN) {
++<<<<<<< HEAD
 +			io_rw_done(kiocb, ret2);
++=======
+ 			kiocb_done(kiocb, ret2, nxt, s->in_async);
++>>>>>>> ba5290ccb6b5 (io_uring: replace s->needs_lock with s->in_async)
  		} else {
- 			/*
- 			 * If ->needs_lock is true, we're already in async
- 			 * context.
- 			 */
- 			if (!s->needs_lock)
+ 			if (!s->in_async)
  				io_async_list_note(READ, req, iov_count);
  			ret = -EAGAIN;
  		}
@@@ -1439,13 -1543,9 +1438,13 @@@ static int io_write(struct io_kiocb *re
  		else
  			ret2 = loop_rw_iter(WRITE, file, kiocb, &iter);
  		if (!force_nonblock || ret2 != -EAGAIN) {
++<<<<<<< HEAD
 +			io_rw_done(kiocb, ret2);
++=======
+ 			kiocb_done(kiocb, ret2, nxt, s->in_async);
++>>>>>>> ba5290ccb6b5 (io_uring: replace s->needs_lock with s->in_async)
  		} else {
- 			/*
- 			 * If ->needs_lock is true, we're already in async
- 			 * context.
- 			 */
- 			if (!s->needs_lock)
+ 			if (!s->in_async)
  				io_async_list_note(WRITE, req, iov_count);
  			ret = -EAGAIN;
  		}
@@@ -1995,9 -2211,9 +1994,9 @@@ restart
  
  		if (!ret) {
  			s->has_user = cur_mm != NULL;
- 			s->needs_lock = true;
+ 			s->in_async = true;
  			do {
 -				ret = __io_submit_sqe(ctx, req, s, &nxt, false);
 +				ret = __io_submit_sqe(ctx, req, s, false);
  				/*
  				 * We can get EAGAIN for polled IO even though
  				 * we're forcing a sync submission from here,
@@@ -2451,13 -2683,13 +2450,20 @@@ static int io_submit_sqes(struct io_rin
  
  out:
  		if (unlikely(mm_fault)) {
 -			io_cqring_add_event(ctx, s.sqe->user_data,
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
  						-EFAULT);
  		} else {
++<<<<<<< HEAD
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
++=======
+ 			s.has_user = has_user;
+ 			s.in_async = true;
+ 			s.needs_fixed_file = true;
+ 			io_submit_sqe(ctx, &s, statep, &link);
++>>>>>>> ba5290ccb6b5 (io_uring: replace s->needs_lock with s->in_async)
  			submitted++;
  		}
  	}
@@@ -2654,21 -2874,10 +2660,21 @@@ static int io_ring_submit(struct io_rin
  
  out:
  		s.has_user = true;
- 		s.needs_lock = false;
+ 		s.in_async = false;
  		s.needs_fixed_file = false;
  		submit++;
 -		io_submit_sqe(ctx, &s, statep, &link);
 +
 +		/*
 +		 * The caller will block for events after submit, submit the
 +		 * last IO non-blocking. This is either the only IO it's
 +		 * submitting, or it already submitted the previous ones. This
 +		 * improves performance by avoiding an async punt that we don't
 +		 * need to do.
 +		 */
 +		if (block_for_last && submit == to_submit)
 +			force_nonblock = false;
 +
 +		io_submit_sqe(ctx, &s, statep, &link, force_nonblock);
  	}
  
  	if (link)
* Unmerged path fs/io_uring.c
