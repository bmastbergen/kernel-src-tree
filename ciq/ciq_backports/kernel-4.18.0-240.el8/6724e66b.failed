net/mlx5: E-Switch, Get reg_c1 value on miss

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paul Blakey <paulb@mellanox.com>
commit 6724e66b90eebb19d146b7623b3e2af15616782b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6724e66b.failed

The HW model implicitly decapsulates tunnels on chain 0 and sets reg_c1
with the mapped tunnel id. On miss, the packet does not have the outer
header and the driver restores the tunnel information from the tunnel id.

Getting reg_c1 value in software requires enabling reg_c1 loopback and
copying reg_c1 to reg_b. reg_b comes up on CQE as cqe->imm_inval_pkey.

Use the reg_c0 restoration rules to also copy reg_c1 to reg_B.

	Signed-off-by: Paul Blakey <paulb@mellanox.com>
	Reviewed-by: Oz Shlomo <ozsh@mellanox.com>
	Reviewed-by: Mark Bloch <markb@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 6724e66b90eebb19d146b7623b3e2af15616782b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
index 9c629f913b96,e7dd2ca84214..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@@ -179,6 -189,10 +179,13 @@@ struct mlx5_eswitch_fdb 
  };
  
  struct mlx5_esw_offload {
++<<<<<<< HEAD
++=======
+ 	struct mlx5_flow_table *ft_offloads_restore;
+ 	struct mlx5_flow_group *restore_group;
+ 	struct mlx5_modify_hdr *restore_copy_hdr_id;
+ 
++>>>>>>> 6724e66b90ee (net/mlx5: E-Switch, Get reg_c1 value on miss)
  	struct mlx5_flow_table *ft_offloads;
  	struct mlx5_flow_group *vport_rx_group;
  	struct mlx5_eswitch_rep *vport_reps;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index 1f7d3f1673cb,e4bd53ef16b1..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@@ -854,173 -840,54 +856,203 @@@ out
  	return err;
  }
  
 -struct mlx5_flow_handle *
 -esw_add_restore_rule(struct mlx5_eswitch *esw, u32 tag)
 +#define ESW_OFFLOADS_NUM_GROUPS  4
 +
 +/* Firmware currently has 4 pool of 4 sizes that it supports (ESW_POOLS),
 + * and a virtual memory region of 16M (ESW_SIZE), this region is duplicated
 + * for each flow table pool. We can allocate up to 16M of each pool,
 + * and we keep track of how much we used via put/get_sz_to_pool.
 + * Firmware doesn't report any of this for now.
 + * ESW_POOL is expected to be sorted from large to small
 + */
 +#define ESW_SIZE (16 * 1024 * 1024)
 +const unsigned int ESW_POOLS[4] = { 4 * 1024 * 1024, 1 * 1024 * 1024,
 +				    64 * 1024, 128 };
 +
 +static int
 +get_sz_from_pool(struct mlx5_eswitch *esw)
  {
 -	struct mlx5_flow_act flow_act = { .flags = FLOW_ACT_NO_APPEND, };
 -	struct mlx5_flow_table *ft = esw->offloads.ft_offloads_restore;
 -	struct mlx5_flow_context *flow_context;
 -	struct mlx5_flow_handle *flow_rule;
 -	struct mlx5_flow_destination dest;
 -	struct mlx5_flow_spec *spec;
 -	void *misc;
 +	int sz = 0, i;
  
 -	spec = kzalloc(sizeof(*spec), GFP_KERNEL);
 -	if (!spec)
 -		return ERR_PTR(-ENOMEM);
 +	for (i = 0; i < ARRAY_SIZE(ESW_POOLS); i++) {
 +		if (esw->fdb_table.offloads.fdb_left[i]) {
 +			--esw->fdb_table.offloads.fdb_left[i];
 +			sz = ESW_POOLS[i];
 +			break;
 +		}
 +	}
  
++<<<<<<< HEAD
 +	return sz;
++=======
+ 	misc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
+ 			    misc_parameters_2);
+ 	MLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,
+ 		 ESW_CHAIN_TAG_METADATA_MASK);
+ 	misc = MLX5_ADDR_OF(fte_match_param, spec->match_value,
+ 			    misc_parameters_2);
+ 	MLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0, tag);
+ 	spec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS_2;
+ 	flow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
+ 			  MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;
+ 	flow_act.modify_hdr = esw->offloads.restore_copy_hdr_id;
+ 
+ 	flow_context = &spec->flow_context;
+ 	flow_context->flags |= FLOW_CONTEXT_HAS_TAG;
+ 	flow_context->flow_tag = tag;
+ 	dest.type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
+ 	dest.ft = esw->offloads.ft_offloads;
+ 
+ 	flow_rule = mlx5_add_flow_rules(ft, spec, &flow_act, &dest, 1);
+ 	kfree(spec);
+ 
+ 	if (IS_ERR(flow_rule))
+ 		esw_warn(esw->dev,
+ 			 "Failed to create restore rule for tag: %d, err(%d)\n",
+ 			 tag, (int)PTR_ERR(flow_rule));
+ 
+ 	return flow_rule;
++>>>>>>> 6724e66b90ee (net/mlx5: E-Switch, Get reg_c1 value on miss)
 +}
 +
 +static void
 +put_sz_to_pool(struct mlx5_eswitch *esw, int sz)
 +{
 +	int i;
 +
 +	for (i = 0; i < ARRAY_SIZE(ESW_POOLS); i++) {
 +		if (sz >= ESW_POOLS[i]) {
 +			++esw->fdb_table.offloads.fdb_left[i];
 +			break;
 +		}
 +	}
 +}
 +
 +static struct mlx5_flow_table *
 +create_next_size_table(struct mlx5_eswitch *esw,
 +		       struct mlx5_flow_namespace *ns,
 +		       u16 table_prio,
 +		       int level,
 +		       u32 flags)
 +{
 +	struct mlx5_flow_table_attr ft_attr = {};
 +	struct mlx5_flow_table *fdb;
 +	int sz;
 +
 +	sz = get_sz_from_pool(esw);
 +	if (!sz)
 +		return ERR_PTR(-ENOSPC);
 +
 +	ft_attr.max_fte = sz;
 +	ft_attr.prio = table_prio;
 +	ft_attr.level = level;
 +	ft_attr.flags = flags;
 +	ft_attr.autogroup.max_num_groups = ESW_OFFLOADS_NUM_GROUPS;
 +	fdb = mlx5_create_auto_grouped_flow_table(ns, &ft_attr);
 +	if (IS_ERR(fdb)) {
 +		esw_warn(esw->dev, "Failed to create FDB Table err %d (table prio: %d, level: %d, size: %d)\n",
 +			 (int)PTR_ERR(fdb), table_prio, level, sz);
 +		put_sz_to_pool(esw, sz);
 +	}
 +
 +	return fdb;
  }
  
 -u32
 -esw_get_max_restore_tag(struct mlx5_eswitch *esw)
 +static struct mlx5_flow_table *
 +esw_get_prio_table(struct mlx5_eswitch *esw, u32 chain, u16 prio, int level)
  {
 -	return ESW_CHAIN_TAG_METADATA_MASK;
 +	struct mlx5_core_dev *dev = esw->dev;
 +	struct mlx5_flow_table *fdb = NULL;
 +	struct mlx5_flow_namespace *ns;
 +	int table_prio, l = 0;
 +	u32 flags = 0;
 +
 +	if (chain == FDB_SLOW_PATH_CHAIN)
 +		return esw->fdb_table.offloads.slow_fdb;
 +
 +	mutex_lock(&esw->fdb_table.offloads.fdb_prio_lock);
 +
 +	fdb = fdb_prio_table(esw, chain, prio, level).fdb;
 +	if (fdb) {
 +		/* take ref on earlier levels as well */
 +		while (level >= 0)
 +			fdb_prio_table(esw, chain, prio, level--).num_rules++;
 +		mutex_unlock(&esw->fdb_table.offloads.fdb_prio_lock);
 +		return fdb;
 +	}
 +
 +	ns = mlx5_get_fdb_sub_ns(dev, chain);
 +	if (!ns) {
 +		esw_warn(dev, "Failed to get FDB sub namespace\n");
 +		mutex_unlock(&esw->fdb_table.offloads.fdb_prio_lock);
 +		return ERR_PTR(-EOPNOTSUPP);
 +	}
 +
 +	if (esw->offloads.encap != DEVLINK_ESWITCH_ENCAP_MODE_NONE)
 +		flags |= (MLX5_FLOW_TABLE_TUNNEL_EN_REFORMAT |
 +			  MLX5_FLOW_TABLE_TUNNEL_EN_DECAP);
 +
 +	table_prio = (chain * FDB_MAX_PRIO) + prio - 1;
 +
 +	/* create earlier levels for correct fs_core lookup when
 +	 * connecting tables
 +	 */
 +	for (l = 0; l <= level; l++) {
 +		if (fdb_prio_table(esw, chain, prio, l).fdb) {
 +			fdb_prio_table(esw, chain, prio, l).num_rules++;
 +			continue;
 +		}
 +
 +		fdb = create_next_size_table(esw, ns, table_prio, l, flags);
 +		if (IS_ERR(fdb)) {
 +			l--;
 +			goto err_create_fdb;
 +		}
 +
 +		fdb_prio_table(esw, chain, prio, l).fdb = fdb;
 +		fdb_prio_table(esw, chain, prio, l).num_rules = 1;
 +	}
 +
 +	mutex_unlock(&esw->fdb_table.offloads.fdb_prio_lock);
 +	return fdb;
 +
 +err_create_fdb:
 +	mutex_unlock(&esw->fdb_table.offloads.fdb_prio_lock);
 +	if (l >= 0)
 +		esw_put_prio_table(esw, chain, prio, l);
 +
 +	return fdb;
 +}
 +
 +static void
 +esw_put_prio_table(struct mlx5_eswitch *esw, u32 chain, u16 prio, int level)
 +{
 +	int l;
 +
 +	if (chain == FDB_SLOW_PATH_CHAIN)
 +		return;
 +
 +	mutex_lock(&esw->fdb_table.offloads.fdb_prio_lock);
 +
 +	for (l = level; l >= 0; l--) {
 +		if (--(fdb_prio_table(esw, chain, prio, l).num_rules) > 0)
 +			continue;
 +
 +		put_sz_to_pool(esw, fdb_prio_table(esw, chain, prio, l).fdb->max_fte);
 +		mlx5_destroy_flow_table(fdb_prio_table(esw, chain, prio, l).fdb);
 +		fdb_prio_table(esw, chain, prio, l).fdb = NULL;
 +	}
 +
 +	mutex_unlock(&esw->fdb_table.offloads.fdb_prio_lock);
 +}
 +
 +static void esw_destroy_offloads_fast_fdb_tables(struct mlx5_eswitch *esw)
 +{
 +	/* If lazy creation isn't supported, deref the fast path tables */
 +	if (!(esw->fdb_table.flags & ESW_FDB_CHAINS_AND_PRIOS_SUPPORTED)) {
 +		esw_put_prio_table(esw, 0, 1, 1);
 +		esw_put_prio_table(esw, 0, 1, 0);
 +	}
  }
  
  #define MAX_PF_SQ 256
@@@ -1370,6 -1217,102 +1402,105 @@@ out
  	return flow_rule;
  }
  
++<<<<<<< HEAD
++=======
+ static void esw_destroy_restore_table(struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_esw_offload *offloads = &esw->offloads;
+ 
+ 	mlx5_modify_header_dealloc(esw->dev, offloads->restore_copy_hdr_id);
+ 	mlx5_destroy_flow_group(offloads->restore_group);
+ 	mlx5_destroy_flow_table(offloads->ft_offloads_restore);
+ }
+ 
+ static int esw_create_restore_table(struct mlx5_eswitch *esw)
+ {
+ 	u8 modact[MLX5_UN_SZ_BYTES(set_action_in_add_action_in_auto)] = {};
+ 	int inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);
+ 	struct mlx5_flow_table_attr ft_attr = {};
+ 	struct mlx5_core_dev *dev = esw->dev;
+ 	struct mlx5_flow_namespace *ns;
+ 	struct mlx5_modify_hdr *mod_hdr;
+ 	void *match_criteria, *misc;
+ 	struct mlx5_flow_table *ft;
+ 	struct mlx5_flow_group *g;
+ 	u32 *flow_group_in;
+ 	int err = 0;
+ 
+ 	ns = mlx5_get_flow_namespace(dev, MLX5_FLOW_NAMESPACE_OFFLOADS);
+ 	if (!ns) {
+ 		esw_warn(esw->dev, "Failed to get offloads flow namespace\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	flow_group_in = kvzalloc(inlen, GFP_KERNEL);
+ 	if (!flow_group_in) {
+ 		err = -ENOMEM;
+ 		goto out_free;
+ 	}
+ 
+ 	ft_attr.max_fte = 1 << ESW_CHAIN_TAG_METADATA_BITS;
+ 	ft = mlx5_create_flow_table(ns, &ft_attr);
+ 	if (IS_ERR(ft)) {
+ 		err = PTR_ERR(ft);
+ 		esw_warn(esw->dev, "Failed to create restore table, err %d\n",
+ 			 err);
+ 		goto out_free;
+ 	}
+ 
+ 	memset(flow_group_in, 0, inlen);
+ 	match_criteria = MLX5_ADDR_OF(create_flow_group_in, flow_group_in,
+ 				      match_criteria);
+ 	misc = MLX5_ADDR_OF(fte_match_param, match_criteria,
+ 			    misc_parameters_2);
+ 
+ 	MLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,
+ 		 ESW_CHAIN_TAG_METADATA_MASK);
+ 	MLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 0);
+ 	MLX5_SET(create_flow_group_in, flow_group_in, end_flow_index,
+ 		 ft_attr.max_fte - 1);
+ 	MLX5_SET(create_flow_group_in, flow_group_in, match_criteria_enable,
+ 		 MLX5_MATCH_MISC_PARAMETERS_2);
+ 	g = mlx5_create_flow_group(ft, flow_group_in);
+ 	if (IS_ERR(g)) {
+ 		err = PTR_ERR(g);
+ 		esw_warn(dev, "Failed to create restore flow group, err: %d\n",
+ 			 err);
+ 		goto err_group;
+ 	}
+ 
+ 	MLX5_SET(copy_action_in, modact, action_type, MLX5_ACTION_TYPE_COPY);
+ 	MLX5_SET(copy_action_in, modact, src_field,
+ 		 MLX5_ACTION_IN_FIELD_METADATA_REG_C_1);
+ 	MLX5_SET(copy_action_in, modact, dst_field,
+ 		 MLX5_ACTION_IN_FIELD_METADATA_REG_B);
+ 	mod_hdr = mlx5_modify_header_alloc(esw->dev,
+ 					   MLX5_FLOW_NAMESPACE_KERNEL, 1,
+ 					   modact);
+ 	if (IS_ERR(mod_hdr)) {
+ 		esw_warn(dev, "Failed to create restore mod header, err: %d\n",
+ 			 err);
+ 		err = PTR_ERR(mod_hdr);
+ 		goto err_mod_hdr;
+ 	}
+ 
+ 	esw->offloads.ft_offloads_restore = ft;
+ 	esw->offloads.restore_group = g;
+ 	esw->offloads.restore_copy_hdr_id = mod_hdr;
+ 
+ 	return 0;
+ 
+ err_mod_hdr:
+ 	mlx5_destroy_flow_group(g);
+ err_group:
+ 	mlx5_destroy_flow_table(ft);
+ out_free:
+ 	kvfree(flow_group_in);
+ 
+ 	return err;
+ }
+ 
++>>>>>>> 6724e66b90ee (net/mlx5: E-Switch, Get reg_c1 value on miss)
  static int esw_offloads_start(struct mlx5_eswitch *esw,
  			      struct netlink_ext_ack *extack)
  {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
