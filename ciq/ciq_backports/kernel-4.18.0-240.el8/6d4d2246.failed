sched/fair: Reorder enqueue/dequeue_task_fair path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Vincent Guittot <vincent.guittot@linaro.org>
commit 6d4d22468dae3d8757af9f8b81b848a76ef4409d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6d4d2246.failed

The walk through the cgroup hierarchy during the enqueue/dequeue of a task
is split in 2 distinct parts for throttled cfs_rq without any added value
but making code less readable.

Change the code ordering such that everything related to a cfs_rq
(throttled or not) will be done in the same loop.

In addition, the same steps ordering is used when updating a cfs_rq:

 - update_load_avg
 - update_cfs_group
 - update *h_nr_running

This reordering enables the use of h_nr_running in PELT algorithm.

No functional and performance changes are expected and have been noticed
during tests.

	Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
	Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Cc: Juri Lelli <juri.lelli@redhat.com>
	Cc: Valentin Schneider <valentin.schneider@arm.com>
	Cc: Phil Auld <pauld@redhat.com>
	Cc: Hillf Danton <hdanton@sina.com>
Link: https://lore.kernel.org/r/20200224095223.13361-5-mgorman@techsingularity.net
(cherry picked from commit 6d4d22468dae3d8757af9f8b81b848a76ef4409d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 0b55f8f47ff7,a6c7f8bfc0e5..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5197,25 -5260,22 +5197,25 @@@ enqueue_task_fair(struct rq *rq, struc
  		cfs_rq = cfs_rq_of(se);
  		enqueue_entity(cfs_rq, se, flags);
  
- 		/*
- 		 * end evaluation on encountering a throttled cfs_rq
- 		 *
- 		 * note: in the case of encountering a throttled cfs_rq we will
- 		 * post the final h_nr_running increment below.
- 		 */
- 		if (cfs_rq_throttled(cfs_rq))
- 			break;
  		cfs_rq->h_nr_running++;
 -		cfs_rq->idle_h_nr_running += idle_h_nr_running;
  
+ 		/* end evaluation on encountering a throttled cfs_rq */
+ 		if (cfs_rq_throttled(cfs_rq))
+ 			goto enqueue_throttle;
+ 
  		flags = ENQUEUE_WAKEUP;
  	}
  
  	for_each_sched_entity(se) {
  		cfs_rq = cfs_rq_of(se);
++<<<<<<< HEAD
 +		cfs_rq->h_nr_running++;
++=======
++>>>>>>> 6d4d22468dae (sched/fair: Reorder enqueue/dequeue_task_fair path)
  
+ 		/* end evaluation on encountering a throttled cfs_rq */
  		if (cfs_rq_throttled(cfs_rq))
- 			break;
+ 			goto enqueue_throttle;
  
  		update_load_avg(cfs_rq, se, UPDATE_TG);
  		update_cfs_group(se);
@@@ -5279,16 -5345,13 +5283,12 @@@ static void dequeue_task_fair(struct r
  		cfs_rq = cfs_rq_of(se);
  		dequeue_entity(cfs_rq, se, flags);
  
- 		/*
- 		 * end evaluation on encountering a throttled cfs_rq
- 		 *
- 		 * note: in the case of encountering a throttled cfs_rq we will
- 		 * post the final h_nr_running decrement below.
- 		*/
- 		if (cfs_rq_throttled(cfs_rq))
- 			break;
  		cfs_rq->h_nr_running--;
 -		cfs_rq->idle_h_nr_running -= idle_h_nr_running;
  
+ 		/* end evaluation on encountering a throttled cfs_rq */
+ 		if (cfs_rq_throttled(cfs_rq))
+ 			goto dequeue_throttle;
+ 
  		/* Don't dequeue parent if it has other entities besides us */
  		if (cfs_rq->load.weight) {
  			/* Avoid re-evaluating load for this entity: */
@@@ -5306,10 -5369,10 +5306,14 @@@
  
  	for_each_sched_entity(se) {
  		cfs_rq = cfs_rq_of(se);
++<<<<<<< HEAD
 +		cfs_rq->h_nr_running--;
++=======
++>>>>>>> 6d4d22468dae (sched/fair: Reorder enqueue/dequeue_task_fair path)
  
+ 		/* end evaluation on encountering a throttled cfs_rq */
  		if (cfs_rq_throttled(cfs_rq))
- 			break;
+ 			goto dequeue_throttle;
  
  		update_load_avg(cfs_rq, se, UPDATE_TG);
  		update_cfs_group(se);
* Unmerged path kernel/sched/fair.c
