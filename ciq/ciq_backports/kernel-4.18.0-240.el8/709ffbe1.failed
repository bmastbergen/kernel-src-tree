net: remove indirect block netdev event registration

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [net] remove indirect block netdev event registration (Marcelo Leitner) [1841300]
Rebuild_FUZZ: 94.95%
commit-author Pablo Neira Ayuso <pablo@netfilter.org>
commit 709ffbe19b777e8fc952e2fdcfd8e6f50c8ef08c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/709ffbe1.failed

Drivers do not register to netdev events to set up indirect blocks
anymore. Remove __flow_indr_block_cb_register() and
__flow_indr_block_cb_unregister().

The frontends set up the callbacks through flow_indr_dev_setup_block()

	Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 709ffbe19b777e8fc952e2fdcfd8e6f50c8ef08c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/flow_offload.c
#	net/netfilter/nf_flow_table_offload.c
#	net/netfilter/nf_tables_offload.c
diff --cc net/core/flow_offload.c
index 9932099e5a4c,0cfc35e6be28..000000000000
--- a/net/core/flow_offload.c
+++ b/net/core/flow_offload.c
@@@ -467,241 -473,3 +467,244 @@@ int flow_indr_dev_setup_offload(struct 
  	return list_empty(&bo->cb_list) ? -EOPNOTSUPP : 0;
  }
  EXPORT_SYMBOL(flow_indr_dev_setup_offload);
++<<<<<<< HEAD
 +
 +static LIST_HEAD(block_cb_list);
 +
 +static struct rhashtable indr_setup_block_ht;
 +
 +struct flow_indr_block_cb {
 +	struct list_head list;
 +	void *cb_priv;
 +	flow_indr_block_bind_cb_t *cb;
 +	void *cb_ident;
 +};
 +
 +struct flow_indr_block_dev {
 +	struct rhash_head ht_node;
 +	struct net_device *dev;
 +	unsigned int refcnt;
 +	struct list_head cb_list;
 +};
 +
 +static const struct rhashtable_params flow_indr_setup_block_ht_params = {
 +	.key_offset	= offsetof(struct flow_indr_block_dev, dev),
 +	.head_offset	= offsetof(struct flow_indr_block_dev, ht_node),
 +	.key_len	= sizeof(struct net_device *),
 +};
 +
 +static struct flow_indr_block_dev *
 +flow_indr_block_dev_lookup(struct net_device *dev)
 +{
 +	return rhashtable_lookup_fast(&indr_setup_block_ht, &dev,
 +				      flow_indr_setup_block_ht_params);
 +}
 +
 +static struct flow_indr_block_dev *
 +flow_indr_block_dev_get(struct net_device *dev)
 +{
 +	struct flow_indr_block_dev *indr_dev;
 +
 +	indr_dev = flow_indr_block_dev_lookup(dev);
 +	if (indr_dev)
 +		goto inc_ref;
 +
 +	indr_dev = kzalloc(sizeof(*indr_dev), GFP_KERNEL);
 +	if (!indr_dev)
 +		return NULL;
 +
 +	INIT_LIST_HEAD(&indr_dev->cb_list);
 +	indr_dev->dev = dev;
 +	if (rhashtable_insert_fast(&indr_setup_block_ht, &indr_dev->ht_node,
 +				   flow_indr_setup_block_ht_params)) {
 +		kfree(indr_dev);
 +		return NULL;
 +	}
 +
 +inc_ref:
 +	indr_dev->refcnt++;
 +	return indr_dev;
 +}
 +
 +static void flow_indr_block_dev_put(struct flow_indr_block_dev *indr_dev)
 +{
 +	if (--indr_dev->refcnt)
 +		return;
 +
 +	rhashtable_remove_fast(&indr_setup_block_ht, &indr_dev->ht_node,
 +			       flow_indr_setup_block_ht_params);
 +	kfree(indr_dev);
 +}
 +
 +static struct flow_indr_block_cb *
 +flow_indr_block_cb_lookup(struct flow_indr_block_dev *indr_dev,
 +			  flow_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	struct flow_indr_block_cb *indr_block_cb;
 +
 +	list_for_each_entry(indr_block_cb, &indr_dev->cb_list, list)
 +		if (indr_block_cb->cb == cb &&
 +		    indr_block_cb->cb_ident == cb_ident)
 +			return indr_block_cb;
 +	return NULL;
 +}
 +
 +static struct flow_indr_block_cb *
 +flow_indr_block_cb_add(struct flow_indr_block_dev *indr_dev, void *cb_priv,
 +		       flow_indr_block_bind_cb_t *cb, void *cb_ident)
 +{
 +	struct flow_indr_block_cb *indr_block_cb;
 +
 +	indr_block_cb = flow_indr_block_cb_lookup(indr_dev, cb, cb_ident);
 +	if (indr_block_cb)
 +		return ERR_PTR(-EEXIST);
 +
 +	indr_block_cb = kzalloc(sizeof(*indr_block_cb), GFP_KERNEL);
 +	if (!indr_block_cb)
 +		return ERR_PTR(-ENOMEM);
 +
 +	indr_block_cb->cb_priv = cb_priv;
 +	indr_block_cb->cb = cb;
 +	indr_block_cb->cb_ident = cb_ident;
 +	list_add(&indr_block_cb->list, &indr_dev->cb_list);
 +
 +	return indr_block_cb;
 +}
 +
 +static void flow_indr_block_cb_del(struct flow_indr_block_cb *indr_block_cb)
 +{
 +	list_del(&indr_block_cb->list);
 +	kfree(indr_block_cb);
 +}
 +
 +static DEFINE_MUTEX(flow_indr_block_cb_lock);
 +
 +static void flow_block_cmd(struct net_device *dev,
 +			   flow_indr_block_bind_cb_t *cb, void *cb_priv,
 +			   enum flow_block_command command)
 +{
 +	struct flow_indr_block_entry *entry;
 +
 +	mutex_lock(&flow_indr_block_cb_lock);
 +	list_for_each_entry(entry, &block_cb_list, list) {
 +		entry->cb(dev, cb, cb_priv, command);
 +	}
 +	mutex_unlock(&flow_indr_block_cb_lock);
 +}
 +
 +int __flow_indr_block_cb_register(struct net_device *dev, void *cb_priv,
 +				  flow_indr_block_bind_cb_t *cb,
 +				  void *cb_ident)
 +{
 +	struct flow_indr_block_cb *indr_block_cb;
 +	struct flow_indr_block_dev *indr_dev;
 +	int err;
 +
 +	indr_dev = flow_indr_block_dev_get(dev);
 +	if (!indr_dev)
 +		return -ENOMEM;
 +
 +	indr_block_cb = flow_indr_block_cb_add(indr_dev, cb_priv, cb, cb_ident);
 +	err = PTR_ERR_OR_ZERO(indr_block_cb);
 +	if (err)
 +		goto err_dev_put;
 +
 +	flow_block_cmd(dev, indr_block_cb->cb, indr_block_cb->cb_priv,
 +		       FLOW_BLOCK_BIND);
 +
 +	return 0;
 +
 +err_dev_put:
 +	flow_indr_block_dev_put(indr_dev);
 +	return err;
 +}
 +EXPORT_SYMBOL_GPL(__flow_indr_block_cb_register);
 +
 +int flow_indr_block_cb_register(struct net_device *dev, void *cb_priv,
 +				flow_indr_block_bind_cb_t *cb,
 +				void *cb_ident)
 +{
 +	int err;
 +
 +	rtnl_lock();
 +	err = __flow_indr_block_cb_register(dev, cb_priv, cb, cb_ident);
 +	rtnl_unlock();
 +
 +	return err;
 +}
 +EXPORT_SYMBOL_GPL(flow_indr_block_cb_register);
 +
 +void __flow_indr_block_cb_unregister(struct net_device *dev,
 +				     flow_indr_block_bind_cb_t *cb,
 +				     void *cb_ident)
 +{
 +	struct flow_indr_block_cb *indr_block_cb;
 +	struct flow_indr_block_dev *indr_dev;
 +
 +	indr_dev = flow_indr_block_dev_lookup(dev);
 +	if (!indr_dev)
 +		return;
 +
 +	indr_block_cb = flow_indr_block_cb_lookup(indr_dev, cb, cb_ident);
 +	if (!indr_block_cb)
 +		return;
 +
 +	flow_block_cmd(dev, indr_block_cb->cb, indr_block_cb->cb_priv,
 +		       FLOW_BLOCK_UNBIND);
 +
 +	flow_indr_block_cb_del(indr_block_cb);
 +	flow_indr_block_dev_put(indr_dev);
 +}
 +EXPORT_SYMBOL_GPL(__flow_indr_block_cb_unregister);
 +
 +void flow_indr_block_cb_unregister(struct net_device *dev,
 +				   flow_indr_block_bind_cb_t *cb,
 +				   void *cb_ident)
 +{
 +	rtnl_lock();
 +	__flow_indr_block_cb_unregister(dev, cb, cb_ident);
 +	rtnl_unlock();
 +}
 +EXPORT_SYMBOL_GPL(flow_indr_block_cb_unregister);
 +
 +void flow_indr_block_call(struct net_device *dev,
 +			  struct flow_block_offload *bo,
 +			  enum flow_block_command command)
 +{
 +	struct flow_indr_block_cb *indr_block_cb;
 +	struct flow_indr_block_dev *indr_dev;
 +
 +	indr_dev = flow_indr_block_dev_lookup(dev);
 +	if (!indr_dev)
 +		return;
 +
 +	list_for_each_entry(indr_block_cb, &indr_dev->cb_list, list)
 +		indr_block_cb->cb(dev, indr_block_cb->cb_priv, TC_SETUP_BLOCK,
 +				  bo);
 +}
 +EXPORT_SYMBOL_GPL(flow_indr_block_call);
 +
 +void flow_indr_add_block_cb(struct flow_indr_block_entry *entry)
 +{
 +	mutex_lock(&flow_indr_block_cb_lock);
 +	list_add_tail(&entry->list, &block_cb_list);
 +	mutex_unlock(&flow_indr_block_cb_lock);
 +}
 +EXPORT_SYMBOL_GPL(flow_indr_add_block_cb);
 +
 +void flow_indr_del_block_cb(struct flow_indr_block_entry *entry)
 +{
 +	mutex_lock(&flow_indr_block_cb_lock);
 +	list_del(&entry->list);
 +	mutex_unlock(&flow_indr_block_cb_lock);
 +}
 +EXPORT_SYMBOL_GPL(flow_indr_del_block_cb);
 +
 +static int __init init_flow_indr_rhashtable(void)
 +{
 +	return rhashtable_init(&indr_setup_block_ht,
 +			       &flow_indr_setup_block_ht_params);
 +}
 +subsys_initcall(init_flow_indr_rhashtable);
++=======
++>>>>>>> 709ffbe19b77 (net: remove indirect block netdev event registration)
* Unmerged path net/netfilter/nf_flow_table_offload.c
* Unmerged path net/netfilter/nf_tables_offload.c
diff --git a/include/net/flow_offload.h b/include/net/flow_offload.h
index d252a0ea7764..01be685dc652 100644
--- a/include/net/flow_offload.h
+++ b/include/net/flow_offload.h
@@ -431,15 +431,6 @@ typedef void flow_indr_block_cmd_t(struct net_device *dev,
 				   flow_indr_block_bind_cb_t *cb, void *cb_priv,
 				   enum flow_block_command command);
 
-struct flow_indr_block_entry {
-	flow_indr_block_cmd_t *cb;
-	struct list_head	list;
-};
-
-void flow_indr_add_block_cb(struct flow_indr_block_entry *entry);
-
-void flow_indr_del_block_cb(struct flow_indr_block_entry *entry);
-
 int __flow_indr_block_cb_register(struct net_device *dev, void *cb_priv,
 				  flow_indr_block_bind_cb_t *cb,
 				  void *cb_ident);
* Unmerged path net/core/flow_offload.c
* Unmerged path net/netfilter/nf_flow_table_offload.c
* Unmerged path net/netfilter/nf_tables_offload.c
diff --git a/net/sched/cls_api.c b/net/sched/cls_api.c
index 5254d5149eb2..4ca38b72db0b 100644
--- a/net/sched/cls_api.c
+++ b/net/sched/cls_api.c
@@ -625,78 +625,6 @@ static void tcf_chain_flush(struct tcf_chain *chain, bool rtnl_held)
 static int tcf_block_setup(struct tcf_block *block,
 			   struct flow_block_offload *bo);
 
-static void tc_indr_block_cmd(struct net_device *dev, struct tcf_block *block,
-			      flow_indr_block_bind_cb_t *cb, void *cb_priv,
-			      enum flow_block_command command, bool ingress)
-{
-	struct flow_block_offload bo = {
-		.command	= command,
-		.binder_type	= ingress ?
-				  FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS :
-				  FLOW_BLOCK_BINDER_TYPE_CLSACT_EGRESS,
-		.net		= dev_net(dev),
-		.block_shared	= tcf_block_non_null_shared(block),
-	};
-	INIT_LIST_HEAD(&bo.cb_list);
-
-	if (!block)
-		return;
-
-	bo.block = &block->flow_block;
-
-	down_write(&block->cb_lock);
-	cb(dev, cb_priv, TC_SETUP_BLOCK, &bo);
-
-	tcf_block_setup(block, &bo);
-	up_write(&block->cb_lock);
-}
-
-static struct tcf_block *tc_dev_block(struct net_device *dev, bool ingress)
-{
-	const struct Qdisc_class_ops *cops;
-	const struct Qdisc_ops *ops;
-	struct Qdisc *qdisc;
-
-	if (!dev_ingress_queue(dev))
-		return NULL;
-
-	qdisc = dev_ingress_queue(dev)->qdisc_sleeping;
-	if (!qdisc)
-		return NULL;
-
-	ops = qdisc->ops;
-	if (!ops)
-		return NULL;
-
-	if (!ingress && !strcmp("ingress", ops->id))
-		return NULL;
-
-	cops = ops->cl_ops;
-	if (!cops)
-		return NULL;
-
-	if (!cops->tcf_block)
-		return NULL;
-
-	return cops->tcf_block(qdisc,
-			       ingress ? TC_H_MIN_INGRESS : TC_H_MIN_EGRESS,
-			       NULL);
-}
-
-static void tc_indr_block_get_and_cmd(struct net_device *dev,
-				      flow_indr_block_bind_cb_t *cb,
-				      void *cb_priv,
-				      enum flow_block_command command)
-{
-	struct tcf_block *block;
-
-	block = tc_dev_block(dev, true);
-	tc_indr_block_cmd(dev, block, cb, cb_priv, command, true);
-
-	block = tc_dev_block(dev, false);
-	tc_indr_block_cmd(dev, block, cb, cb_priv, command, false);
-}
-
 static void tcf_block_offload_init(struct flow_block_offload *bo,
 				   struct net_device *dev,
 				   enum flow_block_command command,
@@ -3748,11 +3676,6 @@ static struct pernet_operations tcf_net_ops = {
 	.size = sizeof(struct tcf_net),
 };
 
-static struct flow_indr_block_entry block_entry = {
-	.cb = tc_indr_block_get_and_cmd,
-	.list = LIST_HEAD_INIT(block_entry.list),
-};
-
 static int __init tc_filter_init(void)
 {
 	int err;
@@ -3765,8 +3688,6 @@ static int __init tc_filter_init(void)
 	if (err)
 		goto err_register_pernet_subsys;
 
-	flow_indr_add_block_cb(&block_entry);
-
 	rtnl_register(PF_UNSPEC, RTM_NEWTFILTER, tc_new_tfilter, NULL,
 		      RTNL_FLAG_DOIT_UNLOCKED);
 	rtnl_register(PF_UNSPEC, RTM_DELTFILTER, tc_del_tfilter, NULL,
