KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 93c380e7b528882396ca463971012222bad7d82e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/93c380e7.failed

Set emulated and transmuted (set based on other features) feature bits
via kvm_cpu_caps now that the CPUID output for KVM_GET_SUPPORTED_CPUID
is direcly overidden with kvm_cpu_caps.

Note, VMX emulation of UMIP already sets kvm_cpu_caps.

No functional change intended.

	Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 93c380e7b528882396ca463971012222bad7d82e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/cpuid.c
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/cpuid.c
index 70fb27088e0c,dedf30fedbcb..000000000000
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@@ -285,12 -261,173 +285,168 @@@ out
  	return r;
  }
  
 -static __always_inline void kvm_cpu_cap_mask(enum cpuid_leafs leaf, u32 mask)
 +static __always_inline void cpuid_mask(u32 *word, int wordnum)
  {
 -	const struct cpuid_reg cpuid = x86_feature_cpuid(leaf * 32);
 -	struct kvm_cpuid_entry2 entry;
 -
 -	reverse_cpuid_check(leaf);
 -	kvm_cpu_caps[leaf] &= mask;
 -
 -	cpuid_count(cpuid.function, cpuid.index,
 -		    &entry.eax, &entry.ebx, &entry.ecx, &entry.edx);
 -
 -	kvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, &cpuid);
 +	reverse_cpuid_check(wordnum);
 +	*word &= boot_cpu_data.x86_capability[wordnum];
  }
  
++<<<<<<< HEAD
++=======
+ void kvm_set_cpu_caps(void)
+ {
+ 	unsigned int f_nx = is_efer_nx() ? F(NX) : 0;
+ #ifdef CONFIG_X86_64
+ 	unsigned int f_gbpages = F(GBPAGES);
+ 	unsigned int f_lm = F(LM);
+ #else
+ 	unsigned int f_gbpages = 0;
+ 	unsigned int f_lm = 0;
+ #endif
+ 
+ 	BUILD_BUG_ON(sizeof(kvm_cpu_caps) >
+ 		     sizeof(boot_cpu_data.x86_capability));
+ 
+ 	memcpy(&kvm_cpu_caps, &boot_cpu_data.x86_capability,
+ 	       sizeof(kvm_cpu_caps));
+ 
+ 	kvm_cpu_cap_mask(CPUID_1_ECX,
+ 		/*
+ 		 * NOTE: MONITOR (and MWAIT) are emulated as NOP, but *not*
+ 		 * advertised to guests via CPUID!
+ 		 */
+ 		F(XMM3) | F(PCLMULQDQ) | 0 /* DTES64, MONITOR */ |
+ 		0 /* DS-CPL, VMX, SMX, EST */ |
+ 		0 /* TM2 */ | F(SSSE3) | 0 /* CNXT-ID */ | 0 /* Reserved */ |
+ 		F(FMA) | F(CX16) | 0 /* xTPR Update, PDCM */ |
+ 		F(PCID) | 0 /* Reserved, DCA */ | F(XMM4_1) |
+ 		F(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |
+ 		0 /* Reserved*/ | F(AES) | F(XSAVE) | 0 /* OSXSAVE */ | F(AVX) |
+ 		F(F16C) | F(RDRAND)
+ 	);
+ 	/* KVM emulates x2apic in software irrespective of host support. */
+ 	kvm_cpu_cap_set(X86_FEATURE_X2APIC);
+ 
+ 	kvm_cpu_cap_mask(CPUID_1_EDX,
+ 		F(FPU) | F(VME) | F(DE) | F(PSE) |
+ 		F(TSC) | F(MSR) | F(PAE) | F(MCE) |
+ 		F(CX8) | F(APIC) | 0 /* Reserved */ | F(SEP) |
+ 		F(MTRR) | F(PGE) | F(MCA) | F(CMOV) |
+ 		F(PAT) | F(PSE36) | 0 /* PSN */ | F(CLFLUSH) |
+ 		0 /* Reserved, DS, ACPI */ | F(MMX) |
+ 		F(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |
+ 		0 /* HTT, TM, Reserved, PBE */
+ 	);
+ 
+ 	kvm_cpu_cap_mask(CPUID_7_0_EBX,
+ 		F(FSGSBASE) | F(BMI1) | F(HLE) | F(AVX2) | F(SMEP) |
+ 		F(BMI2) | F(ERMS) | 0 /*INVPCID*/ | F(RTM) | 0 /*MPX*/ | F(RDSEED) |
+ 		F(ADX) | F(SMAP) | F(AVX512IFMA) | F(AVX512F) | F(AVX512PF) |
+ 		F(AVX512ER) | F(AVX512CD) | F(CLFLUSHOPT) | F(CLWB) | F(AVX512DQ) |
+ 		F(SHA_NI) | F(AVX512BW) | F(AVX512VL) | 0 /*INTEL_PT*/
+ 	);
+ 
+ 	kvm_cpu_cap_mask(CPUID_7_ECX,
+ 		F(AVX512VBMI) | F(LA57) | 0 /*PKU*/ | 0 /*OSPKE*/ | F(RDPID) |
+ 		F(AVX512_VPOPCNTDQ) | F(UMIP) | F(AVX512_VBMI2) | F(GFNI) |
+ 		F(VAES) | F(VPCLMULQDQ) | F(AVX512_VNNI) | F(AVX512_BITALG) |
+ 		F(CLDEMOTE) | F(MOVDIRI) | F(MOVDIR64B) | 0 /*WAITPKG*/
+ 	);
+ 	/* Set LA57 based on hardware capability. */
+ 	if (cpuid_ecx(7) & F(LA57))
+ 		kvm_cpu_cap_set(X86_FEATURE_LA57);
+ 
+ 	kvm_cpu_cap_mask(CPUID_7_EDX,
+ 		F(AVX512_4VNNIW) | F(AVX512_4FMAPS) | F(SPEC_CTRL) |
+ 		F(SPEC_CTRL_SSBD) | F(ARCH_CAPABILITIES) | F(INTEL_STIBP) |
+ 		F(MD_CLEAR)
+ 	);
+ 
+ 	/* TSC_ADJUST and ARCH_CAPABILITIES are emulated in software. */
+ 	kvm_cpu_cap_set(X86_FEATURE_TSC_ADJUST);
+ 	kvm_cpu_cap_set(X86_FEATURE_ARCH_CAPABILITIES);
+ 
+ 	if (boot_cpu_has(X86_FEATURE_IBPB) && boot_cpu_has(X86_FEATURE_IBRS))
+ 		kvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL);
+ 	if (boot_cpu_has(X86_FEATURE_STIBP))
+ 		kvm_cpu_cap_set(X86_FEATURE_INTEL_STIBP);
+ 	if (boot_cpu_has(X86_FEATURE_AMD_SSBD))
+ 		kvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL_SSBD);
+ 
+ 	kvm_cpu_cap_mask(CPUID_7_1_EAX,
+ 		F(AVX512_BF16)
+ 	);
+ 
+ 	kvm_cpu_cap_mask(CPUID_D_1_EAX,
+ 		F(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | F(XSAVES)
+ 	);
+ 
+ 	kvm_cpu_cap_mask(CPUID_8000_0001_ECX,
+ 		F(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |
+ 		F(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |
+ 		F(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |
+ 		0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM) |
+ 		F(TOPOEXT) | F(PERFCTR_CORE)
+ 	);
+ 
+ 	kvm_cpu_cap_mask(CPUID_8000_0001_EDX,
+ 		F(FPU) | F(VME) | F(DE) | F(PSE) |
+ 		F(TSC) | F(MSR) | F(PAE) | F(MCE) |
+ 		F(CX8) | F(APIC) | 0 /* Reserved */ | F(SYSCALL) |
+ 		F(MTRR) | F(PGE) | F(MCA) | F(CMOV) |
+ 		F(PAT) | F(PSE36) | 0 /* Reserved */ |
+ 		f_nx | 0 /* Reserved */ | F(MMXEXT) | F(MMX) |
+ 		F(FXSR) | F(FXSR_OPT) | f_gbpages | F(RDTSCP) |
+ 		0 /* Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW)
+ 	);
+ 
+ 	if (!tdp_enabled && IS_ENABLED(CONFIG_X86_64))
+ 		kvm_cpu_cap_set(X86_FEATURE_GBPAGES);
+ 
+ 	kvm_cpu_cap_mask(CPUID_8000_0008_EBX,
+ 		F(CLZERO) | F(XSAVEERPTR) |
+ 		F(WBNOINVD) | F(AMD_IBPB) | F(AMD_IBRS) | F(AMD_SSBD) | F(VIRT_SSBD) |
+ 		F(AMD_SSB_NO) | F(AMD_STIBP) | F(AMD_STIBP_ALWAYS_ON)
+ 	);
+ 
+ 	/*
+ 	 * AMD has separate bits for each SPEC_CTRL bit.
+ 	 * arch/x86/kernel/cpu/bugs.c is kind enough to
+ 	 * record that in cpufeatures so use them.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_IBPB))
+ 		kvm_cpu_cap_set(X86_FEATURE_AMD_IBPB);
+ 	if (boot_cpu_has(X86_FEATURE_IBRS))
+ 		kvm_cpu_cap_set(X86_FEATURE_AMD_IBRS);
+ 	if (boot_cpu_has(X86_FEATURE_STIBP))
+ 		kvm_cpu_cap_set(X86_FEATURE_AMD_STIBP);
+ 	if (boot_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD))
+ 		kvm_cpu_cap_set(X86_FEATURE_AMD_SSBD);
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		kvm_cpu_cap_set(X86_FEATURE_AMD_SSB_NO);
+ 	/*
+ 	 * The preference is to use SPEC CTRL MSR instead of the
+ 	 * VIRT_SPEC MSR.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&
+ 	    !boot_cpu_has(X86_FEATURE_AMD_SSBD))
+ 		kvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);
+ 
+ 	/*
+ 	 * Hide all SVM features by default, SVM will set the cap bits for
+ 	 * features it emulates and/or exposes for L1.
+ 	 */
+ 	kvm_cpu_cap_mask(CPUID_8000_000A_EDX, 0);
+ 
+ 	kvm_cpu_cap_mask(CPUID_C000_0001_EDX,
+ 		F(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |
+ 		F(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |
+ 		F(PMM) | F(PMM_EN)
+ 	);
+ }
+ EXPORT_SYMBOL_GPL(kvm_set_cpu_caps);
+ 
++>>>>>>> 93c380e7b528 (KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps)
  struct kvm_cpuid_array {
  	struct kvm_cpuid_entry2 *entries;
  	const int maxnent;
@@@ -534,13 -521,8 +690,18 @@@ static inline int __do_cpuid_func(struc
  		entry->eax = min(entry->eax, 0x1fU);
  		break;
  	case 1:
++<<<<<<< HEAD
 +		entry->edx &= kvm_cpuid_1_edx_x86_features;
 +		cpuid_mask(&entry->edx, CPUID_1_EDX);
 +		entry->ecx &= kvm_cpuid_1_ecx_x86_features;
 +		cpuid_mask(&entry->ecx, CPUID_1_ECX);
 +		/* we support x2apic emulation even if host does not support
 +		 * it since we emulate x2apic in software */
 +		entry->ecx |= F(X2APIC);
++=======
+ 		cpuid_entry_override(entry, CPUID_1_EDX);
+ 		cpuid_entry_override(entry, CPUID_1_ECX);
++>>>>>>> 93c380e7b528 (KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps)
  		break;
  	case 2:
  		/*
@@@ -590,10 -563,14 +751,16 @@@
  		break;
  	/* function 7 has additional index. */
  	case 7:
 -		entry->eax = min(entry->eax, 1u);
 -		cpuid_entry_override(entry, CPUID_7_0_EBX);
 -		cpuid_entry_override(entry, CPUID_7_ECX);
 -		cpuid_entry_override(entry, CPUID_7_EDX);
 +		do_cpuid_7_mask(entry);
  
++<<<<<<< HEAD
 +		for (i = 1, max_idx = entry->eax; i <= max_idx; i++) {
 +			entry = do_host_cpuid(array, function, i);
++=======
+ 		/* KVM only supports 0x7.0 and 0x7.1, capped above via min(). */
+ 		if (entry->eax == 1) {
+ 			entry = do_host_cpuid(array, function, 1);
++>>>>>>> 93c380e7b528 (KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps)
  			if (!entry)
  				goto out;
  
@@@ -764,30 -738,7 +931,34 @@@
  			g_phys_as = phys_as;
  		entry->eax = g_phys_as | (virt_as << 8);
  		entry->edx = 0;
++<<<<<<< HEAD
 +		entry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;
 +		cpuid_mask(&entry->ebx, CPUID_8000_0008_EBX);
 +		/*
 +		 * AMD has separate bits for each SPEC_CTRL bit.
 +		 * arch/x86/kernel/cpu/bugs.c is kind enough to
 +		 * record that in cpufeatures so use them.
 +		 */
 +		if (boot_cpu_has(X86_FEATURE_IBPB))
 +			entry->ebx |= F(AMD_IBPB);
 +		if (boot_cpu_has(X86_FEATURE_IBRS))
 +			entry->ebx |= F(AMD_IBRS);
 +		if (boot_cpu_has(X86_FEATURE_STIBP))
 +			entry->ebx |= F(AMD_STIBP);
 +		if (boot_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD))
 +			entry->ebx |= F(AMD_SSBD);
 +		if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
 +			entry->ebx |= F(AMD_SSB_NO);
 +		/*
 +		 * The preference is to use SPEC CTRL MSR instead of the
 +		 * VIRT_SPEC MSR.
 +		 */
 +		if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&
 +		    !boot_cpu_has(X86_FEATURE_AMD_SSBD))
 +			entry->ebx |= F(VIRT_SSBD);
++=======
+ 		cpuid_entry_override(entry, CPUID_8000_0008_EBX);
++>>>>>>> 93c380e7b528 (KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps)
  		break;
  	}
  	case 0x80000019:
diff --cc arch/x86/kvm/svm.c
index 2c7c88ef7253,0236f2f98cbd..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -1372,6 -1367,27 +1372,30 @@@ static void svm_hardware_teardown(void
  	iopm_base = 0;
  }
  
++<<<<<<< HEAD
++=======
+ static __init void svm_set_cpu_caps(void)
+ {
+ 	kvm_set_cpu_caps();
+ 
+ 	/* CPUID 0x80000001 */
+ 	if (nested)
+ 		kvm_cpu_cap_set(X86_FEATURE_SVM);
+ 
+ 	/* CPUID 0x80000008 */
+ 	if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) ||
+ 	    boot_cpu_has(X86_FEATURE_AMD_SSBD))
+ 		kvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);
+ 
+ 	/* CPUID 0x8000000A */
+ 	/* Support next_rip if host supports it */
+ 	kvm_cpu_cap_check_and_set(X86_FEATURE_NRIPS);
+ 
+ 	if (npt_enabled)
+ 		kvm_cpu_cap_set(X86_FEATURE_NPT);
+ }
+ 
++>>>>>>> 93c380e7b528 (KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps)
  static __init int svm_hardware_setup(void)
  {
  	int cpu;
@@@ -6041,27 -6039,26 +6065,33 @@@ static void svm_cpuid_update(struct kvm
  	if (!kvm_vcpu_apicv_active(vcpu))
  		return;
  
 -	/*
 -	 * AVIC does not work with an x2APIC mode guest. If the X2APIC feature
 -	 * is exposed to the guest, disable AVIC.
 -	 */
 -	if (guest_cpuid_has(vcpu, X86_FEATURE_X2APIC))
 -		kvm_request_apicv_update(vcpu->kvm, false,
 -					 APICV_INHIBIT_REASON_X2APIC);
 -
 -	/*
 -	 * Currently, AVIC does not work with nested virtualization.
 -	 * So, we disable AVIC when cpuid for SVM is set in the L1 guest.
 -	 */
 -	if (nested && guest_cpuid_has(vcpu, X86_FEATURE_SVM))
 -		kvm_request_apicv_update(vcpu->kvm, false,
 -					 APICV_INHIBIT_REASON_NESTED);
 +	guest_cpuid_clear(vcpu, X86_FEATURE_X2APIC);
  }
  
++<<<<<<< HEAD
 +#define F feature_bit
 +
 +static void svm_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
 +{
 +	switch (func) {
 +	case 0x1:
 +		if (avic)
 +			entry->ecx &= ~F(X2APIC);
 +		break;
 +	case 0x80000001:
 +		if (nested)
 +			entry->ecx |= (1 << 2); /* Set SVM bit */
 +		break;
 +	case 0x80000008:
 +		if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) ||
 +		     boot_cpu_has(X86_FEATURE_AMD_SSBD))
 +			entry->ebx |= F(VIRT_SSBD);
 +		break;
++=======
+ static void svm_set_supported_cpuid(struct kvm_cpuid_entry2 *entry)
+ {
+ 	switch (entry->function) {
++>>>>>>> 93c380e7b528 (KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps)
  	case 0x8000000A:
  		entry->eax = 1; /* SVM revision 1 */
  		entry->ebx = 8; /* Lets support 8 ASIDs in case we add proper
diff --cc arch/x86/kvm/vmx/vmx.c
index b4f527d117f1,fe7b4ae867d8..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -7251,10 -7123,40 +7251,14 @@@ static void vmx_cpuid_update(struct kvm
  	}
  }
  
++<<<<<<< HEAD
 +static void vmx_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
++=======
+ static void vmx_set_supported_cpuid(struct kvm_cpuid_entry2 *entry)
++>>>>>>> 93c380e7b528 (KVM: x86: Set emulated/transmuted feature bits via kvm_cpu_caps)
  {
 -}
 -
 -static __init void vmx_set_cpu_caps(void)
 -{
 -	kvm_set_cpu_caps();
 -
 -	/* CPUID 0x1 */
 -	if (nested)
 -		kvm_cpu_cap_set(X86_FEATURE_VMX);
 -
 -	/* CPUID 0x7 */
 -	if (kvm_mpx_supported())
 -		kvm_cpu_cap_check_and_set(X86_FEATURE_MPX);
 -	if (cpu_has_vmx_invpcid())
 -		kvm_cpu_cap_check_and_set(X86_FEATURE_INVPCID);
 -	if (vmx_pt_mode_is_host_guest())
 -		kvm_cpu_cap_check_and_set(X86_FEATURE_INTEL_PT);
 -
 -	/* PKU is not yet implemented for shadow paging. */
 -	if (enable_ept && boot_cpu_has(X86_FEATURE_OSPKE))
 -		kvm_cpu_cap_check_and_set(X86_FEATURE_PKU);
 -
 -	if (vmx_umip_emulated())
 -		kvm_cpu_cap_set(X86_FEATURE_UMIP);
 -
 -	/* CPUID 0xD.1 */
 -	if (!vmx_xsaves_supported())
 -		kvm_cpu_cap_clear(X86_FEATURE_XSAVES);
 -
 -	/* CPUID 0x80000001 */
 -	if (!cpu_has_vmx_rdtscp())
 -		kvm_cpu_cap_clear(X86_FEATURE_RDTSCP);
 +	if (func == 1 && nested)
 +		entry->ecx |= feature_bit(VMX);
  }
  
  static void vmx_request_immediate_exit(struct kvm_vcpu *vcpu)
* Unmerged path arch/x86/kvm/cpuid.c
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
