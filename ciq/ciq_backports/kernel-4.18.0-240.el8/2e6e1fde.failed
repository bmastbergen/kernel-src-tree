io_uring: fix error handling in io_queue_link_head

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 2e6e1fde32d7d41cf076c21060c329d3fdbce25c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/2e6e1fde.failed

In case of an error io_submit_sqe() drops a request and continues
without it, even if the request was a part of a link. Not only it
doesn't cancel links, but also may execute wrong sequence of actions.

Stop consuming sqes, and let the user handle errors.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 2e6e1fde32d7d41cf076c21060c329d3fdbce25c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 8c2ecae32a7b,1fca9e2b6e64..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2285,37 -3315,28 +2285,48 @@@ static int io_queue_link_head(struct io
  
  #define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
  
++<<<<<<< HEAD
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
++=======
+ static bool io_submit_sqe(struct io_kiocb *req, struct io_submit_state *state,
+ 			  struct io_kiocb **link)
++>>>>>>> 2e6e1fde32d7 (io_uring: fix error handling in io_queue_link_head)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
  	int ret;
  
 -	req->user_data = req->sqe->user_data;
 -
  	/* enforce forwards compatibility on users */
 -	if (unlikely(req->sqe->flags & ~SQE_VALID_FLAGS)) {
 +	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
  		ret = -EINVAL;
 -		goto err_req;
 +		goto err;
 +	}
 +
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
  	}
  
 -	ret = io_req_set_file(state, req);
 +	ret = io_req_set_file(ctx, s, state, req);
  	if (unlikely(ret)) {
  err_req:
++<<<<<<< HEAD
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		return;
++=======
+ 		io_cqring_add_event(req, ret);
+ 		io_double_put_req(req);
+ 		return false;
++>>>>>>> 2e6e1fde32d7 (io_uring: fix error handling in io_queue_link_head)
  	}
  
 +	req->user_data = s->sqe->user_data;
 +
  	/*
  	 * If we already have a head request, queue this one for async
  	 * submittal once the head completes. If we don't have a head but
@@@ -2342,8 -3371,10 +2353,10 @@@
  		INIT_LIST_HEAD(&req->link_list);
  		*link = req;
  	} else {
 -		io_queue_sqe(req);
 +		io_queue_sqe(ctx, req, s, force_nonblock);
  	}
+ 
+ 	return true;
  }
  
  /*
@@@ -2439,6 -3482,40 +2452,43 @@@ static int io_submit_sqes(struct io_rin
  	}
  
  	for (i = 0; i < nr; i++) {
++<<<<<<< HEAD
++=======
+ 		struct io_kiocb *req;
+ 		unsigned int sqe_flags;
+ 
+ 		req = io_get_req(ctx, statep);
+ 		if (unlikely(!req)) {
+ 			if (!submitted)
+ 				submitted = -EAGAIN;
+ 			break;
+ 		}
+ 		if (!io_get_sqring(ctx, req)) {
+ 			__io_free_req(req);
+ 			break;
+ 		}
+ 
+ 		if (io_sqe_needs_user(req->sqe) && !*mm) {
+ 			mm_fault = mm_fault || !mmget_not_zero(ctx->sqo_mm);
+ 			if (!mm_fault) {
+ 				use_mm(ctx->sqo_mm);
+ 				*mm = ctx->sqo_mm;
+ 			}
+ 		}
+ 
+ 		submitted++;
+ 		sqe_flags = req->sqe->flags;
+ 
+ 		req->ring_file = ring_file;
+ 		req->ring_fd = ring_fd;
+ 		req->has_user = *mm != NULL;
+ 		req->in_async = async;
+ 		req->needs_fixed_file = async;
+ 		trace_io_uring_submit_sqe(ctx, req->sqe->user_data,
+ 					  true, async);
+ 		if (!io_submit_sqe(req, statep, &link))
+ 			break;
++>>>>>>> 2e6e1fde32d7 (io_uring: fix error handling in io_queue_link_head)
  		/*
  		 * If previous wasn't linked and we have a linked command,
  		 * that's the end of the chain. Submit the previous link.
* Unmerged path fs/io_uring.c
