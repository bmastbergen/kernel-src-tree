io_uring: io_wq_submit_work() should not touch req->rw

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit fd6c2e4c063d64511657ad0031a1677b6a914859
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/fd6c2e4c.failed

I've been chasing a weird and obscure crash that was userspace stack
corruption, and finally narrowed it down to a bit flip that made a
stack address invalid. io_wq_submit_work() unconditionally flips
the req->rw.ki_flags IOCB_NOWAIT bit, but since it's a generic work
handler, this isn't valid. Normal read/write operations own that
part of the request, on other types it could be something else.

Move the IOCB_NOWAIT clear to the read/write handlers where it belongs.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit fd6c2e4c063d64511657ad0031a1677b6a914859)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index ab99aea677bc,6f084e3cf835..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1338,23 -1805,35 +1338,32 @@@ static int io_read(struct io_kiocb *req
  	struct iov_iter iter;
  	struct file *file;
  	size_t iov_count;
 -	ssize_t io_size, ret;
 +	ssize_t read_size, ret;
  
 -	if (!req->io) {
 -		ret = io_read_prep(req, &iovec, &iter, force_nonblock);
 -		if (ret < 0)
 -			return ret;
 -	} else {
 -		ret = io_import_iovec(READ, req, &iovec, &iter);
 -		if (ret < 0)
 -			return ret;
 -	}
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
 +		return ret;
 +	file = kiocb->ki_filp;
 +
++<<<<<<< HEAD
 +	if (unlikely(!(file->f_mode & FMODE_READ)))
 +		return -EBADF;
 +
 +	ret = io_import_iovec(req->ctx, READ, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 +	read_size = ret;
++=======
+ 	/* Ensure we clear previously set non-block flag */
+ 	if (!force_nonblock)
+ 		req->rw.ki_flags &= ~IOCB_NOWAIT;
+ 
+ 	file = req->file;
+ 	io_size = ret;
++>>>>>>> fd6c2e4c063d (io_uring: io_wq_submit_work() should not touch req->rw)
  	if (req->flags & REQ_F_LINK)
 -		req->result = io_size;
 -
 -	/*
 -	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
 -	 * we know to async punt it even if it was opened O_NONBLOCK
 -	 */
 -	if (force_nonblock && !io_file_supports_async(file)) {
 -		req->flags |= REQ_F_MUST_PUNT;
 -		goto copy_iov;
 -	}
 +		req->result = read_size;
  
  	iov_count = iov_iter_count(&iter);
  	ret = rw_verify_area(READ, file, &kiocb->ki_pos, iov_count);
@@@ -1401,33 -1898,42 +1410,37 @@@ static int io_write(struct io_kiocb *re
  	struct iov_iter iter;
  	struct file *file;
  	size_t iov_count;
 -	ssize_t ret, io_size;
 +	ssize_t ret;
  
 -	if (!req->io) {
 -		ret = io_write_prep(req, &iovec, &iter, force_nonblock);
 -		if (ret < 0)
 -			return ret;
 -	} else {
 -		ret = io_import_iovec(WRITE, req, &iovec, &iter);
 -		if (ret < 0)
 -			return ret;
 -	}
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
 +		return ret;
  
+ 	/* Ensure we clear previously set non-block flag */
+ 	if (!force_nonblock)
+ 		req->rw.ki_flags &= ~IOCB_NOWAIT;
+ 
  	file = kiocb->ki_filp;
 -	io_size = ret;
 -	if (req->flags & REQ_F_LINK)
 -		req->result = io_size;
 +	if (unlikely(!(file->f_mode & FMODE_WRITE)))
 +		return -EBADF;
  
 -	/*
 -	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
 -	 * we know to async punt it even if it was opened O_NONBLOCK
 -	 */
 -	if (force_nonblock && !io_file_supports_async(req->file)) {
 -		req->flags |= REQ_F_MUST_PUNT;
 -		goto copy_iov;
 -	}
 +	ret = io_import_iovec(req->ctx, WRITE, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 -	/* file path doesn't support NOWAIT for non-direct_IO */
 -	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&
 -	    (req->flags & REQ_F_ISREG))
 -		goto copy_iov;
 +	if (req->flags & REQ_F_LINK)
 +		req->result = ret;
  
  	iov_count = iov_iter_count(&iter);
 +
 +	ret = -EAGAIN;
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
 +		goto out_free;
 +	}
 +
  	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
@@@ -1950,184 -3266,72 +1963,189 @@@ static int __io_submit_sqe(struct io_ri
  	return 0;
  }
  
 -static void io_link_work_cb(struct io_wq_work **workptr)
 +static struct async_list *io_async_list_from_sqe(struct io_ring_ctx *ctx,
 +						 const struct io_uring_sqe *sqe)
  {
 -	struct io_wq_work *work = *workptr;
 -	struct io_kiocb *link = work->data;
 +	switch (sqe->opcode) {
 +	case IORING_OP_READV:
 +	case IORING_OP_READ_FIXED:
 +		return &ctx->pending_async[READ];
 +	case IORING_OP_WRITEV:
 +	case IORING_OP_WRITE_FIXED:
 +		return &ctx->pending_async[WRITE];
 +	default:
 +		return NULL;
 +	}
 +}
 +
 +static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 +{
 +	u8 opcode = READ_ONCE(sqe->opcode);
  
 -	io_queue_linked_timeout(link);
 -	work->func = io_wq_submit_work;
 +	return !(opcode == IORING_OP_READ_FIXED ||
 +		 opcode == IORING_OP_WRITE_FIXED);
  }
  
 -static void io_wq_submit_work(struct io_wq_work **workptr)
 +static void io_sq_wq_submit_work(struct work_struct *work)
  {
 -	struct io_wq_work *work = *workptr;
  	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -	int ret = 0;
 +	struct io_ring_ctx *ctx = req->ctx;
 +	struct mm_struct *cur_mm = NULL;
 +	struct async_list *async_list;
 +	LIST_HEAD(req_list);
 +	mm_segment_t old_fs;
 +	int ret;
 +
++<<<<<<< HEAD
 +	async_list = io_async_list_from_sqe(ctx, req->submit.sqe);
 +restart:
 +	do {
 +		struct sqe_submit *s = &req->submit;
 +		const struct io_uring_sqe *sqe = s->sqe;
 +		unsigned int flags = req->flags;
  
 +		/* Ensure we clear previously set non-block flag */
 +		req->rw.ki_flags &= ~IOCB_NOWAIT;
++=======
+ 	if (work->flags & IO_WQ_WORK_CANCEL)
+ 		ret = -ECANCELED;
++>>>>>>> fd6c2e4c063d (io_uring: io_wq_submit_work() should not touch req->rw)
  
 -	if (!ret) {
 -		req->has_user = (work->flags & IO_WQ_WORK_HAS_MM) != 0;
 -		req->in_async = true;
 -		do {
 -			ret = io_issue_sqe(req, &nxt, false);
 -			/*
 -			 * We can get EAGAIN for polled IO even though we're
 -			 * forcing a sync submission from here, since we can't
 -			 * wait for request slots on the block side.
 -			 */
 -			if (ret != -EAGAIN)
 -				break;
 -			cond_resched();
 -		} while (1);
 -	}
 +		ret = 0;
 +		if (io_sqe_needs_user(sqe) && !cur_mm) {
 +			if (!mmget_not_zero(ctx->sqo_mm)) {
 +				ret = -EFAULT;
 +			} else {
 +				cur_mm = ctx->sqo_mm;
 +				use_mm(cur_mm);
 +				old_fs = get_fs();
 +				set_fs(USER_DS);
 +			}
 +		}
  
 -	/* drop submission reference */
 -	io_put_req(req);
 +		if (!ret) {
 +			s->has_user = cur_mm != NULL;
 +			s->needs_lock = true;
 +			do {
 +				ret = __io_submit_sqe(ctx, req, s, false);
 +				/*
 +				 * We can get EAGAIN for polled IO even though
 +				 * we're forcing a sync submission from here,
 +				 * since we can't wait for request slots on the
 +				 * block side.
 +				 */
 +				if (ret != -EAGAIN)
 +					break;
 +				cond_resched();
 +			} while (1);
 +		}
  
 -	if (ret) {
 -		req_set_fail_links(req);
 -		io_cqring_add_event(req, ret);
 +		/* drop submission reference */
  		io_put_req(req);
 -	}
  
 -	/* if a dependent link is ready, pass it back */
 -	if (!ret && nxt) {
 -		struct io_kiocb *link;
 +		if (ret) {
 +			io_cqring_add_event(ctx, sqe->user_data, ret);
 +			io_put_req(req);
 +		}
 +
 +		/* async context always use a copy of the sqe */
 +		kfree(sqe);
 +
 +		/* req from defer and link list needn't decrease async cnt */
 +		if (flags & (REQ_F_IO_DRAINED | REQ_F_LINK_DONE))
 +			goto out;
 +
 +		if (!async_list)
 +			break;
 +		if (!list_empty(&req_list)) {
 +			req = list_first_entry(&req_list, struct io_kiocb,
 +						list);
 +			list_del(&req->list);
 +			continue;
 +		}
 +		if (list_empty(&async_list->list))
 +			break;
 +
 +		req = NULL;
 +		spin_lock(&async_list->lock);
 +		if (list_empty(&async_list->list)) {
 +			spin_unlock(&async_list->lock);
 +			break;
 +		}
 +		list_splice_init(&async_list->list, &req_list);
 +		spin_unlock(&async_list->lock);
 +
 +		req = list_first_entry(&req_list, struct io_kiocb, list);
 +		list_del(&req->list);
 +	} while (req);
  
 -		io_prep_async_work(nxt, &link);
 -		*workptr = &nxt->work;
 -		if (link) {
 -			nxt->work.flags |= IO_WQ_WORK_CB;
 -			nxt->work.func = io_link_work_cb;
 -			nxt->work.data = link;
 +	/*
 +	 * Rare case of racing with a submitter. If we find the count has
 +	 * dropped to zero AND we have pending work items, then restart
 +	 * the processing. This is a tiny race window.
 +	 */
 +	if (async_list) {
 +		ret = atomic_dec_return(&async_list->cnt);
 +		while (!ret && !list_empty(&async_list->list)) {
 +			spin_lock(&async_list->lock);
 +			atomic_inc(&async_list->cnt);
 +			list_splice_init(&async_list->list, &req_list);
 +			spin_unlock(&async_list->lock);
 +
 +			if (!list_empty(&req_list)) {
 +				req = list_first_entry(&req_list,
 +							struct io_kiocb, list);
 +				list_del(&req->list);
 +				goto restart;
 +			}
 +			ret = atomic_dec_return(&async_list->cnt);
  		}
  	}
 +
 +out:
 +	if (cur_mm) {
 +		set_fs(old_fs);
 +		unuse_mm(cur_mm);
 +		mmput(cur_mm);
 +	}
  }
  
 -static bool io_req_op_valid(int op)
 +/*
 + * See if we can piggy back onto previously submitted work, that is still
 + * running. We currently only allow this if the new request is sequential
 + * to the previous one we punted.
 + */
 +static bool io_add_to_prev_work(struct async_list *list, struct io_kiocb *req)
  {
 -	return op >= IORING_OP_NOP && op < IORING_OP_LAST;
 +	bool ret;
 +
 +	if (!list)
 +		return false;
 +	if (!(req->flags & REQ_F_SEQ_PREV))
 +		return false;
 +	if (!atomic_read(&list->cnt))
 +		return false;
 +
 +	ret = true;
 +	spin_lock(&list->lock);
 +	list_add_tail(&req->list, &list->list);
 +	/*
 +	 * Ensure we see a simultaneous modification from io_sq_wq_submit_work()
 +	 */
 +	smp_mb();
 +	if (!atomic_read(&list->cnt)) {
 +		list_del_init(&req->list);
 +		ret = false;
 +	}
 +	spin_unlock(&list->lock);
 +	return ret;
  }
  
 -static int io_req_needs_file(struct io_kiocb *req)
 +static bool io_op_needs_file(const struct io_uring_sqe *sqe)
  {
 -	switch (req->opcode) {
 +	int op = READ_ONCE(sqe->opcode);
 +
 +	switch (op) {
  	case IORING_OP_NOP:
  	case IORING_OP_POLL_REMOVE:
  	case IORING_OP_TIMEOUT:
* Unmerged path fs/io_uring.c
