RDMA/mlx5: Avoid double lookups on the pagefault path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit b70d785d237c0d3e4235c511f38f8ce64620f945
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b70d785d.failed

Now that the locking is simplified combine pagefault_implicit_mr() with
implicit_mr_get_data() so that we sweep over the idx range only once,
and do the single xlt update at the end, after the child umems are
setup.

This avoids double iteration/xa_loads plus the sketchy failure path if the
xa_load() fails.

Link: https://lore.kernel.org/r/20191009160934.3143-12-jgg@ziepe.ca
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit b70d785d237c0d3e4235c511f38f8ce64620f945)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/odp.c
index df3038ca913b,8bd30db87c21..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -421,137 -342,83 +421,140 @@@ static void mlx5_ib_page_fault_resume(s
  			    wq_num, err);
  }
  
 -static struct mlx5_ib_mr *implicit_get_child_mr(struct mlx5_ib_mr *imr,
 -						unsigned long idx)
 +static struct mlx5_ib_mr *implicit_mr_alloc(struct ib_pd *pd,
 +					    struct ib_umem *umem,
 +					    bool ksm, int access_flags)
  {
 -	struct ib_umem_odp *odp;
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
  	struct mlx5_ib_mr *mr;
 -	struct mlx5_ib_mr *ret;
  	int err;
  
 -	odp = ib_umem_odp_alloc_child(to_ib_umem_odp(imr->umem),
 -				      idx * MLX5_IMR_MTT_SIZE,
 -				      MLX5_IMR_MTT_SIZE);
 -	if (IS_ERR(odp))
 -		return ERR_CAST(odp);
 +	mr = mlx5_mr_cache_alloc(dev, ksm ? MLX5_IMR_KSM_CACHE_ENTRY :
 +					    MLX5_IMR_MTT_CACHE_ENTRY);
  
 -	ret = mr = mlx5_mr_cache_alloc(imr->dev, MLX5_IMR_MTT_CACHE_ENTRY);
  	if (IS_ERR(mr))
 -		goto out_umem;
 +		return mr;
  
 -	err = xa_reserve(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),
 -			 GFP_KERNEL);
 -	if (err) {
 -		ret = ERR_PTR(err);
 -		goto out_mr;
 +	mr->ibmr.pd = pd;
 +
 +	mr->dev = dev;
 +	mr->access_flags = access_flags;
 +	mr->mmkey.iova = 0;
 +	mr->umem = umem;
 +
 +	if (ksm) {
 +		err = mlx5_ib_update_xlt(mr, 0,
 +					 mlx5_imr_ksm_entries,
 +					 MLX5_KSM_PAGE_SHIFT,
 +					 MLX5_IB_UPD_XLT_INDIRECT |
 +					 MLX5_IB_UPD_XLT_ZAP |
 +					 MLX5_IB_UPD_XLT_ENABLE);
 +
 +	} else {
 +		err = mlx5_ib_update_xlt(mr, 0,
 +					 MLX5_IMR_MTT_ENTRIES,
 +					 PAGE_SHIFT,
 +					 MLX5_IB_UPD_XLT_ZAP |
 +					 MLX5_IB_UPD_XLT_ENABLE |
 +					 MLX5_IB_UPD_XLT_ATOMIC);
  	}
  
 -	mr->ibmr.pd = imr->ibmr.pd;
 -	mr->access_flags = imr->access_flags;
 -	mr->umem = &odp->umem;
 +	if (err)
 +		goto fail;
 +
  	mr->ibmr.lkey = mr->mmkey.key;
  	mr->ibmr.rkey = mr->mmkey.key;
 -	mr->mmkey.iova = idx * MLX5_IMR_MTT_SIZE;
 -	mr->parent = imr;
 -	odp->private = mr;
 -	INIT_WORK(&odp->work, mr_leaf_free_action);
 -
 -	err = mlx5_ib_update_xlt(mr, 0,
 -				 MLX5_IMR_MTT_ENTRIES,
 -				 PAGE_SHIFT,
 -				 MLX5_IB_UPD_XLT_ZAP |
 -				 MLX5_IB_UPD_XLT_ENABLE);
 -	if (err) {
 -		ret = ERR_PTR(err);
 -		goto out_release;
 -	}
  
 -	/*
 -	 * Once the store to either xarray completes any error unwind has to
 -	 * use synchronize_srcu(). Avoid this with xa_reserve()
 -	 */
 -	ret = xa_cmpxchg(&imr->implicit_children, idx, NULL, mr, GFP_KERNEL);
 -	if (unlikely(ret)) {
 -		if (xa_is_err(ret)) {
 -			ret = ERR_PTR(xa_err(ret));
 -			goto out_release;
 +	mr->live = 1;
 +
 +	mlx5_ib_dbg(dev, "key %x dev %p mr %p\n",
 +		    mr->mmkey.key, dev->mdev, mr);
 +
 +	return mr;
 +
 +fail:
 +	mlx5_ib_err(dev, "Failed to register MKEY %d\n", err);
 +	mlx5_mr_cache_free(dev, mr);
 +
 +	return ERR_PTR(err);
 +}
 +
++<<<<<<< HEAD
 +static struct ib_umem_odp *implicit_mr_get_data(struct mlx5_ib_mr *mr,
 +						u64 io_virt, size_t bcnt)
 +{
 +	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.pd->device);
 +	struct ib_umem_odp *odp, *result = NULL;
 +	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
 +	u64 addr = io_virt & MLX5_IMR_MTT_MASK;
 +	int nentries = 0, start_idx = 0, ret;
 +	struct mlx5_ib_mr *mtt;
 +
 +	mutex_lock(&odp_mr->umem_mutex);
 +	odp = odp_lookup(addr, 1, mr);
 +
 +	mlx5_ib_dbg(dev, "io_virt:%llx bcnt:%zx addr:%llx odp:%p\n",
 +		    io_virt, bcnt, addr, odp);
 +
 +next_mr:
 +	if (likely(odp)) {
 +		if (nentries)
 +			nentries++;
 +	} else {
 +		odp = ib_alloc_odp_umem(odp_mr, addr,
 +					MLX5_IMR_MTT_SIZE);
 +		if (IS_ERR(odp)) {
 +			mutex_unlock(&odp_mr->umem_mutex);
 +			return ERR_CAST(odp);
  		}
 -		/*
 -		 * Another thread beat us to creating the child mr, use
 -		 * theirs.
 -		 */
 -		goto out_release;
 +
 +		mtt = implicit_mr_alloc(mr->ibmr.pd, &odp->umem, 0,
 +					mr->access_flags);
 +		if (IS_ERR(mtt)) {
 +			mutex_unlock(&odp_mr->umem_mutex);
 +			ib_umem_release(&odp->umem);
 +			return ERR_CAST(mtt);
 +		}
 +
 +		odp->private = mtt;
 +		mtt->umem = &odp->umem;
 +		mtt->mmkey.iova = addr;
 +		mtt->parent = mr;
 +		INIT_WORK(&odp->work, mr_leaf_free_action);
 +
 +		if (!nentries)
 +			start_idx = addr >> MLX5_IMR_MTT_SHIFT;
 +		nentries++;
  	}
  
 -	xa_store(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key),
 -		 &mr->mmkey, GFP_ATOMIC);
 +	/* Return first odp if region not covered by single one */
 +	if (likely(!result))
 +		result = odp;
  
 -	mlx5_ib_dbg(imr->dev, "key %x mr %p\n", mr->mmkey.key, mr);
 -	return mr;
 +	addr += MLX5_IMR_MTT_SIZE;
 +	if (unlikely(addr < io_virt + bcnt)) {
 +		odp = odp_next(odp);
 +		if (odp && ib_umem_start(odp) != addr)
 +			odp = NULL;
 +		goto next_mr;
 +	}
  
 -out_release:
 -	xa_release(&imr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
 -out_mr:
 -	mlx5_mr_cache_free(imr->dev, mr);
 -out_umem:
 -	ib_umem_odp_release(odp);
 -	return ret;
 +	if (unlikely(nentries)) {
 +		ret = mlx5_ib_update_xlt(mr, start_idx, nentries, 0,
 +					 MLX5_IB_UPD_XLT_INDIRECT |
 +					 MLX5_IB_UPD_XLT_ATOMIC);
 +		if (ret) {
 +			mlx5_ib_err(dev, "Failed to update PAS\n");
 +			result = ERR_PTR(ret);
 +		}
 +	}
 +
 +	mutex_unlock(&odp_mr->umem_mutex);
 +	return result;
  }
  
++=======
++>>>>>>> b70d785d237c (RDMA/mlx5: Avoid double lookups on the pagefault path)
  struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
  					     struct ib_udata *udata,
  					     int access_flags)
@@@ -734,6 -585,109 +737,112 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int pagefault_implicit_mr(struct mlx5_ib_mr *imr,
+ 				 struct ib_umem_odp *odp_imr, u64 user_va,
+ 				 size_t bcnt, u32 *bytes_mapped, u32 flags)
+ {
+ 	unsigned long end_idx = (user_va + bcnt - 1) >> MLX5_IMR_MTT_SHIFT;
+ 	unsigned long upd_start_idx = end_idx + 1;
+ 	unsigned long upd_len = 0;
+ 	unsigned long npages = 0;
+ 	int err;
+ 	int ret;
+ 
+ 	if (unlikely(user_va >= mlx5_imr_ksm_entries * MLX5_IMR_MTT_SIZE ||
+ 		     mlx5_imr_ksm_entries * MLX5_IMR_MTT_SIZE - user_va < bcnt))
+ 		return -EFAULT;
+ 
+ 	/* Fault each child mr that intersects with our interval. */
+ 	while (bcnt) {
+ 		unsigned long idx = user_va >> MLX5_IMR_MTT_SHIFT;
+ 		struct ib_umem_odp *umem_odp;
+ 		struct mlx5_ib_mr *mtt;
+ 		u64 len;
+ 
+ 		mtt = xa_load(&imr->implicit_children, idx);
+ 		if (unlikely(!mtt)) {
+ 			mtt = implicit_get_child_mr(imr, idx);
+ 			if (IS_ERR(mtt)) {
+ 				ret = PTR_ERR(mtt);
+ 				goto out;
+ 			}
+ 			upd_start_idx = min(upd_start_idx, idx);
+ 			upd_len = idx - upd_start_idx + 1;
+ 		}
+ 
+ 		umem_odp = to_ib_umem_odp(mtt->umem);
+ 		len = min_t(u64, user_va + bcnt, ib_umem_end(umem_odp)) -
+ 		      user_va;
+ 
+ 		ret = pagefault_real_mr(mtt, umem_odp, user_va, len,
+ 					bytes_mapped, flags);
+ 		if (ret < 0)
+ 			goto out;
+ 		user_va += len;
+ 		bcnt -= len;
+ 		npages += ret;
+ 	}
+ 
+ 	ret = npages;
+ 
+ 	/*
+ 	 * Any time the implicit_children are changed we must perform an
+ 	 * update of the xlt before exiting to ensure the HW and the
+ 	 * implicit_children remains synchronized.
+ 	 */
+ out:
+ 	if (likely(!upd_len))
+ 		return ret;
+ 
+ 	/*
+ 	 * Notice this is not strictly ordered right, the KSM is updated after
+ 	 * the implicit_children is updated, so a parallel page fault could
+ 	 * see a MR that is not yet visible in the KSM.  This is similar to a
+ 	 * parallel page fault seeing a MR that is being concurrently removed
+ 	 * from the KSM. Both of these improbable situations are resolved
+ 	 * safely by resuming the HW and then taking another page fault. The
+ 	 * next pagefault handler will see the new information.
+ 	 */
+ 	mutex_lock(&odp_imr->umem_mutex);
+ 	err = mlx5_ib_update_xlt(imr, upd_start_idx, upd_len, 0,
+ 				 MLX5_IB_UPD_XLT_INDIRECT |
+ 					 MLX5_IB_UPD_XLT_ATOMIC);
+ 	mutex_unlock(&odp_imr->umem_mutex);
+ 	if (err) {
+ 		mlx5_ib_err(imr->dev, "Failed to update PAS\n");
+ 		return err;
+ 	}
+ 	return ret;
+ }
+ 
+ /*
+  * Returns:
+  *  -EFAULT: The io_virt->bcnt is not within the MR, it covers pages that are
+  *           not accessible, or the MR is no longer valid.
+  *  -EAGAIN/-ENOMEM: The operation should be retried
+  *
+  *  -EINVAL/others: General internal malfunction
+  *  >0: Number of pages mapped
+  */
+ static int pagefault_mr(struct mlx5_ib_mr *mr, u64 io_virt, size_t bcnt,
+ 			u32 *bytes_mapped, u32 flags)
+ {
+ 	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
+ 
+ 	if (!odp->is_implicit_odp) {
+ 		if (unlikely(io_virt < ib_umem_start(odp) ||
+ 			     ib_umem_end(odp) - io_virt < bcnt))
+ 			return -EFAULT;
+ 		return pagefault_real_mr(mr, odp, io_virt, bcnt, bytes_mapped,
+ 					 flags);
+ 	}
+ 	return pagefault_implicit_mr(mr, odp, io_virt, bcnt, bytes_mapped,
+ 				     flags);
+ }
+ 
++>>>>>>> b70d785d237c (RDMA/mlx5: Avoid double lookups on the pagefault path)
  struct pf_frame {
  	struct pf_frame *next;
  	u32 key;
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
