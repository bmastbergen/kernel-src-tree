io_uring: ensure async punted read/write requests copy iovec

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit f67676d160c6ee2ed82917fadfed6d29cab8237c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/f67676d1.failed

Currently we don't copy the iovecs when we punt to async context. This
can be problematic for applications that store the iovec on the stack,
as they often assume that it's safe to let the iovec go out of scope
as soon as IO submission has been called. This isn't always safe, as we
will re-copy the iovec once we're in async context.

Make this 100% safe by copying the iovec just once. With this change,
applications may safely store the iovec on the stack for all cases.

	Reported-by: 李通洲 <carter.li@eoitek.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit f67676d160c6ee2ed82917fadfed6d29cab8237c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 671f4f0982a1,1689aea55527..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -305,7 -292,34 +305,38 @@@ struct io_poll_iocb 
  	__poll_t			events;
  	bool				done;
  	bool				canceled;
++<<<<<<< HEAD
 +	struct wait_queue_entry		wait;
++=======
+ 	struct wait_queue_entry		*wait;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	struct io_timeout_data		*data;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	struct io_uring_sqe		sqe;
+ 	union {
+ 		struct io_async_rw	rw;
+ 	};
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  };
  
  /*
@@@ -1036,8 -1422,8 +1067,13 @@@ static int io_prep_rw(struct io_kiocb *
  	if (!req->file)
  		return -EBADF;
  
++<<<<<<< HEAD
 +	if (force_nonblock && !io_file_supports_async(req->file))
 +		force_nonblock = false;
++=======
+ 	if (S_ISREG(file_inode(req->file)->i_mode))
+ 		req->flags |= REQ_F_ISREG;
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  
  	kiocb->ki_pos = READ_ONCE(sqe->off);
  	kiocb->ki_flags = iocb_flags(kiocb->ki_filp);
@@@ -1194,14 -1588,22 +1230,28 @@@ static ssize_t io_import_iovec(struct i
  	 * flag.
  	 */
  	opcode = READ_ONCE(sqe->opcode);
 -	if (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {
 +	if (opcode == IORING_OP_READ_FIXED ||
 +	    opcode == IORING_OP_WRITE_FIXED) {
 +		ssize_t ret = io_import_fixed(ctx, rw, sqe, iter);
  		*iovec = NULL;
 -		return io_import_fixed(req->ctx, rw, sqe, iter);
 +		return ret;
  	}
  
++<<<<<<< HEAD
 +	if (!s->has_user)
++=======
+ 	if (req->io) {
+ 		struct io_async_rw *iorw = &req->io->rw;
+ 
+ 		*iovec = iorw->iov;
+ 		iov_iter_init(iter, rw, *iovec, iorw->nr_segs, iorw->size);
+ 		if (iorw->iov == iorw->fast_iov)
+ 			*iovec = NULL;
+ 		return iorw->size;
+ 	}
+ 
+ 	if (!req->has_user)
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		return -EFAULT;
  
  #ifdef CONFIG_COMPAT
@@@ -1330,7 -1673,51 +1380,55 @@@ static ssize_t loop_rw_iter(int rw, str
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
++=======
+ static void io_req_map_io(struct io_kiocb *req, ssize_t io_size,
+ 			  struct iovec *iovec, struct iovec *fast_iov,
+ 			  struct iov_iter *iter)
+ {
+ 	req->io->rw.nr_segs = iter->nr_segs;
+ 	req->io->rw.size = io_size;
+ 	req->io->rw.iov = iovec;
+ 	if (!req->io->rw.iov) {
+ 		req->io->rw.iov = req->io->rw.fast_iov;
+ 		memcpy(req->io->rw.iov, fast_iov,
+ 			sizeof(struct iovec) * iter->nr_segs);
+ 	}
+ }
+ 
+ static int io_setup_async_io(struct io_kiocb *req, ssize_t io_size,
+ 			     struct iovec *iovec, struct iovec *fast_iov,
+ 			     struct iov_iter *iter)
+ {
+ 	req->io = kmalloc(sizeof(*req->io), GFP_KERNEL);
+ 	if (req->io) {
+ 		io_req_map_io(req, io_size, iovec, fast_iov, iter);
+ 		memcpy(&req->io->sqe, req->sqe, sizeof(req->io->sqe));
+ 		req->sqe = &req->io->sqe;
+ 		return 0;
+ 	}
+ 
+ 	return -ENOMEM;
+ }
+ 
+ static int io_read_prep(struct io_kiocb *req, struct iovec **iovec,
+ 			struct iov_iter *iter, bool force_nonblock)
+ {
+ 	ssize_t ret;
+ 
+ 	ret = io_prep_rw(req, force_nonblock);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (unlikely(!(req->file->f_mode & FMODE_READ)))
+ 		return -EBADF;
+ 
+ 	return io_import_iovec(READ, req, iovec, iter);
+ }
+ 
+ static int io_read(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		   bool force_nonblock)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
@@@ -1338,23 -1725,31 +1436,47 @@@
  	struct iov_iter iter;
  	struct file *file;
  	size_t iov_count;
- 	ssize_t read_size, ret;
+ 	ssize_t io_size, ret;
  
++<<<<<<< HEAD
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
 +		return ret;
 +	file = kiocb->ki_filp;
 +
 +	if (unlikely(!(file->f_mode & FMODE_READ)))
 +		return -EBADF;
 +
 +	ret = io_import_iovec(req->ctx, READ, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
 +
 +	read_size = ret;
++=======
+ 	if (!req->io) {
+ 		ret = io_read_prep(req, &iovec, &iter, force_nonblock);
+ 		if (ret < 0)
+ 			return ret;
+ 	} else {
+ 		ret = io_import_iovec(READ, req, &iovec, &iter);
+ 		if (ret < 0)
+ 			return ret;
+ 	}
+ 
+ 	file = req->file;
+ 	io_size = ret;
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  	if (req->flags & REQ_F_LINK)
- 		req->result = read_size;
+ 		req->result = io_size;
+ 
+ 	/*
+ 	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
+ 	 * we know to async punt it even if it was opened O_NONBLOCK
+ 	 */
+ 	if (force_nonblock && !io_file_supports_async(file)) {
+ 		req->flags |= REQ_F_MUST_PUNT;
+ 		goto copy_iov;
+ 	}
  
  	iov_count = iov_iter_count(&iter);
  	ret = rw_verify_area(READ, file, &kiocb->ki_pos, iov_count);
@@@ -1374,26 -1769,43 +1496,63 @@@
  		 * need async punt anyway, so it's more efficient to do it
  		 * here.
  		 */
++<<<<<<< HEAD
 +		if (force_nonblock && ret2 > 0 && ret2 < read_size)
 +			ret2 = -EAGAIN;
 +		/* Catch -EAGAIN return for forced non-blocking submission */
 +		if (!force_nonblock || ret2 != -EAGAIN) {
 +			io_rw_done(kiocb, ret2);
 +		} else {
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(READ, req, iov_count);
 +			ret = -EAGAIN;
++=======
+ 		if (force_nonblock && !(req->flags & REQ_F_NOWAIT) &&
+ 		    (req->flags & REQ_F_ISREG) &&
+ 		    ret2 > 0 && ret2 < io_size)
+ 			ret2 = -EAGAIN;
+ 		/* Catch -EAGAIN return for forced non-blocking submission */
+ 		if (!force_nonblock || ret2 != -EAGAIN) {
+ 			kiocb_done(kiocb, ret2, nxt, req->in_async);
+ 		} else {
+ copy_iov:
+ 			ret = io_setup_async_io(req, io_size, iovec,
+ 						inline_vecs, &iter);
+ 			if (ret)
+ 				goto out_free;
+ 			return -EAGAIN;
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		}
  	}
+ out_free:
  	kfree(iovec);
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_write(struct io_kiocb *req, const struct sqe_submit *s,
++=======
+ static int io_write_prep(struct io_kiocb *req, struct iovec **iovec,
+ 			 struct iov_iter *iter, bool force_nonblock)
+ {
+ 	ssize_t ret;
+ 
+ 	ret = io_prep_rw(req, force_nonblock);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (unlikely(!(req->file->f_mode & FMODE_WRITE)))
+ 		return -EBADF;
+ 
+ 	return io_import_iovec(WRITE, req, iovec, iter);
+ }
+ 
+ static int io_write(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		    bool force_nonblock)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
@@@ -1401,33 -1813,36 +1560,63 @@@
  	struct iov_iter iter;
  	struct file *file;
  	size_t iov_count;
- 	ssize_t ret;
+ 	ssize_t ret, io_size;
  
++<<<<<<< HEAD
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
 +		return ret;
 +
 +	file = kiocb->ki_filp;
 +	if (unlikely(!(file->f_mode & FMODE_WRITE)))
 +		return -EBADF;
 +
 +	ret = io_import_iovec(req->ctx, WRITE, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
 +
++=======
+ 	if (!req->io) {
+ 		ret = io_write_prep(req, &iovec, &iter, force_nonblock);
+ 		if (ret < 0)
+ 			return ret;
+ 	} else {
+ 		ret = io_import_iovec(WRITE, req, &iovec, &iter);
+ 		if (ret < 0)
+ 			return ret;
+ 	}
+ 
+ 	file = kiocb->ki_filp;
+ 	io_size = ret;
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  	if (req->flags & REQ_F_LINK)
- 		req->result = ret;
+ 		req->result = io_size;
+ 
+ 	/*
+ 	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
+ 	 * we know to async punt it even if it was opened O_NONBLOCK
+ 	 */
+ 	if (force_nonblock && !io_file_supports_async(req->file)) {
+ 		req->flags |= REQ_F_MUST_PUNT;
+ 		goto copy_iov;
+ 	}
+ 
+ 	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT))
+ 		goto copy_iov;
  
  	iov_count = iov_iter_count(&iter);
++<<<<<<< HEAD
 +
 +	ret = -EAGAIN;
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
 +		goto out_free;
 +	}
 +
++=======
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
@@@ -1452,15 -1867,14 +1641,26 @@@
  		else
  			ret2 = loop_rw_iter(WRITE, file, kiocb, &iter);
  		if (!force_nonblock || ret2 != -EAGAIN) {
++<<<<<<< HEAD
 +			io_rw_done(kiocb, ret2);
 +		} else {
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(WRITE, req, iov_count);
 +			ret = -EAGAIN;
++=======
+ 			kiocb_done(kiocb, ret2, nxt, req->in_async);
+ 		} else {
+ copy_iov:
+ 			ret = io_setup_async_io(req, io_size, iovec,
+ 						inline_vecs, &iter);
+ 			if (ret)
+ 				goto out_free;
+ 			return -EAGAIN;
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		}
  	}
  out_free:
@@@ -1853,16 -2416,336 +2053,337 @@@ static int io_poll_add(struct io_kiocb 
  	return ipt.error;
  }
  
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 +	struct io_uring_sqe *sqe_copy;
  
++<<<<<<< HEAD
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
++=======
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * We could be racing with timeout deletion. If the list is empty,
+ 	 * then timeout lookup already found it and will be handling it.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		struct io_kiocb *prev;
+ 
+ 		/*
+ 		 * Adjust the reqs sequence before the current one because it
+ 		 * will consume a slot in the cq_ring and the the cq_tail
+ 		 * pointer will be increased, otherwise other timeout reqs may
+ 		 * return in advance without waiting for enough wait_nr.
+ 		 */
+ 		prev = req;
+ 		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 			prev->sequence++;
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	io_cqring_fill_event(req, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	if (req->flags & REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req(req);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
+ {
+ 	struct io_kiocb *req;
+ 	int ret = -ENOENT;
+ 
+ 	list_for_each_entry(req, &ctx->timeout_list, list) {
+ 		if (user_data == req->user_data) {
+ 			list_del_init(&req->list);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (ret == -ENOENT)
+ 		return ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->timeout.data->timer);
+ 	if (ret == -1)
+ 		return -EALREADY;
+ 
+ 	if (req->flags & REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_cqring_fill_event(req, -ECANCELED);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ /*
+  * Remove or update an existing timeout command
+  */
+ static int io_timeout_remove(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	unsigned flags;
+ 	int ret;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags)
+ 		return -EINVAL;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	ret = io_timeout_cancel(ctx, READ_ONCE(sqe->addr));
+ 
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	io_cqring_ev_posted(ctx);
+ 	if (ret < 0 && req->flags & REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_setup(struct io_kiocb *req)
+ {
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct io_timeout_data *data;
+ 	unsigned flags;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 
+ 	data = kzalloc(sizeof(struct io_timeout_data), GFP_KERNEL);
+ 	if (!data)
+ 		return -ENOMEM;
+ 	data->req = req;
+ 	req->timeout.data = data;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	if (flags & IORING_TIMEOUT_ABS)
+ 		data->mode = HRTIMER_MODE_ABS;
+ 	else
+ 		data->mode = HRTIMER_MODE_REL;
+ 
+ 	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
+ 	return 0;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_timeout_data *data;
+ 	struct list_head *entry;
+ 	unsigned span = 0;
+ 	int ret;
+ 
+ 	ret = io_timeout_setup(req);
+ 	/* common setup allows flags (like links) set, we don't */
+ 	if (!ret && sqe->flags)
+ 		ret = -EINVAL;
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied. If it isn't set, then this is
+ 	 * a pure timeout request, sequence isn't used.
+ 	 */
+ 	count = READ_ONCE(sqe->off);
+ 	if (!count) {
+ 		req->flags |= REQ_F_TIMEOUT_NOSEQ;
+ 		spin_lock_irq(&ctx->completion_lock);
+ 		entry = ctx->timeout_list.prev;
+ 		goto add;
+ 	}
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	req->timeout.data->seq_offset = count;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 		u32 nxt_offset = nxt->timeout.data->seq_offset;
+ 
+ 		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
+ 			continue;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt_offset + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ add:
+ 	list_add(&req->list, entry);
+ 	data = req->timeout.data;
+ 	data->timer.function = io_timeout_fn;
+ 	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	return 0;
+ }
+ 
+ static bool io_cancel_cb(struct io_wq_work *work, void *data)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	return req->user_data == (unsigned long) data;
+ }
+ 
+ static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
+ {
+ 	enum io_wq_cancel cancel_ret;
+ 	int ret = 0;
+ 
+ 	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
+ 	switch (cancel_ret) {
+ 	case IO_WQ_CANCEL_OK:
+ 		ret = 0;
+ 		break;
+ 	case IO_WQ_CANCEL_RUNNING:
+ 		ret = -EALREADY;
+ 		break;
+ 	case IO_WQ_CANCEL_NOTFOUND:
+ 		ret = -ENOENT;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
+ 				     struct io_kiocb *req, __u64 sqe_addr,
+ 				     struct io_kiocb **nxt, int success_ret)
+ {
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
+ 	if (ret != -ENOENT) {
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		goto done;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	ret = io_timeout_cancel(ctx, sqe_addr);
+ 	if (ret != -ENOENT)
+ 		goto done;
+ 	ret = io_poll_cancel(ctx, sqe_addr);
+ done:
+ 	if (!ret)
+ 		ret = success_ret;
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	if (ret < 0 && (req->flags & REQ_F_LINK))
+ 		req->flags |= REQ_F_FAIL_LINK;
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static int io_async_cancel(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			   struct io_kiocb **nxt)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
+ 	    sqe->cancel_flags)
+ 		return -EINVAL;
+ 
+ 	io_async_find_and_cancel(ctx, req, READ_ONCE(sqe->addr), nxt, 0);
+ 	return 0;
+ }
+ 
+ static int io_req_defer_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
+ 	struct iov_iter iter;
+ 	ssize_t ret;
+ 
+ 	memcpy(&io->sqe, req->sqe, sizeof(io->sqe));
+ 	req->sqe = &io->sqe;
+ 
+ 	switch (io->sqe.opcode) {
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 		ret = io_read_prep(req, &iovec, &iter, true);
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 		ret = io_write_prep(req, &iovec, &iter, true);
+ 		break;
+ 	default:
+ 		req->io = io;
+ 		return 0;
+ 	}
+ 
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	req->io = io;
+ 	io_req_map_io(req, ret, iovec, inline_vecs, &iter);
+ 	return 0;
+ }
+ 
+ static int io_req_defer(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_async_ctx *io;
+ 	int ret;
+ 
+ 	/* Still need defer if there is pending req in defer list. */
+ 	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		return 0;
  
 -	io = kmalloc(sizeof(*io), GFP_KERNEL);
 -	if (!io)
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
  		return -EAGAIN;
  
  	spin_lock_irq(&ctx->completion_lock);
@@@ -1872,10 -2755,11 +2393,16 @@@
  		return 0;
  	}
  
++<<<<<<< HEAD
 +	memcpy(sqe_copy, sqe, sizeof(*sqe_copy));
 +	req->submit.sqe = sqe_copy;
++=======
+ 	ret = io_req_defer_prep(req, io);
+ 	if (ret < 0)
+ 		return ret;
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  
 -	trace_io_uring_defer(ctx, req, req->user_data);
 +	INIT_WORK(&req->work, io_sq_wq_submit_work);
  	list_add_tail(&req->list, &ctx->defer_list);
  	spin_unlock_irq(&ctx->completion_lock);
  	return -EIOCBQUEUED;
@@@ -2178,37 -2973,138 +2705,50 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static int io_grab_files(struct io_kiocb *req)
 -{
 -	int ret = -EBADF;
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	rcu_read_lock();
 -	spin_lock_irq(&ctx->inflight_lock);
 -	/*
 -	 * We use the f_ops->flush() handler to ensure that we can flush
 -	 * out work accessing these files if the fd is closed. Check if
 -	 * the fd has changed since we started down this path, and disallow
 -	 * this operation if it has.
 -	 */
 -	if (fcheck(req->ring_fd) == req->ring_file) {
 -		list_add(&req->inflight_entry, &ctx->inflight_list);
 -		req->flags |= REQ_F_INFLIGHT;
 -		req->work.files = current->files;
 -		ret = 0;
 -	}
 -	spin_unlock_irq(&ctx->inflight_lock);
 -	rcu_read_unlock();
 -
 -	return ret;
 -}
 -
 -static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
 -{
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_kiocb *prev = NULL;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -
 -	/*
 -	 * We don't expect the list to be empty, that will only happen if we
 -	 * race with the completion of the linked work.
 -	 */
 -	if (!list_empty(&req->list)) {
 -		prev = list_entry(req->list.prev, struct io_kiocb, link_list);
 -		if (refcount_inc_not_zero(&prev->refs)) {
 -			list_del_init(&req->list);
 -			prev->flags &= ~REQ_F_LINK_TIMEOUT;
 -		} else
 -			prev = NULL;
 -	}
 -
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -
 -	if (prev) {
 -		if (prev->flags & REQ_F_LINK)
 -			prev->flags |= REQ_F_FAIL_LINK;
 -		io_async_find_and_cancel(ctx, req, prev->user_data, NULL,
 -						-ETIME);
 -		io_put_req(prev);
 -	} else {
 -		io_cqring_add_event(req, -ETIME);
 -		io_put_req(req);
 -	}
 -	return HRTIMER_NORESTART;
 -}
 -
 -static void io_queue_linked_timeout(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	/*
 -	 * If the list is now empty, then our linked request finished before
 -	 * we got a chance to setup the timer
 -	 */
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (!list_empty(&req->list)) {
 -		struct io_timeout_data *data = req->timeout.data;
 -
 -		data->timer.function = io_link_timeout_fn;
 -		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
 -				data->mode);
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	/* drop submission reference */
 -	io_put_req(req);
 -}
 -
 -static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
 -{
 -	struct io_kiocb *nxt;
 -
 -	if (!(req->flags & REQ_F_LINK))
 -		return NULL;
 -
 -	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
 -	if (!nxt || nxt->sqe->opcode != IORING_OP_LINK_TIMEOUT)
 -		return NULL;
 -
 -	req->flags |= REQ_F_LINK_TIMEOUT;
 -	return nxt;
 -}
 -
 -static void __io_queue_sqe(struct io_kiocb *req)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_kiocb *linked_timeout = io_prep_linked_timeout(req);
 -	struct io_kiocb *nxt = NULL;
  	int ret;
  
 -	ret = io_issue_sqe(req, &nxt, true);
 -	if (nxt)
 -		io_queue_async_work(nxt);
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
 +		struct io_uring_sqe *sqe_copy;
 +
++<<<<<<< HEAD
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (sqe_copy) {
 +			struct async_list *list;
 +
 +			s->sqe = sqe_copy;
 +			memcpy(&req->submit, s, sizeof(*s));
 +			list = io_async_list_from_sqe(ctx, s->sqe);
 +			if (!io_add_to_prev_work(list, req)) {
 +				if (list)
 +					atomic_inc(&list->cnt);
 +				INIT_WORK(&req->work, io_sq_wq_submit_work);
 +				io_queue_async_work(ctx, req);
 +			}
  
 +			/*
 +			 * Queued up for async execution, worker will release
 +			 * submit reference when the iocb is actually submitted.
 +			 */
 +			return 0;
++=======
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		if (req->work.flags & IO_WQ_WORK_NEEDS_FILES) {
+ 			ret = io_grab_files(req);
+ 			if (ret)
+ 				goto err;
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		}
 -
 -		/*
 -		 * Queued up for async execution, worker will release
 -		 * submit reference when the iocb is actually submitted.
 -		 */
 -		io_queue_async_work(req);
 -		return;
  	}
  
 -err:
  	/* drop submission reference */
  	io_put_req(req);
  
@@@ -2332,13 -3211,14 +2872,20 @@@ err
  			goto err_req;
  		}
  
++<<<<<<< HEAD
 +		s->sqe = sqe_copy;
 +		memcpy(&req->submit, s, sizeof(*s));
++=======
+ 		ret = io_req_defer_prep(req, io);
+ 		if (ret)
+ 			goto err_req;
+ 		trace_io_uring_link(ctx, req, prev);
++>>>>>>> f67676d160c6 (io_uring: ensure async punted read/write requests copy iovec)
  		list_add_tail(&req->list, &prev->link_list);
 -	} else if (req->sqe->flags & IOSQE_IO_LINK) {
 +	} else if (s->sqe->flags & IOSQE_IO_LINK) {
  		req->flags |= REQ_F_LINK;
  
 +		memcpy(&req->submit, s, sizeof(*s));
  		INIT_LIST_HEAD(&req->link_list);
  		*link = req;
  	} else {
* Unmerged path fs/io_uring.c
