io_uring: remove 'fd is io_uring' from close path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 904fbcb115c85090484dfdffaf7f461d96fe8e53
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/904fbcb1.failed

The attempt protecting us from closing the ring itself wasn't really
complete, and we actually don't need it. The referencing of requests
themselve, and the references they hold on the ring, ensures that the
life time of the ring is sane. With the check removed, we can also
remove the need to have the close operation fget() the file.

	Reported-by: Al Viro <viro@zeniv.linux.org.uk>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 904fbcb115c85090484dfdffaf7f461d96fe8e53)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 047c6a5f549f,9fd1257c8404..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -366,8 -673,194 +366,199 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ 	/* needs ->fs */
+ 	unsigned		needs_fs : 1;
+ 	/* set if opcode supports polled "wait" */
+ 	unsigned		pollin : 1;
+ 	unsigned		pollout : 1;
+ 	/* op supports buffer selection */
+ 	unsigned		buffer_select : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_fs		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_SPLICE] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_PROVIDE_BUFFERS] = {},
+ 	[IORING_OP_REMOVE_BUFFERS] = {},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_cleanup_req(struct io_kiocb *req);
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 		       int fd, struct file **out_file, bool fixed);
+ static void __io_queue_sqe(struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe);
++>>>>>>> 904fbcb115c8 (io_uring: remove 'fd is io_uring' from close path)
  
  static struct kmem_cache *req_cachep;
  
@@@ -1511,38 -2817,647 +1702,472 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
 -static bool io_req_cancelled(struct io_kiocb *req)
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		    bool force_nonblock)
  {
 -	if (req->work.flags & IO_WQ_WORK_CANCEL) {
 -		req_set_fail_links(req);
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_put_req(req);
 -		return true;
 -	}
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
 +	int ret;
  
 -	return false;
 -}
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
  
 -static void __io_fsync(struct io_kiocb *req)
 -{
 -	loff_t end = req->sync.off + req->sync.len;
 -	int ret;
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
  
 -	ret = vfs_fsync_range(req->file, req->sync.off,
 +	/* fsync always requires a blocking context */
 +	if (force_nonblock)
 +		return -EAGAIN;
 +
 +	ret = vfs_fsync_range(req->rw.ki_filp, sqe_off,
  				end > 0 ? end : LLONG_MAX,
 -				req->sync.flags & IORING_FSYNC_DATASYNC);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 +				fsync_flags & IORING_FSYNC_DATASYNC);
 +
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
  	io_put_req(req);
 +	return 0;
  }
  
 -static void io_fsync_finish(struct io_wq_work **workptr)
++<<<<<<< HEAD
++=======
++static int io_openat(struct io_kiocb *req, bool force_nonblock)
+ {
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_fsync(req);
 -	io_steal_work(req, workptr);
++	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
++	return io_openat2(req, force_nonblock);
+ }
+ 
 -static int io_fsync(struct io_kiocb *req, bool force_nonblock)
 -{
 -	/* fsync always requires a blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_fsync_finish;
 -		return -EAGAIN;
 -	}
 -	__io_fsync(req);
 -	return 0;
 -}
 -
 -static void __io_fallocate(struct io_kiocb *req)
 -{
 -	int ret;
 -
 -	current->signal->rlim[RLIMIT_FSIZE].rlim_cur = req->fsize;
 -	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
 -				req->sync.len);
 -	current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -}
 -
 -static void io_fallocate_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_fallocate(req);
 -	io_steal_work(req, workptr);
 -}
 -
 -static int io_fallocate_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 -
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->addr);
 -	req->sync.mode = READ_ONCE(sqe->len);
 -	req->fsize = rlimit(RLIMIT_FSIZE);
 -	return 0;
 -}
 -
 -static int io_fallocate(struct io_kiocb *req, bool force_nonblock)
 -{
 -	/* fallocate always requiring blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_fallocate_finish;
 -		return -EAGAIN;
 -	}
 -
 -	__io_fallocate(req);
 -	return 0;
 -}
 -
 -static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	const char __user *fname;
 -	int ret;
 -
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (req->flags & REQ_F_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	req->open.how.mode = READ_ONCE(sqe->len);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	req->open.how.flags = READ_ONCE(sqe->open_flags);
 -	if (force_o_largefile())
 -		req->open.how.flags |= O_LARGEFILE;
 -
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 -	}
 -
 -	req->open.nofile = rlimit(RLIMIT_NOFILE);
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	return 0;
 -}
 -
 -static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	struct open_how __user *how;
 -	const char __user *fname;
 -	size_t len;
 -	int ret;
 -
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (req->flags & REQ_F_FIXED_FILE)
 -		return -EBADF;
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	len = READ_ONCE(sqe->len);
 -
 -	if (len < OPEN_HOW_SIZE_VER0)
 -		return -EINVAL;
 -
 -	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
 -					len);
 -	if (ret)
 -		return ret;
 -
 -	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
 -		req->open.how.flags |= O_LARGEFILE;
 -
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 -	}
 -
 -	req->open.nofile = rlimit(RLIMIT_NOFILE);
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	return 0;
 -}
 -
 -static int io_openat2(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct open_flags op;
 -	struct file *file;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 -
 -	ret = build_open_flags(&req->open.how, &op);
 -	if (ret)
 -		goto err;
 -
 -	ret = __get_unused_fd_flags(req->open.how.flags, req->open.nofile);
 -	if (ret < 0)
 -		goto err;
 -
 -	file = do_filp_open(req->open.dfd, req->open.filename, &op);
 -	if (IS_ERR(file)) {
 -		put_unused_fd(ret);
 -		ret = PTR_ERR(file);
 -	} else {
 -		fsnotify_open(file);
 -		fd_install(ret, file);
 -	}
 -err:
 -	putname(req->open.filename);
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_openat(struct io_kiocb *req, bool force_nonblock)
 -{
 -	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
 -	return io_openat2(req, force_nonblock);
 -}
 -
 -static int io_remove_buffers_prep(struct io_kiocb *req,
 -				  const struct io_uring_sqe *sqe)
++static int io_remove_buffers_prep(struct io_kiocb *req,
++				  const struct io_uring_sqe *sqe)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	u64 tmp;
+ 
+ 	if (sqe->ioprio || sqe->rw_flags || sqe->addr || sqe->len || sqe->off)
+ 		return -EINVAL;
+ 
+ 	tmp = READ_ONCE(sqe->fd);
+ 	if (!tmp || tmp > USHRT_MAX)
+ 		return -EINVAL;
+ 
+ 	memset(p, 0, sizeof(*p));
+ 	p->nbufs = tmp;
+ 	p->bgid = READ_ONCE(sqe->buf_group);
+ 	return 0;
+ }
+ 
+ static int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,
+ 			       int bgid, unsigned nbufs)
+ {
+ 	unsigned i = 0;
+ 
+ 	/* shouldn't happen */
+ 	if (!nbufs)
+ 		return 0;
+ 
+ 	/* the head kbuf is the list itself */
+ 	while (!list_empty(&buf->list)) {
+ 		struct io_buffer *nxt;
+ 
+ 		nxt = list_first_entry(&buf->list, struct io_buffer, list);
+ 		list_del(&nxt->list);
+ 		kfree(nxt);
+ 		if (++i == nbufs)
+ 			return i;
+ 	}
+ 	i++;
+ 	kfree(buf);
+ 	idr_remove(&ctx->io_buffer_idr, bgid);
+ 
+ 	return i;
+ }
+ 
+ static int io_remove_buffers(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_buffer *head;
+ 	int ret = 0;
+ 
+ 	io_ring_submit_lock(ctx, !force_nonblock);
+ 
+ 	lockdep_assert_held(&ctx->uring_lock);
+ 
+ 	ret = -ENOENT;
+ 	head = idr_find(&ctx->io_buffer_idr, p->bgid);
+ 	if (head)
+ 		ret = __io_remove_buffers(ctx, head, p->bgid, p->nbufs);
+ 
+ 	io_ring_submit_lock(ctx, !force_nonblock);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_provide_buffers_prep(struct io_kiocb *req,
+ 				   const struct io_uring_sqe *sqe)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	u64 tmp;
+ 
+ 	if (sqe->ioprio || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	tmp = READ_ONCE(sqe->fd);
+ 	if (!tmp || tmp > USHRT_MAX)
+ 		return -E2BIG;
+ 	p->nbufs = tmp;
+ 	p->addr = READ_ONCE(sqe->addr);
+ 	p->len = READ_ONCE(sqe->len);
+ 
+ 	if (!access_ok(u64_to_user_ptr(p->addr), p->len))
+ 		return -EFAULT;
+ 
+ 	p->bgid = READ_ONCE(sqe->buf_group);
+ 	tmp = READ_ONCE(sqe->off);
+ 	if (tmp > USHRT_MAX)
+ 		return -E2BIG;
+ 	p->bid = tmp;
+ 	return 0;
+ }
+ 
+ static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)
+ {
+ 	struct io_buffer *buf;
+ 	u64 addr = pbuf->addr;
+ 	int i, bid = pbuf->bid;
+ 
+ 	for (i = 0; i < pbuf->nbufs; i++) {
+ 		buf = kmalloc(sizeof(*buf), GFP_KERNEL);
+ 		if (!buf)
+ 			break;
+ 
+ 		buf->addr = addr;
+ 		buf->len = pbuf->len;
+ 		buf->bid = bid;
+ 		addr += pbuf->len;
+ 		bid++;
+ 		if (!*head) {
+ 			INIT_LIST_HEAD(&buf->list);
+ 			*head = buf;
+ 		} else {
+ 			list_add_tail(&buf->list, &(*head)->list);
+ 		}
+ 	}
+ 
+ 	return i ? i : -ENOMEM;
+ }
+ 
+ static int io_provide_buffers(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_buffer *head, *list;
+ 	int ret = 0;
+ 
+ 	io_ring_submit_lock(ctx, !force_nonblock);
+ 
+ 	lockdep_assert_held(&ctx->uring_lock);
+ 
+ 	list = head = idr_find(&ctx->io_buffer_idr, p->bgid);
+ 
+ 	ret = io_add_buffers(p, &head);
+ 	if (ret < 0)
+ 		goto out;
+ 
+ 	if (!list) {
+ 		ret = idr_alloc(&ctx->io_buffer_idr, head, p->bgid, p->bgid + 1,
+ 					GFP_KERNEL);
+ 		if (ret < 0) {
+ 			__io_remove_buffers(ctx, head, p->bgid, -1U);
+ 			goto out;
+ 		}
+ 	}
+ out:
+ 	io_ring_submit_unlock(ctx, !force_nonblock);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_epoll_ctl_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	req->epoll.epfd = READ_ONCE(sqe->fd);
+ 	req->epoll.op = READ_ONCE(sqe->len);
+ 	req->epoll.fd = READ_ONCE(sqe->off);
+ 
+ 	if (ep_op_has_event(req->epoll.op)) {
+ 		struct epoll_event __user *ev;
+ 
+ 		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
+ 			return -EFAULT;
+ 	}
+ 
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_epoll_ctl(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	struct io_epoll *ie = &req->epoll;
+ 	int ret;
+ 
+ 	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
+ 	if (force_nonblock && ret == -EAGAIN)
+ 		return -EAGAIN;
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	if (sqe->ioprio || sqe->buf_index || sqe->off)
+ 		return -EINVAL;
+ 
+ 	req->madvise.addr = READ_ONCE(sqe->addr);
+ 	req->madvise.len = READ_ONCE(sqe->len);
+ 	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	struct io_madvise *ma = &req->madvise;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	ret = do_madvise(ma->addr, ma->len, ma->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->addr)
+ 		return -EINVAL;
+ 
+ 	req->fadvise.offset = READ_ONCE(sqe->off);
+ 	req->fadvise.len = READ_ONCE(sqe->len);
+ 	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ }
+ 
+ static int io_fadvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_fadvise *fa = &req->fadvise;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		switch (fa->advice) {
+ 		case POSIX_FADV_NORMAL:
+ 		case POSIX_FADV_RANDOM:
+ 		case POSIX_FADV_SEQUENTIAL:
+ 			break;
+ 		default:
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	unsigned lookup_flags;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (req->flags & REQ_F_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.mask = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	req->open.how.flags = READ_ONCE(sqe->statx_flags);
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
+ 		return -EINVAL;
+ 
+ 	req->open.filename = getname_flags(fname, lookup_flags, NULL);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_statx(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_open *ctx = &req->open;
+ 	unsigned lookup_flags;
+ 	struct path path;
+ 	struct kstat stat;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		/* only need file table for an actual valid fd */
+ 		if (ctx->dfd == -1 || ctx->dfd == AT_FDCWD)
+ 			req->flags |= REQ_F_NO_FILE_TABLE;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
+ 		return -EINVAL;
+ 
+ retry:
+ 	/* filename_lookup() drops it, keep a reference */
+ 	ctx->filename->refcnt++;
+ 
+ 	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
+ 				NULL);
+ 	if (ret)
+ 		goto err;
+ 
+ 	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
+ 	path_put(&path);
+ 	if (retry_estale(ret, lookup_flags)) {
+ 		lookup_flags |= LOOKUP_REVAL;
+ 		goto retry;
+ 	}
+ 	if (!ret)
+ 		ret = cp_statx(&stat, ctx->buffer);
+ err:
+ 	putname(ctx->filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * If we queue this for async, it must not be cancellable. That would
+ 	 * leave the 'file' in an undeterminate state.
+ 	 */
+ 	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
+ 
+ 	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
+ 	    sqe->rw_flags || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (req->flags & REQ_F_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->close.fd = READ_ONCE(sqe->fd);
+ 	return 0;
+ }
+ 
+ /* only called when __close_fd_get_file() is done */
+ static void __io_close_finish(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = filp_close(req->close.put_file, req->work.files);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	fput(req->close.put_file);
+ 	io_put_req(req);
+ }
+ 
+ static void io_close_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 
+ 	/* not cancellable, don't do io_req_cancelled() */
+ 	__io_close_finish(req);
+ 	io_steal_work(req, workptr);
+ }
+ 
+ static int io_close(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	int ret;
+ 
+ 	req->close.put_file = NULL;
+ 	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
+ 	if (ret < 0) {
+ 		if (ret == -ENOENT)
+ 			ret = -EBADF;
+ 		return ret;
+ 	}
+ 
+ 	/* if the file has a flush method, be safe and punt to async */
+ 	if (req->close.put_file->f_op->flush && force_nonblock) {
+ 		/* submission ref will be dropped, take it for async */
+ 		refcount_inc(&req->refs);
+ 
+ 		req->work.func = io_close_finish;
+ 		/*
+ 		 * Do manual async queue here to avoid grabbing files - we don't
+ 		 * need the files, and it'll cause io_close_finish() to close
+ 		 * the file again and cause a double CQE entry for this request
+ 		 */
+ 		io_queue_async_work(req);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * No ->flush(), safely close from here and just punt the
+ 	 * fput() to async context.
+ 	 */
+ 	__io_close_finish(req);
+ 	return 0;
+ }
+ 
++>>>>>>> 904fbcb115c8 (io_uring: remove 'fd is io_uring' from close path)
  static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
  	struct io_ring_ctx *ctx = req->ctx;
* Unmerged path fs/io_uring.c
