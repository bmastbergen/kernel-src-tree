dm crypt: use crypt_integrity_aead() helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Yang Yingliang <yangyingliang@huawei.com>
commit 3fd53533a8bcc5a7f1fa275e28dfb6b05f28a941
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/3fd53533.failed

Replace test_bit(CRYPT_MODE_INTEGRITY_AEAD, XXX) with
crypt_integrity_aead().

	Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 3fd53533a8bcc5a7f1fa275e28dfb6b05f28a941)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-crypt.c
diff --cc drivers/md/dm-crypt.c
index 93902cb15f57,3df90daba89e..000000000000
--- a/drivers/md/dm-crypt.c
+++ b/drivers/md/dm-crypt.c
@@@ -479,8 -345,14 +481,19 @@@ static int crypt_iv_essiv_gen(struct cr
  static int crypt_iv_benbi_ctr(struct crypt_config *cc, struct dm_target *ti,
  			      const char *opts)
  {
++<<<<<<< HEAD
 +	unsigned bs = crypto_skcipher_blocksize(any_tfm(cc));
 +	int log = ilog2(bs);
++=======
+ 	unsigned bs;
+ 	int log;
+ 
+ 	if (crypt_integrity_aead(cc))
+ 		bs = crypto_aead_blocksize(any_tfm_aead(cc));
+ 	else
+ 		bs = crypto_skcipher_blocksize(any_tfm(cc));
+ 	log = ilog2(bs);
++>>>>>>> 3fd53533a8bc (dm crypt: use crypt_integrity_aead() helper)
  
  	/* we need to calculate how far we must shift the sector count
  	 * to get the cipher block count, we use this shift in _gen */
@@@ -841,6 -711,333 +854,336 @@@ static int crypt_iv_random_gen(struct c
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int crypt_iv_eboiv_ctr(struct crypt_config *cc, struct dm_target *ti,
+ 			    const char *opts)
+ {
+ 	if (crypt_integrity_aead(cc)) {
+ 		ti->error = "AEAD transforms not supported for EBOIV";
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (crypto_skcipher_blocksize(any_tfm(cc)) != cc->iv_size) {
+ 		ti->error = "Block size of EBOIV cipher does "
+ 			    "not match IV size of block cipher";
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int crypt_iv_eboiv_gen(struct crypt_config *cc, u8 *iv,
+ 			    struct dm_crypt_request *dmreq)
+ {
+ 	u8 buf[MAX_CIPHER_BLOCKSIZE] __aligned(__alignof__(__le64));
+ 	struct skcipher_request *req;
+ 	struct scatterlist src, dst;
+ 	struct crypto_wait wait;
+ 	int err;
+ 
+ 	req = skcipher_request_alloc(any_tfm(cc), GFP_NOIO);
+ 	if (!req)
+ 		return -ENOMEM;
+ 
+ 	memset(buf, 0, cc->iv_size);
+ 	*(__le64 *)buf = cpu_to_le64(dmreq->iv_sector * cc->sector_size);
+ 
+ 	sg_init_one(&src, page_address(ZERO_PAGE(0)), cc->iv_size);
+ 	sg_init_one(&dst, iv, cc->iv_size);
+ 	skcipher_request_set_crypt(req, &src, &dst, cc->iv_size, buf);
+ 	skcipher_request_set_callback(req, 0, crypto_req_done, &wait);
+ 	err = crypto_wait_req(crypto_skcipher_encrypt(req), &wait);
+ 	skcipher_request_free(req);
+ 
+ 	return err;
+ }
+ 
+ static void crypt_iv_elephant_dtr(struct crypt_config *cc)
+ {
+ 	struct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;
+ 
+ 	crypto_free_skcipher(elephant->tfm);
+ 	elephant->tfm = NULL;
+ }
+ 
+ static int crypt_iv_elephant_ctr(struct crypt_config *cc, struct dm_target *ti,
+ 			    const char *opts)
+ {
+ 	struct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;
+ 	int r;
+ 
+ 	elephant->tfm = crypto_alloc_skcipher("ecb(aes)", 0, 0);
+ 	if (IS_ERR(elephant->tfm)) {
+ 		r = PTR_ERR(elephant->tfm);
+ 		elephant->tfm = NULL;
+ 		return r;
+ 	}
+ 
+ 	r = crypt_iv_eboiv_ctr(cc, ti, NULL);
+ 	if (r)
+ 		crypt_iv_elephant_dtr(cc);
+ 	return r;
+ }
+ 
+ static void diffuser_disk_to_cpu(u32 *d, size_t n)
+ {
+ #ifndef __LITTLE_ENDIAN
+ 	int i;
+ 
+ 	for (i = 0; i < n; i++)
+ 		d[i] = le32_to_cpu((__le32)d[i]);
+ #endif
+ }
+ 
+ static void diffuser_cpu_to_disk(__le32 *d, size_t n)
+ {
+ #ifndef __LITTLE_ENDIAN
+ 	int i;
+ 
+ 	for (i = 0; i < n; i++)
+ 		d[i] = cpu_to_le32((u32)d[i]);
+ #endif
+ }
+ 
+ static void diffuser_a_decrypt(u32 *d, size_t n)
+ {
+ 	int i, i1, i2, i3;
+ 
+ 	for (i = 0; i < 5; i++) {
+ 		i1 = 0;
+ 		i2 = n - 2;
+ 		i3 = n - 5;
+ 
+ 		while (i1 < (n - 1)) {
+ 			d[i1] += d[i2] ^ (d[i3] << 9 | d[i3] >> 23);
+ 			i1++; i2++; i3++;
+ 
+ 			if (i3 >= n)
+ 				i3 -= n;
+ 
+ 			d[i1] += d[i2] ^ d[i3];
+ 			i1++; i2++; i3++;
+ 
+ 			if (i2 >= n)
+ 				i2 -= n;
+ 
+ 			d[i1] += d[i2] ^ (d[i3] << 13 | d[i3] >> 19);
+ 			i1++; i2++; i3++;
+ 
+ 			d[i1] += d[i2] ^ d[i3];
+ 			i1++; i2++; i3++;
+ 		}
+ 	}
+ }
+ 
+ static void diffuser_a_encrypt(u32 *d, size_t n)
+ {
+ 	int i, i1, i2, i3;
+ 
+ 	for (i = 0; i < 5; i++) {
+ 		i1 = n - 1;
+ 		i2 = n - 2 - 1;
+ 		i3 = n - 5 - 1;
+ 
+ 		while (i1 > 0) {
+ 			d[i1] -= d[i2] ^ d[i3];
+ 			i1--; i2--; i3--;
+ 
+ 			d[i1] -= d[i2] ^ (d[i3] << 13 | d[i3] >> 19);
+ 			i1--; i2--; i3--;
+ 
+ 			if (i2 < 0)
+ 				i2 += n;
+ 
+ 			d[i1] -= d[i2] ^ d[i3];
+ 			i1--; i2--; i3--;
+ 
+ 			if (i3 < 0)
+ 				i3 += n;
+ 
+ 			d[i1] -= d[i2] ^ (d[i3] << 9 | d[i3] >> 23);
+ 			i1--; i2--; i3--;
+ 		}
+ 	}
+ }
+ 
+ static void diffuser_b_decrypt(u32 *d, size_t n)
+ {
+ 	int i, i1, i2, i3;
+ 
+ 	for (i = 0; i < 3; i++) {
+ 		i1 = 0;
+ 		i2 = 2;
+ 		i3 = 5;
+ 
+ 		while (i1 < (n - 1)) {
+ 			d[i1] += d[i2] ^ d[i3];
+ 			i1++; i2++; i3++;
+ 
+ 			d[i1] += d[i2] ^ (d[i3] << 10 | d[i3] >> 22);
+ 			i1++; i2++; i3++;
+ 
+ 			if (i2 >= n)
+ 				i2 -= n;
+ 
+ 			d[i1] += d[i2] ^ d[i3];
+ 			i1++; i2++; i3++;
+ 
+ 			if (i3 >= n)
+ 				i3 -= n;
+ 
+ 			d[i1] += d[i2] ^ (d[i3] << 25 | d[i3] >> 7);
+ 			i1++; i2++; i3++;
+ 		}
+ 	}
+ }
+ 
+ static void diffuser_b_encrypt(u32 *d, size_t n)
+ {
+ 	int i, i1, i2, i3;
+ 
+ 	for (i = 0; i < 3; i++) {
+ 		i1 = n - 1;
+ 		i2 = 2 - 1;
+ 		i3 = 5 - 1;
+ 
+ 		while (i1 > 0) {
+ 			d[i1] -= d[i2] ^ (d[i3] << 25 | d[i3] >> 7);
+ 			i1--; i2--; i3--;
+ 
+ 			if (i3 < 0)
+ 				i3 += n;
+ 
+ 			d[i1] -= d[i2] ^ d[i3];
+ 			i1--; i2--; i3--;
+ 
+ 			if (i2 < 0)
+ 				i2 += n;
+ 
+ 			d[i1] -= d[i2] ^ (d[i3] << 10 | d[i3] >> 22);
+ 			i1--; i2--; i3--;
+ 
+ 			d[i1] -= d[i2] ^ d[i3];
+ 			i1--; i2--; i3--;
+ 		}
+ 	}
+ }
+ 
+ static int crypt_iv_elephant(struct crypt_config *cc, struct dm_crypt_request *dmreq)
+ {
+ 	struct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;
+ 	u8 *es, *ks, *data, *data2, *data_offset;
+ 	struct skcipher_request *req;
+ 	struct scatterlist *sg, *sg2, src, dst;
+ 	struct crypto_wait wait;
+ 	int i, r;
+ 
+ 	req = skcipher_request_alloc(elephant->tfm, GFP_NOIO);
+ 	es = kzalloc(16, GFP_NOIO); /* Key for AES */
+ 	ks = kzalloc(32, GFP_NOIO); /* Elephant sector key */
+ 
+ 	if (!req || !es || !ks) {
+ 		r = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	*(__le64 *)es = cpu_to_le64(dmreq->iv_sector * cc->sector_size);
+ 
+ 	/* E(Ks, e(s)) */
+ 	sg_init_one(&src, es, 16);
+ 	sg_init_one(&dst, ks, 16);
+ 	skcipher_request_set_crypt(req, &src, &dst, 16, NULL);
+ 	skcipher_request_set_callback(req, 0, crypto_req_done, &wait);
+ 	r = crypto_wait_req(crypto_skcipher_encrypt(req), &wait);
+ 	if (r)
+ 		goto out;
+ 
+ 	/* E(Ks, e'(s)) */
+ 	es[15] = 0x80;
+ 	sg_init_one(&dst, &ks[16], 16);
+ 	r = crypto_wait_req(crypto_skcipher_encrypt(req), &wait);
+ 	if (r)
+ 		goto out;
+ 
+ 	sg = crypt_get_sg_data(cc, dmreq->sg_out);
+ 	data = kmap_atomic(sg_page(sg));
+ 	data_offset = data + sg->offset;
+ 
+ 	/* Cannot modify original bio, copy to sg_out and apply Elephant to it */
+ 	if (bio_data_dir(dmreq->ctx->bio_in) == WRITE) {
+ 		sg2 = crypt_get_sg_data(cc, dmreq->sg_in);
+ 		data2 = kmap_atomic(sg_page(sg2));
+ 		memcpy(data_offset, data2 + sg2->offset, cc->sector_size);
+ 		kunmap_atomic(data2);
+ 	}
+ 
+ 	if (bio_data_dir(dmreq->ctx->bio_in) != WRITE) {
+ 		diffuser_disk_to_cpu((u32*)data_offset, cc->sector_size / sizeof(u32));
+ 		diffuser_b_decrypt((u32*)data_offset, cc->sector_size / sizeof(u32));
+ 		diffuser_a_decrypt((u32*)data_offset, cc->sector_size / sizeof(u32));
+ 		diffuser_cpu_to_disk((__le32*)data_offset, cc->sector_size / sizeof(u32));
+ 	}
+ 
+ 	for (i = 0; i < (cc->sector_size / 32); i++)
+ 		crypto_xor(data_offset + i * 32, ks, 32);
+ 
+ 	if (bio_data_dir(dmreq->ctx->bio_in) == WRITE) {
+ 		diffuser_disk_to_cpu((u32*)data_offset, cc->sector_size / sizeof(u32));
+ 		diffuser_a_encrypt((u32*)data_offset, cc->sector_size / sizeof(u32));
+ 		diffuser_b_encrypt((u32*)data_offset, cc->sector_size / sizeof(u32));
+ 		diffuser_cpu_to_disk((__le32*)data_offset, cc->sector_size / sizeof(u32));
+ 	}
+ 
+ 	kunmap_atomic(data);
+ out:
+ 	kzfree(ks);
+ 	kzfree(es);
+ 	skcipher_request_free(req);
+ 	return r;
+ }
+ 
+ static int crypt_iv_elephant_gen(struct crypt_config *cc, u8 *iv,
+ 			    struct dm_crypt_request *dmreq)
+ {
+ 	int r;
+ 
+ 	if (bio_data_dir(dmreq->ctx->bio_in) == WRITE) {
+ 		r = crypt_iv_elephant(cc, dmreq);
+ 		if (r)
+ 			return r;
+ 	}
+ 
+ 	return crypt_iv_eboiv_gen(cc, iv, dmreq);
+ }
+ 
+ static int crypt_iv_elephant_post(struct crypt_config *cc, u8 *iv,
+ 				  struct dm_crypt_request *dmreq)
+ {
+ 	if (bio_data_dir(dmreq->ctx->bio_in) != WRITE)
+ 		return crypt_iv_elephant(cc, dmreq);
+ 
+ 	return 0;
+ }
+ 
+ static int crypt_iv_elephant_init(struct crypt_config *cc)
+ {
+ 	struct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;
+ 	int key_offset = cc->key_size - cc->key_extra_size;
+ 
+ 	return crypto_skcipher_setkey(elephant->tfm, &cc->key[key_offset], cc->key_extra_size);
+ }
+ 
+ static int crypt_iv_elephant_wipe(struct crypt_config *cc)
+ {
+ 	struct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;
+ 	u8 key[ELEPHANT_MAX_KEY_SIZE];
+ 
+ 	memset(key, 0, cc->key_extra_size);
+ 	return crypto_skcipher_setkey(elephant->tfm, key, cc->key_extra_size);
+ }
+ 
++>>>>>>> 3fd53533a8bc (dm crypt: use crypt_integrity_aead() helper)
  static const struct crypt_iv_operations crypt_iv_plain_ops = {
  	.generator = crypt_iv_plain_gen
  };
* Unmerged path drivers/md/dm-crypt.c
