blk-mq: open code __blk_mq_alloc_request in blk_mq_alloc_request_hctx

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 600c3b0cea784aaba77df3ed4a6b4f2ebfa935ce
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/600c3b0c.failed

blk_mq_alloc_request_hctx is only used for NVMeoF connect commands, so
tailor it to the specific requirements, and don't bother the general
fast path code with its special twinkles.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Hannes Reinecke <hare@suse.de
	Reviewed-by: Daniel Wagner <dwagner@suse.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 600c3b0cea784aaba77df3ed4a6b4f2ebfa935ce)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 260262d3164a,560ef5df8993..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -327,24 -332,32 +327,34 @@@ static struct request *blk_mq_rq_ctx_in
  	return rq;
  }
  
 -static struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data)
 +static struct request *blk_mq_get_request(struct request_queue *q,
 +					  struct bio *bio,
 +					  struct blk_mq_alloc_data *data)
  {
 -	struct request_queue *q = data->q;
  	struct elevator_queue *e = q->elevator;
++<<<<<<< HEAD
 +	struct request *rq;
 +	unsigned int tag;
 +	bool clear_ctx_on_error = false;
 +
 +	blk_queue_enter_live(q);
 +	data->q = q;
 +	if (likely(!data->ctx)) {
 +		data->ctx = blk_mq_get_ctx(q);
 +		clear_ctx_on_error = true;
 +	}
 +	if (likely(!data->hctx))
 +		data->hctx = blk_mq_map_queue(q, data->cmd_flags,
 +						data->ctx);
++=======
+ 	u64 alloc_time_ns = 0;
+ 	unsigned int tag;
+ 
+ 	/* alloc_time includes depth and tag waits */
+ 	if (blk_queue_rq_alloc_time(q))
+ 		alloc_time_ns = ktime_get_ns();
+ 
++>>>>>>> 600c3b0cea78 (blk-mq: open code __blk_mq_alloc_request in blk_mq_alloc_request_hctx)
  	if (data->cmd_flags & REQ_NOWAIT)
  		data->flags |= BLK_MQ_REQ_NOWAIT;
  
@@@ -360,31 -373,17 +370,40 @@@
  		    e->type->ops.limit_depth &&
  		    !(data->flags & BLK_MQ_REQ_RESERVED))
  			e->type->ops.limit_depth(data->cmd_flags, data);
- 	} else {
- 		blk_mq_tag_busy(data->hctx);
  	}
  
+ 	data->ctx = blk_mq_get_ctx(q);
+ 	data->hctx = blk_mq_map_queue(q, data->cmd_flags, data->ctx);
+ 	if (!(data->flags & BLK_MQ_REQ_INTERNAL))
+ 		blk_mq_tag_busy(data->hctx);
+ 
  	tag = blk_mq_get_tag(data);
++<<<<<<< HEAD
 +	if (tag == BLK_MQ_NO_TAG) {
 +		if (clear_ctx_on_error)
 +			data->ctx = NULL;
 +		blk_queue_exit(q);
 +		return NULL;
 +	}
 +
 +	rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
 +	if (!op_is_flush(data->cmd_flags)) {
 +		rq->elv.icq = NULL;
 +		if (e && e->type->ops.prepare_request) {
 +			if (e->type->icq_cache)
 +				blk_mq_sched_assign_ioc(rq);
 +
 +			e->type->ops.prepare_request(rq, bio);
 +			rq->rq_flags |= RQF_ELVPRIV;
 +		}
 +	}
 +	data->hctx->queued++;
 +	return rq;
++=======
+ 	if (tag == BLK_MQ_NO_TAG)
+ 		return NULL;
+ 	return blk_mq_rq_ctx_init(data, tag, alloc_time_ns);
++>>>>>>> 600c3b0cea78 (blk-mq: open code __blk_mq_alloc_request in blk_mq_alloc_request_hctx)
  }
  
  struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
@@@ -412,11 -417,20 +431,25 @@@ EXPORT_SYMBOL(blk_mq_alloc_request)
  struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
  	unsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)
  {
++<<<<<<< HEAD
 +	struct blk_mq_alloc_data alloc_data = { .flags = flags, .cmd_flags = op };
 +	struct request *rq;
++=======
+ 	struct blk_mq_alloc_data data = {
+ 		.q		= q,
+ 		.flags		= flags,
+ 		.cmd_flags	= op,
+ 	};
+ 	u64 alloc_time_ns = 0;
++>>>>>>> 600c3b0cea78 (blk-mq: open code __blk_mq_alloc_request in blk_mq_alloc_request_hctx)
  	unsigned int cpu;
+ 	unsigned int tag;
  	int ret;
  
+ 	/* alloc_time includes depth and tag waits */
+ 	if (blk_queue_rq_alloc_time(q))
+ 		alloc_time_ns = ktime_get_ns();
+ 
  	/*
  	 * If the tag allocator sleeps we could get an allocation for a
  	 * different hardware context.  No need to complicate the low level
@@@ -437,21 -451,27 +470,36 @@@
  	 * Check if the hardware context is actually mapped to anything.
  	 * If not tell the caller that it should skip this queue.
  	 */
 -	ret = -EXDEV;
 -	data.hctx = q->queue_hw_ctx[hctx_idx];
 -	if (!blk_mq_hw_queue_mapped(data.hctx))
 -		goto out_queue_exit;
 -	cpu = cpumask_first_and(data.hctx->cpumask, cpu_online_mask);
 -	data.ctx = __blk_mq_get_ctx(q, cpu);
 +	alloc_data.hctx = q->queue_hw_ctx[hctx_idx];
 +	if (!blk_mq_hw_queue_mapped(alloc_data.hctx)) {
 +		blk_queue_exit(q);
 +		return ERR_PTR(-EXDEV);
 +	}
 +	cpu = cpumask_first_and(alloc_data.hctx->cpumask, cpu_online_mask);
 +	alloc_data.ctx = __blk_mq_get_ctx(q, cpu);
  
++<<<<<<< HEAD
 +	rq = blk_mq_get_request(q, NULL, &alloc_data);
++=======
+ 	if (q->elevator)
+ 		data.flags |= BLK_MQ_REQ_INTERNAL;
+ 	else
+ 		blk_mq_tag_busy(data.hctx);
+ 
+ 	ret = -EWOULDBLOCK;
+ 	tag = blk_mq_get_tag(&data);
+ 	if (tag == BLK_MQ_NO_TAG)
+ 		goto out_queue_exit;
+ 	return blk_mq_rq_ctx_init(&data, tag, alloc_time_ns);
+ 
+ out_queue_exit:
++>>>>>>> 600c3b0cea78 (blk-mq: open code __blk_mq_alloc_request in blk_mq_alloc_request_hctx)
  	blk_queue_exit(q);
 -	return ERR_PTR(ret);
 +
 +	if (!rq)
 +		return ERR_PTR(-EWOULDBLOCK);
 +
 +	return rq;
  }
  EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
  
* Unmerged path block/blk-mq.c
