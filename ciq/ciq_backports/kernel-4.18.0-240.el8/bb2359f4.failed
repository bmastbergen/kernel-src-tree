bpf: Change kvfree to kfree in generic_map_lookup_batch()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Denis Efremov <efremov@linux.com>
commit bb2359f4dbe98e8863b4e885fc09269ef4682ec3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/bb2359f4.failed

buf_prevkey in generic_map_lookup_batch() is allocated with
kmalloc(). It's safe to free it with kfree().

Fixes: cb4d03ab499d ("bpf: Add generic support for lookup batch op")
	Signed-off-by: Denis Efremov <efremov@linux.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Song Liu <songliubraving@fb.com>
Link: https://lore.kernel.org/bpf/20200601162814.17426-1-efremov@linux.com
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit bb2359f4dbe98e8863b4e885fc09269ef4682ec3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/syscall.c
diff --cc kernel/bpf/syscall.c
index b5b79e59cfd4,e83b0818b529..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -1124,6 -1252,218 +1124,221 @@@ err_put
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ int generic_map_delete_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 cp, max_count;
+ 	int err = 0;
+ 	void *key;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!key)
+ 		return -ENOMEM;
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		err = -EFAULT;
+ 		if (copy_from_user(key, keys + cp * map->key_size,
+ 				   map->key_size))
+ 			break;
+ 
+ 		if (bpf_map_is_dev_bound(map)) {
+ 			err = bpf_map_offload_delete_elem(map, key);
+ 			break;
+ 		}
+ 
+ 		bpf_disable_instrumentation();
+ 		rcu_read_lock();
+ 		err = map->ops->map_delete_elem(map, key);
+ 		rcu_read_unlock();
+ 		bpf_enable_instrumentation();
+ 		maybe_wait_bpf_programs(map);
+ 		if (err)
+ 			break;
+ 	}
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 
+ 	kfree(key);
+ 	return err;
+ }
+ 
+ int generic_map_update_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 value_size, cp, max_count;
+ 	int ufd = attr->map_fd;
+ 	void *key, *value;
+ 	struct fd f;
+ 	int err = 0;
+ 
+ 	f = fdget(ufd);
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!key)
+ 		return -ENOMEM;
+ 
+ 	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!value) {
+ 		kfree(key);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		err = -EFAULT;
+ 		if (copy_from_user(key, keys + cp * map->key_size,
+ 		    map->key_size) ||
+ 		    copy_from_user(value, values + cp * value_size, value_size))
+ 			break;
+ 
+ 		err = bpf_map_update_value(map, f, key, value,
+ 					   attr->batch.elem_flags);
+ 
+ 		if (err)
+ 			break;
+ 	}
+ 
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 
+ 	kfree(value);
+ 	kfree(key);
+ 	return err;
+ }
+ 
+ #define MAP_LOOKUP_RETRIES 3
+ 
+ int generic_map_lookup_batch(struct bpf_map *map,
+ 				    const union bpf_attr *attr,
+ 				    union bpf_attr __user *uattr)
+ {
+ 	void __user *uobatch = u64_to_user_ptr(attr->batch.out_batch);
+ 	void __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	void *buf, *buf_prevkey, *prev_key, *key, *value;
+ 	int err, retry = MAP_LOOKUP_RETRIES;
+ 	u32 value_size, cp, max_count;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map))
+ 		return -EINVAL;
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	if (put_user(0, &uattr->batch.count))
+ 		return -EFAULT;
+ 
+ 	buf_prevkey = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf_prevkey)
+ 		return -ENOMEM;
+ 
+ 	buf = kmalloc(map->key_size + value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf) {
+ 		kfree(buf_prevkey);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	err = -EFAULT;
+ 	prev_key = NULL;
+ 	if (ubatch && copy_from_user(buf_prevkey, ubatch, map->key_size))
+ 		goto free_buf;
+ 	key = buf;
+ 	value = key + map->key_size;
+ 	if (ubatch)
+ 		prev_key = buf_prevkey;
+ 
+ 	for (cp = 0; cp < max_count;) {
+ 		rcu_read_lock();
+ 		err = map->ops->map_get_next_key(map, prev_key, key);
+ 		rcu_read_unlock();
+ 		if (err)
+ 			break;
+ 		err = bpf_map_copy_value(map, key, value,
+ 					 attr->batch.elem_flags);
+ 
+ 		if (err == -ENOENT) {
+ 			if (retry) {
+ 				retry--;
+ 				continue;
+ 			}
+ 			err = -EINTR;
+ 			break;
+ 		}
+ 
+ 		if (err)
+ 			goto free_buf;
+ 
+ 		if (copy_to_user(keys + cp * map->key_size, key,
+ 				 map->key_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 		if (copy_to_user(values + cp * value_size, value, value_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 
+ 		if (!prev_key)
+ 			prev_key = buf_prevkey;
+ 
+ 		swap(prev_key, key);
+ 		retry = MAP_LOOKUP_RETRIES;
+ 		cp++;
+ 	}
+ 
+ 	if (err == -EFAULT)
+ 		goto free_buf;
+ 
+ 	if ((copy_to_user(&uattr->batch.count, &cp, sizeof(cp)) ||
+ 		    (cp && copy_to_user(uobatch, prev_key, map->key_size))))
+ 		err = -EFAULT;
+ 
+ free_buf:
+ 	kfree(buf_prevkey);
+ 	kfree(buf);
+ 	return err;
+ }
+ 
++>>>>>>> bb2359f4dbe9 (bpf: Change kvfree to kfree in generic_map_lookup_batch())
  #define BPF_MAP_LOOKUP_AND_DELETE_ELEM_LAST_FIELD value
  
  static int map_lookup_and_delete_elem(union bpf_attr *attr)
* Unmerged path kernel/bpf/syscall.c
