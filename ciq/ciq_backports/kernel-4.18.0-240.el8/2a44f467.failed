io_uring: pick up link work on submit reference drop

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 2a44f46781617c5040372b59da33553a02b1f46d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/2a44f467.failed

If work completes inline, then we should pick up a dependent link item
in __io_queue_sqe() as well. If we don't do so, we're forced to go async
with that item, which is suboptimal.

This also fixes an issue with io_put_req_find_next(), which always looks
up the next work item. That should only be done if we're dropping the
last reference to the request, to prevent multiple lookups of the same
work item.

Outside of being a fix, this also enables a good cleanup series for 5.7,
where we never have to pass 'nxt' around or into the work handlers.

	Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 2a44f46781617c5040372b59da33553a02b1f46d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index b545f09e7576,f79ca494bb56..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -677,14 -1445,48 +677,33 @@@ static void io_free_req(struct io_kioc
  	 * dependencies to the next request. In case of failure, fail the rest
  	 * of the chain.
  	 */
 -	if (req->flags & REQ_F_FAIL_LINK) {
 -		io_fail_links(req);
 -	} else if ((req->flags & (REQ_F_LINK_TIMEOUT | REQ_F_COMP_LOCKED)) ==
 -			REQ_F_LINK_TIMEOUT) {
 -		struct io_ring_ctx *ctx = req->ctx;
 -		unsigned long flags;
 -
 -		/*
 -		 * If this is a timeout link, we could be racing with the
 -		 * timeout timer. Grab the completion lock for this case to
 -		 * protect against that.
 -		 */
 -		spin_lock_irqsave(&ctx->completion_lock, flags);
 -		io_req_link_next(req, nxt);
 -		spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -	} else {
 -		io_req_link_next(req, nxt);
 +	if (req->flags & REQ_F_LINK) {
 +		if (req->flags & REQ_F_FAIL_LINK)
 +			io_fail_links(req);
 +		else
 +			io_req_link_next(req);
  	}
 -}
 -
 -static void io_free_req(struct io_kiocb *req)
 -{
 -	struct io_kiocb *nxt = NULL;
  
 -	io_req_find_next(req, &nxt);
  	__io_free_req(req);
++<<<<<<< HEAD
++=======
+ 
+ 	if (nxt)
+ 		io_queue_async_work(nxt);
+ }
+ 
+ /*
+  * Drop reference to request, return next in chain (if there is one) if this
+  * was the last reference to this request.
+  */
+ __attribute__((nonnull))
+ static void io_put_req_find_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	if (refcount_dec_and_test(&req->refs)) {
+ 		io_req_find_next(req, nxtptr);
+ 		__io_free_req(req);
+ 	}
++>>>>>>> 2a44f4678161 (io_uring: pick up link work on submit reference drop)
  }
  
  static void io_put_req(struct io_kiocb *req)
@@@ -2211,8 -4628,156 +2230,161 @@@ static int __io_queue_sqe(struct io_rin
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
++=======
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_timeout_data *data = container_of(timer,
+ 						struct io_timeout_data, timer);
+ 	struct io_kiocb *req = data->req;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->link_list)) {
+ 		prev = list_entry(req->link_list.prev, struct io_kiocb,
+ 				  link_list);
+ 		if (refcount_inc_not_zero(&prev->refs)) {
+ 			list_del_init(&req->link_list);
+ 			prev->flags &= ~REQ_F_LINK_TIMEOUT;
+ 		} else
+ 			prev = NULL;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		req_set_fail_links(prev);
+ 		io_async_find_and_cancel(ctx, req, prev->user_data, NULL,
+ 						-ETIME);
+ 		io_put_req(prev);
+ 	} else {
+ 		io_cqring_add_event(req, -ETIME);
+ 		io_put_req(req);
+ 	}
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static void io_queue_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	/*
+ 	 * If the list is now empty, then our linked request finished before
+ 	 * we got a chance to setup the timer
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (!list_empty(&req->link_list)) {
+ 		struct io_timeout_data *data = &req->io->timeout;
+ 
+ 		data->timer.function = io_link_timeout_fn;
+ 		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
+ 				data->mode);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	/* drop submission reference */
+ 	io_put_req(req);
+ }
+ 
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb,
+ 					link_list);
+ 	if (!nxt || nxt->opcode != IORING_OP_LINK_TIMEOUT)
+ 		return NULL;
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 	return nxt;
+ }
+ 
+ static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_kiocb *linked_timeout;
+ 	struct io_kiocb *nxt = NULL;
+ 	const struct cred *old_creds = NULL;
+ 	int ret;
+ 
+ again:
+ 	linked_timeout = io_prep_linked_timeout(req);
+ 
+ 	if (req->work.creds && req->work.creds != current_cred()) {
+ 		if (old_creds)
+ 			revert_creds(old_creds);
+ 		if (old_creds == req->work.creds)
+ 			old_creds = NULL; /* restored original creds */
+ 		else
+ 			old_creds = override_creds(req->work.creds);
+ 	}
+ 
+ 	ret = io_issue_sqe(req, sqe, &nxt, true);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ punt:
+ 		if (io_op_defs[req->opcode].file_table) {
+ 			ret = io_grab_files(req);
+ 			if (ret)
+ 				goto err;
+ 		}
+ 
+ 		/*
+ 		 * Queued up for async execution, worker will release
+ 		 * submit reference when the iocb is actually submitted.
+ 		 */
+ 		io_queue_async_work(req);
+ 		goto done_req;
+ 	}
+ 
+ err:
+ 	/* drop submission reference */
+ 	io_put_req_find_next(req, &nxt);
+ 
+ 	if (linked_timeout) {
+ 		if (!ret)
+ 			io_queue_linked_timeout(linked_timeout);
+ 		else
+ 			io_put_req(linked_timeout);
+ 	}
+ 
+ 	/* and drop final reference, if we failed */
+ 	if (ret) {
+ 		io_cqring_add_event(req, ret);
+ 		req_set_fail_links(req);
+ 		io_put_req(req);
+ 	}
+ done_req:
+ 	if (nxt) {
+ 		req = nxt;
+ 		nxt = NULL;
+ 
+ 		if (req->flags & REQ_F_FORCE_ASYNC)
+ 			goto punt;
+ 		goto again;
+ 	}
+ 	if (old_creds)
+ 		revert_creds(old_creds);
+ }
+ 
+ static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++>>>>>>> 2a44f4678161 (io_uring: pick up link work on submit reference drop)
  {
  	int ret;
  
* Unmerged path fs/io_uring.c
