io_uring: standardize the prep methods

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 06b76d44ba25e52711dc7cc4fc75b50907bc6b8e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/06b76d44.failed

We currently have a mix of use cases. Most of the newer ones are pretty
uniform, but we have some older ones that use different calling
calling conventions. This is confusing.

For the opcodes that currently rely on the req->io->sqe copy saving
them from reuse, add a request type struct in the io_kiocb command
union to store the data they need.

Prepare for all opcodes having a standard prep method, so we can call
it in a uniform fashion and outside of the opcode handler. This is in
preparation for passing in the 'sqe' pointer, rather than storing it
in the io_kiocb. Once we have uniform prep handlers, we can leave all
the prep work to that part, and not even pass in the sqe to the opcode
handler. This ensures that we don't reuse sqe data inadvertently.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 06b76d44ba25e52711dc7cc4fc75b50907bc6b8e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index ab99aea677bc,2cdfbb451fe2..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -308,6 -299,86 +308,89 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	unsigned			count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	struct user_msghdr __user	*msg;
+ 	int				msg_flags;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
+ };
+ 
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -334,9 -421,17 +417,20 @@@ struct io_kiocb 
  #define REQ_F_IO_DRAIN		16	/* drain existing IO first */
  #define REQ_F_IO_DRAINED	32	/* drain done */
  #define REQ_F_LINK		64	/* linked sqes */
 -#define REQ_F_LINK_TIMEOUT	128	/* has linked timeout */
 +#define REQ_F_LINK_DONE		128	/* linked sqes done */
  #define REQ_F_FAIL_LINK		256	/* fail rest of links */
++<<<<<<< HEAD
 +#define REQ_F_SHADOW_DRAIN	512	/* link-drain shadow req */
++=======
+ #define REQ_F_DRAIN_LINK	512	/* link should be fully drained */
+ #define REQ_F_TIMEOUT		1024	/* timeout request */
+ #define REQ_F_ISREG		2048	/* regular file */
+ #define REQ_F_MUST_PUNT		4096	/* must be punted even for NONBLOCK */
+ #define REQ_F_TIMEOUT_NOSEQ	8192	/* no timeout sequence */
+ #define REQ_F_INFLIGHT		16384	/* on inflight list */
+ #define REQ_F_COMP_LOCKED	32768	/* completion under lock */
+ #define REQ_F_HARDLINK		65536	/* doesn't sever on completion < 0 */
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  	u64			user_data;
  	u32			result;
  	u32			sequence;
@@@ -1077,6 -1546,13 +1173,16 @@@ static int io_prep_rw(struct io_kiocb *
  			return -EINVAL;
  		kiocb->ki_complete = io_complete_rw;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	req->rw.addr = READ_ONCE(req->sqe->addr);
+ 	req->rw.len = READ_ONCE(req->sqe->len);
+ 	/* we own ->private, reuse it for the buffer index */
+ 	req->rw.kiocb.private = (void *) (unsigned long)
+ 					READ_ONCE(req->sqe->buf_index);
+ 	req->sqe = NULL;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  	return 0;
  }
  
@@@ -1330,34 -1757,98 +1436,104 @@@ static ssize_t loop_rw_iter(int rw, str
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
++=======
+ static void io_req_map_rw(struct io_kiocb *req, ssize_t io_size,
+ 			  struct iovec *iovec, struct iovec *fast_iov,
+ 			  struct iov_iter *iter)
+ {
+ 	req->io->rw.nr_segs = iter->nr_segs;
+ 	req->io->rw.size = io_size;
+ 	req->io->rw.iov = iovec;
+ 	if (!req->io->rw.iov) {
+ 		req->io->rw.iov = req->io->rw.fast_iov;
+ 		memcpy(req->io->rw.iov, fast_iov,
+ 			sizeof(struct iovec) * iter->nr_segs);
+ 	}
+ }
+ 
+ static int io_alloc_async_ctx(struct io_kiocb *req)
+ {
+ 	req->io = kmalloc(sizeof(*req->io), GFP_KERNEL);
+ 	return req->io == NULL;
+ }
+ 
+ static void io_rw_async(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct iovec *iov = NULL;
+ 
+ 	if (req->io->rw.iov != req->io->rw.fast_iov)
+ 		iov = req->io->rw.iov;
+ 	io_wq_submit_work(workptr);
+ 	kfree(iov);
+ }
+ 
+ static int io_setup_async_rw(struct io_kiocb *req, ssize_t io_size,
+ 			     struct iovec *iovec, struct iovec *fast_iov,
+ 			     struct iov_iter *iter)
+ {
+ 	if (!req->io && io_alloc_async_ctx(req))
+ 		return -ENOMEM;
+ 
+ 	io_req_map_rw(req, io_size, iovec, fast_iov, iter);
+ 	req->work.func = io_rw_async;
+ 	return 0;
+ }
+ 
+ static int io_read_prep(struct io_kiocb *req, struct iovec **iovec,
+ 			struct iov_iter *iter, bool force_nonblock)
+ {
+ 	ssize_t ret;
+ 
+ 	if (req->sqe) {
+ 		ret = io_prep_rw(req, force_nonblock);
+ 		if (ret)
+ 			return ret;
+ 
+ 		if (unlikely(!(req->file->f_mode & FMODE_READ)))
+ 			return -EBADF;
+ 	}
+ 
+ 	return io_import_iovec(READ, req, iovec, iter);
+ }
+ 
+ static int io_read(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  		   bool force_nonblock)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 -	struct kiocb *kiocb = &req->rw.kiocb;
 +	struct kiocb *kiocb = &req->rw;
  	struct iov_iter iter;
 +	struct file *file;
  	size_t iov_count;
 -	ssize_t io_size, ret;
 +	ssize_t read_size, ret;
  
++<<<<<<< HEAD
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
 +		return ret;
 +	file = kiocb->ki_filp;
++=======
+ 	ret = io_read_prep(req, &iovec, &iter, force_nonblock);
+ 	if (ret < 0)
+ 		return ret;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
 +
 +	if (unlikely(!(file->f_mode & FMODE_READ)))
 +		return -EBADF;
  
 -	/* Ensure we clear previously set non-block flag */
 -	if (!force_nonblock)
 -		req->rw.kiocb.ki_flags &= ~IOCB_NOWAIT;
 +	ret = io_import_iovec(req->ctx, READ, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 -	io_size = ret;
 +	read_size = ret;
  	if (req->flags & REQ_F_LINK)
 -		req->result = io_size;
 -
 -	/*
 -	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
 -	 * we know to async punt it even if it was opened O_NONBLOCK
 -	 */
 -	if (force_nonblock && !io_file_supports_async(req->file)) {
 -		req->flags |= REQ_F_MUST_PUNT;
 -		goto copy_iov;
 -	}
 +		req->result = read_size;
  
  	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(READ, req->file, &kiocb->ki_pos, iov_count);
 +	ret = rw_verify_area(READ, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
  
@@@ -1393,42 -1887,60 +1569,68 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_write(struct io_kiocb *req, const struct sqe_submit *s,
++=======
+ static int io_write_prep(struct io_kiocb *req, struct iovec **iovec,
+ 			 struct iov_iter *iter, bool force_nonblock)
+ {
+ 	ssize_t ret;
+ 
+ 	if (req->sqe) {
+ 		ret = io_prep_rw(req, force_nonblock);
+ 		if (ret)
+ 			return ret;
+ 
+ 		if (unlikely(!(req->file->f_mode & FMODE_WRITE)))
+ 			return -EBADF;
+ 	}
+ 
+ 	return io_import_iovec(WRITE, req, iovec, iter);
+ }
+ 
+ static int io_write(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  		    bool force_nonblock)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 -	struct kiocb *kiocb = &req->rw.kiocb;
 +	struct kiocb *kiocb = &req->rw;
  	struct iov_iter iter;
 +	struct file *file;
  	size_t iov_count;
 -	ssize_t ret, io_size;
 +	ssize_t ret;
  
++<<<<<<< HEAD
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
++=======
+ 	ret = io_write_prep(req, &iovec, &iter, force_nonblock);
+ 	if (ret < 0)
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  		return ret;
  
 -	/* Ensure we clear previously set non-block flag */
 -	if (!force_nonblock)
 -		req->rw.kiocb.ki_flags &= ~IOCB_NOWAIT;
 +	file = kiocb->ki_filp;
 +	if (unlikely(!(file->f_mode & FMODE_WRITE)))
 +		return -EBADF;
 +
 +	ret = io_import_iovec(req->ctx, WRITE, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 -	io_size = ret;
  	if (req->flags & REQ_F_LINK)
 -		req->result = io_size;
 +		req->result = ret;
  
 -	/*
 -	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
 -	 * we know to async punt it even if it was opened O_NONBLOCK
 -	 */
 -	if (force_nonblock && !io_file_supports_async(req->file)) {
 -		req->flags |= REQ_F_MUST_PUNT;
 -		goto copy_iov;
 -	}
 +	iov_count = iov_iter_count(&iter);
  
 -	/* file path doesn't support NOWAIT for non-direct_IO */
 -	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&
 -	    (req->flags & REQ_F_ISREG))
 -		goto copy_iov;
 +	ret = -EAGAIN;
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
 +		goto out_free;
 +	}
  
 -	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(WRITE, req->file, &kiocb->ki_pos, iov_count);
 +	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
  
@@@ -1484,10 -1995,13 +1686,15 @@@ static int io_nop(struct io_kiocb *req
  	return 0;
  }
  
 -static int io_prep_fsync(struct io_kiocb *req)
 +static int io_prep_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 -	const struct io_uring_sqe *sqe = req->sqe;
  	struct io_ring_ctx *ctx = req->ctx;
  
++<<<<<<< HEAD
++=======
+ 	if (!req->sqe)
+ 		return 0;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  	if (!req->file)
  		return -EBADF;
  
@@@ -1496,6 -2010,13 +1703,16 @@@
  	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
  		return -EINVAL;
  
++<<<<<<< HEAD
++=======
+ 	req->sync.flags = READ_ONCE(sqe->fsync_flags);
+ 	if (unlikely(req->sync.flags & ~IORING_FSYNC_DATASYNC))
+ 		return -EINVAL;
+ 
+ 	req->sync.off = READ_ONCE(sqe->off);
+ 	req->sync.len = READ_ONCE(sqe->len);
+ 	req->sqe = NULL;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  	return 0;
  }
  
@@@ -1531,11 -2077,13 +1748,16 @@@ static int io_fsync(struct io_kiocb *re
  	return 0;
  }
  
 -static int io_prep_sfr(struct io_kiocb *req)
 +static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 -	const struct io_uring_sqe *sqe = req->sqe;
  	struct io_ring_ctx *ctx = req->ctx;
 +	int ret = 0;
  
++<<<<<<< HEAD
++=======
+ 	if (!sqe)
+ 		return 0;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  	if (!req->file)
  		return -EBADF;
  
@@@ -1544,19 -2092,39 +1766,27 @@@
  	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	return ret;
++=======
+ 	req->sync.off = READ_ONCE(sqe->off);
+ 	req->sync.len = READ_ONCE(sqe->len);
+ 	req->sync.flags = READ_ONCE(sqe->sync_range_flags);
+ 	req->sqe = NULL;
+ 	return 0;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  }
  
 -static void io_sync_file_range_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -	int ret;
 -
 -	if (io_req_cancelled(req))
 -		return;
 -
 -	ret = sync_file_range(req->file, req->sync.off, req->sync.len,
 -				req->sync.flags);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, &nxt);
 -	if (nxt)
 -		*workptr = &nxt->work;
 -}
 -
 -static int io_sync_file_range(struct io_kiocb *req, struct io_kiocb **nxt,
 +static int io_sync_file_range(struct io_kiocb *req,
 +			      const struct io_uring_sqe *sqe,
  			      bool force_nonblock)
  {
 -	struct io_wq_work *work, *old_work;
 +	loff_t sqe_off;
 +	loff_t sqe_len;
 +	unsigned flags;
  	int ret;
  
 -	ret = io_prep_sfr(req);
 +	ret = io_prep_sfr(req, sqe);
  	if (ret)
  		return ret;
  
@@@ -1578,11 -2143,44 +1808,50 @@@
  }
  
  #if defined(CONFIG_NET)
 -static void io_sendrecv_async(struct io_wq_work **workptr)
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
  {
++<<<<<<< HEAD
++=======
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct iovec *iov = NULL;
+ 
+ 	if (req->io->rw.iov != req->io->rw.fast_iov)
+ 		iov = req->io->msg.iov;
+ 	io_wq_submit_work(workptr);
+ 	kfree(iov);
+ }
+ #endif
+ 
+ static int io_sendmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct io_sr_msg *sr = &req->sr_msg;
+ 	int ret;
+ 
+ 	if (!sqe)
+ 		return 0;
+ 	sr->msg_flags = READ_ONCE(sqe->msg_flags);
+ 	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	io->msg.iov = io->msg.fast_iov;
+ 	ret = sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
+ 					&io->msg.iov);
+ 	req->sqe = NULL;
+ 	return ret;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_sendmsg(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_async_msghdr *kmsg = NULL;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  	struct socket *sock;
  	int ret;
  
@@@ -1610,13 -2228,41 +1879,35 @@@
  			ret = -EINTR;
  	}
  
 -out:
 -	if (!io_wq_current_is_worker() && kmsg && kmsg->iov != kmsg->fast_iov)
 -		kfree(kmsg->iov);
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req_find_next(req, nxt);
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
  	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
  }
++<<<<<<< HEAD
++=======
+ 
+ static int io_recvmsg_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_sr_msg *sr = &req->sr_msg;
+ 	int ret;
+ 
+ 	if (!req->sqe)
+ 		return 0;
+ 
+ 	sr->msg_flags = READ_ONCE(req->sqe->msg_flags);
+ 	sr->msg = u64_to_user_ptr(READ_ONCE(req->sqe->addr));
+ 	io->msg.iov = io->msg.fast_iov;
+ 	ret = recvmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
+ 					&io->msg.uaddr, &io->msg.iov);
+ 	req->sqe = NULL;
+ 	return ret;
+ #else
+ 	return -EOPNOTSUPP;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  #endif
 -}
  
 -static int io_recvmsg(struct io_kiocb *req, struct io_kiocb **nxt,
 +static int io_sendmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
  		      bool force_nonblock)
  {
  #if defined(CONFIG_NET)
@@@ -1626,7 -2328,113 +1917,117 @@@
  #endif
  }
  
++<<<<<<< HEAD
 +static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
++=======
+ static int io_accept_prep(struct io_kiocb *req)
+ {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct io_accept *accept = &req->accept;
+ 
+ 	if (!req->sqe)
+ 		return 0;
+ 
+ 	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->len || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	accept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	accept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	accept->flags = READ_ONCE(sqe->accept_flags);
+ 	req->sqe = NULL;
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ #if defined(CONFIG_NET)
+ static int __io_accept(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		       bool force_nonblock)
+ {
+ 	struct io_accept *accept = &req->accept;
+ 	unsigned file_flags;
+ 	int ret;
+ 
+ 	file_flags = force_nonblock ? O_NONBLOCK : 0;
+ 	ret = __sys_accept4_file(req->file, file_flags, accept->addr,
+ 					accept->addr_len, accept->flags);
+ 	if (ret == -EAGAIN && force_nonblock)
+ 		return -EAGAIN;
+ 	if (ret == -ERESTARTSYS)
+ 		ret = -EINTR;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ }
+ 
+ static void io_accept_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_accept(req, &nxt, false);
+ 	if (nxt)
+ 		*workptr = &nxt->work;
+ }
+ #endif
+ 
+ static int io_accept(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		     bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	int ret;
+ 
+ 	ret = io_accept_prep(req);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = __io_accept(req, nxt, force_nonblock);
+ 	if (ret == -EAGAIN && force_nonblock) {
+ 		req->work.func = io_accept_finish;
+ 		req->work.flags |= IO_WQ_WORK_NEEDS_FILES;
+ 		io_put_req(req);
+ 		return -EAGAIN;
+ 	}
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_connect_prep(struct io_kiocb *req, struct io_async_ctx *io)
+ {
+ #if defined(CONFIG_NET)
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	int ret;
+ 
+ 	if (!sqe)
+ 		return 0;
+ 	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	req->connect.addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->connect.addr_len =  READ_ONCE(sqe->addr2);
+ 	ret = move_addr_to_kernel(req->connect.addr, req->connect.addr_len,
+ 					&io->connect.address);
+ 	req->sqe = NULL;
+ 	return ret;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_connect(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  		      bool force_nonblock)
  {
  #if defined(CONFIG_NET)
@@@ -1663,6 -2509,39 +2064,42 @@@ static void io_poll_remove_all(struct i
  	spin_unlock_irq(&ctx->completion_lock);
  }
  
++<<<<<<< HEAD
++=======
+ static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)
+ {
+ 	struct hlist_head *list;
+ 	struct io_kiocb *req;
+ 
+ 	list = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];
+ 	hlist_for_each_entry(req, list, hash_node) {
+ 		if (sqe_addr == req->user_data) {
+ 			io_poll_remove_one(req);
+ 			return 0;
+ 		}
+ 	}
+ 
+ 	return -ENOENT;
+ }
+ 
+ static int io_poll_remove_prep(struct io_kiocb *req)
+ {
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 
+ 	if (!sqe)
+ 		return 0;
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
+ 	    sqe->poll_events)
+ 		return -EINVAL;
+ 
+ 	req->poll.addr = READ_ONCE(sqe->addr);
+ 	req->sqe = NULL;
+ 	return 0;
+ }
+ 
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  /*
   * Find a running poll command that matches one specified in sqe->addr,
   * and remove it if found.
@@@ -1785,15 -2684,23 +2222,20 @@@ static void io_poll_queue_proc(struct f
  	add_wait_queue(head, &pt->req->poll.wait);
  }
  
 -static void io_poll_req_insert(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct hlist_head *list;
 -
 -	list = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];
 -	hlist_add_head(&req->hash_node, list);
 -}
 -
 -static int io_poll_add_prep(struct io_kiocb *req)
 +static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 -	const struct io_uring_sqe *sqe = req->sqe;
  	struct io_poll_iocb *poll = &req->poll;
 +	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_poll_table ipt;
 +	bool cancel = false;
 +	__poll_t mask;
  	u16 events;
  
++<<<<<<< HEAD
++=======
+ 	if (!sqe)
+ 		return 0;
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
  		return -EINVAL;
  	if (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)
@@@ -1801,10 -2708,27 +2243,34 @@@
  	if (!poll->file)
  		return -EBADF;
  
++<<<<<<< HEAD
 +	req->submit.sqe = NULL;
 +	INIT_WORK(&req->work, io_poll_complete_work);
 +	events = READ_ONCE(sqe->poll_events);
 +	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
++=======
+ 	events = READ_ONCE(sqe->poll_events);
+ 	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
+ 	req->sqe = NULL;
+ 	return 0;
+ }
+ 
+ static int io_poll_add(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	struct io_poll_iocb *poll = &req->poll;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_poll_table ipt;
+ 	bool cancel = false;
+ 	__poll_t mask;
+ 	int ret;
+ 
+ 	ret = io_poll_add_prep(req);
+ 	if (ret)
+ 		return ret;
+ 
+ 	INIT_IO_WORK(&req->work, io_poll_complete_work);
+ 	INIT_HLIST_NODE(&req->hash_node);
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  
  	poll->head = NULL;
  	poll->done = false;
@@@ -1853,22 -2778,422 +2319,421 @@@
  	return ipt.error;
  }
  
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 +	struct io_uring_sqe *sqe_copy;
  
++<<<<<<< HEAD
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
++=======
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * We could be racing with timeout deletion. If the list is empty,
+ 	 * then timeout lookup already found it and will be handling it.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		struct io_kiocb *prev;
+ 
+ 		/*
+ 		 * Adjust the reqs sequence before the current one because it
+ 		 * will consume a slot in the cq_ring and the cq_tail
+ 		 * pointer will be increased, otherwise other timeout reqs may
+ 		 * return in advance without waiting for enough wait_nr.
+ 		 */
+ 		prev = req;
+ 		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 			prev->sequence++;
+ 		list_del_init(&req->list);
+ 	}
+ 
+ 	io_cqring_fill_event(req, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 	req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
+ {
+ 	struct io_kiocb *req;
+ 	int ret = -ENOENT;
+ 
+ 	list_for_each_entry(req, &ctx->timeout_list, list) {
+ 		if (user_data == req->user_data) {
+ 			list_del_init(&req->list);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (ret == -ENOENT)
+ 		return ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
+ 	if (ret == -1)
+ 		return -EALREADY;
+ 
+ 	req_set_fail_links(req);
+ 	io_cqring_fill_event(req, -ECANCELED);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_remove_prep(struct io_kiocb *req)
+ {
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 
+ 	if (!sqe)
+ 		return 0;
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
+ 		return -EINVAL;
+ 
+ 	req->timeout.addr = READ_ONCE(sqe->addr);
+ 	req->timeout.flags = READ_ONCE(sqe->timeout_flags);
+ 	if (req->timeout.flags)
+ 		return -EINVAL;
+ 
+ 	req->sqe = NULL;
+ 	return 0;
+ }
+ 
+ /*
+  * Remove or update an existing timeout command
+  */
+ static int io_timeout_remove(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	ret = io_timeout_remove_prep(req);
+ 	if (ret)
+ 		return ret;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	ret = io_timeout_cancel(ctx, req->timeout.addr);
+ 
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	io_cqring_ev_posted(ctx);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_timeout_prep(struct io_kiocb *req, struct io_async_ctx *io,
+ 			   bool is_timeout_link)
+ {
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 	struct io_timeout_data *data;
+ 	unsigned flags;
+ 
+ 	if (!sqe)
+ 		return 0;
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
+ 		return -EINVAL;
+ 	if (sqe->off && is_timeout_link)
+ 		return -EINVAL;
+ 	flags = READ_ONCE(sqe->timeout_flags);
+ 	if (flags & ~IORING_TIMEOUT_ABS)
+ 		return -EINVAL;
+ 
+ 	req->timeout.count = READ_ONCE(sqe->off);
+ 
+ 	if (!io && io_alloc_async_ctx(req))
+ 		return -ENOMEM;
+ 
+ 	data = &req->io->timeout;
+ 	data->req = req;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	if (flags & IORING_TIMEOUT_ABS)
+ 		data->mode = HRTIMER_MODE_ABS;
+ 	else
+ 		data->mode = HRTIMER_MODE_REL;
+ 
+ 	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
+ 	req->sqe = NULL;
+ 	return 0;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_timeout_data *data;
+ 	struct list_head *entry;
+ 	unsigned span = 0;
+ 	int ret;
+ 
+ 	ret = io_timeout_prep(req, req->io, false);
+ 	if (ret)
+ 		return ret;
+ 	data = &req->io->timeout;
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied. If it isn't set, then this is
+ 	 * a pure timeout request, sequence isn't used.
+ 	 */
+ 	count = req->timeout.count;
+ 	if (!count) {
+ 		req->flags |= REQ_F_TIMEOUT_NOSEQ;
+ 		spin_lock_irq(&ctx->completion_lock);
+ 		entry = ctx->timeout_list.prev;
+ 		goto add;
+ 	}
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	data->seq_offset = count;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 		u32 nxt_offset = nxt->io->timeout.seq_offset;
+ 
+ 		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
+ 			continue;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt_offset + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ add:
+ 	list_add(&req->list, entry);
+ 	data->timer.function = io_timeout_fn;
+ 	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 	return 0;
+ }
+ 
+ static bool io_cancel_cb(struct io_wq_work *work, void *data)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	return req->user_data == (unsigned long) data;
+ }
+ 
+ static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
+ {
+ 	enum io_wq_cancel cancel_ret;
+ 	int ret = 0;
+ 
+ 	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
+ 	switch (cancel_ret) {
+ 	case IO_WQ_CANCEL_OK:
+ 		ret = 0;
+ 		break;
+ 	case IO_WQ_CANCEL_RUNNING:
+ 		ret = -EALREADY;
+ 		break;
+ 	case IO_WQ_CANCEL_NOTFOUND:
+ 		ret = -ENOENT;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
+ 				     struct io_kiocb *req, __u64 sqe_addr,
+ 				     struct io_kiocb **nxt, int success_ret)
+ {
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
+ 	if (ret != -ENOENT) {
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		goto done;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	ret = io_timeout_cancel(ctx, sqe_addr);
+ 	if (ret != -ENOENT)
+ 		goto done;
+ 	ret = io_poll_cancel(ctx, sqe_addr);
+ done:
+ 	if (!ret)
+ 		ret = success_ret;
+ 	io_cqring_fill_event(req, ret);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static int io_async_cancel_prep(struct io_kiocb *req)
+ {
+ 	const struct io_uring_sqe *sqe = req->sqe;
+ 
+ 	if (!sqe)
+ 		return 0;
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
+ 	    sqe->cancel_flags)
+ 		return -EINVAL;
+ 
+ 	req->cancel.addr = READ_ONCE(sqe->addr);
+ 	req->sqe = NULL;
+ 	return 0;
+ }
+ 
+ static int io_async_cancel(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	ret = io_async_cancel_prep(req);
+ 	if (ret)
+ 		return ret;
+ 
+ 	io_async_find_and_cancel(ctx, req, req->cancel.addr, nxt, 0);
+ 	return 0;
+ }
+ 
+ static int io_req_defer_prep(struct io_kiocb *req)
+ {
+ 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
+ 	struct io_async_ctx *io = req->io;
+ 	struct iov_iter iter;
+ 	ssize_t ret = 0;
+ 
+ 	switch (req->opcode) {
+ 	case IORING_OP_NOP:
+ 		break;
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 		/* ensure prep does right import */
+ 		req->io = NULL;
+ 		ret = io_read_prep(req, &iovec, &iter, true);
+ 		req->io = io;
+ 		if (ret < 0)
+ 			break;
+ 		io_req_map_rw(req, ret, iovec, inline_vecs, &iter);
+ 		ret = 0;
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 		/* ensure prep does right import */
+ 		req->io = NULL;
+ 		ret = io_write_prep(req, &iovec, &iter, true);
+ 		req->io = io;
+ 		if (ret < 0)
+ 			break;
+ 		io_req_map_rw(req, ret, iovec, inline_vecs, &iter);
+ 		ret = 0;
+ 		break;
+ 	case IORING_OP_POLL_ADD:
+ 		ret = io_poll_add_prep(req);
+ 		break;
+ 	case IORING_OP_POLL_REMOVE:
+ 		ret = io_poll_remove_prep(req);
+ 		break;
+ 	case IORING_OP_FSYNC:
+ 		ret = io_prep_fsync(req);
+ 		break;
+ 	case IORING_OP_SYNC_FILE_RANGE:
+ 		ret = io_prep_sfr(req);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 		ret = io_sendmsg_prep(req, io);
+ 		break;
+ 	case IORING_OP_RECVMSG:
+ 		ret = io_recvmsg_prep(req, io);
+ 		break;
+ 	case IORING_OP_CONNECT:
+ 		ret = io_connect_prep(req, io);
+ 		break;
+ 	case IORING_OP_TIMEOUT:
+ 		ret = io_timeout_prep(req, io, false);
+ 		break;
+ 	case IORING_OP_TIMEOUT_REMOVE:
+ 		ret = io_timeout_remove_prep(req);
+ 		break;
+ 	case IORING_OP_ASYNC_CANCEL:
+ 		ret = io_async_cancel_prep(req);
+ 		break;
+ 	case IORING_OP_LINK_TIMEOUT:
+ 		ret = io_timeout_prep(req, io, true);
+ 		break;
+ 	case IORING_OP_ACCEPT:
+ 		ret = io_accept_prep(req);
+ 		break;
+ 	default:
+ 		printk_once(KERN_WARNING "io_uring: unhandled opcode %d\n",
+ 				req->opcode);
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int io_req_defer(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
+ 
+ 	/* Still need defer if there is pending req in defer list. */
+ 	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  		return 0;
  
 -	if (io_alloc_async_ctx(req))
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
  		return -EAGAIN;
  
 -	ret = io_req_defer_prep(req);
 -	if (ret < 0)
 -		return ret;
 -
  	spin_lock_irq(&ctx->completion_lock);
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list)) {
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list)) {
  		spin_unlock_irq(&ctx->completion_lock);
 +		kfree(sqe_copy);
  		return 0;
  	}
  
@@@ -1881,51 -3203,61 +2746,58 @@@
  	return -EIOCBQUEUED;
  }
  
 -__attribute__((nonnull))
 -static int io_issue_sqe(struct io_kiocb *req, struct io_kiocb **nxt,
 -			bool force_nonblock)
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 +	int ret, opcode;
  
 -	switch (req->opcode) {
 +	req->user_data = READ_ONCE(s->sqe->user_data);
 +
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
  	case IORING_OP_NOP:
 -		ret = io_nop(req);
 +		ret = io_nop(req, req->user_data);
  		break;
  	case IORING_OP_READV:
++<<<<<<< HEAD
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_WRITEV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
++=======
+ 		ret = io_read(req, nxt, force_nonblock);
+ 		break;
+ 	case IORING_OP_WRITEV:
+ 		ret = io_write(req, nxt, force_nonblock);
++>>>>>>> 06b76d44ba25 (io_uring: standardize the prep methods)
  		break;
  	case IORING_OP_READ_FIXED:
 -		ret = io_read(req, nxt, force_nonblock);
 +		ret = io_read(req, s, force_nonblock);
  		break;
  	case IORING_OP_WRITE_FIXED:
 -		ret = io_write(req, nxt, force_nonblock);
 +		ret = io_write(req, s, force_nonblock);
  		break;
  	case IORING_OP_FSYNC:
 -		ret = io_fsync(req, nxt, force_nonblock);
 +		ret = io_fsync(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_POLL_ADD:
 -		ret = io_poll_add(req, nxt);
 +		ret = io_poll_add(req, s->sqe);
  		break;
  	case IORING_OP_POLL_REMOVE:
 -		ret = io_poll_remove(req);
 +		ret = io_poll_remove(req, s->sqe);
  		break;
  	case IORING_OP_SYNC_FILE_RANGE:
 -		ret = io_sync_file_range(req, nxt, force_nonblock);
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_SENDMSG:
 -		ret = io_sendmsg(req, nxt, force_nonblock);
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_RECVMSG:
 -		ret = io_recvmsg(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		ret = io_timeout(req);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		ret = io_timeout_remove(req);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		ret = io_accept(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_CONNECT:
 -		ret = io_connect(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		ret = io_async_cancel(req, nxt);
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
  		break;
  	default:
  		ret = -EINVAL;
* Unmerged path fs/io_uring.c
