iommu/arm-smmu: Refactor master_cfg/fwspec usage

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit 2465170f98de5fef0d35bafc4dc29490d925ab36
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/2465170f.failed

In preparation for restructuring iommu_fwspec, refactor the way we
access the arm_smmu_master_cfg private data to be less dependent on
the current layout.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
Link: https://lore.kernel.org/r/20200326150841.10083-11-joro@8bytes.org
(cherry picked from commit 2465170f98de5fef0d35bafc4dc29490d925ab36)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm-smmu.c
diff --cc drivers/iommu/arm-smmu.c
index 34ba163e6fcc,3cef2bfd6f3e..000000000000
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@@ -166,131 -98,13 +166,129 @@@ struct arm_smmu_master_cfg 
  	s16				smendx[];
  };
  #define INVALID_SMENDX			-1
- #define __fwspec_cfg(fw) ((struct arm_smmu_master_cfg *)fw->iommu_priv)
- #define fwspec_smmu(fw)  (__fwspec_cfg(fw)->smmu)
- #define fwspec_smendx(fw, i) \
- 	(i >= fw->num_ids ? INVALID_SMENDX : __fwspec_cfg(fw)->smendx[i])
- #define for_each_cfg_sme(fw, i, idx) \
- 	for (i = 0; idx = fwspec_smendx(fw, i), i < fw->num_ids; ++i)
+ #define cfg_smendx(cfg, fw, i) \
+ 	(i >= fw->num_ids ? INVALID_SMENDX : cfg->smendx[i])
+ #define for_each_cfg_sme(cfg, fw, i, idx) \
+ 	for (i = 0; idx = cfg_smendx(cfg, fw, i), i < fw->num_ids; ++i)
  
 +struct arm_smmu_device {
 +	struct device			*dev;
 +
 +	void __iomem			*base;
 +	void __iomem			*cb_base;
 +	unsigned long			pgshift;
 +
 +#define ARM_SMMU_FEAT_COHERENT_WALK	(1 << 0)
 +#define ARM_SMMU_FEAT_STREAM_MATCH	(1 << 1)
 +#define ARM_SMMU_FEAT_TRANS_S1		(1 << 2)
 +#define ARM_SMMU_FEAT_TRANS_S2		(1 << 3)
 +#define ARM_SMMU_FEAT_TRANS_NESTED	(1 << 4)
 +#define ARM_SMMU_FEAT_TRANS_OPS		(1 << 5)
 +#define ARM_SMMU_FEAT_VMID16		(1 << 6)
 +#define ARM_SMMU_FEAT_FMT_AARCH64_4K	(1 << 7)
 +#define ARM_SMMU_FEAT_FMT_AARCH64_16K	(1 << 8)
 +#define ARM_SMMU_FEAT_FMT_AARCH64_64K	(1 << 9)
 +#define ARM_SMMU_FEAT_FMT_AARCH32_L	(1 << 10)
 +#define ARM_SMMU_FEAT_FMT_AARCH32_S	(1 << 11)
 +#define ARM_SMMU_FEAT_EXIDS		(1 << 12)
 +	u32				features;
 +
 +#define ARM_SMMU_OPT_SECURE_CFG_ACCESS (1 << 0)
 +	u32				options;
 +	enum arm_smmu_arch_version	version;
 +	enum arm_smmu_implementation	model;
 +
 +	u32				num_context_banks;
 +	u32				num_s2_context_banks;
 +	DECLARE_BITMAP(context_map, ARM_SMMU_MAX_CBS);
 +	struct arm_smmu_cb		*cbs;
 +	atomic_t			irptndx;
 +
 +	u32				num_mapping_groups;
 +	u16				streamid_mask;
 +	u16				smr_mask_mask;
 +	struct arm_smmu_smr		*smrs;
 +	struct arm_smmu_s2cr		*s2crs;
 +	struct mutex			stream_map_mutex;
 +
 +	unsigned long			va_size;
 +	unsigned long			ipa_size;
 +	unsigned long			pa_size;
 +	unsigned long			pgsize_bitmap;
 +
 +	u32				num_global_irqs;
 +	u32				num_context_irqs;
 +	unsigned int			*irqs;
 +	struct clk_bulk_data		*clks;
 +	int				num_clks;
 +
 +	u32				cavium_id_base; /* Specific to Cavium */
 +
 +	spinlock_t			global_sync_lock;
 +
 +	/* IOMMU core code handle */
 +	struct iommu_device		iommu;
 +};
 +
 +enum arm_smmu_context_fmt {
 +	ARM_SMMU_CTX_FMT_NONE,
 +	ARM_SMMU_CTX_FMT_AARCH64,
 +	ARM_SMMU_CTX_FMT_AARCH32_L,
 +	ARM_SMMU_CTX_FMT_AARCH32_S,
 +};
 +
 +struct arm_smmu_cfg {
 +	u8				cbndx;
 +	u8				irptndx;
 +	union {
 +		u16			asid;
 +		u16			vmid;
 +	};
 +	u32				cbar;
 +	enum arm_smmu_context_fmt	fmt;
 +};
 +#define INVALID_IRPTNDX			0xff
 +
 +enum arm_smmu_domain_stage {
 +	ARM_SMMU_DOMAIN_S1 = 0,
 +	ARM_SMMU_DOMAIN_S2,
 +	ARM_SMMU_DOMAIN_NESTED,
 +	ARM_SMMU_DOMAIN_BYPASS,
 +};
 +
 +struct arm_smmu_flush_ops {
 +	struct iommu_flush_ops		tlb;
 +	void (*tlb_inv_range)(unsigned long iova, size_t size, size_t granule,
 +			      bool leaf, void *cookie);
 +	void (*tlb_sync)(void *cookie);
 +};
 +
 +struct arm_smmu_domain {
 +	struct arm_smmu_device		*smmu;
 +	struct io_pgtable_ops		*pgtbl_ops;
 +	const struct arm_smmu_flush_ops	*flush_ops;
 +	struct arm_smmu_cfg		cfg;
 +	enum arm_smmu_domain_stage	stage;
 +	bool				non_strict;
 +	struct mutex			init_mutex; /* Protects smmu pointer */
 +	spinlock_t			cb_lock; /* Serialises ATS1* ops and TLB syncs */
 +	struct iommu_domain		domain;
 +};
 +
 +struct arm_smmu_option_prop {
 +	u32 opt;
 +	const char *prop;
 +};
 +
 +static atomic_t cavium_smmu_context_count = ATOMIC_INIT(0);
 +
  static bool using_legacy_binding, using_generic_binding;
  
 +static struct arm_smmu_option_prop arm_smmu_options[] = {
 +	{ ARM_SMMU_OPT_SECURE_CFG_ACCESS, "calxeda,smmu-secure-config-access" },
 +	{ 0, NULL},
 +};
 +
  static inline int arm_smmu_rpm_get(struct arm_smmu_device *smmu)
  {
  	if (pm_runtime_enabled(smmu->dev))
@@@ -1202,9 -1067,9 +1200,15 @@@ static int arm_smmu_master_alloc_smes(s
  
  	mutex_lock(&smmu->stream_map_mutex);
  	/* Figure out a viable stream map entry allocation */
++<<<<<<< HEAD
 +	for_each_cfg_sme(fwspec, i, idx) {
 +		u16 sid = fwspec->ids[i];
 +		u16 mask = fwspec->ids[i] >> SMR_MASK_SHIFT;
++=======
+ 	for_each_cfg_sme(cfg, fwspec, i, idx) {
+ 		u16 sid = FIELD_GET(ARM_SMMU_SMR_ID, fwspec->ids[i]);
+ 		u16 mask = FIELD_GET(ARM_SMMU_SMR_MASK, fwspec->ids[i]);
++>>>>>>> 2465170f98de (iommu/arm-smmu: Refactor master_cfg/fwspec usage)
  
  		if (idx != INVALID_SMENDX) {
  			ret = -EEXIST;
@@@ -1339,8 -1205,22 +1346,8 @@@ static int arm_smmu_attach_dev(struct i
  	}
  
  	/* Looks ok, so add the device to the domain */
- 	ret = arm_smmu_domain_add_master(smmu_domain, fwspec);
+ 	ret = arm_smmu_domain_add_master(smmu_domain, cfg, fwspec);
  
 -	/*
 -	 * Setup an autosuspend delay to avoid bouncing runpm state.
 -	 * Otherwise, if a driver for a suspended consumer device
 -	 * unmaps buffers, it will runpm resume/suspend for each one.
 -	 *
 -	 * For example, when used by a GPU device, when an application
 -	 * or game exits, it can trigger unmapping 100s or 1000s of
 -	 * buffers.  With a runpm cycle for each buffer, that adds up
 -	 * to 5-10sec worth of reprogramming the context bank, while
 -	 * the system appears to be locked up to the user.
 -	 */
 -	pm_runtime_set_autosuspend_delay(smmu->dev, 20);
 -	pm_runtime_use_autosuspend(smmu->dev);
 -
  rpm_put:
  	arm_smmu_rpm_put(smmu);
  	return ret;
* Unmerged path drivers/iommu/arm-smmu.c
