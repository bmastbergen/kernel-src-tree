io_uring: make POLL_ADD/POLL_REMOVE scale better

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit eac406c61cd0ec8fe7970ca46ddf23e40a86b579
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/eac406c6.failed

One of the obvious use cases for these commands is networking, where
it's not uncommon to have tons of sockets open and polled for. The
current implementation uses a list for insertion and lookup, which works
fine for file based use cases where the count is usually low, it breaks
down somewhat for higher number of files / sockets. A test case with
30k sockets being polled for and cancelled takes:

real    0m6.968s
user    0m0.002s
sys     0m6.936s

with the patch it takes:

real    0m0.233s
user    0m0.010s
sys     0m0.176s

If you go to 50k sockets, it gets even more abysmal with the current
code:

real    0m40.602s
user    0m0.010s
sys     0m40.555s

with the patch it takes:

real    0m0.398s
user    0m0.000s
sys     0m0.341s

Change is pretty straight forward, just replace the cancel_list with
a red/black tree instead.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit eac406c61cd0ec8fe7970ca46ddf23e40a86b579)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index f656b9c7fa46,5ad652fa24b8..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -276,14 -271,11 +276,21 @@@ struct io_ring_ctx 
  		 * manipulate the list, hence no extra locking is needed there.
  		 */
  		struct list_head	poll_list;
++<<<<<<< HEAD
 +		struct list_head	cancel_list;
++=======
+ 		struct rb_root		cancel_tree;
+ 
+ 		spinlock_t		inflight_lock;
+ 		struct list_head	inflight_list;
++>>>>>>> eac406c61cd0 (io_uring: make POLL_ADD/POLL_REMOVE scale better)
  	} ____cacheline_aligned_in_smp;
 +
 +	struct async_list	pending_async[2];
 +
 +#if defined(CONFIG_UNIX)
 +	struct socket		*ring_sock;
 +#endif
  };
  
  struct sqe_submit {
@@@ -411,29 -429,42 +421,29 @@@ static struct io_ring_ctx *io_ring_ctx_
  
  	ctx->flags = p->flags;
  	init_waitqueue_head(&ctx->cq_wait);
 -	INIT_LIST_HEAD(&ctx->cq_overflow_list);
 -	init_completion(&ctx->completions[0]);
 -	init_completion(&ctx->completions[1]);
 +	init_completion(&ctx->ctx_done);
 +	init_completion(&ctx->sqo_thread_started);
  	mutex_init(&ctx->uring_lock);
  	init_waitqueue_head(&ctx->wait);
 +	for (i = 0; i < ARRAY_SIZE(ctx->pending_async); i++) {
 +		spin_lock_init(&ctx->pending_async[i].lock);
 +		INIT_LIST_HEAD(&ctx->pending_async[i].list);
 +		atomic_set(&ctx->pending_async[i].cnt, 0);
 +	}
  	spin_lock_init(&ctx->completion_lock);
  	INIT_LIST_HEAD(&ctx->poll_list);
- 	INIT_LIST_HEAD(&ctx->cancel_list);
+ 	ctx->cancel_tree = RB_ROOT;
  	INIT_LIST_HEAD(&ctx->defer_list);
 -	INIT_LIST_HEAD(&ctx->timeout_list);
 -	init_waitqueue_head(&ctx->inflight_wait);
 -	spin_lock_init(&ctx->inflight_lock);
 -	INIT_LIST_HEAD(&ctx->inflight_list);
  	return ctx;
 -err:
 -	if (ctx->fallback_req)
 -		kmem_cache_free(req_cachep, ctx->fallback_req);
 -	kfree(ctx->completions);
 -	kfree(ctx);
 -	return NULL;
 -}
 -
 -static inline bool __req_need_defer(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	return req->sequence != ctx->cached_cq_tail + ctx->cached_sq_dropped
 -					+ atomic_read(&ctx->cached_cq_overflow);
  }
  
 -static inline bool req_need_defer(struct io_kiocb *req)
 +static inline bool io_sequence_defer(struct io_ring_ctx *ctx,
 +				     struct io_kiocb *req)
  {
 -	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) == REQ_F_IO_DRAIN)
 -		return __req_need_defer(req);
 +	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) != REQ_F_IO_DRAIN)
 +		return false;
  
 -	return false;
 +	return req->sequence != ctx->cached_cq_tail + ctx->sq_ring->dropped;
  }
  
  static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
@@@ -1629,11 -1953,10 +1647,10 @@@ static void io_poll_remove_one(struct i
  	WRITE_ONCE(poll->canceled, true);
  	if (!list_empty(&poll->wait.entry)) {
  		list_del_init(&poll->wait.entry);
 -		io_queue_async_work(req);
 +		io_queue_async_work(req->ctx, req);
  	}
  	spin_unlock(&poll->head->lock);
- 
- 	list_del_init(&req->list);
+ 	io_poll_remove_req(req);
  }
  
  static void io_poll_remove_all(struct io_ring_ctx *ctx)
@@@ -1648,6 -1972,28 +1666,31 @@@
  	spin_unlock_irq(&ctx->completion_lock);
  }
  
++<<<<<<< HEAD
++=======
+ static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)
+ {
+ 	struct rb_node *p, *parent = NULL;
+ 	struct io_kiocb *req;
+ 
+ 	p = ctx->cancel_tree.rb_node;
+ 	while (p) {
+ 		parent = p;
+ 		req = rb_entry(parent, struct io_kiocb, rb_node);
+ 		if (sqe_addr < req->user_data) {
+ 			p = p->rb_left;
+ 		} else if (sqe_addr > req->user_data) {
+ 			p = p->rb_right;
+ 		} else {
+ 			io_poll_remove_one(req);
+ 			return 0;
+ 		}
+ 	}
+ 
+ 	return -ENOENT;
+ }
+ 
++>>>>>>> eac406c61cd0 (io_uring: make POLL_ADD/POLL_REMOVE scale better)
  /*
   * Find a running poll command that matches one specified in sqe->addr,
   * and remove it if found.
@@@ -1711,8 -2058,8 +1754,13 @@@ static void io_poll_complete_work(struc
  		spin_unlock_irq(&ctx->completion_lock);
  		return;
  	}
++<<<<<<< HEAD
 +	list_del_init(&req->list);
 +	io_poll_complete(ctx, req, mask);
++=======
+ 	io_poll_remove_req(req);
+ 	io_poll_complete(req, mask);
++>>>>>>> eac406c61cd0 (io_uring: make POLL_ADD/POLL_REMOVE scale better)
  	spin_unlock_irq(&ctx->completion_lock);
  
  	io_cqring_ev_posted(ctx);
@@@ -1735,9 -2085,17 +1783,16 @@@ static int io_poll_wake(struct wait_que
  
  	list_del_init(&poll->wait.entry);
  
 -	/*
 -	 * Run completion inline if we can. We're using trylock here because
 -	 * we are violating the completion_lock -> poll wq lock ordering.
 -	 * If we have a link timeout we're going to need the completion_lock
 -	 * for finalizing the request, mark us as having grabbed that already.
 -	 */
  	if (mask && spin_trylock_irqsave(&ctx->completion_lock, flags)) {
++<<<<<<< HEAD
 +		list_del(&req->list);
 +		io_poll_complete(ctx, req, mask);
++=======
+ 		io_poll_remove_req(req);
+ 		io_poll_complete(req, mask);
+ 		req->flags |= REQ_F_COMP_LOCKED;
+ 		io_put_req(req);
++>>>>>>> eac406c61cd0 (io_uring: make POLL_ADD/POLL_REMOVE scale better)
  		spin_unlock_irqrestore(&ctx->completion_lock, flags);
  
  		io_cqring_ev_posted(ctx);
@@@ -1770,7 -2127,27 +1825,31 @@@ static void io_poll_queue_proc(struct f
  	add_wait_queue(head, &pt->req->poll.wait);
  }
  
++<<<<<<< HEAD
 +static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++=======
+ static void io_poll_req_insert(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct rb_node **p = &ctx->cancel_tree.rb_node;
+ 	struct rb_node *parent = NULL;
+ 	struct io_kiocb *tmp;
+ 
+ 	while (*p) {
+ 		parent = *p;
+ 		tmp = rb_entry(parent, struct io_kiocb, rb_node);
+ 		if (req->user_data < tmp->user_data)
+ 			p = &(*p)->rb_left;
+ 		else
+ 			p = &(*p)->rb_right;
+ 	}
+ 	rb_link_node(&req->rb_node, parent, p);
+ 	rb_insert_color(&req->rb_node, &ctx->cancel_tree);
+ }
+ 
+ static int io_poll_add(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 		       struct io_kiocb **nxt)
++>>>>>>> eac406c61cd0 (io_uring: make POLL_ADD/POLL_REMOVE scale better)
  {
  	struct io_poll_iocb *poll = &req->poll;
  	struct io_ring_ctx *ctx = req->ctx;
@@@ -1787,9 -2164,10 +1866,10 @@@
  		return -EBADF;
  
  	req->submit.sqe = NULL;
 -	INIT_IO_WORK(&req->work, io_poll_complete_work);
 +	INIT_WORK(&req->work, io_poll_complete_work);
  	events = READ_ONCE(sqe->poll_events);
  	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
+ 	RB_CLEAR_NODE(&req->rb_node);
  
  	poll->head = NULL;
  	poll->done = false;
* Unmerged path fs/io_uring.c
