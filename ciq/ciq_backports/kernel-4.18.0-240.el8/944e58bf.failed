io_uring: pass only !null to io_req_find_next()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 944e58bfeda0e9b97cd611adafc823c78e0bc464
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/944e58bf.failed

Make io_req_find_next() and io_req_link_next() to accept only non-null
nxt, and handle it in callers.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 944e58bfeda0e9b97cd611adafc823c78e0bc464)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 66de1d702552,95deb45e89cf..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -649,10 -903,22 +649,24 @@@ static void io_req_link_next(struct io_
  			nxt->flags |= REQ_F_LINK;
  		}
  
++<<<<<<< HEAD
 +		nxt->flags |= REQ_F_LINK_DONE;
 +		INIT_WORK(&nxt->work, io_sq_wq_submit_work);
 +		io_queue_async_work(req->ctx, nxt);
++=======
+ 		/*
+ 		 * If we're in async work, we can continue processing the chain
+ 		 * in this context instead of having to queue up new async work.
+ 		 */
+ 		if (nxt) {
+ 			if (io_wq_current_is_worker())
+ 				*nxtptr = nxt;
+ 			else
+ 				io_queue_async_work(nxt);
+ 		}
+ 		break;
++>>>>>>> 944e58bfeda0 (io_uring: pass only !null to io_req_find_next())
  	}
 -
 -	req->flags |= REQ_F_LINK_NEXT;
 -	if (wake_ev)
 -		io_cqring_ev_posted(ctx);
  }
  
  /*
@@@ -679,14 -964,56 +693,64 @@@ static void io_free_req(struct io_kioc
  	 * dependencies to the next request. In case of failure, fail the rest
  	 * of the chain.
  	 */
++<<<<<<< HEAD
 +	if (req->flags & REQ_F_LINK) {
 +		if (req->flags & REQ_F_FAIL_LINK)
 +			io_fail_links(req);
++=======
+ 	if (req->flags & REQ_F_FAIL_LINK) {
+ 		io_fail_links(req);
+ 	} else if ((req->flags & (REQ_F_LINK_TIMEOUT | REQ_F_COMP_LOCKED)) ==
+ 			REQ_F_LINK_TIMEOUT) {
+ 		struct io_ring_ctx *ctx = req->ctx;
+ 		unsigned long flags;
+ 
+ 		/*
+ 		 * If this is a timeout link, we could be racing with the
+ 		 * timeout timer. Grab the completion lock for this case to
+ 		 * protect against that.
+ 		 */
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		io_req_link_next(req, nxt);
+ 		spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	} else {
+ 		io_req_link_next(req, nxt);
+ 	}
+ }
+ 
+ static void io_free_req(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	io_req_find_next(req, &nxt);
+ 	__io_free_req(req);
+ 
+ 	if (nxt)
+ 		io_queue_async_work(nxt);
+ }
+ 
+ /*
+  * Drop reference to request, return next in chain (if there is one) if this
+  * was the last reference to this request.
+  */
+ static void io_put_req_find_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	io_req_find_next(req, &nxt);
+ 
+ 	if (refcount_dec_and_test(&req->refs))
+ 		__io_free_req(req);
+ 
+ 	if (nxt) {
+ 		if (nxtptr)
+ 			*nxtptr = nxt;
++>>>>>>> 944e58bfeda0 (io_uring: pass only !null to io_req_find_next())
  		else
 -			io_queue_async_work(nxt);
 +			io_req_link_next(req);
  	}
 +
 +	__io_free_req(req);
  }
  
  static void io_put_req(struct io_kiocb *req)
* Unmerged path fs/io_uring.c
