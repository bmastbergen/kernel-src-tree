blk-mq: add optional request->alloc_time_ns

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Tejun Heo <tj@kernel.org>
commit 6f816b4b746c2241540e537682d30d8e9997d674
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6f816b4b.failed

There are currently two start time timestamps - start_time_ns and
io_start_time_ns.  The former marks the request allocation and and the
second issue-to-device time.  The planned io.weight controller needs
to measure the total time bios take to execute after it leaves rq_qos
including the time spent waiting for request to become available,
which can easily dominate on saturated devices.

This patch adds request->alloc_time_ns which records when the request
allocation attempt started.  As it isn't used for the usual stats,
make it optional behind CONFIG_BLK_RQ_ALLOC_TIME and
QUEUE_FLAG_RQ_ALLOC_TIME so that it can be compiled out when there are
no users and it's active only on queues which need it even when
compiled in.

v2: s/pre_start_time/alloc_time/ and add CONFIG_BLK_RQ_ALLOC_TIME
    gating as suggested by Jens.

	Signed-off-by: Tejun Heo <tj@kernel.org>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 6f816b4b746c2241540e537682d30d8e9997d674)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/blkdev.h
diff --cc include/linux/blkdev.h
index 43e46f573971,d0ad21e4771b..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -584,54 -585,35 +588,84 @@@ struct request_queue 
  
  #define BLK_MAX_WRITE_HINTS	5
  	u64			write_hints[BLK_MAX_WRITE_HINTS];
 +
 +	/*
 +	 * for reusing dead hctx instance in case of updating
 +	 * nr_hw_queues
 +	 */
 +	RH_KABI_EXTEND(struct list_head	unused_hctx_list)
 +	RH_KABI_EXTEND(spinlock_t	unused_hctx_lock)
 +	/*
 +	 * Protect concurrent access to q_usage_counter by
 +	 * percpu_ref_kill() and percpu_ref_reinit().
 +	 */
 +	RH_KABI_EXTEND(struct mutex		mq_freeze_lock)
 +
 +	RH_KABI_EXTEND(struct mutex		sysfs_dir_lock)
  };
  
++<<<<<<< HEAD
 +#define QUEUE_FLAG_STOPPED	1	/* queue is stopped */
 +#define QUEUE_FLAG_DYING	2	/* queue being torn down */
 +#define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
 +#define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
 +#define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
 +#define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
 +#define QUEUE_FLAG_UNPRIV_SGIO  8	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
 +#define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
 +#define QUEUE_FLAG_IO_STAT     10	/* do IO stats */
 +#define QUEUE_FLAG_DISCARD     11	/* supports DISCARD */
 +#define QUEUE_FLAG_NOXMERGES   12	/* No extended merges */
 +#define QUEUE_FLAG_ADD_RANDOM  13	/* Contributes to random pool */
 +#define QUEUE_FLAG_SECERASE    14	/* supports secure erase */
 +#define QUEUE_FLAG_SAME_FORCE  15	/* force complete on same CPU */
 +#define QUEUE_FLAG_DEAD        16	/* queue tear-down finished */
 +#define QUEUE_FLAG_INIT_DONE   17	/* queue is initialized */
 +#define QUEUE_FLAG_NO_SG_MERGE 18	/* don't attempt to merge SG segments (obsolete) */
 +#define QUEUE_FLAG_POLL	       19	/* IO polling enabled if set */
 +#define QUEUE_FLAG_WC	       20	/* Write back caching */
 +#define QUEUE_FLAG_FUA	       21	/* device supports FUA writes */
 +#define QUEUE_FLAG_DAX         23	/* device supports DAX */
 +#define QUEUE_FLAG_STATS       24	/* track rq completion times */
 +#define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
 +#define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
 +#define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
 +#define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
 +#define QUEUE_FLAG_PCI_P2PDMA  29	/* device supports PCI p2p requests */
 +
 +#define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 +				 (1 << QUEUE_FLAG_SAME_COMP)	|	\
 +				 (1 << QUEUE_FLAG_ADD_RANDOM))
++=======
+ #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
+ #define QUEUE_FLAG_DYING	1	/* queue being torn down */
+ #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
+ #define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
+ #define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
+ #define QUEUE_FLAG_NONROT	6	/* non-rotational device (SSD) */
+ #define QUEUE_FLAG_VIRT		QUEUE_FLAG_NONROT /* paravirt device */
+ #define QUEUE_FLAG_IO_STAT	7	/* do disk/partitions IO accounting */
+ #define QUEUE_FLAG_DISCARD	8	/* supports DISCARD */
+ #define QUEUE_FLAG_NOXMERGES	9	/* No extended merges */
+ #define QUEUE_FLAG_ADD_RANDOM	10	/* Contributes to random pool */
+ #define QUEUE_FLAG_SECERASE	11	/* supports secure erase */
+ #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
+ #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
+ #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
+ #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
+ #define QUEUE_FLAG_WC		17	/* Write back caching */
+ #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
+ #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+ #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
+ #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
+ #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
+ #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+ #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
+ #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
+ #define QUEUE_FLAG_ZONE_RESETALL 26	/* supports Zone Reset All */
+ #define QUEUE_FLAG_RQ_ALLOC_TIME 27	/* record rq->alloc_time_ns */
++>>>>>>> 6f816b4b746c (blk-mq: add optional request->alloc_time_ns)
  
  #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
  				 (1 << QUEUE_FLAG_SAME_COMP))
diff --git a/block/Kconfig b/block/Kconfig
index 8087cfa4988e..8de37a18e8db 100644
--- a/block/Kconfig
+++ b/block/Kconfig
@@ -26,6 +26,9 @@ menuconfig BLOCK
 
 if BLOCK
 
+config BLK_RQ_ALLOC_TIME
+	bool
+
 config BLK_SCSI_REQUEST
 	bool
 
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 14efeb259124..379aecf87dc3 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -290,7 +290,7 @@ static inline bool blk_mq_need_time_stamp(struct request *rq)
 }
 
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
-		unsigned int tag, unsigned int op)
+		unsigned int tag, unsigned int op, u64 alloc_time_ns)
 {
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct request *rq = tags->static_rqs[tag];
@@ -324,6 +324,9 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	RB_CLEAR_NODE(&rq->rb_node);
 	rq->rq_disk = NULL;
 	rq->part = NULL;
+#ifdef CONFIG_BLK_RQ_ALLOC_TIME
+	rq->alloc_time_ns = alloc_time_ns;
+#endif
 	if (blk_mq_need_time_stamp(rq))
 		rq->start_time_ns = ktime_get_ns();
 	else
@@ -357,8 +360,14 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	struct request *rq;
 	unsigned int tag;
 	bool clear_ctx_on_error = false;
+	u64 alloc_time_ns = 0;
 
 	blk_queue_enter_live(q);
+
+	/* alloc_time includes depth and tag waits */
+	if (blk_queue_rq_alloc_time(q))
+		alloc_time_ns = ktime_get_ns();
+
 	data->q = q;
 	if (likely(!data->ctx)) {
 		data->ctx = blk_mq_get_ctx(q);
@@ -394,7 +403,7 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		return NULL;
 	}
 
-	rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
+	rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags, alloc_time_ns);
 	if (!op_is_flush(data->cmd_flags)) {
 		rq->elv.icq = NULL;
 		if (e && e->type->ops.prepare_request) {
* Unmerged path include/linux/blkdev.h
