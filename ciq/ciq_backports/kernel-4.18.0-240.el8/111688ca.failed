sched/fair: Fix negative imbalance in imbalance calculation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Aubrey Li <aubrey.li@intel.com>
commit 111688ca1c4a43a7e482f5401f82c46326b8ed49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/111688ca.failed

A negative imbalance value was observed after imbalance calculation,
this happens when the local sched group type is group_fully_busy,
and the average load of local group is greater than the selected
busiest group. Fix this problem by comparing the average load of the
local and busiest group before imbalance calculation formula.

	Suggested-by: Vincent Guittot <vincent.guittot@linaro.org>
	Reviewed-by: Phil Auld <pauld@redhat.com>
	Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
	Acked-by: Mel Gorman <mgorman@suse.de>
	Signed-off-by: Aubrey Li <aubrey.li@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lkml.kernel.org/r/1585201349-70192-1-git-send-email-aubrey.li@intel.com
(cherry picked from commit 111688ca1c4a43a7e482f5401f82c46326b8ed49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 9bc68e2afc3d,02f323b85b6d..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -8387,17 -9022,28 +8387,39 @@@ static inline void calculate_imbalance(
  	}
  
  	/*
 -	 * Local is fully busy but has to take more load to relieve the
 -	 * busiest group
 +	 * If there aren't any idle CPUs, avoid creating some.
  	 */
++<<<<<<< HEAD
 +	if (busiest->group_type == group_overloaded &&
 +	    local->group_type   == group_overloaded) {
 +		load_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;
 +		if (load_above_capacity > busiest->group_capacity) {
 +			load_above_capacity -= busiest->group_capacity;
 +			load_above_capacity *= scale_load_down(NICE_0_LOAD);
 +			load_above_capacity /= busiest->group_capacity;
 +		} else
 +			load_above_capacity = ~0UL;
++=======
+ 	if (local->group_type < group_overloaded) {
+ 		/*
+ 		 * Local will become overloaded so the avg_load metrics are
+ 		 * finally needed.
+ 		 */
+ 
+ 		local->avg_load = (local->group_load * SCHED_CAPACITY_SCALE) /
+ 				  local->group_capacity;
+ 
+ 		sds->avg_load = (sds->total_load * SCHED_CAPACITY_SCALE) /
+ 				sds->total_capacity;
+ 		/*
+ 		 * If the local group is more loaded than the selected
+ 		 * busiest group don't try to pull any tasks.
+ 		 */
+ 		if (local->avg_load >= busiest->avg_load) {
+ 			env->imbalance = 0;
+ 			return;
+ 		}
++>>>>>>> 111688ca1c4a (sched/fair: Fix negative imbalance in imbalance calculation)
  	}
  
  	/*
* Unmerged path kernel/sched/fair.c
