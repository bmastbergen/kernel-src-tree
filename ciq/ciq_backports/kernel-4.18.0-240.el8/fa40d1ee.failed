mm: vmscan: memcontrol: remove mem_cgroup_select_victim_node()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit fa40d1ee9f156624658ca409a04a78882ca5b3c5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/fa40d1ee.failed

Since commit 1ba6fc9af35b ("mm: vmscan: do not share cgroup iteration
between reclaimers"), the memcg reclaim does not bail out earlier based
on sc->nr_reclaimed and will traverse all the nodes.  All the
reclaimable pages of the memcg on all the nodes will be scanned relative
to the reclaim priority.  So, there is no need to maintain state
regarding which node to start the memcg reclaim from.

This patch effectively reverts the commit 889976dbcb12 ("memcg: reclaim
memory from nodes in round-robin order") and commit 453a9bf347f1
("memcg: fix numa scan information update to be triggered by memory
event").

[shakeelb@google.com: v2]
  Link: http://lkml.kernel.org/r/20191030204232.139424-1-shakeelb@google.com
Link: http://lkml.kernel.org/r/20191029234753.224143-1-shakeelb@google.com
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: Roman Gushchin <guro@fb.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Greg Thelen <gthelen@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fa40d1ee9f156624658ca409a04a78882ca5b3c5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
#	mm/vmscan.c
diff --cc mm/memcontrol.c
index 983e0cfaa4c8,529e12a59131..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -1404,106 -1567,17 +1391,109 @@@ static bool mem_cgroup_out_of_memory(st
  	};
  	bool ret;
  
 -	if (mutex_lock_killable(&oom_lock))
 +	mutex_lock(&oom_lock);
 +	ret = out_of_memory(&oc);
 +	mutex_unlock(&oom_lock);
 +	return ret;
 +}
 +
++<<<<<<< HEAD
 +#if MAX_NUMNODES > 1
 +
 +/**
 + * test_mem_cgroup_node_reclaimable
 + * @memcg: the target memcg
 + * @nid: the node ID to be checked.
 + * @noswap : specify true here if the user wants flle only information.
 + *
 + * This function returns whether the specified memcg contains any
 + * reclaimable pages on a node. Returns true if there are any reclaimable
 + * pages in the node.
 + */
 +static bool test_mem_cgroup_node_reclaimable(struct mem_cgroup *memcg,
 +		int nid, bool noswap)
 +{
 +	if (mem_cgroup_node_nr_lru_pages(memcg, nid, LRU_ALL_FILE))
 +		return true;
 +	if (noswap || !total_swap_pages)
 +		return false;
 +	if (mem_cgroup_node_nr_lru_pages(memcg, nid, LRU_ALL_ANON))
  		return true;
 +	return false;
 +
 +}
 +
 +/*
 + * Always updating the nodemask is not very good - even if we have an empty
 + * list or the wrong list here, we can start from some node and traverse all
 + * nodes based on the zonelist. So update the list loosely once per 10 secs.
 + *
 + */
 +static void mem_cgroup_may_update_nodemask(struct mem_cgroup *memcg)
 +{
 +	int nid;
  	/*
 -	 * A few threads which were not waiting at mutex_lock_killable() can
 -	 * fail to bail out. Therefore, check again after holding oom_lock.
 +	 * numainfo_events > 0 means there was at least NUMAINFO_EVENTS_TARGET
 +	 * pagein/pageout changes since the last update.
  	 */
 -	ret = should_force_charge() || out_of_memory(&oc);
 -	mutex_unlock(&oom_lock);
 -	return ret;
 +	if (!atomic_read(&memcg->numainfo_events))
 +		return;
 +	if (atomic_inc_return(&memcg->numainfo_updating) > 1)
 +		return;
 +
 +	/* make a nodemask where this memcg uses memory from */
 +	memcg->scan_nodes = node_states[N_MEMORY];
 +
 +	for_each_node_mask(nid, node_states[N_MEMORY]) {
 +
 +		if (!test_mem_cgroup_node_reclaimable(memcg, nid, false))
 +			node_clear(nid, memcg->scan_nodes);
 +	}
 +
 +	atomic_set(&memcg->numainfo_events, 0);
 +	atomic_set(&memcg->numainfo_updating, 0);
  }
  
 +/*
 + * Selecting a node where we start reclaim from. Because what we need is just
 + * reducing usage counter, start from anywhere is O,K. Considering
 + * memory reclaim from current node, there are pros. and cons.
 + *
 + * Freeing memory from current node means freeing memory from a node which
 + * we'll use or we've used. So, it may make LRU bad. And if several threads
 + * hit limits, it will see a contention on a node. But freeing from remote
 + * node means more costs for memory reclaim because of memory latency.
 + *
 + * Now, we use round-robin. Better algorithm is welcomed.
 + */
 +int mem_cgroup_select_victim_node(struct mem_cgroup *memcg)
 +{
 +	int node;
 +
 +	mem_cgroup_may_update_nodemask(memcg);
 +	node = memcg->last_scanned_node;
 +
 +	node = next_node_in(node, memcg->scan_nodes);
 +	/*
 +	 * mem_cgroup_may_update_nodemask might have seen no reclaimmable pages
 +	 * last time it really checked all the LRUs due to rate limiting.
 +	 * Fallback to the current node in that case for simplicity.
 +	 */
 +	if (unlikely(node == MAX_NUMNODES))
 +		node = numa_node_id();
 +
 +	memcg->last_scanned_node = node;
 +	return node;
 +}
 +#else
 +int mem_cgroup_select_victim_node(struct mem_cgroup *memcg)
 +{
 +	return 0;
 +}
 +#endif
 +
++=======
++>>>>>>> fa40d1ee9f15 (mm: vmscan: memcontrol: remove mem_cgroup_select_victim_node())
  static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,
  				   pg_data_t *pgdat,
  				   gfp_t gfp_mask,
diff --cc mm/vmscan.c
index 52e40eb6d6d4,2beff0e0dc7b..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -3264,20 -3362,16 +3262,30 @@@ unsigned long try_to_free_mem_cgroup_pa
  		.may_unmap = 1,
  		.may_swap = may_swap,
  	};
+ 	/*
+ 	 * Traverse the ZONELIST_FALLBACK zonelist of the current node to put
+ 	 * equal pressure on all the nodes. This is based on the assumption that
+ 	 * the reclaim does not bail out early.
+ 	 */
+ 	struct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);
  
++<<<<<<< HEAD
 +	/*
 +	 * Unlike direct reclaim via alloc_pages(), memcg's reclaim doesn't
 +	 * take care of from where we get pages. So the node where we start the
 +	 * scan does not need to be the current node.
 +	 */
 +	nid = mem_cgroup_select_victim_node(memcg);
 +
 +	zonelist = &NODE_DATA(nid)->node_zonelists[ZONELIST_FALLBACK];
++=======
+ 	set_task_reclaim_state(current, &sc.reclaim_state);
++>>>>>>> fa40d1ee9f15 (mm: vmscan: memcontrol: remove mem_cgroup_select_victim_node())
  
 -	trace_mm_vmscan_memcg_reclaim_begin(0, sc.gfp_mask);
 +	trace_mm_vmscan_memcg_reclaim_begin(0,
 +					    sc.may_writepage,
 +					    sc.gfp_mask,
 +					    sc.reclaim_idx);
  
  	psi_memstall_enter(&pflags);
  	noreclaim_flag = memalloc_noreclaim_save();
diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index edfd7c1e1c27..0f0b0a4e30ef 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -90,7 +90,6 @@ struct mem_cgroup_id {
 enum mem_cgroup_events_target {
 	MEM_CGROUP_TARGET_THRESH,
 	MEM_CGROUP_TARGET_SOFTLIMIT,
-	MEM_CGROUP_TARGET_NUMAINFO,
 	MEM_CGROUP_NTARGETS,
 };
 
@@ -301,13 +300,6 @@ struct mem_cgroup {
 	struct list_head kmem_caches;
 #endif
 
-	int last_scanned_node;
-#if MAX_NUMNODES > 1
-	nodemask_t	scan_nodes;
-	atomic_t	numainfo_events;
-	atomic_t	numainfo_updating;
-#endif
-
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct list_head cgwb_list;
 	struct wb_domain cgwb_domain;
* Unmerged path mm/memcontrol.c
* Unmerged path mm/vmscan.c
