bpf, xdp: Start using the BPF dispatcher for XDP

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Björn Töpel <bjorn.topel@intel.com>
commit 7e6897f95935973c3253fd756135b5ea58043dc8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/7e6897f9.failed

This commit adds a BPF dispatcher for XDP. The dispatcher is updated
from the XDP control-path, dev_xdp_install(), and used when an XDP
program is run via bpf_prog_run_xdp().

	Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20191213175112.30208-4-bjorn.topel@gmail.com
(cherry picked from commit 7e6897f95935973c3253fd756135b5ea58043dc8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
diff --cc include/linux/bpf.h
index 602dd6841705,5970989b99d1..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -389,13 -397,193 +389,197 @@@ struct bpf_prog_stats 
  	struct u64_stats_sync syncp;
  } __aligned(2 * sizeof(u64));
  
++<<<<<<< HEAD
++=======
+ struct btf_func_model {
+ 	u8 ret_size;
+ 	u8 nr_args;
+ 	u8 arg_size[MAX_BPF_FUNC_ARGS];
+ };
+ 
+ /* Restore arguments before returning from trampoline to let original function
+  * continue executing. This flag is used for fentry progs when there are no
+  * fexit progs.
+  */
+ #define BPF_TRAMP_F_RESTORE_REGS	BIT(0)
+ /* Call original function after fentry progs, but before fexit progs.
+  * Makes sense for fentry/fexit, normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_CALL_ORIG		BIT(1)
+ /* Skip current frame and return to parent.  Makes sense for fentry/fexit
+  * programs only. Should not be used with normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_SKIP_FRAME		BIT(2)
+ 
+ /* Different use cases for BPF trampoline:
+  * 1. replace nop at the function entry (kprobe equivalent)
+  *    flags = BPF_TRAMP_F_RESTORE_REGS
+  *    fentry = a set of programs to run before returning from trampoline
+  *
+  * 2. replace nop at the function entry (kprobe + kretprobe equivalent)
+  *    flags = BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_SKIP_FRAME
+  *    orig_call = fentry_ip + MCOUNT_INSN_SIZE
+  *    fentry = a set of program to run before calling original function
+  *    fexit = a set of program to run after original function
+  *
+  * 3. replace direct call instruction anywhere in the function body
+  *    or assign a function pointer for indirect call (like tcp_congestion_ops->cong_avoid)
+  *    With flags = 0
+  *      fentry = a set of programs to run before returning from trampoline
+  *    With flags = BPF_TRAMP_F_CALL_ORIG
+  *      orig_call = original callback addr or direct function addr
+  *      fentry = a set of program to run before calling original function
+  *      fexit = a set of program to run after original function
+  */
+ int arch_prepare_bpf_trampoline(void *image, struct btf_func_model *m, u32 flags,
+ 				struct bpf_prog **fentry_progs, int fentry_cnt,
+ 				struct bpf_prog **fexit_progs, int fexit_cnt,
+ 				void *orig_call);
+ /* these two functions are called from generated trampoline */
+ u64 notrace __bpf_prog_enter(void);
+ void notrace __bpf_prog_exit(struct bpf_prog *prog, u64 start);
+ 
+ enum bpf_tramp_prog_type {
+ 	BPF_TRAMP_FENTRY,
+ 	BPF_TRAMP_FEXIT,
+ 	BPF_TRAMP_MAX
+ };
+ 
+ struct bpf_trampoline {
+ 	/* hlist for trampoline_table */
+ 	struct hlist_node hlist;
+ 	/* serializes access to fields of this trampoline */
+ 	struct mutex mutex;
+ 	refcount_t refcnt;
+ 	u64 key;
+ 	struct {
+ 		struct btf_func_model model;
+ 		void *addr;
+ 	} func;
+ 	/* list of BPF programs using this trampoline */
+ 	struct hlist_head progs_hlist[BPF_TRAMP_MAX];
+ 	/* Number of attached programs. A counter per kind. */
+ 	int progs_cnt[BPF_TRAMP_MAX];
+ 	/* Executable image of trampoline */
+ 	void *image;
+ 	u64 selector;
+ };
+ 
+ #define BPF_DISPATCHER_MAX 64 /* Fits in 2048B */
+ 
+ struct bpf_dispatcher_prog {
+ 	struct bpf_prog *prog;
+ 	refcount_t users;
+ };
+ 
+ struct bpf_dispatcher {
+ 	/* dispatcher mutex */
+ 	struct mutex mutex;
+ 	void *func;
+ 	struct bpf_dispatcher_prog progs[BPF_DISPATCHER_MAX];
+ 	int num_progs;
+ 	void *image;
+ 	u32 image_off;
+ };
+ 
+ static __always_inline unsigned int bpf_dispatcher_nopfunc(
+ 	const void *ctx,
+ 	const struct bpf_insn *insnsi,
+ 	unsigned int (*bpf_func)(const void *,
+ 				 const struct bpf_insn *))
+ {
+ 	return bpf_func(ctx, insnsi);
+ }
+ #ifdef CONFIG_BPF_JIT
+ struct bpf_trampoline *bpf_trampoline_lookup(u64 key);
+ int bpf_trampoline_link_prog(struct bpf_prog *prog);
+ int bpf_trampoline_unlink_prog(struct bpf_prog *prog);
+ void bpf_trampoline_put(struct bpf_trampoline *tr);
+ void *bpf_jit_alloc_exec_page(void);
+ #define BPF_DISPATCHER_INIT(name) {			\
+ 	.mutex = __MUTEX_INITIALIZER(name.mutex),	\
+ 	.func = &name##func,				\
+ 	.progs = {},					\
+ 	.num_progs = 0,					\
+ 	.image = NULL,					\
+ 	.image_off = 0					\
+ }
+ 
+ #define DEFINE_BPF_DISPATCHER(name)					\
+ 	noinline unsigned int name##func(				\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *))	\
+ 	{								\
+ 		return bpf_func(ctx, insnsi);				\
+ 	}								\
+ 	EXPORT_SYMBOL(name##func);			\
+ 	struct bpf_dispatcher name = BPF_DISPATCHER_INIT(name);
+ #define DECLARE_BPF_DISPATCHER(name)					\
+ 	unsigned int name##func(					\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *));	\
+ 	extern struct bpf_dispatcher name;
+ #define BPF_DISPATCHER_FUNC(name) name##func
+ #define BPF_DISPATCHER_PTR(name) (&name)
+ void bpf_dispatcher_change_prog(struct bpf_dispatcher *d, struct bpf_prog *from,
+ 				struct bpf_prog *to);
+ #else
+ static inline struct bpf_trampoline *bpf_trampoline_lookup(u64 key)
+ {
+ 	return NULL;
+ }
+ static inline int bpf_trampoline_link_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline int bpf_trampoline_unlink_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline void bpf_trampoline_put(struct bpf_trampoline *tr) {}
+ #define DEFINE_BPF_DISPATCHER(name)
+ #define DECLARE_BPF_DISPATCHER(name)
+ #define BPF_DISPATCHER_FUNC(name) bpf_dispatcher_nopfunc
+ #define BPF_DISPATCHER_PTR(name) NULL
+ static inline void bpf_dispatcher_change_prog(struct bpf_dispatcher *d,
+ 					      struct bpf_prog *from,
+ 					      struct bpf_prog *to) {}
+ #endif
+ 
+ struct bpf_func_info_aux {
+ 	bool unreliable;
+ };
+ 
+ enum bpf_jit_poke_reason {
+ 	BPF_POKE_REASON_TAIL_CALL,
+ };
+ 
+ /* Descriptor of pokes pointing /into/ the JITed image. */
+ struct bpf_jit_poke_descriptor {
+ 	void *ip;
+ 	union {
+ 		struct {
+ 			struct bpf_map *map;
+ 			u32 key;
+ 		} tail_call;
+ 	};
+ 	bool ip_stable;
+ 	u8 adj_off;
+ 	u16 reason;
+ };
+ 
++>>>>>>> 7e6897f95935 (bpf, xdp: Start using the BPF dispatcher for XDP)
  struct bpf_prog_aux {
 -	atomic64_t refcnt;
 +	atomic_t refcnt;
  	u32 used_map_cnt;
  	u32 max_ctx_offset;
 -	u32 max_pkt_offset;
 -	u32 max_tp_access;
 +	/* not protected by KABI, safe to extend in the middle */
 +	RH_KABI_BROKEN_INSERT(u32 max_pkt_offset)
 +	RH_KABI_BROKEN_INSERT(u32 max_tp_access)
  	u32 stack_depth;
  	u32 id;
  	u32 func_cnt; /* used by non-func prog as the number of func progs */
@@@ -788,7 -994,19 +972,9 @@@ int btf_struct_access(struct bpf_verifi
  		      const struct btf_type *t, int off, int size,
  		      enum bpf_access_type atype,
  		      u32 *next_btf_id);
 -int btf_resolve_helper_id(struct bpf_verifier_log *log,
 -			  const struct bpf_func_proto *fn, int);
 -
 -int btf_distill_func_proto(struct bpf_verifier_log *log,
 -			   struct btf *btf,
 -			   const struct btf_type *func_proto,
 -			   const char *func_name,
 -			   struct btf_func_model *m);
 -
 -int btf_check_func_arg_match(struct bpf_verifier_env *env, int subprog);
  
+ struct bpf_prog *bpf_prog_by_id(u32 id);
+ 
  #else /* !CONFIG_BPF_SYSCALL */
  static inline struct bpf_prog *bpf_prog_get(u32 ufd)
  {
@@@ -913,6 -1134,15 +1099,18 @@@ static inline int bpf_prog_test_run_flo
  {
  	return -ENOTSUPP;
  }
++<<<<<<< HEAD
++=======
+ 
+ static inline void bpf_map_put(struct bpf_map *map)
+ {
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_by_id(u32 id)
+ {
+ 	return ERR_PTR(-ENOTSUPP);
+ }
++>>>>>>> 7e6897f95935 (bpf, xdp: Start using the BPF dispatcher for XDP)
  #endif /* CONFIG_BPF_SYSCALL */
  
  static inline struct bpf_prog *bpf_prog_get_type(u32 ufd,
* Unmerged path include/linux/bpf.h
diff --git a/include/linux/filter.h b/include/linux/filter.h
index b3373f2e5cac..893ff8472012 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -566,23 +566,26 @@ struct sk_filter {
 
 DECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);
 
-#define BPF_PROG_RUN(prog, ctx)	({				\
-	u32 ret;						\
-	cant_sleep();						\
-	if (static_branch_unlikely(&bpf_stats_enabled_key)) {	\
-		struct bpf_prog_stats *stats;			\
-		u64 start = sched_clock();			\
-		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
-		stats = this_cpu_ptr(prog->aux->stats);		\
-		u64_stats_update_begin(&stats->syncp);		\
-		stats->cnt++;					\
-		stats->nsecs += sched_clock() - start;		\
-		u64_stats_update_end(&stats->syncp);		\
-	} else {						\
-		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
-	}							\
+#define __BPF_PROG_RUN(prog, ctx, dfunc)	({			\
+	u32 ret;							\
+	cant_sleep();							\
+	if (static_branch_unlikely(&bpf_stats_enabled_key)) {		\
+		struct bpf_prog_stats *stats;				\
+		u64 start = sched_clock();				\
+		ret = dfunc(ctx, (prog)->insnsi, (prog)->bpf_func);	\
+		stats = this_cpu_ptr(prog->aux->stats);			\
+		u64_stats_update_begin(&stats->syncp);			\
+		stats->cnt++;						\
+		stats->nsecs += sched_clock() - start;			\
+		u64_stats_update_end(&stats->syncp);			\
+	} else {							\
+		ret = dfunc(ctx, (prog)->insnsi, (prog)->bpf_func);	\
+	}								\
 	ret; })
 
+#define BPF_PROG_RUN(prog, ctx) __BPF_PROG_RUN(prog, ctx,		\
+					       bpf_dispatcher_nopfunc)
+
 #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 
 struct bpf_skb_data_end {
@@ -706,6 +709,8 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 	return res;
 }
 
+DECLARE_BPF_DISPATCHER(bpf_dispatcher_xdp)
+
 static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 					    struct xdp_buff *xdp)
 {
@@ -715,9 +720,12 @@ static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 	 * already takes rcu_read_lock() when fetching the program, so
 	 * it's not necessary here anymore.
 	 */
-	return BPF_PROG_RUN(prog, xdp);
+	return __BPF_PROG_RUN(prog, xdp,
+			      BPF_DISPATCHER_FUNC(bpf_dispatcher_xdp));
 }
 
+void bpf_prog_change_xdp(struct bpf_prog *prev_prog, struct bpf_prog *prog);
+
 static inline u32 bpf_prog_insn_size(const struct bpf_prog *prog)
 {
 	return prog->len * sizeof(struct bpf_insn);
diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4aa6a47aa4b8..7e738d5fc56c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2225,17 +2225,12 @@ static int bpf_obj_get_next_id(const union bpf_attr *attr,
 
 #define BPF_PROG_GET_FD_BY_ID_LAST_FIELD prog_id
 
-static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
+struct bpf_prog *bpf_prog_by_id(u32 id)
 {
 	struct bpf_prog *prog;
-	u32 id = attr->prog_id;
-	int fd;
-
-	if (CHECK_ATTR(BPF_PROG_GET_FD_BY_ID))
-		return -EINVAL;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (!id)
+		return ERR_PTR(-ENOENT);
 
 	spin_lock_bh(&prog_idr_lock);
 	prog = idr_find(&prog_idr, id);
@@ -2244,7 +2239,22 @@ static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
 	else
 		prog = ERR_PTR(-ENOENT);
 	spin_unlock_bh(&prog_idr_lock);
+	return prog;
+}
+
+static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
+{
+	struct bpf_prog *prog;
+	u32 id = attr->prog_id;
+	int fd;
+
+	if (CHECK_ATTR(BPF_PROG_GET_FD_BY_ID))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
+	prog = bpf_prog_by_id(id);
 	if (IS_ERR(prog))
 		return PTR_ERR(prog);
 
diff --git a/net/core/dev.c b/net/core/dev.c
index 1a8bca927008..1c6cc6a15331 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -8134,7 +8134,17 @@ static int dev_xdp_install(struct net_device *dev, bpf_op_t bpf_op,
 			   struct netlink_ext_ack *extack, u32 flags,
 			   struct bpf_prog *prog)
 {
+	bool non_hw = !(flags & XDP_FLAGS_HW_MODE);
+	struct bpf_prog *prev_prog = NULL;
 	struct netdev_bpf xdp;
+	int err;
+
+	if (non_hw) {
+		prev_prog = bpf_prog_by_id(__dev_xdp_query(dev, bpf_op,
+							   XDP_QUERY_PROG));
+		if (IS_ERR(prev_prog))
+			prev_prog = NULL;
+	}
 
 	memset(&xdp, 0, sizeof(xdp));
 	if (flags & XDP_FLAGS_HW_MODE)
@@ -8145,7 +8155,14 @@ static int dev_xdp_install(struct net_device *dev, bpf_op_t bpf_op,
 	xdp.flags = flags;
 	xdp.prog = prog;
 
-	return bpf_op(dev, &xdp);
+	err = bpf_op(dev, &xdp);
+	if (!err && non_hw)
+		bpf_prog_change_xdp(prev_prog, prog);
+
+	if (prev_prog)
+		bpf_prog_put(prev_prog);
+
+	return err;
 }
 
 static void dev_xdp_uninstall(struct net_device *dev)
diff --git a/net/core/filter.c b/net/core/filter.c
index 80b28d966162..c63f3e555fa9 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -8930,3 +8930,11 @@ const struct bpf_verifier_ops sk_reuseport_verifier_ops = {
 const struct bpf_prog_ops sk_reuseport_prog_ops = {
 };
 #endif /* CONFIG_INET */
+
+DEFINE_BPF_DISPATCHER(bpf_dispatcher_xdp)
+
+void bpf_prog_change_xdp(struct bpf_prog *prev_prog, struct bpf_prog *prog)
+{
+	bpf_dispatcher_change_prog(BPF_DISPATCHER_PTR(bpf_dispatcher_xdp),
+				   prev_prog, prog);
+}
