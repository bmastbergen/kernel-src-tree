ftrace: Fix function_graph tracer interaction with BPF trampoline

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Alexei Starovoitov <ast@kernel.org>
commit ff205766dbbee024a4a716638868d98ffb17748a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ff205766.failed

Depending on type of BPF programs served by BPF trampoline it can call original
function. In such case the trampoline will skip one stack frame while
returning. That will confuse function_graph tracer and will cause crashes with
bad RIP. Teach graph tracer to skip functions that have BPF trampoline attached.

	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
(cherry picked from commit ff205766dbbee024a4a716638868d98ffb17748a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/ftrace.h
#	kernel/trace/fgraph.c
#	kernel/trace/ftrace.c
diff --cc include/linux/ftrace.h
index bc7a15c979c6,987c2dc55bde..000000000000
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@@ -243,6 -246,75 +243,78 @@@ static inline void ftrace_free_init_mem
  static inline void ftrace_free_mem(struct module *mod, void *start, void *end) { }
  #endif /* CONFIG_FUNCTION_TRACER */
  
++<<<<<<< HEAD
++=======
+ struct ftrace_func_entry {
+ 	struct hlist_node hlist;
+ 	unsigned long ip;
+ 	unsigned long direct; /* for direct lookup only */
+ };
+ 
+ struct dyn_ftrace;
+ 
+ #ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
+ extern int ftrace_direct_func_count;
+ int register_ftrace_direct(unsigned long ip, unsigned long addr);
+ int unregister_ftrace_direct(unsigned long ip, unsigned long addr);
+ int modify_ftrace_direct(unsigned long ip, unsigned long old_addr, unsigned long new_addr);
+ struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr);
+ int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
+ 				struct dyn_ftrace *rec,
+ 				unsigned long old_addr,
+ 				unsigned long new_addr);
+ unsigned long ftrace_find_rec_direct(unsigned long ip);
+ #else
+ # define ftrace_direct_func_count 0
+ static inline int register_ftrace_direct(unsigned long ip, unsigned long addr)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline int unregister_ftrace_direct(unsigned long ip, unsigned long addr)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline int modify_ftrace_direct(unsigned long ip,
+ 				       unsigned long old_addr, unsigned long new_addr)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr)
+ {
+ 	return NULL;
+ }
+ static inline int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
+ 					      struct dyn_ftrace *rec,
+ 					      unsigned long old_addr,
+ 					      unsigned long new_addr)
+ {
+ 	return -ENODEV;
+ }
+ static inline unsigned long ftrace_find_rec_direct(unsigned long ip)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
+ 
+ #ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
+ /*
+  * This must be implemented by the architecture.
+  * It is the way the ftrace direct_ops helper, when called
+  * via ftrace (because there's other callbacks besides the
+  * direct call), can inform the architecture's trampoline that this
+  * routine has a direct caller, and what the caller is.
+  *
+  * For example, in x86, it returns the direct caller
+  * callback function via the regs->orig_ax parameter.
+  * Then in the ftrace trampoline, if this is set, it makes
+  * the return from the trampoline jump to the direct caller
+  * instead of going back to the function it just traced.
+  */
+ static inline void arch_ftrace_set_direct_caller(struct pt_regs *regs,
+ 						 unsigned long addr) { }
+ #endif /* CONFIG_HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
+ 
++>>>>>>> ff205766dbbe (ftrace: Fix function_graph tracer interaction with BPF trampoline)
  #ifdef CONFIG_STACK_TRACER
  
  extern int stack_tracer_enabled;
diff --cc kernel/trace/ftrace.c
index fc893884b2b3,57477dc683db..000000000000
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@@ -2361,6 -2354,47 +2361,50 @@@ ftrace_find_tramp_ops_new(struct dyn_ft
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
+ /* Protected by rcu_tasks for reading, and direct_mutex for writing */
+ static struct ftrace_hash *direct_functions = EMPTY_HASH;
+ static DEFINE_MUTEX(direct_mutex);
+ int ftrace_direct_func_count;
+ 
+ /*
+  * Search the direct_functions hash to see if the given instruction pointer
+  * has a direct caller attached to it.
+  */
+ unsigned long ftrace_find_rec_direct(unsigned long ip)
+ {
+ 	struct ftrace_func_entry *entry;
+ 
+ 	entry = __ftrace_lookup_ip(direct_functions, ip);
+ 	if (!entry)
+ 		return 0;
+ 
+ 	return entry->direct;
+ }
+ 
+ static void call_direct_funcs(unsigned long ip, unsigned long pip,
+ 			      struct ftrace_ops *ops, struct pt_regs *regs)
+ {
+ 	unsigned long addr;
+ 
+ 	addr = ftrace_find_rec_direct(ip);
+ 	if (!addr)
+ 		return;
+ 
+ 	arch_ftrace_set_direct_caller(regs, addr);
+ }
+ 
+ struct ftrace_ops direct_ops = {
+ 	.func		= call_direct_funcs,
+ 	.flags		= FTRACE_OPS_FL_IPMODIFY | FTRACE_OPS_FL_RECURSION_SAFE
+ 			  | FTRACE_OPS_FL_DIRECT | FTRACE_OPS_FL_SAVE_REGS
+ 			  | FTRACE_OPS_FL_PERMANENT,
+ };
+ #endif /* CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
+ 
++>>>>>>> ff205766dbbe (ftrace: Fix function_graph tracer interaction with BPF trampoline)
  /**
   * ftrace_get_addr_new - Get the call address to set to
   * @rec:  The ftrace record descriptor
@@@ -2374,6 -2408,15 +2418,18 @@@
  unsigned long ftrace_get_addr_new(struct dyn_ftrace *rec)
  {
  	struct ftrace_ops *ops;
++<<<<<<< HEAD
++=======
+ 	unsigned long addr;
+ 
+ 	if ((rec->flags & FTRACE_FL_DIRECT) &&
+ 	    (ftrace_rec_count(rec) == 1)) {
+ 		addr = ftrace_find_rec_direct(rec->ip);
+ 		if (addr)
+ 			return addr;
+ 		WARN_ON_ONCE(1);
+ 	}
++>>>>>>> ff205766dbbe (ftrace: Fix function_graph tracer interaction with BPF trampoline)
  
  	/* Trampolines take precedence over regs */
  	if (rec->flags & FTRACE_FL_TRAMP) {
@@@ -2406,6 -2449,15 +2462,18 @@@
  unsigned long ftrace_get_addr_curr(struct dyn_ftrace *rec)
  {
  	struct ftrace_ops *ops;
++<<<<<<< HEAD
++=======
+ 	unsigned long addr;
+ 
+ 	/* Direct calls take precedence over trampolines */
+ 	if (rec->flags & FTRACE_FL_DIRECT_EN) {
+ 		addr = ftrace_find_rec_direct(rec->ip);
+ 		if (addr)
+ 			return addr;
+ 		WARN_ON_ONCE(1);
+ 	}
++>>>>>>> ff205766dbbe (ftrace: Fix function_graph tracer interaction with BPF trampoline)
  
  	/* Trampolines take precedence over regs */
  	if (rec->flags & FTRACE_FL_TRAMP_EN) {
@@@ -3534,6 -3596,13 +3602,16 @@@ static int t_show(struct seq_file *m, v
  		} else {
  			add_trampoline_func(m, NULL, rec);
  		}
++<<<<<<< HEAD
++=======
+ 		if (rec->flags & FTRACE_FL_DIRECT) {
+ 			unsigned long direct;
+ 
+ 			direct = ftrace_find_rec_direct(rec->ip);
+ 			if (direct)
+ 				seq_printf(m, "\n\tdirect-->%pS", (void *)direct);
+ 		}
++>>>>>>> ff205766dbbe (ftrace: Fix function_graph tracer interaction with BPF trampoline)
  	}	
  
  	seq_putc(m, '\n');
@@@ -4806,9 -4931,369 +4884,372 @@@ static in
  ftrace_set_addr(struct ftrace_ops *ops, unsigned long ip, int remove,
  		int reset, int enable)
  {
 -	return ftrace_set_hash(ops, NULL, 0, ip, remove, reset, enable);
 +	return ftrace_set_hash(ops, 0, 0, ip, remove, reset, enable);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
+ 
+ struct ftrace_direct_func {
+ 	struct list_head	next;
+ 	unsigned long		addr;
+ 	int			count;
+ };
+ 
+ static LIST_HEAD(ftrace_direct_funcs);
+ 
+ /**
+  * ftrace_find_direct_func - test an address if it is a registered direct caller
+  * @addr: The address of a registered direct caller
+  *
+  * This searches to see if a ftrace direct caller has been registered
+  * at a specific address, and if so, it returns a descriptor for it.
+  *
+  * This can be used by architecture code to see if an address is
+  * a direct caller (trampoline) attached to a fentry/mcount location.
+  * This is useful for the function_graph tracer, as it may need to
+  * do adjustments if it traced a location that also has a direct
+  * trampoline attached to it.
+  */
+ struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr)
+ {
+ 	struct ftrace_direct_func *entry;
+ 	bool found = false;
+ 
+ 	/* May be called by fgraph trampoline (protected by rcu tasks) */
+ 	list_for_each_entry_rcu(entry, &ftrace_direct_funcs, next) {
+ 		if (entry->addr == addr) {
+ 			found = true;
+ 			break;
+ 		}
+ 	}
+ 	if (found)
+ 		return entry;
+ 
+ 	return NULL;
+ }
+ 
+ /**
+  * register_ftrace_direct - Call a custom trampoline directly
+  * @ip: The address of the nop at the beginning of a function
+  * @addr: The address of the trampoline to call at @ip
+  *
+  * This is used to connect a direct call from the nop location (@ip)
+  * at the start of ftrace traced functions. The location that it calls
+  * (@addr) must be able to handle a direct call, and save the parameters
+  * of the function being traced, and restore them (or inject new ones
+  * if needed), before returning.
+  *
+  * Returns:
+  *  0 on success
+  *  -EBUSY - Another direct function is already attached (there can be only one)
+  *  -ENODEV - @ip does not point to a ftrace nop location (or not supported)
+  *  -ENOMEM - There was an allocation failure.
+  */
+ int register_ftrace_direct(unsigned long ip, unsigned long addr)
+ {
+ 	struct ftrace_direct_func *direct;
+ 	struct ftrace_func_entry *entry;
+ 	struct ftrace_hash *free_hash = NULL;
+ 	struct dyn_ftrace *rec;
+ 	int ret = -EBUSY;
+ 
+ 	mutex_lock(&direct_mutex);
+ 
+ 	/* See if there's a direct function at @ip already */
+ 	if (ftrace_find_rec_direct(ip))
+ 		goto out_unlock;
+ 
+ 	ret = -ENODEV;
+ 	rec = lookup_rec(ip, ip);
+ 	if (!rec)
+ 		goto out_unlock;
+ 
+ 	/*
+ 	 * Check if the rec says it has a direct call but we didn't
+ 	 * find one earlier?
+ 	 */
+ 	if (WARN_ON(rec->flags & FTRACE_FL_DIRECT))
+ 		goto out_unlock;
+ 
+ 	/* Make sure the ip points to the exact record */
+ 	if (ip != rec->ip) {
+ 		ip = rec->ip;
+ 		/* Need to check this ip for a direct. */
+ 		if (ftrace_find_rec_direct(ip))
+ 			goto out_unlock;
+ 	}
+ 
+ 	ret = -ENOMEM;
+ 	if (ftrace_hash_empty(direct_functions) ||
+ 	    direct_functions->count > 2 * (1 << direct_functions->size_bits)) {
+ 		struct ftrace_hash *new_hash;
+ 		int size = ftrace_hash_empty(direct_functions) ? 0 :
+ 			direct_functions->count + 1;
+ 
+ 		if (size < 32)
+ 			size = 32;
+ 
+ 		new_hash = dup_hash(direct_functions, size);
+ 		if (!new_hash)
+ 			goto out_unlock;
+ 
+ 		free_hash = direct_functions;
+ 		direct_functions = new_hash;
+ 	}
+ 
+ 	entry = kmalloc(sizeof(*entry), GFP_KERNEL);
+ 	if (!entry)
+ 		goto out_unlock;
+ 
+ 	direct = ftrace_find_direct_func(addr);
+ 	if (!direct) {
+ 		direct = kmalloc(sizeof(*direct), GFP_KERNEL);
+ 		if (!direct) {
+ 			kfree(entry);
+ 			goto out_unlock;
+ 		}
+ 		direct->addr = addr;
+ 		direct->count = 0;
+ 		list_add_rcu(&direct->next, &ftrace_direct_funcs);
+ 		ftrace_direct_func_count++;
+ 	}
+ 
+ 	entry->ip = ip;
+ 	entry->direct = addr;
+ 	__add_hash_entry(direct_functions, entry);
+ 
+ 	ret = ftrace_set_filter_ip(&direct_ops, ip, 0, 0);
+ 	if (ret)
+ 		remove_hash_entry(direct_functions, entry);
+ 
+ 	if (!ret && !(direct_ops.flags & FTRACE_OPS_FL_ENABLED)) {
+ 		ret = register_ftrace_function(&direct_ops);
+ 		if (ret)
+ 			ftrace_set_filter_ip(&direct_ops, ip, 1, 0);
+ 	}
+ 
+ 	if (ret) {
+ 		kfree(entry);
+ 		if (!direct->count) {
+ 			list_del_rcu(&direct->next);
+ 			synchronize_rcu_tasks();
+ 			kfree(direct);
+ 			if (free_hash)
+ 				free_ftrace_hash(free_hash);
+ 			free_hash = NULL;
+ 			ftrace_direct_func_count--;
+ 		}
+ 	} else {
+ 		direct->count++;
+ 	}
+  out_unlock:
+ 	mutex_unlock(&direct_mutex);
+ 
+ 	if (free_hash) {
+ 		synchronize_rcu_tasks();
+ 		free_ftrace_hash(free_hash);
+ 	}
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(register_ftrace_direct);
+ 
+ static struct ftrace_func_entry *find_direct_entry(unsigned long *ip,
+ 						   struct dyn_ftrace **recp)
+ {
+ 	struct ftrace_func_entry *entry;
+ 	struct dyn_ftrace *rec;
+ 
+ 	rec = lookup_rec(*ip, *ip);
+ 	if (!rec)
+ 		return NULL;
+ 
+ 	entry = __ftrace_lookup_ip(direct_functions, rec->ip);
+ 	if (!entry) {
+ 		WARN_ON(rec->flags & FTRACE_FL_DIRECT);
+ 		return NULL;
+ 	}
+ 
+ 	WARN_ON(!(rec->flags & FTRACE_FL_DIRECT));
+ 
+ 	/* Passed in ip just needs to be on the call site */
+ 	*ip = rec->ip;
+ 
+ 	if (recp)
+ 		*recp = rec;
+ 
+ 	return entry;
+ }
+ 
+ int unregister_ftrace_direct(unsigned long ip, unsigned long addr)
+ {
+ 	struct ftrace_direct_func *direct;
+ 	struct ftrace_func_entry *entry;
+ 	int ret = -ENODEV;
+ 
+ 	mutex_lock(&direct_mutex);
+ 
+ 	entry = find_direct_entry(&ip, NULL);
+ 	if (!entry)
+ 		goto out_unlock;
+ 
+ 	if (direct_functions->count == 1)
+ 		unregister_ftrace_function(&direct_ops);
+ 
+ 	ret = ftrace_set_filter_ip(&direct_ops, ip, 1, 0);
+ 
+ 	WARN_ON(ret);
+ 
+ 	remove_hash_entry(direct_functions, entry);
+ 
+ 	direct = ftrace_find_direct_func(addr);
+ 	if (!WARN_ON(!direct)) {
+ 		/* This is the good path (see the ! before WARN) */
+ 		direct->count--;
+ 		WARN_ON(direct->count < 0);
+ 		if (!direct->count) {
+ 			list_del_rcu(&direct->next);
+ 			synchronize_rcu_tasks();
+ 			kfree(direct);
+ 			ftrace_direct_func_count--;
+ 		}
+ 	}
+  out_unlock:
+ 	mutex_unlock(&direct_mutex);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(unregister_ftrace_direct);
+ 
+ static struct ftrace_ops stub_ops = {
+ 	.func		= ftrace_stub,
+ };
+ 
+ /**
+  * ftrace_modify_direct_caller - modify ftrace nop directly
+  * @entry: The ftrace hash entry of the direct helper for @rec
+  * @rec: The record representing the function site to patch
+  * @old_addr: The location that the site at @rec->ip currently calls
+  * @new_addr: The location that the site at @rec->ip should call
+  *
+  * An architecture may overwrite this function to optimize the
+  * changing of the direct callback on an ftrace nop location.
+  * This is called with the ftrace_lock mutex held, and no other
+  * ftrace callbacks are on the associated record (@rec). Thus,
+  * it is safe to modify the ftrace record, where it should be
+  * currently calling @old_addr directly, to call @new_addr.
+  *
+  * Safety checks should be made to make sure that the code at
+  * @rec->ip is currently calling @old_addr. And this must
+  * also update entry->direct to @new_addr.
+  */
+ int __weak ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
+ 				       struct dyn_ftrace *rec,
+ 				       unsigned long old_addr,
+ 				       unsigned long new_addr)
+ {
+ 	unsigned long ip = rec->ip;
+ 	int ret;
+ 
+ 	/*
+ 	 * The ftrace_lock was used to determine if the record
+ 	 * had more than one registered user to it. If it did,
+ 	 * we needed to prevent that from changing to do the quick
+ 	 * switch. But if it did not (only a direct caller was attached)
+ 	 * then this function is called. But this function can deal
+ 	 * with attached callers to the rec that we care about, and
+ 	 * since this function uses standard ftrace calls that take
+ 	 * the ftrace_lock mutex, we need to release it.
+ 	 */
+ 	mutex_unlock(&ftrace_lock);
+ 
+ 	/*
+ 	 * By setting a stub function at the same address, we force
+ 	 * the code to call the iterator and the direct_ops helper.
+ 	 * This means that @ip does not call the direct call, and
+ 	 * we can simply modify it.
+ 	 */
+ 	ret = ftrace_set_filter_ip(&stub_ops, ip, 0, 0);
+ 	if (ret)
+ 		goto out_lock;
+ 
+ 	ret = register_ftrace_function(&stub_ops);
+ 	if (ret) {
+ 		ftrace_set_filter_ip(&stub_ops, ip, 1, 0);
+ 		goto out_lock;
+ 	}
+ 
+ 	entry->direct = new_addr;
+ 
+ 	/*
+ 	 * By removing the stub, we put back the direct call, calling
+ 	 * the @new_addr.
+ 	 */
+ 	unregister_ftrace_function(&stub_ops);
+ 	ftrace_set_filter_ip(&stub_ops, ip, 1, 0);
+ 
+  out_lock:
+ 	mutex_lock(&ftrace_lock);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * modify_ftrace_direct - Modify an existing direct call to call something else
+  * @ip: The instruction pointer to modify
+  * @old_addr: The address that the current @ip calls directly
+  * @new_addr: The address that the @ip should call
+  *
+  * This modifies a ftrace direct caller at an instruction pointer without
+  * having to disable it first. The direct call will switch over to the
+  * @new_addr without missing anything.
+  *
+  * Returns: zero on success. Non zero on error, which includes:
+  *  -ENODEV : the @ip given has no direct caller attached
+  *  -EINVAL : the @old_addr does not match the current direct caller
+  */
+ int modify_ftrace_direct(unsigned long ip,
+ 			 unsigned long old_addr, unsigned long new_addr)
+ {
+ 	struct ftrace_func_entry *entry;
+ 	struct dyn_ftrace *rec;
+ 	int ret = -ENODEV;
+ 
+ 	mutex_lock(&direct_mutex);
+ 
+ 	mutex_lock(&ftrace_lock);
+ 	entry = find_direct_entry(&ip, &rec);
+ 	if (!entry)
+ 		goto out_unlock;
+ 
+ 	ret = -EINVAL;
+ 	if (entry->direct != old_addr)
+ 		goto out_unlock;
+ 
+ 	/*
+ 	 * If there's no other ftrace callback on the rec->ip location,
+ 	 * then it can be changed directly by the architecture.
+ 	 * If there is another caller, then we just need to change the
+ 	 * direct caller helper to point to @new_addr.
+ 	 */
+ 	if (ftrace_rec_count(rec) == 1) {
+ 		ret = ftrace_modify_direct_caller(entry, rec, old_addr, new_addr);
+ 	} else {
+ 		entry->direct = new_addr;
+ 		ret = 0;
+ 	}
+ 
+  out_unlock:
+ 	mutex_unlock(&ftrace_lock);
+ 	mutex_unlock(&direct_mutex);
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(modify_ftrace_direct);
+ #endif /* CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS */
+ 
++>>>>>>> ff205766dbbe (ftrace: Fix function_graph tracer interaction with BPF trampoline)
  /**
   * ftrace_set_filter_ip - set a function to filter on in ftrace by address
   * @ops - the ops to set the filter with
* Unmerged path kernel/trace/fgraph.c
* Unmerged path include/linux/ftrace.h
* Unmerged path kernel/trace/fgraph.c
* Unmerged path kernel/trace/ftrace.c
