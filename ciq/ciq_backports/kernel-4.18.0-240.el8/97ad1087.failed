memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mike Rapoport <rppt@linux.vnet.ibm.com>
commit 97ad1087efffed26cb00e310a927f9603332dfcb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/97ad1087.failed

Drop BOOTMEM_ALLOC_ACCESSIBLE and BOOTMEM_ALLOC_ANYWHERE in favor of
identical MEMBLOCK definitions.

Link: http://lkml.kernel.org/r/1536927045-23536-29-git-send-email-rppt@linux.vnet.ibm.com
	Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Chris Zankel <chris@zankel.net>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Greentime Hu <green.hu@gmail.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Guan Xuetao <gxt@pku.edu.cn>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
	Cc: Jonas Bonn <jonas@southpole.se>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Ley Foon Tan <lftan@altera.com>
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Simek <monstr@monstr.eu>
	Cc: Palmer Dabbelt <palmer@sifive.com>
	Cc: Paul Burton <paul.burton@mips.com>
	Cc: Richard Kuo <rkuo@codeaurora.org>
	Cc: Richard Weinberger <richard@nod.at>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Serge Semin <fancer.lancer@gmail.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Vineet Gupta <vgupta@synopsys.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 97ad1087efffed26cb00e310a927f9603332dfcb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/ia64/mm/discontig.c
#	arch/powerpc/kernel/setup_64.c
#	arch/sparc/kernel/smp_64.c
#	arch/x86/mm/kasan_init_64.c
#	mm/kasan/kasan_init.c
#	mm/sparse-vmemmap.c
diff --cc arch/ia64/mm/discontig.c
index 7d9bd20319ff,70609f823960..000000000000
--- a/arch/ia64/mm/discontig.c
+++ b/arch/ia64/mm/discontig.c
@@@ -492,8 -451,10 +492,15 @@@ static void __init *memory_less_node_al
  	if (bestnode == -1)
  		bestnode = anynode;
  
++<<<<<<< HEAD
 +	ptr = __alloc_bootmem_node(pgdat_list[bestnode], pernodesize,
 +		PERCPU_PAGE_SIZE, __pa(MAX_DMA_ADDRESS));
++=======
+ 	ptr = memblock_alloc_try_nid(pernodesize, PERCPU_PAGE_SIZE,
+ 				     __pa(MAX_DMA_ADDRESS),
+ 				     MEMBLOCK_ALLOC_ACCESSIBLE,
+ 				     bestnode);
++>>>>>>> 97ad1087efff (memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants)
  
  	return ptr;
  }
diff --cc arch/powerpc/kernel/setup_64.c
index d9cafc1c5e09,9216c3a7fcfc..000000000000
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@@ -762,8 -763,10 +762,15 @@@ void __init emergency_stack_init(void
  
  static void * __init pcpu_fc_alloc(unsigned int cpu, size_t size, size_t align)
  {
++<<<<<<< HEAD
 +	return __alloc_bootmem_node(NODE_DATA(early_cpu_to_node(cpu)), size, align,
 +				    __pa(MAX_DMA_ADDRESS));
++=======
+ 	return memblock_alloc_try_nid(size, align, __pa(MAX_DMA_ADDRESS),
+ 				      MEMBLOCK_ALLOC_ACCESSIBLE,
+ 				      early_cpu_to_node(cpu));
+ 
++>>>>>>> 97ad1087efff (memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants)
  }
  
  static void __init pcpu_fc_free(void *ptr, size_t size)
diff --cc arch/sparc/kernel/smp_64.c
index d3ea1f3c06a0,6cc80d0f4b9f..000000000000
--- a/arch/sparc/kernel/smp_64.c
+++ b/arch/sparc/kernel/smp_64.c
@@@ -1594,8 -1594,8 +1594,13 @@@ static void * __init pcpu_alloc_bootmem
  		pr_debug("per cpu data for cpu%d %lu bytes at %016lx\n",
  			 cpu, size, __pa(ptr));
  	} else {
++<<<<<<< HEAD
 +		ptr = __alloc_bootmem_node(NODE_DATA(node),
 +					   size, align, goal);
++=======
+ 		ptr = memblock_alloc_try_nid(size, align, goal,
+ 					     MEMBLOCK_ALLOC_ACCESSIBLE, node);
++>>>>>>> 97ad1087efff (memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants)
  		pr_debug("per cpu data for cpu%d %lu bytes on node%d at "
  			 "%016lx\n", cpu, size, node, __pa(ptr));
  	}
diff --cc arch/x86/mm/kasan_init_64.c
index e3e77527f8df,8f87499124b8..000000000000
--- a/arch/x86/mm/kasan_init_64.c
+++ b/arch/x86/mm/kasan_init_64.c
@@@ -28,11 -28,11 +28,19 @@@ static p4d_t tmp_p4d_table[MAX_PTRS_PER
  static __init void *early_alloc(size_t size, int nid, bool panic)
  {
  	if (panic)
++<<<<<<< HEAD
 +		return memblock_virt_alloc_try_nid(size, size,
 +			__pa(MAX_DMA_ADDRESS), BOOTMEM_ALLOC_ACCESSIBLE, nid);
 +	else
 +		return memblock_virt_alloc_try_nid_nopanic(size, size,
 +			__pa(MAX_DMA_ADDRESS), BOOTMEM_ALLOC_ACCESSIBLE, nid);
++=======
+ 		return memblock_alloc_try_nid(size, size,
+ 			__pa(MAX_DMA_ADDRESS), MEMBLOCK_ALLOC_ACCESSIBLE, nid);
+ 	else
+ 		return memblock_alloc_try_nid_nopanic(size, size,
+ 			__pa(MAX_DMA_ADDRESS), MEMBLOCK_ALLOC_ACCESSIBLE, nid);
++>>>>>>> 97ad1087efff (memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants)
  }
  
  static void __init kasan_populate_pmd(pmd_t *pmd, unsigned long addr,
diff --cc mm/kasan/kasan_init.c
index 7a2a2f13f86f,785a9707786b..000000000000
--- a/mm/kasan/kasan_init.c
+++ b/mm/kasan/kasan_init.c
@@@ -83,8 -83,8 +83,13 @@@ static inline bool kasan_zero_page_entr
  
  static __init void *early_alloc(size_t size, int node)
  {
++<<<<<<< HEAD
 +	return memblock_virt_alloc_try_nid(size, size, __pa(MAX_DMA_ADDRESS),
 +					BOOTMEM_ALLOC_ACCESSIBLE, node);
++=======
+ 	return memblock_alloc_try_nid(size, size, __pa(MAX_DMA_ADDRESS),
+ 					MEMBLOCK_ALLOC_ACCESSIBLE, node);
++>>>>>>> 97ad1087efff (memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants)
  }
  
  static void __ref zero_pte_populate(pmd_t *pmd, unsigned long addr,
diff --cc mm/sparse-vmemmap.c
index 560f92e52b56,7408cabed61a..000000000000
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@@ -42,8 -43,8 +43,13 @@@ static void * __ref __earlyonly_bootmem
  				unsigned long align,
  				unsigned long goal)
  {
++<<<<<<< HEAD
 +	return memblock_virt_alloc_try_nid_raw(size, align, goal,
 +					       BOOTMEM_ALLOC_ACCESSIBLE, node);
++=======
+ 	return memblock_alloc_try_nid_raw(size, align, goal,
+ 					       MEMBLOCK_ALLOC_ACCESSIBLE, node);
++>>>>>>> 97ad1087efff (memblock: replace BOOTMEM_ALLOC_* with MEMBLOCK variants)
  }
  
  void * __meminit vmemmap_alloc_block(unsigned long size, int node)
* Unmerged path arch/ia64/mm/discontig.c
* Unmerged path arch/powerpc/kernel/setup_64.c
* Unmerged path arch/sparc/kernel/smp_64.c
diff --git a/arch/x86/kernel/setup_percpu.c b/arch/x86/kernel/setup_percpu.c
index 67d48e26a8f2..f291fc724d13 100644
--- a/arch/x86/kernel/setup_percpu.c
+++ b/arch/x86/kernel/setup_percpu.c
@@ -113,7 +113,7 @@ static void * __init pcpu_alloc_bootmem(unsigned int cpu, unsigned long size,
 			 cpu, size, __pa(ptr));
 	} else {
 		ptr = memblock_alloc_try_nid_nopanic(size, align, goal,
-						     BOOTMEM_ALLOC_ACCESSIBLE,
+						     MEMBLOCK_ALLOC_ACCESSIBLE,
 						     node);
 
 		pr_debug("per cpu data for cpu%d %lu bytes on node%d at %016lx\n",
* Unmerged path arch/x86/mm/kasan_init_64.c
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 449fb9efecb9..e1e54c62e28b 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -16,6 +16,7 @@
 #include <linux/cpuset.h>
 #include <linux/mutex.h>
 #include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/sysfs.h>
 #include <linux/slab.h>
 #include <linux/mmdebug.h>
@@ -2162,7 +2163,7 @@ int __alloc_bootmem_huge_page(struct hstate *h)
 
 		addr = memblock_virt_alloc_try_nid_raw(
 				huge_page_size(h), huge_page_size(h),
-				0, BOOTMEM_ALLOC_ACCESSIBLE, node);
+				0, MEMBLOCK_ALLOC_ACCESSIBLE, node);
 		if (addr) {
 			/*
 			 * Use the beginning of the huge page to store the
* Unmerged path mm/kasan/kasan_init.c
diff --git a/mm/memblock.c b/mm/memblock.c
index cd97dba20bb1..47ae52a87f2b 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -1334,7 +1334,7 @@ phys_addr_t __init memblock_phys_alloc_try_nid(phys_addr_t size, phys_addr_t ali
  * hold the requested memory.
  *
  * The allocation is performed from memory region limited by
- * memblock.current_limit if @max_addr == %BOOTMEM_ALLOC_ACCESSIBLE.
+ * memblock.current_limit if @max_addr == %MEMBLOCK_ALLOC_ACCESSIBLE.
  *
  * The memory block is aligned on %SMP_CACHE_BYTES if @align == 0.
  *
@@ -1423,7 +1423,7 @@ static void * __init memblock_virt_alloc_internal(
  * @min_addr: the lower bound of the memory region from where the allocation
  *	  is preferred (phys address)
  * @max_addr: the upper bound of the memory region from where the allocation
- *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+ *	      is preferred (phys address), or %MEMBLOCK_ALLOC_ACCESSIBLE to
  *	      allocate only from memory limited by memblock.current_limit value
  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
  *
@@ -1460,7 +1460,7 @@ void * __init memblock_virt_alloc_try_nid_raw(
  * @min_addr: the lower bound of the memory region from where the allocation
  *	  is preferred (phys address)
  * @max_addr: the upper bound of the memory region from where the allocation
- *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+ *	      is preferred (phys address), or %MEMBLOCK_ALLOC_ACCESSIBLE to
  *	      allocate only from memory limited by memblock.current_limit value
  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
  *
@@ -1495,7 +1495,7 @@ void * __init memblock_virt_alloc_try_nid_nopanic(
  * @min_addr: the lower bound of the memory region from where the allocation
  *	  is preferred (phys address)
  * @max_addr: the upper bound of the memory region from where the allocation
- *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+ *	      is preferred (phys address), or %MEMBLOCK_ALLOC_ACCESSIBLE to
  *	      allocate only from memory limited by memblock.current_limit value
  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
  *
diff --git a/mm/page_ext.c b/mm/page_ext.c
index 5295ef331165..5a0bc3fc4254 100644
--- a/mm/page_ext.c
+++ b/mm/page_ext.c
@@ -163,7 +163,7 @@ static int __init alloc_node_page_ext(int nid)
 
 	base = memblock_virt_alloc_try_nid_nopanic(
 			table_size, PAGE_SIZE, __pa(MAX_DMA_ADDRESS),
-			BOOTMEM_ALLOC_ACCESSIBLE, nid);
+			MEMBLOCK_ALLOC_ACCESSIBLE, nid);
 	if (!base)
 		return -ENOMEM;
 	NODE_DATA(nid)->node_page_ext = base;
* Unmerged path mm/sparse-vmemmap.c
diff --git a/mm/sparse.c b/mm/sparse.c
index 743faed13a8f..8e169fca4d86 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -6,6 +6,7 @@
 #include <linux/slab.h>
 #include <linux/mmzone.h>
 #include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/compiler.h>
 #include <linux/highmem.h>
 #include <linux/export.h>
@@ -435,7 +436,7 @@ struct page __init *__populate_section_memmap(unsigned long pfn,
 
 	map = memblock_virt_alloc_try_nid(size,
 					  PAGE_SIZE, __pa(MAX_DMA_ADDRESS),
-					  BOOTMEM_ALLOC_ACCESSIBLE, nid);
+					  MEMBLOCK_ALLOC_ACCESSIBLE, nid);
 	return map;
 }
 #endif /* !CONFIG_SPARSEMEM_VMEMMAP */
@@ -449,7 +450,7 @@ static void __init sparse_buffer_init(unsigned long size, int nid)
 	sparsemap_buf =
 		memblock_virt_alloc_try_nid_raw(size, PAGE_SIZE,
 						__pa(MAX_DMA_ADDRESS),
-						BOOTMEM_ALLOC_ACCESSIBLE, nid);
+						MEMBLOCK_ALLOC_ACCESSIBLE, nid);
 	sparsemap_buf_end = sparsemap_buf + size;
 }
 
