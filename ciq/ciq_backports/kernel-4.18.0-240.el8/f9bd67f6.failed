io_uring: only !null ptr to io_issue_sqe()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit f9bd67f69af56d712bfd498f5ad9cf7bb177d600
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/f9bd67f6.failed

Pass only non-null @nxt to io_issue_sqe() and handle it at the caller's
side. And propagate it.

- kiocb_done() is only called from io_read() and io_write(), which are
only called from io_issue_sqe(), so it's @nxt != NULL

- io_put_req_find_next() is called either with explicitly non-null local
nxt, or from one of the functions in io_issue_sqe() switch (or their
callees).

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit f9bd67f69af56d712bfd498f5ad9cf7bb177d600)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 66de1d702552,553fa23120e6..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -679,14 -955,48 +679,59 @@@ static void io_free_req(struct io_kioc
  	 * dependencies to the next request. In case of failure, fail the rest
  	 * of the chain.
  	 */
++<<<<<<< HEAD
 +	if (req->flags & REQ_F_LINK) {
 +		if (req->flags & REQ_F_FAIL_LINK)
 +			io_fail_links(req);
 +		else
 +			io_req_link_next(req);
 +	}
 +
 +	__io_free_req(req);
++=======
+ 	if (req->flags & REQ_F_FAIL_LINK) {
+ 		io_fail_links(req);
+ 	} else if ((req->flags & (REQ_F_LINK_TIMEOUT | REQ_F_COMP_LOCKED)) ==
+ 			REQ_F_LINK_TIMEOUT) {
+ 		struct io_ring_ctx *ctx = req->ctx;
+ 		unsigned long flags;
+ 
+ 		/*
+ 		 * If this is a timeout link, we could be racing with the
+ 		 * timeout timer. Grab the completion lock for this case to
+ 		 * protect against that.
+ 		 */
+ 		spin_lock_irqsave(&ctx->completion_lock, flags);
+ 		io_req_link_next(req, nxt);
+ 		spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	} else {
+ 		io_req_link_next(req, nxt);
+ 	}
+ }
+ 
+ static void io_free_req(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	io_req_find_next(req, &nxt);
+ 	__io_free_req(req);
+ 
+ 	if (nxt)
+ 		io_queue_async_work(nxt);
+ }
+ 
+ /*
+  * Drop reference to request, return next in chain (if there is one) if this
+  * was the last reference to this request.
+  */
+ __attribute__((nonnull))
+ static void io_put_req_find_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	io_req_find_next(req, nxtptr);
+ 
+ 	if (refcount_dec_and_test(&req->refs))
+ 		__io_free_req(req);
++>>>>>>> f9bd67f69af5 (io_uring: only !null ptr to io_issue_sqe())
  }
  
  static void io_put_req(struct io_kiocb *req)
@@@ -1100,6 -1477,15 +1145,18 @@@ static inline void io_rw_done(struct ki
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void kiocb_done(struct kiocb *kiocb, ssize_t ret, struct io_kiocb **nxt,
+ 		       bool in_async)
+ {
+ 	if (in_async && ret >= 0 && kiocb->ki_complete == io_complete_rw)
+ 		*nxt = __io_complete_rw(kiocb, ret);
+ 	else
+ 		io_rw_done(kiocb, ret);
+ }
+ 
++>>>>>>> f9bd67f69af5 (io_uring: only !null ptr to io_issue_sqe())
  static int io_import_fixed(struct io_ring_ctx *ctx, int rw,
  			   const struct io_uring_sqe *sqe,
  			   struct iov_iter *iter)
@@@ -1865,12 -2577,13 +1922,18 @@@ static int io_req_defer(struct io_ring_
  	return -EIOCBQUEUED;
  }
  
++<<<<<<< HEAD
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
++=======
+ __attribute__((nonnull))
+ static int io_issue_sqe(struct io_kiocb *req, struct io_kiocb **nxt,
+ 			bool force_nonblock)
++>>>>>>> f9bd67f69af5 (io_uring: only !null ptr to io_issue_sqe())
  {
  	int ret, opcode;
 -	struct sqe_submit *s = &req->submit;
 -	struct io_ring_ctx *ctx = req->ctx;
 +
 +	req->user_data = READ_ONCE(s->sqe->user_data);
  
  	opcode = READ_ONCE(s->sqe->opcode);
  	switch (opcode) {
@@@ -2162,13 -2791,124 +2225,132 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static int io_grab_files(struct io_kiocb *req)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
++<<<<<<< HEAD
 +	int ret;
 +
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
++=======
+ 	int ret = -EBADF;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irq(&ctx->inflight_lock);
+ 	/*
+ 	 * We use the f_ops->flush() handler to ensure that we can flush
+ 	 * out work accessing these files if the fd is closed. Check if
+ 	 * the fd has changed since we started down this path, and disallow
+ 	 * this operation if it has.
+ 	 */
+ 	if (fcheck(req->submit.ring_fd) == req->submit.ring_file) {
+ 		list_add(&req->inflight_entry, &ctx->inflight_list);
+ 		req->flags |= REQ_F_INFLIGHT;
+ 		req->work.files = current->files;
+ 		ret = 0;
+ 	}
+ 	spin_unlock_irq(&ctx->inflight_lock);
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_timeout_data *data = container_of(timer,
+ 						struct io_timeout_data, timer);
+ 	struct io_kiocb *req = data->req;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		prev = list_entry(req->list.prev, struct io_kiocb, link_list);
+ 		if (refcount_inc_not_zero(&prev->refs)) {
+ 			list_del_init(&req->list);
+ 			prev->flags &= ~REQ_F_LINK_TIMEOUT;
+ 		} else
+ 			prev = NULL;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		if (prev->flags & REQ_F_LINK)
+ 			prev->flags |= REQ_F_FAIL_LINK;
+ 		io_async_find_and_cancel(ctx, req, prev->user_data, NULL,
+ 						-ETIME);
+ 		io_put_req(prev);
+ 	} else {
+ 		io_cqring_add_event(req, -ETIME);
+ 		io_put_req(req);
+ 	}
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static void io_queue_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	/*
+ 	 * If the list is now empty, then our linked request finished before
+ 	 * we got a chance to setup the timer
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (!list_empty(&req->list)) {
+ 		struct io_timeout_data *data = req->timeout.data;
+ 
+ 		data->timer.function = io_link_timeout_fn;
+ 		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
+ 				data->mode);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	/* drop submission reference */
+ 	io_put_req(req);
+ }
+ 
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
+ 	if (!nxt || nxt->submit.sqe->opcode != IORING_OP_LINK_TIMEOUT)
+ 		return NULL;
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 	return nxt;
+ }
+ 
+ static void __io_queue_sqe(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *linked_timeout = io_prep_linked_timeout(req);
+ 	struct io_kiocb *nxt = NULL;
+ 	int ret;
+ 
+ 	ret = io_issue_sqe(req, &nxt, true);
+ 	if (nxt)
+ 		io_queue_async_work(nxt);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		struct sqe_submit *s = &req->submit;
++>>>>>>> f9bd67f69af5 (io_uring: only !null ptr to io_issue_sqe())
  		struct io_uring_sqe *sqe_copy;
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
@@@ -2196,9 -2935,17 +2378,19 @@@
  	/* drop submission reference */
  	io_put_req(req);
  
++<<<<<<< HEAD
++=======
+ 	if (linked_timeout) {
+ 		if (!ret)
+ 			io_queue_linked_timeout(linked_timeout);
+ 		else
+ 			io_put_req(linked_timeout);
+ 	}
+ 
++>>>>>>> f9bd67f69af5 (io_uring: only !null ptr to io_issue_sqe())
  	/* and drop final reference, if we failed */
  	if (ret) {
 -		io_cqring_add_event(req, ret);
 +		io_cqring_add_event(ctx, req->user_data, ret);
  		if (req->flags & REQ_F_LINK)
  			req->flags |= REQ_F_FAIL_LINK;
  		io_put_req(req);
* Unmerged path fs/io_uring.c
