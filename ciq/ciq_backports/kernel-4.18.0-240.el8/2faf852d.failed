io_uring: cleanup fixed file data table references

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 2faf852d1be8a4960d328492298da6448cca0279
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/2faf852d.failed

syzbot reports a use-after-free in io_ring_file_ref_switch() when it
tries to switch back to percpu mode. When we put the final reference to
the table by calling percpu_ref_kill_and_confirm(), we don't want the
zero reference to queue async work for flushing the potentially queued
up items. We currently do a few flush_work(), but they merely paper
around the issue, since the work item may not have been queued yet
depending on the when the percpu-ref callback gets run.

Coming into the file unregister, we know we have the ring quiesced.
io_ring_file_ref_switch() can check for whether or not the ref is dying
or not, and not queue anything async at that point. Once the ref has
been confirmed killed, flush any potential items manually.

	Reported-by: syzbot+7caeaea49c2c8a591e3d@syzkaller.appspotmail.com
Fixes: 05f3fb3c5397 ("io_uring: avoid ring quiesce for fixed file set unregister and update")
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 2faf852d1be8a4960d328492298da6448cca0279)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index ffb8e9d82a6a,deff11e84094..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -366,8 -597,163 +366,168 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_ring_file_ref_flush(struct fixed_file_data *data);
++>>>>>>> 2faf852d1be8 (io_uring: cleanup fixed file data table references)
  
  static struct kmem_cache *req_cachep;
  
@@@ -2749,14 -5246,35 +2909,23 @@@ static void __io_sqe_files_unregister(s
  #endif
  }
  
 -static void io_file_ref_kill(struct percpu_ref *ref)
 -{
 -	struct fixed_file_data *data;
 -
 -	data = container_of(ref, struct fixed_file_data, refs);
 -	complete(&data->done);
 -}
 -
  static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
  {
 -	struct fixed_file_data *data = ctx->file_data;
 -	unsigned nr_tables, i;
 -
 -	if (!data)
 +	if (!ctx->user_files)
  		return -ENXIO;
  
++<<<<<<< HEAD
++=======
+ 	percpu_ref_kill_and_confirm(&data->refs, io_file_ref_kill);
+ 	flush_work(&data->ref_work);
+ 	wait_for_completion(&data->done);
+ 	io_ring_file_ref_flush(data);
+ 	percpu_ref_exit(&data->refs);
+ 
++>>>>>>> 2faf852d1be8 (io_uring: cleanup fixed file data table references)
  	__io_sqe_files_unregister(ctx);
 -	nr_tables = DIV_ROUND_UP(ctx->nr_user_files, IORING_MAX_FILES_TABLE);
 -	for (i = 0; i < nr_tables; i++)
 -		kfree(data->table[i].files);
 -	kfree(data->table);
 -	kfree(data);
 -	ctx->file_data = NULL;
 +	kfree(ctx->user_files);
 +	ctx->user_files = NULL;
  	ctx->nr_user_files = 0;
  	return 0;
  }
@@@ -3030,6 -5497,163 +3199,166 @@@ static void io_sqe_file_unregister(stru
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ struct io_file_put {
+ 	struct llist_node llist;
+ 	struct file *file;
+ 	struct completion *done;
+ };
+ 
+ static void io_ring_file_ref_flush(struct fixed_file_data *data)
+ {
+ 	struct io_file_put *pfile, *tmp;
+ 	struct llist_node *node;
+ 
+ 	while ((node = llist_del_all(&data->put_llist)) != NULL) {
+ 		llist_for_each_entry_safe(pfile, tmp, node, llist) {
+ 			io_ring_file_put(data->ctx, pfile->file);
+ 			if (pfile->done)
+ 				complete(pfile->done);
+ 			else
+ 				kfree(pfile);
+ 		}
+ 	}
+ }
+ 
+ static void io_ring_file_ref_switch(struct work_struct *work)
+ {
+ 	struct fixed_file_data *data;
+ 
+ 	data = container_of(work, struct fixed_file_data, ref_work);
+ 	io_ring_file_ref_flush(data);
+ 	percpu_ref_get(&data->refs);
+ 	percpu_ref_switch_to_percpu(&data->refs);
+ }
+ 
+ static void io_file_data_ref_zero(struct percpu_ref *ref)
+ {
+ 	struct fixed_file_data *data;
+ 
+ 	data = container_of(ref, struct fixed_file_data, refs);
+ 
+ 	/*
+ 	 * We can't safely switch from inside this context, punt to wq. If
+ 	 * the table ref is going away, the table is being unregistered.
+ 	 * Don't queue up the async work for that case, the caller will
+ 	 * handle it.
+ 	 */
+ 	if (!percpu_ref_is_dying(&data->refs))
+ 		queue_work(system_wq, &data->ref_work);
+ }
+ 
+ static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
+ 				 unsigned nr_args)
+ {
+ 	__s32 __user *fds = (__s32 __user *) arg;
+ 	unsigned nr_tables;
+ 	struct file *file;
+ 	int fd, ret = 0;
+ 	unsigned i;
+ 
+ 	if (ctx->file_data)
+ 		return -EBUSY;
+ 	if (!nr_args)
+ 		return -EINVAL;
+ 	if (nr_args > IORING_MAX_FIXED_FILES)
+ 		return -EMFILE;
+ 
+ 	ctx->file_data = kzalloc(sizeof(*ctx->file_data), GFP_KERNEL);
+ 	if (!ctx->file_data)
+ 		return -ENOMEM;
+ 	ctx->file_data->ctx = ctx;
+ 	init_completion(&ctx->file_data->done);
+ 
+ 	nr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);
+ 	ctx->file_data->table = kcalloc(nr_tables,
+ 					sizeof(struct fixed_file_table),
+ 					GFP_KERNEL);
+ 	if (!ctx->file_data->table) {
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	if (percpu_ref_init(&ctx->file_data->refs, io_file_data_ref_zero,
+ 				PERCPU_REF_ALLOW_REINIT, GFP_KERNEL)) {
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 	ctx->file_data->put_llist.first = NULL;
+ 	INIT_WORK(&ctx->file_data->ref_work, io_ring_file_ref_switch);
+ 
+ 	if (io_sqe_alloc_file_tables(ctx, nr_tables, nr_args)) {
+ 		percpu_ref_exit(&ctx->file_data->refs);
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for (i = 0; i < nr_args; i++, ctx->nr_user_files++) {
+ 		struct fixed_file_table *table;
+ 		unsigned index;
+ 
+ 		ret = -EFAULT;
+ 		if (copy_from_user(&fd, &fds[i], sizeof(fd)))
+ 			break;
+ 		/* allow sparse sets */
+ 		if (fd == -1) {
+ 			ret = 0;
+ 			continue;
+ 		}
+ 
+ 		table = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];
+ 		index = i & IORING_FILE_TABLE_MASK;
+ 		file = fget(fd);
+ 
+ 		ret = -EBADF;
+ 		if (!file)
+ 			break;
+ 
+ 		/*
+ 		 * Don't allow io_uring instances to be registered. If UNIX
+ 		 * isn't enabled, then this causes a reference cycle and this
+ 		 * instance can never get freed. If UNIX is enabled we'll
+ 		 * handle it just fine, but there's still no point in allowing
+ 		 * a ring fd as it doesn't support regular read/write anyway.
+ 		 */
+ 		if (file->f_op == &io_uring_fops) {
+ 			fput(file);
+ 			break;
+ 		}
+ 		ret = 0;
+ 		table->files[index] = file;
+ 	}
+ 
+ 	if (ret) {
+ 		for (i = 0; i < ctx->nr_user_files; i++) {
+ 			file = io_file_from_index(ctx, i);
+ 			if (file)
+ 				fput(file);
+ 		}
+ 		for (i = 0; i < nr_tables; i++)
+ 			kfree(ctx->file_data->table[i].files);
+ 
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		ctx->nr_user_files = 0;
+ 		return ret;
+ 	}
+ 
+ 	ret = io_sqe_files_scm(ctx);
+ 	if (ret)
+ 		io_sqe_files_unregister(ctx);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 2faf852d1be8 (io_uring: cleanup fixed file data table references)
  static int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,
  				int index)
  {
* Unmerged path fs/io_uring.c
