io_uring: cleanup return values from the queueing functions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 0e0702dac26b282603261f04a62711a2d9aac17b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/0e0702da.failed

__io_queue_sqe(), io_queue_sqe(), io_queue_link_head() all return 0/err,
but the caller doesn't care since the errors are handled inline. Clean
these up and just make them void.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 0e0702dac26b282603261f04a62711a2d9aac17b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index f656b9c7fa46,d877c7f6368e..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2163,13 -2808,61 +2163,64 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req,
 -					       struct timespec64 *ts,
 -					       enum hrtimer_mode *mode)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_kiocb *nxt;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
++=======
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
+ 	if (!nxt || nxt->submit.sqe->opcode != IORING_OP_LINK_TIMEOUT)
+ 		return NULL;
+ 
+ 	ret = io_validate_link_timeout(nxt->submit.sqe, ts);
+ 	if (ret) {
+ 		list_del_init(&nxt->list);
+ 		io_cqring_add_event(nxt, ret);
+ 		io_double_put_req(nxt);
+ 		return ERR_PTR(-ECANCELED);
+ 	}
+ 
+ 	if (nxt->submit.sqe->timeout_flags & IORING_TIMEOUT_ABS)
+ 		*mode = HRTIMER_MODE_ABS;
+ 	else
+ 		*mode = HRTIMER_MODE_REL;
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 	hrtimer_init(&nxt->timeout.timer, CLOCK_MONOTONIC, *mode);
+ 	return nxt;
+ }
+ 
+ static void __io_queue_sqe(struct io_kiocb *req)
+ {
+ 	enum hrtimer_mode mode;
+ 	struct io_kiocb *nxt;
+ 	struct timespec64 ts;
+ 	int ret;
+ 
+ 	nxt = io_prep_linked_timeout(req, &ts, &mode);
+ 	if (IS_ERR(nxt)) {
+ 		ret = PTR_ERR(nxt);
+ 		nxt = NULL;
+ 		goto err;
+ 	}
+ 
+ 	ret = __io_submit_sqe(req, NULL, true);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
+ 		struct sqe_submit *s = &req->submit;
++>>>>>>> 0e0702dac26b (io_uring: cleanup return values from the queueing functions)
  		struct io_uring_sqe *sqe_copy;
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
@@@ -2190,7 -2880,12 +2241,16 @@@
  			 * Queued up for async execution, worker will release
  			 * submit reference when the iocb is actually submitted.
  			 */
++<<<<<<< HEAD
 +			return 0;
++=======
+ 			io_queue_async_work(req);
+ 
+ 			if (nxt)
+ 				io_queue_linked_timeout(nxt, &ts, &mode);
+ 
+ 			return;
++>>>>>>> 0e0702dac26b (io_uring: cleanup return values from the queueing functions)
  		}
  	}
  
@@@ -2204,36 -2907,32 +2264,53 @@@
  			req->flags |= REQ_F_FAIL_LINK;
  		io_put_req(req);
  	}
- 
- 	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
++=======
+ static void io_queue_sqe(struct io_kiocb *req)
++>>>>>>> 0e0702dac26b (io_uring: cleanup return values from the queueing functions)
  {
  	int ret;
  
 -	ret = io_req_defer(req);
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -			io_cqring_add_event(req, ret);
 -			io_double_put_req(req);
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
  		}
++<<<<<<< HEAD
 +		return 0;
 +	}
 +
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
 +}
 +
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
++=======
+ 	} else
+ 		__io_queue_sqe(req);
+ }
+ 
+ static void io_queue_link_head(struct io_kiocb *req, struct io_kiocb *shadow)
++>>>>>>> 0e0702dac26b (io_uring: cleanup return values from the queueing functions)
  {
  	int ret;
  	int need_submit = false;
 -	struct io_ring_ctx *ctx = req->ctx;
  
++<<<<<<< HEAD
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
++=======
+ 	if (!shadow) {
+ 		io_queue_sqe(req);
+ 		return;
+ 	}
++>>>>>>> 0e0702dac26b (io_uring: cleanup return values from the queueing functions)
  
  	/*
  	 * Mark the first IO in link list as DRAIN, let all the following
@@@ -2241,13 -2940,13 +2318,17 @@@
  	 * list.
  	 */
  	req->flags |= REQ_F_IO_DRAIN;
 -	ret = io_req_defer(req);
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -			io_cqring_add_event(req, ret);
 -			io_double_put_req(req);
 +			io_free_req(req);
  			__io_free_req(shadow);
++<<<<<<< HEAD
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
++=======
+ 			return;
++>>>>>>> 0e0702dac26b (io_uring: cleanup return values from the queueing functions)
  		}
  	} else {
  		/*
@@@ -2263,9 -2963,7 +2344,13 @@@
  	spin_unlock_irq(&ctx->completion_lock);
  
  	if (need_submit)
++<<<<<<< HEAD
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	return 0;
++=======
+ 		__io_queue_sqe(req);
++>>>>>>> 0e0702dac26b (io_uring: cleanup return values from the queueing functions)
  }
  
  #define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
* Unmerged path fs/io_uring.c
