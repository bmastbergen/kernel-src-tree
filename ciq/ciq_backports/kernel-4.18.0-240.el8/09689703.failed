RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 09689703d29a3b75c510c198c3aca85d7d8b50c7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/09689703.failed

For creation, as soon as the umem_odp is created the notifier can be
called, however the underlying MR may not have been setup yet. This would
cause problems if mlx5_ib_invalidate_range() runs. There is some
confusing/ulocked/racy code that might by trying to solve this, but
without locks it isn't going to work right.

Instead trivially solve the problem by short-circuiting the invalidation
if there are not yet any DMA mapped pages. By definition there is nothing
to invalidate in this case.

The create code will have the umem fully setup before anything is DMA
mapped, and npages is fully locked by the umem_mutex.

For destroy, invalidate the entire MR at the HW to stop DMA then DMA unmap
the pages before destroying the MR. This drives npages to zero and
prevents similar racing with invalidate while the MR is undergoing
destruction.

Arguably it would be better if the umem was created after the MR and
destroyed before, but that would require a big rework of the MR code.

Fixes: 6aec21f6a832 ("IB/mlx5: Page faults handling infrastructure")
Link: https://lore.kernel.org/r/20191009160934.3143-15-jgg@ziepe.ca
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 09689703d29a3b75c510c198c3aca85d7d8b50c7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/mr.c
index c7b8ab228778,199f7959aaa5..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -1573,54 -1567,20 +1579,67 @@@ static void dereg_mr(struct mlx5_ib_de
  	int npages = mr->npages;
  	struct ib_umem *umem = mr->umem;
  
- 	if (is_odp_mr(mr)) {
- 		struct ib_umem_odp *umem_odp = to_ib_umem_odp(umem);
+ 	/* Stop all DMA */
+ 	if (is_odp_mr(mr))
+ 		mlx5_ib_fence_odp_mr(mr);
+ 	else
+ 		clean_mr(dev, mr);
  
++<<<<<<< HEAD
 +		/* Prevent new page faults and
 +		 * prefetch requests from succeeding
 +		 */
 +		mr->live = 0;
 +
 +		/* Wait for all running page-fault handlers to finish. */
 +		synchronize_srcu(&dev->mr_srcu);
 +
 +		/* dequeue pending prefetch requests for the mr */
 +		if (atomic_read(&mr->num_pending_prefetch))
 +			flush_workqueue(system_unbound_wq);
 +		WARN_ON(atomic_read(&mr->num_pending_prefetch));
 +
 +		/* Destroy all page mappings */
 +		if (umem_odp->page_list)
 +			mlx5_ib_invalidate_range(umem_odp,
 +						 ib_umem_start(umem_odp),
 +						 ib_umem_end(umem_odp));
 +		else
 +			mlx5_ib_free_implicit_mr(mr);
 +		/*
 +		 * We kill the umem before the MR for ODP,
 +		 * so that there will not be any invalidations in
 +		 * flight, looking at the *mr struct.
 +		 */
 +		ib_umem_release(umem);
 +		atomic_sub(npages, &dev->mdev->priv.reg_pages);
 +
 +		/* Avoid double-freeing the umem. */
 +		umem = NULL;
 +	}
 +
 +	clean_mr(dev, mr);
 +
 +	/*
 +	 * We should unregister the DMA address from the HCA before
 +	 * remove the DMA mapping.
 +	 */
 +	mlx5_mr_cache_free(dev, mr);
 +	ib_umem_release(umem);
 +	if (umem)
 +		atomic_sub(npages, &dev->mdev->priv.reg_pages);
 +
 +	if (!mr->allocated_from_cache)
++=======
+ 	if (mr->allocated_from_cache)
+ 		mlx5_mr_cache_free(dev, mr);
+ 	else
++>>>>>>> 09689703d29a (RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy)
  		kfree(mr);
+ 
+ 	ib_umem_release(umem);
+ 	atomic_sub(npages, &dev->mdev->priv.reg_pages);
+ 
  }
  
  int mlx5_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
diff --cc drivers/infiniband/hw/mlx5/odp.c
index df3038ca913b,bcfc09846697..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -220,31 -144,101 +220,69 @@@ void mlx5_odp_populate_klm(struct mlx5_
  	}
  }
  
++<<<<<<< HEAD
 +static void mr_leaf_free_action(struct work_struct *work)
++=======
+ static void dma_fence_odp_mr(struct mlx5_ib_mr *mr)
+ {
+ 	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
+ 
+ 	/* Ensure mlx5_ib_invalidate_range() will not touch the MR any more */
+ 	mutex_lock(&odp->umem_mutex);
+ 	if (odp->npages) {
+ 		mlx5_mr_cache_invalidate(mr);
+ 		ib_umem_odp_unmap_dma_pages(odp, ib_umem_start(odp),
+ 					    ib_umem_end(odp));
+ 		WARN_ON(odp->npages);
+ 	}
+ 	odp->private = NULL;
+ 	mutex_unlock(&odp->umem_mutex);
+ 
+ 	if (!mr->allocated_from_cache) {
+ 		mlx5_core_destroy_mkey(mr->dev->mdev, &mr->mmkey);
+ 		WARN_ON(mr->descs);
+ 	}
+ }
+ 
+ /*
+  * This must be called after the mr has been removed from implicit_children
+  * and the SRCU synchronized.  NOTE: The MR does not necessarily have to be
+  * empty here, parallel page faults could have raced with the free process and
+  * added pages to it.
+  */
+ static void free_implicit_child_mr(struct mlx5_ib_mr *mr, bool need_imr_xlt)
++>>>>>>> 09689703d29a (RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy)
  {
 -	struct mlx5_ib_mr *imr = mr->parent;
 +	struct ib_umem_odp *odp = container_of(work, struct ib_umem_odp, work);
 +	int idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
 +	struct mlx5_ib_mr *mr = odp->private, *imr = mr->parent;
  	struct ib_umem_odp *odp_imr = to_ib_umem_odp(imr->umem);
 -	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
 -	unsigned long idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
  	int srcu_key;
  
 -	/* implicit_child_mr's are not allowed to have deferred work */
 -	WARN_ON(atomic_read(&mr->num_deferred_work));
 +	mr->parent = NULL;
 +	synchronize_srcu(&mr->dev->mr_srcu);
  
 -	if (need_imr_xlt) {
 -		srcu_key = srcu_read_lock(&mr->dev->odp_srcu);
 +	if (imr->live) {
 +		srcu_key = srcu_read_lock(&mr->dev->mr_srcu);
  		mutex_lock(&odp_imr->umem_mutex);
 -		mlx5_ib_update_xlt(mr->parent, idx, 1, 0,
 +		mlx5_ib_update_xlt(imr, idx, 1, 0,
  				   MLX5_IB_UPD_XLT_INDIRECT |
  				   MLX5_IB_UPD_XLT_ATOMIC);
  		mutex_unlock(&odp_imr->umem_mutex);
 -		srcu_read_unlock(&mr->dev->odp_srcu, srcu_key);
 +		srcu_read_unlock(&mr->dev->mr_srcu, srcu_key);
  	}
++<<<<<<< HEAD
 +	ib_umem_release(&odp->umem);
++=======
+ 
+ 	dma_fence_odp_mr(mr);
+ 
+ 	mr->parent = NULL;
++>>>>>>> 09689703d29a (RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy)
  	mlx5_mr_cache_free(mr->dev, mr);
 -	ib_umem_odp_release(odp);
 -	atomic_dec(&imr->num_deferred_work);
 -}
 -
 -static void free_implicit_child_mr_work(struct work_struct *work)
 -{
 -	struct mlx5_ib_mr *mr =
 -		container_of(work, struct mlx5_ib_mr, odp_destroy.work);
 -
 -	free_implicit_child_mr(mr, true);
 -}
 -
 -static void free_implicit_child_mr_rcu(struct rcu_head *head)
 -{
 -	struct mlx5_ib_mr *mr =
 -		container_of(head, struct mlx5_ib_mr, odp_destroy.rcu);
 -
 -	/* Freeing a MR is a sleeping operation, so bounce to a work queue */
 -	INIT_WORK(&mr->odp_destroy.work, free_implicit_child_mr_work);
 -	queue_work(system_unbound_wq, &mr->odp_destroy.work);
 -}
  
 -static void destroy_unused_implicit_child_mr(struct mlx5_ib_mr *mr)
 -{
 -	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
 -	unsigned long idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
 -	struct mlx5_ib_mr *imr = mr->parent;
 -
 -	xa_lock(&imr->implicit_children);
 -	/*
 -	 * This can race with mlx5_ib_free_implicit_mr(), the first one to
 -	 * reach the xa lock wins the race and destroys the MR.
 -	 */
 -	if (__xa_cmpxchg(&imr->implicit_children, idx, mr, NULL, GFP_ATOMIC) !=
 -	    mr)
 -		goto out_unlock;
 -
 -	atomic_inc(&imr->num_deferred_work);
 -	call_srcu(&mr->dev->odp_srcu, &mr->odp_destroy.rcu,
 -		  free_implicit_child_mr_rcu);
 -
 -out_unlock:
 -	xa_unlock(&imr->implicit_children);
 +	if (atomic_dec_and_test(&imr->num_leaf_free))
 +		wake_up(&imr->q_leaf_free);
  }
  
  void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
@@@ -324,13 -308,10 +360,20 @@@
  
  	ib_umem_odp_unmap_dma_pages(umem_odp, start, end);
  
++<<<<<<< HEAD
 +
 +	if (unlikely(!umem_odp->npages && mr->parent &&
 +		     !umem_odp->dying)) {
 +		WRITE_ONCE(umem_odp->dying, 1);
 +		atomic_inc(&mr->parent->num_leaf_free);
 +		schedule_work(&umem_odp->work);
 +	}
++=======
+ 	if (unlikely(!umem_odp->npages && mr->parent))
+ 		destroy_unused_implicit_child_mr(mr);
+ out:
+ 	mutex_unlock(&umem_odp->umem_mutex);
++>>>>>>> 09689703d29a (RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy)
  }
  
  void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
@@@ -602,43 -524,86 +645,108 @@@ static int mr_leaf_free(struct ib_umem_
  
  void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
  {
 -	struct ib_umem_odp *odp_imr = to_ib_umem_odp(imr->umem);
 -	struct mlx5_ib_dev *dev = imr->dev;
 -	struct list_head destroy_list;
 -	struct mlx5_ib_mr *mtt;
 -	struct mlx5_ib_mr *tmp;
 -	unsigned long idx;
 +	struct ib_ucontext_per_mm *per_mm = mr_to_per_mm(imr);
  
 -	INIT_LIST_HEAD(&destroy_list);
 +	down_read(&per_mm->umem_rwsem);
 +	rbt_ib_umem_for_each_in_range(&per_mm->umem_tree, 0, ULLONG_MAX,
 +				      mr_leaf_free, imr);
 +	up_read(&per_mm->umem_rwsem);
 +
++<<<<<<< HEAD
 +	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
 +}
  
 +#define MLX5_PF_FLAGS_PREFETCH  BIT(0)
++=======
+ 	xa_erase(&dev->odp_mkeys, mlx5_base_mkey(imr->mmkey.key));
+ 	/*
+ 	 * This stops the SRCU protected page fault path from touching either
+ 	 * the imr or any children. The page fault path can only reach the
+ 	 * children xarray via the imr.
+ 	 */
+ 	synchronize_srcu(&dev->odp_srcu);
+ 
+ 	xa_lock(&imr->implicit_children);
+ 	xa_for_each (&imr->implicit_children, idx, mtt) {
+ 		__xa_erase(&imr->implicit_children, idx);
+ 		list_add(&mtt->odp_destroy.elm, &destroy_list);
+ 	}
+ 	xa_unlock(&imr->implicit_children);
+ 
+ 	/*
+ 	 * num_deferred_work can only be incremented inside the odp_srcu, or
+ 	 * under xa_lock while the child is in the xarray. Thus at this point
+ 	 * it is only decreasing, and all work holding it is now on the wq.
+ 	 */
+ 	if (atomic_read(&imr->num_deferred_work)) {
+ 		flush_workqueue(system_unbound_wq);
+ 		WARN_ON(atomic_read(&imr->num_deferred_work));
+ 	}
+ 
+ 	/*
+ 	 * Fence the imr before we destroy the children. This allows us to
+ 	 * skip updating the XLT of the imr during destroy of the child mkey
+ 	 * the imr points to.
+ 	 */
+ 	mlx5_mr_cache_invalidate(imr);
+ 
+ 	list_for_each_entry_safe (mtt, tmp, &destroy_list, odp_destroy.elm)
+ 		free_implicit_child_mr(mtt, false);
+ 
+ 	mlx5_mr_cache_free(dev, imr);
+ 	ib_umem_odp_release(odp_imr);
+ }
+ 
+ /**
+  * mlx5_ib_fence_odp_mr - Stop all access to the ODP MR
+  * @mr: to fence
+  *
+  * On return no parallel threads will be touching this MR and no DMA will be
+  * active.
+  */
+ void mlx5_ib_fence_odp_mr(struct mlx5_ib_mr *mr)
+ {
+ 	/* Prevent new page faults and prefetch requests from succeeding */
+ 	xa_erase(&mr->dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
+ 
+ 	/* Wait for all running page-fault handlers to finish. */
+ 	synchronize_srcu(&mr->dev->odp_srcu);
+ 
+ 	if (atomic_read(&mr->num_deferred_work)) {
+ 		flush_workqueue(system_unbound_wq);
+ 		WARN_ON(atomic_read(&mr->num_deferred_work));
+ 	}
+ 
+ 	dma_fence_odp_mr(mr);
+ }
+ 
++>>>>>>> 09689703d29a (RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy)
  #define MLX5_PF_FLAGS_DOWNGRADE BIT(1)
 -static int pagefault_real_mr(struct mlx5_ib_mr *mr, struct ib_umem_odp *odp,
 -			     u64 user_va, size_t bcnt, u32 *bytes_mapped,
 -			     u32 flags)
 +static int pagefault_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
 +			u64 io_virt, size_t bcnt, u32 *bytes_mapped,
 +			u32 flags)
  {
 -	int current_seq, page_shift, ret, np;
 +	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
  	bool downgrade = flags & MLX5_PF_FLAGS_DOWNGRADE;
 +	bool prefetch = flags & MLX5_PF_FLAGS_PREFETCH;
 +	int npages = 0, current_seq, page_shift, ret, np;
  	u64 access_mask;
  	u64 start_idx, page_mask;
 +	struct ib_umem_odp *odp;
 +	size_t size;
 +
 +	if (!odp_mr->page_list) {
 +		odp = implicit_mr_get_data(mr, io_virt, bcnt);
 +
 +		if (IS_ERR(odp))
 +			return PTR_ERR(odp);
 +		mr = odp->private;
 +	} else {
 +		odp = odp_mr;
 +	}
 +
 +next_mr:
 +	size = min_t(size_t, bcnt, ib_umem_end(odp) - io_virt);
  
  	page_shift = odp->page_shift;
  	page_mask = ~(BIT(page_shift) - 1);
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e2bebcd6069c..a4f0e5fb76fd 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1164,6 +1164,7 @@ struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
 					     struct ib_udata *udata,
 					     int access_flags);
 void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *mr);
+void mlx5_ib_fence_odp_mr(struct mlx5_ib_mr *mr);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 			  u64 length, u64 virt_addr, int access_flags,
 			  struct ib_pd *pd, struct ib_udata *udata);
@@ -1224,6 +1225,8 @@ int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 
 struct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev, int entry);
 void mlx5_mr_cache_free(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr);
+int mlx5_mr_cache_invalidate(struct mlx5_ib_mr *mr);
+
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
