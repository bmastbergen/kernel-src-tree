KVM: x86: replace is_smm checks with kvm_x86_ops.smi_allowed

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit a9fa7cb6aa997ba58294f1a07d402ce5855bafe1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/a9fa7cb6.failed

Do not hardcode is_smm so that all the architectural conditions for
blocking SMIs are listed in a single place.  Well, in two places because
this introduces some code duplication between Intel and AMD.

This ensures that nested SVM obeys GIF in kvm_vcpu_has_events.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit a9fa7cb6aa997ba58294f1a07d402ce5855bafe1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/svm/svm.c
index a95f04022d02,739414028536..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -6409,64 -3040,45 +6409,68 @@@ static int svm_smi_allowed(struct kvm_v
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	BUG_ON(!(gif_set(svm)));
 +	/* Per APM Vol.2 15.22.2 "Response to SMI" */
 +	if (!gif_set(svm))
 +		return 0;
  
 -	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 -	++vcpu->stat.irq_injections;
 +	if (is_guest_mode(&svm->vcpu) &&
 +	    svm->nested.intercept & (1ULL << INTERCEPT_SMI)) {
 +		/* TODO: Might need to set exit_info_1 and exit_info_2 here */
 +		svm->vmcb->control.exit_code = SVM_EXIT_SMI;
 +		svm->nested.exit_required = true;
 +		return 0;
 +	}
  
 -	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 -		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
++<<<<<<< HEAD
 +	return 1;
++=======
++	return !is_smm(vcpu);
++>>>>>>> a9fa7cb6aa99 (KVM: x86: replace is_smm checks with kvm_x86_ops.smi_allowed)
  }
  
 -static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 +	int ret;
  
 -	if (svm_nested_virtualize_tpr(vcpu))
 -		return;
 -
 -	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +	if (is_guest_mode(vcpu)) {
 +		/* FED8h - SVM Guest */
 +		put_smstate(u64, smstate, 0x7ed8, 1);
 +		/* FEE0h - SVM Guest VMCB Physical Address */
 +		put_smstate(u64, smstate, 0x7ee0, svm->nested.vmcb);
  
 -	if (irr == -1)
 -		return;
 +		svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 +		svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 +		svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
  
 -	if (tpr >= irr)
 -		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +		ret = nested_svm_vmexit(svm);
 +		if (ret)
 +			return ret;
 +	}
 +	return 0;
  }
  
 -static bool svm_nmi_allowed(struct kvm_vcpu *vcpu)
 +static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, const char *smstate)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb *vmcb = svm->vmcb;
 -	bool ret;
 +	struct vmcb *nested_vmcb;
 +	struct kvm_host_map map;
 +	u64 guest;
 +	u64 vmcb;
  
 -	ret = !(vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) &&
 -	      !(svm->vcpu.arch.hflags & HF_NMI_MASK);
 -	ret = ret && gif_set(svm);
 +	guest = GET_SMSTATE(u64, smstate, 0x7ed8);
 +	vmcb = GET_SMSTATE(u64, smstate, 0x7ee0);
  
 -	return ret;
 +	if (guest) {
 +		if (kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb), &map) == -EINVAL)
 +			return 1;
 +		nested_vmcb = map.hva;
 +		enter_svm_guest_mode(svm, vmcb, nested_vmcb, &map);
 +	}
 +	return 0;
  }
  
 -static bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)
 +static int enable_smi_window(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
  
diff --cc arch/x86/kvm/vmx/vmx.c
index 55aa817dc688,d0bea514d447..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -7631,8 -7679,8 +7631,13 @@@ static int vmx_smi_allowed(struct kvm_v
  {
  	/* we need a nested vmexit to enter SMM, postpone if run is pending */
  	if (to_vmx(vcpu)->nested.nested_run_pending)
++<<<<<<< HEAD
 +		return 0;
 +	return 1;
++=======
+ 		return false;
+ 	return !is_smm(vcpu);
++>>>>>>> a9fa7cb6aa99 (KVM: x86: replace is_smm checks with kvm_x86_ops.smi_allowed)
  }
  
  static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
diff --cc arch/x86/kvm/x86.c
index 2a5f5d2bc771,446fbdd05bd4..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -7685,8 -7764,7 +7685,12 @@@ static int inject_pending_event(struct 
  	if (kvm_event_needs_reinjection(vcpu))
  		return 0;
  
++<<<<<<< HEAD
 +	if (vcpu->arch.smi_pending && !is_smm(vcpu) &&
 +	    kvm_x86_ops->smi_allowed(vcpu)) {
++=======
+ 	if (vcpu->arch.smi_pending && kvm_x86_ops.smi_allowed(vcpu)) {
++>>>>>>> a9fa7cb6aa99 (KVM: x86: replace is_smm checks with kvm_x86_ops.smi_allowed)
  		vcpu->arch.smi_pending = false;
  		++vcpu->arch.smi_count;
  		enter_smm(vcpu);
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/x86.c
