io_uring: drain next sqe instead of shadowing

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 1b4a51b6d03d21f55effbcf609ba5526d87d9e9d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/1b4a51b6.failed

There's an issue with the shadow drain logic in that we drop the
completion lock after deciding to defer a request, then re-grab it later
and assume that the state is still the same. In the mean time, someone
else completing a request could have found and issued it. This can cause
a stall in the queue, by having a shadow request inserted that nobody is
going to drain.

Additionally, if we fail allocating the shadow request, we simply ignore
the drain.

Instead of using a shadow request, defer the next request/link instead.
This also has the following advantages:

- removes semi-duplicated code
- doesn't allocate memory for shadows
- works better if only the head marked for drain
- doesn't need complex synchronisation

On the flip side, it removes the shadow->seq ==
last_drain_in_in_link->seq optimization. That shouldn't be a common
case, and can always be added back, if needed.

Fixes: 4fe2c963154c ("io_uring: add support for link with drain")
	Cc: Jackie Liu <liuyun01@kylinos.cn>
	Reported-by: Jens Axboe <axboe@kernel.dk>
	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 1b4a51b6d03d21f55effbcf609ba5526d87d9e9d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 66de1d702552,ca980c5878e9..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -215,9 -185,21 +215,14 @@@ struct io_ring_ctx 
  		unsigned int		flags;
  		bool			compat;
  		bool			account_mem;
++<<<<<<< HEAD
++=======
+ 		bool			cq_overflow_flushed;
+ 		bool			drain_next;
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  
 -		/*
 -		 * Ring buffer of indices into array of io_uring_sqe, which is
 -		 * mmapped by the application using the IORING_OFF_SQES offset.
 -		 *
 -		 * This indirection could e.g. be used to assign fixed
 -		 * io_uring_sqe entries to operations and only submit them to
 -		 * the queue when needed.
 -		 *
 -		 * The kernel modifies neither the indices array nor the entries
 -		 * array.
 -		 */
 -		u32			*sq_array;
 +		/* SQ ring */
 +		struct io_sq_ring	*sq_ring;
  		unsigned		cached_sq_head;
  		unsigned		sq_entries;
  		unsigned		sq_mask;
@@@ -333,9 -344,17 +338,20 @@@ struct io_kiocb 
  #define REQ_F_IO_DRAIN		16	/* drain existing IO first */
  #define REQ_F_IO_DRAINED	32	/* drain done */
  #define REQ_F_LINK		64	/* linked sqes */
 -#define REQ_F_LINK_TIMEOUT	128	/* has linked timeout */
 +#define REQ_F_LINK_DONE		128	/* linked sqes done */
  #define REQ_F_FAIL_LINK		256	/* fail rest of links */
++<<<<<<< HEAD
 +#define REQ_F_SHADOW_DRAIN	512	/* link-drain shadow req */
++=======
+ #define REQ_F_DRAIN_LINK	512	/* link should be fully drained */
+ #define REQ_F_TIMEOUT		1024	/* timeout request */
+ #define REQ_F_ISREG		2048	/* regular file */
+ #define REQ_F_MUST_PUNT		4096	/* must be punted even for NONBLOCK */
+ #define REQ_F_TIMEOUT_NOSEQ	8192	/* no timeout sequence */
+ #define REQ_F_INFLIGHT		16384	/* on inflight list */
+ #define REQ_F_COMP_LOCKED	32768	/* completion under lock */
+ #define REQ_F_FREE_SQE		65536	/* free sqe if not async queued */
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  	u64			user_data;
  	u32			result;
  	u32			sequence;
@@@ -490,13 -618,11 +506,8 @@@ static void io_commit_cqring(struct io_
  	__io_commit_cqring(ctx);
  
  	while ((req = io_get_deferred_req(ctx)) != NULL) {
- 		if (req->flags & REQ_F_SHADOW_DRAIN) {
- 			/* Just for drain, free it. */
- 			__io_free_req(req);
- 			continue;
- 		}
  		req->flags |= REQ_F_IO_DRAINED;
 -		io_queue_async_work(req);
 +		io_queue_async_work(ctx, req);
  	}
  }
  
@@@ -2212,71 -2969,45 +2223,92 @@@ static int io_queue_sqe(struct io_ring_
  {
  	int ret;
  
++<<<<<<< HEAD
 +	ret = io_req_defer(ctx, req, s->sqe);
++=======
+ 	if (unlikely(req->ctx->drain_next)) {
+ 		req->flags |= REQ_F_IO_DRAIN;
+ 		req->ctx->drain_next = false;
+ 	}
+ 	req->ctx->drain_next = (req->flags & REQ_F_DRAIN_LINK);
+ 
+ 	ret = io_req_defer(req);
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -			io_cqring_add_event(req, ret);
 -			if (req->flags & REQ_F_LINK)
 -				req->flags |= REQ_F_FAIL_LINK;
 -			io_double_put_req(req);
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
  		}
 -	} else
 -		__io_queue_sqe(req);
 +		return 0;
 +	}
 +
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
  }
  
++<<<<<<< HEAD
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
 +{
 +	int ret;
 +	int need_submit = false;
 +
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	/*
 +	 * Mark the first IO in link list as DRAIN, let all the following
 +	 * IOs enter the defer list. all IO needs to be completed before link
 +	 * list.
 +	 */
 +	req->flags |= REQ_F_IO_DRAIN;
 +	ret = io_req_defer(ctx, req, s->sqe);
 +	if (ret) {
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
 +		}
 +	} else {
 +		/*
 +		 * If ret == 0 means that all IOs in front of link io are
 +		 * running done. let's queue link head.
 +		 */
 +		need_submit = true;
 +	}
 +
 +	/* Insert shadow req to defer_list, blocking next IOs */
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_add_tail(&shadow->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	if (need_submit)
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	return 0;
++=======
+ static inline void io_queue_link_head(struct io_kiocb *req)
+ {
+ 	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
+ 		io_cqring_add_event(req, -ECANCELED);
+ 		io_double_put_req(req);
+ 	} else
+ 		io_queue_sqe(req);
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  }
  
+ 
  #define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
  
 -static void io_submit_sqe(struct io_kiocb *req, struct io_submit_state *state,
 -			  struct io_kiocb **link)
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
  {
 -	struct sqe_submit *s = &req->submit;
 -	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
  	int ret;
  
 -	req->user_data = s->sqe->user_data;
 -
  	/* enforce forwards compatibility on users */
  	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
  		ret = -EINVAL;
@@@ -2309,6 -3031,21 +2341,24 @@@ err
  	 */
  	if (*link) {
  		struct io_kiocb *prev = *link;
++<<<<<<< HEAD
++=======
+ 		struct io_uring_sqe *sqe_copy;
+ 
+ 		if (s->sqe->flags & IOSQE_IO_DRAIN)
+ 			(*link)->flags |= REQ_F_DRAIN_LINK | REQ_F_IO_DRAIN;
+ 
+ 		if (READ_ONCE(s->sqe->opcode) == IORING_OP_LINK_TIMEOUT) {
+ 			ret = io_timeout_setup(req);
+ 			/* common setup allows offset being set, we don't */
+ 			if (!ret && s->sqe->off)
+ 				ret = -EINVAL;
+ 			if (ret) {
+ 				prev->flags |= REQ_F_FAIL_LINK;
+ 				goto err_req;
+ 			}
+ 		}
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
  		if (!sqe_copy) {
@@@ -2413,9 -3154,13 +2463,12 @@@ static int io_submit_sqes(struct io_rin
  {
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
++<<<<<<< HEAD
 +	struct io_kiocb *shadow_req = NULL;
 +	bool prev_was_link = false;
++=======
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  	int i, submitted = 0;
 -	bool mm_fault = false;
 -
 -	if (!list_empty(&ctx->cq_overflow_list)) {
 -		io_cqring_overflow_flush(ctx, false);
 -		return -EBUSY;
 -	}
  
  	if (nr > IO_PLUG_THRESHOLD) {
  		io_submit_state_start(&state, ctx, nr);
@@@ -2423,44 -3168,52 +2476,78 @@@
  	}
  
  	for (i = 0; i < nr; i++) {
 -		struct io_kiocb *req;
 -		unsigned int sqe_flags;
 -
 -		req = io_get_req(ctx, statep);
 -		if (unlikely(!req)) {
 -			if (!submitted)
 -				submitted = -EAGAIN;
 -			break;
 +		/*
 +		 * If previous wasn't linked and we have a linked command,
 +		 * that's the end of the chain. Submit the previous link.
 +		 */
 +		if (!prev_was_link && link) {
 +			io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +						true);
 +			link = NULL;
 +			shadow_req = NULL;
  		}
 -		if (!io_get_sqring(ctx, &req->submit)) {
 -			__io_free_req(req);
 -			break;
 +		prev_was_link = (sqes[i].sqe->flags & IOSQE_IO_LINK) != 0;
 +
++<<<<<<< HEAD
 +		if (link && (sqes[i].sqe->flags & IOSQE_IO_DRAIN)) {
 +			if (!shadow_req) {
 +				shadow_req = io_get_req(ctx, NULL);
 +				if (unlikely(!shadow_req))
 +					goto out;
 +				shadow_req->flags |= (REQ_F_IO_DRAIN | REQ_F_SHADOW_DRAIN);
 +				refcount_dec(&shadow_req->refs);
 +			}
 +			shadow_req->sequence = sqes[i].sequence;
  		}
  
 +out:
 +		if (unlikely(mm_fault)) {
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
 +						-EFAULT);
 +		} else {
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
 +			submitted++;
++=======
+ 		if (io_sqe_needs_user(req->submit.sqe) && !*mm) {
+ 			mm_fault = mm_fault || !mmget_not_zero(ctx->sqo_mm);
+ 			if (!mm_fault) {
+ 				use_mm(ctx->sqo_mm);
+ 				*mm = ctx->sqo_mm;
+ 			}
+ 		}
+ 
+ 		sqe_flags = req->submit.sqe->flags;
+ 
+ 		req->submit.ring_file = ring_file;
+ 		req->submit.ring_fd = ring_fd;
+ 		req->submit.has_user = *mm != NULL;
+ 		req->submit.in_async = async;
+ 		req->submit.needs_fixed_file = async;
+ 		trace_io_uring_submit_sqe(ctx, req->submit.sqe->user_data,
+ 					  true, async);
+ 		io_submit_sqe(req, statep, &link);
+ 		submitted++;
+ 
+ 		/*
+ 		 * If previous wasn't linked and we have a linked command,
+ 		 * that's the end of the chain. Submit the previous link.
+ 		 */
+ 		if (!(sqe_flags & IOSQE_IO_LINK) && link) {
+ 			io_queue_link_head(link);
+ 			link = NULL;
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  		}
  	}
  
  	if (link)
++<<<<<<< HEAD
 +		io_queue_link_head(ctx, link, &link->submit, shadow_req, true);
++=======
+ 		io_queue_link_head(link);
++>>>>>>> 1b4a51b6d03d (io_uring: drain next sqe instead of shadowing)
  	if (statep)
  		io_submit_state_end(&state);
  
* Unmerged path fs/io_uring.c
