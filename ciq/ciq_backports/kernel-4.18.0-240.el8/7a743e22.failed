io_uring: get next work with submission ref drop

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 7a743e225b2a9da772b28a50031e1ccd8a8ce404
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/7a743e22.failed

If after dropping the submission reference req->refs == 1, the request
is done, because this one is for io_put_work() and will be dropped
synchronously shortly after. In this case it's safe to steal a next
work from the request.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 7a743e225b2a9da772b28a50031e1ccd8a8ce404)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,40ca9e6a5ace..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -677,14 -1487,69 +677,54 @@@ static void io_free_req(struct io_kioc
  	 * dependencies to the next request. In case of failure, fail the rest
  	 * of the chain.
  	 */
 -	if (req->flags & REQ_F_FAIL_LINK) {
 -		io_fail_links(req);
 -	} else if ((req->flags & (REQ_F_LINK_TIMEOUT | REQ_F_COMP_LOCKED)) ==
 -			REQ_F_LINK_TIMEOUT) {
 -		struct io_ring_ctx *ctx = req->ctx;
 -		unsigned long flags;
 -
 -		/*
 -		 * If this is a timeout link, we could be racing with the
 -		 * timeout timer. Grab the completion lock for this case to
 -		 * protect against that.
 -		 */
 -		spin_lock_irqsave(&ctx->completion_lock, flags);
 -		io_req_link_next(req, nxt);
 -		spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -	} else {
 -		io_req_link_next(req, nxt);
 +	if (req->flags & REQ_F_LINK) {
 +		if (req->flags & REQ_F_FAIL_LINK)
 +			io_fail_links(req);
 +		else
 +			io_req_link_next(req);
  	}
 -}
 -
 -static void io_free_req(struct io_kiocb *req)
 -{
 -	struct io_kiocb *nxt = NULL;
  
 -	io_req_find_next(req, &nxt);
  	__io_free_req(req);
++<<<<<<< HEAD
++=======
+ 
+ 	if (nxt)
+ 		io_queue_async_work(nxt);
+ }
+ 
+ static void io_link_work_cb(struct io_wq_work **workptr)
+ {
+ 	struct io_wq_work *work = *workptr;
+ 	struct io_kiocb *link = work->data;
+ 
+ 	io_queue_linked_timeout(link);
+ 	io_wq_submit_work(workptr);
+ }
+ 
+ static void io_wq_assign_next(struct io_wq_work **workptr, struct io_kiocb *nxt)
+ {
+ 	struct io_kiocb *link;
+ 
+ 	*workptr = &nxt->work;
+ 	link = io_prep_linked_timeout(nxt);
+ 	if (link) {
+ 		nxt->work.func = io_link_work_cb;
+ 		nxt->work.data = link;
+ 	}
+ }
+ 
+ /*
+  * Drop reference to request, return next in chain (if there is one) if this
+  * was the last reference to this request.
+  */
+ __attribute__((nonnull))
+ static void io_put_req_find_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	if (refcount_dec_and_test(&req->refs)) {
+ 		io_req_find_next(req, nxtptr);
+ 		__io_free_req(req);
+ 	}
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  }
  
  static void io_put_req(struct io_kiocb *req)
@@@ -693,11 -1558,86 +733,40 @@@
  		io_free_req(req);
  }
  
++<<<<<<< HEAD
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
++=======
+ static void io_put_req_async_completion(struct io_kiocb *req,
+ 					struct io_wq_work **workptr)
+ {
+ 	/*
+ 	 * It's in an io-wq worker, so there always should be at least
+ 	 * one reference, which will be dropped in io_put_work() just
+ 	 * after the current handler returns.
+ 	 *
+ 	 * It also means, that if the counter dropped to 1, then there is
+ 	 * no asynchronous users left, so it's safe to steal the next work.
+ 	 */
+ 	refcount_dec(&req->refs);
+ 	if (refcount_read(&req->refs) == 1) {
+ 		struct io_kiocb *nxt = NULL;
+ 
+ 		io_req_find_next(req, &nxt);
+ 		if (nxt)
+ 			io_wq_assign_next(workptr, nxt);
+ 	}
+ }
+ 
+ /*
+  * Must only be used if we don't need to care about links, usually from
+  * within the completion handling itself.
+  */
+ static void __io_double_put_req(struct io_kiocb *req)
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  {
 -	/* drop both submit and complete references */
 -	if (refcount_sub_and_test(2, &req->refs))
 -		__io_free_req(req);
 -}
 -
 -static void io_double_put_req(struct io_kiocb *req)
 -{
 -	/* drop both submit and complete references */
 -	if (refcount_sub_and_test(2, &req->refs))
 -		io_free_req(req);
 -}
 -
 -static unsigned io_cqring_events(struct io_ring_ctx *ctx, bool noflush)
 -{
 -	struct io_rings *rings = ctx->rings;
 -
 -	if (test_bit(0, &ctx->cq_check_overflow)) {
 -		/*
 -		 * noflush == true is from the waitqueue handler, just ensure
 -		 * we wake up the task, and the next invocation will flush the
 -		 * entries. We cannot safely to it from here.
 -		 */
 -		if (noflush && !list_empty(&ctx->cq_overflow_list))
 -			return -1U;
 -
 -		io_cqring_overflow_flush(ctx, false);
 -	}
 -
  	/* See comment at the top of this file */
  	smp_rmb();
 -	return ctx->cached_cq_tail - READ_ONCE(rings->cq.head);
 -}
 -
 -static inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)
 -{
 -	struct io_rings *rings = ctx->rings;
 -
 -	/* make sure SQ entry isn't read before tail */
 -	return smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;
 -}
 -
 -static inline bool io_req_multi_free(struct req_batch *rb, struct io_kiocb *req)
 -{
 -	if ((req->flags & REQ_F_LINK) || io_is_fallback_req(req))
 -		return false;
 -
 -	if (!(req->flags & REQ_F_FIXED_FILE) || req->io)
 -		rb->need_iter++;
 -
 -	rb->reqs[rb->to_free++] = req;
 -	if (unlikely(rb->to_free == ARRAY_SIZE(rb->reqs)))
 -		io_free_req_many(req->ctx, rb);
 -	return true;
 +	return READ_ONCE(ring->r.tail) - READ_ONCE(ring->r.head);
  }
  
  /*
@@@ -1487,24 -2567,180 +1556,185 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
 -static bool io_req_cancelled(struct io_kiocb *req)
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		    bool force_nonblock)
  {
++<<<<<<< HEAD
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
++=======
+ 	if (req->work.flags & IO_WQ_WORK_CANCEL) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, -ECANCELED);
+ 		io_double_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void __io_fsync(struct io_kiocb *req)
+ {
+ 	loff_t end = req->sync.off + req->sync.len;
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  	int ret;
  
 -	ret = vfs_fsync_range(req->file, req->sync.off,
 -				end > 0 ? end : LLONG_MAX,
 -				req->sync.flags & IORING_FSYNC_DATASYNC);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -}
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
 +
++<<<<<<< HEAD
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
  
 +	/* fsync always requires a blocking context */
++=======
+ static void io_fsync_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fsync(req);
+ 	io_put_req_async_completion(req, workptr);
+ }
+ 
+ static int io_fsync(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	/* fsync always requires a blocking context */
+ 	if (force_nonblock) {
+ 		req->work.func = io_fsync_finish;
+ 		return -EAGAIN;
+ 	}
+ 	__io_fsync(req);
+ 	return 0;
+ }
+ 
+ static void __io_fallocate(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
+ 				req->sync.len);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ }
+ 
+ static void io_fallocate_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fallocate(req);
+ 	io_put_req_async_completion(req, workptr);
+ }
+ 
+ static int io_fallocate_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	req->sync.off = READ_ONCE(sqe->off);
+ 	req->sync.len = READ_ONCE(sqe->addr);
+ 	req->sync.mode = READ_ONCE(sqe->len);
+ 	return 0;
+ }
+ 
+ static int io_fallocate(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	/* fallocate always requiring blocking context */
+ 	if (force_nonblock) {
+ 		req->work.func = io_fallocate_finish;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	__io_fallocate(req);
+ 	return 0;
+ }
+ 
+ static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.how.mode = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.how.flags = READ_ONCE(sqe->open_flags);
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct open_how __user *how;
+ 	const char __user *fname;
+ 	size_t len;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	len = READ_ONCE(sqe->len);
+ 
+ 	if (len < OPEN_HOW_SIZE_VER0)
+ 		return -EINVAL;
+ 
+ 	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
+ 					len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
+ 		req->open.how.flags |= O_LARGEFILE;
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_openat2(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct open_flags op;
+ 	struct file *file;
+ 	int ret;
+ 
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  	if (force_nonblock)
  		return -EAGAIN;
  
@@@ -1519,6 -2770,277 +1749,280 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int io_openat(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
+ 	return io_openat2(req, force_nonblock);
+ }
+ 
+ static int io_epoll_ctl_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	req->epoll.epfd = READ_ONCE(sqe->fd);
+ 	req->epoll.op = READ_ONCE(sqe->len);
+ 	req->epoll.fd = READ_ONCE(sqe->off);
+ 
+ 	if (ep_op_has_event(req->epoll.op)) {
+ 		struct epoll_event __user *ev;
+ 
+ 		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
+ 			return -EFAULT;
+ 	}
+ 
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_epoll_ctl(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	struct io_epoll *ie = &req->epoll;
+ 	int ret;
+ 
+ 	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
+ 	if (force_nonblock && ret == -EAGAIN)
+ 		return -EAGAIN;
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	if (sqe->ioprio || sqe->buf_index || sqe->off)
+ 		return -EINVAL;
+ 
+ 	req->madvise.addr = READ_ONCE(sqe->addr);
+ 	req->madvise.len = READ_ONCE(sqe->len);
+ 	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	struct io_madvise *ma = &req->madvise;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	ret = do_madvise(ma->addr, ma->len, ma->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->addr)
+ 		return -EINVAL;
+ 
+ 	req->fadvise.offset = READ_ONCE(sqe->off);
+ 	req->fadvise.len = READ_ONCE(sqe->len);
+ 	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ }
+ 
+ static int io_fadvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_fadvise *fa = &req->fadvise;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		switch (fa->advice) {
+ 		case POSIX_FADV_NORMAL:
+ 		case POSIX_FADV_RANDOM:
+ 		case POSIX_FADV_SEQUENTIAL:
+ 			break;
+ 		default:
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	unsigned lookup_flags;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.mask = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	req->open.how.flags = READ_ONCE(sqe->statx_flags);
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
+ 		return -EINVAL;
+ 
+ 	req->open.filename = getname_flags(fname, lookup_flags, NULL);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_statx(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_open *ctx = &req->open;
+ 	unsigned lookup_flags;
+ 	struct path path;
+ 	struct kstat stat;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
+ 		return -EINVAL;
+ 
+ retry:
+ 	/* filename_lookup() drops it, keep a reference */
+ 	ctx->filename->refcnt++;
+ 
+ 	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
+ 				NULL);
+ 	if (ret)
+ 		goto err;
+ 
+ 	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
+ 	path_put(&path);
+ 	if (retry_estale(ret, lookup_flags)) {
+ 		lookup_flags |= LOOKUP_REVAL;
+ 		goto retry;
+ 	}
+ 	if (!ret)
+ 		ret = cp_statx(&stat, ctx->buffer);
+ err:
+ 	putname(ctx->filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * If we queue this for async, it must not be cancellable. That would
+ 	 * leave the 'file' in an undeterminate state.
+ 	 */
+ 	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
+ 
+ 	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
+ 	    sqe->rw_flags || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->close.fd = READ_ONCE(sqe->fd);
+ 	if (req->file->f_op == &io_uring_fops ||
+ 	    req->close.fd == req->ctx->ring_fd)
+ 		return -EBADF;
+ 
+ 	return 0;
+ }
+ 
+ /* only called when __close_fd_get_file() is done */
+ static void __io_close_finish(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = filp_close(req->close.put_file, req->work.files);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	fput(req->close.put_file);
+ 	io_put_req(req);
+ }
+ 
+ static void io_close_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 
+ 	/* not cancellable, don't do io_req_cancelled() */
+ 	__io_close_finish(req);
+ 	io_put_req_async_completion(req, workptr);
+ }
+ 
+ static int io_close(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	int ret;
+ 
+ 	req->close.put_file = NULL;
+ 	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* if the file has a flush method, be safe and punt to async */
+ 	if (req->close.put_file->f_op->flush && force_nonblock) {
+ 		/* submission ref will be dropped, take it for async */
+ 		refcount_inc(&req->refs);
+ 
+ 		req->work.func = io_close_finish;
+ 		/*
+ 		 * Do manual async queue here to avoid grabbing files - we don't
+ 		 * need the files, and it'll cause io_close_finish() to close
+ 		 * the file again and cause a double CQE entry for this request
+ 		 */
+ 		io_queue_async_work(req);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * No ->flush(), safely close from here and just punt the
+ 	 * fput() to async context.
+ 	 */
+ 	__io_close_finish(req);
+ 	return 0;
+ }
+ 
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
  	struct io_ring_ctx *ctx = req->ctx;
@@@ -1663,93 -3361,184 +2167,114 @@@ static int io_poll_remove(struct io_kio
  
  	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
  		return -EINVAL;
 +	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 +	    sqe->poll_events)
 +		return -EINVAL;
  
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct io_sr_msg *sr = &req->sr_msg;
 -		struct msghdr msg;
 -		struct iovec iov;
 -		unsigned flags;
 -
 -		ret = import_single_range(READ, sr->buf, sr->len, &iov,
 -						&msg.msg_iter);
 -		if (ret)
 -			return ret;
 -
 -		msg.msg_name = NULL;
 -		msg.msg_control = NULL;
 -		msg.msg_controllen = 0;
 -		msg.msg_namelen = 0;
 -		msg.msg_iocb = NULL;
 -		msg.msg_flags = 0;
 -
 -		flags = req->sr_msg.msg_flags;
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 -
 -		ret = sock_recvmsg(sock, &msg, flags);
 -		if (force_nonblock && ret == -EAGAIN)
 -			return -EAGAIN;
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_for_each_entry_safe(poll_req, next, &ctx->cancel_list, list) {
 +		if (READ_ONCE(sqe->addr) == poll_req->user_data) {
 +			io_poll_remove_one(poll_req);
 +			ret = 0;
 +			break;
 +		}
  	}
 +	spin_unlock_irq(&ctx->completion_lock);
  
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
  	io_put_req(req);
  	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
  }
  
 -
 -static int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static void io_poll_complete(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			     __poll_t mask)
  {
 -#if defined(CONFIG_NET)
 -	struct io_accept *accept = &req->accept;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -
 -	accept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	accept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	accept->flags = READ_ONCE(sqe->accept_flags);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
++<<<<<<< HEAD
 +	req->poll.done = true;
 +	io_cqring_fill_event(ctx, req->user_data, mangle_poll(mask));
 +	io_commit_cqring(ctx);
  }
++=======
++	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  
 -#if defined(CONFIG_NET)
 -static int __io_accept(struct io_kiocb *req, bool force_nonblock)
 +static void io_poll_complete_work(struct work_struct *work)
  {
 -	struct io_accept *accept = &req->accept;
 -	unsigned file_flags;
 -	int ret;
 -
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -	ret = __sys_accept4_file(req->file, file_flags, accept->addr,
 -					accept->addr_len, accept->flags);
 -	if (ret == -EAGAIN && force_nonblock)
 -		return -EAGAIN;
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 +	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 +	struct io_poll_iocb *poll = &req->poll;
 +	struct poll_table_struct pt = { ._key = poll->events };
 +	struct io_ring_ctx *ctx = req->ctx;
 +	__poll_t mask = 0;
  
 -static void io_accept_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 +	if (!READ_ONCE(poll->canceled))
 +		mask = vfs_poll(poll->file, &pt) & poll->events;
  
 -	if (io_req_cancelled(req))
 +	/*
 +	 * Note that ->ki_cancel callers also delete iocb from active_reqs after
 +	 * calling ->ki_cancel.  We need the ctx_lock roundtrip here to
 +	 * synchronize with them.  In the cancellation case the list_del_init
 +	 * itself is not actually needed, but harmless so we keep it in to
 +	 * avoid further branches in the fast path.
 +	 */
 +	spin_lock_irq(&ctx->completion_lock);
 +	if (!mask && !READ_ONCE(poll->canceled)) {
 +		add_wait_queue(poll->head, &poll->wait);
 +		spin_unlock_irq(&ctx->completion_lock);
  		return;
++<<<<<<< HEAD
++=======
+ 	__io_accept(req, false);
+ 	io_put_req_async_completion(req, workptr);
+ }
+ #endif
+ 
+ static int io_accept(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	int ret;
+ 
+ 	ret = __io_accept(req, force_nonblock);
+ 	if (ret == -EAGAIN && force_nonblock) {
+ 		req->work.func = io_accept_finish;
+ 		return -EAGAIN;
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  	}
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 +	list_del_init(&req->list);
 +	io_poll_complete(ctx, req, mask);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	io_cqring_ev_posted(ctx);
 +	io_put_req(req);
  }
  
 -static int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 +			void *key)
  {
 -#if defined(CONFIG_NET)
 -	struct io_connect *conn = &req->connect;
 -	struct io_async_ctx *io = req->io;
 -
 -	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)
 -		return -EINVAL;
 -
 -	conn->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	conn->addr_len =  READ_ONCE(sqe->addr2);
 +	struct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,
 +							wait);
 +	struct io_kiocb *req = container_of(poll, struct io_kiocb, poll);
 +	struct io_ring_ctx *ctx = req->ctx;
 +	__poll_t mask = key_to_poll(key);
 +	unsigned long flags;
  
 -	if (!io)
 +	/* for instances that support it check for an event match first: */
 +	if (mask && !(mask & poll->events))
  		return 0;
  
 -	return move_addr_to_kernel(conn->addr, conn->addr_len,
 -					&io->connect.address);
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 +	list_del_init(&poll->wait.entry);
  
 -static int io_connect(struct io_kiocb *req, bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_ctx __io, *io;
 -	unsigned file_flags;
 -	int ret;
 +	if (mask && spin_trylock_irqsave(&ctx->completion_lock, flags)) {
 +		list_del(&req->list);
 +		io_poll_complete(ctx, req, mask);
 +		spin_unlock_irqrestore(&ctx->completion_lock, flags);
  
 -	if (req->io) {
 -		io = req->io;
 +		io_cqring_ev_posted(ctx);
 +		io_put_req(req);
  	} else {
 -		ret = move_addr_to_kernel(req->connect.addr,
 -						req->connect.addr_len,
 -						&__io.connect.address);
 -		if (ret)
 -			goto out;
 -		io = &__io;
 +		io_queue_async_work(ctx, req);
  	}
  
 -	file_flags = force_nonblock ? O_NONBLOCK : 0;
 -
 -	ret = __sys_connect_file(req->file, &io->connect.address,
 -					req->connect.addr_len, file_flags);
 -	if ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {
 -		if (req->io)
 -			return -EAGAIN;
 -		if (io_alloc_async_ctx(req)) {
 -			ret = -ENOMEM;
 -			goto out;
 -		}
 -		memcpy(&req->io->connect, &__io.connect, sizeof(__io.connect));
 -		return -EAGAIN;
 -	}
 -	if (ret == -ERESTARTSYS)
 -		ret = -EINTR;
 -out:
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 +	return 1;
  }
  
  struct io_poll_table {
@@@ -1938,267 -4687,272 +2463,322 @@@ static int __io_submit_sqe(struct io_ri
  	return 0;
  }
  
 -static void io_wq_submit_work(struct io_wq_work **workptr)
 +static struct async_list *io_async_list_from_sqe(struct io_ring_ctx *ctx,
 +						 const struct io_uring_sqe *sqe)
 +{
 +	switch (sqe->opcode) {
 +	case IORING_OP_READV:
 +	case IORING_OP_READ_FIXED:
 +		return &ctx->pending_async[READ];
 +	case IORING_OP_WRITEV:
 +	case IORING_OP_WRITE_FIXED:
 +		return &ctx->pending_async[WRITE];
 +	default:
 +		return NULL;
 +	}
 +}
 +
 +static inline bool io_sqe_needs_user(const struct io_uring_sqe *sqe)
 +{
 +	u8 opcode = READ_ONCE(sqe->opcode);
 +
 +	return !(opcode == IORING_OP_READ_FIXED ||
 +		 opcode == IORING_OP_WRITE_FIXED);
 +}
 +
 +static void io_sq_wq_submit_work(struct work_struct *work)
  {
 -	struct io_wq_work *work = *workptr;
  	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
++<<<<<<< HEAD
++=======
+ 	int ret = 0;
+ 
+ 	/* if NO_CANCEL is set, we must still run the work */
+ 	if ((work->flags & (IO_WQ_WORK_CANCEL|IO_WQ_WORK_NO_CANCEL)) ==
+ 				IO_WQ_WORK_CANCEL) {
+ 		ret = -ECANCELED;
+ 	}
+ 
+ 	if (!ret) {
+ 		do {
+ 			ret = io_issue_sqe(req, NULL, false);
+ 			/*
+ 			 * We can get EAGAIN for polled IO even though we're
+ 			 * forcing a sync submission from here, since we can't
+ 			 * wait for request slots on the block side.
+ 			 */
+ 			if (ret != -EAGAIN)
+ 				break;
+ 			cond_resched();
+ 		} while (1);
+ 	}
+ 
+ 	if (ret) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, ret);
+ 		io_put_req(req);
+ 	}
+ 
+ 	io_put_req_async_completion(req, workptr);
+ }
+ 
+ static int io_req_needs_file(struct io_kiocb *req, int fd)
+ {
+ 	if (!io_op_defs[req->opcode].needs_file)
+ 		return 0;
+ 	if ((fd == -1 || fd == AT_FDCWD) && io_op_defs[req->opcode].fd_non_neg)
+ 		return 0;
+ 	return 1;
+ }
+ 
+ static inline struct file *io_file_from_index(struct io_ring_ctx *ctx,
+ 					      int index)
+ {
+ 	struct fixed_file_table *table;
+ 
+ 	table = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];
+ 	return table->files[index & IORING_FILE_TABLE_MASK];;
+ }
+ 
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 			int fd, struct file **out_file, bool fixed)
+ {
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  	struct io_ring_ctx *ctx = req->ctx;
 -	struct file *file;
 +	struct mm_struct *cur_mm = NULL;
 +	struct async_list *async_list;
 +	LIST_HEAD(req_list);
 +	mm_segment_t old_fs;
 +	int ret;
  
 -	if (fixed) {
 -		if (unlikely(!ctx->file_data ||
 -		    (unsigned) fd >= ctx->nr_user_files))
 -			return -EBADF;
 -		fd = array_index_nospec(fd, ctx->nr_user_files);
 -		file = io_file_from_index(ctx, fd);
 -		if (!file)
 -			return -EBADF;
 -		percpu_ref_get(&ctx->file_data->refs);
 -	} else {
 -		trace_io_uring_file_get(ctx, fd);
 -		file = __io_file_get(state, fd);
 -		if (unlikely(!file))
 -			return -EBADF;
 -	}
 +	async_list = io_async_list_from_sqe(ctx, req->submit.sqe);
 +restart:
 +	do {
 +		struct sqe_submit *s = &req->submit;
 +		const struct io_uring_sqe *sqe = s->sqe;
 +		unsigned int flags = req->flags;
  
 -	*out_file = file;
 -	return 0;
 -}
 +		/* Ensure we clear previously set non-block flag */
 +		req->rw.ki_flags &= ~IOCB_NOWAIT;
  
 -static int io_req_set_file(struct io_submit_state *state, struct io_kiocb *req,
 -			   const struct io_uring_sqe *sqe)
 -{
 -	unsigned flags;
 -	int fd;
 -	bool fixed;
 +		ret = 0;
 +		if (io_sqe_needs_user(sqe) && !cur_mm) {
 +			if (!mmget_not_zero(ctx->sqo_mm)) {
 +				ret = -EFAULT;
 +			} else {
 +				cur_mm = ctx->sqo_mm;
 +				use_mm(cur_mm);
 +				old_fs = get_fs();
 +				set_fs(USER_DS);
 +			}
 +		}
 +
 +		if (!ret) {
 +			s->has_user = cur_mm != NULL;
 +			s->needs_lock = true;
 +			do {
 +				ret = __io_submit_sqe(ctx, req, s, false);
 +				/*
 +				 * We can get EAGAIN for polled IO even though
 +				 * we're forcing a sync submission from here,
 +				 * since we can't wait for request slots on the
 +				 * block side.
 +				 */
 +				if (ret != -EAGAIN)
 +					break;
 +				cond_resched();
 +			} while (1);
 +		}
  
 -	flags = READ_ONCE(sqe->flags);
 -	fd = READ_ONCE(sqe->fd);
 +		/* drop submission reference */
 +		io_put_req(req);
  
 -	if (!io_req_needs_file(req, fd))
 -		return 0;
 +		if (ret) {
 +			io_cqring_add_event(ctx, sqe->user_data, ret);
 +			io_put_req(req);
 +		}
  
 -	fixed = (flags & IOSQE_FIXED_FILE);
 -	if (unlikely(!fixed && req->needs_fixed_file))
 -		return -EBADF;
 +		/* async context always use a copy of the sqe */
 +		kfree(sqe);
  
 -	return io_file_get(state, req, fd, &req->file, fixed);
 -}
 +		/* req from defer and link list needn't decrease async cnt */
 +		if (flags & (REQ_F_IO_DRAINED | REQ_F_LINK_DONE))
 +			goto out;
  
 -static int io_grab_files(struct io_kiocb *req)
 -{
 -	int ret = -EBADF;
 -	struct io_ring_ctx *ctx = req->ctx;
 +		if (!async_list)
 +			break;
 +		if (!list_empty(&req_list)) {
 +			req = list_first_entry(&req_list, struct io_kiocb,
 +						list);
 +			list_del(&req->list);
 +			continue;
 +		}
 +		if (list_empty(&async_list->list))
 +			break;
  
 -	if (req->work.files)
 -		return 0;
 -	if (!ctx->ring_file)
 -		return -EBADF;
 +		req = NULL;
 +		spin_lock(&async_list->lock);
 +		if (list_empty(&async_list->list)) {
 +			spin_unlock(&async_list->lock);
 +			break;
 +		}
 +		list_splice_init(&async_list->list, &req_list);
 +		spin_unlock(&async_list->lock);
 +
 +		req = list_first_entry(&req_list, struct io_kiocb, list);
 +		list_del(&req->list);
 +	} while (req);
  
 -	rcu_read_lock();
 -	spin_lock_irq(&ctx->inflight_lock);
  	/*
 -	 * We use the f_ops->flush() handler to ensure that we can flush
 -	 * out work accessing these files if the fd is closed. Check if
 -	 * the fd has changed since we started down this path, and disallow
 -	 * this operation if it has.
 +	 * Rare case of racing with a submitter. If we find the count has
 +	 * dropped to zero AND we have pending work items, then restart
 +	 * the processing. This is a tiny race window.
  	 */
 -	if (fcheck(ctx->ring_fd) == ctx->ring_file) {
 -		list_add(&req->inflight_entry, &ctx->inflight_list);
 -		req->flags |= REQ_F_INFLIGHT;
 -		req->work.files = current->files;
 -		ret = 0;
 +	if (async_list) {
 +		ret = atomic_dec_return(&async_list->cnt);
 +		while (!ret && !list_empty(&async_list->list)) {
 +			spin_lock(&async_list->lock);
 +			atomic_inc(&async_list->cnt);
 +			list_splice_init(&async_list->list, &req_list);
 +			spin_unlock(&async_list->lock);
 +
 +			if (!list_empty(&req_list)) {
 +				req = list_first_entry(&req_list,
 +							struct io_kiocb, list);
 +				list_del(&req->list);
 +				goto restart;
 +			}
 +			ret = atomic_dec_return(&async_list->cnt);
 +		}
  	}
 -	spin_unlock_irq(&ctx->inflight_lock);
 -	rcu_read_unlock();
  
 -	return ret;
 +out:
 +	if (cur_mm) {
 +		set_fs(old_fs);
 +		unuse_mm(cur_mm);
 +		mmput(cur_mm);
 +	}
  }
  
 -static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
 +/*
 + * See if we can piggy back onto previously submitted work, that is still
 + * running. We currently only allow this if the new request is sequential
 + * to the previous one we punted.
 + */
 +static bool io_add_to_prev_work(struct async_list *list, struct io_kiocb *req)
  {
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_kiocb *prev = NULL;
 -	unsigned long flags;
 +	bool ret;
  
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 +	if (!list)
 +		return false;
 +	if (!(req->flags & REQ_F_SEQ_PREV))
 +		return false;
 +	if (!atomic_read(&list->cnt))
 +		return false;
  
 +	ret = true;
 +	spin_lock(&list->lock);
 +	list_add_tail(&req->list, &list->list);
  	/*
 -	 * We don't expect the list to be empty, that will only happen if we
 -	 * race with the completion of the linked work.
 +	 * Ensure we see a simultaneous modification from io_sq_wq_submit_work()
  	 */
 -	if (!list_empty(&req->link_list)) {
 -		prev = list_entry(req->link_list.prev, struct io_kiocb,
 -				  link_list);
 -		if (refcount_inc_not_zero(&prev->refs)) {
 -			list_del_init(&req->link_list);
 -			prev->flags &= ~REQ_F_LINK_TIMEOUT;
 -		} else
 -			prev = NULL;
 +	smp_mb();
 +	if (!atomic_read(&list->cnt)) {
 +		list_del_init(&req->list);
 +		ret = false;
  	}
 +	spin_unlock(&list->lock);
 +	return ret;
 +}
  
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 +static bool io_op_needs_file(const struct io_uring_sqe *sqe)
 +{
 +	int op = READ_ONCE(sqe->opcode);
  
 -	if (prev) {
 -		req_set_fail_links(prev);
 -		io_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);
 -		io_put_req(prev);
 -	} else {
 -		io_cqring_add_event(req, -ETIME);
 -		io_put_req(req);
 +	switch (op) {
 +	case IORING_OP_NOP:
 +	case IORING_OP_POLL_REMOVE:
 +	case IORING_OP_TIMEOUT:
 +		return false;
 +	default:
 +		return true;
  	}
 -	return HRTIMER_NORESTART;
  }
  
 -static void io_queue_linked_timeout(struct io_kiocb *req)
 +static int io_req_set_file(struct io_ring_ctx *ctx, const struct sqe_submit *s,
 +			   struct io_submit_state *state, struct io_kiocb *req)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 +	unsigned flags;
 +	int fd;
  
 +	flags = READ_ONCE(s->sqe->flags);
 +	fd = READ_ONCE(s->sqe->fd);
 +
 +	if (flags & IOSQE_IO_DRAIN)
 +		req->flags |= REQ_F_IO_DRAIN;
  	/*
 -	 * If the list is now empty, then our linked request finished before
 -	 * we got a chance to setup the timer
 +	 * All io need record the previous position, if LINK vs DARIN,
 +	 * it can be used to mark the position of the first IO in the
 +	 * link list.
  	 */
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (!list_empty(&req->link_list)) {
 -		struct io_timeout_data *data = &req->io->timeout;
 -
 -		data->timer.function = io_link_timeout_fn;
 -		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
 -				data->mode);
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	/* drop submission reference */
 -	io_put_req(req);
 -}
 -
 -static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
 -{
 -	struct io_kiocb *nxt;
 +	req->sequence = s->sequence;
  
 -	if (!(req->flags & REQ_F_LINK))
 -		return NULL;
 -	/* for polled retry, if flag is set, we already went through here */
 -	if (req->flags & REQ_F_POLLED)
 -		return NULL;
 +	if (!io_op_needs_file(s->sqe))
 +		return 0;
  
 -	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb,
 -					link_list);
 -	if (!nxt || nxt->opcode != IORING_OP_LINK_TIMEOUT)
 -		return NULL;
 +	if (flags & IOSQE_FIXED_FILE) {
 +		if (unlikely(!ctx->user_files ||
 +		    (unsigned) fd >= ctx->nr_user_files))
 +			return -EBADF;
 +		fd = array_index_nospec(fd, ctx->nr_user_files);
 +		if (!ctx->user_files[fd])
 +			return -EBADF;
 +		req->file = ctx->user_files[fd];
 +		req->flags |= REQ_F_FIXED_FILE;
 +	} else {
 +		if (s->needs_fixed_file)
 +			return -EBADF;
 +		req->file = io_file_get(state, fd);
 +		if (unlikely(!req->file))
 +			return -EBADF;
 +	}
  
 -	req->flags |= REQ_F_LINK_TIMEOUT;
 -	return nxt;
 +	return 0;
  }
  
 -static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_kiocb *linked_timeout;
 -	struct io_kiocb *nxt;
 -	const struct cred *old_creds = NULL;
  	int ret;
  
 -again:
 -	linked_timeout = io_prep_linked_timeout(req);
 -
 -	if (req->work.creds && req->work.creds != current_cred()) {
 -		if (old_creds)
 -			revert_creds(old_creds);
 -		if (old_creds == req->work.creds)
 -			old_creds = NULL; /* restored original creds */
 -		else
 -			old_creds = override_creds(req->work.creds);
 -	}
 -
 -	ret = io_issue_sqe(req, sqe, true);
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
 +		struct io_uring_sqe *sqe_copy;
 +
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (sqe_copy) {
 +			struct async_list *list;
 +
 +			s->sqe = sqe_copy;
 +			memcpy(&req->submit, s, sizeof(*s));
 +			list = io_async_list_from_sqe(ctx, s->sqe);
 +			if (!io_add_to_prev_work(list, req)) {
 +				if (list)
 +					atomic_inc(&list->cnt);
 +				INIT_WORK(&req->work, io_sq_wq_submit_work);
 +				io_queue_async_work(ctx, req);
 +			}
  
 -	/*
 -	 * We async punt it if the file wasn't marked NOWAIT, or if the file
 -	 * doesn't support non-blocking read/write attempts
 -	 */
 -	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
 -	    (req->flags & REQ_F_MUST_PUNT))) {
 -		if (io_arm_poll_handler(req)) {
 -			if (linked_timeout)
 -				io_queue_linked_timeout(linked_timeout);
 -			goto exit;
 -		}
 -punt:
 -		if (io_op_defs[req->opcode].file_table) {
 -			ret = io_grab_files(req);
 -			if (ret)
 -				goto err;
 +			/*
 +			 * Queued up for async execution, worker will release
 +			 * submit reference when the iocb is actually submitted.
 +			 */
 +			return 0;
  		}
 -
 -		/*
 -		 * Queued up for async execution, worker will release
 -		 * submit reference when the iocb is actually submitted.
 -		 */
 -		io_queue_async_work(req);
 -		goto exit;
  	}
  
 -err:
 -	nxt = NULL;
  	/* drop submission reference */
 -	io_put_req_find_next(req, &nxt);
 -
 -	if (linked_timeout) {
 -		if (!ret)
 -			io_queue_linked_timeout(linked_timeout);
 -		else
 -			io_put_req(linked_timeout);
 -	}
 +	io_put_req(req);
  
  	/* and drop final reference, if we failed */
  	if (ret) {
@@@ -3126,11 -6080,92 +3706,92 @@@ static int io_sqe_files_update(struct i
  		}
  		nr_args--;
  		done++;
 -		up->offset++;
 +		up.offset++;
  	}
  
 -	if (ref_switch)
 -		percpu_ref_switch_to_atomic(&data->refs, io_atomic_switch);
 -
  	return done ? done : err;
  }
++<<<<<<< HEAD
++=======
+ static int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,
+ 			       unsigned nr_args)
+ {
+ 	struct io_uring_files_update up;
+ 
+ 	if (!ctx->file_data)
+ 		return -ENXIO;
+ 	if (!nr_args)
+ 		return -EINVAL;
+ 	if (copy_from_user(&up, arg, sizeof(up)))
+ 		return -EFAULT;
+ 	if (up.resv)
+ 		return -EINVAL;
+ 
+ 	return __io_sqe_files_update(ctx, &up, nr_args);
+ }
+ 
+ static void io_put_work(struct io_wq_work *work)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	/* Consider that io_put_req_async_completion() relies on this ref */
+ 	io_put_req(req);
+ }
+ 
+ static void io_get_work(struct io_wq_work *work)
+ {
+ 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
+ 
+ 	refcount_inc(&req->refs);
+ }
+ 
+ static int io_init_wq_offload(struct io_ring_ctx *ctx,
+ 			      struct io_uring_params *p)
+ {
+ 	struct io_wq_data data;
+ 	struct fd f;
+ 	struct io_ring_ctx *ctx_attach;
+ 	unsigned int concurrency;
+ 	int ret = 0;
+ 
+ 	data.user = ctx->user;
+ 	data.get_work = io_get_work;
+ 	data.put_work = io_put_work;
+ 
+ 	if (!(p->flags & IORING_SETUP_ATTACH_WQ)) {
+ 		/* Do QD, or 4 * CPUS, whatever is smallest */
+ 		concurrency = min(ctx->sq_entries, 4 * num_online_cpus());
+ 
+ 		ctx->io_wq = io_wq_create(concurrency, &data);
+ 		if (IS_ERR(ctx->io_wq)) {
+ 			ret = PTR_ERR(ctx->io_wq);
+ 			ctx->io_wq = NULL;
+ 		}
+ 		return ret;
+ 	}
+ 
+ 	f = fdget(p->wq_fd);
+ 	if (!f.file)
+ 		return -EBADF;
+ 
+ 	if (f.file->f_op != &io_uring_fops) {
+ 		ret = -EINVAL;
+ 		goto out_fput;
+ 	}
+ 
+ 	ctx_attach = f.file->private_data;
+ 	/* @io_wq is protected by holding the fd */
+ 	if (!io_wq_get(ctx_attach->io_wq, &data)) {
+ 		ret = -EINVAL;
+ 		goto out_fput;
+ 	}
+ 
+ 	ctx->io_wq = ctx_attach->io_wq;
+ out_fput:
+ 	fdput(f);
+ 	return ret;
+ }
++>>>>>>> 7a743e225b2a (io_uring: get next work with submission ref drop)
  
  static int io_sq_offload_start(struct io_ring_ctx *ctx,
  			       struct io_uring_params *p)
* Unmerged path fs/io_uring.c
