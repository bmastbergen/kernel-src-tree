io_uring: io_fail_links() should only consider first linked timeout

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 5d960724b0cb0d12469d1c62912e4a8c09c9fd92
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5d960724.failed

We currently clear the linked timeout field if we cancel such a timeout,
but we should only attempt to cancel if it's the first one we see.
Others should simply be freed like other requests, as they haven't
been started yet.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 5d960724b0cb0d12469d1c62912e4a8c09c9fd92)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 79a564f6a100,09fc29599e96..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -660,19 -922,46 +660,36 @@@ static void io_req_link_next(struct io_
   */
  static void io_fail_links(struct io_kiocb *req)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
  	struct io_kiocb *link;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
  
  	while (!list_empty(&req->link_list)) {
 -		const struct io_uring_sqe *sqe_to_free = NULL;
 -
  		link = list_first_entry(&req->link_list, struct io_kiocb, list);
 -		list_del_init(&link->list);
 +		list_del(&link->list);
  
++<<<<<<< HEAD
 +		io_cqring_add_event(req->ctx, link->user_data, -ECANCELED);
 +		__io_free_req(link);
++=======
+ 		trace_io_uring_fail_link(req, link);
+ 
+ 		if (link->flags & REQ_F_FREE_SQE)
+ 			sqe_to_free = link->submit.sqe;
+ 
+ 		if ((req->flags & REQ_F_LINK_TIMEOUT) &&
+ 		    link->submit.sqe->opcode == IORING_OP_LINK_TIMEOUT) {
+ 			io_link_cancel_timeout(link);
+ 		} else {
+ 			io_cqring_fill_event(link, -ECANCELED);
+ 			__io_double_put_req(link);
+ 		}
+ 		kfree(sqe_to_free);
+ 		req->flags &= ~REQ_F_LINK_TIMEOUT;
++>>>>>>> 5d960724b0cb (io_uring: io_fail_links() should only consider first linked timeout)
  	}
 -
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -	io_cqring_ev_posted(ctx);
  }
  
 -static void io_free_req_find_next(struct io_kiocb *req, struct io_kiocb **nxt)
 +static void io_free_req(struct io_kiocb *req)
  {
 -	if (likely(!(req->flags & REQ_F_LINK))) {
 -		__io_free_req(req);
 -		return;
 -	}
 -
  	/*
  	 * If LINK is set, we have dependent requests in this chain. If we
  	 * didn't fail this request, queue the first one up, moving any other
@@@ -2162,13 -2795,121 +2179,118 @@@ static int io_req_set_file(struct io_ri
  	return 0;
  }
  
 -static int io_grab_files(struct io_kiocb *req)
 +static int __io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
++<<<<<<< HEAD
++=======
+ 	int ret = -EBADF;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irq(&ctx->inflight_lock);
+ 	/*
+ 	 * We use the f_ops->flush() handler to ensure that we can flush
+ 	 * out work accessing these files if the fd is closed. Check if
+ 	 * the fd has changed since we started down this path, and disallow
+ 	 * this operation if it has.
+ 	 */
+ 	if (fcheck(req->submit.ring_fd) == req->submit.ring_file) {
+ 		list_add(&req->inflight_entry, &ctx->inflight_list);
+ 		req->flags |= REQ_F_INFLIGHT;
+ 		req->work.files = current->files;
+ 		ret = 0;
+ 	}
+ 	spin_unlock_irq(&ctx->inflight_lock);
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_timeout_data *data = container_of(timer,
+ 						struct io_timeout_data, timer);
+ 	struct io_kiocb *req = data->req;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *prev = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/*
+ 	 * We don't expect the list to be empty, that will only happen if we
+ 	 * race with the completion of the linked work.
+ 	 */
+ 	if (!list_empty(&req->list)) {
+ 		prev = list_entry(req->list.prev, struct io_kiocb, link_list);
+ 		if (refcount_inc_not_zero(&prev->refs)) {
+ 			list_del_init(&req->list);
+ 			prev->flags &= ~REQ_F_LINK_TIMEOUT;
+ 		} else
+ 			prev = NULL;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	if (prev) {
+ 		if (prev->flags & REQ_F_LINK)
+ 			prev->flags |= REQ_F_FAIL_LINK;
+ 		io_async_find_and_cancel(ctx, req, prev->user_data, NULL,
+ 						-ETIME);
+ 		io_put_req(prev);
+ 	} else {
+ 		io_cqring_add_event(req, -ETIME);
+ 		io_put_req(req);
+ 	}
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static void io_queue_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 
+ 	/*
+ 	 * If the list is now empty, then our linked request finished before
+ 	 * we got a chance to setup the timer
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	if (!list_empty(&req->list)) {
+ 		struct io_timeout_data *data = req->timeout.data;
+ 
+ 		data->timer.function = io_link_timeout_fn;
+ 		hrtimer_start(&data->timer, timespec64_to_ktime(data->ts),
+ 				data->mode);
+ 	}
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	/* drop submission reference */
+ 	io_put_req(req);
+ }
+ 
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	if (!(req->flags & REQ_F_LINK))
+ 		return NULL;
+ 
+ 	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb, list);
+ 	if (!nxt || nxt->submit.sqe->opcode != IORING_OP_LINK_TIMEOUT)
+ 		return NULL;
+ 
+ 	req->flags |= REQ_F_LINK_TIMEOUT;
+ 	return nxt;
+ }
+ 
+ static void __io_queue_sqe(struct io_kiocb *req)
+ {
+ 	struct io_kiocb *nxt = io_prep_linked_timeout(req);
++>>>>>>> 5d960724b0cb (io_uring: io_fail_links() should only consider first linked timeout)
  	int ret;
  
 -	ret = __io_submit_sqe(req, NULL, true);
 -
 -	/*
 -	 * We async punt it if the file wasn't marked NOWAIT, or if the file
 -	 * doesn't support non-blocking read/write attempts
 -	 */
 -	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
 -	    (req->flags & REQ_F_MUST_PUNT))) {
 -		struct sqe_submit *s = &req->submit;
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
  		struct io_uring_sqe *sqe_copy;
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
* Unmerged path fs/io_uring.c
