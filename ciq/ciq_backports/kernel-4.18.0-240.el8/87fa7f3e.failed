x86/kvm: Move context tracking where it belongs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 87fa7f3e98a1310ef1ac1900e7ee7f9610a038bc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/87fa7f3e.failed

Context tracking for KVM happens way too early in the vcpu_run()
code. Anything after guest_enter_irqoff() and before guest_exit_irqoff()
cannot use RCU and should also be not instrumented.

The current way of doing this covers way too much code. Move it closer to
the actual vmenter/exit code.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
	Acked-by: Peter Zijlstra <peterz@infradead.org>
	Acked-by: Paolo Bonzini <pbonzini@redhat.com>
Message-Id: <20200708195321.724574345@linutronix.de>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 87fa7f3e98a1310ef1ac1900e7ee7f9610a038bc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/svm.c
diff --cc arch/x86/kvm/svm/svm.c
index 986d068fbec8,0227c4cbe642..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -3057,3320 -1930,1142 +3057,3340 @@@ static int vmmcall_interception(struct 
  	return kvm_emulate_hypercall(&svm->vcpu);
  }
  
 -static int vmload_interception(struct vcpu_svm *svm)
 +static unsigned long nested_svm_get_tdp_cr3(struct kvm_vcpu *vcpu)
  {
 -	struct vmcb *nested_vmcb;
 -	struct kvm_host_map map;
 -	int ret;
 -
 -	if (nested_svm_check_permissions(svm))
 -		return 1;
 +	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);
 -	if (ret) {
 -		if (ret == -EINVAL)
 -			kvm_inject_gp(&svm->vcpu, 0);
 -		return 1;
 -	}
 +	return svm->nested.nested_cr3;
 +}
  
 -	nested_vmcb = map.hva;
 +static u64 nested_svm_get_tdp_pdptr(struct kvm_vcpu *vcpu, int index)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u64 cr3 = svm->nested.nested_cr3;
 +	u64 pdpte;
 +	int ret;
  
 -	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 +	ret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(__sme_clr(cr3)), &pdpte,
 +				       offset_in_page(cr3) + index * 8, 8);
 +	if (ret)
 +		return 0;
 +	return pdpte;
 +}
  
 -	nested_svm_vmloadsave(nested_vmcb, svm->vmcb);
 -	kvm_vcpu_unmap(&svm->vcpu, &map, true);
 +static void nested_svm_set_tdp_cr3(struct kvm_vcpu *vcpu,
 +				   unsigned long root)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	return ret;
 +	svm->vmcb->control.nested_cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_NPT);
  }
  
 -static int vmsave_interception(struct vcpu_svm *svm)
 +static void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,
 +				       struct x86_exception *fault)
  {
 -	struct vmcb *nested_vmcb;
 -	struct kvm_host_map map;
 -	int ret;
 -
 -	if (nested_svm_check_permissions(svm))
 -		return 1;
 +	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);
 -	if (ret) {
 -		if (ret == -EINVAL)
 -			kvm_inject_gp(&svm->vcpu, 0);
 -		return 1;
 +	if (svm->vmcb->control.exit_code != SVM_EXIT_NPF) {
 +		/*
 +		 * TODO: track the cause of the nested page fault, and
 +		 * correctly fill in the high bits of exit_info_1.
 +		 */
 +		svm->vmcb->control.exit_code = SVM_EXIT_NPF;
 +		svm->vmcb->control.exit_code_hi = 0;
 +		svm->vmcb->control.exit_info_1 = (1ULL << 32);
 +		svm->vmcb->control.exit_info_2 = fault->address;
  	}
  
 -	nested_vmcb = map.hva;
 -
 -	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 +	svm->vmcb->control.exit_info_1 &= ~0xffffffffULL;
 +	svm->vmcb->control.exit_info_1 |= fault->error_code;
  
 -	nested_svm_vmloadsave(svm->vmcb, nested_vmcb);
 -	kvm_vcpu_unmap(&svm->vcpu, &map, true);
 +	/*
 +	 * The present bit is always zero for page structure faults on real
 +	 * hardware.
 +	 */
 +	if (svm->vmcb->control.exit_info_1 & (2ULL << 32))
 +		svm->vmcb->control.exit_info_1 &= ~1;
  
 -	return ret;
 +	nested_svm_vmexit(svm);
  }
  
 -static int vmrun_interception(struct vcpu_svm *svm)
 +static void nested_svm_init_mmu_context(struct kvm_vcpu *vcpu)
  {
 -	if (nested_svm_check_permissions(svm))
 -		return 1;
 +	WARN_ON(mmu_is_nested(vcpu));
  
 -	return nested_svm_vmrun(svm);
 +	vcpu->arch.mmu = &vcpu->arch.guest_mmu;
 +	kvm_init_shadow_mmu(vcpu);
 +	vcpu->arch.mmu->set_cr3           = nested_svm_set_tdp_cr3;
 +	vcpu->arch.mmu->get_guest_pgd     = nested_svm_get_tdp_cr3;
 +	vcpu->arch.mmu->get_pdptr         = nested_svm_get_tdp_pdptr;
 +	vcpu->arch.mmu->inject_page_fault = nested_svm_inject_npf_exit;
 +	vcpu->arch.mmu->shadow_root_level = get_npt_level(vcpu);
 +	reset_shadow_zero_bits_mask(vcpu, vcpu->arch.mmu);
 +	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
  }
  
 -void svm_set_gif(struct vcpu_svm *svm, bool value)
 +static void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)
  {
 -	if (value) {
 -		/*
 -		 * If VGIF is enabled, the STGI intercept is only added to
 -		 * detect the opening of the SMI/NMI window; remove it now.
 -		 * Likewise, clear the VINTR intercept, we will set it
 -		 * again while processing KVM_REQ_EVENT if needed.
 -		 */
 -		if (vgif_enabled(svm))
 -			svm_clr_intercept(svm, INTERCEPT_STGI);
 -		if (svm_is_intercept(svm, INTERCEPT_VINTR))
 -			svm_clear_vintr(svm);
 -
 -		enable_gif(svm);
 -		if (svm->vcpu.arch.smi_pending ||
 -		    svm->vcpu.arch.nmi_pending ||
 -		    kvm_cpu_has_injectable_intr(&svm->vcpu))
 -			kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 -	} else {
 -		disable_gif(svm);
 -
 -		/*
 -		 * After a CLGI no interrupts should come.  But if vGIF is
 -		 * in use, we still rely on the VINTR intercept (rather than
 -		 * STGI) to detect an open interrupt window.
 -		*/
 -		if (!vgif_enabled(svm))
 -			svm_clear_vintr(svm);
 -	}
 +	vcpu->arch.mmu = &vcpu->arch.root_mmu;
 +	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
  }
  
 -static int stgi_interception(struct vcpu_svm *svm)
 +static int nested_svm_check_permissions(struct vcpu_svm *svm)
  {
 -	int ret;
 +	if (!(svm->vcpu.arch.efer & EFER_SVME) ||
 +	    !is_paging(&svm->vcpu)) {
 +		kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 +		return 1;
 +	}
  
 -	if (nested_svm_check_permissions(svm))
 +	if (svm->vmcb->save.cpl) {
 +		kvm_inject_gp(&svm->vcpu, 0);
  		return 1;
 +	}
  
 -	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 -	svm_set_gif(svm, true);
 -	return ret;
 +	return 0;
  }
  
 -static int clgi_interception(struct vcpu_svm *svm)
 +static int nested_svm_check_exception(struct vcpu_svm *svm, unsigned nr,
 +				      bool has_error_code, u32 error_code)
  {
 -	int ret;
 +	int vmexit;
  
 -	if (nested_svm_check_permissions(svm))
 -		return 1;
 +	if (!is_guest_mode(&svm->vcpu))
 +		return 0;
  
 -	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 -	svm_set_gif(svm, false);
 -	return ret;
 +	vmexit = nested_svm_intercept(svm);
 +	if (vmexit != NESTED_EXIT_DONE)
 +		return 0;
 +
 +	svm->vmcb->control.exit_code = SVM_EXIT_EXCP_BASE + nr;
 +	svm->vmcb->control.exit_code_hi = 0;
 +	svm->vmcb->control.exit_info_1 = error_code;
 +
 +	/*
 +	 * EXITINFO2 is undefined for all exception intercepts other
 +	 * than #PF.
 +	 */
 +	if (svm->vcpu.arch.exception.nested_apf)
 +		svm->vmcb->control.exit_info_2 = svm->vcpu.arch.apf.nested_apf_token;
 +	else if (svm->vcpu.arch.exception.has_payload)
 +		svm->vmcb->control.exit_info_2 = svm->vcpu.arch.exception.payload;
 +	else
 +		svm->vmcb->control.exit_info_2 = svm->vcpu.arch.cr2;
 +
 +	svm->nested.exit_required = true;
 +	return vmexit;
  }
  
 -static int invlpga_interception(struct vcpu_svm *svm)
 +/* This function returns true if it is save to enable the irq window */
 +static inline bool nested_svm_intr(struct vcpu_svm *svm)
  {
 -	struct kvm_vcpu *vcpu = &svm->vcpu;
 +	if (!is_guest_mode(&svm->vcpu))
 +		return true;
  
 -	trace_kvm_invlpga(svm->vmcb->save.rip, kvm_rcx_read(&svm->vcpu),
 -			  kvm_rax_read(&svm->vcpu));
 +	if (!(svm->vcpu.arch.hflags & HF_VINTR_MASK))
 +		return true;
  
 -	/* Let's treat INVLPGA the same as INVLPG (can be optimized!) */
 -	kvm_mmu_invlpg(vcpu, kvm_rax_read(&svm->vcpu));
 +	if (!(svm->vcpu.arch.hflags & HF_HIF_MASK))
 +		return false;
  
 -	return kvm_skip_emulated_instruction(&svm->vcpu);
 -}
 +	/*
 +	 * if vmexit was already requested (by intercepted exception
 +	 * for instance) do not overwrite it with "external interrupt"
 +	 * vmexit.
 +	 */
 +	if (svm->nested.exit_required)
 +		return false;
  
 -static int skinit_interception(struct vcpu_svm *svm)
 -{
 -	trace_kvm_skinit(svm->vmcb->save.rip, kvm_rax_read(&svm->vcpu));
 +	svm->vmcb->control.exit_code   = SVM_EXIT_INTR;
 +	svm->vmcb->control.exit_info_1 = 0;
 +	svm->vmcb->control.exit_info_2 = 0;
  
 -	kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 -	return 1;
 -}
 +	if (svm->nested.intercept & 1ULL) {
 +		/*
 +		 * The #vmexit can't be emulated here directly because this
 +		 * code path runs with irqs and preemption disabled. A
 +		 * #vmexit emulation might sleep. Only signal request for
 +		 * the #vmexit here.
 +		 */
 +		svm->nested.exit_required = true;
 +		trace_kvm_nested_intr_vmexit(svm->vmcb->save.rip);
 +		return false;
 +	}
  
 -static int wbinvd_interception(struct vcpu_svm *svm)
 -{
 -	return kvm_emulate_wbinvd(&svm->vcpu);
 +	return true;
  }
  
 -static int xsetbv_interception(struct vcpu_svm *svm)
 +/* This function returns true if it is save to enable the nmi window */
 +static inline bool nested_svm_nmi(struct vcpu_svm *svm)
  {
 -	u64 new_bv = kvm_read_edx_eax(&svm->vcpu);
 -	u32 index = kvm_rcx_read(&svm->vcpu);
 +	if (!is_guest_mode(&svm->vcpu))
 +		return true;
  
 -	if (kvm_set_xcr(&svm->vcpu, index, new_bv) == 0) {
 -		return kvm_skip_emulated_instruction(&svm->vcpu);
 -	}
 +	if (!(svm->nested.intercept & (1ULL << INTERCEPT_NMI)))
 +		return true;
  
 -	return 1;
 +	svm->vmcb->control.exit_code = SVM_EXIT_NMI;
 +	svm->nested.exit_required = true;
 +
 +	return false;
  }
  
 -static int rdpru_interception(struct vcpu_svm *svm)
 +static int nested_svm_intercept_ioio(struct vcpu_svm *svm)
  {
 -	kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 -	return 1;
 +	unsigned port, size, iopm_len;
 +	u16 val, mask;
 +	u8 start_bit;
 +	u64 gpa;
 +
 +	if (!(svm->nested.intercept & (1ULL << INTERCEPT_IOIO_PROT)))
 +		return NESTED_EXIT_HOST;
 +
 +	port = svm->vmcb->control.exit_info_1 >> 16;
 +	size = (svm->vmcb->control.exit_info_1 & SVM_IOIO_SIZE_MASK) >>
 +		SVM_IOIO_SIZE_SHIFT;
 +	gpa  = svm->nested.vmcb_iopm + (port / 8);
 +	start_bit = port % 8;
 +	iopm_len = (start_bit + size > 8) ? 2 : 1;
 +	mask = (0xf >> (4 - size)) << start_bit;
 +	val = 0;
 +
 +	if (kvm_vcpu_read_guest(&svm->vcpu, gpa, &val, iopm_len))
 +		return NESTED_EXIT_DONE;
 +
 +	return (val & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;
  }
  
 -static int task_switch_interception(struct vcpu_svm *svm)
 +static int nested_svm_exit_handled_msr(struct vcpu_svm *svm)
  {
 -	u16 tss_selector;
 -	int reason;
 -	int int_type = svm->vmcb->control.exit_int_info &
 -		SVM_EXITINTINFO_TYPE_MASK;
 -	int int_vec = svm->vmcb->control.exit_int_info & SVM_EVTINJ_VEC_MASK;
 -	uint32_t type =
 -		svm->vmcb->control.exit_int_info & SVM_EXITINTINFO_TYPE_MASK;
 -	uint32_t idt_v =
 -		svm->vmcb->control.exit_int_info & SVM_EXITINTINFO_VALID;
 -	bool has_error_code = false;
 -	u32 error_code = 0;
 +	u32 offset, msr, value;
 +	int write, mask;
  
 -	tss_selector = (u16)svm->vmcb->control.exit_info_1;
 +	if (!(svm->nested.intercept & (1ULL << INTERCEPT_MSR_PROT)))
 +		return NESTED_EXIT_HOST;
  
 -	if (svm->vmcb->control.exit_info_2 &
 -	    (1ULL << SVM_EXITINFOSHIFT_TS_REASON_IRET))
 -		reason = TASK_SWITCH_IRET;
 -	else if (svm->vmcb->control.exit_info_2 &
 -		 (1ULL << SVM_EXITINFOSHIFT_TS_REASON_JMP))
 -		reason = TASK_SWITCH_JMP;
 -	else if (idt_v)
 -		reason = TASK_SWITCH_GATE;
 -	else
 -		reason = TASK_SWITCH_CALL;
 +	msr    = svm->vcpu.arch.regs[VCPU_REGS_RCX];
 +	offset = svm_msrpm_offset(msr);
 +	write  = svm->vmcb->control.exit_info_1 & 1;
 +	mask   = 1 << ((2 * (msr & 0xf)) + write);
  
 -	if (reason == TASK_SWITCH_GATE) {
 -		switch (type) {
 -		case SVM_EXITINTINFO_TYPE_NMI:
 -			svm->vcpu.arch.nmi_injected = false;
 -			break;
 -		case SVM_EXITINTINFO_TYPE_EXEPT:
 -			if (svm->vmcb->control.exit_info_2 &
 -			    (1ULL << SVM_EXITINFOSHIFT_TS_HAS_ERROR_CODE)) {
 -				has_error_code = true;
 -				error_code =
 -					(u32)svm->vmcb->control.exit_info_2;
 -			}
 -			kvm_clear_exception_queue(&svm->vcpu);
 -			break;
 -		case SVM_EXITINTINFO_TYPE_INTR:
 -			kvm_clear_interrupt_queue(&svm->vcpu);
 -			break;
 -		default:
 -			break;
 -		}
 -	}
 +	if (offset == MSR_INVALID)
 +		return NESTED_EXIT_DONE;
  
 -	if (reason != TASK_SWITCH_GATE ||
 -	    int_type == SVM_EXITINTINFO_TYPE_SOFT ||
 -	    (int_type == SVM_EXITINTINFO_TYPE_EXEPT &&
 -	     (int_vec == OF_VECTOR || int_vec == BP_VECTOR))) {
 -		if (!skip_emulated_instruction(&svm->vcpu))
 -			return 0;
 -	}
 +	/* Offset is in 32 bit units but need in 8 bit units */
 +	offset *= 4;
  
 -	if (int_type != SVM_EXITINTINFO_TYPE_SOFT)
 -		int_vec = -1;
 +	if (kvm_vcpu_read_guest(&svm->vcpu, svm->nested.vmcb_msrpm + offset, &value, 4))
 +		return NESTED_EXIT_DONE;
  
 -	return kvm_task_switch(&svm->vcpu, tss_selector, int_vec, reason,
 -			       has_error_code, error_code);
 +	return (value & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;
  }
  
 -static int cpuid_interception(struct vcpu_svm *svm)
 +/* DB exceptions for our internal use must not cause vmexit */
 +static int nested_svm_intercept_db(struct vcpu_svm *svm)
  {
 -	return kvm_emulate_cpuid(&svm->vcpu);
 -}
 +	unsigned long dr6;
  
 -static int iret_interception(struct vcpu_svm *svm)
 -{
 -	++svm->vcpu.stat.nmi_window_exits;
 -	svm_clr_intercept(svm, INTERCEPT_IRET);
 -	svm->vcpu.arch.hflags |= HF_IRET_MASK;
 -	svm->nmi_iret_rip = kvm_rip_read(&svm->vcpu);
 -	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 -	return 1;
 +	/* if we're not singlestepping, it's not ours */
 +	if (!svm->nmi_singlestep)
 +		return NESTED_EXIT_DONE;
 +
 +	/* if it's not a singlestep exception, it's not ours */
 +	if (kvm_get_dr(&svm->vcpu, 6, &dr6))
 +		return NESTED_EXIT_DONE;
 +	if (!(dr6 & DR6_BS))
 +		return NESTED_EXIT_DONE;
 +
 +	/* if the guest is singlestepping, it should get the vmexit */
 +	if (svm->nmi_singlestep_guest_rflags & X86_EFLAGS_TF) {
 +		disable_nmi_singlestep(svm);
 +		return NESTED_EXIT_DONE;
 +	}
 +
 +	/* it's ours, the nested hypervisor must not see this one */
 +	return NESTED_EXIT_HOST;
  }
  
 -static int invlpg_interception(struct vcpu_svm *svm)
 +static int nested_svm_exit_special(struct vcpu_svm *svm)
  {
 -	if (!static_cpu_has(X86_FEATURE_DECODEASSISTS))
 -		return kvm_emulate_instruction(&svm->vcpu, 0);
 +	u32 exit_code = svm->vmcb->control.exit_code;
  
 -	kvm_mmu_invlpg(&svm->vcpu, svm->vmcb->control.exit_info_1);
 -	return kvm_skip_emulated_instruction(&svm->vcpu);
 +	switch (exit_code) {
 +	case SVM_EXIT_INTR:
 +	case SVM_EXIT_NMI:
 +	case SVM_EXIT_EXCP_BASE + MC_VECTOR:
 +		return NESTED_EXIT_HOST;
 +	case SVM_EXIT_NPF:
 +		/* For now we are always handling NPFs when using them */
 +		if (npt_enabled)
 +			return NESTED_EXIT_HOST;
 +		break;
 +	case SVM_EXIT_EXCP_BASE + PF_VECTOR:
 +		/* When we're shadowing, trap PFs, but not async PF */
 +		if (!npt_enabled && svm->vcpu.arch.apf.host_apf_reason == 0)
 +			return NESTED_EXIT_HOST;
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	return NESTED_EXIT_CONTINUE;
  }
  
 -static int emulate_on_interception(struct vcpu_svm *svm)
 +static int nested_svm_intercept(struct vcpu_svm *svm)
  {
 -	return kvm_emulate_instruction(&svm->vcpu, 0);
 +	u32 exit_code = svm->vmcb->control.exit_code;
 +	int vmexit = NESTED_EXIT_HOST;
 +
 +	switch (exit_code) {
 +	case SVM_EXIT_MSR:
 +		vmexit = nested_svm_exit_handled_msr(svm);
 +		break;
 +	case SVM_EXIT_IOIO:
 +		vmexit = nested_svm_intercept_ioio(svm);
 +		break;
 +	case SVM_EXIT_READ_CR0 ... SVM_EXIT_WRITE_CR8: {
 +		u32 bit = 1U << (exit_code - SVM_EXIT_READ_CR0);
 +		if (svm->nested.intercept_cr & bit)
 +			vmexit = NESTED_EXIT_DONE;
 +		break;
 +	}
 +	case SVM_EXIT_READ_DR0 ... SVM_EXIT_WRITE_DR7: {
 +		u32 bit = 1U << (exit_code - SVM_EXIT_READ_DR0);
 +		if (svm->nested.intercept_dr & bit)
 +			vmexit = NESTED_EXIT_DONE;
 +		break;
 +	}
 +	case SVM_EXIT_EXCP_BASE ... SVM_EXIT_EXCP_BASE + 0x1f: {
 +		u32 excp_bits = 1 << (exit_code - SVM_EXIT_EXCP_BASE);
 +		if (svm->nested.intercept_exceptions & excp_bits) {
 +			if (exit_code == SVM_EXIT_EXCP_BASE + DB_VECTOR)
 +				vmexit = nested_svm_intercept_db(svm);
 +			else
 +				vmexit = NESTED_EXIT_DONE;
 +		}
 +		/* async page fault always cause vmexit */
 +		else if ((exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR) &&
 +			 svm->vcpu.arch.exception.nested_apf != 0)
 +			vmexit = NESTED_EXIT_DONE;
 +		break;
 +	}
 +	case SVM_EXIT_ERR: {
 +		vmexit = NESTED_EXIT_DONE;
 +		break;
 +	}
 +	default: {
 +		u64 exit_bits = 1ULL << (exit_code - SVM_EXIT_INTR);
 +		if (svm->nested.intercept & exit_bits)
 +			vmexit = NESTED_EXIT_DONE;
 +	}
 +	}
 +
 +	return vmexit;
  }
  
 -static int rsm_interception(struct vcpu_svm *svm)
 +static int nested_svm_exit_handled(struct vcpu_svm *svm)
  {
 -	return kvm_emulate_instruction_from_buffer(&svm->vcpu, rsm_ins_bytes, 2);
 +	int vmexit;
 +
 +	vmexit = nested_svm_intercept(svm);
 +
 +	if (vmexit == NESTED_EXIT_DONE)
 +		nested_svm_vmexit(svm);
 +
 +	return vmexit;
  }
  
 -static int rdpmc_interception(struct vcpu_svm *svm)
 +static inline void copy_vmcb_control_area(struct vmcb *dst_vmcb, struct vmcb *from_vmcb)
  {
 -	int err;
 -
 -	if (!nrips)
 -		return emulate_on_interception(svm);
 +	struct vmcb_control_area *dst  = &dst_vmcb->control;
 +	struct vmcb_control_area *from = &from_vmcb->control;
  
 -	err = kvm_rdpmc(&svm->vcpu);
 -	return kvm_complete_insn_gp(&svm->vcpu, err);
 +	dst->intercept_cr         = from->intercept_cr;
 +	dst->intercept_dr         = from->intercept_dr;
 +	dst->intercept_exceptions = from->intercept_exceptions;
 +	dst->intercept            = from->intercept;
 +	dst->iopm_base_pa         = from->iopm_base_pa;
 +	dst->msrpm_base_pa        = from->msrpm_base_pa;
 +	dst->tsc_offset           = from->tsc_offset;
 +	dst->asid                 = from->asid;
 +	dst->tlb_ctl              = from->tlb_ctl;
 +	dst->int_ctl              = from->int_ctl;
 +	dst->int_vector           = from->int_vector;
 +	dst->int_state            = from->int_state;
 +	dst->exit_code            = from->exit_code;
 +	dst->exit_code_hi         = from->exit_code_hi;
 +	dst->exit_info_1          = from->exit_info_1;
 +	dst->exit_info_2          = from->exit_info_2;
 +	dst->exit_int_info        = from->exit_int_info;
 +	dst->exit_int_info_err    = from->exit_int_info_err;
 +	dst->nested_ctl           = from->nested_ctl;
 +	dst->event_inj            = from->event_inj;
 +	dst->event_inj_err        = from->event_inj_err;
 +	dst->nested_cr3           = from->nested_cr3;
 +	dst->virt_ext              = from->virt_ext;
 +	dst->pause_filter_count   = from->pause_filter_count;
 +	dst->pause_filter_thresh  = from->pause_filter_thresh;
  }
  
 -static bool check_selective_cr0_intercepted(struct vcpu_svm *svm,
 -					    unsigned long val)
 +static int nested_svm_vmexit(struct vcpu_svm *svm)
  {
 -	unsigned long cr0 = svm->vcpu.arch.cr0;
 -	bool ret = false;
 -	u64 intercept;
 +	int rc;
 +	struct vmcb *nested_vmcb;
 +	struct vmcb *hsave = svm->nested.hsave;
 +	struct vmcb *vmcb = svm->vmcb;
 +	struct kvm_host_map map;
  
 -	intercept = svm->nested.ctl.intercept;
 +	trace_kvm_nested_vmexit_inject(vmcb->control.exit_code,
 +				       vmcb->control.exit_info_1,
 +				       vmcb->control.exit_info_2,
 +				       vmcb->control.exit_int_info,
 +				       vmcb->control.exit_int_info_err,
 +				       KVM_ISA_SVM);
  
 -	if (!is_guest_mode(&svm->vcpu) ||
 -	    (!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0))))
 -		return false;
 +	rc = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->nested.vmcb), &map);
 +	if (rc) {
 +		if (rc == -EINVAL)
 +			kvm_inject_gp(&svm->vcpu, 0);
 +		return 1;
 +	}
  
 -	cr0 &= ~SVM_CR0_SELECTIVE_MASK;
 -	val &= ~SVM_CR0_SELECTIVE_MASK;
 +	nested_vmcb = map.hva;
  
 -	if (cr0 ^ val) {
 -		svm->vmcb->control.exit_code = SVM_EXIT_CR0_SEL_WRITE;
 -		ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
 -	}
 +	/* Exit Guest-Mode */
 +	leave_guest_mode(&svm->vcpu);
 +	svm->nested.vmcb = 0;
  
 -	return ret;
 -}
 +	/* Give the current vmcb to the guest */
 +	disable_gif(svm);
 +
 +	nested_vmcb->save.es     = vmcb->save.es;
 +	nested_vmcb->save.cs     = vmcb->save.cs;
 +	nested_vmcb->save.ss     = vmcb->save.ss;
 +	nested_vmcb->save.ds     = vmcb->save.ds;
 +	nested_vmcb->save.gdtr   = vmcb->save.gdtr;
 +	nested_vmcb->save.idtr   = vmcb->save.idtr;
 +	nested_vmcb->save.efer   = svm->vcpu.arch.efer;
 +	nested_vmcb->save.cr0    = kvm_read_cr0(&svm->vcpu);
 +	nested_vmcb->save.cr3    = kvm_read_cr3(&svm->vcpu);
 +	nested_vmcb->save.cr2    = vmcb->save.cr2;
 +	nested_vmcb->save.cr4    = svm->vcpu.arch.cr4;
 +	nested_vmcb->save.rflags = kvm_get_rflags(&svm->vcpu);
 +	nested_vmcb->save.rip    = vmcb->save.rip;
 +	nested_vmcb->save.rsp    = vmcb->save.rsp;
 +	nested_vmcb->save.rax    = vmcb->save.rax;
 +	nested_vmcb->save.dr7    = vmcb->save.dr7;
 +	nested_vmcb->save.dr6    = vmcb->save.dr6;
 +	nested_vmcb->save.cpl    = vmcb->save.cpl;
 +
 +	nested_vmcb->control.int_ctl           = vmcb->control.int_ctl;
 +	nested_vmcb->control.int_vector        = vmcb->control.int_vector;
 +	nested_vmcb->control.int_state         = vmcb->control.int_state;
 +	nested_vmcb->control.exit_code         = vmcb->control.exit_code;
 +	nested_vmcb->control.exit_code_hi      = vmcb->control.exit_code_hi;
 +	nested_vmcb->control.exit_info_1       = vmcb->control.exit_info_1;
 +	nested_vmcb->control.exit_info_2       = vmcb->control.exit_info_2;
 +	nested_vmcb->control.exit_int_info     = vmcb->control.exit_int_info;
 +	nested_vmcb->control.exit_int_info_err = vmcb->control.exit_int_info_err;
 +
 +	if (svm->nrips_enabled)
 +		nested_vmcb->control.next_rip  = vmcb->control.next_rip;
  
 -#define CR_VALID (1ULL << 63)
 +	/*
 +	 * If we emulate a VMRUN/#VMEXIT in the same host #vmexit cycle we have
 +	 * to make sure that we do not lose injected events. So check event_inj
 +	 * here and copy it to exit_int_info if it is valid.
 +	 * Exit_int_info and event_inj can't be both valid because the case
 +	 * below only happens on a VMRUN instruction intercept which has
 +	 * no valid exit_int_info set.
 +	 */
 +	if (vmcb->control.event_inj & SVM_EVTINJ_VALID) {
 +		struct vmcb_control_area *nc = &nested_vmcb->control;
  
 -static int cr_interception(struct vcpu_svm *svm)
 -{
 -	int reg, cr;
 -	unsigned long val;
 -	int err;
 +		nc->exit_int_info     = vmcb->control.event_inj;
 +		nc->exit_int_info_err = vmcb->control.event_inj_err;
 +	}
  
 -	if (!static_cpu_has(X86_FEATURE_DECODEASSISTS))
 -		return emulate_on_interception(svm);
 +	nested_vmcb->control.tlb_ctl           = 0;
 +	nested_vmcb->control.event_inj         = 0;
 +	nested_vmcb->control.event_inj_err     = 0;
  
 -	if (unlikely((svm->vmcb->control.exit_info_1 & CR_VALID) == 0))
 -		return emulate_on_interception(svm);
 +	nested_vmcb->control.pause_filter_count =
 +		svm->vmcb->control.pause_filter_count;
 +	nested_vmcb->control.pause_filter_thresh =
 +		svm->vmcb->control.pause_filter_thresh;
  
 -	reg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;
 -	if (svm->vmcb->control.exit_code == SVM_EXIT_CR0_SEL_WRITE)
 -		cr = SVM_EXIT_WRITE_CR0 - SVM_EXIT_READ_CR0;
 -	else
 -		cr = svm->vmcb->control.exit_code - SVM_EXIT_READ_CR0;
 +	/* We always set V_INTR_MASKING and remember the old value in hflags */
 +	if (!(svm->vcpu.arch.hflags & HF_VINTR_MASK))
 +		nested_vmcb->control.int_ctl &= ~V_INTR_MASKING_MASK;
  
 -	err = 0;
 -	if (cr >= 16) { /* mov to cr */
 -		cr -= 16;
 -		val = kvm_register_read(&svm->vcpu, reg);
 -		switch (cr) {
 -		case 0:
 -			if (!check_selective_cr0_intercepted(svm, val))
 -				err = kvm_set_cr0(&svm->vcpu, val);
 -			else
 -				return 1;
 +	/* Restore the original control entries */
 +	copy_vmcb_control_area(vmcb, hsave);
  
 -			break;
 -		case 3:
 -			err = kvm_set_cr3(&svm->vcpu, val);
 -			break;
 -		case 4:
 -			err = kvm_set_cr4(&svm->vcpu, val);
 -			break;
 -		case 8:
 -			err = kvm_set_cr8(&svm->vcpu, val);
 -			break;
 -		default:
 -			WARN(1, "unhandled write to CR%d", cr);
 -			kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 -			return 1;
 -		}
 -	} else { /* mov from cr */
 -		switch (cr) {
 -		case 0:
 -			val = kvm_read_cr0(&svm->vcpu);
 -			break;
 -		case 2:
 -			val = svm->vcpu.arch.cr2;
 -			break;
 -		case 3:
 -			val = kvm_read_cr3(&svm->vcpu);
 -			break;
 -		case 4:
 -			val = kvm_read_cr4(&svm->vcpu);
 -			break;
 -		case 8:
 -			val = kvm_get_cr8(&svm->vcpu);
 -			break;
 -		default:
 -			WARN(1, "unhandled read from CR%d", cr);
 -			kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 -			return 1;
 -		}
 -		kvm_register_write(&svm->vcpu, reg, val);
 +	svm->vcpu.arch.tsc_offset = svm->vmcb->control.tsc_offset;
 +	kvm_clear_exception_queue(&svm->vcpu);
 +	kvm_clear_interrupt_queue(&svm->vcpu);
 +
 +	svm->nested.nested_cr3 = 0;
 +
 +	/* Restore selected save entries */
 +	svm->vmcb->save.es = hsave->save.es;
 +	svm->vmcb->save.cs = hsave->save.cs;
 +	svm->vmcb->save.ss = hsave->save.ss;
 +	svm->vmcb->save.ds = hsave->save.ds;
 +	svm->vmcb->save.gdtr = hsave->save.gdtr;
 +	svm->vmcb->save.idtr = hsave->save.idtr;
 +	kvm_set_rflags(&svm->vcpu, hsave->save.rflags);
 +	svm_set_efer(&svm->vcpu, hsave->save.efer);
 +	svm_set_cr0(&svm->vcpu, hsave->save.cr0 | X86_CR0_PE);
 +	svm_set_cr4(&svm->vcpu, hsave->save.cr4);
 +	if (npt_enabled) {
 +		svm->vmcb->save.cr3 = hsave->save.cr3;
 +		svm->vcpu.arch.cr3 = hsave->save.cr3;
 +	} else {
 +		(void)kvm_set_cr3(&svm->vcpu, hsave->save.cr3);
  	}
 -	return kvm_complete_insn_gp(&svm->vcpu, err);
 +	kvm_rax_write(&svm->vcpu, hsave->save.rax);
 +	kvm_rsp_write(&svm->vcpu, hsave->save.rsp);
 +	kvm_rip_write(&svm->vcpu, hsave->save.rip);
 +	svm->vmcb->save.dr7 = 0;
 +	svm->vmcb->save.cpl = 0;
 +	svm->vmcb->control.exit_int_info = 0;
 +
 +	mark_all_dirty(svm->vmcb);
 +
 +	kvm_vcpu_unmap(&svm->vcpu, &map, true);
 +
 +	nested_svm_uninit_mmu_context(&svm->vcpu);
 +	kvm_mmu_reset_context(&svm->vcpu);
 +	kvm_mmu_load(&svm->vcpu);
 +
 +	/*
 +	 * Drop what we picked up for L2 via svm_complete_interrupts() so it
 +	 * doesn't end up in L1.
 +	 */
 +	svm->vcpu.arch.nmi_injected = false;
 +	kvm_clear_exception_queue(&svm->vcpu);
 +	kvm_clear_interrupt_queue(&svm->vcpu);
 +
 +	return 0;
  }
  
 -static int dr_interception(struct vcpu_svm *svm)
 +static bool nested_svm_vmrun_msrpm(struct vcpu_svm *svm)
  {
 -	int reg, dr;
 -	unsigned long val;
 +	/*
 +	 * This function merges the msr permission bitmaps of kvm and the
 +	 * nested vmcb. It is optimized in that it only merges the parts where
 +	 * the kvm msr permission bitmap may contain zero bits
 +	 */
 +	int i;
  
 -	if (svm->vcpu.guest_debug == 0) {
 -		/*
 -		 * No more DR vmexits; force a reload of the debug registers
 -		 * and reenter on this instruction.  The next vmexit will
 -		 * retrieve the full state of the debug registers.
 -		 */
 -		clr_dr_intercepts(svm);
 -		svm->vcpu.arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 -		return 1;
 -	}
 +	if (!(svm->nested.intercept & (1ULL << INTERCEPT_MSR_PROT)))
 +		return true;
  
 -	if (!boot_cpu_has(X86_FEATURE_DECODEASSISTS))
 -		return emulate_on_interception(svm);
 +	for (i = 0; i < MSRPM_OFFSETS; i++) {
 +		u32 value, p;
 +		u64 offset;
  
 -	reg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;
 -	dr = svm->vmcb->control.exit_code - SVM_EXIT_READ_DR0;
 +		if (msrpm_offsets[i] == 0xffffffff)
 +			break;
  
 -	if (dr >= 16) { /* mov to DRn */
 -		if (!kvm_require_dr(&svm->vcpu, dr - 16))
 -			return 1;
 -		val = kvm_register_read(&svm->vcpu, reg);
 -		kvm_set_dr(&svm->vcpu, dr - 16, val);
 -	} else {
 -		if (!kvm_require_dr(&svm->vcpu, dr))
 -			return 1;
 -		kvm_get_dr(&svm->vcpu, dr, &val);
 -		kvm_register_write(&svm->vcpu, reg, val);
 +		p      = msrpm_offsets[i];
 +		offset = svm->nested.vmcb_msrpm + (p * 4);
 +
 +		if (kvm_vcpu_read_guest(&svm->vcpu, offset, &value, 4))
 +			return false;
 +
 +		svm->nested.msrpm[p] = svm->msrpm[p] | value;
  	}
  
 -	return kvm_skip_emulated_instruction(&svm->vcpu);
 +	svm->vmcb->control.msrpm_base_pa = __sme_set(__pa(svm->nested.msrpm));
 +
 +	return true;
  }
  
 -static int cr8_write_interception(struct vcpu_svm *svm)
 +static bool nested_vmcb_checks(struct vmcb *vmcb)
  {
 -	struct kvm_run *kvm_run = svm->vcpu.run;
 -	int r;
 +	if ((vmcb->save.efer & EFER_SVME) == 0)
 +		return false;
  
 -	u8 cr8_prev = kvm_get_cr8(&svm->vcpu);
 -	/* instruction emulation calls kvm_set_cr8() */
 -	r = cr_interception(svm);
 -	if (lapic_in_kernel(&svm->vcpu))
 -		return r;
 -	if (cr8_prev <= kvm_get_cr8(&svm->vcpu))
 -		return r;
 -	kvm_run->exit_reason = KVM_EXIT_SET_TPR;
 -	return 0;
 -}
 +	if ((vmcb->control.intercept & (1ULL << INTERCEPT_VMRUN)) == 0)
 +		return false;
  
 -static int svm_get_msr_feature(struct kvm_msr_entry *msr)
 -{
 -	msr->data = 0;
 +	if (vmcb->control.asid == 0)
 +		return false;
  
 -	switch (msr->index) {
 -	case MSR_F10H_DECFG:
 -		if (boot_cpu_has(X86_FEATURE_LFENCE_RDTSC))
 -			msr->data |= MSR_F10H_DECFG_LFENCE_SERIALIZE;
 -		break;
 -	default:
 -		return KVM_MSR_RET_INVALID;
 -	}
 +	if ((vmcb->control.nested_ctl & SVM_NESTED_CTL_NP_ENABLE) &&
 +	    !npt_enabled)
 +		return false;
  
 -	return 0;
 +	return true;
  }
  
 -static int svm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 +static void enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb_gpa,
 +				 struct vmcb *nested_vmcb, struct kvm_host_map *map)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	bool evaluate_pending_interrupts =
 +		is_intercept(svm, INTERCEPT_VINTR) ||
 +		is_intercept(svm, INTERCEPT_IRET);
  
 -	switch (msr_info->index) {
 -	case MSR_STAR:
 -		msr_info->data = svm->vmcb->save.star;
 -		break;
 -#ifdef CONFIG_X86_64
 -	case MSR_LSTAR:
 -		msr_info->data = svm->vmcb->save.lstar;
 -		break;
 -	case MSR_CSTAR:
 -		msr_info->data = svm->vmcb->save.cstar;
 -		break;
 -	case MSR_KERNEL_GS_BASE:
 -		msr_info->data = svm->vmcb->save.kernel_gs_base;
 -		break;
 -	case MSR_SYSCALL_MASK:
 -		msr_info->data = svm->vmcb->save.sfmask;
 -		break;
 -#endif
 -	case MSR_IA32_SYSENTER_CS:
 -		msr_info->data = svm->vmcb->save.sysenter_cs;
 -		break;
 -	case MSR_IA32_SYSENTER_EIP:
 -		msr_info->data = svm->sysenter_eip;
 -		break;
 -	case MSR_IA32_SYSENTER_ESP:
 -		msr_info->data = svm->sysenter_esp;
 -		break;
 -	case MSR_TSC_AUX:
 -		if (!boot_cpu_has(X86_FEATURE_RDTSCP))
 -			return 1;
 -		msr_info->data = svm->tsc_aux;
 -		break;
 -	/*
 -	 * Nobody will change the following 5 values in the VMCB so we can
 -	 * safely return them on rdmsr. They will always be 0 until LBRV is
 -	 * implemented.
 -	 */
 -	case MSR_IA32_DEBUGCTLMSR:
 -		msr_info->data = svm->vmcb->save.dbgctl;
 -		break;
 -	case MSR_IA32_LASTBRANCHFROMIP:
 -		msr_info->data = svm->vmcb->save.br_from;
 -		break;
 -	case MSR_IA32_LASTBRANCHTOIP:
 -		msr_info->data = svm->vmcb->save.br_to;
 -		break;
 -	case MSR_IA32_LASTINTFROMIP:
 -		msr_info->data = svm->vmcb->save.last_excp_from;
 -		break;
 -	case MSR_IA32_LASTINTTOIP:
 -		msr_info->data = svm->vmcb->save.last_excp_to;
 -		break;
 -	case MSR_VM_HSAVE_PA:
 -		msr_info->data = svm->nested.hsave_msr;
 -		break;
 -	case MSR_VM_CR:
 -		msr_info->data = svm->nested.vm_cr_msr;
 -		break;
 -	case MSR_IA32_SPEC_CTRL:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL) &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_STIBP) &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBRS) &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_SSBD))
 -			return 1;
 +	if (kvm_get_rflags(&svm->vcpu) & X86_EFLAGS_IF)
 +		svm->vcpu.arch.hflags |= HF_HIF_MASK;
 +	else
 +		svm->vcpu.arch.hflags &= ~HF_HIF_MASK;
  
 -		msr_info->data = svm->spec_ctrl;
 -		break;
 -	case MSR_AMD64_VIRT_SPEC_CTRL:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))
 -			return 1;
 +	if (nested_vmcb->control.nested_ctl & SVM_NESTED_CTL_NP_ENABLE) {
 +		svm->nested.nested_cr3 = nested_vmcb->control.nested_cr3;
 +		nested_svm_init_mmu_context(&svm->vcpu);
 +	}
  
 -		msr_info->data = svm->virt_spec_ctrl;
 -		break;
 -	case MSR_F15H_IC_CFG: {
 +	/* Load the nested guest state */
 +	svm->vmcb->save.es = nested_vmcb->save.es;
 +	svm->vmcb->save.cs = nested_vmcb->save.cs;
 +	svm->vmcb->save.ss = nested_vmcb->save.ss;
 +	svm->vmcb->save.ds = nested_vmcb->save.ds;
 +	svm->vmcb->save.gdtr = nested_vmcb->save.gdtr;
 +	svm->vmcb->save.idtr = nested_vmcb->save.idtr;
 +	kvm_set_rflags(&svm->vcpu, nested_vmcb->save.rflags);
 +	svm_set_efer(&svm->vcpu, nested_vmcb->save.efer);
 +	svm_set_cr0(&svm->vcpu, nested_vmcb->save.cr0);
 +	svm_set_cr4(&svm->vcpu, nested_vmcb->save.cr4);
 +	if (npt_enabled) {
 +		svm->vmcb->save.cr3 = nested_vmcb->save.cr3;
 +		svm->vcpu.arch.cr3 = nested_vmcb->save.cr3;
 +	} else
 +		(void)kvm_set_cr3(&svm->vcpu, nested_vmcb->save.cr3);
  
 -		int family, model;
 +	/* Guest paging mode is active - reset mmu */
 +	kvm_mmu_reset_context(&svm->vcpu);
  
 -		family = guest_cpuid_family(vcpu);
 -		model  = guest_cpuid_model(vcpu);
 +	svm->vmcb->save.cr2 = svm->vcpu.arch.cr2 = nested_vmcb->save.cr2;
 +	kvm_rax_write(&svm->vcpu, nested_vmcb->save.rax);
 +	kvm_rsp_write(&svm->vcpu, nested_vmcb->save.rsp);
 +	kvm_rip_write(&svm->vcpu, nested_vmcb->save.rip);
 +
 +	/* In case we don't even reach vcpu_run, the fields are not updated */
 +	svm->vmcb->save.rax = nested_vmcb->save.rax;
 +	svm->vmcb->save.rsp = nested_vmcb->save.rsp;
 +	svm->vmcb->save.rip = nested_vmcb->save.rip;
 +	svm->vmcb->save.dr7 = nested_vmcb->save.dr7;
 +	svm->vmcb->save.dr6 = nested_vmcb->save.dr6;
 +	svm->vmcb->save.cpl = nested_vmcb->save.cpl;
 +
 +	svm->nested.vmcb_msrpm = nested_vmcb->control.msrpm_base_pa & ~0x0fffULL;
 +	svm->nested.vmcb_iopm  = nested_vmcb->control.iopm_base_pa  & ~0x0fffULL;
 +
 +	/* cache intercepts */
 +	svm->nested.intercept_cr         = nested_vmcb->control.intercept_cr;
 +	svm->nested.intercept_dr         = nested_vmcb->control.intercept_dr;
 +	svm->nested.intercept_exceptions = nested_vmcb->control.intercept_exceptions;
 +	svm->nested.intercept            = nested_vmcb->control.intercept;
 +
 +	svm_flush_tlb(&svm->vcpu, true);
 +	svm->vmcb->control.int_ctl = nested_vmcb->control.int_ctl | V_INTR_MASKING_MASK;
 +	if (nested_vmcb->control.int_ctl & V_INTR_MASKING_MASK)
 +		svm->vcpu.arch.hflags |= HF_VINTR_MASK;
 +	else
 +		svm->vcpu.arch.hflags &= ~HF_VINTR_MASK;
  
 -		if (family < 0 || model < 0)
 -			return kvm_get_msr_common(vcpu, msr_info);
 +	svm->vcpu.arch.tsc_offset += nested_vmcb->control.tsc_offset;
 +	svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
  
 -		msr_info->data = 0;
 +	svm->vmcb->control.virt_ext = nested_vmcb->control.virt_ext;
 +	svm->vmcb->control.int_vector = nested_vmcb->control.int_vector;
 +	svm->vmcb->control.int_state = nested_vmcb->control.int_state;
 +	svm->vmcb->control.event_inj = nested_vmcb->control.event_inj;
 +	svm->vmcb->control.event_inj_err = nested_vmcb->control.event_inj_err;
  
 -		if (family == 0x15 &&
 -		    (model >= 0x2 && model < 0x20))
 -			msr_info->data = 0x1E;
 -		}
 -		break;
 -	case MSR_F10H_DECFG:
 -		msr_info->data = svm->msr_decfg;
 -		break;
 -	default:
 -		return kvm_get_msr_common(vcpu, msr_info);
 -	}
 -	return 0;
 -}
 -
 -static int rdmsr_interception(struct vcpu_svm *svm)
 -{
 -	return kvm_emulate_rdmsr(&svm->vcpu);
 -}
 +	svm->vmcb->control.pause_filter_count =
 +		nested_vmcb->control.pause_filter_count;
 +	svm->vmcb->control.pause_filter_thresh =
 +		nested_vmcb->control.pause_filter_thresh;
  
 -static int svm_set_vm_cr(struct kvm_vcpu *vcpu, u64 data)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	int svm_dis, chg_mask;
 -
 -	if (data & ~SVM_VM_CR_VALID_MASK)
 -		return 1;
 -
 -	chg_mask = SVM_VM_CR_VALID_MASK;
 +	kvm_vcpu_unmap(&svm->vcpu, map, true);
  
 -	if (svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK)
 -		chg_mask &= ~(SVM_VM_CR_SVM_LOCK_MASK | SVM_VM_CR_SVM_DIS_MASK);
 +	/* Enter Guest-Mode */
 +	enter_guest_mode(&svm->vcpu);
  
 -	svm->nested.vm_cr_msr &= ~chg_mask;
 -	svm->nested.vm_cr_msr |= (data & chg_mask);
 +	/*
 +	 * Merge guest and host intercepts - must be called  with vcpu in
 +	 * guest-mode to take affect here
 +	 */
 +	recalc_intercepts(svm);
  
 -	svm_dis = svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK;
 +	svm->nested.vmcb = vmcb_gpa;
  
 -	/* check for svm_disable while efer.svme is set */
 -	if (svm_dis && (vcpu->arch.efer & EFER_SVME))
 -		return 1;
 +	/*
 +	 * If L1 had a pending IRQ/NMI before executing VMRUN,
 +	 * which wasn't delivered because it was disallowed (e.g.
 +	 * interrupts disabled), L0 needs to evaluate if this pending
 +	 * event should cause an exit from L2 to L1 or be delivered
 +	 * directly to L2.
 +	 *
 +	 * Usually this would be handled by the processor noticing an
 +	 * IRQ/NMI window request.  However, VMRUN can unblock interrupts
 +	 * by implicitly setting GIF, so force L0 to perform pending event
 +	 * evaluation by requesting a KVM_REQ_EVENT.
 +	 */
 +	enable_gif(svm);
 +	if (unlikely(evaluate_pending_interrupts))
 +		kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
  
 -	return 0;
 +	mark_all_dirty(svm->vmcb);
  }
  
 -static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 +static int nested_svm_vmrun(struct vcpu_svm *svm)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	int ret;
 +	struct vmcb *nested_vmcb;
 +	struct vmcb *hsave = svm->nested.hsave;
 +	struct vmcb *vmcb = svm->vmcb;
 +	struct kvm_host_map map;
 +	u64 vmcb_gpa;
  
 -	u32 ecx = msr->index;
 -	u64 data = msr->data;
 -	switch (ecx) {
 -	case MSR_IA32_CR_PAT:
 -		if (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))
 -			return 1;
 -		vcpu->arch.pat = data;
 -		svm->vmcb->save.g_pat = data;
 -		vmcb_mark_dirty(svm->vmcb, VMCB_NPT);
 -		break;
 -	case MSR_IA32_SPEC_CTRL:
 -		if (!msr->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL) &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_STIBP) &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBRS) &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_SSBD))
 -			return 1;
 +	vmcb_gpa = svm->vmcb->save.rax;
  
 -		if (kvm_spec_ctrl_test_value(data))
 -			return 1;
 +	ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb_gpa), &map);
 +	if (ret == -EINVAL) {
 +		kvm_inject_gp(&svm->vcpu, 0);
 +		return 1;
 +	} else if (ret) {
 +		return kvm_skip_emulated_instruction(&svm->vcpu);
 +	}
  
 -		svm->spec_ctrl = data;
 -		if (!data)
 -			break;
 +	ret = kvm_skip_emulated_instruction(&svm->vcpu);
  
 -		/*
 -		 * For non-nested:
 -		 * When it's written (to non-zero) for the first time, pass
 -		 * it through.
 -		 *
 -		 * For nested:
 -		 * The handling of the MSR bitmap for L2 guests is done in
 -		 * nested_svm_vmrun_msrpm.
 -		 * We update the L1 MSR bit as well since it will end up
 -		 * touching the MSR anyway now.
 -		 */
 -		set_msr_interception(svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
 -		break;
 -	case MSR_IA32_PRED_CMD:
 -		if (!msr->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBPB))
 -			return 1;
 +	nested_vmcb = map.hva;
  
 -		if (data & ~PRED_CMD_IBPB)
 -			return 1;
 -		if (!boot_cpu_has(X86_FEATURE_AMD_IBPB))
 -			return 1;
 -		if (!data)
 -			break;
 +	if (!nested_vmcb_checks(nested_vmcb)) {
 +		nested_vmcb->control.exit_code    = SVM_EXIT_ERR;
 +		nested_vmcb->control.exit_code_hi = 0;
 +		nested_vmcb->control.exit_info_1  = 0;
 +		nested_vmcb->control.exit_info_2  = 0;
  
 -		wrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);
 -		set_msr_interception(svm->msrpm, MSR_IA32_PRED_CMD, 0, 1);
 -		break;
 -	case MSR_AMD64_VIRT_SPEC_CTRL:
 -		if (!msr->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))
 -			return 1;
 +		kvm_vcpu_unmap(&svm->vcpu, &map, true);
  
 -		if (data & ~SPEC_CTRL_SSBD)
 -			return 1;
 +		return ret;
 +	}
  
 -		svm->virt_spec_ctrl = data;
 -		break;
 -	case MSR_STAR:
 -		svm->vmcb->save.star = data;
 -		break;
 -#ifdef CONFIG_X86_64
 -	case MSR_LSTAR:
 -		svm->vmcb->save.lstar = data;
 -		break;
 -	case MSR_CSTAR:
 -		svm->vmcb->save.cstar = data;
 -		break;
 -	case MSR_KERNEL_GS_BASE:
 -		svm->vmcb->save.kernel_gs_base = data;
 -		break;
 -	case MSR_SYSCALL_MASK:
 -		svm->vmcb->save.sfmask = data;
 -		break;
 -#endif
 -	case MSR_IA32_SYSENTER_CS:
 -		svm->vmcb->save.sysenter_cs = data;
 -		break;
 -	case MSR_IA32_SYSENTER_EIP:
 -		svm->sysenter_eip = data;
 -		svm->vmcb->save.sysenter_eip = data;
 -		break;
 -	case MSR_IA32_SYSENTER_ESP:
 -		svm->sysenter_esp = data;
 -		svm->vmcb->save.sysenter_esp = data;
 -		break;
 -	case MSR_TSC_AUX:
 -		if (!boot_cpu_has(X86_FEATURE_RDTSCP))
 -			return 1;
 +	trace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb_gpa,
 +			       nested_vmcb->save.rip,
 +			       nested_vmcb->control.int_ctl,
 +			       nested_vmcb->control.event_inj,
 +			       nested_vmcb->control.nested_ctl);
  
 -		/*
 -		 * This is rare, so we update the MSR here instead of using
 -		 * direct_access_msrs.  Doing that would require a rdmsr in
 -		 * svm_vcpu_put.
 -		 */
 -		svm->tsc_aux = data;
 -		wrmsrl(MSR_TSC_AUX, svm->tsc_aux);
 -		break;
 -	case MSR_IA32_DEBUGCTLMSR:
 -		if (!boot_cpu_has(X86_FEATURE_LBRV)) {
 -			vcpu_unimpl(vcpu, "%s: MSR_IA32_DEBUGCTL 0x%llx, nop\n",
 -				    __func__, data);
 -			break;
 -		}
 -		if (data & DEBUGCTL_RESERVED_BITS)
 -			return 1;
 +	trace_kvm_nested_intercepts(nested_vmcb->control.intercept_cr & 0xffff,
 +				    nested_vmcb->control.intercept_cr >> 16,
 +				    nested_vmcb->control.intercept_exceptions,
 +				    nested_vmcb->control.intercept);
  
 -		svm->vmcb->save.dbgctl = data;
 -		vmcb_mark_dirty(svm->vmcb, VMCB_LBR);
 -		if (data & (1ULL<<0))
 -			svm_enable_lbrv(svm);
 -		else
 -			svm_disable_lbrv(svm);
 -		break;
 -	case MSR_VM_HSAVE_PA:
 -		svm->nested.hsave_msr = data;
 -		break;
 -	case MSR_VM_CR:
 -		return svm_set_vm_cr(vcpu, data);
 -	case MSR_VM_IGNNE:
 -		vcpu_unimpl(vcpu, "unimplemented wrmsr: 0x%x data 0x%llx\n", ecx, data);
 -		break;
 -	case MSR_F10H_DECFG: {
 -		struct kvm_msr_entry msr_entry;
 +	/* Clear internal status */
 +	kvm_clear_exception_queue(&svm->vcpu);
 +	kvm_clear_interrupt_queue(&svm->vcpu);
  
 -		msr_entry.index = msr->index;
 -		if (svm_get_msr_feature(&msr_entry))
 -			return 1;
 +	/*
 +	 * Save the old vmcb, so we don't need to pick what we save, but can
 +	 * restore everything when a VMEXIT occurs
 +	 */
 +	hsave->save.es     = vmcb->save.es;
 +	hsave->save.cs     = vmcb->save.cs;
 +	hsave->save.ss     = vmcb->save.ss;
 +	hsave->save.ds     = vmcb->save.ds;
 +	hsave->save.gdtr   = vmcb->save.gdtr;
 +	hsave->save.idtr   = vmcb->save.idtr;
 +	hsave->save.efer   = svm->vcpu.arch.efer;
 +	hsave->save.cr0    = kvm_read_cr0(&svm->vcpu);
 +	hsave->save.cr4    = svm->vcpu.arch.cr4;
 +	hsave->save.rflags = kvm_get_rflags(&svm->vcpu);
 +	hsave->save.rip    = kvm_rip_read(&svm->vcpu);
 +	hsave->save.rsp    = vmcb->save.rsp;
 +	hsave->save.rax    = vmcb->save.rax;
 +	if (npt_enabled)
 +		hsave->save.cr3    = vmcb->save.cr3;
 +	else
 +		hsave->save.cr3    = kvm_read_cr3(&svm->vcpu);
  
 -		/* Check the supported bits */
 -		if (data & ~msr_entry.data)
 -			return 1;
 +	copy_vmcb_control_area(hsave, vmcb);
  
 -		/* Don't allow the guest to change a bit, #GP */
 -		if (!msr->host_initiated && (data ^ msr_entry.data))
 -			return 1;
 +	enter_svm_guest_mode(svm, vmcb_gpa, nested_vmcb, &map);
  
 -		svm->msr_decfg = data;
 -		break;
 -	}
 -	case MSR_IA32_APICBASE:
 -		if (kvm_vcpu_apicv_active(vcpu))
 -			avic_update_vapic_bar(to_svm(vcpu), data);
 -		/* Fall through */
 -	default:
 -		return kvm_set_msr_common(vcpu, msr);
 +	if (!nested_svm_vmrun_msrpm(svm)) {
 +		svm->vmcb->control.exit_code    = SVM_EXIT_ERR;
 +		svm->vmcb->control.exit_code_hi = 0;
 +		svm->vmcb->control.exit_info_1  = 0;
 +		svm->vmcb->control.exit_info_2  = 0;
 +
 +		nested_svm_vmexit(svm);
  	}
 -	return 0;
 -}
  
 -static int wrmsr_interception(struct vcpu_svm *svm)
 -{
 -	return kvm_emulate_wrmsr(&svm->vcpu);
 +	return ret;
  }
  
 -static int msr_interception(struct vcpu_svm *svm)
 +static void nested_svm_vmloadsave(struct vmcb *from_vmcb, struct vmcb *to_vmcb)
  {
 -	if (svm->vmcb->control.exit_info_1)
 -		return wrmsr_interception(svm);
 -	else
 -		return rdmsr_interception(svm);
 +	to_vmcb->save.fs = from_vmcb->save.fs;
 +	to_vmcb->save.gs = from_vmcb->save.gs;
 +	to_vmcb->save.tr = from_vmcb->save.tr;
 +	to_vmcb->save.ldtr = from_vmcb->save.ldtr;
 +	to_vmcb->save.kernel_gs_base = from_vmcb->save.kernel_gs_base;
 +	to_vmcb->save.star = from_vmcb->save.star;
 +	to_vmcb->save.lstar = from_vmcb->save.lstar;
 +	to_vmcb->save.cstar = from_vmcb->save.cstar;
 +	to_vmcb->save.sfmask = from_vmcb->save.sfmask;
 +	to_vmcb->save.sysenter_cs = from_vmcb->save.sysenter_cs;
 +	to_vmcb->save.sysenter_esp = from_vmcb->save.sysenter_esp;
 +	to_vmcb->save.sysenter_eip = from_vmcb->save.sysenter_eip;
  }
  
 -static int interrupt_window_interception(struct vcpu_svm *svm)
 +static int vmload_interception(struct vcpu_svm *svm)
  {
 -	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 -	svm_clear_vintr(svm);
 +	struct vmcb *nested_vmcb;
 +	struct kvm_host_map map;
 +	int ret;
  
 -	/*
 -	 * For AVIC, the only reason to end up here is ExtINTs.
 -	 * In this case AVIC was temporarily disabled for
 -	 * requesting the IRQ window and we have to re-enable it.
 -	 */
 -	svm_toggle_avic_for_irq_window(&svm->vcpu, true);
 +	if (nested_svm_check_permissions(svm))
 +		return 1;
  
 -	++svm->vcpu.stat.irq_window_exits;
 -	return 1;
 -}
 +	ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);
 +	if (ret) {
 +		if (ret == -EINVAL)
 +			kvm_inject_gp(&svm->vcpu, 0);
 +		return 1;
 +	}
  
 -static int pause_interception(struct vcpu_svm *svm)
 -{
 -	struct kvm_vcpu *vcpu = &svm->vcpu;
 -	bool in_kernel = (svm_get_cpl(vcpu) == 0);
 +	nested_vmcb = map.hva;
  
 -	if (pause_filter_thresh)
 -		grow_ple_window(vcpu);
 +	ret = kvm_skip_emulated_instruction(&svm->vcpu);
  
 -	kvm_vcpu_on_spin(vcpu, in_kernel);
 -	return 1;
 -}
 +	nested_svm_vmloadsave(nested_vmcb, svm->vmcb);
 +	kvm_vcpu_unmap(&svm->vcpu, &map, true);
  
 -static int nop_interception(struct vcpu_svm *svm)
 -{
 -	return kvm_skip_emulated_instruction(&(svm->vcpu));
 +	return ret;
  }
  
 -static int monitor_interception(struct vcpu_svm *svm)
 +static int vmsave_interception(struct vcpu_svm *svm)
  {
 -	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
 -	return nop_interception(svm);
 -}
 +	struct vmcb *nested_vmcb;
 +	struct kvm_host_map map;
 +	int ret;
  
 -static int mwait_interception(struct vcpu_svm *svm)
 -{
 -	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 -	return nop_interception(svm);
 +	if (nested_svm_check_permissions(svm))
 +		return 1;
 +
 +	ret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);
 +	if (ret) {
 +		if (ret == -EINVAL)
 +			kvm_inject_gp(&svm->vcpu, 0);
 +		return 1;
 +	}
 +
 +	nested_vmcb = map.hva;
 +
 +	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 +
 +	nested_svm_vmloadsave(svm->vmcb, nested_vmcb);
 +	kvm_vcpu_unmap(&svm->vcpu, &map, true);
 +
 +	return ret;
 +}
 +
 +static int vmrun_interception(struct vcpu_svm *svm)
 +{
 +	if (nested_svm_check_permissions(svm))
 +		return 1;
 +
 +	return nested_svm_vmrun(svm);
 +}
 +
 +static int stgi_interception(struct vcpu_svm *svm)
 +{
 +	int ret;
 +
 +	if (nested_svm_check_permissions(svm))
 +		return 1;
 +
 +	/*
 +	 * If VGIF is enabled, the STGI intercept is only added to
 +	 * detect the opening of the SMI/NMI window; remove it now.
 +	 */
 +	if (vgif_enabled(svm))
 +		clr_intercept(svm, INTERCEPT_STGI);
 +
 +	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +
 +	enable_gif(svm);
 +
 +	return ret;
 +}
 +
 +static int clgi_interception(struct vcpu_svm *svm)
 +{
 +	int ret;
 +
 +	if (nested_svm_check_permissions(svm))
 +		return 1;
 +
 +	ret = kvm_skip_emulated_instruction(&svm->vcpu);
 +
 +	disable_gif(svm);
 +
 +	/* After a CLGI no interrupts should come */
 +	svm_clear_vintr(svm);
 +
 +	return ret;
 +}
 +
 +static int invlpga_interception(struct vcpu_svm *svm)
 +{
 +	struct kvm_vcpu *vcpu = &svm->vcpu;
 +
 +	trace_kvm_invlpga(svm->vmcb->save.rip, kvm_rcx_read(&svm->vcpu),
 +			  kvm_rax_read(&svm->vcpu));
 +
 +	/* Let's treat INVLPGA the same as INVLPG (can be optimized!) */
 +	kvm_mmu_invlpg(vcpu, kvm_rax_read(&svm->vcpu));
 +
 +	return kvm_skip_emulated_instruction(&svm->vcpu);
 +}
 +
 +static int skinit_interception(struct vcpu_svm *svm)
 +{
 +	trace_kvm_skinit(svm->vmcb->save.rip, kvm_rax_read(&svm->vcpu));
 +
 +	kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 +	return 1;
 +}
 +
 +static int wbinvd_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_wbinvd(&svm->vcpu);
 +}
 +
 +static int xsetbv_interception(struct vcpu_svm *svm)
 +{
 +	u64 new_bv = kvm_read_edx_eax(&svm->vcpu);
 +	u32 index = kvm_rcx_read(&svm->vcpu);
 +
 +	if (kvm_set_xcr(&svm->vcpu, index, new_bv) == 0) {
 +		return kvm_skip_emulated_instruction(&svm->vcpu);
 +	}
 +
 +	return 1;
 +}
 +
 +static int rdpru_interception(struct vcpu_svm *svm)
 +{
 +	kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 +	return 1;
 +}
 +
 +static int task_switch_interception(struct vcpu_svm *svm)
 +{
 +	u16 tss_selector;
 +	int reason;
 +	int int_type = svm->vmcb->control.exit_int_info &
 +		SVM_EXITINTINFO_TYPE_MASK;
 +	int int_vec = svm->vmcb->control.exit_int_info & SVM_EVTINJ_VEC_MASK;
 +	uint32_t type =
 +		svm->vmcb->control.exit_int_info & SVM_EXITINTINFO_TYPE_MASK;
 +	uint32_t idt_v =
 +		svm->vmcb->control.exit_int_info & SVM_EXITINTINFO_VALID;
 +	bool has_error_code = false;
 +	u32 error_code = 0;
 +
 +	tss_selector = (u16)svm->vmcb->control.exit_info_1;
 +
 +	if (svm->vmcb->control.exit_info_2 &
 +	    (1ULL << SVM_EXITINFOSHIFT_TS_REASON_IRET))
 +		reason = TASK_SWITCH_IRET;
 +	else if (svm->vmcb->control.exit_info_2 &
 +		 (1ULL << SVM_EXITINFOSHIFT_TS_REASON_JMP))
 +		reason = TASK_SWITCH_JMP;
 +	else if (idt_v)
 +		reason = TASK_SWITCH_GATE;
 +	else
 +		reason = TASK_SWITCH_CALL;
 +
 +	if (reason == TASK_SWITCH_GATE) {
 +		switch (type) {
 +		case SVM_EXITINTINFO_TYPE_NMI:
 +			svm->vcpu.arch.nmi_injected = false;
 +			break;
 +		case SVM_EXITINTINFO_TYPE_EXEPT:
 +			if (svm->vmcb->control.exit_info_2 &
 +			    (1ULL << SVM_EXITINFOSHIFT_TS_HAS_ERROR_CODE)) {
 +				has_error_code = true;
 +				error_code =
 +					(u32)svm->vmcb->control.exit_info_2;
 +			}
 +			kvm_clear_exception_queue(&svm->vcpu);
 +			break;
 +		case SVM_EXITINTINFO_TYPE_INTR:
 +			kvm_clear_interrupt_queue(&svm->vcpu);
 +			break;
 +		default:
 +			break;
 +		}
 +	}
 +
 +	if (reason != TASK_SWITCH_GATE ||
 +	    int_type == SVM_EXITINTINFO_TYPE_SOFT ||
 +	    (int_type == SVM_EXITINTINFO_TYPE_EXEPT &&
 +	     (int_vec == OF_VECTOR || int_vec == BP_VECTOR))) {
 +		if (!skip_emulated_instruction(&svm->vcpu))
 +			return 0;
 +	}
 +
 +	if (int_type != SVM_EXITINTINFO_TYPE_SOFT)
 +		int_vec = -1;
 +
 +	return kvm_task_switch(&svm->vcpu, tss_selector, int_vec, reason,
 +			       has_error_code, error_code);
 +}
 +
 +static int cpuid_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_cpuid(&svm->vcpu);
 +}
 +
 +static int iret_interception(struct vcpu_svm *svm)
 +{
 +	++svm->vcpu.stat.nmi_window_exits;
 +	clr_intercept(svm, INTERCEPT_IRET);
 +	svm->vcpu.arch.hflags |= HF_IRET_MASK;
 +	svm->nmi_iret_rip = kvm_rip_read(&svm->vcpu);
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +	return 1;
 +}
 +
 +static int invlpg_interception(struct vcpu_svm *svm)
 +{
 +	if (!static_cpu_has(X86_FEATURE_DECODEASSISTS))
 +		return kvm_emulate_instruction(&svm->vcpu, 0);
 +
 +	kvm_mmu_invlpg(&svm->vcpu, svm->vmcb->control.exit_info_1);
 +	return kvm_skip_emulated_instruction(&svm->vcpu);
 +}
 +
 +static int emulate_on_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_instruction(&svm->vcpu, 0);
 +}
 +
 +static int rsm_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_instruction_from_buffer(&svm->vcpu, rsm_ins_bytes, 2);
 +}
 +
 +static int rdpmc_interception(struct vcpu_svm *svm)
 +{
 +	int err;
 +
 +	if (!nrips)
 +		return emulate_on_interception(svm);
 +
 +	err = kvm_rdpmc(&svm->vcpu);
 +	return kvm_complete_insn_gp(&svm->vcpu, err);
 +}
 +
 +static bool check_selective_cr0_intercepted(struct vcpu_svm *svm,
 +					    unsigned long val)
 +{
 +	unsigned long cr0 = svm->vcpu.arch.cr0;
 +	bool ret = false;
 +	u64 intercept;
 +
 +	intercept = svm->nested.intercept;
 +
 +	if (!is_guest_mode(&svm->vcpu) ||
 +	    (!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0))))
 +		return false;
 +
 +	cr0 &= ~SVM_CR0_SELECTIVE_MASK;
 +	val &= ~SVM_CR0_SELECTIVE_MASK;
 +
 +	if (cr0 ^ val) {
 +		svm->vmcb->control.exit_code = SVM_EXIT_CR0_SEL_WRITE;
 +		ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
 +	}
 +
 +	return ret;
 +}
 +
 +#define CR_VALID (1ULL << 63)
 +
 +static int cr_interception(struct vcpu_svm *svm)
 +{
 +	int reg, cr;
 +	unsigned long val;
 +	int err;
 +
 +	if (!static_cpu_has(X86_FEATURE_DECODEASSISTS))
 +		return emulate_on_interception(svm);
 +
 +	if (unlikely((svm->vmcb->control.exit_info_1 & CR_VALID) == 0))
 +		return emulate_on_interception(svm);
 +
 +	reg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;
 +	if (svm->vmcb->control.exit_code == SVM_EXIT_CR0_SEL_WRITE)
 +		cr = SVM_EXIT_WRITE_CR0 - SVM_EXIT_READ_CR0;
 +	else
 +		cr = svm->vmcb->control.exit_code - SVM_EXIT_READ_CR0;
 +
 +	err = 0;
 +	if (cr >= 16) { /* mov to cr */
 +		cr -= 16;
 +		val = kvm_register_read(&svm->vcpu, reg);
 +		switch (cr) {
 +		case 0:
 +			if (!check_selective_cr0_intercepted(svm, val))
 +				err = kvm_set_cr0(&svm->vcpu, val);
 +			else
 +				return 1;
 +
 +			break;
 +		case 3:
 +			err = kvm_set_cr3(&svm->vcpu, val);
 +			break;
 +		case 4:
 +			err = kvm_set_cr4(&svm->vcpu, val);
 +			break;
 +		case 8:
 +			err = kvm_set_cr8(&svm->vcpu, val);
 +			break;
 +		default:
 +			WARN(1, "unhandled write to CR%d", cr);
 +			kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 +			return 1;
 +		}
 +	} else { /* mov from cr */
 +		switch (cr) {
 +		case 0:
 +			val = kvm_read_cr0(&svm->vcpu);
 +			break;
 +		case 2:
 +			val = svm->vcpu.arch.cr2;
 +			break;
 +		case 3:
 +			val = kvm_read_cr3(&svm->vcpu);
 +			break;
 +		case 4:
 +			val = kvm_read_cr4(&svm->vcpu);
 +			break;
 +		case 8:
 +			val = kvm_get_cr8(&svm->vcpu);
 +			break;
 +		default:
 +			WARN(1, "unhandled read from CR%d", cr);
 +			kvm_queue_exception(&svm->vcpu, UD_VECTOR);
 +			return 1;
 +		}
 +		kvm_register_write(&svm->vcpu, reg, val);
 +	}
 +	return kvm_complete_insn_gp(&svm->vcpu, err);
 +}
 +
 +static int dr_interception(struct vcpu_svm *svm)
 +{
 +	int reg, dr;
 +	unsigned long val;
 +
 +	if (svm->vcpu.guest_debug == 0) {
 +		/*
 +		 * No more DR vmexits; force a reload of the debug registers
 +		 * and reenter on this instruction.  The next vmexit will
 +		 * retrieve the full state of the debug registers.
 +		 */
 +		clr_dr_intercepts(svm);
 +		svm->vcpu.arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
 +		return 1;
 +	}
 +
 +	if (!boot_cpu_has(X86_FEATURE_DECODEASSISTS))
 +		return emulate_on_interception(svm);
 +
 +	reg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;
 +	dr = svm->vmcb->control.exit_code - SVM_EXIT_READ_DR0;
 +
 +	if (dr >= 16) { /* mov to DRn */
 +		if (!kvm_require_dr(&svm->vcpu, dr - 16))
 +			return 1;
 +		val = kvm_register_read(&svm->vcpu, reg);
 +		kvm_set_dr(&svm->vcpu, dr - 16, val);
 +	} else {
 +		if (!kvm_require_dr(&svm->vcpu, dr))
 +			return 1;
 +		kvm_get_dr(&svm->vcpu, dr, &val);
 +		kvm_register_write(&svm->vcpu, reg, val);
 +	}
 +
 +	return kvm_skip_emulated_instruction(&svm->vcpu);
 +}
 +
 +static int cr8_write_interception(struct vcpu_svm *svm)
 +{
 +	struct kvm_run *kvm_run = svm->vcpu.run;
 +	int r;
 +
 +	u8 cr8_prev = kvm_get_cr8(&svm->vcpu);
 +	/* instruction emulation calls kvm_set_cr8() */
 +	r = cr_interception(svm);
 +	if (lapic_in_kernel(&svm->vcpu))
 +		return r;
 +	if (cr8_prev <= kvm_get_cr8(&svm->vcpu))
 +		return r;
 +	kvm_run->exit_reason = KVM_EXIT_SET_TPR;
 +	return 0;
 +}
 +
 +static int svm_get_msr_feature(struct kvm_msr_entry *msr)
 +{
 +	msr->data = 0;
 +
 +	switch (msr->index) {
 +	case MSR_F10H_DECFG:
 +		if (boot_cpu_has(X86_FEATURE_LFENCE_RDTSC))
 +			msr->data |= MSR_F10H_DECFG_LFENCE_SERIALIZE;
 +		break;
 +	default:
 +		return 1;
 +	}
 +
 +	return 0;
 +}
 +
 +static int svm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	switch (msr_info->index) {
 +	case MSR_STAR:
 +		msr_info->data = svm->vmcb->save.star;
 +		break;
 +#ifdef CONFIG_X86_64
 +	case MSR_LSTAR:
 +		msr_info->data = svm->vmcb->save.lstar;
 +		break;
 +	case MSR_CSTAR:
 +		msr_info->data = svm->vmcb->save.cstar;
 +		break;
 +	case MSR_KERNEL_GS_BASE:
 +		msr_info->data = svm->vmcb->save.kernel_gs_base;
 +		break;
 +	case MSR_SYSCALL_MASK:
 +		msr_info->data = svm->vmcb->save.sfmask;
 +		break;
 +#endif
 +	case MSR_IA32_SYSENTER_CS:
 +		msr_info->data = svm->vmcb->save.sysenter_cs;
 +		break;
 +	case MSR_IA32_SYSENTER_EIP:
 +		msr_info->data = svm->sysenter_eip;
 +		break;
 +	case MSR_IA32_SYSENTER_ESP:
 +		msr_info->data = svm->sysenter_esp;
 +		break;
 +	case MSR_TSC_AUX:
 +		if (!boot_cpu_has(X86_FEATURE_RDTSCP))
 +			return 1;
 +		msr_info->data = svm->tsc_aux;
 +		break;
 +	/*
 +	 * Nobody will change the following 5 values in the VMCB so we can
 +	 * safely return them on rdmsr. They will always be 0 until LBRV is
 +	 * implemented.
 +	 */
 +	case MSR_IA32_DEBUGCTLMSR:
 +		msr_info->data = svm->vmcb->save.dbgctl;
 +		break;
 +	case MSR_IA32_LASTBRANCHFROMIP:
 +		msr_info->data = svm->vmcb->save.br_from;
 +		break;
 +	case MSR_IA32_LASTBRANCHTOIP:
 +		msr_info->data = svm->vmcb->save.br_to;
 +		break;
 +	case MSR_IA32_LASTINTFROMIP:
 +		msr_info->data = svm->vmcb->save.last_excp_from;
 +		break;
 +	case MSR_IA32_LASTINTTOIP:
 +		msr_info->data = svm->vmcb->save.last_excp_to;
 +		break;
 +	case MSR_VM_HSAVE_PA:
 +		msr_info->data = svm->nested.hsave_msr;
 +		break;
 +	case MSR_VM_CR:
 +		msr_info->data = svm->nested.vm_cr_msr;
 +		break;
 +	case MSR_IA32_SPEC_CTRL:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL) &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_STIBP) &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBRS) &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_SSBD))
 +			return 1;
 +
 +		msr_info->data = svm->spec_ctrl;
 +		break;
 +	case MSR_AMD64_VIRT_SPEC_CTRL:
 +		if (!msr_info->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))
 +			return 1;
 +
 +		msr_info->data = svm->virt_spec_ctrl;
 +		break;
 +	case MSR_F15H_IC_CFG: {
 +
 +		int family, model;
 +
 +		family = guest_cpuid_family(vcpu);
 +		model  = guest_cpuid_model(vcpu);
 +
 +		if (family < 0 || model < 0)
 +			return kvm_get_msr_common(vcpu, msr_info);
 +
 +		msr_info->data = 0;
 +
 +		if (family == 0x15 &&
 +		    (model >= 0x2 && model < 0x20))
 +			msr_info->data = 0x1E;
 +		}
 +		break;
 +	case MSR_F10H_DECFG:
 +		msr_info->data = svm->msr_decfg;
 +		break;
 +	default:
 +		return kvm_get_msr_common(vcpu, msr_info);
 +	}
 +	return 0;
 +}
 +
 +static int rdmsr_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_rdmsr(&svm->vcpu);
 +}
 +
 +static int svm_set_vm_cr(struct kvm_vcpu *vcpu, u64 data)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	int svm_dis, chg_mask;
 +
 +	if (data & ~SVM_VM_CR_VALID_MASK)
 +		return 1;
 +
 +	chg_mask = SVM_VM_CR_VALID_MASK;
 +
 +	if (svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK)
 +		chg_mask &= ~(SVM_VM_CR_SVM_LOCK_MASK | SVM_VM_CR_SVM_DIS_MASK);
 +
 +	svm->nested.vm_cr_msr &= ~chg_mask;
 +	svm->nested.vm_cr_msr |= (data & chg_mask);
 +
 +	svm_dis = svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK;
 +
 +	/* check for svm_disable while efer.svme is set */
 +	if (svm_dis && (vcpu->arch.efer & EFER_SVME))
 +		return 1;
 +
 +	return 0;
 +}
 +
 +static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	u32 ecx = msr->index;
 +	u64 data = msr->data;
 +	switch (ecx) {
 +	case MSR_IA32_CR_PAT:
 +		if (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))
 +			return 1;
 +		vcpu->arch.pat = data;
 +		svm->vmcb->save.g_pat = data;
 +		mark_dirty(svm->vmcb, VMCB_NPT);
 +		break;
 +	case MSR_IA32_SPEC_CTRL:
 +		if (!msr->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL) &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_STIBP) &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBRS) &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_SSBD))
 +			return 1;
 +
 +		if (data & ~kvm_spec_ctrl_valid_bits(vcpu))
 +			return 1;
 +
 +		svm->spec_ctrl = data;
 +		if (!data)
 +			break;
 +
 +		/*
 +		 * For non-nested:
 +		 * When it's written (to non-zero) for the first time, pass
 +		 * it through.
 +		 *
 +		 * For nested:
 +		 * The handling of the MSR bitmap for L2 guests is done in
 +		 * nested_svm_vmrun_msrpm.
 +		 * We update the L1 MSR bit as well since it will end up
 +		 * touching the MSR anyway now.
 +		 */
 +		set_msr_interception(svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
 +		break;
 +	case MSR_IA32_PRED_CMD:
 +		if (!msr->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBPB))
 +			return 1;
 +
 +		if (data & ~PRED_CMD_IBPB)
 +			return 1;
 +		if (!boot_cpu_has(X86_FEATURE_AMD_IBPB))
 +			return 1;
 +		if (!data)
 +			break;
 +
 +		wrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);
 +		set_msr_interception(svm->msrpm, MSR_IA32_PRED_CMD, 0, 1);
 +		break;
 +	case MSR_AMD64_VIRT_SPEC_CTRL:
 +		if (!msr->host_initiated &&
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))
 +			return 1;
 +
 +		if (data & ~SPEC_CTRL_SSBD)
 +			return 1;
 +
 +		svm->virt_spec_ctrl = data;
 +		break;
 +	case MSR_STAR:
 +		svm->vmcb->save.star = data;
 +		break;
 +#ifdef CONFIG_X86_64
 +	case MSR_LSTAR:
 +		svm->vmcb->save.lstar = data;
 +		break;
 +	case MSR_CSTAR:
 +		svm->vmcb->save.cstar = data;
 +		break;
 +	case MSR_KERNEL_GS_BASE:
 +		svm->vmcb->save.kernel_gs_base = data;
 +		break;
 +	case MSR_SYSCALL_MASK:
 +		svm->vmcb->save.sfmask = data;
 +		break;
 +#endif
 +	case MSR_IA32_SYSENTER_CS:
 +		svm->vmcb->save.sysenter_cs = data;
 +		break;
 +	case MSR_IA32_SYSENTER_EIP:
 +		svm->sysenter_eip = data;
 +		svm->vmcb->save.sysenter_eip = data;
 +		break;
 +	case MSR_IA32_SYSENTER_ESP:
 +		svm->sysenter_esp = data;
 +		svm->vmcb->save.sysenter_esp = data;
 +		break;
 +	case MSR_TSC_AUX:
 +		if (!boot_cpu_has(X86_FEATURE_RDTSCP))
 +			return 1;
 +
 +		/*
 +		 * This is rare, so we update the MSR here instead of using
 +		 * direct_access_msrs.  Doing that would require a rdmsr in
 +		 * svm_vcpu_put.
 +		 */
 +		svm->tsc_aux = data;
 +		wrmsrl(MSR_TSC_AUX, svm->tsc_aux);
 +		break;
 +	case MSR_IA32_DEBUGCTLMSR:
 +		if (!boot_cpu_has(X86_FEATURE_LBRV)) {
 +			vcpu_unimpl(vcpu, "%s: MSR_IA32_DEBUGCTL 0x%llx, nop\n",
 +				    __func__, data);
 +			break;
 +		}
 +		if (data & DEBUGCTL_RESERVED_BITS)
 +			return 1;
 +
 +		svm->vmcb->save.dbgctl = data;
 +		mark_dirty(svm->vmcb, VMCB_LBR);
 +		if (data & (1ULL<<0))
 +			svm_enable_lbrv(svm);
 +		else
 +			svm_disable_lbrv(svm);
 +		break;
 +	case MSR_VM_HSAVE_PA:
 +		svm->nested.hsave_msr = data;
 +		break;
 +	case MSR_VM_CR:
 +		return svm_set_vm_cr(vcpu, data);
 +	case MSR_VM_IGNNE:
 +		vcpu_unimpl(vcpu, "unimplemented wrmsr: 0x%x data 0x%llx\n", ecx, data);
 +		break;
 +	case MSR_F10H_DECFG: {
 +		struct kvm_msr_entry msr_entry;
 +
 +		msr_entry.index = msr->index;
 +		if (svm_get_msr_feature(&msr_entry))
 +			return 1;
 +
 +		/* Check the supported bits */
 +		if (data & ~msr_entry.data)
 +			return 1;
 +
 +		/* Don't allow the guest to change a bit, #GP */
 +		if (!msr->host_initiated && (data ^ msr_entry.data))
 +			return 1;
 +
 +		svm->msr_decfg = data;
 +		break;
 +	}
 +	case MSR_IA32_APICBASE:
 +		if (kvm_vcpu_apicv_active(vcpu))
 +			avic_update_vapic_bar(to_svm(vcpu), data);
 +		/* Fall through */
 +	default:
 +		return kvm_set_msr_common(vcpu, msr);
 +	}
 +	return 0;
 +}
 +
 +static int wrmsr_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_emulate_wrmsr(&svm->vcpu);
 +}
 +
 +static int msr_interception(struct vcpu_svm *svm)
 +{
 +	if (svm->vmcb->control.exit_info_1)
 +		return wrmsr_interception(svm);
 +	else
 +		return rdmsr_interception(svm);
 +}
 +
 +static int interrupt_window_interception(struct vcpu_svm *svm)
 +{
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +	svm_clear_vintr(svm);
 +	svm->vmcb->control.int_ctl &= ~V_IRQ_MASK;
 +	mark_dirty(svm->vmcb, VMCB_INTR);
 +	++svm->vcpu.stat.irq_window_exits;
 +	return 1;
 +}
 +
 +static int pause_interception(struct vcpu_svm *svm)
 +{
 +	struct kvm_vcpu *vcpu = &svm->vcpu;
 +	bool in_kernel = (svm_get_cpl(vcpu) == 0);
 +
 +	if (pause_filter_thresh)
 +		grow_ple_window(vcpu);
 +
 +	kvm_vcpu_on_spin(vcpu, in_kernel);
 +	return 1;
 +}
 +
 +static int nop_interception(struct vcpu_svm *svm)
 +{
 +	return kvm_skip_emulated_instruction(&(svm->vcpu));
 +}
 +
 +static int monitor_interception(struct vcpu_svm *svm)
 +{
 +	printk_once(KERN_WARNING "kvm: MONITOR instruction emulated as NOP!\n");
 +	return nop_interception(svm);
 +}
 +
 +static int mwait_interception(struct vcpu_svm *svm)
 +{
 +	printk_once(KERN_WARNING "kvm: MWAIT instruction emulated as NOP!\n");
 +	return nop_interception(svm);
 +}
 +
 +enum avic_ipi_failure_cause {
 +	AVIC_IPI_FAILURE_INVALID_INT_TYPE,
 +	AVIC_IPI_FAILURE_TARGET_NOT_RUNNING,
 +	AVIC_IPI_FAILURE_INVALID_TARGET,
 +	AVIC_IPI_FAILURE_INVALID_BACKING_PAGE,
 +};
 +
 +static int avic_incomplete_ipi_interception(struct vcpu_svm *svm)
 +{
 +	u32 icrh = svm->vmcb->control.exit_info_1 >> 32;
 +	u32 icrl = svm->vmcb->control.exit_info_1;
 +	u32 id = svm->vmcb->control.exit_info_2 >> 32;
 +	u32 index = svm->vmcb->control.exit_info_2 & 0xFF;
 +	struct kvm_lapic *apic = svm->vcpu.arch.apic;
 +
 +	trace_kvm_avic_incomplete_ipi(svm->vcpu.vcpu_id, icrh, icrl, id, index);
 +
 +	switch (id) {
 +	case AVIC_IPI_FAILURE_INVALID_INT_TYPE:
 +		/*
 +		 * AVIC hardware handles the generation of
 +		 * IPIs when the specified Message Type is Fixed
 +		 * (also known as fixed delivery mode) and
 +		 * the Trigger Mode is edge-triggered. The hardware
 +		 * also supports self and broadcast delivery modes
 +		 * specified via the Destination Shorthand(DSH)
 +		 * field of the ICRL. Logical and physical APIC ID
 +		 * formats are supported. All other IPI types cause
 +		 * a #VMEXIT, which needs to emulated.
 +		 */
 +		kvm_lapic_reg_write(apic, APIC_ICR2, icrh);
 +		kvm_lapic_reg_write(apic, APIC_ICR, icrl);
 +		break;
 +	case AVIC_IPI_FAILURE_TARGET_NOT_RUNNING: {
 +		int i;
 +		struct kvm_vcpu *vcpu;
 +		struct kvm *kvm = svm->vcpu.kvm;
 +		struct kvm_lapic *apic = svm->vcpu.arch.apic;
 +
 +		/*
 +		 * At this point, we expect that the AVIC HW has already
 +		 * set the appropriate IRR bits on the valid target
 +		 * vcpus. So, we just need to kick the appropriate vcpu.
 +		 */
 +		kvm_for_each_vcpu(i, vcpu, kvm) {
 +			bool m = kvm_apic_match_dest(vcpu, apic,
 +						     icrl & APIC_SHORT_MASK,
 +						     GET_APIC_DEST_FIELD(icrh),
 +						     icrl & APIC_DEST_MASK);
 +
 +			if (m && !avic_vcpu_is_running(vcpu))
 +				kvm_vcpu_wake_up(vcpu);
 +		}
 +		break;
 +	}
 +	case AVIC_IPI_FAILURE_INVALID_TARGET:
 +		WARN_ONCE(1, "Invalid IPI target: index=%u, vcpu=%d, icr=%#0x:%#0x\n",
 +			  index, svm->vcpu.vcpu_id, icrh, icrl);
 +		break;
 +	case AVIC_IPI_FAILURE_INVALID_BACKING_PAGE:
 +		WARN_ONCE(1, "Invalid backing page\n");
 +		break;
 +	default:
 +		pr_err("Unknown IPI interception\n");
 +	}
 +
 +	return 1;
 +}
 +
 +static u32 *avic_get_logical_id_entry(struct kvm_vcpu *vcpu, u32 ldr, bool flat)
 +{
 +	struct kvm_svm *kvm_svm = to_kvm_svm(vcpu->kvm);
 +	int index;
 +	u32 *logical_apic_id_table;
 +	int dlid = GET_APIC_LOGICAL_ID(ldr);
 +
 +	if (!dlid)
 +		return NULL;
 +
 +	if (flat) { /* flat */
 +		index = ffs(dlid) - 1;
 +		if (index > 7)
 +			return NULL;
 +	} else { /* cluster */
 +		int cluster = (dlid & 0xf0) >> 4;
 +		int apic = ffs(dlid & 0x0f) - 1;
 +
 +		if ((apic < 0) || (apic > 7) ||
 +		    (cluster >= 0xf))
 +			return NULL;
 +		index = (cluster << 2) + apic;
 +	}
 +
 +	logical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);
 +
 +	return &logical_apic_id_table[index];
 +}
 +
 +static int avic_ldr_write(struct kvm_vcpu *vcpu, u8 g_physical_id, u32 ldr)
 +{
 +	bool flat;
 +	u32 *entry, new_entry;
 +
 +	flat = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR) == APIC_DFR_FLAT;
 +	entry = avic_get_logical_id_entry(vcpu, ldr, flat);
 +	if (!entry)
 +		return -EINVAL;
 +
 +	new_entry = READ_ONCE(*entry);
 +	new_entry &= ~AVIC_LOGICAL_ID_ENTRY_GUEST_PHYSICAL_ID_MASK;
 +	new_entry |= (g_physical_id & AVIC_LOGICAL_ID_ENTRY_GUEST_PHYSICAL_ID_MASK);
 +	new_entry |= AVIC_LOGICAL_ID_ENTRY_VALID_MASK;
 +	WRITE_ONCE(*entry, new_entry);
 +
 +	return 0;
 +}
 +
 +static void avic_invalidate_logical_id_entry(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	bool flat = svm->dfr_reg == APIC_DFR_FLAT;
 +	u32 *entry = avic_get_logical_id_entry(vcpu, svm->ldr_reg, flat);
 +
 +	if (entry)
 +		clear_bit(AVIC_LOGICAL_ID_ENTRY_VALID_BIT, (unsigned long *)entry);
 +}
 +
 +static int avic_handle_ldr_update(struct kvm_vcpu *vcpu)
 +{
 +	int ret = 0;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u32 ldr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LDR);
 +	u32 id = kvm_xapic_id(vcpu->arch.apic);
 +
 +	if (ldr == svm->ldr_reg)
 +		return 0;
 +
 +	avic_invalidate_logical_id_entry(vcpu);
 +
 +	if (ldr)
 +		ret = avic_ldr_write(vcpu, id, ldr);
 +
 +	if (!ret)
 +		svm->ldr_reg = ldr;
 +
 +	return ret;
 +}
 +
 +static int avic_handle_apic_id_update(struct kvm_vcpu *vcpu)
 +{
 +	u64 *old, *new;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u32 id = kvm_xapic_id(vcpu->arch.apic);
 +
 +	if (vcpu->vcpu_id == id)
 +		return 0;
 +
 +	old = avic_get_physical_id_entry(vcpu, vcpu->vcpu_id);
 +	new = avic_get_physical_id_entry(vcpu, id);
 +	if (!new || !old)
 +		return 1;
 +
 +	/* We need to move physical_id_entry to new offset */
 +	*new = *old;
 +	*old = 0ULL;
 +	to_svm(vcpu)->avic_physical_id_cache = new;
 +
 +	/*
 +	 * Also update the guest physical APIC ID in the logical
 +	 * APIC ID table entry if already setup the LDR.
 +	 */
 +	if (svm->ldr_reg)
 +		avic_handle_ldr_update(vcpu);
 +
 +	return 0;
 +}
 +
 +static void avic_handle_dfr_update(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u32 dfr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR);
 +
 +	if (svm->dfr_reg == dfr)
 +		return;
 +
 +	avic_invalidate_logical_id_entry(vcpu);
 +	svm->dfr_reg = dfr;
 +}
 +
 +static int avic_unaccel_trap_write(struct vcpu_svm *svm)
 +{
 +	struct kvm_lapic *apic = svm->vcpu.arch.apic;
 +	u32 offset = svm->vmcb->control.exit_info_1 &
 +				AVIC_UNACCEL_ACCESS_OFFSET_MASK;
 +
 +	switch (offset) {
 +	case APIC_ID:
 +		if (avic_handle_apic_id_update(&svm->vcpu))
 +			return 0;
 +		break;
 +	case APIC_LDR:
 +		if (avic_handle_ldr_update(&svm->vcpu))
 +			return 0;
 +		break;
 +	case APIC_DFR:
 +		avic_handle_dfr_update(&svm->vcpu);
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	kvm_lapic_reg_write(apic, offset, kvm_lapic_get_reg(apic, offset));
 +
 +	return 1;
 +}
 +
 +static bool is_avic_unaccelerated_access_trap(u32 offset)
 +{
 +	bool ret = false;
 +
 +	switch (offset) {
 +	case APIC_ID:
 +	case APIC_EOI:
 +	case APIC_RRR:
 +	case APIC_LDR:
 +	case APIC_DFR:
 +	case APIC_SPIV:
 +	case APIC_ESR:
 +	case APIC_ICR:
 +	case APIC_LVTT:
 +	case APIC_LVTTHMR:
 +	case APIC_LVTPC:
 +	case APIC_LVT0:
 +	case APIC_LVT1:
 +	case APIC_LVTERR:
 +	case APIC_TMICT:
 +	case APIC_TDCR:
 +		ret = true;
 +		break;
 +	default:
 +		break;
 +	}
 +	return ret;
 +}
 +
 +static int avic_unaccelerated_access_interception(struct vcpu_svm *svm)
 +{
 +	int ret = 0;
 +	u32 offset = svm->vmcb->control.exit_info_1 &
 +		     AVIC_UNACCEL_ACCESS_OFFSET_MASK;
 +	u32 vector = svm->vmcb->control.exit_info_2 &
 +		     AVIC_UNACCEL_ACCESS_VECTOR_MASK;
 +	bool write = (svm->vmcb->control.exit_info_1 >> 32) &
 +		     AVIC_UNACCEL_ACCESS_WRITE_MASK;
 +	bool trap = is_avic_unaccelerated_access_trap(offset);
 +
 +	trace_kvm_avic_unaccelerated_access(svm->vcpu.vcpu_id, offset,
 +					    trap, write, vector);
 +	if (trap) {
 +		/* Handling Trap */
 +		WARN_ONCE(!write, "svm: Handling trap read.\n");
 +		ret = avic_unaccel_trap_write(svm);
 +	} else {
 +		/* Handling Fault */
 +		ret = kvm_emulate_instruction(&svm->vcpu, 0);
 +	}
 +
 +	return ret;
 +}
 +
 +static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {
 +	[SVM_EXIT_READ_CR0]			= cr_interception,
 +	[SVM_EXIT_READ_CR3]			= cr_interception,
 +	[SVM_EXIT_READ_CR4]			= cr_interception,
 +	[SVM_EXIT_READ_CR8]			= cr_interception,
 +	[SVM_EXIT_CR0_SEL_WRITE]		= cr_interception,
 +	[SVM_EXIT_WRITE_CR0]			= cr_interception,
 +	[SVM_EXIT_WRITE_CR3]			= cr_interception,
 +	[SVM_EXIT_WRITE_CR4]			= cr_interception,
 +	[SVM_EXIT_WRITE_CR8]			= cr8_write_interception,
 +	[SVM_EXIT_READ_DR0]			= dr_interception,
 +	[SVM_EXIT_READ_DR1]			= dr_interception,
 +	[SVM_EXIT_READ_DR2]			= dr_interception,
 +	[SVM_EXIT_READ_DR3]			= dr_interception,
 +	[SVM_EXIT_READ_DR4]			= dr_interception,
 +	[SVM_EXIT_READ_DR5]			= dr_interception,
 +	[SVM_EXIT_READ_DR6]			= dr_interception,
 +	[SVM_EXIT_READ_DR7]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR0]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR1]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR2]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR3]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR4]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR5]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR6]			= dr_interception,
 +	[SVM_EXIT_WRITE_DR7]			= dr_interception,
 +	[SVM_EXIT_EXCP_BASE + DB_VECTOR]	= db_interception,
 +	[SVM_EXIT_EXCP_BASE + BP_VECTOR]	= bp_interception,
 +	[SVM_EXIT_EXCP_BASE + UD_VECTOR]	= ud_interception,
 +	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,
 +	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,
 +	[SVM_EXIT_EXCP_BASE + AC_VECTOR]	= ac_interception,
 +	[SVM_EXIT_EXCP_BASE + GP_VECTOR]	= gp_interception,
 +	[SVM_EXIT_INTR]				= intr_interception,
 +	[SVM_EXIT_NMI]				= nmi_interception,
 +	[SVM_EXIT_SMI]				= nop_on_interception,
 +	[SVM_EXIT_INIT]				= nop_on_interception,
 +	[SVM_EXIT_VINTR]			= interrupt_window_interception,
 +	[SVM_EXIT_RDPMC]			= rdpmc_interception,
 +	[SVM_EXIT_CPUID]			= cpuid_interception,
 +	[SVM_EXIT_IRET]                         = iret_interception,
 +	[SVM_EXIT_INVD]                         = emulate_on_interception,
 +	[SVM_EXIT_PAUSE]			= pause_interception,
 +	[SVM_EXIT_HLT]				= halt_interception,
 +	[SVM_EXIT_INVLPG]			= invlpg_interception,
 +	[SVM_EXIT_INVLPGA]			= invlpga_interception,
 +	[SVM_EXIT_IOIO]				= io_interception,
 +	[SVM_EXIT_MSR]				= msr_interception,
 +	[SVM_EXIT_TASK_SWITCH]			= task_switch_interception,
 +	[SVM_EXIT_SHUTDOWN]			= shutdown_interception,
 +	[SVM_EXIT_VMRUN]			= vmrun_interception,
 +	[SVM_EXIT_VMMCALL]			= vmmcall_interception,
 +	[SVM_EXIT_VMLOAD]			= vmload_interception,
 +	[SVM_EXIT_VMSAVE]			= vmsave_interception,
 +	[SVM_EXIT_STGI]				= stgi_interception,
 +	[SVM_EXIT_CLGI]				= clgi_interception,
 +	[SVM_EXIT_SKINIT]			= skinit_interception,
 +	[SVM_EXIT_WBINVD]                       = wbinvd_interception,
 +	[SVM_EXIT_MONITOR]			= monitor_interception,
 +	[SVM_EXIT_MWAIT]			= mwait_interception,
 +	[SVM_EXIT_XSETBV]			= xsetbv_interception,
 +	[SVM_EXIT_RDPRU]			= rdpru_interception,
 +	[SVM_EXIT_NPF]				= npf_interception,
 +	[SVM_EXIT_RSM]                          = rsm_interception,
 +	[SVM_EXIT_AVIC_INCOMPLETE_IPI]		= avic_incomplete_ipi_interception,
 +	[SVM_EXIT_AVIC_UNACCELERATED_ACCESS]	= avic_unaccelerated_access_interception,
 +};
 +
 +static void dump_vmcb(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb_control_area *control = &svm->vmcb->control;
 +	struct vmcb_save_area *save = &svm->vmcb->save;
 +
 +	if (!dump_invalid_vmcb) {
 +		pr_warn_ratelimited("set kvm_amd.dump_invalid_vmcb=1 to dump internal KVM state.\n");
 +		return;
 +	}
 +
 +	pr_err("VMCB Control Area:\n");
 +	pr_err("%-20s%04x\n", "cr_read:", control->intercept_cr & 0xffff);
 +	pr_err("%-20s%04x\n", "cr_write:", control->intercept_cr >> 16);
 +	pr_err("%-20s%04x\n", "dr_read:", control->intercept_dr & 0xffff);
 +	pr_err("%-20s%04x\n", "dr_write:", control->intercept_dr >> 16);
 +	pr_err("%-20s%08x\n", "exceptions:", control->intercept_exceptions);
 +	pr_err("%-20s%016llx\n", "intercepts:", control->intercept);
 +	pr_err("%-20s%d\n", "pause filter count:", control->pause_filter_count);
 +	pr_err("%-20s%d\n", "pause filter threshold:",
 +	       control->pause_filter_thresh);
 +	pr_err("%-20s%016llx\n", "iopm_base_pa:", control->iopm_base_pa);
 +	pr_err("%-20s%016llx\n", "msrpm_base_pa:", control->msrpm_base_pa);
 +	pr_err("%-20s%016llx\n", "tsc_offset:", control->tsc_offset);
 +	pr_err("%-20s%d\n", "asid:", control->asid);
 +	pr_err("%-20s%d\n", "tlb_ctl:", control->tlb_ctl);
 +	pr_err("%-20s%08x\n", "int_ctl:", control->int_ctl);
 +	pr_err("%-20s%08x\n", "int_vector:", control->int_vector);
 +	pr_err("%-20s%08x\n", "int_state:", control->int_state);
 +	pr_err("%-20s%08x\n", "exit_code:", control->exit_code);
 +	pr_err("%-20s%016llx\n", "exit_info1:", control->exit_info_1);
 +	pr_err("%-20s%016llx\n", "exit_info2:", control->exit_info_2);
 +	pr_err("%-20s%08x\n", "exit_int_info:", control->exit_int_info);
 +	pr_err("%-20s%08x\n", "exit_int_info_err:", control->exit_int_info_err);
 +	pr_err("%-20s%lld\n", "nested_ctl:", control->nested_ctl);
 +	pr_err("%-20s%016llx\n", "nested_cr3:", control->nested_cr3);
 +	pr_err("%-20s%016llx\n", "avic_vapic_bar:", control->avic_vapic_bar);
 +	pr_err("%-20s%08x\n", "event_inj:", control->event_inj);
 +	pr_err("%-20s%08x\n", "event_inj_err:", control->event_inj_err);
 +	pr_err("%-20s%lld\n", "virt_ext:", control->virt_ext);
 +	pr_err("%-20s%016llx\n", "next_rip:", control->next_rip);
 +	pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
 +	pr_err("%-20s%016llx\n", "avic_logical_id:", control->avic_logical_id);
 +	pr_err("%-20s%016llx\n", "avic_physical_id:", control->avic_physical_id);
 +	pr_err("VMCB State Save Area:\n");
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "es:",
 +	       save->es.selector, save->es.attrib,
 +	       save->es.limit, save->es.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "cs:",
 +	       save->cs.selector, save->cs.attrib,
 +	       save->cs.limit, save->cs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ss:",
 +	       save->ss.selector, save->ss.attrib,
 +	       save->ss.limit, save->ss.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ds:",
 +	       save->ds.selector, save->ds.attrib,
 +	       save->ds.limit, save->ds.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "fs:",
 +	       save->fs.selector, save->fs.attrib,
 +	       save->fs.limit, save->fs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "gs:",
 +	       save->gs.selector, save->gs.attrib,
 +	       save->gs.limit, save->gs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "gdtr:",
 +	       save->gdtr.selector, save->gdtr.attrib,
 +	       save->gdtr.limit, save->gdtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ldtr:",
 +	       save->ldtr.selector, save->ldtr.attrib,
 +	       save->ldtr.limit, save->ldtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "idtr:",
 +	       save->idtr.selector, save->idtr.attrib,
 +	       save->idtr.limit, save->idtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "tr:",
 +	       save->tr.selector, save->tr.attrib,
 +	       save->tr.limit, save->tr.base);
 +	pr_err("cpl:            %d                efer:         %016llx\n",
 +		save->cpl, save->efer);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cr0:", save->cr0, "cr2:", save->cr2);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cr3:", save->cr3, "cr4:", save->cr4);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "dr6:", save->dr6, "dr7:", save->dr7);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "rip:", save->rip, "rflags:", save->rflags);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "rsp:", save->rsp, "rax:", save->rax);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "star:", save->star, "lstar:", save->lstar);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cstar:", save->cstar, "sfmask:", save->sfmask);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "kernel_gs_base:", save->kernel_gs_base,
 +	       "sysenter_cs:", save->sysenter_cs);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "sysenter_esp:", save->sysenter_esp,
 +	       "sysenter_eip:", save->sysenter_eip);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "gpat:", save->g_pat, "dbgctl:", save->dbgctl);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "br_from:", save->br_from, "br_to:", save->br_to);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "excp_from:", save->last_excp_from,
 +	       "excp_to:", save->last_excp_to);
 +}
 +
 +static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +{
 +	struct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;
 +
 +	*info1 = control->exit_info_1;
 +	*info2 = control->exit_info_2;
 +}
 +
 +static int handle_exit(struct kvm_vcpu *vcpu,
 +	enum exit_fastpath_completion exit_fastpath)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct kvm_run *kvm_run = vcpu->run;
 +	u32 exit_code = svm->vmcb->control.exit_code;
 +
 +	trace_kvm_exit(exit_code, vcpu, KVM_ISA_SVM);
 +
 +	if (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))
 +		vcpu->arch.cr0 = svm->vmcb->save.cr0;
 +	if (npt_enabled)
 +		vcpu->arch.cr3 = svm->vmcb->save.cr3;
 +
 +	if (unlikely(svm->nested.exit_required)) {
 +		nested_svm_vmexit(svm);
 +		svm->nested.exit_required = false;
 +
 +		return 1;
 +	}
 +
 +	if (is_guest_mode(vcpu)) {
 +		int vmexit;
 +
 +		trace_kvm_nested_vmexit(svm->vmcb->save.rip, exit_code,
 +					svm->vmcb->control.exit_info_1,
 +					svm->vmcb->control.exit_info_2,
 +					svm->vmcb->control.exit_int_info,
 +					svm->vmcb->control.exit_int_info_err,
 +					KVM_ISA_SVM);
 +
 +		vmexit = nested_svm_exit_special(svm);
 +
 +		if (vmexit == NESTED_EXIT_CONTINUE)
 +			vmexit = nested_svm_exit_handled(svm);
 +
 +		if (vmexit == NESTED_EXIT_DONE)
 +			return 1;
 +	}
 +
 +	svm_complete_interrupts(svm);
 +
 +	if (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {
 +		kvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 +		kvm_run->fail_entry.hardware_entry_failure_reason
 +			= svm->vmcb->control.exit_code;
 +		dump_vmcb(vcpu);
 +		return 0;
 +	}
 +
 +	if (is_external_interrupt(svm->vmcb->control.exit_int_info) &&
 +	    exit_code != SVM_EXIT_EXCP_BASE + PF_VECTOR &&
 +	    exit_code != SVM_EXIT_NPF && exit_code != SVM_EXIT_TASK_SWITCH &&
 +	    exit_code != SVM_EXIT_INTR && exit_code != SVM_EXIT_NMI)
 +		printk(KERN_ERR "%s: unexpected exit_int_info 0x%x "
 +		       "exit_code 0x%x\n",
 +		       __func__, svm->vmcb->control.exit_int_info,
 +		       exit_code);
 +
 +	if (exit_fastpath == EXIT_FASTPATH_SKIP_EMUL_INS) {
 +		kvm_skip_emulated_instruction(vcpu);
 +		return 1;
 +	} else if (exit_code >= ARRAY_SIZE(svm_exit_handlers)
 +	    || !svm_exit_handlers[exit_code]) {
 +		vcpu_unimpl(vcpu, "svm: unexpected exit reason 0x%x\n", exit_code);
 +		dump_vmcb(vcpu);
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror =
 +			KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
 +		vcpu->run->internal.ndata = 1;
 +		vcpu->run->internal.data[0] = exit_code;
 +		return 0;
 +	}
 +
 +#ifdef CONFIG_RETPOLINE
 +	if (exit_code == SVM_EXIT_MSR)
 +		return msr_interception(svm);
 +	else if (exit_code == SVM_EXIT_VINTR)
 +		return interrupt_window_interception(svm);
 +	else if (exit_code == SVM_EXIT_INTR)
 +		return intr_interception(svm);
 +	else if (exit_code == SVM_EXIT_HLT)
 +		return halt_interception(svm);
 +	else if (exit_code == SVM_EXIT_NPF)
 +		return npf_interception(svm);
 +#endif
 +	return svm_exit_handlers[exit_code](svm);
 +}
 +
 +static void reload_tss(struct kvm_vcpu *vcpu)
 +{
 +	int cpu = raw_smp_processor_id();
 +
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +	sd->tss_desc->type = 9; /* available 32/64-bit TSS */
 +	load_TR_desc();
 +}
 +
 +static void pre_sev_run(struct vcpu_svm *svm, int cpu)
 +{
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +	int asid = sev_get_asid(svm->vcpu.kvm);
 +
 +	/* Assign the asid allocated with this SEV guest */
 +	svm->vmcb->control.asid = asid;
 +
 +	/*
 +	 * Flush guest TLB:
 +	 *
 +	 * 1) when different VMCB for the same ASID is to be run on the same host CPU.
 +	 * 2) or this VMCB was executed on different host CPU in previous VMRUNs.
 +	 */
 +	if (sd->sev_vmcbs[asid] == svm->vmcb &&
 +	    svm->last_cpu == cpu)
 +		return;
 +
 +	svm->last_cpu = cpu;
 +	sd->sev_vmcbs[asid] = svm->vmcb;
 +	svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 +	mark_dirty(svm->vmcb, VMCB_ASID);
 +}
 +
 +static void pre_svm_run(struct vcpu_svm *svm)
 +{
 +	int cpu = raw_smp_processor_id();
 +
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +
 +	if (sev_guest(svm->vcpu.kvm))
 +		return pre_sev_run(svm, cpu);
 +
 +	/* FIXME: handle wraparound of asid_generation */
 +	if (svm->asid_generation != sd->asid_generation)
 +		new_asid(svm, sd);
 +}
 +
 +static void svm_inject_nmi(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;
 +	vcpu->arch.hflags |= HF_NMI_MASK;
 +	set_intercept(svm, INTERCEPT_IRET);
 +	++vcpu->stat.nmi_injections;
 +}
 +
 +static void svm_set_irq(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	BUG_ON(!(gif_set(svm)));
 +
 +	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 +	++vcpu->stat.irq_injections;
 +
 +	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 +		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
 +}
 +
 +static inline bool svm_nested_virtualize_tpr(struct kvm_vcpu *vcpu)
 +{
 +	return is_guest_mode(vcpu) && (vcpu->arch.hflags & HF_VINTR_MASK);
 +}
 +
 +static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (svm_nested_virtualize_tpr(vcpu))
 +		return;
 +
 +	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +
 +	if (irr == -1)
 +		return;
 +
 +	if (tpr >= irr)
 +		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +}
 +
 +static void svm_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 +{
 +	return;
 +}
 +
 +static void svm_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 +{
 +}
 +
 +static void svm_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 +{
 +}
 +
 +static int svm_set_pi_irte_mode(struct kvm_vcpu *vcpu, bool activate)
 +{
 +	int ret = 0;
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *ir;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (!kvm_arch_has_assigned_device(vcpu->kvm))
 +		return 0;
 +
 +	/*
 +	 * Here, we go through the per-vcpu ir_list to update all existing
 +	 * interrupt remapping table entry targeting this vcpu.
 +	 */
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +
 +	if (list_empty(&svm->ir_list))
 +		goto out;
 +
 +	list_for_each_entry(ir, &svm->ir_list, node) {
 +		if (activate)
 +			ret = amd_iommu_activate_guest_mode(ir->data);
 +		else
 +			ret = amd_iommu_deactivate_guest_mode(ir->data);
 +		if (ret)
 +			break;
 +	}
 +out:
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +	return ret;
 +}
 +
 +static void svm_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	bool activated = kvm_vcpu_apicv_active(vcpu);
 +
 +	if (!avic)
 +		return;
 +
 +	if (activated) {
 +		/**
 +		 * During AVIC temporary deactivation, guest could update
 +		 * APIC ID, DFR and LDR registers, which would not be trapped
 +		 * by avic_unaccelerated_access_interception(). In this case,
 +		 * we need to check and update the AVIC logical APIC ID table
 +		 * accordingly before re-activating.
 +		 */
 +		avic_post_state_restore(vcpu);
 +		vmcb->control.int_ctl |= AVIC_ENABLE_MASK;
 +	} else {
 +		vmcb->control.int_ctl &= ~AVIC_ENABLE_MASK;
 +	}
 +	mark_dirty(vmcb, VMCB_AVIC);
 +
 +	svm_set_pi_irte_mode(vcpu, activated);
 +}
 +
 +static void svm_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 +{
 +	return;
 +}
 +
 +static int svm_deliver_avic_intr(struct kvm_vcpu *vcpu, int vec)
 +{
 +	if (!vcpu->arch.apicv_active)
 +		return -1;
 +
 +	kvm_lapic_set_irr(vec, vcpu->arch.apic);
 +	smp_mb__after_atomic();
 +
 +	if (avic_vcpu_is_running(vcpu)) {
 +		int cpuid = vcpu->cpu;
 +
 +		if (cpuid != get_cpu())
 +			wrmsrl(SVM_AVIC_DOORBELL, kvm_cpu_get_apicid(cpuid));
 +		put_cpu();
 +	} else
 +		kvm_vcpu_wake_up(vcpu);
 +
 +	return 0;
 +}
 +
 +static bool svm_dy_apicv_has_pending_interrupt(struct kvm_vcpu *vcpu)
 +{
 +	return false;
 +}
 +
 +static void svm_ir_list_del(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 +{
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *cur;
 +
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +	list_for_each_entry(cur, &svm->ir_list, node) {
 +		if (cur->data != pi->ir_data)
 +			continue;
 +		list_del(&cur->node);
 +		kfree(cur);
 +		break;
 +	}
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +}
 +
 +static int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 +{
 +	int ret = 0;
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *ir;
 +
 +	/**
 +	 * In some cases, the existing irte is updaed and re-set,
 +	 * so we need to check here if it's already been * added
 +	 * to the ir_list.
 +	 */
 +	if (pi->ir_data && (pi->prev_ga_tag != 0)) {
 +		struct kvm *kvm = svm->vcpu.kvm;
 +		u32 vcpu_id = AVIC_GATAG_TO_VCPUID(pi->prev_ga_tag);
 +		struct kvm_vcpu *prev_vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
 +		struct vcpu_svm *prev_svm;
 +
 +		if (!prev_vcpu) {
 +			ret = -EINVAL;
 +			goto out;
 +		}
 +
 +		prev_svm = to_svm(prev_vcpu);
 +		svm_ir_list_del(prev_svm, pi);
 +	}
 +
 +	/**
 +	 * Allocating new amd_iommu_pi_data, which will get
 +	 * add to the per-vcpu ir_list.
 +	 */
 +	ir = kzalloc(sizeof(struct amd_svm_iommu_ir), GFP_KERNEL_ACCOUNT);
 +	if (!ir) {
 +		ret = -ENOMEM;
 +		goto out;
 +	}
 +	ir->data = pi->ir_data;
 +
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +	list_add(&ir->node, &svm->ir_list);
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +out:
 +	return ret;
 +}
 +
 +/**
 + * Note:
 + * The HW cannot support posting multicast/broadcast
 + * interrupts to a vCPU. So, we still use legacy interrupt
 + * remapping for these kind of interrupts.
 + *
 + * For lowest-priority interrupts, we only support
 + * those with single CPU as the destination, e.g. user
 + * configures the interrupts via /proc/irq or uses
 + * irqbalance to make the interrupts single-CPU.
 + */
 +static int
 +get_pi_vcpu_info(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 +		 struct vcpu_data *vcpu_info, struct vcpu_svm **svm)
 +{
 +	struct kvm_lapic_irq irq;
 +	struct kvm_vcpu *vcpu = NULL;
 +
 +	kvm_set_msi_irq(kvm, e, &irq);
 +
 +	if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
 +	    !kvm_irq_is_postable(&irq)) {
 +		pr_debug("SVM: %s: use legacy intr remap mode for irq %u\n",
 +			 __func__, irq.vector);
 +		return -1;
 +	}
 +
 +	pr_debug("SVM: %s: use GA mode for irq %u\n", __func__,
 +		 irq.vector);
 +	*svm = to_svm(vcpu);
 +	vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
 +	vcpu_info->vector = irq.vector;
 +
 +	return 0;
 +}
 +
 +/*
 + * svm_update_pi_irte - set IRTE for Posted-Interrupts
 + *
 + * @kvm: kvm
 + * @host_irq: host irq of the interrupt
 + * @guest_irq: gsi of the interrupt
 + * @set: set or unset PI
 + * returns 0 on success, < 0 on failure
 + */
 +static int svm_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 +			      uint32_t guest_irq, bool set)
 +{
 +	struct kvm_kernel_irq_routing_entry *e;
 +	struct kvm_irq_routing_table *irq_rt;
 +	int idx, ret = -EINVAL;
 +
 +	if (!kvm_arch_has_assigned_device(kvm) ||
 +	    !irq_remapping_cap(IRQ_POSTING_CAP))
 +		return 0;
 +
 +	pr_debug("SVM: %s: host_irq=%#x, guest_irq=%#x, set=%#x\n",
 +		 __func__, host_irq, guest_irq, set);
 +
 +	idx = srcu_read_lock(&kvm->irq_srcu);
 +	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
 +	WARN_ON(guest_irq >= irq_rt->nr_rt_entries);
 +
 +	hlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {
 +		struct vcpu_data vcpu_info;
 +		struct vcpu_svm *svm = NULL;
 +
 +		if (e->type != KVM_IRQ_ROUTING_MSI)
 +			continue;
 +
 +		/**
 +		 * Here, we setup with legacy mode in the following cases:
 +		 * 1. When cannot target interrupt to a specific vcpu.
 +		 * 2. Unsetting posted interrupt.
 +		 * 3. APIC virtialization is disabled for the vcpu.
 +		 * 4. IRQ has incompatible delivery mode (SMI, INIT, etc)
 +		 */
 +		if (!get_pi_vcpu_info(kvm, e, &vcpu_info, &svm) && set &&
 +		    kvm_vcpu_apicv_active(&svm->vcpu)) {
 +			struct amd_iommu_pi_data pi;
 +
 +			/* Try to enable guest_mode in IRTE */
 +			pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
 +					    AVIC_HPA_MASK);
 +			pi.ga_tag = AVIC_GATAG(to_kvm_svm(kvm)->avic_vm_id,
 +						     svm->vcpu.vcpu_id);
 +			pi.is_guest_mode = true;
 +			pi.vcpu_data = &vcpu_info;
 +			ret = irq_set_vcpu_affinity(host_irq, &pi);
 +
 +			/**
 +			 * Here, we successfully setting up vcpu affinity in
 +			 * IOMMU guest mode. Now, we need to store the posted
 +			 * interrupt information in a per-vcpu ir_list so that
 +			 * we can reference to them directly when we update vcpu
 +			 * scheduling information in IOMMU irte.
 +			 */
 +			if (!ret && pi.is_guest_mode)
 +				svm_ir_list_add(svm, &pi);
 +		} else {
 +			/* Use legacy mode in IRTE */
 +			struct amd_iommu_pi_data pi;
 +
 +			/**
 +			 * Here, pi is used to:
 +			 * - Tell IOMMU to use legacy mode for this interrupt.
 +			 * - Retrieve ga_tag of prior interrupt remapping data.
 +			 */
 +			pi.is_guest_mode = false;
 +			ret = irq_set_vcpu_affinity(host_irq, &pi);
 +
 +			/**
 +			 * Check if the posted interrupt was previously
 +			 * setup with the guest_mode by checking if the ga_tag
 +			 * was cached. If so, we need to clean up the per-vcpu
 +			 * ir_list.
 +			 */
 +			if (!ret && pi.prev_ga_tag) {
 +				int id = AVIC_GATAG_TO_VCPUID(pi.prev_ga_tag);
 +				struct kvm_vcpu *vcpu;
 +
 +				vcpu = kvm_get_vcpu_by_id(kvm, id);
 +				if (vcpu)
 +					svm_ir_list_del(to_svm(vcpu), &pi);
 +			}
 +		}
 +
 +		if (!ret && svm) {
 +			trace_kvm_pi_irte_update(host_irq, svm->vcpu.vcpu_id,
 +						 e->gsi, vcpu_info.vector,
 +						 vcpu_info.pi_desc_addr, set);
 +		}
 +
 +		if (ret < 0) {
 +			pr_err("%s: failed to update PI IRTE\n", __func__);
 +			goto out;
 +		}
 +	}
 +
 +	ret = 0;
 +out:
 +	srcu_read_unlock(&kvm->irq_srcu, idx);
 +	return ret;
 +}
 +
 +static int svm_nmi_allowed(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	int ret;
 +	ret = !(vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) &&
 +	      !(svm->vcpu.arch.hflags & HF_NMI_MASK);
 +	ret = ret && gif_set(svm) && nested_svm_nmi(svm);
 +
 +	return ret;
 +}
 +
 +static bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	return !!(svm->vcpu.arch.hflags & HF_NMI_MASK);
 +}
 +
 +static void svm_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (masked) {
 +		svm->vcpu.arch.hflags |= HF_NMI_MASK;
 +		set_intercept(svm, INTERCEPT_IRET);
 +	} else {
 +		svm->vcpu.arch.hflags &= ~HF_NMI_MASK;
 +		clr_intercept(svm, INTERCEPT_IRET);
 +	}
 +}
 +
 +static int svm_interrupt_allowed(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	int ret;
 +
 +	if (!gif_set(svm) ||
 +	     (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK))
 +		return 0;
 +
 +	ret = !!(kvm_get_rflags(vcpu) & X86_EFLAGS_IF);
 +
 +	if (is_guest_mode(vcpu))
 +		return ret && !(svm->vcpu.arch.hflags & HF_VINTR_MASK);
 +
 +	return ret;
 +}
 +
 +static void enable_irq_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	/*
 +	 * In case GIF=0 we can't rely on the CPU to tell us when GIF becomes
 +	 * 1, because that's a separate STGI/VMRUN intercept.  The next time we
 +	 * get that intercept, this function will be called again though and
 +	 * we'll get the vintr intercept. However, if the vGIF feature is
 +	 * enabled, the STGI interception will not occur. Enable the irq
 +	 * window under the assumption that the hardware will set the GIF.
 +	 */
 +	if ((vgif_enabled(svm) || gif_set(svm)) && nested_svm_intr(svm)) {
 +		svm_set_vintr(svm);
 +	}
 +}
 +
 +static void enable_nmi_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if ((svm->vcpu.arch.hflags & (HF_NMI_MASK | HF_IRET_MASK))
 +	    == HF_NMI_MASK)
 +		return; /* IRET will cause a vm exit */
 +
 +	if (!gif_set(svm)) {
 +		if (vgif_enabled(svm))
 +			set_intercept(svm, INTERCEPT_STGI);
 +		return; /* STGI will cause a vm exit */
 +	}
 +
 +	if (svm->nested.exit_required)
 +		return; /* we're not going to run the guest yet */
 +
 +	/*
 +	 * Something prevents NMI from been injected. Single step over possible
 +	 * problem (IRET or exception injection or interrupt shadow)
 +	 */
 +	svm->nmi_singlestep_guest_rflags = svm_get_rflags(vcpu);
 +	svm->nmi_singlestep = true;
 +	svm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);
 +}
 +
 +static int svm_set_tss_addr(struct kvm *kvm, unsigned int addr)
 +{
 +	return 0;
 +}
 +
 +static int svm_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 +{
 +	return 0;
 +}
 +
 +static void svm_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	/*
 +	 * Flush only the current ASID even if the TLB flush was invoked via
 +	 * kvm_flush_remote_tlbs().  Although flushing remote TLBs requires all
 +	 * ASIDs to be flushed, KVM uses a single ASID for L1 and L2, and
 +	 * unconditionally does a TLB flush on both nested VM-Enter and nested
 +	 * VM-Exit (via kvm_mmu_reset_context()).
 +	 */
 +	if (static_cpu_has(X86_FEATURE_FLUSHBYASID))
 +		svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 +	else
 +		svm->asid_generation--;
 +}
 +
 +static void svm_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t gva)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	invlpga(gva, svm->vmcb->control.asid);
 +}
 +
 +static void svm_flush_tlb_guest(struct kvm_vcpu *vcpu)
 +{
 +	svm_flush_tlb(vcpu, false);
 +}
 +
 +static void svm_prepare_guest_switch(struct kvm_vcpu *vcpu)
 +{
 +}
 +
 +static inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (svm_nested_virtualize_tpr(vcpu))
 +		return;
 +
 +	if (!is_cr_intercept(svm, INTERCEPT_CR8_WRITE)) {
 +		int cr8 = svm->vmcb->control.int_ctl & V_TPR_MASK;
 +		kvm_set_cr8(vcpu, cr8);
 +	}
  }
  
 -static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {
 -	[SVM_EXIT_READ_CR0]			= cr_interception,
 -	[SVM_EXIT_READ_CR3]			= cr_interception,
 -	[SVM_EXIT_READ_CR4]			= cr_interception,
 -	[SVM_EXIT_READ_CR8]			= cr_interception,
 -	[SVM_EXIT_CR0_SEL_WRITE]		= cr_interception,
 -	[SVM_EXIT_WRITE_CR0]			= cr_interception,
 -	[SVM_EXIT_WRITE_CR3]			= cr_interception,
 -	[SVM_EXIT_WRITE_CR4]			= cr_interception,
 -	[SVM_EXIT_WRITE_CR8]			= cr8_write_interception,
 -	[SVM_EXIT_READ_DR0]			= dr_interception,
 -	[SVM_EXIT_READ_DR1]			= dr_interception,
 -	[SVM_EXIT_READ_DR2]			= dr_interception,
 -	[SVM_EXIT_READ_DR3]			= dr_interception,
 -	[SVM_EXIT_READ_DR4]			= dr_interception,
 -	[SVM_EXIT_READ_DR5]			= dr_interception,
 -	[SVM_EXIT_READ_DR6]			= dr_interception,
 -	[SVM_EXIT_READ_DR7]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR0]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR1]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR2]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR3]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR4]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR5]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR6]			= dr_interception,
 -	[SVM_EXIT_WRITE_DR7]			= dr_interception,
 -	[SVM_EXIT_EXCP_BASE + DB_VECTOR]	= db_interception,
 -	[SVM_EXIT_EXCP_BASE + BP_VECTOR]	= bp_interception,
 -	[SVM_EXIT_EXCP_BASE + UD_VECTOR]	= ud_interception,
 -	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,
 -	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,
 -	[SVM_EXIT_EXCP_BASE + AC_VECTOR]	= ac_interception,
 -	[SVM_EXIT_EXCP_BASE + GP_VECTOR]	= gp_interception,
 -	[SVM_EXIT_INTR]				= intr_interception,
 -	[SVM_EXIT_NMI]				= nmi_interception,
 -	[SVM_EXIT_SMI]				= nop_on_interception,
 -	[SVM_EXIT_INIT]				= nop_on_interception,
 -	[SVM_EXIT_VINTR]			= interrupt_window_interception,
 -	[SVM_EXIT_RDPMC]			= rdpmc_interception,
 -	[SVM_EXIT_CPUID]			= cpuid_interception,
 -	[SVM_EXIT_IRET]                         = iret_interception,
 -	[SVM_EXIT_INVD]                         = emulate_on_interception,
 -	[SVM_EXIT_PAUSE]			= pause_interception,
 -	[SVM_EXIT_HLT]				= halt_interception,
 -	[SVM_EXIT_INVLPG]			= invlpg_interception,
 -	[SVM_EXIT_INVLPGA]			= invlpga_interception,
 -	[SVM_EXIT_IOIO]				= io_interception,
 -	[SVM_EXIT_MSR]				= msr_interception,
 -	[SVM_EXIT_TASK_SWITCH]			= task_switch_interception,
 -	[SVM_EXIT_SHUTDOWN]			= shutdown_interception,
 -	[SVM_EXIT_VMRUN]			= vmrun_interception,
 -	[SVM_EXIT_VMMCALL]			= vmmcall_interception,
 -	[SVM_EXIT_VMLOAD]			= vmload_interception,
 -	[SVM_EXIT_VMSAVE]			= vmsave_interception,
 -	[SVM_EXIT_STGI]				= stgi_interception,
 -	[SVM_EXIT_CLGI]				= clgi_interception,
 -	[SVM_EXIT_SKINIT]			= skinit_interception,
 -	[SVM_EXIT_WBINVD]                       = wbinvd_interception,
 -	[SVM_EXIT_MONITOR]			= monitor_interception,
 -	[SVM_EXIT_MWAIT]			= mwait_interception,
 -	[SVM_EXIT_XSETBV]			= xsetbv_interception,
 -	[SVM_EXIT_RDPRU]			= rdpru_interception,
 -	[SVM_EXIT_NPF]				= npf_interception,
 -	[SVM_EXIT_RSM]                          = rsm_interception,
 -	[SVM_EXIT_AVIC_INCOMPLETE_IPI]		= avic_incomplete_ipi_interception,
 -	[SVM_EXIT_AVIC_UNACCELERATED_ACCESS]	= avic_unaccelerated_access_interception,
 -};
 +static inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u64 cr8;
 +
 +	if (svm_nested_virtualize_tpr(vcpu) ||
 +	    kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	cr8 = kvm_get_cr8(vcpu);
 +	svm->vmcb->control.int_ctl &= ~V_TPR_MASK;
 +	svm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;
 +}
 +
 +static void svm_complete_interrupts(struct vcpu_svm *svm)
 +{
 +	u8 vector;
 +	int type;
 +	u32 exitintinfo = svm->vmcb->control.exit_int_info;
 +	unsigned int3_injected = svm->int3_injected;
 +
 +	svm->int3_injected = 0;
 +
 +	/*
 +	 * If we've made progress since setting HF_IRET_MASK, we've
 +	 * executed an IRET and can allow NMI injection.
 +	 */
 +	if ((svm->vcpu.arch.hflags & HF_IRET_MASK)
 +	    && kvm_rip_read(&svm->vcpu) != svm->nmi_iret_rip) {
 +		svm->vcpu.arch.hflags &= ~(HF_NMI_MASK | HF_IRET_MASK);
 +		kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +	}
 +
 +	svm->vcpu.arch.nmi_injected = false;
 +	kvm_clear_exception_queue(&svm->vcpu);
 +	kvm_clear_interrupt_queue(&svm->vcpu);
 +
 +	if (!(exitintinfo & SVM_EXITINTINFO_VALID))
 +		return;
 +
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +
 +	vector = exitintinfo & SVM_EXITINTINFO_VEC_MASK;
 +	type = exitintinfo & SVM_EXITINTINFO_TYPE_MASK;
 +
 +	switch (type) {
 +	case SVM_EXITINTINFO_TYPE_NMI:
 +		svm->vcpu.arch.nmi_injected = true;
 +		break;
 +	case SVM_EXITINTINFO_TYPE_EXEPT:
 +		/*
 +		 * In case of software exceptions, do not reinject the vector,
 +		 * but re-execute the instruction instead. Rewind RIP first
 +		 * if we emulated INT3 before.
 +		 */
 +		if (kvm_exception_is_soft(vector)) {
 +			if (vector == BP_VECTOR && int3_injected &&
 +			    kvm_is_linear_rip(&svm->vcpu, svm->int3_rip))
 +				kvm_rip_write(&svm->vcpu,
 +					      kvm_rip_read(&svm->vcpu) -
 +					      int3_injected);
 +			break;
 +		}
 +		if (exitintinfo & SVM_EXITINTINFO_VALID_ERR) {
 +			u32 err = svm->vmcb->control.exit_int_info_err;
 +			kvm_requeue_exception_e(&svm->vcpu, vector, err);
 +
 +		} else
 +			kvm_requeue_exception(&svm->vcpu, vector);
 +		break;
 +	case SVM_EXITINTINFO_TYPE_INTR:
 +		kvm_queue_interrupt(&svm->vcpu, vector, false);
 +		break;
 +	default:
 +		break;
 +	}
 +}
 +
 +static void svm_cancel_injection(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb_control_area *control = &svm->vmcb->control;
 +
 +	control->exit_int_info = control->event_inj;
 +	control->exit_int_info_err = control->event_inj_err;
 +	control->event_inj = 0;
 +	svm_complete_interrupts(svm);
 +}
 +
 +static void svm_vcpu_run(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 +	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 +	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
 +
 +	/*
 +	 * A vmexit emulation is required before the vcpu can be executed
 +	 * again.
 +	 */
 +	if (unlikely(svm->nested.exit_required))
 +		return;
 +
 +	/*
 +	 * Disable singlestep if we're injecting an interrupt/exception.
 +	 * We don't want our modified rflags to be pushed on the stack where
 +	 * we might not be able to easily reset them if we disabled NMI
 +	 * singlestep later.
 +	 */
 +	if (svm->nmi_singlestep && svm->vmcb->control.event_inj) {
 +		/*
 +		 * Event injection happens before external interrupts cause a
 +		 * vmexit and interrupts are disabled here, so smp_send_reschedule
 +		 * is enough to force an immediate vmexit.
 +		 */
 +		disable_nmi_singlestep(svm);
 +		smp_send_reschedule(vcpu->cpu);
 +	}
 +
 +	pre_svm_run(svm);
 +
 +	sync_lapic_to_cr8(vcpu);
 +
 +	svm->vmcb->save.cr2 = vcpu->arch.cr2;
 +
 +	clgi();
 +	kvm_load_guest_xsave_state(vcpu);
 +
 +	if (lapic_in_kernel(vcpu) &&
 +		vcpu->arch.apic->lapic_timer.timer_advance_ns)
 +		kvm_wait_lapic_expire(vcpu);
 +
 +	/*
 +	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 +	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 +	 * is no need to worry about the conditional branch over the wrmsr
 +	 * being speculatively taken.
 +	 */
 +	x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
 +
++<<<<<<< HEAD
 +	local_irq_enable();
 +
 +	asm volatile (
 +		"push %%" _ASM_BP "; \n\t"
 +		"mov %c[rbx](%[svm]), %%" _ASM_BX " \n\t"
 +		"mov %c[rcx](%[svm]), %%" _ASM_CX " \n\t"
 +		"mov %c[rdx](%[svm]), %%" _ASM_DX " \n\t"
 +		"mov %c[rsi](%[svm]), %%" _ASM_SI " \n\t"
 +		"mov %c[rdi](%[svm]), %%" _ASM_DI " \n\t"
 +		"mov %c[rbp](%[svm]), %%" _ASM_BP " \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %c[r8](%[svm]),  %%r8  \n\t"
 +		"mov %c[r9](%[svm]),  %%r9  \n\t"
 +		"mov %c[r10](%[svm]), %%r10 \n\t"
 +		"mov %c[r11](%[svm]), %%r11 \n\t"
 +		"mov %c[r12](%[svm]), %%r12 \n\t"
 +		"mov %c[r13](%[svm]), %%r13 \n\t"
 +		"mov %c[r14](%[svm]), %%r14 \n\t"
 +		"mov %c[r15](%[svm]), %%r15 \n\t"
 +#endif
 +
 +		/* Enter guest mode */
 +		"push %%" _ASM_AX " \n\t"
 +		"mov %c[vmcb](%[svm]), %%" _ASM_AX " \n\t"
 +		__ex("vmload %%" _ASM_AX) "\n\t"
 +		__ex("vmrun %%" _ASM_AX) "\n\t"
 +		__ex("vmsave %%" _ASM_AX) "\n\t"
 +		"pop %%" _ASM_AX " \n\t"
 +
 +		/* Save guest registers, load host registers */
 +		"mov %%" _ASM_BX ", %c[rbx](%[svm]) \n\t"
 +		"mov %%" _ASM_CX ", %c[rcx](%[svm]) \n\t"
 +		"mov %%" _ASM_DX ", %c[rdx](%[svm]) \n\t"
 +		"mov %%" _ASM_SI ", %c[rsi](%[svm]) \n\t"
 +		"mov %%" _ASM_DI ", %c[rdi](%[svm]) \n\t"
 +		"mov %%" _ASM_BP ", %c[rbp](%[svm]) \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %%r8,  %c[r8](%[svm]) \n\t"
 +		"mov %%r9,  %c[r9](%[svm]) \n\t"
 +		"mov %%r10, %c[r10](%[svm]) \n\t"
 +		"mov %%r11, %c[r11](%[svm]) \n\t"
 +		"mov %%r12, %c[r12](%[svm]) \n\t"
 +		"mov %%r13, %c[r13](%[svm]) \n\t"
 +		"mov %%r14, %c[r14](%[svm]) \n\t"
 +		"mov %%r15, %c[r15](%[svm]) \n\t"
 +		/*
 +		* Clear host registers marked as clobbered to prevent
 +		* speculative use.
 +		*/
 +		"xor %%r8d, %%r8d \n\t"
 +		"xor %%r9d, %%r9d \n\t"
 +		"xor %%r10d, %%r10d \n\t"
 +		"xor %%r11d, %%r11d \n\t"
 +		"xor %%r12d, %%r12d \n\t"
 +		"xor %%r13d, %%r13d \n\t"
 +		"xor %%r14d, %%r14d \n\t"
 +		"xor %%r15d, %%r15d \n\t"
 +#endif
 +		"xor %%ebx, %%ebx \n\t"
 +		"xor %%ecx, %%ecx \n\t"
 +		"xor %%edx, %%edx \n\t"
 +		"xor %%esi, %%esi \n\t"
 +		"xor %%edi, %%edi \n\t"
 +		"pop %%" _ASM_BP
 +		:
 +		: [svm]"a"(svm),
 +		  [vmcb]"i"(offsetof(struct vcpu_svm, vmcb_pa)),
 +		  [rbx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBX])),
 +		  [rcx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RCX])),
 +		  [rdx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDX])),
 +		  [rsi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RSI])),
 +		  [rdi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDI])),
 +		  [rbp]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBP]))
 +#ifdef CONFIG_X86_64
 +		  , [r8]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R8])),
 +		  [r9]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R9])),
 +		  [r10]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R10])),
 +		  [r11]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R11])),
 +		  [r12]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R12])),
 +		  [r13]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R13])),
 +		  [r14]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R14])),
 +		  [r15]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R15]))
 +#endif
 +		: "cc", "memory"
 +#ifdef CONFIG_X86_64
 +		, "rbx", "rcx", "rdx", "rsi", "rdi"
 +		, "r8", "r9", "r10", "r11" , "r12", "r13", "r14", "r15"
 +#else
 +		, "ebx", "ecx", "edx", "esi", "edi"
 +#endif
 +		);
 +
 +	/* Eliminate branch target predictions from guest mode */
 +	vmexit_fill_RSB();
++=======
++	/*
++	 * Tell context tracking that this CPU is about to enter guest
++	 * mode. This has to be after x86_spec_ctrl_set_guest() because
++	 * that can take locks (lockdep needs RCU) and calls into world and
++	 * some more.
++	 */
++	guest_enter_irqoff();
++
++	__svm_vcpu_run(svm->vmcb_pa, (unsigned long *)&svm->vcpu.arch.regs);
++>>>>>>> 87fa7f3e98a1 (x86/kvm: Move context tracking where it belongs)
 +
 +#ifdef CONFIG_X86_64
 +	wrmsrl(MSR_GS_BASE, svm->host.gs_base);
 +#else
 +	loadsegment(fs, svm->host.fs);
 +#ifndef CONFIG_X86_32_LAZY_GS
 +	loadsegment(gs, svm->host.gs);
 +#endif
 +#endif
++	/*
++	 * Tell context tracking that this CPU is back.
++	 *
++	 * This needs to be done before the below as native_read_msr()
++	 * contains a tracepoint and x86_spec_ctrl_restore_host() calls
++	 * into world and some more.
++	 */
++	guest_exit_irqoff();
 +
 +	/*
 +	 * We do not use IBRS in the kernel. If this vCPU has used the
 +	 * SPEC_CTRL MSR it may have left it on; save the value and
 +	 * turn it off. This is much more efficient than blindly adding
 +	 * it to the atomic save/restore list. Especially as the former
 +	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 +	 *
 +	 * For non-nested case:
 +	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 *
 +	 * For nested case:
 +	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 */
 +	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 +		svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
  
 -static void dump_vmcb(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb_control_area *control = &svm->vmcb->control;
 -	struct vmcb_save_area *save = &svm->vmcb->save;
 +	reload_tss(vcpu);
  
 -	if (!dump_invalid_vmcb) {
 -		pr_warn_ratelimited("set kvm_amd.dump_invalid_vmcb=1 to dump internal KVM state.\n");
 -		return;
 +	local_irq_disable();
 +
 +	x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
 +
 +	vcpu->arch.cr2 = svm->vmcb->save.cr2;
 +	vcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;
 +	vcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;
 +	vcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;
 +
 +	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 +		kvm_before_interrupt(&svm->vcpu);
 +
 +	kvm_load_host_xsave_state(vcpu);
 +	stgi();
 +
 +	/* Any pending NMI will happen here */
 +
 +	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 +		kvm_after_interrupt(&svm->vcpu);
 +
 +	sync_cr8_to_lapic(vcpu);
 +
 +	svm->next_rip = 0;
 +
 +	svm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;
 +
 +	/* if exit due to PF check for async PF */
 +	if (svm->vmcb->control.exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR)
 +		svm->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +
 +	if (npt_enabled) {
 +		vcpu->arch.regs_avail &= ~(1 << VCPU_EXREG_PDPTR);
 +		vcpu->arch.regs_dirty &= ~(1 << VCPU_EXREG_PDPTR);
  	}
  
 -	pr_err("VMCB Control Area:\n");
 -	pr_err("%-20s%04x\n", "cr_read:", control->intercept_cr & 0xffff);
 -	pr_err("%-20s%04x\n", "cr_write:", control->intercept_cr >> 16);
 -	pr_err("%-20s%04x\n", "dr_read:", control->intercept_dr & 0xffff);
 -	pr_err("%-20s%04x\n", "dr_write:", control->intercept_dr >> 16);
 -	pr_err("%-20s%08x\n", "exceptions:", control->intercept_exceptions);
 -	pr_err("%-20s%016llx\n", "intercepts:", control->intercept);
 -	pr_err("%-20s%d\n", "pause filter count:", control->pause_filter_count);
 -	pr_err("%-20s%d\n", "pause filter threshold:",
 -	       control->pause_filter_thresh);
 -	pr_err("%-20s%016llx\n", "iopm_base_pa:", control->iopm_base_pa);
 -	pr_err("%-20s%016llx\n", "msrpm_base_pa:", control->msrpm_base_pa);
 -	pr_err("%-20s%016llx\n", "tsc_offset:", control->tsc_offset);
 -	pr_err("%-20s%d\n", "asid:", control->asid);
 -	pr_err("%-20s%d\n", "tlb_ctl:", control->tlb_ctl);
 -	pr_err("%-20s%08x\n", "int_ctl:", control->int_ctl);
 -	pr_err("%-20s%08x\n", "int_vector:", control->int_vector);
 -	pr_err("%-20s%08x\n", "int_state:", control->int_state);
 -	pr_err("%-20s%08x\n", "exit_code:", control->exit_code);
 -	pr_err("%-20s%016llx\n", "exit_info1:", control->exit_info_1);
 -	pr_err("%-20s%016llx\n", "exit_info2:", control->exit_info_2);
 -	pr_err("%-20s%08x\n", "exit_int_info:", control->exit_int_info);
 -	pr_err("%-20s%08x\n", "exit_int_info_err:", control->exit_int_info_err);
 -	pr_err("%-20s%lld\n", "nested_ctl:", control->nested_ctl);
 -	pr_err("%-20s%016llx\n", "nested_cr3:", control->nested_cr3);
 -	pr_err("%-20s%016llx\n", "avic_vapic_bar:", control->avic_vapic_bar);
 -	pr_err("%-20s%08x\n", "event_inj:", control->event_inj);
 -	pr_err("%-20s%08x\n", "event_inj_err:", control->event_inj_err);
 -	pr_err("%-20s%lld\n", "virt_ext:", control->virt_ext);
 -	pr_err("%-20s%016llx\n", "next_rip:", control->next_rip);
 -	pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
 -	pr_err("%-20s%016llx\n", "avic_logical_id:", control->avic_logical_id);
 -	pr_err("%-20s%016llx\n", "avic_physical_id:", control->avic_physical_id);
 -	pr_err("VMCB State Save Area:\n");
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "es:",
 -	       save->es.selector, save->es.attrib,
 -	       save->es.limit, save->es.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "cs:",
 -	       save->cs.selector, save->cs.attrib,
 -	       save->cs.limit, save->cs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ss:",
 -	       save->ss.selector, save->ss.attrib,
 -	       save->ss.limit, save->ss.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ds:",
 -	       save->ds.selector, save->ds.attrib,
 -	       save->ds.limit, save->ds.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "fs:",
 -	       save->fs.selector, save->fs.attrib,
 -	       save->fs.limit, save->fs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "gs:",
 -	       save->gs.selector, save->gs.attrib,
 -	       save->gs.limit, save->gs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "gdtr:",
 -	       save->gdtr.selector, save->gdtr.attrib,
 -	       save->gdtr.limit, save->gdtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ldtr:",
 -	       save->ldtr.selector, save->ldtr.attrib,
 -	       save->ldtr.limit, save->ldtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "idtr:",
 -	       save->idtr.selector, save->idtr.attrib,
 -	       save->idtr.limit, save->idtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "tr:",
 -	       save->tr.selector, save->tr.attrib,
 -	       save->tr.limit, save->tr.base);
 -	pr_err("cpl:            %d                efer:         %016llx\n",
 -		save->cpl, save->efer);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cr0:", save->cr0, "cr2:", save->cr2);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cr3:", save->cr3, "cr4:", save->cr4);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "dr6:", save->dr6, "dr7:", save->dr7);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "rip:", save->rip, "rflags:", save->rflags);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "rsp:", save->rsp, "rax:", save->rax);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "star:", save->star, "lstar:", save->lstar);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cstar:", save->cstar, "sfmask:", save->sfmask);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "kernel_gs_base:", save->kernel_gs_base,
 -	       "sysenter_cs:", save->sysenter_cs);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "sysenter_esp:", save->sysenter_esp,
 -	       "sysenter_eip:", save->sysenter_eip);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "gpat:", save->g_pat, "dbgctl:", save->dbgctl);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "br_from:", save->br_from, "br_to:", save->br_to);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "excp_from:", save->last_excp_from,
 -	       "excp_to:", save->last_excp_to);
 +	/*
 +	 * We need to handle MC intercepts here before the vcpu has a chance to
 +	 * change the physical cpu
 +	 */
 +	if (unlikely(svm->vmcb->control.exit_code ==
 +		     SVM_EXIT_EXCP_BASE + MC_VECTOR))
 +		svm_handle_mce(svm);
 +
 +	mark_all_clean(svm->vmcb);
  }
  
 -static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +static void svm_set_cr3(struct kvm_vcpu *vcpu, unsigned long root)
  {
 -	struct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;
 +	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	*info1 = control->exit_info_1;
 -	*info2 = control->exit_info_2;
 +	svm->vmcb->save.cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_CR);
  }
  
 -static int handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 +static void set_tdp_cr3(struct kvm_vcpu *vcpu, unsigned long root)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct kvm_run *kvm_run = vcpu->run;
 -	u32 exit_code = svm->vmcb->control.exit_code;
  
 -	trace_kvm_exit(exit_code, vcpu, KVM_ISA_SVM);
 +	svm->vmcb->control.nested_cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_NPT);
  
 -	if (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))
 -		vcpu->arch.cr0 = svm->vmcb->save.cr0;
 -	if (npt_enabled)
 -		vcpu->arch.cr3 = svm->vmcb->save.cr3;
 +	/* Also sync guest cr3 here in case we live migrate */
 +	svm->vmcb->save.cr3 = kvm_read_cr3(vcpu);
 +	mark_dirty(svm->vmcb, VMCB_CR);
 +}
  
 -	svm_complete_interrupts(svm);
 +static int is_disabled(void)
 +{
 +	u64 vm_cr;
  
 -	if (is_guest_mode(vcpu)) {
 -		int vmexit;
 +	rdmsrl(MSR_VM_CR, vm_cr);
 +	if (vm_cr & (1 << SVM_VM_CR_SVM_DISABLE))
 +		return 1;
  
 -		trace_kvm_nested_vmexit(svm->vmcb->save.rip, exit_code,
 -					svm->vmcb->control.exit_info_1,
 -					svm->vmcb->control.exit_info_2,
 -					svm->vmcb->control.exit_int_info,
 -					svm->vmcb->control.exit_int_info_err,
 -					KVM_ISA_SVM);
 +	return 0;
 +}
  
 -		vmexit = nested_svm_exit_special(svm);
 +static void
 +svm_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +{
 +	/*
 +	 * Patch in the VMMCALL instruction:
 +	 */
 +	hypercall[0] = 0x0f;
 +	hypercall[1] = 0x01;
 +	hypercall[2] = 0xd9;
 +}
  
 -		if (vmexit == NESTED_EXIT_CONTINUE)
 -			vmexit = nested_svm_exit_handled(svm);
 +static int __init svm_check_processor_compat(void)
 +{
 +	return 0;
 +}
  
 -		if (vmexit == NESTED_EXIT_DONE)
 -			return 1;
 -	}
 +static bool svm_cpu_has_accelerated_tpr(void)
 +{
 +	return false;
 +}
  
 -	if (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {
 -		kvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 -		kvm_run->fail_entry.hardware_entry_failure_reason
 -			= svm->vmcb->control.exit_code;
 -		kvm_run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;
 -		dump_vmcb(vcpu);
 -		return 0;
 +static bool svm_has_emulated_msr(u32 index)
 +{
 +	switch (index) {
 +	case MSR_IA32_MCG_EXT_CTL:
 +	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 +		return false;
 +	default:
 +		break;
  	}
  
 -	if (is_external_interrupt(svm->vmcb->control.exit_int_info) &&
 -	    exit_code != SVM_EXIT_EXCP_BASE + PF_VECTOR &&
 -	    exit_code != SVM_EXIT_NPF && exit_code != SVM_EXIT_TASK_SWITCH &&
 -	    exit_code != SVM_EXIT_INTR && exit_code != SVM_EXIT_NMI)
 -		printk(KERN_ERR "%s: unexpected exit_int_info 0x%x "
 -		       "exit_code 0x%x\n",
 -		       __func__, svm->vmcb->control.exit_int_info,
 -		       exit_code);
 +	return true;
 +}
 +
 +static u64 svm_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 +{
 +	return 0;
 +}
 +
 +static void svm_cpuid_update(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	vcpu->arch.xsaves_enabled = guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&
 +				    boot_cpu_has(X86_FEATURE_XSAVE) &&
 +				    boot_cpu_has(X86_FEATURE_XSAVES);
 +
 +	/* Update nrips enabled cache */
 +	svm->nrips_enabled = !!guest_cpuid_has(&svm->vcpu, X86_FEATURE_NRIPS);
 +
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
  
 -	if (exit_fastpath != EXIT_FASTPATH_NONE)
 -		return 1;
 +	guest_cpuid_clear(vcpu, X86_FEATURE_X2APIC);
 +}
 +
 +#define F feature_bit
 +
 +static void svm_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
 +{
 +	switch (func) {
 +	case 0x1:
 +		if (avic)
 +			entry->ecx &= ~F(X2APIC);
 +		break;
 +	case 0x80000001:
 +		if (nested)
 +			entry->ecx |= (1 << 2); /* Set SVM bit */
 +		break;
 +	case 0x80000008:
 +		if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) ||
 +		     boot_cpu_has(X86_FEATURE_AMD_SSBD))
 +			entry->ebx |= F(VIRT_SSBD);
 +		break;
 +	case 0x8000000A:
 +		entry->eax = 1; /* SVM revision 1 */
 +		entry->ebx = 8; /* Lets support 8 ASIDs in case we add proper
 +				   ASID emulation to nested SVM */
 +		entry->ecx = 0; /* Reserved */
 +		entry->edx = 0; /* Per default do not support any
 +				   additional features */
 +
 +		/* Support next_rip if host supports it */
 +		if (boot_cpu_has(X86_FEATURE_NRIPS))
 +			entry->edx |= F(NRIPS);
 +
 +		/* Support NPT for the guest if enabled */
 +		if (npt_enabled)
 +			entry->edx |= F(NPT);
  
 -	if (exit_code >= ARRAY_SIZE(svm_exit_handlers)
 -	    || !svm_exit_handlers[exit_code]) {
 -		vcpu_unimpl(vcpu, "svm: unexpected exit reason 0x%x\n", exit_code);
 -		dump_vmcb(vcpu);
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror =
 -			KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
 -		vcpu->run->internal.ndata = 2;
 -		vcpu->run->internal.data[0] = exit_code;
 -		vcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;
 -		return 0;
  	}
 +}
  
 -#ifdef CONFIG_RETPOLINE
 -	if (exit_code == SVM_EXIT_MSR)
 -		return msr_interception(svm);
 -	else if (exit_code == SVM_EXIT_VINTR)
 -		return interrupt_window_interception(svm);
 -	else if (exit_code == SVM_EXIT_INTR)
 -		return intr_interception(svm);
 -	else if (exit_code == SVM_EXIT_HLT)
 -		return halt_interception(svm);
 -	else if (exit_code == SVM_EXIT_NPF)
 -		return npf_interception(svm);
 -#endif
 -	return svm_exit_handlers[exit_code](svm);
 +static int svm_get_lpage_level(void)
 +{
 +	return PT_PDPE_LEVEL;
  }
  
 -static void reload_tss(struct kvm_vcpu *vcpu)
 +static bool svm_rdtscp_supported(void)
  {
 -	struct svm_cpu_data *sd = per_cpu(svm_data, vcpu->cpu);
 +	return boot_cpu_has(X86_FEATURE_RDTSCP);
 +}
  
 -	sd->tss_desc->type = 9; /* available 32/64-bit TSS */
 -	load_TR_desc();
 +static bool svm_invpcid_supported(void)
 +{
 +	return false;
  }
  
 -static void pre_svm_run(struct vcpu_svm *svm)
 +static bool svm_mpx_supported(void)
  {
 -	struct svm_cpu_data *sd = per_cpu(svm_data, svm->vcpu.cpu);
 +	return false;
 +}
  
 -	if (sev_guest(svm->vcpu.kvm))
 -		return pre_sev_run(svm, svm->vcpu.cpu);
 +static bool svm_xsaves_supported(void)
 +{
 +	return boot_cpu_has(X86_FEATURE_XSAVES);
 +}
  
 -	/* FIXME: handle wraparound of asid_generation */
 -	if (svm->asid_generation != sd->asid_generation)
 -		new_asid(svm, sd);
 +static bool svm_umip_emulated(void)
 +{
 +	return false;
  }
  
 -static void svm_inject_nmi(struct kvm_vcpu *vcpu)
 +static bool svm_pt_supported(void)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	return false;
 +}
  
 -	svm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;
 -	vcpu->arch.hflags |= HF_NMI_MASK;
 -	svm_set_intercept(svm, INTERCEPT_IRET);
 -	++vcpu->stat.nmi_injections;
 +static bool svm_has_wbinvd_exit(void)
 +{
 +	return true;
  }
  
 -static void svm_set_irq(struct kvm_vcpu *vcpu)
 +static bool svm_pku_supported(void)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	return false;
 +}
  
 -	BUG_ON(!(gif_set(svm)));
 +#define PRE_EX(exit)  { .exit_code = (exit), \
 +			.stage = X86_ICPT_PRE_EXCEPT, }
 +#define POST_EX(exit) { .exit_code = (exit), \
 +			.stage = X86_ICPT_POST_EXCEPT, }
 +#define POST_MEM(exit) { .exit_code = (exit), \
 +			.stage = X86_ICPT_POST_MEMACCESS, }
  
 -	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 -	++vcpu->stat.irq_injections;
 +static const struct __x86_intercept {
 +	u32 exit_code;
 +	enum x86_intercept_stage stage;
 +} x86_intercept_map[] = {
 +	[x86_intercept_cr_read]		= POST_EX(SVM_EXIT_READ_CR0),
 +	[x86_intercept_cr_write]	= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_clts]		= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_lmsw]		= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_smsw]		= POST_EX(SVM_EXIT_READ_CR0),
 +	[x86_intercept_dr_read]		= POST_EX(SVM_EXIT_READ_DR0),
 +	[x86_intercept_dr_write]	= POST_EX(SVM_EXIT_WRITE_DR0),
 +	[x86_intercept_sldt]		= POST_EX(SVM_EXIT_LDTR_READ),
 +	[x86_intercept_str]		= POST_EX(SVM_EXIT_TR_READ),
 +	[x86_intercept_lldt]		= POST_EX(SVM_EXIT_LDTR_WRITE),
 +	[x86_intercept_ltr]		= POST_EX(SVM_EXIT_TR_WRITE),
 +	[x86_intercept_sgdt]		= POST_EX(SVM_EXIT_GDTR_READ),
 +	[x86_intercept_sidt]		= POST_EX(SVM_EXIT_IDTR_READ),
 +	[x86_intercept_lgdt]		= POST_EX(SVM_EXIT_GDTR_WRITE),
 +	[x86_intercept_lidt]		= POST_EX(SVM_EXIT_IDTR_WRITE),
 +	[x86_intercept_vmrun]		= POST_EX(SVM_EXIT_VMRUN),
 +	[x86_intercept_vmmcall]		= POST_EX(SVM_EXIT_VMMCALL),
 +	[x86_intercept_vmload]		= POST_EX(SVM_EXIT_VMLOAD),
 +	[x86_intercept_vmsave]		= POST_EX(SVM_EXIT_VMSAVE),
 +	[x86_intercept_stgi]		= POST_EX(SVM_EXIT_STGI),
 +	[x86_intercept_clgi]		= POST_EX(SVM_EXIT_CLGI),
 +	[x86_intercept_skinit]		= POST_EX(SVM_EXIT_SKINIT),
 +	[x86_intercept_invlpga]		= POST_EX(SVM_EXIT_INVLPGA),
 +	[x86_intercept_rdtscp]		= POST_EX(SVM_EXIT_RDTSCP),
 +	[x86_intercept_monitor]		= POST_MEM(SVM_EXIT_MONITOR),
 +	[x86_intercept_mwait]		= POST_EX(SVM_EXIT_MWAIT),
 +	[x86_intercept_invlpg]		= POST_EX(SVM_EXIT_INVLPG),
 +	[x86_intercept_invd]		= POST_EX(SVM_EXIT_INVD),
 +	[x86_intercept_wbinvd]		= POST_EX(SVM_EXIT_WBINVD),
 +	[x86_intercept_wrmsr]		= POST_EX(SVM_EXIT_MSR),
 +	[x86_intercept_rdtsc]		= POST_EX(SVM_EXIT_RDTSC),
 +	[x86_intercept_rdmsr]		= POST_EX(SVM_EXIT_MSR),
 +	[x86_intercept_rdpmc]		= POST_EX(SVM_EXIT_RDPMC),
 +	[x86_intercept_cpuid]		= PRE_EX(SVM_EXIT_CPUID),
 +	[x86_intercept_rsm]		= PRE_EX(SVM_EXIT_RSM),
 +	[x86_intercept_pause]		= PRE_EX(SVM_EXIT_PAUSE),
 +	[x86_intercept_pushf]		= PRE_EX(SVM_EXIT_PUSHF),
 +	[x86_intercept_popf]		= PRE_EX(SVM_EXIT_POPF),
 +	[x86_intercept_intn]		= PRE_EX(SVM_EXIT_SWINT),
 +	[x86_intercept_iret]		= PRE_EX(SVM_EXIT_IRET),
 +	[x86_intercept_icebp]		= PRE_EX(SVM_EXIT_ICEBP),
 +	[x86_intercept_hlt]		= POST_EX(SVM_EXIT_HLT),
 +	[x86_intercept_in]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_ins]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_out]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_outs]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_xsetbv]		= PRE_EX(SVM_EXIT_XSETBV),
 +};
  
 -	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 -		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
 -}
 +#undef PRE_EX
 +#undef POST_EX
 +#undef POST_MEM
  
 -static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +static int svm_check_intercept(struct kvm_vcpu *vcpu,
 +			       struct x86_instruction_info *info,
 +			       enum x86_intercept_stage stage)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 +	int vmexit, ret = X86EMUL_CONTINUE;
 +	struct __x86_intercept icpt_info;
 +	struct vmcb *vmcb = svm->vmcb;
  
 -	if (nested_svm_virtualize_tpr(vcpu))
 -		return;
 +	if (info->intercept >= ARRAY_SIZE(x86_intercept_map))
 +		goto out;
  
 -	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +	icpt_info = x86_intercept_map[info->intercept];
  
 -	if (irr == -1)
 -		return;
 +	if (stage != icpt_info.stage)
 +		goto out;
  
 -	if (tpr >= irr)
 -		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 -}
 +	switch (icpt_info.exit_code) {
 +	case SVM_EXIT_READ_CR0:
 +		if (info->intercept == x86_intercept_cr_read)
 +			icpt_info.exit_code += info->modrm_reg;
 +		break;
 +	case SVM_EXIT_WRITE_CR0: {
 +		unsigned long cr0, val;
 +		u64 intercept;
  
 -bool svm_nmi_blocked(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb *vmcb = svm->vmcb;
 -	bool ret;
 +		if (info->intercept == x86_intercept_cr_write)
 +			icpt_info.exit_code += info->modrm_reg;
  
 -	if (!gif_set(svm))
 -		return true;
 +		if (icpt_info.exit_code != SVM_EXIT_WRITE_CR0 ||
 +		    info->intercept == x86_intercept_clts)
 +			break;
  
 -	if (is_guest_mode(vcpu) && nested_exit_on_nmi(svm))
 -		return false;
 +		intercept = svm->nested.intercept;
 +
 +		if (!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0)))
 +			break;
 +
 +		cr0 = vcpu->arch.cr0 & ~SVM_CR0_SELECTIVE_MASK;
 +		val = info->src_val  & ~SVM_CR0_SELECTIVE_MASK;
 +
 +		if (info->intercept == x86_intercept_lmsw) {
 +			cr0 &= 0xfUL;
 +			val &= 0xfUL;
 +			/* lmsw can't clear PE - catch this here */
 +			if (cr0 & X86_CR0_PE)
 +				val |= X86_CR0_PE;
 +		}
 +
 +		if (cr0 ^ val)
 +			icpt_info.exit_code = SVM_EXIT_CR0_SEL_WRITE;
 +
 +		break;
 +	}
 +	case SVM_EXIT_READ_DR0:
 +	case SVM_EXIT_WRITE_DR0:
 +		icpt_info.exit_code += info->modrm_reg;
 +		break;
 +	case SVM_EXIT_MSR:
 +		if (info->intercept == x86_intercept_wrmsr)
 +			vmcb->control.exit_info_1 = 1;
 +		else
 +			vmcb->control.exit_info_1 = 0;
 +		break;
 +	case SVM_EXIT_PAUSE:
 +		/*
 +		 * We get this for NOP only, but pause
 +		 * is rep not, check this here
 +		 */
 +		if (info->rep_prefix != REPE_PREFIX)
 +			goto out;
 +		break;
 +	case SVM_EXIT_IOIO: {
 +		u64 exit_info;
 +		u32 bytes;
 +
 +		if (info->intercept == x86_intercept_in ||
 +		    info->intercept == x86_intercept_ins) {
 +			exit_info = ((info->src_val & 0xffff) << 16) |
 +				SVM_IOIO_TYPE_MASK;
 +			bytes = info->dst_bytes;
 +		} else {
 +			exit_info = (info->dst_val & 0xffff) << 16;
 +			bytes = info->src_bytes;
 +		}
 +
 +		if (info->intercept == x86_intercept_outs ||
 +		    info->intercept == x86_intercept_ins)
 +			exit_info |= SVM_IOIO_STR_MASK;
  
 -	ret = (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) ||
 -	      (svm->vcpu.arch.hflags & HF_NMI_MASK);
 +		if (info->rep_prefix)
 +			exit_info |= SVM_IOIO_REP_MASK;
 +
 +		bytes = min(bytes, 4u);
 +
 +		exit_info |= bytes << SVM_IOIO_SIZE_SHIFT;
 +
 +		exit_info |= (u32)info->ad_bytes << (SVM_IOIO_ASIZE_SHIFT - 1);
 +
 +		vmcb->control.exit_info_1 = exit_info;
 +		vmcb->control.exit_info_2 = info->next_rip;
 +
 +		break;
 +	}
 +	default:
 +		break;
 +	}
 +
 +	/* TODO: Advertise NRIPS to guest hypervisor unconditionally */
 +	if (static_cpu_has(X86_FEATURE_NRIPS))
 +		vmcb->control.next_rip  = info->next_rip;
 +	vmcb->control.exit_code = icpt_info.exit_code;
 +	vmexit = nested_svm_exit_handled(svm);
 +
 +	ret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED
 +					   : X86EMUL_CONTINUE;
  
 +out:
  	return ret;
  }
  
* Unmerged path arch/x86/kvm/svm/svm.c
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 17efa7f8e722..2da9a554c6f3 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -6725,6 +6725,11 @@ static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	 */
 	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
 
+	/*
+	 * Tell context tracking that this CPU is about to enter guest mode.
+	 */
+	guest_enter_irqoff();
+
 	/* L1D Flush includes CPU buffer clear to mitigate MDS */
 	if (static_branch_unlikely(&vmx_l1d_should_flush))
 		vmx_l1d_flush(vcpu);
@@ -6739,6 +6744,11 @@ static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.cr2 = read_cr2();
 
+	/*
+	 * Tell context tracking that this CPU is back.
+	 */
+	guest_exit_irqoff();
+
 	/*
 	 * We do not use IBRS in the kernel. If this vCPU has used the
 	 * SPEC_CTRL MSR it may have left it on; save the value and
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 603372755e5d..c10e65ed568f 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -8293,7 +8293,6 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	}
 
 	trace_kvm_entry(vcpu->vcpu_id);
-	guest_enter_irqoff();
 
 	fpregs_assert_state_consistent();
 	if (test_thread_flag(TIF_NEED_FPU_LOAD))
@@ -8356,7 +8355,6 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	local_irq_disable();
 	kvm_after_interrupt(vcpu);
 
-	guest_exit_irqoff();
 	if (lapic_in_kernel(vcpu)) {
 		s64 delta = vcpu->arch.apic->lapic_timer.advance_expire_delta;
 		if (delta != S64_MIN) {
