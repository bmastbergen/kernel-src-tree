net/mlx5e: Extract TC-specific code from en_rep.c to rep/tc.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Vlad Buslov <vladbu@mellanox.com>
commit 768c3667e6f36bc9db0dac854aa198651b27412f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/768c3667.failed

As a preparation for introducing new kconfig option that controls
compilation of all TC offloads code in mlx5, extract TC-specific code from
en_rep.c to standalone file. This allows easily compiling out the code by
only including new source in make file when corresponding kconfig is
enabled instead of adding multiple ifdef blocks to en_rep.

	Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 768c3667e6f36bc9db0dac854aa198651b27412f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/Makefile
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/Makefile
index 8d3b7a3aee92,c21453970dbb..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/Makefile
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Makefile
@@@ -32,9 -33,12 +32,18 @@@ mlx5_core-$(CONFIG_MLX5_CORE_EN) += en_
  mlx5_core-$(CONFIG_MLX5_EN_ARFS)     += en_arfs.o
  mlx5_core-$(CONFIG_MLX5_EN_RXNFC)    += en_fs_ethtool.o
  mlx5_core-$(CONFIG_MLX5_CORE_EN_DCB) += en_dcbnl.o en/port_buffer.o
++<<<<<<< HEAD
 +mlx5_core-$(CONFIG_MLX5_ESWITCH)     += en_rep.o en_tc.o en/tc_tun.o lib/port_tun.o lag_mp.o \
 +					lib/geneve.o en/tc_tun_vxlan.o en/tc_tun_gre.o \
 +					en/tc_tun_geneve.o
++=======
+ mlx5_core-$(CONFIG_MLX5_ESWITCH)     += en_rep.o en_tc.o en/rep/tc.o en/tc_tun.o lib/port_tun.o \
+ 					lag_mp.o \
+ 					lib/geneve.o en/mapping.o en/tc_tun_vxlan.o en/tc_tun_gre.o \
+ 					en/tc_tun_geneve.o diag/en_tc_tracepoint.o
+ mlx5_core-$(CONFIG_PCI_HYPERV_INTERFACE) += en/hv_vhca_stats.o
+ mlx5_core-$(CONFIG_MLX5_TC_CT)	     += en/tc_ct.o
++>>>>>>> 768c3667e6f3 (net/mlx5e: Extract TC-specific code from en_rep.c to rep/tc.c)
  
  #
  # Core extra
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index c30e19ff93cf,c84f0d9b516e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@@ -44,12 -44,15 +44,18 @@@
  #include "en.h"
  #include "en_rep.h"
  #include "en_tc.h"
- #include "en/tc_tun.h"
+ #include "en/rep/tc.h"
  #include "fs_core.h"
++<<<<<<< HEAD
 +#include "lib/port_tun.h"
++=======
+ #include "lib/mlx5.h"
+ #define CREATE_TRACE_POINTS
+ #include "diag/en_rep_tracepoint.h"
++>>>>>>> 768c3667e6f3 (net/mlx5e: Extract TC-specific code from en_rep.c to rep/tc.c)
  
  #define MLX5E_REP_PARAMS_DEF_LOG_SQ_SIZE \
 -        max(0x7, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE)
 +	max(0x7, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE)
  #define MLX5E_REP_PARAMS_DEF_NUM_CHANNELS 1
  
  static const char mlx5e_rep_driver_name[] = "mlx5e_rep";
@@@ -1213,130 -866,6 +861,133 @@@ static int mlx5e_rep_close(struct net_d
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int
 +mlx5e_rep_setup_tc_cls_flower(struct mlx5e_priv *priv,
 +			      struct flow_cls_offload *cls_flower, int flags)
 +{
 +	switch (cls_flower->command) {
 +	case FLOW_CLS_REPLACE:
 +		return mlx5e_configure_flower(priv->netdev, priv, cls_flower,
 +					      flags);
 +	case FLOW_CLS_DESTROY:
 +		return mlx5e_delete_flower(priv->netdev, priv, cls_flower,
 +					   flags);
 +	case FLOW_CLS_STATS:
 +		return mlx5e_stats_flower(priv->netdev, priv, cls_flower,
 +					  flags);
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
 +static
 +int mlx5e_rep_setup_tc_cls_matchall(struct mlx5e_priv *priv,
 +				    struct tc_cls_matchall_offload *ma)
 +{
 +	switch (ma->command) {
 +	case TC_CLSMATCHALL_REPLACE:
 +		return mlx5e_tc_configure_matchall(priv, ma);
 +	case TC_CLSMATCHALL_DESTROY:
 +		return mlx5e_tc_delete_matchall(priv, ma);
 +	case TC_CLSMATCHALL_STATS:
 +		mlx5e_tc_stats_matchall(priv, ma);
 +		return 0;
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
 +static int mlx5e_rep_setup_tc_cb(enum tc_setup_type type, void *type_data,
 +				 void *cb_priv)
 +{
 +	unsigned long flags = MLX5_TC_FLAG(INGRESS) | MLX5_TC_FLAG(ESW_OFFLOAD);
 +	struct mlx5e_priv *priv = cb_priv;
 +
 +	switch (type) {
 +	case TC_SETUP_CLSFLOWER:
 +		return mlx5e_rep_setup_tc_cls_flower(priv, type_data, flags);
 +	case TC_SETUP_CLSMATCHALL:
 +		return mlx5e_rep_setup_tc_cls_matchall(priv, type_data);
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
 +static int mlx5e_rep_setup_ft_cb(enum tc_setup_type type, void *type_data,
 +				 void *cb_priv)
 +{
 +	struct flow_cls_offload tmp, *f = type_data;
 +	struct mlx5e_priv *priv = cb_priv;
 +	struct mlx5_eswitch *esw;
 +	unsigned long flags;
 +	int err;
 +
 +	flags = MLX5_TC_FLAG(INGRESS) |
 +		MLX5_TC_FLAG(ESW_OFFLOAD) |
 +		MLX5_TC_FLAG(FT_OFFLOAD);
 +	esw = priv->mdev->priv.eswitch;
 +
 +	switch (type) {
 +	case TC_SETUP_CLSFLOWER:
 +		memcpy(&tmp, f, sizeof(*f));
 +
 +		if (!mlx5_eswitch_prios_supported(esw) ||
 +		    tmp.common.chain_index)
 +			return -EOPNOTSUPP;
 +
 +		/* Re-use tc offload path by moving the ft flow to the
 +		 * reserved ft chain.
 +		 *
 +		 * FT offload can use prio range [0, INT_MAX], so we
 +		 * normalize it to range [1, mlx5_eswitch_get_prio_range(esw)]
 +		 * as with tc, where prio 0 isn't supported.
 +		 *
 +		 * We only support chain 0 of FT offload.
 +		 */
 +		if (tmp.common.prio >= mlx5_eswitch_get_prio_range(esw))
 +			return -EOPNOTSUPP;
 +		if (tmp.common.chain_index != 0)
 +			return -EOPNOTSUPP;
 +
 +		tmp.common.chain_index = mlx5_eswitch_get_ft_chain(esw);
 +		tmp.common.prio++;
 +		err = mlx5e_rep_setup_tc_cls_flower(priv, &tmp, flags);
 +		memcpy(&f->stats, &tmp.stats, sizeof(f->stats));
 +		return err;
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
 +static LIST_HEAD(mlx5e_rep_block_tc_cb_list);
 +static LIST_HEAD(mlx5e_rep_block_ft_cb_list);
 +static int mlx5e_rep_setup_tc(struct net_device *dev, enum tc_setup_type type,
 +			      void *type_data)
 +{
 +	struct mlx5e_priv *priv = netdev_priv(dev);
 +	struct flow_block_offload *f = type_data;
 +
 +	f->unlocked_driver_cb = true;
 +
 +	switch (type) {
 +	case TC_SETUP_BLOCK:
 +		return flow_block_cb_setup_simple(type_data,
 +						  &mlx5e_rep_block_tc_cb_list,
 +						  mlx5e_rep_setup_tc_cb,
 +						  priv, priv, true);
 +	case TC_SETUP_FT:
 +		return flow_block_cb_setup_simple(type_data,
 +						  &mlx5e_rep_block_ft_cb_list,
 +						  mlx5e_rep_setup_ft_cb,
 +						  priv, priv, true);
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +}
 +
++=======
++>>>>>>> 768c3667e6f3 (net/mlx5e: Extract TC-specific code from en_rep.c to rep/tc.c)
  bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv)
  {
  	struct mlx5e_rep_priv *rpriv = priv->ppriv;
@@@ -1923,13 -1429,10 +1552,12 @@@ static void mlx5e_uplink_rep_enable(str
  static void mlx5e_uplink_rep_disable(struct mlx5e_priv *priv)
  {
  	struct mlx5_core_dev *mdev = priv->mdev;
- 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
  
 +#ifdef CONFIG_MLX5_CORE_EN_DCB
  	mlx5e_dcbnl_delete_app(priv);
 +#endif
  	mlx5_notifier_unregister(mdev, &priv->events_nb);
- 	cancel_work_sync(&rpriv->uplink_priv.reoffload_flows_work);
+ 	mlx5e_rep_tc_disable(priv);
  	mlx5_lag_remove(mdev);
  }
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index d358fdb49b84,251975ccbdf7..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -49,8 -49,10 +49,9 @@@
  #include <net/ipv6_stubs.h>
  #include "en.h"
  #include "en_rep.h"
+ #include "en/rep/tc.h"
  #include "en_tc.h"
  #include "eswitch.h"
 -#include "esw/chains.h"
  #include "fs_core.h"
  #include "en/port.h"
  #include "en/tc_tun.h"
@@@ -149,6 -159,87 +150,90 @@@ struct mlx5e_tc_flow_parse_attr 
  #define MLX5E_TC_TABLE_NUM_GROUPS 4
  #define MLX5E_TC_TABLE_MAX_GROUP_SIZE BIT(16)
  
++<<<<<<< HEAD
++=======
+ struct mlx5e_tc_attr_to_reg_mapping mlx5e_tc_attr_to_reg_mappings[] = {
+ 	[CHAIN_TO_REG] = {
+ 		.mfield = MLX5_ACTION_IN_FIELD_METADATA_REG_C_0,
+ 		.moffset = 0,
+ 		.mlen = 2,
+ 	},
+ 	[TUNNEL_TO_REG] = {
+ 		.mfield = MLX5_ACTION_IN_FIELD_METADATA_REG_C_1,
+ 		.moffset = 3,
+ 		.mlen = 1,
+ 		.soffset = MLX5_BYTE_OFF(fte_match_param,
+ 					 misc_parameters_2.metadata_reg_c_1),
+ 	},
+ 	[ZONE_TO_REG] = zone_to_reg_ct,
+ 	[CTSTATE_TO_REG] = ctstate_to_reg_ct,
+ 	[MARK_TO_REG] = mark_to_reg_ct,
+ 	[LABELS_TO_REG] = labels_to_reg_ct,
+ 	[FTEID_TO_REG] = fteid_to_reg_ct,
+ 	[TUPLEID_TO_REG] = tupleid_to_reg_ct,
+ };
+ 
+ static void mlx5e_put_flow_tunnel_id(struct mlx5e_tc_flow *flow);
+ 
+ void
+ mlx5e_tc_match_to_reg_match(struct mlx5_flow_spec *spec,
+ 			    enum mlx5e_tc_attr_to_reg type,
+ 			    u32 data,
+ 			    u32 mask)
+ {
+ 	int soffset = mlx5e_tc_attr_to_reg_mappings[type].soffset;
+ 	int match_len = mlx5e_tc_attr_to_reg_mappings[type].mlen;
+ 	void *headers_c = spec->match_criteria;
+ 	void *headers_v = spec->match_value;
+ 	void *fmask, *fval;
+ 
+ 	fmask = headers_c + soffset;
+ 	fval = headers_v + soffset;
+ 
+ 	mask = cpu_to_be32(mask) >> (32 - (match_len * 8));
+ 	data = cpu_to_be32(data) >> (32 - (match_len * 8));
+ 
+ 	memcpy(fmask, &mask, match_len);
+ 	memcpy(fval, &data, match_len);
+ 
+ 	spec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS_2;
+ }
+ 
+ int
+ mlx5e_tc_match_to_reg_set(struct mlx5_core_dev *mdev,
+ 			  struct mlx5e_tc_mod_hdr_acts *mod_hdr_acts,
+ 			  enum mlx5e_tc_attr_to_reg type,
+ 			  u32 data)
+ {
+ 	int moffset = mlx5e_tc_attr_to_reg_mappings[type].moffset;
+ 	int mfield = mlx5e_tc_attr_to_reg_mappings[type].mfield;
+ 	int mlen = mlx5e_tc_attr_to_reg_mappings[type].mlen;
+ 	char *modact;
+ 	int err;
+ 
+ 	err = alloc_mod_hdr_actions(mdev, MLX5_FLOW_NAMESPACE_FDB,
+ 				    mod_hdr_acts);
+ 	if (err)
+ 		return err;
+ 
+ 	modact = mod_hdr_acts->actions +
+ 		 (mod_hdr_acts->num_actions * MLX5_MH_ACT_SZ);
+ 
+ 	/* Firmware has 5bit length field and 0 means 32bits */
+ 	if (mlen == 4)
+ 		mlen = 0;
+ 
+ 	MLX5_SET(set_action_in, modact, action_type, MLX5_ACTION_TYPE_SET);
+ 	MLX5_SET(set_action_in, modact, field, mfield);
+ 	MLX5_SET(set_action_in, modact, offset, moffset * 8);
+ 	MLX5_SET(set_action_in, modact, length, mlen * 8);
+ 	MLX5_SET(set_action_in, modact, data, data);
+ 	mod_hdr_acts->num_actions++;
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 768c3667e6f3 (net/mlx5e: Extract TC-specific code from en_rep.c to rep/tc.c)
  struct mlx5e_hairpin {
  	struct mlx5_hairpin *pair;
  
@@@ -4225,135 -4778,6 +4310,138 @@@ void mlx5e_tc_reoffload_flows_work(stru
  	mutex_unlock(&rpriv->unready_flows_lock);
  }
  
++<<<<<<< HEAD
 +#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)
 +static bool mlx5e_restore_tunnel(struct mlx5e_priv *priv, struct sk_buff *skb,
 +				 struct mlx5e_tc_update_priv *tc_priv,
 +				 u32 tunnel_id)
 +{
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct flow_dissector_key_enc_opts enc_opts = {};
 +	struct mlx5_rep_uplink_priv *uplink_priv;
 +	struct mlx5e_rep_priv *uplink_rpriv;
 +	struct metadata_dst *tun_dst;
 +	struct tunnel_match_key key;
 +	u32 tun_id, enc_opts_id;
 +	struct net_device *dev;
 +	int err;
 +
 +	enc_opts_id = tunnel_id & ENC_OPTS_BITS_MASK;
 +	tun_id = tunnel_id >> ENC_OPTS_BITS;
 +
 +	if (!tun_id)
 +		return true;
 +
 +	uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
 +	uplink_priv = &uplink_rpriv->uplink_priv;
 +
 +	err = mapping_find(uplink_priv->tunnel_mapping, tun_id, &key);
 +	if (err) {
 +		WARN_ON_ONCE(true);
 +		netdev_dbg(priv->netdev,
 +			   "Couldn't find tunnel for tun_id: %d, err: %d\n",
 +			   tun_id, err);
 +		return false;
 +	}
 +
 +	if (enc_opts_id) {
 +		err = mapping_find(uplink_priv->tunnel_enc_opts_mapping,
 +				   enc_opts_id, &enc_opts);
 +		if (err) {
 +			netdev_dbg(priv->netdev,
 +				   "Couldn't find tunnel (opts) for tun_id: %d, err: %d\n",
 +				   enc_opts_id, err);
 +			return false;
 +		}
 +	}
 +
 +	tun_dst = tun_rx_dst(enc_opts.len);
 +	if (!tun_dst) {
 +		WARN_ON_ONCE(true);
 +		return false;
 +	}
 +
 +	ip_tunnel_key_init(&tun_dst->u.tun_info.key,
 +			   key.enc_ipv4.src, key.enc_ipv4.dst,
 +			   key.enc_ip.tos, key.enc_ip.ttl,
 +			   0, /* label */
 +			   key.enc_tp.src, key.enc_tp.dst,
 +			   key32_to_tunnel_id(key.enc_key_id.keyid),
 +			   TUNNEL_KEY);
 +
 +	if (enc_opts.len)
 +		ip_tunnel_info_opts_set(&tun_dst->u.tun_info, enc_opts.data,
 +					enc_opts.len, enc_opts.dst_opt_type);
 +
 +	skb_dst_set(skb, (struct dst_entry *)tun_dst);
 +	dev = dev_get_by_index(&init_net, key.filter_ifindex);
 +	if (!dev) {
 +		netdev_dbg(priv->netdev,
 +			   "Couldn't find tunnel device with ifindex: %d\n",
 +			   key.filter_ifindex);
 +		return false;
 +	}
 +
 +	/* Set tun_dev so we do dev_put() after datapath */
 +	tc_priv->tun_dev = dev;
 +
 +	skb->dev = dev;
 +
 +	return true;
 +}
 +#endif /* CONFIG_NET_TC_SKB_EXT */
 +
 +bool mlx5e_tc_rep_update_skb(struct mlx5_cqe64 *cqe,
 +			     struct sk_buff *skb,
 +			     struct mlx5e_tc_update_priv *tc_priv)
 +{
 +#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)
 +	u32 chain = 0, reg_c0, reg_c1, tunnel_id;
 +	struct tc_skb_ext *tc_skb_ext;
 +	struct mlx5_eswitch *esw;
 +	struct mlx5e_priv *priv;
 +	int tunnel_moffset;
 +	int err;
 +
 +	reg_c0 = (be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK);
 +	if (reg_c0 == MLX5_FS_DEFAULT_FLOW_TAG)
 +		reg_c0 = 0;
 +	reg_c1 = be32_to_cpu(cqe->ft_metadata);
 +
 +	if (!reg_c0)
 +		return true;
 +
 +	priv = netdev_priv(skb->dev);
 +	esw = priv->mdev->priv.eswitch;
 +
 +	err = mlx5_eswitch_get_chain_for_tag(esw, reg_c0, &chain);
 +	if (err) {
 +		netdev_dbg(priv->netdev,
 +			   "Couldn't find chain for chain tag: %d, err: %d\n",
 +			   reg_c0, err);
 +		return false;
 +	}
 +
 +	if (chain) {
 +		tc_skb_ext = skb_ext_add(skb, TC_SKB_EXT);
 +		if (!tc_skb_ext) {
 +			WARN_ON(1);
 +			return false;
 +		}
 +
 +		tc_skb_ext->chain = chain;
 +	}
 +
 +	tunnel_moffset = mlx5e_tc_attr_to_reg_mappings[TUNNEL_TO_REG].moffset;
 +	tunnel_id = reg_c1 >> (8 * tunnel_moffset);
 +	return mlx5e_restore_tunnel(priv, skb, tc_priv, tunnel_id);
 +#endif /* CONFIG_NET_TC_SKB_EXT */
 +
 +	return true;
 +}
 +
++=======
++>>>>>>> 768c3667e6f3 (net/mlx5e: Extract TC-specific code from en_rep.c to rep/tc.c)
  void mlx5_tc_rep_post_napi_receive(struct mlx5e_tc_update_priv *tc_priv)
  {
  	if (tc_priv->tun_dev)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/Makefile
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c
new file mode 100644
index 000000000000..edc574582135
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c
@@ -0,0 +1,710 @@
+// SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB
+/* Copyright (c) 2020 Mellanox Technologies. */
+
+#include <net/dst_metadata.h>
+#include <linux/netdevice.h>
+#include <linux/list.h>
+#include <linux/rculist.h>
+#include <linux/rtnetlink.h>
+#include <linux/workqueue.h>
+#include <linux/spinlock.h>
+#include "tc.h"
+#include "en_rep.h"
+#include "eswitch.h"
+#include "esw/chains.h"
+#include "en/tc_ct.h"
+#include "en/mapping.h"
+#include "en/tc_tun.h"
+#include "lib/port_tun.h"
+
+struct mlx5e_rep_indr_block_priv {
+	struct net_device *netdev;
+	struct mlx5e_rep_priv *rpriv;
+
+	struct list_head list;
+};
+
+int mlx5e_rep_encap_entry_attach(struct mlx5e_priv *priv,
+				 struct mlx5e_encap_entry *e)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+	struct mlx5_rep_uplink_priv *uplink_priv = &rpriv->uplink_priv;
+	struct mlx5_tun_entropy *tun_entropy = &uplink_priv->tun_entropy;
+	struct mlx5e_neigh_hash_entry *nhe;
+	int err;
+
+	err = mlx5_tun_entropy_refcount_inc(tun_entropy, e->reformat_type);
+	if (err)
+		return err;
+
+	mutex_lock(&rpriv->neigh_update.encap_lock);
+	nhe = mlx5e_rep_neigh_entry_lookup(priv, &e->m_neigh);
+	if (!nhe) {
+		err = mlx5e_rep_neigh_entry_create(priv, e, &nhe);
+		if (err) {
+			mutex_unlock(&rpriv->neigh_update.encap_lock);
+			mlx5_tun_entropy_refcount_dec(tun_entropy,
+						      e->reformat_type);
+			return err;
+		}
+	}
+
+	e->nhe = nhe;
+	spin_lock(&nhe->encap_list_lock);
+	list_add_rcu(&e->encap_list, &nhe->encap_list);
+	spin_unlock(&nhe->encap_list_lock);
+
+	mutex_unlock(&rpriv->neigh_update.encap_lock);
+
+	return 0;
+}
+
+void mlx5e_rep_encap_entry_detach(struct mlx5e_priv *priv,
+				  struct mlx5e_encap_entry *e)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+	struct mlx5_rep_uplink_priv *uplink_priv = &rpriv->uplink_priv;
+	struct mlx5_tun_entropy *tun_entropy = &uplink_priv->tun_entropy;
+
+	if (!e->nhe)
+		return;
+
+	spin_lock(&e->nhe->encap_list_lock);
+	list_del_rcu(&e->encap_list);
+	spin_unlock(&e->nhe->encap_list_lock);
+
+	mlx5e_rep_neigh_entry_release(e->nhe);
+	e->nhe = NULL;
+	mlx5_tun_entropy_refcount_dec(tun_entropy, e->reformat_type);
+}
+
+void mlx5e_rep_update_flows(struct mlx5e_priv *priv,
+			    struct mlx5e_encap_entry *e,
+			    bool neigh_connected,
+			    unsigned char ha[ETH_ALEN])
+{
+	struct ethhdr *eth = (struct ethhdr *)e->encap_header;
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+	bool encap_connected;
+	LIST_HEAD(flow_list);
+
+	ASSERT_RTNL();
+
+	/* wait for encap to be fully initialized */
+	wait_for_completion(&e->res_ready);
+
+	mutex_lock(&esw->offloads.encap_tbl_lock);
+	encap_connected = !!(e->flags & MLX5_ENCAP_ENTRY_VALID);
+	if (e->compl_result < 0 || (encap_connected == neigh_connected &&
+				    ether_addr_equal(e->h_dest, ha)))
+		goto unlock;
+
+	mlx5e_take_all_encap_flows(e, &flow_list);
+
+	if ((e->flags & MLX5_ENCAP_ENTRY_VALID) &&
+	    (!neigh_connected || !ether_addr_equal(e->h_dest, ha)))
+		mlx5e_tc_encap_flows_del(priv, e, &flow_list);
+
+	if (neigh_connected && !(e->flags & MLX5_ENCAP_ENTRY_VALID)) {
+		ether_addr_copy(e->h_dest, ha);
+		ether_addr_copy(eth->h_dest, ha);
+		/* Update the encap source mac, in case that we delete
+		 * the flows when encap source mac changed.
+		 */
+		ether_addr_copy(eth->h_source, e->route_dev->dev_addr);
+
+		mlx5e_tc_encap_flows_add(priv, e, &flow_list);
+	}
+unlock:
+	mutex_unlock(&esw->offloads.encap_tbl_lock);
+	mlx5e_put_encap_flow_list(priv, &flow_list);
+}
+
+static int
+mlx5e_rep_setup_tc_cls_flower(struct mlx5e_priv *priv,
+			      struct flow_cls_offload *cls_flower, int flags)
+{
+	switch (cls_flower->command) {
+	case FLOW_CLS_REPLACE:
+		return mlx5e_configure_flower(priv->netdev, priv, cls_flower,
+					      flags);
+	case FLOW_CLS_DESTROY:
+		return mlx5e_delete_flower(priv->netdev, priv, cls_flower,
+					   flags);
+	case FLOW_CLS_STATS:
+		return mlx5e_stats_flower(priv->netdev, priv, cls_flower,
+					  flags);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static
+int mlx5e_rep_setup_tc_cls_matchall(struct mlx5e_priv *priv,
+				    struct tc_cls_matchall_offload *ma)
+{
+	switch (ma->command) {
+	case TC_CLSMATCHALL_REPLACE:
+		return mlx5e_tc_configure_matchall(priv, ma);
+	case TC_CLSMATCHALL_DESTROY:
+		return mlx5e_tc_delete_matchall(priv, ma);
+	case TC_CLSMATCHALL_STATS:
+		mlx5e_tc_stats_matchall(priv, ma);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mlx5e_rep_setup_tc_cb(enum tc_setup_type type, void *type_data,
+				 void *cb_priv)
+{
+	unsigned long flags = MLX5_TC_FLAG(INGRESS) | MLX5_TC_FLAG(ESW_OFFLOAD);
+	struct mlx5e_priv *priv = cb_priv;
+
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		return mlx5e_rep_setup_tc_cls_flower(priv, type_data, flags);
+	case TC_SETUP_CLSMATCHALL:
+		return mlx5e_rep_setup_tc_cls_matchall(priv, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mlx5e_rep_setup_ft_cb(enum tc_setup_type type, void *type_data,
+				 void *cb_priv)
+{
+	struct flow_cls_offload tmp, *f = type_data;
+	struct mlx5e_priv *priv = cb_priv;
+	struct mlx5_eswitch *esw;
+	unsigned long flags;
+	int err;
+
+	flags = MLX5_TC_FLAG(INGRESS) |
+		MLX5_TC_FLAG(ESW_OFFLOAD) |
+		MLX5_TC_FLAG(FT_OFFLOAD);
+	esw = priv->mdev->priv.eswitch;
+
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		memcpy(&tmp, f, sizeof(*f));
+
+		if (!mlx5_esw_chains_prios_supported(esw))
+			return -EOPNOTSUPP;
+
+		/* Re-use tc offload path by moving the ft flow to the
+		 * reserved ft chain.
+		 *
+		 * FT offload can use prio range [0, INT_MAX], so we normalize
+		 * it to range [1, mlx5_esw_chains_get_prio_range(esw)]
+		 * as with tc, where prio 0 isn't supported.
+		 *
+		 * We only support chain 0 of FT offload.
+		 */
+		if (tmp.common.prio >= mlx5_esw_chains_get_prio_range(esw))
+			return -EOPNOTSUPP;
+		if (tmp.common.chain_index != 0)
+			return -EOPNOTSUPP;
+
+		tmp.common.chain_index = mlx5_esw_chains_get_ft_chain(esw);
+		tmp.common.prio++;
+		err = mlx5e_rep_setup_tc_cls_flower(priv, &tmp, flags);
+		memcpy(&f->stats, &tmp.stats, sizeof(f->stats));
+		return err;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static LIST_HEAD(mlx5e_rep_block_tc_cb_list);
+static LIST_HEAD(mlx5e_rep_block_ft_cb_list);
+int mlx5e_rep_setup_tc(struct net_device *dev, enum tc_setup_type type,
+		       void *type_data)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+	struct flow_block_offload *f = type_data;
+
+	f->unlocked_driver_cb = true;
+
+	switch (type) {
+	case TC_SETUP_BLOCK:
+		return flow_block_cb_setup_simple(type_data,
+						  &mlx5e_rep_block_tc_cb_list,
+						  mlx5e_rep_setup_tc_cb,
+						  priv, priv, true);
+	case TC_SETUP_FT:
+		return flow_block_cb_setup_simple(type_data,
+						  &mlx5e_rep_block_ft_cb_list,
+						  mlx5e_rep_setup_ft_cb,
+						  priv, priv, true);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+int mlx5e_rep_tc_init(struct mlx5e_rep_priv *rpriv)
+{
+	struct mlx5_rep_uplink_priv *uplink_priv = &rpriv->uplink_priv;
+	int err;
+
+	mutex_init(&uplink_priv->unready_flows_lock);
+	INIT_LIST_HEAD(&uplink_priv->unready_flows);
+
+	/* init shared tc flow table */
+	err = mlx5e_tc_esw_init(&uplink_priv->tc_ht);
+	return err;
+}
+
+void mlx5e_rep_tc_cleanup(struct mlx5e_rep_priv *rpriv)
+{
+	/* delete shared tc flow table */
+	mlx5e_tc_esw_cleanup(&rpriv->uplink_priv.tc_ht);
+	mutex_destroy(&rpriv->uplink_priv.unready_flows_lock);
+}
+
+void mlx5e_rep_tc_enable(struct mlx5e_priv *priv)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+
+	INIT_WORK(&rpriv->uplink_priv.reoffload_flows_work,
+		  mlx5e_tc_reoffload_flows_work);
+}
+
+void mlx5e_rep_tc_disable(struct mlx5e_priv *priv)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+
+	cancel_work_sync(&rpriv->uplink_priv.reoffload_flows_work);
+}
+
+int mlx5e_rep_tc_event_port_affinity(struct mlx5e_priv *priv)
+{
+	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+
+	queue_work(priv->wq, &rpriv->uplink_priv.reoffload_flows_work);
+
+	return NOTIFY_OK;
+}
+
+static struct mlx5e_rep_indr_block_priv *
+mlx5e_rep_indr_block_priv_lookup(struct mlx5e_rep_priv *rpriv,
+				 struct net_device *netdev)
+{
+	struct mlx5e_rep_indr_block_priv *cb_priv;
+
+	/* All callback list access should be protected by RTNL. */
+	ASSERT_RTNL();
+
+	list_for_each_entry(cb_priv,
+			    &rpriv->uplink_priv.tc_indr_block_priv_list,
+			    list)
+		if (cb_priv->netdev == netdev)
+			return cb_priv;
+
+	return NULL;
+}
+
+static void mlx5e_rep_indr_unregister_block(struct mlx5e_rep_priv *rpriv,
+					    struct net_device *netdev);
+
+void mlx5e_rep_indr_clean_block_privs(struct mlx5e_rep_priv *rpriv)
+{
+	struct mlx5e_rep_indr_block_priv *cb_priv, *temp;
+	struct list_head *head = &rpriv->uplink_priv.tc_indr_block_priv_list;
+
+	list_for_each_entry_safe(cb_priv, temp, head, list) {
+		mlx5e_rep_indr_unregister_block(rpriv, cb_priv->netdev);
+		kfree(cb_priv);
+	}
+}
+
+static int
+mlx5e_rep_indr_offload(struct net_device *netdev,
+		       struct flow_cls_offload *flower,
+		       struct mlx5e_rep_indr_block_priv *indr_priv,
+		       unsigned long flags)
+{
+	struct mlx5e_priv *priv = netdev_priv(indr_priv->rpriv->netdev);
+	int err = 0;
+
+	switch (flower->command) {
+	case FLOW_CLS_REPLACE:
+		err = mlx5e_configure_flower(netdev, priv, flower, flags);
+		break;
+	case FLOW_CLS_DESTROY:
+		err = mlx5e_delete_flower(netdev, priv, flower, flags);
+		break;
+	case FLOW_CLS_STATS:
+		err = mlx5e_stats_flower(netdev, priv, flower, flags);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static int mlx5e_rep_indr_setup_tc_cb(enum tc_setup_type type,
+				      void *type_data, void *indr_priv)
+{
+	unsigned long flags = MLX5_TC_FLAG(EGRESS) | MLX5_TC_FLAG(ESW_OFFLOAD);
+	struct mlx5e_rep_indr_block_priv *priv = indr_priv;
+
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		return mlx5e_rep_indr_offload(priv->netdev, type_data, priv,
+					      flags);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mlx5e_rep_indr_setup_ft_cb(enum tc_setup_type type,
+				      void *type_data, void *indr_priv)
+{
+	struct mlx5e_rep_indr_block_priv *priv = indr_priv;
+	struct flow_cls_offload *f = type_data;
+	struct flow_cls_offload tmp;
+	struct mlx5e_priv *mpriv;
+	struct mlx5_eswitch *esw;
+	unsigned long flags;
+	int err;
+
+	mpriv = netdev_priv(priv->rpriv->netdev);
+	esw = mpriv->mdev->priv.eswitch;
+
+	flags = MLX5_TC_FLAG(EGRESS) |
+		MLX5_TC_FLAG(ESW_OFFLOAD) |
+		MLX5_TC_FLAG(FT_OFFLOAD);
+
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		memcpy(&tmp, f, sizeof(*f));
+
+		/* Re-use tc offload path by moving the ft flow to the
+		 * reserved ft chain.
+		 *
+		 * FT offload can use prio range [0, INT_MAX], so we normalize
+		 * it to range [1, mlx5_esw_chains_get_prio_range(esw)]
+		 * as with tc, where prio 0 isn't supported.
+		 *
+		 * We only support chain 0 of FT offload.
+		 */
+		if (!mlx5_esw_chains_prios_supported(esw) ||
+		    tmp.common.prio >= mlx5_esw_chains_get_prio_range(esw) ||
+		    tmp.common.chain_index)
+			return -EOPNOTSUPP;
+
+		tmp.common.chain_index = mlx5_esw_chains_get_ft_chain(esw);
+		tmp.common.prio++;
+		err = mlx5e_rep_indr_offload(priv->netdev, &tmp, priv, flags);
+		memcpy(&f->stats, &tmp.stats, sizeof(f->stats));
+		return err;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void mlx5e_rep_indr_block_unbind(void *cb_priv)
+{
+	struct mlx5e_rep_indr_block_priv *indr_priv = cb_priv;
+
+	list_del(&indr_priv->list);
+	kfree(indr_priv);
+}
+
+static LIST_HEAD(mlx5e_block_cb_list);
+
+static int
+mlx5e_rep_indr_setup_block(struct net_device *netdev,
+			   struct mlx5e_rep_priv *rpriv,
+			   struct flow_block_offload *f,
+			   flow_setup_cb_t *setup_cb)
+{
+	struct mlx5e_rep_indr_block_priv *indr_priv;
+	struct flow_block_cb *block_cb;
+
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	f->unlocked_driver_cb = true;
+	f->driver_block_list = &mlx5e_block_cb_list;
+
+	switch (f->command) {
+	case FLOW_BLOCK_BIND:
+		indr_priv = mlx5e_rep_indr_block_priv_lookup(rpriv, netdev);
+		if (indr_priv)
+			return -EEXIST;
+
+		indr_priv = kmalloc(sizeof(*indr_priv), GFP_KERNEL);
+		if (!indr_priv)
+			return -ENOMEM;
+
+		indr_priv->netdev = netdev;
+		indr_priv->rpriv = rpriv;
+		list_add(&indr_priv->list,
+			 &rpriv->uplink_priv.tc_indr_block_priv_list);
+
+		block_cb = flow_block_cb_alloc(setup_cb, indr_priv, indr_priv,
+					       mlx5e_rep_indr_block_unbind);
+		if (IS_ERR(block_cb)) {
+			list_del(&indr_priv->list);
+			kfree(indr_priv);
+			return PTR_ERR(block_cb);
+		}
+		flow_block_cb_add(block_cb, f);
+		list_add_tail(&block_cb->driver_list, &mlx5e_block_cb_list);
+
+		return 0;
+	case FLOW_BLOCK_UNBIND:
+		indr_priv = mlx5e_rep_indr_block_priv_lookup(rpriv, netdev);
+		if (!indr_priv)
+			return -ENOENT;
+
+		block_cb = flow_block_cb_lookup(f->block, setup_cb, indr_priv);
+		if (!block_cb)
+			return -ENOENT;
+
+		flow_block_cb_remove(block_cb, f);
+		list_del(&block_cb->driver_list);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+	return 0;
+}
+
+static
+int mlx5e_rep_indr_setup_cb(struct net_device *netdev, void *cb_priv,
+			    enum tc_setup_type type, void *type_data)
+{
+	switch (type) {
+	case TC_SETUP_BLOCK:
+		return mlx5e_rep_indr_setup_block(netdev, cb_priv, type_data,
+						  mlx5e_rep_indr_setup_tc_cb);
+	case TC_SETUP_FT:
+		return mlx5e_rep_indr_setup_block(netdev, cb_priv, type_data,
+						  mlx5e_rep_indr_setup_ft_cb);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int mlx5e_rep_indr_register_block(struct mlx5e_rep_priv *rpriv,
+					 struct net_device *netdev)
+{
+	int err;
+
+	err = __flow_indr_block_cb_register(netdev, rpriv,
+					    mlx5e_rep_indr_setup_cb,
+					    rpriv);
+	if (err) {
+		struct mlx5e_priv *priv = netdev_priv(rpriv->netdev);
+
+		mlx5_core_err(priv->mdev, "Failed to register remote block notifier for %s err=%d\n",
+			      netdev_name(netdev), err);
+	}
+	return err;
+}
+
+static void mlx5e_rep_indr_unregister_block(struct mlx5e_rep_priv *rpriv,
+					    struct net_device *netdev)
+{
+	__flow_indr_block_cb_unregister(netdev, mlx5e_rep_indr_setup_cb,
+					rpriv);
+}
+
+static int mlx5e_nic_rep_netdevice_event(struct notifier_block *nb,
+					 unsigned long event, void *ptr)
+{
+	struct mlx5e_rep_priv *rpriv = container_of(nb, struct mlx5e_rep_priv,
+						     uplink_priv.netdevice_nb);
+	struct mlx5e_priv *priv = netdev_priv(rpriv->netdev);
+	struct net_device *netdev = netdev_notifier_info_to_dev(ptr);
+
+	if (!mlx5e_tc_tun_device_to_offload(priv, netdev) &&
+	    !(is_vlan_dev(netdev) && vlan_dev_real_dev(netdev) == rpriv->netdev))
+		return NOTIFY_OK;
+
+	switch (event) {
+	case NETDEV_REGISTER:
+		mlx5e_rep_indr_register_block(rpriv, netdev);
+		break;
+	case NETDEV_UNREGISTER:
+		mlx5e_rep_indr_unregister_block(rpriv, netdev);
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+int mlx5e_rep_tc_netdevice_event_register(struct mlx5e_rep_priv *rpriv)
+{
+	struct mlx5_rep_uplink_priv *uplink_priv = &rpriv->uplink_priv;
+	int err;
+
+	/* init indirect block notifications */
+	INIT_LIST_HEAD(&uplink_priv->tc_indr_block_priv_list);
+
+	uplink_priv->netdevice_nb.notifier_call = mlx5e_nic_rep_netdevice_event;
+	err = register_netdevice_notifier_dev_net(rpriv->netdev,
+						  &uplink_priv->netdevice_nb,
+						  &uplink_priv->netdevice_nn);
+	return err;
+}
+
+void mlx5e_rep_tc_netdevice_event_unregister(struct mlx5e_rep_priv *rpriv)
+{
+	struct mlx5_rep_uplink_priv *uplink_priv = &rpriv->uplink_priv;
+
+	/* clean indirect TC block notifications */
+	unregister_netdevice_notifier_dev_net(rpriv->netdev,
+					      &uplink_priv->netdevice_nb,
+					      &uplink_priv->netdevice_nn);
+}
+
+#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)
+static bool mlx5e_restore_tunnel(struct mlx5e_priv *priv, struct sk_buff *skb,
+				 struct mlx5e_tc_update_priv *tc_priv,
+				 u32 tunnel_id)
+{
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+	struct tunnel_match_enc_opts enc_opts = {};
+	struct mlx5_rep_uplink_priv *uplink_priv;
+	struct mlx5e_rep_priv *uplink_rpriv;
+	struct metadata_dst *tun_dst;
+	struct tunnel_match_key key;
+	u32 tun_id, enc_opts_id;
+	struct net_device *dev;
+	int err;
+
+	enc_opts_id = tunnel_id & ENC_OPTS_BITS_MASK;
+	tun_id = tunnel_id >> ENC_OPTS_BITS;
+
+	if (!tun_id)
+		return true;
+
+	uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
+	uplink_priv = &uplink_rpriv->uplink_priv;
+
+	err = mapping_find(uplink_priv->tunnel_mapping, tun_id, &key);
+	if (err) {
+		WARN_ON_ONCE(true);
+		netdev_dbg(priv->netdev,
+			   "Couldn't find tunnel for tun_id: %d, err: %d\n",
+			   tun_id, err);
+		return false;
+	}
+
+	if (enc_opts_id) {
+		err = mapping_find(uplink_priv->tunnel_enc_opts_mapping,
+				   enc_opts_id, &enc_opts);
+		if (err) {
+			netdev_dbg(priv->netdev,
+				   "Couldn't find tunnel (opts) for tun_id: %d, err: %d\n",
+				   enc_opts_id, err);
+			return false;
+		}
+	}
+
+	tun_dst = tun_rx_dst(enc_opts.key.len);
+	if (!tun_dst) {
+		WARN_ON_ONCE(true);
+		return false;
+	}
+
+	ip_tunnel_key_init(&tun_dst->u.tun_info.key,
+			   key.enc_ipv4.src, key.enc_ipv4.dst,
+			   key.enc_ip.tos, key.enc_ip.ttl,
+			   0, /* label */
+			   key.enc_tp.src, key.enc_tp.dst,
+			   key32_to_tunnel_id(key.enc_key_id.keyid),
+			   TUNNEL_KEY);
+
+	if (enc_opts.key.len)
+		ip_tunnel_info_opts_set(&tun_dst->u.tun_info,
+					enc_opts.key.data,
+					enc_opts.key.len,
+					enc_opts.key.dst_opt_type);
+
+	skb_dst_set(skb, (struct dst_entry *)tun_dst);
+	dev = dev_get_by_index(&init_net, key.filter_ifindex);
+	if (!dev) {
+		netdev_dbg(priv->netdev,
+			   "Couldn't find tunnel device with ifindex: %d\n",
+			   key.filter_ifindex);
+		return false;
+	}
+
+	/* Set tun_dev so we do dev_put() after datapath */
+	tc_priv->tun_dev = dev;
+
+	skb->dev = dev;
+
+	return true;
+}
+#endif /* CONFIG_NET_TC_SKB_EXT */
+
+bool mlx5e_rep_tc_update_skb(struct mlx5_cqe64 *cqe,
+			     struct sk_buff *skb,
+			     struct mlx5e_tc_update_priv *tc_priv)
+{
+#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)
+	u32 chain = 0, reg_c0, reg_c1, tunnel_id, tuple_id;
+	struct mlx5_rep_uplink_priv *uplink_priv;
+	struct mlx5e_rep_priv *uplink_rpriv;
+	struct tc_skb_ext *tc_skb_ext;
+	struct mlx5_eswitch *esw;
+	struct mlx5e_priv *priv;
+	int tunnel_moffset;
+	int err;
+
+	reg_c0 = (be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK);
+	if (reg_c0 == MLX5_FS_DEFAULT_FLOW_TAG)
+		reg_c0 = 0;
+	reg_c1 = be32_to_cpu(cqe->ft_metadata);
+
+	if (!reg_c0)
+		return true;
+
+	priv = netdev_priv(skb->dev);
+	esw = priv->mdev->priv.eswitch;
+
+	err = mlx5_eswitch_get_chain_for_tag(esw, reg_c0, &chain);
+	if (err) {
+		netdev_dbg(priv->netdev,
+			   "Couldn't find chain for chain tag: %d, err: %d\n",
+			   reg_c0, err);
+		return false;
+	}
+
+	if (chain) {
+		tc_skb_ext = skb_ext_add(skb, TC_SKB_EXT);
+		if (!tc_skb_ext) {
+			WARN_ON(1);
+			return false;
+		}
+
+		tc_skb_ext->chain = chain;
+
+		tuple_id = reg_c1 & TUPLE_ID_MAX;
+
+		uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
+		uplink_priv = &uplink_rpriv->uplink_priv;
+		if (!mlx5e_tc_ct_restore_flow(uplink_priv, skb, tuple_id))
+			return false;
+	}
+
+	tunnel_moffset = mlx5e_tc_attr_to_reg_mappings[TUNNEL_TO_REG].moffset;
+	tunnel_id = reg_c1 >> (8 * tunnel_moffset);
+	return mlx5e_restore_tunnel(priv, skb, tc_priv, tunnel_id);
+#endif /* CONFIG_NET_TC_SKB_EXT */
+
+	return true;
+}
+
+void mlx5_rep_tc_post_napi_receive(struct mlx5e_tc_update_priv *tc_priv)
+{
+	if (tc_priv->tun_dev)
+		dev_put(tc_priv->tun_dev);
+}
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.h b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.h
new file mode 100644
index 000000000000..90da00626b97
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.h
@@ -0,0 +1,45 @@
+/* SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB */
+/* Copyright (c) 2020 Mellanox Technologies. */
+
+#ifndef __MLX5_EN_REP_TC_H__
+#define __MLX5_EN_REP_TC_H__
+
+#include <linux/skbuff.h>
+#include "en.h"
+#include "en_tc.h"
+#include "en_rep.h"
+
+struct mlx5e_rep_priv;
+int mlx5e_rep_tc_init(struct mlx5e_rep_priv *rpriv);
+void mlx5e_rep_tc_cleanup(struct mlx5e_rep_priv *rpriv);
+
+int mlx5e_rep_tc_netdevice_event_register(struct mlx5e_rep_priv *rpriv);
+void mlx5e_rep_tc_netdevice_event_unregister(struct mlx5e_rep_priv *rpriv);
+
+void mlx5e_rep_tc_enable(struct mlx5e_priv *priv);
+void mlx5e_rep_tc_disable(struct mlx5e_priv *priv);
+
+int mlx5e_rep_tc_event_port_affinity(struct mlx5e_priv *priv);
+
+struct mlx5e_encap_entry;
+void mlx5e_rep_update_flows(struct mlx5e_priv *priv,
+			    struct mlx5e_encap_entry *e,
+			    bool neigh_connected,
+			    unsigned char ha[ETH_ALEN]);
+
+int mlx5e_rep_encap_entry_attach(struct mlx5e_priv *priv,
+				 struct mlx5e_encap_entry *e);
+void mlx5e_rep_encap_entry_detach(struct mlx5e_priv *priv,
+				  struct mlx5e_encap_entry *e);
+
+int mlx5e_rep_setup_tc(struct net_device *dev, enum tc_setup_type type,
+		       void *type_data);
+void mlx5e_rep_indr_clean_block_privs(struct mlx5e_rep_priv *rpriv);
+
+struct mlx5e_tc_update_priv;
+bool mlx5e_rep_tc_update_skb(struct mlx5_cqe64 *cqe,
+			     struct sk_buff *skb,
+			     struct mlx5e_tc_update_priv *tc_priv);
+void mlx5_rep_tc_post_napi_receive(struct mlx5e_tc_update_priv *tc_priv);
+
+#endif /* __MLX5_EN_REP_TC_H__ */
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
index 43187d8c2261..2339cded5390 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
@@ -6,6 +6,7 @@
 #include <net/geneve.h>
 #include "en/tc_tun.h"
 #include "en_tc.h"
+#include "rep/tc.h"
 
 struct mlx5e_tc_tunnel *mlx5e_get_tc_tun(struct net_device *tunnel_dev)
 {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
index a0bd9f11375b..b2a53d2aa906 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.h
@@ -195,11 +195,6 @@ void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq_rep(struct mlx5e_rq *rq,
 				   struct mlx5_cqe64 *cqe);
 
-int mlx5e_rep_encap_entry_attach(struct mlx5e_priv *priv,
-				 struct mlx5e_encap_entry *e);
-void mlx5e_rep_encap_entry_detach(struct mlx5e_priv *priv,
-				  struct mlx5e_encap_entry *e);
-
 void mlx5e_rep_queue_neigh_stats_work(struct mlx5e_priv *priv);
 
 bool mlx5e_eswitch_vf_rep(struct net_device *netdev);
@@ -210,6 +205,14 @@ static inline bool mlx5e_eswitch_rep(struct net_device *netdev)
 	       mlx5e_eswitch_uplink_rep(netdev);
 }
 
+struct mlx5e_neigh_hash_entry *
+mlx5e_rep_neigh_entry_lookup(struct mlx5e_priv *priv,
+			     struct mlx5e_neigh *m_neigh);
+int mlx5e_rep_neigh_entry_create(struct mlx5e_priv *priv,
+				 struct mlx5e_encap_entry *e,
+				 struct mlx5e_neigh_hash_entry **nhe);
+void mlx5e_rep_neigh_entry_release(struct mlx5e_neigh_hash_entry *nhe);
+
 #else /* CONFIG_MLX5_ESWITCH */
 static inline bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv) { return false; }
 static inline int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv) { return 0; }
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 2642545d102e..2f18af98eeaa 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -43,6 +43,7 @@
 #include "en_tc.h"
 #include "eswitch.h"
 #include "en_rep.h"
+#include "en/rep/tc.h"
 #include "ipoib/ipoib.h"
 #include "en_accel/ipsec_rxtx.h"
 #include "en_accel/tls_rxtx.h"
@@ -1216,12 +1217,12 @@ void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 	if (rep->vlan && skb_vlan_tag_present(skb))
 		skb_vlan_pop(skb);
 
-	if (!mlx5e_tc_rep_update_skb(cqe, skb, &tc_priv))
+	if (!mlx5e_rep_tc_update_skb(cqe, skb, &tc_priv))
 		goto free_wqe;
 
 	napi_gro_receive(rq->cq.napi, skb);
 
-	mlx5_tc_rep_post_napi_receive(&tc_priv);
+	mlx5_rep_tc_post_napi_receive(&tc_priv);
 
 free_wqe:
 	mlx5e_free_rx_wqe(rq, wi, true);
@@ -1272,12 +1273,12 @@ void mlx5e_handle_rx_cqe_mpwrq_rep(struct mlx5e_rq *rq,
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
 
-	if (!mlx5e_tc_rep_update_skb(cqe, skb, &tc_priv))
+	if (!mlx5e_rep_tc_update_skb(cqe, skb, &tc_priv))
 		goto mpwrq_cqe_out;
 
 	napi_gro_receive(rq->cq.napi, skb);
 
-	mlx5_tc_rep_post_napi_receive(&tc_priv);
+	mlx5_rep_tc_post_napi_receive(&tc_priv);
 
 mpwrq_cqe_out:
 	if (likely(wi->consumed_strides < rq->mpwqe.num_strides))
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
index 22d5efd4edec..2b7fcc35f68e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
@@ -34,11 +34,41 @@
 #define __MLX5_EN_TC_H__
 
 #include <net/pkt_cls.h>
+#include "en.h"
 
 #define MLX5E_TC_FLOW_ID_MASK 0x0000ffff
 
 #ifdef CONFIG_MLX5_ESWITCH
 
+struct tunnel_match_key {
+	struct flow_dissector_key_control enc_control;
+	struct flow_dissector_key_keyid enc_key_id;
+	struct flow_dissector_key_ports enc_tp;
+	struct flow_dissector_key_ip enc_ip;
+	union {
+		struct flow_dissector_key_ipv4_addrs enc_ipv4;
+		struct flow_dissector_key_ipv6_addrs enc_ipv6;
+	};
+
+	int filter_ifindex;
+};
+
+struct tunnel_match_enc_opts {
+	struct flow_dissector_key_enc_opts key;
+	struct flow_dissector_key_enc_opts mask;
+};
+
+/* Tunnel_id mapping is TUNNEL_INFO_BITS + ENC_OPTS_BITS.
+ * Upper TUNNEL_INFO_BITS for general tunnel info.
+ * Lower ENC_OPTS_BITS bits for enc_opts.
+ */
+#define TUNNEL_INFO_BITS 6
+#define TUNNEL_INFO_BITS_MASK GENMASK(TUNNEL_INFO_BITS - 1, 0)
+#define ENC_OPTS_BITS 2
+#define ENC_OPTS_BITS_MASK GENMASK(ENC_OPTS_BITS - 1, 0)
+#define TUNNEL_ID_BITS (TUNNEL_INFO_BITS + ENC_OPTS_BITS)
+#define TUNNEL_ID_MASK GENMASK(TUNNEL_ID_BITS - 1, 0)
+
 enum {
 	MLX5E_TC_FLAG_INGRESS_BIT,
 	MLX5E_TC_FLAG_EGRESS_BIT,
@@ -98,11 +128,6 @@ struct mlx5e_tc_update_priv {
 	struct net_device *tun_dev;
 };
 
-bool mlx5e_tc_rep_update_skb(struct mlx5_cqe64 *cqe, struct sk_buff *skb,
-			     struct mlx5e_tc_update_priv *tc_priv);
-
-void mlx5_tc_rep_post_napi_receive(struct mlx5e_tc_update_priv *tc_priv);
-
 struct mlx5e_tc_mod_hdr_acts {
 	int num_actions;
 	int max_actions;
