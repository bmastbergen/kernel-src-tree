io_uring: support buffer selection for OP_READ and OP_RECV

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit bcda7baaa3f15c7a95db3c024bb046d6e298f76b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/bcda7baa.failed

If a server process has tons of pending socket connections, generally
it uses epoll to wait for activity. When the socket is ready for reading
(or writing), the task can select a buffer and issue a recv/send on the
given fd.

Now that we have fast (non-async thread) support, a task can have tons
of pending reads or writes pending. But that means they need buffers to
back that data, and if the number of connections is high enough, having
them preallocated for all possible connections is unfeasible.

With IORING_OP_PROVIDE_BUFFERS, an application can register buffers to
use for any request. The request then sets IOSQE_BUFFER_SELECT in the
sqe, and a given group ID in sqe->buf_group. When the fd becomes ready,
a free buffer from the specified group is selected. If none are
available, the request is terminated with -ENOBUFS. If successful, the
CQE on completion will contain the buffer ID chosen in the cqe->flags
member, encoded as:

	(buffer_id << IORING_CQE_BUFFER_SHIFT) | IORING_CQE_F_BUFFER;

Once a buffer has been consumed by a request, it is no longer available
and must be registered again with IORING_OP_PROVIDE_BUFFERS.

Requests need to support this feature. For now, IORING_OP_READ and
IORING_OP_RECV support it. This is checked on SQE submission, a CQE with
res == -EOPNOTSUPP will be posted if attempted on unsupported requests.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit bcda7baaa3f15c7a95db3c024bb046d6e298f76b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index 7842c6de7135,a80b5c189c14..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -308,6 -334,237 +308,240 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_close {
+ 	struct file			*file;
+ 	struct file			*put_file;
+ 	int				fd;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ 	int				mode;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	unsigned			count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	union {
+ 		struct user_msghdr __user *msg;
+ 		void __user		*buf;
+ 	};
+ 	int				msg_flags;
+ 	int				bgid;
+ 	size_t				len;
+ 	struct io_buffer		*kbuf;
+ };
+ 
+ struct io_open {
+ 	struct file			*file;
+ 	int				dfd;
+ 	union {
+ 		unsigned		mask;
+ 	};
+ 	struct filename			*filename;
+ 	struct statx __user		*buffer;
+ 	struct open_how			how;
+ };
+ 
+ struct io_files_update {
+ 	struct file			*file;
+ 	u64				arg;
+ 	u32				nr_args;
+ 	u32				offset;
+ };
+ 
+ struct io_fadvise {
+ 	struct file			*file;
+ 	u64				offset;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_madvise {
+ 	struct file			*file;
+ 	u64				addr;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_epoll {
+ 	struct file			*file;
+ 	int				epfd;
+ 	int				op;
+ 	int				fd;
+ 	struct epoll_event		event;
+ };
+ 
+ struct io_splice {
+ 	struct file			*file_out;
+ 	struct file			*file_in;
+ 	loff_t				off_out;
+ 	loff_t				off_in;
+ 	u64				len;
+ 	unsigned int			flags;
+ };
+ 
+ struct io_provide_buf {
+ 	struct file			*file;
+ 	__u64				addr;
+ 	__s32				len;
+ 	__u32				bgid;
+ 	__u16				nbufs;
+ 	__u16				bid;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ 	struct sockaddr_storage		addr;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
+ };
+ 
+ enum {
+ 	REQ_F_FIXED_FILE_BIT	= IOSQE_FIXED_FILE_BIT,
+ 	REQ_F_IO_DRAIN_BIT	= IOSQE_IO_DRAIN_BIT,
+ 	REQ_F_LINK_BIT		= IOSQE_IO_LINK_BIT,
+ 	REQ_F_HARDLINK_BIT	= IOSQE_IO_HARDLINK_BIT,
+ 	REQ_F_FORCE_ASYNC_BIT	= IOSQE_ASYNC_BIT,
+ 	REQ_F_BUFFER_SELECT_BIT	= IOSQE_BUFFER_SELECT_BIT,
+ 
+ 	REQ_F_LINK_NEXT_BIT,
+ 	REQ_F_FAIL_LINK_BIT,
+ 	REQ_F_INFLIGHT_BIT,
+ 	REQ_F_CUR_POS_BIT,
+ 	REQ_F_NOWAIT_BIT,
+ 	REQ_F_IOPOLL_COMPLETED_BIT,
+ 	REQ_F_LINK_TIMEOUT_BIT,
+ 	REQ_F_TIMEOUT_BIT,
+ 	REQ_F_ISREG_BIT,
+ 	REQ_F_MUST_PUNT_BIT,
+ 	REQ_F_TIMEOUT_NOSEQ_BIT,
+ 	REQ_F_COMP_LOCKED_BIT,
+ 	REQ_F_NEED_CLEANUP_BIT,
+ 	REQ_F_OVERFLOW_BIT,
+ 	REQ_F_POLLED_BIT,
+ 	REQ_F_BUFFER_SELECTED_BIT,
+ };
+ 
+ enum {
+ 	/* ctx owns file */
+ 	REQ_F_FIXED_FILE	= BIT(REQ_F_FIXED_FILE_BIT),
+ 	/* drain existing IO first */
+ 	REQ_F_IO_DRAIN		= BIT(REQ_F_IO_DRAIN_BIT),
+ 	/* linked sqes */
+ 	REQ_F_LINK		= BIT(REQ_F_LINK_BIT),
+ 	/* doesn't sever on completion < 0 */
+ 	REQ_F_HARDLINK		= BIT(REQ_F_HARDLINK_BIT),
+ 	/* IOSQE_ASYNC */
+ 	REQ_F_FORCE_ASYNC	= BIT(REQ_F_FORCE_ASYNC_BIT),
+ 	/* IOSQE_BUFFER_SELECT */
+ 	REQ_F_BUFFER_SELECT	= BIT(REQ_F_BUFFER_SELECT_BIT),
+ 
+ 	/* already grabbed next link */
+ 	REQ_F_LINK_NEXT		= BIT(REQ_F_LINK_NEXT_BIT),
+ 	/* fail rest of links */
+ 	REQ_F_FAIL_LINK		= BIT(REQ_F_FAIL_LINK_BIT),
+ 	/* on inflight list */
+ 	REQ_F_INFLIGHT		= BIT(REQ_F_INFLIGHT_BIT),
+ 	/* read/write uses file position */
+ 	REQ_F_CUR_POS		= BIT(REQ_F_CUR_POS_BIT),
+ 	/* must not punt to workers */
+ 	REQ_F_NOWAIT		= BIT(REQ_F_NOWAIT_BIT),
+ 	/* polled IO has completed */
+ 	REQ_F_IOPOLL_COMPLETED	= BIT(REQ_F_IOPOLL_COMPLETED_BIT),
+ 	/* has linked timeout */
+ 	REQ_F_LINK_TIMEOUT	= BIT(REQ_F_LINK_TIMEOUT_BIT),
+ 	/* timeout request */
+ 	REQ_F_TIMEOUT		= BIT(REQ_F_TIMEOUT_BIT),
+ 	/* regular file */
+ 	REQ_F_ISREG		= BIT(REQ_F_ISREG_BIT),
+ 	/* must be punted even for NONBLOCK */
+ 	REQ_F_MUST_PUNT		= BIT(REQ_F_MUST_PUNT_BIT),
+ 	/* no timeout sequence */
+ 	REQ_F_TIMEOUT_NOSEQ	= BIT(REQ_F_TIMEOUT_NOSEQ_BIT),
+ 	/* completion under lock */
+ 	REQ_F_COMP_LOCKED	= BIT(REQ_F_COMP_LOCKED_BIT),
+ 	/* needs cleanup */
+ 	REQ_F_NEED_CLEANUP	= BIT(REQ_F_NEED_CLEANUP_BIT),
+ 	/* in overflow list */
+ 	REQ_F_OVERFLOW		= BIT(REQ_F_OVERFLOW_BIT),
+ 	/* already went through poll handler */
+ 	REQ_F_POLLED		= BIT(REQ_F_POLLED_BIT),
+ 	/* buffer already selected */
+ 	REQ_F_BUFFER_SELECTED	= BIT(REQ_F_BUFFER_SELECTED_BIT),
+ };
+ 
+ struct async_poll {
+ 	struct io_poll_iocb	poll;
+ 	struct io_wq_work	work;
+ };
+ 
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -341,7 -605,25 +575,29 @@@ struct io_kiocb 
  	u32			result;
  	u32			sequence;
  
++<<<<<<< HEAD
 +	struct work_struct	work;
++=======
+ 	struct list_head	link_list;
+ 
+ 	struct list_head	inflight_entry;
+ 
+ 	union {
+ 		/*
+ 		 * Only commands that never go async can use the below fields,
+ 		 * obviously. Right now only IORING_OP_POLL_ADD uses them, and
+ 		 * async armed poll handlers for regular commands. The latter
+ 		 * restore the work, if needed.
+ 		 */
+ 		struct {
+ 			struct callback_head	task_work;
+ 			struct hlist_node	hash_node;
+ 			struct async_poll	*apoll;
+ 			int			cflags;
+ 		};
+ 		struct io_wq_work	work;
+ 	};
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  };
  
  #define IO_PLUG_THRESHOLD		2
@@@ -366,8 -648,200 +622,205 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ 	/* needs ->fs */
+ 	unsigned		needs_fs : 1;
+ 	/* set if opcode supports polled "wait" */
+ 	unsigned		pollin : 1;
+ 	unsigned		pollout : 1;
+ 	/* op supports buffer selection */
+ 	unsigned		buffer_select : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 		.pollin			= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollout		= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.pollin			= 1,
+ 		.buffer_select		= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_SPLICE] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_PROVIDE_BUFFERS] = {},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_ring_file_ref_flush(struct fixed_file_data *data);
+ static void io_cleanup_req(struct io_kiocb *req);
+ static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
+ 		       int fd, struct file **out_file, bool fixed);
+ static void __io_queue_sqe(struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  
  static struct kmem_cache *req_cachep;
  
@@@ -550,19 -1147,139 +1003,149 @@@ static void io_cqring_ev_posted(struct 
  		eventfd_signal(ctx->cq_ev_fd, 1);
  }
  
 -/* Returns true if there are no backlogged entries after the flush */
 -static bool io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force)
 +static void io_cqring_add_event(struct io_ring_ctx *ctx, u64 user_data,
 +				long res)
  {
++<<<<<<< HEAD
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&ctx->completion_lock, flags);
 +	io_cqring_fill_event(ctx, user_data, res);
++=======
+ 	struct io_rings *rings = ctx->rings;
+ 	struct io_uring_cqe *cqe;
+ 	struct io_kiocb *req;
+ 	unsigned long flags;
+ 	LIST_HEAD(list);
+ 
+ 	if (!force) {
+ 		if (list_empty_careful(&ctx->cq_overflow_list))
+ 			return true;
+ 		if ((ctx->cached_cq_tail - READ_ONCE(rings->cq.head) ==
+ 		    rings->cq_ring_entries))
+ 			return false;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/* if force is set, the ring is going away. always drop after that */
+ 	if (force)
+ 		ctx->cq_overflow_flushed = 1;
+ 
+ 	cqe = NULL;
+ 	while (!list_empty(&ctx->cq_overflow_list)) {
+ 		cqe = io_get_cqring(ctx);
+ 		if (!cqe && !force)
+ 			break;
+ 
+ 		req = list_first_entry(&ctx->cq_overflow_list, struct io_kiocb,
+ 						list);
+ 		list_move(&req->list, &list);
+ 		req->flags &= ~REQ_F_OVERFLOW;
+ 		if (cqe) {
+ 			WRITE_ONCE(cqe->user_data, req->user_data);
+ 			WRITE_ONCE(cqe->res, req->result);
+ 			WRITE_ONCE(cqe->flags, req->cflags);
+ 		} else {
+ 			WRITE_ONCE(ctx->rings->cq_overflow,
+ 				atomic_inc_return(&ctx->cached_cq_overflow));
+ 		}
+ 	}
+ 
+ 	io_commit_cqring(ctx);
+ 	if (cqe) {
+ 		clear_bit(0, &ctx->sq_check_overflow);
+ 		clear_bit(0, &ctx->cq_check_overflow);
+ 	}
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	while (!list_empty(&list)) {
+ 		req = list_first_entry(&list, struct io_kiocb, list);
+ 		list_del(&req->list);
+ 		io_put_req(req);
+ 	}
+ 
+ 	return cqe != NULL;
+ }
+ 
+ static void __io_cqring_fill_event(struct io_kiocb *req, long res, long cflags)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_uring_cqe *cqe;
+ 
+ 	trace_io_uring_complete(ctx, req->user_data, res);
+ 
+ 	/*
+ 	 * If we can't get a cq entry, userspace overflowed the
+ 	 * submission (by quite a lot). Increment the overflow count in
+ 	 * the ring.
+ 	 */
+ 	cqe = io_get_cqring(ctx);
+ 	if (likely(cqe)) {
+ 		WRITE_ONCE(cqe->user_data, req->user_data);
+ 		WRITE_ONCE(cqe->res, res);
+ 		WRITE_ONCE(cqe->flags, cflags);
+ 	} else if (ctx->cq_overflow_flushed) {
+ 		WRITE_ONCE(ctx->rings->cq_overflow,
+ 				atomic_inc_return(&ctx->cached_cq_overflow));
+ 	} else {
+ 		if (list_empty(&ctx->cq_overflow_list)) {
+ 			set_bit(0, &ctx->sq_check_overflow);
+ 			set_bit(0, &ctx->cq_check_overflow);
+ 		}
+ 		req->flags |= REQ_F_OVERFLOW;
+ 		refcount_inc(&req->refs);
+ 		req->result = res;
+ 		req->cflags = cflags;
+ 		list_add_tail(&req->list, &ctx->cq_overflow_list);
+ 	}
+ }
+ 
+ static void io_cqring_fill_event(struct io_kiocb *req, long res)
+ {
+ 	__io_cqring_fill_event(req, res, 0);
+ }
+ 
+ static void __io_cqring_add_event(struct io_kiocb *req, long res, long cflags)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	__io_cqring_fill_event(req, res, cflags);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  	io_commit_cqring(ctx);
  	spin_unlock_irqrestore(&ctx->completion_lock, flags);
  
  	io_cqring_ev_posted(ctx);
  }
  
++<<<<<<< HEAD
++=======
+ static void io_cqring_add_event(struct io_kiocb *req, long res)
+ {
+ 	__io_cqring_add_event(req, res, 0);
+ }
+ 
+ static inline bool io_is_fallback_req(struct io_kiocb *req)
+ {
+ 	return req == (struct io_kiocb *)
+ 			((unsigned long) req->ctx->fallback_req & ~1UL);
+ }
+ 
+ static struct io_kiocb *io_get_fallback_req(struct io_ring_ctx *ctx)
+ {
+ 	struct io_kiocb *req;
+ 
+ 	req = ctx->fallback_req;
+ 	if (!test_and_set_bit_lock(0, (unsigned long *) ctx->fallback_req))
+ 		return req;
+ 
+ 	return NULL;
+ }
+ 
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  static struct io_kiocb *io_get_req(struct io_ring_ctx *ctx,
  				   struct io_submit_state *state)
  {
@@@ -693,46 -1603,124 +1276,67 @@@ static void io_put_req(struct io_kiocb 
  		io_free_req(req);
  }
  
 -static void io_steal_work(struct io_kiocb *req,
 -			  struct io_wq_work **workptr)
 -{
 -	/*
 -	 * It's in an io-wq worker, so there always should be at least
 -	 * one reference, which will be dropped in io_put_work() just
 -	 * after the current handler returns.
 -	 *
 -	 * It also means, that if the counter dropped to 1, then there is
 -	 * no asynchronous users left, so it's safe to steal the next work.
 -	 */
 -	if (refcount_read(&req->refs) == 1) {
 -		struct io_kiocb *nxt = NULL;
 -
 -		io_req_find_next(req, &nxt);
 -		if (nxt)
 -			io_wq_assign_next(workptr, nxt);
 -	}
 -}
 -
 -/*
 - * Must only be used if we don't need to care about links, usually from
 - * within the completion handling itself.
 - */
 -static void __io_double_put_req(struct io_kiocb *req)
 -{
 -	/* drop both submit and complete references */
 -	if (refcount_sub_and_test(2, &req->refs))
 -		__io_free_req(req);
 -}
 -
 -static void io_double_put_req(struct io_kiocb *req)
 -{
 -	/* drop both submit and complete references */
 -	if (refcount_sub_and_test(2, &req->refs))
 -		io_free_req(req);
 -}
 -
 -static unsigned io_cqring_events(struct io_ring_ctx *ctx, bool noflush)
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
  {
 -	struct io_rings *rings = ctx->rings;
 -
 -	if (test_bit(0, &ctx->cq_check_overflow)) {
 -		/*
 -		 * noflush == true is from the waitqueue handler, just ensure
 -		 * we wake up the task, and the next invocation will flush the
 -		 * entries. We cannot safely to it from here.
 -		 */
 -		if (noflush && !list_empty(&ctx->cq_overflow_list))
 -			return -1U;
 -
 -		io_cqring_overflow_flush(ctx, false);
 -	}
 -
  	/* See comment at the top of this file */
  	smp_rmb();
 -	return ctx->cached_cq_tail - READ_ONCE(rings->cq.head);
 -}
 -
 -static inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)
 -{
 -	struct io_rings *rings = ctx->rings;
 -
 -	/* make sure SQ entry isn't read before tail */
 -	return smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;
 -}
 -
 -static inline bool io_req_multi_free(struct req_batch *rb, struct io_kiocb *req)
 -{
 -	if ((req->flags & REQ_F_LINK) || io_is_fallback_req(req))
 -		return false;
 -
 -	if (!(req->flags & REQ_F_FIXED_FILE) || req->io)
 -		rb->need_iter++;
 -
 -	rb->reqs[rb->to_free++] = req;
 -	if (unlikely(rb->to_free == ARRAY_SIZE(rb->reqs)))
 -		io_free_req_many(req->ctx, rb);
 -	return true;
 +	return READ_ONCE(ring->r.tail) - READ_ONCE(ring->r.head);
  }
  
+ static int io_put_kbuf(struct io_kiocb *req)
+ {
+ 	struct io_buffer *kbuf = (struct io_buffer *) req->rw.addr;
+ 	int cflags;
+ 
+ 	cflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;
+ 	cflags |= IORING_CQE_F_BUFFER;
+ 	req->rw.addr = 0;
+ 	kfree(kbuf);
+ 	return cflags;
+ }
+ 
  /*
   * Find and free completed poll iocbs
   */
  static void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,
  			       struct list_head *done)
  {
 -	struct req_batch rb;
 +	void *reqs[IO_IOPOLL_BATCH];
  	struct io_kiocb *req;
 +	int to_free;
  
 -	rb.to_free = rb.need_iter = 0;
 +	to_free = 0;
  	while (!list_empty(done)) {
+ 		int cflags = 0;
+ 
  		req = list_first_entry(done, struct io_kiocb, list);
  		list_del(&req->list);
  
++<<<<<<< HEAD
 +		io_cqring_fill_event(ctx, req->user_data, req->result);
++=======
+ 		if (req->flags & REQ_F_BUFFER_SELECTED)
+ 			cflags = io_put_kbuf(req);
+ 
+ 		__io_cqring_fill_event(req, req->result, cflags);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  		(*nr_events)++;
  
 -		if (refcount_dec_and_test(&req->refs) &&
 -		    !io_req_multi_free(&rb, req))
 -			io_free_req(req);
 +		if (refcount_dec_and_test(&req->refs)) {
 +			/* If we're not using fixed files, we have to pair the
 +			 * completion part with the file put. Use regular
 +			 * completions for those, only batch free for fixed
 +			 * file and non-linked commands.
 +			 */
 +			if ((req->flags & (REQ_F_FIXED_FILE|REQ_F_LINK)) ==
 +			    REQ_F_FIXED_FILE) {
 +				reqs[to_free++] = req;
 +				if (to_free == ARRAY_SIZE(reqs))
 +					io_free_req_many(ctx, reqs, &to_free);
 +			} else {
 +				io_free_req(req);
 +			}
 +		}
  	}
  
  	io_commit_cqring(ctx);
@@@ -879,19 -1867,39 +1483,44 @@@ static int io_iopoll_check(struct io_ri
  	return ret;
  }
  
 -static void kiocb_end_write(struct io_kiocb *req)
 +static void kiocb_end_write(struct kiocb *kiocb)
  {
 -	/*
 -	 * Tell lockdep we inherited freeze protection from submission
 -	 * thread.
 -	 */
 -	if (req->flags & REQ_F_ISREG) {
 -		struct inode *inode = file_inode(req->file);
 +	if (kiocb->ki_flags & IOCB_WRITE) {
 +		struct inode *inode = file_inode(kiocb->ki_filp);
  
 -		__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		/*
 +		 * Tell lockdep we inherited freeze protection from submission
 +		 * thread.
 +		 */
 +		if (S_ISREG(inode->i_mode))
 +			__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		file_end_write(kiocb->ki_filp);
  	}
++<<<<<<< HEAD
++=======
+ 	file_end_write(req->file);
+ }
+ 
+ static inline void req_set_fail_links(struct io_kiocb *req)
+ {
+ 	if ((req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) == REQ_F_LINK)
+ 		req->flags |= REQ_F_FAIL_LINK;
+ }
+ 
+ static void io_complete_rw_common(struct kiocb *kiocb, long res)
+ {
+ 	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);
+ 	int cflags = 0;
+ 
+ 	if (kiocb->ki_flags & IOCB_WRITE)
+ 		kiocb_end_write(req);
+ 
+ 	if (res != req->result)
+ 		req_set_fail_links(req);
+ 	if (req->flags & REQ_F_BUFFER_SELECTED)
+ 		cflags = io_put_kbuf(req);
+ 	__io_cqring_add_event(req, res, cflags);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  }
  
  static void io_complete_rw(struct kiocb *kiocb, long res, long res2)
@@@ -1065,6 -2074,12 +1694,15 @@@ static int io_prep_rw(struct io_kiocb *
  			return -EINVAL;
  		kiocb->ki_complete = io_complete_rw;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	req->rw.addr = READ_ONCE(sqe->addr);
+ 	req->rw.len = READ_ONCE(sqe->len);
+ 	/* we own ->private, reuse it for the buffer index  / buffer ID */
+ 	req->rw.kiocb.private = (void *) (unsigned long)
+ 					READ_ONCE(sqe->buf_index);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  	return 0;
  }
  
@@@ -1159,32 -2187,99 +1797,123 @@@ static int io_import_fixed(struct io_ri
  		}
  	}
  
 -	return len;
 +	/* don't drop a reference to these pages */
 +	iter->type |= ITER_BVEC_FLAG_NO_REF;
 +	return 0;
  }
  
++<<<<<<< HEAD
 +static ssize_t io_import_iovec(struct io_ring_ctx *ctx, int rw,
 +			       const struct sqe_submit *s, struct iovec **iovec,
 +			       struct iov_iter *iter)
++=======
+ static void io_ring_submit_unlock(struct io_ring_ctx *ctx, bool needs_lock)
+ {
+ 	if (needs_lock)
+ 		mutex_unlock(&ctx->uring_lock);
+ }
+ 
+ static void io_ring_submit_lock(struct io_ring_ctx *ctx, bool needs_lock)
+ {
+ 	/*
+ 	 * "Normal" inline submissions always hold the uring_lock, since we
+ 	 * grab it from the system call. Same is true for the SQPOLL offload.
+ 	 * The only exception is when we've detached the request and issue it
+ 	 * from an async worker thread, grab the lock for that case.
+ 	 */
+ 	if (needs_lock)
+ 		mutex_lock(&ctx->uring_lock);
+ }
+ 
+ static struct io_buffer *io_buffer_select(struct io_kiocb *req, size_t *len,
+ 					  int bgid, struct io_buffer *kbuf,
+ 					  bool needs_lock)
+ {
+ 	struct io_buffer *head;
+ 
+ 	if (req->flags & REQ_F_BUFFER_SELECTED)
+ 		return kbuf;
+ 
+ 	io_ring_submit_lock(req->ctx, needs_lock);
+ 
+ 	lockdep_assert_held(&req->ctx->uring_lock);
+ 
+ 	head = idr_find(&req->ctx->io_buffer_idr, bgid);
+ 	if (head) {
+ 		if (!list_empty(&head->list)) {
+ 			kbuf = list_last_entry(&head->list, struct io_buffer,
+ 							list);
+ 			list_del(&kbuf->list);
+ 		} else {
+ 			kbuf = head;
+ 			idr_remove(&req->ctx->io_buffer_idr, bgid);
+ 		}
+ 		if (*len > kbuf->len)
+ 			*len = kbuf->len;
+ 	} else {
+ 		kbuf = ERR_PTR(-ENOBUFS);
+ 	}
+ 
+ 	io_ring_submit_unlock(req->ctx, needs_lock);
+ 
+ 	return kbuf;
+ }
+ 
+ static ssize_t io_import_iovec(int rw, struct io_kiocb *req,
+ 			       struct iovec **iovec, struct iov_iter *iter,
+ 			       bool needs_lock)
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  {
 -	void __user *buf = u64_to_user_ptr(req->rw.addr);
 -	size_t sqe_len = req->rw.len;
 +	const struct io_uring_sqe *sqe = s->sqe;
 +	void __user *buf = u64_to_user_ptr(READ_ONCE(sqe->addr));
 +	size_t sqe_len = READ_ONCE(sqe->len);
  	u8 opcode;
  
++<<<<<<< HEAD
 +	/*
 +	 * We're reading ->opcode for the second time, but the first read
 +	 * doesn't care whether it's _FIXED or not, so it doesn't matter
 +	 * whether ->opcode changes concurrently. The first read does care
 +	 * about whether it is a READ or a WRITE, so we don't trust this read
 +	 * for that purpose and instead let the caller pass in the read/write
 +	 * flag.
 +	 */
 +	opcode = READ_ONCE(sqe->opcode);
 +	if (opcode == IORING_OP_READ_FIXED ||
 +	    opcode == IORING_OP_WRITE_FIXED) {
 +		ssize_t ret = io_import_fixed(ctx, rw, sqe, iter);
++=======
+ 	opcode = req->opcode;
+ 	if (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {
+ 		*iovec = NULL;
+ 		return io_import_fixed(req, rw, iter);
+ 	}
+ 
+ 	/* buffer index only valid with fixed read/write, or buffer select  */
+ 	if (req->rw.kiocb.private && !(req->flags & REQ_F_BUFFER_SELECT))
+ 		return -EINVAL;
+ 
+ 	if (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {
+ 		ssize_t ret;
+ 
+ 		if (req->flags & REQ_F_BUFFER_SELECT) {
+ 			struct io_buffer *kbuf = (struct io_buffer *) req->rw.addr;
+ 			int bgid;
+ 
+ 			bgid = (int) (unsigned long) req->rw.kiocb.private;
+ 			kbuf = io_buffer_select(req, &sqe_len, bgid, kbuf,
+ 						needs_lock);
+ 			if (IS_ERR(kbuf)) {
+ 				*iovec = NULL;
+ 				return PTR_ERR(kbuf);
+ 			}
+ 			req->rw.addr = (u64) kbuf;
+ 			req->flags |= REQ_F_BUFFER_SELECTED;
+ 			buf = u64_to_user_ptr(kbuf->addr);
+ 		}
+ 
+ 		ret = import_single_range(rw, buf, sqe_len, *iovec, iter);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  		*iovec = NULL;
  		return ret < 0 ? ret : sqe_len;
  	}
@@@ -1318,25 -2361,84 +2047,102 @@@ static ssize_t loop_rw_iter(int rw, str
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
 +		   bool force_nonblock)
++=======
+ static void io_req_map_rw(struct io_kiocb *req, ssize_t io_size,
+ 			  struct iovec *iovec, struct iovec *fast_iov,
+ 			  struct iov_iter *iter)
+ {
+ 	req->io->rw.nr_segs = iter->nr_segs;
+ 	req->io->rw.size = io_size;
+ 	req->io->rw.iov = iovec;
+ 	if (!req->io->rw.iov) {
+ 		req->io->rw.iov = req->io->rw.fast_iov;
+ 		memcpy(req->io->rw.iov, fast_iov,
+ 			sizeof(struct iovec) * iter->nr_segs);
+ 	} else {
+ 		req->flags |= REQ_F_NEED_CLEANUP;
+ 	}
+ }
+ 
+ static int io_alloc_async_ctx(struct io_kiocb *req)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 	req->io = kmalloc(sizeof(*req->io), GFP_KERNEL);
+ 	return req->io == NULL;
+ }
+ 
+ static int io_setup_async_rw(struct io_kiocb *req, ssize_t io_size,
+ 			     struct iovec *iovec, struct iovec *fast_iov,
+ 			     struct iov_iter *iter)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 	if (!req->io) {
+ 		if (io_alloc_async_ctx(req))
+ 			return -ENOMEM;
+ 
+ 		io_req_map_rw(req, io_size, iovec, fast_iov, iter);
+ 	}
+ 	return 0;
+ }
+ 
+ static int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			bool force_nonblock)
+ {
+ 	struct io_async_ctx *io;
+ 	struct iov_iter iter;
+ 	ssize_t ret;
+ 
+ 	ret = io_prep_rw(req, sqe, force_nonblock);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (unlikely(!(req->file->f_mode & FMODE_READ)))
+ 		return -EBADF;
+ 
+ 	/* either don't need iovec imported or already have it */
+ 	if (!req->io || req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	io = req->io;
+ 	io->rw.iov = io->rw.fast_iov;
+ 	req->io = NULL;
+ 	ret = io_import_iovec(READ, req, &io->rw.iov, &iter, !force_nonblock);
+ 	req->io = io;
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
+ 	return 0;
+ }
+ 
+ static int io_read(struct io_kiocb *req, bool force_nonblock)
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 -	struct kiocb *kiocb = &req->rw.kiocb;
 +	struct kiocb *kiocb = &req->rw;
  	struct iov_iter iter;
 +	struct file *file;
  	size_t iov_count;
 -	ssize_t io_size, ret;
 +	ssize_t read_size, ret;
 +
++<<<<<<< HEAD
 +	ret = io_prep_rw(req, s, force_nonblock);
 +	if (ret)
 +		return ret;
 +	file = kiocb->ki_filp;
 +
 +	if (unlikely(!(file->f_mode & FMODE_READ)))
 +		return -EBADF;
  
 +	ret = io_import_iovec(req->ctx, READ, s, &iovec, &iter);
++=======
+ 	ret = io_import_iovec(READ, req, &iovec, &iter, !force_nonblock);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  	if (ret < 0)
  		return ret;
  
@@@ -1395,28 -2500,60 +2201,65 @@@ static int io_write(struct io_kiocb *re
  	if (ret)
  		return ret;
  
 -	if (unlikely(!(req->file->f_mode & FMODE_WRITE)))
 +	file = kiocb->ki_filp;
 +	if (unlikely(!(file->f_mode & FMODE_WRITE)))
  		return -EBADF;
  
++<<<<<<< HEAD
 +	ret = io_import_iovec(req->ctx, WRITE, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
 +
++=======
+ 	/* either don't need iovec imported or already have it */
+ 	if (!req->io || req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	io = req->io;
+ 	io->rw.iov = io->rw.fast_iov;
+ 	req->io = NULL;
+ 	ret = io_import_iovec(WRITE, req, &io->rw.iov, &iter, !force_nonblock);
+ 	req->io = io;
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
+ 	return 0;
+ }
+ 
+ static int io_write(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
+ 	struct kiocb *kiocb = &req->rw.kiocb;
+ 	struct iov_iter iter;
+ 	size_t iov_count;
+ 	ssize_t ret, io_size;
+ 
+ 	ret = io_import_iovec(WRITE, req, &iovec, &iter, !force_nonblock);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* Ensure we clear previously set non-block flag */
+ 	if (!force_nonblock)
+ 		req->rw.kiocb.ki_flags &= ~IOCB_NOWAIT;
+ 
+ 	req->result = 0;
+ 	io_size = ret;
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  	if (req->flags & REQ_F_LINK)
 -		req->result = io_size;
 +		req->result = ret;
  
 -	/*
 -	 * If the file doesn't support async, mark it as REQ_F_MUST_PUNT so
 -	 * we know to async punt it even if it was opened O_NONBLOCK
 -	 */
 -	if (force_nonblock && !io_file_supports_async(req->file))
 -		goto copy_iov;
 +	iov_count = iov_iter_count(&iter);
  
 -	/* file path doesn't support NOWAIT for non-direct_IO */
 -	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&
 -	    (req->flags & REQ_F_ISREG))
 -		goto copy_iov;
 +	ret = -EAGAIN;
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
 +		goto out_free;
 +	}
  
 -	iov_count = iov_iter_count(&iter);
 -	ret = rw_verify_area(WRITE, req->file, &kiocb->ki_pos, iov_count);
 +	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
  		ssize_t ret2;
  
@@@ -1519,58 -2904,481 +2362,428 @@@ static int io_fsync(struct io_kiocb *re
  	return 0;
  }
  
- static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++<<<<<<< HEAD
++=======
+ static int io_openat(struct io_kiocb *req, bool force_nonblock)
  {
- 	struct io_ring_ctx *ctx = req->ctx;
- 	int ret = 0;
+ 	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
+ 	return io_openat2(req, force_nonblock);
+ }
  
- 	if (!req->file)
- 		return -EBADF;
+ static int io_provide_buffers_prep(struct io_kiocb *req,
+ 				   const struct io_uring_sqe *sqe)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	u64 tmp;
  
- 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
- 		return -EINVAL;
- 	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
+ 	if (sqe->ioprio || sqe->rw_flags)
  		return -EINVAL;
  
- 	return ret;
+ 	tmp = READ_ONCE(sqe->fd);
+ 	if (!tmp || tmp > USHRT_MAX)
+ 		return -E2BIG;
+ 	p->nbufs = tmp;
+ 	p->addr = READ_ONCE(sqe->addr);
+ 	p->len = READ_ONCE(sqe->len);
+ 
+ 	if (!access_ok(u64_to_user_ptr(p->addr), p->len))
+ 		return -EFAULT;
+ 
+ 	p->bgid = READ_ONCE(sqe->buf_group);
+ 	tmp = READ_ONCE(sqe->off);
+ 	if (tmp > USHRT_MAX)
+ 		return -E2BIG;
+ 	p->bid = tmp;
+ 	return 0;
+ }
+ 
+ static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)
+ {
+ 	struct io_buffer *buf;
+ 	u64 addr = pbuf->addr;
+ 	int i, bid = pbuf->bid;
+ 
+ 	for (i = 0; i < pbuf->nbufs; i++) {
+ 		buf = kmalloc(sizeof(*buf), GFP_KERNEL);
+ 		if (!buf)
+ 			break;
+ 
+ 		buf->addr = addr;
+ 		buf->len = pbuf->len;
+ 		buf->bid = bid;
+ 		addr += pbuf->len;
+ 		bid++;
+ 		if (!*head) {
+ 			INIT_LIST_HEAD(&buf->list);
+ 			*head = buf;
+ 		} else {
+ 			list_add_tail(&buf->list, &(*head)->list);
+ 		}
+ 	}
+ 
+ 	return i ? i : -ENOMEM;
+ }
+ 
+ static int io_provide_buffers(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_provide_buf *p = &req->pbuf;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_buffer *head, *list;
+ 	int ret = 0;
+ 
+ 	io_ring_submit_lock(ctx, !force_nonblock);
+ 
+ 	lockdep_assert_held(&ctx->uring_lock);
+ 
+ 	list = head = idr_find(&ctx->io_buffer_idr, p->bgid);
+ 
+ 	ret = io_add_buffers(p, &head);
+ 	if (ret < 0)
+ 		goto out;
+ 
+ 	if (!list) {
+ 		ret = idr_alloc(&ctx->io_buffer_idr, head, p->bgid, p->bgid + 1,
+ 					GFP_KERNEL);
+ 		if (ret < 0) {
+ 			while (!list_empty(&head->list)) {
+ 				struct io_buffer *buf;
+ 
+ 				buf = list_first_entry(&head->list,
+ 							struct io_buffer, list);
+ 				list_del(&buf->list);
+ 				kfree(buf);
+ 			}
+ 			kfree(head);
+ 			goto out;
+ 		}
+ 	}
+ out:
+ 	io_ring_submit_unlock(ctx, !force_nonblock);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_epoll_ctl_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	req->epoll.epfd = READ_ONCE(sqe->fd);
+ 	req->epoll.op = READ_ONCE(sqe->len);
+ 	req->epoll.fd = READ_ONCE(sqe->off);
+ 
+ 	if (ep_op_has_event(req->epoll.op)) {
+ 		struct epoll_event __user *ev;
+ 
+ 		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
+ 			return -EFAULT;
+ 	}
+ 
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_epoll_ctl(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_EPOLL)
+ 	struct io_epoll *ie = &req->epoll;
+ 	int ret;
+ 
+ 	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
+ 	if (force_nonblock && ret == -EAGAIN)
+ 		return -EAGAIN;
+ 
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	if (sqe->ioprio || sqe->buf_index || sqe->off)
+ 		return -EINVAL;
+ 
+ 	req->madvise.addr = READ_ONCE(sqe->addr);
+ 	req->madvise.len = READ_ONCE(sqe->len);
+ 	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_madvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
+ 	struct io_madvise *ma = &req->madvise;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	ret = do_madvise(ma->addr, ma->len, ma->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->addr)
+ 		return -EINVAL;
+ 
+ 	req->fadvise.offset = READ_ONCE(sqe->off);
+ 	req->fadvise.len = READ_ONCE(sqe->len);
+ 	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
+ 	return 0;
+ }
+ 
+ static int io_fadvise(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_fadvise *fa = &req->fadvise;
+ 	int ret;
+ 
+ 	if (force_nonblock) {
+ 		switch (fa->advice) {
+ 		case POSIX_FADV_NORMAL:
+ 		case POSIX_FADV_RANDOM:
+ 		case POSIX_FADV_SEQUENTIAL:
+ 			break;
+ 		default:
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	unsigned lookup_flags;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.mask = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	req->open.how.flags = READ_ONCE(sqe->statx_flags);
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
+ 		return -EINVAL;
+ 
+ 	req->open.filename = getname_flags(fname, lookup_flags, NULL);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_statx(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_open *ctx = &req->open;
+ 	unsigned lookup_flags;
+ 	struct path path;
+ 	struct kstat stat;
+ 	int ret;
+ 
+ 	if (force_nonblock)
+ 		return -EAGAIN;
+ 
+ 	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
+ 		return -EINVAL;
+ 
+ retry:
+ 	/* filename_lookup() drops it, keep a reference */
+ 	ctx->filename->refcnt++;
+ 
+ 	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
+ 				NULL);
+ 	if (ret)
+ 		goto err;
+ 
+ 	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
+ 	path_put(&path);
+ 	if (retry_estale(ret, lookup_flags)) {
+ 		lookup_flags |= LOOKUP_REVAL;
+ 		goto retry;
+ 	}
+ 	if (!ret)
+ 		ret = cp_statx(&stat, ctx->buffer);
+ err:
+ 	putname(ctx->filename);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req(req);
+ 	return 0;
+ }
+ 
+ static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * If we queue this for async, it must not be cancellable. That would
+ 	 * leave the 'file' in an undeterminate state.
+ 	 */
+ 	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
+ 
+ 	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
+ 	    sqe->rw_flags || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 
+ 	req->close.fd = READ_ONCE(sqe->fd);
+ 	if (req->file->f_op == &io_uring_fops ||
+ 	    req->close.fd == req->ctx->ring_fd)
+ 		return -EBADF;
+ 
+ 	return 0;
+ }
+ 
+ /* only called when __close_fd_get_file() is done */
+ static void __io_close_finish(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = filp_close(req->close.put_file, req->work.files);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	fput(req->close.put_file);
+ 	io_put_req(req);
+ }
+ 
+ static void io_close_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 
+ 	/* not cancellable, don't do io_req_cancelled() */
+ 	__io_close_finish(req);
+ 	io_steal_work(req, workptr);
+ }
+ 
+ static int io_close(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	int ret;
+ 
+ 	req->close.put_file = NULL;
+ 	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	/* if the file has a flush method, be safe and punt to async */
+ 	if (req->close.put_file->f_op->flush && force_nonblock) {
+ 		/* submission ref will be dropped, take it for async */
+ 		refcount_inc(&req->refs);
+ 
+ 		req->work.func = io_close_finish;
+ 		/*
+ 		 * Do manual async queue here to avoid grabbing files - we don't
+ 		 * need the files, and it'll cause io_close_finish() to close
+ 		 * the file again and cause a double CQE entry for this request
+ 		 */
+ 		io_queue_async_work(req);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * No ->flush(), safely close from here and just punt the
+ 	 * fput() to async context.
+ 	 */
+ 	__io_close_finish(req);
+ 	return 0;
+ }
+ 
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
+ static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
++	int ret = 0;
+ 
+ 	if (!req->file)
+ 		return -EBADF;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
+ 		return -EINVAL;
+ 
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->len);
 -	req->sync.flags = READ_ONCE(sqe->sync_range_flags);
 -	return 0;
++	return ret;
  }
  
 -static void __io_sync_file_range(struct io_kiocb *req)
 +static int io_sync_file_range(struct io_kiocb *req,
 +			      const struct io_uring_sqe *sqe,
 +			      bool force_nonblock)
  {
 +	loff_t sqe_off;
 +	loff_t sqe_len;
 +	unsigned flags;
  	int ret;
  
 -	ret = sync_file_range(req->file, req->sync.off, req->sync.len,
 -				req->sync.flags);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -}
 -
 -
 -static void io_sync_file_range_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -
 -	if (io_req_cancelled(req))
 -		return;
 -	__io_sync_file_range(req);
 -	io_put_req(req); /* put submission ref */
 -	if (nxt)
 -		io_wq_assign_next(workptr, nxt);
 -}
 +	ret = io_prep_sfr(req, sqe);
 +	if (ret)
 +		return ret;
  
 -static int io_sync_file_range(struct io_kiocb *req, bool force_nonblock)
 -{
  	/* sync_file_range always requires a blocking context */
 -	if (force_nonblock) {
 -		req->work.func = io_sync_file_range_finish;
 -		return -EAGAIN;
 -	}
 -
 -	__io_sync_file_range(req);
 -	return 0;
 -}
 -
 -static int io_setup_async_msg(struct io_kiocb *req,
 -			      struct io_async_msghdr *kmsg)
 -{
 -	if (req->io)
 +	if (force_nonblock)
  		return -EAGAIN;
 -	if (io_alloc_async_ctx(req)) {
 -		if (kmsg->iov != kmsg->fast_iov)
 -			kfree(kmsg->iov);
 -		return -ENOMEM;
 -	}
 -	req->flags |= REQ_F_NEED_CLEANUP;
 -	memcpy(&req->io->msg, kmsg, sizeof(*kmsg));
 -	return -EAGAIN;
 -}
 -
 -static int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct io_async_ctx *io = req->io;
 -	int ret;
  
 -	sr->msg_flags = READ_ONCE(sqe->msg_flags);
 -	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	sr->len = READ_ONCE(sqe->len);
 -
 -#ifdef CONFIG_COMPAT
 -	if (req->ctx->compat)
 -		sr->msg_flags |= MSG_CMSG_COMPAT;
 -#endif
 +	sqe_off = READ_ONCE(sqe->off);
 +	sqe_len = READ_ONCE(sqe->len);
 +	flags = READ_ONCE(sqe->sync_range_flags);
  
 -	if (!io || req->opcode == IORING_OP_SEND)
 -		return 0;
 -	/* iovec is already imported */
 -	if (req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 +	ret = sync_file_range(req->rw.ki_filp, sqe_off, sqe_len, flags);
  
 -	io->msg.iov = io->msg.fast_iov;
 -	ret = sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
 -					&io->msg.iov);
 -	if (!ret)
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -	return ret;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
 +	return 0;
  }
  
 -static int io_sendmsg(struct io_kiocb *req, bool force_nonblock)
 -{
  #if defined(CONFIG_NET)
 -	struct io_async_msghdr *kmsg = NULL;
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
 +{
  	struct socket *sock;
  	int ret;
  
@@@ -1614,70 -3486,226 +2827,241 @@@ static int io_sendmsg(struct io_kiocb *
  #endif
  }
  
++<<<<<<< HEAD
 +static int io_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		      bool force_nonblock)
 +{
 +#if defined(CONFIG_NET)
 +	return io_send_recvmsg(req, sqe, force_nonblock, __sys_recvmsg_sock);
++=======
+ static struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,
+ 					       int *cflags, bool needs_lock)
+ {
+ 	struct io_sr_msg *sr = &req->sr_msg;
+ 	struct io_buffer *kbuf;
+ 
+ 	if (!(req->flags & REQ_F_BUFFER_SELECT))
+ 		return NULL;
+ 
+ 	kbuf = io_buffer_select(req, &sr->len, sr->bgid, sr->kbuf, needs_lock);
+ 	if (IS_ERR(kbuf))
+ 		return kbuf;
+ 
+ 	sr->kbuf = kbuf;
+ 	req->flags |= REQ_F_BUFFER_SELECTED;
+ 
+ 	*cflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;
+ 	*cflags |= IORING_CQE_F_BUFFER;
+ 	return kbuf;
+ }
+ 
+ static int io_recvmsg_prep(struct io_kiocb *req,
+ 			   const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_sr_msg *sr = &req->sr_msg;
+ 	struct io_async_ctx *io = req->io;
+ 	int ret;
+ 
+ 	sr->msg_flags = READ_ONCE(sqe->msg_flags);
+ 	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	sr->len = READ_ONCE(sqe->len);
+ 	sr->bgid = READ_ONCE(sqe->buf_group);
+ 
+ #ifdef CONFIG_COMPAT
+ 	if (req->ctx->compat)
+ 		sr->msg_flags |= MSG_CMSG_COMPAT;
+ #endif
+ 
+ 	if (!io || req->opcode == IORING_OP_RECV)
+ 		return 0;
+ 	/* iovec is already imported */
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	io->msg.iov = io->msg.fast_iov;
+ 	ret = recvmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
+ 					&io->msg.uaddr, &io->msg.iov);
+ 	if (!ret)
+ 		req->flags |= REQ_F_NEED_CLEANUP;
+ 	return ret;
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  #else
  	return -EOPNOTSUPP;
  #endif
  }
  
 -static int io_recvmsg(struct io_kiocb *req, bool force_nonblock)
 +static void io_poll_remove_one(struct io_kiocb *req)
  {
 -#if defined(CONFIG_NET)
 -	struct io_async_msghdr *kmsg = NULL;
 -	struct socket *sock;
 -	int ret;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 +	struct io_poll_iocb *poll = &req->poll;
  
 -	sock = sock_from_file(req->file, &ret);
 -	if (sock) {
 -		struct io_async_ctx io;
 -		unsigned flags;
 +	spin_lock(&poll->head->lock);
 +	WRITE_ONCE(poll->canceled, true);
 +	if (!list_empty(&poll->wait.entry)) {
 +		list_del_init(&poll->wait.entry);
 +		io_queue_async_work(req->ctx, req);
 +	}
 +	spin_unlock(&poll->head->lock);
  
 -		if (req->io) {
 -			kmsg = &req->io->msg;
 -			kmsg->msg.msg_name = &req->io->msg.addr;
 -			/* if iov is set, it's allocated already */
 -			if (!kmsg->iov)
 -				kmsg->iov = kmsg->fast_iov;
 -			kmsg->msg.msg_iter.iov = kmsg->iov;
 -		} else {
 -			struct io_sr_msg *sr = &req->sr_msg;
 +	list_del_init(&req->list);
 +}
  
 -			kmsg = &io.msg;
 -			kmsg->msg.msg_name = &io.msg.addr;
 +static void io_poll_remove_all(struct io_ring_ctx *ctx)
 +{
 +	struct io_kiocb *req;
  
 -			io.msg.iov = io.msg.fast_iov;
 -			ret = recvmsg_copy_msghdr(&io.msg.msg, sr->msg,
 -					sr->msg_flags, &io.msg.uaddr,
 -					&io.msg.iov);
 -			if (ret)
 -				return ret;
 -		}
 +	spin_lock_irq(&ctx->completion_lock);
 +	while (!list_empty(&ctx->cancel_list)) {
 +		req = list_first_entry(&ctx->cancel_list, struct io_kiocb,list);
 +		io_poll_remove_one(req);
 +	}
 +	spin_unlock_irq(&ctx->completion_lock);
 +}
  
 -		flags = req->sr_msg.msg_flags;
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 +/*
 + * Find a running poll command that matches one specified in sqe->addr,
 + * and remove it if found.
 + */
 +static int io_poll_remove(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +{
 +	struct io_ring_ctx *ctx = req->ctx;
 +	struct io_kiocb *poll_req, *next;
 +	int ret = -ENOENT;
  
 -		ret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.msg,
 -						kmsg->uaddr, flags);
 -		if (force_nonblock && ret == -EAGAIN)
 -			return io_setup_async_msg(req, kmsg);
 -		if (ret == -ERESTARTSYS)
 -			ret = -EINTR;
 +	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 +		return -EINVAL;
 +	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 +	    sqe->poll_events)
 +		return -EINVAL;
 +
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_for_each_entry_safe(poll_req, next, &ctx->cancel_list, list) {
 +		if (READ_ONCE(sqe->addr) == poll_req->user_data) {
 +			io_poll_remove_one(poll_req);
 +			ret = 0;
 +			break;
 +		}
  	}
 +	spin_unlock_irq(&ctx->completion_lock);
  
++<<<<<<< HEAD
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
++=======
+ 	if (kmsg && kmsg->iov != kmsg->fast_iov)
+ 		kfree(kmsg->iov);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ static int io_recv(struct io_kiocb *req, bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_buffer *kbuf = NULL;
+ 	struct socket *sock;
+ 	int ret, cflags = 0;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_sr_msg *sr = &req->sr_msg;
+ 		void __user *buf = sr->buf;
+ 		struct msghdr msg;
+ 		struct iovec iov;
+ 		unsigned flags;
+ 
+ 		kbuf = io_recv_buffer_select(req, &cflags, !force_nonblock);
+ 		if (IS_ERR(kbuf))
+ 			return PTR_ERR(kbuf);
+ 		else if (kbuf)
+ 			buf = u64_to_user_ptr(kbuf->addr);
+ 
+ 		ret = import_single_range(READ, buf, sr->len, &iov,
+ 						&msg.msg_iter);
+ 		if (ret) {
+ 			kfree(kbuf);
+ 			return ret;
+ 		}
+ 
+ 		req->flags |= REQ_F_NEED_CLEANUP;
+ 		msg.msg_name = NULL;
+ 		msg.msg_control = NULL;
+ 		msg.msg_controllen = 0;
+ 		msg.msg_namelen = 0;
+ 		msg.msg_iocb = NULL;
+ 		msg.msg_flags = 0;
+ 
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
+ 		ret = sock_recvmsg(sock, &msg, flags);
+ 		if (force_nonblock && ret == -EAGAIN)
+ 			return -EAGAIN;
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
+ 	}
+ 
+ 	kfree(kbuf);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	__io_cqring_add_event(req, ret, cflags);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req(req);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ 
+ static int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ #if defined(CONFIG_NET)
+ 	struct io_accept *accept = &req->accept;
+ 
+ 	if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
+ 		return -EINVAL;
+ 	if (sqe->ioprio || sqe->len || sqe->buf_index)
+ 		return -EINVAL;
+ 
+ 	accept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	accept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	accept->flags = READ_ONCE(sqe->accept_flags);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
+ #if defined(CONFIG_NET)
+ static int __io_accept(struct io_kiocb *req, bool force_nonblock)
+ {
+ 	struct io_accept *accept = &req->accept;
+ 	unsigned file_flags;
+ 	int ret;
+ 
+ 	file_flags = force_nonblock ? O_NONBLOCK : 0;
+ 	ret = __sys_accept4_file(req->file, file_flags, accept->addr,
+ 					accept->addr_len, accept->flags);
+ 	if (ret == -EAGAIN && force_nonblock)
+ 		return -EAGAIN;
+ 	if (ret == -ERESTARTSYS)
+ 		ret = -EINTR;
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  	io_put_req(req);
  	return 0;
  }
@@@ -1825,95 -3945,1003 +3209,130 @@@ static int io_poll_add(struct io_kiocb 
  		else if (cancel)
  			WRITE_ONCE(poll->canceled, true);
  		else if (!poll->done) /* actually waiting for an event */
 -			io_poll_req_insert(req);
 +			list_add_tail(&req->list, &ctx->cancel_list);
  		spin_unlock(&poll->head->lock);
  	}
 -
 -	return mask;
 -}
 -
 -static bool io_arm_poll_handler(struct io_kiocb *req)
 -{
 -	const struct io_op_def *def = &io_op_defs[req->opcode];
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct async_poll *apoll;
 -	struct io_poll_table ipt;
 -	__poll_t mask, ret;
 -
 -	if (!req->file || !file_can_poll(req->file))
 -		return false;
 -	if (req->flags & (REQ_F_MUST_PUNT | REQ_F_POLLED))
 -		return false;
 -	if (!def->pollin && !def->pollout)
 -		return false;
 -
 -	apoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);
 -	if (unlikely(!apoll))
 -		return false;
 -
 -	req->flags |= REQ_F_POLLED;
 -	memcpy(&apoll->work, &req->work, sizeof(req->work));
 -
 -	/*
 -	 * Don't need a reference here, as we're adding it to the task
 -	 * task_works list. If the task exits, the list is pruned.
 -	 */
 -	req->task = current;
 -	req->apoll = apoll;
 -	INIT_HLIST_NODE(&req->hash_node);
 -
 -	mask = 0;
 -	if (def->pollin)
 -		mask |= POLLIN | POLLRDNORM;
 -	if (def->pollout)
 -		mask |= POLLOUT | POLLWRNORM;
 -	mask |= POLLERR | POLLPRI;
 -
 -	ipt.pt._qproc = io_async_queue_proc;
 -
 -	ret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,
 -					io_async_wake);
 -	if (ret) {
 +	if (mask) { /* no async, we'd stolen it */
  		ipt.error = 0;
 -		apoll->poll.done = true;
 -		spin_unlock_irq(&ctx->completion_lock);
 -		memcpy(&req->work, &apoll->work, sizeof(req->work));
 -		kfree(apoll);
 -		return false;
 +		io_poll_complete(ctx, req, mask);
  	}
  	spin_unlock_irq(&ctx->completion_lock);
 -	trace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,
 -					apoll->poll.events);
 -	return true;
 -}
  
 -static bool __io_poll_remove_one(struct io_kiocb *req,
 -				 struct io_poll_iocb *poll)
 -{
 -	bool do_complete = false;
 -
 -	spin_lock(&poll->head->lock);
 -	WRITE_ONCE(poll->canceled, true);
 -	if (!list_empty(&poll->wait.entry)) {
 -		list_del_init(&poll->wait.entry);
 -		do_complete = true;
 +	if (mask) {
 +		io_cqring_ev_posted(ctx);
 +		io_put_req(req);
  	}
 -	spin_unlock(&poll->head->lock);
 -	return do_complete;
 +	return ipt.error;
  }
  
 -static bool io_poll_remove_one(struct io_kiocb *req)
 +static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			const struct io_uring_sqe *sqe)
  {
 -	bool do_complete;
 -
 -	if (req->opcode == IORING_OP_POLL_ADD) {
 -		do_complete = __io_poll_remove_one(req, &req->poll);
 -	} else {
 -		/* non-poll requests have submit ref still */
 -		do_complete = __io_poll_remove_one(req, &req->apoll->poll);
 -		if (do_complete)
 -			io_put_req(req);
 -	}
 -
 -	hash_del(&req->hash_node);
 +	struct io_uring_sqe *sqe_copy;
  
 -	if (do_complete) {
 -		io_cqring_fill_event(req, -ECANCELED);
 -		io_commit_cqring(req->ctx);
 -		req->flags |= REQ_F_COMP_LOCKED;
 -		io_put_req(req);
 -	}
 -
 -	return do_complete;
 -}
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list))
 +		return 0;
  
 -static void io_poll_remove_all(struct io_ring_ctx *ctx)
 -{
 -	struct hlist_node *tmp;
 -	struct io_kiocb *req;
 -	int i;
 +	sqe_copy = kmalloc(sizeof(*sqe_copy), GFP_KERNEL);
 +	if (!sqe_copy)
 +		return -EAGAIN;
  
  	spin_lock_irq(&ctx->completion_lock);
 -	for (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {
 -		struct hlist_head *list;
 -
 -		list = &ctx->cancel_hash[i];
 -		hlist_for_each_entry_safe(req, tmp, list, hash_node)
 -			io_poll_remove_one(req);
 +	if (!io_sequence_defer(ctx, req) && list_empty(&ctx->defer_list)) {
 +		spin_unlock_irq(&ctx->completion_lock);
 +		kfree(sqe_copy);
 +		return 0;
  	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	io_cqring_ev_posted(ctx);
 -}
 -
 -static int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)
 -{
 -	struct hlist_head *list;
 -	struct io_kiocb *req;
  
 -	list = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];
 -	hlist_for_each_entry(req, list, hash_node) {
 -		if (sqe_addr != req->user_data)
 -			continue;
 -		if (io_poll_remove_one(req))
 -			return 0;
 -		return -EALREADY;
 -	}
 +	memcpy(sqe_copy, sqe, sizeof(*sqe_copy));
 +	req->submit.sqe = sqe_copy;
  
 -	return -ENOENT;
 +	INIT_WORK(&req->work, io_sq_wq_submit_work);
 +	list_add_tail(&req->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +	return -EIOCBQUEUED;
  }
  
 -static int io_poll_remove_prep(struct io_kiocb *req,
 -			       const struct io_uring_sqe *sqe)
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
  {
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||
 -	    sqe->poll_events)
 -		return -EINVAL;
 -
 -	req->poll.addr = READ_ONCE(sqe->addr);
 -	return 0;
 -}
 -
 -/*
 - * Find a running poll command that matches one specified in sqe->addr,
 - * and remove it if found.
 - */
 -static int io_poll_remove(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	u64 addr;
 -	int ret;
 -
 -	addr = req->poll.addr;
 -	spin_lock_irq(&ctx->completion_lock);
 -	ret = io_poll_cancel(ctx, addr);
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	io_cqring_add_event(req, ret);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static void io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	req->poll.done = true;
 -	io_cqring_fill_event(req, error ? error : mangle_poll(mask));
 -	io_commit_cqring(ctx);
 -}
 -
 -static void io_poll_task_handler(struct io_kiocb *req, struct io_kiocb **nxt)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	hash_del(&req->hash_node);
 -	io_poll_complete(req, req->result, 0);
 -	req->flags |= REQ_F_COMP_LOCKED;
 -	io_put_req_find_next(req, nxt);
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	io_cqring_ev_posted(ctx);
 -}
 -
 -static void io_poll_task_func(struct callback_head *cb)
 -{
 -	struct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);
 -	struct io_kiocb *nxt = NULL;
 -
 -	io_poll_task_handler(req, &nxt);
 -	if (nxt) {
 -		struct io_ring_ctx *ctx = nxt->ctx;
 -
 -		mutex_lock(&ctx->uring_lock);
 -		__io_queue_sqe(nxt, NULL);
 -		mutex_unlock(&ctx->uring_lock);
 -	}
 -}
 -
 -static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 -			void *key)
 -{
 -	struct io_kiocb *req = wait->private;
 -	struct io_poll_iocb *poll = &req->poll;
 -
 -	return __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);
 -}
 -
 -static void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,
 -			       struct poll_table_struct *p)
 -{
 -	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
 -
 -	__io_queue_proc(&pt->req->poll, pt, head);
 -}
 -
 -static int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	struct io_poll_iocb *poll = &req->poll;
 -	u16 events;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)
 -		return -EINVAL;
 -	if (!poll->file)
 -		return -EBADF;
 -
 -	events = READ_ONCE(sqe->poll_events);
 -	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
 -
 -	/*
 -	 * Don't need a reference here, as we're adding it to the task
 -	 * task_works list. If the task exits, the list is pruned.
 -	 */
 -	req->task = current;
 -	return 0;
 -}
 -
 -static int io_poll_add(struct io_kiocb *req)
 -{
 -	struct io_poll_iocb *poll = &req->poll;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_poll_table ipt;
 -	__poll_t mask;
 -
 -	INIT_HLIST_NODE(&req->hash_node);
 -	INIT_LIST_HEAD(&req->list);
 -	ipt.pt._qproc = io_poll_queue_proc;
 -
 -	mask = __io_arm_poll_handler(req, &req->poll, &ipt, poll->events,
 -					io_poll_wake);
 -
 -	if (mask) { /* no async, we'd stolen it */
 -		ipt.error = 0;
 -		io_poll_complete(req, mask, 0);
 -	}
 -	spin_unlock_irq(&ctx->completion_lock);
 -
 -	if (mask) {
 -		io_cqring_ev_posted(ctx);
 -		io_put_req(req);
 -	}
 -	return ipt.error;
 -}
 -
 -static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
 -{
 -	struct io_timeout_data *data = container_of(timer,
 -						struct io_timeout_data, timer);
 -	struct io_kiocb *req = data->req;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned long flags;
 -
 -	atomic_inc(&ctx->cq_timeouts);
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -	/*
 -	 * We could be racing with timeout deletion. If the list is empty,
 -	 * then timeout lookup already found it and will be handling it.
 -	 */
 -	if (!list_empty(&req->list)) {
 -		struct io_kiocb *prev;
 -
 -		/*
 -		 * Adjust the reqs sequence before the current one because it
 -		 * will consume a slot in the cq_ring and the cq_tail
 -		 * pointer will be increased, otherwise other timeout reqs may
 -		 * return in advance without waiting for enough wait_nr.
 -		 */
 -		prev = req;
 -		list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
 -			prev->sequence++;
 -		list_del_init(&req->list);
 -	}
 -
 -	io_cqring_fill_event(req, -ETIME);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -
 -	io_cqring_ev_posted(ctx);
 -	req_set_fail_links(req);
 -	io_put_req(req);
 -	return HRTIMER_NORESTART;
 -}
 -
 -static int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)
 -{
 -	struct io_kiocb *req;
 -	int ret = -ENOENT;
 -
 -	list_for_each_entry(req, &ctx->timeout_list, list) {
 -		if (user_data == req->user_data) {
 -			list_del_init(&req->list);
 -			ret = 0;
 -			break;
 -		}
 -	}
 -
 -	if (ret == -ENOENT)
 -		return ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
 -	if (ret == -1)
 -		return -EALREADY;
 -
 -	req_set_fail_links(req);
 -	io_cqring_fill_event(req, -ECANCELED);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_timeout_remove_prep(struct io_kiocb *req,
 -				  const struct io_uring_sqe *sqe)
 -{
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->len)
 -		return -EINVAL;
 -
 -	req->timeout.addr = READ_ONCE(sqe->addr);
 -	req->timeout.flags = READ_ONCE(sqe->timeout_flags);
 -	if (req->timeout.flags)
 -		return -EINVAL;
 -
 -	return 0;
 -}
 -
 -/*
 - * Remove or update an existing timeout command
 - */
 -static int io_timeout_remove(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	ret = io_timeout_cancel(ctx, req->timeout.addr);
 -
 -	io_cqring_fill_event(req, ret);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irq(&ctx->completion_lock);
 -	io_cqring_ev_posted(ctx);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			   bool is_timeout_link)
 -{
 -	struct io_timeout_data *data;
 -	unsigned flags;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->ioprio || sqe->buf_index || sqe->len != 1)
 -		return -EINVAL;
 -	if (sqe->off && is_timeout_link)
 -		return -EINVAL;
 -	flags = READ_ONCE(sqe->timeout_flags);
 -	if (flags & ~IORING_TIMEOUT_ABS)
 -		return -EINVAL;
 -
 -	req->timeout.count = READ_ONCE(sqe->off);
 -
 -	if (!req->io && io_alloc_async_ctx(req))
 -		return -ENOMEM;
 -
 -	data = &req->io->timeout;
 -	data->req = req;
 -	req->flags |= REQ_F_TIMEOUT;
 -
 -	if (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))
 -		return -EFAULT;
 -
 -	if (flags & IORING_TIMEOUT_ABS)
 -		data->mode = HRTIMER_MODE_ABS;
 -	else
 -		data->mode = HRTIMER_MODE_REL;
 -
 -	hrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);
 -	return 0;
 -}
 -
 -static int io_timeout(struct io_kiocb *req)
 -{
 -	unsigned count;
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_timeout_data *data;
 -	struct list_head *entry;
 -	unsigned span = 0;
 -
 -	data = &req->io->timeout;
 -
 -	/*
 -	 * sqe->off holds how many events that need to occur for this
 -	 * timeout event to be satisfied. If it isn't set, then this is
 -	 * a pure timeout request, sequence isn't used.
 -	 */
 -	count = req->timeout.count;
 -	if (!count) {
 -		req->flags |= REQ_F_TIMEOUT_NOSEQ;
 -		spin_lock_irq(&ctx->completion_lock);
 -		entry = ctx->timeout_list.prev;
 -		goto add;
 -	}
 -
 -	req->sequence = ctx->cached_sq_head + count - 1;
 -	data->seq_offset = count;
 -
 -	/*
 -	 * Insertion sort, ensuring the first entry in the list is always
 -	 * the one we need first.
 -	 */
 -	spin_lock_irq(&ctx->completion_lock);
 -	list_for_each_prev(entry, &ctx->timeout_list) {
 -		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
 -		unsigned nxt_sq_head;
 -		long long tmp, tmp_nxt;
 -		u32 nxt_offset = nxt->io->timeout.seq_offset;
 -
 -		if (nxt->flags & REQ_F_TIMEOUT_NOSEQ)
 -			continue;
 -
 -		/*
 -		 * Since cached_sq_head + count - 1 can overflow, use type long
 -		 * long to store it.
 -		 */
 -		tmp = (long long)ctx->cached_sq_head + count - 1;
 -		nxt_sq_head = nxt->sequence - nxt_offset + 1;
 -		tmp_nxt = (long long)nxt_sq_head + nxt_offset - 1;
 -
 -		/*
 -		 * cached_sq_head may overflow, and it will never overflow twice
 -		 * once there is some timeout req still be valid.
 -		 */
 -		if (ctx->cached_sq_head < nxt_sq_head)
 -			tmp += UINT_MAX;
 -
 -		if (tmp > tmp_nxt)
 -			break;
 -
 -		/*
 -		 * Sequence of reqs after the insert one and itself should
 -		 * be adjusted because each timeout req consumes a slot.
 -		 */
 -		span++;
 -		nxt->sequence++;
 -	}
 -	req->sequence -= span;
 -add:
 -	list_add(&req->list, entry);
 -	data->timer.function = io_timeout_fn;
 -	hrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);
 -	spin_unlock_irq(&ctx->completion_lock);
 -	return 0;
 -}
 -
 -static bool io_cancel_cb(struct io_wq_work *work, void *data)
 -{
 -	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
 -
 -	return req->user_data == (unsigned long) data;
 -}
 -
 -static int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)
 -{
 -	enum io_wq_cancel cancel_ret;
 -	int ret = 0;
 -
 -	cancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr);
 -	switch (cancel_ret) {
 -	case IO_WQ_CANCEL_OK:
 -		ret = 0;
 -		break;
 -	case IO_WQ_CANCEL_RUNNING:
 -		ret = -EALREADY;
 -		break;
 -	case IO_WQ_CANCEL_NOTFOUND:
 -		ret = -ENOENT;
 -		break;
 -	}
 -
 -	return ret;
 -}
 -
 -static void io_async_find_and_cancel(struct io_ring_ctx *ctx,
 -				     struct io_kiocb *req, __u64 sqe_addr,
 -				     int success_ret)
 -{
 -	unsigned long flags;
 -	int ret;
 -
 -	ret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);
 -	if (ret != -ENOENT) {
 -		spin_lock_irqsave(&ctx->completion_lock, flags);
 -		goto done;
 -	}
 -
 -	spin_lock_irqsave(&ctx->completion_lock, flags);
 -	ret = io_timeout_cancel(ctx, sqe_addr);
 -	if (ret != -ENOENT)
 -		goto done;
 -	ret = io_poll_cancel(ctx, sqe_addr);
 -done:
 -	if (!ret)
 -		ret = success_ret;
 -	io_cqring_fill_event(req, ret);
 -	io_commit_cqring(ctx);
 -	spin_unlock_irqrestore(&ctx->completion_lock, flags);
 -	io_cqring_ev_posted(ctx);
 -
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_put_req(req);
 -}
 -
 -static int io_async_cancel_prep(struct io_kiocb *req,
 -				const struct io_uring_sqe *sqe)
 -{
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (sqe->flags || sqe->ioprio || sqe->off || sqe->len ||
 -	    sqe->cancel_flags)
 -		return -EINVAL;
 -
 -	req->cancel.addr = READ_ONCE(sqe->addr);
 -	return 0;
 -}
 -
 -static int io_async_cancel(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	io_async_find_and_cancel(ctx, req, req->cancel.addr, 0);
 -	return 0;
 -}
 -
 -static int io_files_update_prep(struct io_kiocb *req,
 -				const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->flags || sqe->ioprio || sqe->rw_flags)
 -		return -EINVAL;
 -
 -	req->files_update.offset = READ_ONCE(sqe->off);
 -	req->files_update.nr_args = READ_ONCE(sqe->len);
 -	if (!req->files_update.nr_args)
 -		return -EINVAL;
 -	req->files_update.arg = READ_ONCE(sqe->addr);
 -	return 0;
 -}
 -
 -static int io_files_update(struct io_kiocb *req, bool force_nonblock)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	struct io_uring_files_update up;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 -
 -	up.offset = req->files_update.offset;
 -	up.fds = req->files_update.arg;
 -
 -	mutex_lock(&ctx->uring_lock);
 -	ret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);
 -	mutex_unlock(&ctx->uring_lock);
 -
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req(req);
 -	return 0;
 -}
 -
 -static int io_req_defer_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -	ssize_t ret = 0;
 -
 -	if (io_op_defs[req->opcode].file_table) {
 -		ret = io_grab_files(req);
 -		if (unlikely(ret))
 -			return ret;
 -	}
 -
 -	io_req_work_grab_env(req, &io_op_defs[req->opcode]);
 -
 -	switch (req->opcode) {
 -	case IORING_OP_NOP:
 -		break;
 -	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -	case IORING_OP_READ:
 -		ret = io_read_prep(req, sqe, true);
 -		break;
 -	case IORING_OP_WRITEV:
 -	case IORING_OP_WRITE_FIXED:
 -	case IORING_OP_WRITE:
 -		ret = io_write_prep(req, sqe, true);
 -		break;
 -	case IORING_OP_POLL_ADD:
 -		ret = io_poll_add_prep(req, sqe);
 -		break;
 -	case IORING_OP_POLL_REMOVE:
 -		ret = io_poll_remove_prep(req, sqe);
 -		break;
 -	case IORING_OP_FSYNC:
 -		ret = io_prep_fsync(req, sqe);
 -		break;
 -	case IORING_OP_SYNC_FILE_RANGE:
 -		ret = io_prep_sfr(req, sqe);
 -		break;
 -	case IORING_OP_SENDMSG:
 -	case IORING_OP_SEND:
 -		ret = io_sendmsg_prep(req, sqe);
 -		break;
 -	case IORING_OP_RECVMSG:
 -	case IORING_OP_RECV:
 -		ret = io_recvmsg_prep(req, sqe);
 -		break;
 -	case IORING_OP_CONNECT:
 -		ret = io_connect_prep(req, sqe);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		ret = io_timeout_prep(req, sqe, false);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		ret = io_timeout_remove_prep(req, sqe);
 -		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		ret = io_async_cancel_prep(req, sqe);
 -		break;
 -	case IORING_OP_LINK_TIMEOUT:
 -		ret = io_timeout_prep(req, sqe, true);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		ret = io_accept_prep(req, sqe);
 -		break;
 -	case IORING_OP_FALLOCATE:
 -		ret = io_fallocate_prep(req, sqe);
 -		break;
 -	case IORING_OP_OPENAT:
 -		ret = io_openat_prep(req, sqe);
 -		break;
 -	case IORING_OP_CLOSE:
 -		ret = io_close_prep(req, sqe);
 -		break;
 -	case IORING_OP_FILES_UPDATE:
 -		ret = io_files_update_prep(req, sqe);
 -		break;
 -	case IORING_OP_STATX:
 -		ret = io_statx_prep(req, sqe);
 -		break;
 -	case IORING_OP_FADVISE:
 -		ret = io_fadvise_prep(req, sqe);
 -		break;
 -	case IORING_OP_MADVISE:
 -		ret = io_madvise_prep(req, sqe);
 -		break;
 -	case IORING_OP_OPENAT2:
 -		ret = io_openat2_prep(req, sqe);
 -		break;
 -	case IORING_OP_EPOLL_CTL:
 -		ret = io_epoll_ctl_prep(req, sqe);
 -		break;
 -	case IORING_OP_SPLICE:
 -		ret = io_splice_prep(req, sqe);
 -		break;
 -	case IORING_OP_PROVIDE_BUFFERS:
 -		ret = io_provide_buffers_prep(req, sqe);
 -		break;
 -	default:
 -		printk_once(KERN_WARNING "io_uring: unhandled opcode %d\n",
 -				req->opcode);
 -		ret = -EINVAL;
 -		break;
 -	}
 -
 -	return ret;
 -}
 -
 -static int io_req_defer(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	/* Still need defer if there is pending req in defer list. */
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list))
 -		return 0;
 -
 -	if (!req->io && io_alloc_async_ctx(req))
 -		return -EAGAIN;
 -
 -	ret = io_req_defer_prep(req, sqe);
 -	if (ret < 0)
 -		return ret;
 -
 -	spin_lock_irq(&ctx->completion_lock);
 -	if (!req_need_defer(req) && list_empty(&ctx->defer_list)) {
 -		spin_unlock_irq(&ctx->completion_lock);
 -		return 0;
 -	}
 -
 -	trace_io_uring_defer(ctx, req, req->user_data);
 -	list_add_tail(&req->list, &ctx->defer_list);
 -	spin_unlock_irq(&ctx->completion_lock);
 -	return -EIOCBQUEUED;
 -}
 -
 -static void io_cleanup_req(struct io_kiocb *req)
 -{
 -	struct io_async_ctx *io = req->io;
 +	int ret, opcode;
  
++<<<<<<< HEAD
 +	req->user_data = READ_ONCE(s->sqe->user_data);
++=======
+ 	switch (req->opcode) {
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 		if (req->flags & REQ_F_BUFFER_SELECTED)
+ 			kfree((void *)(unsigned long)req->rw.addr);
+ 		/* fallthrough */
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 	case IORING_OP_WRITE:
+ 		if (io->rw.iov != io->rw.fast_iov)
+ 			kfree(io->rw.iov);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 	case IORING_OP_RECVMSG:
+ 		if (io->msg.iov != io->msg.fast_iov)
+ 			kfree(io->msg.iov);
+ 		break;
+ 	case IORING_OP_RECV:
+ 		if (req->flags & REQ_F_BUFFER_SELECTED)
+ 			kfree(req->sr_msg.kbuf);
+ 		break;
+ 	case IORING_OP_OPENAT:
+ 	case IORING_OP_OPENAT2:
+ 	case IORING_OP_STATX:
+ 		putname(req->open.filename);
+ 		break;
+ 	case IORING_OP_SPLICE:
+ 		io_put_file(req, req->splice.file_in,
+ 			    (req->splice.flags & SPLICE_F_FD_IN_FIXED));
+ 		break;
+ 	}
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  
 -	req->flags &= ~REQ_F_NEED_CLEANUP;
 -}
 -
 -static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			bool force_nonblock)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	switch (req->opcode) {
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
  	case IORING_OP_NOP:
 -		ret = io_nop(req);
 +		ret = io_nop(req, req->user_data);
  		break;
  	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -	case IORING_OP_READ:
 -		if (sqe) {
 -			ret = io_read_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_read(req, force_nonblock);
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
  		break;
  	case IORING_OP_WRITEV:
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
 +		break;
 +	case IORING_OP_READ_FIXED:
 +		ret = io_read(req, s, force_nonblock);
 +		break;
  	case IORING_OP_WRITE_FIXED:
 -	case IORING_OP_WRITE:
 -		if (sqe) {
 -			ret = io_write_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_write(req, force_nonblock);
 +		ret = io_write(req, s, force_nonblock);
  		break;
  	case IORING_OP_FSYNC:
 -		if (sqe) {
 -			ret = io_prep_fsync(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_fsync(req, force_nonblock);
 +		ret = io_fsync(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_POLL_ADD:
 -		if (sqe) {
 -			ret = io_poll_add_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_poll_add(req);
 +		ret = io_poll_add(req, s->sqe);
  		break;
  	case IORING_OP_POLL_REMOVE:
 -		if (sqe) {
 -			ret = io_poll_remove_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_poll_remove(req);
 +		ret = io_poll_remove(req, s->sqe);
  		break;
  	case IORING_OP_SYNC_FILE_RANGE:
 -		if (sqe) {
 -			ret = io_prep_sfr(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_sync_file_range(req, force_nonblock);
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_SENDMSG:
 -	case IORING_OP_SEND:
 -		if (sqe) {
 -			ret = io_sendmsg_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_SENDMSG)
 -			ret = io_sendmsg(req, force_nonblock);
 -		else
 -			ret = io_send(req, force_nonblock);
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
  		break;
  	case IORING_OP_RECVMSG:
 -	case IORING_OP_RECV:
 -		if (sqe) {
 -			ret = io_recvmsg_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_RECVMSG)
 -			ret = io_recvmsg(req, force_nonblock);
 -		else
 -			ret = io_recv(req, force_nonblock);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		if (sqe) {
 -			ret = io_timeout_prep(req, sqe, false);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout(req);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		if (sqe) {
 -			ret = io_timeout_remove_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout_remove(req);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		if (sqe) {
 -			ret = io_accept_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_accept(req, force_nonblock);
 -		break;
 -	case IORING_OP_CONNECT:
 -		if (sqe) {
 -			ret = io_connect_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_connect(req, force_nonblock);
 -		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		if (sqe) {
 -			ret = io_async_cancel_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_async_cancel(req);
 -		break;
 -	case IORING_OP_FALLOCATE:
 -		if (sqe) {
 -			ret = io_fallocate_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fallocate(req, force_nonblock);
 -		break;
 -	case IORING_OP_OPENAT:
 -		if (sqe) {
 -			ret = io_openat_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat(req, force_nonblock);
 -		break;
 -	case IORING_OP_CLOSE:
 -		if (sqe) {
 -			ret = io_close_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_close(req, force_nonblock);
 -		break;
 -	case IORING_OP_FILES_UPDATE:
 -		if (sqe) {
 -			ret = io_files_update_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_files_update(req, force_nonblock);
 -		break;
 -	case IORING_OP_STATX:
 -		if (sqe) {
 -			ret = io_statx_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_statx(req, force_nonblock);
 -		break;
 -	case IORING_OP_FADVISE:
 -		if (sqe) {
 -			ret = io_fadvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fadvise(req, force_nonblock);
 -		break;
 -	case IORING_OP_MADVISE:
 -		if (sqe) {
 -			ret = io_madvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_madvise(req, force_nonblock);
 -		break;
 -	case IORING_OP_OPENAT2:
 -		if (sqe) {
 -			ret = io_openat2_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat2(req, force_nonblock);
 -		break;
 -	case IORING_OP_EPOLL_CTL:
 -		if (sqe) {
 -			ret = io_epoll_ctl_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_epoll_ctl(req, force_nonblock);
 -		break;
 -	case IORING_OP_SPLICE:
 -		if (sqe) {
 -			ret = io_splice_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_splice(req, force_nonblock);
 -		break;
 -	case IORING_OP_PROVIDE_BUFFERS:
 -		if (sqe) {
 -			ret = io_provide_buffers_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_provide_buffers(req, force_nonblock);
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
  		break;
  	default:
  		ret = -EINVAL;
@@@ -2202,108 -5239,106 +3621,139 @@@ static int __io_queue_sqe(struct io_rin
  
  	/* and drop final reference, if we failed */
  	if (ret) {
 -		io_cqring_add_event(req, ret);
 -		req_set_fail_links(req);
 +		io_cqring_add_event(ctx, req->user_data, ret);
 +		if (req->flags & REQ_F_LINK)
 +			req->flags |= REQ_F_FAIL_LINK;
  		io_put_req(req);
  	}
 -	if (nxt) {
 -		req = nxt;
  
 -		if (req->flags & REQ_F_FORCE_ASYNC)
 -			goto punt;
 -		goto again;
 +	return ret;
 +}
 +
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
 +{
 +	int ret;
 +
 +	ret = io_req_defer(ctx, req, s->sqe);
 +	if (ret) {
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		}
 +		return 0;
  	}
 -exit:
 -	if (old_creds)
 -		revert_creds(old_creds);
 +
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
  }
  
 -static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
  {
  	int ret;
 +	int need_submit = false;
  
 -	ret = io_req_defer(req, sqe);
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	/*
 +	 * Mark the first IO in link list as DRAIN, let all the following
 +	 * IOs enter the defer list. all IO needs to be completed before link
 +	 * list.
 +	 */
 +	req->flags |= REQ_F_IO_DRAIN;
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -fail_req:
 -			io_cqring_add_event(req, ret);
 -			req_set_fail_links(req);
 -			io_double_put_req(req);
 +			io_free_req(req);
 +			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
  		}
 -	} else if (req->flags & REQ_F_FORCE_ASYNC) {
 -		ret = io_req_defer_prep(req, sqe);
 -		if (unlikely(ret < 0))
 -			goto fail_req;
 +	} else {
  		/*
 -		 * Never try inline submit of IOSQE_ASYNC is set, go straight
 -		 * to async execution.
 +		 * If ret == 0 means that all IOs in front of link io are
 +		 * running done. let's queue link head.
  		 */
 -		req->work.flags |= IO_WQ_WORK_CONCURRENT;
 -		io_queue_async_work(req);
 -	} else {
 -		__io_queue_sqe(req, sqe);
 +		need_submit = true;
  	}
 -}
  
 -static inline void io_queue_link_head(struct io_kiocb *req)
 -{
 -	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_double_put_req(req);
 -	} else
 -		io_queue_sqe(req, NULL);
 +	/* Insert shadow req to defer_list, blocking next IOs */
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_add_tail(&shadow->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	if (need_submit)
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	return 0;
  }
  
++<<<<<<< HEAD
 +#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
++=======
+ #define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|	\
+ 				IOSQE_IO_HARDLINK | IOSQE_ASYNC | \
+ 				IOSQE_BUFFER_SELECT)
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  
 -static bool io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			  struct io_submit_state *state, struct io_kiocb **link)
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned int sqe_flags;
 -	int ret, id;
 -
 -	sqe_flags = READ_ONCE(sqe->flags);
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
 +	int ret;
  
  	/* enforce forwards compatibility on users */
 -	if (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {
 +	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
  		ret = -EINVAL;
 -		goto err_req;
 +		goto err;
 +	}
 +
++<<<<<<< HEAD
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
  	}
  
 +	ret = io_req_set_file(ctx, s, state, req);
++=======
+ 	if ((sqe_flags & IOSQE_BUFFER_SELECT) &&
+ 	    !io_op_defs[req->opcode].buffer_select) {
+ 		ret = -EOPNOTSUPP;
+ 		goto err_req;
+ 	}
+ 
+ 	id = READ_ONCE(sqe->personality);
+ 	if (id) {
+ 		req->work.creds = idr_find(&ctx->personality_idr, id);
+ 		if (unlikely(!req->work.creds)) {
+ 			ret = -EINVAL;
+ 			goto err_req;
+ 		}
+ 		get_cred(req->work.creds);
+ 	}
+ 
+ 	/* same numerical values with corresponding REQ_F_*, safe to copy */
+ 	req->flags |= sqe_flags & (IOSQE_IO_DRAIN | IOSQE_IO_HARDLINK |
+ 					IOSQE_ASYNC | IOSQE_FIXED_FILE |
+ 					IOSQE_BUFFER_SELECT);
+ 
+ 	ret = io_req_set_file(state, req, sqe);
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  	if (unlikely(ret)) {
  err_req:
 -		io_cqring_add_event(req, ret);
 -		io_double_put_req(req);
 -		return false;
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		return;
  	}
  
 +	req->user_data = s->sqe->user_data;
 +
  	/*
  	 * If we already have a head request, queue this one for async
  	 * submittal once the head completes. If we don't have a head but
diff --cc include/uapi/linux/io_uring.h
index dd4a49ec83b7,9b263d9b24e6..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -36,12 -60,30 +36,39 @@@ struct io_uring_sqe 
  	};
  };
  
++<<<<<<< HEAD
 +/*
 + * sqe->flags
 + */
 +#define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
 +#define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
 +#define IOSQE_IO_LINK		(1U << 2)	/* links next sqe */
++=======
+ enum {
+ 	IOSQE_FIXED_FILE_BIT,
+ 	IOSQE_IO_DRAIN_BIT,
+ 	IOSQE_IO_LINK_BIT,
+ 	IOSQE_IO_HARDLINK_BIT,
+ 	IOSQE_ASYNC_BIT,
+ 	IOSQE_BUFFER_SELECT_BIT,
+ };
+ 
+ /*
+  * sqe->flags
+  */
+ /* use fixed fileset */
+ #define IOSQE_FIXED_FILE	(1U << IOSQE_FIXED_FILE_BIT)
+ /* issue after inflight IO */
+ #define IOSQE_IO_DRAIN		(1U << IOSQE_IO_DRAIN_BIT)
+ /* links next sqe */
+ #define IOSQE_IO_LINK		(1U << IOSQE_IO_LINK_BIT)
+ /* like LINK, but stronger */
+ #define IOSQE_IO_HARDLINK	(1U << IOSQE_IO_HARDLINK_BIT)
+ /* always go async */
+ #define IOSQE_ASYNC		(1U << IOSQE_ASYNC_BIT)
+ /* select buffer from sqe->buf_group */
+ #define IOSQE_BUFFER_SELECT	(1U << IOSQE_BUFFER_SELECT_BIT)
++>>>>>>> bcda7baaa3f1 (io_uring: support buffer selection for OP_READ and OP_RECV)
  
  /*
   * io_uring_setup() flags
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
