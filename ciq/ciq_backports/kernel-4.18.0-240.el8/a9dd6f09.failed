KVM: x86: Allocate vcpu struct in common x86 code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit a9dd6f09d7e54d3f58be32d7d051196f7a00e69e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/a9dd6f09.failed

Move allocation of VMX and SVM vcpus to common x86.  Although the struct
being allocated is technically a VMX/SVM struct, it can be interpreted
directly as a 'struct kvm_vcpu' because of the pre-existing requirement
that 'struct kvm_vcpu' be located at offset zero of the arch/vendor vcpu
struct.

Remove the message from the build-time assertions regarding placement of
the struct, as compatibility with the arch usercopy region is no longer
the sole dependent on 'struct kvm_vcpu' being at offset zero.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit a9dd6f09d7e54d3f58be32d7d051196f7a00e69e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/svm.c
index 4b247e55061a,319c487e2222..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -2210,34 -2197,28 +2211,39 @@@ static int svm_create_vcpu(struct kvm *
  	struct page *nested_msrpm_pages;
  	int err;
  
++<<<<<<< HEAD
 +	BUILD_BUG_ON_MSG(offsetof(struct vcpu_svm, vcpu) != 0,
 +		"struct kvm_vcpu must be at offset 0 for arch usercopy region");
 +
 +	svm = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL_ACCOUNT);
 +	if (!svm) {
 +		err = -ENOMEM;
 +		goto out;
 +	}
++=======
+ 	BUILD_BUG_ON(offsetof(struct vcpu_svm, vcpu) != 0);
+ 	svm = to_svm(vcpu);
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  
 -	vcpu->arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
 -						GFP_KERNEL_ACCOUNT);
 -	if (!vcpu->arch.user_fpu) {
 +	svm->vcpu.arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +						     GFP_KERNEL_ACCOUNT);
 +	if (!svm->vcpu.arch.user_fpu) {
  		printk(KERN_ERR "kvm: failed to allocate kvm userspace's fpu\n");
  		err = -ENOMEM;
- 		goto free_partial_svm;
+ 		goto out;
  	}
  
 -	vcpu->arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
 -						 GFP_KERNEL_ACCOUNT);
 -	if (!vcpu->arch.guest_fpu) {
 +	svm->vcpu.arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +						     GFP_KERNEL_ACCOUNT);
 +	if (!svm->vcpu.arch.guest_fpu) {
  		printk(KERN_ERR "kvm: failed to allocate vcpu's fpu\n");
  		err = -ENOMEM;
  		goto free_user_fpu;
  	}
  
 -	err = kvm_vcpu_init(vcpu, kvm, id);
 +	err = kvm_vcpu_init(&svm->vcpu, kvm, id);
  	if (err)
- 		goto free_svm;
+ 		goto free_guest_fpu;
  
  	err = -ENOMEM;
  	page = alloc_page(GFP_KERNEL_ACCOUNT);
@@@ -2279,10 -2260,9 +2285,14 @@@
  	svm->asid_generation = 0;
  	init_vmcb(svm);
  
 -	svm_init_osvw(vcpu);
 +	svm_init_osvw(&svm->vcpu);
 +	svm->vcpu.arch.microcode_version = 0x01000065;
  
++<<<<<<< HEAD
 +	return &svm->vcpu;
++=======
+ 	return 0;
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  
  free_page4:
  	__free_page(hsave_page);
@@@ -2293,15 -2273,13 +2303,23 @@@ free_page2
  free_page1:
  	__free_page(page);
  uninit:
++<<<<<<< HEAD
 +	kvm_vcpu_uninit(&svm->vcpu);
 +free_svm:
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.guest_fpu);
 +free_user_fpu:
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.user_fpu);
 +free_partial_svm:
 +	kmem_cache_free(kvm_vcpu_cache, svm);
++=======
+ 	kvm_vcpu_uninit(vcpu);
+ free_guest_fpu:
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
+ free_user_fpu:
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  out:
- 	return ERR_PTR(err);
+ 	return err;
  }
  
  static void svm_clear_current_vmcb(struct vmcb *vmcb)
@@@ -2328,9 -2306,8 +2346,14 @@@ static void svm_free_vcpu(struct kvm_vc
  	__free_page(virt_to_page(svm->nested.hsave));
  	__free_pages(virt_to_page(svm->nested.msrpm), MSRPM_ALLOC_ORDER);
  	kvm_vcpu_uninit(vcpu);
++<<<<<<< HEAD
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.user_fpu);
 +	kmem_cache_free(x86_fpu_cache, svm->vcpu.arch.guest_fpu);
 +	kmem_cache_free(kvm_vcpu_cache, svm);
++=======
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  }
  
  static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
diff --cc arch/x86/kvm/vmx/vmx.c
index aace07a1fbd9,2cbeb0a638aa..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -6753,36 -6682,31 +6753,54 @@@ static void vmx_free_vcpu(struct kvm_vc
  	nested_vmx_free_vcpu(vcpu);
  	free_loaded_vmcs(vmx->loaded_vmcs);
  	kvm_vcpu_uninit(vcpu);
++<<<<<<< HEAD
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.user_fpu);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
++=======
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  }
  
- static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
+ static int vmx_create_vcpu(struct kvm *kvm, struct kvm_vcpu *vcpu,
+ 			   unsigned int id)
  {
++<<<<<<< HEAD
 +	int err;
++=======
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  	struct vcpu_vmx *vmx;
  	unsigned long *msr_bitmap;
 -	int i, cpu, err;
 +	int cpu;
 +
++<<<<<<< HEAD
 +	BUILD_BUG_ON_MSG(offsetof(struct vcpu_vmx, vcpu) != 0,
 +		"struct kvm_vcpu must be at offset 0 for arch usercopy region");
 +
 +	vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL_ACCOUNT);
 +	if (!vmx)
 +		return ERR_PTR(-ENOMEM);
  
 +	vmx->vcpu.arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +			GFP_KERNEL_ACCOUNT);
 +	if (!vmx->vcpu.arch.user_fpu) {
++=======
+ 	BUILD_BUG_ON(offsetof(struct vcpu_vmx, vcpu) != 0);
+ 	vmx = to_vmx(vcpu);
+ 
+ 	vcpu->arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
+ 						GFP_KERNEL_ACCOUNT);
+ 	if (!vcpu->arch.user_fpu) {
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  		printk(KERN_ERR "kvm: failed to allocate kvm userspace's fpu\n");
  		err = -ENOMEM;
- 		goto free_partial_vcpu;
+ 		goto out;
  	}
  
 -	vcpu->arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
 -						 GFP_KERNEL_ACCOUNT);
 -	if (!vcpu->arch.guest_fpu) {
 +	vmx->vcpu.arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
 +			GFP_KERNEL_ACCOUNT);
 +	if (!vmx->vcpu.arch.guest_fpu) {
  		printk(KERN_ERR "kvm: failed to allocate vcpu's fpu\n");
  		err = -ENOMEM;
  		goto free_user_fpu;
@@@ -6871,22 -6822,21 +6889,32 @@@
  
  	vmx->ept_pointer = INVALID_PAGE;
  
++<<<<<<< HEAD
 +	return &vmx->vcpu;
++=======
+ 	return 0;
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  
  free_vmcs:
  	free_loaded_vmcs(vmx->loaded_vmcs);
  free_pml:
  	vmx_destroy_pml_buffer(vmx);
  uninit_vcpu:
 -	kvm_vcpu_uninit(vcpu);
 +	kvm_vcpu_uninit(&vmx->vcpu);
  	free_vpid(vmx->vpid);
  free_vcpu:
 -	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.guest_fpu);
  free_user_fpu:
++<<<<<<< HEAD
 +	kmem_cache_free(x86_fpu_cache, vmx->vcpu.arch.user_fpu);
 +free_partial_vcpu:
 +	kmem_cache_free(kvm_vcpu_cache, vmx);
 +	return ERR_PTR(err);
++=======
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
+ out:
+ 	return err;
++>>>>>>> a9dd6f09d7e5 (KVM: x86: Allocate vcpu struct in common x86 code)
  }
  
  #define L1TF_MSG_SMT "L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n"
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index b0a84267e07c..7f9c57aa5a51 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1053,7 +1053,7 @@ struct kvm_x86_ops {
 	void (*vm_destroy)(struct kvm *kvm);
 
 	/* Create, but do not attach this VCPU */
-	struct kvm_vcpu *(*vcpu_create)(struct kvm *kvm, unsigned id);
+	int (*vcpu_create)(struct kvm *kvm, struct kvm_vcpu *vcpu, unsigned id);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
 	void (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);
 
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 28c9e8821b43..7db4e61a45f1 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -9150,26 +9150,34 @@ static void fx_init(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 {
-	void *wbinvd_dirty_mask = vcpu->arch.wbinvd_dirty_mask;
-
 	kvmclock_reset(vcpu);
 
 	kvm_x86_ops->vcpu_free(vcpu);
-	free_cpumask_var(wbinvd_dirty_mask);
+
+	free_cpumask_var(vcpu->arch.wbinvd_dirty_mask);
+	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 						unsigned int id)
 {
 	struct kvm_vcpu *vcpu;
+	int r;
 
 	if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
 		printk_once(KERN_WARNING
 		"kvm: SMP vm created on host with unstable TSC; "
 		"guest TSC will not be reliable\n");
 
-	vcpu = kvm_x86_ops->vcpu_create(kvm, id);
+	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL_ACCOUNT);
+	if (!vcpu)
+		return ERR_PTR(-ENOMEM);
 
+	r = kvm_x86_ops->vcpu_create(kvm, vcpu, id);
+	if (r) {
+		kmem_cache_free(kvm_vcpu_cache, vcpu);
+		return ERR_PTR(r);
+	}
 	return vcpu;
 }
 
