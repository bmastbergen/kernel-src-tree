sched/uclamp: Extend sched_setattr() to support utilization clamping

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Patrick Bellasi <patrick.bellasi@arm.com>
commit a509a7cd79747074a2c018a45bbbc52d1f4aed44
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/a509a7cd.failed

The SCHED_DEADLINE scheduling class provides an advanced and formal
model to define tasks requirements that can translate into proper
decisions for both task placements and frequencies selections. Other
classes have a more simplified model based on the POSIX concept of
priorities.

Such a simple priority based model however does not allow to exploit
most advanced features of the Linux scheduler like, for example, driving
frequencies selection via the schedutil cpufreq governor. However, also
for non SCHED_DEADLINE tasks, it's still interesting to define tasks
properties to support scheduler decisions.

Utilization clamping exposes to user-space a new set of per-task
attributes the scheduler can use as hints about the expected/required
utilization for a task. This allows to implement a "proactive" per-task
frequency control policy, a more advanced policy than the current one
based just on "passive" measured task utilization. For example, it's
possible to boost interactive tasks (e.g. to get better performance) or
cap background tasks (e.g. to be more energy/thermal efficient).

Introduce a new API to set utilization clamping values for a specified
task by extending sched_setattr(), a syscall which already allows to
define task specific properties for different scheduling classes. A new
pair of attributes allows to specify a minimum and maximum utilization
the scheduler can consider for a task.

Do that by validating the required clamp values before and then applying
the required changes using _the_ same pattern already in use for
__setscheduler(). This ensures that the task is re-enqueued with the new
clamp values.

	Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Alessio Balsini <balsini@android.com>
	Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
	Cc: Joel Fernandes <joelaf@google.com>
	Cc: Juri Lelli <juri.lelli@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Morten Rasmussen <morten.rasmussen@arm.com>
	Cc: Paul Turner <pjt@google.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Quentin Perret <quentin.perret@arm.com>
	Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
	Cc: Steve Muckle <smuckle@google.com>
	Cc: Suren Baghdasaryan <surenb@google.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Todd Kjos <tkjos@google.com>
	Cc: Vincent Guittot <vincent.guittot@linaro.org>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
Link: https://lkml.kernel.org/r/20190621084217.8167-7-patrick.bellasi@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a509a7cd79747074a2c018a45bbbc52d1f4aed44)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched.h
#	include/uapi/linux/sched.h
#	kernel/sched/core.c
diff --cc include/linux/sched.h
index 32f2b17b1618,1113dd4706ae..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -566,6 -578,41 +566,44 @@@ struct sched_dl_entity 
  	struct hrtimer inactive_timer;
  };
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_UCLAMP_TASK
+ /* Number of utilization clamp buckets (shorter alias) */
+ #define UCLAMP_BUCKETS CONFIG_UCLAMP_BUCKETS_COUNT
+ 
+ /*
+  * Utilization clamp for a scheduling entity
+  * @value:		clamp value "assigned" to a se
+  * @bucket_id:		bucket index corresponding to the "assigned" value
+  * @active:		the se is currently refcounted in a rq's bucket
+  * @user_defined:	the requested clamp value comes from user-space
+  *
+  * The bucket_id is the index of the clamp bucket matching the clamp value
+  * which is pre-computed and stored to avoid expensive integer divisions from
+  * the fast path.
+  *
+  * The active bit is set whenever a task has got an "effective" value assigned,
+  * which can be different from the clamp value "requested" from user-space.
+  * This allows to know a task is refcounted in the rq's bucket corresponding
+  * to the "effective" bucket_id.
+  *
+  * The user_defined bit is set whenever a task has got a task-specific clamp
+  * value requested from userspace, i.e. the system defaults apply to this task
+  * just as a restriction. This allows to relax default clamps when a less
+  * restrictive task-specific value has been requested, thus allowing to
+  * implement a "nice" semantic. For example, a task running with a 20%
+  * default boost can still drop its own boosting to 0%.
+  */
+ struct uclamp_se {
+ 	unsigned int value		: bits_per(SCHED_CAPACITY_SCALE);
+ 	unsigned int bucket_id		: bits_per(UCLAMP_BUCKETS);
+ 	unsigned int active		: 1;
+ 	unsigned int user_defined	: 1;
+ };
+ #endif /* CONFIG_UCLAMP_TASK */
+ 
++>>>>>>> a509a7cd7974 (sched/uclamp: Extend sched_setattr() to support utilization clamping)
  union rcu_special {
  	struct {
  		u8			blocked;
diff --cc include/uapi/linux/sched.h
index 22627f80063e,617bb59aa8ba..000000000000
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@@ -50,9 -51,21 +50,28 @@@
  #define SCHED_FLAG_RESET_ON_FORK	0x01
  #define SCHED_FLAG_RECLAIM		0x02
  #define SCHED_FLAG_DL_OVERRUN		0x04
++<<<<<<< HEAD
 +
 +#define SCHED_FLAG_ALL	(SCHED_FLAG_RESET_ON_FORK	| \
 +			 SCHED_FLAG_RECLAIM		| \
 +			 SCHED_FLAG_DL_OVERRUN)
++=======
+ #define SCHED_FLAG_KEEP_POLICY		0x08
+ #define SCHED_FLAG_KEEP_PARAMS		0x10
+ #define SCHED_FLAG_UTIL_CLAMP_MIN	0x20
+ #define SCHED_FLAG_UTIL_CLAMP_MAX	0x40
+ 
+ #define SCHED_FLAG_KEEP_ALL	(SCHED_FLAG_KEEP_POLICY | \
+ 				 SCHED_FLAG_KEEP_PARAMS)
+ 
+ #define SCHED_FLAG_UTIL_CLAMP	(SCHED_FLAG_UTIL_CLAMP_MIN | \
+ 				 SCHED_FLAG_UTIL_CLAMP_MAX)
+ 
+ #define SCHED_FLAG_ALL	(SCHED_FLAG_RESET_ON_FORK	| \
+ 			 SCHED_FLAG_RECLAIM		| \
+ 			 SCHED_FLAG_DL_OVERRUN		| \
+ 			 SCHED_FLAG_KEEP_ALL		| \
+ 			 SCHED_FLAG_UTIL_CLAMP)
++>>>>>>> a509a7cd7974 (sched/uclamp: Extend sched_setattr() to support utilization clamping)
  
  #endif /* _UAPI_LINUX_SCHED_H */
diff --cc kernel/sched/core.c
index 9079f8ce0c0d,e9a669266fa9..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -760,6 -772,354 +760,357 @@@ static void set_load_weight(struct task
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_UCLAMP_TASK
+ /* Max allowed minimum utilization */
+ unsigned int sysctl_sched_uclamp_util_min = SCHED_CAPACITY_SCALE;
+ 
+ /* Max allowed maximum utilization */
+ unsigned int sysctl_sched_uclamp_util_max = SCHED_CAPACITY_SCALE;
+ 
+ /* All clamps are required to be less or equal than these values */
+ static struct uclamp_se uclamp_default[UCLAMP_CNT];
+ 
+ /* Integer rounded range for each bucket */
+ #define UCLAMP_BUCKET_DELTA DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)
+ 
+ #define for_each_clamp_id(clamp_id) \
+ 	for ((clamp_id) = 0; (clamp_id) < UCLAMP_CNT; (clamp_id)++)
+ 
+ static inline unsigned int uclamp_bucket_id(unsigned int clamp_value)
+ {
+ 	return clamp_value / UCLAMP_BUCKET_DELTA;
+ }
+ 
+ static inline unsigned int uclamp_bucket_base_value(unsigned int clamp_value)
+ {
+ 	return UCLAMP_BUCKET_DELTA * uclamp_bucket_id(clamp_value);
+ }
+ 
+ static inline unsigned int uclamp_none(int clamp_id)
+ {
+ 	if (clamp_id == UCLAMP_MIN)
+ 		return 0;
+ 	return SCHED_CAPACITY_SCALE;
+ }
+ 
+ static inline void uclamp_se_set(struct uclamp_se *uc_se,
+ 				 unsigned int value, bool user_defined)
+ {
+ 	uc_se->value = value;
+ 	uc_se->bucket_id = uclamp_bucket_id(value);
+ 	uc_se->user_defined = user_defined;
+ }
+ 
+ static inline unsigned int
+ uclamp_idle_value(struct rq *rq, unsigned int clamp_id,
+ 		  unsigned int clamp_value)
+ {
+ 	/*
+ 	 * Avoid blocked utilization pushing up the frequency when we go
+ 	 * idle (which drops the max-clamp) by retaining the last known
+ 	 * max-clamp.
+ 	 */
+ 	if (clamp_id == UCLAMP_MAX) {
+ 		rq->uclamp_flags |= UCLAMP_FLAG_IDLE;
+ 		return clamp_value;
+ 	}
+ 
+ 	return uclamp_none(UCLAMP_MIN);
+ }
+ 
+ static inline void uclamp_idle_reset(struct rq *rq, unsigned int clamp_id,
+ 				     unsigned int clamp_value)
+ {
+ 	/* Reset max-clamp retention only on idle exit */
+ 	if (!(rq->uclamp_flags & UCLAMP_FLAG_IDLE))
+ 		return;
+ 
+ 	WRITE_ONCE(rq->uclamp[clamp_id].value, clamp_value);
+ }
+ 
+ static inline
+ unsigned int uclamp_rq_max_value(struct rq *rq, unsigned int clamp_id,
+ 				 unsigned int clamp_value)
+ {
+ 	struct uclamp_bucket *bucket = rq->uclamp[clamp_id].bucket;
+ 	int bucket_id = UCLAMP_BUCKETS - 1;
+ 
+ 	/*
+ 	 * Since both min and max clamps are max aggregated, find the
+ 	 * top most bucket with tasks in.
+ 	 */
+ 	for ( ; bucket_id >= 0; bucket_id--) {
+ 		if (!bucket[bucket_id].tasks)
+ 			continue;
+ 		return bucket[bucket_id].value;
+ 	}
+ 
+ 	/* No tasks -- default clamp values */
+ 	return uclamp_idle_value(rq, clamp_id, clamp_value);
+ }
+ 
+ /*
+  * The effective clamp bucket index of a task depends on, by increasing
+  * priority:
+  * - the task specific clamp value, when explicitly requested from userspace
+  * - the system default clamp value, defined by the sysadmin
+  */
+ static inline struct uclamp_se
+ uclamp_eff_get(struct task_struct *p, unsigned int clamp_id)
+ {
+ 	struct uclamp_se uc_req = p->uclamp_req[clamp_id];
+ 	struct uclamp_se uc_max = uclamp_default[clamp_id];
+ 
+ 	/* System default restrictions always apply */
+ 	if (unlikely(uc_req.value > uc_max.value))
+ 		return uc_max;
+ 
+ 	return uc_req;
+ }
+ 
+ /*
+  * When a task is enqueued on a rq, the clamp bucket currently defined by the
+  * task's uclamp::bucket_id is refcounted on that rq. This also immediately
+  * updates the rq's clamp value if required.
+  *
+  * Tasks can have a task-specific value requested from user-space, track
+  * within each bucket the maximum value for tasks refcounted in it.
+  * This "local max aggregation" allows to track the exact "requested" value
+  * for each bucket when all its RUNNABLE tasks require the same clamp.
+  */
+ static inline void uclamp_rq_inc_id(struct rq *rq, struct task_struct *p,
+ 				    unsigned int clamp_id)
+ {
+ 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
+ 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
+ 	struct uclamp_bucket *bucket;
+ 
+ 	lockdep_assert_held(&rq->lock);
+ 
+ 	/* Update task effective clamp */
+ 	p->uclamp[clamp_id] = uclamp_eff_get(p, clamp_id);
+ 
+ 	bucket = &uc_rq->bucket[uc_se->bucket_id];
+ 	bucket->tasks++;
+ 	uc_se->active = true;
+ 
+ 	uclamp_idle_reset(rq, clamp_id, uc_se->value);
+ 
+ 	/*
+ 	 * Local max aggregation: rq buckets always track the max
+ 	 * "requested" clamp value of its RUNNABLE tasks.
+ 	 */
+ 	if (bucket->tasks == 1 || uc_se->value > bucket->value)
+ 		bucket->value = uc_se->value;
+ 
+ 	if (uc_se->value > READ_ONCE(uc_rq->value))
+ 		WRITE_ONCE(uc_rq->value, uc_se->value);
+ }
+ 
+ /*
+  * When a task is dequeued from a rq, the clamp bucket refcounted by the task
+  * is released. If this is the last task reference counting the rq's max
+  * active clamp value, then the rq's clamp value is updated.
+  *
+  * Both refcounted tasks and rq's cached clamp values are expected to be
+  * always valid. If it's detected they are not, as defensive programming,
+  * enforce the expected state and warn.
+  */
+ static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
+ 				    unsigned int clamp_id)
+ {
+ 	struct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];
+ 	struct uclamp_se *uc_se = &p->uclamp[clamp_id];
+ 	struct uclamp_bucket *bucket;
+ 	unsigned int bkt_clamp;
+ 	unsigned int rq_clamp;
+ 
+ 	lockdep_assert_held(&rq->lock);
+ 
+ 	bucket = &uc_rq->bucket[uc_se->bucket_id];
+ 	SCHED_WARN_ON(!bucket->tasks);
+ 	if (likely(bucket->tasks))
+ 		bucket->tasks--;
+ 	uc_se->active = false;
+ 
+ 	/*
+ 	 * Keep "local max aggregation" simple and accept to (possibly)
+ 	 * overboost some RUNNABLE tasks in the same bucket.
+ 	 * The rq clamp bucket value is reset to its base value whenever
+ 	 * there are no more RUNNABLE tasks refcounting it.
+ 	 */
+ 	if (likely(bucket->tasks))
+ 		return;
+ 
+ 	rq_clamp = READ_ONCE(uc_rq->value);
+ 	/*
+ 	 * Defensive programming: this should never happen. If it happens,
+ 	 * e.g. due to future modification, warn and fixup the expected value.
+ 	 */
+ 	SCHED_WARN_ON(bucket->value > rq_clamp);
+ 	if (bucket->value >= rq_clamp) {
+ 		bkt_clamp = uclamp_rq_max_value(rq, clamp_id, uc_se->value);
+ 		WRITE_ONCE(uc_rq->value, bkt_clamp);
+ 	}
+ }
+ 
+ static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)
+ {
+ 	unsigned int clamp_id;
+ 
+ 	if (unlikely(!p->sched_class->uclamp_enabled))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id)
+ 		uclamp_rq_inc_id(rq, p, clamp_id);
+ 
+ 	/* Reset clamp idle holding when there is one RUNNABLE task */
+ 	if (rq->uclamp_flags & UCLAMP_FLAG_IDLE)
+ 		rq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;
+ }
+ 
+ static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)
+ {
+ 	unsigned int clamp_id;
+ 
+ 	if (unlikely(!p->sched_class->uclamp_enabled))
+ 		return;
+ 
+ 	for_each_clamp_id(clamp_id)
+ 		uclamp_rq_dec_id(rq, p, clamp_id);
+ }
+ 
+ int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,
+ 				void __user *buffer, size_t *lenp,
+ 				loff_t *ppos)
+ {
+ 	int old_min, old_max;
+ 	static DEFINE_MUTEX(mutex);
+ 	int result;
+ 
+ 	mutex_lock(&mutex);
+ 	old_min = sysctl_sched_uclamp_util_min;
+ 	old_max = sysctl_sched_uclamp_util_max;
+ 
+ 	result = proc_dointvec(table, write, buffer, lenp, ppos);
+ 	if (result)
+ 		goto undo;
+ 	if (!write)
+ 		goto done;
+ 
+ 	if (sysctl_sched_uclamp_util_min > sysctl_sched_uclamp_util_max ||
+ 	    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE) {
+ 		result = -EINVAL;
+ 		goto undo;
+ 	}
+ 
+ 	if (old_min != sysctl_sched_uclamp_util_min) {
+ 		uclamp_se_set(&uclamp_default[UCLAMP_MIN],
+ 			      sysctl_sched_uclamp_util_min, false);
+ 	}
+ 	if (old_max != sysctl_sched_uclamp_util_max) {
+ 		uclamp_se_set(&uclamp_default[UCLAMP_MAX],
+ 			      sysctl_sched_uclamp_util_max, false);
+ 	}
+ 
+ 	/*
+ 	 * Updating all the RUNNABLE task is expensive, keep it simple and do
+ 	 * just a lazy update at each next enqueue time.
+ 	 */
+ 	goto done;
+ 
+ undo:
+ 	sysctl_sched_uclamp_util_min = old_min;
+ 	sysctl_sched_uclamp_util_max = old_max;
+ done:
+ 	mutex_unlock(&mutex);
+ 
+ 	return result;
+ }
+ 
+ static int uclamp_validate(struct task_struct *p,
+ 			   const struct sched_attr *attr)
+ {
+ 	unsigned int lower_bound = p->uclamp_req[UCLAMP_MIN].value;
+ 	unsigned int upper_bound = p->uclamp_req[UCLAMP_MAX].value;
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN)
+ 		lower_bound = attr->sched_util_min;
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX)
+ 		upper_bound = attr->sched_util_max;
+ 
+ 	if (lower_bound > upper_bound)
+ 		return -EINVAL;
+ 	if (upper_bound > SCHED_CAPACITY_SCALE)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static void __setscheduler_uclamp(struct task_struct *p,
+ 				  const struct sched_attr *attr)
+ {
+ 	if (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))
+ 		return;
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN) {
+ 		uclamp_se_set(&p->uclamp_req[UCLAMP_MIN],
+ 			      attr->sched_util_min, true);
+ 	}
+ 
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX) {
+ 		uclamp_se_set(&p->uclamp_req[UCLAMP_MAX],
+ 			      attr->sched_util_max, true);
+ 	}
+ }
+ 
+ static void uclamp_fork(struct task_struct *p)
+ {
+ 	unsigned int clamp_id;
+ 
+ 	for_each_clamp_id(clamp_id)
+ 		p->uclamp[clamp_id].active = false;
+ }
+ 
+ static void __init init_uclamp(void)
+ {
+ 	struct uclamp_se uc_max = {};
+ 	unsigned int clamp_id;
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		memset(&cpu_rq(cpu)->uclamp, 0, sizeof(struct uclamp_rq));
+ 		cpu_rq(cpu)->uclamp_flags = 0;
+ 	}
+ 
+ 	for_each_clamp_id(clamp_id) {
+ 		uclamp_se_set(&init_task.uclamp_req[clamp_id],
+ 			      uclamp_none(clamp_id), false);
+ 	}
+ 
+ 	/* System defaults allow max clamp values for both indexes */
+ 	uclamp_se_set(&uc_max, uclamp_none(UCLAMP_MAX), false);
+ 	for_each_clamp_id(clamp_id)
+ 		uclamp_default[clamp_id] = uc_max;
+ }
+ 
+ #else /* CONFIG_UCLAMP_TASK */
+ static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }
+ static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }
+ static inline int uclamp_validate(struct task_struct *p,
+ 				  const struct sched_attr *attr)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ static void __setscheduler_uclamp(struct task_struct *p,
+ 				  const struct sched_attr *attr) { }
+ static inline void uclamp_fork(struct task_struct *p) { }
+ static inline void init_uclamp(void) { }
+ #endif /* CONFIG_UCLAMP_TASK */
+ 
++>>>>>>> a509a7cd7974 (sched/uclamp: Extend sched_setattr() to support utilization clamping)
  static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
  {
  	if (!(flags & ENQUEUE_NOCLOCK))
@@@ -4280,10 -4637,12 +4645,12 @@@ recheck
  			goto change;
  		if (dl_policy(policy) && dl_param_changed(p, attr))
  			goto change;
+ 		if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)
+ 			goto change;
  
  		p->sched_reset_on_fork = reset_on_fork;
 -		task_rq_unlock(rq, p, &rf);
 -		return 0;
 +		retval = 0;
 +		goto unlock;
  	}
  change:
  
* Unmerged path include/linux/sched.h
* Unmerged path include/uapi/linux/sched.h
diff --git a/include/uapi/linux/sched/types.h b/include/uapi/linux/sched/types.h
index 10fbb8031930..c852153ddb0d 100644
--- a/include/uapi/linux/sched/types.h
+++ b/include/uapi/linux/sched/types.h
@@ -9,6 +9,7 @@ struct sched_param {
 };
 
 #define SCHED_ATTR_SIZE_VER0	48	/* sizeof first published struct */
+#define SCHED_ATTR_SIZE_VER1	56	/* add: util_{min,max} */
 
 /*
  * Extended scheduling parameters data structure.
@@ -21,8 +22,33 @@ struct sched_param {
  * the tasks may be useful for a wide variety of application fields, e.g.,
  * multimedia, streaming, automation and control, and many others.
  *
- * This variant (sched_attr) is meant at describing a so-called
- * sporadic time-constrained task. In such model a task is specified by:
+ * This variant (sched_attr) allows to define additional attributes to
+ * improve the scheduler knowledge about task requirements.
+ *
+ * Scheduling Class Attributes
+ * ===========================
+ *
+ * A subset of sched_attr attributes specifies the
+ * scheduling policy and relative POSIX attributes:
+ *
+ *  @size		size of the structure, for fwd/bwd compat.
+ *
+ *  @sched_policy	task's scheduling policy
+ *  @sched_nice		task's nice value      (SCHED_NORMAL/BATCH)
+ *  @sched_priority	task's static priority (SCHED_FIFO/RR)
+ *
+ * Certain more advanced scheduling features can be controlled by a
+ * predefined set of flags via the attribute:
+ *
+ *  @sched_flags	for customizing the scheduler behaviour
+ *
+ * Sporadic Time-Constrained Task Attributes
+ * =========================================
+ *
+ * A subset of sched_attr attributes allows to describe a so-called
+ * sporadic time-constrained task.
+ *
+ * In such a model a task is specified by:
  *  - the activation period or minimum instance inter-arrival time;
  *  - the maximum (or average, depending on the actual scheduling
  *    discipline) computation time of all instances, a.k.a. runtime;
@@ -34,14 +60,8 @@ struct sched_param {
  * than the runtime and must be completed by time instant t equal to
  * the instance activation time + the deadline.
  *
- * This is reflected by the actual fields of the sched_attr structure:
+ * This is reflected by the following fields of the sched_attr structure:
  *
- *  @size		size of the structure, for fwd/bwd compat.
- *
- *  @sched_policy	task's scheduling policy
- *  @sched_flags	for customizing the scheduler behaviour
- *  @sched_nice		task's nice value      (SCHED_NORMAL/BATCH)
- *  @sched_priority	task's static priority (SCHED_FIFO/RR)
  *  @sched_deadline	representative of the task's deadline
  *  @sched_runtime	representative of the task's runtime
  *  @sched_period	representative of the task's period
@@ -53,6 +73,29 @@ struct sched_param {
  * As of now, the SCHED_DEADLINE policy (sched_dl scheduling class) is the
  * only user of this new interface. More information about the algorithm
  * available in the scheduling class file or in Documentation/.
+ *
+ * Task Utilization Attributes
+ * ===========================
+ *
+ * A subset of sched_attr attributes allows to specify the utilization
+ * expected for a task. These attributes allow to inform the scheduler about
+ * the utilization boundaries within which it should schedule the task. These
+ * boundaries are valuable hints to support scheduler decisions on both task
+ * placement and frequency selection.
+ *
+ *  @sched_util_min	represents the minimum utilization
+ *  @sched_util_max	represents the maximum utilization
+ *
+ * Utilization is a value in the range [0..SCHED_CAPACITY_SCALE]. It
+ * represents the percentage of CPU time used by a task when running at the
+ * maximum frequency on the highest capacity CPU of the system. For example, a
+ * 20% utilization task is a task running for 2ms every 10ms at maximum
+ * frequency.
+ *
+ * A task with a min utilization value bigger than 0 is more likely scheduled
+ * on a CPU with a capacity big enough to fit the specified value.
+ * A task with a max utilization value smaller than 1024 is more likely
+ * scheduled on a CPU with no more capacity than the specified value.
  */
 struct sched_attr {
 	__u32 size;
@@ -70,6 +113,11 @@ struct sched_attr {
 	__u64 sched_runtime;
 	__u64 sched_deadline;
 	__u64 sched_period;
+
+	/* Utilization hints */
+	__u32 sched_util_min;
+	__u32 sched_util_max;
+
 };
 
 #endif /* _UAPI_LINUX_SCHED_TYPES_H */
* Unmerged path kernel/sched/core.c
