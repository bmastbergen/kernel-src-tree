dma-direct: relax addressability checks in dma_direct_supported

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 91ef26f914171cf753330f13724fd9142b5b1640
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/91ef26f9.failed

dma_direct_supported tries to find the minimum addressable bitmask
based on the end pfn and optional magic that architectures can use
to communicate the size of the magic ZONE_DMA that can be used
for bounce buffering.  But between the DMA offsets that can change
per device (or sometimes even region), the fact the ZONE_DMA isn't
even guaranteed to be the lowest addresses and failure of having
proper interfaces to the MM code this fails at least for one
arm subarchitecture.

As all the legacy DMA implementations have supported 32-bit DMA
masks, and 32-bit masks are guranteed to always work by the API
contract (using bounce buffers if needed), we can short cut the
complicated check and always return true without breaking existing
assumptions.  Hopefully we can properly clean up the interaction
with the arch defined zones and the bootmem allocator eventually.

Fixes: ad3c7b18c5b3 ("arm: use swiotlb for bounce buffering on LPAE configs")
	Reported-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Peter Ujfalusi <peter.ujfalusi@ti.com>
(cherry picked from commit 91ef26f914171cf753330f13724fd9142b5b1640)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index f4581748eeeb,32ec69cdba54..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -365,22 -405,85 +365,103 @@@ out_unmap
  }
  EXPORT_SYMBOL(dma_direct_map_sg);
  
++<<<<<<< HEAD
 +/*
 + * Because 32-bit DMA masks are so common we expect every architecture to be
 + * able to satisfy them - either by not supporting more physical memory, or by
 + * providing a ZONE_DMA32.  If neither is the case, the architecture needs to
 + * use an IOMMU instead of the direct mapping.
 + */
++=======
+ dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	dma_addr_t dma_addr = paddr;
+ 
+ 	if (unlikely(!dma_capable(dev, dma_addr, size, false))) {
+ 		report_addr(dev, dma_addr, size);
+ 		return DMA_MAPPING_ERROR;
+ 	}
+ 
+ 	return dma_addr;
+ }
+ EXPORT_SYMBOL(dma_direct_map_resource);
+ 
+ int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	struct page *page = dma_direct_to_page(dev, dma_addr);
+ 	int ret;
+ 
+ 	ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+ 	if (!ret)
+ 		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_MMU
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return dev_is_dma_coherent(dev) ||
+ 		IS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	unsigned long user_count = vma_pages(vma);
+ 	unsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	unsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));
+ 	int ret = -ENXIO;
+ 
+ 	vma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);
+ 
+ 	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
+ 		return ret;
+ 
+ 	if (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)
+ 		return -ENXIO;
+ 	return remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,
+ 			user_count << PAGE_SHIFT, vma->vm_page_prot);
+ }
+ #else /* CONFIG_MMU */
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return false;
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ #endif /* CONFIG_MMU */
+ 
++>>>>>>> 91ef26f91417 (dma-direct: relax addressability checks in dma_direct_supported)
  int dma_direct_supported(struct device *dev, u64 mask)
  {
- 	u64 min_mask;
+ 	u64 min_mask = (max_pfn - 1) << PAGE_SHIFT;
  
++<<<<<<< HEAD
 +	if (IS_ENABLED(CONFIG_ZONE_DMA))
 +		min_mask = DMA_BIT_MASK(ARCH_ZONE_DMA_BITS);
 +	else
 +		min_mask = DMA_BIT_MASK(32);
 +
 +	min_mask = min_t(u64, min_mask, (max_pfn - 1) << PAGE_SHIFT);
++=======
+ 	/*
+ 	 * Because 32-bit DMA masks are so common we expect every architecture
+ 	 * to be able to satisfy them - either by not supporting more physical
+ 	 * memory, or by providing a ZONE_DMA32.  If neither is the case, the
+ 	 * architecture needs to use an IOMMU instead of the direct mapping.
+ 	 */
+ 	if (mask >= DMA_BIT_MASK(32))
+ 		return 1;
++>>>>>>> 91ef26f91417 (dma-direct: relax addressability checks in dma_direct_supported)
  
  	/*
  	 * This check needs to be against the actual bit mask value, so
* Unmerged path kernel/dma/direct.c
