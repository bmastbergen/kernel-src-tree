io_uring: fix iovec leaks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 99bc4c38537d774e667d043c520914082da19abf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/99bc4c38.failed

Allocated iovec is freed only in io_{read,write,send,recv)(), and just
leaves it if an error occured. There are plenty of such cases:
- cancellation of non-head requests
- fail grabbing files in __io_queue_sqe()
- set REQ_F_NOWAIT and returning in __io_queue_sqe()

Add REQ_F_NEED_CLEANUP, which will force such requests with custom
allocated resourses go through cleanup handlers on put.

	Cc: stable@vger.kernel.org # 5.5
	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 99bc4c38537d774e667d043c520914082da19abf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index ffb8e9d82a6a,5353e96029c7..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -308,6 -328,199 +308,202 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_close {
+ 	struct file			*file;
+ 	struct file			*put_file;
+ 	int				fd;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ 	int				mode;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	unsigned			count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	union {
+ 		struct user_msghdr __user *msg;
+ 		void __user		*buf;
+ 	};
+ 	int				msg_flags;
+ 	size_t				len;
+ };
+ 
+ struct io_open {
+ 	struct file			*file;
+ 	int				dfd;
+ 	union {
+ 		unsigned		mask;
+ 	};
+ 	struct filename			*filename;
+ 	struct statx __user		*buffer;
+ 	struct open_how			how;
+ };
+ 
+ struct io_files_update {
+ 	struct file			*file;
+ 	u64				arg;
+ 	u32				nr_args;
+ 	u32				offset;
+ };
+ 
+ struct io_fadvise {
+ 	struct file			*file;
+ 	u64				offset;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_madvise {
+ 	struct file			*file;
+ 	u64				addr;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_epoll {
+ 	struct file			*file;
+ 	int				epfd;
+ 	int				op;
+ 	int				fd;
+ 	struct epoll_event		event;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
+ };
+ 
+ enum {
+ 	REQ_F_FIXED_FILE_BIT	= IOSQE_FIXED_FILE_BIT,
+ 	REQ_F_IO_DRAIN_BIT	= IOSQE_IO_DRAIN_BIT,
+ 	REQ_F_LINK_BIT		= IOSQE_IO_LINK_BIT,
+ 	REQ_F_HARDLINK_BIT	= IOSQE_IO_HARDLINK_BIT,
+ 	REQ_F_FORCE_ASYNC_BIT	= IOSQE_ASYNC_BIT,
+ 
+ 	REQ_F_LINK_NEXT_BIT,
+ 	REQ_F_FAIL_LINK_BIT,
+ 	REQ_F_INFLIGHT_BIT,
+ 	REQ_F_CUR_POS_BIT,
+ 	REQ_F_NOWAIT_BIT,
+ 	REQ_F_IOPOLL_COMPLETED_BIT,
+ 	REQ_F_LINK_TIMEOUT_BIT,
+ 	REQ_F_TIMEOUT_BIT,
+ 	REQ_F_ISREG_BIT,
+ 	REQ_F_MUST_PUNT_BIT,
+ 	REQ_F_TIMEOUT_NOSEQ_BIT,
+ 	REQ_F_COMP_LOCKED_BIT,
+ 	REQ_F_NEED_CLEANUP_BIT,
+ };
+ 
+ enum {
+ 	/* ctx owns file */
+ 	REQ_F_FIXED_FILE	= BIT(REQ_F_FIXED_FILE_BIT),
+ 	/* drain existing IO first */
+ 	REQ_F_IO_DRAIN		= BIT(REQ_F_IO_DRAIN_BIT),
+ 	/* linked sqes */
+ 	REQ_F_LINK		= BIT(REQ_F_LINK_BIT),
+ 	/* doesn't sever on completion < 0 */
+ 	REQ_F_HARDLINK		= BIT(REQ_F_HARDLINK_BIT),
+ 	/* IOSQE_ASYNC */
+ 	REQ_F_FORCE_ASYNC	= BIT(REQ_F_FORCE_ASYNC_BIT),
+ 
+ 	/* already grabbed next link */
+ 	REQ_F_LINK_NEXT		= BIT(REQ_F_LINK_NEXT_BIT),
+ 	/* fail rest of links */
+ 	REQ_F_FAIL_LINK		= BIT(REQ_F_FAIL_LINK_BIT),
+ 	/* on inflight list */
+ 	REQ_F_INFLIGHT		= BIT(REQ_F_INFLIGHT_BIT),
+ 	/* read/write uses file position */
+ 	REQ_F_CUR_POS		= BIT(REQ_F_CUR_POS_BIT),
+ 	/* must not punt to workers */
+ 	REQ_F_NOWAIT		= BIT(REQ_F_NOWAIT_BIT),
+ 	/* polled IO has completed */
+ 	REQ_F_IOPOLL_COMPLETED	= BIT(REQ_F_IOPOLL_COMPLETED_BIT),
+ 	/* has linked timeout */
+ 	REQ_F_LINK_TIMEOUT	= BIT(REQ_F_LINK_TIMEOUT_BIT),
+ 	/* timeout request */
+ 	REQ_F_TIMEOUT		= BIT(REQ_F_TIMEOUT_BIT),
+ 	/* regular file */
+ 	REQ_F_ISREG		= BIT(REQ_F_ISREG_BIT),
+ 	/* must be punted even for NONBLOCK */
+ 	REQ_F_MUST_PUNT		= BIT(REQ_F_MUST_PUNT_BIT),
+ 	/* no timeout sequence */
+ 	REQ_F_TIMEOUT_NOSEQ	= BIT(REQ_F_TIMEOUT_NOSEQ_BIT),
+ 	/* completion under lock */
+ 	REQ_F_COMP_LOCKED	= BIT(REQ_F_COMP_LOCKED_BIT),
+ 	/* needs cleanup */
+ 	REQ_F_NEED_CLEANUP	= BIT(REQ_F_NEED_CLEANUP_BIT),
+ };
+ 
++>>>>>>> 99bc4c38537d (io_uring: fix iovec leaks)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -366,8 -594,164 +562,169 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_ring_file_ref_flush(struct fixed_file_data *data);
+ static void io_cleanup_req(struct io_kiocb *req);
++>>>>>>> 99bc4c38537d (io_uring: fix iovec leaks)
  
  static struct kmem_cache *req_cachep;
  
@@@ -623,15 -1237,105 +980,34 @@@ static void io_free_req_many(struct io_
  
  static void __io_free_req(struct io_kiocb *req)
  {
++<<<<<<< HEAD
 +	if (req->file && !(req->flags & REQ_F_FIXED_FILE))
 +		fput(req->file);
++=======
+ 	__io_req_aux_free(req);
+ 
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		io_cleanup_req(req);
+ 
+ 	if (req->flags & REQ_F_INFLIGHT) {
+ 		struct io_ring_ctx *ctx = req->ctx;
+ 		unsigned long flags;
+ 
+ 		spin_lock_irqsave(&ctx->inflight_lock, flags);
+ 		list_del(&req->inflight_entry);
+ 		if (waitqueue_active(&ctx->inflight_wait))
+ 			wake_up(&ctx->inflight_wait);
+ 		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
+ 	}
+ 
++>>>>>>> 99bc4c38537d (io_uring: fix iovec leaks)
  	percpu_ref_put(&req->ctx->refs);
 -	__io_req_do_free(req);
 +	kmem_cache_free(req_cachep, req);
  }
  
 -struct req_batch {
 -	void *reqs[IO_IOPOLL_BATCH];
 -	int to_free;
 -	int need_iter;
 -};
 -
 -static void io_free_req_many(struct io_ring_ctx *ctx, struct req_batch *rb)
 +static void io_req_link_next(struct io_kiocb *req)
  {
 -	int fixed_refs = rb->to_free;
 -
 -	if (!rb->to_free)
 -		return;
 -	if (rb->need_iter) {
 -		int i, inflight = 0;
 -		unsigned long flags;
 -
 -		fixed_refs = 0;
 -		for (i = 0; i < rb->to_free; i++) {
 -			struct io_kiocb *req = rb->reqs[i];
 -
 -			if (req->flags & REQ_F_FIXED_FILE) {
 -				req->file = NULL;
 -				fixed_refs++;
 -			}
 -			if (req->flags & REQ_F_INFLIGHT)
 -				inflight++;
 -			__io_req_aux_free(req);
 -		}
 -		if (!inflight)
 -			goto do_free;
 -
 -		spin_lock_irqsave(&ctx->inflight_lock, flags);
 -		for (i = 0; i < rb->to_free; i++) {
 -			struct io_kiocb *req = rb->reqs[i];
 -
 -			if (req->flags & REQ_F_INFLIGHT) {
 -				list_del(&req->inflight_entry);
 -				if (!--inflight)
 -					break;
 -			}
 -		}
 -		spin_unlock_irqrestore(&ctx->inflight_lock, flags);
 -
 -		if (waitqueue_active(&ctx->inflight_wait))
 -			wake_up(&ctx->inflight_wait);
 -	}
 -do_free:
 -	kmem_cache_free_bulk(req_cachep, rb->to_free, rb->reqs);
 -	if (fixed_refs)
 -		percpu_ref_put_many(&ctx->file_data->refs, fixed_refs);
 -	percpu_ref_put_many(&ctx->refs, rb->to_free);
 -	rb->to_free = rb->need_iter = 0;
 -}
 -
 -static bool io_link_cancel_timeout(struct io_kiocb *req)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 -
 -	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
 -	if (ret != -1) {
 -		io_cqring_fill_event(req, -ECANCELED);
 -		io_commit_cqring(ctx);
 -		req->flags &= ~REQ_F_LINK;
 -		io_put_req(req);
 -		return true;
 -	}
 -
 -	return false;
 -}
 -
 -static void io_req_link_next(struct io_kiocb *req, struct io_kiocb **nxtptr)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -	bool wake_ev = false;
 -
 -	/* Already got next link */
 -	if (req->flags & REQ_F_LINK_NEXT)
 -		return;
 +	struct io_kiocb *nxt;
  
  	/*
  	 * The list should never be empty when we are called here. But could
@@@ -1327,7 -2124,75 +1703,79 @@@ static ssize_t loop_rw_iter(int rw, str
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
++=======
+ static void io_req_map_rw(struct io_kiocb *req, ssize_t io_size,
+ 			  struct iovec *iovec, struct iovec *fast_iov,
+ 			  struct iov_iter *iter)
+ {
+ 	req->io->rw.nr_segs = iter->nr_segs;
+ 	req->io->rw.size = io_size;
+ 	req->io->rw.iov = iovec;
+ 	if (!req->io->rw.iov) {
+ 		req->io->rw.iov = req->io->rw.fast_iov;
+ 		memcpy(req->io->rw.iov, fast_iov,
+ 			sizeof(struct iovec) * iter->nr_segs);
+ 	} else {
+ 		req->flags |= REQ_F_NEED_CLEANUP;
+ 	}
+ }
+ 
+ static int io_alloc_async_ctx(struct io_kiocb *req)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 	req->io = kmalloc(sizeof(*req->io), GFP_KERNEL);
+ 	return req->io == NULL;
+ }
+ 
+ static int io_setup_async_rw(struct io_kiocb *req, ssize_t io_size,
+ 			     struct iovec *iovec, struct iovec *fast_iov,
+ 			     struct iov_iter *iter)
+ {
+ 	if (!io_op_defs[req->opcode].async_ctx)
+ 		return 0;
+ 	if (!req->io) {
+ 		if (io_alloc_async_ctx(req))
+ 			return -ENOMEM;
+ 
+ 		io_req_map_rw(req, io_size, iovec, fast_iov, iter);
+ 	}
+ 	return 0;
+ }
+ 
+ static int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			bool force_nonblock)
+ {
+ 	struct io_async_ctx *io;
+ 	struct iov_iter iter;
+ 	ssize_t ret;
+ 
+ 	ret = io_prep_rw(req, sqe, force_nonblock);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (unlikely(!(req->file->f_mode & FMODE_READ)))
+ 		return -EBADF;
+ 
+ 	if (!req->io)
+ 		return 0;
+ 
+ 	io = req->io;
+ 	io->rw.iov = io->rw.fast_iov;
+ 	req->io = NULL;
+ 	ret = io_import_iovec(READ, req, &io->rw.iov, &iter);
+ 	req->io = io;
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
+ 	return 0;
+ }
+ 
+ static int io_read(struct io_kiocb *req, struct io_kiocb **nxt,
++>>>>>>> 99bc4c38537d (io_uring: fix iovec leaks)
  		   bool force_nonblock)
  {
  	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
@@@ -1358,35 -2228,26 +1806,36 @@@
  	if (!ret) {
  		ssize_t ret2;
  
 -		if (req->file->f_op->read_iter)
 -			ret2 = call_read_iter(req->file, kiocb, &iter);
 +		if (file->f_op->read_iter)
 +			ret2 = call_read_iter(file, kiocb, &iter);
  		else
 -			ret2 = loop_rw_iter(READ, req->file, kiocb, &iter);
 +			ret2 = loop_rw_iter(READ, file, kiocb, &iter);
  
 +		/*
 +		 * In case of a short read, punt to async. This can happen
 +		 * if we have data partially cached. Alternatively we can
 +		 * return the short read, in which case the application will
 +		 * need to issue another SQE and wait for it. That SQE will
 +		 * need async punt anyway, so it's more efficient to do it
 +		 * here.
 +		 */
 +		if (force_nonblock && ret2 > 0 && ret2 < read_size)
 +			ret2 = -EAGAIN;
  		/* Catch -EAGAIN return for forced non-blocking submission */
  		if (!force_nonblock || ret2 != -EAGAIN) {
 -			kiocb_done(kiocb, ret2, nxt, req->in_async);
 +			io_rw_done(kiocb, ret2);
  		} else {
 -copy_iov:
 -			ret = io_setup_async_rw(req, io_size, iovec,
 -						inline_vecs, &iter);
 -			if (ret)
 -				goto out_free;
 -			return -EAGAIN;
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(READ, req, iov_count);
 +			ret = -EAGAIN;
  		}
  	}
 -out_free:
  	kfree(iovec);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
  	return ret;
  }
  
@@@ -1574,35 -2515,580 +2024,267 @@@ static int io_sync_file_range(struct io
  	return 0;
  }
  
 -static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	const char __user *fname;
 +#if defined(CONFIG_NET)
++<<<<<<< HEAD
 +static int io_send_recvmsg(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +			   bool force_nonblock,
 +		   long (*fn)(struct socket *, struct user_msghdr __user *,
 +				unsigned int))
++=======
++	struct io_sr_msg *sr = &req->sr_msg;
++	struct io_async_ctx *io = req->io;
+ 	int ret;
+ 
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	req->open.how.mode = READ_ONCE(sqe->len);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	req->open.how.flags = READ_ONCE(sqe->open_flags);
++	sr->msg_flags = READ_ONCE(sqe->msg_flags);
++	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
++	sr->len = READ_ONCE(sqe->len);
+ 
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 -	}
++	if (!io || req->opcode == IORING_OP_SEND)
++		return 0;
+ 
 -	return 0;
++	io->msg.iov = io->msg.fast_iov;
++	ret = sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
++					&io->msg.iov);
++	if (!ret)
++		req->flags |= REQ_F_NEED_CLEANUP;
++	return ret;
++#else
++	return -EOPNOTSUPP;
++#endif
+ }
+ 
 -static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++static int io_sendmsg(struct io_kiocb *req, struct io_kiocb **nxt,
++		      bool force_nonblock)
++>>>>>>> 99bc4c38537d (io_uring: fix iovec leaks)
  {
 -	struct open_how __user *how;
 -	const char __user *fname;
 -	size_t len;
 +	struct socket *sock;
  	int ret;
  
 -	if (sqe->ioprio || sqe->buf_index)
 +	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
  		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	len = READ_ONCE(sqe->len);
  
 -	if (len < OPEN_HOW_SIZE_VER0)
 -		return -EINVAL;
 +	sock = sock_from_file(req->file, &ret);
 +	if (sock) {
 +		struct user_msghdr __user *msg;
 +		unsigned flags;
  
 -	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
 -					len);
 -	if (ret)
 -		return ret;
 +		flags = READ_ONCE(sqe->msg_flags);
 +		if (flags & MSG_DONTWAIT)
 +			req->flags |= REQ_F_NOWAIT;
 +		else if (force_nonblock)
 +			flags |= MSG_DONTWAIT;
  
 -	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
 -		req->open.how.flags |= O_LARGEFILE;
++<<<<<<< HEAD
 +		msg = (struct user_msghdr __user *) (unsigned long)
 +			READ_ONCE(sqe->addr);
  
 -	req->open.filename = getname(fname);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 +		ret = fn(sock, msg, flags);
 +		if (force_nonblock && ret == -EAGAIN)
++=======
++		ret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);
++		if (force_nonblock && ret == -EAGAIN) {
++			if (req->io)
++				return -EAGAIN;
++			if (io_alloc_async_ctx(req)) {
++				if (kmsg && kmsg->iov != kmsg->fast_iov)
++					kfree(kmsg->iov);
++				return -ENOMEM;
++			}
++			req->flags |= REQ_F_NEED_CLEANUP;
++			memcpy(&req->io->msg, &io.msg, sizeof(io.msg));
++			return -EAGAIN;
++		}
++		if (ret == -ERESTARTSYS)
++			ret = -EINTR;
+ 	}
+ 
 -	return 0;
 -}
 -
 -static int io_openat2(struct io_kiocb *req, struct io_kiocb **nxt,
 -		      bool force_nonblock)
 -{
 -	struct open_flags op;
 -	struct file *file;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 -
 -	ret = build_open_flags(&req->open.how, &op);
 -	if (ret)
 -		goto err;
 -
 -	ret = get_unused_fd_flags(req->open.how.flags);
 -	if (ret < 0)
 -		goto err;
 -
 -	file = do_filp_open(req->open.dfd, req->open.filename, &op);
 -	if (IS_ERR(file)) {
 -		put_unused_fd(ret);
 -		ret = PTR_ERR(file);
 -	} else {
 -		fsnotify_open(file);
 -		fd_install(ret, file);
 -	}
 -err:
 -	putname(req->open.filename);
++	if (kmsg && kmsg->iov != kmsg->fast_iov)
++		kfree(kmsg->iov);
++	req->flags &= ~REQ_F_NEED_CLEANUP;
++	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
++#else
++	return -EOPNOTSUPP;
++#endif
+ }
+ 
 -static int io_openat(struct io_kiocb *req, struct io_kiocb **nxt,
 -		     bool force_nonblock)
++static int io_send(struct io_kiocb *req, struct io_kiocb **nxt,
++		   bool force_nonblock)
+ {
 -	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
 -	return io_openat2(req, nxt, force_nonblock);
 -}
++#if defined(CONFIG_NET)
++	struct socket *sock;
++	int ret;
+ 
 -static int io_epoll_ctl_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_EPOLL)
 -	if (sqe->ioprio || sqe->buf_index)
++	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
 -	req->epoll.epfd = READ_ONCE(sqe->fd);
 -	req->epoll.op = READ_ONCE(sqe->len);
 -	req->epoll.fd = READ_ONCE(sqe->off);
 -
 -	if (ep_op_has_event(req->epoll.op)) {
 -		struct epoll_event __user *ev;
++	sock = sock_from_file(req->file, &ret);
++	if (sock) {
++		struct io_sr_msg *sr = &req->sr_msg;
++		struct msghdr msg;
++		struct iovec iov;
++		unsigned flags;
+ 
 -		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
 -			return -EFAULT;
 -	}
++		ret = import_single_range(WRITE, sr->buf, sr->len, &iov,
++						&msg.msg_iter);
++		if (ret)
++			return ret;
+ 
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
++		msg.msg_name = NULL;
++		msg.msg_control = NULL;
++		msg.msg_controllen = 0;
++		msg.msg_namelen = 0;
+ 
 -static int io_epoll_ctl(struct io_kiocb *req, struct io_kiocb **nxt,
 -			bool force_nonblock)
 -{
 -#if defined(CONFIG_EPOLL)
 -	struct io_epoll *ie = &req->epoll;
 -	int ret;
++		flags = req->sr_msg.msg_flags;
++		if (flags & MSG_DONTWAIT)
++			req->flags |= REQ_F_NOWAIT;
++		else if (force_nonblock)
++			flags |= MSG_DONTWAIT;
+ 
 -	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
 -	if (force_nonblock && ret == -EAGAIN)
 -		return -EAGAIN;
++		msg.msg_flags = flags;
++		ret = sock_sendmsg(sock, &msg);
++		if (force_nonblock && ret == -EAGAIN)
++			return -EAGAIN;
++		if (ret == -ERESTARTSYS)
++			ret = -EINTR;
++	}
+ 
++	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
 -static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
 -	if (sqe->ioprio || sqe->buf_index || sqe->off)
 -		return -EINVAL;
 -
 -	req->madvise.addr = READ_ONCE(sqe->addr);
 -	req->madvise.len = READ_ONCE(sqe->len);
 -	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_madvise(struct io_kiocb *req, struct io_kiocb **nxt,
 -		      bool force_nonblock)
++static int io_recvmsg_prep(struct io_kiocb *req,
++			   const struct io_uring_sqe *sqe)
+ {
 -#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
 -	struct io_madvise *ma = &req->madvise;
++#if defined(CONFIG_NET)
++	struct io_sr_msg *sr = &req->sr_msg;
++	struct io_async_ctx *io = req->io;
+ 	int ret;
+ 
 -	if (force_nonblock)
 -		return -EAGAIN;
++	sr->msg_flags = READ_ONCE(sqe->msg_flags);
++	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
++	sr->len = READ_ONCE(sqe->len);
+ 
 -	ret = do_madvise(ma->addr, ma->len, ma->advice);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 -	return 0;
++	if (!io || req->opcode == IORING_OP_RECV)
++		return 0;
++
++	io->msg.iov = io->msg.fast_iov;
++	ret = recvmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
++					&io->msg.uaddr, &io->msg.iov);
++	if (!ret)
++		req->flags |= REQ_F_NEED_CLEANUP;
++	return ret;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
 -static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->ioprio || sqe->buf_index || sqe->addr)
 -		return -EINVAL;
 -
 -	req->fadvise.offset = READ_ONCE(sqe->off);
 -	req->fadvise.len = READ_ONCE(sqe->len);
 -	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
 -	return 0;
 -}
 -
 -static int io_fadvise(struct io_kiocb *req, struct io_kiocb **nxt,
++static int io_recvmsg(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
 -	struct io_fadvise *fa = &req->fadvise;
++#if defined(CONFIG_NET)
++	struct io_async_msghdr *kmsg = NULL;
++	struct socket *sock;
+ 	int ret;
+ 
 -	if (force_nonblock) {
 -		switch (fa->advice) {
 -		case POSIX_FADV_NORMAL:
 -		case POSIX_FADV_RANDOM:
 -		case POSIX_FADV_SEQUENTIAL:
 -			break;
 -		default:
 -			return -EAGAIN;
 -		}
 -	}
 -
 -	ret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 -	return 0;
 -}
 -
 -static int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	const char __user *fname;
 -	unsigned lookup_flags;
 -	int ret;
 -
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -
 -	req->open.dfd = READ_ONCE(sqe->fd);
 -	req->open.mask = READ_ONCE(sqe->len);
 -	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	req->open.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));
 -	req->open.how.flags = READ_ONCE(sqe->statx_flags);
 -
 -	if (vfs_stat_set_lookup_flags(&lookup_flags, req->open.how.flags))
 -		return -EINVAL;
 -
 -	req->open.filename = getname_flags(fname, lookup_flags, NULL);
 -	if (IS_ERR(req->open.filename)) {
 -		ret = PTR_ERR(req->open.filename);
 -		req->open.filename = NULL;
 -		return ret;
 -	}
 -
 -	return 0;
 -}
 -
 -static int io_statx(struct io_kiocb *req, struct io_kiocb **nxt,
 -		    bool force_nonblock)
 -{
 -	struct io_open *ctx = &req->open;
 -	unsigned lookup_flags;
 -	struct path path;
 -	struct kstat stat;
 -	int ret;
 -
 -	if (force_nonblock)
 -		return -EAGAIN;
 -
 -	if (vfs_stat_set_lookup_flags(&lookup_flags, ctx->how.flags))
 -		return -EINVAL;
 -
 -retry:
 -	/* filename_lookup() drops it, keep a reference */
 -	ctx->filename->refcnt++;
 -
 -	ret = filename_lookup(ctx->dfd, ctx->filename, lookup_flags, &path,
 -				NULL);
 -	if (ret)
 -		goto err;
 -
 -	ret = vfs_getattr(&path, &stat, ctx->mask, ctx->how.flags);
 -	path_put(&path);
 -	if (retry_estale(ret, lookup_flags)) {
 -		lookup_flags |= LOOKUP_REVAL;
 -		goto retry;
 -	}
 -	if (!ret)
 -		ret = cp_statx(&stat, ctx->buffer);
 -err:
 -	putname(ctx->filename);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
 -	return 0;
 -}
 -
 -static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	/*
 -	 * If we queue this for async, it must not be cancellable. That would
 -	 * leave the 'file' in an undeterminate state.
 -	 */
 -	req->work.flags |= IO_WQ_WORK_NO_CANCEL;
 -
 -	if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||
 -	    sqe->rw_flags || sqe->buf_index)
 -		return -EINVAL;
 -	if (sqe->flags & IOSQE_FIXED_FILE)
 -		return -EBADF;
 -
 -	req->close.fd = READ_ONCE(sqe->fd);
 -	if (req->file->f_op == &io_uring_fops ||
 -	    req->close.fd == req->ctx->ring_fd)
 -		return -EBADF;
 -
 -	return 0;
 -}
 -
 -static void io_close_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -
 -	/* Invoked with files, we need to do the close */
 -	if (req->work.files) {
 -		int ret;
 -
 -		ret = filp_close(req->close.put_file, req->work.files);
 -		if (ret < 0)
 -			req_set_fail_links(req);
 -		io_cqring_add_event(req, ret);
 -	}
 -
 -	fput(req->close.put_file);
 -
 -	io_put_req_find_next(req, &nxt);
 -	if (nxt)
 -		io_wq_assign_next(workptr, nxt);
 -}
 -
 -static int io_close(struct io_kiocb *req, struct io_kiocb **nxt,
 -		    bool force_nonblock)
 -{
 -	int ret;
 -
 -	req->close.put_file = NULL;
 -	ret = __close_fd_get_file(req->close.fd, &req->close.put_file);
 -	if (ret < 0)
 -		return ret;
 -
 -	/* if the file has a flush method, be safe and punt to async */
 -	if (req->close.put_file->f_op->flush && !io_wq_current_is_worker())
 -		goto eagain;
 -
 -	/*
 -	 * No ->flush(), safely close from here and just punt the
 -	 * fput() to async context.
 -	 */
 -	ret = filp_close(req->close.put_file, current->files);
 -
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -
 -	if (io_wq_current_is_worker()) {
 -		struct io_wq_work *old_work, *work;
 -
 -		old_work = work = &req->work;
 -		io_close_finish(&work);
 -		if (work && work != old_work)
 -			*nxt = container_of(work, struct io_kiocb, work);
 -		return 0;
 -	}
 -
 -eagain:
 -	req->work.func = io_close_finish;
 -	/*
 -	 * Do manual async queue here to avoid grabbing files - we don't
 -	 * need the files, and it'll cause io_close_finish() to close
 -	 * the file again and cause a double CQE entry for this request
 -	 */
 -	io_queue_async_work(req);
 -	return 0;
 -}
 -
 -static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	struct io_ring_ctx *ctx = req->ctx;
 -
 -	if (!req->file)
 -		return -EBADF;
 -
 -	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
 -	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
 -		return -EINVAL;
 -
 -	req->sync.off = READ_ONCE(sqe->off);
 -	req->sync.len = READ_ONCE(sqe->len);
 -	req->sync.flags = READ_ONCE(sqe->sync_range_flags);
 -	return 0;
 -}
 -
 -static void io_sync_file_range_finish(struct io_wq_work **workptr)
 -{
 -	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
 -	struct io_kiocb *nxt = NULL;
 -	int ret;
 -
 -	if (io_req_cancelled(req))
 -		return;
 -
 -	ret = sync_file_range(req->file, req->sync.off, req->sync.len,
 -				req->sync.flags);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, &nxt);
 -	if (nxt)
 -		io_wq_assign_next(workptr, nxt);
 -}
 -
 -static int io_sync_file_range(struct io_kiocb *req, struct io_kiocb **nxt,
 -			      bool force_nonblock)
 -{
 -	struct io_wq_work *work, *old_work;
 -
 -	/* sync_file_range always requires a blocking context */
 -	if (force_nonblock) {
 -		io_put_req(req);
 -		req->work.func = io_sync_file_range_finish;
 -		return -EAGAIN;
 -	}
 -
 -	work = old_work = &req->work;
 -	io_sync_file_range_finish(&work);
 -	if (work && work != old_work)
 -		*nxt = container_of(work, struct io_kiocb, work);
 -	return 0;
 -}
 -
 -static int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_sr_msg *sr = &req->sr_msg;
 -	struct io_async_ctx *io = req->io;
 -	int ret;
 -
 -	sr->msg_flags = READ_ONCE(sqe->msg_flags);
 -	sr->msg = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -	sr->len = READ_ONCE(sqe->len);
 -
 -	if (!io || req->opcode == IORING_OP_SEND)
 -		return 0;
 -
 -	io->msg.iov = io->msg.fast_iov;
 -	ret = sendmsg_copy_msghdr(&io->msg.msg, sr->msg, sr->msg_flags,
 -					&io->msg.iov);
 -	if (!ret)
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -	return ret;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 -}
 -
 -static int io_sendmsg(struct io_kiocb *req, struct io_kiocb **nxt,
 -		      bool force_nonblock)
 -{
 -#if defined(CONFIG_NET)
 -	struct io_async_msghdr *kmsg = NULL;
 -	struct socket *sock;
 -	int ret;
 -
 -	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
 -		return -EINVAL;
++	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
++		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_async_ctx io;
+ 		struct sockaddr_storage addr;
+ 		unsigned flags;
+ 
+ 		if (req->io) {
+ 			kmsg = &req->io->msg;
+ 			kmsg->msg.msg_name = &addr;
+ 			/* if iov is set, it's allocated already */
+ 			if (!kmsg->iov)
+ 				kmsg->iov = kmsg->fast_iov;
+ 			kmsg->msg.msg_iter.iov = kmsg->iov;
+ 		} else {
+ 			struct io_sr_msg *sr = &req->sr_msg;
+ 
+ 			kmsg = &io.msg;
+ 			kmsg->msg.msg_name = &addr;
+ 
+ 			io.msg.iov = io.msg.fast_iov;
 -			ret = sendmsg_copy_msghdr(&io.msg.msg, sr->msg,
 -					sr->msg_flags, &io.msg.iov);
++			ret = recvmsg_copy_msghdr(&io.msg.msg, sr->msg,
++					sr->msg_flags, &io.msg.uaddr,
++					&io.msg.iov);
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		flags = req->sr_msg.msg_flags;
+ 		if (flags & MSG_DONTWAIT)
+ 			req->flags |= REQ_F_NOWAIT;
+ 		else if (force_nonblock)
+ 			flags |= MSG_DONTWAIT;
+ 
 -		ret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);
++		ret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.msg,
++						kmsg->uaddr, flags);
+ 		if (force_nonblock && ret == -EAGAIN) {
+ 			if (req->io)
+ 				return -EAGAIN;
+ 			if (io_alloc_async_ctx(req)) {
+ 				if (kmsg && kmsg->iov != kmsg->fast_iov)
+ 					kfree(kmsg->iov);
+ 				return -ENOMEM;
+ 			}
 -			req->flags |= REQ_F_NEED_CLEANUP;
+ 			memcpy(&req->io->msg, &io.msg, sizeof(io.msg));
++			req->flags |= REQ_F_NEED_CLEANUP;
+ 			return -EAGAIN;
+ 		}
+ 		if (ret == -ERESTARTSYS)
+ 			ret = -EINTR;
+ 	}
+ 
+ 	if (kmsg && kmsg->iov != kmsg->fast_iov)
+ 		kfree(kmsg->iov);
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ 	io_cqring_add_event(req, ret);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_put_req_find_next(req, nxt);
+ 	return 0;
+ #else
+ 	return -EOPNOTSUPP;
+ #endif
+ }
+ 
 -static int io_send(struct io_kiocb *req, struct io_kiocb **nxt,
++static int io_recv(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		   bool force_nonblock)
+ {
+ #if defined(CONFIG_NET)
+ 	struct socket *sock;
+ 	int ret;
+ 
+ 	if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 
+ 	sock = sock_from_file(req->file, &ret);
+ 	if (sock) {
+ 		struct io_sr_msg *sr = &req->sr_msg;
+ 		struct msghdr msg;
+ 		struct iovec iov;
+ 		unsigned flags;
+ 
 -		ret = import_single_range(WRITE, sr->buf, sr->len, &iov,
++		ret = import_single_range(READ, sr->buf, sr->len, &iov,
+ 						&msg.msg_iter);
+ 		if (ret)
++>>>>>>> 99bc4c38537d (io_uring: fix iovec leaks)
  			return ret;
 -
 -		msg.msg_name = NULL;
 -		msg.msg_control = NULL;
 -		msg.msg_controllen = 0;
 -		msg.msg_namelen = 0;
 -
 -		flags = req->sr_msg.msg_flags;
 -		if (flags & MSG_DONTWAIT)
 -			req->flags |= REQ_F_NOWAIT;
 -		else if (force_nonblock)
 -			flags |= MSG_DONTWAIT;
 -
 -		msg.msg_flags = flags;
 -		ret = sock_sendmsg(sock, &msg);
 -		if (force_nonblock && ret == -EAGAIN)
 -			return -EAGAIN;
  		if (ret == -ERESTARTSYS)
  			ret = -EINTR;
  	}
@@@ -1878,51 -4204,227 +2560,80 @@@ static int io_req_defer(struct io_ring_
  	return -EIOCBQUEUED;
  }
  
++<<<<<<< HEAD
 +static int __io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			   const struct sqe_submit *s, bool force_nonblock)
++=======
+ static void io_cleanup_req(struct io_kiocb *req)
+ {
+ 	struct io_async_ctx *io = req->io;
+ 
+ 	switch (req->opcode) {
+ 	case IORING_OP_READV:
+ 	case IORING_OP_READ_FIXED:
+ 	case IORING_OP_READ:
+ 	case IORING_OP_WRITEV:
+ 	case IORING_OP_WRITE_FIXED:
+ 	case IORING_OP_WRITE:
+ 		if (io->rw.iov != io->rw.fast_iov)
+ 			kfree(io->rw.iov);
+ 		break;
+ 	case IORING_OP_SENDMSG:
+ 	case IORING_OP_RECVMSG:
+ 		if (io->msg.iov != io->msg.fast_iov)
+ 			kfree(io->msg.iov);
+ 		break;
+ 	}
+ 
+ 	req->flags &= ~REQ_F_NEED_CLEANUP;
+ }
+ 
+ static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			struct io_kiocb **nxt, bool force_nonblock)
++>>>>>>> 99bc4c38537d (io_uring: fix iovec leaks)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	int ret;
 +	int ret, opcode;
  
 -	switch (req->opcode) {
 +	req->user_data = READ_ONCE(s->sqe->user_data);
 +
 +	opcode = READ_ONCE(s->sqe->opcode);
 +	switch (opcode) {
  	case IORING_OP_NOP:
 -		ret = io_nop(req);
 +		ret = io_nop(req, req->user_data);
  		break;
  	case IORING_OP_READV:
 -	case IORING_OP_READ_FIXED:
 -	case IORING_OP_READ:
 -		if (sqe) {
 -			ret = io_read_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_read(req, nxt, force_nonblock);
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_read(req, s, force_nonblock);
  		break;
  	case IORING_OP_WRITEV:
 -	case IORING_OP_WRITE_FIXED:
 -	case IORING_OP_WRITE:
 -		if (sqe) {
 -			ret = io_write_prep(req, sqe, force_nonblock);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_write(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_FSYNC:
 -		if (sqe) {
 -			ret = io_prep_fsync(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_fsync(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_POLL_ADD:
 -		if (sqe) {
 -			ret = io_poll_add_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_poll_add(req, nxt);
 -		break;
 -	case IORING_OP_POLL_REMOVE:
 -		if (sqe) {
 -			ret = io_poll_remove_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_poll_remove(req);
 -		break;
 -	case IORING_OP_SYNC_FILE_RANGE:
 -		if (sqe) {
 -			ret = io_prep_sfr(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		ret = io_sync_file_range(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_SENDMSG:
 -	case IORING_OP_SEND:
 -		if (sqe) {
 -			ret = io_sendmsg_prep(req, sqe);
 -			if (ret < 0)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_SENDMSG)
 -			ret = io_sendmsg(req, nxt, force_nonblock);
 -		else
 -			ret = io_send(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_RECVMSG:
 -	case IORING_OP_RECV:
 -		if (sqe) {
 -			ret = io_recvmsg_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		if (req->opcode == IORING_OP_RECVMSG)
 -			ret = io_recvmsg(req, nxt, force_nonblock);
 -		else
 -			ret = io_recv(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_TIMEOUT:
 -		if (sqe) {
 -			ret = io_timeout_prep(req, sqe, false);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout(req);
 -		break;
 -	case IORING_OP_TIMEOUT_REMOVE:
 -		if (sqe) {
 -			ret = io_timeout_remove_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_timeout_remove(req);
 -		break;
 -	case IORING_OP_ACCEPT:
 -		if (sqe) {
 -			ret = io_accept_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_accept(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_CONNECT:
 -		if (sqe) {
 -			ret = io_connect_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_connect(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_ASYNC_CANCEL:
 -		if (sqe) {
 -			ret = io_async_cancel_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_async_cancel(req, nxt);
 -		break;
 -	case IORING_OP_FALLOCATE:
 -		if (sqe) {
 -			ret = io_fallocate_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fallocate(req, nxt, force_nonblock);
 -		break;
 -	case IORING_OP_OPENAT:
 -		if (sqe) {
 -			ret = io_openat_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat(req, nxt, force_nonblock);
 +		if (unlikely(s->sqe->buf_index))
 +			return -EINVAL;
 +		ret = io_write(req, s, force_nonblock);
  		break;
 -	case IORING_OP_CLOSE:
 -		if (sqe) {
 -			ret = io_close_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_close(req, nxt, force_nonblock);
 +	case IORING_OP_READ_FIXED:
 +		ret = io_read(req, s, force_nonblock);
  		break;
 -	case IORING_OP_FILES_UPDATE:
 -		if (sqe) {
 -			ret = io_files_update_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_files_update(req, force_nonblock);
 +	case IORING_OP_WRITE_FIXED:
 +		ret = io_write(req, s, force_nonblock);
  		break;
 -	case IORING_OP_STATX:
 -		if (sqe) {
 -			ret = io_statx_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_statx(req, nxt, force_nonblock);
 +	case IORING_OP_FSYNC:
 +		ret = io_fsync(req, s->sqe, force_nonblock);
  		break;
 -	case IORING_OP_FADVISE:
 -		if (sqe) {
 -			ret = io_fadvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_fadvise(req, nxt, force_nonblock);
 +	case IORING_OP_POLL_ADD:
 +		ret = io_poll_add(req, s->sqe);
  		break;
 -	case IORING_OP_MADVISE:
 -		if (sqe) {
 -			ret = io_madvise_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_madvise(req, nxt, force_nonblock);
 +	case IORING_OP_POLL_REMOVE:
 +		ret = io_poll_remove(req, s->sqe);
  		break;
 -	case IORING_OP_OPENAT2:
 -		if (sqe) {
 -			ret = io_openat2_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_openat2(req, nxt, force_nonblock);
 +	case IORING_OP_SYNC_FILE_RANGE:
 +		ret = io_sync_file_range(req, s->sqe, force_nonblock);
  		break;
 -	case IORING_OP_EPOLL_CTL:
 -		if (sqe) {
 -			ret = io_epoll_ctl_prep(req, sqe);
 -			if (ret)
 -				break;
 -		}
 -		ret = io_epoll_ctl(req, nxt, force_nonblock);
 +	case IORING_OP_SENDMSG:
 +		ret = io_sendmsg(req, s->sqe, force_nonblock);
 +		break;
 +	case IORING_OP_RECVMSG:
 +		ret = io_recvmsg(req, s->sqe, force_nonblock);
  		break;
  	default:
  		ret = -EINVAL;
* Unmerged path fs/io_uring.c
