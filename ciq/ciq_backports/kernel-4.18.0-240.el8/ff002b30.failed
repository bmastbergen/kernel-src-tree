io_uring: grab ->fs as part of async preparation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit ff002b30181d30cdfbca316dadd099c3ca0d739c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ff002b30.failed

This passes it in to io-wq, so it assumes the right fs_struct when
executing async work that may need to do lookups.

	Cc: stable@vger.kernel.org # 5.3+
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ff002b30181d30cdfbca316dadd099c3ca0d739c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index ffb8e9d82a6a,2a7bb178986e..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -71,6 -71,14 +71,17 @@@
  #include <linux/sizes.h>
  #include <linux/hugetlb.h>
  #include <linux/highmem.h>
++<<<<<<< HEAD
++=======
+ #include <linux/namei.h>
+ #include <linux/fsnotify.h>
+ #include <linux/fadvise.h>
+ #include <linux/eventpoll.h>
+ #include <linux/fs_struct.h>
+ 
+ #define CREATE_TRACE_POINTS
+ #include <trace/events/io_uring.h>
++>>>>>>> ff002b30181d (io_uring: grab ->fs as part of async preparation)
  
  #include <uapi/linux/io_uring.h>
  
@@@ -366,8 -595,171 +377,176 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ 	/* needs file table */
+ 	unsigned		file_table : 1;
+ 	/* needs ->fs */
+ 	unsigned		needs_fs : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	[IORING_OP_NOP] = {},
+ 	[IORING_OP_READV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITEV] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FSYNC] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_READ_FIXED] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE_FIXED] = {
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_ADD] = {
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_POLL_REMOVE] = {},
+ 	[IORING_OP_SYNC_FILE_RANGE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_SENDMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_RECVMSG] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_TIMEOUT_REMOVE] = {},
+ 	[IORING_OP_ACCEPT] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_ASYNC_CANCEL] = {},
+ 	[IORING_OP_LINK_TIMEOUT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_CONNECT] = {
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FALLOCATE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_OPENAT] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_CLOSE] = {
+ 		.needs_file		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_FILES_UPDATE] = {
+ 		.needs_mm		= 1,
+ 		.file_table		= 1,
+ 	},
+ 	[IORING_OP_STATX] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_READ] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_WRITE] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_FADVISE] = {
+ 		.needs_file		= 1,
+ 	},
+ 	[IORING_OP_MADVISE] = {
+ 		.needs_mm		= 1,
+ 	},
+ 	[IORING_OP_SEND] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_RECV] = {
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	[IORING_OP_OPENAT2] = {
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 		.file_table		= 1,
+ 		.needs_fs		= 1,
+ 	},
+ 	[IORING_OP_EPOLL_CTL] = {
+ 		.unbound_nonreg_file	= 1,
+ 		.file_table		= 1,
+ 	},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
+ static int io_grab_files(struct io_kiocb *req);
+ static void io_ring_file_ref_flush(struct fixed_file_data *data);
+ static void io_cleanup_req(struct io_kiocb *req);
++>>>>>>> ff002b30181d (io_uring: grab ->fs as part of async preparation)
  
  static struct kmem_cache *req_cachep;
  
@@@ -480,7 -890,127 +659,131 @@@ static inline void io_queue_async_work(
  		}
  	}
  
++<<<<<<< HEAD
 +	queue_work(ctx->sqo_wq[rw], &req->work);
++=======
+ 	return NULL;
+ }
+ 
+ static void __io_commit_cqring(struct io_ring_ctx *ctx)
+ {
+ 	struct io_rings *rings = ctx->rings;
+ 
+ 	/* order cqe stores with ring update */
+ 	smp_store_release(&rings->cq.tail, ctx->cached_cq_tail);
+ 
+ 	if (wq_has_sleeper(&ctx->cq_wait)) {
+ 		wake_up_interruptible(&ctx->cq_wait);
+ 		kill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);
+ 	}
+ }
+ 
+ static inline void io_req_work_grab_env(struct io_kiocb *req,
+ 					const struct io_op_def *def)
+ {
+ 	if (!req->work.mm && def->needs_mm) {
+ 		mmgrab(current->mm);
+ 		req->work.mm = current->mm;
+ 	}
+ 	if (!req->work.creds)
+ 		req->work.creds = get_current_cred();
+ 	if (!req->work.fs && def->needs_fs) {
+ 		spin_lock(&current->fs->lock);
+ 		if (!current->fs->in_exec) {
+ 			req->work.fs = current->fs;
+ 			req->work.fs->users++;
+ 		} else {
+ 			req->work.flags |= IO_WQ_WORK_CANCEL;
+ 		}
+ 		spin_unlock(&current->fs->lock);
+ 	}
+ }
+ 
+ static inline void io_req_work_drop_env(struct io_kiocb *req)
+ {
+ 	if (req->work.mm) {
+ 		mmdrop(req->work.mm);
+ 		req->work.mm = NULL;
+ 	}
+ 	if (req->work.creds) {
+ 		put_cred(req->work.creds);
+ 		req->work.creds = NULL;
+ 	}
+ 	if (req->work.fs) {
+ 		struct fs_struct *fs = req->work.fs;
+ 
+ 		spin_lock(&req->work.fs->lock);
+ 		if (--fs->users)
+ 			fs = NULL;
+ 		spin_unlock(&req->work.fs->lock);
+ 		if (fs)
+ 			free_fs_struct(fs);
+ 	}
+ }
+ 
+ static inline bool io_prep_async_work(struct io_kiocb *req,
+ 				      struct io_kiocb **link)
+ {
+ 	const struct io_op_def *def = &io_op_defs[req->opcode];
+ 	bool do_hashed = false;
+ 
+ 	if (req->flags & REQ_F_ISREG) {
+ 		if (def->hash_reg_file)
+ 			do_hashed = true;
+ 	} else {
+ 		if (def->unbound_nonreg_file)
+ 			req->work.flags |= IO_WQ_WORK_UNBOUND;
+ 	}
+ 
+ 	io_req_work_grab_env(req, def);
+ 
+ 	*link = io_prep_linked_timeout(req);
+ 	return do_hashed;
+ }
+ 
+ static inline void io_queue_async_work(struct io_kiocb *req)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_kiocb *link;
+ 	bool do_hashed;
+ 
+ 	do_hashed = io_prep_async_work(req, &link);
+ 
+ 	trace_io_uring_queue_async_work(ctx, do_hashed, req, &req->work,
+ 					req->flags);
+ 	if (!do_hashed) {
+ 		io_wq_enqueue(ctx->io_wq, &req->work);
+ 	} else {
+ 		io_wq_enqueue_hashed(ctx->io_wq, &req->work,
+ 					file_inode(req->file));
+ 	}
+ 
+ 	if (link)
+ 		io_queue_linked_timeout(link);
+ }
+ 
+ static void io_kill_timeout(struct io_kiocb *req)
+ {
+ 	int ret;
+ 
+ 	ret = hrtimer_try_to_cancel(&req->io->timeout.timer);
+ 	if (ret != -1) {
+ 		atomic_inc(&req->ctx->cq_timeouts);
+ 		list_del_init(&req->list);
+ 		io_cqring_fill_event(req, 0);
+ 		io_put_req(req);
+ 	}
+ }
+ 
+ static void io_kill_timeouts(struct io_ring_ctx *ctx)
+ {
+ 	struct io_kiocb *req, *tmp;
+ 
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_entry_safe(req, tmp, &ctx->timeout_list, list)
+ 		io_kill_timeout(req);
+ 	spin_unlock_irq(&ctx->completion_lock);
++>>>>>>> ff002b30181d (io_uring: grab ->fs as part of async preparation)
  }
  
  static void io_commit_cqring(struct io_ring_ctx *ctx)
* Unmerged path fs/io_uring.c
