io_uring: fix sequence logic for timeout requests

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 7adf4eaf60f3d8c3584bed51fe7066d4dfc2cbe1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/7adf4eaf.failed

We have two ways a request can be deferred:

1) It's a regular request that depends on another one
2) It's a timeout that tracks completions

We have a shared helper to determine whether to defer, and that
attempts to make the right decision based on the request. But we
only have some of this information in the caller. Un-share the
two timeout/defer helpers so the caller can use the right one.

Fixes: 5262f567987d ("io_uring: IORING_OP_TIMEOUT support")
	Reported-by: yangerkun <yangerkun@huawei.com>
	Reviewed-by: Jackie Liu <liuyun01@kylinos.cn>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 7adf4eaf60f3d8c3584bed51fe7066d4dfc2cbe1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index e831d5a8df98,38d274fc0f25..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -427,24 -414,41 +427,59 @@@ static struct io_ring_ctx *io_ring_ctx_
  	return ctx;
  }
  
+ static inline bool __io_sequence_defer(struct io_ring_ctx *ctx,
+ 				       struct io_kiocb *req)
+ {
++<<<<<<< HEAD
++	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) != REQ_F_IO_DRAIN)
++		return false;
++
++	return req->sequence != ctx->cached_cq_tail + ctx->sq_ring->dropped;
++}
++
++=======
+ 	return req->sequence != ctx->cached_cq_tail + ctx->rings->sq_dropped;
+ }
+ 
  static inline bool io_sequence_defer(struct io_ring_ctx *ctx,
  				     struct io_kiocb *req)
  {
  	if ((req->flags & (REQ_F_IO_DRAIN|REQ_F_IO_DRAINED)) != REQ_F_IO_DRAIN)
  		return false;
  
- 	return req->sequence != ctx->cached_cq_tail + ctx->sq_ring->dropped;
+ 	return __io_sequence_defer(ctx, req);
  }
  
++>>>>>>> 7adf4eaf60f3 (io_uring: fix sequence logic for timeout requests)
  static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
  {
  	struct io_kiocb *req;
  
++<<<<<<< HEAD
 +	if (list_empty(&ctx->defer_list))
 +		return NULL;
 +
 +	req = list_first_entry(&ctx->defer_list, struct io_kiocb, list);
 +	if (!io_sequence_defer(ctx, req)) {
++=======
+ 	req = list_first_entry_or_null(&ctx->defer_list, struct io_kiocb, list);
+ 	if (req && !io_sequence_defer(ctx, req)) {
++>>>>>>> 7adf4eaf60f3 (io_uring: fix sequence logic for timeout requests)
+ 		list_del_init(&req->list);
+ 		return req;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
++<<<<<<< HEAD
++=======
+ static struct io_kiocb *io_get_timeout_req(struct io_ring_ctx *ctx)
+ {
+ 	struct io_kiocb *req;
+ 
+ 	req = list_first_entry_or_null(&ctx->timeout_list, struct io_kiocb, list);
+ 	if (req && !__io_sequence_defer(ctx, req)) {
  		list_del_init(&req->list);
  		return req;
  	}
@@@ -452,13 -456,13 +487,14 @@@
  	return NULL;
  }
  
++>>>>>>> 7adf4eaf60f3 (io_uring: fix sequence logic for timeout requests)
  static void __io_commit_cqring(struct io_ring_ctx *ctx)
  {
 -	struct io_rings *rings = ctx->rings;
 +	struct io_cq_ring *ring = ctx->cq_ring;
  
 -	if (ctx->cached_cq_tail != READ_ONCE(rings->cq.tail)) {
 +	if (ctx->cached_cq_tail != READ_ONCE(ring->r.tail)) {
  		/* order cqe stores with ring update */
 -		smp_store_release(&rings->cq.tail, ctx->cached_cq_tail);
 +		smp_store_release(&ring->r.tail, ctx->cached_cq_tail);
  
  		if (wq_has_sleeper(&ctx->cq_wait)) {
  			wake_up_interruptible(&ctx->cq_wait);
* Unmerged path fs/io_uring.c
