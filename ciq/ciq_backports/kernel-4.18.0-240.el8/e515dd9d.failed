xprtrdma: DMA map rr_rdma_buf as each rpcrdma_rep is created

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Chuck Lever <chuck.lever@oracle.com>
commit e515dd9d76d22446b67f1568e3fc39ec84635360
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e515dd9d.failed

Clean up: This simplifies the logic in rpcrdma_post_recvs.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit e515dd9d76d22446b67f1568e3fc39ec84635360)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
diff --cc net/sunrpc/xprtrdma/verbs.c
index e03a8d720a48,353f61ac8d51..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -1022,15 -1071,48 +1022,24 @@@ static void rpcrdma_reqs_reset(struct r
  {
  	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
  	struct rpcrdma_req *req;
 -	int rc;
  
  	list_for_each_entry(req, &buf->rb_allreqs, rl_all) {
 -		rc = rpcrdma_req_setup(r_xprt, req);
 -		if (rc)
 -			return rc;
 +		/* Credits are valid only for one connection */
 +		req->rl_slot.rq_cong = 0;
  	}
 -	return 0;
 -}
 -
 -static void rpcrdma_req_reset(struct rpcrdma_req *req)
 -{
 -	/* Credits are valid for only one connection */
 -	req->rl_slot.rq_cong = 0;
 -
 -	rpcrdma_regbuf_free(req->rl_rdmabuf);
 -	req->rl_rdmabuf = NULL;
 -
 -	rpcrdma_regbuf_dma_unmap(req->rl_sendbuf);
 -	rpcrdma_regbuf_dma_unmap(req->rl_recvbuf);
 -}
 -
 -/* ASSUMPTION: the rb_allreqs list is stable for the duration,
 - * and thus can be walked without holding rb_lock. Eg. the
 - * caller is holding the transport send lock to exclude
 - * device removal or disconnection.
 - */
 -static void rpcrdma_reqs_reset(struct rpcrdma_xprt *r_xprt)
 -{
 -	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
 -	struct rpcrdma_req *req;
 -
 -	list_for_each_entry(req, &buf->rb_allreqs, rl_all)
 -		rpcrdma_req_reset(req);
  }
  
++<<<<<<< HEAD
 +static struct rpcrdma_rep *rpcrdma_rep_create(struct rpcrdma_xprt *r_xprt,
 +					      bool temp)
++=======
+ /* No locking needed here. This function is called only by the
+  * Receive completion handler.
+  */
+ static noinline
+ struct rpcrdma_rep *rpcrdma_rep_create(struct rpcrdma_xprt *r_xprt,
+ 				       bool temp)
++>>>>>>> e515dd9d76d2 (xprtrdma: DMA map rr_rdma_buf as each rpcrdma_rep is created)
  {
  	struct rpcrdma_rep *rep;
  
@@@ -1052,8 -1137,11 +1064,10 @@@
  	rep->rr_recv_wr.sg_list = &rep->rr_rdmabuf->rg_iov;
  	rep->rr_recv_wr.num_sge = 1;
  	rep->rr_temp = temp;
 -	list_add(&rep->rr_all, &r_xprt->rx_buf.rb_all_reps);
  	return rep;
  
+ out_free_regbuf:
+ 	rpcrdma_regbuf_free(rep->rr_rdmabuf);
  out_free:
  	kfree(rep);
  out:
@@@ -1517,16 -1579,6 +1533,19 @@@ void rpcrdma_post_recvs(struct rpcrdma_
  	if (!wr)
  		goto out;
  
++<<<<<<< HEAD
 +	for (i = wr; i; i = i->next) {
 +		rep = container_of(i, struct rpcrdma_rep, rr_recv_wr);
 +
 +		if (!rpcrdma_regbuf_dma_map(r_xprt, rep->rr_rdmabuf))
 +			goto release_wrs;
 +
 +		trace_xprtrdma_post_recv(rep->rr_recv_wr.wr_cqe);
 +		++count;
 +	}
 +
++=======
++>>>>>>> e515dd9d76d2 (xprtrdma: DMA map rr_rdma_buf as each rpcrdma_rep is created)
  	rc = ib_post_recv(r_xprt->rx_ia.ri_id->qp, wr,
  			  (const struct ib_recv_wr **)&bad_wr);
  out:
* Unmerged path net/sunrpc/xprtrdma/verbs.c
