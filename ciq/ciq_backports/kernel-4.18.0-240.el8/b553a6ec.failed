bpf: Simplify __bpf_arch_text_poke poke type handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit b553a6ec570044fc1ae300c6fb24f9ce204c5894
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b553a6ec.failed

Given that we have BPF_MOD_NOP_TO_{CALL,JUMP}, BPF_MOD_{CALL,JUMP}_TO_NOP
and BPF_MOD_{CALL,JUMP}_TO_{CALL,JUMP} poke types and that we also pass in
old_addr as well as new_addr, it's a bit redundant and unnecessarily
complicates __bpf_arch_text_poke() itself since we can derive the same from
the *_addr that were passed in. Hence simplify and use BPF_MOD_{CALL,JUMP}
as types which also allows to clean up call-sites.

In addition to that, __bpf_arch_text_poke() currently verifies that text
matches expected old_insn before we invoke text_poke_bp(). Also add a check
on new_insn and skip rewrite if it already matches. Reason why this is rather
useful is that it avoids making any special casing in prog_array_map_poke_run()
when old and new prog were NULL and has the benefit that also for this case
we perform a check on text whether it really matches our expectations.

	Suggested-by: Andrii Nakryiko <andriin@fb.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/fcb00a2b0b288d6c73de4ef58116a821c8fe8f2f.1574555798.git.daniel@iogearbox.net
(cherry picked from commit b553a6ec570044fc1ae300c6fb24f9ce204c5894)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
#	include/linux/bpf.h
#	kernel/bpf/arraymap.c
#	kernel/bpf/trampoline.c
diff --cc arch/x86/net/bpf_jit_comp.c
index 490d8e35c7c9,b8be18427277..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -239,6 -239,89 +239,92 @@@ static void emit_prologue(u8 **pprog, u
  	*pprog = prog;
  }
  
++<<<<<<< HEAD
++=======
+ static int emit_patch(u8 **pprog, void *func, void *ip, u8 opcode)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 	s64 offset;
+ 
+ 	offset = func - (ip + X86_PATCH_SIZE);
+ 	if (!is_simm32(offset)) {
+ 		pr_err("Target call %p is out of range\n", func);
+ 		return -ERANGE;
+ 	}
+ 	EMIT1_off32(opcode, offset);
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static int emit_call(u8 **pprog, void *func, void *ip)
+ {
+ 	return emit_patch(pprog, func, ip, 0xE8);
+ }
+ 
+ static int emit_jump(u8 **pprog, void *func, void *ip)
+ {
+ 	return emit_patch(pprog, func, ip, 0xE9);
+ }
+ 
+ static int __bpf_arch_text_poke(void *ip, enum bpf_text_poke_type t,
+ 				void *old_addr, void *new_addr,
+ 				const bool text_live)
+ {
+ 	const u8 *nop_insn = ideal_nops[NOP_ATOMIC5];
+ 	u8 old_insn[X86_PATCH_SIZE];
+ 	u8 new_insn[X86_PATCH_SIZE];
+ 	u8 *prog;
+ 	int ret;
+ 
+ 	memcpy(old_insn, nop_insn, X86_PATCH_SIZE);
+ 	if (old_addr) {
+ 		prog = old_insn;
+ 		ret = t == BPF_MOD_CALL ?
+ 		      emit_call(&prog, old_addr, ip) :
+ 		      emit_jump(&prog, old_addr, ip);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	memcpy(new_insn, nop_insn, X86_PATCH_SIZE);
+ 	if (new_addr) {
+ 		prog = new_insn;
+ 		ret = t == BPF_MOD_CALL ?
+ 		      emit_call(&prog, new_addr, ip) :
+ 		      emit_jump(&prog, new_addr, ip);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	ret = -EBUSY;
+ 	mutex_lock(&text_mutex);
+ 	if (memcmp(ip, old_insn, X86_PATCH_SIZE))
+ 		goto out;
+ 	if (memcmp(ip, new_insn, X86_PATCH_SIZE)) {
+ 		if (text_live)
+ 			text_poke_bp(ip, new_insn, X86_PATCH_SIZE, NULL);
+ 		else
+ 			memcpy(ip, new_insn, X86_PATCH_SIZE);
+ 	}
+ 	ret = 0;
+ out:
+ 	mutex_unlock(&text_mutex);
+ 	return ret;
+ }
+ 
+ int bpf_arch_text_poke(void *ip, enum bpf_text_poke_type t,
+ 		       void *old_addr, void *new_addr)
+ {
+ 	if (!is_kernel_text((long)ip) &&
+ 	    !is_bpf_text_address((long)ip))
+ 		/* BPF poking in modules is not supported */
+ 		return -EINVAL;
+ 
+ 	return __bpf_arch_text_poke(ip, t, old_addr, new_addr, true);
+ }
+ 
++>>>>>>> b553a6ec5700 (bpf: Simplify __bpf_arch_text_poke poke type handling)
  /*
   * Generate the following code:
   *
@@@ -320,6 -403,68 +406,71 @@@ static void emit_bpf_tail_call(u8 **ppr
  	*pprog = prog;
  }
  
++<<<<<<< HEAD
++=======
+ static void emit_bpf_tail_call_direct(struct bpf_jit_poke_descriptor *poke,
+ 				      u8 **pprog, int addr, u8 *image)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	/*
+ 	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)
+ 	 *	goto out;
+ 	 */
+ 	EMIT2_off32(0x8B, 0x85, -36 - MAX_BPF_STACK); /* mov eax, dword ptr [rbp - 548] */
+ 	EMIT3(0x83, 0xF8, MAX_TAIL_CALL_CNT);         /* cmp eax, MAX_TAIL_CALL_CNT */
+ 	EMIT2(X86_JA, 14);                            /* ja out */
+ 	EMIT3(0x83, 0xC0, 0x01);                      /* add eax, 1 */
+ 	EMIT2_off32(0x89, 0x85, -36 - MAX_BPF_STACK); /* mov dword ptr [rbp -548], eax */
+ 
+ 	poke->ip = image + (addr - X86_PATCH_SIZE);
+ 	poke->adj_off = PROLOGUE_SIZE;
+ 
+ 	memcpy(prog, ideal_nops[NOP_ATOMIC5], X86_PATCH_SIZE);
+ 	prog += X86_PATCH_SIZE;
+ 	/* out: */
+ 
+ 	*pprog = prog;
+ }
+ 
+ static void bpf_tail_call_direct_fixup(struct bpf_prog *prog)
+ {
+ 	struct bpf_jit_poke_descriptor *poke;
+ 	struct bpf_array *array;
+ 	struct bpf_prog *target;
+ 	int i, ret;
+ 
+ 	for (i = 0; i < prog->aux->size_poke_tab; i++) {
+ 		poke = &prog->aux->poke_tab[i];
+ 		WARN_ON_ONCE(READ_ONCE(poke->ip_stable));
+ 
+ 		if (poke->reason != BPF_POKE_REASON_TAIL_CALL)
+ 			continue;
+ 
+ 		array = container_of(poke->tail_call.map, struct bpf_array, map);
+ 		mutex_lock(&array->aux->poke_mutex);
+ 		target = array->ptrs[poke->tail_call.key];
+ 		if (target) {
+ 			/* Plain memcpy is used when image is not live yet
+ 			 * and still not locked as read-only. Once poke
+ 			 * location is active (poke->ip_stable), any parallel
+ 			 * bpf_arch_text_poke() might occur still on the
+ 			 * read-write image until we finally locked it as
+ 			 * read-only. Both modifications on the given image
+ 			 * are under text_mutex to avoid interference.
+ 			 */
+ 			ret = __bpf_arch_text_poke(poke->ip, BPF_MOD_JUMP, NULL,
+ 						   (u8 *)target->bpf_func +
+ 						   poke->adj_off, false);
+ 			BUG_ON(ret < 0);
+ 		}
+ 		WRITE_ONCE(poke->ip_stable, true);
+ 		mutex_unlock(&array->aux->poke_mutex);
+ 	}
+ }
+ 
++>>>>>>> b553a6ec5700 (bpf: Simplify __bpf_arch_text_poke poke type handling)
  static void emit_mov_imm32(u8 **pprog, bool sign_propagate,
  			   u32 dst_reg, const u32 imm32)
  {
diff --cc include/linux/bpf.h
index 8c8f4e236fdc,35903f148be5..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -1182,10 -1324,10 +1182,15 @@@ static inline u32 bpf_xdp_sock_convert_
  #endif /* CONFIG_INET */
  
  enum bpf_text_poke_type {
++<<<<<<< HEAD
 +	BPF_MOD_NOP_TO_CALL,
 +	BPF_MOD_CALL_TO_CALL,
 +	BPF_MOD_CALL_TO_NOP,
++=======
+ 	BPF_MOD_CALL,
+ 	BPF_MOD_JUMP,
++>>>>>>> b553a6ec5700 (bpf: Simplify __bpf_arch_text_poke poke type handling)
  };
 -
  int bpf_arch_text_poke(void *ip, enum bpf_text_poke_type t,
  		       void *addr1, void *addr2);
  
diff --cc kernel/bpf/arraymap.c
index d8ca7db5991a,f0d19bbb9211..000000000000
--- a/kernel/bpf/arraymap.c
+++ b/kernel/bpf/arraymap.c
@@@ -633,6 -686,142 +633,145 @@@ static void prog_array_map_seq_show_ele
  	rcu_read_unlock();
  }
  
++<<<<<<< HEAD
++=======
+ struct prog_poke_elem {
+ 	struct list_head list;
+ 	struct bpf_prog_aux *aux;
+ };
+ 
+ static int prog_array_map_poke_track(struct bpf_map *map,
+ 				     struct bpf_prog_aux *prog_aux)
+ {
+ 	struct prog_poke_elem *elem;
+ 	struct bpf_array_aux *aux;
+ 	int ret = 0;
+ 
+ 	aux = container_of(map, struct bpf_array, map)->aux;
+ 	mutex_lock(&aux->poke_mutex);
+ 	list_for_each_entry(elem, &aux->poke_progs, list) {
+ 		if (elem->aux == prog_aux)
+ 			goto out;
+ 	}
+ 
+ 	elem = kmalloc(sizeof(*elem), GFP_KERNEL);
+ 	if (!elem) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	INIT_LIST_HEAD(&elem->list);
+ 	/* We must track the program's aux info at this point in time
+ 	 * since the program pointer itself may not be stable yet, see
+ 	 * also comment in prog_array_map_poke_run().
+ 	 */
+ 	elem->aux = prog_aux;
+ 
+ 	list_add_tail(&elem->list, &aux->poke_progs);
+ out:
+ 	mutex_unlock(&aux->poke_mutex);
+ 	return ret;
+ }
+ 
+ static void prog_array_map_poke_untrack(struct bpf_map *map,
+ 					struct bpf_prog_aux *prog_aux)
+ {
+ 	struct prog_poke_elem *elem, *tmp;
+ 	struct bpf_array_aux *aux;
+ 
+ 	aux = container_of(map, struct bpf_array, map)->aux;
+ 	mutex_lock(&aux->poke_mutex);
+ 	list_for_each_entry_safe(elem, tmp, &aux->poke_progs, list) {
+ 		if (elem->aux == prog_aux) {
+ 			list_del_init(&elem->list);
+ 			kfree(elem);
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&aux->poke_mutex);
+ }
+ 
+ static void prog_array_map_poke_run(struct bpf_map *map, u32 key,
+ 				    struct bpf_prog *old,
+ 				    struct bpf_prog *new)
+ {
+ 	struct prog_poke_elem *elem;
+ 	struct bpf_array_aux *aux;
+ 
+ 	aux = container_of(map, struct bpf_array, map)->aux;
+ 	WARN_ON_ONCE(!mutex_is_locked(&aux->poke_mutex));
+ 
+ 	list_for_each_entry(elem, &aux->poke_progs, list) {
+ 		struct bpf_jit_poke_descriptor *poke;
+ 		int i, ret;
+ 
+ 		for (i = 0; i < elem->aux->size_poke_tab; i++) {
+ 			poke = &elem->aux->poke_tab[i];
+ 
+ 			/* Few things to be aware of:
+ 			 *
+ 			 * 1) We can only ever access aux in this context, but
+ 			 *    not aux->prog since it might not be stable yet and
+ 			 *    there could be danger of use after free otherwise.
+ 			 * 2) Initially when we start tracking aux, the program
+ 			 *    is not JITed yet and also does not have a kallsyms
+ 			 *    entry. We skip these as poke->ip_stable is not
+ 			 *    active yet. The JIT will do the final fixup before
+ 			 *    setting it stable. The various poke->ip_stable are
+ 			 *    successively activated, so tail call updates can
+ 			 *    arrive from here while JIT is still finishing its
+ 			 *    final fixup for non-activated poke entries.
+ 			 * 3) On program teardown, the program's kallsym entry gets
+ 			 *    removed out of RCU callback, but we can only untrack
+ 			 *    from sleepable context, therefore bpf_arch_text_poke()
+ 			 *    might not see that this is in BPF text section and
+ 			 *    bails out with -EINVAL. As these are unreachable since
+ 			 *    RCU grace period already passed, we simply skip them.
+ 			 * 4) Also programs reaching refcount of zero while patching
+ 			 *    is in progress is okay since we're protected under
+ 			 *    poke_mutex and untrack the programs before the JIT
+ 			 *    buffer is freed. When we're still in the middle of
+ 			 *    patching and suddenly kallsyms entry of the program
+ 			 *    gets evicted, we just skip the rest which is fine due
+ 			 *    to point 3).
+ 			 * 5) Any other error happening below from bpf_arch_text_poke()
+ 			 *    is a unexpected bug.
+ 			 */
+ 			if (!READ_ONCE(poke->ip_stable))
+ 				continue;
+ 			if (poke->reason != BPF_POKE_REASON_TAIL_CALL)
+ 				continue;
+ 			if (poke->tail_call.map != map ||
+ 			    poke->tail_call.key != key)
+ 				continue;
+ 
+ 			ret = bpf_arch_text_poke(poke->ip, BPF_MOD_JUMP,
+ 						 old ? (u8 *)old->bpf_func +
+ 						 poke->adj_off : NULL,
+ 						 new ? (u8 *)new->bpf_func +
+ 						 poke->adj_off : NULL);
+ 			BUG_ON(ret < 0 && ret != -EINVAL);
+ 		}
+ 	}
+ }
+ 
+ static void prog_array_map_clear_deferred(struct work_struct *work)
+ {
+ 	struct bpf_map *map = container_of(work, struct bpf_array_aux,
+ 					   work)->map;
+ 	bpf_fd_array_map_clear(map);
+ 	bpf_map_put(map);
+ }
+ 
+ static void prog_array_map_clear(struct bpf_map *map)
+ {
+ 	struct bpf_array_aux *aux = container_of(map, struct bpf_array,
+ 						 map)->aux;
+ 	bpf_map_inc(map);
+ 	schedule_work(&aux->work);
+ }
+ 
++>>>>>>> b553a6ec5700 (bpf: Simplify __bpf_arch_text_poke poke type handling)
  static struct bpf_map *prog_array_map_alloc(union bpf_attr *attr)
  {
  	struct bpf_array_aux *aux;
* Unmerged path kernel/bpf/trampoline.c
* Unmerged path arch/x86/net/bpf_jit_comp.c
* Unmerged path include/linux/bpf.h
* Unmerged path kernel/bpf/arraymap.c
* Unmerged path kernel/bpf/trampoline.c
