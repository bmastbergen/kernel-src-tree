io_uring: add support for probing opcodes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 66f4af93da5761d2fa05c0dc673a47003cdb9cfe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/66f4af93.failed

The application currently has no way of knowing if a given opcode is
supported or not without having to try and issue one and see if we get
-EINVAL or not. And even this approach is fraught with peril, as maybe
we're getting -EINVAL due to some fields being missing, or maybe it's
just not that easy to issue that particular command without doing some
other leg work in terms of setup first.

This adds IORING_REGISTER_PROBE, which fills in a structure with info
on what it supported or not. This will work even with sparse opcode
fields, which may happen in the future or even today if someone
backports specific features to older kernels.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 66f4af93da5761d2fa05c0dc673a47003cdb9cfe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index 28a601d08266,7715a729271a..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -367,8 -548,183 +367,188 @@@ struct io_submit_state 
  	unsigned int		ios_left;
  };
  
++<<<<<<< HEAD
 +static void io_sq_wq_submit_work(struct work_struct *work);
 +static void __io_free_req(struct io_kiocb *req);
++=======
+ struct io_op_def {
+ 	/* needs req->io allocated for deferral/async */
+ 	unsigned		async_ctx : 1;
+ 	/* needs current->mm setup, does mm access */
+ 	unsigned		needs_mm : 1;
+ 	/* needs req->file assigned */
+ 	unsigned		needs_file : 1;
+ 	/* needs req->file assigned IFF fd is >= 0 */
+ 	unsigned		fd_non_neg : 1;
+ 	/* hash wq insertion if file is a regular file */
+ 	unsigned		hash_reg_file : 1;
+ 	/* unbound wq insertion if file is a non-regular file */
+ 	unsigned		unbound_nonreg_file : 1;
+ 	/* opcode is not supported by this kernel */
+ 	unsigned		not_supported : 1;
+ };
+ 
+ static const struct io_op_def io_op_defs[] = {
+ 	{
+ 		/* IORING_OP_NOP */
+ 	},
+ 	{
+ 		/* IORING_OP_READV */
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_WRITEV */
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_FSYNC */
+ 		.needs_file		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_READ_FIXED */
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_WRITE_FIXED */
+ 		.needs_file		= 1,
+ 		.hash_reg_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_POLL_ADD */
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_POLL_REMOVE */
+ 	},
+ 	{
+ 		/* IORING_OP_SYNC_FILE_RANGE */
+ 		.needs_file		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_SENDMSG */
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_RECVMSG */
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_TIMEOUT */
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_TIMEOUT_REMOVE */
+ 	},
+ 	{
+ 		/* IORING_OP_ACCEPT */
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_ASYNC_CANCEL */
+ 	},
+ 	{
+ 		/* IORING_OP_LINK_TIMEOUT */
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_CONNECT */
+ 		.async_ctx		= 1,
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_FALLOCATE */
+ 		.needs_file		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_OPENAT */
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_CLOSE */
+ 		.needs_file		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_FILES_UPDATE */
+ 		.needs_mm		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_STATX */
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_READ */
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_WRITE */
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_FADVISE */
+ 		.needs_file		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_MADVISE */
+ 		.needs_mm		= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_SEND */
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_RECV */
+ 		.needs_mm		= 1,
+ 		.needs_file		= 1,
+ 		.unbound_nonreg_file	= 1,
+ 	},
+ 	{
+ 		/* IORING_OP_OPENAT2 */
+ 		.needs_file		= 1,
+ 		.fd_non_neg		= 1,
+ 	},
+ };
+ 
+ static void io_wq_submit_work(struct io_wq_work **workptr);
+ static void io_cqring_fill_event(struct io_kiocb *req, long res);
+ static void io_put_req(struct io_kiocb *req);
+ static void __io_double_put_req(struct io_kiocb *req);
+ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);
+ static void io_queue_linked_timeout(struct io_kiocb *req);
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *ip,
+ 				 unsigned nr_args);
++>>>>>>> 66f4af93da57 (io_uring: add support for probing opcodes)
  
  static struct kmem_cache *req_cachep;
  
@@@ -3942,18 -6622,28 +4161,25 @@@ static int __io_uring_register(struct i
  	if (percpu_ref_is_dying(&ctx->refs))
  		return -ENXIO;
  
++<<<<<<< HEAD
 +	percpu_ref_kill(&ctx->refs);
++=======
+ 	if (opcode != IORING_UNREGISTER_FILES &&
+ 	    opcode != IORING_REGISTER_FILES_UPDATE &&
+ 	    opcode != IORING_REGISTER_PROBE) {
+ 		percpu_ref_kill(&ctx->refs);
++>>>>>>> 66f4af93da57 (io_uring: add support for probing opcodes)
  
 -		/*
 -		 * Drop uring mutex before waiting for references to exit. If
 -		 * another thread is currently inside io_uring_enter() it might
 -		 * need to grab the uring_lock to make progress. If we hold it
 -		 * here across the drain wait, then we can deadlock. It's safe
 -		 * to drop the mutex here, since no new references will come in
 -		 * after we've killed the percpu ref.
 -		 */
 -		mutex_unlock(&ctx->uring_lock);
 -		ret = wait_for_completion_interruptible(&ctx->completions[0]);
 -		mutex_lock(&ctx->uring_lock);
 -		if (ret) {
 -			percpu_ref_resurrect(&ctx->refs);
 -			ret = -EINTR;
 -			goto out;
 -		}
 -	}
 +	/*
 +	 * Drop uring mutex before waiting for references to exit. If another
 +	 * thread is currently inside io_uring_enter() it might need to grab
 +	 * the uring_lock to make progress. If we hold it here across the drain
 +	 * wait, then we can deadlock. It's safe to drop the mutex here, since
 +	 * no new references will come in after we've killed the percpu ref.
 +	 */
 +	mutex_unlock(&ctx->uring_lock);
 +	wait_for_completion(&ctx->ctx_done);
 +	mutex_lock(&ctx->uring_lock);
  
  	switch (opcode) {
  	case IORING_REGISTER_BUFFERS:
@@@ -3994,9 -6697,15 +4226,21 @@@
  		break;
  	}
  
++<<<<<<< HEAD
 +	/* bring the ctx back to life */
 +	reinit_completion(&ctx->ctx_done);
 +	percpu_ref_reinit(&ctx->refs);
++=======
+ 
+ 	if (opcode != IORING_UNREGISTER_FILES &&
+ 	    opcode != IORING_REGISTER_FILES_UPDATE &&
+ 	    opcode != IORING_REGISTER_PROBE) {
+ 		/* bring the ctx back to life */
+ 		percpu_ref_reinit(&ctx->refs);
+ out:
+ 		reinit_completion(&ctx->completions[0]);
+ 	}
++>>>>>>> 66f4af93da57 (io_uring: add support for probing opcodes)
  	return ret;
  }
  
diff --cc include/uapi/linux/io_uring.h
index dd4a49ec83b7,955fd477e530..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -145,6 -193,8 +145,11 @@@ struct io_uring_params 
  #define IORING_REGISTER_EVENTFD		4
  #define IORING_UNREGISTER_EVENTFD	5
  #define IORING_REGISTER_FILES_UPDATE	6
++<<<<<<< HEAD
++=======
+ #define IORING_REGISTER_EVENTFD_ASYNC	7
+ #define IORING_REGISTER_PROBE		8
++>>>>>>> 66f4af93da57 (io_uring: add support for probing opcodes)
  
  struct io_uring_files_update {
  	__u32 offset;
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
