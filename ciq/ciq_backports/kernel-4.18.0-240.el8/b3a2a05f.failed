atomics/treewide: Make conditional inc/dec ops optional

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit b3a2a05f9111de0b79312e577608a27b0318c0a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b3a2a05f.failed

The conditional inc/dec ops differ for atomic_t and atomic64_t:

- atomic_inc_unless_positive() is optional for atomic_t, and doesn't exist for atomic64_t.
- atomic_dec_unless_negative() is optional for atomic_t, and doesn't exist for atomic64_t.
- atomic_dec_if_positive is optional for atomic_t, and is mandatory for atomic64_t.

Let's make these consistently optional for both. At the same time, let's
clean up the existing fallbacks to use atomic_try_cmpxchg().

The instrumented atomics are updated accordingly.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Reviewed-by: Will Deacon <will.deacon@arm.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Boqun Feng <boqun.feng@gmail.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: https://lore.kernel.org/lkml/20180621121321.4761-18-mark.rutland@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit b3a2a05f9111de0b79312e577608a27b0318c0a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/atomic.h
#	arch/ia64/include/asm/atomic.h
#	arch/parisc/include/asm/atomic.h
#	arch/s390/include/asm/atomic.h
#	arch/x86/include/asm/atomic64_64.h
diff --cc arch/arm64/include/asm/atomic.h
index c0235e0ff849,9bca54dda75c..000000000000
--- a/arch/arm64/include/asm/atomic.h
+++ b/arch/arm64/include/asm/atomic.h
@@@ -195,16 -157,9 +195,20 @@@
  #define atomic64_cmpxchg_release	atomic_cmpxchg_release
  #define atomic64_cmpxchg		atomic_cmpxchg
  
 +#define atomic64_inc(v)			atomic64_add(1, (v))
 +#define atomic64_dec(v)			atomic64_sub(1, (v))
 +#define atomic64_inc_and_test(v)	(atomic64_inc_return(v) == 0)
 +#define atomic64_dec_and_test(v)	(atomic64_dec_return(v) == 0)
 +#define atomic64_sub_and_test(i, v)	(atomic64_sub_return((i), (v)) == 0)
 +#define atomic64_add_negative(i, v)	(atomic64_add_return((i), (v)) < 0)
 +#define atomic64_add_unless(v, a, u)	(___atomic_add_unless(v, a, u, 64) != u)
  #define atomic64_andnot			atomic64_andnot
  
++<<<<<<< HEAD
 +#define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1, 0)
++=======
+ #define atomic64_dec_if_positive	atomic64_dec_if_positive
++>>>>>>> b3a2a05f9111 (atomics/treewide: Make conditional inc/dec ops optional)
  
  #endif
  #endif
diff --cc arch/ia64/include/asm/atomic.h
index 2524fb60fbc2,206530d0751b..000000000000
--- a/arch/ia64/include/asm/atomic.h
+++ b/arch/ia64/include/asm/atomic.h
@@@ -215,87 -215,8 +215,90 @@@ ATOMIC64_FETCH_OP(xor, ^
  	(cmpxchg(&((v)->counter), old, new))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
 +
 +static __inline__ long atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	long c, old;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic64_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != (u);
 +}
 +
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
 +static __inline__ long atomic64_dec_if_positive(atomic64_t *v)
 +{
 +	long c, old, dec;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		dec = c - 1;
 +		if (unlikely(dec < 0))
 +			break;
 +		old = atomic64_cmpxchg((v), c, dec);
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return dec;
 +}
 +
 +/*
 + * Atomically add I to V and return TRUE if the resulting value is
 + * negative.
 + */
 +static __inline__ int
 +atomic_add_negative (int i, atomic_t *v)
 +{
 +	return atomic_add_return(i, v) < 0;
 +}
 +
 +static __inline__ long
 +atomic64_add_negative (__s64 i, atomic64_t *v)
 +{
 +	return atomic64_add_return(i, v) < 0;
 +}
 +
 +#define atomic_dec_return(v)		atomic_sub_return(1, (v))
 +#define atomic_inc_return(v)		atomic_add_return(1, (v))
 +#define atomic64_dec_return(v)		atomic64_sub_return(1, (v))
 +#define atomic64_inc_return(v)		atomic64_add_return(1, (v))
 +
 +#define atomic_sub_and_test(i,v)	(atomic_sub_return((i), (v)) == 0)
 +#define atomic_dec_and_test(v)		(atomic_sub_return(1, (v)) == 0)
 +#define atomic_inc_and_test(v)		(atomic_add_return(1, (v)) == 0)
 +#define atomic64_sub_and_test(i,v)	(atomic64_sub_return((i), (v)) == 0)
 +#define atomic64_dec_and_test(v)	(atomic64_sub_return(1, (v)) == 0)
 +#define atomic64_inc_and_test(v)	(atomic64_add_return(1, (v)) == 0)
 +
++=======
++>>>>>>> b3a2a05f9111 (atomics/treewide: Make conditional inc/dec ops optional)
  #define atomic_add(i,v)			(void)atomic_add_return((i), (v))
  #define atomic_sub(i,v)			(void)atomic_sub_return((i), (v))
 +#define atomic_inc(v)			atomic_add(1, (v))
 +#define atomic_dec(v)			atomic_sub(1, (v))
  
  #define atomic64_add(i,v)		(void)atomic64_add_return((i), (v))
  #define atomic64_sub(i,v)		(void)atomic64_sub_return((i), (v))
diff --cc arch/parisc/include/asm/atomic.h
index 88bae6676c9b,118953d41763..000000000000
--- a/arch/parisc/include/asm/atomic.h
+++ b/arch/parisc/include/asm/atomic.h
@@@ -281,55 -223,6 +281,58 @@@ atomic64_read(const atomic64_t *v
  	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +/**
 + * atomic64_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic64_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	long c, old;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic64_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != (u);
 +}
 +
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
 +/*
 + * atomic64_dec_if_positive - decrement by 1 if old value positive
 + * @v: pointer of type atomic_t
 + *
 + * The function returns the old value of *v minus 1, even if
 + * the atomic variable, v, was not decremented.
 + */
 +static inline long atomic64_dec_if_positive(atomic64_t *v)
 +{
 +	long c, old, dec;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		dec = c - 1;
 +		if (unlikely(dec < 0))
 +			break;
 +		old = atomic64_cmpxchg((v), c, dec);
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return dec;
 +}
 +
++=======
++>>>>>>> b3a2a05f9111 (atomics/treewide: Make conditional inc/dec ops optional)
  #endif /* !CONFIG_64BIT */
  
  
diff --cc arch/s390/include/asm/atomic.h
index 4b55532f15c4,fd20ab5d4cf7..000000000000
--- a/arch/s390/include/asm/atomic.h
+++ b/arch/s390/include/asm/atomic.h
@@@ -168,43 -145,6 +168,46 @@@ ATOMIC64_OPS(xor
  
  #undef ATOMIC64_OPS
  
++<<<<<<< HEAD
 +static inline int atomic64_add_unless(atomic64_t *v, long i, long u)
 +{
 +	long c, old;
 +
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == u))
 +			break;
 +		old = atomic64_cmpxchg(v, c, c + i);
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != u;
 +}
 +
 +static inline long atomic64_dec_if_positive(atomic64_t *v)
 +{
 +	long c, old, dec;
 +
 +	c = atomic64_read(v);
 +	for (;;) {
 +		dec = c - 1;
 +		if (unlikely(dec < 0))
 +			break;
 +		old = atomic64_cmpxchg((v), c, dec);
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return dec;
 +}
 +
 +#define atomic64_add_negative(_i, _v)	(atomic64_add_return(_i, _v) < 0)
 +#define atomic64_inc(_v)		atomic64_add(1, _v)
 +#define atomic64_inc_return(_v)		atomic64_add_return(1, _v)
 +#define atomic64_inc_and_test(_v)	(atomic64_add_return(1, _v) == 0)
++=======
++>>>>>>> b3a2a05f9111 (atomics/treewide: Make conditional inc/dec ops optional)
  #define atomic64_sub_return(_i, _v)	atomic64_add_return(-(long)(_i), _v)
  #define atomic64_fetch_sub(_i, _v)	atomic64_fetch_add(-(long)(_i), _v)
  #define atomic64_sub(_i, _v)		atomic64_add(-(long)(_i), _v)
diff --cc arch/x86/include/asm/atomic64_64.h
index c99f33271b13,849f1c566a11..000000000000
--- a/arch/x86/include/asm/atomic64_64.h
+++ b/arch/x86/include/asm/atomic64_64.h
@@@ -188,45 -191,6 +188,48 @@@ static inline long arch_atomic64_xchg(a
  	return xchg(&v->counter, new);
  }
  
++<<<<<<< HEAD
 +/**
 + * arch_atomic64_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic64_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static inline bool arch_atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	s64 c = arch_atomic64_read(v);
 +	do {
 +		if (unlikely(c == u))
 +			return false;
 +	} while (!arch_atomic64_try_cmpxchg(v, &c, c + a));
 +	return true;
 +}
 +
 +#define arch_atomic64_inc_not_zero(v) arch_atomic64_add_unless((v), 1, 0)
 +
 +/*
 + * arch_atomic64_dec_if_positive - decrement by 1 if old value positive
 + * @v: pointer of type atomic_t
 + *
 + * The function returns the old value of *v minus 1, even if
 + * the atomic variable, v, was not decremented.
 + */
 +static inline long arch_atomic64_dec_if_positive(atomic64_t *v)
 +{
 +	s64 dec, c = arch_atomic64_read(v);
 +	do {
 +		dec = c - 1;
 +		if (unlikely(dec < 0))
 +			break;
 +	} while (!arch_atomic64_try_cmpxchg(v, &c, dec));
 +	return dec;
 +}
 +
++=======
++>>>>>>> b3a2a05f9111 (atomics/treewide: Make conditional inc/dec ops optional)
  static inline void arch_atomic64_and(long i, atomic64_t *v)
  {
  	asm volatile(LOCK_PREFIX "andq %1,%0"
diff --git a/arch/alpha/include/asm/atomic.h b/arch/alpha/include/asm/atomic.h
index 767bfdd42992..ba69ee5a2b71 100644
--- a/arch/alpha/include/asm/atomic.h
+++ b/arch/alpha/include/asm/atomic.h
@@ -295,6 +295,7 @@ static inline long atomic64_dec_if_positive(atomic64_t *v)
 	smp_mb();
 	return old - 1;
 }
+#define atomic64_dec_if_positive atomic64_dec_if_positive
 
 #define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 
diff --git a/arch/arc/include/asm/atomic.h b/arch/arc/include/asm/atomic.h
index 11859287c52a..6d64eaa86145 100644
--- a/arch/arc/include/asm/atomic.h
+++ b/arch/arc/include/asm/atomic.h
@@ -559,6 +559,7 @@ static inline long long atomic64_dec_if_positive(atomic64_t *v)
 
 	return val;
 }
+#define atomic64_dec_if_positive atomic64_dec_if_positive
 
 /**
  * atomic64_add_unless - add unless the number is a given value
diff --git a/arch/arm/include/asm/atomic.h b/arch/arm/include/asm/atomic.h
index 66d0e215a773..b0967c372488 100644
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@ -494,6 +494,7 @@ static inline long long atomic64_dec_if_positive(atomic64_t *v)
 
 	return result;
 }
+#define atomic64_dec_if_positive atomic64_dec_if_positive
 
 static inline int atomic64_add_unless(atomic64_t *v, long long a, long long u)
 {
* Unmerged path arch/arm64/include/asm/atomic.h
* Unmerged path arch/ia64/include/asm/atomic.h
* Unmerged path arch/parisc/include/asm/atomic.h
diff --git a/arch/powerpc/include/asm/atomic.h b/arch/powerpc/include/asm/atomic.h
index 682b3e6a1e21..3d8eac2013e6 100644
--- a/arch/powerpc/include/asm/atomic.h
+++ b/arch/powerpc/include/asm/atomic.h
@@ -513,6 +513,7 @@ static __inline__ long atomic64_dec_if_positive(atomic64_t *v)
 
 	return t;
 }
+#define atomic64_dec_if_positive atomic64_dec_if_positive
 
 #define atomic64_cmpxchg(v, o, n) (cmpxchg(&((v)->counter), (o), (n)))
 #define atomic64_cmpxchg_relaxed(v, o, n) \
* Unmerged path arch/s390/include/asm/atomic.h
diff --git a/arch/sparc/include/asm/atomic_64.h b/arch/sparc/include/asm/atomic_64.h
index 28db058d471b..6cab554fcb7d 100644
--- a/arch/sparc/include/asm/atomic_64.h
+++ b/arch/sparc/include/asm/atomic_64.h
@@ -126,5 +126,6 @@ static inline long atomic64_add_unless(atomic64_t *v, long a, long u)
 #define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 
 long atomic64_dec_if_positive(atomic64_t *v);
+#define atomic64_dec_if_positive atomic64_dec_if_positive
 
 #endif /* !(__ARCH_SPARC64_ATOMIC__) */
diff --git a/arch/x86/include/asm/atomic64_32.h b/arch/x86/include/asm/atomic64_32.h
index 92212bf0484f..99989017e718 100644
--- a/arch/x86/include/asm/atomic64_32.h
+++ b/arch/x86/include/asm/atomic64_32.h
@@ -304,6 +304,7 @@ static inline int arch_atomic64_inc_not_zero(atomic64_t *v)
 	return r;
 }
 
+#define arch_atomic64_dec_if_positive arch_atomic64_dec_if_positive
 static inline long long arch_atomic64_dec_if_positive(atomic64_t *v)
 {
 	long long r;
* Unmerged path arch/x86/include/asm/atomic64_64.h
diff --git a/include/asm-generic/atomic-instrumented.h b/include/asm-generic/atomic-instrumented.h
index cfee349ddd5a..9171fa413152 100644
--- a/include/asm-generic/atomic-instrumented.h
+++ b/include/asm-generic/atomic-instrumented.h
@@ -211,11 +211,14 @@ static __always_inline bool atomic64_inc_not_zero(atomic64_t *v)
 	return arch_atomic64_inc_not_zero(v);
 }
 
+#ifdef arch_atomic64_dec_if_positive
+#define atomic64_dec_if_positive atomic64_dec_if_positive
 static __always_inline s64 atomic64_dec_if_positive(atomic64_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	return arch_atomic64_dec_if_positive(v);
 }
+#endif
 
 static __always_inline bool atomic_dec_and_test(atomic_t *v)
 {
diff --git a/include/asm-generic/atomic64.h b/include/asm-generic/atomic64.h
index 6b016e354b56..5f8101cc4c61 100644
--- a/include/asm-generic/atomic64.h
+++ b/include/asm-generic/atomic64.h
@@ -51,6 +51,7 @@ ATOMIC64_OPS(xor)
 #undef ATOMIC64_OP
 
 extern long long atomic64_dec_if_positive(atomic64_t *v);
+#define atomic64_dec_if_positive atomic64_dec_if_positive
 extern long long atomic64_cmpxchg(atomic64_t *v, long long o, long long n);
 extern long long atomic64_xchg(atomic64_t *v, long long new);
 extern long long atomic64_fetch_add_unless(atomic64_t *v, long long a, long long u);
diff --git a/include/linux/atomic.h b/include/linux/atomic.h
index 6ebab115d8ad..b68e14223a0e 100644
--- a/include/linux/atomic.h
+++ b/include/linux/atomic.h
@@ -574,28 +574,30 @@ static inline int atomic_fetch_andnot_release(int i, atomic_t *v)
 #endif
 
 #ifndef atomic_inc_unless_negative
-static inline bool atomic_inc_unless_negative(atomic_t *p)
+static inline bool atomic_inc_unless_negative(atomic_t *v)
 {
-	int v, v1;
-	for (v = 0; v >= 0; v = v1) {
-		v1 = atomic_cmpxchg(p, v, v + 1);
-		if (likely(v1 == v))
-			return true;
-	}
-	return false;
+	int c = atomic_read(v);
+
+	do {
+		if (unlikely(c < 0))
+			return false;
+	} while (!atomic_try_cmpxchg(v, &c, c + 1));
+
+	return true;
 }
 #endif
 
 #ifndef atomic_dec_unless_positive
-static inline bool atomic_dec_unless_positive(atomic_t *p)
+static inline bool atomic_dec_unless_positive(atomic_t *v)
 {
-	int v, v1;
-	for (v = 0; v <= 0; v = v1) {
-		v1 = atomic_cmpxchg(p, v, v - 1);
-		if (likely(v1 == v))
-			return true;
-	}
-	return false;
+	int c = atomic_read(v);
+
+	do {
+		if (unlikely(c > 0))
+			return false;
+	} while (!atomic_try_cmpxchg(v, &c, c - 1));
+
+	return true;
 }
 #endif
 
@@ -609,17 +611,14 @@ static inline bool atomic_dec_unless_positive(atomic_t *p)
 #ifndef atomic_dec_if_positive
 static inline int atomic_dec_if_positive(atomic_t *v)
 {
-	int c, old, dec;
-	c = atomic_read(v);
-	for (;;) {
+	int dec, c = atomic_read(v);
+
+	do {
 		dec = c - 1;
 		if (unlikely(dec < 0))
 			break;
-		old = atomic_cmpxchg((v), c, dec);
-		if (likely(old == c))
-			break;
-		c = old;
-	}
+	} while (!atomic_try_cmpxchg(v, &c, dec));
+
 	return dec;
 }
 #endif
@@ -1046,6 +1045,56 @@ static inline long long atomic64_fetch_andnot_release(long long i, atomic64_t *v
 }
 #endif
 
+#ifndef atomic64_inc_unless_negative
+static inline bool atomic64_inc_unless_negative(atomic64_t *v)
+{
+	long long c = atomic64_read(v);
+
+	do {
+		if (unlikely(c < 0))
+			return false;
+	} while (!atomic64_try_cmpxchg(v, &c, c + 1));
+
+	return true;
+}
+#endif
+
+#ifndef atomic64_dec_unless_positive
+static inline bool atomic64_dec_unless_positive(atomic64_t *v)
+{
+	long long c = atomic64_read(v);
+
+	do {
+		if (unlikely(c > 0))
+			return false;
+	} while (!atomic64_try_cmpxchg(v, &c, c - 1));
+
+	return true;
+}
+#endif
+
+/*
+ * atomic64_dec_if_positive - decrement by 1 if old value positive
+ * @v: pointer of type atomic64_t
+ *
+ * The function returns the old value of *v minus 1, even if
+ * the atomic64 variable, v, was not decremented.
+ */
+#ifndef atomic64_dec_if_positive
+static inline long long atomic64_dec_if_positive(atomic64_t *v)
+{
+	long long dec, c = atomic64_read(v);
+
+	do {
+		dec = c - 1;
+		if (unlikely(dec < 0))
+			break;
+	} while (!atomic64_try_cmpxchg(v, &c, dec));
+
+	return dec;
+}
+#endif
+
 #define atomic64_cond_read_relaxed(v, c)	smp_cond_load_relaxed(&(v)->counter, (c))
 #define atomic64_cond_read_acquire(v, c)	smp_cond_load_acquire(&(v)->counter, (c))
 
