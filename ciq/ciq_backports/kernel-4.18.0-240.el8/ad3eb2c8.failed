io_uring: split overflow state into SQ and CQ side

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit ad3eb2c89fb24d14ac81f43eff8e85fece2c934d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ad3eb2c8.failed

We currently check ->cq_overflow_list from both SQ and CQ context, which
causes some bouncing of that cache line. Add separate bits of state for
this instead, so that the SQ side can check using its own state, and
likewise for the CQ side.

This adds ->sq_check_overflow with the SQ state, and ->cq_check_overflow
with the CQ state. If we hit an overflow condition, both of these bits
are set. Likewise for overflow flush clear, we clear both bits. For the
fast path of just checking if there's an overflow condition on either
the SQ or CQ side, we can use our own private bit for this.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ad3eb2c89fb24d14ac81f43eff8e85fece2c934d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index eb3b77d5111e,dfa99da8b2d1..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -223,13 -222,22 +223,26 @@@ struct io_ring_ctx 
  		unsigned		sq_entries;
  		unsigned		sq_mask;
  		unsigned		sq_thread_idle;
++<<<<<<< HEAD
 +		struct io_uring_sqe	*sq_sqes;
 +
 +		struct list_head	defer_list;
++=======
+ 		unsigned		cached_sq_dropped;
+ 		atomic_t		cached_cq_overflow;
+ 		unsigned long		sq_check_overflow;
+ 
+ 		struct list_head	defer_list;
+ 		struct list_head	timeout_list;
+ 		struct list_head	cq_overflow_list;
+ 
+ 		wait_queue_head_t	inflight_wait;
+ 		struct io_uring_sqe	*sq_sqes;
++>>>>>>> ad3eb2c89fb2 (io_uring: split overflow state into SQ and CQ side)
  	} ____cacheline_aligned_in_smp;
  
 -	struct io_rings	*rings;
 -
  	/* IO offload */
 -	struct io_wq		*io_wq;
 +	struct workqueue_struct	*sqo_wq[2];
  	struct task_struct	*sqo_thread;	/* if using sq thread polling */
  	struct mm_struct	*sqo_mm;
  	wait_queue_head_t	sqo_wait;
@@@ -260,7 -256,28 +273,32 @@@
  
  	struct user_struct	*user;
  
++<<<<<<< HEAD
 +	struct completion	ctx_done;
++=======
+ 	const struct cred	*creds;
+ 
+ 	/* 0 is for ctx quiesce/reinit/free, 1 is for sqo_thread started */
+ 	struct completion	*completions;
+ 
+ 	/* if all else fails... */
+ 	struct io_kiocb		*fallback_req;
+ 
+ #if defined(CONFIG_UNIX)
+ 	struct socket		*ring_sock;
+ #endif
+ 
+ 	struct {
+ 		unsigned		cached_cq_tail;
+ 		unsigned		cq_entries;
+ 		unsigned		cq_mask;
+ 		atomic_t		cq_timeouts;
+ 		unsigned long		cq_check_overflow;
+ 		struct wait_queue_head	cq_wait;
+ 		struct fasync_struct	*cq_fasync;
+ 		struct eventfd_ctx	*cq_ev_fd;
+ 	} ____cacheline_aligned_in_smp;
++>>>>>>> ad3eb2c89fb2 (io_uring: split overflow state into SQ and CQ side)
  
  	struct {
  		struct mutex		uring_lock;
@@@ -551,9 -909,99 +589,102 @@@ static void io_cqring_ev_posted(struct 
  		eventfd_signal(ctx->cq_ev_fd, 1);
  }
  
 -/* Returns true if there are no backlogged entries after the flush */
 -static bool io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force)
 +static void io_cqring_add_event(struct io_ring_ctx *ctx, u64 user_data,
 +				long res)
  {
++<<<<<<< HEAD
++=======
+ 	struct io_rings *rings = ctx->rings;
+ 	struct io_uring_cqe *cqe;
+ 	struct io_kiocb *req;
+ 	unsigned long flags;
+ 	LIST_HEAD(list);
+ 
+ 	if (!force) {
+ 		if (list_empty_careful(&ctx->cq_overflow_list))
+ 			return true;
+ 		if ((ctx->cached_cq_tail - READ_ONCE(rings->cq.head) ==
+ 		    rings->cq_ring_entries))
+ 			return false;
+ 	}
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 
+ 	/* if force is set, the ring is going away. always drop after that */
+ 	if (force)
+ 		ctx->cq_overflow_flushed = true;
+ 
+ 	cqe = NULL;
+ 	while (!list_empty(&ctx->cq_overflow_list)) {
+ 		cqe = io_get_cqring(ctx);
+ 		if (!cqe && !force)
+ 			break;
+ 
+ 		req = list_first_entry(&ctx->cq_overflow_list, struct io_kiocb,
+ 						list);
+ 		list_move(&req->list, &list);
+ 		if (cqe) {
+ 			WRITE_ONCE(cqe->user_data, req->user_data);
+ 			WRITE_ONCE(cqe->res, req->result);
+ 			WRITE_ONCE(cqe->flags, 0);
+ 		} else {
+ 			WRITE_ONCE(ctx->rings->cq_overflow,
+ 				atomic_inc_return(&ctx->cached_cq_overflow));
+ 		}
+ 	}
+ 
+ 	io_commit_cqring(ctx);
+ 	if (cqe) {
+ 		clear_bit(0, &ctx->sq_check_overflow);
+ 		clear_bit(0, &ctx->cq_check_overflow);
+ 	}
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	while (!list_empty(&list)) {
+ 		req = list_first_entry(&list, struct io_kiocb, list);
+ 		list_del(&req->list);
+ 		io_put_req(req);
+ 	}
+ 
+ 	return cqe != NULL;
+ }
+ 
+ static void io_cqring_fill_event(struct io_kiocb *req, long res)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct io_uring_cqe *cqe;
+ 
+ 	trace_io_uring_complete(ctx, req->user_data, res);
+ 
+ 	/*
+ 	 * If we can't get a cq entry, userspace overflowed the
+ 	 * submission (by quite a lot). Increment the overflow count in
+ 	 * the ring.
+ 	 */
+ 	cqe = io_get_cqring(ctx);
+ 	if (likely(cqe)) {
+ 		WRITE_ONCE(cqe->user_data, req->user_data);
+ 		WRITE_ONCE(cqe->res, res);
+ 		WRITE_ONCE(cqe->flags, 0);
+ 	} else if (ctx->cq_overflow_flushed) {
+ 		WRITE_ONCE(ctx->rings->cq_overflow,
+ 				atomic_inc_return(&ctx->cached_cq_overflow));
+ 	} else {
+ 		if (list_empty(&ctx->cq_overflow_list)) {
+ 			set_bit(0, &ctx->sq_check_overflow);
+ 			set_bit(0, &ctx->cq_check_overflow);
+ 		}
+ 		refcount_inc(&req->refs);
+ 		req->result = res;
+ 		list_add_tail(&req->list, &ctx->cq_overflow_list);
+ 	}
+ }
+ 
+ static void io_cqring_add_event(struct io_kiocb *req, long res)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
++>>>>>>> ad3eb2c89fb2 (io_uring: split overflow state into SQ and CQ side)
  	unsigned long flags;
  
  	spin_lock_irqsave(&ctx->completion_lock, flags);
@@@ -696,11 -1273,51 +827,53 @@@ static void io_put_req(struct io_kiocb 
  		io_free_req(req);
  }
  
 -/*
 - * Must only be used if we don't need to care about links, usually from
 - * within the completion handling itself.
 - */
 -static void __io_double_put_req(struct io_kiocb *req)
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
  {
++<<<<<<< HEAD
 +	/* See comment at the top of this file */
 +	smp_rmb();
 +	return READ_ONCE(ring->r.tail) - READ_ONCE(ring->r.head);
++=======
+ 	/* drop both submit and complete references */
+ 	if (refcount_sub_and_test(2, &req->refs))
+ 		__io_free_req(req);
+ }
+ 
+ static void io_double_put_req(struct io_kiocb *req)
+ {
+ 	/* drop both submit and complete references */
+ 	if (refcount_sub_and_test(2, &req->refs))
+ 		io_free_req(req);
+ }
+ 
+ static unsigned io_cqring_events(struct io_ring_ctx *ctx, bool noflush)
+ {
+ 	struct io_rings *rings = ctx->rings;
+ 
+ 	if (test_bit(0, &ctx->cq_check_overflow)) {
+ 		/*
+ 		 * noflush == true is from the waitqueue handler, just ensure
+ 		 * we wake up the task, and the next invocation will flush the
+ 		 * entries. We cannot safely to it from here.
+ 		 */
+ 		if (noflush && !list_empty(&ctx->cq_overflow_list))
+ 			return -1U;
+ 
+ 		io_cqring_overflow_flush(ctx, false);
+ 	}
+ 
+ 	/* See comment at the top of this file */
+ 	smp_rmb();
+ 	return ctx->cached_cq_tail - READ_ONCE(rings->cq.head);
+ }
+ 
+ static inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)
+ {
+ 	struct io_rings *rings = ctx->rings;
+ 
+ 	/* make sure SQ entry isn't read before tail */
+ 	return smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;
++>>>>>>> ad3eb2c89fb2 (io_uring: split overflow state into SQ and CQ side)
  }
  
  /*
@@@ -2429,9 -4317,15 +2602,20 @@@ static int io_submit_sqes(struct io_rin
  {
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
 +	struct io_kiocb *shadow_req = NULL;
 +	bool prev_was_link = false;
  	int i, submitted = 0;
++<<<<<<< HEAD
++=======
+ 	bool mm_fault = false;
+ 
+ 	/* if we have a backlog and couldn't flush it all, return BUSY */
+ 	if (test_bit(0, &ctx->sq_check_overflow)) {
+ 		if (!list_empty(&ctx->cq_overflow_list) &&
+ 		    !io_cqring_overflow_flush(ctx, false))
+ 			return -EBUSY;
+ 	}
++>>>>>>> ad3eb2c89fb2 (io_uring: split overflow state into SQ and CQ side)
  
  	if (nr > IO_PLUG_THRESHOLD) {
  		io_submit_state_start(&state, nr);
* Unmerged path fs/io_uring.c
