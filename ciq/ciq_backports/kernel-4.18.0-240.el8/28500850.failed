blk-mq: always allow reserved allocation in hctx_may_queue

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Ming Lei <ming.lei@redhat.com>
commit 285008501c65a3fcee05d2c2c26cbf629ceff2f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/28500850.failed

NVMe shares tagset between fabric queue and admin queue or between
connect_q and NS queue, so hctx_may_queue() can be called to allocate
request for these queues.

Tags can be reserved in these tagset. Before error recovery, there is
often lots of in-flight requests which can't be completed, and new
reserved request may be needed in error recovery path. However,
hctx_may_queue() can always return false because there is too many
in-flight requests which can't be completed during error handling.
Finally, nothing can proceed.

Fix this issue by always allowing reserved tag allocation in
hctx_may_queue(). This is reasonable because reserved tags are supposed
to always be available.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Hannes Reinecke <hare@suse.de>
	Cc: David Milburn <dmilburn@redhat.com>
	Cc: Ewan D. Milne <emilne@redhat.com>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 285008501c65a3fcee05d2c2c26cbf629ceff2f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-tag.c
#	block/blk-mq.c
diff --cc block/blk-mq-tag.c
index 0086c40c4411,aacf10decdbd..000000000000
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@@ -89,9 -76,10 +89,14 @@@ static inline bool hctx_may_queue(struc
  static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
  			    struct sbitmap_queue *bt)
  {
++<<<<<<< HEAD
 +	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 +	    !hctx_may_queue(data->hctx, bt))
++=======
+ 	if (!data->q->elevator && !(data->flags & BLK_MQ_REQ_RESERVED) &&
+ 			!hctx_may_queue(data->hctx, bt))
++>>>>>>> 285008501c65 (blk-mq: always allow reserved allocation in hctx_may_queue)
  		return BLK_MQ_NO_TAG;
 -
  	if (data->shallow_depth)
  		return __sbitmap_queue_get_shallow(bt, data->shallow_depth);
  	else
diff --cc block/blk-mq.c
index d419198c14e1,e04b759add75..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1020,6 -1094,46 +1020,49 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
++<<<<<<< HEAD
++=======
+ static bool __blk_mq_get_driver_tag(struct request *rq)
+ {
+ 	struct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;
+ 	unsigned int tag_offset = rq->mq_hctx->tags->nr_reserved_tags;
+ 	int tag;
+ 
+ 	blk_mq_tag_busy(rq->mq_hctx);
+ 
+ 	if (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {
+ 		bt = rq->mq_hctx->tags->breserved_tags;
+ 		tag_offset = 0;
+ 	} else {
+ 		if (!hctx_may_queue(rq->mq_hctx, bt))
+ 			return false;
+ 	}
+ 
+ 	tag = __sbitmap_queue_get(bt);
+ 	if (tag == BLK_MQ_NO_TAG)
+ 		return false;
+ 
+ 	rq->tag = tag + tag_offset;
+ 	return true;
+ }
+ 
+ static bool blk_mq_get_driver_tag(struct request *rq)
+ {
+ 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
+ 
+ 	if (rq->tag == BLK_MQ_NO_TAG && !__blk_mq_get_driver_tag(rq))
+ 		return false;
+ 
+ 	if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+ 			!(rq->rq_flags & RQF_MQ_INFLIGHT)) {
+ 		rq->rq_flags |= RQF_MQ_INFLIGHT;
+ 		__blk_mq_inc_active_requests(hctx);
+ 	}
+ 	hctx->tags->rqs[rq->tag] = rq;
+ 	return true;
+ }
+ 
++>>>>>>> 285008501c65 (blk-mq: always allow reserved allocation in hctx_may_queue)
  static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
  				int flags, void *key)
  {
* Unmerged path block/blk-mq-tag.c
* Unmerged path block/blk-mq.c
