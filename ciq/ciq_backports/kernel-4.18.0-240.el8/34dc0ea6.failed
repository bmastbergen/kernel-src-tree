dma-direct: provide mmap and get_sgtable method overrides

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 34dc0ea6bc960f1f57b2148f01a3f4da23f87013
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/34dc0ea6.failed

For dma-direct we know that the DMA address is an encoding of the
physical address that we can trivially decode.  Use that fact to
provide implementations that do not need the arch_dma_coherent_to_pfn
architecture hook.  Note that we still can only support mmap of
non-coherent memory only if the architecture provides a way to set an
uncached bit in the page tables.  This must be true for architectures
that use the generic remap helpers, but other architectures can also
manually select it.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>
(cherry picked from commit 34dc0ea6bc960f1f57b2148f01a3f4da23f87013)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/Kconfig
#	arch/arm/Kconfig
#	arch/arm/mm/dma-mapping.c
#	arch/arm64/Kconfig
#	arch/ia64/Kconfig
#	arch/ia64/kernel/dma-mapping.c
#	arch/microblaze/Kconfig
#	arch/mips/Kconfig
#	arch/mips/mm/dma-noncoherent.c
#	arch/powerpc/platforms/Kconfig.cputype
#	include/linux/dma-noncoherent.h
#	kernel/dma/Kconfig
#	kernel/dma/direct.c
#	kernel/dma/mapping.c
diff --cc arch/arc/Kconfig
index 753b0cdde165,4d7b671c8ff4..000000000000
--- a/arch/arc/Kconfig
+++ b/arch/arc/Kconfig
@@@ -9,10 -6,13 +9,16 @@@
  config ARC
  	def_bool y
  	select ARC_TIMERS
++<<<<<<< HEAD
++=======
+ 	select ARCH_HAS_DMA_PREP_COHERENT
+ 	select ARCH_HAS_PTE_SPECIAL
+ 	select ARCH_HAS_SETUP_DMA_OPS
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	select ARCH_HAS_SYNC_DMA_FOR_CPU
  	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
 +	select ARCH_HAS_SG_CHAIN
  	select ARCH_SUPPORTS_ATOMIC_RMW if ARC_HAS_LLSC
 -	select ARCH_32BIT_OFF_T
  	select BUILDTIME_EXTABLE_SORT
  	select CLONE_BACKWARDS
  	select COMMON_CLK
diff --cc arch/arm/Kconfig
index c23576a9a9d8,80e795aacd3a..000000000000
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@@ -2,15 -2,20 +2,19 @@@
  config ARM
  	bool
  	default y
 -	select ARCH_32BIT_OFF_T
  	select ARCH_CLOCKSOURCE_DATA
 -	select ARCH_HAS_BINFMT_FLAT
 +	select ARCH_DISCARD_MEMBLOCK if !HAVE_ARCH_PFN_VALID && !KEXEC
  	select ARCH_HAS_DEBUG_VIRTUAL if MMU
  	select ARCH_HAS_DEVMEM_IS_ALLOWED
++<<<<<<< HEAD
++=======
+ 	select ARCH_HAS_DMA_WRITE_COMBINE if !ARM_DMA_MEM_BUFFERABLE
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	select ARCH_HAS_ELF_RANDOMIZE
  	select ARCH_HAS_FORTIFY_SOURCE
 -	select ARCH_HAS_KEEPINITRD
  	select ARCH_HAS_KCOV
 -	select ARCH_HAS_MEMBARRIER_SYNC_CORE
  	select ARCH_HAS_PTE_SPECIAL if ARM_LPAE
  	select ARCH_HAS_PHYS_TO_DMA
 -	select ARCH_HAS_SETUP_DMA_OPS
  	select ARCH_HAS_SET_MEMORY
  	select ARCH_HAS_STRICT_KERNEL_RWX if MMU && !XIP_KERNEL
  	select ARCH_HAS_STRICT_MODULE_RWX if MMU
diff --cc arch/arm/mm/dma-mapping.c
index 796ffc05b4d3,f3cbeba7f9cb..000000000000
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@@ -2383,4 -2327,36 +2383,37 @@@ void arch_teardown_dma_ops(struct devic
  		return;
  
  	arm_teardown_iommu_dma_ops(dev);
 -	/* Let arch_setup_dma_ops() start again from scratch upon re-probe */
 -	set_dma_ops(dev, NULL);
  }
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_SWIOTLB
+ void arch_sync_dma_for_device(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_page_cpu_to_dev(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),
+ 			      size, dir);
+ }
+ 
+ void arch_sync_dma_for_cpu(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_page_dev_to_cpu(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),
+ 			      size, dir);
+ }
+ 
+ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t gfp, unsigned long attrs)
+ {
+ 	return __dma_alloc(dev, size, dma_handle, gfp,
+ 			   __get_dma_pgprot(attrs, PAGE_KERNEL), false,
+ 			   attrs, __builtin_return_address(0));
+ }
+ 
+ void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ 	__arm_dma_free(dev, size, cpu_addr, dma_handle, attrs, false);
+ }
+ #endif /* CONFIG_SWIOTLB */
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
diff --cc arch/arm64/Kconfig
index 347f141befab,57606307fe34..000000000000
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@@ -11,11 -12,8 +11,14 @@@ config ARM6
  	select ARCH_CLOCKSOURCE_DATA
  	select ARCH_HAS_DEBUG_VIRTUAL
  	select ARCH_HAS_DEVMEM_IS_ALLOWED
++<<<<<<< HEAD
 +	select ARCH_HAS_DMA_COHERENT_TO_PFN
 +	select ARCH_HAS_DMA_MMAP_PGPROT
++=======
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	select ARCH_HAS_DMA_PREP_COHERENT
  	select ARCH_HAS_ACPI_TABLE_UPGRADE if ACPI
 +	select ARCH_HAS_ELF_RANDOMIZE
  	select ARCH_HAS_FAST_MULTIPLIER
  	select ARCH_HAS_FORTIFY_SOURCE
  	select ARCH_HAS_GCOV_PROFILE_ALL
diff --cc arch/ia64/Kconfig
index d45b2b4bf85b,bab7cd878464..000000000000
--- a/arch/ia64/Kconfig
+++ b/arch/ia64/Kconfig
@@@ -30,13 -31,11 +30,18 @@@ config IA6
  	select HAVE_FUNCTION_TRACER
  	select TTY
  	select HAVE_ARCH_TRACEHOOK
 +	select HAVE_MEMBLOCK
  	select HAVE_MEMBLOCK_NODE_MAP
  	select HAVE_VIRT_CPU_ACCOUNTING
++<<<<<<< HEAD
 +	select ARCH_HAS_DMA_MARK_CLEAN
 +	select ARCH_HAS_SG_CHAIN
++=======
+ 	select DMA_NONCOHERENT_MMAP
+ 	select ARCH_HAS_SYNC_DMA_FOR_CPU
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	select VIRT_TO_BUS
 +	select ARCH_DISCARD_MEMBLOCK
  	select GENERIC_IRQ_PROBE
  	select GENERIC_PENDING_IRQ if SMP
  	select GENERIC_IRQ_SHOW
diff --cc arch/ia64/kernel/dma-mapping.c
index 7a471d8d67d4,09ef9ce9988d..000000000000
--- a/arch/ia64/kernel/dma-mapping.c
+++ b/arch/ia64/kernel/dma-mapping.c
@@@ -9,16 -8,14 +9,21 @@@ int iommu_detected __read_mostly
  const struct dma_map_ops *dma_ops;
  EXPORT_SYMBOL(dma_ops);
  
 -void *arch_dma_alloc(struct device *dev, size_t size,
 -		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
 +const struct dma_map_ops *dma_get_ops(struct device *dev)
  {
 -	return dma_direct_alloc_pages(dev, size, dma_handle, gfp, attrs);
 +	return dma_ops;
  }
 +EXPORT_SYMBOL(dma_get_ops);
  
 -void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
 -		dma_addr_t dma_addr, unsigned long attrs)
 +#ifdef CONFIG_SWIOTLB
 +void __init swiotlb_dma_init(void)
  {
++<<<<<<< HEAD
 +	dma_ops = &swiotlb_dma_ops;
 +	swiotlb_init(1);
 +}
 +#endif
++=======
+ 	dma_direct_free_pages(dev, size, cpu_addr, dma_addr, attrs);
+ }
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
diff --cc arch/microblaze/Kconfig
index 3218a8b35f9c,261c26df1c9f..000000000000
--- a/arch/microblaze/Kconfig
+++ b/arch/microblaze/Kconfig
@@@ -1,8 -1,15 +1,15 @@@
 -# SPDX-License-Identifier: GPL-2.0-only
  config MICROBLAZE
  	def_bool y
++<<<<<<< HEAD
++=======
+ 	select ARCH_32BIT_OFF_T
+ 	select ARCH_NO_SWAP
+ 	select ARCH_HAS_BINFMT_FLAT if !MMU
+ 	select ARCH_HAS_DMA_PREP_COHERENT
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	select ARCH_HAS_GCOV_PROFILE_ALL
 -	select ARCH_HAS_SYNC_DMA_FOR_CPU
 -	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
 -	select ARCH_HAS_UNCACHED_SEGMENT if !MMU
  	select ARCH_MIGHT_HAVE_PC_PARPORT
 +	select ARCH_NO_COHERENT_DMA_MMAP if !MMU
  	select ARCH_WANT_IPC_PARSE_VERSION
  	select BUILDTIME_EXTABLE_SORT
  	select TIMER_OF
diff --cc arch/mips/Kconfig
index 29f3ec40278f,248d39b8a160..000000000000
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@@ -1116,6 -1124,18 +1116,21 @@@ config DMA_COHEREN
  
  config DMA_NONCOHERENT
  	bool
++<<<<<<< HEAD
++=======
+ 	#
+ 	# MIPS allows mixing "slightly different" Cacheability and Coherency
+ 	# Attribute bits.  It is believed that the uncached access through
+ 	# KSEG1 and the implementation specific "uncached accelerated" used
+ 	# by pgprot_writcombine can be mixed, and the latter sometimes provides
+ 	# significant advantages.
+ 	#
+ 	select ARCH_HAS_DMA_WRITE_COMBINE
+ 	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
+ 	select ARCH_HAS_UNCACHED_SEGMENT
+ 	select DMA_NONCOHERENT_MMAP
+ 	select DMA_NONCOHERENT_CACHE_SYNC
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	select NEED_DMA_MAP_STATE
  
  config SYS_HAS_EARLY_PRINTK
diff --cc arch/powerpc/platforms/Kconfig.cputype
index 42fa065292ba,303752f97c19..000000000000
--- a/arch/powerpc/platforms/Kconfig.cputype
+++ b/arch/powerpc/platforms/Kconfig.cputype
@@@ -385,9 -459,10 +385,13 @@@ config NOT_COHERENT_CACH
  	bool
  	depends on 4xx || PPC_8xx || E200 || PPC_MPC512x || \
  		GAMECUBE_COMMON || AMIGAONE
++<<<<<<< HEAD
 +	select ARCH_HAS_DMA_COHERENT_TO_PFN
++=======
+ 	select ARCH_HAS_DMA_PREP_COHERENT
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
  	select ARCH_HAS_SYNC_DMA_FOR_CPU
 -	select DMA_DIRECT_REMAP
  	default n if PPC_47x
  	default y
  
diff --cc include/linux/dma-noncoherent.h
index 029424e95b2b,e30fca1f1b12..000000000000
--- a/include/linux/dma-noncoherent.h
+++ b/include/linux/dma-noncoherent.h
@@@ -40,12 -41,18 +40,15 @@@ void *arch_dma_alloc(struct device *dev
  		gfp_t gfp, unsigned long attrs);
  void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
  		dma_addr_t dma_addr, unsigned long attrs);
++<<<<<<< HEAD
 +long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
 +		dma_addr_t dma_addr);
 +pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
 +		unsigned long attrs);
++=======
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  
  #ifdef CONFIG_MMU
 -/*
 - * Page protection so that devices that can't snoop CPU caches can use the
 - * memory coherently.  We default to pgprot_noncached which is usually used
 - * for ioremap as a safe bet, but architectures can override this with less
 - * strict semantics if possible.
 - */
 -#ifndef pgprot_dmacoherent
 -#define pgprot_dmacoherent(prot)	pgprot_noncached(prot)
 -#endif
 -
  pgprot_t dma_pgprot(struct device *dev, pgprot_t prot, unsigned long attrs);
  #else
  static inline pgprot_t dma_pgprot(struct device *dev, pgprot_t prot,
diff --cc kernel/dma/Kconfig
index 26366e0dab12,4c103a24e380..000000000000
--- a/kernel/dma/Kconfig
+++ b/kernel/dma/Kconfig
@@@ -38,12 -51,6 +38,15 @@@ config ARCH_HAS_SYNC_DMA_FOR_CPU_AL
  config ARCH_HAS_DMA_PREP_COHERENT
  	bool
  
++<<<<<<< HEAD
 +config ARCH_HAS_DMA_COHERENT_TO_PFN
 +	bool
 +
 +config ARCH_HAS_DMA_MMAP_PGPROT
 +	bool
 +
++=======
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  config ARCH_HAS_FORCE_DMA_UNENCRYPTED
  	bool
  
diff --cc kernel/dma/direct.c
index f4581748eeeb,58beaa9ddd27..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -365,6 -371,73 +371,76 @@@ out_unmap
  }
  EXPORT_SYMBOL(dma_direct_map_sg);
  
++<<<<<<< HEAD
++=======
+ dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	dma_addr_t dma_addr = paddr;
+ 
+ 	if (unlikely(!dma_direct_possible(dev, dma_addr, size))) {
+ 		report_addr(dev, dma_addr, size);
+ 		return DMA_MAPPING_ERROR;
+ 	}
+ 
+ 	return dma_addr;
+ }
+ EXPORT_SYMBOL(dma_direct_map_resource);
+ 
+ int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	struct page *page = dma_direct_to_page(dev, dma_addr);
+ 	int ret;
+ 
+ 	ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+ 	if (!ret)
+ 		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_MMU
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return dev_is_dma_coherent(dev) ||
+ 		IS_ENABLED(CONFIG_DMA_NONCOHERENT_MMAP);
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	unsigned long user_count = vma_pages(vma);
+ 	unsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	unsigned long pfn = PHYS_PFN(dma_to_phys(dev, dma_addr));
+ 	int ret = -ENXIO;
+ 
+ 	vma->vm_page_prot = dma_pgprot(dev, vma->vm_page_prot, attrs);
+ 
+ 	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &ret))
+ 		return ret;
+ 
+ 	if (vma->vm_pgoff >= count || user_count > count - vma->vm_pgoff)
+ 		return -ENXIO;
+ 	return remap_pfn_range(vma, vma->vm_start, pfn + vma->vm_pgoff,
+ 			user_count << PAGE_SHIFT, vma->vm_page_prot);
+ }
+ #else /* CONFIG_MMU */
+ bool dma_direct_can_mmap(struct device *dev)
+ {
+ 	return false;
+ }
+ 
+ int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+ 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 		unsigned long attrs)
+ {
+ 	return -ENXIO;
+ }
+ #endif /* CONFIG_MMU */
+ 
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  /*
   * Because 32-bit DMA masks are so common we expect every architecture to be
   * able to satisfy them - either by not supporting more physical memory, or by
diff --cc kernel/dma/mapping.c
index 423189f01982,12ff766ec1fa..000000000000
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@@ -153,11 -138,12 +138,20 @@@ int dma_get_sgtable_attrs(struct devic
  {
  	const struct dma_map_ops *ops = get_dma_ops(dev);
  
++<<<<<<< HEAD
 +	if (!dma_is_direct(ops) && ops->get_sgtable)
 +		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
 +					attrs);
 +	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
 +			attrs);
++=======
+ 	if (dma_is_direct(ops))
+ 		return dma_direct_get_sgtable(dev, sgt, cpu_addr, dma_addr,
+ 				size, attrs);
+ 	if (!ops->get_sgtable)
+ 		return -ENXIO;
+ 	return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size, attrs);
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  }
  EXPORT_SYMBOL(dma_get_sgtable_attrs);
  
@@@ -230,14 -206,8 +212,19 @@@ bool dma_can_mmap(struct device *dev
  {
  	const struct dma_map_ops *ops = get_dma_ops(dev);
  
++<<<<<<< HEAD
 +	if (IS_ENABLED(CONFIG_ARCH_NO_COHERENT_DMA_MMAP))
 +		return false;
 +
 +	if (dma_is_direct(ops)) {
 +		return dev_is_dma_coherent(dev) ||
 +			IS_ENABLED(CONFIG_ARCH_HAS_DMA_COHERENT_TO_PFN);
 +	}
 +
++=======
+ 	if (dma_is_direct(ops))
+ 		return dma_direct_can_mmap(dev);
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  	return ops->mmap != NULL;
  }
  EXPORT_SYMBOL_GPL(dma_can_mmap);
@@@ -261,9 -231,12 +248,18 @@@ int dma_mmap_attrs(struct device *dev, 
  {
  	const struct dma_map_ops *ops = get_dma_ops(dev);
  
++<<<<<<< HEAD
 +	if (!dma_is_direct(ops) && ops->mmap)
 +		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 +	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
++=======
+ 	if (dma_is_direct(ops))
+ 		return dma_direct_mmap(dev, vma, cpu_addr, dma_addr, size,
+ 				attrs);
+ 	if (!ops->mmap)
+ 		return -ENXIO;
+ 	return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
++>>>>>>> 34dc0ea6bc96 (dma-direct: provide mmap and get_sgtable method overrides)
  }
  EXPORT_SYMBOL(dma_mmap_attrs);
  
* Unmerged path arch/mips/mm/dma-noncoherent.c
* Unmerged path arch/arc/Kconfig
* Unmerged path arch/arm/Kconfig
* Unmerged path arch/arm/mm/dma-mapping.c
* Unmerged path arch/arm64/Kconfig
* Unmerged path arch/ia64/Kconfig
* Unmerged path arch/ia64/kernel/dma-mapping.c
* Unmerged path arch/microblaze/Kconfig
* Unmerged path arch/mips/Kconfig
* Unmerged path arch/mips/mm/dma-noncoherent.c
* Unmerged path arch/powerpc/platforms/Kconfig.cputype
diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 3238177e65ad..6db863c3eb93 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -76,5 +76,12 @@ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
 struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 		gfp_t gfp, unsigned long attrs);
+int dma_direct_get_sgtable(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
+bool dma_direct_can_mmap(struct device *dev);
+int dma_direct_mmap(struct device *dev, struct vm_area_struct *vma,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
 int dma_direct_supported(struct device *dev, u64 mask);
 #endif /* _LINUX_DMA_DIRECT_H */
* Unmerged path include/linux/dma-noncoherent.h
* Unmerged path kernel/dma/Kconfig
* Unmerged path kernel/dma/direct.c
* Unmerged path kernel/dma/mapping.c
diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index eaac29ecf933..a0bf48bfac16 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -242,10 +242,4 @@ void arch_dma_free(struct device *dev, size_t size, void *vaddr,
 		dma_free_contiguous(dev, page, size);
 	}
 }
-
-long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
-		dma_addr_t dma_addr)
-{
-	return __phys_to_pfn(dma_to_phys(dev, dma_addr));
-}
 #endif /* CONFIG_DMA_DIRECT_REMAP */
