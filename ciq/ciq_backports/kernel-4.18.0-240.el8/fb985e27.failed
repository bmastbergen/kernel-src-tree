RDMA/mlx5: Use SRCU properly in ODP prefetch

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit fb985e278a30224183fdf3d56e2f69cfdef88d4e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/fb985e27.failed

When working with SRCU protected xarrays the xarray itself should be the
SRCU 'update' point. Instead prefetch is using live as the SRCU update
point and this prevents switching the locking design to use the xarray
instead.

To solve this the prefetch must only read from the xarray once, and hold
on to the actual MR pointer for the duration of the async
operation. Incrementing num_pending_prefetch delays destruction of the MR,
so it is suitable.

Prefetch calls directly to the pagefault_mr using the MR pointer and only
does a single xarray lookup.

All the testing if a MR is prefetchable or not is now done only in the
prefetch code and removed from the pagefault critical path.

Link: https://lore.kernel.org/r/20191009160934.3143-2-jgg@ziepe.ca
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit fb985e278a30224183fdf3d56e2f69cfdef88d4e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/odp.c
index df3038ca913b,09dac97e4ca4..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -612,16 -606,13 +612,17 @@@ void mlx5_ib_free_implicit_mr(struct ml
  	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
  }
  
- #define MLX5_PF_FLAGS_PREFETCH  BIT(0)
  #define MLX5_PF_FLAGS_DOWNGRADE BIT(1)
- static int pagefault_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
- 			u64 io_virt, size_t bcnt, u32 *bytes_mapped,
- 			u32 flags)
+ static int pagefault_mr(struct mlx5_ib_mr *mr, u64 io_virt, size_t bcnt,
+ 			u32 *bytes_mapped, u32 flags)
  {
 -	int npages = 0, current_seq, page_shift, ret, np;
  	struct ib_umem_odp *odp_mr = to_ib_umem_odp(mr->umem);
  	bool downgrade = flags & MLX5_PF_FLAGS_DOWNGRADE;
++<<<<<<< HEAD
 +	bool prefetch = flags & MLX5_PF_FLAGS_PREFETCH;
 +	int npages = 0, current_seq, page_shift, ret, np;
++=======
++>>>>>>> fb985e278a30 (RDMA/mlx5: Use SRCU properly in ODP prefetch)
  	u64 access_mask;
  	u64 start_idx, page_mask;
  	struct ib_umem_odp *odp;
@@@ -1636,114 -1596,138 +1612,183 @@@ int mlx5_ib_odp_init(void
  
  struct prefetch_mr_work {
  	struct work_struct work;
- 	struct ib_pd *pd;
  	u32 pf_flags;
  	u32 num_sge;
- 	struct ib_sge sg_list[0];
+ 	struct {
+ 		u64 io_virt;
+ 		struct mlx5_ib_mr *mr;
+ 		size_t length;
+ 	} frags[];
  };
  
- static void num_pending_prefetch_dec(struct mlx5_ib_dev *dev,
- 				     struct ib_sge *sg_list, u32 num_sge,
- 				     u32 from)
+ static void destroy_prefetch_work(struct prefetch_mr_work *work)
  {
  	u32 i;
- 	int srcu_key;
  
- 	srcu_key = srcu_read_lock(&dev->mr_srcu);
+ 	for (i = 0; i < work->num_sge; ++i)
+ 		atomic_dec(&work->frags[i].mr->num_pending_prefetch);
+ 	kvfree(work);
+ }
  
- 	for (i = from; i < num_sge; ++i) {
- 		struct mlx5_core_mkey *mmkey;
- 		struct mlx5_ib_mr *mr;
+ static struct mlx5_ib_mr *
+ get_prefetchable_mr(struct ib_pd *pd, enum ib_uverbs_advise_mr_advice advice,
+ 		    u32 lkey)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct mlx5_core_mkey *mmkey;
+ 	struct ib_umem_odp *odp;
+ 	struct mlx5_ib_mr *mr;
  
- 		mmkey = xa_load(&dev->mdev->priv.mkey_table,
- 				mlx5_base_mkey(sg_list[i].lkey));
- 		mr = container_of(mmkey, struct mlx5_ib_mr, mmkey);
- 		atomic_dec(&mr->num_pending_prefetch);
- 	}
+ 	lockdep_assert_held(&dev->mr_srcu);
  
- 	srcu_read_unlock(&dev->mr_srcu, srcu_key);
+ 	mmkey = xa_load(&dev->mdev->priv.mkey_table, mlx5_base_mkey(lkey));
+ 	if (!mmkey || mmkey->key != lkey || mmkey->type != MLX5_MKEY_MR)
+ 		return NULL;
+ 
+ 	mr = container_of(mmkey, struct mlx5_ib_mr, mmkey);
+ 
+ 	if (!smp_load_acquire(&mr->live))
+ 		return NULL;
+ 
+ 	if (mr->ibmr.pd != pd || !is_odp_mr(mr))
+ 		return NULL;
+ 
+ 	/*
+ 	 * Implicit child MRs are internal and userspace should not refer to
+ 	 * them.
+ 	 */
+ 	if (mr->parent)
+ 		return NULL;
+ 
+ 	odp = to_ib_umem_odp(mr->umem);
+ 
+ 	/* prefetch with write-access must be supported by the MR */
+ 	if (advice == IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH_WRITE &&
+ 	    !odp->umem.writable)
+ 		return NULL;
+ 
+ 	return mr;
+ }
+ 
+ static void mlx5_ib_prefetch_mr_work(struct work_struct *w)
+ {
+ 	struct prefetch_mr_work *work =
+ 		container_of(w, struct prefetch_mr_work, work);
+ 	u32 bytes_mapped = 0;
+ 	u32 i;
+ 
+ 	for (i = 0; i < work->num_sge; ++i)
+ 		pagefault_mr(work->frags[i].mr, work->frags[i].io_virt,
+ 			     work->frags[i].length, &bytes_mapped,
+ 			     work->pf_flags);
+ 
+ 	destroy_prefetch_work(work);
  }
  
- static bool num_pending_prefetch_inc(struct ib_pd *pd,
- 				     struct ib_sge *sg_list, u32 num_sge)
+ static bool init_prefetch_work(struct ib_pd *pd,
+ 			       enum ib_uverbs_advise_mr_advice advice,
+ 			       u32 pf_flags, struct prefetch_mr_work *work,
+ 			       struct ib_sge *sg_list, u32 num_sge)
+ {
+ 	u32 i;
+ 
+ 	INIT_WORK(&work->work, mlx5_ib_prefetch_mr_work);
+ 	work->pf_flags = pf_flags;
+ 
+ 	for (i = 0; i < num_sge; ++i) {
+ 		work->frags[i].io_virt = sg_list[i].addr;
+ 		work->frags[i].length = sg_list[i].length;
+ 		work->frags[i].mr =
+ 			get_prefetchable_mr(pd, advice, sg_list[i].lkey);
+ 		if (!work->frags[i].mr) {
+ 			work->num_sge = i - 1;
+ 			if (i)
+ 				destroy_prefetch_work(work);
+ 			return false;
+ 		}
+ 
+ 		/* Keep the MR pointer will valid outside the SRCU */
+ 		atomic_inc(&work->frags[i].mr->num_pending_prefetch);
+ 	}
+ 	work->num_sge = num_sge;
+ 	return true;
+ }
+ 
+ static int mlx5_ib_prefetch_sg_list(struct ib_pd *pd,
+ 				    enum ib_uverbs_advise_mr_advice advice,
+ 				    u32 pf_flags, struct ib_sge *sg_list,
+ 				    u32 num_sge)
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
- 	bool ret = true;
+ 	u32 bytes_mapped = 0;
+ 	int srcu_key;
+ 	int ret = 0;
  	u32 i;
  
+ 	srcu_key = srcu_read_lock(&dev->mr_srcu);
  	for (i = 0; i < num_sge; ++i) {
- 		struct mlx5_core_mkey *mmkey;
  		struct mlx5_ib_mr *mr;
  
- 		mmkey = xa_load(&dev->mdev->priv.mkey_table,
- 				mlx5_base_mkey(sg_list[i].lkey));
- 		if (!mmkey || mmkey->key != sg_list[i].lkey) {
- 			ret = false;
- 			break;
+ 		mr = get_prefetchable_mr(pd, advice, sg_list[i].lkey);
+ 		if (!mr) {
+ 			ret = -ENOENT;
+ 			goto out;
  		}
++<<<<<<< HEAD
 +
 +		if (mmkey->type != MLX5_MKEY_MR) {
 +			ret = false;
 +			break;
 +		}
 +
 +		mr = container_of(mmkey, struct mlx5_ib_mr, mmkey);
 +
 +		if (mr->ibmr.pd != pd) {
 +			ret = false;
 +			break;
 +		}
 +
 +		if (!mr->live) {
 +			ret = false;
 +			break;
 +		}
 +
 +		atomic_inc(&mr->num_pending_prefetch);
 +	}
 +
 +	if (!ret)
 +		num_pending_prefetch_dec(dev, sg_list, i, 0);
 +
 +	return ret;
 +}
 +
 +static int mlx5_ib_prefetch_sg_list(struct ib_pd *pd, u32 pf_flags,
 +				    struct ib_sge *sg_list, u32 num_sge)
 +{
 +	u32 i;
 +	int ret = 0;
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +
 +	for (i = 0; i < num_sge; ++i) {
 +		struct ib_sge *sg = &sg_list[i];
 +		int bytes_committed = 0;
 +
 +		ret = pagefault_single_data_segment(dev, pd, sg->lkey, sg->addr,
 +						    sg->length,
 +						    &bytes_committed, NULL,
 +						    pf_flags);
++=======
+ 		ret = pagefault_mr(mr, sg_list[i].addr, sg_list[i].length,
+ 				   &bytes_mapped, pf_flags);
++>>>>>>> fb985e278a30 (RDMA/mlx5: Use SRCU properly in ODP prefetch)
  		if (ret < 0)
- 			break;
- 	}
- 
- 	return ret < 0 ? ret : 0;
- }
- 
- static void mlx5_ib_prefetch_mr_work(struct work_struct *work)
- {
- 	struct prefetch_mr_work *w =
- 		container_of(work, struct prefetch_mr_work, work);
- 
- 	if (ib_device_try_get(w->pd->device)) {
- 		mlx5_ib_prefetch_sg_list(w->pd, w->pf_flags, w->sg_list,
- 					 w->num_sge);
- 		ib_device_put(w->pd->device);
+ 			goto out;
  	}
+ 	ret = 0;
  
- 	num_pending_prefetch_dec(to_mdev(w->pd->device), w->sg_list,
- 				 w->num_sge, 0);
- 	kvfree(w);
+ out:
+ 	srcu_read_unlock(&dev->mr_srcu, srcu_key);
+ 	return ret;
  }
  
  int mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
