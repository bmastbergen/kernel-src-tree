iommu/vt-d: Apply per-device dma_ops

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Lu Baolu <baolu.lu@linux.intel.com>
commit 6fc7020cf298aaec343df423746b44d99c6efaa5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6fc7020c.failed

Current Intel IOMMU driver sets the system level dma_ops. This causes
each dma API to go through the IOMMU driver even the devices are using
identity mapped domains. This sets per-device dma_ops only if a device
is using a DMA domain. Otherwise, use the default system level dma_ops
for direct dma.

	Signed-off-by: Lu Baolu <baolu.lu@linux.intel.com>
	Tested-by: Daniel Drake <drake@endlessm.com>
	Reviewed-by: Jon Derrick <jonathan.derrick@intel.com>
	Reviewed-by: Jerry Snitselaar <jsnitsel@redhat.com>
Link: https://lore.kernel.org/r/20200506015947.28662-4-baolu.lu@linux.intel.com
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 6fc7020cf298aaec343df423746b44d99c6efaa5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/intel-iommu.c
diff --cc drivers/iommu/intel-iommu.c
index e339a5e3c53b,29d3940847d3..000000000000
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@@ -3380,100 -3304,6 +3369,103 @@@ static unsigned long intel_alloc_iova(s
  	return iova_pfn;
  }
  
++<<<<<<< HEAD
 +static struct dmar_domain *get_private_domain_for_dev(struct device *dev)
 +{
 +	struct dmar_domain *domain, *tmp;
 +	struct dmar_rmrr_unit *rmrr;
 +	struct device *i_dev;
 +	int i, ret;
 +
 +	/* Device shouldn't be attached by any domains. */
 +	domain = find_domain(dev);
 +	if (domain)
 +		return NULL;
 +
 +	domain = find_or_alloc_domain(dev, DEFAULT_DOMAIN_ADDRESS_WIDTH);
 +	if (!domain)
 +		goto out;
 +
 +	/* We have a new domain - setup possible RMRRs for the device */
 +	rcu_read_lock();
 +	for_each_rmrr_units(rmrr) {
 +		for_each_active_dev_scope(rmrr->devices, rmrr->devices_cnt,
 +					  i, i_dev) {
 +			if (i_dev != dev)
 +				continue;
 +
 +			ret = domain_prepare_identity_map(dev, domain,
 +							  rmrr->base_address,
 +							  rmrr->end_address);
 +			if (ret)
 +				dev_err(dev, "Mapping reserved region failed\n");
 +		}
 +	}
 +	rcu_read_unlock();
 +
 +	tmp = set_domain_for_dev(dev, domain);
 +	if (!tmp || domain != tmp) {
 +		domain_exit(domain);
 +		domain = tmp;
 +	}
 +
 +out:
 +	if (!domain)
 +		dev_err(dev, "Allocating domain failed\n");
 +	else
 +		domain->domain.type = IOMMU_DOMAIN_DMA;
 +
 +	return domain;
 +}
 +
 +/* Check if the dev needs to go through non-identity map and unmap process.*/
 +static bool iommu_need_mapping(struct device *dev)
 +{
 +	int ret;
 +
 +	if (iommu_dummy(dev))
 +		return false;
 +
 +	if (unlikely(attach_deferred(dev)))
 +		do_deferred_attach(dev);
 +
 +	ret = identity_mapping(dev);
 +	if (ret) {
 +		u64 dma_mask = *dev->dma_mask;
 +
 +		if (dev->coherent_dma_mask && dev->coherent_dma_mask < dma_mask)
 +			dma_mask = dev->coherent_dma_mask;
 +
 +		if (dma_mask >= dma_get_required_mask(dev))
 +			return false;
 +
 +		/*
 +		 * 32 bit DMA is removed from si_domain and fall back to
 +		 * non-identity mapping.
 +		 */
 +		dmar_remove_one_dev_info(dev);
 +		ret = iommu_request_dma_domain_for_dev(dev);
 +		if (ret) {
 +			struct iommu_domain *domain;
 +			struct dmar_domain *dmar_domain;
 +
 +			domain = iommu_get_domain_for_dev(dev);
 +			if (domain) {
 +				dmar_domain = to_dmar_domain(domain);
 +				dmar_domain->flags |= DOMAIN_FLAG_LOSE_CHILDREN;
 +			}
 +			dmar_remove_one_dev_info(dev);
 +			get_private_domain_for_dev(dev);
 +		}
 +
 +		dev_info(dev, "32bit DMA uses non-identity mapping\n");
 +	}
 +
 +	return true;
 +}
 +
++=======
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  static dma_addr_t __intel_map_single(struct device *dev, phys_addr_t paddr,
  				     size_t size, int dir, u64 dma_mask)
  {
@@@ -3487,8 -3317,8 +3479,13 @@@
  
  	BUG_ON(dir == DMA_NONE);
  
++<<<<<<< HEAD
 +	if (!iommu_need_mapping(dev))
 +		return paddr;
++=======
+ 	if (unlikely(attach_deferred(dev)))
+ 		do_deferred_attach(dev);
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  
  	domain = find_domain(dev);
  	if (!domain)
@@@ -3541,8 -3371,8 +3538,13 @@@ static dma_addr_t intel_map_page(struc
  				 enum dma_data_direction dir,
  				 unsigned long attrs)
  {
++<<<<<<< HEAD
 +	return __intel_map_single(dev, page_to_phys(page) + offset, size,
 +				  dir, *dev->dma_mask);
++=======
+ 	return __intel_map_single(dev, page_to_phys(page) + offset,
+ 				  size, dir, *dev->dma_mask);
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  }
  
  static dma_addr_t intel_map_resource(struct device *dev, phys_addr_t phys_addr,
@@@ -3604,6 -3431,12 +3606,15 @@@ static void intel_unmap_page(struct dev
  			     unsigned long attrs)
  {
  	intel_unmap(dev, dev_addr, size);
++<<<<<<< HEAD
++=======
+ }
+ 
+ static void intel_unmap_resource(struct device *dev, dma_addr_t dev_addr,
+ 		size_t size, enum dma_data_direction dir, unsigned long attrs)
+ {
+ 	intel_unmap(dev, dev_addr, size);
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  }
  
  static void *intel_alloc_coherent(struct device *dev, size_t size,
@@@ -3613,6 -3446,9 +3624,12 @@@
  	struct page *page = NULL;
  	int order;
  
++<<<<<<< HEAD
++=======
+ 	if (unlikely(attach_deferred(dev)))
+ 		do_deferred_attach(dev);
+ 
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  	size = PAGE_ALIGN(size);
  	order = get_order(size);
  
@@@ -3714,8 -3522,9 +3731,14 @@@ static int intel_map_sg(struct device *
  	struct intel_iommu *iommu;
  
  	BUG_ON(dir == DMA_NONE);
++<<<<<<< HEAD
 +	if (!iommu_need_mapping(dev))
 +		return intel_nontranslate_map_sg(dev, sglist, nelems, dir);
++=======
+ 
+ 	if (unlikely(attach_deferred(dev)))
+ 		do_deferred_attach(dev);
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  
  	domain = find_domain(dev);
  	if (!domain)
@@@ -3760,6 -3569,11 +3783,14 @@@
  	return nelems;
  }
  
++<<<<<<< HEAD
++=======
+ static u64 intel_get_required_mask(struct device *dev)
+ {
+ 	return DMA_BIT_MASK(32);
+ }
+ 
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  static const struct dma_map_ops intel_dma_ops = {
  	.alloc = intel_alloc_coherent,
  	.free = intel_free_coherent,
@@@ -4782,18 -4852,6 +4813,21 @@@ int __init intel_iommu_init(void
  	}
  	up_write(&dmar_global_lock);
  
++<<<<<<< HEAD
 +#if defined(CONFIG_X86) && defined(CONFIG_SWIOTLB)
 +	/*
 +	 * If the system has no untrusted device or the user has decided
 +	 * to disable the bounce page mechanisms, we don't need swiotlb.
 +	 * Mark this and the pre-allocated bounce pages will be released
 +	 * later.
 +	 */
 +	if (!has_untrusted_dev() || intel_no_bounce)
 +		swiotlb = 0;
 +#endif
 +	dma_ops = &intel_dma_ops;
 +
++=======
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  	init_iommu_pm_ops();
  
  	down_read(&dmar_global_lock);
@@@ -5372,55 -5441,10 +5406,59 @@@ static int intel_iommu_add_device(struc
  	if (translation_pre_enabled(iommu))
  		dev->archdata.iommu = DEFER_DEVICE_DOMAIN_INFO;
  
++<<<<<<< HEAD
 +	group = iommu_group_get_for_dev(dev);
 +
 +	if (IS_ERR(group)) {
 +		ret = PTR_ERR(group);
 +		goto unlink;
 +	}
 +
 +	iommu_group_put(group);
 +
 +	domain = iommu_get_domain_for_dev(dev);
 +	dmar_domain = to_dmar_domain(domain);
 +	if (domain->type == IOMMU_DOMAIN_DMA) {
 +		if (device_def_domain_type(dev) == IOMMU_DOMAIN_IDENTITY) {
 +			ret = iommu_request_dm_for_dev(dev);
 +			if (ret) {
 +				dmar_remove_one_dev_info(dev);
 +				dmar_domain->flags |= DOMAIN_FLAG_LOSE_CHILDREN;
 +				domain_add_dev_info(si_domain, dev);
 +				dev_info(dev,
 +					 "Device uses a private identity domain.\n");
 +			}
 +		}
 +	} else {
 +		if (device_def_domain_type(dev) == IOMMU_DOMAIN_DMA) {
 +			ret = iommu_request_dma_domain_for_dev(dev);
 +			if (ret) {
 +				dmar_remove_one_dev_info(dev);
 +				dmar_domain->flags |= DOMAIN_FLAG_LOSE_CHILDREN;
 +				if (!get_private_domain_for_dev(dev)) {
 +					dev_warn(dev,
 +						 "Failed to get a private domain.\n");
 +					ret = -ENOMEM;
 +					goto unlink;
 +				}
 +
 +				dev_info(dev,
 +					 "Device uses a private dma domain.\n");
 +			}
 +		}
 +	}
 +
 +	return 0;
 +
 +unlink:
 +	iommu_device_unlink(&iommu->iommu, dev);
 +	return ret;
++=======
+ 	return &iommu->iommu;
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  }
  
 -static void intel_iommu_release_device(struct device *dev)
 +static void intel_iommu_remove_device(struct device *dev)
  {
  	struct intel_iommu *iommu;
  	u8 bus, devfn;
@@@ -5431,9 -5455,20 +5469,26 @@@
  
  	dmar_remove_one_dev_info(dev);
  
++<<<<<<< HEAD
 +	iommu_group_remove_device(dev);
 +
 +	iommu_device_unlink(&iommu->iommu, dev);
++=======
+ 	set_dma_ops(dev, NULL);
+ }
+ 
+ static void intel_iommu_probe_finalize(struct device *dev)
+ {
+ 	struct iommu_domain *domain;
+ 
+ 	domain = iommu_get_domain_for_dev(dev);
+ 	if (device_needs_bounce(dev))
+ 		set_dma_ops(dev, &bounce_dma_ops);
+ 	else if (domain && domain->type == IOMMU_DOMAIN_DMA)
+ 		set_dma_ops(dev, &intel_dma_ops);
+ 	else
+ 		set_dma_ops(dev, NULL);
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  }
  
  static void intel_iommu_get_resv_regions(struct device *device,
@@@ -5731,8 -5798,9 +5786,14 @@@ const struct iommu_ops intel_iommu_ops 
  	.map			= intel_iommu_map,
  	.unmap			= intel_iommu_unmap,
  	.iova_to_phys		= intel_iommu_iova_to_phys,
++<<<<<<< HEAD
 +	.add_device		= intel_iommu_add_device,
 +	.remove_device		= intel_iommu_remove_device,
++=======
+ 	.probe_device		= intel_iommu_probe_device,
+ 	.probe_finalize		= intel_iommu_probe_finalize,
+ 	.release_device		= intel_iommu_release_device,
++>>>>>>> 6fc7020cf298 (iommu/vt-d: Apply per-device dma_ops)
  	.get_resv_regions	= intel_iommu_get_resv_regions,
  	.put_resv_regions	= generic_iommu_put_resv_regions,
  	.apply_resv_region	= intel_iommu_apply_resv_region,
* Unmerged path drivers/iommu/intel-iommu.c
