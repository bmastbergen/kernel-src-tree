io_uring: optimise head checks in io_get_sqring()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit ee7d46d9db19ded7b7222af95add63606318a480
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ee7d46d9.failed

A user may ask to submit more than there is in the ring, and then
io_uring will submit as much as it can. However, in the last iteration
it will allocate an io_kiocb and immediately free it. It could do
better and adjust @to_submit to what is in the ring.

And since the ring's head is already checked here, there is no need to
do it in the loop, spamming with smp_load_acquire()'s barriers

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ee7d46d9db19ded7b7222af95add63606318a480)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 28a601d08266,3398f4052ec0..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2392,9 -4519,10 +2392,13 @@@ static void io_commit_sqring(struct io_
   * used, it's important that those reads are done through READ_ONCE() to
   * prevent a re-load down the line.
   */
 -static bool io_get_sqring(struct io_ring_ctx *ctx, struct io_kiocb *req,
 -			  const struct io_uring_sqe **sqe_ptr)
 +static bool io_get_sqring(struct io_ring_ctx *ctx, struct sqe_submit *s)
  {
++<<<<<<< HEAD
 +	struct io_sq_ring *ring = ctx->sq_ring;
++=======
+ 	u32 *sq_array = ctx->sq_array;
++>>>>>>> ee7d46d9db19 (io_uring: optimise head checks in io_get_sqring())
  	unsigned head;
  
  	/*
@@@ -2405,22 -4533,25 +2409,41 @@@
  	 * 2) allows the kernel side to track the head on its own, even
  	 *    though the application is the one updating it.
  	 */
++<<<<<<< HEAD
 +	head = ctx->cached_sq_head;
 +	/* make sure SQ entry isn't read before tail */
 +	if (head == smp_load_acquire(&ring->r.tail))
 +		return false;
 +
 +	head = READ_ONCE(ring->array[head & ctx->sq_mask]);
 +	if (head < ctx->sq_entries) {
 +		s->sqe = &ctx->sq_sqes[head];
 +		s->sequence = ctx->cached_sq_head;
++=======
+ 	head = READ_ONCE(sq_array[ctx->cached_sq_head & ctx->sq_mask]);
+ 	if (likely(head < ctx->sq_entries)) {
+ 		/*
+ 		 * All io need record the previous position, if LINK vs DARIN,
+ 		 * it can be used to mark the position of the first IO in the
+ 		 * link list.
+ 		 */
+ 		req->sequence = ctx->cached_sq_head;
+ 		*sqe_ptr = &ctx->sq_sqes[head];
+ 		req->opcode = READ_ONCE((*sqe_ptr)->opcode);
+ 		req->user_data = READ_ONCE((*sqe_ptr)->user_data);
++>>>>>>> ee7d46d9db19 (io_uring: optimise head checks in io_get_sqring())
  		ctx->cached_sq_head++;
  		return true;
  	}
  
  	/* drop invalid entries */
  	ctx->cached_sq_head++;
++<<<<<<< HEAD
 +	ring->dropped++;
++=======
+ 	ctx->cached_sq_dropped++;
+ 	WRITE_ONCE(ctx->rings->sq_dropped, ctx->cached_sq_dropped);
++>>>>>>> ee7d46d9db19 (io_uring: optimise head checks in io_get_sqring())
  	return false;
  }
  
@@@ -2429,9 -4561,21 +2452,26 @@@ static int io_submit_sqes(struct io_rin
  {
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
 +	struct io_kiocb *shadow_req = NULL;
 +	bool prev_was_link = false;
  	int i, submitted = 0;
++<<<<<<< HEAD
++=======
+ 	bool mm_fault = false;
+ 
+ 	/* if we have a backlog and couldn't flush it all, return BUSY */
+ 	if (test_bit(0, &ctx->sq_check_overflow)) {
+ 		if (!list_empty(&ctx->cq_overflow_list) &&
+ 		    !io_cqring_overflow_flush(ctx, false))
+ 			return -EBUSY;
+ 	}
+ 
+ 	/* make sure SQ entry isn't read before tail */
+ 	nr = min3(nr, ctx->sq_entries, io_sqring_entries(ctx));
+ 
+ 	if (!percpu_ref_tryget_many(&ctx->refs, nr))
+ 		return -EAGAIN;
++>>>>>>> ee7d46d9db19 (io_uring: optimise head checks in io_get_sqring())
  
  	if (nr > IO_PLUG_THRESHOLD) {
  		io_submit_state_start(&state, nr);
* Unmerged path fs/io_uring.c
