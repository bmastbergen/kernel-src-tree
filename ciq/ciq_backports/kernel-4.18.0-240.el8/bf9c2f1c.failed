io_uring: track mm through current->mm

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit bf9c2f1cdcc718b6d2d41172f6ca005fe22cc7ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/bf9c2f1c.failed

As a preparation for extracting request init bits, remove self-coded mm
tracking from io_submit_sqes(), but rely on current->mm. It's more
convenient, than passing this piece of state in other functions.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit bf9c2f1cdcc718b6d2d41172f6ca005fe22cc7ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 2afa3b27779e,f7825d3de400..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2415,27 -5774,45 +2415,54 @@@ static bool io_get_sqring(struct io_rin
  	 * 2) allows the kernel side to track the head on its own, even
  	 *    though the application is the one updating it.
  	 */
 -	head = READ_ONCE(sq_array[ctx->cached_sq_head & ctx->sq_mask]);
 -	if (likely(head < ctx->sq_entries))
 -		return &ctx->sq_sqes[head];
 +	head = ctx->cached_sq_head;
 +	/* make sure SQ entry isn't read before tail */
 +	if (head == smp_load_acquire(&ring->r.tail))
 +		return false;
  
 -	/* drop invalid entries */
 -	ctx->cached_sq_dropped++;
 -	WRITE_ONCE(ctx->rings->sq_dropped, ctx->cached_sq_dropped);
 -	return NULL;
 -}
 +	head = READ_ONCE(ring->array[head & ctx->sq_mask]);
 +	if (head < ctx->sq_entries) {
 +		s->sqe = &ctx->sq_sqes[head];
 +		s->sequence = ctx->cached_sq_head;
 +		ctx->cached_sq_head++;
 +		return true;
 +	}
  
 -static inline void io_consume_sqe(struct io_ring_ctx *ctx)
 -{
 +	/* drop invalid entries */
  	ctx->cached_sq_head++;
 +	ring->dropped++;
 +	return false;
  }
  
++<<<<<<< HEAD
 +static int io_submit_sqes(struct io_ring_ctx *ctx, struct sqe_submit *sqes,
 +			  unsigned int nr, bool has_user, bool mm_fault)
++=======
+ static void io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,
+ 			const struct io_uring_sqe *sqe)
+ {
+ 	/*
+ 	 * All io need record the previous position, if LINK vs DARIN,
+ 	 * it can be used to mark the position of the first IO in the
+ 	 * link list.
+ 	 */
+ 	req->sequence = ctx->cached_sq_head;
+ 	req->opcode = READ_ONCE(sqe->opcode);
+ 	req->user_data = READ_ONCE(sqe->user_data);
+ 	req->io = NULL;
+ 	req->file = NULL;
+ 	req->ctx = ctx;
+ 	req->flags = 0;
+ 	/* one is dropped after submission, the other at completion */
+ 	refcount_set(&req->refs, 2);
+ 	req->task = NULL;
+ 	req->result = 0;
+ 	INIT_IO_WORK(&req->work, io_wq_submit_work);
+ }
+ 
+ static int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr,
+ 			  struct file *ring_file, int ring_fd, bool async)
++>>>>>>> bf9c2f1cdcc7 (io_uring: track mm through current->mm)
  {
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
@@@ -2448,62 -5836,90 +2475,98 @@@
  		statep = &state;
  	}
  
 -	ctx->ring_fd = ring_fd;
 -	ctx->ring_file = ring_file;
 -
  	for (i = 0; i < nr; i++) {
 -		const struct io_uring_sqe *sqe;
 -		struct io_kiocb *req;
 -		int err;
 -
 -		sqe = io_get_sqe(ctx);
 -		if (unlikely(!sqe)) {
 -			io_consume_sqe(ctx);
 -			break;
 -		}
 -		req = io_alloc_req(ctx, statep);
 -		if (unlikely(!req)) {
 -			if (!submitted)
 -				submitted = -EAGAIN;
 -			break;
 +		/*
 +		 * If previous wasn't linked and we have a linked command,
 +		 * that's the end of the chain. Submit the previous link.
 +		 */
 +		if (!prev_was_link && link) {
 +			io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +						true);
 +			link = NULL;
 +			shadow_req = NULL;
  		}
 -
 +		prev_was_link = (sqes[i].sqe->flags & IOSQE_IO_LINK) != 0;
 +
++<<<<<<< HEAD
 +		if (link && (sqes[i].sqe->flags & IOSQE_IO_DRAIN)) {
 +			if (!shadow_req) {
 +				shadow_req = io_get_req(ctx, NULL);
 +				if (unlikely(!shadow_req))
 +					goto out;
 +				shadow_req->flags |= (REQ_F_IO_DRAIN | REQ_F_SHADOW_DRAIN);
 +				refcount_dec(&shadow_req->refs);
 +			}
 +			shadow_req->sequence = sqes[i].sequence;
++=======
+ 		io_init_req(ctx, req, sqe);
+ 		io_consume_sqe(ctx);
+ 		/* will complete beyond this point, count as submitted */
+ 		submitted++;
+ 
+ 		if (unlikely(req->opcode >= IORING_OP_LAST)) {
+ 			err = -EINVAL;
+ fail_req:
+ 			io_cqring_add_event(req, err);
+ 			io_double_put_req(req);
+ 			break;
+ 		}
+ 
+ 		if (io_op_defs[req->opcode].needs_mm && !current->mm) {
+ 			if (unlikely(!mmget_not_zero(ctx->sqo_mm))) {
+ 				err = -EFAULT;
+ 				goto fail_req;
+ 			}
+ 			use_mm(ctx->sqo_mm);
++>>>>>>> bf9c2f1cdcc7 (io_uring: track mm through current->mm)
  		}
  
 -		req->needs_fixed_file = async;
 -		trace_io_uring_submit_sqe(ctx, req->opcode, req->user_data,
 -						true, async);
 -		if (!io_submit_sqe(req, sqe, statep, &link))
 -			break;
 +out:
 +		if (unlikely(mm_fault)) {
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
 +						-EFAULT);
 +		} else {
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
 +			submitted++;
 +		}
  	}
  
 -	if (unlikely(submitted != nr)) {
 -		int ref_used = (submitted == -EAGAIN) ? 0 : submitted;
 -
 -		percpu_ref_put_many(&ctx->refs, nr - ref_used);
 -	}
  	if (link)
 -		io_queue_link_head(link);
 +		io_queue_link_head(ctx, link, &link->submit, shadow_req, true);
  	if (statep)
  		io_submit_state_end(&state);
  
  	return submitted;
  }
  
+ static inline void io_sq_thread_drop_mm(struct io_ring_ctx *ctx)
+ {
+ 	struct mm_struct *mm = current->mm;
+ 
+ 	if (mm) {
+ 		unuse_mm(mm);
+ 		mmput(mm);
+ 	}
+ }
+ 
  static int io_sq_thread(void *data)
  {
 +	struct sqe_submit sqes[IO_IOPOLL_BATCH];
  	struct io_ring_ctx *ctx = data;
++<<<<<<< HEAD
 +	struct mm_struct *cur_mm = NULL;
++=======
+ 	const struct cred *old_cred;
++>>>>>>> bf9c2f1cdcc7 (io_uring: track mm through current->mm)
  	mm_segment_t old_fs;
  	DEFINE_WAIT(wait);
 +	unsigned inflight;
  	unsigned long timeout;
 -	int ret = 0;
  
 -	complete(&ctx->completions[1]);
 +	complete(&ctx->sqo_thread_started);
  
  	old_fs = get_fs();
  	set_fs(USER_DS);
@@@ -2590,41 -6013,21 +2649,54 @@@
  			}
  			finish_wait(&ctx->sqo_wait, &wait);
  
 -			ctx->rings->sq_flags &= ~IORING_SQ_NEED_WAKEUP;
 +			ctx->sq_ring->flags &= ~IORING_SQ_NEED_WAKEUP;
  		}
  
++<<<<<<< HEAD
 +		i = 0;
 +		all_fixed = true;
 +		do {
 +			if (all_fixed && io_sqe_needs_user(sqes[i].sqe))
 +				all_fixed = false;
++=======
+ 		mutex_lock(&ctx->uring_lock);
+ 		ret = io_submit_sqes(ctx, to_submit, NULL, -1, true);
+ 		mutex_unlock(&ctx->uring_lock);
+ 		timeout = jiffies + ctx->sq_thread_idle;
+ 	}
++>>>>>>> bf9c2f1cdcc7 (io_uring: track mm through current->mm)
 +
 +			i++;
 +			if (i == ARRAY_SIZE(sqes))
 +				break;
 +		} while (io_get_sqring(ctx, &sqes[i]));
 +
 +		/* Unless all new commands are FIXED regions, grab mm */
 +		if (!all_fixed && !cur_mm) {
 +			mm_fault = !mmget_not_zero(ctx->sqo_mm);
 +			if (!mm_fault) {
 +				use_mm(ctx->sqo_mm);
 +				cur_mm = ctx->sqo_mm;
 +			}
 +		}
 +
 +		inflight += io_submit_sqes(ctx, sqes, i, cur_mm != NULL,
 +						mm_fault);
  
 -	if (current->task_works)
 -		task_work_run();
 +		/* Commit SQ ring head once we've consumed all SQEs */
 +		io_commit_sqring(ctx);
 +	}
  
  	set_fs(old_fs);
++<<<<<<< HEAD
 +	if (cur_mm) {
 +		unuse_mm(cur_mm);
 +		mmput(cur_mm);
 +	}
++=======
+ 	io_sq_thread_drop_mm(ctx);
+ 	revert_creds(old_cred);
++>>>>>>> bf9c2f1cdcc7 (io_uring: track mm through current->mm)
  
  	kthread_parkme();
  
@@@ -3653,21 -7507,8 +3725,26 @@@ SYSCALL_DEFINE6(io_uring_enter, unsigne
  			wake_up(&ctx->sqo_wait);
  		submitted = to_submit;
  	} else if (to_submit) {
++<<<<<<< HEAD
 +		bool block_for_last = false;
 +
 +		to_submit = min(to_submit, ctx->sq_entries);
 +
 +		/*
 +		 * Allow last submission to block in a series, IFF the caller
 +		 * asked to wait for events and we don't currently have
 +		 * enough. This potentially avoids an async punt.
 +		 */
 +		if (to_submit == min_complete &&
 +		    io_cqring_events(ctx->rings) < min_complete)
 +			block_for_last = true;
 +
 +		mutex_lock(&ctx->uring_lock);
 +		submitted = io_ring_submit(ctx, to_submit, block_for_last);
++=======
+ 		mutex_lock(&ctx->uring_lock);
+ 		submitted = io_submit_sqes(ctx, to_submit, f.file, fd, false);
++>>>>>>> bf9c2f1cdcc7 (io_uring: track mm through current->mm)
  		mutex_unlock(&ctx->uring_lock);
  
  		if (submitted != to_submit)
* Unmerged path fs/io_uring.c
