KVM: arm64: Unify handling THP backed host memory

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Suzuki K Poulose <suzuki.poulose@arm.com>
commit 0529c9021252a58b6d3808da86986a614b900b1b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/0529c902.failed

We support mapping host memory backed by PMD transparent hugepages
at stage2 as huge pages. However the checks are now spread across
two different places. Let us unify the handling of the THPs to
keep the code cleaner (and future proof for PUD THP support).
This patch moves transparent_hugepage_adjust() closer to the caller
to avoid a forward declaration for fault_supports_stage2_huge_mappings().

Also, since we already handle the case where the host VA and the guest
PA may not be aligned, the explicit VM_BUG_ON() is not required.

	Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
	Signed-off-by: Zenghui Yu <yuzenghui@huawei.com>
	Signed-off-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20200507123546.1875-3-yuzenghui@huawei.com
(cherry picked from commit 0529c9021252a58b6d3808da86986a614b900b1b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	virt/kvm/arm/mmu.c
diff --cc virt/kvm/arm/mmu.c
index b2e7a1d80c9c,66eb8e3f6e8c..000000000000
--- a/virt/kvm/arm/mmu.c
+++ b/virt/kvm/arm/mmu.c
@@@ -1388,53 -1375,6 +1388,56 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD:virt/kvm/arm/mmu.c
 +static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)
 +{
 +	kvm_pfn_t pfn = *pfnp;
 +	gfn_t gfn = *ipap >> PAGE_SHIFT;
 +	struct page *page = pfn_to_page(pfn);
 +
 +	/*
 +	 * PageTransCompoundMap() returns true for THP and
 +	 * hugetlbfs. Make sure the adjustment is done only for THP
 +	 * pages.
 +	 */
 +	if (!PageHuge(page) && PageTransCompoundMap(page)) {
 +		unsigned long mask;
 +		/*
 +		 * The address we faulted on is backed by a transparent huge
 +		 * page.  However, because we map the compound huge page and
 +		 * not the individual tail page, we need to transfer the
 +		 * refcount to the head page.  We have to be careful that the
 +		 * THP doesn't start to split while we are adjusting the
 +		 * refcounts.
 +		 *
 +		 * We are sure this doesn't happen, because mmu_notifier_retry
 +		 * was successful and we are holding the mmu_lock, so if this
 +		 * THP is trying to split, it will be blocked in the mmu
 +		 * notifier before touching any of the pages, specifically
 +		 * before being able to call __split_huge_page_refcount().
 +		 *
 +		 * We can therefore safely transfer the refcount from PG_tail
 +		 * to PG_head and switch the pfn from a tail page to the head
 +		 * page accordingly.
 +		 */
 +		mask = PTRS_PER_PMD - 1;
 +		VM_BUG_ON((gfn & mask) != (pfn & mask));
 +		if (pfn & mask) {
 +			*ipap &= PMD_MASK;
 +			kvm_release_pfn_clean(pfn);
 +			pfn &= ~mask;
 +			kvm_get_pfn(pfn);
 +			*pfnp = pfn;
 +		}
 +
 +		return true;
 +	}
 +
 +	return false;
 +}
 +
++=======
++>>>>>>> 0529c9021252 (KVM: arm64: Unify handling THP backed host memory):arch/arm64/kvm/mmu.c
  /**
   * stage2_wp_ptes - write protect PMD range
   * @pmd:	pointer to pmd entry
* Unmerged path virt/kvm/arm/mmu.c
