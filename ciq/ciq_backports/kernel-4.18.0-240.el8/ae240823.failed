bpf: Introduce BPF_MODIFY_RETURN

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author KP Singh <kpsingh@google.com>
commit ae24082331d9bbaae283aafbe930a8f0eb85605a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ae240823.failed

When multiple programs are attached, each program receives the return
value from the previous program on the stack and the last program
provides the return value to the attached function.

The fmod_ret bpf programs are run after the fentry programs and before
the fexit programs. The original function is only called if all the
fmod_ret programs return 0 to avoid any unintended side-effects. The
success value, i.e. 0 is not currently configurable but can be made so
where user-space can specify it at load time.

For example:

int func_to_be_attached(int a, int b)
{  <--- do_fentry

do_fmod_ret:
   <update ret by calling fmod_ret>
   if (ret != 0)
        goto do_fexit;

original_function:

    <side_effects_happen_here>

}  <--- do_fexit

The fmod_ret program attached to this function can be defined as:

SEC("fmod_ret/func_to_be_attached")
int BPF_PROG(func_name, int a, int b, int ret)
{
        // This will skip the original function logic.
        return 1;
}

The first fmod_ret program is passed 0 in its return argument.

	Signed-off-by: KP Singh <kpsingh@google.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
Link: https://lore.kernel.org/bpf/20200304191853.1529-4-kpsingh@chromium.org
(cherry picked from commit ae24082331d9bbaae283aafbe930a8f0eb85605a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
#	include/linux/bpf.h
#	include/uapi/linux/bpf.h
#	kernel/bpf/btf.c
#	kernel/bpf/syscall.c
#	kernel/bpf/trampoline.c
#	kernel/bpf/verifier.c
#	tools/include/uapi/linux/bpf.h
diff --cc arch/x86/net/bpf_jit_comp.c
index 490d8e35c7c9,b1fd000feb89..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -1242,6 -1328,487 +1242,490 @@@ emit_jmp
  	return proglen;
  }
  
++<<<<<<< HEAD
++=======
+ static void save_regs(const struct btf_func_model *m, u8 **prog, int nr_args,
+ 		      int stack_size)
+ {
+ 	int i;
+ 	/* Store function arguments to stack.
+ 	 * For a function that accepts two pointers the sequence will be:
+ 	 * mov QWORD PTR [rbp-0x10],rdi
+ 	 * mov QWORD PTR [rbp-0x8],rsi
+ 	 */
+ 	for (i = 0; i < min(nr_args, 6); i++)
+ 		emit_stx(prog, bytes_to_bpf_size(m->arg_size[i]),
+ 			 BPF_REG_FP,
+ 			 i == 5 ? X86_REG_R9 : BPF_REG_1 + i,
+ 			 -(stack_size - i * 8));
+ }
+ 
+ static void restore_regs(const struct btf_func_model *m, u8 **prog, int nr_args,
+ 			 int stack_size)
+ {
+ 	int i;
+ 
+ 	/* Restore function arguments from stack.
+ 	 * For a function that accepts two pointers the sequence will be:
+ 	 * EMIT4(0x48, 0x8B, 0x7D, 0xF0); mov rdi,QWORD PTR [rbp-0x10]
+ 	 * EMIT4(0x48, 0x8B, 0x75, 0xF8); mov rsi,QWORD PTR [rbp-0x8]
+ 	 */
+ 	for (i = 0; i < min(nr_args, 6); i++)
+ 		emit_ldx(prog, bytes_to_bpf_size(m->arg_size[i]),
+ 			 i == 5 ? X86_REG_R9 : BPF_REG_1 + i,
+ 			 BPF_REG_FP,
+ 			 -(stack_size - i * 8));
+ }
+ 
+ static int invoke_bpf_prog(const struct btf_func_model *m, u8 **pprog,
+ 			   struct bpf_prog *p, int stack_size, bool mod_ret)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	if (emit_call(&prog, __bpf_prog_enter, prog))
+ 		return -EINVAL;
+ 	/* remember prog start time returned by __bpf_prog_enter */
+ 	emit_mov_reg(&prog, true, BPF_REG_6, BPF_REG_0);
+ 
+ 	/* arg1: lea rdi, [rbp - stack_size] */
+ 	EMIT4(0x48, 0x8D, 0x7D, -stack_size);
+ 	/* arg2: progs[i]->insnsi for interpreter */
+ 	if (!p->jited)
+ 		emit_mov_imm64(&prog, BPF_REG_2,
+ 			       (long) p->insnsi >> 32,
+ 			       (u32) (long) p->insnsi);
+ 	/* call JITed bpf program or interpreter */
+ 	if (emit_call(&prog, p->bpf_func, prog))
+ 		return -EINVAL;
+ 
+ 	/* BPF_TRAMP_MODIFY_RETURN trampolines can modify the return
+ 	 * of the previous call which is then passed on the stack to
+ 	 * the next BPF program.
+ 	 */
+ 	if (mod_ret)
+ 		emit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);
+ 
+ 	/* arg1: mov rdi, progs[i] */
+ 	emit_mov_imm64(&prog, BPF_REG_1, (long) p >> 32,
+ 		       (u32) (long) p);
+ 	/* arg2: mov rsi, rbx <- start time in nsec */
+ 	emit_mov_reg(&prog, true, BPF_REG_2, BPF_REG_6);
+ 	if (emit_call(&prog, __bpf_prog_exit, prog))
+ 		return -EINVAL;
+ 
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static void emit_nops(u8 **pprog, unsigned int len)
+ {
+ 	unsigned int i, noplen;
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	while (len > 0) {
+ 		noplen = len;
+ 
+ 		if (noplen > ASM_NOP_MAX)
+ 			noplen = ASM_NOP_MAX;
+ 
+ 		for (i = 0; i < noplen; i++)
+ 			EMIT1(ideal_nops[noplen][i]);
+ 		len -= noplen;
+ 	}
+ 
+ 	*pprog = prog;
+ }
+ 
+ static void emit_align(u8 **pprog, u32 align)
+ {
+ 	u8 *target, *prog = *pprog;
+ 
+ 	target = PTR_ALIGN(prog, align);
+ 	if (target != prog)
+ 		emit_nops(&prog, target - prog);
+ 
+ 	*pprog = prog;
+ }
+ 
+ static int emit_cond_near_jump(u8 **pprog, void *func, void *ip, u8 jmp_cond)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 	s64 offset;
+ 
+ 	offset = func - (ip + 2 + 4);
+ 	if (!is_simm32(offset)) {
+ 		pr_err("Target %p is out of range\n", func);
+ 		return -EINVAL;
+ 	}
+ 	EMIT2_off32(0x0F, jmp_cond + 0x10, offset);
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static int emit_mod_ret_check_imm8(u8 **pprog, int value)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	if (!is_imm8(value))
+ 		return -EINVAL;
+ 
+ 	if (value == 0)
+ 		EMIT2(0x85, add_2reg(0xC0, BPF_REG_0, BPF_REG_0));
+ 	else
+ 		EMIT3(0x83, add_1reg(0xF8, BPF_REG_0), value);
+ 
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static int invoke_bpf(const struct btf_func_model *m, u8 **pprog,
+ 		      struct bpf_tramp_progs *tp, int stack_size)
+ {
+ 	int i;
+ 	u8 *prog = *pprog;
+ 
+ 	for (i = 0; i < tp->nr_progs; i++) {
+ 		if (invoke_bpf_prog(m, &prog, tp->progs[i], stack_size, false))
+ 			return -EINVAL;
+ 	}
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static int invoke_bpf_mod_ret(const struct btf_func_model *m, u8 **pprog,
+ 			      struct bpf_tramp_progs *tp, int stack_size,
+ 			      u8 **branches)
+ {
+ 	u8 *prog = *pprog;
+ 	int i;
+ 
+ 	/* The first fmod_ret program will receive a garbage return value.
+ 	 * Set this to 0 to avoid confusing the program.
+ 	 */
+ 	emit_mov_imm32(&prog, false, BPF_REG_0, 0);
+ 	emit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);
+ 	for (i = 0; i < tp->nr_progs; i++) {
+ 		if (invoke_bpf_prog(m, &prog, tp->progs[i], stack_size, true))
+ 			return -EINVAL;
+ 
+ 		/* Generate a branch:
+ 		 *
+ 		 * if (ret !=  0)
+ 		 *	goto do_fexit;
+ 		 *
+ 		 * If needed this can be extended to any integer value which can
+ 		 * be passed by user-space when the program is loaded.
+ 		 */
+ 		if (emit_mod_ret_check_imm8(&prog, 0))
+ 			return -EINVAL;
+ 
+ 		/* Save the location of the branch and Generate 6 nops
+ 		 * (4 bytes for an offset and 2 bytes for the jump) These nops
+ 		 * are replaced with a conditional jump once do_fexit (i.e. the
+ 		 * start of the fexit invocation) is finalized.
+ 		 */
+ 		branches[i] = prog;
+ 		emit_nops(&prog, 4 + 2);
+ 	}
+ 
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ /* Example:
+  * __be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev);
+  * its 'struct btf_func_model' will be nr_args=2
+  * The assembly code when eth_type_trans is executing after trampoline:
+  *
+  * push rbp
+  * mov rbp, rsp
+  * sub rsp, 16                     // space for skb and dev
+  * push rbx                        // temp regs to pass start time
+  * mov qword ptr [rbp - 16], rdi   // save skb pointer to stack
+  * mov qword ptr [rbp - 8], rsi    // save dev pointer to stack
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time in bpf stats are enabled
+  * lea rdi, [rbp - 16]             // R1==ctx of bpf prog
+  * call addr_of_jited_FENTRY_prog
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rdi, qword ptr [rbp - 16]   // restore skb pointer from stack
+  * mov rsi, qword ptr [rbp - 8]    // restore dev pointer from stack
+  * pop rbx
+  * leave
+  * ret
+  *
+  * eth_type_trans has 5 byte nop at the beginning. These 5 bytes will be
+  * replaced with 'call generated_bpf_trampoline'. When it returns
+  * eth_type_trans will continue executing with original skb and dev pointers.
+  *
+  * The assembly code when eth_type_trans is called from trampoline:
+  *
+  * push rbp
+  * mov rbp, rsp
+  * sub rsp, 24                     // space for skb, dev, return value
+  * push rbx                        // temp regs to pass start time
+  * mov qword ptr [rbp - 24], rdi   // save skb pointer to stack
+  * mov qword ptr [rbp - 16], rsi   // save dev pointer to stack
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time if bpf stats are enabled
+  * lea rdi, [rbp - 24]             // R1==ctx of bpf prog
+  * call addr_of_jited_FENTRY_prog  // bpf prog can access skb and dev
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rdi, qword ptr [rbp - 24]   // restore skb pointer from stack
+  * mov rsi, qword ptr [rbp - 16]   // restore dev pointer from stack
+  * call eth_type_trans+5           // execute body of eth_type_trans
+  * mov qword ptr [rbp - 8], rax    // save return value
+  * call __bpf_prog_enter           // rcu_read_lock and preempt_disable
+  * mov rbx, rax                    // remember start time in bpf stats are enabled
+  * lea rdi, [rbp - 24]             // R1==ctx of bpf prog
+  * call addr_of_jited_FEXIT_prog   // bpf prog can access skb, dev, return value
+  * movabsq rdi, 64bit_addr_of_struct_bpf_prog  // unused if bpf stats are off
+  * mov rsi, rbx                    // prog start time
+  * call __bpf_prog_exit            // rcu_read_unlock, preempt_enable and stats math
+  * mov rax, qword ptr [rbp - 8]    // restore eth_type_trans's return value
+  * pop rbx
+  * leave
+  * add rsp, 8                      // skip eth_type_trans's frame
+  * ret                             // return to its caller
+  */
+ int arch_prepare_bpf_trampoline(void *image, void *image_end,
+ 				const struct btf_func_model *m, u32 flags,
+ 				struct bpf_tramp_progs *tprogs,
+ 				void *orig_call)
+ {
+ 	int ret, i, cnt = 0, nr_args = m->nr_args;
+ 	int stack_size = nr_args * 8;
+ 	struct bpf_tramp_progs *fentry = &tprogs[BPF_TRAMP_FENTRY];
+ 	struct bpf_tramp_progs *fexit = &tprogs[BPF_TRAMP_FEXIT];
+ 	struct bpf_tramp_progs *fmod_ret = &tprogs[BPF_TRAMP_MODIFY_RETURN];
+ 	u8 **branches = NULL;
+ 	u8 *prog;
+ 
+ 	/* x86-64 supports up to 6 arguments. 7+ can be added in the future */
+ 	if (nr_args > 6)
+ 		return -ENOTSUPP;
+ 
+ 	if ((flags & BPF_TRAMP_F_RESTORE_REGS) &&
+ 	    (flags & BPF_TRAMP_F_SKIP_FRAME))
+ 		return -EINVAL;
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG)
+ 		stack_size += 8; /* room for return value of orig_call */
+ 
+ 	if (flags & BPF_TRAMP_F_SKIP_FRAME)
+ 		/* skip patched call instruction and point orig_call to actual
+ 		 * body of the kernel function.
+ 		 */
+ 		orig_call += X86_PATCH_SIZE;
+ 
+ 	prog = image;
+ 
+ 	EMIT1(0x55);		 /* push rbp */
+ 	EMIT3(0x48, 0x89, 0xE5); /* mov rbp, rsp */
+ 	EMIT4(0x48, 0x83, 0xEC, stack_size); /* sub rsp, stack_size */
+ 	EMIT1(0x53);		 /* push rbx */
+ 
+ 	save_regs(m, &prog, nr_args, stack_size);
+ 
+ 	if (fentry->nr_progs)
+ 		if (invoke_bpf(m, &prog, fentry, stack_size))
+ 			return -EINVAL;
+ 
+ 	if (fmod_ret->nr_progs) {
+ 		branches = kcalloc(fmod_ret->nr_progs, sizeof(u8 *),
+ 				   GFP_KERNEL);
+ 		if (!branches)
+ 			return -ENOMEM;
+ 
+ 		if (invoke_bpf_mod_ret(m, &prog, fmod_ret, stack_size,
+ 				       branches)) {
+ 			ret = -EINVAL;
+ 			goto cleanup;
+ 		}
+ 	}
+ 
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG) {
+ 		if (fentry->nr_progs || fmod_ret->nr_progs)
+ 			restore_regs(m, &prog, nr_args, stack_size);
+ 
+ 		/* call original function */
+ 		if (emit_call(&prog, orig_call, prog)) {
+ 			ret = -EINVAL;
+ 			goto cleanup;
+ 		}
+ 		/* remember return value in a stack for bpf prog to access */
+ 		emit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);
+ 	}
+ 
+ 	if (fmod_ret->nr_progs) {
+ 		/* From Intel 64 and IA-32 Architectures Optimization
+ 		 * Reference Manual, 3.4.1.4 Code Alignment, Assembly/Compiler
+ 		 * Coding Rule 11: All branch targets should be 16-byte
+ 		 * aligned.
+ 		 */
+ 		emit_align(&prog, 16);
+ 		/* Update the branches saved in invoke_bpf_mod_ret with the
+ 		 * aligned address of do_fexit.
+ 		 */
+ 		for (i = 0; i < fmod_ret->nr_progs; i++)
+ 			emit_cond_near_jump(&branches[i], prog, branches[i],
+ 					    X86_JNE);
+ 	}
+ 
+ 	if (fexit->nr_progs)
+ 		if (invoke_bpf(m, &prog, fexit, stack_size)) {
+ 			ret = -EINVAL;
+ 			goto cleanup;
+ 		}
+ 
+ 	if (flags & BPF_TRAMP_F_RESTORE_REGS)
+ 		restore_regs(m, &prog, nr_args, stack_size);
+ 
+ 	/* This needs to be done regardless. If there were fmod_ret programs,
+ 	 * the return value is only updated on the stack and still needs to be
+ 	 * restored to R0.
+ 	 */
+ 	if (flags & BPF_TRAMP_F_CALL_ORIG)
+ 		/* restore original return value back into RAX */
+ 		emit_ldx(&prog, BPF_DW, BPF_REG_0, BPF_REG_FP, -8);
+ 
+ 	EMIT1(0x5B); /* pop rbx */
+ 	EMIT1(0xC9); /* leave */
+ 	if (flags & BPF_TRAMP_F_SKIP_FRAME)
+ 		/* skip our return address and return to parent */
+ 		EMIT4(0x48, 0x83, 0xC4, 8); /* add rsp, 8 */
+ 	EMIT1(0xC3); /* ret */
+ 	/* Make sure the trampoline generation logic doesn't overflow */
+ 	if (WARN_ON_ONCE(prog > (u8 *)image_end - BPF_INSN_SAFETY)) {
+ 		ret = -EFAULT;
+ 		goto cleanup;
+ 	}
+ 	ret = prog - (u8 *)image;
+ 
+ cleanup:
+ 	kfree(branches);
+ 	return ret;
+ }
+ 
+ static int emit_fallback_jump(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int err = 0;
+ 
+ #ifdef CONFIG_RETPOLINE
+ 	/* Note that this assumes the the compiler uses external
+ 	 * thunks for indirect calls. Both clang and GCC use the same
+ 	 * naming convention for external thunks.
+ 	 */
+ 	err = emit_jump(&prog, __x86_indirect_thunk_rdx, prog);
+ #else
+ 	int cnt = 0;
+ 
+ 	EMIT2(0xFF, 0xE2);	/* jmp rdx */
+ #endif
+ 	*pprog = prog;
+ 	return err;
+ }
+ 
+ static int emit_bpf_dispatcher(u8 **pprog, int a, int b, s64 *progs)
+ {
+ 	u8 *jg_reloc, *prog = *pprog;
+ 	int pivot, err, jg_bytes = 1, cnt = 0;
+ 	s64 jg_offset;
+ 
+ 	if (a == b) {
+ 		/* Leaf node of recursion, i.e. not a range of indices
+ 		 * anymore.
+ 		 */
+ 		EMIT1(add_1mod(0x48, BPF_REG_3));	/* cmp rdx,func */
+ 		if (!is_simm32(progs[a]))
+ 			return -1;
+ 		EMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3),
+ 			    progs[a]);
+ 		err = emit_cond_near_jump(&prog,	/* je func */
+ 					  (void *)progs[a], prog,
+ 					  X86_JE);
+ 		if (err)
+ 			return err;
+ 
+ 		err = emit_fallback_jump(&prog);	/* jmp thunk/indirect */
+ 		if (err)
+ 			return err;
+ 
+ 		*pprog = prog;
+ 		return 0;
+ 	}
+ 
+ 	/* Not a leaf node, so we pivot, and recursively descend into
+ 	 * the lower and upper ranges.
+ 	 */
+ 	pivot = (b - a) / 2;
+ 	EMIT1(add_1mod(0x48, BPF_REG_3));		/* cmp rdx,func */
+ 	if (!is_simm32(progs[a + pivot]))
+ 		return -1;
+ 	EMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3), progs[a + pivot]);
+ 
+ 	if (pivot > 2) {				/* jg upper_part */
+ 		/* Require near jump. */
+ 		jg_bytes = 4;
+ 		EMIT2_off32(0x0F, X86_JG + 0x10, 0);
+ 	} else {
+ 		EMIT2(X86_JG, 0);
+ 	}
+ 	jg_reloc = prog;
+ 
+ 	err = emit_bpf_dispatcher(&prog, a, a + pivot,	/* emit lower_part */
+ 				  progs);
+ 	if (err)
+ 		return err;
+ 
+ 	/* From Intel 64 and IA-32 Architectures Optimization
+ 	 * Reference Manual, 3.4.1.4 Code Alignment, Assembly/Compiler
+ 	 * Coding Rule 11: All branch targets should be 16-byte
+ 	 * aligned.
+ 	 */
+ 	emit_align(&prog, 16);
+ 	jg_offset = prog - jg_reloc;
+ 	emit_code(jg_reloc - jg_bytes, jg_offset, jg_bytes);
+ 
+ 	err = emit_bpf_dispatcher(&prog, a + pivot + 1,	/* emit upper_part */
+ 				  b, progs);
+ 	if (err)
+ 		return err;
+ 
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
+ static int cmp_ips(const void *a, const void *b)
+ {
+ 	const s64 *ipa = a;
+ 	const s64 *ipb = b;
+ 
+ 	if (*ipa > *ipb)
+ 		return 1;
+ 	if (*ipa < *ipb)
+ 		return -1;
+ 	return 0;
+ }
+ 
+ int arch_prepare_bpf_dispatcher(void *image, s64 *funcs, int num_funcs)
+ {
+ 	u8 *prog = image;
+ 
+ 	sort(funcs, num_funcs, sizeof(funcs[0]), cmp_ips, NULL);
+ 	return emit_bpf_dispatcher(&prog, 0, num_funcs - 1, funcs);
+ }
+ 
++>>>>>>> ae24082331d9 (bpf: Introduce BPF_MODIFY_RETURN)
  struct x64_jit_data {
  	struct bpf_binary_header *header;
  	int *addrs;
diff --cc include/linux/bpf.h
index 609313a9dbb2,f748b31e5888..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -389,13 -413,222 +389,226 @@@ struct bpf_prog_stats 
  	struct u64_stats_sync syncp;
  } __aligned(2 * sizeof(u64));
  
++<<<<<<< HEAD
++=======
+ struct btf_func_model {
+ 	u8 ret_size;
+ 	u8 nr_args;
+ 	u8 arg_size[MAX_BPF_FUNC_ARGS];
+ };
+ 
+ /* Restore arguments before returning from trampoline to let original function
+  * continue executing. This flag is used for fentry progs when there are no
+  * fexit progs.
+  */
+ #define BPF_TRAMP_F_RESTORE_REGS	BIT(0)
+ /* Call original function after fentry progs, but before fexit progs.
+  * Makes sense for fentry/fexit, normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_CALL_ORIG		BIT(1)
+ /* Skip current frame and return to parent.  Makes sense for fentry/fexit
+  * programs only. Should not be used with normal calls and indirect calls.
+  */
+ #define BPF_TRAMP_F_SKIP_FRAME		BIT(2)
+ 
+ /* Each call __bpf_prog_enter + call bpf_func + call __bpf_prog_exit is ~50
+  * bytes on x86.  Pick a number to fit into BPF_IMAGE_SIZE / 2
+  */
+ #define BPF_MAX_TRAMP_PROGS 40
+ 
+ struct bpf_tramp_progs {
+ 	struct bpf_prog *progs[BPF_MAX_TRAMP_PROGS];
+ 	int nr_progs;
+ };
+ 
+ /* Different use cases for BPF trampoline:
+  * 1. replace nop at the function entry (kprobe equivalent)
+  *    flags = BPF_TRAMP_F_RESTORE_REGS
+  *    fentry = a set of programs to run before returning from trampoline
+  *
+  * 2. replace nop at the function entry (kprobe + kretprobe equivalent)
+  *    flags = BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_SKIP_FRAME
+  *    orig_call = fentry_ip + MCOUNT_INSN_SIZE
+  *    fentry = a set of program to run before calling original function
+  *    fexit = a set of program to run after original function
+  *
+  * 3. replace direct call instruction anywhere in the function body
+  *    or assign a function pointer for indirect call (like tcp_congestion_ops->cong_avoid)
+  *    With flags = 0
+  *      fentry = a set of programs to run before returning from trampoline
+  *    With flags = BPF_TRAMP_F_CALL_ORIG
+  *      orig_call = original callback addr or direct function addr
+  *      fentry = a set of program to run before calling original function
+  *      fexit = a set of program to run after original function
+  */
+ int arch_prepare_bpf_trampoline(void *image, void *image_end,
+ 				const struct btf_func_model *m, u32 flags,
+ 				struct bpf_tramp_progs *tprogs,
+ 				void *orig_call);
+ /* these two functions are called from generated trampoline */
+ u64 notrace __bpf_prog_enter(void);
+ void notrace __bpf_prog_exit(struct bpf_prog *prog, u64 start);
+ 
+ enum bpf_tramp_prog_type {
+ 	BPF_TRAMP_FENTRY,
+ 	BPF_TRAMP_FEXIT,
+ 	BPF_TRAMP_MODIFY_RETURN,
+ 	BPF_TRAMP_MAX,
+ 	BPF_TRAMP_REPLACE, /* more than MAX */
+ };
+ 
+ struct bpf_trampoline {
+ 	/* hlist for trampoline_table */
+ 	struct hlist_node hlist;
+ 	/* serializes access to fields of this trampoline */
+ 	struct mutex mutex;
+ 	refcount_t refcnt;
+ 	u64 key;
+ 	struct {
+ 		struct btf_func_model model;
+ 		void *addr;
+ 		bool ftrace_managed;
+ 	} func;
+ 	/* if !NULL this is BPF_PROG_TYPE_EXT program that extends another BPF
+ 	 * program by replacing one of its functions. func.addr is the address
+ 	 * of the function it replaced.
+ 	 */
+ 	struct bpf_prog *extension_prog;
+ 	/* list of BPF programs using this trampoline */
+ 	struct hlist_head progs_hlist[BPF_TRAMP_MAX];
+ 	/* Number of attached programs. A counter per kind. */
+ 	int progs_cnt[BPF_TRAMP_MAX];
+ 	/* Executable image of trampoline */
+ 	void *image;
+ 	u64 selector;
+ };
+ 
+ #define BPF_DISPATCHER_MAX 48 /* Fits in 2048B */
+ 
+ struct bpf_dispatcher_prog {
+ 	struct bpf_prog *prog;
+ 	refcount_t users;
+ };
+ 
+ struct bpf_dispatcher {
+ 	/* dispatcher mutex */
+ 	struct mutex mutex;
+ 	void *func;
+ 	struct bpf_dispatcher_prog progs[BPF_DISPATCHER_MAX];
+ 	int num_progs;
+ 	void *image;
+ 	u32 image_off;
+ };
+ 
+ static __always_inline unsigned int bpf_dispatcher_nopfunc(
+ 	const void *ctx,
+ 	const struct bpf_insn *insnsi,
+ 	unsigned int (*bpf_func)(const void *,
+ 				 const struct bpf_insn *))
+ {
+ 	return bpf_func(ctx, insnsi);
+ }
+ #ifdef CONFIG_BPF_JIT
+ struct bpf_trampoline *bpf_trampoline_lookup(u64 key);
+ int bpf_trampoline_link_prog(struct bpf_prog *prog);
+ int bpf_trampoline_unlink_prog(struct bpf_prog *prog);
+ void bpf_trampoline_put(struct bpf_trampoline *tr);
+ #define BPF_DISPATCHER_INIT(name) {			\
+ 	.mutex = __MUTEX_INITIALIZER(name.mutex),	\
+ 	.func = &name##func,				\
+ 	.progs = {},					\
+ 	.num_progs = 0,					\
+ 	.image = NULL,					\
+ 	.image_off = 0					\
+ }
+ 
+ #define DEFINE_BPF_DISPATCHER(name)					\
+ 	noinline unsigned int name##func(				\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *))	\
+ 	{								\
+ 		return bpf_func(ctx, insnsi);				\
+ 	}								\
+ 	EXPORT_SYMBOL(name##func);			\
+ 	struct bpf_dispatcher name = BPF_DISPATCHER_INIT(name);
+ #define DECLARE_BPF_DISPATCHER(name)					\
+ 	unsigned int name##func(					\
+ 		const void *ctx,					\
+ 		const struct bpf_insn *insnsi,				\
+ 		unsigned int (*bpf_func)(const void *,			\
+ 					 const struct bpf_insn *));	\
+ 	extern struct bpf_dispatcher name;
+ #define BPF_DISPATCHER_FUNC(name) name##func
+ #define BPF_DISPATCHER_PTR(name) (&name)
+ void bpf_dispatcher_change_prog(struct bpf_dispatcher *d, struct bpf_prog *from,
+ 				struct bpf_prog *to);
+ struct bpf_image {
+ 	struct latch_tree_node tnode;
+ 	unsigned char data[];
+ };
+ #define BPF_IMAGE_SIZE (PAGE_SIZE - sizeof(struct bpf_image))
+ bool is_bpf_image_address(unsigned long address);
+ void *bpf_image_alloc(void);
+ #else
+ static inline struct bpf_trampoline *bpf_trampoline_lookup(u64 key)
+ {
+ 	return NULL;
+ }
+ static inline int bpf_trampoline_link_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline int bpf_trampoline_unlink_prog(struct bpf_prog *prog)
+ {
+ 	return -ENOTSUPP;
+ }
+ static inline void bpf_trampoline_put(struct bpf_trampoline *tr) {}
+ #define DEFINE_BPF_DISPATCHER(name)
+ #define DECLARE_BPF_DISPATCHER(name)
+ #define BPF_DISPATCHER_FUNC(name) bpf_dispatcher_nopfunc
+ #define BPF_DISPATCHER_PTR(name) NULL
+ static inline void bpf_dispatcher_change_prog(struct bpf_dispatcher *d,
+ 					      struct bpf_prog *from,
+ 					      struct bpf_prog *to) {}
+ static inline bool is_bpf_image_address(unsigned long address)
+ {
+ 	return false;
+ }
+ #endif
+ 
+ struct bpf_func_info_aux {
+ 	u16 linkage;
+ 	bool unreliable;
+ };
+ 
+ enum bpf_jit_poke_reason {
+ 	BPF_POKE_REASON_TAIL_CALL,
+ };
+ 
+ /* Descriptor of pokes pointing /into/ the JITed image. */
+ struct bpf_jit_poke_descriptor {
+ 	void *ip;
+ 	union {
+ 		struct {
+ 			struct bpf_map *map;
+ 			u32 key;
+ 		} tail_call;
+ 	};
+ 	bool ip_stable;
+ 	u8 adj_off;
+ 	u16 reason;
+ };
+ 
++>>>>>>> ae24082331d9 (bpf: Introduce BPF_MODIFY_RETURN)
  struct bpf_prog_aux {
 -	atomic64_t refcnt;
 +	atomic_t refcnt;
  	u32 used_map_cnt;
  	u32 max_ctx_offset;
 -	u32 max_pkt_offset;
 -	u32 max_tp_access;
 +	/* not protected by KABI, safe to extend in the middle */
 +	RH_KABI_BROKEN_INSERT(u32 max_pkt_offset)
 +	RH_KABI_BROKEN_INSERT(u32 max_tp_access)
  	u32 stack_depth;
  	u32 id;
  	u32 func_cnt; /* used by non-func prog as the number of func progs */
diff --cc include/uapi/linux/bpf.h
index c9871d53e313,40b2d9476268..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -203,7 -207,10 +203,14 @@@ enum bpf_attach_type 
  	BPF_CGROUP_UDP6_RECVMSG,
  	BPF_CGROUP_GETSOCKOPT,
  	BPF_CGROUP_SETSOCKOPT,
++<<<<<<< HEAD
 +#endif /* __GENKSYMS__ */
++=======
+ 	BPF_TRACE_RAW_TP,
+ 	BPF_TRACE_FENTRY,
+ 	BPF_TRACE_FEXIT,
+ 	BPF_MODIFY_RETURN,
++>>>>>>> ae24082331d9 (bpf: Introduce BPF_MODIFY_RETURN)
  	__MAX_BPF_ATTACH_TYPE
  };
  
diff --cc kernel/bpf/btf.c
index d659110c662a,30841fb8b3c0..000000000000
--- a/kernel/bpf/btf.c
+++ b/kernel/bpf/btf.c
@@@ -3574,20 -3702,36 +3574,41 @@@ bool btf_ctx_access(int off, int size, 
  	}
  	arg = off / 8;
  	args = (const struct btf_param *)(t + 1);
 -	/* if (t == NULL) Fall back to default BPF prog with 5 u64 arguments */
 -	nr_args = t ? btf_type_vlen(t) : 5;
 -	if (prog->aux->attach_btf_trace) {
 -		/* skip first 'void *__data' argument in btf_trace_##name typedef */
 -		args++;
 -		nr_args--;
 +	/* skip first 'void *__data' argument in btf_trace_##name typedef */
 +	args++;
 +	nr_args = btf_type_vlen(t) - 1;
 +	if (arg >= nr_args) {
 +		bpf_log(log, "raw_tp '%s' doesn't have %d-th argument\n",
 +			tname, arg);
 +		return false;
  	}
  
++<<<<<<< HEAD
 +	t = btf_type_by_id(btf_vmlinux, args[arg].type);
++=======
+ 	if ((prog->expected_attach_type == BPF_TRACE_FEXIT ||
+ 	     prog->expected_attach_type == BPF_MODIFY_RETURN) &&
+ 	    arg == nr_args) {
+ 		if (!t)
+ 			/* Default prog with 5 args. 6th arg is retval. */
+ 			return true;
+ 		/* function return type */
+ 		t = btf_type_by_id(btf, t->type);
+ 	} else if (arg >= nr_args) {
+ 		bpf_log(log, "func '%s' doesn't have %d-th argument\n",
+ 			tname, arg + 1);
+ 		return false;
+ 	} else {
+ 		if (!t)
+ 			/* Default prog with 5 args */
+ 			return true;
+ 		t = btf_type_by_id(btf, args[arg].type);
+ 	}
++>>>>>>> ae24082331d9 (bpf: Introduce BPF_MODIFY_RETURN)
  	/* skip modifiers */
  	while (btf_type_is_modifier(t))
 -		t = btf_type_by_id(btf, t->type);
 -	if (btf_type_is_int(t) || btf_type_is_enum(t))
 +		t = btf_type_by_id(btf_vmlinux, t->type);
 +	if (btf_type_is_int(t))
  		/* accessing a scalar */
  		return true;
  	if (!btf_type_is_ptr(t)) {
diff --cc kernel/bpf/syscall.c
index b5b79e59cfd4,7ce0815793dd..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -1879,6 -2276,105 +1879,108 @@@ static const struct file_operations bpf
  	.write		= bpf_dummy_write,
  };
  
++<<<<<<< HEAD
++=======
+ int bpf_link_new_fd(struct bpf_link *link)
+ {
+ 	return anon_inode_getfd("bpf-link", &bpf_link_fops, link, O_CLOEXEC);
+ }
+ 
+ struct bpf_link *bpf_link_get_from_fd(u32 ufd)
+ {
+ 	struct fd f = fdget(ufd);
+ 	struct bpf_link *link;
+ 
+ 	if (!f.file)
+ 		return ERR_PTR(-EBADF);
+ 	if (f.file->f_op != &bpf_link_fops) {
+ 		fdput(f);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	link = f.file->private_data;
+ 	bpf_link_inc(link);
+ 	fdput(f);
+ 
+ 	return link;
+ }
+ 
+ struct bpf_tracing_link {
+ 	struct bpf_link link;
+ };
+ 
+ static void bpf_tracing_link_release(struct bpf_link *link)
+ {
+ 	struct bpf_tracing_link *tr_link =
+ 		container_of(link, struct bpf_tracing_link, link);
+ 
+ 	WARN_ON_ONCE(bpf_trampoline_unlink_prog(link->prog));
+ 	kfree(tr_link);
+ }
+ 
+ static const struct bpf_link_ops bpf_tracing_link_lops = {
+ 	.release = bpf_tracing_link_release,
+ };
+ 
+ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
+ {
+ 	struct bpf_tracing_link *link;
+ 	int link_fd, err;
+ 
+ 	if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
+ 	    prog->expected_attach_type != BPF_TRACE_FEXIT &&
+ 	    prog->expected_attach_type != BPF_MODIFY_RETURN &&
+ 	    prog->type != BPF_PROG_TYPE_EXT) {
+ 		err = -EINVAL;
+ 		goto out_put_prog;
+ 	}
+ 
+ 	link = kzalloc(sizeof(*link), GFP_USER);
+ 	if (!link) {
+ 		err = -ENOMEM;
+ 		goto out_put_prog;
+ 	}
+ 	bpf_link_init(&link->link, &bpf_tracing_link_lops, prog);
+ 
+ 	err = bpf_trampoline_link_prog(prog);
+ 	if (err)
+ 		goto out_free_link;
+ 
+ 	link_fd = bpf_link_new_fd(&link->link);
+ 	if (link_fd < 0) {
+ 		WARN_ON_ONCE(bpf_trampoline_unlink_prog(prog));
+ 		err = link_fd;
+ 		goto out_free_link;
+ 	}
+ 	return link_fd;
+ 
+ out_free_link:
+ 	kfree(link);
+ out_put_prog:
+ 	bpf_prog_put(prog);
+ 	return err;
+ }
+ 
+ struct bpf_raw_tp_link {
+ 	struct bpf_link link;
+ 	struct bpf_raw_event_map *btp;
+ };
+ 
+ static void bpf_raw_tp_link_release(struct bpf_link *link)
+ {
+ 	struct bpf_raw_tp_link *raw_tp =
+ 		container_of(link, struct bpf_raw_tp_link, link);
+ 
+ 	bpf_probe_unregister(raw_tp->btp, raw_tp->link.prog);
+ 	bpf_put_raw_tracepoint(raw_tp->btp);
+ 	kfree(raw_tp);
+ }
+ 
+ static const struct bpf_link_ops bpf_raw_tp_lops = {
+ 	.release = bpf_raw_tp_link_release,
+ };
+ 
++>>>>>>> ae24082331d9 (bpf: Introduce BPF_MODIFY_RETURN)
  #define BPF_RAW_TRACEPOINT_OPEN_LAST_FIELD raw_tracepoint.prog_fd
  
  static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
diff --cc kernel/bpf/verifier.c
index 1889f4bbcf69,2460c8e6b5be..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -9555,6 -9749,265 +9555,268 @@@ static void print_verification_stats(st
  		env->peak_states, env->longest_mark_read_walk);
  }
  
++<<<<<<< HEAD
++=======
+ static int check_struct_ops_btf_id(struct bpf_verifier_env *env)
+ {
+ 	const struct btf_type *t, *func_proto;
+ 	const struct bpf_struct_ops *st_ops;
+ 	const struct btf_member *member;
+ 	struct bpf_prog *prog = env->prog;
+ 	u32 btf_id, member_idx;
+ 	const char *mname;
+ 
+ 	btf_id = prog->aux->attach_btf_id;
+ 	st_ops = bpf_struct_ops_find(btf_id);
+ 	if (!st_ops) {
+ 		verbose(env, "attach_btf_id %u is not a supported struct\n",
+ 			btf_id);
+ 		return -ENOTSUPP;
+ 	}
+ 
+ 	t = st_ops->type;
+ 	member_idx = prog->expected_attach_type;
+ 	if (member_idx >= btf_type_vlen(t)) {
+ 		verbose(env, "attach to invalid member idx %u of struct %s\n",
+ 			member_idx, st_ops->name);
+ 		return -EINVAL;
+ 	}
+ 
+ 	member = &btf_type_member(t)[member_idx];
+ 	mname = btf_name_by_offset(btf_vmlinux, member->name_off);
+ 	func_proto = btf_type_resolve_func_ptr(btf_vmlinux, member->type,
+ 					       NULL);
+ 	if (!func_proto) {
+ 		verbose(env, "attach to invalid member %s(@idx %u) of struct %s\n",
+ 			mname, member_idx, st_ops->name);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (st_ops->check_member) {
+ 		int err = st_ops->check_member(t, member);
+ 
+ 		if (err) {
+ 			verbose(env, "attach to unsupported member %s of struct %s\n",
+ 				mname, st_ops->name);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	prog->aux->attach_func_proto = func_proto;
+ 	prog->aux->attach_func_name = mname;
+ 	env->ops = st_ops->verifier_ops;
+ 
+ 	return 0;
+ }
+ 
+ static int check_attach_btf_id(struct bpf_verifier_env *env)
+ {
+ 	struct bpf_prog *prog = env->prog;
+ 	bool prog_extension = prog->type == BPF_PROG_TYPE_EXT;
+ 	struct bpf_prog *tgt_prog = prog->aux->linked_prog;
+ 	u32 btf_id = prog->aux->attach_btf_id;
+ 	const char prefix[] = "btf_trace_";
+ 	int ret = 0, subprog = -1, i;
+ 	struct bpf_trampoline *tr;
+ 	const struct btf_type *t;
+ 	bool conservative = true;
+ 	const char *tname;
+ 	struct btf *btf;
+ 	long addr;
+ 	u64 key;
+ 
+ 	if (prog->type == BPF_PROG_TYPE_STRUCT_OPS)
+ 		return check_struct_ops_btf_id(env);
+ 
+ 	if (prog->type != BPF_PROG_TYPE_TRACING && !prog_extension)
+ 		return 0;
+ 
+ 	if (!btf_id) {
+ 		verbose(env, "Tracing programs must provide btf_id\n");
+ 		return -EINVAL;
+ 	}
+ 	btf = bpf_prog_get_target_btf(prog);
+ 	if (!btf) {
+ 		verbose(env,
+ 			"FENTRY/FEXIT program can only be attached to another program annotated with BTF\n");
+ 		return -EINVAL;
+ 	}
+ 	t = btf_type_by_id(btf, btf_id);
+ 	if (!t) {
+ 		verbose(env, "attach_btf_id %u is invalid\n", btf_id);
+ 		return -EINVAL;
+ 	}
+ 	tname = btf_name_by_offset(btf, t->name_off);
+ 	if (!tname) {
+ 		verbose(env, "attach_btf_id %u doesn't have a name\n", btf_id);
+ 		return -EINVAL;
+ 	}
+ 	if (tgt_prog) {
+ 		struct bpf_prog_aux *aux = tgt_prog->aux;
+ 
+ 		for (i = 0; i < aux->func_info_cnt; i++)
+ 			if (aux->func_info[i].type_id == btf_id) {
+ 				subprog = i;
+ 				break;
+ 			}
+ 		if (subprog == -1) {
+ 			verbose(env, "Subprog %s doesn't exist\n", tname);
+ 			return -EINVAL;
+ 		}
+ 		conservative = aux->func_info_aux[subprog].unreliable;
+ 		if (prog_extension) {
+ 			if (conservative) {
+ 				verbose(env,
+ 					"Cannot replace static functions\n");
+ 				return -EINVAL;
+ 			}
+ 			if (!prog->jit_requested) {
+ 				verbose(env,
+ 					"Extension programs should be JITed\n");
+ 				return -EINVAL;
+ 			}
+ 			env->ops = bpf_verifier_ops[tgt_prog->type];
+ 		}
+ 		if (!tgt_prog->jited) {
+ 			verbose(env, "Can attach to only JITed progs\n");
+ 			return -EINVAL;
+ 		}
+ 		if (tgt_prog->type == prog->type) {
+ 			/* Cannot fentry/fexit another fentry/fexit program.
+ 			 * Cannot attach program extension to another extension.
+ 			 * It's ok to attach fentry/fexit to extension program.
+ 			 */
+ 			verbose(env, "Cannot recursively attach\n");
+ 			return -EINVAL;
+ 		}
+ 		if (tgt_prog->type == BPF_PROG_TYPE_TRACING &&
+ 		    prog_extension &&
+ 		    (tgt_prog->expected_attach_type == BPF_TRACE_FENTRY ||
+ 		     tgt_prog->expected_attach_type == BPF_TRACE_FEXIT)) {
+ 			/* Program extensions can extend all program types
+ 			 * except fentry/fexit. The reason is the following.
+ 			 * The fentry/fexit programs are used for performance
+ 			 * analysis, stats and can be attached to any program
+ 			 * type except themselves. When extension program is
+ 			 * replacing XDP function it is necessary to allow
+ 			 * performance analysis of all functions. Both original
+ 			 * XDP program and its program extension. Hence
+ 			 * attaching fentry/fexit to BPF_PROG_TYPE_EXT is
+ 			 * allowed. If extending of fentry/fexit was allowed it
+ 			 * would be possible to create long call chain
+ 			 * fentry->extension->fentry->extension beyond
+ 			 * reasonable stack size. Hence extending fentry is not
+ 			 * allowed.
+ 			 */
+ 			verbose(env, "Cannot extend fentry/fexit\n");
+ 			return -EINVAL;
+ 		}
+ 		key = ((u64)aux->id) << 32 | btf_id;
+ 	} else {
+ 		if (prog_extension) {
+ 			verbose(env, "Cannot replace kernel functions\n");
+ 			return -EINVAL;
+ 		}
+ 		key = btf_id;
+ 	}
+ 
+ 	switch (prog->expected_attach_type) {
+ 	case BPF_TRACE_RAW_TP:
+ 		if (tgt_prog) {
+ 			verbose(env,
+ 				"Only FENTRY/FEXIT progs are attachable to another BPF prog\n");
+ 			return -EINVAL;
+ 		}
+ 		if (!btf_type_is_typedef(t)) {
+ 			verbose(env, "attach_btf_id %u is not a typedef\n",
+ 				btf_id);
+ 			return -EINVAL;
+ 		}
+ 		if (strncmp(prefix, tname, sizeof(prefix) - 1)) {
+ 			verbose(env, "attach_btf_id %u points to wrong type name %s\n",
+ 				btf_id, tname);
+ 			return -EINVAL;
+ 		}
+ 		tname += sizeof(prefix) - 1;
+ 		t = btf_type_by_id(btf, t->type);
+ 		if (!btf_type_is_ptr(t))
+ 			/* should never happen in valid vmlinux build */
+ 			return -EINVAL;
+ 		t = btf_type_by_id(btf, t->type);
+ 		if (!btf_type_is_func_proto(t))
+ 			/* should never happen in valid vmlinux build */
+ 			return -EINVAL;
+ 
+ 		/* remember two read only pointers that are valid for
+ 		 * the life time of the kernel
+ 		 */
+ 		prog->aux->attach_func_name = tname;
+ 		prog->aux->attach_func_proto = t;
+ 		prog->aux->attach_btf_trace = true;
+ 		return 0;
+ 	default:
+ 		if (!prog_extension)
+ 			return -EINVAL;
+ 		/* fallthrough */
+ 	case BPF_MODIFY_RETURN:
+ 	case BPF_TRACE_FENTRY:
+ 	case BPF_TRACE_FEXIT:
+ 		if (!btf_type_is_func(t)) {
+ 			verbose(env, "attach_btf_id %u is not a function\n",
+ 				btf_id);
+ 			return -EINVAL;
+ 		}
+ 		if (prog_extension &&
+ 		    btf_check_type_match(env, prog, btf, t))
+ 			return -EINVAL;
+ 		t = btf_type_by_id(btf, t->type);
+ 		if (!btf_type_is_func_proto(t))
+ 			return -EINVAL;
+ 		tr = bpf_trampoline_lookup(key);
+ 		if (!tr)
+ 			return -ENOMEM;
+ 		prog->aux->attach_func_name = tname;
+ 		/* t is either vmlinux type or another program's type */
+ 		prog->aux->attach_func_proto = t;
+ 		mutex_lock(&tr->mutex);
+ 		if (tr->func.addr) {
+ 			prog->aux->trampoline = tr;
+ 			goto out;
+ 		}
+ 		if (tgt_prog && conservative) {
+ 			prog->aux->attach_func_proto = NULL;
+ 			t = NULL;
+ 		}
+ 		ret = btf_distill_func_proto(&env->log, btf, t,
+ 					     tname, &tr->func.model);
+ 		if (ret < 0)
+ 			goto out;
+ 		if (tgt_prog) {
+ 			if (subprog == 0)
+ 				addr = (long) tgt_prog->bpf_func;
+ 			else
+ 				addr = (long) tgt_prog->aux->func[subprog]->bpf_func;
+ 		} else {
+ 			addr = kallsyms_lookup_name(tname);
+ 			if (!addr) {
+ 				verbose(env,
+ 					"The address of function %s cannot be found\n",
+ 					tname);
+ 				ret = -ENOENT;
+ 				goto out;
+ 			}
+ 		}
+ 		tr->func.addr = (void *)addr;
+ 		prog->aux->trampoline = tr;
+ out:
+ 		mutex_unlock(&tr->mutex);
+ 		if (ret)
+ 			bpf_trampoline_put(tr);
+ 		return ret;
+ 	}
+ }
+ 
++>>>>>>> ae24082331d9 (bpf: Introduce BPF_MODIFY_RETURN)
  int bpf_check(struct bpf_prog **prog, union bpf_attr *attr,
  	      union bpf_attr __user *uattr)
  {
diff --cc tools/include/uapi/linux/bpf.h
index 2bc35095902a,40b2d9476268..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -199,6 -207,10 +199,13 @@@ enum bpf_attach_type 
  	BPF_CGROUP_UDP6_RECVMSG,
  	BPF_CGROUP_GETSOCKOPT,
  	BPF_CGROUP_SETSOCKOPT,
++<<<<<<< HEAD
++=======
+ 	BPF_TRACE_RAW_TP,
+ 	BPF_TRACE_FENTRY,
+ 	BPF_TRACE_FEXIT,
+ 	BPF_MODIFY_RETURN,
++>>>>>>> ae24082331d9 (bpf: Introduce BPF_MODIFY_RETURN)
  	__MAX_BPF_ATTACH_TYPE
  };
  
* Unmerged path kernel/bpf/trampoline.c
* Unmerged path arch/x86/net/bpf_jit_comp.c
* Unmerged path include/linux/bpf.h
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/bpf/btf.c
* Unmerged path kernel/bpf/syscall.c
* Unmerged path kernel/bpf/trampoline.c
* Unmerged path kernel/bpf/verifier.c
* Unmerged path tools/include/uapi/linux/bpf.h
