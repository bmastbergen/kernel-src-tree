selftests/bpf: Add bpf_read_branch_records() selftest

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Daniel Xu <dxu@dxuuu.xyz>
commit 67306f84ca78c2ca5136f21791710c126a55a19b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/67306f84.failed

Add a selftest to test:

* default bpf_read_branch_records() behavior
* BPF_F_GET_BRANCH_RECORDS_SIZE flag behavior
* error path on non branch record perf events
* using helper to write to stack
* using helper to write to global

On host with hardware counter support:

    # ./test_progs -t perf_branches
    #27/1 perf_branches_hw:OK
    #27/2 perf_branches_no_hw:OK
    #27 perf_branches:OK
    Summary: 1/2 PASSED, 0 SKIPPED, 0 FAILED

On host without hardware counter support (VM):

    # ./test_progs -t perf_branches
    #27/1 perf_branches_hw:OK
    #27/2 perf_branches_no_hw:OK
    #27 perf_branches:OK
    Summary: 1/2 PASSED, 1 SKIPPED, 0 FAILED

Also sync tools/include/uapi/linux/bpf.h.

	Signed-off-by: Daniel Xu <dxu@dxuuu.xyz>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
Link: https://lore.kernel.org/bpf/20200218030432.4600-3-dxu@dxuuu.xyz
(cherry picked from commit 67306f84ca78c2ca5136f21791710c126a55a19b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/include/uapi/linux/bpf.h
diff --cc tools/include/uapi/linux/bpf.h
index 2bc35095902a,a7e59756853f..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -2718,6 -2748,169 +2718,172 @@@ union bpf_attr 
   *		**-EPERM** if no permission to send the *sig*.
   *
   *		**-EAGAIN** if bpf program can try again.
++<<<<<<< HEAD
++=======
+  *
+  * s64 bpf_tcp_gen_syncookie(struct bpf_sock *sk, void *iph, u32 iph_len, struct tcphdr *th, u32 th_len)
+  *	Description
+  *		Try to issue a SYN cookie for the packet with corresponding
+  *		IP/TCP headers, *iph* and *th*, on the listening socket in *sk*.
+  *
+  *		*iph* points to the start of the IPv4 or IPv6 header, while
+  *		*iph_len* contains **sizeof**\ (**struct iphdr**) or
+  *		**sizeof**\ (**struct ip6hdr**).
+  *
+  *		*th* points to the start of the TCP header, while *th_len*
+  *		contains the length of the TCP header.
+  *
+  *	Return
+  *		On success, lower 32 bits hold the generated SYN cookie in
+  *		followed by 16 bits which hold the MSS value for that cookie,
+  *		and the top 16 bits are unused.
+  *
+  *		On failure, the returned value is one of the following:
+  *
+  *		**-EINVAL** SYN cookie cannot be issued due to error
+  *
+  *		**-ENOENT** SYN cookie should not be issued (no SYN flood)
+  *
+  *		**-EOPNOTSUPP** kernel configuration does not enable SYN cookies
+  *
+  *		**-EPROTONOSUPPORT** IP packet version is not 4 or 6
+  *
+  * int bpf_skb_output(void *ctx, struct bpf_map *map, u64 flags, void *data, u64 size)
+  * 	Description
+  * 		Write raw *data* blob into a special BPF perf event held by
+  * 		*map* of type **BPF_MAP_TYPE_PERF_EVENT_ARRAY**. This perf
+  * 		event must have the following attributes: **PERF_SAMPLE_RAW**
+  * 		as **sample_type**, **PERF_TYPE_SOFTWARE** as **type**, and
+  * 		**PERF_COUNT_SW_BPF_OUTPUT** as **config**.
+  *
+  * 		The *flags* are used to indicate the index in *map* for which
+  * 		the value must be put, masked with **BPF_F_INDEX_MASK**.
+  * 		Alternatively, *flags* can be set to **BPF_F_CURRENT_CPU**
+  * 		to indicate that the index of the current CPU core should be
+  * 		used.
+  *
+  * 		The value to write, of *size*, is passed through eBPF stack and
+  * 		pointed by *data*.
+  *
+  * 		*ctx* is a pointer to in-kernel struct sk_buff.
+  *
+  * 		This helper is similar to **bpf_perf_event_output**\ () but
+  * 		restricted to raw_tracepoint bpf programs.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from user space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_kernel(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Safely attempt to read *size* bytes from kernel space address
+  * 		*unsafe_ptr* and store the data in *dst*.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_probe_read_user_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe user address
+  * 		*unsafe_ptr* to *dst*. The *size* should include the
+  * 		terminating NUL byte. In case the string length is smaller than
+  * 		*size*, the target is not padded with further NUL bytes. If the
+  * 		string length is larger than *size*, just *size*-1 bytes are
+  * 		copied and the last byte is set to NUL.
+  *
+  * 		On success, the length of the copied string is returned. This
+  * 		makes this helper useful in tracing programs for reading
+  * 		strings, and more importantly to get its length at runtime. See
+  * 		the following snippet:
+  *
+  * 		::
+  *
+  * 			SEC("kprobe/sys_open")
+  * 			void bpf_sys_open(struct pt_regs *ctx)
+  * 			{
+  * 			        char buf[PATHLEN]; // PATHLEN is defined to 256
+  * 			        int res = bpf_probe_read_user_str(buf, sizeof(buf),
+  * 				                                  ctx->di);
+  *
+  * 				// Consume buf, for example push it to
+  * 				// userspace via bpf_perf_event_output(); we
+  * 				// can use res (the string length) as event
+  * 				// size, after checking its boundaries.
+  * 			}
+  *
+  * 		In comparison, using **bpf_probe_read_user()** helper here
+  * 		instead to read the string would require to estimate the length
+  * 		at compile time, and would often result in copying more memory
+  * 		than necessary.
+  *
+  * 		Another useful use case is when parsing individual process
+  * 		arguments or individual environment variables navigating
+  * 		*current*\ **->mm->arg_start** and *current*\
+  * 		**->mm->env_start**: using this helper and the return value,
+  * 		one can quickly iterate at the right offset of the memory area.
+  * 	Return
+  * 		On success, the strictly positive length of the string,
+  * 		including the trailing NUL character. On error, a negative
+  * 		value.
+  *
+  * int bpf_probe_read_kernel_str(void *dst, u32 size, const void *unsafe_ptr)
+  * 	Description
+  * 		Copy a NUL terminated string from an unsafe kernel address *unsafe_ptr*
+  * 		to *dst*. Same semantics as with bpf_probe_read_user_str() apply.
+  * 	Return
+  * 		On success, the strictly positive length of the string,	including
+  * 		the trailing NUL character. On error, a negative value.
+  *
+  * int bpf_tcp_send_ack(void *tp, u32 rcv_nxt)
+  *	Description
+  *		Send out a tcp-ack. *tp* is the in-kernel struct tcp_sock.
+  *		*rcv_nxt* is the ack_seq to be sent out.
+  *	Return
+  *		0 on success, or a negative error in case of failure.
+  *
+  * int bpf_send_signal_thread(u32 sig)
+  *	Description
+  *		Send signal *sig* to the thread corresponding to the current task.
+  *	Return
+  *		0 on success or successfully queued.
+  *
+  *		**-EBUSY** if work queue under nmi is full.
+  *
+  *		**-EINVAL** if *sig* is invalid.
+  *
+  *		**-EPERM** if no permission to send the *sig*.
+  *
+  *		**-EAGAIN** if bpf program can try again.
+  *
+  * u64 bpf_jiffies64(void)
+  *	Description
+  *		Obtain the 64bit jiffies
+  *	Return
+  *		The 64 bit jiffies
+  *
+  * int bpf_read_branch_records(struct bpf_perf_event_data *ctx, void *buf, u32 size, u64 flags)
+  *	Description
+  *		For an eBPF program attached to a perf event, retrieve the
+  *		branch records (struct perf_branch_entry) associated to *ctx*
+  *		and store it in	the buffer pointed by *buf* up to size
+  *		*size* bytes.
+  *	Return
+  *		On success, number of bytes written to *buf*. On error, a
+  *		negative value.
+  *
+  *		The *flags* can be set to **BPF_F_GET_BRANCH_RECORDS_SIZE** to
+  *		instead	return the number of bytes required to store all the
+  *		branch entries. If this flag is set, *buf* may be NULL.
+  *
+  *		**-EINVAL** if arguments invalid or **size** not a multiple
+  *		of sizeof(struct perf_branch_entry).
+  *
+  *		**-ENOENT** if architecture does not support branch records.
++>>>>>>> 67306f84ca78 (selftests/bpf: Add bpf_read_branch_records() selftest)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -2829,7 -3022,17 +2995,21 @@@
  	FN(strtoul),			\
  	FN(sk_storage_get),		\
  	FN(sk_storage_delete),		\
++<<<<<<< HEAD
 +	FN(send_signal),
++=======
+ 	FN(send_signal),		\
+ 	FN(tcp_gen_syncookie),		\
+ 	FN(skb_output),			\
+ 	FN(probe_read_user),		\
+ 	FN(probe_read_kernel),		\
+ 	FN(probe_read_user_str),	\
+ 	FN(probe_read_kernel_str),	\
+ 	FN(tcp_send_ack),		\
+ 	FN(send_signal_thread),		\
+ 	FN(jiffies64),			\
+ 	FN(read_branch_records),
++>>>>>>> 67306f84ca78 (selftests/bpf: Add bpf_read_branch_records() selftest)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
* Unmerged path tools/include/uapi/linux/bpf.h
diff --git a/tools/testing/selftests/bpf/prog_tests/perf_branches.c b/tools/testing/selftests/bpf/prog_tests/perf_branches.c
new file mode 100644
index 000000000000..e35c444902a7
--- /dev/null
+++ b/tools/testing/selftests/bpf/prog_tests/perf_branches.c
@@ -0,0 +1,170 @@
+// SPDX-License-Identifier: GPL-2.0
+#define _GNU_SOURCE
+#include <pthread.h>
+#include <sched.h>
+#include <sys/socket.h>
+#include <test_progs.h>
+#include "bpf/libbpf_internal.h"
+#include "test_perf_branches.skel.h"
+
+static void check_good_sample(struct test_perf_branches *skel)
+{
+	int written_global = skel->bss->written_global_out;
+	int required_size = skel->bss->required_size_out;
+	int written_stack = skel->bss->written_stack_out;
+	int pbe_size = sizeof(struct perf_branch_entry);
+	int duration = 0;
+
+	if (CHECK(!skel->bss->valid, "output not valid",
+		 "no valid sample from prog"))
+		return;
+
+	/*
+	 * It's hard to validate the contents of the branch entries b/c it
+	 * would require some kind of disassembler and also encoding the
+	 * valid jump instructions for supported architectures. So just check
+	 * the easy stuff for now.
+	 */
+	CHECK(required_size <= 0, "read_branches_size", "err %d\n", required_size);
+	CHECK(written_stack < 0, "read_branches_stack", "err %d\n", written_stack);
+	CHECK(written_stack % pbe_size != 0, "read_branches_stack",
+	      "stack bytes written=%d not multiple of struct size=%d\n",
+	      written_stack, pbe_size);
+	CHECK(written_global < 0, "read_branches_global", "err %d\n", written_global);
+	CHECK(written_global % pbe_size != 0, "read_branches_global",
+	      "global bytes written=%d not multiple of struct size=%d\n",
+	      written_global, pbe_size);
+	CHECK(written_global < written_stack, "read_branches_size",
+	      "written_global=%d < written_stack=%d\n", written_global, written_stack);
+}
+
+static void check_bad_sample(struct test_perf_branches *skel)
+{
+	int written_global = skel->bss->written_global_out;
+	int required_size = skel->bss->required_size_out;
+	int written_stack = skel->bss->written_stack_out;
+	int duration = 0;
+
+	if (CHECK(!skel->bss->valid, "output not valid",
+		 "no valid sample from prog"))
+		return;
+
+	CHECK((required_size != -EINVAL && required_size != -ENOENT),
+	      "read_branches_size", "err %d\n", required_size);
+	CHECK((written_stack != -EINVAL && written_stack != -ENOENT),
+	      "read_branches_stack", "written %d\n", written_stack);
+	CHECK((written_global != -EINVAL && written_global != -ENOENT),
+	      "read_branches_global", "written %d\n", written_global);
+}
+
+static void test_perf_branches_common(int perf_fd,
+				      void (*cb)(struct test_perf_branches *))
+{
+	struct test_perf_branches *skel;
+	int err, i, duration = 0;
+	bool detached = false;
+	struct bpf_link *link;
+	volatile int j = 0;
+	cpu_set_t cpu_set;
+
+	skel = test_perf_branches__open_and_load();
+	if (CHECK(!skel, "test_perf_branches_load",
+		  "perf_branches skeleton failed\n"))
+		return;
+
+	/* attach perf_event */
+	link = bpf_program__attach_perf_event(skel->progs.perf_branches, perf_fd);
+	if (CHECK(IS_ERR(link), "attach_perf_event", "err %ld\n", PTR_ERR(link)))
+		goto out_destroy_skel;
+
+	/* generate some branches on cpu 0 */
+	CPU_ZERO(&cpu_set);
+	CPU_SET(0, &cpu_set);
+	err = pthread_setaffinity_np(pthread_self(), sizeof(cpu_set), &cpu_set);
+	if (CHECK(err, "set_affinity", "cpu #0, err %d\n", err))
+		goto out_destroy;
+	/* spin the loop for a while (random high number) */
+	for (i = 0; i < 1000000; ++i)
+		++j;
+
+	test_perf_branches__detach(skel);
+	detached = true;
+
+	cb(skel);
+out_destroy:
+	bpf_link__destroy(link);
+out_destroy_skel:
+	if (!detached)
+		test_perf_branches__detach(skel);
+	test_perf_branches__destroy(skel);
+}
+
+static void test_perf_branches_hw(void)
+{
+	struct perf_event_attr attr = {0};
+	int duration = 0;
+	int pfd;
+
+	/* create perf event */
+	attr.size = sizeof(attr);
+	attr.type = PERF_TYPE_HARDWARE;
+	attr.config = PERF_COUNT_HW_CPU_CYCLES;
+	attr.freq = 1;
+	attr.sample_freq = 4000;
+	attr.sample_type = PERF_SAMPLE_BRANCH_STACK;
+	attr.branch_sample_type = PERF_SAMPLE_BRANCH_USER | PERF_SAMPLE_BRANCH_ANY;
+	pfd = syscall(__NR_perf_event_open, &attr, -1, 0, -1, PERF_FLAG_FD_CLOEXEC);
+
+	/*
+	 * Some setups don't support branch records (virtual machines, !x86),
+	 * so skip test in this case.
+	 */
+	if (pfd == -1) {
+		if (errno == ENOENT || errno == EOPNOTSUPP) {
+			printf("%s:SKIP:no PERF_SAMPLE_BRANCH_STACK\n",
+			       __func__);
+			test__skip();
+			return;
+		}
+		if (CHECK(pfd < 0, "perf_event_open", "err %d errno %d\n",
+			  pfd, errno))
+			return;
+	}
+
+	test_perf_branches_common(pfd, check_good_sample);
+
+	close(pfd);
+}
+
+/*
+ * Tests negative case -- run bpf_read_branch_records() on improperly configured
+ * perf event.
+ */
+static void test_perf_branches_no_hw(void)
+{
+	struct perf_event_attr attr = {0};
+	int duration = 0;
+	int pfd;
+
+	/* create perf event */
+	attr.size = sizeof(attr);
+	attr.type = PERF_TYPE_SOFTWARE;
+	attr.config = PERF_COUNT_SW_CPU_CLOCK;
+	attr.freq = 1;
+	attr.sample_freq = 4000;
+	pfd = syscall(__NR_perf_event_open, &attr, -1, 0, -1, PERF_FLAG_FD_CLOEXEC);
+	if (CHECK(pfd < 0, "perf_event_open", "err %d\n", pfd))
+		return;
+
+	test_perf_branches_common(pfd, check_bad_sample);
+
+	close(pfd);
+}
+
+void test_perf_branches(void)
+{
+	if (test__start_subtest("perf_branches_hw"))
+		test_perf_branches_hw();
+	if (test__start_subtest("perf_branches_no_hw"))
+		test_perf_branches_no_hw();
+}
diff --git a/tools/testing/selftests/bpf/progs/test_perf_branches.c b/tools/testing/selftests/bpf/progs/test_perf_branches.c
new file mode 100644
index 000000000000..0f7e27d97567
--- /dev/null
+++ b/tools/testing/selftests/bpf/progs/test_perf_branches.c
@@ -0,0 +1,50 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (c) 2019 Facebook
+
+#include <stddef.h>
+#include <linux/ptrace.h>
+#include <linux/bpf.h>
+#include <bpf/bpf_helpers.h>
+#include "bpf_trace_helpers.h"
+
+int valid = 0;
+int required_size_out = 0;
+int written_stack_out = 0;
+int written_global_out = 0;
+
+struct {
+	__u64 _a;
+	__u64 _b;
+	__u64 _c;
+} fpbe[30] = {0};
+
+SEC("perf_event")
+int perf_branches(void *ctx)
+{
+	__u64 entries[4 * 3] = {0};
+	int required_size, written_stack, written_global;
+
+	/* write to stack */
+	written_stack = bpf_read_branch_records(ctx, entries, sizeof(entries), 0);
+	/* ignore spurious events */
+	if (!written_stack)
+		return 1;
+
+	/* get required size */
+	required_size = bpf_read_branch_records(ctx, NULL, 0,
+						BPF_F_GET_BRANCH_RECORDS_SIZE);
+
+	written_global = bpf_read_branch_records(ctx, fpbe, sizeof(fpbe), 0);
+	/* ignore spurious events */
+	if (!written_global)
+		return 1;
+
+	required_size_out = required_size;
+	written_stack_out = written_stack;
+	written_global_out = written_global;
+	valid = 1;
+
+	return 0;
+}
+
+char _license[] SEC("license") = "GPL";
