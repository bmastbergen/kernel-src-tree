KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 293e306e7faac4eafaefb9518a1cd6eaecad88e9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/293e306e.failed

Fold max_mapping_level() into kvm_mmu_hugepage_adjust() now that HugeTLB
mappings are handled in kvm_mmu_hugepage_adjust(), i.e. there isn't a
need to pre-calculate the max mapping level.  Co-locating all hugepage
checks eliminates a memslot lookup, at the cost of performing the
__mmu_gfn_lpage_is_disallowed() checks while holding mmu_lock.

The latency of lpage_is_disallowed() is likely negligible relative to
the rest of the code run while holding mmu_lock, and can be offset to
some extent by eliminating the mmu_gfn_lpage_is_disallowed() check in
set_spte() in a future patch.  Eliminating the check in set_spte() is
made possible by performing the initial lpage_is_disallowed() checks
while holding mmu_lock.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 293e306e7faac4eafaefb9518a1cd6eaecad88e9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/paging_tmpl.h
diff --cc arch/x86/kvm/mmu/mmu.c
index f41b4d90aa8d,812c69f7f552..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -1330,45 -1310,6 +1330,48 @@@ gfn_to_memslot_dirty_bitmap(struct kvm_
  	return slot;
  }
  
++<<<<<<< HEAD
 +static int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn,
 +			 int *max_levelp)
 +{
 +	int host_level, max_level = *max_levelp;
 +	struct kvm_memory_slot *slot;
 +
 +	if (unlikely(max_level == PT_PAGE_TABLE_LEVEL))
 +		return PT_PAGE_TABLE_LEVEL;
 +
 +	slot = kvm_vcpu_gfn_to_memslot(vcpu, large_gfn);
 +	if (!memslot_valid_for_gpte(slot, true)) {
 +		*max_levelp = PT_PAGE_TABLE_LEVEL;
 +		return PT_PAGE_TABLE_LEVEL;
 +	}
 +
 +	max_level = min(max_level, kvm_x86_ops->get_lpage_level());
 +	for ( ; max_level > PT_PAGE_TABLE_LEVEL; max_level--) {
 +		if (!__mmu_gfn_lpage_is_disallowed(large_gfn, max_level, slot))
 +			break;
 +	}
 +
 +	*max_levelp = max_level;
 +
 +	if (max_level == PT_PAGE_TABLE_LEVEL)
 +		return PT_PAGE_TABLE_LEVEL;
 +
 +	/*
 +	 * Note, host_mapping_level() does *not* handle transparent huge pages.
 +	 * As suggested by "mapping", it reflects the page size established by
 +	 * the associated vma, if there is one, i.e. host_mapping_level() will
 +	 * return a huge page level if and only if a vma exists and the backing
 +	 * implementation for the vma uses huge pages, e.g. hugetlbfs and dax.
 +	 * So, do not propagate host_mapping_level() to max_level as KVM can
 +	 * still promote the guest mapping to a huge page in the THP case.
 +	 */
 +	host_level = host_mapping_level(vcpu, large_gfn);
 +	return min(host_level, max_level);
 +}
 +
++=======
++>>>>>>> 293e306e7faa (KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust())
  /*
   * About rmap_head encoding:
   *
@@@ -3139,10 -3080,11 +3142,18 @@@ static int set_spte(struct kvm_vcpu *vc
  	if (pte_access & ACC_WRITE_MASK) {
  
  		/*
++<<<<<<< HEAD
 +		 * Other vcpu creates new sp in the window between
 +		 * mapping_level() and acquiring mmu-lock. We can
 +		 * allow guest to retry the access, the mapping can
 +		 * be fixed if guest refault.
++=======
+ 		 * Legacy code to handle an obsolete scenario where a different
+ 		 * vcpu creates new sp in the window between this vcpu's query
+ 		 * of lpage_is_disallowed() and acquiring mmu_lock.  No longer
+ 		 * necessary now that lpage_is_disallowed() is called after
+ 		 * acquiring mmu_lock.
++>>>>>>> 293e306e7faa (KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust())
  		 */
  		if (level > PT_PAGE_TABLE_LEVEL &&
  		    mmu_gfn_lpage_is_disallowed(vcpu, gfn, level))
@@@ -3332,33 -3274,80 +3343,101 @@@ static void direct_pte_prefetch(struct 
  	__direct_pte_prefetch(vcpu, sp, sptep);
  }
  
++<<<<<<< HEAD
 +static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,
 +					gfn_t gfn, kvm_pfn_t *pfnp,
 +					int *levelp)
++=======
+ static int host_pfn_mapping_level(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 				  kvm_pfn_t pfn, struct kvm_memory_slot *slot)
  {
+ 	unsigned long hva;
+ 	pte_t *pte;
+ 	int level;
+ 
+ 	BUILD_BUG_ON(PT_PAGE_TABLE_LEVEL != (int)PG_LEVEL_4K ||
+ 		     PT_DIRECTORY_LEVEL != (int)PG_LEVEL_2M ||
+ 		     PT_PDPE_LEVEL != (int)PG_LEVEL_1G);
+ 
+ 	if (!PageCompound(pfn_to_page(pfn)))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	/*
+ 	 * Note, using the already-retrieved memslot and __gfn_to_hva_memslot()
+ 	 * is not solely for performance, it's also necessary to avoid the
+ 	 * "writable" check in __gfn_to_hva_many(), which will always fail on
+ 	 * read-only memslots due to gfn_to_hva() assuming writes.  Earlier
+ 	 * page fault steps have already verified the guest isn't writing a
+ 	 * read-only memslot.
+ 	 */
+ 	hva = __gfn_to_hva_memslot(slot, gfn);
+ 
+ 	pte = lookup_address_in_mm(vcpu->kvm->mm, hva, &level);
+ 	if (unlikely(!pte))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	return level;
+ }
+ 
+ static int kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 				   int max_level, kvm_pfn_t *pfnp)
++>>>>>>> 293e306e7faa (KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust())
+ {
+ 	struct kvm_memory_slot *slot;
  	kvm_pfn_t pfn = *pfnp;
++<<<<<<< HEAD
 +	int level = *levelp;
++=======
+ 	kvm_pfn_t mask;
+ 	int level;
+ 
+ 	if (unlikely(max_level == PT_PAGE_TABLE_LEVEL))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	if (is_error_noslot_pfn(pfn) || kvm_is_reserved_pfn(pfn) ||
+ 	    kvm_is_zone_device_pfn(pfn))
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, true);
+ 	if (!slot)
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	max_level = min(max_level, kvm_x86_ops->get_lpage_level());
+ 	for ( ; max_level > PT_PAGE_TABLE_LEVEL; max_level--) {
+ 		if (!__mmu_gfn_lpage_is_disallowed(gfn, max_level, slot))
+ 			break;
+ 	}
+ 
+ 	if (max_level == PT_PAGE_TABLE_LEVEL)
+ 		return PT_PAGE_TABLE_LEVEL;
+ 
+ 	level = host_pfn_mapping_level(vcpu, gfn, pfn, slot);
+ 	if (level == PT_PAGE_TABLE_LEVEL)
+ 		return level;
+ 
+ 	level = min(level, max_level);
++>>>>>>> 293e306e7faa (KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust())
  
  	/*
 -	 * mmu_notifier_retry() was successful and mmu_lock is held, so
 -	 * the pmd can't be split from under us.
 +	 * Check if it's a transparent hugepage. If this would be an
 +	 * hugetlbfs page, level wouldn't be set to
 +	 * PT_PAGE_TABLE_LEVEL and there would be no adjustment done
 +	 * here.
  	 */
 -	mask = KVM_PAGES_PER_HPAGE(level) - 1;
 -	VM_BUG_ON((gfn & mask) != (pfn & mask));
 -	*pfnp = pfn & ~mask;
 +	if (!is_error_noslot_pfn(pfn) && !kvm_is_reserved_pfn(pfn) &&
 +	    !kvm_is_zone_device_pfn(pfn) && level == PT_PAGE_TABLE_LEVEL &&
 +	    PageTransCompoundMap(pfn_to_page(pfn))) {
 +		unsigned long mask;
  
 -	return level;
 +		/*
 +		 * mmu_notifier_retry() was successful and mmu_lock is held, so
 +		 * the pmd can't be split from under us.
 +		 */
 +		*levelp = level = PT_DIRECTORY_LEVEL;
 +		mask = KVM_PAGES_PER_HPAGE(level) - 1;
 +		VM_BUG_ON((gfn & mask) != (pfn & mask));
 +		*pfnp = pfn & ~mask;
 +	}
  }
  
  static void disallowed_hugepage_adjust(struct kvm_shadow_walk_iterator it,
@@@ -4187,10 -4174,6 +4266,13 @@@ static int direct_page_fault(struct kvm
  	if (lpage_disallowed)
  		max_level = PT_PAGE_TABLE_LEVEL;
  
++<<<<<<< HEAD
 +	level = mapping_level(vcpu, gfn, &max_level);
 +	if (level > PT_PAGE_TABLE_LEVEL)
 +		gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
 +
++=======
++>>>>>>> 293e306e7faa (KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust())
  	if (fast_page_fault(vcpu, gpa, error_code))
  		return RET_PF_RETRY;
  
diff --cc arch/x86/kvm/mmu/paging_tmpl.h
index 64abb6a4320c,4e1ef0473663..000000000000
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@@ -844,10 -832,6 +844,13 @@@ static int FNAME(page_fault)(struct kvm
  	else
  		max_level = walker.level;
  
++<<<<<<< HEAD
 +	level = mapping_level(vcpu, walker.gfn, &max_level);
 +	if (level > PT_PAGE_TABLE_LEVEL)
 +		walker.gfn = walker.gfn & ~(KVM_PAGES_PER_HPAGE(level) - 1);
 +
++=======
++>>>>>>> 293e306e7faa (KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust())
  	mmu_seq = vcpu->kvm->mmu_notifier_seq;
  	smp_rmb();
  
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/paging_tmpl.h
