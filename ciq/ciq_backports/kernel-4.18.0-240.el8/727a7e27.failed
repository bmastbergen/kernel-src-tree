KVM: x86: rename set_cr3 callback and related flags to load_mmu_pgd

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 727a7e27cf88a261c5a0f14f4f9ee4d767352766
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/727a7e27.failed

The set_cr3 callback is not setting the guest CR3, it is setting the
root of the guest page tables, either shadow or two-dimensional.
To make this clearer as well as to indicate that the MMU calls it
via kvm_mmu_load_cr3, rename it to load_mmu_pgd.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 727a7e27cf88a261c5a0f14f4f9ee4d767352766)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.h
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/mmu.h
index a647601c9e1c,e6bfe79e94d8..000000000000
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@@ -95,11 -95,11 +95,16 @@@ static inline unsigned long kvm_get_act
  	return kvm_get_pcid(vcpu, kvm_read_cr3(vcpu));
  }
  
 -static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 +static inline void kvm_mmu_load_cr3(struct kvm_vcpu *vcpu)
  {
  	if (VALID_PAGE(vcpu->arch.mmu->root_hpa))
++<<<<<<< HEAD
 +		vcpu->arch.mmu->set_cr3(vcpu, vcpu->arch.mmu->root_hpa |
 +					      kvm_get_active_pcid(vcpu));
++=======
+ 		kvm_x86_ops->load_mmu_pgd(vcpu, vcpu->arch.mmu->root_hpa |
+ 					        kvm_get_active_pcid(vcpu));
++>>>>>>> 727a7e27cf88 (KVM: x86: rename set_cr3 callback and related flags to load_mmu_pgd)
  }
  
  int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
diff --cc arch/x86/kvm/svm.c
index 720353d5a356,9b983162af73..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -5952,24 -5942,30 +5952,24 @@@ static void svm_vcpu_run(struct kvm_vcp
  }
  STACK_FRAME_NON_STANDARD(svm_vcpu_run);
  
- static void svm_set_cr3(struct kvm_vcpu *vcpu, unsigned long root)
+ static void svm_load_mmu_pgd(struct kvm_vcpu *vcpu, unsigned long root)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	bool update_guest_cr3 = true;
 -	unsigned long cr3;
  
 -	cr3 = __sme_set(root);
 -	if (npt_enabled) {
 -		svm->vmcb->control.nested_cr3 = cr3;
 -		mark_dirty(svm->vmcb, VMCB_NPT);
 +	svm->vmcb->save.cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_CR);
 +}
  
 -		/* Loading L2's CR3 is handled by enter_svm_guest_mode.  */
 -		if (is_guest_mode(vcpu))
 -			update_guest_cr3 = false;
 -		else if (test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))
 -			cr3 = vcpu->arch.cr3;
 -		else /* CR3 is already up-to-date.  */
 -			update_guest_cr3 = false;
 -	}
 +static void set_tdp_cr3(struct kvm_vcpu *vcpu, unsigned long root)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
  
 -	if (update_guest_cr3) {
 -		svm->vmcb->save.cr3 = cr3;
 -		mark_dirty(svm->vmcb, VMCB_CR);
 -	}
 +	svm->vmcb->control.nested_cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_NPT);
 +
 +	/* Also sync guest cr3 here in case we live migrate */
 +	svm->vmcb->save.cr3 = kvm_read_cr3(vcpu);
 +	mark_dirty(svm->vmcb, VMCB_CR);
  }
  
  static int is_disabled(void)
@@@ -7474,7 -7413,7 +7473,11 @@@ static struct kvm_x86_ops svm_x86_ops _
  	.read_l1_tsc_offset = svm_read_l1_tsc_offset,
  	.write_l1_tsc_offset = svm_write_l1_tsc_offset,
  
++<<<<<<< HEAD
 +	.set_tdp_cr3 = set_tdp_cr3,
++=======
+ 	.load_mmu_pgd = svm_load_mmu_pgd,
++>>>>>>> 727a7e27cf88 (KVM: x86: rename set_cr3 callback and related flags to load_mmu_pgd)
  
  	.check_intercept = svm_check_intercept,
  	.handle_exit_irqoff = svm_handle_exit_irqoff,
diff --cc arch/x86/kvm/vmx/vmx.c
index 81f81c6fe900,e961633182f8..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -7981,7 -7921,7 +7980,11 @@@ static struct kvm_x86_ops vmx_x86_ops _
  	.read_l1_tsc_offset = vmx_read_l1_tsc_offset,
  	.write_l1_tsc_offset = vmx_write_l1_tsc_offset,
  
++<<<<<<< HEAD
 +	.set_tdp_cr3 = vmx_set_cr3,
++=======
+ 	.load_mmu_pgd = vmx_load_mmu_pgd,
++>>>>>>> 727a7e27cf88 (KVM: x86: rename set_cr3 callback and related flags to load_mmu_pgd)
  
  	.check_intercept = vmx_check_intercept,
  	.handle_exit_irqoff = vmx_handle_exit_irqoff,
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 57bbfbf61b11..51f276320de3 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -61,7 +61,7 @@
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
-#define KVM_REQ_LOAD_CR3		KVM_ARCH_REQ(5)
+#define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
@@ -1086,7 +1086,6 @@ struct kvm_x86_ops {
 	void (*decache_cr0_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*decache_cr4_guest_bits)(struct kvm_vcpu *vcpu);
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
-	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 	int (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
 	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);
 	void (*get_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
@@ -1155,6 +1154,8 @@ struct kvm_x86_ops {
 
 	void (*set_supported_cpuid)(u32 func, struct kvm_cpuid_entry2 *entry);
 
+	void (*load_mmu_pgd)(struct kvm_vcpu *vcpu, unsigned long cr3);
+
 	bool (*has_wbinvd_exit)(void);
 
 	u64 (*read_l1_tsc_offset)(struct kvm_vcpu *vcpu);
* Unmerged path arch/x86/kvm/mmu.h
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 7b990f5b5c82..2f1322243db4 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -4369,7 +4369,7 @@ static bool fast_cr3_switch(struct kvm_vcpu *vcpu, gpa_t new_cr3,
 			 * accompanied by KVM_REQ_MMU_RELOAD, which will free
 			 * the root set here and allocate a new one.
 			 */
-			kvm_make_request(KVM_REQ_LOAD_CR3, vcpu);
+			kvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);
 			if (!skip_tlb_flush) {
 				kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);
 				kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
@@ -5241,7 +5241,7 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	kvm_mmu_sync_roots(vcpu);
 	if (r)
 		goto out;
-	kvm_mmu_load_cr3(vcpu);
+	kvm_mmu_load_pgd(vcpu);
 	kvm_x86_ops->tlb_flush(vcpu, true);
 out:
 	return r;
* Unmerged path arch/x86/kvm/svm.c
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 04723b1103e1..63da7734d0b2 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -2475,9 +2475,9 @@ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			 * If L1 use EPT, then L0 needs to execute INVEPT on
 			 * EPTP02 instead of EPTP01. Therefore, delay TLB
 			 * flush until vmcs02->eptp is fully updated by
-			 * KVM_REQ_LOAD_CR3. Note that this assumes
+			 * KVM_REQ_LOAD_MMU_PGD. Note that this assumes
 			 * KVM_REQ_TLB_FLUSH is evaluated after
-			 * KVM_REQ_LOAD_CR3 in vcpu_enter_guest().
+			 * KVM_REQ_LOAD_MMU_PGD in vcpu_enter_guest().
 			 */
 			kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 		}
@@ -2522,7 +2522,7 @@ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 	/*
 	 * Immediately write vmcs02.GUEST_CR3.  It will be propagated to vmcs12
 	 * on nested VM-Exit, which can occur without actually running L2 and
-	 * thus without hitting vmx_set_cr3(), e.g. if L1 is entering L2 with
+	 * thus without hitting vmx_load_mmu_pgd(), e.g. if L1 is entering L2 with
 	 * vmcs12.GUEST_ACTIVITYSTATE=HLT, in which case KVM will intercept the
 	 * transition to HLT instead of running L2.
 	 */
@@ -4033,7 +4033,7 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 	 *
 	 * If vmcs12 uses EPT, we need to execute this flush on EPTP01
 	 * and therefore we request the TLB flush to happen only after VMCS EPTP
-	 * has been set by KVM_REQ_LOAD_CR3.
+	 * has been set by KVM_REQ_LOAD_MMU_PGD.
 	 */
 	if (enable_vpid &&
 	    (!nested_cpu_has_vpid(vmcs12) || !nested_has_guest_tlb_tag(vcpu))) {
* Unmerged path arch/x86/kvm/vmx/vmx.c
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index ac1b05ff4e4b..980c2afbbb56 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -335,9 +335,9 @@ u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu);
 void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask);
 void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer);
 void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
-void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
 int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
 void set_cr4_guest_host_mask(struct vcpu_vmx *vmx);
+void vmx_load_mmu_pgd(struct kvm_vcpu *vcpu, unsigned long cr3);
 void ept_save_pdptrs(struct kvm_vcpu *vcpu);
 void vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
 void vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index d84116c1b5a2..5a88cfdb1462 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -8060,8 +8060,8 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 		if (kvm_check_request(KVM_REQ_MMU_SYNC, vcpu))
 			kvm_mmu_sync_roots(vcpu);
-		if (kvm_check_request(KVM_REQ_LOAD_CR3, vcpu))
-			kvm_mmu_load_cr3(vcpu);
+		if (kvm_check_request(KVM_REQ_LOAD_MMU_PGD, vcpu))
+			kvm_mmu_load_pgd(vcpu);
 		if (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))
 			kvm_vcpu_flush_tlb(vcpu, true);
 		if (kvm_check_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu)) {
