KVM: SVM: Split out architectural interrupt/NMI/SMI blocking checks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit cae96af18452336aff85c95c6892aab1ed955eed
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/cae96af1.failed

Move the architectural (non-KVM specific) interrupt/NMI/SMI blocking checks
to a separate helper so that they can be used in a future patch by
svm_check_nested_events().

No functional change intended.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit cae96af18452336aff85c95c6892aab1ed955eed)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/svm/svm.h
diff --cc arch/x86/kvm/svm/svm.c
index a95f04022d02,5738be661a1f..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -4948,2273 -2790,987 +4948,2338 @@@ static int (*const svm_exit_handlers[])
  	[SVM_EXIT_AVIC_UNACCELERATED_ACCESS]	= avic_unaccelerated_access_interception,
  };
  
 -static void dump_vmcb(struct kvm_vcpu *vcpu)
 +static void dump_vmcb(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb_control_area *control = &svm->vmcb->control;
 +	struct vmcb_save_area *save = &svm->vmcb->save;
 +
 +	if (!dump_invalid_vmcb) {
 +		pr_warn_ratelimited("set kvm_amd.dump_invalid_vmcb=1 to dump internal KVM state.\n");
 +		return;
 +	}
 +
 +	pr_err("VMCB Control Area:\n");
 +	pr_err("%-20s%04x\n", "cr_read:", control->intercept_cr & 0xffff);
 +	pr_err("%-20s%04x\n", "cr_write:", control->intercept_cr >> 16);
 +	pr_err("%-20s%04x\n", "dr_read:", control->intercept_dr & 0xffff);
 +	pr_err("%-20s%04x\n", "dr_write:", control->intercept_dr >> 16);
 +	pr_err("%-20s%08x\n", "exceptions:", control->intercept_exceptions);
 +	pr_err("%-20s%016llx\n", "intercepts:", control->intercept);
 +	pr_err("%-20s%d\n", "pause filter count:", control->pause_filter_count);
 +	pr_err("%-20s%d\n", "pause filter threshold:",
 +	       control->pause_filter_thresh);
 +	pr_err("%-20s%016llx\n", "iopm_base_pa:", control->iopm_base_pa);
 +	pr_err("%-20s%016llx\n", "msrpm_base_pa:", control->msrpm_base_pa);
 +	pr_err("%-20s%016llx\n", "tsc_offset:", control->tsc_offset);
 +	pr_err("%-20s%d\n", "asid:", control->asid);
 +	pr_err("%-20s%d\n", "tlb_ctl:", control->tlb_ctl);
 +	pr_err("%-20s%08x\n", "int_ctl:", control->int_ctl);
 +	pr_err("%-20s%08x\n", "int_vector:", control->int_vector);
 +	pr_err("%-20s%08x\n", "int_state:", control->int_state);
 +	pr_err("%-20s%08x\n", "exit_code:", control->exit_code);
 +	pr_err("%-20s%016llx\n", "exit_info1:", control->exit_info_1);
 +	pr_err("%-20s%016llx\n", "exit_info2:", control->exit_info_2);
 +	pr_err("%-20s%08x\n", "exit_int_info:", control->exit_int_info);
 +	pr_err("%-20s%08x\n", "exit_int_info_err:", control->exit_int_info_err);
 +	pr_err("%-20s%lld\n", "nested_ctl:", control->nested_ctl);
 +	pr_err("%-20s%016llx\n", "nested_cr3:", control->nested_cr3);
 +	pr_err("%-20s%016llx\n", "avic_vapic_bar:", control->avic_vapic_bar);
 +	pr_err("%-20s%08x\n", "event_inj:", control->event_inj);
 +	pr_err("%-20s%08x\n", "event_inj_err:", control->event_inj_err);
 +	pr_err("%-20s%lld\n", "virt_ext:", control->virt_ext);
 +	pr_err("%-20s%016llx\n", "next_rip:", control->next_rip);
 +	pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
 +	pr_err("%-20s%016llx\n", "avic_logical_id:", control->avic_logical_id);
 +	pr_err("%-20s%016llx\n", "avic_physical_id:", control->avic_physical_id);
 +	pr_err("VMCB State Save Area:\n");
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "es:",
 +	       save->es.selector, save->es.attrib,
 +	       save->es.limit, save->es.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "cs:",
 +	       save->cs.selector, save->cs.attrib,
 +	       save->cs.limit, save->cs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ss:",
 +	       save->ss.selector, save->ss.attrib,
 +	       save->ss.limit, save->ss.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ds:",
 +	       save->ds.selector, save->ds.attrib,
 +	       save->ds.limit, save->ds.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "fs:",
 +	       save->fs.selector, save->fs.attrib,
 +	       save->fs.limit, save->fs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "gs:",
 +	       save->gs.selector, save->gs.attrib,
 +	       save->gs.limit, save->gs.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "gdtr:",
 +	       save->gdtr.selector, save->gdtr.attrib,
 +	       save->gdtr.limit, save->gdtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "ldtr:",
 +	       save->ldtr.selector, save->ldtr.attrib,
 +	       save->ldtr.limit, save->ldtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "idtr:",
 +	       save->idtr.selector, save->idtr.attrib,
 +	       save->idtr.limit, save->idtr.base);
 +	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 +	       "tr:",
 +	       save->tr.selector, save->tr.attrib,
 +	       save->tr.limit, save->tr.base);
 +	pr_err("cpl:            %d                efer:         %016llx\n",
 +		save->cpl, save->efer);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cr0:", save->cr0, "cr2:", save->cr2);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cr3:", save->cr3, "cr4:", save->cr4);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "dr6:", save->dr6, "dr7:", save->dr7);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "rip:", save->rip, "rflags:", save->rflags);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "rsp:", save->rsp, "rax:", save->rax);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "star:", save->star, "lstar:", save->lstar);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "cstar:", save->cstar, "sfmask:", save->sfmask);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "kernel_gs_base:", save->kernel_gs_base,
 +	       "sysenter_cs:", save->sysenter_cs);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "sysenter_esp:", save->sysenter_esp,
 +	       "sysenter_eip:", save->sysenter_eip);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "gpat:", save->g_pat, "dbgctl:", save->dbgctl);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "br_from:", save->br_from, "br_to:", save->br_to);
 +	pr_err("%-15s %016llx %-13s %016llx\n",
 +	       "excp_from:", save->last_excp_from,
 +	       "excp_to:", save->last_excp_to);
 +}
 +
 +static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +{
 +	struct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;
 +
 +	*info1 = control->exit_info_1;
 +	*info2 = control->exit_info_2;
 +}
 +
 +static int handle_exit(struct kvm_vcpu *vcpu,
 +	enum exit_fastpath_completion exit_fastpath)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct kvm_run *kvm_run = vcpu->run;
 +	u32 exit_code = svm->vmcb->control.exit_code;
 +
 +	trace_kvm_exit(exit_code, vcpu, KVM_ISA_SVM);
 +
 +	if (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))
 +		vcpu->arch.cr0 = svm->vmcb->save.cr0;
 +	if (npt_enabled)
 +		vcpu->arch.cr3 = svm->vmcb->save.cr3;
 +
 +	if (unlikely(svm->nested.exit_required)) {
 +		nested_svm_vmexit(svm);
 +		svm->nested.exit_required = false;
 +
 +		return 1;
 +	}
 +
 +	if (is_guest_mode(vcpu)) {
 +		int vmexit;
 +
 +		trace_kvm_nested_vmexit(svm->vmcb->save.rip, exit_code,
 +					svm->vmcb->control.exit_info_1,
 +					svm->vmcb->control.exit_info_2,
 +					svm->vmcb->control.exit_int_info,
 +					svm->vmcb->control.exit_int_info_err,
 +					KVM_ISA_SVM);
 +
 +		vmexit = nested_svm_exit_special(svm);
 +
 +		if (vmexit == NESTED_EXIT_CONTINUE)
 +			vmexit = nested_svm_exit_handled(svm);
 +
 +		if (vmexit == NESTED_EXIT_DONE)
 +			return 1;
 +	}
 +
 +	svm_complete_interrupts(svm);
 +
 +	if (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {
 +		kvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 +		kvm_run->fail_entry.hardware_entry_failure_reason
 +			= svm->vmcb->control.exit_code;
 +		dump_vmcb(vcpu);
 +		return 0;
 +	}
 +
 +	if (is_external_interrupt(svm->vmcb->control.exit_int_info) &&
 +	    exit_code != SVM_EXIT_EXCP_BASE + PF_VECTOR &&
 +	    exit_code != SVM_EXIT_NPF && exit_code != SVM_EXIT_TASK_SWITCH &&
 +	    exit_code != SVM_EXIT_INTR && exit_code != SVM_EXIT_NMI)
 +		printk(KERN_ERR "%s: unexpected exit_int_info 0x%x "
 +		       "exit_code 0x%x\n",
 +		       __func__, svm->vmcb->control.exit_int_info,
 +		       exit_code);
 +
 +	if (exit_fastpath == EXIT_FASTPATH_SKIP_EMUL_INS) {
 +		kvm_skip_emulated_instruction(vcpu);
 +		return 1;
 +	} else if (exit_code >= ARRAY_SIZE(svm_exit_handlers)
 +	    || !svm_exit_handlers[exit_code]) {
 +		vcpu_unimpl(vcpu, "svm: unexpected exit reason 0x%x\n", exit_code);
 +		dump_vmcb(vcpu);
 +		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 +		vcpu->run->internal.suberror =
 +			KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
 +		vcpu->run->internal.ndata = 1;
 +		vcpu->run->internal.data[0] = exit_code;
 +		return 0;
 +	}
 +
 +#ifdef CONFIG_RETPOLINE
 +	if (exit_code == SVM_EXIT_MSR)
 +		return msr_interception(svm);
 +	else if (exit_code == SVM_EXIT_VINTR)
 +		return interrupt_window_interception(svm);
 +	else if (exit_code == SVM_EXIT_INTR)
 +		return intr_interception(svm);
 +	else if (exit_code == SVM_EXIT_HLT)
 +		return halt_interception(svm);
 +	else if (exit_code == SVM_EXIT_NPF)
 +		return npf_interception(svm);
 +#endif
 +	return svm_exit_handlers[exit_code](svm);
 +}
 +
 +static void reload_tss(struct kvm_vcpu *vcpu)
 +{
 +	int cpu = raw_smp_processor_id();
 +
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +	sd->tss_desc->type = 9; /* available 32/64-bit TSS */
 +	load_TR_desc();
 +}
 +
 +static void pre_sev_run(struct vcpu_svm *svm, int cpu)
 +{
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +	int asid = sev_get_asid(svm->vcpu.kvm);
 +
 +	/* Assign the asid allocated with this SEV guest */
 +	svm->vmcb->control.asid = asid;
 +
 +	/*
 +	 * Flush guest TLB:
 +	 *
 +	 * 1) when different VMCB for the same ASID is to be run on the same host CPU.
 +	 * 2) or this VMCB was executed on different host CPU in previous VMRUNs.
 +	 */
 +	if (sd->sev_vmcbs[asid] == svm->vmcb &&
 +	    svm->last_cpu == cpu)
 +		return;
 +
 +	svm->last_cpu = cpu;
 +	sd->sev_vmcbs[asid] = svm->vmcb;
 +	svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 +	mark_dirty(svm->vmcb, VMCB_ASID);
 +}
 +
 +static void pre_svm_run(struct vcpu_svm *svm)
 +{
 +	int cpu = raw_smp_processor_id();
 +
 +	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +
 +	if (sev_guest(svm->vcpu.kvm))
 +		return pre_sev_run(svm, cpu);
 +
 +	/* FIXME: handle wraparound of asid_generation */
 +	if (svm->asid_generation != sd->asid_generation)
 +		new_asid(svm, sd);
 +}
 +
 +static void svm_inject_nmi(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;
 +	vcpu->arch.hflags |= HF_NMI_MASK;
 +	set_intercept(svm, INTERCEPT_IRET);
 +	++vcpu->stat.nmi_injections;
 +}
 +
 +static void svm_set_irq(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	BUG_ON(!(gif_set(svm)));
 +
 +	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 +	++vcpu->stat.irq_injections;
 +
 +	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 +		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
 +}
 +
 +static inline bool svm_nested_virtualize_tpr(struct kvm_vcpu *vcpu)
 +{
 +	return is_guest_mode(vcpu) && (vcpu->arch.hflags & HF_VINTR_MASK);
 +}
 +
 +static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (svm_nested_virtualize_tpr(vcpu))
 +		return;
 +
 +	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +
 +	if (irr == -1)
 +		return;
 +
 +	if (tpr >= irr)
 +		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +}
 +
++<<<<<<< HEAD
 +static void svm_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 +{
 +	return;
 +}
 +
 +static void svm_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 +{
 +}
 +
 +static void svm_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 +{
 +}
 +
 +static int svm_set_pi_irte_mode(struct kvm_vcpu *vcpu, bool activate)
 +{
 +	int ret = 0;
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *ir;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (!kvm_arch_has_assigned_device(vcpu->kvm))
 +		return 0;
 +
 +	/*
 +	 * Here, we go through the per-vcpu ir_list to update all existing
 +	 * interrupt remapping table entry targeting this vcpu.
 +	 */
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +
 +	if (list_empty(&svm->ir_list))
 +		goto out;
 +
 +	list_for_each_entry(ir, &svm->ir_list, node) {
 +		if (activate)
 +			ret = amd_iommu_activate_guest_mode(ir->data);
 +		else
 +			ret = amd_iommu_deactivate_guest_mode(ir->data);
 +		if (ret)
 +			break;
 +	}
 +out:
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +	return ret;
 +}
 +
 +static void svm_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
++=======
++bool svm_nmi_blocked(struct kvm_vcpu *vcpu)
++>>>>>>> cae96af18452 (KVM: SVM: Split out architectural interrupt/NMI/SMI blocking checks)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	bool activated = kvm_vcpu_apicv_active(vcpu);
 +
++<<<<<<< HEAD
 +	if (!avic)
 +		return;
 +
 +	if (activated) {
 +		/**
 +		 * During AVIC temporary deactivation, guest could update
 +		 * APIC ID, DFR and LDR registers, which would not be trapped
 +		 * by avic_unaccelerated_access_interception(). In this case,
 +		 * we need to check and update the AVIC logical APIC ID table
 +		 * accordingly before re-activating.
 +		 */
 +		avic_post_state_restore(vcpu);
 +		vmcb->control.int_ctl |= AVIC_ENABLE_MASK;
 +	} else {
 +		vmcb->control.int_ctl &= ~AVIC_ENABLE_MASK;
 +	}
 +	mark_dirty(vmcb, VMCB_AVIC);
 +
 +	svm_set_pi_irte_mode(vcpu, activated);
 +}
 +
 +static void svm_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 +{
 +	return;
 +}
 +
 +static int svm_deliver_avic_intr(struct kvm_vcpu *vcpu, int vec)
 +{
 +	if (!vcpu->arch.apicv_active)
 +		return -1;
 +
 +	kvm_lapic_set_irr(vec, vcpu->arch.apic);
 +	smp_mb__after_atomic();
 +
 +	if (avic_vcpu_is_running(vcpu)) {
 +		int cpuid = vcpu->cpu;
 +
 +		if (cpuid != get_cpu())
 +			wrmsrl(SVM_AVIC_DOORBELL, kvm_cpu_get_apicid(cpuid));
 +		put_cpu();
 +	} else
 +		kvm_vcpu_wake_up(vcpu);
 +
 +	return 0;
 +}
 +
 +static bool svm_dy_apicv_has_pending_interrupt(struct kvm_vcpu *vcpu)
 +{
 +	return false;
 +}
 +
 +static void svm_ir_list_del(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 +{
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *cur;
 +
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +	list_for_each_entry(cur, &svm->ir_list, node) {
 +		if (cur->data != pi->ir_data)
 +			continue;
 +		list_del(&cur->node);
 +		kfree(cur);
 +		break;
 +	}
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +}
 +
 +static int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 +{
 +	int ret = 0;
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *ir;
 +
 +	/**
 +	 * In some cases, the existing irte is updaed and re-set,
 +	 * so we need to check here if it's already been * added
 +	 * to the ir_list.
 +	 */
 +	if (pi->ir_data && (pi->prev_ga_tag != 0)) {
 +		struct kvm *kvm = svm->vcpu.kvm;
 +		u32 vcpu_id = AVIC_GATAG_TO_VCPUID(pi->prev_ga_tag);
 +		struct kvm_vcpu *prev_vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
 +		struct vcpu_svm *prev_svm;
 +
 +		if (!prev_vcpu) {
 +			ret = -EINVAL;
 +			goto out;
 +		}
 +
 +		prev_svm = to_svm(prev_vcpu);
 +		svm_ir_list_del(prev_svm, pi);
 +	}
 +
 +	/**
 +	 * Allocating new amd_iommu_pi_data, which will get
 +	 * add to the per-vcpu ir_list.
 +	 */
 +	ir = kzalloc(sizeof(struct amd_svm_iommu_ir), GFP_KERNEL_ACCOUNT);
 +	if (!ir) {
 +		ret = -ENOMEM;
 +		goto out;
 +	}
 +	ir->data = pi->ir_data;
 +
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +	list_add(&ir->node, &svm->ir_list);
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +out:
 +	return ret;
 +}
 +
 +/**
 + * Note:
 + * The HW cannot support posting multicast/broadcast
 + * interrupts to a vCPU. So, we still use legacy interrupt
 + * remapping for these kind of interrupts.
 + *
 + * For lowest-priority interrupts, we only support
 + * those with single CPU as the destination, e.g. user
 + * configures the interrupts via /proc/irq or uses
 + * irqbalance to make the interrupts single-CPU.
 + */
 +static int
 +get_pi_vcpu_info(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 +		 struct vcpu_data *vcpu_info, struct vcpu_svm **svm)
 +{
 +	struct kvm_lapic_irq irq;
 +	struct kvm_vcpu *vcpu = NULL;
 +
 +	kvm_set_msi_irq(kvm, e, &irq);
 +
 +	if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
 +	    !kvm_irq_is_postable(&irq)) {
 +		pr_debug("SVM: %s: use legacy intr remap mode for irq %u\n",
 +			 __func__, irq.vector);
 +		return -1;
 +	}
 +
 +	pr_debug("SVM: %s: use GA mode for irq %u\n", __func__,
 +		 irq.vector);
 +	*svm = to_svm(vcpu);
 +	vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
 +	vcpu_info->vector = irq.vector;
 +
 +	return 0;
 +}
 +
 +/*
 + * svm_update_pi_irte - set IRTE for Posted-Interrupts
 + *
 + * @kvm: kvm
 + * @host_irq: host irq of the interrupt
 + * @guest_irq: gsi of the interrupt
 + * @set: set or unset PI
 + * returns 0 on success, < 0 on failure
 + */
 +static int svm_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 +			      uint32_t guest_irq, bool set)
 +{
 +	struct kvm_kernel_irq_routing_entry *e;
 +	struct kvm_irq_routing_table *irq_rt;
 +	int idx, ret = -EINVAL;
 +
 +	if (!kvm_arch_has_assigned_device(kvm) ||
 +	    !irq_remapping_cap(IRQ_POSTING_CAP))
 +		return 0;
 +
 +	pr_debug("SVM: %s: host_irq=%#x, guest_irq=%#x, set=%#x\n",
 +		 __func__, host_irq, guest_irq, set);
 +
 +	idx = srcu_read_lock(&kvm->irq_srcu);
 +	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
 +	WARN_ON(guest_irq >= irq_rt->nr_rt_entries);
 +
 +	hlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {
 +		struct vcpu_data vcpu_info;
 +		struct vcpu_svm *svm = NULL;
 +
 +		if (e->type != KVM_IRQ_ROUTING_MSI)
 +			continue;
 +
 +		/**
 +		 * Here, we setup with legacy mode in the following cases:
 +		 * 1. When cannot target interrupt to a specific vcpu.
 +		 * 2. Unsetting posted interrupt.
 +		 * 3. APIC virtialization is disabled for the vcpu.
 +		 * 4. IRQ has incompatible delivery mode (SMI, INIT, etc)
 +		 */
 +		if (!get_pi_vcpu_info(kvm, e, &vcpu_info, &svm) && set &&
 +		    kvm_vcpu_apicv_active(&svm->vcpu)) {
 +			struct amd_iommu_pi_data pi;
 +
 +			/* Try to enable guest_mode in IRTE */
 +			pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
 +					    AVIC_HPA_MASK);
 +			pi.ga_tag = AVIC_GATAG(to_kvm_svm(kvm)->avic_vm_id,
 +						     svm->vcpu.vcpu_id);
 +			pi.is_guest_mode = true;
 +			pi.vcpu_data = &vcpu_info;
 +			ret = irq_set_vcpu_affinity(host_irq, &pi);
 +
 +			/**
 +			 * Here, we successfully setting up vcpu affinity in
 +			 * IOMMU guest mode. Now, we need to store the posted
 +			 * interrupt information in a per-vcpu ir_list so that
 +			 * we can reference to them directly when we update vcpu
 +			 * scheduling information in IOMMU irte.
 +			 */
 +			if (!ret && pi.is_guest_mode)
 +				svm_ir_list_add(svm, &pi);
 +		} else {
 +			/* Use legacy mode in IRTE */
 +			struct amd_iommu_pi_data pi;
 +
 +			/**
 +			 * Here, pi is used to:
 +			 * - Tell IOMMU to use legacy mode for this interrupt.
 +			 * - Retrieve ga_tag of prior interrupt remapping data.
 +			 */
 +			pi.is_guest_mode = false;
 +			ret = irq_set_vcpu_affinity(host_irq, &pi);
 +
 +			/**
 +			 * Check if the posted interrupt was previously
 +			 * setup with the guest_mode by checking if the ga_tag
 +			 * was cached. If so, we need to clean up the per-vcpu
 +			 * ir_list.
 +			 */
 +			if (!ret && pi.prev_ga_tag) {
 +				int id = AVIC_GATAG_TO_VCPUID(pi.prev_ga_tag);
 +				struct kvm_vcpu *vcpu;
 +
 +				vcpu = kvm_get_vcpu_by_id(kvm, id);
 +				if (vcpu)
 +					svm_ir_list_del(to_svm(vcpu), &pi);
 +			}
 +		}
 +
 +		if (!ret && svm) {
 +			trace_kvm_pi_irte_update(host_irq, svm->vcpu.vcpu_id,
 +						 e->gsi, vcpu_info.vector,
 +						 vcpu_info.pi_desc_addr, set);
 +		}
 +
 +		if (ret < 0) {
 +			pr_err("%s: failed to update PI IRTE\n", __func__);
 +			goto out;
 +		}
 +	}
 +
 +	ret = 0;
 +out:
 +	srcu_read_unlock(&kvm->irq_srcu, idx);
 +	return ret;
 +}
 +
 +static int svm_nmi_allowed(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	int ret;
 +	ret = !(vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) &&
 +	      !(svm->vcpu.arch.hflags & HF_NMI_MASK);
 +	ret = ret && gif_set(svm) && nested_svm_nmi(svm);
++=======
++	if (!gif_set(svm))
++		return true;
++
++	if (is_guest_mode(vcpu) && nested_exit_on_nmi(svm))
++		return false;
++
++	ret = (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) ||
++	      (svm->vcpu.arch.hflags & HF_NMI_MASK);
++>>>>>>> cae96af18452 (KVM: SVM: Split out architectural interrupt/NMI/SMI blocking checks)
 +
 +	return ret;
 +}
 +
++static bool svm_nmi_allowed(struct kvm_vcpu *vcpu)
++{
++	struct vcpu_svm *svm = to_svm(vcpu);
++	if (svm->nested.nested_run_pending)
++		return false;
++
++        return !svm_nmi_blocked(vcpu);
++}
++
 +static bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	return !!(svm->vcpu.arch.hflags & HF_NMI_MASK);
 +}
 +
 +static void svm_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (masked) {
 +		svm->vcpu.arch.hflags |= HF_NMI_MASK;
 +		set_intercept(svm, INTERCEPT_IRET);
 +	} else {
 +		svm->vcpu.arch.hflags &= ~HF_NMI_MASK;
 +		clr_intercept(svm, INTERCEPT_IRET);
 +	}
 +}
 +
++<<<<<<< HEAD
 +static int svm_interrupt_allowed(struct kvm_vcpu *vcpu)
++=======
++bool svm_interrupt_blocked(struct kvm_vcpu *vcpu)
++>>>>>>> cae96af18452 (KVM: SVM: Split out architectural interrupt/NMI/SMI blocking checks)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb *vmcb = svm->vmcb;
 +	int ret;
 +
 +	if (!gif_set(svm) ||
 +	     (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK))
++<<<<<<< HEAD
 +		return 0;
 +
 +	ret = !!(kvm_get_rflags(vcpu) & X86_EFLAGS_IF);
 +
 +	if (is_guest_mode(vcpu))
 +		return ret && !(svm->vcpu.arch.hflags & HF_VINTR_MASK);
 +
 +	return ret;
++=======
++		return true;
++
++	if (is_guest_mode(vcpu) && (svm->vcpu.arch.hflags & HF_VINTR_MASK))
++		return !(svm->vcpu.arch.hflags & HF_HIF_MASK);
++	else
++		return !(kvm_get_rflags(vcpu) & X86_EFLAGS_IF);
++}
++
++static bool svm_interrupt_allowed(struct kvm_vcpu *vcpu)
++{
++	struct vcpu_svm *svm = to_svm(vcpu);
++	if (svm->nested.nested_run_pending)
++		return false;
++
++        return !svm_interrupt_blocked(vcpu);
++>>>>>>> cae96af18452 (KVM: SVM: Split out architectural interrupt/NMI/SMI blocking checks)
 +}
 +
 +static void enable_irq_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	/*
 +	 * In case GIF=0 we can't rely on the CPU to tell us when GIF becomes
 +	 * 1, because that's a separate STGI/VMRUN intercept.  The next time we
 +	 * get that intercept, this function will be called again though and
 +	 * we'll get the vintr intercept. However, if the vGIF feature is
 +	 * enabled, the STGI interception will not occur. Enable the irq
 +	 * window under the assumption that the hardware will set the GIF.
 +	 */
 +	if ((vgif_enabled(svm) || gif_set(svm)) && nested_svm_intr(svm)) {
 +		svm_set_vintr(svm);
 +	}
 +}
 +
 +static void enable_nmi_window(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if ((svm->vcpu.arch.hflags & (HF_NMI_MASK | HF_IRET_MASK))
 +	    == HF_NMI_MASK)
 +		return; /* IRET will cause a vm exit */
 +
 +	if (!gif_set(svm)) {
 +		if (vgif_enabled(svm))
 +			set_intercept(svm, INTERCEPT_STGI);
 +		return; /* STGI will cause a vm exit */
 +	}
 +
 +	if (svm->nested.exit_required)
 +		return; /* we're not going to run the guest yet */
 +
 +	/*
 +	 * Something prevents NMI from been injected. Single step over possible
 +	 * problem (IRET or exception injection or interrupt shadow)
 +	 */
 +	svm->nmi_singlestep_guest_rflags = svm_get_rflags(vcpu);
 +	svm->nmi_singlestep = true;
 +	svm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);
 +}
 +
 +static int svm_set_tss_addr(struct kvm *kvm, unsigned int addr)
 +{
 +	return 0;
 +}
 +
 +static int svm_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 +{
 +	return 0;
 +}
 +
 +static void svm_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	/*
 +	 * Flush only the current ASID even if the TLB flush was invoked via
 +	 * kvm_flush_remote_tlbs().  Although flushing remote TLBs requires all
 +	 * ASIDs to be flushed, KVM uses a single ASID for L1 and L2, and
 +	 * unconditionally does a TLB flush on both nested VM-Enter and nested
 +	 * VM-Exit (via kvm_mmu_reset_context()).
 +	 */
 +	if (static_cpu_has(X86_FEATURE_FLUSHBYASID))
 +		svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 +	else
 +		svm->asid_generation--;
 +}
 +
 +static void svm_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t gva)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	invlpga(gva, svm->vmcb->control.asid);
 +}
 +
 +static void svm_flush_tlb_guest(struct kvm_vcpu *vcpu)
 +{
 +	svm_flush_tlb(vcpu, false);
 +}
 +
 +static void svm_prepare_guest_switch(struct kvm_vcpu *vcpu)
 +{
 +}
 +
 +static inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (svm_nested_virtualize_tpr(vcpu))
 +		return;
 +
 +	if (!is_cr_intercept(svm, INTERCEPT_CR8_WRITE)) {
 +		int cr8 = svm->vmcb->control.int_ctl & V_TPR_MASK;
 +		kvm_set_cr8(vcpu, cr8);
 +	}
 +}
 +
 +static inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u64 cr8;
 +
 +	if (svm_nested_virtualize_tpr(vcpu) ||
 +	    kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	cr8 = kvm_get_cr8(vcpu);
 +	svm->vmcb->control.int_ctl &= ~V_TPR_MASK;
 +	svm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;
 +}
 +
 +static void svm_complete_interrupts(struct vcpu_svm *svm)
 +{
 +	u8 vector;
 +	int type;
 +	u32 exitintinfo = svm->vmcb->control.exit_int_info;
 +	unsigned int3_injected = svm->int3_injected;
 +
 +	svm->int3_injected = 0;
 +
 +	/*
 +	 * If we've made progress since setting HF_IRET_MASK, we've
 +	 * executed an IRET and can allow NMI injection.
 +	 */
 +	if ((svm->vcpu.arch.hflags & HF_IRET_MASK)
 +	    && kvm_rip_read(&svm->vcpu) != svm->nmi_iret_rip) {
 +		svm->vcpu.arch.hflags &= ~(HF_NMI_MASK | HF_IRET_MASK);
 +		kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +	}
 +
 +	svm->vcpu.arch.nmi_injected = false;
 +	kvm_clear_exception_queue(&svm->vcpu);
 +	kvm_clear_interrupt_queue(&svm->vcpu);
 +
 +	if (!(exitintinfo & SVM_EXITINTINFO_VALID))
 +		return;
 +
 +	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +
 +	vector = exitintinfo & SVM_EXITINTINFO_VEC_MASK;
 +	type = exitintinfo & SVM_EXITINTINFO_TYPE_MASK;
 +
 +	switch (type) {
 +	case SVM_EXITINTINFO_TYPE_NMI:
 +		svm->vcpu.arch.nmi_injected = true;
 +		break;
 +	case SVM_EXITINTINFO_TYPE_EXEPT:
 +		/*
 +		 * In case of software exceptions, do not reinject the vector,
 +		 * but re-execute the instruction instead. Rewind RIP first
 +		 * if we emulated INT3 before.
 +		 */
 +		if (kvm_exception_is_soft(vector)) {
 +			if (vector == BP_VECTOR && int3_injected &&
 +			    kvm_is_linear_rip(&svm->vcpu, svm->int3_rip))
 +				kvm_rip_write(&svm->vcpu,
 +					      kvm_rip_read(&svm->vcpu) -
 +					      int3_injected);
 +			break;
 +		}
 +		if (exitintinfo & SVM_EXITINTINFO_VALID_ERR) {
 +			u32 err = svm->vmcb->control.exit_int_info_err;
 +			kvm_requeue_exception_e(&svm->vcpu, vector, err);
 +
 +		} else
 +			kvm_requeue_exception(&svm->vcpu, vector);
 +		break;
 +	case SVM_EXITINTINFO_TYPE_INTR:
 +		kvm_queue_interrupt(&svm->vcpu, vector, false);
 +		break;
 +	default:
 +		break;
 +	}
 +}
 +
 +static void svm_cancel_injection(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct vmcb_control_area *control = &svm->vmcb->control;
 +
 +	control->exit_int_info = control->event_inj;
 +	control->exit_int_info_err = control->event_inj_err;
 +	control->event_inj = 0;
 +	svm_complete_interrupts(svm);
 +}
 +
 +static void svm_vcpu_run(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 +	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 +	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
 +
 +	/*
 +	 * A vmexit emulation is required before the vcpu can be executed
 +	 * again.
 +	 */
 +	if (unlikely(svm->nested.exit_required))
 +		return;
 +
 +	/*
 +	 * Disable singlestep if we're injecting an interrupt/exception.
 +	 * We don't want our modified rflags to be pushed on the stack where
 +	 * we might not be able to easily reset them if we disabled NMI
 +	 * singlestep later.
 +	 */
 +	if (svm->nmi_singlestep && svm->vmcb->control.event_inj) {
 +		/*
 +		 * Event injection happens before external interrupts cause a
 +		 * vmexit and interrupts are disabled here, so smp_send_reschedule
 +		 * is enough to force an immediate vmexit.
 +		 */
 +		disable_nmi_singlestep(svm);
 +		smp_send_reschedule(vcpu->cpu);
 +	}
 +
 +	pre_svm_run(svm);
 +
 +	sync_lapic_to_cr8(vcpu);
 +
 +	svm->vmcb->save.cr2 = vcpu->arch.cr2;
 +
 +	clgi();
 +	kvm_load_guest_xsave_state(vcpu);
 +
 +	if (lapic_in_kernel(vcpu) &&
 +		vcpu->arch.apic->lapic_timer.timer_advance_ns)
 +		kvm_wait_lapic_expire(vcpu);
 +
 +	/*
 +	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 +	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 +	 * is no need to worry about the conditional branch over the wrmsr
 +	 * being speculatively taken.
 +	 */
 +	x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
 +
 +	local_irq_enable();
 +
 +	asm volatile (
 +		"push %%" _ASM_BP "; \n\t"
 +		"mov %c[rbx](%[svm]), %%" _ASM_BX " \n\t"
 +		"mov %c[rcx](%[svm]), %%" _ASM_CX " \n\t"
 +		"mov %c[rdx](%[svm]), %%" _ASM_DX " \n\t"
 +		"mov %c[rsi](%[svm]), %%" _ASM_SI " \n\t"
 +		"mov %c[rdi](%[svm]), %%" _ASM_DI " \n\t"
 +		"mov %c[rbp](%[svm]), %%" _ASM_BP " \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %c[r8](%[svm]),  %%r8  \n\t"
 +		"mov %c[r9](%[svm]),  %%r9  \n\t"
 +		"mov %c[r10](%[svm]), %%r10 \n\t"
 +		"mov %c[r11](%[svm]), %%r11 \n\t"
 +		"mov %c[r12](%[svm]), %%r12 \n\t"
 +		"mov %c[r13](%[svm]), %%r13 \n\t"
 +		"mov %c[r14](%[svm]), %%r14 \n\t"
 +		"mov %c[r15](%[svm]), %%r15 \n\t"
 +#endif
 +
 +		/* Enter guest mode */
 +		"push %%" _ASM_AX " \n\t"
 +		"mov %c[vmcb](%[svm]), %%" _ASM_AX " \n\t"
 +		__ex("vmload %%" _ASM_AX) "\n\t"
 +		__ex("vmrun %%" _ASM_AX) "\n\t"
 +		__ex("vmsave %%" _ASM_AX) "\n\t"
 +		"pop %%" _ASM_AX " \n\t"
 +
 +		/* Save guest registers, load host registers */
 +		"mov %%" _ASM_BX ", %c[rbx](%[svm]) \n\t"
 +		"mov %%" _ASM_CX ", %c[rcx](%[svm]) \n\t"
 +		"mov %%" _ASM_DX ", %c[rdx](%[svm]) \n\t"
 +		"mov %%" _ASM_SI ", %c[rsi](%[svm]) \n\t"
 +		"mov %%" _ASM_DI ", %c[rdi](%[svm]) \n\t"
 +		"mov %%" _ASM_BP ", %c[rbp](%[svm]) \n\t"
 +#ifdef CONFIG_X86_64
 +		"mov %%r8,  %c[r8](%[svm]) \n\t"
 +		"mov %%r9,  %c[r9](%[svm]) \n\t"
 +		"mov %%r10, %c[r10](%[svm]) \n\t"
 +		"mov %%r11, %c[r11](%[svm]) \n\t"
 +		"mov %%r12, %c[r12](%[svm]) \n\t"
 +		"mov %%r13, %c[r13](%[svm]) \n\t"
 +		"mov %%r14, %c[r14](%[svm]) \n\t"
 +		"mov %%r15, %c[r15](%[svm]) \n\t"
 +		/*
 +		* Clear host registers marked as clobbered to prevent
 +		* speculative use.
 +		*/
 +		"xor %%r8d, %%r8d \n\t"
 +		"xor %%r9d, %%r9d \n\t"
 +		"xor %%r10d, %%r10d \n\t"
 +		"xor %%r11d, %%r11d \n\t"
 +		"xor %%r12d, %%r12d \n\t"
 +		"xor %%r13d, %%r13d \n\t"
 +		"xor %%r14d, %%r14d \n\t"
 +		"xor %%r15d, %%r15d \n\t"
 +#endif
 +		"xor %%ebx, %%ebx \n\t"
 +		"xor %%ecx, %%ecx \n\t"
 +		"xor %%edx, %%edx \n\t"
 +		"xor %%esi, %%esi \n\t"
 +		"xor %%edi, %%edi \n\t"
 +		"pop %%" _ASM_BP
 +		:
 +		: [svm]"a"(svm),
 +		  [vmcb]"i"(offsetof(struct vcpu_svm, vmcb_pa)),
 +		  [rbx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBX])),
 +		  [rcx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RCX])),
 +		  [rdx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDX])),
 +		  [rsi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RSI])),
 +		  [rdi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDI])),
 +		  [rbp]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBP]))
 +#ifdef CONFIG_X86_64
 +		  , [r8]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R8])),
 +		  [r9]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R9])),
 +		  [r10]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R10])),
 +		  [r11]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R11])),
 +		  [r12]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R12])),
 +		  [r13]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R13])),
 +		  [r14]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R14])),
 +		  [r15]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R15]))
 +#endif
 +		: "cc", "memory"
 +#ifdef CONFIG_X86_64
 +		, "rbx", "rcx", "rdx", "rsi", "rdi"
 +		, "r8", "r9", "r10", "r11" , "r12", "r13", "r14", "r15"
 +#else
 +		, "ebx", "ecx", "edx", "esi", "edi"
 +#endif
 +		);
 +
 +	/* Eliminate branch target predictions from guest mode */
 +	vmexit_fill_RSB();
 +
 +#ifdef CONFIG_X86_64
 +	wrmsrl(MSR_GS_BASE, svm->host.gs_base);
 +#else
 +	loadsegment(fs, svm->host.fs);
 +#ifndef CONFIG_X86_32_LAZY_GS
 +	loadsegment(gs, svm->host.gs);
 +#endif
 +#endif
 +
 +	/*
 +	 * We do not use IBRS in the kernel. If this vCPU has used the
 +	 * SPEC_CTRL MSR it may have left it on; save the value and
 +	 * turn it off. This is much more efficient than blindly adding
 +	 * it to the atomic save/restore list. Especially as the former
 +	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 +	 *
 +	 * For non-nested case:
 +	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 *
 +	 * For nested case:
 +	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 +	 * save it.
 +	 */
 +	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 +		svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 +
 +	reload_tss(vcpu);
 +
 +	local_irq_disable();
 +
 +	x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
 +
 +	vcpu->arch.cr2 = svm->vmcb->save.cr2;
 +	vcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;
 +	vcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;
 +	vcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;
 +
 +	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 +		kvm_before_interrupt(&svm->vcpu);
 +
 +	kvm_load_host_xsave_state(vcpu);
 +	stgi();
 +
 +	/* Any pending NMI will happen here */
 +
 +	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 +		kvm_after_interrupt(&svm->vcpu);
 +
 +	sync_cr8_to_lapic(vcpu);
 +
 +	svm->next_rip = 0;
 +
 +	svm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;
 +
 +	/* if exit due to PF check for async PF */
 +	if (svm->vmcb->control.exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR)
 +		svm->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +
 +	if (npt_enabled) {
 +		vcpu->arch.regs_avail &= ~(1 << VCPU_EXREG_PDPTR);
 +		vcpu->arch.regs_dirty &= ~(1 << VCPU_EXREG_PDPTR);
 +	}
 +
 +	/*
 +	 * We need to handle MC intercepts here before the vcpu has a chance to
 +	 * change the physical cpu
 +	 */
 +	if (unlikely(svm->vmcb->control.exit_code ==
 +		     SVM_EXIT_EXCP_BASE + MC_VECTOR))
 +		svm_handle_mce(svm);
 +
 +	mark_all_clean(svm->vmcb);
 +}
 +
 +static void svm_set_cr3(struct kvm_vcpu *vcpu, unsigned long root)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb_control_area *control = &svm->vmcb->control;
 -	struct vmcb_save_area *save = &svm->vmcb->save;
  
 -	if (!dump_invalid_vmcb) {
 -		pr_warn_ratelimited("set kvm_amd.dump_invalid_vmcb=1 to dump internal KVM state.\n");
 +	svm->vmcb->save.cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_CR);
 +}
 +
 +static void set_tdp_cr3(struct kvm_vcpu *vcpu, unsigned long root)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->vmcb->control.nested_cr3 = __sme_set(root);
 +	mark_dirty(svm->vmcb, VMCB_NPT);
 +
 +	/* Also sync guest cr3 here in case we live migrate */
 +	svm->vmcb->save.cr3 = kvm_read_cr3(vcpu);
 +	mark_dirty(svm->vmcb, VMCB_CR);
 +}
 +
 +static int is_disabled(void)
 +{
 +	u64 vm_cr;
 +
 +	rdmsrl(MSR_VM_CR, vm_cr);
 +	if (vm_cr & (1 << SVM_VM_CR_SVM_DISABLE))
 +		return 1;
 +
 +	return 0;
 +}
 +
 +static void
 +svm_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +{
 +	/*
 +	 * Patch in the VMMCALL instruction:
 +	 */
 +	hypercall[0] = 0x0f;
 +	hypercall[1] = 0x01;
 +	hypercall[2] = 0xd9;
 +}
 +
 +static int __init svm_check_processor_compat(void)
 +{
 +	return 0;
 +}
 +
 +static bool svm_cpu_has_accelerated_tpr(void)
 +{
 +	return false;
 +}
 +
 +static bool svm_has_emulated_msr(int index)
 +{
 +	switch (index) {
 +	case MSR_IA32_MCG_EXT_CTL:
 +	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 +		return false;
 +	default:
 +		break;
 +	}
 +
 +	return true;
 +}
 +
 +static u64 svm_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 +{
 +	return 0;
 +}
 +
 +static void svm_cpuid_update(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	vcpu->arch.xsaves_enabled = guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&
 +				    boot_cpu_has(X86_FEATURE_XSAVE) &&
 +				    boot_cpu_has(X86_FEATURE_XSAVES);
 +
 +	/* Update nrips enabled cache */
 +	svm->nrips_enabled = !!guest_cpuid_has(&svm->vcpu, X86_FEATURE_NRIPS);
 +
 +	if (!kvm_vcpu_apicv_active(vcpu))
  		return;
 +
 +	guest_cpuid_clear(vcpu, X86_FEATURE_X2APIC);
 +}
 +
 +#define F feature_bit
 +
 +static void svm_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)
 +{
 +	switch (func) {
 +	case 0x1:
 +		if (avic)
 +			entry->ecx &= ~F(X2APIC);
 +		break;
 +	case 0x80000001:
 +		if (nested)
 +			entry->ecx |= (1 << 2); /* Set SVM bit */
 +		break;
 +	case 0x80000008:
 +		if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) ||
 +		     boot_cpu_has(X86_FEATURE_AMD_SSBD))
 +			entry->ebx |= F(VIRT_SSBD);
 +		break;
 +	case 0x8000000A:
 +		entry->eax = 1; /* SVM revision 1 */
 +		entry->ebx = 8; /* Lets support 8 ASIDs in case we add proper
 +				   ASID emulation to nested SVM */
 +		entry->ecx = 0; /* Reserved */
 +		entry->edx = 0; /* Per default do not support any
 +				   additional features */
 +
 +		/* Support next_rip if host supports it */
 +		if (boot_cpu_has(X86_FEATURE_NRIPS))
 +			entry->edx |= F(NRIPS);
 +
 +		/* Support NPT for the guest if enabled */
 +		if (npt_enabled)
 +			entry->edx |= F(NPT);
 +
  	}
 +}
  
 -	pr_err("VMCB Control Area:\n");
 -	pr_err("%-20s%04x\n", "cr_read:", control->intercept_cr & 0xffff);
 -	pr_err("%-20s%04x\n", "cr_write:", control->intercept_cr >> 16);
 -	pr_err("%-20s%04x\n", "dr_read:", control->intercept_dr & 0xffff);
 -	pr_err("%-20s%04x\n", "dr_write:", control->intercept_dr >> 16);
 -	pr_err("%-20s%08x\n", "exceptions:", control->intercept_exceptions);
 -	pr_err("%-20s%016llx\n", "intercepts:", control->intercept);
 -	pr_err("%-20s%d\n", "pause filter count:", control->pause_filter_count);
 -	pr_err("%-20s%d\n", "pause filter threshold:",
 -	       control->pause_filter_thresh);
 -	pr_err("%-20s%016llx\n", "iopm_base_pa:", control->iopm_base_pa);
 -	pr_err("%-20s%016llx\n", "msrpm_base_pa:", control->msrpm_base_pa);
 -	pr_err("%-20s%016llx\n", "tsc_offset:", control->tsc_offset);
 -	pr_err("%-20s%d\n", "asid:", control->asid);
 -	pr_err("%-20s%d\n", "tlb_ctl:", control->tlb_ctl);
 -	pr_err("%-20s%08x\n", "int_ctl:", control->int_ctl);
 -	pr_err("%-20s%08x\n", "int_vector:", control->int_vector);
 -	pr_err("%-20s%08x\n", "int_state:", control->int_state);
 -	pr_err("%-20s%08x\n", "exit_code:", control->exit_code);
 -	pr_err("%-20s%016llx\n", "exit_info1:", control->exit_info_1);
 -	pr_err("%-20s%016llx\n", "exit_info2:", control->exit_info_2);
 -	pr_err("%-20s%08x\n", "exit_int_info:", control->exit_int_info);
 -	pr_err("%-20s%08x\n", "exit_int_info_err:", control->exit_int_info_err);
 -	pr_err("%-20s%lld\n", "nested_ctl:", control->nested_ctl);
 -	pr_err("%-20s%016llx\n", "nested_cr3:", control->nested_cr3);
 -	pr_err("%-20s%016llx\n", "avic_vapic_bar:", control->avic_vapic_bar);
 -	pr_err("%-20s%08x\n", "event_inj:", control->event_inj);
 -	pr_err("%-20s%08x\n", "event_inj_err:", control->event_inj_err);
 -	pr_err("%-20s%lld\n", "virt_ext:", control->virt_ext);
 -	pr_err("%-20s%016llx\n", "next_rip:", control->next_rip);
 -	pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
 -	pr_err("%-20s%016llx\n", "avic_logical_id:", control->avic_logical_id);
 -	pr_err("%-20s%016llx\n", "avic_physical_id:", control->avic_physical_id);
 -	pr_err("VMCB State Save Area:\n");
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "es:",
 -	       save->es.selector, save->es.attrib,
 -	       save->es.limit, save->es.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "cs:",
 -	       save->cs.selector, save->cs.attrib,
 -	       save->cs.limit, save->cs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ss:",
 -	       save->ss.selector, save->ss.attrib,
 -	       save->ss.limit, save->ss.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ds:",
 -	       save->ds.selector, save->ds.attrib,
 -	       save->ds.limit, save->ds.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "fs:",
 -	       save->fs.selector, save->fs.attrib,
 -	       save->fs.limit, save->fs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "gs:",
 -	       save->gs.selector, save->gs.attrib,
 -	       save->gs.limit, save->gs.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "gdtr:",
 -	       save->gdtr.selector, save->gdtr.attrib,
 -	       save->gdtr.limit, save->gdtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "ldtr:",
 -	       save->ldtr.selector, save->ldtr.attrib,
 -	       save->ldtr.limit, save->ldtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "idtr:",
 -	       save->idtr.selector, save->idtr.attrib,
 -	       save->idtr.limit, save->idtr.base);
 -	pr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",
 -	       "tr:",
 -	       save->tr.selector, save->tr.attrib,
 -	       save->tr.limit, save->tr.base);
 -	pr_err("cpl:            %d                efer:         %016llx\n",
 -		save->cpl, save->efer);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cr0:", save->cr0, "cr2:", save->cr2);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cr3:", save->cr3, "cr4:", save->cr4);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "dr6:", save->dr6, "dr7:", save->dr7);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "rip:", save->rip, "rflags:", save->rflags);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "rsp:", save->rsp, "rax:", save->rax);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "star:", save->star, "lstar:", save->lstar);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "cstar:", save->cstar, "sfmask:", save->sfmask);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "kernel_gs_base:", save->kernel_gs_base,
 -	       "sysenter_cs:", save->sysenter_cs);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "sysenter_esp:", save->sysenter_esp,
 -	       "sysenter_eip:", save->sysenter_eip);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "gpat:", save->g_pat, "dbgctl:", save->dbgctl);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "br_from:", save->br_from, "br_to:", save->br_to);
 -	pr_err("%-15s %016llx %-13s %016llx\n",
 -	       "excp_from:", save->last_excp_from,
 -	       "excp_to:", save->last_excp_to);
 +static int svm_get_lpage_level(void)
 +{
 +	return PT_PDPE_LEVEL;
  }
  
 -static void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)
 +static bool svm_rdtscp_supported(void)
  {
 -	struct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;
 +	return boot_cpu_has(X86_FEATURE_RDTSCP);
 +}
 +
 +static bool svm_invpcid_supported(void)
 +{
 +	return false;
 +}
 +
 +static bool svm_mpx_supported(void)
 +{
 +	return false;
 +}
 +
 +static bool svm_xsaves_supported(void)
 +{
 +	return boot_cpu_has(X86_FEATURE_XSAVES);
 +}
 +
 +static bool svm_umip_emulated(void)
 +{
 +	return false;
 +}
 +
 +static bool svm_pt_supported(void)
 +{
 +	return false;
 +}
 +
 +static bool svm_has_wbinvd_exit(void)
 +{
 +	return true;
 +}
 +
 +static bool svm_pku_supported(void)
 +{
 +	return false;
 +}
 +
 +#define PRE_EX(exit)  { .exit_code = (exit), \
 +			.stage = X86_ICPT_PRE_EXCEPT, }
 +#define POST_EX(exit) { .exit_code = (exit), \
 +			.stage = X86_ICPT_POST_EXCEPT, }
 +#define POST_MEM(exit) { .exit_code = (exit), \
 +			.stage = X86_ICPT_POST_MEMACCESS, }
 +
 +static const struct __x86_intercept {
 +	u32 exit_code;
 +	enum x86_intercept_stage stage;
 +} x86_intercept_map[] = {
 +	[x86_intercept_cr_read]		= POST_EX(SVM_EXIT_READ_CR0),
 +	[x86_intercept_cr_write]	= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_clts]		= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_lmsw]		= POST_EX(SVM_EXIT_WRITE_CR0),
 +	[x86_intercept_smsw]		= POST_EX(SVM_EXIT_READ_CR0),
 +	[x86_intercept_dr_read]		= POST_EX(SVM_EXIT_READ_DR0),
 +	[x86_intercept_dr_write]	= POST_EX(SVM_EXIT_WRITE_DR0),
 +	[x86_intercept_sldt]		= POST_EX(SVM_EXIT_LDTR_READ),
 +	[x86_intercept_str]		= POST_EX(SVM_EXIT_TR_READ),
 +	[x86_intercept_lldt]		= POST_EX(SVM_EXIT_LDTR_WRITE),
 +	[x86_intercept_ltr]		= POST_EX(SVM_EXIT_TR_WRITE),
 +	[x86_intercept_sgdt]		= POST_EX(SVM_EXIT_GDTR_READ),
 +	[x86_intercept_sidt]		= POST_EX(SVM_EXIT_IDTR_READ),
 +	[x86_intercept_lgdt]		= POST_EX(SVM_EXIT_GDTR_WRITE),
 +	[x86_intercept_lidt]		= POST_EX(SVM_EXIT_IDTR_WRITE),
 +	[x86_intercept_vmrun]		= POST_EX(SVM_EXIT_VMRUN),
 +	[x86_intercept_vmmcall]		= POST_EX(SVM_EXIT_VMMCALL),
 +	[x86_intercept_vmload]		= POST_EX(SVM_EXIT_VMLOAD),
 +	[x86_intercept_vmsave]		= POST_EX(SVM_EXIT_VMSAVE),
 +	[x86_intercept_stgi]		= POST_EX(SVM_EXIT_STGI),
 +	[x86_intercept_clgi]		= POST_EX(SVM_EXIT_CLGI),
 +	[x86_intercept_skinit]		= POST_EX(SVM_EXIT_SKINIT),
 +	[x86_intercept_invlpga]		= POST_EX(SVM_EXIT_INVLPGA),
 +	[x86_intercept_rdtscp]		= POST_EX(SVM_EXIT_RDTSCP),
 +	[x86_intercept_monitor]		= POST_MEM(SVM_EXIT_MONITOR),
 +	[x86_intercept_mwait]		= POST_EX(SVM_EXIT_MWAIT),
 +	[x86_intercept_invlpg]		= POST_EX(SVM_EXIT_INVLPG),
 +	[x86_intercept_invd]		= POST_EX(SVM_EXIT_INVD),
 +	[x86_intercept_wbinvd]		= POST_EX(SVM_EXIT_WBINVD),
 +	[x86_intercept_wrmsr]		= POST_EX(SVM_EXIT_MSR),
 +	[x86_intercept_rdtsc]		= POST_EX(SVM_EXIT_RDTSC),
 +	[x86_intercept_rdmsr]		= POST_EX(SVM_EXIT_MSR),
 +	[x86_intercept_rdpmc]		= POST_EX(SVM_EXIT_RDPMC),
 +	[x86_intercept_cpuid]		= PRE_EX(SVM_EXIT_CPUID),
 +	[x86_intercept_rsm]		= PRE_EX(SVM_EXIT_RSM),
 +	[x86_intercept_pause]		= PRE_EX(SVM_EXIT_PAUSE),
 +	[x86_intercept_pushf]		= PRE_EX(SVM_EXIT_PUSHF),
 +	[x86_intercept_popf]		= PRE_EX(SVM_EXIT_POPF),
 +	[x86_intercept_intn]		= PRE_EX(SVM_EXIT_SWINT),
 +	[x86_intercept_iret]		= PRE_EX(SVM_EXIT_IRET),
 +	[x86_intercept_icebp]		= PRE_EX(SVM_EXIT_ICEBP),
 +	[x86_intercept_hlt]		= POST_EX(SVM_EXIT_HLT),
 +	[x86_intercept_in]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_ins]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_out]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_outs]		= POST_EX(SVM_EXIT_IOIO),
 +	[x86_intercept_xsetbv]		= PRE_EX(SVM_EXIT_XSETBV),
 +};
  
 -	*info1 = control->exit_info_1;
 -	*info2 = control->exit_info_2;
 -}
 +#undef PRE_EX
 +#undef POST_EX
 +#undef POST_MEM
  
 -static int handle_exit(struct kvm_vcpu *vcpu,
 -	enum exit_fastpath_completion exit_fastpath)
 +static int svm_check_intercept(struct kvm_vcpu *vcpu,
 +			       struct x86_instruction_info *info,
 +			       enum x86_intercept_stage stage)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct kvm_run *kvm_run = vcpu->run;
 -	u32 exit_code = svm->vmcb->control.exit_code;
 +	int vmexit, ret = X86EMUL_CONTINUE;
 +	struct __x86_intercept icpt_info;
 +	struct vmcb *vmcb = svm->vmcb;
  
 -	trace_kvm_exit(exit_code, vcpu, KVM_ISA_SVM);
 +	if (info->intercept >= ARRAY_SIZE(x86_intercept_map))
 +		goto out;
  
 -	if (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))
 -		vcpu->arch.cr0 = svm->vmcb->save.cr0;
 -	if (npt_enabled)
 -		vcpu->arch.cr3 = svm->vmcb->save.cr3;
 +	icpt_info = x86_intercept_map[info->intercept];
  
 -	if (unlikely(svm->nested.exit_required)) {
 -		nested_svm_vmexit(svm);
 -		svm->nested.exit_required = false;
 +	if (stage != icpt_info.stage)
 +		goto out;
  
 -		return 1;
 -	}
 +	switch (icpt_info.exit_code) {
 +	case SVM_EXIT_READ_CR0:
 +		if (info->intercept == x86_intercept_cr_read)
 +			icpt_info.exit_code += info->modrm_reg;
 +		break;
 +	case SVM_EXIT_WRITE_CR0: {
 +		unsigned long cr0, val;
 +		u64 intercept;
  
 -	if (is_guest_mode(vcpu)) {
 -		int vmexit;
 +		if (info->intercept == x86_intercept_cr_write)
 +			icpt_info.exit_code += info->modrm_reg;
  
 -		trace_kvm_nested_vmexit(svm->vmcb->save.rip, exit_code,
 -					svm->vmcb->control.exit_info_1,
 -					svm->vmcb->control.exit_info_2,
 -					svm->vmcb->control.exit_int_info,
 -					svm->vmcb->control.exit_int_info_err,
 -					KVM_ISA_SVM);
 +		if (icpt_info.exit_code != SVM_EXIT_WRITE_CR0 ||
 +		    info->intercept == x86_intercept_clts)
 +			break;
  
 -		vmexit = nested_svm_exit_special(svm);
 +		intercept = svm->nested.intercept;
  
 -		if (vmexit == NESTED_EXIT_CONTINUE)
 -			vmexit = nested_svm_exit_handled(svm);
 +		if (!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0)))
 +			break;
  
 -		if (vmexit == NESTED_EXIT_DONE)
 -			return 1;
 -	}
 +		cr0 = vcpu->arch.cr0 & ~SVM_CR0_SELECTIVE_MASK;
 +		val = info->src_val  & ~SVM_CR0_SELECTIVE_MASK;
  
 -	svm_complete_interrupts(svm);
 +		if (info->intercept == x86_intercept_lmsw) {
 +			cr0 &= 0xfUL;
 +			val &= 0xfUL;
 +			/* lmsw can't clear PE - catch this here */
 +			if (cr0 & X86_CR0_PE)
 +				val |= X86_CR0_PE;
 +		}
  
 -	if (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {
 -		kvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 -		kvm_run->fail_entry.hardware_entry_failure_reason
 -			= svm->vmcb->control.exit_code;
 -		dump_vmcb(vcpu);
 -		return 0;
 +		if (cr0 ^ val)
 +			icpt_info.exit_code = SVM_EXIT_CR0_SEL_WRITE;
 +
 +		break;
  	}
 +	case SVM_EXIT_READ_DR0:
 +	case SVM_EXIT_WRITE_DR0:
 +		icpt_info.exit_code += info->modrm_reg;
 +		break;
 +	case SVM_EXIT_MSR:
 +		if (info->intercept == x86_intercept_wrmsr)
 +			vmcb->control.exit_info_1 = 1;
 +		else
 +			vmcb->control.exit_info_1 = 0;
 +		break;
 +	case SVM_EXIT_PAUSE:
 +		/*
 +		 * We get this for NOP only, but pause
 +		 * is rep not, check this here
 +		 */
 +		if (info->rep_prefix != REPE_PREFIX)
 +			goto out;
 +		break;
 +	case SVM_EXIT_IOIO: {
 +		u64 exit_info;
 +		u32 bytes;
  
 -	if (is_external_interrupt(svm->vmcb->control.exit_int_info) &&
 -	    exit_code != SVM_EXIT_EXCP_BASE + PF_VECTOR &&
 -	    exit_code != SVM_EXIT_NPF && exit_code != SVM_EXIT_TASK_SWITCH &&
 -	    exit_code != SVM_EXIT_INTR && exit_code != SVM_EXIT_NMI)
 -		printk(KERN_ERR "%s: unexpected exit_int_info 0x%x "
 -		       "exit_code 0x%x\n",
 -		       __func__, svm->vmcb->control.exit_int_info,
 -		       exit_code);
 +		if (info->intercept == x86_intercept_in ||
 +		    info->intercept == x86_intercept_ins) {
 +			exit_info = ((info->src_val & 0xffff) << 16) |
 +				SVM_IOIO_TYPE_MASK;
 +			bytes = info->dst_bytes;
 +		} else {
 +			exit_info = (info->dst_val & 0xffff) << 16;
 +			bytes = info->src_bytes;
 +		}
  
 -	if (exit_fastpath == EXIT_FASTPATH_SKIP_EMUL_INS) {
 -		kvm_skip_emulated_instruction(vcpu);
 -		return 1;
 -	} else if (exit_code >= ARRAY_SIZE(svm_exit_handlers)
 -	    || !svm_exit_handlers[exit_code]) {
 -		vcpu_unimpl(vcpu, "svm: unexpected exit reason 0x%x\n", exit_code);
 -		dump_vmcb(vcpu);
 -		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 -		vcpu->run->internal.suberror =
 -			KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
 -		vcpu->run->internal.ndata = 1;
 -		vcpu->run->internal.data[0] = exit_code;
 -		return 0;
 -	}
 +		if (info->intercept == x86_intercept_outs ||
 +		    info->intercept == x86_intercept_ins)
 +			exit_info |= SVM_IOIO_STR_MASK;
  
 -#ifdef CONFIG_RETPOLINE
 -	if (exit_code == SVM_EXIT_MSR)
 -		return msr_interception(svm);
 -	else if (exit_code == SVM_EXIT_VINTR)
 -		return interrupt_window_interception(svm);
 -	else if (exit_code == SVM_EXIT_INTR)
 -		return intr_interception(svm);
 -	else if (exit_code == SVM_EXIT_HLT)
 -		return halt_interception(svm);
 -	else if (exit_code == SVM_EXIT_NPF)
 -		return npf_interception(svm);
 -#endif
 -	return svm_exit_handlers[exit_code](svm);
 -}
 +		if (info->rep_prefix)
 +			exit_info |= SVM_IOIO_REP_MASK;
  
 -static void reload_tss(struct kvm_vcpu *vcpu)
 -{
 -	int cpu = raw_smp_processor_id();
 +		bytes = min(bytes, 4u);
  
 -	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 -	sd->tss_desc->type = 9; /* available 32/64-bit TSS */
 -	load_TR_desc();
 -}
 +		exit_info |= bytes << SVM_IOIO_SIZE_SHIFT;
  
 -static void pre_svm_run(struct vcpu_svm *svm)
 -{
 -	int cpu = raw_smp_processor_id();
 +		exit_info |= (u32)info->ad_bytes << (SVM_IOIO_ASIZE_SHIFT - 1);
  
 -	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
 +		vmcb->control.exit_info_1 = exit_info;
 +		vmcb->control.exit_info_2 = info->next_rip;
  
 -	if (sev_guest(svm->vcpu.kvm))
 -		return pre_sev_run(svm, cpu);
 +		break;
 +	}
 +	default:
 +		break;
 +	}
  
 -	/* FIXME: handle wraparound of asid_generation */
 -	if (svm->asid_generation != sd->asid_generation)
 -		new_asid(svm, sd);
 -}
 +	/* TODO: Advertise NRIPS to guest hypervisor unconditionally */
 +	if (static_cpu_has(X86_FEATURE_NRIPS))
 +		vmcb->control.next_rip  = info->next_rip;
 +	vmcb->control.exit_code = icpt_info.exit_code;
 +	vmexit = nested_svm_exit_handled(svm);
  
 -static void svm_inject_nmi(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	ret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED
 +					   : X86EMUL_CONTINUE;
  
 -	svm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;
 -	vcpu->arch.hflags |= HF_NMI_MASK;
 -	set_intercept(svm, INTERCEPT_IRET);
 -	++vcpu->stat.nmi_injections;
 +out:
 +	return ret;
  }
  
 -static void svm_set_irq(struct kvm_vcpu *vcpu)
 +static void svm_handle_exit_irqoff(struct kvm_vcpu *vcpu,
 +	enum exit_fastpath_completion *exit_fastpath)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -
 -	BUG_ON(!(gif_set(svm)));
 -
 -	trace_kvm_inj_virq(vcpu->arch.interrupt.nr);
 -	++vcpu->stat.irq_injections;
 -
 -	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 -		SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;
 +	if (!is_guest_mode(vcpu) &&
 +	    to_svm(vcpu)->vmcb->control.exit_code == SVM_EXIT_MSR &&
 +	    to_svm(vcpu)->vmcb->control.exit_info_1)
 +		*exit_fastpath = handle_fastpath_set_msr_irqoff(vcpu);
  }
  
 -static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 +static void svm_sched_in(struct kvm_vcpu *vcpu, int cpu)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -
 -	if (svm_nested_virtualize_tpr(vcpu))
 -		return;
 -
 -	clr_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +	if (pause_filter_thresh)
 +		shrink_ple_window(vcpu);
 +}
  
 -	if (irr == -1)
 +static inline void avic_post_state_restore(struct kvm_vcpu *vcpu)
 +{
 +	if (avic_handle_apic_id_update(vcpu) != 0)
  		return;
 +	avic_handle_dfr_update(vcpu);
 +	avic_handle_ldr_update(vcpu);
 +}
  
 -	if (tpr >= irr)
 -		set_cr_intercept(svm, INTERCEPT_CR8_WRITE);
 +static void svm_setup_mce(struct kvm_vcpu *vcpu)
 +{
 +	/* [63:9] are reserved. */
 +	vcpu->arch.mcg_cap &= 0x1ff;
  }
  
 -bool svm_nmi_blocked(struct kvm_vcpu *vcpu)
++<<<<<<< HEAD
 +static int svm_smi_allowed(struct kvm_vcpu *vcpu)
++=======
++bool svm_smi_blocked(struct kvm_vcpu *vcpu)
++>>>>>>> cae96af18452 (KVM: SVM: Split out architectural interrupt/NMI/SMI blocking checks)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb *vmcb = svm->vmcb;
 -	bool ret;
  
 +	/* Per APM Vol.2 15.22.2 "Response to SMI" */
  	if (!gif_set(svm))
 -		return true;
++<<<<<<< HEAD
 +		return 0;
  
 -	if (is_guest_mode(vcpu) && nested_exit_on_nmi(svm))
 -		return false;
 +	if (is_guest_mode(&svm->vcpu) &&
 +	    svm->nested.intercept & (1ULL << INTERCEPT_SMI)) {
 +		/* TODO: Might need to set exit_info_1 and exit_info_2 here */
 +		svm->vmcb->control.exit_code = SVM_EXIT_SMI;
 +		svm->nested.exit_required = true;
 +		return 0;
 +	}
  
 -	ret = (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) ||
 -	      (svm->vcpu.arch.hflags & HF_NMI_MASK);
 +	return 1;
++=======
++		return true;
+ 
 -	return ret;
++	return is_smm(vcpu);
+ }
+ 
 -static bool svm_nmi_allowed(struct kvm_vcpu *vcpu)
++static bool svm_smi_allowed(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 	if (svm->nested.nested_run_pending)
+ 		return false;
+ 
 -        return !svm_nmi_blocked(vcpu);
++	return !svm_smi_blocked(vcpu);
++>>>>>>> cae96af18452 (KVM: SVM: Split out architectural interrupt/NMI/SMI blocking checks)
  }
  
 -static bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)
 +static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 +	int ret;
  
 -	return !!(svm->vcpu.arch.hflags & HF_NMI_MASK);
 -}
 +	if (is_guest_mode(vcpu)) {
 +		/* FED8h - SVM Guest */
 +		put_smstate(u64, smstate, 0x7ed8, 1);
 +		/* FEE0h - SVM Guest VMCB Physical Address */
 +		put_smstate(u64, smstate, 0x7ee0, svm->nested.vmcb);
  
 -static void svm_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +		svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 +		svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 +		svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
  
 -	if (masked) {
 -		svm->vcpu.arch.hflags |= HF_NMI_MASK;
 -		set_intercept(svm, INTERCEPT_IRET);
 -	} else {
 -		svm->vcpu.arch.hflags &= ~HF_NMI_MASK;
 -		clr_intercept(svm, INTERCEPT_IRET);
 +		ret = nested_svm_vmexit(svm);
 +		if (ret)
 +			return ret;
  	}
 +	return 0;
  }
  
 -bool svm_interrupt_blocked(struct kvm_vcpu *vcpu)
 +static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, const char *smstate)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb *vmcb = svm->vmcb;
 +	struct vmcb *nested_vmcb;
 +	struct kvm_host_map map;
 +	u64 guest;
 +	u64 vmcb;
  
 -	if (!gif_set(svm) ||
 -	     (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK))
 -		return true;
 +	guest = GET_SMSTATE(u64, smstate, 0x7ed8);
 +	vmcb = GET_SMSTATE(u64, smstate, 0x7ee0);
  
 -	if (is_guest_mode(vcpu) && (svm->vcpu.arch.hflags & HF_VINTR_MASK))
 -		return !(svm->vcpu.arch.hflags & HF_HIF_MASK);
 -	else
 -		return !(kvm_get_rflags(vcpu) & X86_EFLAGS_IF);
 +	if (guest) {
 +		if (kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb), &map) == -EINVAL)
 +			return 1;
 +		nested_vmcb = map.hva;
 +		enter_svm_guest_mode(svm, vmcb, nested_vmcb, &map);
 +	}
 +	return 0;
  }
  
 -static bool svm_interrupt_allowed(struct kvm_vcpu *vcpu)
 +static int enable_smi_window(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
 -	if (svm->nested.nested_run_pending)
 -		return false;
  
 -        return !svm_interrupt_blocked(vcpu);
 +	if (!gif_set(svm)) {
 +		if (vgif_enabled(svm))
 +			set_intercept(svm, INTERCEPT_STGI);
 +		/* STGI will cause a vm exit */
 +		return 1;
 +	}
 +	return 0;
  }
  
 -static void enable_irq_window(struct kvm_vcpu *vcpu)
 +static int sev_flush_asids(void)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	int ret, error;
  
  	/*
 -	 * In case GIF=0 we can't rely on the CPU to tell us when GIF becomes
 -	 * 1, because that's a separate STGI/VMRUN intercept.  The next time we
 -	 * get that intercept, this function will be called again though and
 -	 * we'll get the vintr intercept. However, if the vGIF feature is
 -	 * enabled, the STGI interception will not occur. Enable the irq
 -	 * window under the assumption that the hardware will set the GIF.
 +	 * DEACTIVATE will clear the WBINVD indicator causing DF_FLUSH to fail,
 +	 * so it must be guarded.
  	 */
 -	if (vgif_enabled(svm) || gif_set(svm)) {
 -		/*
 -		 * IRQ window is not needed when AVIC is enabled,
 -		 * unless we have pending ExtINT since it cannot be injected
 -		 * via AVIC. In such case, we need to temporarily disable AVIC,
 -		 * and fallback to injecting IRQ via V_IRQ.
 -		 */
 -		svm_toggle_avic_for_irq_window(vcpu, false);
 -		svm_set_vintr(svm);
 -	}
 -}
 +	down_write(&sev_deactivate_lock);
  
 -static void enable_nmi_window(struct kvm_vcpu *vcpu)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	wbinvd_on_all_cpus();
 +	ret = sev_guest_df_flush(&error);
  
 -	if ((svm->vcpu.arch.hflags & (HF_NMI_MASK | HF_IRET_MASK))
 -	    == HF_NMI_MASK)
 -		return; /* IRET will cause a vm exit */
 +	up_write(&sev_deactivate_lock);
  
 -	if (!gif_set(svm)) {
 -		if (vgif_enabled(svm))
 -			set_intercept(svm, INTERCEPT_STGI);
 -		return; /* STGI will cause a vm exit */
 -	}
 +	if (ret)
 +		pr_err("SEV: DF_FLUSH failed, ret=%d, error=%#x\n", ret, error);
  
 -	/*
 -	 * Something prevents NMI from been injected. Single step over possible
 -	 * problem (IRET or exception injection or interrupt shadow)
 -	 */
 -	svm->nmi_singlestep_guest_rflags = svm_get_rflags(vcpu);
 -	svm->nmi_singlestep = true;
 -	svm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);
 +	return ret;
  }
  
 -static int svm_set_tss_addr(struct kvm *kvm, unsigned int addr)
 +/* Must be called with the sev_bitmap_lock held */
 +static bool __sev_recycle_asids(void)
  {
 -	return 0;
 -}
 +	int pos;
  
 -static int svm_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)
 -{
 -	return 0;
 +	/* Check if there are any ASIDs to reclaim before performing a flush */
 +	pos = find_next_bit(sev_reclaim_asid_bitmap,
 +			    max_sev_asid, min_sev_asid - 1);
 +	if (pos >= max_sev_asid)
 +		return false;
 +
 +	if (sev_flush_asids())
 +		return false;
 +
 +	bitmap_xor(sev_asid_bitmap, sev_asid_bitmap, sev_reclaim_asid_bitmap,
 +		   max_sev_asid);
 +	bitmap_zero(sev_reclaim_asid_bitmap, max_sev_asid);
 +
 +	return true;
  }
  
 -void svm_flush_tlb(struct kvm_vcpu *vcpu)
 +static int sev_asid_new(void)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	bool retry = true;
 +	int pos;
 +
 +	mutex_lock(&sev_bitmap_lock);
  
  	/*
 -	 * Flush only the current ASID even if the TLB flush was invoked via
 -	 * kvm_flush_remote_tlbs().  Although flushing remote TLBs requires all
 -	 * ASIDs to be flushed, KVM uses a single ASID for L1 and L2, and
 -	 * unconditionally does a TLB flush on both nested VM-Enter and nested
 -	 * VM-Exit (via kvm_mmu_reset_context()).
 +	 * SEV-enabled guest must use asid from min_sev_asid to max_sev_asid.
  	 */
 -	if (static_cpu_has(X86_FEATURE_FLUSHBYASID))
 -		svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 -	else
 -		svm->asid_generation--;
 -}
 +again:
 +	pos = find_next_zero_bit(sev_asid_bitmap, max_sev_asid, min_sev_asid - 1);
 +	if (pos >= max_sev_asid) {
 +		if (retry && __sev_recycle_asids()) {
 +			retry = false;
 +			goto again;
 +		}
 +		mutex_unlock(&sev_bitmap_lock);
 +		return -EBUSY;
 +	}
  
 -static void svm_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t gva)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	__set_bit(pos, sev_asid_bitmap);
  
 -	invlpga(gva, svm->vmcb->control.asid);
 +	mutex_unlock(&sev_bitmap_lock);
 +
 +	return pos + 1;
  }
  
 -static void svm_prepare_guest_switch(struct kvm_vcpu *vcpu)
 +static int sev_guest_init(struct kvm *kvm, struct kvm_sev_cmd *argp)
  {
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	int asid, ret;
 +
 +	ret = -EBUSY;
 +	if (unlikely(sev->active))
 +		return ret;
 +
 +	asid = sev_asid_new();
 +	if (asid < 0)
 +		return ret;
 +
 +	ret = sev_platform_init(&argp->error);
 +	if (ret)
 +		goto e_free;
 +
 +	sev->active = true;
 +	sev->asid = asid;
 +	INIT_LIST_HEAD(&sev->regions_list);
 +
 +	return 0;
 +
 +e_free:
 +	sev_asid_free(asid);
 +	return ret;
  }
  
 -static inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)
 +static int sev_bind_asid(struct kvm *kvm, unsigned int handle, int *error)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	struct sev_data_activate *data;
 +	int asid = sev_get_asid(kvm);
 +	int ret;
  
 -	if (svm_nested_virtualize_tpr(vcpu))
 -		return;
 +	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
 +	if (!data)
 +		return -ENOMEM;
  
 -	if (!is_cr_intercept(svm, INTERCEPT_CR8_WRITE)) {
 -		int cr8 = svm->vmcb->control.int_ctl & V_TPR_MASK;
 -		kvm_set_cr8(vcpu, cr8);
 -	}
 +	/* activate ASID on the given handle */
 +	data->handle = handle;
 +	data->asid   = asid;
 +	ret = sev_guest_activate(data, error);
 +	kfree(data);
 +
 +	return ret;
  }
  
 -static inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)
 +static int __sev_issue_cmd(int fd, int id, void *data, int *error)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	u64 cr8;
 +	struct fd f;
 +	int ret;
  
 -	if (svm_nested_virtualize_tpr(vcpu) ||
 -	    kvm_vcpu_apicv_active(vcpu))
 -		return;
 +	f = fdget(fd);
 +	if (!f.file)
 +		return -EBADF;
  
 -	cr8 = kvm_get_cr8(vcpu);
 -	svm->vmcb->control.int_ctl &= ~V_TPR_MASK;
 -	svm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;
 +	ret = sev_issue_cmd_external_user(f.file, id, data, error);
 +
 +	fdput(f);
 +	return ret;
  }
  
 -static void svm_complete_interrupts(struct vcpu_svm *svm)
 +static int sev_issue_cmd(struct kvm *kvm, int id, void *data, int *error)
  {
 -	u8 vector;
 -	int type;
 -	u32 exitintinfo = svm->vmcb->control.exit_int_info;
 -	unsigned int3_injected = svm->int3_injected;
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
  
 -	svm->int3_injected = 0;
 +	return __sev_issue_cmd(sev->fd, id, data, error);
 +}
  
 -	/*
 -	 * If we've made progress since setting HF_IRET_MASK, we've
 -	 * executed an IRET and can allow NMI injection.
 -	 */
 -	if ((svm->vcpu.arch.hflags & HF_IRET_MASK)
 -	    && kvm_rip_read(&svm->vcpu) != svm->nmi_iret_rip) {
 -		svm->vcpu.arch.hflags &= ~(HF_NMI_MASK | HF_IRET_MASK);
 -		kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 -	}
 +static int sev_launch_start(struct kvm *kvm, struct kvm_sev_cmd *argp)
 +{
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct sev_data_launch_start *start;
 +	struct kvm_sev_launch_start params;
 +	void *dh_blob, *session_blob;
 +	int *error = &argp->error;
 +	int ret;
  
 -	svm->vcpu.arch.nmi_injected = false;
 -	kvm_clear_exception_queue(&svm->vcpu);
 -	kvm_clear_interrupt_queue(&svm->vcpu);
 +	if (!sev_guest(kvm))
 +		return -ENOTTY;
  
 -	if (!(exitintinfo & SVM_EXITINTINFO_VALID))
 -		return;
 +	if (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))
 +		return -EFAULT;
  
 -	kvm_make_request(KVM_REQ_EVENT, &svm->vcpu);
 +	start = kzalloc(sizeof(*start), GFP_KERNEL_ACCOUNT);
 +	if (!start)
 +		return -ENOMEM;
  
 -	vector = exitintinfo & SVM_EXITINTINFO_VEC_MASK;
 -	type = exitintinfo & SVM_EXITINTINFO_TYPE_MASK;
 +	dh_blob = NULL;
 +	if (params.dh_uaddr) {
 +		dh_blob = psp_copy_user_blob(params.dh_uaddr, params.dh_len);
 +		if (IS_ERR(dh_blob)) {
 +			ret = PTR_ERR(dh_blob);
 +			goto e_free;
 +		}
  
 -	switch (type) {
 -	case SVM_EXITINTINFO_TYPE_NMI:
 -		svm->vcpu.arch.nmi_injected = true;
 -		break;
 -	case SVM_EXITINTINFO_TYPE_EXEPT:
 -		/*
 -		 * In case of software exceptions, do not reinject the vector,
 -		 * but re-execute the instruction instead. Rewind RIP first
 -		 * if we emulated INT3 before.
 -		 */
 -		if (kvm_exception_is_soft(vector)) {
 -			if (vector == BP_VECTOR && int3_injected &&
 -			    kvm_is_linear_rip(&svm->vcpu, svm->int3_rip))
 -				kvm_rip_write(&svm->vcpu,
 -					      kvm_rip_read(&svm->vcpu) -
 -					      int3_injected);
 -			break;
 +		start->dh_cert_address = __sme_set(__pa(dh_blob));
 +		start->dh_cert_len = params.dh_len;
 +	}
 +
 +	session_blob = NULL;
 +	if (params.session_uaddr) {
 +		session_blob = psp_copy_user_blob(params.session_uaddr, params.session_len);
 +		if (IS_ERR(session_blob)) {
 +			ret = PTR_ERR(session_blob);
 +			goto e_free_dh;
  		}
 -		if (exitintinfo & SVM_EXITINTINFO_VALID_ERR) {
 -			u32 err = svm->vmcb->control.exit_int_info_err;
 -			kvm_requeue_exception_e(&svm->vcpu, vector, err);
  
 -		} else
 -			kvm_requeue_exception(&svm->vcpu, vector);
 -		break;
 -	case SVM_EXITINTINFO_TYPE_INTR:
 -		kvm_queue_interrupt(&svm->vcpu, vector, false);
 -		break;
 -	default:
 -		break;
 +		start->session_address = __sme_set(__pa(session_blob));
 +		start->session_len = params.session_len;
 +	}
 +
 +	start->handle = params.handle;
 +	start->policy = params.policy;
 +
 +	/* create memory encryption context */
 +	ret = __sev_issue_cmd(argp->sev_fd, SEV_CMD_LAUNCH_START, start, error);
 +	if (ret)
 +		goto e_free_session;
 +
 +	/* Bind ASID to this guest */
 +	ret = sev_bind_asid(kvm, start->handle, error);
 +	if (ret)
 +		goto e_free_session;
 +
 +	/* return handle to userspace */
 +	params.handle = start->handle;
 +	if (copy_to_user((void __user *)(uintptr_t)argp->data, &params, sizeof(params))) {
 +		sev_unbind_asid(kvm, start->handle);
 +		ret = -EFAULT;
 +		goto e_free_session;
  	}
 +
 +	sev->handle = start->handle;
 +	sev->fd = argp->sev_fd;
 +
 +e_free_session:
 +	kfree(session_blob);
 +e_free_dh:
 +	kfree(dh_blob);
 +e_free:
 +	kfree(start);
 +	return ret;
  }
  
 -static void svm_cancel_injection(struct kvm_vcpu *vcpu)
 +static unsigned long get_num_contig_pages(unsigned long idx,
 +				struct page **inpages, unsigned long npages)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	struct vmcb_control_area *control = &svm->vmcb->control;
 +	unsigned long paddr, next_paddr;
 +	unsigned long i = idx + 1, pages = 1;
  
 -	control->exit_int_info = control->event_inj;
 -	control->exit_int_info_err = control->event_inj_err;
 -	control->event_inj = 0;
 -	svm_complete_interrupts(svm);
 +	/* find the number of contiguous pages starting from idx */
 +	paddr = __sme_page_pa(inpages[idx]);
 +	while (i < npages) {
 +		next_paddr = __sme_page_pa(inpages[i++]);
 +		if ((paddr + PAGE_SIZE) == next_paddr) {
 +			pages++;
 +			paddr = next_paddr;
 +			continue;
 +		}
 +		break;
 +	}
 +
 +	return pages;
  }
  
 -static enum exit_fastpath_completion svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 +static int sev_launch_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)
  {
 -	if (!is_guest_mode(vcpu) &&
 -	    to_svm(vcpu)->vmcb->control.exit_code == SVM_EXIT_MSR &&
 -	    to_svm(vcpu)->vmcb->control.exit_info_1)
 -		return handle_fastpath_set_msr_irqoff(vcpu);
 +	unsigned long vaddr, vaddr_end, next_vaddr, npages, pages, size, i;
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct kvm_sev_launch_update_data params;
 +	struct sev_data_launch_update_data *data;
 +	struct page **inpages;
 +	int ret;
  
 -	return EXIT_FASTPATH_NONE;
 -}
 +	if (!sev_guest(kvm))
 +		return -ENOTTY;
  
 -void __svm_vcpu_run(unsigned long vmcb_pa, unsigned long *regs);
 +	if (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))
 +		return -EFAULT;
  
 -static enum exit_fastpath_completion svm_vcpu_run(struct kvm_vcpu *vcpu)
 -{
 -	enum exit_fastpath_completion exit_fastpath;
 -	struct vcpu_svm *svm = to_svm(vcpu);
 +	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
 +	if (!data)
 +		return -ENOMEM;
  
 -	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 -	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 -	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
 +	vaddr = params.uaddr;
 +	size = params.len;
 +	vaddr_end = vaddr + size;
  
 -	/*
 -	 * A vmexit emulation is required before the vcpu can be executed
 -	 * again.
 -	 */
 -	if (unlikely(svm->nested.exit_required))
 -		return EXIT_FASTPATH_NONE;
 +	/* Lock the user memory. */
 +	inpages = sev_pin_memory(kvm, vaddr, size, &npages, 1);
 +	if (!inpages) {
 +		ret = -ENOMEM;
 +		goto e_free;
 +	}
  
  	/*
 -	 * Disable singlestep if we're injecting an interrupt/exception.
 -	 * We don't want our modified rflags to be pushed on the stack where
 -	 * we might not be able to easily reset them if we disabled NMI
 -	 * singlestep later.
 +	 * The LAUNCH_UPDATE command will perform in-place encryption of the
 +	 * memory content (i.e it will write the same memory region with C=1).
 +	 * It's possible that the cache may contain the data with C=0, i.e.,
 +	 * unencrypted so invalidate it first.
  	 */
 -	if (svm->nmi_singlestep && svm->vmcb->control.event_inj) {
 +	sev_clflush_pages(inpages, npages);
 +
 +	for (i = 0; vaddr < vaddr_end; vaddr = next_vaddr, i += pages) {
 +		int offset, len;
 +
  		/*
 -		 * Event injection happens before external interrupts cause a
 -		 * vmexit and interrupts are disabled here, so smp_send_reschedule
 -		 * is enough to force an immediate vmexit.
 +		 * If the user buffer is not page-aligned, calculate the offset
 +		 * within the page.
  		 */
 -		disable_nmi_singlestep(svm);
 -		smp_send_reschedule(vcpu->cpu);
 -	}
 -
 -	pre_svm_run(svm);
 +		offset = vaddr & (PAGE_SIZE - 1);
  
 -	sync_lapic_to_cr8(vcpu);
 +		/* Calculate the number of pages that can be encrypted in one go. */
 +		pages = get_num_contig_pages(i, inpages, npages);
  
 -	svm->vmcb->save.cr2 = vcpu->arch.cr2;
 +		len = min_t(size_t, ((pages * PAGE_SIZE) - offset), size);
  
 -	/*
 -	 * Run with all-zero DR6 unless needed, so that we can get the exact cause
 -	 * of a #DB.
 -	 */
 -	if (unlikely(svm->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT))
 -		svm_set_dr6(svm, vcpu->arch.dr6);
 -	else
 -		svm_set_dr6(svm, DR6_FIXED_1 | DR6_RTM);
 +		data->handle = sev->handle;
 +		data->len = len;
 +		data->address = __sme_page_pa(inpages[i]) + offset;
 +		ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_UPDATE_DATA, data, &argp->error);
 +		if (ret)
 +			goto e_unpin;
  
 -	clgi();
 -	kvm_load_guest_xsave_state(vcpu);
 +		size -= len;
 +		next_vaddr = vaddr + len;
 +	}
  
 -	if (lapic_in_kernel(vcpu) &&
 -		vcpu->arch.apic->lapic_timer.timer_advance_ns)
 -		kvm_wait_lapic_expire(vcpu);
 +e_unpin:
 +	/* content of memory is updated, mark pages dirty */
 +	for (i = 0; i < npages; i++) {
 +		set_page_dirty_lock(inpages[i]);
 +		mark_page_accessed(inpages[i]);
 +	}
 +	/* unlock the user pages */
 +	sev_unpin_memory(kvm, inpages, npages);
 +e_free:
 +	kfree(data);
 +	return ret;
 +}
  
 -	/*
 -	 * If this vCPU has touched SPEC_CTRL, restore the guest's value if
 -	 * it's non-zero. Since vmentry is serialising on affected CPUs, there
 -	 * is no need to worry about the conditional branch over the wrmsr
 -	 * being speculatively taken.
 -	 */
 -	x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
 +static int sev_launch_measure(struct kvm *kvm, struct kvm_sev_cmd *argp)
 +{
 +	void __user *measure = (void __user *)(uintptr_t)argp->data;
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct sev_data_launch_measure *data;
 +	struct kvm_sev_launch_measure params;
 +	void __user *p = NULL;
 +	void *blob = NULL;
 +	int ret;
  
 -	__svm_vcpu_run(svm->vmcb_pa, (unsigned long *)&svm->vcpu.arch.regs);
 +	if (!sev_guest(kvm))
 +		return -ENOTTY;
  
 -#ifdef CONFIG_X86_64
 -	wrmsrl(MSR_GS_BASE, svm->host.gs_base);
 -#else
 -	loadsegment(fs, svm->host.fs);
 -#ifndef CONFIG_X86_32_LAZY_GS
 -	loadsegment(gs, svm->host.gs);
 -#endif
 -#endif
 +	if (copy_from_user(&params, measure, sizeof(params)))
 +		return -EFAULT;
  
 -	/*
 -	 * We do not use IBRS in the kernel. If this vCPU has used the
 -	 * SPEC_CTRL MSR it may have left it on; save the value and
 -	 * turn it off. This is much more efficient than blindly adding
 -	 * it to the atomic save/restore list. Especially as the former
 -	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
 -	 *
 -	 * For non-nested case:
 -	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
 -	 * save it.
 -	 *
 -	 * For nested case:
 -	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
 -	 * save it.
 -	 */
 -	if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
 -		svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 +	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
 +	if (!data)
 +		return -ENOMEM;
  
 -	reload_tss(vcpu);
 +	/* User wants to query the blob length */
 +	if (!params.len)
 +		goto cmd;
  
 -	x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
 +	p = (void __user *)(uintptr_t)params.uaddr;
 +	if (p) {
 +		if (params.len > SEV_FW_BLOB_MAX_SIZE) {
 +			ret = -EINVAL;
 +			goto e_free;
 +		}
  
 -	vcpu->arch.cr2 = svm->vmcb->save.cr2;
 -	vcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;
 -	vcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;
 -	vcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;
 +		ret = -ENOMEM;
 +		blob = kmalloc(params.len, GFP_KERNEL);
 +		if (!blob)
 +			goto e_free;
  
 -	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 -		kvm_before_interrupt(&svm->vcpu);
 +		data->address = __psp_pa(blob);
 +		data->len = params.len;
 +	}
  
 -	kvm_load_host_xsave_state(vcpu);
 -	stgi();
 +cmd:
 +	data->handle = sev->handle;
 +	ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_MEASURE, data, &argp->error);
  
 -	/* Any pending NMI will happen here */
 -	exit_fastpath = svm_exit_handlers_fastpath(vcpu);
 +	/*
 +	 * If we query the session length, FW responded with expected data.
 +	 */
 +	if (!params.len)
 +		goto done;
  
 -	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 -		kvm_after_interrupt(&svm->vcpu);
 +	if (ret)
 +		goto e_free_blob;
  
 -	sync_cr8_to_lapic(vcpu);
 +	if (blob) {
 +		if (copy_to_user(p, blob, params.len))
 +			ret = -EFAULT;
 +	}
  
 -	svm->next_rip = 0;
 -	svm->nested.nested_run_pending = 0;
 +done:
 +	params.len = data->len;
 +	if (copy_to_user(measure, &params, sizeof(params)))
 +		ret = -EFAULT;
 +e_free_blob:
 +	kfree(blob);
 +e_free:
 +	kfree(data);
 +	return ret;
 +}
  
 -	svm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;
 +static int sev_launch_finish(struct kvm *kvm, struct kvm_sev_cmd *argp)
 +{
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct sev_data_launch_finish *data;
 +	int ret;
  
 -	/* if exit due to PF check for async PF */
 -	if (svm->vmcb->control.exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR)
 -		svm->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();
 +	if (!sev_guest(kvm))
 +		return -ENOTTY;
  
 -	if (npt_enabled) {
 -		vcpu->arch.regs_avail &= ~(1 << VCPU_EXREG_PDPTR);
 -		vcpu->arch.regs_dirty &= ~(1 << VCPU_EXREG_PDPTR);
 -	}
 +	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
 +	if (!data)
 +		return -ENOMEM;
  
 -	/*
 -	 * We need to handle MC intercepts here before the vcpu has a chance to
 -	 * change the physical cpu
 -	 */
 -	if (unlikely(svm->vmcb->control.exit_code ==
 -		     SVM_EXIT_EXCP_BASE + MC_VECTOR))
 -		svm_handle_mce(svm);
 +	data->handle = sev->handle;
 +	ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_FINISH, data, &argp->error);
  
 -	mark_all_clean(svm->vmcb);
 -	return exit_fastpath;
 +	kfree(data);
 +	return ret;
  }
  
 -static void svm_load_mmu_pgd(struct kvm_vcpu *vcpu, unsigned long root)
 +static int sev_guest_status(struct kvm *kvm, struct kvm_sev_cmd *argp)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	bool update_guest_cr3 = true;
 -	unsigned long cr3;
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct kvm_sev_guest_status params;
 +	struct sev_data_guest_status *data;
 +	int ret;
  
 -	cr3 = __sme_set(root);
 -	if (npt_enabled) {
 -		svm->vmcb->control.nested_cr3 = cr3;
 -		mark_dirty(svm->vmcb, VMCB_NPT);
 +	if (!sev_guest(kvm))
 +		return -ENOTTY;
  
 -		/* Loading L2's CR3 is handled by enter_svm_guest_mode.  */
 -		if (is_guest_mode(vcpu))
 -			update_guest_cr3 = false;
 -		else if (test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))
 -			cr3 = vcpu->arch.cr3;
 -		else /* CR3 is already up-to-date.  */
 -			update_guest_cr3 = false;
 -	}
 +	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
 +	if (!data)
 +		return -ENOMEM;
  
 -	if (update_guest_cr3) {
 -		svm->vmcb->save.cr3 = cr3;
 -		mark_dirty(svm->vmcb, VMCB_CR);
 -	}
 +	data->handle = sev->handle;
 +	ret = sev_issue_cmd(kvm, SEV_CMD_GUEST_STATUS, data, &argp->error);
 +	if (ret)
 +		goto e_free;
 +
 +	params.policy = data->policy;
 +	params.state = data->state;
 +	params.handle = data->handle;
 +
 +	if (copy_to_user((void __user *)(uintptr_t)argp->data, &params, sizeof(params)))
 +		ret = -EFAULT;
 +e_free:
 +	kfree(data);
 +	return ret;
  }
  
 -static int is_disabled(void)
 +static int __sev_issue_dbg_cmd(struct kvm *kvm, unsigned long src,
 +			       unsigned long dst, int size,
 +			       int *error, bool enc)
  {
 -	u64 vm_cr;
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct sev_data_dbg *data;
 +	int ret;
  
 -	rdmsrl(MSR_VM_CR, vm_cr);
 -	if (vm_cr & (1 << SVM_VM_CR_SVM_DISABLE))
 -		return 1;
 +	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
 +	if (!data)
 +		return -ENOMEM;
  
 -	return 0;
 +	data->handle = sev->handle;
 +	data->dst_addr = dst;
 +	data->src_addr = src;
 +	data->len = size;
 +
 +	ret = sev_issue_cmd(kvm,
 +			    enc ? SEV_CMD_DBG_ENCRYPT : SEV_CMD_DBG_DECRYPT,
 +			    data, error);
 +	kfree(data);
 +	return ret;
  }
  
 -static void
 -svm_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 +static int __sev_dbg_decrypt(struct kvm *kvm, unsigned long src_paddr,
 +			     unsigned long dst_paddr, int sz, int *err)
  {
 +	int offset;
 +
  	/*
 -	 * Patch in the VMMCALL instruction:
 +	 * Its safe to read more than we are asked, caller should ensure that
 +	 * destination has enough space.
  	 */
 -	hypercall[0] = 0x0f;
 -	hypercall[1] = 0x01;
 -	hypercall[2] = 0xd9;
 -}
 +	src_paddr = round_down(src_paddr, 16);
 +	offset = src_paddr & 15;
 +	sz = round_up(sz + offset, 16);
  
 -static int __init svm_check_processor_compat(void)
 -{
 -	return 0;
 +	return __sev_issue_dbg_cmd(kvm, src_paddr, dst_paddr, sz, err, false);
  }
  
 -static bool svm_cpu_has_accelerated_tpr(void)
 +static int __sev_dbg_decrypt_user(struct kvm *kvm, unsigned long paddr,
 +				  unsigned long __user dst_uaddr,
 +				  unsigned long dst_paddr,
 +				  int size, int *err)
  {
 -	return false;
 -}
 +	struct page *tpage = NULL;
 +	int ret, offset;
  
 -static bool svm_has_emulated_msr(int index)
 -{
 -	switch (index) {
 -	case MSR_IA32_MCG_EXT_CTL:
 -	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
 -		return false;
 -	default:
 -		break;
 +	/* if inputs are not 16-byte then use intermediate buffer */
 +	if (!IS_ALIGNED(dst_paddr, 16) ||
 +	    !IS_ALIGNED(paddr,     16) ||
 +	    !IS_ALIGNED(size,      16)) {
 +		tpage = (void *)alloc_page(GFP_KERNEL);
 +		if (!tpage)
 +			return -ENOMEM;
 +
 +		dst_paddr = __sme_page_pa(tpage);
  	}
  
 -	return true;
 -}
 +	ret = __sev_dbg_decrypt(kvm, paddr, dst_paddr, size, err);
 +	if (ret)
 +		goto e_free;
  
 -static u64 svm_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)
 -{
 -	return 0;
 +	if (tpage) {
 +		offset = paddr & 15;
 +		if (copy_to_user((void __user *)(uintptr_t)dst_uaddr,
 +				 page_address(tpage) + offset, size))
 +			ret = -EFAULT;
 +	}
 +
 +e_free:
 +	if (tpage)
 +		__free_page(tpage);
 +
 +	return ret;
  }
  
 -static void svm_cpuid_update(struct kvm_vcpu *vcpu)
 +static int __sev_dbg_encrypt_user(struct kvm *kvm, unsigned long paddr,
 +				  unsigned long __user vaddr,
 +				  unsigned long dst_paddr,
 +				  unsigned long __user dst_vaddr,
 +				  int size, int *error)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -
 -	vcpu->arch.xsaves_enabled = guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&
 -				    boot_cpu_has(X86_FEATURE_XSAVE) &&
 -				    boot_cpu_has(X86_FEATURE_XSAVES);
 +	struct page *src_tpage = NULL;
 +	struct page *dst_tpage = NULL;
 +	int ret, len = size;
  
 -	/* Update nrips enabled cache */
 -	svm->nrips_enabled = kvm_cpu_cap_has(X86_FEATURE_NRIPS) &&
 -			     guest_cpuid_has(&svm->vcpu, X86_FEATURE_NRIPS);
 +	/* If source buffer is not aligned then use an intermediate buffer */
 +	if (!IS_ALIGNED(vaddr, 16)) {
 +		src_tpage = alloc_page(GFP_KERNEL);
 +		if (!src_tpage)
 +			return -ENOMEM;
  
 -	if (!kvm_vcpu_apicv_active(vcpu))
 -		return;
 +		if (copy_from_user(page_address(src_tpage),
 +				(void __user *)(uintptr_t)vaddr, size)) {
 +			__free_page(src_tpage);
 +			return -EFAULT;
 +		}
  
 -	/*
 -	 * AVIC does not work with an x2APIC mode guest. If the X2APIC feature
 -	 * is exposed to the guest, disable AVIC.
 -	 */
 -	if (guest_cpuid_has(vcpu, X86_FEATURE_X2APIC))
 -		kvm_request_apicv_update(vcpu->kvm, false,
 -					 APICV_INHIBIT_REASON_X2APIC);
 +		paddr = __sme_page_pa(src_tpage);
 +	}
  
  	/*
 -	 * Currently, AVIC does not work with nested virtualization.
 -	 * So, we disable AVIC when cpuid for SVM is set in the L1 guest.
 +	 *  If destination buffer or length is not aligned then do read-modify-write:
 +	 *   - decrypt destination in an intermediate buffer
 +	 *   - copy the source buffer in an intermediate buffer
 +	 *   - use the intermediate buffer as source buffer
  	 */
 -	if (nested && guest_cpuid_has(vcpu, X86_FEATURE_SVM))
 -		kvm_request_apicv_update(vcpu->kvm, false,
 -					 APICV_INHIBIT_REASON_NESTED);
 -}
 +	if (!IS_ALIGNED(dst_vaddr, 16) || !IS_ALIGNED(size, 16)) {
 +		int dst_offset;
  
 -static bool svm_has_wbinvd_exit(void)
 -{
 -	return true;
 -}
 +		dst_tpage = alloc_page(GFP_KERNEL);
 +		if (!dst_tpage) {
 +			ret = -ENOMEM;
 +			goto e_free;
 +		}
  
 -#define PRE_EX(exit)  { .exit_code = (exit), \
 -			.stage = X86_ICPT_PRE_EXCEPT, }
 -#define POST_EX(exit) { .exit_code = (exit), \
 -			.stage = X86_ICPT_POST_EXCEPT, }
 -#define POST_MEM(exit) { .exit_code = (exit), \
 -			.stage = X86_ICPT_POST_MEMACCESS, }
 +		ret = __sev_dbg_decrypt(kvm, dst_paddr,
 +					__sme_page_pa(dst_tpage), size, error);
 +		if (ret)
 +			goto e_free;
  
 -static const struct __x86_intercept {
 -	u32 exit_code;
 -	enum x86_intercept_stage stage;
 -} x86_intercept_map[] = {
 -	[x86_intercept_cr_read]		= POST_EX(SVM_EXIT_READ_CR0),
 -	[x86_intercept_cr_write]	= POST_EX(SVM_EXIT_WRITE_CR0),
 -	[x86_intercept_clts]		= POST_EX(SVM_EXIT_WRITE_CR0),
 -	[x86_intercept_lmsw]		= POST_EX(SVM_EXIT_WRITE_CR0),
 -	[x86_intercept_smsw]		= POST_EX(SVM_EXIT_READ_CR0),
 -	[x86_intercept_dr_read]		= POST_EX(SVM_EXIT_READ_DR0),
 -	[x86_intercept_dr_write]	= POST_EX(SVM_EXIT_WRITE_DR0),
 -	[x86_intercept_sldt]		= POST_EX(SVM_EXIT_LDTR_READ),
 -	[x86_intercept_str]		= POST_EX(SVM_EXIT_TR_READ),
 -	[x86_intercept_lldt]		= POST_EX(SVM_EXIT_LDTR_WRITE),
 -	[x86_intercept_ltr]		= POST_EX(SVM_EXIT_TR_WRITE),
 -	[x86_intercept_sgdt]		= POST_EX(SVM_EXIT_GDTR_READ),
 -	[x86_intercept_sidt]		= POST_EX(SVM_EXIT_IDTR_READ),
 -	[x86_intercept_lgdt]		= POST_EX(SVM_EXIT_GDTR_WRITE),
 -	[x86_intercept_lidt]		= POST_EX(SVM_EXIT_IDTR_WRITE),
 -	[x86_intercept_vmrun]		= POST_EX(SVM_EXIT_VMRUN),
 -	[x86_intercept_vmmcall]		= POST_EX(SVM_EXIT_VMMCALL),
 -	[x86_intercept_vmload]		= POST_EX(SVM_EXIT_VMLOAD),
 -	[x86_intercept_vmsave]		= POST_EX(SVM_EXIT_VMSAVE),
 -	[x86_intercept_stgi]		= POST_EX(SVM_EXIT_STGI),
 -	[x86_intercept_clgi]		= POST_EX(SVM_EXIT_CLGI),
 -	[x86_intercept_skinit]		= POST_EX(SVM_EXIT_SKINIT),
 -	[x86_intercept_invlpga]		= POST_EX(SVM_EXIT_INVLPGA),
 -	[x86_intercept_rdtscp]		= POST_EX(SVM_EXIT_RDTSCP),
 -	[x86_intercept_monitor]		= POST_MEM(SVM_EXIT_MONITOR),
 -	[x86_intercept_mwait]		= POST_EX(SVM_EXIT_MWAIT),
 -	[x86_intercept_invlpg]		= POST_EX(SVM_EXIT_INVLPG),
 -	[x86_intercept_invd]		= POST_EX(SVM_EXIT_INVD),
 -	[x86_intercept_wbinvd]		= POST_EX(SVM_EXIT_WBINVD),
 -	[x86_intercept_wrmsr]		= POST_EX(SVM_EXIT_MSR),
 -	[x86_intercept_rdtsc]		= POST_EX(SVM_EXIT_RDTSC),
 -	[x86_intercept_rdmsr]		= POST_EX(SVM_EXIT_MSR),
 -	[x86_intercept_rdpmc]		= POST_EX(SVM_EXIT_RDPMC),
 -	[x86_intercept_cpuid]		= PRE_EX(SVM_EXIT_CPUID),
 -	[x86_intercept_rsm]		= PRE_EX(SVM_EXIT_RSM),
 -	[x86_intercept_pause]		= PRE_EX(SVM_EXIT_PAUSE),
 -	[x86_intercept_pushf]		= PRE_EX(SVM_EXIT_PUSHF),
 -	[x86_intercept_popf]		= PRE_EX(SVM_EXIT_POPF),
 -	[x86_intercept_intn]		= PRE_EX(SVM_EXIT_SWINT),
 -	[x86_intercept_iret]		= PRE_EX(SVM_EXIT_IRET),
 -	[x86_intercept_icebp]		= PRE_EX(SVM_EXIT_ICEBP),
 -	[x86_intercept_hlt]		= POST_EX(SVM_EXIT_HLT),
 -	[x86_intercept_in]		= POST_EX(SVM_EXIT_IOIO),
 -	[x86_intercept_ins]		= POST_EX(SVM_EXIT_IOIO),
 -	[x86_intercept_out]		= POST_EX(SVM_EXIT_IOIO),
 -	[x86_intercept_outs]		= POST_EX(SVM_EXIT_IOIO),
 -	[x86_intercept_xsetbv]		= PRE_EX(SVM_EXIT_XSETBV),
 -};
 +		/*
 +		 *  If source is kernel buffer then use memcpy() otherwise
 +		 *  copy_from_user().
 +		 */
 +		dst_offset = dst_paddr & 15;
  
 -#undef PRE_EX
 -#undef POST_EX
 -#undef POST_MEM
 +		if (src_tpage)
 +			memcpy(page_address(dst_tpage) + dst_offset,
 +			       page_address(src_tpage), size);
 +		else {
 +			if (copy_from_user(page_address(dst_tpage) + dst_offset,
 +					   (void __user *)(uintptr_t)vaddr, size)) {
 +				ret = -EFAULT;
 +				goto e_free;
 +			}
 +		}
  
 -static int svm_check_intercept(struct kvm_vcpu *vcpu,
 -			       struct x86_instruction_info *info,
 -			       enum x86_intercept_stage stage,
 -			       struct x86_exception *exception)
 -{
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	int vmexit, ret = X86EMUL_CONTINUE;
 -	struct __x86_intercept icpt_info;
 -	struct vmcb *vmcb = svm->vmcb;
 +		paddr = __sme_page_pa(dst_tpage);
 +		dst_paddr = round_down(dst_paddr, 16);
 +		len = round_up(size, 16);
 +	}
  
 -	if (info->intercept >= ARRAY_SIZE(x86_intercept_map))
 -		goto out;
 +	ret = __sev_issue_dbg_cmd(kvm, paddr, dst_paddr, len, error, true);
  
 -	icpt_info = x86_intercept_map[info->intercept];
 +e_free:
 +	if (src_tpage)
 +		__free_page(src_tpage);
 +	if (dst_tpage)
 +		__free_page(dst_tpage);
 +	return ret;
 +}
  
 -	if (stage != icpt_info.stage)
 -		goto out;
 +static int sev_dbg_crypt(struct kvm *kvm, struct kvm_sev_cmd *argp, bool dec)
 +{
 +	unsigned long vaddr, vaddr_end, next_vaddr;
 +	unsigned long dst_vaddr;
 +	struct page **src_p, **dst_p;
 +	struct kvm_sev_dbg debug;
 +	unsigned long n;
 +	unsigned int size;
 +	int ret;
  
 -	switch (icpt_info.exit_code) {
 -	case SVM_EXIT_READ_CR0:
 -		if (info->intercept == x86_intercept_cr_read)
 -			icpt_info.exit_code += info->modrm_reg;
 -		break;
 -	case SVM_EXIT_WRITE_CR0: {
 -		unsigned long cr0, val;
 -		u64 intercept;
 +	if (!sev_guest(kvm))
 +		return -ENOTTY;
  
 -		if (info->intercept == x86_intercept_cr_write)
 -			icpt_info.exit_code += info->modrm_reg;
 +	if (copy_from_user(&debug, (void __user *)(uintptr_t)argp->data, sizeof(debug)))
 +		return -EFAULT;
  
 -		if (icpt_info.exit_code != SVM_EXIT_WRITE_CR0 ||
 -		    info->intercept == x86_intercept_clts)
 -			break;
 +	if (!debug.len || debug.src_uaddr + debug.len < debug.src_uaddr)
 +		return -EINVAL;
 +	if (!debug.dst_uaddr)
 +		return -EINVAL;
  
 -		intercept = svm->nested.intercept;
 +	vaddr = debug.src_uaddr;
 +	size = debug.len;
 +	vaddr_end = vaddr + size;
 +	dst_vaddr = debug.dst_uaddr;
  
 -		if (!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0)))
 -			break;
 +	for (; vaddr < vaddr_end; vaddr = next_vaddr) {
 +		int len, s_off, d_off;
  
 -		cr0 = vcpu->arch.cr0 & ~SVM_CR0_SELECTIVE_MASK;
 -		val = info->src_val  & ~SVM_CR0_SELECTIVE_MASK;
 +		/* lock userspace source and destination page */
 +		src_p = sev_pin_memory(kvm, vaddr & PAGE_MASK, PAGE_SIZE, &n, 0);
 +		if (!src_p)
 +			return -EFAULT;
  
 -		if (info->intercept == x86_intercept_lmsw) {
 -			cr0 &= 0xfUL;
 -			val &= 0xfUL;
 -			/* lmsw can't clear PE - catch this here */
 -			if (cr0 & X86_CR0_PE)
 -				val |= X86_CR0_PE;
 +		dst_p = sev_pin_memory(kvm, dst_vaddr & PAGE_MASK, PAGE_SIZE, &n, 1);
 +		if (!dst_p) {
 +			sev_unpin_memory(kvm, src_p, n);
 +			return -EFAULT;
  		}
  
 -		if (cr0 ^ val)
 -			icpt_info.exit_code = SVM_EXIT_CR0_SEL_WRITE;
 +		/*
 +		 * The DBG_{DE,EN}CRYPT commands will perform {dec,en}cryption of the
 +		 * memory content (i.e it will write the same memory region with C=1).
 +		 * It's possible that the cache may contain the data with C=0, i.e.,
 +		 * unencrypted so invalidate it first.
 +		 */
 +		sev_clflush_pages(src_p, 1);
 +		sev_clflush_pages(dst_p, 1);
  
 -		break;
 -	}
 -	case SVM_EXIT_READ_DR0:
 -	case SVM_EXIT_WRITE_DR0:
 -		icpt_info.exit_code += info->modrm_reg;
 -		break;
 -	case SVM_EXIT_MSR:
 -		if (info->intercept == x86_intercept_wrmsr)
 -			vmcb->control.exit_info_1 = 1;
 -		else
 -			vmcb->control.exit_info_1 = 0;
 -		break;
 -	case SVM_EXIT_PAUSE:
  		/*
 -		 * We get this for NOP only, but pause
 -		 * is rep not, check this here
 +		 * Since user buffer may not be page aligned, calculate the
 +		 * offset within the page.
  		 */
 -		if (info->rep_prefix != REPE_PREFIX)
 -			goto out;
 -		break;
 -	case SVM_EXIT_IOIO: {
 -		u64 exit_info;
 -		u32 bytes;
 +		s_off = vaddr & ~PAGE_MASK;
 +		d_off = dst_vaddr & ~PAGE_MASK;
 +		len = min_t(size_t, (PAGE_SIZE - s_off), size);
 +
 +		if (dec)
 +			ret = __sev_dbg_decrypt_user(kvm,
 +						     __sme_page_pa(src_p[0]) + s_off,
 +						     dst_vaddr,
 +						     __sme_page_pa(dst_p[0]) + d_off,
 +						     len, &argp->error);
 +		else
 +			ret = __sev_dbg_encrypt_user(kvm,
 +						     __sme_page_pa(src_p[0]) + s_off,
 +						     vaddr,
 +						     __sme_page_pa(dst_p[0]) + d_off,
 +						     dst_vaddr,
 +						     len, &argp->error);
  
 -		if (info->intercept == x86_intercept_in ||
 -		    info->intercept == x86_intercept_ins) {
 -			exit_info = ((info->src_val & 0xffff) << 16) |
 -				SVM_IOIO_TYPE_MASK;
 -			bytes = info->dst_bytes;
 -		} else {
 -			exit_info = (info->dst_val & 0xffff) << 16;
 -			bytes = info->src_bytes;
 -		}
 +		sev_unpin_memory(kvm, src_p, n);
 +		sev_unpin_memory(kvm, dst_p, n);
  
 -		if (info->intercept == x86_intercept_outs ||
 -		    info->intercept == x86_intercept_ins)
 -			exit_info |= SVM_IOIO_STR_MASK;
 +		if (ret)
 +			goto err;
  
 -		if (info->rep_prefix)
 -			exit_info |= SVM_IOIO_REP_MASK;
 +		next_vaddr = vaddr + len;
 +		dst_vaddr = dst_vaddr + len;
 +		size -= len;
 +	}
 +err:
 +	return ret;
 +}
  
 -		bytes = min(bytes, 4u);
 +static int sev_launch_secret(struct kvm *kvm, struct kvm_sev_cmd *argp)
 +{
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct sev_data_launch_secret *data;
 +	struct kvm_sev_launch_secret params;
 +	struct page **pages;
 +	void *blob, *hdr;
 +	unsigned long n;
 +	int ret, offset;
  
 -		exit_info |= bytes << SVM_IOIO_SIZE_SHIFT;
 +	if (!sev_guest(kvm))
 +		return -ENOTTY;
  
 -		exit_info |= (u32)info->ad_bytes << (SVM_IOIO_ASIZE_SHIFT - 1);
 +	if (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))
 +		return -EFAULT;
  
 -		vmcb->control.exit_info_1 = exit_info;
 -		vmcb->control.exit_info_2 = info->next_rip;
 +	pages = sev_pin_memory(kvm, params.guest_uaddr, params.guest_len, &n, 1);
 +	if (!pages)
 +		return -ENOMEM;
  
 -		break;
 +	/*
 +	 * The secret must be copied into contiguous memory region, lets verify
 +	 * that userspace memory pages are contiguous before we issue command.
 +	 */
 +	if (get_num_contig_pages(0, pages, n) != n) {
 +		ret = -EINVAL;
 +		goto e_unpin_memory;
  	}
 -	default:
 -		break;
 +
 +	ret = -ENOMEM;
 +	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
 +	if (!data)
 +		goto e_unpin_memory;
 +
 +	offset = params.guest_uaddr & (PAGE_SIZE - 1);
 +	data->guest_address = __sme_page_pa(pages[0]) + offset;
 +	data->guest_len = params.guest_len;
 +
 +	blob = psp_copy_user_blob(params.trans_uaddr, params.trans_len);
 +	if (IS_ERR(blob)) {
 +		ret = PTR_ERR(blob);
 +		goto e_free;
  	}
  
 -	/* TODO: Advertise NRIPS to guest hypervisor unconditionally */
 -	if (static_cpu_has(X86_FEATURE_NRIPS))
 -		vmcb->control.next_rip  = info->next_rip;
 -	vmcb->control.exit_code = icpt_info.exit_code;
 -	vmexit = nested_svm_exit_handled(svm);
 +	data->trans_address = __psp_pa(blob);
 +	data->trans_len = params.trans_len;
  
 -	ret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED
 -					   : X86EMUL_CONTINUE;
 +	hdr = psp_copy_user_blob(params.hdr_uaddr, params.hdr_len);
 +	if (IS_ERR(hdr)) {
 +		ret = PTR_ERR(hdr);
 +		goto e_free_blob;
 +	}
 +	data->hdr_address = __psp_pa(hdr);
 +	data->hdr_len = params.hdr_len;
  
 -out:
 +	data->handle = sev->handle;
 +	ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_UPDATE_SECRET, data, &argp->error);
 +
 +	kfree(hdr);
 +
 +e_free_blob:
 +	kfree(blob);
 +e_free:
 +	kfree(data);
 +e_unpin_memory:
 +	sev_unpin_memory(kvm, pages, n);
  	return ret;
  }
  
* Unmerged path arch/x86/kvm/svm/svm.h
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/svm/svm.h
