libperf: Move mmap allocation to perf_evlist__mmap_ops::get

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jiri Olsa <jolsa@kernel.org>
commit 3805e4f303314c2b53fb217dd8549a5b9eb06b11
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/3805e4f3.failed

Move allocation of the mmap array into perf_evlist__mmap_ops::get, to
centralize the mmap allocation.

Also move nr_mmap setup to perf_evlist__mmap_ops so it's centralized and
shared by both perf and libperf mmap code.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Jin Yao <yao.jin@linux.intel.com>
	Cc: Michael Petlan <mpetlan@redhat.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
Link: http://lore.kernel.org/lkml/20191017105918.20873-3-jolsa@kernel.org
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit 3805e4f303314c2b53fb217dd8549a5b9eb06b11)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/lib/evlist.c
#	tools/perf/util/evlist.c
diff --cc tools/perf/util/evlist.c
index 29a998d183ce,5cded4ec5806..000000000000
--- a/tools/perf/util/evlist.c
+++ b/tools/perf/util/evlist.c
@@@ -716,16 -586,20 +716,20 @@@ void perf_evlist__munmap(struct perf_ev
  	zfree(&evlist->overwrite_mmap);
  }
  
 -static void perf_mmap__unmap_cb(struct perf_mmap *map)
 -{
 -	struct mmap *m = container_of(map, struct mmap, core);
 -
 -	mmap__munmap(m);
 -}
 -
 -static struct mmap *evlist__alloc_mmap(struct evlist *evlist,
 -				       bool overwrite)
 +static struct perf_mmap *perf_evlist__alloc_mmap(struct perf_evlist *evlist,
 +						 bool overwrite)
  {
  	int i;
 -	struct mmap *map;
 -
 +	struct perf_mmap *map;
 +
++<<<<<<< HEAD
 +	evlist->nr_mmaps = cpu_map__nr(evlist->cpus);
 +	if (cpu_map__empty(evlist->cpus))
 +		evlist->nr_mmaps = thread_map__nr(evlist->threads);
 +	map = zalloc(evlist->nr_mmaps * sizeof(struct perf_mmap));
++=======
+ 	map = zalloc(evlist->core.nr_mmaps * sizeof(struct mmap));
++>>>>>>> 3805e4f30331 (libperf: Move mmap allocation to perf_evlist__mmap_ops::get)
  	if (!map)
  		return NULL;
  
@@@ -746,112 -620,37 +750,131 @@@
  	return map;
  }
  
 -static void
 -perf_evlist__mmap_cb_idx(struct perf_evlist *_evlist,
 -			 struct perf_mmap_param *_mp,
 -			 int idx, bool per_cpu)
 +static bool
 +perf_evlist__should_poll(struct perf_evlist *evlist __maybe_unused,
 +			 struct perf_evsel *evsel)
  {
 -	struct evlist *evlist = container_of(_evlist, struct evlist, core);
 -	struct mmap_params *mp = container_of(_mp, struct mmap_params, core);
 -
 -	auxtrace_mmap_params__set_idx(&mp->auxtrace_mp, evlist, idx, per_cpu);
 +	if (evsel->attr.write_backward)
 +		return false;
 +	return true;
  }
  
 -static struct perf_mmap*
 -perf_evlist__mmap_cb_get(struct perf_evlist *_evlist, bool overwrite, int idx)
 +static int perf_evlist__mmap_per_evsel(struct perf_evlist *evlist, int idx,
 +				       struct mmap_params *mp, int cpu_idx,
 +				       int thread, int *_output, int *_output_overwrite)
  {
++<<<<<<< HEAD
 +	struct perf_evsel *evsel;
 +	int revent;
 +	int evlist_cpu = cpu_map__cpu(evlist->cpus, cpu_idx);
 +
 +	evlist__for_each_entry(evlist, evsel) {
 +		struct perf_mmap *maps = evlist->mmap;
 +		int *output = _output;
 +		int fd;
 +		int cpu;
 +
 +		mp->prot = PROT_READ | PROT_WRITE;
 +		if (evsel->attr.write_backward) {
 +			output = _output_overwrite;
 +			maps = evlist->overwrite_mmap;
 +
 +			if (!maps) {
 +				maps = perf_evlist__alloc_mmap(evlist, true);
 +				if (!maps)
 +					return -1;
 +				evlist->overwrite_mmap = maps;
 +				if (evlist->bkw_mmap_state == BKW_MMAP_NOTREADY)
 +					perf_evlist__toggle_bkw_mmap(evlist, BKW_MMAP_RUNNING);
 +			}
 +			mp->prot &= ~PROT_WRITE;
 +		}
 +
 +		if (evsel->system_wide && thread)
 +			continue;
 +
 +		cpu = cpu_map__idx(evsel->cpus, evlist_cpu);
 +		if (cpu == -1)
 +			continue;
 +
 +		fd = FD(evsel, cpu, thread);
 +
 +		if (*output == -1) {
 +			*output = fd;
 +
 +			if (perf_mmap__mmap(&maps[idx], mp, *output, evlist_cpu) < 0)
 +				return -1;
 +		} else {
 +			if (ioctl(fd, PERF_EVENT_IOC_SET_OUTPUT, *output) != 0)
 +				return -1;
 +
 +			perf_mmap__get(&maps[idx]);
 +		}
 +
 +		revent = perf_evlist__should_poll(evlist, evsel) ? POLLIN : 0;
 +
 +		/*
 +		 * The system_wide flag causes a selected event to be opened
 +		 * always without a pid.  Consequently it will never get a
 +		 * POLLHUP, but it is used for tracking in combination with
 +		 * other events, so it should not need to be polled anyway.
 +		 * Therefore don't add it for polling.
 +		 */
 +		if (!evsel->system_wide &&
 +		    __perf_evlist__add_pollfd(evlist, fd, &maps[idx], revent) < 0) {
 +			perf_mmap__put(&maps[idx]);
 +			return -1;
 +		}
 +
 +		if (evsel->attr.read_format & PERF_FORMAT_ID) {
 +			if (perf_evlist__id_add_fd(evlist, evsel, cpu, thread,
 +						   fd) < 0)
 +				return -1;
 +			perf_evlist__set_sid_idx(evlist, evsel, idx, cpu,
 +						 thread);
++=======
+ 	struct evlist *evlist = container_of(_evlist, struct evlist, core);
+ 	struct mmap *maps;
+ 
+ 	maps = overwrite ? evlist->overwrite_mmap : evlist->mmap;
+ 
+ 	if (!maps) {
+ 		maps = evlist__alloc_mmap(evlist, overwrite);
+ 		if (!maps)
+ 			return NULL;
+ 
+ 		if (overwrite) {
+ 			evlist->overwrite_mmap = maps;
+ 			if (evlist->bkw_mmap_state == BKW_MMAP_NOTREADY)
+ 				perf_evlist__toggle_bkw_mmap(evlist, BKW_MMAP_RUNNING);
+ 		} else {
+ 			evlist->mmap = maps;
++>>>>>>> 3805e4f30331 (libperf: Move mmap allocation to perf_evlist__mmap_ops::get)
 +		}
 +	}
 +
 +	return 0;
 +}
 +
 +static int perf_evlist__mmap_per_cpu(struct perf_evlist *evlist,
 +				     struct mmap_params *mp)
 +{
 +	int cpu, thread;
 +	int nr_cpus = cpu_map__nr(evlist->cpus);
 +	int nr_threads = thread_map__nr(evlist->threads);
 +
 +	pr_debug2("perf event ring buffer mmapped per cpu\n");
 +	for (cpu = 0; cpu < nr_cpus; cpu++) {
 +		int output = -1;
 +		int output_overwrite = -1;
 +
 +		auxtrace_mmap_params__set_idx(&mp->auxtrace_mp, evlist, cpu,
 +					      true);
 +
 +		for (thread = 0; thread < nr_threads; thread++) {
 +			if (perf_evlist__mmap_per_evsel(evlist, cpu, mp, cpu,
 +							thread, &output, &output_overwrite))
 +				goto out_unmap;
  		}
  	}
  
@@@ -1023,47 -799,38 +1046,53 @@@ int perf_evlist__mmap_ex(struct perf_ev
  	 * Its value is decided by evsel's write_backward.
  	 * So &mp should not be passed through const pointer.
  	 */
 -	struct mmap_params mp = {
 -		.nr_cblocks	= nr_cblocks,
 -		.affinity	= affinity,
 -		.flush		= flush,
 -		.comp_level	= comp_level
 -	};
 -	struct perf_evlist_mmap_ops ops = {
 -		.idx  = perf_evlist__mmap_cb_idx,
 -		.get  = perf_evlist__mmap_cb_get,
 -		.mmap = perf_evlist__mmap_cb_mmap,
 -	};
 +	struct mmap_params mp = { .nr_cblocks = nr_cblocks, .affinity = affinity, .flush = flush,
 +				  .comp_level = comp_level };
 +
++<<<<<<< HEAD
 +	if (!evlist->mmap)
 +		evlist->mmap = perf_evlist__alloc_mmap(evlist, false);
 +	if (!evlist->mmap)
 +		return -ENOMEM;
  
 +	if (evlist->pollfd.entries == NULL && perf_evlist__alloc_pollfd(evlist) < 0)
 +		return -ENOMEM;
++=======
+ 	evlist->core.mmap_len = evlist__mmap_size(pages);
+ 	pr_debug("mmap size %zuB\n", evlist->core.mmap_len);
+ 	mp.core.mask = evlist->core.mmap_len - page_size - 1;
++>>>>>>> 3805e4f30331 (libperf: Move mmap allocation to perf_evlist__mmap_ops::get)
 +
 +	evlist->mmap_len = perf_evlist__mmap_size(pages);
 +	pr_debug("mmap size %zuB\n", evlist->mmap_len);
 +	mp.mask = evlist->mmap_len - page_size - 1;
  
 -	auxtrace_mmap_params__init(&mp.auxtrace_mp, evlist->core.mmap_len,
 +	auxtrace_mmap_params__init(&mp.auxtrace_mp, evlist->mmap_len,
  				   auxtrace_pages, auxtrace_overwrite);
  
 -	return perf_evlist__mmap_ops(&evlist->core, &ops, &mp.core);
 +	evlist__for_each_entry(evlist, evsel) {
 +		if ((evsel->attr.read_format & PERF_FORMAT_ID) &&
 +		    evsel->sample_id == NULL &&
 +		    perf_evsel__alloc_id(evsel, cpu_map__nr(cpus), threads->nr) < 0)
 +			return -ENOMEM;
 +	}
 +
 +	if (cpu_map__empty(cpus))
 +		return perf_evlist__mmap_per_thread(evlist, &mp);
 +
 +	return perf_evlist__mmap_per_cpu(evlist, &mp);
  }
  
 -int evlist__mmap(struct evlist *evlist, unsigned int pages)
 +int perf_evlist__mmap(struct perf_evlist *evlist, unsigned int pages)
  {
 -	return evlist__mmap_ex(evlist, pages, 0, false, 0, PERF_AFFINITY_SYS, 1, 0);
 +	return perf_evlist__mmap_ex(evlist, pages, 0, false, 0, PERF_AFFINITY_SYS, 1, 0);
  }
  
 -int perf_evlist__create_maps(struct evlist *evlist, struct target *target)
 +int perf_evlist__create_maps(struct perf_evlist *evlist, struct target *target)
  {
  	bool all_threads = (target->per_thread && target->system_wide);
 -	struct perf_cpu_map *cpus;
 -	struct perf_thread_map *threads;
 +	struct cpu_map *cpus;
 +	struct thread_map *threads;
  
  	/*
  	 * If specify '-a' and '--per-thread' to perf record, perf record
* Unmerged path tools/perf/lib/evlist.c
* Unmerged path tools/perf/lib/evlist.c
* Unmerged path tools/perf/util/evlist.c
