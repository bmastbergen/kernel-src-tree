KVM: SVM: Move SEV code to separate file

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Joerg Roedel <jroedel@suse.de>
commit eaf78265a4ab33935d3a0f1407ce4a91aac4d4d5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/eaf78265.failed

Move the SEV specific parts of svm.c into the new sev.c file.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
Message-Id: <20200324094154.32352-5-joro@8bytes.org>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit eaf78265a4ab33935d3a0f1407ce4a91aac4d4d5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/Makefile
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/svm/svm.h
diff --cc arch/x86/kvm/Makefile
index 0ef050982c37,e5a71aa0967b..000000000000
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@@ -14,7 -14,7 +14,11 @@@ kvm-y			+= x86.o emulate.o i8259.o irq.
  			   hyperv.o debugfs.o mmu/mmu.o mmu/page_track.o
  
  kvm-intel-y		+= vmx/vmx.o vmx/vmenter.o vmx/pmu_intel.o vmx/vmcs12.o vmx/evmcs.o vmx/nested.o
++<<<<<<< HEAD
 +kvm-amd-y		+= svm/svm.o svm/pmu.o
++=======
+ kvm-amd-y		+= svm/svm.o svm/pmu.o svm/nested.o svm/avic.o svm/sev.o
++>>>>>>> eaf78265a4ab (KVM: SVM: Move SEV code to separate file)
  
  obj-$(CONFIG_KVM)	+= kvm.o
  obj-$(CONFIG_KVM_INTEL)	+= kvm-intel.o
diff --cc arch/x86/kvm/svm/svm.c
index dd22d2cd361d,05d77b395fc1..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -390,303 -194,8 +390,306 @@@ module_param(dump_invalid_vmcb, bool, 0
  
  static u8 rsm_ins_bytes[] = "\x0f\xaa";
  
 +static void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 +static void svm_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa);
  static void svm_complete_interrupts(struct vcpu_svm *svm);
 +static inline void avic_post_state_restore(struct kvm_vcpu *vcpu);
 +
 +static int nested_svm_exit_handled(struct vcpu_svm *svm);
 +static int nested_svm_intercept(struct vcpu_svm *svm);
 +static int nested_svm_vmexit(struct vcpu_svm *svm);
 +static int nested_svm_check_exception(struct vcpu_svm *svm, unsigned nr,
 +				      bool has_error_code, u32 error_code);
 +
 +enum {
 +	VMCB_INTERCEPTS, /* Intercept vectors, TSC offset,
 +			    pause filter count */
 +	VMCB_PERM_MAP,   /* IOPM Base and MSRPM Base */
 +	VMCB_ASID,	 /* ASID */
 +	VMCB_INTR,	 /* int_ctl, int_vector */
 +	VMCB_NPT,        /* npt_en, nCR3, gPAT */
 +	VMCB_CR,	 /* CR0, CR3, CR4, EFER */
 +	VMCB_DR,         /* DR6, DR7 */
 +	VMCB_DT,         /* GDT, IDT */
 +	VMCB_SEG,        /* CS, DS, SS, ES, CPL */
 +	VMCB_CR2,        /* CR2 only */
 +	VMCB_LBR,        /* DBGCTL, BR_FROM, BR_TO, LAST_EX_FROM, LAST_EX_TO */
 +	VMCB_AVIC,       /* AVIC APIC_BAR, AVIC APIC_BACKING_PAGE,
 +			  * AVIC PHYSICAL_TABLE pointer,
 +			  * AVIC LOGICAL_TABLE pointer
 +			  */
 +	VMCB_DIRTY_MAX,
 +};
 +
 +/* TPR and CR2 are always written before VMRUN */
 +#define VMCB_ALWAYS_DIRTY_MASK	((1U << VMCB_INTR) | (1U << VMCB_CR2))
 +
 +#define VMCB_AVIC_APIC_BAR_MASK		0xFFFFFFFFFF000ULL
 +
++<<<<<<< HEAD
 +static int sev_flush_asids(void);
 +static DECLARE_RWSEM(sev_deactivate_lock);
 +static DEFINE_MUTEX(sev_bitmap_lock);
 +static unsigned int max_sev_asid;
 +static unsigned int min_sev_asid;
 +static unsigned long *sev_asid_bitmap;
 +static unsigned long *sev_reclaim_asid_bitmap;
 +#define __sme_page_pa(x) __sme_set(page_to_pfn(x) << PAGE_SHIFT)
 +
 +struct enc_region {
 +	struct list_head list;
 +	unsigned long npages;
 +	struct page **pages;
 +	unsigned long uaddr;
 +	unsigned long size;
 +};
 +
 +
 +static inline struct kvm_svm *to_kvm_svm(struct kvm *kvm)
 +{
 +	return container_of(kvm, struct kvm_svm, kvm);
 +}
 +
 +static inline bool svm_sev_enabled(void)
 +{
 +	return IS_ENABLED(CONFIG_KVM_AMD_SEV) ? max_sev_asid : 0;
 +}
 +
 +static inline bool sev_guest(struct kvm *kvm)
 +{
 +#ifdef CONFIG_KVM_AMD_SEV
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +
 +	return sev->active;
 +#else
 +	return false;
 +#endif
 +}
 +
 +static inline int sev_get_asid(struct kvm *kvm)
 +{
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +
 +	return sev->asid;
 +}
 +
 +static inline void mark_all_dirty(struct vmcb *vmcb)
 +{
 +	vmcb->control.clean = 0;
 +}
 +
 +static inline void mark_all_clean(struct vmcb *vmcb)
 +{
 +	vmcb->control.clean = ((1 << VMCB_DIRTY_MAX) - 1)
 +			       & ~VMCB_ALWAYS_DIRTY_MASK;
 +}
 +
 +static inline void mark_dirty(struct vmcb *vmcb, int bit)
 +{
 +	vmcb->control.clean &= ~(1 << bit);
 +}
 +
 +static inline struct vcpu_svm *to_svm(struct kvm_vcpu *vcpu)
 +{
 +	return container_of(vcpu, struct vcpu_svm, vcpu);
 +}
 +
 +static inline void avic_update_vapic_bar(struct vcpu_svm *svm, u64 data)
 +{
 +	svm->vmcb->control.avic_vapic_bar = data & VMCB_AVIC_APIC_BAR_MASK;
 +	mark_dirty(svm->vmcb, VMCB_AVIC);
 +}
 +
 +static inline bool avic_vcpu_is_running(struct kvm_vcpu *vcpu)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u64 *entry = svm->avic_physical_id_cache;
 +
 +	if (!entry)
 +		return false;
 +
 +	return (READ_ONCE(*entry) & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
 +}
 +
 +static void recalc_intercepts(struct vcpu_svm *svm)
 +{
 +	struct vmcb_control_area *c, *h;
 +	struct nested_state *g;
 +
 +	mark_dirty(svm->vmcb, VMCB_INTERCEPTS);
 +
 +	if (!is_guest_mode(&svm->vcpu))
 +		return;
 +
 +	c = &svm->vmcb->control;
 +	h = &svm->nested.hsave->control;
 +	g = &svm->nested;
 +
 +	c->intercept_cr = h->intercept_cr;
 +	c->intercept_dr = h->intercept_dr;
 +	c->intercept_exceptions = h->intercept_exceptions;
 +	c->intercept = h->intercept;
 +
 +	if (svm->vcpu.arch.hflags & HF_VINTR_MASK) {
 +		/* We only want the cr8 intercept bits of L1 */
 +		c->intercept_cr &= ~(1U << INTERCEPT_CR8_READ);
 +		c->intercept_cr &= ~(1U << INTERCEPT_CR8_WRITE);
 +
 +		/*
 +		 * Once running L2 with HF_VINTR_MASK, EFLAGS.IF does not
 +		 * affect any interrupt we may want to inject; therefore,
 +		 * interrupt window vmexits are irrelevant to L0.
 +		 */
 +		c->intercept &= ~(1ULL << INTERCEPT_VINTR);
 +	}
 +
 +	/* We don't want to see VMMCALLs from a nested guest */
 +	c->intercept &= ~(1ULL << INTERCEPT_VMMCALL);
 +
 +	c->intercept_cr |= g->intercept_cr;
 +	c->intercept_dr |= g->intercept_dr;
 +	c->intercept_exceptions |= g->intercept_exceptions;
 +	c->intercept |= g->intercept;
 +}
 +
 +static inline struct vmcb *get_host_vmcb(struct vcpu_svm *svm)
 +{
 +	if (is_guest_mode(&svm->vcpu))
 +		return svm->nested.hsave;
 +	else
 +		return svm->vmcb;
 +}
 +
 +static inline void set_cr_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept_cr |= (1U << bit);
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline void clr_cr_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept_cr &= ~(1U << bit);
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline bool is_cr_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	return vmcb->control.intercept_cr & (1U << bit);
 +}
 +
 +static inline void set_dr_intercepts(struct vcpu_svm *svm)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept_dr = (1 << INTERCEPT_DR0_READ)
 +		| (1 << INTERCEPT_DR1_READ)
 +		| (1 << INTERCEPT_DR2_READ)
 +		| (1 << INTERCEPT_DR3_READ)
 +		| (1 << INTERCEPT_DR4_READ)
 +		| (1 << INTERCEPT_DR5_READ)
 +		| (1 << INTERCEPT_DR6_READ)
 +		| (1 << INTERCEPT_DR7_READ)
 +		| (1 << INTERCEPT_DR0_WRITE)
 +		| (1 << INTERCEPT_DR1_WRITE)
 +		| (1 << INTERCEPT_DR2_WRITE)
 +		| (1 << INTERCEPT_DR3_WRITE)
 +		| (1 << INTERCEPT_DR4_WRITE)
 +		| (1 << INTERCEPT_DR5_WRITE)
 +		| (1 << INTERCEPT_DR6_WRITE)
 +		| (1 << INTERCEPT_DR7_WRITE);
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline void clr_dr_intercepts(struct vcpu_svm *svm)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept_dr = 0;
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline void set_exception_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept_exceptions |= (1U << bit);
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline void clr_exception_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept_exceptions &= ~(1U << bit);
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline void set_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept |= (1ULL << bit);
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline void clr_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	struct vmcb *vmcb = get_host_vmcb(svm);
 +
 +	vmcb->control.intercept &= ~(1ULL << bit);
 +
 +	recalc_intercepts(svm);
 +}
 +
 +static inline bool is_intercept(struct vcpu_svm *svm, int bit)
 +{
 +	return (svm->vmcb->control.intercept & (1ULL << bit)) != 0;
 +}
 +
 +static inline bool vgif_enabled(struct vcpu_svm *svm)
 +{
 +	return !!(svm->vmcb->control.int_ctl & V_GIF_ENABLE_MASK);
 +}
 +
 +static inline void enable_gif(struct vcpu_svm *svm)
 +{
 +	if (vgif_enabled(svm))
 +		svm->vmcb->control.int_ctl |= V_GIF_MASK;
 +	else
 +		svm->vcpu.arch.hflags |= HF_GIF_MASK;
 +}
 +
 +static inline void disable_gif(struct vcpu_svm *svm)
 +{
 +	if (vgif_enabled(svm))
 +		svm->vmcb->control.int_ctl &= ~V_GIF_MASK;
 +	else
 +		svm->vcpu.arch.hflags &= ~HF_GIF_MASK;
 +}
 +
 +static inline bool gif_set(struct vcpu_svm *svm)
 +{
 +	if (vgif_enabled(svm))
 +		return !!(svm->vmcb->control.int_ctl & V_GIF_MASK);
 +	else
 +		return !!(svm->vcpu.arch.hflags & HF_GIF_MASK);
 +}
  
++=======
++>>>>>>> eaf78265a4ab (KVM: SVM: Move SEV code to separate file)
  static unsigned long iopm_base;
  
  struct kvm_ldttss_desc {
@@@ -1213,96 -706,6 +1200,99 @@@ static void disable_nmi_singlestep(stru
  	}
  }
  
++<<<<<<< HEAD
 +/* Note:
 + * This hash table is used to map VM_ID to a struct kvm_svm,
 + * when handling AMD IOMMU GALOG notification to schedule in
 + * a particular vCPU.
 + */
 +#define SVM_VM_DATA_HASH_BITS	8
 +static DEFINE_HASHTABLE(svm_vm_data_hash, SVM_VM_DATA_HASH_BITS);
 +static u32 next_vm_id = 0;
 +static bool next_vm_id_wrapped = 0;
 +static DEFINE_SPINLOCK(svm_vm_data_hash_lock);
 +
 +/* Note:
 + * This function is called from IOMMU driver to notify
 + * SVM to schedule in a particular vCPU of a particular VM.
 + */
 +static int avic_ga_log_notifier(u32 ga_tag)
 +{
 +	unsigned long flags;
 +	struct kvm_svm *kvm_svm;
 +	struct kvm_vcpu *vcpu = NULL;
 +	u32 vm_id = AVIC_GATAG_TO_VMID(ga_tag);
 +	u32 vcpu_id = AVIC_GATAG_TO_VCPUID(ga_tag);
 +
 +	pr_debug("SVM: %s: vm_id=%#x, vcpu_id=%#x\n", __func__, vm_id, vcpu_id);
 +
 +	spin_lock_irqsave(&svm_vm_data_hash_lock, flags);
 +	hash_for_each_possible(svm_vm_data_hash, kvm_svm, hnode, vm_id) {
 +		if (kvm_svm->avic_vm_id != vm_id)
 +			continue;
 +		vcpu = kvm_get_vcpu_by_id(&kvm_svm->kvm, vcpu_id);
 +		break;
 +	}
 +	spin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);
 +
 +	/* Note:
 +	 * At this point, the IOMMU should have already set the pending
 +	 * bit in the vAPIC backing page. So, we just need to schedule
 +	 * in the vcpu.
 +	 */
 +	if (vcpu)
 +		kvm_vcpu_wake_up(vcpu);
 +
 +	return 0;
 +}
 +
 +static __init int sev_hardware_setup(void)
 +{
 +	struct sev_user_data_status *status;
 +	int rc;
 +
 +	/* Maximum number of encrypted guests supported simultaneously */
 +	max_sev_asid = cpuid_ecx(0x8000001F);
 +
 +	if (!max_sev_asid)
 +		return 1;
 +
 +	/* Minimum ASID value that should be used for SEV guest */
 +	min_sev_asid = cpuid_edx(0x8000001F);
 +
 +	/* Initialize SEV ASID bitmaps */
 +	sev_asid_bitmap = bitmap_zalloc(max_sev_asid, GFP_KERNEL);
 +	if (!sev_asid_bitmap)
 +		return 1;
 +
 +	sev_reclaim_asid_bitmap = bitmap_zalloc(max_sev_asid, GFP_KERNEL);
 +	if (!sev_reclaim_asid_bitmap)
 +		return 1;
 +
 +	status = kmalloc(sizeof(*status), GFP_KERNEL);
 +	if (!status)
 +		return 1;
 +
 +	/*
 +	 * Check SEV platform status.
 +	 *
 +	 * PLATFORM_STATUS can be called in any state, if we failed to query
 +	 * the PLATFORM status then either PSP firmware does not support SEV
 +	 * feature or SEV firmware is dead.
 +	 */
 +	rc = sev_platform_status(status, NULL);
 +	if (rc)
 +		goto err;
 +
 +	pr_info("SEV supported\n");
 +
 +err:
 +	kfree(status);
 +	return rc;
 +}
 +
++=======
++>>>>>>> eaf78265a4ab (KVM: SVM: Move SEV code to separate file)
  static void grow_ple_window(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
@@@ -1739,464 -1144,18 +1725,467 @@@ static void init_vmcb(struct vcpu_svm *
  
  }
  
 -static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
++<<<<<<< HEAD
 +static u64 *avic_get_physical_id_entry(struct kvm_vcpu *vcpu,
 +				       unsigned int index)
  {
 -	struct vcpu_svm *svm = to_svm(vcpu);
 -	u32 dummy;
 -	u32 eax = 1;
 +	u64 *avic_physical_id_table;
 +	struct kvm_svm *kvm_svm = to_kvm_svm(vcpu->kvm);
  
 -	svm->spec_ctrl = 0;
 -	svm->virt_spec_ctrl = 0;
 +	if (index >= AVIC_MAX_PHYSICAL_ID_COUNT)
 +		return NULL;
  
 -	if (!init_event) {
 -		svm->vcpu.arch.apic_base = APIC_DEFAULT_PHYS_BASE |
 -					   MSR_IA32_APICBASE_ENABLE;
 +	avic_physical_id_table = page_address(kvm_svm->avic_physical_id_table_page);
 +
 +	return &avic_physical_id_table[index];
 +}
 +
 +/**
 + * Note:
 + * AVIC hardware walks the nested page table to check permissions,
 + * but does not use the SPA address specified in the leaf page
 + * table entry since it uses  address in the AVIC_BACKING_PAGE pointer
 + * field of the VMCB. Therefore, we set up the
 + * APIC_ACCESS_PAGE_PRIVATE_MEMSLOT (4KB) here.
 + */
 +static int avic_update_access_page(struct kvm *kvm, bool activate)
 +{
 +	int ret = 0;
 +
 +	mutex_lock(&kvm->slots_lock);
 +	if (kvm->arch.apic_access_page_done == activate)
 +		goto out;
 +
 +	ret = __x86_set_memory_region(kvm,
 +				      APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
 +				      APIC_DEFAULT_PHYS_BASE,
 +				      activate ? PAGE_SIZE : 0);
 +	if (ret)
 +		goto out;
 +
 +	kvm->arch.apic_access_page_done = activate;
 +out:
 +	mutex_unlock(&kvm->slots_lock);
 +	return ret;
 +}
 +
 +static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 +{
 +	u64 *entry, new_entry;
 +	int id = vcpu->vcpu_id;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (id >= AVIC_MAX_PHYSICAL_ID_COUNT)
 +		return -EINVAL;
 +
 +	if (!svm->vcpu.arch.apic->regs)
 +		return -EINVAL;
 +
 +	if (kvm_apicv_activated(vcpu->kvm)) {
 +		int ret;
 +
 +		ret = avic_update_access_page(vcpu->kvm, true);
 +		if (ret)
 +			return ret;
 +	}
 +
 +	svm->avic_backing_page = virt_to_page(svm->vcpu.arch.apic->regs);
 +
 +	/* Setting AVIC backing page address in the phy APIC ID table */
 +	entry = avic_get_physical_id_entry(vcpu, id);
 +	if (!entry)
 +		return -EINVAL;
 +
 +	new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
 +			      AVIC_PHYSICAL_ID_ENTRY_BACKING_PAGE_MASK) |
 +			      AVIC_PHYSICAL_ID_ENTRY_VALID_MASK);
 +	WRITE_ONCE(*entry, new_entry);
 +
 +	svm->avic_physical_id_cache = entry;
 +
 +	return 0;
 +}
 +
 +static void sev_asid_free(int asid)
 +{
 +	struct svm_cpu_data *sd;
 +	int cpu, pos;
 +
 +	mutex_lock(&sev_bitmap_lock);
 +
 +	pos = asid - 1;
 +	__set_bit(pos, sev_reclaim_asid_bitmap);
 +
 +	for_each_possible_cpu(cpu) {
 +		sd = per_cpu(svm_data, cpu);
 +		sd->sev_vmcbs[pos] = NULL;
 +	}
 +
 +	mutex_unlock(&sev_bitmap_lock);
 +}
 +
 +static void sev_unbind_asid(struct kvm *kvm, unsigned int handle)
 +{
 +	struct sev_data_decommission *decommission;
 +	struct sev_data_deactivate *data;
 +
 +	if (!handle)
 +		return;
 +
 +	data = kzalloc(sizeof(*data), GFP_KERNEL);
 +	if (!data)
 +		return;
 +
 +	/* deactivate handle */
 +	data->handle = handle;
 +
 +	/* Guard DEACTIVATE against WBINVD/DF_FLUSH used in ASID recycling */
 +	down_read(&sev_deactivate_lock);
 +	sev_guest_deactivate(data, NULL);
 +	up_read(&sev_deactivate_lock);
 +
 +	kfree(data);
 +
 +	decommission = kzalloc(sizeof(*decommission), GFP_KERNEL);
 +	if (!decommission)
 +		return;
 +
 +	/* decommission handle */
 +	decommission->handle = handle;
 +	sev_guest_decommission(decommission, NULL);
 +
 +	kfree(decommission);
 +}
 +
 +static struct page **sev_pin_memory(struct kvm *kvm, unsigned long uaddr,
 +				    unsigned long ulen, unsigned long *n,
 +				    int write)
 +{
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	unsigned long npages, npinned, size;
 +	unsigned long locked, lock_limit;
 +	struct page **pages;
 +	unsigned long first, last;
 +
 +	if (ulen == 0 || uaddr + ulen < uaddr)
 +		return NULL;
 +
 +	/* Calculate number of pages. */
 +	first = (uaddr & PAGE_MASK) >> PAGE_SHIFT;
 +	last = ((uaddr + ulen - 1) & PAGE_MASK) >> PAGE_SHIFT;
 +	npages = (last - first + 1);
 +
 +	locked = sev->pages_locked + npages;
 +	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 +	if (locked > lock_limit && !capable(CAP_IPC_LOCK)) {
 +		pr_err("SEV: %lu locked pages exceed the lock limit of %lu.\n", locked, lock_limit);
 +		return NULL;
 +	}
 +
 +	/* Avoid using vmalloc for smaller buffers. */
 +	size = npages * sizeof(struct page *);
 +	if (size > PAGE_SIZE)
 +		pages = __vmalloc(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO,
 +				  PAGE_KERNEL);
 +	else
 +		pages = kmalloc(size, GFP_KERNEL_ACCOUNT);
 +
 +	if (!pages)
 +		return NULL;
 +
 +	/* Pin the user virtual address. */
 +	npinned = get_user_pages_fast(uaddr, npages, FOLL_WRITE, pages);
 +	if (npinned != npages) {
 +		pr_err("SEV: Failure locking %lu pages.\n", npages);
 +		goto err;
 +	}
 +
 +	*n = npages;
 +	sev->pages_locked = locked;
 +
 +	return pages;
 +
 +err:
 +	if (npinned > 0)
 +		release_pages(pages, npinned);
 +
 +	kvfree(pages);
 +	return NULL;
 +}
 +
 +static void sev_unpin_memory(struct kvm *kvm, struct page **pages,
 +			     unsigned long npages)
 +{
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +
 +	release_pages(pages, npages);
 +	kvfree(pages);
 +	sev->pages_locked -= npages;
 +}
 +
 +static void sev_clflush_pages(struct page *pages[], unsigned long npages)
 +{
 +	uint8_t *page_virtual;
 +	unsigned long i;
 +
 +	if (npages == 0 || pages == NULL)
 +		return;
 +
 +	for (i = 0; i < npages; i++) {
 +		page_virtual = kmap_atomic(pages[i]);
 +		clflush_cache_range(page_virtual, PAGE_SIZE);
 +		kunmap_atomic(page_virtual);
 +	}
 +}
 +
 +static void __unregister_enc_region_locked(struct kvm *kvm,
 +					   struct enc_region *region)
 +{
 +	sev_unpin_memory(kvm, region->pages, region->npages);
 +	list_del(&region->list);
 +	kfree(region);
 +}
 +
 +static struct kvm *svm_vm_alloc(void)
 +{
 +	BUILD_BUG_ON(offsetof(struct kvm_svm, kvm) != 0);
 +
 +	return __vmalloc(sizeof(struct kvm_svm),
 +			 GFP_KERNEL_ACCOUNT | __GFP_ZERO, PAGE_KERNEL);
 +}
 +
 +static void svm_vm_free(struct kvm *kvm)
 +{
 +	vfree(kvm);
 +}
 +
 +static void sev_vm_destroy(struct kvm *kvm)
 +{
 +	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
 +	struct list_head *head = &sev->regions_list;
 +	struct list_head *pos, *q;
 +
 +	if (!sev_guest(kvm))
 +		return;
 +
 +	mutex_lock(&kvm->lock);
 +
 +	/*
 +	 * Ensure that all guest tagged cache entries are flushed before
 +	 * releasing the pages back to the system for use. CLFLUSH will
 +	 * not do this, so issue a WBINVD.
 +	 */
 +	wbinvd_on_all_cpus();
 +
 +	/*
 +	 * if userspace was terminated before unregistering the memory regions
 +	 * then lets unpin all the registered memory.
 +	 */
 +	if (!list_empty(head)) {
 +		list_for_each_safe(pos, q, head) {
 +			__unregister_enc_region_locked(kvm,
 +				list_entry(pos, struct enc_region, list));
 +		}
 +	}
 +
 +	mutex_unlock(&kvm->lock);
 +
 +	sev_unbind_asid(kvm, sev->handle);
 +	sev_asid_free(sev->asid);
 +}
 +
 +static void avic_vm_destroy(struct kvm *kvm)
 +{
 +	unsigned long flags;
 +	struct kvm_svm *kvm_svm = to_kvm_svm(kvm);
 +
 +	if (!avic)
 +		return;
 +
 +	if (kvm_svm->avic_logical_id_table_page)
 +		__free_page(kvm_svm->avic_logical_id_table_page);
 +	if (kvm_svm->avic_physical_id_table_page)
 +		__free_page(kvm_svm->avic_physical_id_table_page);
 +
 +	spin_lock_irqsave(&svm_vm_data_hash_lock, flags);
 +	hash_del(&kvm_svm->hnode);
 +	spin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);
 +}
 +
 +static void svm_vm_destroy(struct kvm *kvm)
 +{
 +	avic_vm_destroy(kvm);
 +	sev_vm_destroy(kvm);
 +}
 +
 +static int avic_vm_init(struct kvm *kvm)
 +{
 +	unsigned long flags;
 +	int err = -ENOMEM;
 +	struct kvm_svm *kvm_svm = to_kvm_svm(kvm);
 +	struct kvm_svm *k2;
 +	struct page *p_page;
 +	struct page *l_page;
 +	u32 vm_id;
 +
 +	static bool nested_taint_added = false;
 +
 +	/* RHEL-only: running as a nested hypervisor is TechPreview */
 +	if (!nested_taint_added && boot_cpu_has(X86_FEATURE_HYPERVISOR)) {
 +		mark_tech_preview("Running as a nested hypervisor", THIS_MODULE);
 +		nested_taint_added = true;
 +	}
 +
 +	if (!avic)
 +		return 0;
 +
 +	/* Allocating physical APIC ID table (4KB) */
 +	p_page = alloc_page(GFP_KERNEL_ACCOUNT);
 +	if (!p_page)
 +		goto free_avic;
 +
 +	kvm_svm->avic_physical_id_table_page = p_page;
 +	clear_page(page_address(p_page));
 +
 +	/* Allocating logical APIC ID table (4KB) */
 +	l_page = alloc_page(GFP_KERNEL_ACCOUNT);
 +	if (!l_page)
 +		goto free_avic;
 +
 +	kvm_svm->avic_logical_id_table_page = l_page;
 +	clear_page(page_address(l_page));
 +
 +	spin_lock_irqsave(&svm_vm_data_hash_lock, flags);
 + again:
 +	vm_id = next_vm_id = (next_vm_id + 1) & AVIC_VM_ID_MASK;
 +	if (vm_id == 0) { /* id is 1-based, zero is not okay */
 +		next_vm_id_wrapped = 1;
 +		goto again;
 +	}
 +	/* Is it still in use? Only possible if wrapped at least once */
 +	if (next_vm_id_wrapped) {
 +		hash_for_each_possible(svm_vm_data_hash, k2, hnode, vm_id) {
 +			if (k2->avic_vm_id == vm_id)
 +				goto again;
 +		}
 +	}
 +	kvm_svm->avic_vm_id = vm_id;
 +	hash_add(svm_vm_data_hash, &kvm_svm->hnode, kvm_svm->avic_vm_id);
 +	spin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);
 +
 +	return 0;
 +
 +free_avic:
 +	avic_vm_destroy(kvm);
 +	return err;
 +}
 +
 +static inline int
 +avic_update_iommu_vcpu_affinity(struct kvm_vcpu *vcpu, int cpu, bool r)
 +{
 +	int ret = 0;
 +	unsigned long flags;
 +	struct amd_svm_iommu_ir *ir;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (!kvm_arch_has_assigned_device(vcpu->kvm))
 +		return 0;
 +
 +	/*
 +	 * Here, we go through the per-vcpu ir_list to update all existing
 +	 * interrupt remapping table entry targeting this vcpu.
 +	 */
 +	spin_lock_irqsave(&svm->ir_list_lock, flags);
 +
 +	if (list_empty(&svm->ir_list))
 +		goto out;
 +
 +	list_for_each_entry(ir, &svm->ir_list, node) {
 +		ret = amd_iommu_update_ga(cpu, r, ir->data);
 +		if (ret)
 +			break;
 +	}
 +out:
 +	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 +	return ret;
 +}
 +
 +static void avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 +{
 +	u64 entry;
 +	/* ID = 0xff (broadcast), ID > 0xff (reserved) */
 +	int h_physical_id = kvm_cpu_get_apicid(cpu);
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	/*
 +	 * Since the host physical APIC id is 8 bits,
 +	 * we can support host APIC ID upto 255.
 +	 */
 +	if (WARN_ON(h_physical_id > AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK))
 +		return;
 +
 +	entry = READ_ONCE(*(svm->avic_physical_id_cache));
 +	WARN_ON(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
 +
 +	entry &= ~AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK;
 +	entry |= (h_physical_id & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK);
 +
 +	entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
 +	if (svm->avic_is_running)
 +		entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
 +
 +	WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
 +	avic_update_iommu_vcpu_affinity(vcpu, h_physical_id,
 +					svm->avic_is_running);
 +}
 +
 +static void avic_vcpu_put(struct kvm_vcpu *vcpu)
 +{
 +	u64 entry;
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	if (!kvm_vcpu_apicv_active(vcpu))
 +		return;
 +
 +	entry = READ_ONCE(*(svm->avic_physical_id_cache));
 +	if (entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK)
 +		avic_update_iommu_vcpu_affinity(vcpu, -1, 0);
 +
 +	entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
 +	WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
 +}
 +
 +/**
 + * This function is called during VCPU halt/unhalt.
 + */
 +static void avic_set_running(struct kvm_vcpu *vcpu, bool is_run)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +
 +	svm->avic_is_running = is_run;
 +	if (is_run)
 +		avic_vcpu_load(vcpu, vcpu->cpu);
 +	else
 +		avic_vcpu_put(vcpu);
 +}
 +
++=======
++>>>>>>> eaf78265a4ab (KVM: SVM: Move SEV code to separate file)
 +static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 +{
 +	struct vcpu_svm *svm = to_svm(vcpu);
 +	u32 dummy;
 +	u32 eax = 1;
 +
 +	svm->spec_ctrl = 0;
 +	svm->virt_spec_ctrl = 0;
 +
 +	if (!init_event) {
 +		svm->vcpu.arch.apic_base = APIC_DEFAULT_PHYS_BASE |
 +					   MSR_IA32_APICBASE_ENABLE;
  		if (kvm_vcpu_is_reset_bsp(&svm->vcpu))
  			svm->vcpu.arch.apic_base |= MSR_IA32_APICBASE_BSP;
  	}
@@@ -7417,12 -3966,26 +6488,34 @@@ static bool svm_apic_init_signal_blocke
  		   (svm->vmcb->control.intercept & (1ULL << INTERCEPT_INIT));
  }
  
++<<<<<<< HEAD
 +static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
 +	.cpu_has_kvm_support = has_svm,
 +	.disabled_by_bios = is_disabled,
 +	.hardware_setup = svm_hardware_setup,
++=======
+ static void svm_vm_destroy(struct kvm *kvm)
+ {
+ 	avic_vm_destroy(kvm);
+ 	sev_vm_destroy(kvm);
+ }
+ 
+ static int svm_vm_init(struct kvm *kvm)
+ {
+ 	if (avic) {
+ 		int ret = avic_vm_init(kvm);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	kvm_apicv_init(kvm, avic);
+ 	return 0;
+ }
+ 
+ static struct kvm_x86_ops svm_x86_ops __initdata = {
++>>>>>>> eaf78265a4ab (KVM: SVM: Move SEV code to separate file)
  	.hardware_unsetup = svm_hardware_teardown,
 +	.check_processor_compatibility = svm_check_processor_compat,
  	.hardware_enable = svm_hardware_enable,
  	.hardware_disable = svm_hardware_disable,
  	.cpu_has_accelerated_tpr = svm_cpu_has_accelerated_tpr,
* Unmerged path arch/x86/kvm/svm/svm.h
* Unmerged path arch/x86/kvm/Makefile
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
new file mode 100644
index 000000000000..0e3fc311d7da
--- /dev/null
+++ b/arch/x86/kvm/svm/sev.c
@@ -0,0 +1,1187 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Kernel-based Virtual Machine driver for Linux
+ *
+ * AMD SVM-SEV support
+ *
+ * Copyright 2010 Red Hat, Inc. and/or its affiliates.
+ */
+
+#include <linux/kvm_types.h>
+#include <linux/kvm_host.h>
+#include <linux/kernel.h>
+#include <linux/highmem.h>
+#include <linux/psp-sev.h>
+#include <linux/swap.h>
+
+#include "x86.h"
+#include "svm.h"
+
+static int sev_flush_asids(void);
+static DECLARE_RWSEM(sev_deactivate_lock);
+static DEFINE_MUTEX(sev_bitmap_lock);
+unsigned int max_sev_asid;
+static unsigned int min_sev_asid;
+static unsigned long *sev_asid_bitmap;
+static unsigned long *sev_reclaim_asid_bitmap;
+#define __sme_page_pa(x) __sme_set(page_to_pfn(x) << PAGE_SHIFT)
+
+struct enc_region {
+	struct list_head list;
+	unsigned long npages;
+	struct page **pages;
+	unsigned long uaddr;
+	unsigned long size;
+};
+
+static int sev_flush_asids(void)
+{
+	int ret, error = 0;
+
+	/*
+	 * DEACTIVATE will clear the WBINVD indicator causing DF_FLUSH to fail,
+	 * so it must be guarded.
+	 */
+	down_write(&sev_deactivate_lock);
+
+	wbinvd_on_all_cpus();
+	ret = sev_guest_df_flush(&error);
+
+	up_write(&sev_deactivate_lock);
+
+	if (ret)
+		pr_err("SEV: DF_FLUSH failed, ret=%d, error=%#x\n", ret, error);
+
+	return ret;
+}
+
+/* Must be called with the sev_bitmap_lock held */
+static bool __sev_recycle_asids(void)
+{
+	int pos;
+
+	/* Check if there are any ASIDs to reclaim before performing a flush */
+	pos = find_next_bit(sev_reclaim_asid_bitmap,
+			    max_sev_asid, min_sev_asid - 1);
+	if (pos >= max_sev_asid)
+		return false;
+
+	if (sev_flush_asids())
+		return false;
+
+	bitmap_xor(sev_asid_bitmap, sev_asid_bitmap, sev_reclaim_asid_bitmap,
+		   max_sev_asid);
+	bitmap_zero(sev_reclaim_asid_bitmap, max_sev_asid);
+
+	return true;
+}
+
+static int sev_asid_new(void)
+{
+	bool retry = true;
+	int pos;
+
+	mutex_lock(&sev_bitmap_lock);
+
+	/*
+	 * SEV-enabled guest must use asid from min_sev_asid to max_sev_asid.
+	 */
+again:
+	pos = find_next_zero_bit(sev_asid_bitmap, max_sev_asid, min_sev_asid - 1);
+	if (pos >= max_sev_asid) {
+		if (retry && __sev_recycle_asids()) {
+			retry = false;
+			goto again;
+		}
+		mutex_unlock(&sev_bitmap_lock);
+		return -EBUSY;
+	}
+
+	__set_bit(pos, sev_asid_bitmap);
+
+	mutex_unlock(&sev_bitmap_lock);
+
+	return pos + 1;
+}
+
+static int sev_get_asid(struct kvm *kvm)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+
+	return sev->asid;
+}
+
+static void sev_asid_free(int asid)
+{
+	struct svm_cpu_data *sd;
+	int cpu, pos;
+
+	mutex_lock(&sev_bitmap_lock);
+
+	pos = asid - 1;
+	__set_bit(pos, sev_reclaim_asid_bitmap);
+
+	for_each_possible_cpu(cpu) {
+		sd = per_cpu(svm_data, cpu);
+		sd->sev_vmcbs[pos] = NULL;
+	}
+
+	mutex_unlock(&sev_bitmap_lock);
+}
+
+static void sev_unbind_asid(struct kvm *kvm, unsigned int handle)
+{
+	struct sev_data_decommission *decommission;
+	struct sev_data_deactivate *data;
+
+	if (!handle)
+		return;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return;
+
+	/* deactivate handle */
+	data->handle = handle;
+
+	/* Guard DEACTIVATE against WBINVD/DF_FLUSH used in ASID recycling */
+	down_read(&sev_deactivate_lock);
+	sev_guest_deactivate(data, NULL);
+	up_read(&sev_deactivate_lock);
+
+	kfree(data);
+
+	decommission = kzalloc(sizeof(*decommission), GFP_KERNEL);
+	if (!decommission)
+		return;
+
+	/* decommission handle */
+	decommission->handle = handle;
+	sev_guest_decommission(decommission, NULL);
+
+	kfree(decommission);
+}
+
+static int sev_guest_init(struct kvm *kvm, struct kvm_sev_cmd *argp)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	int asid, ret;
+
+	ret = -EBUSY;
+	if (unlikely(sev->active))
+		return ret;
+
+	asid = sev_asid_new();
+	if (asid < 0)
+		return ret;
+
+	ret = sev_platform_init(&argp->error);
+	if (ret)
+		goto e_free;
+
+	sev->active = true;
+	sev->asid = asid;
+	INIT_LIST_HEAD(&sev->regions_list);
+
+	return 0;
+
+e_free:
+	sev_asid_free(asid);
+	return ret;
+}
+
+static int sev_bind_asid(struct kvm *kvm, unsigned int handle, int *error)
+{
+	struct sev_data_activate *data;
+	int asid = sev_get_asid(kvm);
+	int ret;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
+	if (!data)
+		return -ENOMEM;
+
+	/* activate ASID on the given handle */
+	data->handle = handle;
+	data->asid   = asid;
+	ret = sev_guest_activate(data, error);
+	kfree(data);
+
+	return ret;
+}
+
+static int __sev_issue_cmd(int fd, int id, void *data, int *error)
+{
+	struct fd f;
+	int ret;
+
+	f = fdget(fd);
+	if (!f.file)
+		return -EBADF;
+
+	ret = sev_issue_cmd_external_user(f.file, id, data, error);
+
+	fdput(f);
+	return ret;
+}
+
+static int sev_issue_cmd(struct kvm *kvm, int id, void *data, int *error)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+
+	return __sev_issue_cmd(sev->fd, id, data, error);
+}
+
+static int sev_launch_start(struct kvm *kvm, struct kvm_sev_cmd *argp)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct sev_data_launch_start *start;
+	struct kvm_sev_launch_start params;
+	void *dh_blob, *session_blob;
+	int *error = &argp->error;
+	int ret;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	if (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))
+		return -EFAULT;
+
+	start = kzalloc(sizeof(*start), GFP_KERNEL_ACCOUNT);
+	if (!start)
+		return -ENOMEM;
+
+	dh_blob = NULL;
+	if (params.dh_uaddr) {
+		dh_blob = psp_copy_user_blob(params.dh_uaddr, params.dh_len);
+		if (IS_ERR(dh_blob)) {
+			ret = PTR_ERR(dh_blob);
+			goto e_free;
+		}
+
+		start->dh_cert_address = __sme_set(__pa(dh_blob));
+		start->dh_cert_len = params.dh_len;
+	}
+
+	session_blob = NULL;
+	if (params.session_uaddr) {
+		session_blob = psp_copy_user_blob(params.session_uaddr, params.session_len);
+		if (IS_ERR(session_blob)) {
+			ret = PTR_ERR(session_blob);
+			goto e_free_dh;
+		}
+
+		start->session_address = __sme_set(__pa(session_blob));
+		start->session_len = params.session_len;
+	}
+
+	start->handle = params.handle;
+	start->policy = params.policy;
+
+	/* create memory encryption context */
+	ret = __sev_issue_cmd(argp->sev_fd, SEV_CMD_LAUNCH_START, start, error);
+	if (ret)
+		goto e_free_session;
+
+	/* Bind ASID to this guest */
+	ret = sev_bind_asid(kvm, start->handle, error);
+	if (ret)
+		goto e_free_session;
+
+	/* return handle to userspace */
+	params.handle = start->handle;
+	if (copy_to_user((void __user *)(uintptr_t)argp->data, &params, sizeof(params))) {
+		sev_unbind_asid(kvm, start->handle);
+		ret = -EFAULT;
+		goto e_free_session;
+	}
+
+	sev->handle = start->handle;
+	sev->fd = argp->sev_fd;
+
+e_free_session:
+	kfree(session_blob);
+e_free_dh:
+	kfree(dh_blob);
+e_free:
+	kfree(start);
+	return ret;
+}
+
+static struct page **sev_pin_memory(struct kvm *kvm, unsigned long uaddr,
+				    unsigned long ulen, unsigned long *n,
+				    int write)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	unsigned long npages, npinned, size;
+	unsigned long locked, lock_limit;
+	struct page **pages;
+	unsigned long first, last;
+
+	if (ulen == 0 || uaddr + ulen < uaddr)
+		return NULL;
+
+	/* Calculate number of pages. */
+	first = (uaddr & PAGE_MASK) >> PAGE_SHIFT;
+	last = ((uaddr + ulen - 1) & PAGE_MASK) >> PAGE_SHIFT;
+	npages = (last - first + 1);
+
+	locked = sev->pages_locked + npages;
+	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+	if (locked > lock_limit && !capable(CAP_IPC_LOCK)) {
+		pr_err("SEV: %lu locked pages exceed the lock limit of %lu.\n", locked, lock_limit);
+		return NULL;
+	}
+
+	/* Avoid using vmalloc for smaller buffers. */
+	size = npages * sizeof(struct page *);
+	if (size > PAGE_SIZE)
+		pages = __vmalloc(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO,
+				  PAGE_KERNEL);
+	else
+		pages = kmalloc(size, GFP_KERNEL_ACCOUNT);
+
+	if (!pages)
+		return NULL;
+
+	/* Pin the user virtual address. */
+	npinned = get_user_pages_fast(uaddr, npages, FOLL_WRITE, pages);
+	if (npinned != npages) {
+		pr_err("SEV: Failure locking %lu pages.\n", npages);
+		goto err;
+	}
+
+	*n = npages;
+	sev->pages_locked = locked;
+
+	return pages;
+
+err:
+	if (npinned > 0)
+		release_pages(pages, npinned);
+
+	kvfree(pages);
+	return NULL;
+}
+
+static void sev_unpin_memory(struct kvm *kvm, struct page **pages,
+			     unsigned long npages)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+
+	release_pages(pages, npages);
+	kvfree(pages);
+	sev->pages_locked -= npages;
+}
+
+static void sev_clflush_pages(struct page *pages[], unsigned long npages)
+{
+	uint8_t *page_virtual;
+	unsigned long i;
+
+	if (npages == 0 || pages == NULL)
+		return;
+
+	for (i = 0; i < npages; i++) {
+		page_virtual = kmap_atomic(pages[i]);
+		clflush_cache_range(page_virtual, PAGE_SIZE);
+		kunmap_atomic(page_virtual);
+	}
+}
+
+static unsigned long get_num_contig_pages(unsigned long idx,
+				struct page **inpages, unsigned long npages)
+{
+	unsigned long paddr, next_paddr;
+	unsigned long i = idx + 1, pages = 1;
+
+	/* find the number of contiguous pages starting from idx */
+	paddr = __sme_page_pa(inpages[idx]);
+	while (i < npages) {
+		next_paddr = __sme_page_pa(inpages[i++]);
+		if ((paddr + PAGE_SIZE) == next_paddr) {
+			pages++;
+			paddr = next_paddr;
+			continue;
+		}
+		break;
+	}
+
+	return pages;
+}
+
+static int sev_launch_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)
+{
+	unsigned long vaddr, vaddr_end, next_vaddr, npages, pages, size, i;
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct kvm_sev_launch_update_data params;
+	struct sev_data_launch_update_data *data;
+	struct page **inpages;
+	int ret;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	if (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))
+		return -EFAULT;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
+	if (!data)
+		return -ENOMEM;
+
+	vaddr = params.uaddr;
+	size = params.len;
+	vaddr_end = vaddr + size;
+
+	/* Lock the user memory. */
+	inpages = sev_pin_memory(kvm, vaddr, size, &npages, 1);
+	if (!inpages) {
+		ret = -ENOMEM;
+		goto e_free;
+	}
+
+	/*
+	 * The LAUNCH_UPDATE command will perform in-place encryption of the
+	 * memory content (i.e it will write the same memory region with C=1).
+	 * It's possible that the cache may contain the data with C=0, i.e.,
+	 * unencrypted so invalidate it first.
+	 */
+	sev_clflush_pages(inpages, npages);
+
+	for (i = 0; vaddr < vaddr_end; vaddr = next_vaddr, i += pages) {
+		int offset, len;
+
+		/*
+		 * If the user buffer is not page-aligned, calculate the offset
+		 * within the page.
+		 */
+		offset = vaddr & (PAGE_SIZE - 1);
+
+		/* Calculate the number of pages that can be encrypted in one go. */
+		pages = get_num_contig_pages(i, inpages, npages);
+
+		len = min_t(size_t, ((pages * PAGE_SIZE) - offset), size);
+
+		data->handle = sev->handle;
+		data->len = len;
+		data->address = __sme_page_pa(inpages[i]) + offset;
+		ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_UPDATE_DATA, data, &argp->error);
+		if (ret)
+			goto e_unpin;
+
+		size -= len;
+		next_vaddr = vaddr + len;
+	}
+
+e_unpin:
+	/* content of memory is updated, mark pages dirty */
+	for (i = 0; i < npages; i++) {
+		set_page_dirty_lock(inpages[i]);
+		mark_page_accessed(inpages[i]);
+	}
+	/* unlock the user pages */
+	sev_unpin_memory(kvm, inpages, npages);
+e_free:
+	kfree(data);
+	return ret;
+}
+
+static int sev_launch_measure(struct kvm *kvm, struct kvm_sev_cmd *argp)
+{
+	void __user *measure = (void __user *)(uintptr_t)argp->data;
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct sev_data_launch_measure *data;
+	struct kvm_sev_launch_measure params;
+	void __user *p = NULL;
+	void *blob = NULL;
+	int ret;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	if (copy_from_user(&params, measure, sizeof(params)))
+		return -EFAULT;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
+	if (!data)
+		return -ENOMEM;
+
+	/* User wants to query the blob length */
+	if (!params.len)
+		goto cmd;
+
+	p = (void __user *)(uintptr_t)params.uaddr;
+	if (p) {
+		if (params.len > SEV_FW_BLOB_MAX_SIZE) {
+			ret = -EINVAL;
+			goto e_free;
+		}
+
+		ret = -ENOMEM;
+		blob = kmalloc(params.len, GFP_KERNEL);
+		if (!blob)
+			goto e_free;
+
+		data->address = __psp_pa(blob);
+		data->len = params.len;
+	}
+
+cmd:
+	data->handle = sev->handle;
+	ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_MEASURE, data, &argp->error);
+
+	/*
+	 * If we query the session length, FW responded with expected data.
+	 */
+	if (!params.len)
+		goto done;
+
+	if (ret)
+		goto e_free_blob;
+
+	if (blob) {
+		if (copy_to_user(p, blob, params.len))
+			ret = -EFAULT;
+	}
+
+done:
+	params.len = data->len;
+	if (copy_to_user(measure, &params, sizeof(params)))
+		ret = -EFAULT;
+e_free_blob:
+	kfree(blob);
+e_free:
+	kfree(data);
+	return ret;
+}
+
+static int sev_launch_finish(struct kvm *kvm, struct kvm_sev_cmd *argp)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct sev_data_launch_finish *data;
+	int ret;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
+	if (!data)
+		return -ENOMEM;
+
+	data->handle = sev->handle;
+	ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_FINISH, data, &argp->error);
+
+	kfree(data);
+	return ret;
+}
+
+static int sev_guest_status(struct kvm *kvm, struct kvm_sev_cmd *argp)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct kvm_sev_guest_status params;
+	struct sev_data_guest_status *data;
+	int ret;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
+	if (!data)
+		return -ENOMEM;
+
+	data->handle = sev->handle;
+	ret = sev_issue_cmd(kvm, SEV_CMD_GUEST_STATUS, data, &argp->error);
+	if (ret)
+		goto e_free;
+
+	params.policy = data->policy;
+	params.state = data->state;
+	params.handle = data->handle;
+
+	if (copy_to_user((void __user *)(uintptr_t)argp->data, &params, sizeof(params)))
+		ret = -EFAULT;
+e_free:
+	kfree(data);
+	return ret;
+}
+
+static int __sev_issue_dbg_cmd(struct kvm *kvm, unsigned long src,
+			       unsigned long dst, int size,
+			       int *error, bool enc)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct sev_data_dbg *data;
+	int ret;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
+	if (!data)
+		return -ENOMEM;
+
+	data->handle = sev->handle;
+	data->dst_addr = dst;
+	data->src_addr = src;
+	data->len = size;
+
+	ret = sev_issue_cmd(kvm,
+			    enc ? SEV_CMD_DBG_ENCRYPT : SEV_CMD_DBG_DECRYPT,
+			    data, error);
+	kfree(data);
+	return ret;
+}
+
+static int __sev_dbg_decrypt(struct kvm *kvm, unsigned long src_paddr,
+			     unsigned long dst_paddr, int sz, int *err)
+{
+	int offset;
+
+	/*
+	 * Its safe to read more than we are asked, caller should ensure that
+	 * destination has enough space.
+	 */
+	src_paddr = round_down(src_paddr, 16);
+	offset = src_paddr & 15;
+	sz = round_up(sz + offset, 16);
+
+	return __sev_issue_dbg_cmd(kvm, src_paddr, dst_paddr, sz, err, false);
+}
+
+static int __sev_dbg_decrypt_user(struct kvm *kvm, unsigned long paddr,
+				  unsigned long __user dst_uaddr,
+				  unsigned long dst_paddr,
+				  int size, int *err)
+{
+	struct page *tpage = NULL;
+	int ret, offset;
+
+	/* if inputs are not 16-byte then use intermediate buffer */
+	if (!IS_ALIGNED(dst_paddr, 16) ||
+	    !IS_ALIGNED(paddr,     16) ||
+	    !IS_ALIGNED(size,      16)) {
+		tpage = (void *)alloc_page(GFP_KERNEL);
+		if (!tpage)
+			return -ENOMEM;
+
+		dst_paddr = __sme_page_pa(tpage);
+	}
+
+	ret = __sev_dbg_decrypt(kvm, paddr, dst_paddr, size, err);
+	if (ret)
+		goto e_free;
+
+	if (tpage) {
+		offset = paddr & 15;
+		if (copy_to_user((void __user *)(uintptr_t)dst_uaddr,
+				 page_address(tpage) + offset, size))
+			ret = -EFAULT;
+	}
+
+e_free:
+	if (tpage)
+		__free_page(tpage);
+
+	return ret;
+}
+
+static int __sev_dbg_encrypt_user(struct kvm *kvm, unsigned long paddr,
+				  unsigned long __user vaddr,
+				  unsigned long dst_paddr,
+				  unsigned long __user dst_vaddr,
+				  int size, int *error)
+{
+	struct page *src_tpage = NULL;
+	struct page *dst_tpage = NULL;
+	int ret, len = size;
+
+	/* If source buffer is not aligned then use an intermediate buffer */
+	if (!IS_ALIGNED(vaddr, 16)) {
+		src_tpage = alloc_page(GFP_KERNEL);
+		if (!src_tpage)
+			return -ENOMEM;
+
+		if (copy_from_user(page_address(src_tpage),
+				(void __user *)(uintptr_t)vaddr, size)) {
+			__free_page(src_tpage);
+			return -EFAULT;
+		}
+
+		paddr = __sme_page_pa(src_tpage);
+	}
+
+	/*
+	 *  If destination buffer or length is not aligned then do read-modify-write:
+	 *   - decrypt destination in an intermediate buffer
+	 *   - copy the source buffer in an intermediate buffer
+	 *   - use the intermediate buffer as source buffer
+	 */
+	if (!IS_ALIGNED(dst_vaddr, 16) || !IS_ALIGNED(size, 16)) {
+		int dst_offset;
+
+		dst_tpage = alloc_page(GFP_KERNEL);
+		if (!dst_tpage) {
+			ret = -ENOMEM;
+			goto e_free;
+		}
+
+		ret = __sev_dbg_decrypt(kvm, dst_paddr,
+					__sme_page_pa(dst_tpage), size, error);
+		if (ret)
+			goto e_free;
+
+		/*
+		 *  If source is kernel buffer then use memcpy() otherwise
+		 *  copy_from_user().
+		 */
+		dst_offset = dst_paddr & 15;
+
+		if (src_tpage)
+			memcpy(page_address(dst_tpage) + dst_offset,
+			       page_address(src_tpage), size);
+		else {
+			if (copy_from_user(page_address(dst_tpage) + dst_offset,
+					   (void __user *)(uintptr_t)vaddr, size)) {
+				ret = -EFAULT;
+				goto e_free;
+			}
+		}
+
+		paddr = __sme_page_pa(dst_tpage);
+		dst_paddr = round_down(dst_paddr, 16);
+		len = round_up(size, 16);
+	}
+
+	ret = __sev_issue_dbg_cmd(kvm, paddr, dst_paddr, len, error, true);
+
+e_free:
+	if (src_tpage)
+		__free_page(src_tpage);
+	if (dst_tpage)
+		__free_page(dst_tpage);
+	return ret;
+}
+
+static int sev_dbg_crypt(struct kvm *kvm, struct kvm_sev_cmd *argp, bool dec)
+{
+	unsigned long vaddr, vaddr_end, next_vaddr;
+	unsigned long dst_vaddr;
+	struct page **src_p, **dst_p;
+	struct kvm_sev_dbg debug;
+	unsigned long n;
+	unsigned int size;
+	int ret;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	if (copy_from_user(&debug, (void __user *)(uintptr_t)argp->data, sizeof(debug)))
+		return -EFAULT;
+
+	if (!debug.len || debug.src_uaddr + debug.len < debug.src_uaddr)
+		return -EINVAL;
+	if (!debug.dst_uaddr)
+		return -EINVAL;
+
+	vaddr = debug.src_uaddr;
+	size = debug.len;
+	vaddr_end = vaddr + size;
+	dst_vaddr = debug.dst_uaddr;
+
+	for (; vaddr < vaddr_end; vaddr = next_vaddr) {
+		int len, s_off, d_off;
+
+		/* lock userspace source and destination page */
+		src_p = sev_pin_memory(kvm, vaddr & PAGE_MASK, PAGE_SIZE, &n, 0);
+		if (!src_p)
+			return -EFAULT;
+
+		dst_p = sev_pin_memory(kvm, dst_vaddr & PAGE_MASK, PAGE_SIZE, &n, 1);
+		if (!dst_p) {
+			sev_unpin_memory(kvm, src_p, n);
+			return -EFAULT;
+		}
+
+		/*
+		 * The DBG_{DE,EN}CRYPT commands will perform {dec,en}cryption of the
+		 * memory content (i.e it will write the same memory region with C=1).
+		 * It's possible that the cache may contain the data with C=0, i.e.,
+		 * unencrypted so invalidate it first.
+		 */
+		sev_clflush_pages(src_p, 1);
+		sev_clflush_pages(dst_p, 1);
+
+		/*
+		 * Since user buffer may not be page aligned, calculate the
+		 * offset within the page.
+		 */
+		s_off = vaddr & ~PAGE_MASK;
+		d_off = dst_vaddr & ~PAGE_MASK;
+		len = min_t(size_t, (PAGE_SIZE - s_off), size);
+
+		if (dec)
+			ret = __sev_dbg_decrypt_user(kvm,
+						     __sme_page_pa(src_p[0]) + s_off,
+						     dst_vaddr,
+						     __sme_page_pa(dst_p[0]) + d_off,
+						     len, &argp->error);
+		else
+			ret = __sev_dbg_encrypt_user(kvm,
+						     __sme_page_pa(src_p[0]) + s_off,
+						     vaddr,
+						     __sme_page_pa(dst_p[0]) + d_off,
+						     dst_vaddr,
+						     len, &argp->error);
+
+		sev_unpin_memory(kvm, src_p, n);
+		sev_unpin_memory(kvm, dst_p, n);
+
+		if (ret)
+			goto err;
+
+		next_vaddr = vaddr + len;
+		dst_vaddr = dst_vaddr + len;
+		size -= len;
+	}
+err:
+	return ret;
+}
+
+static int sev_launch_secret(struct kvm *kvm, struct kvm_sev_cmd *argp)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct sev_data_launch_secret *data;
+	struct kvm_sev_launch_secret params;
+	struct page **pages;
+	void *blob, *hdr;
+	unsigned long n;
+	int ret, offset;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	if (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))
+		return -EFAULT;
+
+	pages = sev_pin_memory(kvm, params.guest_uaddr, params.guest_len, &n, 1);
+	if (!pages)
+		return -ENOMEM;
+
+	/*
+	 * The secret must be copied into contiguous memory region, lets verify
+	 * that userspace memory pages are contiguous before we issue command.
+	 */
+	if (get_num_contig_pages(0, pages, n) != n) {
+		ret = -EINVAL;
+		goto e_unpin_memory;
+	}
+
+	ret = -ENOMEM;
+	data = kzalloc(sizeof(*data), GFP_KERNEL_ACCOUNT);
+	if (!data)
+		goto e_unpin_memory;
+
+	offset = params.guest_uaddr & (PAGE_SIZE - 1);
+	data->guest_address = __sme_page_pa(pages[0]) + offset;
+	data->guest_len = params.guest_len;
+
+	blob = psp_copy_user_blob(params.trans_uaddr, params.trans_len);
+	if (IS_ERR(blob)) {
+		ret = PTR_ERR(blob);
+		goto e_free;
+	}
+
+	data->trans_address = __psp_pa(blob);
+	data->trans_len = params.trans_len;
+
+	hdr = psp_copy_user_blob(params.hdr_uaddr, params.hdr_len);
+	if (IS_ERR(hdr)) {
+		ret = PTR_ERR(hdr);
+		goto e_free_blob;
+	}
+	data->hdr_address = __psp_pa(hdr);
+	data->hdr_len = params.hdr_len;
+
+	data->handle = sev->handle;
+	ret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_UPDATE_SECRET, data, &argp->error);
+
+	kfree(hdr);
+
+e_free_blob:
+	kfree(blob);
+e_free:
+	kfree(data);
+e_unpin_memory:
+	sev_unpin_memory(kvm, pages, n);
+	return ret;
+}
+
+int svm_mem_enc_op(struct kvm *kvm, void __user *argp)
+{
+	struct kvm_sev_cmd sev_cmd;
+	int r;
+
+	if (!svm_sev_enabled())
+		return -ENOTTY;
+
+	if (!argp)
+		return 0;
+
+	if (copy_from_user(&sev_cmd, argp, sizeof(struct kvm_sev_cmd)))
+		return -EFAULT;
+
+	mutex_lock(&kvm->lock);
+
+	switch (sev_cmd.id) {
+	case KVM_SEV_INIT:
+		r = sev_guest_init(kvm, &sev_cmd);
+		break;
+	case KVM_SEV_LAUNCH_START:
+		r = sev_launch_start(kvm, &sev_cmd);
+		break;
+	case KVM_SEV_LAUNCH_UPDATE_DATA:
+		r = sev_launch_update_data(kvm, &sev_cmd);
+		break;
+	case KVM_SEV_LAUNCH_MEASURE:
+		r = sev_launch_measure(kvm, &sev_cmd);
+		break;
+	case KVM_SEV_LAUNCH_FINISH:
+		r = sev_launch_finish(kvm, &sev_cmd);
+		break;
+	case KVM_SEV_GUEST_STATUS:
+		r = sev_guest_status(kvm, &sev_cmd);
+		break;
+	case KVM_SEV_DBG_DECRYPT:
+		r = sev_dbg_crypt(kvm, &sev_cmd, true);
+		break;
+	case KVM_SEV_DBG_ENCRYPT:
+		r = sev_dbg_crypt(kvm, &sev_cmd, false);
+		break;
+	case KVM_SEV_LAUNCH_SECRET:
+		r = sev_launch_secret(kvm, &sev_cmd);
+		break;
+	default:
+		r = -EINVAL;
+		goto out;
+	}
+
+	if (copy_to_user(argp, &sev_cmd, sizeof(struct kvm_sev_cmd)))
+		r = -EFAULT;
+
+out:
+	mutex_unlock(&kvm->lock);
+	return r;
+}
+
+int svm_register_enc_region(struct kvm *kvm,
+			    struct kvm_enc_region *range)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct enc_region *region;
+	int ret = 0;
+
+	if (!sev_guest(kvm))
+		return -ENOTTY;
+
+	if (range->addr > ULONG_MAX || range->size > ULONG_MAX)
+		return -EINVAL;
+
+	region = kzalloc(sizeof(*region), GFP_KERNEL_ACCOUNT);
+	if (!region)
+		return -ENOMEM;
+
+	region->pages = sev_pin_memory(kvm, range->addr, range->size, &region->npages, 1);
+	if (!region->pages) {
+		ret = -ENOMEM;
+		goto e_free;
+	}
+
+	/*
+	 * The guest may change the memory encryption attribute from C=0 -> C=1
+	 * or vice versa for this memory range. Lets make sure caches are
+	 * flushed to ensure that guest data gets written into memory with
+	 * correct C-bit.
+	 */
+	sev_clflush_pages(region->pages, region->npages);
+
+	region->uaddr = range->addr;
+	region->size = range->size;
+
+	mutex_lock(&kvm->lock);
+	list_add_tail(&region->list, &sev->regions_list);
+	mutex_unlock(&kvm->lock);
+
+	return ret;
+
+e_free:
+	kfree(region);
+	return ret;
+}
+
+static struct enc_region *
+find_enc_region(struct kvm *kvm, struct kvm_enc_region *range)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct list_head *head = &sev->regions_list;
+	struct enc_region *i;
+
+	list_for_each_entry(i, head, list) {
+		if (i->uaddr == range->addr &&
+		    i->size == range->size)
+			return i;
+	}
+
+	return NULL;
+}
+
+static void __unregister_enc_region_locked(struct kvm *kvm,
+					   struct enc_region *region)
+{
+	sev_unpin_memory(kvm, region->pages, region->npages);
+	list_del(&region->list);
+	kfree(region);
+}
+
+int svm_unregister_enc_region(struct kvm *kvm,
+			      struct kvm_enc_region *range)
+{
+	struct enc_region *region;
+	int ret;
+
+	mutex_lock(&kvm->lock);
+
+	if (!sev_guest(kvm)) {
+		ret = -ENOTTY;
+		goto failed;
+	}
+
+	region = find_enc_region(kvm, range);
+	if (!region) {
+		ret = -EINVAL;
+		goto failed;
+	}
+
+	/*
+	 * Ensure that all guest tagged cache entries are flushed before
+	 * releasing the pages back to the system for use. CLFLUSH will
+	 * not do this, so issue a WBINVD.
+	 */
+	wbinvd_on_all_cpus();
+
+	__unregister_enc_region_locked(kvm, region);
+
+	mutex_unlock(&kvm->lock);
+	return 0;
+
+failed:
+	mutex_unlock(&kvm->lock);
+	return ret;
+}
+
+void sev_vm_destroy(struct kvm *kvm)
+{
+	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+	struct list_head *head = &sev->regions_list;
+	struct list_head *pos, *q;
+
+	if (!sev_guest(kvm))
+		return;
+
+	mutex_lock(&kvm->lock);
+
+	/*
+	 * Ensure that all guest tagged cache entries are flushed before
+	 * releasing the pages back to the system for use. CLFLUSH will
+	 * not do this, so issue a WBINVD.
+	 */
+	wbinvd_on_all_cpus();
+
+	/*
+	 * if userspace was terminated before unregistering the memory regions
+	 * then lets unpin all the registered memory.
+	 */
+	if (!list_empty(head)) {
+		list_for_each_safe(pos, q, head) {
+			__unregister_enc_region_locked(kvm,
+				list_entry(pos, struct enc_region, list));
+		}
+	}
+
+	mutex_unlock(&kvm->lock);
+
+	sev_unbind_asid(kvm, sev->handle);
+	sev_asid_free(sev->asid);
+}
+
+int __init sev_hardware_setup(void)
+{
+	struct sev_user_data_status *status;
+	int rc;
+
+	/* Maximum number of encrypted guests supported simultaneously */
+	max_sev_asid = cpuid_ecx(0x8000001F);
+
+	if (!max_sev_asid)
+		return 1;
+
+	/* Minimum ASID value that should be used for SEV guest */
+	min_sev_asid = cpuid_edx(0x8000001F);
+
+	/* Initialize SEV ASID bitmaps */
+	sev_asid_bitmap = bitmap_zalloc(max_sev_asid, GFP_KERNEL);
+	if (!sev_asid_bitmap)
+		return 1;
+
+	sev_reclaim_asid_bitmap = bitmap_zalloc(max_sev_asid, GFP_KERNEL);
+	if (!sev_reclaim_asid_bitmap)
+		return 1;
+
+	status = kmalloc(sizeof(*status), GFP_KERNEL);
+	if (!status)
+		return 1;
+
+	/*
+	 * Check SEV platform status.
+	 *
+	 * PLATFORM_STATUS can be called in any state, if we failed to query
+	 * the PLATFORM status then either PSP firmware does not support SEV
+	 * feature or SEV firmware is dead.
+	 */
+	rc = sev_platform_status(status, NULL);
+	if (rc)
+		goto err;
+
+	pr_info("SEV supported\n");
+
+err:
+	kfree(status);
+	return rc;
+}
+
+void sev_hardware_teardown(void)
+{
+	bitmap_free(sev_asid_bitmap);
+	bitmap_free(sev_reclaim_asid_bitmap);
+
+	sev_flush_asids();
+}
+
+void pre_sev_run(struct vcpu_svm *svm, int cpu)
+{
+	struct svm_cpu_data *sd = per_cpu(svm_data, cpu);
+	int asid = sev_get_asid(svm->vcpu.kvm);
+
+	/* Assign the asid allocated with this SEV guest */
+	svm->vmcb->control.asid = asid;
+
+	/*
+	 * Flush guest TLB:
+	 *
+	 * 1) when different VMCB for the same ASID is to be run on the same host CPU.
+	 * 2) or this VMCB was executed on different host CPU in previous VMRUNs.
+	 */
+	if (sd->sev_vmcbs[asid] == svm->vmcb &&
+	    svm->last_cpu == cpu)
+		return;
+
+	svm->last_cpu = cpu;
+	sd->sev_vmcbs[asid] = svm->vmcb;
+	svm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
+	mark_dirty(svm->vmcb, VMCB_ASID);
+}
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/svm/svm.h
