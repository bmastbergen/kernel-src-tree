KVM: x86: Rename ->tlb_flush() to ->tlb_flush_all()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit 7780938cc70b848650722762fa4c7496fa68f9ec
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/7780938c.failed

Rename ->tlb_flush() to ->tlb_flush_all() in preparation for adding a
new hook to flush only the current ASID/context.

Opportunstically replace the comment in vmx_flush_tlb() that explains
why it flushes all EPTP/VPID contexts with a comment explaining why it
unconditionally uses INVEPT when EPT is enabled.  I.e. rely on the "all"
part of the name to clarify why it does global INVEPT/INVVPID.

No functional change intended.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Message-Id: <20200320212833.3507-23-sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 7780938cc70b848650722762fa4c7496fa68f9ec)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/include/asm/kvm_host.h
index 10275c160b4e,e6305d5e28fa..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1101,7 -1103,7 +1101,11 @@@ struct kvm_x86_ops 
  	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
  	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
  
++<<<<<<< HEAD
 +	void (*tlb_flush)(struct kvm_vcpu *vcpu, bool invalidate_gpa);
++=======
+ 	void (*tlb_flush_all)(struct kvm_vcpu *vcpu);
++>>>>>>> 7780938cc70b (KVM: x86: Rename ->tlb_flush() to ->tlb_flush_all())
  	int  (*tlb_remote_flush)(struct kvm *kvm);
  	int  (*tlb_remote_flush_with_range)(struct kvm *kvm,
  			struct kvm_tlb_range *range);
diff --cc arch/x86/kvm/mmu/mmu.c
index dbd97923dc2c,85e17a057094..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -5235,8 -5178,8 +5235,13 @@@ int kvm_mmu_load(struct kvm_vcpu *vcpu
  	kvm_mmu_sync_roots(vcpu);
  	if (r)
  		goto out;
++<<<<<<< HEAD
 +	kvm_mmu_load_cr3(vcpu);
 +	kvm_x86_ops->tlb_flush(vcpu, true);
++=======
+ 	kvm_mmu_load_pgd(vcpu);
+ 	kvm_x86_ops.tlb_flush_all(vcpu);
++>>>>>>> 7780938cc70b (KVM: x86: Rename ->tlb_flush() to ->tlb_flush_all())
  out:
  	return r;
  }
diff --cc arch/x86/kvm/vmx/vmx.c
index dbadf7ce3b2f,da868846d96d..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -2931,6 -2838,45 +2931,48 @@@ static void exit_lmode(struct kvm_vcpu 
  
  #endif
  
++<<<<<<< HEAD
++=======
+ static void vmx_flush_tlb_all(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 
+ 	/*
+ 	 * INVEPT must be issued when EPT is enabled, irrespective of VPID, as
+ 	 * the CPU is not required to invalidate guest-physical mappings on
+ 	 * VM-Entry, even if VPID is disabled.  Guest-physical mappings are
+ 	 * associated with the root EPT structure and not any particular VPID
+ 	 * (INVVPID also isn't required to invalidate guest-physical mappings).
+ 	 */
+ 	if (enable_ept) {
+ 		ept_sync_global();
+ 	} else if (enable_vpid) {
+ 		if (cpu_has_vmx_invvpid_global()) {
+ 			vpid_sync_vcpu_global();
+ 		} else {
+ 			vpid_sync_vcpu_single(vmx->vpid);
+ 			vpid_sync_vcpu_single(vmx->nested.vpid02);
+ 		}
+ 	}
+ }
+ 
+ static void vmx_flush_tlb_current(struct kvm_vcpu *vcpu)
+ {
+ 	u64 root_hpa = vcpu->arch.mmu->root_hpa;
+ 
+ 	/* No flush required if the current context is invalid. */
+ 	if (!VALID_PAGE(root_hpa))
+ 		return;
+ 
+ 	if (enable_ept)
+ 		ept_sync_context(construct_eptp(vcpu, root_hpa));
+ 	else if (!is_guest_mode(vcpu))
+ 		vpid_sync_context(to_vmx(vcpu)->vpid);
+ 	else
+ 		vpid_sync_context(nested_get_vpid02(vcpu));
+ }
+ 
++>>>>>>> 7780938cc70b (KVM: x86: Rename ->tlb_flush() to ->tlb_flush_all())
  static void vmx_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t addr)
  {
  	/*
@@@ -7871,166 -8030,13 +7913,166 @@@ static __init int hardware_setup(void
  	return r;
  }
  
 -static struct kvm_x86_init_ops vmx_init_ops __initdata = {
 +static __exit void hardware_unsetup(void)
 +{
 +	if (nested)
 +		nested_vmx_hardware_unsetup();
 +
 +	free_kvm_area();
 +}
 +
 +static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
  	.cpu_has_kvm_support = cpu_has_kvm_support,
  	.disabled_by_bios = vmx_disabled_by_bios,
 -	.check_processor_compatibility = vmx_check_processor_compat,
  	.hardware_setup = hardware_setup,
 +	.hardware_unsetup = hardware_unsetup,
 +	.check_processor_compatibility = vmx_check_processor_compat,
 +	.hardware_enable = hardware_enable,
 +	.hardware_disable = hardware_disable,
 +	.cpu_has_accelerated_tpr = report_flexpriority,
 +	.has_emulated_msr = vmx_has_emulated_msr,
 +
 +	.vm_init = vmx_vm_init,
 +	.vm_alloc = vmx_vm_alloc,
 +	.vm_free = vmx_vm_free,
 +
 +	.vcpu_create = vmx_create_vcpu,
 +	.vcpu_free = vmx_free_vcpu,
 +	.vcpu_reset = vmx_vcpu_reset,
 +
 +	.prepare_guest_switch = vmx_prepare_switch_to_guest,
 +	.vcpu_load = vmx_vcpu_load,
 +	.vcpu_put = vmx_vcpu_put,
 +
 +	.update_bp_intercept = update_exception_bitmap,
 +	.get_msr_feature = vmx_get_msr_feature,
 +	.get_msr = vmx_get_msr,
 +	.set_msr = vmx_set_msr,
 +	.get_segment_base = vmx_get_segment_base,
 +	.get_segment = vmx_get_segment,
 +	.set_segment = vmx_set_segment,
 +	.get_cpl = vmx_get_cpl,
 +	.get_cs_db_l_bits = vmx_get_cs_db_l_bits,
 +	.decache_cr0_guest_bits = vmx_decache_cr0_guest_bits,
 +	.decache_cr4_guest_bits = vmx_decache_cr4_guest_bits,
 +	.set_cr0 = vmx_set_cr0,
 +	.set_cr3 = vmx_set_cr3,
 +	.set_cr4 = vmx_set_cr4,
 +	.set_efer = vmx_set_efer,
 +	.get_idt = vmx_get_idt,
 +	.set_idt = vmx_set_idt,
 +	.get_gdt = vmx_get_gdt,
 +	.set_gdt = vmx_set_gdt,
 +	.get_dr6 = vmx_get_dr6,
 +	.set_dr6 = vmx_set_dr6,
 +	.set_dr7 = vmx_set_dr7,
 +	.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs,
 +	.cache_reg = vmx_cache_reg,
 +	.get_rflags = vmx_get_rflags,
 +	.set_rflags = vmx_set_rflags,
 +
- 	.tlb_flush = vmx_flush_tlb,
++	.tlb_flush_all = vmx_flush_tlb_all,
 +	.tlb_flush_gva = vmx_flush_tlb_gva,
 +	.tlb_flush_guest = vmx_flush_tlb_guest,
  
 -	.runtime_ops = &vmx_x86_ops,
 +	.run = vmx_vcpu_run,
 +	.handle_exit = vmx_handle_exit,
 +	.skip_emulated_instruction = vmx_skip_emulated_instruction,
 +	.update_emulated_instruction = vmx_update_emulated_instruction,
 +	.set_interrupt_shadow = vmx_set_interrupt_shadow,
 +	.get_interrupt_shadow = vmx_get_interrupt_shadow,
 +	.patch_hypercall = vmx_patch_hypercall,
 +	.set_irq = vmx_inject_irq,
 +	.set_nmi = vmx_inject_nmi,
 +	.queue_exception = vmx_queue_exception,
 +	.cancel_injection = vmx_cancel_injection,
 +	.interrupt_allowed = vmx_interrupt_allowed,
 +	.nmi_allowed = vmx_nmi_allowed,
 +	.get_nmi_mask = vmx_get_nmi_mask,
 +	.set_nmi_mask = vmx_set_nmi_mask,
 +	.enable_nmi_window = enable_nmi_window,
 +	.enable_irq_window = enable_irq_window,
 +	.update_cr8_intercept = update_cr8_intercept,
 +	.set_virtual_apic_mode = vmx_set_virtual_apic_mode,
 +	.set_apic_access_page_addr = vmx_set_apic_access_page_addr,
 +	.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl,
 +	.load_eoi_exitmap = vmx_load_eoi_exitmap,
 +	.apicv_post_state_restore = vmx_apicv_post_state_restore,
 +	.hwapic_irr_update = vmx_hwapic_irr_update,
 +	.hwapic_isr_update = vmx_hwapic_isr_update,
 +	.guest_apic_has_interrupt = vmx_guest_apic_has_interrupt,
 +	.sync_pir_to_irr = vmx_sync_pir_to_irr,
 +	.deliver_posted_interrupt = vmx_deliver_posted_interrupt,
 +	.dy_apicv_has_pending_interrupt = vmx_dy_apicv_has_pending_interrupt,
 +
 +	.set_tss_addr = vmx_set_tss_addr,
 +	.set_identity_map_addr = vmx_set_identity_map_addr,
 +	.get_tdp_level = get_ept_level,
 +	.get_mt_mask = vmx_get_mt_mask,
 +
 +	.get_exit_info = vmx_get_exit_info,
 +
 +	.get_lpage_level = vmx_get_lpage_level,
 +
 +	.cpuid_update = vmx_cpuid_update,
 +
 +	.rdtscp_supported = vmx_rdtscp_supported,
 +	.invpcid_supported = vmx_invpcid_supported,
 +
 +	.set_supported_cpuid = vmx_set_supported_cpuid,
 +
 +	.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit,
 +
 +	.read_l1_tsc_offset = vmx_read_l1_tsc_offset,
 +	.write_l1_tsc_offset = vmx_write_l1_tsc_offset,
 +
 +	.set_tdp_cr3 = vmx_set_cr3,
 +
 +	.check_intercept = vmx_check_intercept,
 +	.handle_exit_irqoff = vmx_handle_exit_irqoff,
 +	.mpx_supported = vmx_mpx_supported,
 +	.xsaves_supported = vmx_xsaves_supported,
 +	.umip_emulated = vmx_umip_emulated,
 +	.pt_supported = vmx_pt_supported,
 +	.pku_supported = vmx_pku_supported,
 +
 +	.request_immediate_exit = vmx_request_immediate_exit,
 +
 +	.sched_in = vmx_sched_in,
 +
 +	.slot_enable_log_dirty = vmx_slot_enable_log_dirty,
 +	.slot_disable_log_dirty = vmx_slot_disable_log_dirty,
 +	.flush_log_dirty = vmx_flush_log_dirty,
 +	.enable_log_dirty_pt_masked = vmx_enable_log_dirty_pt_masked,
 +	.write_log_dirty = vmx_write_pml_buffer,
 +
 +	.pre_block = vmx_pre_block,
 +	.post_block = vmx_post_block,
 +
 +	.pmu_ops = &intel_pmu_ops,
 +
 +	.update_pi_irte = vmx_update_pi_irte,
 +
 +#ifdef CONFIG_X86_64
 +	.set_hv_timer = vmx_set_hv_timer,
 +	.cancel_hv_timer = vmx_cancel_hv_timer,
 +#endif
 +
 +	.setup_mce = vmx_setup_mce,
 +
 +	.smi_allowed = vmx_smi_allowed,
 +	.pre_enter_smm = vmx_pre_enter_smm,
 +	.pre_leave_smm = vmx_pre_leave_smm,
 +	.enable_smi_window = enable_smi_window,
 +
 +	.check_nested_events = NULL,
 +	.get_nested_state = NULL,
 +	.set_nested_state = NULL,
 +	.get_vmcs12_pages = NULL,
 +	.nested_enable_evmcs = NULL,
 +	.nested_get_evmcs_version = NULL,
 +	.need_emulation_on_page_fault = vmx_need_emulation_on_page_fault,
 +	.apic_init_signal_blocked = vmx_apic_init_signal_blocked,
  };
  
  static void vmx_cleanup_l1d_flush(void)
diff --cc arch/x86/kvm/x86.c
index 0a8a4dd82886,cd2a3d01bffb..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -2652,10 -2690,16 +2652,23 @@@ static void kvmclock_reset(struct kvm_v
  	vcpu->arch.time = 0;
  }
  
++<<<<<<< HEAD
 +static void kvm_vcpu_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)
 +{
 +	++vcpu->stat.tlb_flush;
 +	kvm_x86_ops->tlb_flush(vcpu, invalidate_gpa);
++=======
+ static void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)
+ {
+ 	++vcpu->stat.tlb_flush;
+ 	kvm_x86_ops.tlb_flush_all(vcpu);
+ }
+ 
+ static void kvm_vcpu_flush_tlb_guest(struct kvm_vcpu *vcpu)
+ {
+ 	++vcpu->stat.tlb_flush;
+ 	kvm_x86_ops.tlb_flush_guest(vcpu);
++>>>>>>> 7780938cc70b (KVM: x86: Rename ->tlb_flush() to ->tlb_flush_all())
  }
  
  static void record_steal_time(struct kvm_vcpu *vcpu)
@@@ -8088,10 -8220,12 +8101,16 @@@ static int vcpu_enter_guest(struct kvm_
  		}
  		if (kvm_check_request(KVM_REQ_MMU_SYNC, vcpu))
  			kvm_mmu_sync_roots(vcpu);
 -		if (kvm_check_request(KVM_REQ_LOAD_MMU_PGD, vcpu))
 -			kvm_mmu_load_pgd(vcpu);
 +		if (kvm_check_request(KVM_REQ_LOAD_CR3, vcpu))
 +			kvm_mmu_load_cr3(vcpu);
  		if (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))
++<<<<<<< HEAD
 +			kvm_vcpu_flush_tlb(vcpu, true);
++=======
+ 			kvm_vcpu_flush_tlb_all(vcpu);
+ 		if (kvm_check_request(KVM_REQ_HV_TLB_FLUSH, vcpu))
+ 			kvm_vcpu_flush_tlb_guest(vcpu);
++>>>>>>> 7780938cc70b (KVM: x86: Rename ->tlb_flush() to ->tlb_flush_all())
  		if (kvm_check_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu)) {
  			vcpu->run->exit_reason = KVM_EXIT_TPR_ACCESS;
  			r = 0;
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/mmu/mmu.c
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index b85940aff326..ad1419db9ec1 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -7481,7 +7481,7 @@ static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
 	.get_rflags = svm_get_rflags,
 	.set_rflags = svm_set_rflags,
 
-	.tlb_flush = svm_flush_tlb,
+	.tlb_flush_all = svm_flush_tlb,
 	.tlb_flush_gva = svm_flush_tlb_gva,
 	.tlb_flush_guest = svm_flush_tlb_guest,
 
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/x86.c
