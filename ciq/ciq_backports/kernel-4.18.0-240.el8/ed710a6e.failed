drm/nouveau/nouveau: fix page fault on device private memory

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Ralph Campbell <rcampbell@nvidia.com>
commit ed710a6ed797430026aa5116dd0ab22378798b69
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ed710a6e.failed

If system memory is migrated to device private memory and no GPU MMU
page table entry exists, the GPU will fault and call hmm_range_fault()
to get the PFN for the page. Since the .dev_private_owner pointer in
struct hmm_range is not set, hmm_range_fault returns an error which
results in the GPU program stopping with a fatal fault.
Fix this by setting .dev_private_owner appropriately.

Fixes: 08ddddda667b ("mm/hmm: check the device private page owner in hmm_range_fault()")
	Cc: stable@vger.kernel.org
	Signed-off-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Ben Skeggs <bskeggs@redhat.com>
(cherry picked from commit ed710a6ed797430026aa5116dd0ab22378798b69)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/nouveau/nouveau_svm.c
diff --cc drivers/gpu/drm/nouveau/nouveau_svm.c
index 516be2bce20b,6586d9d39874..000000000000
--- a/drivers/gpu/drm/nouveau/nouveau_svm.c
+++ b/drivers/gpu/drm/nouveau/nouveau_svm.c
@@@ -481,12 -481,127 +481,126 @@@ nouveau_svm_fault_cache(struct nouveau_
  		fault->inst, fault->addr, fault->access);
  }
  
 -struct svm_notifier {
 -	struct mmu_interval_notifier notifier;
 -	struct nouveau_svmm *svmm;
 -};
 -
 -static bool nouveau_svm_range_invalidate(struct mmu_interval_notifier *mni,
 -					 const struct mmu_notifier_range *range,
 -					 unsigned long cur_seq)
 +static inline bool
 +nouveau_range_done(struct hmm_range *range)
  {
++<<<<<<< HEAD
 +	bool ret = hmm_range_valid(range);
++=======
+ 	struct svm_notifier *sn =
+ 		container_of(mni, struct svm_notifier, notifier);
+ 
+ 	/*
+ 	 * serializes the update to mni->invalidate_seq done by caller and
+ 	 * prevents invalidation of the PTE from progressing while HW is being
+ 	 * programmed. This is very hacky and only works because the normal
+ 	 * notifier that does invalidation is always called after the range
+ 	 * notifier.
+ 	 */
+ 	if (mmu_notifier_range_blockable(range))
+ 		mutex_lock(&sn->svmm->mutex);
+ 	else if (!mutex_trylock(&sn->svmm->mutex))
+ 		return false;
+ 	mmu_interval_set_seq(mni, cur_seq);
+ 	mutex_unlock(&sn->svmm->mutex);
+ 	return true;
+ }
+ 
+ static const struct mmu_interval_notifier_ops nouveau_svm_mni_ops = {
+ 	.invalidate = nouveau_svm_range_invalidate,
+ };
+ 
+ static void nouveau_hmm_convert_pfn(struct nouveau_drm *drm,
+ 				    struct hmm_range *range, u64 *ioctl_addr)
+ {
+ 	unsigned long i, npages;
+ 
+ 	/*
+ 	 * The ioctl_addr prepared here is passed through nvif_object_ioctl()
+ 	 * to an eventual DMA map in something like gp100_vmm_pgt_pfn()
+ 	 *
+ 	 * This is all just encoding the internal hmm representation into a
+ 	 * different nouveau internal representation.
+ 	 */
+ 	npages = (range->end - range->start) >> PAGE_SHIFT;
+ 	for (i = 0; i < npages; ++i) {
+ 		struct page *page;
+ 
+ 		if (!(range->hmm_pfns[i] & HMM_PFN_VALID)) {
+ 			ioctl_addr[i] = 0;
+ 			continue;
+ 		}
+ 
+ 		page = hmm_pfn_to_page(range->hmm_pfns[i]);
+ 		if (is_device_private_page(page))
+ 			ioctl_addr[i] = nouveau_dmem_page_addr(page) |
+ 					NVIF_VMM_PFNMAP_V0_V |
+ 					NVIF_VMM_PFNMAP_V0_VRAM;
+ 		else
+ 			ioctl_addr[i] = page_to_phys(page) |
+ 					NVIF_VMM_PFNMAP_V0_V |
+ 					NVIF_VMM_PFNMAP_V0_HOST;
+ 		if (range->hmm_pfns[i] & HMM_PFN_WRITE)
+ 			ioctl_addr[i] |= NVIF_VMM_PFNMAP_V0_W;
+ 	}
+ }
+ 
+ static int nouveau_range_fault(struct nouveau_svmm *svmm,
+ 			       struct nouveau_drm *drm, void *data, u32 size,
+ 			       unsigned long hmm_pfns[], u64 *ioctl_addr,
+ 			       struct svm_notifier *notifier)
+ {
+ 	unsigned long timeout =
+ 		jiffies + msecs_to_jiffies(HMM_RANGE_DEFAULT_TIMEOUT);
+ 	/* Have HMM fault pages within the fault window to the GPU. */
+ 	struct hmm_range range = {
+ 		.notifier = &notifier->notifier,
+ 		.start = notifier->notifier.interval_tree.start,
+ 		.end = notifier->notifier.interval_tree.last + 1,
+ 		.pfn_flags_mask = HMM_PFN_REQ_FAULT | HMM_PFN_REQ_WRITE,
+ 		.hmm_pfns = hmm_pfns,
+ 		.dev_private_owner = drm->dev,
+ 	};
+ 	struct mm_struct *mm = notifier->notifier.mm;
+ 	int ret;
+ 
+ 	while (true) {
+ 		if (time_after(jiffies, timeout))
+ 			return -EBUSY;
+ 
+ 		range.notifier_seq = mmu_interval_read_begin(range.notifier);
+ 		mmap_read_lock(mm);
+ 		ret = hmm_range_fault(&range);
+ 		mmap_read_unlock(mm);
+ 		if (ret) {
+ 			/*
+ 			 * FIXME: the input PFN_REQ flags are destroyed on
+ 			 * -EBUSY, we need to regenerate them, also for the
+ 			 * other continue below
+ 			 */
+ 			if (ret == -EBUSY)
+ 				continue;
+ 			return ret;
+ 		}
+ 
+ 		mutex_lock(&svmm->mutex);
+ 		if (mmu_interval_read_retry(range.notifier,
+ 					    range.notifier_seq)) {
+ 			mutex_unlock(&svmm->mutex);
+ 			continue;
+ 		}
+ 		break;
+ 	}
+ 
+ 	nouveau_hmm_convert_pfn(drm, &range, ioctl_addr);
+ 
+ 	svmm->vmm->vmm.object.client->super = true;
+ 	ret = nvif_object_ioctl(&svmm->vmm->vmm.object, data, size, NULL);
+ 	svmm->vmm->vmm.object.client->super = false;
+ 	mutex_unlock(&svmm->mutex);
++>>>>>>> ed710a6ed797 (drm/nouveau/nouveau: fix page fault on device private memory)
  
 +	hmm_range_unregister(range);
  	return ret;
  }
  
* Unmerged path drivers/gpu/drm/nouveau/nouveau_svm.c
