bpf: Attachment verification for BPF_MODIFY_RETURN

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author KP Singh <kpsingh@google.com>
commit 6ba43b761c41349140662e223401bec0e48950e7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6ba43b76.failed

- Allow BPF_MODIFY_RETURN attachment only to functions that are:

    * Whitelisted for error injection by checking
      within_error_injection_list. Similar discussions happened for the
      bpf_override_return helper.

    * security hooks, this is expected to be cleaned up with the LSM
      changes after the KRSI patches introduce the LSM_HOOK macro:

        https://lore.kernel.org/bpf/20200220175250.10795-1-kpsingh@chromium.org/

- The attachment is currently limited to functions that return an int.
  This can be extended later other types (e.g. PTR).

	Signed-off-by: KP Singh <kpsingh@google.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
Link: https://lore.kernel.org/bpf/20200304191853.1529-5-kpsingh@chromium.org
(cherry picked from commit 6ba43b761c41349140662e223401bec0e48950e7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/btf.c
#	kernel/bpf/verifier.c
diff --cc kernel/bpf/btf.c
index d659110c662a,50080add2ab9..000000000000
--- a/kernel/bpf/btf.c
+++ b/kernel/bpf/btf.c
@@@ -3574,20 -3702,48 +3574,53 @@@ bool btf_ctx_access(int off, int size, 
  	}
  	arg = off / 8;
  	args = (const struct btf_param *)(t + 1);
 -	/* if (t == NULL) Fall back to default BPF prog with 5 u64 arguments */
 -	nr_args = t ? btf_type_vlen(t) : 5;
 -	if (prog->aux->attach_btf_trace) {
 -		/* skip first 'void *__data' argument in btf_trace_##name typedef */
 -		args++;
 -		nr_args--;
 +	/* skip first 'void *__data' argument in btf_trace_##name typedef */
 +	args++;
 +	nr_args = btf_type_vlen(t) - 1;
 +	if (arg >= nr_args) {
 +		bpf_log(log, "raw_tp '%s' doesn't have %d-th argument\n",
 +			tname, arg);
 +		return false;
  	}
  
++<<<<<<< HEAD
 +	t = btf_type_by_id(btf_vmlinux, args[arg].type);
++=======
+ 	if (arg == nr_args) {
+ 		if (prog->expected_attach_type == BPF_TRACE_FEXIT) {
+ 			if (!t)
+ 				return true;
+ 			t = btf_type_by_id(btf, t->type);
+ 		} else if (prog->expected_attach_type == BPF_MODIFY_RETURN) {
+ 			/* For now the BPF_MODIFY_RETURN can only be attached to
+ 			 * functions that return an int.
+ 			 */
+ 			if (!t)
+ 				return false;
+ 
+ 			t = btf_type_skip_modifiers(btf, t->type, NULL);
+ 			if (!btf_type_is_int(t)) {
+ 				bpf_log(log,
+ 					"ret type %s not allowed for fmod_ret\n",
+ 					btf_kind_str[BTF_INFO_KIND(t->info)]);
+ 				return false;
+ 			}
+ 		}
+ 	} else if (arg >= nr_args) {
+ 		bpf_log(log, "func '%s' doesn't have %d-th argument\n",
+ 			tname, arg + 1);
+ 		return false;
+ 	} else {
+ 		if (!t)
+ 			/* Default prog with 5 args */
+ 			return true;
+ 		t = btf_type_by_id(btf, args[arg].type);
+ 	}
++>>>>>>> 6ba43b761c41 (bpf: Attachment verification for BPF_MODIFY_RETURN)
  	/* skip modifiers */
  	while (btf_type_is_modifier(t))
 -		t = btf_type_by_id(btf, t->type);
 -	if (btf_type_is_int(t) || btf_type_is_enum(t))
 +		t = btf_type_by_id(btf_vmlinux, t->type);
 +	if (btf_type_is_int(t))
  		/* accessing a scalar */
  		return true;
  	if (!btf_type_is_ptr(t)) {
diff --cc kernel/bpf/verifier.c
index 1889f4bbcf69,ae32517d4ccd..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -9555,6 -9750,295 +9556,298 @@@ static void print_verification_stats(st
  		env->peak_states, env->longest_mark_read_walk);
  }
  
++<<<<<<< HEAD
++=======
+ static int check_struct_ops_btf_id(struct bpf_verifier_env *env)
+ {
+ 	const struct btf_type *t, *func_proto;
+ 	const struct bpf_struct_ops *st_ops;
+ 	const struct btf_member *member;
+ 	struct bpf_prog *prog = env->prog;
+ 	u32 btf_id, member_idx;
+ 	const char *mname;
+ 
+ 	btf_id = prog->aux->attach_btf_id;
+ 	st_ops = bpf_struct_ops_find(btf_id);
+ 	if (!st_ops) {
+ 		verbose(env, "attach_btf_id %u is not a supported struct\n",
+ 			btf_id);
+ 		return -ENOTSUPP;
+ 	}
+ 
+ 	t = st_ops->type;
+ 	member_idx = prog->expected_attach_type;
+ 	if (member_idx >= btf_type_vlen(t)) {
+ 		verbose(env, "attach to invalid member idx %u of struct %s\n",
+ 			member_idx, st_ops->name);
+ 		return -EINVAL;
+ 	}
+ 
+ 	member = &btf_type_member(t)[member_idx];
+ 	mname = btf_name_by_offset(btf_vmlinux, member->name_off);
+ 	func_proto = btf_type_resolve_func_ptr(btf_vmlinux, member->type,
+ 					       NULL);
+ 	if (!func_proto) {
+ 		verbose(env, "attach to invalid member %s(@idx %u) of struct %s\n",
+ 			mname, member_idx, st_ops->name);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (st_ops->check_member) {
+ 		int err = st_ops->check_member(t, member);
+ 
+ 		if (err) {
+ 			verbose(env, "attach to unsupported member %s of struct %s\n",
+ 				mname, st_ops->name);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	prog->aux->attach_func_proto = func_proto;
+ 	prog->aux->attach_func_name = mname;
+ 	env->ops = st_ops->verifier_ops;
+ 
+ 	return 0;
+ }
+ #define SECURITY_PREFIX "security_"
+ 
+ static int check_attach_modify_return(struct bpf_verifier_env *env)
+ {
+ 	struct bpf_prog *prog = env->prog;
+ 	unsigned long addr = (unsigned long) prog->aux->trampoline->func.addr;
+ 
+ 	if (within_error_injection_list(addr))
+ 		return 0;
+ 
+ 	/* This is expected to be cleaned up in the future with the KRSI effort
+ 	 * introducing the LSM_HOOK macro for cleaning up lsm_hooks.h.
+ 	 */
+ 	if (!strncmp(SECURITY_PREFIX, prog->aux->attach_func_name,
+ 		     sizeof(SECURITY_PREFIX) - 1)) {
+ 
+ 		if (!capable(CAP_MAC_ADMIN))
+ 			return -EPERM;
+ 
+ 		return 0;
+ 	}
+ 
+ 	verbose(env, "fmod_ret attach_btf_id %u (%s) is not modifiable\n",
+ 		prog->aux->attach_btf_id, prog->aux->attach_func_name);
+ 
+ 	return -EINVAL;
+ }
+ 
+ static int check_attach_btf_id(struct bpf_verifier_env *env)
+ {
+ 	struct bpf_prog *prog = env->prog;
+ 	bool prog_extension = prog->type == BPF_PROG_TYPE_EXT;
+ 	struct bpf_prog *tgt_prog = prog->aux->linked_prog;
+ 	u32 btf_id = prog->aux->attach_btf_id;
+ 	const char prefix[] = "btf_trace_";
+ 	int ret = 0, subprog = -1, i;
+ 	struct bpf_trampoline *tr;
+ 	const struct btf_type *t;
+ 	bool conservative = true;
+ 	const char *tname;
+ 	struct btf *btf;
+ 	long addr;
+ 	u64 key;
+ 
+ 	if (prog->type == BPF_PROG_TYPE_STRUCT_OPS)
+ 		return check_struct_ops_btf_id(env);
+ 
+ 	if (prog->type != BPF_PROG_TYPE_TRACING && !prog_extension)
+ 		return 0;
+ 
+ 	if (!btf_id) {
+ 		verbose(env, "Tracing programs must provide btf_id\n");
+ 		return -EINVAL;
+ 	}
+ 	btf = bpf_prog_get_target_btf(prog);
+ 	if (!btf) {
+ 		verbose(env,
+ 			"FENTRY/FEXIT program can only be attached to another program annotated with BTF\n");
+ 		return -EINVAL;
+ 	}
+ 	t = btf_type_by_id(btf, btf_id);
+ 	if (!t) {
+ 		verbose(env, "attach_btf_id %u is invalid\n", btf_id);
+ 		return -EINVAL;
+ 	}
+ 	tname = btf_name_by_offset(btf, t->name_off);
+ 	if (!tname) {
+ 		verbose(env, "attach_btf_id %u doesn't have a name\n", btf_id);
+ 		return -EINVAL;
+ 	}
+ 	if (tgt_prog) {
+ 		struct bpf_prog_aux *aux = tgt_prog->aux;
+ 
+ 		for (i = 0; i < aux->func_info_cnt; i++)
+ 			if (aux->func_info[i].type_id == btf_id) {
+ 				subprog = i;
+ 				break;
+ 			}
+ 		if (subprog == -1) {
+ 			verbose(env, "Subprog %s doesn't exist\n", tname);
+ 			return -EINVAL;
+ 		}
+ 		conservative = aux->func_info_aux[subprog].unreliable;
+ 		if (prog_extension) {
+ 			if (conservative) {
+ 				verbose(env,
+ 					"Cannot replace static functions\n");
+ 				return -EINVAL;
+ 			}
+ 			if (!prog->jit_requested) {
+ 				verbose(env,
+ 					"Extension programs should be JITed\n");
+ 				return -EINVAL;
+ 			}
+ 			env->ops = bpf_verifier_ops[tgt_prog->type];
+ 		}
+ 		if (!tgt_prog->jited) {
+ 			verbose(env, "Can attach to only JITed progs\n");
+ 			return -EINVAL;
+ 		}
+ 		if (tgt_prog->type == prog->type) {
+ 			/* Cannot fentry/fexit another fentry/fexit program.
+ 			 * Cannot attach program extension to another extension.
+ 			 * It's ok to attach fentry/fexit to extension program.
+ 			 */
+ 			verbose(env, "Cannot recursively attach\n");
+ 			return -EINVAL;
+ 		}
+ 		if (tgt_prog->type == BPF_PROG_TYPE_TRACING &&
+ 		    prog_extension &&
+ 		    (tgt_prog->expected_attach_type == BPF_TRACE_FENTRY ||
+ 		     tgt_prog->expected_attach_type == BPF_TRACE_FEXIT)) {
+ 			/* Program extensions can extend all program types
+ 			 * except fentry/fexit. The reason is the following.
+ 			 * The fentry/fexit programs are used for performance
+ 			 * analysis, stats and can be attached to any program
+ 			 * type except themselves. When extension program is
+ 			 * replacing XDP function it is necessary to allow
+ 			 * performance analysis of all functions. Both original
+ 			 * XDP program and its program extension. Hence
+ 			 * attaching fentry/fexit to BPF_PROG_TYPE_EXT is
+ 			 * allowed. If extending of fentry/fexit was allowed it
+ 			 * would be possible to create long call chain
+ 			 * fentry->extension->fentry->extension beyond
+ 			 * reasonable stack size. Hence extending fentry is not
+ 			 * allowed.
+ 			 */
+ 			verbose(env, "Cannot extend fentry/fexit\n");
+ 			return -EINVAL;
+ 		}
+ 		key = ((u64)aux->id) << 32 | btf_id;
+ 	} else {
+ 		if (prog_extension) {
+ 			verbose(env, "Cannot replace kernel functions\n");
+ 			return -EINVAL;
+ 		}
+ 		key = btf_id;
+ 	}
+ 
+ 	switch (prog->expected_attach_type) {
+ 	case BPF_TRACE_RAW_TP:
+ 		if (tgt_prog) {
+ 			verbose(env,
+ 				"Only FENTRY/FEXIT progs are attachable to another BPF prog\n");
+ 			return -EINVAL;
+ 		}
+ 		if (!btf_type_is_typedef(t)) {
+ 			verbose(env, "attach_btf_id %u is not a typedef\n",
+ 				btf_id);
+ 			return -EINVAL;
+ 		}
+ 		if (strncmp(prefix, tname, sizeof(prefix) - 1)) {
+ 			verbose(env, "attach_btf_id %u points to wrong type name %s\n",
+ 				btf_id, tname);
+ 			return -EINVAL;
+ 		}
+ 		tname += sizeof(prefix) - 1;
+ 		t = btf_type_by_id(btf, t->type);
+ 		if (!btf_type_is_ptr(t))
+ 			/* should never happen in valid vmlinux build */
+ 			return -EINVAL;
+ 		t = btf_type_by_id(btf, t->type);
+ 		if (!btf_type_is_func_proto(t))
+ 			/* should never happen in valid vmlinux build */
+ 			return -EINVAL;
+ 
+ 		/* remember two read only pointers that are valid for
+ 		 * the life time of the kernel
+ 		 */
+ 		prog->aux->attach_func_name = tname;
+ 		prog->aux->attach_func_proto = t;
+ 		prog->aux->attach_btf_trace = true;
+ 		return 0;
+ 	default:
+ 		if (!prog_extension)
+ 			return -EINVAL;
+ 		/* fallthrough */
+ 	case BPF_MODIFY_RETURN:
+ 	case BPF_TRACE_FENTRY:
+ 	case BPF_TRACE_FEXIT:
+ 		if (!btf_type_is_func(t)) {
+ 			verbose(env, "attach_btf_id %u is not a function\n",
+ 				btf_id);
+ 			return -EINVAL;
+ 		}
+ 		if (prog_extension &&
+ 		    btf_check_type_match(env, prog, btf, t))
+ 			return -EINVAL;
+ 		t = btf_type_by_id(btf, t->type);
+ 		if (!btf_type_is_func_proto(t))
+ 			return -EINVAL;
+ 		tr = bpf_trampoline_lookup(key);
+ 		if (!tr)
+ 			return -ENOMEM;
+ 		prog->aux->attach_func_name = tname;
+ 		/* t is either vmlinux type or another program's type */
+ 		prog->aux->attach_func_proto = t;
+ 		mutex_lock(&tr->mutex);
+ 		if (tr->func.addr) {
+ 			prog->aux->trampoline = tr;
+ 			goto out;
+ 		}
+ 		if (tgt_prog && conservative) {
+ 			prog->aux->attach_func_proto = NULL;
+ 			t = NULL;
+ 		}
+ 		ret = btf_distill_func_proto(&env->log, btf, t,
+ 					     tname, &tr->func.model);
+ 		if (ret < 0)
+ 			goto out;
+ 		if (tgt_prog) {
+ 			if (subprog == 0)
+ 				addr = (long) tgt_prog->bpf_func;
+ 			else
+ 				addr = (long) tgt_prog->aux->func[subprog]->bpf_func;
+ 		} else {
+ 			addr = kallsyms_lookup_name(tname);
+ 			if (!addr) {
+ 				verbose(env,
+ 					"The address of function %s cannot be found\n",
+ 					tname);
+ 				ret = -ENOENT;
+ 				goto out;
+ 			}
+ 		}
+ 		tr->func.addr = (void *)addr;
+ 		prog->aux->trampoline = tr;
+ 
+ 		if (prog->expected_attach_type == BPF_MODIFY_RETURN)
+ 			ret = check_attach_modify_return(env);
+ out:
+ 		mutex_unlock(&tr->mutex);
+ 		if (ret)
+ 			bpf_trampoline_put(tr);
+ 		return ret;
+ 	}
+ }
+ 
++>>>>>>> 6ba43b761c41 (bpf: Attachment verification for BPF_MODIFY_RETURN)
  int bpf_check(struct bpf_prog **prog, union bpf_attr *attr,
  	      union bpf_attr __user *uattr)
  {
* Unmerged path kernel/bpf/btf.c
* Unmerged path kernel/bpf/verifier.c
