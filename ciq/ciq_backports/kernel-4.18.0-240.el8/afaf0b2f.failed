KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit afaf0b2f9b801c6eb2278b52d49e6a7d7b659cf1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/afaf0b2f.failed

Replace the kvm_x86_ops pointer in common x86 with an instance of the
struct to save one pointer dereference when invoking functions.  Copy the
struct by value to set the ops during kvm_init().

Arbitrarily use kvm_x86_ops.hardware_enable to track whether or not the
ops have been initialized, i.e. a vendor KVM module has been loaded.

	Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Message-Id: <20200321202603.19355-7-sean.j.christopherson@intel.com>
	Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit afaf0b2f9b801c6eb2278b52d49e6a7d7b659cf1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/mmu.h
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/trace.h
#	arch/x86/kvm/x86.c
diff --cc arch/x86/include/asm/kvm_host.h
index 57bbfbf61b11,54f991244fae..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1270,19 -1272,18 +1270,30 @@@ struct kvm_arch_async_pf 
  	bool direct_map;
  };
  
++<<<<<<< HEAD
 +extern struct kvm_x86_ops *kvm_x86_ops;
++=======
+ extern u64 __read_mostly host_efer;
+ 
+ extern struct kvm_x86_ops kvm_x86_ops;
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  extern struct kmem_cache *x86_fpu_cache;
  
  #define __KVM_HAVE_ARCH_VM_ALLOC
  static inline struct kvm *kvm_arch_alloc_vm(void)
  {
++<<<<<<< HEAD
 +	return kvm_x86_ops->vm_alloc();
 +}
 +
 +static inline void kvm_arch_free_vm(struct kvm *kvm)
 +{
 +	return kvm_x86_ops->vm_free(kvm);
++=======
+ 	return __vmalloc(kvm_x86_ops.vm_size,
+ 			 GFP_KERNEL_ACCOUNT | __GFP_ZERO, PAGE_KERNEL);
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  }
 -void kvm_arch_free_vm(struct kvm *kvm);
  
  #define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLB
  static inline int kvm_arch_flush_remote_tlb(struct kvm *kvm)
diff --cc arch/x86/kvm/mmu.h
index a647601c9e1c,8a3b1bce722a..000000000000
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@@ -95,11 -95,11 +95,16 @@@ static inline unsigned long kvm_get_act
  	return kvm_get_pcid(vcpu, kvm_read_cr3(vcpu));
  }
  
 -static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 +static inline void kvm_mmu_load_cr3(struct kvm_vcpu *vcpu)
  {
  	if (VALID_PAGE(vcpu->arch.mmu->root_hpa))
++<<<<<<< HEAD
 +		vcpu->arch.mmu->set_cr3(vcpu, vcpu->arch.mmu->root_hpa |
 +					      kvm_get_active_pcid(vcpu));
++=======
+ 		kvm_x86_ops.load_mmu_pgd(vcpu, vcpu->arch.mmu->root_hpa |
+ 					       kvm_get_active_pcid(vcpu));
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  }
  
  int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
diff --cc arch/x86/kvm/mmu/mmu.c
index 7b990f5b5c82,8071952e9cf2..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -4987,9 -4930,8 +4987,9 @@@ static void init_kvm_tdp_mmu(struct kvm
  	context->sync_page = nonpaging_sync_page;
  	context->invlpg = nonpaging_invlpg;
  	context->update_pte = nonpaging_update_pte;
- 	context->shadow_root_level = kvm_x86_ops->get_tdp_level(vcpu);
+ 	context->shadow_root_level = kvm_x86_ops.get_tdp_level(vcpu);
  	context->direct_map = true;
 +	context->set_cr3 = kvm_x86_ops->set_tdp_cr3;
  	context->get_guest_pgd = get_cr3;
  	context->get_pdptr = kvm_pdptr_read;
  	context->inject_page_fault = kvm_inject_page_fault;
@@@ -5241,8 -5182,8 +5241,13 @@@ int kvm_mmu_load(struct kvm_vcpu *vcpu
  	kvm_mmu_sync_roots(vcpu);
  	if (r)
  		goto out;
++<<<<<<< HEAD
 +	kvm_mmu_load_cr3(vcpu);
 +	kvm_x86_ops->tlb_flush(vcpu, true);
++=======
+ 	kvm_mmu_load_pgd(vcpu);
+ 	kvm_x86_ops.tlb_flush(vcpu, true);
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  out:
  	return r;
  }
diff --cc arch/x86/kvm/trace.h
index 7c741a0c5f80,aa59e1697bb3..000000000000
--- a/arch/x86/kvm/trace.h
+++ b/arch/x86/kvm/trace.h
@@@ -744,14 -750,14 +744,21 @@@ TRACE_EVENT(kvm_emulate_insn
  		),
  
  	TP_fast_assign(
++<<<<<<< HEAD
 +		__entry->csbase = kvm_x86_ops->get_segment_base(vcpu, VCPU_SREG_CS);
 +		__entry->len = vcpu->arch.emulate_ctxt.fetch.ptr
 +			       - vcpu->arch.emulate_ctxt.fetch.data;
 +		__entry->rip = vcpu->arch.emulate_ctxt._eip - __entry->len;
++=======
+ 		__entry->csbase = kvm_x86_ops.get_segment_base(vcpu, VCPU_SREG_CS);
+ 		__entry->len = vcpu->arch.emulate_ctxt->fetch.ptr
+ 			       - vcpu->arch.emulate_ctxt->fetch.data;
+ 		__entry->rip = vcpu->arch.emulate_ctxt->_eip - __entry->len;
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  		memcpy(__entry->insn,
 -		       vcpu->arch.emulate_ctxt->fetch.data,
 +		       vcpu->arch.emulate_ctxt.fetch.data,
  		       15);
 -		__entry->flags = kei_decode_mode(vcpu->arch.emulate_ctxt->mode);
 +		__entry->flags = kei_decode_mode(vcpu->arch.emulate_ctxt.mode);
  		__entry->failed = failed;
  		),
  
diff --cc arch/x86/kvm/x86.c
index 6c9a4f53f4db,f055a79f93b0..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -5208,26 -5264,20 +5208,31 @@@ static void kvm_init_msr_list(void
  			break;
  		}
  
 -		msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
 +		if (j < i)
 +			msrs_to_save[j] = msrs_to_save[i];
 +		j++;
  	}
 +	num_msrs_to_save = j;
  
++<<<<<<< HEAD
 +	for (i = j = 0; i < ARRAY_SIZE(emulated_msrs); i++) {
 +		if (!kvm_x86_ops->has_emulated_msr(emulated_msrs[i]))
++=======
+ 	for (i = 0; i < ARRAY_SIZE(emulated_msrs_all); i++) {
+ 		if (!kvm_x86_ops.has_emulated_msr(emulated_msrs_all[i]))
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  			continue;
  
 -		emulated_msrs[num_emulated_msrs++] = emulated_msrs_all[i];
 +		if (j < i)
 +			emulated_msrs[j] = emulated_msrs[i];
 +		j++;
  	}
 +	num_emulated_msrs = j;
  
 -	for (i = 0; i < ARRAY_SIZE(msr_based_features_all); i++) {
 +	for (i = j = 0; i < ARRAY_SIZE(msr_based_features); i++) {
  		struct kvm_msr_entry msr;
  
 -		msr.index = msr_based_features_all[i];
 +		msr.index = msr_based_features[i];
  		if (kvm_get_msr_feature(&msr))
  			continue;
  
@@@ -6193,7 -6241,8 +6198,12 @@@ static int emulator_intercept(struct x8
  			      struct x86_instruction_info *info,
  			      enum x86_intercept_stage stage)
  {
++<<<<<<< HEAD
 +	return kvm_x86_ops->check_intercept(emul_to_vcpu(ctxt), info, stage);
++=======
+ 	return kvm_x86_ops.check_intercept(emul_to_vcpu(ctxt), info, stage,
+ 					    &ctxt->exception);
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  }
  
  static bool emulator_get_cpuid(struct x86_emulate_ctxt *ctxt,
@@@ -6338,12 -6388,29 +6348,12 @@@ static bool inject_emulated_exception(s
  	return false;
  }
  
 -static struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)
 -{
 -	struct x86_emulate_ctxt *ctxt;
 -
 -	ctxt = kmem_cache_zalloc(x86_emulator_cache, GFP_KERNEL_ACCOUNT);
 -	if (!ctxt) {
 -		pr_err("kvm: failed to allocate vcpu's emulator\n");
 -		return NULL;
 -	}
 -
 -	ctxt->vcpu = vcpu;
 -	ctxt->ops = &emulate_ops;
 -	vcpu->arch.emulate_ctxt = ctxt;
 -
 -	return ctxt;
 -}
 -
  static void init_emulate_ctxt(struct kvm_vcpu *vcpu)
  {
 -	struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
 +	struct x86_emulate_ctxt *ctxt = &vcpu->arch.emulate_ctxt;
  	int cs_db, cs_l;
  
- 	kvm_x86_ops->get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
+ 	kvm_x86_ops.get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
  
  	ctxt->gpa_available = false;
  	ctxt->eflags = kvm_get_rflags(vcpu);
@@@ -7238,10 -7303,10 +7248,10 @@@ static struct notifier_block pvclock_gt
  
  int kvm_arch_init(void *opaque)
  {
 -	struct kvm_x86_init_ops *ops = opaque;
  	int r;
 +	struct kvm_x86_ops *ops = opaque;
  
- 	if (kvm_x86_ops) {
+ 	if (kvm_x86_ops.hardware_enable) {
  		printk(KERN_ERR "kvm: already loaded the other module\n");
  		r = -EEXIST;
  		goto out;
@@@ -7955,6 -8032,54 +7965,57 @@@ void kvm_make_scan_ioapic_request(struc
  	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
  }
  
++<<<<<<< HEAD
++=======
+ void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
+ {
+ 	if (!lapic_in_kernel(vcpu))
+ 		return;
+ 
+ 	vcpu->arch.apicv_active = kvm_apicv_activated(vcpu->kvm);
+ 	kvm_apic_update_apicv(vcpu);
+ 	kvm_x86_ops.refresh_apicv_exec_ctrl(vcpu);
+ }
+ EXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);
+ 
+ /*
+  * NOTE: Do not hold any lock prior to calling this.
+  *
+  * In particular, kvm_request_apicv_update() expects kvm->srcu not to be
+  * locked, because it calls __x86_set_memory_region() which does
+  * synchronize_srcu(&kvm->srcu).
+  */
+ void kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
+ {
+ 	unsigned long old, new, expected;
+ 
+ 	if (!kvm_x86_ops.check_apicv_inhibit_reasons ||
+ 	    !kvm_x86_ops.check_apicv_inhibit_reasons(bit))
+ 		return;
+ 
+ 	old = READ_ONCE(kvm->arch.apicv_inhibit_reasons);
+ 	do {
+ 		expected = new = old;
+ 		if (activate)
+ 			__clear_bit(bit, &new);
+ 		else
+ 			__set_bit(bit, &new);
+ 		if (new == old)
+ 			break;
+ 		old = cmpxchg(&kvm->arch.apicv_inhibit_reasons, expected, new);
+ 	} while (old != expected);
+ 
+ 	if (!!old == !!new)
+ 		return;
+ 
+ 	trace_kvm_apicv_update_request(activate, bit);
+ 	if (kvm_x86_ops.pre_update_apicv_exec_ctrl)
+ 		kvm_x86_ops.pre_update_apicv_exec_ctrl(kvm, activate);
+ 	kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+ }
+ EXPORT_SYMBOL_GPL(kvm_request_apicv_update);
+ 
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
  {
  	if (!kvm_apic_present(vcpu))
@@@ -7986,11 -8111,12 +8047,11 @@@ static void vcpu_load_eoi_exitmap(struc
  
  	bitmap_or((ulong *)eoi_exit_bitmap, vcpu->arch.ioapic_handled_vectors,
  		  vcpu_to_synic(vcpu)->vec_bitmap, 256);
- 	kvm_x86_ops->load_eoi_exitmap(vcpu, eoi_exit_bitmap);
+ 	kvm_x86_ops.load_eoi_exitmap(vcpu, eoi_exit_bitmap);
  }
  
 -int kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
 -		unsigned long start, unsigned long end,
 -		bool blockable)
 +void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
 +		unsigned long start, unsigned long end)
  {
  	unsigned long apic_address;
  
@@@ -9145,36 -9275,93 +9206,104 @@@ static void fx_init(struct kvm_vcpu *vc
  	vcpu->arch.cr0 |= X86_CR0_ET;
  }
  
 -int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 +void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
  {
 -	if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
 -		pr_warn_once("kvm: SMP vm created on host with unstable TSC; "
 -			     "guest TSC will not be reliable\n");
 +	void *wbinvd_dirty_mask = vcpu->arch.wbinvd_dirty_mask;
 +	struct gfn_to_pfn_cache *cache = &vcpu->arch.st.cache;
  
 -	return 0;
 +	kvm_release_pfn(cache->pfn, cache->dirty, cache);
 +
 +	kvmclock_reset(vcpu);
 +
 +	kvm_x86_ops->vcpu_free(vcpu);
 +	free_cpumask_var(wbinvd_dirty_mask);
  }
  
 -int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 +struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
 +						unsigned int id)
  {
 -	struct page *page;
 -	int r;
 +	struct kvm_vcpu *vcpu;
  
 -	if (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu))
 -		vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
 -	else
 -		vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
 +	if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
 +		printk_once(KERN_WARNING
 +		"kvm: SMP vm created on host with unstable TSC; "
 +		"guest TSC will not be reliable\n");
  
 -	kvm_set_tsc_khz(vcpu, max_tsc_khz);
 +	vcpu = kvm_x86_ops->vcpu_create(kvm, id);
  
++<<<<<<< HEAD
 +	return vcpu;
 +}
++=======
+ 	r = kvm_mmu_create(vcpu);
+ 	if (r < 0)
+ 		return r;
+ 
+ 	if (irqchip_in_kernel(vcpu->kvm)) {
+ 		r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ 		if (r < 0)
+ 			goto fail_mmu_destroy;
+ 		if (kvm_apicv_activated(vcpu->kvm))
+ 			vcpu->arch.apicv_active = true;
+ 	} else
+ 		static_key_slow_inc(&kvm_no_apic_vcpu);
+ 
+ 	r = -ENOMEM;
+ 
+ 	page = alloc_page(GFP_KERNEL | __GFP_ZERO);
+ 	if (!page)
+ 		goto fail_free_lapic;
+ 	vcpu->arch.pio_data = page_address(page);
+ 
+ 	vcpu->arch.mce_banks = kzalloc(KVM_MAX_MCE_BANKS * sizeof(u64) * 4,
+ 				       GFP_KERNEL_ACCOUNT);
+ 	if (!vcpu->arch.mce_banks)
+ 		goto fail_free_pio_data;
+ 	vcpu->arch.mcg_cap = KVM_MAX_MCE_BANKS;
+ 
+ 	if (!zalloc_cpumask_var(&vcpu->arch.wbinvd_dirty_mask,
+ 				GFP_KERNEL_ACCOUNT))
+ 		goto fail_free_mce_banks;
+ 
+ 	if (!alloc_emulate_ctxt(vcpu))
+ 		goto free_wbinvd_dirty_mask;
+ 
+ 	vcpu->arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,
+ 						GFP_KERNEL_ACCOUNT);
+ 	if (!vcpu->arch.user_fpu) {
+ 		pr_err("kvm: failed to allocate userspace's fpu\n");
+ 		goto free_emulate_ctxt;
+ 	}
+ 
+ 	vcpu->arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,
+ 						 GFP_KERNEL_ACCOUNT);
+ 	if (!vcpu->arch.guest_fpu) {
+ 		pr_err("kvm: failed to allocate vcpu's fpu\n");
+ 		goto free_user_fpu;
+ 	}
+ 	fx_init(vcpu);
+ 
+ 	vcpu->arch.guest_xstate_size = XSAVE_HDR_SIZE + XSAVE_HDR_OFFSET;
+ 
+ 	vcpu->arch.maxphyaddr = cpuid_query_maxphyaddr(vcpu);
+ 
+ 	vcpu->arch.pat = MSR_IA32_CR_PAT_DEFAULT;
+ 
+ 	kvm_async_pf_hash_reset(vcpu);
+ 	kvm_pmu_init(vcpu);
+ 
+ 	vcpu->arch.pending_external_vector = -1;
+ 	vcpu->arch.preempted_in_kernel = false;
+ 
+ 	kvm_hv_vcpu_init(vcpu);
+ 
+ 	r = kvm_x86_ops.vcpu_create(vcpu);
+ 	if (r)
+ 		goto free_guest_fpu;
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  
 +int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
 +{
  	vcpu->arch.arch_capabilities = kvm_get_arch_capabilities();
  	vcpu->arch.msr_platform_info = MSR_PLATFORM_INFO_CPUID_FAULT;
  	kvm_vcpu_mtrr_init(vcpu);
@@@ -9213,7 -9418,30 +9342,34 @@@ void kvm_arch_vcpu_postcreate(struct kv
  
  void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
  {
++<<<<<<< HEAD
 +	kvm_arch_vcpu_free(vcpu);
++=======
+ 	struct gfn_to_pfn_cache *cache = &vcpu->arch.st.cache;
+ 	int idx;
+ 
+ 	kvm_release_pfn(cache->pfn, cache->dirty, cache);
+ 
+ 	kvmclock_reset(vcpu);
+ 
+ 	kvm_x86_ops.vcpu_free(vcpu);
+ 
+ 	kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+ 	free_cpumask_var(vcpu->arch.wbinvd_dirty_mask);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);
+ 	kmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);
+ 
+ 	kvm_hv_vcpu_uninit(vcpu);
+ 	kvm_pmu_destroy(vcpu);
+ 	kfree(vcpu->arch.mce_banks);
+ 	kvm_free_lapic(vcpu);
+ 	idx = srcu_read_lock(&vcpu->kvm->srcu);
+ 	kvm_mmu_destroy(vcpu);
+ 	srcu_read_unlock(&vcpu->kvm->srcu, idx);
+ 	free_page((unsigned long)vcpu->arch.pio_data);
+ 	if (!lapic_in_kernel(vcpu))
+ 		static_key_slow_dec(&kvm_no_apic_vcpu);
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  }
  
  void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
@@@ -9404,6 -9638,11 +9560,14 @@@ int kvm_arch_hardware_setup(void *opaqu
  	if (r != 0)
  		return r;
  
++<<<<<<< HEAD
++=======
+ 	memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
+ 
+ 	if (!kvm_cpu_cap_has(X86_FEATURE_XSAVES))
+ 		supported_xss = 0;
+ 
++>>>>>>> afaf0b2f9b80 (KVM: x86: Copy kvm_x86_ops by value to eliminate layer of indirection)
  	cr4_reserved_bits = kvm_host_cr4_reserved_bits(&boot_cpu_data);
  
  	if (kvm_has_tsc_control) {
@@@ -9559,9 -9704,16 +9723,9 @@@ void kvm_arch_sched_in(struct kvm_vcpu 
  		pmu->need_cleanup = true;
  		kvm_make_request(KVM_REQ_PMU, vcpu);
  	}
- 	kvm_x86_ops->sched_in(vcpu, cpu);
+ 	kvm_x86_ops.sched_in(vcpu, cpu);
  }
  
 -void kvm_arch_free_vm(struct kvm *kvm)
 -{
 -	kfree(kvm->arch.hyperv.hv_pa_pg);
 -	vfree(kvm);
 -}
 -
 -
  int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
  {
  	if (type)
* Unmerged path arch/x86/include/asm/kvm_host.h
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 78d6db1ce0f2..b98e79115274 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -236,7 +236,7 @@ int kvm_vcpu_ioctl_set_cpuid(struct kvm_vcpu *vcpu,
 	vcpu->arch.cpuid_nent = cpuid->nent;
 	cpuid_fix_nx_cap(vcpu);
 	kvm_apic_set_version(vcpu);
-	kvm_x86_ops->cpuid_update(vcpu);
+	kvm_x86_ops.cpuid_update(vcpu);
 	r = kvm_update_cpuid(vcpu);
 
 out:
@@ -259,7 +259,7 @@ int kvm_vcpu_ioctl_set_cpuid2(struct kvm_vcpu *vcpu,
 		goto out;
 	vcpu->arch.cpuid_nent = cpuid->nent;
 	kvm_apic_set_version(vcpu);
-	kvm_x86_ops->cpuid_update(vcpu);
+	kvm_x86_ops.cpuid_update(vcpu);
 	r = kvm_update_cpuid(vcpu);
 out:
 	return r;
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 1d09e7ed5d6f..0ee2a9973001 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -1024,7 +1024,7 @@ static int kvm_hv_set_msr_pw(struct kvm_vcpu *vcpu, u32 msr, u64 data,
 		addr = gfn_to_hva(kvm, gfn);
 		if (kvm_is_error_hva(addr))
 			return 1;
-		kvm_x86_ops->patch_hypercall(vcpu, instructions);
+		kvm_x86_ops.patch_hypercall(vcpu, instructions);
 		((unsigned char *)instructions)[3] = 0xc3; /* ret */
 		if (__copy_to_user((void __user *)addr, instructions, 4))
 			return 1;
@@ -1609,7 +1609,7 @@ int kvm_hv_hypercall(struct kvm_vcpu *vcpu)
 	 * hypercall generates UD from non zero cpl and real mode
 	 * per HYPER-V spec
 	 */
-	if (kvm_x86_ops->get_cpl(vcpu) != 0 || !is_protmode(vcpu)) {
+	if (kvm_x86_ops.get_cpl(vcpu) != 0 || !is_protmode(vcpu)) {
 		kvm_queue_exception(vcpu, UD_VECTOR);
 		return 1;
 	}
@@ -1802,8 +1802,8 @@ int kvm_vcpu_ioctl_get_hv_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid2 *cpuid,
 	};
 	int i, nent = ARRAY_SIZE(cpuid_entries);
 
-	if (kvm_x86_ops->nested_get_evmcs_version)
-		evmcs_ver = kvm_x86_ops->nested_get_evmcs_version(vcpu);
+	if (kvm_x86_ops.nested_get_evmcs_version)
+		evmcs_ver = kvm_x86_ops.nested_get_evmcs_version(vcpu);
 
 	/* Skip NESTED_FEATURES if eVMCS is not supported */
 	if (!evmcs_ver)
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 58767020de41..62558b9bdda7 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -68,7 +68,7 @@ static inline unsigned long kvm_register_read(struct kvm_vcpu *vcpu, int reg)
 		return 0;
 
 	if (!kvm_register_is_available(vcpu, reg))
-		kvm_x86_ops->cache_reg(vcpu, reg);
+		kvm_x86_ops.cache_reg(vcpu, reg);
 
 	return vcpu->arch.regs[reg];
 }
@@ -108,7 +108,7 @@ static inline u64 kvm_pdptr_read(struct kvm_vcpu *vcpu, int index)
 	might_sleep();  /* on svm */
 
 	if (!kvm_register_is_available(vcpu, VCPU_EXREG_PDPTR))
-		kvm_x86_ops->cache_reg(vcpu, VCPU_EXREG_PDPTR);
+		kvm_x86_ops.cache_reg(vcpu, VCPU_EXREG_PDPTR);
 
 	return vcpu->arch.walk_mmu->pdptrs[index];
 }
@@ -117,7 +117,7 @@ static inline ulong kvm_read_cr0_bits(struct kvm_vcpu *vcpu, ulong mask)
 {
 	ulong tmask = mask & KVM_POSSIBLE_CR0_GUEST_BITS;
 	if (tmask & vcpu->arch.cr0_guest_owned_bits)
-		kvm_x86_ops->decache_cr0_guest_bits(vcpu);
+		kvm_x86_ops.decache_cr0_guest_bits(vcpu);
 	return vcpu->arch.cr0 & mask;
 }
 
@@ -130,14 +130,14 @@ static inline ulong kvm_read_cr4_bits(struct kvm_vcpu *vcpu, ulong mask)
 {
 	ulong tmask = mask & KVM_POSSIBLE_CR4_GUEST_BITS;
 	if (tmask & vcpu->arch.cr4_guest_owned_bits)
-		kvm_x86_ops->decache_cr4_guest_bits(vcpu);
+		kvm_x86_ops.decache_cr4_guest_bits(vcpu);
 	return vcpu->arch.cr4 & mask;
 }
 
 static inline ulong kvm_read_cr3(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))
-		kvm_x86_ops->cache_reg(vcpu, VCPU_EXREG_CR3);
+		kvm_x86_ops.cache_reg(vcpu, VCPU_EXREG_CR3);
 	return vcpu->arch.cr3;
 }
 
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index a040ee181b5f..ca8ccc0bd09a 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -465,7 +465,7 @@ static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 	if (unlikely(vcpu->arch.apicv_active)) {
 		/* need to update RVI */
 		kvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);
-		kvm_x86_ops->hwapic_irr_update(vcpu,
+		kvm_x86_ops.hwapic_irr_update(vcpu,
 				apic_find_highest_irr(apic));
 	} else {
 		apic->irr_pending = false;
@@ -490,7 +490,7 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	 * just set SVI.
 	 */
 	if (unlikely(vcpu->arch.apicv_active))
-		kvm_x86_ops->hwapic_isr_update(vcpu, vec);
+		kvm_x86_ops.hwapic_isr_update(vcpu, vec);
 	else {
 		++apic->isr_count;
 		BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
@@ -538,7 +538,7 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	 * and must be left alone.
 	 */
 	if (unlikely(vcpu->arch.apicv_active))
-		kvm_x86_ops->hwapic_isr_update(vcpu,
+		kvm_x86_ops.hwapic_isr_update(vcpu,
 					       apic_find_highest_isr(apic));
 	else {
 		--apic->isr_count;
@@ -676,7 +676,7 @@ static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
 	if (apic->vcpu->arch.apicv_active)
-		highest_irr = kvm_x86_ops->sync_pir_to_irr(apic->vcpu);
+		highest_irr = kvm_x86_ops.sync_pir_to_irr(apic->vcpu);
 	else
 		highest_irr = apic_find_highest_irr(apic);
 	if (highest_irr == -1 || (highest_irr & 0xF0) <= ppr)
@@ -1065,7 +1065,7 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
-		if (kvm_x86_ops->deliver_posted_interrupt(vcpu, vector)) {
+		if (kvm_x86_ops.deliver_posted_interrupt(vcpu, vector)) {
 			kvm_lapic_set_irr(vector, apic);
 			kvm_make_request(KVM_REQ_EVENT, vcpu);
 			kvm_vcpu_kick(vcpu);
@@ -1748,7 +1748,7 @@ static void cancel_hv_timer(struct kvm_lapic *apic)
 {
 	WARN_ON(preemptible());
 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
-	kvm_x86_ops->cancel_hv_timer(apic->vcpu);
+	kvm_x86_ops.cancel_hv_timer(apic->vcpu);
 	apic->lapic_timer.hv_timer_in_use = false;
 }
 
@@ -1759,13 +1759,13 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	bool expired;
 
 	WARN_ON(preemptible());
-	if (!kvm_x86_ops->set_hv_timer)
+	if (!kvm_x86_ops.set_hv_timer)
 		return false;
 
 	if (!ktimer->tscdeadline)
 		return false;
 
-	if (kvm_x86_ops->set_hv_timer(vcpu, ktimer->tscdeadline, &expired))
+	if (kvm_x86_ops.set_hv_timer(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
 	ktimer->hv_timer_in_use = true;
@@ -2192,7 +2192,7 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 		kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
 
 	if ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE))
-		kvm_x86_ops->set_virtual_apic_mode(vcpu);
+		kvm_x86_ops.set_virtual_apic_mode(vcpu);
 
 	apic->base_address = apic->vcpu->arch.apic_base &
 			     MSR_IA32_APICBASE_BASE;
@@ -2270,9 +2270,9 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 	vcpu->arch.pv_eoi.msr_val = 0;
 	apic_update_ppr(apic);
 	if (vcpu->arch.apicv_active) {
-		kvm_x86_ops->apicv_post_state_restore(vcpu);
-		kvm_x86_ops->hwapic_irr_update(vcpu, -1);
-		kvm_x86_ops->hwapic_isr_update(vcpu, -1);
+		kvm_x86_ops.apicv_post_state_restore(vcpu);
+		kvm_x86_ops.hwapic_irr_update(vcpu, -1);
+		kvm_x86_ops.hwapic_isr_update(vcpu, -1);
 	}
 
 	vcpu->arch.apic_arb_prio = 0;
@@ -2523,10 +2523,10 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	kvm_apic_update_apicv(vcpu);
 	apic->highest_isr_cache = -1;
 	if (vcpu->arch.apicv_active) {
-		kvm_x86_ops->apicv_post_state_restore(vcpu);
-		kvm_x86_ops->hwapic_irr_update(vcpu,
+		kvm_x86_ops.apicv_post_state_restore(vcpu);
+		kvm_x86_ops.hwapic_irr_update(vcpu,
 				apic_find_highest_irr(apic));
-		kvm_x86_ops->hwapic_isr_update(vcpu,
+		kvm_x86_ops.hwapic_isr_update(vcpu,
 				apic_find_highest_isr(apic));
 	}
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
* Unmerged path arch/x86/kvm/mmu.h
* Unmerged path arch/x86/kvm/mmu/mmu.c
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index b520d237dbf9..80044831e797 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -214,7 +214,7 @@ void reprogram_gp_counter(struct kvm_pmc *pmc, u64 eventsel)
 			  ARCH_PERFMON_EVENTSEL_CMASK |
 			  HSW_IN_TX |
 			  HSW_IN_TX_CHECKPOINTED))) {
-		config = kvm_x86_ops->pmu_ops->find_arch_event(pmc_to_pmu(pmc),
+		config = kvm_x86_ops.pmu_ops->find_arch_event(pmc_to_pmu(pmc),
 						      event_select,
 						      unit_mask);
 		if (config != PERF_COUNT_HW_MAX)
@@ -268,7 +268,7 @@ void reprogram_fixed_counter(struct kvm_pmc *pmc, u8 ctrl, int idx)
 
 	pmc->current_config = (u64)ctrl;
 	pmc_reprogram_counter(pmc, PERF_TYPE_HARDWARE,
-			      kvm_x86_ops->pmu_ops->find_fixed_event(idx),
+			      kvm_x86_ops.pmu_ops->find_fixed_event(idx),
 			      !(en_field & 0x2), /* exclude user */
 			      !(en_field & 0x1), /* exclude kernel */
 			      pmi, false, false);
@@ -277,7 +277,7 @@ EXPORT_SYMBOL_GPL(reprogram_fixed_counter);
 
 void reprogram_counter(struct kvm_pmu *pmu, int pmc_idx)
 {
-	struct kvm_pmc *pmc = kvm_x86_ops->pmu_ops->pmc_idx_to_pmc(pmu, pmc_idx);
+	struct kvm_pmc *pmc = kvm_x86_ops.pmu_ops->pmc_idx_to_pmc(pmu, pmc_idx);
 
 	if (!pmc)
 		return;
@@ -299,7 +299,7 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 	int bit;
 
 	for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
-		struct kvm_pmc *pmc = kvm_x86_ops->pmu_ops->pmc_idx_to_pmc(pmu, bit);
+		struct kvm_pmc *pmc = kvm_x86_ops.pmu_ops->pmc_idx_to_pmc(pmu, bit);
 
 		if (unlikely(!pmc || !pmc->perf_event)) {
 			clear_bit(bit, pmu->reprogram_pmi);
@@ -321,7 +321,7 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 /* check if idx is a valid index to access PMU */
 int kvm_pmu_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
-	return kvm_x86_ops->pmu_ops->is_valid_rdpmc_ecx(vcpu, idx);
+	return kvm_x86_ops.pmu_ops->is_valid_rdpmc_ecx(vcpu, idx);
 }
 
 bool is_vmware_backdoor_pmc(u32 pmc_idx)
@@ -371,7 +371,7 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	if (is_vmware_backdoor_pmc(idx))
 		return kvm_pmu_rdpmc_vmware(vcpu, idx, data);
 
-	pmc = kvm_x86_ops->pmu_ops->rdpmc_ecx_to_pmc(vcpu, idx, &mask);
+	pmc = kvm_x86_ops.pmu_ops->rdpmc_ecx_to_pmc(vcpu, idx, &mask);
 	if (!pmc)
 		return 1;
 
@@ -387,14 +387,14 @@ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 
 bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
-	return kvm_x86_ops->pmu_ops->msr_idx_to_pmc(vcpu, msr) ||
-		kvm_x86_ops->pmu_ops->is_valid_msr(vcpu, msr);
+	return kvm_x86_ops.pmu_ops->msr_idx_to_pmc(vcpu, msr) ||
+		kvm_x86_ops.pmu_ops->is_valid_msr(vcpu, msr);
 }
 
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
-	struct kvm_pmc *pmc = kvm_x86_ops->pmu_ops->msr_idx_to_pmc(vcpu, msr);
+	struct kvm_pmc *pmc = kvm_x86_ops.pmu_ops->msr_idx_to_pmc(vcpu, msr);
 
 	if (pmc)
 		__set_bit(pmc->idx, pmu->pmc_in_use);
@@ -402,13 +402,13 @@ static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 {
-	return kvm_x86_ops->pmu_ops->get_msr(vcpu, msr, data);
+	return kvm_x86_ops.pmu_ops->get_msr(vcpu, msr, data);
 }
 
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
-	return kvm_x86_ops->pmu_ops->set_msr(vcpu, msr_info);
+	return kvm_x86_ops.pmu_ops->set_msr(vcpu, msr_info);
 }
 
 /* refresh PMU settings. This function generally is called when underlying
@@ -417,7 +417,7 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
-	kvm_x86_ops->pmu_ops->refresh(vcpu);
+	kvm_x86_ops.pmu_ops->refresh(vcpu);
 }
 
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
@@ -425,7 +425,7 @@ void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
 	irq_work_sync(&pmu->irq_work);
-	kvm_x86_ops->pmu_ops->reset(vcpu);
+	kvm_x86_ops.pmu_ops->reset(vcpu);
 }
 
 void kvm_pmu_init(struct kvm_vcpu *vcpu)
@@ -433,7 +433,7 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
 	memset(pmu, 0, sizeof(*pmu));
-	kvm_x86_ops->pmu_ops->init(vcpu);
+	kvm_x86_ops.pmu_ops->init(vcpu);
 	init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
 	pmu->event_count = 0;
 	pmu->need_cleanup = false;
@@ -465,7 +465,7 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 		      pmu->pmc_in_use, X86_PMC_IDX_MAX);
 
 	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {
-		pmc = kvm_x86_ops->pmu_ops->pmc_idx_to_pmc(pmu, i);
+		pmc = kvm_x86_ops.pmu_ops->pmc_idx_to_pmc(pmu, i);
 
 		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
 			pmc_stop_counter(pmc);
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index d7da2b9e0755..a6c78a797cb1 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -88,7 +88,7 @@ static inline bool pmc_is_fixed(struct kvm_pmc *pmc)
 
 static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
 {
-	return kvm_x86_ops->pmu_ops->pmc_is_enabled(pmc);
+	return kvm_x86_ops.pmu_ops->pmc_is_enabled(pmc);
 }
 
 static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index dd22d2cd361d..7d474c72b014 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -7410,7 +7410,7 @@ static bool svm_apic_init_signal_blocked(struct kvm_vcpu *vcpu)
 	 * TODO: Last condition latch INIT signals on vCPU when
 	 * vCPU is in guest-mode and vmcb12 defines intercept on INIT.
 	 * To properly emulate the INIT intercept, SVM should implement
-	 * kvm_x86_ops->check_nested_events() and call nested_svm_vmexit()
+	 * kvm_x86_ops.check_nested_events() and call nested_svm_vmexit()
 	 * there if an INIT signal is pending.
 	 */
 	return !gif_set(svm) ||
* Unmerged path arch/x86/kvm/trace.h
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 2632df1c1f2b..f5dc229499ec 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -4536,7 +4536,7 @@ void nested_vmx_pmu_entry_exit_ctls_update(struct kvm_vcpu *vcpu)
 		return;
 
 	vmx = to_vmx(vcpu);
-	if (kvm_x86_ops->pmu_ops->is_valid_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL)) {
+	if (kvm_x86_ops.pmu_ops->is_valid_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL)) {
 		vmx->nested.msrs.entry_ctls_high |=
 				VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;
 		vmx->nested.msrs.exit_ctls_high |=
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 833b3f7d7961..0e6db08d42eb 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -3089,7 +3089,7 @@ void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 		eptp = construct_eptp(vcpu, cr3);
 		vmcs_write64(EPT_POINTER, eptp);
 
-		if (kvm_x86_ops->tlb_remote_flush) {
+		if (kvm_x86_ops.tlb_remote_flush) {
 			spin_lock(&to_kvm_vmx(kvm)->ept_pointer_lock);
 			to_vmx(vcpu)->ept_pointer = eptp;
 			to_kvm_vmx(kvm)->ept_pointers_match
@@ -7540,7 +7540,7 @@ static void pi_post_block(struct kvm_vcpu *vcpu)
 
 static void vmx_post_block(struct kvm_vcpu *vcpu)
 {
-	if (kvm_x86_ops->set_hv_timer)
+	if (kvm_x86_ops.set_hv_timer)
 		kvm_lapic_switch_to_hv_timer(vcpu);
 
 	pi_post_block(vcpu);
* Unmerged path arch/x86/kvm/x86.c
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 916e17e33414..0b5ae232ce83 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -97,7 +97,7 @@ static inline bool is_64_bit_mode(struct kvm_vcpu *vcpu)
 
 	if (!is_long_mode(vcpu))
 		return false;
-	kvm_x86_ops->get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
+	kvm_x86_ops.get_cs_db_l_bits(vcpu, &cs_db, &cs_l);
 	return cs_l;
 }
 
@@ -237,7 +237,7 @@ static inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)
 
 static inline bool kvm_vcpu_latch_init(struct kvm_vcpu *vcpu)
 {
-	return is_smm(vcpu) || kvm_x86_ops->apic_init_signal_blocked(vcpu);
+	return is_smm(vcpu) || kvm_x86_ops.apic_init_signal_blocked(vcpu);
 }
 
 void kvm_set_pending_timer(struct kvm_vcpu *vcpu);
