KVM: nVMX: Migrate the VMX-preemption timer

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jim Mattson <jmattson@google.com>
commit 93dff2fed2fb4a513196b7df05742c6fcdfd5178
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/93dff2fe.failed

The hrtimer used to emulate the VMX-preemption timer must be pinned to
the same logical processor as the vCPU thread to be interrupted if we
want to have any hope of adhering to the architectural specification
of the VMX-preemption timer. Even with this change, the emulated
VMX-preemption timer VM-exit occasionally arrives too late.

	Signed-off-by: Jim Mattson <jmattson@google.com>
	Reviewed-by: Peter Shier <pshier@google.com>
	Reviewed-by: Oliver Upton <oupton@google.com>
Message-Id: <20200508203643.85477-4-jmattson@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 93dff2fed2fb4a513196b7df05742c6fcdfd5178)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/vmx/vmx.c
index 539026efd9e1,6a03c27ff314..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -7723,6 -7814,162 +7723,165 @@@ static bool vmx_apic_init_signal_blocke
  	return to_vmx(vcpu)->nested.vmxon;
  }
  
++<<<<<<< HEAD
++=======
+ static void vmx_migrate_timers(struct kvm_vcpu *vcpu)
+ {
+ 	if (is_guest_mode(vcpu)) {
+ 		struct hrtimer *timer = &to_vmx(vcpu)->nested.preemption_timer;
+ 
+ 		if (hrtimer_try_to_cancel(timer) == 1)
+ 			hrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED);
+ 	}
+ }
+ 
+ static void hardware_unsetup(void)
+ {
+ 	if (nested)
+ 		nested_vmx_hardware_unsetup();
+ 
+ 	free_kvm_area();
+ }
+ 
+ static bool vmx_check_apicv_inhibit_reasons(ulong bit)
+ {
+ 	ulong supported = BIT(APICV_INHIBIT_REASON_DISABLE) |
+ 			  BIT(APICV_INHIBIT_REASON_HYPERV);
+ 
+ 	return supported & BIT(bit);
+ }
+ 
+ static struct kvm_x86_ops vmx_x86_ops __initdata = {
+ 	.hardware_unsetup = hardware_unsetup,
+ 
+ 	.hardware_enable = hardware_enable,
+ 	.hardware_disable = hardware_disable,
+ 	.cpu_has_accelerated_tpr = report_flexpriority,
+ 	.has_emulated_msr = vmx_has_emulated_msr,
+ 
+ 	.vm_size = sizeof(struct kvm_vmx),
+ 	.vm_init = vmx_vm_init,
+ 
+ 	.vcpu_create = vmx_create_vcpu,
+ 	.vcpu_free = vmx_free_vcpu,
+ 	.vcpu_reset = vmx_vcpu_reset,
+ 
+ 	.prepare_guest_switch = vmx_prepare_switch_to_guest,
+ 	.vcpu_load = vmx_vcpu_load,
+ 	.vcpu_put = vmx_vcpu_put,
+ 
+ 	.update_bp_intercept = update_exception_bitmap,
+ 	.get_msr_feature = vmx_get_msr_feature,
+ 	.get_msr = vmx_get_msr,
+ 	.set_msr = vmx_set_msr,
+ 	.get_segment_base = vmx_get_segment_base,
+ 	.get_segment = vmx_get_segment,
+ 	.set_segment = vmx_set_segment,
+ 	.get_cpl = vmx_get_cpl,
+ 	.get_cs_db_l_bits = vmx_get_cs_db_l_bits,
+ 	.set_cr0 = vmx_set_cr0,
+ 	.set_cr4 = vmx_set_cr4,
+ 	.set_efer = vmx_set_efer,
+ 	.get_idt = vmx_get_idt,
+ 	.set_idt = vmx_set_idt,
+ 	.get_gdt = vmx_get_gdt,
+ 	.set_gdt = vmx_set_gdt,
+ 	.set_dr7 = vmx_set_dr7,
+ 	.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs,
+ 	.cache_reg = vmx_cache_reg,
+ 	.get_rflags = vmx_get_rflags,
+ 	.set_rflags = vmx_set_rflags,
+ 
+ 	.tlb_flush_all = vmx_flush_tlb_all,
+ 	.tlb_flush_current = vmx_flush_tlb_current,
+ 	.tlb_flush_gva = vmx_flush_tlb_gva,
+ 	.tlb_flush_guest = vmx_flush_tlb_guest,
+ 
+ 	.run = vmx_vcpu_run,
+ 	.handle_exit = vmx_handle_exit,
+ 	.skip_emulated_instruction = vmx_skip_emulated_instruction,
+ 	.update_emulated_instruction = vmx_update_emulated_instruction,
+ 	.set_interrupt_shadow = vmx_set_interrupt_shadow,
+ 	.get_interrupt_shadow = vmx_get_interrupt_shadow,
+ 	.patch_hypercall = vmx_patch_hypercall,
+ 	.set_irq = vmx_inject_irq,
+ 	.set_nmi = vmx_inject_nmi,
+ 	.queue_exception = vmx_queue_exception,
+ 	.cancel_injection = vmx_cancel_injection,
+ 	.interrupt_allowed = vmx_interrupt_allowed,
+ 	.nmi_allowed = vmx_nmi_allowed,
+ 	.get_nmi_mask = vmx_get_nmi_mask,
+ 	.set_nmi_mask = vmx_set_nmi_mask,
+ 	.enable_nmi_window = enable_nmi_window,
+ 	.enable_irq_window = enable_irq_window,
+ 	.update_cr8_intercept = update_cr8_intercept,
+ 	.set_virtual_apic_mode = vmx_set_virtual_apic_mode,
+ 	.set_apic_access_page_addr = vmx_set_apic_access_page_addr,
+ 	.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl,
+ 	.load_eoi_exitmap = vmx_load_eoi_exitmap,
+ 	.apicv_post_state_restore = vmx_apicv_post_state_restore,
+ 	.check_apicv_inhibit_reasons = vmx_check_apicv_inhibit_reasons,
+ 	.hwapic_irr_update = vmx_hwapic_irr_update,
+ 	.hwapic_isr_update = vmx_hwapic_isr_update,
+ 	.guest_apic_has_interrupt = vmx_guest_apic_has_interrupt,
+ 	.sync_pir_to_irr = vmx_sync_pir_to_irr,
+ 	.deliver_posted_interrupt = vmx_deliver_posted_interrupt,
+ 	.dy_apicv_has_pending_interrupt = vmx_dy_apicv_has_pending_interrupt,
+ 
+ 	.set_tss_addr = vmx_set_tss_addr,
+ 	.set_identity_map_addr = vmx_set_identity_map_addr,
+ 	.get_tdp_level = vmx_get_tdp_level,
+ 	.get_mt_mask = vmx_get_mt_mask,
+ 
+ 	.get_exit_info = vmx_get_exit_info,
+ 
+ 	.cpuid_update = vmx_cpuid_update,
+ 
+ 	.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit,
+ 
+ 	.write_l1_tsc_offset = vmx_write_l1_tsc_offset,
+ 
+ 	.load_mmu_pgd = vmx_load_mmu_pgd,
+ 
+ 	.check_intercept = vmx_check_intercept,
+ 	.handle_exit_irqoff = vmx_handle_exit_irqoff,
+ 
+ 	.request_immediate_exit = vmx_request_immediate_exit,
+ 
+ 	.sched_in = vmx_sched_in,
+ 
+ 	.slot_enable_log_dirty = vmx_slot_enable_log_dirty,
+ 	.slot_disable_log_dirty = vmx_slot_disable_log_dirty,
+ 	.flush_log_dirty = vmx_flush_log_dirty,
+ 	.enable_log_dirty_pt_masked = vmx_enable_log_dirty_pt_masked,
+ 	.write_log_dirty = vmx_write_pml_buffer,
+ 
+ 	.pre_block = vmx_pre_block,
+ 	.post_block = vmx_post_block,
+ 
+ 	.pmu_ops = &intel_pmu_ops,
+ 	.nested_ops = &vmx_nested_ops,
+ 
+ 	.update_pi_irte = vmx_update_pi_irte,
+ 
+ #ifdef CONFIG_X86_64
+ 	.set_hv_timer = vmx_set_hv_timer,
+ 	.cancel_hv_timer = vmx_cancel_hv_timer,
+ #endif
+ 
+ 	.setup_mce = vmx_setup_mce,
+ 
+ 	.smi_allowed = vmx_smi_allowed,
+ 	.pre_enter_smm = vmx_pre_enter_smm,
+ 	.pre_leave_smm = vmx_pre_leave_smm,
+ 	.enable_smi_window = enable_smi_window,
+ 
+ 	.need_emulation_on_page_fault = vmx_need_emulation_on_page_fault,
+ 	.apic_init_signal_blocked = vmx_apic_init_signal_blocked,
+ 	.migrate_timers = vmx_migrate_timers,
+ };
+ 
++>>>>>>> 93dff2fed2fb (KVM: nVMX: Migrate the VMX-preemption timer)
  static __init int hardware_setup(void)
  {
  	unsigned long host_bndcfgs;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 64859dc07ab5..558581c65823 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1267,6 +1267,8 @@ struct kvm_x86_ops {
 
 	bool (*apic_init_signal_blocked)(struct kvm_vcpu *vcpu);
 	int (*enable_direct_tlbflush)(struct kvm_vcpu *vcpu);
+
+	void (*migrate_timers)(struct kvm_vcpu *vcpu);
 };
 
 struct kvm_arch_async_pf {
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index 007bc654f928..43fd05b49c11 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -171,6 +171,8 @@ void __kvm_migrate_timers(struct kvm_vcpu *vcpu)
 {
 	__kvm_migrate_apic_timer(vcpu);
 	__kvm_migrate_pit_timer(vcpu);
+	if (kvm_x86_ops.migrate_timers)
+		kvm_x86_ops.migrate_timers(vcpu);
 }
 
 bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
* Unmerged path arch/x86/kvm/vmx/vmx.c
