intel_idle: Convert to new X86 CPU match macros

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 4a9f45a0533f47bcff27761821ee568875c5aee4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/4a9f45a0.failed

The new macro set has a consistent namespace and uses C99 initializers
instead of the grufty C89 ones.

Get rid the of the local macro wrappers for consistency.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Link: https://lkml.kernel.org/r/20200320131510.193755545@linutronix.de
(cherry picked from commit 4a9f45a0533f47bcff27761821ee568875c5aee4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/idle/intel_idle.c
diff --cc drivers/idle/intel_idle.c
index 076189997131,88f6f926d967..000000000000
--- a/drivers/idle/intel_idle.c
+++ b/drivers/idle/intel_idle.c
@@@ -1067,108 -1067,208 +1067,183 @@@ static const struct idle_cpu idle_cpu_d
  };
  
  static const struct x86_cpu_id intel_idle_ids[] __initconst = {
++<<<<<<< HEAD
 +	INTEL_CPU_FAM6(NEHALEM_EP,		idle_cpu_nehalem),
 +	INTEL_CPU_FAM6(NEHALEM,			idle_cpu_nehalem),
 +	INTEL_CPU_FAM6(NEHALEM_G,		idle_cpu_nehalem),
 +	INTEL_CPU_FAM6(WESTMERE,		idle_cpu_nehalem),
 +	INTEL_CPU_FAM6(WESTMERE_EP,		idle_cpu_nehalem),
 +	INTEL_CPU_FAM6(NEHALEM_EX,		idle_cpu_nehalem),
 +	INTEL_CPU_FAM6(ATOM_BONNELL,		idle_cpu_atom),
 +	INTEL_CPU_FAM6(ATOM_BONNELL_MID,	idle_cpu_lincroft),
 +	INTEL_CPU_FAM6(WESTMERE_EX,		idle_cpu_nehalem),
 +	INTEL_CPU_FAM6(SANDYBRIDGE,		idle_cpu_snb),
 +	INTEL_CPU_FAM6(SANDYBRIDGE_X,		idle_cpu_snb),
 +	INTEL_CPU_FAM6(ATOM_SALTWELL,		idle_cpu_atom),
 +	INTEL_CPU_FAM6(ATOM_SILVERMONT,		idle_cpu_byt),
 +	INTEL_CPU_FAM6(ATOM_SILVERMONT_MID,	idle_cpu_tangier),
 +	INTEL_CPU_FAM6(ATOM_AIRMONT,		idle_cpu_cht),
 +	INTEL_CPU_FAM6(IVYBRIDGE,		idle_cpu_ivb),
 +	INTEL_CPU_FAM6(IVYBRIDGE_X,		idle_cpu_ivt),
 +	INTEL_CPU_FAM6(HASWELL,		idle_cpu_hsw),
 +	INTEL_CPU_FAM6(HASWELL_X,		idle_cpu_hsw),
 +	INTEL_CPU_FAM6(HASWELL_L,		idle_cpu_hsw),
 +	INTEL_CPU_FAM6(HASWELL_G,		idle_cpu_hsw),
 +	INTEL_CPU_FAM6(ATOM_SILVERMONT_D,	idle_cpu_avn),
 +	INTEL_CPU_FAM6(BROADWELL,		idle_cpu_bdw),
 +	INTEL_CPU_FAM6(BROADWELL_G,		idle_cpu_bdw),
 +	INTEL_CPU_FAM6(BROADWELL_X,		idle_cpu_bdw),
 +	INTEL_CPU_FAM6(BROADWELL_D,	idle_cpu_bdw),
 +	INTEL_CPU_FAM6(SKYLAKE_L,		idle_cpu_skl),
 +	INTEL_CPU_FAM6(SKYLAKE,		idle_cpu_skl),
 +	INTEL_CPU_FAM6(KABYLAKE_L,		idle_cpu_skl),
 +	INTEL_CPU_FAM6(KABYLAKE,	idle_cpu_skl),
 +	INTEL_CPU_FAM6(SKYLAKE_X,		idle_cpu_skx),
 +	INTEL_CPU_FAM6(XEON_PHI_KNL,		idle_cpu_knl),
 +	INTEL_CPU_FAM6(XEON_PHI_KNM,		idle_cpu_knl),
 +	INTEL_CPU_FAM6(ATOM_GOLDMONT,		idle_cpu_bxt),
 +	INTEL_CPU_FAM6(ATOM_GOLDMONT_PLUS,	idle_cpu_bxt),
 +	INTEL_CPU_FAM6(ATOM_GOLDMONT_D,		idle_cpu_dnv),
 +	INTEL_CPU_FAM6(ATOM_TREMONT_D,		idle_cpu_dnv),
 +	{}
 +};
 +
 +/*
 + * intel_idle_probe()
++=======
+ 	X86_MATCH_INTEL_FAM6_MODEL(NEHALEM_EP,		&idle_cpu_nhx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(NEHALEM,		&idle_cpu_nehalem),
+ 	X86_MATCH_INTEL_FAM6_MODEL(NEHALEM_G,		&idle_cpu_nehalem),
+ 	X86_MATCH_INTEL_FAM6_MODEL(WESTMERE,		&idle_cpu_nehalem),
+ 	X86_MATCH_INTEL_FAM6_MODEL(WESTMERE_EP,		&idle_cpu_nhx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(NEHALEM_EX,		&idle_cpu_nhx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_BONNELL,	&idle_cpu_atom),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_BONNELL_MID,	&idle_cpu_lincroft),
+ 	X86_MATCH_INTEL_FAM6_MODEL(WESTMERE_EX,		&idle_cpu_nhx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(SANDYBRIDGE,		&idle_cpu_snb),
+ 	X86_MATCH_INTEL_FAM6_MODEL(SANDYBRIDGE_X,	&idle_cpu_snx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_SALTWELL,	&idle_cpu_atom),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_SILVERMONT,	&idle_cpu_byt),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_SILVERMONT_MID,	&idle_cpu_tangier),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_AIRMONT,	&idle_cpu_cht),
+ 	X86_MATCH_INTEL_FAM6_MODEL(IVYBRIDGE,		&idle_cpu_ivb),
+ 	X86_MATCH_INTEL_FAM6_MODEL(IVYBRIDGE_X,		&idle_cpu_ivt),
+ 	X86_MATCH_INTEL_FAM6_MODEL(HASWELL,		&idle_cpu_hsw),
+ 	X86_MATCH_INTEL_FAM6_MODEL(HASWELL_X,		&idle_cpu_hsx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(HASWELL_L,		&idle_cpu_hsw),
+ 	X86_MATCH_INTEL_FAM6_MODEL(HASWELL_G,		&idle_cpu_hsw),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_SILVERMONT_D,	&idle_cpu_avn),
+ 	X86_MATCH_INTEL_FAM6_MODEL(BROADWELL,		&idle_cpu_bdw),
+ 	X86_MATCH_INTEL_FAM6_MODEL(BROADWELL_G,		&idle_cpu_bdw),
+ 	X86_MATCH_INTEL_FAM6_MODEL(BROADWELL_X,		&idle_cpu_bdx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(BROADWELL_D,		&idle_cpu_bdx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(SKYLAKE_L,		&idle_cpu_skl),
+ 	X86_MATCH_INTEL_FAM6_MODEL(SKYLAKE,		&idle_cpu_skl),
+ 	X86_MATCH_INTEL_FAM6_MODEL(KABYLAKE_L,		&idle_cpu_skl),
+ 	X86_MATCH_INTEL_FAM6_MODEL(KABYLAKE,		&idle_cpu_skl),
+ 	X86_MATCH_INTEL_FAM6_MODEL(SKYLAKE_X,		&idle_cpu_skx),
+ 	X86_MATCH_INTEL_FAM6_MODEL(XEON_PHI_KNL,	&idle_cpu_knl),
+ 	X86_MATCH_INTEL_FAM6_MODEL(XEON_PHI_KNM,	&idle_cpu_knl),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_GOLDMONT,	&idle_cpu_bxt),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_GOLDMONT_PLUS,	&idle_cpu_bxt),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_GOLDMONT_D,	&idle_cpu_dnv),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_D,	&idle_cpu_dnv),
+ 	{}
+ };
+ 
+ static const struct x86_cpu_id intel_mwait_ids[] __initconst = {
+ 	X86_MATCH_VENDOR_FAM_FEATURE(INTEL, 6, X86_FEATURE_MWAIT, NULL),
+ 	{}
+ };
+ 
+ static bool __init intel_idle_max_cstate_reached(int cstate)
+ {
+ 	if (cstate + 1 > max_cstate) {
+ 		pr_info("max_cstate %d reached\n", max_cstate);
+ 		return true;
+ 	}
+ 	return false;
+ }
+ 
+ #ifdef CONFIG_ACPI_PROCESSOR_CSTATE
+ #include <acpi/processor.h>
+ 
+ static bool no_acpi __read_mostly;
+ module_param(no_acpi, bool, 0444);
+ MODULE_PARM_DESC(no_acpi, "Do not use ACPI _CST for building the idle states list");
+ 
+ static bool force_use_acpi __read_mostly; /* No effect if no_acpi is set. */
+ module_param_named(use_acpi, force_use_acpi, bool, 0444);
+ MODULE_PARM_DESC(use_acpi, "Use ACPI _CST for building the idle states list");
+ 
+ static struct acpi_processor_power acpi_state_table __initdata;
+ 
+ /**
+  * intel_idle_cst_usable - Check if the _CST information can be used.
+  *
+  * Check if all of the C-states listed by _CST in the max_cstate range are
+  * ACPI_CSTATE_FFH, which means that they should be entered via MWAIT.
++>>>>>>> 4a9f45a0533f (intel_idle: Convert to new X86 CPU match macros)
   */
 -static bool __init intel_idle_cst_usable(void)
 +static int __init intel_idle_probe(void)
  {
 -	int cstate, limit;
 -
 -	limit = min_t(int, min_t(int, CPUIDLE_STATE_MAX, max_cstate + 1),
 -		      acpi_state_table.count);
 -
 -	for (cstate = 1; cstate < limit; cstate++) {
 -		struct acpi_processor_cx *cx = &acpi_state_table.states[cstate];
 +	unsigned int eax, ebx, ecx;
 +	const struct x86_cpu_id *id;
  
 -		if (cx->entry_method != ACPI_CSTATE_FFH)
 -			return false;
 +	if (max_cstate == 0) {
 +		pr_debug("disabled\n");
 +		return -EPERM;
  	}
  
 -	return true;
 -}
 -
 -static bool __init intel_idle_acpi_cst_extract(void)
 -{
 -	unsigned int cpu;
 -
 -	if (no_acpi) {
 -		pr_debug("Not allowed to use ACPI _CST\n");
 -		return false;
 +	id = x86_match_cpu(intel_idle_ids);
 +	if (!id) {
 +		if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
 +		    boot_cpu_data.x86 == 6)
 +			pr_debug("does not run on family %d model %d\n",
 +				 boot_cpu_data.x86, boot_cpu_data.x86_model);
 +		return -ENODEV;
  	}
  
 -	for_each_possible_cpu(cpu) {
 -		struct acpi_processor *pr = per_cpu(processors, cpu);
 -
 -		if (!pr)
 -			continue;
 -
 -		if (acpi_processor_evaluate_cst(pr->handle, cpu, &acpi_state_table))
 -			continue;
 -
 -		acpi_state_table.count++;
 -
 -		if (!intel_idle_cst_usable())
 -			continue;
 +	if (!boot_cpu_has(X86_FEATURE_MWAIT)) {
 +		pr_debug("Please enable MWAIT in BIOS SETUP\n");
 +		return -ENODEV;
 +	}
  
 -		if (!acpi_processor_claim_cst_control()) {
 -			acpi_state_table.count = 0;
 -			return false;
 -		}
 +	if (boot_cpu_data.cpuid_level < CPUID_MWAIT_LEAF)
 +		return -ENODEV;
  
 -		return true;
 -	}
 +	cpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &mwait_substates);
  
 -	pr_debug("ACPI _CST not found or not usable\n");
 -	return false;
 -}
 +	if (!(ecx & CPUID5_ECX_EXTENSIONS_SUPPORTED) ||
 +	    !(ecx & CPUID5_ECX_INTERRUPT_BREAK) ||
 +	    !mwait_substates)
 +			return -ENODEV;
  
 -static void __init intel_idle_init_cstates_acpi(struct cpuidle_driver *drv)
 -{
 -	int cstate, limit = min_t(int, CPUIDLE_STATE_MAX, acpi_state_table.count);
 +	pr_debug("MWAIT substates: 0x%x\n", mwait_substates);
  
 -	/*
 -	 * If limit > 0, intel_idle_cst_usable() has returned 'true', so all of
 -	 * the interesting states are ACPI_CSTATE_FFH.
 -	 */
 -	for (cstate = 1; cstate < limit; cstate++) {
 -		struct acpi_processor_cx *cx;
 -		struct cpuidle_state *state;
 +	icpu = (const struct idle_cpu *)id->driver_data;
 +	cpuidle_state_table = icpu->state_table;
  
 -		if (intel_idle_max_cstate_reached(cstate))
 -			break;
 +	pr_debug("v" INTEL_IDLE_VERSION " model 0x%X\n",
 +		 boot_cpu_data.x86_model);
  
 -		cx = &acpi_state_table.states[cstate];
 -
 -		state = &drv->states[drv->state_count++];
 -
 -		snprintf(state->name, CPUIDLE_NAME_LEN, "C%d_ACPI", cstate);
 -		strlcpy(state->desc, cx->desc, CPUIDLE_DESC_LEN);
 -		state->exit_latency = cx->latency;
 -		/*
 -		 * For C1-type C-states use the same number for both the exit
 -		 * latency and target residency, because that is the case for
 -		 * C1 in the majority of the static C-states tables above.
 -		 * For the other types of C-states, however, set the target
 -		 * residency to 3 times the exit latency which should lead to
 -		 * a reasonable balance between energy-efficiency and
 -		 * performance in the majority of interesting cases.
 -		 */
 -		state->target_residency = cx->latency;
 -		if (cx->type > ACPI_STATE_C1)
 -			state->target_residency *= 3;
 -
 -		state->flags = MWAIT2flg(cx->address);
 -		if (cx->type > ACPI_STATE_C2)
 -			state->flags |= CPUIDLE_FLAG_TLB_FLUSHED;
 -
 -		if (disabled_states_mask & BIT(cstate))
 -			state->flags |= CPUIDLE_FLAG_OFF;
 -
 -		state->enter = intel_idle;
 -		state->enter_s2idle = intel_idle_s2idle;
 -	}
 +	return 0;
  }
  
 -static bool __init intel_idle_off_by_default(u32 mwait_hint)
 +/*
 + * intel_idle_cpuidle_devices_uninit()
 + * Unregisters the cpuidle devices.
 + */
 +static void intel_idle_cpuidle_devices_uninit(void)
  {
 -	int cstate, limit;
 -
 -	/*
 -	 * If there are no _CST C-states, do not disable any C-states by
 -	 * default.
 -	 */
 -	if (!acpi_state_table.count)
 -		return false;
 +	int i;
 +	struct cpuidle_device *dev;
  
 -	limit = min_t(int, CPUIDLE_STATE_MAX, acpi_state_table.count);
 -	/*
 -	 * If limit > 0, intel_idle_cst_usable() has returned 'true', so all of
 -	 * the interesting states are ACPI_CSTATE_FFH.
 -	 */
 -	for (cstate = 1; cstate < limit; cstate++) {
 -		if (acpi_state_table.states[cstate].address == mwait_hint)
 -			return false;
 +	for_each_online_cpu(i) {
 +		dev = per_cpu_ptr(intel_idle_cpuidle_devices, i);
 +		cpuidle_unregister_device(dev);
  	}
 -	return true;
  }
 -#else /* !CONFIG_ACPI_PROCESSOR_CSTATE */
 -#define force_use_acpi	(false)
 -
 -static inline bool intel_idle_acpi_cst_extract(void) { return false; }
 -static inline void intel_idle_init_cstates_acpi(struct cpuidle_driver *drv) { }
 -static inline bool intel_idle_off_by_default(u32 mwait_hint) { return false; }
 -#endif /* !CONFIG_ACPI_PROCESSOR_CSTATE */
  
  /*
   * ivt_idle_state_table_update(void)
* Unmerged path drivers/idle/intel_idle.c
