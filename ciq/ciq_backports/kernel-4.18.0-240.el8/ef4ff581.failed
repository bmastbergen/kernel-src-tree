io_uring: move all request init code in one place

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit ef4ff581102a917a69877feca2e5347e2f3e458c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ef4ff581.failed

Requests initialisation is scattered across several functions, namely
io_init_req(), io_submit_sqes(), io_submit_sqe(). Put it
in io_init_req() for better data locality and code clarity.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ef4ff581102a917a69877feca2e5347e2f3e458c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 2afa3b27779e,c0cf57764329..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2225,107 -5555,63 +2225,115 @@@ static int __io_queue_sqe(struct io_rin
  
  	/* and drop final reference, if we failed */
  	if (ret) {
 -		io_cqring_add_event(req, ret);
 -		req_set_fail_links(req);
 +		io_cqring_add_event(ctx, req->user_data, ret);
 +		if (req->flags & REQ_F_LINK)
 +			req->flags |= REQ_F_FAIL_LINK;
  		io_put_req(req);
  	}
 -	if (nxt) {
 -		req = nxt;
  
 -		if (req->flags & REQ_F_FORCE_ASYNC)
 -			goto punt;
 -		goto again;
 +	return ret;
 +}
 +
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
 +{
 +	int ret;
 +
 +	ret = io_req_defer(ctx, req, s->sqe);
 +	if (ret) {
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		}
 +		return 0;
  	}
 -exit:
 -	if (old_creds)
 -		revert_creds(old_creds);
 +
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
  }
  
 -static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
  {
  	int ret;
 +	int need_submit = false;
  
 -	ret = io_req_defer(req, sqe);
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	/*
 +	 * Mark the first IO in link list as DRAIN, let all the following
 +	 * IOs enter the defer list. all IO needs to be completed before link
 +	 * list.
 +	 */
 +	req->flags |= REQ_F_IO_DRAIN;
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -fail_req:
 -			io_cqring_add_event(req, ret);
 -			req_set_fail_links(req);
 -			io_double_put_req(req);
 +			io_free_req(req);
 +			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
  		}
 -	} else if (req->flags & REQ_F_FORCE_ASYNC) {
 -		ret = io_req_defer_prep(req, sqe);
 -		if (unlikely(ret < 0))
 -			goto fail_req;
 +	} else {
  		/*
 -		 * Never try inline submit of IOSQE_ASYNC is set, go straight
 -		 * to async execution.
 +		 * If ret == 0 means that all IOs in front of link io are
 +		 * running done. let's queue link head.
  		 */
 -		req->work.flags |= IO_WQ_WORK_CONCURRENT;
 -		io_queue_async_work(req);
 -	} else {
 -		__io_queue_sqe(req, sqe);
 +		need_submit = true;
  	}
 +
 +	/* Insert shadow req to defer_list, blocking next IOs */
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_add_tail(&shadow->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	if (need_submit)
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	return 0;
  }
  
 -static inline void io_queue_link_head(struct io_kiocb *req)
++<<<<<<< HEAD
 +#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
 +
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
  {
 -	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_double_put_req(req);
 -	} else
 -		io_queue_sqe(req, NULL);
 -}
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
 +	int ret;
 +
 +	/* enforce forwards compatibility on users */
 +	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
 +		ret = -EINVAL;
 +		goto err;
 +	}
 +
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
 +	}
 +
 +	ret = io_req_set_file(ctx, s, state, req);
 +	if (unlikely(ret)) {
 +err_req:
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		return;
 +	}
  
 +	req->user_data = s->sqe->user_data;
++=======
+ static int io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
+ 			  struct io_submit_state *state, struct io_kiocb **link)
+ {
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	int ret;
++>>>>>>> ef4ff581102a (io_uring: move all request init code in one place)
  
  	/*
  	 * If we already have a head request, queue this one for async
@@@ -2335,26 -5621,58 +2343,66 @@@
  	 * conditions are true (normal request), then just queue it.
  	 */
  	if (*link) {
 -		struct io_kiocb *head = *link;
 -
 +		struct io_kiocb *prev = *link;
 +
++<<<<<<< HEAD
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (!sqe_copy) {
 +			ret = -EAGAIN;
 +			goto err_req;
++=======
+ 		/*
+ 		 * Taking sequential execution of a link, draining both sides
+ 		 * of the link also fullfils IOSQE_IO_DRAIN semantics for all
+ 		 * requests in the link. So, it drains the head and the
+ 		 * next after the link request. The last one is done via
+ 		 * drain_next flag to persist the effect across calls.
+ 		 */
+ 		if (req->flags & REQ_F_IO_DRAIN) {
+ 			head->flags |= REQ_F_IO_DRAIN;
+ 			ctx->drain_next = 1;
++>>>>>>> ef4ff581102a (io_uring: move all request init code in one place)
  		}
 -		if (io_alloc_async_ctx(req))
 -			return -EAGAIN;
  
 -		ret = io_req_defer_prep(req, sqe);
 -		if (ret) {
 -			/* fail even hard links since we don't submit */
 -			head->flags |= REQ_F_FAIL_LINK;
 -			return ret;
 -		}
 -		trace_io_uring_link(ctx, req, head);
 -		list_add_tail(&req->link_list, &head->link_list);
 +		s->sqe = sqe_copy;
 +		memcpy(&req->submit, s, sizeof(*s));
 +		list_add_tail(&req->list, &prev->link_list);
 +	} else if (s->sqe->flags & IOSQE_IO_LINK) {
 +		req->flags |= REQ_F_LINK;
  
++<<<<<<< HEAD
 +		memcpy(&req->submit, s, sizeof(*s));
 +		INIT_LIST_HEAD(&req->link_list);
 +		*link = req;
 +	} else {
 +		io_queue_sqe(ctx, req, s, force_nonblock);
++=======
+ 		/* last request of a link, enqueue the link */
+ 		if (!(req->flags & (REQ_F_LINK | REQ_F_HARDLINK))) {
+ 			io_queue_link_head(head);
+ 			*link = NULL;
+ 		}
+ 	} else {
+ 		if (unlikely(ctx->drain_next)) {
+ 			req->flags |= REQ_F_IO_DRAIN;
+ 			ctx->drain_next = 0;
+ 		}
+ 		if (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {
+ 			req->flags |= REQ_F_LINK_HEAD;
+ 			INIT_LIST_HEAD(&req->link_list);
+ 
+ 			if (io_alloc_async_ctx(req))
+ 				return -EAGAIN;
+ 
+ 			ret = io_req_defer_prep(req, sqe);
+ 			if (ret)
+ 				req->flags |= REQ_F_FAIL_LINK;
+ 			*link = req;
+ 		} else {
+ 			io_queue_sqe(req, sqe);
+ 		}
++>>>>>>> ef4ff581102a (io_uring: move all request init code in one place)
  	}
 -
 -	return 0;
  }
  
  /*
@@@ -2415,27 -5731,88 +2463,97 @@@ static bool io_get_sqring(struct io_rin
  	 * 2) allows the kernel side to track the head on its own, even
  	 *    though the application is the one updating it.
  	 */
 -	head = READ_ONCE(sq_array[ctx->cached_sq_head & ctx->sq_mask]);
 -	if (likely(head < ctx->sq_entries))
 -		return &ctx->sq_sqes[head];
 +	head = ctx->cached_sq_head;
 +	/* make sure SQ entry isn't read before tail */
 +	if (head == smp_load_acquire(&ring->r.tail))
 +		return false;
  
 -	/* drop invalid entries */
 -	ctx->cached_sq_dropped++;
 -	WRITE_ONCE(ctx->rings->sq_dropped, ctx->cached_sq_dropped);
 -	return NULL;
 -}
 +	head = READ_ONCE(ring->array[head & ctx->sq_mask]);
 +	if (head < ctx->sq_entries) {
 +		s->sqe = &ctx->sq_sqes[head];
 +		s->sequence = ctx->cached_sq_head;
 +		ctx->cached_sq_head++;
 +		return true;
 +	}
  
 -static inline void io_consume_sqe(struct io_ring_ctx *ctx)
 -{
 +	/* drop invalid entries */
  	ctx->cached_sq_head++;
 +	ring->dropped++;
 +	return false;
  }
  
++<<<<<<< HEAD
 +static int io_submit_sqes(struct io_ring_ctx *ctx, struct sqe_submit *sqes,
 +			  unsigned int nr, bool has_user, bool mm_fault)
++=======
+ #define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|	\
+ 				IOSQE_IO_HARDLINK | IOSQE_ASYNC | \
+ 				IOSQE_BUFFER_SELECT)
+ 
+ static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,
+ 		       const struct io_uring_sqe *sqe,
+ 		       struct io_submit_state *state, bool async)
+ {
+ 	unsigned int sqe_flags;
+ 	int id, fd;
+ 
+ 	/*
+ 	 * All io need record the previous position, if LINK vs DARIN,
+ 	 * it can be used to mark the position of the first IO in the
+ 	 * link list.
+ 	 */
+ 	req->sequence = ctx->cached_sq_head;
+ 	req->opcode = READ_ONCE(sqe->opcode);
+ 	req->user_data = READ_ONCE(sqe->user_data);
+ 	req->io = NULL;
+ 	req->file = NULL;
+ 	req->ctx = ctx;
+ 	req->flags = 0;
+ 	/* one is dropped after submission, the other at completion */
+ 	refcount_set(&req->refs, 2);
+ 	req->task = NULL;
+ 	req->result = 0;
+ 	req->needs_fixed_file = async;
+ 	INIT_IO_WORK(&req->work, io_wq_submit_work);
+ 
+ 	if (unlikely(req->opcode >= IORING_OP_LAST))
+ 		return -EINVAL;
+ 
+ 	if (io_op_defs[req->opcode].needs_mm && !current->mm) {
+ 		if (unlikely(!mmget_not_zero(ctx->sqo_mm)))
+ 			return -EFAULT;
+ 		use_mm(ctx->sqo_mm);
+ 	}
+ 
+ 	sqe_flags = READ_ONCE(sqe->flags);
+ 	/* enforce forwards compatibility on users */
+ 	if (unlikely(sqe_flags & ~SQE_VALID_FLAGS))
+ 		return -EINVAL;
+ 
+ 	if ((sqe_flags & IOSQE_BUFFER_SELECT) &&
+ 	    !io_op_defs[req->opcode].buffer_select)
+ 		return -EOPNOTSUPP;
+ 
+ 	id = READ_ONCE(sqe->personality);
+ 	if (id) {
+ 		req->work.creds = idr_find(&ctx->personality_idr, id);
+ 		if (unlikely(!req->work.creds))
+ 			return -EINVAL;
+ 		get_cred(req->work.creds);
+ 	}
+ 
+ 	/* same numerical values with corresponding REQ_F_*, safe to copy */
+ 	req->flags |= sqe_flags & (IOSQE_IO_DRAIN | IOSQE_IO_HARDLINK |
+ 					IOSQE_ASYNC | IOSQE_FIXED_FILE |
+ 					IOSQE_BUFFER_SELECT | IOSQE_IO_LINK);
+ 
+ 	fd = READ_ONCE(sqe->fd);
+ 	return io_req_set_file(state, req, fd, sqe_flags);
+ }
+ 
+ static int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr,
+ 			  struct file *ring_file, int ring_fd, bool async)
++>>>>>>> ef4ff581102a (io_uring: move all request init code in one place)
  {
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
@@@ -2448,45 -5836,52 +2566,65 @@@
  		statep = &state;
  	}
  
 -	ctx->ring_fd = ring_fd;
 -	ctx->ring_file = ring_file;
 -
  	for (i = 0; i < nr; i++) {
 -		const struct io_uring_sqe *sqe;
 -		struct io_kiocb *req;
 -		int err;
 -
 -		sqe = io_get_sqe(ctx);
 -		if (unlikely(!sqe)) {
 -			io_consume_sqe(ctx);
 -			break;
 +		/*
 +		 * If previous wasn't linked and we have a linked command,
 +		 * that's the end of the chain. Submit the previous link.
 +		 */
 +		if (!prev_was_link && link) {
 +			io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +						true);
 +			link = NULL;
 +			shadow_req = NULL;
  		}
 -		req = io_alloc_req(ctx, statep);
 -		if (unlikely(!req)) {
 -			if (!submitted)
 -				submitted = -EAGAIN;
 -			break;
 +		prev_was_link = (sqes[i].sqe->flags & IOSQE_IO_LINK) != 0;
 +
++<<<<<<< HEAD
 +		if (link && (sqes[i].sqe->flags & IOSQE_IO_DRAIN)) {
 +			if (!shadow_req) {
 +				shadow_req = io_get_req(ctx, NULL);
 +				if (unlikely(!shadow_req))
 +					goto out;
 +				shadow_req->flags |= (REQ_F_IO_DRAIN | REQ_F_SHADOW_DRAIN);
 +				refcount_dec(&shadow_req->refs);
 +			}
 +			shadow_req->sequence = sqes[i].sequence;
  		}
  
 +out:
 +		if (unlikely(mm_fault)) {
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
 +						-EFAULT);
 +		} else {
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
 +			submitted++;
 +		}
++=======
+ 		err = io_init_req(ctx, req, sqe, statep, async);
+ 		io_consume_sqe(ctx);
+ 		/* will complete beyond this point, count as submitted */
+ 		submitted++;
+ 
+ 		if (unlikely(err)) {
+ fail_req:
+ 			io_cqring_add_event(req, err);
+ 			io_double_put_req(req);
+ 			break;
+ 		}
+ 
+ 		trace_io_uring_submit_sqe(ctx, req->opcode, req->user_data,
+ 						true, async);
+ 		err = io_submit_sqe(req, sqe, statep, &link);
+ 		if (err)
+ 			goto fail_req;
++>>>>>>> ef4ff581102a (io_uring: move all request init code in one place)
  	}
  
 -	if (unlikely(submitted != nr)) {
 -		int ref_used = (submitted == -EAGAIN) ? 0 : submitted;
 -
 -		percpu_ref_put_many(&ctx->refs, nr - ref_used);
 -	}
  	if (link)
 -		io_queue_link_head(link);
 +		io_queue_link_head(ctx, link, &link->submit, shadow_req, true);
  	if (statep)
  		io_submit_state_end(&state);
  
* Unmerged path fs/io_uring.c
