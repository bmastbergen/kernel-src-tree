io_uring: don't dynamically allocate poll data

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 392edb45b24337eaa0bc1ecd4e3cf897e662ec61
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/392edb45.failed

This essentially reverts commit e944475e6984. For high poll ops
workloads, like TAO, the dynamic allocation of the wait_queue
entry for IORING_OP_POLL_ADD adds considerable extra overhead.
Go back to embedding the wait_queue_entry, but keep the usage of
wait->private for the pointer stashing.

	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 392edb45b24337eaa0bc1ecd4e3cf897e662ec61)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7e2b8c92aeeb,89a2c8b4f8fb..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -306,6 -294,42 +306,45 @@@ struct io_poll_iocb 
  	bool				done;
  	bool				canceled;
  	struct wait_queue_entry		wait;
++<<<<<<< HEAD
++=======
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_ctx {
+ 	struct io_uring_sqe		sqe;
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 	};
++>>>>>>> 392edb45b243 (io_uring: don't dynamically allocate poll data)
  };
  
  /*
@@@ -1644,11 -2288,10 +1683,15 @@@ static void io_poll_remove_one(struct i
  	WRITE_ONCE(poll->canceled, true);
  	if (!list_empty(&poll->wait.entry)) {
  		list_del_init(&poll->wait.entry);
++<<<<<<< HEAD
 +		io_queue_async_work(req->ctx, req);
++=======
+ 		io_queue_async_work(req);
++>>>>>>> 392edb45b243 (io_uring: don't dynamically allocate poll data)
  	}
  	spin_unlock(&poll->head->lock);
 -	hash_del(&req->hash_node);
 +
 +	list_del_init(&req->list);
  }
  
  static void io_poll_remove_all(struct io_ring_ctx *ctx)
@@@ -1694,11 -2353,15 +1737,18 @@@ static int io_poll_remove(struct io_kio
  	return 0;
  }
  
 -static void io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)
 +static void io_poll_complete(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			     __poll_t mask)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -
  	req->poll.done = true;
++<<<<<<< HEAD
 +	io_cqring_fill_event(ctx, req->user_data, mangle_poll(mask));
++=======
+ 	if (error)
+ 		io_cqring_fill_event(req, error);
+ 	else
+ 		io_cqring_fill_event(req, mangle_poll(mask));
++>>>>>>> 392edb45b243 (io_uring: don't dynamically allocate poll data)
  	io_commit_cqring(ctx);
  }
  
@@@ -1721,7 -2394,7 +1771,11 @@@ static void io_poll_complete_work(struc
  	 * avoid further branches in the fast path.
  	 */
  	spin_lock_irq(&ctx->completion_lock);
++<<<<<<< HEAD
 +	if (!mask && !READ_ONCE(poll->canceled)) {
++=======
+ 	if (!mask && ret != -ECANCELED) {
++>>>>>>> 392edb45b243 (io_uring: don't dynamically allocate poll data)
  		add_wait_queue(poll->head, &poll->wait);
  		spin_unlock_irq(&ctx->completion_lock);
  		return;
@@@ -1801,10 -2495,11 +1855,15 @@@ static int io_poll_add(struct io_kiocb 
  	if (!poll->file)
  		return -EBADF;
  
++<<<<<<< HEAD
 +	req->submit.sqe = NULL;
 +	INIT_WORK(&req->work, io_poll_complete_work);
++=======
+ 	req->io = NULL;
+ 	INIT_IO_WORK(&req->work, io_poll_complete_work);
++>>>>>>> 392edb45b243 (io_uring: don't dynamically allocate poll data)
  	events = READ_ONCE(sqe->poll_events);
  	poll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP;
 -	INIT_HLIST_NODE(&req->hash_node);
  
  	poll->head = NULL;
  	poll->done = false;
@@@ -1818,6 -2513,7 +1877,10 @@@
  	/* initialized the list so that we can do list_empty checks */
  	INIT_LIST_HEAD(&poll->wait.entry);
  	init_waitqueue_func_entry(&poll->wait, io_poll_wake);
++<<<<<<< HEAD
++=======
+ 	poll->wait.private = poll;
++>>>>>>> 392edb45b243 (io_uring: don't dynamically allocate poll data)
  
  	INIT_LIST_HEAD(&req->list);
  
* Unmerged path fs/io_uring.c
