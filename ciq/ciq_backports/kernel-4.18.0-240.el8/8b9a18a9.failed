x86/split_lock: Add Tremont family CPU models

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Tony Luck <tony.luck@intel.com>
commit 8b9a18a9f2494144fe23fe630d0734310fa65301
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/8b9a18a9.failed

Tremont CPUs support IA32_CORE_CAPABILITIES bits to indicate whether
specific SKUs have support for split lock detection.

	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20200416205754.21177-4-tony.luck@intel.com

(cherry picked from commit 8b9a18a9f2494144fe23fe630d0734310fa65301)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/intel.c
diff --cc arch/x86/kernel/cpu/intel.c
index ae064a2ca368,a19a680542ce..000000000000
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@@ -1034,3 -967,205 +1034,208 @@@ static const struct cpu_dev intel_cpu_d
  
  cpu_dev_register(intel_cpu_dev);
  
++<<<<<<< HEAD
++=======
+ #undef pr_fmt
+ #define pr_fmt(fmt) "x86/split lock detection: " fmt
+ 
+ static const struct {
+ 	const char			*option;
+ 	enum split_lock_detect_state	state;
+ } sld_options[] __initconst = {
+ 	{ "off",	sld_off   },
+ 	{ "warn",	sld_warn  },
+ 	{ "fatal",	sld_fatal },
+ };
+ 
+ static inline bool match_option(const char *arg, int arglen, const char *opt)
+ {
+ 	int len = strlen(opt);
+ 
+ 	return len == arglen && !strncmp(arg, opt, len);
+ }
+ 
+ static bool split_lock_verify_msr(bool on)
+ {
+ 	u64 ctrl, tmp;
+ 
+ 	if (rdmsrl_safe(MSR_TEST_CTRL, &ctrl))
+ 		return false;
+ 	if (on)
+ 		ctrl |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	else
+ 		ctrl &= ~MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 	if (wrmsrl_safe(MSR_TEST_CTRL, ctrl))
+ 		return false;
+ 	rdmsrl(MSR_TEST_CTRL, tmp);
+ 	return ctrl == tmp;
+ }
+ 
+ static void __init split_lock_setup(void)
+ {
+ 	enum split_lock_detect_state state = sld_warn;
+ 	char arg[20];
+ 	int i, ret;
+ 
+ 	if (!split_lock_verify_msr(false)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	ret = cmdline_find_option(boot_command_line, "split_lock_detect",
+ 				  arg, sizeof(arg));
+ 	if (ret >= 0) {
+ 		for (i = 0; i < ARRAY_SIZE(sld_options); i++) {
+ 			if (match_option(arg, ret, sld_options[i].option)) {
+ 				state = sld_options[i].state;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	switch (state) {
+ 	case sld_off:
+ 		pr_info("disabled\n");
+ 		return;
+ 	case sld_warn:
+ 		pr_info("warning about user-space split_locks\n");
+ 		break;
+ 	case sld_fatal:
+ 		pr_info("sending SIGBUS on user-space split_locks\n");
+ 		break;
+ 	}
+ 
+ 	rdmsrl(MSR_TEST_CTRL, msr_test_ctrl_cache);
+ 
+ 	if (!split_lock_verify_msr(true)) {
+ 		pr_info("MSR access failed: Disabled\n");
+ 		return;
+ 	}
+ 
+ 	sld_state = state;
+ 	setup_force_cpu_cap(X86_FEATURE_SPLIT_LOCK_DETECT);
+ }
+ 
+ /*
+  * MSR_TEST_CTRL is per core, but we treat it like a per CPU MSR. Locking
+  * is not implemented as one thread could undo the setting of the other
+  * thread immediately after dropping the lock anyway.
+  */
+ static void sld_update_msr(bool on)
+ {
+ 	u64 test_ctrl_val = msr_test_ctrl_cache;
+ 
+ 	if (on)
+ 		test_ctrl_val |= MSR_TEST_CTRL_SPLIT_LOCK_DETECT;
+ 
+ 	wrmsrl(MSR_TEST_CTRL, test_ctrl_val);
+ }
+ 
+ static void split_lock_init(void)
+ {
+ 	split_lock_verify_msr(sld_state != sld_off);
+ }
+ 
+ static void split_lock_warn(unsigned long ip)
+ {
+ 	pr_warn_ratelimited("#AC: %s/%d took a split_lock trap at address: 0x%lx\n",
+ 			    current->comm, current->pid, ip);
+ 
+ 	/*
+ 	 * Disable the split lock detection for this task so it can make
+ 	 * progress and set TIF_SLD so the detection is re-enabled via
+ 	 * switch_to_sld() when the task is scheduled out.
+ 	 */
+ 	sld_update_msr(false);
+ 	set_tsk_thread_flag(current, TIF_SLD);
+ }
+ 
+ bool handle_guest_split_lock(unsigned long ip)
+ {
+ 	if (sld_state == sld_warn) {
+ 		split_lock_warn(ip);
+ 		return true;
+ 	}
+ 
+ 	pr_warn_once("#AC: %s/%d %s split_lock trap at address: 0x%lx\n",
+ 		     current->comm, current->pid,
+ 		     sld_state == sld_fatal ? "fatal" : "bogus", ip);
+ 
+ 	current->thread.error_code = 0;
+ 	current->thread.trap_nr = X86_TRAP_AC;
+ 	force_sig_fault(SIGBUS, BUS_ADRALN, NULL);
+ 	return false;
+ }
+ EXPORT_SYMBOL_GPL(handle_guest_split_lock);
+ 
+ bool handle_user_split_lock(struct pt_regs *regs, long error_code)
+ {
+ 	if ((regs->flags & X86_EFLAGS_AC) || sld_state == sld_fatal)
+ 		return false;
+ 	split_lock_warn(regs->ip);
+ 	return true;
+ }
+ 
+ /*
+  * This function is called only when switching between tasks with
+  * different split-lock detection modes. It sets the MSR for the
+  * mode of the new task. This is right most of the time, but since
+  * the MSR is shared by hyperthreads on a physical core there can
+  * be glitches when the two threads need different modes.
+  */
+ void switch_to_sld(unsigned long tifn)
+ {
+ 	sld_update_msr(!(tifn & _TIF_SLD));
+ }
+ 
+ /*
+  * Bits in the IA32_CORE_CAPABILITIES are not architectural, so they should
+  * only be trusted if it is confirmed that a CPU model implements a
+  * specific feature at a particular bit position.
+  *
+  * The possible driver data field values:
+  *
+  * - 0: CPU models that are known to have the per-core split-lock detection
+  *	feature even though they do not enumerate IA32_CORE_CAPABILITIES.
+  *
+  * - 1: CPU models which may enumerate IA32_CORE_CAPABILITIES and if so use
+  *      bit 5 to enumerate the per-core split-lock detection feature.
+  */
+ static const struct x86_cpu_id split_lock_cpu_ids[] __initconst = {
+ 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,		0),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_L,		0),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT,	1),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_D,	1),
+ 	X86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_L,	1),
+ 	{}
+ };
+ 
+ void __init cpu_set_core_cap_bits(struct cpuinfo_x86 *c)
+ {
+ 	const struct x86_cpu_id *m;
+ 	u64 ia32_core_caps;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+ 		return;
+ 
+ 	m = x86_match_cpu(split_lock_cpu_ids);
+ 	if (!m)
+ 		return;
+ 
+ 	switch (m->driver_data) {
+ 	case 0:
+ 		break;
+ 	case 1:
+ 		if (!cpu_has(c, X86_FEATURE_CORE_CAPABILITIES))
+ 			return;
+ 		rdmsrl(MSR_IA32_CORE_CAPS, ia32_core_caps);
+ 		if (!(ia32_core_caps & MSR_IA32_CORE_CAPS_SPLIT_LOCK_DETECT))
+ 			return;
+ 		break;
+ 	default:
+ 		return;
+ 	}
+ 
+ 	split_lock_setup();
+ }
++>>>>>>> 8b9a18a9f249 (x86/split_lock: Add Tremont family CPU models)
* Unmerged path arch/x86/kernel/cpu/intel.c
