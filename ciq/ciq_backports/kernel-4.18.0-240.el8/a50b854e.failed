mm: introduce page_size()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [net] xdp: mm: introduce page_size() (Jiri Benc) [1819630]
Rebuild_FUZZ: 90.91%
commit-author Matthew Wilcox (Oracle) <willy@infradead.org>
commit a50b854e073cd3335bbbada8dcff83a857297dd7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/a50b854e.failed

Patch series "Make working with compound pages easier", v2.

These three patches add three helpers and convert the appropriate
places to use them.

This patch (of 3):

It's unnecessarily hard to find out the size of a potentially huge page.
Replace 'PAGE_SIZE << compound_order(page)' with page_size(page).

Link: http://lkml.kernel.org/r/20190721104612.19120-2-willy@infradead.org
	Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a50b854e073cd3335bbbada8dcff83a857297dd7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/kasan/kasan.c
#	mm/rmap.c
#	mm/slub.c
diff --cc mm/kasan/kasan.c
index c7365a12b42a,307631d9c62b..000000000000
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@@ -396,8 -334,11 +396,16 @@@ size_t kasan_metadata_size(struct kmem_
  
  void kasan_poison_slab(struct page *page)
  {
++<<<<<<< HEAD:mm/kasan/kasan.c
 +	kasan_poison_shadow(page_address(page),
 +			PAGE_SIZE << compound_order(page),
++=======
+ 	unsigned long i;
+ 
+ 	for (i = 0; i < (1 << compound_order(page)); i++)
+ 		page_kasan_tag_reset(page + i);
+ 	kasan_poison_shadow(page_address(page), page_size(page),
++>>>>>>> a50b854e073c (mm: introduce page_size()):mm/kasan/common.c
  			KASAN_KMALLOC_REDZONE);
  }
  
diff --cc mm/rmap.c
index 4ca7a0db9645,f401732b20e8..000000000000
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@@ -902,14 -896,15 +902,21 @@@ static bool page_mkclean_one(struct pag
  	 * We have to assume the worse case ie pmd for invalidation. Note that
  	 * the page can not be free from this function.
  	 */
++<<<<<<< HEAD
 +	end = min(vma->vm_end, start + (PAGE_SIZE << compound_order(page)));
 +	mmu_notifier_invalidate_range_start(vma->vm_mm, start, end);
++=======
+ 	mmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,
+ 				0, vma, vma->vm_mm, address,
+ 				min(vma->vm_end, address + page_size(page)));
+ 	mmu_notifier_invalidate_range_start(&range);
++>>>>>>> a50b854e073c (mm: introduce page_size())
  
  	while (page_vma_mapped_walk(&pvmw)) {
 +		unsigned long cstart;
  		int ret = 0;
  
 -		address = pvmw.address;
 +		cstart = address = pvmw.address;
  		if (pvmw.pte) {
  			pte_t entry;
  			pte_t *pte = pvmw.pte;
@@@ -1375,7 -1369,9 +1382,13 @@@ static bool try_to_unmap_one(struct pag
  	 * Note that the page can not be free in this function as call of
  	 * try_to_unmap() must hold a reference on the page.
  	 */
++<<<<<<< HEAD
 +	end = min(vma->vm_end, start + (PAGE_SIZE << compound_order(page)));
++=======
+ 	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
+ 				address,
+ 				min(vma->vm_end, address + page_size(page)));
++>>>>>>> a50b854e073c (mm: introduce page_size())
  	if (PageHuge(page)) {
  		/*
  		 * If sharing is possible, start and end will be adjusted
diff --cc mm/slub.c
index b0f12226fcc4,42c1b3af3c98..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -1089,6 -1074,17 +1089,20 @@@ static void setup_object_debug(struct k
  	init_tracking(s, object);
  }
  
++<<<<<<< HEAD
++=======
+ static
+ void setup_page_debug(struct kmem_cache *s, struct page *page, void *addr)
+ {
+ 	if (!(s->flags & SLAB_POISON))
+ 		return;
+ 
+ 	metadata_access_enable();
+ 	memset(addr, POISON_INUSE, page_size(page));
+ 	metadata_access_disable();
+ }
+ 
++>>>>>>> a50b854e073c (mm: introduce page_size())
  static inline int alloc_consistency_checks(struct kmem_cache *s,
  					struct page *page, void *object)
  {
@@@ -1343,6 -1341,8 +1357,11 @@@ slab_flags_t kmem_cache_flags(unsigned 
  #else /* !CONFIG_SLUB_DEBUG */
  static inline void setup_object_debug(struct kmem_cache *s,
  			struct page *page, void *object) {}
++<<<<<<< HEAD
++=======
+ static inline
+ void setup_page_debug(struct kmem_cache *s, struct page *page, void *addr) {}
++>>>>>>> a50b854e073c (mm: introduce page_size())
  
  static inline int alloc_debug_processing(struct kmem_cache *s,
  	struct page *page, void *object, unsigned long addr) { return 0; }
@@@ -1612,8 -1639,8 +1631,13 @@@ static struct page *allocate_slab(struc
  	struct page *page;
  	struct kmem_cache_order_objects oo = s->oo;
  	gfp_t alloc_gfp;
++<<<<<<< HEAD
 +	void *start, *p;
 +	int idx, order;
++=======
+ 	void *start, *p, *next;
+ 	int idx;
++>>>>>>> a50b854e073c (mm: introduce page_size())
  	bool shuffle;
  
  	flags &= gfp_allowed_mask;
@@@ -1653,12 -1679,11 +1676,16 @@@
  	if (page_is_pfmemalloc(page))
  		SetPageSlabPfmemalloc(page);
  
 -	kasan_poison_slab(page);
 -
  	start = page_address(page);
  
++<<<<<<< HEAD
 +	if (unlikely(s->flags & SLAB_POISON))
 +		memset(start, POISON_INUSE, PAGE_SIZE << order);
 +
 +	kasan_poison_slab(page);
++=======
+ 	setup_page_debug(s, page, start);
++>>>>>>> a50b854e073c (mm: introduce page_size())
  
  	shuffle = shuffle_freelist(s, page);
  
diff --git a/arch/arm/mm/flush.c b/arch/arm/mm/flush.c
index 58469623b015..c68a120de28b 100644
--- a/arch/arm/mm/flush.c
+++ b/arch/arm/mm/flush.c
@@ -207,8 +207,7 @@ void __flush_dcache_page(struct address_space *mapping, struct page *page)
 	 * coherent with the kernels mapping.
 	 */
 	if (!PageHighMem(page)) {
-		size_t page_size = PAGE_SIZE << compound_order(page);
-		__cpuc_flush_dcache_area(page_address(page), page_size);
+		__cpuc_flush_dcache_area(page_address(page), page_size(page));
 	} else {
 		unsigned long i;
 		if (cache_is_vipt_nonaliasing()) {
diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 8c4da27b1caf..6b2cc004eefb 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -67,8 +67,7 @@ void __sync_icache_dcache(pte_t pte)
 	struct page *page = pte_page(pte);
 
 	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
-		sync_icache_aliases(page_address(page),
-				    PAGE_SIZE << compound_order(page));
+		sync_icache_aliases(page_address(page), page_size(page));
 }
 
 /*
diff --git a/arch/ia64/mm/init.c b/arch/ia64/mm/init.c
index 0c1cd494d2af..cbe7e2531328 100644
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@ -63,7 +63,7 @@ __ia64_sync_icache_dcache (pte_t pte)
 	if (test_bit(PG_arch_1, &page->flags))
 		return;				/* i-cache is already coherent with d-cache */
 
-	flush_icache_range(addr, addr + (PAGE_SIZE << compound_order(page)));
+	flush_icache_range(addr, addr + page_size(page));
 	set_bit(PG_arch_1, &page->flags);	/* mark page as clean */
 }
 
diff --git a/drivers/crypto/chelsio/chtls/chtls_io.c b/drivers/crypto/chelsio/chtls/chtls_io.c
index 18f553fcc167..97bf5ba3a543 100644
--- a/drivers/crypto/chelsio/chtls/chtls_io.c
+++ b/drivers/crypto/chelsio/chtls/chtls_io.c
@@ -1082,7 +1082,7 @@ int chtls_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
 			bool merge;
 
 			if (page)
-				pg_size <<= compound_order(page);
+				pg_size = page_size(page);
 			if (off < pg_size &&
 			    skb_can_coalesce(skb, i, page, off)) {
 				merge = 1;
@@ -1109,8 +1109,7 @@ int chtls_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
 							   __GFP_NORETRY,
 							   order);
 					if (page)
-						pg_size <<=
-							compound_order(page);
+						pg_size <<= order;
 				}
 				if (!page) {
 					page = alloc_page(gfp);
diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c
index 701eb9f3b0f1..0962379d70de 100644
--- a/drivers/staging/android/ion/ion_system_heap.c
+++ b/drivers/staging/android/ion/ion_system_heap.c
@@ -120,7 +120,7 @@ static int ion_system_heap_allocate(struct ion_heap *heap,
 		if (!page)
 			goto free_pages;
 		list_add_tail(&page->lru, &pages);
-		size_remaining -= PAGE_SIZE << compound_order(page);
+		size_remaining -= page_size(page);
 		max_order = compound_order(page);
 		i++;
 	}
@@ -133,7 +133,7 @@ static int ion_system_heap_allocate(struct ion_heap *heap,
 
 	sg = table->sgl;
 	list_for_each_entry_safe(page, tmp_page, &pages, lru) {
-		sg_set_page(sg, page, PAGE_SIZE << compound_order(page), 0);
+		sg_set_page(sg, page, page_size(page), 0);
 		sg = sg_next(sg);
 		list_del(&page->lru);
 	}
diff --git a/drivers/target/tcm_fc/tfc_io.c b/drivers/target/tcm_fc/tfc_io.c
index 1eb1f58e00e4..83c1ec65dbcc 100644
--- a/drivers/target/tcm_fc/tfc_io.c
+++ b/drivers/target/tcm_fc/tfc_io.c
@@ -148,8 +148,7 @@ int ft_queue_data_in(struct se_cmd *se_cmd)
 					   page, off_in_page, tlen);
 			fr_len(fp) += tlen;
 			fp_skb(fp)->data_len += tlen;
-			fp_skb(fp)->truesize +=
-					PAGE_SIZE << compound_order(page);
+			fp_skb(fp)->truesize += page_size(page);
 		} else {
 			BUG_ON(!page);
 			from = kmap_atomic(page + (mem_off >> PAGE_SHIFT));
diff --git a/fs/io_uring.c b/fs/io_uring.c
index 76af33052de8..536cb5938030 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -3167,7 +3167,7 @@ static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
 	}
 
 	page = virt_to_head_page(ptr);
-	if (sz > (PAGE_SIZE << compound_order(page)))
+	if (sz > page_size(page))
 		return -EINVAL;
 
 	pfn = virt_to_phys(ptr) >> PAGE_SHIFT;
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 75b7d65145a4..b42c44539a9b 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -470,7 +470,7 @@ static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
 static inline struct hstate *page_hstate(struct page *page)
 {
 	VM_BUG_ON_PAGE(!PageHuge(page), page);
-	return size_to_hstate(PAGE_SIZE << compound_order(page));
+	return size_to_hstate(page_size(page));
 }
 
 static inline unsigned hstate_index_to_shift(unsigned index)
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 7814b1c09166..acf2a864bad9 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -731,6 +731,12 @@ static inline void set_compound_order(struct page *page, unsigned int order)
 	page[1].compound_order = order;
 }
 
+/* Returns the number of bytes in this potentially compound page. */
+static inline unsigned long page_size(struct page *page)
+{
+	return PAGE_SIZE << compound_order(page);
+}
+
 void free_compound_page(struct page *page);
 
 #ifdef CONFIG_MMU
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index fbe818b005c6..2402ef863c49 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -826,7 +826,7 @@ static inline bool page_copy_sane(struct page *page, size_t offset, size_t n)
 	struct page *head = compound_head(page);
 	size_t v = n + offset + page_address(page) - page_address(head);
 
-	if (likely(n <= v && v <= (PAGE_SIZE << compound_order(head))))
+	if (likely(n <= v && v <= (page_size(head))))
 		return true;
 	WARN_ON(1);
 	return false;
* Unmerged path mm/kasan/kasan.c
diff --git a/mm/nommu.c b/mm/nommu.c
index 39635d5b9019..ad6e20478143 100644
--- a/mm/nommu.c
+++ b/mm/nommu.c
@@ -107,7 +107,7 @@ unsigned int kobjsize(const void *objp)
 	 * The ksize() function is only guaranteed to work for pointers
 	 * returned by kmalloc(). So handle arbitrary pointers here.
 	 */
-	return PAGE_SIZE << compound_order(page);
+	return page_size(page);
 }
 
 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c
index 11df03e71288..eff4b4520c8d 100644
--- a/mm/page_vma_mapped.c
+++ b/mm/page_vma_mapped.c
@@ -153,8 +153,7 @@ bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)
 
 	if (unlikely(PageHuge(pvmw->page))) {
 		/* when pud is not present, pte will be NULL */
-		pvmw->pte = huge_pte_offset(mm, pvmw->address,
-					    PAGE_SIZE << compound_order(page));
+		pvmw->pte = huge_pte_offset(mm, pvmw->address, page_size(page));
 		if (!pvmw->pte)
 			return false;
 
* Unmerged path mm/rmap.c
diff --git a/mm/slob.c b/mm/slob.c
index 84aefd9b91ee..8aa468803d8f 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -539,7 +539,7 @@ size_t ksize(const void *block)
 
 	sp = virt_to_page(block);
 	if (unlikely(!PageSlab(sp)))
-		return PAGE_SIZE << compound_order(sp);
+		return page_size(sp);
 
 	align = max_t(size_t, ARCH_KMALLOC_MINALIGN, ARCH_SLAB_MINALIGN);
 	m = (unsigned int *)(block - align);
* Unmerged path mm/slub.c
diff --git a/net/xdp/xsk.c b/net/xdp/xsk.c
index 745a2da48b58..4a5e9e2b5fa7 100644
--- a/net/xdp/xsk.c
+++ b/net/xdp/xsk.c
@@ -976,7 +976,7 @@ static int xsk_mmap(struct file *file, struct socket *sock,
 	/* Matches the smp_wmb() in xsk_init_queue */
 	smp_rmb();
 	qpg = virt_to_head_page(q->ring);
-	if (size > (PAGE_SIZE << compound_order(qpg)))
+	if (size > page_size(qpg))
 		return -EINVAL;
 
 	pfn = virt_to_phys(q->ring) >> PAGE_SHIFT;
