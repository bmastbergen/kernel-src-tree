io_uring: add mapping support for NOMMU archs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Roman Penyaev <rpenyaev@suse.de>
commit 6c5c240e412682f97aecd233c1e706822704aa28
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6c5c240e.failed

That is a bit weird scenario but I find it interesting to run fio loads
using LKL linux, where MMU is disabled.  Probably other real archs which
run uClinux can also benefit from this patch.

	Signed-off-by: Roman Penyaev <rpenyaev@suse.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 6c5c240e412682f97aecd233c1e706822704aa28)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 9058714611f6,e6fc401e341f..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -3577,12 -4355,58 +3577,62 @@@ static int io_uring_release(struct inod
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
++=======
+ static void io_uring_cancel_files(struct io_ring_ctx *ctx,
+ 				  struct files_struct *files)
+ {
+ 	struct io_kiocb *req;
+ 	DEFINE_WAIT(wait);
+ 
+ 	while (!list_empty_careful(&ctx->inflight_list)) {
+ 		struct io_kiocb *cancel_req = NULL;
+ 
+ 		spin_lock_irq(&ctx->inflight_lock);
+ 		list_for_each_entry(req, &ctx->inflight_list, inflight_entry) {
+ 			if (req->work.files != files)
+ 				continue;
+ 			/* req is being completed, ignore */
+ 			if (!refcount_inc_not_zero(&req->refs))
+ 				continue;
+ 			cancel_req = req;
+ 			break;
+ 		}
+ 		if (cancel_req)
+ 			prepare_to_wait(&ctx->inflight_wait, &wait,
+ 						TASK_UNINTERRUPTIBLE);
+ 		spin_unlock_irq(&ctx->inflight_lock);
+ 
+ 		/* We need to keep going until we don't find a matching req */
+ 		if (!cancel_req)
+ 			break;
+ 
+ 		io_wq_cancel_work(ctx->io_wq, &cancel_req->work);
+ 		io_put_req(cancel_req);
+ 		schedule();
+ 	}
+ 	finish_wait(&ctx->inflight_wait, &wait);
+ }
+ 
+ static int io_uring_flush(struct file *file, void *data)
  {
- 	loff_t offset = (loff_t) vma->vm_pgoff << PAGE_SHIFT;
- 	unsigned long sz = vma->vm_end - vma->vm_start;
  	struct io_ring_ctx *ctx = file->private_data;
- 	unsigned long pfn;
+ 
+ 	io_uring_cancel_files(ctx, data);
+ 	if (fatal_signal_pending(current) || (current->flags & PF_EXITING)) {
+ 		io_cqring_overflow_flush(ctx, true);
+ 		io_wq_cancel_all(ctx->io_wq);
+ 	}
+ 	return 0;
+ }
+ 
+ static void *io_uring_validate_mmap_request(struct file *file,
+ 					    loff_t pgoff, size_t sz)
++>>>>>>> 6c5c240e4126 (io_uring: add mapping support for NOMMU archs)
+ {
+ 	struct io_ring_ctx *ctx = file->private_data;
+ 	loff_t offset = pgoff << PAGE_SHIFT;
  	struct page *page;
  	void *ptr;
  
@@@ -3593,16 -4418,28 +3643,36 @@@
  	case IORING_OFF_SQES:
  		ptr = ctx->sq_sqes;
  		break;
 +	case IORING_OFF_CQ_RING:
 +		ptr = ctx->cq_ring;
 +		break;
  	default:
- 		return -EINVAL;
+ 		return ERR_PTR(-EINVAL);
  	}
  
  	page = virt_to_head_page(ptr);
++<<<<<<< HEAD
 +	if (sz > (PAGE_SIZE << compound_order(page)))
 +		return -EINVAL;
++=======
+ 	if (sz > page_size(page))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	return ptr;
+ }
+ 
+ #ifdef CONFIG_MMU
+ 
+ static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
+ {
+ 	size_t sz = vma->vm_end - vma->vm_start;
+ 	unsigned long pfn;
+ 	void *ptr;
+ 
+ 	ptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);
+ 	if (IS_ERR(ptr))
+ 		return PTR_ERR(ptr);
++>>>>>>> 6c5c240e4126 (io_uring: add mapping support for NOMMU archs)
  
  	pfn = virt_to_phys(ptr) >> PAGE_SHIFT;
  	return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);
@@@ -3681,7 -4540,12 +3778,11 @@@ out_fput
  
  static const struct file_operations io_uring_fops = {
  	.release	= io_uring_release,
 -	.flush		= io_uring_flush,
  	.mmap		= io_uring_mmap,
+ #ifndef CONFIG_MMU
+ 	.get_unmapped_area = io_uring_nommu_get_unmapped_area,
+ 	.mmap_capabilities = io_uring_nommu_mmap_capabilities,
+ #endif
  	.poll		= io_uring_poll,
  	.fasync		= io_uring_fasync,
  };
* Unmerged path fs/io_uring.c
