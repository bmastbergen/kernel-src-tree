io_uring: Fix NULL pointer dereference in loop_rw_iter()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Guoyu Huang <hgy5945@gmail.com>
commit 2dd2111d0d383df104b144e0d1f6b5a00cb7cd88
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/2dd2111d.failed

loop_rw_iter() does not check whether the file has a read or
write function. This can lead to NULL pointer dereference
when the user passes in a file descriptor that does not have
read or write function.

The crash log looks like this:

[   99.834071] BUG: kernel NULL pointer dereference, address: 0000000000000000
[   99.835364] #PF: supervisor instruction fetch in kernel mode
[   99.836522] #PF: error_code(0x0010) - not-present page
[   99.837771] PGD 8000000079d62067 P4D 8000000079d62067 PUD 79d8c067 PMD 0
[   99.839649] Oops: 0010 [#2] SMP PTI
[   99.840591] CPU: 1 PID: 333 Comm: io_wqe_worker-0 Tainted: G      D           5.8.0 #2
[   99.842622] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1 04/01/2014
[   99.845140] RIP: 0010:0x0
[   99.845840] Code: Bad RIP value.
[   99.846672] RSP: 0018:ffffa1c7c01ebc08 EFLAGS: 00010202
[   99.848018] RAX: 0000000000000000 RBX: ffff92363bd67300 RCX: ffff92363d461208
[   99.849854] RDX: 0000000000000010 RSI: 00007ffdbf696bb0 RDI: ffff92363bd67300
[   99.851743] RBP: ffffa1c7c01ebc40 R08: 0000000000000000 R09: 0000000000000000
[   99.853394] R10: ffffffff9ec692a0 R11: 0000000000000000 R12: 0000000000000010
[   99.855148] R13: 0000000000000000 R14: ffff92363d461208 R15: ffffa1c7c01ebc68
[   99.856914] FS:  0000000000000000(0000) GS:ffff92363dd00000(0000) knlGS:0000000000000000
[   99.858651] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[   99.860032] CR2: ffffffffffffffd6 CR3: 000000007ac66000 CR4: 00000000000006e0
[   99.861979] Call Trace:
[   99.862617]  loop_rw_iter.part.0+0xad/0x110
[   99.863838]  io_write+0x2ae/0x380
[   99.864644]  ? kvm_sched_clock_read+0x11/0x20
[   99.865595]  ? sched_clock+0x9/0x10
[   99.866453]  ? sched_clock_cpu+0x11/0xb0
[   99.867326]  ? newidle_balance+0x1d4/0x3c0
[   99.868283]  io_issue_sqe+0xd8f/0x1340
[   99.869216]  ? __switch_to+0x7f/0x450
[   99.870280]  ? __switch_to_asm+0x42/0x70
[   99.871254]  ? __switch_to_asm+0x36/0x70
[   99.872133]  ? lock_timer_base+0x72/0xa0
[   99.873155]  ? switch_mm_irqs_off+0x1bf/0x420
[   99.874152]  io_wq_submit_work+0x64/0x180
[   99.875192]  ? kthread_use_mm+0x71/0x100
[   99.876132]  io_worker_handle_work+0x267/0x440
[   99.877233]  io_wqe_worker+0x297/0x350
[   99.878145]  kthread+0x112/0x150
[   99.878849]  ? __io_worker_unuse+0x100/0x100
[   99.879935]  ? kthread_park+0x90/0x90
[   99.880874]  ret_from_fork+0x22/0x30
[   99.881679] Modules linked in:
[   99.882493] CR2: 0000000000000000
[   99.883324] ---[ end trace 4453745f4673190b ]---
[   99.884289] RIP: 0010:0x0
[   99.884837] Code: Bad RIP value.
[   99.885492] RSP: 0018:ffffa1c7c01ebc08 EFLAGS: 00010202
[   99.886851] RAX: 0000000000000000 RBX: ffff92363acd7f00 RCX: ffff92363d461608
[   99.888561] RDX: 0000000000000010 RSI: 00007ffe040d9e10 RDI: ffff92363acd7f00
[   99.890203] RBP: ffffa1c7c01ebc40 R08: 0000000000000000 R09: 0000000000000000
[   99.891907] R10: ffffffff9ec692a0 R11: 0000000000000000 R12: 0000000000000010
[   99.894106] R13: 0000000000000000 R14: ffff92363d461608 R15: ffffa1c7c01ebc68
[   99.896079] FS:  0000000000000000(0000) GS:ffff92363dd00000(0000) knlGS:0000000000000000
[   99.898017] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[   99.899197] CR2: ffffffffffffffd6 CR3: 000000007ac66000 CR4: 00000000000006e0

Fixes: 32960613b7c3 ("io_uring: correctly handle non ->{read,write}_iter() file_operations")
	Cc: stable@vger.kernel.org
	Signed-off-by: Guoyu Huang <hgy5945@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 2dd2111d0d383df104b144e0d1f6b5a00cb7cd88)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index bff2058d3f07,8f96566603f3..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1342,66 -2868,271 +1342,133 @@@ static ssize_t loop_rw_iter(int rw, str
  	return ret;
  }
  
 -static void io_req_map_rw(struct io_kiocb *req, ssize_t io_size,
 -			  struct iovec *iovec, struct iovec *fast_iov,
 -			  struct iov_iter *iter)
 +static int io_read(struct io_kiocb *req, const struct sqe_submit *s,
 +		   bool force_nonblock)
  {
 -	struct io_async_rw *rw = &req->io->rw;
 -
 -	rw->nr_segs = iter->nr_segs;
 -	rw->size = io_size;
 -	if (!iovec) {
 -		rw->iov = rw->fast_iov;
 -		if (rw->iov != fast_iov)
 -			memcpy(rw->iov, fast_iov,
 -			       sizeof(struct iovec) * iter->nr_segs);
 -	} else {
 -		rw->iov = iovec;
 -		req->flags |= REQ_F_NEED_CLEANUP;
 -	}
 -}
 -
 -static inline int __io_alloc_async_ctx(struct io_kiocb *req)
 -{
 -	req->io = kmalloc(sizeof(*req->io), GFP_KERNEL);
 -	return req->io == NULL;
 -}
 -
 -static int io_alloc_async_ctx(struct io_kiocb *req)
 -{
 -	if (!io_op_defs[req->opcode].async_ctx)
 -		return 0;
 -
 -	return  __io_alloc_async_ctx(req);
 -}
 -
 -static int io_setup_async_rw(struct io_kiocb *req, ssize_t io_size,
 -			     struct iovec *iovec, struct iovec *fast_iov,
 -			     struct iov_iter *iter)
 -{
 -	if (!io_op_defs[req->opcode].async_ctx)
 -		return 0;
 -	if (!req->io) {
 -		if (__io_alloc_async_ctx(req))
 -			return -ENOMEM;
 -
 -		io_req_map_rw(req, io_size, iovec, fast_iov, iter);
 -	}
 -	return 0;
 -}
 -
 -static inline int io_rw_prep_async(struct io_kiocb *req, int rw,
 -				   bool force_nonblock)
 -{
 -	struct io_async_ctx *io = req->io;
 +	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 +	struct kiocb *kiocb = &req->rw;
  	struct iov_iter iter;
 -	ssize_t ret;
 -
 -	io->rw.iov = io->rw.fast_iov;
 -	req->io = NULL;
 -	ret = io_import_iovec(rw, req, &io->rw.iov, &iter, !force_nonblock);
 -	req->io = io;
 -	if (unlikely(ret < 0))
 -		return ret;
 -
 -	io_req_map_rw(req, ret, io->rw.iov, io->rw.fast_iov, &iter);
 -	return 0;
 -}
 -
 -static int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			bool force_nonblock)
 -{
 -	ssize_t ret;
 +	struct file *file;
 +	size_t iov_count;
 +	ssize_t read_size, ret;
  
 -	ret = io_prep_rw(req, sqe, force_nonblock);
 +	ret = io_prep_rw(req, s, force_nonblock);
  	if (ret)
  		return ret;
 +	file = kiocb->ki_filp;
  
 -	if (unlikely(!(req->file->f_mode & FMODE_READ)))
 +	if (unlikely(!(file->f_mode & FMODE_READ)))
  		return -EBADF;
  
 -	/* either don't need iovec imported or already have it */
 -	if (!req->io || req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -	return io_rw_prep_async(req, READ, force_nonblock);
 -}
 -
 -/*
 - * This is our waitqueue callback handler, registered through lock_page_async()
 - * when we initially tried to do the IO with the iocb armed our waitqueue.
 - * This gets called when the page is unlocked, and we generally expect that to
 - * happen when the page IO is completed and the page is now uptodate. This will
 - * queue a task_work based retry of the operation, attempting to copy the data
 - * again. If the latter fails because the page was NOT uptodate, then we will
 - * do a thread based blocking retry of the operation. That's the unexpected
 - * slow path.
 - */
 -static int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,
 -			     int sync, void *arg)
 -{
 -	struct wait_page_queue *wpq;
 -	struct io_kiocb *req = wait->private;
 -	struct wait_page_key *key = arg;
 -	int ret;
 -
 -	wpq = container_of(wait, struct wait_page_queue, wait);
 -
 -	if (!wake_page_match(wpq, key))
 -		return 0;
 -
 -	list_del_init(&wait->entry);
 -
 -	init_task_work(&req->task_work, io_req_task_submit);
 -	/* submit ref gets dropped, acquire a new one */
 -	refcount_inc(&req->refs);
 -	ret = io_req_task_work_add(req, &req->task_work);
 -	if (unlikely(ret)) {
 -		struct task_struct *tsk;
 -
 -		/* queue just for cancelation */
 -		init_task_work(&req->task_work, io_req_task_cancel);
 -		tsk = io_wq_get_task(req->ctx->io_wq);
 -		task_work_add(tsk, &req->task_work, 0);
 -		wake_up_process(tsk);
 -	}
 -	return 1;
 -}
 -
 -static inline int kiocb_wait_page_queue_init(struct kiocb *kiocb,
 -					     struct wait_page_queue *wait,
 -					     wait_queue_func_t func,
 -					     void *data)
 -{
 -	/* Can't support async wakeup with polled IO */
 -	if (kiocb->ki_flags & IOCB_HIPRI)
 -		return -EINVAL;
 -	if (kiocb->ki_filp->f_mode & FMODE_BUF_RASYNC) {
 -		wait->wait.func = func;
 -		wait->wait.private = data;
 -		wait->wait.flags = 0;
 -		INIT_LIST_HEAD(&wait->wait.entry);
 -		kiocb->ki_flags |= IOCB_WAITQ;
 -		kiocb->ki_waitq = wait;
 -		return 0;
 -	}
 -
 -	return -EOPNOTSUPP;
 -}
 -
 -/*
 - * This controls whether a given IO request should be armed for async page
 - * based retry. If we return false here, the request is handed to the async
 - * worker threads for retry. If we're doing buffered reads on a regular file,
 - * we prepare a private wait_page_queue entry and retry the operation. This
 - * will either succeed because the page is now uptodate and unlocked, or it
 - * will register a callback when the page is unlocked at IO completion. Through
 - * that callback, io_uring uses task_work to setup a retry of the operation.
 - * That retry will attempt the buffered read again. The retry will generally
 - * succeed, or in rare cases where it fails, we then fall back to using the
 - * async worker threads for a blocking retry.
 - */
 -static bool io_rw_should_retry(struct io_kiocb *req)
 -{
 -	struct kiocb *kiocb = &req->rw.kiocb;
 -	int ret;
 -
 -	/* never retry for NOWAIT, we just complete with -EAGAIN */
 -	if (req->flags & REQ_F_NOWAIT)
 -		return false;
 -
 -	/* already tried, or we're doing O_DIRECT */
 -	if (kiocb->ki_flags & (IOCB_DIRECT | IOCB_WAITQ))
 -		return false;
 -	/*
 -	 * just use poll if we can, and don't attempt if the fs doesn't
 -	 * support callback based unlocks
 -	 */
 -	if (file_can_poll(req->file) || !(req->file->f_mode & FMODE_BUF_RASYNC))
 -		return false;
 +	ret = io_import_iovec(req->ctx, READ, s, &iovec, &iter);
 +	if (ret < 0)
 +		return ret;
  
 -	/*
 -	 * If request type doesn't require req->io to defer in general,
 -	 * we need to allocate it here
 -	 */
 -	if (!req->io && __io_alloc_async_ctx(req))
 -		return false;
 +	read_size = ret;
 +	if (req->flags & REQ_F_LINK)
 +		req->result = read_size;
  
 -	ret = kiocb_wait_page_queue_init(kiocb, &req->io->rw.wpq,
 -						io_async_buf_func, req);
 +	iov_count = iov_iter_count(&iter);
 +	ret = rw_verify_area(READ, file, &kiocb->ki_pos, iov_count);
  	if (!ret) {
 -		io_get_req_task(req);
 -		return true;
 -	}
 +		ssize_t ret2;
  
 -	return false;
 -}
 +		if (file->f_op->read_iter)
 +			ret2 = call_read_iter(file, kiocb, &iter);
 +		else
 +			ret2 = loop_rw_iter(READ, file, kiocb, &iter);
  
++<<<<<<< HEAD
 +		/*
 +		 * In case of a short read, punt to async. This can happen
 +		 * if we have data partially cached. Alternatively we can
 +		 * return the short read, in which case the application will
 +		 * need to issue another SQE and wait for it. That SQE will
 +		 * need async punt anyway, so it's more efficient to do it
 +		 * here.
 +		 */
 +		if (force_nonblock && ret2 > 0 && ret2 < read_size)
 +			ret2 = -EAGAIN;
 +		/* Catch -EAGAIN return for forced non-blocking submission */
 +		if (!force_nonblock || ret2 != -EAGAIN) {
 +			io_rw_done(kiocb, ret2);
 +		} else {
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(READ, req, iov_count);
 +			ret = -EAGAIN;
++=======
+ static int io_iter_do_read(struct io_kiocb *req, struct iov_iter *iter)
+ {
+ 	if (req->file->f_op->read_iter)
+ 		return call_read_iter(req->file, &req->rw.kiocb, iter);
+ 	else if (req->file->f_op->read)
+ 		return loop_rw_iter(READ, req->file, &req->rw.kiocb, iter);
+ 	else
+ 		return -EINVAL;
+ }
+ 
+ static int io_read(struct io_kiocb *req, bool force_nonblock,
+ 		   struct io_comp_state *cs)
+ {
+ 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
+ 	struct kiocb *kiocb = &req->rw.kiocb;
+ 	struct iov_iter iter;
+ 	size_t iov_count;
+ 	ssize_t io_size, ret, ret2;
+ 	unsigned long nr_segs;
+ 
+ 	ret = io_import_iovec(READ, req, &iovec, &iter, !force_nonblock);
+ 	if (ret < 0)
+ 		return ret;
+ 	io_size = ret;
+ 	req->result = io_size;
+ 
+ 	/* Ensure we clear previously set non-block flag */
+ 	if (!force_nonblock)
+ 		kiocb->ki_flags &= ~IOCB_NOWAIT;
+ 
+ 	/* If the file doesn't support async, just async punt */
+ 	if (force_nonblock && !io_file_supports_async(req->file, READ))
+ 		goto copy_iov;
+ 
+ 	iov_count = iov_iter_count(&iter);
+ 	nr_segs = iter.nr_segs;
+ 	ret = rw_verify_area(READ, req->file, &kiocb->ki_pos, iov_count);
+ 	if (unlikely(ret))
+ 		goto out_free;
+ 
+ 	ret2 = io_iter_do_read(req, &iter);
+ 
+ 	/* Catch -EAGAIN return for forced non-blocking submission */
+ 	if (!force_nonblock || (ret2 != -EAGAIN && ret2 != -EIO)) {
+ 		kiocb_done(kiocb, ret2, cs);
+ 	} else {
+ 		iter.count = iov_count;
+ 		iter.nr_segs = nr_segs;
+ copy_iov:
+ 		ret = io_setup_async_rw(req, io_size, iovec, inline_vecs,
+ 					&iter);
+ 		if (ret)
+ 			goto out_free;
+ 		/* it's copied and will be cleaned with ->io */
+ 		iovec = NULL;
+ 		/* if we can retry, do so with the callbacks armed */
+ 		if (io_rw_should_retry(req)) {
+ 			ret2 = io_iter_do_read(req, &iter);
+ 			if (ret2 == -EIOCBQUEUED) {
+ 				goto out_free;
+ 			} else if (ret2 != -EAGAIN) {
+ 				kiocb_done(kiocb, ret2, cs);
+ 				goto out_free;
+ 			}
++>>>>>>> 2dd2111d0d38 (io_uring: Fix NULL pointer dereference in loop_rw_iter())
  		}
 -		kiocb->ki_flags &= ~IOCB_WAITQ;
 -		return -EAGAIN;
  	}
 -out_free:
 -	if (iovec)
 -		kfree(iovec);
 +	kfree(iovec);
  	return ret;
  }
  
@@@ -1419,64 -3145,95 +1486,73 @@@ static int io_write(struct io_kiocb *re
  	if (ret)
  		return ret;
  
 -	if (unlikely(!(req->file->f_mode & FMODE_WRITE)))
 +	file = kiocb->ki_filp;
 +	if (unlikely(!(file->f_mode & FMODE_WRITE)))
  		return -EBADF;
  
 -	/* either don't need iovec imported or already have it */
 -	if (!req->io || req->flags & REQ_F_NEED_CLEANUP)
 -		return 0;
 -	return io_rw_prep_async(req, WRITE, force_nonblock);
 -}
 -
 -static int io_write(struct io_kiocb *req, bool force_nonblock,
 -		    struct io_comp_state *cs)
 -{
 -	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
 -	struct kiocb *kiocb = &req->rw.kiocb;
 -	struct iov_iter iter;
 -	size_t iov_count;
 -	ssize_t ret, ret2, io_size;
 -	unsigned long nr_segs;
 -
 -	ret = io_import_iovec(WRITE, req, &iovec, &iter, !force_nonblock);
 +	ret = io_import_iovec(req->ctx, WRITE, s, &iovec, &iter);
  	if (ret < 0)
  		return ret;
 -	io_size = ret;
 -	req->result = io_size;
 -
 -	/* Ensure we clear previously set non-block flag */
 -	if (!force_nonblock)
 -		req->rw.kiocb.ki_flags &= ~IOCB_NOWAIT;
 -
 -	/* If the file doesn't support async, just async punt */
 -	if (force_nonblock && !io_file_supports_async(req->file, WRITE))
 -		goto copy_iov;
  
 -	/* file path doesn't support NOWAIT for non-direct_IO */
 -	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&
 -	    (req->flags & REQ_F_ISREG))
 -		goto copy_iov;
 +	if (req->flags & REQ_F_LINK)
 +		req->result = ret;
  
  	iov_count = iov_iter_count(&iter);
 -	nr_segs = iter.nr_segs;
 -	ret = rw_verify_area(WRITE, req->file, &kiocb->ki_pos, iov_count);
 -	if (unlikely(ret))
 -		goto out_free;
  
 -	/*
 -	 * Open-code file_start_write here to grab freeze protection,
 -	 * which will be released by another thread in
 -	 * io_complete_rw().  Fool lockdep by telling it the lock got
 -	 * released so that it doesn't complain about the held lock when
 -	 * we return to userspace.
 -	 */
 -	if (req->flags & REQ_F_ISREG) {
 -		__sb_start_write(file_inode(req->file)->i_sb,
 -					SB_FREEZE_WRITE, true);
 -		__sb_writers_release(file_inode(req->file)->i_sb,
 -					SB_FREEZE_WRITE);
 +	ret = -EAGAIN;
 +	if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT)) {
 +		/* If ->needs_lock is true, we're already in async context. */
 +		if (!s->needs_lock)
 +			io_async_list_note(WRITE, req, iov_count);
 +		goto out_free;
  	}
 -	kiocb->ki_flags |= IOCB_WRITE;
  
++<<<<<<< HEAD
 +	ret = rw_verify_area(WRITE, file, &kiocb->ki_pos, iov_count);
 +	if (!ret) {
 +		ssize_t ret2;
++=======
+ 	if (req->file->f_op->write_iter)
+ 		ret2 = call_write_iter(req->file, kiocb, &iter);
+ 	else if (req->file->f_op->write)
+ 		ret2 = loop_rw_iter(WRITE, req->file, kiocb, &iter);
+ 	else
+ 		ret2 = -EINVAL;
++>>>>>>> 2dd2111d0d38 (io_uring: Fix NULL pointer dereference in loop_rw_iter())
  
 -	/*
 -	 * Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just
 -	 * retry them without IOCB_NOWAIT.
 -	 */
 -	if (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT))
 -		ret2 = -EAGAIN;
 -	if (!force_nonblock || ret2 != -EAGAIN) {
 -		kiocb_done(kiocb, ret2, cs);
 -	} else {
 -		iter.count = iov_count;
 -		iter.nr_segs = nr_segs;
 -copy_iov:
 -		ret = io_setup_async_rw(req, io_size, iovec, inline_vecs,
 -					&iter);
 -		if (ret)
 -			goto out_free;
 -		/* it's copied and will be cleaned with ->io */
 -		iovec = NULL;
 -		return -EAGAIN;
 +		/*
 +		 * Open-code file_start_write here to grab freeze protection,
 +		 * which will be released by another thread in
 +		 * io_complete_rw().  Fool lockdep by telling it the lock got
 +		 * released so that it doesn't complain about the held lock when
 +		 * we return to userspace.
 +		 */
 +		if (S_ISREG(file_inode(file)->i_mode)) {
 +			__sb_start_write(file_inode(file)->i_sb,
 +						SB_FREEZE_WRITE, true);
 +			__sb_writers_release(file_inode(file)->i_sb,
 +						SB_FREEZE_WRITE);
 +		}
 +		kiocb->ki_flags |= IOCB_WRITE;
 +
 +		if (file->f_op->write_iter)
 +			ret2 = call_write_iter(file, kiocb, &iter);
 +		else
 +			ret2 = loop_rw_iter(WRITE, file, kiocb, &iter);
 +		if (!force_nonblock || ret2 != -EAGAIN) {
 +			io_rw_done(kiocb, ret2);
 +		} else {
 +			/*
 +			 * If ->needs_lock is true, we're already in async
 +			 * context.
 +			 */
 +			if (!s->needs_lock)
 +				io_async_list_note(WRITE, req, iov_count);
 +			ret = -EAGAIN;
 +		}
  	}
  out_free:
 -	if (iovec)
 -		kfree(iovec);
 +	kfree(iovec);
  	return ret;
  }
  
* Unmerged path fs/io_uring.c
