x86/vmware: Enable steal time accounting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
Rebuild_CHGLOG: - [x86] vmware: Enable steal time accounting (Vitaly Kuznetsov) [1807448]
Rebuild_FUZZ: 94.74%
commit-author Alexey Makhalov <amakhalov@vmware.com>
commit e73a8f38f82dd1c41b70a06556bea7dc250cc384
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/e73a8f38.failed

Set paravirt_steal_rq_enabled if steal clock present.
paravirt_steal_rq_enabled is used in sched/core.c to adjust task
progress by offsetting stolen time. Use 'no-steal-acc' off switch (share
same name with KVM) to disable steal time accounting.

	Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20200323195707.31242-5-amakhalov@vmware.com
(cherry picked from commit e73a8f38f82dd1c41b70a06556bea7dc250cc384)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/admin-guide/kernel-parameters.txt
#	arch/x86/kernel/cpu/vmware.c
diff --cc Documentation/admin-guide/kernel-parameters.txt
index 400fa632735e,863b37d6bd2e..000000000000
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@@ -3060,9 -3174,9 +3060,15 @@@
  			[X86,PV_OPS] Disable paravirtualized VMware scheduler
  			clock and use the default one.
  
++<<<<<<< HEAD
 +	no-steal-acc	[X86,KVM] Disable paravirtualized steal time accounting.
 +			steal time is computed, but won't influence scheduler
 +			behaviour
++=======
+ 	no-steal-acc	[X86,PV_OPS,ARM64] Disable paravirtualized steal time
+ 			accounting. steal time is computed, but won't
+ 			influence scheduler behaviour
++>>>>>>> e73a8f38f82d (x86/vmware: Enable steal time accounting)
  
  	nolapic		[X86-32,APIC] Do not enable or use the local APIC.
  
diff --cc arch/x86/kernel/cpu/vmware.c
index 8ff3b6f9b52a,e885f73bebd4..000000000000
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@@ -104,6 -123,9 +104,12 @@@ static unsigned long vmware_get_tsc_khz
  #ifdef CONFIG_PARAVIRT
  static struct cyc2ns_data vmware_cyc2ns __ro_after_init;
  static int vmw_sched_clock __initdata = 1;
++<<<<<<< HEAD
++=======
+ static DEFINE_PER_CPU_DECRYPTED(struct vmware_steal_time, vmw_steal_time) __aligned(64);
+ static bool has_steal_clock;
+ static bool steal_acc __initdata = true; /* steal time accounting */
++>>>>>>> e73a8f38f82d (x86/vmware: Enable steal time accounting)
  
  static __init int setup_vmw_sched_clock(char *s)
  {
@@@ -132,10 -161,169 +145,173 @@@ static void __init vmware_sched_clock_s
  	d->cyc2ns_offset = mul_u64_u32_shr(tsc_now, d->cyc2ns_mul,
  					   d->cyc2ns_shift);
  
 -	pr_info("using clock offset of %llu ns\n", d->cyc2ns_offset);
 +	pv_time_ops.sched_clock = vmware_sched_clock;
 +	pr_info("using sched offset of %llu ns\n", d->cyc2ns_offset);
  }
  
++<<<<<<< HEAD
++=======
+ static int vmware_cmd_stealclock(uint32_t arg1, uint32_t arg2)
+ {
+ 	uint32_t result, info;
+ 
+ 	asm volatile (VMWARE_HYPERCALL :
+ 		"=a"(result),
+ 		"=c"(info) :
+ 		"a"(VMWARE_HYPERVISOR_MAGIC),
+ 		"b"(0),
+ 		"c"(VMWARE_CMD_STEALCLOCK),
+ 		"d"(0),
+ 		"S"(arg1),
+ 		"D"(arg2) :
+ 		"memory");
+ 	return result;
+ }
+ 
+ static bool stealclock_enable(phys_addr_t pa)
+ {
+ 	return vmware_cmd_stealclock(upper_32_bits(pa),
+ 				     lower_32_bits(pa)) == STEALCLOCK_ENABLED;
+ }
+ 
+ static int __stealclock_disable(void)
+ {
+ 	return vmware_cmd_stealclock(0, 1);
+ }
+ 
+ static void stealclock_disable(void)
+ {
+ 	__stealclock_disable();
+ }
+ 
+ static bool vmware_is_stealclock_available(void)
+ {
+ 	return __stealclock_disable() != STEALCLOCK_NOT_AVAILABLE;
+ }
+ 
+ /**
+  * vmware_steal_clock() - read the per-cpu steal clock
+  * @cpu:            the cpu number whose steal clock we want to read
+  *
+  * The function reads the steal clock if we are on a 64-bit system, otherwise
+  * reads it in parts, checking that the high part didn't change in the
+  * meantime.
+  *
+  * Return:
+  *      The steal clock reading in ns.
+  */
+ static uint64_t vmware_steal_clock(int cpu)
+ {
+ 	struct vmware_steal_time *steal = &per_cpu(vmw_steal_time, cpu);
+ 	uint64_t clock;
+ 
+ 	if (IS_ENABLED(CONFIG_64BIT))
+ 		clock = READ_ONCE(steal->clock);
+ 	else {
+ 		uint32_t initial_high, low, high;
+ 
+ 		do {
+ 			initial_high = READ_ONCE(steal->clock_high);
+ 			/* Do not reorder initial_high and high readings */
+ 			virt_rmb();
+ 			low = READ_ONCE(steal->clock_low);
+ 			/* Keep low reading in between */
+ 			virt_rmb();
+ 			high = READ_ONCE(steal->clock_high);
+ 		} while (initial_high != high);
+ 
+ 		clock = ((uint64_t)high << 32) | low;
+ 	}
+ 
+ 	return mul_u64_u32_shr(clock, vmware_cyc2ns.cyc2ns_mul,
+ 			     vmware_cyc2ns.cyc2ns_shift);
+ }
+ 
+ static void vmware_register_steal_time(void)
+ {
+ 	int cpu = smp_processor_id();
+ 	struct vmware_steal_time *st = &per_cpu(vmw_steal_time, cpu);
+ 
+ 	if (!has_steal_clock)
+ 		return;
+ 
+ 	if (!stealclock_enable(slow_virt_to_phys(st))) {
+ 		has_steal_clock = false;
+ 		return;
+ 	}
+ 
+ 	pr_info("vmware-stealtime: cpu %d, pa %llx\n",
+ 		cpu, (unsigned long long) slow_virt_to_phys(st));
+ }
+ 
+ static void vmware_disable_steal_time(void)
+ {
+ 	if (!has_steal_clock)
+ 		return;
+ 
+ 	stealclock_disable();
+ }
+ 
+ static void vmware_guest_cpu_init(void)
+ {
+ 	if (has_steal_clock)
+ 		vmware_register_steal_time();
+ }
+ 
+ static void vmware_pv_guest_cpu_reboot(void *unused)
+ {
+ 	vmware_disable_steal_time();
+ }
+ 
+ static int vmware_pv_reboot_notify(struct notifier_block *nb,
+ 				unsigned long code, void *unused)
+ {
+ 	if (code == SYS_RESTART)
+ 		on_each_cpu(vmware_pv_guest_cpu_reboot, NULL, 1);
+ 	return NOTIFY_DONE;
+ }
+ 
+ static struct notifier_block vmware_pv_reboot_nb = {
+ 	.notifier_call = vmware_pv_reboot_notify,
+ };
+ 
+ #ifdef CONFIG_SMP
+ static void __init vmware_smp_prepare_boot_cpu(void)
+ {
+ 	vmware_guest_cpu_init();
+ 	native_smp_prepare_boot_cpu();
+ }
+ 
+ static int vmware_cpu_online(unsigned int cpu)
+ {
+ 	local_irq_disable();
+ 	vmware_guest_cpu_init();
+ 	local_irq_enable();
+ 	return 0;
+ }
+ 
+ static int vmware_cpu_down_prepare(unsigned int cpu)
+ {
+ 	local_irq_disable();
+ 	vmware_disable_steal_time();
+ 	local_irq_enable();
+ 	return 0;
+ }
+ #endif
+ 
+ static __init int activate_jump_labels(void)
+ {
+ 	if (has_steal_clock) {
+ 		static_key_slow_inc(&paravirt_steal_enabled);
+ 		if (steal_acc)
+ 			static_key_slow_inc(&paravirt_steal_rq_enabled);
+ 	}
+ 
+ 	return 0;
+ }
+ arch_initcall(activate_jump_labels);
+ 
++>>>>>>> e73a8f38f82d (x86/vmware: Enable steal time accounting)
  static void __init vmware_paravirt_ops_setup(void)
  {
  	pv_info.name = "VMware hypervisor";
* Unmerged path Documentation/admin-guide/kernel-parameters.txt
* Unmerged path arch/x86/kernel/cpu/vmware.c
