net/smc: do not call dma sync for unmapped memory

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Karsten Graul <kgraul@linux.ibm.com>
commit 741a49a4dc5fd7e61b37b259dde915083c2c5327
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/741a49a4.failed

The dma related ...sync_sg... functions check the link state before the
dma function is actually called. But the check in smc_link_usable()
allows links in ACTIVATING state which are not yet mapped to dma memory.
Under high load it may happen that the sync_sg functions are called for
such a link which results in an debug output like
   DMA-API: mlx5_core 0002:00:00.0: device driver tries to sync
   DMA memory it has not allocated [device address=0x0000000103370000]
   [size=65536 bytes]
To fix that introduce a helper to check for the link state ACTIVE and
use it where appropriate. And move the link state update to ACTIVATING
to the end of smcr_link_init() when most initial setup is done.

	Reviewed-by: Ursula Braun <ubraun@linux.ibm.com>
Fixes: d854fcbfaeda ("net/smc: add new link state and related helpers")
	Signed-off-by: Karsten Graul <kgraul@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 741a49a4dc5fd7e61b37b259dde915083c2c5327)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/smc/smc_core.c
#	net/smc/smc_llc.c
diff --cc net/smc/smc_core.c
index 399bc3ffb64e,42ba227f3e97..000000000000
--- a/net/smc/smc_core.c
+++ b/net/smc/smc_core.c
@@@ -476,10 -463,150 +476,153 @@@ out
  	return rc;
  }
  
++<<<<<<< HEAD
++=======
+ static int smc_write_space(struct smc_connection *conn)
+ {
+ 	int buffer_len = conn->peer_rmbe_size;
+ 	union smc_host_cursor prod;
+ 	union smc_host_cursor cons;
+ 	int space;
+ 
+ 	smc_curs_copy(&prod, &conn->local_tx_ctrl.prod, conn);
+ 	smc_curs_copy(&cons, &conn->local_rx_ctrl.cons, conn);
+ 	/* determine rx_buf space */
+ 	space = buffer_len - smc_curs_diff(buffer_len, &cons, &prod);
+ 	return space;
+ }
+ 
+ static int smc_switch_cursor(struct smc_sock *smc, struct smc_cdc_tx_pend *pend,
+ 			     struct smc_wr_buf *wr_buf)
+ {
+ 	struct smc_connection *conn = &smc->conn;
+ 	union smc_host_cursor cons, fin;
+ 	int rc = 0;
+ 	int diff;
+ 
+ 	smc_curs_copy(&conn->tx_curs_sent, &conn->tx_curs_fin, conn);
+ 	smc_curs_copy(&fin, &conn->local_tx_ctrl_fin, conn);
+ 	/* set prod cursor to old state, enforce tx_rdma_writes() */
+ 	smc_curs_copy(&conn->local_tx_ctrl.prod, &fin, conn);
+ 	smc_curs_copy(&cons, &conn->local_rx_ctrl.cons, conn);
+ 
+ 	if (smc_curs_comp(conn->peer_rmbe_size, &cons, &fin) < 0) {
+ 		/* cons cursor advanced more than fin, and prod was set
+ 		 * fin above, so now prod is smaller than cons. Fix that.
+ 		 */
+ 		diff = smc_curs_diff(conn->peer_rmbe_size, &fin, &cons);
+ 		smc_curs_add(conn->sndbuf_desc->len,
+ 			     &conn->tx_curs_sent, diff);
+ 		smc_curs_add(conn->sndbuf_desc->len,
+ 			     &conn->tx_curs_fin, diff);
+ 
+ 		smp_mb__before_atomic();
+ 		atomic_add(diff, &conn->sndbuf_space);
+ 		smp_mb__after_atomic();
+ 
+ 		smc_curs_add(conn->peer_rmbe_size,
+ 			     &conn->local_tx_ctrl.prod, diff);
+ 		smc_curs_add(conn->peer_rmbe_size,
+ 			     &conn->local_tx_ctrl_fin, diff);
+ 	}
+ 	/* recalculate, value is used by tx_rdma_writes() */
+ 	atomic_set(&smc->conn.peer_rmbe_space, smc_write_space(conn));
+ 
+ 	if (smc->sk.sk_state != SMC_INIT &&
+ 	    smc->sk.sk_state != SMC_CLOSED) {
+ 		rc = smcr_cdc_msg_send_validation(conn, pend, wr_buf);
+ 		if (!rc) {
+ 			schedule_delayed_work(&conn->tx_work, 0);
+ 			smc->sk.sk_data_ready(&smc->sk);
+ 		}
+ 	} else {
+ 		smc_wr_tx_put_slot(conn->lnk,
+ 				   (struct smc_wr_tx_pend_priv *)pend);
+ 	}
+ 	return rc;
+ }
+ 
+ struct smc_link *smc_switch_conns(struct smc_link_group *lgr,
+ 				  struct smc_link *from_lnk, bool is_dev_err)
+ {
+ 	struct smc_link *to_lnk = NULL;
+ 	struct smc_cdc_tx_pend *pend;
+ 	struct smc_connection *conn;
+ 	struct smc_wr_buf *wr_buf;
+ 	struct smc_sock *smc;
+ 	struct rb_node *node;
+ 	int i, rc = 0;
+ 
+ 	/* link is inactive, wake up tx waiters */
+ 	smc_wr_wakeup_tx_wait(from_lnk);
+ 
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		if (!smc_link_active(&lgr->lnk[i]) || i == from_lnk->link_idx)
+ 			continue;
+ 		if (is_dev_err && from_lnk->smcibdev == lgr->lnk[i].smcibdev &&
+ 		    from_lnk->ibport == lgr->lnk[i].ibport) {
+ 			continue;
+ 		}
+ 		to_lnk = &lgr->lnk[i];
+ 		break;
+ 	}
+ 	if (!to_lnk) {
+ 		smc_lgr_terminate_sched(lgr);
+ 		return NULL;
+ 	}
+ again:
+ 	read_lock_bh(&lgr->conns_lock);
+ 	for (node = rb_first(&lgr->conns_all); node; node = rb_next(node)) {
+ 		conn = rb_entry(node, struct smc_connection, alert_node);
+ 		if (conn->lnk != from_lnk)
+ 			continue;
+ 		smc = container_of(conn, struct smc_sock, conn);
+ 		/* conn->lnk not yet set in SMC_INIT state */
+ 		if (smc->sk.sk_state == SMC_INIT)
+ 			continue;
+ 		if (smc->sk.sk_state == SMC_CLOSED ||
+ 		    smc->sk.sk_state == SMC_PEERCLOSEWAIT1 ||
+ 		    smc->sk.sk_state == SMC_PEERCLOSEWAIT2 ||
+ 		    smc->sk.sk_state == SMC_APPFINCLOSEWAIT ||
+ 		    smc->sk.sk_state == SMC_APPCLOSEWAIT1 ||
+ 		    smc->sk.sk_state == SMC_APPCLOSEWAIT2 ||
+ 		    smc->sk.sk_state == SMC_PEERFINCLOSEWAIT ||
+ 		    smc->sk.sk_state == SMC_PEERABORTWAIT ||
+ 		    smc->sk.sk_state == SMC_PROCESSABORT) {
+ 			spin_lock_bh(&conn->send_lock);
+ 			conn->lnk = to_lnk;
+ 			spin_unlock_bh(&conn->send_lock);
+ 			continue;
+ 		}
+ 		sock_hold(&smc->sk);
+ 		read_unlock_bh(&lgr->conns_lock);
+ 		/* pre-fetch buffer outside of send_lock, might sleep */
+ 		rc = smc_cdc_get_free_slot(conn, to_lnk, &wr_buf, NULL, &pend);
+ 		if (rc) {
+ 			smcr_link_down_cond_sched(to_lnk);
+ 			return NULL;
+ 		}
+ 		/* avoid race with smcr_tx_sndbuf_nonempty() */
+ 		spin_lock_bh(&conn->send_lock);
+ 		conn->lnk = to_lnk;
+ 		rc = smc_switch_cursor(smc, pend, wr_buf);
+ 		spin_unlock_bh(&conn->send_lock);
+ 		sock_put(&smc->sk);
+ 		if (rc) {
+ 			smcr_link_down_cond_sched(to_lnk);
+ 			return NULL;
+ 		}
+ 		goto again;
+ 	}
+ 	read_unlock_bh(&lgr->conns_lock);
+ 	return to_lnk;
+ }
+ 
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  static void smcr_buf_unuse(struct smc_buf_desc *rmb_desc,
 -			   struct smc_link_group *lgr)
 +			   struct smc_link *lnk)
  {
 -	int rc;
 +	struct smc_link_group *lgr = lnk->lgr;
  
  	if (rmb_desc->is_conf_rkey && !list_empty(&lgr->list)) {
  		/* unregister rmb with peer */
@@@ -1422,14 -1716,14 +1565,22 @@@ static int __smc_buf_create(struct smc_
  
  void smc_sndbuf_sync_sg_for_cpu(struct smc_connection *conn)
  {
++<<<<<<< HEAD
 +	if (!conn->lgr || conn->lgr->is_smcd)
++=======
+ 	if (!conn->lgr || conn->lgr->is_smcd || !smc_link_active(conn->lnk))
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  		return;
  	smc_ib_sync_sg_for_cpu(conn->lnk, conn->sndbuf_desc, DMA_TO_DEVICE);
  }
  
  void smc_sndbuf_sync_sg_for_device(struct smc_connection *conn)
  {
++<<<<<<< HEAD
 +	if (!conn->lgr || conn->lgr->is_smcd)
++=======
+ 	if (!conn->lgr || conn->lgr->is_smcd || !smc_link_active(conn->lnk))
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  		return;
  	smc_ib_sync_sg_for_device(conn->lnk, conn->sndbuf_desc, DMA_TO_DEVICE);
  }
@@@ -1441,8 -1735,7 +1592,12 @@@ void smc_rmb_sync_sg_for_cpu(struct smc
  	if (!conn->lgr || conn->lgr->is_smcd)
  		return;
  	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
++<<<<<<< HEAD
 +		if (conn->lgr->lnk[i].state != SMC_LNK_ACTIVE &&
 +		    conn->lgr->lnk[i].state != SMC_LNK_ACTIVATING)
++=======
+ 		if (!smc_link_active(&conn->lgr->lnk[i]))
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  			continue;
  		smc_ib_sync_sg_for_cpu(&conn->lgr->lnk[i], conn->rmb_desc,
  				       DMA_FROM_DEVICE);
@@@ -1456,8 -1749,7 +1611,12 @@@ void smc_rmb_sync_sg_for_device(struct 
  	if (!conn->lgr || conn->lgr->is_smcd)
  		return;
  	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
++<<<<<<< HEAD
 +		if (conn->lgr->lnk[i].state != SMC_LNK_ACTIVE &&
 +		    conn->lgr->lnk[i].state != SMC_LNK_ACTIVATING)
++=======
+ 		if (!smc_link_active(&conn->lgr->lnk[i]))
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  			continue;
  		smc_ib_sync_sg_for_device(&conn->lgr->lnk[i], conn->rmb_desc,
  					  DMA_FROM_DEVICE);
diff --cc net/smc/smc_llc.c
index 4119cdb6b6bf,df5b0a6ea848..000000000000
--- a/net/smc/smc_llc.c
+++ b/net/smc/smc_llc.c
@@@ -230,12 -424,28 +230,31 @@@ static int smc_llc_send_confirm_rkey(st
  	memset(rkeyllc, 0, sizeof(*rkeyllc));
  	rkeyllc->hd.common.type = SMC_LLC_CONFIRM_RKEY;
  	rkeyllc->hd.length = sizeof(struct smc_llc_msg_confirm_rkey);
++<<<<<<< HEAD
++=======
+ 
+ 	rtok_ix = 1;
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		link = &send_link->lgr->lnk[i];
+ 		if (smc_link_active(link) && link != send_link) {
+ 			rkeyllc->rtoken[rtok_ix].link_id = link->link_id;
+ 			rkeyllc->rtoken[rtok_ix].rmb_key =
+ 				htonl(rmb_desc->mr_rx[link->link_idx]->rkey);
+ 			rkeyllc->rtoken[rtok_ix].rmb_vaddr = cpu_to_be64(
+ 				(u64)sg_dma_address(
+ 					rmb_desc->sgt[link->link_idx].sgl));
+ 			rtok_ix++;
+ 		}
+ 	}
+ 	/* rkey of send_link is in rtoken[0] */
+ 	rkeyllc->rtoken[0].num_rkeys = rtok_ix - 1;
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  	rkeyllc->rtoken[0].rmb_key =
 -		htonl(rmb_desc->mr_rx[send_link->link_idx]->rkey);
 +		htonl(rmb_desc->mr_rx[link->link_idx]->rkey);
  	rkeyllc->rtoken[0].rmb_vaddr = cpu_to_be64(
 -		(u64)sg_dma_address(rmb_desc->sgt[send_link->link_idx].sgl));
 +		(u64)sg_dma_address(rmb_desc->sgt[link->link_idx].sgl));
  	/* send llc message */
 -	rc = smc_wr_tx_send(send_link, pend);
 +	rc = smc_wr_tx_send(link, pend);
  	return rc;
  }
  
@@@ -398,96 -821,501 +417,167 @@@ static int smc_llc_send_message(struct 
  	return 0;
  }
  
 -static void smc_llc_save_add_link_info(struct smc_link *link,
 -				       struct smc_llc_msg_add_link *add_llc)
 -{
 -	link->peer_qpn = ntoh24(add_llc->sender_qp_num);
 -	memcpy(link->peer_gid, add_llc->sender_gid, SMC_GID_SIZE);
 -	memcpy(link->peer_mac, add_llc->sender_mac, ETH_ALEN);
 -	link->peer_psn = ntoh24(add_llc->initial_psn);
 -	link->peer_mtu = add_llc->qp_mtu;
 -}
 +/********************************* receive ***********************************/
  
 -/* as an SMC client, process an add link request */
 -int smc_llc_cli_add_link(struct smc_link *link, struct smc_llc_qentry *qentry)
 +static void smc_llc_rx_confirm_link(struct smc_link *link,
 +				    struct smc_llc_msg_confirm_link *llc)
  {
 -	struct smc_llc_msg_add_link *llc = &qentry->msg.add_link;
 -	enum smc_lgr_type lgr_new_t = SMC_LGR_SYMMETRIC;
  	struct smc_link_group *lgr = smc_get_lgr(link);
 -	struct smc_link *lnk_new = NULL;
 -	struct smc_init_info ini;
 -	int lnk_idx, rc = 0;
 -
 -	ini.vlan_id = lgr->vlan_id;
 -	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
 -	if (!memcmp(llc->sender_gid, link->peer_gid, SMC_GID_SIZE) &&
 -	    !memcmp(llc->sender_mac, link->peer_mac, ETH_ALEN)) {
 -		if (!ini.ib_dev)
 -			goto out_reject;
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_PEER;
 -	}
 -	if (!ini.ib_dev) {
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_LOCAL;
 -		ini.ib_dev = link->smcibdev;
 -		ini.ib_port = link->ibport;
 -	}
 -	lnk_idx = smc_llc_alloc_alt_link(lgr, lgr_new_t);
 -	if (lnk_idx < 0)
 -		goto out_reject;
 -	lnk_new = &lgr->lnk[lnk_idx];
 -	rc = smcr_link_init(lgr, lnk_new, lnk_idx, &ini);
 -	if (rc)
 -		goto out_reject;
 -	smc_llc_save_add_link_info(lnk_new, llc);
 -	lnk_new->link_id = llc->link_num;	/* SMC server assigns link id */
 -	smc_llc_link_set_uid(lnk_new);
 +	int conf_rc;
  
 -	rc = smc_ib_ready_link(lnk_new);
 -	if (rc)
 -		goto out_clear_lnk;
 -
 -	rc = smcr_buf_map_lgr(lnk_new);
 -	if (rc)
 -		goto out_clear_lnk;
 +	/* RMBE eyecatchers are not supported */
 +	if (llc->hd.flags & SMC_LLC_FLAG_NO_RMBE_EYEC)
 +		conf_rc = 0;
 +	else
 +		conf_rc = ENOTSUPP;
  
 -	rc = smc_llc_send_add_link(link,
 -				   lnk_new->smcibdev->mac[ini.ib_port - 1],
 -				   lnk_new->gid, lnk_new, SMC_LLC_RESP);
 -	if (rc)
 -		goto out_clear_lnk;
 -	rc = smc_llc_cli_rkey_exchange(link, lnk_new);
 -	if (rc) {
 -		rc = 0;
 -		goto out_clear_lnk;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (lgr->role == SMC_SERV &&
 +		    link->state == SMC_LNK_ACTIVATING) {
 +			link->llc_confirm_resp_rc = conf_rc;
 +			complete(&link->llc_confirm_resp);
 +		}
 +	} else {
 +		if (lgr->role == SMC_CLNT &&
 +		    link->state == SMC_LNK_ACTIVATING) {
 +			link->llc_confirm_rc = conf_rc;
 +			link->link_id = llc->link_num;
 +			complete(&link->llc_confirm);
 +		}
  	}
 -	rc = smc_llc_cli_conf_link(link, &ini, lnk_new, lgr_new_t);
 -	if (!rc)
 -		goto out;
 -out_clear_lnk:
 -	smcr_link_clear(lnk_new, false);
 -out_reject:
 -	smc_llc_cli_add_link_reject(qentry);
 -out:
 -	kfree(qentry);
 -	return rc;
  }
  
 -/* as an SMC client, invite server to start the add_link processing */
 -static void smc_llc_cli_add_link_invite(struct smc_link *link,
 -					struct smc_llc_qentry *qentry)
 +static void smc_llc_rx_add_link(struct smc_link *link,
 +				struct smc_llc_msg_add_link *llc)
  {
  	struct smc_link_group *lgr = smc_get_lgr(link);
 -	struct smc_init_info ini;
  
++<<<<<<< HEAD
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (link->state == SMC_LNK_ACTIVATING)
 +			complete(&link->llc_add_resp);
 +	} else {
 +		if (link->state == SMC_LNK_ACTIVATING) {
 +			complete(&link->llc_add);
 +			return;
++=======
+ 	if (lgr->type == SMC_LGR_SYMMETRIC ||
+ 	    lgr->type == SMC_LGR_ASYMMETRIC_PEER)
+ 		goto out;
+ 
+ 	ini.vlan_id = lgr->vlan_id;
+ 	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
+ 	if (!ini.ib_dev)
+ 		goto out;
+ 
+ 	smc_llc_send_add_link(link, ini.ib_dev->mac[ini.ib_port - 1],
+ 			      ini.ib_gid, NULL, SMC_LLC_REQ);
+ out:
+ 	kfree(qentry);
+ }
+ 
+ static bool smc_llc_is_local_add_link(union smc_llc_msg *llc)
+ {
+ 	if (llc->raw.hdr.common.type == SMC_LLC_ADD_LINK &&
+ 	    !llc->add_link.qp_mtu && !llc->add_link.link_num)
+ 		return true;
+ 	return false;
+ }
+ 
+ static void smc_llc_process_cli_add_link(struct smc_link_group *lgr)
+ {
+ 	struct smc_llc_qentry *qentry;
+ 
+ 	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
+ 
+ 	mutex_lock(&lgr->llc_conf_mutex);
+ 	if (smc_llc_is_local_add_link(&qentry->msg))
+ 		smc_llc_cli_add_link_invite(qentry->link, qentry);
+ 	else
+ 		smc_llc_cli_add_link(qentry->link, qentry);
+ 	mutex_unlock(&lgr->llc_conf_mutex);
+ }
+ 
+ static int smc_llc_active_link_count(struct smc_link_group *lgr)
+ {
+ 	int i, link_count = 0;
+ 
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		if (!smc_link_active(&lgr->lnk[i]))
+ 			continue;
+ 		link_count++;
+ 	}
+ 	return link_count;
+ }
+ 
+ /* find the asymmetric link when 3 links are established  */
+ static struct smc_link *smc_llc_find_asym_link(struct smc_link_group *lgr)
+ {
+ 	int asym_idx = -ENOENT;
+ 	int i, j, k;
+ 	bool found;
+ 
+ 	/* determine asymmetric link */
+ 	found = false;
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		for (j = i + 1; j < SMC_LINKS_PER_LGR_MAX; j++) {
+ 			if (!smc_link_usable(&lgr->lnk[i]) ||
+ 			    !smc_link_usable(&lgr->lnk[j]))
+ 				continue;
+ 			if (!memcmp(lgr->lnk[i].gid, lgr->lnk[j].gid,
+ 				    SMC_GID_SIZE)) {
+ 				found = true;	/* asym_lnk is i or j */
+ 				break;
+ 			}
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  		}
 -		if (found)
 -			break;
 -	}
 -	if (!found)
 -		goto out; /* no asymmetric link */
 -	for (k = 0; k < SMC_LINKS_PER_LGR_MAX; k++) {
 -		if (!smc_link_usable(&lgr->lnk[k]))
 -			continue;
 -		if (k != i &&
 -		    !memcmp(lgr->lnk[i].peer_gid, lgr->lnk[k].peer_gid,
 -			    SMC_GID_SIZE)) {
 -			asym_idx = i;
 -			break;
 -		}
 -		if (k != j &&
 -		    !memcmp(lgr->lnk[j].peer_gid, lgr->lnk[k].peer_gid,
 -			    SMC_GID_SIZE)) {
 -			asym_idx = j;
 -			break;
 -		}
 -	}
 -out:
 -	return (asym_idx < 0) ? NULL : &lgr->lnk[asym_idx];
 -}
  
 -static void smc_llc_delete_asym_link(struct smc_link_group *lgr)
 -{
 -	struct smc_link *lnk_new = NULL, *lnk_asym;
 -	struct smc_llc_qentry *qentry;
 -	int rc;
 +		if (lgr->role == SMC_SERV) {
 +			smc_llc_prep_add_link(llc, link,
 +					link->smcibdev->mac[link->ibport - 1],
 +					link->gid, SMC_LLC_REQ);
  
 -	lnk_asym = smc_llc_find_asym_link(lgr);
 -	if (!lnk_asym)
 -		return; /* no asymmetric link */
 -	if (!smc_link_downing(&lnk_asym->state))
 -		return;
 -	lnk_new = smc_switch_conns(lgr, lnk_asym, false);
 -	smc_wr_tx_wait_no_pending_sends(lnk_asym);
 -	if (!lnk_new)
 -		goto out_free;
 -	/* change flow type from ADD_LINK into DEL_LINK */
 -	lgr->llc_flow_lcl.type = SMC_LLC_FLOW_DEL_LINK;
 -	rc = smc_llc_send_delete_link(lnk_new, lnk_asym->link_id, SMC_LLC_REQ,
 -				      true, SMC_LLC_DEL_NO_ASYM_NEEDED);
 -	if (rc) {
 -		smcr_link_down_cond(lnk_new);
 -		goto out_free;
 -	}
 -	qentry = smc_llc_wait(lgr, lnk_new, SMC_LLC_WAIT_TIME,
 -			      SMC_LLC_DELETE_LINK);
 -	if (!qentry) {
 -		smcr_link_down_cond(lnk_new);
 -		goto out_free;
 -	}
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -out_free:
 -	smcr_link_clear(lnk_asym, true);
 -}
 -
 -static int smc_llc_srv_rkey_exchange(struct smc_link *link,
 -				     struct smc_link *link_new)
 -{
 -	struct smc_llc_msg_add_link_cont *addc_llc;
 -	struct smc_link_group *lgr = link->lgr;
 -	u8 max, num_rkeys_send, num_rkeys_recv;
 -	struct smc_llc_qentry *qentry = NULL;
 -	struct smc_buf_desc *buf_pos;
 -	int buf_lst;
 -	int rc = 0;
 -	int i;
 -
 -	mutex_lock(&lgr->rmbs_lock);
 -	num_rkeys_send = lgr->conns_num;
 -	buf_pos = smc_llc_get_first_rmb(lgr, &buf_lst);
 -	do {
 -		smc_llc_add_link_cont(link, link_new, &num_rkeys_send,
 -				      &buf_lst, &buf_pos);
 -		qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_TIME,
 -				      SMC_LLC_ADD_LINK_CONT);
 -		if (!qentry) {
 -			rc = -ETIMEDOUT;
 -			goto out;
 -		}
 -		addc_llc = &qentry->msg.add_link_cont;
 -		num_rkeys_recv = addc_llc->num_rkeys;
 -		max = min_t(u8, num_rkeys_recv, SMC_LLC_RKEYS_PER_CONT_MSG);
 -		for (i = 0; i < max; i++) {
 -			smc_rtoken_set(lgr, link->link_idx, link_new->link_idx,
 -				       addc_llc->rt[i].rmb_key,
 -				       addc_llc->rt[i].rmb_vaddr_new,
 -				       addc_llc->rt[i].rmb_key_new);
 -			num_rkeys_recv--;
 +		} else {
 +			smc_llc_prep_add_link(llc, link,
 +					link->smcibdev->mac[link->ibport - 1],
 +					link->gid, SMC_LLC_RESP);
  		}
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -	} while (num_rkeys_send || num_rkeys_recv);
 -out:
 -	mutex_unlock(&lgr->rmbs_lock);
 -	return rc;
 -}
 -
 -static int smc_llc_srv_conf_link(struct smc_link *link,
 -				 struct smc_link *link_new,
 -				 enum smc_lgr_type lgr_new_t)
 -{
 -	struct smc_link_group *lgr = link->lgr;
 -	struct smc_llc_qentry *qentry = NULL;
 -	int rc;
 -
 -	/* send CONFIRM LINK request over the RoCE fabric */
 -	rc = smc_llc_send_confirm_link(link_new, SMC_LLC_REQ);
 -	if (rc)
 -		return -ENOLINK;
 -	/* receive CONFIRM LINK response over the RoCE fabric */
 -	qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_FIRST_TIME, 0);
 -	if (!qentry ||
 -	    qentry->msg.raw.hdr.common.type != SMC_LLC_CONFIRM_LINK) {
 -		/* send DELETE LINK */
 -		smc_llc_send_delete_link(link, link_new->link_id, SMC_LLC_REQ,
 -					 false, SMC_LLC_DEL_LOST_PATH);
 -		if (qentry)
 -			smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		return -ENOLINK;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -	smc_llc_save_peer_uid(qentry);
 -	smc_llc_link_active(link_new);
 -	if (lgr_new_t == SMC_LGR_ASYMMETRIC_LOCAL ||
 -	    lgr_new_t == SMC_LGR_ASYMMETRIC_PEER)
 -		smcr_lgr_set_type_asym(lgr, lgr_new_t, link_new->link_idx);
 -	else
 -		smcr_lgr_set_type(lgr, lgr_new_t);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -	return 0;
  }
  
 -int smc_llc_srv_add_link(struct smc_link *link)
 +static void smc_llc_rx_delete_link(struct smc_link *link,
 +				   struct smc_llc_msg_del_link *llc)
  {
 -	enum smc_lgr_type lgr_new_t = SMC_LGR_SYMMETRIC;
 -	struct smc_link_group *lgr = link->lgr;
 -	struct smc_llc_msg_add_link *add_llc;
 -	struct smc_llc_qentry *qentry = NULL;
 -	struct smc_link *link_new;
 -	struct smc_init_info ini;
 -	int lnk_idx, rc = 0;
 -
 -	/* ignore client add link recommendation, start new flow */
 -	ini.vlan_id = lgr->vlan_id;
 -	smc_pnet_find_alt_roce(lgr, &ini, link->smcibdev);
 -	if (!ini.ib_dev) {
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_LOCAL;
 -		ini.ib_dev = link->smcibdev;
 -		ini.ib_port = link->ibport;
 -	}
 -	lnk_idx = smc_llc_alloc_alt_link(lgr, lgr_new_t);
 -	if (lnk_idx < 0)
 -		return 0;
 -
 -	rc = smcr_link_init(lgr, &lgr->lnk[lnk_idx], lnk_idx, &ini);
 -	if (rc)
 -		return rc;
 -	link_new = &lgr->lnk[lnk_idx];
 -	rc = smc_llc_send_add_link(link,
 -				   link_new->smcibdev->mac[ini.ib_port - 1],
 -				   link_new->gid, link_new, SMC_LLC_REQ);
 -	if (rc)
 -		goto out_err;
 -	/* receive ADD LINK response over the RoCE fabric */
 -	qentry = smc_llc_wait(lgr, link, SMC_LLC_WAIT_TIME, SMC_LLC_ADD_LINK);
 -	if (!qentry) {
 -		rc = -ETIMEDOUT;
 -		goto out_err;
 -	}
 -	add_llc = &qentry->msg.add_link;
 -	if (add_llc->hd.flags & SMC_LLC_FLAG_ADD_LNK_REJ) {
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		rc = -ENOLINK;
 -		goto out_err;
 -	}
 -	if (lgr->type == SMC_LGR_SINGLE &&
 -	    (!memcmp(add_llc->sender_gid, link->peer_gid, SMC_GID_SIZE) &&
 -	     !memcmp(add_llc->sender_mac, link->peer_mac, ETH_ALEN))) {
 -		lgr_new_t = SMC_LGR_ASYMMETRIC_PEER;
 -	}
 -	smc_llc_save_add_link_info(link_new, add_llc);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -
 -	rc = smc_ib_ready_link(link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smcr_buf_map_lgr(link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smcr_buf_reg_lgr(link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smc_llc_srv_rkey_exchange(link, link_new);
 -	if (rc)
 -		goto out_err;
 -	rc = smc_llc_srv_conf_link(link, link_new, lgr_new_t);
 -	if (rc)
 -		goto out_err;
 -	return 0;
 -out_err:
 -	smcr_link_clear(link_new, false);
 -	return rc;
 -}
 -
 -static void smc_llc_process_srv_add_link(struct smc_link_group *lgr)
 -{
 -	struct smc_link *link = lgr->llc_flow_lcl.qentry->link;
 -	int rc;
 -
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -
 -	mutex_lock(&lgr->llc_conf_mutex);
 -	rc = smc_llc_srv_add_link(link);
 -	if (!rc && lgr->type == SMC_LGR_SYMMETRIC) {
 -		/* delete any asymmetric link */
 -		smc_llc_delete_asym_link(lgr);
 -	}
 -	mutex_unlock(&lgr->llc_conf_mutex);
 -}
 -
 -/* enqueue a local add_link req to trigger a new add_link flow */
 -void smc_llc_add_link_local(struct smc_link *link)
 -{
 -	struct smc_llc_msg_add_link add_llc = {0};
 -
 -	add_llc.hd.length = sizeof(add_llc);
 -	add_llc.hd.common.type = SMC_LLC_ADD_LINK;
 -	/* no dev and port needed */
 -	smc_llc_enqueue(link, (union smc_llc_msg *)&add_llc);
 -}
 -
 -/* worker to process an add link message */
 -static void smc_llc_add_link_work(struct work_struct *work)
 -{
 -	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
 -						  llc_add_link_work);
 -
 -	if (list_empty(&lgr->list)) {
 -		/* link group is terminating */
 -		smc_llc_flow_qentry_del(&lgr->llc_flow_lcl);
 -		goto out;
 -	}
 -
 -	if (lgr->role == SMC_CLNT)
 -		smc_llc_process_cli_add_link(lgr);
 -	else
 -		smc_llc_process_srv_add_link(lgr);
 -out:
 -	smc_llc_flow_stop(lgr, &lgr->llc_flow_lcl);
 -}
 -
 -/* enqueue a local del_link msg to trigger a new del_link flow,
 - * called only for role SMC_SERV
 - */
 -void smc_llc_srv_delete_link_local(struct smc_link *link, u8 del_link_id)
 -{
 -	struct smc_llc_msg_del_link del_llc = {0};
 -
 -	del_llc.hd.length = sizeof(del_llc);
 -	del_llc.hd.common.type = SMC_LLC_DELETE_LINK;
 -	del_llc.link_num = del_link_id;
 -	del_llc.reason = htonl(SMC_LLC_DEL_LOST_PATH);
 -	del_llc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ORDERLY;
 -	smc_llc_enqueue(link, (union smc_llc_msg *)&del_llc);
 -}
 -
 -static void smc_llc_process_cli_delete_link(struct smc_link_group *lgr)
 -{
 -	struct smc_link *lnk_del = NULL, *lnk_asym, *lnk;
 -	struct smc_llc_msg_del_link *del_llc;
 -	struct smc_llc_qentry *qentry;
 -	int active_links;
 -	int lnk_idx;
 -
 -	qentry = smc_llc_flow_qentry_clr(&lgr->llc_flow_lcl);
 -	lnk = qentry->link;
 -	del_llc = &qentry->msg.delete_link;
 -
 -	if (del_llc->hd.flags & SMC_LLC_FLAG_DEL_LINK_ALL) {
 -		smc_lgr_terminate_sched(lgr);
 -		goto out;
 -	}
 -	mutex_lock(&lgr->llc_conf_mutex);
 -	/* delete single link */
 -	for (lnk_idx = 0; lnk_idx < SMC_LINKS_PER_LGR_MAX; lnk_idx++) {
 -		if (lgr->lnk[lnk_idx].link_id != del_llc->link_num)
 -			continue;
 -		lnk_del = &lgr->lnk[lnk_idx];
 -		break;
 -	}
 -	del_llc->hd.flags |= SMC_LLC_FLAG_RESP;
 -	if (!lnk_del) {
 -		/* link was not found */
 -		del_llc->reason = htonl(SMC_LLC_DEL_NOLNK);
 -		smc_llc_send_message(lnk, &qentry->msg);
 -		goto out_unlock;
 -	}
 -	lnk_asym = smc_llc_find_asym_link(lgr);
 -
 -	del_llc->reason = 0;
 -	smc_llc_send_message(lnk, &qentry->msg); /* response */
 +	struct smc_link_group *lgr = smc_get_lgr(link);
  
 -	if (smc_link_downing(&lnk_del->state)) {
 -		if (smc_switch_conns(lgr, lnk_del, false))
 -			smc_wr_tx_wait_no_pending_sends(lnk_del);
 -	}
 -	smcr_link_clear(lnk_del, true);
 -
 -	active_links = smc_llc_active_link_count(lgr);
 -	if (lnk_del == lnk_asym) {
 -		/* expected deletion of asym link, don't change lgr state */
 -	} else if (active_links == 1) {
 -		smcr_lgr_set_type(lgr, SMC_LGR_SINGLE);
 -	} else if (!active_links) {
 -		smcr_lgr_set_type(lgr, SMC_LGR_NONE);
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (lgr->role == SMC_SERV)
 +			smc_lgr_schedule_free_work_fast(lgr);
 +	} else {
 +		smc_lgr_forget(lgr);
 +		smc_llc_link_deleting(link);
 +		if (lgr->role == SMC_SERV) {
 +			/* client asks to delete this link, send request */
 +			smc_llc_prep_delete_link(llc, link, SMC_LLC_REQ, true);
 +		} else {
 +			/* server requests to delete this link, send response */
 +			smc_llc_prep_delete_link(llc, link, SMC_LLC_RESP, true);
 +		}
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  		smc_lgr_terminate_sched(lgr);
  	}
 -out_unlock:
 -	mutex_unlock(&lgr->llc_conf_mutex);
 -out:
 -	kfree(qentry);
  }
  
 -/* try to send a DELETE LINK ALL request on any active link,
 - * waiting for send completion
 - */
 -void smc_llc_send_link_delete_all(struct smc_link_group *lgr, bool ord, u32 rsn)
 +static void smc_llc_rx_test_link(struct smc_link *link,
 +				 struct smc_llc_msg_test_link *llc)
  {
 -	struct smc_llc_msg_del_link delllc = {0};
 -	int i;
 -
 -	delllc.hd.common.type = SMC_LLC_DELETE_LINK;
 -	delllc.hd.length = sizeof(delllc);
 -	if (ord)
 -		delllc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ORDERLY;
 -	delllc.hd.flags |= SMC_LLC_FLAG_DEL_LINK_ALL;
 -	delllc.reason = htonl(rsn);
 -
 -	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
 -		if (!smc_link_usable(&lgr->lnk[i]))
 -			continue;
 -		if (!smc_llc_send_message_wait(&lgr->lnk[i], &delllc))
 -			break;
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		if (link->state == SMC_LNK_ACTIVE)
 +			complete(&link->llc_testlink_resp);
 +	} else {
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
  }
  
@@@ -532,27 -1449,239 +622,130 @@@ static void smc_llc_rx_delete_rkey(stru
  	u8 err_mask = 0;
  	int i, max;
  
 -	qentry = lgr->llc_flow_rmt.qentry;
 -	llc = &qentry->msg.delete_rkey;
 -	link = qentry->link;
 -
 -	max = min_t(u8, llc->num_rkeys, SMC_LLC_DEL_RKEY_MAX);
 -	for (i = 0; i < max; i++) {
 -		if (smc_rtoken_delete(link, llc->rkey[i]))
 -			err_mask |= 1 << (SMC_LLC_DEL_RKEY_MAX - 1 - i);
 -	}
 -	if (err_mask) {
 -		llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 -		llc->err_mask = err_mask;
 -	}
 -	llc->hd.flags |= SMC_LLC_FLAG_RESP;
 -	smc_llc_send_message(link, &qentry->msg);
 -	smc_llc_flow_qentry_del(&lgr->llc_flow_rmt);
 -}
 -
 -static void smc_llc_protocol_violation(struct smc_link_group *lgr, u8 type)
 -{
 -	pr_warn_ratelimited("smc: SMC-R lg %*phN LLC protocol violation: "
 -			    "llc_type %d\n", SMC_LGR_ID_SIZE, &lgr->id, type);
 -	smc_llc_set_termination_rsn(lgr, SMC_LLC_DEL_PROT_VIOL);
 -	smc_lgr_terminate_sched(lgr);
 -}
 -
 -/* flush the llc event queue */
 -static void smc_llc_event_flush(struct smc_link_group *lgr)
 -{
 -	struct smc_llc_qentry *qentry, *q;
 -
 -	spin_lock_bh(&lgr->llc_event_q_lock);
 -	list_for_each_entry_safe(qentry, q, &lgr->llc_event_q, list) {
 -		list_del_init(&qentry->list);
 -		kfree(qentry);
 -	}
 -	spin_unlock_bh(&lgr->llc_event_q_lock);
 -}
 -
 -static void smc_llc_event_handler(struct smc_llc_qentry *qentry)
 -{
 -	union smc_llc_msg *llc = &qentry->msg;
 -	struct smc_link *link = qentry->link;
 -	struct smc_link_group *lgr = link->lgr;
 -
 -	if (!smc_link_usable(link))
 -		goto out;
 -
 -	switch (llc->raw.hdr.common.type) {
 -	case SMC_LLC_TEST_LINK:
 -		llc->test_link.hd.flags |= SMC_LLC_FLAG_RESP;
 -		smc_llc_send_message(link, llc);
 -		break;
 -	case SMC_LLC_ADD_LINK:
 -		if (list_empty(&lgr->list))
 -			goto out;	/* lgr is terminating */
 -		if (lgr->role == SMC_CLNT) {
 -			if (smc_llc_is_local_add_link(llc)) {
 -				if (lgr->llc_flow_lcl.type ==
 -				    SMC_LLC_FLOW_ADD_LINK)
 -					break;	/* add_link in progress */
 -				if (smc_llc_flow_start(&lgr->llc_flow_lcl,
 -						       qentry)) {
 -					schedule_work(&lgr->llc_add_link_work);
 -				}
 -				return;
 -			}
 -			if (lgr->llc_flow_lcl.type == SMC_LLC_FLOW_ADD_LINK &&
 -			    !lgr->llc_flow_lcl.qentry) {
 -				/* a flow is waiting for this message */
 -				smc_llc_flow_qentry_set(&lgr->llc_flow_lcl,
 -							qentry);
 -				wake_up(&lgr->llc_msg_waiter);
 -			} else if (smc_llc_flow_start(&lgr->llc_flow_lcl,
 -						      qentry)) {
 -				schedule_work(&lgr->llc_add_link_work);
 -			}
 -		} else if (smc_llc_flow_start(&lgr->llc_flow_lcl, qentry)) {
 -			/* as smc server, handle client suggestion */
 -			schedule_work(&lgr->llc_add_link_work);
 -		}
 -		return;
 -	case SMC_LLC_CONFIRM_LINK:
 -	case SMC_LLC_ADD_LINK_CONT:
 -		if (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {
 -			/* a flow is waiting for this message */
 -			smc_llc_flow_qentry_set(&lgr->llc_flow_lcl, qentry);
 -			wake_up(&lgr->llc_msg_waiter);
 -			return;
 -		}
 -		break;
 -	case SMC_LLC_DELETE_LINK:
 -		if (lgr->llc_flow_lcl.type == SMC_LLC_FLOW_ADD_LINK &&
 -		    !lgr->llc_flow_lcl.qentry) {
 -			/* DEL LINK REQ during ADD LINK SEQ */
 -			smc_llc_flow_qentry_set(&lgr->llc_flow_lcl, qentry);
 -			wake_up(&lgr->llc_msg_waiter);
 -		} else if (smc_llc_flow_start(&lgr->llc_flow_lcl, qentry)) {
 -			schedule_work(&lgr->llc_del_link_work);
 -		}
 -		return;
 -	case SMC_LLC_CONFIRM_RKEY:
 -		/* new request from remote, assign to remote flow */
 -		if (smc_llc_flow_start(&lgr->llc_flow_rmt, qentry)) {
 -			/* process here, does not wait for more llc msgs */
 -			smc_llc_rmt_conf_rkey(lgr);
 -			smc_llc_flow_stop(lgr, &lgr->llc_flow_rmt);
 +	if (llc->hd.flags & SMC_LLC_FLAG_RESP) {
 +		link->llc_delete_rkey_rc = llc->hd.flags &
 +					    SMC_LLC_FLAG_RKEY_NEG;
 +		complete(&link->llc_delete_rkey);
 +	} else {
 +		max = min_t(u8, llc->num_rkeys, SMC_LLC_DEL_RKEY_MAX);
 +		for (i = 0; i < max; i++) {
 +			if (smc_rtoken_delete(link, llc->rkey[i]))
 +				err_mask |= 1 << (SMC_LLC_DEL_RKEY_MAX - 1 - i);
  		}
 -		return;
 -	case SMC_LLC_CONFIRM_RKEY_CONT:
 -		/* not used because max links is 3, and 3 rkeys fit into
 -		 * one CONFIRM_RKEY message
 -		 */
 -		break;
 -	case SMC_LLC_DELETE_RKEY:
 -		/* new request from remote, assign to remote flow */
 -		if (smc_llc_flow_start(&lgr->llc_flow_rmt, qentry)) {
 -			/* process here, does not wait for more llc msgs */
 -			smc_llc_rmt_delete_rkey(lgr);
 -			smc_llc_flow_stop(lgr, &lgr->llc_flow_rmt);
 +
 +		if (err_mask) {
 +			llc->hd.flags |= SMC_LLC_FLAG_RKEY_NEG;
 +			llc->err_mask = err_mask;
  		}
 -		return;
 -	default:
 -		smc_llc_protocol_violation(lgr, llc->raw.hdr.common.type);
 -		break;
 +
 +		llc->hd.flags |= SMC_LLC_FLAG_RESP;
 +		smc_llc_send_message(link, llc, sizeof(*llc));
  	}
 -out:
 -	kfree(qentry);
  }
  
++<<<<<<< HEAD
++=======
+ /* worker to process llc messages on the event queue */
+ static void smc_llc_event_work(struct work_struct *work)
+ {
+ 	struct smc_link_group *lgr = container_of(work, struct smc_link_group,
+ 						  llc_event_work);
+ 	struct smc_llc_qentry *qentry;
+ 
+ 	if (!lgr->llc_flow_lcl.type && lgr->delayed_event) {
+ 		if (smc_link_usable(lgr->delayed_event->link)) {
+ 			smc_llc_event_handler(lgr->delayed_event);
+ 		} else {
+ 			qentry = lgr->delayed_event;
+ 			lgr->delayed_event = NULL;
+ 			kfree(qentry);
+ 		}
+ 	}
+ 
+ again:
+ 	spin_lock_bh(&lgr->llc_event_q_lock);
+ 	if (!list_empty(&lgr->llc_event_q)) {
+ 		qentry = list_first_entry(&lgr->llc_event_q,
+ 					  struct smc_llc_qentry, list);
+ 		list_del_init(&qentry->list);
+ 		spin_unlock_bh(&lgr->llc_event_q_lock);
+ 		smc_llc_event_handler(qentry);
+ 		goto again;
+ 	}
+ 	spin_unlock_bh(&lgr->llc_event_q_lock);
+ }
+ 
+ /* process llc responses in tasklet context */
+ static void smc_llc_rx_response(struct smc_link *link,
+ 				struct smc_llc_qentry *qentry)
+ {
+ 	enum smc_llc_flowtype flowtype = link->lgr->llc_flow_lcl.type;
+ 	struct smc_llc_flow *flow = &link->lgr->llc_flow_lcl;
+ 	u8 llc_type = qentry->msg.raw.hdr.common.type;
+ 
+ 	switch (llc_type) {
+ 	case SMC_LLC_TEST_LINK:
+ 		if (smc_link_active(link))
+ 			complete(&link->llc_testlink_resp);
+ 		break;
+ 	case SMC_LLC_ADD_LINK:
+ 	case SMC_LLC_ADD_LINK_CONT:
+ 	case SMC_LLC_CONFIRM_LINK:
+ 		if (flowtype != SMC_LLC_FLOW_ADD_LINK || flow->qentry)
+ 			break;	/* drop out-of-flow response */
+ 		goto assign;
+ 	case SMC_LLC_DELETE_LINK:
+ 		if (flowtype != SMC_LLC_FLOW_DEL_LINK || flow->qentry)
+ 			break;	/* drop out-of-flow response */
+ 		goto assign;
+ 	case SMC_LLC_CONFIRM_RKEY:
+ 	case SMC_LLC_DELETE_RKEY:
+ 		if (flowtype != SMC_LLC_FLOW_RKEY || flow->qentry)
+ 			break;	/* drop out-of-flow response */
+ 		goto assign;
+ 	case SMC_LLC_CONFIRM_RKEY_CONT:
+ 		/* not used because max links is 3 */
+ 		break;
+ 	default:
+ 		smc_llc_protocol_violation(link->lgr, llc_type);
+ 		break;
+ 	}
+ 	kfree(qentry);
+ 	return;
+ assign:
+ 	/* assign responses to the local flow, we requested them */
+ 	smc_llc_flow_qentry_set(&link->lgr->llc_flow_lcl, qentry);
+ 	wake_up(&link->lgr->llc_msg_waiter);
+ }
+ 
+ static void smc_llc_enqueue(struct smc_link *link, union smc_llc_msg *llc)
+ {
+ 	struct smc_link_group *lgr = link->lgr;
+ 	struct smc_llc_qentry *qentry;
+ 	unsigned long flags;
+ 
+ 	qentry = kmalloc(sizeof(*qentry), GFP_ATOMIC);
+ 	if (!qentry)
+ 		return;
+ 	qentry->link = link;
+ 	INIT_LIST_HEAD(&qentry->list);
+ 	memcpy(&qentry->msg, llc, sizeof(union smc_llc_msg));
+ 
+ 	/* process responses immediately */
+ 	if (llc->raw.hdr.flags & SMC_LLC_FLAG_RESP) {
+ 		smc_llc_rx_response(link, qentry);
+ 		return;
+ 	}
+ 
+ 	/* add requests to event queue */
+ 	spin_lock_irqsave(&lgr->llc_event_q_lock, flags);
+ 	list_add_tail(&qentry->list, &lgr->llc_event_q);
+ 	spin_unlock_irqrestore(&lgr->llc_event_q_lock, flags);
+ 	schedule_work(&lgr->llc_event_work);
+ }
+ 
+ /* copy received msg and add it to the event queue */
++>>>>>>> 741a49a4dc5f (net/smc: do not call dma sync for unmapped memory)
  static void smc_llc_rx_handler(struct ib_wc *wc, void *buf)
  {
  	struct smc_link *link = (struct smc_link *)wc->qp->qp_context;
@@@ -613,10 -1718,10 +806,10 @@@ static void smc_llc_testlink_work(struc
  	/* receive TEST LINK response over RoCE fabric */
  	rc = wait_for_completion_interruptible_timeout(&link->llc_testlink_resp,
  						       SMC_LLC_WAIT_TIME);
- 	if (link->state != SMC_LNK_ACTIVE)
+ 	if (!smc_link_active(link))
  		return;		/* link state changed */
  	if (rc <= 0) {
 -		smcr_link_down_cond_sched(link);
 +		smc_lgr_terminate(smc_get_lgr(link), true);
  		return;
  	}
  	next_interval = link->llc_testlink_time;
diff --git a/net/smc/af_smc.c b/net/smc/af_smc.c
index 61adbee56cf2..d60902884e16 100644
--- a/net/smc/af_smc.c
+++ b/net/smc/af_smc.c
@@ -368,7 +368,7 @@ static int smcr_lgr_reg_rmbs(struct smc_link_group *lgr,
 	int i, rc;
 
 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
-		if (lgr->lnk[i].state != SMC_LNK_ACTIVE)
+		if (!smc_link_active(&lgr->lnk[i]))
 			continue;
 		rc = smcr_link_reg_rmb(&lgr->lnk[i], rmb_desc, true);
 		if (rc)
* Unmerged path net/smc/smc_core.c
diff --git a/net/smc/smc_core.h b/net/smc/smc_core.h
index dc42fae85777..d289cba379e1 100644
--- a/net/smc/smc_core.h
+++ b/net/smc/smc_core.h
@@ -302,6 +302,11 @@ static inline void smc_lgr_terminate_sched(struct smc_link_group *lgr)
 		schedule_work(&lgr->terminate_work);
 }
 
+static inline bool smc_link_active(struct smc_link *lnk)
+{
+	return lnk->state == SMC_LNK_ACTIVE;
+}
+
 struct smc_sock;
 struct smc_clc_msg_accept_confirm;
 struct smc_clc_msg_local;
* Unmerged path net/smc/smc_llc.c
