atomics/treewide: Make atomic_fetch_add_unless() optional

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit eccc2da8c03f316bba202e15af2be4615f461900
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/eccc2da8.failed

Several architectures these have a near-identical implementation based
on atomic_read() and atomic_cmpxchg() which we can instead define in
<linux/atomic.h>, so let's do so, using something close to the existing
x86 implementation with try_cmpxchg().

Where an architecture provides its own atomic_fetch_add_unless(), it
must define a preprocessor symbol for it. The instrumented atomics are
updated accordingly.

Note that arch/arc's existing atomic_fetch_add_unless() had redundant
barriers, as these are already present in its atomic_cmpxchg()
implementation.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Reviewed-by: Geert Uytterhoeven <geert@linux-m68k.org>
	Reviewed-by: Will Deacon <will.deacon@arm.com>
	Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Palmer Dabbelt <palmer@sifive.com>
	Cc: Boqun Feng <boqun.feng@gmail.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vineet Gupta <vgupta@synopsys.com>
Link: https://lore.kernel.org/lkml/20180621121321.4761-7-mark.rutland@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit eccc2da8c03f316bba202e15af2be4615f461900)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/include/asm/atomic.h
#	arch/arm/include/asm/atomic.h
#	arch/arm64/include/asm/atomic.h
#	arch/ia64/include/asm/atomic.h
#	arch/m68k/include/asm/atomic.h
#	arch/mips/include/asm/atomic.h
#	arch/parisc/include/asm/atomic.h
#	arch/s390/include/asm/atomic.h
#	arch/sh/include/asm/atomic.h
#	arch/sparc/include/asm/atomic_64.h
#	arch/x86/include/asm/atomic.h
#	arch/xtensa/include/asm/atomic.h
#	include/asm-generic/atomic-instrumented.h
#	include/asm-generic/atomic.h
diff --cc arch/arc/include/asm/atomic.h
index 11859287c52a,60da80481c5d..000000000000
--- a/arch/arc/include/asm/atomic.h
+++ b/arch/arc/include/asm/atomic.h
@@@ -308,36 -308,6 +308,39 @@@ ATOMIC_OPS(xor, ^=, CTOP_INST_AXOR_DI_R
  #undef ATOMIC_OP_RETURN
  #undef ATOMIC_OP
  
++<<<<<<< HEAD
 +/**
 + * __atomic_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v
 + */
 +#define __atomic_add_unless(v, a, u)					\
 +({									\
 +	int c, old;							\
 +									\
 +	/*								\
 +	 * Explicit full memory barrier needed before/after as		\
 +	 * LLOCK/SCOND thmeselves don't provide any such semantics	\
 +	 */								\
 +	smp_mb();							\
 +									\
 +	c = atomic_read(v);						\
 +	while (c != (u) && (old = atomic_cmpxchg((v), c, c + (a))) != c)\
 +		c = old;						\
 +									\
 +	smp_mb();							\
 +									\
 +	c;								\
 +})
 +
 +#define atomic_inc_not_zero(v)		atomic_add_unless((v), 1, 0)
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #define atomic_inc(v)			atomic_add(1, v)
  #define atomic_dec(v)			atomic_sub(1, v)
  
diff --cc arch/arm/include/asm/atomic.h
index 66d0e215a773,74460aa00fa0..000000000000
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@@ -215,16 -216,6 +216,19 @@@ static inline int atomic_cmpxchg(atomic
  	return ret;
  }
  
++<<<<<<< HEAD
 +static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +
 +	c = atomic_read(v);
 +	while (c != u && (old = atomic_cmpxchg((v), c, c + a)) != c)
 +		c = old;
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #endif /* __LINUX_ARM_ARCH__ */
  
  #define ATOMIC_OPS(op, c_op, asm_op)					\
diff --cc arch/arm64/include/asm/atomic.h
index c0235e0ff849,22c8c43d6689..000000000000
--- a/arch/arm64/include/asm/atomic.h
+++ b/arch/arm64/include/asm/atomic.h
@@@ -125,7 -125,6 +125,10 @@@
  #define atomic_dec_and_test(v)		(atomic_dec_return(v) == 0)
  #define atomic_sub_and_test(i, v)	(atomic_sub_return((i), (v)) == 0)
  #define atomic_add_negative(i, v)	(atomic_add_return((i), (v)) < 0)
++<<<<<<< HEAD
 +#define __atomic_add_unless(v, a, u)	___atomic_add_unless(v, a, u,)
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #define atomic_andnot			atomic_andnot
  
  /*
diff --cc arch/ia64/include/asm/atomic.h
index 2524fb60fbc2,cfe44086338e..000000000000
--- a/arch/ia64/include/asm/atomic.h
+++ b/arch/ia64/include/asm/atomic.h
@@@ -215,22 -215,6 +215,25 @@@ ATOMIC64_FETCH_OP(xor, ^
  	(cmpxchg(&((v)->counter), old, new))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  static __inline__ long atomic64_add_unless(atomic64_t *v, long a, long u)
  {
  	long c, old;
diff --cc arch/m68k/include/asm/atomic.h
index e993e2860ee1,596882cda224..000000000000
--- a/arch/m68k/include/asm/atomic.h
+++ b/arch/m68k/include/asm/atomic.h
@@@ -211,19 -211,4 +211,22 @@@ static inline int atomic_add_negative(i
  	return c != 0;
  }
  
++<<<<<<< HEAD
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #endif /* __ARCH_M68K_ATOMIC __ */
diff --cc arch/mips/include/asm/atomic.h
index 0ab176bdb8e8,794734e730d9..000000000000
--- a/arch/mips/include/asm/atomic.h
+++ b/arch/mips/include/asm/atomic.h
@@@ -274,30 -274,6 +274,33 @@@ static __inline__ int atomic_sub_if_pos
  #define atomic_cmpxchg(v, o, n) (cmpxchg(&((v)->counter), (o), (n)))
  #define atomic_xchg(v, new) (xchg(&((v)->counter), (new)))
  
++<<<<<<< HEAD
 +/**
 + * __atomic_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #define atomic_dec_return(v) atomic_sub_return(1, (v))
  #define atomic_inc_return(v) atomic_add_return(1, (v))
  
diff --cc arch/parisc/include/asm/atomic.h
index 88bae6676c9b,b2b6261d05e7..000000000000
--- a/arch/parisc/include/asm/atomic.h
+++ b/arch/parisc/include/asm/atomic.h
@@@ -77,30 -77,6 +77,33 @@@ static __inline__ int atomic_read(cons
  #define atomic_cmpxchg(v, o, n) (cmpxchg(&((v)->counter), (o), (n)))
  #define atomic_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +/**
 + * __atomic_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #define ATOMIC_OP(op, c_op)						\
  static __inline__ void atomic_##op(int i, atomic_t *v)			\
  {									\
diff --cc arch/s390/include/asm/atomic.h
index 4b55532f15c4,26c6b713a7a3..000000000000
--- a/arch/s390/include/asm/atomic.h
+++ b/arch/s390/include/asm/atomic.h
@@@ -90,21 -90,6 +90,24 @@@ static inline int atomic_cmpxchg(atomic
  	return __atomic_cmpxchg(&v->counter, old, new);
  }
  
++<<<<<<< HEAD
 +static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == u))
 +			break;
 +		old = atomic_cmpxchg(v, c, c + a);
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #define ATOMIC64_INIT(i)  { (i) }
  
  static inline long atomic64_read(const atomic64_t *v)
diff --cc arch/sh/include/asm/atomic.h
index 0fd0099f43cc,422fac764ca1..000000000000
--- a/arch/sh/include/asm/atomic.h
+++ b/arch/sh/include/asm/atomic.h
@@@ -45,31 -45,6 +45,34 @@@
  #define atomic_xchg(v, new)		(xchg(&((v)->counter), new))
  #define atomic_cmpxchg(v, o, n)		(cmpxchg(&((v)->counter), (o), (n)))
  
++<<<<<<< HEAD
 +/**
 + * __atomic_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #endif /* CONFIG_CPU_J2 */
  
  #endif /* __ASM_SH_ATOMIC_H */
diff --cc arch/sparc/include/asm/atomic_64.h
index 28db058d471b,e4f1c93db31f..000000000000
--- a/arch/sparc/include/asm/atomic_64.h
+++ b/arch/sparc/include/asm/atomic_64.h
@@@ -89,21 -89,6 +89,24 @@@ static inline int atomic_xchg(atomic_t 
  	return xchg(&v->counter, new);
  }
  
++<<<<<<< HEAD
 +static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #define atomic64_cmpxchg(v, o, n) \
  	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), new))
diff --cc arch/x86/include/asm/atomic.h
index 7cf580771c57,616327ac9d39..000000000000
--- a/arch/x86/include/asm/atomic.h
+++ b/arch/x86/include/asm/atomic.h
@@@ -253,27 -253,6 +253,30 @@@ static inline int arch_atomic_fetch_xor
  	return val;
  }
  
++<<<<<<< HEAD
 +/**
 + * __arch_atomic_add_unless - add unless the number is already a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as @v was not already @u.
 + * Returns the old value of @v.
 + */
 +static __always_inline int __arch_atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c = arch_atomic_read(v);
 +
 +	do {
 +		if (unlikely(c == u))
 +			break;
 +	} while (!arch_atomic_try_cmpxchg(v, &c, c + a));
 +
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #ifdef CONFIG_X86_32
  # include <asm/atomic64_32.h>
  #else
diff --cc arch/xtensa/include/asm/atomic.h
index e7a23f2a519a,f4c9f82c40c6..000000000000
--- a/arch/xtensa/include/asm/atomic.h
+++ b/arch/xtensa/include/asm/atomic.h
@@@ -274,30 -274,6 +274,33 @@@ ATOMIC_OPS(xor
  #define atomic_cmpxchg(v, o, n) ((int)cmpxchg(&((v)->counter), (o), (n)))
  #define atomic_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +/**
 + * __atomic_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #endif /* __KERNEL__ */
  
  #endif /* _XTENSA_ATOMIC_H */
diff --cc include/asm-generic/atomic-instrumented.h
index cfee349ddd5a,1f9b2a767d3c..000000000000
--- a/include/asm-generic/atomic-instrumented.h
+++ b/include/asm-generic/atomic-instrumented.h
@@@ -84,12 -84,14 +84,18 @@@ static __always_inline bool atomic64_tr
  }
  #endif
  
++<<<<<<< HEAD
 +static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)
++=======
+ #ifdef arch_atomic_fetch_add_unless
+ #define atomic_fetch_add_unless atomic_fetch_add_unless
+ static __always_inline int atomic_fetch_add_unless(atomic_t *v, int a, int u)
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  {
  	kasan_check_write(v, sizeof(*v));
 -	return arch_atomic_fetch_add_unless(v, a, u);
 +	return __arch_atomic_add_unless(v, a, u);
  }
- 
+ #endif
  
  static __always_inline bool atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
  {
diff --cc include/asm-generic/atomic.h
index abe6dd9ca2a8,757e45821220..000000000000
--- a/include/asm-generic/atomic.h
+++ b/include/asm-generic/atomic.h
@@@ -221,15 -221,4 +221,18 @@@ static inline void atomic_dec(atomic_t 
  #define atomic_xchg(ptr, v)		(xchg(&(ptr)->counter, (v)))
  #define atomic_cmpxchg(v, old, new)	(cmpxchg(&((v)->counter), (old), (new)))
  
++<<<<<<< HEAD
 +#ifndef __atomic_add_unless
 +static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	while (c != u && (old = atomic_cmpxchg(v, c, c + a)) != c)
 +		c = old;
 +	return c;
 +}
 +#endif
 +
++=======
++>>>>>>> eccc2da8c03f (atomics/treewide: Make atomic_fetch_add_unless() optional)
  #endif /* __ASM_GENERIC_ATOMIC_H */
diff --git a/arch/alpha/include/asm/atomic.h b/arch/alpha/include/asm/atomic.h
index 767bfdd42992..78fc0bf26498 100644
--- a/arch/alpha/include/asm/atomic.h
+++ b/arch/alpha/include/asm/atomic.h
@@ -235,7 +235,7 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 	smp_mb();
 	return old;
 }
-
+#define atomic_fetch_add_unless atomic_fetch_add_unless
 
 /**
  * atomic64_add_unless - add unless the number is a given value
* Unmerged path arch/arc/include/asm/atomic.h
* Unmerged path arch/arm/include/asm/atomic.h
* Unmerged path arch/arm64/include/asm/atomic.h
diff --git a/arch/h8300/include/asm/atomic.h b/arch/h8300/include/asm/atomic.h
index 941e7554e886..ae99c0e01e8a 100644
--- a/arch/h8300/include/asm/atomic.h
+++ b/arch/h8300/include/asm/atomic.h
@@ -106,5 +106,6 @@ static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 	arch_local_irq_restore(flags);
 	return ret;
 }
+#define atomic_fetch_add_unless		atomic_fetch_add_unless
 
 #endif /* __ARCH_H8300_ATOMIC __ */
diff --git a/arch/hexagon/include/asm/atomic.h b/arch/hexagon/include/asm/atomic.h
index fb3dfb2a667e..8026d5783837 100644
--- a/arch/hexagon/include/asm/atomic.h
+++ b/arch/hexagon/include/asm/atomic.h
@@ -196,6 +196,7 @@ static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 	);
 	return __oldval;
 }
+#define atomic_fetch_add_unless atomic_fetch_add_unless
 
 #define atomic_inc_not_zero(v) atomic_add_unless((v), 1, 0)
 
* Unmerged path arch/ia64/include/asm/atomic.h
* Unmerged path arch/m68k/include/asm/atomic.h
* Unmerged path arch/mips/include/asm/atomic.h
* Unmerged path arch/parisc/include/asm/atomic.h
diff --git a/arch/powerpc/include/asm/atomic.h b/arch/powerpc/include/asm/atomic.h
index 682b3e6a1e21..363f5dbc2475 100644
--- a/arch/powerpc/include/asm/atomic.h
+++ b/arch/powerpc/include/asm/atomic.h
@@ -248,6 +248,7 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 
 	return t;
 }
+#define atomic_fetch_add_unless atomic_fetch_add_unless
 
 /**
  * atomic_inc_not_zero - increment unless the number is zero
diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h
index 855115ace98c..94f924132060 100644
--- a/arch/riscv/include/asm/atomic.h
+++ b/arch/riscv/include/asm/atomic.h
@@ -349,6 +349,7 @@ static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)
 		: "memory");
 	return prev;
 }
+#define atomic_fetch_add_unless atomic_fetch_add_unless
 
 #ifndef CONFIG_GENERIC_ATOMIC64
 static __always_inline long __atomic64_add_unless(atomic64_t *v, long a, long u)
* Unmerged path arch/s390/include/asm/atomic.h
* Unmerged path arch/sh/include/asm/atomic.h
diff --git a/arch/sparc/include/asm/atomic_32.h b/arch/sparc/include/asm/atomic_32.h
index d13ce517f4b9..5910249ef515 100644
--- a/arch/sparc/include/asm/atomic_32.h
+++ b/arch/sparc/include/asm/atomic_32.h
@@ -30,6 +30,8 @@ int atomic_xchg(atomic_t *, int);
 int __atomic_add_unless(atomic_t *, int, int);
 void atomic_set(atomic_t *, int);
 
+#define atomic_fetch_add_unless	atomic_fetch_add_unless
+
 #define atomic_set_release(v, i)	atomic_set((v), (i))
 
 #define atomic_read(v)          READ_ONCE((v)->counter)
* Unmerged path arch/sparc/include/asm/atomic_64.h
* Unmerged path arch/x86/include/asm/atomic.h
* Unmerged path arch/xtensa/include/asm/atomic.h
* Unmerged path include/asm-generic/atomic-instrumented.h
* Unmerged path include/asm-generic/atomic.h
diff --git a/include/linux/atomic.h b/include/linux/atomic.h
index 6ebab115d8ad..00be38250b44 100644
--- a/include/linux/atomic.h
+++ b/include/linux/atomic.h
@@ -521,6 +521,29 @@
 #endif
 #endif /* xchg_relaxed */
 
+/**
+ * atomic_fetch_add_unless - add unless the number is already a given value
+ * @v: pointer of type atomic_t
+ * @a: the amount to add to v...
+ * @u: ...unless v is equal to u.
+ *
+ * Atomically adds @a to @v, if @v was not already @u.
+ * Returns the original value of @v.
+ */
+#ifndef atomic_fetch_add_unless
+static inline int atomic_fetch_add_unless(atomic_t *v, int a, int u)
+{
+	int c = atomic_read(v);
+
+	do {
+		if (unlikely(c == u))
+			break;
+	} while (!atomic_try_cmpxchg(v, &c, c + a));
+
+	return c;
+}
+#endif
+
 /**
  * atomic_add_unless - add unless the number is already a given value
  * @v: pointer of type atomic_t
