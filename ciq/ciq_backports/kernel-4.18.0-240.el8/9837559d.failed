atomics/treewide: Make unconditional inc/dec ops optional

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit 9837559d8eb01ce834e56fc9a567c1d94ebd3698
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/9837559d.failed

Many of the inc/dec ops are mandatory, but for most architectures inc/dec are
simply trivial wrappers around their corresponding add/sub ops.

Let's make all the inc/dec ops optional, so that we can get rid of these
boilerplate wrappers.

The instrumented atomics are updated accordingly.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Reviewed-by: Will Deacon <will.deacon@arm.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Palmer Dabbelt <palmer@sifive.com>
	Cc: Boqun Feng <boqun.feng@gmail.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: https://lore.kernel.org/lkml/20180621121321.4761-17-mark.rutland@arm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 9837559d8eb01ce834e56fc9a567c1d94ebd3698)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/alpha/include/asm/atomic.h
#	arch/arc/include/asm/atomic.h
#	arch/arm/include/asm/atomic.h
#	arch/arm64/include/asm/atomic.h
#	arch/h8300/include/asm/atomic.h
#	arch/hexagon/include/asm/atomic.h
#	arch/ia64/include/asm/atomic.h
#	arch/mips/include/asm/atomic.h
#	arch/parisc/include/asm/atomic.h
#	arch/riscv/include/asm/atomic.h
#	arch/s390/include/asm/atomic.h
#	arch/sh/include/asm/atomic.h
#	arch/sparc/include/asm/atomic_32.h
#	arch/sparc/include/asm/atomic_64.h
#	arch/xtensa/include/asm/atomic.h
#	include/asm-generic/atomic.h
#	include/asm-generic/atomic64.h
diff --cc arch/alpha/include/asm/atomic.h
index 767bfdd42992,f6410cb68058..000000000000
--- a/arch/alpha/include/asm/atomic.h
+++ b/arch/alpha/include/asm/atomic.h
@@@ -296,30 -297,4 +296,33 @@@ static inline long atomic64_dec_if_posi
  	return old - 1;
  }
  
++<<<<<<< HEAD
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
 +#define atomic_add_negative(a, v) (atomic_add_return((a), (v)) < 0)
 +#define atomic64_add_negative(a, v) (atomic64_add_return((a), (v)) < 0)
 +
 +#define atomic_dec_return(v) atomic_sub_return(1,(v))
 +#define atomic64_dec_return(v) atomic64_sub_return(1,(v))
 +
 +#define atomic_inc_return(v) atomic_add_return(1,(v))
 +#define atomic64_inc_return(v) atomic64_add_return(1,(v))
 +
 +#define atomic_sub_and_test(i,v) (atomic_sub_return((i), (v)) == 0)
 +#define atomic64_sub_and_test(i,v) (atomic64_sub_return((i), (v)) == 0)
 +
 +#define atomic_inc_and_test(v) (atomic_add_return(1, (v)) == 0)
 +#define atomic64_inc_and_test(v) (atomic64_add_return(1, (v)) == 0)
 +
 +#define atomic_dec_and_test(v) (atomic_sub_return(1, (v)) == 0)
 +#define atomic64_dec_and_test(v) (atomic64_sub_return(1, (v)) == 0)
 +
 +#define atomic_inc(v) atomic_add(1,(v))
 +#define atomic64_inc(v) atomic64_add(1,(v))
 +
 +#define atomic_dec(v) atomic_sub(1,(v))
 +#define atomic64_dec(v) atomic64_sub(1,(v))
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #endif /* _ALPHA_ATOMIC_H */
diff --cc arch/arc/include/asm/atomic.h
index 11859287c52a,27b95a928c1e..000000000000
--- a/arch/arc/include/asm/atomic.h
+++ b/arch/arc/include/asm/atomic.h
@@@ -308,48 -308,6 +308,51 @@@ ATOMIC_OPS(xor, ^=, CTOP_INST_AXOR_DI_R
  #undef ATOMIC_OP_RETURN
  #undef ATOMIC_OP
  
++<<<<<<< HEAD
 +/**
 + * __atomic_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v
 + */
 +#define __atomic_add_unless(v, a, u)					\
 +({									\
 +	int c, old;							\
 +									\
 +	/*								\
 +	 * Explicit full memory barrier needed before/after as		\
 +	 * LLOCK/SCOND thmeselves don't provide any such semantics	\
 +	 */								\
 +	smp_mb();							\
 +									\
 +	c = atomic_read(v);						\
 +	while (c != (u) && (old = atomic_cmpxchg((v), c, c + (a))) != c)\
 +		c = old;						\
 +									\
 +	smp_mb();							\
 +									\
 +	c;								\
 +})
 +
 +#define atomic_inc_not_zero(v)		atomic_add_unless((v), 1, 0)
 +
 +#define atomic_inc(v)			atomic_add(1, v)
 +#define atomic_dec(v)			atomic_sub(1, v)
 +
 +#define atomic_inc_and_test(v)		(atomic_add_return(1, v) == 0)
 +#define atomic_dec_and_test(v)		(atomic_sub_return(1, v) == 0)
 +#define atomic_inc_return(v)		atomic_add_return(1, (v))
 +#define atomic_dec_return(v)		atomic_sub_return(1, (v))
 +#define atomic_sub_and_test(i, v)	(atomic_sub_return(i, v) == 0)
 +
 +#define atomic_add_negative(i, v)	(atomic_add_return(i, v) < 0)
 +
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #ifdef CONFIG_GENERIC_ATOMIC64
  
  #include <asm-generic/atomic64.h>
@@@ -594,19 -550,10 +597,22 @@@ static inline int atomic64_add_unless(a
  
  	smp_mb();
  
 -	return old;
 +	return op_done;
  }
 -#define atomic64_fetch_add_unless atomic64_fetch_add_unless
  
++<<<<<<< HEAD
 +#define atomic64_add_negative(a, v)	(atomic64_add_return((a), (v)) < 0)
 +#define atomic64_inc(v)			atomic64_add(1LL, (v))
 +#define atomic64_inc_return(v)		atomic64_add_return(1LL, (v))
 +#define atomic64_inc_and_test(v)	(atomic64_inc_return(v) == 0)
 +#define atomic64_sub_and_test(a, v)	(atomic64_sub_return((a), (v)) == 0)
 +#define atomic64_dec(v)			atomic64_sub(1LL, (v))
 +#define atomic64_dec_return(v)		atomic64_sub_return(1LL, (v))
 +#define atomic64_dec_and_test(v)	(atomic64_dec_return((v)) == 0)
 +#define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1LL, 0LL)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #endif	/* !CONFIG_GENERIC_ATOMIC64 */
  
  #endif	/* !__ASSEMBLY__ */
diff --cc arch/arm/include/asm/atomic.h
index 66d0e215a773,5a58d061d3d2..000000000000
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@@ -254,17 -245,6 +254,20 @@@ ATOMIC_OPS(xor, ^=, eor
  
  #define atomic_xchg(v, new) (xchg(&((v)->counter), new))
  
++<<<<<<< HEAD
 +#define atomic_inc(v)		atomic_add(1, v)
 +#define atomic_dec(v)		atomic_sub(1, v)
 +
 +#define atomic_inc_and_test(v)	(atomic_add_return(1, v) == 0)
 +#define atomic_dec_and_test(v)	(atomic_sub_return(1, v) == 0)
 +#define atomic_inc_return_relaxed(v)    (atomic_add_return_relaxed(1, v))
 +#define atomic_dec_return_relaxed(v)    (atomic_sub_return_relaxed(1, v))
 +#define atomic_sub_and_test(i, v) (atomic_sub_return(i, v) == 0)
 +
 +#define atomic_add_negative(i,v) (atomic_add_return(i, v) < 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #ifndef CONFIG_GENERIC_ATOMIC64
  typedef struct {
  	long long counter;
@@@ -520,22 -499,13 +523,25 @@@ static inline int atomic64_add_unless(a
  	: "r" (&v->counter), "r" (u), "r" (a)
  	: "cc");
  
 -	if (oldval != u)
 +	if (ret)
  		smp_mb();
  
 -	return oldval;
 +	return ret;
  }
 -#define atomic64_fetch_add_unless atomic64_fetch_add_unless
  
++<<<<<<< HEAD
 +#define atomic64_add_negative(a, v)	(atomic64_add_return((a), (v)) < 0)
 +#define atomic64_inc(v)			atomic64_add(1LL, (v))
 +#define atomic64_inc_return_relaxed(v)	atomic64_add_return_relaxed(1LL, (v))
 +#define atomic64_inc_and_test(v)	(atomic64_inc_return(v) == 0)
 +#define atomic64_sub_and_test(a, v)	(atomic64_sub_return((a), (v)) == 0)
 +#define atomic64_dec(v)			atomic64_sub(1LL, (v))
 +#define atomic64_dec_return_relaxed(v)	atomic64_sub_return_relaxed(1LL, (v))
 +#define atomic64_dec_and_test(v)	(atomic64_dec_return((v)) == 0)
 +#define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1LL, 0LL)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #endif /* !CONFIG_GENERIC_ATOMIC64 */
  #endif
  #endif
diff --cc arch/arm64/include/asm/atomic.h
index c0235e0ff849,078f785cd97f..000000000000
--- a/arch/arm64/include/asm/atomic.h
+++ b/arch/arm64/include/asm/atomic.h
@@@ -119,13 -98,6 +109,16 @@@
  	cmpxchg_release(&((v)->counter), (old), (new))
  #define atomic_cmpxchg(v, old, new)	cmpxchg(&((v)->counter), (old), (new))
  
++<<<<<<< HEAD
 +#define atomic_inc(v)			atomic_add(1, (v))
 +#define atomic_dec(v)			atomic_sub(1, (v))
 +#define atomic_inc_and_test(v)		(atomic_inc_return(v) == 0)
 +#define atomic_dec_and_test(v)		(atomic_dec_return(v) == 0)
 +#define atomic_sub_and_test(i, v)	(atomic_sub_return((i), (v)) == 0)
 +#define atomic_add_negative(i, v)	(atomic_add_return((i), (v)) < 0)
 +#define __atomic_add_unless(v, a, u)	___atomic_add_unless(v, a, u,)
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define atomic_andnot			atomic_andnot
  
  /*
@@@ -195,16 -157,7 +178,19 @@@
  #define atomic64_cmpxchg_release	atomic_cmpxchg_release
  #define atomic64_cmpxchg		atomic_cmpxchg
  
++<<<<<<< HEAD
 +#define atomic64_inc(v)			atomic64_add(1, (v))
 +#define atomic64_dec(v)			atomic64_sub(1, (v))
 +#define atomic64_inc_and_test(v)	(atomic64_inc_return(v) == 0)
 +#define atomic64_dec_and_test(v)	(atomic64_dec_return(v) == 0)
 +#define atomic64_sub_and_test(i, v)	(atomic64_sub_return((i), (v)) == 0)
 +#define atomic64_add_negative(i, v)	(atomic64_add_return((i), (v)) < 0)
 +#define atomic64_add_unless(v, a, u)	(___atomic_add_unless(v, a, u, 64) != u)
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define atomic64_andnot			atomic64_andnot
  
 +#define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1, 0)
 +
  #endif
  #endif
diff --cc arch/h8300/include/asm/atomic.h
index 941e7554e886,c6b6a06231b2..000000000000
--- a/arch/h8300/include/asm/atomic.h
+++ b/arch/h8300/include/asm/atomic.h
@@@ -69,18 -69,6 +69,21 @@@ ATOMIC_OPS(sub, -=
  #undef ATOMIC_OP_RETURN
  #undef ATOMIC_OP
  
++<<<<<<< HEAD
 +#define atomic_add_negative(a, v)	(atomic_add_return((a), (v)) < 0)
 +#define atomic_sub_and_test(i, v)	(atomic_sub_return(i, v) == 0)
 +
 +#define atomic_inc_return(v)		atomic_add_return(1, v)
 +#define atomic_dec_return(v)		atomic_sub_return(1, v)
 +
 +#define atomic_inc(v)			(void)atomic_inc_return(v)
 +#define atomic_inc_and_test(v)		(atomic_inc_return(v) == 0)
 +
 +#define atomic_dec(v)			(void)atomic_dec_return(v)
 +#define atomic_dec_and_test(v)		(atomic_dec_return(v) == 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  static inline int atomic_cmpxchg(atomic_t *v, int old, int new)
  {
  	int ret;
diff --cc arch/hexagon/include/asm/atomic.h
index fb3dfb2a667e,311b9894ccc8..000000000000
--- a/arch/hexagon/include/asm/atomic.h
+++ b/arch/hexagon/include/asm/atomic.h
@@@ -196,18 -196,6 +196,21 @@@ static inline int __atomic_add_unless(a
  	);
  	return __oldval;
  }
 -#define atomic_fetch_add_unless atomic_fetch_add_unless
  
 +#define atomic_inc_not_zero(v) atomic_add_unless((v), 1, 0)
 +
++<<<<<<< HEAD
 +#define atomic_inc(v) atomic_add(1, (v))
 +#define atomic_dec(v) atomic_sub(1, (v))
 +
 +#define atomic_inc_and_test(v) (atomic_add_return(1, (v)) == 0)
 +#define atomic_dec_and_test(v) (atomic_sub_return(1, (v)) == 0)
 +#define atomic_sub_and_test(i, v) (atomic_sub_return(i, (v)) == 0)
 +#define atomic_add_negative(i, v) (atomic_add_return(i, (v)) < 0)
 +
 +#define atomic_inc_return(v) (atomic_add_return(1, v))
 +#define atomic_dec_return(v) (atomic_sub_return(1, v))
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #endif
diff --cc arch/ia64/include/asm/atomic.h
index 2524fb60fbc2,46a15a974bed..000000000000
--- a/arch/ia64/include/asm/atomic.h
+++ b/arch/ia64/include/asm/atomic.h
@@@ -264,38 -231,8 +264,39 @@@ static __inline__ long atomic64_dec_if_
  	return dec;
  }
  
++<<<<<<< HEAD
 +/*
 + * Atomically add I to V and return TRUE if the resulting value is
 + * negative.
 + */
 +static __inline__ int
 +atomic_add_negative (int i, atomic_t *v)
 +{
 +	return atomic_add_return(i, v) < 0;
 +}
 +
 +static __inline__ long
 +atomic64_add_negative (__s64 i, atomic64_t *v)
 +{
 +	return atomic64_add_return(i, v) < 0;
 +}
 +
 +#define atomic_dec_return(v)		atomic_sub_return(1, (v))
 +#define atomic_inc_return(v)		atomic_add_return(1, (v))
 +#define atomic64_dec_return(v)		atomic64_sub_return(1, (v))
 +#define atomic64_inc_return(v)		atomic64_add_return(1, (v))
 +
 +#define atomic_sub_and_test(i,v)	(atomic_sub_return((i), (v)) == 0)
 +#define atomic_dec_and_test(v)		(atomic_sub_return(1, (v)) == 0)
 +#define atomic_inc_and_test(v)		(atomic_add_return(1, (v)) == 0)
 +#define atomic64_sub_and_test(i,v)	(atomic64_sub_return((i), (v)) == 0)
 +#define atomic64_dec_and_test(v)	(atomic64_sub_return(1, (v)) == 0)
 +#define atomic64_inc_and_test(v)	(atomic64_add_return(1, (v)) == 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define atomic_add(i,v)			(void)atomic_add_return((i), (v))
  #define atomic_sub(i,v)			(void)atomic_sub_return((i), (v))
- #define atomic_inc(v)			atomic_add(1, (v))
- #define atomic_dec(v)			atomic_sub(1, (v))
  
  #define atomic64_add(i,v)		(void)atomic64_add_return((i), (v))
  #define atomic64_sub(i,v)		(void)atomic64_sub_return((i), (v))
diff --cc arch/mips/include/asm/atomic.h
index 0ab176bdb8e8,79be687de4ab..000000000000
--- a/arch/mips/include/asm/atomic.h
+++ b/arch/mips/include/asm/atomic.h
@@@ -274,97 -274,12 +274,103 @@@ static __inline__ int atomic_sub_if_pos
  #define atomic_cmpxchg(v, o, n) (cmpxchg(&((v)->counter), (o), (n)))
  #define atomic_xchg(v, new) (xchg(&((v)->counter), (new)))
  
++<<<<<<< HEAD
 +/**
 + * __atomic_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns the old value of @v.
 + */
 +static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 +{
 +	int c, old;
 +	c = atomic_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c;
 +}
 +
 +#define atomic_dec_return(v) atomic_sub_return(1, (v))
 +#define atomic_inc_return(v) atomic_add_return(1, (v))
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
 +/*
 + * atomic_sub_and_test - subtract value from variable and test result
 + * @i: integer value to subtract
 + * @v: pointer of type atomic_t
 + *
 + * Atomically subtracts @i from @v and returns
 + * true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic_sub_and_test(i, v) (atomic_sub_return((i), (v)) == 0)
 +
 +/*
 + * atomic_inc_and_test - increment and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1
 + * and returns true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic_inc_and_test(v) (atomic_inc_return(v) == 0)
 +
 +/*
 + * atomic_dec_and_test - decrement by 1 and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically decrements @v by 1 and
 + * returns true if the result is 0, or false for all other
 + * cases.
 + */
 +#define atomic_dec_and_test(v) (atomic_sub_return(1, (v)) == 0)
 +
  /*
   * atomic_dec_if_positive - decrement by 1 if old value positive
   * @v: pointer of type atomic_t
   */
  #define atomic_dec_if_positive(v)	atomic_sub_if_positive(1, v)
  
++<<<<<<< HEAD
 +/*
 + * atomic_inc - increment atomic variable
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1.
 + */
 +#define atomic_inc(v) atomic_add(1, (v))
 +
 +/*
 + * atomic_dec - decrement and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically decrements @v by 1.
 + */
 +#define atomic_dec(v) atomic_sub(1, (v))
 +
 +/*
 + * atomic_add_negative - add and test if negative
 + * @v: pointer of type atomic_t
 + * @i: integer value to add
 + *
 + * Atomically adds @i to @v and returns true
 + * if the result is negative, or false when
 + * result is greater than or equal to zero.
 + */
 +#define atomic_add_negative(i, v) (atomic_add_return(i, (v)) < 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #ifdef CONFIG_64BIT
  
  #define ATOMIC64_INIT(i)    { (i) }
@@@ -620,99 -535,12 +626,105 @@@ static __inline__ long atomic64_sub_if_
  	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
  #define atomic64_xchg(v, new) (xchg(&((v)->counter), (new)))
  
++<<<<<<< HEAD
 +/**
 + * atomic64_add_unless - add unless the number is a given value
 + * @v: pointer of type atomic64_t
 + * @a: the amount to add to v...
 + * @u: ...unless v is equal to u.
 + *
 + * Atomically adds @a to @v, so long as it was not @u.
 + * Returns true iff @v was not @u.
 + */
 +static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
 +{
 +	long c, old;
 +	c = atomic64_read(v);
 +	for (;;) {
 +		if (unlikely(c == (u)))
 +			break;
 +		old = atomic64_cmpxchg((v), c, c + (a));
 +		if (likely(old == c))
 +			break;
 +		c = old;
 +	}
 +	return c != (u);
 +}
 +
 +#define atomic64_inc_not_zero(v) atomic64_add_unless((v), 1, 0)
 +
 +#define atomic64_dec_return(v) atomic64_sub_return(1, (v))
 +#define atomic64_inc_return(v) atomic64_add_return(1, (v))
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
 +/*
 + * atomic64_sub_and_test - subtract value from variable and test result
 + * @i: integer value to subtract
 + * @v: pointer of type atomic64_t
 + *
 + * Atomically subtracts @i from @v and returns
 + * true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic64_sub_and_test(i, v) (atomic64_sub_return((i), (v)) == 0)
 +
 +/*
 + * atomic64_inc_and_test - increment and test
 + * @v: pointer of type atomic64_t
 + *
 + * Atomically increments @v by 1
 + * and returns true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic64_inc_and_test(v) (atomic64_inc_return(v) == 0)
 +
 +/*
 + * atomic64_dec_and_test - decrement by 1 and test
 + * @v: pointer of type atomic64_t
 + *
 + * Atomically decrements @v by 1 and
 + * returns true if the result is 0, or false for all other
 + * cases.
 + */
 +#define atomic64_dec_and_test(v) (atomic64_sub_return(1, (v)) == 0)
 +
  /*
   * atomic64_dec_if_positive - decrement by 1 if old value positive
   * @v: pointer of type atomic64_t
   */
  #define atomic64_dec_if_positive(v)	atomic64_sub_if_positive(1, v)
  
++<<<<<<< HEAD
 +/*
 + * atomic64_inc - increment atomic variable
 + * @v: pointer of type atomic64_t
 + *
 + * Atomically increments @v by 1.
 + */
 +#define atomic64_inc(v) atomic64_add(1, (v))
 +
 +/*
 + * atomic64_dec - decrement and test
 + * @v: pointer of type atomic64_t
 + *
 + * Atomically decrements @v by 1.
 + */
 +#define atomic64_dec(v) atomic64_sub(1, (v))
 +
 +/*
 + * atomic64_add_negative - add and test if negative
 + * @v: pointer of type atomic64_t
 + * @i: integer value to add
 + *
 + * Atomically adds @i to @v and returns true
 + * if the result is negative, or false when
 + * result is greater than or equal to zero.
 + */
 +#define atomic64_add_negative(i, v) (atomic64_add_return(i, (v)) < 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #endif /* CONFIG_64BIT */
  
  #endif /* _ASM_ATOMIC_H */
diff --cc arch/parisc/include/asm/atomic.h
index 88bae6676c9b,10bc490327c1..000000000000
--- a/arch/parisc/include/asm/atomic.h
+++ b/arch/parisc/include/asm/atomic.h
@@@ -160,28 -136,6 +160,31 @@@ ATOMIC_OPS(xor, ^=
  #undef ATOMIC_OP_RETURN
  #undef ATOMIC_OP
  
++<<<<<<< HEAD
 +#define atomic_inc(v)	(atomic_add(   1,(v)))
 +#define atomic_dec(v)	(atomic_add(  -1,(v)))
 +
 +#define atomic_inc_return(v)	(atomic_add_return(   1,(v)))
 +#define atomic_dec_return(v)	(atomic_add_return(  -1,(v)))
 +
 +#define atomic_add_negative(a, v)	(atomic_add_return((a), (v)) < 0)
 +
 +/*
 + * atomic_inc_and_test - increment and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1
 + * and returns true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic_inc_and_test(v) (atomic_inc_return(v) == 0)
 +
 +#define atomic_dec_and_test(v)	(atomic_dec_return(v) == 0)
 +
 +#define atomic_sub_and_test(i,v)	(atomic_sub_return((i),(v)) == 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define ATOMIC_INIT(i)	{ (i) }
  
  #ifdef CONFIG_64BIT
@@@ -264,18 -218,6 +267,21 @@@ atomic64_read(const atomic64_t *v
  	return READ_ONCE((v)->counter);
  }
  
++<<<<<<< HEAD
 +#define atomic64_inc(v)		(atomic64_add(   1,(v)))
 +#define atomic64_dec(v)		(atomic64_add(  -1,(v)))
 +
 +#define atomic64_inc_return(v)		(atomic64_add_return(   1,(v)))
 +#define atomic64_dec_return(v)		(atomic64_add_return(  -1,(v)))
 +
 +#define atomic64_add_negative(a, v)	(atomic64_add_return((a), (v)) < 0)
 +
 +#define atomic64_inc_and_test(v) 	(atomic64_inc_return(v) == 0)
 +#define atomic64_dec_and_test(v)	(atomic64_dec_return(v) == 0)
 +#define atomic64_sub_and_test(i,v)	(atomic64_sub_return((i),(v)) == 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  /* exported interface */
  #define atomic64_cmpxchg(v, o, n) \
  	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
diff --cc arch/riscv/include/asm/atomic.h
index 855115ace98c,512b89485790..000000000000
--- a/arch/riscv/include/asm/atomic.h
+++ b/arch/riscv/include/asm/atomic.h
@@@ -209,130 -209,8 +209,133 @@@ ATOMIC_OPS(xor, xor, i
  #undef ATOMIC_FETCH_OP
  #undef ATOMIC_OP_RETURN
  
++<<<<<<< HEAD
 +/*
 + * The extra atomic operations that are constructed from one of the core
 + * AMO-based operations above (aside from sub, which is easier to fit above).
 + * These are required to perform a full barrier, but they're OK this way
 + * because atomic_*_return is also required to perform a full barrier.
 + *
 + */
 +#define ATOMIC_OP(op, func_op, comp_op, I, c_type, prefix)		\
 +static __always_inline							\
 +bool atomic##prefix##_##op(c_type i, atomic##prefix##_t *v)		\
 +{									\
 +	return atomic##prefix##_##func_op##_return(i, v) comp_op I;	\
 +}
 +
 +#ifdef CONFIG_GENERIC_ATOMIC64
 +#define ATOMIC_OPS(op, func_op, comp_op, I)				\
 +        ATOMIC_OP(op, func_op, comp_op, I,  int,   )
 +#else
 +#define ATOMIC_OPS(op, func_op, comp_op, I)				\
 +        ATOMIC_OP(op, func_op, comp_op, I,  int,   )			\
 +        ATOMIC_OP(op, func_op, comp_op, I, long, 64)
 +#endif
 +
 +ATOMIC_OPS(add_and_test, add, ==, 0)
 +ATOMIC_OPS(sub_and_test, sub, ==, 0)
 +ATOMIC_OPS(add_negative, add,  <, 0)
 +
 +#undef ATOMIC_OP
 +#undef ATOMIC_OPS
 +
 +#define ATOMIC_OP(op, func_op, I, c_type, prefix)			\
 +static __always_inline							\
 +void atomic##prefix##_##op(atomic##prefix##_t *v)			\
 +{									\
 +	atomic##prefix##_##func_op(I, v);				\
 +}
 +
 +#define ATOMIC_FETCH_OP(op, func_op, I, c_type, prefix)			\
 +static __always_inline							\
 +c_type atomic##prefix##_fetch_##op##_relaxed(atomic##prefix##_t *v)	\
 +{									\
 +	return atomic##prefix##_fetch_##func_op##_relaxed(I, v);	\
 +}									\
 +static __always_inline							\
 +c_type atomic##prefix##_fetch_##op(atomic##prefix##_t *v)		\
 +{									\
 +	return atomic##prefix##_fetch_##func_op(I, v);			\
 +}
 +
 +#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, c_type, prefix)		\
 +static __always_inline							\
 +c_type atomic##prefix##_##op##_return_relaxed(atomic##prefix##_t *v)	\
 +{									\
 +        return atomic##prefix##_fetch_##op##_relaxed(v) c_op I;		\
 +}									\
 +static __always_inline							\
 +c_type atomic##prefix##_##op##_return(atomic##prefix##_t *v)		\
 +{									\
 +        return atomic##prefix##_fetch_##op(v) c_op I;			\
 +}
 +
 +#ifdef CONFIG_GENERIC_ATOMIC64
 +#define ATOMIC_OPS(op, asm_op, c_op, I)					\
 +        ATOMIC_OP(       op, asm_op,       I,  int,   )			\
 +        ATOMIC_FETCH_OP( op, asm_op,       I,  int,   )			\
 +        ATOMIC_OP_RETURN(op, asm_op, c_op, I,  int,   )
 +#else
 +#define ATOMIC_OPS(op, asm_op, c_op, I)					\
 +        ATOMIC_OP(       op, asm_op,       I,  int,   )			\
 +        ATOMIC_FETCH_OP( op, asm_op,       I,  int,   )			\
 +        ATOMIC_OP_RETURN(op, asm_op, c_op, I,  int,   )			\
 +        ATOMIC_OP(       op, asm_op,       I, long, 64)			\
 +        ATOMIC_FETCH_OP( op, asm_op,       I, long, 64)			\
 +        ATOMIC_OP_RETURN(op, asm_op, c_op, I, long, 64)
 +#endif
 +
 +ATOMIC_OPS(inc, add, +,  1)
 +ATOMIC_OPS(dec, add, +, -1)
 +
 +#define atomic_inc_return_relaxed	atomic_inc_return_relaxed
 +#define atomic_dec_return_relaxed	atomic_dec_return_relaxed
 +#define atomic_inc_return		atomic_inc_return
 +#define atomic_dec_return		atomic_dec_return
 +
 +#define atomic_fetch_inc_relaxed	atomic_fetch_inc_relaxed
 +#define atomic_fetch_dec_relaxed	atomic_fetch_dec_relaxed
 +#define atomic_fetch_inc		atomic_fetch_inc
 +#define atomic_fetch_dec		atomic_fetch_dec
 +
 +#ifndef CONFIG_GENERIC_ATOMIC64
 +#define atomic64_inc_return_relaxed	atomic64_inc_return_relaxed
 +#define atomic64_dec_return_relaxed	atomic64_dec_return_relaxed
 +#define atomic64_inc_return		atomic64_inc_return
 +#define atomic64_dec_return		atomic64_dec_return
 +
 +#define atomic64_fetch_inc_relaxed	atomic64_fetch_inc_relaxed
 +#define atomic64_fetch_dec_relaxed	atomic64_fetch_dec_relaxed
 +#define atomic64_fetch_inc		atomic64_fetch_inc
 +#define atomic64_fetch_dec		atomic64_fetch_dec
 +#endif
 +
 +#undef ATOMIC_OPS
 +#undef ATOMIC_OP
 +#undef ATOMIC_FETCH_OP
 +#undef ATOMIC_OP_RETURN
 +
 +#define ATOMIC_OP(op, func_op, comp_op, I, prefix)			\
 +static __always_inline							\
 +bool atomic##prefix##_##op(atomic##prefix##_t *v)			\
 +{									\
 +	return atomic##prefix##_##func_op##_return(v) comp_op I;	\
 +}
 +
 +ATOMIC_OP(inc_and_test, inc, ==, 0,   )
 +ATOMIC_OP(dec_and_test, dec, ==, 0,   )
 +#ifndef CONFIG_GENERIC_ATOMIC64
 +ATOMIC_OP(inc_and_test, inc, ==, 0, 64)
 +ATOMIC_OP(dec_and_test, dec, ==, 0, 64)
 +#endif
 +
 +#undef ATOMIC_OP
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  /* This is required to provide a full barrier on success. */
 -static __always_inline int atomic_fetch_add_unless(atomic_t *v, int a, int u)
 +static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)
  {
         int prev, rc;
  
diff --cc arch/s390/include/asm/atomic.h
index 4b55532f15c4,376e64af951f..000000000000
--- a/arch/s390/include/asm/atomic.h
+++ b/arch/s390/include/asm/atomic.h
@@@ -55,17 -55,9 +55,23 @@@ static inline void atomic_add(int i, at
  	__atomic_add(i, &v->counter);
  }
  
++<<<<<<< HEAD
 +#define atomic_add_negative(_i, _v)	(atomic_add_return(_i, _v) < 0)
 +#define atomic_inc(_v)			atomic_add(1, _v)
 +#define atomic_inc_return(_v)		atomic_add_return(1, _v)
 +#define atomic_inc_and_test(_v)		(atomic_add_return(1, _v) == 0)
  #define atomic_sub(_i, _v)		atomic_add(-(int)(_i), _v)
  #define atomic_sub_return(_i, _v)	atomic_add_return(-(int)(_i), _v)
  #define atomic_fetch_sub(_i, _v)	atomic_fetch_add(-(int)(_i), _v)
 +#define atomic_sub_and_test(_i, _v)	(atomic_sub_return(_i, _v) == 0)
 +#define atomic_dec(_v)			atomic_sub(1, _v)
 +#define atomic_dec_return(_v)		atomic_sub_return(1, _v)
 +#define atomic_dec_and_test(_v)		(atomic_sub_return(1, _v) == 0)
++=======
++#define atomic_sub(_i, _v)		atomic_add(-(int)(_i), _v)
++#define atomic_sub_return(_i, _v)	atomic_add_return(-(int)(_i), _v)
++#define atomic_fetch_sub(_i, _v)	atomic_fetch_add(-(int)(_i), _v)
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  
  #define ATOMIC_OPS(op)							\
  static inline void atomic_##op(int i, atomic_t *v)			\
@@@ -201,17 -162,8 +207,23 @@@ static inline long atomic64_dec_if_posi
  	return dec;
  }
  
++<<<<<<< HEAD
 +#define atomic64_add_negative(_i, _v)	(atomic64_add_return(_i, _v) < 0)
 +#define atomic64_inc(_v)		atomic64_add(1, _v)
 +#define atomic64_inc_return(_v)		atomic64_add_return(1, _v)
 +#define atomic64_inc_and_test(_v)	(atomic64_add_return(1, _v) == 0)
 +#define atomic64_sub_return(_i, _v)	atomic64_add_return(-(long)(_i), _v)
 +#define atomic64_fetch_sub(_i, _v)	atomic64_fetch_add(-(long)(_i), _v)
 +#define atomic64_sub(_i, _v)		atomic64_add(-(long)(_i), _v)
 +#define atomic64_sub_and_test(_i, _v)	(atomic64_sub_return(_i, _v) == 0)
 +#define atomic64_dec(_v)		atomic64_sub(1, _v)
 +#define atomic64_dec_return(_v)		atomic64_sub_return(1, _v)
 +#define atomic64_dec_and_test(_v)	(atomic64_sub_return(1, _v) == 0)
 +#define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1, 0)
++=======
+ #define atomic64_sub_return(_i, _v)	atomic64_add_return(-(long)(_i), _v)
+ #define atomic64_fetch_sub(_i, _v)	atomic64_fetch_add(-(long)(_i), _v)
+ #define atomic64_sub(_i, _v)		atomic64_add(-(long)(_i), _v)
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  
  #endif /* __ARCH_S390_ATOMIC__  */
diff --cc arch/sh/include/asm/atomic.h
index 0fd0099f43cc,f37b95a80232..000000000000
--- a/arch/sh/include/asm/atomic.h
+++ b/arch/sh/include/asm/atomic.h
@@@ -32,16 -32,6 +32,19 @@@
  #include <asm/atomic-irq.h>
  #endif
  
++<<<<<<< HEAD
 +#define atomic_add_negative(a, v)	(atomic_add_return((a), (v)) < 0)
 +#define atomic_dec_return(v)		atomic_sub_return(1, (v))
 +#define atomic_inc_return(v)		atomic_add_return(1, (v))
 +#define atomic_inc_and_test(v)		(atomic_inc_return(v) == 0)
 +#define atomic_sub_and_test(i,v)	(atomic_sub_return((i), (v)) == 0)
 +#define atomic_dec_and_test(v)		(atomic_sub_return(1, (v)) == 0)
 +
 +#define atomic_inc(v)			atomic_add(1, (v))
 +#define atomic_dec(v)			atomic_sub(1, (v))
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define atomic_xchg(v, new)		(xchg(&((v)->counter), new))
  #define atomic_cmpxchg(v, o, n)		(cmpxchg(&((v)->counter), (o), (n)))
  
diff --cc arch/sparc/include/asm/atomic_32.h
index d13ce517f4b9,94c930f0bc62..000000000000
--- a/arch/sparc/include/asm/atomic_32.h
+++ b/arch/sparc/include/asm/atomic_32.h
@@@ -46,22 -46,4 +44,25 @@@ void atomic_set(atomic_t *, int)
  #define atomic_sub_return(i, v)	(atomic_add_return(-(int)(i), (v)))
  #define atomic_fetch_sub(i, v)  (atomic_fetch_add (-(int)(i), (v)))
  
++<<<<<<< HEAD
 +#define atomic_inc_return(v)	(atomic_add_return(        1, (v)))
 +#define atomic_dec_return(v)	(atomic_add_return(       -1, (v)))
 +
 +#define atomic_add_negative(a, v)	(atomic_add_return((a), (v)) < 0)
 +
 +/*
 + * atomic_inc_and_test - increment and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1
 + * and returns true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic_inc_and_test(v) (atomic_inc_return(v) == 0)
 +
 +#define atomic_dec_and_test(v) (atomic_dec_return(v) == 0)
 +#define atomic_sub_and_test(i, v) (atomic_sub_return(i, v) == 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #endif /* !(__ARCH_SPARC_ATOMIC__) */
diff --cc arch/sparc/include/asm/atomic_64.h
index 28db058d471b,304865c7cdbb..000000000000
--- a/arch/sparc/include/asm/atomic_64.h
+++ b/arch/sparc/include/asm/atomic_64.h
@@@ -50,38 -50,6 +50,41 @@@ ATOMIC_OPS(xor
  #undef ATOMIC_OP_RETURN
  #undef ATOMIC_OP
  
++<<<<<<< HEAD
 +#define atomic_dec_return(v)   atomic_sub_return(1, v)
 +#define atomic64_dec_return(v) atomic64_sub_return(1, v)
 +
 +#define atomic_inc_return(v)   atomic_add_return(1, v)
 +#define atomic64_inc_return(v) atomic64_add_return(1, v)
 +
 +/*
 + * atomic_inc_and_test - increment and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1
 + * and returns true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic_inc_and_test(v) (atomic_inc_return(v) == 0)
 +#define atomic64_inc_and_test(v) (atomic64_inc_return(v) == 0)
 +
 +#define atomic_sub_and_test(i, v) (atomic_sub_return(i, v) == 0)
 +#define atomic64_sub_and_test(i, v) (atomic64_sub_return(i, v) == 0)
 +
 +#define atomic_dec_and_test(v) (atomic_sub_return(1, v) == 0)
 +#define atomic64_dec_and_test(v) (atomic64_sub_return(1, v) == 0)
 +
 +#define atomic_inc(v) atomic_add(1, v)
 +#define atomic64_inc(v) atomic64_add(1, v)
 +
 +#define atomic_dec(v) atomic_sub(1, v)
 +#define atomic64_dec(v) atomic64_sub(1, v)
 +
 +#define atomic_add_negative(i, v) (atomic_add_return(i, v) < 0)
 +#define atomic64_add_negative(i, v) (atomic64_add_return(i, v) < 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define atomic_cmpxchg(v, o, n) (cmpxchg(&((v)->counter), (o), (n)))
  
  static inline int atomic_xchg(atomic_t *v, int new)
diff --cc arch/xtensa/include/asm/atomic.h
index e7a23f2a519a,7de0149e1cf7..000000000000
--- a/arch/xtensa/include/asm/atomic.h
+++ b/arch/xtensa/include/asm/atomic.h
@@@ -197,80 -197,6 +197,83 @@@ ATOMIC_OPS(xor
  #undef ATOMIC_OP_RETURN
  #undef ATOMIC_OP
  
++<<<<<<< HEAD
 +/**
 + * atomic_sub_and_test - subtract value from variable and test result
 + * @i: integer value to subtract
 + * @v: pointer of type atomic_t
 + *
 + * Atomically subtracts @i from @v and returns
 + * true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic_sub_and_test(i,v) (atomic_sub_return((i),(v)) == 0)
 +
 +/**
 + * atomic_inc - increment atomic variable
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1.
 + */
 +#define atomic_inc(v) atomic_add(1,(v))
 +
 +/**
 + * atomic_inc - increment atomic variable
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1.
 + */
 +#define atomic_inc_return(v) atomic_add_return(1,(v))
 +
 +/**
 + * atomic_dec - decrement atomic variable
 + * @v: pointer of type atomic_t
 + *
 + * Atomically decrements @v by 1.
 + */
 +#define atomic_dec(v) atomic_sub(1,(v))
 +
 +/**
 + * atomic_dec_return - decrement atomic variable
 + * @v: pointer of type atomic_t
 + *
 + * Atomically decrements @v by 1.
 + */
 +#define atomic_dec_return(v) atomic_sub_return(1,(v))
 +
 +/**
 + * atomic_dec_and_test - decrement and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically decrements @v by 1 and
 + * returns true if the result is 0, or false for all other
 + * cases.
 + */
 +#define atomic_dec_and_test(v) (atomic_sub_return(1,(v)) == 0)
 +
 +/**
 + * atomic_inc_and_test - increment and test
 + * @v: pointer of type atomic_t
 + *
 + * Atomically increments @v by 1
 + * and returns true if the result is zero, or false for all
 + * other cases.
 + */
 +#define atomic_inc_and_test(v) (atomic_add_return(1,(v)) == 0)
 +
 +/**
 + * atomic_add_negative - add and test if negative
 + * @v: pointer of type atomic_t
 + * @i: integer value to add
 + *
 + * Atomically adds @i to @v and returns true
 + * if the result is negative, or false when
 + * result is greater than or equal to zero.
 + */
 +#define atomic_add_negative(i,v) (atomic_add_return((i),(v)) < 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define atomic_cmpxchg(v, o, n) ((int)cmpxchg(&((v)->counter), (o), (n)))
  #define atomic_xchg(v, new) (xchg(&((v)->counter), new))
  
diff --cc include/asm-generic/atomic.h
index abe6dd9ca2a8,13324aa828eb..000000000000
--- a/include/asm-generic/atomic.h
+++ b/include/asm-generic/atomic.h
@@@ -201,23 -196,6 +201,26 @@@ static inline void atomic_sub(int i, at
  	atomic_sub_return(i, v);
  }
  
++<<<<<<< HEAD
 +static inline void atomic_inc(atomic_t *v)
 +{
 +	atomic_add_return(1, v);
 +}
 +
 +static inline void atomic_dec(atomic_t *v)
 +{
 +	atomic_sub_return(1, v);
 +}
 +
 +#define atomic_dec_return(v)		atomic_sub_return(1, (v))
 +#define atomic_inc_return(v)		atomic_add_return(1, (v))
 +
 +#define atomic_sub_and_test(i, v)	(atomic_sub_return((i), (v)) == 0)
 +#define atomic_dec_and_test(v)		(atomic_dec_return(v) == 0)
 +#define atomic_inc_and_test(v)		(atomic_inc_return(v) == 0)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #define atomic_xchg(ptr, v)		(xchg(&(ptr)->counter, (v)))
  #define atomic_cmpxchg(v, old, new)	(cmpxchg(&((v)->counter), (old), (new)))
  
diff --cc include/asm-generic/atomic64.h
index 6b016e354b56,242b79ae0b57..000000000000
--- a/include/asm-generic/atomic64.h
+++ b/include/asm-generic/atomic64.h
@@@ -56,14 -56,4 +56,17 @@@ extern long long atomic64_xchg(atomic64
  extern long long atomic64_fetch_add_unless(atomic64_t *v, long long a, long long u);
  #define atomic64_fetch_add_unless atomic64_fetch_add_unless
  
++<<<<<<< HEAD
 +#define atomic64_add_negative(a, v)	(atomic64_add_return((a), (v)) < 0)
 +#define atomic64_inc(v)			atomic64_add(1LL, (v))
 +#define atomic64_inc_return(v)		atomic64_add_return(1LL, (v))
 +#define atomic64_inc_and_test(v) 	(atomic64_inc_return(v) == 0)
 +#define atomic64_sub_and_test(a, v)	(atomic64_sub_return((a), (v)) == 0)
 +#define atomic64_dec(v)			atomic64_sub(1LL, (v))
 +#define atomic64_dec_return(v)		atomic64_sub_return(1LL, (v))
 +#define atomic64_dec_and_test(v)	(atomic64_dec_return((v)) == 0)
 +#define atomic64_inc_not_zero(v) 	atomic64_add_unless((v), 1LL, 0LL)
 +
++=======
++>>>>>>> 9837559d8eb0 (atomics/treewide: Make unconditional inc/dec ops optional)
  #endif  /*  _ASM_GENERIC_ATOMIC64_H  */
* Unmerged path arch/alpha/include/asm/atomic.h
* Unmerged path arch/arc/include/asm/atomic.h
* Unmerged path arch/arm/include/asm/atomic.h
* Unmerged path arch/arm64/include/asm/atomic.h
* Unmerged path arch/h8300/include/asm/atomic.h
* Unmerged path arch/hexagon/include/asm/atomic.h
* Unmerged path arch/ia64/include/asm/atomic.h
diff --git a/arch/m68k/include/asm/atomic.h b/arch/m68k/include/asm/atomic.h
index e993e2860ee1..3201a3e38f22 100644
--- a/arch/m68k/include/asm/atomic.h
+++ b/arch/m68k/include/asm/atomic.h
@@ -126,11 +126,13 @@ static inline void atomic_inc(atomic_t *v)
 {
 	__asm__ __volatile__("addql #1,%0" : "+m" (*v));
 }
+#define atomic_inc atomic_inc
 
 static inline void atomic_dec(atomic_t *v)
 {
 	__asm__ __volatile__("subql #1,%0" : "+m" (*v));
 }
+#define atomic_dec atomic_dec
 
 static inline int atomic_dec_and_test(atomic_t *v)
 {
@@ -190,9 +192,6 @@ static inline int atomic_xchg(atomic_t *v, int new)
 
 #endif /* !CONFIG_RMW_INSNS */
 
-#define atomic_dec_return(v)	atomic_sub_return(1, (v))
-#define atomic_inc_return(v)	atomic_add_return(1, (v))
-
 static inline int atomic_sub_and_test(int i, atomic_t *v)
 {
 	char c;
* Unmerged path arch/mips/include/asm/atomic.h
* Unmerged path arch/parisc/include/asm/atomic.h
diff --git a/arch/powerpc/include/asm/atomic.h b/arch/powerpc/include/asm/atomic.h
index 682b3e6a1e21..a35c890f549a 100644
--- a/arch/powerpc/include/asm/atomic.h
+++ b/arch/powerpc/include/asm/atomic.h
@@ -145,6 +145,7 @@ static __inline__ void atomic_inc(atomic_t *v)
 	: "r" (&v->counter)
 	: "cc", "xer");
 }
+#define atomic_inc atomic_inc
 
 static __inline__ int atomic_inc_return_relaxed(atomic_t *v)
 {
@@ -187,6 +188,7 @@ static __inline__ void atomic_dec(atomic_t *v)
 	: "r" (&v->counter)
 	: "cc", "xer");
 }
+#define atomic_dec atomic_dec
 
 static __inline__ int atomic_dec_return_relaxed(atomic_t *v)
 {
@@ -427,6 +429,7 @@ static __inline__ void atomic64_inc(atomic64_t *v)
 	: "r" (&v->counter)
 	: "cc", "xer");
 }
+#define atomic64_inc atomic64_inc
 
 static __inline__ long atomic64_inc_return_relaxed(atomic64_t *v)
 {
@@ -467,6 +470,7 @@ static __inline__ void atomic64_dec(atomic64_t *v)
 	: "r" (&v->counter)
 	: "cc", "xer");
 }
+#define atomic64_dec atomic64_dec
 
 static __inline__ long atomic64_dec_return_relaxed(atomic64_t *v)
 {
* Unmerged path arch/riscv/include/asm/atomic.h
* Unmerged path arch/s390/include/asm/atomic.h
* Unmerged path arch/sh/include/asm/atomic.h
* Unmerged path arch/sparc/include/asm/atomic_32.h
* Unmerged path arch/sparc/include/asm/atomic_64.h
diff --git a/arch/x86/include/asm/atomic.h b/arch/x86/include/asm/atomic.h
index 7cf580771c57..f6669024a2f8 100644
--- a/arch/x86/include/asm/atomic.h
+++ b/arch/x86/include/asm/atomic.h
@@ -91,6 +91,7 @@ static __always_inline bool arch_atomic_sub_and_test(int i, atomic_t *v)
  *
  * Atomically increments @v by 1.
  */
+#define arch_atomic_inc arch_atomic_inc
 static __always_inline void arch_atomic_inc(atomic_t *v)
 {
 	asm volatile(LOCK_PREFIX "incl %0"
@@ -103,6 +104,7 @@ static __always_inline void arch_atomic_inc(atomic_t *v)
  *
  * Atomically decrements @v by 1.
  */
+#define arch_atomic_dec arch_atomic_dec
 static __always_inline void arch_atomic_dec(atomic_t *v)
 {
 	asm volatile(LOCK_PREFIX "decl %0"
@@ -173,9 +175,6 @@ static __always_inline int arch_atomic_sub_return(int i, atomic_t *v)
 	return arch_atomic_add_return(-i, v);
 }
 
-#define arch_atomic_inc_return(v)  (arch_atomic_add_return(1, v))
-#define arch_atomic_dec_return(v)  (arch_atomic_sub_return(1, v))
-
 static __always_inline int arch_atomic_fetch_add(int i, atomic_t *v)
 {
 	return xadd(&v->counter, i);
diff --git a/arch/x86/include/asm/atomic64_32.h b/arch/x86/include/asm/atomic64_32.h
index 92212bf0484f..8acbe1e6b902 100644
--- a/arch/x86/include/asm/atomic64_32.h
+++ b/arch/x86/include/asm/atomic64_32.h
@@ -158,6 +158,7 @@ static inline long long arch_atomic64_inc_return(atomic64_t *v)
 			     "S" (v) : "memory", "ecx");
 	return a;
 }
+#define arch_atomic64_inc_return arch_atomic64_inc_return
 
 static inline long long arch_atomic64_dec_return(atomic64_t *v)
 {
@@ -166,6 +167,7 @@ static inline long long arch_atomic64_dec_return(atomic64_t *v)
 			     "S" (v) : "memory", "ecx");
 	return a;
 }
+#define arch_atomic64_dec_return arch_atomic64_dec_return
 
 /**
  * arch_atomic64_add - add integer to atomic64 variable
@@ -217,6 +219,7 @@ static inline int arch_atomic64_sub_and_test(long long i, atomic64_t *v)
  *
  * Atomically increments @v by 1.
  */
+#define arch_atomic64_inc arch_atomic64_inc
 static inline void arch_atomic64_inc(atomic64_t *v)
 {
 	__alternative_atomic64(inc, inc_return, /* no output */,
@@ -229,6 +232,7 @@ static inline void arch_atomic64_inc(atomic64_t *v)
  *
  * Atomically decrements @v by 1.
  */
+#define arch_atomic64_dec arch_atomic64_dec
 static inline void arch_atomic64_dec(atomic64_t *v)
 {
 	__alternative_atomic64(dec, dec_return, /* no output */,
diff --git a/arch/x86/include/asm/atomic64_64.h b/arch/x86/include/asm/atomic64_64.h
index c99f33271b13..a720535b1381 100644
--- a/arch/x86/include/asm/atomic64_64.h
+++ b/arch/x86/include/asm/atomic64_64.h
@@ -82,6 +82,7 @@ static inline bool arch_atomic64_sub_and_test(long i, atomic64_t *v)
  *
  * Atomically increments @v by 1.
  */
+#define arch_atomic64_inc arch_atomic64_inc
 static __always_inline void arch_atomic64_inc(atomic64_t *v)
 {
 	asm volatile(LOCK_PREFIX "incq %0"
@@ -95,6 +96,7 @@ static __always_inline void arch_atomic64_inc(atomic64_t *v)
  *
  * Atomically decrements @v by 1.
  */
+#define arch_atomic64_dec arch_atomic64_dec
 static __always_inline void arch_atomic64_dec(atomic64_t *v)
 {
 	asm volatile(LOCK_PREFIX "decq %0"
@@ -169,9 +171,6 @@ static inline long arch_atomic64_fetch_sub(long i, atomic64_t *v)
 	return xadd(&v->counter, -i);
 }
 
-#define arch_atomic64_inc_return(v)  (arch_atomic64_add_return(1, (v)))
-#define arch_atomic64_dec_return(v)  (arch_atomic64_sub_return(1, (v)))
-
 static inline long arch_atomic64_cmpxchg(atomic64_t *v, long old, long new)
 {
 	return arch_cmpxchg(&v->counter, old, new);
* Unmerged path arch/xtensa/include/asm/atomic.h
diff --git a/include/asm-generic/atomic-instrumented.h b/include/asm-generic/atomic-instrumented.h
index cfee349ddd5a..9790a69084fc 100644
--- a/include/asm-generic/atomic-instrumented.h
+++ b/include/asm-generic/atomic-instrumented.h
@@ -97,29 +97,41 @@ static __always_inline bool atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
 	return arch_atomic64_add_unless(v, a, u);
 }
 
+#ifdef arch_atomic_inc
+#define atomic_inc atomic_inc
 static __always_inline void atomic_inc(atomic_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	arch_atomic_inc(v);
 }
+#endif
 
+#ifdef arch_atomic64_inc
+#define atomic64_inc atomic64_inc
 static __always_inline void atomic64_inc(atomic64_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	arch_atomic64_inc(v);
 }
+#endif
 
+#ifdef arch_atomic_dec
+#define atomic_dec atomic_dec
 static __always_inline void atomic_dec(atomic_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	arch_atomic_dec(v);
 }
+#endif
 
+#ifdef atch_atomic64_dec
+#define atomic64_dec
 static __always_inline void atomic64_dec(atomic64_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	arch_atomic64_dec(v);
 }
+#endif
 
 static __always_inline void atomic_add(int i, atomic_t *v)
 {
@@ -181,29 +193,41 @@ static __always_inline void atomic64_xor(s64 i, atomic64_t *v)
 	arch_atomic64_xor(i, v);
 }
 
+#ifdef arch_atomic_inc_return
+#define atomic_inc_return atomic_inc_return
 static __always_inline int atomic_inc_return(atomic_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	return arch_atomic_inc_return(v);
 }
+#endif
 
+#ifdef arch_atomic64_in_return
+#define atomic64_inc_return atomic64_inc_return
 static __always_inline s64 atomic64_inc_return(atomic64_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	return arch_atomic64_inc_return(v);
 }
+#endif
 
+#ifdef arch_atomic_dec_return
+#define atomic_dec_return atomic_dec_return
 static __always_inline int atomic_dec_return(atomic_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	return arch_atomic_dec_return(v);
 }
+#endif
 
+#ifdef arch_atomic64_dec_return
+#define atomic64_dec_return atomic64_dec_return
 static __always_inline s64 atomic64_dec_return(atomic64_t *v)
 {
 	kasan_check_write(v, sizeof(*v));
 	return arch_atomic64_dec_return(v);
 }
+#endif
 
 static __always_inline bool atomic64_inc_not_zero(atomic64_t *v)
 {
* Unmerged path include/asm-generic/atomic.h
* Unmerged path include/asm-generic/atomic64.h
diff --git a/include/linux/atomic.h b/include/linux/atomic.h
index 6ebab115d8ad..dfffc01b2105 100644
--- a/include/linux/atomic.h
+++ b/include/linux/atomic.h
@@ -97,11 +97,23 @@
 #endif
 #endif /* atomic_add_return_relaxed */
 
+#ifndef atomic_inc
+#define atomic_inc(v)			atomic_add(1, (v))
+#endif
+
 /* atomic_inc_return_relaxed */
 #ifndef atomic_inc_return_relaxed
+
+#ifndef atomic_inc_return
+#define atomic_inc_return(v)		atomic_add_return(1, (v))
+#define atomic_inc_return_relaxed(v)	atomic_add_return_relaxed(1, (v))
+#define atomic_inc_return_acquire(v)	atomic_add_return_acquire(1, (v))
+#define atomic_inc_return_release(v)	atomic_add_return_release(1, (v))
+#else /* atomic_inc_return */
 #define  atomic_inc_return_relaxed	atomic_inc_return
 #define  atomic_inc_return_acquire	atomic_inc_return
 #define  atomic_inc_return_release	atomic_inc_return
+#endif /* atomic_inc_return */
 
 #else /* atomic_inc_return_relaxed */
 
@@ -145,11 +157,23 @@
 #endif
 #endif /* atomic_sub_return_relaxed */
 
+#ifndef atomic_dec
+#define atomic_dec(v)			atomic_sub(1, (v))
+#endif
+
 /* atomic_dec_return_relaxed */
 #ifndef atomic_dec_return_relaxed
+
+#ifndef atomic_dec_return
+#define atomic_dec_return(v)		atomic_sub_return(1, (v))
+#define atomic_dec_return_relaxed(v)	atomic_sub_return_relaxed(1, (v))
+#define atomic_dec_return_acquire(v)	atomic_sub_return_acquire(1, (v))
+#define atomic_dec_return_release(v)	atomic_sub_return_release(1, (v))
+#else /* atomic_dec_return */
 #define  atomic_dec_return_relaxed	atomic_dec_return
 #define  atomic_dec_return_acquire	atomic_dec_return
 #define  atomic_dec_return_release	atomic_dec_return
+#endif /* atomic_dec_return */
 
 #else /* atomic_dec_return_relaxed */
 
@@ -663,11 +687,23 @@ static inline int atomic_dec_if_positive(atomic_t *v)
 #endif
 #endif /* atomic64_add_return_relaxed */
 
+#ifndef atomic64_inc
+#define atomic64_inc(v)			atomic64_add(1, (v))
+#endif
+
 /* atomic64_inc_return_relaxed */
 #ifndef atomic64_inc_return_relaxed
+
+#ifndef atomic64_inc_return
+#define atomic64_inc_return(v)		atomic64_add_return(1, (v))
+#define atomic64_inc_return_relaxed(v)	atomic64_add_return_relaxed(1, (v))
+#define atomic64_inc_return_acquire(v)	atomic64_add_return_acquire(1, (v))
+#define atomic64_inc_return_release(v)	atomic64_add_return_release(1, (v))
+#else /* atomic64_inc_return */
 #define  atomic64_inc_return_relaxed	atomic64_inc_return
 #define  atomic64_inc_return_acquire	atomic64_inc_return
 #define  atomic64_inc_return_release	atomic64_inc_return
+#endif /* atomic64_inc_return */
 
 #else /* atomic64_inc_return_relaxed */
 
@@ -712,11 +748,23 @@ static inline int atomic_dec_if_positive(atomic_t *v)
 #endif
 #endif /* atomic64_sub_return_relaxed */
 
+#ifndef atomic64_dec
+#define atomic64_dec(v)			atomic64_sub(1, (v))
+#endif
+
 /* atomic64_dec_return_relaxed */
 #ifndef atomic64_dec_return_relaxed
+
+#ifndef atomic64_dec_return
+#define atomic64_dec_return(v)		atomic64_sub_return(1, (v))
+#define atomic64_dec_return_relaxed(v)	atomic64_sub_return_relaxed(1, (v))
+#define atomic64_dec_return_acquire(v)	atomic64_sub_return_acquire(1, (v))
+#define atomic64_dec_return_release(v)	atomic64_sub_return_release(1, (v))
+#else /* atomic64_dec_return */
 #define  atomic64_dec_return_relaxed	atomic64_dec_return
 #define  atomic64_dec_return_acquire	atomic64_dec_return
 #define  atomic64_dec_return_release	atomic64_dec_return
+#endif /* atomic64_dec_return */
 
 #else /* atomic64_dec_return_relaxed */
 
