io_uring: run dependent links inline if possible

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit ba816ad61fdf31f59f423a773b00bfa2ed38243a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ba816ad6.failed

Currently any dependent link is executed from a new workqueue context,
which means that we'll be doing a context switch per link in the chain.
If we are running the completion of the current request from our async
workqueue and find that the next request is a link, then run it directly
from the workqueue context instead of forcing another switch.

This improves the performance of linked SQEs, and reduces the CPU
overhead.

	Reviewed-by: Jackie Liu <liuyun01@kylinos.cn>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ba816ad61fdf31f59f423a773b00bfa2ed38243a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index e78c4294f4a5,5db0854fec74..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -690,13 -732,36 +698,36 @@@ static void io_free_req(struct io_kioc
  	__io_free_req(req);
  }
  
- static void io_put_req(struct io_kiocb *req)
+ /*
+  * Drop reference to request, return next in chain (if there is one) if this
+  * was the last reference to this request.
+  */
+ static struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)
  {
+ 	struct io_kiocb *nxt = NULL;
+ 
  	if (refcount_dec_and_test(&req->refs))
- 		io_free_req(req);
+ 		io_free_req(req, &nxt);
+ 
+ 	return nxt;
+ }
+ 
+ static void io_put_req(struct io_kiocb *req, struct io_kiocb **nxtptr)
+ {
+ 	struct io_kiocb *nxt;
+ 
+ 	nxt = io_put_req_find_next(req);
+ 	if (nxt) {
+ 		if (nxtptr) {
+ 			*nxtptr = nxt;
+ 		} else {
+ 			INIT_WORK(&nxt->work, io_sq_wq_submit_work);
+ 			io_queue_async_work(nxt->ctx, nxt);
+ 		}
+ 	}
  }
  
 -static unsigned io_cqring_events(struct io_rings *rings)
 +static unsigned io_cqring_events(struct io_cq_ring *ring)
  {
  	/* See comment at the top of this file */
  	smp_rmb();
@@@ -891,22 -964,21 +922,22 @@@ static int io_iopoll_check(struct io_ri
  	return ret;
  }
  
 -static void kiocb_end_write(struct io_kiocb *req)
 +static void kiocb_end_write(struct kiocb *kiocb)
  {
 -	/*
 -	 * Tell lockdep we inherited freeze protection from submission
 -	 * thread.
 -	 */
 -	if (req->flags & REQ_F_ISREG) {
 -		struct inode *inode = file_inode(req->file);
 +	if (kiocb->ki_flags & IOCB_WRITE) {
 +		struct inode *inode = file_inode(kiocb->ki_filp);
  
 -		__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		/*
 +		 * Tell lockdep we inherited freeze protection from submission
 +		 * thread.
 +		 */
 +		if (S_ISREG(inode->i_mode))
 +			__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);
 +		file_end_write(kiocb->ki_filp);
  	}
 -	file_end_write(req->file);
  }
  
- static void io_complete_rw(struct kiocb *kiocb, long res, long res2)
+ static void io_complete_rw_common(struct kiocb *kiocb, long res)
  {
  	struct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw);
  
@@@ -1837,6 -1949,114 +1895,117 @@@ static int io_poll_add(struct io_kiocb 
  	return ipt.error;
  }
  
++<<<<<<< HEAD
++=======
+ static enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)
+ {
+ 	struct io_ring_ctx *ctx;
+ 	struct io_kiocb *req, *prev;
+ 	unsigned long flags;
+ 
+ 	req = container_of(timer, struct io_kiocb, timeout.timer);
+ 	ctx = req->ctx;
+ 	atomic_inc(&ctx->cq_timeouts);
+ 
+ 	spin_lock_irqsave(&ctx->completion_lock, flags);
+ 	/*
+ 	 * Adjust the reqs sequence before the current one because it
+ 	 * will consume a slot in the cq_ring and the the cq_tail pointer
+ 	 * will be increased, otherwise other timeout reqs may return in
+ 	 * advance without waiting for enough wait_nr.
+ 	 */
+ 	prev = req;
+ 	list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+ 		prev->sequence++;
+ 	list_del(&req->list);
+ 
+ 	io_cqring_fill_event(ctx, req->user_data, -ETIME);
+ 	io_commit_cqring(ctx);
+ 	spin_unlock_irqrestore(&ctx->completion_lock, flags);
+ 
+ 	io_cqring_ev_posted(ctx);
+ 
+ 	io_put_req(req, NULL);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static int io_timeout(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	unsigned count;
+ 	struct io_ring_ctx *ctx = req->ctx;
+ 	struct list_head *entry;
+ 	struct timespec64 ts;
+ 	unsigned span = 0;
+ 
+ 	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ 		return -EINVAL;
+ 	if (sqe->flags || sqe->ioprio || sqe->buf_index || sqe->timeout_flags ||
+ 	    sqe->len != 1)
+ 		return -EINVAL;
+ 
+ 	if (get_timespec64(&ts, u64_to_user_ptr(sqe->addr)))
+ 		return -EFAULT;
+ 
+ 	/*
+ 	 * sqe->off holds how many events that need to occur for this
+ 	 * timeout event to be satisfied.
+ 	 */
+ 	count = READ_ONCE(sqe->off);
+ 	if (!count)
+ 		count = 1;
+ 
+ 	req->sequence = ctx->cached_sq_head + count - 1;
+ 	/* reuse it to store the count */
+ 	req->submit.sequence = count;
+ 	req->flags |= REQ_F_TIMEOUT;
+ 
+ 	/*
+ 	 * Insertion sort, ensuring the first entry in the list is always
+ 	 * the one we need first.
+ 	 */
+ 	spin_lock_irq(&ctx->completion_lock);
+ 	list_for_each_prev(entry, &ctx->timeout_list) {
+ 		struct io_kiocb *nxt = list_entry(entry, struct io_kiocb, list);
+ 		unsigned nxt_sq_head;
+ 		long long tmp, tmp_nxt;
+ 
+ 		/*
+ 		 * Since cached_sq_head + count - 1 can overflow, use type long
+ 		 * long to store it.
+ 		 */
+ 		tmp = (long long)ctx->cached_sq_head + count - 1;
+ 		nxt_sq_head = nxt->sequence - nxt->submit.sequence + 1;
+ 		tmp_nxt = (long long)nxt_sq_head + nxt->submit.sequence - 1;
+ 
+ 		/*
+ 		 * cached_sq_head may overflow, and it will never overflow twice
+ 		 * once there is some timeout req still be valid.
+ 		 */
+ 		if (ctx->cached_sq_head < nxt_sq_head)
+ 			tmp += UINT_MAX;
+ 
+ 		if (tmp > tmp_nxt)
+ 			break;
+ 
+ 		/*
+ 		 * Sequence of reqs after the insert one and itself should
+ 		 * be adjusted because each timeout req consumes a slot.
+ 		 */
+ 		span++;
+ 		nxt->sequence++;
+ 	}
+ 	req->sequence -= span;
+ 	list_add(&req->list, entry);
+ 	spin_unlock_irq(&ctx->completion_lock);
+ 
+ 	hrtimer_init(&req->timeout.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+ 	req->timeout.timer.function = io_timeout_fn;
+ 	hrtimer_start(&req->timeout.timer, timespec64_to_ktime(ts),
+ 			HRTIMER_MODE_REL);
+ 	return 0;
+ }
+ 
++>>>>>>> ba816ad61fdf (io_uring: run dependent links inline if possible)
  static int io_req_defer(struct io_ring_ctx *ctx, struct io_kiocb *req,
  			const struct io_uring_sqe *sqe)
  {
@@@ -1906,14 -2127,17 +2076,14 @@@ static int __io_submit_sqe(struct io_ri
  		ret = io_poll_remove(req, s->sqe);
  		break;
  	case IORING_OP_SYNC_FILE_RANGE:
- 		ret = io_sync_file_range(req, s->sqe, force_nonblock);
+ 		ret = io_sync_file_range(req, s->sqe, nxt, force_nonblock);
  		break;
  	case IORING_OP_SENDMSG:
- 		ret = io_sendmsg(req, s->sqe, force_nonblock);
+ 		ret = io_sendmsg(req, s->sqe, nxt, force_nonblock);
  		break;
  	case IORING_OP_RECVMSG:
- 		ret = io_recvmsg(req, s->sqe, force_nonblock);
+ 		ret = io_recvmsg(req, s->sqe, nxt, force_nonblock);
  		break;
 -	case IORING_OP_TIMEOUT:
 -		ret = io_timeout(req, s->sqe);
 -		break;
  	default:
  		ret = -EINVAL;
  		break;
@@@ -2166,8 -2397,14 +2343,19 @@@ static int __io_queue_sqe(struct io_rin
  {
  	int ret;
  
++<<<<<<< HEAD
 +	ret = __io_submit_sqe(ctx, req, s, force_nonblock);
 +	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
++=======
+ 	ret = __io_submit_sqe(ctx, req, s, NULL, true);
+ 
+ 	/*
+ 	 * We async punt it if the file wasn't marked NOWAIT, or if the file
+ 	 * doesn't support non-blocking read/write attempts
+ 	 */
+ 	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
+ 	    (req->flags & REQ_F_MUST_PUNT))) {
++>>>>>>> ba816ad61fdf (io_uring: run dependent links inline if possible)
  		struct io_uring_sqe *sqe_copy;
  
  		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
* Unmerged path fs/io_uring.c
