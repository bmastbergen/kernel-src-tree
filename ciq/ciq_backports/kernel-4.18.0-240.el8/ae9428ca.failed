io_uring: Merge io_submit_sqes and io_ring_submit

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit ae9428ca61271b6b7f52ebbc359676c9fdfde523
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/ae9428ca.failed

io_submit_sqes() and io_ring_submit() are doing the same stuff with
a little difference. Deduplicate them.

	Reviewed-byï¼šBob Liu <bob.liu@oracle.com>
	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit ae9428ca61271b6b7f52ebbc359676c9fdfde523)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index fca9cdc96d77,375c09a43d32..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2408,8 -2693,9 +2408,14 @@@ static bool io_get_sqring(struct io_rin
  	return false;
  }
  
++<<<<<<< HEAD
 +static int io_submit_sqes(struct io_ring_ctx *ctx, struct sqe_submit *sqes,
 +			  unsigned int nr, bool has_user, bool mm_fault)
++=======
+ static int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr,
+ 			  struct file *ring_file, int ring_fd,
+ 			  struct mm_struct **mm, bool async)
++>>>>>>> ae9428ca6127 (io_uring: Merge io_submit_sqes and io_ring_submit)
  {
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
@@@ -2447,16 -2746,14 +2453,27 @@@
  		}
  
  out:
++<<<<<<< HEAD
 +		if (unlikely(mm_fault)) {
 +			io_cqring_add_event(ctx, sqes[i].sqe->user_data,
 +						-EFAULT);
 +		} else {
 +			sqes[i].has_user = has_user;
 +			sqes[i].needs_lock = true;
 +			sqes[i].needs_fixed_file = true;
 +			io_submit_sqe(ctx, &sqes[i], statep, &link, true);
 +			submitted++;
 +		}
++=======
+ 		s.ring_file = ring_file;
+ 		s.ring_fd = ring_fd;
+ 		s.has_user = *mm != NULL;
+ 		s.in_async = async;
+ 		s.needs_fixed_file = async;
+ 		trace_io_uring_submit_sqe(ctx, s.sqe->user_data, true, async);
+ 		io_submit_sqe(ctx, &s, statep, &link);
+ 		submitted++;
++>>>>>>> ae9428ca6127 (io_uring: Merge io_submit_sqes and io_ring_submit)
  	}
  
  	if (link)
@@@ -2564,34 -2864,12 +2584,40 @@@ static int io_sq_thread(void *data
  			}
  			finish_wait(&ctx->sqo_wait, &wait);
  
 -			ctx->rings->sq_flags &= ~IORING_SQ_NEED_WAKEUP;
 +			ctx->sq_ring->flags &= ~IORING_SQ_NEED_WAKEUP;
 +		}
 +
++<<<<<<< HEAD
 +		i = 0;
 +		all_fixed = true;
 +		do {
 +			if (all_fixed && io_sqe_needs_user(sqes[i].sqe))
 +				all_fixed = false;
 +
 +			i++;
 +			if (i == ARRAY_SIZE(sqes))
 +				break;
 +		} while (io_get_sqring(ctx, &sqes[i]));
 +
 +		/* Unless all new commands are FIXED regions, grab mm */
 +		if (!all_fixed && !cur_mm) {
 +			mm_fault = !mmget_not_zero(ctx->sqo_mm);
 +			if (!mm_fault) {
 +				use_mm(ctx->sqo_mm);
 +				cur_mm = ctx->sqo_mm;
 +			}
  		}
  
 +		inflight += io_submit_sqes(ctx, sqes, i, cur_mm != NULL,
 +						mm_fault);
 +
 +		/* Commit SQ ring head once we've consumed all SQEs */
 +		io_commit_sqring(ctx);
++=======
+ 		to_submit = min(to_submit, ctx->sq_entries);
+ 		inflight += io_submit_sqes(ctx, to_submit, NULL, -1, &cur_mm,
+ 					   true);
++>>>>>>> ae9428ca6127 (io_uring: Merge io_submit_sqes and io_ring_submit)
  	}
  
  	set_fs(old_fs);
@@@ -2605,78 -2883,36 +2631,111 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_ring_submit(struct io_ring_ctx *ctx, unsigned int to_submit,
 +			  bool block_for_last)
 +{
 +	struct io_submit_state state, *statep = NULL;
 +	struct io_kiocb *link = NULL;
 +	struct io_kiocb *shadow_req = NULL;
 +	bool prev_was_link = false;
 +	int i, submit = 0;
 +
 +	if (to_submit > IO_PLUG_THRESHOLD) {
 +		io_submit_state_start(&state, ctx, to_submit);
 +		statep = &state;
 +	}
 +
 +	for (i = 0; i < to_submit; i++) {
 +		bool force_nonblock = true;
 +		struct sqe_submit s;
 +
 +		if (!io_get_sqring(ctx, &s))
 +			break;
 +
 +		/*
 +		 * If previous wasn't linked and we have a linked command,
 +		 * that's the end of the chain. Submit the previous link.
 +		 */
 +		if (!prev_was_link && link) {
 +			io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +						force_nonblock);
 +			link = NULL;
 +			shadow_req = NULL;
 +		}
 +		prev_was_link = (s.sqe->flags & IOSQE_IO_LINK) != 0;
 +
 +		if (link && (s.sqe->flags & IOSQE_IO_DRAIN)) {
 +			if (!shadow_req) {
 +				shadow_req = io_get_req(ctx, NULL);
 +				if (unlikely(!shadow_req))
 +					goto out;
 +				shadow_req->flags |= (REQ_F_IO_DRAIN | REQ_F_SHADOW_DRAIN);
 +				refcount_dec(&shadow_req->refs);
 +			}
 +			shadow_req->sequence = s.sequence;
 +		}
 +
 +out:
 +		s.has_user = true;
 +		s.needs_lock = false;
 +		s.needs_fixed_file = false;
 +		submit++;
 +
 +		/*
 +		 * The caller will block for events after submit, submit the
 +		 * last IO non-blocking. This is either the only IO it's
 +		 * submitting, or it already submitted the previous ones. This
 +		 * improves performance by avoiding an async punt that we don't
 +		 * need to do.
 +		 */
 +		if (block_for_last && submit == to_submit)
 +			force_nonblock = false;
 +
 +		io_submit_sqe(ctx, &s, statep, &link, force_nonblock);
 +	}
 +
 +	if (link)
 +		io_queue_link_head(ctx, link, &link->submit, shadow_req,
 +					!block_for_last);
 +	if (statep)
 +		io_submit_state_end(statep);
 +
 +	io_commit_sqring(ctx);
 +
 +	return submit;
++=======
+ struct io_wait_queue {
+ 	struct wait_queue_entry wq;
+ 	struct io_ring_ctx *ctx;
+ 	unsigned to_wait;
+ 	unsigned nr_timeouts;
+ };
+ 
+ static inline bool io_should_wake(struct io_wait_queue *iowq)
+ {
+ 	struct io_ring_ctx *ctx = iowq->ctx;
+ 
+ 	/*
+ 	 * Wake up if we have enough events, or if a timeout occured since we
+ 	 * started waiting. For timeouts, we always want to return to userspace,
+ 	 * regardless of event count.
+ 	 */
+ 	return io_cqring_events(ctx->rings) >= iowq->to_wait ||
+ 			atomic_read(&ctx->cq_timeouts) != iowq->nr_timeouts;
+ }
+ 
+ static int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,
+ 			    int wake_flags, void *key)
+ {
+ 	struct io_wait_queue *iowq = container_of(curr, struct io_wait_queue,
+ 							wq);
+ 
+ 	if (!io_should_wake(iowq))
+ 		return -1;
+ 
+ 	return autoremove_wake_function(curr, mode, wake_flags, key);
++>>>>>>> ae9428ca6127 (io_uring: Merge io_submit_sqes and io_ring_submit)
  }
  
  /*
@@@ -3631,21 -4003,14 +3690,32 @@@ SYSCALL_DEFINE6(io_uring_enter, unsigne
  			wake_up(&ctx->sqo_wait);
  		submitted = to_submit;
  	} else if (to_submit) {
++<<<<<<< HEAD
 +		bool block_for_last = false;
 +
 +		to_submit = min(to_submit, ctx->sq_entries);
 +
 +		/*
 +		 * Allow last submission to block in a series, IFF the caller
 +		 * asked to wait for events and we don't currently have
 +		 * enough. This potentially avoids an async punt.
 +		 */
 +		if (to_submit == min_complete &&
 +		    io_cqring_events(ctx->rings) < min_complete)
 +			block_for_last = true;
 +
 +		mutex_lock(&ctx->uring_lock);
 +		submitted = io_ring_submit(ctx, to_submit, block_for_last);
++=======
+ 		struct mm_struct *cur_mm;
+ 
+ 		to_submit = min(to_submit, ctx->sq_entries);
+ 		mutex_lock(&ctx->uring_lock);
+ 		/* already have mm, so io_submit_sqes() won't try to grab it */
+ 		cur_mm = ctx->sqo_mm;
+ 		submitted = io_submit_sqes(ctx, to_submit, f.file, fd,
+ 					   &cur_mm, false);
++>>>>>>> ae9428ca6127 (io_uring: Merge io_submit_sqes and io_ring_submit)
  		mutex_unlock(&ctx->uring_lock);
  	}
  	if (flags & IORING_ENTER_GETEVENTS) {
* Unmerged path fs/io_uring.c
