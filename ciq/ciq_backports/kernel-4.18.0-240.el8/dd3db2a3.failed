io_uring: drop file set ref put/get on switch

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit dd3db2a34cff14e152da7c8e320297719a35abf9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/dd3db2a3.failed

Dan reports that he triggered a warning on ring exit doing some testing:

percpu ref (io_file_data_ref_zero) <= 0 (0) after switching to atomic
WARNING: CPU: 3 PID: 0 at lib/percpu-refcount.c:160 percpu_ref_switch_to_atomic_rcu+0xe8/0xf0
Modules linked in:
CPU: 3 PID: 0 Comm: swapper/3 Not tainted 5.6.0-rc3+ #5648
Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1ubuntu1 04/01/2014
RIP: 0010:percpu_ref_switch_to_atomic_rcu+0xe8/0xf0
Code: e7 ff 55 e8 eb d2 80 3d bd 02 d2 00 00 75 8b 48 8b 55 d8 48 c7 c7 e8 70 e6 81 c6 05 a9 02 d2 00 01 48 8b 75 e8 e8 3a d0 c5 ff <0f> 0b e9 69 ff ff ff 90 55 48 89 fd 53 48 89 f3 48 83 ec 28 48 83
RSP: 0018:ffffc90000110ef8 EFLAGS: 00010292
RAX: 0000000000000045 RBX: 7fffffffffffffff RCX: 0000000000000000
RDX: 0000000000000045 RSI: ffffffff825be7a5 RDI: ffffffff825bc32c
RBP: ffff8881b75eac38 R08: 000000042364b941 R09: 0000000000000045
R10: ffffffff825beb40 R11: ffffffff825be78a R12: 0000607e46005aa0
R13: ffff888107dcdd00 R14: 0000000000000000 R15: 0000000000000009
FS:  0000000000000000(0000) GS:ffff8881b9d80000(0000) knlGS:0000000000000000
CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
CR2: 00007f49e6a5ea20 CR3: 00000001b747c004 CR4: 00000000001606e0
DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
Call Trace:
 <IRQ>
 rcu_core+0x1e4/0x4d0
 __do_softirq+0xdb/0x2f1
 irq_exit+0xa0/0xb0
 smp_apic_timer_interrupt+0x60/0x140
 apic_timer_interrupt+0xf/0x20
 </IRQ>
RIP: 0010:default_idle+0x23/0x170
Code: ff eb ab cc cc cc cc 0f 1f 44 00 00 41 54 55 53 65 8b 2d 10 96 92 7e 0f 1f 44 00 00 e9 07 00 00 00 0f 00 2d 21 d0 51 00 fb f4 <65> 8b 2d f6 95 92 7e 0f 1f 44 00 00 5b 5d 41 5c c3 65 8b 05 e5 95

Turns out that this is due to percpu_ref_switch_to_atomic() only
grabbing a reference to the percpu refcount if it's not already in
atomic mode. io_uring drops a ref and re-gets it when switching back to
percpu mode. We attempt to protect against this with the FFD_F_ATOMIC
bit, but that isn't reliable.

We don't actually need to juggle these refcounts between atomic and
percpu switch, we can just do them when we've switched to atomic mode.
This removes the need for FFD_F_ATOMIC, which wasn't reliable.

Fixes: 05f3fb3c5397 ("io_uring: avoid ring quiesce for fixed file set unregister and update")
	Reported-by: Dan Melnic <dmm@fb.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit dd3db2a34cff14e152da7c8e320297719a35abf9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index b70b484c2a41,e412a1761d93..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -197,14 -179,18 +197,25 @@@ struct io_mapped_ubuf 
  	unsigned int	nr_bvecs;
  };
  
 -struct fixed_file_table {
 -	struct file		**files;
 -};
 +struct async_list {
 +	spinlock_t		lock;
 +	atomic_t		cnt;
 +	struct list_head	list;
  
++<<<<<<< HEAD
 +	struct file		*file;
 +	off_t			io_start;
 +	size_t			io_len;
++=======
+ struct fixed_file_data {
+ 	struct fixed_file_table		*table;
+ 	struct io_ring_ctx		*ctx;
+ 
+ 	struct percpu_ref		refs;
+ 	struct llist_head		put_llist;
+ 	struct work_struct		ref_work;
+ 	struct completion		done;
++>>>>>>> dd3db2a34cff (io_uring: drop file set ref put/get on switch)
  };
  
  struct io_ring_ctx {
@@@ -3021,6 -5562,162 +3032,165 @@@ static void io_sqe_file_unregister(stru
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ struct io_file_put {
+ 	struct llist_node llist;
+ 	struct file *file;
+ 	struct completion *done;
+ };
+ 
+ static void io_ring_file_ref_flush(struct fixed_file_data *data)
+ {
+ 	struct io_file_put *pfile, *tmp;
+ 	struct llist_node *node;
+ 
+ 	while ((node = llist_del_all(&data->put_llist)) != NULL) {
+ 		llist_for_each_entry_safe(pfile, tmp, node, llist) {
+ 			io_ring_file_put(data->ctx, pfile->file);
+ 			if (pfile->done)
+ 				complete(pfile->done);
+ 			else
+ 				kfree(pfile);
+ 		}
+ 	}
+ }
+ 
+ static void io_ring_file_ref_switch(struct work_struct *work)
+ {
+ 	struct fixed_file_data *data;
+ 
+ 	data = container_of(work, struct fixed_file_data, ref_work);
+ 	io_ring_file_ref_flush(data);
+ 	percpu_ref_switch_to_percpu(&data->refs);
+ }
+ 
+ static void io_file_data_ref_zero(struct percpu_ref *ref)
+ {
+ 	struct fixed_file_data *data;
+ 
+ 	data = container_of(ref, struct fixed_file_data, refs);
+ 
+ 	/*
+ 	 * We can't safely switch from inside this context, punt to wq. If
+ 	 * the table ref is going away, the table is being unregistered.
+ 	 * Don't queue up the async work for that case, the caller will
+ 	 * handle it.
+ 	 */
+ 	if (!percpu_ref_is_dying(&data->refs))
+ 		queue_work(system_wq, &data->ref_work);
+ }
+ 
+ static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
+ 				 unsigned nr_args)
+ {
+ 	__s32 __user *fds = (__s32 __user *) arg;
+ 	unsigned nr_tables;
+ 	struct file *file;
+ 	int fd, ret = 0;
+ 	unsigned i;
+ 
+ 	if (ctx->file_data)
+ 		return -EBUSY;
+ 	if (!nr_args)
+ 		return -EINVAL;
+ 	if (nr_args > IORING_MAX_FIXED_FILES)
+ 		return -EMFILE;
+ 
+ 	ctx->file_data = kzalloc(sizeof(*ctx->file_data), GFP_KERNEL);
+ 	if (!ctx->file_data)
+ 		return -ENOMEM;
+ 	ctx->file_data->ctx = ctx;
+ 	init_completion(&ctx->file_data->done);
+ 
+ 	nr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);
+ 	ctx->file_data->table = kcalloc(nr_tables,
+ 					sizeof(struct fixed_file_table),
+ 					GFP_KERNEL);
+ 	if (!ctx->file_data->table) {
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	if (percpu_ref_init(&ctx->file_data->refs, io_file_data_ref_zero,
+ 				PERCPU_REF_ALLOW_REINIT, GFP_KERNEL)) {
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 	ctx->file_data->put_llist.first = NULL;
+ 	INIT_WORK(&ctx->file_data->ref_work, io_ring_file_ref_switch);
+ 
+ 	if (io_sqe_alloc_file_tables(ctx, nr_tables, nr_args)) {
+ 		percpu_ref_exit(&ctx->file_data->refs);
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for (i = 0; i < nr_args; i++, ctx->nr_user_files++) {
+ 		struct fixed_file_table *table;
+ 		unsigned index;
+ 
+ 		ret = -EFAULT;
+ 		if (copy_from_user(&fd, &fds[i], sizeof(fd)))
+ 			break;
+ 		/* allow sparse sets */
+ 		if (fd == -1) {
+ 			ret = 0;
+ 			continue;
+ 		}
+ 
+ 		table = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];
+ 		index = i & IORING_FILE_TABLE_MASK;
+ 		file = fget(fd);
+ 
+ 		ret = -EBADF;
+ 		if (!file)
+ 			break;
+ 
+ 		/*
+ 		 * Don't allow io_uring instances to be registered. If UNIX
+ 		 * isn't enabled, then this causes a reference cycle and this
+ 		 * instance can never get freed. If UNIX is enabled we'll
+ 		 * handle it just fine, but there's still no point in allowing
+ 		 * a ring fd as it doesn't support regular read/write anyway.
+ 		 */
+ 		if (file->f_op == &io_uring_fops) {
+ 			fput(file);
+ 			break;
+ 		}
+ 		ret = 0;
+ 		table->files[index] = file;
+ 	}
+ 
+ 	if (ret) {
+ 		for (i = 0; i < ctx->nr_user_files; i++) {
+ 			file = io_file_from_index(ctx, i);
+ 			if (file)
+ 				fput(file);
+ 		}
+ 		for (i = 0; i < nr_tables; i++)
+ 			kfree(ctx->file_data->table[i].files);
+ 
+ 		kfree(ctx->file_data->table);
+ 		kfree(ctx->file_data);
+ 		ctx->file_data = NULL;
+ 		ctx->nr_user_files = 0;
+ 		return ret;
+ 	}
+ 
+ 	ret = io_sqe_files_scm(ctx);
+ 	if (ret)
+ 		io_sqe_files_unregister(ctx);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> dd3db2a34cff (io_uring: drop file set ref put/get on switch)
  static int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,
  				int index)
  {
@@@ -3064,10 -5761,55 +3234,60 @@@
  #endif
  }
  
 -static void io_atomic_switch(struct percpu_ref *ref)
 +static int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,
 +			       unsigned nr_args)
  {
++<<<<<<< HEAD
 +	struct io_uring_files_update up;
++=======
+ 	struct fixed_file_data *data;
+ 
+ 	/*
+ 	 * Juggle reference to ensure we hit zero, if needed, so we can
+ 	 * switch back to percpu mode
+ 	 */
+ 	data = container_of(ref, struct fixed_file_data, refs);
+ 	percpu_ref_put(&data->refs);
+ 	percpu_ref_get(&data->refs);
+ }
+ 
+ static bool io_queue_file_removal(struct fixed_file_data *data,
+ 				  struct file *file)
+ {
+ 	struct io_file_put *pfile, pfile_stack;
+ 	DECLARE_COMPLETION_ONSTACK(done);
+ 
+ 	/*
+ 	 * If we fail allocating the struct we need for doing async reomval
+ 	 * of this file, just punt to sync and wait for it.
+ 	 */
+ 	pfile = kzalloc(sizeof(*pfile), GFP_KERNEL);
+ 	if (!pfile) {
+ 		pfile = &pfile_stack;
+ 		pfile->done = &done;
+ 	}
+ 
+ 	pfile->file = file;
+ 	llist_add(&pfile->llist, &data->put_llist);
+ 
+ 	if (pfile == &pfile_stack) {
+ 		percpu_ref_switch_to_atomic(&data->refs, io_atomic_switch);
+ 		wait_for_completion(&done);
+ 		flush_work(&data->ref_work);
+ 		return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
+ 				 struct io_uring_files_update *up,
+ 				 unsigned nr_args)
+ {
+ 	struct fixed_file_data *data = ctx->file_data;
+ 	bool ref_switch = false;
+ 	struct file *file;
++>>>>>>> dd3db2a34cff (io_uring: drop file set ref put/get on switch)
  	__s32 __user *fds;
  	int fd, i, err;
  	__u32 done;
@@@ -3126,8 -5865,11 +3346,16 @@@
  		}
  		nr_args--;
  		done++;
++<<<<<<< HEAD
 +		up.offset++;
 +	}
++=======
+ 		up->offset++;
+ 	}
+ 
+ 	if (ref_switch)
+ 		percpu_ref_switch_to_atomic(&data->refs, io_atomic_switch);
++>>>>>>> dd3db2a34cff (io_uring: drop file set ref put/get on switch)
  
  	return done ? done : err;
  }
* Unmerged path fs/io_uring.c
