io_uring: clamp to_submit in io_submit_sqes()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 9ef4f124894b7b9241a3cf5f9b40db0812783d66
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/9ef4f124.failed

Make io_submit_sqes() to clamp @to_submit itself. It removes duplicated
code and prepares for following changes.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 9ef4f124894b7b9241a3cf5f9b40db0812783d66)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 28a601d08266,497ed610e8b2..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -2429,9 -4567,20 +2429,25 @@@ static int io_submit_sqes(struct io_rin
  {
  	struct io_submit_state state, *statep = NULL;
  	struct io_kiocb *link = NULL;
 +	struct io_kiocb *shadow_req = NULL;
 +	bool prev_was_link = false;
  	int i, submitted = 0;
++<<<<<<< HEAD
++=======
+ 	bool mm_fault = false;
+ 
+ 	/* if we have a backlog and couldn't flush it all, return BUSY */
+ 	if (test_bit(0, &ctx->sq_check_overflow)) {
+ 		if (!list_empty(&ctx->cq_overflow_list) &&
+ 		    !io_cqring_overflow_flush(ctx, false))
+ 			return -EBUSY;
+ 	}
+ 
+ 	nr = min(nr, ctx->sq_entries);
+ 
+ 	if (!percpu_ref_tryget_many(&ctx->refs, nr))
+ 		return -EAGAIN;
++>>>>>>> 9ef4f124894b (io_uring: clamp to_submit in io_submit_sqes())
  
  	if (nr > IO_PLUG_THRESHOLD) {
  		io_submit_state_start(&state, nr);
@@@ -2580,34 -4750,14 +2596,42 @@@ static int io_sq_thread(void *data
  			}
  			finish_wait(&ctx->sqo_wait, &wait);
  
 -			ctx->rings->sq_flags &= ~IORING_SQ_NEED_WAKEUP;
 +			ctx->sq_ring->flags &= ~IORING_SQ_NEED_WAKEUP;
  		}
  
++<<<<<<< HEAD
 +		i = 0;
 +		all_fixed = true;
 +		do {
 +			if (all_fixed && io_sqe_needs_user(sqes[i].sqe))
 +				all_fixed = false;
 +
 +			i++;
 +			if (i == ARRAY_SIZE(sqes))
 +				break;
 +		} while (io_get_sqring(ctx, &sqes[i]));
 +
 +		/* Unless all new commands are FIXED regions, grab mm */
 +		if (!all_fixed && !cur_mm) {
 +			mm_fault = !mmget_not_zero(ctx->sqo_mm);
 +			if (!mm_fault) {
 +				use_mm(ctx->sqo_mm);
 +				cur_mm = ctx->sqo_mm;
 +			}
 +		}
 +
 +		inflight += io_submit_sqes(ctx, sqes, i, cur_mm != NULL,
 +						mm_fault);
 +
 +		/* Commit SQ ring head once we've consumed all SQEs */
 +		io_commit_sqring(ctx);
++=======
+ 		mutex_lock(&ctx->uring_lock);
+ 		ret = io_submit_sqes(ctx, to_submit, NULL, -1, &cur_mm, true);
+ 		mutex_unlock(&ctx->uring_lock);
+ 		if (ret > 0)
+ 			inflight += ret;
++>>>>>>> 9ef4f124894b (io_uring: clamp to_submit in io_submit_sqes())
  	}
  
  	set_fs(old_fs);
@@@ -3657,19 -6101,11 +3681,22 @@@ SYSCALL_DEFINE6(io_uring_enter, unsigne
  			goto out;
  		}
  
++<<<<<<< HEAD
 +		to_submit = min(to_submit, ctx->sq_entries);
 +
 +		/*
 +		 * Allow last submission to block in a series, IFF the caller
 +		 * asked to wait for events and we don't currently have
 +		 * enough. This potentially avoids an async punt.
 +		 */
 +		if (to_submit == min_complete &&
 +		    io_cqring_events(ctx->rings) < min_complete)
 +			block_for_last = true;
 +
++=======
++>>>>>>> 9ef4f124894b (io_uring: clamp to_submit in io_submit_sqes())
  		mutex_lock(&ctx->uring_lock);
 -		/* already have mm, so io_submit_sqes() won't try to grab it */
 -		cur_mm = ctx->sqo_mm;
 -		submitted = io_submit_sqes(ctx, to_submit, f.file, fd,
 -					   &cur_mm, false);
 +		submitted = io_ring_submit(ctx, to_submit, block_for_last);
  		mutex_unlock(&ctx->uring_lock);
  
  		if (submitted != to_submit)
* Unmerged path fs/io_uring.c
