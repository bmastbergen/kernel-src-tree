dma-mapping: remove arch_dma_mmap_pgprot

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 419e2f1838819e954071dfa1d1f820ab3386ada1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/419e2f18.failed

arch_dma_mmap_pgprot is used for two things:

 1) to override the "normal" uncached page attributes for mapping
    memory coherent to devices that can't snoop the CPU caches
 2) to provide the special DMA_ATTR_WRITE_COMBINE semantics on older
    arm systems and some mips platforms

Replace one with the pgprot_dmacoherent macro that is already provided
by arm and much simpler to use, and lift the DMA_ATTR_WRITE_COMBINE
handling to common code with an explicit arch opt-in.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>	# m68k
	Acked-by: Paul Burton <paul.burton@mips.com>		# mips
(cherry picked from commit 419e2f1838819e954071dfa1d1f820ab3386ada1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/Kconfig
#	arch/arm/mm/dma-mapping.c
#	arch/m68k/Kconfig
#	arch/m68k/kernel/dma.c
#	arch/mips/Kconfig
#	arch/mips/mm/dma-noncoherent.c
#	kernel/dma/Kconfig
diff --cc arch/arm/Kconfig
index c23576a9a9d8,217083caeabd..000000000000
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@@ -2,15 -2,21 +2,20 @@@
  config ARM
  	bool
  	default y
 -	select ARCH_32BIT_OFF_T
  	select ARCH_CLOCKSOURCE_DATA
 -	select ARCH_HAS_BINFMT_FLAT
 +	select ARCH_DISCARD_MEMBLOCK if !HAVE_ARCH_PFN_VALID && !KEXEC
  	select ARCH_HAS_DEBUG_VIRTUAL if MMU
  	select ARCH_HAS_DEVMEM_IS_ALLOWED
++<<<<<<< HEAD
++=======
+ 	select ARCH_HAS_DMA_COHERENT_TO_PFN if SWIOTLB
+ 	select ARCH_HAS_DMA_WRITE_COMBINE if !ARM_DMA_MEM_BUFFERABLE
++>>>>>>> 419e2f183881 (dma-mapping: remove arch_dma_mmap_pgprot)
  	select ARCH_HAS_ELF_RANDOMIZE
  	select ARCH_HAS_FORTIFY_SOURCE
 -	select ARCH_HAS_KEEPINITRD
  	select ARCH_HAS_KCOV
 -	select ARCH_HAS_MEMBARRIER_SYNC_CORE
  	select ARCH_HAS_PTE_SPECIAL if ARM_LPAE
  	select ARCH_HAS_PHYS_TO_DMA
 -	select ARCH_HAS_SETUP_DMA_OPS
  	select ARCH_HAS_SET_MEMORY
  	select ARCH_HAS_STRICT_KERNEL_RWX if MMU && !XIP_KERNEL
  	select ARCH_HAS_STRICT_MODULE_RWX if MMU
diff --cc arch/arm/mm/dma-mapping.c
index ed17026d70f8,d27b12f61737..000000000000
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@@ -2399,4 -2377,42 +2399,43 @@@ void arch_teardown_dma_ops(struct devic
  		return;
  
  	arm_teardown_iommu_dma_ops(dev);
 -	/* Let arch_setup_dma_ops() start again from scratch upon re-probe */
 -	set_dma_ops(dev, NULL);
  }
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_SWIOTLB
+ void arch_sync_dma_for_device(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_page_cpu_to_dev(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),
+ 			      size, dir);
+ }
+ 
+ void arch_sync_dma_for_cpu(struct device *dev, phys_addr_t paddr,
+ 		size_t size, enum dma_data_direction dir)
+ {
+ 	__dma_page_dev_to_cpu(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),
+ 			      size, dir);
+ }
+ 
+ long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
+ 		dma_addr_t dma_addr)
+ {
+ 	return dma_to_pfn(dev, dma_addr);
+ }
+ 
+ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
+ 		gfp_t gfp, unsigned long attrs)
+ {
+ 	return __dma_alloc(dev, size, dma_handle, gfp,
+ 			   __get_dma_pgprot(attrs, PAGE_KERNEL), false,
+ 			   attrs, __builtin_return_address(0));
+ }
+ 
+ void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
+ 		dma_addr_t dma_handle, unsigned long attrs)
+ {
+ 	__arm_dma_free(dev, size, cpu_addr, dma_handle, attrs, false);
+ }
+ #endif /* CONFIG_SWIOTLB */
++>>>>>>> 419e2f183881 (dma-mapping: remove arch_dma_mmap_pgprot)
diff --cc arch/m68k/Kconfig
index afadda00401e,a9e564306d3e..000000000000
--- a/arch/m68k/Kconfig
+++ b/arch/m68k/Kconfig
@@@ -2,8 -2,15 +2,15 @@@
  config M68K
  	bool
  	default y
++<<<<<<< HEAD
++=======
+ 	select ARCH_32BIT_OFF_T
+ 	select ARCH_HAS_BINFMT_FLAT
+ 	select ARCH_HAS_DMA_PREP_COHERENT if HAS_DMA && MMU && !COLDFIRE
+ 	select ARCH_HAS_SYNC_DMA_FOR_DEVICE if HAS_DMA
++>>>>>>> 419e2f183881 (dma-mapping: remove arch_dma_mmap_pgprot)
  	select ARCH_MIGHT_HAVE_PC_PARPORT if ISA
  	select ARCH_NO_COHERENT_DMA_MMAP if !MMU
 -	select ARCH_NO_PREEMPT if !COLDFIRE
 -	select BINFMT_FLAT_ARGVP_ENVP_ON_STACK
 -	select DMA_DIRECT_REMAP if HAS_DMA && MMU && !COLDFIRE
  	select HAVE_IDE
  	select HAVE_AOUT if MMU
  	select HAVE_DEBUG_BUGVERBOSE
diff --cc arch/m68k/kernel/dma.c
index 463572c4943f,35064150e348..000000000000
--- a/arch/m68k/kernel/dma.c
+++ b/arch/m68k/kernel/dma.c
@@@ -18,57 -18,21 +18,61 @@@
  #include <asm/pgalloc.h>
  
  #if defined(CONFIG_MMU) && !defined(CONFIG_COLDFIRE)
 -void arch_dma_prep_coherent(struct page *page, size_t size)
 +
 +static void *m68k_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 +		gfp_t flag, unsigned long attrs)
  {
 -	cache_push(page_to_phys(page), size);
 +	struct page *page, **map;
 +	pgprot_t pgprot;
 +	void *addr;
 +	int i, order;
 +
 +	pr_debug("dma_alloc_coherent: %d,%x\n", size, flag);
 +
 +	size = PAGE_ALIGN(size);
 +	order = get_order(size);
 +
 +	page = alloc_pages(flag, order);
 +	if (!page)
 +		return NULL;
 +
 +	*handle = page_to_phys(page);
 +	map = kmalloc(sizeof(struct page *) << order, flag & ~__GFP_DMA);
 +	if (!map) {
 +		__free_pages(page, order);
 +		return NULL;
 +	}
 +	split_page(page, order);
 +
 +	order = 1 << order;
 +	size >>= PAGE_SHIFT;
 +	map[0] = page;
 +	for (i = 1; i < size; i++)
 +		map[i] = page + i;
 +	for (; i < order; i++)
 +		__free_page(page + i);
 +	pgprot = __pgprot(_PAGE_PRESENT | _PAGE_ACCESSED | _PAGE_DIRTY);
 +	if (CPU_IS_040_OR_060)
 +		pgprot_val(pgprot) |= _PAGE_GLOBAL040 | _PAGE_NOCACHE_S;
 +	else
 +		pgprot_val(pgprot) |= _PAGE_NOCACHE030;
 +	addr = vmap(map, size, VM_MAP, pgprot);
 +	kfree(map);
 +
 +	return addr;
  }
  
++<<<<<<< HEAD
 +static void m68k_dma_free(struct device *dev, size_t size, void *addr,
 +		dma_addr_t handle, unsigned long attrs)
++=======
+ pgprot_t pgprot_dmacoherent(pgprot_t prot)
++>>>>>>> 419e2f183881 (dma-mapping: remove arch_dma_mmap_pgprot)
  {
 -	if (CPU_IS_040_OR_060) {
 -		pgprot_val(prot) &= ~_PAGE_CACHE040;
 -		pgprot_val(prot) |= _PAGE_GLOBAL040 | _PAGE_NOCACHE_S;
 -	} else {
 -		pgprot_val(prot) |= _PAGE_NOCACHE030;
 -	}
 -	return prot;
 +	pr_debug("dma_free_coherent: %p, %x\n", addr, handle);
 +	vfree(addr);
  }
 +
  #else
  
  #include <asm/cacheflush.h>
diff --cc arch/mips/Kconfig
index 29f3ec40278f,fc88f68ea1ee..000000000000
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@@ -1116,7 -1119,12 +1116,13 @@@ config DMA_COHEREN
  
  config DMA_NONCOHERENT
  	bool
++<<<<<<< HEAD
++=======
+ 	select ARCH_HAS_DMA_WRITE_COMBINE
+ 	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
+ 	select ARCH_HAS_UNCACHED_SEGMENT
++>>>>>>> 419e2f183881 (dma-mapping: remove arch_dma_mmap_pgprot)
  	select NEED_DMA_MAP_STATE
 -	select ARCH_HAS_DMA_COHERENT_TO_PFN
 -	select DMA_NONCOHERENT_CACHE_SYNC
  
  config SYS_HAS_EARLY_PRINTK
  	bool
diff --cc kernel/dma/Kconfig
index 26366e0dab12,73c5c2b8e824..000000000000
--- a/kernel/dma/Kconfig
+++ b/kernel/dma/Kconfig
@@@ -19,7 -20,16 +19,20 @@@ config ARCH_HAS_DMA_COHERENCE_
  config ARCH_HAS_DMA_SET_MASK
  	bool
  
++<<<<<<< HEAD
 +config HAVE_GENERIC_DMA_COHERENT
++=======
+ #
+ # Select this option if the architecture needs special handling for
+ # DMA_ATTR_WRITE_COMBINE.  Normally the "uncached" mapping should be what
+ # people thing of when saying write combine, so very few platforms should
+ # need to enable this.
+ #
+ config ARCH_HAS_DMA_WRITE_COMBINE
+ 	bool
+ 
+ config DMA_DECLARE_COHERENT
++>>>>>>> 419e2f183881 (dma-mapping: remove arch_dma_mmap_pgprot)
  	bool
  
  config ARCH_HAS_SETUP_DMA_OPS
* Unmerged path arch/mips/mm/dma-noncoherent.c
* Unmerged path arch/arm/Kconfig
* Unmerged path arch/arm/mm/dma-mapping.c
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index d9a7ce52dbae..ece5f610b03a 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -12,7 +12,6 @@ config ARM64
 	select ARCH_HAS_DEBUG_VIRTUAL
 	select ARCH_HAS_DEVMEM_IS_ALLOWED
 	select ARCH_HAS_DMA_COHERENT_TO_PFN
-	select ARCH_HAS_DMA_MMAP_PGPROT
 	select ARCH_HAS_DMA_PREP_COHERENT
 	select ARCH_HAS_ACPI_TABLE_UPGRADE if ACPI
 	select ARCH_HAS_ELF_RANDOMIZE
diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e3b7d8b34f5c..984f6b27b1bb 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -410,6 +410,10 @@ static inline int pmd_protnone(pmd_t pmd)
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
 #define pgprot_device(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRE) | PTE_PXN | PTE_UXN)
+#define pgprot_dmacoherent(prot) \
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, \
+			PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
+
 #define __HAVE_PHYS_MEM_ACCESS_PROT
 struct file;
 extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c
index fa63a9f40045..dd52107ca0ad 100644
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@ -34,12 +34,6 @@
 
 #include <asm/cacheflush.h>
 
-pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
-		unsigned long attrs)
-{
-	return pgprot_writecombine(prot);
-}
-
 void arch_sync_dma_for_device(struct device *dev, phys_addr_t paddr,
 		size_t size, enum dma_data_direction dir)
 {
* Unmerged path arch/m68k/Kconfig
diff --git a/arch/m68k/include/asm/pgtable_mm.h b/arch/m68k/include/asm/pgtable_mm.h
index fe3ddd73a0cc..fde4534b974f 100644
--- a/arch/m68k/include/asm/pgtable_mm.h
+++ b/arch/m68k/include/asm/pgtable_mm.h
@@ -169,6 +169,9 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 	    ? (__pgprot((pgprot_val(prot) & _CACHEMASK040) | _PAGE_NOCACHE_S))	\
 	    : (prot)))
 
+pgprot_t pgprot_dmacoherent(pgprot_t prot);
+#define pgprot_dmacoherent(prot)	pgprot_dmacoherent(prot)
+
 #endif /* CONFIG_COLDFIRE */
 #include <asm-generic/pgtable.h>
 #endif /* !__ASSEMBLY__ */
* Unmerged path arch/m68k/kernel/dma.c
* Unmerged path arch/mips/Kconfig
* Unmerged path arch/mips/mm/dma-noncoherent.c
diff --git a/include/linux/dma-noncoherent.h b/include/linux/dma-noncoherent.h
index 029424e95b2b..c4cfd08e5bd4 100644
--- a/include/linux/dma-noncoherent.h
+++ b/include/linux/dma-noncoherent.h
@@ -3,6 +3,7 @@
 #define _LINUX_DMA_NONCOHERENT_H 1
 
 #include <linux/dma-mapping.h>
+#include <asm/pgtable.h>
 
 #ifdef CONFIG_ARCH_HAS_DMA_COHERENCE_H
 #include <asm/dma-coherence.h>
@@ -42,10 +43,18 @@ void arch_dma_free(struct device *dev, size_t size, void *cpu_addr,
 		dma_addr_t dma_addr, unsigned long attrs);
 long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
 		dma_addr_t dma_addr);
-pgprot_t arch_dma_mmap_pgprot(struct device *dev, pgprot_t prot,
-		unsigned long attrs);
 
 #ifdef CONFIG_MMU
+/*
+ * Page protection so that devices that can't snoop CPU caches can use the
+ * memory coherently.  We default to pgprot_noncached which is usually used
+ * for ioremap as a safe bet, but architectures can override this with less
+ * strict semantics if possible.
+ */
+#ifndef pgprot_dmacoherent
+#define pgprot_dmacoherent(prot)	pgprot_noncached(prot)
+#endif
+
 pgprot_t dma_pgprot(struct device *dev, pgprot_t prot, unsigned long attrs);
 #else
 static inline pgprot_t dma_pgprot(struct device *dev, pgprot_t prot,
* Unmerged path kernel/dma/Kconfig
diff --git a/kernel/dma/mapping.c b/kernel/dma/mapping.c
index 339979f81f1c..cd726f6f532b 100644
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@ -161,9 +161,11 @@ pgprot_t dma_pgprot(struct device *dev, pgprot_t prot, unsigned long attrs)
 	    (IS_ENABLED(CONFIG_DMA_NONCOHERENT_CACHE_SYNC) &&
              (attrs & DMA_ATTR_NON_CONSISTENT)))
 		return prot;
-	if (IS_ENABLED(CONFIG_ARCH_HAS_DMA_MMAP_PGPROT))
-		return arch_dma_mmap_pgprot(dev, prot, attrs);
-	return pgprot_noncached(prot);
+#ifdef CONFIG_ARCH_HAS_DMA_WRITE_COMBINE
+	if (attrs & DMA_ATTR_WRITE_COMBINE)
+		return pgprot_writecombine(prot);
+#endif
+	return pgprot_dmacoherent(prot);
 }
 #endif /* CONFIG_MMU */
 
