RDMA: Connect between the mmap entry and the umap_priv structure

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Michal Kalderon <michal.kalderon@marvell.com>
commit c043ff2cfb7f6fdd9a1cb1a7ba3800f19b70bf65
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/c043ff2c.failed

The rdma_user_mmap_io interface created a common interface for drivers to
correctly map hw resources and zap them once the ucontext is destroyed
enabling the drivers to safely free the hw resources.

However, this meant the drivers need to delay freeing the resource to the
ucontext destroy phase to ensure they were no longer mapped.  The new
mechanism for a common way of handling user/driver address mapping enabled
notifying the driver if all umap_priv mappings were removed, and enabled
freeing the hw resources when they are done with and not delay it until
ucontext destroy.

Since not all drivers use the mechanism, NULL can be sent to the
rdma_user_mmap_io interface to continue working as before.  Drivers that
use the mmap_xa interface can pass the entry being mapped to the
rdma_user_mmap_io function to be linked together.

Link: https://lore.kernel.org/r/20191030094417.16866-4-michal.kalderon@marvell.com
	Signed-off-by: Ariel Elior <ariel.elior@marvell.com>
	Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit c043ff2cfb7f6fdd9a1cb1a7ba3800f19b70bf65)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/core_priv.h
#	drivers/infiniband/core/ib_core_uverbs.c
#	include/rdma/ib_verbs.h
diff --cc drivers/infiniband/core/core_priv.h
index 9c4d2756c453,6be180eb8cb3..000000000000
--- a/drivers/infiniband/core/core_priv.h
+++ b/drivers/infiniband/core/core_priv.h
@@@ -366,4 -384,18 +366,21 @@@ void ib_port_unregister_module_stat(str
  
  int ib_device_set_netns_put(struct sk_buff *skb,
  			    struct ib_device *dev, u32 ns_fd);
++<<<<<<< HEAD
++=======
+ 
+ int rdma_nl_net_init(struct rdma_dev_net *rnet);
+ void rdma_nl_net_exit(struct rdma_dev_net *rnet);
+ 
+ struct rdma_umap_priv {
+ 	struct vm_area_struct *vma;
+ 	struct list_head list;
+ 	struct rdma_user_mmap_entry *entry;
+ };
+ 
+ void rdma_umap_priv_init(struct rdma_umap_priv *priv,
+ 			 struct vm_area_struct *vma,
+ 			 struct rdma_user_mmap_entry *entry);
+ 
++>>>>>>> c043ff2cfb7f (RDMA: Connect between the mmap entry and the umap_priv structure)
  #endif /* _CORE_PRIV_H */
diff --cc include/rdma/ib_verbs.h
index b2f13568889f,416e72ea80d9..000000000000
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@@ -2770,18 -2826,21 +2770,34 @@@ void  ib_set_client_data(struct ib_devi
  void ib_set_device_ops(struct ib_device *device,
  		       const struct ib_device_ops *ops);
  
- #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
  int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
++<<<<<<< HEAD
 +		      unsigned long pfn, unsigned long size, pgprot_t prot);
 +#else
 +static inline int rdma_user_mmap_io(struct ib_ucontext *ucontext,
 +				    struct vm_area_struct *vma,
 +				    unsigned long pfn, unsigned long size,
 +				    pgprot_t prot)
 +{
 +	return -EINVAL;
 +}
 +#endif
++=======
+ 		      unsigned long pfn, unsigned long size, pgprot_t prot,
+ 		      struct rdma_user_mmap_entry *entry);
+ int rdma_user_mmap_entry_insert(struct ib_ucontext *ucontext,
+ 				struct rdma_user_mmap_entry *entry,
+ 				size_t length);
+ struct rdma_user_mmap_entry *
+ rdma_user_mmap_entry_get_pgoff(struct ib_ucontext *ucontext,
+ 			       unsigned long pgoff);
+ struct rdma_user_mmap_entry *
+ rdma_user_mmap_entry_get(struct ib_ucontext *ucontext,
+ 			 struct vm_area_struct *vma);
+ void rdma_user_mmap_entry_put(struct rdma_user_mmap_entry *entry);
+ 
+ void rdma_user_mmap_entry_remove(struct rdma_user_mmap_entry *entry);
++>>>>>>> c043ff2cfb7f (RDMA: Connect between the mmap entry and the umap_priv structure)
  
  static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t len)
  {
* Unmerged path drivers/infiniband/core/ib_core_uverbs.c
* Unmerged path drivers/infiniband/core/core_priv.h
* Unmerged path drivers/infiniband/core/ib_core_uverbs.c
diff --git a/drivers/infiniband/core/uverbs_main.c b/drivers/infiniband/core/uverbs_main.c
index 8651d5f3e86a..205aa5de1e39 100644
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@ -845,7 +845,7 @@ static void rdma_umap_open(struct vm_area_struct *vma)
 	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 	if (!priv)
 		goto out_unlock;
-	rdma_umap_priv_init(priv, vma);
+	rdma_umap_priv_init(priv, vma, opriv->entry);
 
 	up_read(&ufile->hw_destroy_rwsem);
 	return;
@@ -876,6 +876,9 @@ static void rdma_umap_close(struct vm_area_struct *vma)
 	 * this point.
 	 */
 	mutex_lock(&ufile->umap_lock);
+	if (priv->entry)
+		rdma_user_mmap_entry_put(priv->entry);
+
 	list_del(&priv->list);
 	mutex_unlock(&ufile->umap_lock);
 	kfree(priv);
@@ -1014,6 +1017,11 @@ void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)
 
 			zap_vma_ptes(vma, vma->vm_start,
 				     vma->vm_end - vma->vm_start);
+
+			if (priv->entry) {
+				rdma_user_mmap_entry_put(priv->entry);
+				priv->entry = NULL;
+			}
 		}
 		mutex_unlock(&ufile->umap_lock);
 	skip_mm:
diff --git a/drivers/infiniband/hw/efa/efa_verbs.c b/drivers/infiniband/hw/efa/efa_verbs.c
index f36071a92f97..f957b75239d2 100644
--- a/drivers/infiniband/hw/efa/efa_verbs.c
+++ b/drivers/infiniband/hw/efa/efa_verbs.c
@@ -1587,11 +1587,13 @@ static int __efa_mmap(struct efa_dev *dev, struct efa_ucontext *ucontext,
 	switch (entry->mmap_flag) {
 	case EFA_MMAP_IO_NC:
 		err = rdma_user_mmap_io(&ucontext->ibucontext, vma, pfn, length,
-					pgprot_noncached(vma->vm_page_prot));
+					pgprot_noncached(vma->vm_page_prot),
+					NULL);
 		break;
 	case EFA_MMAP_IO_WC:
 		err = rdma_user_mmap_io(&ucontext->ibucontext, vma, pfn, length,
-					pgprot_writecombine(vma->vm_page_prot));
+					pgprot_writecombine(vma->vm_page_prot),
+					NULL);
 		break;
 	case EFA_MMAP_DMA_PAGE:
 		for (va = vma->vm_start; va < vma->vm_end;
diff --git a/drivers/infiniband/hw/hns/hns_roce_main.c b/drivers/infiniband/hw/hns/hns_roce_main.c
index 78f604a21755..ea0725eaefa0 100644
--- a/drivers/infiniband/hw/hns/hns_roce_main.c
+++ b/drivers/infiniband/hw/hns/hns_roce_main.c
@@ -359,7 +359,8 @@ static int hns_roce_mmap(struct ib_ucontext *context,
 		return rdma_user_mmap_io(context, vma,
 					 to_hr_ucontext(context)->uar.pfn,
 					 PAGE_SIZE,
-					 pgprot_noncached(vma->vm_page_prot));
+					 pgprot_noncached(vma->vm_page_prot),
+					 NULL);
 
 	/* vm_pgoff: 1 -- TPTR */
 	case 1:
@@ -372,7 +373,8 @@ static int hns_roce_mmap(struct ib_ucontext *context,
 		return rdma_user_mmap_io(context, vma,
 					 hr_dev->tptr_dma_addr >> PAGE_SHIFT,
 					 hr_dev->tptr_size,
-					 vma->vm_page_prot);
+					 vma->vm_page_prot,
+					 NULL);
 
 	default:
 		return -EINVAL;
diff --git a/drivers/infiniband/hw/mlx4/main.c b/drivers/infiniband/hw/mlx4/main.c
index 907d99822bf0..211df0287f57 100644
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@ -1146,7 +1146,8 @@ static int mlx4_ib_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)
 		return rdma_user_mmap_io(context, vma,
 					 to_mucontext(context)->uar.pfn,
 					 PAGE_SIZE,
-					 pgprot_noncached(vma->vm_page_prot));
+					 pgprot_noncached(vma->vm_page_prot),
+					 NULL);
 
 	case 1:
 		if (dev->dev->caps.bf_reg_size == 0)
@@ -1155,7 +1156,8 @@ static int mlx4_ib_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)
 			context, vma,
 			to_mucontext(context)->uar.pfn +
 				dev->dev->caps.num_uars,
-			PAGE_SIZE, pgprot_writecombine(vma->vm_page_prot));
+			PAGE_SIZE, pgprot_writecombine(vma->vm_page_prot),
+			NULL);
 
 	case 3: {
 		struct mlx4_clock_params params;
@@ -1171,7 +1173,8 @@ static int mlx4_ib_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)
 					    params.bar) +
 			 params.offset) >>
 				PAGE_SHIFT,
-			PAGE_SIZE, pgprot_noncached(vma->vm_page_prot));
+			PAGE_SIZE, pgprot_noncached(vma->vm_page_prot),
+			NULL);
 	}
 
 	default:
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index 56ae19319ceb..8cf114c140eb 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -2163,7 +2163,7 @@ static int uar_mmap(struct mlx5_ib_dev *dev, enum mlx5_ib_mmap_cmd cmd,
 	mlx5_ib_dbg(dev, "uar idx 0x%lx, pfn %pa\n", idx, &pfn);
 
 	err = rdma_user_mmap_io(&context->ibucontext, vma, pfn, PAGE_SIZE,
-				prot);
+				prot, NULL);
 	if (err) {
 		mlx5_ib_err(dev,
 			    "rdma_user_mmap_io failed with error=%d, mmap_cmd=%s\n",
@@ -2205,7 +2205,8 @@ static int dm_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)
 	      PAGE_SHIFT) +
 	      page_idx;
 	return rdma_user_mmap_io(context, vma, pfn, map_size,
-				 pgprot_writecombine(vma->vm_page_prot));
+				 pgprot_writecombine(vma->vm_page_prot),
+				 NULL);
 }
 
 static int mlx5_ib_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vma)
@@ -2243,7 +2244,8 @@ static int mlx5_ib_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vm
 			PAGE_SHIFT;
 		return rdma_user_mmap_io(&context->ibucontext, vma, pfn,
 					 PAGE_SIZE,
-					 pgprot_noncached(vma->vm_page_prot));
+					 pgprot_noncached(vma->vm_page_prot),
+					 NULL);
 	case MLX5_IB_MMAP_CLOCK_INFO:
 		return mlx5_ib_mmap_clock_info_page(dev, vma, context);
 
* Unmerged path include/rdma/ib_verbs.h
