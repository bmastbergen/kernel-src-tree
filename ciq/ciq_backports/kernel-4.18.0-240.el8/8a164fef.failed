mm: simplify ZONE_DEVICE page private data

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 8a164fef9c4ccf6ff7757170397222860e40d192
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/8a164fef.failed

Remove the clumsy hmm_devmem_page_{get,set}_drvdata helpers, and
instead just access the page directly.  Also make the page data
a void pointer, and thus much easier to use.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 8a164fef9c4ccf6ff7757170397222860e40d192)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
diff --cc include/linux/hmm.h
index 4bbad0472233,3d00e9550e77..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -504,174 -519,69 +504,177 @@@ long hmm_range_dma_unmap(struct hmm_ran
   */
  #define HMM_RANGE_DEFAULT_TIMEOUT 1000
  
 -/* This is a temporary helper to avoid merge conflict between trees. */
 -static inline bool hmm_vma_range_done(struct hmm_range *range)
 +/* Below are for HMM internal use only! Not to be used by device driver! */
 +static inline void hmm_mm_init(struct mm_struct *mm)
  {
 -	bool ret = hmm_range_valid(range);
 -
 -	hmm_range_unregister(range);
 -	return ret;
 +	mm->hmm = NULL;
  }
 +#else /* IS_ENABLED(CONFIG_HMM_MIRROR) */
 +static inline void hmm_mm_init(struct mm_struct *mm) {}
 +#endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
  
 -/* This is a temporary helper to avoid merge conflict between trees. */
 -static inline int hmm_vma_fault(struct hmm_range *range, bool block)
 -{
 -	long ret;
++<<<<<<< HEAD
 +#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)
 +struct hmm_devmem;
  
 +/*
 + * struct hmm_devmem_ops - callback for ZONE_DEVICE memory events
 + *
 + * @free: call when refcount on page reach 1 and thus is no longer use
 + * @fault: call when there is a page fault to unaddressable memory
 + *
 + * Both callback happens from page_free() and page_fault() callback of struct
 + * dev_pagemap respectively. See include/linux/memremap.h for more details on
 + * those.
 + *
 + * The hmm_devmem_ops callback are just here to provide a coherent and
 + * uniq API to device driver and device driver should not register their
 + * own page_free() or page_fault() but rely on the hmm_devmem_ops call-
 + * back.
 + */
 +struct hmm_devmem_ops {
  	/*
 -	 * With the old API the driver must set each individual entries with
 -	 * the requested flags (valid, write, ...). So here we set the mask to
 -	 * keep intact the entries provided by the driver and zero out the
 -	 * default_flags.
 +	 * free() - free a device page
 +	 * @devmem: device memory structure (see struct hmm_devmem)
 +	 * @page: pointer to struct page being freed
 +	 *
 +	 * Call back occurs whenever a device page refcount reach 1 which
 +	 * means that no one is holding any reference on the page anymore
 +	 * (ZONE_DEVICE page have an elevated refcount of 1 as default so
 +	 * that they are not release to the general page allocator).
 +	 *
 +	 * Note that callback has exclusive ownership of the page (as no
 +	 * one is holding any reference).
  	 */
 -	range->default_flags = 0;
 -	range->pfn_flags_mask = -1UL;
 -
 -	ret = hmm_range_register(range, range->vma->vm_mm,
 -				 range->start, range->end,
 -				 PAGE_SHIFT);
 -	if (ret)
 -		return (int)ret;
 -
 -	if (!hmm_range_wait_until_valid(range, HMM_RANGE_DEFAULT_TIMEOUT)) {
 -		/*
 -		 * The mmap_sem was taken by driver we release it here and
 -		 * returns -EAGAIN which correspond to mmap_sem have been
 -		 * drop in the old API.
 -		 */
 -		up_read(&range->vma->vm_mm->mmap_sem);
 -		return -EAGAIN;
 -	}
 -
 -	ret = hmm_range_fault(range, block);
 -	if (ret <= 0) {
 -		if (ret == -EBUSY || !ret) {
 -			/* Same as above  drop mmap_sem to match old API. */
 -			up_read(&range->vma->vm_mm->mmap_sem);
 -			ret = -EBUSY;
 -		} else if (ret == -EAGAIN)
 -			ret = -EBUSY;
 -		hmm_range_unregister(range);
 -		return ret;
 -	}
 -	return 0;
 -}
 +	void (*free)(struct hmm_devmem *devmem, struct page *page);
 +	/*
 +	 * fault() - CPU page fault or get user page (GUP)
 +	 * @devmem: device memory structure (see struct hmm_devmem)
 +	 * @vma: virtual memory area containing the virtual address
 +	 * @addr: virtual address that faulted or for which there is a GUP
 +	 * @page: pointer to struct page backing virtual address (unreliable)
 +	 * @flags: FAULT_FLAG_* (see include/linux/mm.h)
 +	 * @pmdp: page middle directory
 +	 * Return: VM_FAULT_MINOR/MAJOR on success or one of VM_FAULT_ERROR
 +	 *   on error
 +	 *
 +	 * The callback occurs whenever there is a CPU page fault or GUP on a
 +	 * virtual address. This means that the device driver must migrate the
 +	 * page back to regular memory (CPU accessible).
 +	 *
 +	 * The device driver is free to migrate more than one page from the
 +	 * fault() callback as an optimization. However if the device decides
 +	 * to migrate more than one page it must always priotirize the faulting
 +	 * address over the others.
 +	 *
 +	 * The struct page pointer is only given as a hint to allow quick
 +	 * lookup of internal device driver data. A concurrent migration
 +	 * might have already freed that page and the virtual address might
 +	 * no longer be backed by it. So it should not be modified by the
 +	 * callback.
 +	 *
 +	 * Note that mmap semaphore is held in read mode at least when this
 +	 * callback occurs, hence the vma is valid upon callback entry.
 +	 */
 +	vm_fault_t (*fault)(struct hmm_devmem *devmem,
 +			    struct vm_area_struct *vma,
 +			    unsigned long addr,
 +			    const struct page *page,
 +			    unsigned int flags,
 +			    pmd_t *pmdp);
 +};
  
 -/* Below are for HMM internal use only! Not to be used by device driver! */
 -void hmm_mm_destroy(struct mm_struct *mm);
 +/*
 + * struct hmm_devmem - track device memory
 + *
 + * @completion: completion object for device memory
 + * @pfn_first: first pfn for this resource (set by hmm_devmem_add())
 + * @pfn_last: last pfn for this resource (set by hmm_devmem_add())
 + * @resource: IO resource reserved for this chunk of memory
 + * @pagemap: device page map for that chunk
 + * @device: device to bind resource to
 + * @ops: memory operations callback
 + * @ref: per CPU refcount
 + * @page_fault: callback when CPU fault on an unaddressable device page
 + *
 + * This is a helper structure for device drivers that do not wish to implement
 + * the gory details related to hotplugging new memoy and allocating struct
 + * pages.
 + *
 + * Device drivers can directly use ZONE_DEVICE memory on their own if they
 + * wish to do so.
 + *
 + * The page_fault() callback must migrate page back, from device memory to
 + * system memory, so that the CPU can access it. This might fail for various
 + * reasons (device issues,  device have been unplugged, ...). When such error
 + * conditions happen, the page_fault() callback must return VM_FAULT_SIGBUS and
 + * set the CPU page table entry to "poisoned".
 + *
 + * Note that because memory cgroup charges are transferred to the device memory,
 + * this should never fail due to memory restrictions. However, allocation
 + * of a regular system page might still fail because we are out of memory. If
 + * that happens, the page_fault() callback must return VM_FAULT_OOM.
 + *
 + * The page_fault() callback can also try to migrate back multiple pages in one
 + * chunk, as an optimization. It must, however, prioritize the faulting address
 + * over all the others.
 + */
  
 -static inline void hmm_mm_init(struct mm_struct *mm)
 +struct hmm_devmem {
 +	struct completion		completion;
 +	unsigned long			pfn_first;
 +	unsigned long			pfn_last;
 +	struct resource			*resource;
 +	struct device			*device;
 +	struct dev_pagemap		pagemap;
 +	const struct hmm_devmem_ops	*ops;
 +	struct percpu_ref		ref;
 +};
 +
 +/*
 + * To add (hotplug) device memory, HMM assumes that there is no real resource
 + * that reserves a range in the physical address space (this is intended to be
 + * use by unaddressable device memory). It will reserve a physical range big
 + * enough and allocate struct page for it.
 + *
 + * The device driver can wrap the hmm_devmem struct inside a private device
 + * driver struct.
 + */
 +struct hmm_devmem *hmm_devmem_add(const struct hmm_devmem_ops *ops,
 +				  struct device *device,
 +				  unsigned long size);
 +
 +/*
 + * hmm_devmem_page_set_drvdata - set per-page driver data field
 + *
 + * @page: pointer to struct page
 + * @data: driver data value to set
 + *
 + * Because page can not be on lru we have an unsigned long that driver can use
 + * to store a per page field. This just a simple helper to do that.
 + */
 +static inline void hmm_devmem_page_set_drvdata(struct page *page,
 +					       unsigned long data)
  {
 -	mm->hmm = NULL;
 +	page->hmm_data = data;
  }
 -#else /* IS_ENABLED(CONFIG_HMM_MIRROR) */
 +
 +/*
 + * hmm_devmem_page_get_drvdata - get per page driver data field
 + *
 + * @page: pointer to struct page
 + * Return: driver data value
 + */
 +static inline unsigned long hmm_devmem_page_get_drvdata(const struct page *page)
 +{
 +	return page->hmm_data;
 +}
 +#endif /* CONFIG_DEVICE_PRIVATE */
 +#else /* IS_ENABLED(CONFIG_HMM) */
  static inline void hmm_mm_destroy(struct mm_struct *mm) {}
  static inline void hmm_mm_init(struct mm_struct *mm) {}
 -#endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
 +#endif /* IS_ENABLED(CONFIG_HMM) */
  
++=======
++>>>>>>> 8a164fef9c4c (mm: simplify ZONE_DEVICE page private data)
  #endif /* LINUX_HMM_H */
* Unmerged path include/linux/hmm.h
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 6aa761b8d9b8..9aa45703155b 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -157,7 +157,7 @@ struct page {
 		struct {	/* ZONE_DEVICE pages */
 			/** @pgmap: Points to the hosting device page map. */
 			struct dev_pagemap *pgmap;
-			unsigned long hmm_data;
+			void *zone_device_data;
 			unsigned long _zd_pad_1;	/* uses mapping */
 		};
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index da754b26a5b4..edb9a47c2955 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -5711,12 +5711,12 @@ void __ref memmap_init_zone_device(struct zone *zone,
 		__SetPageReserved(page);
 
 		/*
-		 * ZONE_DEVICE pages union ->lru with a ->pgmap back
-		 * pointer and hmm_data.  It is a bug if a ZONE_DEVICE
-		 * page is ever freed or placed on a driver-private list.
+		 * ZONE_DEVICE pages union ->lru with a ->pgmap back pointer
+		 * and zone_device_data.  It is a bug if a ZONE_DEVICE page is
+		 * ever freed or placed on a driver-private list.
 		 */
 		page->pgmap = pgmap;
-		page->hmm_data = 0;
+		page->zone_device_data = NULL;
 
 		/*
 		 * Mark the block movable so that blocks are reserved for
