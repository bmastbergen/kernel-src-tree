bpf: Replace open coded recursion prevention in sys_bpf()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit b6e5dae15a61b0cc9219799926813baad0b58967
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b6e5dae1.failed

The required protection is that the caller cannot be migrated to a
different CPU as these functions end up in places which take either a hash
bucket lock or might trigger a kprobe inside the memory allocator. Both
scenarios can lead to deadlocks. The deadlock prevention is per CPU by
incrementing a per CPU variable which temporarily blocks the invocation of
BPF programs from perf and kprobes.

Replace the open coded preempt_[dis|en]able and __this_cpu_[inc|dec] pairs
with the new helper functions. These functions are already prepared to make
BPF work on PREEMPT_RT enabled kernels. No functional change for !RT
kernels.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200224145644.317843926@linutronix.de
(cherry picked from commit b6e5dae15a61b0cc9219799926813baad0b58967)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/syscall.c
diff --cc kernel/bpf/syscall.c
index b5b79e59cfd4,a79743a89815..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -156,7 -129,146 +156,150 @@@ static struct bpf_map *find_and_alloc_m
  	return map;
  }
  
++<<<<<<< HEAD
 +void *bpf_map_area_alloc(u64 size, int numa_node)
++=======
+ static u32 bpf_map_value_size(struct bpf_map *map)
+ {
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY ||
+ 	    map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)
+ 		return round_up(map->value_size, 8) * num_possible_cpus();
+ 	else if (IS_FD_MAP(map))
+ 		return sizeof(u32);
+ 	else
+ 		return  map->value_size;
+ }
+ 
+ static void maybe_wait_bpf_programs(struct bpf_map *map)
+ {
+ 	/* Wait for any running BPF programs to complete so that
+ 	 * userspace, when we return to it, knows that all programs
+ 	 * that could be running use the new map value.
+ 	 */
+ 	if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS ||
+ 	    map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)
+ 		synchronize_rcu();
+ }
+ 
+ static int bpf_map_update_value(struct bpf_map *map, struct fd f, void *key,
+ 				void *value, __u64 flags)
+ {
+ 	int err;
+ 
+ 	/* Need to create a kthread, thus must support schedule */
+ 	if (bpf_map_is_dev_bound(map)) {
+ 		return bpf_map_offload_update_elem(map, key, value, flags);
+ 	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP ||
+ 		   map->map_type == BPF_MAP_TYPE_SOCKHASH ||
+ 		   map->map_type == BPF_MAP_TYPE_SOCKMAP ||
+ 		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+ 		return map->ops->map_update_elem(map, key, value, flags);
+ 	} else if (IS_FD_PROG_ARRAY(map)) {
+ 		return bpf_fd_array_map_update_elem(map, f.file, key, value,
+ 						    flags);
+ 	}
+ 
+ 	bpf_disable_instrumentation();
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+ 		err = bpf_percpu_hash_update(map, key, value, flags);
+ 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
+ 		err = bpf_percpu_array_update(map, key, value, flags);
+ 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
+ 		err = bpf_percpu_cgroup_storage_update(map, key, value,
+ 						       flags);
+ 	} else if (IS_FD_ARRAY(map)) {
+ 		rcu_read_lock();
+ 		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
+ 						   flags);
+ 		rcu_read_unlock();
+ 	} else if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {
+ 		rcu_read_lock();
+ 		err = bpf_fd_htab_map_update_elem(map, f.file, key, value,
+ 						  flags);
+ 		rcu_read_unlock();
+ 	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
+ 		/* rcu_read_lock() is not needed */
+ 		err = bpf_fd_reuseport_array_update_elem(map, key, value,
+ 							 flags);
+ 	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
+ 		   map->map_type == BPF_MAP_TYPE_STACK) {
+ 		err = map->ops->map_push_elem(map, value, flags);
+ 	} else {
+ 		rcu_read_lock();
+ 		err = map->ops->map_update_elem(map, key, value, flags);
+ 		rcu_read_unlock();
+ 	}
+ 	bpf_enable_instrumentation();
+ 	maybe_wait_bpf_programs(map);
+ 
+ 	return err;
+ }
+ 
+ static int bpf_map_copy_value(struct bpf_map *map, void *key, void *value,
+ 			      __u64 flags)
+ {
+ 	void *ptr;
+ 	int err;
+ 
+ 	if (bpf_map_is_dev_bound(map))
+ 		return bpf_map_offload_lookup_elem(map, key, value);
+ 
+ 	bpf_disable_instrumentation();
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+ 		err = bpf_percpu_hash_copy(map, key, value);
+ 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
+ 		err = bpf_percpu_array_copy(map, key, value);
+ 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
+ 		err = bpf_percpu_cgroup_storage_copy(map, key, value);
+ 	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
+ 		err = bpf_stackmap_copy(map, key, value);
+ 	} else if (IS_FD_ARRAY(map) || IS_FD_PROG_ARRAY(map)) {
+ 		err = bpf_fd_array_map_lookup_elem(map, key, value);
+ 	} else if (IS_FD_HASH(map)) {
+ 		err = bpf_fd_htab_map_lookup_elem(map, key, value);
+ 	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
+ 		err = bpf_fd_reuseport_array_lookup_elem(map, key, value);
+ 	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
+ 		   map->map_type == BPF_MAP_TYPE_STACK) {
+ 		err = map->ops->map_peek_elem(map, value);
+ 	} else if (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+ 		/* struct_ops map requires directly updating "value" */
+ 		err = bpf_struct_ops_map_sys_lookup_elem(map, key, value);
+ 	} else {
+ 		rcu_read_lock();
+ 		if (map->ops->map_lookup_elem_sys_only)
+ 			ptr = map->ops->map_lookup_elem_sys_only(map, key);
+ 		else
+ 			ptr = map->ops->map_lookup_elem(map, key);
+ 		if (IS_ERR(ptr)) {
+ 			err = PTR_ERR(ptr);
+ 		} else if (!ptr) {
+ 			err = -ENOENT;
+ 		} else {
+ 			err = 0;
+ 			if (flags & BPF_F_LOCK)
+ 				/* lock 'ptr' and copy everything but lock */
+ 				copy_map_value_locked(map, value, ptr, true);
+ 			else
+ 				copy_map_value(map, value, ptr);
+ 			/* mask lock, since value wasn't zero inited */
+ 			check_and_init_map_lock(map, value);
+ 		}
+ 		rcu_read_unlock();
+ 	}
+ 
+ 	bpf_enable_instrumentation();
+ 	maybe_wait_bpf_programs(map);
+ 
+ 	return err;
+ }
+ 
+ static void *__bpf_map_area_alloc(u64 size, int numa_node, bool mmapable)
++>>>>>>> b6e5dae15a61 (bpf: Replace open coded recursion prevention in sys_bpf())
  {
  	/* We really just want to fail instead of triggering OOM killer
  	 * under memory pressure, therefore we set __GFP_NORETRY to kmalloc,
@@@ -1040,10 -1122,14 +1183,9 @@@ static int map_delete_elem(union bpf_at
  	if (bpf_map_is_dev_bound(map)) {
  		err = bpf_map_offload_delete_elem(map, key);
  		goto out;
 -	} else if (IS_FD_PROG_ARRAY(map) ||
 -		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
 -		/* These maps require sleepable context */
 -		err = map->ops->map_delete_elem(map, key);
 -		goto out;
  	}
  
- 	preempt_disable();
- 	__this_cpu_inc(bpf_prog_active);
+ 	bpf_disable_instrumentation();
  	rcu_read_lock();
  	err = map->ops->map_delete_elem(map, key);
  	rcu_read_unlock();
@@@ -1124,6 -1209,218 +1265,221 @@@ err_put
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ int generic_map_delete_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 cp, max_count;
+ 	int err = 0;
+ 	void *key;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!key)
+ 		return -ENOMEM;
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		err = -EFAULT;
+ 		if (copy_from_user(key, keys + cp * map->key_size,
+ 				   map->key_size))
+ 			break;
+ 
+ 		if (bpf_map_is_dev_bound(map)) {
+ 			err = bpf_map_offload_delete_elem(map, key);
+ 			break;
+ 		}
+ 
+ 		bpf_disable_instrumentation();
+ 		rcu_read_lock();
+ 		err = map->ops->map_delete_elem(map, key);
+ 		rcu_read_unlock();
+ 		bpf_enable_instrumentation();
+ 		maybe_wait_bpf_programs(map);
+ 		if (err)
+ 			break;
+ 	}
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 
+ 	kfree(key);
+ 	return err;
+ }
+ 
+ int generic_map_update_batch(struct bpf_map *map,
+ 			     const union bpf_attr *attr,
+ 			     union bpf_attr __user *uattr)
+ {
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	u32 value_size, cp, max_count;
+ 	int ufd = attr->map_fd;
+ 	void *key, *value;
+ 	struct fd f;
+ 	int err = 0;
+ 
+ 	f = fdget(ufd);
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map)) {
+ 		return -EINVAL;
+ 	}
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!key)
+ 		return -ENOMEM;
+ 
+ 	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!value) {
+ 		kfree(key);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for (cp = 0; cp < max_count; cp++) {
+ 		err = -EFAULT;
+ 		if (copy_from_user(key, keys + cp * map->key_size,
+ 		    map->key_size) ||
+ 		    copy_from_user(value, values + cp * value_size, value_size))
+ 			break;
+ 
+ 		err = bpf_map_update_value(map, f, key, value,
+ 					   attr->batch.elem_flags);
+ 
+ 		if (err)
+ 			break;
+ 	}
+ 
+ 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+ 		err = -EFAULT;
+ 
+ 	kfree(value);
+ 	kfree(key);
+ 	return err;
+ }
+ 
+ #define MAP_LOOKUP_RETRIES 3
+ 
+ int generic_map_lookup_batch(struct bpf_map *map,
+ 				    const union bpf_attr *attr,
+ 				    union bpf_attr __user *uattr)
+ {
+ 	void __user *uobatch = u64_to_user_ptr(attr->batch.out_batch);
+ 	void __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);
+ 	void __user *values = u64_to_user_ptr(attr->batch.values);
+ 	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+ 	void *buf, *buf_prevkey, *prev_key, *key, *value;
+ 	int err, retry = MAP_LOOKUP_RETRIES;
+ 	u32 value_size, cp, max_count;
+ 
+ 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+ 		return -EINVAL;
+ 
+ 	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+ 	    !map_value_has_spin_lock(map))
+ 		return -EINVAL;
+ 
+ 	value_size = bpf_map_value_size(map);
+ 
+ 	max_count = attr->batch.count;
+ 	if (!max_count)
+ 		return 0;
+ 
+ 	if (put_user(0, &uattr->batch.count))
+ 		return -EFAULT;
+ 
+ 	buf_prevkey = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf_prevkey)
+ 		return -ENOMEM;
+ 
+ 	buf = kmalloc(map->key_size + value_size, GFP_USER | __GFP_NOWARN);
+ 	if (!buf) {
+ 		kvfree(buf_prevkey);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	err = -EFAULT;
+ 	prev_key = NULL;
+ 	if (ubatch && copy_from_user(buf_prevkey, ubatch, map->key_size))
+ 		goto free_buf;
+ 	key = buf;
+ 	value = key + map->key_size;
+ 	if (ubatch)
+ 		prev_key = buf_prevkey;
+ 
+ 	for (cp = 0; cp < max_count;) {
+ 		rcu_read_lock();
+ 		err = map->ops->map_get_next_key(map, prev_key, key);
+ 		rcu_read_unlock();
+ 		if (err)
+ 			break;
+ 		err = bpf_map_copy_value(map, key, value,
+ 					 attr->batch.elem_flags);
+ 
+ 		if (err == -ENOENT) {
+ 			if (retry) {
+ 				retry--;
+ 				continue;
+ 			}
+ 			err = -EINTR;
+ 			break;
+ 		}
+ 
+ 		if (err)
+ 			goto free_buf;
+ 
+ 		if (copy_to_user(keys + cp * map->key_size, key,
+ 				 map->key_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 		if (copy_to_user(values + cp * value_size, value, value_size)) {
+ 			err = -EFAULT;
+ 			goto free_buf;
+ 		}
+ 
+ 		if (!prev_key)
+ 			prev_key = buf_prevkey;
+ 
+ 		swap(prev_key, key);
+ 		retry = MAP_LOOKUP_RETRIES;
+ 		cp++;
+ 	}
+ 
+ 	if (err == -EFAULT)
+ 		goto free_buf;
+ 
+ 	if ((copy_to_user(&uattr->batch.count, &cp, sizeof(cp)) ||
+ 		    (cp && copy_to_user(uobatch, prev_key, map->key_size))))
+ 		err = -EFAULT;
+ 
+ free_buf:
+ 	kfree(buf_prevkey);
+ 	kfree(buf);
+ 	return err;
+ }
+ 
++>>>>>>> b6e5dae15a61 (bpf: Replace open coded recursion prevention in sys_bpf())
  #define BPF_MAP_LOOKUP_AND_DELETE_ELEM_LAST_FIELD value
  
  static int map_lookup_and_delete_elem(union bpf_attr *attr)
* Unmerged path kernel/bpf/syscall.c
