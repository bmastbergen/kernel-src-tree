iommu/amd: Convert AMD iommu driver to the dma-iommu api

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Tom Murphy <murphyt7@tcd.ie>
commit be62dbf554c5b50718a54a359372c148cd9975c7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/be62dbf5.failed

Convert the AMD iommu driver to the dma-iommu api. Remove the iova
handling and reserve region code from the AMD iommu driver.

	Signed-off-by: Tom Murphy <murphyt7@tcd.ie>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit be62dbf554c5b50718a54a359372c148cd9975c7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index 1745092fddfa,1881fe8bdfed..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -2414,456 -2300,6 +2314,459 @@@ static void update_domain(struct protec
  	domain_flush_tlb_pde(domain);
  }
  
++<<<<<<< HEAD
 +static int dir2prot(enum dma_data_direction direction)
 +{
 +	if (direction == DMA_TO_DEVICE)
 +		return IOMMU_PROT_IR;
 +	else if (direction == DMA_FROM_DEVICE)
 +		return IOMMU_PROT_IW;
 +	else if (direction == DMA_BIDIRECTIONAL)
 +		return IOMMU_PROT_IW | IOMMU_PROT_IR;
 +	else
 +		return 0;
 +}
 +
 +/*
 + * This function contains common code for mapping of a physically
 + * contiguous memory region into DMA address space. It is used by all
 + * mapping functions provided with this IOMMU driver.
 + * Must be called with the domain lock held.
 + */
 +static dma_addr_t __map_single(struct device *dev,
 +			       struct dma_ops_domain *dma_dom,
 +			       phys_addr_t paddr,
 +			       size_t size,
 +			       enum dma_data_direction direction,
 +			       u64 dma_mask)
 +{
 +	dma_addr_t offset = paddr & ~PAGE_MASK;
 +	dma_addr_t address, start, ret;
 +	unsigned long flags;
 +	unsigned int pages;
 +	int prot = 0;
 +	int i;
 +
 +	pages = iommu_num_pages(paddr, size, PAGE_SIZE);
 +	paddr &= PAGE_MASK;
 +
 +	address = dma_ops_alloc_iova(dev, dma_dom, pages, dma_mask);
 +	if (!address)
 +		goto out;
 +
 +	prot = dir2prot(direction);
 +
 +	start = address;
 +	for (i = 0; i < pages; ++i) {
 +		ret = iommu_map_page(&dma_dom->domain, start, paddr,
 +				     PAGE_SIZE, prot, GFP_ATOMIC);
 +		if (ret)
 +			goto out_unmap;
 +
 +		paddr += PAGE_SIZE;
 +		start += PAGE_SIZE;
 +	}
 +	address += offset;
 +
 +	domain_flush_np_cache(&dma_dom->domain, address, size);
 +
 +out:
 +	return address;
 +
 +out_unmap:
 +
 +	for (--i; i >= 0; --i) {
 +		start -= PAGE_SIZE;
 +		iommu_unmap_page(&dma_dom->domain, start, PAGE_SIZE);
 +	}
 +
 +	spin_lock_irqsave(&dma_dom->domain.lock, flags);
 +	domain_flush_tlb(&dma_dom->domain);
 +	domain_flush_complete(&dma_dom->domain);
 +	spin_unlock_irqrestore(&dma_dom->domain.lock, flags);
 +
 +	dma_ops_free_iova(dma_dom, address, pages);
 +
 +	return DMA_MAPPING_ERROR;
 +}
 +
 +/*
 + * Does the reverse of the __map_single function. Must be called with
 + * the domain lock held too
 + */
 +static void __unmap_single(struct dma_ops_domain *dma_dom,
 +			   dma_addr_t dma_addr,
 +			   size_t size,
 +			   int dir)
 +{
 +	dma_addr_t i, start;
 +	unsigned int pages;
 +
 +	pages = iommu_num_pages(dma_addr, size, PAGE_SIZE);
 +	dma_addr &= PAGE_MASK;
 +	start = dma_addr;
 +
 +	for (i = 0; i < pages; ++i) {
 +		iommu_unmap_page(&dma_dom->domain, start, PAGE_SIZE);
 +		start += PAGE_SIZE;
 +	}
 +
 +	if (amd_iommu_unmap_flush) {
 +		unsigned long flags;
 +
 +		spin_lock_irqsave(&dma_dom->domain.lock, flags);
 +		domain_flush_tlb(&dma_dom->domain);
 +		domain_flush_complete(&dma_dom->domain);
 +		spin_unlock_irqrestore(&dma_dom->domain.lock, flags);
 +		dma_ops_free_iova(dma_dom, dma_addr, pages);
 +	} else {
 +		pages = __roundup_pow_of_two(pages);
 +		queue_iova(&dma_dom->iovad, dma_addr >> PAGE_SHIFT, pages, 0);
 +	}
 +}
 +
 +/*
 + * The exported map_single function for dma_ops.
 + */
 +static dma_addr_t map_page(struct device *dev, struct page *page,
 +			   unsigned long offset, size_t size,
 +			   enum dma_data_direction dir,
 +			   unsigned long attrs)
 +{
 +	phys_addr_t paddr = page_to_phys(page) + offset;
 +	struct protection_domain *domain;
 +	struct dma_ops_domain *dma_dom;
 +	u64 dma_mask;
 +
 +	domain = get_domain(dev);
 +	if (PTR_ERR(domain) == -EINVAL)
 +		return (dma_addr_t)paddr;
 +	else if (IS_ERR(domain))
 +		return DMA_MAPPING_ERROR;
 +
 +	dma_mask = *dev->dma_mask;
 +	dma_dom = to_dma_ops_domain(domain);
 +
 +	return __map_single(dev, dma_dom, paddr, size, dir, dma_mask);
 +}
 +
 +/*
 + * The exported unmap_single function for dma_ops.
 + */
 +static void unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		       enum dma_data_direction dir, unsigned long attrs)
 +{
 +	struct protection_domain *domain;
 +	struct dma_ops_domain *dma_dom;
 +
 +	domain = get_domain(dev);
 +	if (IS_ERR(domain))
 +		return;
 +
 +	dma_dom = to_dma_ops_domain(domain);
 +
 +	__unmap_single(dma_dom, dma_addr, size, dir);
 +}
 +
 +static int sg_num_pages(struct device *dev,
 +			struct scatterlist *sglist,
 +			int nelems)
 +{
 +	unsigned long mask, boundary_size;
 +	struct scatterlist *s;
 +	int i, npages = 0;
 +
 +	mask          = dma_get_seg_boundary(dev);
 +	boundary_size = mask + 1 ? ALIGN(mask + 1, PAGE_SIZE) >> PAGE_SHIFT :
 +				   1UL << (BITS_PER_LONG - PAGE_SHIFT);
 +
 +	for_each_sg(sglist, s, nelems, i) {
 +		int p, n;
 +
 +		s->dma_address = npages << PAGE_SHIFT;
 +		p = npages % boundary_size;
 +		n = iommu_num_pages(sg_phys(s), s->length, PAGE_SIZE);
 +		if (p + n > boundary_size)
 +			npages += boundary_size - p;
 +		npages += n;
 +	}
 +
 +	return npages;
 +}
 +
 +/*
 + * The exported map_sg function for dma_ops (handles scatter-gather
 + * lists).
 + */
 +static int map_sg(struct device *dev, struct scatterlist *sglist,
 +		  int nelems, enum dma_data_direction direction,
 +		  unsigned long attrs)
 +{
 +	int mapped_pages = 0, npages = 0, prot = 0, i;
 +	struct protection_domain *domain;
 +	struct dma_ops_domain *dma_dom;
 +	struct scatterlist *s;
 +	unsigned long address;
 +	u64 dma_mask;
 +	int ret;
 +
 +	domain = get_domain(dev);
 +	if (IS_ERR(domain))
 +		return 0;
 +
 +	dma_dom  = to_dma_ops_domain(domain);
 +	dma_mask = *dev->dma_mask;
 +
 +	npages = sg_num_pages(dev, sglist, nelems);
 +
 +	address = dma_ops_alloc_iova(dev, dma_dom, npages, dma_mask);
 +	if (!address)
 +		goto out_err;
 +
 +	prot = dir2prot(direction);
 +
 +	/* Map all sg entries */
 +	for_each_sg(sglist, s, nelems, i) {
 +		int j, pages = iommu_num_pages(sg_phys(s), s->length, PAGE_SIZE);
 +
 +		for (j = 0; j < pages; ++j) {
 +			unsigned long bus_addr, phys_addr;
 +
 +			bus_addr  = address + s->dma_address + (j << PAGE_SHIFT);
 +			phys_addr = (sg_phys(s) & PAGE_MASK) + (j << PAGE_SHIFT);
 +			ret = iommu_map_page(domain, bus_addr, phys_addr,
 +					     PAGE_SIZE, prot,
 +					     GFP_ATOMIC | __GFP_NOWARN);
 +			if (ret)
 +				goto out_unmap;
 +
 +			mapped_pages += 1;
 +		}
 +	}
 +
 +	/* Everything is mapped - write the right values into s->dma_address */
 +	for_each_sg(sglist, s, nelems, i) {
 +		/*
 +		 * Add in the remaining piece of the scatter-gather offset that
 +		 * was masked out when we were determining the physical address
 +		 * via (sg_phys(s) & PAGE_MASK) earlier.
 +		 */
 +		s->dma_address += address + (s->offset & ~PAGE_MASK);
 +		s->dma_length   = s->length;
 +	}
 +
 +	if (s)
 +		domain_flush_np_cache(domain, s->dma_address, s->dma_length);
 +
 +	return nelems;
 +
 +out_unmap:
 +	dev_err(dev, "IOMMU mapping error in map_sg (io-pages: %d reason: %d)\n",
 +		npages, ret);
 +
 +	for_each_sg(sglist, s, nelems, i) {
 +		int j, pages = iommu_num_pages(sg_phys(s), s->length, PAGE_SIZE);
 +
 +		for (j = 0; j < pages; ++j) {
 +			unsigned long bus_addr;
 +
 +			bus_addr  = address + s->dma_address + (j << PAGE_SHIFT);
 +			iommu_unmap_page(domain, bus_addr, PAGE_SIZE);
 +
 +			if (--mapped_pages == 0)
 +				goto out_free_iova;
 +		}
 +	}
 +
 +out_free_iova:
 +	free_iova_fast(&dma_dom->iovad, address >> PAGE_SHIFT, npages);
 +
 +out_err:
 +	return 0;
 +}
 +
 +/*
 + * The exported map_sg function for dma_ops (handles scatter-gather
 + * lists).
 + */
 +static void unmap_sg(struct device *dev, struct scatterlist *sglist,
 +		     int nelems, enum dma_data_direction dir,
 +		     unsigned long attrs)
 +{
 +	struct protection_domain *domain;
 +	struct dma_ops_domain *dma_dom;
 +	unsigned long startaddr;
 +	int npages;
 +
 +	domain = get_domain(dev);
 +	if (IS_ERR(domain))
 +		return;
 +
 +	startaddr = sg_dma_address(sglist) & PAGE_MASK;
 +	dma_dom   = to_dma_ops_domain(domain);
 +	npages    = sg_num_pages(dev, sglist, nelems);
 +
 +	__unmap_single(dma_dom, startaddr, npages << PAGE_SHIFT, dir);
 +}
 +
 +/*
 + * The exported alloc_coherent function for dma_ops.
 + */
 +static void *alloc_coherent(struct device *dev, size_t size,
 +			    dma_addr_t *dma_addr, gfp_t flag,
 +			    unsigned long attrs)
 +{
 +	u64 dma_mask = dev->coherent_dma_mask;
 +	struct protection_domain *domain;
 +	struct dma_ops_domain *dma_dom;
 +	struct page *page;
 +
 +	domain = get_domain(dev);
 +	if (PTR_ERR(domain) == -EINVAL) {
 +		page = alloc_pages(flag, get_order(size));
 +		*dma_addr = page_to_phys(page);
 +		return page_address(page);
 +	} else if (IS_ERR(domain))
 +		return NULL;
 +
 +	dma_dom   = to_dma_ops_domain(domain);
 +	size	  = PAGE_ALIGN(size);
 +	dma_mask  = dev->coherent_dma_mask;
 +	flag     &= ~(__GFP_DMA | __GFP_HIGHMEM | __GFP_DMA32);
 +	flag     |= __GFP_ZERO;
 +
 +	page = alloc_pages(flag | __GFP_NOWARN,  get_order(size));
 +	if (!page) {
 +		if (!gfpflags_allow_blocking(flag))
 +			return NULL;
 +
 +		page = dma_alloc_from_contiguous(dev, size >> PAGE_SHIFT,
 +					get_order(size), flag & __GFP_NOWARN);
 +		if (!page)
 +			return NULL;
 +	}
 +
 +	if (!dma_mask)
 +		dma_mask = *dev->dma_mask;
 +
 +	*dma_addr = __map_single(dev, dma_dom, page_to_phys(page),
 +				 size, DMA_BIDIRECTIONAL, dma_mask);
 +
 +	if (*dma_addr == DMA_MAPPING_ERROR)
 +		goto out_free;
 +
 +	return page_address(page);
 +
 +out_free:
 +
 +	if (!dma_release_from_contiguous(dev, page, size >> PAGE_SHIFT))
 +		__free_pages(page, get_order(size));
 +
 +	return NULL;
 +}
 +
 +/*
 + * The exported free_coherent function for dma_ops.
 + */
 +static void free_coherent(struct device *dev, size_t size,
 +			  void *virt_addr, dma_addr_t dma_addr,
 +			  unsigned long attrs)
 +{
 +	struct protection_domain *domain;
 +	struct dma_ops_domain *dma_dom;
 +	struct page *page;
 +
 +	page = virt_to_page(virt_addr);
 +	size = PAGE_ALIGN(size);
 +
 +	domain = get_domain(dev);
 +	if (IS_ERR(domain))
 +		goto free_mem;
 +
 +	dma_dom = to_dma_ops_domain(domain);
 +
 +	__unmap_single(dma_dom, dma_addr, size, DMA_BIDIRECTIONAL);
 +
 +free_mem:
 +	if (!dma_release_from_contiguous(dev, page, size >> PAGE_SHIFT))
 +		__free_pages(page, get_order(size));
 +}
 +
 +/*
 + * This function is called by the DMA layer to find out if we can handle a
 + * particular device. It is part of the dma_ops.
 + */
 +static int amd_iommu_dma_supported(struct device *dev, u64 mask)
 +{
 +	if (!dma_direct_supported(dev, mask))
 +		return 0;
 +	return check_device(dev);
 +}
 +
 +static const struct dma_map_ops amd_iommu_dma_ops = {
 +	.alloc		= alloc_coherent,
 +	.free		= free_coherent,
 +	.map_page	= map_page,
 +	.unmap_page	= unmap_page,
 +	.map_sg		= map_sg,
 +	.unmap_sg	= unmap_sg,
 +	.dma_supported	= amd_iommu_dma_supported,
 +};
 +
 +static int init_reserved_iova_ranges(void)
 +{
 +	struct pci_dev *pdev = NULL;
 +	struct iova *val;
 +
 +	init_iova_domain(&reserved_iova_ranges, PAGE_SIZE, IOVA_START_PFN);
 +
 +	lockdep_set_class(&reserved_iova_ranges.iova_rbtree_lock,
 +			  &reserved_rbtree_key);
 +
 +	/* MSI memory range */
 +	val = reserve_iova(&reserved_iova_ranges,
 +			   IOVA_PFN(MSI_RANGE_START), IOVA_PFN(MSI_RANGE_END));
 +	if (!val) {
 +		pr_err("Reserving MSI range failed\n");
 +		return -ENOMEM;
 +	}
 +
 +	/* HT memory range */
 +	val = reserve_iova(&reserved_iova_ranges,
 +			   IOVA_PFN(HT_RANGE_START), IOVA_PFN(HT_RANGE_END));
 +	if (!val) {
 +		pr_err("Reserving HT range failed\n");
 +		return -ENOMEM;
 +	}
 +
 +	/*
 +	 * Memory used for PCI resources
 +	 * FIXME: Check whether we can reserve the PCI-hole completly
 +	 */
 +	for_each_pci_dev(pdev) {
 +		int i;
 +
 +		for (i = 0; i < PCI_NUM_RESOURCES; ++i) {
 +			struct resource *r = &pdev->resource[i];
 +
 +			if (!(r->flags & IORESOURCE_MEM))
 +				continue;
 +
 +			val = reserve_iova(&reserved_iova_ranges,
 +					   IOVA_PFN(r->start),
 +					   IOVA_PFN(r->end));
 +			if (!val) {
 +				pci_err(pdev, "Reserve pci-resource range %pR failed\n", r);
 +				return -ENOMEM;
 +			}
 +		}
 +	}
 +
 +	return 0;
 +}
 +
++=======
++>>>>>>> be62dbf554c5 (iommu/amd: Convert AMD iommu driver to the dma-iommu api)
  int __init amd_iommu_init_api(void)
  {
  	int ret, err = 0;
diff --git a/drivers/iommu/Kconfig b/drivers/iommu/Kconfig
index 100ea2118b88..434bec77f49d 100644
--- a/drivers/iommu/Kconfig
+++ b/drivers/iommu/Kconfig
@@ -137,6 +137,7 @@ config AMD_IOMMU
 	select PCI_PASID
 	select IOMMU_API
 	select IOMMU_IOVA
+	select IOMMU_DMA
 	depends on X86_64 && PCI && ACPI
 	---help---
 	  With this option you can enable support for AMD IOMMU hardware in
* Unmerged path drivers/iommu/amd_iommu.c
