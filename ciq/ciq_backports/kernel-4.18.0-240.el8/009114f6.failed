drm/amd/display: Added locking for atomic update stream and update planes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Anthony Koo <Anthony.Koo@amd.com>
commit 009114f6df84150a567b05537aa29b9d660e419b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/009114f6.failed

[Why]
Screen flickering when HDR switches between FP16 and ARGB2101010

[How]
Moved pipe_control_lock so stream update and plane update occur atomically

	Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
	Signed-off-by: Lucy Li <lucy.li@amd.com>
	Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
	Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
	Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
(cherry picked from commit 009114f6df84150a567b05537aa29b9d660e419b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/amd/display/dc/core/dc.c
#	drivers/gpu/drm/amd/display/dc/dce110/dce110_hw_sequencer.c
#	drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.c
#	drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.h
#	drivers/gpu/drm/amd/display/dc/dcn10/dcn10_init.c
#	drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.c
#	drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.h
#	drivers/gpu/drm/amd/display/dc/dcn20/dcn20_init.c
#	drivers/gpu/drm/amd/display/dc/dcn21/dcn21_init.c
#	drivers/gpu/drm/amd/display/dc/inc/hw_sequencer.h
diff --cc drivers/gpu/drm/amd/display/dc/core/dc.c
index d8c0d01bb191,2208282ea6cb..000000000000
--- a/drivers/gpu/drm/amd/display/dc/core/dc.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc.c
@@@ -719,8 -760,30 +719,31 @@@ static bool disable_all_writeback_pipes
  
  	return true;
  }
 +#endif
  
+ void apply_ctx_interdependent_lock(struct dc *dc, struct dc_state *context, struct dc_stream_state *stream, bool lock)
+ {
+ 	int i = 0;
+ 
+ 	/* Checks if interdependent update function pointer is NULL or not, takes care of DCE110 case */
+ 	if (dc->hwss.interdependent_update_lock)
+ 		dc->hwss.interdependent_update_lock(dc, context, lock);
+ 	else {
+ 		for (i = 0; i < dc->res_pool->pipe_count; i++) {
+ 			struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i];
+ 			struct pipe_ctx *old_pipe_ctx = &dc->current_state->res_ctx.pipe_ctx[i];
+ 
+ 			// Copied conditions that were previously in dce110_apply_ctx_for_surface
+ 			if (stream == pipe_ctx->stream) {
+ 				if (!pipe_ctx->top_pipe &&
+ 					(pipe_ctx->plane_state || old_pipe_ctx->plane_state))
+ 					dc->hwss.pipe_control_lock(dc, pipe_ctx, lock);
+ 				break;
+ 			}
+ 		}
+ 	}
+ }
+ 
  static void disable_dangling_plane(struct dc *dc, struct dc_state *context)
  {
  	int i, j;
@@@ -745,10 -808,20 +768,26 @@@
  		}
  		if (should_disable && old_stream) {
  			dc_rem_all_planes_for_stream(dc, old_stream, dangling_context);
 +#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
  			disable_all_writeback_pipes_for_stream(dc, old_stream, dangling_context);
++<<<<<<< HEAD
 +#endif
 +			dc->hwss.apply_ctx_for_surface(dc, old_stream, 0, dangling_context);
++=======
+ 
+ 			if (dc->hwss.apply_ctx_for_surface) {
+ 				apply_ctx_interdependent_lock(dc, dc->current_state, old_stream, true);
+ 				dc->hwss.apply_ctx_for_surface(dc, old_stream, 0, dangling_context);
+ 				apply_ctx_interdependent_lock(dc, dc->current_state, old_stream, false);
+ 				dc->hwss.post_unlock_program_front_end(dc, dangling_context);
+ 			}
+ 			if (dc->hwss.program_front_end_for_ctx) {
+ 				dc->hwss.interdependent_update_lock(dc, dc->current_state, true);
+ 				dc->hwss.program_front_end_for_ctx(dc, dangling_context);
+ 				dc->hwss.interdependent_update_lock(dc, dc->current_state, false);
+ 				dc->hwss.post_unlock_program_front_end(dc, dangling_context);
+ 			}
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  		}
  	}
  
@@@ -1050,14 -1242,18 +1089,29 @@@ static enum dc_status dc_commit_state_n
  	/* re-program planes for existing stream, in case we need to
  	 * free up plane resource for later use
  	 */
++<<<<<<< HEAD
 +	for (i = 0; i < context->stream_count; i++) {
 +		if (context->streams[i]->mode_changed)
 +			continue;
 +
 +		dc->hwss.apply_ctx_for_surface(
 +			dc, context->streams[i],
 +			context->stream_status[i].plane_count,
 +			context); /* use new pipe config in new context */
++=======
+ 	if (dc->hwss.apply_ctx_for_surface) {
+ 		for (i = 0; i < context->stream_count; i++) {
+ 			if (context->streams[i]->mode_changed)
+ 				continue;
+ 			apply_ctx_interdependent_lock(dc, context, context->streams[i], true);
+ 			dc->hwss.apply_ctx_for_surface(
+ 				dc, context->streams[i],
+ 				context->stream_status[i].plane_count,
+ 				context); /* use new pipe config in new context */
+ 			apply_ctx_interdependent_lock(dc, context, context->streams[i], false);
+ 			dc->hwss.post_unlock_program_front_end(dc, context);
+ 		}
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  	}
  
  	/* Program hardware */
@@@ -1077,16 -1273,27 +1131,37 @@@
  	}
  
  	/* Program all planes within new context*/
++<<<<<<< HEAD
++=======
+ 	if (dc->hwss.program_front_end_for_ctx) {
+ 		dc->hwss.interdependent_update_lock(dc, context, true);
+ 		dc->hwss.program_front_end_for_ctx(dc, context);
+ 		dc->hwss.interdependent_update_lock(dc, context, false);
+ 		dc->hwss.post_unlock_program_front_end(dc, context);
+ 	}
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  	for (i = 0; i < context->stream_count; i++) {
  		const struct dc_link *link = context->streams[i]->link;
  
  		if (!context->streams[i]->mode_changed)
  			continue;
  
++<<<<<<< HEAD
 +		dc->hwss.apply_ctx_for_surface(
 +				dc, context->streams[i],
 +				context->stream_status[i].plane_count,
 +				context);
++=======
+ 		if (dc->hwss.apply_ctx_for_surface) {
+ 			apply_ctx_interdependent_lock(dc, context, context->streams[i], true);
+ 			dc->hwss.apply_ctx_for_surface(
+ 					dc, context->streams[i],
+ 					context->stream_status[i].plane_count,
+ 					context);
+ 			apply_ctx_interdependent_lock(dc, context, context->streams[i], false);
+ 			dc->hwss.post_unlock_program_front_end(dc, context);
+ 		}
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  
  		/*
  		 * enable stereo
@@@ -1830,30 -2130,31 +1905,44 @@@ static void commit_planes_do_stream_upd
  					odm_pipe->stream_res.opp->funcs->opp_program_fmt(odm_pipe->stream_res.opp,
  							&stream->bit_depth_params,
  							&stream->clamping);
 -					odm_pipe = odm_pipe->next_odm_pipe;
 -				}
 +#endif
  			}
  
++<<<<<<< HEAD
 +#if defined(CONFIG_DRM_AMD_DC_DSC_SUPPORT)
 +			if (stream_update->dsc_config && dc->hwss.pipe_control_lock_global) {
 +				dc->hwss.pipe_control_lock_global(dc, pipe_ctx, true);
 +				dp_update_dsc_config(pipe_ctx);
 +				dc->hwss.pipe_control_lock_global(dc, pipe_ctx, false);
 +			}
 +#endif
 +			/* Full fe update*/
 +			if (update_type == UPDATE_TYPE_FAST)
 +				continue;
 +
 +			if (stream_update->dpms_off) {
 +				dc->hwss.pipe_control_lock(dc, pipe_ctx, true);
++=======
+ 			/* Full fe update*/
+ 			if (update_type == UPDATE_TYPE_FAST)
+ 				continue;
+ 
+ 			if (stream_update->dsc_config)
+ 				dp_update_dsc_config(pipe_ctx);
+ 
+ 			if (stream_update->dpms_off) {
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  				if (*stream_update->dpms_off) {
 -					core_link_disable_stream(pipe_ctx);
 -					/* for dpms, keep acquired resources*/
 -					if (pipe_ctx->stream_res.audio && !dc->debug.az_endpoint_mute_only)
 -						pipe_ctx->stream_res.audio->funcs->az_disable(pipe_ctx->stream_res.audio);
 -
 +					core_link_disable_stream(pipe_ctx, KEEP_ACQUIRED_RESOURCE);
  					dc->hwss.optimize_bandwidth(dc, dc->current_state);
  				} else {
 -					if (dc->optimize_seamless_boot_streams == 0)
 -						dc->hwss.prepare_bandwidth(dc, dc->current_state);
 -
 +					dc->hwss.prepare_bandwidth(dc, dc->current_state);
  					core_link_enable_stream(dc->current_state, pipe_ctx);
  				}
++<<<<<<< HEAD
 +				dc->hwss.pipe_control_lock(dc, pipe_ctx, false);
++=======
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  			}
  
  			if (stream_update->abm_level && pipe_ctx->stream_res.abm) {
@@@ -1909,7 -2240,17 +2019,21 @@@ static void commit_planes_for_stream(st
  		 * In case of turning off screen, no need to program front end a second time.
  		 * just return after program blank.
  		 */
++<<<<<<< HEAD
 +		dc->hwss.apply_ctx_for_surface(dc, stream, 0, context);
++=======
+ 		if (dc->hwss.apply_ctx_for_surface)
+ 			dc->hwss.apply_ctx_for_surface(dc, stream, 0, context);
+ 		if (dc->hwss.program_front_end_for_ctx)
+ 			dc->hwss.program_front_end_for_ctx(dc, context);
+ 
+ 		if ((update_type != UPDATE_TYPE_FAST) && dc->hwss.interdependent_update_lock)
+ 			dc->hwss.interdependent_update_lock(dc, context, false);
+ 		else
+ 			dc->hwss.pipe_control_lock(dc, top_pipe_to_program, false);
+ 
+ 		dc->hwss.post_unlock_program_front_end(dc, context);
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  		return;
  	}
  
@@@ -1975,13 -2312,24 +2097,16 @@@
  
  	// Update Type FAST, Surface updates
  	if (update_type == UPDATE_TYPE_FAST) {
++<<<<<<< HEAD
 +		/* Lock the top pipe while updating plane addrs, since freesync requires
 +		 *  plane addr update event triggers to be synchronized.
 +		 *  top_pipe_to_program is expected to never be NULL
 +		 */
 +		dc->hwss.pipe_control_lock(dc, top_pipe_to_program, true);
 +
 +#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
++=======
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  		if (dc->hwss.set_flip_control_gsl)
  			for (i = 0; i < surface_count; i++) {
  				struct dc_plane_state *plane_state = srf_updates[i].surface;
@@@ -2026,10 -2371,16 +2151,19 @@@
  					dc->hwss.update_plane_addr(dc, pipe_ctx);
  			}
  		}
+ 	}
  
++<<<<<<< HEAD
++=======
+ 	if ((update_type != UPDATE_TYPE_FAST) && dc->hwss.interdependent_update_lock)
+ 		dc->hwss.interdependent_update_lock(dc, context, false);
+ 	else
  		dc->hwss.pipe_control_lock(dc, top_pipe_to_program, false);
- 	}
  
+ 	if (update_type != UPDATE_TYPE_FAST)
+ 		dc->hwss.post_unlock_program_front_end(dc, context);
+ 
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  	// Fire manual trigger only when bottom plane is flipped
  	for (j = 0; j < dc->res_pool->pipe_count; j++) {
  		struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j];
diff --cc drivers/gpu/drm/amd/display/dc/dce110/dce110_hw_sequencer.c
index fafb4b470140,56d4ec7bdad7..000000000000
--- a/drivers/gpu/drm/amd/display/dc/dce110/dce110_hw_sequencer.c
+++ b/drivers/gpu/drm/amd/display/dc/dce110/dce110_hw_sequencer.c
@@@ -2645,8 -2600,15 +2624,17 @@@ static void dce110_apply_ctx_for_surfac
  		enable_fbc(dc, context);
  }
  
++<<<<<<< HEAD
++=======
+ static void dce110_post_unlock_program_front_end(
+ 		struct dc *dc,
+ 		struct dc_state *context)
+ {
+ }
+ 
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  static void dce110_power_down_fe(struct dc *dc, struct pipe_ctx *pipe_ctx)
  {
 -	struct dce_hwseq *hws = dc->hwseq;
  	int fe_idx = pipe_ctx->plane_res.mi ?
  		pipe_ctx->plane_res.mi->inst : pipe_ctx->pipe_idx;
  
@@@ -2761,10 -2720,9 +2749,11 @@@ static const struct hw_sequencer_funcs 
  	.blank_stream = dce110_blank_stream,
  	.enable_audio_stream = dce110_enable_audio_stream,
  	.disable_audio_stream = dce110_disable_audio_stream,
 +	.enable_display_pipe_clock_gating = enable_display_pipe_clock_gating,
 +	.enable_display_power_gating = dce110_enable_display_power_gating,
  	.disable_plane = dce110_power_down_fe,
  	.pipe_control_lock = dce_pipe_control_lock,
+ 	.interdependent_update_lock = NULL,
  	.prepare_bandwidth = dce110_prepare_bandwidth,
  	.optimize_bandwidth = dce110_optimize_bandwidth,
  	.set_drr = set_drr,
diff --cc drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.c
index 2118ea21d7e9,7fc559acffcd..000000000000
--- a/drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.c
+++ b/drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.c
@@@ -81,6 -82,34 +81,37 @@@ void print_microsec(struct dc_context *
  			us_x10 % frac);
  }
  
++<<<<<<< HEAD
++=======
+ void dcn10_lock_all_pipes(struct dc *dc,
+ 	struct dc_state *context,
+ 	bool lock)
+ {
+ 	struct pipe_ctx *pipe_ctx;
+ 	struct timing_generator *tg;
+ 	int i;
+ 
+ 	for (i = 0; i < dc->res_pool->pipe_count; i++) {
+ 		pipe_ctx = &context->res_ctx.pipe_ctx[i];
+ 		tg = pipe_ctx->stream_res.tg;
+ 
+ 		/*
+ 		 * Only lock the top pipe's tg to prevent redundant
+ 		 * (un)locking. Also skip if pipe is disabled.
+ 		 */
+ 		if (pipe_ctx->top_pipe ||
+ 		    !pipe_ctx->stream || !pipe_ctx->plane_state ||
+ 		    !tg->funcs->is_tg_enabled(tg))
+ 			continue;
+ 
+ 		if (lock)
+ 			dc->hwss.pipe_control_lock(dc, pipe_ctx, true);
+ 		else
+ 			dc->hwss.pipe_control_lock(dc, pipe_ctx, false);
+ 	}
+ }
+ 
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  static void log_mpc_crc(struct dc *dc,
  	struct dc_log_buffer_ctx *log_ctx)
  {
@@@ -2419,14 -2528,9 +2450,17 @@@ static void dcn10_apply_ctx_for_surface
  
  	underflow_check_delay_us = dc->debug.underflow_assert_delay_us;
  
 -	if (underflow_check_delay_us != 0xFFFFFFFF && hws->funcs.did_underflow_occur)
 -		ASSERT(hws->funcs.did_underflow_occur(dc, top_pipe_to_program));
 +	if (underflow_check_delay_us != 0xFFFFFFFF && dc->hwss.did_underflow_occur)
 +		ASSERT(dc->hwss.did_underflow_occur(dc, top_pipe_to_program));
  
++<<<<<<< HEAD
 +	if (interdependent_update)
 +		lock_all_pipes(dc, context, true);
 +	else
 +		dcn10_pipe_control_lock(dc, top_pipe_to_program, true);
 +
++=======
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  	if (underflow_check_delay_us != 0xFFFFFFFF)
  		udelay(underflow_check_delay_us);
  
@@@ -2443,18 -2547,8 +2477,23 @@@
  		struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i];
  		struct pipe_ctx *old_pipe_ctx =
  				&dc->current_state->res_ctx.pipe_ctx[i];
++<<<<<<< HEAD
 +		/*
 +		 * Powergate reused pipes that are not powergated
 +		 * fairly hacky right now, using opp_id as indicator
 +		 * TODO: After move dc_post to dc_update, this will
 +		 * be removed.
 +		 */
 +		if (pipe_ctx->plane_state && !old_pipe_ctx->plane_state) {
 +			if (old_pipe_ctx->stream_res.tg == tg &&
 +			    old_pipe_ctx->plane_res.hubp &&
 +			    old_pipe_ctx->plane_res.hubp->opp_id != OPP_ID_INVALID)
 +				dcn10_disable_plane(dc, old_pipe_ctx);
 +		}
++=======
+ 
+ 		pipe_ctx->update_flags.raw = 0;
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  
  		if ((!pipe_ctx->plane_state ||
  		     pipe_ctx->stream_res.tg != old_pipe_ctx->stream_res.tg) &&
@@@ -2490,21 -2582,41 +2529,25 @@@
  				&pipe_ctx->dlg_regs,
  				&pipe_ctx->ttu_regs);
  		}
 -}
++<<<<<<< HEAD
  
 -void dcn10_post_unlock_program_front_end(
 -		struct dc *dc,
 -		struct dc_state *context)
 -{
 -	int i, j;
 -
 -	DC_LOGGER_INIT(dc->ctx->logger);
 -
 -	for (i = 0; i < dc->res_pool->pipe_count; i++) {
 -		struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i];
 -
 -		if (!pipe_ctx->top_pipe &&
 -			!pipe_ctx->prev_odm_pipe &&
 -			pipe_ctx->stream) {
 -			struct dc_stream_status *stream_status = NULL;
 -			struct timing_generator *tg = pipe_ctx->stream_res.tg;
 +	if (interdependent_update)
 +		lock_all_pipes(dc, context, false);
 +	else
 +		dcn10_pipe_control_lock(dc, top_pipe_to_program, false);
++=======
++}
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  
 -			for (j = 0; j < context->stream_count; j++) {
 -				if (pipe_ctx->stream == context->streams[j])
 -					stream_status = &context->stream_status[j];
 -			}
 -
 -			if (context->stream_status[i].plane_count == 0)
 -				false_optc_underflow_wa(dc, pipe_ctx->stream, tg);
 -		}
 -	}
 +	if (num_planes == 0)
 +		false_optc_underflow_wa(dc, stream, tg);
  
  	for (i = 0; i < dc->res_pool->pipe_count; i++)
 -		if (context->res_ctx.pipe_ctx[i].update_flags.bits.disable)
 -			dc->hwss.disable_plane(dc, &dc->current_state->res_ctx.pipe_ctx[i]);
 +		if (removed_pipe[i])
 +			dcn10_disable_plane(dc, &dc->current_state->res_ctx.pipe_ctx[i]);
  
  	for (i = 0; i < dc->res_pool->pipe_count; i++)
 -		if (context->res_ctx.pipe_ctx[i].update_flags.bits.disable) {
 +		if (removed_pipe[i]) {
  			dc->hwss.optimize_bandwidth(dc, context);
  			break;
  		}
diff --cc drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.h
index d3616b1948cc,16a50e05ffbf..000000000000
--- a/drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.h
+++ b/drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.h
@@@ -31,16 -32,70 +31,82 @@@
  struct dc;
  
  void dcn10_hw_sequencer_construct(struct dc *dc);
 +extern void fill_display_configs(
 +	const struct dc_state *context,
 +	struct dm_pp_display_configuration *pp_display_cfg);
  
 +bool is_rgb_cspace(enum dc_color_space output_color_space);
 +
 +void hwss1_plane_atomic_disconnect(struct dc *dc, struct pipe_ctx *pipe_ctx);
 +
 +void dcn10_verify_allow_pstate_change_high(struct dc *dc);
 +
++<<<<<<< HEAD
++=======
+ int dcn10_get_vupdate_offset_from_vsync(struct pipe_ctx *pipe_ctx);
+ void dcn10_setup_vupdate_interrupt(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ enum dc_status dcn10_enable_stream_timing(
+ 		struct pipe_ctx *pipe_ctx,
+ 		struct dc_state *context,
+ 		struct dc *dc);
+ void dcn10_optimize_bandwidth(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ void dcn10_prepare_bandwidth(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ void dcn10_pipe_control_lock(
+ 	struct dc *dc,
+ 	struct pipe_ctx *pipe,
+ 	bool lock);
+ void dcn10_blank_pixel_data(
+ 		struct dc *dc,
+ 		struct pipe_ctx *pipe_ctx,
+ 		bool blank);
+ void dcn10_unblank_stream(struct pipe_ctx *pipe_ctx,
+ 		struct dc_link_settings *link_settings);
+ void dcn10_program_output_csc(struct dc *dc,
+ 		struct pipe_ctx *pipe_ctx,
+ 		enum dc_color_space colorspace,
+ 		uint16_t *matrix,
+ 		int opp_id);
+ bool dcn10_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx,
+ 				const struct dc_stream_state *stream);
+ bool dcn10_set_input_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx,
+ 			const struct dc_plane_state *plane_state);
+ void dcn10_update_plane_addr(const struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn10_update_mpcc(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn10_reset_hw_ctx_wrap(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ void dcn10_disable_plane(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn10_lock_all_pipes(
+ 		struct dc *dc,
+ 		struct dc_state *context,
+ 		bool lock);
+ void dcn10_apply_ctx_for_surface(
+ 		struct dc *dc,
+ 		const struct dc_stream_state *stream,
+ 		int num_planes,
+ 		struct dc_state *context);
+ void dcn10_post_unlock_program_front_end(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ void dcn10_hubp_pg_control(
+ 		struct dce_hwseq *hws,
+ 		unsigned int hubp_inst,
+ 		bool power_on);
+ void dcn10_dpp_pg_control(
+ 		struct dce_hwseq *hws,
+ 		unsigned int dpp_inst,
+ 		bool power_on);
+ void dcn10_enable_power_gating_plane(
+ 	struct dce_hwseq *hws,
+ 	bool enable);
+ void dcn10_plane_atomic_disable(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn10_disable_vga(
+ 	struct dce_hwseq *hws);
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  void dcn10_program_pipe(
  		struct dc *dc,
  		struct pipe_ctx *pipe_ctx,
diff --cc drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.c
index fbbf9758dc66,129a91e8f250..000000000000
--- a/drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.c
+++ b/drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.c
@@@ -1172,90 -1088,6 +1172,93 @@@ void dcn20_enable_plane
  //	}
  }
  
++<<<<<<< HEAD
 +
 +static void dcn20_program_pipe(
 +		struct dc *dc,
 +		struct pipe_ctx *pipe_ctx,
 +		struct dc_state *context)
 +{
 +	pipe_ctx->plane_state->update_flags.bits.full_update =
 +			context->commit_hints.full_update_needed ? 1 : pipe_ctx->plane_state->update_flags.bits.full_update;
 +
 +	if (pipe_ctx->plane_state->update_flags.bits.full_update)
 +		dcn20_enable_plane(dc, pipe_ctx, context);
 +
 +	update_dchubp_dpp(dc, pipe_ctx, context);
 +
 +	set_hdr_multiplier(pipe_ctx);
 +
 +	if (pipe_ctx->plane_state->update_flags.bits.full_update ||
 +			pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change ||
 +			pipe_ctx->plane_state->update_flags.bits.gamma_change)
 +		dc->hwss.set_input_transfer_func(pipe_ctx, pipe_ctx->plane_state);
 +
 +	/* dcn10_translate_regamma_to_hw_format takes 750us to finish
 +	 * only do gamma programming for full update.
 +	 * TODO: This can be further optimized/cleaned up
 +	 * Always call this for now since it does memcmp inside before
 +	 * doing heavy calculation and programming
 +	 */
 +	if (pipe_ctx->plane_state->update_flags.bits.full_update)
 +		dc->hwss.set_output_transfer_func(pipe_ctx, pipe_ctx->stream);
 +}
 +
 +static void dcn20_program_all_pipe_in_tree(
 +		struct dc *dc,
 +		struct pipe_ctx *pipe_ctx,
 +		struct dc_state *context)
 +{
 +	if (pipe_ctx->top_pipe == NULL) {
 +		bool blank = !is_pipe_tree_visible(pipe_ctx);
 +
 +		pipe_ctx->stream_res.tg->funcs->program_global_sync(
 +				pipe_ctx->stream_res.tg,
 +				pipe_ctx->pipe_dlg_param.vready_offset,
 +				pipe_ctx->pipe_dlg_param.vstartup_start,
 +				pipe_ctx->pipe_dlg_param.vupdate_offset,
 +				pipe_ctx->pipe_dlg_param.vupdate_width);
 +
 +		pipe_ctx->stream_res.tg->funcs->set_vtg_params(
 +				pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing);
 +
 +		dc->hwss.blank_pixel_data(dc, pipe_ctx, blank);
 +
 +		if (dc->hwss.update_odm)
 +			dc->hwss.update_odm(dc, context, pipe_ctx);
 +	}
 +
 +	if (pipe_ctx->plane_state != NULL)
 +		dcn20_program_pipe(dc, pipe_ctx, context);
 +
 +	if (pipe_ctx->bottom_pipe != NULL && pipe_ctx->bottom_pipe != pipe_ctx)
 +		dcn20_program_all_pipe_in_tree(dc, pipe_ctx->bottom_pipe, context);
 +}
 +
 +void dcn20_pipe_control_lock_global(
 +		struct dc *dc,
 +		struct pipe_ctx *pipe,
 +		bool lock)
 +{
 +	if (lock) {
 +		pipe->stream_res.tg->funcs->lock_doublebuffer_enable(
 +				pipe->stream_res.tg);
 +		pipe->stream_res.tg->funcs->lock(pipe->stream_res.tg);
 +	} else {
 +		pipe->stream_res.tg->funcs->unlock(pipe->stream_res.tg);
 +		pipe->stream_res.tg->funcs->wait_for_state(pipe->stream_res.tg,
 +				CRTC_STATE_VACTIVE);
 +		pipe->stream_res.tg->funcs->wait_for_state(pipe->stream_res.tg,
 +				CRTC_STATE_VBLANK);
 +		pipe->stream_res.tg->funcs->wait_for_state(pipe->stream_res.tg,
 +				CRTC_STATE_VACTIVE);
 +		pipe->stream_res.tg->funcs->lock_doublebuffer_disable(
 +				pipe->stream_res.tg);
 +	}
 +}
 +
++=======
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  void dcn20_pipe_control_lock(
  	struct dc *dc,
  	struct pipe_ctx *pipe,
@@@ -1310,117 -1156,498 +1327,250 @@@
  		else
  			pipe->stream_res.tg->funcs->unlock(pipe->stream_res.tg);
  	}
+ 
+ 	if (dig_update_required) {
+ 		if (!lock) {
+ 			pipe->stream_res.tg->funcs->wait_for_state(pipe->stream_res.tg,
+ 					CRTC_STATE_VACTIVE);
+ 			pipe->stream_res.tg->funcs->wait_for_state(pipe->stream_res.tg,
+ 					CRTC_STATE_VBLANK);
+ 			pipe->stream_res.tg->funcs->wait_for_state(pipe->stream_res.tg,
+ 					CRTC_STATE_VACTIVE);
+ 			pipe->stream_res.tg->funcs->lock_doublebuffer_disable(
+ 					pipe->stream_res.tg);
+ 		}
+ 	}
  }
  
 -static void dcn20_detect_pipe_changes(struct pipe_ctx *old_pipe, struct pipe_ctx *new_pipe)
 -{
 -	new_pipe->update_flags.raw = 0;
 -
 -	/* Exit on unchanged, unused pipe */
 -	if (!old_pipe->plane_state && !new_pipe->plane_state)
 -		return;
 -	/* Detect pipe enable/disable */
 -	if (!old_pipe->plane_state && new_pipe->plane_state) {
 -		new_pipe->update_flags.bits.enable = 1;
 -		new_pipe->update_flags.bits.mpcc = 1;
 -		new_pipe->update_flags.bits.dppclk = 1;
 -		new_pipe->update_flags.bits.hubp_interdependent = 1;
 -		new_pipe->update_flags.bits.hubp_rq_dlg_ttu = 1;
 -		new_pipe->update_flags.bits.gamut_remap = 1;
 -		new_pipe->update_flags.bits.scaler = 1;
 -		new_pipe->update_flags.bits.viewport = 1;
 -		if (!new_pipe->top_pipe && !new_pipe->prev_odm_pipe) {
 -			new_pipe->update_flags.bits.odm = 1;
 -			new_pipe->update_flags.bits.global_sync = 1;
 -		}
 -		return;
 -	}
 -	if (old_pipe->plane_state && !new_pipe->plane_state) {
 -		new_pipe->update_flags.bits.disable = 1;
 -		return;
 -	}
 -
 -	/* Detect top pipe only changes */
 -	if (!new_pipe->top_pipe && !new_pipe->prev_odm_pipe) {
 -		/* Detect odm changes */
 -		if ((old_pipe->next_odm_pipe && new_pipe->next_odm_pipe
 -			&& old_pipe->next_odm_pipe->pipe_idx != new_pipe->next_odm_pipe->pipe_idx)
 -				|| (!old_pipe->next_odm_pipe && new_pipe->next_odm_pipe)
 -				|| (old_pipe->next_odm_pipe && !new_pipe->next_odm_pipe)
 -				|| old_pipe->stream_res.opp != new_pipe->stream_res.opp)
 -			new_pipe->update_flags.bits.odm = 1;
 -
 -		/* Detect global sync changes */
 -		if (old_pipe->pipe_dlg_param.vready_offset != new_pipe->pipe_dlg_param.vready_offset
 -				|| old_pipe->pipe_dlg_param.vstartup_start != new_pipe->pipe_dlg_param.vstartup_start
 -				|| old_pipe->pipe_dlg_param.vupdate_offset != new_pipe->pipe_dlg_param.vupdate_offset
 -				|| old_pipe->pipe_dlg_param.vupdate_width != new_pipe->pipe_dlg_param.vupdate_width)
 -			new_pipe->update_flags.bits.global_sync = 1;
 -	}
 -
 -	/*
 -	 * Detect opp / tg change, only set on change, not on enable
 -	 * Assume mpcc inst = pipe index, if not this code needs to be updated
 -	 * since mpcc is what is affected by these. In fact all of our sequence
 -	 * makes this assumption at the moment with how hubp reset is matched to
 -	 * same index mpcc reset.
 -	 */
 -	if (old_pipe->stream_res.opp != new_pipe->stream_res.opp)
 -		new_pipe->update_flags.bits.opp_changed = 1;
 -	if (old_pipe->stream_res.tg != new_pipe->stream_res.tg)
 -		new_pipe->update_flags.bits.tg_changed = 1;
 -
 -	/* Detect mpcc blending changes, only dpp inst and bot matter here */
 -	if (old_pipe->plane_res.dpp != new_pipe->plane_res.dpp
 -			|| old_pipe->stream_res.opp != new_pipe->stream_res.opp
 -			|| (!old_pipe->bottom_pipe && new_pipe->bottom_pipe)
 -			|| (old_pipe->bottom_pipe && !new_pipe->bottom_pipe)
 -			|| (old_pipe->bottom_pipe && new_pipe->bottom_pipe
 -				&& old_pipe->bottom_pipe->plane_res.mpcc_inst
 -					!= new_pipe->bottom_pipe->plane_res.mpcc_inst))
 -		new_pipe->update_flags.bits.mpcc = 1;
 -
 -	/* Detect dppclk change */
 -	if (old_pipe->plane_res.bw.dppclk_khz != new_pipe->plane_res.bw.dppclk_khz)
 -		new_pipe->update_flags.bits.dppclk = 1;
 -
 -	/* Check for scl update */
 -	if (memcmp(&old_pipe->plane_res.scl_data, &new_pipe->plane_res.scl_data, sizeof(struct scaler_data)))
 -			new_pipe->update_flags.bits.scaler = 1;
 -	/* Check for vp update */
 -	if (memcmp(&old_pipe->plane_res.scl_data.viewport, &new_pipe->plane_res.scl_data.viewport, sizeof(struct rect))
 -			|| memcmp(&old_pipe->plane_res.scl_data.viewport_c,
 -				&new_pipe->plane_res.scl_data.viewport_c, sizeof(struct rect)))
 -		new_pipe->update_flags.bits.viewport = 1;
 -
 -	/* Detect dlg/ttu/rq updates */
 -	{
 -		struct _vcs_dpi_display_dlg_regs_st old_dlg_attr = old_pipe->dlg_regs;
 -		struct _vcs_dpi_display_ttu_regs_st old_ttu_attr = old_pipe->ttu_regs;
 -		struct _vcs_dpi_display_dlg_regs_st *new_dlg_attr = &new_pipe->dlg_regs;
 -		struct _vcs_dpi_display_ttu_regs_st *new_ttu_attr = &new_pipe->ttu_regs;
 -
 -		/* Detect pipe interdependent updates */
 -		if (old_dlg_attr.dst_y_prefetch != new_dlg_attr->dst_y_prefetch ||
 -				old_dlg_attr.vratio_prefetch != new_dlg_attr->vratio_prefetch ||
 -				old_dlg_attr.vratio_prefetch_c != new_dlg_attr->vratio_prefetch_c ||
 -				old_dlg_attr.dst_y_per_vm_vblank != new_dlg_attr->dst_y_per_vm_vblank ||
 -				old_dlg_attr.dst_y_per_row_vblank != new_dlg_attr->dst_y_per_row_vblank ||
 -				old_dlg_attr.dst_y_per_vm_flip != new_dlg_attr->dst_y_per_vm_flip ||
 -				old_dlg_attr.dst_y_per_row_flip != new_dlg_attr->dst_y_per_row_flip ||
 -				old_dlg_attr.refcyc_per_meta_chunk_vblank_l != new_dlg_attr->refcyc_per_meta_chunk_vblank_l ||
 -				old_dlg_attr.refcyc_per_meta_chunk_vblank_c != new_dlg_attr->refcyc_per_meta_chunk_vblank_c ||
 -				old_dlg_attr.refcyc_per_meta_chunk_flip_l != new_dlg_attr->refcyc_per_meta_chunk_flip_l ||
 -				old_dlg_attr.refcyc_per_line_delivery_pre_l != new_dlg_attr->refcyc_per_line_delivery_pre_l ||
 -				old_dlg_attr.refcyc_per_line_delivery_pre_c != new_dlg_attr->refcyc_per_line_delivery_pre_c ||
 -				old_ttu_attr.refcyc_per_req_delivery_pre_l != new_ttu_attr->refcyc_per_req_delivery_pre_l ||
 -				old_ttu_attr.refcyc_per_req_delivery_pre_c != new_ttu_attr->refcyc_per_req_delivery_pre_c ||
 -				old_ttu_attr.refcyc_per_req_delivery_pre_cur0 != new_ttu_attr->refcyc_per_req_delivery_pre_cur0 ||
 -				old_ttu_attr.refcyc_per_req_delivery_pre_cur1 != new_ttu_attr->refcyc_per_req_delivery_pre_cur1 ||
 -				old_ttu_attr.min_ttu_vblank != new_ttu_attr->min_ttu_vblank ||
 -				old_ttu_attr.qos_level_flip != new_ttu_attr->qos_level_flip) {
 -			old_dlg_attr.dst_y_prefetch = new_dlg_attr->dst_y_prefetch;
 -			old_dlg_attr.vratio_prefetch = new_dlg_attr->vratio_prefetch;
 -			old_dlg_attr.vratio_prefetch_c = new_dlg_attr->vratio_prefetch_c;
 -			old_dlg_attr.dst_y_per_vm_vblank = new_dlg_attr->dst_y_per_vm_vblank;
 -			old_dlg_attr.dst_y_per_row_vblank = new_dlg_attr->dst_y_per_row_vblank;
 -			old_dlg_attr.dst_y_per_vm_flip = new_dlg_attr->dst_y_per_vm_flip;
 -			old_dlg_attr.dst_y_per_row_flip = new_dlg_attr->dst_y_per_row_flip;
 -			old_dlg_attr.refcyc_per_meta_chunk_vblank_l = new_dlg_attr->refcyc_per_meta_chunk_vblank_l;
 -			old_dlg_attr.refcyc_per_meta_chunk_vblank_c = new_dlg_attr->refcyc_per_meta_chunk_vblank_c;
 -			old_dlg_attr.refcyc_per_meta_chunk_flip_l = new_dlg_attr->refcyc_per_meta_chunk_flip_l;
 -			old_dlg_attr.refcyc_per_line_delivery_pre_l = new_dlg_attr->refcyc_per_line_delivery_pre_l;
 -			old_dlg_attr.refcyc_per_line_delivery_pre_c = new_dlg_attr->refcyc_per_line_delivery_pre_c;
 -			old_ttu_attr.refcyc_per_req_delivery_pre_l = new_ttu_attr->refcyc_per_req_delivery_pre_l;
 -			old_ttu_attr.refcyc_per_req_delivery_pre_c = new_ttu_attr->refcyc_per_req_delivery_pre_c;
 -			old_ttu_attr.refcyc_per_req_delivery_pre_cur0 = new_ttu_attr->refcyc_per_req_delivery_pre_cur0;
 -			old_ttu_attr.refcyc_per_req_delivery_pre_cur1 = new_ttu_attr->refcyc_per_req_delivery_pre_cur1;
 -			old_ttu_attr.min_ttu_vblank = new_ttu_attr->min_ttu_vblank;
 -			old_ttu_attr.qos_level_flip = new_ttu_attr->qos_level_flip;
 -			new_pipe->update_flags.bits.hubp_interdependent = 1;
 -		}
 -		/* Detect any other updates to ttu/rq/dlg */
 -		if (memcmp(&old_dlg_attr, &new_pipe->dlg_regs, sizeof(old_dlg_attr)) ||
 -				memcmp(&old_ttu_attr, &new_pipe->ttu_regs, sizeof(old_ttu_attr)) ||
 -				memcmp(&old_pipe->rq_regs, &new_pipe->rq_regs, sizeof(old_pipe->rq_regs)))
 -			new_pipe->update_flags.bits.hubp_rq_dlg_ttu = 1;
 -	}
 -}
 -
 -static void dcn20_update_dchubp_dpp(
 -	struct dc *dc,
 -	struct pipe_ctx *pipe_ctx,
 -	struct dc_state *context)
 -{
 -	struct dce_hwseq *hws = dc->hwseq;
 -	struct hubp *hubp = pipe_ctx->plane_res.hubp;
 -	struct dpp *dpp = pipe_ctx->plane_res.dpp;
 -	struct dc_plane_state *plane_state = pipe_ctx->plane_state;
 -	bool viewport_changed = false;
 -
 -	if (pipe_ctx->update_flags.bits.dppclk)
 -		dpp->funcs->dpp_dppclk_control(dpp, false, true);
 -
 -	/* TODO: Need input parameter to tell current DCHUB pipe tie to which OTG
 -	 * VTG is within DCHUBBUB which is commond block share by each pipe HUBP.
 -	 * VTG is 1:1 mapping with OTG. Each pipe HUBP will select which VTG
 -	 */
 -	if (pipe_ctx->update_flags.bits.hubp_rq_dlg_ttu) {
 -		hubp->funcs->hubp_vtg_sel(hubp, pipe_ctx->stream_res.tg->inst);
 -
 -		hubp->funcs->hubp_setup(
 -			hubp,
 -			&pipe_ctx->dlg_regs,
 -			&pipe_ctx->ttu_regs,
 -			&pipe_ctx->rq_regs,
 -			&pipe_ctx->pipe_dlg_param);
 -	}
 -	if (pipe_ctx->update_flags.bits.hubp_interdependent)
 -		hubp->funcs->hubp_setup_interdependent(
 -			hubp,
 -			&pipe_ctx->dlg_regs,
 -			&pipe_ctx->ttu_regs);
 -
 -	if (pipe_ctx->update_flags.bits.enable ||
 -			plane_state->update_flags.bits.bpp_change ||
 -			plane_state->update_flags.bits.input_csc_change ||
 -			plane_state->update_flags.bits.color_space_change ||
 -			plane_state->update_flags.bits.coeff_reduction_change) {
 -		struct dc_bias_and_scale bns_params = {0};
 -
 -		// program the input csc
 -		dpp->funcs->dpp_setup(dpp,
 -				plane_state->format,
 -				EXPANSION_MODE_ZERO,
 -				plane_state->input_csc_color_matrix,
 -				plane_state->color_space,
 -				NULL);
 -
 -		if (dpp->funcs->dpp_program_bias_and_scale) {
 -			//TODO :for CNVC set scale and bias registers if necessary
 -			build_prescale_params(&bns_params, plane_state);
 -			dpp->funcs->dpp_program_bias_and_scale(dpp, &bns_params);
 -		}
 -	}
 -
 -	if (pipe_ctx->update_flags.bits.mpcc
 -			|| plane_state->update_flags.bits.global_alpha_change
 -			|| plane_state->update_flags.bits.per_pixel_alpha_change) {
 -		// MPCC inst is equal to pipe index in practice
 -		int mpcc_inst = hubp->inst;
 -		int opp_inst;
 -		int opp_count = dc->res_pool->pipe_count;
 -
 -		for (opp_inst = 0; opp_inst < opp_count; opp_inst++) {
 -			if (dc->res_pool->opps[opp_inst]->mpcc_disconnect_pending[mpcc_inst]) {
 -				dc->res_pool->mpc->funcs->wait_for_idle(dc->res_pool->mpc, mpcc_inst);
 -				dc->res_pool->opps[opp_inst]->mpcc_disconnect_pending[mpcc_inst] = false;
 -				break;
 -			}
 -		}
 -		hws->funcs.update_mpcc(dc, pipe_ctx);
 -	}
 -
 -	if (pipe_ctx->update_flags.bits.scaler ||
 -			plane_state->update_flags.bits.scaling_change ||
 -			plane_state->update_flags.bits.position_change ||
 -			plane_state->update_flags.bits.per_pixel_alpha_change ||
 -			pipe_ctx->stream->update_flags.bits.scaling) {
 -		pipe_ctx->plane_res.scl_data.lb_params.alpha_en = pipe_ctx->plane_state->per_pixel_alpha;
 -		ASSERT(pipe_ctx->plane_res.scl_data.lb_params.depth == LB_PIXEL_DEPTH_30BPP);
 -		/* scaler configuration */
 -		pipe_ctx->plane_res.dpp->funcs->dpp_set_scaler(
 -				pipe_ctx->plane_res.dpp, &pipe_ctx->plane_res.scl_data);
 -	}
 -
 -	if (pipe_ctx->update_flags.bits.viewport ||
 -			(context == dc->current_state && plane_state->update_flags.bits.scaling_change) ||
 -			(context == dc->current_state && pipe_ctx->stream->update_flags.bits.scaling)) {
 -
 -		hubp->funcs->mem_program_viewport(
 -			hubp,
 -			&pipe_ctx->plane_res.scl_data.viewport,
 -			&pipe_ctx->plane_res.scl_data.viewport_c);
 -		viewport_changed = true;
 -	}
 -
 -	/* Any updates are handled in dc interface, just need to apply existing for plane enable */
 -	if ((pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed ||
 -			pipe_ctx->update_flags.bits.scaler || pipe_ctx->update_flags.bits.viewport)
 -			&& pipe_ctx->stream->cursor_attributes.address.quad_part != 0) {
 -		dc->hwss.set_cursor_position(pipe_ctx);
 -		dc->hwss.set_cursor_attribute(pipe_ctx);
 -
 -		if (dc->hwss.set_cursor_sdr_white_level)
 -			dc->hwss.set_cursor_sdr_white_level(pipe_ctx);
 -	}
 -
 -	/* Any updates are handled in dc interface, just need
 -	 * to apply existing for plane enable / opp change */
 -	if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed
 -			|| pipe_ctx->stream->update_flags.bits.gamut_remap
 -			|| pipe_ctx->stream->update_flags.bits.out_csc) {
 -			/* dpp/cm gamut remap*/
 -			dc->hwss.program_gamut_remap(pipe_ctx);
 -
 -		/*call the dcn2 method which uses mpc csc*/
 -		dc->hwss.program_output_csc(dc,
 -				pipe_ctx,
 -				pipe_ctx->stream->output_color_space,
 -				pipe_ctx->stream->csc_color_matrix.matrix,
 -				hubp->opp_id);
 -	}
 -
 -	if (pipe_ctx->update_flags.bits.enable ||
 -			pipe_ctx->update_flags.bits.opp_changed ||
 -			plane_state->update_flags.bits.pixel_format_change ||
 -			plane_state->update_flags.bits.horizontal_mirror_change ||
 -			plane_state->update_flags.bits.rotation_change ||
 -			plane_state->update_flags.bits.swizzle_change ||
 -			plane_state->update_flags.bits.dcc_change ||
 -			plane_state->update_flags.bits.bpp_change ||
 -			plane_state->update_flags.bits.scaling_change ||
 -			plane_state->update_flags.bits.plane_size_change) {
 -		struct plane_size size = plane_state->plane_size;
 -
 -		size.surface_size = pipe_ctx->plane_res.scl_data.viewport;
 -		hubp->funcs->hubp_program_surface_config(
 -			hubp,
 -			plane_state->format,
 -			&plane_state->tiling_info,
 -			&size,
 -			plane_state->rotation,
 -			&plane_state->dcc,
 -			plane_state->horizontal_mirror,
 -			0);
 -		hubp->power_gated = false;
 -	}
 -
 -	if (hubp->funcs->apply_PLAT_54186_wa && viewport_changed)
 -		hubp->funcs->apply_PLAT_54186_wa(hubp, &plane_state->address);
 -
 -	if (pipe_ctx->update_flags.bits.enable || plane_state->update_flags.bits.addr_update)
 -		hws->funcs.update_plane_addr(dc, pipe_ctx);
 -
 -
 -
 -	if (pipe_ctx->update_flags.bits.enable)
 -		hubp->funcs->set_blank(hubp, false);
 -}
 -
 -
 -static void dcn20_program_pipe(
 +static void dcn20_apply_ctx_for_surface(
  		struct dc *dc,
 -		struct pipe_ctx *pipe_ctx,
 +		const struct dc_stream_state *stream,
 +		int num_planes,
  		struct dc_state *context)
  {
 -	struct dce_hwseq *hws = dc->hwseq;
 -	/* Only need to unblank on top pipe */
 -	if ((pipe_ctx->update_flags.bits.enable || pipe_ctx->stream->update_flags.bits.abm_level)
 -			&& !pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe)
 -		hws->funcs.blank_pixel_data(dc, pipe_ctx, !pipe_ctx->plane_state->visible);
  
++<<<<<<< HEAD
 +	int i;
 +	struct timing_generator *tg;
 +	bool removed_pipe[6] = { false };
 +	bool interdependent_update = false;
 +	struct pipe_ctx *top_pipe_to_program =
 +			find_top_pipe_for_stream(dc, context, stream);
 +	DC_LOGGER_INIT(dc->ctx->logger);
 +
 +	if (!top_pipe_to_program)
 +		return;
 +
 +	/* Carry over GSL groups in case the context is changing. */
++=======
+ 	if (pipe_ctx->update_flags.bits.global_sync) {
+ 		pipe_ctx->stream_res.tg->funcs->program_global_sync(
+ 				pipe_ctx->stream_res.tg,
+ 				pipe_ctx->pipe_dlg_param.vready_offset,
+ 				pipe_ctx->pipe_dlg_param.vstartup_start,
+ 				pipe_ctx->pipe_dlg_param.vupdate_offset,
+ 				pipe_ctx->pipe_dlg_param.vupdate_width);
+ 
+ 		pipe_ctx->stream_res.tg->funcs->set_vtg_params(
+ 				pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing);
+ 
+ 		if (hws->funcs.setup_vupdate_interrupt)
+ 			hws->funcs.setup_vupdate_interrupt(dc, pipe_ctx);
+ 	}
+ 
+ 	if (pipe_ctx->update_flags.bits.odm)
+ 		hws->funcs.update_odm(dc, context, pipe_ctx);
+ 
+ 	if (pipe_ctx->update_flags.bits.enable)
+ 		dcn20_enable_plane(dc, pipe_ctx, context);
+ 
+ 	if (pipe_ctx->update_flags.raw || pipe_ctx->plane_state->update_flags.raw || pipe_ctx->stream->update_flags.raw)
+ 		dcn20_update_dchubp_dpp(dc, pipe_ctx, context);
+ 
+ 	if (pipe_ctx->update_flags.bits.enable
+ 			|| pipe_ctx->plane_state->update_flags.bits.hdr_mult)
+ 		hws->funcs.set_hdr_multiplier(pipe_ctx);
+ 
+ 	if (pipe_ctx->update_flags.bits.enable ||
+ 			pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change ||
+ 			pipe_ctx->plane_state->update_flags.bits.gamma_change)
+ 		hws->funcs.set_input_transfer_func(dc, pipe_ctx, pipe_ctx->plane_state);
+ 
+ 	/* dcn10_translate_regamma_to_hw_format takes 750us to finish
+ 	 * only do gamma programming for powering on, internal memcmp to avoid
+ 	 * updating on slave planes
+ 	 */
+ 	if (pipe_ctx->update_flags.bits.enable || pipe_ctx->stream->update_flags.bits.out_tf)
+ 		hws->funcs.set_output_transfer_func(dc, pipe_ctx, pipe_ctx->stream);
+ 
+ 	/* If the pipe has been enabled or has a different opp, we
+ 	 * should reprogram the fmt. This deals with cases where
+ 	 * interation between mpc and odm combine on different streams
+ 	 * causes a different pipe to be chosen to odm combine with.
+ 	 */
+ 	if (pipe_ctx->update_flags.bits.enable
+ 	    || pipe_ctx->update_flags.bits.opp_changed) {
+ 
+ 		pipe_ctx->stream_res.opp->funcs->opp_set_dyn_expansion(
+ 			pipe_ctx->stream_res.opp,
+ 			COLOR_SPACE_YCBCR601,
+ 			pipe_ctx->stream->timing.display_color_depth,
+ 			pipe_ctx->stream->signal);
+ 
+ 		pipe_ctx->stream_res.opp->funcs->opp_program_fmt(
+ 			pipe_ctx->stream_res.opp,
+ 			&pipe_ctx->stream->bit_depth_params,
+ 			&pipe_ctx->stream->clamping);
+ 	}
+ }
+ 
+ void dcn20_program_front_end_for_ctx(
+ 		struct dc *dc,
+ 		struct dc_state *context)
+ {
+ 	int i;
+ 	struct dce_hwseq *hws = dc->hwseq;
+ 	DC_LOGGER_INIT(dc->ctx->logger);
+ 
+ 	for (i = 0; i < dc->res_pool->pipe_count; i++) {
+ 		struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i];
+ 
+ 		if (!pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe && pipe_ctx->plane_state) {
+ 			ASSERT(!pipe_ctx->plane_state->triplebuffer_flips);
+ 			if (dc->hwss.program_triplebuffer != NULL &&
+ 				!dc->debug.disable_tri_buf) {
+ 				/*turn off triple buffer for full update*/
+ 				dc->hwss.program_triplebuffer(
+ 					dc, pipe_ctx, pipe_ctx->plane_state->triplebuffer_flips);
+ 			}
+ 		}
+ 	}
+ 
+ 	/* Carry over GSL groups in case the context is changing. */
+ 	for (i = 0; i < dc->res_pool->pipe_count; i++)
+ 		if (context->res_ctx.pipe_ctx[i].stream == dc->current_state->res_ctx.pipe_ctx[i].stream)
+ 			context->res_ctx.pipe_ctx[i].stream_res.gsl_group =
+ 				dc->current_state->res_ctx.pipe_ctx[i].stream_res.gsl_group;
+ 
+ 	/* Set pipe update flags and lock pipes */
+ 	for (i = 0; i < dc->res_pool->pipe_count; i++)
+ 		dcn20_detect_pipe_changes(&dc->current_state->res_ctx.pipe_ctx[i],
+ 				&context->res_ctx.pipe_ctx[i]);
+ 
+ 	/* OTG blank before disabling all front ends */
+ 	for (i = 0; i < dc->res_pool->pipe_count; i++)
+ 		if (context->res_ctx.pipe_ctx[i].update_flags.bits.disable
+ 				&& !context->res_ctx.pipe_ctx[i].top_pipe
+ 				&& !context->res_ctx.pipe_ctx[i].prev_odm_pipe
+ 				&& context->res_ctx.pipe_ctx[i].stream)
+ 			hws->funcs.blank_pixel_data(dc, &context->res_ctx.pipe_ctx[i], true);
+ 
+ 	/* Disconnect mpcc */
+ 	for (i = 0; i < dc->res_pool->pipe_count; i++)
+ 		if (context->res_ctx.pipe_ctx[i].update_flags.bits.disable
+ 				|| context->res_ctx.pipe_ctx[i].update_flags.bits.opp_changed) {
+ 			hws->funcs.plane_atomic_disconnect(dc, &dc->current_state->res_ctx.pipe_ctx[i]);
+ 			DC_LOG_DC("Reset mpcc for pipe %d\n", dc->current_state->res_ctx.pipe_ctx[i].pipe_idx);
+ 		}
+ 
+ 	/*
+ 	 * Program all updated pipes, order matters for mpcc setup. Start with
+ 	 * top pipe and program all pipes that follow in order
+ 	 */
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  	for (i = 0; i < dc->res_pool->pipe_count; i++) {
 -		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
 +		struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i];
 +		struct pipe_ctx *old_pipe_ctx =
 +			&dc->current_state->res_ctx.pipe_ctx[i];
  
 -		if (pipe->plane_state && !pipe->top_pipe) {
 -			while (pipe) {
 -				dcn20_program_pipe(dc, pipe, context);
 -				pipe = pipe->bottom_pipe;
 -			}
 -			/* Program secondary blending tree and writeback pipes */
 -			pipe = &context->res_ctx.pipe_ctx[i];
 -			if (!pipe->prev_odm_pipe && pipe->stream->num_wb_info > 0
 -					&& (pipe->update_flags.raw || pipe->plane_state->update_flags.raw || pipe->stream->update_flags.raw)
 -					&& hws->funcs.program_all_writeback_pipes_in_tree)
 -				hws->funcs.program_all_writeback_pipes_in_tree(dc, pipe->stream, context);
 -		}
 +		if (pipe_ctx->stream == stream &&
 +		    pipe_ctx->stream == old_pipe_ctx->stream)
 +			pipe_ctx->stream_res.gsl_group =
 +				old_pipe_ctx->stream_res.gsl_group;
  	}
 -}
  
 -void dcn20_post_unlock_program_front_end(
 -		struct dc *dc,
 -		struct dc_state *context)
 -{
 -	int i;
 -	const unsigned int TIMEOUT_FOR_PIPE_ENABLE_MS = 100;
 +	tg = top_pipe_to_program->stream_res.tg;
  
 -	DC_LOGGER_INIT(dc->ctx->logger);
 +	interdependent_update = top_pipe_to_program->plane_state &&
 +		top_pipe_to_program->plane_state->update_flags.bits.full_update;
  
 -	for (i = 0; i < dc->res_pool->pipe_count; i++)
 -		if (context->res_ctx.pipe_ctx[i].update_flags.bits.disable)
 -			dc->hwss.disable_plane(dc, &dc->current_state->res_ctx.pipe_ctx[i]);
 +	if (interdependent_update)
 +		lock_all_pipes(dc, context, true);
 +	else
 +		dcn20_pipe_control_lock(dc, top_pipe_to_program, true);
 +
 +	if (num_planes == 0) {
 +		/* OTG blank before remove all front end */
 +		dc->hwss.blank_pixel_data(dc, top_pipe_to_program, true);
 +	}
 +
 +	/* Disconnect unused mpcc */
 +	for (i = 0; i < dc->res_pool->pipe_count; i++) {
 +		struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i];
 +		struct pipe_ctx *old_pipe_ctx =
 +				&dc->current_state->res_ctx.pipe_ctx[i];
 +		/*
 +		 * Powergate reused pipes that are not powergated
 +		 * fairly hacky right now, using opp_id as indicator
 +		 * TODO: After move dc_post to dc_update, this will
 +		 * be removed.
 +		 */
 +		if (pipe_ctx->plane_state && !old_pipe_ctx->plane_state) {
 +			if (old_pipe_ctx->stream_res.tg == tg &&
 +			    old_pipe_ctx->plane_res.hubp &&
 +			    old_pipe_ctx->plane_res.hubp->opp_id != OPP_ID_INVALID)
 +				dcn20_disable_plane(dc, old_pipe_ctx);
 +		}
 +
 +		if ((!pipe_ctx->plane_state ||
 +		     pipe_ctx->stream_res.tg != old_pipe_ctx->stream_res.tg) &&
 +		     old_pipe_ctx->plane_state &&
 +		     old_pipe_ctx->stream_res.tg == tg) {
 +
 +			dc->hwss.plane_atomic_disconnect(dc, old_pipe_ctx);
 +			removed_pipe[i] = true;
 +
 +			DC_LOG_DC("Reset mpcc for pipe %d\n",
 +					old_pipe_ctx->pipe_idx);
 +		}
 +	}
++<<<<<<< HEAD
 +
 +	if (num_planes > 0)
 +		dcn20_program_all_pipe_in_tree(dc, top_pipe_to_program, context);
 +
 +	/* Program secondary blending tree and writeback pipes */
 +	if ((stream->num_wb_info > 0) && (dc->hwss.program_all_writeback_pipes_in_tree))
 +		dc->hwss.program_all_writeback_pipes_in_tree(dc, stream, context);
  
 -	/*
 -	 * If we are enabling a pipe, we need to wait for pending clear as this is a critical
 -	 * part of the enable operation otherwise, DM may request an immediate flip which
 -	 * will cause HW to perform an "immediate enable" (as opposed to "vsync enable") which
 -	 * is unsupported on DCN.
 -	 */
 -	for (i = 0; i < dc->res_pool->pipe_count; i++) {
 -		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
 +	if (interdependent_update)
 +		for (i = 0; i < dc->res_pool->pipe_count; i++) {
 +			struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i];
  
 -		if (pipe->plane_state && !pipe->top_pipe && pipe->update_flags.bits.enable) {
 -			struct hubp *hubp = pipe->plane_res.hubp;
 -			int j = 0;
 +			/* Skip inactive pipes and ones already updated */
 +			if (!pipe_ctx->stream || pipe_ctx->stream == stream ||
 +			    !pipe_ctx->plane_state || !tg->funcs->is_tg_enabled(tg))
 +				continue;
  
 -			for (j = 0; j < TIMEOUT_FOR_PIPE_ENABLE_MS*1000
 -					&& hubp->funcs->hubp_is_flip_pending(hubp); j++)
 -				mdelay(1);
 +			pipe_ctx->plane_res.hubp->funcs->hubp_setup_interdependent(
 +				pipe_ctx->plane_res.hubp,
 +				&pipe_ctx->dlg_regs,
 +				&pipe_ctx->ttu_regs);
  		}
 -	}
++=======
++}
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
 +
 +	if (interdependent_update)
 +		lock_all_pipes(dc, context, false);
 +	else
 +		dcn20_pipe_control_lock(dc, top_pipe_to_program, false);
  
 -	/* WA to apply WM setting*/
 -	if (dc->hwseq->wa.DEGVIDCN21)
 -		dc->res_pool->hubbub->funcs->apply_DEDCN21_147_wa(dc->res_pool->hubbub);
 +	for (i = 0; i < dc->res_pool->pipe_count; i++)
 +		if (removed_pipe[i])
 +			dcn20_disable_plane(dc, &dc->current_state->res_ctx.pipe_ctx[i]);
  }
  
- 
  void dcn20_prepare_bandwidth(
  		struct dc *dc,
  		struct dc_state *context)
diff --cc drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.h
index 2b0409454073,63ce763f148e..000000000000
--- a/drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.h
+++ b/drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.h
@@@ -45,59 -49,88 +45,104 @@@ void dcn20_program_output_csc(struct d
  		enum dc_color_space colorspace,
  		uint16_t *matrix,
  		int opp_id);
 -void dcn20_enable_stream(struct pipe_ctx *pipe_ctx);
 -void dcn20_unblank_stream(struct pipe_ctx *pipe_ctx,
 -		struct dc_link_settings *link_settings);
 -void dcn20_disable_plane(struct dc *dc, struct pipe_ctx *pipe_ctx);
 -void dcn20_blank_pixel_data(
 +
 +void dcn20_prepare_bandwidth(
 +		struct dc *dc,
 +		struct dc_state *context);
 +
 +void dcn20_optimize_bandwidth(
 +		struct dc *dc,
 +		struct dc_state *context);
 +
 +bool dcn20_update_bandwidth(
 +		struct dc *dc,
 +		struct dc_state *context);
 +
 +void dcn20_disable_writeback(
  		struct dc *dc,
 +		unsigned int dwb_pipe_inst);
 +
 +bool dcn20_hwss_wait_for_blank_complete(
 +		struct output_pixel_processor *opp);
 +
 +bool dcn20_set_output_transfer_func(struct pipe_ctx *pipe_ctx,
 +			const struct dc_stream_state *stream);
 +
 +bool dcn20_set_input_transfer_func(struct pipe_ctx *pipe_ctx,
 +			const struct dc_plane_state *plane_state);
 +
 +bool dcn20_dmdata_status_done(struct pipe_ctx *pipe_ctx);
 +
 +void dcn20_set_dmdata_attributes(struct pipe_ctx *pipe_ctx);
 +
 +void dcn20_disable_stream(struct pipe_ctx *pipe_ctx, int option);
 +
 +void dcn20_program_tripleBuffer(
 +		const struct dc *dc,
  		struct pipe_ctx *pipe_ctx,
++<<<<<<< HEAD
 +		bool enableTripleBuffer);
 +
 +void dcn20_setup_vupdate_interrupt(struct pipe_ctx *pipe_ctx);
 +
 +void dcn20_pipe_control_lock_global(
 +		struct dc *dc,
 +		struct pipe_ctx *pipe,
 +		bool lock);
 +void dcn20_setup_gsl_group_as_lock(const struct dc *dc,
 +				struct pipe_ctx *pipe_ctx,
 +				bool enable);
++=======
+ 		bool blank);
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  void dcn20_pipe_control_lock(
  	struct dc *dc,
  	struct pipe_ctx *pipe,
  	bool lock);
++<<<<<<< HEAD
 +void dcn20_disable_plane(struct dc *dc, struct pipe_ctx *pipe_ctx);
 +void dcn20_enable_plane(
 +	struct dc *dc,
++=======
+ void dcn20_prepare_bandwidth(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ void dcn20_optimize_bandwidth(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ bool dcn20_update_bandwidth(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ void dcn20_reset_hw_ctx_wrap(
+ 		struct dc *dc,
+ 		struct dc_state *context);
+ enum dc_status dcn20_enable_stream_timing(
+ 		struct pipe_ctx *pipe_ctx,
+ 		struct dc_state *context,
+ 		struct dc *dc);
+ void dcn20_disable_stream_gating(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn20_enable_stream_gating(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn20_setup_vupdate_interrupt(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn20_init_blank(
+ 		struct dc *dc,
+ 		struct timing_generator *tg);
+ void dcn20_disable_vga(
+ 	struct dce_hwseq *hws);
+ void dcn20_plane_atomic_disable(struct dc *dc, struct pipe_ctx *pipe_ctx);
+ void dcn20_enable_power_gating_plane(
+ 	struct dce_hwseq *hws,
+ 	bool enable);
+ void dcn20_dpp_pg_control(
+ 		struct dce_hwseq *hws,
+ 		unsigned int dpp_inst,
+ 		bool power_on);
+ void dcn20_hubp_pg_control(
+ 		struct dce_hwseq *hws,
+ 		unsigned int hubp_inst,
+ 		bool power_on);
+ void dcn20_program_triple_buffer(
+ 	const struct dc *dc,
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  	struct pipe_ctx *pipe_ctx,
 -	bool enable_triple_buffer);
 -void dcn20_enable_writeback(
 -		struct dc *dc,
 -		struct dc_writeback_info *wb_info,
 -		struct dc_state *context);
 -void dcn20_disable_writeback(
 -		struct dc *dc,
 -		unsigned int dwb_pipe_inst);
 -void dcn20_update_odm(struct dc *dc, struct dc_state *context, struct pipe_ctx *pipe_ctx);
 -bool dcn20_dmdata_status_done(struct pipe_ctx *pipe_ctx);
 -void dcn20_program_dmdata_engine(struct pipe_ctx *pipe_ctx);
 -void dcn20_set_dmdata_attributes(struct pipe_ctx *pipe_ctx);
 -void dcn20_init_vm_ctx(
 -		struct dce_hwseq *hws,
 -		struct dc *dc,
 -		struct dc_virtual_addr_space_config *va_config,
 -		int vmid);
 -void dcn20_set_flip_control_gsl(
 -		struct pipe_ctx *pipe_ctx,
 -		bool flip_immediate);
 -void dcn20_dsc_pg_control(
 -		struct dce_hwseq *hws,
 -		unsigned int dsc_inst,
 -		bool power_on);
 -void dcn20_fpga_init_hw(struct dc *dc);
 -bool dcn20_wait_for_blank_complete(
 -		struct output_pixel_processor *opp);
 -void dcn20_dccg_init(struct dce_hwseq *hws);
 -int dcn20_init_sys_ctx(struct dce_hwseq *hws,
 -		struct dc *dc,
 -		struct dc_phy_addr_space_config *pa_config);
 -
 +	struct dc_state *context);
  #endif /* __DC_HWSS_DCN20_H__ */
 -
diff --cc drivers/gpu/drm/amd/display/dc/inc/hw_sequencer.h
index c6fd0f92eb43,d4c1fb242c63..000000000000
--- a/drivers/gpu/drm/amd/display/dc/inc/hw_sequencer.h
+++ b/drivers/gpu/drm/amd/display/dc/inc/hw_sequencer.h
@@@ -43,160 -37,79 +43,170 @@@ enum vline_select 
  	VLINE1
  };
  
 +struct dce_hwseq_wa {
 +	bool blnd_crtc_trigger;
 +	bool DEGVIDCN10_253;
 +	bool false_optc_underflow;
 +	bool DEGVIDCN10_254;
 +};
 +
 +struct hwseq_wa_state {
 +	bool DEGVIDCN10_253_applied;
 +};
 +
 +struct dce_hwseq {
 +	struct dc_context *ctx;
 +	const struct dce_hwseq_registers *regs;
 +	const struct dce_hwseq_shift *shifts;
 +	const struct dce_hwseq_mask *masks;
 +	struct dce_hwseq_wa wa;
 +	struct hwseq_wa_state wa_state;
 +};
 +
  struct pipe_ctx;
  struct dc_state;
 +#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
  struct dc_stream_status;
  struct dc_writeback_info;
 +#endif
  struct dchub_init_data;
 -struct dc_static_screen_params;
 +struct dc_static_screen_events;
  struct resource_pool;
 +struct resource_context;
 +struct stream_resource;
 +#ifdef CONFIG_DRM_AMD_DC_DCN2_0
  struct dc_phy_addr_space_config;
  struct dc_virtual_addr_space_config;
 -struct dpp;
 -struct dce_hwseq;
 +#endif
  
  struct hw_sequencer_funcs {
 -	/* Embedded Display Related */
 -	void (*edp_power_control)(struct dc_link *link, bool enable);
 -	void (*edp_wait_for_hpd_ready)(struct dc_link *link, bool power_up);
  
 -	/* Pipe Programming Related */
 +	void (*disable_stream_gating)(struct dc *dc, struct pipe_ctx *pipe_ctx);
 +
 +	void (*enable_stream_gating)(struct dc *dc, struct pipe_ctx *pipe_ctx);
 +
  	void (*init_hw)(struct dc *dc);
 -	void (*enable_accelerated_mode)(struct dc *dc,
 -			struct dc_state *context);
 -	enum dc_status (*apply_ctx_to_hw)(struct dc *dc,
 -			struct dc_state *context);
 -	void (*disable_plane)(struct dc *dc, struct pipe_ctx *pipe_ctx);
 -	void (*apply_ctx_for_surface)(struct dc *dc,
 +
 +	void (*init_pipes)(struct dc *dc, struct dc_state *context);
 +
 +	enum dc_status (*apply_ctx_to_hw)(
 +			struct dc *dc, struct dc_state *context);
 +
 +	void (*reset_hw_ctx_wrap)(
 +			struct dc *dc, struct dc_state *context);
 +
 +	void (*apply_ctx_for_surface)(
 +			struct dc *dc,
  			const struct dc_stream_state *stream,
 -			int num_planes, struct dc_state *context);
 -	void (*program_front_end_for_ctx)(struct dc *dc,
 +			int num_planes,
  			struct dc_state *context);
 -	void (*post_unlock_program_front_end)(struct dc *dc,
 -			struct dc_state *context);
 -	void (*update_plane_addr)(const struct dc *dc,
 -			struct pipe_ctx *pipe_ctx);
 -	void (*update_dchub)(struct dce_hwseq *hws,
 -			struct dchub_init_data *dh_data);
 -	void (*wait_for_mpcc_disconnect)(struct dc *dc,
 -			struct resource_pool *res_pool,
 -			struct pipe_ctx *pipe_ctx);
 -	void (*program_triplebuffer)(const struct dc *dc,
 -		struct pipe_ctx *pipe_ctx, bool enableTripleBuffer);
 -	void (*update_pending_status)(struct pipe_ctx *pipe_ctx);
  
++<<<<<<< HEAD
 +	void (*program_gamut_remap)(
 +			struct pipe_ctx *pipe_ctx);
++=======
+ 	/* Pipe Lock Related */
+ 	void (*pipe_control_lock)(struct dc *dc,
+ 			struct pipe_ctx *pipe, bool lock);
+ 	void (*interdependent_update_lock)(struct dc *dc,
+ 			struct dc_state *context, bool lock);
+ 	void (*set_flip_control_gsl)(struct pipe_ctx *pipe_ctx,
+ 			bool flip_immediate);
++>>>>>>> 009114f6df84 (drm/amd/display: Added locking for atomic update stream and update planes)
  
 -	/* Timing Related */
 -	void (*get_position)(struct pipe_ctx **pipe_ctx, int num_pipes,
 -			struct crtc_position *position);
 -	int (*get_vupdate_offset_from_vsync)(struct pipe_ctx *pipe_ctx);
 -	void (*enable_per_frame_crtc_position_reset)(struct dc *dc,
 -			int group_size, struct pipe_ctx *grouped_pipes[]);
 -	void (*enable_timing_synchronization)(struct dc *dc,
 -			int group_index, int group_size,
 -			struct pipe_ctx *grouped_pipes[]);
 -	void (*setup_periodic_interrupt)(struct dc *dc,
 +	void (*program_output_csc)(struct dc *dc,
  			struct pipe_ctx *pipe_ctx,
 -			enum vline_select vline);
 -	void (*set_drr)(struct pipe_ctx **pipe_ctx, int num_pipes,
 -			unsigned int vmin, unsigned int vmax,
 -			unsigned int vmid, unsigned int vmid_frame_number);
 -	void (*set_static_screen_control)(struct pipe_ctx **pipe_ctx,
 -			int num_pipes,
 -			const struct dc_static_screen_params *events);
 +			enum dc_color_space colorspace,
 +			uint16_t *matrix,
 +			int opp_id);
 +
 +#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 +	void (*program_triplebuffer)(
 +		const struct dc *dc,
 +		struct pipe_ctx *pipe_ctx,
 +		bool enableTripleBuffer);
 +	void (*set_flip_control_gsl)(
 +		struct pipe_ctx *pipe_ctx,
 +		bool flip_immediate);
 +#endif
 +
 +	void (*update_plane_addr)(
 +		const struct dc *dc,
 +		struct pipe_ctx *pipe_ctx);
 +
 +	void (*plane_atomic_disconnect)(
 +		struct dc *dc,
 +		struct pipe_ctx *pipe_ctx);
 +
 +	void (*update_dchub)(
 +		struct dce_hwseq *hws,
 +		struct dchub_init_data *dh_data);
 +
 +#ifdef CONFIG_DRM_AMD_DC_DCN2_0
 +	int (*init_sys_ctx)(
 +			struct dce_hwseq *hws,
 +			struct dc *dc,
 +			struct dc_phy_addr_space_config *pa_config);
 +	void (*init_vm_ctx)(
 +			struct dce_hwseq *hws,
 +			struct dc *dc,
 +			struct dc_virtual_addr_space_config *va_config,
 +			int vmid);
 +#endif
 +	void (*update_mpcc)(
 +		struct dc *dc,
 +		struct pipe_ctx *pipe_ctx);
 +
 +	void (*update_pending_status)(
 +			struct pipe_ctx *pipe_ctx);
 +
 +	bool (*set_input_transfer_func)(
 +				struct pipe_ctx *pipe_ctx,
 +				const struct dc_plane_state *plane_state);
 +
 +	bool (*set_output_transfer_func)(
 +				struct pipe_ctx *pipe_ctx,
 +				const struct dc_stream_state *stream);
 +
 +	void (*power_down)(struct dc *dc);
 +
 +	void (*enable_accelerated_mode)(struct dc *dc, struct dc_state *context);
 +
 +	void (*enable_timing_synchronization)(
 +			struct dc *dc,
 +			int group_index,
 +			int group_size,
 +			struct pipe_ctx *grouped_pipes[]);
 +
 +	void (*enable_per_frame_crtc_position_reset)(
 +			struct dc *dc,
 +			int group_size,
 +			struct pipe_ctx *grouped_pipes[]);
 +
 +	void (*enable_display_pipe_clock_gating)(
 +					struct dc_context *ctx,
 +					bool clock_gating);
 +
 +	bool (*enable_display_power_gating)(
 +					struct dc *dc,
 +					uint8_t controller_id,
 +					struct dc_bios *dcb,
 +					enum pipe_gating_control power_gating);
 +
 +	void (*disable_plane)(struct dc *dc, struct pipe_ctx *pipe_ctx);
 +
 +	void (*update_info_frame)(struct pipe_ctx *pipe_ctx);
 +
 +	void (*send_immediate_sdp_message)(
 +				struct pipe_ctx *pipe_ctx,
 +				const uint8_t *custom_sdp_message,
 +				unsigned int sdp_message_size);
  
 -	/* Stream Related */
  	void (*enable_stream)(struct pipe_ctx *pipe_ctx);
 -	void (*disable_stream)(struct pipe_ctx *pipe_ctx);
 -	void (*blank_stream)(struct pipe_ctx *pipe_ctx);
 +
 +	void (*disable_stream)(struct pipe_ctx *pipe_ctx,
 +			int option);
 +
  	void (*unblank_stream)(struct pipe_ctx *pipe_ctx,
  			struct dc_link_settings *link_settings);
  
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn10/dcn10_init.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn20/dcn20_init.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn21/dcn21_init.c
* Unmerged path drivers/gpu/drm/amd/display/dc/core/dc.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dce110/dce110_hw_sequencer.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn10/dcn10_hw_sequencer.h
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn10/dcn10_init.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn20/dcn20_hwseq.h
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn20/dcn20_init.c
* Unmerged path drivers/gpu/drm/amd/display/dc/dcn21/dcn21_init.c
* Unmerged path drivers/gpu/drm/amd/display/dc/inc/hw_sequencer.h
