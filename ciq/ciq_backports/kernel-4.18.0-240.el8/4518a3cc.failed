io_uring: fix flush req->refs underflow

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 4518a3cc273cf82efdd36522fb1f13baad173c70
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/4518a3cc.failed

In io_uring_cancel_files(), after refcount_sub_and_test() leaves 0
req->refs, it calls io_put_req(), which would also put a ref. Call
io_free_req() instead.

	Cc: stable@vger.kernel.org
Fixes: 2ca10259b418 ("io_uring: prune request from overflow list on flush")
	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 4518a3cc273cf82efdd36522fb1f13baad173c70)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 047c6a5f549f,37422fcdaa7f..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -3586,12 -7490,83 +3586,73 @@@ static int io_uring_release(struct inod
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
++=======
+ static void io_uring_cancel_files(struct io_ring_ctx *ctx,
+ 				  struct files_struct *files)
+ {
+ 	while (!list_empty_careful(&ctx->inflight_list)) {
+ 		struct io_kiocb *cancel_req = NULL, *req;
+ 		DEFINE_WAIT(wait);
+ 
+ 		spin_lock_irq(&ctx->inflight_lock);
+ 		list_for_each_entry(req, &ctx->inflight_list, inflight_entry) {
+ 			if (req->work.files != files)
+ 				continue;
+ 			/* req is being completed, ignore */
+ 			if (!refcount_inc_not_zero(&req->refs))
+ 				continue;
+ 			cancel_req = req;
+ 			break;
+ 		}
+ 		if (cancel_req)
+ 			prepare_to_wait(&ctx->inflight_wait, &wait,
+ 						TASK_UNINTERRUPTIBLE);
+ 		spin_unlock_irq(&ctx->inflight_lock);
+ 
+ 		/* We need to keep going until we don't find a matching req */
+ 		if (!cancel_req)
+ 			break;
+ 
+ 		if (cancel_req->flags & REQ_F_OVERFLOW) {
+ 			spin_lock_irq(&ctx->completion_lock);
+ 			list_del(&cancel_req->list);
+ 			cancel_req->flags &= ~REQ_F_OVERFLOW;
+ 			if (list_empty(&ctx->cq_overflow_list)) {
+ 				clear_bit(0, &ctx->sq_check_overflow);
+ 				clear_bit(0, &ctx->cq_check_overflow);
+ 			}
+ 			spin_unlock_irq(&ctx->completion_lock);
+ 
+ 			WRITE_ONCE(ctx->rings->cq_overflow,
+ 				atomic_inc_return(&ctx->cached_cq_overflow));
+ 
+ 			/*
+ 			 * Put inflight ref and overflow ref. If that's
+ 			 * all we had, then we're done with this request.
+ 			 */
+ 			if (refcount_sub_and_test(2, &cancel_req->refs)) {
+ 				io_free_req(cancel_req);
+ 				finish_wait(&ctx->inflight_wait, &wait);
+ 				continue;
+ 			}
+ 		}
+ 
+ 		io_wq_cancel_work(ctx->io_wq, &cancel_req->work);
+ 		io_put_req(cancel_req);
+ 		schedule();
+ 		finish_wait(&ctx->inflight_wait, &wait);
+ 	}
+ }
+ 
+ static int io_uring_flush(struct file *file, void *data)
++>>>>>>> 4518a3cc273c (io_uring: fix flush req->refs underflow)
  {
 +	loff_t offset = (loff_t) vma->vm_pgoff << PAGE_SHIFT;
 +	unsigned long sz = vma->vm_end - vma->vm_start;
  	struct io_ring_ctx *ctx = file->private_data;
 -
 -	io_uring_cancel_files(ctx, data);
 -
 -	/*
 -	 * If the task is going away, cancel work it may have pending
 -	 */
 -	if (fatal_signal_pending(current) || (current->flags & PF_EXITING))
 -		io_wq_cancel_pid(ctx->io_wq, task_pid_vnr(current));
 -
 -	return 0;
 -}
 -
 -static void *io_uring_validate_mmap_request(struct file *file,
 -					    loff_t pgoff, size_t sz)
 -{
 -	struct io_ring_ctx *ctx = file->private_data;
 -	loff_t offset = pgoff << PAGE_SHIFT;
 +	unsigned long pfn;
  	struct page *page;
  	void *ptr;
  
* Unmerged path fs/io_uring.c
