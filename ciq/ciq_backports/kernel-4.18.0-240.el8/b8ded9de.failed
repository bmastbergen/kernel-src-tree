net/smc: pre-fetch send buffer outside of send_lock

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Karsten Graul <kgraul@linux.ibm.com>
commit b8ded9de8db34dd209a3dece94cf54fc414e78f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/b8ded9de.failed

Pre-fetch send buffer for the CDC validation message before entering the
send_lock. Without that the send call might fail with -EBUSY because
there are no free buffers and waiting for buffers is not possible under
send_lock.

	Signed-off-by: Karsten Graul <kgraul@linux.ibm.com>
	Reviewed-by: Ursula Braun <ubraun@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit b8ded9de8db34dd209a3dece94cf54fc414e78f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/smc/smc_cdc.c
#	net/smc/smc_cdc.h
#	net/smc/smc_core.c
diff --cc net/smc/smc_cdc.c
index b25788f91ceb,a47e8855e045..000000000000
--- a/net/smc/smc_cdc.c
+++ b/net/smc/smc_cdc.c
@@@ -115,6 -115,27 +115,30 @@@ int smc_cdc_msg_send(struct smc_connect
  	return rc;
  }
  
++<<<<<<< HEAD
++=======
+ /* send a validation msg indicating the move of a conn to an other QP link */
+ int smcr_cdc_msg_send_validation(struct smc_connection *conn,
+ 				 struct smc_cdc_tx_pend *pend,
+ 				 struct smc_wr_buf *wr_buf)
+ {
+ 	struct smc_host_cdc_msg *local = &conn->local_tx_ctrl;
+ 	struct smc_link *link = conn->lnk;
+ 	struct smc_cdc_msg *peer;
+ 	int rc;
+ 
+ 	peer = (struct smc_cdc_msg *)wr_buf;
+ 	peer->common.type = local->common.type;
+ 	peer->len = local->len;
+ 	peer->seqno = htons(conn->tx_cdc_seq_fin); /* seqno last compl. tx */
+ 	peer->token = htonl(local->token);
+ 	peer->prod_flags.failover_validation = 1;
+ 
+ 	rc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);
+ 	return rc;
+ }
+ 
++>>>>>>> b8ded9de8db3 (net/smc: pre-fetch send buffer outside of send_lock)
  static int smcr_cdc_get_slot_and_msg_send(struct smc_connection *conn)
  {
  	struct smc_cdc_tx_pend *pend;
diff --cc net/smc/smc_cdc.h
index 5a19e5e2280e,0a0a89abd38b..000000000000
--- a/net/smc/smc_cdc.h
+++ b/net/smc/smc_cdc.h
@@@ -295,6 -296,9 +295,12 @@@ int smc_cdc_msg_send(struct smc_connect
  		     struct smc_cdc_tx_pend *pend);
  int smc_cdc_get_slot_and_msg_send(struct smc_connection *conn);
  int smcd_cdc_msg_send(struct smc_connection *conn);
++<<<<<<< HEAD
++=======
+ int smcr_cdc_msg_send_validation(struct smc_connection *conn,
+ 				 struct smc_cdc_tx_pend *pend,
+ 				 struct smc_wr_buf *wr_buf);
++>>>>>>> b8ded9de8db3 (net/smc: pre-fetch send buffer outside of send_lock)
  int smc_cdc_init(void) __init;
  void smcd_cdc_rx_init(struct smc_connection *conn);
  
diff --cc net/smc/smc_core.c
index 399bc3ffb64e,7964a21e5e6f..000000000000
--- a/net/smc/smc_core.c
+++ b/net/smc/smc_core.c
@@@ -476,10 -469,151 +476,154 @@@ out
  	return rc;
  }
  
++<<<<<<< HEAD
++=======
+ static int smc_write_space(struct smc_connection *conn)
+ {
+ 	int buffer_len = conn->peer_rmbe_size;
+ 	union smc_host_cursor prod;
+ 	union smc_host_cursor cons;
+ 	int space;
+ 
+ 	smc_curs_copy(&prod, &conn->local_tx_ctrl.prod, conn);
+ 	smc_curs_copy(&cons, &conn->local_rx_ctrl.cons, conn);
+ 	/* determine rx_buf space */
+ 	space = buffer_len - smc_curs_diff(buffer_len, &cons, &prod);
+ 	return space;
+ }
+ 
+ static int smc_switch_cursor(struct smc_sock *smc, struct smc_cdc_tx_pend *pend,
+ 			     struct smc_wr_buf *wr_buf)
+ {
+ 	struct smc_connection *conn = &smc->conn;
+ 	union smc_host_cursor cons, fin;
+ 	int rc = 0;
+ 	int diff;
+ 
+ 	smc_curs_copy(&conn->tx_curs_sent, &conn->tx_curs_fin, conn);
+ 	smc_curs_copy(&fin, &conn->local_tx_ctrl_fin, conn);
+ 	/* set prod cursor to old state, enforce tx_rdma_writes() */
+ 	smc_curs_copy(&conn->local_tx_ctrl.prod, &fin, conn);
+ 	smc_curs_copy(&cons, &conn->local_rx_ctrl.cons, conn);
+ 
+ 	if (smc_curs_comp(conn->peer_rmbe_size, &cons, &fin) < 0) {
+ 		/* cons cursor advanced more than fin, and prod was set
+ 		 * fin above, so now prod is smaller than cons. Fix that.
+ 		 */
+ 		diff = smc_curs_diff(conn->peer_rmbe_size, &fin, &cons);
+ 		smc_curs_add(conn->sndbuf_desc->len,
+ 			     &conn->tx_curs_sent, diff);
+ 		smc_curs_add(conn->sndbuf_desc->len,
+ 			     &conn->tx_curs_fin, diff);
+ 
+ 		smp_mb__before_atomic();
+ 		atomic_add(diff, &conn->sndbuf_space);
+ 		smp_mb__after_atomic();
+ 
+ 		smc_curs_add(conn->peer_rmbe_size,
+ 			     &conn->local_tx_ctrl.prod, diff);
+ 		smc_curs_add(conn->peer_rmbe_size,
+ 			     &conn->local_tx_ctrl_fin, diff);
+ 	}
+ 	/* recalculate, value is used by tx_rdma_writes() */
+ 	atomic_set(&smc->conn.peer_rmbe_space, smc_write_space(conn));
+ 
+ 	if (smc->sk.sk_state != SMC_INIT &&
+ 	    smc->sk.sk_state != SMC_CLOSED) {
+ 		rc = smcr_cdc_msg_send_validation(conn, pend, wr_buf);
+ 		if (!rc) {
+ 			schedule_delayed_work(&conn->tx_work, 0);
+ 			smc->sk.sk_data_ready(&smc->sk);
+ 		}
+ 	} else {
+ 		smc_wr_tx_put_slot(conn->lnk,
+ 				   (struct smc_wr_tx_pend_priv *)pend);
+ 	}
+ 	return rc;
+ }
+ 
+ struct smc_link *smc_switch_conns(struct smc_link_group *lgr,
+ 				  struct smc_link *from_lnk, bool is_dev_err)
+ {
+ 	struct smc_link *to_lnk = NULL;
+ 	struct smc_cdc_tx_pend *pend;
+ 	struct smc_connection *conn;
+ 	struct smc_wr_buf *wr_buf;
+ 	struct smc_sock *smc;
+ 	struct rb_node *node;
+ 	int i, rc = 0;
+ 
+ 	/* link is inactive, wake up tx waiters */
+ 	smc_wr_wakeup_tx_wait(from_lnk);
+ 
+ 	for (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {
+ 		if (lgr->lnk[i].state != SMC_LNK_ACTIVE ||
+ 		    i == from_lnk->link_idx)
+ 			continue;
+ 		if (is_dev_err && from_lnk->smcibdev == lgr->lnk[i].smcibdev &&
+ 		    from_lnk->ibport == lgr->lnk[i].ibport) {
+ 			continue;
+ 		}
+ 		to_lnk = &lgr->lnk[i];
+ 		break;
+ 	}
+ 	if (!to_lnk) {
+ 		smc_lgr_terminate_sched(lgr);
+ 		return NULL;
+ 	}
+ again:
+ 	read_lock_bh(&lgr->conns_lock);
+ 	for (node = rb_first(&lgr->conns_all); node; node = rb_next(node)) {
+ 		conn = rb_entry(node, struct smc_connection, alert_node);
+ 		if (conn->lnk != from_lnk)
+ 			continue;
+ 		smc = container_of(conn, struct smc_sock, conn);
+ 		/* conn->lnk not yet set in SMC_INIT state */
+ 		if (smc->sk.sk_state == SMC_INIT)
+ 			continue;
+ 		if (smc->sk.sk_state == SMC_CLOSED ||
+ 		    smc->sk.sk_state == SMC_PEERCLOSEWAIT1 ||
+ 		    smc->sk.sk_state == SMC_PEERCLOSEWAIT2 ||
+ 		    smc->sk.sk_state == SMC_APPFINCLOSEWAIT ||
+ 		    smc->sk.sk_state == SMC_APPCLOSEWAIT1 ||
+ 		    smc->sk.sk_state == SMC_APPCLOSEWAIT2 ||
+ 		    smc->sk.sk_state == SMC_PEERFINCLOSEWAIT ||
+ 		    smc->sk.sk_state == SMC_PEERABORTWAIT ||
+ 		    smc->sk.sk_state == SMC_PROCESSABORT) {
+ 			spin_lock_bh(&conn->send_lock);
+ 			conn->lnk = to_lnk;
+ 			spin_unlock_bh(&conn->send_lock);
+ 			continue;
+ 		}
+ 		sock_hold(&smc->sk);
+ 		read_unlock_bh(&lgr->conns_lock);
+ 		/* pre-fetch buffer outside of send_lock, might sleep */
+ 		rc = smc_cdc_get_free_slot(conn, to_lnk, &wr_buf, NULL, &pend);
+ 		if (rc) {
+ 			smcr_link_down_cond_sched(to_lnk);
+ 			return NULL;
+ 		}
+ 		/* avoid race with smcr_tx_sndbuf_nonempty() */
+ 		spin_lock_bh(&conn->send_lock);
+ 		conn->lnk = to_lnk;
+ 		rc = smc_switch_cursor(smc, pend, wr_buf);
+ 		spin_unlock_bh(&conn->send_lock);
+ 		sock_put(&smc->sk);
+ 		if (rc) {
+ 			smcr_link_down_cond_sched(to_lnk);
+ 			return NULL;
+ 		}
+ 		goto again;
+ 	}
+ 	read_unlock_bh(&lgr->conns_lock);
+ 	return to_lnk;
+ }
+ 
++>>>>>>> b8ded9de8db3 (net/smc: pre-fetch send buffer outside of send_lock)
  static void smcr_buf_unuse(struct smc_buf_desc *rmb_desc,
 -			   struct smc_link_group *lgr)
 +			   struct smc_link *lnk)
  {
 -	int rc;
 +	struct smc_link_group *lgr = lnk->lgr;
  
  	if (rmb_desc->is_conf_rkey && !list_empty(&lgr->list)) {
  		/* unregister rmb with peer */
* Unmerged path net/smc/smc_cdc.c
* Unmerged path net/smc/smc_cdc.h
* Unmerged path net/smc/smc_core.c
