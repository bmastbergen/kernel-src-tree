libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jiri Olsa <jolsa@kernel.org>
commit 315c0a1f0ccdd44c65f80ccbc62202fed8a23050
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/315c0a1f.failed

So it's part of the libperf library as one of basic functions operating
on the perf_cpu_map class.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Michael Petlan <mpetlan@redhat.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/20190822111141.25823-4-jolsa@kernel.org
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit 315c0a1f0ccdd44c65f80ccbc62202fed8a23050)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/arch/x86/util/intel-pt.c
#	tools/perf/builtin-stat.c
#	tools/perf/lib/cpumap.c
#	tools/perf/lib/include/perf/cpumap.h
#	tools/perf/lib/libperf.map
#	tools/perf/util/cpumap.c
#	tools/perf/util/cpumap.h
#	tools/perf/util/evlist.c
#	tools/perf/util/record.c
diff --cc tools/perf/arch/x86/util/intel-pt.c
index 3a851647e6f4,c72a77a82b39..000000000000
--- a/tools/perf/arch/x86/util/intel-pt.c
+++ b/tools/perf/arch/x86/util/intel-pt.c
@@@ -374,7 -365,7 +374,11 @@@ static int intel_pt_info_fill(struct au
  			ui__warning("Intel Processor Trace: TSC not available\n");
  	}
  
++<<<<<<< HEAD
 +	per_cpu_mmaps = !cpu_map__empty(session->evlist->cpus);
++=======
+ 	per_cpu_mmaps = !perf_cpu_map__empty(session->evlist->core.cpus);
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  
  	auxtrace_info->type = PERF_AUXTRACE_INTEL_PT;
  	auxtrace_info->priv[INTEL_PT_PMU_TYPE] = intel_pt_pmu->type;
diff --cc tools/perf/builtin-stat.c
index b71c4390d333,90636a811b36..000000000000
--- a/tools/perf/builtin-stat.c
+++ b/tools/perf/builtin-stat.c
@@@ -928,8 -927,8 +928,13 @@@ static int perf_stat_init_aggr_mode(voi
  	 * taking the highest cpu number to be the size of
  	 * the aggregation translate cpumap.
  	 */
++<<<<<<< HEAD
 +	nr = cpu_map__get_max(evsel_list->cpus);
 +	stat_config.cpus_aggr_map = cpu_map__empty_new(nr + 1);
++=======
+ 	nr = cpu_map__get_max(evsel_list->core.cpus);
+ 	stat_config.cpus_aggr_map = perf_cpu_map__empty_new(nr + 1);
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  	return stat_config.cpus_aggr_map ? 0 : -ENOMEM;
  }
  
diff --cc tools/perf/lib/libperf.map
index 3536242c545c,3373dd51fcda..000000000000
--- a/tools/perf/lib/libperf.map
+++ b/tools/perf/lib/libperf.map
@@@ -1,6 -1,41 +1,44 @@@
  LIBPERF_0.0.1 {
  	global:
  		libperf_set_print;
++<<<<<<< HEAD
++=======
+ 		perf_cpu_map__dummy_new;
+ 		perf_cpu_map__get;
+ 		perf_cpu_map__put;
+ 		perf_cpu_map__new;
+ 		perf_cpu_map__read;
+ 		perf_cpu_map__nr;
+ 		perf_cpu_map__cpu;
+ 		perf_cpu_map__empty;
+ 		perf_thread_map__new_dummy;
+ 		perf_thread_map__set_pid;
+ 		perf_thread_map__comm;
+ 		perf_thread_map__get;
+ 		perf_thread_map__put;
+ 		perf_evsel__new;
+ 		perf_evsel__delete;
+ 		perf_evsel__enable;
+ 		perf_evsel__disable;
+ 		perf_evsel__init;
+ 		perf_evsel__open;
+ 		perf_evsel__close;
+ 		perf_evsel__read;
+ 		perf_evsel__cpus;
+ 		perf_evsel__threads;
+ 		perf_evsel__attr;
+ 		perf_evlist__new;
+ 		perf_evlist__delete;
+ 		perf_evlist__open;
+ 		perf_evlist__close;
+ 		perf_evlist__enable;
+ 		perf_evlist__disable;
+ 		perf_evlist__init;
+ 		perf_evlist__add;
+ 		perf_evlist__remove;
+ 		perf_evlist__next;
+ 		perf_evlist__set_maps;
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  	local:
  		*;
  };
diff --cc tools/perf/util/cpumap.c
index 77925b3c8d91,4402e67445a4..000000000000
--- a/tools/perf/util/cpumap.c
+++ b/tools/perf/util/cpumap.c
@@@ -17,196 -17,11 +17,196 @@@ static int max_present_cpu_num
  static int max_node_num;
  static int *cpunode_map;
  
 -static struct perf_cpu_map *cpu_map__from_entries(struct cpu_map_entries *cpus)
 +static struct cpu_map *cpu_map__default_new(void)
  {
 -	struct perf_cpu_map *map;
 +	struct cpu_map *cpus;
 +	int nr_cpus;
 +
 +	nr_cpus = sysconf(_SC_NPROCESSORS_ONLN);
 +	if (nr_cpus < 0)
 +		return NULL;
 +
 +	cpus = malloc(sizeof(*cpus) + nr_cpus * sizeof(int));
 +	if (cpus != NULL) {
 +		int i;
 +		for (i = 0; i < nr_cpus; ++i)
 +			cpus->map[i] = i;
 +
 +		cpus->nr = nr_cpus;
 +		refcount_set(&cpus->refcnt, 1);
 +	}
 +
 +	return cpus;
 +}
 +
 +static struct cpu_map *cpu_map__trim_new(int nr_cpus, int *tmp_cpus)
 +{
 +	size_t payload_size = nr_cpus * sizeof(int);
 +	struct cpu_map *cpus = malloc(sizeof(*cpus) + payload_size);
 +
 +	if (cpus != NULL) {
 +		cpus->nr = nr_cpus;
 +		memcpy(cpus->map, tmp_cpus, payload_size);
 +		refcount_set(&cpus->refcnt, 1);
 +	}
 +
 +	return cpus;
 +}
 +
 +struct cpu_map *cpu_map__read(FILE *file)
 +{
 +	struct cpu_map *cpus = NULL;
 +	int nr_cpus = 0;
 +	int *tmp_cpus = NULL, *tmp;
 +	int max_entries = 0;
 +	int n, cpu, prev;
 +	char sep;
 +
 +	sep = 0;
 +	prev = -1;
 +	for (;;) {
 +		n = fscanf(file, "%u%c", &cpu, &sep);
 +		if (n <= 0)
 +			break;
 +		if (prev >= 0) {
 +			int new_max = nr_cpus + cpu - prev - 1;
 +
 +			WARN_ONCE(new_max >= MAX_NR_CPUS, "Perf can support %d CPUs. "
 +							  "Consider raising MAX_NR_CPUS\n", MAX_NR_CPUS);
 +
 +			if (new_max >= max_entries) {
 +				max_entries = new_max + MAX_NR_CPUS / 2;
 +				tmp = realloc(tmp_cpus, max_entries * sizeof(int));
 +				if (tmp == NULL)
 +					goto out_free_tmp;
 +				tmp_cpus = tmp;
 +			}
 +
 +			while (++prev < cpu)
 +				tmp_cpus[nr_cpus++] = prev;
 +		}
 +		if (nr_cpus == max_entries) {
 +			max_entries += MAX_NR_CPUS;
 +			tmp = realloc(tmp_cpus, max_entries * sizeof(int));
 +			if (tmp == NULL)
 +				goto out_free_tmp;
 +			tmp_cpus = tmp;
 +		}
 +
 +		tmp_cpus[nr_cpus++] = cpu;
 +		if (n == 2 && sep == '-')
 +			prev = cpu;
 +		else
 +			prev = -1;
 +		if (n == 1 || sep == '\n')
 +			break;
 +	}
 +
 +	if (nr_cpus > 0)
 +		cpus = cpu_map__trim_new(nr_cpus, tmp_cpus);
 +	else
 +		cpus = cpu_map__default_new();
 +out_free_tmp:
 +	free(tmp_cpus);
 +	return cpus;
 +}
 +
 +static struct cpu_map *cpu_map__read_all_cpu_map(void)
 +{
 +	struct cpu_map *cpus = NULL;
 +	FILE *onlnf;
 +
 +	onlnf = fopen("/sys/devices/system/cpu/online", "r");
 +	if (!onlnf)
 +		return cpu_map__default_new();
 +
 +	cpus = cpu_map__read(onlnf);
 +	fclose(onlnf);
 +	return cpus;
 +}
 +
 +struct cpu_map *cpu_map__new(const char *cpu_list)
 +{
 +	struct cpu_map *cpus = NULL;
 +	unsigned long start_cpu, end_cpu = 0;
 +	char *p = NULL;
 +	int i, nr_cpus = 0;
 +	int *tmp_cpus = NULL, *tmp;
 +	int max_entries = 0;
 +
 +	if (!cpu_list)
 +		return cpu_map__read_all_cpu_map();
 +
 +	/*
 +	 * must handle the case of empty cpumap to cover
 +	 * TOPOLOGY header for NUMA nodes with no CPU
 +	 * ( e.g., because of CPU hotplug)
 +	 */
 +	if (!isdigit(*cpu_list) && *cpu_list != '\0')
 +		goto out;
 +
 +	while (isdigit(*cpu_list)) {
 +		p = NULL;
 +		start_cpu = strtoul(cpu_list, &p, 0);
 +		if (start_cpu >= INT_MAX
 +		    || (*p != '\0' && *p != ',' && *p != '-'))
 +			goto invalid;
 +
 +		if (*p == '-') {
 +			cpu_list = ++p;
 +			p = NULL;
 +			end_cpu = strtoul(cpu_list, &p, 0);
 +
 +			if (end_cpu >= INT_MAX || (*p != '\0' && *p != ','))
 +				goto invalid;
 +
 +			if (end_cpu < start_cpu)
 +				goto invalid;
 +		} else {
 +			end_cpu = start_cpu;
 +		}
 +
 +		WARN_ONCE(end_cpu >= MAX_NR_CPUS, "Perf can support %d CPUs. "
 +						  "Consider raising MAX_NR_CPUS\n", MAX_NR_CPUS);
 +
 +		for (; start_cpu <= end_cpu; start_cpu++) {
 +			/* check for duplicates */
 +			for (i = 0; i < nr_cpus; i++)
 +				if (tmp_cpus[i] == (int)start_cpu)
 +					goto invalid;
 +
 +			if (nr_cpus == max_entries) {
 +				max_entries += MAX_NR_CPUS;
 +				tmp = realloc(tmp_cpus, max_entries * sizeof(int));
 +				if (tmp == NULL)
 +					goto invalid;
 +				tmp_cpus = tmp;
 +			}
 +			tmp_cpus[nr_cpus++] = (int)start_cpu;
 +		}
 +		if (*p)
 +			++p;
 +
 +		cpu_list = p;
 +	}
 +
 +	if (nr_cpus > 0)
 +		cpus = cpu_map__trim_new(nr_cpus, tmp_cpus);
 +	else if (*cpu_list != '\0')
 +		cpus = cpu_map__default_new();
 +	else
 +		cpus = cpu_map__dummy_new();
 +invalid:
 +	free(tmp_cpus);
 +out:
 +	return cpus;
 +}
 +
 +static struct cpu_map *cpu_map__from_entries(struct cpu_map_entries *cpus)
 +{
 +	struct cpu_map *map;
  
- 	map = cpu_map__empty_new(cpus->nr);
+ 	map = perf_cpu_map__empty_new(cpus->nr);
  	if (map) {
  		unsigned i;
  
@@@ -262,22 -77,9 +262,26 @@@ size_t cpu_map__fprintf(struct cpu_map 
  #undef BUFSIZE
  }
  
++<<<<<<< HEAD
 +struct cpu_map *cpu_map__dummy_new(void)
++=======
+ struct perf_cpu_map *perf_cpu_map__empty_new(int nr)
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  {
 -	struct perf_cpu_map *cpus = malloc(sizeof(*cpus) + sizeof(int) * nr);
 +	struct cpu_map *cpus = malloc(sizeof(*cpus) + sizeof(int));
 +
 +	if (cpus != NULL) {
 +		cpus->nr = 1;
 +		cpus->map[0] = -1;
 +		refcount_set(&cpus->refcnt, 1);
 +	}
 +
 +	return cpus;
 +}
 +
 +struct cpu_map *cpu_map__empty_new(int nr)
 +{
 +	struct cpu_map *cpus = malloc(sizeof(*cpus) + sizeof(int) * nr);
  
  	if (cpus != NULL) {
  		int i;
diff --cc tools/perf/util/cpumap.h
index 1265f0e33920,3e068090612f..000000000000
--- a/tools/perf/util/cpumap.h
+++ b/tools/perf/util/cpumap.h
@@@ -9,35 -11,23 +9,53 @@@
  #include "perf.h"
  #include "util/debug.h"
  
++<<<<<<< HEAD
 +struct cpu_map {
 +	refcount_t refcnt;
 +	int nr;
 +	int map[];
 +};
++=======
+ struct perf_cpu_map *perf_cpu_map__empty_new(int nr);
+ struct perf_cpu_map *cpu_map__new_data(struct cpu_map_data *data);
+ size_t cpu_map__snprint(struct perf_cpu_map *map, char *buf, size_t size);
+ size_t cpu_map__snprint_mask(struct perf_cpu_map *map, char *buf, size_t size);
+ size_t cpu_map__fprintf(struct perf_cpu_map *map, FILE *fp);
+ int cpu_map__get_socket_id(int cpu);
+ int cpu_map__get_socket(struct perf_cpu_map *map, int idx, void *data);
+ int cpu_map__get_die_id(int cpu);
+ int cpu_map__get_die(struct perf_cpu_map *map, int idx, void *data);
+ int cpu_map__get_core_id(int cpu);
+ int cpu_map__get_core(struct perf_cpu_map *map, int idx, void *data);
+ int cpu_map__build_socket_map(struct perf_cpu_map *cpus, struct perf_cpu_map **sockp);
+ int cpu_map__build_die_map(struct perf_cpu_map *cpus, struct perf_cpu_map **diep);
+ int cpu_map__build_core_map(struct perf_cpu_map *cpus, struct perf_cpu_map **corep);
+ const struct perf_cpu_map *cpu_map__online(void); /* thread unsafe */
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  
 -static inline int cpu_map__socket(struct perf_cpu_map *sock, int s)
 +struct cpu_map *cpu_map__new(const char *cpu_list);
 +struct cpu_map *cpu_map__empty_new(int nr);
 +struct cpu_map *cpu_map__dummy_new(void);
 +struct cpu_map *cpu_map__new_data(struct cpu_map_data *data);
 +struct cpu_map *cpu_map__read(FILE *file);
 +size_t cpu_map__snprint(struct cpu_map *map, char *buf, size_t size);
 +size_t cpu_map__snprint_mask(struct cpu_map *map, char *buf, size_t size);
 +size_t cpu_map__fprintf(struct cpu_map *map, FILE *fp);
 +int cpu_map__get_socket_id(int cpu);
 +int cpu_map__get_socket(struct cpu_map *map, int idx, void *data);
 +int cpu_map__get_die_id(int cpu);
 +int cpu_map__get_die(struct cpu_map *map, int idx, void *data);
 +int cpu_map__get_core_id(int cpu);
 +int cpu_map__get_core(struct cpu_map *map, int idx, void *data);
 +int cpu_map__build_socket_map(struct cpu_map *cpus, struct cpu_map **sockp);
 +int cpu_map__build_die_map(struct cpu_map *cpus, struct cpu_map **diep);
 +int cpu_map__build_core_map(struct cpu_map *cpus, struct cpu_map **corep);
 +const struct cpu_map *cpu_map__online(void); /* thread unsafe */
 +
 +struct cpu_map *cpu_map__get(struct cpu_map *map);
 +void cpu_map__put(struct cpu_map *map);
 +
 +static inline int cpu_map__socket(struct cpu_map *sock, int s)
  {
  	if (!sock || s > sock->nr || s < 0)
  		return 0;
@@@ -59,16 -49,6 +77,19 @@@ static inline int cpu_map__id_to_cpu(in
  	return id & 0xffff;
  }
  
++<<<<<<< HEAD
 +static inline int cpu_map__nr(const struct cpu_map *map)
 +{
 +	return map ? map->nr : 1;
 +}
 +
 +static inline bool cpu_map__empty(const struct cpu_map *map)
 +{
 +	return map ? map->map[0] == -1 : true;
 +}
 +
++=======
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  int cpu__setup_cpunode_map(void);
  
  int cpu__max_node(void);
diff --cc tools/perf/util/evlist.c
index 4739b7914374,ba49b5ecffd0..000000000000
--- a/tools/perf/util/evlist.c
+++ b/tools/perf/util/evlist.c
@@@ -410,10 -383,10 +410,14 @@@ static int perf_evlist__enable_event_th
  	return 0;
  }
  
 -int perf_evlist__enable_event_idx(struct evlist *evlist,
 -				  struct evsel *evsel, int idx)
 +int perf_evlist__enable_event_idx(struct perf_evlist *evlist,
 +				  struct perf_evsel *evsel, int idx)
  {
++<<<<<<< HEAD
 +	bool per_cpu_mmaps = !cpu_map__empty(evlist->cpus);
++=======
+ 	bool per_cpu_mmaps = !perf_cpu_map__empty(evlist->core.cpus);
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  
  	if (per_cpu_mmaps)
  		return perf_evlist__enable_event_cpu(evlist, evsel, idx);
@@@ -719,9 -692,9 +723,15 @@@ static struct perf_mmap *perf_evlist__a
  	int i;
  	struct perf_mmap *map;
  
++<<<<<<< HEAD
 +	evlist->nr_mmaps = cpu_map__nr(evlist->cpus);
 +	if (cpu_map__empty(evlist->cpus))
 +		evlist->nr_mmaps = thread_map__nr(evlist->threads);
++=======
+ 	evlist->nr_mmaps = perf_cpu_map__nr(evlist->core.cpus);
+ 	if (perf_cpu_map__empty(evlist->core.cpus))
+ 		evlist->nr_mmaps = thread_map__nr(evlist->core.threads);
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  	map = zalloc(evlist->nr_mmaps * sizeof(struct perf_mmap));
  	if (!map)
  		return NULL;
diff --cc tools/perf/util/record.c
index 9cfc7bf16531,51bbd0714e6d..000000000000
--- a/tools/perf/util/record.c
+++ b/tools/perf/util/record.c
@@@ -274,13 -275,13 +274,18 @@@ bool perf_evlist__can_select_event(stru
  
  	evsel = perf_evlist__last(temp_evlist);
  
++<<<<<<< HEAD
 +	if (!evlist || cpu_map__empty(evlist->cpus)) {
 +		struct cpu_map *cpus = cpu_map__new(NULL);
++=======
+ 	if (!evlist || perf_cpu_map__empty(evlist->core.cpus)) {
+ 		struct perf_cpu_map *cpus = perf_cpu_map__new(NULL);
++>>>>>>> 315c0a1f0ccd (libperf: Move perf's cpu_map__empty() to perf_cpu_map__empty())
  
  		cpu =  cpus ? cpus->map[0] : 0;
 -		perf_cpu_map__put(cpus);
 +		cpu_map__put(cpus);
  	} else {
 -		cpu = evlist->core.cpus->map[0];
 +		cpu = evlist->cpus->map[0];
  	}
  
  	while (1) {
* Unmerged path tools/perf/lib/cpumap.c
* Unmerged path tools/perf/lib/include/perf/cpumap.h
diff --git a/tools/perf/arch/arm/util/cs-etm.c b/tools/perf/arch/arm/util/cs-etm.c
index 4208974c24f8..611f433c06a9 100644
--- a/tools/perf/arch/arm/util/cs-etm.c
+++ b/tools/perf/arch/arm/util/cs-etm.c
@@ -396,7 +396,7 @@ static int cs_etm_recording_options(struct auxtrace_record *itr,
 	 * AUX event.  We also need the contextID in order to be notified
 	 * when a context switch happened.
 	 */
-	if (!cpu_map__empty(cpus)) {
+	if (!perf_cpu_map__empty(cpus)) {
 		perf_evsel__set_sample_bit(cs_etm_evsel, CPU);
 
 		err = cs_etm_set_option(itr, cs_etm_evsel,
@@ -420,7 +420,7 @@ static int cs_etm_recording_options(struct auxtrace_record *itr,
 		tracking_evsel->attr.sample_period = 1;
 
 		/* In per-cpu case, always need the time of mmap events etc */
-		if (!cpu_map__empty(cpus))
+		if (!perf_cpu_map__empty(cpus))
 			perf_evsel__set_sample_bit(tracking_evsel, TIME);
 	}
 
@@ -493,7 +493,7 @@ cs_etm_info_priv_size(struct auxtrace_record *itr __maybe_unused,
 	struct cpu_map *online_cpus = cpu_map__new(NULL);
 
 	/* cpu map is not empty, we have specific CPUs to work with */
-	if (!cpu_map__empty(event_cpus)) {
+	if (!perf_cpu_map__empty(event_cpus)) {
 		for (i = 0; i < cpu__max_cpu(); i++) {
 			if (!cpu_map__has(event_cpus, i) ||
 			    !cpu_map__has(online_cpus, i))
@@ -649,7 +649,7 @@ static int cs_etm_info_fill(struct auxtrace_record *itr,
 		return -EINVAL;
 
 	/* If the cpu_map is empty all online CPUs are involved */
-	if (cpu_map__empty(event_cpus)) {
+	if (perf_cpu_map__empty(event_cpus)) {
 		cpu_map = online_cpus;
 	} else {
 		/* Make sure all specified CPUs are online */
diff --git a/tools/perf/arch/x86/util/intel-bts.c b/tools/perf/arch/x86/util/intel-bts.c
index 4cbd3d775c19..b9eba95bf36c 100644
--- a/tools/perf/arch/x86/util/intel-bts.c
+++ b/tools/perf/arch/x86/util/intel-bts.c
@@ -142,7 +142,7 @@ static int intel_bts_recording_options(struct auxtrace_record *itr,
 	if (!opts->full_auxtrace)
 		return 0;
 
-	if (opts->full_auxtrace && !cpu_map__empty(cpus)) {
+	if (opts->full_auxtrace && !perf_cpu_map__empty(cpus)) {
 		pr_err(INTEL_BTS_PMU_NAME " does not support per-cpu recording\n");
 		return -EINVAL;
 	}
@@ -223,7 +223,7 @@ static int intel_bts_recording_options(struct auxtrace_record *itr,
 		 * In the case of per-cpu mmaps, we need the CPU on the
 		 * AUX event.
 		 */
-		if (!cpu_map__empty(cpus))
+		if (!perf_cpu_map__empty(cpus))
 			perf_evsel__set_sample_bit(intel_bts_evsel, CPU);
 	}
 
* Unmerged path tools/perf/arch/x86/util/intel-pt.c
diff --git a/tools/perf/builtin-c2c.c b/tools/perf/builtin-c2c.c
index 5744c6803d23..f39b53a83a31 100644
--- a/tools/perf/builtin-c2c.c
+++ b/tools/perf/builtin-c2c.c
@@ -2060,7 +2060,7 @@ static int setup_nodes(struct perf_session *session)
 		nodes[node] = set;
 
 		/* empty node, skip */
-		if (cpu_map__empty(map))
+		if (perf_cpu_map__empty(map))
 			continue;
 
 		for (cpu = 0; cpu < map->nr; cpu++) {
* Unmerged path tools/perf/builtin-stat.c
* Unmerged path tools/perf/lib/cpumap.c
* Unmerged path tools/perf/lib/include/perf/cpumap.h
* Unmerged path tools/perf/lib/libperf.map
* Unmerged path tools/perf/util/cpumap.c
* Unmerged path tools/perf/util/cpumap.h
diff --git a/tools/perf/util/event.c b/tools/perf/util/event.c
index f1f4848947ce..76623ba21a4a 100644
--- a/tools/perf/util/event.c
+++ b/tools/perf/util/event.c
@@ -1055,7 +1055,7 @@ static size_t mask_size(struct cpu_map *map, int *max)
 void *cpu_map_data__alloc(struct cpu_map *map, size_t *size, u16 *type, int *max)
 {
 	size_t size_cpus, size_mask;
-	bool is_dummy = cpu_map__empty(map);
+	bool is_dummy = perf_cpu_map__empty(map);
 
 	/*
 	 * Both array and mask data have variable size based
* Unmerged path tools/perf/util/evlist.c
* Unmerged path tools/perf/util/record.c
diff --git a/tools/perf/util/stat.c b/tools/perf/util/stat.c
index da269d46c09f..c5311372f052 100644
--- a/tools/perf/util/stat.c
+++ b/tools/perf/util/stat.c
@@ -223,7 +223,7 @@ static int check_per_pkg(struct perf_evsel *counter,
 	if (!counter->per_pkg)
 		return 0;
 
-	if (cpu_map__empty(cpus))
+	if (perf_cpu_map__empty(cpus))
 		return 0;
 
 	if (!mask) {
