io_uring: used cached copies of sq->dropped and cq->overflow

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Jens Axboe <axboe@kernel.dk>
commit 498ccd9eda49117c34e0041563d0da6ac40e52b8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/498ccd9e.failed

We currently use the ring values directly, but that can lead to issues
if the application is malicious and changes these values on our behalf.
Created in-kernel cached versions of them, and just overwrite the user
side when we update them. This is similar to how we treat the sq/cq
ring tail/head updates.

	Reported-by: Pavel Begunkov <asml.silence@gmail.com>
	Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 498ccd9eda49117c34e0041563d0da6ac40e52b8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 4704cbeb632c,292c4c733cbe..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -234,9 -212,8 +235,10 @@@ struct io_ring_ctx 
  	struct completion	sqo_thread_started;
  
  	struct {
 +		/* CQ ring */
 +		struct io_cq_ring	*cq_ring;
  		unsigned		cached_cq_tail;
+ 		atomic_t		cached_cq_overflow;
  		unsigned		cq_entries;
  		unsigned		cq_mask;
  		struct wait_queue_head	cq_wait;
@@@ -427,6 -418,14 +429,16 @@@ static struct io_ring_ctx *io_ring_ctx_
  	return ctx;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool __io_sequence_defer(struct io_ring_ctx *ctx,
+ 				       struct io_kiocb *req)
+ {
+ 	return req->sequence != ctx->cached_cq_tail + ctx->cached_sq_dropped
+ 					+ atomic_read(&ctx->cached_cq_overflow);
+ }
+ 
++>>>>>>> 498ccd9eda49 (io_uring: used cached copies of sq->dropped and cq->overflow)
  static inline bool io_sequence_defer(struct io_ring_ctx *ctx,
  				     struct io_kiocb *req)
  {
@@@ -535,9 -570,8 +547,14 @@@ static void io_cqring_fill_event(struc
  		WRITE_ONCE(cqe->res, res);
  		WRITE_ONCE(cqe->flags, 0);
  	} else {
++<<<<<<< HEAD
 +		unsigned overflow = READ_ONCE(ctx->cq_ring->overflow);
 +
 +		WRITE_ONCE(ctx->cq_ring->overflow, overflow + 1);
++=======
+ 		WRITE_ONCE(ctx->rings->cq_overflow,
+ 				atomic_inc_return(&ctx->cached_cq_overflow));
++>>>>>>> 498ccd9eda49 (io_uring: used cached copies of sq->dropped and cq->overflow)
  	}
  }
  
@@@ -2396,7 -2566,8 +2413,12 @@@ static bool io_get_sqring(struct io_rin
  
  	/* drop invalid entries */
  	ctx->cached_sq_head++;
++<<<<<<< HEAD
 +	ring->dropped++;
++=======
+ 	ctx->cached_sq_dropped++;
+ 	WRITE_ONCE(rings->sq_dropped, ctx->cached_sq_dropped);
++>>>>>>> 498ccd9eda49 (io_uring: used cached copies of sq->dropped and cq->overflow)
  	return false;
  }
  
* Unmerged path fs/io_uring.c
