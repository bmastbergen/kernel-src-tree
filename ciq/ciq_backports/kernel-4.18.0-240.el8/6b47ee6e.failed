io_uring: optimise sqe-to-req flags translation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 6b47ee6ecab142f938a40bf3b297abac74218ee2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/6b47ee6e.failed

For each IOSQE_* flag there is a corresponding REQ_F_* flag. And there
is a repetitive pattern of their translation:
e.g. if (sqe->flags & SQE_FLAG*) req->flags |= REQ_F_FLAG*

Use same numeric values/bits for them and copy instead of manual
handling.

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 6b47ee6ecab142f938a40bf3b297abac74218ee2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
#	include/uapi/linux/io_uring.h
diff --cc fs/io_uring.c
index 28a601d08266,cf5bad51f752..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -308,6 -325,193 +309,196 @@@ struct io_poll_iocb 
  	struct wait_queue_entry		wait;
  };
  
++<<<<<<< HEAD
++=======
+ struct io_close {
+ 	struct file			*file;
+ 	struct file			*put_file;
+ 	int				fd;
+ };
+ 
+ struct io_timeout_data {
+ 	struct io_kiocb			*req;
+ 	struct hrtimer			timer;
+ 	struct timespec64		ts;
+ 	enum hrtimer_mode		mode;
+ 	u32				seq_offset;
+ };
+ 
+ struct io_accept {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int __user			*addr_len;
+ 	int				flags;
+ };
+ 
+ struct io_sync {
+ 	struct file			*file;
+ 	loff_t				len;
+ 	loff_t				off;
+ 	int				flags;
+ 	int				mode;
+ };
+ 
+ struct io_cancel {
+ 	struct file			*file;
+ 	u64				addr;
+ };
+ 
+ struct io_timeout {
+ 	struct file			*file;
+ 	u64				addr;
+ 	int				flags;
+ 	unsigned			count;
+ };
+ 
+ struct io_rw {
+ 	/* NOTE: kiocb has the file as the first member, so don't do it here */
+ 	struct kiocb			kiocb;
+ 	u64				addr;
+ 	u64				len;
+ };
+ 
+ struct io_connect {
+ 	struct file			*file;
+ 	struct sockaddr __user		*addr;
+ 	int				addr_len;
+ };
+ 
+ struct io_sr_msg {
+ 	struct file			*file;
+ 	union {
+ 		struct user_msghdr __user *msg;
+ 		void __user		*buf;
+ 	};
+ 	int				msg_flags;
+ 	size_t				len;
+ };
+ 
+ struct io_open {
+ 	struct file			*file;
+ 	int				dfd;
+ 	union {
+ 		unsigned		mask;
+ 	};
+ 	struct filename			*filename;
+ 	struct statx __user		*buffer;
+ 	struct open_how			how;
+ };
+ 
+ struct io_files_update {
+ 	struct file			*file;
+ 	u64				arg;
+ 	u32				nr_args;
+ 	u32				offset;
+ };
+ 
+ struct io_fadvise {
+ 	struct file			*file;
+ 	u64				offset;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_madvise {
+ 	struct file			*file;
+ 	u64				addr;
+ 	u32				len;
+ 	u32				advice;
+ };
+ 
+ struct io_async_connect {
+ 	struct sockaddr_storage		address;
+ };
+ 
+ struct io_async_msghdr {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	struct sockaddr __user		*uaddr;
+ 	struct msghdr			msg;
+ };
+ 
+ struct io_async_rw {
+ 	struct iovec			fast_iov[UIO_FASTIOV];
+ 	struct iovec			*iov;
+ 	ssize_t				nr_segs;
+ 	ssize_t				size;
+ };
+ 
+ struct io_async_open {
+ 	struct filename			*filename;
+ };
+ 
+ struct io_async_ctx {
+ 	union {
+ 		struct io_async_rw	rw;
+ 		struct io_async_msghdr	msg;
+ 		struct io_async_connect	connect;
+ 		struct io_timeout_data	timeout;
+ 		struct io_async_open	open;
+ 	};
+ };
+ 
+ enum {
+ 	REQ_F_FIXED_FILE_BIT	= IOSQE_FIXED_FILE_BIT,
+ 	REQ_F_IO_DRAIN_BIT	= IOSQE_IO_DRAIN_BIT,
+ 	REQ_F_LINK_BIT		= IOSQE_IO_LINK_BIT,
+ 	REQ_F_HARDLINK_BIT	= IOSQE_IO_HARDLINK_BIT,
+ 	REQ_F_FORCE_ASYNC_BIT	= IOSQE_ASYNC_BIT,
+ 
+ 	REQ_F_LINK_NEXT_BIT,
+ 	REQ_F_FAIL_LINK_BIT,
+ 	REQ_F_INFLIGHT_BIT,
+ 	REQ_F_CUR_POS_BIT,
+ 	REQ_F_NOWAIT_BIT,
+ 	REQ_F_IOPOLL_COMPLETED_BIT,
+ 	REQ_F_LINK_TIMEOUT_BIT,
+ 	REQ_F_TIMEOUT_BIT,
+ 	REQ_F_ISREG_BIT,
+ 	REQ_F_MUST_PUNT_BIT,
+ 	REQ_F_TIMEOUT_NOSEQ_BIT,
+ 	REQ_F_COMP_LOCKED_BIT,
+ };
+ 
+ enum {
+ 	/* ctx owns file */
+ 	REQ_F_FIXED_FILE	= BIT(REQ_F_FIXED_FILE_BIT),
+ 	/* drain existing IO first */
+ 	REQ_F_IO_DRAIN		= BIT(REQ_F_IO_DRAIN_BIT),
+ 	/* linked sqes */
+ 	REQ_F_LINK		= BIT(REQ_F_LINK_BIT),
+ 	/* doesn't sever on completion < 0 */
+ 	REQ_F_HARDLINK		= BIT(REQ_F_HARDLINK_BIT),
+ 	/* IOSQE_ASYNC */
+ 	REQ_F_FORCE_ASYNC	= BIT(REQ_F_FORCE_ASYNC_BIT),
+ 
+ 	/* already grabbed next link */
+ 	REQ_F_LINK_NEXT		= BIT(REQ_F_LINK_NEXT_BIT),
+ 	/* fail rest of links */
+ 	REQ_F_FAIL_LINK		= BIT(REQ_F_FAIL_LINK_BIT),
+ 	/* on inflight list */
+ 	REQ_F_INFLIGHT		= BIT(REQ_F_INFLIGHT_BIT),
+ 	/* read/write uses file position */
+ 	REQ_F_CUR_POS		= BIT(REQ_F_CUR_POS_BIT),
+ 	/* must not punt to workers */
+ 	REQ_F_NOWAIT		= BIT(REQ_F_NOWAIT_BIT),
+ 	/* polled IO has completed */
+ 	REQ_F_IOPOLL_COMPLETED	= BIT(REQ_F_IOPOLL_COMPLETED_BIT),
+ 	/* has linked timeout */
+ 	REQ_F_LINK_TIMEOUT	= BIT(REQ_F_LINK_TIMEOUT_BIT),
+ 	/* timeout request */
+ 	REQ_F_TIMEOUT		= BIT(REQ_F_TIMEOUT_BIT),
+ 	/* regular file */
+ 	REQ_F_ISREG		= BIT(REQ_F_ISREG_BIT),
+ 	/* must be punted even for NONBLOCK */
+ 	REQ_F_MUST_PUNT		= BIT(REQ_F_MUST_PUNT_BIT),
+ 	/* no timeout sequence */
+ 	REQ_F_TIMEOUT_NOSEQ	= BIT(REQ_F_TIMEOUT_NOSEQ_BIT),
+ 	/* completion under lock */
+ 	REQ_F_COMP_LOCKED	= BIT(REQ_F_COMP_LOCKED_BIT),
+ };
+ 
++>>>>>>> 6b47ee6ecab1 (io_uring: optimise sqe-to-req flags translation)
  /*
   * NOTE! Each of the iocb union members has the file pointer
   * as the first entry in their struct definition. So you can
@@@ -328,15 -554,6 +519,18 @@@ struct io_kiocb 
  	struct list_head	link_list;
  	unsigned int		flags;
  	refcount_t		refs;
++<<<<<<< HEAD
 +#define REQ_F_NOWAIT		1	/* must not punt to workers */
 +#define REQ_F_IOPOLL_COMPLETED	2	/* polled IO has completed */
 +#define REQ_F_FIXED_FILE	4	/* ctx owns file */
 +#define REQ_F_IO_DRAIN		16	/* drain existing IO first */
 +#define REQ_F_IO_DRAINED	32	/* drain done */
 +#define REQ_F_LINK		64	/* linked sqes */
 +#define REQ_F_LINK_DONE		128	/* linked sqes done */
 +#define REQ_F_FAIL_LINK		256	/* fail rest of links */
 +#define REQ_F_SHADOW_DRAIN	512	/* link-drain shadow req */
++=======
++>>>>>>> 6b47ee6ecab1 (io_uring: optimise sqe-to-req flags translation)
  	u64			user_data;
  	u32			result;
  	u32			sequence;
@@@ -2143,19 -4394,11 +2337,23 @@@ static int io_req_set_file(struct io_ri
  	unsigned flags;
  	int fd;
  
 -	flags = READ_ONCE(sqe->flags);
 -	fd = READ_ONCE(sqe->fd);
 +	flags = READ_ONCE(s->sqe->flags);
 +	fd = READ_ONCE(s->sqe->fd);
 +
++<<<<<<< HEAD
 +	if (flags & IOSQE_IO_DRAIN)
 +		req->flags |= REQ_F_IO_DRAIN;
 +	/*
 +	 * All io need record the previous position, if LINK vs DARIN,
 +	 * it can be used to mark the position of the first IO in the
 +	 * link list.
 +	 */
 +	req->sequence = s->sequence;
  
 +	if (!io_op_needs_file(s->sqe))
++=======
+ 	if (!io_req_needs_file(req, fd))
++>>>>>>> 6b47ee6ecab1 (io_uring: optimise sqe-to-req flags translation)
  		return 0;
  
  	if (flags & IOSQE_FIXED_FILE) {
@@@ -2211,111 -4509,142 +2409,117 @@@ static int __io_queue_sqe(struct io_rin
  
  	/* drop submission reference */
  	io_put_req(req);
 -}
 -
 -static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
 -{
 -	struct io_kiocb *nxt;
 -
 -	if (!(req->flags & REQ_F_LINK))
 -		return NULL;
  
 -	nxt = list_first_entry_or_null(&req->link_list, struct io_kiocb,
 -					link_list);
 -	if (!nxt || nxt->opcode != IORING_OP_LINK_TIMEOUT)
 -		return NULL;
 +	/* and drop final reference, if we failed */
 +	if (ret) {
 +		io_cqring_add_event(ctx, req->user_data, ret);
 +		if (req->flags & REQ_F_LINK)
 +			req->flags |= REQ_F_FAIL_LINK;
 +		io_put_req(req);
 +	}
  
 -	req->flags |= REQ_F_LINK_TIMEOUT;
 -	return nxt;
 +	return ret;
  }
  
 -static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_queue_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			struct sqe_submit *s, bool force_nonblock)
  {
 -	struct io_kiocb *linked_timeout;
 -	struct io_kiocb *nxt = NULL;
  	int ret;
  
 -again:
 -	linked_timeout = io_prep_linked_timeout(req);
 -
 -	ret = io_issue_sqe(req, sqe, &nxt, true);
 -
 -	/*
 -	 * We async punt it if the file wasn't marked NOWAIT, or if the file
 -	 * doesn't support non-blocking read/write attempts
 -	 */
 -	if (ret == -EAGAIN && (!(req->flags & REQ_F_NOWAIT) ||
 -	    (req->flags & REQ_F_MUST_PUNT))) {
 -		if (req->work.flags & IO_WQ_WORK_NEEDS_FILES) {
 -			ret = io_grab_files(req);
 -			if (ret)
 -				goto err;
 -		}
 -
 -		/*
 -		 * Queued up for async execution, worker will release
 -		 * submit reference when the iocb is actually submitted.
 -		 */
 -		io_queue_async_work(req);
 -		goto done_req;
 -	}
 -
 -err:
 -	/* drop submission reference */
 -	io_put_req(req);
 -
 -	if (linked_timeout) {
 -		if (!ret)
 -			io_queue_linked_timeout(linked_timeout);
 -		else
 -			io_put_req(linked_timeout);
 -	}
 -
 -	/* and drop final reference, if we failed */
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
 -		io_cqring_add_event(req, ret);
 -		req_set_fail_links(req);
 -		io_put_req(req);
 -	}
 -done_req:
 -	if (nxt) {
 -		req = nxt;
 -		nxt = NULL;
 -		goto again;
 +		if (ret != -EIOCBQUEUED) {
 +			io_free_req(req);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		}
 +		return 0;
  	}
 +
 +	return __io_queue_sqe(ctx, req, s, force_nonblock);
  }
  
 -static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 +static int io_queue_link_head(struct io_ring_ctx *ctx, struct io_kiocb *req,
 +			      struct sqe_submit *s, struct io_kiocb *shadow,
 +			      bool force_nonblock)
  {
  	int ret;
 +	int need_submit = false;
  
 -	ret = io_req_defer(req, sqe);
 +	if (!shadow)
 +		return io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	/*
 +	 * Mark the first IO in link list as DRAIN, let all the following
 +	 * IOs enter the defer list. all IO needs to be completed before link
 +	 * list.
 +	 */
 +	req->flags |= REQ_F_IO_DRAIN;
 +	ret = io_req_defer(ctx, req, s->sqe);
  	if (ret) {
  		if (ret != -EIOCBQUEUED) {
 -			io_cqring_add_event(req, ret);
 -			req_set_fail_links(req);
 -			io_double_put_req(req);
 +			io_free_req(req);
 +			__io_free_req(shadow);
 +			io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +			return 0;
  		}
 -	} else if (req->flags & REQ_F_FORCE_ASYNC) {
 +	} else {
  		/*
 -		 * Never try inline submit of IOSQE_ASYNC is set, go straight
 -		 * to async execution.
 +		 * If ret == 0 means that all IOs in front of link io are
 +		 * running done. let's queue link head.
  		 */
 -		req->work.flags |= IO_WQ_WORK_CONCURRENT;
 -		io_queue_async_work(req);
 -	} else {
 -		__io_queue_sqe(req, sqe);
 +		need_submit = true;
  	}
 -}
  
 -static inline void io_queue_link_head(struct io_kiocb *req)
 -{
 -	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
 -		io_cqring_add_event(req, -ECANCELED);
 -		io_double_put_req(req);
 -	} else
 -		io_queue_sqe(req, NULL);
 +	/* Insert shadow req to defer_list, blocking next IOs */
 +	spin_lock_irq(&ctx->completion_lock);
 +	list_add_tail(&shadow->list, &ctx->defer_list);
 +	spin_unlock_irq(&ctx->completion_lock);
 +
 +	if (need_submit)
 +		return __io_queue_sqe(ctx, req, s, force_nonblock);
 +
 +	return 0;
  }
  
 -#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|	\
 -				IOSQE_IO_HARDLINK | IOSQE_ASYNC)
 +#define SQE_VALID_FLAGS	(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK)
  
 -static bool io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 -			  struct io_submit_state *state, struct io_kiocb **link)
 +static void io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 +			  struct io_submit_state *state, struct io_kiocb **link,
 +			  bool force_nonblock)
  {
 -	struct io_ring_ctx *ctx = req->ctx;
 -	unsigned int sqe_flags;
 +	struct io_uring_sqe *sqe_copy;
 +	struct io_kiocb *req;
  	int ret;
  
 -	sqe_flags = READ_ONCE(sqe->flags);
 -
  	/* enforce forwards compatibility on users */
 -	if (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {
 +	if (unlikely(s->sqe->flags & ~SQE_VALID_FLAGS)) {
  		ret = -EINVAL;
 -		goto err_req;
 +		goto err;
  	}
++<<<<<<< HEAD
++=======
+ 	/* same numerical values with corresponding REQ_F_*, safe to copy */
+ 	req->flags |= sqe_flags & (IOSQE_IO_DRAIN|IOSQE_IO_HARDLINK|
+ 					IOSQE_ASYNC);
++>>>>>>> 6b47ee6ecab1 (io_uring: optimise sqe-to-req flags translation)
 +
 +	req = io_get_req(ctx, state);
 +	if (unlikely(!req)) {
 +		ret = -EAGAIN;
 +		goto err;
 +	}
  
 -	ret = io_req_set_file(state, req, sqe);
 +	ret = io_req_set_file(ctx, s, state, req);
  	if (unlikely(ret)) {
  err_req:
 -		io_cqring_add_event(req, ret);
 -		io_double_put_req(req);
 -		return false;
 +		io_free_req(req);
 +err:
 +		io_cqring_add_event(ctx, s->sqe->user_data, ret);
 +		return;
  	}
  
 +	req->user_data = s->sqe->user_data;
 +
  	/*
  	 * If we already have a head request, queue this one for async
  	 * submittal once the head completes. If we don't have a head but
@@@ -2324,26 -4653,49 +2528,51 @@@
  	 * conditions are true (normal request), then just queue it.
  	 */
  	if (*link) {
 -		struct io_kiocb *head = *link;
 +		struct io_kiocb *prev = *link;
  
++<<<<<<< HEAD
 +		sqe_copy = kmemdup(s->sqe, sizeof(*sqe_copy), GFP_KERNEL);
 +		if (!sqe_copy) {
++=======
+ 		if (sqe_flags & IOSQE_IO_DRAIN) {
+ 			head->flags |= REQ_F_IO_DRAIN;
+ 			ctx->drain_next = 1;
+ 		}
+ 		if (io_alloc_async_ctx(req)) {
++>>>>>>> 6b47ee6ecab1 (io_uring: optimise sqe-to-req flags translation)
  			ret = -EAGAIN;
  			goto err_req;
  		}
  
 -		ret = io_req_defer_prep(req, sqe);
 -		if (ret) {
 -			/* fail even hard links since we don't submit */
 -			head->flags |= REQ_F_FAIL_LINK;
 -			goto err_req;
 -		}
 -		trace_io_uring_link(ctx, req, head);
 -		list_add_tail(&req->link_list, &head->link_list);
 +		s->sqe = sqe_copy;
 +		memcpy(&req->submit, s, sizeof(*s));
 +		list_add_tail(&req->list, &prev->link_list);
 +	} else if (s->sqe->flags & IOSQE_IO_LINK) {
 +		req->flags |= REQ_F_LINK;
  
 -		/* last request of a link, enqueue the link */
 -		if (!(sqe_flags & (IOSQE_IO_LINK|IOSQE_IO_HARDLINK))) {
 -			io_queue_link_head(head);
 -			*link = NULL;
 -		}
 +		memcpy(&req->submit, s, sizeof(*s));
 +		INIT_LIST_HEAD(&req->link_list);
 +		*link = req;
  	} else {
++<<<<<<< HEAD
 +		io_queue_sqe(ctx, req, s, force_nonblock);
++=======
+ 		if (unlikely(ctx->drain_next)) {
+ 			req->flags |= REQ_F_IO_DRAIN;
+ 			req->ctx->drain_next = 0;
+ 		}
+ 		if (sqe_flags & (IOSQE_IO_LINK|IOSQE_IO_HARDLINK)) {
+ 			req->flags |= REQ_F_LINK;
+ 			INIT_LIST_HEAD(&req->link_list);
+ 			ret = io_req_defer_prep(req, sqe);
+ 			if (ret)
+ 				req->flags |= REQ_F_FAIL_LINK;
+ 			*link = req;
+ 		} else {
+ 			io_queue_sqe(req, sqe);
+ 		}
++>>>>>>> 6b47ee6ecab1 (io_uring: optimise sqe-to-req flags translation)
  	}
 -
 -	return true;
  }
  
  /*
diff --cc include/uapi/linux/io_uring.h
index dd4a49ec83b7,57d05cc5e271..000000000000
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@@ -39,9 -56,16 +47,22 @@@ enum 
  /*
   * sqe->flags
   */
++<<<<<<< HEAD
 +#define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
 +#define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
 +#define IOSQE_IO_LINK		(1U << 2)	/* links next sqe */
++=======
+ /* use fixed fileset */
+ #define IOSQE_FIXED_FILE	(1U << IOSQE_FIXED_FILE_BIT)
+ /* issue after inflight IO */
+ #define IOSQE_IO_DRAIN		(1U << IOSQE_IO_DRAIN_BIT)
+ /* links next sqe */
+ #define IOSQE_IO_LINK		(1U << IOSQE_IO_LINK_BIT)
+ /* like LINK, but stronger */
+ #define IOSQE_IO_HARDLINK	(1U << IOSQE_IO_HARDLINK_BIT)
+ /* always go async */
+ #define IOSQE_ASYNC		(1U << IOSQE_ASYNC_BIT)
++>>>>>>> 6b47ee6ecab1 (io_uring: optimise sqe-to-req flags translation)
  
  /*
   * io_uring_setup() flags
* Unmerged path fs/io_uring.c
* Unmerged path include/uapi/linux/io_uring.h
