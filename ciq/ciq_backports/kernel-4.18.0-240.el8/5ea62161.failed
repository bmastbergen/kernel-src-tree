io_uring: don't call work.func from sync ctx

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Pavel Begunkov <asml.silence@gmail.com>
commit 5ea62161167eb8297249d3f4dc63741016f01413
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/5ea62161.failed

Many operations define custom work.func before getting into an io-wq.
There are several points against:
- it calls io_wq_assign_next() from outside io-wq, that may be confusing
- sync context would go unnecessary through io_req_cancelled()
- prototypes are quite different, so work!=old_work looks strange
- makes async/sync responsibilities fuzzy
- adds extra overhead

Don't call generic path and io-wq handlers from each other, but use
helpers instead

	Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 5ea62161167eb8297249d3f4dc63741016f01413)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/io_uring.c
diff --cc fs/io_uring.c
index 7842c6de7135,83ae190a3d31..000000000000
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@@ -1487,24 -2422,212 +1487,227 @@@ static int io_prep_fsync(struct io_kioc
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int io_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 +		    bool force_nonblock)
 +{
 +	loff_t sqe_off = READ_ONCE(sqe->off);
 +	loff_t sqe_len = READ_ONCE(sqe->len);
 +	loff_t end = sqe_off + sqe_len;
 +	unsigned fsync_flags;
 +	int ret;
 +
 +	fsync_flags = READ_ONCE(sqe->fsync_flags);
 +	if (unlikely(fsync_flags & ~IORING_FSYNC_DATASYNC))
 +		return -EINVAL;
 +
 +	ret = io_prep_fsync(req, sqe);
 +	if (ret)
 +		return ret;
 +
 +	/* fsync always requires a blocking context */
++=======
+ static bool io_req_cancelled(struct io_kiocb *req)
+ {
+ 	if (req->work.flags & IO_WQ_WORK_CANCEL) {
+ 		req_set_fail_links(req);
+ 		io_cqring_add_event(req, -ECANCELED);
+ 		io_put_req(req);
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void io_link_work_cb(struct io_wq_work **workptr)
+ {
+ 	struct io_wq_work *work = *workptr;
+ 	struct io_kiocb *link = work->data;
+ 
+ 	io_queue_linked_timeout(link);
+ 	work->func = io_wq_submit_work;
+ }
+ 
+ static void io_wq_assign_next(struct io_wq_work **workptr, struct io_kiocb *nxt)
+ {
+ 	struct io_kiocb *link;
+ 
+ 	io_prep_async_work(nxt, &link);
+ 	*workptr = &nxt->work;
+ 	if (link) {
+ 		nxt->work.flags |= IO_WQ_WORK_CB;
+ 		nxt->work.func = io_link_work_cb;
+ 		nxt->work.data = link;
+ 	}
+ }
+ 
+ static void __io_fsync(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	loff_t end = req->sync.off + req->sync.len;
+ 	int ret;
+ 
+ 	ret = vfs_fsync_range(req->file, req->sync.off,
+ 				end > 0 ? end : LLONG_MAX,
+ 				req->sync.flags & IORING_FSYNC_DATASYNC);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_fsync_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 	__io_fsync(req, &nxt);
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_fsync(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		    bool force_nonblock)
+ {
+ 	/* fsync always requires a blocking context */
+ 	if (force_nonblock) {
+ 		io_put_req(req);
+ 		req->work.func = io_fsync_finish;
+ 		return -EAGAIN;
+ 	}
+ 	__io_fsync(req, nxt);
+ 	return 0;
+ }
+ 
+ static void __io_fallocate(struct io_kiocb *req, struct io_kiocb **nxt)
+ {
+ 	int ret;
+ 
+ 	if (io_req_cancelled(req))
+ 		return;
+ 
+ 	ret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,
+ 				req->sync.len);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
+ }
+ 
+ static void io_fallocate_finish(struct io_wq_work **workptr)
+ {
+ 	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
+ 	struct io_kiocb *nxt = NULL;
+ 
+ 	__io_fallocate(req, &nxt);
+ 	if (nxt)
+ 		io_wq_assign_next(workptr, nxt);
+ }
+ 
+ static int io_fallocate_prep(struct io_kiocb *req,
+ 			     const struct io_uring_sqe *sqe)
+ {
+ 	if (sqe->ioprio || sqe->buf_index || sqe->rw_flags)
+ 		return -EINVAL;
+ 
+ 	req->sync.off = READ_ONCE(sqe->off);
+ 	req->sync.len = READ_ONCE(sqe->addr);
+ 	req->sync.mode = READ_ONCE(sqe->len);
+ 	return 0;
+ }
+ 
+ static int io_fallocate(struct io_kiocb *req, struct io_kiocb **nxt,
+ 			bool force_nonblock)
+ {
+ 	/* fallocate always requiring blocking context */
+ 	if (force_nonblock) {
+ 		io_put_req(req);
+ 		req->work.func = io_fallocate_finish;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	__io_fallocate(req, nxt);
+ 	return 0;
+ }
+ 
+ static int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	const char __user *fname;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	req->open.how.mode = READ_ONCE(sqe->len);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	req->open.how.flags = READ_ONCE(sqe->open_flags);
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
+ {
+ 	struct open_how __user *how;
+ 	const char __user *fname;
+ 	size_t len;
+ 	int ret;
+ 
+ 	if (sqe->ioprio || sqe->buf_index)
+ 		return -EINVAL;
+ 	if (sqe->flags & IOSQE_FIXED_FILE)
+ 		return -EBADF;
+ 	if (req->flags & REQ_F_NEED_CLEANUP)
+ 		return 0;
+ 
+ 	req->open.dfd = READ_ONCE(sqe->fd);
+ 	fname = u64_to_user_ptr(READ_ONCE(sqe->addr));
+ 	how = u64_to_user_ptr(READ_ONCE(sqe->addr2));
+ 	len = READ_ONCE(sqe->len);
+ 
+ 	if (len < OPEN_HOW_SIZE_VER0)
+ 		return -EINVAL;
+ 
+ 	ret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,
+ 					len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (!(req->open.how.flags & O_PATH) && force_o_largefile())
+ 		req->open.how.flags |= O_LARGEFILE;
+ 
+ 	req->open.filename = getname(fname);
+ 	if (IS_ERR(req->open.filename)) {
+ 		ret = PTR_ERR(req->open.filename);
+ 		req->open.filename = NULL;
+ 		return ret;
+ 	}
+ 
+ 	req->flags |= REQ_F_NEED_CLEANUP;
+ 	return 0;
+ }
+ 
+ static int io_openat2(struct io_kiocb *req, struct io_kiocb **nxt,
+ 		      bool force_nonblock)
+ {
+ 	struct open_flags op;
+ 	struct file *file;
+ 	int ret;
+ 
++>>>>>>> 5ea62161167e (io_uring: don't call work.func from sync ctx)
  	if (force_nonblock)
  		return -EAGAIN;
  
@@@ -1519,49 -2657,103 +1722,86 @@@
  	return 0;
  }
  
 -static int io_openat(struct io_kiocb *req, struct io_kiocb **nxt,
 -		     bool force_nonblock)
 -{
 -	req->open.how = build_open_how(req->open.how.flags, req->open.how.mode);
 -	return io_openat2(req, nxt, force_nonblock);
 -}
 -
 -static int io_epoll_ctl_prep(struct io_kiocb *req,
 -			     const struct io_uring_sqe *sqe)
 +static int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)
  {
 -#if defined(CONFIG_EPOLL)
 -	if (sqe->ioprio || sqe->buf_index)
 -		return -EINVAL;
 -
 -	req->epoll.epfd = READ_ONCE(sqe->fd);
 -	req->epoll.op = READ_ONCE(sqe->len);
 -	req->epoll.fd = READ_ONCE(sqe->off);
 +	struct io_ring_ctx *ctx = req->ctx;
 +	int ret = 0;
  
 -	if (ep_op_has_event(req->epoll.op)) {
 -		struct epoll_event __user *ev;
 +	if (!req->file)
 +		return -EBADF;
  
 -		ev = u64_to_user_ptr(READ_ONCE(sqe->addr));
 -		if (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))
 -			return -EFAULT;
 -	}
 +	if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
 +		return -EINVAL;
 +	if (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))
 +		return -EINVAL;
  
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
 +	return ret;
  }
  
 -static int io_epoll_ctl(struct io_kiocb *req, struct io_kiocb **nxt,
 -			bool force_nonblock)
++<<<<<<< HEAD
 +static int io_sync_file_range(struct io_kiocb *req,
 +			      const struct io_uring_sqe *sqe,
 +			      bool force_nonblock)
  {
 -#if defined(CONFIG_EPOLL)
 -	struct io_epoll *ie = &req->epoll;
 +	loff_t sqe_off;
 +	loff_t sqe_len;
 +	unsigned flags;
  	int ret;
  
 -	ret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);
 -	if (force_nonblock && ret == -EAGAIN)
 -		return -EAGAIN;
 +	ret = io_prep_sfr(req, sqe);
 +	if (ret)
 +		return ret;
 +
++=======
++static void __io_sync_file_range(struct io_kiocb *req, struct io_kiocb **nxt)
++{
++	int ret;
+ 
++	ret = sync_file_range(req->file, req->sync.off, req->sync.len,
++				req->sync.flags);
+ 	if (ret < 0)
+ 		req_set_fail_links(req);
+ 	io_cqring_add_event(req, ret);
+ 	io_put_req_find_next(req, nxt);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
+ }
+ 
 -static int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
++
++static void io_sync_file_range_finish(struct io_wq_work **workptr)
+ {
 -#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
 -	if (sqe->ioprio || sqe->buf_index || sqe->off)
 -		return -EINVAL;
++	struct io_kiocb *req = container_of(*workptr, struct io_kiocb, work);
++	struct io_kiocb *nxt = NULL;
+ 
 -	req->madvise.addr = READ_ONCE(sqe->addr);
 -	req->madvise.len = READ_ONCE(sqe->len);
 -	req->madvise.advice = READ_ONCE(sqe->fadvise_advice);
 -	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
++	if (io_req_cancelled(req))
++		return;
++	__io_sync_file_range(req, &nxt);
++	if (nxt)
++		io_wq_assign_next(workptr, nxt);
+ }
+ 
 -static int io_madvise(struct io_kiocb *req, struct io_kiocb **nxt,
 -		      bool force_nonblock)
++static int io_sync_file_range(struct io_kiocb *req, struct io_kiocb **nxt,
++			      bool force_nonblock)
+ {
 -#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)
 -	struct io_madvise *ma = &req->madvise;
 -	int ret;
 -
++>>>>>>> 5ea62161167e (io_uring: don't call work.func from sync ctx)
 +	/* sync_file_range always requires a blocking context */
  	if (force_nonblock)
  		return -EAGAIN;
  
 -	ret = do_madvise(ma->addr, ma->len, ma->advice);
 -	if (ret < 0)
 -		req_set_fail_links(req);
 -	io_cqring_add_event(req, ret);
 -	io_put_req_find_next(req, nxt);
++<<<<<<< HEAD
 +	sqe_off = READ_ONCE(sqe->off);
 +	sqe_len = READ_ONCE(sqe->len);
 +	flags = READ_ONCE(sqe->sync_range_flags);
++=======
++	__io_sync_file_range(req, nxt);
+ 	return 0;
 -#else
 -	return -EOPNOTSUPP;
 -#endif
+ }
++>>>>>>> 5ea62161167e (io_uring: don't call work.func from sync ctx)
  
 -static int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 -{
 -	if (sqe->ioprio || sqe->buf_index || sqe->addr)
 -		return -EINVAL;
 +	ret = sync_file_range(req->rw.ki_filp, sqe_off, sqe_len, flags);
  
 -	req->fadvise.offset = READ_ONCE(sqe->off);
 -	req->fadvise.len = READ_ONCE(sqe->len);
 -	req->fadvise.advice = READ_ONCE(sqe->fadvise_advice);
 +	if (ret < 0 && (req->flags & REQ_F_LINK))
 +		req->flags |= REQ_F_FAIL_LINK;
 +	io_cqring_add_event(req->ctx, sqe->user_data, ret);
 +	io_put_req(req);
  	return 0;
  }
  
* Unmerged path fs/io_uring.c
