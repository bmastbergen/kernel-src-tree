dm: add ->flush() dax operation support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit abebfbe2f7315dd3ec9a0c69596a76e32beb5749
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/abebfbe2.failed

Allow device-mapper to route flush operations to the
per-target implementation. In order for the device stacking to work we
need a dax_dev and a pgoff relative to that device. This gives each
layer of the stack the information it needs to look up the operation
pointer for the next level.

This conceptually allows for an array of mixed device drivers with
varying flush implementations.

	Reviewed-by: Toshi Kani <toshi.kani@hpe.com>
	Reviewed-by: Mike Snitzer <snitzer@redhat.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit abebfbe2f7315dd3ec9a0c69596a76e32beb5749)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/super.c
#	drivers/md/dm-linear.c
#	drivers/md/dm-stripe.c
#	drivers/md/dm.c
#	include/linux/dax.h
#	include/linux/device-mapper.h
diff --cc drivers/dax/super.c
index bc81d3253618,b7729e4d351a..000000000000
--- a/drivers/dax/super.c
+++ b/drivers/dax/super.c
@@@ -180,6 -173,29 +180,32 @@@ long dax_direct_access(struct dax_devic
  }
  EXPORT_SYMBOL_GPL(dax_direct_access);
  
++<<<<<<< HEAD
++=======
+ size_t dax_copy_from_iter(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t bytes, struct iov_iter *i)
+ {
+ 	if (!dax_alive(dax_dev))
+ 		return 0;
+ 
+ 	if (!dax_dev->ops->copy_from_iter)
+ 		return copy_from_iter(addr, bytes, i);
+ 	return dax_dev->ops->copy_from_iter(dax_dev, pgoff, addr, bytes, i);
+ }
+ EXPORT_SYMBOL_GPL(dax_copy_from_iter);
+ 
+ void dax_flush(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t size)
+ {
+ 	if (!dax_alive(dax_dev))
+ 		return;
+ 
+ 	if (dax_dev->ops->flush)
+ 		dax_dev->ops->flush(dax_dev, pgoff, addr, size);
+ }
+ EXPORT_SYMBOL_GPL(dax_flush);
+ 
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  bool dax_alive(struct dax_device *dax_dev)
  {
  	lockdep_assert_held(&dax_srcu);
diff --cc drivers/md/dm-linear.c
index d40e867fe4c7,25e661974319..000000000000
--- a/drivers/md/dm-linear.c
+++ b/drivers/md/dm-linear.c
@@@ -172,6 -159,34 +172,37 @@@ static long linear_dax_direct_access(st
  	return dax_direct_access(dax_dev, pgoff, nr_pages, kaddr, pfn);
  }
  
++<<<<<<< HEAD
++=======
+ static size_t linear_dax_copy_from_iter(struct dm_target *ti, pgoff_t pgoff,
+ 		void *addr, size_t bytes, struct iov_iter *i)
+ {
+ 	struct linear_c *lc = ti->private;
+ 	struct block_device *bdev = lc->dev->bdev;
+ 	struct dax_device *dax_dev = lc->dev->dax_dev;
+ 	sector_t dev_sector, sector = pgoff * PAGE_SECTORS;
+ 
+ 	dev_sector = linear_map_sector(ti, sector);
+ 	if (bdev_dax_pgoff(bdev, dev_sector, ALIGN(bytes, PAGE_SIZE), &pgoff))
+ 		return 0;
+ 	return dax_copy_from_iter(dax_dev, pgoff, addr, bytes, i);
+ }
+ 
+ static void linear_dax_flush(struct dm_target *ti, pgoff_t pgoff, void *addr,
+ 		size_t size)
+ {
+ 	struct linear_c *lc = ti->private;
+ 	struct block_device *bdev = lc->dev->bdev;
+ 	struct dax_device *dax_dev = lc->dev->dax_dev;
+ 	sector_t dev_sector, sector = pgoff * PAGE_SECTORS;
+ 
+ 	dev_sector = linear_map_sector(ti, sector);
+ 	if (bdev_dax_pgoff(bdev, dev_sector, ALIGN(size, PAGE_SIZE), &pgoff))
+ 		return;
+ 	dax_flush(dax_dev, pgoff, addr, size);
+ }
+ 
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  static struct target_type linear_target = {
  	.name   = "linear",
  	.version = {1, 3, 0},
@@@ -181,9 -197,10 +212,14 @@@
  	.map    = linear_map,
  	.status = linear_status,
  	.prepare_ioctl = linear_prepare_ioctl,
 +	.merge  = linear_merge,
  	.iterate_devices = linear_iterate_devices,
  	.direct_access = linear_dax_direct_access,
++<<<<<<< HEAD
++=======
+ 	.dax_copy_from_iter = linear_dax_copy_from_iter,
+ 	.dax_flush = linear_dax_flush,
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  };
  
  int __init dm_linear_init(void)
diff --cc drivers/md/dm-stripe.c
index db56c81ef539,8e73517967b6..000000000000
--- a/drivers/md/dm-stripe.c
+++ b/drivers/md/dm-stripe.c
@@@ -327,6 -332,44 +327,47 @@@ static long stripe_dax_direct_access(st
  	return dax_direct_access(dax_dev, pgoff, nr_pages, kaddr, pfn);
  }
  
++<<<<<<< HEAD
++=======
+ static size_t stripe_dax_copy_from_iter(struct dm_target *ti, pgoff_t pgoff,
+ 		void *addr, size_t bytes, struct iov_iter *i)
+ {
+ 	sector_t dev_sector, sector = pgoff * PAGE_SECTORS;
+ 	struct stripe_c *sc = ti->private;
+ 	struct dax_device *dax_dev;
+ 	struct block_device *bdev;
+ 	uint32_t stripe;
+ 
+ 	stripe_map_sector(sc, sector, &stripe, &dev_sector);
+ 	dev_sector += sc->stripe[stripe].physical_start;
+ 	dax_dev = sc->stripe[stripe].dev->dax_dev;
+ 	bdev = sc->stripe[stripe].dev->bdev;
+ 
+ 	if (bdev_dax_pgoff(bdev, dev_sector, ALIGN(bytes, PAGE_SIZE), &pgoff))
+ 		return 0;
+ 	return dax_copy_from_iter(dax_dev, pgoff, addr, bytes, i);
+ }
+ 
+ static void stripe_dax_flush(struct dm_target *ti, pgoff_t pgoff, void *addr,
+ 		size_t size)
+ {
+ 	sector_t dev_sector, sector = pgoff * PAGE_SECTORS;
+ 	struct stripe_c *sc = ti->private;
+ 	struct dax_device *dax_dev;
+ 	struct block_device *bdev;
+ 	uint32_t stripe;
+ 
+ 	stripe_map_sector(sc, sector, &stripe, &dev_sector);
+ 	dev_sector += sc->stripe[stripe].physical_start;
+ 	dax_dev = sc->stripe[stripe].dev->dax_dev;
+ 	bdev = sc->stripe[stripe].dev->bdev;
+ 
+ 	if (bdev_dax_pgoff(bdev, dev_sector, ALIGN(size, PAGE_SIZE), &pgoff))
+ 		return;
+ 	dax_flush(dax_dev, pgoff, addr, size);
+ }
+ 
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  /*
   * Stripe status:
   *
@@@ -464,8 -488,9 +505,13 @@@ static struct target_type stripe_targe
  	.status = stripe_status,
  	.iterate_devices = stripe_iterate_devices,
  	.io_hints = stripe_io_hints,
 +	.merge  = stripe_merge,
  	.direct_access = stripe_dax_direct_access,
++<<<<<<< HEAD
++=======
+ 	.dax_copy_from_iter = stripe_dax_copy_from_iter,
+ 	.dax_flush = stripe_dax_flush,
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  };
  
  int __init dm_stripe_init(void)
diff --cc drivers/md/dm.c
index 14d7215727e9,09b3efdc8abf..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -963,6 -970,88 +963,91 @@@ static long dm_dax_direct_access(struc
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static size_t dm_dax_copy_from_iter(struct dax_device *dax_dev, pgoff_t pgoff,
+ 		void *addr, size_t bytes, struct iov_iter *i)
+ {
+ 	struct mapped_device *md = dax_get_private(dax_dev);
+ 	sector_t sector = pgoff * PAGE_SECTORS;
+ 	struct dm_target *ti;
+ 	long ret = 0;
+ 	int srcu_idx;
+ 
+ 	ti = dm_dax_get_live_target(md, sector, &srcu_idx);
+ 
+ 	if (!ti)
+ 		goto out;
+ 	if (!ti->type->dax_copy_from_iter) {
+ 		ret = copy_from_iter(addr, bytes, i);
+ 		goto out;
+ 	}
+ 	ret = ti->type->dax_copy_from_iter(ti, pgoff, addr, bytes, i);
+  out:
+ 	dm_put_live_table(md, srcu_idx);
+ 
+ 	return ret;
+ }
+ 
+ static void dm_dax_flush(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t size)
+ {
+ 	struct mapped_device *md = dax_get_private(dax_dev);
+ 	sector_t sector = pgoff * PAGE_SECTORS;
+ 	struct dm_target *ti;
+ 	int srcu_idx;
+ 
+ 	ti = dm_dax_get_live_target(md, sector, &srcu_idx);
+ 
+ 	if (!ti)
+ 		goto out;
+ 	if (ti->type->dax_flush)
+ 		ti->type->dax_flush(ti, pgoff, addr, size);
+  out:
+ 	dm_put_live_table(md, srcu_idx);
+ }
+ 
+ /*
+  * A target may call dm_accept_partial_bio only from the map routine.  It is
+  * allowed for all bio types except REQ_PREFLUSH.
+  *
+  * dm_accept_partial_bio informs the dm that the target only wants to process
+  * additional n_sectors sectors of the bio and the rest of the data should be
+  * sent in a next bio.
+  *
+  * A diagram that explains the arithmetics:
+  * +--------------------+---------------+-------+
+  * |         1          |       2       |   3   |
+  * +--------------------+---------------+-------+
+  *
+  * <-------------- *tio->len_ptr --------------->
+  *                      <------- bi_size ------->
+  *                      <-- n_sectors -->
+  *
+  * Region 1 was already iterated over with bio_advance or similar function.
+  *	(it may be empty if the target doesn't use bio_advance)
+  * Region 2 is the remaining bio size that the target wants to process.
+  *	(it may be empty if region 1 is non-empty, although there is no reason
+  *	 to make it empty)
+  * The target requires that region 3 is to be sent in the next bio.
+  *
+  * If the target wants to receive multiple copies of the bio (via num_*bios, etc),
+  * the partially processed part (the sum of regions 1+2) must be the same for all
+  * copies of the bio.
+  */
+ void dm_accept_partial_bio(struct bio *bio, unsigned n_sectors)
+ {
+ 	struct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);
+ 	unsigned bi_size = bio->bi_iter.bi_size >> SECTOR_SHIFT;
+ 	BUG_ON(bio->bi_opf & REQ_PREFLUSH);
+ 	BUG_ON(bi_size > *tio->len_ptr);
+ 	BUG_ON(n_sectors > bi_size);
+ 	*tio->len_ptr -= bi_size - n_sectors;
+ 	bio->bi_iter.bi_size = n_sectors << SECTOR_SHIFT;
+ }
+ EXPORT_SYMBOL_GPL(dm_accept_partial_bio);
+ 
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  /*
   * Flush current->bio_list when the target map method blocks.
   * This fixes deadlocks in snapshot and possibly in other targets.
@@@ -3093,6 -2902,8 +3178,11 @@@ static const struct block_device_operat
  
  static const struct dax_operations dm_dax_ops = {
  	.direct_access = dm_dax_direct_access,
++<<<<<<< HEAD
++=======
+ 	.copy_from_iter = dm_dax_copy_from_iter,
+ 	.flush = dm_dax_flush,
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  };
  
  /*
diff --cc include/linux/dax.h
index b7b81d6cc271,1f6b6072af64..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -6,6 -6,85 +6,88 @@@
  #include <linux/radix-tree.h>
  #include <asm/pgtable.h>
  
++<<<<<<< HEAD
++=======
+ struct iomap_ops;
+ struct dax_device;
+ struct dax_operations {
+ 	/*
+ 	 * direct_access: translate a device-relative
+ 	 * logical-page-offset into an absolute physical pfn. Return the
+ 	 * number of pages available for DAX at that pfn.
+ 	 */
+ 	long (*direct_access)(struct dax_device *, pgoff_t, long,
+ 			void **, pfn_t *);
+ 	/* copy_from_iter: dax-driver override for default copy_from_iter */
+ 	size_t (*copy_from_iter)(struct dax_device *, pgoff_t, void *, size_t,
+ 			struct iov_iter *);
+ 	/* flush: optional driver-specific cache management after writes */
+ 	void (*flush)(struct dax_device *, pgoff_t, void *, size_t);
+ };
+ 
+ #if IS_ENABLED(CONFIG_DAX)
+ struct dax_device *dax_get_by_host(const char *host);
+ void put_dax(struct dax_device *dax_dev);
+ #else
+ static inline struct dax_device *dax_get_by_host(const char *host)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void put_dax(struct dax_device *dax_dev)
+ {
+ }
+ #endif
+ 
+ int bdev_dax_pgoff(struct block_device *, sector_t, size_t, pgoff_t *pgoff);
+ #if IS_ENABLED(CONFIG_FS_DAX)
+ int __bdev_dax_supported(struct super_block *sb, int blocksize);
+ static inline int bdev_dax_supported(struct super_block *sb, int blocksize)
+ {
+ 	return __bdev_dax_supported(sb, blocksize);
+ }
+ 
+ static inline struct dax_device *fs_dax_get_by_host(const char *host)
+ {
+ 	return dax_get_by_host(host);
+ }
+ 
+ static inline void fs_put_dax(struct dax_device *dax_dev)
+ {
+ 	put_dax(dax_dev);
+ }
+ 
+ #else
+ static inline int bdev_dax_supported(struct super_block *sb, int blocksize)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static inline struct dax_device *fs_dax_get_by_host(const char *host)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void fs_put_dax(struct dax_device *dax_dev)
+ {
+ }
+ #endif
+ 
+ int dax_read_lock(void);
+ void dax_read_unlock(int id);
+ struct dax_device *alloc_dax(void *private, const char *host,
+ 		const struct dax_operations *ops);
+ bool dax_alive(struct dax_device *dax_dev);
+ void kill_dax(struct dax_device *dax_dev);
+ void *dax_get_private(struct dax_device *dax_dev);
+ long dax_direct_access(struct dax_device *dax_dev, pgoff_t pgoff, long nr_pages,
+ 		void **kaddr, pfn_t *pfn);
+ size_t dax_copy_from_iter(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t bytes, struct iov_iter *i);
+ void dax_flush(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t size);
+ 
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  /*
   * We use lowest available bit in exceptional entry for locking, one bit for
   * the entry size (PMD) and two more to tell us if the entry is a huge zero
diff --cc include/linux/device-mapper.h
index eca858277b1d,67bfe8ddcb32..000000000000
--- a/include/linux/device-mapper.h
+++ b/include/linux/device-mapper.h
@@@ -137,6 -132,10 +137,13 @@@ typedef int (*dm_busy_fn) (struct dm_ta
   */
  typedef long (*dm_dax_direct_access_fn) (struct dm_target *ti, pgoff_t pgoff,
  		long nr_pages, void **kaddr, pfn_t *pfn);
++<<<<<<< HEAD
++=======
+ typedef size_t (*dm_dax_copy_from_iter_fn)(struct dm_target *ti, pgoff_t pgoff,
+ 		void *addr, size_t bytes, struct iov_iter *i);
+ typedef void (*dm_dax_flush_fn)(struct dm_target *ti, pgoff_t pgoff, void *addr,
+ 		size_t size);
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  #define PAGE_SECTORS (PAGE_SIZE / 512)
  
  void dm_error(const char *message);
@@@ -188,6 -185,8 +195,11 @@@ struct target_type 
  	dm_iterate_devices_fn iterate_devices;
  	dm_io_hints_fn io_hints;
  	dm_dax_direct_access_fn direct_access;
++<<<<<<< HEAD
++=======
+ 	dm_dax_copy_from_iter_fn dax_copy_from_iter;
+ 	dm_dax_flush_fn dax_flush;
++>>>>>>> abebfbe2f731 (dm: add ->flush() dax operation support)
  
  	/* For internal device-mapper use. */
  	struct list_head list;
* Unmerged path drivers/dax/super.c
* Unmerged path drivers/md/dm-linear.c
* Unmerged path drivers/md/dm-stripe.c
* Unmerged path drivers/md/dm.c
* Unmerged path include/linux/dax.h
* Unmerged path include/linux/device-mapper.h
