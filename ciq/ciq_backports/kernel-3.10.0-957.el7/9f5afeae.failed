tcp: use an RB tree for ooo receive queue

jira LE-1907
cve CVE-2018-5390
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Yaogong Wang <wygivan@google.com>
commit 9f5afeae51526b3ad7b7cb21ee8b145ce6ea7a7a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9f5afeae.failed

Over the years, TCP BDP has increased by several orders of magnitude,
and some people are considering to reach the 2 Gbytes limit.

Even with current window scale limit of 14, ~1 Gbytes maps to ~740,000
MSS.

In presence of packet losses (or reorders), TCP stores incoming packets
into an out of order queue, and number of skbs sitting there waiting for
the missing packets to be received can be in the 10^5 range.

Most packets are appended to the tail of this queue, and when
packets can finally be transferred to receive queue, we scan the queue
from its head.

However, in presence of heavy losses, we might have to find an arbitrary
point in this queue, involving a linear scan for every incoming packet,
throwing away cpu caches.

This patch converts it to a RB tree, to get bounded latencies.

Yaogong wrote a preliminary patch about 2 years ago.
Eric did the rebase, added ofo_last_skb cache, polishing and tests.

Tested with network dropping between 1 and 10 % packets, with good
success (about 30 % increase of throughput in stress tests)

Next step would be to also use an RB tree for the write queue at sender
side ;)

	Signed-off-by: Yaogong Wang <wygivan@google.com>
	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Yuchung Cheng <ycheng@google.com>
	Cc: Neal Cardwell <ncardwell@google.com>
	Cc: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Acked-By: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 9f5afeae51526b3ad7b7cb21ee8b145ce6ea7a7a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_input.c
diff --cc net/ipv4/tcp_input.c
index 4ec1e4f8ab44,a5934c4c8cd4..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -4246,11 -4360,12 +4249,12 @@@ static void tcp_ofo_queue(struct sock *
  				dsack_high = TCP_SKB_CB(skb)->end_seq;
  			tcp_dsack_extend(sk, TCP_SKB_CB(skb)->seq, dsack);
  		}
+ 		p = rb_next(p);
+ 		rb_erase(&skb->rbnode, &tp->out_of_order_queue);
  
- 		__skb_unlink(skb, &tp->out_of_order_queue);
- 		if (!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt)) {
+ 		if (unlikely(!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))) {
  			SOCK_DEBUG(sk, "ofo packet was already received\n");
 -			tcp_drop(sk, skb);
 +			__kfree_skb(skb);
  			continue;
  		}
  		SOCK_DEBUG(sk, "ofo requeuing : rcv_next %X seq %X - %X\n",
@@@ -4295,14 -4414,16 +4306,16 @@@ static int tcp_try_rmem_schedule(struc
  static void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)
  {
  	struct tcp_sock *tp = tcp_sk(sk);
+ 	struct rb_node **p, *q, *parent;
  	struct sk_buff *skb1;
  	u32 seq, end_seq;
+ 	bool fragstolen;
  
 -	tcp_ecn_check_ce(tp, skb);
 +	TCP_ECN_check_ce(tp, skb);
  
  	if (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {
 -		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);
 -		tcp_drop(sk, skb);
 +		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFODROP);
 +		__kfree_skb(skb);
  		return;
  	}
  
@@@ -4310,12 -4431,14 +4323,18 @@@
  	tp->pred_flags = 0;
  	inet_csk_schedule_ack(sk);
  
++<<<<<<< HEAD
 +	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);
++=======
+ 	NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);
+ 	seq = TCP_SKB_CB(skb)->seq;
+ 	end_seq = TCP_SKB_CB(skb)->end_seq;
++>>>>>>> 9f5afeae5152 (tcp: use an RB tree for ooo receive queue)
  	SOCK_DEBUG(sk, "out of order segment: rcv_next %X seq %X - %X\n",
- 		   tp->rcv_nxt, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);
+ 		   tp->rcv_nxt, seq, end_seq);
  
- 	skb1 = skb_peek_tail(&tp->out_of_order_queue);
- 	if (!skb1) {
+ 	p = &tp->out_of_order_queue.rb_node;
+ 	if (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {
  		/* Initial out of order segment, build 1 SACK. */
  		if (tcp_is_sack(tp)) {
  			tp->rx_opt.num_sacks = 1;
@@@ -4327,72 -4451,66 +4347,97 @@@
  		goto end;
  	}
  
- 	seq = TCP_SKB_CB(skb)->seq;
- 	end_seq = TCP_SKB_CB(skb)->end_seq;
- 
- 	if (seq == TCP_SKB_CB(skb1)->end_seq) {
- 		bool fragstolen;
- 
- 		if (!tcp_try_coalesce(sk, skb1, skb, &fragstolen)) {
- 			__skb_queue_after(&tp->out_of_order_queue, skb1, skb);
- 		} else {
- 			tcp_grow_window(sk, skb);
- 			kfree_skb_partial(skb, fragstolen);
- 			skb = NULL;
- 		}
- 
- 		if (!tp->rx_opt.num_sacks ||
- 		    tp->selective_acks[0].end_seq != seq)
- 			goto add_sack;
- 
- 		/* Common case: data arrive in order after hole. */
- 		tp->selective_acks[0].end_seq = end_seq;
- 		goto end;
+ 	/* In the typical case, we are adding an skb to the end of the list.
+ 	 * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.
+ 	 */
+ 	if (tcp_try_coalesce(sk, tp->ooo_last_skb, skb, &fragstolen)) {
+ coalesce_done:
+ 		tcp_grow_window(sk, skb);
+ 		kfree_skb_partial(skb, fragstolen);
+ 		skb = NULL;
+ 		goto add_sack;
  	}
  
- 	/* Find place to insert this segment. */
- 	while (1) {
- 		if (!after(TCP_SKB_CB(skb1)->seq, seq))
- 			break;
- 		if (skb_queue_is_first(&tp->out_of_order_queue, skb1)) {
- 			skb1 = NULL;
- 			break;
+ 	/* Find place to insert this segment. Handle overlaps on the way. */
+ 	parent = NULL;
+ 	while (*p) {
+ 		parent = *p;
+ 		skb1 = rb_entry(parent, struct sk_buff, rbnode);
+ 		if (before(seq, TCP_SKB_CB(skb1)->seq)) {
+ 			p = &parent->rb_left;
+ 			continue;
  		}
- 		skb1 = skb_queue_prev(&tp->out_of_order_queue, skb1);
+ 		if (before(seq, TCP_SKB_CB(skb1)->end_seq)) {
+ 			if (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {
+ 				/* All the bits are present. Drop. */
+ 				NET_INC_STATS(sock_net(sk),
+ 					      LINUX_MIB_TCPOFOMERGE);
+ 				__kfree_skb(skb);
+ 				skb = NULL;
+ 				tcp_dsack_set(sk, seq, end_seq);
+ 				goto add_sack;
+ 			}
+ 			if (after(seq, TCP_SKB_CB(skb1)->seq)) {
+ 				/* Partial overlap. */
+ 				tcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)->end_seq);
+ 			} else {
+ 				/* skb's seq == skb1's seq and skb covers skb1.
+ 				 * Replace skb1 with skb.
+ 				 */
+ 				rb_replace_node(&skb1->rbnode, &skb->rbnode,
+ 						&tp->out_of_order_queue);
+ 				tcp_dsack_extend(sk,
+ 						 TCP_SKB_CB(skb1)->seq,
+ 						 TCP_SKB_CB(skb1)->end_seq);
+ 				NET_INC_STATS(sock_net(sk),
+ 					      LINUX_MIB_TCPOFOMERGE);
+ 				__kfree_skb(skb1);
+ 				goto add_sack;
+ 			}
+ 		} else if (tcp_try_coalesce(sk, skb1, skb, &fragstolen)) {
+ 			goto coalesce_done;
+ 		}
+ 		p = &parent->rb_right;
  	}
  
++<<<<<<< HEAD
 +	/* Do skb overlap to previous one? */
 +	if (skb1 && before(seq, TCP_SKB_CB(skb1)->end_seq)) {
 +		if (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {
 +			/* All the bits are present. Drop. */
 +			NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFOMERGE);
 +			__kfree_skb(skb);
 +			skb = NULL;
 +			tcp_dsack_set(sk, seq, end_seq);
 +			goto add_sack;
 +		}
 +		if (after(seq, TCP_SKB_CB(skb1)->seq)) {
 +			/* Partial overlap. */
 +			tcp_dsack_set(sk, seq,
 +				      TCP_SKB_CB(skb1)->end_seq);
 +		} else {
 +			if (skb_queue_is_first(&tp->out_of_order_queue,
 +					       skb1))
 +				skb1 = NULL;
 +			else
 +				skb1 = skb_queue_prev(
 +					&tp->out_of_order_queue,
 +					skb1);
 +		}
 +	}
 +	if (!skb1)
 +		__skb_queue_head(&tp->out_of_order_queue, skb);
 +	else
 +		__skb_queue_after(&tp->out_of_order_queue, skb1, skb);
++=======
+ 	/* Insert segment into RB tree. */
+ 	rb_link_node(&skb->rbnode, parent, p);
+ 	rb_insert_color(&skb->rbnode, &tp->out_of_order_queue);
++>>>>>>> 9f5afeae5152 (tcp: use an RB tree for ooo receive queue)
  
- 	/* And clean segments covered by new one as whole. */
- 	while (!skb_queue_is_last(&tp->out_of_order_queue, skb)) {
- 		skb1 = skb_queue_next(&tp->out_of_order_queue, skb);
+ 	/* Remove other segments covered by skb. */
+ 	while ((q = rb_next(&skb->rbnode)) != NULL) {
+ 		skb1 = rb_entry(q, struct sk_buff, rbnode);
  
  		if (!after(end_seq, TCP_SKB_CB(skb1)->seq))
  			break;
@@@ -4401,12 -4519,15 +4446,15 @@@
  					 end_seq);
  			break;
  		}
- 		__skb_unlink(skb1, &tp->out_of_order_queue);
+ 		rb_erase(&skb1->rbnode, &tp->out_of_order_queue);
  		tcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,
  				 TCP_SKB_CB(skb1)->end_seq);
 -		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);
 -		tcp_drop(sk, skb1);
 +		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFOMERGE);
 +		__kfree_skb(skb1);
  	}
+ 	/* If there is no skb after us, we are the last_skb ! */
+ 	if (!q)
+ 		tp->ooo_last_skb = skb;
  
  add_sack:
  	if (tcp_is_sack(tp))
@@@ -4588,17 -4724,27 +4636,27 @@@ drop
  	tcp_data_queue_ofo(sk, skb);
  }
  
+ static struct sk_buff *tcp_skb_next(struct sk_buff *skb, struct sk_buff_head *list)
+ {
+ 	if (list)
+ 		return !skb_queue_is_last(list, skb) ? skb->next : NULL;
+ 
+ 	return rb_entry_safe(rb_next(&skb->rbnode), struct sk_buff, rbnode);
+ }
+ 
  static struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,
- 					struct sk_buff_head *list)
+ 					struct sk_buff_head *list,
+ 					struct rb_root *root)
  {
- 	struct sk_buff *next = NULL;
+ 	struct sk_buff *next = tcp_skb_next(skb, list);
  
- 	if (!skb_queue_is_last(list, skb))
- 		next = skb_queue_next(list, skb);
+ 	if (list)
+ 		__skb_unlink(skb, list);
+ 	else
+ 		rb_erase(&skb->rbnode, root);
  
- 	__skb_unlink(skb, list);
  	__kfree_skb(skb);
 -	NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOLLAPSED);
 +	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPRCVCOLLAPSED);
  
  	return next;
  }
@@@ -4707,26 -4876,31 +4788,35 @@@ end
  static void tcp_collapse_ofo_queue(struct sock *sk)
  {
  	struct tcp_sock *tp = tcp_sk(sk);
- 	struct sk_buff *skb = skb_peek(&tp->out_of_order_queue);
- 	struct sk_buff *head;
+ 	struct sk_buff *skb, *head;
+ 	struct rb_node *p;
  	u32 start, end;
  
++<<<<<<< HEAD
 +	if (skb == NULL)
++=======
+ 	p = rb_first(&tp->out_of_order_queue);
+ 	skb = rb_entry_safe(p, struct sk_buff, rbnode);
+ new_range:
+ 	if (!skb) {
+ 		p = rb_last(&tp->out_of_order_queue);
+ 		/* Note: This is possible p is NULL here. We do not
+ 		 * use rb_entry_safe(), as ooo_last_skb is valid only
+ 		 * if rbtree is not empty.
+ 		 */
+ 		tp->ooo_last_skb = rb_entry(p, struct sk_buff, rbnode);
++>>>>>>> 9f5afeae5152 (tcp: use an RB tree for ooo receive queue)
  		return;
- 
+ 	}
  	start = TCP_SKB_CB(skb)->seq;
  	end = TCP_SKB_CB(skb)->end_seq;
- 	head = skb;
  
- 	for (;;) {
- 		struct sk_buff *next = NULL;
+ 	for (head = skb;;) {
+ 		skb = tcp_skb_next(skb, NULL);
  
- 		if (!skb_queue_is_last(&tp->out_of_order_queue, skb))
- 			next = skb_queue_next(&tp->out_of_order_queue, skb);
- 		skb = next;
- 
- 		/* Segment is terminated when we see gap or when
- 		 * we are at the end of all the queue. */
+ 		/* Range is terminated when we see a gap or when
+ 		 * we are at the queue end.
+ 		 */
  		if (!skb ||
  		    after(TCP_SKB_CB(skb)->seq, end) ||
  		    before(TCP_SKB_CB(skb)->end_seq, start)) {
@@@ -4754,23 -4929,33 +4839,53 @@@
  static bool tcp_prune_ofo_queue(struct sock *sk)
  {
  	struct tcp_sock *tp = tcp_sk(sk);
++<<<<<<< HEAD
 +	bool res = false;
 +
 +	if (!skb_queue_empty(&tp->out_of_order_queue)) {
 +		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_OFOPRUNED);
 +		__skb_queue_purge(&tp->out_of_order_queue);
 +
 +		/* Reset SACK state.  A conforming SACK implementation will
 +		 * do the same at a timeout based retransmit.  When a connection
 +		 * is in a sad state like this, we care only about integrity
 +		 * of the connection not performance.
 +		 */
 +		if (tp->rx_opt.sack_ok)
 +			tcp_sack_reset(&tp->rx_opt);
 +		sk_mem_reclaim(sk);
 +		res = true;
 +	}
 +	return res;
++=======
+ 	struct rb_node *node, *prev;
+ 
+ 	if (RB_EMPTY_ROOT(&tp->out_of_order_queue))
+ 		return false;
+ 
+ 	NET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);
+ 	node = &tp->ooo_last_skb->rbnode;
+ 	do {
+ 		prev = rb_prev(node);
+ 		rb_erase(node, &tp->out_of_order_queue);
+ 		tcp_drop(sk, rb_entry(node, struct sk_buff, rbnode));
+ 		sk_mem_reclaim(sk);
+ 		if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&
+ 		    !tcp_under_memory_pressure(sk))
+ 			break;
+ 		node = prev;
+ 	} while (node);
+ 	tp->ooo_last_skb = rb_entry(prev, struct sk_buff, rbnode);
+ 
+ 	/* Reset SACK state.  A conforming SACK implementation will
+ 	 * do the same at a timeout based retransmit.  When a connection
+ 	 * is in a sad state like this, we care only about integrity
+ 	 * of the connection not performance.
+ 	 */
+ 	if (tp->rx_opt.sack_ok)
+ 		tcp_sack_reset(&tp->rx_opt);
+ 	return true;
++>>>>>>> 9f5afeae5152 (tcp: use an RB tree for ooo receive queue)
  }
  
  /* Reduce allocated memory if we can, trying to get
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 6ad965e15f51..c4adcfb16631 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -2459,6 +2459,8 @@ static inline void __skb_queue_purge(struct sk_buff_head *list)
 		kfree_skb(skb);
 }
 
+void skb_rbtree_purge(struct rb_root *root);
+
 void *netdev_alloc_frag(unsigned int fragsz);
 
 struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,
diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fc67d8b22bbb..73910552195e 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -272,10 +272,9 @@ struct tcp_sock {
 	struct sk_buff* lost_skb_hint;
 	struct sk_buff *retransmit_skb_hint;
 
-	/* OOO segments go in this list. Note that socket lock must be held,
-	 * as we do not use sk_buff_head lock.
-	 */
-	struct sk_buff_head	out_of_order_queue;
+	/* OOO segments go in this rbtree. Socket lock must be held. */
+	struct rb_root	out_of_order_queue;
+	struct sk_buff	*ooo_last_skb; /* cache rb_last(out_of_order_queue) */
 
 	/* SACKs data, these 2 need to be together (see tcp_options_write) */
 	struct tcp_sack_block duplicate_sack[1]; /* D-SACK block */
diff --git a/include/net/tcp.h b/include/net/tcp.h
index 466b92159e99..a8a2278295ca 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -627,7 +627,7 @@ static inline void tcp_fast_path_check(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	if (skb_queue_empty(&tp->out_of_order_queue) &&
+	if (RB_EMPTY_ROOT(&tp->out_of_order_queue) &&
 	    tp->rcv_wnd &&
 	    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&
 	    !tp->urg_data)
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 9488c5317ec3..9ba0a9d72456 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -2549,6 +2549,25 @@ void skb_queue_purge(struct sk_buff_head *list)
 }
 EXPORT_SYMBOL(skb_queue_purge);
 
+/**
+ *	skb_rbtree_purge - empty a skb rbtree
+ *	@root: root of the rbtree to empty
+ *
+ *	Delete all buffers on an &sk_buff rbtree. Each buffer is removed from
+ *	the list and one reference dropped. This function does not take
+ *	any lock. Synchronization should be handled by the caller (e.g., TCP
+ *	out-of-order queue is protected by the socket lock).
+ */
+void skb_rbtree_purge(struct rb_root *root)
+{
+	struct sk_buff *skb, *next;
+
+	rbtree_postorder_for_each_entry_safe(skb, next, root, rbnode)
+		kfree_skb(skb);
+
+	*root = RB_ROOT;
+}
+
 /**
  *	skb_queue_head - queue a buffer at the list head
  *	@list: list to use
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 102a156f9e5e..fe442700b4e0 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -378,7 +378,7 @@ void tcp_init_sock(struct sock *sk)
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	__skb_queue_head_init(&tp->out_of_order_queue);
+	tp->out_of_order_queue = RB_ROOT;
 	tcp_init_xmit_timers(sk);
 	tcp_prequeue_init(tp);
 	INIT_LIST_HEAD(&tp->tsq_node);
@@ -2217,7 +2217,7 @@ int tcp_disconnect(struct sock *sk, int flags)
 	tcp_clear_xmit_timers(sk);
 	__skb_queue_purge(&sk->sk_receive_queue);
 	tcp_write_queue_purge(sk);
-	__skb_queue_purge(&tp->out_of_order_queue);
+	skb_rbtree_purge(&tp->out_of_order_queue);
 
 	inet->inet_dport = 0;
 
* Unmerged path net/ipv4/tcp_input.c
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index b635ea161b80..cfce3e358d1d 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -1867,7 +1867,7 @@ void tcp_v4_destroy_sock(struct sock *sk)
 	tcp_write_queue_purge(sk);
 
 	/* Cleans up our, hopefully empty, out_of_order_queue. */
-	__skb_queue_purge(&tp->out_of_order_queue);
+	skb_rbtree_purge(&tp->out_of_order_queue);
 
 #ifdef CONFIG_TCP_MD5SIG
 	/* Clean up the MD5 key list, if any */
diff --git a/net/ipv4/tcp_minisocks.c b/net/ipv4/tcp_minisocks.c
index 83a25e106ff7..152fe39303e7 100644
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@ -498,7 +498,6 @@ struct sock *tcp_create_openreq_child(struct sock *sk, struct request_sock *req,
 		newtp->snd_cwnd_cnt = 0;
 
 		tcp_init_xmit_timers(newsk);
-		__skb_queue_head_init(&newtp->out_of_order_queue);
 		newtp->write_seq = newtp->pushed_seq = treq->snt_isn + 1;
 
 		newtp->rx_opt.saw_tstamp = 0;
