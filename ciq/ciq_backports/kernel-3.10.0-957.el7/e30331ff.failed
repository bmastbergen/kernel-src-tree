dax: relocate some dax functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ross Zwisler <ross.zwisler@linux.intel.com>
commit e30331ff05f689f8f2faeb51664299c4d7841f15
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/e30331ff.failed

dax_load_hole() will soon need to call dax_insert_mapping_entry(), so it
needs to be moved lower in dax.c so the definition exists.

dax_wake_mapping_entry_waiter() will soon be removed from dax.h and be
made static to dax.c, so we need to move its definition above all its
callers.

Link: http://lkml.kernel.org/r/20170724170616.25810-3-ross.zwisler@linux.intel.com
	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Cc: "Darrick J. Wong" <darrick.wong@oracle.com>
	Cc: "Theodore Ts'o" <tytso@mit.edu>
	Cc: Alexander Viro <viro@zeniv.linux.org.uk>
	Cc: Andreas Dilger <adilger.kernel@dilger.ca>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e30331ff05f689f8f2faeb51664299c4d7841f15)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 679214f8898b,b8882b5ce6ed..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -488,50 -468,6 +488,53 @@@ int dax_invalidate_mapping_entry_sync(s
  	return __dax_invalidate_mapping_entry(mapping, index, false);
  }
  
++<<<<<<< HEAD
 +/*
 + * The user has performed a load from a hole in the file.  Allocating
 + * a new page in the file would cause excessive storage usage for
 + * workloads with sparse files.  We allocate a page cache page instead.
 + * We'll kick it out of the page cache if it's ever written to,
 + * otherwise it will simply fall out of the page cache under memory
 + * pressure without ever having been dirtied.
 + */
 +static int dax_load_hole(struct address_space *mapping, void **entry,
 +			 struct vm_fault *vmf)
 +{
 +	struct inode *inode = mapping->host;
 +	struct page *page;
 +	int ret;
 +
 +	/* Hole page already exists? Return it...  */
 +	if (!radix_tree_exceptional_entry(*entry)) {
 +		page = *entry;
 +		goto finish_fault;
 +	}
 +
 +	/* This will replace locked radix tree entry with a hole page */
 +	page = find_or_create_page(mapping, vmf->pgoff,
 +		mapping_gfp_mask(mapping) | __GFP_FS | __GFP_IO | __GFP_ZERO);
 +	if (!page) {
 +		ret = VM_FAULT_OOM;
 +		goto out;
 +	}
 +
 +finish_fault:
 +	vmf->page = page;
 +	ret = finish_fault(vmf);
 +	vmf->page = NULL;
 +	*entry = page;
 +	if (!ret) {
 +		/* Grab reference for PTE that is now referencing the page */
 +		get_page(page);
 +		ret = VM_FAULT_NOPAGE;
 +	}
 +out:
 +	trace_dax_load_hole(inode, vmf, ret);
 +	return ret;
 +}
 +
++=======
++>>>>>>> e30331ff05f6 (dax: relocate some dax functions)
  static int copy_user_dax(struct block_device *bdev, struct dax_device *dax_dev,
  		sector_t sector, size_t size, struct page *to,
  		unsigned long vaddr)
* Unmerged path fs/dax.c
