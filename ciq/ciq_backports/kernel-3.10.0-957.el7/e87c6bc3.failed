bpf: permit multiple bpf attachments for a single perf event

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Yonghong Song <yhs@fb.com>
commit e87c6bc3852b981e71c757be20771546ce9f76f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/e87c6bc3.failed

This patch enables multiple bpf attachments for a
kprobe/uprobe/tracepoint single trace event.
Each trace_event keeps a list of attached perf events.
When an event happens, all attached bpf programs will
be executed based on the order of attachment.

A global bpf_event_mutex lock is introduced to protect
prog_array attaching and detaching. An alternative will
be introduce a mutex lock in every trace_event_call
structure, but it takes a lot of extra memory.
So a global bpf_event_mutex lock is a good compromise.

The bpf prog detachment involves allocation of memory.
If the allocation fails, a dummy do-nothing program
will replace to-be-detached program in-place.

	Signed-off-by: Yonghong Song <yhs@fb.com>
	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Martin KaFai Lau <kafai@fb.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e87c6bc3852b981e71c757be20771546ce9f76f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	include/linux/trace_events.h
#	include/trace/perf.h
#	kernel/bpf/core.c
#	kernel/events/core.c
#	kernel/trace/bpf_trace.c
#	kernel/trace/trace_kprobe.c
#	kernel/trace/trace_syscalls.c
#	kernel/trace/trace_uprobe.c
diff --cc include/linux/bpf.h
index d4c1f9049ad3,172be7faf7ba..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -89,28 -168,369 +89,359 @@@ struct bpf_func_proto 
  struct bpf_verifier_ops {
  	/* return eBPF function prototype for verification */
  	const struct bpf_func_proto *(*get_func_proto)(enum bpf_func_id func_id);
 +};
  
 -	/* return true if 'size' wide access at offset 'off' within bpf_context
 -	 * with 'type' (read or write) is allowed
 -	 */
 -	bool (*is_valid_access)(int off, int size, enum bpf_access_type type,
 -				struct bpf_insn_access_aux *info);
 -	int (*gen_prologue)(struct bpf_insn *insn, bool direct_write,
 -			    const struct bpf_prog *prog);
 -	u32 (*convert_ctx_access)(enum bpf_access_type type,
 -				  const struct bpf_insn *src,
 -				  struct bpf_insn *dst,
 -				  struct bpf_prog *prog, u32 *target_size);
 +struct bpf_prog_type_list {
 +	struct list_head list_node;
 +	struct bpf_verifier_ops *ops;
 +	enum bpf_prog_type type;
  };
  
 +void bpf_register_prog_type(struct bpf_prog_type_list *tl);
 +
 +struct bpf_prog;
 +
  struct bpf_prog_aux {
  	atomic_t refcnt;
 -	u32 used_map_cnt;
 -	u32 max_ctx_offset;
 -	u32 stack_depth;
 +	bool is_gpl_compatible;
 +	enum bpf_prog_type prog_type;
 +	struct bpf_verifier_ops *ops;
  	u32 id;
 -	struct latch_tree_node ksym_tnode;
 -	struct list_head ksym_lnode;
 -	const struct bpf_prog_ops *ops;
  	struct bpf_map **used_maps;
 +	u32 used_map_cnt;
  	struct bpf_prog *prog;
 -	struct user_struct *user;
 -	u64 load_time; /* ns since boottime */
 -	char name[BPF_OBJ_NAME_LEN];
 -#ifdef CONFIG_SECURITY
 -	void *security;
 -#endif
 -	union {
 -		struct work_struct work;
 -		struct rcu_head	rcu;
 -	};
 +	struct work_struct work;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_array {
+ 	struct bpf_map map;
+ 	u32 elem_size;
+ 	/* 'ownership' of prog_array is claimed by the first program that
+ 	 * is going to use this map or by the first program which FD is stored
+ 	 * in the map to make sure that all callers and callees have the same
+ 	 * prog_type and JITed flag
+ 	 */
+ 	enum bpf_prog_type owner_prog_type;
+ 	bool owner_jited;
+ 	union {
+ 		char value[0] __aligned(8);
+ 		void *ptrs[0] __aligned(8);
+ 		void __percpu *pptrs[0] __aligned(8);
+ 	};
+ };
+ 
+ #define MAX_TAIL_CALL_CNT 32
+ 
+ struct bpf_event_entry {
+ 	struct perf_event *event;
+ 	struct file *perf_file;
+ 	struct file *map_file;
+ 	struct rcu_head rcu;
+ };
+ 
+ u64 bpf_tail_call(u64 ctx, u64 r2, u64 index, u64 r4, u64 r5);
+ u64 bpf_get_stackid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
+ bool bpf_prog_array_compatible(struct bpf_array *array, const struct bpf_prog *fp);
+ int bpf_prog_calc_tag(struct bpf_prog *fp);
+ 
+ const struct bpf_func_proto *bpf_get_trace_printk_proto(void);
+ 
+ typedef unsigned long (*bpf_ctx_copy_t)(void *dst, const void *src,
+ 					unsigned long off, unsigned long len);
+ 
+ u64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,
+ 		     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy);
+ 
+ int bpf_prog_test_run_xdp(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ int bpf_prog_test_run_skb(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ 
+ /* an array of programs to be executed under rcu_lock.
+  *
+  * Typical usage:
+  * ret = BPF_PROG_RUN_ARRAY(&bpf_prog_array, ctx, BPF_PROG_RUN);
+  *
+  * the structure returned by bpf_prog_array_alloc() should be populated
+  * with program pointers and the last pointer must be NULL.
+  * The user has to keep refcnt on the program and make sure the program
+  * is removed from the array before bpf_prog_put().
+  * The 'struct bpf_prog_array *' should only be replaced with xchg()
+  * since other cpus are walking the array of pointers in parallel.
+  */
+ struct bpf_prog_array {
+ 	struct rcu_head rcu;
+ 	struct bpf_prog *progs[0];
+ };
+ 
+ struct bpf_prog_array __rcu *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags);
+ void bpf_prog_array_free(struct bpf_prog_array __rcu *progs);
+ int bpf_prog_array_length(struct bpf_prog_array __rcu *progs);
+ int bpf_prog_array_copy_to_user(struct bpf_prog_array __rcu *progs,
+ 				__u32 __user *prog_ids, u32 cnt);
+ 
+ void bpf_prog_array_delete_safe(struct bpf_prog_array __rcu *progs,
+ 				struct bpf_prog *old_prog);
+ int bpf_prog_array_copy(struct bpf_prog_array __rcu *old_array,
+ 			struct bpf_prog *exclude_prog,
+ 			struct bpf_prog *include_prog,
+ 			struct bpf_prog_array **new_array);
+ 
+ #define __BPF_PROG_RUN_ARRAY(array, ctx, func, check_non_null)	\
+ 	({						\
+ 		struct bpf_prog **_prog, *__prog;	\
+ 		struct bpf_prog_array *_array;		\
+ 		u32 _ret = 1;				\
+ 		rcu_read_lock();			\
+ 		_array = rcu_dereference(array);	\
+ 		if (unlikely(check_non_null && !_array))\
+ 			goto _out;			\
+ 		_prog = _array->progs;			\
+ 		while ((__prog = READ_ONCE(*_prog))) {	\
+ 			_ret &= func(__prog, ctx);	\
+ 			_prog++;			\
+ 		}					\
+ _out:							\
+ 		rcu_read_unlock();			\
+ 		_ret;					\
+ 	 })
+ 
+ #define BPF_PROG_RUN_ARRAY(array, ctx, func)		\
+ 	__BPF_PROG_RUN_ARRAY(array, ctx, func, false)
+ 
+ #define BPF_PROG_RUN_ARRAY_CHECK(array, ctx, func)	\
+ 	__BPF_PROG_RUN_ARRAY(array, ctx, func, true)
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ DECLARE_PER_CPU(int, bpf_prog_active);
+ 
+ extern const struct file_operations bpf_map_fops;
+ extern const struct file_operations bpf_prog_fops;
+ 
+ #define BPF_PROG_TYPE(_id, _name) \
+ 	extern const struct bpf_prog_ops _name ## _prog_ops; \
+ 	extern const struct bpf_verifier_ops _name ## _verifier_ops;
+ #define BPF_MAP_TYPE(_id, _ops) \
+ 	extern const struct bpf_map_ops _ops;
+ #include <linux/bpf_types.h>
+ #undef BPF_PROG_TYPE
+ #undef BPF_MAP_TYPE
+ 
+ extern const struct bpf_verifier_ops tc_cls_act_analyzer_ops;
+ extern const struct bpf_verifier_ops xdp_analyzer_ops;
+ 
+ struct bpf_prog *bpf_prog_get(u32 ufd);
+ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type);
+ struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog, int i);
+ void bpf_prog_sub(struct bpf_prog *prog, int i);
+ struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog);
+ struct bpf_prog * __must_check bpf_prog_inc_not_zero(struct bpf_prog *prog);
+ void bpf_prog_put(struct bpf_prog *prog);
+ int __bpf_prog_charge(struct user_struct *user, u32 pages);
+ void __bpf_prog_uncharge(struct user_struct *user, u32 pages);
+ 
+ struct bpf_map *bpf_map_get_with_uref(u32 ufd);
+ struct bpf_map *__bpf_map_get(struct fd f);
+ struct bpf_map * __must_check bpf_map_inc(struct bpf_map *map, bool uref);
+ void bpf_map_put_with_uref(struct bpf_map *map);
+ void bpf_map_put(struct bpf_map *map);
+ int bpf_map_precharge_memlock(u32 pages);
+ void *bpf_map_area_alloc(size_t size, int numa_node);
+ void bpf_map_area_free(void *base);
+ 
+ extern int sysctl_unprivileged_bpf_disabled;
+ 
+ int bpf_map_new_fd(struct bpf_map *map, int flags);
+ int bpf_prog_new_fd(struct bpf_prog *prog);
+ 
+ int bpf_obj_pin_user(u32 ufd, const char __user *pathname);
+ int bpf_obj_get_user(const char __user *pathname, int flags);
+ 
+ int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,
+ 			   u64 flags);
+ int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,
+ 			    u64 flags);
+ 
+ int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value);
+ 
+ int bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				 void *key, void *value, u64 map_flags);
+ int bpf_fd_array_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ void bpf_fd_array_map_clear(struct bpf_map *map);
+ int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				void *key, void *value, u64 map_flags);
+ int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ 
+ int bpf_get_file_flag(int flags);
+ 
+ /* memcpy that is used with 8-byte aligned pointers, power-of-8 size and
+  * forced to use 'long' read/writes to try to atomically copy long counters.
+  * Best-effort only.  No barriers here, since it _will_ race with concurrent
+  * updates from BPF programs. Called from bpf syscall and mostly used with
+  * size 8 or 16 bytes, so ask compiler to inline it.
+  */
+ static inline void bpf_long_memcpy(void *dst, const void *src, u32 size)
+ {
+ 	const long *lsrc = src;
+ 	long *ldst = dst;
+ 
+ 	size /= sizeof(long);
+ 	while (size--)
+ 		*ldst++ = *lsrc++;
+ }
+ 
+ /* verify correctness of eBPF program */
+ int bpf_check(struct bpf_prog **fp, union bpf_attr *attr);
+ 
+ /* Map specifics */
+ struct net_device  *__dev_map_lookup_elem(struct bpf_map *map, u32 key);
+ void __dev_map_insert_ctx(struct bpf_map *map, u32 index);
+ void __dev_map_flush(struct bpf_map *map);
+ 
+ struct bpf_cpu_map_entry *__cpu_map_lookup_elem(struct bpf_map *map, u32 key);
+ void __cpu_map_insert_ctx(struct bpf_map *map, u32 index);
+ void __cpu_map_flush(struct bpf_map *map);
+ struct xdp_buff;
+ int cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_buff *xdp,
+ 		    struct net_device *dev_rx);
+ 
+ /* Return map's numa specified by userspace */
+ static inline int bpf_map_attr_numa_node(const union bpf_attr *attr)
+ {
+ 	return (attr->map_flags & BPF_F_NUMA_NODE) ?
+ 		attr->numa_node : NUMA_NO_NODE;
+ }
+ 
+ #else /* !CONFIG_BPF_SYSCALL */
+ static inline struct bpf_prog *bpf_prog_get(u32 ufd)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get_type(u32 ufd,
+ 						 enum bpf_prog_type type)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ static inline struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog,
+ 							  int i)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void bpf_prog_sub(struct bpf_prog *prog, int i)
+ {
+ }
+ 
+ static inline void bpf_prog_put(struct bpf_prog *prog)
+ {
+ }
+ 
+ static inline struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *__must_check
+ bpf_prog_inc_not_zero(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline int __bpf_prog_charge(struct user_struct *user, u32 pages)
+ {
+ 	return 0;
+ }
+ 
+ static inline void __bpf_prog_uncharge(struct user_struct *user, u32 pages)
+ {
+ }
+ 
+ static inline int bpf_obj_get_user(const char __user *pathname, int flags)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static inline struct net_device  *__dev_map_lookup_elem(struct bpf_map *map,
+ 						       u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void __dev_map_insert_ctx(struct bpf_map *map, u32 index)
+ {
+ }
+ 
+ static inline void __dev_map_flush(struct bpf_map *map)
+ {
+ }
+ 
+ static inline
+ struct bpf_cpu_map_entry *__cpu_map_lookup_elem(struct bpf_map *map, u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void __cpu_map_insert_ctx(struct bpf_map *map, u32 index)
+ {
+ }
+ 
+ static inline void __cpu_map_flush(struct bpf_map *map)
+ {
+ }
+ 
+ struct xdp_buff;
+ static inline int cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu,
+ 				  struct xdp_buff *xdp,
+ 				  struct net_device *dev_rx)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_BPF_SYSCALL */
+ 
+ #if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_BPF_SYSCALL)
+ struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key);
+ int sock_map_prog(struct bpf_map *map, struct bpf_prog *prog, u32 type);
+ #else
+ static inline struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline int sock_map_prog(struct bpf_map *map,
+ 				struct bpf_prog *prog,
+ 				u32 type)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ #endif
+ 
+ /* verifier prototypes for helper functions called from eBPF programs */
+ extern const struct bpf_func_proto bpf_map_lookup_elem_proto;
+ extern const struct bpf_func_proto bpf_map_update_elem_proto;
+ extern const struct bpf_func_proto bpf_map_delete_elem_proto;
+ 
+ extern const struct bpf_func_proto bpf_get_prandom_u32_proto;
+ extern const struct bpf_func_proto bpf_get_smp_processor_id_proto;
+ extern const struct bpf_func_proto bpf_get_numa_node_id_proto;
+ extern const struct bpf_func_proto bpf_tail_call_proto;
+ extern const struct bpf_func_proto bpf_ktime_get_ns_proto;
+ extern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;
+ extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
+ extern const struct bpf_func_proto bpf_get_current_comm_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_push_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_pop_proto;
+ extern const struct bpf_func_proto bpf_get_stackid_proto;
+ extern const struct bpf_func_proto bpf_sock_map_update_proto;
+ 
+ /* Shared helpers among cBPF and eBPF. */
+ void bpf_user_rnd_init_once(void);
+ u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  #endif /* _LINUX_BPF_H */
diff --cc kernel/events/core.c
index 3da42ad5a6b0,9660ee65fbef..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -7645,24 -7949,50 +7645,45 @@@ static int perf_tp_event_match(struct p
  	return 1;
  }
  
++<<<<<<< HEAD
 +void perf_tp_event(u64 addr, u64 count, void *record, int entry_size,
++=======
+ void perf_trace_run_bpf_submit(void *raw_data, int size, int rctx,
+ 			       struct trace_event_call *call, u64 count,
+ 			       struct pt_regs *regs, struct hlist_head *head,
+ 			       struct task_struct *task)
+ {
+ 	if (bpf_prog_array_valid(call)) {
+ 		*(struct pt_regs **)raw_data = regs;
+ 		if (!trace_call_bpf(call, raw_data) || hlist_empty(head)) {
+ 			perf_swevent_put_recursion_context(rctx);
+ 			return;
+ 		}
+ 	}
+ 	perf_tp_event(call->event.type, count, raw_data, size, regs, head,
+ 		      rctx, task, NULL);
+ }
+ EXPORT_SYMBOL_GPL(perf_trace_run_bpf_submit);
+ 
+ void perf_tp_event(u16 event_type, u64 count, void *record, int entry_size,
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  		   struct pt_regs *regs, struct hlist_head *head, int rctx,
 -		   struct task_struct *task, struct perf_event *event)
 +		   struct task_struct *task)
  {
  	struct perf_sample_data data;
 +	struct perf_event *event;
  
  	struct perf_raw_record raw = {
 -		.frag = {
 -			.size = entry_size,
 -			.data = record,
 -		},
 +		.size = entry_size,
 +		.data = record,
  	};
  
 -	perf_sample_data_init(&data, 0, 0);
 +	perf_sample_data_init(&data, addr, 0);
  	data.raw = &raw;
  
 -	perf_trace_buf_update(record, event_type);
 -
 -	/* Use the given event instead of the hlist */
 -	if (event) {
 +	hlist_for_each_entry_rcu(event, head, hlist_entry) {
  		if (perf_tp_event_match(event, &data, regs))
  			perf_swevent_event(event, count, &data, regs);
 -	} else {
 -		hlist_for_each_entry_rcu(event, head, hlist_entry) {
 -			if (perf_tp_event_match(event, &data, regs))
 -				perf_swevent_event(event, count, &data, regs);
 -		}
  	}
  
  	/*
@@@ -7742,6 -8072,127 +7763,130 @@@ static void perf_event_free_filter(stru
  	ftrace_profile_free_filter(event);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_BPF_SYSCALL
+ static void bpf_overflow_handler(struct perf_event *event,
+ 				 struct perf_sample_data *data,
+ 				 struct pt_regs *regs)
+ {
+ 	struct bpf_perf_event_data_kern ctx = {
+ 		.data = data,
+ 		.regs = regs,
+ 		.event = event,
+ 	};
+ 	int ret = 0;
+ 
+ 	preempt_disable();
+ 	if (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1))
+ 		goto out;
+ 	rcu_read_lock();
+ 	ret = BPF_PROG_RUN(event->prog, &ctx);
+ 	rcu_read_unlock();
+ out:
+ 	__this_cpu_dec(bpf_prog_active);
+ 	preempt_enable();
+ 	if (!ret)
+ 		return;
+ 
+ 	event->orig_overflow_handler(event, data, regs);
+ }
+ 
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	struct bpf_prog *prog;
+ 
+ 	if (event->overflow_handler_context)
+ 		/* hw breakpoint or kernel counter */
+ 		return -EINVAL;
+ 
+ 	if (event->prog)
+ 		return -EEXIST;
+ 
+ 	prog = bpf_prog_get_type(prog_fd, BPF_PROG_TYPE_PERF_EVENT);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	event->prog = prog;
+ 	event->orig_overflow_handler = READ_ONCE(event->overflow_handler);
+ 	WRITE_ONCE(event->overflow_handler, bpf_overflow_handler);
+ 	return 0;
+ }
+ 
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ 	struct bpf_prog *prog = event->prog;
+ 
+ 	if (!prog)
+ 		return;
+ 
+ 	WRITE_ONCE(event->overflow_handler, event->orig_overflow_handler);
+ 	event->prog = NULL;
+ 	bpf_prog_put(prog);
+ }
+ #else
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ }
+ #endif
+ 
+ static int perf_event_set_bpf_prog(struct perf_event *event, u32 prog_fd)
+ {
+ 	bool is_kprobe, is_tracepoint, is_syscall_tp;
+ 	struct bpf_prog *prog;
+ 	int ret;
+ 
+ 	if (event->attr.type != PERF_TYPE_TRACEPOINT)
+ 		return perf_event_set_bpf_handler(event, prog_fd);
+ 
+ 	is_kprobe = event->tp_event->flags & TRACE_EVENT_FL_UKPROBE;
+ 	is_tracepoint = event->tp_event->flags & TRACE_EVENT_FL_TRACEPOINT;
+ 	is_syscall_tp = is_syscall_trace_event(event->tp_event);
+ 	if (!is_kprobe && !is_tracepoint && !is_syscall_tp)
+ 		/* bpf programs can only be attached to u/kprobe or tracepoint */
+ 		return -EINVAL;
+ 
+ 	prog = bpf_prog_get(prog_fd);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	if ((is_kprobe && prog->type != BPF_PROG_TYPE_KPROBE) ||
+ 	    (is_tracepoint && prog->type != BPF_PROG_TYPE_TRACEPOINT) ||
+ 	    (is_syscall_tp && prog->type != BPF_PROG_TYPE_TRACEPOINT)) {
+ 		/* valid fd, but invalid bpf program type */
+ 		bpf_prog_put(prog);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (is_tracepoint || is_syscall_tp) {
+ 		int off = trace_event_get_offsets(event->tp_event);
+ 
+ 		if (prog->aux->max_ctx_offset > off) {
+ 			bpf_prog_put(prog);
+ 			return -EACCES;
+ 		}
+ 	}
+ 
+ 	ret = perf_event_attach_bpf_prog(event, prog);
+ 	if (ret)
+ 		bpf_prog_put(prog);
+ 	return ret;
+ }
+ 
+ static void perf_event_free_bpf_prog(struct perf_event *event)
+ {
+ 	if (event->attr.type != PERF_TYPE_TRACEPOINT) {
+ 		perf_event_free_bpf_handler(event);
+ 		return;
+ 	}
+ 	perf_event_detach_bpf_prog(event);
+ }
+ 
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  #else
  
  static inline void perf_tp_register(void)
diff --cc kernel/trace/trace_kprobe.c
index 0a5b1d7d9b73,abf92e478cfb..000000000000
--- a/kernel/trace/trace_kprobe.c
+++ b/kernel/trace/trace_kprobe.c
@@@ -1272,56 -1170,63 +1272,88 @@@ static int set_print_fmt(struct trace_p
  #ifdef CONFIG_PERF_EVENTS
  
  /* Kprobe profile handler */
 -static void
 -kprobe_perf_func(struct trace_kprobe *tk, struct pt_regs *regs)
 +static __kprobes void
 +kprobe_perf_func(struct trace_probe *tp, struct pt_regs *regs)
  {
++<<<<<<< HEAD
 +	struct ftrace_event_call *call = &tp->call;
++=======
+ 	struct trace_event_call *call = &tk->tp.call;
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  	struct kprobe_trace_entry_head *entry;
  	struct hlist_head *head;
  	int size, __size, dsize;
  	int rctx;
  
++<<<<<<< HEAD
 +	dsize = __get_data_size(tp, regs);
 +	__size = sizeof(*entry) + tp->size + dsize;
++=======
+ 	if (bpf_prog_array_valid(call) && !trace_call_bpf(call, regs))
+ 		return;
+ 
+ 	head = this_cpu_ptr(call->perf_events);
+ 	if (hlist_empty(head))
+ 		return;
+ 
+ 	dsize = __get_data_size(&tk->tp, regs);
+ 	__size = sizeof(*entry) + tk->tp.size + dsize;
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  	size = ALIGN(__size + sizeof(u32), sizeof(u64));
  	size -= sizeof(u32);
 +	if (WARN_ONCE(size > PERF_MAX_TRACE_SIZE,
 +		     "profile buffer not large enough"))
 +		return;
  
 -	entry = perf_trace_buf_alloc(size, NULL, &rctx);
 +	entry = perf_trace_buf_prepare(size, call->event.type, NULL, &rctx);
  	if (!entry)
  		return;
  
 -	entry->ip = (unsigned long)tk->rp.kp.addr;
 +	entry->ip = (unsigned long)tp->rp.kp.addr;
  	memset(&entry[1], 0, dsize);
 -	store_trace_args(sizeof(*entry), &tk->tp, regs, (u8 *)&entry[1], dsize);
 -	perf_trace_buf_submit(entry, size, rctx, call->event.type, 1, regs,
 -			      head, NULL, NULL);
 +	store_trace_args(sizeof(*entry), tp, regs, (u8 *)&entry[1], dsize);
 +
 +	head = this_cpu_ptr(call->perf_events);
 +	perf_trace_buf_submit(entry, size, rctx,
 +					entry->ip, 1, regs, head, NULL);
  }
 -NOKPROBE_SYMBOL(kprobe_perf_func);
  
  /* Kretprobe profile handler */
 -static void
 -kretprobe_perf_func(struct trace_kprobe *tk, struct kretprobe_instance *ri,
 +static __kprobes void
 +kretprobe_perf_func(struct trace_probe *tp, struct kretprobe_instance *ri,
  		    struct pt_regs *regs)
  {
++<<<<<<< HEAD
 +	struct ftrace_event_call *call = &tp->call;
++=======
+ 	struct trace_event_call *call = &tk->tp.call;
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  	struct kretprobe_trace_entry_head *entry;
  	struct hlist_head *head;
  	int size, __size, dsize;
  	int rctx;
  
++<<<<<<< HEAD
 +	dsize = __get_data_size(tp, regs);
 +	__size = sizeof(*entry) + tp->size + dsize;
++=======
+ 	if (bpf_prog_array_valid(call) && !trace_call_bpf(call, regs))
+ 		return;
+ 
+ 	head = this_cpu_ptr(call->perf_events);
+ 	if (hlist_empty(head))
+ 		return;
+ 
+ 	dsize = __get_data_size(&tk->tp, regs);
+ 	__size = sizeof(*entry) + tk->tp.size + dsize;
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  	size = ALIGN(__size + sizeof(u32), sizeof(u64));
  	size -= sizeof(u32);
 +	if (WARN_ONCE(size > PERF_MAX_TRACE_SIZE,
 +		     "profile buffer not large enough"))
 +		return;
  
 -	entry = perf_trace_buf_alloc(size, NULL, &rctx);
 +	entry = perf_trace_buf_prepare(size, call->event.type, NULL, &rctx);
  	if (!entry)
  		return;
  
diff --cc kernel/trace/trace_syscalls.c
index 7a1e2d2a9b1a,71a6af34d7a9..000000000000
--- a/kernel/trace/trace_syscalls.c
+++ b/kernel/trace/trace_syscalls.c
@@@ -547,11 -559,30 +547,36 @@@ static DECLARE_BITMAP(enabled_perf_exit
  static int sys_perf_refcount_enter;
  static int sys_perf_refcount_exit;
  
++<<<<<<< HEAD
++=======
+ static int perf_call_bpf_enter(struct trace_event_call *call, struct pt_regs *regs,
+ 			       struct syscall_metadata *sys_data,
+ 			       struct syscall_trace_enter *rec)
+ {
+ 	struct syscall_tp_t {
+ 		unsigned long long regs;
+ 		unsigned long syscall_nr;
+ 		unsigned long args[SYSCALL_DEFINE_MAXARGS];
+ 	} param;
+ 	int i;
+ 
+ 	*(struct pt_regs **)&param = regs;
+ 	param.syscall_nr = rec->nr;
+ 	for (i = 0; i < sys_data->nb_args; i++)
+ 		param.args[i] = rec->args[i];
+ 	return trace_call_bpf(call, &param);
+ }
+ 
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  static void perf_syscall_enter(void *ignore, struct pt_regs *regs, long id)
  {
  	struct syscall_metadata *sys_data;
  	struct syscall_trace_enter *rec;
  	struct hlist_head *head;
++<<<<<<< HEAD
++=======
+ 	bool valid_prog_array;
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  	int syscall_nr;
  	int rctx;
  	int size;
@@@ -567,7 -598,8 +592,12 @@@
  		return;
  
  	head = this_cpu_ptr(sys_data->enter_event->perf_events);
++<<<<<<< HEAD
 +	if (hlist_empty(head))
++=======
+ 	valid_prog_array = bpf_prog_array_valid(sys_data->enter_event);
+ 	if (!valid_prog_array && hlist_empty(head))
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  		return;
  
  	/* get the size after alignment with the u32 buffer size field */
@@@ -587,10 -614,20 +617,24 @@@
  	rec->nr = syscall_nr;
  	syscall_get_arguments(current, regs, 0, sys_data->nb_args,
  			       (unsigned long *)&rec->args);
++<<<<<<< HEAD
 +	perf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);
++=======
+ 
+ 	if ((valid_prog_array &&
+ 	     !perf_call_bpf_enter(sys_data->enter_event, regs, sys_data, rec)) ||
+ 	    hlist_empty(head)) {
+ 		perf_swevent_put_recursion_context(rctx);
+ 		return;
+ 	}
+ 
+ 	perf_trace_buf_submit(rec, size, rctx,
+ 			      sys_data->enter_event->event.type, 1, regs,
+ 			      head, NULL, NULL);
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  }
  
 -static int perf_sysenter_enable(struct trace_event_call *call)
 +static int perf_sysenter_enable(struct ftrace_event_call *call)
  {
  	int ret = 0;
  	int num;
@@@ -625,11 -661,27 +669,33 @@@ static void perf_sysenter_disable(struc
  	mutex_unlock(&syscall_trace_lock);
  }
  
++<<<<<<< HEAD
++=======
+ static int perf_call_bpf_exit(struct trace_event_call *call, struct pt_regs *regs,
+ 			      struct syscall_trace_exit *rec)
+ {
+ 	struct syscall_tp_t {
+ 		unsigned long long regs;
+ 		unsigned long syscall_nr;
+ 		unsigned long ret;
+ 	} param;
+ 
+ 	*(struct pt_regs **)&param = regs;
+ 	param.syscall_nr = rec->nr;
+ 	param.ret = rec->ret;
+ 	return trace_call_bpf(call, &param);
+ }
+ 
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  static void perf_syscall_exit(void *ignore, struct pt_regs *regs, long ret)
  {
  	struct syscall_metadata *sys_data;
  	struct syscall_trace_exit *rec;
  	struct hlist_head *head;
++<<<<<<< HEAD
++=======
+ 	bool valid_prog_array;
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  	int syscall_nr;
  	int rctx;
  	int size;
@@@ -645,7 -697,8 +711,12 @@@
  		return;
  
  	head = this_cpu_ptr(sys_data->exit_event->perf_events);
++<<<<<<< HEAD
 +	if (hlist_empty(head))
++=======
+ 	valid_prog_array = bpf_prog_array_valid(sys_data->exit_event);
+ 	if (!valid_prog_array && hlist_empty(head))
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  		return;
  
  	/* We can probably do that at build time */
@@@ -667,10 -711,19 +738,23 @@@
  
  	rec->nr = syscall_nr;
  	rec->ret = syscall_get_return_value(current, regs);
++<<<<<<< HEAD
 +	perf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);
++=======
+ 
+ 	if ((valid_prog_array &&
+ 	     !perf_call_bpf_exit(sys_data->exit_event, regs, rec)) ||
+ 	    hlist_empty(head)) {
+ 		perf_swevent_put_recursion_context(rctx);
+ 		return;
+ 	}
+ 
+ 	perf_trace_buf_submit(rec, size, rctx, sys_data->exit_event->event.type,
+ 			      1, regs, head, NULL, NULL);
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  }
  
 -static int perf_sysexit_enable(struct trace_event_call *call)
 +static int perf_sysexit_enable(struct ftrace_event_call *call)
  {
  	int ret = 0;
  	int num;
diff --cc kernel/trace/trace_uprobe.c
index 84f228258d8e,153c0e411461..000000000000
--- a/kernel/trace/trace_uprobe.c
+++ b/kernel/trace/trace_uprobe.c
@@@ -983,10 -1115,16 +983,20 @@@ static void uprobe_perf_print(struct tr
  	struct uprobe_trace_entry_head *entry;
  	struct hlist_head *head;
  	void *data;
 -	int size, esize;
 -	int rctx;
 +	int size, rctx, i;
  
++<<<<<<< HEAD
 +	size = SIZEOF_TRACE_ENTRY(is_ret_probe(tu));
 +	size = ALIGN(size + tu->size + sizeof(u32), sizeof(u64)) - sizeof(u32);
++=======
+ 	if (bpf_prog_array_valid(call) && !trace_call_bpf(call, regs))
+ 		return;
+ 
+ 	esize = SIZEOF_TRACE_ENTRY(is_ret_probe(tu));
+ 
+ 	size = esize + tu->tp.size + dsize;
+ 	size = ALIGN(size + sizeof(u32), sizeof(u64)) - sizeof(u32);
++>>>>>>> e87c6bc3852b (bpf: permit multiple bpf attachments for a single perf event)
  	if (WARN_ONCE(size > PERF_MAX_TRACE_SIZE, "profile buffer not large enough"))
  		return;
  
* Unmerged path include/linux/trace_events.h
* Unmerged path include/trace/perf.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/trace/bpf_trace.c
* Unmerged path include/linux/bpf.h
* Unmerged path include/linux/trace_events.h
* Unmerged path include/trace/perf.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/events/core.c
* Unmerged path kernel/trace/bpf_trace.c
* Unmerged path kernel/trace/trace_kprobe.c
* Unmerged path kernel/trace/trace_syscalls.c
* Unmerged path kernel/trace/trace_uprobe.c
