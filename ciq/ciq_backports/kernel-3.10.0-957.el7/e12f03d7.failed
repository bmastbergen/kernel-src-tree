perf/core: Implement the 'perf_kprobe' PMU

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Song Liu <songliubraving@fb.com>
commit e12f03d7031a977356e3d7b75a68c2185ff8d155
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/e12f03d7.failed

A new PMU type, perf_kprobe is added. Based on attr from perf_event_open(),
perf_kprobe creates a kprobe (or kretprobe) for the perf_event. This
kprobe is private to this perf_event, and thus not added to global
lists, and not available in tracefs.

Two functions, create_local_trace_kprobe() and
destroy_local_trace_kprobe()  are added to created and destroy these
local trace_kprobe.

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Yonghong Song <yhs@fb.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Cc: <daniel@iogearbox.net>
	Cc: <davem@davemloft.net>
	Cc: <kernel-team@fb.com>
	Cc: <rostedt@goodmis.org>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20171206224518.3598254-6-songliubraving@fb.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit e12f03d7031a977356e3d7b75a68c2185ff8d155)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/trace_events.h
#	kernel/events/core.c
#	kernel/trace/trace_kprobe.c
#	kernel/trace/trace_probe.h
diff --cc kernel/events/core.c
index 3da42ad5a6b0,333735531968..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -7742,6 -8070,142 +7810,145 @@@ static void perf_event_free_filter(stru
  	ftrace_profile_free_filter(event);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_BPF_SYSCALL
+ static void bpf_overflow_handler(struct perf_event *event,
+ 				 struct perf_sample_data *data,
+ 				 struct pt_regs *regs)
+ {
+ 	struct bpf_perf_event_data_kern ctx = {
+ 		.data = data,
+ 		.event = event,
+ 	};
+ 	int ret = 0;
+ 
+ 	ctx.regs = perf_arch_bpf_user_pt_regs(regs);
+ 	preempt_disable();
+ 	if (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1))
+ 		goto out;
+ 	rcu_read_lock();
+ 	ret = BPF_PROG_RUN(event->prog, &ctx);
+ 	rcu_read_unlock();
+ out:
+ 	__this_cpu_dec(bpf_prog_active);
+ 	preempt_enable();
+ 	if (!ret)
+ 		return;
+ 
+ 	event->orig_overflow_handler(event, data, regs);
+ }
+ 
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	struct bpf_prog *prog;
+ 
+ 	if (event->overflow_handler_context)
+ 		/* hw breakpoint or kernel counter */
+ 		return -EINVAL;
+ 
+ 	if (event->prog)
+ 		return -EEXIST;
+ 
+ 	prog = bpf_prog_get_type(prog_fd, BPF_PROG_TYPE_PERF_EVENT);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	event->prog = prog;
+ 	event->orig_overflow_handler = READ_ONCE(event->overflow_handler);
+ 	WRITE_ONCE(event->overflow_handler, bpf_overflow_handler);
+ 	return 0;
+ }
+ 
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ 	struct bpf_prog *prog = event->prog;
+ 
+ 	if (!prog)
+ 		return;
+ 
+ 	WRITE_ONCE(event->overflow_handler, event->orig_overflow_handler);
+ 	event->prog = NULL;
+ 	bpf_prog_put(prog);
+ }
+ #else
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ }
+ #endif
+ 
+ /*
+  * returns true if the event is a tracepoint, or a kprobe/upprobe created
+  * with perf_event_open()
+  */
+ static inline bool perf_event_is_tracing(struct perf_event *event)
+ {
+ 	if (event->pmu == &perf_tracepoint)
+ 		return true;
+ #ifdef CONFIG_KPROBE_EVENTS
+ 	if (event->pmu == &perf_kprobe)
+ 		return true;
+ #endif
+ 	return false;
+ }
+ 
+ static int perf_event_set_bpf_prog(struct perf_event *event, u32 prog_fd)
+ {
+ 	bool is_kprobe, is_tracepoint, is_syscall_tp;
+ 	struct bpf_prog *prog;
+ 	int ret;
+ 
+ 	if (!perf_event_is_tracing(event))
+ 		return perf_event_set_bpf_handler(event, prog_fd);
+ 
+ 	is_kprobe = event->tp_event->flags & TRACE_EVENT_FL_UKPROBE;
+ 	is_tracepoint = event->tp_event->flags & TRACE_EVENT_FL_TRACEPOINT;
+ 	is_syscall_tp = is_syscall_trace_event(event->tp_event);
+ 	if (!is_kprobe && !is_tracepoint && !is_syscall_tp)
+ 		/* bpf programs can only be attached to u/kprobe or tracepoint */
+ 		return -EINVAL;
+ 
+ 	prog = bpf_prog_get(prog_fd);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	if ((is_kprobe && prog->type != BPF_PROG_TYPE_KPROBE) ||
+ 	    (is_tracepoint && prog->type != BPF_PROG_TYPE_TRACEPOINT) ||
+ 	    (is_syscall_tp && prog->type != BPF_PROG_TYPE_TRACEPOINT)) {
+ 		/* valid fd, but invalid bpf program type */
+ 		bpf_prog_put(prog);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (is_tracepoint || is_syscall_tp) {
+ 		int off = trace_event_get_offsets(event->tp_event);
+ 
+ 		if (prog->aux->max_ctx_offset > off) {
+ 			bpf_prog_put(prog);
+ 			return -EACCES;
+ 		}
+ 	}
+ 
+ 	ret = perf_event_attach_bpf_prog(event, prog);
+ 	if (ret)
+ 		bpf_prog_put(prog);
+ 	return ret;
+ }
+ 
+ static void perf_event_free_bpf_prog(struct perf_event *event)
+ {
+ 	if (!perf_event_is_tracing(event)) {
+ 		perf_event_free_bpf_handler(event);
+ 		return;
+ 	}
+ 	perf_event_detach_bpf_prog(event);
+ }
+ 
++>>>>>>> e12f03d7031a (perf/core: Implement the 'perf_kprobe' PMU)
  #else
  
  static inline void perf_tp_register(void)
diff --cc kernel/trace/trace_kprobe.c
index 0a5b1d7d9b73,246c786c851c..000000000000
--- a/kernel/trace/trace_kprobe.c
+++ b/kernel/trace/trace_kprobe.c
@@@ -494,9 -439,14 +494,20 @@@ disable_trace_probe(struct trace_probe 
  		wait = 1;
  	}
  
++<<<<<<< HEAD
 + out_unlock:
 +	mutex_unlock(&probe_enable_lock);
 +
++=======
+ 	/*
+ 	 * if tk is not added to any list, it must be a local trace_kprobe
+ 	 * created with perf_event_open. We don't need to wait for these
+ 	 * trace_kprobes
+ 	 */
+ 	if (list_empty(&tk->list))
+ 		wait = 0;
+  out:
++>>>>>>> e12f03d7031a (perf/core: Implement the 'perf_kprobe' PMU)
  	if (wait) {
  		/*
  		 * Synchronize with kprobe_trace_func/kretprobe_trace_func
@@@ -1403,35 -1321,43 +1414,61 @@@ static struct trace_event_functions kpr
  	.trace		= print_kprobe_event
  };
  
++<<<<<<< HEAD
 +static int register_probe_event(struct trace_probe *tp)
 +{
 +	struct ftrace_event_call *call = &tp->call;
 +	int ret;
 +
 +	/* Initialize ftrace_event_call */
++=======
+ static inline void init_trace_event_call(struct trace_kprobe *tk,
+ 					 struct trace_event_call *call)
+ {
++>>>>>>> e12f03d7031a (perf/core: Implement the 'perf_kprobe' PMU)
  	INIT_LIST_HEAD(&call->class->fields);
 -	if (trace_kprobe_is_return(tk)) {
 +	if (trace_probe_is_return(tp)) {
  		call->event.funcs = &kretprobe_funcs;
  		call->class->define_fields = kretprobe_event_define_fields;
  	} else {
  		call->event.funcs = &kprobe_funcs;
  		call->class->define_fields = kprobe_event_define_fields;
  	}
++<<<<<<< HEAD
 +	if (set_print_fmt(tp) < 0)
++=======
+ 
+ 	call->flags = TRACE_EVENT_FL_KPROBE;
+ 	call->class->reg = kprobe_register;
+ 	call->data = tk;
+ }
+ 
+ static int register_kprobe_event(struct trace_kprobe *tk)
+ {
+ 	struct trace_event_call *call = &tk->tp.call;
+ 	int ret = 0;
+ 
+ 	init_trace_event_call(tk, call);
+ 
+ 	if (set_print_fmt(&tk->tp, trace_kprobe_is_return(tk)) < 0)
++>>>>>>> e12f03d7031a (perf/core: Implement the 'perf_kprobe' PMU)
  		return -ENOMEM;
 -	ret = register_trace_event(&call->event);
 +	ret = register_ftrace_event(&call->event);
  	if (!ret) {
  		kfree(call->print_fmt);
  		return -ENODEV;
  	}
++<<<<<<< HEAD
 +	call->flags = 0;
 +	call->class->reg = kprobe_register;
 +	call->data = tp;
++=======
++>>>>>>> e12f03d7031a (perf/core: Implement the 'perf_kprobe' PMU)
  	ret = trace_add_event_call(call);
  	if (ret) {
 -		pr_info("Failed to register kprobe event: %s\n",
 -			trace_event_name(call));
 +		pr_info("Failed to register kprobe event: %s\n", call->name);
  		kfree(call->print_fmt);
 -		unregister_trace_event(&call->event);
 +		unregister_ftrace_event(&call->event);
  	}
  	return ret;
  }
@@@ -1447,7 -1373,67 +1484,71 @@@ static int unregister_probe_event(struc
  	return ret;
  }
  
++<<<<<<< HEAD
 +/* Make a debugfs interface for controlling probe points */
++=======
+ #ifdef CONFIG_PERF_EVENTS
+ /* create a trace_kprobe, but don't add it to global lists */
+ struct trace_event_call *
+ create_local_trace_kprobe(char *func, void *addr, unsigned long offs,
+ 			  bool is_return)
+ {
+ 	struct trace_kprobe *tk;
+ 	int ret;
+ 	char *event;
+ 
+ 	/*
+ 	 * local trace_kprobes are not added to probe_list, so they are never
+ 	 * searched in find_trace_kprobe(). Therefore, there is no concern of
+ 	 * duplicated name here.
+ 	 */
+ 	event = func ? func : "DUMMY_EVENT";
+ 
+ 	tk = alloc_trace_kprobe(KPROBE_EVENT_SYSTEM, event, (void *)addr, func,
+ 				offs, 0 /* maxactive */, 0 /* nargs */,
+ 				is_return);
+ 
+ 	if (IS_ERR(tk)) {
+ 		pr_info("Failed to allocate trace_probe.(%d)\n",
+ 			(int)PTR_ERR(tk));
+ 		return ERR_CAST(tk);
+ 	}
+ 
+ 	init_trace_event_call(tk, &tk->tp.call);
+ 
+ 	if (set_print_fmt(&tk->tp, trace_kprobe_is_return(tk)) < 0) {
+ 		ret = -ENOMEM;
+ 		goto error;
+ 	}
+ 
+ 	ret = __register_trace_kprobe(tk);
+ 	if (ret < 0)
+ 		goto error;
+ 
+ 	return &tk->tp.call;
+ error:
+ 	free_trace_kprobe(tk);
+ 	return ERR_PTR(ret);
+ }
+ 
+ void destroy_local_trace_kprobe(struct trace_event_call *event_call)
+ {
+ 	struct trace_kprobe *tk;
+ 
+ 	tk = container_of(event_call, struct trace_kprobe, tp.call);
+ 
+ 	if (trace_probe_is_enabled(&tk->tp)) {
+ 		WARN_ON(1);
+ 		return;
+ 	}
+ 
+ 	__unregister_trace_kprobe(tk);
+ 	free_trace_kprobe(tk);
+ }
+ #endif /* CONFIG_PERF_EVENTS */
+ 
+ /* Make a tracefs interface for controlling probe points */
++>>>>>>> e12f03d7031a (perf/core: Implement the 'perf_kprobe' PMU)
  static __init int init_kprobe_trace(void)
  {
  	struct dentry *d_tracer;
diff --cc kernel/trace/trace_probe.h
index 5b4f8bdfb3a5,e3d940a49dcd..000000000000
--- a/kernel/trace/trace_probe.h
+++ b/kernel/trace/trace_probe.h
@@@ -352,3 -402,12 +352,15 @@@ store_trace_args(int ent_size, struct t
  				   data + tp->args[i].offset);
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ extern int set_print_fmt(struct trace_probe *tp, bool is_return);
+ 
+ #ifdef CONFIG_PERF_EVENTS
+ extern struct trace_event_call *
+ create_local_trace_kprobe(char *func, void *addr, unsigned long offs,
+ 			  bool is_return);
+ extern void destroy_local_trace_kprobe(struct trace_event_call *event_call);
+ #endif
++>>>>>>> e12f03d7031a (perf/core: Implement the 'perf_kprobe' PMU)
* Unmerged path include/linux/trace_events.h
* Unmerged path include/linux/trace_events.h
* Unmerged path kernel/events/core.c
diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index ae2877cbe443..f8fecd674709 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -8,6 +8,7 @@
 #include <linux/module.h>
 #include <linux/kprobes.h>
 #include "trace.h"
+#include "trace_probe.h"
 
 static char __percpu *perf_trace_buf[PERF_NR_CONTEXTS];
 
@@ -213,6 +214,54 @@ void perf_trace_destroy(struct perf_event *p_event)
 	mutex_unlock(&event_mutex);
 }
 
+#ifdef CONFIG_KPROBE_EVENTS
+int perf_kprobe_init(struct perf_event *p_event, bool is_retprobe)
+{
+	int ret;
+	char *func = NULL;
+	struct trace_event_call *tp_event;
+
+	if (p_event->attr.kprobe_func) {
+		func = kzalloc(KSYM_NAME_LEN, GFP_KERNEL);
+		if (!func)
+			return -ENOMEM;
+		ret = strncpy_from_user(
+			func, u64_to_user_ptr(p_event->attr.kprobe_func),
+			KSYM_NAME_LEN);
+		if (ret < 0)
+			goto out;
+
+		if (func[0] == '\0') {
+			kfree(func);
+			func = NULL;
+		}
+	}
+
+	tp_event = create_local_trace_kprobe(
+		func, (void *)(unsigned long)(p_event->attr.kprobe_addr),
+		p_event->attr.probe_offset, is_retprobe);
+	if (IS_ERR(tp_event)) {
+		ret = PTR_ERR(tp_event);
+		goto out;
+	}
+
+	ret = perf_trace_event_init(tp_event, p_event);
+	if (ret)
+		destroy_local_trace_kprobe(tp_event);
+out:
+	kfree(func);
+	return ret;
+}
+
+void perf_kprobe_destroy(struct perf_event *p_event)
+{
+	perf_trace_event_close(p_event);
+	perf_trace_event_unreg(p_event);
+
+	destroy_local_trace_kprobe(p_event->tp_event);
+}
+#endif /* CONFIG_KPROBE_EVENTS */
+
 int perf_trace_add(struct perf_event *p_event, int flags)
 {
 	struct ftrace_event_call *tp_event = p_event->tp_event;
* Unmerged path kernel/trace/trace_kprobe.c
* Unmerged path kernel/trace/trace_probe.h
