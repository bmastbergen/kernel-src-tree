md/r5cache: generate R5LOG_PAYLOAD_FLUSH

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] r5cache: generate R5LOG_PAYLOAD_FLUSH (Nigel Croxon) [1494474]
Rebuild_FUZZ: 96.10%
commit-author Song Liu <songliubraving@fb.com>
commit ea17481fb48888fa11f412766bde36be9171247e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/ea17481f.failed

In r5c_finish_stripe_write_out(), R5LOG_PAYLOAD_FLUSH is append to
log->current_io.

Appending R5LOG_PAYLOAD_FLUSH in quiesce needs extra writes to
journal. To simplify the logic, we just skip R5LOG_PAYLOAD_FLUSH in
quiesce.

Even R5LOG_PAYLOAD_FLUSH supports multiple stripes per payload.
However, current implementation is one stripe per R5LOG_PAYLOAD_FLUSH,
which is simpler.

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit ea17481fb48888fa11f412766bde36be9171247e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5-cache.c
diff --cc drivers/md/raid5-cache.c
index cf3561d68686,64493132470b..000000000000
--- a/drivers/md/raid5-cache.c
+++ b/drivers/md/raid5-cache.c
@@@ -539,6 -612,66 +539,69 @@@ static void r5l_log_endio(struct bio *b
  
  	if (log->need_cache_flush)
  		md_wakeup_thread(log->rdev->mddev->thread);
++<<<<<<< HEAD
++=======
+ 
+ 	if (io->has_null_flush) {
+ 		struct bio *bi;
+ 
+ 		WARN_ON(bio_list_empty(&io->flush_barriers));
+ 		while ((bi = bio_list_pop(&io->flush_barriers)) != NULL) {
+ 			bio_endio(bi);
+ 			atomic_dec(&io->pending_stripe);
+ 		}
+ 	}
+ 
+ 	/* finish flush only io_unit and PAYLOAD_FLUSH only io_unit */
+ 	if (atomic_read(&io->pending_stripe) == 0)
+ 		__r5l_stripe_write_finished(io);
+ }
+ 
+ static void r5l_do_submit_io(struct r5l_log *log, struct r5l_io_unit *io)
+ {
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&log->io_list_lock, flags);
+ 	__r5l_set_io_unit_state(io, IO_UNIT_IO_START);
+ 	spin_unlock_irqrestore(&log->io_list_lock, flags);
+ 
+ 	if (io->has_flush)
+ 		io->current_bio->bi_opf |= REQ_PREFLUSH;
+ 	if (io->has_fua)
+ 		io->current_bio->bi_opf |= REQ_FUA;
+ 	submit_bio(io->current_bio);
+ 
+ 	if (!io->split_bio)
+ 		return;
+ 
+ 	if (io->has_flush)
+ 		io->split_bio->bi_opf |= REQ_PREFLUSH;
+ 	if (io->has_fua)
+ 		io->split_bio->bi_opf |= REQ_FUA;
+ 	submit_bio(io->split_bio);
+ }
+ 
+ /* deferred io_unit will be dispatched here */
+ static void r5l_submit_io_async(struct work_struct *work)
+ {
+ 	struct r5l_log *log = container_of(work, struct r5l_log,
+ 					   deferred_io_work);
+ 	struct r5l_io_unit *io = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&log->io_list_lock, flags);
+ 	if (!list_empty(&log->running_ios)) {
+ 		io = list_first_entry(&log->running_ios, struct r5l_io_unit,
+ 				      log_sibling);
+ 		if (!io->io_deferred)
+ 			io = NULL;
+ 		else
+ 			io->io_deferred = 0;
+ 	}
+ 	spin_unlock_irqrestore(&log->io_list_lock, flags);
+ 	if (io)
+ 		r5l_do_submit_io(log, io);
++>>>>>>> ea17481fb488 (md/r5cache: generate R5LOG_PAYLOAD_FLUSH)
  }
  
  static void r5c_disable_writeback_async(struct work_struct *work)
@@@ -2422,17 -2782,50 +2520,57 @@@ void r5c_finish_stripe_write_out(struc
  	if (do_wakeup)
  		wake_up(&conf->wait_for_overlap);
  
 -	spin_lock_irq(&log->stripe_in_journal_lock);
 +	spin_lock_irq(&conf->log->stripe_in_journal_lock);
  	list_del_init(&sh->r5c);
 -	spin_unlock_irq(&log->stripe_in_journal_lock);
 +	spin_unlock_irq(&conf->log->stripe_in_journal_lock);
  	sh->log_start = MaxSector;
++<<<<<<< HEAD
 +	atomic_dec(&conf->log->stripe_in_journal_count);
 +	r5c_update_log_state(conf->log);
++=======
+ 
+ 	atomic_dec(&log->stripe_in_journal_count);
+ 	r5c_update_log_state(log);
+ 
+ 	/* stop counting this stripe in big_stripe_tree */
+ 	if (test_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state) ||
+ 	    test_bit(STRIPE_R5C_FULL_STRIPE, &sh->state)) {
+ 		tree_index = r5c_tree_index(conf, sh->sector);
+ 		spin_lock(&log->tree_lock);
+ 		pslot = radix_tree_lookup_slot(&log->big_stripe_tree,
+ 					       tree_index);
+ 		BUG_ON(pslot == NULL);
+ 		refcount = (uintptr_t)radix_tree_deref_slot_protected(
+ 			pslot, &log->tree_lock) >>
+ 			R5C_RADIX_COUNT_SHIFT;
+ 		if (refcount == 1)
+ 			radix_tree_delete(&log->big_stripe_tree, tree_index);
+ 		else
+ 			radix_tree_replace_slot(
+ 				&log->big_stripe_tree, pslot,
+ 				(void *)((refcount - 1) << R5C_RADIX_COUNT_SHIFT));
+ 		spin_unlock(&log->tree_lock);
+ 	}
+ 
+ 	if (test_and_clear_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state)) {
+ 		BUG_ON(atomic_read(&conf->r5c_cached_partial_stripes) == 0);
+ 		atomic_dec(&conf->r5c_flushing_partial_stripes);
+ 		atomic_dec(&conf->r5c_cached_partial_stripes);
+ 	}
+ 
+ 	if (test_and_clear_bit(STRIPE_R5C_FULL_STRIPE, &sh->state)) {
+ 		BUG_ON(atomic_read(&conf->r5c_cached_full_stripes) == 0);
+ 		atomic_dec(&conf->r5c_flushing_full_stripes);
+ 		atomic_dec(&conf->r5c_cached_full_stripes);
+ 	}
+ 
+ 	r5l_append_flush_payload(log, sh->sector);
++>>>>>>> ea17481fb488 (md/r5cache: generate R5LOG_PAYLOAD_FLUSH)
  }
  
 -int r5c_cache_data(struct r5l_log *log, struct stripe_head *sh)
 +int
 +r5c_cache_data(struct r5l_log *log, struct stripe_head *sh,
 +	       struct stripe_head_state *s)
  {
  	struct r5conf *conf = sh->raid_conf;
  	int pages = 0;
* Unmerged path drivers/md/raid5-cache.c
