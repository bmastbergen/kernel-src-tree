perf/core: Implement the 'perf_uprobe' PMU

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Song Liu <songliubraving@fb.com>
commit 33ea4b24277b06dbc55d7f5772a46f029600255e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/33ea4b24.failed

This patch adds perf_uprobe support with similar pattern as previous
patch (for kprobe).

Two functions, create_local_trace_uprobe() and
destroy_local_trace_uprobe(), are created so a uprobe can be created
and attached to the file descriptor created by perf_event_open().

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Yonghong Song <yhs@fb.com>
	Reviewed-by: Josef Bacik <jbacik@fb.com>
	Cc: <daniel@iogearbox.net>
	Cc: <davem@davemloft.net>
	Cc: <kernel-team@fb.com>
	Cc: <rostedt@goodmis.org>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20171206224518.3598254-7-songliubraving@fb.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 33ea4b24277b06dbc55d7f5772a46f029600255e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/trace_events.h
#	kernel/events/core.c
#	kernel/trace/trace_event_perf.c
#	kernel/trace/trace_probe.h
#	kernel/trace/trace_uprobe.c
diff --cc kernel/events/core.c
index 3da42ad5a6b0,5a54630db86b..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -7732,9 -7992,119 +7732,125 @@@ static struct pmu perf_tracepoint = 
  	.read		= perf_swevent_read,
  };
  
++<<<<<<< HEAD
 +static inline void perf_tp_register(void)
 +{
 +	perf_pmu_register(&perf_tracepoint, "tracepoint", PERF_TYPE_TRACEPOINT);
++=======
+ #if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)
+ /*
+  * Flags in config, used by dynamic PMU kprobe and uprobe
+  * The flags should match following PMU_FORMAT_ATTR().
+  *
+  * PERF_PROBE_CONFIG_IS_RETPROBE if set, create kretprobe/uretprobe
+  *                               if not set, create kprobe/uprobe
+  */
+ enum perf_probe_config {
+ 	PERF_PROBE_CONFIG_IS_RETPROBE = 1U << 0,  /* [k,u]retprobe */
+ };
+ 
+ PMU_FORMAT_ATTR(retprobe, "config:0");
+ 
+ static struct attribute *probe_attrs[] = {
+ 	&format_attr_retprobe.attr,
+ 	NULL,
+ };
+ 
+ static struct attribute_group probe_format_group = {
+ 	.name = "format",
+ 	.attrs = probe_attrs,
+ };
+ 
+ static const struct attribute_group *probe_attr_groups[] = {
+ 	&probe_format_group,
+ 	NULL,
+ };
+ #endif
+ 
+ #ifdef CONFIG_KPROBE_EVENTS
+ static int perf_kprobe_event_init(struct perf_event *event);
+ static struct pmu perf_kprobe = {
+ 	.task_ctx_nr	= perf_sw_context,
+ 	.event_init	= perf_kprobe_event_init,
+ 	.add		= perf_trace_add,
+ 	.del		= perf_trace_del,
+ 	.start		= perf_swevent_start,
+ 	.stop		= perf_swevent_stop,
+ 	.read		= perf_swevent_read,
+ 	.attr_groups	= probe_attr_groups,
+ };
+ 
+ static int perf_kprobe_event_init(struct perf_event *event)
+ {
+ 	int err;
+ 	bool is_retprobe;
+ 
+ 	if (event->attr.type != perf_kprobe.type)
+ 		return -ENOENT;
+ 	/*
+ 	 * no branch sampling for probe events
+ 	 */
+ 	if (has_branch_stack(event))
+ 		return -EOPNOTSUPP;
+ 
+ 	is_retprobe = event->attr.config & PERF_PROBE_CONFIG_IS_RETPROBE;
+ 	err = perf_kprobe_init(event, is_retprobe);
+ 	if (err)
+ 		return err;
+ 
+ 	event->destroy = perf_kprobe_destroy;
+ 
+ 	return 0;
+ }
+ #endif /* CONFIG_KPROBE_EVENTS */
+ 
+ #ifdef CONFIG_UPROBE_EVENTS
+ static int perf_uprobe_event_init(struct perf_event *event);
+ static struct pmu perf_uprobe = {
+ 	.task_ctx_nr	= perf_sw_context,
+ 	.event_init	= perf_uprobe_event_init,
+ 	.add		= perf_trace_add,
+ 	.del		= perf_trace_del,
+ 	.start		= perf_swevent_start,
+ 	.stop		= perf_swevent_stop,
+ 	.read		= perf_swevent_read,
+ 	.attr_groups	= probe_attr_groups,
+ };
+ 
+ static int perf_uprobe_event_init(struct perf_event *event)
+ {
+ 	int err;
+ 	bool is_retprobe;
+ 
+ 	if (event->attr.type != perf_uprobe.type)
+ 		return -ENOENT;
+ 	/*
+ 	 * no branch sampling for probe events
+ 	 */
+ 	if (has_branch_stack(event))
+ 		return -EOPNOTSUPP;
+ 
+ 	is_retprobe = event->attr.config & PERF_PROBE_CONFIG_IS_RETPROBE;
+ 	err = perf_uprobe_init(event, is_retprobe);
+ 	if (err)
+ 		return err;
+ 
+ 	event->destroy = perf_uprobe_destroy;
+ 
+ 	return 0;
+ }
+ #endif /* CONFIG_UPROBE_EVENTS */
+ 
+ static inline void perf_tp_register(void)
+ {
+ 	perf_pmu_register(&perf_tracepoint, "tracepoint", PERF_TYPE_TRACEPOINT);
+ #ifdef CONFIG_KPROBE_EVENTS
+ 	perf_pmu_register(&perf_kprobe, "kprobe", -1);
+ #endif
+ #ifdef CONFIG_UPROBE_EVENTS
+ 	perf_pmu_register(&perf_uprobe, "uprobe", -1);
+ #endif
++>>>>>>> 33ea4b24277b (perf/core: Implement the 'perf_uprobe' PMU)
  }
  
  static void perf_event_free_filter(struct perf_event *event)
@@@ -7742,6 -8112,146 +7858,149 @@@
  	ftrace_profile_free_filter(event);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_BPF_SYSCALL
+ static void bpf_overflow_handler(struct perf_event *event,
+ 				 struct perf_sample_data *data,
+ 				 struct pt_regs *regs)
+ {
+ 	struct bpf_perf_event_data_kern ctx = {
+ 		.data = data,
+ 		.event = event,
+ 	};
+ 	int ret = 0;
+ 
+ 	ctx.regs = perf_arch_bpf_user_pt_regs(regs);
+ 	preempt_disable();
+ 	if (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1))
+ 		goto out;
+ 	rcu_read_lock();
+ 	ret = BPF_PROG_RUN(event->prog, &ctx);
+ 	rcu_read_unlock();
+ out:
+ 	__this_cpu_dec(bpf_prog_active);
+ 	preempt_enable();
+ 	if (!ret)
+ 		return;
+ 
+ 	event->orig_overflow_handler(event, data, regs);
+ }
+ 
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	struct bpf_prog *prog;
+ 
+ 	if (event->overflow_handler_context)
+ 		/* hw breakpoint or kernel counter */
+ 		return -EINVAL;
+ 
+ 	if (event->prog)
+ 		return -EEXIST;
+ 
+ 	prog = bpf_prog_get_type(prog_fd, BPF_PROG_TYPE_PERF_EVENT);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	event->prog = prog;
+ 	event->orig_overflow_handler = READ_ONCE(event->overflow_handler);
+ 	WRITE_ONCE(event->overflow_handler, bpf_overflow_handler);
+ 	return 0;
+ }
+ 
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ 	struct bpf_prog *prog = event->prog;
+ 
+ 	if (!prog)
+ 		return;
+ 
+ 	WRITE_ONCE(event->overflow_handler, event->orig_overflow_handler);
+ 	event->prog = NULL;
+ 	bpf_prog_put(prog);
+ }
+ #else
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ }
+ #endif
+ 
+ /*
+  * returns true if the event is a tracepoint, or a kprobe/upprobe created
+  * with perf_event_open()
+  */
+ static inline bool perf_event_is_tracing(struct perf_event *event)
+ {
+ 	if (event->pmu == &perf_tracepoint)
+ 		return true;
+ #ifdef CONFIG_KPROBE_EVENTS
+ 	if (event->pmu == &perf_kprobe)
+ 		return true;
+ #endif
+ #ifdef CONFIG_UPROBE_EVENTS
+ 	if (event->pmu == &perf_uprobe)
+ 		return true;
+ #endif
+ 	return false;
+ }
+ 
+ static int perf_event_set_bpf_prog(struct perf_event *event, u32 prog_fd)
+ {
+ 	bool is_kprobe, is_tracepoint, is_syscall_tp;
+ 	struct bpf_prog *prog;
+ 	int ret;
+ 
+ 	if (!perf_event_is_tracing(event))
+ 		return perf_event_set_bpf_handler(event, prog_fd);
+ 
+ 	is_kprobe = event->tp_event->flags & TRACE_EVENT_FL_UKPROBE;
+ 	is_tracepoint = event->tp_event->flags & TRACE_EVENT_FL_TRACEPOINT;
+ 	is_syscall_tp = is_syscall_trace_event(event->tp_event);
+ 	if (!is_kprobe && !is_tracepoint && !is_syscall_tp)
+ 		/* bpf programs can only be attached to u/kprobe or tracepoint */
+ 		return -EINVAL;
+ 
+ 	prog = bpf_prog_get(prog_fd);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	if ((is_kprobe && prog->type != BPF_PROG_TYPE_KPROBE) ||
+ 	    (is_tracepoint && prog->type != BPF_PROG_TYPE_TRACEPOINT) ||
+ 	    (is_syscall_tp && prog->type != BPF_PROG_TYPE_TRACEPOINT)) {
+ 		/* valid fd, but invalid bpf program type */
+ 		bpf_prog_put(prog);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (is_tracepoint || is_syscall_tp) {
+ 		int off = trace_event_get_offsets(event->tp_event);
+ 
+ 		if (prog->aux->max_ctx_offset > off) {
+ 			bpf_prog_put(prog);
+ 			return -EACCES;
+ 		}
+ 	}
+ 
+ 	ret = perf_event_attach_bpf_prog(event, prog);
+ 	if (ret)
+ 		bpf_prog_put(prog);
+ 	return ret;
+ }
+ 
+ static void perf_event_free_bpf_prog(struct perf_event *event)
+ {
+ 	if (!perf_event_is_tracing(event)) {
+ 		perf_event_free_bpf_handler(event);
+ 		return;
+ 	}
+ 	perf_event_detach_bpf_prog(event);
+ }
+ 
++>>>>>>> 33ea4b24277b (perf/core: Implement the 'perf_uprobe' PMU)
  #else
  
  static inline void perf_tp_register(void)
diff --cc kernel/trace/trace_event_perf.c
index ae2877cbe443,2c416509b834..000000000000
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@@ -213,15 -238,110 +213,119 @@@ void perf_trace_destroy(struct perf_eve
  	mutex_unlock(&event_mutex);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_KPROBE_EVENTS
+ int perf_kprobe_init(struct perf_event *p_event, bool is_retprobe)
+ {
+ 	int ret;
+ 	char *func = NULL;
+ 	struct trace_event_call *tp_event;
+ 
+ 	if (p_event->attr.kprobe_func) {
+ 		func = kzalloc(KSYM_NAME_LEN, GFP_KERNEL);
+ 		if (!func)
+ 			return -ENOMEM;
+ 		ret = strncpy_from_user(
+ 			func, u64_to_user_ptr(p_event->attr.kprobe_func),
+ 			KSYM_NAME_LEN);
+ 		if (ret < 0)
+ 			goto out;
+ 
+ 		if (func[0] == '\0') {
+ 			kfree(func);
+ 			func = NULL;
+ 		}
+ 	}
+ 
+ 	tp_event = create_local_trace_kprobe(
+ 		func, (void *)(unsigned long)(p_event->attr.kprobe_addr),
+ 		p_event->attr.probe_offset, is_retprobe);
+ 	if (IS_ERR(tp_event)) {
+ 		ret = PTR_ERR(tp_event);
+ 		goto out;
+ 	}
+ 
+ 	ret = perf_trace_event_init(tp_event, p_event);
+ 	if (ret)
+ 		destroy_local_trace_kprobe(tp_event);
+ out:
+ 	kfree(func);
+ 	return ret;
+ }
+ 
+ void perf_kprobe_destroy(struct perf_event *p_event)
+ {
+ 	perf_trace_event_close(p_event);
+ 	perf_trace_event_unreg(p_event);
+ 
+ 	destroy_local_trace_kprobe(p_event->tp_event);
+ }
+ #endif /* CONFIG_KPROBE_EVENTS */
+ 
+ #ifdef CONFIG_UPROBE_EVENTS
+ int perf_uprobe_init(struct perf_event *p_event, bool is_retprobe)
+ {
+ 	int ret;
+ 	char *path = NULL;
+ 	struct trace_event_call *tp_event;
+ 
+ 	if (!p_event->attr.uprobe_path)
+ 		return -EINVAL;
+ 	path = kzalloc(PATH_MAX, GFP_KERNEL);
+ 	if (!path)
+ 		return -ENOMEM;
+ 	ret = strncpy_from_user(
+ 		path, u64_to_user_ptr(p_event->attr.uprobe_path), PATH_MAX);
+ 	if (ret < 0)
+ 		goto out;
+ 	if (path[0] == '\0') {
+ 		ret = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	tp_event = create_local_trace_uprobe(
+ 		path, p_event->attr.probe_offset, is_retprobe);
+ 	if (IS_ERR(tp_event)) {
+ 		ret = PTR_ERR(tp_event);
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * local trace_uprobe need to hold event_mutex to call
+ 	 * uprobe_buffer_enable() and uprobe_buffer_disable().
+ 	 * event_mutex is not required for local trace_kprobes.
+ 	 */
+ 	mutex_lock(&event_mutex);
+ 	ret = perf_trace_event_init(tp_event, p_event);
+ 	if (ret)
+ 		destroy_local_trace_uprobe(tp_event);
+ 	mutex_unlock(&event_mutex);
+ out:
+ 	kfree(path);
+ 	return ret;
+ }
+ 
+ void perf_uprobe_destroy(struct perf_event *p_event)
+ {
+ 	mutex_lock(&event_mutex);
+ 	perf_trace_event_close(p_event);
+ 	perf_trace_event_unreg(p_event);
+ 	mutex_unlock(&event_mutex);
+ 	destroy_local_trace_uprobe(p_event->tp_event);
+ }
+ #endif /* CONFIG_UPROBE_EVENTS */
+ 
++>>>>>>> 33ea4b24277b (perf/core: Implement the 'perf_uprobe' PMU)
  int perf_trace_add(struct perf_event *p_event, int flags)
  {
 -	struct trace_event_call *tp_event = p_event->tp_event;
 +	struct ftrace_event_call *tp_event = p_event->tp_event;
 +	struct hlist_head __percpu *pcpu_list;
 +	struct hlist_head *list;
 +
 +	pcpu_list = tp_event->perf_events;
 +	if (WARN_ON_ONCE(!pcpu_list))
 +		return -EINVAL;
  
  	if (!(flags & PERF_EF_START))
  		p_event->hw.state = PERF_HES_STOPPED;
diff --cc kernel/trace/trace_probe.h
index 5b4f8bdfb3a5,27de3174050a..000000000000
--- a/kernel/trace/trace_probe.h
+++ b/kernel/trace/trace_probe.h
@@@ -352,3 -402,16 +352,19 @@@ store_trace_args(int ent_size, struct t
  				   data + tp->args[i].offset);
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ extern int set_print_fmt(struct trace_probe *tp, bool is_return);
+ 
+ #ifdef CONFIG_PERF_EVENTS
+ extern struct trace_event_call *
+ create_local_trace_kprobe(char *func, void *addr, unsigned long offs,
+ 			  bool is_return);
+ extern void destroy_local_trace_kprobe(struct trace_event_call *event_call);
+ 
+ extern struct trace_event_call *
+ create_local_trace_uprobe(char *name, unsigned long offs, bool is_return);
+ extern void destroy_local_trace_uprobe(struct trace_event_call *event_call);
+ #endif
++>>>>>>> 33ea4b24277b (perf/core: Implement the 'perf_uprobe' PMU)
diff --cc kernel/trace/trace_uprobe.c
index 84f228258d8e,e3cbae702a53..000000000000
--- a/kernel/trace/trace_uprobe.c
+++ b/kernel/trace/trace_uprobe.c
@@@ -1119,27 -1292,34 +1119,51 @@@ static struct trace_event_functions upr
  	.trace		= print_uprobe_event
  };
  
- static int register_uprobe_event(struct trace_uprobe *tu)
+ static inline void init_trace_event_call(struct trace_uprobe *tu,
+ 					 struct trace_event_call *call)
  {
++<<<<<<< HEAD
 +	struct ftrace_event_call *call = &tu->call;
 +	int ret;
 +
 +	/* Initialize ftrace_event_call */
++=======
++>>>>>>> 33ea4b24277b (perf/core: Implement the 'perf_uprobe' PMU)
  	INIT_LIST_HEAD(&call->class->fields);
  	call->event.funcs = &uprobe_funcs;
  	call->class->define_fields = uprobe_event_define_fields;
  
++<<<<<<< HEAD
 +	if (set_print_fmt(tu) < 0)
++=======
+ 	call->flags = TRACE_EVENT_FL_UPROBE;
+ 	call->class->reg = trace_uprobe_register;
+ 	call->data = tu;
+ }
+ 
+ static int register_uprobe_event(struct trace_uprobe *tu)
+ {
+ 	struct trace_event_call *call = &tu->tp.call;
+ 	int ret = 0;
+ 
+ 	init_trace_event_call(tu, call);
+ 
+ 	if (set_print_fmt(&tu->tp, is_ret_probe(tu)) < 0)
++>>>>>>> 33ea4b24277b (perf/core: Implement the 'perf_uprobe' PMU)
  		return -ENOMEM;
  
 -	ret = register_trace_event(&call->event);
 +	ret = register_ftrace_event(&call->event);
  	if (!ret) {
  		kfree(call->print_fmt);
  		return -ENODEV;
  	}
++<<<<<<< HEAD
 +	call->flags = 0;
 +	call->class->reg = trace_uprobe_register;
 +	call->data = tu;
++=======
+ 
++>>>>>>> 33ea4b24277b (perf/core: Implement the 'perf_uprobe' PMU)
  	ret = trace_add_event_call(call);
  
  	if (ret) {
* Unmerged path include/linux/trace_events.h
* Unmerged path include/linux/trace_events.h
* Unmerged path kernel/events/core.c
* Unmerged path kernel/trace/trace_event_perf.c
* Unmerged path kernel/trace/trace_probe.h
* Unmerged path kernel/trace/trace_uprobe.c
