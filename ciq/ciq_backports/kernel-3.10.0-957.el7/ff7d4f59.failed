iio: buffer: remove unneeded test

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iio] buffer: remove unneeded test (Tony Camuso) [1559170]
Rebuild_FUZZ: 91.80%
commit-author Laurent Navet <laurent.navet@gmail.com>
commit ff7d4f5981a8a139ead70adef3c1d0ed574bca01
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/ff7d4f59.failed

The same code is executed regardless ret value, so this test
can be removed.
Also fix coverity scan CID 1268786.

	Signed-off-by: Laurent Navet <laurent.navet@gmail.com>
	Signed-off-by: Jonathan Cameron <jic23@kernel.org>
(cherry picked from commit ff7d4f5981a8a139ead70adef3c1d0ed574bca01)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iio/industrialio-buffer.c
diff --cc drivers/iio/industrialio-buffer.c
index ed6b8aa675ce,dad61ab9b36f..000000000000
--- a/drivers/iio/industrialio-buffer.c
+++ b/drivers/iio/industrialio-buffer.c
@@@ -386,11 -441,524 +386,466 @@@ error_ret
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t iio_buffer_read_length(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 
+ 	return sprintf(buf, "%d\n", buffer->length);
+ }
+ 
+ static ssize_t iio_buffer_write_length(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf, size_t len)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 	unsigned int val;
+ 	int ret;
+ 
+ 	ret = kstrtouint(buf, 10, &val);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (val == buffer->length)
+ 		return len;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 	if (iio_buffer_is_active(indio_dev->buffer)) {
+ 		ret = -EBUSY;
+ 	} else {
+ 		buffer->access->set_length(buffer, val);
+ 		ret = 0;
+ 	}
+ 	if (ret)
+ 		goto out;
+ 	if (buffer->length && buffer->length < buffer->watermark)
+ 		buffer->watermark = buffer->length;
+ out:
+ 	mutex_unlock(&indio_dev->mlock);
+ 
+ 	return ret ? ret : len;
+ }
+ 
+ static ssize_t iio_buffer_show_enable(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	return sprintf(buf, "%d\n", iio_buffer_is_active(indio_dev->buffer));
+ }
+ 
+ static int iio_compute_scan_bytes(struct iio_dev *indio_dev,
+ 				const unsigned long *mask, bool timestamp)
+ {
+ 	const struct iio_chan_spec *ch;
+ 	unsigned bytes = 0;
+ 	int length, i;
+ 
+ 	/* How much space will the demuxed element take? */
+ 	for_each_set_bit(i, mask,
+ 			 indio_dev->masklength) {
+ 		ch = iio_find_channel_from_si(indio_dev, i);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	if (timestamp) {
+ 		ch = iio_find_channel_from_si(indio_dev,
+ 					      indio_dev->scan_index_timestamp);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	return bytes;
+ }
+ 
+ static void iio_buffer_activate(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	iio_buffer_get(buffer);
+ 	list_add(&buffer->buffer_list, &indio_dev->buffer_list);
+ }
+ 
+ static void iio_buffer_deactivate(struct iio_buffer *buffer)
+ {
+ 	list_del_init(&buffer->buffer_list);
+ 	wake_up_interruptible(&buffer->pollq);
+ 	iio_buffer_put(buffer);
+ }
+ 
+ static void iio_buffer_deactivate_all(struct iio_dev *indio_dev)
+ {
+ 	struct iio_buffer *buffer, *_buffer;
+ 
+ 	list_for_each_entry_safe(buffer, _buffer,
+ 			&indio_dev->buffer_list, buffer_list)
+ 		iio_buffer_deactivate(buffer);
+ }
+ 
+ static void iio_buffer_update_bytes_per_datum(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	unsigned int bytes;
+ 
+ 	if (!buffer->access->set_bytes_per_datum)
+ 		return;
+ 
+ 	bytes = iio_compute_scan_bytes(indio_dev, buffer->scan_mask,
+ 		buffer->scan_timestamp);
+ 
+ 	buffer->access->set_bytes_per_datum(buffer, bytes);
+ }
+ 
+ static int iio_buffer_request_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	int ret;
+ 
+ 	iio_buffer_update_bytes_per_datum(indio_dev, buffer);
+ 	if (buffer->access->request_update) {
+ 		ret = buffer->access->request_update(buffer);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer parameter update failed (%d)\n",
+ 				ret);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void iio_free_scan_mask(struct iio_dev *indio_dev,
+ 	const unsigned long *mask)
+ {
+ 	/* If the mask is dynamically allocated free it, otherwise do nothing */
+ 	if (!indio_dev->available_scan_masks)
+ 		kfree(mask);
+ }
+ 
+ struct iio_device_config {
+ 	unsigned int mode;
+ 	const unsigned long *scan_mask;
+ 	unsigned int scan_bytes;
+ 	bool scan_timestamp;
+ };
+ 
+ static int iio_verify_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *insert_buffer, struct iio_buffer *remove_buffer,
+ 	struct iio_device_config *config)
+ {
+ 	unsigned long *compound_mask;
+ 	const unsigned long *scan_mask;
+ 	struct iio_buffer *buffer;
+ 	bool scan_timestamp;
+ 
+ 	memset(config, 0, sizeof(*config));
+ 
+ 	/*
+ 	 * If there is just one buffer and we are removing it there is nothing
+ 	 * to verify.
+ 	 */
+ 	if (remove_buffer && !insert_buffer &&
+ 		list_is_singular(&indio_dev->buffer_list))
+ 			return 0;
+ 
+ 	/* Definitely possible for devices to support both of these. */
+ 	if ((indio_dev->modes & INDIO_BUFFER_TRIGGERED) && indio_dev->trig) {
+ 		config->mode = INDIO_BUFFER_TRIGGERED;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_HARDWARE) {
+ 		config->mode = INDIO_BUFFER_HARDWARE;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_SOFTWARE) {
+ 		config->mode = INDIO_BUFFER_SOFTWARE;
+ 	} else {
+ 		/* Can only occur on first buffer */
+ 		if (indio_dev->modes & INDIO_BUFFER_TRIGGERED)
+ 			dev_dbg(&indio_dev->dev, "Buffer not started: no trigger\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* What scan mask do we actually have? */
+ 	compound_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),
+ 				sizeof(long), GFP_KERNEL);
+ 	if (compound_mask == NULL)
+ 		return -ENOMEM;
+ 
+ 	scan_timestamp = false;
+ 
+ 	list_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {
+ 		if (buffer == remove_buffer)
+ 			continue;
+ 		bitmap_or(compound_mask, compound_mask, buffer->scan_mask,
+ 			  indio_dev->masklength);
+ 		scan_timestamp |= buffer->scan_timestamp;
+ 	}
+ 
+ 	if (insert_buffer) {
+ 		bitmap_or(compound_mask, compound_mask,
+ 			  insert_buffer->scan_mask, indio_dev->masklength);
+ 		scan_timestamp |= insert_buffer->scan_timestamp;
+ 	}
+ 
+ 	if (indio_dev->available_scan_masks) {
+ 		scan_mask = iio_scan_mask_match(indio_dev->available_scan_masks,
+ 				    indio_dev->masklength,
+ 				    compound_mask);
+ 		kfree(compound_mask);
+ 		if (scan_mask == NULL)
+ 			return -EINVAL;
+ 	} else {
+ 	    scan_mask = compound_mask;
+ 	}
+ 
+ 	config->scan_bytes = iio_compute_scan_bytes(indio_dev,
+ 				    scan_mask, scan_timestamp);
+ 	config->scan_mask = scan_mask;
+ 	config->scan_timestamp = scan_timestamp;
+ 
+ 	return 0;
+ }
+ 
+ static int iio_enable_buffers(struct iio_dev *indio_dev,
+ 	struct iio_device_config *config)
+ {
+ 	int ret;
+ 
+ 	indio_dev->active_scan_mask = config->scan_mask;
+ 	indio_dev->scan_timestamp = config->scan_timestamp;
+ 	indio_dev->scan_bytes = config->scan_bytes;
+ 
+ 	iio_update_demux(indio_dev);
+ 
+ 	/* Wind up again */
+ 	if (indio_dev->setup_ops->preenable) {
+ 		ret = indio_dev->setup_ops->preenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer preenable failed (%d)\n", ret);
+ 			goto err_undo_config;
+ 		}
+ 	}
+ 
+ 	if (indio_dev->info->update_scan_mode) {
+ 		ret = indio_dev->info
+ 			->update_scan_mode(indio_dev,
+ 					   indio_dev->active_scan_mask);
+ 		if (ret < 0) {
+ 			dev_dbg(&indio_dev->dev,
+ 				"Buffer not started: update scan mode failed (%d)\n",
+ 				ret);
+ 			goto err_run_postdisable;
+ 		}
+ 	}
+ 
+ 	indio_dev->currentmode = config->mode;
+ 
+ 	if (indio_dev->setup_ops->postenable) {
+ 		ret = indio_dev->setup_ops->postenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: postenable failed (%d)\n", ret);
+ 			goto err_run_postdisable;
+ 		}
+ 	}
+ 
+ 	return 0;
+ 
+ err_run_postdisable:
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 	if (indio_dev->setup_ops->postdisable)
+ 		indio_dev->setup_ops->postdisable(indio_dev);
+ err_undo_config:
+ 	indio_dev->active_scan_mask = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static int iio_disable_buffers(struct iio_dev *indio_dev)
+ {
+ 	int ret = 0;
+ 	int ret2;
+ 
+ 	/* Wind down existing buffers - iff there are any */
+ 	if (list_empty(&indio_dev->buffer_list))
+ 		return 0;
+ 
+ 	/*
+ 	 * If things go wrong at some step in disable we still need to continue
+ 	 * to perform the other steps, otherwise we leave the device in a
+ 	 * inconsistent state. We return the error code for the first error we
+ 	 * encountered.
+ 	 */
+ 
+ 	if (indio_dev->setup_ops->predisable) {
+ 		ret2 = indio_dev->setup_ops->predisable(indio_dev);
+ 		if (ret2 && !ret)
+ 			ret = ret2;
+ 	}
+ 
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 
+ 	if (indio_dev->setup_ops->postdisable) {
+ 		ret2 = indio_dev->setup_ops->postdisable(indio_dev);
+ 		if (ret2 && !ret)
+ 			ret = ret2;
+ 	}
+ 
+ 	iio_free_scan_mask(indio_dev, indio_dev->active_scan_mask);
+ 	indio_dev->active_scan_mask = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static int __iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	struct iio_device_config new_config;
+ 	int ret;
+ 
+ 	ret = iio_verify_update(indio_dev, insert_buffer, remove_buffer,
+ 		&new_config);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (insert_buffer) {
+ 		ret = iio_buffer_request_update(indio_dev, insert_buffer);
+ 		if (ret)
+ 			goto err_free_config;
+ 	}
+ 
+ 	ret = iio_disable_buffers(indio_dev);
+ 	if (ret)
+ 		goto err_deactivate_all;
+ 
+ 	if (remove_buffer)
+ 		iio_buffer_deactivate(remove_buffer);
+ 	if (insert_buffer)
+ 		iio_buffer_activate(indio_dev, insert_buffer);
+ 
+ 	/* If no buffers in list, we are done */
+ 	if (list_empty(&indio_dev->buffer_list))
+ 		return 0;
+ 
+ 	ret = iio_enable_buffers(indio_dev, &new_config);
+ 	if (ret)
+ 		goto err_deactivate_all;
+ 
+ 	return 0;
+ 
+ err_deactivate_all:
+ 	/*
+ 	 * We've already verified that the config is valid earlier. If things go
+ 	 * wrong in either enable or disable the most likely reason is an IO
+ 	 * error from the device. In this case there is no good recovery
+ 	 * strategy. Just make sure to disable everything and leave the device
+ 	 * in a sane state.  With a bit of luck the device might come back to
+ 	 * life again later and userspace can try again.
+ 	 */
+ 	iio_buffer_deactivate_all(indio_dev);
+ 
+ err_free_config:
+ 	iio_free_scan_mask(indio_dev, new_config.scan_mask);
+ 	return ret;
+ }
+ 
+ int iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 
+ 	if (insert_buffer == remove_buffer)
+ 		return 0;
+ 
+ 	mutex_lock(&indio_dev->info_exist_lock);
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	if (insert_buffer && iio_buffer_is_active(insert_buffer))
+ 		insert_buffer = NULL;
+ 
+ 	if (remove_buffer && !iio_buffer_is_active(remove_buffer))
+ 		remove_buffer = NULL;
+ 
+ 	if (!insert_buffer && !remove_buffer) {
+ 		ret = 0;
+ 		goto out_unlock;
+ 	}
+ 
+ 	if (indio_dev->info == NULL) {
+ 		ret = -ENODEV;
+ 		goto out_unlock;
+ 	}
+ 
+ 	ret = __iio_update_buffers(indio_dev, insert_buffer, remove_buffer);
+ 
+ out_unlock:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	mutex_unlock(&indio_dev->info_exist_lock);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(iio_update_buffers);
+ 
+ void iio_disable_all_buffers(struct iio_dev *indio_dev)
+ {
+ 	iio_disable_buffers(indio_dev);
+ 	iio_buffer_deactivate_all(indio_dev);
+ }
+ 
+ static ssize_t iio_buffer_store_enable(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf,
+ 				       size_t len)
+ {
+ 	int ret;
+ 	bool requested_state;
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	bool inlist;
+ 
+ 	ret = strtobool(buf, &requested_state);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	/* Find out if it is in the list */
+ 	inlist = iio_buffer_is_active(indio_dev->buffer);
+ 	/* Already in desired state */
+ 	if (inlist == requested_state)
+ 		goto done;
+ 
+ 	if (requested_state)
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 indio_dev->buffer, NULL);
+ 	else
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 NULL, indio_dev->buffer);
+ 
+ done:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	return (ret < 0) ? ret : len;
+ }
+ 
++>>>>>>> ff7d4f5981a8 (iio: buffer: remove unneeded test)
  static const char * const iio_scan_elements_group_name = "scan_elements";
  
 -static ssize_t iio_buffer_show_watermark(struct device *dev,
 -					 struct device_attribute *attr,
 -					 char *buf)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -
 -	return sprintf(buf, "%u\n", buffer->watermark);
 -}
 -
 -static ssize_t iio_buffer_store_watermark(struct device *dev,
 -					  struct device_attribute *attr,
 -					  const char *buf,
 -					  size_t len)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -	unsigned int val;
 -	int ret;
 -
 -	ret = kstrtouint(buf, 10, &val);
 -	if (ret)
 -		return ret;
 -	if (!val)
 -		return -EINVAL;
 -
 -	mutex_lock(&indio_dev->mlock);
 -
 -	if (val > buffer->length) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	if (iio_buffer_is_active(indio_dev->buffer)) {
 -		ret = -EBUSY;
 -		goto out;
 -	}
 -
 -	buffer->watermark = val;
 -
 -	if (indio_dev->info->hwfifo_set_watermark)
 -		indio_dev->info->hwfifo_set_watermark(indio_dev, val);
 -out:
 -	mutex_unlock(&indio_dev->mlock);
 -
 -	return ret ? ret : len;
 -}
 -
 -static DEVICE_ATTR(length, S_IRUGO | S_IWUSR, iio_buffer_read_length,
 -		   iio_buffer_write_length);
 -static struct device_attribute dev_attr_length_ro = __ATTR(length,
 -	S_IRUGO, iio_buffer_read_length, NULL);
 -static DEVICE_ATTR(enable, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_enable, iio_buffer_store_enable);
 -static DEVICE_ATTR(watermark, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_watermark, iio_buffer_store_watermark);
 -
 -static struct attribute *iio_buffer_attrs[] = {
 -	&dev_attr_length.attr,
 -	&dev_attr_enable.attr,
 -	&dev_attr_watermark.attr,
 -};
 -
 -int iio_buffer_alloc_sysfs_and_mask(struct iio_dev *indio_dev)
 +int iio_buffer_register(struct iio_dev *indio_dev,
 +			const struct iio_chan_spec *channels,
 +			int num_channels)
  {
  	struct iio_dev_attr *p;
  	struct attribute **attr;
* Unmerged path drivers/iio/industrialio-buffer.c
