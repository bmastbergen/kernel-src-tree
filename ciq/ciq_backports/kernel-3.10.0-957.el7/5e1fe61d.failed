nvme-rdma: teardown admin/io queues once on error recovery

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Sagi Grimberg <sagi@grimberg.me>
commit 5e1fe61d4170b1f498e20de92c7ce4cd5e40c3c5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/5e1fe61d.failed

Relying on the queue state while tearing down on every reconnect
attempt is not a good design. We should do it once in err_work
and simply try to establish the queues for each reconnect attempt.

	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 5e1fe61d4170b1f498e20de92c7ce4cd5e40c3c5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index bd242f85edd3,95837c5317b4..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -729,45 -925,14 +729,53 @@@ static void nvme_rdma_reconnect_ctrl_wo
  
  	++ctrl->ctrl.nr_reconnects;
  
++<<<<<<< HEAD
 +	if (ctrl->ctrl.queue_count > 1) {
 +		nvme_rdma_free_io_queues(ctrl);
 +
 +		ret = blk_mq_reinit_tagset(&ctrl->tag_set);
 +		if (ret)
 +			goto requeue;
 +	}
 +
 +	nvme_rdma_stop_and_free_queue(&ctrl->queues[0]);
 +
 +	ret = blk_mq_reinit_tagset(&ctrl->admin_tag_set);
 +	if (ret)
 +		goto requeue;
 +
 +	ret = nvme_rdma_init_queue(ctrl, 0, NVME_AQ_DEPTH);
 +	if (ret)
 +		goto requeue;
 +
 +	ret = nvmf_connect_admin_queue(&ctrl->ctrl);
 +	if (ret)
 +		goto requeue;
 +
 +	set_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[0].flags);
 +
 +	ret = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
++=======
+ 	ret = nvme_rdma_configure_admin_queue(ctrl, false);
++>>>>>>> 5e1fe61d4170 (nvme-rdma: teardown admin/io queues once on error recovery)
  	if (ret)
  		goto requeue;
  
  	if (ctrl->ctrl.queue_count > 1) {
 -		ret = nvme_rdma_configure_io_queues(ctrl, false);
 +		ret = nvme_rdma_init_io_queues(ctrl);
 +		if (ret)
++<<<<<<< HEAD
 +			goto requeue;
 +
 +		ret = nvme_rdma_connect_io_queues(ctrl);
  		if (ret)
 +			goto requeue;
 +
 +		blk_mq_update_nr_hw_queues(&ctrl->tag_set,
 +				ctrl->ctrl.queue_count - 1);
++=======
+ 			goto destroy_admin;
++>>>>>>> 5e1fe61d4170 (nvme-rdma: teardown admin/io queues once on error recovery)
  	}
  
  	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
@@@ -799,19 -966,17 +810,27 @@@ static void nvme_rdma_error_recovery_wo
  
  	nvme_stop_keep_alive(&ctrl->ctrl);
  
 -	if (ctrl->ctrl.queue_count > 1) {
 +	for (i = 0; i < ctrl->ctrl.queue_count; i++)
 +		clear_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[i].flags);
 +
 +	if (ctrl->ctrl.queue_count > 1)
  		nvme_stop_queues(&ctrl->ctrl);
++<<<<<<< HEAD
 +	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
 +
 +	/* We must take care of fastfail/requeue all our inflight requests */
 +	if (ctrl->ctrl.queue_count > 1)
++=======
++>>>>>>> 5e1fe61d4170 (nvme-rdma: teardown admin/io queues once on error recovery)
  		blk_mq_tagset_busy_iter(&ctrl->tag_set,
  					nvme_cancel_request, &ctrl->ctrl);
+ 		nvme_rdma_destroy_io_queues(ctrl, false);
+ 	}
+ 
+ 	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
  	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
  				nvme_cancel_request, &ctrl->ctrl);
+ 	nvme_rdma_destroy_admin_queue(ctrl, false);
  
  	/*
  	 * queues are not a live anymore, so restart the queues to fail fast
* Unmerged path drivers/nvme/host/rdma.c
