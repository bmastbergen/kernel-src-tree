x86/mm: Relocate page fault error codes to traps.h

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] mm: Relocate page fault error codes to traps.h (Gopal Tiwari) [1456572]
Rebuild_FUZZ: 95.83%
commit-author Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
commit 1067f030994c69ca1fba8c607437c8895dcf8509
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/1067f030.failed

Up to this point, only fault.c used the definitions of the page fault error
codes. Thus, it made sense to keep them within such file. Other portions of
code might be interested in those definitions too. For instance, the User-
Mode Instruction Prevention emulation code will use such definitions to
emulate a page fault when it is unable to successfully copy the results
of the emulated instructions to user space.

While relocating the error code enumeration, the prefix X86_ is used to
make it consistent with the rest of the definitions in traps.h. Of course,
code using the enumeration had to be updated as well. No functional changes
were performed.

	Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Andy Lutomirski <luto@kernel.org>
	Cc: "Michael S. Tsirkin" <mst@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: ricardo.neri@intel.com
	Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
	Cc: Huang Rui <ray.huang@amd.com>
	Cc: Shuah Khan <shuah@kernel.org>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Jiri Slaby <jslaby@suse.cz>
	Cc: "Ravi V. Shankar" <ravi.v.shankar@intel.com>
	Cc: Chris Metcalf <cmetcalf@mellanox.com>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Chen Yucong <slaoub@gmail.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Masami Hiramatsu <mhiramat@kernel.org>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
Link: https://lkml.kernel.org/r/1509135945-13762-2-git-send-email-ricardo.neri-calderon@linux.intel.com

(cherry picked from commit 1067f030994c69ca1fba8c607437c8895dcf8509)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/fault.c
diff --cc arch/x86/mm/fault.c
index 7472d907016e,db71c73530bd..000000000000
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@@ -646,10 -677,15 +626,10 @@@ show_fault_oops(struct pt_regs *regs, u
  	if (!oops_may_print())
  		return;
  
- 	if (error_code & PF_INSTR) {
+ 	if (error_code & X86_PF_INSTR) {
  		unsigned int level;
 -		pgd_t *pgd;
 -		pte_t *pte;
 -
 -		pgd = __va(read_cr3_pa());
 -		pgd += pgd_index(address);
  
 -		pte = lookup_address_in_pgd(pgd, address, &level);
 +		pte_t *pte = lookup_address(address, &level);
  
  		if (pte && pte_present(*pte) && !pte_exec(*pte))
  			printk(nx_warning, from_kuid(&init_user_ns, current_uid()));
@@@ -699,17 -738,28 +679,17 @@@ no_context(struct pt_regs *regs, unsign
  	   unsigned long address, int signal, int si_code)
  {
  	struct task_struct *tsk = current;
 +	unsigned long *stackend;
  	unsigned long flags;
  	int sig;
 +	/* No context means no VMA to pass down */
 +	struct vm_area_struct *vma = NULL;
  
  	/* Are we prepared to handle this kernel fault? */
 -	if (fixup_exception(regs, X86_TRAP_PF)) {
 -		/*
 -		 * Any interrupt that takes a fault gets the fixup. This makes
 -		 * the below recursive fault logic only apply to a faults from
 -		 * task context.
 -		 */
 -		if (in_interrupt())
 -			return;
 -
 -		/*
 -		 * Per the above we're !in_interrupt(), aka. task context.
 -		 *
 -		 * In this case we need to make sure we're not recursively
 -		 * faulting through the emulate_vsyscall() logic.
 -		 */
 -		if (current->thread.sig_on_uaccess_err && signal) {
 +	if (fixup_exception(regs)) {
 +		if (current_thread_info()->sig_on_uaccess_error && signal) {
  			tsk->thread.trap_nr = X86_TRAP_PF;
- 			tsk->thread.error_code = error_code | PF_USER;
+ 			tsk->thread.error_code = error_code | X86_PF_USER;
  			tsk->thread.cr2 = address;
  
  			/* XXX: hwpoison faults will set the wrong code. */
@@@ -815,15 -898,20 +795,31 @@@ __bad_area_nosemaphore(struct pt_regs *
  		 * Instruction fetch faults in the vsyscall page might need
  		 * emulation.
  		 */
++<<<<<<< HEAD
 +		if (unlikely((error_code & PF_INSTR) &&
 +			     ((address & ~0xfff) == VSYSCALL_START))) {
++=======
+ 		if (unlikely((error_code & X86_PF_INSTR) &&
+ 			     ((address & ~0xfff) == VSYSCALL_ADDR))) {
++>>>>>>> 1067f030994c (x86/mm: Relocate page fault error codes to traps.h)
  			if (emulate_vsyscall(regs, address))
  				return;
  		}
  #endif
++<<<<<<< HEAD
 +		/* Kernel addresses are always protection faults: */
 +		if (address >= TASK_SIZE)
 +			error_code |= PF_PROT;
++=======
+ 
+ 		/*
+ 		 * To avoid leaking information about the kernel page table
+ 		 * layout, pretend that user-mode accesses to kernel addresses
+ 		 * are always protection faults.
+ 		 */
+ 		if (address >= TASK_SIZE_MAX)
+ 			error_code |= X86_PF_PROT;
++>>>>>>> 1067f030994c (x86/mm: Relocate page fault error codes to traps.h)
  
  		if (likely(show_unhandled_signals))
  			show_signal_msg(regs, error_code, address, tsk);
@@@ -937,10 -1030,9 +933,10 @@@ do_sigbus(struct pt_regs *regs, unsigne
  
  static noinline void
  mm_fault_error(struct pt_regs *regs, unsigned long error_code,
 -	       unsigned long address, u32 *pkey, unsigned int fault)
 +	       unsigned long address, struct vm_area_struct *vma,
 +	       unsigned int fault)
  {
- 	if (fatal_signal_pending(current) && !(error_code & PF_USER)) {
+ 	if (fatal_signal_pending(current) && !(error_code & X86_PF_USER)) {
  		no_context(regs, error_code, address, 0, 0);
  		return;
  	}
@@@ -1116,10 -1217,16 +1112,20 @@@ static int fault_in_kernel_space(unsign
  
  static inline bool smap_violation(int error_code, struct pt_regs *regs)
  {
++<<<<<<< HEAD
 +	if (error_code & PF_USER)
++=======
+ 	if (!IS_ENABLED(CONFIG_X86_SMAP))
+ 		return false;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_SMAP))
+ 		return false;
+ 
+ 	if (error_code & X86_PF_USER)
++>>>>>>> 1067f030994c (x86/mm: Relocate page fault error codes to traps.h)
  		return false;
  
 -	if (!user_mode(regs) && (regs->flags & X86_EFLAGS_AC))
 +	if (!user_mode_vm(regs) && (regs->flags & X86_EFLAGS_AC))
  		return false;
  
  	return true;
@@@ -1196,15 -1304,12 +1202,15 @@@ __do_page_fault(struct pt_regs *regs, u
  	if (unlikely(kprobes_fault(regs)))
  		return;
  
- 	if (unlikely(error_code & PF_RSVD))
+ 	if (unlikely(error_code & X86_PF_RSVD))
  		pgtable_bad(regs, error_code, address);
  
 -	if (unlikely(smap_violation(error_code, regs))) {
 -		bad_area_nosemaphore(regs, error_code, address, NULL);
 -		return;
 +	if (static_cpu_has(X86_FEATURE_SMAP)) {
 +		if (unlikely(smap_violation(error_code, regs))) {
 +			bad_area_nosemaphore(regs, error_code, address,
 +				NULL);
 +			return;
 +		}
  	}
  
  	/*
@@@ -1223,9 -1328,9 +1229,9 @@@
  	 * User-mode registers count as a user access even for any
  	 * potential system fault or CPU buglet:
  	 */
 -	if (user_mode(regs)) {
 +	if (user_mode_vm(regs)) {
  		local_irq_enable();
- 		error_code |= PF_USER;
+ 		error_code |= X86_PF_USER;
  		flags |= FAULT_FLAG_USER;
  	} else {
  		if (regs->flags & X86_EFLAGS_IF)
diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h
index b4528dc3987c..14c53ca4badc 100644
--- a/arch/x86/include/asm/traps.h
+++ b/arch/x86/include/asm/traps.h
@@ -142,4 +142,22 @@ enum {
 	X86_TRAP_IRET = 32,	/* 32, IRET Exception */
 };
 
+/*
+ * Page fault error code bits:
+ *
+ *   bit 0 ==	 0: no page found	1: protection fault
+ *   bit 1 ==	 0: read access		1: write access
+ *   bit 2 ==	 0: kernel-mode access	1: user-mode access
+ *   bit 3 ==				1: use of reserved bit detected
+ *   bit 4 ==				1: fault was an instruction fetch
+ *   bit 5 ==				1: protection keys block access
+ */
+enum x86_pf_error_code {
+	X86_PF_PROT	=		1 << 0,
+	X86_PF_WRITE	=		1 << 1,
+	X86_PF_USER	=		1 << 2,
+	X86_PF_RSVD	=		1 << 3,
+	X86_PF_INSTR	=		1 << 4,
+	X86_PF_PK	=		1 << 5,
+};
 #endif /* _ASM_X86_TRAPS_H */
* Unmerged path arch/x86/mm/fault.c
