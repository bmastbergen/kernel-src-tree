cxgb4: fix error return code in cxgb4_set_hash_filter()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Wei Yongjun <weiyongjun1@huawei.com>
commit a882d20cdb7775ff8b4aac880255eff6a2c1c85e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a882d20c.failed

Fix to return a negative error code from thecxgb4_alloc_atid()
error handling case instead of 0.

Fixes: 12b276fbf6e0 ("cxgb4: add support to create hash filters")
	Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
Acked-By: Kumar Sanghvi <kumaras@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a882d20cdb7775ff8b4aac880255eff6a2c1c85e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
index 1207efa9b0e5,5980f308a253..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
@@@ -558,6 -766,418 +558,421 @@@ static void fill_default_mask(struct ch
  		fs->mask.fport = ~0;
  }
  
++<<<<<<< HEAD
++=======
+ static bool is_addr_all_mask(u8 *ipmask, int family)
+ {
+ 	if (family == AF_INET) {
+ 		struct in_addr *addr;
+ 
+ 		addr = (struct in_addr *)ipmask;
+ 		if (addr->s_addr == 0xffffffff)
+ 			return true;
+ 	} else if (family == AF_INET6) {
+ 		struct in6_addr *addr6;
+ 
+ 		addr6 = (struct in6_addr *)ipmask;
+ 		if (addr6->s6_addr32[0] == 0xffffffff &&
+ 		    addr6->s6_addr32[1] == 0xffffffff &&
+ 		    addr6->s6_addr32[2] == 0xffffffff &&
+ 		    addr6->s6_addr32[3] == 0xffffffff)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ static bool is_inaddr_any(u8 *ip, int family)
+ {
+ 	int addr_type;
+ 
+ 	if (family == AF_INET) {
+ 		struct in_addr *addr;
+ 
+ 		addr = (struct in_addr *)ip;
+ 		if (addr->s_addr == htonl(INADDR_ANY))
+ 			return true;
+ 	} else if (family == AF_INET6) {
+ 		struct in6_addr *addr6;
+ 
+ 		addr6 = (struct in6_addr *)ip;
+ 		addr_type = ipv6_addr_type((const struct in6_addr *)
+ 					   &addr6);
+ 		if (addr_type == IPV6_ADDR_ANY)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ bool is_filter_exact_match(struct adapter *adap,
+ 			   struct ch_filter_specification *fs)
+ {
+ 	struct tp_params *tp = &adap->params.tp;
+ 	u64 hash_filter_mask = tp->hash_filter_mask;
+ 	u32 mask;
+ 
+ 	if (!is_hashfilter(adap))
+ 		return false;
+ 
+ 	if (fs->type) {
+ 		if (is_inaddr_any(fs->val.fip, AF_INET6) ||
+ 		    !is_addr_all_mask(fs->mask.fip, AF_INET6))
+ 			return false;
+ 
+ 		if (is_inaddr_any(fs->val.lip, AF_INET6) ||
+ 		    !is_addr_all_mask(fs->mask.lip, AF_INET6))
+ 			return false;
+ 	} else {
+ 		if (is_inaddr_any(fs->val.fip, AF_INET) ||
+ 		    !is_addr_all_mask(fs->mask.fip, AF_INET))
+ 			return false;
+ 
+ 		if (is_inaddr_any(fs->val.lip, AF_INET) ||
+ 		    !is_addr_all_mask(fs->mask.lip, AF_INET))
+ 			return false;
+ 	}
+ 
+ 	if (!fs->val.lport || fs->mask.lport != 0xffff)
+ 		return false;
+ 
+ 	if (!fs->val.fport || fs->mask.fport != 0xffff)
+ 		return false;
+ 
+ 	if (tp->fcoe_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->fcoe_shift) & FT_FCOE_W;
+ 		if (mask && !fs->mask.fcoe)
+ 			return false;
+ 	}
+ 
+ 	if (tp->port_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->port_shift) & FT_PORT_W;
+ 		if (mask && !fs->mask.iport)
+ 			return false;
+ 	}
+ 
+ 	if (tp->vnic_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->vnic_shift) & FT_VNIC_ID_W;
+ 
+ 		if ((adap->params.tp.ingress_config & VNIC_F)) {
+ 			if (mask && !fs->mask.pfvf_vld)
+ 				return false;
+ 		} else {
+ 			if (mask && !fs->mask.ovlan_vld)
+ 				return false;
+ 		}
+ 	}
+ 
+ 	if (tp->vlan_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->vlan_shift) & FT_VLAN_W;
+ 		if (mask && !fs->mask.ivlan)
+ 			return false;
+ 	}
+ 
+ 	if (tp->tos_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->tos_shift) & FT_TOS_W;
+ 		if (mask && !fs->mask.tos)
+ 			return false;
+ 	}
+ 
+ 	if (tp->protocol_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->protocol_shift) & FT_PROTOCOL_W;
+ 		if (mask && !fs->mask.proto)
+ 			return false;
+ 	}
+ 
+ 	if (tp->ethertype_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->ethertype_shift) &
+ 			FT_ETHERTYPE_W;
+ 		if (mask && !fs->mask.ethtype)
+ 			return false;
+ 	}
+ 
+ 	if (tp->macmatch_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->macmatch_shift) & FT_MACMATCH_W;
+ 		if (mask && !fs->mask.macidx)
+ 			return false;
+ 	}
+ 
+ 	if (tp->matchtype_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->matchtype_shift) &
+ 			FT_MPSHITTYPE_W;
+ 		if (mask && !fs->mask.matchtype)
+ 			return false;
+ 	}
+ 	if (tp->frag_shift >= 0) {
+ 		mask = (hash_filter_mask >> tp->frag_shift) &
+ 			FT_FRAGMENTATION_W;
+ 		if (mask && !fs->mask.frag)
+ 			return false;
+ 	}
+ 	return true;
+ }
+ 
+ static u64 hash_filter_ntuple(struct ch_filter_specification *fs,
+ 			      struct net_device *dev)
+ {
+ 	struct adapter *adap = netdev2adap(dev);
+ 	struct tp_params *tp = &adap->params.tp;
+ 	u64 ntuple = 0;
+ 
+ 	/* Initialize each of the fields which we care about which are present
+ 	 * in the Compressed Filter Tuple.
+ 	 */
+ 	if (tp->vlan_shift >= 0 && fs->mask.ivlan)
+ 		ntuple |= (FT_VLAN_VLD_F | fs->val.ivlan) << tp->vlan_shift;
+ 
+ 	if (tp->port_shift >= 0 && fs->mask.iport)
+ 		ntuple |= (u64)fs->val.iport << tp->port_shift;
+ 
+ 	if (tp->protocol_shift >= 0) {
+ 		if (!fs->val.proto)
+ 			ntuple |= (u64)IPPROTO_TCP << tp->protocol_shift;
+ 		else
+ 			ntuple |= (u64)fs->val.proto << tp->protocol_shift;
+ 	}
+ 
+ 	if (tp->tos_shift >= 0 && fs->mask.tos)
+ 		ntuple |= (u64)(fs->val.tos) << tp->tos_shift;
+ 
+ 	if (tp->vnic_shift >= 0) {
+ 		if ((adap->params.tp.ingress_config & VNIC_F) &&
+ 		    fs->mask.pfvf_vld)
+ 			ntuple |= (u64)((fs->val.pfvf_vld << 16) |
+ 					(fs->val.pf << 13) |
+ 					(fs->val.vf)) << tp->vnic_shift;
+ 		else
+ 			ntuple |= (u64)((fs->val.ovlan_vld << 16) |
+ 					(fs->val.ovlan)) << tp->vnic_shift;
+ 	}
+ 
+ 	if (tp->macmatch_shift >= 0 && fs->mask.macidx)
+ 		ntuple |= (u64)(fs->val.macidx) << tp->macmatch_shift;
+ 
+ 	if (tp->ethertype_shift >= 0 && fs->mask.ethtype)
+ 		ntuple |= (u64)(fs->val.ethtype) << tp->ethertype_shift;
+ 
+ 	if (tp->matchtype_shift >= 0 && fs->mask.matchtype)
+ 		ntuple |= (u64)(fs->val.matchtype) << tp->matchtype_shift;
+ 
+ 	if (tp->frag_shift >= 0 && fs->mask.frag)
+ 		ntuple |= (u64)(fs->val.frag) << tp->frag_shift;
+ 
+ 	if (tp->fcoe_shift >= 0 && fs->mask.fcoe)
+ 		ntuple |= (u64)(fs->val.fcoe) << tp->fcoe_shift;
+ 	return ntuple;
+ }
+ 
+ static void mk_act_open_req6(struct filter_entry *f, struct sk_buff *skb,
+ 			     unsigned int qid_filterid, struct adapter *adap)
+ {
+ 	struct cpl_t6_act_open_req6 *t6req = NULL;
+ 	struct cpl_act_open_req6 *req = NULL;
+ 
+ 	t6req = (struct cpl_t6_act_open_req6 *)__skb_put(skb, sizeof(*t6req));
+ 	INIT_TP_WR(t6req, 0);
+ 	req = (struct cpl_act_open_req6 *)t6req;
+ 	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ACT_OPEN_REQ6, qid_filterid));
+ 	req->local_port = cpu_to_be16(f->fs.val.lport);
+ 	req->peer_port = cpu_to_be16(f->fs.val.fport);
+ 	req->local_ip_hi = *(__be64 *)(&f->fs.val.lip);
+ 	req->local_ip_lo = *(((__be64 *)&f->fs.val.lip) + 1);
+ 	req->peer_ip_hi = *(__be64 *)(&f->fs.val.fip);
+ 	req->peer_ip_lo = *(((__be64 *)&f->fs.val.fip) + 1);
+ 	req->opt0 = cpu_to_be64(NAGLE_V(f->fs.newvlan == VLAN_REMOVE ||
+ 					f->fs.newvlan == VLAN_REWRITE) |
+ 				DELACK_V(f->fs.hitcnts) |
+ 				L2T_IDX_V(f->l2t ? f->l2t->idx : 0) |
+ 				SMAC_SEL_V((cxgb4_port_viid(f->dev) &
+ 					    0x7F) << 1) |
+ 				TX_CHAN_V(f->fs.eport) |
+ 				NO_CONG_V(f->fs.rpttid) |
+ 				ULP_MODE_V(f->fs.nat_mode ?
+ 					   ULP_MODE_TCPDDP : ULP_MODE_NONE) |
+ 				TCAM_BYPASS_F | NON_OFFLOAD_F);
+ 	t6req->params = cpu_to_be64(FILTER_TUPLE_V(hash_filter_ntuple(&f->fs,
+ 								      f->dev)));
+ 	t6req->opt2 = htonl(RSS_QUEUE_VALID_F |
+ 			    RSS_QUEUE_V(f->fs.iq) |
+ 			    TX_QUEUE_V(f->fs.nat_mode) |
+ 			    T5_OPT_2_VALID_F |
+ 			    RX_CHANNEL_F |
+ 			    CONG_CNTRL_V((f->fs.action == FILTER_DROP) |
+ 					 (f->fs.dirsteer << 1)) |
+ 			    PACE_V((f->fs.maskhash) |
+ 				   ((f->fs.dirsteerhash) << 1)) |
+ 			    CCTRL_ECN_V(f->fs.action == FILTER_SWITCH));
+ }
+ 
+ static void mk_act_open_req(struct filter_entry *f, struct sk_buff *skb,
+ 			    unsigned int qid_filterid, struct adapter *adap)
+ {
+ 	struct cpl_t6_act_open_req *t6req = NULL;
+ 	struct cpl_act_open_req *req = NULL;
+ 
+ 	t6req = (struct cpl_t6_act_open_req *)__skb_put(skb, sizeof(*t6req));
+ 	INIT_TP_WR(t6req, 0);
+ 	req = (struct cpl_act_open_req *)t6req;
+ 	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ACT_OPEN_REQ, qid_filterid));
+ 	req->local_port = cpu_to_be16(f->fs.val.lport);
+ 	req->peer_port = cpu_to_be16(f->fs.val.fport);
+ 	req->local_ip = f->fs.val.lip[0] | f->fs.val.lip[1] << 8 |
+ 		f->fs.val.lip[2] << 16 | f->fs.val.lip[3] << 24;
+ 	req->peer_ip = f->fs.val.fip[0] | f->fs.val.fip[1] << 8 |
+ 		f->fs.val.fip[2] << 16 | f->fs.val.fip[3] << 24;
+ 	req->opt0 = cpu_to_be64(NAGLE_V(f->fs.newvlan == VLAN_REMOVE ||
+ 					f->fs.newvlan == VLAN_REWRITE) |
+ 				DELACK_V(f->fs.hitcnts) |
+ 				L2T_IDX_V(f->l2t ? f->l2t->idx : 0) |
+ 				SMAC_SEL_V((cxgb4_port_viid(f->dev) &
+ 					    0x7F) << 1) |
+ 				TX_CHAN_V(f->fs.eport) |
+ 				NO_CONG_V(f->fs.rpttid) |
+ 				ULP_MODE_V(f->fs.nat_mode ?
+ 					   ULP_MODE_TCPDDP : ULP_MODE_NONE) |
+ 				TCAM_BYPASS_F | NON_OFFLOAD_F);
+ 
+ 	t6req->params = cpu_to_be64(FILTER_TUPLE_V(hash_filter_ntuple(&f->fs,
+ 								      f->dev)));
+ 	t6req->opt2 = htonl(RSS_QUEUE_VALID_F |
+ 			    RSS_QUEUE_V(f->fs.iq) |
+ 			    TX_QUEUE_V(f->fs.nat_mode) |
+ 			    T5_OPT_2_VALID_F |
+ 			    RX_CHANNEL_F |
+ 			    CONG_CNTRL_V((f->fs.action == FILTER_DROP) |
+ 					 (f->fs.dirsteer << 1)) |
+ 			    PACE_V((f->fs.maskhash) |
+ 				   ((f->fs.dirsteerhash) << 1)) |
+ 			    CCTRL_ECN_V(f->fs.action == FILTER_SWITCH));
+ }
+ 
+ static int cxgb4_set_hash_filter(struct net_device *dev,
+ 				 struct ch_filter_specification *fs,
+ 				 struct filter_ctx *ctx)
+ {
+ 	struct adapter *adapter = netdev2adap(dev);
+ 	struct tid_info *t = &adapter->tids;
+ 	struct filter_entry *f;
+ 	struct sk_buff *skb;
+ 	int iq, atid, size;
+ 	int ret = 0;
+ 	u32 iconf;
+ 
+ 	fill_default_mask(fs);
+ 	ret = validate_filter(dev, fs);
+ 	if (ret)
+ 		return ret;
+ 
+ 	iq = get_filter_steerq(dev, fs);
+ 	if (iq < 0)
+ 		return iq;
+ 
+ 	f = kzalloc(sizeof(*f), GFP_KERNEL);
+ 	if (!f)
+ 		return -ENOMEM;
+ 
+ 	f->fs = *fs;
+ 	f->ctx = ctx;
+ 	f->dev = dev;
+ 	f->fs.iq = iq;
+ 
+ 	/* If the new filter requires loopback Destination MAC and/or VLAN
+ 	 * rewriting then we need to allocate a Layer 2 Table (L2T) entry for
+ 	 * the filter.
+ 	 */
+ 	if (f->fs.newdmac || f->fs.newvlan) {
+ 		/* allocate L2T entry for new filter */
+ 		f->l2t = t4_l2t_alloc_switching(adapter, f->fs.vlan,
+ 						f->fs.eport, f->fs.dmac);
+ 		if (!f->l2t) {
+ 			ret = -ENOMEM;
+ 			goto out_err;
+ 		}
+ 	}
+ 
+ 	/* If the new filter requires loopback Source MAC rewriting then
+ 	 * we need to allocate a SMT entry for the filter.
+ 	 */
+ 	if (f->fs.newsmac) {
+ 		f->smt = cxgb4_smt_alloc_switching(f->dev, f->fs.smac);
+ 		if (!f->smt) {
+ 			if (f->l2t) {
+ 				cxgb4_l2t_release(f->l2t);
+ 				f->l2t = NULL;
+ 			}
+ 			ret = -ENOMEM;
+ 			goto free_l2t;
+ 		}
+ 	}
+ 
+ 	atid = cxgb4_alloc_atid(t, f);
+ 	if (atid < 0) {
+ 		ret = atid;
+ 		goto free_smt;
+ 	}
+ 
+ 	iconf = adapter->params.tp.ingress_config;
+ 	if (iconf & VNIC_F) {
+ 		f->fs.val.ovlan = (fs->val.pf << 13) | fs->val.vf;
+ 		f->fs.mask.ovlan = (fs->mask.pf << 13) | fs->mask.vf;
+ 		f->fs.val.ovlan_vld = fs->val.pfvf_vld;
+ 		f->fs.mask.ovlan_vld = fs->mask.pfvf_vld;
+ 	}
+ 
+ 	size = sizeof(struct cpl_t6_act_open_req);
+ 	if (f->fs.type) {
+ 		ret = cxgb4_clip_get(f->dev, (const u32 *)&f->fs.val.lip, 1);
+ 		if (ret)
+ 			goto free_atid;
+ 
+ 		skb = alloc_skb(size, GFP_KERNEL);
+ 		if (!skb) {
+ 			ret = -ENOMEM;
+ 			goto free_clip;
+ 		}
+ 
+ 		mk_act_open_req6(f, skb,
+ 				 ((adapter->sge.fw_evtq.abs_id << 14) | atid),
+ 				 adapter);
+ 	} else {
+ 		skb = alloc_skb(size, GFP_KERNEL);
+ 		if (!skb) {
+ 			ret = -ENOMEM;
+ 			goto free_atid;
+ 		}
+ 
+ 		mk_act_open_req(f, skb,
+ 				((adapter->sge.fw_evtq.abs_id << 14) | atid),
+ 				adapter);
+ 	}
+ 
+ 	f->pending = 1;
+ 	set_wr_txq(skb, CPL_PRIORITY_SETUP, f->fs.val.iport & 0x3);
+ 	t4_ofld_send(adapter, skb);
+ 	return 0;
+ 
+ free_clip:
+ 	cxgb4_clip_release(f->dev, (const u32 *)&f->fs.val.lip, 1);
+ 
+ free_atid:
+ 	cxgb4_free_atid(t, atid);
+ 
+ free_smt:
+ 	if (f->smt) {
+ 		cxgb4_smt_release(f->smt);
+ 		f->smt = NULL;
+ 	}
+ 
+ free_l2t:
+ 	if (f->l2t) {
+ 		cxgb4_l2t_release(f->l2t);
+ 		f->l2t = NULL;
+ 	}
+ 
+ out_err:
+ 	kfree(f);
+ 	return ret;
+ }
+ 
++>>>>>>> a882d20cdb77 (cxgb4: fix error return code in cxgb4_set_hash_filter())
  /* Check a Chelsio Filter Request for validity, convert it into our internal
   * format and send it to the hardware.  Return 0 on success, an error number
   * otherwise.  We attach any provided filter operation context to the internal
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
