md: prepare for managing resync I/O pages in clean way

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] prepare for managing resync I/O pages in clean way (Nigel Croxon) [1494474]
Rebuild_FUZZ: 96.15%
commit-author Ming Lei <tom.leiming@gmail.com>
commit 513e2faa0138462ce014e1b0e226ca45c83bc6c1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/513e2faa.failed

Now resync I/O use bio's bec table to manage pages,
this way is very hacky, and may not work any more
once multipage bvec is introduced.

So introduce helpers and new data structure for
managing resync I/O pages more cleanly.

	Signed-off-by: Ming Lei <tom.leiming@gmail.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 513e2faa0138462ce014e1b0e226ca45c83bc6c1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/md.h
diff --cc drivers/md/md.h
index 629c80ea88dd,0418b29945e7..000000000000
--- a/drivers/md/md.h
+++ b/drivers/md/md.h
@@@ -723,4 -711,66 +723,69 @@@ static inline void mddev_clear_unsuppor
  {
  	mddev->flags &= ~unsupported_flags;
  }
++<<<<<<< HEAD
++=======
+ 
+ static inline void mddev_check_writesame(struct mddev *mddev, struct bio *bio)
+ {
+ 	if (bio_op(bio) == REQ_OP_WRITE_SAME &&
+ 	    !bdev_get_queue(bio->bi_bdev)->limits.max_write_same_sectors)
+ 		mddev->queue->limits.max_write_same_sectors = 0;
+ }
+ 
+ /* Maximum size of each resync request */
+ #define RESYNC_BLOCK_SIZE (64*1024)
+ #define RESYNC_PAGES ((RESYNC_BLOCK_SIZE + PAGE_SIZE-1) / PAGE_SIZE)
+ 
+ /* for managing resync I/O pages */
+ struct resync_pages {
+ 	unsigned	idx;	/* for get/put page from the pool */
+ 	void		*raid_bio;
+ 	struct page	*pages[RESYNC_PAGES];
+ };
+ 
+ static inline int resync_alloc_pages(struct resync_pages *rp,
+ 				     gfp_t gfp_flags)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < RESYNC_PAGES; i++) {
+ 		rp->pages[i] = alloc_page(gfp_flags);
+ 		if (!rp->pages[i])
+ 			goto out_free;
+ 	}
+ 
+ 	return 0;
+ 
+ out_free:
+ 	while (--i >= 0)
+ 		put_page(rp->pages[i]);
+ 	return -ENOMEM;
+ }
+ 
+ static inline void resync_free_pages(struct resync_pages *rp)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < RESYNC_PAGES; i++)
+ 		put_page(rp->pages[i]);
+ }
+ 
+ static inline void resync_get_all_pages(struct resync_pages *rp)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < RESYNC_PAGES; i++)
+ 		get_page(rp->pages[i]);
+ }
+ 
+ static inline struct page *resync_fetch_page(struct resync_pages *rp,
+ 					     unsigned idx)
+ {
+ 	if (WARN_ON_ONCE(idx >= RESYNC_PAGES))
+ 		return NULL;
+ 	return rp->pages[idx];
+ }
+ 
++>>>>>>> 513e2faa0138 (md: prepare for managing resync I/O pages in clean way)
  #endif /* _MD_MD_H */
* Unmerged path drivers/md/md.h
