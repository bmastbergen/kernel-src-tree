cpu/hotplug: Online siblings when SMT control is turned on

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 215af5499d9e2b55f111d2431ea20218115f29b3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/215af549.failed

Writing 'off' to /sys/devices/system/cpu/smt/control offlines all SMT
siblings. Writing 'on' merily enables the abilify to online them, but does
not online them automatically.

Make 'on' more useful by onlining all offline siblings.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 215af5499d9e2b55f111d2431ea20218115f29b3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cpu.c
diff --cc kernel/cpu.c
index 0d9e250d0ea0,d79e24df2420..000000000000
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@@ -707,6 -1268,881 +707,884 @@@ void notify_cpu_starting(unsigned int c
  
  #endif /* CONFIG_SMP */
  
++<<<<<<< HEAD
++=======
+ /* Boot processor state steps */
+ static struct cpuhp_step cpuhp_hp_states[] = {
+ 	[CPUHP_OFFLINE] = {
+ 		.name			= "offline",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= NULL,
+ 	},
+ #ifdef CONFIG_SMP
+ 	[CPUHP_CREATE_THREADS]= {
+ 		.name			= "threads:prepare",
+ 		.startup.single		= smpboot_create_threads,
+ 		.teardown.single	= NULL,
+ 		.cant_stop		= true,
+ 	},
+ 	[CPUHP_PERF_PREPARE] = {
+ 		.name			= "perf:prepare",
+ 		.startup.single		= perf_event_init_cpu,
+ 		.teardown.single	= perf_event_exit_cpu,
+ 	},
+ 	[CPUHP_WORKQUEUE_PREP] = {
+ 		.name			= "workqueue:prepare",
+ 		.startup.single		= workqueue_prepare_cpu,
+ 		.teardown.single	= NULL,
+ 	},
+ 	[CPUHP_HRTIMERS_PREPARE] = {
+ 		.name			= "hrtimers:prepare",
+ 		.startup.single		= hrtimers_prepare_cpu,
+ 		.teardown.single	= hrtimers_dead_cpu,
+ 	},
+ 	[CPUHP_SMPCFD_PREPARE] = {
+ 		.name			= "smpcfd:prepare",
+ 		.startup.single		= smpcfd_prepare_cpu,
+ 		.teardown.single	= smpcfd_dead_cpu,
+ 	},
+ 	[CPUHP_RELAY_PREPARE] = {
+ 		.name			= "relay:prepare",
+ 		.startup.single		= relay_prepare_cpu,
+ 		.teardown.single	= NULL,
+ 	},
+ 	[CPUHP_SLAB_PREPARE] = {
+ 		.name			= "slab:prepare",
+ 		.startup.single		= slab_prepare_cpu,
+ 		.teardown.single	= slab_dead_cpu,
+ 	},
+ 	[CPUHP_RCUTREE_PREP] = {
+ 		.name			= "RCU/tree:prepare",
+ 		.startup.single		= rcutree_prepare_cpu,
+ 		.teardown.single	= rcutree_dead_cpu,
+ 	},
+ 	/*
+ 	 * On the tear-down path, timers_dead_cpu() must be invoked
+ 	 * before blk_mq_queue_reinit_notify() from notify_dead(),
+ 	 * otherwise a RCU stall occurs.
+ 	 */
+ 	[CPUHP_TIMERS_PREPARE] = {
+ 		.name			= "timers:dead",
+ 		.startup.single		= timers_prepare_cpu,
+ 		.teardown.single	= timers_dead_cpu,
+ 	},
+ 	/* Kicks the plugged cpu into life */
+ 	[CPUHP_BRINGUP_CPU] = {
+ 		.name			= "cpu:bringup",
+ 		.startup.single		= bringup_cpu,
+ 		.teardown.single	= NULL,
+ 		.cant_stop		= true,
+ 	},
+ 	/* Final state before CPU kills itself */
+ 	[CPUHP_AP_IDLE_DEAD] = {
+ 		.name			= "idle:dead",
+ 	},
+ 	/*
+ 	 * Last state before CPU enters the idle loop to die. Transient state
+ 	 * for synchronization.
+ 	 */
+ 	[CPUHP_AP_OFFLINE] = {
+ 		.name			= "ap:offline",
+ 		.cant_stop		= true,
+ 	},
+ 	/* First state is scheduler control. Interrupts are disabled */
+ 	[CPUHP_AP_SCHED_STARTING] = {
+ 		.name			= "sched:starting",
+ 		.startup.single		= sched_cpu_starting,
+ 		.teardown.single	= sched_cpu_dying,
+ 	},
+ 	[CPUHP_AP_RCUTREE_DYING] = {
+ 		.name			= "RCU/tree:dying",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= rcutree_dying_cpu,
+ 	},
+ 	[CPUHP_AP_SMPCFD_DYING] = {
+ 		.name			= "smpcfd:dying",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= smpcfd_dying_cpu,
+ 	},
+ 	/* Entry state on starting. Interrupts enabled from here on. Transient
+ 	 * state for synchronsization */
+ 	[CPUHP_AP_ONLINE] = {
+ 		.name			= "ap:online",
+ 	},
+ 	/*
+ 	 * Handled on controll processor until the plugged processor manages
+ 	 * this itself.
+ 	 */
+ 	[CPUHP_TEARDOWN_CPU] = {
+ 		.name			= "cpu:teardown",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= takedown_cpu,
+ 		.cant_stop		= true,
+ 	},
+ 	/* Handle smpboot threads park/unpark */
+ 	[CPUHP_AP_SMPBOOT_THREADS] = {
+ 		.name			= "smpboot/threads:online",
+ 		.startup.single		= smpboot_unpark_threads,
+ 		.teardown.single	= smpboot_park_threads,
+ 	},
+ 	[CPUHP_AP_IRQ_AFFINITY_ONLINE] = {
+ 		.name			= "irq/affinity:online",
+ 		.startup.single		= irq_affinity_online_cpu,
+ 		.teardown.single	= NULL,
+ 	},
+ 	[CPUHP_AP_PERF_ONLINE] = {
+ 		.name			= "perf:online",
+ 		.startup.single		= perf_event_init_cpu,
+ 		.teardown.single	= perf_event_exit_cpu,
+ 	},
+ 	[CPUHP_AP_WORKQUEUE_ONLINE] = {
+ 		.name			= "workqueue:online",
+ 		.startup.single		= workqueue_online_cpu,
+ 		.teardown.single	= workqueue_offline_cpu,
+ 	},
+ 	[CPUHP_AP_RCUTREE_ONLINE] = {
+ 		.name			= "RCU/tree:online",
+ 		.startup.single		= rcutree_online_cpu,
+ 		.teardown.single	= rcutree_offline_cpu,
+ 	},
+ #endif
+ 	/*
+ 	 * The dynamically registered state space is here
+ 	 */
+ 
+ #ifdef CONFIG_SMP
+ 	/* Last state is scheduler control setting the cpu active */
+ 	[CPUHP_AP_ACTIVE] = {
+ 		.name			= "sched:active",
+ 		.startup.single		= sched_cpu_activate,
+ 		.teardown.single	= sched_cpu_deactivate,
+ 	},
+ #endif
+ 
+ 	/* CPU is fully up and running. */
+ 	[CPUHP_ONLINE] = {
+ 		.name			= "online",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= NULL,
+ 	},
+ };
+ 
+ /* Sanity check for callbacks */
+ static int cpuhp_cb_check(enum cpuhp_state state)
+ {
+ 	if (state <= CPUHP_OFFLINE || state >= CPUHP_ONLINE)
+ 		return -EINVAL;
+ 	return 0;
+ }
+ 
+ /*
+  * Returns a free for dynamic slot assignment of the Online state. The states
+  * are protected by the cpuhp_slot_states mutex and an empty slot is identified
+  * by having no name assigned.
+  */
+ static int cpuhp_reserve_state(enum cpuhp_state state)
+ {
+ 	enum cpuhp_state i, end;
+ 	struct cpuhp_step *step;
+ 
+ 	switch (state) {
+ 	case CPUHP_AP_ONLINE_DYN:
+ 		step = cpuhp_hp_states + CPUHP_AP_ONLINE_DYN;
+ 		end = CPUHP_AP_ONLINE_DYN_END;
+ 		break;
+ 	case CPUHP_BP_PREPARE_DYN:
+ 		step = cpuhp_hp_states + CPUHP_BP_PREPARE_DYN;
+ 		end = CPUHP_BP_PREPARE_DYN_END;
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	for (i = state; i <= end; i++, step++) {
+ 		if (!step->name)
+ 			return i;
+ 	}
+ 	WARN(1, "No more dynamic states available for CPU hotplug\n");
+ 	return -ENOSPC;
+ }
+ 
+ static int cpuhp_store_callbacks(enum cpuhp_state state, const char *name,
+ 				 int (*startup)(unsigned int cpu),
+ 				 int (*teardown)(unsigned int cpu),
+ 				 bool multi_instance)
+ {
+ 	/* (Un)Install the callbacks for further cpu hotplug operations */
+ 	struct cpuhp_step *sp;
+ 	int ret = 0;
+ 
+ 	/*
+ 	 * If name is NULL, then the state gets removed.
+ 	 *
+ 	 * CPUHP_AP_ONLINE_DYN and CPUHP_BP_PREPARE_DYN are handed out on
+ 	 * the first allocation from these dynamic ranges, so the removal
+ 	 * would trigger a new allocation and clear the wrong (already
+ 	 * empty) state, leaving the callbacks of the to be cleared state
+ 	 * dangling, which causes wreckage on the next hotplug operation.
+ 	 */
+ 	if (name && (state == CPUHP_AP_ONLINE_DYN ||
+ 		     state == CPUHP_BP_PREPARE_DYN)) {
+ 		ret = cpuhp_reserve_state(state);
+ 		if (ret < 0)
+ 			return ret;
+ 		state = ret;
+ 	}
+ 	sp = cpuhp_get_step(state);
+ 	if (name && sp->name)
+ 		return -EBUSY;
+ 
+ 	sp->startup.single = startup;
+ 	sp->teardown.single = teardown;
+ 	sp->name = name;
+ 	sp->multi_instance = multi_instance;
+ 	INIT_HLIST_HEAD(&sp->list);
+ 	return ret;
+ }
+ 
+ static void *cpuhp_get_teardown_cb(enum cpuhp_state state)
+ {
+ 	return cpuhp_get_step(state)->teardown.single;
+ }
+ 
+ /*
+  * Call the startup/teardown function for a step either on the AP or
+  * on the current CPU.
+  */
+ static int cpuhp_issue_call(int cpu, enum cpuhp_state state, bool bringup,
+ 			    struct hlist_node *node)
+ {
+ 	struct cpuhp_step *sp = cpuhp_get_step(state);
+ 	int ret;
+ 
+ 	/*
+ 	 * If there's nothing to do, we done.
+ 	 * Relies on the union for multi_instance.
+ 	 */
+ 	if ((bringup && !sp->startup.single) ||
+ 	    (!bringup && !sp->teardown.single))
+ 		return 0;
+ 	/*
+ 	 * The non AP bound callbacks can fail on bringup. On teardown
+ 	 * e.g. module removal we crash for now.
+ 	 */
+ #ifdef CONFIG_SMP
+ 	if (cpuhp_is_ap_state(state))
+ 		ret = cpuhp_invoke_ap_callback(cpu, state, bringup, node);
+ 	else
+ 		ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ #else
+ 	ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ #endif
+ 	BUG_ON(ret && !bringup);
+ 	return ret;
+ }
+ 
+ /*
+  * Called from __cpuhp_setup_state on a recoverable failure.
+  *
+  * Note: The teardown callbacks for rollback are not allowed to fail!
+  */
+ static void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,
+ 				   struct hlist_node *node)
+ {
+ 	int cpu;
+ 
+ 	/* Roll back the already executed steps on the other cpus */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpu >= failedcpu)
+ 			break;
+ 
+ 		/* Did we invoke the startup call on that cpu ? */
+ 		if (cpustate >= state)
+ 			cpuhp_issue_call(cpu, state, false, node);
+ 	}
+ }
+ 
+ int __cpuhp_state_add_instance_cpuslocked(enum cpuhp_state state,
+ 					  struct hlist_node *node,
+ 					  bool invoke)
+ {
+ 	struct cpuhp_step *sp;
+ 	int cpu;
+ 	int ret;
+ 
+ 	lockdep_assert_cpus_held();
+ 
+ 	sp = cpuhp_get_step(state);
+ 	if (sp->multi_instance == false)
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 
+ 	if (!invoke || !sp->startup.multi)
+ 		goto add_node;
+ 
+ 	/*
+ 	 * Try to call the startup callback for each present cpu
+ 	 * depending on the hotplug state of the cpu.
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate < state)
+ 			continue;
+ 
+ 		ret = cpuhp_issue_call(cpu, state, true, node);
+ 		if (ret) {
+ 			if (sp->teardown.multi)
+ 				cpuhp_rollback_install(cpu, state, node);
+ 			goto unlock;
+ 		}
+ 	}
+ add_node:
+ 	ret = 0;
+ 	hlist_add_head(node, &sp->list);
+ unlock:
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	return ret;
+ }
+ 
+ int __cpuhp_state_add_instance(enum cpuhp_state state, struct hlist_node *node,
+ 			       bool invoke)
+ {
+ 	int ret;
+ 
+ 	cpus_read_lock();
+ 	ret = __cpuhp_state_add_instance_cpuslocked(state, node, invoke);
+ 	cpus_read_unlock();
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(__cpuhp_state_add_instance);
+ 
+ /**
+  * __cpuhp_setup_state_cpuslocked - Setup the callbacks for an hotplug machine state
+  * @state:		The state to setup
+  * @invoke:		If true, the startup function is invoked for cpus where
+  *			cpu state >= @state
+  * @startup:		startup callback function
+  * @teardown:		teardown callback function
+  * @multi_instance:	State is set up for multiple instances which get
+  *			added afterwards.
+  *
+  * The caller needs to hold cpus read locked while calling this function.
+  * Returns:
+  *   On success:
+  *      Positive state number if @state is CPUHP_AP_ONLINE_DYN
+  *      0 for all other states
+  *   On failure: proper (negative) error code
+  */
+ int __cpuhp_setup_state_cpuslocked(enum cpuhp_state state,
+ 				   const char *name, bool invoke,
+ 				   int (*startup)(unsigned int cpu),
+ 				   int (*teardown)(unsigned int cpu),
+ 				   bool multi_instance)
+ {
+ 	int cpu, ret = 0;
+ 	bool dynstate;
+ 
+ 	lockdep_assert_cpus_held();
+ 
+ 	if (cpuhp_cb_check(state) || !name)
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 
+ 	ret = cpuhp_store_callbacks(state, name, startup, teardown,
+ 				    multi_instance);
+ 
+ 	dynstate = state == CPUHP_AP_ONLINE_DYN;
+ 	if (ret > 0 && dynstate) {
+ 		state = ret;
+ 		ret = 0;
+ 	}
+ 
+ 	if (ret || !invoke || !startup)
+ 		goto out;
+ 
+ 	/*
+ 	 * Try to call the startup callback for each present cpu
+ 	 * depending on the hotplug state of the cpu.
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate < state)
+ 			continue;
+ 
+ 		ret = cpuhp_issue_call(cpu, state, true, NULL);
+ 		if (ret) {
+ 			if (teardown)
+ 				cpuhp_rollback_install(cpu, state, NULL);
+ 			cpuhp_store_callbacks(state, NULL, NULL, NULL, false);
+ 			goto out;
+ 		}
+ 	}
+ out:
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	/*
+ 	 * If the requested state is CPUHP_AP_ONLINE_DYN, return the
+ 	 * dynamically allocated state in case of success.
+ 	 */
+ 	if (!ret && dynstate)
+ 		return state;
+ 	return ret;
+ }
+ EXPORT_SYMBOL(__cpuhp_setup_state_cpuslocked);
+ 
+ int __cpuhp_setup_state(enum cpuhp_state state,
+ 			const char *name, bool invoke,
+ 			int (*startup)(unsigned int cpu),
+ 			int (*teardown)(unsigned int cpu),
+ 			bool multi_instance)
+ {
+ 	int ret;
+ 
+ 	cpus_read_lock();
+ 	ret = __cpuhp_setup_state_cpuslocked(state, name, invoke, startup,
+ 					     teardown, multi_instance);
+ 	cpus_read_unlock();
+ 	return ret;
+ }
+ EXPORT_SYMBOL(__cpuhp_setup_state);
+ 
+ int __cpuhp_state_remove_instance(enum cpuhp_state state,
+ 				  struct hlist_node *node, bool invoke)
+ {
+ 	struct cpuhp_step *sp = cpuhp_get_step(state);
+ 	int cpu;
+ 
+ 	BUG_ON(cpuhp_cb_check(state));
+ 
+ 	if (!sp->multi_instance)
+ 		return -EINVAL;
+ 
+ 	cpus_read_lock();
+ 	mutex_lock(&cpuhp_state_mutex);
+ 
+ 	if (!invoke || !cpuhp_get_teardown_cb(state))
+ 		goto remove;
+ 	/*
+ 	 * Call the teardown callback for each present cpu depending
+ 	 * on the hotplug state of the cpu. This function is not
+ 	 * allowed to fail currently!
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate >= state)
+ 			cpuhp_issue_call(cpu, state, false, node);
+ 	}
+ 
+ remove:
+ 	hlist_del(node);
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	cpus_read_unlock();
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(__cpuhp_state_remove_instance);
+ 
+ /**
+  * __cpuhp_remove_state_cpuslocked - Remove the callbacks for an hotplug machine state
+  * @state:	The state to remove
+  * @invoke:	If true, the teardown function is invoked for cpus where
+  *		cpu state >= @state
+  *
+  * The caller needs to hold cpus read locked while calling this function.
+  * The teardown callback is currently not allowed to fail. Think
+  * about module removal!
+  */
+ void __cpuhp_remove_state_cpuslocked(enum cpuhp_state state, bool invoke)
+ {
+ 	struct cpuhp_step *sp = cpuhp_get_step(state);
+ 	int cpu;
+ 
+ 	BUG_ON(cpuhp_cb_check(state));
+ 
+ 	lockdep_assert_cpus_held();
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	if (sp->multi_instance) {
+ 		WARN(!hlist_empty(&sp->list),
+ 		     "Error: Removing state %d which has instances left.\n",
+ 		     state);
+ 		goto remove;
+ 	}
+ 
+ 	if (!invoke || !cpuhp_get_teardown_cb(state))
+ 		goto remove;
+ 
+ 	/*
+ 	 * Call the teardown callback for each present cpu depending
+ 	 * on the hotplug state of the cpu. This function is not
+ 	 * allowed to fail currently!
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate >= state)
+ 			cpuhp_issue_call(cpu, state, false, NULL);
+ 	}
+ remove:
+ 	cpuhp_store_callbacks(state, NULL, NULL, NULL, false);
+ 	mutex_unlock(&cpuhp_state_mutex);
+ }
+ EXPORT_SYMBOL(__cpuhp_remove_state_cpuslocked);
+ 
+ void __cpuhp_remove_state(enum cpuhp_state state, bool invoke)
+ {
+ 	cpus_read_lock();
+ 	__cpuhp_remove_state_cpuslocked(state, invoke);
+ 	cpus_read_unlock();
+ }
+ EXPORT_SYMBOL(__cpuhp_remove_state);
+ 
+ #if defined(CONFIG_SYSFS) && defined(CONFIG_HOTPLUG_CPU)
+ static ssize_t show_cpuhp_state(struct device *dev,
+ 				struct device_attribute *attr, char *buf)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 
+ 	return sprintf(buf, "%d\n", st->state);
+ }
+ static DEVICE_ATTR(state, 0444, show_cpuhp_state, NULL);
+ 
+ static ssize_t write_cpuhp_target(struct device *dev,
+ 				  struct device_attribute *attr,
+ 				  const char *buf, size_t count)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 	struct cpuhp_step *sp;
+ 	int target, ret;
+ 
+ 	ret = kstrtoint(buf, 10, &target);
+ 	if (ret)
+ 		return ret;
+ 
+ #ifdef CONFIG_CPU_HOTPLUG_STATE_CONTROL
+ 	if (target < CPUHP_OFFLINE || target > CPUHP_ONLINE)
+ 		return -EINVAL;
+ #else
+ 	if (target != CPUHP_OFFLINE && target != CPUHP_ONLINE)
+ 		return -EINVAL;
+ #endif
+ 
+ 	ret = lock_device_hotplug_sysfs();
+ 	if (ret)
+ 		return ret;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	sp = cpuhp_get_step(target);
+ 	ret = !sp->name || sp->cant_stop ? -EINVAL : 0;
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (st->state < target)
+ 		ret = do_cpu_up(dev->id, target);
+ 	else
+ 		ret = do_cpu_down(dev->id, target);
+ out:
+ 	unlock_device_hotplug();
+ 	return ret ? ret : count;
+ }
+ 
+ static ssize_t show_cpuhp_target(struct device *dev,
+ 				 struct device_attribute *attr, char *buf)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 
+ 	return sprintf(buf, "%d\n", st->target);
+ }
+ static DEVICE_ATTR(target, 0644, show_cpuhp_target, write_cpuhp_target);
+ 
+ 
+ static ssize_t write_cpuhp_fail(struct device *dev,
+ 				struct device_attribute *attr,
+ 				const char *buf, size_t count)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 	struct cpuhp_step *sp;
+ 	int fail, ret;
+ 
+ 	ret = kstrtoint(buf, 10, &fail);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * Cannot fail STARTING/DYING callbacks.
+ 	 */
+ 	if (cpuhp_is_atomic_state(fail))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Cannot fail anything that doesn't have callbacks.
+ 	 */
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	sp = cpuhp_get_step(fail);
+ 	if (!sp->startup.single && !sp->teardown.single)
+ 		ret = -EINVAL;
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	if (ret)
+ 		return ret;
+ 
+ 	st->fail = fail;
+ 
+ 	return count;
+ }
+ 
+ static ssize_t show_cpuhp_fail(struct device *dev,
+ 			       struct device_attribute *attr, char *buf)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 
+ 	return sprintf(buf, "%d\n", st->fail);
+ }
+ 
+ static DEVICE_ATTR(fail, 0644, show_cpuhp_fail, write_cpuhp_fail);
+ 
+ static struct attribute *cpuhp_cpu_attrs[] = {
+ 	&dev_attr_state.attr,
+ 	&dev_attr_target.attr,
+ 	&dev_attr_fail.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group cpuhp_cpu_attr_group = {
+ 	.attrs = cpuhp_cpu_attrs,
+ 	.name = "hotplug",
+ 	NULL
+ };
+ 
+ static ssize_t show_cpuhp_states(struct device *dev,
+ 				 struct device_attribute *attr, char *buf)
+ {
+ 	ssize_t cur, res = 0;
+ 	int i;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	for (i = CPUHP_OFFLINE; i <= CPUHP_ONLINE; i++) {
+ 		struct cpuhp_step *sp = cpuhp_get_step(i);
+ 
+ 		if (sp->name) {
+ 			cur = sprintf(buf, "%3d: %s\n", i, sp->name);
+ 			buf += cur;
+ 			res += cur;
+ 		}
+ 	}
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	return res;
+ }
+ static DEVICE_ATTR(states, 0444, show_cpuhp_states, NULL);
+ 
+ static struct attribute *cpuhp_cpu_root_attrs[] = {
+ 	&dev_attr_states.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group cpuhp_cpu_root_attr_group = {
+ 	.attrs = cpuhp_cpu_root_attrs,
+ 	.name = "hotplug",
+ 	NULL
+ };
+ 
+ #ifdef CONFIG_HOTPLUG_SMT
+ 
+ static const char *smt_states[] = {
+ 	[CPU_SMT_ENABLED]		= "on",
+ 	[CPU_SMT_DISABLED]		= "off",
+ 	[CPU_SMT_FORCE_DISABLED]	= "forceoff",
+ 	[CPU_SMT_NOT_SUPPORTED]		= "notsupported",
+ };
+ 
+ static ssize_t
+ show_smt_control(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return snprintf(buf, PAGE_SIZE - 2, "%s\n", smt_states[cpu_smt_control]);
+ }
+ 
+ static void cpuhp_offline_cpu_device(unsigned int cpu)
+ {
+ 	struct device *dev = get_cpu_device(cpu);
+ 
+ 	dev->offline = true;
+ 	/* Tell user space about the state change */
+ 	kobject_uevent(&dev->kobj, KOBJ_OFFLINE);
+ }
+ 
+ static void cpuhp_online_cpu_device(unsigned int cpu)
+ {
+ 	struct device *dev = get_cpu_device(cpu);
+ 
+ 	dev->offline = false;
+ 	/* Tell user space about the state change */
+ 	kobject_uevent(&dev->kobj, KOBJ_ONLINE);
+ }
+ 
+ static int cpuhp_smt_disable(enum cpuhp_smt_control ctrlval)
+ {
+ 	int cpu, ret = 0;
+ 
+ 	cpu_maps_update_begin();
+ 	for_each_online_cpu(cpu) {
+ 		if (topology_is_primary_thread(cpu))
+ 			continue;
+ 		ret = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);
+ 		if (ret)
+ 			break;
+ 		/*
+ 		 * As this needs to hold the cpu maps lock it's impossible
+ 		 * to call device_offline() because that ends up calling
+ 		 * cpu_down() which takes cpu maps lock. cpu maps lock
+ 		 * needs to be held as this might race against in kernel
+ 		 * abusers of the hotplug machinery (thermal management).
+ 		 *
+ 		 * So nothing would update device:offline state. That would
+ 		 * leave the sysfs entry stale and prevent onlining after
+ 		 * smt control has been changed to 'off' again. This is
+ 		 * called under the sysfs hotplug lock, so it is properly
+ 		 * serialized against the regular offline usage.
+ 		 */
+ 		cpuhp_offline_cpu_device(cpu);
+ 	}
+ 	if (!ret)
+ 		cpu_smt_control = ctrlval;
+ 	cpu_maps_update_done();
+ 	return ret;
+ }
+ 
+ static int cpuhp_smt_enable(void)
+ {
+ 	int cpu, ret = 0;
+ 
+ 	cpu_maps_update_begin();
+ 	cpu_smt_control = CPU_SMT_ENABLED;
+ 	for_each_present_cpu(cpu) {
+ 		/* Skip online CPUs and CPUs on offline nodes */
+ 		if (cpu_online(cpu) || !node_online(cpu_to_node(cpu)))
+ 			continue;
+ 		ret = _cpu_up(cpu, 0, CPUHP_ONLINE);
+ 		if (ret)
+ 			break;
+ 		/* See comment in cpuhp_smt_disable() */
+ 		cpuhp_online_cpu_device(cpu);
+ 	}
+ 	cpu_maps_update_done();
+ 	return ret;
+ }
+ 
+ static ssize_t
+ store_smt_control(struct device *dev, struct device_attribute *attr,
+ 		  const char *buf, size_t count)
+ {
+ 	int ctrlval, ret;
+ 
+ 	if (sysfs_streq(buf, "on"))
+ 		ctrlval = CPU_SMT_ENABLED;
+ 	else if (sysfs_streq(buf, "off"))
+ 		ctrlval = CPU_SMT_DISABLED;
+ 	else if (sysfs_streq(buf, "forceoff"))
+ 		ctrlval = CPU_SMT_FORCE_DISABLED;
+ 	else
+ 		return -EINVAL;
+ 
+ 	if (cpu_smt_control == CPU_SMT_FORCE_DISABLED)
+ 		return -EPERM;
+ 
+ 	if (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)
+ 		return -ENODEV;
+ 
+ 	ret = lock_device_hotplug_sysfs();
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (ctrlval != cpu_smt_control) {
+ 		switch (ctrlval) {
+ 		case CPU_SMT_ENABLED:
+ 			ret = cpuhp_smt_enable();
+ 			break;
+ 		case CPU_SMT_DISABLED:
+ 		case CPU_SMT_FORCE_DISABLED:
+ 			ret = cpuhp_smt_disable(ctrlval);
+ 			break;
+ 		}
+ 	}
+ 
+ 	unlock_device_hotplug();
+ 	return ret ? ret : count;
+ }
+ static DEVICE_ATTR(control, 0644, show_smt_control, store_smt_control);
+ 
+ static ssize_t
+ show_smt_active(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	bool active = topology_max_smt_threads() > 1;
+ 
+ 	return snprintf(buf, PAGE_SIZE - 2, "%d\n", active);
+ }
+ static DEVICE_ATTR(active, 0444, show_smt_active, NULL);
+ 
+ static struct attribute *cpuhp_smt_attrs[] = {
+ 	&dev_attr_control.attr,
+ 	&dev_attr_active.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group cpuhp_smt_attr_group = {
+ 	.attrs = cpuhp_smt_attrs,
+ 	.name = "smt",
+ 	NULL
+ };
+ 
+ static int __init cpu_smt_state_init(void)
+ {
+ 	if (!topology_smt_supported())
+ 		cpu_smt_control = CPU_SMT_NOT_SUPPORTED;
+ 
+ 	return sysfs_create_group(&cpu_subsys.dev_root->kobj,
+ 				  &cpuhp_smt_attr_group);
+ }
+ 
+ #else
+ static inline int cpu_smt_state_init(void) { return 0; }
+ #endif
+ 
+ static int __init cpuhp_sysfs_init(void)
+ {
+ 	int cpu, ret;
+ 
+ 	ret = cpu_smt_state_init();
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = sysfs_create_group(&cpu_subsys.dev_root->kobj,
+ 				 &cpuhp_cpu_root_attr_group);
+ 	if (ret)
+ 		return ret;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct device *dev = get_cpu_device(cpu);
+ 
+ 		if (!dev)
+ 			continue;
+ 		ret = sysfs_create_group(&dev->kobj, &cpuhp_cpu_attr_group);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 	return 0;
+ }
+ device_initcall(cpuhp_sysfs_init);
+ #endif
+ 
++>>>>>>> 215af5499d9e (cpu/hotplug: Online siblings when SMT control is turned on)
  /*
   * cpu_bit_bitmap[] is a special, "compressed" data structure that
   * represents all NR_CPUS bits binary values of 1<<nr.
* Unmerged path kernel/cpu.c
