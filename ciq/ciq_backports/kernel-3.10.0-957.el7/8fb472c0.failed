ipmr: improve hash scalability

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Nikolay Aleksandrov <nikolay@cumulusnetworks.com>
commit 8fb472c09b9df478a062eacc7841448e40fc3c17
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/8fb472c0.failed

Recently we started using ipmr with thousands of entries and easily hit
soft lockups on smaller devices. The reason is that the hash function
uses the high order bits from the src and dst, but those don't change in
many common cases, also the hash table  is only 64 elements so with
thousands it doesn't scale at all.
This patch migrates the hash table to rhashtable, and in particular the
rhl interface which allows for duplicate elements to be chained because
of the MFC_PROXY support (*,G; *,*,oif cases) which allows for multiple
duplicate entries to be added with different interfaces (IMO wrong, but
it's been in for a long time).

And here are some results from tests I've run in a VM:
 mr_table size (default, allocated for all namespaces):
  Before                    After
   49304 bytes               2400 bytes

 Add 65000 routes (the diff is much larger on smaller devices):
  Before                    After
   1m42s                     58s

 Forwarding 256 byte packets with 65000 routes (test done in a VM):
  Before                    After
   3 Mbps / ~1465 pps        122 Mbps / ~59000 pps

As a bonus we no longer see the soft lockups on smaller devices which
showed up even with 2000 entries before.

	Signed-off-by: Nikolay Aleksandrov <nikolay@cumulusnetworks.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 8fb472c09b9df478a062eacc7841448e40fc3c17)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mroute.h
#	net/ipv4/ipmr.c
diff --cc include/linux/mroute.h
index 50e6b3419aab,d7f63339ef0b..000000000000
--- a/include/linux/mroute.h
+++ b/include/linux/mroute.h
@@@ -59,6 -60,25 +60,28 @@@ struct vif_device 
  
  #define VIFF_STATIC 0x8000
  
++<<<<<<< HEAD
++=======
+ #define VIF_EXISTS(_mrt, _idx) ((_mrt)->vif_table[_idx].dev != NULL)
+ 
+ struct mr_table {
+ 	struct list_head	list;
+ 	possible_net_t		net;
+ 	u32			id;
+ 	struct sock __rcu	*mroute_sk;
+ 	struct timer_list	ipmr_expire_timer;
+ 	struct list_head	mfc_unres_queue;
+ 	struct vif_device	vif_table[MAXVIFS];
+ 	struct rhltable		mfc_hash;
+ 	struct list_head	mfc_cache_list;
+ 	int			maxvif;
+ 	atomic_t		cache_resolve_queue_len;
+ 	bool			mroute_do_assert;
+ 	bool			mroute_do_pim;
+ 	int			mroute_reg_vif_num;
+ };
+ 
++>>>>>>> 8fb472c09b9d (ipmr: improve hash scalability)
  /* mfc_flags:
   * MFC_STATIC - the entry was added statically (not by a routing daemon)
   */
@@@ -92,14 -144,6 +147,17 @@@ struct mfc_cache 
  	struct rcu_head	rcu;
  };
  
++<<<<<<< HEAD
 +#define MFC_LINES 64
 +
 +#ifdef __BIG_ENDIAN
 +#define MFC_HASH(a,b)	(((((__force u32)(__be32)a)>>24)^(((__force u32)(__be32)b)>>26))&(MFC_LINES-1))
 +#else
 +#define MFC_HASH(a,b)	((((__force u32)(__be32)a)^(((__force u32)(__be32)b)>>2))&(MFC_LINES-1))
 +#endif
 +
++=======
++>>>>>>> 8fb472c09b9d (ipmr: improve hash scalability)
  struct rtmsg;
  int ipmr_get_route(struct net *net, struct sk_buff *skb,
  		   __be32 saddr, __be32 daddr,
diff --cc net/ipv4/ipmr.c
index 2741c7ea1966,beacd028848c..000000000000
--- a/net/ipv4/ipmr.c
+++ b/net/ipv4/ipmr.c
@@@ -316,10 -322,13 +336,9 @@@ static const struct rhashtable_params i
  static struct mr_table *ipmr_new_table(struct net *net, u32 id)
  {
  	struct mr_table *mrt;
- 	unsigned int i;
  
 -	/* "pimreg%u" should not exceed 16 bytes (IFNAMSIZ) */
 -	if (id != RT_TABLE_DEFAULT && id >= 1000000000)
 -		return ERR_PTR(-EINVAL);
 -
  	mrt = ipmr_get_table(net, id);
 -	if (mrt)
 +	if (mrt != NULL)
  		return mrt;
  
  	mrt = kzalloc(sizeof(*mrt), GFP_KERNEL);
@@@ -870,9 -919,27 +897,33 @@@ skip
  	return ipmr_cache_find_any_parent(mrt, vifi);
  }
  
++<<<<<<< HEAD
 +/*
 + *	Allocate a multicast cache entry
 + */
++=======
+ /* Look for a (S,G,iif) entry if parent != -1 */
+ static struct mfc_cache *ipmr_cache_find_parent(struct mr_table *mrt,
+ 						__be32 origin, __be32 mcastgrp,
+ 						int parent)
+ {
+ 	struct mfc_cache_cmp_arg arg = {
+ 			.mfc_mcastgrp = mcastgrp,
+ 			.mfc_origin = origin,
+ 	};
+ 	struct rhlist_head *tmp, *list;
+ 	struct mfc_cache *c;
+ 
+ 	list = rhltable_lookup(&mrt->mfc_hash, &arg, ipmr_rht_params);
+ 	rhl_for_each_entry_rcu(c, tmp, list, mnode)
+ 		if (parent == -1 || parent == c->mfc_parent)
+ 			return c;
+ 
+ 	return NULL;
+ }
+ 
+ /* Allocate a multicast cache entry */
++>>>>>>> 8fb472c09b9d (ipmr: improve hash scalability)
  static struct mfc_cache *ipmr_cache_alloc(void)
  {
  	struct mfc_cache *c = kmem_cache_zalloc(mrt_cachep, GFP_KERNEL);
@@@ -1020,17 -1071,14 +1071,17 @@@ static int ipmr_cache_report(struct mr_
  	return ret;
  }
  
 -/* Queue a packet for resolution. It gets locked cache entry! */
 -static int ipmr_cache_unresolved(struct mr_table *mrt, vifi_t vifi,
 -				 struct sk_buff *skb)
 +/*
 + *	Queue a packet for resolution. It gets locked cache entry!
 + */
 +
 +static int
 +ipmr_cache_unresolved(struct mr_table *mrt, vifi_t vifi, struct sk_buff *skb)
  {
+ 	const struct iphdr *iph = ip_hdr(skb);
+ 	struct mfc_cache *c;
  	bool found = false;
  	int err;
- 	struct mfc_cache *c;
- 	const struct iphdr *iph = ip_hdr(skb);
  
  	spin_lock_bh(&mfc_unres_lock);
  	list_for_each_entry(c, &mrt->mfc_unres_queue, list) {
@@@ -1165,11 -1200,16 +1209,24 @@@ static int ipmr_mfc_add(struct net *net
  	if (!mrtsock)
  		c->mfc_flags |= MFC_STATIC;
  
++<<<<<<< HEAD
 +	list_add_rcu(&c->list, &mrt->mfc_cache_array[line]);
 +
 +	/*
 +	 *	Check to see if we resolved a queued list. If so we
 +	 *	need to send on the frames and tidy up.
++=======
+ 	ret = rhltable_insert_key(&mrt->mfc_hash, &c->cmparg, &c->mnode,
+ 				  ipmr_rht_params);
+ 	if (ret) {
+ 		pr_err("ipmr: rhtable insert error %d\n", ret);
+ 		ipmr_cache_free(c);
+ 		return ret;
+ 	}
+ 	list_add_tail_rcu(&c->list, &mrt->mfc_cache_list);
+ 	/* Check to see if we resolved a queued list. If so we
+ 	 * need to send on the frames and tidy up.
++>>>>>>> 8fb472c09b9d (ipmr: improve hash scalability)
  	 */
  	found = false;
  	spin_lock_bh(&mfc_unres_lock);
@@@ -1194,18 -1234,14 +1251,18 @@@
  	return 0;
  }
  
 -/* Close the multicast socket, and clear the vif tables etc */
 +/*
 + *	Close the multicast socket, and clear the vif tables etc
 + */
 +
  static void mroute_clean_tables(struct mr_table *mrt, bool all)
  {
- 	int i;
+ 	struct mfc_cache *c, *tmp;
  	LIST_HEAD(list);
- 	struct mfc_cache *c, *next;
+ 	int i;
  
  	/* Shut down all active vif entries */
 +
  	for (i = 0; i < mrt->maxvif; i++) {
  		if (!all && (mrt->vif_table[i].flags & VIFF_STATIC))
  			continue;
@@@ -1214,15 -1250,13 +1271,25 @@@
  	unregister_netdevice_many(&list);
  
  	/* Wipe the cache */
++<<<<<<< HEAD
 +
 +	for (i = 0; i < MFC_LINES; i++) {
 +		list_for_each_entry_safe(c, next, &mrt->mfc_cache_array[i], list) {
 +			if (!all && (c->mfc_flags & MFC_STATIC))
 +				continue;
 +			list_del_rcu(&c->list);
 +			mroute_netlink_event(mrt, c, RTM_DELROUTE);
 +			ipmr_cache_free(c);
 +		}
++=======
+ 	list_for_each_entry_safe(c, tmp, &mrt->mfc_cache_list, list) {
+ 		if (!all && (c->mfc_flags & MFC_STATIC))
+ 			continue;
+ 		rhltable_remove(&mrt->mfc_hash, &c->mnode, ipmr_rht_params);
+ 		list_del_rcu(&c->list);
+ 		mroute_netlink_event(mrt, c, RTM_DELROUTE);
+ 		ipmr_cache_free(c);
++>>>>>>> 8fb472c09b9d (ipmr: improve hash scalability)
  	}
  
  	if (atomic_read(&mrt->cache_resolve_queue_len) != 0) {
* Unmerged path include/linux/mroute.h
* Unmerged path net/ipv4/ipmr.c
