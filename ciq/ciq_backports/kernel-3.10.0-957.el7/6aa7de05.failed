locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Mark Rutland <mark.rutland@arm.com>
commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6aa7de05.failed

Please do not apply this to mainline directly, instead please re-run the
coccinelle script shown below and apply its output.

For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
preference to ACCESS_ONCE(), and new code is expected to use one of the
former. So far, there's been no reason to change most existing uses of
ACCESS_ONCE(), as these aren't harmful, and changing them results in
churn.

However, for some features, the read/write distinction is critical to
correct operation. To distinguish these cases, separate read/write
accessors must be used. This patch migrates (most) remaining
ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
coccinelle script:

----
// Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
// WRITE_ONCE()

// $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch

virtual patch

@ depends on patch @
expression E1, E2;
@@

- ACCESS_ONCE(E1) = E2
+ WRITE_ONCE(E1, E2)

@ depends on patch @
expression E;
@@

- ACCESS_ONCE(E)
+ READ_ONCE(E)
----

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: davem@davemloft.net
	Cc: linux-arch@vger.kernel.org
	Cc: mpe@ellerman.id.au
	Cc: shuah@kernel.org
	Cc: snitzer@redhat.com
	Cc: thor.thayer@linux.intel.com
	Cc: tj@kernel.org
	Cc: viro@zeniv.linux.org.uk
	Cc: will.deacon@arm.com
Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 6aa7de059173a986114ac43b8f50b297a86f09a8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/kernel/smp.c
#	arch/arm/vdso/vgettimeofday.c
#	arch/mips/include/asm/vdso.h
#	arch/mips/kernel/pm-cps.c
#	arch/powerpc/platforms/powernv/opal-msglog.c
#	arch/s390/include/asm/spinlock.h
#	arch/s390/lib/spinlock.c
#	arch/tile/kernel/ptrace.c
#	arch/x86/entry/common.c
#	arch/x86/events/core.c
#	arch/x86/include/asm/vgtod.h
#	arch/x86/kernel/espfix_64.c
#	arch/x86/kernel/nmi.c
#	arch/x86/vdso/vclock_gettime.c
#	arch/x86/xen/p2m.c
#	arch/xtensa/platforms/xtfpga/lcd.c
#	block/blk-wbt.c
#	drivers/input/misc/regulator-haptic.c
#	drivers/md/dm-bufio.c
#	drivers/md/dm-stats.c
#	drivers/misc/mic/scif/scif_rb.c
#	drivers/misc/mic/scif/scif_rma_list.c
#	drivers/net/ethernet/hisilicon/hip04_eth.c
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
#	drivers/net/ethernet/sfc/ptp.c
#	drivers/net/tap.c
#	drivers/usb/gadget/udc/gr_udc.c
#	drivers/vhost/scsi.c
#	fs/crypto/keyinfo.c
#	fs/direct-io.c
#	fs/exec.c
#	fs/fuse/dev.c
#	fs/nfs/dir.c
#	fs/proc_namespace.c
#	kernel/exit.c
#	kernel/trace/ring_buffer.c
#	kernel/trace/trace.h
#	kernel/trace/trace_stack.c
#	mm/huge_memory.c
#	net/core/dev.c
#	net/ipv4/inet_fragment.c
#	net/ipv4/route.c
#	net/netlabel/netlabel_calipso.c
#	sound/firewire/amdtp-am824.c
#	sound/firewire/amdtp-stream.c
#	sound/firewire/amdtp-stream.h
#	sound/firewire/digi00x/amdtp-dot.c
#	sound/firewire/fireface/amdtp-ff.c
#	sound/firewire/fireface/ff-midi.c
#	sound/firewire/fireface/ff-transaction.c
#	sound/firewire/motu/amdtp-motu.c
#	sound/firewire/oxfw/oxfw-scs1x.c
#	sound/firewire/tascam/amdtp-tascam.c
#	sound/firewire/tascam/tascam-transaction.c
#	sound/soc/xtensa/xtfpga-i2s.c
diff --cc arch/arc/kernel/smp.c
index 5c7fd603d216,94cabe73664b..000000000000
--- a/arch/arc/kernel/smp.c
+++ b/arch/arc/kernel/smp.c
@@@ -227,14 -240,25 +227,25 @@@ static void ipi_send_msg(const struct c
  
  	local_irq_save(flags);
  
++<<<<<<< HEAD
 +	for_each_cpu(cpu, callmap) {
 +		struct ipi_data *ipi = &per_cpu(ipi_data, cpu);
 +		set_bit(msg, &ipi->bits);
 +	}
++=======
+ 	/*
+ 	 * Atomically write new msg bit (in case others are writing too),
+ 	 * and read back old value
+ 	 */
+ 	do {
+ 		new = old = READ_ONCE(*ipi_data_ptr);
+ 		new |= 1U << msg;
+ 	} while (cmpxchg(ipi_data_ptr, old, new) != old);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  
 -	/*
 -	 * Call the platform specific IPI kick function, but avoid if possible:
 -	 * Only do so if there's no pending msg from other concurrent sender(s).
 -	 * Otherwise, recevier will see this msg as well when it takes the
 -	 * IPI corresponding to that msg. This is true, even if it is already in
 -	 * IPI handler, because !@old means it has not yet dequeued the msg(s)
 -	 * so @new msg can be a free-loader
 -	 */
 -	if (plat_smp_ops.ipi_send && !old)
 -		plat_smp_ops.ipi_send(cpu);
 +	/* Call the platform specific cross-CPU call function  */
 +	if (plat_smp_ops.ipi_send)
 +		plat_smp_ops.ipi_send((void *)callmap);
  
  	local_irq_restore(flags);
  }
diff --cc arch/mips/include/asm/vdso.h
index cca56aa40ff4,91bf0c2c265c..000000000000
--- a/arch/mips/include/asm/vdso.h
+++ b/arch/mips/include/asm/vdso.h
@@@ -9,21 -11,126 +9,63 @@@
  #ifndef __ASM_VDSO_H
  #define __ASM_VDSO_H
  
 -#include <linux/mm_types.h>
 +#include <linux/types.h>
  
 -#include <asm/barrier.h>
  
 -/**
 - * struct mips_vdso_image - Details of a VDSO image.
 - * @data: Pointer to VDSO image data (page-aligned).
 - * @size: Size of the VDSO image data (page-aligned).
 - * @off_sigreturn: Offset of the sigreturn() trampoline.
 - * @off_rt_sigreturn: Offset of the rt_sigreturn() trampoline.
 - * @mapping: Special mapping structure.
 - *
 - * This structure contains details of a VDSO image, including the image data
 - * and offsets of certain symbols required by the kernel. It is generated as
 - * part of the VDSO build process, aside from the mapping page array, which is
 - * populated at runtime.
 - */
 -struct mips_vdso_image {
 -	void *data;
 -	unsigned long size;
 -
 -	unsigned long off_sigreturn;
 -	unsigned long off_rt_sigreturn;
 -
 -	struct vm_special_mapping mapping;
 +#ifdef CONFIG_32BIT
 +struct mips_vdso {
 +	u32 signal_trampoline[2];
 +	u32 rt_signal_trampoline[2];
  };
 -
 -/*
 - * The following structures are auto-generated as part of the build for each
 - * ABI by genvdso, see arch/mips/vdso/Makefile.
 - */
 -
 -extern struct mips_vdso_image vdso_image;
 -
 -#ifdef CONFIG_MIPS32_O32
 -extern struct mips_vdso_image vdso_image_o32;
 -#endif
 -
 -#ifdef CONFIG_MIPS32_N32
 -extern struct mips_vdso_image vdso_image_n32;
 -#endif
 -
 -/**
 - * union mips_vdso_data - Data provided by the kernel for the VDSO.
 - * @xtime_sec:		Current real time (seconds part).
 - * @xtime_nsec:		Current real time (nanoseconds part, shifted).
 - * @wall_to_mono_sec:	Wall-to-monotonic offset (seconds part).
 - * @wall_to_mono_nsec:	Wall-to-monotonic offset (nanoseconds part).
 - * @seq_count:		Counter to synchronise updates (odd = updating).
 - * @cs_shift:		Clocksource shift value.
 - * @clock_mode:		Clocksource to use for time functions.
 - * @cs_mult:		Clocksource multiplier value.
 - * @cs_cycle_last:	Clock cycle value at last update.
 - * @cs_mask:		Clocksource mask value.
 - * @tz_minuteswest:	Minutes west of Greenwich (from timezone).
 - * @tz_dsttime:		Type of DST correction (from timezone).
 - *
 - * This structure contains data needed by functions within the VDSO. It is
 - * populated by the kernel and mapped read-only into user memory. The time
 - * fields are mirrors of internal data from the timekeeping infrastructure.
 - *
 - * Note: Care should be taken when modifying as the layout must remain the same
 - * for both 64- and 32-bit (for 32-bit userland on 64-bit kernel).
 - */
 -union mips_vdso_data {
 -	struct {
 -		u64 xtime_sec;
 -		u64 xtime_nsec;
 -		u64 wall_to_mono_sec;
 -		u64 wall_to_mono_nsec;
 -		u32 seq_count;
 -		u32 cs_shift;
 -		u8 clock_mode;
 -		u32 cs_mult;
 -		u64 cs_cycle_last;
 -		u64 cs_mask;
 -		s32 tz_minuteswest;
 -		s32 tz_dsttime;
 -	};
 -
 -	u8 page[PAGE_SIZE];
 +#else  /* !CONFIG_32BIT */
 +struct mips_vdso {
 +	u32 o32_signal_trampoline[2];
 +	u32 o32_rt_signal_trampoline[2];
 +	u32 rt_signal_trampoline[2];
 +	u32 n32_rt_signal_trampoline[2];
  };
++<<<<<<< HEAD
 +#endif /* CONFIG_32BIT */
++=======
+ 
+ static inline u32 vdso_data_read_begin(const union mips_vdso_data *data)
+ {
+ 	u32 seq;
+ 
+ 	while (true) {
+ 		seq = READ_ONCE(data->seq_count);
+ 		if (likely(!(seq & 1))) {
+ 			/* Paired with smp_wmb() in vdso_data_write_*(). */
+ 			smp_rmb();
+ 			return seq;
+ 		}
+ 
+ 		cpu_relax();
+ 	}
+ }
+ 
+ static inline bool vdso_data_read_retry(const union mips_vdso_data *data,
+ 					u32 start_seq)
+ {
+ 	/* Paired with smp_wmb() in vdso_data_write_*(). */
+ 	smp_rmb();
+ 	return unlikely(data->seq_count != start_seq);
+ }
+ 
+ static inline void vdso_data_write_begin(union mips_vdso_data *data)
+ {
+ 	++data->seq_count;
+ 
+ 	/* Ensure sequence update is written before other data page values. */
+ 	smp_wmb();
+ }
+ 
+ static inline void vdso_data_write_end(union mips_vdso_data *data)
+ {
+ 	/* Ensure data values are written before updating sequence again. */
+ 	smp_wmb();
+ 	++data->seq_count;
+ }
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  
  #endif /* __ASM_VDSO_H */
diff --cc arch/powerpc/platforms/powernv/opal-msglog.c
index 44ed78af1a0d,acd3206dfae3..000000000000
--- a/arch/powerpc/platforms/powernv/opal-msglog.c
+++ b/arch/powerpc/platforms/powernv/opal-msglog.c
@@@ -41,10 -40,10 +41,14 @@@ static ssize_t opal_msglog_read(struct 
  	size_t first_read = 0;
  	uint32_t out_pos, avail;
  
 -	if (!opal_memcons)
 +	if (!mc)
  		return -ENODEV;
  
++<<<<<<< HEAD
 +	out_pos = be32_to_cpu(ACCESS_ONCE(mc->out_pos));
++=======
+ 	out_pos = be32_to_cpu(READ_ONCE(opal_memcons->out_pos));
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  
  	/* Now we've read out_pos, put a barrier in before reading the new
  	 * data it points to in conbuf. */
diff --cc arch/s390/include/asm/spinlock.h
index 00e38c51a9f3,66f4160010ef..000000000000
--- a/arch/s390/include/asm/spinlock.h
+++ b/arch/s390/include/asm/spinlock.h
@@@ -126,21 -117,60 +126,31 @@@ extern int _raw_write_trylock_retry(arc
  
  static inline int arch_read_trylock_once(arch_rwlock_t *rw)
  {
++<<<<<<< HEAD
 +	unsigned int old = ACCESS_ONCE(rw->lock);
 +	return likely((int) old >= 0 &&
 +		      _raw_compare_and_swap(&rw->lock, old, old + 1));
++=======
+ 	int old = READ_ONCE(rw->lock);
+ 	return likely(old >= 0 &&
+ 		      __atomic_cmpxchg_bool(&rw->lock, old, old + 1));
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  }
  
  static inline int arch_write_trylock_once(arch_rwlock_t *rw)
  {
++<<<<<<< HEAD
 +	unsigned int old = ACCESS_ONCE(rw->lock);
++=======
+ 	int old = READ_ONCE(rw->lock);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	return likely(old == 0 &&
 -		      __atomic_cmpxchg_bool(&rw->lock, 0, 0x80000000));
 +		      _raw_compare_and_swap(&rw->lock, 0, 0x80000000));
  }
  
 -#ifdef CONFIG_HAVE_MARCH_Z196_FEATURES
 -
 -#define __RAW_OP_OR	"lao"
 -#define __RAW_OP_AND	"lan"
 -#define __RAW_OP_ADD	"laa"
 -
 -#define __RAW_LOCK(ptr, op_val, op_string)		\
 -({							\
 -	int old_val;					\
 -							\
 -	typecheck(int *, ptr);				\
 -	asm volatile(					\
 -		op_string "	%0,%2,%1\n"		\
 -		"bcr	14,0\n"				\
 -		: "=d" (old_val), "+Q" (*ptr)		\
 -		: "d" (op_val)				\
 -		: "cc", "memory");			\
 -	old_val;					\
 -})
 -
 -#define __RAW_UNLOCK(ptr, op_val, op_string)		\
 -({							\
 -	int old_val;					\
 -							\
 -	typecheck(int *, ptr);				\
 -	asm volatile(					\
 -		op_string "	%0,%2,%1\n"		\
 -		: "=d" (old_val), "+Q" (*ptr)		\
 -		: "d" (op_val)				\
 -		: "cc", "memory");			\
 -	old_val;					\
 -})
 -
 -extern void _raw_read_lock_wait(arch_rwlock_t *lp);
 -extern void _raw_write_lock_wait(arch_rwlock_t *lp, int prev);
 -
  static inline void arch_read_lock(arch_rwlock_t *rw)
  {
 -	int old;
 -
 -	old = __RAW_LOCK(&rw->lock, 1, __RAW_OP_ADD);
 -	if (old < 0)
 +	if (!arch_read_trylock_once(rw))
  		_raw_read_lock_wait(rw);
  }
  
@@@ -152,11 -208,11 +162,16 @@@ static inline void arch_read_lock_flags
  
  static inline void arch_read_unlock(arch_rwlock_t *rw)
  {
 -	int old;
 +	unsigned int old;
  
  	do {
++<<<<<<< HEAD
 +		old = ACCESS_ONCE(rw->lock);
 +	} while (!_raw_compare_and_swap(&rw->lock, old, old - 1));
++=======
+ 		old = READ_ONCE(rw->lock);
+ 	} while (!__atomic_cmpxchg_bool(&rw->lock, old, old - 1));
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  }
  
  static inline void arch_write_lock(arch_rwlock_t *rw)
diff --cc arch/s390/lib/spinlock.c
index 9beb186b3af5,34e30b9ea234..000000000000
--- a/arch/s390/lib/spinlock.c
+++ b/arch/s390/lib/spinlock.c
@@@ -123,76 -149,96 +123,131 @@@ EXPORT_SYMBOL(arch_spin_trylock_retry)
  
  void _raw_read_lock_wait(arch_rwlock_t *rw)
  {
 +	unsigned int old;
  	int count = spin_retry;
 -	int owner, old;
  
 -#ifdef CONFIG_HAVE_MARCH_Z196_FEATURES
 -	__RAW_LOCK(&rw->lock, -1, __RAW_OP_ADD);
 -#endif
 -	owner = 0;
  	while (1) {
  		if (count-- <= 0) {
 -			if (owner && arch_vcpu_is_preempted(~owner))
 -				smp_yield_cpu(~owner);
 +			smp_yield();
  			count = spin_retry;
  		}
++<<<<<<< HEAD
 +		old = ACCESS_ONCE(rw->lock);
 +		if ((int) old < 0)
++=======
+ 		old = READ_ONCE(rw->lock);
+ 		owner = READ_ONCE(rw->owner);
+ 		if (old < 0)
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  			continue;
 -		if (__atomic_cmpxchg_bool(&rw->lock, old, old + 1))
 +		if (_raw_compare_and_swap(&rw->lock, old, old + 1))
  			return;
  	}
  }
  EXPORT_SYMBOL(_raw_read_lock_wait);
  
 +void _raw_read_lock_wait_flags(arch_rwlock_t *rw, unsigned long flags)
 +{
 +	unsigned int old;
 +	int count = spin_retry;
 +
 +	local_irq_restore(flags);
 +	while (1) {
 +		if (count-- <= 0) {
 +			smp_yield();
 +			count = spin_retry;
 +		}
 +		old = ACCESS_ONCE(rw->lock);
 +		if ((int) old < 0)
 +			continue;
 +		local_irq_disable();
 +		if (_raw_compare_and_swap(&rw->lock, old, old + 1))
 +			return;
 +		local_irq_restore(flags);
 +	}
 +}
 +EXPORT_SYMBOL(_raw_read_lock_wait_flags);
 +
  int _raw_read_trylock_retry(arch_rwlock_t *rw)
  {
 +	unsigned int old;
  	int count = spin_retry;
 -	int old;
  
  	while (count-- > 0) {
++<<<<<<< HEAD
 +		old = ACCESS_ONCE(rw->lock);
 +		if ((int) old < 0)
++=======
+ 		old = READ_ONCE(rw->lock);
+ 		if (old < 0)
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  			continue;
 -		if (__atomic_cmpxchg_bool(&rw->lock, old, old + 1))
 +		if (_raw_compare_and_swap(&rw->lock, old, old + 1))
  			return 1;
  	}
  	return 0;
  }
  EXPORT_SYMBOL(_raw_read_trylock_retry);
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_HAVE_MARCH_Z196_FEATURES
+ 
+ void _raw_write_lock_wait(arch_rwlock_t *rw, int prev)
+ {
+ 	int count = spin_retry;
+ 	int owner, old;
+ 
+ 	owner = 0;
+ 	while (1) {
+ 		if (count-- <= 0) {
+ 			if (owner && arch_vcpu_is_preempted(~owner))
+ 				smp_yield_cpu(~owner);
+ 			count = spin_retry;
+ 		}
+ 		old = READ_ONCE(rw->lock);
+ 		owner = READ_ONCE(rw->owner);
+ 		smp_mb();
+ 		if (old >= 0) {
+ 			prev = __RAW_LOCK(&rw->lock, 0x80000000, __RAW_OP_OR);
+ 			old = prev;
+ 		}
+ 		if ((old & 0x7fffffff) == 0 && prev >= 0)
+ 			break;
+ 	}
+ }
+ EXPORT_SYMBOL(_raw_write_lock_wait);
+ 
+ #else /* CONFIG_HAVE_MARCH_Z196_FEATURES */
+ 
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  void _raw_write_lock_wait(arch_rwlock_t *rw)
  {
 +	unsigned int old;
  	int count = spin_retry;
 -	int owner, old, prev;
  
 -	prev = 0x80000000;
 -	owner = 0;
  	while (1) {
  		if (count-- <= 0) {
 -			if (owner && arch_vcpu_is_preempted(~owner))
 -				smp_yield_cpu(~owner);
 +			smp_yield();
  			count = spin_retry;
  		}
++<<<<<<< HEAD
 +		old = ACCESS_ONCE(rw->lock);
 +		if (old)
 +			continue;
 +		if (_raw_compare_and_swap(&rw->lock, 0, 0x80000000))
 +			return;
++=======
+ 		old = READ_ONCE(rw->lock);
+ 		owner = READ_ONCE(rw->owner);
+ 		if (old >= 0 &&
+ 		    __atomic_cmpxchg_bool(&rw->lock, old, old | 0x80000000))
+ 			prev = old;
+ 		else
+ 			smp_mb();
+ 		if ((old & 0x7fffffff) == 0 && prev >= 0)
+ 			break;
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	}
  }
  EXPORT_SYMBOL(_raw_write_lock_wait);
@@@ -221,14 -247,14 +276,14 @@@ EXPORT_SYMBOL(_raw_write_lock_wait_flag
  
  int _raw_write_trylock_retry(arch_rwlock_t *rw)
  {
 +	unsigned int old;
  	int count = spin_retry;
 -	int old;
  
  	while (count-- > 0) {
- 		old = ACCESS_ONCE(rw->lock);
+ 		old = READ_ONCE(rw->lock);
  		if (old)
  			continue;
 -		if (__atomic_cmpxchg_bool(&rw->lock, 0, 0x80000000))
 +		if (_raw_compare_and_swap(&rw->lock, 0, 0x80000000))
  			return 1;
  	}
  	return 0;
diff --cc arch/tile/kernel/ptrace.c
index 0f83ed4602b2,d516d61751c2..000000000000
--- a/arch/tile/kernel/ptrace.c
+++ b/arch/tile/kernel/ptrace.c
@@@ -252,12 -255,18 +252,21 @@@ long compat_arch_ptrace(struct task_str
  
  int do_syscall_trace_enter(struct pt_regs *regs)
  {
++<<<<<<< HEAD
 +	if (test_thread_flag(TIF_SYSCALL_TRACE)) {
 +		if (tracehook_report_syscall_entry(regs))
 +			regs->regs[TREG_SYSCALL_NR] = -1;
++=======
+ 	u32 work = READ_ONCE(current_thread_info()->flags);
+ 
+ 	if ((work & _TIF_SYSCALL_TRACE) &&
+ 	    tracehook_report_syscall_entry(regs)) {
+ 		regs->regs[TREG_SYSCALL_NR] = -1;
+ 		return -1;
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	}
  
 -	if (secure_computing(NULL) == -1)
 -		return -1;
 -
 -	if (work & _TIF_SYSCALL_TRACEPOINT)
 +	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
  		trace_sys_enter(regs, regs->regs[TREG_SYSCALL_NR]);
  
  	return regs->regs[TREG_SYSCALL_NR];
diff --cc arch/x86/events/core.c
index d8ebf9d6cf72,140d33288e78..000000000000
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@@ -2036,6 -2118,9 +2036,12 @@@ static int x86_pmu_event_init(struct pe
  			event->destroy(event);
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (READ_ONCE(x86_pmu.attr_rdpmc))
+ 		event->hw.flags |= PERF_X86_EVENT_RDPMC_ALLOWED;
+ 
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	return err;
  }
  
diff --cc arch/x86/include/asm/vgtod.h
index 0217674bb5e3,53dd162576a8..000000000000
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@@ -27,4 -37,69 +27,72 @@@ struct vsyscall_gtod_data 
  };
  extern struct vsyscall_gtod_data vsyscall_gtod_data;
  
++<<<<<<< HEAD
++=======
+ extern int vclocks_used;
+ static inline bool vclock_was_used(int vclock)
+ {
+ 	return READ_ONCE(vclocks_used) & (1 << vclock);
+ }
+ 
+ static inline unsigned gtod_read_begin(const struct vsyscall_gtod_data *s)
+ {
+ 	unsigned ret;
+ 
+ repeat:
+ 	ret = READ_ONCE(s->seq);
+ 	if (unlikely(ret & 1)) {
+ 		cpu_relax();
+ 		goto repeat;
+ 	}
+ 	smp_rmb();
+ 	return ret;
+ }
+ 
+ static inline int gtod_read_retry(const struct vsyscall_gtod_data *s,
+ 					unsigned start)
+ {
+ 	smp_rmb();
+ 	return unlikely(s->seq != start);
+ }
+ 
+ static inline void gtod_write_begin(struct vsyscall_gtod_data *s)
+ {
+ 	++s->seq;
+ 	smp_wmb();
+ }
+ 
+ static inline void gtod_write_end(struct vsyscall_gtod_data *s)
+ {
+ 	smp_wmb();
+ 	++s->seq;
+ }
+ 
+ #ifdef CONFIG_X86_64
+ 
+ #define VGETCPU_CPU_MASK 0xfff
+ 
+ static inline unsigned int __getcpu(void)
+ {
+ 	unsigned int p;
+ 
+ 	/*
+ 	 * Load per CPU data from GDT.  LSL is faster than RDTSCP and
+ 	 * works on all CPUs.  This is volatile so that it orders
+ 	 * correctly wrt barrier() and to keep gcc from cleverly
+ 	 * hoisting it out of the calling function.
+ 	 *
+ 	 * If RDPID is available, use it.
+ 	 */
+ 	alternative_io ("lsl %[p],%[seg]",
+ 			".byte 0xf3,0x0f,0xc7,0xf8", /* RDPID %eax/rax */
+ 			X86_FEATURE_RDPID,
+ 			[p] "=a" (p), [seg] "r" (__PER_CPU_SEG));
+ 
+ 	return p;
+ }
+ 
+ #endif /* CONFIG_X86_64 */
+ 
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  #endif /* _ASM_X86_VGTOD_H */
diff --cc arch/x86/kernel/nmi.c
index dffffa5903a7,18bc9b51ac9b..000000000000
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@@ -93,7 -101,21 +93,25 @@@ static int __init nmi_warning_debugfs(v
  }
  fs_initcall(nmi_warning_debugfs);
  
++<<<<<<< HEAD
 +static int __kprobes nmi_handle(unsigned int type, struct pt_regs *regs, bool b2b)
++=======
+ static void nmi_max_handler(struct irq_work *w)
+ {
+ 	struct nmiaction *a = container_of(w, struct nmiaction, irq_work);
+ 	int remainder_ns, decimal_msecs;
+ 	u64 whole_msecs = READ_ONCE(a->max_duration);
+ 
+ 	remainder_ns = do_div(whole_msecs, (1000 * 1000));
+ 	decimal_msecs = remainder_ns / 1000;
+ 
+ 	printk_ratelimited(KERN_INFO
+ 		"INFO: NMI handler (%ps) took too long to run: %lld.%03d msecs\n",
+ 		a->handler, whole_msecs, decimal_msecs);
+ }
+ 
+ static int nmi_handle(unsigned int type, struct pt_regs *regs)
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  {
  	struct nmi_desc *desc = nmi_to_desc(type);
  	struct nmiaction *a;
diff --cc arch/x86/vdso/vclock_gettime.c
index 7bd0b1186cc9,11b13c4b43d5..000000000000
--- a/arch/x86/vdso/vclock_gettime.c
+++ b/arch/x86/vdso/vclock_gettime.c
@@@ -293,8 -317,8 +293,13 @@@ int gettimeofday(struct timeval *, stru
   */
  notrace time_t __vdso_time(time_t *t)
  {
++<<<<<<< HEAD:arch/x86/vdso/vclock_gettime.c
 +	/* This is atomic on x86_64 so we don't need any locks. */
 +	time_t result = ACCESS_ONCE(VVAR(vsyscall_gtod_data).wall_time_sec);
++=======
+ 	/* This is atomic on x86 so we don't need any locks. */
+ 	time_t result = READ_ONCE(gtod->wall_time_sec);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()):arch/x86/entry/vdso/vclock_gettime.c
  
  	if (t)
  		*t = result;
diff --cc arch/x86/xen/p2m.c
index 95fb2aa5927e,13b4f19b9131..000000000000
--- a/arch/x86/xen/p2m.c
+++ b/arch/x86/xen/p2m.c
@@@ -515,59 -523,65 +515,66 @@@ static void free_p2m_page(void *p
   * the new pages are installed with cmpxchg; if we lose the race then
   * simply free the page we allocated and use the one that's there.
   */
 -int xen_alloc_p2m_entry(unsigned long pfn)
 +static bool alloc_p2m(unsigned long pfn)
  {
 -	unsigned topidx;
 +	unsigned topidx, mididx;
 +	unsigned long ***top_p, **mid;
  	unsigned long *top_mfn_p, *mid_mfn;
 -	pte_t *ptep, *pte_pg;
 -	unsigned int level;
 -	unsigned long flags;
 -	unsigned long addr = (unsigned long)(xen_p2m_addr + pfn);
 -	unsigned long p2m_pfn;
 -
 -	ptep = lookup_address(addr, &level);
 -	BUG_ON(!ptep || level != PG_LEVEL_4K);
 -	pte_pg = (pte_t *)((unsigned long)ptep & ~(PAGE_SIZE - 1));
 -
 -	if (pte_pg == p2m_missing_pte || pte_pg == p2m_identity_pte) {
 -		/* PMD level is missing, allocate a new one */
 -		ptep = alloc_p2m_pmd(addr, pte_pg);
 -		if (!ptep)
 -			return -ENOMEM;
 +
 +	topidx = p2m_top_index(pfn);
 +	mididx = p2m_mid_index(pfn);
 +
 +	top_p = &p2m_top[topidx];
 +	mid = *top_p;
 +
 +	if (mid == p2m_mid_missing) {
 +		/* Mid level is missing, allocate a new one */
 +		mid = alloc_p2m_page();
 +		if (!mid)
 +			return false;
 +
 +		p2m_mid_init(mid);
 +
 +		if (cmpxchg(top_p, p2m_mid_missing, mid) != p2m_mid_missing)
 +			free_p2m_page(mid);
  	}
  
++<<<<<<< HEAD
 +	top_mfn_p = &p2m_top_mfn[topidx];
 +	mid_mfn = p2m_top_mfn_p[topidx];
++=======
+ 	if (p2m_top_mfn && pfn < MAX_P2M_PFN) {
+ 		topidx = p2m_top_index(pfn);
+ 		top_mfn_p = &p2m_top_mfn[topidx];
+ 		mid_mfn = READ_ONCE(p2m_top_mfn_p[topidx]);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  
 -		BUG_ON(virt_to_mfn(mid_mfn) != *top_mfn_p);
 +	BUG_ON(virt_to_mfn(mid_mfn) != *top_mfn_p);
  
 -		if (mid_mfn == p2m_mid_missing_mfn) {
 -			/* Separately check the mid mfn level */
 -			unsigned long missing_mfn;
 -			unsigned long mid_mfn_mfn;
 -			unsigned long old_mfn;
 +	if (mid_mfn == p2m_mid_missing_mfn) {
 +		/* Separately check the mid mfn level */
 +		unsigned long missing_mfn;
 +		unsigned long mid_mfn_mfn;
  
 -			mid_mfn = alloc_p2m_page();
 -			if (!mid_mfn)
 -				return -ENOMEM;
 +		mid_mfn = alloc_p2m_page();
 +		if (!mid_mfn)
 +			return false;
  
 -			p2m_mid_mfn_init(mid_mfn, p2m_missing);
 +		p2m_mid_mfn_init(mid_mfn);
  
 -			missing_mfn = virt_to_mfn(p2m_mid_missing_mfn);
 -			mid_mfn_mfn = virt_to_mfn(mid_mfn);
 -			old_mfn = cmpxchg(top_mfn_p, missing_mfn, mid_mfn_mfn);
 -			if (old_mfn != missing_mfn) {
 -				free_p2m_page(mid_mfn);
 -				mid_mfn = mfn_to_virt(old_mfn);
 -			} else {
 -				p2m_top_mfn_p[topidx] = mid_mfn;
 -			}
 -		}
 -	} else {
 -		mid_mfn = NULL;
 +		missing_mfn = virt_to_mfn(p2m_mid_missing_mfn);
 +		mid_mfn_mfn = virt_to_mfn(mid_mfn);
 +		if (cmpxchg(top_mfn_p, missing_mfn, mid_mfn_mfn) != missing_mfn)
 +			free_p2m_page(mid_mfn);
 +		else
 +			p2m_top_mfn_p[topidx] = mid_mfn;
  	}
  
 -	p2m_pfn = pte_pfn(READ_ONCE(*ptep));
 -	if (p2m_pfn == PFN_DOWN(__pa(p2m_identity)) ||
 -	    p2m_pfn == PFN_DOWN(__pa(p2m_missing))) {
 +	if (p2m_top[topidx][mididx] == p2m_identity ||
 +	    p2m_top[topidx][mididx] == p2m_missing) {
  		/* p2m leaf page is missing */
  		unsigned long *p2m;
 +		unsigned long *p2m_orig = p2m_top[topidx][mididx];
  
  		p2m = alloc_p2m_page();
  		if (!p2m)
diff --cc arch/xtensa/platforms/xtfpga/lcd.c
index 2872301598df,2f7eb66c23ec..000000000000
--- a/arch/xtensa/platforms/xtfpga/lcd.c
+++ b/arch/xtensa/platforms/xtfpga/lcd.c
@@@ -34,17 -31,33 +34,40 @@@
  #define LCD_SHIFT_LEFT		0x18
  #define LCD_SHIFT_RIGHT		0x1c
  
++<<<<<<< HEAD
 +static int __init lcd_init(void)
 +{
 +	*LCD_INSTR_ADDR = LCD_DISPLAY_MODE8BIT;
 +	mdelay(5);
 +	*LCD_INSTR_ADDR = LCD_DISPLAY_MODE8BIT;
 +	udelay(200);
 +	*LCD_INSTR_ADDR = LCD_DISPLAY_MODE8BIT;
 +	udelay(50);
 +	*LCD_INSTR_ADDR = LCD_DISPLAY_ON;
++=======
+ static void lcd_put_byte(u8 *addr, u8 data)
+ {
+ #ifdef CONFIG_XTFPGA_LCD_8BIT_ACCESS
+ 	WRITE_ONCE(*addr, data);
+ #else
+ 	WRITE_ONCE(*addr, data & 0xf0);
+ 	WRITE_ONCE(*addr, (data << 4) & 0xf0);
+ #endif
+ }
+ 
+ static int __init lcd_init(void)
+ {
+ 	WRITE_ONCE(*LCD_INSTR_ADDR, LCD_DISPLAY_MODE8BIT);
+ 	mdelay(5);
+ 	WRITE_ONCE(*LCD_INSTR_ADDR, LCD_DISPLAY_MODE8BIT);
+ 	udelay(200);
+ 	WRITE_ONCE(*LCD_INSTR_ADDR, LCD_DISPLAY_MODE8BIT);
+ 	udelay(50);
+ #ifndef CONFIG_XTFPGA_LCD_8BIT_ACCESS
+ 	WRITE_ONCE(*LCD_INSTR_ADDR, LCD_DISPLAY_MODE4BIT);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	udelay(50);
 -	lcd_put_byte(LCD_INSTR_ADDR, LCD_DISPLAY_MODE4BIT);
 -	udelay(50);
 -#endif
 -	lcd_put_byte(LCD_INSTR_ADDR, LCD_DISPLAY_ON);
 -	udelay(50);
 -	lcd_put_byte(LCD_INSTR_ADDR, LCD_CLEAR);
 +	*LCD_INSTR_ADDR = LCD_CLEAR;
  	mdelay(10);
  	lcd_disp_at_pos("XTENSA LINUX", 0);
  	return 0;
diff --cc drivers/md/dm-bufio.c
index d33811aebb23,33bb074d6941..000000000000
--- a/drivers/md/dm-bufio.c
+++ b/drivers/md/dm-bufio.c
@@@ -1581,18 -1634,20 +1581,27 @@@ static int shrink(struct shrinker *shri
  	if (sc->gfp_mask & __GFP_FS)
  		dm_bufio_lock(c);
  	else if (!dm_bufio_trylock(c))
 -		return SHRINK_STOP;
 +		return !nr_to_scan ? 0 : -1;
 +
 +	if (nr_to_scan)
 +		__scan(c, nr_to_scan, sc);
 +
 +	r = c->n_buffers[LIST_CLEAN] + c->n_buffers[LIST_DIRTY];
 +	if (r > INT_MAX)
 +		r = INT_MAX;
  
 -	freed  = __scan(c, sc->nr_to_scan, sc->gfp_mask);
  	dm_bufio_unlock(c);
 -	return freed;
 -}
  
++<<<<<<< HEAD
 +	return r;
++=======
+ static unsigned long
+ dm_bufio_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
+ {
+ 	struct dm_bufio_client *c = container_of(shrink, struct dm_bufio_client, shrinker);
+ 
+ 	return READ_ONCE(c->n_buffers[LIST_CLEAN]) + READ_ONCE(c->n_buffers[LIST_DIRTY]);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  }
  
  /*
@@@ -1754,9 -1810,15 +1763,9 @@@ void dm_bufio_client_destroy(struct dm_
  }
  EXPORT_SYMBOL_GPL(dm_bufio_client_destroy);
  
 -void dm_bufio_set_sector_offset(struct dm_bufio_client *c, sector_t start)
 -{
 -	c->start = start;
 -}
 -EXPORT_SYMBOL_GPL(dm_bufio_set_sector_offset);
 -
  static unsigned get_max_age_hz(void)
  {
- 	unsigned max_age = ACCESS_ONCE(dm_bufio_max_age);
+ 	unsigned max_age = READ_ONCE(dm_bufio_max_age);
  
  	if (max_age > UINT_MAX / HZ)
  		max_age = UINT_MAX / HZ;
diff --cc drivers/md/dm-stats.c
index 8e767cc587b9,a1a5eec783cc..000000000000
--- a/drivers/md/dm-stats.c
+++ b/drivers/md/dm-stats.c
@@@ -638,14 -637,14 +638,20 @@@ void dm_stats_account_io(struct dm_stat
  		 * A race condition can at worst result in the merged flag being
  		 * misrepresented, so we don't have to disable preemption here.
  		 */
 -		last = raw_cpu_ptr(stats->last);
 +		last = __this_cpu_ptr(stats->last);
  		stats_aux->merged =
++<<<<<<< HEAD
 +			(bi_sector == (ACCESS_ONCE(last->last_sector) &&
 +				       ((bi_rw & (REQ_WRITE | REQ_DISCARD)) ==
 +					(ACCESS_ONCE(last->last_rw) & (REQ_WRITE | REQ_DISCARD)))
++=======
+ 			(bi_sector == (READ_ONCE(last->last_sector) &&
+ 				       ((bi_rw == WRITE) ==
+ 					(READ_ONCE(last->last_rw) == WRITE))
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  				       ));
- 		ACCESS_ONCE(last->last_sector) = end_sector;
- 		ACCESS_ONCE(last->last_rw) = bi_rw;
+ 		WRITE_ONCE(last->last_sector, end_sector);
+ 		WRITE_ONCE(last->last_rw, bi_rw);
  	}
  
  	rcu_read_lock();
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index bc2d9678add4,2224e691ee07..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -8527,19 -8646,14 +8527,30 @@@ static void ixgbe_get_stats64(struct ne
  	}
  
  	for (i = 0; i < adapter->num_tx_queues; i++) {
++<<<<<<< HEAD
 +		struct ixgbe_ring *ring = ACCESS_ONCE(adapter->tx_ring[i]);
 +		u64 bytes, packets;
 +		unsigned int start;
 +
 +		if (ring) {
 +			do {
 +				start = u64_stats_fetch_begin_irq(&ring->syncp);
 +				packets = ring->stats.packets;
 +				bytes   = ring->stats.bytes;
 +			} while (u64_stats_fetch_retry_irq(&ring->syncp, start));
 +			stats->tx_packets += packets;
 +			stats->tx_bytes   += bytes;
 +		}
++=======
+ 		struct ixgbe_ring *ring = READ_ONCE(adapter->tx_ring[i]);
+ 
+ 		ixgbe_get_ring_stats64(stats, ring);
+ 	}
+ 	for (i = 0; i < adapter->num_xdp_queues; i++) {
+ 		struct ixgbe_ring *ring = READ_ONCE(adapter->xdp_ring[i]);
+ 
+ 		ixgbe_get_ring_stats64(stats, ring);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	}
  	rcu_read_unlock();
  
diff --cc drivers/net/ethernet/sfc/ptp.c
index 2147a1c1ab7b,56c2db398def..000000000000
--- a/drivers/net/ethernet/sfc/ptp.c
+++ b/drivers/net/ethernet/sfc/ptp.c
@@@ -667,22 -651,24 +667,33 @@@ static void efx_ptp_send_times(struct e
  	int *mc_running = ptp->start.addr;
  
  	pps_get_ts(&now);
 -	start = now.ts_real;
  	limit = now.ts_real;
 -	timespec64_add_ns(&limit, SYNCHRONISE_PERIOD_NS);
 +	timespec_add_ns(&limit, SYNCHRONISE_PERIOD_NS);
  
  	/* Write host time for specified period or until MC is done */
++<<<<<<< HEAD
 +	while ((timespec_compare(&now.ts_real, &limit) < 0) &&
 +	       ACCESS_ONCE(*mc_running)) {
 +		struct timespec update_time;
++=======
+ 	while ((timespec64_compare(&now.ts_real, &limit) < 0) &&
+ 	       READ_ONCE(*mc_running)) {
+ 		struct timespec64 update_time;
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  		unsigned int host_time;
  
  		/* Don't update continuously to avoid saturating the PCIe bus */
  		update_time = now.ts_real;
 -		timespec64_add_ns(&update_time, SYNCHRONISATION_GRANULARITY_NS);
 +		timespec_add_ns(&update_time, SYNCHRONISATION_GRANULARITY_NS);
  		do {
  			pps_get_ts(&now);
++<<<<<<< HEAD
 +		} while ((timespec_compare(&now.ts_real, &update_time) < 0) &&
 +			 ACCESS_ONCE(*mc_running));
++=======
+ 		} while ((timespec64_compare(&now.ts_real, &update_time) < 0) &&
+ 			 READ_ONCE(*mc_running));
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  
  		/* Synchronise NIC with single word of time only */
  		host_time = (now.ts_real.tv_sec << MC_NANOSECOND_BITS |
diff --cc drivers/vhost/scsi.c
index 27e0c2b877d6,35e929f132e8..000000000000
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@@ -850,58 -929,134 +850,83 @@@ static void vhost_scsi_handle_vq(struc
  			continue;
  		}
  
++<<<<<<< HEAD
 +		exp_data_len = 0;
 +		for (i = 0; i < data_num; i++)
 +			exp_data_len += vq->iov[data_first + i].iov_len;
++=======
+ 		tpg = READ_ONCE(vs_tpg[*target]);
+ 		if (unlikely(!tpg)) {
+ 			/* Target does not exist, fail the request */
+ 			vhost_scsi_send_bad_target(vs, vq, head, out);
+ 			continue;
+ 		}
+ 		/*
+ 		 * Determine data_direction by calculating the total outgoing
+ 		 * iovec sizes + incoming iovec sizes vs. virtio-scsi request +
+ 		 * response headers respectively.
+ 		 *
+ 		 * For DMA_TO_DEVICE this is out_iter, which is already pointing
+ 		 * to the right place.
+ 		 *
+ 		 * For DMA_FROM_DEVICE, the iovec will be just past the end
+ 		 * of the virtio-scsi response header in either the same
+ 		 * or immediately following iovec.
+ 		 *
+ 		 * Any associated T10_PI bytes for the outgoing / incoming
+ 		 * payloads are included in calculation of exp_data_len here.
+ 		 */
+ 		prot_bytes = 0;
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
 +
 +		tv_cmd = vhost_scsi_allocate_cmd(vq, tv_tpg, &v_req,
 +					exp_data_len, data_direction);
 +		if (IS_ERR(tv_cmd)) {
 +			vq_err(vq, "vhost_scsi_allocate_cmd failed %ld\n",
 +					PTR_ERR(tv_cmd));
 +			goto err_cmd;
 +		}
 +		pr_debug("Allocated tv_cmd: %p exp_data_len: %d, data_direction"
 +			": %d\n", tv_cmd, exp_data_len, data_direction);
  
 -		if (out_size > req_size) {
 -			data_direction = DMA_TO_DEVICE;
 -			exp_data_len = out_size - req_size;
 -			data_iter = out_iter;
 -		} else if (in_size > rsp_size) {
 -			data_direction = DMA_FROM_DEVICE;
 -			exp_data_len = in_size - rsp_size;
 +		tv_cmd->tvc_vhost = vs;
 +		tv_cmd->tvc_vq = vq;
 +		tv_cmd->tvc_resp = vq->iov[out].iov_base;
  
 -			iov_iter_init(&in_iter, READ, &vq->iov[out], in,
 -				      rsp_size + exp_data_len);
 -			iov_iter_advance(&in_iter, rsp_size);
 -			data_iter = in_iter;
 -		} else {
 -			data_direction = DMA_NONE;
 -			exp_data_len = 0;
 -		}
  		/*
 -		 * If T10_PI header + payload is present, setup prot_iter values
 -		 * and recalculate data_iter for vhost_scsi_mapal() mapping to
 -		 * host scatterlists via get_user_pages_fast().
 +		 * Copy in the recieved CDB descriptor into tv_cmd->tvc_cdb
 +		 * that will be used by tcm_vhost_new_cmd_map() and down into
 +		 * target_setup_cmd_from_cdb()
  		 */
 -		if (t10_pi) {
 -			if (v_req_pi.pi_bytesout) {
 -				if (data_direction != DMA_TO_DEVICE) {
 -					vq_err(vq, "Received non zero pi_bytesout,"
 -						" but wrong data_direction\n");
 -					vhost_scsi_send_bad_target(vs, vq, head, out);
 -					continue;
 -				}
 -				prot_bytes = vhost32_to_cpu(vq, v_req_pi.pi_bytesout);
 -			} else if (v_req_pi.pi_bytesin) {
 -				if (data_direction != DMA_FROM_DEVICE) {
 -					vq_err(vq, "Received non zero pi_bytesin,"
 -						" but wrong data_direction\n");
 -					vhost_scsi_send_bad_target(vs, vq, head, out);
 -					continue;
 -				}
 -				prot_bytes = vhost32_to_cpu(vq, v_req_pi.pi_bytesin);
 -			}
 -			/*
 -			 * Set prot_iter to data_iter, and advance past any
 -			 * preceeding prot_bytes that may be present.
 -			 *
 -			 * Also fix up the exp_data_len to reflect only the
 -			 * actual data payload length.
 -			 */
 -			if (prot_bytes) {
 -				exp_data_len -= prot_bytes;
 -				prot_iter = data_iter;
 -				iov_iter_advance(&data_iter, prot_bytes);
 -			}
 -			tag = vhost64_to_cpu(vq, v_req_pi.tag);
 -			task_attr = v_req_pi.task_attr;
 -			cdb = &v_req_pi.cdb[0];
 -			lun = ((v_req_pi.lun[2] << 8) | v_req_pi.lun[3]) & 0x3FFF;
 -		} else {
 -			tag = vhost64_to_cpu(vq, v_req.tag);
 -			task_attr = v_req.task_attr;
 -			cdb = &v_req.cdb[0];
 -			lun = ((v_req.lun[2] << 8) | v_req.lun[3]) & 0x3FFF;
 -		}
 +		memcpy(tv_cmd->tvc_cdb, v_req.cdb, TCM_VHOST_MAX_CDB_SIZE);
  		/*
 -		 * Check that the received CDB size does not exceeded our
 -		 * hardcoded max for vhost-scsi, then get a pre-allocated
 -		 * cmd descriptor for the new virtio-scsi tag.
 -		 *
 -		 * TODO what if cdb was too small for varlen cdb header?
 +		 * Check that the recieved CDB size does not exceeded our
 +		 * hardcoded max for tcm_vhost
  		 */
 -		if (unlikely(scsi_command_size(cdb) > VHOST_SCSI_MAX_CDB_SIZE)) {
 +		/* TODO what if cdb was too small for varlen cdb header? */
 +		if (unlikely(scsi_command_size(tv_cmd->tvc_cdb) >
 +					TCM_VHOST_MAX_CDB_SIZE)) {
  			vq_err(vq, "Received SCSI CDB with command_size: %d that"
  				" exceeds SCSI_MAX_VARLEN_CDB_SIZE: %d\n",
 -				scsi_command_size(cdb), VHOST_SCSI_MAX_CDB_SIZE);
 -			vhost_scsi_send_bad_target(vs, vq, head, out);
 -			continue;
 +				scsi_command_size(tv_cmd->tvc_cdb),
 +				TCM_VHOST_MAX_CDB_SIZE);
 +			goto err_free;
  		}
 -		cmd = vhost_scsi_get_tag(vq, tpg, cdb, tag, lun, task_attr,
 -					 exp_data_len + prot_bytes,
 -					 data_direction);
 -		if (IS_ERR(cmd)) {
 -			vq_err(vq, "vhost_scsi_get_tag failed %ld\n",
 -			       PTR_ERR(cmd));
 -			vhost_scsi_send_bad_target(vs, vq, head, out);
 -			continue;
 -		}
 -		cmd->tvc_vhost = vs;
 -		cmd->tvc_vq = vq;
 -		cmd->tvc_resp_iov = vq->iov[out];
 -		cmd->tvc_in_iovs = in;
 +		tv_cmd->tvc_lun = ((v_req.lun[2] << 8) | v_req.lun[3]) & 0x3FFF;
  
  		pr_debug("vhost_scsi got command opcode: %#02x, lun: %d\n",
 -			 cmd->tvc_cdb[0], cmd->tvc_lun);
 -		pr_debug("cmd: %p exp_data_len: %d, prot_bytes: %d data_direction:"
 -			 " %d\n", cmd, exp_data_len, prot_bytes, data_direction);
 +			tv_cmd->tvc_cdb[0], tv_cmd->tvc_lun);
  
  		if (data_direction != DMA_NONE) {
 -			ret = vhost_scsi_mapal(cmd,
 -					       prot_bytes, &prot_iter,
 -					       exp_data_len, &data_iter);
 +			ret = vhost_scsi_map_iov_to_sgl(tv_cmd,
 +					&vq->iov[data_first], data_num,
 +					data_direction == DMA_FROM_DEVICE);
  			if (unlikely(ret)) {
  				vq_err(vq, "Failed to map iov to sgl\n");
 -				vhost_scsi_release_cmd(&cmd->tvc_se_cmd);
 -				vhost_scsi_send_bad_target(vs, vq, head, out);
 -				continue;
 +				goto err_free;
  			}
  		}
 +
  		/*
  		 * Save the descriptor from vhost_get_vq_desc() to be used to
  		 * complete the virtio-scsi request in TCM callback context via
diff --cc fs/direct-io.c
index 2cde7d0049a9,98fe1325da9d..000000000000
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@@ -1189,15 -1147,12 +1189,19 @@@ static inline int drop_refcount(struct 
   * for the whole file.
   */
  static inline ssize_t
 -do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 -		      struct block_device *bdev, struct iov_iter *iter,
 -		      get_block_t get_block, dio_iodone_t end_io,
 -		      dio_submit_t submit_io, int flags)
 +do_blockdev_direct_IO(int rw, struct kiocb *iocb, struct inode *inode,
 +	struct block_device *bdev, const struct iovec *iov, loff_t offset, 
 +	unsigned long nr_segs, get_block_t get_block, dio_iodone_t end_io,
 +	dio_submit_t submit_io,	int flags)
  {
++<<<<<<< HEAD
 +	int seg;
 +	size_t size;
 +	unsigned long addr;
 +	unsigned i_blkbits = ACCESS_ONCE(inode->i_blkbits);
++=======
+ 	unsigned i_blkbits = READ_ONCE(inode->i_blkbits);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	unsigned blkbits = i_blkbits;
  	unsigned blocksize_mask = (1 << blkbits) - 1;
  	ssize_t retval = -EINVAL;
diff --cc fs/exec.c
index 25871b5322aa,1d6243d9f2b6..000000000000
--- a/fs/exec.c
+++ b/fs/exec.c
@@@ -1697,42 -1905,15 +1697,49 @@@ EXPORT_SYMBOL(set_binfmt)
   */
  void set_dumpable(struct mm_struct *mm, int value)
  {
 -	unsigned long old, new;
 +	switch (value) {
 +	case SUID_DUMP_DISABLE:
 +		clear_bit(MMF_DUMPABLE, &mm->flags);
 +		smp_wmb();
 +		clear_bit(MMF_DUMP_SECURELY, &mm->flags);
 +		break;
 +	case SUID_DUMP_USER:
 +		set_bit(MMF_DUMPABLE, &mm->flags);
 +		smp_wmb();
 +		clear_bit(MMF_DUMP_SECURELY, &mm->flags);
 +		break;
 +	case SUID_DUMP_ROOT:
 +		set_bit(MMF_DUMP_SECURELY, &mm->flags);
 +		smp_wmb();
 +		set_bit(MMF_DUMPABLE, &mm->flags);
 +		break;
 +	}
 +}
  
 -	if (WARN_ON((unsigned)value > SUID_DUMP_ROOT))
 -		return;
 +int __get_dumpable(unsigned long mm_flags)
 +{
 +	int ret;
 +
++<<<<<<< HEAD
 +	ret = mm_flags & MMF_DUMPABLE_MASK;
 +	return (ret > SUID_DUMP_USER) ? SUID_DUMP_ROOT : ret;
 +}
  
 +/*
 + * This returns the actual value of the suid_dumpable flag. For things
 + * that are using this for checking for privilege transitions, it must
 + * test against SUID_DUMP_USER rather than treating it as a boolean
 + * value.
 + */
 +int get_dumpable(struct mm_struct *mm)
 +{
 +	return __get_dumpable(mm->flags);
++=======
+ 	do {
+ 		old = READ_ONCE(mm->flags);
+ 		new = (old & ~MMF_DUMPABLE_MASK) | value;
+ 	} while (cmpxchg(&mm->flags, old, new) != old);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  }
  
  SYSCALL_DEFINE3(execve,
diff --cc fs/fuse/dev.c
index ad00311500d7,a42d89371748..000000000000
--- a/fs/fuse/dev.c
+++ b/fs/fuse/dev.c
@@@ -33,7 -33,7 +33,11 @@@ static struct fuse_conn *fuse_get_conn(
  	 * Lockless access is OK, because file->private data is set
  	 * once during mount and is valid until the file is released.
  	 */
++<<<<<<< HEAD
 +	return file->private_data;
++=======
+ 	return READ_ONCE(file->private_data);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  }
  
  static void fuse_request_init(struct fuse_req *req, struct page **pages,
diff --cc fs/nfs/dir.c
index 09e511dfc6d8,f439f1c45008..000000000000
--- a/fs/nfs/dir.c
+++ b/fs/nfs/dir.c
@@@ -1126,8 -1081,8 +1126,13 @@@ static int nfs_lookup_revalidate(struc
  	int error;
  
  	if (flags & LOOKUP_RCU) {
++<<<<<<< HEAD
 +		parent = ACCESS_ONCE(dentry->d_parent);
 +		dir = ACCESS_ONCE(parent->d_inode);
++=======
+ 		parent = READ_ONCE(dentry->d_parent);
+ 		dir = d_inode_rcu(parent);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  		if (!dir)
  			return -ECHILD;
  	} else {
@@@ -1613,8 -1582,8 +1618,13 @@@ static int nfs4_lookup_revalidate(struc
  		struct inode *dir;
  
  		if (flags & LOOKUP_RCU) {
++<<<<<<< HEAD
 +			parent = ACCESS_ONCE(dentry->d_parent);
 +			dir = ACCESS_ONCE(parent->d_inode);
++=======
+ 			parent = READ_ONCE(dentry->d_parent);
+ 			dir = d_inode_rcu(parent);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  			if (!dir)
  				return -ECHILD;
  		} else {
diff --cc fs/proc_namespace.c
index 73ca1740d839,03afd5150916..000000000000
--- a/fs/proc_namespace.c
+++ b/fs/proc_namespace.c
@@@ -24,9 -27,9 +24,15 @@@ static unsigned mounts_poll(struct fil
  
  	poll_wait(file, &p->ns->poll, wait);
  
++<<<<<<< HEAD
 +	event = ACCESS_ONCE(ns->event);
 +	if (p->m.poll_event != event) {
 +		p->m.poll_event = event;
++=======
+ 	event = READ_ONCE(ns->event);
+ 	if (m->poll_event != event) {
+ 		m->poll_event = event;
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  		res |= POLLERR | POLLPRI;
  	}
  
diff --cc kernel/exit.c
index 1afa79930cfc,6b4298a41167..000000000000
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@@ -1360,7 -1334,18 +1360,22 @@@ static int wait_task_continued(struct w
  static int wait_consider_task(struct wait_opts *wo, int ptrace,
  				struct task_struct *p)
  {
++<<<<<<< HEAD
 +	int ret = eligible_child(wo, p);
++=======
+ 	/*
+ 	 * We can race with wait_task_zombie() from another thread.
+ 	 * Ensure that EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE transition
+ 	 * can't confuse the checks below.
+ 	 */
+ 	int exit_state = READ_ONCE(p->exit_state);
+ 	int ret;
+ 
+ 	if (unlikely(exit_state == EXIT_DEAD))
+ 		return 0;
+ 
+ 	ret = eligible_child(wo, ptrace, p);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	if (!ret)
  		return ret;
  
diff --cc kernel/trace/ring_buffer.c
index b6fd21ee8ebf,845f3805c73d..000000000000
--- a/kernel/trace/ring_buffer.c
+++ b/kernel/trace/ring_buffer.c
@@@ -2665,22 -2597,188 +2665,134 @@@ static __always_inline int trace_recurs
  	return 0;
  }
  
 -static __always_inline void
 -trace_recursive_unlock(struct ring_buffer_per_cpu *cpu_buffer)
 +static __always_inline void trace_recursive_unlock(void)
  {
 -	cpu_buffer->current_context &= cpu_buffer->current_context - 1;
 +	unsigned int val = this_cpu_read(current_context);
 +
 +	val--;
 +	val &= this_cpu_read(current_context);
 +	this_cpu_write(current_context, val);
  }
  
 -/**
 - * ring_buffer_unlock_commit - commit a reserved
 - * @buffer: The buffer to commit to
 - * @event: The event pointer to commit.
 - *
 - * This commits the data to the ring buffer, and releases any locks held.
 - *
 - * Must be paired with ring_buffer_lock_reserve.
 - */
 -int ring_buffer_unlock_commit(struct ring_buffer *buffer,
 -			      struct ring_buffer_event *event)
 -{
 -	struct ring_buffer_per_cpu *cpu_buffer;
 -	int cpu = raw_smp_processor_id();
 +#else
  
 -	cpu_buffer = buffer->buffers[cpu];
 +#define trace_recursive_lock()		(0)
 +#define trace_recursive_unlock()	do { } while (0)
  
++<<<<<<< HEAD
++=======
+ 	rb_commit(cpu_buffer, event);
+ 
+ 	rb_wakeups(buffer, cpu_buffer);
+ 
+ 	trace_recursive_unlock(cpu_buffer);
+ 
+ 	preempt_enable_notrace();
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(ring_buffer_unlock_commit);
+ 
+ static noinline void
+ rb_handle_timestamp(struct ring_buffer_per_cpu *cpu_buffer,
+ 		    struct rb_event_info *info)
+ {
+ 	WARN_ONCE(info->delta > (1ULL << 59),
+ 		  KERN_WARNING "Delta way too big! %llu ts=%llu write stamp = %llu\n%s",
+ 		  (unsigned long long)info->delta,
+ 		  (unsigned long long)info->ts,
+ 		  (unsigned long long)cpu_buffer->write_stamp,
+ 		  sched_clock_stable() ? "" :
+ 		  "If you just came from a suspend/resume,\n"
+ 		  "please switch to the trace global clock:\n"
+ 		  "  echo global > /sys/kernel/debug/tracing/trace_clock\n");
+ 	info->add_timestamp = 1;
+ }
+ 
+ static struct ring_buffer_event *
+ __rb_reserve_next(struct ring_buffer_per_cpu *cpu_buffer,
+ 		  struct rb_event_info *info)
+ {
+ 	struct ring_buffer_event *event;
+ 	struct buffer_page *tail_page;
+ 	unsigned long tail, write;
+ 
+ 	/*
+ 	 * If the time delta since the last event is too big to
+ 	 * hold in the time field of the event, then we append a
+ 	 * TIME EXTEND event ahead of the data event.
+ 	 */
+ 	if (unlikely(info->add_timestamp))
+ 		info->length += RB_LEN_TIME_EXTEND;
+ 
+ 	/* Don't let the compiler play games with cpu_buffer->tail_page */
+ 	tail_page = info->tail_page = READ_ONCE(cpu_buffer->tail_page);
+ 	write = local_add_return(info->length, &tail_page->write);
+ 
+ 	/* set write to only the index of the write */
+ 	write &= RB_WRITE_MASK;
+ 	tail = write - info->length;
+ 
+ 	/*
+ 	 * If this is the first commit on the page, then it has the same
+ 	 * timestamp as the page itself.
+ 	 */
+ 	if (!tail)
+ 		info->delta = 0;
+ 
+ 	/* See if we shot pass the end of this buffer page */
+ 	if (unlikely(write > BUF_PAGE_SIZE))
+ 		return rb_move_tail(cpu_buffer, tail, info);
+ 
+ 	/* We reserved something on the buffer */
+ 
+ 	event = __rb_page_index(tail_page, tail);
+ 	kmemcheck_annotate_bitfield(event, bitfield);
+ 	rb_update_event(cpu_buffer, event, info);
+ 
+ 	local_inc(&tail_page->entries);
+ 
+ 	/*
+ 	 * If this is the first commit on the page, then update
+ 	 * its timestamp.
+ 	 */
+ 	if (!tail)
+ 		tail_page->page->time_stamp = info->ts;
+ 
+ 	/* account for these added bytes */
+ 	local_add(info->length, &cpu_buffer->entries_bytes);
+ 
+ 	return event;
+ }
+ 
+ static __always_inline struct ring_buffer_event *
+ rb_reserve_next_event(struct ring_buffer *buffer,
+ 		      struct ring_buffer_per_cpu *cpu_buffer,
+ 		      unsigned long length)
+ {
+ 	struct ring_buffer_event *event;
+ 	struct rb_event_info info;
+ 	int nr_loops = 0;
+ 	u64 diff;
+ 
+ 	rb_start_commit(cpu_buffer);
+ 
+ #ifdef CONFIG_RING_BUFFER_ALLOW_SWAP
+ 	/*
+ 	 * Due to the ability to swap a cpu buffer from a buffer
+ 	 * it is possible it was swapped before we committed.
+ 	 * (committing stops a swap). We check for it here and
+ 	 * if it happened, we have to fail the write.
+ 	 */
+ 	barrier();
+ 	if (unlikely(READ_ONCE(cpu_buffer->buffer) != buffer)) {
+ 		local_dec(&cpu_buffer->committing);
+ 		local_dec(&cpu_buffer->commits);
+ 		return NULL;
+ 	}
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  #endif
  
 -	info.length = rb_calculate_event_length(length);
 - again:
 -	info.add_timestamp = 0;
 -	info.delta = 0;
 -
 -	/*
 -	 * We allow for interrupts to reenter here and do a trace.
 -	 * If one does, it will cause this original code to loop
 -	 * back here. Even with heavy interrupts happening, this
 -	 * should only happen a few times in a row. If this happens
 -	 * 1000 times in a row, there must be either an interrupt
 -	 * storm or we have something buggy.
 -	 * Bail!
 -	 */
 -	if (RB_WARN_ON(cpu_buffer, ++nr_loops > 1000))
 -		goto out_fail;
 -
 -	info.ts = rb_time_stamp(cpu_buffer->buffer);
 -	diff = info.ts - cpu_buffer->write_stamp;
 -
 -	/* make sure this diff is calculated here */
 -	barrier();
 -
 -	/* Did the write stamp get updated already? */
 -	if (likely(info.ts >= cpu_buffer->write_stamp)) {
 -		info.delta = diff;
 -		if (unlikely(test_time_stamp(info.delta)))
 -			rb_handle_timestamp(cpu_buffer, &info);
 -	}
 -
 -	event = __rb_reserve_next(cpu_buffer, &info);
 -
 -	if (unlikely(PTR_ERR(event) == -EAGAIN)) {
 -		if (info.add_timestamp)
 -			info.length -= RB_LEN_TIME_EXTEND;
 -		goto again;
 -	}
 -
 -	if (!event)
 -		goto out_fail;
 -
 -	return event;
 -
 - out_fail:
 -	rb_end_commit(cpu_buffer);
 -	return NULL;
 -}
 -
  /**
   * ring_buffer_lock_reserve - reserve a part of the buffer
   * @buffer: the ring buffer to reserve from
diff --cc kernel/trace/trace.h
index 4d4d21b84af4,9050c8b3ccde..000000000000
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@@ -1037,6 -1453,15 +1037,18 @@@ extern void trace_event_enable_cmd_reco
  extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
  extern int event_trace_del_tracer(struct trace_array *tr);
  
++<<<<<<< HEAD
++=======
+ extern struct trace_event_file *find_event_file(struct trace_array *tr,
+ 						const char *system,
+ 						const char *event);
+ 
+ static inline void *event_file_data(struct file *filp)
+ {
+ 	return READ_ONCE(file_inode(filp)->i_private);
+ }
+ 
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  extern struct mutex event_mutex;
  extern struct list_head ftrace_events;
  
diff --cc kernel/trace/trace_stack.c
index b20428c5efe2,780262210c9a..000000000000
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@@ -50,14 -41,44 +50,19 @@@ static DEFINE_MUTEX(stack_sysctl_mutex)
  int stack_tracer_enabled;
  static int last_stack_tracer_enabled;
  
 -void stack_trace_print(void)
 -{
 -	long i;
 -	int size;
 -
 -	pr_emerg("        Depth    Size   Location    (%d entries)\n"
 -			   "        -----    ----   --------\n",
 -			   stack_trace_max.nr_entries);
 -
 -	for (i = 0; i < stack_trace_max.nr_entries; i++) {
 -		if (stack_dump_trace[i] == ULONG_MAX)
 -			break;
 -		if (i+1 == stack_trace_max.nr_entries ||
 -				stack_dump_trace[i+1] == ULONG_MAX)
 -			size = stack_trace_index[i];
 -		else
 -			size = stack_trace_index[i] - stack_trace_index[i+1];
 -
 -		pr_emerg("%3ld) %8d   %5d   %pS\n", i, stack_trace_index[i],
 -				size, (void *)stack_dump_trace[i]);
 -	}
 -}
 -
 -/*
 - * When arch-specific code overrides this function, the following
 - * data should be filled up, assuming stack_trace_max_lock is held to
 - * prevent concurrent updates.
 - *     stack_trace_index[]
 - *     stack_trace_max
 - *     stack_trace_max_size
 - */
 -void __weak
 +static inline void
  check_stack(unsigned long ip, unsigned long *stack)
  {
 -	unsigned long this_size, flags; unsigned long *p, *top, *start;
 +	unsigned long this_size, flags;
 +	unsigned long *p, *top, *start;
  	static int tracer_frame;
++<<<<<<< HEAD
 +	int frame_size = ACCESS_ONCE(tracer_frame);
 +	int i;
++=======
+ 	int frame_size = READ_ONCE(tracer_frame);
+ 	int i, x;
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  
  	this_size = ((unsigned long)stack) & (THREAD_SIZE-1);
  	this_size = THREAD_SIZE - this_size;
diff --cc mm/huge_memory.c
index 075f651aabed,c3bf907a03ee..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -3398,6 -2282,614 +3398,617 @@@ void vma_adjust_trans_huge(struct vm_ar
  		if (nstart & ~HPAGE_PMD_MASK &&
  		    (nstart & HPAGE_PMD_MASK) >= next->vm_start &&
  		    (nstart & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= next->vm_end)
 -			split_huge_pmd_address(next, nstart, false, NULL);
 +			split_huge_page_address(next->vm_mm, nstart);
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ static void freeze_page(struct page *page)
+ {
+ 	enum ttu_flags ttu_flags = TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS |
+ 		TTU_RMAP_LOCKED | TTU_SPLIT_HUGE_PMD;
+ 	bool unmap_success;
+ 
+ 	VM_BUG_ON_PAGE(!PageHead(page), page);
+ 
+ 	if (PageAnon(page))
+ 		ttu_flags |= TTU_SPLIT_FREEZE;
+ 
+ 	unmap_success = try_to_unmap(page, ttu_flags);
+ 	VM_BUG_ON_PAGE(!unmap_success, page);
+ }
+ 
+ static void unfreeze_page(struct page *page)
+ {
+ 	int i;
+ 	if (PageTransHuge(page)) {
+ 		remove_migration_ptes(page, page, true);
+ 	} else {
+ 		for (i = 0; i < HPAGE_PMD_NR; i++)
+ 			remove_migration_ptes(page + i, page + i, true);
+ 	}
+ }
+ 
+ static void __split_huge_page_tail(struct page *head, int tail,
+ 		struct lruvec *lruvec, struct list_head *list)
+ {
+ 	struct page *page_tail = head + tail;
+ 
+ 	VM_BUG_ON_PAGE(atomic_read(&page_tail->_mapcount) != -1, page_tail);
+ 	VM_BUG_ON_PAGE(page_ref_count(page_tail) != 0, page_tail);
+ 
+ 	/*
+ 	 * tail_page->_refcount is zero and not changing from under us. But
+ 	 * get_page_unless_zero() may be running from under us on the
+ 	 * tail_page. If we used atomic_set() below instead of atomic_inc() or
+ 	 * atomic_add(), we would then run atomic_set() concurrently with
+ 	 * get_page_unless_zero(), and atomic_set() is implemented in C not
+ 	 * using locked ops. spin_unlock on x86 sometime uses locked ops
+ 	 * because of PPro errata 66, 92, so unless somebody can guarantee
+ 	 * atomic_set() here would be safe on all archs (and not only on x86),
+ 	 * it's safer to use atomic_inc()/atomic_add().
+ 	 */
+ 	if (PageAnon(head) && !PageSwapCache(head)) {
+ 		page_ref_inc(page_tail);
+ 	} else {
+ 		/* Additional pin to radix tree */
+ 		page_ref_add(page_tail, 2);
+ 	}
+ 
+ 	page_tail->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
+ 	page_tail->flags |= (head->flags &
+ 			((1L << PG_referenced) |
+ 			 (1L << PG_swapbacked) |
+ 			 (1L << PG_swapcache) |
+ 			 (1L << PG_mlocked) |
+ 			 (1L << PG_uptodate) |
+ 			 (1L << PG_active) |
+ 			 (1L << PG_locked) |
+ 			 (1L << PG_unevictable) |
+ 			 (1L << PG_dirty)));
+ 
+ 	/*
+ 	 * After clearing PageTail the gup refcount can be released.
+ 	 * Page flags also must be visible before we make the page non-compound.
+ 	 */
+ 	smp_wmb();
+ 
+ 	clear_compound_head(page_tail);
+ 
+ 	if (page_is_young(head))
+ 		set_page_young(page_tail);
+ 	if (page_is_idle(head))
+ 		set_page_idle(page_tail);
+ 
+ 	/* ->mapping in first tail page is compound_mapcount */
+ 	VM_BUG_ON_PAGE(tail > 2 && page_tail->mapping != TAIL_MAPPING,
+ 			page_tail);
+ 	page_tail->mapping = head->mapping;
+ 
+ 	page_tail->index = head->index + tail;
+ 	page_cpupid_xchg_last(page_tail, page_cpupid_last(head));
+ 	lru_add_page_tail(head, page_tail, lruvec, list);
+ }
+ 
+ static void __split_huge_page(struct page *page, struct list_head *list,
+ 		unsigned long flags)
+ {
+ 	struct page *head = compound_head(page);
+ 	struct zone *zone = page_zone(head);
+ 	struct lruvec *lruvec;
+ 	pgoff_t end = -1;
+ 	int i;
+ 
+ 	lruvec = mem_cgroup_page_lruvec(head, zone->zone_pgdat);
+ 
+ 	/* complete memcg works before add pages to LRU */
+ 	mem_cgroup_split_huge_fixup(head);
+ 
+ 	if (!PageAnon(page))
+ 		end = DIV_ROUND_UP(i_size_read(head->mapping->host), PAGE_SIZE);
+ 
+ 	for (i = HPAGE_PMD_NR - 1; i >= 1; i--) {
+ 		__split_huge_page_tail(head, i, lruvec, list);
+ 		/* Some pages can be beyond i_size: drop them from page cache */
+ 		if (head[i].index >= end) {
+ 			__ClearPageDirty(head + i);
+ 			__delete_from_page_cache(head + i, NULL);
+ 			if (IS_ENABLED(CONFIG_SHMEM) && PageSwapBacked(head))
+ 				shmem_uncharge(head->mapping->host, 1);
+ 			put_page(head + i);
+ 		}
+ 	}
+ 
+ 	ClearPageCompound(head);
+ 	/* See comment in __split_huge_page_tail() */
+ 	if (PageAnon(head)) {
+ 		/* Additional pin to radix tree of swap cache */
+ 		if (PageSwapCache(head))
+ 			page_ref_add(head, 2);
+ 		else
+ 			page_ref_inc(head);
+ 	} else {
+ 		/* Additional pin to radix tree */
+ 		page_ref_add(head, 2);
+ 		spin_unlock(&head->mapping->tree_lock);
+ 	}
+ 
+ 	spin_unlock_irqrestore(zone_lru_lock(page_zone(head)), flags);
+ 
+ 	unfreeze_page(head);
+ 
+ 	for (i = 0; i < HPAGE_PMD_NR; i++) {
+ 		struct page *subpage = head + i;
+ 		if (subpage == page)
+ 			continue;
+ 		unlock_page(subpage);
+ 
+ 		/*
+ 		 * Subpages may be freed if there wasn't any mapping
+ 		 * like if add_to_swap() is running on a lru page that
+ 		 * had its mapping zapped. And freeing these pages
+ 		 * requires taking the lru_lock so we do the put_page
+ 		 * of the tail pages after the split is complete.
+ 		 */
+ 		put_page(subpage);
+ 	}
+ }
+ 
+ int total_mapcount(struct page *page)
+ {
+ 	int i, compound, ret;
+ 
+ 	VM_BUG_ON_PAGE(PageTail(page), page);
+ 
+ 	if (likely(!PageCompound(page)))
+ 		return atomic_read(&page->_mapcount) + 1;
+ 
+ 	compound = compound_mapcount(page);
+ 	if (PageHuge(page))
+ 		return compound;
+ 	ret = compound;
+ 	for (i = 0; i < HPAGE_PMD_NR; i++)
+ 		ret += atomic_read(&page[i]._mapcount) + 1;
+ 	/* File pages has compound_mapcount included in _mapcount */
+ 	if (!PageAnon(page))
+ 		return ret - compound * HPAGE_PMD_NR;
+ 	if (PageDoubleMap(page))
+ 		ret -= HPAGE_PMD_NR;
+ 	return ret;
+ }
+ 
+ /*
+  * This calculates accurately how many mappings a transparent hugepage
+  * has (unlike page_mapcount() which isn't fully accurate). This full
+  * accuracy is primarily needed to know if copy-on-write faults can
+  * reuse the page and change the mapping to read-write instead of
+  * copying them. At the same time this returns the total_mapcount too.
+  *
+  * The function returns the highest mapcount any one of the subpages
+  * has. If the return value is one, even if different processes are
+  * mapping different subpages of the transparent hugepage, they can
+  * all reuse it, because each process is reusing a different subpage.
+  *
+  * The total_mapcount is instead counting all virtual mappings of the
+  * subpages. If the total_mapcount is equal to "one", it tells the
+  * caller all mappings belong to the same "mm" and in turn the
+  * anon_vma of the transparent hugepage can become the vma->anon_vma
+  * local one as no other process may be mapping any of the subpages.
+  *
+  * It would be more accurate to replace page_mapcount() with
+  * page_trans_huge_mapcount(), however we only use
+  * page_trans_huge_mapcount() in the copy-on-write faults where we
+  * need full accuracy to avoid breaking page pinning, because
+  * page_trans_huge_mapcount() is slower than page_mapcount().
+  */
+ int page_trans_huge_mapcount(struct page *page, int *total_mapcount)
+ {
+ 	int i, ret, _total_mapcount, mapcount;
+ 
+ 	/* hugetlbfs shouldn't call it */
+ 	VM_BUG_ON_PAGE(PageHuge(page), page);
+ 
+ 	if (likely(!PageTransCompound(page))) {
+ 		mapcount = atomic_read(&page->_mapcount) + 1;
+ 		if (total_mapcount)
+ 			*total_mapcount = mapcount;
+ 		return mapcount;
+ 	}
+ 
+ 	page = compound_head(page);
+ 
+ 	_total_mapcount = ret = 0;
+ 	for (i = 0; i < HPAGE_PMD_NR; i++) {
+ 		mapcount = atomic_read(&page[i]._mapcount) + 1;
+ 		ret = max(ret, mapcount);
+ 		_total_mapcount += mapcount;
+ 	}
+ 	if (PageDoubleMap(page)) {
+ 		ret -= 1;
+ 		_total_mapcount -= HPAGE_PMD_NR;
+ 	}
+ 	mapcount = compound_mapcount(page);
+ 	ret += mapcount;
+ 	_total_mapcount += mapcount;
+ 	if (total_mapcount)
+ 		*total_mapcount = _total_mapcount;
+ 	return ret;
+ }
+ 
+ /* Racy check whether the huge page can be split */
+ bool can_split_huge_page(struct page *page, int *pextra_pins)
+ {
+ 	int extra_pins;
+ 
+ 	/* Additional pins from radix tree */
+ 	if (PageAnon(page))
+ 		extra_pins = PageSwapCache(page) ? HPAGE_PMD_NR : 0;
+ 	else
+ 		extra_pins = HPAGE_PMD_NR;
+ 	if (pextra_pins)
+ 		*pextra_pins = extra_pins;
+ 	return total_mapcount(page) == page_count(page) - extra_pins - 1;
+ }
+ 
+ /*
+  * This function splits huge page into normal pages. @page can point to any
+  * subpage of huge page to split. Split doesn't change the position of @page.
+  *
+  * Only caller must hold pin on the @page, otherwise split fails with -EBUSY.
+  * The huge page must be locked.
+  *
+  * If @list is null, tail pages will be added to LRU list, otherwise, to @list.
+  *
+  * Both head page and tail pages will inherit mapping, flags, and so on from
+  * the hugepage.
+  *
+  * GUP pin and PG_locked transferred to @page. Rest subpages can be freed if
+  * they are not mapped.
+  *
+  * Returns 0 if the hugepage is split successfully.
+  * Returns -EBUSY if the page is pinned or if anon_vma disappeared from under
+  * us.
+  */
+ int split_huge_page_to_list(struct page *page, struct list_head *list)
+ {
+ 	struct page *head = compound_head(page);
+ 	struct pglist_data *pgdata = NODE_DATA(page_to_nid(head));
+ 	struct anon_vma *anon_vma = NULL;
+ 	struct address_space *mapping = NULL;
+ 	int count, mapcount, extra_pins, ret;
+ 	bool mlocked;
+ 	unsigned long flags;
+ 
+ 	VM_BUG_ON_PAGE(is_huge_zero_page(page), page);
+ 	VM_BUG_ON_PAGE(!PageLocked(page), page);
+ 	VM_BUG_ON_PAGE(!PageCompound(page), page);
+ 
+ 	if (PageWriteback(page))
+ 		return -EBUSY;
+ 
+ 	if (PageAnon(head)) {
+ 		/*
+ 		 * The caller does not necessarily hold an mmap_sem that would
+ 		 * prevent the anon_vma disappearing so we first we take a
+ 		 * reference to it and then lock the anon_vma for write. This
+ 		 * is similar to page_lock_anon_vma_read except the write lock
+ 		 * is taken to serialise against parallel split or collapse
+ 		 * operations.
+ 		 */
+ 		anon_vma = page_get_anon_vma(head);
+ 		if (!anon_vma) {
+ 			ret = -EBUSY;
+ 			goto out;
+ 		}
+ 		mapping = NULL;
+ 		anon_vma_lock_write(anon_vma);
+ 	} else {
+ 		mapping = head->mapping;
+ 
+ 		/* Truncated ? */
+ 		if (!mapping) {
+ 			ret = -EBUSY;
+ 			goto out;
+ 		}
+ 
+ 		anon_vma = NULL;
+ 		i_mmap_lock_read(mapping);
+ 	}
+ 
+ 	/*
+ 	 * Racy check if we can split the page, before freeze_page() will
+ 	 * split PMDs
+ 	 */
+ 	if (!can_split_huge_page(head, &extra_pins)) {
+ 		ret = -EBUSY;
+ 		goto out_unlock;
+ 	}
+ 
+ 	mlocked = PageMlocked(page);
+ 	freeze_page(head);
+ 	VM_BUG_ON_PAGE(compound_mapcount(head), head);
+ 
+ 	/* Make sure the page is not on per-CPU pagevec as it takes pin */
+ 	if (mlocked)
+ 		lru_add_drain();
+ 
+ 	/* prevent PageLRU to go away from under us, and freeze lru stats */
+ 	spin_lock_irqsave(zone_lru_lock(page_zone(head)), flags);
+ 
+ 	if (mapping) {
+ 		void **pslot;
+ 
+ 		spin_lock(&mapping->tree_lock);
+ 		pslot = radix_tree_lookup_slot(&mapping->page_tree,
+ 				page_index(head));
+ 		/*
+ 		 * Check if the head page is present in radix tree.
+ 		 * We assume all tail are present too, if head is there.
+ 		 */
+ 		if (radix_tree_deref_slot_protected(pslot,
+ 					&mapping->tree_lock) != head)
+ 			goto fail;
+ 	}
+ 
+ 	/* Prevent deferred_split_scan() touching ->_refcount */
+ 	spin_lock(&pgdata->split_queue_lock);
+ 	count = page_count(head);
+ 	mapcount = total_mapcount(head);
+ 	if (!mapcount && page_ref_freeze(head, 1 + extra_pins)) {
+ 		if (!list_empty(page_deferred_list(head))) {
+ 			pgdata->split_queue_len--;
+ 			list_del(page_deferred_list(head));
+ 		}
+ 		if (mapping)
+ 			__dec_node_page_state(page, NR_SHMEM_THPS);
+ 		spin_unlock(&pgdata->split_queue_lock);
+ 		__split_huge_page(page, list, flags);
+ 		if (PageSwapCache(head)) {
+ 			swp_entry_t entry = { .val = page_private(head) };
+ 
+ 			ret = split_swap_cluster(entry);
+ 		} else
+ 			ret = 0;
+ 	} else {
+ 		if (IS_ENABLED(CONFIG_DEBUG_VM) && mapcount) {
+ 			pr_alert("total_mapcount: %u, page_count(): %u\n",
+ 					mapcount, count);
+ 			if (PageTail(page))
+ 				dump_page(head, NULL);
+ 			dump_page(page, "total_mapcount(head) > 0");
+ 			BUG();
+ 		}
+ 		spin_unlock(&pgdata->split_queue_lock);
+ fail:		if (mapping)
+ 			spin_unlock(&mapping->tree_lock);
+ 		spin_unlock_irqrestore(zone_lru_lock(page_zone(head)), flags);
+ 		unfreeze_page(head);
+ 		ret = -EBUSY;
+ 	}
+ 
+ out_unlock:
+ 	if (anon_vma) {
+ 		anon_vma_unlock_write(anon_vma);
+ 		put_anon_vma(anon_vma);
+ 	}
+ 	if (mapping)
+ 		i_mmap_unlock_read(mapping);
+ out:
+ 	count_vm_event(!ret ? THP_SPLIT_PAGE : THP_SPLIT_PAGE_FAILED);
+ 	return ret;
+ }
+ 
+ void free_transhuge_page(struct page *page)
+ {
+ 	struct pglist_data *pgdata = NODE_DATA(page_to_nid(page));
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
+ 	if (!list_empty(page_deferred_list(page))) {
+ 		pgdata->split_queue_len--;
+ 		list_del(page_deferred_list(page));
+ 	}
+ 	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
+ 	free_compound_page(page);
+ }
+ 
+ void deferred_split_huge_page(struct page *page)
+ {
+ 	struct pglist_data *pgdata = NODE_DATA(page_to_nid(page));
+ 	unsigned long flags;
+ 
+ 	VM_BUG_ON_PAGE(!PageTransHuge(page), page);
+ 
+ 	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
+ 	if (list_empty(page_deferred_list(page))) {
+ 		count_vm_event(THP_DEFERRED_SPLIT_PAGE);
+ 		list_add_tail(page_deferred_list(page), &pgdata->split_queue);
+ 		pgdata->split_queue_len++;
+ 	}
+ 	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
+ }
+ 
+ static unsigned long deferred_split_count(struct shrinker *shrink,
+ 		struct shrink_control *sc)
+ {
+ 	struct pglist_data *pgdata = NODE_DATA(sc->nid);
+ 	return READ_ONCE(pgdata->split_queue_len);
+ }
+ 
+ static unsigned long deferred_split_scan(struct shrinker *shrink,
+ 		struct shrink_control *sc)
+ {
+ 	struct pglist_data *pgdata = NODE_DATA(sc->nid);
+ 	unsigned long flags;
+ 	LIST_HEAD(list), *pos, *next;
+ 	struct page *page;
+ 	int split = 0;
+ 
+ 	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
+ 	/* Take pin on all head pages to avoid freeing them under us */
+ 	list_for_each_safe(pos, next, &pgdata->split_queue) {
+ 		page = list_entry((void *)pos, struct page, mapping);
+ 		page = compound_head(page);
+ 		if (get_page_unless_zero(page)) {
+ 			list_move(page_deferred_list(page), &list);
+ 		} else {
+ 			/* We lost race with put_compound_page() */
+ 			list_del_init(page_deferred_list(page));
+ 			pgdata->split_queue_len--;
+ 		}
+ 		if (!--sc->nr_to_scan)
+ 			break;
+ 	}
+ 	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
+ 
+ 	list_for_each_safe(pos, next, &list) {
+ 		page = list_entry((void *)pos, struct page, mapping);
+ 		lock_page(page);
+ 		/* split_huge_page() removes page from list on success */
+ 		if (!split_huge_page(page))
+ 			split++;
+ 		unlock_page(page);
+ 		put_page(page);
+ 	}
+ 
+ 	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
+ 	list_splice_tail(&list, &pgdata->split_queue);
+ 	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
+ 
+ 	/*
+ 	 * Stop shrinker if we didn't split any page, but the queue is empty.
+ 	 * This can happen if pages were freed under us.
+ 	 */
+ 	if (!split && list_empty(&pgdata->split_queue))
+ 		return SHRINK_STOP;
+ 	return split;
+ }
+ 
+ static struct shrinker deferred_split_shrinker = {
+ 	.count_objects = deferred_split_count,
+ 	.scan_objects = deferred_split_scan,
+ 	.seeks = DEFAULT_SEEKS,
+ 	.flags = SHRINKER_NUMA_AWARE,
+ };
+ 
+ #ifdef CONFIG_DEBUG_FS
+ static int split_huge_pages_set(void *data, u64 val)
+ {
+ 	struct zone *zone;
+ 	struct page *page;
+ 	unsigned long pfn, max_zone_pfn;
+ 	unsigned long total = 0, split = 0;
+ 
+ 	if (val != 1)
+ 		return -EINVAL;
+ 
+ 	for_each_populated_zone(zone) {
+ 		max_zone_pfn = zone_end_pfn(zone);
+ 		for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++) {
+ 			if (!pfn_valid(pfn))
+ 				continue;
+ 
+ 			page = pfn_to_page(pfn);
+ 			if (!get_page_unless_zero(page))
+ 				continue;
+ 
+ 			if (zone != page_zone(page))
+ 				goto next;
+ 
+ 			if (!PageHead(page) || PageHuge(page) || !PageLRU(page))
+ 				goto next;
+ 
+ 			total++;
+ 			lock_page(page);
+ 			if (!split_huge_page(page))
+ 				split++;
+ 			unlock_page(page);
+ next:
+ 			put_page(page);
+ 		}
+ 	}
+ 
+ 	pr_info("%lu of %lu THP split\n", split, total);
+ 
+ 	return 0;
+ }
+ DEFINE_SIMPLE_ATTRIBUTE(split_huge_pages_fops, NULL, split_huge_pages_set,
+ 		"%llu\n");
+ 
+ static int __init split_huge_pages_debugfs(void)
+ {
+ 	void *ret;
+ 
+ 	ret = debugfs_create_file("split_huge_pages", 0200, NULL, NULL,
+ 			&split_huge_pages_fops);
+ 	if (!ret)
+ 		pr_warn("Failed to create split_huge_pages in debugfs");
+ 	return 0;
+ }
+ late_initcall(split_huge_pages_debugfs);
+ #endif
+ 
+ #ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+ void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,
+ 		struct page *page)
+ {
+ 	struct vm_area_struct *vma = pvmw->vma;
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned long address = pvmw->address;
+ 	pmd_t pmdval;
+ 	swp_entry_t entry;
+ 	pmd_t pmdswp;
+ 
+ 	if (!(pvmw->pmd && !pvmw->pte))
+ 		return;
+ 
+ 	mmu_notifier_invalidate_range_start(mm, address,
+ 			address + HPAGE_PMD_SIZE);
+ 
+ 	flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);
+ 	pmdval = *pvmw->pmd;
+ 	pmdp_invalidate(vma, address, pvmw->pmd);
+ 	if (pmd_dirty(pmdval))
+ 		set_page_dirty(page);
+ 	entry = make_migration_entry(page, pmd_write(pmdval));
+ 	pmdswp = swp_entry_to_pmd(entry);
+ 	if (pmd_soft_dirty(pmdval))
+ 		pmdswp = pmd_swp_mksoft_dirty(pmdswp);
+ 	set_pmd_at(mm, address, pvmw->pmd, pmdswp);
+ 	page_remove_rmap(page, true);
+ 	put_page(page);
+ 
+ 	mmu_notifier_invalidate_range_end(mm, address,
+ 			address + HPAGE_PMD_SIZE);
+ }
+ 
+ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)
+ {
+ 	struct vm_area_struct *vma = pvmw->vma;
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned long address = pvmw->address;
+ 	unsigned long mmun_start = address & HPAGE_PMD_MASK;
+ 	pmd_t pmde;
+ 	swp_entry_t entry;
+ 
+ 	if (!(pvmw->pmd && !pvmw->pte))
+ 		return;
+ 
+ 	entry = pmd_to_swp_entry(*pvmw->pmd);
+ 	get_page(new);
+ 	pmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));
+ 	if (pmd_swp_soft_dirty(*pvmw->pmd))
+ 		pmde = pmd_mksoft_dirty(pmde);
+ 	if (is_write_migration_entry(entry))
+ 		pmde = maybe_pmd_mkwrite(pmde, vma);
+ 
+ 	flush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);
+ 	page_add_anon_rmap(new, vma, mmun_start, true);
+ 	set_pmd_at(mm, mmun_start, pvmw->pmd, pmde);
+ 	if (vma->vm_flags & VM_LOCKED)
+ 		mlock_vma_page(new);
+ 	update_mmu_cache_pmd(vma, address, pvmw->pmd);
+ }
+ #endif
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
diff --cc net/core/dev.c
index 057b965c99c7,61559ca3980b..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -3722,8 -3725,8 +3722,13 @@@ bool rps_may_expire_flow(struct net_dev
  	flow_table = rcu_dereference(rxqueue->rps_flow_table);
  	if (flow_table && flow_id <= flow_table->mask) {
  		rflow = &flow_table->flows[flow_id];
++<<<<<<< HEAD
 +		cpu = ACCESS_ONCE(rflow->cpu);
 +		if (rflow->filter == filter_id && cpu != RPS_NO_CPU &&
++=======
+ 		cpu = READ_ONCE(rflow->cpu);
+ 		if (rflow->filter == filter_id && cpu < nr_cpu_ids &&
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  		    ((int)(per_cpu(softnet_data, cpu).input_queue_head -
  			   rflow->last_qtail) <
  		     (int)(10 * flow_table->mask)))
diff --cc net/ipv4/inet_fragment.c
index e5c94fababe6,f9597ba26599..000000000000
--- a/net/ipv4/inet_fragment.c
+++ b/net/ipv4/inet_fragment.c
@@@ -72,16 -93,99 +72,88 @@@ static void inet_frag_secret_rebuild(un
  
  				/* Relink to new hash chain. */
  				hb_dest = &f->hash[hval];
 -
 -				/* This is the only place where we take
 -				 * another chain_lock while already holding
 -				 * one.  As this will not run concurrently,
 -				 * we cannot deadlock on hb_dest lock below, if its
 -				 * already locked it will be released soon since
 -				 * other caller cannot be waiting for hb lock
 -				 * that we've taken above.
 -				 */
 -				spin_lock_nested(&hb_dest->chain_lock,
 -						 SINGLE_DEPTH_NESTING);
  				hlist_add_head(&q->list, &hb_dest->chain);
 -				spin_unlock(&hb_dest->chain_lock);
  			}
  		}
 -		spin_unlock(&hb->chain_lock);
  	}
 +	write_unlock(&f->lock);
  
 -	f->rebuild = false;
 -	f->last_rebuild_jiffies = jiffies;
 -out:
 -	write_sequnlock_bh(&f->rnd_seqlock);
 +	mod_timer(&f->secret_timer, now + f->secret_interval);
  }
  
++<<<<<<< HEAD
 +void inet_frags_init(struct inet_frags *f)
++=======
+ static bool inet_fragq_should_evict(const struct inet_frag_queue *q)
+ {
+ 	return q->net->low_thresh == 0 ||
+ 	       frag_mem_limit(q->net) >= q->net->low_thresh;
+ }
+ 
+ static unsigned int
+ inet_evict_bucket(struct inet_frags *f, struct inet_frag_bucket *hb)
+ {
+ 	struct inet_frag_queue *fq;
+ 	struct hlist_node *n;
+ 	unsigned int evicted = 0;
+ 	HLIST_HEAD(expired);
+ 
+ 	spin_lock(&hb->chain_lock);
+ 
+ 	hlist_for_each_entry_safe(fq, n, &hb->chain, list) {
+ 		if (!inet_fragq_should_evict(fq))
+ 			continue;
+ 
+ 		if (!del_timer(&fq->timer))
+ 			continue;
+ 
+ 		hlist_add_head(&fq->list_evictor, &expired);
+ 		++evicted;
+ 	}
+ 
+ 	spin_unlock(&hb->chain_lock);
+ 
+ 	hlist_for_each_entry_safe(fq, n, &expired, list_evictor)
+ 		f->frag_expire((unsigned long) fq);
+ 
+ 	return evicted;
+ }
+ 
+ static void inet_frag_worker(struct work_struct *work)
+ {
+ 	unsigned int budget = INETFRAGS_EVICT_BUCKETS;
+ 	unsigned int i, evicted = 0;
+ 	struct inet_frags *f;
+ 
+ 	f = container_of(work, struct inet_frags, frags_work);
+ 
+ 	BUILD_BUG_ON(INETFRAGS_EVICT_BUCKETS >= INETFRAGS_HASHSZ);
+ 
+ 	local_bh_disable();
+ 
+ 	for (i = READ_ONCE(f->next_bucket); budget; --budget) {
+ 		evicted += inet_evict_bucket(f, &f->hash[i]);
+ 		i = (i + 1) & (INETFRAGS_HASHSZ - 1);
+ 		if (evicted > INETFRAGS_EVICT_MAX)
+ 			break;
+ 	}
+ 
+ 	f->next_bucket = i;
+ 
+ 	local_bh_enable();
+ 
+ 	if (f->rebuild && inet_frag_may_rebuild(f))
+ 		inet_frag_secret_rebuild(f);
+ }
+ 
+ static void inet_frag_schedule_worker(struct inet_frags *f)
+ {
+ 	if (unlikely(!work_pending(&f->frags_work)))
+ 		schedule_work(&f->frags_work);
+ }
+ 
+ int inet_frags_init(struct inet_frags *f)
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  {
  	int i;
  
diff --cc net/ipv4/route.c
index 1f3ede4dd8fb,c0864562083b..000000000000
--- a/net/ipv4/route.c
+++ b/net/ipv4/route.c
@@@ -501,15 -493,22 +501,21 @@@ static struct ip_ident_bucket *ip_ident
   */
  u32 ip_idents_reserve(u32 hash, int segs)
  {
++<<<<<<< HEAD
 +	struct ip_ident_bucket *bucket = ip_idents + hash % IP_IDENTS_SZ;
 +	u32 old = ACCESS_ONCE(bucket->stamp32);
++=======
+ 	u32 *p_tstamp = ip_tstamps + hash % IP_IDENTS_SZ;
+ 	atomic_t *p_id = ip_idents + hash % IP_IDENTS_SZ;
+ 	u32 old = READ_ONCE(*p_tstamp);
++>>>>>>> 6aa7de059173 (locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE())
  	u32 now = (u32)jiffies;
 -	u32 new, delta = 0;
 +	u32 delta = 0;
  
 -	if (old != now && cmpxchg(p_tstamp, old, now) == old)
 +	if (old != now && cmpxchg(&bucket->stamp32, old, now) == old)
  		delta = prandom_u32_max(now - old);
  
 -	/* Do not use atomic_add_return() as it makes UBSAN unhappy */
 -	do {
 -		old = (u32)atomic_read(p_id);
 -		new = old + delta + segs;
 -	} while (atomic_cmpxchg(p_id, old, new) != old);
 -
 -	return new - segs;
 +	return atomic_add_return(segs + delta, &bucket->id) - segs;
  }
  EXPORT_SYMBOL(ip_idents_reserve);
  
* Unmerged path arch/arm/vdso/vgettimeofday.c
* Unmerged path arch/mips/kernel/pm-cps.c
* Unmerged path arch/x86/entry/common.c
* Unmerged path arch/x86/kernel/espfix_64.c
* Unmerged path block/blk-wbt.c
* Unmerged path drivers/input/misc/regulator-haptic.c
* Unmerged path drivers/misc/mic/scif/scif_rb.c
* Unmerged path drivers/misc/mic/scif/scif_rma_list.c
* Unmerged path drivers/net/ethernet/hisilicon/hip04_eth.c
* Unmerged path drivers/net/tap.c
* Unmerged path drivers/usb/gadget/udc/gr_udc.c
* Unmerged path fs/crypto/keyinfo.c
* Unmerged path net/netlabel/netlabel_calipso.c
* Unmerged path sound/firewire/amdtp-am824.c
* Unmerged path sound/firewire/amdtp-stream.c
* Unmerged path sound/firewire/amdtp-stream.h
* Unmerged path sound/firewire/digi00x/amdtp-dot.c
* Unmerged path sound/firewire/fireface/amdtp-ff.c
* Unmerged path sound/firewire/fireface/ff-midi.c
* Unmerged path sound/firewire/fireface/ff-transaction.c
* Unmerged path sound/firewire/motu/amdtp-motu.c
* Unmerged path sound/firewire/oxfw/oxfw-scs1x.c
* Unmerged path sound/firewire/tascam/amdtp-tascam.c
* Unmerged path sound/firewire/tascam/tascam-transaction.c
* Unmerged path sound/soc/xtensa/xtfpga-i2s.c
* Unmerged path arch/arc/kernel/smp.c
diff --git a/arch/arm/include/asm/spinlock.h b/arch/arm/include/asm/spinlock.h
index 6220e9fdf4c7..afe8193d611b 100644
--- a/arch/arm/include/asm/spinlock.h
+++ b/arch/arm/include/asm/spinlock.h
@@ -89,7 +89,7 @@ static inline void arch_spin_lock(arch_spinlock_t *lock)
 
 	while (lockval.tickets.next != lockval.tickets.owner) {
 		wfe();
-		lockval.tickets.owner = ACCESS_ONCE(lock->tickets.owner);
+		lockval.tickets.owner = READ_ONCE(lock->tickets.owner);
 	}
 
 	smp_mb();
diff --git a/arch/arm/mach-tegra/cpuidle-tegra20.c b/arch/arm/mach-tegra/cpuidle-tegra20.c
index 0cdba8de8c77..58f108bcd3a3 100644
--- a/arch/arm/mach-tegra/cpuidle-tegra20.c
+++ b/arch/arm/mach-tegra/cpuidle-tegra20.c
@@ -181,7 +181,7 @@ static int tegra20_idle_lp2_coupled(struct cpuidle_device *dev,
 	bool entered_lp2 = false;
 
 	if (tegra_pending_sgi())
-		ACCESS_ONCE(abort_flag) = true;
+		WRITE_ONCE(abort_flag, true);
 
 	cpuidle_coupled_parallel_barrier(dev, &abort_barrier);
 
* Unmerged path arch/arm/vdso/vgettimeofday.c
diff --git a/arch/ia64/include/asm/spinlock.h b/arch/ia64/include/asm/spinlock.h
index 54ff557d474e..2a374bf67c6c 100644
--- a/arch/ia64/include/asm/spinlock.h
+++ b/arch/ia64/include/asm/spinlock.h
@@ -59,7 +59,7 @@ static __always_inline void __ticket_spin_lock(arch_spinlock_t *lock)
 
 static __always_inline int __ticket_spin_trylock(arch_spinlock_t *lock)
 {
-	int tmp = ACCESS_ONCE(lock->lock);
+	int tmp = READ_ONCE(lock->lock);
 
 	if (!(((tmp >> TICKET_SHIFT) ^ tmp) & TICKET_MASK))
 		return ia64_cmpxchg(acq, &lock->lock, tmp, tmp + 1, sizeof (tmp)) == tmp;
@@ -71,7 +71,7 @@ static __always_inline void __ticket_spin_unlock(arch_spinlock_t *lock)
 	unsigned short	*p = (unsigned short *)&lock->lock + 1, tmp;
 
 	asm volatile ("ld2.bias %0=[%1]" : "=r"(tmp) : "r"(p));
-	ACCESS_ONCE(*p) = (tmp + 2) & ~1;
+	WRITE_ONCE(*p, (tmp + 2) & ~1);
 }
 
 static __always_inline void __ticket_spin_unlock_wait(arch_spinlock_t *lock)
@@ -90,14 +90,14 @@ static __always_inline void __ticket_spin_unlock_wait(arch_spinlock_t *lock)
 
 static inline int __ticket_spin_is_locked(arch_spinlock_t *lock)
 {
-	long tmp = ACCESS_ONCE(lock->lock);
+	long tmp = READ_ONCE(lock->lock);
 
 	return !!(((tmp >> TICKET_SHIFT) ^ tmp) & TICKET_MASK);
 }
 
 static inline int __ticket_spin_is_contended(arch_spinlock_t *lock)
 {
-	long tmp = ACCESS_ONCE(lock->lock);
+	long tmp = READ_ONCE(lock->lock);
 
 	return ((tmp - (tmp >> TICKET_SHIFT)) & TICKET_MASK) > 1;
 }
* Unmerged path arch/mips/include/asm/vdso.h
* Unmerged path arch/mips/kernel/pm-cps.c
diff --git a/arch/mn10300/kernel/mn10300-serial.c b/arch/mn10300/kernel/mn10300-serial.c
index bf6e949a2f87..afe14275163d 100644
--- a/arch/mn10300/kernel/mn10300-serial.c
+++ b/arch/mn10300/kernel/mn10300-serial.c
@@ -543,7 +543,7 @@ static void mn10300_serial_receive_interrupt(struct mn10300_serial_port *port)
 
 try_again:
 	/* pull chars out of the hat */
-	ix = ACCESS_ONCE(port->rx_outp);
+	ix = READ_ONCE(port->rx_outp);
 	if (CIRC_CNT(port->rx_inp, ix, MNSC_BUFFER_SIZE) == 0) {
 		if (push && !tport->low_latency)
 			tty_flip_buffer_push(tport);
@@ -1724,7 +1724,7 @@ static int mn10300_serial_poll_get_char(struct uart_port *_port)
 	if (mn10300_serial_int_tbl[port->rx_irq].port != NULL) {
 		do {
 			/* pull chars out of the hat */
-			ix = ACCESS_ONCE(port->rx_outp);
+			ix = READ_ONCE(port->rx_outp);
 			if (CIRC_CNT(port->rx_inp, ix, MNSC_BUFFER_SIZE) == 0)
 				return NO_POLL_CHAR;
 
diff --git a/arch/parisc/include/asm/atomic.h b/arch/parisc/include/asm/atomic.h
index f3ab59092740..4491f5e27282 100644
--- a/arch/parisc/include/asm/atomic.h
+++ b/arch/parisc/include/asm/atomic.h
@@ -179,7 +179,7 @@ atomic64_set(atomic64_t *v, s64 i)
 static __inline__ s64
 atomic64_read(const atomic64_t *v)
 {
-	return ACCESS_ONCE((v)->counter);
+	return READ_ONCE((v)->counter);
 }
 
 #define atomic64_add(i,v)	((void)(__atomic64_add_return( ((s64)(i)),(v))))
* Unmerged path arch/powerpc/platforms/powernv/opal-msglog.c
* Unmerged path arch/s390/include/asm/spinlock.h
* Unmerged path arch/s390/lib/spinlock.c
diff --git a/arch/sparc/include/asm/atomic_32.h b/arch/sparc/include/asm/atomic_32.h
index 5a59d0f5fb41..57e73b76e57c 100644
--- a/arch/sparc/include/asm/atomic_32.h
+++ b/arch/sparc/include/asm/atomic_32.h
@@ -25,7 +25,7 @@ extern int atomic_cmpxchg(atomic_t *, int, int);
 extern int __atomic_add_unless(atomic_t *, int, int);
 extern void atomic_set(atomic_t *, int);
 
-#define atomic_read(v)          ACCESS_ONCE((v)->counter)
+#define atomic_read(v)          READ_ONCE((v)->counter)
 
 #define atomic_add(i, v)	((void)__atomic_add_return( (int)(i), (v)))
 #define atomic_sub(i, v)	((void)__atomic_add_return(-(int)(i), (v)))
diff --git a/arch/tile/gxio/dma_queue.c b/arch/tile/gxio/dma_queue.c
index baa60357f8ba..b7ba577d82ca 100644
--- a/arch/tile/gxio/dma_queue.c
+++ b/arch/tile/gxio/dma_queue.c
@@ -163,14 +163,14 @@ int __gxio_dma_queue_is_complete(__gxio_dma_queue_t *dma_queue,
 				 int64_t completion_slot, int update)
 {
 	if (update) {
-		if (ACCESS_ONCE(dma_queue->hw_complete_count) >
+		if (READ_ONCE(dma_queue->hw_complete_count) >
 		    completion_slot)
 			return 1;
 
 		__gxio_dma_queue_update_credits(dma_queue);
 	}
 
-	return ACCESS_ONCE(dma_queue->hw_complete_count) > completion_slot;
+	return READ_ONCE(dma_queue->hw_complete_count) > completion_slot;
 }
 
 EXPORT_SYMBOL_GPL(__gxio_dma_queue_is_complete);
diff --git a/arch/tile/include/gxio/dma_queue.h b/arch/tile/include/gxio/dma_queue.h
index b9e45e37649e..c8fd47edba30 100644
--- a/arch/tile/include/gxio/dma_queue.h
+++ b/arch/tile/include/gxio/dma_queue.h
@@ -121,7 +121,7 @@ static inline int64_t __gxio_dma_queue_reserve(__gxio_dma_queue_t *dma_queue,
 		 * if the result is LESS than "hw_complete_count".
 		 */
 		uint64_t complete;
-		complete = ACCESS_ONCE(dma_queue->hw_complete_count);
+		complete = READ_ONCE(dma_queue->hw_complete_count);
 		slot |= (complete & 0xffffffffff000000);
 		if (slot < complete)
 			slot += 0x1000000;
* Unmerged path arch/tile/kernel/ptrace.c
* Unmerged path arch/x86/entry/common.c
* Unmerged path arch/x86/events/core.c
* Unmerged path arch/x86/include/asm/vgtod.h
* Unmerged path arch/x86/kernel/espfix_64.c
* Unmerged path arch/x86/kernel/nmi.c
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index e579c8a846b6..00d90bf131e7 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -441,7 +441,7 @@ static u64 __update_clear_spte_slow(u64 *sptep, u64 spte)
 
 static u64 __get_spte_lockless(u64 *sptep)
 {
-	return ACCESS_ONCE(*sptep);
+	return READ_ONCE(*sptep);
 }
 #else
 union split_spte {
@@ -4759,7 +4759,7 @@ static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 	 * If we don't have indirect shadow pages, it means no page is
 	 * write-protected, so we can exit simply.
 	 */
-	if (!ACCESS_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
+	if (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))
 		return;
 
 	zap_page = remote_flush = local_flush = false;
diff --git a/arch/x86/kvm/page_track.c b/arch/x86/kvm/page_track.c
index 088bfbd69ce6..e6c025e9b000 100644
--- a/arch/x86/kvm/page_track.c
+++ b/arch/x86/kvm/page_track.c
@@ -155,7 +155,7 @@ bool kvm_page_track_is_active(struct kvm_vcpu *vcpu, gfn_t gfn,
 		return false;
 
 	index = gfn_to_index(gfn, slot->base_gfn, PT_PAGE_TABLE_LEVEL);
-	return !!ACCESS_ONCE(slot->arch.gfn_track[mode][index]);
+	return !!READ_ONCE(slot->arch.gfn_track[mode][index]);
 }
 
 void kvm_page_track_cleanup(struct kvm *kvm)
* Unmerged path arch/x86/vdso/vclock_gettime.c
* Unmerged path arch/x86/xen/p2m.c
* Unmerged path arch/xtensa/platforms/xtfpga/lcd.c
* Unmerged path block/blk-wbt.c
diff --git a/drivers/base/core.c b/drivers/base/core.c
index 607c1beb7454..472082b52493 100644
--- a/drivers/base/core.c
+++ b/drivers/base/core.c
@@ -106,7 +106,7 @@ const char *dev_driver_string(const struct device *dev)
 	 * so be careful about accessing it.  dev->bus and dev->class should
 	 * never change once they are set, so they don't need special care.
 	 */
-	drv = ACCESS_ONCE(dev->driver);
+	drv = READ_ONCE(dev->driver);
 	return drv ? drv->name :
 			(dev->bus ? dev->bus->name :
 			(dev->class ? dev->class->name : ""));
diff --git a/drivers/base/power/runtime.c b/drivers/base/power/runtime.c
index 5c408e356681..295d820d97e0 100644
--- a/drivers/base/power/runtime.c
+++ b/drivers/base/power/runtime.c
@@ -136,11 +136,11 @@ unsigned long pm_runtime_autosuspend_expiration(struct device *dev)
 	if (!dev->power.use_autosuspend)
 		goto out;
 
-	autosuspend_delay = ACCESS_ONCE(dev->power.autosuspend_delay);
+	autosuspend_delay = READ_ONCE(dev->power.autosuspend_delay);
 	if (autosuspend_delay < 0)
 		goto out;
 
-	last_busy = ACCESS_ONCE(dev->power.last_busy);
+	last_busy = READ_ONCE(dev->power.last_busy);
 	elapsed = jiffies - last_busy;
 	if (elapsed < 0)
 		goto out;	/* jiffies has wrapped around. */
diff --git a/drivers/char/random.c b/drivers/char/random.c
index 00290d078cbe..4855be7e8f2a 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -652,7 +652,7 @@ static void credit_entropy_bits(struct entropy_store *r, int nbits)
 		return;
 
 retry:
-	entropy_count = orig = ACCESS_ONCE(r->entropy_count);
+	entropy_count = orig = READ_ONCE(r->entropy_count);
 	if (nfrac < 0) {
 		/* Debit */
 		entropy_count += nfrac;
@@ -1288,7 +1288,7 @@ static size_t account(struct entropy_store *r, size_t nbytes, int min,
 
 	/* Can we pull enough? */
 retry:
-	entropy_count = orig = ACCESS_ONCE(r->entropy_count);
+	entropy_count = orig = READ_ONCE(r->entropy_count);
 	ibytes = nbytes;
 	/* If limited, never pull more than available */
 	if (r->limit) {
diff --git a/drivers/clocksource/bcm2835_timer.c b/drivers/clocksource/bcm2835_timer.c
index 766611d29945..77541a9c31bd 100644
--- a/drivers/clocksource/bcm2835_timer.c
+++ b/drivers/clocksource/bcm2835_timer.c
@@ -86,7 +86,7 @@ static irqreturn_t bcm2835_time_interrupt(int irq, void *dev_id)
 	if (readl_relaxed(timer->control) & timer->match_mask) {
 		writel_relaxed(timer->match_mask, timer->control);
 
-		event_handler = ACCESS_ONCE(timer->evt.event_handler);
+		event_handler = READ_ONCE(timer->evt.event_handler);
 		if (event_handler)
 			event_handler(&timer->evt);
 		return IRQ_HANDLED;
diff --git a/drivers/crypto/caam/jr.c b/drivers/crypto/caam/jr.c
index b4aa773ecbc8..79bdf0afffea 100644
--- a/drivers/crypto/caam/jr.c
+++ b/drivers/crypto/caam/jr.c
@@ -61,7 +61,7 @@ static void caam_jr_dequeue(unsigned long devarg)
 
 	while (rd_reg32(&jrp->rregs->outring_used)) {
 
-		head = ACCESS_ONCE(jrp->head);
+		head = READ_ONCE(jrp->head);
 
 		spin_lock(&jrp->outlock);
 
@@ -239,7 +239,7 @@ int caam_jr_enqueue(struct device *dev, u32 *desc,
 	spin_lock_bh(&jrp->inplock);
 
 	head = jrp->head;
-	tail = ACCESS_ONCE(jrp->tail);
+	tail = READ_ONCE(jrp->tail);
 
 	if (!rd_reg32(&jrp->rregs->inpring_avail) ||
 	    CIRC_SPACE(head, tail, JOBR_DEPTH) <= 0) {
diff --git a/drivers/crypto/nx/nx-842-powernv.c b/drivers/crypto/nx/nx-842-powernv.c
index 2f73b9a55949..7fa04a947822 100644
--- a/drivers/crypto/nx/nx-842-powernv.c
+++ b/drivers/crypto/nx/nx-842-powernv.c
@@ -173,7 +173,7 @@ static int wait_for_csb(struct nx842_workmem *wmem,
 	ktime_t start = wmem->start, now = ktime_get();
 	ktime_t timeout = ktime_add_ms(start, CSB_WAIT_MAX);
 
-	while (!(ACCESS_ONCE(csb->flags) & CSB_V)) {
+	while (!(READ_ONCE(csb->flags) & CSB_V)) {
 		cpu_relax();
 		now = ktime_get();
 		if (ktime_after(now, timeout))
diff --git a/drivers/firewire/ohci.c b/drivers/firewire/ohci.c
index 0f3e3047e29c..77b6df699df4 100644
--- a/drivers/firewire/ohci.c
+++ b/drivers/firewire/ohci.c
@@ -727,7 +727,7 @@ static unsigned int ar_search_last_active_buffer(struct ar_context *ctx,
 	__le16 res_count, next_res_count;
 
 	i = ar_first_buffer_index(ctx);
-	res_count = ACCESS_ONCE(ctx->descriptors[i].res_count);
+	res_count = READ_ONCE(ctx->descriptors[i].res_count);
 
 	/* A buffer that is not yet completely filled must be the last one. */
 	while (i != last && res_count == 0) {
@@ -735,8 +735,7 @@ static unsigned int ar_search_last_active_buffer(struct ar_context *ctx,
 		/* Peek at the next descriptor. */
 		next_i = ar_next_buffer_index(i);
 		rmb(); /* read descriptors in order */
-		next_res_count = ACCESS_ONCE(
-				ctx->descriptors[next_i].res_count);
+		next_res_count = READ_ONCE(ctx->descriptors[next_i].res_count);
 		/*
 		 * If the next descriptor is still empty, we must stop at this
 		 * descriptor.
@@ -752,8 +751,7 @@ static unsigned int ar_search_last_active_buffer(struct ar_context *ctx,
 			if (MAX_AR_PACKET_SIZE > PAGE_SIZE && i != last) {
 				next_i = ar_next_buffer_index(next_i);
 				rmb();
-				next_res_count = ACCESS_ONCE(
-					ctx->descriptors[next_i].res_count);
+				next_res_count = READ_ONCE(ctx->descriptors[next_i].res_count);
 				if (next_res_count != cpu_to_le16(PAGE_SIZE))
 					goto next_buffer_is_active;
 			}
@@ -2806,7 +2804,7 @@ static int handle_ir_buffer_fill(struct context *context,
 	u32 buffer_dma;
 
 	req_count = le16_to_cpu(last->req_count);
-	res_count = le16_to_cpu(ACCESS_ONCE(last->res_count));
+	res_count = le16_to_cpu(READ_ONCE(last->res_count));
 	completed = req_count - res_count;
 	buffer_dma = le32_to_cpu(last->data_address);
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
index 333bad749067..303b5e099a98 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c
@@ -260,7 +260,7 @@ static void amdgpu_fence_fallback(unsigned long arg)
  */
 int amdgpu_fence_wait_empty(struct amdgpu_ring *ring)
 {
-	uint64_t seq = ACCESS_ONCE(ring->fence_drv.sync_seq);
+	uint64_t seq = READ_ONCE(ring->fence_drv.sync_seq);
 	struct dma_fence *fence, **ptr;
 	int r;
 
@@ -300,7 +300,7 @@ unsigned amdgpu_fence_count_emitted(struct amdgpu_ring *ring)
 	amdgpu_fence_process(ring);
 	emitted = 0x100000000ull;
 	emitted -= atomic_read(&ring->fence_drv.last_seq);
-	emitted += ACCESS_ONCE(ring->fence_drv.sync_seq);
+	emitted += READ_ONCE(ring->fence_drv.sync_seq);
 	return lower_32_bits(emitted);
 }
 
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
index 7171968f261e..6149a47fe63d 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
@@ -788,11 +788,11 @@ static int amdgpu_debugfs_gem_bo_info(int id, void *ptr, void *data)
 	seq_printf(m, "\t0x%08x: %12ld byte %s",
 		   id, amdgpu_bo_size(bo), placement);
 
-	offset = ACCESS_ONCE(bo->tbo.mem.start);
+	offset = READ_ONCE(bo->tbo.mem.start);
 	if (offset != AMDGPU_BO_INVALID_OFFSET)
 		seq_printf(m, " @ 0x%010Lx", offset);
 
-	pin_count = ACCESS_ONCE(bo->pin_count);
+	pin_count = READ_ONCE(bo->pin_count);
 	if (pin_count)
 		seq_printf(m, " pin count %d", pin_count);
 	seq_printf(m, "\n");
diff --git a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
index 38cea6fb25a8..a25f6c72f219 100644
--- a/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
+++ b/drivers/gpu/drm/amd/scheduler/gpu_scheduler.c
@@ -187,7 +187,7 @@ static bool amd_sched_entity_is_ready(struct amd_sched_entity *entity)
 	if (kfifo_is_empty(&entity->job_queue))
 		return false;
 
-	if (ACCESS_ONCE(entity->dependency))
+	if (READ_ONCE(entity->dependency))
 		return false;
 
 	return true;
diff --git a/drivers/gpu/drm/radeon/radeon_gem.c b/drivers/gpu/drm/radeon/radeon_gem.c
index 3386452bd2f0..cf3deb283da5 100644
--- a/drivers/gpu/drm/radeon/radeon_gem.c
+++ b/drivers/gpu/drm/radeon/radeon_gem.c
@@ -451,7 +451,7 @@ int radeon_gem_busy_ioctl(struct drm_device *dev, void *data,
 	else
 		r = 0;
 
-	cur_placement = ACCESS_ONCE(robj->tbo.mem.mem_type);
+	cur_placement = READ_ONCE(robj->tbo.mem.mem_type);
 	args->domain = radeon_mem_type_to_domain(cur_placement);
 	drm_gem_object_put_unlocked(gobj);
 	return r;
@@ -481,7 +481,7 @@ int radeon_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
 		r = ret;
 
 	/* Flush HDP cache via MMIO if necessary */
-	cur_placement = ACCESS_ONCE(robj->tbo.mem.mem_type);
+	cur_placement = READ_ONCE(robj->tbo.mem.mem_type);
 	if (rdev->asic->mmio_hdp_flush &&
 	    radeon_mem_type_to_domain(cur_placement) == RADEON_GEM_DOMAIN_VRAM)
 		robj->rdev->asic->mmio_hdp_flush(rdev);
diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c b/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c
index a552e4ea5440..6ac094ee8983 100644
--- a/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c
+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c
@@ -904,7 +904,7 @@ vmw_surface_handle_reference(struct vmw_private *dev_priv,
 		if (unlikely(drm_is_render_client(file_priv)))
 			require_exist = true;
 
-		if (ACCESS_ONCE(vmw_fpriv(file_priv)->locked_master)) {
+		if (READ_ONCE(vmw_fpriv(file_priv)->locked_master)) {
 			DRM_ERROR("Locked master refused legacy "
 				  "surface reference.\n");
 			return -EACCES;
diff --git a/drivers/infiniband/hw/hfi1/file_ops.c b/drivers/infiniband/hw/hfi1/file_ops.c
index bff95cdc5713..ae688415b5d3 100644
--- a/drivers/infiniband/hw/hfi1/file_ops.c
+++ b/drivers/infiniband/hw/hfi1/file_ops.c
@@ -331,7 +331,7 @@ static long hfi1_file_ioctl(struct file *fp, unsigned int cmd,
 		if (sc->flags & SCF_FROZEN) {
 			wait_event_interruptible_timeout(
 				dd->event_queue,
-				!(ACCESS_ONCE(dd->flags) & HFI1_FROZEN),
+				!(READ_ONCE(dd->flags) & HFI1_FROZEN),
 				msecs_to_jiffies(SEND_CTXT_HALT_TIMEOUT));
 			if (dd->flags & HFI1_FROZEN)
 				return -ENOLCK;
diff --git a/drivers/infiniband/hw/hfi1/pio.c b/drivers/infiniband/hw/hfi1/pio.c
index 51046d444ac6..40dac4d16eb8 100644
--- a/drivers/infiniband/hw/hfi1/pio.c
+++ b/drivers/infiniband/hw/hfi1/pio.c
@@ -1407,14 +1407,14 @@ retry:
 			goto done;
 		}
 		/* copy from receiver cache line and recalculate */
-		sc->alloc_free = ACCESS_ONCE(sc->free);
+		sc->alloc_free = READ_ONCE(sc->free);
 		avail =
 			(unsigned long)sc->credits -
 			(sc->fill - sc->alloc_free);
 		if (blocks > avail) {
 			/* still no room, actively update */
 			sc_release_update(sc);
-			sc->alloc_free = ACCESS_ONCE(sc->free);
+			sc->alloc_free = READ_ONCE(sc->free);
 			trycount++;
 			goto retry;
 		}
@@ -1651,7 +1651,7 @@ void sc_release_update(struct send_context *sc)
 
 	/* call sent buffer callbacks */
 	code = -1;				/* code not yet set */
-	head = ACCESS_ONCE(sc->sr_head);	/* snapshot the head */
+	head = READ_ONCE(sc->sr_head);	/* snapshot the head */
 	tail = sc->sr_tail;
 	while (head != tail) {
 		pbuf = &sc->sr[tail].pbuf;
diff --git a/drivers/infiniband/hw/hfi1/ruc.c b/drivers/infiniband/hw/hfi1/ruc.c
index 145747564a15..99707a76ac42 100644
--- a/drivers/infiniband/hw/hfi1/ruc.c
+++ b/drivers/infiniband/hw/hfi1/ruc.c
@@ -352,7 +352,7 @@ static void ruc_loopback(struct rvt_qp *sqp)
 
 again:
 	smp_read_barrier_depends(); /* see post_one_send() */
-	if (sqp->s_last == ACCESS_ONCE(sqp->s_head))
+	if (sqp->s_last == READ_ONCE(sqp->s_head))
 		goto clr_busy;
 	wqe = rvt_get_swqe_ptr(sqp, sqp->s_last);
 
diff --git a/drivers/infiniband/hw/hfi1/sdma.c b/drivers/infiniband/hw/hfi1/sdma.c
index d13982956bf8..1b44489decce 100644
--- a/drivers/infiniband/hw/hfi1/sdma.c
+++ b/drivers/infiniband/hw/hfi1/sdma.c
@@ -1722,7 +1722,7 @@ retry:
 
 		swhead = sde->descq_head & sde->sdma_mask;
 		/* this code is really bad for cache line trading */
-		swtail = ACCESS_ONCE(sde->descq_tail) & sde->sdma_mask;
+		swtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;
 		cnt = sde->descq_cnt;
 
 		if (swhead < swtail)
@@ -1869,7 +1869,7 @@ retry:
 	if ((status & sde->idle_mask) && !idle_check_done) {
 		u16 swtail;
 
-		swtail = ACCESS_ONCE(sde->descq_tail) & sde->sdma_mask;
+		swtail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;
 		if (swtail != hwhead) {
 			hwhead = (u16)read_sde_csr(sde, SD(HEAD));
 			idle_check_done = 1;
@@ -2217,7 +2217,7 @@ void sdma_seqfile_dump_sde(struct seq_file *s, struct sdma_engine *sde)
 	u16 len;
 
 	head = sde->descq_head & sde->sdma_mask;
-	tail = ACCESS_ONCE(sde->descq_tail) & sde->sdma_mask;
+	tail = READ_ONCE(sde->descq_tail) & sde->sdma_mask;
 	seq_printf(s, SDE_FMT, sde->this_idx,
 		   sde->cpu,
 		   sdma_state_name(sde->state.current_state),
@@ -3301,7 +3301,7 @@ int sdma_ahg_alloc(struct sdma_engine *sde)
 		return -EINVAL;
 	}
 	while (1) {
-		nr = ffz(ACCESS_ONCE(sde->ahg_bits));
+		nr = ffz(READ_ONCE(sde->ahg_bits));
 		if (nr > 31) {
 			trace_hfi1_ahg_allocate(sde, -ENOSPC);
 			return -ENOSPC;
diff --git a/drivers/infiniband/hw/hfi1/sdma.h b/drivers/infiniband/hw/hfi1/sdma.h
index 52bb70a40cac..46c775f255d1 100644
--- a/drivers/infiniband/hw/hfi1/sdma.h
+++ b/drivers/infiniband/hw/hfi1/sdma.h
@@ -446,7 +446,7 @@ static inline u16 sdma_descq_freecnt(struct sdma_engine *sde)
 {
 	return sde->descq_cnt -
 		(sde->descq_tail -
-		 ACCESS_ONCE(sde->descq_head)) - 1;
+		 READ_ONCE(sde->descq_head)) - 1;
 }
 
 static inline u16 sdma_descq_inprocess(struct sdma_engine *sde)
diff --git a/drivers/infiniband/hw/hfi1/uc.c b/drivers/infiniband/hw/hfi1/uc.c
index fdad430a56dc..8030c38a27f2 100644
--- a/drivers/infiniband/hw/hfi1/uc.c
+++ b/drivers/infiniband/hw/hfi1/uc.c
@@ -80,7 +80,7 @@ int hfi1_make_uc_req(struct rvt_qp *qp, struct hfi1_pkt_state *ps)
 			goto bail;
 		/* We are in the error state, flush the work request. */
 		smp_read_barrier_depends(); /* see post_one_send() */
-		if (qp->s_last == ACCESS_ONCE(qp->s_head))
+		if (qp->s_last == READ_ONCE(qp->s_head))
 			goto bail;
 		/* If DMAs are in progress, we can't flush immediately. */
 		if (iowait_sdma_pending(&priv->s_iowait)) {
@@ -120,7 +120,7 @@ int hfi1_make_uc_req(struct rvt_qp *qp, struct hfi1_pkt_state *ps)
 			goto bail;
 		/* Check if send work queue is empty. */
 		smp_read_barrier_depends(); /* see post_one_send() */
-		if (qp->s_cur == ACCESS_ONCE(qp->s_head)) {
+		if (qp->s_cur == READ_ONCE(qp->s_head)) {
 			clear_ahg(qp);
 			goto bail;
 		}
diff --git a/drivers/infiniband/hw/hfi1/ud.c b/drivers/infiniband/hw/hfi1/ud.c
index d8edc86b24eb..2582647d4a3d 100644
--- a/drivers/infiniband/hw/hfi1/ud.c
+++ b/drivers/infiniband/hw/hfi1/ud.c
@@ -490,7 +490,7 @@ int hfi1_make_ud_req(struct rvt_qp *qp, struct hfi1_pkt_state *ps)
 			goto bail;
 		/* We are in the error state, flush the work request. */
 		smp_read_barrier_depends(); /* see post_one_send */
-		if (qp->s_last == ACCESS_ONCE(qp->s_head))
+		if (qp->s_last == READ_ONCE(qp->s_head))
 			goto bail;
 		/* If DMAs are in progress, we can't flush immediately. */
 		if (iowait_sdma_pending(&priv->s_iowait)) {
@@ -504,7 +504,7 @@ int hfi1_make_ud_req(struct rvt_qp *qp, struct hfi1_pkt_state *ps)
 
 	/* see post_one_send() */
 	smp_read_barrier_depends();
-	if (qp->s_cur == ACCESS_ONCE(qp->s_head))
+	if (qp->s_cur == READ_ONCE(qp->s_head))
 		goto bail;
 
 	wqe = rvt_get_swqe_ptr(qp, qp->s_cur);
diff --git a/drivers/infiniband/hw/hfi1/user_sdma.c b/drivers/infiniband/hw/hfi1/user_sdma.c
index 833d0b7277bc..a3a7b33196d6 100644
--- a/drivers/infiniband/hw/hfi1/user_sdma.c
+++ b/drivers/infiniband/hw/hfi1/user_sdma.c
@@ -276,7 +276,7 @@ int hfi1_user_sdma_free_queues(struct hfi1_filedata *fd,
 		/* Wait until all requests have been freed. */
 		wait_event_interruptible(
 			pq->wait,
-			(ACCESS_ONCE(pq->state) == SDMA_PKT_Q_INACTIVE));
+			(READ_ONCE(pq->state) == SDMA_PKT_Q_INACTIVE));
 		kfree(pq->reqs);
 		kfree(pq->req_in_use);
 		kmem_cache_destroy(pq->txreq_cache);
@@ -591,7 +591,7 @@ int hfi1_user_sdma_process_request(struct hfi1_filedata *fd,
 			if (ret != -EBUSY) {
 				req->status = ret;
 				WRITE_ONCE(req->has_error, 1);
-				if (ACCESS_ONCE(req->seqcomp) ==
+				if (READ_ONCE(req->seqcomp) ==
 				    req->seqsubmitted - 1)
 					goto free_req;
 				return ret;
@@ -825,7 +825,7 @@ static int user_sdma_send_pkts(struct user_sdma_request *req, unsigned maxpkts)
 		 */
 		if (req->data_len) {
 			iovec = &req->iovs[req->iov_idx];
-			if (ACCESS_ONCE(iovec->offset) == iovec->iov.iov_len) {
+			if (READ_ONCE(iovec->offset) == iovec->iov.iov_len) {
 				if (++req->iov_idx == req->data_iovs) {
 					ret = -EFAULT;
 					goto free_txreq;
@@ -1408,7 +1408,7 @@ static void user_sdma_txreq_cb(struct sdma_txreq *txreq, int status)
 	} else {
 		if (status != SDMA_TXREQ_S_OK)
 			req->status = status;
-		if (req->seqcomp == (ACCESS_ONCE(req->seqsubmitted) - 1) &&
+		if (req->seqcomp == (READ_ONCE(req->seqsubmitted) - 1) &&
 		    (READ_ONCE(req->done) ||
 		     READ_ONCE(req->has_error))) {
 			user_sdma_free_request(req, false);
diff --git a/drivers/infiniband/hw/qib/qib_ruc.c b/drivers/infiniband/hw/qib/qib_ruc.c
index 53efbb0b40c4..9a37e844d4c8 100644
--- a/drivers/infiniband/hw/qib/qib_ruc.c
+++ b/drivers/infiniband/hw/qib/qib_ruc.c
@@ -368,7 +368,7 @@ static void qib_ruc_loopback(struct rvt_qp *sqp)
 
 again:
 	smp_read_barrier_depends(); /* see post_one_send() */
-	if (sqp->s_last == ACCESS_ONCE(sqp->s_head))
+	if (sqp->s_last == READ_ONCE(sqp->s_head))
 		goto clr_busy;
 	wqe = rvt_get_swqe_ptr(sqp, sqp->s_last);
 
diff --git a/drivers/infiniband/hw/qib/qib_uc.c b/drivers/infiniband/hw/qib/qib_uc.c
index 518c45e65627..d311db6487cf 100644
--- a/drivers/infiniband/hw/qib/qib_uc.c
+++ b/drivers/infiniband/hw/qib/qib_uc.c
@@ -61,7 +61,7 @@ int qib_make_uc_req(struct rvt_qp *qp, unsigned long *flags)
 			goto bail;
 		/* We are in the error state, flush the work request. */
 		smp_read_barrier_depends(); /* see post_one_send() */
-		if (qp->s_last == ACCESS_ONCE(qp->s_head))
+		if (qp->s_last == READ_ONCE(qp->s_head))
 			goto bail;
 		/* If DMAs are in progress, we can't flush immediately. */
 		if (atomic_read(&priv->s_dma_busy)) {
@@ -91,7 +91,7 @@ int qib_make_uc_req(struct rvt_qp *qp, unsigned long *flags)
 			goto bail;
 		/* Check if send work queue is empty. */
 		smp_read_barrier_depends(); /* see post_one_send() */
-		if (qp->s_cur == ACCESS_ONCE(qp->s_head))
+		if (qp->s_cur == READ_ONCE(qp->s_head))
 			goto bail;
 		/*
 		 * Start a new request.
diff --git a/drivers/infiniband/hw/qib/qib_ud.c b/drivers/infiniband/hw/qib/qib_ud.c
index 210ba1c8a76f..59a4f0333803 100644
--- a/drivers/infiniband/hw/qib/qib_ud.c
+++ b/drivers/infiniband/hw/qib/qib_ud.c
@@ -253,7 +253,7 @@ int qib_make_ud_req(struct rvt_qp *qp, unsigned long *flags)
 			goto bail;
 		/* We are in the error state, flush the work request. */
 		smp_read_barrier_depends(); /* see post_one_send */
-		if (qp->s_last == ACCESS_ONCE(qp->s_head))
+		if (qp->s_last == READ_ONCE(qp->s_head))
 			goto bail;
 		/* If DMAs are in progress, we can't flush immediately. */
 		if (atomic_read(&priv->s_dma_busy)) {
@@ -267,7 +267,7 @@ int qib_make_ud_req(struct rvt_qp *qp, unsigned long *flags)
 
 	/* see post_one_send() */
 	smp_read_barrier_depends();
-	if (qp->s_cur == ACCESS_ONCE(qp->s_head))
+	if (qp->s_cur == READ_ONCE(qp->s_head))
 		goto bail;
 
 	wqe = rvt_get_swqe_ptr(qp, qp->s_cur);
diff --git a/drivers/infiniband/sw/rdmavt/qp.c b/drivers/infiniband/sw/rdmavt/qp.c
index ed468011fd7f..75305ace49d6 100644
--- a/drivers/infiniband/sw/rdmavt/qp.c
+++ b/drivers/infiniband/sw/rdmavt/qp.c
@@ -1071,7 +1071,7 @@ int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err)
 	rdi->driver_f.notify_error_qp(qp);
 
 	/* Schedule the sending tasklet to drain the send work queue. */
-	if (ACCESS_ONCE(qp->s_last) != qp->s_head)
+	if (READ_ONCE(qp->s_last) != qp->s_head)
 		rdi->driver_f.schedule_send(qp);
 
 	rvt_clear_mr_refs(qp, 0);
@@ -1685,7 +1685,7 @@ static inline int rvt_qp_is_avail(
 	if (likely(qp->s_avail))
 		return 0;
 	smp_read_barrier_depends(); /* see rc.c */
-	slast = ACCESS_ONCE(qp->s_last);
+	slast = READ_ONCE(qp->s_last);
 	if (qp->s_head >= slast)
 		avail = qp->s_size - (qp->s_head - slast);
 	else
@@ -1916,7 +1916,7 @@ int rvt_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 	 * ahead and kick the send engine into gear. Otherwise we will always
 	 * just schedule the send to happen later.
 	 */
-	call_send = qp->s_head == ACCESS_ONCE(qp->s_last) && !wr->next;
+	call_send = qp->s_head == READ_ONCE(qp->s_last) && !wr->next;
 
 	for (; wr; wr = wr->next) {
 		err = rvt_post_one_wr(qp, wr, &call_send);
* Unmerged path drivers/input/misc/regulator-haptic.c
* Unmerged path drivers/md/dm-bufio.c
diff --git a/drivers/md/dm-kcopyd.c b/drivers/md/dm-kcopyd.c
index f77767312f4e..a9f8856cb60b 100644
--- a/drivers/md/dm-kcopyd.c
+++ b/drivers/md/dm-kcopyd.c
@@ -107,7 +107,7 @@ static void io_job_start(struct dm_kcopyd_throttle *t)
 try_again:
 	spin_lock_irq(&throttle_spinlock);
 
-	throttle = ACCESS_ONCE(t->throttle);
+	throttle = READ_ONCE(t->throttle);
 
 	if (likely(throttle >= 100))
 		goto skip_limit;
@@ -157,7 +157,7 @@ static void io_job_finish(struct dm_kcopyd_throttle *t)
 
 	t->num_io_jobs--;
 
-	if (likely(ACCESS_ONCE(t->throttle) >= 100))
+	if (likely(READ_ONCE(t->throttle) >= 100))
 		goto skip_limit;
 
 	if (!t->num_io_jobs) {
* Unmerged path drivers/md/dm-stats.c
diff --git a/drivers/md/dm-switch.c b/drivers/md/dm-switch.c
index be7dda9e4e22..4cf5b5449e50 100644
--- a/drivers/md/dm-switch.c
+++ b/drivers/md/dm-switch.c
@@ -144,7 +144,7 @@ static unsigned switch_region_table_read(struct switch_ctx *sctx, unsigned long
 
 	switch_get_position(sctx, region_nr, &region_index, &bit);
 
-	return (ACCESS_ONCE(sctx->region_table[region_index]) >> bit) &
+	return (READ_ONCE(sctx->region_table[region_index]) >> bit) &
 		((1 << sctx->region_table_entry_bits) - 1);
 }
 
diff --git a/drivers/md/dm-thin.c b/drivers/md/dm-thin.c
index 3fad13a0994a..f96407ad17d0 100644
--- a/drivers/md/dm-thin.c
+++ b/drivers/md/dm-thin.c
@@ -2529,7 +2529,7 @@ static void set_pool_mode(struct pool *pool, enum pool_mode new_mode)
 	struct pool_c *pt = pool->ti->private;
 	bool needs_check = dm_pool_metadata_needs_check(pool->pmd);
 	enum pool_mode old_mode = get_pool_mode(pool);
-	unsigned long no_space_timeout = ACCESS_ONCE(no_space_timeout_secs) * HZ;
+	unsigned long no_space_timeout = READ_ONCE(no_space_timeout_secs) * HZ;
 
 	/*
 	 * Never allow the pool to transition to PM_WRITE mode if user
diff --git a/drivers/md/dm-verity-target.c b/drivers/md/dm-verity-target.c
index 0bdd44a761ab..074381d92457 100644
--- a/drivers/md/dm-verity-target.c
+++ b/drivers/md/dm-verity-target.c
@@ -455,7 +455,7 @@ static void verity_prefetch_io(struct work_struct *work)
 		verity_hash_at_level(v, pw->block, i, &hash_block_start, NULL);
 		verity_hash_at_level(v, pw->block + pw->n_blocks - 1, i, &hash_block_end, NULL);
 		if (!i) {
-			unsigned cluster = ACCESS_ONCE(dm_verity_prefetch_cluster);
+			unsigned cluster = READ_ONCE(dm_verity_prefetch_cluster);
 
 			cluster >>= v->data_dev_block_bits;
 			if (unlikely(!cluster))
diff --git a/drivers/md/dm.c b/drivers/md/dm.c
index 14d7215727e9..abb71d3610a0 100644
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -114,7 +114,7 @@ static unsigned reserved_bio_based_ios = RESERVED_BIO_BASED_IOS;
 
 static int __dm_get_module_param_int(int *module_param, int min, int max)
 {
-	int param = ACCESS_ONCE(*module_param);
+	int param = READ_ONCE(*module_param);
 	int modified_param = 0;
 	bool modified = true;
 
@@ -136,7 +136,7 @@ static int __dm_get_module_param_int(int *module_param, int min, int max)
 unsigned __dm_get_module_param(unsigned *module_param,
 			       unsigned def, unsigned max)
 {
-	unsigned param = ACCESS_ONCE(*module_param);
+	unsigned param = READ_ONCE(*module_param);
 	unsigned modified_param = 0;
 
 	if (!param)
diff --git a/drivers/md/md.c b/drivers/md/md.c
index 9bf51228c85b..8d860dfff26a 100644
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@ -2611,7 +2611,7 @@ state_show(struct md_rdev *rdev, char *page)
 {
 	char *sep = ",";
 	size_t len = 0;
-	unsigned long flags = ACCESS_ONCE(rdev->flags);
+	unsigned long flags = READ_ONCE(rdev->flags);
 
 	if (test_bit(Faulty, &flags) ||
 	    (!test_bit(ExternalBbl, &flags) &&
diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index 5c35f05eceef..66aca7cc67f6 100644
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@ -5857,7 +5857,7 @@ static inline sector_t raid5_sync_request(struct mddev *mddev, sector_t sector_n
 	 */
 	rcu_read_lock();
 	for (i = 0; i < conf->raid_disks; i++) {
-		struct md_rdev *rdev = ACCESS_ONCE(conf->disks[i].rdev);
+		struct md_rdev *rdev = READ_ONCE(conf->disks[i].rdev);
 
 		if (rdev == NULL || test_bit(Faulty, &rdev->flags))
 			still_degraded = 1;
* Unmerged path drivers/misc/mic/scif/scif_rb.c
* Unmerged path drivers/misc/mic/scif/scif_rma_list.c
diff --git a/drivers/net/bonding/bond_alb.c b/drivers/net/bonding/bond_alb.c
index 7d50d66d81b0..6c4d8a229df7 100644
--- a/drivers/net/bonding/bond_alb.c
+++ b/drivers/net/bonding/bond_alb.c
@@ -1333,7 +1333,7 @@ int bond_tlb_xmit(struct sk_buff *skb, struct net_device *bond_dev)
 				unsigned int count;
 
 				slaves = rcu_dereference(bond->slave_arr);
-				count = slaves ? ACCESS_ONCE(slaves->count) : 0;
+				count = slaves ? READ_ONCE(slaves->count) : 0;
 				if (likely(count))
 					tx_slave = slaves->arr[hash_index %
 							       count];
diff --git a/drivers/net/bonding/bond_main.c b/drivers/net/bonding/bond_main.c
index 75fe2902ab33..874b0a6574e8 100644
--- a/drivers/net/bonding/bond_main.c
+++ b/drivers/net/bonding/bond_main.c
@@ -1167,7 +1167,7 @@ static rx_handler_result_t bond_handle_frame(struct sk_buff **pskb)
 	slave = bond_slave_get_rcu(skb->dev);
 	bond = slave->bond;
 
-	recv_probe = ACCESS_ONCE(bond->recv_probe);
+	recv_probe = READ_ONCE(bond->recv_probe);
 	if (recv_probe) {
 		ret = recv_probe(skb, bond, slave);
 		if (ret == RX_HANDLER_CONSUMED) {
@@ -3822,7 +3822,7 @@ static int bond_xmit_roundrobin(struct sk_buff *skb, struct net_device *bond_dev
 		else
 			bond_xmit_slave_id(bond, skb, 0);
 	} else {
-		int slave_cnt = ACCESS_ONCE(bond->slave_cnt);
+		int slave_cnt = READ_ONCE(bond->slave_cnt);
 
 		if (likely(slave_cnt)) {
 			slave_id = bond_rr_gen_slave_id(bond);
@@ -3984,7 +3984,7 @@ static int bond_3ad_xor_xmit(struct sk_buff *skb, struct net_device *dev)
 	unsigned int count;
 
 	slaves = rcu_dereference(bond->slave_arr);
-	count = slaves ? ACCESS_ONCE(slaves->count) : 0;
+	count = slaves ? READ_ONCE(slaves->count) : 0;
 	if (likely(count)) {
 		slave = slaves->arr[bond_xmit_hash(bond, skb) % count];
 		bond_dev_queue_xmit(bond, skb, slave->dev);
diff --git a/drivers/net/ethernet/chelsio/cxgb4/sge.c b/drivers/net/ethernet/chelsio/cxgb4/sge.c
index a64f3863f5c9..d150e0843167 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@ -402,7 +402,7 @@ void free_tx_desc(struct adapter *adap, struct sge_txq *q,
  */
 static inline int reclaimable(const struct sge_txq *q)
 {
-	int hw_cidx = ntohs(ACCESS_ONCE(q->stat->cidx));
+	int hw_cidx = ntohs(READ_ONCE(q->stat->cidx));
 	hw_cidx -= q->cidx;
 	return hw_cidx < 0 ? hw_cidx + q->size : hw_cidx;
 }
@@ -1323,7 +1323,7 @@ out_free:	dev_kfree_skb_any(skb);
  */
 static inline void reclaim_completed_tx_imm(struct sge_txq *q)
 {
-	int hw_cidx = ntohs(ACCESS_ONCE(q->stat->cidx));
+	int hw_cidx = ntohs(READ_ONCE(q->stat->cidx));
 	int reclaim = hw_cidx - q->cidx;
 
 	if (reclaim < 0)
diff --git a/drivers/net/ethernet/emulex/benet/be_main.c b/drivers/net/ethernet/emulex/benet/be_main.c
index 19aaddae7e99..eb164900553d 100644
--- a/drivers/net/ethernet/emulex/benet/be_main.c
+++ b/drivers/net/ethernet/emulex/benet/be_main.c
@@ -605,7 +605,7 @@ static void accumulate_16bit_val(u32 *acc, u16 val)
 
 	if (wrapped)
 		newacc += 65536;
-	ACCESS_ONCE(*acc) = newacc;
+	WRITE_ONCE(*acc, newacc);
 }
 
 static void populate_erx_stats(struct be_adapter *adapter,
* Unmerged path drivers/net/ethernet/hisilicon/hip04_eth.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_debugfs.c b/drivers/net/ethernet/intel/i40e/i40e_debugfs.c
index 5725f8955864..9ddfd9f841af 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_debugfs.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_debugfs.c
@@ -264,7 +264,7 @@ static void i40e_dbg_dump_vsi_seid(struct i40e_pf *pf, int seid)
 		 vsi->rx_buf_failed, vsi->rx_page_failed);
 	rcu_read_lock();
 	for (i = 0; i < vsi->num_queue_pairs; i++) {
-		struct i40e_ring *rx_ring = ACCESS_ONCE(vsi->rx_rings[i]);
+		struct i40e_ring *rx_ring = READ_ONCE(vsi->rx_rings[i]);
 
 		if (!rx_ring)
 			continue;
@@ -320,7 +320,7 @@ static void i40e_dbg_dump_vsi_seid(struct i40e_pf *pf, int seid)
 			 ITR_IS_DYNAMIC(rx_ring->rx_itr_setting) ? "dynamic" : "fixed");
 	}
 	for (i = 0; i < vsi->num_queue_pairs; i++) {
-		struct i40e_ring *tx_ring = ACCESS_ONCE(vsi->tx_rings[i]);
+		struct i40e_ring *tx_ring = READ_ONCE(vsi->tx_rings[i]);
 
 		if (!tx_ring)
 			continue;
diff --git a/drivers/net/ethernet/intel/i40e/i40e_ethtool.c b/drivers/net/ethernet/intel/i40e/i40e_ethtool.c
index 32a3d5990e9f..d11cc7edc859 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_ethtool.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_ethtool.c
@@ -1548,7 +1548,7 @@ static void i40e_get_ethtool_stats(struct net_device *netdev,
 	}
 	rcu_read_lock();
 	for (j = 0; j < vsi->num_queue_pairs; j++) {
-		tx_ring = ACCESS_ONCE(vsi->tx_rings[j]);
+		tx_ring = READ_ONCE(vsi->tx_rings[j]);
 
 		if (!tx_ring)
 			continue;
diff --git a/drivers/net/ethernet/intel/i40e/i40e_main.c b/drivers/net/ethernet/intel/i40e/i40e_main.c
index 00cd1ff6816c..05afe7e4f7b1 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_main.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c
@@ -434,7 +434,7 @@ static void i40e_get_netdev_stats_struct(struct net_device *netdev,
 		u64 bytes, packets;
 		unsigned int start;
 
-		tx_ring = ACCESS_ONCE(vsi->tx_rings[i]);
+		tx_ring = READ_ONCE(vsi->tx_rings[i]);
 		if (!tx_ring)
 			continue;
 
@@ -788,7 +788,7 @@ static void i40e_update_vsi_stats(struct i40e_vsi *vsi)
 	rcu_read_lock();
 	for (q = 0; q < vsi->num_queue_pairs; q++) {
 		/* locate Tx ring */
-		p = ACCESS_ONCE(vsi->tx_rings[q]);
+		p = READ_ONCE(vsi->tx_rings[q]);
 
 		do {
 			start = u64_stats_fetch_begin_irq(&p->syncp);
diff --git a/drivers/net/ethernet/intel/i40e/i40e_ptp.c b/drivers/net/ethernet/intel/i40e/i40e_ptp.c
index d8456c381c99..97381238eb7c 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_ptp.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_ptp.c
@@ -130,7 +130,7 @@ static int i40e_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)
 	}
 
 	smp_mb(); /* Force any pending update before accessing. */
-	adj = ACCESS_ONCE(pf->ptp_base_adj);
+	adj = READ_ONCE(pf->ptp_base_adj);
 
 	freq = adj;
 	freq *= ppb;
@@ -499,7 +499,7 @@ void i40e_ptp_set_increment(struct i40e_pf *pf)
 	wr32(hw, I40E_PRTTSYN_INC_H, incval >> 32);
 
 	/* Update the base adjustement value. */
-	ACCESS_ONCE(pf->ptp_base_adj) = incval;
+	WRITE_ONCE(pf->ptp_base_adj, incval);
 	smp_mb(); /* Force the above update. */
 }
 
diff --git a/drivers/net/ethernet/intel/igb/e1000_regs.h b/drivers/net/ethernet/intel/igb/e1000_regs.h
index 8eee081d395f..568c96842f28 100644
--- a/drivers/net/ethernet/intel/igb/e1000_regs.h
+++ b/drivers/net/ethernet/intel/igb/e1000_regs.h
@@ -375,7 +375,7 @@ u32 igb_rd32(struct e1000_hw *hw, u32 reg);
 /* write operations, indexed using DWORDS */
 #define wr32(reg, val) \
 do { \
-	u8 __iomem *hw_addr = ACCESS_ONCE((hw)->hw_addr); \
+	u8 __iomem *hw_addr = READ_ONCE((hw)->hw_addr); \
 	if (!E1000_REMOVED(hw_addr)) \
 		writel((val), &hw_addr[(reg)]); \
 } while (0)
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index b0a7a0c0d1f4..91e96b0eff2d 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -765,7 +765,7 @@ static void igb_cache_ring_register(struct igb_adapter *adapter)
 u32 igb_rd32(struct e1000_hw *hw, u32 reg)
 {
 	struct igb_adapter *igb = container_of(hw, struct igb_adapter, hw);
-	u8 __iomem *hw_addr = ACCESS_ONCE(hw->hw_addr);
+	u8 __iomem *hw_addr = READ_ONCE(hw->hw_addr);
 	u32 value = 0;
 
 	if (E1000_REMOVED(hw_addr))
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_common.h b/drivers/net/ethernet/intel/ixgbe/ixgbe_common.h
index 9630811d26fb..c8a7db310f50 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_common.h
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_common.h
@@ -168,7 +168,7 @@ static inline bool ixgbe_removed(void __iomem *addr)
 
 static inline void ixgbe_write_reg(struct ixgbe_hw *hw, u32 reg, u32 value)
 {
-	u8 __iomem *reg_addr = ACCESS_ONCE(hw->hw_addr);
+	u8 __iomem *reg_addr = READ_ONCE(hw->hw_addr);
 
 	if (ixgbe_removed(reg_addr))
 		return;
@@ -187,7 +187,7 @@ static inline void writeq(u64 val, void __iomem *addr)
 
 static inline void ixgbe_write_reg64(struct ixgbe_hw *hw, u32 reg, u64 value)
 {
-	u8 __iomem *reg_addr = ACCESS_ONCE(hw->hw_addr);
+	u8 __iomem *reg_addr = READ_ONCE(hw->hw_addr);
 
 	if (ixgbe_removed(reg_addr))
 		return;
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c
index a76e92e21b02..f6cc9166082a 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c
@@ -378,7 +378,7 @@ static int ixgbe_ptp_adjfreq_82599(struct ptp_clock_info *ptp, s32 ppb)
 	}
 
 	smp_mb();
-	incval = ACCESS_ONCE(adapter->base_incval);
+	incval = READ_ONCE(adapter->base_incval);
 
 	freq = incval;
 	freq *= ppb;
@@ -1159,7 +1159,7 @@ void ixgbe_ptp_start_cyclecounter(struct ixgbe_adapter *adapter)
 	}
 
 	/* update the base incval used to calculate frequency adjustment */
-	ACCESS_ONCE(adapter->base_incval) = incval;
+	WRITE_ONCE(adapter->base_incval, incval);
 	smp_mb();
 
 	/* need lock to prevent incorrect read while modifying cyclecounter */
diff --git a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index ff409af6cd65..ab2a58fe93c8 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@ -170,7 +170,7 @@ static void ixgbevf_check_remove(struct ixgbe_hw *hw, u32 reg)
 
 u32 ixgbevf_read_reg(struct ixgbe_hw *hw, u32 reg)
 {
-	u8 __iomem *reg_addr = ACCESS_ONCE(hw->hw_addr);
+	u8 __iomem *reg_addr = READ_ONCE(hw->hw_addr);
 	u32 value;
 
 	if (IXGBE_REMOVED(reg_addr))
diff --git a/drivers/net/ethernet/intel/ixgbevf/vf.h b/drivers/net/ethernet/intel/ixgbevf/vf.h
index 04d8d4ee4f04..c651fefcc3d2 100644
--- a/drivers/net/ethernet/intel/ixgbevf/vf.h
+++ b/drivers/net/ethernet/intel/ixgbevf/vf.h
@@ -182,7 +182,7 @@ struct ixgbevf_info {
 
 static inline void ixgbe_write_reg(struct ixgbe_hw *hw, u32 reg, u32 value)
 {
-	u8 __iomem *reg_addr = ACCESS_ONCE(hw->hw_addr);
+	u8 __iomem *reg_addr = READ_ONCE(hw->hw_addr);
 
 	if (IXGBE_REMOVED(reg_addr))
 		return;
diff --git a/drivers/net/ethernet/mellanox/mlx4/en_tx.c b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
index 0949c7934152..72ea52068bc8 100644
--- a/drivers/net/ethernet/mellanox/mlx4/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
@@ -394,8 +394,8 @@ static bool mlx4_en_process_tx_cq(struct net_device *dev,
 
 	index = cons_index & size_mask;
 	cqe = mlx4_en_get_cqe(buf, index, priv->cqe_size) + factor;
-	last_nr_txbb = ACCESS_ONCE(ring->last_nr_txbb);
-	ring_cons = ACCESS_ONCE(ring->cons);
+	last_nr_txbb = READ_ONCE(ring->last_nr_txbb);
+	ring_cons = READ_ONCE(ring->cons);
 	ring_index = ring_cons & size_mask;
 	stamp_index = ring_index;
 
@@ -459,8 +459,8 @@ static bool mlx4_en_process_tx_cq(struct net_device *dev,
 	wmb();
 
 	/* we want to dirty this cache line once */
-	ACCESS_ONCE(ring->last_nr_txbb) = last_nr_txbb;
-	ACCESS_ONCE(ring->cons) = ring_cons + txbbs_skipped;
+	WRITE_ONCE(ring->last_nr_txbb, last_nr_txbb);
+	WRITE_ONCE(ring->cons, ring_cons + txbbs_skipped);
 
 	netdev_tx_completed_queue(ring->tx_queue, packets, bytes);
 
@@ -835,7 +835,7 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev)
 		goto tx_drop;
 
 	/* fetch ring->cons far ahead before needing it to avoid stall */
-	ring_cons = ACCESS_ONCE(ring->cons);
+	ring_cons = READ_ONCE(ring->cons);
 
 	real_size = get_real_size(skb, shinfo, dev, &lso_header_size,
 				  &inline_ok, &fragptr);
@@ -1043,7 +1043,7 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev)
 		 */
 		smp_rmb();
 
-		ring_cons = ACCESS_ONCE(ring->cons);
+		ring_cons = READ_ONCE(ring->cons);
 		if (unlikely(!mlx4_en_is_tx_ring_full(ring))) {
 			netif_tx_wake_queue(ring->tx_queue);
 			ring->wake_queue++;
diff --git a/drivers/net/ethernet/neterion/vxge/vxge-main.c b/drivers/net/ethernet/neterion/vxge/vxge-main.c
index 58127161ca20..5d250366f87d 100644
--- a/drivers/net/ethernet/neterion/vxge/vxge-main.c
+++ b/drivers/net/ethernet/neterion/vxge/vxge-main.c
@@ -2630,7 +2630,7 @@ static void vxge_poll_vp_lockup(unsigned long data)
 		ring = &vdev->vpaths[i].ring;
 
 		/* Truncated to machine word size number of frames */
-		rx_frms = ACCESS_ONCE(ring->stats.rx_frms);
+		rx_frms = READ_ONCE(ring->stats.rx_frms);
 
 		/* Did this vpath received any packets */
 		if (ring->stats.prev_rx_frms == rx_frms) {
diff --git a/drivers/net/ethernet/sfc/ef10.c b/drivers/net/ethernet/sfc/ef10.c
index 4fcde9306ea4..ec30d02bcf84 100644
--- a/drivers/net/ethernet/sfc/ef10.c
+++ b/drivers/net/ethernet/sfc/ef10.c
@@ -2231,7 +2231,7 @@ static irqreturn_t efx_ef10_msi_interrupt(int irq, void *dev_id)
 	netif_vdbg(efx, intr, efx->net_dev,
 		   "IRQ %d on CPU %d\n", irq, raw_smp_processor_id());
 
-	if (likely(ACCESS_ONCE(efx->irq_soft_enabled))) {
+	if (likely(READ_ONCE(efx->irq_soft_enabled))) {
 		/* Note test interrupts */
 		if (context->index == efx->irq_level)
 			efx->last_irq_cpu = raw_smp_processor_id();
@@ -2246,7 +2246,7 @@ static irqreturn_t efx_ef10_msi_interrupt(int irq, void *dev_id)
 static irqreturn_t efx_ef10_legacy_interrupt(int irq, void *dev_id)
 {
 	struct efx_nic *efx = dev_id;
-	bool soft_enabled = ACCESS_ONCE(efx->irq_soft_enabled);
+	bool soft_enabled = READ_ONCE(efx->irq_soft_enabled);
 	struct efx_channel *channel;
 	efx_dword_t reg;
 	u32 queues;
@@ -3517,7 +3517,7 @@ static int efx_ef10_handle_rx_event(struct efx_channel *channel,
 	bool rx_cont;
 	u16 flags = 0;
 
-	if (unlikely(ACCESS_ONCE(efx->reset_pending)))
+	if (unlikely(READ_ONCE(efx->reset_pending)))
 		return 0;
 
 	/* Basic packet information */
@@ -3654,7 +3654,7 @@ efx_ef10_handle_tx_event(struct efx_channel *channel, efx_qword_t *event)
 	unsigned int tx_ev_q_label;
 	int tx_descs = 0;
 
-	if (unlikely(ACCESS_ONCE(efx->reset_pending)))
+	if (unlikely(READ_ONCE(efx->reset_pending)))
 		return 0;
 
 	if (unlikely(EFX_QWORD_FIELD(*event, ESF_DZ_TX_DROP_EVENT)))
@@ -5578,7 +5578,7 @@ static void efx_ef10_filter_remove_old(struct efx_nic *efx)
 	int i;
 
 	for (i = 0; i < HUNT_FILTER_TBL_ROWS; i++) {
-		if (ACCESS_ONCE(table->entry[i].spec) &
+		if (READ_ONCE(table->entry[i].spec) &
 		    EFX_EF10_FILTER_FLAG_AUTO_OLD) {
 			rc = efx_ef10_filter_remove_internal(efx,
 					1U << EFX_FILTER_PRI_AUTO, i, true);
diff --git a/drivers/net/ethernet/sfc/efx.c b/drivers/net/ethernet/sfc/efx.c
index 9559ba0654b0..c3e1d88ef3ef 100644
--- a/drivers/net/ethernet/sfc/efx.c
+++ b/drivers/net/ethernet/sfc/efx.c
@@ -2838,7 +2838,7 @@ static void efx_reset_work(struct work_struct *data)
 	unsigned long pending;
 	enum reset_type method;
 
-	pending = ACCESS_ONCE(efx->reset_pending);
+	pending = READ_ONCE(efx->reset_pending);
 	method = fls(pending) - 1;
 
 	if (method == RESET_TYPE_MC_BIST)
@@ -2903,7 +2903,7 @@ void efx_schedule_reset(struct efx_nic *efx, enum reset_type type)
 	/* If we're not READY then just leave the flags set as the cue
 	 * to abort probing or reschedule the reset later.
 	 */
-	if (ACCESS_ONCE(efx->state) != STATE_READY)
+	if (READ_ONCE(efx->state) != STATE_READY)
 		return;
 
 	/* efx_process_channel() will no longer read events once a
diff --git a/drivers/net/ethernet/sfc/falcon/efx.c b/drivers/net/ethernet/sfc/falcon/efx.c
index 1e92f176f694..d36c6a63bda4 100644
--- a/drivers/net/ethernet/sfc/falcon/efx.c
+++ b/drivers/net/ethernet/sfc/falcon/efx.c
@@ -2556,7 +2556,7 @@ static void ef4_reset_work(struct work_struct *data)
 	unsigned long pending;
 	enum reset_type method;
 
-	pending = ACCESS_ONCE(efx->reset_pending);
+	pending = READ_ONCE(efx->reset_pending);
 	method = fls(pending) - 1;
 
 	if ((method == RESET_TYPE_RECOVER_OR_DISABLE ||
@@ -2616,7 +2616,7 @@ void ef4_schedule_reset(struct ef4_nic *efx, enum reset_type type)
 	/* If we're not READY then just leave the flags set as the cue
 	 * to abort probing or reschedule the reset later.
 	 */
-	if (ACCESS_ONCE(efx->state) != STATE_READY)
+	if (READ_ONCE(efx->state) != STATE_READY)
 		return;
 
 	queue_work(reset_workqueue, &efx->reset_work);
diff --git a/drivers/net/ethernet/sfc/falcon/falcon.c b/drivers/net/ethernet/sfc/falcon/falcon.c
index c6ff0cc5ef18..d8dab256447e 100644
--- a/drivers/net/ethernet/sfc/falcon/falcon.c
+++ b/drivers/net/ethernet/sfc/falcon/falcon.c
@@ -450,7 +450,7 @@ static irqreturn_t falcon_legacy_interrupt_a1(int irq, void *dev_id)
 		   "IRQ %d on CPU %d status " EF4_OWORD_FMT "\n",
 		   irq, raw_smp_processor_id(), EF4_OWORD_VAL(*int_ker));
 
-	if (!likely(ACCESS_ONCE(efx->irq_soft_enabled)))
+	if (!likely(READ_ONCE(efx->irq_soft_enabled)))
 		return IRQ_HANDLED;
 
 	/* Check to see if we have a serious error condition */
@@ -1370,7 +1370,7 @@ static void falcon_reconfigure_mac_wrapper(struct ef4_nic *efx)
 	ef4_oword_t reg;
 	int link_speed, isolate;
 
-	isolate = !!ACCESS_ONCE(efx->reset_pending);
+	isolate = !!READ_ONCE(efx->reset_pending);
 
 	switch (link_state->speed) {
 	case 10000: link_speed = 3; break;
diff --git a/drivers/net/ethernet/sfc/falcon/farch.c b/drivers/net/ethernet/sfc/falcon/farch.c
index 05916c710d8c..494884f6af4a 100644
--- a/drivers/net/ethernet/sfc/falcon/farch.c
+++ b/drivers/net/ethernet/sfc/falcon/farch.c
@@ -834,7 +834,7 @@ ef4_farch_handle_tx_event(struct ef4_channel *channel, ef4_qword_t *event)
 	struct ef4_nic *efx = channel->efx;
 	int tx_packets = 0;
 
-	if (unlikely(ACCESS_ONCE(efx->reset_pending)))
+	if (unlikely(READ_ONCE(efx->reset_pending)))
 		return 0;
 
 	if (likely(EF4_QWORD_FIELD(*event, FSF_AZ_TX_EV_COMP))) {
@@ -990,7 +990,7 @@ ef4_farch_handle_rx_event(struct ef4_channel *channel, const ef4_qword_t *event)
 	struct ef4_rx_queue *rx_queue;
 	struct ef4_nic *efx = channel->efx;
 
-	if (unlikely(ACCESS_ONCE(efx->reset_pending)))
+	if (unlikely(READ_ONCE(efx->reset_pending)))
 		return;
 
 	rx_ev_cont = EF4_QWORD_FIELD(*event, FSF_AZ_RX_EV_JUMBO_CONT);
@@ -1504,7 +1504,7 @@ irqreturn_t ef4_farch_fatal_interrupt(struct ef4_nic *efx)
 irqreturn_t ef4_farch_legacy_interrupt(int irq, void *dev_id)
 {
 	struct ef4_nic *efx = dev_id;
-	bool soft_enabled = ACCESS_ONCE(efx->irq_soft_enabled);
+	bool soft_enabled = READ_ONCE(efx->irq_soft_enabled);
 	ef4_oword_t *int_ker = efx->irq_status.addr;
 	irqreturn_t result = IRQ_NONE;
 	struct ef4_channel *channel;
@@ -1596,7 +1596,7 @@ irqreturn_t ef4_farch_msi_interrupt(int irq, void *dev_id)
 		   "IRQ %d on CPU %d status " EF4_OWORD_FMT "\n",
 		   irq, raw_smp_processor_id(), EF4_OWORD_VAL(*int_ker));
 
-	if (!likely(ACCESS_ONCE(efx->irq_soft_enabled)))
+	if (!likely(READ_ONCE(efx->irq_soft_enabled)))
 		return IRQ_HANDLED;
 
 	/* Handle non-event-queue sources */
diff --git a/drivers/net/ethernet/sfc/falcon/nic.h b/drivers/net/ethernet/sfc/falcon/nic.h
index a4c4592f6023..54ca457cdb15 100644
--- a/drivers/net/ethernet/sfc/falcon/nic.h
+++ b/drivers/net/ethernet/sfc/falcon/nic.h
@@ -83,7 +83,7 @@ static inline struct ef4_tx_queue *ef4_tx_queue_partner(struct ef4_tx_queue *tx_
 static inline bool __ef4_nic_tx_is_empty(struct ef4_tx_queue *tx_queue,
 					 unsigned int write_count)
 {
-	unsigned int empty_read_count = ACCESS_ONCE(tx_queue->empty_read_count);
+	unsigned int empty_read_count = READ_ONCE(tx_queue->empty_read_count);
 
 	if (empty_read_count == 0)
 		return false;
@@ -464,11 +464,11 @@ irqreturn_t ef4_farch_fatal_interrupt(struct ef4_nic *efx);
 
 static inline int ef4_nic_event_test_irq_cpu(struct ef4_channel *channel)
 {
-	return ACCESS_ONCE(channel->event_test_cpu);
+	return READ_ONCE(channel->event_test_cpu);
 }
 static inline int ef4_nic_irq_test_irq_cpu(struct ef4_nic *efx)
 {
-	return ACCESS_ONCE(efx->last_irq_cpu);
+	return READ_ONCE(efx->last_irq_cpu);
 }
 
 /* Global Resources */
diff --git a/drivers/net/ethernet/sfc/falcon/tx.c b/drivers/net/ethernet/sfc/falcon/tx.c
index 2071a5a15f70..4c91327dd17d 100644
--- a/drivers/net/ethernet/sfc/falcon/tx.c
+++ b/drivers/net/ethernet/sfc/falcon/tx.c
@@ -134,8 +134,8 @@ static void ef4_tx_maybe_stop_queue(struct ef4_tx_queue *txq1)
 	 */
 	netif_tx_stop_queue(txq1->core_txq);
 	smp_mb();
-	txq1->old_read_count = ACCESS_ONCE(txq1->read_count);
-	txq2->old_read_count = ACCESS_ONCE(txq2->read_count);
+	txq1->old_read_count = READ_ONCE(txq1->read_count);
+	txq2->old_read_count = READ_ONCE(txq2->read_count);
 
 	fill_level = max(txq1->insert_count - txq1->old_read_count,
 			 txq2->insert_count - txq2->old_read_count);
@@ -526,7 +526,7 @@ void ef4_xmit_done(struct ef4_tx_queue *tx_queue, unsigned int index)
 
 	/* Check whether the hardware queue is now empty */
 	if ((int)(tx_queue->read_count - tx_queue->old_write_count) >= 0) {
-		tx_queue->old_write_count = ACCESS_ONCE(tx_queue->write_count);
+		tx_queue->old_write_count = READ_ONCE(tx_queue->write_count);
 		if (tx_queue->read_count == tx_queue->old_write_count) {
 			smp_mb();
 			tx_queue->empty_read_count =
diff --git a/drivers/net/ethernet/sfc/farch.c b/drivers/net/ethernet/sfc/farch.c
index 1f1b062a8d89..68c71254c063 100644
--- a/drivers/net/ethernet/sfc/farch.c
+++ b/drivers/net/ethernet/sfc/farch.c
@@ -827,7 +827,7 @@ efx_farch_handle_tx_event(struct efx_channel *channel, efx_qword_t *event)
 	struct efx_nic *efx = channel->efx;
 	int tx_packets = 0;
 
-	if (unlikely(ACCESS_ONCE(efx->reset_pending)))
+	if (unlikely(READ_ONCE(efx->reset_pending)))
 		return 0;
 
 	if (likely(EFX_QWORD_FIELD(*event, FSF_AZ_TX_EV_COMP))) {
@@ -983,7 +983,7 @@ efx_farch_handle_rx_event(struct efx_channel *channel, const efx_qword_t *event)
 	struct efx_rx_queue *rx_queue;
 	struct efx_nic *efx = channel->efx;
 
-	if (unlikely(ACCESS_ONCE(efx->reset_pending)))
+	if (unlikely(READ_ONCE(efx->reset_pending)))
 		return;
 
 	rx_ev_cont = EFX_QWORD_FIELD(*event, FSF_AZ_RX_EV_JUMBO_CONT);
@@ -1524,7 +1524,7 @@ irqreturn_t efx_farch_fatal_interrupt(struct efx_nic *efx)
 irqreturn_t efx_farch_legacy_interrupt(int irq, void *dev_id)
 {
 	struct efx_nic *efx = dev_id;
-	bool soft_enabled = ACCESS_ONCE(efx->irq_soft_enabled);
+	bool soft_enabled = READ_ONCE(efx->irq_soft_enabled);
 	efx_oword_t *int_ker = efx->irq_status.addr;
 	irqreturn_t result = IRQ_NONE;
 	struct efx_channel *channel;
@@ -1616,7 +1616,7 @@ irqreturn_t efx_farch_msi_interrupt(int irq, void *dev_id)
 		   "IRQ %d on CPU %d status " EFX_OWORD_FMT "\n",
 		   irq, raw_smp_processor_id(), EFX_OWORD_VAL(*int_ker));
 
-	if (!likely(ACCESS_ONCE(efx->irq_soft_enabled)))
+	if (!likely(READ_ONCE(efx->irq_soft_enabled)))
 		return IRQ_HANDLED;
 
 	/* Handle non-event-queue sources */
diff --git a/drivers/net/ethernet/sfc/nic.h b/drivers/net/ethernet/sfc/nic.h
index d285e4e58f7e..ca7a32c87370 100644
--- a/drivers/net/ethernet/sfc/nic.h
+++ b/drivers/net/ethernet/sfc/nic.h
@@ -81,7 +81,7 @@ static struct efx_tx_queue *efx_tx_queue_partner(struct efx_tx_queue *tx_queue)
 static inline bool __efx_nic_tx_is_empty(struct efx_tx_queue *tx_queue,
 					 unsigned int write_count)
 {
-	unsigned int empty_read_count = ACCESS_ONCE(tx_queue->empty_read_count);
+	unsigned int empty_read_count = READ_ONCE(tx_queue->empty_read_count);
 
 	if (empty_read_count == 0)
 		return false;
@@ -639,11 +639,11 @@ irqreturn_t efx_farch_fatal_interrupt(struct efx_nic *efx);
 
 static inline int efx_nic_event_test_irq_cpu(struct efx_channel *channel)
 {
-	return ACCESS_ONCE(channel->event_test_cpu);
+	return READ_ONCE(channel->event_test_cpu);
 }
 static inline int efx_nic_irq_test_irq_cpu(struct efx_nic *efx)
 {
-	return ACCESS_ONCE(efx->last_irq_cpu);
+	return READ_ONCE(efx->last_irq_cpu);
 }
 
 /* Global Resources */
* Unmerged path drivers/net/ethernet/sfc/ptp.c
diff --git a/drivers/net/ethernet/sfc/tx.c b/drivers/net/ethernet/sfc/tx.c
index 6f000e72f611..49cf970ba739 100644
--- a/drivers/net/ethernet/sfc/tx.c
+++ b/drivers/net/ethernet/sfc/tx.c
@@ -137,8 +137,8 @@ static void efx_tx_maybe_stop_queue(struct efx_tx_queue *txq1)
 	 */
 	netif_tx_stop_queue(txq1->core_txq);
 	smp_mb();
-	txq1->old_read_count = ACCESS_ONCE(txq1->read_count);
-	txq2->old_read_count = ACCESS_ONCE(txq2->read_count);
+	txq1->old_read_count = READ_ONCE(txq1->read_count);
+	txq2->old_read_count = READ_ONCE(txq2->read_count);
 
 	fill_level = max(txq1->insert_count - txq1->old_read_count,
 			 txq2->insert_count - txq2->old_read_count);
@@ -774,7 +774,7 @@ void efx_xmit_done(struct efx_tx_queue *tx_queue, unsigned int index)
 
 	/* Check whether the hardware queue is now empty */
 	if ((int)(tx_queue->read_count - tx_queue->old_write_count) >= 0) {
-		tx_queue->old_write_count = ACCESS_ONCE(tx_queue->write_count);
+		tx_queue->old_write_count = READ_ONCE(tx_queue->write_count);
 		if (tx_queue->read_count == tx_queue->old_write_count) {
 			smp_mb();
 			tx_queue->empty_read_count =
diff --git a/drivers/net/ethernet/sun/niu.c b/drivers/net/ethernet/sun/niu.c
index 0678219c98c2..08f2ba68f4e3 100644
--- a/drivers/net/ethernet/sun/niu.c
+++ b/drivers/net/ethernet/sun/niu.c
@@ -6244,7 +6244,7 @@ static void niu_get_rx_stats(struct niu *np,
 
 	pkts = dropped = errors = bytes = 0;
 
-	rx_rings = ACCESS_ONCE(np->rx_rings);
+	rx_rings = READ_ONCE(np->rx_rings);
 	if (!rx_rings)
 		goto no_rings;
 
@@ -6275,7 +6275,7 @@ static void niu_get_tx_stats(struct niu *np,
 
 	pkts = errors = bytes = 0;
 
-	tx_rings = ACCESS_ONCE(np->tx_rings);
+	tx_rings = READ_ONCE(np->tx_rings);
 	if (!tx_rings)
 		goto no_rings;
 
* Unmerged path drivers/net/tap.c
diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 3c46a6b55234..b500aa8a1258 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -452,7 +452,7 @@ static u16 tun_select_queue(struct net_device *dev, struct sk_buff *skb,
 	u32 numqueues = 0;
 
 	rcu_read_lock();
-	numqueues = ACCESS_ONCE(tun->numqueues);
+	numqueues = READ_ONCE(tun->numqueues);
 
 	txq = __skb_get_hash_symmetric(skb);
 	if (txq) {
@@ -835,7 +835,7 @@ static netdev_tx_t tun_net_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	rcu_read_lock();
 	tfile = rcu_dereference(tun->tfiles[txq]);
-	numqueues = ACCESS_ONCE(tun->numqueues);
+	numqueues = READ_ONCE(tun->numqueues);
 
 	/* Drop packet if interface is not attached */
 	if (txq >= numqueues)
diff --git a/drivers/net/wireless/ath/ath5k/desc.c b/drivers/net/wireless/ath/ath5k/desc.c
index bd8d4392d68b..80f75139495f 100644
--- a/drivers/net/wireless/ath/ath5k/desc.c
+++ b/drivers/net/wireless/ath/ath5k/desc.c
@@ -500,13 +500,13 @@ ath5k_hw_proc_4word_tx_status(struct ath5k_hw *ah,
 
 	tx_status = &desc->ud.ds_tx5212.tx_stat;
 
-	txstat1 = ACCESS_ONCE(tx_status->tx_status_1);
+	txstat1 = READ_ONCE(tx_status->tx_status_1);
 
 	/* No frame has been send or error */
 	if (unlikely(!(txstat1 & AR5K_DESC_TX_STATUS1_DONE)))
 		return -EINPROGRESS;
 
-	txstat0 = ACCESS_ONCE(tx_status->tx_status_0);
+	txstat0 = READ_ONCE(tx_status->tx_status_0);
 
 	/*
 	 * Get descriptor status
@@ -700,14 +700,14 @@ ath5k_hw_proc_5212_rx_status(struct ath5k_hw *ah,
 	u32 rxstat0, rxstat1;
 
 	rx_status = &desc->ud.ds_rx.rx_stat;
-	rxstat1 = ACCESS_ONCE(rx_status->rx_status_1);
+	rxstat1 = READ_ONCE(rx_status->rx_status_1);
 
 	/* No frame received / not ready */
 	if (unlikely(!(rxstat1 & AR5K_5212_RX_DESC_STATUS1_DONE)))
 		return -EINPROGRESS;
 
 	memset(rs, 0, sizeof(struct ath5k_rx_status));
-	rxstat0 = ACCESS_ONCE(rx_status->rx_status_0);
+	rxstat0 = READ_ONCE(rx_status->rx_status_0);
 
 	/*
 	 * Frame receive status
diff --git a/drivers/net/wireless/broadcom/brcm80211/brcmfmac/sdio.c b/drivers/net/wireless/broadcom/brcm80211/brcmfmac/sdio.c
index 0efac4f4e23b..cc12fe96d17c 100644
--- a/drivers/net/wireless/broadcom/brcm80211/brcmfmac/sdio.c
+++ b/drivers/net/wireless/broadcom/brcm80211/brcmfmac/sdio.c
@@ -3632,7 +3632,7 @@ static void brcmf_sdio_dataworker(struct work_struct *work)
 
 	bus->dpc_running = true;
 	wmb();
-	while (ACCESS_ONCE(bus->dpc_triggered)) {
+	while (READ_ONCE(bus->dpc_triggered)) {
 		bus->dpc_triggered = false;
 		brcmf_sdio_dpc(bus);
 		bus->idlecount = 0;
diff --git a/drivers/net/wireless/intel/iwlwifi/mvm/ops.c b/drivers/net/wireless/intel/iwlwifi/mvm/ops.c
index 9fb40955d5f4..53193ee48f20 100644
--- a/drivers/net/wireless/intel/iwlwifi/mvm/ops.c
+++ b/drivers/net/wireless/intel/iwlwifi/mvm/ops.c
@@ -1119,7 +1119,7 @@ void iwl_mvm_set_hw_ctkill_state(struct iwl_mvm *mvm, bool state)
 static bool iwl_mvm_set_hw_rfkill_state(struct iwl_op_mode *op_mode, bool state)
 {
 	struct iwl_mvm *mvm = IWL_OP_MODE_GET_MVM(op_mode);
-	bool calibrating = ACCESS_ONCE(mvm->calibrating);
+	bool calibrating = READ_ONCE(mvm->calibrating);
 
 	if (state)
 		set_bit(IWL_MVM_STATUS_HW_RFKILL, &mvm->status);
diff --git a/drivers/net/wireless/intel/iwlwifi/mvm/tx.c b/drivers/net/wireless/intel/iwlwifi/mvm/tx.c
index 887a504ce64a..153c29f02c97 100644
--- a/drivers/net/wireless/intel/iwlwifi/mvm/tx.c
+++ b/drivers/net/wireless/intel/iwlwifi/mvm/tx.c
@@ -652,7 +652,7 @@ int iwl_mvm_tx_skb_non_sta(struct iwl_mvm *mvm, struct sk_buff *skb)
 				return -1;
 		} else if (info.control.vif->type == NL80211_IFTYPE_STATION &&
 			   is_multicast_ether_addr(hdr->addr1)) {
-			u8 ap_sta_id = ACCESS_ONCE(mvmvif->ap_sta_id);
+			u8 ap_sta_id = READ_ONCE(mvmvif->ap_sta_id);
 
 			if (ap_sta_id != IWL_MVM_INVALID_STA)
 				sta_id = ap_sta_id;
@@ -701,7 +701,7 @@ static int iwl_mvm_tx_tso(struct iwl_mvm *mvm, struct sk_buff *skb,
 	snap_ip_tcp = 8 + skb_transport_header(skb) - skb_network_header(skb) +
 		tcp_hdrlen(skb);
 
-	dbg_max_amsdu_len = ACCESS_ONCE(mvm->max_amsdu_len);
+	dbg_max_amsdu_len = READ_ONCE(mvm->max_amsdu_len);
 
 	if (!sta->max_amsdu_len ||
 	    !ieee80211_is_data_qos(hdr->frame_control) ||
diff --git a/drivers/net/wireless/intel/iwlwifi/pcie/rx.c b/drivers/net/wireless/intel/iwlwifi/pcie/rx.c
index a06b6612b658..f25ce3a1ea50 100644
--- a/drivers/net/wireless/intel/iwlwifi/pcie/rx.c
+++ b/drivers/net/wireless/intel/iwlwifi/pcie/rx.c
@@ -1247,7 +1247,7 @@ restart:
 	spin_lock(&rxq->lock);
 	/* uCode's read index (stored in shared DRAM) indicates the last Rx
 	 * buffer that the driver may process (last buffer filled by ucode). */
-	r = le16_to_cpu(ACCESS_ONCE(rxq->rb_stts->closed_rb_num)) & 0x0FFF;
+	r = le16_to_cpu(READ_ONCE(rxq->rb_stts->closed_rb_num)) & 0x0FFF;
 	i = rxq->read;
 
 	/* W/A 9000 device step A0 wrap-around bug */
diff --git a/drivers/net/wireless/intel/iwlwifi/pcie/trans.c b/drivers/net/wireless/intel/iwlwifi/pcie/trans.c
index 2e3e013ec95a..9ad3f4fe5894 100644
--- a/drivers/net/wireless/intel/iwlwifi/pcie/trans.c
+++ b/drivers/net/wireless/intel/iwlwifi/pcie/trans.c
@@ -2076,12 +2076,12 @@ static int iwl_trans_pcie_wait_txq_empty(struct iwl_trans *trans, int txq_idx)
 
 	IWL_DEBUG_TX_QUEUES(trans, "Emptying queue %d...\n", txq_idx);
 	txq = trans_pcie->txq[txq_idx];
-	wr_ptr = ACCESS_ONCE(txq->write_ptr);
+	wr_ptr = READ_ONCE(txq->write_ptr);
 
-	while (txq->read_ptr != ACCESS_ONCE(txq->write_ptr) &&
+	while (txq->read_ptr != READ_ONCE(txq->write_ptr) &&
 	       !time_after(jiffies,
 			   now + msecs_to_jiffies(IWL_FLUSH_WAIT_MS))) {
-		u8 write_ptr = ACCESS_ONCE(txq->write_ptr);
+		u8 write_ptr = READ_ONCE(txq->write_ptr);
 
 		if (WARN_ONCE(wr_ptr != write_ptr,
 			      "WR pointer moved while flushing %d -> %d\n",
@@ -2553,7 +2553,7 @@ static u32 iwl_trans_pcie_dump_rbs(struct iwl_trans *trans,
 
 	spin_lock(&rxq->lock);
 
-	r = le16_to_cpu(ACCESS_ONCE(rxq->rb_stts->closed_rb_num)) & 0x0FFF;
+	r = le16_to_cpu(READ_ONCE(rxq->rb_stts->closed_rb_num)) & 0x0FFF;
 
 	for (i = rxq->read, j = 0;
 	     i != r && j < allocated_rb_nums;
@@ -2814,7 +2814,7 @@ static struct iwl_trans_dump_data
 		/* Dump RBs is supported only for pre-9000 devices (1 queue) */
 		struct iwl_rxq *rxq = &trans_pcie->rxq[0];
 		/* RBs */
-		num_rbs = le16_to_cpu(ACCESS_ONCE(rxq->rb_stts->closed_rb_num))
+		num_rbs = le16_to_cpu(READ_ONCE(rxq->rb_stts->closed_rb_num))
 				      & 0x0FFF;
 		num_rbs = (num_rbs - rxq->read) & RX_QUEUE_MASK;
 		len += num_rbs * (sizeof(*data) +
diff --git a/drivers/net/wireless/mac80211_hwsim.c b/drivers/net/wireless/mac80211_hwsim.c
index 7778a708ddb6..5f64e5e652c2 100644
--- a/drivers/net/wireless/mac80211_hwsim.c
+++ b/drivers/net/wireless/mac80211_hwsim.c
@@ -1380,7 +1380,7 @@ static void mac80211_hwsim_tx(struct ieee80211_hw *hw,
 	mac80211_hwsim_monitor_rx(hw, skb, channel);
 
 	/* wmediumd mode check */
-	_portid = ACCESS_ONCE(data->wmediumd);
+	_portid = READ_ONCE(data->wmediumd);
 
 	if (_portid)
 		return mac80211_hwsim_tx_frame_nl(hw, skb, _portid);
@@ -1477,7 +1477,7 @@ static void mac80211_hwsim_tx_frame(struct ieee80211_hw *hw,
 				    struct ieee80211_channel *chan)
 {
 	struct mac80211_hwsim_data *data = hw->priv;
-	u32 _pid = ACCESS_ONCE(data->wmediumd);
+	u32 _pid = READ_ONCE(data->wmediumd);
 
 	if (ieee80211_hw_check(hw, SUPPORTS_RC_TABLE)) {
 		struct ieee80211_tx_info *txi = IEEE80211_SKB_CB(skb);
diff --git a/drivers/scsi/qla2xxx/qla_target.c b/drivers/scsi/qla2xxx/qla_target.c
index 753c5e0953f9..581e979bf23b 100644
--- a/drivers/scsi/qla2xxx/qla_target.c
+++ b/drivers/scsi/qla2xxx/qla_target.c
@@ -993,7 +993,7 @@ static void qlt_free_session_done(struct work_struct *work)
 	if (logout_started) {
 		bool traced = false;
 
-		while (!ACCESS_ONCE(sess->logout_completed)) {
+		while (!READ_ONCE(sess->logout_completed)) {
 			if (!traced) {
 				ql_dbg(ql_dbg_tgt_mgt, vha, 0xf086,
 					"%s: waiting for sess %p logout\n",
diff --git a/drivers/target/target_core_user.c b/drivers/target/target_core_user.c
index 07b665cb5768..3be47b977b71 100644
--- a/drivers/target/target_core_user.c
+++ b/drivers/target/target_core_user.c
@@ -834,7 +834,7 @@ static unsigned int tcmu_handle_completions(struct tcmu_dev *udev)
 	mb = udev->mb_addr;
 	tcmu_flush_dcache_range(mb, sizeof(*mb));
 
-	while (udev->cmdr_last_cleaned != ACCESS_ONCE(mb->cmd_tail)) {
+	while (udev->cmdr_last_cleaned != READ_ONCE(mb->cmd_tail)) {
 
 		struct tcmu_cmd_entry *entry = (void *) mb + CMDR_OFF + udev->cmdr_last_cleaned;
 		struct tcmu_cmd *cmd;
diff --git a/drivers/usb/class/cdc-wdm.c b/drivers/usb/class/cdc-wdm.c
index 3e865dbf878c..fbaa2a90d25d 100644
--- a/drivers/usb/class/cdc-wdm.c
+++ b/drivers/usb/class/cdc-wdm.c
@@ -483,7 +483,7 @@ static ssize_t wdm_read
 	if (rv < 0)
 		return -ERESTARTSYS;
 
-	cntr = ACCESS_ONCE(desc->length);
+	cntr = READ_ONCE(desc->length);
 	if (cntr == 0) {
 		desc->read = 0;
 retry:
diff --git a/drivers/usb/core/devio.c b/drivers/usb/core/devio.c
index ac77c8f486db..60c530377418 100644
--- a/drivers/usb/core/devio.c
+++ b/drivers/usb/core/devio.c
@@ -135,7 +135,7 @@ static int usbfs_increase_memory_usage(u64 amount)
 {
 	u64 lim;
 
-	lim = ACCESS_ONCE(usbfs_memory_mb);
+	lim = READ_ONCE(usbfs_memory_mb);
 	lim <<= 20;
 
 	atomic64_add(amount, &usbfs_memory_usage);
diff --git a/drivers/usb/core/sysfs.c b/drivers/usb/core/sysfs.c
index 3fe8c26a1d24..a0dbe9b42b92 100644
--- a/drivers/usb/core/sysfs.c
+++ b/drivers/usb/core/sysfs.c
@@ -974,7 +974,7 @@ static ssize_t interface_show(struct device *dev, struct device_attribute *attr,
 	char *string;
 
 	intf = to_usb_interface(dev);
-	string = ACCESS_ONCE(intf->cur_altsetting->string);
+	string = READ_ONCE(intf->cur_altsetting->string);
 	if (!string)
 		return 0;
 	return sprintf(buf, "%s\n", string);
@@ -990,7 +990,7 @@ static ssize_t modalias_show(struct device *dev, struct device_attribute *attr,
 
 	intf = to_usb_interface(dev);
 	udev = interface_to_usbdev(intf);
-	alt = ACCESS_ONCE(intf->cur_altsetting);
+	alt = READ_ONCE(intf->cur_altsetting);
 
 	return sprintf(buf, "usb:v%04Xp%04Xd%04Xdc%02Xdsc%02Xdp%02X"
 			"ic%02Xisc%02Xip%02Xin%02X\n",
* Unmerged path drivers/usb/gadget/udc/gr_udc.c
diff --git a/drivers/usb/host/ohci-hcd.c b/drivers/usb/host/ohci-hcd.c
index 5d54c4ac571a..e704d3525edf 100644
--- a/drivers/usb/host/ohci-hcd.c
+++ b/drivers/usb/host/ohci-hcd.c
@@ -786,7 +786,7 @@ static void io_watchdog_func(unsigned long _ohci)
 		}
 
 		/* find the last TD processed by the controller. */
-		head = hc32_to_cpu(ohci, ACCESS_ONCE(ed->hwHeadP)) & TD_MASK;
+		head = hc32_to_cpu(ohci, READ_ONCE(ed->hwHeadP)) & TD_MASK;
 		td_start = td;
 		td_next = list_prepare_entry(td, &ed->td_list, td_list);
 		list_for_each_entry_continue(td_next, &ed->td_list, td_list) {
diff --git a/drivers/usb/host/uhci-hcd.h b/drivers/usb/host/uhci-hcd.h
index ef5c52b89736..1bf1ecce5b5b 100644
--- a/drivers/usb/host/uhci-hcd.h
+++ b/drivers/usb/host/uhci-hcd.h
@@ -187,7 +187,7 @@ struct uhci_qh {
  * We need a special accessor for the element pointer because it is
  * subject to asynchronous updates by the controller.
  */
-#define qh_element(qh)		ACCESS_ONCE((qh)->element)
+#define qh_element(qh)		READ_ONCE((qh)->element)
 
 #define LINK_TO_QH(uhci, qh)	(UHCI_PTR_QH((uhci)) | \
 				cpu_to_hc32((uhci), (qh)->dma_handle))
@@ -275,7 +275,7 @@ struct uhci_td {
  * subject to asynchronous updates by the controller.
  */
 #define td_status(uhci, td)		hc32_to_cpu((uhci), \
-						ACCESS_ONCE((td)->status))
+						READ_ONCE((td)->status))
 
 #define LINK_TO_TD(uhci, td)		(cpu_to_hc32((uhci), (td)->dma_handle))
 
diff --git a/drivers/vfio/vfio.c b/drivers/vfio/vfio.c
index d2c861332a61..41a4faa752ac 100644
--- a/drivers/vfio/vfio.c
+++ b/drivers/vfio/vfio.c
@@ -672,7 +672,7 @@ static int vfio_dev_viable(struct device *dev, void *data)
 {
 	struct vfio_group *group = data;
 	struct vfio_device *device;
-	struct device_driver *drv = ACCESS_ONCE(dev->driver);
+	struct device_driver *drv = READ_ONCE(dev->driver);
 	struct vfio_unbound_dev *unbound;
 	int ret = -EINVAL;
 
* Unmerged path drivers/vhost/scsi.c
diff --git a/fs/aio.c b/fs/aio.c
index f7905a41f561..371d02e25694 100644
--- a/fs/aio.c
+++ b/fs/aio.c
@@ -501,7 +501,7 @@ static int kiocb_cancel(struct kioctx *ctx, struct kiocb *kiocb,
 	 * actually has a cancel function, hence the cmpxchg()
 	 */
 
-	cancel = ACCESS_ONCE(kiocb->ki_cancel);
+	cancel = READ_ONCE(kiocb->ki_cancel);
 	do {
 		if (!cancel || cancel == KIOCB_CANCELLED)
 			return ret;
diff --git a/fs/buffer.c b/fs/buffer.c
index d4d8ca8b872a..446595df364f 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -1604,7 +1604,8 @@ static struct buffer_head *create_page_buffers(struct page *page, struct inode *
 	BUG_ON(!PageLocked(page));
 
 	if (!page_has_buffers(page))
-		create_empty_buffers(page, 1 << ACCESS_ONCE(inode->i_blkbits), b_state);
+		create_empty_buffers(page, 1 << READ_ONCE(inode->i_blkbits),
+				     b_state);
 	return page_buffers(page);
 }
 
* Unmerged path fs/crypto/keyinfo.c
* Unmerged path fs/direct-io.c
* Unmerged path fs/exec.c
diff --git a/fs/fcntl.c b/fs/fcntl.c
index 5c030954e9f9..c62aeaf534c2 100644
--- a/fs/fcntl.c
+++ b/fs/fcntl.c
@@ -478,7 +478,7 @@ static void send_sigio_to_task(struct task_struct *p,
 	 * F_SETSIG can change ->signum lockless in parallel, make
 	 * sure we read it once and use the same value throughout.
 	 */
-	int signum = ACCESS_ONCE(fown->signum);
+	int signum = READ_ONCE(fown->signum);
 
 	if (!sigio_perm(p, fown, signum))
 		return;
diff --git a/fs/fs_pin.c b/fs/fs_pin.c
index 611b5408f6ec..946e595e7a4a 100644
--- a/fs/fs_pin.c
+++ b/fs/fs_pin.c
@@ -78,7 +78,7 @@ void mnt_pin_kill(struct mount *m)
 	while (1) {
 		struct hlist_node *p;
 		rcu_read_lock();
-		p = ACCESS_ONCE(m->mnt_pins.first);
+		p = READ_ONCE(m->mnt_pins.first);
 		if (!p) {
 			rcu_read_unlock();
 			break;
@@ -92,7 +92,7 @@ void group_pin_kill(struct hlist_head *p)
 	while (1) {
 		struct hlist_node *q;
 		rcu_read_lock();
-		q = ACCESS_ONCE(p->first);
+		q = READ_ONCE(p->first);
 		if (!q) {
 			rcu_read_unlock();
 			break;
* Unmerged path fs/fuse/dev.c
diff --git a/fs/inode.c b/fs/inode.c
index 879ae2fb82f3..7feae318c882 100644
--- a/fs/inode.c
+++ b/fs/inode.c
@@ -2007,7 +2007,7 @@ void inode_set_flags(struct inode *inode, unsigned int flags,
 
 	WARN_ON_ONCE(flags & ~mask);
 	do {
-		old_flags = ACCESS_ONCE(inode->i_flags);
+		old_flags = READ_ONCE(inode->i_flags);
 		new_flags = (old_flags & ~mask) | flags;
 	} while (unlikely(cmpxchg(&inode->i_flags, old_flags,
 				  new_flags) != old_flags));
diff --git a/fs/namei.c b/fs/namei.c
index 8c7468b2431b..f72983e3024f 100644
--- a/fs/namei.c
+++ b/fs/namei.c
@@ -1058,7 +1058,7 @@ static int follow_managed(struct path *path, unsigned flags)
 	/* Given that we're not holding a lock here, we retain the value in a
 	 * local variable for each dentry as we look at it so that we don't see
 	 * the components of that value change under us */
-	while (managed = ACCESS_ONCE(path->dentry->d_flags),
+	while (managed = READ_ONCE(path->dentry->d_flags),
 	       managed &= DCACHE_MANAGED_DENTRY,
 	       unlikely(managed != 0)) {
 		/* Allow the filesystem to manage the transit without i_mutex
@@ -1233,7 +1233,7 @@ int follow_down(struct path *path)
 	unsigned managed;
 	int ret;
 
-	while (managed = ACCESS_ONCE(path->dentry->d_flags),
+	while (managed = READ_ONCE(path->dentry->d_flags),
 	       unlikely(managed & DCACHE_MANAGED_DENTRY)) {
 		/* Allow the filesystem to manage the transit without i_mutex
 		 * being held.
diff --git a/fs/namespace.c b/fs/namespace.c
index b931013c73ea..feb57d316b5f 100644
--- a/fs/namespace.c
+++ b/fs/namespace.c
@@ -351,7 +351,7 @@ int __mnt_want_write(struct vfsmount *m)
 	 * incremented count after it has set MNT_WRITE_HOLD.
 	 */
 	smp_mb();
-	while (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
+	while (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
 		cpu_relax();
 	/*
 	 * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will
* Unmerged path fs/nfs/dir.c
diff --git a/fs/proc/array.c b/fs/proc/array.c
index d7c97e89352e..6634d95344e0 100644
--- a/fs/proc/array.c
+++ b/fs/proc/array.c
@@ -465,7 +465,7 @@ static int do_task_stat(struct seq_file *m, struct pid_namespace *ns,
 		cutime = sig->cutime;
 		cstime = sig->cstime;
 		cgtime = sig->cgtime;
-		rsslim = ACCESS_ONCE(sig->rlim[RLIMIT_RSS].rlim_cur);
+		rsslim = READ_ONCE(sig->rlim[RLIMIT_RSS].rlim_cur);
 
 		/* add up live thread stats at the group level */
 		if (whole) {
* Unmerged path fs/proc_namespace.c
diff --git a/fs/splice.c b/fs/splice.c
index c14c6237c3bc..caaa045ed53f 100644
--- a/fs/splice.c
+++ b/fs/splice.c
@@ -278,7 +278,7 @@ void spd_release_page(struct splice_pipe_desc *spd, unsigned int i)
  */
 int splice_grow_spd(const struct pipe_inode_info *pipe, struct splice_pipe_desc *spd)
 {
-	unsigned int buffers = ACCESS_ONCE(pipe->buffers);
+	unsigned int buffers = READ_ONCE(pipe->buffers);
 
 	spd->nr_pages_max = buffers;
 	if (buffers <= PIPE_DEF_BUFFERS)
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 6ed224e77642..1d099700a23a 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -379,7 +379,7 @@ int handle_userfault(struct vm_fault *vmf, unsigned long reason)
 	 * in __get_user_pages if userfaultfd_release waits on the
 	 * caller of handle_userfault to release the mmap_sem.
 	 */
-	if (unlikely(ACCESS_ONCE(ctx->released))) {
+	if (unlikely(READ_ONCE(ctx->released))) {
 		/*
 		 * Don't return VM_FAULT_SIGBUS in this case, so a non
 		 * cooperative manager can close the uffd after the
@@ -474,7 +474,7 @@ int handle_userfault(struct vm_fault *vmf, unsigned long reason)
 						       vmf->flags, reason);
 	up_read(&mm->mmap_sem);
 
-	if (likely(must_wait && !ACCESS_ONCE(ctx->released) &&
+	if (likely(must_wait && !READ_ONCE(ctx->released) &&
 		   (return_to_userland ? !signal_pending(current) :
 		    !fatal_signal_pending(current)))) {
 		wake_up_poll(&ctx->fd_wqh, POLLIN);
@@ -586,7 +586,7 @@ static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,
 		set_current_state(TASK_KILLABLE);
 		if (ewq->msg.event == 0)
 			break;
-		if (ACCESS_ONCE(ctx->released) ||
+		if (READ_ONCE(ctx->released) ||
 		    fatal_signal_pending(current)) {
 			/*
 			 * &ewq->wq may be queued in fork_event, but
@@ -846,7 +846,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)
 	struct userfaultfd_wake_range range = { .len = 0, };
 	unsigned long new_flags;
 
-	ACCESS_ONCE(ctx->released) = true;
+	WRITE_ONCE(ctx->released, true);
 
 	if (!mmget_not_zero(mm))
 		goto wakeup;
diff --git a/fs/xfs/xfs_log_priv.h b/fs/xfs/xfs_log_priv.h
index 70855d2973a6..fcbbc647fa28 100644
--- a/fs/xfs/xfs_log_priv.h
+++ b/fs/xfs/xfs_log_priv.h
@@ -591,9 +591,9 @@ xlog_valid_lsn(
 	 * a transiently forward state. Instead, we can see the LSN in a
 	 * transiently behind state if we happen to race with a cycle wrap.
 	 */
-	cur_cycle = ACCESS_ONCE(log->l_curr_cycle);
+	cur_cycle = READ_ONCE(log->l_curr_cycle);
 	smp_rmb();
-	cur_block = ACCESS_ONCE(log->l_curr_block);
+	cur_block = READ_ONCE(log->l_curr_block);
 
 	if ((CYCLE_LSN(lsn) > cur_cycle) ||
 	    (CYCLE_LSN(lsn) == cur_cycle && BLOCK_LSN(lsn) > cur_block)) {
diff --git a/include/linux/bitops.h b/include/linux/bitops.h
index d375548e3c03..8905790ff4a9 100644
--- a/include/linux/bitops.h
+++ b/include/linux/bitops.h
@@ -263,7 +263,7 @@ static __always_inline void set_bit32(long nr, volatile u32 *addr)
 	typeof(*ptr) old, new;					\
 								\
 	do {							\
-		old = ACCESS_ONCE(*ptr);			\
+		old = READ_ONCE(*ptr);			\
 		new = (old & ~mask) | bits;			\
 	} while (cmpxchg(ptr, old, new) != old);		\
 								\
@@ -278,7 +278,7 @@ static __always_inline void set_bit32(long nr, volatile u32 *addr)
 	typeof(*ptr) old, new;					\
 								\
 	do {							\
-		old = ACCESS_ONCE(*ptr);			\
+		old = READ_ONCE(*ptr);			\
 		new = old & ~clear;				\
 	} while (!(old & test) &&				\
 		 cmpxchg(ptr, old, new) != old);		\
diff --git a/include/linux/dynamic_queue_limits.h b/include/linux/dynamic_queue_limits.h
index a4be70398ce1..36dd4ffb5715 100644
--- a/include/linux/dynamic_queue_limits.h
+++ b/include/linux/dynamic_queue_limits.h
@@ -88,7 +88,7 @@ static inline void dql_queued(struct dql *dql, unsigned int count)
 /* Returns how many objects can be queued, < 0 indicates over limit. */
 static inline int dql_avail(const struct dql *dql)
 {
-	return ACCESS_ONCE(dql->adj_limit) - ACCESS_ONCE(dql->num_queued);
+	return READ_ONCE(dql->adj_limit) - READ_ONCE(dql->num_queued);
 }
 
 /* Record number of completed objects and recalculate the limit. */
diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index 26ec98241400..41fb98b910a5 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -226,7 +226,7 @@ extern struct page *huge_zero_page;
 
 static inline bool is_huge_zero_page(struct page *page)
 {
-	return ACCESS_ONCE(huge_zero_page) == page;
+	return READ_ONCE(huge_zero_page) == page;
 }
 
 static inline bool is_huge_zero_page_release(struct page *page)
diff --git a/include/linux/if_team.h b/include/linux/if_team.h
index c05216a8fbac..5f008f3042b3 100644
--- a/include/linux/if_team.h
+++ b/include/linux/if_team.h
@@ -247,7 +247,7 @@ static inline struct team_port *team_get_port_by_index(struct team *team,
 
 static inline int team_num_to_port_index(struct team *team, unsigned int num)
 {
-	int en_port_count = ACCESS_ONCE(team->en_port_count);
+	int en_port_count = READ_ONCE(team->en_port_count);
 
 	if (unlikely(!en_port_count))
 		return 0;
diff --git a/include/linux/llist.h b/include/linux/llist.h
index fbf10a0bc095..628df28cfe2f 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -157,7 +157,7 @@ static inline void init_llist_head(struct llist_head *list)
  */
 static inline bool llist_empty(const struct llist_head *head)
 {
-	return ACCESS_ONCE(head->first) == NULL;
+	return READ_ONCE(head->first) == NULL;
 }
 
 static inline struct llist_node *llist_next(struct llist_node *node)
diff --git a/include/linux/pm_runtime.h b/include/linux/pm_runtime.h
index d8461976d94a..700bb90fd423 100644
--- a/include/linux/pm_runtime.h
+++ b/include/linux/pm_runtime.h
@@ -121,7 +121,7 @@ static inline bool pm_runtime_callbacks_present(struct device *dev)
 
 static inline void pm_runtime_mark_last_busy(struct device *dev)
 {
-	ACCESS_ONCE(dev->power.last_busy) = jiffies;
+	WRITE_ONCE(dev->power.last_busy, jiffies);
 }
 
 #else /* !CONFIG_PM_RUNTIME */
diff --git a/include/net/ip_vs.h b/include/net/ip_vs.h
index 2d7cb932edee..c05b8b53b269 100644
--- a/include/net/ip_vs.h
+++ b/include/net/ip_vs.h
@@ -1025,12 +1025,12 @@ static inline int sysctl_sync_threshold(struct netns_ipvs *ipvs)
 
 static inline int sysctl_sync_period(struct netns_ipvs *ipvs)
 {
-	return ACCESS_ONCE(ipvs->sysctl_sync_threshold[1]);
+	return READ_ONCE(ipvs->sysctl_sync_threshold[1]);
 }
 
 static inline unsigned int sysctl_sync_refresh_period(struct netns_ipvs *ipvs)
 {
-	return ACCESS_ONCE(ipvs->sysctl_sync_refresh_period);
+	return READ_ONCE(ipvs->sysctl_sync_refresh_period);
 }
 
 static inline int sysctl_sync_retries(struct netns_ipvs *ipvs)
@@ -1045,7 +1045,7 @@ static inline int sysctl_sync_ver(struct netns_ipvs *ipvs)
 
 static inline int sysctl_sync_ports(struct netns_ipvs *ipvs)
 {
-	return ACCESS_ONCE(ipvs->sysctl_sync_ports);
+	return READ_ONCE(ipvs->sysctl_sync_ports);
 }
 
 static inline int sysctl_sync_qlen_max(struct netns_ipvs *ipvs)
diff --git a/kernel/acct.c b/kernel/acct.c
index 64fc6eba7ae1..a69580e9caae 100644
--- a/kernel/acct.c
+++ b/kernel/acct.c
@@ -144,7 +144,7 @@ static struct bsd_acct_struct *acct_get(struct pid_namespace *ns)
 again:
 	smp_rmb();
 	rcu_read_lock();
-	res = to_acct(ACCESS_ONCE(ns->bacct));
+	res = to_acct(READ_ONCE(ns->bacct));
 	if (!res) {
 		rcu_read_unlock();
 		return NULL;
@@ -156,7 +156,7 @@ again:
 	}
 	rcu_read_unlock();
 	mutex_lock(&res->lock);
-	if (res != to_acct(ACCESS_ONCE(ns->bacct))) {
+	if (res != to_acct(READ_ONCE(ns->bacct))) {
 		mutex_unlock(&res->lock);
 		acct_put(res);
 		goto again;
diff --git a/kernel/events/core.c b/kernel/events/core.c
index e490cd411934..e5beecb3d113 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -1164,7 +1164,7 @@ perf_event_ctx_lock_nested(struct perf_event *event, int nesting)
 
 again:
 	rcu_read_lock();
-	ctx = ACCESS_ONCE(event->ctx);
+	ctx = READ_ONCE(event->ctx);
 	if (!atomic_inc_not_zero(&ctx->refcount)) {
 		rcu_read_unlock();
 		goto again;
@@ -5210,8 +5210,8 @@ static int perf_mmap(struct file *file, struct vm_area_struct *vma)
 		if (!rb)
 			goto aux_unlock;
 
-		aux_offset = ACCESS_ONCE(rb->user_page->aux_offset);
-		aux_size = ACCESS_ONCE(rb->user_page->aux_size);
+		aux_offset = READ_ONCE(rb->user_page->aux_offset);
+		aux_size = READ_ONCE(rb->user_page->aux_size);
 
 		if (aux_offset < perf_data_size(rb) + PAGE_SIZE)
 			goto aux_unlock;
diff --git a/kernel/events/ring_buffer.c b/kernel/events/ring_buffer.c
index 9d599d4bb328..d619100807ae 100644
--- a/kernel/events/ring_buffer.c
+++ b/kernel/events/ring_buffer.c
@@ -381,7 +381,7 @@ void *perf_aux_output_begin(struct perf_output_handle *handle,
 	 * (B) <-> (C) ordering is still observed by the pmu driver.
 	 */
 	if (!rb->aux_overwrite) {
-		aux_tail = ACCESS_ONCE(rb->user_page->aux_tail);
+		aux_tail = READ_ONCE(rb->user_page->aux_tail);
 		handle->wakeup = rb->aux_wakeup + rb->aux_watermark;
 		if (aux_head - aux_tail < perf_aux_size(rb))
 			handle->size = CIRC_SPACE(aux_head, aux_tail, perf_aux_size(rb));
* Unmerged path kernel/exit.c
* Unmerged path kernel/trace/ring_buffer.c
* Unmerged path kernel/trace/trace.h
* Unmerged path kernel/trace/trace_stack.c
diff --git a/kernel/user_namespace.c b/kernel/user_namespace.c
index 94c9b00b2cae..6b8321b86e8d 100644
--- a/kernel/user_namespace.c
+++ b/kernel/user_namespace.c
@@ -906,7 +906,7 @@ static bool new_idmap_permitted(const struct file *file,
 int proc_setgroups_show(struct seq_file *seq, void *v)
 {
 	struct user_namespace *ns = seq->private;
-	unsigned long userns_flags = ACCESS_ONCE(ns->flags);
+	unsigned long userns_flags = READ_ONCE(ns->flags);
 
 	seq_printf(seq, "%s\n",
 		   (userns_flags & USERNS_SETGROUPS_ALLOWED) ?
diff --git a/lib/assoc_array.c b/lib/assoc_array.c
index 035c2364e60b..aabde3d13d64 100644
--- a/lib/assoc_array.c
+++ b/lib/assoc_array.c
@@ -38,7 +38,7 @@ begin_node:
 		/* Descend through a shortcut */
 		shortcut = assoc_array_ptr_to_shortcut(cursor);
 		smp_read_barrier_depends();
-		cursor = ACCESS_ONCE(shortcut->next_node);
+		cursor = READ_ONCE(shortcut->next_node);
 	}
 
 	node = assoc_array_ptr_to_node(cursor);
@@ -54,7 +54,7 @@ begin_node:
 	 */
 	has_meta = 0;
 	for (; slot < ASSOC_ARRAY_FAN_OUT; slot++) {
-		ptr = ACCESS_ONCE(node->slots[slot]);
+		ptr = READ_ONCE(node->slots[slot]);
 		has_meta |= (unsigned long)ptr;
 		if (ptr && assoc_array_ptr_is_leaf(ptr)) {
 			/* We need a barrier between the read of the pointer
@@ -88,7 +88,7 @@ continue_node:
 	smp_read_barrier_depends();
 
 	for (; slot < ASSOC_ARRAY_FAN_OUT; slot++) {
-		ptr = ACCESS_ONCE(node->slots[slot]);
+		ptr = READ_ONCE(node->slots[slot]);
 		if (assoc_array_ptr_is_meta(ptr)) {
 			cursor = ptr;
 			goto begin_node;
@@ -97,7 +97,7 @@ continue_node:
 
 finished_node:
 	/* Move up to the parent (may need to skip back over a shortcut) */
-	parent = ACCESS_ONCE(node->back_pointer);
+	parent = READ_ONCE(node->back_pointer);
 	slot = node->parent_slot;
 	if (parent == stop)
 		return 0;
@@ -106,7 +106,7 @@ finished_node:
 		shortcut = assoc_array_ptr_to_shortcut(parent);
 		smp_read_barrier_depends();
 		cursor = parent;
-		parent = ACCESS_ONCE(shortcut->back_pointer);
+		parent = READ_ONCE(shortcut->back_pointer);
 		slot = shortcut->parent_slot;
 		if (parent == stop)
 			return 0;
@@ -146,7 +146,7 @@ int assoc_array_iterate(const struct assoc_array *array,
 					void *iterator_data),
 			void *iterator_data)
 {
-	struct assoc_array_ptr *root = ACCESS_ONCE(array->root);
+	struct assoc_array_ptr *root = READ_ONCE(array->root);
 
 	if (!root)
 		return 0;
@@ -193,7 +193,7 @@ assoc_array_walk(const struct assoc_array *array,
 
 	pr_devel("-->%s()\n", __func__);
 
-	cursor = ACCESS_ONCE(array->root);
+	cursor = READ_ONCE(array->root);
 	if (!cursor)
 		return assoc_array_walk_tree_empty;
 
@@ -219,7 +219,7 @@ consider_node:
 
 	slot = segments >> (level & ASSOC_ARRAY_KEY_CHUNK_MASK);
 	slot &= ASSOC_ARRAY_FAN_MASK;
-	ptr = ACCESS_ONCE(node->slots[slot]);
+	ptr = READ_ONCE(node->slots[slot]);
 
 	pr_devel("consider slot %x [ix=%d type=%lu]\n",
 		 slot, level, (unsigned long)ptr & 3);
@@ -293,7 +293,7 @@ follow_shortcut:
 	} while (sc_level < shortcut->skip_to_level);
 
 	/* The shortcut matches the leaf's index to this point. */
-	cursor = ACCESS_ONCE(shortcut->next_node);
+	cursor = READ_ONCE(shortcut->next_node);
 	if (((level ^ sc_level) & ~ASSOC_ARRAY_KEY_CHUNK_MASK) != 0) {
 		level = sc_level;
 		goto jumped;
@@ -336,7 +336,7 @@ void *assoc_array_find(const struct assoc_array *array,
 	 * the terminal node.
 	 */
 	for (slot = 0; slot < ASSOC_ARRAY_FAN_OUT; slot++) {
-		ptr = ACCESS_ONCE(node->slots[slot]);
+		ptr = READ_ONCE(node->slots[slot]);
 		if (ptr && assoc_array_ptr_is_leaf(ptr)) {
 			/* We need a barrier between the read of the pointer
 			 * and dereferencing the pointer - but only if we are
diff --git a/lib/dynamic_queue_limits.c b/lib/dynamic_queue_limits.c
index 0777c5a45fa0..584d314b73a9 100644
--- a/lib/dynamic_queue_limits.c
+++ b/lib/dynamic_queue_limits.c
@@ -20,7 +20,7 @@ void dql_completed(struct dql *dql, unsigned int count)
 	unsigned int ovlimit, completed, num_queued;
 	bool all_prev_completed;
 
-	num_queued = ACCESS_ONCE(dql->num_queued);
+	num_queued = READ_ONCE(dql->num_queued);
 
 	/* Can't complete more than what's in queue */
 	BUG_ON(count > num_queued - dql->num_completed);
diff --git a/lib/llist.c b/lib/llist.c
index f76196d07409..54908c917836 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -42,7 +42,7 @@ bool llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 	struct llist_node *first;
 
 	do {
-		new_last->next = first = ACCESS_ONCE(head->first);
+		new_last->next = first = READ_ONCE(head->first);
 	} while (cmpxchg(&head->first, first, new_first) != first);
 
 	return !first;
diff --git a/lib/vsprintf.c b/lib/vsprintf.c
index 21f2f53f7cfa..ff9f5f3e5588 100644
--- a/lib/vsprintf.c
+++ b/lib/vsprintf.c
@@ -573,8 +573,8 @@ char *dentry_name(char *buf, char *end, const struct dentry *d, struct printf_sp
 
 	rcu_read_lock();
 	for (i = 0; i < depth; i++, d = p) {
-		p = ACCESS_ONCE(d->d_parent);
-		array[i] = ACCESS_ONCE(d->d_name.name);
+		p = READ_ONCE(d->d_parent);
+		array[i] = READ_ONCE(d->d_name.name);
 		if (p == d) {
 			if (i)
 				array[i] = "";
* Unmerged path mm/huge_memory.c
* Unmerged path net/core/dev.c
diff --git a/net/core/pktgen.c b/net/core/pktgen.c
index d560022368d0..44a4176f743f 100644
--- a/net/core/pktgen.c
+++ b/net/core/pktgen.c
@@ -3288,7 +3288,7 @@ static void pktgen_wait_for_skb(struct pktgen_dev *pkt_dev)
 
 static void pktgen_xmit(struct pktgen_dev *pkt_dev)
 {
-	unsigned int burst = ACCESS_ONCE(pkt_dev->burst);
+	unsigned int burst = READ_ONCE(pkt_dev->burst);
 	struct net_device *odev = pkt_dev->odev;
 	struct netdev_queue *txq;
 	int ret;
* Unmerged path net/ipv4/inet_fragment.c
* Unmerged path net/ipv4/route.c
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index bbae85f17be4..83dc7ac597aa 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -1772,7 +1772,7 @@ static bool tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb,
 	if ((skb != tcp_write_queue_tail(sk)) && (limit >= skb->len))
 		goto send_now;
 
-	win_divisor = ACCESS_ONCE(sysctl_tcp_tso_win_divisor);
+	win_divisor = READ_ONCE(sysctl_tcp_tso_win_divisor);
 	if (win_divisor) {
 		u32 chunk = min(tp->snd_wnd, tp->snd_cwnd * tp->mss_cache);
 
diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index 46564863ba64..766684c431bc 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -1644,7 +1644,7 @@ int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)
 		 */
 
 		/* if we're overly short, let UDP handle it */
-		encap_rcv = ACCESS_ONCE(up->encap_rcv);
+		encap_rcv = READ_ONCE(up->encap_rcv);
 		if (encap_rcv) {
 			int ret;
 
@@ -2075,7 +2075,7 @@ void udp_destroy_sock(struct sock *sk)
 	unlock_sock_fast(sk, slow);
 	if (static_key_false(&udp_encap_needed) && up->encap_type) {
 		void (*encap_destroy)(struct sock *sk);
-		encap_destroy = ACCESS_ONCE(up->encap_destroy);
+		encap_destroy = READ_ONCE(up->encap_destroy);
 		if (encap_destroy)
 			encap_destroy(sk);
 	}
diff --git a/net/ipv6/ip6_tunnel.c b/net/ipv6/ip6_tunnel.c
index a12e6c346803..55a34d007d33 100644
--- a/net/ipv6/ip6_tunnel.c
+++ b/net/ipv6/ip6_tunnel.c
@@ -479,7 +479,7 @@ ip6_tnl_err(struct sk_buff *skb, __u8 ipproto, struct inet6_skb_parm *opt,
 					&ipv6h->saddr)) == NULL)
 		goto out;
 
-	tproto = ACCESS_ONCE(t->parms.proto);
+	tproto = READ_ONCE(t->parms.proto);
 	if (tproto != ipproto && tproto != 0)
 		goto out;
 
@@ -885,7 +885,7 @@ static int ipxip6_rcv(struct sk_buff *skb, u8 ipproto,
 	t = ip6_tnl_lookup(dev_net(skb->dev), &ipv6h->saddr, &ipv6h->daddr);
 
 	if (t) {
-		u8 tproto = ACCESS_ONCE(t->parms.proto);
+		u8 tproto = READ_ONCE(t->parms.proto);
 
 		if (tproto != ipproto && tproto != 0)
 			goto drop;
@@ -1186,7 +1186,7 @@ ip4ip6_tnl_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	memset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));
 
-	tproto = ACCESS_ONCE(t->parms.proto);
+	tproto = READ_ONCE(t->parms.proto);
 	if (tproto != IPPROTO_IPIP && tproto != 0)
 		return -1;
 
@@ -1230,7 +1230,7 @@ ip6ip6_tnl_xmit(struct sk_buff *skb, struct net_device *dev)
 	u8 tproto;
 	int err;
 
-	tproto = ACCESS_ONCE(t->parms.proto);
+	tproto = READ_ONCE(t->parms.proto);
 	if ((tproto != IPPROTO_IPV6 && tproto != 0) ||
 	    ip6_tnl_addr_conflict(t, ipv6h))
 		return -1;
diff --git a/net/ipv6/udp.c b/net/ipv6/udp.c
index 258fc63c51f4..ddb1618fedf6 100644
--- a/net/ipv6/udp.c
+++ b/net/ipv6/udp.c
@@ -616,7 +616,7 @@ int udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)
 		 */
 
 		/* if we're overly short, let UDP handle it */
-		encap_rcv = ACCESS_ONCE(up->encap_rcv);
+		encap_rcv = READ_ONCE(up->encap_rcv);
 		if (encap_rcv) {
 			int ret;
 
@@ -1353,7 +1353,7 @@ void udpv6_destroy_sock(struct sock *sk)
 
 	if (static_key_false(&udpv6_encap_needed) && up->encap_type) {
 		void (*encap_destroy)(struct sock *sk);
-		encap_destroy = ACCESS_ONCE(up->encap_destroy);
+		encap_destroy = READ_ONCE(up->encap_destroy);
 		if (encap_destroy)
 			encap_destroy(sk);
 	}
diff --git a/net/llc/llc_input.c b/net/llc/llc_input.c
index dd3e83328ad5..82cb93f66b9b 100644
--- a/net/llc/llc_input.c
+++ b/net/llc/llc_input.c
@@ -193,7 +193,7 @@ int llc_rcv(struct sk_buff *skb, struct net_device *dev,
 	 */
 	rcv = rcu_dereference(sap->rcv_func);
 	dest = llc_pdu_type(skb);
-	sap_handler = dest ? ACCESS_ONCE(llc_type_handlers[dest - 1]) : NULL;
+	sap_handler = dest ? READ_ONCE(llc_type_handlers[dest - 1]) : NULL;
 	if (unlikely(!sap_handler)) {
 		if (rcv)
 			rcv(skb, dev, pt, orig_dev);
@@ -214,7 +214,7 @@ drop:
 	kfree_skb(skb);
 	goto out;
 handle_station:
-	sta_handler = ACCESS_ONCE(llc_station_handler);
+	sta_handler = READ_ONCE(llc_station_handler);
 	if (!sta_handler)
 		goto drop;
 	sta_handler(skb);
diff --git a/net/mac80211/sta_info.c b/net/mac80211/sta_info.c
index ef350dabb63e..3e39c6c0cdea 100644
--- a/net/mac80211/sta_info.c
+++ b/net/mac80211/sta_info.c
@@ -2017,7 +2017,7 @@ static void sta_stats_decode_rate(struct ieee80211_local *local, u16 rate,
 
 static int sta_set_rate_info_rx(struct sta_info *sta, struct rate_info *rinfo)
 {
-	u16 rate = ACCESS_ONCE(sta_get_last_rx_stats(sta)->last_rate);
+	u16 rate = READ_ONCE(sta_get_last_rx_stats(sta)->last_rate);
 
 	if (rate == STA_STATS_RATE_INVALID)
 		return -EINVAL;
* Unmerged path net/netlabel/netlabel_calipso.c
diff --git a/net/wireless/nl80211.c b/net/wireless/nl80211.c
index 0ce553ea988c..8061944a2448 100644
--- a/net/wireless/nl80211.c
+++ b/net/wireless/nl80211.c
@@ -14194,7 +14194,7 @@ static bool __nl80211_unexpected_frame(struct net_device *dev, u8 cmd,
 	struct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);
 	struct sk_buff *msg;
 	void *hdr;
-	u32 nlportid = ACCESS_ONCE(wdev->ap_unexpected_nlportid);
+	u32 nlportid = READ_ONCE(wdev->ap_unexpected_nlportid);
 
 	if (!nlportid)
 		return false;
* Unmerged path sound/firewire/amdtp-am824.c
* Unmerged path sound/firewire/amdtp-stream.c
* Unmerged path sound/firewire/amdtp-stream.h
* Unmerged path sound/firewire/digi00x/amdtp-dot.c
* Unmerged path sound/firewire/fireface/amdtp-ff.c
* Unmerged path sound/firewire/fireface/ff-midi.c
* Unmerged path sound/firewire/fireface/ff-transaction.c
diff --git a/sound/firewire/isight.c b/sound/firewire/isight.c
index 36693001467f..73d21a911a40 100644
--- a/sound/firewire/isight.c
+++ b/sound/firewire/isight.c
@@ -96,7 +96,7 @@ static void isight_update_pointers(struct isight *isight, unsigned int count)
 	ptr += count;
 	if (ptr >= runtime->buffer_size)
 		ptr -= runtime->buffer_size;
-	ACCESS_ONCE(isight->buffer_pointer) = ptr;
+	WRITE_ONCE(isight->buffer_pointer, ptr);
 
 	isight->period_counter += count;
 	if (isight->period_counter >= runtime->period_size) {
@@ -111,7 +111,7 @@ static void isight_samples(struct isight *isight,
 	struct snd_pcm_runtime *runtime;
 	unsigned int count1;
 
-	if (!ACCESS_ONCE(isight->pcm_running))
+	if (!READ_ONCE(isight->pcm_running))
 		return;
 
 	runtime = isight->pcm->runtime;
@@ -131,7 +131,7 @@ static void isight_samples(struct isight *isight,
 
 static void isight_pcm_abort(struct isight *isight)
 {
-	if (ACCESS_ONCE(isight->pcm_active))
+	if (READ_ONCE(isight->pcm_active))
 		snd_pcm_stop_xrun(isight->pcm);
 }
 
@@ -141,7 +141,7 @@ static void isight_dropped_samples(struct isight *isight, unsigned int total)
 	u32 dropped;
 	unsigned int count1;
 
-	if (!ACCESS_ONCE(isight->pcm_running))
+	if (!READ_ONCE(isight->pcm_running))
 		return;
 
 	runtime = isight->pcm->runtime;
@@ -301,7 +301,7 @@ static int isight_hw_params(struct snd_pcm_substream *substream,
 	if (err < 0)
 		return err;
 
-	ACCESS_ONCE(isight->pcm_active) = true;
+	WRITE_ONCE(isight->pcm_active, true);
 
 	return 0;
 }
@@ -334,7 +334,7 @@ static int isight_hw_free(struct snd_pcm_substream *substream)
 {
 	struct isight *isight = substream->private_data;
 
-	ACCESS_ONCE(isight->pcm_active) = false;
+	WRITE_ONCE(isight->pcm_active, false);
 
 	mutex_lock(&isight->mutex);
 	isight_stop_streaming(isight);
@@ -427,10 +427,10 @@ static int isight_trigger(struct snd_pcm_substream *substream, int cmd)
 
 	switch (cmd) {
 	case SNDRV_PCM_TRIGGER_START:
-		ACCESS_ONCE(isight->pcm_running) = true;
+		WRITE_ONCE(isight->pcm_running, true);
 		break;
 	case SNDRV_PCM_TRIGGER_STOP:
-		ACCESS_ONCE(isight->pcm_running) = false;
+		WRITE_ONCE(isight->pcm_running, false);
 		break;
 	default:
 		return -EINVAL;
@@ -442,7 +442,7 @@ static snd_pcm_uframes_t isight_pointer(struct snd_pcm_substream *substream)
 {
 	struct isight *isight = substream->private_data;
 
-	return ACCESS_ONCE(isight->buffer_pointer);
+	return READ_ONCE(isight->buffer_pointer);
 }
 
 static int isight_create_pcm(struct isight *isight)
* Unmerged path sound/firewire/motu/amdtp-motu.c
* Unmerged path sound/firewire/oxfw/oxfw-scs1x.c
* Unmerged path sound/firewire/tascam/amdtp-tascam.c
* Unmerged path sound/firewire/tascam/tascam-transaction.c
* Unmerged path sound/soc/xtensa/xtfpga-i2s.c
diff --git a/sound/usb/bcd2000/bcd2000.c b/sound/usb/bcd2000/bcd2000.c
index 7371e5b06035..fc579f330601 100644
--- a/sound/usb/bcd2000/bcd2000.c
+++ b/sound/usb/bcd2000/bcd2000.c
@@ -108,7 +108,7 @@ static void bcd2000_midi_handle_input(struct bcd2000 *bcd2k,
 	unsigned int payload_length, tocopy;
 	struct snd_rawmidi_substream *midi_receive_substream;
 
-	midi_receive_substream = ACCESS_ONCE(bcd2k->midi_receive_substream);
+	midi_receive_substream = READ_ONCE(bcd2k->midi_receive_substream);
 	if (!midi_receive_substream)
 		return;
 
@@ -139,7 +139,7 @@ static void bcd2000_midi_send(struct bcd2000 *bcd2k)
 
 	BUILD_BUG_ON(sizeof(device_cmd_prefix) >= BUFSIZE);
 
-	midi_out_substream = ACCESS_ONCE(bcd2k->midi_out_substream);
+	midi_out_substream = READ_ONCE(bcd2k->midi_out_substream);
 	if (!midi_out_substream)
 		return;
 
diff --git a/tools/arch/x86/include/asm/atomic.h b/tools/arch/x86/include/asm/atomic.h
index 328eeceec709..96e2d06cb031 100644
--- a/tools/arch/x86/include/asm/atomic.h
+++ b/tools/arch/x86/include/asm/atomic.h
@@ -24,7 +24,7 @@
  */
 static inline int atomic_read(const atomic_t *v)
 {
-	return ACCESS_ONCE((v)->counter);
+	return READ_ONCE((v)->counter);
 }
 
 /**
diff --git a/tools/include/asm-generic/atomic-gcc.h b/tools/include/asm-generic/atomic-gcc.h
index 5e9738f97bf3..97427e700e3b 100644
--- a/tools/include/asm-generic/atomic-gcc.h
+++ b/tools/include/asm-generic/atomic-gcc.h
@@ -21,7 +21,7 @@
  */
 static inline int atomic_read(const atomic_t *v)
 {
-	return ACCESS_ONCE((v)->counter);
+	return READ_ONCE((v)->counter);
 }
 
 /**
diff --git a/tools/perf/util/auxtrace.h b/tools/perf/util/auxtrace.h
index f5594e8722cf..9ccc504c7d5d 100644
--- a/tools/perf/util/auxtrace.h
+++ b/tools/perf/util/auxtrace.h
@@ -377,7 +377,7 @@ struct addr_filters {
 static inline u64 auxtrace_mmap__read_snapshot_head(struct auxtrace_mmap *mm)
 {
 	struct perf_event_mmap_page *pc = mm->userpg;
-	u64 head = ACCESS_ONCE(pc->aux_head);
+	u64 head = READ_ONCE(pc->aux_head);
 
 	/* Ensure all reads are done after we read the head */
 	rmb();
@@ -388,7 +388,7 @@ static inline u64 auxtrace_mmap__read_head(struct auxtrace_mmap *mm)
 {
 	struct perf_event_mmap_page *pc = mm->userpg;
 #if BITS_PER_LONG == 64 || !defined(HAVE_SYNC_COMPARE_AND_SWAP_SUPPORT)
-	u64 head = ACCESS_ONCE(pc->aux_head);
+	u64 head = READ_ONCE(pc->aux_head);
 #else
 	u64 head = __sync_val_compare_and_swap(&pc->aux_head, 0, 0);
 #endif
diff --git a/tools/perf/util/session.h b/tools/perf/util/session.h
index 5ac96c782cd4..898b4e1d00e0 100644
--- a/tools/perf/util/session.h
+++ b/tools/perf/util/session.h
@@ -113,7 +113,7 @@ int __perf_session__set_tracepoints_handlers(struct perf_session *session,
 
 extern volatile int session_done;
 
-#define session_done()	ACCESS_ONCE(session_done)
+#define session_done()	READ_ONCE(session_done)
 
 int perf_session__deliver_synth_event(struct perf_session *session,
 				      union perf_event *event,
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 365126b76acd..4ba9ef615a7d 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2260,7 +2260,7 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *me)
 				continue;
 			} else if (pass && i > last_boosted_vcpu)
 				break;
-			if (!ACCESS_ONCE(vcpu->preempted))
+			if (!READ_ONCE(vcpu->preempted))
 				continue;
 			if (vcpu == me)
 				continue;
