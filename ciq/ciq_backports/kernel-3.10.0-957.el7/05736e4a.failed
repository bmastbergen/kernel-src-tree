cpu/hotplug: Provide knobs to control SMT

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [kernel] cpu/hotplug: provide knobs to control smt, part 2 (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 91.11%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 05736e4ac13c08a4a9b1ef2de26dd31a32cbee57
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/05736e4a.failed

Provide a command line and a sysfs knob to control SMT.

The command line options are:

 'nosmt':	Enumerate secondary threads, but do not online them
 		
 'nosmt=force': Ignore secondary threads completely during enumeration
 		via MP table and ACPI/MADT.

The sysfs control file has the following states (read/write):

 'on':		 SMT is enabled. Secondary threads can be freely onlined
 'off':		 SMT is disabled. Secondary threads, even if enumerated
 		 cannot be onlined
 'forceoff':	 SMT is permanentely disabled. Writes to the control
 		 file are rejected.
 'notsupported': SMT is not supported by the CPU

The command line option 'nosmt' sets the sysfs control to 'off'. This
can be changed to 'on' to reenable SMT during runtime.

The command line option 'nosmt=force' sets the sysfs control to
'forceoff'. This cannot be changed during runtime.

When SMT is 'on' and the control file is changed to 'off' then all online
secondary threads are offlined and attempts to online a secondary thread
later on are rejected.

When SMT is 'off' and the control file is changed to 'on' then secondary
threads can be onlined again. The 'off' -> 'on' transition does not
automatically online the secondary threads.

When the control file is set to 'forceoff', the behaviour is the same as
setting it to 'off', but the operation is irreversible and later writes to
the control file are rejected.

When the control status is 'notsupported' then writes to the control file
are rejected.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Acked-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 05736e4ac13c08a4a9b1ef2de26dd31a32cbee57)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/ABI/testing/sysfs-devices-system-cpu
#	Documentation/kernel-parameters.txt
#	arch/Kconfig
#	arch/x86/Kconfig
#	kernel/cpu.c
diff --cc Documentation/ABI/testing/sysfs-devices-system-cpu
index 7b467f328883,65d9b844ecfd..000000000000
--- a/Documentation/ABI/testing/sysfs-devices-system-cpu
+++ b/Documentation/ABI/testing/sysfs-devices-system-cpu
@@@ -284,4 -486,24 +284,28 @@@ Description:	Information about CPU vuln
  
  		"Not affected"	  CPU is not affected by the vulnerability
  		"Vulnerable"	  CPU is affected and no mitigation in effect
++<<<<<<< HEAD
 +		"Mitigation: $M"  CPU is affetcted and mitigation $M is in effect
++=======
+ 		"Mitigation: $M"  CPU is affected and mitigation $M is in effect
+ 
+ What:		/sys/devices/system/cpu/smt
+ 		/sys/devices/system/cpu/smt/active
+ 		/sys/devices/system/cpu/smt/control
+ Date:		June 2018
+ Contact:	Linux kernel mailing list <linux-kernel@vger.kernel.org>
+ Description:	Control Symetric Multi Threading (SMT)
+ 
+ 		active:  Tells whether SMT is active (enabled and siblings online)
+ 
+ 		control: Read/write interface to control SMT. Possible
+ 			 values:
+ 
+ 			 "on"		SMT is enabled
+ 			 "off"		SMT is disabled
+ 			 "forceoff"	SMT is force disabled. Cannot be changed.
+ 			 "notsupported" SMT is not supported by the CPU
+ 
+ 			 If control status is "forceoff" or "notsupported" writes
+ 			 are rejected.
++>>>>>>> 05736e4ac13c (cpu/hotplug: Provide knobs to control SMT)
diff --cc Documentation/kernel-parameters.txt
index a0f1f1428af0,8e29c4b6756f..000000000000
--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@@ -2203,6 -2684,17 +2203,20 @@@ bytes respectively. Such letter suffixe
  
  	nohugeiomap	[KNL,x86] Disable kernel huge I/O mappings.
  
++<<<<<<< HEAD:Documentation/kernel-parameters.txt
++=======
+ 	nosmt		[KNL,S390] Disable symmetric multithreading (SMT).
+ 			Equivalent to smt=1.
+ 
+ 			[KNL,x86] Disable symmetric multithreading (SMT).
+ 			nosmt=force: Force disable SMT, similar to disabling
+ 				     it in the BIOS except that some of the
+ 				     resource partitioning effects which are
+ 				     caused by having SMT enabled in the BIOS
+ 				     cannot be undone. Depending on the CPU
+ 				     type this might have a performance impact.
+ 
++>>>>>>> 05736e4ac13c (cpu/hotplug: Provide knobs to control SMT):Documentation/admin-guide/kernel-parameters.txt
  	nospectre_v2	[X86] Disable all mitigations for the Spectre variant 2
  			(indirect branch prediction) vulnerability. System may
  			allow data leaks with this option, which is equivalent
diff --cc arch/Kconfig
index d766cae653ce,d1f2ed462ac8..000000000000
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@@ -9,6 -10,12 +9,15 @@@ config KEXEC_COR
  	select CRASH_CORE
  	bool
  
++<<<<<<< HEAD
++=======
+ config HAVE_IMA_KEXEC
+ 	bool
+ 
+ config HOTPLUG_SMT
+ 	bool
+ 
++>>>>>>> 05736e4ac13c (cpu/hotplug: Provide knobs to control SMT)
  config OPROFILE
  	tristate "OProfile system profiling"
  	depends on PROFILING
diff --cc arch/x86/Kconfig
index 48ae09959d87,7a34fdf8daf0..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -15,132 -22,185 +15,142 @@@ config X86_3
  config X86_64
  	def_bool y
  	depends on 64BIT
 -	# Options that are inherently 64-bit kernel only:
 -	select ARCH_HAS_GIGANTIC_PAGE if (MEMORY_ISOLATION && COMPACTION) || CMA
 -	select ARCH_SUPPORTS_INT128
 -	select ARCH_USE_CMPXCHG_LOCKREF
 -	select HAVE_ARCH_SOFT_DIRTY
 -	select MODULES_USE_ELF_RELA
 -	select NEED_DMA_MAP_STATE
 -	select SWIOTLB
  	select X86_DEV_DMA_OPS
 -	select ARCH_HAS_SYSCALL_WRAPPER
 -
 -#
 -# Arch settings
 -#
 -# ( Note that options that are marked 'if X86_64' could in principle be
 -#   ported to 32-bit as well. )
 -#
 +	select ARCH_USE_CMPXCHG_LOCKREF
 +	select HAVE_LIVEPATCH
 +
 +### Arch settings
  config X86
  	def_bool y
 -	#
 -	# Note: keep this list sorted alphabetically
 -	#
 -	select ACPI_LEGACY_TABLES_LOOKUP	if ACPI
 -	select ACPI_SYSTEM_POWER_STATES_SUPPORT	if ACPI
 -	select ANON_INODES
 -	select ARCH_CLOCKSOURCE_DATA
 -	select ARCH_DISCARD_MEMBLOCK
 -	select ARCH_HAS_ACPI_TABLE_UPGRADE	if ACPI
 -	select ARCH_HAS_DEBUG_VIRTUAL
 -	select ARCH_HAS_DEVMEM_IS_ALLOWED
 -	select ARCH_HAS_ELF_RANDOMIZE
 -	select ARCH_HAS_FAST_MULTIPLIER
 -	select ARCH_HAS_FILTER_PGPROT
 -	select ARCH_HAS_FORTIFY_SOURCE
 -	select ARCH_HAS_GCOV_PROFILE_ALL
 -	select ARCH_HAS_KCOV			if X86_64
 -	select ARCH_HAS_MEMBARRIER_SYNC_CORE
 +	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
  	select ARCH_HAS_PMEM_API		if X86_64
 -	select ARCH_HAS_PTE_SPECIAL
 -	select ARCH_HAS_REFCOUNT
  	select ARCH_HAS_UACCESS_FLUSHCACHE	if X86_64
 -	select ARCH_HAS_UACCESS_MCSAFE		if X86_64
 -	select ARCH_HAS_SET_MEMORY
 -	select ARCH_HAS_SG_CHAIN
 -	select ARCH_HAS_STRICT_KERNEL_RWX
 -	select ARCH_HAS_STRICT_MODULE_RWX
 -	select ARCH_HAS_SYNC_CORE_BEFORE_USERMODE
 -	select ARCH_HAS_UBSAN_SANITIZE_ALL
 -	select ARCH_HAS_ZONE_DEVICE		if X86_64
 -	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 -	select ARCH_MIGHT_HAVE_ACPI_PDC		if ACPI
 -	select ARCH_MIGHT_HAVE_PC_PARPORT
 -	select ARCH_MIGHT_HAVE_PC_SERIO
 -	select ARCH_SUPPORTS_ATOMIC_RMW
 -	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
 -	select ARCH_USE_BUILTIN_BSWAP
 -	select ARCH_USE_QUEUED_RWLOCKS
 -	select ARCH_USE_QUEUED_SPINLOCKS
 -	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 -	select ARCH_WANTS_DYNAMIC_TASK_STRUCT
 -	select ARCH_WANTS_THP_SWAP		if X86_64
 -	select BUILDTIME_EXTABLE_SORT
 -	select CLKEVT_I8253
 -	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
 -	select CLOCKSOURCE_WATCHDOG
 -	select DCACHE_WORD_ACCESS
 -	select DMA_DIRECT_OPS
 -	select EDAC_ATOMIC_SCRUB
 -	select EDAC_SUPPORT
 -	select GENERIC_CLOCKEVENTS
 -	select GENERIC_CLOCKEVENTS_BROADCAST	if X86_64 || (X86_32 && X86_LOCAL_APIC)
 -	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 -	select GENERIC_CMOS_UPDATE
 -	select GENERIC_CPU_AUTOPROBE
 -	select GENERIC_CPU_VULNERABILITIES
 -	select GENERIC_EARLY_IOREMAP
 -	select GENERIC_FIND_FIRST_BIT
 -	select GENERIC_IOMAP
 -	select GENERIC_IRQ_EFFECTIVE_AFF_MASK	if SMP
 -	select GENERIC_IRQ_MATRIX_ALLOCATOR	if X86_LOCAL_APIC
 -	select GENERIC_IRQ_MIGRATION		if SMP
 -	select GENERIC_IRQ_PROBE
 -	select GENERIC_IRQ_RESERVATION_MODE
 -	select GENERIC_IRQ_SHOW
 -	select GENERIC_PENDING_IRQ		if SMP
 -	select GENERIC_SMP_IDLE_THREAD
 -	select GENERIC_STRNCPY_FROM_USER
 -	select GENERIC_STRNLEN_USER
 -	select GENERIC_TIME_VSYSCALL
 -	select HARDLOCKUP_CHECK_TIMESTAMP	if X86_64
 -	select HAVE_ACPI_APEI			if ACPI
 -	select HAVE_ACPI_APEI_NMI		if ACPI
 -	select HAVE_ALIGNED_STRUCT_PAGE		if SLUB
 -	select HAVE_ARCH_AUDITSYSCALL
 -	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
 -	select HAVE_ARCH_JUMP_LABEL
 -	select HAVE_ARCH_KASAN			if X86_64
 -	select HAVE_ARCH_KGDB
 +	select ARCH_HAS_MMIO_FLUSH
 +	select HAVE_AOUT if X86_32
 +	select HAVE_UNSTABLE_SCHED_CLOCK
 +	select ARCH_SUPPORTS_NUMA_BALANCING
 +	select ARCH_SUPPORTS_INT128 if X86_64
 +	select ARCH_WANTS_PROT_NUMA_PROT_NONE
 +	select HAVE_IDE
 +	select HAVE_OPROFILE
 +	select HAVE_PCSPKR_PLATFORM
 +	select HAVE_PERF_EVENTS
 +	select HAVE_IOREMAP_PROT
 +	select HAVE_KPROBES
 +	select HAVE_MEMBLOCK
 +	select HAVE_MEMBLOCK_NODE_MAP
 +	select ARCH_DISCARD_MEMBLOCK
 +	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 +	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
 +	select ARCH_WANT_OPTIONAL_GPIOLIB
 +	select ARCH_WANT_FRAME_POINTERS
 +	select HAVE_DMA_CONTIGUOUS if !SWIOTLB
 +	select HAVE_KRETPROBES
 +	select HAVE_OPTPROBES
 +	select HAVE_KPROBES_ON_FTRACE
 +	select HAVE_FTRACE_MCOUNT_RECORD
 +	select HAVE_FENTRY if X86_64
  	select HAVE_ARCH_MMAP_RND_BITS		if MMU
  	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
 -	select HAVE_ARCH_COMPAT_MMAP_BASES	if MMU && COMPAT
 -	select HAVE_ARCH_SECCOMP_FILTER
 -	select HAVE_ARCH_THREAD_STRUCT_WHITELIST
 -	select HAVE_ARCH_TRACEHOOK
 -	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 -	select HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD if X86_64
 -	select HAVE_ARCH_VMAP_STACK		if X86_64
 -	select HAVE_ARCH_WITHIN_STACK_FRAMES
 -	select HAVE_CMPXCHG_DOUBLE
 -	select HAVE_CMPXCHG_LOCAL
 -	select HAVE_CONTEXT_TRACKING		if X86_64
 -	select HAVE_COPY_THREAD_TLS
  	select HAVE_C_RECORDMCOUNT
 -	select HAVE_DEBUG_KMEMLEAK
 -	select HAVE_DEBUG_STACKOVERFLOW
 -	select HAVE_DMA_CONTIGUOUS
  	select HAVE_DYNAMIC_FTRACE
  	select HAVE_DYNAMIC_FTRACE_WITH_REGS
 -	select HAVE_EBPF_JIT
 -	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 -	select HAVE_EXIT_THREAD
 -	select HAVE_FENTRY			if X86_64 || DYNAMIC_FTRACE
 -	select HAVE_FTRACE_MCOUNT_RECORD
 -	select HAVE_FUNCTION_GRAPH_TRACER
  	select HAVE_FUNCTION_TRACER
 -	select HAVE_GCC_PLUGINS
 -	select HAVE_HW_BREAKPOINT
 -	select HAVE_IDE
 -	select HAVE_IOREMAP_PROT
 -	select HAVE_IRQ_EXIT_ON_IRQ_STACK	if X86_64
 -	select HAVE_IRQ_TIME_ACCOUNTING
 -	select HAVE_KERNEL_BZIP2
 +	select HAVE_FUNCTION_GRAPH_TRACER
 +	select HAVE_SYSCALL_TRACEPOINTS
 +	select SYSCTL_EXCEPTION_TRACE
 +	select HAVE_KVM
 +	select HAVE_ARCH_KGDB
 +	select HAVE_ARCH_TRACEHOOK
 +	select HAVE_GENERIC_DMA_COHERENT if X86_32
 +	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 +	select USER_STACKTRACE_SUPPORT
 +	select HAVE_REGS_AND_STACK_ACCESS_API
 +	select HAVE_DMA_API_DEBUG
  	select HAVE_KERNEL_GZIP
 -	select HAVE_KERNEL_LZ4
 +	select HAVE_KERNEL_BZIP2
  	select HAVE_KERNEL_LZMA
 -	select HAVE_KERNEL_LZO
  	select HAVE_KERNEL_XZ
 -	select HAVE_KPROBES
 -	select HAVE_KPROBES_ON_FTRACE
 -	select HAVE_FUNCTION_ERROR_INJECTION
 -	select HAVE_KRETPROBES
 -	select HAVE_KVM
 -	select HAVE_LIVEPATCH			if X86_64
 -	select HAVE_MEMBLOCK
 -	select HAVE_MEMBLOCK_NODE_MAP
 +	select HAVE_KERNEL_LZO
 +	select HAVE_HW_BREAKPOINT
  	select HAVE_MIXED_BREAKPOINTS_REGS
 -	select HAVE_MOD_ARCH_SPECIFIC
 -	select HAVE_NMI
 -	select HAVE_OPROFILE
 -	select HAVE_OPTPROBES
 -	select HAVE_PCSPKR_PLATFORM
 -	select HAVE_PERF_EVENTS
 +	select PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
 -	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
 +	select HAVE_DEBUG_KMEMLEAK
 +	select ANON_INODES
 +	select HAVE_ALIGNED_STRUCT_PAGE if SLUB
 +	select HAVE_CMPXCHG_LOCAL
 +	select HAVE_CMPXCHG_DOUBLE
 +	select HAVE_ARCH_KMEMCHECK
  	select HAVE_RCU_TABLE_FREE
 -	select HAVE_REGS_AND_STACK_ACCESS_API
 -	select HAVE_RELIABLE_STACKTRACE		if X86_64 && UNWINDER_FRAME_POINTER && STACK_VALIDATION
 -	select HAVE_STACKPROTECTOR		if CC_HAS_SANE_STACKPROTECTOR
 -	select HAVE_STACK_VALIDATION		if X86_64
 -	select HAVE_RSEQ
 -	select HAVE_SYSCALL_TRACEPOINTS
 -	select HAVE_UNSTABLE_SCHED_CLOCK
  	select HAVE_USER_RETURN_NOTIFIER
++<<<<<<< HEAD
 +	select ARCH_HAS_ELF_RANDOMIZE
 +	select HAVE_ARCH_JUMP_LABEL
 +	select HAVE_TEXT_POKE_SMP
 +	select HAVE_GENERIC_HARDIRQS
 +	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
++=======
+ 	select HOTPLUG_SMT			if SMP
+ 	select IRQ_FORCED_THREADING
+ 	select NEED_SG_DMA_LENGTH
+ 	select PCI_LOCKLESS_CONFIG
+ 	select PERF_EVENTS
+ 	select RTC_LIB
+ 	select RTC_MC146818_LIB
++>>>>>>> 05736e4ac13c (cpu/hotplug: Provide knobs to control SMT)
  	select SPARSE_IRQ
 -	select SRCU
 -	select SYSCTL_EXCEPTION_TRACE
 -	select THREAD_INFO_IN_TASK
 -	select USER_STACKTRACE_SUPPORT
 +	select GENERIC_FIND_FIRST_BIT
 +	select GENERIC_IRQ_PROBE
 +	select GENERIC_PENDING_IRQ if SMP
 +	select GENERIC_IRQ_SHOW
 +	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 +	select IRQ_FORCED_THREADING
 +	select USE_GENERIC_SMP_HELPERS if SMP
 +	select HAVE_BPF_JIT if X86_64
 +	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 +	select HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD if X86_64
 +	select HAVE_ARCH_HUGE_VMAP if X86_64 || (X86_32 && X86_PAE)
 +	select CLKEVT_I8253
 +	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 +	select GENERIC_IOMAP
 +	select DCACHE_WORD_ACCESS
 +	select GENERIC_SMP_IDLE_THREAD
 +	select ARCH_WANT_IPC_PARSE_VERSION if X86_32
 +	select HAVE_ARCH_SECCOMP_FILTER
 +	select BUILDTIME_EXTABLE_SORT
 +	select GENERIC_CMOS_UPDATE
 +	select HAVE_ARCH_SOFT_DIRTY
 +	select GENERIC_CLOCKEVENTS
 +	select ARCH_CLOCKSOURCE_DATA if X86_64
 +	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
 +	select GENERIC_CLOCKEVENTS_BROADCAST if X86_64 || (X86_32 && X86_LOCAL_APIC)
 +	select GENERIC_TIME_VSYSCALL if X86_64
 +	select HARDLOCKUP_CHECK_TIMESTAMP if X86_64
 +	select KTIME_SCALAR if X86_32
 +	select GENERIC_STRNCPY_FROM_USER
 +	select GENERIC_STRNLEN_USER
 +	select HAVE_CONTEXT_TRACKING if X86_64
 +	select HAVE_IRQ_TIME_ACCOUNTING
  	select VIRT_TO_BUS
 -	select X86_FEATURE_NAMES		if PROC_FS
 +	select MODULES_USE_ELF_REL if X86_32
 +	select MODULES_USE_ELF_RELA if X86_64
 +	select CLONE_BACKWARDS if X86_32
 +	select ARCH_USE_BUILTIN_BSWAP
 +	select ARCH_USE_QUEUED_SPINLOCKS
 +	select ARCH_USE_QUEUED_RWLOCKS
 +	select OLD_SIGSUSPEND3 if X86_32 || IA32_EMULATION
 +	select OLD_SIGACTION if X86_32
 +	select COMPAT_OLD_SIGACTION if IA32_EMULATION
 +	select RTC_LIB
 +	select HAVE_CC_STACKPROTECTOR
 +	select HAVE_STACK_VALIDATION if X86_64
 +	select HAVE_RELIABLE_STACKTRACE		if X86_64 && FRAME_POINTER && STACK_VALIDATION
 +	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS
 +	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS
 +	select GENERIC_CPU_VULNERABILITIES
  
  config INSTRUCTION_DECODER
  	def_bool y
diff --cc kernel/cpu.c
index 0d9e250d0ea0,d29fdd7e57bb..000000000000
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@@ -461,20 -900,113 +461,87 @@@ out
  	cpu_maps_update_done();
  	return err;
  }
 -
 -int cpu_down(unsigned int cpu)
 -{
 -	return do_cpu_down(cpu, CPUHP_OFFLINE);
 -}
  EXPORT_SYMBOL(cpu_down);
 -
 -#else
 -#define takedown_cpu		NULL
  #endif /*CONFIG_HOTPLUG_CPU*/
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_HOTPLUG_SMT
+ enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
+ 
+ static int __init smt_cmdline_disable(char *str)
+ {
+ 	cpu_smt_control = CPU_SMT_DISABLED;
+ 	if (str && !strcmp(str, "force")) {
+ 		pr_info("SMT: Force disabled\n");
+ 		cpu_smt_control = CPU_SMT_FORCE_DISABLED;
+ 	}
+ 	return 0;
+ }
+ early_param("nosmt", smt_cmdline_disable);
+ 
+ static inline bool cpu_smt_allowed(unsigned int cpu)
+ {
+ 	return cpu_smt_control == CPU_SMT_ENABLED ||
+ 		topology_is_primary_thread(cpu);
+ }
+ #else
+ static inline bool cpu_smt_allowed(unsigned int cpu) { return true; }
+ #endif
+ 
+ /**
+  * notify_cpu_starting(cpu) - Invoke the callbacks on the starting CPU
+  * @cpu: cpu that just started
+  *
+  * It must be called by the arch code on the new cpu, before the new cpu
+  * enables interrupts and before the "boot" cpu returns from __cpu_up().
+  */
+ void notify_cpu_starting(unsigned int cpu)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 	enum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);
+ 	int ret;
+ 
+ 	rcu_cpu_starting(cpu);	/* Enables RCU usage on this CPU. */
+ 	while (st->state < target) {
+ 		st->state++;
+ 		ret = cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
+ 		/*
+ 		 * STARTING must not fail!
+ 		 */
+ 		WARN_ON_ONCE(ret);
+ 	}
+ }
+ 
+ /*
+  * Called from the idle task. Wake up the controlling task which brings the
+  * stopper and the hotplug thread of the upcoming CPU up and then delegates
+  * the rest of the online bringup to the hotplug thread.
+  */
+ void cpuhp_online_idle(enum cpuhp_state state)
+ {
+ 	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
+ 
+ 	/* Happens for the boot cpu */
+ 	if (state != CPUHP_AP_ONLINE_IDLE)
+ 		return;
+ 
+ 	st->state = CPUHP_AP_ONLINE_IDLE;
+ 	complete_ap_thread(st, true);
+ }
+ 
++>>>>>>> 05736e4ac13c (cpu/hotplug: Provide knobs to control SMT)
  /* Requires cpu_add_remove_lock to be held */
 -static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)
 +static int _cpu_up(unsigned int cpu, int tasks_frozen)
  {
 -	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 +	int ret, nr_calls = 0;
 +	void *hcpu = (void *)(long)cpu;
 +	unsigned long mod = tasks_frozen ? CPU_TASKS_FROZEN : 0;
  	struct task_struct *idle;
 -	int ret = 0;
  
 -	cpus_write_lock();
 +	cpu_hotplug_begin();
  
 -	if (!cpu_present(cpu)) {
 +	if (cpu_online(cpu) || !cpu_present(cpu)) {
  		ret = -EINVAL;
  		goto out;
  	}
@@@ -542,9 -1079,12 +609,13 @@@ int cpu_up(unsigned int cpu
  		err = -EBUSY;
  		goto out;
  	}
+ 	if (!cpu_smt_allowed(cpu)) {
+ 		err = -EPERM;
+ 		goto out;
+ 	}
  
 -	err = _cpu_up(cpu, 0, target);
 +	err = _cpu_up(cpu, 0);
 +
  out:
  	cpu_maps_update_done();
  	return err;
@@@ -707,6 -1244,859 +778,862 @@@ void notify_cpu_starting(unsigned int c
  
  #endif /* CONFIG_SMP */
  
++<<<<<<< HEAD
++=======
+ /* Boot processor state steps */
+ static struct cpuhp_step cpuhp_hp_states[] = {
+ 	[CPUHP_OFFLINE] = {
+ 		.name			= "offline",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= NULL,
+ 	},
+ #ifdef CONFIG_SMP
+ 	[CPUHP_CREATE_THREADS]= {
+ 		.name			= "threads:prepare",
+ 		.startup.single		= smpboot_create_threads,
+ 		.teardown.single	= NULL,
+ 		.cant_stop		= true,
+ 	},
+ 	[CPUHP_PERF_PREPARE] = {
+ 		.name			= "perf:prepare",
+ 		.startup.single		= perf_event_init_cpu,
+ 		.teardown.single	= perf_event_exit_cpu,
+ 	},
+ 	[CPUHP_WORKQUEUE_PREP] = {
+ 		.name			= "workqueue:prepare",
+ 		.startup.single		= workqueue_prepare_cpu,
+ 		.teardown.single	= NULL,
+ 	},
+ 	[CPUHP_HRTIMERS_PREPARE] = {
+ 		.name			= "hrtimers:prepare",
+ 		.startup.single		= hrtimers_prepare_cpu,
+ 		.teardown.single	= hrtimers_dead_cpu,
+ 	},
+ 	[CPUHP_SMPCFD_PREPARE] = {
+ 		.name			= "smpcfd:prepare",
+ 		.startup.single		= smpcfd_prepare_cpu,
+ 		.teardown.single	= smpcfd_dead_cpu,
+ 	},
+ 	[CPUHP_RELAY_PREPARE] = {
+ 		.name			= "relay:prepare",
+ 		.startup.single		= relay_prepare_cpu,
+ 		.teardown.single	= NULL,
+ 	},
+ 	[CPUHP_SLAB_PREPARE] = {
+ 		.name			= "slab:prepare",
+ 		.startup.single		= slab_prepare_cpu,
+ 		.teardown.single	= slab_dead_cpu,
+ 	},
+ 	[CPUHP_RCUTREE_PREP] = {
+ 		.name			= "RCU/tree:prepare",
+ 		.startup.single		= rcutree_prepare_cpu,
+ 		.teardown.single	= rcutree_dead_cpu,
+ 	},
+ 	/*
+ 	 * On the tear-down path, timers_dead_cpu() must be invoked
+ 	 * before blk_mq_queue_reinit_notify() from notify_dead(),
+ 	 * otherwise a RCU stall occurs.
+ 	 */
+ 	[CPUHP_TIMERS_PREPARE] = {
+ 		.name			= "timers:dead",
+ 		.startup.single		= timers_prepare_cpu,
+ 		.teardown.single	= timers_dead_cpu,
+ 	},
+ 	/* Kicks the plugged cpu into life */
+ 	[CPUHP_BRINGUP_CPU] = {
+ 		.name			= "cpu:bringup",
+ 		.startup.single		= bringup_cpu,
+ 		.teardown.single	= NULL,
+ 		.cant_stop		= true,
+ 	},
+ 	/* Final state before CPU kills itself */
+ 	[CPUHP_AP_IDLE_DEAD] = {
+ 		.name			= "idle:dead",
+ 	},
+ 	/*
+ 	 * Last state before CPU enters the idle loop to die. Transient state
+ 	 * for synchronization.
+ 	 */
+ 	[CPUHP_AP_OFFLINE] = {
+ 		.name			= "ap:offline",
+ 		.cant_stop		= true,
+ 	},
+ 	/* First state is scheduler control. Interrupts are disabled */
+ 	[CPUHP_AP_SCHED_STARTING] = {
+ 		.name			= "sched:starting",
+ 		.startup.single		= sched_cpu_starting,
+ 		.teardown.single	= sched_cpu_dying,
+ 	},
+ 	[CPUHP_AP_RCUTREE_DYING] = {
+ 		.name			= "RCU/tree:dying",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= rcutree_dying_cpu,
+ 	},
+ 	[CPUHP_AP_SMPCFD_DYING] = {
+ 		.name			= "smpcfd:dying",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= smpcfd_dying_cpu,
+ 	},
+ 	/* Entry state on starting. Interrupts enabled from here on. Transient
+ 	 * state for synchronsization */
+ 	[CPUHP_AP_ONLINE] = {
+ 		.name			= "ap:online",
+ 	},
+ 	/*
+ 	 * Handled on controll processor until the plugged processor manages
+ 	 * this itself.
+ 	 */
+ 	[CPUHP_TEARDOWN_CPU] = {
+ 		.name			= "cpu:teardown",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= takedown_cpu,
+ 		.cant_stop		= true,
+ 	},
+ 	/* Handle smpboot threads park/unpark */
+ 	[CPUHP_AP_SMPBOOT_THREADS] = {
+ 		.name			= "smpboot/threads:online",
+ 		.startup.single		= smpboot_unpark_threads,
+ 		.teardown.single	= smpboot_park_threads,
+ 	},
+ 	[CPUHP_AP_IRQ_AFFINITY_ONLINE] = {
+ 		.name			= "irq/affinity:online",
+ 		.startup.single		= irq_affinity_online_cpu,
+ 		.teardown.single	= NULL,
+ 	},
+ 	[CPUHP_AP_PERF_ONLINE] = {
+ 		.name			= "perf:online",
+ 		.startup.single		= perf_event_init_cpu,
+ 		.teardown.single	= perf_event_exit_cpu,
+ 	},
+ 	[CPUHP_AP_WORKQUEUE_ONLINE] = {
+ 		.name			= "workqueue:online",
+ 		.startup.single		= workqueue_online_cpu,
+ 		.teardown.single	= workqueue_offline_cpu,
+ 	},
+ 	[CPUHP_AP_RCUTREE_ONLINE] = {
+ 		.name			= "RCU/tree:online",
+ 		.startup.single		= rcutree_online_cpu,
+ 		.teardown.single	= rcutree_offline_cpu,
+ 	},
+ #endif
+ 	/*
+ 	 * The dynamically registered state space is here
+ 	 */
+ 
+ #ifdef CONFIG_SMP
+ 	/* Last state is scheduler control setting the cpu active */
+ 	[CPUHP_AP_ACTIVE] = {
+ 		.name			= "sched:active",
+ 		.startup.single		= sched_cpu_activate,
+ 		.teardown.single	= sched_cpu_deactivate,
+ 	},
+ #endif
+ 
+ 	/* CPU is fully up and running. */
+ 	[CPUHP_ONLINE] = {
+ 		.name			= "online",
+ 		.startup.single		= NULL,
+ 		.teardown.single	= NULL,
+ 	},
+ };
+ 
+ /* Sanity check for callbacks */
+ static int cpuhp_cb_check(enum cpuhp_state state)
+ {
+ 	if (state <= CPUHP_OFFLINE || state >= CPUHP_ONLINE)
+ 		return -EINVAL;
+ 	return 0;
+ }
+ 
+ /*
+  * Returns a free for dynamic slot assignment of the Online state. The states
+  * are protected by the cpuhp_slot_states mutex and an empty slot is identified
+  * by having no name assigned.
+  */
+ static int cpuhp_reserve_state(enum cpuhp_state state)
+ {
+ 	enum cpuhp_state i, end;
+ 	struct cpuhp_step *step;
+ 
+ 	switch (state) {
+ 	case CPUHP_AP_ONLINE_DYN:
+ 		step = cpuhp_hp_states + CPUHP_AP_ONLINE_DYN;
+ 		end = CPUHP_AP_ONLINE_DYN_END;
+ 		break;
+ 	case CPUHP_BP_PREPARE_DYN:
+ 		step = cpuhp_hp_states + CPUHP_BP_PREPARE_DYN;
+ 		end = CPUHP_BP_PREPARE_DYN_END;
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	for (i = state; i <= end; i++, step++) {
+ 		if (!step->name)
+ 			return i;
+ 	}
+ 	WARN(1, "No more dynamic states available for CPU hotplug\n");
+ 	return -ENOSPC;
+ }
+ 
+ static int cpuhp_store_callbacks(enum cpuhp_state state, const char *name,
+ 				 int (*startup)(unsigned int cpu),
+ 				 int (*teardown)(unsigned int cpu),
+ 				 bool multi_instance)
+ {
+ 	/* (Un)Install the callbacks for further cpu hotplug operations */
+ 	struct cpuhp_step *sp;
+ 	int ret = 0;
+ 
+ 	/*
+ 	 * If name is NULL, then the state gets removed.
+ 	 *
+ 	 * CPUHP_AP_ONLINE_DYN and CPUHP_BP_PREPARE_DYN are handed out on
+ 	 * the first allocation from these dynamic ranges, so the removal
+ 	 * would trigger a new allocation and clear the wrong (already
+ 	 * empty) state, leaving the callbacks of the to be cleared state
+ 	 * dangling, which causes wreckage on the next hotplug operation.
+ 	 */
+ 	if (name && (state == CPUHP_AP_ONLINE_DYN ||
+ 		     state == CPUHP_BP_PREPARE_DYN)) {
+ 		ret = cpuhp_reserve_state(state);
+ 		if (ret < 0)
+ 			return ret;
+ 		state = ret;
+ 	}
+ 	sp = cpuhp_get_step(state);
+ 	if (name && sp->name)
+ 		return -EBUSY;
+ 
+ 	sp->startup.single = startup;
+ 	sp->teardown.single = teardown;
+ 	sp->name = name;
+ 	sp->multi_instance = multi_instance;
+ 	INIT_HLIST_HEAD(&sp->list);
+ 	return ret;
+ }
+ 
+ static void *cpuhp_get_teardown_cb(enum cpuhp_state state)
+ {
+ 	return cpuhp_get_step(state)->teardown.single;
+ }
+ 
+ /*
+  * Call the startup/teardown function for a step either on the AP or
+  * on the current CPU.
+  */
+ static int cpuhp_issue_call(int cpu, enum cpuhp_state state, bool bringup,
+ 			    struct hlist_node *node)
+ {
+ 	struct cpuhp_step *sp = cpuhp_get_step(state);
+ 	int ret;
+ 
+ 	/*
+ 	 * If there's nothing to do, we done.
+ 	 * Relies on the union for multi_instance.
+ 	 */
+ 	if ((bringup && !sp->startup.single) ||
+ 	    (!bringup && !sp->teardown.single))
+ 		return 0;
+ 	/*
+ 	 * The non AP bound callbacks can fail on bringup. On teardown
+ 	 * e.g. module removal we crash for now.
+ 	 */
+ #ifdef CONFIG_SMP
+ 	if (cpuhp_is_ap_state(state))
+ 		ret = cpuhp_invoke_ap_callback(cpu, state, bringup, node);
+ 	else
+ 		ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ #else
+ 	ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ #endif
+ 	BUG_ON(ret && !bringup);
+ 	return ret;
+ }
+ 
+ /*
+  * Called from __cpuhp_setup_state on a recoverable failure.
+  *
+  * Note: The teardown callbacks for rollback are not allowed to fail!
+  */
+ static void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,
+ 				   struct hlist_node *node)
+ {
+ 	int cpu;
+ 
+ 	/* Roll back the already executed steps on the other cpus */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpu >= failedcpu)
+ 			break;
+ 
+ 		/* Did we invoke the startup call on that cpu ? */
+ 		if (cpustate >= state)
+ 			cpuhp_issue_call(cpu, state, false, node);
+ 	}
+ }
+ 
+ int __cpuhp_state_add_instance_cpuslocked(enum cpuhp_state state,
+ 					  struct hlist_node *node,
+ 					  bool invoke)
+ {
+ 	struct cpuhp_step *sp;
+ 	int cpu;
+ 	int ret;
+ 
+ 	lockdep_assert_cpus_held();
+ 
+ 	sp = cpuhp_get_step(state);
+ 	if (sp->multi_instance == false)
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 
+ 	if (!invoke || !sp->startup.multi)
+ 		goto add_node;
+ 
+ 	/*
+ 	 * Try to call the startup callback for each present cpu
+ 	 * depending on the hotplug state of the cpu.
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate < state)
+ 			continue;
+ 
+ 		ret = cpuhp_issue_call(cpu, state, true, node);
+ 		if (ret) {
+ 			if (sp->teardown.multi)
+ 				cpuhp_rollback_install(cpu, state, node);
+ 			goto unlock;
+ 		}
+ 	}
+ add_node:
+ 	ret = 0;
+ 	hlist_add_head(node, &sp->list);
+ unlock:
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	return ret;
+ }
+ 
+ int __cpuhp_state_add_instance(enum cpuhp_state state, struct hlist_node *node,
+ 			       bool invoke)
+ {
+ 	int ret;
+ 
+ 	cpus_read_lock();
+ 	ret = __cpuhp_state_add_instance_cpuslocked(state, node, invoke);
+ 	cpus_read_unlock();
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(__cpuhp_state_add_instance);
+ 
+ /**
+  * __cpuhp_setup_state_cpuslocked - Setup the callbacks for an hotplug machine state
+  * @state:		The state to setup
+  * @invoke:		If true, the startup function is invoked for cpus where
+  *			cpu state >= @state
+  * @startup:		startup callback function
+  * @teardown:		teardown callback function
+  * @multi_instance:	State is set up for multiple instances which get
+  *			added afterwards.
+  *
+  * The caller needs to hold cpus read locked while calling this function.
+  * Returns:
+  *   On success:
+  *      Positive state number if @state is CPUHP_AP_ONLINE_DYN
+  *      0 for all other states
+  *   On failure: proper (negative) error code
+  */
+ int __cpuhp_setup_state_cpuslocked(enum cpuhp_state state,
+ 				   const char *name, bool invoke,
+ 				   int (*startup)(unsigned int cpu),
+ 				   int (*teardown)(unsigned int cpu),
+ 				   bool multi_instance)
+ {
+ 	int cpu, ret = 0;
+ 	bool dynstate;
+ 
+ 	lockdep_assert_cpus_held();
+ 
+ 	if (cpuhp_cb_check(state) || !name)
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 
+ 	ret = cpuhp_store_callbacks(state, name, startup, teardown,
+ 				    multi_instance);
+ 
+ 	dynstate = state == CPUHP_AP_ONLINE_DYN;
+ 	if (ret > 0 && dynstate) {
+ 		state = ret;
+ 		ret = 0;
+ 	}
+ 
+ 	if (ret || !invoke || !startup)
+ 		goto out;
+ 
+ 	/*
+ 	 * Try to call the startup callback for each present cpu
+ 	 * depending on the hotplug state of the cpu.
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate < state)
+ 			continue;
+ 
+ 		ret = cpuhp_issue_call(cpu, state, true, NULL);
+ 		if (ret) {
+ 			if (teardown)
+ 				cpuhp_rollback_install(cpu, state, NULL);
+ 			cpuhp_store_callbacks(state, NULL, NULL, NULL, false);
+ 			goto out;
+ 		}
+ 	}
+ out:
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	/*
+ 	 * If the requested state is CPUHP_AP_ONLINE_DYN, return the
+ 	 * dynamically allocated state in case of success.
+ 	 */
+ 	if (!ret && dynstate)
+ 		return state;
+ 	return ret;
+ }
+ EXPORT_SYMBOL(__cpuhp_setup_state_cpuslocked);
+ 
+ int __cpuhp_setup_state(enum cpuhp_state state,
+ 			const char *name, bool invoke,
+ 			int (*startup)(unsigned int cpu),
+ 			int (*teardown)(unsigned int cpu),
+ 			bool multi_instance)
+ {
+ 	int ret;
+ 
+ 	cpus_read_lock();
+ 	ret = __cpuhp_setup_state_cpuslocked(state, name, invoke, startup,
+ 					     teardown, multi_instance);
+ 	cpus_read_unlock();
+ 	return ret;
+ }
+ EXPORT_SYMBOL(__cpuhp_setup_state);
+ 
+ int __cpuhp_state_remove_instance(enum cpuhp_state state,
+ 				  struct hlist_node *node, bool invoke)
+ {
+ 	struct cpuhp_step *sp = cpuhp_get_step(state);
+ 	int cpu;
+ 
+ 	BUG_ON(cpuhp_cb_check(state));
+ 
+ 	if (!sp->multi_instance)
+ 		return -EINVAL;
+ 
+ 	cpus_read_lock();
+ 	mutex_lock(&cpuhp_state_mutex);
+ 
+ 	if (!invoke || !cpuhp_get_teardown_cb(state))
+ 		goto remove;
+ 	/*
+ 	 * Call the teardown callback for each present cpu depending
+ 	 * on the hotplug state of the cpu. This function is not
+ 	 * allowed to fail currently!
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate >= state)
+ 			cpuhp_issue_call(cpu, state, false, node);
+ 	}
+ 
+ remove:
+ 	hlist_del(node);
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	cpus_read_unlock();
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(__cpuhp_state_remove_instance);
+ 
+ /**
+  * __cpuhp_remove_state_cpuslocked - Remove the callbacks for an hotplug machine state
+  * @state:	The state to remove
+  * @invoke:	If true, the teardown function is invoked for cpus where
+  *		cpu state >= @state
+  *
+  * The caller needs to hold cpus read locked while calling this function.
+  * The teardown callback is currently not allowed to fail. Think
+  * about module removal!
+  */
+ void __cpuhp_remove_state_cpuslocked(enum cpuhp_state state, bool invoke)
+ {
+ 	struct cpuhp_step *sp = cpuhp_get_step(state);
+ 	int cpu;
+ 
+ 	BUG_ON(cpuhp_cb_check(state));
+ 
+ 	lockdep_assert_cpus_held();
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	if (sp->multi_instance) {
+ 		WARN(!hlist_empty(&sp->list),
+ 		     "Error: Removing state %d which has instances left.\n",
+ 		     state);
+ 		goto remove;
+ 	}
+ 
+ 	if (!invoke || !cpuhp_get_teardown_cb(state))
+ 		goto remove;
+ 
+ 	/*
+ 	 * Call the teardown callback for each present cpu depending
+ 	 * on the hotplug state of the cpu. This function is not
+ 	 * allowed to fail currently!
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 		int cpustate = st->state;
+ 
+ 		if (cpustate >= state)
+ 			cpuhp_issue_call(cpu, state, false, NULL);
+ 	}
+ remove:
+ 	cpuhp_store_callbacks(state, NULL, NULL, NULL, false);
+ 	mutex_unlock(&cpuhp_state_mutex);
+ }
+ EXPORT_SYMBOL(__cpuhp_remove_state_cpuslocked);
+ 
+ void __cpuhp_remove_state(enum cpuhp_state state, bool invoke)
+ {
+ 	cpus_read_lock();
+ 	__cpuhp_remove_state_cpuslocked(state, invoke);
+ 	cpus_read_unlock();
+ }
+ EXPORT_SYMBOL(__cpuhp_remove_state);
+ 
+ #if defined(CONFIG_SYSFS) && defined(CONFIG_HOTPLUG_CPU)
+ static ssize_t show_cpuhp_state(struct device *dev,
+ 				struct device_attribute *attr, char *buf)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 
+ 	return sprintf(buf, "%d\n", st->state);
+ }
+ static DEVICE_ATTR(state, 0444, show_cpuhp_state, NULL);
+ 
+ static ssize_t write_cpuhp_target(struct device *dev,
+ 				  struct device_attribute *attr,
+ 				  const char *buf, size_t count)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 	struct cpuhp_step *sp;
+ 	int target, ret;
+ 
+ 	ret = kstrtoint(buf, 10, &target);
+ 	if (ret)
+ 		return ret;
+ 
+ #ifdef CONFIG_CPU_HOTPLUG_STATE_CONTROL
+ 	if (target < CPUHP_OFFLINE || target > CPUHP_ONLINE)
+ 		return -EINVAL;
+ #else
+ 	if (target != CPUHP_OFFLINE && target != CPUHP_ONLINE)
+ 		return -EINVAL;
+ #endif
+ 
+ 	ret = lock_device_hotplug_sysfs();
+ 	if (ret)
+ 		return ret;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	sp = cpuhp_get_step(target);
+ 	ret = !sp->name || sp->cant_stop ? -EINVAL : 0;
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (st->state < target)
+ 		ret = do_cpu_up(dev->id, target);
+ 	else
+ 		ret = do_cpu_down(dev->id, target);
+ out:
+ 	unlock_device_hotplug();
+ 	return ret ? ret : count;
+ }
+ 
+ static ssize_t show_cpuhp_target(struct device *dev,
+ 				 struct device_attribute *attr, char *buf)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 
+ 	return sprintf(buf, "%d\n", st->target);
+ }
+ static DEVICE_ATTR(target, 0644, show_cpuhp_target, write_cpuhp_target);
+ 
+ 
+ static ssize_t write_cpuhp_fail(struct device *dev,
+ 				struct device_attribute *attr,
+ 				const char *buf, size_t count)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 	struct cpuhp_step *sp;
+ 	int fail, ret;
+ 
+ 	ret = kstrtoint(buf, 10, &fail);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * Cannot fail STARTING/DYING callbacks.
+ 	 */
+ 	if (cpuhp_is_atomic_state(fail))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Cannot fail anything that doesn't have callbacks.
+ 	 */
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	sp = cpuhp_get_step(fail);
+ 	if (!sp->startup.single && !sp->teardown.single)
+ 		ret = -EINVAL;
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	if (ret)
+ 		return ret;
+ 
+ 	st->fail = fail;
+ 
+ 	return count;
+ }
+ 
+ static ssize_t show_cpuhp_fail(struct device *dev,
+ 			       struct device_attribute *attr, char *buf)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
+ 
+ 	return sprintf(buf, "%d\n", st->fail);
+ }
+ 
+ static DEVICE_ATTR(fail, 0644, show_cpuhp_fail, write_cpuhp_fail);
+ 
+ static struct attribute *cpuhp_cpu_attrs[] = {
+ 	&dev_attr_state.attr,
+ 	&dev_attr_target.attr,
+ 	&dev_attr_fail.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group cpuhp_cpu_attr_group = {
+ 	.attrs = cpuhp_cpu_attrs,
+ 	.name = "hotplug",
+ 	NULL
+ };
+ 
+ static ssize_t show_cpuhp_states(struct device *dev,
+ 				 struct device_attribute *attr, char *buf)
+ {
+ 	ssize_t cur, res = 0;
+ 	int i;
+ 
+ 	mutex_lock(&cpuhp_state_mutex);
+ 	for (i = CPUHP_OFFLINE; i <= CPUHP_ONLINE; i++) {
+ 		struct cpuhp_step *sp = cpuhp_get_step(i);
+ 
+ 		if (sp->name) {
+ 			cur = sprintf(buf, "%3d: %s\n", i, sp->name);
+ 			buf += cur;
+ 			res += cur;
+ 		}
+ 	}
+ 	mutex_unlock(&cpuhp_state_mutex);
+ 	return res;
+ }
+ static DEVICE_ATTR(states, 0444, show_cpuhp_states, NULL);
+ 
+ static struct attribute *cpuhp_cpu_root_attrs[] = {
+ 	&dev_attr_states.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group cpuhp_cpu_root_attr_group = {
+ 	.attrs = cpuhp_cpu_root_attrs,
+ 	.name = "hotplug",
+ 	NULL
+ };
+ 
+ #ifdef CONFIG_HOTPLUG_SMT
+ 
+ static const char *smt_states[] = {
+ 	[CPU_SMT_ENABLED]		= "on",
+ 	[CPU_SMT_DISABLED]		= "off",
+ 	[CPU_SMT_FORCE_DISABLED]	= "forceoff",
+ 	[CPU_SMT_NOT_SUPPORTED]		= "notsupported",
+ };
+ 
+ static ssize_t
+ show_smt_control(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return snprintf(buf, PAGE_SIZE - 2, "%s\n", smt_states[cpu_smt_control]);
+ }
+ 
+ static void cpuhp_offline_cpu_device(unsigned int cpu)
+ {
+ 	struct device *dev = get_cpu_device(cpu);
+ 
+ 	dev->offline = true;
+ 	/* Tell user space about the state change */
+ 	kobject_uevent(&dev->kobj, KOBJ_OFFLINE);
+ }
+ 
+ static int cpuhp_smt_disable(enum cpuhp_smt_control ctrlval)
+ {
+ 	int cpu, ret = 0;
+ 
+ 	cpu_maps_update_begin();
+ 	for_each_online_cpu(cpu) {
+ 		if (topology_is_primary_thread(cpu))
+ 			continue;
+ 		ret = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);
+ 		if (ret)
+ 			break;
+ 		/*
+ 		 * As this needs to hold the cpu maps lock it's impossible
+ 		 * to call device_offline() because that ends up calling
+ 		 * cpu_down() which takes cpu maps lock. cpu maps lock
+ 		 * needs to be held as this might race against in kernel
+ 		 * abusers of the hotplug machinery (thermal management).
+ 		 *
+ 		 * So nothing would update device:offline state. That would
+ 		 * leave the sysfs entry stale and prevent onlining after
+ 		 * smt control has been changed to 'off' again. This is
+ 		 * called under the sysfs hotplug lock, so it is properly
+ 		 * serialized against the regular offline usage.
+ 		 */
+ 		cpuhp_offline_cpu_device(cpu);
+ 	}
+ 	if (!ret)
+ 		cpu_smt_control = ctrlval;
+ 	cpu_maps_update_done();
+ 	return ret;
+ }
+ 
+ static void cpuhp_smt_enable(void)
+ {
+ 	cpu_maps_update_begin();
+ 	cpu_smt_control = CPU_SMT_ENABLED;
+ 	cpu_maps_update_done();
+ }
+ 
+ static ssize_t
+ store_smt_control(struct device *dev, struct device_attribute *attr,
+ 		  const char *buf, size_t count)
+ {
+ 	int ctrlval, ret;
+ 
+ 	if (sysfs_streq(buf, "on"))
+ 		ctrlval = CPU_SMT_ENABLED;
+ 	else if (sysfs_streq(buf, "off"))
+ 		ctrlval = CPU_SMT_DISABLED;
+ 	else if (sysfs_streq(buf, "forceoff"))
+ 		ctrlval = CPU_SMT_FORCE_DISABLED;
+ 	else
+ 		return -EINVAL;
+ 
+ 	if (cpu_smt_control == CPU_SMT_FORCE_DISABLED)
+ 		return -EPERM;
+ 
+ 	if (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)
+ 		return -ENODEV;
+ 
+ 	ret = lock_device_hotplug_sysfs();
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (ctrlval != cpu_smt_control) {
+ 		switch (ctrlval) {
+ 		case CPU_SMT_ENABLED:
+ 			cpuhp_smt_enable();
+ 			break;
+ 		case CPU_SMT_DISABLED:
+ 		case CPU_SMT_FORCE_DISABLED:
+ 			ret = cpuhp_smt_disable(ctrlval);
+ 			break;
+ 		}
+ 	}
+ 
+ 	unlock_device_hotplug();
+ 	return ret ? ret : count;
+ }
+ static DEVICE_ATTR(control, 0644, show_smt_control, store_smt_control);
+ 
+ static ssize_t
+ show_smt_active(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	bool active = topology_max_smt_threads() > 1;
+ 
+ 	return snprintf(buf, PAGE_SIZE - 2, "%d\n", active);
+ }
+ static DEVICE_ATTR(active, 0444, show_smt_active, NULL);
+ 
+ static struct attribute *cpuhp_smt_attrs[] = {
+ 	&dev_attr_control.attr,
+ 	&dev_attr_active.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group cpuhp_smt_attr_group = {
+ 	.attrs = cpuhp_smt_attrs,
+ 	.name = "smt",
+ 	NULL
+ };
+ 
+ static int __init cpu_smt_state_init(void)
+ {
+ 	if (!topology_smt_supported())
+ 		cpu_smt_control = CPU_SMT_NOT_SUPPORTED;
+ 
+ 	return sysfs_create_group(&cpu_subsys.dev_root->kobj,
+ 				  &cpuhp_smt_attr_group);
+ }
+ 
+ #else
+ static inline int cpu_smt_state_init(void) { return 0; }
+ #endif
+ 
+ static int __init cpuhp_sysfs_init(void)
+ {
+ 	int cpu, ret;
+ 
+ 	ret = cpu_smt_state_init();
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = sysfs_create_group(&cpu_subsys.dev_root->kobj,
+ 				 &cpuhp_cpu_root_attr_group);
+ 	if (ret)
+ 		return ret;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct device *dev = get_cpu_device(cpu);
+ 
+ 		if (!dev)
+ 			continue;
+ 		ret = sysfs_create_group(&dev->kobj, &cpuhp_cpu_attr_group);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 	return 0;
+ }
+ device_initcall(cpuhp_sysfs_init);
+ #endif
+ 
++>>>>>>> 05736e4ac13c (cpu/hotplug: Provide knobs to control SMT)
  /*
   * cpu_bit_bitmap[] is a special, "compressed" data structure that
   * represents all NR_CPUS bits binary values of 1<<nr.
* Unmerged path Documentation/ABI/testing/sysfs-devices-system-cpu
* Unmerged path Documentation/kernel-parameters.txt
* Unmerged path arch/Kconfig
* Unmerged path arch/x86/Kconfig
diff --git a/include/linux/cpu.h b/include/linux/cpu.h
index 1cc4c784a301..674633f92e72 100644
--- a/include/linux/cpu.h
+++ b/include/linux/cpu.h
@@ -292,4 +292,17 @@ bool cpu_wait_death(unsigned int cpu, int seconds);
 bool cpu_report_death(void);
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
 
+enum cpuhp_smt_control {
+	CPU_SMT_ENABLED,
+	CPU_SMT_DISABLED,
+	CPU_SMT_FORCE_DISABLED,
+	CPU_SMT_NOT_SUPPORTED,
+};
+
+#if defined(CONFIG_SMP) && defined(CONFIG_HOTPLUG_SMT)
+extern enum cpuhp_smt_control cpu_smt_control;
+#else
+# define cpu_smt_control		(CPU_SMT_ENABLED)
+#endif
+
 #endif /* _LINUX_CPU_H_ */
* Unmerged path kernel/cpu.c
