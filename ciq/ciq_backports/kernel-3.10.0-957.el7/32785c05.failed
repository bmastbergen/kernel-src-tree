fs/dcache.c: add cond_resched() in shrink_dentry_list()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [fs] dcache.c: add cond_resched() in shrink_dentry_list() (Aaron Tomlin) [1584693]
Rebuild_FUZZ: 97.20%
commit-author Nikolay Borisov <nborisov@suse.com>
commit 32785c0539b7e96f77a14a4f4ab225712665a5a4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/32785c05.failed

As previously reported (https://patchwork.kernel.org/patch/8642031/)
it's possible to call shrink_dentry_list with a large number of dentries
(> 10000).  This, in turn, could trigger the softlockup detector and
possibly trigger a panic.  In addition to the unmount path being
vulnerable to this scenario, at SuSE we've observed similar situation
happening during process exit on processes that touch a lot of dentries.
Here is an excerpt from a crash dump.  The number after the colon are
the number of dentries on the list passed to shrink_dentry_list:

PID 99760: 10722
PID 107530: 215
PID 108809: 24134
PID 108877: 21331
PID 141708: 16487

So we want to kill between 15k-25k dentries without yielding.

And one possible call stack looks like:

4 [ffff8839ece41db0] _raw_spin_lock at ffffffff8152a5f8
5 [ffff8839ece41db0] evict at ffffffff811c3026
6 [ffff8839ece41dd0] __dentry_kill at ffffffff811bf258
7 [ffff8839ece41df0] shrink_dentry_list at ffffffff811bf593
8 [ffff8839ece41e18] shrink_dcache_parent at ffffffff811bf830
9 [ffff8839ece41e50] proc_flush_task at ffffffff8120dd61
10 [ffff8839ece41ec0] release_task at ffffffff81059ebd
11 [ffff8839ece41f08] do_exit at ffffffff8105b8ce
12 [ffff8839ece41f78] sys_exit at ffffffff8105bd53
13 [ffff8839ece41f80] system_call_fastpath at ffffffff81532909

While some of the callers of shrink_dentry_list do use cond_resched,
this is not sufficient to prevent softlockups.  So just move
cond_resched into shrink_dentry_list from its callers.

David said: I've found hundreds of occurrences of warnings that we emit
when need_resched stays set for a prolonged period of time with the
stack trace that is included in the change log.

Link: http://lkml.kernel.org/r/1521718946-31521-1-git-send-email-nborisov@suse.com
	Signed-off-by: Nikolay Borisov <nborisov@suse.com>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: David Rientjes <rientjes@google.com>
	Cc: Alexander Viro <viro@zeniv.linux.org.uk>
	Cc: Goldwyn Rodrigues <rgoldwyn@suse.de>
	Cc: Jeff Mahoney <jeffm@suse.com>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 32785c0539b7e96f77a14a4f4ab225712665a5a4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dcache.c
diff --cc fs/dcache.c
index 656269cc223e,86d2de63461e..000000000000
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@@ -768,49 -998,82 +768,55 @@@ EXPORT_SYMBOL(d_prune_aliases)
  
  static void shrink_dentry_list(struct list_head *list)
  {
 -	while (!list_empty(list)) {
 -		struct dentry *dentry, *parent;
 +	struct dentry *dentry, *parent;
  
++<<<<<<< HEAD
 +	rcu_read_lock();
 +	for (;;) {
 +		struct inode *inode;
 +		dentry = list_entry_rcu(list->prev, struct dentry, d_lru);
 +		if (&dentry->d_lru == list)
 +			break; /* empty */
++=======
+ 		cond_resched();
+ 
+ 		dentry = list_entry(list->prev, struct dentry, d_lru);
++>>>>>>> 32785c0539b7 (fs/dcache.c: add cond_resched() in shrink_dentry_list())
  		spin_lock(&dentry->d_lock);
 -		rcu_read_lock();
 -		if (!shrink_lock_dentry(dentry)) {
 -			bool can_free = false;
 -			rcu_read_unlock();
 -			d_shrink_del(dentry);
 -			if (dentry->d_lockref.count < 0)
 -				can_free = dentry->d_flags & DCACHE_MAY_FREE;
 +		if (dentry != list_entry(list->prev, struct dentry, d_lru)) {
  			spin_unlock(&dentry->d_lock);
 -			if (can_free)
 -				dentry_free(dentry);
  			continue;
  		}
 +
 +		parent = lock_parent(dentry);
 +
 +		/*
 +		 * We found an inuse dentry which was not removed from
 +		 * the LRU because of laziness during lookup.  Do not free
 +		 * it - just keep it off the LRU list.
 +		 */
 +		if (dentry->d_lockref.count) {
 +			dentry_lru_del(dentry);
 +			spin_unlock(&dentry->d_lock);
 +			if (parent)
 +				spin_unlock(&parent->d_lock);
 +			continue;
 +		}
 +
  		rcu_read_unlock();
 -		d_shrink_del(dentry);
 -		parent = dentry->d_parent;
 -		__dentry_kill(dentry);
 -		if (parent == dentry)
 +
 +		inode = dentry->d_inode;
 +		if (inode && unlikely(!spin_trylock(&inode->i_lock))) {
 +			spin_unlock(&dentry->d_lock);
 +			if (parent)
 +				spin_unlock(&parent->d_lock);
 +			cpu_relax();
 +			rcu_read_lock();
  			continue;
 +		}
 +
 +		__dentry_kill(dentry);
 +
  		/*
  		 * We need to prune ancestors too. This is necessary to prevent
  		 * quadratic behavior of shrink_dcache_parent(), but is also
@@@ -903,134 -1197,19 +909,146 @@@ relock
   */
  void shrink_dcache_sb(struct super_block *sb)
  {
 -	long freed;
 +	LIST_HEAD(tmp);
  
++<<<<<<< HEAD
 +	spin_lock(&dcache_lru_lock);
 +	while (!list_empty(&sb->s_dentry_lru)) {
 +		list_splice_init(&sb->s_dentry_lru, &tmp);
 +		spin_unlock(&dcache_lru_lock);
 +		shrink_dentry_list(&tmp);
 +		spin_lock(&dcache_lru_lock);
 +	}
 +	spin_unlock(&dcache_lru_lock);
++=======
+ 	do {
+ 		LIST_HEAD(dispose);
+ 
+ 		freed = list_lru_walk(&sb->s_dentry_lru,
+ 			dentry_lru_isolate_shrink, &dispose, 1024);
+ 
+ 		this_cpu_sub(nr_dentry_unused, freed);
+ 		shrink_dentry_list(&dispose);
+ 	} while (list_lru_count(&sb->s_dentry_lru) > 0);
++>>>>>>> 32785c0539b7 (fs/dcache.c: add cond_resched() in shrink_dentry_list())
 +}
 +EXPORT_SYMBOL(shrink_dcache_sb);
 +
 +/*
 + * destroy a single subtree of dentries for unmount
 + * - see the comments on shrink_dcache_for_umount() for a description of the
 + *   locking
 + */
 +#define RESCHED_CHECK_BATCH 1024
 +static void shrink_dcache_for_umount_subtree(struct dentry *dentry)
 +{
 +	struct dentry *parent;
 +	int batch = RESCHED_CHECK_BATCH;
 +
 +	BUG_ON(!IS_ROOT(dentry));
 +
 +	for (;;) {
 +		/* descend to the first leaf in the current subtree */
 +		while (!list_empty(&dentry->d_subdirs))
 +			dentry = list_entry(dentry->d_subdirs.next,
 +					    struct dentry, d_u.d_child);
 +
 +		/* consume the dentries from this leaf up through its parents
 +		 * until we find one with children or run out altogether */
 +		do {
 +			struct inode *inode;
 +
 +			/*
 +			 * inform the fs that this dentry is about to be
 +			 * unhashed and destroyed.
 +			 */
 +			if ((dentry->d_flags & DCACHE_OP_PRUNE) &&
 +			    !d_unhashed(dentry))
 +				dentry->d_op->d_prune(dentry);
 +
 +			dentry_lru_del(dentry);
 +			__d_shrink(dentry);
 +
 +			if (dentry->d_lockref.count != 0) {
 +				printk(KERN_ERR
 +				       "BUG: Dentry %p{i=%lx,n=%s}"
 +				       " still in use (%d)"
 +				       " [unmount of %s %s]\n",
 +				       dentry,
 +				       dentry->d_inode ?
 +				       dentry->d_inode->i_ino : 0UL,
 +				       dentry->d_name.name,
 +				       dentry->d_lockref.count,
 +				       dentry->d_sb->s_type->name,
 +				       dentry->d_sb->s_id);
 +				BUG();
 +			}
 +
 +			if (IS_ROOT(dentry)) {
 +				parent = NULL;
 +				list_del(&dentry->d_u.d_child);
 +			} else {
 +				parent = dentry->d_parent;
 +				parent->d_lockref.count--;
 +				list_del(&dentry->d_u.d_child);
 +			}
 +
 +			inode = dentry->d_inode;
 +			if (inode) {
 +				dentry->d_inode = NULL;
 +				hlist_del_init(&dentry->d_alias);
 +				if (dentry->d_op && dentry->d_op->d_iput)
 +					dentry->d_op->d_iput(dentry, inode);
 +				else
 +					iput(inode);
 +			}
 +
 +			d_free(dentry);
 +
 +			/* finished when we fall off the top of the tree,
 +			 * otherwise we ascend to the parent and move to the
 +			 * next sibling if there is one */
 +			if (!parent)
 +				return;
 +			dentry = parent;
 +			if (!--batch) {
 +				cond_resched();
 +				batch = RESCHED_CHECK_BATCH;
 +			}
 +		} while (list_empty(&dentry->d_subdirs));
 +
 +		dentry = list_entry(dentry->d_subdirs.next,
 +				    struct dentry, d_u.d_child);
 +	}
 +}
 +
 +/*
 + * destroy the dentries attached to a superblock on unmounting
 + * - we don't need to use dentry->d_lock because:
 + *   - the superblock is detached from all mountings and open files, so the
 + *     dentry trees will not be rearranged by the VFS
 + *   - s_umount is write-locked, so the memory pressure shrinker will ignore
 + *     any dentries belonging to this superblock that it comes across
 + *   - the filesystem itself is no longer permitted to rearrange the dentries
 + *     in this superblock
 + */
 +void shrink_dcache_for_umount(struct super_block *sb)
 +{
 +	struct dentry *dentry;
 +
 +	if (down_read_trylock(&sb->s_umount))
 +		BUG();
 +
 +	dentry = sb->s_root;
 +	sb->s_root = NULL;
 +	dentry->d_lockref.count--;
 +	shrink_dcache_for_umount_subtree(dentry);
 +
 +	while (!hlist_bl_empty(&sb->s_anon)) {
 +		dentry = hlist_bl_entry(hlist_bl_first(&sb->s_anon), struct dentry, d_hash);
 +		shrink_dcache_for_umount_subtree(dentry);
 +	}
  }
 -EXPORT_SYMBOL(shrink_dcache_sb);
  
  /**
   * enum d_walk_ret - action to talke during tree walk
@@@ -1414,23 -1605,15 +1431,26 @@@ int d_invalidate(struct dentry *dentry
  
  		d_walk(dentry, &data, detach_and_collect, check_and_drop);
  
 -		if (!list_empty(&data.select.dispose))
 +		if (data.select.found)
  			shrink_dentry_list(&data.select.dispose);
 -		else if (!data.mountpoint)
 -			return;
  
  		if (data.mountpoint) {
 -			detach_mounts(data.mountpoint);
 -			dput(data.mountpoint);
 +			if (may_detach_mounts) {
 +				detach_mounts(data.mountpoint);
 +				dput(data.mountpoint);
 +			} else {
 +				dput(data.mountpoint);
 +				return -EBUSY;
 +			}
  		}
++<<<<<<< HEAD
 +
 +		if (!data.mountpoint && !data.select.found)
 +			return 0;
 +
 +		cond_resched();
++=======
++>>>>>>> 32785c0539b7 (fs/dcache.c: add cond_resched() in shrink_dentry_list())
  	}
  }
  EXPORT_SYMBOL(d_invalidate);
* Unmerged path fs/dcache.c
