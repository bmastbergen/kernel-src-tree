md/raid10: refactor some codes from raid10_write_request

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] raid10: refactor some codes from raid10 write request (Nigel Croxon) [1494474]
Rebuild_FUZZ: 93.58%
commit-author Guoqing Jiang <gqjiang@suse.com>
commit 27f26a0f3767b6688b9a88b9becb6f8e760421f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/27f26a0f.failed

Previously, we clone both bio and repl_bio in raid10_write_request,
then add the cloned bio to plug->pending or conf->pending_bio_list
based on plug or not, and most of the logics are same for the two
conditions.

So introduce raid10_write_one_disk for it, and use replacement parameter
to distinguish the difference. No functional changes in the patch.

	Signed-off-by: Guoqing Jiang <gqjiang@suse.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 27f26a0f3767b6688b9a88b9becb6f8e760421f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid10.c
diff --cc drivers/md/raid10.c
index e568b64df05f,28c62e0c42aa..000000000000
--- a/drivers/md/raid10.c
+++ b/drivers/md/raid10.c
@@@ -1162,80 -1088,182 +1162,252 @@@ static void raid10_unplug(struct blk_pl
  	kfree(plug);
  }
  
 -static void raid10_read_request(struct mddev *mddev, struct bio *bio,
 -				struct r10bio *r10_bio)
 +static bool raid10_make_request(struct mddev *mddev, struct bio * bio)
  {
  	struct r10conf *conf = mddev->private;
 +	struct r10bio *r10_bio;
  	struct bio *read_bio;
++<<<<<<< HEAD
 +	int i;
 +	sector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);
 +	int chunk_sects = chunk_mask + 1;
 +	const int rw = bio_data_dir(bio);
 +	const unsigned long do_sync = (bio->bi_rw & REQ_SYNC);
 +	const unsigned long do_fua = (bio->bi_rw & REQ_FUA);
 +	const unsigned long do_discard = (bio->bi_rw
 +					  & (REQ_DISCARD | REQ_SECURE));
 +	const unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);
 +	unsigned long flags;
 +	struct md_rdev *blocked_rdev;
 +	struct blk_plug_cb *cb;
 +	struct raid10_plug_cb *plug = NULL;
++=======
+ 	const int op = bio_op(bio);
+ 	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
+ 	int sectors_handled;
+ 	int max_sectors;
+ 	sector_t sectors;
+ 	struct md_rdev *rdev;
+ 	int slot;
+ 
+ 	/*
+ 	 * Register the new request and wait if the reconstruction
+ 	 * thread has put up a bar for new requests.
+ 	 * Continue immediately if no resync is active currently.
+ 	 */
+ 	wait_barrier(conf);
+ 
+ 	sectors = bio_sectors(bio);
+ 	while (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
+ 	    bio->bi_iter.bi_sector < conf->reshape_progress &&
+ 	    bio->bi_iter.bi_sector + sectors > conf->reshape_progress) {
+ 		/*
+ 		 * IO spans the reshape position.  Need to wait for reshape to
+ 		 * pass
+ 		 */
+ 		raid10_log(conf->mddev, "wait reshape");
+ 		allow_barrier(conf);
+ 		wait_event(conf->wait_barrier,
+ 			   conf->reshape_progress <= bio->bi_iter.bi_sector ||
+ 			   conf->reshape_progress >= bio->bi_iter.bi_sector +
+ 			   sectors);
+ 		wait_barrier(conf);
+ 	}
+ 
+ read_again:
+ 	rdev = read_balance(conf, r10_bio, &max_sectors);
+ 	if (!rdev) {
+ 		raid_end_bio_io(r10_bio);
+ 		return;
+ 	}
+ 	slot = r10_bio->read_slot;
+ 
+ 	read_bio = bio_clone_fast(bio, GFP_NOIO, mddev->bio_set);
+ 	bio_trim(read_bio, r10_bio->sector - bio->bi_iter.bi_sector,
+ 		 max_sectors);
+ 
+ 	r10_bio->devs[slot].bio = read_bio;
+ 	r10_bio->devs[slot].rdev = rdev;
+ 
+ 	read_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +
+ 		choose_data_offset(r10_bio, rdev);
+ 	read_bio->bi_bdev = rdev->bdev;
+ 	read_bio->bi_end_io = raid10_end_read_request;
+ 	bio_set_op_attrs(read_bio, op, do_sync);
+ 	if (test_bit(FailFast, &rdev->flags) &&
+ 	    test_bit(R10BIO_FailFast, &r10_bio->state))
+ 	        read_bio->bi_opf |= MD_FAILFAST;
+ 	read_bio->bi_private = r10_bio;
+ 
+ 	if (mddev->gendisk)
+ 	        trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
+ 	                              read_bio, disk_devt(mddev->gendisk),
+ 	                              r10_bio->sector);
+ 	if (max_sectors < r10_bio->sectors) {
+ 		/*
+ 		 * Could not read all from this device, so we will need another
+ 		 * r10_bio.
+ 		 */
+ 		sectors_handled = (r10_bio->sector + max_sectors
+ 				   - bio->bi_iter.bi_sector);
+ 		r10_bio->sectors = max_sectors;
+ 		inc_pending(conf);
+ 		bio_inc_remaining(bio);
+ 		/*
+ 		 * Cannot call generic_make_request directly as that will be
+ 		 * queued in __generic_make_request and subsequent
+ 		 * mempool_alloc might block waiting for it.  so hand bio over
+ 		 * to raid10d.
+ 		 */
+ 		reschedule_retry(r10_bio);
+ 
+ 		r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
+ 
+ 		r10_bio->master_bio = bio;
+ 		r10_bio->sectors = bio_sectors(bio) - sectors_handled;
+ 		r10_bio->state = 0;
+ 		r10_bio->mddev = mddev;
+ 		r10_bio->sector = bio->bi_iter.bi_sector + sectors_handled;
+ 		goto read_again;
+ 	} else
+ 		generic_make_request(read_bio);
+ 	return;
+ }
+ 
+ static void raid10_write_one_disk(struct mddev *mddev, struct r10bio *r10_bio,
+ 				  struct bio *bio, bool replacement,
+ 				  int n_copy, int max_sectors)
+ {
+ 	const int op = bio_op(bio);
+ 	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
+ 	const unsigned long do_fua = (bio->bi_opf & REQ_FUA);
+ 	unsigned long flags;
+ 	struct blk_plug_cb *cb;
+ 	struct raid10_plug_cb *plug = NULL;
+ 	struct r10conf *conf = mddev->private;
+ 	struct md_rdev *rdev;
+ 	int devnum = r10_bio->devs[n_copy].devnum;
+ 	struct bio *mbio;
+ 
+ 	if (replacement) {
+ 		rdev = conf->mirrors[devnum].replacement;
+ 		if (rdev == NULL) {
+ 			/* Replacement just got moved to main 'rdev' */
+ 			smp_mb();
+ 			rdev = conf->mirrors[devnum].rdev;
+ 		}
+ 	} else
+ 		rdev = conf->mirrors[devnum].rdev;
+ 
+ 	mbio = bio_clone_fast(bio, GFP_NOIO, mddev->bio_set);
+ 	bio_trim(mbio, r10_bio->sector - bio->bi_iter.bi_sector, max_sectors);
+ 	if (replacement)
+ 		r10_bio->devs[n_copy].repl_bio = mbio;
+ 	else
+ 		r10_bio->devs[n_copy].bio = mbio;
+ 
+ 	mbio->bi_iter.bi_sector	= (r10_bio->devs[n_copy].addr +
+ 				   choose_data_offset(r10_bio, rdev));
+ 	mbio->bi_bdev = rdev->bdev;
+ 	mbio->bi_end_io	= raid10_end_write_request;
+ 	bio_set_op_attrs(mbio, op, do_sync | do_fua);
+ 	if (!replacement && test_bit(FailFast,
+ 				     &conf->mirrors[devnum].rdev->flags)
+ 			 && enough(conf, devnum))
+ 		mbio->bi_opf |= MD_FAILFAST;
+ 	mbio->bi_private = r10_bio;
+ 
+ 	if (conf->mddev->gendisk)
+ 		trace_block_bio_remap(bdev_get_queue(mbio->bi_bdev),
+ 				      mbio, disk_devt(conf->mddev->gendisk),
+ 				      r10_bio->sector);
+ 	/* flush_pending_writes() needs access to the rdev so...*/
+ 	mbio->bi_bdev = (void *)rdev;
+ 
+ 	atomic_inc(&r10_bio->remaining);
+ 
+ 	cb = blk_check_plugged(raid10_unplug, mddev, sizeof(*plug));
+ 	if (cb)
+ 		plug = container_of(cb, struct raid10_plug_cb, cb);
+ 	else
+ 		plug = NULL;
+ 	spin_lock_irqsave(&conf->device_lock, flags);
+ 	if (plug) {
+ 		bio_list_add(&plug->pending, mbio);
+ 		plug->pending_cnt++;
+ 	} else {
+ 		bio_list_add(&conf->pending_bio_list, mbio);
+ 		conf->pending_count++;
+ 	}
+ 	spin_unlock_irqrestore(&conf->device_lock, flags);
+ 	if (!plug)
+ 		md_wakeup_thread(mddev->thread);
+ }
+ 
+ static void raid10_write_request(struct mddev *mddev, struct bio *bio,
+ 				 struct r10bio *r10_bio)
+ {
+ 	struct r10conf *conf = mddev->private;
+ 	int i;
+ 	struct md_rdev *blocked_rdev;
+ 	sector_t sectors;
++>>>>>>> 27f26a0f3767 (md/raid10: refactor some codes from raid10_write_request)
  	int sectors_handled;
  	int max_sectors;
 +	int sectors;
 +
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
 +		md_flush_request(mddev, bio);
 +		return true;
 +	}
 +
 +	/* If this request crosses a chunk boundary, we need to
 +	 * split it.  This will only happen for 1 PAGE (or less) requests.
 +	 */
 +	if (unlikely((bio->bi_sector & chunk_mask) + bio_sectors(bio)
 +		     > chunk_sects
 +		     && (conf->geo.near_copies < conf->geo.raid_disks
 +			 || conf->prev.near_copies < conf->prev.raid_disks))) {
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		bp = bio_split(bio,
 +			       chunk_sects - (bio->bi_sector & (chunk_sects - 1)) );
 +
 +		/* Each of these 'make_request' calls will call 'wait_barrier'.
 +		 * If the first succeeds but the second blocks due to the resync
 +		 * thread raising the barrier, we will deadlock because the
 +		 * IO to the underlying device will be queued in generic_make_request
 +		 * and will never complete, so will never reduce nr_pending.
 +		 * So increment nr_waiting here so no new raise_barriers will
 +		 * succeed, and so the second wait_barrier cannot block.
 +		 */
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting++;
 +		spin_unlock_irq(&conf->resync_lock);
 +
 +		raid10_make_request(mddev, &bp->bio1);
 +		raid10_make_request(mddev, &bp->bio2);
 +
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting--;
 +		wake_up(&conf->wait_barrier);
 +		spin_unlock_irq(&conf->resync_lock);
 +
 +		bio_pair_release(bp);
 +		return true;
 +	bad_map:
 +		printk("md/raid10:%s: make_request bug: can't convert block across chunks"
 +		       " or bigger than %dk %llu %d\n", mdname(mddev), chunk_sects/2,
 +		       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
 +
 +		bio_io_error(bio);
 +		return true;
 +	}
  
  	md_write_start(mddev, bio);
  
@@@ -1509,91 -1451,12 +1681,100 @@@ retry_write
  	bitmap_startwrite(mddev->bitmap, r10_bio->sector, r10_bio->sectors, 0);
  
  	for (i = 0; i < conf->copies; i++) {
++<<<<<<< HEAD
 +		struct bio *mbio;
 +		int d = r10_bio->devs[i].devnum;
 +		if (r10_bio->devs[i].bio) {
 +			struct md_rdev *rdev = conf->mirrors[d].rdev;
 +			mbio = bio_clone_mddev(bio, GFP_NOIO, mddev);
 +			bio_trim(mbio, r10_bio->sector - bio->bi_sector,
 +				 max_sectors);
 +			r10_bio->devs[i].bio = mbio;
 +
 +			mbio->bi_sector	= (r10_bio->devs[i].addr+
 +					   choose_data_offset(r10_bio,
 +							      rdev));
 +			mbio->bi_bdev = rdev->bdev;
 +			mbio->bi_end_io	= raid10_end_write_request;
 +			mbio->bi_rw =
 +				WRITE | do_sync | do_fua | do_discard | do_same;
 +			if (test_bit(FailFast, &conf->mirrors[d].rdev->flags) &&
 +			    enough(conf, d))
 +				mbio->bi_rw |= MD_FAILFAST;
 +			mbio->bi_private = r10_bio;
 +
 +			atomic_inc(&r10_bio->remaining);
 +
 +			cb = blk_check_plugged(raid10_unplug, mddev,
 +					       sizeof(*plug));
 +			if (cb)
 +				plug = container_of(cb, struct raid10_plug_cb,
 +						    cb);
 +			else
 +				plug = NULL;
 +			spin_lock_irqsave(&conf->device_lock, flags);
 +			if (plug) {
 +				bio_list_add(&plug->pending, mbio);
 +				plug->pending_cnt++;
 +			} else {
 +				bio_list_add(&conf->pending_bio_list, mbio);
 +				conf->pending_count++;
 +			}
 +			spin_unlock_irqrestore(&conf->device_lock, flags);
 +			if (!plug)
 +				md_wakeup_thread(mddev->thread);
 +		}
 +
 +		if (r10_bio->devs[i].repl_bio) {
 +			struct md_rdev *rdev = conf->mirrors[d].replacement;
 +			if (rdev == NULL) {
 +				/* Replacement just got moved to main 'rdev' */
 +				smp_mb();
 +				rdev = conf->mirrors[d].rdev;
 +			}
 +			mbio = bio_clone_mddev(bio, GFP_NOIO, mddev);
 +			bio_trim(mbio, r10_bio->sector - bio->bi_sector,
 +				 max_sectors);
 +			r10_bio->devs[i].repl_bio = mbio;
 +
 +			mbio->bi_sector	= (r10_bio->devs[i].addr +
 +					   choose_data_offset(
 +						   r10_bio, rdev));
 +			mbio->bi_bdev = rdev->bdev;
 +			mbio->bi_end_io	= raid10_end_write_request;
 +			mbio->bi_rw =
 +				WRITE | do_sync | do_fua | do_discard | do_same;
 +			mbio->bi_private = r10_bio;
 +
 +			atomic_inc(&r10_bio->remaining);
 +
 +			cb = blk_check_plugged(raid10_unplug, mddev,
 +					       sizeof(*plug));
 +			if (cb)
 +				plug = container_of(cb, struct raid10_plug_cb,
 +						    cb);
 +			else
 +				plug = NULL;
 +			spin_lock_irqsave(&conf->device_lock, flags);
 +			if (plug) {
 +				bio_list_add(&plug->pending, mbio);
 +				plug->pending_cnt++;
 +			} else {
 +				bio_list_add(&conf->pending_bio_list, mbio);
 +				conf->pending_count++;
 +			}
 +			spin_unlock_irqrestore(&conf->device_lock, flags);
 +			if (!plug)
 +				md_wakeup_thread(mddev->thread);
 +		}
++=======
+ 		if (r10_bio->devs[i].bio)
+ 			raid10_write_one_disk(mddev, r10_bio, bio, false,
+ 					      i, max_sectors);
+ 		if (r10_bio->devs[i].repl_bio)
+ 			raid10_write_one_disk(mddev, r10_bio, bio, true,
+ 					      i, max_sectors);
++>>>>>>> 27f26a0f3767 (md/raid10: refactor some codes from raid10_write_request)
  	}
  
  	/* Don't remove the bias on 'remaining' (one_write_done) until
* Unmerged path drivers/md/raid10.c
