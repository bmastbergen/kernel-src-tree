tcp: add tcp_ooo_try_coalesce() helper

jira LE-1907
cve CVE-2018-5390
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 58152ecbbcc6a0ce7fddd5bf5f6ee535834ece0c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/58152ecb.failed

In case skb in out_or_order_queue is the result of
multiple skbs coalescing, we would like to get a proper gso_segs
counter tracking, so that future tcp_drop() can report an accurate
number.

I chose to not implement this tracking for skbs in receive queue,
since they are not dropped, unless socket is disconnected.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
	Acked-by: Yuchung Cheng <ycheng@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 58152ecbbcc6a0ce7fddd5bf5f6ee535834ece0c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_input.c
diff --cc net/ipv4/tcp_input.c
index f78fd9207b7a,3bcd30a2ba06..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -4226,6 -4352,35 +4226,32 @@@ static bool tcp_try_coalesce(struct soc
  	return true;
  }
  
++<<<<<<< HEAD
++=======
+ static bool tcp_ooo_try_coalesce(struct sock *sk,
+ 			     struct sk_buff *to,
+ 			     struct sk_buff *from,
+ 			     bool *fragstolen)
+ {
+ 	bool res = tcp_try_coalesce(sk, to, from, fragstolen);
+ 
+ 	/* In case tcp_drop() is called later, update to->gso_segs */
+ 	if (res) {
+ 		u32 gso_segs = max_t(u16, 1, skb_shinfo(to)->gso_segs) +
+ 			       max_t(u16, 1, skb_shinfo(from)->gso_segs);
+ 
+ 		skb_shinfo(to)->gso_segs = min_t(u32, gso_segs, 0xFFFF);
+ 	}
+ 	return res;
+ }
+ 
+ static void tcp_drop(struct sock *sk, struct sk_buff *skb)
+ {
+ 	sk_drops_add(sk, skb);
+ 	__kfree_skb(skb);
+ }
+ 
++>>>>>>> 58152ecbbcc6 (tcp: add tcp_ooo_try_coalesce() helper)
  /* This one checks to see if we can put data from the
   * out_of_order queue into the receive_queue.
   */
@@@ -4327,73 -4495,74 +4353,127 @@@ static void tcp_data_queue_ofo(struct s
  		goto end;
  	}
  
++<<<<<<< HEAD
 +	seq = TCP_SKB_CB(skb)->seq;
 +	end_seq = TCP_SKB_CB(skb)->end_seq;
 +
 +	if (seq == TCP_SKB_CB(skb1)->end_seq) {
 +		bool fragstolen;
 +
 +		if (!tcp_try_coalesce(sk, skb1, skb, &fragstolen)) {
 +			__skb_queue_after(&tp->out_of_order_queue, skb1, skb);
 +		} else {
 +			tcp_grow_window(sk, skb);
 +			kfree_skb_partial(skb, fragstolen);
 +			skb = NULL;
 +		}
 +
 +		if (!tp->rx_opt.num_sacks ||
 +		    tp->selective_acks[0].end_seq != seq)
 +			goto add_sack;
 +
 +		/* Common case: data arrive in order after hole. */
 +		tp->selective_acks[0].end_seq = end_seq;
 +		goto end;
++=======
+ 	/* In the typical case, we are adding an skb to the end of the list.
+ 	 * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.
+ 	 */
+ 	if (tcp_ooo_try_coalesce(sk, tp->ooo_last_skb,
+ 				 skb, &fragstolen)) {
+ coalesce_done:
+ 		tcp_grow_window(sk, skb);
+ 		kfree_skb_partial(skb, fragstolen);
+ 		skb = NULL;
+ 		goto add_sack;
+ 	}
+ 	/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */
+ 	if (!before(seq, TCP_SKB_CB(tp->ooo_last_skb)->end_seq)) {
+ 		parent = &tp->ooo_last_skb->rbnode;
+ 		p = &parent->rb_right;
+ 		goto insert;
++>>>>>>> 58152ecbbcc6 (tcp: add tcp_ooo_try_coalesce() helper)
  	}
  
 -	/* Find place to insert this segment. Handle overlaps on the way. */
 -	parent = NULL;
 -	while (*p) {
 -		parent = *p;
 -		skb1 = rb_to_skb(parent);
 -		if (before(seq, TCP_SKB_CB(skb1)->seq)) {
 -			p = &parent->rb_left;
 -			continue;
 +	/* Find place to insert this segment. */
 +	while (1) {
 +		if (!after(TCP_SKB_CB(skb1)->seq, seq))
 +			break;
 +		if (skb_queue_is_first(&tp->out_of_order_queue, skb1)) {
 +			skb1 = NULL;
 +			break;
  		}
++<<<<<<< HEAD
 +		skb1 = skb_queue_prev(&tp->out_of_order_queue, skb1);
++=======
+ 		if (before(seq, TCP_SKB_CB(skb1)->end_seq)) {
+ 			if (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {
+ 				/* All the bits are present. Drop. */
+ 				NET_INC_STATS(sock_net(sk),
+ 					      LINUX_MIB_TCPOFOMERGE);
+ 				tcp_drop(sk, skb);
+ 				skb = NULL;
+ 				tcp_dsack_set(sk, seq, end_seq);
+ 				goto add_sack;
+ 			}
+ 			if (after(seq, TCP_SKB_CB(skb1)->seq)) {
+ 				/* Partial overlap. */
+ 				tcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)->end_seq);
+ 			} else {
+ 				/* skb's seq == skb1's seq and skb covers skb1.
+ 				 * Replace skb1 with skb.
+ 				 */
+ 				rb_replace_node(&skb1->rbnode, &skb->rbnode,
+ 						&tp->out_of_order_queue);
+ 				tcp_dsack_extend(sk,
+ 						 TCP_SKB_CB(skb1)->seq,
+ 						 TCP_SKB_CB(skb1)->end_seq);
+ 				NET_INC_STATS(sock_net(sk),
+ 					      LINUX_MIB_TCPOFOMERGE);
+ 				tcp_drop(sk, skb1);
+ 				goto merge_right;
+ 			}
+ 		} else if (tcp_ooo_try_coalesce(sk, skb1,
+ 						skb, &fragstolen)) {
+ 			goto coalesce_done;
+ 		}
+ 		p = &parent->rb_right;
++>>>>>>> 58152ecbbcc6 (tcp: add tcp_ooo_try_coalesce() helper)
  	}
 -insert:
 -	/* Insert segment into RB tree. */
 -	rb_link_node(&skb->rbnode, parent, p);
 -	rb_insert_color(&skb->rbnode, &tp->out_of_order_queue);
  
 -merge_right:
 -	/* Remove other segments covered by skb. */
 -	while ((skb1 = skb_rb_next(skb)) != NULL) {
 +	/* Do skb overlap to previous one? */
 +	if (skb1 && before(seq, TCP_SKB_CB(skb1)->end_seq)) {
 +		if (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {
 +			/* All the bits are present. Drop. */
 +			NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFOMERGE);
 +			__kfree_skb(skb);
 +			skb = NULL;
 +			tcp_dsack_set(sk, seq, end_seq);
 +			goto add_sack;
 +		}
 +		if (after(seq, TCP_SKB_CB(skb1)->seq)) {
 +			/* Partial overlap. */
 +			tcp_dsack_set(sk, seq,
 +				      TCP_SKB_CB(skb1)->end_seq);
 +		} else {
 +			if (skb_queue_is_first(&tp->out_of_order_queue,
 +					       skb1))
 +				skb1 = NULL;
 +			else
 +				skb1 = skb_queue_prev(
 +					&tp->out_of_order_queue,
 +					skb1);
 +		}
 +	}
 +	if (!skb1)
 +		__skb_queue_head(&tp->out_of_order_queue, skb);
 +	else
 +		__skb_queue_after(&tp->out_of_order_queue, skb1, skb);
 +
 +	/* And clean segments covered by new one as whole. */
 +	while (!skb_queue_is_last(&tp->out_of_order_queue, skb)) {
 +		skb1 = skb_queue_next(&tp->out_of_order_queue, skb);
 +
  		if (!after(end_seq, TCP_SKB_CB(skb1)->seq))
  			break;
  		if (before(end_seq, TCP_SKB_CB(skb1)->end_seq)) {
* Unmerged path net/ipv4/tcp_input.c
