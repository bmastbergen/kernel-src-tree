bpf: introduce BPF_JIT_ALWAYS_ON config

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Alexei Starovoitov <ast@kernel.org>
commit 290af86629b25ffd1ed6232c4e9107da031705cb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/290af866.failed

The BPF interpreter has been used as part of the spectre 2 attack CVE-2017-5715.

A quote from goolge project zero blog:
"At this point, it would normally be necessary to locate gadgets in
the host kernel code that can be used to actually leak data by reading
from an attacker-controlled location, shifting and masking the result
appropriately and then using the result of that as offset to an
attacker-controlled address for a load. But piecing gadgets together
and figuring out which ones work in a speculation context seems annoying.
So instead, we decided to use the eBPF interpreter, which is built into
the host kernel - while there is no legitimate way to invoke it from inside
a VM, the presence of the code in the host kernel's text section is sufficient
to make it usable for the attack, just like with ordinary ROP gadgets."

To make attacker job harder introduce BPF_JIT_ALWAYS_ON config
option that removes interpreter from the kernel in favor of JIT-only mode.
So far eBPF JIT is supported by:
x64, arm64, arm32, sparc64, s390, powerpc64, mips64

The start of JITed program is randomized and code page is marked as read-only.
In addition "constant blinding" can be turned on with net.core.bpf_jit_harden

v2->v3:
- move __bpf_prog_ret0 under ifdef (Daniel)

v1->v2:
- fix init order, test_bpf and cBPF (Daniel's feedback)
- fix offloaded bpf (Jakub's feedback)
- add 'return 0' dummy in case something can invoke prog->bpf_func
- retarget bpf tree. For bpf-next the patch would need one extra hunk.
  It will be sent when the trees are merged back to net-next

Considered doing:
  int bpf_jit_enable __read_mostly = BPF_EBPF_JIT_DEFAULT;
but it seems better to land the patch as-is and in bpf-next remove
bpf_jit_enable global variable from all JITs, consolidate in one place
and remove this jit_init() function.

	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 290af86629b25ffd1ed6232c4e9107da031705cb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	init/Kconfig
#	kernel/bpf/core.c
#	lib/test_bpf.c
#	net/core/filter.c
diff --cc init/Kconfig
index 598b4d05d99c,5e2a4a391ba9..000000000000
--- a/init/Kconfig
+++ b/init/Kconfig
@@@ -1444,6 -1298,107 +1444,110 @@@ config AI
  	  by some high performance threaded applications. Disabling
  	  this option saves about 7k.
  
++<<<<<<< HEAD
++=======
+ config ADVISE_SYSCALLS
+ 	bool "Enable madvise/fadvise syscalls" if EXPERT
+ 	default y
+ 	help
+ 	  This option enables the madvise and fadvise syscalls, used by
+ 	  applications to advise the kernel about their future memory or file
+ 	  usage, improving performance. If building an embedded system where no
+ 	  applications use these syscalls, you can disable this option to save
+ 	  space.
+ 
+ config MEMBARRIER
+ 	bool "Enable membarrier() system call" if EXPERT
+ 	default y
+ 	help
+ 	  Enable the membarrier() system call that allows issuing memory
+ 	  barriers across all running threads, which can be used to distribute
+ 	  the cost of user-space memory barriers asymmetrically by transforming
+ 	  pairs of memory barriers into pairs consisting of membarrier() and a
+ 	  compiler barrier.
+ 
+ 	  If unsure, say Y.
+ 
+ config CHECKPOINT_RESTORE
+ 	bool "Checkpoint/restore support" if EXPERT
+ 	select PROC_CHILDREN
+ 	default n
+ 	help
+ 	  Enables additional kernel features in a sake of checkpoint/restore.
+ 	  In particular it adds auxiliary prctl codes to setup process text,
+ 	  data and heap segment sizes, and a few additional /proc filesystem
+ 	  entries.
+ 
+ 	  If unsure, say N here.
+ 
+ config KALLSYMS
+ 	 bool "Load all symbols for debugging/ksymoops" if EXPERT
+ 	 default y
+ 	 help
+ 	   Say Y here to let the kernel print out symbolic crash information and
+ 	   symbolic stack backtraces. This increases the size of the kernel
+ 	   somewhat, as all symbols have to be loaded into the kernel image.
+ 
+ config KALLSYMS_ALL
+ 	bool "Include all symbols in kallsyms"
+ 	depends on DEBUG_KERNEL && KALLSYMS
+ 	help
+ 	   Normally kallsyms only contains the symbols of functions for nicer
+ 	   OOPS messages and backtraces (i.e., symbols from the text and inittext
+ 	   sections). This is sufficient for most cases. And only in very rare
+ 	   cases (e.g., when a debugger is used) all symbols are required (e.g.,
+ 	   names of variables from the data sections, etc).
+ 
+ 	   This option makes sure that all symbols are loaded into the kernel
+ 	   image (i.e., symbols from all sections) in cost of increased kernel
+ 	   size (depending on the kernel configuration, it may be 300KiB or
+ 	   something like this).
+ 
+ 	   Say N unless you really need all symbols.
+ 
+ config KALLSYMS_ABSOLUTE_PERCPU
+ 	bool
+ 	depends on KALLSYMS
+ 	default X86_64 && SMP
+ 
+ config KALLSYMS_BASE_RELATIVE
+ 	bool
+ 	depends on KALLSYMS
+ 	default !IA64 && !(TILE && 64BIT)
+ 	help
+ 	  Instead of emitting them as absolute values in the native word size,
+ 	  emit the symbol references in the kallsyms table as 32-bit entries,
+ 	  each containing a relative value in the range [base, base + U32_MAX]
+ 	  or, when KALLSYMS_ABSOLUTE_PERCPU is in effect, each containing either
+ 	  an absolute value in the range [0, S32_MAX] or a relative value in the
+ 	  range [base, base + S32_MAX], where base is the lowest relative symbol
+ 	  address encountered in the image.
+ 
+ 	  On 64-bit builds, this reduces the size of the address table by 50%,
+ 	  but more importantly, it results in entries whose values are build
+ 	  time constants, and no relocation pass is required at runtime to fix
+ 	  up the entries based on the runtime load address of the kernel.
+ 
+ # end of the "standard kernel features (expert users)" menu
+ 
+ # syscall, maps, verifier
+ config BPF_SYSCALL
+ 	bool "Enable bpf() system call"
+ 	select ANON_INODES
+ 	select BPF
+ 	default n
+ 	help
+ 	  Enable the bpf() system call that allows to manipulate eBPF
+ 	  programs and maps via file descriptors.
+ 
+ config BPF_JIT_ALWAYS_ON
+ 	bool "Permanently enable BPF JIT and remove BPF interpreter"
+ 	depends on BPF_SYSCALL && HAVE_EBPF_JIT && BPF_JIT
+ 	help
+ 	  Enables BPF JIT and removes BPF interpreter to avoid
+ 	  speculative execution of BPF instructions by the interpreter
+ 
++>>>>>>> 290af86629b2 (bpf: introduce BPF_JIT_ALWAYS_ON config)
  config USERFAULTFD
  	bool "Enable userfaultfd() system call"
  	select ANON_INODES
diff --cc net/core/filter.c
index 060ed5f86613,d339ef170df6..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -647,12 -949,127 +647,133 @@@ void sk_filter_release_rcu(struct rcu_h
  {
  	struct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);
  
 -	__sk_filter_release(fp);
 +	bpf_jit_free(fp);
 +	kfree(fp);
  }
 +EXPORT_SYMBOL(sk_filter_release_rcu);
  
++<<<<<<< HEAD
 +static int __sk_prepare_filter(struct sk_filter *fp)
++=======
+ /**
+  *	sk_filter_release - release a socket filter
+  *	@fp: filter to remove
+  *
+  *	Remove a filter from a socket and release its resources.
+  */
+ static void sk_filter_release(struct sk_filter *fp)
+ {
+ 	if (refcount_dec_and_test(&fp->refcnt))
+ 		call_rcu(&fp->rcu, sk_filter_release_rcu);
+ }
+ 
+ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)
+ {
+ 	u32 filter_size = bpf_prog_size(fp->prog->len);
+ 
+ 	atomic_sub(filter_size, &sk->sk_omem_alloc);
+ 	sk_filter_release(fp);
+ }
+ 
+ /* try to charge the socket memory if there is space available
+  * return true on success
+  */
+ static bool __sk_filter_charge(struct sock *sk, struct sk_filter *fp)
+ {
+ 	u32 filter_size = bpf_prog_size(fp->prog->len);
+ 
+ 	/* same check as in sock_kmalloc() */
+ 	if (filter_size <= sysctl_optmem_max &&
+ 	    atomic_read(&sk->sk_omem_alloc) + filter_size < sysctl_optmem_max) {
+ 		atomic_add(filter_size, &sk->sk_omem_alloc);
+ 		return true;
+ 	}
+ 	return false;
+ }
+ 
+ bool sk_filter_charge(struct sock *sk, struct sk_filter *fp)
+ {
+ 	if (!refcount_inc_not_zero(&fp->refcnt))
+ 		return false;
+ 
+ 	if (!__sk_filter_charge(sk, fp)) {
+ 		sk_filter_release(fp);
+ 		return false;
+ 	}
+ 	return true;
+ }
+ 
+ static struct bpf_prog *bpf_migrate_filter(struct bpf_prog *fp)
+ {
+ 	struct sock_filter *old_prog;
+ 	struct bpf_prog *old_fp;
+ 	int err, new_len, old_len = fp->len;
+ 
+ 	/* We are free to overwrite insns et al right here as it
+ 	 * won't be used at this point in time anymore internally
+ 	 * after the migration to the internal BPF instruction
+ 	 * representation.
+ 	 */
+ 	BUILD_BUG_ON(sizeof(struct sock_filter) !=
+ 		     sizeof(struct bpf_insn));
+ 
+ 	/* Conversion cannot happen on overlapping memory areas,
+ 	 * so we need to keep the user BPF around until the 2nd
+ 	 * pass. At this time, the user BPF is stored in fp->insns.
+ 	 */
+ 	old_prog = kmemdup(fp->insns, old_len * sizeof(struct sock_filter),
+ 			   GFP_KERNEL | __GFP_NOWARN);
+ 	if (!old_prog) {
+ 		err = -ENOMEM;
+ 		goto out_err;
+ 	}
+ 
+ 	/* 1st pass: calculate the new program length. */
+ 	err = bpf_convert_filter(old_prog, old_len, NULL, &new_len);
+ 	if (err)
+ 		goto out_err_free;
+ 
+ 	/* Expand fp for appending the new filter representation. */
+ 	old_fp = fp;
+ 	fp = bpf_prog_realloc(old_fp, bpf_prog_size(new_len), 0);
+ 	if (!fp) {
+ 		/* The old_fp is still around in case we couldn't
+ 		 * allocate new memory, so uncharge on that one.
+ 		 */
+ 		fp = old_fp;
+ 		err = -ENOMEM;
+ 		goto out_err_free;
+ 	}
+ 
+ 	fp->len = new_len;
+ 
+ 	/* 2nd pass: remap sock_filter insns into bpf_insn insns. */
+ 	err = bpf_convert_filter(old_prog, old_len, fp, &new_len);
+ 	if (err)
+ 		/* 2nd bpf_convert_filter() can fail only if it fails
+ 		 * to allocate memory, remapping must succeed. Note,
+ 		 * that at this time old_fp has already been released
+ 		 * by krealloc().
+ 		 */
+ 		goto out_err_free;
+ 
+ 	fp = bpf_prog_select_runtime(fp, &err);
+ 	if (err)
+ 		goto out_err_free;
+ 
+ 	kfree(old_prog);
+ 	return fp;
+ 
+ out_err_free:
+ 	kfree(old_prog);
+ out_err:
+ 	__bpf_prog_release(fp);
+ 	return ERR_PTR(err);
+ }
+ 
+ static struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp,
+ 					   bpf_aux_classic_check_t trans)
++>>>>>>> 290af86629b2 (bpf: introduce BPF_JIT_ALWAYS_ON config)
  {
  	int err;
  
* Unmerged path kernel/bpf/core.c
* Unmerged path lib/test_bpf.c
* Unmerged path init/Kconfig
* Unmerged path kernel/bpf/core.c
* Unmerged path lib/test_bpf.c
* Unmerged path net/core/filter.c
diff --git a/net/core/sysctl_net_core.c b/net/core/sysctl_net_core.c
index ca2d9afec36e..497aee65330a 100644
--- a/net/core/sysctl_net_core.c
+++ b/net/core/sysctl_net_core.c
@@ -213,7 +213,13 @@ static struct ctl_table net_core_table[] = {
 		.data		= &bpf_jit_enable,
 		.maxlen		= sizeof(int),
 		.mode		= 0644,
+#ifndef CONFIG_BPF_JIT_ALWAYS_ON
 		.proc_handler	= proc_dointvec
+#else
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= &one,
+		.extra2		= &one,
+#endif
 	},
 #endif
 	{
diff --git a/net/socket.c b/net/socket.c
index b4122d2a2a25..78db7a27693e 100644
--- a/net/socket.c
+++ b/net/socket.c
@@ -2724,6 +2724,15 @@ out_fs:
 
 core_initcall(sock_init);	/* early initcall */
 
+static int __init jit_init(void)
+{
+#ifdef CONFIG_BPF_JIT_ALWAYS_ON
+	bpf_jit_enable = 1;
+#endif
+	return 0;
+}
+pure_initcall(jit_init);
+
 #ifdef CONFIG_PROC_FS
 void socket_seq_show(struct seq_file *seq)
 {
