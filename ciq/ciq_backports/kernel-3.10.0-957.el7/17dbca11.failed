x86/speculation/l1tf: Add sysfs reporting for l1tf

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [kernel] x86, l1tf: add sysfs reporting for l1tf (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 83.15%
commit-author Andi Kleen <ak@linux.intel.com>
commit 17dbca119312b4e8173d4e25ff64262119fcef38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/17dbca11.failed

L1TF core kernel workarounds are cheap and normally always enabled, However
they still should be reported in sysfs if the system is vulnerable or
mitigated. Add the necessary CPU feature/bug bits.

- Extend the existing checks for Meltdowns to determine if the system is
  vulnerable. All CPUs which are not vulnerable to Meltdown are also not
  vulnerable to L1TF

- Check for 32bit non PAE and emit a warning as there is no practical way
  for mitigation due to the limited physical address bits

- If the system has more than MAX_PA/2 physical memory the invert page
  workarounds don't protect the system against the L1TF attack anymore,
  because an inverted physical address will also point to valid
  memory. Print a warning in this case and report that the system is
  vulnerable.

Add a function which returns the PFN limit for the L1TF mitigation, which
will be used in follow up patches for sanity and range checks.

[ tglx: Renamed the CPU feature bit to L1TF_PTEINV ]

	Signed-off-by: Andi Kleen <ak@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
	Acked-by: Dave Hansen <dave.hansen@intel.com>


(cherry picked from commit 17dbca119312b4e8173d4e25ff64262119fcef38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
#	drivers/base/cpu.c
#	include/linux/cpu.h
diff --cc arch/x86/include/asm/cpufeatures.h
index 7a3e9c71ed7d,f41cf9df4a83..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -190,126 -190,151 +190,159 @@@
   *
   * Reuse free bits when adding new feature flags!
   */
++<<<<<<< HEAD
 +
 +#define X86_FEATURE_RING3MWAIT	(7*32+ 0) /* Ring 3 MONITOR/MWAIT */
 +#define X86_FEATURE_CPB		(7*32+ 2) /* AMD Core Performance Boost */
 +#define X86_FEATURE_EPB		(7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 +#define X86_FEATURE_CAT_L3	(7*32+ 4) /* Cache Allocation Technology L3 */
 +#define X86_FEATURE_CAT_L2	(7*32+ 5) /* Cache Allocation Technology L2 */
 +#define X86_FEATURE_CDP_L3	(7*32+ 6) /* Code and Data Prioritization L3 */
 +
 +#define X86_FEATURE_HW_PSTATE	(7*32+ 8) /* AMD HW-PState */
 +#define X86_FEATURE_PROC_FEEDBACK (7*32+ 9) /* AMD ProcFeedbackInterface */
 +#define X86_FEATURE_SME		( 7*32+10) /* AMD Secure Memory Encryption */
 +#define X86_FEATURE_RETPOLINE_AMD (7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */
 +#define X86_FEATURE_INTEL_PPIN	( 7*32+14) /* Intel Processor Inventory Number */
 +#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */
 +
 +#define X86_FEATURE_MBA		( 7*32+18) /* Memory Bandwidth Allocation */
 +#define X86_FEATURE_IBP_DISABLE ( 7*32+21) /* Old AMD Indirect Branch Predictor Disable */
++=======
+ #define X86_FEATURE_RING3MWAIT		( 7*32+ 0) /* Ring 3 MONITOR/MWAIT instructions */
+ #define X86_FEATURE_CPUID_FAULT		( 7*32+ 1) /* Intel CPUID faulting */
+ #define X86_FEATURE_CPB			( 7*32+ 2) /* AMD Core Performance Boost */
+ #define X86_FEATURE_EPB			( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
+ #define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
+ #define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
+ #define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
+ #define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID && CR4.PCIDE=1 */
+ #define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
+ #define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
+ #define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
+ #define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
+ #define X86_FEATURE_RETPOLINE		( 7*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
+ #define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* "" AMD Retpoline mitigation for Spectre variant 2 */
+ #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
+ #define X86_FEATURE_CDP_L2		( 7*32+15) /* Code and Data Prioritization L2 */
+ #define X86_FEATURE_MSR_SPEC_CTRL	( 7*32+16) /* "" MSR SPEC_CTRL is implemented */
+ #define X86_FEATURE_SSBD		( 7*32+17) /* Speculative Store Bypass Disable */
+ #define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
+ #define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* "" Fill RSB on context switches */
+ #define X86_FEATURE_SEV			( 7*32+20) /* AMD Secure Encrypted Virtualization */
+ #define X86_FEATURE_USE_IBPB		( 7*32+21) /* "" Indirect Branch Prediction Barrier enabled */
+ #define X86_FEATURE_USE_IBRS_FW		( 7*32+22) /* "" Use IBRS during runtime firmware calls */
+ #define X86_FEATURE_SPEC_STORE_BYPASS_DISABLE	( 7*32+23) /* "" Disable Speculative Store Bypass. */
+ #define X86_FEATURE_LS_CFG_SSBD		( 7*32+24)  /* "" AMD SSBD implementation via LS_CFG MSR */
+ #define X86_FEATURE_IBRS		( 7*32+25) /* Indirect Branch Restricted Speculation */
+ #define X86_FEATURE_IBPB		( 7*32+26) /* Indirect Branch Prediction Barrier */
+ #define X86_FEATURE_STIBP		( 7*32+27) /* Single Thread Indirect Branch Predictors */
+ #define X86_FEATURE_ZEN			( 7*32+28) /* "" CPU is AMD family 0x17 (Zen) */
+ #define X86_FEATURE_L1TF_PTEINV		( 7*32+29) /* "" L1TF workaround PTE inversion */
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  
  /* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 -
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 -
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
  
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (EDX), word 11 */
 -#define X86_FEATURE_CQM_LLC		(11*32+ 1) /* LLC QoS if 1 */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (EDX), word 12 */
 -#define X86_FEATURE_CQM_OCCUP_LLC	(12*32+ 0) /* LLC occupancy monitoring */
 -#define X86_FEATURE_CQM_MBM_TOTAL	(12*32+ 1) /* LLC Total MBM monitoring */
 -#define X86_FEATURE_CQM_MBM_LOCAL	(12*32+ 2) /* LLC Local MBM monitoring */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
 -#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 -#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 -#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
 -#define X86_FEATURE_AMD_IBPB		(13*32+12) /* "" Indirect Branch Prediction Barrier */
 -#define X86_FEATURE_AMD_IBRS		(13*32+14) /* "" Indirect Branch Restricted Speculation */
 -#define X86_FEATURE_AMD_STIBP		(13*32+15) /* "" Single Thread Indirect Branch Predictors */
 -#define X86_FEATURE_AMD_SSBD		(13*32+24) /* "" Speculative Store Bypass Disable */
 -#define X86_FEATURE_VIRT_SSBD		(13*32+25) /* Virtualized Speculative Store Bypass Disable */
 -#define X86_FEATURE_AMD_SSB_NO		(13*32+26) /* "" Speculative Store Bypass is fixed in hardware. */
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
 +#define X86_FEATURE_IBPB		(13*32+12) /* Indirect Branch Prediction Barrier */
 +#define X86_FEATURE_IBRS		(13*32+14) /* Indirect Branch Restricted Speculation */
 +#define X86_FEATURE_STIBP		(13*32+15) /* Single Thread Indirect Branch Predictors */
  
 -/* Thermal and Power Management Leaf, CPUID level 0x00000006 (EAX), word 14 */
 -#define X86_FEATURE_DTHERM		(14*32+ 0) /* Digital Thermal Sensor */
 -#define X86_FEATURE_IDA			(14*32+ 1) /* Intel Dynamic Acceleration */
 -#define X86_FEATURE_ARAT		(14*32+ 2) /* Always Running APIC Timer */
 -#define X86_FEATURE_PLN			(14*32+ 4) /* Intel Power Limit Notification */
 -#define X86_FEATURE_PTS			(14*32+ 6) /* Intel Package Thermal Status */
 -#define X86_FEATURE_HWP			(14*32+ 7) /* Intel Hardware P-states */
 -#define X86_FEATURE_HWP_NOTIFY		(14*32+ 8) /* HWP Notification */
 -#define X86_FEATURE_HWP_ACT_WINDOW	(14*32+ 9) /* HWP Activity Window */
 -#define X86_FEATURE_HWP_EPP		(14*32+10) /* HWP Energy Perf. Preference */
 -#define X86_FEATURE_HWP_PKG_REQ		(14*32+11) /* HWP Package Level Request */
 +/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */
 +#define X86_FEATURE_OVERFLOW_RECOV (17*32+0) /* MCA overflow recovery support */
 +#define X86_FEATURE_SUCCOR	(17*32+1) /* Uncorrectable error containment and recovery */
 +#define X86_FEATURE_SMCA	(17*32+3) /* Scalable MCA */
  
 -/* AMD SVM Feature Identification, CPUID level 0x8000000a (EDX), word 15 */
 -#define X86_FEATURE_NPT			(15*32+ 0) /* Nested Page Table support */
 -#define X86_FEATURE_LBRV		(15*32+ 1) /* LBR Virtualization support */
 -#define X86_FEATURE_SVML		(15*32+ 2) /* "svm_lock" SVM locking MSR */
 -#define X86_FEATURE_NRIPS		(15*32+ 3) /* "nrip_save" SVM next_rip save */
 -#define X86_FEATURE_TSCRATEMSR		(15*32+ 4) /* "tsc_scale" TSC scaling support */
 -#define X86_FEATURE_VMCBCLEAN		(15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 -#define X86_FEATURE_FLUSHBYASID		(15*32+ 6) /* flush-by-ASID support */
 -#define X86_FEATURE_DECODEASSISTS	(15*32+ 7) /* Decode Assists support */
 -#define X86_FEATURE_PAUSEFILTER		(15*32+10) /* filtered pause intercept */
 -#define X86_FEATURE_PFTHRESHOLD		(15*32+12) /* pause filter threshold */
 -#define X86_FEATURE_AVIC		(15*32+13) /* Virtual Interrupt Controller */
 -#define X86_FEATURE_V_VMSAVE_VMLOAD	(15*32+15) /* Virtual VMSAVE VMLOAD */
 -#define X86_FEATURE_VGIF		(15*32+16) /* Virtual GIF */
 +/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */
 +#define X86_FEATURE_DTHERM	(14*32+ 0) /* Digital Thermal Sensor */
 +#define X86_FEATURE_IDA		(14*32+ 1) /* Intel Dynamic Acceleration */
 +#define X86_FEATURE_ARAT	(14*32+ 2) /* Always Running APIC Timer */
 +#define X86_FEATURE_PLN		(14*32+ 4) /* Intel Power Limit Notification */
 +#define X86_FEATURE_PTS		(14*32+ 6) /* Intel Package Thermal Status */
 +#define X86_FEATURE_HWP		(14*32+ 7) /* Intel Hardware P-states */
 +#define X86_FEATURE_HWP_NOTIFY	(14*32+ 8) /* HWP Notification */
 +#define X86_FEATURE_HWP_ACT_WINDOW (14*32+ 9) /* HWP Activity Window */
 +#define X86_FEATURE_HWP_EPP	(14*32+10) /* HWP Energy Perf. Preference */
 +#define X86_FEATURE_HWP_PKG_REQ (14*32+11) /* HWP Package Level Request */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (ECX), word 16 */
 -#define X86_FEATURE_AVX512VBMI		(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 -#define X86_FEATURE_UMIP		(16*32+ 2) /* User Mode Instruction Protection */
 -#define X86_FEATURE_PKU			(16*32+ 3) /* Protection Keys for Userspace */
 -#define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */
 -#define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 -#define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */
 -#define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */
 -#define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */
 -#define X86_FEATURE_AVX512_VNNI		(16*32+11) /* Vector Neural Network Instructions */
 -#define X86_FEATURE_AVX512_BITALG	(16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB instructions */
 -#define X86_FEATURE_TME			(16*32+13) /* Intel Total Memory Encryption */
 -#define X86_FEATURE_AVX512_VPOPCNTDQ	(16*32+14) /* POPCNT for vectors of DW/QW */
 -#define X86_FEATURE_LA57		(16*32+16) /* 5-level page tables */
 -#define X86_FEATURE_RDPID		(16*32+22) /* RDPID instruction */
 -#define X86_FEATURE_CLDEMOTE		(16*32+25) /* CLDEMOTE instruction */
 +/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */
 +#define X86_FEATURE_NPT		(15*32+ 0) /* Nested Page Table support */
 +#define X86_FEATURE_LBRV	(15*32+ 1) /* LBR Virtualization support */
 +#define X86_FEATURE_SVML	(15*32+ 2) /* "svm_lock" SVM locking MSR */
 +#define X86_FEATURE_NRIPS	(15*32+ 3) /* "nrip_save" SVM next_rip save */
 +#define X86_FEATURE_TSCRATEMSR  (15*32+ 4) /* "tsc_scale" TSC scaling support */
 +#define X86_FEATURE_VMCBCLEAN   (15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 +#define X86_FEATURE_FLUSHBYASID (15*32+ 6) /* flush-by-ASID support */
 +#define X86_FEATURE_DECODEASSISTS (15*32+ 7) /* Decode Assists support */
 +#define X86_FEATURE_PAUSEFILTER (15*32+10) /* filtered pause intercept */
 +#define X86_FEATURE_PFTHRESHOLD (15*32+12) /* pause filter threshold */
 +#define X86_FEATURE_AVIC	(15*32+13) /* Virtual Interrupt Controller */
 +#define X86_FEATURE_V_VMSAVE_VMLOAD (15*32+15) /* Virtual VMSAVE VMLOAD */
 +#define X86_FEATURE_VGIF	(15*32+16) /* Virtual GIF */
  
 -/* AMD-defined CPU features, CPUID level 0x80000007 (EBX), word 17 */
 -#define X86_FEATURE_OVERFLOW_RECOV	(17*32+ 0) /* MCA overflow recovery support */
 -#define X86_FEATURE_SUCCOR		(17*32+ 1) /* Uncorrectable error containment and recovery */
 -#define X86_FEATURE_SMCA		(17*32+ 3) /* Scalable MCA */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */
 +#define X86_FEATURE_AVX512VBMI	(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 +#define X86_FEATURE_PKU		(16*32+ 3) /* Protection Keys for Userspace */
 +#define X86_FEATURE_OSPKE	(16*32+ 4) /* OS Protection Keys Enable */
 +#define X86_FEATURE_AVX512_VBMI2 (16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 +#define X86_FEATURE_GFNI	(16*32+ 8) /* Galois Field New Instructions */
 +#define X86_FEATURE_VAES	(16*32+ 9) /* Vector AES */
 +#define X86_FEATURE_VPCLMULQDQ	(16*32+ 10) /* Carry-Less Multiplication Double Quadword */
 +#define X86_FEATURE_AVX512_VNNI (16*32+ 11) /* Vector Neural Network Instructions */
 +#define X86_FEATURE_AVX512_BITALG (16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB */
 +#define X86_FEATURE_AVX512_VPOPCNTDQ (16*32+14) /* POPCNT for vectors of DW/QW */
  
  /* Intel-defined CPU features, CPUID level 0x00000007:0 (EDX), word 18 */
  #define X86_FEATURE_AVX512_4VNNIW	(18*32+ 2) /* AVX-512 Neural Network Instructions */
@@@ -321,12 -348,32 +354,40 @@@
  /*
   * BUG word(s)
   */
 -#define X86_BUG(x)			(NCAPINTS*32 + (x))
 +#define X86_BUG(x)		(NCAPINTS*32 + (x))
  
++<<<<<<< HEAD
 +#define X86_BUG_F00F		X86_BUG(0) /* Intel F00F */
 +#define X86_BUG_FDIV		X86_BUG(1) /* FPU FDIV */
 +#define X86_BUG_COMA		X86_BUG(2) /* Cyrix 6x86 coma */
 +#define X86_BUG_AMD_TLB_MMATCH	X86_BUG(3) /* AMD Erratum 383 */
 +#define X86_BUG_AMD_APIC_C1E	X86_BUG(4) /* AMD Erratum 400 */
++=======
+ #define X86_BUG_F00F			X86_BUG(0) /* Intel F00F */
+ #define X86_BUG_FDIV			X86_BUG(1) /* FPU FDIV */
+ #define X86_BUG_COMA			X86_BUG(2) /* Cyrix 6x86 coma */
+ #define X86_BUG_AMD_TLB_MMATCH		X86_BUG(3) /* "tlb_mmatch" AMD Erratum 383 */
+ #define X86_BUG_AMD_APIC_C1E		X86_BUG(4) /* "apic_c1e" AMD Erratum 400 */
+ #define X86_BUG_11AP			X86_BUG(5) /* Bad local APIC aka 11AP */
+ #define X86_BUG_FXSAVE_LEAK		X86_BUG(6) /* FXSAVE leaks FOP/FIP/FOP */
+ #define X86_BUG_CLFLUSH_MONITOR		X86_BUG(7) /* AAI65, CLFLUSH required before MONITOR */
+ #define X86_BUG_SYSRET_SS_ATTRS		X86_BUG(8) /* SYSRET doesn't fix up SS attrs */
+ #ifdef CONFIG_X86_32
+ /*
+  * 64-bit kernels don't use X86_BUG_ESPFIX.  Make the define conditional
+  * to avoid confusion.
+  */
+ #define X86_BUG_ESPFIX			X86_BUG(9) /* "" IRET to 16-bit SS corrupts ESP/RSP high bits */
+ #endif
+ #define X86_BUG_NULL_SEG		X86_BUG(10) /* Nulling a selector preserves the base */
+ #define X86_BUG_SWAPGS_FENCE		X86_BUG(11) /* SWAPGS without input dep on GS */
+ #define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */
+ #define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */
+ #define X86_BUG_CPU_MELTDOWN		X86_BUG(14) /* CPU is affected by meltdown attack and needs kernel page table isolation */
+ #define X86_BUG_SPECTRE_V1		X86_BUG(15) /* CPU is affected by Spectre variant 1 attack with conditional branches */
+ #define X86_BUG_SPECTRE_V2		X86_BUG(16) /* CPU is affected by Spectre variant 2 attack with indirect branches */
+ #define X86_BUG_SPEC_STORE_BYPASS	X86_BUG(17) /* CPU is affected by speculative store bypass attack */
+ #define X86_BUG_L1TF			X86_BUG(18) /* CPU is affected by L1 Terminal Fault */
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  
  #endif /* _ASM_X86_CPUFEATURES_H */
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,3ce9674e4887..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -21,10 -25,33 +21,40 @@@
  #include <asm/paravirt.h>
  #include <asm/alternative.h>
  #include <asm/pgtable.h>
++<<<<<<< HEAD
 +#include <asm/cacheflush.h>
 +#include <asm/spec_ctrl.h>
 +
 +static void __init spectre_v2_select_mitigation(void);
++=======
+ #include <asm/set_memory.h>
+ #include <asm/intel-family.h>
+ #include <asm/e820/api.h>
+ 
+ static void __init spectre_v2_select_mitigation(void);
+ static void __init ssb_select_mitigation(void);
+ static void __init l1tf_select_mitigation(void);
+ 
+ /*
+  * Our boot-time value of the SPEC_CTRL MSR. We read it once so that any
+  * writes to SPEC_CTRL contain whatever reserved bits have been set.
+  */
+ u64 __ro_after_init x86_spec_ctrl_base;
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_base);
+ 
+ /*
+  * The vendor and possibly platform specific bits which can be modified in
+  * x86_spec_ctrl_base.
+  */
+ static u64 __ro_after_init x86_spec_ctrl_mask = SPEC_CTRL_IBRS;
+ 
+ /*
+  * AMD specific MSR info for Speculative Store Bypass control.
+  * x86_amd_ls_cfg_ssbd_mask is initialized in identify_boot_cpu().
+  */
+ u64 __ro_after_init x86_amd_ls_cfg_base;
+ u64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  
  void __init check_bugs(void)
  {
@@@ -35,10 -62,29 +65,12 @@@
  		print_cpu_info(&boot_cpu_data);
  	}
  
 -	/*
 -	 * Read the SPEC_CTRL MSR to account for reserved bits which may
 -	 * have unknown values. AMD64_LS_CFG MSR is cached in the early AMD
 -	 * init code as it is not enumerated and depends on the family.
 -	 */
 -	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 -		rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
 -
 -	/* Allow STIBP in MSR_SPEC_CTRL if supported */
 -	if (boot_cpu_has(X86_FEATURE_STIBP))
 -		x86_spec_ctrl_mask |= SPEC_CTRL_STIBP;
 -
 -	/* Select the proper spectre mitigation before patching alternatives */
 +	spec_ctrl_init();
  	spectre_v2_select_mitigation();
 -
 -	/*
 -	 * Select proper mitigation for any exposure to the Speculative Store
 -	 * Bypass vulnerability.
 -	 */
 -	ssb_select_mitigation();
 +	spec_ctrl_cpu_init();
  
+ 	l1tf_select_mitigation();
+ 
  #ifdef CONFIG_X86_32
  	/*
  	 * Check whether we are able to run this kernel safely on SMP.
@@@ -104,6 -138,141 +136,144 @@@ enum spectre_v2_mitigation_cmd spectre_
  #undef pr_fmt
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
++<<<<<<< HEAD
++=======
+ static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
+ 	SPECTRE_V2_NONE;
+ 
+ void
+ x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)
+ {
+ 	u64 msrval, guestval, hostval = x86_spec_ctrl_base;
+ 	struct thread_info *ti = current_thread_info();
+ 
+ 	/* Is MSR_SPEC_CTRL implemented ? */
+ 	if (static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL)) {
+ 		/*
+ 		 * Restrict guest_spec_ctrl to supported values. Clear the
+ 		 * modifiable bits in the host base value and or the
+ 		 * modifiable bits from the guest value.
+ 		 */
+ 		guestval = hostval & ~x86_spec_ctrl_mask;
+ 		guestval |= guest_spec_ctrl & x86_spec_ctrl_mask;
+ 
+ 		/* SSBD controlled in MSR_SPEC_CTRL */
+ 		if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD))
+ 			hostval |= ssbd_tif_to_spec_ctrl(ti->flags);
+ 
+ 		if (hostval != guestval) {
+ 			msrval = setguest ? guestval : hostval;
+ 			wrmsrl(MSR_IA32_SPEC_CTRL, msrval);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * If SSBD is not handled in MSR_SPEC_CTRL on AMD, update
+ 	 * MSR_AMD64_L2_CFG or MSR_VIRT_SPEC_CTRL if supported.
+ 	 */
+ 	if (!static_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&
+ 	    !static_cpu_has(X86_FEATURE_VIRT_SSBD))
+ 		return;
+ 
+ 	/*
+ 	 * If the host has SSBD mitigation enabled, force it in the host's
+ 	 * virtual MSR value. If its not permanently enabled, evaluate
+ 	 * current's TIF_SSBD thread flag.
+ 	 */
+ 	if (static_cpu_has(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE))
+ 		hostval = SPEC_CTRL_SSBD;
+ 	else
+ 		hostval = ssbd_tif_to_spec_ctrl(ti->flags);
+ 
+ 	/* Sanitize the guest value */
+ 	guestval = guest_virt_spec_ctrl & SPEC_CTRL_SSBD;
+ 
+ 	if (hostval != guestval) {
+ 		unsigned long tif;
+ 
+ 		tif = setguest ? ssbd_spec_ctrl_to_tif(guestval) :
+ 				 ssbd_spec_ctrl_to_tif(hostval);
+ 
+ 		speculative_store_bypass_update(tif);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(x86_virt_spec_ctrl);
+ 
+ static void x86_amd_ssb_disable(void)
+ {
+ 	u64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_ssbd_mask;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_VIRT_SSBD))
+ 		wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, SPEC_CTRL_SSBD);
+ 	else if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD))
+ 		wrmsrl(MSR_AMD64_LS_CFG, msrval);
+ }
+ 
+ static void __init l1tf_select_mitigation(void)
+ {
+ 	u64 half_pa;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
+ 		return;
+ 
+ #if CONFIG_PGTABLE_LEVELS == 2
+ 	pr_warn("Kernel not compiled for PAE. No mitigation for L1TF\n");
+ 	return;
+ #endif
+ 
+ 	/*
+ 	 * This is extremely unlikely to happen because almost all
+ 	 * systems have far more MAX_PA/2 than RAM can be fit into
+ 	 * DIMM slots.
+ 	 */
+ 	half_pa = (u64)l1tf_pfn_limit() << PAGE_SHIFT;
+ 	if (e820__mapped_any(half_pa, ULLONG_MAX - half_pa, E820_TYPE_RAM)) {
+ 		pr_warn("System has more than MAX_PA/2 memory. L1TF mitigation not effective.\n");
+ 		return;
+ 	}
+ 
+ 	setup_force_cpu_cap(X86_FEATURE_L1TF_PTEINV);
+ }
+ 
+ #ifdef RETPOLINE
+ static bool spectre_v2_bad_module;
+ 
+ bool retpoline_module_ok(bool has_retpoline)
+ {
+ 	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
+ 		return true;
+ 
+ 	pr_err("System may be vulnerable to spectre v2\n");
+ 	spectre_v2_bad_module = true;
+ 	return false;
+ }
+ 
+ static inline const char *spectre_v2_module_string(void)
+ {
+ 	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
+ }
+ #else
+ static inline const char *spectre_v2_module_string(void) { return ""; }
+ #endif
+ 
+ static void __init spec2_print_if_insecure(const char *reason)
+ {
+ 	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static void __init spec2_print_if_secure(const char *reason)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static inline bool retp_compiler(void)
+ {
+ 	return __is_defined(RETPOLINE);
+ }
+ 
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  static inline bool match_option(const char *arg, int arglen, const char *opt)
  {
  	int len = strlen(opt);
@@@ -153,73 -377,371 +323,110 @@@ void __spectre_v2_select_mitigation(voi
  
  	case SPECTRE_V2_CMD_FORCE:
  	case SPECTRE_V2_CMD_AUTO:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_auto;
 -		break;
 -	case SPECTRE_V2_CMD_RETPOLINE_AMD:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_amd;
 -		break;
 -	case SPECTRE_V2_CMD_RETPOLINE_GENERIC:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_generic;
  		break;
 -	case SPECTRE_V2_CMD_RETPOLINE:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_auto;
 -		break;
 -	}
 -	pr_err("Spectre mitigation: kernel not compiled with retpoline; no mitigation available!");
 -	return;
 -
 -retpoline_auto:
 -	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {
 -	retpoline_amd:
 -		if (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {
 -			pr_err("Spectre mitigation: LFENCE not serializing, switching to generic retpoline\n");
 -			goto retpoline_generic;
 -		}
 -		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :
 -					 SPECTRE_V2_RETPOLINE_MINIMAL_AMD;
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 -	} else {
 -	retpoline_generic:
 -		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_GENERIC :
 -					 SPECTRE_V2_RETPOLINE_MINIMAL;
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 -	}
 -
 -	spectre_v2_enabled = mode;
 -	pr_info("%s\n", spectre_v2_strings[mode]);
 -
 -	/*
 -	 * If neither SMEP nor PTI are available, there is a risk of
 -	 * hitting userspace addresses in the RSB after a context switch
 -	 * from a shallow call stack to a deeper one. To prevent this fill
 -	 * the entire RSB, even when using IBRS.
 -	 *
 -	 * Skylake era CPUs have a separate issue with *underflow* of the
 -	 * RSB, when they will predict 'ret' targets from the generic BTB.
 -	 * The proper mitigation for this is IBRS. If IBRS is not supported
 -	 * or deactivated in favour of retpolines the RSB fill on context
 -	 * switch is required.
 -	 */
 -	if ((!boot_cpu_has(X86_FEATURE_PTI) &&
 -	     !boot_cpu_has(X86_FEATURE_SMEP)) || is_skylake_era()) {
 -		setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 -		pr_info("Spectre v2 mitigation: Filling RSB on context switch\n");
 -	}
 -
 -	/* Initialize Indirect Branch Prediction Barrier if supported */
 -	if (boot_cpu_has(X86_FEATURE_IBPB)) {
 -		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 -		pr_info("Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\n");
 -	}
  
 -	/*
 -	 * Retpoline means the kernel is safe because it has no indirect
 -	 * branches. But firmware isn't, so use IBRS to protect that.
 -	 */
 -	if (boot_cpu_has(X86_FEATURE_IBRS)) {
 -		setup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);
 -		pr_info("Enabling Restricted Speculation for firmware calls\n");
 -	}
 -}
 -
 -#undef pr_fmt
 -#define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
 -
 -static enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;
 -
 -/* The kernel command line selection */
 -enum ssb_mitigation_cmd {
 -	SPEC_STORE_BYPASS_CMD_NONE,
 -	SPEC_STORE_BYPASS_CMD_AUTO,
 -	SPEC_STORE_BYPASS_CMD_ON,
 -	SPEC_STORE_BYPASS_CMD_PRCTL,
 -	SPEC_STORE_BYPASS_CMD_SECCOMP,
 -};
 -
 -static const char *ssb_strings[] = {
 -	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
 -	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled",
 -	[SPEC_STORE_BYPASS_PRCTL]	= "Mitigation: Speculative Store Bypass disabled via prctl",
 -	[SPEC_STORE_BYPASS_SECCOMP]	= "Mitigation: Speculative Store Bypass disabled via prctl and seccomp",
 -};
 -
 -static const struct {
 -	const char *option;
 -	enum ssb_mitigation_cmd cmd;
 -} ssb_mitigation_options[] = {
 -	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO },    /* Platform decides */
 -	{ "on",		SPEC_STORE_BYPASS_CMD_ON },      /* Disable Speculative Store Bypass */
 -	{ "off",	SPEC_STORE_BYPASS_CMD_NONE },    /* Don't touch Speculative Store Bypass */
 -	{ "prctl",	SPEC_STORE_BYPASS_CMD_PRCTL },   /* Disable Speculative Store Bypass via prctl */
 -	{ "seccomp",	SPEC_STORE_BYPASS_CMD_SECCOMP }, /* Disable Speculative Store Bypass via prctl and seccomp */
 -};
 -
 -static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
 -{
 -	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
 -	char arg[20];
 -	int ret, i;
 -
 -	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
 -		return SPEC_STORE_BYPASS_CMD_NONE;
 -	} else {
 -		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
 -					  arg, sizeof(arg));
 -		if (ret < 0)
 -			return SPEC_STORE_BYPASS_CMD_AUTO;
 -
 -		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
 -			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
 -				continue;
 -
 -			cmd = ssb_mitigation_options[i].cmd;
 -			break;
 -		}
 -
 -		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
 -			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
 -			return SPEC_STORE_BYPASS_CMD_AUTO;
 -		}
 -	}
 -
 -	return cmd;
 -}
 -
 -static enum ssb_mitigation __init __ssb_select_mitigation(void)
 -{
 -	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
 -	enum ssb_mitigation_cmd cmd;
 -
 -	if (!boot_cpu_has(X86_FEATURE_SSBD))
 -		return mode;
 -
 -	cmd = ssb_parse_cmdline();
 -	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
 -	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
 -	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
 -		return mode;
 +	case SPECTRE_V2_CMD_RETPOLINE:
 +		spec_ctrl_enable_retpoline();
 +		return;
  
 -	switch (cmd) {
 -	case SPEC_STORE_BYPASS_CMD_AUTO:
 -	case SPEC_STORE_BYPASS_CMD_SECCOMP:
 -		/*
 -		 * Choose prctl+seccomp as the default mode if seccomp is
 -		 * enabled.
 -		 */
 -		if (IS_ENABLED(CONFIG_SECCOMP))
 -			mode = SPEC_STORE_BYPASS_SECCOMP;
 -		else
 -			mode = SPEC_STORE_BYPASS_PRCTL;
 -		break;
 -	case SPEC_STORE_BYPASS_CMD_ON:
 -		mode = SPEC_STORE_BYPASS_DISABLE;
 -		break;
 -	case SPEC_STORE_BYPASS_CMD_PRCTL:
 -		mode = SPEC_STORE_BYPASS_PRCTL;
 +	case SPECTRE_V2_CMD_IBRS:
 +		if (spec_ctrl_force_enable_ibrs())
 +			return;
  		break;
 -	case SPEC_STORE_BYPASS_CMD_NONE:
 -		break;
 -	}
 -
 -	/*
 -	 * We have three CPU feature flags that are in play here:
 -	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.
 -	 *  - X86_FEATURE_SSBD - CPU is able to turn off speculative store bypass
 -	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation
 -	 */
 -	if (mode == SPEC_STORE_BYPASS_DISABLE) {
 -		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
 -		/*
 -		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD may
 -		 * use a completely different MSR and bit dependent on family.
 -		 */
 -		if (!static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 -			x86_amd_ssb_disable();
 -		else {
 -			x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
 -			x86_spec_ctrl_mask |= SPEC_CTRL_SSBD;
 -			wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
 -		}
 -	}
 -
 -	return mode;
 -}
 -
 -static void ssb_select_mitigation(void)
 -{
 -	ssb_mode = __ssb_select_mitigation();
 -
 -	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
 -		pr_info("%s\n", ssb_strings[ssb_mode]);
 -}
 -
 -#undef pr_fmt
 -#define pr_fmt(fmt)     "Speculation prctl: " fmt
  
 -static int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)
 -{
 -	bool update;
 -
 -	if (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&
 -	    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)
 -		return -ENXIO;
 -
 -	switch (ctrl) {
 -	case PR_SPEC_ENABLE:
 -		/* If speculation is force disabled, enable is not allowed */
 -		if (task_spec_ssb_force_disable(task))
 -			return -EPERM;
 -		task_clear_spec_ssb_disable(task);
 -		update = test_and_clear_tsk_thread_flag(task, TIF_SSBD);
 -		break;
 -	case PR_SPEC_DISABLE:
 -		task_set_spec_ssb_disable(task);
 -		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
 +	case SPECTRE_V2_CMD_IBRS_ALWAYS:
 +		if (spec_ctrl_enable_ibrs_always() ||
 +		    spec_ctrl_force_enable_ibp_disabled())
 +			return;
  		break;
 -	case PR_SPEC_FORCE_DISABLE:
 -		task_set_spec_ssb_disable(task);
 -		task_set_spec_ssb_force_disable(task);
 -		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
 +
 +	case SPECTRE_V2_CMD_RETPOLINE_IBRS_USER:
 +		if (spec_ctrl_enable_retpoline_ibrs_user())
 +			return;
  		break;
 -	default:
 -		return -ERANGE;
  	}
  
 -	/*
 -	 * If being set on non-current task, delay setting the CPU
 -	 * mitigation until it is next scheduled.
 -	 */
 -	if (task == current && update)
 -		speculative_store_bypass_update_current();
 +	if (spec_ctrl_cond_enable_ibrs(full_retpoline))
 +		return;
  
 -	return 0;
 -}
 +	if (spec_ctrl_cond_enable_ibp_disabled())
 +		return;
  
 -int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
 -			     unsigned long ctrl)
 -{
 -	switch (which) {
 -	case PR_SPEC_STORE_BYPASS:
 -		return ssb_prctl_set(task, ctrl);
 -	default:
 -		return -ENODEV;
 -	}
 +	spec_ctrl_enable_retpoline();
  }
  
 -#ifdef CONFIG_SECCOMP
 -void arch_seccomp_spec_mitigate(struct task_struct *task)
 +void spectre_v2_print_mitigation(void)
  {
 -	if (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)
 -		ssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);
 -}
 -#endif
  
 -static int ssb_prctl_get(struct task_struct *task)
 -{
 -	switch (ssb_mode) {
 -	case SPEC_STORE_BYPASS_DISABLE:
 -		return PR_SPEC_DISABLE;
 -	case SPEC_STORE_BYPASS_SECCOMP:
 -	case SPEC_STORE_BYPASS_PRCTL:
 -		if (task_spec_ssb_force_disable(task))
 -			return PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;
 -		if (task_spec_ssb_disable(task))
 -			return PR_SPEC_PRCTL | PR_SPEC_DISABLE;
 -		return PR_SPEC_PRCTL | PR_SPEC_ENABLE;
 -	default:
 -		if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
 -			return PR_SPEC_ENABLE;
 -		return PR_SPEC_NOT_AFFECTED;
 -	}
 +	pr_info("%s\n", spectre_v2_strings[spec_ctrl_get_mitigation()]);
  }
  
 -int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)
 +static void __init spectre_v2_select_mitigation(void)
  {
 -	switch (which) {
 -	case PR_SPEC_STORE_BYPASS:
 -		return ssb_prctl_get(task);
 -	default:
 -		return -ENODEV;
 -	}
 +	spectre_v2_cmd = spectre_v2_parse_cmdline();
 +	__spectre_v2_select_mitigation();
 +	spectre_v2_print_mitigation();
  }
  
 -void x86_spec_ctrl_setup_ap(void)
 -{
 -	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 -		wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
 -
 -	if (ssb_mode == SPEC_STORE_BYPASS_DISABLE)
 -		x86_amd_ssb_disable();
 -}
 +#undef pr_fmt
  
  #ifdef CONFIG_SYSFS
 -
 -static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			       char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
++<<<<<<< HEAD
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
++=======
+ 
+ 	switch (bug) {
+ 	case X86_BUG_CPU_MELTDOWN:
+ 		if (boot_cpu_has(X86_FEATURE_PTI))
+ 			return sprintf(buf, "Mitigation: PTI\n");
+ 
+ 		break;
+ 
+ 	case X86_BUG_SPECTRE_V1:
+ 		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
+ 
+ 	case X86_BUG_SPECTRE_V2:
+ 		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
+ 			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
+ 			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
+ 			       spectre_v2_module_string());
+ 
+ 	case X86_BUG_SPEC_STORE_BYPASS:
+ 		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
+ 
+ 	case X86_BUG_L1TF:
+ 		if (boot_cpu_has(X86_FEATURE_L1TF_PTEINV))
+ 			return sprintf(buf, "Mitigation: Page Table Inversion\n");
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ 
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  	return sprintf(buf, "Vulnerable\n");
  }
  
 -ssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)
 -{
 -	return cpu_show_common(dev, attr, buf, X86_BUG_CPU_MELTDOWN);
 -}
 -
 -ssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr, char *buf)
 -{
 -	return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);
 -}
 -
 -ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr, char *buf)
 +ssize_t cpu_show_spectre_v1(struct device *dev,
 +			    struct device_attribute *attr, char *buf)
  {
 -	return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V2);
 +	return sprintf(buf, "Mitigation: Load fences\n");
  }
  
 -ssize_t cpu_show_spec_store_bypass(struct device *dev, struct device_attribute *attr, char *buf)
 +ssize_t cpu_show_spectre_v2(struct device *dev,
 +			    struct device_attribute *attr, char *buf)
  {
 -	return cpu_show_common(dev, attr, buf, X86_BUG_SPEC_STORE_BYPASS);
 +	return sprintf(buf, "%s\n", spectre_v2_strings[spec_ctrl_get_mitigation()]);
  }
+ 
+ ssize_t cpu_show_l1tf(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return cpu_show_common(dev, attr, buf, X86_BUG_L1TF);
+ }
  #endif
diff --cc arch/x86/kernel/cpu/common.c
index 49cb90f121df,9baf684e2b55..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -853,6 -950,88 +853,91 @@@ static void identify_cpu_without_cpuid(
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ static const __initconst struct x86_cpu_id cpu_no_speculation[] = {
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CEDARVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CLOVERVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_LINCROFT,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PENWELL,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PINEVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_CENTAUR,	5 },
+ 	{ X86_VENDOR_INTEL,	5 },
+ 	{ X86_VENDOR_NSC,	5 },
+ 	{ X86_VENDOR_ANY,	4 },
+ 	{}
+ };
+ 
+ static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
+ 	{ X86_VENDOR_AMD },
+ 	{}
+ };
+ 
+ /* Only list CPUs which speculate but are non susceptible to SSB */
+ static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_CORE_YONAH		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
+ 	{ X86_VENDOR_AMD,	0x12,					},
+ 	{ X86_VENDOR_AMD,	0x11,					},
+ 	{ X86_VENDOR_AMD,	0x10,					},
+ 	{ X86_VENDOR_AMD,	0xf,					},
+ 	{}
+ };
+ 
+ static const __initconst struct x86_cpu_id cpu_no_l1tf[] = {
+ 	/* in addition to cpu_no_speculation */
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MOOREFIELD	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GOLDMONT	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_DENVERTON	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_GEMINI_LAKE	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
+ 	{}
+ };
+ 
+ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
+ {
+ 	u64 ia32_cap = 0;
+ 
+ 	if (x86_match_cpu(cpu_no_speculation))
+ 		return;
+ 
+ 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+ 	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+ 
+ 	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
+ 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
+ 
+ 	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
+ 	   !(ia32_cap & ARCH_CAP_SSB_NO) &&
+ 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
+ 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
+ 
+ 	if (x86_match_cpu(cpu_no_meltdown))
+ 		return;
+ 
+ 	/* Rogue Data Cache Load? No! */
+ 	if (ia32_cap & ARCH_CAP_RDCL_NO)
+ 		return;
+ 
+ 	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
+ 
+ 	if (x86_match_cpu(cpu_no_l1tf))
+ 		return;
+ 
+ 	setup_force_cpu_bug(X86_BUG_L1TF);
+ }
+ 
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  /*
   * Do minimum CPU detection early.
   * Fields really needed: vendor, cpuid_level, family, model, mask,
diff --cc drivers/base/cpu.c
index 4263273d1a98,eb9443d5bae1..000000000000
--- a/drivers/base/cpu.c
+++ b/drivers/base/cpu.c
@@@ -414,14 -534,30 +414,39 @@@ ssize_t __weak cpu_show_spectre_v2(stru
  	return sprintf(buf, "Not affected\n");
  }
  
++<<<<<<< HEAD
 +static DEVICE_ATTR(meltdown, 0400, cpu_show_meltdown, NULL);
 +static DEVICE_ATTR(spectre_v1, 0400, cpu_show_spectre_v1, NULL);
 +static DEVICE_ATTR(spectre_v2, 0400, cpu_show_spectre_v2, NULL);
++=======
+ ssize_t __weak cpu_show_spec_store_bypass(struct device *dev,
+ 					  struct device_attribute *attr, char *buf)
+ {
+ 	return sprintf(buf, "Not affected\n");
+ }
+ 
+ ssize_t __weak cpu_show_l1tf(struct device *dev,
+ 			     struct device_attribute *attr, char *buf)
+ {
+ 	return sprintf(buf, "Not affected\n");
+ }
+ 
+ static DEVICE_ATTR(meltdown, 0444, cpu_show_meltdown, NULL);
+ static DEVICE_ATTR(spectre_v1, 0444, cpu_show_spectre_v1, NULL);
+ static DEVICE_ATTR(spectre_v2, 0444, cpu_show_spectre_v2, NULL);
+ static DEVICE_ATTR(spec_store_bypass, 0444, cpu_show_spec_store_bypass, NULL);
+ static DEVICE_ATTR(l1tf, 0444, cpu_show_l1tf, NULL);
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  
  static struct attribute *cpu_root_vulnerabilities_attrs[] = {
  	&dev_attr_meltdown.attr,
  	&dev_attr_spectre_v1.attr,
  	&dev_attr_spectre_v2.attr,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_spec_store_bypass.attr,
+ 	&dev_attr_l1tf.attr,
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  	NULL
  };
  
diff --cc include/linux/cpu.h
index 1cc4c784a301,d3da5184aa1c..000000000000
--- a/include/linux/cpu.h
+++ b/include/linux/cpu.h
@@@ -42,7 -53,15 +42,14 @@@ extern ssize_t cpu_show_spectre_v1(stru
  				   struct device_attribute *attr, char *buf);
  extern ssize_t cpu_show_spectre_v2(struct device *dev,
  				   struct device_attribute *attr, char *buf);
++<<<<<<< HEAD
++=======
+ extern ssize_t cpu_show_spec_store_bypass(struct device *dev,
+ 					  struct device_attribute *attr, char *buf);
+ extern ssize_t cpu_show_l1tf(struct device *dev,
+ 			     struct device_attribute *attr, char *buf);
++>>>>>>> 17dbca119312 (x86/speculation/l1tf: Add sysfs reporting for l1tf)
  
 -extern __printf(4, 5)
 -struct device *cpu_device_create(struct device *parent, void *drvdata,
 -				 const struct attribute_group **groups,
 -				 const char *fmt, ...);
  #ifdef CONFIG_HOTPLUG_CPU
  extern void unregister_cpu(struct cpu *cpu);
  extern ssize_t arch_cpu_probe(const char *, size_t);
* Unmerged path arch/x86/include/asm/cpufeatures.h
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 926574da6e32..9b766ee19cd5 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -191,6 +191,11 @@ extern const struct seq_operations cpuinfo_op;
 
 extern void cpu_detect(struct cpuinfo_x86 *c);
 
+static inline unsigned long l1tf_pfn_limit(void)
+{
+	return BIT(boot_cpu_data.x86_phys_bits - 1 - PAGE_SHIFT) - 1;
+}
+
 extern void early_cpu_init(void);
 extern void identify_boot_cpu(void);
 extern void identify_secondary_cpu(struct cpuinfo_x86 *);
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
* Unmerged path drivers/base/cpu.c
* Unmerged path include/linux/cpu.h
