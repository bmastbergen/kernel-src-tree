dax: Inline dax_insert_mapping() into the callsite

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jan Kara <jack@suse.cz>
commit 1b5a1cb21e0cdfb001050c76dc31039cdece1a63
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/1b5a1cb2.failed

dax_insert_mapping() has only one callsite and we will need to further
fine tune what it does for synchronous faults. Just inline it into the
callsite so that we don't have to pass awkward bools around.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 1b5a1cb21e0cdfb001050c76dc31039cdece1a63)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 1e11e516c09f,5b20c6456926..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -896,75 -820,78 +896,110 @@@ out
  }
  EXPORT_SYMBOL_GPL(dax_writeback_mapping_range);
  
 -static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
 +static int dax_insert_mapping(struct address_space *mapping,
 +		struct block_device *bdev, struct dax_device *dax_dev,
 +		sector_t sector, size_t size, void **entryp,
 +		struct vm_area_struct *vma, struct vm_fault *vmf)
  {
 -	return iomap->blkno + (((pos & PAGE_MASK) - iomap->offset) >> 9);
 -}
 -
 -static int dax_iomap_pfn(struct iomap *iomap, loff_t pos, size_t size,
 -			 pfn_t *pfnp)
 -{
 -	const sector_t sector = dax_iomap_sector(iomap, pos);
 +	unsigned long vaddr = (unsigned long)vmf->virtual_address;
 +	void *entry = *entryp;
 +	void *ret, *kaddr;
  	pgoff_t pgoff;
 -	void *kaddr;
  	int id, rc;
++<<<<<<< HEAD
 +	pfn_t pfn;
 +
 +	rc = bdev_dax_pgoff(bdev, sector, size, &pgoff);
 +	if (rc)
 +		return rc;
 +
 +	id = dax_read_lock();
 +	rc = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
 +	if (rc < 0) {
 +		dax_read_unlock(id);
 +		return rc;
 +	}
 +	dax_read_unlock(id);
 +
 +	ret = dax_insert_mapping_entry(mapping, vmf, entry, sector, 0);
 +	if (IS_ERR(ret))
 +		return PTR_ERR(ret);
 +	*entryp = ret;
 +
 +	trace_dax_insert_mapping(mapping->host, vmf, ret);
 +	return vm_insert_mixed(vma, vaddr, pfn);
 +}
 +
 +/**
 + * dax_pfn_mkwrite - handle first write to DAX page
 + * @vma: The virtual memory area where the fault occurred
 + * @vmf: The description of the fault
++=======
+ 	long length;
+ 
+ 	rc = bdev_dax_pgoff(iomap->bdev, sector, size, &pgoff);
+ 	if (rc)
+ 		return rc;
+ 	id = dax_read_lock();
+ 	length = dax_direct_access(iomap->dax_dev, pgoff, PHYS_PFN(size),
+ 				   &kaddr, pfnp);
+ 	if (length < 0) {
+ 		rc = length;
+ 		goto out;
+ 	}
+ 	rc = -EINVAL;
+ 	if (PFN_PHYS(length) < size)
+ 		goto out;
+ 	if (pfn_t_to_pfn(*pfnp) & (PHYS_PFN(size)-1))
+ 		goto out;
+ 	/* For larger pages we need devmap */
+ 	if (length > 1 && !pfn_t_devmap(*pfnp))
+ 		goto out;
+ 	rc = 0;
+ out:
+ 	dax_read_unlock(id);
+ 	return rc;
+ }
+ 
+ /*
+  * The user has performed a load from a hole in the file.  Allocating a new
+  * page in the file would cause excessive storage usage for workloads with
+  * sparse files.  Instead we insert a read-only mapping of the 4k zero page.
+  * If this page is ever written to we will re-fault and change the mapping to
+  * point to real DAX storage instead.
++>>>>>>> 1b5a1cb21e0c (dax: Inline dax_insert_mapping() into the callsite)
   */
 -static int dax_load_hole(struct address_space *mapping, void *entry,
 -			 struct vm_fault *vmf)
 +int dax_pfn_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
  {
 +	struct file *file = vma->vm_file;
 +	struct address_space *mapping = file->f_mapping;
  	struct inode *inode = mapping->host;
 -	unsigned long vaddr = vmf->address;
 -	int ret = VM_FAULT_NOPAGE;
 -	struct page *zero_page;
 -	void *entry2;
 -
 -	zero_page = ZERO_PAGE(0);
 -	if (unlikely(!zero_page)) {
 -		ret = VM_FAULT_OOM;
 -		goto out;
 -	}
 +	void *entry, **slot;
 +	pgoff_t index = vmf->pgoff;
  
 -	entry2 = dax_insert_mapping_entry(mapping, vmf, entry, 0,
 -			RADIX_DAX_ZERO_PAGE);
 -	if (IS_ERR(entry2)) {
 -		ret = VM_FAULT_SIGBUS;
 -		goto out;
 +	spin_lock_irq(&mapping->tree_lock);
 +	entry = get_unlocked_mapping_entry(mapping, index, &slot);
 +	if (!entry || !radix_tree_exceptional_entry(entry)) {
 +		if (entry)
 +			put_unlocked_mapping_entry(mapping, index, entry);
 +		spin_unlock_irq(&mapping->tree_lock);
 +		trace_dax_pfn_mkwrite_no_entry(inode, vmf, VM_FAULT_NOPAGE);
 +		return VM_FAULT_NOPAGE;
  	}
 -
 -	vm_insert_mixed(vmf->vma, vaddr, page_to_pfn_t(zero_page));
 -out:
 -	trace_dax_load_hole(inode, vmf, ret);
 -	return ret;
 +	radix_tree_tag_set(&mapping->page_tree, index, PAGECACHE_TAG_DIRTY);
 +	entry = lock_slot(mapping, slot);
 +	spin_unlock_irq(&mapping->tree_lock);
 +	/*
 +	 * If we race with somebody updating the PTE and finish_mkwrite_fault()
 +	 * fails, we don't care. We need to return VM_FAULT_NOPAGE and retry
 +	 * the fault in either case.
 +	 */
 +	finish_mkwrite_fault(vmf);
 +	put_locked_mapping_entry(mapping, index, entry);
 +	trace_dax_pfn_mkwrite(inode, vmf, VM_FAULT_NOPAGE);
 +	return VM_FAULT_NOPAGE;
  }
 +EXPORT_SYMBOL_GPL(dax_pfn_mkwrite);
  
  static bool dax_range_is_aligned(struct block_device *bdev,
  				 unsigned int offset, unsigned int length)
@@@ -1189,8 -1090,10 +1224,9 @@@ static int dax_iomap_pte_fault(struct v
  	struct iomap iomap = { 0 };
  	unsigned flags = IOMAP_FAULT;
  	int error, major = 0;
 -	bool write = vmf->flags & FAULT_FLAG_WRITE;
  	int vmf_ret = 0;
  	void *entry;
+ 	pfn_t pfn;
  
  	trace_dax_pte_fault(inode, vmf, vmf_ret);
  	/*
@@@ -1270,12 -1173,27 +1306,33 @@@
  	case IOMAP_MAPPED:
  		if (iomap.flags & IOMAP_F_NEW) {
  			count_vm_event(PGMAJFAULT);
 -			count_memcg_event_mm(vma->vm_mm, PGMAJFAULT);
 +			mem_cgroup_count_vm_event(vmf->vma->vm_mm,
 +					PGMAJFAULT);
  			major = VM_FAULT_MAJOR;
  		}
++<<<<<<< HEAD
 +		error = dax_insert_mapping(mapping, iomap.bdev, iomap.dax_dev,
 +				sector, PAGE_SIZE, &entry, vmf->vma, vmf);
++=======
+ 		error = dax_iomap_pfn(&iomap, pos, PAGE_SIZE, &pfn);
+ 		if (error < 0)
+ 			goto error_finish_iomap;
+ 
+ 		entry = dax_insert_mapping_entry(mapping, vmf, entry,
+ 						 dax_iomap_sector(&iomap, pos),
+ 						 0);
+ 		if (IS_ERR(entry)) {
+ 			error = PTR_ERR(entry);
+ 			goto error_finish_iomap;
+ 		}
+ 
+ 		trace_dax_insert_mapping(inode, vmf, entry);
+ 		if (write)
+ 			error = vm_insert_mixed_mkwrite(vma, vaddr, pfn);
+ 		else
+ 			error = vm_insert_mixed(vma, vaddr, pfn);
+ 
++>>>>>>> 1b5a1cb21e0c (dax: Inline dax_insert_mapping() into the callsite)
  		/* -EBUSY is fine, somebody else faulted on the same PTE */
  		if (error == -EBUSY)
  			error = 0;
* Unmerged path fs/dax.c
