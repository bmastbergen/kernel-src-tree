x86/microcode: Allow late microcode loading with SMT disabled

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] microcode: Allow late microcode loading with SMT disabled (Josh Poimboeuf) [1614515]
Rebuild_FUZZ: 96.61%
commit-author Josh Poimboeuf <jpoimboe@redhat.com>
commit 07d981ad4cf1e78361c6db1c28ee5ba105f96cc1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/07d981ad.failed

The kernel unnecessarily prevents late microcode loading when SMT is
disabled.  It should be safe to allow it if all the primary threads are
online.

	Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
	Acked-by: Borislav Petkov <bp@suse.de>
	Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
(cherry picked from commit 07d981ad4cf1e78361c6db1c28ee5ba105f96cc1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/microcode/core.c
diff --cc arch/x86/kernel/cpu/microcode/core.c
index 18f83375cd58,b9bc8a1a584e..000000000000
--- a/arch/x86/kernel/cpu/microcode/core.c
+++ b/arch/x86/kernel/cpu/microcode/core.c
@@@ -379,22 -494,120 +379,124 @@@ static void __exit microcode_dev_exit(v
  /* fake device for request_firmware */
  static struct platform_device	*microcode_pdev;
  
 -/*
 - * Late loading dance. Why the heavy-handed stomp_machine effort?
 - *
 - * - HT siblings must be idle and not execute other code while the other sibling
 - *   is loading microcode in order to avoid any negative interactions caused by
 - *   the loading.
 - *
 - * - In addition, microcode update on the cores must be serialized until this
 - *   requirement can be relaxed in the future. Right now, this is conservative
 - *   and good.
 - */
 -#define SPINUNIT 100 /* 100 nsec */
 -
 -static int check_online_cpus(void)
 +static int reload_for_cpu(int cpu)
  {
++<<<<<<< HEAD
 +	struct ucode_cpu_info *uci = ucode_cpu_info + cpu;
 +	enum ucode_state ustate;
 +	int err = 0;
 +
 +	if (!uci->valid)
 +		return err;
 +
 +	ustate = microcode_ops->request_microcode_fw(cpu, &microcode_pdev->dev, true);
 +	if (ustate == UCODE_OK)
 +		apply_microcode_on_target(cpu);
 +	else
 +		if (ustate == UCODE_ERROR)
 +			err = -EINVAL;
 +	return err;
++=======
+ 	unsigned int cpu;
+ 
+ 	/*
+ 	 * Make sure all CPUs are online.  It's fine for SMT to be disabled if
+ 	 * all the primary threads are still online.
+ 	 */
+ 	for_each_present_cpu(cpu) {
+ 		if (topology_is_primary_thread(cpu) && !cpu_online(cpu)) {
+ 			pr_err("Not all CPUs online, aborting microcode update.\n");
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static atomic_t late_cpus_in;
+ static atomic_t late_cpus_out;
+ 
+ static int __wait_for_cpus(atomic_t *t, long long timeout)
+ {
+ 	int all_cpus = num_online_cpus();
+ 
+ 	atomic_inc(t);
+ 
+ 	while (atomic_read(t) < all_cpus) {
+ 		if (timeout < SPINUNIT) {
+ 			pr_err("Timeout while waiting for CPUs rendezvous, remaining: %d\n",
+ 				all_cpus - atomic_read(t));
+ 			return 1;
+ 		}
+ 
+ 		ndelay(SPINUNIT);
+ 		timeout -= SPINUNIT;
+ 
+ 		touch_nmi_watchdog();
+ 	}
+ 	return 0;
+ }
+ 
+ /*
+  * Returns:
+  * < 0 - on error
+  *   0 - no update done
+  *   1 - microcode was updated
+  */
+ static int __reload_late(void *info)
+ {
+ 	int cpu = smp_processor_id();
+ 	enum ucode_state err;
+ 	int ret = 0;
+ 
+ 	/*
+ 	 * Wait for all CPUs to arrive. A load will not be attempted unless all
+ 	 * CPUs show up.
+ 	 * */
+ 	if (__wait_for_cpus(&late_cpus_in, NSEC_PER_SEC))
+ 		return -1;
+ 
+ 	raw_spin_lock(&update_lock);
+ 	apply_microcode_local(&err);
+ 	raw_spin_unlock(&update_lock);
+ 
+ 	/* siblings return UCODE_OK because their engine got updated already */
+ 	if (err > UCODE_NFOUND) {
+ 		pr_warn("Error reloading microcode on CPU %d\n", cpu);
+ 		ret = -1;
+ 	} else if (err == UCODE_UPDATED || err == UCODE_OK) {
+ 		ret = 1;
+ 	}
+ 
+ 	/*
+ 	 * Increase the wait timeout to a safe value here since we're
+ 	 * serializing the microcode update and that could take a while on a
+ 	 * large number of CPUs. And that is fine as the *actual* timeout will
+ 	 * be determined by the last CPU finished updating and thus cut short.
+ 	 */
+ 	if (__wait_for_cpus(&late_cpus_out, NSEC_PER_SEC * num_online_cpus()))
+ 		panic("Timeout during microcode update!\n");
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Reload microcode late on all CPUs. Wait for a sec until they
+  * all gather together.
+  */
+ static int microcode_reload_late(void)
+ {
+ 	int ret;
+ 
+ 	atomic_set(&late_cpus_in,  0);
+ 	atomic_set(&late_cpus_out, 0);
+ 
+ 	ret = stop_machine_cpuslocked(__reload_late, NULL, cpu_online_mask);
+ 	if (ret > 0)
+ 		microcode_check();
+ 
+ 	return ret;
++>>>>>>> 07d981ad4cf1 (x86/microcode: Allow late microcode loading with SMT disabled)
  }
  
  static ssize_t reload_store(struct device *dev,
* Unmerged path arch/x86/kernel/cpu/microcode/core.c
