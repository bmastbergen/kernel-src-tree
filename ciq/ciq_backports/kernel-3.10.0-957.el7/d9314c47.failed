i40e: add support for XDP_REDIRECT

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Björn Töpel <bjorn.topel@intel.com>
commit d9314c474d4fc1985e836b92fba4c40dd84885a7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/d9314c47.failed

The driver now acts upon the XDP_REDIRECT return action. Two new ndos
are implemented, ndo_xdp_xmit and ndo_xdp_flush.

XDP_REDIRECT action enables XDP program to redirect frames to other
netdevs.

	Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit d9314c474d4fc1985e836b92fba4c40dd84885a7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_main.c
#	drivers/net/ethernet/intel/i40e/i40e_txrx.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_main.c
index 3db54f062cec,16229998fb1e..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_main.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c
@@@ -10886,7 -11814,9 +10886,13 @@@ static const struct net_device_ops i40e
  	.ndo_features_check	= i40e_features_check,
  	.ndo_bridge_getlink	= i40e_ndo_bridge_getlink,
  	.ndo_bridge_setlink	= i40e_ndo_bridge_setlink,
++<<<<<<< HEAD
 +	.ndo_xdp		= i40e_xdp,
++=======
+ 	.ndo_bpf		= i40e_xdp,
+ 	.ndo_xdp_xmit		= i40e_xdp_xmit,
+ 	.ndo_xdp_flush		= i40e_xdp_flush,
++>>>>>>> d9314c474d4f (i40e: add support for XDP_REDIRECT)
  };
  
  /**
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 3b4ef0138639,f174c72480ab..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@@ -1993,7 -2214,8 +1993,12 @@@ static bool i40e_is_non_eop(struct i40e
  static struct sk_buff *i40e_run_xdp(struct i40e_ring *rx_ring,
  				    struct xdp_buff *xdp)
  {
++<<<<<<< HEAD
 +	int result = I40E_XDP_PASS;
++=======
+ 	int err, result = I40E_XDP_PASS;
+ 	struct i40e_ring *xdp_ring;
++>>>>>>> d9314c474d4f (i40e: add support for XDP_REDIRECT)
  	struct bpf_prog *xdp_prog;
  	u32 act;
  
@@@ -2007,9 -2229,16 +2012,20 @@@
  	switch (act) {
  	case XDP_PASS:
  		break;
++<<<<<<< HEAD
++=======
+ 	case XDP_TX:
+ 		xdp_ring = rx_ring->vsi->xdp_rings[rx_ring->queue_index];
+ 		result = i40e_xmit_xdp_ring(xdp, xdp_ring);
+ 		break;
+ 	case XDP_REDIRECT:
+ 		err = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);
+ 		result = !err ? I40E_XDP_TX : I40E_XDP_CONSUMED;
+ 		break;
++>>>>>>> d9314c474d4f (i40e: add support for XDP_REDIRECT)
  	default:
  		bpf_warn_invalid_xdp_action(act);
 +	case XDP_TX:
  	case XDP_ABORTED:
  		trace_xdp_exception(rx_ring->netdev, xdp_prog, act);
  		/* fallthrough -- handle aborts by dropping packet */
@@@ -2023,6 -2252,36 +2039,39 @@@ xdp_out
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * i40e_rx_buffer_flip - adjusted rx_buffer to point to an unused region
+  * @rx_ring: Rx ring
+  * @rx_buffer: Rx buffer to adjust
+  * @size: Size of adjustment
+  **/
+ static void i40e_rx_buffer_flip(struct i40e_ring *rx_ring,
+ 				struct i40e_rx_buffer *rx_buffer,
+ 				unsigned int size)
+ {
+ #if (PAGE_SIZE < 8192)
+ 	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
+ 
+ 	rx_buffer->page_offset ^= truesize;
+ #else
+ 	unsigned int truesize = SKB_DATA_ALIGN(i40e_rx_offset(rx_ring) + size);
+ 
+ 	rx_buffer->page_offset += truesize;
+ #endif
+ }
+ 
+ static inline void i40e_xdp_ring_update_tail(struct i40e_ring *xdp_ring)
+ {
+ 	/* Force memory writes to complete before letting h/w
+ 	 * know there are new descriptors to fetch.
+ 	 */
+ 	wmb();
+ 	writel_relaxed(xdp_ring->next_to_use, xdp_ring->tail);
+ }
+ 
+ /**
++>>>>>>> d9314c474d4f (i40e: add support for XDP_REDIRECT)
   * i40e_clean_rx_irq - Clean completed descriptors from Rx ring - bounce buf
   * @rx_ring: rx descriptor ring to transact packets on
   * @budget: Total limit on number of packets to process
@@@ -2147,6 -2414,14 +2196,17 @@@ static int i40e_clean_rx_irq(struct i40
  		total_rx_packets++;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (xdp_xmit) {
+ 		struct i40e_ring *xdp_ring =
+ 			rx_ring->vsi->xdp_rings[rx_ring->queue_index];
+ 
+ 		i40e_xdp_ring_update_tail(xdp_ring);
+ 		xdp_do_flush_map();
+ 	}
+ 
++>>>>>>> d9314c474d4f (i40e: add support for XDP_REDIRECT)
  	rx_ring->skb = skb;
  
  	u64_stats_update_begin(&rx_ring->syncp);
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_main.c
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.h b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
index 3eaaa3113384..dd7638429a44 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
@@ -491,6 +491,8 @@ void i40e_force_wb(struct i40e_vsi *vsi, struct i40e_q_vector *q_vector);
 u32 i40e_get_tx_pending(struct i40e_ring *ring);
 int __i40e_maybe_stop_tx(struct i40e_ring *tx_ring, int size);
 bool __i40e_chk_linearize(struct sk_buff *skb);
+int i40e_xdp_xmit(struct net_device *dev, struct xdp_buff *xdp);
+void i40e_xdp_flush(struct net_device *dev);
 
 /**
  * i40e_get_head - Retrieve head from head writeback
