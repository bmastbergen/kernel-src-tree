nvme-fabrics: refactor queue ready check

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 3bc32bb1186ccaf3177cbf29caa6cc14dc510b7b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3bc32bb1.failed

Move the is_connected check to the fibre channel transport, as it has no
meaning for other transports.  To facilitate this split out a new
nvmf_fail_nonready_command helper that is called by the transport when
it is asked to handle a command on a queue that is not ready.

Also avoid a function call for the queue live fast path by inlining
the check.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: James Smart <james.smart@broadcom.com>
(cherry picked from commit 3bc32bb1186ccaf3177cbf29caa6cc14dc510b7b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/fabrics.c
#	drivers/nvme/host/fabrics.h
#	drivers/nvme/host/fc.c
#	drivers/nvme/host/rdma.c
#	drivers/nvme/target/loop.c
diff --cc drivers/nvme/host/fabrics.c
index a4c054fbefbd,6b4e253b9347..000000000000
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@@ -536,6 -536,57 +536,60 @@@ static struct nvmf_transport_ops *nvmf_
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * For something we're not in a state to send to the device the default action
+  * is to busy it and retry it after the controller state is recovered.  However,
+  * anything marked for failfast or nvme multipath is immediately failed.
+  *
+  * Note: commands used to initialize the controller will be marked for failfast.
+  * Note: nvme cli/ioctl commands are marked for failfast.
+  */
+ blk_status_t nvmf_fail_nonready_command(struct request *rq)
+ {
+ 	if (!blk_noretry_request(rq) && !(rq->cmd_flags & REQ_NVME_MPATH))
+ 		return BLK_STS_RESOURCE;
+ 	nvme_req(rq)->status = NVME_SC_ABORT_REQ;
+ 	return BLK_STS_IOERR;
+ }
+ EXPORT_SYMBOL_GPL(nvmf_fail_nonready_command);
+ 
+ bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
+ 		bool queue_live)
+ {
+ 	struct nvme_command *cmd = nvme_req(rq)->cmd;
+ 
+ 	switch (ctrl->state) {
+ 	case NVME_CTRL_NEW:
+ 	case NVME_CTRL_CONNECTING:
+ 	case NVME_CTRL_DELETING:
+ 		/*
+ 		 * If queue is live, allow only commands that are internally
+ 		 * generated pass through.  These are commands on the admin
+ 		 * queue to initialize the controller. This will reject any
+ 		 * ioctl admin cmds received while initializing.
+ 		 */
+ 		if (queue_live && !(nvme_req(rq)->flags & NVME_REQ_USERCMD))
+ 			return true;
+ 
+ 		/*
+ 		 * If the queue is not live, allow only a connect command.  This
+ 		 * will reject any ioctl admin cmd as well as initialization
+ 		 * commands if the controller reverted the queue to non-live.
+ 		 */
+ 		if (!queue_live && blk_rq_is_passthrough(rq) &&
+ 		     cmd->common.opcode == nvme_fabrics_command &&
+ 		     cmd->fabrics.fctype == nvme_fabrics_type_connect)
+ 			return true;
+ 		return false;
+ 	default:
+ 		return false;
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(__nvmf_check_ready);
+ 
++>>>>>>> 3bc32bb1186c (nvme-fabrics: refactor queue ready check)
  static const match_table_t opt_tokens = {
  	{ NVMF_OPT_TRANSPORT,		"transport=%s"		},
  	{ NVMF_OPT_TRADDR,		"traddr=%s"		},
diff --cc drivers/nvme/host/fabrics.h
index a8a5714322b0,2ea949a3868c..000000000000
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@@ -157,5 -162,16 +157,19 @@@ void nvmf_unregister_transport(struct n
  void nvmf_free_options(struct nvmf_ctrl_options *opts);
  int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size);
  bool nvmf_should_reconnect(struct nvme_ctrl *ctrl);
++<<<<<<< HEAD
++=======
+ blk_status_t nvmf_fail_nonready_command(struct request *rq);
+ bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
+ 		bool queue_live);
+ 
+ static inline bool nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
+ 		bool queue_live)
+ {
+ 	if (likely(ctrl->state == NVME_CTRL_LIVE))
+ 		return true;
+ 	return __nvmf_check_ready(ctrl, rq, queue_live);
+ }
++>>>>>>> 3bc32bb1186c (nvme-fabrics: refactor queue ready check)
  
  #endif /* _NVME_FABRICS_H */
diff --cc drivers/nvme/host/fc.c
index d165c0527376,b528a2f5826c..000000000000
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@@ -2353,8 -2266,13 +2353,17 @@@ nvme_fc_queue_rq(struct blk_mq_hw_ctx *
  	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
  	struct nvme_command *sqe = &cmdiu->sqe;
  	enum nvmefc_fcp_datadir	io_dir;
+ 	bool queue_ready = test_bit(NVME_FC_Q_LIVE, &queue->flags);
  	u32 data_len;
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	blk_status_t ret;
+ 
+ 	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE ||
+ 	    !nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ 		return nvmf_fail_nonready_command(rq);
++>>>>>>> 3bc32bb1186c (nvme-fabrics: refactor queue ready check)
  
  	ret = nvme_setup_cmd(ns, rq, sqe);
  	if (ret)
diff --cc drivers/nvme/host/rdma.c
index 215f7f62fdfb,c9424da0d23e..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -1490,16 -1629,15 +1490,27 @@@ static int nvme_rdma_queue_rq(struct bl
  	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
  	struct nvme_rdma_qe *sqe = &req->sqe;
  	struct nvme_command *c = sqe->data;
 +	bool flush = false;
  	struct ib_device *dev;
++<<<<<<< HEAD
 +	unsigned int map_len;
 +	int ret;
 +
 +	WARN_ON_ONCE(rq->tag < 0);
 +
 +	ret = nvme_rdma_queue_is_ready(queue, rq);
 +	if (unlikely(ret))
 +		goto err;
++=======
+ 	bool queue_ready = test_bit(NVME_RDMA_Q_LIVE, &queue->flags);
+ 	blk_status_t ret;
+ 	int err;
+ 
+ 	WARN_ON_ONCE(rq->tag < 0);
+ 
+ 	if (!nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+ 		return nvmf_fail_nonready_command(rq);
++>>>>>>> 3bc32bb1186c (nvme-fabrics: refactor queue ready check)
  
  	dev = queue->device->dev;
  	ib_dma_sync_single_for_cpu(dev, sqe->dma,
diff --cc drivers/nvme/target/loop.c
index 0bc9fd83b7e3,d8d91f04bd7e..000000000000
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@@ -160,29 -158,29 +160,37 @@@ static int nvme_loop_queue_rq(struct bl
  	struct nvme_loop_queue *queue = hctx->driver_data;
  	struct request *req = bd->rq;
  	struct nvme_loop_iod *iod = blk_mq_rq_to_pdu(req);
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	bool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);
+ 	blk_status_t ret;
+ 
+ 	if (!nvmf_check_ready(&queue->ctrl->ctrl, req, queue_ready))
+ 		return nvmf_fail_nonready_command(req);
++>>>>>>> 3bc32bb1186c (nvme-fabrics: refactor queue ready check)
  
  	ret = nvme_setup_cmd(ns, req, &iod->cmd);
 -	if (ret)
 +	if (ret != BLK_MQ_RQ_QUEUE_OK)
  		return ret;
  
 -	blk_mq_start_request(req);
  	iod->cmd.common.flags |= NVME_CMD_SGL_METABUF;
 -	iod->req.port = queue->ctrl->port;
 +	iod->req.port = nvmet_loop_port;
  	if (!nvmet_req_init(&iod->req, &queue->nvme_cq,
 -			&queue->nvme_sq, &nvme_loop_ops))
 -		return BLK_STS_OK;
 +			&queue->nvme_sq, &nvme_loop_ops)) {
 +		nvme_cleanup_cmd(req);
 +		blk_mq_start_request(req);
 +		nvme_loop_queue_response(&iod->req);
 +		return BLK_MQ_RQ_QUEUE_OK;
 +	}
  
 -	if (blk_rq_nr_phys_segments(req)) {
 +	if (blk_rq_bytes(req)) {
  		iod->sg_table.sgl = iod->first_sgl;
 -		if (sg_alloc_table_chained(&iod->sg_table,
 -				blk_rq_nr_phys_segments(req),
 -				iod->sg_table.sgl))
 -			return BLK_STS_RESOURCE;
 +		ret = sg_alloc_table_chained(&iod->sg_table,
 +					     req->nr_phys_segments, GFP_ATOMIC, 
 +					     iod->sg_table.sgl);
 +		if (ret)
 +			return BLK_MQ_RQ_QUEUE_BUSY;
  
  		iod->req.sg = iod->sg_table.sgl;
  		iod->req.sg_cnt = blk_rq_map_sg(req->q, req, iod->sg_table.sgl);
* Unmerged path drivers/nvme/host/fabrics.c
* Unmerged path drivers/nvme/host/fabrics.h
* Unmerged path drivers/nvme/host/fc.c
* Unmerged path drivers/nvme/host/rdma.c
* Unmerged path drivers/nvme/target/loop.c
