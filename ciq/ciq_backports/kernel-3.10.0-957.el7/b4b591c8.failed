nvme-rdma: don't suppress send completions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [nvme] rdma: don't suppress send completions (David Milburn) [1547273]
Rebuild_FUZZ: 93.67%
commit-author Sagi Grimberg <sagi@grimberg.me>
commit b4b591c87f2b0f4ebaf3a68d4f13873b241aa584
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/b4b591c8.failed

The entire completions suppress mechanism is currently broken because the
HCA might retry a send operation (due to dropped ack) after the nvme
transaction has completed.

In order to handle this, we signal all send completions and introduce a
separate done handler for async events as they will be handled differently
(as they don't include in-capsule data by definition).

	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit b4b591c87f2b0f4ebaf3a68d4f13873b241aa584)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index 9840af805b39,61511bed8aca..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -1075,24 -1218,7 +1059,28 @@@ static int nvme_rdma_post_send(struct n
  	wr.sg_list    = sge;
  	wr.num_sge    = num_sge;
  	wr.opcode     = IB_WR_SEND;
++<<<<<<< HEAD
 +	wr.send_flags = 0;
 +
 +	/*
 +	 * Unsignalled send completions are another giant desaster in the
 +	 * IB Verbs spec:  If we don't regularly post signalled sends
 +	 * the send queue will fill up and only a QP reset will rescue us.
 +	 * Would have been way to obvious to handle this in hardware or
 +	 * at least the RDMA stack..
 +	 *
 +	 * Always signal the flushes. The magic request used for the flush
 +	 * sequencer is not allocated in our driver's tagset and it's
 +	 * triggered to be freed by blk_cleanup_queue(). So we need to
 +	 * always mark it as signaled to ensure that the "wr_cqe", which is
 +	 * embeded in request's payload, is not freed when __ib_process_cq()
 +	 * calls wr_cqe->done().
 +	 */
 +	if (nvme_rdma_queue_sig_limit(queue) || flush)
 +		wr.send_flags |= IB_SEND_SIGNALED;
++=======
+ 	wr.send_flags = IB_SEND_SIGNALED;
++>>>>>>> b4b591c87f2b (nvme-rdma: don't suppress send completions)
  
  	if (first)
  		first->next = &wr;
@@@ -1142,7 -1268,13 +1130,17 @@@ static struct blk_mq_tags *nvme_rdma_ta
  	return queue->ctrl->tag_set.tags[queue_idx - 1];
  }
  
++<<<<<<< HEAD
 +static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg, int aer_idx)
++=======
+ static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	if (unlikely(wc->status != IB_WC_SUCCESS))
+ 		nvme_rdma_wr_error(cq, wc, "ASYNC");
+ }
+ 
+ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
++>>>>>>> b4b591c87f2b (nvme-rdma: don't suppress send completions)
  {
  	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
  	struct nvme_rdma_queue *queue = &ctrl->queues[0];
@@@ -1465,10 -1582,9 +1465,9 @@@ static int nvme_rdma_queue_rq(struct bl
  	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
  	struct nvme_rdma_qe *sqe = &req->sqe;
  	struct nvme_command *c = sqe->data;
- 	bool flush = false;
  	struct ib_device *dev;
 -	blk_status_t ret;
 -	int err;
 +	unsigned int map_len;
 +	int ret;
  
  	WARN_ON_ONCE(rq->tag < 0);
  
@@@ -1498,11 -1615,9 +1499,17 @@@
  	ib_dma_sync_single_for_device(dev, sqe->dma,
  			sizeof(struct nvme_command), DMA_TO_DEVICE);
  
++<<<<<<< HEAD
 +	if (rq->cmd_type == REQ_TYPE_FS && rq->cmd_flags & REQ_FLUSH)
 +		flush = true;
 +	ret = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
 +			req->mr->need_inval ? &req->reg_wr.wr : NULL, flush);
 +	if (ret) {
++=======
+ 	err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
+ 			req->mr->need_inval ? &req->reg_wr.wr : NULL);
+ 	if (unlikely(err)) {
++>>>>>>> b4b591c87f2b (nvme-rdma: don't suppress send completions)
  		nvme_rdma_unmap_data(queue, rq);
  		goto err;
  	}
* Unmerged path drivers/nvme/host/rdma.c
