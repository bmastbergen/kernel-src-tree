nfp: bpf: implement bpf map offload

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 1bba4c413a328bfd216d59a212bec371e032391b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/1bba4c41.failed

Plug in to the stack's map offload callbacks for BPF map offload.
Get next call needs some special handling on the FW side, since
we can't send a NULL pointer to the FW there is a get first entry
FW command.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 1bba4c413a328bfd216d59a212bec371e032391b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/main.c
#	drivers/net/ethernet/netronome/nfp/bpf/main.h
#	drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.c
index 60a7af297852,8823c8360047..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.c
@@@ -143,6 -201,190 +143,193 @@@ static bool nfp_bpf_tc_busy(struct nfp_
  	return !!bv->tc_prog;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ nfp_bpf_change_mtu(struct nfp_app *app, struct net_device *netdev, int new_mtu)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 	unsigned int max_mtu;
+ 
+ 	if (~nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
+ 		return 0;
+ 
+ 	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
+ 	if (new_mtu > max_mtu) {
+ 		nn_info(nn, "BPF offload active, MTU over %u not supported\n",
+ 			max_mtu);
+ 		return -EBUSY;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_parse_cap_adjust_head(struct nfp_app_bpf *bpf, void __iomem *value,
+ 			      u32 length)
+ {
+ 	struct nfp_bpf_cap_tlv_adjust_head __iomem *cap = value;
+ 	struct nfp_cpp *cpp = bpf->app->pf->cpp;
+ 
+ 	if (length < sizeof(*cap)) {
+ 		nfp_err(cpp, "truncated adjust_head TLV: %d\n", length);
+ 		return -EINVAL;
+ 	}
+ 
+ 	bpf->adjust_head.flags = readl(&cap->flags);
+ 	bpf->adjust_head.off_min = readl(&cap->off_min);
+ 	bpf->adjust_head.off_max = readl(&cap->off_max);
+ 	bpf->adjust_head.guaranteed_sub = readl(&cap->guaranteed_sub);
+ 	bpf->adjust_head.guaranteed_add = readl(&cap->guaranteed_add);
+ 
+ 	if (bpf->adjust_head.off_min > bpf->adjust_head.off_max) {
+ 		nfp_err(cpp, "invalid adjust_head TLV: min > max\n");
+ 		return -EINVAL;
+ 	}
+ 	if (!FIELD_FIT(UR_REG_IMM_MAX, bpf->adjust_head.off_min) ||
+ 	    !FIELD_FIT(UR_REG_IMM_MAX, bpf->adjust_head.off_max)) {
+ 		nfp_warn(cpp, "disabling adjust_head - driver expects min/max to fit in as immediates\n");
+ 		memset(&bpf->adjust_head, 0, sizeof(bpf->adjust_head));
+ 		return 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_parse_cap_func(struct nfp_app_bpf *bpf, void __iomem *value, u32 length)
+ {
+ 	struct nfp_bpf_cap_tlv_func __iomem *cap = value;
+ 
+ 	if (length < sizeof(*cap)) {
+ 		nfp_err(bpf->app->cpp, "truncated function TLV: %d\n", length);
+ 		return -EINVAL;
+ 	}
+ 
+ 	switch (readl(&cap->func_id)) {
+ 	case BPF_FUNC_map_lookup_elem:
+ 		bpf->helpers.map_lookup = readl(&cap->func_addr);
+ 		break;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_parse_cap_maps(struct nfp_app_bpf *bpf, void __iomem *value, u32 length)
+ {
+ 	struct nfp_bpf_cap_tlv_maps __iomem *cap = value;
+ 
+ 	if (length < sizeof(*cap)) {
+ 		nfp_err(bpf->app->cpp, "truncated maps TLV: %d\n", length);
+ 		return -EINVAL;
+ 	}
+ 
+ 	bpf->maps.types = readl(&cap->types);
+ 	bpf->maps.max_maps = readl(&cap->max_maps);
+ 	bpf->maps.max_elems = readl(&cap->max_elems);
+ 	bpf->maps.max_key_sz = readl(&cap->max_key_sz);
+ 	bpf->maps.max_val_sz = readl(&cap->max_val_sz);
+ 	bpf->maps.max_elem_sz = readl(&cap->max_elem_sz);
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_bpf_parse_capabilities(struct nfp_app *app)
+ {
+ 	struct nfp_cpp *cpp = app->pf->cpp;
+ 	struct nfp_cpp_area *area;
+ 	u8 __iomem *mem, *start;
+ 
+ 	mem = nfp_rtsym_map(app->pf->rtbl, "_abi_bpf_capabilities", "bpf.cap",
+ 			    8, &area);
+ 	if (IS_ERR(mem))
+ 		return PTR_ERR(mem) == -ENOENT ? 0 : PTR_ERR(mem);
+ 
+ 	start = mem;
+ 	while (mem - start + 8 < nfp_cpp_area_size(area)) {
+ 		u8 __iomem *value;
+ 		u32 type, length;
+ 
+ 		type = readl(mem);
+ 		length = readl(mem + 4);
+ 		value = mem + 8;
+ 
+ 		mem += 8 + length;
+ 		if (mem - start > nfp_cpp_area_size(area))
+ 			goto err_release_free;
+ 
+ 		switch (type) {
+ 		case NFP_BPF_CAP_TYPE_FUNC:
+ 			if (nfp_bpf_parse_cap_func(app->priv, value, length))
+ 				goto err_release_free;
+ 			break;
+ 		case NFP_BPF_CAP_TYPE_ADJUST_HEAD:
+ 			if (nfp_bpf_parse_cap_adjust_head(app->priv, value,
+ 							  length))
+ 				goto err_release_free;
+ 			break;
+ 		case NFP_BPF_CAP_TYPE_MAPS:
+ 			if (nfp_bpf_parse_cap_maps(app->priv, value, length))
+ 				goto err_release_free;
+ 			break;
+ 		default:
+ 			nfp_dbg(cpp, "unknown BPF capability: %d\n", type);
+ 			break;
+ 		}
+ 	}
+ 	if (mem - start != nfp_cpp_area_size(area)) {
+ 		nfp_err(cpp, "BPF capabilities left after parsing, parsed:%zd total length:%zu\n",
+ 			mem - start, nfp_cpp_area_size(area));
+ 		goto err_release_free;
+ 	}
+ 
+ 	nfp_cpp_area_release_free(area);
+ 
+ 	return 0;
+ 
+ err_release_free:
+ 	nfp_err(cpp, "invalid BPF capabilities at offset:%zd\n", mem - start);
+ 	nfp_cpp_area_release_free(area);
+ 	return -EINVAL;
+ }
+ 
+ static int nfp_bpf_init(struct nfp_app *app)
+ {
+ 	struct nfp_app_bpf *bpf;
+ 	int err;
+ 
+ 	bpf = kzalloc(sizeof(*bpf), GFP_KERNEL);
+ 	if (!bpf)
+ 		return -ENOMEM;
+ 	bpf->app = app;
+ 	app->priv = bpf;
+ 
+ 	skb_queue_head_init(&bpf->cmsg_replies);
+ 	init_waitqueue_head(&bpf->cmsg_wq);
+ 	INIT_LIST_HEAD(&bpf->map_list);
+ 
+ 	err = nfp_bpf_parse_capabilities(app);
+ 	if (err)
+ 		goto err_free_bpf;
+ 
+ 	return 0;
+ 
+ err_free_bpf:
+ 	kfree(bpf);
+ 	return err;
+ }
+ 
+ static void nfp_bpf_clean(struct nfp_app *app)
+ {
+ 	struct nfp_app_bpf *bpf = app->priv;
+ 
+ 	WARN_ON(!skb_queue_empty(&bpf->cmsg_replies));
+ 	WARN_ON(!list_empty(&bpf->map_list));
+ 	WARN_ON(bpf->maps_in_use || bpf->map_elems_in_use);
+ 	kfree(bpf);
+ }
+ 
++>>>>>>> 1bba4c413a32 (nfp: bpf: implement bpf map offload)
  const struct nfp_app_type app_bpf = {
  	.id		= NFP_APP_BPF_NIC,
  	.name		= "ebpf",
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.h
index 5212b54abaf7,b80e75a8ecda..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.h
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.h
@@@ -85,6 -96,89 +85,92 @@@ enum nfp_bpf_action_type 
  #define NFP_BPF_ABI_FLAGS	reg_imm(0)
  #define   NFP_BPF_ABI_FLAG_MARK	1
  
++<<<<<<< HEAD
++=======
+ /**
+  * struct nfp_app_bpf - bpf app priv structure
+  * @app:		backpointer to the app
+  *
+  * @tag_allocator:	bitmap of control message tags in use
+  * @tag_alloc_next:	next tag bit to allocate
+  * @tag_alloc_last:	next tag bit to be freed
+  *
+  * @cmsg_replies:	received cmsg replies waiting to be consumed
+  * @cmsg_wq:		work queue for waiting for cmsg replies
+  *
+  * @map_list:		list of offloaded maps
+  * @maps_in_use:	number of currently offloaded maps
+  * @map_elems_in_use:	number of elements allocated to offloaded maps
+  *
+  * @adjust_head:	adjust head capability
+  * @flags:		extra flags for adjust head
+  * @off_min:		minimal packet offset within buffer required
+  * @off_max:		maximum packet offset within buffer required
+  * @guaranteed_sub:	amount of negative adjustment guaranteed possible
+  * @guaranteed_add:	amount of positive adjustment guaranteed possible
+  *
+  * @maps:		map capability
+  * @types:		supported map types
+  * @max_maps:		max number of maps supported
+  * @max_elems:		max number of entries in each map
+  * @max_key_sz:		max size of map key
+  * @max_val_sz:		max size of map value
+  * @max_elem_sz:	max size of map entry (key + value)
+  *
+  * @helpers:		helper addressess for various calls
+  * @map_lookup:		map lookup helper address
+  */
+ struct nfp_app_bpf {
+ 	struct nfp_app *app;
+ 
+ 	DECLARE_BITMAP(tag_allocator, U16_MAX + 1);
+ 	u16 tag_alloc_next;
+ 	u16 tag_alloc_last;
+ 
+ 	struct sk_buff_head cmsg_replies;
+ 	struct wait_queue_head cmsg_wq;
+ 
+ 	struct list_head map_list;
+ 	unsigned int maps_in_use;
+ 	unsigned int map_elems_in_use;
+ 
+ 	struct nfp_bpf_cap_adjust_head {
+ 		u32 flags;
+ 		int off_min;
+ 		int off_max;
+ 		int guaranteed_sub;
+ 		int guaranteed_add;
+ 	} adjust_head;
+ 
+ 	struct {
+ 		u32 types;
+ 		u32 max_maps;
+ 		u32 max_elems;
+ 		u32 max_key_sz;
+ 		u32 max_val_sz;
+ 		u32 max_elem_sz;
+ 	} maps;
+ 
+ 	struct {
+ 		u32 map_lookup;
+ 	} helpers;
+ };
+ 
+ /**
+  * struct nfp_bpf_map - private per-map data attached to BPF maps for offload
+  * @offmap:	pointer to the offloaded BPF map
+  * @bpf:	back pointer to bpf app private structure
+  * @tid:	table id identifying map on datapath
+  * @l:		link on the nfp_app_bpf->map_list list
+  */
+ struct nfp_bpf_map {
+ 	struct bpf_offloaded_map *offmap;
+ 	struct nfp_app_bpf *bpf;
+ 	u32 tid;
+ 	struct list_head l;
+ };
+ 
++>>>>>>> 1bba4c413a32 (nfp: bpf: implement bpf map offload)
  struct nfp_prog;
  struct nfp_insn_meta;
  typedef int (*instr_cb_t)(struct nfp_prog *, struct nfp_insn_meta *);
diff --cc drivers/net/ethernet/netronome/nfp/bpf/offload.c
index de79faf0874b,e2859b2e9c6a..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/offload.c
@@@ -88,81 -91,193 +91,209 @@@ static void nfp_net_bpf_stats_reset(str
  }
  
  static int
 -nfp_bpf_verifier_prep(struct nfp_app *app, struct nfp_net *nn,
 -		      struct netdev_bpf *bpf)
 +nfp_net_bpf_stats_update(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
  {
 -	struct bpf_prog *prog = bpf->verifier.prog;
 -	struct nfp_prog *nfp_prog;
 -	int ret;
 +	struct tc_action *a;
 +	LIST_HEAD(actions);
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bytes, pkts;
  
 -	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
 -	if (!nfp_prog)
 -		return -ENOMEM;
 -	prog->aux->offload->dev_priv = nfp_prog;
 +	pkts = priv->rx_filter.pkts - priv->rx_filter_prev.pkts;
 +	bytes = priv->rx_filter.bytes - priv->rx_filter_prev.bytes;
 +	bytes -= pkts * ETH_HLEN;
  
 -	INIT_LIST_HEAD(&nfp_prog->insns);
 -	nfp_prog->type = prog->type;
 -	nfp_prog->bpf = app->priv;
 +	priv->rx_filter_prev = priv->rx_filter;
  
 -	ret = nfp_prog_prepare(nfp_prog, prog->insnsi, prog->len);
 -	if (ret)
 -		goto err_free;
 +	preempt_disable();
  
 -	nfp_prog->verifier_meta = nfp_prog_first_meta(nfp_prog);
 -	bpf->verifier.ops = &nfp_bpf_analyzer_ops;
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list)
 +		tcf_action_stats_update(a, bytes, pkts, nn->rx_filter_change);
  
 -	return 0;
 +	preempt_enable();
  
 -err_free:
 -	nfp_prog_free(nfp_prog);
 -
 -	return ret;
 +	return 0;
  }
  
 -static int nfp_bpf_translate(struct nfp_net *nn, struct bpf_prog *prog)
 +static int
 +nfp_net_bpf_get_act(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
  {
 -	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
 -	unsigned int stack_size;
 -	unsigned int max_instr;
 +	const struct tc_action *a;
 +	LIST_HEAD(actions);
 +
 +	if (!cls_bpf->exts)
 +		return NN_ACT_XDP;
 +
 +	/* TC direct action */
 +	if (cls_bpf->exts_integrated) {
 +		if (tc_no_actions(cls_bpf->exts))
 +			return NN_ACT_DIRECT;
  
 -	stack_size = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;
 -	if (prog->aux->stack_depth > stack_size) {
 -		nn_info(nn, "stack too large: program %dB > FW stack %dB\n",
 -			prog->aux->stack_depth, stack_size);
  		return -EOPNOTSUPP;
  	}
 -	nfp_prog->stack_depth = round_up(prog->aux->stack_depth, 4);
  
 -	max_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);
 -	nfp_prog->__prog_alloc_len = max_instr * sizeof(u64);
 +	/* TC legacy mode */
 +	if (!tc_single_action(cls_bpf->exts))
 +		return -EOPNOTSUPP;
  
 -	nfp_prog->prog = kvmalloc(nfp_prog->__prog_alloc_len, GFP_KERNEL);
 -	if (!nfp_prog->prog)
 -		return -ENOMEM;
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list) {
 +		if (is_tcf_gact_shot(a))
 +			return NN_ACT_TC_DROP;
  
++<<<<<<< HEAD
 +		if (is_tcf_mirred_egress_redirect(a) &&
 +		    tcf_mirred_ifindex(a) == nn->dp.netdev->ifindex)
 +			return NN_ACT_TC_REDIR;
++=======
+ 	return nfp_bpf_jit(nfp_prog);
+ }
+ 
+ static int nfp_bpf_destroy(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 
+ 	kvfree(nfp_prog->prog);
+ 	nfp_prog_free(nfp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_map_get_next_key(struct bpf_offloaded_map *offmap,
+ 			 void *key, void *next_key)
+ {
+ 	if (!key)
+ 		return nfp_bpf_ctrl_getfirst_entry(offmap, next_key);
+ 	return nfp_bpf_ctrl_getnext_entry(offmap, key, next_key);
+ }
+ 
+ static int
+ nfp_bpf_map_delete_elem(struct bpf_offloaded_map *offmap, void *key)
+ {
+ 	return nfp_bpf_ctrl_del_entry(offmap, key);
+ }
+ 
+ static const struct bpf_map_dev_ops nfp_bpf_map_ops = {
+ 	.map_get_next_key	= nfp_bpf_map_get_next_key,
+ 	.map_lookup_elem	= nfp_bpf_ctrl_lookup_entry,
+ 	.map_update_elem	= nfp_bpf_ctrl_update_entry,
+ 	.map_delete_elem	= nfp_bpf_map_delete_elem,
+ };
+ 
+ static int
+ nfp_bpf_map_alloc(struct nfp_app_bpf *bpf, struct bpf_offloaded_map *offmap)
+ {
+ 	struct nfp_bpf_map *nfp_map;
+ 	long long int res;
+ 
+ 	if (!bpf->maps.types)
+ 		return -EOPNOTSUPP;
+ 
+ 	if (offmap->map.map_flags ||
+ 	    offmap->map.numa_node != NUMA_NO_NODE) {
+ 		pr_info("map flags are not supported\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!(bpf->maps.types & 1 << offmap->map.map_type)) {
+ 		pr_info("map type not supported\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 	if (bpf->maps.max_maps == bpf->maps_in_use) {
+ 		pr_info("too many maps for a device\n");
+ 		return -ENOMEM;
+ 	}
+ 	if (bpf->maps.max_elems - bpf->map_elems_in_use <
+ 	    offmap->map.max_entries) {
+ 		pr_info("map with too many elements: %u, left: %u\n",
+ 			offmap->map.max_entries,
+ 			bpf->maps.max_elems - bpf->map_elems_in_use);
+ 		return -ENOMEM;
+ 	}
+ 	if (offmap->map.key_size > bpf->maps.max_key_sz ||
+ 	    offmap->map.value_size > bpf->maps.max_val_sz ||
+ 	    round_up(offmap->map.key_size, 8) +
+ 	    round_up(offmap->map.value_size, 8) > bpf->maps.max_elem_sz) {
+ 		pr_info("elements don't fit in device constraints\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	nfp_map = kzalloc(sizeof(*nfp_map), GFP_USER);
+ 	if (!nfp_map)
+ 		return -ENOMEM;
+ 
+ 	offmap->dev_priv = nfp_map;
+ 	nfp_map->offmap = offmap;
+ 	nfp_map->bpf = bpf;
+ 
+ 	res = nfp_bpf_ctrl_alloc_map(bpf, &offmap->map);
+ 	if (res < 0) {
+ 		kfree(nfp_map);
+ 		return res;
+ 	}
+ 
+ 	nfp_map->tid = res;
+ 	offmap->dev_ops = &nfp_bpf_map_ops;
+ 	bpf->maps_in_use++;
+ 	bpf->map_elems_in_use += offmap->map.max_entries;
+ 	list_add_tail(&nfp_map->l, &bpf->map_list);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_map_free(struct nfp_app_bpf *bpf, struct bpf_offloaded_map *offmap)
+ {
+ 	struct nfp_bpf_map *nfp_map = offmap->dev_priv;
+ 
+ 	nfp_bpf_ctrl_free_map(bpf, nfp_map);
+ 	list_del_init(&nfp_map->l);
+ 	bpf->map_elems_in_use -= offmap->map.max_entries;
+ 	bpf->maps_in_use--;
+ 	kfree(nfp_map);
+ 
+ 	return 0;
+ }
+ 
+ int nfp_ndo_bpf(struct nfp_app *app, struct nfp_net *nn, struct netdev_bpf *bpf)
+ {
+ 	switch (bpf->command) {
+ 	case BPF_OFFLOAD_VERIFIER_PREP:
+ 		return nfp_bpf_verifier_prep(app, nn, bpf);
+ 	case BPF_OFFLOAD_TRANSLATE:
+ 		return nfp_bpf_translate(nn, bpf->offload.prog);
+ 	case BPF_OFFLOAD_DESTROY:
+ 		return nfp_bpf_destroy(nn, bpf->offload.prog);
+ 	case BPF_OFFLOAD_MAP_ALLOC:
+ 		return nfp_bpf_map_alloc(app->priv, bpf->offmap);
+ 	case BPF_OFFLOAD_MAP_FREE:
+ 		return nfp_bpf_map_free(app->priv, bpf->offmap);
+ 	default:
+ 		return -EINVAL;
++>>>>>>> 1bba4c413a32 (nfp: bpf: implement bpf map offload)
  	}
 +
 +	return -EOPNOTSUPP;
  }
  
 -static int nfp_net_bpf_load(struct nfp_net *nn, struct bpf_prog *prog)
 +static int
 +nfp_net_bpf_offload_prepare(struct nfp_net *nn,
 +			    struct tc_cls_bpf_offload *cls_bpf,
 +			    struct nfp_bpf_result *res,
 +			    void **code, dma_addr_t *dma_addr, u16 max_instr)
  {
 -	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
 +	unsigned int code_sz = max_instr * sizeof(u64);
 +	enum nfp_bpf_action_type act;
 +	unsigned int stack_size;
 +	u16 start_off, done_off;
  	unsigned int max_mtu;
 -	dma_addr_t dma_addr;
 -	void *img;
 -	int err;
 +	int ret;
 +
 +	ret = nfp_net_bpf_get_act(nn, cls_bpf);
 +	if (ret < 0)
 +		return ret;
 +	act = ret;
  
  	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
  	if (max_mtu < nn->dp.netdev->mtu) {
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.c
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.h
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/offload.c
