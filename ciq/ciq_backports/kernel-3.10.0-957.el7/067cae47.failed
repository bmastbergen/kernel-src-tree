bpf: Use char in prog and map name

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Martin KaFai Lau <kafai@fb.com>
commit 067cae47771c864604969fd902efe10916e0d79c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/067cae47.failed

Instead of u8, use char for prog and map name.  It can avoid the
userspace tool getting compiler's signess warning.  The
bpf_prog_aux, bpf_map, bpf_attr, bpf_prog_info and
bpf_map_info are changed.

	Signed-off-by: Martin KaFai Lau <kafai@fb.com>
	Cc: Jakub Kicinski <jakub.kicinski@netronome.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Alexei Starovoitov <ast@fb.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 067cae47771c864604969fd902efe10916e0d79c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	include/uapi/linux/bpf.h
#	tools/include/uapi/linux/bpf.h
diff --cc include/linux/bpf.h
index d4c1f9049ad3,bc7da2ddfcaf..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -32,50 -47,51 +32,56 @@@ struct bpf_map 
  	u32 key_size;
  	u32 value_size;
  	u32 max_entries;
 -	u32 map_flags;
 -	u32 pages;
 -	u32 id;
 -	int numa_node;
 -	struct user_struct *user;
 -	const struct bpf_map_ops *ops;
 +	struct bpf_map_ops *ops;
  	struct work_struct work;
++<<<<<<< HEAD
++=======
+ 	atomic_t usercnt;
+ 	struct bpf_map *inner_map_meta;
+ 	char name[BPF_OBJ_NAME_LEN];
++>>>>>>> 067cae47771c (bpf: Use char in prog and map name)
  };
  
 -/* function argument constraints */
 -enum bpf_arg_type {
 -	ARG_DONTCARE = 0,	/* unused argument in helper function */
 +struct bpf_map_type_list {
 +	struct list_head list_node;
 +	struct bpf_map_ops *ops;
 +	enum bpf_map_type type;
 +};
  
 -	/* the following constraints used to prototype
 -	 * bpf_map_lookup/update/delete_elem() functions
 -	 */
 -	ARG_CONST_MAP_PTR,	/* const argument used as pointer to bpf_map */
 -	ARG_PTR_TO_MAP_KEY,	/* pointer to stack used as map key */
 -	ARG_PTR_TO_MAP_VALUE,	/* pointer to stack used as map value */
 +void bpf_register_map_type(struct bpf_map_type_list *tl);
  
 -	/* the following constraints used to prototype bpf_memcmp() and other
 -	 * functions that access data on eBPF program stack
 -	 */
 -	ARG_PTR_TO_MEM,		/* pointer to valid memory (stack, packet, map value) */
 -	ARG_PTR_TO_UNINIT_MEM,	/* pointer to memory does not need to be initialized,
 -				 * helper function must fill all bytes or clear
 -				 * them in error case.
 -				 */
 +static inline struct bpf_prog *bpf_prog_get(u32 ufd)
 +{
 +	return NULL;
 +}
  
 -	ARG_CONST_SIZE,		/* number of bytes accessed from memory */
 -	ARG_CONST_SIZE_OR_ZERO,	/* number of bytes accessed from memory or 0 */
 +static inline struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 +{
 +	return NULL;
 +}
  
 -	ARG_PTR_TO_CTX,		/* pointer to context */
 -	ARG_ANYTHING,		/* any (initialized) argument is ok */
 -};
 +static inline struct bpf_prog *bpf_prog_add(struct bpf_prog *prog, int i)
 +{
 +	return NULL;
 +}
  
 -/* type of values returned from helper functions */
 -enum bpf_return_type {
 -	RET_INTEGER,			/* function returns integer */
 -	RET_VOID,			/* function doesn't return anything */
 -	RET_PTR_TO_MAP_VALUE_OR_NULL,	/* returns a pointer to map elem value or NULL */
 -};
 +static inline struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 +{
 +	return prog;
 +}
 +
 +static inline void bpf_prog_put(struct bpf_prog *prog)
 +{
 +	return;
 +}
 +
 +
 +struct bpf_map *bpf_map_get_with_uref(u32 ufd);
 +struct bpf_map *__bpf_map_get(struct fd f);
 +struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref);
 +void bpf_map_put_with_uref(struct bpf_map *map);
 +void bpf_map_put(struct bpf_map *map);
 +struct bpf_map *bpf_map_get(struct fd f);
  
  /* eBPF function prototype used by verifier to allow BPF_CALLs from eBPF programs
   * to in-kernel helper functions and for adjusting imm32 field in BPF_CALL
@@@ -89,28 -112,353 +95,38 @@@ struct bpf_func_proto 
  struct bpf_verifier_ops {
  	/* return eBPF function prototype for verification */
  	const struct bpf_func_proto *(*get_func_proto)(enum bpf_func_id func_id);
 +};
  
 -	/* return true if 'size' wide access at offset 'off' within bpf_context
 -	 * with 'type' (read or write) is allowed
 -	 */
 -	bool (*is_valid_access)(int off, int size, enum bpf_access_type type,
 -				struct bpf_insn_access_aux *info);
 -	int (*gen_prologue)(struct bpf_insn *insn, bool direct_write,
 -			    const struct bpf_prog *prog);
 -	u32 (*convert_ctx_access)(enum bpf_access_type type,
 -				  const struct bpf_insn *src,
 -				  struct bpf_insn *dst,
 -				  struct bpf_prog *prog, u32 *target_size);
 -	int (*test_run)(struct bpf_prog *prog, const union bpf_attr *kattr,
 -			union bpf_attr __user *uattr);
 +struct bpf_prog_type_list {
 +	struct list_head list_node;
 +	struct bpf_verifier_ops *ops;
 +	enum bpf_prog_type type;
  };
  
 +void bpf_register_prog_type(struct bpf_prog_type_list *tl);
 +
 +struct bpf_prog;
 +
  struct bpf_prog_aux {
  	atomic_t refcnt;
 -	u32 used_map_cnt;
 -	u32 max_ctx_offset;
 -	u32 stack_depth;
 +	bool is_gpl_compatible;
 +	enum bpf_prog_type prog_type;
 +	struct bpf_verifier_ops *ops;
  	u32 id;
 -	struct latch_tree_node ksym_tnode;
 -	struct list_head ksym_lnode;
 -	const struct bpf_verifier_ops *ops;
  	struct bpf_map **used_maps;
 +	u32 used_map_cnt;
  	struct bpf_prog *prog;
++<<<<<<< HEAD
 +	struct work_struct work;
++=======
+ 	struct user_struct *user;
+ 	u64 load_time; /* ns since boottime */
+ 	char name[BPF_OBJ_NAME_LEN];
+ 	union {
+ 		struct work_struct work;
+ 		struct rcu_head	rcu;
+ 	};
++>>>>>>> 067cae47771c (bpf: Use char in prog and map name)
  };
  
 -struct bpf_array {
 -	struct bpf_map map;
 -	u32 elem_size;
 -	/* 'ownership' of prog_array is claimed by the first program that
 -	 * is going to use this map or by the first program which FD is stored
 -	 * in the map to make sure that all callers and callees have the same
 -	 * prog_type and JITed flag
 -	 */
 -	enum bpf_prog_type owner_prog_type;
 -	bool owner_jited;
 -	union {
 -		char value[0] __aligned(8);
 -		void *ptrs[0] __aligned(8);
 -		void __percpu *pptrs[0] __aligned(8);
 -	};
 -};
 -
 -#define MAX_TAIL_CALL_CNT 32
 -
 -struct bpf_event_entry {
 -	struct perf_event *event;
 -	struct file *perf_file;
 -	struct file *map_file;
 -	struct rcu_head rcu;
 -};
 -
 -u64 bpf_tail_call(u64 ctx, u64 r2, u64 index, u64 r4, u64 r5);
 -u64 bpf_get_stackid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 -
 -bool bpf_prog_array_compatible(struct bpf_array *array, const struct bpf_prog *fp);
 -int bpf_prog_calc_tag(struct bpf_prog *fp);
 -
 -const struct bpf_func_proto *bpf_get_trace_printk_proto(void);
 -
 -typedef unsigned long (*bpf_ctx_copy_t)(void *dst, const void *src,
 -					unsigned long off, unsigned long len);
 -
 -u64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,
 -		     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy);
 -
 -int bpf_prog_test_run_xdp(struct bpf_prog *prog, const union bpf_attr *kattr,
 -			  union bpf_attr __user *uattr);
 -int bpf_prog_test_run_skb(struct bpf_prog *prog, const union bpf_attr *kattr,
 -			  union bpf_attr __user *uattr);
 -
 -/* an array of programs to be executed under rcu_lock.
 - *
 - * Typical usage:
 - * ret = BPF_PROG_RUN_ARRAY(&bpf_prog_array, ctx, BPF_PROG_RUN);
 - *
 - * the structure returned by bpf_prog_array_alloc() should be populated
 - * with program pointers and the last pointer must be NULL.
 - * The user has to keep refcnt on the program and make sure the program
 - * is removed from the array before bpf_prog_put().
 - * The 'struct bpf_prog_array *' should only be replaced with xchg()
 - * since other cpus are walking the array of pointers in parallel.
 - */
 -struct bpf_prog_array {
 -	struct rcu_head rcu;
 -	struct bpf_prog *progs[0];
 -};
 -
 -struct bpf_prog_array __rcu *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags);
 -void bpf_prog_array_free(struct bpf_prog_array __rcu *progs);
 -int bpf_prog_array_length(struct bpf_prog_array __rcu *progs);
 -int bpf_prog_array_copy_to_user(struct bpf_prog_array __rcu *progs,
 -				__u32 __user *prog_ids, u32 cnt);
 -
 -#define BPF_PROG_RUN_ARRAY(array, ctx, func)		\
 -	({						\
 -		struct bpf_prog **_prog;		\
 -		u32 _ret = 1;				\
 -		rcu_read_lock();			\
 -		_prog = rcu_dereference(array)->progs;	\
 -		for (; *_prog; _prog++)			\
 -			_ret &= func(*_prog, ctx);	\
 -		rcu_read_unlock();			\
 -		_ret;					\
 -	 })
 -
 -#ifdef CONFIG_BPF_SYSCALL
 -DECLARE_PER_CPU(int, bpf_prog_active);
 -
 -#define BPF_PROG_TYPE(_id, _ops) \
 -	extern const struct bpf_verifier_ops _ops;
 -#define BPF_MAP_TYPE(_id, _ops) \
 -	extern const struct bpf_map_ops _ops;
 -#include <linux/bpf_types.h>
 -#undef BPF_PROG_TYPE
 -#undef BPF_MAP_TYPE
 -
 -struct bpf_prog *bpf_prog_get(u32 ufd);
 -struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type);
 -struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog, int i);
 -void bpf_prog_sub(struct bpf_prog *prog, int i);
 -struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog);
 -struct bpf_prog * __must_check bpf_prog_inc_not_zero(struct bpf_prog *prog);
 -void bpf_prog_put(struct bpf_prog *prog);
 -int __bpf_prog_charge(struct user_struct *user, u32 pages);
 -void __bpf_prog_uncharge(struct user_struct *user, u32 pages);
 -
 -struct bpf_map *bpf_map_get_with_uref(u32 ufd);
 -struct bpf_map *__bpf_map_get(struct fd f);
 -struct bpf_map * __must_check bpf_map_inc(struct bpf_map *map, bool uref);
 -void bpf_map_put_with_uref(struct bpf_map *map);
 -void bpf_map_put(struct bpf_map *map);
 -int bpf_map_precharge_memlock(u32 pages);
 -void *bpf_map_area_alloc(size_t size, int numa_node);
 -void bpf_map_area_free(void *base);
 -
 -extern int sysctl_unprivileged_bpf_disabled;
 -
 -int bpf_map_new_fd(struct bpf_map *map);
 -int bpf_prog_new_fd(struct bpf_prog *prog);
 -
 -int bpf_obj_pin_user(u32 ufd, const char __user *pathname);
 -int bpf_obj_get_user(const char __user *pathname);
 -
 -int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
 -int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
 -int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,
 -			   u64 flags);
 -int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,
 -			    u64 flags);
 -
 -int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value);
 -
 -int bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,
 -				 void *key, void *value, u64 map_flags);
 -int bpf_fd_array_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
 -void bpf_fd_array_map_clear(struct bpf_map *map);
 -int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,
 -				void *key, void *value, u64 map_flags);
 -int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
 -
 -/* memcpy that is used with 8-byte aligned pointers, power-of-8 size and
 - * forced to use 'long' read/writes to try to atomically copy long counters.
 - * Best-effort only.  No barriers here, since it _will_ race with concurrent
 - * updates from BPF programs. Called from bpf syscall and mostly used with
 - * size 8 or 16 bytes, so ask compiler to inline it.
 - */
 -static inline void bpf_long_memcpy(void *dst, const void *src, u32 size)
 -{
 -	const long *lsrc = src;
 -	long *ldst = dst;
 -
 -	size /= sizeof(long);
 -	while (size--)
 -		*ldst++ = *lsrc++;
 -}
 -
 -/* verify correctness of eBPF program */
 -int bpf_check(struct bpf_prog **fp, union bpf_attr *attr);
 -
 -/* Map specifics */
 -struct net_device  *__dev_map_lookup_elem(struct bpf_map *map, u32 key);
 -void __dev_map_insert_ctx(struct bpf_map *map, u32 index);
 -void __dev_map_flush(struct bpf_map *map);
 -
 -/* Return map's numa specified by userspace */
 -static inline int bpf_map_attr_numa_node(const union bpf_attr *attr)
 -{
 -	return (attr->map_flags & BPF_F_NUMA_NODE) ?
 -		attr->numa_node : NUMA_NO_NODE;
 -}
 -
 -#else
 -static inline struct bpf_prog *bpf_prog_get(u32 ufd)
 -{
 -	return ERR_PTR(-EOPNOTSUPP);
 -}
 -
 -static inline struct bpf_prog *bpf_prog_get_type(u32 ufd,
 -						 enum bpf_prog_type type)
 -{
 -	return ERR_PTR(-EOPNOTSUPP);
 -}
 -static inline struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog,
 -							  int i)
 -{
 -	return ERR_PTR(-EOPNOTSUPP);
 -}
 -
 -static inline void bpf_prog_sub(struct bpf_prog *prog, int i)
 -{
 -}
 -
 -static inline void bpf_prog_put(struct bpf_prog *prog)
 -{
 -}
 -
 -static inline struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog)
 -{
 -	return ERR_PTR(-EOPNOTSUPP);
 -}
 -
 -static inline struct bpf_prog *__must_check
 -bpf_prog_inc_not_zero(struct bpf_prog *prog)
 -{
 -	return ERR_PTR(-EOPNOTSUPP);
 -}
 -
 -static inline int __bpf_prog_charge(struct user_struct *user, u32 pages)
 -{
 -	return 0;
 -}
 -
 -static inline void __bpf_prog_uncharge(struct user_struct *user, u32 pages)
 -{
 -}
 -
 -static inline struct net_device  *__dev_map_lookup_elem(struct bpf_map *map,
 -						       u32 key)
 -{
 -	return NULL;
 -}
 -
 -static inline void __dev_map_insert_ctx(struct bpf_map *map, u32 index)
 -{
 -}
 -
 -static inline void __dev_map_flush(struct bpf_map *map)
 -{
 -}
 -#endif /* CONFIG_BPF_SYSCALL */
 -
 -#if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_BPF_SYSCALL)
 -struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key);
 -int sock_map_prog(struct bpf_map *map, struct bpf_prog *prog, u32 type);
 -#else
 -static inline struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key)
 -{
 -	return NULL;
 -}
 -
 -static inline int sock_map_prog(struct bpf_map *map,
 -				struct bpf_prog *prog,
 -				u32 type)
 -{
 -	return -EOPNOTSUPP;
 -}
 -#endif
 -
 -/* verifier prototypes for helper functions called from eBPF programs */
 -extern const struct bpf_func_proto bpf_map_lookup_elem_proto;
 -extern const struct bpf_func_proto bpf_map_update_elem_proto;
 -extern const struct bpf_func_proto bpf_map_delete_elem_proto;
 -
 -extern const struct bpf_func_proto bpf_get_prandom_u32_proto;
 -extern const struct bpf_func_proto bpf_get_smp_processor_id_proto;
 -extern const struct bpf_func_proto bpf_get_numa_node_id_proto;
 -extern const struct bpf_func_proto bpf_tail_call_proto;
 -extern const struct bpf_func_proto bpf_ktime_get_ns_proto;
 -extern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;
 -extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
 -extern const struct bpf_func_proto bpf_get_current_comm_proto;
 -extern const struct bpf_func_proto bpf_skb_vlan_push_proto;
 -extern const struct bpf_func_proto bpf_skb_vlan_pop_proto;
 -extern const struct bpf_func_proto bpf_get_stackid_proto;
 -extern const struct bpf_func_proto bpf_sock_map_update_proto;
 -
 -/* Shared helpers among cBPF and eBPF. */
 -void bpf_user_rnd_init_once(void);
 -u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 -
  #endif /* _LINUX_BPF_H */
diff --cc include/uapi/linux/bpf.h
index e369860b690e,6db9e1d679cd..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -123,6 -223,14 +123,17 @@@ union bpf_attr 
  		__u32	key_size;	/* size of key in bytes */
  		__u32	value_size;	/* size of value in bytes */
  		__u32	max_entries;	/* max number of entries in a map */
++<<<<<<< HEAD
++=======
+ 		__u32	map_flags;	/* BPF_MAP_CREATE related
+ 					 * flags defined above.
+ 					 */
+ 		__u32	inner_map_fd;	/* fd pointing to the inner map */
+ 		__u32	numa_node;	/* numa node (effective only if
+ 					 * BPF_F_NUMA_NODE is set).
+ 					 */
+ 		char	map_name[BPF_OBJ_NAME_LEN];
++>>>>>>> 067cae47771c (bpf: Use char in prog and map name)
  	};
  
  	struct { /* anonymous struct used by BPF_MAP_*_ELEM commands */
@@@ -132,13 -240,616 +143,76 @@@
  			__aligned_u64 value;
  			__aligned_u64 next_key;
  		};
 -		__u64		flags;
  	};
++<<<<<<< HEAD
++=======
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_LOAD command */
+ 		__u32		prog_type;	/* one of enum bpf_prog_type */
+ 		__u32		insn_cnt;
+ 		__aligned_u64	insns;
+ 		__aligned_u64	license;
+ 		__u32		log_level;	/* verbosity level of verifier */
+ 		__u32		log_size;	/* size of user buffer */
+ 		__aligned_u64	log_buf;	/* user supplied buffer */
+ 		__u32		kern_version;	/* checked when prog_type=kprobe */
+ 		__u32		prog_flags;
+ 		char		prog_name[BPF_OBJ_NAME_LEN];
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_OBJ_* commands */
+ 		__aligned_u64	pathname;
+ 		__u32		bpf_fd;
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_ATTACH/DETACH commands */
+ 		__u32		target_fd;	/* container object to attach to */
+ 		__u32		attach_bpf_fd;	/* eBPF program to attach */
+ 		__u32		attach_type;
+ 		__u32		attach_flags;
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_TEST_RUN command */
+ 		__u32		prog_fd;
+ 		__u32		retval;
+ 		__u32		data_size_in;
+ 		__u32		data_size_out;
+ 		__aligned_u64	data_in;
+ 		__aligned_u64	data_out;
+ 		__u32		repeat;
+ 		__u32		duration;
+ 	} test;
+ 
+ 	struct { /* anonymous struct used by BPF_*_GET_*_ID */
+ 		union {
+ 			__u32		start_id;
+ 			__u32		prog_id;
+ 			__u32		map_id;
+ 		};
+ 		__u32		next_id;
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_OBJ_GET_INFO_BY_FD */
+ 		__u32		bpf_fd;
+ 		__u32		info_len;
+ 		__aligned_u64	info;
+ 	} info;
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_QUERY command */
+ 		__u32		target_fd;	/* container object to query */
+ 		__u32		attach_type;
+ 		__u32		query_flags;
+ 		__u32		attach_flags;
+ 		__aligned_u64	prog_ids;
+ 		__u32		prog_cnt;
+ 	} query;
++>>>>>>> 067cae47771c (bpf: Use char in prog and map name)
  } __attribute__((aligned(8)));
  
 -/* BPF helper function descriptions:
 - *
 - * void *bpf_map_lookup_elem(&map, &key)
 - *     Return: Map value or NULL
 - *
 - * int bpf_map_update_elem(&map, &key, &value, flags)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_map_delete_elem(&map, &key)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_probe_read(void *dst, int size, void *src)
 - *     Return: 0 on success or negative error
 - *
 - * u64 bpf_ktime_get_ns(void)
 - *     Return: current ktime
 - *
 - * int bpf_trace_printk(const char *fmt, int fmt_size, ...)
 - *     Return: length of buffer written or negative error
 - *
 - * u32 bpf_prandom_u32(void)
 - *     Return: random value
 - *
 - * u32 bpf_raw_smp_processor_id(void)
 - *     Return: SMP processor ID
 - *
 - * int bpf_skb_store_bytes(skb, offset, from, len, flags)
 - *     store bytes into packet
 - *     @skb: pointer to skb
 - *     @offset: offset within packet from skb->mac_header
 - *     @from: pointer where to copy bytes from
 - *     @len: number of bytes to store into packet
 - *     @flags: bit 0 - if true, recompute skb->csum
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_l3_csum_replace(skb, offset, from, to, flags)
 - *     recompute IP checksum
 - *     @skb: pointer to skb
 - *     @offset: offset within packet where IP checksum is located
 - *     @from: old value of header field
 - *     @to: new value of header field
 - *     @flags: bits 0-3 - size of header field
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_l4_csum_replace(skb, offset, from, to, flags)
 - *     recompute TCP/UDP checksum
 - *     @skb: pointer to skb
 - *     @offset: offset within packet where TCP/UDP checksum is located
 - *     @from: old value of header field
 - *     @to: new value of header field
 - *     @flags: bits 0-3 - size of header field
 - *             bit 4 - is pseudo header
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_tail_call(ctx, prog_array_map, index)
 - *     jump into another BPF program
 - *     @ctx: context pointer passed to next program
 - *     @prog_array_map: pointer to map which type is BPF_MAP_TYPE_PROG_ARRAY
 - *     @index: 32-bit index inside array that selects specific program to run
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_clone_redirect(skb, ifindex, flags)
 - *     redirect to another netdev
 - *     @skb: pointer to skb
 - *     @ifindex: ifindex of the net device
 - *     @flags: bit 0 - if set, redirect to ingress instead of egress
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * u64 bpf_get_current_pid_tgid(void)
 - *     Return: current->tgid << 32 | current->pid
 - *
 - * u64 bpf_get_current_uid_gid(void)
 - *     Return: current_gid << 32 | current_uid
 - *
 - * int bpf_get_current_comm(char *buf, int size_of_buf)
 - *     stores current->comm into buf
 - *     Return: 0 on success or negative error
 - *
 - * u32 bpf_get_cgroup_classid(skb)
 - *     retrieve a proc's classid
 - *     @skb: pointer to skb
 - *     Return: classid if != 0
 - *
 - * int bpf_skb_vlan_push(skb, vlan_proto, vlan_tci)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_vlan_pop(skb)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_get_tunnel_key(skb, key, size, flags)
 - * int bpf_skb_set_tunnel_key(skb, key, size, flags)
 - *     retrieve or populate tunnel metadata
 - *     @skb: pointer to skb
 - *     @key: pointer to 'struct bpf_tunnel_key'
 - *     @size: size of 'struct bpf_tunnel_key'
 - *     @flags: room for future extensions
 - *     Return: 0 on success or negative error
 - *
 - * u64 bpf_perf_event_read(map, flags)
 - *     read perf event counter value
 - *     @map: pointer to perf_event_array map
 - *     @flags: index of event in the map or bitmask flags
 - *     Return: value of perf event counter read or error code
 - *
 - * int bpf_redirect(ifindex, flags)
 - *     redirect to another netdev
 - *     @ifindex: ifindex of the net device
 - *     @flags:
 - *	  cls_bpf:
 - *          bit 0 - if set, redirect to ingress instead of egress
 - *          other bits - reserved
 - *	  xdp_bpf:
 - *	    all bits - reserved
 - *     Return: cls_bpf: TC_ACT_REDIRECT on success or TC_ACT_SHOT on error
 - *	       xdp_bfp: XDP_REDIRECT on success or XDP_ABORT on error
 - * int bpf_redirect_map(map, key, flags)
 - *     redirect to endpoint in map
 - *     @map: pointer to dev map
 - *     @key: index in map to lookup
 - *     @flags: --
 - *     Return: XDP_REDIRECT on success or XDP_ABORT on error
 - *
 - * u32 bpf_get_route_realm(skb)
 - *     retrieve a dst's tclassid
 - *     @skb: pointer to skb
 - *     Return: realm if != 0
 - *
 - * int bpf_perf_event_output(ctx, map, flags, data, size)
 - *     output perf raw sample
 - *     @ctx: struct pt_regs*
 - *     @map: pointer to perf_event_array map
 - *     @flags: index of event in the map or bitmask flags
 - *     @data: data on stack to be output as raw data
 - *     @size: size of data
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_get_stackid(ctx, map, flags)
 - *     walk user or kernel stack and return id
 - *     @ctx: struct pt_regs*
 - *     @map: pointer to stack_trace map
 - *     @flags: bits 0-7 - numer of stack frames to skip
 - *             bit 8 - collect user stack instead of kernel
 - *             bit 9 - compare stacks by hash only
 - *             bit 10 - if two different stacks hash into the same stackid
 - *                      discard old
 - *             other bits - reserved
 - *     Return: >= 0 stackid on success or negative error
 - *
 - * s64 bpf_csum_diff(from, from_size, to, to_size, seed)
 - *     calculate csum diff
 - *     @from: raw from buffer
 - *     @from_size: length of from buffer
 - *     @to: raw to buffer
 - *     @to_size: length of to buffer
 - *     @seed: optional seed
 - *     Return: csum result or negative error code
 - *
 - * int bpf_skb_get_tunnel_opt(skb, opt, size)
 - *     retrieve tunnel options metadata
 - *     @skb: pointer to skb
 - *     @opt: pointer to raw tunnel option data
 - *     @size: size of @opt
 - *     Return: option size
 - *
 - * int bpf_skb_set_tunnel_opt(skb, opt, size)
 - *     populate tunnel options metadata
 - *     @skb: pointer to skb
 - *     @opt: pointer to raw tunnel option data
 - *     @size: size of @opt
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_change_proto(skb, proto, flags)
 - *     Change protocol of the skb. Currently supported is v4 -> v6,
 - *     v6 -> v4 transitions. The helper will also resize the skb. eBPF
 - *     program is expected to fill the new headers via skb_store_bytes
 - *     and lX_csum_replace.
 - *     @skb: pointer to skb
 - *     @proto: new skb->protocol type
 - *     @flags: reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_change_type(skb, type)
 - *     Change packet type of skb.
 - *     @skb: pointer to skb
 - *     @type: new skb->pkt_type type
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_under_cgroup(skb, map, index)
 - *     Check cgroup2 membership of skb
 - *     @skb: pointer to skb
 - *     @map: pointer to bpf_map in BPF_MAP_TYPE_CGROUP_ARRAY type
 - *     @index: index of the cgroup in the bpf_map
 - *     Return:
 - *       == 0 skb failed the cgroup2 descendant test
 - *       == 1 skb succeeded the cgroup2 descendant test
 - *        < 0 error
 - *
 - * u32 bpf_get_hash_recalc(skb)
 - *     Retrieve and possibly recalculate skb->hash.
 - *     @skb: pointer to skb
 - *     Return: hash
 - *
 - * u64 bpf_get_current_task(void)
 - *     Returns current task_struct
 - *     Return: current
 - *
 - * int bpf_probe_write_user(void *dst, void *src, int len)
 - *     safely attempt to write to a location
 - *     @dst: destination address in userspace
 - *     @src: source address on stack
 - *     @len: number of bytes to copy
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_current_task_under_cgroup(map, index)
 - *     Check cgroup2 membership of current task
 - *     @map: pointer to bpf_map in BPF_MAP_TYPE_CGROUP_ARRAY type
 - *     @index: index of the cgroup in the bpf_map
 - *     Return:
 - *       == 0 current failed the cgroup2 descendant test
 - *       == 1 current succeeded the cgroup2 descendant test
 - *        < 0 error
 - *
 - * int bpf_skb_change_tail(skb, len, flags)
 - *     The helper will resize the skb to the given new size, to be used f.e.
 - *     with control messages.
 - *     @skb: pointer to skb
 - *     @len: new skb length
 - *     @flags: reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_pull_data(skb, len)
 - *     The helper will pull in non-linear data in case the skb is non-linear
 - *     and not all of len are part of the linear section. Only needed for
 - *     read/write with direct packet access.
 - *     @skb: pointer to skb
 - *     @len: len to make read/writeable
 - *     Return: 0 on success or negative error
 - *
 - * s64 bpf_csum_update(skb, csum)
 - *     Adds csum into skb->csum in case of CHECKSUM_COMPLETE.
 - *     @skb: pointer to skb
 - *     @csum: csum to add
 - *     Return: csum on success or negative error
 - *
 - * void bpf_set_hash_invalid(skb)
 - *     Invalidate current skb->hash.
 - *     @skb: pointer to skb
 - *
 - * int bpf_get_numa_node_id()
 - *     Return: Id of current NUMA node.
 - *
 - * int bpf_skb_change_head()
 - *     Grows headroom of skb and adjusts MAC header offset accordingly.
 - *     Will extends/reallocae as required automatically.
 - *     May change skb data pointer and will thus invalidate any check
 - *     performed for direct packet access.
 - *     @skb: pointer to skb
 - *     @len: length of header to be pushed in front
 - *     @flags: Flags (unused for now)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_xdp_adjust_head(xdp_md, delta)
 - *     Adjust the xdp_md.data by delta
 - *     @xdp_md: pointer to xdp_md
 - *     @delta: An positive/negative integer to be added to xdp_md.data
 - *     Return: 0 on success or negative on error
 - *
 - * int bpf_probe_read_str(void *dst, int size, const void *unsafe_ptr)
 - *     Copy a NUL terminated string from unsafe address. In case the string
 - *     length is smaller than size, the target is not padded with further NUL
 - *     bytes. In case the string length is larger than size, just count-1
 - *     bytes are copied and the last byte is set to NUL.
 - *     @dst: destination address
 - *     @size: maximum number of bytes to copy, including the trailing NUL
 - *     @unsafe_ptr: unsafe address
 - *     Return:
 - *       > 0 length of the string including the trailing NUL on success
 - *       < 0 error
 - *
 - * u64 bpf_get_socket_cookie(skb)
 - *     Get the cookie for the socket stored inside sk_buff.
 - *     @skb: pointer to skb
 - *     Return: 8 Bytes non-decreasing number on success or 0 if the socket
 - *     field is missing inside sk_buff
 - *
 - * u32 bpf_get_socket_uid(skb)
 - *     Get the owner uid of the socket stored inside sk_buff.
 - *     @skb: pointer to skb
 - *     Return: uid of the socket owner on success or overflowuid if failed.
 - *
 - * u32 bpf_set_hash(skb, hash)
 - *     Set full skb->hash.
 - *     @skb: pointer to skb
 - *     @hash: hash to set
 - *
 - * int bpf_setsockopt(bpf_socket, level, optname, optval, optlen)
 - *     Calls setsockopt. Not all opts are available, only those with
 - *     integer optvals plus TCP_CONGESTION.
 - *     Supported levels: SOL_SOCKET and IPROTO_TCP
 - *     @bpf_socket: pointer to bpf_socket
 - *     @level: SOL_SOCKET or IPROTO_TCP
 - *     @optname: option name
 - *     @optval: pointer to option value
 - *     @optlen: length of optval in byes
 - *     Return: 0 or negative error
 - *
 - * int bpf_skb_adjust_room(skb, len_diff, mode, flags)
 - *     Grow or shrink room in sk_buff.
 - *     @skb: pointer to skb
 - *     @len_diff: (signed) amount of room to grow/shrink
 - *     @mode: operation mode (enum bpf_adj_room_mode)
 - *     @flags: reserved for future use
 - *     Return: 0 on success or negative error code
 - *
 - * int bpf_sk_redirect_map(map, key, flags)
 - *     Redirect skb to a sock in map using key as a lookup key for the
 - *     sock in map.
 - *     @map: pointer to sockmap
 - *     @key: key to lookup sock in map
 - *     @flags: reserved for future use
 - *     Return: SK_REDIRECT
 - *
 - * int bpf_sock_map_update(skops, map, key, flags)
 - *	@skops: pointer to bpf_sock_ops
 - *	@map: pointer to sockmap to update
 - *	@key: key to insert/update sock in map
 - *	@flags: same flags as map update elem
 - *
 - * int bpf_xdp_adjust_meta(xdp_md, delta)
 - *     Adjust the xdp_md.data_meta by delta
 - *     @xdp_md: pointer to xdp_md
 - *     @delta: An positive/negative integer to be added to xdp_md.data_meta
 - *     Return: 0 on success or negative on error
 - *
 - * int bpf_perf_event_read_value(map, flags, buf, buf_size)
 - *     read perf event counter value and perf event enabled/running time
 - *     @map: pointer to perf_event_array map
 - *     @flags: index of event in the map or bitmask flags
 - *     @buf: buf to fill
 - *     @buf_size: size of the buf
 - *     Return: 0 on success or negative error code
 - *
 - * int bpf_perf_prog_read_value(ctx, buf, buf_size)
 - *     read perf prog attached perf event counter and enabled/running time
 - *     @ctx: pointer to ctx
 - *     @buf: buf to fill
 - *     @buf_size: size of the buf
 - *     Return : 0 on success or negative error code
 - */
 -#define __BPF_FUNC_MAPPER(FN)		\
 -	FN(unspec),			\
 -	FN(map_lookup_elem),		\
 -	FN(map_update_elem),		\
 -	FN(map_delete_elem),		\
 -	FN(probe_read),			\
 -	FN(ktime_get_ns),		\
 -	FN(trace_printk),		\
 -	FN(get_prandom_u32),		\
 -	FN(get_smp_processor_id),	\
 -	FN(skb_store_bytes),		\
 -	FN(l3_csum_replace),		\
 -	FN(l4_csum_replace),		\
 -	FN(tail_call),			\
 -	FN(clone_redirect),		\
 -	FN(get_current_pid_tgid),	\
 -	FN(get_current_uid_gid),	\
 -	FN(get_current_comm),		\
 -	FN(get_cgroup_classid),		\
 -	FN(skb_vlan_push),		\
 -	FN(skb_vlan_pop),		\
 -	FN(skb_get_tunnel_key),		\
 -	FN(skb_set_tunnel_key),		\
 -	FN(perf_event_read),		\
 -	FN(redirect),			\
 -	FN(get_route_realm),		\
 -	FN(perf_event_output),		\
 -	FN(skb_load_bytes),		\
 -	FN(get_stackid),		\
 -	FN(csum_diff),			\
 -	FN(skb_get_tunnel_opt),		\
 -	FN(skb_set_tunnel_opt),		\
 -	FN(skb_change_proto),		\
 -	FN(skb_change_type),		\
 -	FN(skb_under_cgroup),		\
 -	FN(get_hash_recalc),		\
 -	FN(get_current_task),		\
 -	FN(probe_write_user),		\
 -	FN(current_task_under_cgroup),	\
 -	FN(skb_change_tail),		\
 -	FN(skb_pull_data),		\
 -	FN(csum_update),		\
 -	FN(set_hash_invalid),		\
 -	FN(get_numa_node_id),		\
 -	FN(skb_change_head),		\
 -	FN(xdp_adjust_head),		\
 -	FN(probe_read_str),		\
 -	FN(get_socket_cookie),		\
 -	FN(get_socket_uid),		\
 -	FN(set_hash),			\
 -	FN(setsockopt),			\
 -	FN(skb_adjust_room),		\
 -	FN(redirect_map),		\
 -	FN(sk_redirect_map),		\
 -	FN(sock_map_update),		\
 -	FN(xdp_adjust_meta),		\
 -	FN(perf_event_read_value),	\
 -	FN(perf_prog_read_value),
 -
 -/* integer value in 'imm' field of BPF_CALL instruction selects which helper
 - * function eBPF program intends to call
 - */
 -#define __BPF_ENUM_FN(x) BPF_FUNC_ ## x
 -enum bpf_func_id {
 -	__BPF_FUNC_MAPPER(__BPF_ENUM_FN)
 -	__BPF_FUNC_MAX_ID,
 -};
 -#undef __BPF_ENUM_FN
 -
 -/* All flags used by eBPF helper functions, placed here. */
 -
 -/* BPF_FUNC_skb_store_bytes flags. */
 -#define BPF_F_RECOMPUTE_CSUM		(1ULL << 0)
 -#define BPF_F_INVALIDATE_HASH		(1ULL << 1)
 -
 -/* BPF_FUNC_l3_csum_replace and BPF_FUNC_l4_csum_replace flags.
 - * First 4 bits are for passing the header field size.
 - */
 -#define BPF_F_HDR_FIELD_MASK		0xfULL
 -
 -/* BPF_FUNC_l4_csum_replace flags. */
 -#define BPF_F_PSEUDO_HDR		(1ULL << 4)
 -#define BPF_F_MARK_MANGLED_0		(1ULL << 5)
 -#define BPF_F_MARK_ENFORCE		(1ULL << 6)
 -
 -/* BPF_FUNC_clone_redirect and BPF_FUNC_redirect flags. */
 -#define BPF_F_INGRESS			(1ULL << 0)
 -
 -/* BPF_FUNC_skb_set_tunnel_key and BPF_FUNC_skb_get_tunnel_key flags. */
 -#define BPF_F_TUNINFO_IPV6		(1ULL << 0)
 -
 -/* BPF_FUNC_get_stackid flags. */
 -#define BPF_F_SKIP_FIELD_MASK		0xffULL
 -#define BPF_F_USER_STACK		(1ULL << 8)
 -#define BPF_F_FAST_STACK_CMP		(1ULL << 9)
 -#define BPF_F_REUSE_STACKID		(1ULL << 10)
 -
 -/* BPF_FUNC_skb_set_tunnel_key flags. */
 -#define BPF_F_ZERO_CSUM_TX		(1ULL << 1)
 -#define BPF_F_DONT_FRAGMENT		(1ULL << 2)
 -
 -/* BPF_FUNC_perf_event_output, BPF_FUNC_perf_event_read and
 - * BPF_FUNC_perf_event_read_value flags.
 - */
 -#define BPF_F_INDEX_MASK		0xffffffffULL
 -#define BPF_F_CURRENT_CPU		BPF_F_INDEX_MASK
 -/* BPF_FUNC_perf_event_output for sk_buff input context. */
 -#define BPF_F_CTXLEN_MASK		(0xfffffULL << 32)
 -
 -/* Mode for BPF_FUNC_skb_adjust_room helper. */
 -enum bpf_adj_room_mode {
 -	BPF_ADJ_ROOM_NET,
 -};
 -
 -/* user accessible mirror of in-kernel sk_buff.
 - * new fields can only be added to the end of this structure
 - */
 -struct __sk_buff {
 -	__u32 len;
 -	__u32 pkt_type;
 -	__u32 mark;
 -	__u32 queue_mapping;
 -	__u32 protocol;
 -	__u32 vlan_present;
 -	__u32 vlan_tci;
 -	__u32 vlan_proto;
 -	__u32 priority;
 -	__u32 ingress_ifindex;
 -	__u32 ifindex;
 -	__u32 tc_index;
 -	__u32 cb[5];
 -	__u32 hash;
 -	__u32 tc_classid;
 -	__u32 data;
 -	__u32 data_end;
 -	__u32 napi_id;
 -
 -	/* Accessed by BPF_PROG_TYPE_sk_skb types from here to ... */
 -	__u32 family;
 -	__u32 remote_ip4;	/* Stored in network byte order */
 -	__u32 local_ip4;	/* Stored in network byte order */
 -	__u32 remote_ip6[4];	/* Stored in network byte order */
 -	__u32 local_ip6[4];	/* Stored in network byte order */
 -	__u32 remote_port;	/* Stored in network byte order */
 -	__u32 local_port;	/* stored in host byte order */
 -	/* ... here. */
 -
 -	__u32 data_meta;
 -};
 -
 -struct bpf_tunnel_key {
 -	__u32 tunnel_id;
 -	union {
 -		__u32 remote_ipv4;
 -		__u32 remote_ipv6[4];
 -	};
 -	__u8 tunnel_tos;
 -	__u8 tunnel_ttl;
 -	__u16 tunnel_ext;
 -	__u32 tunnel_label;
 -};
 -
 -/* Generic BPF return codes which all BPF program types may support.
 - * The values are binary compatible with their TC_ACT_* counter-part to
 - * provide backwards compatibility with existing SCHED_CLS and SCHED_ACT
 - * programs.
 - *
 - * XDP is handled seprately, see XDP_*.
 - */
 -enum bpf_ret_code {
 -	BPF_OK = 0,
 -	/* 1 reserved */
 -	BPF_DROP = 2,
 -	/* 3-6 reserved */
 -	BPF_REDIRECT = 7,
 -	/* >127 are reserved for prog type specific return codes */
 -};
 -
 -struct bpf_sock {
 -	__u32 bound_dev_if;
 -	__u32 family;
 -	__u32 type;
 -	__u32 protocol;
 -	__u32 mark;
 -	__u32 priority;
 -};
 -
 -#define XDP_PACKET_HEADROOM 256
 -
  /* User return codes for XDP prog type.
   * A valid XDP program must return one of these defined values. All other
 - * return codes are reserved for future use. Unknown return codes will
 - * result in packet drops and a warning via bpf_warn_invalid_xdp_action().
 + * return codes are reserved for future use. Unknown return codes will result
 + * in packet drop.
   */
  enum xdp_action {
  	XDP_ABORTED = 0,
@@@ -153,16 -865,98 +227,51 @@@
  struct xdp_md {
  	__u32 data;
  	__u32 data_end;
 -	__u32 data_meta;
  };
  
 -enum sk_action {
 -	SK_ABORTED = 0,
 -	SK_DROP,
 -	SK_REDIRECT,
 -};
 +#define XDP_PACKET_HEADROOM 256
  
++<<<<<<< HEAD
 +/* integer value in 'imm' field of BPF_CALL instruction selects which helper
 + * function eBPF program intends to call
++=======
+ #define BPF_TAG_SIZE	8
+ 
+ struct bpf_prog_info {
+ 	__u32 type;
+ 	__u32 id;
+ 	__u8  tag[BPF_TAG_SIZE];
+ 	__u32 jited_prog_len;
+ 	__u32 xlated_prog_len;
+ 	__aligned_u64 jited_prog_insns;
+ 	__aligned_u64 xlated_prog_insns;
+ 	__u64 load_time;	/* ns since boottime */
+ 	__u32 created_by_uid;
+ 	__u32 nr_map_ids;
+ 	__aligned_u64 map_ids;
+ 	char name[BPF_OBJ_NAME_LEN];
+ } __attribute__((aligned(8)));
+ 
+ struct bpf_map_info {
+ 	__u32 type;
+ 	__u32 id;
+ 	__u32 key_size;
+ 	__u32 value_size;
+ 	__u32 max_entries;
+ 	__u32 map_flags;
+ 	char  name[BPF_OBJ_NAME_LEN];
+ } __attribute__((aligned(8)));
+ 
+ /* User bpf_sock_ops struct to access socket values and specify request ops
+  * and their replies.
+  * Some of this fields are in network (bigendian) byte order and may need
+  * to be converted before use (bpf_ntohl() defined in samples/bpf/bpf_endian.h).
+  * New fields can only be added at the end of this structure
++>>>>>>> 067cae47771c (bpf: Use char in prog and map name)
   */
 -struct bpf_sock_ops {
 -	__u32 op;
 -	union {
 -		__u32 reply;
 -		__u32 replylong[4];
 -	};
 -	__u32 family;
 -	__u32 remote_ip4;	/* Stored in network byte order */
 -	__u32 local_ip4;	/* Stored in network byte order */
 -	__u32 remote_ip6[4];	/* Stored in network byte order */
 -	__u32 local_ip6[4];	/* Stored in network byte order */
 -	__u32 remote_port;	/* Stored in network byte order */
 -	__u32 local_port;	/* stored in host byte order */
 -};
 -
 -/* List of known BPF sock_ops operators.
 - * New entries can only be added at the end
 - */
 -enum {
 -	BPF_SOCK_OPS_VOID,
 -	BPF_SOCK_OPS_TIMEOUT_INIT,	/* Should return SYN-RTO value to use or
 -					 * -1 if default value should be used
 -					 */
 -	BPF_SOCK_OPS_RWND_INIT,		/* Should return initial advertized
 -					 * window (in packets) or -1 if default
 -					 * value should be used
 -					 */
 -	BPF_SOCK_OPS_TCP_CONNECT_CB,	/* Calls BPF program right before an
 -					 * active connection is initialized
 -					 */
 -	BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB,	/* Calls BPF program when an
 -						 * active connection is
 -						 * established
 -						 */
 -	BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB,	/* Calls BPF program when a
 -						 * passive connection is
 -						 * established
 -						 */
 -	BPF_SOCK_OPS_NEEDS_ECN,		/* If connection's congestion control
 -					 * needs ECN
 -					 */
 -};
 -
 -#define TCP_BPF_IW		1001	/* Set TCP initial congestion window */
 -#define TCP_BPF_SNDCWND_CLAMP	1002	/* Set sndcwnd_clamp */
 -
 -struct bpf_perf_event_value {
 -	__u64 counter;
 -	__u64 enabled;
 -	__u64 running;
 +enum bpf_func_id {
 +	BPF_FUNC_unspec,
 +	__BPF_FUNC_MAX_ID,
  };
  
  #endif /* _UAPI__LINUX_BPF_H__ */
* Unmerged path tools/include/uapi/linux/bpf.h
* Unmerged path include/linux/bpf.h
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path tools/include/uapi/linux/bpf.h
