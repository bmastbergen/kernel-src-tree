bpf: Allow selecting numa node during map creation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Martin KaFai Lau <kafai@fb.com>
commit 96eabe7a40aa17e613cf3db2c742ee8b1fc764d0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/96eabe7a.failed

The current map creation API does not allow to provide the numa-node
preference.  The memory usually comes from where the map-creation-process
is running.  The performance is not ideal if the bpf_prog is known to
always run in a numa node different from the map-creation-process.

One of the use case is sharding on CPU to different LRU maps (i.e.
an array of LRU maps).  Here is the test result of map_perf_test on
the INNER_LRU_HASH_PREALLOC test if we force the lru map used by
CPU0 to be allocated from a remote numa node:

[ The machine has 20 cores. CPU0-9 at node 0. CPU10-19 at node 1 ]

># taskset -c 10 ./map_perf_test 512 8 1260000 8000000
5:inner_lru_hash_map_perf pre-alloc 1628380 events per sec
4:inner_lru_hash_map_perf pre-alloc 1626396 events per sec
3:inner_lru_hash_map_perf pre-alloc 1626144 events per sec
6:inner_lru_hash_map_perf pre-alloc 1621657 events per sec
2:inner_lru_hash_map_perf pre-alloc 1621534 events per sec
1:inner_lru_hash_map_perf pre-alloc 1620292 events per sec
7:inner_lru_hash_map_perf pre-alloc 1613305 events per sec
0:inner_lru_hash_map_perf pre-alloc 1239150 events per sec  #<<<

After specifying numa node:
># taskset -c 10 ./map_perf_test 512 8 1260000 8000000
5:inner_lru_hash_map_perf pre-alloc 1629627 events per sec
3:inner_lru_hash_map_perf pre-alloc 1628057 events per sec
1:inner_lru_hash_map_perf pre-alloc 1623054 events per sec
6:inner_lru_hash_map_perf pre-alloc 1616033 events per sec
2:inner_lru_hash_map_perf pre-alloc 1614630 events per sec
4:inner_lru_hash_map_perf pre-alloc 1612651 events per sec
7:inner_lru_hash_map_perf pre-alloc 1609337 events per sec
0:inner_lru_hash_map_perf pre-alloc 1619340 events per sec #<<<

This patch adds one field, numa_node, to the bpf_attr.  Since numa node 0
is a valid node, a new flag BPF_F_NUMA_NODE is also added.  The numa_node
field is honored if and only if the BPF_F_NUMA_NODE flag is set.

Numa node selection is not supported for percpu map.

This patch does not change all the kmalloc.  F.e.
'htab = kzalloc()' is not changed since the object
is small enough to stay in the cache.

	Signed-off-by: Martin KaFai Lau <kafai@fb.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Alexei Starovoitov <ast@fb.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 96eabe7a40aa17e613cf3db2c742ee8b1fc764d0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	include/uapi/linux/bpf.h
#	kernel/bpf/arraymap.c
#	kernel/bpf/devmap.c
#	kernel/bpf/hashtab.c
#	kernel/bpf/lpm_trie.c
#	kernel/bpf/sockmap.c
#	kernel/bpf/stackmap.c
#	kernel/bpf/syscall.c
diff --cc include/linux/bpf.h
index d4c1f9049ad3,55b88e329804..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -32,50 -48,50 +32,59 @@@ struct bpf_map 
  	u32 key_size;
  	u32 value_size;
  	u32 max_entries;
++<<<<<<< HEAD
 +	struct bpf_map_ops *ops;
++=======
+ 	u32 map_flags;
+ 	u32 pages;
+ 	u32 id;
+ 	int numa_node;
+ 	struct user_struct *user;
+ 	const struct bpf_map_ops *ops;
++>>>>>>> 96eabe7a40aa (bpf: Allow selecting numa node during map creation)
  	struct work_struct work;
 -	atomic_t usercnt;
 -	struct bpf_map *inner_map_meta;
  };
  
 -/* function argument constraints */
 -enum bpf_arg_type {
 -	ARG_DONTCARE = 0,	/* unused argument in helper function */
 +struct bpf_map_type_list {
 +	struct list_head list_node;
 +	struct bpf_map_ops *ops;
 +	enum bpf_map_type type;
 +};
  
 -	/* the following constraints used to prototype
 -	 * bpf_map_lookup/update/delete_elem() functions
 -	 */
 -	ARG_CONST_MAP_PTR,	/* const argument used as pointer to bpf_map */
 -	ARG_PTR_TO_MAP_KEY,	/* pointer to stack used as map key */
 -	ARG_PTR_TO_MAP_VALUE,	/* pointer to stack used as map value */
 +void bpf_register_map_type(struct bpf_map_type_list *tl);
  
 -	/* the following constraints used to prototype bpf_memcmp() and other
 -	 * functions that access data on eBPF program stack
 -	 */
 -	ARG_PTR_TO_MEM,		/* pointer to valid memory (stack, packet, map value) */
 -	ARG_PTR_TO_UNINIT_MEM,	/* pointer to memory does not need to be initialized,
 -				 * helper function must fill all bytes or clear
 -				 * them in error case.
 -				 */
 +static inline struct bpf_prog *bpf_prog_get(u32 ufd)
 +{
 +	return NULL;
 +}
  
 -	ARG_CONST_SIZE,		/* number of bytes accessed from memory */
 -	ARG_CONST_SIZE_OR_ZERO,	/* number of bytes accessed from memory or 0 */
 +static inline struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 +{
 +	return NULL;
 +}
  
 -	ARG_PTR_TO_CTX,		/* pointer to context */
 -	ARG_ANYTHING,		/* any (initialized) argument is ok */
 -};
 +static inline struct bpf_prog *bpf_prog_add(struct bpf_prog *prog, int i)
 +{
 +	return NULL;
 +}
  
 -/* type of values returned from helper functions */
 -enum bpf_return_type {
 -	RET_INTEGER,			/* function returns integer */
 -	RET_VOID,			/* function doesn't return anything */
 -	RET_PTR_TO_MAP_VALUE_OR_NULL,	/* returns a pointer to map elem value or NULL */
 -};
 +static inline struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 +{
 +	return prog;
 +}
 +
 +static inline void bpf_prog_put(struct bpf_prog *prog)
 +{
 +	return;
 +}
 +
 +
 +struct bpf_map *bpf_map_get_with_uref(u32 ufd);
 +struct bpf_map *__bpf_map_get(struct fd f);
 +struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref);
 +void bpf_map_put_with_uref(struct bpf_map *map);
 +void bpf_map_put(struct bpf_map *map);
 +struct bpf_map *bpf_map_get(struct fd f);
  
  /* eBPF function prototype used by verifier to allow BPF_CALLs from eBPF programs
   * to in-kernel helper functions and for adjusting imm32 field in BPF_CALL
@@@ -89,28 -123,296 +98,253 @@@ struct bpf_func_proto 
  struct bpf_verifier_ops {
  	/* return eBPF function prototype for verification */
  	const struct bpf_func_proto *(*get_func_proto)(enum bpf_func_id func_id);
 +};
  
 -	/* return true if 'size' wide access at offset 'off' within bpf_context
 -	 * with 'type' (read or write) is allowed
 -	 */
 -	bool (*is_valid_access)(int off, int size, enum bpf_access_type type,
 -				struct bpf_insn_access_aux *info);
 -	int (*gen_prologue)(struct bpf_insn *insn, bool direct_write,
 -			    const struct bpf_prog *prog);
 -	u32 (*convert_ctx_access)(enum bpf_access_type type,
 -				  const struct bpf_insn *src,
 -				  struct bpf_insn *dst,
 -				  struct bpf_prog *prog, u32 *target_size);
 -	int (*test_run)(struct bpf_prog *prog, const union bpf_attr *kattr,
 -			union bpf_attr __user *uattr);
 +struct bpf_prog_type_list {
 +	struct list_head list_node;
 +	struct bpf_verifier_ops *ops;
 +	enum bpf_prog_type type;
  };
  
 +void bpf_register_prog_type(struct bpf_prog_type_list *tl);
 +
 +struct bpf_prog;
 +
  struct bpf_prog_aux {
  	atomic_t refcnt;
 -	u32 used_map_cnt;
 -	u32 max_ctx_offset;
 -	u32 stack_depth;
 +	bool is_gpl_compatible;
 +	enum bpf_prog_type prog_type;
 +	struct bpf_verifier_ops *ops;
  	u32 id;
 -	struct latch_tree_node ksym_tnode;
 -	struct list_head ksym_lnode;
 -	const struct bpf_verifier_ops *ops;
  	struct bpf_map **used_maps;
 +	u32 used_map_cnt;
  	struct bpf_prog *prog;
 -	struct user_struct *user;
 -	union {
 -		struct work_struct work;
 -		struct rcu_head	rcu;
 -	};
 +	struct work_struct work;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_array {
+ 	struct bpf_map map;
+ 	u32 elem_size;
+ 	/* 'ownership' of prog_array is claimed by the first program that
+ 	 * is going to use this map or by the first program which FD is stored
+ 	 * in the map to make sure that all callers and callees have the same
+ 	 * prog_type and JITed flag
+ 	 */
+ 	enum bpf_prog_type owner_prog_type;
+ 	bool owner_jited;
+ 	union {
+ 		char value[0] __aligned(8);
+ 		void *ptrs[0] __aligned(8);
+ 		void __percpu *pptrs[0] __aligned(8);
+ 	};
+ };
+ 
+ #define MAX_TAIL_CALL_CNT 32
+ 
+ struct bpf_event_entry {
+ 	struct perf_event *event;
+ 	struct file *perf_file;
+ 	struct file *map_file;
+ 	struct rcu_head rcu;
+ };
+ 
+ u64 bpf_tail_call(u64 ctx, u64 r2, u64 index, u64 r4, u64 r5);
+ u64 bpf_get_stackid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
+ bool bpf_prog_array_compatible(struct bpf_array *array, const struct bpf_prog *fp);
+ int bpf_prog_calc_tag(struct bpf_prog *fp);
+ 
+ const struct bpf_func_proto *bpf_get_trace_printk_proto(void);
+ 
+ typedef unsigned long (*bpf_ctx_copy_t)(void *dst, const void *src,
+ 					unsigned long off, unsigned long len);
+ 
+ u64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,
+ 		     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy);
+ 
+ int bpf_prog_test_run_xdp(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ int bpf_prog_test_run_skb(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ DECLARE_PER_CPU(int, bpf_prog_active);
+ 
+ #define BPF_PROG_TYPE(_id, _ops) \
+ 	extern const struct bpf_verifier_ops _ops;
+ #define BPF_MAP_TYPE(_id, _ops) \
+ 	extern const struct bpf_map_ops _ops;
+ #include <linux/bpf_types.h>
+ #undef BPF_PROG_TYPE
+ #undef BPF_MAP_TYPE
+ 
+ struct bpf_prog *bpf_prog_get(u32 ufd);
+ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type);
+ struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog, int i);
+ void bpf_prog_sub(struct bpf_prog *prog, int i);
+ struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog);
+ struct bpf_prog * __must_check bpf_prog_inc_not_zero(struct bpf_prog *prog);
+ void bpf_prog_put(struct bpf_prog *prog);
+ int __bpf_prog_charge(struct user_struct *user, u32 pages);
+ void __bpf_prog_uncharge(struct user_struct *user, u32 pages);
+ 
+ struct bpf_map *bpf_map_get_with_uref(u32 ufd);
+ struct bpf_map *__bpf_map_get(struct fd f);
+ struct bpf_map * __must_check bpf_map_inc(struct bpf_map *map, bool uref);
+ void bpf_map_put_with_uref(struct bpf_map *map);
+ void bpf_map_put(struct bpf_map *map);
+ int bpf_map_precharge_memlock(u32 pages);
+ void *bpf_map_area_alloc(size_t size, int numa_node);
+ void bpf_map_area_free(void *base);
+ 
+ extern int sysctl_unprivileged_bpf_disabled;
+ 
+ int bpf_map_new_fd(struct bpf_map *map);
+ int bpf_prog_new_fd(struct bpf_prog *prog);
+ 
+ int bpf_obj_pin_user(u32 ufd, const char __user *pathname);
+ int bpf_obj_get_user(const char __user *pathname);
+ 
+ int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,
+ 			   u64 flags);
+ int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,
+ 			    u64 flags);
+ 
+ int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value);
+ 
+ int bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				 void *key, void *value, u64 map_flags);
+ int bpf_fd_array_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ void bpf_fd_array_map_clear(struct bpf_map *map);
+ int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				void *key, void *value, u64 map_flags);
+ int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ 
+ /* memcpy that is used with 8-byte aligned pointers, power-of-8 size and
+  * forced to use 'long' read/writes to try to atomically copy long counters.
+  * Best-effort only.  No barriers here, since it _will_ race with concurrent
+  * updates from BPF programs. Called from bpf syscall and mostly used with
+  * size 8 or 16 bytes, so ask compiler to inline it.
+  */
+ static inline void bpf_long_memcpy(void *dst, const void *src, u32 size)
+ {
+ 	const long *lsrc = src;
+ 	long *ldst = dst;
+ 
+ 	size /= sizeof(long);
+ 	while (size--)
+ 		*ldst++ = *lsrc++;
+ }
+ 
+ /* verify correctness of eBPF program */
+ int bpf_check(struct bpf_prog **fp, union bpf_attr *attr);
+ 
+ /* Map specifics */
+ struct net_device  *__dev_map_lookup_elem(struct bpf_map *map, u32 key);
+ void __dev_map_insert_ctx(struct bpf_map *map, u32 index);
+ void __dev_map_flush(struct bpf_map *map);
+ 
+ /* Return map's numa specified by userspace */
+ static inline int bpf_map_attr_numa_node(const union bpf_attr *attr)
+ {
+ 	return (attr->map_flags & BPF_F_NUMA_NODE) ?
+ 		attr->numa_node : NUMA_NO_NODE;
+ }
+ 
+ #else
+ static inline struct bpf_prog *bpf_prog_get(u32 ufd)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get_type(u32 ufd,
+ 						 enum bpf_prog_type type)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ static inline struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog,
+ 							  int i)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void bpf_prog_sub(struct bpf_prog *prog, int i)
+ {
+ }
+ 
+ static inline void bpf_prog_put(struct bpf_prog *prog)
+ {
+ }
+ 
+ static inline struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *__must_check
+ bpf_prog_inc_not_zero(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline int __bpf_prog_charge(struct user_struct *user, u32 pages)
+ {
+ 	return 0;
+ }
+ 
+ static inline void __bpf_prog_uncharge(struct user_struct *user, u32 pages)
+ {
+ }
+ 
+ static inline struct net_device  *__dev_map_lookup_elem(struct bpf_map *map,
+ 						       u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void __dev_map_insert_ctx(struct bpf_map *map, u32 index)
+ {
+ }
+ 
+ static inline void __dev_map_flush(struct bpf_map *map)
+ {
+ }
+ #endif /* CONFIG_BPF_SYSCALL */
+ 
+ #if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_BPF_SYSCALL)
+ struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key);
+ #else
+ static inline struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key)
+ {
+ 	return NULL;
+ }
+ #endif
+ 
+ /* verifier prototypes for helper functions called from eBPF programs */
+ extern const struct bpf_func_proto bpf_map_lookup_elem_proto;
+ extern const struct bpf_func_proto bpf_map_update_elem_proto;
+ extern const struct bpf_func_proto bpf_map_delete_elem_proto;
+ 
+ extern const struct bpf_func_proto bpf_get_prandom_u32_proto;
+ extern const struct bpf_func_proto bpf_get_smp_processor_id_proto;
+ extern const struct bpf_func_proto bpf_get_numa_node_id_proto;
+ extern const struct bpf_func_proto bpf_tail_call_proto;
+ extern const struct bpf_func_proto bpf_ktime_get_ns_proto;
+ extern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;
+ extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
+ extern const struct bpf_func_proto bpf_get_current_comm_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_push_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_pop_proto;
+ extern const struct bpf_func_proto bpf_get_stackid_proto;
+ extern const struct bpf_func_proto bpf_sock_map_update_proto;
+ 
+ /* Shared helpers among cBPF and eBPF. */
+ void bpf_user_rnd_init_once(void);
+ u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
++>>>>>>> 96eabe7a40aa (bpf: Allow selecting numa node during map creation)
  #endif /* _LINUX_BPF_H */
diff --cc include/uapi/linux/bpf.h
index e369860b690e,843818dff96d..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -115,14 -115,81 +115,73 @@@ enum bpf_map_type 
  
  enum bpf_prog_type {
  	BPF_PROG_TYPE_UNSPEC,
 -	BPF_PROG_TYPE_SOCKET_FILTER,
 -	BPF_PROG_TYPE_KPROBE,
 -	BPF_PROG_TYPE_SCHED_CLS,
 -	BPF_PROG_TYPE_SCHED_ACT,
 -	BPF_PROG_TYPE_TRACEPOINT,
 -	BPF_PROG_TYPE_XDP,
 -	BPF_PROG_TYPE_PERF_EVENT,
 -	BPF_PROG_TYPE_CGROUP_SKB,
 -	BPF_PROG_TYPE_CGROUP_SOCK,
 -	BPF_PROG_TYPE_LWT_IN,
 -	BPF_PROG_TYPE_LWT_OUT,
 -	BPF_PROG_TYPE_LWT_XMIT,
 -	BPF_PROG_TYPE_SOCK_OPS,
 -	BPF_PROG_TYPE_SK_SKB,
  };
  
++<<<<<<< HEAD
++=======
+ enum bpf_attach_type {
+ 	BPF_CGROUP_INET_INGRESS,
+ 	BPF_CGROUP_INET_EGRESS,
+ 	BPF_CGROUP_INET_SOCK_CREATE,
+ 	BPF_CGROUP_SOCK_OPS,
+ 	BPF_CGROUP_SMAP_INGRESS,
+ 	__MAX_BPF_ATTACH_TYPE
+ };
+ 
+ #define MAX_BPF_ATTACH_TYPE __MAX_BPF_ATTACH_TYPE
+ 
+ /* If BPF_SOCKMAP_STRPARSER is used sockmap will use strparser on receive */
+ #define BPF_SOCKMAP_STRPARSER	(1U << 0)
+ 
+ /* If BPF_F_ALLOW_OVERRIDE flag is used in BPF_PROG_ATTACH command
+  * to the given target_fd cgroup the descendent cgroup will be able to
+  * override effective bpf program that was inherited from this cgroup
+  */
+ #define BPF_F_ALLOW_OVERRIDE	(1U << 0)
+ 
+ /* If BPF_F_STRICT_ALIGNMENT is used in BPF_PROG_LOAD command, the
+  * verifier will perform strict alignment checking as if the kernel
+  * has been built with CONFIG_EFFICIENT_UNALIGNED_ACCESS not set,
+  * and NET_IP_ALIGN defined to 2.
+  */
+ #define BPF_F_STRICT_ALIGNMENT	(1U << 0)
+ 
+ #define BPF_PSEUDO_MAP_FD	1
+ 
+ /* flags for BPF_MAP_UPDATE_ELEM command */
+ #define BPF_ANY		0 /* create new element or update existing */
+ #define BPF_NOEXIST	1 /* create new element if it didn't exist */
+ #define BPF_EXIST	2 /* update existing element */
+ 
+ /* flags for BPF_MAP_CREATE command */
+ #define BPF_F_NO_PREALLOC	(1U << 0)
+ /* Instead of having one common LRU list in the
+  * BPF_MAP_TYPE_LRU_[PERCPU_]HASH map, use a percpu LRU list
+  * which can scale and perform better.
+  * Note, the LRU nodes (including free nodes) cannot be moved
+  * across different LRU lists.
+  */
+ #define BPF_F_NO_COMMON_LRU	(1U << 1)
+ /* Specify numa node during map creation */
+ #define BPF_F_NUMA_NODE		(1U << 2)
+ 
++>>>>>>> 96eabe7a40aa (bpf: Allow selecting numa node during map creation)
  union bpf_attr {
  	struct { /* anonymous struct used by BPF_MAP_CREATE command */
  		__u32	map_type;	/* one of enum bpf_map_type */
  		__u32	key_size;	/* size of key in bytes */
  		__u32	value_size;	/* size of value in bytes */
  		__u32	max_entries;	/* max number of entries in a map */
++<<<<<<< HEAD
++=======
+ 		__u32	map_flags;	/* BPF_MAP_CREATE related
+ 					 * flags defined above.
+ 					 */
+ 		__u32	inner_map_fd;	/* fd pointing to the inner map */
+ 		__u32	numa_node;	/* numa node (effective only if
+ 					 * BPF_F_NUMA_NODE is set).
+ 					 */
++>>>>>>> 96eabe7a40aa (bpf: Allow selecting numa node during map creation)
  	};
  
  	struct { /* anonymous struct used by BPF_MAP_*_ELEM commands */
* Unmerged path kernel/bpf/arraymap.c
* Unmerged path kernel/bpf/devmap.c
* Unmerged path kernel/bpf/hashtab.c
* Unmerged path kernel/bpf/lpm_trie.c
* Unmerged path kernel/bpf/sockmap.c
* Unmerged path kernel/bpf/stackmap.c
* Unmerged path kernel/bpf/syscall.c
* Unmerged path include/linux/bpf.h
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/bpf/arraymap.c
* Unmerged path kernel/bpf/devmap.c
* Unmerged path kernel/bpf/hashtab.c
* Unmerged path kernel/bpf/lpm_trie.c
* Unmerged path kernel/bpf/sockmap.c
* Unmerged path kernel/bpf/stackmap.c
* Unmerged path kernel/bpf/syscall.c
