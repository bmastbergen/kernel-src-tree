mm: introduce VM_LOCKONFAULT

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] mlock: introduce VM_LOCKONFAULT (Rafael Aquini) [1560030]
Rebuild_FUZZ: 91.53%
commit-author Eric B Munson <emunson@akamai.com>
commit de60f5f10c58d4f34b68622442c0e04180367f3f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/de60f5f1.failed

The cost of faulting in all memory to be locked can be very high when
working with large mappings.  If only portions of the mapping will be used
this can incur a high penalty for locking.

For the example of a large file, this is the usage pattern for a large
statical language model (probably applies to other statical or graphical
models as well).  For the security example, any application transacting in
data that cannot be swapped out (credit card data, medical records, etc).

This patch introduces the ability to request that pages are not
pre-faulted, but are placed on the unevictable LRU when they are finally
faulted in.  The VM_LOCKONFAULT flag will be used together with VM_LOCKED
and has no effect when set without VM_LOCKED.  Setting the VM_LOCKONFAULT
flag for a VMA will cause pages faulted into that VMA to be added to the
unevictable LRU when they are faulted or if they are already present, but
will not cause any missing pages to be faulted in.

Exposing this new lock state means that we cannot overload the meaning of
the FOLL_POPULATE flag any longer.  Prior to this patch it was used to
mean that the VMA for a fault was locked.  This means we need the new
FOLL_MLOCK flag to communicate the locked state of a VMA.  FOLL_POPULATE
will now only control if the VMA should be populated and in the case of
VM_LOCKONFAULT, it will not be set.

	Signed-off-by: Eric B Munson <emunson@akamai.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Guenter Roeck <linux@roeck-us.net>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Michael Kerrisk <mtk.manpages@gmail.com>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: Shuah Khan <shuahkh@osg.samsung.com>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit de60f5f10c58d4f34b68622442c0e04180367f3f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	kernel/fork.c
#	mm/debug.c
#	mm/gup.c
#	mm/huge_memory.c
diff --cc include/linux/mm.h
index e9281aafa708,906c46a05707..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -135,8 -139,7 +135,12 @@@ extern unsigned int kobjsize(const voi
  
  #define VM_DONTCOPY	0x00020000      /* Do not copy this vma on fork */
  #define VM_DONTEXPAND	0x00040000	/* Cannot expand with mremap() */
++<<<<<<< HEAD
 +#define VM_FOP_EXTEND	0x00080000	/* RH usage only, not for external modules use */
 +
++=======
+ #define VM_LOCKONFAULT	0x00080000	/* Lock the pages covered when they are faulted in */
++>>>>>>> de60f5f10c58 (mm: introduce VM_LOCKONFAULT)
  #define VM_ACCOUNT	0x00100000	/* Is a VM accounted object */
  #define VM_NORESERVE	0x00200000	/* should the VM suppress accounting */
  #define VM_HUGETLB	0x00400000	/* Huge TLB Page VM */
@@@ -2276,8 -2141,7 +2283,12 @@@ static inline struct page *follow_page(
  #define FOLL_NUMA	0x200	/* force NUMA hinting page fault */
  #define FOLL_MIGRATION	0x400	/* wait for page to replace migration entry */
  #define FOLL_TRIED	0x800	/* a retry, previous pass started an IO */
++<<<<<<< HEAD
 +#define FOLL_REMOTE	0x2000	/* we are working on non-current tsk/mm */
 +#define FOLL_COW	0x4000	/* internal GUP flag */
++=======
+ #define FOLL_MLOCK	0x1000	/* lock present pages */
++>>>>>>> de60f5f10c58 (mm: introduce VM_LOCKONFAULT)
  
  typedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,
  			void *data);
diff --cc kernel/fork.c
index 55e4ca475546,a30fae45b486..000000000000
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@@ -458,19 -448,16 +458,24 @@@ static int dup_mmap(struct mm_struct *m
  			goto fail_nomem;
  		*tmp = *mpnt;
  		INIT_LIST_HEAD(&tmp->anon_vma_chain);
 -		retval = vma_dup_policy(mpnt, tmp);
 -		if (retval)
 +		pol = mpol_dup(vma_policy(mpnt));
 +		retval = PTR_ERR(pol);
 +		if (IS_ERR(pol))
  			goto fail_nomem_policy;
 +		vma_set_policy(tmp, pol);
  		tmp->vm_mm = mm;
 +		retval = dup_userfaultfd(tmp, &uf);
 +		if (retval)
 +			goto fail_nomem_anon_vma_fork;
  		if (anon_vma_fork(tmp, mpnt))
  			goto fail_nomem_anon_vma_fork;
++<<<<<<< HEAD
 +		tmp->vm_flags &= ~(VM_LOCKED);
++=======
+ 		tmp->vm_flags &=
+ 			~(VM_LOCKED|VM_LOCKONFAULT|VM_UFFD_MISSING|VM_UFFD_WP);
++>>>>>>> de60f5f10c58 (mm: introduce VM_LOCKONFAULT)
  		tmp->vm_next = tmp->vm_prev = NULL;
 -		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
  		file = tmp->vm_file;
  		if (file) {
  			struct inode *inode = file_inode(file);
diff --cc mm/gup.c
index 20b926f646c5,deafa2c91b36..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -269,21 -157,241 +269,174 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -/**
 - * follow_page_mask - look up a page descriptor from a user-virtual address
 - * @vma: vm_area_struct mapping @address
 - * @address: virtual address to look up
 - * @flags: flags modifying lookup behaviour
 - * @page_mask: on output, *page_mask is set according to the size of the page
 - *
 - * @flags can have FOLL_ flags set, defined in <linux/mm.h>
 - *
 - * Returns the mapped (struct page *), %NULL if no mapping exists, or
 - * an error pointer if there is a mapping to something not represented
 - * by a page descriptor (see also vm_normal_page()).
 - */
 -struct page *follow_page_mask(struct vm_area_struct *vma,
 -			      unsigned long address, unsigned int flags,
 -			      unsigned int *page_mask)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	*page_mask = 0;
 -
 -	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 -	if (!IS_ERR(page)) {
 -		BUG_ON(flags & FOLL_GET);
  		return page;
++<<<<<<< HEAD
++=======
+ 	}
+ 
+ 	pgd = pgd_offset(mm, address);
+ 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pud = pud_offset(pgd, address);
+ 	if (pud_none(*pud))
+ 		return no_page_table(vma, flags);
+ 	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pud(mm, address, pud, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (unlikely(pud_bad(*pud)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pmd(mm, address, pmd, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_trans_huge(*pmd)) {
+ 		if (flags & FOLL_SPLIT) {
+ 			split_huge_page_pmd(vma, address, pmd);
+ 			return follow_page_pte(vma, address, pmd, flags);
+ 		}
+ 		ptl = pmd_lock(mm, pmd);
+ 		if (likely(pmd_trans_huge(*pmd))) {
+ 			if (unlikely(pmd_trans_splitting(*pmd))) {
+ 				spin_unlock(ptl);
+ 				wait_split_huge_page(vma->anon_vma, pmd);
+ 			} else {
+ 				page = follow_trans_huge_pmd(vma, address,
+ 							     pmd, flags);
+ 				spin_unlock(ptl);
+ 				*page_mask = HPAGE_PMD_NR - 1;
+ 				return page;
+ 			}
+ 		} else
+ 			spin_unlock(ptl);
+ 	}
+ 	return follow_page_pte(vma, address, pmd, flags);
+ }
+ 
+ static int get_gate_page(struct mm_struct *mm, unsigned long address,
+ 		unsigned int gup_flags, struct vm_area_struct **vma,
+ 		struct page **page)
+ {
+ 	pgd_t *pgd;
+ 	pud_t *pud;
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 	int ret = -EFAULT;
+ 
+ 	/* user gate pages are read-only */
+ 	if (gup_flags & FOLL_WRITE)
+ 		return -EFAULT;
+ 	if (address > TASK_SIZE)
+ 		pgd = pgd_offset_k(address);
+ 	else
+ 		pgd = pgd_offset_gate(mm, address);
+ 	BUG_ON(pgd_none(*pgd));
+ 	pud = pud_offset(pgd, address);
+ 	BUG_ON(pud_none(*pud));
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return -EFAULT;
+ 	VM_BUG_ON(pmd_trans_huge(*pmd));
+ 	pte = pte_offset_map(pmd, address);
+ 	if (pte_none(*pte))
+ 		goto unmap;
+ 	*vma = get_gate_vma(mm);
+ 	if (!page)
+ 		goto out;
+ 	*page = vm_normal_page(*vma, address, *pte);
+ 	if (!*page) {
+ 		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
+ 			goto unmap;
+ 		*page = pte_page(*pte);
+ 	}
+ 	get_page(*page);
+ out:
+ 	ret = 0;
+ unmap:
+ 	pte_unmap(pte);
+ 	return ret;
+ }
+ 
+ /*
+  * mmap_sem must be held on entry.  If @nonblocking != NULL and
+  * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.
+  * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
+  */
+ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
+ 		unsigned long address, unsigned int *flags, int *nonblocking)
+ {
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned int fault_flags = 0;
+ 	int ret;
+ 
+ 	/* mlock all present pages, but do not fault in new pages */
+ 	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
+ 		return -ENOENT;
+ 	/* For mm_populate(), just skip the stack guard page. */
+ 	if ((*flags & FOLL_POPULATE) &&
+ 			(stack_guard_page_start(vma, address) ||
+ 			 stack_guard_page_end(vma, address + PAGE_SIZE)))
+ 		return -ENOENT;
+ 	if (*flags & FOLL_WRITE)
+ 		fault_flags |= FAULT_FLAG_WRITE;
+ 	if (nonblocking)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
+ 	if (*flags & FOLL_NOWAIT)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
+ 	if (*flags & FOLL_TRIED) {
+ 		VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
+ 		fault_flags |= FAULT_FLAG_TRIED;
+ 	}
+ 
+ 	ret = handle_mm_fault(mm, vma, address, fault_flags);
+ 	if (ret & VM_FAULT_ERROR) {
+ 		if (ret & VM_FAULT_OOM)
+ 			return -ENOMEM;
+ 		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
+ 			return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;
+ 		if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
+ 			return -EFAULT;
+ 		BUG();
+ 	}
+ 
+ 	if (tsk) {
+ 		if (ret & VM_FAULT_MAJOR)
+ 			tsk->maj_flt++;
+ 		else
+ 			tsk->min_flt++;
+ 	}
+ 
+ 	if (ret & VM_FAULT_RETRY) {
+ 		if (nonblocking)
+ 			*nonblocking = 0;
+ 		return -EBUSY;
+ 	}
++>>>>>>> de60f5f10c58 (mm: introduce VM_LOCKONFAULT)
  
 +no_page_table:
  	/*
 -	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
 -	 * necessary, even if maybe_mkwrite decided not to set pte_write. We
 -	 * can thus safely do subsequent page lookups as if they were reads.
 -	 * But only do so when looping for pte_write is futile: in some cases
 -	 * userspace may also be wanting to write to the gotten user page,
 -	 * which a read fault here might prevent (a readonly page might get
 -	 * reCOWed by userspace write).
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
  	 */
 -	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
 -		*flags &= ~FOLL_WRITE;
 -	return 0;
 -}
 -
 -static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 -{
 -	vm_flags_t vm_flags = vma->vm_flags;
 -
 -	if (vm_flags & (VM_IO | VM_PFNMAP))
 -		return -EFAULT;
 -
 -	if (gup_flags & FOLL_WRITE) {
 -		if (!(vm_flags & VM_WRITE)) {
 -			if (!(gup_flags & FOLL_FORCE))
 -				return -EFAULT;
 -			/*
 -			 * We used to let the write,force case do COW in a
 -			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
 -			 * set a breakpoint in a read-only mapping of an
 -			 * executable, without corrupting the file (yet only
 -			 * when that file had been opened for writing!).
 -			 * Anon pages in shared mappings are surprising: now
 -			 * just reject it.
 -			 */
 -			if (!is_cow_mapping(vm_flags)) {
 -				WARN_ON_ONCE(vm_flags & VM_MAYWRITE);
 -				return -EFAULT;
 -			}
 -		}
 -	} else if (!(vm_flags & VM_READ)) {
 -		if (!(gup_flags & FOLL_FORCE))
 -			return -EFAULT;
 -		/*
 -		 * Is there actually any vma we can reach here which does not
 -		 * have VM_MAYREAD set?
 -		 */
 -		if (!(vm_flags & VM_MAYREAD))
 -			return -EFAULT;
 -	}
 -	return 0;
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
 +	return page;
  }
  
  /**
@@@ -878,31 -852,135 +1031,95 @@@ EXPORT_SYMBOL(get_user_pages_unlocked)
   * should use get_user_pages because it cannot pass
   * FAULT_FLAG_ALLOW_RETRY to handle_mm_fault.
   */
 -long get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 -		unsigned long start, unsigned long nr_pages, int write,
 -		int force, struct page **pages, struct vm_area_struct **vmas)
 +long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,
 +		unsigned long start, unsigned long nr_pages,
 +		int write, int force, struct page **pages,
 +		struct vm_area_struct **vmas)
  {
  	return __get_user_pages_locked(tsk, mm, start, nr_pages, write, force,
++<<<<<<< HEAD
 +				       pages, vmas, NULL, false,
 +				       FOLL_TOUCH | FOLL_REMOTE);
++=======
+ 				       pages, vmas, NULL, false, FOLL_TOUCH);
+ }
+ EXPORT_SYMBOL(get_user_pages);
+ 
+ /**
+  * populate_vma_page_range() -  populate a range of pages in the vma.
+  * @vma:   target vma
+  * @start: start address
+  * @end:   end address
+  * @nonblocking:
+  *
+  * This takes care of mlocking the pages too if VM_LOCKED is set.
+  *
+  * return 0 on success, negative error code on error.
+  *
+  * vma->vm_mm->mmap_sem must be held.
+  *
+  * If @nonblocking is NULL, it may be held for read or write and will
+  * be unperturbed.
+  *
+  * If @nonblocking is non-NULL, it must held for read only and may be
+  * released.  If it's released, *@nonblocking will be set to 0.
+  */
+ long populate_vma_page_range(struct vm_area_struct *vma,
+ 		unsigned long start, unsigned long end, int *nonblocking)
+ {
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned long nr_pages = (end - start) / PAGE_SIZE;
+ 	int gup_flags;
+ 
+ 	VM_BUG_ON(start & ~PAGE_MASK);
+ 	VM_BUG_ON(end   & ~PAGE_MASK);
+ 	VM_BUG_ON_VMA(start < vma->vm_start, vma);
+ 	VM_BUG_ON_VMA(end   > vma->vm_end, vma);
+ 	VM_BUG_ON_MM(!rwsem_is_locked(&mm->mmap_sem), mm);
+ 
+ 	gup_flags = FOLL_TOUCH | FOLL_POPULATE | FOLL_MLOCK;
+ 	if (vma->vm_flags & VM_LOCKONFAULT)
+ 		gup_flags &= ~FOLL_POPULATE;
+ 
+ 	/*
+ 	 * We want to touch writable mappings with a write fault in order
+ 	 * to break COW, except for shared mappings because these don't COW
+ 	 * and we would not want to dirty them for nothing.
+ 	 */
+ 	if ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)
+ 		gup_flags |= FOLL_WRITE;
+ 
+ 	/*
+ 	 * We want mlock to succeed for regions that have any permissions
+ 	 * other than PROT_NONE.
+ 	 */
+ 	if (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))
+ 		gup_flags |= FOLL_FORCE;
+ 
+ 	/*
+ 	 * We made sure addr is within a VMA, so the following will
+ 	 * not result in a stack expansion that recurses back here.
+ 	 */
+ 	return __get_user_pages(current, mm, start, nr_pages, gup_flags,
+ 				NULL, NULL, nonblocking);
++>>>>>>> de60f5f10c58 (mm: introduce VM_LOCKONFAULT)
  }
 +EXPORT_SYMBOL(get_user_pages_remote);
  
  /*
 - * __mm_populate - populate and/or mlock pages within a range of address space.
 - *
 - * This is used to implement mlock() and the MAP_POPULATE / MAP_LOCKED mmap
 - * flags. VMAs must be already marked with the desired vm_flags, and
 - * mmap_sem must not be held.
 + * This is the same as get_user_pages_remote() for the time
 + * being.
   */
 -int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)
 +long get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 +		unsigned long start, unsigned long nr_pages,
 +		int write, int force, struct page **pages,
 +		struct vm_area_struct **vmas)
  {
 -	struct mm_struct *mm = current->mm;
 -	unsigned long end, nstart, nend;
 -	struct vm_area_struct *vma = NULL;
 -	int locked = 0;
 -	long ret = 0;
 -
 -	VM_BUG_ON(start & ~PAGE_MASK);
 -	VM_BUG_ON(len != PAGE_ALIGN(len));
 -	end = start + len;
 -
 -	for (nstart = start; nstart < end; nstart = nend) {
 -		/*
 -		 * We want to fault in pages for [nstart; end) address range.
 -		 * Find first corresponding VMA.
 -		 */
 -		if (!locked) {
 -			locked = 1;
 -			down_read(&mm->mmap_sem);
 -			vma = find_vma(mm, nstart);
 -		} else if (nstart >= vma->vm_end)
 -			vma = vma->vm_next;
 -		if (!vma || vma->vm_start >= end)
 -			break;
 -		/*
 -		 * Set [nstart; nend) to intersection of desired address
 -		 * range with the first VMA. Also, skip undesirable VMA types.
 -		 */
 -		nend = min(end, vma->vm_end);
 -		if (vma->vm_flags & (VM_IO | VM_PFNMAP))
 -			continue;
 -		if (nstart < vma->vm_start)
 -			nstart = vma->vm_start;
 -		/*
 -		 * Now fault in a range of pages. populate_vma_page_range()
 -		 * double checks the vma flags, so that it won't mlock pages
 -		 * if the vma was already munlocked.
 -		 */
 -		ret = populate_vma_page_range(vma, nstart, nend, &locked);
 -		if (ret < 0) {
 -			if (ignore_errors) {
 -				ret = 0;
 -				continue;	/* continue at next VMA */
 -			}
 -			break;
 -		}
 -		nend = nstart + ret * PAGE_SIZE;
 -		ret = 0;
 -	}
 -	if (locked)
 -		up_read(&mm->mmap_sem);
 -	return ret;	/* 0 or negative error code */
 +	return __get_user_pages_locked(tsk, mm, start, nr_pages,
 +				       write, force, pages, vmas, NULL, false,
 +				       FOLL_TOUCH);
  }
 +EXPORT_SYMBOL(get_user_pages);
  
  /**
   * get_dump_page() - pin user page in memory while writing it to core dump
diff --cc mm/huge_memory.c
index 14da2891fc9a,f5c08b46fef8..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1605,8 -1292,21 +1605,26 @@@ struct page *follow_trans_huge_pmd(stru
  
  	page = pmd_page(*pmd);
  	VM_BUG_ON_PAGE(!PageHead(page), page);
++<<<<<<< HEAD
 +	if (flags & FOLL_TOUCH)
 +		touch_pmd(vma, addr, pmd);
++=======
+ 	if (flags & FOLL_TOUCH) {
+ 		pmd_t _pmd;
+ 		/*
+ 		 * We should set the dirty bit only for FOLL_WRITE but
+ 		 * for now the dirty bit in the pmd is meaningless.
+ 		 * And if the dirty bit will become meaningful and
+ 		 * we'll only set it with FOLL_WRITE, an atomic
+ 		 * set_bit will be required on the pmd to set the
+ 		 * young bit, instead of the current set_pmd_at.
+ 		 */
+ 		_pmd = pmd_mkyoung(pmd_mkdirty(*pmd));
+ 		if (pmdp_set_access_flags(vma, addr & HPAGE_PMD_MASK,
+ 					  pmd, _pmd,  1))
+ 			update_mmu_cache_pmd(vma, addr, pmd);
+ 	}
++>>>>>>> de60f5f10c58 (mm: introduce VM_LOCKONFAULT)
  	if ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
  		if (page->mapping && trylock_page(page)) {
  			lru_add_drain();
* Unmerged path mm/debug.c
* Unmerged path include/linux/mm.h
* Unmerged path kernel/fork.c
* Unmerged path mm/debug.c
* Unmerged path mm/gup.c
* Unmerged path mm/huge_memory.c
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index e183328e0ee9..8e3cb8fa92fa 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -4550,8 +4550,8 @@ static unsigned long page_table_shareable(struct vm_area_struct *svma,
 	unsigned long s_end = sbase + PUD_SIZE;
 
 	/* Allow segments to share if only one is marked locked */
-	unsigned long vm_flags = vma->vm_flags & ~VM_LOCKED;
-	unsigned long svm_flags = svma->vm_flags & ~VM_LOCKED;
+	unsigned long vm_flags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;
+	unsigned long svm_flags = svma->vm_flags & VM_LOCKED_CLEAR_MASK;
 
 	/*
 	 * match the virtual addresses, permission and the alignment of the
diff --git a/mm/mlock.c b/mm/mlock.c
index 51bce8fdbf14..2d291cc5d05f 100644
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@ -443,7 +443,7 @@ static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,
 void munlock_vma_pages_range(struct vm_area_struct *vma,
 			     unsigned long start, unsigned long end)
 {
-	vma->vm_flags &= ~VM_LOCKED;
+	vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
 
 	while (start < end) {
 		struct page *page = NULL;
diff --git a/mm/mmap.c b/mm/mmap.c
index 774cc98b230c..567ac198659b 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1823,7 +1823,7 @@ out:
 					vma == get_gate_vma(current->mm)))
 			mm->locked_vm += (len >> PAGE_SHIFT);
 		else
-			vma->vm_flags &= ~VM_LOCKED;
+			vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
 	}
 
 	if (file)
