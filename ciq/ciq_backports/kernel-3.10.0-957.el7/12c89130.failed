x86/asm/memcpy_mcsafe: Add write-protection-fault handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] asm/memcpy_mcsafe: Add write-protection-fault handling (Jeff Moyer) [1608674]
Rebuild_FUZZ: 96.43%
commit-author Dan Williams <dan.j.williams@intel.com>
commit 12c89130a56ae8e8d85db753d70333c4ee0ea835
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/12c89130.failed

In preparation for using memcpy_mcsafe() to handle user copies it needs
to be to handle write-protection faults while writing user pages. Add
MMU-fault handlers alongside the machine-check exception handlers.

Note that the machine check fault exception handling makes assumptions
about source buffer alignment and poison alignment. In the write fault
case, given the destination buffer is arbitrarily aligned, it needs a
separate / additional fault handling approach. The mcsafe_handle_tail()
helper is reused. The @limit argument is set to @len since there is no
safety concern about retriggering an MMU fault, and this simplifies the
assembly.

Co-developed-by: Tony Luck <tony.luck@intel.com>
	Reported-by: Mika Penttil√§ <mika.penttila@nextfour.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Cc: Al Viro <viro@zeniv.linux.org.uk>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: hch@lst.de
	Cc: linux-fsdevel@vger.kernel.org
	Cc: linux-nvdimm@lists.01.org
Link: http://lkml.kernel.org/r/152539238635.31796.14056325365122961778.stgit@dwillia2-desk3.amr.corp.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 12c89130a56ae8e8d85db753d70333c4ee0ea835)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/lib/memcpy_64.S
diff --cc arch/x86/lib/memcpy_64.S
index 36b962df086c,c3b527a9f95d..000000000000
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@@ -299,25 -248,40 +299,45 @@@ ENTRY(memcpy_mcsafe_unrolled
  .L_done_memcpy_trap:
  	xorq %rax, %rax
  	ret
 -ENDPROC(__memcpy_mcsafe)
 -EXPORT_SYMBOL_GPL(__memcpy_mcsafe)
 +ENDPROC(memcpy_mcsafe_unrolled)
  
  	.section .fixup, "ax"
 -	/*
 -	 * Return number of bytes not copied for any failure. Note that
 -	 * there is no "tail" handling since the source buffer is 8-byte
 -	 * aligned and poison is cacheline aligned.
 -	 */
 -.E_read_words:
 -	shll	$3, %ecx
 -.E_leading_bytes:
 -	addl	%edx, %ecx
 -.E_trailing_bytes:
 -	mov	%ecx, %eax
 +	/* Return -EFAULT for any failure */
 +.L_memcpy_mcsafe_fail:
 +	mov	$-EFAULT, %rax
  	ret
  
+ 	/*
+ 	 * For write fault handling, given the destination is unaligned,
+ 	 * we handle faults on multi-byte writes with a byte-by-byte
+ 	 * copy up to the write-protected page.
+ 	 */
+ .E_write_words:
+ 	shll	$3, %ecx
+ 	addl	%edx, %ecx
+ 	movl	%ecx, %edx
+ 	jmp mcsafe_handle_tail
+ 
  	.previous
  
++<<<<<<< HEAD
 +	_ASM_EXTABLE_FAULT(.L_copy_leading_bytes, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w0, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w1, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w2, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w4, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w5, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w6, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_cache_w7, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_copy_trailing_words, .L_memcpy_mcsafe_fail)
 +	_ASM_EXTABLE_FAULT(.L_copy_trailing_bytes, .L_memcpy_mcsafe_fail)
++=======
+ 	_ASM_EXTABLE_FAULT(.L_read_leading_bytes, .E_leading_bytes)
+ 	_ASM_EXTABLE_FAULT(.L_read_words, .E_read_words)
+ 	_ASM_EXTABLE_FAULT(.L_read_trailing_bytes, .E_trailing_bytes)
+ 	_ASM_EXTABLE(.L_write_leading_bytes, .E_leading_bytes)
+ 	_ASM_EXTABLE(.L_write_words, .E_write_words)
+ 	_ASM_EXTABLE(.L_write_trailing_bytes, .E_trailing_bytes)
++>>>>>>> 12c89130a56a (x86/asm/memcpy_mcsafe: Add write-protection-fault handling)
  #endif
diff --git a/arch/x86/include/asm/uaccess_64.h b/arch/x86/include/asm/uaccess_64.h
index 109207779f98..50b3a3333bf1 100644
--- a/arch/x86/include/asm/uaccess_64.h
+++ b/arch/x86/include/asm/uaccess_64.h
@@ -259,4 +259,7 @@ __copy_from_user_flushcache(void *dst, const void __user *src, unsigned size)
 unsigned long
 copy_user_handle_tail(char *to, char *from, unsigned len, unsigned zerorest);
 
+unsigned long
+mcsafe_handle_tail(char *to, char *from, unsigned len);
+
 #endif /* _ASM_X86_UACCESS_64_H */
* Unmerged path arch/x86/lib/memcpy_64.S
diff --git a/arch/x86/lib/usercopy_64.c b/arch/x86/lib/usercopy_64.c
index 8ff245e9a964..7a73897c027b 100644
--- a/arch/x86/lib/usercopy_64.c
+++ b/arch/x86/lib/usercopy_64.c
@@ -89,6 +89,27 @@ copy_user_handle_tail(char *to, char *from, unsigned len, unsigned zerorest)
 	return len;
 }
 
+/*
+ * Similar to copy_user_handle_tail, probe for the write fault point,
+ * but reuse __memcpy_mcsafe in case a new read error is encountered.
+ * clac() is handled in _copy_to_iter_mcsafe().
+ */
+__visible unsigned long
+mcsafe_handle_tail(char *to, char *from, unsigned len)
+{
+	for (; len; --len, to++, from++) {
+		/*
+		 * Call the assembly routine back directly since
+		 * memcpy_mcsafe() may silently fallback to memcpy.
+		 */
+		unsigned long rem = __memcpy_mcsafe(to, from, 1);
+
+		if (rem)
+			break;
+	}
+	return len;
+}
+
 #ifdef CONFIG_ARCH_HAS_UACCESS_FLUSHCACHE
 /**
  * clean_cache_range - write back a cache range with CLWB
