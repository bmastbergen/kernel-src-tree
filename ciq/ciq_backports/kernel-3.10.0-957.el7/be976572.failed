mm, compaction: properly signal and act upon lock and need_sched() contention

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] compaction: properly signal and act upon lock and need_sched() contention (Andrea Arcangeli) [1610560]
Rebuild_FUZZ: 97.33%
commit-author Vlastimil Babka <vbabka@suse.cz>
commit be9765722e6b7ece8263cbab857490332339bd6f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/be976572.failed

Compaction uses compact_checklock_irqsave() function to periodically check
for lock contention and need_resched() to either abort async compaction,
or to free the lock, schedule and retake the lock.  When aborting,
cc->contended is set to signal the contended state to the caller.  Two
problems have been identified in this mechanism.

First, compaction also calls directly cond_resched() in both scanners when
no lock is yet taken.  This call either does not abort async compaction,
or set cc->contended appropriately.  This patch introduces a new
compact_should_abort() function to achieve both.  In isolate_freepages(),
the check frequency is reduced to once by SWAP_CLUSTER_MAX pageblocks to
match what the migration scanner does in the preliminary page checks.  In
case a pageblock is found suitable for calling isolate_freepages_block(),
the checks within there are done on higher frequency.

Second, isolate_freepages() does not check if isolate_freepages_block()
aborted due to contention, and advances to the next pageblock.  This
violates the principle of aborting on contention, and might result in
pageblocks not being scanned completely, since the scanning cursor is
advanced.  This problem has been noticed in the code by Joonsoo Kim when
reviewing related patches.  This patch makes isolate_freepages_block()
check the cc->contended flag and abort.

In case isolate_freepages() has already isolated some pages before
aborting due to contention, page migration will proceed, which is OK since
we do not want to waste the work that has been done, and page migration
has own checks for contention.  However, we do not want another isolation
attempt by either of the scanners, so cc->contended flag check is added
also to compaction_alloc() and compact_finished() to make sure compaction
is aborted right after the migration.

The outcome of the patch should be reduced lock contention by async
compaction and lower latencies for higher-order allocations where direct
compaction is involved.

[akpm@linux-foundation.org: fix typo in comment]
	Reported-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
	Cc: Michal Nazarewicz <mina86@mina86.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Rik van Riel <riel@redhat.com>
	Acked-by: Michal Nazarewicz <mina86@mina86.com>
	Tested-by: Shawn Guo <shawn.guo@linaro.org>
	Tested-by: Kevin Hilman <khilman@linaro.org>
	Tested-by: Stephen Warren <swarren@nvidia.com>
	Tested-by: Fabio Estevam <fabio.estevam@freescale.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit be9765722e6b7ece8263cbab857490332339bd6f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/compaction.c
diff --cc mm/compaction.c
index 138bdb9a0f06,21bf292b642a..000000000000
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@@ -208,10 -222,28 +208,35 @@@ static bool compact_checklock_irqsave(s
  	return true;
  }
  
++<<<<<<< HEAD
 +static inline bool compact_trylock_irqsave(spinlock_t *lock,
 +			unsigned long *flags, struct compact_control *cc)
 +{
 +	return compact_checklock_irqsave(lock, flags, false, cc);
++=======
+ /*
+  * Aside from avoiding lock contention, compaction also periodically checks
+  * need_resched() and either schedules in sync compaction or aborts async
+  * compaction. This is similar to what compact_checklock_irqsave() does, but
+  * is used where no lock is concerned.
+  *
+  * Returns false when no scheduling was needed, or sync compaction scheduled.
+  * Returns true when async compaction should abort.
+  */
+ static inline bool compact_should_abort(struct compact_control *cc)
+ {
+ 	/* async compaction aborts if contended */
+ 	if (need_resched()) {
+ 		if (cc->mode == MIGRATE_ASYNC) {
+ 			cc->contended = true;
+ 			return true;
+ 		}
+ 
+ 		cond_resched();
+ 	}
+ 
+ 	return false;
++>>>>>>> be9765722e6b (mm, compaction: properly signal and act upon lock and need_sched() contention)
  }
  
  /* Returns true if the page is within a block suitable for migration to */
@@@ -483,8 -518,10 +508,14 @@@ isolate_migratepages_range(struct zone 
  			return 0;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (compact_should_abort(cc))
+ 		return 0;
+ 
++>>>>>>> be9765722e6b (mm, compaction: properly signal and act upon lock and need_sched() contention)
  	/* Time to isolate some pages for migration */
 +	cond_resched();
  	for (; low_pfn < end_pfn; low_pfn++) {
  		/* give a chance to irqs before checking need_resched() */
  		if (locked && !(low_pfn % SWAP_CLUSTER_MAX)) {
@@@ -695,11 -741,13 +726,13 @@@ static void isolate_freepages(struct zo
  		/*
  		 * This can iterate a massively long zone without finding any
  		 * suitable migration targets, so periodically check if we need
- 		 * to schedule.
+ 		 * to schedule, or even abort async compaction.
  		 */
- 		cond_resched();
+ 		if (!(block_start_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages))
+ 						&& compact_should_abort(cc))
+ 			break;
  
 -		if (!pfn_valid(block_start_pfn))
 +		if (!pfn_valid(pfn))
  			continue;
  
  		/*
@@@ -734,14 -776,20 +767,24 @@@
  		nr_freepages += isolated;
  
  		/*
 -		 * Set a flag that we successfully isolated in this pageblock.
 -		 * In the next loop iteration, zone->compact_cached_free_pfn
 -		 * will not be updated and thus it will effectively contain the
 -		 * highest pageblock we isolated pages from.
 +		 * Record the highest PFN we isolated pages from. When next
 +		 * looking for free pages, the search will restart here as
 +		 * page migration may have returned some pages to the allocator
  		 */
 -		if (isolated)
 +		if (isolated) {
  			cc->finished_update_free = true;
++<<<<<<< HEAD
 +			high_pfn = max(high_pfn, pfn);
 +		}
++=======
+ 
+ 		/*
+ 		 * isolate_freepages_block() might have aborted due to async
+ 		 * compaction being contended
+ 		 */
+ 		if (cc->contended)
+ 			break;
++>>>>>>> be9765722e6b (mm, compaction: properly signal and act upon lock and need_sched() contention)
  	}
  
  	/* split_free_page does not map the pages */
* Unmerged path mm/compaction.c
diff --git a/mm/internal.h b/mm/internal.h
index 362e1a30bdaf..4fc0abb64340 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -131,7 +131,10 @@ struct compact_control {
 	int order;			/* order a direct compactor needs */
 	int migratetype;		/* MOVABLE, RECLAIMABLE etc */
 	struct zone *zone;
-	bool contended;			/* True if a lock was contended */
+	bool contended;			/* True if a lock was contended, or
+					 * need_resched() true during async
+					 * compaction
+					 */
 };
 
 unsigned long
