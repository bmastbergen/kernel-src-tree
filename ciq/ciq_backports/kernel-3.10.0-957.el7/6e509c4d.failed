iio: __iio_update_buffers: Verify configuration before starting to apply it

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iio] __iio_update_buffers: Verify configuration before starting to apply it (Tony Camuso) [1559170]
Rebuild_FUZZ: 96.55%
commit-author Lars-Peter Clausen <lars@metafoo.de>
commit 6e509c4d91632b6f8f05f0bee3a20fd50ca2263b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6e509c4d.failed

Currently __iio_update_buffers() verifies whether the new configuration
will work in the middle of the update sequence. This means if the new
configuration is invalid we need to rollback the changes already made. This
patch moves the validation of the new configuration at the beginning of
__iio_update_buffers() and will not start to make any changes if the new
configuration is invalid.

	Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
	Signed-off-by: Jonathan Cameron <jic23@kernel.org>
(cherry picked from commit 6e509c4d91632b6f8f05f0bee3a20fd50ca2263b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iio/industrialio-buffer.c
diff --cc drivers/iio/industrialio-buffer.c
index ed6b8aa675ce,0b4fe63c529e..000000000000
--- a/drivers/iio/industrialio-buffer.c
+++ b/drivers/iio/industrialio-buffer.c
@@@ -386,11 -441,501 +386,443 @@@ error_ret
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t iio_buffer_read_length(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 
+ 	return sprintf(buf, "%d\n", buffer->length);
+ }
+ 
+ static ssize_t iio_buffer_write_length(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf, size_t len)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 	unsigned int val;
+ 	int ret;
+ 
+ 	ret = kstrtouint(buf, 10, &val);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (val == buffer->length)
+ 		return len;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 	if (iio_buffer_is_active(indio_dev->buffer)) {
+ 		ret = -EBUSY;
+ 	} else {
+ 		buffer->access->set_length(buffer, val);
+ 		ret = 0;
+ 	}
+ 	if (ret)
+ 		goto out;
+ 	if (buffer->length && buffer->length < buffer->watermark)
+ 		buffer->watermark = buffer->length;
+ out:
+ 	mutex_unlock(&indio_dev->mlock);
+ 
+ 	return ret ? ret : len;
+ }
+ 
+ static ssize_t iio_buffer_show_enable(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	return sprintf(buf, "%d\n", iio_buffer_is_active(indio_dev->buffer));
+ }
+ 
+ static int iio_compute_scan_bytes(struct iio_dev *indio_dev,
+ 				const unsigned long *mask, bool timestamp)
+ {
+ 	const struct iio_chan_spec *ch;
+ 	unsigned bytes = 0;
+ 	int length, i;
+ 
+ 	/* How much space will the demuxed element take? */
+ 	for_each_set_bit(i, mask,
+ 			 indio_dev->masklength) {
+ 		ch = iio_find_channel_from_si(indio_dev, i);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	if (timestamp) {
+ 		ch = iio_find_channel_from_si(indio_dev,
+ 					      indio_dev->scan_index_timestamp);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	return bytes;
+ }
+ 
+ static void iio_buffer_activate(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	iio_buffer_get(buffer);
+ 	list_add(&buffer->buffer_list, &indio_dev->buffer_list);
+ }
+ 
+ static void iio_buffer_deactivate(struct iio_buffer *buffer)
+ {
+ 	list_del_init(&buffer->buffer_list);
+ 	wake_up_interruptible(&buffer->pollq);
+ 	iio_buffer_put(buffer);
+ }
+ 
+ void iio_disable_all_buffers(struct iio_dev *indio_dev)
+ {
+ 	struct iio_buffer *buffer, *_buffer;
+ 
+ 	if (list_empty(&indio_dev->buffer_list))
+ 		return;
+ 
+ 	if (indio_dev->setup_ops->predisable)
+ 		indio_dev->setup_ops->predisable(indio_dev);
+ 
+ 	list_for_each_entry_safe(buffer, _buffer,
+ 			&indio_dev->buffer_list, buffer_list)
+ 		iio_buffer_deactivate(buffer);
+ 
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 	if (indio_dev->setup_ops->postdisable)
+ 		indio_dev->setup_ops->postdisable(indio_dev);
+ 
+ 	if (indio_dev->available_scan_masks == NULL)
+ 		kfree(indio_dev->active_scan_mask);
+ }
+ 
+ static void iio_buffer_update_bytes_per_datum(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	unsigned int bytes;
+ 
+ 	if (!buffer->access->set_bytes_per_datum)
+ 		return;
+ 
+ 	bytes = iio_compute_scan_bytes(indio_dev, buffer->scan_mask,
+ 		buffer->scan_timestamp);
+ 
+ 	buffer->access->set_bytes_per_datum(buffer, bytes);
+ }
+ 
+ static int iio_buffer_request_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	int ret;
+ 
+ 	iio_buffer_update_bytes_per_datum(indio_dev, buffer);
+ 	if (buffer->access->request_update) {
+ 		ret = buffer->access->request_update(buffer);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer parameter update failed (%d)\n",
+ 				ret);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void iio_free_scan_mask(struct iio_dev *indio_dev,
+ 	const unsigned long *mask)
+ {
+ 	/* If the mask is dynamically allocated free it, otherwise do nothing */
+ 	if (!indio_dev->available_scan_masks)
+ 		kfree(mask);
+ }
+ 
+ struct iio_device_config {
+ 	unsigned int mode;
+ 	const unsigned long *scan_mask;
+ 	unsigned int scan_bytes;
+ 	bool scan_timestamp;
+ };
+ 
+ static int iio_verify_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *insert_buffer, struct iio_buffer *remove_buffer,
+ 	struct iio_device_config *config)
+ {
+ 	unsigned long *compound_mask;
+ 	const unsigned long *scan_mask;
+ 	struct iio_buffer *buffer;
+ 	bool scan_timestamp;
+ 
+ 	memset(config, 0, sizeof(*config));
+ 
+ 	/*
+ 	 * If there is just one buffer and we are removing it there is nothing
+ 	 * to verify.
+ 	 */
+ 	if (remove_buffer && !insert_buffer &&
+ 		list_is_singular(&indio_dev->buffer_list))
+ 			return 0;
+ 
+ 	/* Definitely possible for devices to support both of these. */
+ 	if ((indio_dev->modes & INDIO_BUFFER_TRIGGERED) && indio_dev->trig) {
+ 		config->mode = INDIO_BUFFER_TRIGGERED;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_HARDWARE) {
+ 		config->mode = INDIO_BUFFER_HARDWARE;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_SOFTWARE) {
+ 		config->mode = INDIO_BUFFER_SOFTWARE;
+ 	} else {
+ 		/* Can only occur on first buffer */
+ 		if (indio_dev->modes & INDIO_BUFFER_TRIGGERED)
+ 			dev_dbg(&indio_dev->dev, "Buffer not started: no trigger\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* What scan mask do we actually have? */
+ 	compound_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),
+ 				sizeof(long), GFP_KERNEL);
+ 	if (compound_mask == NULL)
+ 		return -ENOMEM;
+ 
+ 	scan_timestamp = false;
+ 
+ 	list_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {
+ 		if (buffer == remove_buffer)
+ 			continue;
+ 		bitmap_or(compound_mask, compound_mask, buffer->scan_mask,
+ 			  indio_dev->masklength);
+ 		scan_timestamp |= buffer->scan_timestamp;
+ 	}
+ 
+ 	if (insert_buffer) {
+ 		bitmap_or(compound_mask, compound_mask,
+ 			  insert_buffer->scan_mask, indio_dev->masklength);
+ 		scan_timestamp |= insert_buffer->scan_timestamp;
+ 	}
+ 
+ 	if (indio_dev->available_scan_masks) {
+ 		scan_mask = iio_scan_mask_match(indio_dev->available_scan_masks,
+ 				    indio_dev->masklength,
+ 				    compound_mask);
+ 		kfree(compound_mask);
+ 		if (scan_mask == NULL)
+ 			return -EINVAL;
+ 	} else {
+ 	    scan_mask = compound_mask;
+ 	}
+ 
+ 	config->scan_bytes = iio_compute_scan_bytes(indio_dev,
+ 				    scan_mask, scan_timestamp);
+ 	config->scan_mask = scan_mask;
+ 	config->scan_timestamp = scan_timestamp;
+ 
+ 	return 0;
+ }
+ 
+ static int __iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 	const unsigned long *old_mask;
+ 	struct iio_device_config new_config;
+ 
+ 	ret = iio_verify_update(indio_dev, insert_buffer, remove_buffer,
+ 		&new_config);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (insert_buffer) {
+ 		ret = iio_buffer_request_update(indio_dev, insert_buffer);
+ 		if (ret)
+ 			goto err_free_config;
+ 	}
+ 
+ 	/* Wind down existing buffers - iff there are any */
+ 	if (!list_empty(&indio_dev->buffer_list)) {
+ 		if (indio_dev->setup_ops->predisable) {
+ 			ret = indio_dev->setup_ops->predisable(indio_dev);
+ 			if (ret)
+ 				goto err_free_config;
+ 		}
+ 		indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 		if (indio_dev->setup_ops->postdisable) {
+ 			ret = indio_dev->setup_ops->postdisable(indio_dev);
+ 			if (ret)
+ 				goto err_free_config;
+ 		}
+ 	}
+ 	/* Keep a copy of current setup to allow roll back */
+ 	old_mask = indio_dev->active_scan_mask;
+ 	if (!indio_dev->available_scan_masks)
+ 		indio_dev->active_scan_mask = NULL;
+ 
+ 	if (remove_buffer)
+ 		iio_buffer_deactivate(remove_buffer);
+ 	if (insert_buffer)
+ 		iio_buffer_activate(indio_dev, insert_buffer);
+ 
+ 	/* If no buffers in list, we are done */
+ 	if (list_empty(&indio_dev->buffer_list)) {
+ 		indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 		iio_free_scan_mask(indio_dev, old_mask);
+ 		return 0;
+ 	}
+ 
+ 	indio_dev->active_scan_mask = new_config.scan_mask;
+ 	indio_dev->scan_timestamp = new_config.scan_timestamp;
+ 	indio_dev->scan_bytes = new_config.scan_bytes;
+ 
+ 	iio_update_demux(indio_dev);
+ 
+ 	/* Wind up again */
+ 	if (indio_dev->setup_ops->preenable) {
+ 		ret = indio_dev->setup_ops->preenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer preenable failed (%d)\n", ret);
+ 			goto error_remove_inserted;
+ 		}
+ 	}
+ 
+ 	if (indio_dev->info->update_scan_mode) {
+ 		ret = indio_dev->info
+ 			->update_scan_mode(indio_dev,
+ 					   indio_dev->active_scan_mask);
+ 		if (ret < 0) {
+ 			dev_dbg(&indio_dev->dev,
+ 				"Buffer not started: update scan mode failed (%d)\n",
+ 				ret);
+ 			goto error_run_postdisable;
+ 		}
+ 	}
+ 
+ 	indio_dev->currentmode = new_config.mode;
+ 
+ 	if (indio_dev->setup_ops->postenable) {
+ 		ret = indio_dev->setup_ops->postenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: postenable failed (%d)\n", ret);
+ 			indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 			if (indio_dev->setup_ops->postdisable)
+ 				indio_dev->setup_ops->postdisable(indio_dev);
+ 			goto error_disable_all_buffers;
+ 		}
+ 	}
+ 
+ 	iio_free_scan_mask(indio_dev, old_mask);
+ 
+ 	return 0;
+ 
+ error_disable_all_buffers:
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ error_run_postdisable:
+ 	if (indio_dev->setup_ops->postdisable)
+ 		indio_dev->setup_ops->postdisable(indio_dev);
+ error_remove_inserted:
+ 	if (insert_buffer)
+ 		iio_buffer_deactivate(insert_buffer);
+ 	iio_free_scan_mask(indio_dev, indio_dev->active_scan_mask);
+ 	indio_dev->active_scan_mask = old_mask;
+ 	return ret;
+ 
+ err_free_config:
+ 	iio_free_scan_mask(indio_dev, new_config.scan_mask);
+ 	return ret;
+ }
+ 
+ int iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 
+ 	if (insert_buffer == remove_buffer)
+ 		return 0;
+ 
+ 	mutex_lock(&indio_dev->info_exist_lock);
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	if (insert_buffer && iio_buffer_is_active(insert_buffer))
+ 		insert_buffer = NULL;
+ 
+ 	if (remove_buffer && !iio_buffer_is_active(remove_buffer))
+ 		remove_buffer = NULL;
+ 
+ 	if (!insert_buffer && !remove_buffer) {
+ 		ret = 0;
+ 		goto out_unlock;
+ 	}
+ 
+ 	if (indio_dev->info == NULL) {
+ 		ret = -ENODEV;
+ 		goto out_unlock;
+ 	}
+ 
+ 	ret = __iio_update_buffers(indio_dev, insert_buffer, remove_buffer);
+ 
+ out_unlock:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	mutex_unlock(&indio_dev->info_exist_lock);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(iio_update_buffers);
+ 
+ static ssize_t iio_buffer_store_enable(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf,
+ 				       size_t len)
+ {
+ 	int ret;
+ 	bool requested_state;
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	bool inlist;
+ 
+ 	ret = strtobool(buf, &requested_state);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	/* Find out if it is in the list */
+ 	inlist = iio_buffer_is_active(indio_dev->buffer);
+ 	/* Already in desired state */
+ 	if (inlist == requested_state)
+ 		goto done;
+ 
+ 	if (requested_state)
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 indio_dev->buffer, NULL);
+ 	else
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 NULL, indio_dev->buffer);
+ 
+ 	if (ret < 0)
+ 		goto done;
+ done:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	return (ret < 0) ? ret : len;
+ }
+ 
++>>>>>>> 6e509c4d9163 (iio: __iio_update_buffers: Verify configuration before starting to apply it)
  static const char * const iio_scan_elements_group_name = "scan_elements";
  
 -static ssize_t iio_buffer_show_watermark(struct device *dev,
 -					 struct device_attribute *attr,
 -					 char *buf)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -
 -	return sprintf(buf, "%u\n", buffer->watermark);
 -}
 -
 -static ssize_t iio_buffer_store_watermark(struct device *dev,
 -					  struct device_attribute *attr,
 -					  const char *buf,
 -					  size_t len)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -	unsigned int val;
 -	int ret;
 -
 -	ret = kstrtouint(buf, 10, &val);
 -	if (ret)
 -		return ret;
 -	if (!val)
 -		return -EINVAL;
 -
 -	mutex_lock(&indio_dev->mlock);
 -
 -	if (val > buffer->length) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	if (iio_buffer_is_active(indio_dev->buffer)) {
 -		ret = -EBUSY;
 -		goto out;
 -	}
 -
 -	buffer->watermark = val;
 -
 -	if (indio_dev->info->hwfifo_set_watermark)
 -		indio_dev->info->hwfifo_set_watermark(indio_dev, val);
 -out:
 -	mutex_unlock(&indio_dev->mlock);
 -
 -	return ret ? ret : len;
 -}
 -
 -static DEVICE_ATTR(length, S_IRUGO | S_IWUSR, iio_buffer_read_length,
 -		   iio_buffer_write_length);
 -static struct device_attribute dev_attr_length_ro = __ATTR(length,
 -	S_IRUGO, iio_buffer_read_length, NULL);
 -static DEVICE_ATTR(enable, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_enable, iio_buffer_store_enable);
 -static DEVICE_ATTR(watermark, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_watermark, iio_buffer_store_watermark);
 -
 -static struct attribute *iio_buffer_attrs[] = {
 -	&dev_attr_length.attr,
 -	&dev_attr_enable.attr,
 -	&dev_attr_watermark.attr,
 -};
 -
 -int iio_buffer_alloc_sysfs_and_mask(struct iio_dev *indio_dev)
 +int iio_buffer_register(struct iio_dev *indio_dev,
 +			const struct iio_chan_spec *channels,
 +			int num_channels)
  {
  	struct iio_dev_attr *p;
  	struct attribute **attr;
* Unmerged path drivers/iio/industrialio-buffer.c
