bpf: allow for correlation of maps and helpers in dump

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 7105e828c087de970fcb5a9509db51bfe6bd7894
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/7105e828.failed

Currently a dump of an xlated prog (post verifier stage) doesn't
correlate used helpers as well as maps. The prog info lists
involved map ids, however there's no correlation of where in the
program they are used as of today. Likewise, bpftool does not
correlate helper calls with the target functions.

The latter can be done w/o any kernel changes through kallsyms,
and also has the advantage that this works with inlined helpers
and BPF calls.

Example, via interpreter:

  # tc filter show dev foo ingress
  filter protocol all pref 49152 bpf chain 0
  filter protocol all pref 49152 bpf chain 0 handle 0x1 foo.o:[ingress] \
                      direct-action not_in_hw id 1 tag c74773051b364165   <-- prog id:1

  * Output before patch (calls/maps remain unclear):

  # bpftool prog dump xlated id 1             <-- dump prog id:1
   0: (b7) r1 = 2
   1: (63) *(u32 *)(r10 -4) = r1
   2: (bf) r2 = r10
   3: (07) r2 += -4
   4: (18) r1 = 0xffff95c47a8d4800
   6: (85) call unknown#73040
   7: (15) if r0 == 0x0 goto pc+18
   8: (bf) r2 = r10
   9: (07) r2 += -4
  10: (bf) r1 = r0
  11: (85) call unknown#73040
  12: (15) if r0 == 0x0 goto pc+23
  [...]

  * Output after patch:

  # bpftool prog dump xlated id 1
   0: (b7) r1 = 2
   1: (63) *(u32 *)(r10 -4) = r1
   2: (bf) r2 = r10
   3: (07) r2 += -4
   4: (18) r1 = map[id:2]                     <-- map id:2
   6: (85) call bpf_map_lookup_elem#73424     <-- helper call
   7: (15) if r0 == 0x0 goto pc+18
   8: (bf) r2 = r10
   9: (07) r2 += -4
  10: (bf) r1 = r0
  11: (85) call bpf_map_lookup_elem#73424
  12: (15) if r0 == 0x0 goto pc+23
  [...]

  # bpftool map show id 2                     <-- show/dump/etc map id:2
  2: hash_of_maps  flags 0x0
        key 4B  value 4B  max_entries 3  memlock 4096B

Example, JITed, same prog:

  # tc filter show dev foo ingress
  filter protocol all pref 49152 bpf chain 0
  filter protocol all pref 49152 bpf chain 0 handle 0x1 foo.o:[ingress] \
                  direct-action not_in_hw id 3 tag c74773051b364165 jited

  # bpftool prog show id 3
  3: sched_cls  tag c74773051b364165
        loaded_at Dec 19/13:48  uid 0
        xlated 384B  jited 257B  memlock 4096B  map_ids 2

  # bpftool prog dump xlated id 3
   0: (b7) r1 = 2
   1: (63) *(u32 *)(r10 -4) = r1
   2: (bf) r2 = r10
   3: (07) r2 += -4
   4: (18) r1 = map[id:2]                      <-- map id:2
   6: (85) call __htab_map_lookup_elem#77408   <-+ inlined rewrite
   7: (15) if r0 == 0x0 goto pc+2                |
   8: (07) r0 += 56                              |
   9: (79) r0 = *(u64 *)(r0 +0)                <-+
  10: (15) if r0 == 0x0 goto pc+24
  11: (bf) r2 = r10
  12: (07) r2 += -4
  [...]

Example, same prog, but kallsyms disabled (in that case we are
also not allowed to pass any relative offsets, etc, so prog
becomes pointer sanitized on dump):

  # sysctl kernel.kptr_restrict=2
  kernel.kptr_restrict = 2

  # bpftool prog dump xlated id 3
   0: (b7) r1 = 2
   1: (63) *(u32 *)(r10 -4) = r1
   2: (bf) r2 = r10
   3: (07) r2 += -4
   4: (18) r1 = map[id:2]
   6: (85) call bpf_unspec#0
   7: (15) if r0 == 0x0 goto pc+2
  [...]

Example, BPF calls via interpreter:

  # bpftool prog dump xlated id 1
   0: (85) call pc+2#__bpf_prog_run_args32
   1: (b7) r0 = 1
   2: (95) exit
   3: (b7) r0 = 2
   4: (95) exit

Example, BPF calls via JIT:

  # sysctl net.core.bpf_jit_enable=1
  net.core.bpf_jit_enable = 1
  # sysctl net.core.bpf_jit_kallsyms=1
  net.core.bpf_jit_kallsyms = 1

  # bpftool prog dump xlated id 1
   0: (85) call pc+2#bpf_prog_3b185187f1855c4c_F
   1: (b7) r0 = 1
   2: (95) exit
   3: (b7) r0 = 2
   4: (95) exit

And finally, an example for tail calls that is now working
as well wrt correlation:

  # bpftool prog dump xlated id 2
  [...]
  10: (b7) r2 = 8
  11: (85) call bpf_trace_printk#-41312
  12: (bf) r1 = r6
  13: (18) r2 = map[id:1]
  15: (b7) r3 = 0
  16: (85) call bpf_tail_call#12
  17: (b7) r1 = 42
  18: (6b) *(u16 *)(r6 +46) = r1
  19: (b7) r0 = 0
  20: (95) exit

  # bpftool map show id 1
  1: prog_array  flags 0x0
        key 4B  value 4B  max_entries 1  memlock 4096B

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit 7105e828c087de970fcb5a9509db51bfe6bd7894)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/filter.h
#	kernel/bpf/core.c
#	kernel/bpf/disasm.c
#	kernel/bpf/disasm.h
#	kernel/bpf/syscall.c
#	kernel/bpf/verifier.c
#	tools/bpf/bpftool/prog.c
diff --cc include/linux/filter.h
index d322ed880333,2b0df2703671..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -4,20 -5,444 +4,35 @@@
  #ifndef __LINUX_FILTER_H__
  #define __LINUX_FILTER_H__
  
 -#include <stdarg.h>
 -
  #include <linux/atomic.h>
 -#include <linux/refcount.h>
  #include <linux/compat.h>
++<<<<<<< HEAD
++=======
+ #include <linux/skbuff.h>
+ #include <linux/linkage.h>
+ #include <linux/printk.h>
+ #include <linux/workqueue.h>
+ #include <linux/sched.h>
+ #include <linux/capability.h>
+ #include <linux/cryptohash.h>
+ #include <linux/set_memory.h>
+ #include <linux/kallsyms.h>
+ 
+ #include <net/sch_generic.h>
+ 
++>>>>>>> 7105e828c087 (bpf: allow for correlation of maps and helpers in dump)
  #include <uapi/linux/filter.h>
 -#include <uapi/linux/bpf.h>
 -
 -struct sk_buff;
 -struct sock;
 -struct seccomp_data;
 -struct bpf_prog_aux;
 -
 -/* ArgX, context and stack frame pointer register positions. Note,
 - * Arg1, Arg2, Arg3, etc are used as argument mappings of function
 - * calls in BPF_CALL instruction.
 - */
 -#define BPF_REG_ARG1	BPF_REG_1
 -#define BPF_REG_ARG2	BPF_REG_2
 -#define BPF_REG_ARG3	BPF_REG_3
 -#define BPF_REG_ARG4	BPF_REG_4
 -#define BPF_REG_ARG5	BPF_REG_5
 -#define BPF_REG_CTX	BPF_REG_6
 -#define BPF_REG_FP	BPF_REG_10
 -
 -/* Additional register mappings for converted user programs. */
 -#define BPF_REG_A	BPF_REG_0
 -#define BPF_REG_X	BPF_REG_7
 -#define BPF_REG_TMP	BPF_REG_8
 -
 -/* Kernel hidden auxiliary/helper register for hardening step.
 - * Only used by eBPF JITs. It's nothing more than a temporary
 - * register that JITs use internally, only that here it's part
 - * of eBPF instructions that have been rewritten for blinding
 - * constants. See JIT pre-step in bpf_jit_blind_constants().
 - */
 -#define BPF_REG_AX		MAX_BPF_REG
 -#define MAX_BPF_JIT_REG		(MAX_BPF_REG + 1)
 -
 -/* unused opcode to mark special call to bpf_tail_call() helper */
 -#define BPF_TAIL_CALL	0xf0
 -
 -/* unused opcode to mark call to interpreter with arguments */
 -#define BPF_CALL_ARGS	0xe0
 -
 -/* As per nm, we expose JITed images as text (code) section for
 - * kallsyms. That way, tools like perf can find it to match
 - * addresses.
 - */
 -#define BPF_SYM_ELF_TYPE	't'
 -
 -/* BPF program can access up to 512 bytes of stack space. */
 -#define MAX_BPF_STACK	512
 -
 -/* Helper macros for filter block array initializers. */
 -
 -/* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
 -
 -#define BPF_ALU64_REG(OP, DST, SRC)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -#define BPF_ALU32_REG(OP, DST, SRC)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
 -
 -#define BPF_ALU64_IMM(OP, DST, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_ALU32_IMM(OP, DST, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
 -
 -#define BPF_ENDIAN(TYPE, DST, LEN)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = LEN })
 -
 -/* Short form of mov, dst_reg = src_reg */
 -
 -#define BPF_MOV64_REG(DST, SRC)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -#define BPF_MOV32_REG(DST, SRC)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* Short form of mov, dst_reg = imm32 */
 -
 -#define BPF_MOV64_IMM(DST, IMM)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_MOV32_IMM(DST, IMM)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
 -#define BPF_LD_IMM64(DST, IMM)					\
 -	BPF_LD_IMM64_RAW(DST, 0, IMM)
 -
 -#define BPF_LD_IMM64_RAW(DST, SRC, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LD | BPF_DW | BPF_IMM,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = (__u32) (IMM) }),			\
 -	((struct bpf_insn) {					\
 -		.code  = 0, /* zero is reserved opcode */	\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = ((__u64) (IMM)) >> 32 })
 -
 -/* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */
 -#define BPF_LD_MAP_FD(DST, MAP_FD)				\
 -	BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)
 -
 -/* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
 -
 -#define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
 -
 -#define BPF_LD_ABS(SIZE, IMM)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
 -
 -#define BPF_LD_IND(SIZE, SRC, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
 -		.dst_reg = 0,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Memory load, dst_reg = *(uint *) (src_reg + off16) */
 -
 -#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Memory store, *(uint *) (dst_reg + off16) = src_reg */
 -
 -#define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
 -
 -#define BPF_STX_XADD(SIZE, DST, SRC, OFF)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_XADD,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Memory store, *(uint *) (dst_reg + off16) = imm32 */
 -
 -#define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
 -
 -#define BPF_JMP_REG(OP, DST, SRC, OFF)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
 -
 -#define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Unconditional jumps, goto pc + off16 */
 -
 -#define BPF_JMP_A(OFF)						\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_JA,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Function call */
 -
 -#define BPF_EMIT_CALL(FUNC)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_CALL,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = ((FUNC) - __bpf_call_base) })
 -
 -/* Raw code statement block */
 -
 -#define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
 -	((struct bpf_insn) {					\
 -		.code  = CODE,					\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Program exit */
 -
 -#define BPF_EXIT_INSN()						\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_EXIT,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* Internal classic blocks for direct assignment */
 -
 -#define __BPF_STMT(CODE, K)					\
 -	((struct sock_filter) BPF_STMT(CODE, K))
 -
 -#define __BPF_JUMP(CODE, K, JT, JF)				\
 -	((struct sock_filter) BPF_JUMP(CODE, K, JT, JF))
 -
 -#define bytes_to_bpf_size(bytes)				\
 -({								\
 -	int bpf_size = -EINVAL;					\
 -								\
 -	if (bytes == sizeof(u8))				\
 -		bpf_size = BPF_B;				\
 -	else if (bytes == sizeof(u16))				\
 -		bpf_size = BPF_H;				\
 -	else if (bytes == sizeof(u32))				\
 -		bpf_size = BPF_W;				\
 -	else if (bytes == sizeof(u64))				\
 -		bpf_size = BPF_DW;				\
 -								\
 -	bpf_size;						\
 -})
 -
 -#define bpf_size_to_bytes(bpf_size)				\
 -({								\
 -	int bytes = -EINVAL;					\
 -								\
 -	if (bpf_size == BPF_B)					\
 -		bytes = sizeof(u8);				\
 -	else if (bpf_size == BPF_H)				\
 -		bytes = sizeof(u16);				\
 -	else if (bpf_size == BPF_W)				\
 -		bytes = sizeof(u32);				\
 -	else if (bpf_size == BPF_DW)				\
 -		bytes = sizeof(u64);				\
 -								\
 -	bytes;							\
 -})
 -
 -#define BPF_SIZEOF(type)					\
 -	({							\
 -		const int __size = bytes_to_bpf_size(sizeof(type)); \
 -		BUILD_BUG_ON(__size < 0);			\
 -		__size;						\
 -	})
 -
 -#define BPF_FIELD_SIZEOF(type, field)				\
 -	({							\
 -		const int __size = bytes_to_bpf_size(FIELD_SIZEOF(type, field)); \
 -		BUILD_BUG_ON(__size < 0);			\
 -		__size;						\
 -	})
 -
 -#define BPF_LDST_BYTES(insn)					\
 -	({							\
 -		const int __size = bpf_size_to_bytes(BPF_SIZE(insn->code)); \
 -		WARN_ON(__size < 0);				\
 -		__size;						\
 -	})
 -
 -#define __BPF_MAP_0(m, v, ...) v
 -#define __BPF_MAP_1(m, v, t, a, ...) m(t, a)
 -#define __BPF_MAP_2(m, v, t, a, ...) m(t, a), __BPF_MAP_1(m, v, __VA_ARGS__)
 -#define __BPF_MAP_3(m, v, t, a, ...) m(t, a), __BPF_MAP_2(m, v, __VA_ARGS__)
 -#define __BPF_MAP_4(m, v, t, a, ...) m(t, a), __BPF_MAP_3(m, v, __VA_ARGS__)
 -#define __BPF_MAP_5(m, v, t, a, ...) m(t, a), __BPF_MAP_4(m, v, __VA_ARGS__)
 -
 -#define __BPF_REG_0(...) __BPF_PAD(5)
 -#define __BPF_REG_1(...) __BPF_MAP(1, __VA_ARGS__), __BPF_PAD(4)
 -#define __BPF_REG_2(...) __BPF_MAP(2, __VA_ARGS__), __BPF_PAD(3)
 -#define __BPF_REG_3(...) __BPF_MAP(3, __VA_ARGS__), __BPF_PAD(2)
 -#define __BPF_REG_4(...) __BPF_MAP(4, __VA_ARGS__), __BPF_PAD(1)
 -#define __BPF_REG_5(...) __BPF_MAP(5, __VA_ARGS__)
 -
 -#define __BPF_MAP(n, ...) __BPF_MAP_##n(__VA_ARGS__)
 -#define __BPF_REG(n, ...) __BPF_REG_##n(__VA_ARGS__)
 -
 -#define __BPF_CAST(t, a)						       \
 -	(__force t)							       \
 -	(__force							       \
 -	 typeof(__builtin_choose_expr(sizeof(t) == sizeof(unsigned long),      \
 -				      (unsigned long)0, (t)0))) a
 -#define __BPF_V void
 -#define __BPF_N
 -
 -#define __BPF_DECL_ARGS(t, a) t   a
 -#define __BPF_DECL_REGS(t, a) u64 a
 -
 -#define __BPF_PAD(n)							       \
 -	__BPF_MAP(n, __BPF_DECL_ARGS, __BPF_N, u64, __ur_1, u64, __ur_2,       \
 -		  u64, __ur_3, u64, __ur_4, u64, __ur_5)
 -
 -#define BPF_CALL_x(x, name, ...)					       \
 -	static __always_inline						       \
 -	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__));   \
 -	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__));	       \
 -	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__))	       \
 -	{								       \
 -		return ____##name(__BPF_MAP(x,__BPF_CAST,__BPF_N,__VA_ARGS__));\
 -	}								       \
 -	static __always_inline						       \
 -	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__))
 -
 -#define BPF_CALL_0(name, ...)	BPF_CALL_x(0, name, __VA_ARGS__)
 -#define BPF_CALL_1(name, ...)	BPF_CALL_x(1, name, __VA_ARGS__)
 -#define BPF_CALL_2(name, ...)	BPF_CALL_x(2, name, __VA_ARGS__)
 -#define BPF_CALL_3(name, ...)	BPF_CALL_x(3, name, __VA_ARGS__)
 -#define BPF_CALL_4(name, ...)	BPF_CALL_x(4, name, __VA_ARGS__)
 -#define BPF_CALL_5(name, ...)	BPF_CALL_x(5, name, __VA_ARGS__)
 -
 -#define bpf_ctx_range(TYPE, MEMBER)						\
 -	offsetof(TYPE, MEMBER) ... offsetofend(TYPE, MEMBER) - 1
 -#define bpf_ctx_range_till(TYPE, MEMBER1, MEMBER2)				\
 -	offsetof(TYPE, MEMBER1) ... offsetofend(TYPE, MEMBER2) - 1
 -
 -#define bpf_target_off(TYPE, MEMBER, SIZE, PTR_SIZE)				\
 -	({									\
 -		BUILD_BUG_ON(FIELD_SIZEOF(TYPE, MEMBER) != (SIZE));		\
 -		*(PTR_SIZE) = (SIZE);						\
 -		offsetof(TYPE, MEMBER);						\
 -	})
 +#ifndef __GENKSYMS__
 +#include <net/sch_generic.h>
 +#endif
  
  #ifdef CONFIG_COMPAT
 -/* A struct sock_filter is architecture independent. */
 +/*
 + * A struct sock_filter is architecture independent.
 + */
  struct compat_sock_fprog {
  	u16		len;
 -	compat_uptr_t	filter;	/* struct sock_filter * */
 +	compat_uptr_t	filter;		/* struct sock_filter * */
  };
  #endif
  
@@@ -65,35 -683,108 +80,95 @@@ static inline int sk_filter(struct soc
  	return sk_filter_trim_cap(sk, skb, 1);
  }
  
 -struct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err);
 -void bpf_prog_free(struct bpf_prog *fp);
 -
 -struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
 -struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 -				  gfp_t gfp_extra_flags);
 -void __bpf_prog_free(struct bpf_prog *fp);
 +extern unsigned int sk_run_filter(const struct sk_buff *skb,
 +				  const struct sock_filter *filter);
 +extern int sk_unattached_filter_create(struct sk_filter **pfp,
 +				       struct sock_fprog *fprog);
 +extern void sk_unattached_filter_destroy(struct sk_filter *fp);
 +extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 +extern int sk_detach_filter(struct sock *sk);
 +extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 +extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 +extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
  
 -static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 +static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 +				   struct xdp_buff *xdp)
  {
 -	bpf_prog_unlock_ro(fp);
 -	__bpf_prog_free(fp);
 +	return 0;
  }
  
++<<<<<<< HEAD
 +static inline void bpf_warn_invalid_xdp_action(u32 act)
++=======
+ typedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,
+ 				       unsigned int flen);
+ 
+ int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
+ int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
+ 			      bpf_aux_classic_check_t trans, bool save_orig);
+ void bpf_prog_destroy(struct bpf_prog *fp);
+ 
+ int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+ int sk_attach_bpf(u32 ufd, struct sock *sk);
+ int sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+ int sk_reuseport_attach_bpf(u32 ufd, struct sock *sk);
+ int sk_detach_filter(struct sock *sk);
+ int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
+ 		  unsigned int len);
+ 
+ bool sk_filter_charge(struct sock *sk, struct sk_filter *fp);
+ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
+ 
+ u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ #define __bpf_call_base_args \
+ 	((u64 (*)(u64, u64, u64, u64, u64, const struct bpf_insn *)) \
+ 	 __bpf_call_base)
+ 
+ struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);
+ void bpf_jit_compile(struct bpf_prog *prog);
+ bool bpf_helper_changes_pkt_data(void *func);
+ 
+ static inline bool bpf_dump_raw_ok(void)
+ {
+ 	/* Reconstruction of call-sites is dependent on kallsyms,
+ 	 * thus make dump the same restriction.
+ 	 */
+ 	return kallsyms_show_value() == 1;
+ }
+ 
+ struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
+ 				       const struct bpf_insn *patch, u32 len);
+ 
+ /* The pair of xdp_do_redirect and xdp_do_flush_map MUST be called in the
+  * same cpu context. Further for best results no more than a single map
+  * for the do_redirect/do_flush pair should be used. This limitation is
+  * because we only track one map and force a flush when the map changes.
+  * This does not appear to be a real limitation for existing software.
+  */
+ int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,
+ 			    struct bpf_prog *prog);
+ int xdp_do_redirect(struct net_device *dev,
+ 		    struct xdp_buff *xdp,
+ 		    struct bpf_prog *prog);
+ void xdp_do_flush_map(void);
+ 
+ /* Drivers not supporting XDP metadata can use this helper, which
+  * rejects any room expansion for metadata as a result.
+  */
+ static __always_inline void
+ xdp_set_data_meta_invalid(struct xdp_buff *xdp)
++>>>>>>> 7105e828c087 (bpf: allow for correlation of maps and helpers in dump)
  {
 -	xdp->data_meta = xdp->data + 1;
 +	return;
  }
  
 -static __always_inline bool
 -xdp_data_meta_unsupported(const struct xdp_buff *xdp)
 -{
 -	return unlikely(xdp->data_meta > xdp->data);
 -}
 -
 -void bpf_warn_invalid_xdp_action(u32 act);
 -
 -struct sock *do_sk_redirect_map(struct sk_buff *skb);
 -
  #ifdef CONFIG_BPF_JIT
 -extern int bpf_jit_enable;
 -extern int bpf_jit_harden;
 -extern int bpf_jit_kallsyms;
 -
 -typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
 -
 -struct bpf_binary_header *
 -bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
 -		     unsigned int alignment,
 -		     bpf_jit_fill_hole_t bpf_fill_ill_insns);
 -void bpf_jit_binary_free(struct bpf_binary_header *hdr);
 -
 -void bpf_jit_free(struct bpf_prog *fp);
 +#include <stdarg.h>
 +#include <linux/linkage.h>
 +#include <linux/printk.h>
  
 -struct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *fp);
 -void bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other);
 +extern void bpf_jit_compile(struct sk_filter *fp);
 +extern void bpf_jit_free(struct sk_filter *fp);
  
  static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
  				u32 pass, void *image)
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/disasm.c
* Unmerged path kernel/bpf/disasm.h
* Unmerged path kernel/bpf/syscall.c
* Unmerged path kernel/bpf/verifier.c
* Unmerged path tools/bpf/bpftool/prog.c
* Unmerged path include/linux/filter.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/disasm.c
* Unmerged path kernel/bpf/disasm.h
* Unmerged path kernel/bpf/syscall.c
* Unmerged path kernel/bpf/verifier.c
* Unmerged path tools/bpf/bpftool/prog.c
