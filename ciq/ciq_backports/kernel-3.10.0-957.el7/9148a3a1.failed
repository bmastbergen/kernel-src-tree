sched/debug: Add SCHED_WARN_ON()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 9148a3a10e0b74c5722174a0bbef16d821f8a48b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9148a3a1.failed

Provide SCHED_WARN_ON as wrapper for WARN_ON_ONCE() to avoid
CONFIG_SCHED_DEBUG wrappery.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 9148a3a10e0b74c5722174a0bbef16d821f8a48b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/sched.h
diff --cc kernel/sched/sched.h
index 98cec6922405,5489d07a4643..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -16,25 -15,32 +16,41 @@@
  #include "cpudeadline.h"
  #include "cpuacct.h"
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_SCHED_DEBUG
+ #define SCHED_WARN_ON(x)	WARN_ONCE(x, #x)
+ #else
+ #define SCHED_WARN_ON(x)	((void)(x))
+ #endif
+ 
+ struct rq;
+ struct cpuidle_state;
+ 
+ /* task_struct::on_rq states: */
+ #define TASK_ON_RQ_QUEUED	1
+ #define TASK_ON_RQ_MIGRATING	2
+ 
++>>>>>>> 9148a3a10e0b (sched/debug: Add SCHED_WARN_ON())
  extern __read_mostly int scheduler_running;
  
 -extern unsigned long calc_load_update;
 -extern atomic_long_t calc_load_tasks;
 -
 -extern void calc_global_load_tick(struct rq *this_rq);
 -extern long calc_load_fold_active(struct rq *this_rq, long adjust);
 +/*
 + * Convert user-nice values [ -20 ... 0 ... 19 ]
 + * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
 + * and back.
 + */
 +#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
 +#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
 +#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
  
 -#ifdef CONFIG_SMP
 -extern void cpu_load_update_active(struct rq *this_rq);
 -#else
 -static inline void cpu_load_update_active(struct rq *this_rq) { }
 -#endif
 +/*
 + * 'User priority' is the nice value converted to something we
 + * can work with better when scaling various scheduler parameters,
 + * it's a [ 0 ... 39 ] range.
 + */
 +#define USER_PRIO(p)		((p)-MAX_RT_PRIO)
 +#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
 +#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
  
  /*
   * Helpers for converting nanosecond timing to jiffy resolution
@@@ -1249,26 -1298,29 +1265,37 @@@ extern const struct sched_class idle_sc
  
  #ifdef CONFIG_SMP
  
 -extern void update_group_capacity(struct sched_domain *sd, int cpu);
 +extern void update_group_power(struct sched_domain *sd, int cpu);
  
 -extern void trigger_load_balance(struct rq *rq);
 -
 -extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask);
 +extern void trigger_load_balance(struct rq *rq, int cpu);
 +extern void idle_balance(int this_cpu, struct rq *this_rq);
  
 +/*
 + * Only depends on SMP, FAIR_GROUP_SCHED may be removed when runnable_avg
 + * becomes useful in lb
 + */
 +#if defined(CONFIG_FAIR_GROUP_SCHED)
 +extern void idle_enter_fair(struct rq *this_rq);
 +extern void idle_exit_fair(struct rq *this_rq);
 +#else
 +static inline void idle_enter_fair(struct rq *this_rq) {}
 +static inline void idle_exit_fair(struct rq *this_rq) {}
  #endif
  
 -#ifdef CONFIG_CPU_IDLE
 -static inline void idle_set_state(struct rq *rq,
 -				  struct cpuidle_state *idle_state)
 -{
 -	rq->idle_state = idle_state;
 -}
 +#else	/* CONFIG_SMP */
  
++<<<<<<< HEAD
 +static inline void idle_balance(int cpu, struct rq *rq)
++=======
+ static inline struct cpuidle_state *idle_get_state(struct rq *rq)
+ {
+ 	SCHED_WARN_ON(!rcu_read_lock_held());
+ 	return rq->idle_state;
+ }
+ #else
+ static inline void idle_set_state(struct rq *rq,
+ 				  struct cpuidle_state *idle_state)
++>>>>>>> 9148a3a10e0b (sched/debug: Add SCHED_WARN_ON())
  {
  }
  
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index a455c3157828..04f8eeedfd22 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -248,9 +248,7 @@ static inline struct rq *rq_of(struct cfs_rq *cfs_rq)
 
 static inline struct task_struct *task_of(struct sched_entity *se)
 {
-#ifdef CONFIG_SCHED_DEBUG
-	WARN_ON_ONCE(!entity_is_task(se));
-#endif
+	SCHED_WARN_ON(!entity_is_task(se));
 	return container_of(se, struct task_struct, se);
 }
 
@@ -1879,7 +1877,7 @@ void task_numa_work(struct callback_head *work)
 	unsigned long nr_pte_updates = 0;
 	long pages, virtpages;
 
-	WARN_ON_ONCE(p != container_of(work, struct task_struct, numa_work));
+	SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
 
 	work->next = work; /* protect against double add */
 	/*
@@ -3887,7 +3885,7 @@ static void hrtick_start_fair(struct rq *rq, struct task_struct *p)
 	struct sched_entity *se = &p->se;
 	struct cfs_rq *cfs_rq = cfs_rq_of(se);
 
-	WARN_ON(task_rq(p) != rq);
+	SCHED_WARN_ON(task_rq(p) != rq);
 
 	if (cfs_rq->nr_running > 1) {
 		u64 slice = sched_slice(cfs_rq, se);
* Unmerged path kernel/sched/sched.h
