memremap: split devm_memremap_pages() and memremap() infrastructure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 5981690ddb8f72f9546a2d017a914cf56095fc1f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/5981690d.failed

Currently, kernel/memremap.c contains generic code for supporting
memremap() (CONFIG_HAS_IOMEM) and devm_memremap_pages()
(CONFIG_ZONE_DEVICE). This causes ongoing build maintenance problems as
additions to memremap.c, especially for the ZONE_DEVICE case, need to be
careful about being placed in ifdef guards. Remove the need for these
ifdef guards by moving the ZONE_DEVICE support functions to their own
compilation unit.

	Cc: "Jérôme Glisse" <jglisse@redhat.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 5981690ddb8f72f9546a2d017a914cf56095fc1f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/memremap.c
diff --cc kernel/memremap.c
index eca98ec515d8,37a9604133f6..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -1,17 -1,6 +1,7 @@@
- /*
-  * Copyright(c) 2015 Intel Corporation. All rights reserved.
-  *
-  * This program is free software; you can redistribute it and/or modify
-  * it under the terms of version 2 of the GNU General Public License as
-  * published by the Free Software Foundation.
-  *
-  * This program is distributed in the hope that it will be useful, but
-  * WITHOUT ANY WARRANTY; without even the implied warranty of
-  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-  * General Public License for more details.
-  */
+ /* SPDX-License-Identifier: GPL-2.0 */
+ /* Copyright(c) 2015 Intel Corporation. All rights reserved. */
  #include <linux/radix-tree.h>
 +#include <linux/memremap.h>
  #include <linux/device.h>
  #include <linux/types.h>
  #include <linux/pfn_t.h>
@@@ -21,168 -10,6 +11,171 @@@
  #include <linux/swap.h>
  #include <linux/swapops.h>
  
++<<<<<<< HEAD
 +#ifndef ioremap_cache
 +/* temporary while we convert existing ioremap_cache users to memremap */
 +__weak void __iomem *ioremap_cache(resource_size_t offset, unsigned long size)
 +{
 +	return ioremap(offset, size);
 +}
 +#endif
 +
 +#ifndef arch_memremap_wb
 +static void *arch_memremap_wb(resource_size_t offset, unsigned long size)
 +{
 +	return (__force void *)ioremap_cache(offset, size);
 +}
 +#endif
 +
 +#ifndef arch_memremap_can_ram_remap
 +static bool arch_memremap_can_ram_remap(resource_size_t offset, size_t size,
 +					unsigned long flags)
 +{
 +	return true;
 +}
 +#endif
 +
 +static void *try_ram_remap(resource_size_t offset, size_t size,
 +			   unsigned long flags)
 +{
 +	unsigned long pfn = PHYS_PFN(offset);
 +
 +	/* In the simple case just return the existing linear address */
 +	if (pfn_valid(pfn) && !PageHighMem(pfn_to_page(pfn)) &&
 +	    arch_memremap_can_ram_remap(offset, size, flags))
 +		return __va(offset);
 +
 +	return NULL; /* fallback to arch_memremap_wb */
 +}
 +
 +/**
 + * memremap() - remap an iomem_resource as cacheable memory
 + * @offset: iomem resource start address
 + * @size: size of remap
 + * @flags: any of MEMREMAP_WB, MEMREMAP_WT, MEMREMAP_WC,
 + *		  MEMREMAP_ENC, MEMREMAP_DEC
 + *
 + * memremap() is "ioremap" for cases where it is known that the resource
 + * being mapped does not have i/o side effects and the __iomem
 + * annotation is not applicable. In the case of multiple flags, the different
 + * mapping types will be attempted in the order listed below until one of
 + * them succeeds.
 + *
 + * MEMREMAP_WB - matches the default mapping for "System RAM" on
 + * the architecture.  This is usually a read-allocate write-back cache.
 + * Morever, if MEMREMAP_WB is specified and the requested remap region is RAM
 + * memremap() will bypass establishing a new mapping and instead return
 + * a pointer into the direct map.
 + *
 + * MEMREMAP_WT - establish a mapping whereby writes either bypass the
 + * cache or are written through to memory and never exist in a
 + * cache-dirty state with respect to program visibility.  Attempts to
 + * map "System RAM" with this mapping type will fail.
 + *
 + * MEMREMAP_WC - establish a writecombine mapping, whereby writes may
 + * be coalesced together (e.g. in the CPU's write buffers), but is otherwise
 + * uncached. Attempts to map System RAM with this mapping type will fail.
 + */
 +void *memremap(resource_size_t offset, size_t size, unsigned long flags)
 +{
 +	int is_ram = region_intersects_ram(offset, size);
 +	void *addr = NULL;
 +
 +	if (!flags)
 +		return NULL;
 +
 +	if (is_ram == REGION_MIXED) {
 +		WARN_ONCE(1, "memremap attempted on mixed range %pa size: %#lx\n",
 +				&offset, (unsigned long) size);
 +		return NULL;
 +	}
 +
 +	/* Try all mapping types requested until one returns non-NULL */
 +	if (flags & MEMREMAP_WB) {
 +		/*
 +		 * MEMREMAP_WB is special in that it can be satisifed
 +		 * from the direct map.  Some archs depend on the
 +		 * capability of memremap() to autodetect cases where
 +		 * the requested range is potentially in "System RAM"
 +		 */
 +		if (is_ram == REGION_INTERSECTS)
 +			addr = try_ram_remap(offset, size, flags);
 +		if (!addr)
 +			addr = arch_memremap_wb(offset, size);
 +	}
 +
 +	/*
 +	 * If we don't have a mapping yet and other request flags are
 +	 * present then we will be attempting to establish a new virtual
 +	 * address mapping.  Enforce that this mapping is not aliasing
 +	 * "System RAM"
 +	 */
 +	if (!addr && is_ram == REGION_INTERSECTS && flags != MEMREMAP_WB) {
 +		WARN_ONCE(1, "memremap attempted on ram %pa size: %#lx\n",
 +				&offset, (unsigned long) size);
 +		return NULL;
 +	}
 +
 +	if (!addr && (flags & MEMREMAP_WT))
 +		addr = ioremap_nocache(offset, size);
 +
 +	if (!addr && (flags & MEMREMAP_WC))
 +		addr = ioremap_wc(offset, size);
 +
 +	return addr;
 +}
 +EXPORT_SYMBOL(memremap);
 +
 +void memunmap(void *addr)
 +{
 +	if (is_vmalloc_addr(addr))
 +		iounmap((void __iomem *) addr);
 +}
 +EXPORT_SYMBOL(memunmap);
 +
 +static void devm_memremap_release(struct device *dev, void *res)
 +{
 +	memunmap(*(void **)res);
 +}
 +
 +static int devm_memremap_match(struct device *dev, void *res, void *match_data)
 +{
 +	return *(void **)res == match_data;
 +}
 +
 +void *devm_memremap(struct device *dev, resource_size_t offset,
 +		size_t size, unsigned long flags)
 +{
 +	void **ptr, *addr;
 +
 +	ptr = devres_alloc_node(devm_memremap_release, sizeof(*ptr), GFP_KERNEL,
 +			dev_to_node(dev));
 +	if (!ptr)
 +		return ERR_PTR(-ENOMEM);
 +
 +	addr = memremap(offset, size, flags);
 +	if (addr) {
 +		*ptr = addr;
 +		devres_add(dev, ptr);
 +	} else {
 +		devres_free(ptr);
 +		return ERR_PTR(-ENXIO);
 +	}
 +
 +	return addr;
 +}
 +EXPORT_SYMBOL(devm_memremap);
 +
 +void devm_memunmap(struct device *dev, void *addr)
 +{
 +	WARN_ON(devres_release(dev, devm_memremap_release,
 +				devm_memremap_match, addr));
 +}
 +EXPORT_SYMBOL(devm_memunmap);
 +
 +#ifdef CONFIG_ZONE_DEVICE
++=======
++>>>>>>> 5981690ddb8f (memremap: split devm_memremap_pages() and memremap() infrastructure)
  static DEFINE_MUTEX(pgmap_lock);
  static RADIX_TREE(pgmap_radix, GFP_KERNEL);
  #define SECTION_MASK ~((1UL << PA_SECTION_SHIFT) - 1)
@@@ -479,31 -269,58 +472,58 @@@ void vmem_altmap_free(struct vmem_altma
  	altmap->alloc -= nr_pfns;
  }
  
 -/**
 - * get_dev_pagemap() - take a new live reference on the dev_pagemap for @pfn
 - * @pfn: page frame number to lookup page_map
 - * @pgmap: optional known pgmap that already has a reference
 - *
 - * If @pgmap is non-NULL and covers @pfn it will be returned as-is.  If @pgmap
 - * is non-NULL but does not cover @pfn the reference to it will be released.
 - */
 -struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
 -		struct dev_pagemap *pgmap)
 +#ifdef CONFIG_SPARSEMEM_VMEMMAP
 +struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
  {
 -	resource_size_t phys = PFN_PHYS(pfn);
 -
  	/*
 -	 * In the cached case we're already holding a live reference.
 +	 * 'memmap_start' is the virtual address for the first "struct
 +	 * page" in this range of the vmemmap array.  In the case of
 +	 * CONFIG_SPARSE_VMEMMAP a page_to_pfn conversion is simple
 +	 * pointer arithmetic, so we can perform this to_vmem_altmap()
 +	 * conversion without concern for the initialization state of
 +	 * the struct page fields.
  	 */
 -	if (pgmap) {
 -		if (phys >= pgmap->res.start && phys <= pgmap->res.end)
 -			return pgmap;
 -		put_dev_pagemap(pgmap);
 -	}
 +	struct page *page = (struct page *) memmap_start;
 +	struct dev_pagemap *pgmap;
  
 -	/* fall back to slow path lookup */
 +	/*
 +	 * Uncoditionally retrieve a dev_pagemap associated with the
 +	 * given physical address, this is only for use in the
 +	 * arch_{add|remove}_memory() for setting up and tearing down
 +	 * the memmap.
 +	 */
  	rcu_read_lock();
 -	pgmap = radix_tree_lookup(&pgmap_radix, PHYS_PFN(phys));
 -	if (pgmap && !percpu_ref_tryget_live(pgmap->ref))
 -		pgmap = NULL;
 +	pgmap = find_dev_pagemap(__pfn_to_phys(page_to_pfn(page)));
  	rcu_read_unlock();
  
 -	return pgmap;
 +	return pgmap ? pgmap->altmap : NULL;
  }
++<<<<<<< HEAD
 +#endif /* CONFIG_SPARSEMEM_VMEMMAP */
 +#endif /* CONFIG_ZONE_DEVICE */
++=======
+ 
+ #if IS_ENABLED(CONFIG_DEVICE_PRIVATE) ||  IS_ENABLED(CONFIG_DEVICE_PUBLIC)
+ void put_zone_device_private_or_public_page(struct page *page)
+ {
+ 	int count = page_ref_dec_return(page);
+ 
+ 	/*
+ 	 * If refcount is 1 then page is freed and refcount is stable as nobody
+ 	 * holds a reference on the page.
+ 	 */
+ 	if (count == 1) {
+ 		/* Clear Active bit in case of parallel mark_page_accessed */
+ 		__ClearPageActive(page);
+ 		__ClearPageWaiters(page);
+ 
+ 		page->mapping = NULL;
+ 		mem_cgroup_uncharge(page);
+ 
+ 		page->pgmap->page_free(page, page->pgmap->data);
+ 	} else if (!count)
+ 		__put_page(page);
+ }
+ EXPORT_SYMBOL(put_zone_device_private_or_public_page);
+ #endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */
++>>>>>>> 5981690ddb8f (memremap: split devm_memremap_pages() and memremap() infrastructure)
diff --git a/kernel/Makefile b/kernel/Makefile
index 2fb90fa3b53b..8a06cd78b6ff 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -121,7 +121,8 @@ obj-$(CONFIG_CRASH_DUMP) += crash_dump.o
 obj-$(CONFIG_JUMP_LABEL) += jump_label.o
 obj-$(CONFIG_CONTEXT_TRACKING) += context_tracking.o
 
-obj-$(CONFIG_HAS_IOMEM) += memremap.o
+obj-$(CONFIG_HAS_IOMEM) += iomem.o
+obj-$(CONFIG_ZONE_DEVICE) += memremap.o
 
 $(obj)/configs.o: $(obj)/config_data.h
 
diff --git a/kernel/iomem.c b/kernel/iomem.c
new file mode 100644
index 000000000000..f7525e14ebc6
--- /dev/null
+++ b/kernel/iomem.c
@@ -0,0 +1,167 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#include <linux/device.h>
+#include <linux/types.h>
+#include <linux/io.h>
+#include <linux/mm.h>
+
+#ifndef ioremap_cache
+/* temporary while we convert existing ioremap_cache users to memremap */
+__weak void __iomem *ioremap_cache(resource_size_t offset, unsigned long size)
+{
+	return ioremap(offset, size);
+}
+#endif
+
+#ifndef arch_memremap_wb
+static void *arch_memremap_wb(resource_size_t offset, unsigned long size)
+{
+	return (__force void *)ioremap_cache(offset, size);
+}
+#endif
+
+#ifndef arch_memremap_can_ram_remap
+static bool arch_memremap_can_ram_remap(resource_size_t offset, size_t size,
+					unsigned long flags)
+{
+	return true;
+}
+#endif
+
+static void *try_ram_remap(resource_size_t offset, size_t size,
+			   unsigned long flags)
+{
+	unsigned long pfn = PHYS_PFN(offset);
+
+	/* In the simple case just return the existing linear address */
+	if (pfn_valid(pfn) && !PageHighMem(pfn_to_page(pfn)) &&
+	    arch_memremap_can_ram_remap(offset, size, flags))
+		return __va(offset);
+
+	return NULL; /* fallback to arch_memremap_wb */
+}
+
+/**
+ * memremap() - remap an iomem_resource as cacheable memory
+ * @offset: iomem resource start address
+ * @size: size of remap
+ * @flags: any of MEMREMAP_WB, MEMREMAP_WT, MEMREMAP_WC,
+ *		  MEMREMAP_ENC, MEMREMAP_DEC
+ *
+ * memremap() is "ioremap" for cases where it is known that the resource
+ * being mapped does not have i/o side effects and the __iomem
+ * annotation is not applicable. In the case of multiple flags, the different
+ * mapping types will be attempted in the order listed below until one of
+ * them succeeds.
+ *
+ * MEMREMAP_WB - matches the default mapping for System RAM on
+ * the architecture.  This is usually a read-allocate write-back cache.
+ * Morever, if MEMREMAP_WB is specified and the requested remap region is RAM
+ * memremap() will bypass establishing a new mapping and instead return
+ * a pointer into the direct map.
+ *
+ * MEMREMAP_WT - establish a mapping whereby writes either bypass the
+ * cache or are written through to memory and never exist in a
+ * cache-dirty state with respect to program visibility.  Attempts to
+ * map System RAM with this mapping type will fail.
+ *
+ * MEMREMAP_WC - establish a writecombine mapping, whereby writes may
+ * be coalesced together (e.g. in the CPU's write buffers), but is otherwise
+ * uncached. Attempts to map System RAM with this mapping type will fail.
+ */
+void *memremap(resource_size_t offset, size_t size, unsigned long flags)
+{
+	int is_ram = region_intersects(offset, size,
+				       IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);
+	void *addr = NULL;
+
+	if (!flags)
+		return NULL;
+
+	if (is_ram == REGION_MIXED) {
+		WARN_ONCE(1, "memremap attempted on mixed range %pa size: %#lx\n",
+				&offset, (unsigned long) size);
+		return NULL;
+	}
+
+	/* Try all mapping types requested until one returns non-NULL */
+	if (flags & MEMREMAP_WB) {
+		/*
+		 * MEMREMAP_WB is special in that it can be satisifed
+		 * from the direct map.  Some archs depend on the
+		 * capability of memremap() to autodetect cases where
+		 * the requested range is potentially in System RAM.
+		 */
+		if (is_ram == REGION_INTERSECTS)
+			addr = try_ram_remap(offset, size, flags);
+		if (!addr)
+			addr = arch_memremap_wb(offset, size);
+	}
+
+	/*
+	 * If we don't have a mapping yet and other request flags are
+	 * present then we will be attempting to establish a new virtual
+	 * address mapping.  Enforce that this mapping is not aliasing
+	 * System RAM.
+	 */
+	if (!addr && is_ram == REGION_INTERSECTS && flags != MEMREMAP_WB) {
+		WARN_ONCE(1, "memremap attempted on ram %pa size: %#lx\n",
+				&offset, (unsigned long) size);
+		return NULL;
+	}
+
+	if (!addr && (flags & MEMREMAP_WT))
+		addr = ioremap_wt(offset, size);
+
+	if (!addr && (flags & MEMREMAP_WC))
+		addr = ioremap_wc(offset, size);
+
+	return addr;
+}
+EXPORT_SYMBOL(memremap);
+
+void memunmap(void *addr)
+{
+	if (is_vmalloc_addr(addr))
+		iounmap((void __iomem *) addr);
+}
+EXPORT_SYMBOL(memunmap);
+
+static void devm_memremap_release(struct device *dev, void *res)
+{
+	memunmap(*(void **)res);
+}
+
+static int devm_memremap_match(struct device *dev, void *res, void *match_data)
+{
+	return *(void **)res == match_data;
+}
+
+void *devm_memremap(struct device *dev, resource_size_t offset,
+		size_t size, unsigned long flags)
+{
+	void **ptr, *addr;
+
+	ptr = devres_alloc_node(devm_memremap_release, sizeof(*ptr), GFP_KERNEL,
+			dev_to_node(dev));
+	if (!ptr)
+		return ERR_PTR(-ENOMEM);
+
+	addr = memremap(offset, size, flags);
+	if (addr) {
+		*ptr = addr;
+		devres_add(dev, ptr);
+	} else {
+		devres_free(ptr);
+		return ERR_PTR(-ENXIO);
+	}
+
+	return addr;
+}
+EXPORT_SYMBOL(devm_memremap);
+
+void devm_memunmap(struct device *dev, void *addr)
+{
+	WARN_ON(devres_release(dev, devm_memremap_release,
+				devm_memremap_match, addr));
+}
+EXPORT_SYMBOL(devm_memunmap);
* Unmerged path kernel/memremap.c
