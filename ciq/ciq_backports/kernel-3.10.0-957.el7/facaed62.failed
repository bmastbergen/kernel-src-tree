drm/nouveau/kms/gv100: initial support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ben Skeggs <bskeggs@redhat.com>
commit facaed62b4cba3a6334fc1798fa8f51ea6a1962d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/facaed62.failed

	Signed-off-by: Ben Skeggs <bskeggs@redhat.com>
(cherry picked from commit facaed62b4cba3a6334fc1798fa8f51ea6a1962d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/amd/powerplay/inc/pp_soc15.h
#	drivers/gpu/drm/nouveau/dispnv50/Kbuild
#	drivers/gpu/drm/nouveau/dispnv50/atom.h
#	drivers/gpu/drm/nouveau/dispnv50/core.c
#	drivers/gpu/drm/nouveau/dispnv50/core.h
#	drivers/gpu/drm/nouveau/dispnv50/curs.h
#	drivers/gpu/drm/nouveau/dispnv50/disp.c
#	drivers/gpu/drm/nouveau/dispnv50/disp.h
#	drivers/gpu/drm/nouveau/dispnv50/head.c
#	drivers/gpu/drm/nouveau/dispnv50/head.h
#	drivers/gpu/drm/nouveau/dispnv50/head917d.c
#	drivers/gpu/drm/nouveau/dispnv50/wndw.c
#	drivers/gpu/drm/nouveau/dispnv50/wndw.h
diff --cc drivers/gpu/drm/amd/powerplay/inc/pp_soc15.h
index a511611ec7e0,f592087338c4..000000000000
--- a/drivers/gpu/drm/amd/powerplay/inc/pp_soc15.h
+++ b/drivers/gpu/drm/amd/powerplay/inc/pp_soc15.h
@@@ -18,33 -18,35 +18,51 @@@
   * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
   * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
   * OTHER DEALINGS IN THE SOFTWARE.
 + *
   */
 -#include "curs.h"
 +#ifndef PP_SOC15_H
 +#define PP_SOC15_H
  
 -#include <nvif/class.h>
 +#include "vega10/soc15ip.h"
  
 -int
 -nv50_curs_new(struct nouveau_drm *drm, int head, struct nv50_wndw **pwndw)
 +inline static uint32_t soc15_get_register_offset(
 +		uint32_t hw_id,
 +		uint32_t inst,
 +		uint32_t segment,
 +		uint32_t offset)
  {
++<<<<<<< HEAD:drivers/gpu/drm/amd/powerplay/inc/pp_soc15.h
 +	uint32_t reg = 0;
++=======
+ 	struct {
+ 		s32 oclass;
+ 		int version;
+ 		int (*new)(struct nouveau_drm *, int, s32, struct nv50_wndw **);
+ 	} curses[] = {
+ 		{ GV100_DISP_CURSOR, 0, cursc37a_new },
+ 		{ GK104_DISP_CURSOR, 0, curs907a_new },
+ 		{ GF110_DISP_CURSOR, 0, curs907a_new },
+ 		{ GT214_DISP_CURSOR, 0, curs507a_new },
+ 		{   G82_DISP_CURSOR, 0, curs507a_new },
+ 		{  NV50_DISP_CURSOR, 0, curs507a_new },
+ 		{}
+ 	};
+ 	struct nv50_disp *disp = nv50_disp(drm->dev);
+ 	int cid;
++>>>>>>> facaed62b4cb (drm/nouveau/kms/gv100: initial support):drivers/gpu/drm/nouveau/dispnv50/curs.c
  
 -	cid = nvif_mclass(&disp->disp->object, curses);
 -	if (cid < 0) {
 -		NV_ERROR(drm, "No supported cursor immediate class\n");
 -		return cid;
 -	}
 +	if (hw_id == THM_HWID)
 +		reg = THM_BASE.instance[inst].segment[segment] + offset;
 +	else if (hw_id == NBIF_HWID)
 +		reg = NBIF_BASE.instance[inst].segment[segment] + offset;
 +	else if (hw_id == MP1_HWID)
 +		reg = MP1_BASE.instance[inst].segment[segment] + offset;
 +	else if (hw_id == DF_HWID)
 +		reg = DF_BASE.instance[inst].segment[segment] + offset;
 +	else if (hw_id == GC_HWID)
 +		reg = GC_BASE.instance[inst].segment[segment] + offset;
  
 -	return curses[cid].new(drm, head, curses[cid].oclass, pwndw);
 +	return reg;
  }
 +
 +#endif
diff --cc drivers/gpu/drm/nouveau/dispnv50/Kbuild
index 43fc8be49391,849b0f45afb8..000000000000
--- a/drivers/gpu/drm/nouveau/dispnv50/Kbuild
+++ b/drivers/gpu/drm/nouveau/dispnv50/Kbuild
@@@ -1,1 -1,51 +1,54 @@@
  nouveau-y += dispnv50/disp.o
++<<<<<<< HEAD
++=======
+ nouveau-y += dispnv50/lut.o
+ 
+ nouveau-y += dispnv50/core.o
+ nouveau-y += dispnv50/core507d.o
+ nouveau-y += dispnv50/core827d.o
+ nouveau-y += dispnv50/core907d.o
+ nouveau-y += dispnv50/core917d.o
+ nouveau-y += dispnv50/corec37d.o
+ 
+ nouveau-y += dispnv50/dac507d.o
+ nouveau-y += dispnv50/dac907d.o
+ 
+ nouveau-y += dispnv50/pior507d.o
+ 
+ nouveau-y += dispnv50/sor507d.o
+ nouveau-y += dispnv50/sor907d.o
+ nouveau-y += dispnv50/sorc37d.o
+ 
+ nouveau-y += dispnv50/head.o
+ nouveau-y += dispnv50/head507d.o
+ nouveau-y += dispnv50/head827d.o
+ nouveau-y += dispnv50/head907d.o
+ nouveau-y += dispnv50/head917d.o
+ nouveau-y += dispnv50/headc37d.o
+ 
+ nouveau-y += dispnv50/wimm.o
+ nouveau-y += dispnv50/wimmc37b.o
+ 
+ nouveau-y += dispnv50/wndw.o
+ nouveau-y += dispnv50/wndwc37e.o
+ 
+ nouveau-y += dispnv50/base.o
+ nouveau-y += dispnv50/base507c.o
+ nouveau-y += dispnv50/base827c.o
+ nouveau-y += dispnv50/base907c.o
+ nouveau-y += dispnv50/base917c.o
+ 
+ nouveau-y += dispnv50/curs.o
+ nouveau-y += dispnv50/curs507a.o
+ nouveau-y += dispnv50/curs907a.o
+ nouveau-y += dispnv50/cursc37a.o
+ 
+ nouveau-y += dispnv50/oimm.o
+ nouveau-y += dispnv50/oimm507b.o
+ 
+ nouveau-y += dispnv50/ovly.o
+ nouveau-y += dispnv50/ovly507e.o
+ nouveau-y += dispnv50/ovly827e.o
+ nouveau-y += dispnv50/ovly907e.o
+ nouveau-y += dispnv50/ovly917e.o
++>>>>>>> facaed62b4cb (drm/nouveau/kms/gv100: initial support)
diff --cc drivers/gpu/drm/nouveau/dispnv50/disp.c
index a2ea42f947c7,b83465ae7c1b..000000000000
--- a/drivers/gpu/drm/nouveau/dispnv50/disp.c
+++ b/drivers/gpu/drm/nouveau/dispnv50/disp.c
@@@ -73,2129 -57,55 +73,2132 @@@
  /******************************************************************************
   * Atomic state
   *****************************************************************************/
 +#define nv50_atom(p) container_of((p), struct nv50_atom, state)
 +
 +struct nv50_atom {
 +	struct drm_atomic_state state;
 +
 +	struct list_head outp;
 +	bool lock_core;
 +	bool flush_disable;
 +};
 +
 +struct nv50_outp_atom {
 +	struct list_head head;
 +
 +	struct drm_encoder *encoder;
 +	bool flush_disable;
 +
 +	union {
 +		struct {
 +			bool ctrl:1;
 +		};
 +		u8 mask;
 +	} clr;
 +
 +	union {
 +		struct {
 +			bool ctrl:1;
 +		};
 +		u8 mask;
 +	} set;
 +};
 +
 +#define nv50_head_atom(p) container_of((p), struct nv50_head_atom, state)
 +
 +struct nv50_head_atom {
 +	struct drm_crtc_state state;
 +
 +	struct {
 +		u16 iW;
 +		u16 iH;
 +		u16 oW;
 +		u16 oH;
 +	} view;
 +
 +	struct nv50_head_mode {
 +		bool interlace;
 +		u32 clock;
 +		struct {
 +			u16 active;
 +			u16 synce;
 +			u16 blanke;
 +			u16 blanks;
 +		} h;
 +		struct {
 +			u32 active;
 +			u16 synce;
 +			u16 blanke;
 +			u16 blanks;
 +			u16 blank2s;
 +			u16 blank2e;
 +			u16 blankus;
 +		} v;
 +	} mode;
 +
 +	struct {
 +		u32 handle;
 +		u64 offset:40;
 +	} lut;
 +
 +	struct {
 +		bool visible;
 +		u32 handle;
 +		u64 offset:40;
 +		u8  format;
 +		u8  kind:7;
 +		u8  layout:1;
 +		u8  block:4;
 +		u32 pitch:20;
 +		u16 x;
 +		u16 y;
 +		u16 w;
 +		u16 h;
 +	} core;
 +
 +	struct {
 +		bool visible;
 +		u32 handle;
 +		u64 offset:40;
 +		u8  layout:1;
 +		u8  format:1;
 +	} curs;
 +
 +	struct {
 +		u8  depth;
 +		u8  cpp;
 +		u16 x;
 +		u16 y;
 +		u16 w;
 +		u16 h;
 +	} base;
 +
 +	struct {
 +		u8 cpp;
 +	} ovly;
 +
 +	struct {
 +		bool enable:1;
 +		u8 bits:2;
 +		u8 mode:4;
 +	} dither;
 +
 +	struct {
 +		struct {
 +			u16 cos:12;
 +			u16 sin:12;
 +		} sat;
 +	} procamp;
 +
 +	union {
 +		struct {
 +			bool core:1;
 +			bool curs:1;
 +		};
 +		u8 mask;
 +	} clr;
 +
 +	union {
 +		struct {
 +			bool core:1;
 +			bool curs:1;
 +			bool view:1;
 +			bool mode:1;
 +			bool base:1;
 +			bool ovly:1;
 +			bool dither:1;
 +			bool procamp:1;
 +		};
 +		u16 mask;
 +	} set;
 +};
 +
 +static inline struct nv50_head_atom *
 +nv50_head_atom_get(struct drm_atomic_state *state, struct drm_crtc *crtc)
 +{
 +	struct drm_crtc_state *statec = drm_atomic_get_crtc_state(state, crtc);
 +	if (IS_ERR(statec))
 +		return (void *)statec;
 +	return nv50_head_atom(statec);
 +}
 +
 +#define nv50_wndw_atom(p) container_of((p), struct nv50_wndw_atom, state)
 +
 +struct nv50_wndw_atom {
 +	struct drm_plane_state state;
 +	u8 interval;
 +
 +	struct drm_rect clip;
 +
 +	struct {
 +		u32  handle;
 +		u16  offset:12;
 +		bool awaken:1;
 +	} ntfy;
 +
 +	struct {
 +		u32 handle;
 +		u16 offset:12;
 +		u32 acquire;
 +		u32 release;
 +	} sema;
 +
 +	struct {
 +		u8 enable:2;
 +	} lut;
 +
 +	struct {
 +		u8  mode:2;
 +		u8  interval:4;
 +
 +		u8  format;
 +		u8  kind:7;
 +		u8  layout:1;
 +		u8  block:4;
 +		u32 pitch:20;
 +		u16 w;
 +		u16 h;
 +
 +		u32 handle;
 +		u64 offset;
 +	} image;
 +
 +	struct {
 +		u16 x;
 +		u16 y;
 +	} point;
 +
 +	union {
 +		struct {
 +			bool ntfy:1;
 +			bool sema:1;
 +			bool image:1;
 +		};
 +		u8 mask;
 +	} clr;
 +
 +	union {
 +		struct {
 +			bool ntfy:1;
 +			bool sema:1;
 +			bool image:1;
 +			bool lut:1;
 +			bool point:1;
 +		};
 +		u8 mask;
 +	} set;
 +};
 +
 +/******************************************************************************
 + * EVO channel
 + *****************************************************************************/
 +
 +struct nv50_chan {
 +	struct nvif_object user;
 +	struct nvif_device *device;
 +};
 +
 +static int
 +nv50_chan_create(struct nvif_device *device, struct nvif_object *disp,
 +		 const s32 *oclass, u8 head, void *data, u32 size,
 +		 struct nv50_chan *chan)
 +{
 +	struct nvif_sclass *sclass;
 +	int ret, i, n;
 +
 +	chan->device = device;
 +
 +	ret = n = nvif_object_sclass_get(disp, &sclass);
 +	if (ret < 0)
 +		return ret;
 +
 +	while (oclass[0]) {
 +		for (i = 0; i < n; i++) {
 +			if (sclass[i].oclass == oclass[0]) {
 +				ret = nvif_object_init(disp, 0, oclass[0],
 +						       data, size, &chan->user);
 +				if (ret == 0)
 +					nvif_object_map(&chan->user);
 +				nvif_object_sclass_put(&sclass);
 +				return ret;
 +			}
 +		}
 +		oclass++;
 +	}
 +
 +	nvif_object_sclass_put(&sclass);
 +	return -ENOSYS;
 +}
 +
 +static void
 +nv50_chan_destroy(struct nv50_chan *chan)
 +{
 +	nvif_object_fini(&chan->user);
 +}
 +
 +/******************************************************************************
 + * PIO EVO channel
 + *****************************************************************************/
 +
 +struct nv50_pioc {
 +	struct nv50_chan base;
 +};
 +
 +static void
 +nv50_pioc_destroy(struct nv50_pioc *pioc)
 +{
 +	nv50_chan_destroy(&pioc->base);
 +}
 +
 +static int
 +nv50_pioc_create(struct nvif_device *device, struct nvif_object *disp,
 +		 const s32 *oclass, u8 head, void *data, u32 size,
 +		 struct nv50_pioc *pioc)
 +{
 +	return nv50_chan_create(device, disp, oclass, head, data, size,
 +				&pioc->base);
 +}
 +
 +/******************************************************************************
 + * Overlay Immediate
 + *****************************************************************************/
 +
 +struct nv50_oimm {
 +	struct nv50_pioc base;
 +};
 +
 +static int
 +nv50_oimm_create(struct nvif_device *device, struct nvif_object *disp,
 +		 int head, struct nv50_oimm *oimm)
 +{
 +	struct nv50_disp_cursor_v0 args = {
 +		.head = head,
 +	};
 +	static const s32 oclass[] = {
 +		GK104_DISP_OVERLAY,
 +		GF110_DISP_OVERLAY,
 +		GT214_DISP_OVERLAY,
 +		G82_DISP_OVERLAY,
 +		NV50_DISP_OVERLAY,
 +		0
 +	};
 +
 +	return nv50_pioc_create(device, disp, oclass, head, &args, sizeof(args),
 +				&oimm->base);
 +}
 +
 +/******************************************************************************
 + * DMA EVO channel
 + *****************************************************************************/
 +
 +struct nv50_dmac_ctxdma {
 +	struct list_head head;
 +	struct nvif_object object;
 +};
 +
 +struct nv50_dmac {
 +	struct nv50_chan base;
 +	dma_addr_t handle;
 +	u32 *ptr;
 +
 +	struct nvif_object sync;
 +	struct nvif_object vram;
 +	struct list_head ctxdma;
 +
 +	/* Protects against concurrent pushbuf access to this channel, lock is
 +	 * grabbed by evo_wait (if the pushbuf reservation is successful) and
 +	 * dropped again by evo_kick. */
 +	struct mutex lock;
 +};
 +
 +static void
 +nv50_dmac_ctxdma_del(struct nv50_dmac_ctxdma *ctxdma)
 +{
 +	nvif_object_fini(&ctxdma->object);
 +	list_del(&ctxdma->head);
 +	kfree(ctxdma);
 +}
 +
 +static struct nv50_dmac_ctxdma *
 +nv50_dmac_ctxdma_new(struct nv50_dmac *dmac, struct nouveau_framebuffer *fb)
 +{
 +	struct nouveau_drm *drm = nouveau_drm(fb->base.dev);
 +	struct nv50_dmac_ctxdma *ctxdma;
 +	const u8    kind = (fb->nvbo->tile_flags & 0x0000ff00) >> 8;
 +	const u32 handle = 0xfb000000 | kind;
 +	struct {
 +		struct nv_dma_v0 base;
 +		union {
 +			struct nv50_dma_v0 nv50;
 +			struct gf100_dma_v0 gf100;
 +			struct gf119_dma_v0 gf119;
 +		};
 +	} args = {};
 +	u32 argc = sizeof(args.base);
 +	int ret;
 +
 +	list_for_each_entry(ctxdma, &dmac->ctxdma, head) {
 +		if (ctxdma->object.handle == handle)
 +			return ctxdma;
 +	}
 +
 +	if (!(ctxdma = kzalloc(sizeof(*ctxdma), GFP_KERNEL)))
 +		return ERR_PTR(-ENOMEM);
 +	list_add(&ctxdma->head, &dmac->ctxdma);
 +
 +	args.base.target = NV_DMA_V0_TARGET_VRAM;
 +	args.base.access = NV_DMA_V0_ACCESS_RDWR;
 +	args.base.start  = 0;
 +	args.base.limit  = drm->client.device.info.ram_user - 1;
 +
 +	if (drm->client.device.info.chipset < 0x80) {
 +		args.nv50.part = NV50_DMA_V0_PART_256;
 +		argc += sizeof(args.nv50);
 +	} else
 +	if (drm->client.device.info.chipset < 0xc0) {
 +		args.nv50.part = NV50_DMA_V0_PART_256;
 +		args.nv50.kind = kind;
 +		argc += sizeof(args.nv50);
 +	} else
 +	if (drm->client.device.info.chipset < 0xd0) {
 +		args.gf100.kind = kind;
 +		argc += sizeof(args.gf100);
 +	} else {
 +		args.gf119.page = GF119_DMA_V0_PAGE_LP;
 +		args.gf119.kind = kind;
 +		argc += sizeof(args.gf119);
 +	}
 +
 +	ret = nvif_object_init(&dmac->base.user, handle, NV_DMA_IN_MEMORY,
 +			       &args, argc, &ctxdma->object);
 +	if (ret) {
 +		nv50_dmac_ctxdma_del(ctxdma);
 +		return ERR_PTR(ret);
 +	}
 +
 +	return ctxdma;
 +}
 +
 +static void
 +nv50_dmac_destroy(struct nv50_dmac *dmac, struct nvif_object *disp)
 +{
 +	struct nvif_device *device = dmac->base.device;
 +	struct nv50_dmac_ctxdma *ctxdma, *ctxtmp;
 +
 +	list_for_each_entry_safe(ctxdma, ctxtmp, &dmac->ctxdma, head) {
 +		nv50_dmac_ctxdma_del(ctxdma);
 +	}
 +
 +	nvif_object_fini(&dmac->vram);
 +	nvif_object_fini(&dmac->sync);
 +
 +	nv50_chan_destroy(&dmac->base);
 +
 +	if (dmac->ptr) {
 +		struct device *dev = nvxx_device(device)->dev;
 +		dma_free_coherent(dev, PAGE_SIZE, dmac->ptr, dmac->handle);
 +	}
 +}
 +
 +static int
 +nv50_dmac_create(struct nvif_device *device, struct nvif_object *disp,
 +		 const s32 *oclass, u8 head, void *data, u32 size, u64 syncbuf,
 +		 struct nv50_dmac *dmac)
 +{
 +	struct nv50_disp_core_channel_dma_v0 *args = data;
 +	struct nvif_object pushbuf;
 +	int ret;
 +
 +	mutex_init(&dmac->lock);
 +
 +	dmac->ptr = dma_alloc_coherent(nvxx_device(device)->dev, PAGE_SIZE,
 +				       &dmac->handle, GFP_KERNEL);
 +	if (!dmac->ptr)
 +		return -ENOMEM;
 +
 +	ret = nvif_object_init(&device->object, 0, NV_DMA_FROM_MEMORY,
 +			       &(struct nv_dma_v0) {
 +					.target = NV_DMA_V0_TARGET_PCI_US,
 +					.access = NV_DMA_V0_ACCESS_RD,
 +					.start = dmac->handle + 0x0000,
 +					.limit = dmac->handle + 0x0fff,
 +			       }, sizeof(struct nv_dma_v0), &pushbuf);
 +	if (ret)
 +		return ret;
 +
 +	args->pushbuf = nvif_handle(&pushbuf);
 +
 +	ret = nv50_chan_create(device, disp, oclass, head, data, size,
 +			       &dmac->base);
 +	nvif_object_fini(&pushbuf);
 +	if (ret)
 +		return ret;
 +
++	if (!syncbuf)
++		return 0;
++
 +	ret = nvif_object_init(&dmac->base.user, 0xf0000000, NV_DMA_IN_MEMORY,
 +			       &(struct nv_dma_v0) {
 +					.target = NV_DMA_V0_TARGET_VRAM,
 +					.access = NV_DMA_V0_ACCESS_RDWR,
 +					.start = syncbuf + 0x0000,
 +					.limit = syncbuf + 0x0fff,
 +			       }, sizeof(struct nv_dma_v0),
 +			       &dmac->sync);
 +	if (ret)
 +		return ret;
 +
 +	ret = nvif_object_init(&dmac->base.user, 0xf0000001, NV_DMA_IN_MEMORY,
 +			       &(struct nv_dma_v0) {
 +					.target = NV_DMA_V0_TARGET_VRAM,
 +					.access = NV_DMA_V0_ACCESS_RDWR,
 +					.start = 0,
 +					.limit = device->info.ram_user - 1,
 +			       }, sizeof(struct nv_dma_v0),
 +			       &dmac->vram);
 +	if (ret)
 +		return ret;
 +
 +	INIT_LIST_HEAD(&dmac->ctxdma);
 +	return ret;
 +}
 +
 +/******************************************************************************
 + * Core
 + *****************************************************************************/
 +
 +struct nv50_mast {
 +	struct nv50_dmac base;
 +};
 +
 +static int
 +nv50_core_create(struct nvif_device *device, struct nvif_object *disp,
 +		 u64 syncbuf, struct nv50_mast *core)
 +{
 +	struct nv50_disp_core_channel_dma_v0 args = {
 +		.pushbuf = 0xb0007d00,
 +	};
 +	static const s32 oclass[] = {
 +		GP102_DISP_CORE_CHANNEL_DMA,
 +		GP100_DISP_CORE_CHANNEL_DMA,
 +		GM200_DISP_CORE_CHANNEL_DMA,
 +		GM107_DISP_CORE_CHANNEL_DMA,
 +		GK110_DISP_CORE_CHANNEL_DMA,
 +		GK104_DISP_CORE_CHANNEL_DMA,
 +		GF110_DISP_CORE_CHANNEL_DMA,
 +		GT214_DISP_CORE_CHANNEL_DMA,
 +		GT206_DISP_CORE_CHANNEL_DMA,
 +		GT200_DISP_CORE_CHANNEL_DMA,
 +		G82_DISP_CORE_CHANNEL_DMA,
 +		NV50_DISP_CORE_CHANNEL_DMA,
 +		0
 +	};
 +
 +	return nv50_dmac_create(device, disp, oclass, 0, &args, sizeof(args),
 +				syncbuf, &core->base);
 +}
 +
 +/******************************************************************************
 + * Base
 + *****************************************************************************/
 +
 +struct nv50_sync {
 +	struct nv50_dmac base;
 +	u32 addr;
 +	u32 data;
 +};
 +
 +static int
 +nv50_base_create(struct nvif_device *device, struct nvif_object *disp,
 +		 int head, u64 syncbuf, struct nv50_sync *base)
 +{
 +	struct nv50_disp_base_channel_dma_v0 args = {
 +		.pushbuf = 0xb0007c00 | head,
 +		.head = head,
 +	};
 +	static const s32 oclass[] = {
 +		GK110_DISP_BASE_CHANNEL_DMA,
 +		GK104_DISP_BASE_CHANNEL_DMA,
 +		GF110_DISP_BASE_CHANNEL_DMA,
 +		GT214_DISP_BASE_CHANNEL_DMA,
 +		GT200_DISP_BASE_CHANNEL_DMA,
 +		G82_DISP_BASE_CHANNEL_DMA,
 +		NV50_DISP_BASE_CHANNEL_DMA,
 +		0
 +	};
 +
 +	return nv50_dmac_create(device, disp, oclass, head, &args, sizeof(args),
 +				syncbuf, &base->base);
 +}
 +
 +/******************************************************************************
 + * Overlay
 + *****************************************************************************/
 +
 +struct nv50_ovly {
 +	struct nv50_dmac base;
 +};
 +
 +static int
 +nv50_ovly_create(struct nvif_device *device, struct nvif_object *disp,
 +		 int head, u64 syncbuf, struct nv50_ovly *ovly)
 +{
 +	struct nv50_disp_overlay_channel_dma_v0 args = {
 +		.pushbuf = 0xb0007e00 | head,
 +		.head = head,
 +	};
 +	static const s32 oclass[] = {
 +		GK104_DISP_OVERLAY_CONTROL_DMA,
 +		GF110_DISP_OVERLAY_CONTROL_DMA,
 +		GT214_DISP_OVERLAY_CHANNEL_DMA,
 +		GT200_DISP_OVERLAY_CHANNEL_DMA,
 +		G82_DISP_OVERLAY_CHANNEL_DMA,
 +		NV50_DISP_OVERLAY_CHANNEL_DMA,
 +		0
 +	};
 +
 +	return nv50_dmac_create(device, disp, oclass, head, &args, sizeof(args),
 +				syncbuf, &ovly->base);
 +}
 +
 +struct nv50_head {
 +	struct nouveau_crtc base;
 +	struct nv50_ovly ovly;
 +	struct nv50_oimm oimm;
 +};
 +
 +#define nv50_head(c) ((struct nv50_head *)nouveau_crtc(c))
 +#define nv50_ovly(c) (&nv50_head(c)->ovly)
 +#define nv50_oimm(c) (&nv50_head(c)->oimm)
 +#define nv50_chan(c) (&(c)->base.base)
 +#define nv50_vers(c) nv50_chan(c)->user.oclass
 +
 +struct nv50_disp {
 +	struct nvif_object *disp;
 +	struct nv50_mast mast;
 +
 +	struct nouveau_bo *sync;
 +
 +	struct mutex mutex;
 +};
 +
 +static struct nv50_disp *
 +nv50_disp(struct drm_device *dev)
 +{
 +	return nouveau_display(dev)->priv;
 +}
 +
 +#define nv50_mast(d) (&nv50_disp(d)->mast)
 +
 +/******************************************************************************
 + * EVO channel helpers
 + *****************************************************************************/
 +static u32 *
 +evo_wait(void *evoc, int nr)
 +{
 +	struct nv50_dmac *dmac = evoc;
 +	struct nvif_device *device = dmac->base.device;
 +	u32 put = nvif_rd32(&dmac->base.user, 0x0000) / 4;
 +
 +	mutex_lock(&dmac->lock);
 +	if (put + nr >= (PAGE_SIZE / 4) - 8) {
 +		dmac->ptr[put] = 0x20000000;
 +
 +		nvif_wr32(&dmac->base.user, 0x0000, 0x00000000);
 +		if (nvif_msec(device, 2000,
 +			if (!nvif_rd32(&dmac->base.user, 0x0004))
 +				break;
 +		) < 0) {
 +			mutex_unlock(&dmac->lock);
 +			pr_err("nouveau: evo channel stalled\n");
 +			return NULL;
 +		}
 +
 +		put = 0;
 +	}
 +
 +	return dmac->ptr + put;
 +}
 +
 +static void
 +evo_kick(u32 *push, void *evoc)
 +{
 +	struct nv50_dmac *dmac = evoc;
 +	nvif_wr32(&dmac->base.user, 0x0000, (push - dmac->ptr) << 2);
 +	mutex_unlock(&dmac->lock);
 +}
 +
 +#define evo_mthd(p, m, s) do {						\
 +	const u32 _m = (m), _s = (s);					\
 +	if (drm_debug & DRM_UT_KMS)					\
 +		pr_err("%04x %d %s\n", _m, _s, __func__);		\
 +	*((p)++) = ((_s << 18) | _m);					\
 +} while(0)
 +
 +#define evo_data(p, d) do {						\
 +	const u32 _d = (d);						\
 +	if (drm_debug & DRM_UT_KMS)					\
 +		pr_err("\t%08x\n", _d);					\
 +	*((p)++) = _d;							\
 +} while(0)
 +
 +/******************************************************************************
 + * Plane
 + *****************************************************************************/
 +#define nv50_wndw(p) container_of((p), struct nv50_wndw, plane)
 +
 +struct nv50_wndw {
 +	const struct nv50_wndw_func *func;
 +	struct nv50_dmac *dmac;
 +
 +	struct drm_plane plane;
 +
 +	struct nvif_notify notify;
 +	u16 ntfy;
 +	u16 sema;
 +	u32 data;
 +};
 +
 +struct nv50_wndw_func {
 +	void *(*dtor)(struct nv50_wndw *);
 +	int (*acquire)(struct nv50_wndw *, struct nv50_wndw_atom *asyw,
 +		       struct nv50_head_atom *asyh);
 +	void (*release)(struct nv50_wndw *, struct nv50_wndw_atom *asyw,
 +			struct nv50_head_atom *asyh);
 +	void (*prepare)(struct nv50_wndw *, struct nv50_head_atom *asyh,
 +			struct nv50_wndw_atom *asyw);
 +
 +	void (*sema_set)(struct nv50_wndw *, struct nv50_wndw_atom *);
 +	void (*sema_clr)(struct nv50_wndw *);
 +	void (*ntfy_set)(struct nv50_wndw *, struct nv50_wndw_atom *);
 +	void (*ntfy_clr)(struct nv50_wndw *);
 +	int (*ntfy_wait_begun)(struct nv50_wndw *, struct nv50_wndw_atom *);
 +	void (*image_set)(struct nv50_wndw *, struct nv50_wndw_atom *);
 +	void (*image_clr)(struct nv50_wndw *);
 +	void (*lut)(struct nv50_wndw *, struct nv50_wndw_atom *);
 +	void (*point)(struct nv50_wndw *, struct nv50_wndw_atom *);
 +
 +	u32 (*update)(struct nv50_wndw *, u32 interlock);
 +};
 +
 +static int
 +nv50_wndw_wait_armed(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
 +{
 +	if (asyw->set.ntfy)
 +		return wndw->func->ntfy_wait_begun(wndw, asyw);
 +	return 0;
 +}
 +
 +static u32
 +nv50_wndw_flush_clr(struct nv50_wndw *wndw, u32 interlock, bool flush,
 +		    struct nv50_wndw_atom *asyw)
 +{
 +	if (asyw->clr.sema && (!asyw->set.sema || flush))
 +		wndw->func->sema_clr(wndw);
 +	if (asyw->clr.ntfy && (!asyw->set.ntfy || flush))
 +		wndw->func->ntfy_clr(wndw);
 +	if (asyw->clr.image && (!asyw->set.image || flush))
 +		wndw->func->image_clr(wndw);
 +
 +	return flush ? wndw->func->update(wndw, interlock) : 0;
 +}
 +
 +static u32
 +nv50_wndw_flush_set(struct nv50_wndw *wndw, u32 interlock,
 +		    struct nv50_wndw_atom *asyw)
 +{
 +	if (interlock) {
 +		asyw->image.mode = 0;
 +		asyw->image.interval = 1;
 +	}
 +
 +	if (asyw->set.sema ) wndw->func->sema_set (wndw, asyw);
 +	if (asyw->set.ntfy ) wndw->func->ntfy_set (wndw, asyw);
 +	if (asyw->set.image) wndw->func->image_set(wndw, asyw);
 +	if (asyw->set.lut  ) wndw->func->lut      (wndw, asyw);
 +	if (asyw->set.point) wndw->func->point    (wndw, asyw);
 +
 +	return wndw->func->update(wndw, interlock);
 +}
 +
 +static void
 +nv50_wndw_atomic_check_release(struct nv50_wndw *wndw,
 +			       struct nv50_wndw_atom *asyw,
 +			       struct nv50_head_atom *asyh)
 +{
 +	struct nouveau_drm *drm = nouveau_drm(wndw->plane.dev);
 +	NV_ATOMIC(drm, "%s release\n", wndw->plane.name);
 +	wndw->func->release(wndw, asyw, asyh);
 +	asyw->ntfy.handle = 0;
 +	asyw->sema.handle = 0;
 +}
 +
 +static int
 +nv50_wndw_atomic_check_acquire(struct nv50_wndw *wndw,
 +			       struct nv50_wndw_atom *asyw,
 +			       struct nv50_head_atom *asyh)
 +{
 +	struct nouveau_framebuffer *fb = nouveau_framebuffer(asyw->state.fb);
 +	struct nouveau_drm *drm = nouveau_drm(wndw->plane.dev);
 +	int ret;
 +
 +	NV_ATOMIC(drm, "%s acquire\n", wndw->plane.name);
 +	asyw->clip.x1 = 0;
 +	asyw->clip.y1 = 0;
 +	asyw->clip.x2 = asyh->state.mode.hdisplay;
 +	asyw->clip.y2 = asyh->state.mode.vdisplay;
 +
 +	asyw->image.w = fb->base.width;
 +	asyw->image.h = fb->base.height;
 +	asyw->image.kind = (fb->nvbo->tile_flags & 0x0000ff00) >> 8;
 +
 +	if (asyh->state.pageflip_flags & DRM_MODE_PAGE_FLIP_ASYNC)
 +		asyw->interval = 0;
 +	else
 +		asyw->interval = 1;
 +
 +	if (asyw->image.kind) {
 +		asyw->image.layout = 0;
 +		if (drm->client.device.info.chipset >= 0xc0)
 +			asyw->image.block = fb->nvbo->tile_mode >> 4;
 +		else
 +			asyw->image.block = fb->nvbo->tile_mode;
 +		asyw->image.pitch = (fb->base.pitches[0] / 4) << 4;
 +	} else {
 +		asyw->image.layout = 1;
 +		asyw->image.block  = 0;
 +		asyw->image.pitch  = fb->base.pitches[0];
 +	}
 +
 +	ret = wndw->func->acquire(wndw, asyw, asyh);
 +	if (ret)
 +		return ret;
 +
 +	if (asyw->set.image) {
 +		if (!(asyw->image.mode = asyw->interval ? 0 : 1))
 +			asyw->image.interval = asyw->interval;
 +		else
 +			asyw->image.interval = 0;
 +	}
 +
 +	return 0;
 +}
 +
 +static int
 +nv50_wndw_atomic_check(struct drm_plane *plane, struct drm_plane_state *state)
 +{
 +	struct nouveau_drm *drm = nouveau_drm(plane->dev);
 +	struct nv50_wndw *wndw = nv50_wndw(plane);
 +	struct nv50_wndw_atom *armw = nv50_wndw_atom(wndw->plane.state);
 +	struct nv50_wndw_atom *asyw = nv50_wndw_atom(state);
 +	struct nv50_head_atom *harm = NULL, *asyh = NULL;
 +	bool varm = false, asyv = false, asym = false;
 +	int ret;
 +
 +	NV_ATOMIC(drm, "%s atomic_check\n", plane->name);
 +	if (asyw->state.crtc) {
 +		asyh = nv50_head_atom_get(asyw->state.state, asyw->state.crtc);
 +		if (IS_ERR(asyh))
 +			return PTR_ERR(asyh);
 +		asym = drm_atomic_crtc_needs_modeset(&asyh->state);
 +		asyv = asyh->state.active;
 +	}
 +
 +	if (armw->state.crtc) {
 +		harm = nv50_head_atom_get(asyw->state.state, armw->state.crtc);
 +		if (IS_ERR(harm))
 +			return PTR_ERR(harm);
 +		varm = harm->state.crtc->state->active;
 +	}
 +
 +	if (asyv) {
 +		asyw->point.x = asyw->state.crtc_x;
 +		asyw->point.y = asyw->state.crtc_y;
 +		if (memcmp(&armw->point, &asyw->point, sizeof(asyw->point)))
 +			asyw->set.point = true;
 +
 +		ret = nv50_wndw_atomic_check_acquire(wndw, asyw, asyh);
 +		if (ret)
 +			return ret;
 +	} else
 +	if (varm) {
 +		nv50_wndw_atomic_check_release(wndw, asyw, harm);
 +	} else {
 +		return 0;
 +	}
 +
 +	if (!asyv || asym) {
 +		asyw->clr.ntfy = armw->ntfy.handle != 0;
 +		asyw->clr.sema = armw->sema.handle != 0;
 +		if (wndw->func->image_clr)
 +			asyw->clr.image = armw->image.handle != 0;
 +		asyw->set.lut = wndw->func->lut && asyv;
 +	}
 +
 +	return 0;
 +}
 +
 +static void
 +nv50_wndw_cleanup_fb(struct drm_plane *plane, struct drm_plane_state *old_state)
 +{
 +	struct nouveau_framebuffer *fb = nouveau_framebuffer(old_state->fb);
 +	struct nouveau_drm *drm = nouveau_drm(plane->dev);
 +
 +	NV_ATOMIC(drm, "%s cleanup: %p\n", plane->name, old_state->fb);
 +	if (!old_state->fb)
 +		return;
 +
 +	nouveau_bo_unpin(fb->nvbo);
 +}
 +
 +static int
 +nv50_wndw_prepare_fb(struct drm_plane *plane, struct drm_plane_state *state)
 +{
 +	struct nouveau_framebuffer *fb = nouveau_framebuffer(state->fb);
 +	struct nouveau_drm *drm = nouveau_drm(plane->dev);
 +	struct nv50_wndw *wndw = nv50_wndw(plane);
 +	struct nv50_wndw_atom *asyw = nv50_wndw_atom(state);
 +	struct nv50_head_atom *asyh;
 +	struct nv50_dmac_ctxdma *ctxdma;
 +	int ret;
 +
 +	NV_ATOMIC(drm, "%s prepare: %p\n", plane->name, state->fb);
 +	if (!asyw->state.fb)
 +		return 0;
 +
 +	ret = nouveau_bo_pin(fb->nvbo, TTM_PL_FLAG_VRAM, true);
 +	if (ret)
 +		return ret;
 +
 +	ctxdma = nv50_dmac_ctxdma_new(wndw->dmac, fb);
 +	if (IS_ERR(ctxdma)) {
 +		nouveau_bo_unpin(fb->nvbo);
 +		return PTR_ERR(ctxdma);
 +	}
 +
 +	asyw->state.fence = reservation_object_get_excl_rcu(fb->nvbo->bo.resv);
 +	asyw->image.handle = ctxdma->object.handle;
 +	asyw->image.offset = fb->nvbo->bo.offset;
 +
 +	if (wndw->func->prepare) {
 +		asyh = nv50_head_atom_get(asyw->state.state, asyw->state.crtc);
 +		if (IS_ERR(asyh))
 +			return PTR_ERR(asyh);
 +
 +		wndw->func->prepare(wndw, asyh, asyw);
 +	}
 +
 +	return 0;
 +}
 +
 +static const struct drm_plane_helper_funcs
 +nv50_wndw_helper = {
 +	.prepare_fb = nv50_wndw_prepare_fb,
 +	.cleanup_fb = nv50_wndw_cleanup_fb,
 +	.atomic_check = nv50_wndw_atomic_check,
 +};
 +
 +static void
 +nv50_wndw_atomic_destroy_state(struct drm_plane *plane,
 +			       struct drm_plane_state *state)
 +{
 +	struct nv50_wndw_atom *asyw = nv50_wndw_atom(state);
 +	__drm_atomic_helper_plane_destroy_state(&asyw->state);
 +	kfree(asyw);
 +}
 +
 +static struct drm_plane_state *
 +nv50_wndw_atomic_duplicate_state(struct drm_plane *plane)
 +{
 +	struct nv50_wndw_atom *armw = nv50_wndw_atom(plane->state);
 +	struct nv50_wndw_atom *asyw;
 +	if (!(asyw = kmalloc(sizeof(*asyw), GFP_KERNEL)))
 +		return NULL;
 +	__drm_atomic_helper_plane_duplicate_state(plane, &asyw->state);
 +	asyw->interval = 1;
 +	asyw->sema = armw->sema;
 +	asyw->ntfy = armw->ntfy;
 +	asyw->image = armw->image;
 +	asyw->point = armw->point;
 +	asyw->lut = armw->lut;
 +	asyw->clr.mask = 0;
 +	asyw->set.mask = 0;
 +	return &asyw->state;
 +}
 +
 +static void
 +nv50_wndw_reset(struct drm_plane *plane)
 +{
 +	struct nv50_wndw_atom *asyw;
 +
 +	if (WARN_ON(!(asyw = kzalloc(sizeof(*asyw), GFP_KERNEL))))
 +		return;
 +
 +	if (plane->state)
 +		plane->funcs->atomic_destroy_state(plane, plane->state);
 +	plane->state = &asyw->state;
 +	plane->state->plane = plane;
 +	plane->state->rotation = DRM_MODE_ROTATE_0;
 +}
 +
 +static void
 +nv50_wndw_destroy(struct drm_plane *plane)
 +{
 +	struct nv50_wndw *wndw = nv50_wndw(plane);
 +	void *data;
 +	nvif_notify_fini(&wndw->notify);
 +	data = wndw->func->dtor(wndw);
 +	drm_plane_cleanup(&wndw->plane);
 +	kfree(data);
 +}
 +
 +static const struct drm_plane_funcs
 +nv50_wndw = {
 +	.update_plane = drm_atomic_helper_update_plane,
 +	.disable_plane = drm_atomic_helper_disable_plane,
 +	.destroy = nv50_wndw_destroy,
 +	.reset = nv50_wndw_reset,
 +	.atomic_duplicate_state = nv50_wndw_atomic_duplicate_state,
 +	.atomic_destroy_state = nv50_wndw_atomic_destroy_state,
 +};
 +
 +static void
 +nv50_wndw_fini(struct nv50_wndw *wndw)
 +{
 +	nvif_notify_put(&wndw->notify);
 +}
 +
 +static void
 +nv50_wndw_init(struct nv50_wndw *wndw)
 +{
 +	nvif_notify_get(&wndw->notify);
 +}
 +
 +static int
 +nv50_wndw_ctor(const struct nv50_wndw_func *func, struct drm_device *dev,
 +	       enum drm_plane_type type, const char *name, int index,
 +	       struct nv50_dmac *dmac, const u32 *format, int nformat,
 +	       struct nv50_wndw *wndw)
 +{
 +	int ret;
 +
 +	wndw->func = func;
 +	wndw->dmac = dmac;
 +
 +	ret = drm_universal_plane_init(dev, &wndw->plane, 0, &nv50_wndw,
 +				       format, nformat, NULL,
 +				       type, "%s-%d", name, index);
 +	if (ret)
 +		return ret;
 +
 +	drm_plane_helper_add(&wndw->plane, &nv50_wndw_helper);
 +	return 0;
 +}
 +
 +/******************************************************************************
 + * Cursor plane
 + *****************************************************************************/
 +#define nv50_curs(p) container_of((p), struct nv50_curs, wndw)
 +
 +struct nv50_curs {
 +	struct nv50_wndw wndw;
 +	struct nvif_object chan;
 +};
 +
 +static u32
 +nv50_curs_update(struct nv50_wndw *wndw, u32 interlock)
 +{
 +	struct nv50_curs *curs = nv50_curs(wndw);
 +	nvif_wr32(&curs->chan, 0x0080, 0x00000000);
 +	return 0;
 +}
 +
 +static void
 +nv50_curs_point(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
 +{
 +	struct nv50_curs *curs = nv50_curs(wndw);
 +	nvif_wr32(&curs->chan, 0x0084, (asyw->point.y << 16) | asyw->point.x);
 +}
 +
 +static void
 +nv50_curs_prepare(struct nv50_wndw *wndw, struct nv50_head_atom *asyh,
 +		  struct nv50_wndw_atom *asyw)
 +{
 +	u32 handle = nv50_disp(wndw->plane.dev)->mast.base.vram.handle;
 +	u32 offset = asyw->image.offset;
 +	if (asyh->curs.handle != handle || asyh->curs.offset != offset) {
 +		asyh->curs.handle = handle;
 +		asyh->curs.offset = offset;
 +		asyh->set.curs = asyh->curs.visible;
 +	}
 +}
 +
 +static void
 +nv50_curs_release(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw,
 +		  struct nv50_head_atom *asyh)
 +{
 +	asyh->curs.visible = false;
 +}
 +
 +static int
 +nv50_curs_acquire(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw,
 +		  struct nv50_head_atom *asyh)
 +{
 +	int ret;
 +
 +	ret = drm_plane_helper_check_state(&asyw->state, &asyw->clip,
 +					   DRM_PLANE_HELPER_NO_SCALING,
 +					   DRM_PLANE_HELPER_NO_SCALING,
 +					   true, true);
 +	asyh->curs.visible = asyw->state.visible;
 +	if (ret || !asyh->curs.visible)
 +		return ret;
 +
 +	switch (asyw->state.fb->width) {
 +	case 32: asyh->curs.layout = 0; break;
 +	case 64: asyh->curs.layout = 1; break;
 +	default:
 +		return -EINVAL;
 +	}
 +
 +	if (asyw->state.fb->width != asyw->state.fb->height)
 +		return -EINVAL;
 +
 +	switch (asyw->state.fb->format->format) {
 +	case DRM_FORMAT_ARGB8888: asyh->curs.format = 1; break;
 +	default:
 +		WARN_ON(1);
 +		return -EINVAL;
 +	}
 +
 +	return 0;
 +}
 +
 +static void *
 +nv50_curs_dtor(struct nv50_wndw *wndw)
 +{
 +	struct nv50_curs *curs = nv50_curs(wndw);
 +	nvif_object_fini(&curs->chan);
 +	return curs;
 +}
 +
 +static const u32
 +nv50_curs_format[] = {
 +	DRM_FORMAT_ARGB8888,
 +};
 +
 +static const struct nv50_wndw_func
 +nv50_curs = {
 +	.dtor = nv50_curs_dtor,
 +	.acquire = nv50_curs_acquire,
 +	.release = nv50_curs_release,
 +	.prepare = nv50_curs_prepare,
 +	.point = nv50_curs_point,
 +	.update = nv50_curs_update,
 +};
 +
 +static int
 +nv50_curs_new(struct nouveau_drm *drm, struct nv50_head *head,
 +	      struct nv50_curs **pcurs)
 +{
 +	static const struct nvif_mclass curses[] = {
 +		{ GK104_DISP_CURSOR, 0 },
 +		{ GF110_DISP_CURSOR, 0 },
 +		{ GT214_DISP_CURSOR, 0 },
 +		{   G82_DISP_CURSOR, 0 },
 +		{  NV50_DISP_CURSOR, 0 },
 +		{}
 +	};
 +	struct nv50_disp_cursor_v0 args = {
 +		.head = head->base.index,
 +	};
 +	struct nv50_disp *disp = nv50_disp(drm->dev);
 +	struct nv50_curs *curs;
 +	int cid, ret;
 +
 +	cid = nvif_mclass(disp->disp, curses);
 +	if (cid < 0) {
 +		NV_ERROR(drm, "No supported cursor immediate class\n");
 +		return cid;
 +	}
 +
 +	if (!(curs = *pcurs = kzalloc(sizeof(*curs), GFP_KERNEL)))
 +		return -ENOMEM;
 +
 +	ret = nv50_wndw_ctor(&nv50_curs, drm->dev, DRM_PLANE_TYPE_CURSOR,
 +			     "curs", head->base.index, &disp->mast.base,
 +			     nv50_curs_format, ARRAY_SIZE(nv50_curs_format),
 +			     &curs->wndw);
 +	if (ret) {
 +		kfree(curs);
 +		return ret;
 +	}
 +
 +	ret = nvif_object_init(disp->disp, 0, curses[cid].oclass, &args,
 +			       sizeof(args), &curs->chan);
 +	if (ret) {
 +		NV_ERROR(drm, "curs%04x allocation failed: %d\n",
 +			 curses[cid].oclass, ret);
 +		return ret;
 +	}
 +
 +	return 0;
 +}
 +
 +/******************************************************************************
 + * Primary plane
 + *****************************************************************************/
 +#define nv50_base(p) container_of((p), struct nv50_base, wndw)
 +
 +struct nv50_base {
 +	struct nv50_wndw wndw;
 +	struct nv50_sync chan;
 +	int id;
 +};
 +
 +static int
 +nv50_base_notify(struct nvif_notify *notify)
 +{
 +	return NVIF_NOTIFY_KEEP;
 +}
 +
 +static void
 +nv50_base_lut(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	u32 *push;
 +	if ((push = evo_wait(&base->chan, 2))) {
 +		evo_mthd(push, 0x00e0, 1);
 +		evo_data(push, asyw->lut.enable << 30);
 +		evo_kick(push, &base->chan);
 +	}
 +}
 +
 +static void
 +nv50_base_image_clr(struct nv50_wndw *wndw)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	u32 *push;
 +	if ((push = evo_wait(&base->chan, 4))) {
 +		evo_mthd(push, 0x0084, 1);
 +		evo_data(push, 0x00000000);
 +		evo_mthd(push, 0x00c0, 1);
 +		evo_data(push, 0x00000000);
 +		evo_kick(push, &base->chan);
 +	}
 +}
 +
 +static void
 +nv50_base_image_set(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	const s32 oclass = base->chan.base.base.user.oclass;
 +	u32 *push;
 +	if ((push = evo_wait(&base->chan, 10))) {
 +		evo_mthd(push, 0x0084, 1);
 +		evo_data(push, (asyw->image.mode << 8) |
 +			       (asyw->image.interval << 4));
 +		evo_mthd(push, 0x00c0, 1);
 +		evo_data(push, asyw->image.handle);
 +		if (oclass < G82_DISP_BASE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0800, 5);
 +			evo_data(push, asyw->image.offset >> 8);
 +			evo_data(push, 0x00000000);
 +			evo_data(push, (asyw->image.h << 16) | asyw->image.w);
 +			evo_data(push, (asyw->image.layout << 20) |
 +					asyw->image.pitch |
 +					asyw->image.block);
 +			evo_data(push, (asyw->image.kind << 16) |
 +				       (asyw->image.format << 8));
 +		} else
 +		if (oclass < GF110_DISP_BASE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0800, 5);
 +			evo_data(push, asyw->image.offset >> 8);
 +			evo_data(push, 0x00000000);
 +			evo_data(push, (asyw->image.h << 16) | asyw->image.w);
 +			evo_data(push, (asyw->image.layout << 20) |
 +					asyw->image.pitch |
 +					asyw->image.block);
 +			evo_data(push, asyw->image.format << 8);
 +		} else {
 +			evo_mthd(push, 0x0400, 5);
 +			evo_data(push, asyw->image.offset >> 8);
 +			evo_data(push, 0x00000000);
 +			evo_data(push, (asyw->image.h << 16) | asyw->image.w);
 +			evo_data(push, (asyw->image.layout << 24) |
 +					asyw->image.pitch |
 +					asyw->image.block);
 +			evo_data(push, asyw->image.format << 8);
 +		}
 +		evo_kick(push, &base->chan);
 +	}
 +}
 +
 +static void
 +nv50_base_ntfy_clr(struct nv50_wndw *wndw)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	u32 *push;
 +	if ((push = evo_wait(&base->chan, 2))) {
 +		evo_mthd(push, 0x00a4, 1);
 +		evo_data(push, 0x00000000);
 +		evo_kick(push, &base->chan);
 +	}
 +}
 +
 +static void
 +nv50_base_ntfy_set(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	u32 *push;
 +	if ((push = evo_wait(&base->chan, 3))) {
 +		evo_mthd(push, 0x00a0, 2);
 +		evo_data(push, (asyw->ntfy.awaken << 30) | asyw->ntfy.offset);
 +		evo_data(push, asyw->ntfy.handle);
 +		evo_kick(push, &base->chan);
 +	}
 +}
 +
 +static void
 +nv50_base_sema_clr(struct nv50_wndw *wndw)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	u32 *push;
 +	if ((push = evo_wait(&base->chan, 2))) {
 +		evo_mthd(push, 0x0094, 1);
 +		evo_data(push, 0x00000000);
 +		evo_kick(push, &base->chan);
 +	}
 +}
 +
 +static void
 +nv50_base_sema_set(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	u32 *push;
 +	if ((push = evo_wait(&base->chan, 5))) {
 +		evo_mthd(push, 0x0088, 4);
 +		evo_data(push, asyw->sema.offset);
 +		evo_data(push, asyw->sema.acquire);
 +		evo_data(push, asyw->sema.release);
 +		evo_data(push, asyw->sema.handle);
 +		evo_kick(push, &base->chan);
 +	}
 +}
 +
 +static u32
 +nv50_base_update(struct nv50_wndw *wndw, u32 interlock)
 +{
 +	struct nv50_base *base = nv50_base(wndw);
 +	u32 *push;
 +
 +	if (!(push = evo_wait(&base->chan, 2)))
 +		return 0;
 +	evo_mthd(push, 0x0080, 1);
 +	evo_data(push, interlock);
 +	evo_kick(push, &base->chan);
 +
 +	if (base->chan.base.base.user.oclass < GF110_DISP_BASE_CHANNEL_DMA)
 +		return interlock ? 2 << (base->id * 8) : 0;
 +	return interlock ? 2 << (base->id * 4) : 0;
 +}
 +
 +static int
 +nv50_base_ntfy_wait_begun(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
 +{
 +	struct nouveau_drm *drm = nouveau_drm(wndw->plane.dev);
 +	struct nv50_disp *disp = nv50_disp(wndw->plane.dev);
 +	if (nvif_msec(&drm->client.device, 2000ULL,
 +		u32 data = nouveau_bo_rd32(disp->sync, asyw->ntfy.offset / 4);
 +		if ((data & 0xc0000000) == 0x40000000)
 +			break;
 +		usleep_range(1, 2);
 +	) < 0)
 +		return -ETIMEDOUT;
 +	return 0;
 +}
 +
 +static void
 +nv50_base_release(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw,
 +		  struct nv50_head_atom *asyh)
 +{
 +	asyh->base.cpp = 0;
 +}
 +
 +static int
 +nv50_base_acquire(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw,
 +		  struct nv50_head_atom *asyh)
 +{
 +	const struct drm_framebuffer *fb = asyw->state.fb;
 +	int ret;
 +
 +	if (!fb->format->depth)
 +		return -EINVAL;
 +
 +	ret = drm_plane_helper_check_state(&asyw->state, &asyw->clip,
 +					   DRM_PLANE_HELPER_NO_SCALING,
 +					   DRM_PLANE_HELPER_NO_SCALING,
 +					   false, true);
 +	if (ret)
 +		return ret;
 +
 +	asyh->base.depth = fb->format->depth;
 +	asyh->base.cpp = fb->format->cpp[0];
 +	asyh->base.x = asyw->state.src.x1 >> 16;
 +	asyh->base.y = asyw->state.src.y1 >> 16;
 +	asyh->base.w = asyw->state.fb->width;
 +	asyh->base.h = asyw->state.fb->height;
 +
 +	switch (fb->format->format) {
 +	case DRM_FORMAT_C8         : asyw->image.format = 0x1e; break;
 +	case DRM_FORMAT_RGB565     : asyw->image.format = 0xe8; break;
 +	case DRM_FORMAT_XRGB1555   :
 +	case DRM_FORMAT_ARGB1555   : asyw->image.format = 0xe9; break;
 +	case DRM_FORMAT_XRGB8888   :
 +	case DRM_FORMAT_ARGB8888   : asyw->image.format = 0xcf; break;
 +	case DRM_FORMAT_XBGR2101010:
 +	case DRM_FORMAT_ABGR2101010: asyw->image.format = 0xd1; break;
 +	case DRM_FORMAT_XBGR8888   :
 +	case DRM_FORMAT_ABGR8888   : asyw->image.format = 0xd5; break;
 +	default:
 +		WARN_ON(1);
 +		return -EINVAL;
 +	}
 +
 +	asyw->lut.enable = 1;
 +	asyw->set.image = true;
 +	return 0;
 +}
 +
 +static void *
 +nv50_base_dtor(struct nv50_wndw *wndw)
 +{
 +	struct nv50_disp *disp = nv50_disp(wndw->plane.dev);
 +	struct nv50_base *base = nv50_base(wndw);
 +	nv50_dmac_destroy(&base->chan.base, disp->disp);
 +	return base;
 +}
 +
 +static const u32
 +nv50_base_format[] = {
 +	DRM_FORMAT_C8,
 +	DRM_FORMAT_RGB565,
 +	DRM_FORMAT_XRGB1555,
 +	DRM_FORMAT_ARGB1555,
 +	DRM_FORMAT_XRGB8888,
 +	DRM_FORMAT_ARGB8888,
 +	DRM_FORMAT_XBGR2101010,
 +	DRM_FORMAT_ABGR2101010,
 +	DRM_FORMAT_XBGR8888,
 +	DRM_FORMAT_ABGR8888,
 +};
 +
 +static const struct nv50_wndw_func
 +nv50_base = {
 +	.dtor = nv50_base_dtor,
 +	.acquire = nv50_base_acquire,
 +	.release = nv50_base_release,
 +	.sema_set = nv50_base_sema_set,
 +	.sema_clr = nv50_base_sema_clr,
 +	.ntfy_set = nv50_base_ntfy_set,
 +	.ntfy_clr = nv50_base_ntfy_clr,
 +	.ntfy_wait_begun = nv50_base_ntfy_wait_begun,
 +	.image_set = nv50_base_image_set,
 +	.image_clr = nv50_base_image_clr,
 +	.lut = nv50_base_lut,
 +	.update = nv50_base_update,
 +};
 +
 +static int
 +nv50_base_new(struct nouveau_drm *drm, struct nv50_head *head,
 +	      struct nv50_base **pbase)
 +{
 +	struct nv50_disp *disp = nv50_disp(drm->dev);
 +	struct nv50_base *base;
 +	int ret;
 +
 +	if (!(base = *pbase = kzalloc(sizeof(*base), GFP_KERNEL)))
 +		return -ENOMEM;
 +	base->id = head->base.index;
 +	base->wndw.ntfy = EVO_FLIP_NTFY0(base->id);
 +	base->wndw.sema = EVO_FLIP_SEM0(base->id);
 +	base->wndw.data = 0x00000000;
 +
 +	ret = nv50_wndw_ctor(&nv50_base, drm->dev, DRM_PLANE_TYPE_PRIMARY,
 +			     "base", base->id, &base->chan.base,
 +			     nv50_base_format, ARRAY_SIZE(nv50_base_format),
 +			     &base->wndw);
 +	if (ret) {
 +		kfree(base);
 +		return ret;
 +	}
 +
 +	ret = nv50_base_create(&drm->client.device, disp->disp, base->id,
 +			       disp->sync->bo.offset, &base->chan);
 +	if (ret)
 +		return ret;
 +
 +	return nvif_notify_init(&base->chan.base.base.user, nv50_base_notify,
 +				false,
 +				NV50_DISP_BASE_CHANNEL_DMA_V0_NTFY_UEVENT,
 +				&(struct nvif_notify_uevent_req) {},
 +				sizeof(struct nvif_notify_uevent_req),
 +				sizeof(struct nvif_notify_uevent_rep),
 +				&base->wndw.notify);
 +}
 +
 +/******************************************************************************
 + * Head
 + *****************************************************************************/
 +static void
 +nv50_head_procamp(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 2))) {
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA)
 +			evo_mthd(push, 0x08a8 + (head->base.index * 0x400), 1);
 +		else
 +			evo_mthd(push, 0x0498 + (head->base.index * 0x300), 1);
 +		evo_data(push, (asyh->procamp.sat.sin << 20) |
 +			       (asyh->procamp.sat.cos << 8));
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_dither(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 2))) {
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA)
 +			evo_mthd(push, 0x08a0 + (head->base.index * 0x0400), 1);
 +		else
 +		if (core->base.user.oclass < GK104_DISP_CORE_CHANNEL_DMA)
 +			evo_mthd(push, 0x0490 + (head->base.index * 0x0300), 1);
 +		else
 +			evo_mthd(push, 0x04a0 + (head->base.index * 0x0300), 1);
 +		evo_data(push, (asyh->dither.mode << 3) |
 +			       (asyh->dither.bits << 1) |
 +			        asyh->dither.enable);
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_ovly(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 bounds = 0;
 +	u32 *push;
 +
 +	if (asyh->base.cpp) {
 +		switch (asyh->base.cpp) {
 +		case 8: bounds |= 0x00000500; break;
 +		case 4: bounds |= 0x00000300; break;
 +		case 2: bounds |= 0x00000100; break;
 +		default:
 +			WARN_ON(1);
 +			break;
 +		}
 +		bounds |= 0x00000001;
 +	}
 +
 +	if ((push = evo_wait(core, 2))) {
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA)
 +			evo_mthd(push, 0x0904 + head->base.index * 0x400, 1);
 +		else
 +			evo_mthd(push, 0x04d4 + head->base.index * 0x300, 1);
 +		evo_data(push, bounds);
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_base(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 bounds = 0;
 +	u32 *push;
 +
 +	if (asyh->base.cpp) {
 +		switch (asyh->base.cpp) {
 +		case 8: bounds |= 0x00000500; break;
 +		case 4: bounds |= 0x00000300; break;
 +		case 2: bounds |= 0x00000100; break;
 +		case 1: bounds |= 0x00000000; break;
 +		default:
 +			WARN_ON(1);
 +			break;
 +		}
 +		bounds |= 0x00000001;
 +	}
 +
 +	if ((push = evo_wait(core, 2))) {
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA)
 +			evo_mthd(push, 0x0900 + head->base.index * 0x400, 1);
 +		else
 +			evo_mthd(push, 0x04d0 + head->base.index * 0x300, 1);
 +		evo_data(push, bounds);
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_curs_clr(struct nv50_head *head)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 4))) {
 +		if (core->base.user.oclass < G82_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0880 + head->base.index * 0x400, 1);
 +			evo_data(push, 0x05000000);
 +		} else
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0880 + head->base.index * 0x400, 1);
 +			evo_data(push, 0x05000000);
 +			evo_mthd(push, 0x089c + head->base.index * 0x400, 1);
 +			evo_data(push, 0x00000000);
 +		} else {
 +			evo_mthd(push, 0x0480 + head->base.index * 0x300, 1);
 +			evo_data(push, 0x05000000);
 +			evo_mthd(push, 0x048c + head->base.index * 0x300, 1);
 +			evo_data(push, 0x00000000);
 +		}
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_curs_set(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 5))) {
 +		if (core->base.user.oclass < G82_DISP_BASE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0880 + head->base.index * 0x400, 2);
 +			evo_data(push, 0x80000000 | (asyh->curs.layout << 26) |
 +						    (asyh->curs.format << 24));
 +			evo_data(push, asyh->curs.offset >> 8);
 +		} else
 +		if (core->base.user.oclass < GF110_DISP_BASE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0880 + head->base.index * 0x400, 2);
 +			evo_data(push, 0x80000000 | (asyh->curs.layout << 26) |
 +						    (asyh->curs.format << 24));
 +			evo_data(push, asyh->curs.offset >> 8);
 +			evo_mthd(push, 0x089c + head->base.index * 0x400, 1);
 +			evo_data(push, asyh->curs.handle);
 +		} else {
 +			evo_mthd(push, 0x0480 + head->base.index * 0x300, 2);
 +			evo_data(push, 0x80000000 | (asyh->curs.layout << 26) |
 +						    (asyh->curs.format << 24));
 +			evo_data(push, asyh->curs.offset >> 8);
 +			evo_mthd(push, 0x048c + head->base.index * 0x300, 1);
 +			evo_data(push, asyh->curs.handle);
 +		}
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_core_clr(struct nv50_head *head)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 2))) {
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA)
 +			evo_mthd(push, 0x0874 + head->base.index * 0x400, 1);
 +		else
 +			evo_mthd(push, 0x0474 + head->base.index * 0x300, 1);
 +		evo_data(push, 0x00000000);
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_core_set(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 9))) {
 +		if (core->base.user.oclass < G82_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0860 + head->base.index * 0x400, 1);
 +			evo_data(push, asyh->core.offset >> 8);
 +			evo_mthd(push, 0x0868 + head->base.index * 0x400, 4);
 +			evo_data(push, (asyh->core.h << 16) | asyh->core.w);
 +			evo_data(push, asyh->core.layout << 20 |
 +				       (asyh->core.pitch >> 8) << 8 |
 +				       asyh->core.block);
 +			evo_data(push, asyh->core.kind << 16 |
 +				       asyh->core.format << 8);
 +			evo_data(push, asyh->core.handle);
 +			evo_mthd(push, 0x08c0 + head->base.index * 0x400, 1);
 +			evo_data(push, (asyh->core.y << 16) | asyh->core.x);
 +			/* EVO will complain with INVALID_STATE if we have an
 +			 * active cursor and (re)specify HeadSetContextDmaIso
 +			 * without also updating HeadSetOffsetCursor.
 +			 */
 +			asyh->set.curs = asyh->curs.visible;
 +		} else
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0860 + head->base.index * 0x400, 1);
 +			evo_data(push, asyh->core.offset >> 8);
 +			evo_mthd(push, 0x0868 + head->base.index * 0x400, 4);
 +			evo_data(push, (asyh->core.h << 16) | asyh->core.w);
 +			evo_data(push, asyh->core.layout << 20 |
 +				       (asyh->core.pitch >> 8) << 8 |
 +				       asyh->core.block);
 +			evo_data(push, asyh->core.format << 8);
 +			evo_data(push, asyh->core.handle);
 +			evo_mthd(push, 0x08c0 + head->base.index * 0x400, 1);
 +			evo_data(push, (asyh->core.y << 16) | asyh->core.x);
 +		} else {
 +			evo_mthd(push, 0x0460 + head->base.index * 0x300, 1);
 +			evo_data(push, asyh->core.offset >> 8);
 +			evo_mthd(push, 0x0468 + head->base.index * 0x300, 4);
 +			evo_data(push, (asyh->core.h << 16) | asyh->core.w);
 +			evo_data(push, asyh->core.layout << 24 |
 +				       (asyh->core.pitch >> 8) << 8 |
 +				       asyh->core.block);
 +			evo_data(push, asyh->core.format << 8);
 +			evo_data(push, asyh->core.handle);
 +			evo_mthd(push, 0x04b0 + head->base.index * 0x300, 1);
 +			evo_data(push, (asyh->core.y << 16) | asyh->core.x);
 +		}
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_lut_clr(struct nv50_head *head)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 4))) {
 +		if (core->base.user.oclass < G82_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0840 + (head->base.index * 0x400), 1);
 +			evo_data(push, 0x40000000);
 +		} else
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0840 + (head->base.index * 0x400), 1);
 +			evo_data(push, 0x40000000);
 +			evo_mthd(push, 0x085c + (head->base.index * 0x400), 1);
 +			evo_data(push, 0x00000000);
 +		} else {
 +			evo_mthd(push, 0x0440 + (head->base.index * 0x300), 1);
 +			evo_data(push, 0x03000000);
 +			evo_mthd(push, 0x045c + (head->base.index * 0x300), 1);
 +			evo_data(push, 0x00000000);
 +		}
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_lut_set(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 7))) {
 +		if (core->base.user.oclass < G82_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0840 + (head->base.index * 0x400), 2);
 +			evo_data(push, 0xc0000000);
 +			evo_data(push, asyh->lut.offset >> 8);
 +		} else
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0840 + (head->base.index * 0x400), 2);
 +			evo_data(push, 0xc0000000);
 +			evo_data(push, asyh->lut.offset >> 8);
 +			evo_mthd(push, 0x085c + (head->base.index * 0x400), 1);
 +			evo_data(push, asyh->lut.handle);
 +		} else {
 +			evo_mthd(push, 0x0440 + (head->base.index * 0x300), 4);
 +			evo_data(push, 0x83000000);
 +			evo_data(push, asyh->lut.offset >> 8);
 +			evo_data(push, 0x00000000);
 +			evo_data(push, 0x00000000);
 +			evo_mthd(push, 0x045c + (head->base.index * 0x300), 1);
 +			evo_data(push, asyh->lut.handle);
 +		}
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_mode(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	struct nv50_head_mode *m = &asyh->mode;
 +	u32 *push;
 +	if ((push = evo_wait(core, 14))) {
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x0804 + (head->base.index * 0x400), 2);
 +			evo_data(push, 0x00800000 | m->clock);
 +			evo_data(push, m->interlace ? 0x00000002 : 0x00000000);
 +			evo_mthd(push, 0x0810 + (head->base.index * 0x400), 7);
 +			evo_data(push, 0x00000000);
 +			evo_data(push, (m->v.active  << 16) | m->h.active );
 +			evo_data(push, (m->v.synce   << 16) | m->h.synce  );
 +			evo_data(push, (m->v.blanke  << 16) | m->h.blanke );
 +			evo_data(push, (m->v.blanks  << 16) | m->h.blanks );
 +			evo_data(push, (m->v.blank2e << 16) | m->v.blank2s);
 +			evo_data(push, asyh->mode.v.blankus);
 +			evo_mthd(push, 0x082c + (head->base.index * 0x400), 1);
 +			evo_data(push, 0x00000000);
 +		} else {
 +			evo_mthd(push, 0x0410 + (head->base.index * 0x300), 6);
 +			evo_data(push, 0x00000000);
 +			evo_data(push, (m->v.active  << 16) | m->h.active );
 +			evo_data(push, (m->v.synce   << 16) | m->h.synce  );
 +			evo_data(push, (m->v.blanke  << 16) | m->h.blanke );
 +			evo_data(push, (m->v.blanks  << 16) | m->h.blanks );
 +			evo_data(push, (m->v.blank2e << 16) | m->v.blank2s);
 +			evo_mthd(push, 0x042c + (head->base.index * 0x300), 2);
 +			evo_data(push, 0x00000000); /* ??? */
 +			evo_data(push, 0xffffff00);
 +			evo_mthd(push, 0x0450 + (head->base.index * 0x300), 3);
 +			evo_data(push, m->clock * 1000);
 +			evo_data(push, 0x00200000); /* ??? */
 +			evo_data(push, m->clock * 1000);
 +		}
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_view(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->mast.base;
 +	u32 *push;
 +	if ((push = evo_wait(core, 10))) {
 +		if (core->base.user.oclass < GF110_DISP_CORE_CHANNEL_DMA) {
 +			evo_mthd(push, 0x08a4 + (head->base.index * 0x400), 1);
 +			evo_data(push, 0x00000000);
 +			evo_mthd(push, 0x08c8 + (head->base.index * 0x400), 1);
 +			evo_data(push, (asyh->view.iH << 16) | asyh->view.iW);
 +			evo_mthd(push, 0x08d8 + (head->base.index * 0x400), 2);
 +			evo_data(push, (asyh->view.oH << 16) | asyh->view.oW);
 +			evo_data(push, (asyh->view.oH << 16) | asyh->view.oW);
 +		} else {
 +			evo_mthd(push, 0x0494 + (head->base.index * 0x300), 1);
 +			evo_data(push, 0x00000000);
 +			evo_mthd(push, 0x04b8 + (head->base.index * 0x300), 1);
 +			evo_data(push, (asyh->view.iH << 16) | asyh->view.iW);
 +			evo_mthd(push, 0x04c0 + (head->base.index * 0x300), 3);
 +			evo_data(push, (asyh->view.oH << 16) | asyh->view.oW);
 +			evo_data(push, (asyh->view.oH << 16) | asyh->view.oW);
 +			evo_data(push, (asyh->view.oH << 16) | asyh->view.oW);
 +		}
 +		evo_kick(push, core);
 +	}
 +}
 +
 +static void
 +nv50_head_flush_clr(struct nv50_head *head, struct nv50_head_atom *asyh, bool y)
 +{
 +	if (asyh->clr.core && (!asyh->set.core || y))
 +		nv50_head_lut_clr(head);
 +	if (asyh->clr.core && (!asyh->set.core || y))
 +		nv50_head_core_clr(head);
 +	if (asyh->clr.curs && (!asyh->set.curs || y))
 +		nv50_head_curs_clr(head);
 +}
 +
 +static void
 +nv50_head_flush_set(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	if (asyh->set.view   ) nv50_head_view    (head, asyh);
 +	if (asyh->set.mode   ) nv50_head_mode    (head, asyh);
 +	if (asyh->set.core   ) nv50_head_lut_set (head, asyh);
 +	if (asyh->set.core   ) nv50_head_core_set(head, asyh);
 +	if (asyh->set.curs   ) nv50_head_curs_set(head, asyh);
 +	if (asyh->set.base   ) nv50_head_base    (head, asyh);
 +	if (asyh->set.ovly   ) nv50_head_ovly    (head, asyh);
 +	if (asyh->set.dither ) nv50_head_dither  (head, asyh);
 +	if (asyh->set.procamp) nv50_head_procamp (head, asyh);
 +}
 +
 +static void
 +nv50_head_atomic_check_procamp(struct nv50_head_atom *armh,
 +			       struct nv50_head_atom *asyh,
 +			       struct nouveau_conn_atom *asyc)
 +{
 +	const int vib = asyc->procamp.color_vibrance - 100;
 +	const int hue = asyc->procamp.vibrant_hue - 90;
 +	const int adj = (vib > 0) ? 50 : 0;
 +	asyh->procamp.sat.cos = ((vib * 2047 + adj) / 100) & 0xfff;
 +	asyh->procamp.sat.sin = ((hue * 2047) / 100) & 0xfff;
 +	asyh->set.procamp = true;
 +}
 +
 +static void
 +nv50_head_atomic_check_dither(struct nv50_head_atom *armh,
 +			      struct nv50_head_atom *asyh,
 +			      struct nouveau_conn_atom *asyc)
 +{
 +	struct drm_connector *connector = asyc->state.connector;
 +	u32 mode = 0x00;
 +
 +	if (asyc->dither.mode == DITHERING_MODE_AUTO) {
 +		if (asyh->base.depth > connector->display_info.bpc * 3)
 +			mode = DITHERING_MODE_DYNAMIC2X2;
 +	} else {
 +		mode = asyc->dither.mode;
 +	}
 +
 +	if (asyc->dither.depth == DITHERING_DEPTH_AUTO) {
 +		if (connector->display_info.bpc >= 8)
 +			mode |= DITHERING_DEPTH_8BPC;
 +	} else {
 +		mode |= asyc->dither.depth;
 +	}
 +
 +	asyh->dither.enable = mode;
 +	asyh->dither.bits = mode >> 1;
 +	asyh->dither.mode = mode >> 3;
 +	asyh->set.dither = true;
 +}
 +
 +static void
 +nv50_head_atomic_check_view(struct nv50_head_atom *armh,
 +			    struct nv50_head_atom *asyh,
 +			    struct nouveau_conn_atom *asyc)
 +{
 +	struct drm_connector *connector = asyc->state.connector;
 +	struct drm_display_mode *omode = &asyh->state.adjusted_mode;
 +	struct drm_display_mode *umode = &asyh->state.mode;
 +	int mode = asyc->scaler.mode;
 +	struct edid *edid;
 +	int umode_vdisplay, omode_hdisplay, omode_vdisplay;
 +
 +	if (connector->edid_blob_ptr)
 +		edid = (struct edid *)connector->edid_blob_ptr->data;
 +	else
 +		edid = NULL;
 +
 +	if (!asyc->scaler.full) {
 +		if (mode == DRM_MODE_SCALE_NONE)
 +			omode = umode;
 +	} else {
 +		/* Non-EDID LVDS/eDP mode. */
 +		mode = DRM_MODE_SCALE_FULLSCREEN;
 +	}
 +
 +	/* For the user-specified mode, we must ignore doublescan and
 +	 * the like, but honor frame packing.
 +	 */
 +	umode_vdisplay = umode->vdisplay;
 +	if ((umode->flags & DRM_MODE_FLAG_3D_MASK) == DRM_MODE_FLAG_3D_FRAME_PACKING)
 +		umode_vdisplay += umode->vtotal;
 +	asyh->view.iW = umode->hdisplay;
 +	asyh->view.iH = umode_vdisplay;
 +	/* For the output mode, we can just use the stock helper. */
 +	drm_mode_get_hv_timing(omode, &omode_hdisplay, &omode_vdisplay);
 +	asyh->view.oW = omode_hdisplay;
 +	asyh->view.oH = omode_vdisplay;
 +
 +	/* Add overscan compensation if necessary, will keep the aspect
 +	 * ratio the same as the backend mode unless overridden by the
 +	 * user setting both hborder and vborder properties.
 +	 */
 +	if ((asyc->scaler.underscan.mode == UNDERSCAN_ON ||
 +	    (asyc->scaler.underscan.mode == UNDERSCAN_AUTO &&
 +	     drm_detect_hdmi_monitor(edid)))) {
 +		u32 bX = asyc->scaler.underscan.hborder;
 +		u32 bY = asyc->scaler.underscan.vborder;
 +		u32 r = (asyh->view.oH << 19) / asyh->view.oW;
 +
 +		if (bX) {
 +			asyh->view.oW -= (bX * 2);
 +			if (bY) asyh->view.oH -= (bY * 2);
 +			else    asyh->view.oH  = ((asyh->view.oW * r) + (r / 2)) >> 19;
 +		} else {
 +			asyh->view.oW -= (asyh->view.oW >> 4) + 32;
 +			if (bY) asyh->view.oH -= (bY * 2);
 +			else    asyh->view.oH  = ((asyh->view.oW * r) + (r / 2)) >> 19;
 +		}
 +	}
  
 -struct nv50_outp_atom {
 -	struct list_head head;
 +	/* Handle CENTER/ASPECT scaling, taking into account the areas
 +	 * removed already for overscan compensation.
 +	 */
 +	switch (mode) {
 +	case DRM_MODE_SCALE_CENTER:
 +		asyh->view.oW = min((u16)umode->hdisplay, asyh->view.oW);
 +		asyh->view.oH = min((u16)umode_vdisplay, asyh->view.oH);
 +		/* fall-through */
 +	case DRM_MODE_SCALE_ASPECT:
 +		if (asyh->view.oH < asyh->view.oW) {
 +			u32 r = (asyh->view.iW << 19) / asyh->view.iH;
 +			asyh->view.oW = ((asyh->view.oH * r) + (r / 2)) >> 19;
 +		} else {
 +			u32 r = (asyh->view.iH << 19) / asyh->view.iW;
 +			asyh->view.oH = ((asyh->view.oW * r) + (r / 2)) >> 19;
 +		}
 +		break;
 +	default:
 +		break;
 +	}
  
 -	struct drm_encoder *encoder;
 -	bool flush_disable;
 +	asyh->set.view = true;
 +}
  
 -	union nv50_outp_atom_mask {
 -		struct {
 -			bool ctrl:1;
 -		};
 -		u8 mask;
 -	} set, clr;
 -};
 +static void
 +nv50_head_atomic_check_mode(struct nv50_head *head, struct nv50_head_atom *asyh)
 +{
 +	struct drm_display_mode *mode = &asyh->state.adjusted_mode;
 +	struct nv50_head_mode *m = &asyh->mode;
 +	u32 blankus;
  
 -/******************************************************************************
 - * EVO channel
 - *****************************************************************************/
 +	drm_mode_set_crtcinfo(mode, CRTC_INTERLACE_HALVE_V | CRTC_STEREO_DOUBLE);
 +
 +	/*
 +	 * DRM modes are defined in terms of a repeating interval
 +	 * starting with the active display area.  The hardware modes
 +	 * are defined in terms of a repeating interval starting one
 +	 * unit (pixel or line) into the sync pulse.  So, add bias.
 +	 */
 +
 +	m->h.active = mode->crtc_htotal;
 +	m->h.synce  = mode->crtc_hsync_end - mode->crtc_hsync_start - 1;
 +	m->h.blanke = mode->crtc_hblank_end - mode->crtc_hsync_start - 1;
 +	m->h.blanks = m->h.blanke + mode->crtc_hdisplay;
 +
 +	m->v.active = mode->crtc_vtotal;
 +	m->v.synce  = mode->crtc_vsync_end - mode->crtc_vsync_start - 1;
 +	m->v.blanke = mode->crtc_vblank_end - mode->crtc_vsync_start - 1;
 +	m->v.blanks = m->v.blanke + mode->crtc_vdisplay;
 +
 +	/*XXX: Safe underestimate, even "0" works */
 +	blankus = (m->v.active - mode->crtc_vdisplay - 2) * m->h.active;
 +	blankus *= 1000;
 +	blankus /= mode->crtc_clock;
 +	m->v.blankus = blankus;
 +
 +	if (mode->flags & DRM_MODE_FLAG_INTERLACE) {
 +		m->v.blank2e =  m->v.active + m->v.blanke;
 +		m->v.blank2s =  m->v.blank2e + mode->crtc_vdisplay;
 +		m->v.active  = (m->v.active * 2) + 1;
 +		m->interlace = true;
 +	} else {
 +		m->v.blank2e = 0;
 +		m->v.blank2s = 1;
 +		m->interlace = false;
 +	}
 +	m->clock = mode->crtc_clock;
 +
 +	asyh->set.mode = true;
 +}
  
  static int
 -nv50_chan_create(struct nvif_device *device, struct nvif_object *disp,
 -		 const s32 *oclass, u8 head, void *data, u32 size,
 -		 struct nv50_chan *chan)
 +nv50_head_atomic_check(struct drm_crtc *crtc, struct drm_crtc_state *state)
  {
 -	struct nvif_sclass *sclass;
 -	int ret, i, n;
 -
 -	chan->device = device;
 +	struct nouveau_drm *drm = nouveau_drm(crtc->dev);
 +	struct nv50_disp *disp = nv50_disp(crtc->dev);
 +	struct nv50_head *head = nv50_head(crtc);
 +	struct nv50_head_atom *armh = nv50_head_atom(crtc->state);
 +	struct nv50_head_atom *asyh = nv50_head_atom(state);
 +	struct nouveau_conn_atom *asyc = NULL;
 +	struct drm_connector_state *conns;
 +	struct drm_connector *conn;
 +	int i;
  
 -	ret = n = nvif_object_sclass_get(disp, &sclass);
 -	if (ret < 0)
 -		return ret;
 +	NV_ATOMIC(drm, "%s atomic_check %d\n", crtc->name, asyh->state.active);
 +	if (asyh->state.active) {
 +		for_each_new_connector_in_state(asyh->state.state, conn, conns, i) {
 +			if (conns->crtc == crtc) {
 +				asyc = nouveau_conn_atom(conns);
 +				break;
 +			}
 +		}
  
 -	while (oclass[0]) {
 -		for (i = 0; i < n; i++) {
 -			if (sclass[i].oclass == oclass[0]) {
 -				ret = nvif_object_init(disp, 0, oclass[0],
 -						       data, size, &chan->user);
 -				if (ret == 0)
 -					nvif_object_map(&chan->user, NULL, 0);
 -				nvif_object_sclass_put(&sclass);
 -				return ret;
 +		if (armh->state.active) {
 +			if (asyc) {
 +				if (asyh->state.mode_changed)
 +					asyc->set.scaler = true;
 +				if (armh->base.depth != asyh->base.depth)
 +					asyc->set.dither = true;
  			}
 +		} else {
 +			if (asyc)
 +				asyc->set.mask = ~0;
 +			asyh->set.mask = ~0;
  		}
 -		oclass++;
 +
 +		if (asyh->state.mode_changed)
 +			nv50_head_atomic_check_mode(head, asyh);
 +
 +		if (asyc) {
 +			if (asyc->set.scaler)
 +				nv50_head_atomic_check_view(armh, asyh, asyc);
 +			if (asyc->set.dither)
 +				nv50_head_atomic_check_dither(armh, asyh, asyc);
 +			if (asyc->set.procamp)
 +				nv50_head_atomic_check_procamp(armh, asyh, asyc);
 +		}
 +
 +		if ((asyh->core.visible = (asyh->base.cpp != 0))) {
 +			asyh->core.x = asyh->base.x;
 +			asyh->core.y = asyh->base.y;
 +			asyh->core.w = asyh->base.w;
 +			asyh->core.h = asyh->base.h;
 +		} else
 +		if ((asyh->core.visible = asyh->curs.visible)) {
 +			/*XXX: We need to either find some way of having the
 +			 *     primary base layer appear black, while still
 +			 *     being able to display the other layers, or we
 +			 *     need to allocate a dummy black surface here.
 +			 */
 +			asyh->core.x = 0;
 +			asyh->core.y = 0;
 +			asyh->core.w = asyh->state.mode.hdisplay;
 +			asyh->core.h = asyh->state.mode.vdisplay;
 +		}
 +		asyh->core.handle = disp->mast.base.vram.handle;
 +		asyh->core.offset = 0;
 +		asyh->core.format = 0xcf;
 +		asyh->core.kind = 0;
 +		asyh->core.layout = 1;
 +		asyh->core.block = 0;
 +		asyh->core.pitch = ALIGN(asyh->core.w, 64) * 4;
 +		asyh->lut.handle = disp->mast.base.vram.handle;
 +		asyh->lut.offset = head->base.lut.nvbo->bo.offset;
 +		asyh->set.base = armh->base.cpp != asyh->base.cpp;
 +		asyh->set.ovly = armh->ovly.cpp != asyh->ovly.cpp;
 +	} else {
 +		asyh->core.visible = false;
 +		asyh->curs.visible = false;
 +		asyh->base.cpp = 0;
 +		asyh->ovly.cpp = 0;
  	}
  
 -	nvif_object_sclass_put(&sclass);
 -	return -ENOSYS;
 +	if (!drm_atomic_crtc_needs_modeset(&asyh->state)) {
 +		if (asyh->core.visible) {
 +			if (memcmp(&armh->core, &asyh->core, sizeof(asyh->core)))
 +				asyh->set.core = true;
 +		} else
 +		if (armh->core.visible) {
 +			asyh->clr.core = true;
 +		}
 +
 +		if (asyh->curs.visible) {
 +			if (memcmp(&armh->curs, &asyh->curs, sizeof(asyh->curs)))
 +				asyh->set.curs = true;
 +		} else
 +		if (armh->curs.visible) {
 +			asyh->clr.curs = true;
 +		}
 +	} else {
 +		asyh->clr.core = armh->core.visible;
 +		asyh->clr.curs = armh->curs.visible;
 +		asyh->set.core = asyh->core.visible;
 +		asyh->set.curs = asyh->curs.visible;
 +	}
 +
 +	if (asyh->clr.mask || asyh->set.mask)
 +		nv50_atom(asyh->state.state)->lock_core = true;
 +	return 0;
  }
  
  static void
@@@ -4451,7 -2173,10 +4454,14 @@@ nv50_display_create(struct drm_device *
  		goto out;
  
  	/* create crtc objects to represent the hw heads */
++<<<<<<< HEAD
 +	if (disp->disp->oclass >= GF110_DISP)
++=======
+ 	if (disp->disp->object.oclass >= GV100_DISP)
+ 		crtcs = nvif_rd32(&device->object, 0x610060) & 0xff;
+ 	else
+ 	if (disp->disp->object.oclass >= GF110_DISP)
++>>>>>>> facaed62b4cb (drm/nouveau/kms/gv100: initial support)
  		crtcs = nvif_rd32(&device->object, 0x612004) & 0xf;
  	else
  		crtcs = 0x3;
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/atom.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/core.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/core.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/curs.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/disp.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/head.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/head.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/head917d.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/wndw.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/wndw.h
* Unmerged path drivers/gpu/drm/amd/powerplay/inc/pp_soc15.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/Kbuild
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/atom.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/core.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/core.h
diff --git a/drivers/gpu/drm/nouveau/dispnv50/corec37d.c b/drivers/gpu/drm/nouveau/dispnv50/corec37d.c
new file mode 100644
index 000000000000..b5c17c948918
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/corec37d.c
@@ -0,0 +1,110 @@
+/*
+ * Copyright 2018 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include "core.h"
+#include "head.h"
+
+#include <nouveau_bo.h>
+
+static void
+corec37d_update(struct nv50_core *core, u32 *interlock, bool ntfy)
+{
+	u32 *push;
+	if ((push = evo_wait(&core->chan, 9))) {
+		if (ntfy) {
+			evo_mthd(push, 0x020c, 1);
+			evo_data(push, 0x00001000 | NV50_DISP_CORE_NTFY);
+		}
+
+		evo_mthd(push, 0x0218, 2);
+		evo_data(push, interlock[NV50_DISP_INTERLOCK_CURS]);
+		evo_data(push, interlock[NV50_DISP_INTERLOCK_WNDW]);
+		evo_mthd(push, 0x0200, 1);
+		evo_data(push, 0x00000001);
+
+		if (ntfy) {
+			evo_mthd(push, 0x020c, 1);
+			evo_data(push, 0x00000000);
+		}
+		evo_kick(push, &core->chan);
+	}
+}
+
+int
+corec37d_ntfy_wait_done(struct nouveau_bo *bo, u32 offset,
+			struct nvif_device *device)
+{
+	u32 data;
+	s64 time = nvif_msec(device, 2000ULL,
+		data = nouveau_bo_rd32(bo, offset / 4 + 0);
+		if ((data & 0xc0000000) == 0x80000000)
+			break;
+		usleep_range(1, 2);
+	);
+	return time < 0 ? time : 0;
+}
+
+void
+corec37d_ntfy_init(struct nouveau_bo *bo, u32 offset)
+{
+	nouveau_bo_wr32(bo, offset / 4 + 0, 0x00000000);
+	nouveau_bo_wr32(bo, offset / 4 + 1, 0x00000000);
+	nouveau_bo_wr32(bo, offset / 4 + 2, 0x00000000);
+	nouveau_bo_wr32(bo, offset / 4 + 3, 0x00000000);
+}
+
+void
+corec37d_init(struct nv50_core *core)
+{
+	const u32 windows = 8; /*XXX*/
+	u32 *push, i;
+	if ((push = evo_wait(&core->chan, 2 + 6 * windows + 2))) {
+		evo_mthd(push, 0x0208, 1);
+		evo_data(push, core->chan.sync.handle);
+		for (i = 0; i < windows; i++) {
+			evo_mthd(push, 0x1000 + (i * 0x080), 3);
+			evo_data(push, i >> 1);
+			evo_data(push, 0x00000017);
+			evo_data(push, 0x00000000);
+			evo_mthd(push, 0x1010 + (i * 0x080), 1);
+			evo_data(push, 0x00127fff);
+		}
+		evo_mthd(push, 0x0200, 1);
+		evo_data(push, 0x00000001);
+		evo_kick(push, &core->chan);
+	}
+}
+
+static const struct nv50_core_func
+corec37d = {
+	.init = corec37d_init,
+	.ntfy_init = corec37d_ntfy_init,
+	.ntfy_wait_done = corec37d_ntfy_wait_done,
+	.update = corec37d_update,
+	.head = &headc37d,
+	.sor = &sorc37d,
+};
+
+int
+corec37d_new(struct nouveau_drm *drm, s32 oclass, struct nv50_core **pcore)
+{
+	return core507d_new_(&corec37d, drm, oclass, pcore);
+}
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/curs.h
diff --git a/drivers/gpu/drm/nouveau/dispnv50/cursc37a.c b/drivers/gpu/drm/nouveau/dispnv50/cursc37a.c
new file mode 100644
index 000000000000..23fb29d41efe
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/cursc37a.c
@@ -0,0 +1,50 @@
+/*
+ * Copyright 2018 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include "curs.h"
+#include "atom.h"
+
+static void
+cursc37a_update(struct nv50_wndw *wndw, u32 *interlock)
+{
+	nvif_wr32(&wndw->wimm.base.user, 0x0200, 0x00000001);
+}
+
+static void
+cursc37a_point(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
+{
+	nvif_wr32(&wndw->wimm.base.user, 0x0208, asyw->point.y << 16 |
+						 asyw->point.x);
+}
+
+static const struct nv50_wimm_func
+cursc37a = {
+	.point = cursc37a_point,
+	.update = cursc37a_update,
+};
+
+int
+cursc37a_new(struct nouveau_drm *drm, int head, s32 oclass,
+	     struct nv50_wndw **pwndw)
+{
+	return curs507a_new_(&cursc37a, drm, head, oclass,
+			     0x00000001 << head, pwndw);
+}
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/disp.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/disp.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/head.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/head.h
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/head917d.c
diff --git a/drivers/gpu/drm/nouveau/dispnv50/headc37d.c b/drivers/gpu/drm/nouveau/dispnv50/headc37d.c
new file mode 100644
index 000000000000..989c14083066
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/headc37d.c
@@ -0,0 +1,212 @@
+/*
+ * Copyright 2018 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include "head.h"
+#include "atom.h"
+#include "core.h"
+
+static void
+headc37d_or(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 2))) {
+		/*XXX: This is a dirty hack until OR depth handling is
+		 *     improved later for deep colour etc.
+		 */
+		switch (asyh->or.depth) {
+		case 6: asyh->or.depth = 5; break;
+		case 5: asyh->or.depth = 4; break;
+		case 2: asyh->or.depth = 1; break;
+		case 0:	asyh->or.depth = 4; break;
+		default:
+			WARN_ON(1);
+			break;
+		}
+
+		evo_mthd(push, 0x2004 + (head->base.index * 0x400), 1);
+		evo_data(push, 0x00000001 |
+			       asyh->or.depth << 4 |
+			       asyh->or.nvsync << 3 |
+			       asyh->or.nhsync << 2);
+		evo_kick(push, core);
+	}
+}
+
+static void
+headc37d_procamp(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 2))) {
+		evo_mthd(push, 0x2000 + (head->base.index * 0x400), 1);
+		evo_data(push, 0x80000000 |
+			       asyh->procamp.sat.sin << 16 |
+			       asyh->procamp.sat.cos << 4);
+		evo_kick(push, core);
+	}
+}
+
+static void
+headc37d_dither(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 2))) {
+		evo_mthd(push, 0x2018 + (head->base.index * 0x0400), 1);
+		evo_data(push, asyh->dither.mode << 8 |
+			       asyh->dither.bits << 4 |
+			       asyh->dither.enable);
+		evo_kick(push, core);
+	}
+}
+
+static void
+headc37d_curs_clr(struct nv50_head *head)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 4))) {
+		evo_mthd(push, 0x209c + head->base.index * 0x400, 1);
+		evo_data(push, 0x000000cf);
+		evo_mthd(push, 0x2088 + head->base.index * 0x400, 1);
+		evo_data(push, 0x00000000);
+		evo_kick(push, core);
+	}
+}
+
+static void
+headc37d_curs_set(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 7))) {
+		evo_mthd(push, 0x209c + head->base.index * 0x400, 2);
+		evo_data(push, 0x80000000 |
+			       asyh->curs.layout << 8 |
+			       asyh->curs.format << 0);
+		evo_data(push, 0x000072ff);
+		evo_mthd(push, 0x2088 + head->base.index * 0x400, 1);
+		evo_data(push, asyh->curs.handle);
+		evo_mthd(push, 0x2090 + head->base.index * 0x400, 1);
+		evo_data(push, asyh->curs.offset >> 8);
+		evo_kick(push, core);
+	}
+}
+
+static int
+headc37d_curs_format(struct nv50_head *head, struct nv50_wndw_atom *asyw,
+		     struct nv50_head_atom *asyh)
+{
+	asyh->curs.format = asyw->image.format;
+	return 0;
+}
+
+static void
+headc37d_olut_clr(struct nv50_head *head)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 2))) {
+		evo_mthd(push, 0x20ac + (head->base.index * 0x400), 1);
+		evo_data(push, 0x00000000);
+		evo_kick(push, core);
+	}
+}
+
+static void
+headc37d_olut_set(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 4))) {
+		evo_mthd(push, 0x20a4 + (head->base.index * 0x400), 3);
+		evo_data(push, asyh->olut.output_mode << 8 |
+			       asyh->olut.range << 4 |
+			       asyh->olut.size);
+		evo_data(push, asyh->olut.offset >> 8);
+		evo_data(push, asyh->olut.handle);
+		evo_kick(push, core);
+	}
+}
+
+static void
+headc37d_olut(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	asyh->olut.mode = 2;
+	asyh->olut.size = 0;
+	asyh->olut.range = 0;
+	asyh->olut.output_mode = 1;
+}
+
+static void
+headc37d_mode(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	struct nv50_head_mode *m = &asyh->mode;
+	u32 *push;
+	if ((push = evo_wait(core, 12))) {
+		evo_mthd(push, 0x2064 + (head->base.index * 0x400), 5);
+		evo_data(push, (m->v.active  << 16) | m->h.active );
+		evo_data(push, (m->v.synce   << 16) | m->h.synce  );
+		evo_data(push, (m->v.blanke  << 16) | m->h.blanke );
+		evo_data(push, (m->v.blanks  << 16) | m->h.blanks );
+		evo_data(push, (m->v.blank2e << 16) | m->v.blank2s);
+		evo_mthd(push, 0x200c + (head->base.index * 0x400), 1);
+		evo_data(push, m->clock * 1000);
+		evo_mthd(push, 0x2028 + (head->base.index * 0x400), 1);
+		evo_data(push, m->clock * 1000);
+		/*XXX: HEAD_USAGE_BOUNDS, doesn't belong here. */
+		evo_mthd(push, 0x2030 + (head->base.index * 0x400), 1);
+		evo_data(push, 0x00000124);
+		evo_kick(push, core);
+	}
+}
+
+static void
+headc37d_view(struct nv50_head *head, struct nv50_head_atom *asyh)
+{
+	struct nv50_dmac *core = &nv50_disp(head->base.base.dev)->core->chan;
+	u32 *push;
+	if ((push = evo_wait(core, 4))) {
+		evo_mthd(push, 0x204c + (head->base.index * 0x400), 1);
+		evo_data(push, (asyh->view.iH << 16) | asyh->view.iW);
+		evo_mthd(push, 0x2058 + (head->base.index * 0x400), 1);
+		evo_data(push, (asyh->view.oH << 16) | asyh->view.oW);
+		evo_kick(push, core);
+	}
+}
+
+const struct nv50_head_func
+headc37d = {
+	.view = headc37d_view,
+	.mode = headc37d_mode,
+	.olut = headc37d_olut,
+	.olut_set = headc37d_olut_set,
+	.olut_clr = headc37d_olut_clr,
+	.curs_layout = head917d_curs_layout,
+	.curs_format = headc37d_curs_format,
+	.curs_set = headc37d_curs_set,
+	.curs_clr = headc37d_curs_clr,
+	.dither = headc37d_dither,
+	.procamp = headc37d_procamp,
+	.or = headc37d_or,
+};
diff --git a/drivers/gpu/drm/nouveau/dispnv50/sorc37d.c b/drivers/gpu/drm/nouveau/dispnv50/sorc37d.c
new file mode 100644
index 000000000000..dff059241c5d
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/sorc37d.c
@@ -0,0 +1,39 @@
+/*
+ * Copyright 2018 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include "core.h"
+
+static void
+sorc37d_ctrl(struct nv50_core *core, int or, u32 ctrl,
+	     struct nv50_head_atom *asyh)
+{
+	u32 *push;
+	if ((push = evo_wait(&core->chan, 2))) {
+		evo_mthd(push, 0x0300 + (or * 0x20), 1);
+		evo_data(push, ctrl);
+		evo_kick(push, &core->chan);
+	}
+}
+
+const struct nv50_outp_func
+sorc37d = {
+	.ctrl = sorc37d_ctrl,
+};
diff --git a/drivers/gpu/drm/nouveau/dispnv50/wimm.c b/drivers/gpu/drm/nouveau/dispnv50/wimm.c
new file mode 100644
index 000000000000..fc36e0696407
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/wimm.c
@@ -0,0 +1,47 @@
+/*
+ * Copyright 2018 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include "wimm.h"
+
+#include <nvif/class.h>
+
+int
+nv50_wimm_init(struct nouveau_drm *drm, struct nv50_wndw *wndw)
+{
+	struct {
+		s32 oclass;
+		int version;
+		int (*init)(struct nouveau_drm *, s32, struct nv50_wndw *);
+	} wimms[] = {
+		{ GV100_DISP_WINDOW_IMM_CHANNEL_DMA, 0, wimmc37b_init },
+		{}
+	};
+	struct nv50_disp *disp = nv50_disp(drm->dev);
+	int cid;
+
+	cid = nvif_mclass(&disp->disp->object, wimms);
+	if (cid < 0) {
+		NV_ERROR(drm, "No supported window immediate class\n");
+		return cid;
+	}
+
+	return wimms[cid].init(drm, wimms[cid].oclass, wndw);
+}
diff --git a/drivers/gpu/drm/nouveau/dispnv50/wimm.h b/drivers/gpu/drm/nouveau/dispnv50/wimm.h
new file mode 100644
index 000000000000..363052309be9
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/wimm.h
@@ -0,0 +1,8 @@
+#ifndef __NV50_KMS_WIMM_H__
+#define __NV50_KMS_WIMM_H__
+#include "wndw.h"
+
+int nv50_wimm_init(struct nouveau_drm *drm, struct nv50_wndw *);
+
+int wimmc37b_init(struct nouveau_drm *, s32, struct nv50_wndw *);
+#endif
diff --git a/drivers/gpu/drm/nouveau/dispnv50/wimmc37b.c b/drivers/gpu/drm/nouveau/dispnv50/wimmc37b.c
new file mode 100644
index 000000000000..9103b8494279
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/wimmc37b.c
@@ -0,0 +1,86 @@
+/*
+ * Copyright 2018 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include "wimm.h"
+#include "atom.h"
+#include "wndw.h"
+
+#include <nvif/clc37b.h>
+
+static void
+wimmc37b_update(struct nv50_wndw *wndw, u32 *interlock)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wimm, 2))) {
+		evo_mthd(push, 0x0200, 1);
+		if (interlock[NV50_DISP_INTERLOCK_WNDW] & wndw->interlock.data)
+			evo_data(push, 0x00000003);
+		else
+			evo_data(push, 0x00000001);
+		evo_kick(push, &wndw->wimm);
+	}
+}
+
+static void
+wimmc37b_point(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wimm, 2))) {
+		evo_mthd(push, 0x0208, 1);
+		evo_data(push, asyw->point.y << 16 | asyw->point.x);
+		evo_kick(push, &wndw->wimm);
+	}
+}
+
+static const struct nv50_wimm_func
+wimmc37b = {
+	.point = wimmc37b_point,
+	.update = wimmc37b_update,
+};
+
+static int
+wimmc37b_init_(const struct nv50_wimm_func *func, struct nouveau_drm *drm,
+	       s32 oclass, struct nv50_wndw *wndw)
+{
+	struct nvc37b_window_imm_channel_dma_v0 args = {
+		.pushbuf = 0xb0007b00 | wndw->id,
+		.index = wndw->id,
+	};
+	struct nv50_disp *disp = nv50_disp(drm->dev);
+	int ret;
+
+	ret = nv50_dmac_create(&drm->client.device, &disp->disp->object,
+			       &oclass, 0, &args, sizeof(args), 0,
+			       &wndw->wimm);
+	if (ret) {
+		NV_ERROR(drm, "wimm%04x allocation failed: %d\n", oclass, ret);
+		return ret;
+	}
+
+	wndw->immd = func;
+	return 0;
+}
+
+int
+wimmc37b_init(struct nouveau_drm *drm, s32 oclass, struct nv50_wndw *wndw)
+{
+	return wimmc37b_init_(&wimmc37b, drm, oclass, wndw);
+}
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/wndw.c
* Unmerged path drivers/gpu/drm/nouveau/dispnv50/wndw.h
diff --git a/drivers/gpu/drm/nouveau/dispnv50/wndwc37e.c b/drivers/gpu/drm/nouveau/dispnv50/wndwc37e.c
new file mode 100644
index 000000000000..44afb0f069a5
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/dispnv50/wndwc37e.c
@@ -0,0 +1,278 @@
+/*
+ * Copyright 2018 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include "wndw.h"
+#include "atom.h"
+
+#include <drm/drm_atomic_helper.h>
+#include <drm/drm_plane_helper.h>
+#include <nouveau_bo.h>
+
+#include <nvif/clc37e.h>
+
+static void
+wndwc37e_ilut_clr(struct nv50_wndw *wndw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 2))) {
+		evo_mthd(push, 0x02b8, 1);
+		evo_data(push, 0x00000000);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_ilut_set(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 4))) {
+		evo_mthd(push, 0x02b0, 3);
+		evo_data(push, asyw->xlut.i.output_mode << 8 |
+			       asyw->xlut.i.range << 4 |
+			       asyw->xlut.i.size);
+		evo_data(push, asyw->xlut.i.offset >> 8);
+		evo_data(push, asyw->xlut.handle);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_ilut(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
+{
+	asyw->xlut.i.mode = 2;
+	asyw->xlut.i.size = 0;
+	asyw->xlut.i.range = 0;
+	asyw->xlut.i.output_mode = 1;
+}
+
+static void
+wndwc37e_image_clr(struct nv50_wndw *wndw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 4))) {
+		evo_mthd(push, 0x0308, 1);
+		evo_data(push, 0x00000000);
+		evo_mthd(push, 0x0240, 1);
+		evo_data(push, 0x00000000);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_image_set(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
+{
+	u32 *push;
+
+	if (!(push = evo_wait(&wndw->wndw, 25)))
+		return;
+
+	evo_mthd(push, 0x0308, 1);
+	evo_data(push, asyw->image.mode << 4 | asyw->image.interval);
+	evo_mthd(push, 0x0224, 4);
+	evo_data(push, asyw->image.h << 16 | asyw->image.w);
+	evo_data(push, asyw->image.layout << 4 | asyw->image.blockh);
+	evo_data(push, asyw->image.colorspace << 8 | asyw->image.format);
+	evo_data(push, asyw->image.blocks[0] | (asyw->image.pitch[0] >> 6));
+	evo_mthd(push, 0x0240, 1);
+	evo_data(push, asyw->image.handle[0]);
+	evo_mthd(push, 0x0260, 1);
+	evo_data(push, asyw->image.offset[0] >> 8);
+	evo_mthd(push, 0x0290, 1);
+	evo_data(push, (asyw->state.src_y >> 16) << 16 |
+		       (asyw->state.src_x >> 16));
+	evo_mthd(push, 0x0298, 1);
+	evo_data(push, (asyw->state.src_h >> 16) << 16 |
+		       (asyw->state.src_w >> 16));
+	evo_mthd(push, 0x02a4, 1);
+	evo_data(push, asyw->state.crtc_h << 16 |
+		       asyw->state.crtc_w);
+
+	/*XXX: Composition-related stuff.  Need to implement properly. */
+	evo_mthd(push, 0x02ec, 1);
+	evo_data(push, (2 - (wndw->id & 1)) << 4);
+	evo_mthd(push, 0x02f4, 5);
+	evo_data(push, 0x00000011);
+	evo_data(push, 0xffff0000);
+	evo_data(push, 0xffff0000);
+	evo_data(push, 0xffff0000);
+	evo_data(push, 0xffff0000);
+	evo_kick(push, &wndw->wndw);
+}
+
+static void
+wndwc37e_ntfy_clr(struct nv50_wndw *wndw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 2))) {
+		evo_mthd(push, 0x021c, 1);
+		evo_data(push, 0x00000000);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_ntfy_set(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 3))) {
+		evo_mthd(push, 0x021c, 2);
+		evo_data(push, asyw->ntfy.handle);
+		evo_data(push, asyw->ntfy.offset | asyw->ntfy.awaken);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_sema_clr(struct nv50_wndw *wndw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 2))) {
+		evo_mthd(push, 0x0218, 1);
+		evo_data(push, 0x00000000);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_sema_set(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 5))) {
+		evo_mthd(push, 0x020c, 4);
+		evo_data(push, asyw->sema.offset);
+		evo_data(push, asyw->sema.acquire);
+		evo_data(push, asyw->sema.release);
+		evo_data(push, asyw->sema.handle);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_update(struct nv50_wndw *wndw, u32 *interlock)
+{
+	u32 *push;
+	if ((push = evo_wait(&wndw->wndw, 5))) {
+		evo_mthd(push, 0x0370, 2);
+		evo_data(push, interlock[NV50_DISP_INTERLOCK_CURS] << 1 |
+			       interlock[NV50_DISP_INTERLOCK_CORE]);
+		evo_data(push, interlock[NV50_DISP_INTERLOCK_WNDW]);
+		evo_mthd(push, 0x0200, 1);
+		if (interlock[NV50_DISP_INTERLOCK_WIMM] & wndw->interlock.data)
+			evo_data(push, 0x00001001);
+		else
+			evo_data(push, 0x00000001);
+		evo_kick(push, &wndw->wndw);
+	}
+}
+
+static void
+wndwc37e_release(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw,
+		 struct nv50_head_atom *asyh)
+{
+}
+
+static int
+wndwc37e_acquire(struct nv50_wndw *wndw, struct nv50_wndw_atom *asyw,
+		 struct nv50_head_atom *asyh)
+{
+	return drm_atomic_helper_check_plane_state(&asyw->state, &asyh->state,
+						   DRM_PLANE_HELPER_NO_SCALING,
+						   DRM_PLANE_HELPER_NO_SCALING,
+						   true, true);
+}
+
+static const u32
+wndwc37e_format[] = {
+	DRM_FORMAT_C8,
+	DRM_FORMAT_YUYV,
+	DRM_FORMAT_UYVY,
+	DRM_FORMAT_XRGB8888,
+	DRM_FORMAT_ARGB8888,
+	DRM_FORMAT_RGB565,
+	DRM_FORMAT_XRGB1555,
+	DRM_FORMAT_ARGB1555,
+	DRM_FORMAT_XBGR2101010,
+	DRM_FORMAT_ABGR2101010,
+	DRM_FORMAT_XBGR8888,
+	DRM_FORMAT_ABGR8888,
+	DRM_FORMAT_XRGB2101010,
+	DRM_FORMAT_ARGB2101010,
+	0
+};
+
+static const struct nv50_wndw_func
+wndwc37e = {
+	.acquire = wndwc37e_acquire,
+	.release = wndwc37e_release,
+	.sema_set = wndwc37e_sema_set,
+	.sema_clr = wndwc37e_sema_clr,
+	.ntfy_set = wndwc37e_ntfy_set,
+	.ntfy_clr = wndwc37e_ntfy_clr,
+	.ntfy_reset = corec37d_ntfy_init,
+	.ntfy_wait_begun = base507c_ntfy_wait_begun,
+	.ilut = wndwc37e_ilut,
+	.xlut_set = wndwc37e_ilut_set,
+	.xlut_clr = wndwc37e_ilut_clr,
+	.image_set = wndwc37e_image_set,
+	.image_clr = wndwc37e_image_clr,
+	.update = wndwc37e_update,
+};
+
+static int
+wndwc37e_new_(const struct nv50_wndw_func *func, struct nouveau_drm *drm,
+	      enum drm_plane_type type, int index, s32 oclass, u32 heads,
+	      struct nv50_wndw **pwndw)
+{
+	struct nvc37e_window_channel_dma_v0 args = {
+		.pushbuf = 0xb0007e00 | index,
+		.index = index,
+	};
+	struct nv50_disp *disp = nv50_disp(drm->dev);
+	struct nv50_wndw *wndw;
+	int ret;
+
+	ret = nv50_wndw_new_(func, drm->dev, type, "wndw", index,
+			     wndwc37e_format, heads, NV50_DISP_INTERLOCK_WNDW,
+			     BIT(index), &wndw);
+	if (*pwndw = wndw, ret)
+		return ret;
+
+	ret = nv50_dmac_create(&drm->client.device, &disp->disp->object,
+			       &oclass, 0, &args, sizeof(args),
+			       disp->sync->bo.offset, &wndw->wndw);
+	if (ret) {
+		NV_ERROR(drm, "qndw%04x allocation failed: %d\n", oclass, ret);
+		return ret;
+	}
+
+	wndw->ntfy = NV50_DISP_WNDW_NTFY(wndw->id);
+	wndw->sema = NV50_DISP_WNDW_SEM0(wndw->id);
+	wndw->data = 0x00000000;
+	return 0;
+}
+
+int
+wndwc37e_new(struct nouveau_drm *drm, enum drm_plane_type type, int index,
+	     s32 oclass, struct nv50_wndw **pwndw)
+{
+	return wndwc37e_new_(&wndwc37e, drm, type, index, oclass,
+			     BIT(index >> 1), pwndw);
+}
