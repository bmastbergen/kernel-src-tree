dax: store pfns in the radix

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 3fe0791c295cfd3cd735de7a32cc0780949c009f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3fe0791c.failed

In preparation for examining the busy state of dax pages in the truncate
path, switch from sectors to pfns in the radix.

	Cc: Jeff Moyer <jmoyer@redhat.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 3fe0791c295cfd3cd735de7a32cc0780949c009f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 879d2cfa39b7,b646a46e4d12..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -58,6 -58,39 +58,42 @@@ static int __init init_dax_wait_table(v
  }
  fs_initcall(init_dax_wait_table);
  
++<<<<<<< HEAD
++=======
+ /*
+  * We use lowest available bit in exceptional entry for locking, one bit for
+  * the entry size (PMD) and two more to tell us if the entry is a zero page or
+  * an empty entry that is just used for locking.  In total four special bits.
+  *
+  * If the PMD bit isn't set the entry has size PAGE_SIZE, and if the ZERO_PAGE
+  * and EMPTY bits aren't set the entry is a normal DAX entry with a filesystem
+  * block allocation.
+  */
+ #define RADIX_DAX_SHIFT		(RADIX_TREE_EXCEPTIONAL_SHIFT + 4)
+ #define RADIX_DAX_ENTRY_LOCK	(1 << RADIX_TREE_EXCEPTIONAL_SHIFT)
+ #define RADIX_DAX_PMD		(1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 1))
+ #define RADIX_DAX_ZERO_PAGE	(1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 2))
+ #define RADIX_DAX_EMPTY		(1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 3))
+ 
+ static unsigned long dax_radix_pfn(void *entry)
+ {
+ 	return (unsigned long)entry >> RADIX_DAX_SHIFT;
+ }
+ 
+ static void *dax_radix_locked_entry(unsigned long pfn, unsigned long flags)
+ {
+ 	return (void *)(RADIX_TREE_EXCEPTIONAL_ENTRY | flags |
+ 			(pfn << RADIX_DAX_SHIFT) | RADIX_DAX_ENTRY_LOCK);
+ }
+ 
+ static unsigned int dax_radix_order(void *entry)
+ {
+ 	if ((unsigned long)entry & RADIX_DAX_PMD)
+ 		return PMD_SHIFT - PAGE_SHIFT;
+ 	return 0;
+ }
+ 
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  static int dax_is_pmd_entry(void *entry)
  {
  	return (unsigned long)entry & RADIX_DAX_PMD;
@@@ -571,53 -525,30 +607,62 @@@ static int copy_user_dax(struct block_d
   */
  static void *dax_insert_mapping_entry(struct address_space *mapping,
  				      struct vm_fault *vmf,
++<<<<<<< HEAD
 +				      void *entry, sector_t sector,
 +				      unsigned long flags)
 +{
 +	struct radix_tree_root *page_tree = &mapping->page_tree;
 +	int error = 0;
 +	bool hole_fill = false;
 +	void *new_entry;
++=======
+ 				      void *entry, pfn_t pfn_t,
+ 				      unsigned long flags, bool dirty)
+ {
+ 	struct radix_tree_root *page_tree = &mapping->page_tree;
+ 	unsigned long pfn = pfn_t_to_pfn(pfn_t);
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  	pgoff_t index = vmf->pgoff;
+ 	void *new_entry;
  
 -	if (dirty)
 +	if (vmf->flags & FAULT_FLAG_WRITE)
  		__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);
  
 -	if (dax_is_zero_entry(entry) && !(flags & RADIX_DAX_ZERO_PAGE)) {
 -		/* we are replacing a zero page with block mapping */
 -		if (dax_is_pmd_entry(entry))
 -			unmap_mapping_pages(mapping, index & ~PG_PMD_COLOUR,
 -							PG_PMD_NR, false);
 -		else /* pte entry */
 -			unmap_mapping_pages(mapping, vmf->pgoff, 1, false);
 +	/* Replacing hole page with block mapping? */
 +	if (!radix_tree_exceptional_entry(entry)) {
 +		hole_fill = true;
 +		/*
 +		 * Unmap the page now before we remove it from page cache below.
 +		 * The page is locked so it cannot be faulted in again.
 +		 */
 +		unmap_mapping_range(mapping, vmf->pgoff << PAGE_SHIFT,
 +				    PAGE_SIZE, 0);
 +		error = radix_tree_preload(mapping_gfp_mask(mapping) & ~__GFP_HIGHMEM);
 +		if (error)
 +			return ERR_PTR(error);
 +	} else if (dax_is_zero_entry(entry) && !(flags & RADIX_DAX_HZP)) {
 +		/* replacing huge zero page with PMD block mapping */
 +		unmap_mapping_range(mapping,
 +			(vmf->pgoff << PAGE_SHIFT) & PMD_MASK, PMD_SIZE, 0);
 +
  	}
  
  	spin_lock_irq(&mapping->tree_lock);
- 	new_entry = dax_radix_locked_entry(sector, flags);
+ 	new_entry = dax_radix_locked_entry(pfn, flags);
  
 -	if (dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {
 +	if (hole_fill) {
 +		__delete_from_page_cache(entry, NULL);
 +		mem_cgroup_uncharge_page(entry);
 +		/* Drop pagecache reference */
 +		page_cache_release(entry);
 +		error = __radix_tree_insert(page_tree, index,
 +				dax_radix_order(new_entry), new_entry);
 +		if (error) {
 +			new_entry = ERR_PTR(error);
 +			goto unlock;
 +		}
 +		mapping->nrexceptional++;
 +	} else if (dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {
  		/*
  		 * Only swap our new entry into the radix tree if the current
  		 * entry is a zero page or an empty entry.  If a normal PTE or
@@@ -726,23 -652,19 +771,20 @@@ unlock_pte
  			pte_unmap_unlock(ptep, ptl);
  		}
  
 -		mmu_notifier_invalidate_range_end(vma->vm_mm, start, end);
 +		if (changed)
 +			mmu_notifier_invalidate_page(vma->vm_mm, address);
  	}
 -	i_mmap_unlock_read(mapping);
 +	mutex_unlock(&mapping->i_mmap_mutex);
  }
  
- static int dax_writeback_one(struct block_device *bdev,
- 		struct dax_device *dax_dev, struct address_space *mapping,
- 		pgoff_t index, void *entry)
+ static int dax_writeback_one(struct dax_device *dax_dev,
+ 		struct address_space *mapping, pgoff_t index, void *entry)
  {
  	struct radix_tree_root *page_tree = &mapping->page_tree;
- 	void *entry2, **slot, *kaddr;
- 	long ret = 0, id;
- 	sector_t sector;
- 	pgoff_t pgoff;
+ 	void *entry2, **slot;
+ 	unsigned long pfn;
+ 	long ret = 0;
  	size_t size;
- 	pfn_t pfn;
  
  	/*
  	 * A page got tagged dirty in DAX mapping? Something is seriously
@@@ -787,33 -709,15 +829,38 @@@
  	/*
  	 * Even if dax_writeback_mapping_range() was given a wbc->range_start
  	 * in the middle of a PMD, the 'index' we are given will be aligned to
- 	 * the start index of the PMD, as will the sector we pull from
- 	 * 'entry'.  This allows us to flush for PMD_SIZE and not have to
- 	 * worry about partial PMD writebacks.
+ 	 * the start index of the PMD, as will the pfn we pull from 'entry'.
+ 	 * This allows us to flush for PMD_SIZE and not have to worry about
+ 	 * partial PMD writebacks.
  	 */
- 	sector = dax_radix_sector(entry);
+ 	pfn = dax_radix_pfn(entry);
  	size = PAGE_SIZE << dax_radix_order(entry);
  
++<<<<<<< HEAD
 +	id = dax_read_lock();
 +	ret = bdev_dax_pgoff(bdev, sector, size, &pgoff);
 +	if (ret)
 +		goto dax_unlock;
 +
 +	/*
 +	 * dax_direct_access() may sleep, so cannot hold tree_lock over
 +	 * its invocation.
 +	 */
 +	ret = dax_direct_access(dax_dev, pgoff, size / PAGE_SIZE, &kaddr, &pfn);
 +	if (ret < 0)
 +		goto dax_unlock;
 +
 +	if (WARN_ON_ONCE(ret < size / PAGE_SIZE)) {
 +		ret = -EIO;
 +		goto dax_unlock;
 +	}
 +
 +	dax_mapping_entry_mkclean(mapping, index, pfn_t_to_pfn(pfn));
 +	wb_cache_pmem(kaddr, size);
++=======
+ 	dax_mapping_entry_mkclean(mapping, index, pfn);
+ 	dax_flush(dax_dev, page_address(pfn_to_page(pfn)), size);
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  	/*
  	 * After we have flushed the cache, we can clear the dirty tag. There
  	 * cannot be new dirty data in the pfn after the flush has completed as
@@@ -824,9 -728,7 +871,13 @@@
  	radix_tree_tag_clear(page_tree, index, PAGECACHE_TAG_DIRTY);
  	spin_unlock_irq(&mapping->tree_lock);
  	trace_dax_writeback_one(mapping->host, index, size >> PAGE_SHIFT);
++<<<<<<< HEAD
 + dax_unlock:
 +	dax_read_unlock(id);
 +	put_locked_mapping_entry(mapping, index, entry);
++=======
+ 	put_locked_mapping_entry(mapping, index);
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  	return ret;
  
   put_unlocked:
@@@ -883,10 -785,10 +934,10 @@@ int dax_writeback_mapping_range(struct 
  				break;
  			}
  
- 			ret = dax_writeback_one(bdev, dax_dev, mapping,
- 					indices[i], pvec.pages[i]);
+ 			ret = dax_writeback_one(dax_dev, mapping, indices[i],
+ 					pvec.pages[i]);
  			if (ret < 0) {
 -				mapping_set_error(mapping, ret);
 +				put_dax(dax_dev);
  				goto out;
  			}
  		}
@@@ -899,75 -801,80 +950,99 @@@ out
  }
  EXPORT_SYMBOL_GPL(dax_writeback_mapping_range);
  
 -static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
 -{
 -	return (iomap->addr + (pos & PAGE_MASK) - iomap->offset) >> 9;
 -}
 -
 -static int dax_iomap_pfn(struct iomap *iomap, loff_t pos, size_t size,
 -			 pfn_t *pfnp)
 +static int dax_insert_mapping(struct address_space *mapping,
 +		struct block_device *bdev, struct dax_device *dax_dev,
 +		sector_t sector, size_t size, void **entryp,
 +		struct vm_area_struct *vma, struct vm_fault *vmf)
  {
 -	const sector_t sector = dax_iomap_sector(iomap, pos);
 +	unsigned long vaddr = (unsigned long)vmf->virtual_address;
 +	void *entry = *entryp;
 +	void *ret, *kaddr;
  	pgoff_t pgoff;
 -	void *kaddr;
  	int id, rc;
 -	long length;
 +	pfn_t pfn;
  
 -	rc = bdev_dax_pgoff(iomap->bdev, sector, size, &pgoff);
 +	rc = bdev_dax_pgoff(bdev, sector, size, &pgoff);
  	if (rc)
  		return rc;
 +
  	id = dax_read_lock();
 -	length = dax_direct_access(iomap->dax_dev, pgoff, PHYS_PFN(size),
 -				   &kaddr, pfnp);
 -	if (length < 0) {
 -		rc = length;
 -		goto out;
 +	rc = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
 +	if (rc < 0) {
 +		dax_read_unlock(id);
 +		return rc;
  	}
 -	rc = -EINVAL;
 -	if (PFN_PHYS(length) < size)
 -		goto out;
 -	if (pfn_t_to_pfn(*pfnp) & (PHYS_PFN(size)-1))
 -		goto out;
 -	/* For larger pages we need devmap */
 -	if (length > 1 && !pfn_t_devmap(*pfnp))
 -		goto out;
 -	rc = 0;
 -out:
  	dax_read_unlock(id);
 -	return rc;
 +
 +	ret = dax_insert_mapping_entry(mapping, vmf, entry, sector, 0);
 +	if (IS_ERR(ret))
 +		return PTR_ERR(ret);
 +	*entryp = ret;
 +
 +	trace_dax_insert_mapping(mapping->host, vmf, ret);
 +	return vm_insert_mixed(vma, vaddr, pfn);
  }
  
 -/*
 - * The user has performed a load from a hole in the file.  Allocating a new
 - * page in the file would cause excessive storage usage for workloads with
 - * sparse files.  Instead we insert a read-only mapping of the 4k zero page.
 - * If this page is ever written to we will re-fault and change the mapping to
 - * point to real DAX storage instead.
 +/**
 + * dax_pfn_mkwrite - handle first write to DAX page
 + * @vma: The virtual memory area where the fault occurred
 + * @vmf: The description of the fault
   */
 -static int dax_load_hole(struct address_space *mapping, void *entry,
 -			 struct vm_fault *vmf)
 +int dax_pfn_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
  {
 +	struct file *file = vma->vm_file;
 +	struct address_space *mapping = file->f_mapping;
  	struct inode *inode = mapping->host;
++<<<<<<< HEAD
 +	void *entry, **slot;
 +	pgoff_t index = vmf->pgoff;
++=======
+ 	unsigned long vaddr = vmf->address;
+ 	int ret = VM_FAULT_NOPAGE;
+ 	struct page *zero_page;
+ 	void *entry2;
+ 	pfn_t pfn;
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  
 -	zero_page = ZERO_PAGE(0);
 -	if (unlikely(!zero_page)) {
 -		ret = VM_FAULT_OOM;
 -		goto out;
 +	spin_lock_irq(&mapping->tree_lock);
 +	entry = get_unlocked_mapping_entry(mapping, index, &slot);
 +	if (!entry || !radix_tree_exceptional_entry(entry)) {
 +		if (entry)
 +			put_unlocked_mapping_entry(mapping, index, entry);
 +		spin_unlock_irq(&mapping->tree_lock);
 +		trace_dax_pfn_mkwrite_no_entry(inode, vmf, VM_FAULT_NOPAGE);
 +		return VM_FAULT_NOPAGE;
  	}
++<<<<<<< HEAD
 +	radix_tree_tag_set(&mapping->page_tree, index, PAGECACHE_TAG_DIRTY);
 +	entry = lock_slot(mapping, slot);
 +	spin_unlock_irq(&mapping->tree_lock);
 +	/*
 +	 * If we race with somebody updating the PTE and finish_mkwrite_fault()
 +	 * fails, we don't care. We need to return VM_FAULT_NOPAGE and retry
 +	 * the fault in either case.
 +	 */
 +	finish_mkwrite_fault(vmf);
 +	put_locked_mapping_entry(mapping, index, entry);
 +	trace_dax_pfn_mkwrite(inode, vmf, VM_FAULT_NOPAGE);
 +	return VM_FAULT_NOPAGE;
++=======
+ 
+ 	pfn = page_to_pfn_t(zero_page);
+ 	entry2 = dax_insert_mapping_entry(mapping, vmf, entry, pfn,
+ 			RADIX_DAX_ZERO_PAGE, false);
+ 	if (IS_ERR(entry2)) {
+ 		ret = VM_FAULT_SIGBUS;
+ 		goto out;
+ 	}
+ 
+ 	vm_insert_mixed(vmf->vma, vaddr, pfn);
+ out:
+ 	trace_dax_load_hole(inode, vmf, ret);
+ 	return ret;
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  }
 +EXPORT_SYMBOL_GPL(dax_pfn_mkwrite);
  
  static bool dax_range_is_aligned(struct block_device *bdev,
  				 unsigned int offset, unsigned int length)
@@@ -1273,12 -1172,41 +1348,47 @@@ static int dax_iomap_pte_fault(struct v
  	case IOMAP_MAPPED:
  		if (iomap.flags & IOMAP_F_NEW) {
  			count_vm_event(PGMAJFAULT);
 -			count_memcg_event_mm(vma->vm_mm, PGMAJFAULT);
 +			mem_cgroup_count_vm_event(vmf->vma->vm_mm,
 +					PGMAJFAULT);
  			major = VM_FAULT_MAJOR;
  		}
++<<<<<<< HEAD
 +		error = dax_insert_mapping(mapping, iomap.bdev, iomap.dax_dev,
 +				sector, PAGE_SIZE, &entry, vmf->vma, vmf);
++=======
+ 		error = dax_iomap_pfn(&iomap, pos, PAGE_SIZE, &pfn);
+ 		if (error < 0)
+ 			goto error_finish_iomap;
+ 
+ 		entry = dax_insert_mapping_entry(mapping, vmf, entry, pfn,
+ 						 0, write && !sync);
+ 		if (IS_ERR(entry)) {
+ 			error = PTR_ERR(entry);
+ 			goto error_finish_iomap;
+ 		}
+ 
+ 		/*
+ 		 * If we are doing synchronous page fault and inode needs fsync,
+ 		 * we can insert PTE into page tables only after that happens.
+ 		 * Skip insertion for now and return the pfn so that caller can
+ 		 * insert it after fsync is done.
+ 		 */
+ 		if (sync) {
+ 			if (WARN_ON_ONCE(!pfnp)) {
+ 				error = -EIO;
+ 				goto error_finish_iomap;
+ 			}
+ 			*pfnp = pfn;
+ 			vmf_ret = VM_FAULT_NEEDDSYNC | major;
+ 			goto finish_iomap;
+ 		}
+ 		trace_dax_insert_mapping(inode, vmf, entry);
+ 		if (write)
+ 			error = vm_insert_mixed_mkwrite(vma, vaddr, pfn);
+ 		else
+ 			error = vm_insert_mixed(vma, vaddr, pfn);
+ 
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  		/* -EBUSY is fine, somebody else faulted on the same PTE */
  		if (error == -EBUSY)
  			error = 0;
@@@ -1381,20 -1258,21 +1491,27 @@@ static int dax_pmd_load_hole(struct vm_
  	void *ret = NULL;
  	spinlock_t *ptl;
  	pmd_t pmd_entry;
+ 	pfn_t pfn;
  
 -	zero_page = mm_get_huge_zero_page(vmf->vma->vm_mm);
 +	zero_page = get_huge_zero_page();
  
  	if (unlikely(!zero_page))
  		goto fallback;
  
++<<<<<<< HEAD
 +	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, 0,
 +			RADIX_DAX_PMD | RADIX_DAX_HZP);
++=======
+ 	pfn = page_to_pfn_t(zero_page);
+ 	ret = dax_insert_mapping_entry(mapping, vmf, entry, pfn,
+ 			RADIX_DAX_PMD | RADIX_DAX_ZERO_PAGE, false);
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  	if (IS_ERR(ret))
  		goto fallback;
 +	*entryp = ret;
  
  	ptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);
 -	if (!pmd_none(*(vmf->pmd))) {
 +	if (!pmd_none(*vmf->pmd)) {
  		spin_unlock(ptl);
  		goto fallback;
  	}
@@@ -1502,9 -1381,36 +1619,38 @@@ static int dax_iomap_pmd_fault(struct v
  	if (iomap.offset + iomap.length < pos + PMD_SIZE)
  		goto finish_iomap;
  
 -	sync = dax_fault_is_synchronous(iomap_flags, vma, &iomap);
 -
  	switch (iomap.type) {
  	case IOMAP_MAPPED:
++<<<<<<< HEAD
 +		result = dax_pmd_insert_mapping(vmf, &iomap, pos, &entry);
++=======
+ 		error = dax_iomap_pfn(&iomap, pos, PMD_SIZE, &pfn);
+ 		if (error < 0)
+ 			goto finish_iomap;
+ 
+ 		entry = dax_insert_mapping_entry(mapping, vmf, entry, pfn,
+ 						RADIX_DAX_PMD, write && !sync);
+ 		if (IS_ERR(entry))
+ 			goto finish_iomap;
+ 
+ 		/*
+ 		 * If we are doing synchronous page fault and inode needs fsync,
+ 		 * we can insert PMD into page tables only after that happens.
+ 		 * Skip insertion for now and return the pfn so that caller can
+ 		 * insert it after fsync is done.
+ 		 */
+ 		if (sync) {
+ 			if (WARN_ON_ONCE(!pfnp))
+ 				goto finish_iomap;
+ 			*pfnp = pfn;
+ 			result = VM_FAULT_NEEDDSYNC;
+ 			goto finish_iomap;
+ 		}
+ 
+ 		trace_dax_pmd_insert_mapping(inode, vmf, PMD_SIZE, pfn, entry);
+ 		result = vmf_insert_pfn_pmd(vma, vmf->address, vmf->pmd, pfn,
+ 					    write);
++>>>>>>> 3fe0791c295c (dax: store pfns in the radix)
  		break;
  	case IOMAP_UNWRITTEN:
  	case IOMAP_HOLE:
diff --git a/drivers/dax/super.c b/drivers/dax/super.c
index 949ff36e0910..891dcb37cbbc 100644
--- a/drivers/dax/super.c
+++ b/drivers/dax/super.c
@@ -114,10 +114,19 @@ int __bdev_dax_supported(struct super_block *sb, int blocksize)
 		return len < 0 ? len : -EIO;
 	}
 
-	if ((IS_ENABLED(CONFIG_FS_DAX_LIMITED) && pfn_t_special(pfn))
-			|| pfn_t_devmap(pfn))
+	if (IS_ENABLED(CONFIG_FS_DAX_LIMITED) && pfn_t_special(pfn)) {
+		/*
+		 * An arch that has enabled the pmem api should also
+		 * have its drivers support pfn_t_devmap()
+		 *
+		 * This is a developer warning and should not trigger in
+		 * production. dax_flush() will crash since it depends
+		 * on being able to do (page_address(pfn_to_page())).
+		 */
+		WARN_ON(IS_ENABLED(CONFIG_ARCH_HAS_PMEM_API));
+	} else if (pfn_t_devmap(pfn)) {
 		/* pass */;
-	else {
+	} else {
 		pr_debug("VFS (%s): error: dax support not enabled\n",
 				sb->s_id);
 		return -EOPNOTSUPP;
* Unmerged path fs/dax.c
