powerpc/rfi-flush: Make it possible to call setup_rfi_flush() again

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [powerpc] rfi-flush: Make it possible to call setup_rfi_flush() again (Mauricio Oliveira) [1561785]
Rebuild_FUZZ: 93.65%
commit-author Michael Ellerman <mpe@ellerman.id.au>
commit abf110f3e1cea40f5ea15e85f5d67c39c14568a7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/abf110f3.failed

For PowerVM migration we want to be able to call setup_rfi_flush()
again after we've migrated the partition.

To support that we need to check that we're not trying to allocate the
fallback flush area after memblock has gone away (i.e., boot-time only).

	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
	Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit abf110f3e1cea40f5ea15e85f5d67c39c14568a7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kernel/setup_64.c
diff --cc arch/powerpc/kernel/setup_64.c
index 5dba321c5b53,d60e2f7eff1b..000000000000
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@@ -816,32 -855,37 +816,62 @@@ void rfi_flush_enable(bool enable
  	rfi_flush = enable;
  }
  
++<<<<<<< HEAD
 +void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)
++=======
+ static void init_fallback_flush(void)
+ {
+ 	u64 l1d_size, limit;
+ 	int cpu;
+ 
+ 	/* Only allocate the fallback flush area once (at boot time). */
+ 	if (l1d_flush_fallback_area)
+ 		return;
+ 
+ 	l1d_size = ppc64_caches.l1d.size;
+ 	limit = min(ppc64_bolted_size(), ppc64_rma_size);
+ 
+ 	/*
+ 	 * Align to L1d size, and size it at 2x L1d size, to catch possible
+ 	 * hardware prefetch runoff. We don't have a recipe for load patterns to
+ 	 * reliably avoid the prefetcher.
+ 	 */
+ 	l1d_flush_fallback_area = __va(memblock_alloc_base(l1d_size * 2, l1d_size, limit));
+ 	memset(l1d_flush_fallback_area, 0, l1d_size * 2);
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		paca[cpu].rfi_flush_fallback_area = l1d_flush_fallback_area;
+ 		paca[cpu].l1d_flush_size = l1d_size;
+ 	}
+ }
+ 
+ void setup_rfi_flush(enum l1d_flush_type types, bool enable)
++>>>>>>> abf110f3e1ce (powerpc/rfi-flush: Make it possible to call setup_rfi_flush() again)
  {
  	if (types & L1D_FLUSH_FALLBACK) {
 +		int cpu;
 +		u64 l1d_size = ppc64_caches.dsize;
 +
  		pr_info("rfi-flush: Using fallback displacement flush\n");
 -		init_fallback_flush();
 +
 +		/*
 +		 * We allocate 2x L1d size for the dummy area, to
 +		 * catch possible hardware prefetch runoff.
 +		 *
 +		 * We can't use memblock_alloc here because bootmem has
 +		 * been initialized, and the bootmem APIs don't work well
 +		 * with an upper limit we need, so we allocate it statically
 +		 * from BSS. The biggest L1d supported by this kernel is
 +		 * 64kB (POWER8), so 128kB is reserved above.
 +		 */
 +
 +		WARN_ON(l1d_size > MAX_L1D_SIZE);
 +
 +		for_each_possible_cpu(cpu) {
 +			struct paca_aux_struct *paca_aux = paca_aux_of(cpu);
 +			paca_aux->rfi_flush_fallback_area = l1d_flush_fallback_area;
 +			paca_aux->l1d_flush_size = l1d_size;
 +		}
  	}
  
  	if (types & L1D_FLUSH_ORI)
diff --git a/arch/powerpc/include/asm/setup.h b/arch/powerpc/include/asm/setup.h
index b4423301696b..cb1e56079d12 100644
--- a/arch/powerpc/include/asm/setup.h
+++ b/arch/powerpc/include/asm/setup.h
@@ -38,7 +38,7 @@ enum l1d_flush_type {
 	L1D_FLUSH_MTTRIG	= 0x8,
 };
 
-void __init setup_rfi_flush(enum l1d_flush_type, bool enable);
+void setup_rfi_flush(enum l1d_flush_type, bool enable);
 void do_rfi_flush_fixups(enum l1d_flush_type types);
 
 #endif /* !__ASSEMBLY__ */
* Unmerged path arch/powerpc/kernel/setup_64.c
