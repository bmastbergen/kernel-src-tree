nfp: bpf: move to new BPF program offload infrastructure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit c6c580d7bc390f864488c66153a487057e76d9d8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/c6c580d7.failed

Following steps are taken in the driver to offload an XDP program:

XDP_SETUP_PROG:
 * prepare:
   - allocate program state;
   - run verifier (bpf_analyzer());
   - run translation;
 * load:
   - stop old program if needed;
   - load program;
   - enable BPF if not enabled;
 * clean up:
   - free program image.

With new infrastructure the flow will look like this:

BPF_OFFLOAD_VERIFIER_PREP:
  - allocate program state;
BPF_OFFLOAD_TRANSLATE:
   - run translation;
XDP_SETUP_PROG:
   - stop old program if needed;
   - load program;
   - enable BPF if not enabled;
BPF_OFFLOAD_DESTROY:
   - free program image.

Take advantage of the new infrastructure.  Allocation of driver
metadata has to be moved from jit.c to offload.c since it's now
done at a different stage.  Since there is no separate driver
private data for verification step, move temporary nfp_meta
pointer into nfp_prog.  We will now use user space context
offsets.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c6c580d7bc390f864488c66153a487057e76d9d8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/jit.c
#	drivers/net/ethernet/netronome/nfp/bpf/main.h
#	drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/jit.c
index a86508b776ac,995e95410b11..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/jit.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/jit.c
@@@ -2200,51 -2243,13 +2200,58 @@@ static int nfp_bpf_ustore_calc(struct n
  	return 0;
  }
  
++<<<<<<< HEAD
 +/**
 + * nfp_bpf_jit() - translate BPF code into NFP assembly
 + * @filter:	kernel BPF filter struct
 + * @prog_mem:	memory to store assembler instructions
 + * @act:	action attached to this eBPF program
 + * @prog_start:	offset of the first instruction when loaded
 + * @prog_done:	where to jump on exit
 + * @prog_sz:	size of @prog_mem in instructions
 + * @res:	achieved parameters of translation results
 + */
 +int
 +nfp_bpf_jit(struct bpf_prog *filter, void *prog_mem,
 +	    enum nfp_bpf_action_type act,
 +	    unsigned int prog_start, unsigned int prog_done,
 +	    unsigned int prog_sz, struct nfp_bpf_result *res)
++=======
+ int nfp_bpf_jit(struct nfp_prog *nfp_prog)
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  {
 +	struct nfp_prog *nfp_prog;
  	int ret;
  
++<<<<<<< HEAD
 +	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
 +	if (!nfp_prog)
 +		return -ENOMEM;
 +
 +	INIT_LIST_HEAD(&nfp_prog->insns);
 +	nfp_prog->act = act;
 +	nfp_prog->start_off = prog_start;
 +	nfp_prog->tgt_done = prog_done;
 +
 +	ret = nfp_prog_prepare(nfp_prog, filter->insnsi, filter->len);
 +	if (ret)
 +		goto out;
 +
 +	ret = nfp_prog_verify(nfp_prog, filter);
 +	if (ret)
 +		goto out;
 +
++=======
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  	ret = nfp_bpf_optimize(nfp_prog);
  	if (ret)
 -		return ret;
 +		goto out;
 +
 +	nfp_prog->num_regs = MAX_BPF_REG;
 +	nfp_prog->regs_per_thread = 32;
 +
 +	nfp_prog->prog = prog_mem;
 +	nfp_prog->__prog_alloc_len = prog_sz;
  
  	ret = nfp_translate(nfp_prog);
  	if (ret) {
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.h
index cc2a5beba757,082a15f6dfb5..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.h
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.h
@@@ -146,9 -139,8 +146,14 @@@ static inline u8 mbpf_mode(const struc
   * @prog: machine code
   * @prog_len: number of valid instructions in @prog array
   * @__prog_alloc_len: alloc size of @prog array
++<<<<<<< HEAD
 + * @act: BPF program/action type (TC DA, TC with action, XDP etc.)
 + * @num_regs: number of registers used by this program
 + * @regs_per_thread: number of basic registers allocated per thread
++=======
+  * @verifier_meta: temporary storage for verifier's insn meta
+  * @type: BPF program type
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
   * @start_off: address of the first instruction in the memory
   * @tgt_out: jump target for normal exit
   * @tgt_abort: jump target for abort (e.g. access outside of packet buffer)
@@@ -163,10 -155,9 +168,16 @@@ struct nfp_prog 
  	unsigned int prog_len;
  	unsigned int __prog_alloc_len;
  
++<<<<<<< HEAD
 +	enum nfp_bpf_action_type act;
 +
 +	unsigned int num_regs;
 +	unsigned int regs_per_thread;
++=======
+ 	struct nfp_insn_meta *verifier_meta;
+ 
+ 	enum bpf_prog_type type;
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  
  	unsigned int start_off;
  	unsigned int tgt_out;
@@@ -181,37 -172,21 +192,49 @@@
  	struct list_head insns;
  };
  
++<<<<<<< HEAD
 +struct nfp_bpf_result {
 +	unsigned int n_instr;
 +	bool dense_mode;
 +};
 +
 +int
 +nfp_bpf_jit(struct bpf_prog *filter, void *prog, enum nfp_bpf_action_type act,
 +	    unsigned int prog_start, unsigned int prog_done,
 +	    unsigned int prog_sz, struct nfp_bpf_result *res);
++=======
+ int nfp_bpf_jit(struct nfp_prog *prog);
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  
- int nfp_prog_verify(struct nfp_prog *nfp_prog, struct bpf_prog *prog);
+ extern const struct bpf_ext_analyzer_ops nfp_bpf_analyzer_ops;
  
+ struct netdev_bpf;
+ struct nfp_app;
  struct nfp_net;
 +struct tc_cls_bpf_offload;
 +
 +/**
 + * struct nfp_net_bpf_priv - per-vNIC BPF private data
 + * @rx_filter:		Filter offload statistics - dropped packets/bytes
 + * @rx_filter_prev:	Filter offload statistics - values from previous update
 + * @rx_filter_change:	Jiffies when statistics last changed
 + * @rx_filter_stats_timer:  Timer for polling filter offload statistics
 + * @rx_filter_lock:	Lock protecting timer state changes (teardown)
 + */
 +struct nfp_net_bpf_priv {
 +	struct nfp_stat_pair rx_filter, rx_filter_prev;
 +	unsigned long rx_filter_change;
 +	struct timer_list rx_filter_stats_timer;
 +	spinlock_t rx_filter_lock;
 +};
  
 -int nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,
 -			bool old_prog);
 +int nfp_net_bpf_offload(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf);
 +void nfp_net_filter_stats_timer(unsigned long data);
  
+ int nfp_bpf_verifier_prep(struct nfp_app *app, struct nfp_net *nn,
+ 			  struct netdev_bpf *bpf);
+ int nfp_bpf_translate(struct nfp_app *app, struct nfp_net *nn,
+ 		      struct bpf_prog *prog);
+ int nfp_bpf_destroy(struct nfp_app *app, struct nfp_net *nn,
+ 		    struct bpf_prog *prog);
  #endif
diff --cc drivers/net/ethernet/netronome/nfp/bpf/offload.c
index de79faf0874b,b6cee71f49d3..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/offload.c
@@@ -112,57 -73,92 +112,130 @@@ nfp_net_bpf_stats_update(struct nfp_ne
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int
 +nfp_net_bpf_get_act(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
 +{
 +	const struct tc_action *a;
 +	LIST_HEAD(actions);
 +
 +	if (!cls_bpf->exts)
 +		return NN_ACT_XDP;
 +
 +	/* TC direct action */
 +	if (cls_bpf->exts_integrated) {
 +		if (tc_no_actions(cls_bpf->exts))
 +			return NN_ACT_DIRECT;
++=======
+ static void nfp_prog_free(struct nfp_prog *nfp_prog)
+ {
+ 	struct nfp_insn_meta *meta, *tmp;
+ 
+ 	list_for_each_entry_safe(meta, tmp, &nfp_prog->insns, l) {
+ 		list_del(&meta->l);
+ 		kfree(meta);
+ 	}
+ 	kfree(nfp_prog);
+ }
+ 
+ int nfp_bpf_verifier_prep(struct nfp_app *app, struct nfp_net *nn,
+ 			  struct netdev_bpf *bpf)
+ {
+ 	struct bpf_prog *prog = bpf->verifier.prog;
+ 	struct nfp_prog *nfp_prog;
+ 	int ret;
+ 
+ 	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
+ 	if (!nfp_prog)
+ 		return -ENOMEM;
+ 	prog->aux->offload->dev_priv = nfp_prog;
+ 
+ 	INIT_LIST_HEAD(&nfp_prog->insns);
+ 	nfp_prog->type = prog->type;
+ 
+ 	ret = nfp_prog_prepare(nfp_prog, prog->insnsi, prog->len);
+ 	if (ret)
+ 		goto err_free;
+ 
+ 	nfp_prog->verifier_meta = nfp_prog_first_meta(nfp_prog);
+ 	bpf->verifier.ops = &nfp_bpf_analyzer_ops;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	nfp_prog_free(nfp_prog);
+ 
+ 	return ret;
+ }
+ 
+ int nfp_bpf_translate(struct nfp_app *app, struct nfp_net *nn,
+ 		      struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 	unsigned int stack_size;
+ 	unsigned int max_instr;
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  
 -	stack_size = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;
 -	if (prog->aux->stack_depth > stack_size) {
 -		nn_info(nn, "stack too large: program %dB > FW stack %dB\n",
 -			prog->aux->stack_depth, stack_size);
  		return -EOPNOTSUPP;
  	}
  
 -	nfp_prog->stack_depth = prog->aux->stack_depth;
 -	nfp_prog->start_off = nn_readw(nn, NFP_NET_CFG_BPF_START);
 -	nfp_prog->tgt_done = nn_readw(nn, NFP_NET_CFG_BPF_DONE);
 +	/* TC legacy mode */
 +	if (!tc_single_action(cls_bpf->exts))
 +		return -EOPNOTSUPP;
  
 -	max_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);
 -	nfp_prog->__prog_alloc_len = max_instr * sizeof(u64);
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list) {
 +		if (is_tcf_gact_shot(a))
 +			return NN_ACT_TC_DROP;
  
 -	nfp_prog->prog = kmalloc(nfp_prog->__prog_alloc_len, GFP_KERNEL);
 -	if (!nfp_prog->prog)
 -		return -ENOMEM;
 +		if (is_tcf_mirred_egress_redirect(a) &&
 +		    tcf_mirred_ifindex(a) == nn->dp.netdev->ifindex)
 +			return NN_ACT_TC_REDIR;
 +	}
 +
++<<<<<<< HEAD
 +	return -EOPNOTSUPP;
 +}
  
 +static int
 +nfp_net_bpf_offload_prepare(struct nfp_net *nn,
 +			    struct tc_cls_bpf_offload *cls_bpf,
 +			    struct nfp_bpf_result *res,
 +			    void **code, dma_addr_t *dma_addr, u16 max_instr)
 +{
 +	unsigned int code_sz = max_instr * sizeof(u64);
 +	enum nfp_bpf_action_type act;
 +	unsigned int stack_size;
 +	u16 start_off, done_off;
 +	unsigned int max_mtu;
 +	int ret;
 +
 +	ret = nfp_net_bpf_get_act(nn, cls_bpf);
 +	if (ret < 0)
 +		return ret;
 +	act = ret;
++=======
+ 	return nfp_bpf_jit(nfp_prog);
+ }
+ 
+ int nfp_bpf_destroy(struct nfp_app *app, struct nfp_net *nn,
+ 		    struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 
+ 	kfree(nfp_prog->prog);
+ 	nfp_prog_free(nfp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_net_bpf_load(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 	unsigned int max_mtu;
+ 	dma_addr_t dma_addr;
+ 	int err;
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  
  	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
  	if (max_mtu < nn->dp.netdev->mtu) {
@@@ -170,61 -166,30 +243,82 @@@
  		return -EOPNOTSUPP;
  	}
  
++<<<<<<< HEAD
 +	start_off = nn_readw(nn, NFP_NET_CFG_BPF_START);
 +	done_off = nn_readw(nn, NFP_NET_CFG_BPF_DONE);
 +
 +	if (cls_bpf->prog->aux->stack_depth > 64) {
 +		nn_info(nn, "large stack not supported: program %dB > 64B\n",
 +			cls_bpf->prog->aux->stack_depth);
 +		return -EOPNOTSUPP;
 +	}
 +
 +	stack_size = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;
 +	if (cls_bpf->prog->aux->stack_depth > stack_size) {
 +		nn_info(nn, "stack too large: program %dB > FW stack %dB\n",
 +			cls_bpf->prog->aux->stack_depth, stack_size);
 +		return -EOPNOTSUPP;
 +	}
 +
 +	*code = dma_zalloc_coherent(nn->dp.dev, code_sz, dma_addr, GFP_KERNEL);
 +	if (!*code)
 +		return -ENOMEM;
 +
 +	ret = nfp_bpf_jit(cls_bpf->prog, *code, act, start_off, done_off,
 +			  max_instr, res);
 +	if (ret)
 +		goto out;
 +
 +	return 0;
 +
 +out:
 +	dma_free_coherent(nn->dp.dev, code_sz, *code, *dma_addr);
 +	return ret;
 +}
 +
 +static void
 +nfp_net_bpf_load_and_start(struct nfp_net *nn, u32 tc_flags,
 +			   void *code, dma_addr_t dma_addr,
 +			   unsigned int code_sz, unsigned int n_instr,
 +			   bool dense_mode)
 +{
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bpf_addr = dma_addr;
 +	int err;
++=======
+ 	dma_addr = dma_map_single(nn->dp.dev, nfp_prog->prog,
+ 				  nfp_prog->prog_len * sizeof(u64),
+ 				  DMA_TO_DEVICE);
+ 	if (dma_mapping_error(nn->dp.dev, dma_addr))
+ 		return -ENOMEM;
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
 +
 +	nn->dp.bpf_offload_skip_sw = !!(tc_flags & TCA_CLS_FLAGS_SKIP_SW);
 +
 +	if (dense_mode)
 +		bpf_addr |= NFP_NET_CFG_BPF_CFG_8CTX;
  
 -	nn_writew(nn, NFP_NET_CFG_BPF_SIZE, nfp_prog->prog_len);
 -	nn_writeq(nn, NFP_NET_CFG_BPF_ADDR, dma_addr);
 +	nn_writew(nn, NFP_NET_CFG_BPF_SIZE, n_instr);
 +	nn_writeq(nn, NFP_NET_CFG_BPF_ADDR, bpf_addr);
  
  	/* Load up the JITed code */
  	err = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_BPF);
  	if (err)
  		nn_err(nn, "FW command error while loading BPF: %d\n", err);
  
++<<<<<<< HEAD
++=======
+ 	dma_unmap_single(nn->dp.dev, dma_addr, nfp_prog->prog_len * sizeof(u64),
+ 			 DMA_TO_DEVICE);
+ 
+ 	return err;
+ }
+ 
+ static void nfp_net_bpf_start(struct nfp_net *nn)
+ {
+ 	int err;
+ 
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  	/* Enable passing packets through BPF function */
  	nn->dp.ctrl |= NFP_NET_CFG_CTRL_BPF;
  	nn_writel(nn, NFP_NET_CFG_CTRL, nn->dp.ctrl);
@@@ -246,76 -203,43 +340,94 @@@ static int nfp_net_bpf_stop(struct nfp_
  	if (!(nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF))
  		return 0;
  
 +	spin_lock_bh(&priv->rx_filter_lock);
  	nn->dp.ctrl &= ~NFP_NET_CFG_CTRL_BPF;
 +	spin_unlock_bh(&priv->rx_filter_lock);
  	nn_writel(nn, NFP_NET_CFG_CTRL, nn->dp.ctrl);
  
 +	del_timer_sync(&priv->rx_filter_stats_timer);
 +	nn->dp.bpf_offload_skip_sw = 0;
 +
  	return nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);
  }
 +#endif
  
 -int nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,
 -			bool old_prog)
 +int nfp_net_bpf_offload(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
  {
++<<<<<<< HEAD
 +#if 0 /* Not in RHEL7 */
 +	struct nfp_bpf_result res;
 +	dma_addr_t dma_addr;
 +	u16 max_instr;
 +	void *code;
 +	int err;
++=======
+ 	int err;
+ 
+ 	if (prog && !prog->aux->offload)
+ 		return -EINVAL;
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  
 -	if (prog && old_prog) {
 -		u8 cap;
 +	max_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);
  
 -		cap = nn_readb(nn, NFP_NET_CFG_BPF_CAP);
 -		if (!(cap & NFP_NET_BPF_CAP_RELO)) {
 -			nn_err(nn, "FW does not support live reload\n");
 +	switch (cls_bpf->command) {
 +	case TC_CLSBPF_REPLACE:
 +		/* There is nothing stopping us from implementing seamless
 +		 * replace but the simple method of loading I adopted in
 +		 * the firmware does not handle atomic replace (i.e. we have to
 +		 * stop the BPF offload and re-enable it).  Leaking-in a few
 +		 * frames which didn't have BPF applied in the hardware should
 +		 * be fine if software fallback is available, though.
 +		 */
 +		if (nn->dp.bpf_offload_skip_sw)
  			return -EBUSY;
 -		}
 -	}
  
 -	/* Something else is loaded, different program type? */
 -	if (!old_prog && nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 -		return -EBUSY;
 +		err = nfp_net_bpf_offload_prepare(nn, cls_bpf, &res, &code,
 +						  &dma_addr, max_instr);
 +		if (err)
 +			return err;
 +
 +		nfp_net_bpf_stop(nn);
 +		nfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,
 +					   dma_addr, max_instr * sizeof(u64),
 +					   res.n_instr, res.dense_mode);
 +		return 0;
 +
 +	case TC_CLSBPF_ADD:
 +		if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 +			return -EBUSY;
  
 -	if (old_prog && !prog)
 +		err = nfp_net_bpf_offload_prepare(nn, cls_bpf, &res, &code,
 +						  &dma_addr, max_instr);
 +		if (err)
 +			return err;
 +
 +		nfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,
 +					   dma_addr, max_instr * sizeof(u64),
 +					   res.n_instr, res.dense_mode);
 +		return 0;
 +
 +	case TC_CLSBPF_DESTROY:
  		return nfp_net_bpf_stop(nn);
  
++<<<<<<< HEAD
 +	case TC_CLSBPF_STATS:
 +		return nfp_net_bpf_stats_update(nn, cls_bpf);
 +
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +#else
 +	return -EOPNOTSUPP;
 +#endif
++=======
+ 	err = nfp_net_bpf_load(nn, prog);
+ 	if (err)
+ 		return err;
+ 
+ 	if (!old_prog)
+ 		nfp_net_bpf_start(nn);
+ 
+ 	return 0;
++>>>>>>> c6c580d7bc39 (nfp: bpf: move to new BPF program offload infrastructure)
  }
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/jit.c
diff --git a/drivers/net/ethernet/netronome/nfp/bpf/main.c b/drivers/net/ethernet/netronome/nfp/bpf/main.c
index 4ca14182b178..b9066dc96be5 100644
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.c
@@ -153,4 +153,8 @@ const struct nfp_app_type app_bpf = {
 	.setup_tc	= nfp_bpf_setup_tc,
 	.tc_busy	= nfp_bpf_tc_busy,
 	.xdp_offload	= nfp_bpf_xdp_offload,
+
+	.bpf_verifier_prep	= nfp_bpf_verifier_prep,
+	.bpf_translate		= nfp_bpf_translate,
+	.bpf_destroy		= nfp_bpf_destroy,
 };
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.h
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --git a/drivers/net/ethernet/netronome/nfp/bpf/verifier.c b/drivers/net/ethernet/netronome/nfp/bpf/verifier.c
index 8b981cfbeb65..4a34e29acbc9 100644
--- a/drivers/net/ethernet/netronome/nfp/bpf/verifier.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/verifier.c
@@ -40,12 +40,6 @@
 
 #include "main.h"
 
-/* Analyzer/verifier definitions */
-struct nfp_bpf_analyzer_priv {
-	struct nfp_prog *prog;
-	struct nfp_insn_meta *meta;
-};
-
 static struct nfp_insn_meta *
 nfp_bpf_goto_meta(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,
 		  unsigned int insn_idx, unsigned int n_insns)
@@ -166,11 +160,11 @@ nfp_bpf_check_ptr(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,
 static int
 nfp_verify_insn(struct bpf_verifier_env *env, int insn_idx, int prev_insn_idx)
 {
-	struct nfp_bpf_analyzer_priv *priv = env->analyzer_priv;
-	struct nfp_insn_meta *meta = priv->meta;
+	struct nfp_prog *nfp_prog = env->prog->aux->offload->dev_priv;
+	struct nfp_insn_meta *meta = nfp_prog->verifier_meta;
 
-	meta = nfp_bpf_goto_meta(priv->prog, meta, insn_idx, env->prog->len);
-	priv->meta = meta;
+	meta = nfp_bpf_goto_meta(nfp_prog, meta, insn_idx, env->prog->len);
+	nfp_prog->verifier_meta = meta;
 
 	if (meta->insn.src_reg >= MAX_BPF_REG ||
 	    meta->insn.dst_reg >= MAX_BPF_REG) {
@@ -179,39 +173,18 @@ nfp_verify_insn(struct bpf_verifier_env *env, int insn_idx, int prev_insn_idx)
 	}
 
 	if (meta->insn.code == (BPF_JMP | BPF_EXIT))
-		return nfp_bpf_check_exit(priv->prog, env);
+		return nfp_bpf_check_exit(nfp_prog, env);
 
 	if ((meta->insn.code & ~BPF_SIZE_MASK) == (BPF_LDX | BPF_MEM))
-		return nfp_bpf_check_ptr(priv->prog, meta, env,
+		return nfp_bpf_check_ptr(nfp_prog, meta, env,
 					 meta->insn.src_reg);
 	if ((meta->insn.code & ~BPF_SIZE_MASK) == (BPF_STX | BPF_MEM))
-		return nfp_bpf_check_ptr(priv->prog, meta, env,
+		return nfp_bpf_check_ptr(nfp_prog, meta, env,
 					 meta->insn.dst_reg);
 
 	return 0;
 }
 
-static const struct bpf_ext_analyzer_ops nfp_bpf_analyzer_ops = {
+const struct bpf_ext_analyzer_ops nfp_bpf_analyzer_ops = {
 	.insn_hook = nfp_verify_insn,
 };
-
-int nfp_prog_verify(struct nfp_prog *nfp_prog, struct bpf_prog *prog)
-{
-	struct nfp_bpf_analyzer_priv *priv;
-	int ret;
-
-	nfp_prog->stack_depth = prog->aux->stack_depth;
-
-	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
-	if (!priv)
-		return -ENOMEM;
-
-	priv->prog = nfp_prog;
-	priv->meta = nfp_prog_first_meta(nfp_prog);
-
-	ret = bpf_analyzer(prog, &nfp_bpf_analyzer_ops, priv);
-
-	kfree(priv);
-
-	return ret;
-}
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_app.h b/drivers/net/ethernet/netronome/nfp/nfp_app.h
index b6035aad75b0..a2e55e5f67bb 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_app.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_app.h
@@ -42,6 +42,7 @@
 
 struct bpf_prog;
 struct net_device;
+struct netdev_bpf;
 struct pci_dev;
 struct sk_buff;
 struct tc_to_netdev;
@@ -84,6 +85,9 @@ extern const struct nfp_app_type app_flower;
  * @setup_tc:	setup TC ndo
  * @tc_busy:	TC HW offload busy (rules loaded)
  * @xdp_offload:    offload an XDP program
+ * @bpf_verifier_prep:	verifier prep for dev-specific BPF programs
+ * @bpf_translate:	translate call for dev-specific BPF programs
+ * @bpf_destroy:	destroy for dev-specific BPF programs
  * @eswitch_mode_get:    get SR-IOV eswitch mode
  * @sriov_enable: app-specific sriov initialisation
  * @sriov_disable: app-specific sriov clean-up
@@ -119,6 +123,12 @@ struct nfp_app_type {
 	bool (*tc_busy)(struct nfp_app *app, struct nfp_net *nn);
 	int (*xdp_offload)(struct nfp_app *app, struct nfp_net *nn,
 			   struct bpf_prog *prog);
+	int (*bpf_verifier_prep)(struct nfp_app *app, struct nfp_net *nn,
+				 struct netdev_bpf *bpf);
+	int (*bpf_translate)(struct nfp_app *app, struct nfp_net *nn,
+			     struct bpf_prog *prog);
+	int (*bpf_destroy)(struct nfp_app *app, struct nfp_net *nn,
+			   struct bpf_prog *prog);
 
 	int (*sriov_enable)(struct nfp_app *app, int num_vfs);
 	void (*sriov_disable)(struct nfp_app *app);
@@ -273,6 +283,33 @@ static inline int nfp_app_xdp_offload(struct nfp_app *app, struct nfp_net *nn,
 	return app->type->xdp_offload(app, nn, prog);
 }
 
+static inline int
+nfp_app_bpf_verifier_prep(struct nfp_app *app, struct nfp_net *nn,
+			  struct netdev_bpf *bpf)
+{
+	if (!app || !app->type->bpf_verifier_prep)
+		return -EOPNOTSUPP;
+	return app->type->bpf_verifier_prep(app, nn, bpf);
+}
+
+static inline int
+nfp_app_bpf_translate(struct nfp_app *app, struct nfp_net *nn,
+		      struct bpf_prog *prog)
+{
+	if (!app || !app->type->bpf_translate)
+		return -EOPNOTSUPP;
+	return app->type->bpf_translate(app, nn, prog);
+}
+
+static inline int
+nfp_app_bpf_destroy(struct nfp_app *app, struct nfp_net *nn,
+		    struct bpf_prog *prog)
+{
+	if (!app || !app->type->bpf_destroy)
+		return -EOPNOTSUPP;
+	return app->type->bpf_destroy(app, nn, prog);
+}
+
 static inline bool nfp_app_ctrl_tx(struct nfp_app *app, struct sk_buff *skb)
 {
 	trace_devlink_hwmsg(priv_to_devlink(app->pf), false, 0,
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 3846e4eea8e4..371310e7f1e7 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@ -3415,6 +3415,14 @@ static int nfp_net_xdp(struct net_device *netdev, struct netdev_xdp *xdp)
 			xdp->prog_attached = XDP_ATTACHED_HW;
 		xdp->prog_id = nn->xdp_prog ? nn->xdp_prog->aux->id : 0;
 		return 0;
+	case BPF_OFFLOAD_VERIFIER_PREP:
+		return nfp_app_bpf_verifier_prep(nn->app, nn, xdp);
+	case BPF_OFFLOAD_TRANSLATE:
+		return nfp_app_bpf_translate(nn->app, nn,
+					     xdp->offload.prog);
+	case BPF_OFFLOAD_DESTROY:
+		return nfp_app_bpf_destroy(nn->app, nn,
+					   xdp->offload.prog);
 	default:
 		return -EINVAL;
 	}
