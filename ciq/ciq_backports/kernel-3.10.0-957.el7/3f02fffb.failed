nvme-rdma: don't free tagset on resets

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Sagi Grimberg <sagi@grimberg.me>
commit 3f02fffb74ae7486232fef6ca4341c5e9719c759
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3f02fffb.failed

We're not supposed to do that.

	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 3f02fffb74ae7486232fef6ca4341c5e9719c759)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index 6d8e34242c5f,b8c07f2f2204..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -675,14 -646,160 +675,170 @@@ out_free_queues
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl)
++=======
+ static void nvme_rdma_free_tagset(struct nvme_ctrl *nctrl, bool admin)
+ {
+ 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ 	struct blk_mq_tag_set *set = admin ?
+ 			&ctrl->admin_tag_set : &ctrl->tag_set;
+ 
+ 	blk_mq_free_tag_set(set);
+ 	nvme_rdma_dev_put(ctrl->device);
+ }
+ 
+ static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
+ 		bool admin)
+ {
+ 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ 	struct blk_mq_tag_set *set;
+ 	int ret;
+ 
+ 	if (admin) {
+ 		set = &ctrl->admin_tag_set;
+ 		memset(set, 0, sizeof(*set));
+ 		set->ops = &nvme_rdma_admin_mq_ops;
+ 		set->queue_depth = NVME_RDMA_AQ_BLKMQ_DEPTH;
+ 		set->reserved_tags = 2; /* connect + keep-alive */
+ 		set->numa_node = NUMA_NO_NODE;
+ 		set->cmd_size = sizeof(struct nvme_rdma_request) +
+ 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+ 		set->driver_data = ctrl;
+ 		set->nr_hw_queues = 1;
+ 		set->timeout = ADMIN_TIMEOUT;
+ 	} else {
+ 		set = &ctrl->tag_set;
+ 		memset(set, 0, sizeof(*set));
+ 		set->ops = &nvme_rdma_mq_ops;
+ 		set->queue_depth = nctrl->opts->queue_size;
+ 		set->reserved_tags = 1; /* fabric connect */
+ 		set->numa_node = NUMA_NO_NODE;
+ 		set->flags = BLK_MQ_F_SHOULD_MERGE;
+ 		set->cmd_size = sizeof(struct nvme_rdma_request) +
+ 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+ 		set->driver_data = ctrl;
+ 		set->nr_hw_queues = nctrl->queue_count - 1;
+ 		set->timeout = NVME_IO_TIMEOUT;
+ 	}
+ 
+ 	ret = blk_mq_alloc_tag_set(set);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/*
+ 	 * We need a reference on the device as long as the tag_set is alive,
+ 	 * as the MRs in the request structures need a valid ib_device.
+ 	 */
+ 	ret = nvme_rdma_dev_get(ctrl->device);
+ 	if (!ret) {
+ 		ret = -EINVAL;
+ 		goto out_free_tagset;
+ 	}
+ 
+ 	return set;
+ 
+ out_free_tagset:
+ 	blk_mq_free_tag_set(set);
+ out:
+ 	return ERR_PTR(ret);
+ }
+ 
+ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
+ 		bool remove)
++>>>>>>> 3f02fffb74ae (nvme-rdma: don't free tagset on resets)
  {
  	nvme_rdma_free_qe(ctrl->queues[0].device->dev, &ctrl->async_event_sqe,
  			sizeof(struct nvme_command), DMA_TO_DEVICE);
  	nvme_rdma_stop_and_free_queue(&ctrl->queues[0]);
++<<<<<<< HEAD
 +	blk_cleanup_queue(ctrl->ctrl.admin_q);
 +	blk_mq_free_tag_set(&ctrl->admin_tag_set);
 +	nvme_rdma_dev_put(ctrl->device);
++=======
+ 	if (remove) {
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, true);
+ 	}
+ }
+ 
+ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
+ 		bool new)
+ {
+ 	int error;
+ 
+ 	error = nvme_rdma_init_queue(ctrl, 0, NVME_AQ_DEPTH);
+ 	if (error)
+ 		return error;
+ 
+ 	ctrl->device = ctrl->queues[0].device;
+ 
+ 	ctrl->max_fr_pages = min_t(u32, NVME_RDMA_MAX_SEGMENTS,
+ 		ctrl->device->dev->attrs.max_fast_reg_page_list_len);
+ 
+ 	if (new) {
+ 		ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ 		if (IS_ERR(ctrl->ctrl.admin_tagset))
+ 			goto out_free_queue;
+ 
+ 		ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ 		if (IS_ERR(ctrl->ctrl.admin_q)) {
+ 			error = PTR_ERR(ctrl->ctrl.admin_q);
+ 			goto out_free_tagset;
+ 		}
+ 	} else {
+ 		error = blk_mq_reinit_tagset(&ctrl->admin_tag_set,
+ 					     nvme_rdma_reinit_request);
+ 		if (error)
+ 			goto out_free_queue;
+ 	}
+ 
+ 	error = nvmf_connect_admin_queue(&ctrl->ctrl);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	set_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[0].flags);
+ 
+ 	error = nvmf_reg_read64(&ctrl->ctrl, NVME_REG_CAP,
+ 			&ctrl->ctrl.cap);
+ 	if (error) {
+ 		dev_err(ctrl->ctrl.device,
+ 			"prop_get NVME_REG_CAP failed\n");
+ 		goto out_cleanup_queue;
+ 	}
+ 
+ 	ctrl->ctrl.sqsize =
+ 		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
+ 
+ 	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	ctrl->ctrl.max_hw_sectors =
+ 		(ctrl->max_fr_pages - 1) << (PAGE_SHIFT - 9);
+ 
+ 	error = nvme_init_identify(&ctrl->ctrl);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	error = nvme_rdma_alloc_qe(ctrl->queues[0].device->dev,
+ 			&ctrl->async_event_sqe, sizeof(struct nvme_command),
+ 			DMA_TO_DEVICE);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	return 0;
+ 
+ out_cleanup_queue:
+ 	if (new)
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ out_free_tagset:
+ 	if (new)
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, true);
+ out_free_queue:
+ 	nvme_rdma_free_queue(&ctrl->queues[0]);
+ 	return error;
++>>>>>>> 3f02fffb74ae (nvme-rdma: don't free tagset on resets)
  }
  
  static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
@@@ -1667,13 -1678,16 +1823,18 @@@ static void nvme_rdma_shutdown_ctrl(str
  		nvme_rdma_free_io_queues(ctrl);
  	}
  
 -	if (shutdown)
 +	if (test_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[0].flags))
  		nvme_shutdown_ctrl(&ctrl->ctrl);
 -	else
 -		nvme_disable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
  
 -	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
 +	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
  	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
  				nvme_cancel_request, &ctrl->ctrl);
++<<<<<<< HEAD
 +	nvme_rdma_destroy_admin_queue(ctrl);
++=======
+ 	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ 	nvme_rdma_destroy_admin_queue(ctrl, shutdown);
++>>>>>>> 3f02fffb74ae (nvme-rdma: don't free tagset on resets)
  }
  
  static void __nvme_rdma_remove_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
@@@ -1746,15 -1759,9 +1907,15 @@@ static void nvme_rdma_reset_ctrl_work(s
  	bool changed;
  
  	nvme_stop_ctrl(&ctrl->ctrl);
 -	nvme_rdma_shutdown_ctrl(ctrl, false);
 +	nvme_rdma_shutdown_ctrl(ctrl);
 +
 +	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RECONNECTING)) {
 +		/* state change failure should never happen */
 +		WARN_ON_ONCE(1);
 +		return;
 +	}
  
- 	ret = nvme_rdma_configure_admin_queue(ctrl);
+ 	ret = nvme_rdma_configure_admin_queue(ctrl, false);
  	if (ret) {
  		/* ctrl is already shutdown, just remove the ctrl */
  		INIT_WORK(&ctrl->delete_work, nvme_rdma_remove_ctrl_work);
* Unmerged path drivers/nvme/host/rdma.c
