dax: Factor out getting of pfn out of iomap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jan Kara <jack@suse.cz>
commit 5e161e4066d3ebeaff95a4b979b42f8bf00494d5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/5e161e40.failed

Factor out code to get pfn out of iomap that is shared between PTE and
PMD fault path.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 5e161e4066d3ebeaff95a4b979b42f8bf00494d5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 679214f8898b,116eef8d6c69..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -896,30 -820,59 +896,84 @@@ out
  }
  EXPORT_SYMBOL_GPL(dax_writeback_mapping_range);
  
 -static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
 +static int dax_insert_mapping(struct address_space *mapping,
 +		struct block_device *bdev, struct dax_device *dax_dev,
 +		sector_t sector, size_t size, void **entryp,
 +		struct vm_area_struct *vma, struct vm_fault *vmf)
  {
++<<<<<<< HEAD
 +	unsigned long vaddr = (unsigned long)vmf->virtual_address;
 +	void *entry = *entryp;
 +	void *ret, *kaddr;
 +	pgoff_t pgoff;
 +	int id, rc;
 +	pfn_t pfn;
 +
 +	rc = bdev_dax_pgoff(bdev, sector, size, &pgoff);
 +	if (rc)
 +		return rc;
 +
 +	id = dax_read_lock();
 +	rc = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
 +	if (rc < 0) {
 +		dax_read_unlock(id);
 +		return rc;
 +	}
 +	dax_read_unlock(id);
 +
++=======
+ 	return iomap->blkno + (((pos & PAGE_MASK) - iomap->offset) >> 9);
+ }
+ 
+ static int dax_iomap_pfn(struct iomap *iomap, loff_t pos, size_t size,
+ 			 pfn_t *pfnp)
+ {
+ 	const sector_t sector = dax_iomap_sector(iomap, pos);
+ 	pgoff_t pgoff;
+ 	void *kaddr;
+ 	int id, rc;
+ 	long length;
+ 
+ 	rc = bdev_dax_pgoff(iomap->bdev, sector, size, &pgoff);
+ 	if (rc)
+ 		return rc;
+ 	id = dax_read_lock();
+ 	length = dax_direct_access(iomap->dax_dev, pgoff, PHYS_PFN(size),
+ 				   &kaddr, pfnp);
+ 	if (length < 0) {
+ 		rc = length;
+ 		goto out;
+ 	}
+ 	rc = -EINVAL;
+ 	if (PFN_PHYS(length) < size)
+ 		goto out;
+ 	if (pfn_t_to_pfn(*pfnp) & (PHYS_PFN(size)-1))
+ 		goto out;
+ 	/* For larger pages we need devmap */
+ 	if (length > 1 && !pfn_t_devmap(*pfnp))
+ 		goto out;
+ 	rc = 0;
+ out:
+ 	dax_read_unlock(id);
+ 	return rc;
+ }
+ 
+ static int dax_insert_mapping(struct vm_fault *vmf, struct iomap *iomap,
+ 			      loff_t pos, void *entry)
+ {
+ 	const sector_t sector = dax_iomap_sector(iomap, pos);
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	unsigned long vaddr = vmf->address;
+ 	void *ret;
+ 	int rc;
+ 	pfn_t pfn;
+ 
+ 	rc = dax_iomap_pfn(iomap, pos, PAGE_SIZE, &pfn);
+ 	if (rc < 0)
+ 		return rc;
+ 
++>>>>>>> 5e161e4066d3 (dax: Factor out getting of pfn out of iomap)
  	ret = dax_insert_mapping_entry(mapping, vmf, entry, sector, 0);
  	if (IS_ERR(ret))
  		return PTR_ERR(ret);
@@@ -1317,59 -1241,31 +1371,71 @@@ static int dax_iomap_pte_fault(struct v
  }
  
  #ifdef CONFIG_FS_DAX_PMD
 +/*
 + * The 'colour' (ie low bits) within a PMD of a page offset.  This comes up
 + * more often than one might expect in the below functions.
 + */
 +#define PG_PMD_COLOUR	((PMD_SIZE >> PAGE_SHIFT) - 1)
 +
  static int dax_pmd_insert_mapping(struct vm_fault *vmf, struct iomap *iomap,
 -		loff_t pos, void *entry)
 +		loff_t pos, void **entryp)
  {
 +	unsigned long address = (unsigned long)vmf->virtual_address;
  	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
  	const sector_t sector = dax_iomap_sector(iomap, pos);
- 	struct dax_device *dax_dev = iomap->dax_dev;
- 	struct block_device *bdev = iomap->bdev;
  	struct inode *inode = mapping->host;
++<<<<<<< HEAD
 +	const size_t size = PMD_SIZE;
 +	void *ret = NULL, *kaddr;
 +	long length = 0;
 +	pgoff_t pgoff;
 +	pfn_t pfn;
 +	int id;
++=======
+ 	void *ret = NULL;
+ 	pfn_t pfn = {};
+ 	int rc;
++>>>>>>> 5e161e4066d3 (dax: Factor out getting of pfn out of iomap)
  
- 	if (bdev_dax_pgoff(bdev, sector, size, &pgoff) != 0)
+ 	rc = dax_iomap_pfn(iomap, pos, PMD_SIZE, &pfn);
+ 	if (rc < 0)
  		goto fallback;
  
++<<<<<<< HEAD
 +	id = dax_read_lock();
 +	length = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
 +	if (length < 0)
 +		goto unlock_fallback;
 +	length = PFN_PHYS(length);
 +
 +	if (length < size)
 +		goto unlock_fallback;
 +	if (pfn_t_to_pfn(pfn) & PG_PMD_COLOUR)
 +		goto unlock_fallback;
 +	if (!pfn_t_devmap(pfn))
 +		goto unlock_fallback;
 +	dax_read_unlock(id);
 +
 +	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, sector,
++=======
+ 	ret = dax_insert_mapping_entry(mapping, vmf, entry, sector,
++>>>>>>> 5e161e4066d3 (dax: Factor out getting of pfn out of iomap)
  			RADIX_DAX_PMD);
  	if (IS_ERR(ret))
  		goto fallback;
 +	*entryp = ret;
  
++<<<<<<< HEAD
 +	trace_dax_pmd_insert_mapping(inode, vmf, length, pfn, ret);
 +	return vmf_insert_pfn_pmd(vmf->vma, address, vmf->pmd,
++=======
+ 	trace_dax_pmd_insert_mapping(inode, vmf, PMD_SIZE, pfn, ret);
+ 	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd,
++>>>>>>> 5e161e4066d3 (dax: Factor out getting of pfn out of iomap)
  			pfn, vmf->flags & FAULT_FLAG_WRITE);
  
- unlock_fallback:
- 	dax_read_unlock(id);
  fallback:
- 	trace_dax_pmd_insert_mapping_fallback(inode, vmf, length, pfn, ret);
+ 	trace_dax_pmd_insert_mapping_fallback(inode, vmf, PMD_SIZE, pfn, ret);
  	return VM_FAULT_FALLBACK;
  }
  
* Unmerged path fs/dax.c
