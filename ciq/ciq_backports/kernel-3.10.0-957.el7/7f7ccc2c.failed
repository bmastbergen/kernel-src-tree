proc: do not access cmdline nor environ from file-backed areas

jira LE-1907
cve CVE-2018-1120
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Willy Tarreau <w@1wt.eu>
commit 7f7ccc2ccc2e70c6054685f5e3522efa81556830
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/7f7ccc2c.failed

proc_pid_cmdline_read() and environ_read() directly access the target
process' VM to retrieve the command line and environment. If this
process remaps these areas onto a file via mmap(), the requesting
process may experience various issues such as extra delays if the
underlying device is slow to respond.

Let's simply refuse to access file-backed areas in these functions.
For this we add a new FOLL_ANON gup flag that is passed to all calls
to access_remote_vm(). The code already takes care of such failures
(including unmapped areas). Accesses via /proc/pid/mem were not
changed though.

This was assigned CVE-2018-1120.

Note for stable backports: the patch may apply to kernels prior to 4.11
but silently miss one location; it must be checked that no call to
access_remote_vm() keeps zero as the last argument.

	Reported-by: Qualys Security Advisory <qsa@qualys.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: stable@vger.kernel.org
	Signed-off-by: Willy Tarreau <w@1wt.eu>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 7f7ccc2ccc2e70c6054685f5e3522efa81556830)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/base.c
#	mm/gup.c
diff --cc fs/proc/base.c
index 6b05bce3a573,1a76d751cf3c..000000000000
--- a/fs/proc/base.c
+++ b/fs/proc/base.c
@@@ -340,49 -301,66 +340,58 @@@ skip_argv
  		 * Command line (1 string) occupies ARGV and
  		 * extends into ENVP.
  		 */
 -		struct {
 -			unsigned long p;
 -			unsigned long len;
 -		} cmdline[2] = {
 -			{ .p = arg_start, .len = len1 },
 -			{ .p = env_start, .len = len2 },
 -		};
 -		loff_t pos1 = *pos;
 -		unsigned int i;
 -
 -		i = 0;
 -		while (i < 2 && pos1 >= cmdline[i].len) {
 -			pos1 -= cmdline[i].len;
 -			i++;
 +		if (len1 <= *pos) {
 +			p = env_start + *pos - len1;
 +			len = len1 + len2 - *pos;
 +		} else {
 +			p = env_start;
 +			len = len2;
  		}
 -		while (i < 2) {
 -			p = cmdline[i].p + pos1;
 -			len = cmdline[i].len - pos1;
 -			while (count > 0 && len > 0) {
 -				unsigned int _count, l;
 -				int nr_read;
 -				bool final;
 +		while (count > 0 && len > 0) {
 +			unsigned int _count, l;
 +			int nr_read;
 +			bool final;
  
++<<<<<<< HEAD
 +			_count = min3(count, len, PAGE_SIZE);
 +			nr_read = access_remote_vm(mm, p, page, _count, 0);
 +			if (nr_read < 0)
 +			        rv = nr_read;
 +			if (nr_read <= 0)
 +			        goto out_free_page;
++=======
+ 				_count = min3(count, len, PAGE_SIZE);
+ 				nr_read = access_remote_vm(mm, p, page, _count, FOLL_ANON);
+ 				if (nr_read < 0)
+ 					rv = nr_read;
+ 				if (nr_read <= 0)
+ 					goto out_free_page;
++>>>>>>> 7f7ccc2ccc2e (proc: do not access cmdline nor environ from file-backed areas)
 +
 +			/* Find EOS. */
 +			final = false;
 +			l = strnlen(page, nr_read);
 +			if (l < nr_read) {
 +			        nr_read = l;
 +			        final = true;
 +			}
  
 -				/*
 -				 * Command line can be shorter than whole ARGV
 -				 * even if last "marker" byte says it is not.
 -				 */
 -				final = false;
 -				l = strnlen(page, nr_read);
 -				if (l < nr_read) {
 -					nr_read = l;
 -					final = true;
 -				}
 -
 -				if (copy_to_user(buf, page, nr_read)) {
 -					rv = -EFAULT;
 -					goto out_free_page;
 -				}
 -
 -				p	+= nr_read;
 -				len	-= nr_read;
 -				buf	+= nr_read;
 -				count	-= nr_read;
 -				rv	+= nr_read;
 -
 -				if (final)
 -					goto out_free_page;
 +			if (copy_to_user(buf, page, nr_read)) {
 +			        rv = -EFAULT;
 +			        goto out_free_page;
  			}
  
 -			/* Only first chunk can be read partially. */
 -			pos1 = 0;
 -			i++;
 +			p       += nr_read;
 +			len     -= nr_read;
 +			buf     += nr_read;
 +			count   -= nr_read;
 +			rv      += nr_read;
 +
 +			if (final)
 +			        goto out_free_page;
  		}
 +skip_argv_envp:
 +		;
  	}
  
  out_free_page:
@@@ -1047,8 -946,7 +1056,12 @@@ static ssize_t environ_read(struct fil
  		max_len = min_t(size_t, PAGE_SIZE, count);
  		this_len = min(max_len, this_len);
  
++<<<<<<< HEAD
 +		retval = access_remote_vm(mm, (env_start + src),
 +			page, this_len, 0);
++=======
+ 		retval = access_remote_vm(mm, (env_start + src), page, this_len, FOLL_ANON);
++>>>>>>> 7f7ccc2ccc2e (proc: do not access cmdline nor environ from file-backed areas)
  
  		if (retval <= 0) {
  			ret = retval;
diff --cc mm/gup.c
index 20b926f646c5,541904a7c60f..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -269,21 -204,165 +269,74 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -static struct page *follow_pmd_mask(struct vm_area_struct *vma,
 -				    unsigned long address, pud_t *pudp,
 -				    unsigned int flags, unsigned int *page_mask)
 -{
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	pmd = pmd_offset(pudp, address);
 -	if (pmd_none(*pmd))
 -		return no_page_table(vma, flags);
 -	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
 -		page = follow_huge_pmd(mm, address, pmd, flags);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if (is_hugepd(__hugepd(pmd_val(*pmd)))) {
 -		page = follow_huge_pd(vma, address,
 -				      __hugepd(pmd_val(*pmd)), flags,
 -				      PMD_SHIFT);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -retry:
 -	if (!pmd_present(*pmd)) {
 -		if (likely(!(flags & FOLL_MIGRATION)))
 -			return no_page_table(vma, flags);
 -		VM_BUG_ON(thp_migration_supported() &&
 -				  !is_pmd_migration_entry(*pmd));
 -		if (is_pmd_migration_entry(*pmd))
 -			pmd_migration_entry_wait(mm, pmd);
 -		goto retry;
 -	}
 -	if (pmd_devmap(*pmd)) {
 -		ptl = pmd_lock(mm, pmd);
 -		page = follow_devmap_pmd(vma, address, pmd, flags);
 -		spin_unlock(ptl);
 -		if (page)
 -			return page;
 -	}
 -	if (likely(!pmd_trans_huge(*pmd)))
 -		return follow_page_pte(vma, address, pmd, flags);
 -
 -	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
 -		return no_page_table(vma, flags);
 -
 -retry_locked:
 -	ptl = pmd_lock(mm, pmd);
 -	if (unlikely(!pmd_present(*pmd))) {
 -		spin_unlock(ptl);
 -		if (likely(!(flags & FOLL_MIGRATION)))
 -			return no_page_table(vma, flags);
 -		pmd_migration_entry_wait(mm, pmd);
 -		goto retry_locked;
 -	}
 -	if (unlikely(!pmd_trans_huge(*pmd))) {
 -		spin_unlock(ptl);
 -		return follow_page_pte(vma, address, pmd, flags);
 -	}
 -	if (flags & FOLL_SPLIT) {
 -		int ret;
 -		page = pmd_page(*pmd);
 -		if (is_huge_zero_page(page)) {
 -			spin_unlock(ptl);
 -			ret = 0;
 -			split_huge_pmd(vma, pmd, address);
 -			if (pmd_trans_unstable(pmd))
 -				ret = -EBUSY;
 -		} else {
 -			get_page(page);
 -			spin_unlock(ptl);
 -			lock_page(page);
 -			ret = split_huge_page(page);
 -			unlock_page(page);
 -			put_page(page);
 -			if (pmd_none(*pmd))
 -				return no_page_table(vma, flags);
 -		}
 +		return page;
  
 -		return ret ? ERR_PTR(ret) :
 -			follow_page_pte(vma, address, pmd, flags);
 -	}
 -	page = follow_trans_huge_pmd(vma, address, pmd, flags);
 -	spin_unlock(ptl);
 -	*page_mask = HPAGE_PMD_NR - 1;
 +no_page_table:
 +	/*
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
 +	 */
++<<<<<<< HEAD
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
  	return page;
++=======
++	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
++		*flags |= FOLL_COW;
++	return 0;
+ }
+ 
 -
 -static struct page *follow_pud_mask(struct vm_area_struct *vma,
 -				    unsigned long address, p4d_t *p4dp,
 -				    unsigned int flags, unsigned int *page_mask)
++static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
+ {
 -	pud_t *pud;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	pud = pud_offset(p4dp, address);
 -	if (pud_none(*pud))
 -		return no_page_table(vma, flags);
 -	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
 -		page = follow_huge_pud(mm, address, pud, flags);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if (is_hugepd(__hugepd(pud_val(*pud)))) {
 -		page = follow_huge_pd(vma, address,
 -				      __hugepd(pud_val(*pud)), flags,
 -				      PUD_SHIFT);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if (pud_devmap(*pud)) {
 -		ptl = pud_lock(mm, pud);
 -		page = follow_devmap_pud(vma, address, pud, flags);
 -		spin_unlock(ptl);
 -		if (page)
 -			return page;
 -	}
 -	if (unlikely(pud_bad(*pud)))
 -		return no_page_table(vma, flags);
 -
 -	return follow_pmd_mask(vma, address, pud, flags, page_mask);
 -}
++	vm_flags_t vm_flags = vma->vm_flags;
++	int write = (gup_flags & FOLL_WRITE);
++	int foreign = (gup_flags & FOLL_REMOTE);
+ 
++	if (vm_flags & (VM_IO | VM_PFNMAP))
++		return -EFAULT;
+ 
 -static struct page *follow_p4d_mask(struct vm_area_struct *vma,
 -				    unsigned long address, pgd_t *pgdp,
 -				    unsigned int flags, unsigned int *page_mask)
 -{
 -	p4d_t *p4d;
 -	struct page *page;
++	if (gup_flags & FOLL_ANON && !vma_is_anonymous(vma))
++		return -EFAULT;
+ 
 -	p4d = p4d_offset(pgdp, address);
 -	if (p4d_none(*p4d))
 -		return no_page_table(vma, flags);
 -	BUILD_BUG_ON(p4d_huge(*p4d));
 -	if (unlikely(p4d_bad(*p4d)))
 -		return no_page_table(vma, flags);
 -
 -	if (is_hugepd(__hugepd(p4d_val(*p4d)))) {
 -		page = follow_huge_pd(vma, address,
 -				      __hugepd(p4d_val(*p4d)), flags,
 -				      P4D_SHIFT);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
++	if (write) {
++		if (!(vm_flags & VM_WRITE)) {
++			if (!(gup_flags & FOLL_FORCE))
++				return -EFAULT;
++			/*
++			 * We used to let the write,force case do COW in a
++			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
++			 * set a breakpoint in a read-only mapping of an
++			 * executable, without corrupting the file (yet only
++			 * when that file had been opened for writing!).
++			 * Anon pages in shared mappings are surprising: now
++			 * just reject it.
++			 */
++			if (!is_cow_mapping(vm_flags))
++				return -EFAULT;
++		}
++	} else if (!(vm_flags & VM_READ)) {
++		if (!(gup_flags & FOLL_FORCE))
++			return -EFAULT;
++		/*
++		 * Is there actually any vma we can reach here which does not
++		 * have VM_MAYREAD set?
++		 */
++		if (!(vm_flags & VM_MAYREAD))
++			return -EFAULT;
+ 	}
 -	return follow_pud_mask(vma, address, p4d, flags, page_mask);
++	/*
++	 * gups are always data accesses, not instruction
++	 * fetches, so execute=false here
++	 */
++	if (!arch_vma_access_permitted(vma, write, false, foreign))
++		return -EFAULT;
++	return 0;
++>>>>>>> 7f7ccc2ccc2e (proc: do not access cmdline nor environ from file-backed areas)
  }
  
  /**
* Unmerged path fs/proc/base.c
diff --git a/include/linux/mm.h b/include/linux/mm.h
index d2f11a68bcdf..67759d31887d 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2279,6 +2279,7 @@ static inline struct page *follow_page(struct vm_area_struct *vma,
 #define FOLL_TRIED	0x800	/* a retry, previous pass started an IO */
 #define FOLL_REMOTE	0x2000	/* we are working on non-current tsk/mm */
 #define FOLL_COW	0x4000	/* internal GUP flag */
+#define FOLL_ANON	0x8000	/* don't do file mappings */
 
 typedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,
 			void *data);
* Unmerged path mm/gup.c
