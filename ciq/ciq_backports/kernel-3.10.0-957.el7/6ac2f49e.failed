x86/bugs: Add AMD's SPEC_CTRL MSR usage

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] bugs: Add AMD's SPEC_CTRL MSR usage (Waiman Long) [1584569] {CVE-2018-3639}
Rebuild_FUZZ: 94.59%
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 6ac2f49edb1ef5446089c7c660017732886d62d6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6ac2f49e.failed

The AMD document outlining the SSBD handling
124441_AMD64_SpeculativeStoreBypassDisable_Whitepaper_final.pdf
mentions that if CPUID 8000_0008.EBX[24] is set we should be using
the SPEC_CTRL MSR (0x48) over the VIRT SPEC_CTRL MSR (0xC001_011f)
for speculative store bypass disable.

This in effect means we should clear the X86_FEATURE_VIRT_SSBD
flag so that we would prefer the SPEC_CTRL MSR.

See the document titled:
   124441_AMD64_SpeculativeStoreBypassDisable_Whitepaper_final.pdf

A copy of this document is available at
   https://bugzilla.kernel.org/show_bug.cgi?id=199889

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Janakarajan Natarajan <Janakarajan.Natarajan@amd.com>
	Cc: kvm@vger.kernel.org
	Cc: KarimAllah Ahmed <karahmed@amazon.de>
	Cc: andrew.cooper3@citrix.com
	Cc: Joerg Roedel <joro@8bytes.org>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: David Woodhouse <dwmw@amazon.co.uk>
	Cc: Kees Cook <keescook@chromium.org>
Link: https://lkml.kernel.org/r/20180601145921.9500-3-konrad.wilk@oracle.com

(cherry picked from commit 6ac2f49edb1ef5446089c7c660017732886d62d6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
#	arch/x86/kvm/cpuid.c
#	arch/x86/kvm/svm.c
diff --cc arch/x86/include/asm/cpufeatures.h
index 7a3e9c71ed7d,5701f5cecd31..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -190,126 -190,150 +190,138 @@@
   *
   * Reuse free bits when adding new feature flags!
   */
 -#define X86_FEATURE_RING3MWAIT		( 7*32+ 0) /* Ring 3 MONITOR/MWAIT instructions */
 -#define X86_FEATURE_CPUID_FAULT		( 7*32+ 1) /* Intel CPUID faulting */
 -#define X86_FEATURE_CPB			( 7*32+ 2) /* AMD Core Performance Boost */
 -#define X86_FEATURE_EPB			( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 -#define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
 -#define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
 -#define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
 -#define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID && CR4.PCIDE=1 */
 -#define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
 -#define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 -#define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
 -#define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
 -#define X86_FEATURE_RETPOLINE		( 7*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* "" AMD Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 -#define X86_FEATURE_CDP_L2		( 7*32+15) /* Code and Data Prioritization L2 */
 -#define X86_FEATURE_MSR_SPEC_CTRL	( 7*32+16) /* "" MSR SPEC_CTRL is implemented */
 -#define X86_FEATURE_SSBD		( 7*32+17) /* Speculative Store Bypass Disable */
 -#define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
 -#define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* "" Fill RSB on context switches */
 -#define X86_FEATURE_SEV			( 7*32+20) /* AMD Secure Encrypted Virtualization */
 -#define X86_FEATURE_USE_IBPB		( 7*32+21) /* "" Indirect Branch Prediction Barrier enabled */
 -#define X86_FEATURE_USE_IBRS_FW		( 7*32+22) /* "" Use IBRS during runtime firmware calls */
 -#define X86_FEATURE_SPEC_STORE_BYPASS_DISABLE	( 7*32+23) /* "" Disable Speculative Store Bypass. */
 -#define X86_FEATURE_LS_CFG_SSBD		( 7*32+24)  /* "" AMD SSBD implementation via LS_CFG MSR */
 -#define X86_FEATURE_IBRS		( 7*32+25) /* Indirect Branch Restricted Speculation */
 -#define X86_FEATURE_IBPB		( 7*32+26) /* Indirect Branch Prediction Barrier */
 -#define X86_FEATURE_STIBP		( 7*32+27) /* Single Thread Indirect Branch Predictors */
 -#define X86_FEATURE_ZEN			( 7*32+28) /* "" CPU is AMD family 0x17 (Zen) */
  
 -/* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_RING3MWAIT	(7*32+ 0) /* Ring 3 MONITOR/MWAIT */
 +#define X86_FEATURE_CPB		(7*32+ 2) /* AMD Core Performance Boost */
 +#define X86_FEATURE_EPB		(7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 +#define X86_FEATURE_CAT_L3	(7*32+ 4) /* Cache Allocation Technology L3 */
 +#define X86_FEATURE_CAT_L2	(7*32+ 5) /* Cache Allocation Technology L2 */
 +#define X86_FEATURE_CDP_L3	(7*32+ 6) /* Code and Data Prioritization L3 */
  
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 +#define X86_FEATURE_HW_PSTATE	(7*32+ 8) /* AMD HW-PState */
 +#define X86_FEATURE_PROC_FEEDBACK (7*32+ 9) /* AMD ProcFeedbackInterface */
 +#define X86_FEATURE_SME		( 7*32+10) /* AMD Secure Memory Encryption */
 +#define X86_FEATURE_RETPOLINE_AMD (7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */
 +#define X86_FEATURE_INTEL_PPIN	( 7*32+14) /* Intel Processor Inventory Number */
 +#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */
  
 +#define X86_FEATURE_MBA		( 7*32+18) /* Memory Bandwidth Allocation */
 +#define X86_FEATURE_IBP_DISABLE ( 7*32+21) /* Old AMD Indirect Branch Predictor Disable */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +/* Virtualization flags: Linux defined, word 8 */
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
 +
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
  
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (EDX), word 11 */
 -#define X86_FEATURE_CQM_LLC		(11*32+ 1) /* LLC QoS if 1 */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (EDX), word 12 */
 -#define X86_FEATURE_CQM_OCCUP_LLC	(12*32+ 0) /* LLC occupancy monitoring */
 -#define X86_FEATURE_CQM_MBM_TOTAL	(12*32+ 1) /* LLC Total MBM monitoring */
 -#define X86_FEATURE_CQM_MBM_LOCAL	(12*32+ 2) /* LLC Local MBM monitoring */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
++<<<<<<< HEAD
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
 +#define X86_FEATURE_IBPB		(13*32+12) /* Indirect Branch Prediction Barrier */
 +#define X86_FEATURE_IBRS		(13*32+14) /* Indirect Branch Restricted Speculation */
 +#define X86_FEATURE_STIBP		(13*32+15) /* Single Thread Indirect Branch Predictors */
++=======
+ #define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
+ #define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
+ #define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
+ #define X86_FEATURE_AMD_IBPB		(13*32+12) /* "" Indirect Branch Prediction Barrier */
+ #define X86_FEATURE_AMD_IBRS		(13*32+14) /* "" Indirect Branch Restricted Speculation */
+ #define X86_FEATURE_AMD_STIBP		(13*32+15) /* "" Single Thread Indirect Branch Predictors */
+ #define X86_FEATURE_AMD_SSBD		(13*32+24) /* "" Speculative Store Bypass Disable */
+ #define X86_FEATURE_VIRT_SSBD		(13*32+25) /* Virtualized Speculative Store Bypass Disable */
+ #define X86_FEATURE_AMD_SSB_NO		(13*32+26) /* "" Speculative Store Bypass is fixed in hardware. */
++>>>>>>> 6ac2f49edb1e (x86/bugs: Add AMD's SPEC_CTRL MSR usage)
  
 -/* Thermal and Power Management Leaf, CPUID level 0x00000006 (EAX), word 14 */
 -#define X86_FEATURE_DTHERM		(14*32+ 0) /* Digital Thermal Sensor */
 -#define X86_FEATURE_IDA			(14*32+ 1) /* Intel Dynamic Acceleration */
 -#define X86_FEATURE_ARAT		(14*32+ 2) /* Always Running APIC Timer */
 -#define X86_FEATURE_PLN			(14*32+ 4) /* Intel Power Limit Notification */
 -#define X86_FEATURE_PTS			(14*32+ 6) /* Intel Package Thermal Status */
 -#define X86_FEATURE_HWP			(14*32+ 7) /* Intel Hardware P-states */
 -#define X86_FEATURE_HWP_NOTIFY		(14*32+ 8) /* HWP Notification */
 -#define X86_FEATURE_HWP_ACT_WINDOW	(14*32+ 9) /* HWP Activity Window */
 -#define X86_FEATURE_HWP_EPP		(14*32+10) /* HWP Energy Perf. Preference */
 -#define X86_FEATURE_HWP_PKG_REQ		(14*32+11) /* HWP Package Level Request */
 +/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */
 +#define X86_FEATURE_OVERFLOW_RECOV (17*32+0) /* MCA overflow recovery support */
 +#define X86_FEATURE_SUCCOR	(17*32+1) /* Uncorrectable error containment and recovery */
 +#define X86_FEATURE_SMCA	(17*32+3) /* Scalable MCA */
  
 -/* AMD SVM Feature Identification, CPUID level 0x8000000a (EDX), word 15 */
 -#define X86_FEATURE_NPT			(15*32+ 0) /* Nested Page Table support */
 -#define X86_FEATURE_LBRV		(15*32+ 1) /* LBR Virtualization support */
 -#define X86_FEATURE_SVML		(15*32+ 2) /* "svm_lock" SVM locking MSR */
 -#define X86_FEATURE_NRIPS		(15*32+ 3) /* "nrip_save" SVM next_rip save */
 -#define X86_FEATURE_TSCRATEMSR		(15*32+ 4) /* "tsc_scale" TSC scaling support */
 -#define X86_FEATURE_VMCBCLEAN		(15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 -#define X86_FEATURE_FLUSHBYASID		(15*32+ 6) /* flush-by-ASID support */
 -#define X86_FEATURE_DECODEASSISTS	(15*32+ 7) /* Decode Assists support */
 -#define X86_FEATURE_PAUSEFILTER		(15*32+10) /* filtered pause intercept */
 -#define X86_FEATURE_PFTHRESHOLD		(15*32+12) /* pause filter threshold */
 -#define X86_FEATURE_AVIC		(15*32+13) /* Virtual Interrupt Controller */
 -#define X86_FEATURE_V_VMSAVE_VMLOAD	(15*32+15) /* Virtual VMSAVE VMLOAD */
 -#define X86_FEATURE_VGIF		(15*32+16) /* Virtual GIF */
 +/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */
 +#define X86_FEATURE_DTHERM	(14*32+ 0) /* Digital Thermal Sensor */
 +#define X86_FEATURE_IDA		(14*32+ 1) /* Intel Dynamic Acceleration */
 +#define X86_FEATURE_ARAT	(14*32+ 2) /* Always Running APIC Timer */
 +#define X86_FEATURE_PLN		(14*32+ 4) /* Intel Power Limit Notification */
 +#define X86_FEATURE_PTS		(14*32+ 6) /* Intel Package Thermal Status */
 +#define X86_FEATURE_HWP		(14*32+ 7) /* Intel Hardware P-states */
 +#define X86_FEATURE_HWP_NOTIFY	(14*32+ 8) /* HWP Notification */
 +#define X86_FEATURE_HWP_ACT_WINDOW (14*32+ 9) /* HWP Activity Window */
 +#define X86_FEATURE_HWP_EPP	(14*32+10) /* HWP Energy Perf. Preference */
 +#define X86_FEATURE_HWP_PKG_REQ (14*32+11) /* HWP Package Level Request */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (ECX), word 16 */
 -#define X86_FEATURE_AVX512VBMI		(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 -#define X86_FEATURE_UMIP		(16*32+ 2) /* User Mode Instruction Protection */
 -#define X86_FEATURE_PKU			(16*32+ 3) /* Protection Keys for Userspace */
 -#define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */
 -#define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 -#define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */
 -#define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */
 -#define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */
 -#define X86_FEATURE_AVX512_VNNI		(16*32+11) /* Vector Neural Network Instructions */
 -#define X86_FEATURE_AVX512_BITALG	(16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB instructions */
 -#define X86_FEATURE_TME			(16*32+13) /* Intel Total Memory Encryption */
 -#define X86_FEATURE_AVX512_VPOPCNTDQ	(16*32+14) /* POPCNT for vectors of DW/QW */
 -#define X86_FEATURE_LA57		(16*32+16) /* 5-level page tables */
 -#define X86_FEATURE_RDPID		(16*32+22) /* RDPID instruction */
 -#define X86_FEATURE_CLDEMOTE		(16*32+25) /* CLDEMOTE instruction */
 +/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */
 +#define X86_FEATURE_NPT		(15*32+ 0) /* Nested Page Table support */
 +#define X86_FEATURE_LBRV	(15*32+ 1) /* LBR Virtualization support */
 +#define X86_FEATURE_SVML	(15*32+ 2) /* "svm_lock" SVM locking MSR */
 +#define X86_FEATURE_NRIPS	(15*32+ 3) /* "nrip_save" SVM next_rip save */
 +#define X86_FEATURE_TSCRATEMSR  (15*32+ 4) /* "tsc_scale" TSC scaling support */
 +#define X86_FEATURE_VMCBCLEAN   (15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 +#define X86_FEATURE_FLUSHBYASID (15*32+ 6) /* flush-by-ASID support */
 +#define X86_FEATURE_DECODEASSISTS (15*32+ 7) /* Decode Assists support */
 +#define X86_FEATURE_PAUSEFILTER (15*32+10) /* filtered pause intercept */
 +#define X86_FEATURE_PFTHRESHOLD (15*32+12) /* pause filter threshold */
 +#define X86_FEATURE_AVIC	(15*32+13) /* Virtual Interrupt Controller */
 +#define X86_FEATURE_V_VMSAVE_VMLOAD (15*32+15) /* Virtual VMSAVE VMLOAD */
 +#define X86_FEATURE_VGIF	(15*32+16) /* Virtual GIF */
  
 -/* AMD-defined CPU features, CPUID level 0x80000007 (EBX), word 17 */
 -#define X86_FEATURE_OVERFLOW_RECOV	(17*32+ 0) /* MCA overflow recovery support */
 -#define X86_FEATURE_SUCCOR		(17*32+ 1) /* Uncorrectable error containment and recovery */
 -#define X86_FEATURE_SMCA		(17*32+ 3) /* Scalable MCA */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */
 +#define X86_FEATURE_AVX512VBMI	(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 +#define X86_FEATURE_PKU		(16*32+ 3) /* Protection Keys for Userspace */
 +#define X86_FEATURE_OSPKE	(16*32+ 4) /* OS Protection Keys Enable */
 +#define X86_FEATURE_AVX512_VBMI2 (16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 +#define X86_FEATURE_GFNI	(16*32+ 8) /* Galois Field New Instructions */
 +#define X86_FEATURE_VAES	(16*32+ 9) /* Vector AES */
 +#define X86_FEATURE_VPCLMULQDQ	(16*32+ 10) /* Carry-Less Multiplication Double Quadword */
 +#define X86_FEATURE_AVX512_VNNI (16*32+ 11) /* Vector Neural Network Instructions */
 +#define X86_FEATURE_AVX512_BITALG (16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB */
 +#define X86_FEATURE_AVX512_VPOPCNTDQ (16*32+14) /* POPCNT for vectors of DW/QW */
  
  /* Intel-defined CPU features, CPUID level 0x00000007:0 (EDX), word 18 */
  #define X86_FEATURE_AVX512_4VNNIW	(18*32+ 2) /* AVX-512 Neural Network Instructions */
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,6bea81855cdd..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -199,15 -422,271 +199,251 @@@ static void __init spectre_v2_select_mi
  }
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
+ 
+ static enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;
+ 
+ /* The kernel command line selection */
+ enum ssb_mitigation_cmd {
+ 	SPEC_STORE_BYPASS_CMD_NONE,
+ 	SPEC_STORE_BYPASS_CMD_AUTO,
+ 	SPEC_STORE_BYPASS_CMD_ON,
+ 	SPEC_STORE_BYPASS_CMD_PRCTL,
+ 	SPEC_STORE_BYPASS_CMD_SECCOMP,
+ };
+ 
+ static const char *ssb_strings[] = {
+ 	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
+ 	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled",
+ 	[SPEC_STORE_BYPASS_PRCTL]	= "Mitigation: Speculative Store Bypass disabled via prctl",
+ 	[SPEC_STORE_BYPASS_SECCOMP]	= "Mitigation: Speculative Store Bypass disabled via prctl and seccomp",
+ };
+ 
+ static const struct {
+ 	const char *option;
+ 	enum ssb_mitigation_cmd cmd;
+ } ssb_mitigation_options[] = {
+ 	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO },    /* Platform decides */
+ 	{ "on",		SPEC_STORE_BYPASS_CMD_ON },      /* Disable Speculative Store Bypass */
+ 	{ "off",	SPEC_STORE_BYPASS_CMD_NONE },    /* Don't touch Speculative Store Bypass */
+ 	{ "prctl",	SPEC_STORE_BYPASS_CMD_PRCTL },   /* Disable Speculative Store Bypass via prctl */
+ 	{ "seccomp",	SPEC_STORE_BYPASS_CMD_SECCOMP }, /* Disable Speculative Store Bypass via prctl and seccomp */
+ };
+ 
+ static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
+ {
+ 	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
+ 		return SPEC_STORE_BYPASS_CMD_NONE;
+ 	} else {
+ 		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
+ 					  arg, sizeof(arg));
+ 		if (ret < 0)
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
+ 			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
+ 				continue;
+ 
+ 			cmd = ssb_mitigation_options[i].cmd;
+ 			break;
+ 		}
+ 
+ 		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
+ 			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 		}
+ 	}
+ 
+ 	return cmd;
+ }
+ 
+ static enum ssb_mitigation __init __ssb_select_mitigation(void)
+ {
+ 	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
+ 	enum ssb_mitigation_cmd cmd;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_SSBD))
+ 		return mode;
+ 
+ 	cmd = ssb_parse_cmdline();
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
+ 	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
+ 	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
+ 		return mode;
+ 
+ 	switch (cmd) {
+ 	case SPEC_STORE_BYPASS_CMD_AUTO:
+ 	case SPEC_STORE_BYPASS_CMD_SECCOMP:
+ 		/*
+ 		 * Choose prctl+seccomp as the default mode if seccomp is
+ 		 * enabled.
+ 		 */
+ 		if (IS_ENABLED(CONFIG_SECCOMP))
+ 			mode = SPEC_STORE_BYPASS_SECCOMP;
+ 		else
+ 			mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_ON:
+ 		mode = SPEC_STORE_BYPASS_DISABLE;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_PRCTL:
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_NONE:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * We have three CPU feature flags that are in play here:
+ 	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.
+ 	 *  - X86_FEATURE_SSBD - CPU is able to turn off speculative store bypass
+ 	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation
+ 	 */
+ 	if (mode == SPEC_STORE_BYPASS_DISABLE) {
+ 		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
+ 		/*
+ 		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD may
+ 		 * use a completely different MSR and bit dependent on family.
+ 		 */
+ 		switch (boot_cpu_data.x86_vendor) {
+ 		case X86_VENDOR_INTEL:
+ 		case X86_VENDOR_AMD:
+ 			if (!static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL)) {
+ 				x86_amd_ssb_disable();
+ 				break;
+ 			}
+ 			x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
+ 			x86_spec_ctrl_mask |= SPEC_CTRL_SSBD;
+ 			wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ 			break;
+ 		}
+ 	}
+ 
+ 	return mode;
+ }
+ 
+ static void ssb_select_mitigation(void)
+ {
+ 	ssb_mode = __ssb_select_mitigation();
+ 
+ 	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		pr_info("%s\n", ssb_strings[ssb_mode]);
+ }
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Speculation prctl: " fmt
+ 
+ static int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)
+ {
+ 	bool update;
+ 
+ 	if (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&
+ 	    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)
+ 		return -ENXIO;
+ 
+ 	switch (ctrl) {
+ 	case PR_SPEC_ENABLE:
+ 		/* If speculation is force disabled, enable is not allowed */
+ 		if (task_spec_ssb_force_disable(task))
+ 			return -EPERM;
+ 		task_clear_spec_ssb_disable(task);
+ 		update = test_and_clear_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	case PR_SPEC_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	case PR_SPEC_FORCE_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		task_set_spec_ssb_force_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	default:
+ 		return -ERANGE;
+ 	}
+ 
+ 	/*
+ 	 * If being set on non-current task, delay setting the CPU
+ 	 * mitigation until it is next scheduled.
+ 	 */
+ 	if (task == current && update)
+ 		speculative_store_bypass_update_current();
+ 
+ 	return 0;
+ }
+ 
+ int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
+ 			     unsigned long ctrl)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_set(task, ctrl);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ #ifdef CONFIG_SECCOMP
+ void arch_seccomp_spec_mitigate(struct task_struct *task)
+ {
+ 	if (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)
+ 		ssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);
+ }
+ #endif
+ 
+ static int ssb_prctl_get(struct task_struct *task)
+ {
+ 	switch (ssb_mode) {
+ 	case SPEC_STORE_BYPASS_DISABLE:
+ 		return PR_SPEC_DISABLE;
+ 	case SPEC_STORE_BYPASS_SECCOMP:
+ 	case SPEC_STORE_BYPASS_PRCTL:
+ 		if (task_spec_ssb_force_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;
+ 		if (task_spec_ssb_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_DISABLE;
+ 		return PR_SPEC_PRCTL | PR_SPEC_ENABLE;
+ 	default:
+ 		if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 			return PR_SPEC_ENABLE;
+ 		return PR_SPEC_NOT_AFFECTED;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_get(task);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ void x86_spec_ctrl_setup_ap(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ 
+ 	if (ssb_mode == SPEC_STORE_BYPASS_DISABLE)
+ 		x86_amd_ssb_disable();
+ }
++>>>>>>> 6ac2f49edb1e (x86/bugs: Add AMD's SPEC_CTRL MSR usage)
  
  #ifdef CONFIG_SYSFS
 -
 -static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			       char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
 -
 -	switch (bug) {
 -	case X86_BUG_CPU_MELTDOWN:
 -		if (boot_cpu_has(X86_FEATURE_PTI))
 -			return sprintf(buf, "Mitigation: PTI\n");
 -
 -		break;
 -
 -	case X86_BUG_SPECTRE_V1:
 -		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 -
 -	case X86_BUG_SPECTRE_V2:
 -		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
 -			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
 -			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 -			       spectre_v2_module_string());
 -
 -	case X86_BUG_SPEC_STORE_BYPASS:
 -		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
 -
 -	default:
 -		break;
 -	}
 -
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
  	return sprintf(buf, "Vulnerable\n");
  }
  
diff --cc arch/x86/kernel/cpu/common.c
index 49cb90f121df,910b47ee8078..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -723,9 -781,34 +723,35 @@@ static void init_speculation_control(st
  	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {
  		set_cpu_cap(c, X86_FEATURE_IBRS);
  		set_cpu_cap(c, X86_FEATURE_IBPB);
 -		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
  	}
 -
  	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
  		set_cpu_cap(c, X86_FEATURE_STIBP);
++<<<<<<< HEAD
++=======
+ 
+ 	if (cpu_has(c, X86_FEATURE_SPEC_CTRL_SSBD) ||
+ 	    cpu_has(c, X86_FEATURE_VIRT_SSBD))
+ 		set_cpu_cap(c, X86_FEATURE_SSBD);
+ 
+ 	if (cpu_has(c, X86_FEATURE_AMD_IBRS)) {
+ 		set_cpu_cap(c, X86_FEATURE_IBRS);
+ 		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+ 	}
+ 
+ 	if (cpu_has(c, X86_FEATURE_AMD_IBPB))
+ 		set_cpu_cap(c, X86_FEATURE_IBPB);
+ 
+ 	if (cpu_has(c, X86_FEATURE_AMD_STIBP)) {
+ 		set_cpu_cap(c, X86_FEATURE_STIBP);
+ 		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+ 	}
+ 
+ 	if (cpu_has(c, X86_FEATURE_AMD_SSBD)) {
+ 		set_cpu_cap(c, X86_FEATURE_SSBD);
+ 		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+ 		clear_cpu_cap(c, X86_FEATURE_VIRT_SSBD);
+ 	}
++>>>>>>> 6ac2f49edb1e (x86/bugs: Add AMD's SPEC_CTRL MSR usage)
  }
  
  void get_cpu_cap(struct cpuinfo_x86 *c)
diff --cc arch/x86/kvm/cpuid.c
index 7bb663af32bb,f4f30d0c25c4..000000000000
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@@ -357,7 -374,13 +357,17 @@@ static inline int __do_cpuid_ent(struc
  		F(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |
  		F(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |
  		F(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |
++<<<<<<< HEAD
 +		0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM);
++=======
+ 		0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM) |
+ 		F(TOPOEXT) | F(PERFCTR_CORE);
+ 
+ 	/* cpuid 0x80000008.ebx */
+ 	const u32 kvm_cpuid_8000_0008_ebx_x86_features =
+ 		F(AMD_IBPB) | F(AMD_IBRS) | F(AMD_SSBD) | F(VIRT_SSBD) |
+ 		F(AMD_SSB_NO);
++>>>>>>> 6ac2f49edb1e (x86/bugs: Add AMD's SPEC_CTRL MSR usage)
  
  	/* cpuid 0xC0000001.edx */
  	const u32 kvm_cpuid_C000_0001_edx_x86_features =
@@@ -623,9 -652,26 +633,20 @@@
  		if (!g_phys_as)
  			g_phys_as = phys_as;
  		entry->eax = g_phys_as | (virt_as << 8);
 -		entry->edx = 0;
 -		/*
 -		 * IBRS, IBPB and VIRT_SSBD aren't necessarily present in
 -		 * hardware cpuid
 -		 */
 -		if (boot_cpu_has(X86_FEATURE_AMD_IBPB))
 -			entry->ebx |= F(AMD_IBPB);
 -		if (boot_cpu_has(X86_FEATURE_AMD_IBRS))
 -			entry->ebx |= F(AMD_IBRS);
 -		if (boot_cpu_has(X86_FEATURE_VIRT_SSBD))
 -			entry->ebx |= F(VIRT_SSBD);
  		entry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;
++<<<<<<< HEAD
 +		cpuid_mask(&entry->ecx, CPUID_8000_0008_EBX);
 +		entry->edx = 0;
++=======
+ 		cpuid_mask(&entry->ebx, CPUID_8000_0008_EBX);
+ 		/*
+ 		 * The preference is to use SPEC CTRL MSR instead of the
+ 		 * VIRT_SPEC MSR.
+ 		 */
+ 		if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&
+ 		    !boot_cpu_has(X86_FEATURE_AMD_SSBD))
+ 			entry->ebx |= F(VIRT_SSBD);
++>>>>>>> 6ac2f49edb1e (x86/bugs: Add AMD's SPEC_CTRL MSR usage)
  		break;
  	}
  	case 0x80000019:
diff --cc arch/x86/kvm/svm.c
index 6bd91543d358,950ec50f77c3..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -3653,10 -4114,39 +3653,18 @@@ static int svm_get_msr(struct kvm_vcpu 
  		msr_info->data = svm->nested.vm_cr_msr;
  		break;
  	case MSR_IA32_SPEC_CTRL:
++<<<<<<< HEAD
++=======
+ 		if (!msr_info->host_initiated &&
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBRS) &&
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_SSBD))
+ 			return 1;
+ 
++>>>>>>> 6ac2f49edb1e (x86/bugs: Add AMD's SPEC_CTRL MSR usage)
  		msr_info->data = svm->spec_ctrl;
  		break;
 -	case MSR_AMD64_VIRT_SPEC_CTRL:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))
 -			return 1;
 -
 -		msr_info->data = svm->virt_spec_ctrl;
 -		break;
 -	case MSR_F15H_IC_CFG: {
 -
 -		int family, model;
 -
 -		family = guest_cpuid_family(vcpu);
 -		model  = guest_cpuid_model(vcpu);
 -
 -		if (family < 0 || model < 0)
 -			return kvm_get_msr_common(vcpu, msr_info);
 -
 -		msr_info->data = 0;
 -
 -		if (family == 0x15 &&
 -		    (model >= 0x2 && model < 0x20))
 -			msr_info->data = 0x1E;
 -		}
 -		break;
 -	case MSR_F10H_DECFG:
 -		msr_info->data = svm->msr_decfg;
 +	case MSR_IA32_UCODE_REV:
 +		msr_info->data = 0x01000065;
  		break;
  	default:
  		return kvm_get_msr_common(vcpu, msr_info);
@@@ -3726,8 -4216,33 +3734,38 @@@ static int svm_set_msr(struct kvm_vcpu 
  		svm->vmcb->save.g_pat = data;
  		mark_dirty(svm->vmcb, VMCB_NPT);
  		break;
++<<<<<<< HEAD
 +	case MSR_IA32_TSC:
 +		kvm_write_tsc(vcpu, msr);
++=======
+ 	case MSR_IA32_SPEC_CTRL:
+ 		if (!msr->host_initiated &&
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBRS) &&
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_SSBD))
+ 			return 1;
+ 
+ 		/* The STIBP bit doesn't fault even if it's not advertised */
+ 		if (data & ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP | SPEC_CTRL_SSBD))
+ 			return 1;
+ 
+ 		svm->spec_ctrl = data;
+ 
+ 		if (!data)
+ 			break;
+ 
+ 		/*
+ 		 * For non-nested:
+ 		 * When it's written (to non-zero) for the first time, pass
+ 		 * it through.
+ 		 *
+ 		 * For nested:
+ 		 * The handling of the MSR bitmap for L2 guests is done in
+ 		 * nested_svm_vmrun_msrpm.
+ 		 * We update the L1 MSR bit as well since it will end up
+ 		 * touching the MSR anyway now.
+ 		 */
+ 		set_msr_interception(svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
++>>>>>>> 6ac2f49edb1e (x86/bugs: Add AMD's SPEC_CTRL MSR usage)
  		break;
  	case MSR_IA32_PRED_CMD:
  		if (!msr->host_initiated &&
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
* Unmerged path arch/x86/kvm/cpuid.c
* Unmerged path arch/x86/kvm/svm.c
