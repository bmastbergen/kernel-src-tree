md: disable WRITE SAME if it fails in underlayer disks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] disable WRITE SAME if it fails in underlayer disks (Nigel Croxon) [1494474]
Rebuild_FUZZ: 96.15%
commit-author Shaohua Li <shli@fb.com>
commit 26483819f89c5cf9d27620d70c95afeeeb9bece5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/26483819.failed

This makes md do the same thing as dm for write same IO failure. Please
see 7eee4ae(dm: disable WRITE SAME if it fails) for details why we need
this.

We did a little bit different than dm. Instead of disabling writesame in
the first IO error, we disable it till next writesame IO coming after
the first IO error. This way we don't need to clone a bio.

Also reported here: https://bugzilla.kernel.org/show_bug.cgi?id=118581

	Suggested-by: NeilBrown <neilb@suse.com>
	Acked-by: NeilBrown <neilb@suse.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 26483819f89c5cf9d27620d70c95afeeeb9bece5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/linear.c
#	drivers/md/raid0.c
diff --cc drivers/md/linear.c
index 2d3f4228a31e,789008bc94ff..000000000000
--- a/drivers/md/linear.c
+++ b/drivers/md/linear.c
@@@ -256,61 -244,67 +256,78 @@@ static void linear_free(struct mddev *m
  	kfree(conf);
  }
  
 -static void linear_make_request(struct mddev *mddev, struct bio *bio)
 +static bool linear_make_request(struct mddev *mddev, struct bio *bio)
  {
 -	char b[BDEVNAME_SIZE];
  	struct dev_info *tmp_dev;
 -	struct bio *split;
 -	sector_t start_sector, end_sector, data_offset;
 +	sector_t start_sector;
  
 -	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
  		md_flush_request(mddev, bio);
 -		return;
 +		return true;
  	}
  
 -	do {
 -		sector_t bio_sector = bio->bi_iter.bi_sector;
 -		tmp_dev = which_dev(mddev, bio_sector);
 -		start_sector = tmp_dev->end_sector - tmp_dev->rdev->sectors;
 -		end_sector = tmp_dev->end_sector;
 -		data_offset = tmp_dev->rdev->data_offset;
 -		bio->bi_bdev = tmp_dev->rdev->bdev;
 -
 -		if (unlikely(bio_sector >= end_sector ||
 -			     bio_sector < start_sector))
 -			goto out_of_bounds;
 -
 -		if (unlikely(bio_end_sector(bio) > end_sector)) {
 -			/* This bio crosses a device boundary, so we have to
 -			 * split it.
 -			 */
 -			split = bio_split(bio, end_sector - bio_sector,
 -					  GFP_NOIO, fs_bio_set);
 -			bio_chain(split, bio);
 -		} else {
 -			split = bio;
 -		}
 +	tmp_dev = which_dev(mddev, bio->bi_sector);
 +	start_sector = tmp_dev->end_sector - tmp_dev->rdev->sectors;
  
 -		split->bi_iter.bi_sector = split->bi_iter.bi_sector -
 -			start_sector + data_offset;
  
 +	if (unlikely(bio->bi_sector >= (tmp_dev->end_sector)
 +		     || (bio->bi_sector < start_sector))) {
 +		char b[BDEVNAME_SIZE];
 +
 +		pr_err("md/linear:%s: make_request: Sector %llu out of bounds on dev %s: %llu sectors, offset %llu\n",
 +		       mdname(mddev),
 +		       (unsigned long long)bio->bi_sector,
 +		       bdevname(tmp_dev->rdev->bdev, b),
 +		       (unsigned long long)tmp_dev->rdev->sectors,
 +		       (unsigned long long)start_sector);
 +		bio_io_error(bio);
 +		return true;
 +	}
 +	if (unlikely(bio_end_sector(bio) > tmp_dev->end_sector)) {
 +		/* This bio crosses a device boundary, so we have to
 +		 * split it.
 +		 */
 +		struct bio_pair *bp;
 +		sector_t end_sector = tmp_dev->end_sector;
 +
++<<<<<<< HEAD
 +		bp = bio_split(bio, end_sector - bio->bi_sector);
++=======
+ 		if (unlikely((bio_op(split) == REQ_OP_DISCARD) &&
+ 			 !blk_queue_discard(bdev_get_queue(split->bi_bdev)))) {
+ 			/* Just ignore it */
+ 			bio_endio(split);
+ 		} else {
+ 			if (mddev->gendisk)
+ 				trace_block_bio_remap(bdev_get_queue(split->bi_bdev),
+ 						      split, disk_devt(mddev->gendisk),
+ 						      bio_sector);
+ 			mddev_check_writesame(mddev, split);
+ 			generic_make_request(split);
+ 		}
+ 	} while (split != bio);
+ 	return;
++>>>>>>> 26483819f89c (md: disable WRITE SAME if it fails in underlayer disks)
 +
 +		linear_make_request(mddev, &bp->bio1);
 +		linear_make_request(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return true;
 +	}
 +		    
 +	bio->bi_bdev = tmp_dev->rdev->bdev;
 +	bio->bi_sector = bio->bi_sector - start_sector
 +		+ tmp_dev->rdev->data_offset;
 +
 +	if (unlikely((bio->bi_rw & REQ_DISCARD) &&
 +		     !blk_queue_discard(bdev_get_queue(bio->bi_bdev)))) {
 +		/* Just ignore it */
 +		bio_endio(bio, 0);
 +		return true;
 +	}
  
 -out_of_bounds:
 -	pr_err("md/linear:%s: make_request: Sector %llu out of bounds on dev %s: %llu sectors, offset %llu\n",
 -	       mdname(mddev),
 -	       (unsigned long long)bio->bi_iter.bi_sector,
 -	       bdevname(tmp_dev->rdev->bdev, b),
 -	       (unsigned long long)tmp_dev->rdev->sectors,
 -	       (unsigned long long)start_sector);
 -	bio_io_error(bio);
 +	generic_make_request(bio);
 +	return true;
  }
  
  static void linear_status (struct seq_file *seq, struct mddev *mddev)
diff --cc drivers/md/raid0.c
index 084297c45268,b3d264452fd5..000000000000
--- a/drivers/md/raid0.c
+++ b/drivers/md/raid0.c
@@@ -513,236 -457,56 +513,252 @@@ static inline int is_io_in_chunk_bounda
  	}
  }
  
 -static void raid0_make_request(struct mddev *mddev, struct bio *bio)
 +static struct bio *next_bio(struct bio *bio, int rw, unsigned int nr_pages,
 +			   gfp_t gfp)
  {
 -	struct strip_zone *zone;
 -	struct md_rdev *tmp_dev;
 -	struct bio *split;
 +	struct bio *new = bio_alloc(gfp, nr_pages);
  
 -	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
 -		md_flush_request(mddev, bio);
 -		return;
 +	if (bio) {
 +		bio_chain(bio, new);
 +		submit_bio(rw, bio);
  	}
  
 -	do {
 -		sector_t bio_sector = bio->bi_iter.bi_sector;
 -		sector_t sector = bio_sector;
 -		unsigned chunk_sects = mddev->chunk_sectors;
 +	return new;
 +}
 +
 +static int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 +				 sector_t nr_sects, gfp_t gfp_mask, int type,
 +				 struct bio **biop)
 +{
 +	struct request_queue *q = bdev_get_queue(bdev);
 +	struct bio *bio = *biop;
 +	unsigned int max_discard_sectors, granularity;
 +	int alignment;
 +
 +	if (!q)
 +		return -ENXIO;
 +	if (!blk_queue_discard(q))
 +		return -EOPNOTSUPP;
 +	if ((type & REQ_SECURE) && !blk_queue_secdiscard(q))
 +		return -EOPNOTSUPP;
 +
 +	/* Zero-sector (unknown) and one-sector granularities are the same.  */
 +	granularity = max(q->limits.discard_granularity >> 9, 1U);
 +	alignment = (bdev_discard_alignment(bdev) >> 9) % granularity;
  
 -		unsigned sectors = chunk_sects -
 -			(likely(is_power_of_2(chunk_sects))
 -			 ? (sector & (chunk_sects-1))
 -			 : sector_div(sector, chunk_sects));
 +	/*
 +	 * Ensure that max_discard_sectors is of the proper
 +	 * granularity, so that requests stay aligned after a split.
 +	 */
 +	max_discard_sectors = min(q->limits.max_discard_sectors, UINT_MAX >> 9);
 +	max_discard_sectors -= max_discard_sectors % granularity;
 +	if (unlikely(!max_discard_sectors)) {
 +		/* Avoid infinite loop below. Being cautious never hurts. */
 +		return -EOPNOTSUPP;
 +	}
  
 -		/* Restore due to sector_div */
 -		sector = bio_sector;
 +	while (nr_sects) {
 +		unsigned int req_sects;
 +		sector_t end_sect, tmp;
  
 -		if (sectors < bio_sectors(bio)) {
 -			split = bio_split(bio, sectors, GFP_NOIO, fs_bio_set);
 -			bio_chain(split, bio);
 -		} else {
 -			split = bio;
 +		req_sects = min_t(sector_t, nr_sects, max_discard_sectors);
 +
 +		/**
 +		 * If splitting a request, and the next starting sector would be
 +		 * misaligned, stop the discard at the previous aligned sector.
 +		 */
 +		end_sect = sector + req_sects;
 +		tmp = end_sect;
 +		if (req_sects < nr_sects &&
 +		    sector_div(tmp, granularity) != alignment) {
 +			end_sect = end_sect - alignment;
 +			sector_div(end_sect, granularity);
 +			end_sect = end_sect * granularity + alignment;
 +			req_sects = end_sect - sector;
  		}
  
 -		zone = find_zone(mddev->private, &sector);
 -		tmp_dev = map_sector(mddev, zone, sector, &sector);
 -		split->bi_bdev = tmp_dev->bdev;
 -		split->bi_iter.bi_sector = sector + zone->dev_start +
 -			tmp_dev->data_offset;
 +		bio = next_bio(bio, type, 1, gfp_mask);
 +		bio->bi_sector = sector;
 +		bio->bi_bdev = bdev;
 +
++<<<<<<< HEAD
 +		bio->bi_size = req_sects << 9;
 +		nr_sects -= req_sects;
 +		sector = end_sect;
 +
 +		/*
 +		 * We can loop for a long time in here, if someone does
 +		 * full device discards (like mkfs). Be nice and allow
 +		 * us to schedule out to avoid softlocking if preempt
 +		 * is disabled.
 +		 */
 +		cond_resched();
 +	}
 +
 +	*biop = bio;
 +	return 0;
 +}
 +
 +static void raid0_handle_discard(struct mddev *mddev, struct bio *bio)
 +{
 +	struct r0conf *conf = mddev->private;
 +	struct strip_zone *zone;
 +	sector_t start = bio->bi_sector;
 +	sector_t end;
 +	unsigned int stripe_size;
 +	sector_t first_stripe_index, last_stripe_index;
 +	sector_t start_disk_offset;
 +	unsigned int start_disk_index;
 +	sector_t end_disk_offset;
 +	unsigned int end_disk_index;
 +	unsigned int disk;
 +
 +	zone = find_zone(conf, &start);
 +
 +	if (bio_end_sector(bio) > zone->zone_end) {
 +		struct bio_pair *bp = bio_split(bio, zone->zone_end -
 +						     bio->bi_sector);
 +		raid0_handle_discard(mddev, &bp->bio1);
 +		raid0_handle_discard(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return;
 +	} else
 +		end = bio_end_sector(bio);
 +
 +	if (zone != conf->strip_zone)
 +		end = end - zone[-1].zone_end;
 +
 +	/* Now start and end is the offset in zone */
 +	stripe_size = zone->nb_dev * mddev->chunk_sectors;
 +
 +	first_stripe_index = start;
 +	sector_div(first_stripe_index, stripe_size);
 +	last_stripe_index = end;
 +	sector_div(last_stripe_index, stripe_size);
 +
 +	start_disk_index = (int)(start - first_stripe_index * stripe_size) /
 +		mddev->chunk_sectors;
 +	start_disk_offset = ((int)(start - first_stripe_index * stripe_size) %
 +		mddev->chunk_sectors) +
 +		first_stripe_index * mddev->chunk_sectors;
 +	end_disk_index = (int)(end - last_stripe_index * stripe_size) /
 +		mddev->chunk_sectors;
 +	end_disk_offset = ((int)(end - last_stripe_index * stripe_size) %
 +		mddev->chunk_sectors) +
 +		last_stripe_index * mddev->chunk_sectors;
 +
 +	for (disk = 0; disk < zone->nb_dev; disk++) {
 +		sector_t dev_start, dev_end;
 +		struct bio *discard_bio = NULL;
 +		struct md_rdev *rdev;
 +
 +		if (disk < start_disk_index)
 +			dev_start = (first_stripe_index + 1) *
 +				mddev->chunk_sectors;
 +		else if (disk > start_disk_index)
 +			dev_start = first_stripe_index * mddev->chunk_sectors;
 +		else
 +			dev_start = start_disk_offset;
 +
 +		if (disk < end_disk_index)
 +			dev_end = (last_stripe_index + 1) * mddev->chunk_sectors;
 +		else if (disk > end_disk_index)
 +			dev_end = last_stripe_index * mddev->chunk_sectors;
 +		else
 +			dev_end = end_disk_offset;
 +
 +		if (dev_end <= dev_start)
 +			continue;
 +
 +		rdev = conf->devlist[(zone - conf->strip_zone) *
 +			conf->strip_zone[0].nb_dev + disk];
 +
 +		if (__blkdev_issue_discard(rdev->bdev,
 +			dev_start + zone->dev_start + rdev->data_offset,
 +			dev_end - dev_start, GFP_NOIO, REQ_WRITE | REQ_DISCARD,
 +			&discard_bio) ||
 +		    !discard_bio)
 +			continue;
 +		bio_chain(discard_bio, bio);
 +		submit_bio(REQ_WRITE | REQ_DISCARD, discard_bio);
 +	}
 +	bio_endio(bio, 0);
 +}
 +
 +
 +static bool raid0_make_request(struct mddev *mddev, struct bio *bio)
 +{
 +	unsigned int chunk_sects;
 +	sector_t sector_offset;
 +	struct strip_zone *zone;
 +	struct md_rdev *tmp_dev;
 +
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
 +		md_flush_request(mddev, bio);
 +		return true;
 +	}
 +
 +	if (unlikely((bio_op(bio) == REQ_OP_DISCARD))) {
 +		raid0_handle_discard(mddev, bio);
 +		return true;
 +	}
 +
 +	chunk_sects = mddev->chunk_sectors;
 +	if (unlikely(!is_io_in_chunk_boundary(mddev, chunk_sects, bio))) {
 +		sector_t sector = bio->bi_sector;
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		if (likely(is_power_of_2(chunk_sects)))
 +			bp = bio_split(bio, chunk_sects - (sector &
 +							   (chunk_sects-1)));
 +		else
 +			bp = bio_split(bio, chunk_sects -
 +				       sector_div(sector, chunk_sects));
 +		raid0_make_request(mddev, &bp->bio1);
 +		raid0_make_request(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return true;
 +	}
  
 +	sector_offset = bio->bi_sector;
 +	zone = find_zone(mddev->private, &sector_offset);
 +	tmp_dev = map_sector(mddev, zone, bio->bi_sector,
 +			     &sector_offset);
 +	bio->bi_bdev = tmp_dev->bdev;
 +	bio->bi_sector = sector_offset + zone->dev_start +
 +		tmp_dev->data_offset;
 +
 +	generic_make_request(bio);
 +	return true;
 +
 +bad_map:
 +	printk("md/raid0:%s: make_request bug: can't convert block across chunks"
 +	       " or bigger than %dk %llu %d\n",
 +	       mdname(mddev), chunk_sects / 2,
 +	       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
 +
 +	bio_io_error(bio);
 +	return true;
++=======
+ 		if (unlikely((bio_op(split) == REQ_OP_DISCARD) &&
+ 			 !blk_queue_discard(bdev_get_queue(split->bi_bdev)))) {
+ 			/* Just ignore it */
+ 			bio_endio(split);
+ 		} else {
+ 			if (mddev->gendisk)
+ 				trace_block_bio_remap(bdev_get_queue(split->bi_bdev),
+ 						      split, disk_devt(mddev->gendisk),
+ 						      bio_sector);
+ 			mddev_check_writesame(mddev, split);
+ 			generic_make_request(split);
+ 		}
+ 	} while (split != bio);
++>>>>>>> 26483819f89c (md: disable WRITE SAME if it fails in underlayer disks)
  }
  
  static void raid0_status(struct seq_file *seq, struct mddev *mddev)
* Unmerged path drivers/md/linear.c
diff --git a/drivers/md/md.h b/drivers/md/md.h
index 629c80ea88dd..95efc748335a 100644
--- a/drivers/md/md.h
+++ b/drivers/md/md.h
@@ -723,4 +723,11 @@ static inline void mddev_clear_unsupported_flags(struct mddev *mddev,
 {
 	mddev->flags &= ~unsupported_flags;
 }
+
+static inline void mddev_check_writesame(struct mddev *mddev, struct bio *bio)
+{
+	if (bio_op(bio) == REQ_OP_WRITE_SAME &&
+	    !bdev_get_queue(bio->bi_bdev)->limits.max_write_same_sectors)
+		mddev->queue->limits.max_write_same_sectors = 0;
+}
 #endif /* _MD_MD_H */
diff --git a/drivers/md/multipath.c b/drivers/md/multipath.c
index 6bcfa138b496..367496b9933a 100644
--- a/drivers/md/multipath.c
+++ b/drivers/md/multipath.c
@@ -136,6 +136,7 @@ static bool multipath_make_request(struct mddev *mddev, struct bio * bio)
 	mp_bh->bio.bi_rw |= REQ_FAILFAST_TRANSPORT;
 	mp_bh->bio.bi_end_io = multipath_end_request;
 	mp_bh->bio.bi_private = mp_bh;
+	mddev_check_writesame(mddev, &mp_bh->bio);
 	generic_make_request(&mp_bh->bio);
 	return true;
 }
* Unmerged path drivers/md/raid0.c
