cpu/hotplug: Split do_cpu_down()

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit cc1fe215e1efa406b03aa4389e6269b61342dec5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/cc1fe215.failed

Split out the inner workings of do_cpu_down() to allow reuse of that
function for the upcoming SMT disabling mechanism.

No functional change.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Acked-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit cc1fe215e1efa406b03aa4389e6269b61342dec5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cpu.c
diff --cc kernel/cpu.c
index 0d9e250d0ea0,e266cb529c1a..000000000000
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@@ -432,32 -791,134 +432,157 @@@ static int __ref _cpu_down(unsigned in
  	/* This actually kills the CPU. */
  	__cpu_die(cpu);
  
 -	tick_cleanup_dead_cpu(cpu);
 -	rcutree_migrate_callbacks(cpu);
 -	return 0;
 +	/* CPU is completely dead: tell everyone.  Too late to complain. */
 +	cpu_notify_nofail(CPU_DEAD | mod, hcpu);
 +
 +	check_for_tasks(cpu);
 +
 +out_release:
 +	cpu_hotplug_done();
 +	if (!err)
 +		cpu_notify_nofail(CPU_POST_DEAD | mod, hcpu);
 +	return err;
  }
  
++<<<<<<< HEAD
 +int __ref cpu_down(unsigned int cpu)
++=======
+ static void cpuhp_complete_idle_dead(void *arg)
+ {
+ 	struct cpuhp_cpu_state *st = arg;
+ 
+ 	complete_ap_thread(st, false);
+ }
+ 
+ void cpuhp_report_idle_dead(void)
+ {
+ 	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
+ 
+ 	BUG_ON(st->state != CPUHP_AP_OFFLINE);
+ 	rcu_report_dead(smp_processor_id());
+ 	st->state = CPUHP_AP_IDLE_DEAD;
+ 	/*
+ 	 * We cannot call complete after rcu_report_dead() so we delegate it
+ 	 * to an online cpu.
+ 	 */
+ 	smp_call_function_single(cpumask_first(cpu_online_mask),
+ 				 cpuhp_complete_idle_dead, st, 0);
+ }
+ 
+ static void undo_cpu_down(unsigned int cpu, struct cpuhp_cpu_state *st)
+ {
+ 	for (st->state++; st->state < st->target; st->state++) {
+ 		struct cpuhp_step *step = cpuhp_get_step(st->state);
+ 
+ 		if (!step->skip_onerr)
+ 			cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
+ 	}
+ }
+ 
+ static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
+ 				enum cpuhp_state target)
+ {
+ 	enum cpuhp_state prev_state = st->state;
+ 	int ret = 0;
+ 
+ 	for (; st->state > target; st->state--) {
+ 		ret = cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);
+ 		if (ret) {
+ 			st->target = prev_state;
+ 			undo_cpu_down(cpu, st);
+ 			break;
+ 		}
+ 	}
+ 	return ret;
+ }
+ 
+ /* Requires cpu_add_remove_lock to be held */
+ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,
+ 			   enum cpuhp_state target)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 	int prev_state, ret = 0;
+ 
+ 	if (num_online_cpus() == 1)
+ 		return -EBUSY;
+ 
+ 	if (!cpu_present(cpu))
+ 		return -EINVAL;
+ 
+ 	cpus_write_lock();
+ 
+ 	cpuhp_tasks_frozen = tasks_frozen;
+ 
+ 	prev_state = cpuhp_set_state(st, target);
+ 	/*
+ 	 * If the current CPU state is in the range of the AP hotplug thread,
+ 	 * then we need to kick the thread.
+ 	 */
+ 	if (st->state > CPUHP_TEARDOWN_CPU) {
+ 		st->target = max((int)target, CPUHP_TEARDOWN_CPU);
+ 		ret = cpuhp_kick_ap_work(cpu);
+ 		/*
+ 		 * The AP side has done the error rollback already. Just
+ 		 * return the error code..
+ 		 */
+ 		if (ret)
+ 			goto out;
+ 
+ 		/*
+ 		 * We might have stopped still in the range of the AP hotplug
+ 		 * thread. Nothing to do anymore.
+ 		 */
+ 		if (st->state > CPUHP_TEARDOWN_CPU)
+ 			goto out;
+ 
+ 		st->target = target;
+ 	}
+ 	/*
+ 	 * The AP brought itself down to CPUHP_TEARDOWN_CPU. So we need
+ 	 * to do the further cleanups.
+ 	 */
+ 	ret = cpuhp_down_callbacks(cpu, st, target);
+ 	if (ret && st->state > CPUHP_TEARDOWN_CPU && st->state < prev_state) {
+ 		cpuhp_reset_state(st, prev_state);
+ 		__cpuhp_kick_ap(st);
+ 	}
+ 
+ out:
+ 	cpus_write_unlock();
+ 	/*
+ 	 * Do post unplug cleanup. This is still protected against
+ 	 * concurrent CPU hotplug via cpu_add_remove_lock.
+ 	 */
+ 	lockup_detector_cleanup();
+ 	return ret;
+ }
+ 
+ static int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)
+ {
+ 	if (cpu_hotplug_disabled)
+ 		return -EBUSY;
+ 	return _cpu_down(cpu, 0, target);
+ }
+ 
+ static int do_cpu_down(unsigned int cpu, enum cpuhp_state target)
++>>>>>>> cc1fe215e1ef (cpu/hotplug: Split do_cpu_down())
  {
  	int err;
  
  	cpu_maps_update_begin();
++<<<<<<< HEAD
 +
 +	if (cpu_hotplug_disabled) {
 +		err = -EBUSY;
 +		goto out;
 +	}
 +
 +	err = _cpu_down(cpu, 0);
 +
 +out:
++=======
+ 	err = cpu_down_maps_locked(cpu, target);
++>>>>>>> cc1fe215e1ef (cpu/hotplug: Split do_cpu_down())
  	cpu_maps_update_done();
  	return err;
  }
* Unmerged path kernel/cpu.c
