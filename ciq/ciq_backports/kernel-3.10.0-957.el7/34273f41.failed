x86, espfix: Make it possible to disable 16-bit support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] espfix: Make it possible to disable 16-bit support (Gopal Tiwari) [1456572]
Rebuild_FUZZ: 95.24%
commit-author H. Peter Anvin <hpa@zytor.com>
commit 34273f41d57ee8d854dcd2a1d754cbb546cb548f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/34273f41.failed

Embedded systems, which may be very memory-size-sensitive, are
extremely unlikely to ever encounter any 16-bit software, so make it
a CONFIG_EXPERT option to turn off support for any 16-bit software
whatsoever.

	Signed-off-by: H. Peter Anvin <hpa@zytor.com>
Link: http://lkml.kernel.org/r/1398816946-3351-1-git-send-email-hpa@linux.intel.com
(cherry picked from commit 34273f41d57ee8d854dcd2a1d754cbb546cb548f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/kernel/entry_64.S
diff --cc arch/x86/Kconfig
index 48ae09959d87,956c7702471e..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -1092,11 -909,28 +1092,31 @@@ config VM8
  	default y
  	depends on X86_32
  	---help---
- 	  This option is required by programs like DOSEMU to run 16-bit legacy
- 	  code on X86 processors. It also may be needed by software like
- 	  XFree86 to initialize some video cards via BIOS. Disabling this
- 	  option saves about 6k.
+ 	  This option is required by programs like DOSEMU to run
+ 	  16-bit real mode legacy code on x86 processors. It also may
+ 	  be needed by software like XFree86 to initialize some video
+ 	  cards via BIOS. Disabling this option saves about 6K.
  
+ config X86_16BIT
+ 	bool "Enable support for 16-bit segments" if EXPERT
+ 	default y
+ 	---help---
+ 	  This option is required by programs like Wine to run 16-bit
+ 	  protected mode legacy code on x86 processors.  Disabling
+ 	  this option saves about 300 bytes on i386, or around 6K text
+ 	  plus 16K runtime memory on x86-64,
+ 
+ config X86_ESPFIX32
+ 	def_bool y
+ 	depends on X86_16BIT && X86_32
+ 
++<<<<<<< HEAD
++=======
+ config X86_ESPFIX64
+ 	def_bool y
+ 	depends on X86_16BIT && X86_64
+ 
++>>>>>>> 34273f41d57e (x86, espfix: Make it possible to disable 16-bit support)
  config TOSHIBA
  	tristate "Toshiba Laptop support"
  	depends on X86_32
diff --cc arch/x86/kernel/entry_64.S
index 5efac8c584c2,da0b9bdcc32e..000000000000
--- a/arch/x86/kernel/entry_64.S
+++ b/arch/x86/kernel/entry_64.S
@@@ -932,8 -1040,19 +932,21 @@@ retint_restore_args:	/* return to kerne
  	RESTORE_ARGS 1,8,1
  
  irq_return:
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Are we returning to a stack segment from the LDT?  Note: in
+ 	 * 64-bit mode SS:RSP on the exception stack is always valid.
+ 	 */
+ #ifdef CONFIG_X86_ESPFIX64
+ 	testb $4,(SS-RIP)(%rsp)
+ 	jnz irq_return_ldt
+ #endif
+ 
+ irq_return_iret:
++>>>>>>> 34273f41d57e (x86, espfix: Make it possible to disable 16-bit support)
  	INTERRUPT_RETURN
 -	_ASM_EXTABLE(irq_return_iret, bad_iret)
 +	_ASM_EXTABLE(irq_return, bad_iret)
  
  #ifdef CONFIG_PARAVIRT
  ENTRY(native_iret)
@@@ -941,6 -1060,32 +954,35 @@@
  	_ASM_EXTABLE(native_iret, bad_iret)
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_ESPFIX64
+ irq_return_ldt:
+ 	pushq_cfi %rax
+ 	pushq_cfi %rdi
+ 	SWAPGS
+ 	movq PER_CPU_VAR(espfix_waddr),%rdi
+ 	movq %rax,(0*8)(%rdi)	/* RAX */
+ 	movq (2*8)(%rsp),%rax	/* RIP */
+ 	movq %rax,(1*8)(%rdi)
+ 	movq (3*8)(%rsp),%rax	/* CS */
+ 	movq %rax,(2*8)(%rdi)
+ 	movq (4*8)(%rsp),%rax	/* RFLAGS */
+ 	movq %rax,(3*8)(%rdi)
+ 	movq (6*8)(%rsp),%rax	/* SS */
+ 	movq %rax,(5*8)(%rdi)
+ 	movq (5*8)(%rsp),%rax	/* RSP */
+ 	movq %rax,(4*8)(%rdi)
+ 	andl $0xffff0000,%eax
+ 	popq_cfi %rdi
+ 	orq PER_CPU_VAR(espfix_stack),%rax
+ 	SWAPGS
+ 	movq %rax,%rsp
+ 	popq_cfi %rax
+ 	jmp irq_return_iret
+ #endif
+ 
++>>>>>>> 34273f41d57e (x86, espfix: Make it possible to disable 16-bit support)
  	.section .fixup,"ax"
  bad_iret:
  	/*
@@@ -1013,9 -1147,45 +1055,49 @@@ ENTRY(retint_kernel
  	call preempt_schedule_irq
  	jmp exit_intr
  #endif
 +
  	CFI_ENDPROC
  END(common_interrupt)
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * If IRET takes a fault on the espfix stack, then we
+ 	 * end up promoting it to a doublefault.  In that case,
+ 	 * modify the stack to make it look like we just entered
+ 	 * the #GP handler from user space, similar to bad_iret.
+ 	 */
+ #ifdef CONFIG_X86_ESPFIX64
+ 	ALIGN
+ __do_double_fault:
+ 	XCPT_FRAME 1 RDI+8
+ 	movq RSP(%rdi),%rax		/* Trap on the espfix stack? */
+ 	sarq $PGDIR_SHIFT,%rax
+ 	cmpl $ESPFIX_PGD_ENTRY,%eax
+ 	jne do_double_fault		/* No, just deliver the fault */
+ 	cmpl $__KERNEL_CS,CS(%rdi)
+ 	jne do_double_fault
+ 	movq RIP(%rdi),%rax
+ 	cmpq $irq_return_iret,%rax
+ #ifdef CONFIG_PARAVIRT
+ 	je 1f
+ 	cmpq $native_iret,%rax
+ #endif
+ 	jne do_double_fault		/* This shouldn't happen... */
+ 1:
+ 	movq PER_CPU_VAR(kernel_stack),%rax
+ 	subq $(6*8-KERNEL_STACK_OFFSET),%rax	/* Reset to original stack */
+ 	movq %rax,RSP(%rdi)
+ 	movq $0,(%rax)			/* Missing (lost) #GP error code */
+ 	movq $general_protection,RIP(%rdi)
+ 	retq
+ 	CFI_ENDPROC
+ END(__do_double_fault)
+ #else
+ # define __do_double_fault do_double_fault
+ #endif
+ 
++>>>>>>> 34273f41d57e (x86, espfix: Make it possible to disable 16-bit support)
  /*
   * End of kprobes section
   */
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/kernel/entry_32.S b/arch/x86/kernel/entry_32.S
index e497ebbd56a8..b9ec4eb081a7 100644
--- a/arch/x86/kernel/entry_32.S
+++ b/arch/x86/kernel/entry_32.S
@@ -529,6 +529,7 @@ syscall_exit:
 restore_all:
 	TRACE_IRQS_IRET
 restore_all_notrace:
+#ifdef CONFIG_X86_ESPFIX32
 	movl PT_EFLAGS(%esp), %eax	# mix EFLAGS, SS and CS
 	# Warning: PT_OLDSS(%esp) contains the wrong/random values if we
 	# are returning to the kernel.
@@ -539,6 +540,7 @@ restore_all_notrace:
 	cmpl $((SEGMENT_LDT << 8) | USER_RPL), %eax
 	CFI_REMEMBER_STATE
 	je ldt_ss			# returning to user-space with LDT SS
+#endif
 restore_nocheck:
 	RESTORE_REGS 4			# skip orig_eax/error_code
 irq_return:
@@ -551,6 +553,7 @@ ENTRY(iret_exc)
 .previous
 	_ASM_EXTABLE(irq_return,iret_exc)
 
+#ifdef CONFIG_X86_ESPFIX32
 	CFI_RESTORE_STATE
 ldt_ss:
 	larl PT_OLDSS(%esp), %eax
@@ -599,6 +602,7 @@ ldt_ss:
 	lss (%esp), %esp		/* switch to espfix segment */
 	CFI_ADJUST_CFA_OFFSET -8
 	jmp restore_nocheck
+#endif
 	CFI_ENDPROC
 ENDPROC(system_call)
 
@@ -706,6 +710,7 @@ END(syscall_badsys)
  * the high word of the segment base from the GDT and swiches to the
  * normal stack and adjusts ESP with the matching offset.
  */
+#ifdef CONFIG_X86_ESPFIX32
 	/* fixup the stack */
 	mov GDT_ESPFIX_SS + 4, %al /* bits 16..23 */
 	mov GDT_ESPFIX_SS + 7, %ah /* bits 24..31 */
@@ -715,8 +720,10 @@ END(syscall_badsys)
 	pushl_cfi %eax
 	lss (%esp), %esp		/* switch to the normal stack segment */
 	CFI_ADJUST_CFA_OFFSET -8
+#endif
 .endm
 .macro UNWIND_ESPFIX_STACK
+#ifdef CONFIG_X86_ESPFIX32
 	movl %ss, %eax
 	/* see if on espfix stack */
 	cmpw $__ESPFIX_SS, %ax
@@ -727,6 +734,7 @@ END(syscall_badsys)
 	/* switch to normal stack */
 	FIXUP_ESPFIX_STACK
 27:
+#endif
 .endm
 
 /*
@@ -1345,11 +1353,13 @@ END(debug)
 ENTRY(nmi)
 	RING0_INT_FRAME
 	ASM_CLAC
+#ifdef CONFIG_X86_ESPFIX32
 	pushl_cfi %eax
 	movl %ss, %eax
 	cmpw $__ESPFIX_SS, %ax
 	popl_cfi %eax
 	je nmi_espfix_stack
+#endif
 	cmpl $ia32_sysenter_target,(%esp)
 	je nmi_stack_fixup
 	pushl_cfi %eax
@@ -1389,6 +1399,7 @@ nmi_debug_stack_check:
 	FIX_STACK 24, nmi_stack_correct, 1
 	jmp nmi_stack_correct
 
+#ifdef CONFIG_X86_ESPFIX32
 nmi_espfix_stack:
 	/* We have a RING0_INT_FRAME here.
 	 *
@@ -1410,6 +1421,7 @@ nmi_espfix_stack:
 	lss 12+4(%esp), %esp		# back to espfix stack
 	CFI_ADJUST_CFA_OFFSET -24
 	jmp irq_return
+#endif
 	CFI_ENDPROC
 END(nmi)
 
* Unmerged path arch/x86/kernel/entry_64.S
diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c
index 6b6d22561e6f..7c4c7ef281d1 100644
--- a/arch/x86/kernel/ldt.c
+++ b/arch/x86/kernel/ldt.c
@@ -247,6 +247,11 @@ static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)
 		}
 	}
 
+	if (!IS_ENABLED(CONFIG_X86_16BIT) && !ldt_info.seg_32bit) {
+		error = -EINVAL;
+		goto out_unlock;
+	}
+
 	fill_ldt(&ldt, &ldt_info);
 	if (oldmode)
 		ldt.avl = 0;
