iio: __iio_update_buffers: Split enable and disable path into helper functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iio] __iio_update_buffers: Split enable and disable path into helper functions (Tony Camuso) [1559170]
Rebuild_FUZZ: 96.69%
commit-author Lars-Peter Clausen <lars@metafoo.de>
commit 623d74e37f12c9276b15c2c0540b438e684af0d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/623d74e3.failed

__iio_update_buffers is already a rather large function with many different
error paths and it is going to get even larger. This patch factors out the
device enable and device disable paths into separate helper functions.

The patch also re-implements iio_disable_all_buffers() using the new
iio_disable_buffers() function removing a fair bit of redundant code.

	Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
	Signed-off-by: Jonathan Cameron <jic23@kernel.org>
(cherry picked from commit 623d74e37f12c9276b15c2c0540b438e684af0d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iio/industrialio-buffer.c
diff --cc drivers/iio/industrialio-buffer.c
index ed6b8aa675ce,b4d7dba163cf..000000000000
--- a/drivers/iio/industrialio-buffer.c
+++ b/drivers/iio/industrialio-buffer.c
@@@ -386,11 -441,516 +386,458 @@@ error_ret
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t iio_buffer_read_length(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 
+ 	return sprintf(buf, "%d\n", buffer->length);
+ }
+ 
+ static ssize_t iio_buffer_write_length(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf, size_t len)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 	unsigned int val;
+ 	int ret;
+ 
+ 	ret = kstrtouint(buf, 10, &val);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (val == buffer->length)
+ 		return len;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 	if (iio_buffer_is_active(indio_dev->buffer)) {
+ 		ret = -EBUSY;
+ 	} else {
+ 		buffer->access->set_length(buffer, val);
+ 		ret = 0;
+ 	}
+ 	if (ret)
+ 		goto out;
+ 	if (buffer->length && buffer->length < buffer->watermark)
+ 		buffer->watermark = buffer->length;
+ out:
+ 	mutex_unlock(&indio_dev->mlock);
+ 
+ 	return ret ? ret : len;
+ }
+ 
+ static ssize_t iio_buffer_show_enable(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	return sprintf(buf, "%d\n", iio_buffer_is_active(indio_dev->buffer));
+ }
+ 
+ static int iio_compute_scan_bytes(struct iio_dev *indio_dev,
+ 				const unsigned long *mask, bool timestamp)
+ {
+ 	const struct iio_chan_spec *ch;
+ 	unsigned bytes = 0;
+ 	int length, i;
+ 
+ 	/* How much space will the demuxed element take? */
+ 	for_each_set_bit(i, mask,
+ 			 indio_dev->masklength) {
+ 		ch = iio_find_channel_from_si(indio_dev, i);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	if (timestamp) {
+ 		ch = iio_find_channel_from_si(indio_dev,
+ 					      indio_dev->scan_index_timestamp);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	return bytes;
+ }
+ 
+ static void iio_buffer_activate(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	iio_buffer_get(buffer);
+ 	list_add(&buffer->buffer_list, &indio_dev->buffer_list);
+ }
+ 
+ static void iio_buffer_deactivate(struct iio_buffer *buffer)
+ {
+ 	list_del_init(&buffer->buffer_list);
+ 	wake_up_interruptible(&buffer->pollq);
+ 	iio_buffer_put(buffer);
+ }
+ 
+ static void iio_buffer_update_bytes_per_datum(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	unsigned int bytes;
+ 
+ 	if (!buffer->access->set_bytes_per_datum)
+ 		return;
+ 
+ 	bytes = iio_compute_scan_bytes(indio_dev, buffer->scan_mask,
+ 		buffer->scan_timestamp);
+ 
+ 	buffer->access->set_bytes_per_datum(buffer, bytes);
+ }
+ 
+ static int iio_buffer_request_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	int ret;
+ 
+ 	iio_buffer_update_bytes_per_datum(indio_dev, buffer);
+ 	if (buffer->access->request_update) {
+ 		ret = buffer->access->request_update(buffer);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer parameter update failed (%d)\n",
+ 				ret);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void iio_free_scan_mask(struct iio_dev *indio_dev,
+ 	const unsigned long *mask)
+ {
+ 	/* If the mask is dynamically allocated free it, otherwise do nothing */
+ 	if (!indio_dev->available_scan_masks)
+ 		kfree(mask);
+ }
+ 
+ struct iio_device_config {
+ 	unsigned int mode;
+ 	const unsigned long *scan_mask;
+ 	unsigned int scan_bytes;
+ 	bool scan_timestamp;
+ };
+ 
+ static int iio_verify_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *insert_buffer, struct iio_buffer *remove_buffer,
+ 	struct iio_device_config *config)
+ {
+ 	unsigned long *compound_mask;
+ 	const unsigned long *scan_mask;
+ 	struct iio_buffer *buffer;
+ 	bool scan_timestamp;
+ 
+ 	memset(config, 0, sizeof(*config));
+ 
+ 	/*
+ 	 * If there is just one buffer and we are removing it there is nothing
+ 	 * to verify.
+ 	 */
+ 	if (remove_buffer && !insert_buffer &&
+ 		list_is_singular(&indio_dev->buffer_list))
+ 			return 0;
+ 
+ 	/* Definitely possible for devices to support both of these. */
+ 	if ((indio_dev->modes & INDIO_BUFFER_TRIGGERED) && indio_dev->trig) {
+ 		config->mode = INDIO_BUFFER_TRIGGERED;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_HARDWARE) {
+ 		config->mode = INDIO_BUFFER_HARDWARE;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_SOFTWARE) {
+ 		config->mode = INDIO_BUFFER_SOFTWARE;
+ 	} else {
+ 		/* Can only occur on first buffer */
+ 		if (indio_dev->modes & INDIO_BUFFER_TRIGGERED)
+ 			dev_dbg(&indio_dev->dev, "Buffer not started: no trigger\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* What scan mask do we actually have? */
+ 	compound_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),
+ 				sizeof(long), GFP_KERNEL);
+ 	if (compound_mask == NULL)
+ 		return -ENOMEM;
+ 
+ 	scan_timestamp = false;
+ 
+ 	list_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {
+ 		if (buffer == remove_buffer)
+ 			continue;
+ 		bitmap_or(compound_mask, compound_mask, buffer->scan_mask,
+ 			  indio_dev->masklength);
+ 		scan_timestamp |= buffer->scan_timestamp;
+ 	}
+ 
+ 	if (insert_buffer) {
+ 		bitmap_or(compound_mask, compound_mask,
+ 			  insert_buffer->scan_mask, indio_dev->masklength);
+ 		scan_timestamp |= insert_buffer->scan_timestamp;
+ 	}
+ 
+ 	if (indio_dev->available_scan_masks) {
+ 		scan_mask = iio_scan_mask_match(indio_dev->available_scan_masks,
+ 				    indio_dev->masklength,
+ 				    compound_mask);
+ 		kfree(compound_mask);
+ 		if (scan_mask == NULL)
+ 			return -EINVAL;
+ 	} else {
+ 	    scan_mask = compound_mask;
+ 	}
+ 
+ 	config->scan_bytes = iio_compute_scan_bytes(indio_dev,
+ 				    scan_mask, scan_timestamp);
+ 	config->scan_mask = scan_mask;
+ 	config->scan_timestamp = scan_timestamp;
+ 
+ 	return 0;
+ }
+ 
+ static int iio_enable_buffers(struct iio_dev *indio_dev,
+ 	struct iio_device_config *config)
+ {
+ 	int ret;
+ 
+ 	indio_dev->active_scan_mask = config->scan_mask;
+ 	indio_dev->scan_timestamp = config->scan_timestamp;
+ 	indio_dev->scan_bytes = config->scan_bytes;
+ 
+ 	iio_update_demux(indio_dev);
+ 
+ 	/* Wind up again */
+ 	if (indio_dev->setup_ops->preenable) {
+ 		ret = indio_dev->setup_ops->preenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer preenable failed (%d)\n", ret);
+ 			goto err_undo_config;
+ 		}
+ 	}
+ 
+ 	if (indio_dev->info->update_scan_mode) {
+ 		ret = indio_dev->info
+ 			->update_scan_mode(indio_dev,
+ 					   indio_dev->active_scan_mask);
+ 		if (ret < 0) {
+ 			dev_dbg(&indio_dev->dev,
+ 				"Buffer not started: update scan mode failed (%d)\n",
+ 				ret);
+ 			goto err_run_postdisable;
+ 		}
+ 	}
+ 
+ 	indio_dev->currentmode = config->mode;
+ 
+ 	if (indio_dev->setup_ops->postenable) {
+ 		ret = indio_dev->setup_ops->postenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: postenable failed (%d)\n", ret);
+ 			goto err_run_postdisable;
+ 		}
+ 	}
+ 
+ 	return 0;
+ 
+ err_run_postdisable:
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 	if (indio_dev->setup_ops->postdisable)
+ 		indio_dev->setup_ops->postdisable(indio_dev);
+ err_undo_config:
+ 	indio_dev->active_scan_mask = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static int iio_disable_buffers(struct iio_dev *indio_dev)
+ {
+ 	int ret;
+ 
+ 	/* Wind down existing buffers - iff there are any */
+ 	if (list_empty(&indio_dev->buffer_list))
+ 		return 0;
+ 
+ 	if (indio_dev->setup_ops->predisable) {
+ 		ret = indio_dev->setup_ops->predisable(indio_dev);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 
+ 	if (indio_dev->setup_ops->postdisable) {
+ 		ret = indio_dev->setup_ops->postdisable(indio_dev);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int __iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 	const unsigned long *old_mask;
+ 	struct iio_device_config new_config;
+ 
+ 	ret = iio_verify_update(indio_dev, insert_buffer, remove_buffer,
+ 		&new_config);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (insert_buffer) {
+ 		ret = iio_buffer_request_update(indio_dev, insert_buffer);
+ 		if (ret)
+ 			goto err_free_config;
+ 	}
+ 
+ 	/* Keep a copy of current setup to allow roll back */
+ 	old_mask = indio_dev->active_scan_mask;
+ 	indio_dev->active_scan_mask = NULL;
+ 
+ 	ret = iio_disable_buffers(indio_dev);
+ 	if (ret) {
+ 		iio_free_scan_mask(indio_dev, old_mask);
+ 		goto err_free_config;
+ 	}
+ 
+ 	if (remove_buffer)
+ 		iio_buffer_deactivate(remove_buffer);
+ 	if (insert_buffer)
+ 		iio_buffer_activate(indio_dev, insert_buffer);
+ 
+ 	/* If no buffers in list, we are done */
+ 	if (list_empty(&indio_dev->buffer_list)) {
+ 		iio_free_scan_mask(indio_dev, old_mask);
+ 		return 0;
+ 	}
+ 
+ 	ret = iio_enable_buffers(indio_dev, &new_config);
+ 	if (ret) {
+ 		if (insert_buffer)
+ 			iio_buffer_deactivate(insert_buffer);
+ 		indio_dev->active_scan_mask = old_mask;
+ 		goto err_free_config;
+ 	}
+ 
+ 	iio_free_scan_mask(indio_dev, old_mask);
+ 	return 0;
+ 
+ err_free_config:
+ 	iio_free_scan_mask(indio_dev, new_config.scan_mask);
+ 	return ret;
+ }
+ 
+ int iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 
+ 	if (insert_buffer == remove_buffer)
+ 		return 0;
+ 
+ 	mutex_lock(&indio_dev->info_exist_lock);
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	if (insert_buffer && iio_buffer_is_active(insert_buffer))
+ 		insert_buffer = NULL;
+ 
+ 	if (remove_buffer && !iio_buffer_is_active(remove_buffer))
+ 		remove_buffer = NULL;
+ 
+ 	if (!insert_buffer && !remove_buffer) {
+ 		ret = 0;
+ 		goto out_unlock;
+ 	}
+ 
+ 	if (indio_dev->info == NULL) {
+ 		ret = -ENODEV;
+ 		goto out_unlock;
+ 	}
+ 
+ 	ret = __iio_update_buffers(indio_dev, insert_buffer, remove_buffer);
+ 
+ out_unlock:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	mutex_unlock(&indio_dev->info_exist_lock);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(iio_update_buffers);
+ 
+ void iio_disable_all_buffers(struct iio_dev *indio_dev)
+ {
+ 	struct iio_buffer *buffer, *_buffer;
+ 
+ 	iio_disable_buffers(indio_dev);
+ 	iio_free_scan_mask(indio_dev, indio_dev->active_scan_mask);
+ 	indio_dev->active_scan_mask = NULL;
+ 
+ 	list_for_each_entry_safe(buffer, _buffer,
+ 			&indio_dev->buffer_list, buffer_list)
+ 		iio_buffer_deactivate(buffer);
+ }
+ 
+ static ssize_t iio_buffer_store_enable(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf,
+ 				       size_t len)
+ {
+ 	int ret;
+ 	bool requested_state;
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	bool inlist;
+ 
+ 	ret = strtobool(buf, &requested_state);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	/* Find out if it is in the list */
+ 	inlist = iio_buffer_is_active(indio_dev->buffer);
+ 	/* Already in desired state */
+ 	if (inlist == requested_state)
+ 		goto done;
+ 
+ 	if (requested_state)
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 indio_dev->buffer, NULL);
+ 	else
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 NULL, indio_dev->buffer);
+ 
+ 	if (ret < 0)
+ 		goto done;
+ done:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	return (ret < 0) ? ret : len;
+ }
+ 
++>>>>>>> 623d74e37f12 (iio: __iio_update_buffers: Split enable and disable path into helper functions)
  static const char * const iio_scan_elements_group_name = "scan_elements";
  
 -static ssize_t iio_buffer_show_watermark(struct device *dev,
 -					 struct device_attribute *attr,
 -					 char *buf)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -
 -	return sprintf(buf, "%u\n", buffer->watermark);
 -}
 -
 -static ssize_t iio_buffer_store_watermark(struct device *dev,
 -					  struct device_attribute *attr,
 -					  const char *buf,
 -					  size_t len)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -	unsigned int val;
 -	int ret;
 -
 -	ret = kstrtouint(buf, 10, &val);
 -	if (ret)
 -		return ret;
 -	if (!val)
 -		return -EINVAL;
 -
 -	mutex_lock(&indio_dev->mlock);
 -
 -	if (val > buffer->length) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	if (iio_buffer_is_active(indio_dev->buffer)) {
 -		ret = -EBUSY;
 -		goto out;
 -	}
 -
 -	buffer->watermark = val;
 -
 -	if (indio_dev->info->hwfifo_set_watermark)
 -		indio_dev->info->hwfifo_set_watermark(indio_dev, val);
 -out:
 -	mutex_unlock(&indio_dev->mlock);
 -
 -	return ret ? ret : len;
 -}
 -
 -static DEVICE_ATTR(length, S_IRUGO | S_IWUSR, iio_buffer_read_length,
 -		   iio_buffer_write_length);
 -static struct device_attribute dev_attr_length_ro = __ATTR(length,
 -	S_IRUGO, iio_buffer_read_length, NULL);
 -static DEVICE_ATTR(enable, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_enable, iio_buffer_store_enable);
 -static DEVICE_ATTR(watermark, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_watermark, iio_buffer_store_watermark);
 -
 -static struct attribute *iio_buffer_attrs[] = {
 -	&dev_attr_length.attr,
 -	&dev_attr_enable.attr,
 -	&dev_attr_watermark.attr,
 -};
 -
 -int iio_buffer_alloc_sysfs_and_mask(struct iio_dev *indio_dev)
 +int iio_buffer_register(struct iio_dev *indio_dev,
 +			const struct iio_chan_spec *channels,
 +			int num_channels)
  {
  	struct iio_dev_attr *p;
  	struct attribute **attr;
* Unmerged path drivers/iio/industrialio-buffer.c
