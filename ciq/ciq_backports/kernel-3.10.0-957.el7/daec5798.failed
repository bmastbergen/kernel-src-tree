sched/deadline: Reclaim bandwidth not used by dl tasks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Luca Abeni <luca.abeni@santannapisa.it>
commit daec5798367012951cdb54fdb5c006e4379c9ae9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/daec5798.failed

This commit introduces a per-runqueue "extra utilization" that can be
reclaimed by deadline tasks. In this way, the maximum fraction of CPU
time that can reclaimed by deadline tasks is fixed (and configurable)
and does not depend on the total deadline utilization.
The GRUB accounting rule is modified to add this "extra utilization"
to the inactive utilization of the runqueue, and to avoid reclaiming
more than a maximum fraction of the CPU time.

	Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
	Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Claudio Scordino <claudio@evidence.eu.com>
	Cc: Joel Fernandes <joelaf@google.com>
	Cc: Juri Lelli <juri.lelli@arm.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
Link: http://lkml.kernel.org/r/1495138417-6203-10-git-send-email-luca.abeni@santannapisa.it
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit daec5798367012951cdb54fdb5c006e4379c9ae9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/deadline.c
#	kernel/sched/sched.h
diff --cc kernel/sched/core.c
index 9015c1e86d89,799647927c4c..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -2223,15 -2499,29 +2223,34 @@@ static int dl_overflow(struct task_stru
  	cpus = dl_bw_cpus(task_cpu(p));
  	if (dl_policy(policy) && !task_has_dl_policy(p) &&
  	    !__dl_overflow(dl_b, cpus, 0, new_bw)) {
++<<<<<<< HEAD
 +		__dl_add(dl_b, new_bw);
 +		err = 0;
 +	} else if (dl_policy(policy) && task_has_dl_policy(p) &&
 +		   !__dl_overflow(dl_b, cpus, p->dl.dl_bw, new_bw)) {
 +		__dl_clear(dl_b, p->dl.dl_bw);
 +		__dl_add(dl_b, new_bw);
++=======
+ 		if (hrtimer_active(&p->dl.inactive_timer))
+ 			__dl_clear(dl_b, p->dl.dl_bw, cpus);
+ 		__dl_add(dl_b, new_bw, cpus);
+ 		err = 0;
+ 	} else if (dl_policy(policy) && task_has_dl_policy(p) &&
+ 		   !__dl_overflow(dl_b, cpus, p->dl.dl_bw, new_bw)) {
+ 		/*
+ 		 * XXX this is slightly incorrect: when the task
+ 		 * utilization decreases, we should delay the total
+ 		 * utilization change until the task's 0-lag point.
+ 		 * But this would require to set the task's "inactive
+ 		 * timer" when the task is not inactive.
+ 		 */
+ 		__dl_clear(dl_b, p->dl.dl_bw, cpus);
+ 		__dl_add(dl_b, new_bw, cpus);
+ 		dl_change_utilization(p, new_bw);
++>>>>>>> daec57983670 (sched/deadline: Reclaim bandwidth not used by dl tasks)
  		err = 0;
  	} else if (!dl_policy(policy) && task_has_dl_policy(p)) {
 -		/*
 -		 * Do not decrease the total deadline utilization here,
 -		 * switched_from_dl() will take care to do it at the correct
 -		 * (0-lag) time.
 -		 */
 +		__dl_clear(dl_b, p->dl.dl_bw);
  		err = 0;
  	}
  	raw_spin_unlock(&dl_b->lock);
@@@ -3435,2433 -3716,300 +3454,2433 @@@ pick_next_task(struct rq *rq
  }
  
  /*
 - * rt_mutex_setprio - set the current priority of a task
 - * @p: task to boost
 - * @pi_task: donor task
 + * __schedule() is the main scheduler function.
   *
 - * This function changes the 'effective' priority of a task. It does
 - * not touch ->normal_prio like __setscheduler().
 + * The main means of driving the scheduler and thus entering this function are:
 + *
 + *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
 + *
 + *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
 + *      paths. For example, see arch/x86/entry_64.S.
 + *
 + *      To drive preemption between tasks, the scheduler sets the flag in timer
 + *      interrupt handler scheduler_tick().
 + *
 + *   3. Wakeups don't really cause entry into schedule(). They add a
 + *      task to the run-queue and that's it.
 + *
 + *      Now, if the new task added to the run-queue preempts the current
 + *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 + *      called on the nearest possible occasion:
 + *
 + *       - If the kernel is preemptible (CONFIG_PREEMPT=y):
 + *
 + *         - in syscall or exception context, at the next outmost
 + *           preempt_enable(). (this might be as soon as the wake_up()'s
 + *           spin_unlock()!)
 + *
 + *         - in IRQ context, return from interrupt-handler to
 + *           preemptible context
 + *
 + *       - If the kernel is not preemptible (CONFIG_PREEMPT is not set)
 + *         then at the next:
   *
 - * Used by the rt_mutex code to implement priority inheritance
 - * logic. Call site only calls if the priority of the task changed.
 + *          - cond_resched() call
 + *          - explicit schedule() call
 + *          - return from syscall or exception to user-space
 + *          - return from interrupt-handler to user-space
   */
 -void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 +static void __sched __schedule(void)
  {
 -	int prio, oldprio, queued, running, queue_flag =
 -		DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
 -	const struct sched_class *prev_class;
 -	struct rq_flags rf;
 +	struct task_struct *prev, *next;
 +	unsigned long *switch_count;
  	struct rq *rq;
 +	int cpu;
  
 -	/* XXX used to be waiter->prio, not waiter->task->prio */
 -	prio = __rt_effective_prio(pi_task, p->normal_prio);
 +need_resched:
 +	preempt_disable();
 +	cpu = smp_processor_id();
 +	rq = cpu_rq(cpu);
 +	rcu_note_context_switch(cpu);
 +	prev = rq->curr;
  
 -	/*
 -	 * If nothing changed; bail early.
 -	 */
 -	if (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))
 -		return;
 +	schedule_debug(prev);
  
 -	rq = __task_rq_lock(p, &rf);
 -	update_rq_clock(rq);
 -	/*
 -	 * Set under pi_lock && rq->lock, such that the value can be used under
 -	 * either lock.
 -	 *
 -	 * Note that there is loads of tricky to make this pointer cache work
 -	 * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to
 -	 * ensure a task is de-boosted (pi_task is set to NULL) before the
 -	 * task is allowed to run again (and can exit). This ensures the pointer
 -	 * points to a blocked task -- which guaratees the task is present.
 -	 */
 -	p->pi_top_task = pi_task;
 +	if (sched_feat(HRTICK))
 +		hrtick_clear(rq);
  
  	/*
 -	 * For FIFO/RR we only need to set prio, if that matches we're done.
 +	 * Make sure that signal_pending_state()->signal_pending() below
 +	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
 +	 * done by the caller to avoid the race with signal_wake_up().
  	 */
 -	if (prio == p->prio && !dl_prio(prio))
 -		goto out_unlock;
 +	smp_mb__before_spinlock();
 +	raw_spin_lock_irq(&rq->lock);
  
 -	/*
 -	 * Idle task boosting is a nono in general. There is one
 -	 * exception, when PREEMPT_RT and NOHZ is active:
 -	 *
 -	 * The idle task calls get_next_timer_interrupt() and holds
 -	 * the timer wheel base->lock on the CPU and another CPU wants
 -	 * to access the timer (probably to cancel it). We can safely
 -	 * ignore the boosting request, as the idle CPU runs this code
 -	 * with interrupts disabled and will complete the lock
 -	 * protected section without being interrupted. So there is no
 -	 * real need to boost.
 -	 */
 -	if (unlikely(p == rq->idle)) {
 -		WARN_ON(p != rq->curr);
 -		WARN_ON(p->pi_blocked_on);
 -		goto out_unlock;
 -	}
 +	switch_count = &prev->nivcsw;
 +	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
 +		if (unlikely(signal_pending_state(prev->state, prev))) {
 +			prev->state = TASK_RUNNING;
 +		} else {
 +			deactivate_task(rq, prev, DEQUEUE_SLEEP);
 +			prev->on_rq = 0;
  
 -	trace_sched_pi_setprio(p, pi_task);
 -	oldprio = p->prio;
 +			/*
 +			 * If a worker went to sleep, notify and ask workqueue
 +			 * whether it wants to wake up a task to maintain
 +			 * concurrency.
 +			 */
 +			if (prev->flags & PF_WQ_WORKER) {
 +				struct task_struct *to_wakeup;
  
 -	if (oldprio == prio)
 -		queue_flag &= ~DEQUEUE_MOVE;
 +				to_wakeup = wq_worker_sleeping(prev, cpu);
 +				if (to_wakeup)
 +					try_to_wake_up_local(to_wakeup);
 +			}
 +		}
 +		switch_count = &prev->nvcsw;
 +	}
  
 -	prev_class = p->sched_class;
 -	queued = task_on_rq_queued(p);
 -	running = task_current(rq, p);
 -	if (queued)
 -		dequeue_task(rq, p, queue_flag);
 -	if (running)
 -		put_prev_task(rq, p);
 +	pre_schedule(rq, prev);
  
 -	/*
 -	 * Boosting condition are:
 -	 * 1. -rt task is running and holds mutex A
 -	 *      --> -dl task blocks on mutex A
 -	 *
 -	 * 2. -dl task is running and holds mutex A
 -	 *      --> -dl task blocks on mutex A and could preempt the
 -	 *          running task
 -	 */
 -	if (dl_prio(prio)) {
 -		if (!dl_prio(p->normal_prio) ||
 -		    (pi_task && dl_entity_preempt(&pi_task->dl, &p->dl))) {
 -			p->dl.dl_boosted = 1;
 -			queue_flag |= ENQUEUE_REPLENISH;
 -		} else
 -			p->dl.dl_boosted = 0;
 -		p->sched_class = &dl_sched_class;
 -	} else if (rt_prio(prio)) {
 -		if (dl_prio(oldprio))
 -			p->dl.dl_boosted = 0;
 -		if (oldprio < prio)
 -			queue_flag |= ENQUEUE_HEAD;
 -		p->sched_class = &rt_sched_class;
 -	} else {
 -		if (dl_prio(oldprio))
 -			p->dl.dl_boosted = 0;
 -		if (rt_prio(oldprio))
 -			p->rt.timeout = 0;
 -		p->sched_class = &fair_sched_class;
 -	}
 +	if (unlikely(!rq->nr_running))
 +		idle_balance(cpu, rq);
  
 -	p->prio = prio;
 +	put_prev_task(rq, prev);
 +	next = pick_next_task(rq);
 +	clear_tsk_need_resched(prev);
 +	rq->skip_clock_update = 0;
  
 -	if (queued)
 -		enqueue_task(rq, p, queue_flag);
 -	if (running)
 -		set_curr_task(rq, p);
 +	if (likely(prev != next)) {
 +		rq->nr_switches++;
 +		rq->curr = next;
 +		++*switch_count;
  
 -	check_class_changed(rq, p, prev_class, oldprio);
 -out_unlock:
 -	/* Avoid rq from going away on us: */
 -	preempt_disable();
 -	__task_rq_unlock(rq, &rf);
 +		context_switch(rq, prev, next); /* unlocks the rq */
 +		/*
 +		 * The context switch have flipped the stack from under us
 +		 * and restored the local variables which were saved when
 +		 * this task called schedule() in the past. prev == current
 +		 * is still correct, but it can be moved to another cpu/rq.
 +		 */
 +		cpu = smp_processor_id();
 +		rq = cpu_rq(cpu);
 +	} else
 +		raw_spin_unlock_irq(&rq->lock);
  
 -	balance_callback(rq);
 -	preempt_enable();
 -}
 -#else
 -static inline int rt_effective_prio(struct task_struct *p, int prio)
 -{
 -	return prio;
 +	post_schedule(rq);
 +
 +	sched_preempt_enable_no_resched();
 +	if (need_resched())
 +		goto need_resched;
  }
 -#endif
 +STACK_FRAME_NON_STANDARD(__schedule); /* switch_to() */
  
 -void set_user_nice(struct task_struct *p, long nice)
 +static inline void sched_submit_work(struct task_struct *tsk)
  {
 -	bool queued, running;
 -	int old_prio, delta;
 -	struct rq_flags rf;
 -	struct rq *rq;
 -
 -	if (task_nice(p) == nice || nice < MIN_NICE || nice > MAX_NICE)
 +	if (!tsk->state || tsk_is_pi_blocked(tsk))
  		return;
  	/*
 -	 * We have to be careful, if called from sys_setpriority(),
 -	 * the task might be in the middle of scheduling on another CPU.
 -	 */
 -	rq = task_rq_lock(p, &rf);
 -	update_rq_clock(rq);
 -
 -	/*
 -	 * The RT priorities are set via sched_setscheduler(), but we still
 -	 * allow the 'normal' nice value to be set - but as expected
 -	 * it wont have any effect on scheduling until the task is
 -	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:
 +	 * If we are going to sleep and we have plugged IO queued,
 +	 * make sure to submit it to avoid deadlocks.
  	 */
 -	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
 -		p->static_prio = NICE_TO_PRIO(nice);
 -		goto out_unlock;
 -	}
 -	queued = task_on_rq_queued(p);
 -	running = task_current(rq, p);
 -	if (queued)
 -		dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
 -	if (running)
 -		put_prev_task(rq, p);
 -
 -	p->static_prio = NICE_TO_PRIO(nice);
 -	set_load_weight(p);
 -	old_prio = p->prio;
 -	p->prio = effective_prio(p);
 -	delta = p->prio - old_prio;
 -
 -	if (queued) {
 -		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
 -		/*
 -		 * If the task increased its priority or is running and
 -		 * lowered its priority, then reschedule its CPU:
 -		 */
 -		if (delta < 0 || (delta > 0 && task_running(rq, p)))
 -			resched_curr(rq);
 -	}
 -	if (running)
 -		set_curr_task(rq, p);
 -out_unlock:
 -	task_rq_unlock(rq, p, &rf);
 +	if (blk_needs_flush_plug(tsk))
 +		blk_schedule_flush_plug(tsk);
  }
 -EXPORT_SYMBOL(set_user_nice);
  
 -/*
 - * can_nice - check if a task can reduce its nice value
 - * @p: task
 - * @nice: nice value
 - */
 -int can_nice(const struct task_struct *p, const int nice)
 +asmlinkage void __sched schedule(void)
  {
 -	/* Convert nice value [19,-20] to rlimit style value [1,40]: */
 -	int nice_rlim = nice_to_rlimit(nice);
 +	struct task_struct *tsk = current;
  
 -	return (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||
 -		capable(CAP_SYS_NICE));
 +	sched_submit_work(tsk);
 +	__schedule();
  }
 +EXPORT_SYMBOL(schedule);
  
 -#ifdef __ARCH_WANT_SYS_NICE
 +#ifdef CONFIG_CONTEXT_TRACKING
 +asmlinkage void __sched schedule_user(void)
 +{
 +	/*
 +	 * If we come here after a random call to set_need_resched(),
 +	 * or we have been woken up remotely but the IPI has not yet arrived,
 +	 * we haven't yet exited the RCU idle mode. Do it here manually until
 +	 * we find a better solution.
 +	 *
 +	 * NB: There are buggy callers of this function.  Ideally we
 +	 * should warn if prev_state != CONTEXT_USER, but that will trigger
 +	 * too frequently to make sense yet.
 +	 */
 +	enum ctx_state prev_state = exception_enter();
 +	schedule();
 +	exception_exit(prev_state);
 +}
 +#endif
  
 -/*
 - * sys_nice - change the priority of the current process.
 - * @increment: priority increment
 +/**
 + * schedule_preempt_disabled - called with preemption disabled
   *
 - * sys_setpriority is a more generic, but much slower function that
 - * does similar things.
 + * Returns with preemption disabled. Note: preempt_count must be 1
   */
 -SYSCALL_DEFINE1(nice, int, increment)
 +void __sched schedule_preempt_disabled(void)
  {
 -	long nice, retval;
 +	sched_preempt_enable_no_resched();
 +	schedule();
 +	preempt_disable();
 +}
  
 +#ifdef CONFIG_PREEMPT
 +/*
 + * this is the entry point to schedule() from in-kernel preemption
 + * off of preempt_enable. Kernel preemptions off return from interrupt
 + * occur there and call schedule directly.
 + */
 +asmlinkage void __sched notrace preempt_schedule(void)
 +{
  	/*
 -	 * Setpriority might change our priority at the same moment.
 -	 * We don't have to worry. Conceptually one call occurs first
 -	 * and we have a single winner.
 +	 * If there is a non-zero preempt_count or interrupts are disabled,
 +	 * we do not want to preempt the current task. Just return..
  	 */
 -	increment = clamp(increment, -NICE_WIDTH, NICE_WIDTH);
 -	nice = task_nice(current) + increment;
 +	if (likely(!preemptible()))
 +		return;
  
 -	nice = clamp_val(nice, MIN_NICE, MAX_NICE);
 -	if (increment < 0 && !can_nice(current, nice))
 -		return -EPERM;
 +	do {
 +		add_preempt_count_notrace(PREEMPT_ACTIVE);
 +		__schedule();
 +		sub_preempt_count_notrace(PREEMPT_ACTIVE);
  
 -	retval = security_task_setnice(current, nice);
 -	if (retval)
 -		return retval;
 +		/*
 +		 * Check again in case we missed a preemption opportunity
 +		 * between schedule and now.
 +		 */
 +		barrier();
 +	} while (need_resched());
 +}
 +EXPORT_SYMBOL(preempt_schedule);
  
 -	set_user_nice(current, nice);
 +/*
 + * this is the entry point to schedule() from kernel preemption
 + * off of irq context.
 + * Note, that this is called and return with irqs disabled. This will
 + * protect us against recursive calling from irq.
 + */
 +asmlinkage void __sched preempt_schedule_irq(void)
 +{
 +	struct thread_info *ti = current_thread_info();
 +	enum ctx_state prev_state;
 +
 +	/* Catch callers which need to be fixed */
 +	BUG_ON(ti->preempt_count || !irqs_disabled());
 +
 +	prev_state = exception_enter();
 +
 +	do {
 +		add_preempt_count(PREEMPT_ACTIVE);
 +		local_irq_enable();
 +		__schedule();
 +		local_irq_disable();
 +		sub_preempt_count(PREEMPT_ACTIVE);
 +
 +		/*
 +		 * Check again in case we missed a preemption opportunity
 +		 * between schedule and now.
 +		 */
 +		barrier();
 +	} while (need_resched());
 +
 +	exception_exit(prev_state);
 +}
 +
 +#endif /* CONFIG_PREEMPT */
 +
 +int default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags,
 +			  void *key)
 +{
 +	return try_to_wake_up(curr->private, mode, wake_flags);
 +}
 +EXPORT_SYMBOL(default_wake_function);
 +
 +/*
 + * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just
 + * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve
 + * number) then we wake all the non-exclusive tasks and one exclusive task.
 + *
 + * There are circumstances in which we can try to wake a task which has already
 + * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns
 + * zero in this (rare) case, and we handle it by continuing to scan the queue.
 + */
 +static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
 +			int nr_exclusive, int wake_flags, void *key)
 +{
 +	wait_queue_t *curr, *next;
 +
 +	list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
 +		unsigned flags = curr->flags;
 +
 +		if (curr->func(curr, mode, wake_flags, key) &&
 +				(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
 +			break;
 +	}
 +}
 +
 +/**
 + * __wake_up - wake up threads blocked on a waitqueue.
 + * @q: the waitqueue
 + * @mode: which threads
 + * @nr_exclusive: how many wake-one or wake-many threads to wake up
 + * @key: is directly passed to the wakeup function
 + *
 + * It may be assumed that this function implies a write memory barrier before
 + * changing the task state if and only if any tasks are woken up.
 + */
 +void __wake_up(wait_queue_head_t *q, unsigned int mode,
 +			int nr_exclusive, void *key)
 +{
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&q->lock, flags);
 +	__wake_up_common(q, mode, nr_exclusive, 0, key);
 +	spin_unlock_irqrestore(&q->lock, flags);
 +}
 +EXPORT_SYMBOL(__wake_up);
 +
 +/*
 + * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
 + */
 +void __wake_up_locked(wait_queue_head_t *q, unsigned int mode, int nr)
 +{
 +	__wake_up_common(q, mode, nr, 0, NULL);
 +}
 +EXPORT_SYMBOL_GPL(__wake_up_locked);
 +
 +void __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)
 +{
 +	__wake_up_common(q, mode, 1, 0, key);
 +}
 +EXPORT_SYMBOL_GPL(__wake_up_locked_key);
 +
 +/**
 + * __wake_up_sync_key - wake up threads blocked on a waitqueue.
 + * @q: the waitqueue
 + * @mode: which threads
 + * @nr_exclusive: how many wake-one or wake-many threads to wake up
 + * @key: opaque value to be passed to wakeup targets
 + *
 + * The sync wakeup differs that the waker knows that it will schedule
 + * away soon, so while the target thread will be woken up, it will not
 + * be migrated to another CPU - ie. the two threads are 'synchronized'
 + * with each other. This can prevent needless bouncing between CPUs.
 + *
 + * On UP it can prevent extra preemption.
 + *
 + * It may be assumed that this function implies a write memory barrier before
 + * changing the task state if and only if any tasks are woken up.
 + */
 +void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,
 +			int nr_exclusive, void *key)
 +{
 +	unsigned long flags;
 +	int wake_flags = WF_SYNC;
 +
 +	if (unlikely(!q))
 +		return;
 +
 +	if (unlikely(!nr_exclusive))
 +		wake_flags = 0;
 +
 +	spin_lock_irqsave(&q->lock, flags);
 +	__wake_up_common(q, mode, nr_exclusive, wake_flags, key);
 +	spin_unlock_irqrestore(&q->lock, flags);
 +}
 +EXPORT_SYMBOL_GPL(__wake_up_sync_key);
 +
 +/*
 + * __wake_up_sync - see __wake_up_sync_key()
 + */
 +void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
 +{
 +	__wake_up_sync_key(q, mode, nr_exclusive, NULL);
 +}
 +EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
 +
 +/**
 + * complete: - signals a single thread waiting on this completion
 + * @x:  holds the state of this particular completion
 + *
 + * This will wake up a single thread waiting on this completion. Threads will be
 + * awakened in the same order in which they were queued.
 + *
 + * See also complete_all(), wait_for_completion() and related routines.
 + *
 + * It may be assumed that this function implies a write memory barrier before
 + * changing the task state if and only if any tasks are woken up.
 + */
 +void complete(struct completion *x)
 +{
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&x->wait.lock, flags);
 +	x->done++;
 +	__wake_up_common(&x->wait, TASK_NORMAL, 1, 0, NULL);
 +	spin_unlock_irqrestore(&x->wait.lock, flags);
 +}
 +EXPORT_SYMBOL(complete);
 +
 +/**
 + * complete_all: - signals all threads waiting on this completion
 + * @x:  holds the state of this particular completion
 + *
 + * This will wake up all threads waiting on this particular completion event.
 + *
 + * It may be assumed that this function implies a write memory barrier before
 + * changing the task state if and only if any tasks are woken up.
 + */
 +void complete_all(struct completion *x)
 +{
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&x->wait.lock, flags);
 +	x->done += UINT_MAX/2;
 +	__wake_up_common(&x->wait, TASK_NORMAL, 0, 0, NULL);
 +	spin_unlock_irqrestore(&x->wait.lock, flags);
 +}
 +EXPORT_SYMBOL(complete_all);
 +
 +static inline long __sched
 +do_wait_for_common(struct completion *x,
 +		   long (*action)(long), long timeout, int state)
 +{
 +	if (!x->done) {
 +		DECLARE_WAITQUEUE(wait, current);
 +
 +		__add_wait_queue_tail_exclusive(&x->wait, &wait);
 +		do {
 +			if (signal_pending_state(state, current)) {
 +				timeout = -ERESTARTSYS;
 +				break;
 +			}
 +			__set_current_state(state);
 +			spin_unlock_irq(&x->wait.lock);
 +			timeout = action(timeout);
 +			spin_lock_irq(&x->wait.lock);
 +		} while (!x->done && timeout);
 +		__remove_wait_queue(&x->wait, &wait);
 +		if (!x->done)
 +			return timeout;
 +	}
 +	x->done--;
 +	return timeout ?: 1;
 +}
 +
 +static inline long __sched
 +__wait_for_common(struct completion *x,
 +		  long (*action)(long), long timeout, int state)
 +{
 +	might_sleep();
 +
 +	spin_lock_irq(&x->wait.lock);
 +	timeout = do_wait_for_common(x, action, timeout, state);
 +	spin_unlock_irq(&x->wait.lock);
 +	return timeout;
 +}
 +
 +static long __sched
 +wait_for_common(struct completion *x, long timeout, int state)
 +{
 +	return __wait_for_common(x, schedule_timeout, timeout, state);
 +}
 +
 +static long __sched
 +wait_for_common_io(struct completion *x, long timeout, int state)
 +{
 +	return __wait_for_common(x, io_schedule_timeout, timeout, state);
 +}
 +
 +/**
 + * wait_for_completion: - waits for completion of a task
 + * @x:  holds the state of this particular completion
 + *
 + * This waits to be signaled for completion of a specific task. It is NOT
 + * interruptible and there is no timeout.
 + *
 + * See also similar routines (i.e. wait_for_completion_timeout()) with timeout
 + * and interrupt capability. Also see complete().
 + */
 +void __sched wait_for_completion(struct completion *x)
 +{
 +	wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);
 +}
 +EXPORT_SYMBOL(wait_for_completion);
 +
 +/**
 + * wait_for_completion_timeout: - waits for completion of a task (w/timeout)
 + * @x:  holds the state of this particular completion
 + * @timeout:  timeout value in jiffies
 + *
 + * This waits for either a completion of a specific task to be signaled or for a
 + * specified timeout to expire. The timeout is in jiffies. It is not
 + * interruptible.
 + *
 + * Return: 0 if timed out, and positive (at least 1, or number of jiffies left
 + * till timeout) if completed.
 + */
 +unsigned long __sched
 +wait_for_completion_timeout(struct completion *x, unsigned long timeout)
 +{
 +	return wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);
 +}
 +EXPORT_SYMBOL(wait_for_completion_timeout);
 +
 +/**
 + * wait_for_completion_io: - waits for completion of a task
 + * @x:  holds the state of this particular completion
 + *
 + * This waits to be signaled for completion of a specific task. It is NOT
 + * interruptible and there is no timeout. The caller is accounted as waiting
 + * for IO.
 + */
 +void __sched wait_for_completion_io(struct completion *x)
 +{
 +	wait_for_common_io(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);
 +}
 +EXPORT_SYMBOL(wait_for_completion_io);
 +
 +/**
 + * wait_for_completion_io_timeout: - waits for completion of a task (w/timeout)
 + * @x:  holds the state of this particular completion
 + * @timeout:  timeout value in jiffies
 + *
 + * This waits for either a completion of a specific task to be signaled or for a
 + * specified timeout to expire. The timeout is in jiffies. It is not
 + * interruptible. The caller is accounted as waiting for IO.
 + *
 + * Return: 0 if timed out, and positive (at least 1, or number of jiffies left
 + * till timeout) if completed.
 + */
 +unsigned long __sched
 +wait_for_completion_io_timeout(struct completion *x, unsigned long timeout)
 +{
 +	return wait_for_common_io(x, timeout, TASK_UNINTERRUPTIBLE);
 +}
 +EXPORT_SYMBOL(wait_for_completion_io_timeout);
 +
 +/**
 + * wait_for_completion_interruptible: - waits for completion of a task (w/intr)
 + * @x:  holds the state of this particular completion
 + *
 + * This waits for completion of a specific task to be signaled. It is
 + * interruptible.
 + *
 + * Return: -ERESTARTSYS if interrupted, 0 if completed.
 + */
 +int __sched wait_for_completion_interruptible(struct completion *x)
 +{
 +	long t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);
 +	if (t == -ERESTARTSYS)
 +		return t;
  	return 0;
  }
 +EXPORT_SYMBOL(wait_for_completion_interruptible);
  
 -#endif
 +/**
 + * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))
 + * @x:  holds the state of this particular completion
 + * @timeout:  timeout value in jiffies
 + *
 + * This waits for either a completion of a specific task to be signaled or for a
 + * specified timeout to expire. It is interruptible. The timeout is in jiffies.
 + *
 + * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,
 + * or number of jiffies left till timeout) if completed.
 + */
 +long __sched
 +wait_for_completion_interruptible_timeout(struct completion *x,
 +					  unsigned long timeout)
 +{
 +	return wait_for_common(x, timeout, TASK_INTERRUPTIBLE);
 +}
 +EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);
  
  /**
 - * task_prio - return the priority value of a given task.
 - * @p: the task in question.
 + * wait_for_completion_killable: - waits for completion of a task (killable)
 + * @x:  holds the state of this particular completion
   *
 - * Return: The priority value as seen by users in /proc.
 - * RT tasks are offset by -200. Normal tasks are centered
 - * around 0, value goes from -16 to +15.
 + * This waits to be signaled for completion of a specific task. It can be
 + * interrupted by a kill signal.
 + *
 + * Return: -ERESTARTSYS if interrupted, 0 if completed.
   */
 -int task_prio(const struct task_struct *p)
 +int __sched wait_for_completion_killable(struct completion *x)
  {
 -	return p->prio - MAX_RT_PRIO;
 +	long t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);
 +	if (t == -ERESTARTSYS)
 +		return t;
 +	return 0;
  }
 +EXPORT_SYMBOL(wait_for_completion_killable);
  
  /**
 - * idle_cpu - is a given CPU idle currently?
 - * @cpu: the processor in question.
 + * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable))
 + * @x:  holds the state of this particular completion
 + * @timeout:  timeout value in jiffies
   *
 - * Return: 1 if the CPU is currently idle. 0 otherwise.
 + * This waits for either a completion of a specific task to be
 + * signaled or for a specified timeout to expire. It can be
 + * interrupted by a kill signal. The timeout is in jiffies.
 + *
 + * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,
 + * or number of jiffies left till timeout) if completed.
   */
 -int idle_cpu(int cpu)
 +long __sched
 +wait_for_completion_killable_timeout(struct completion *x,
 +				     unsigned long timeout)
  {
 -	struct rq *rq = cpu_rq(cpu);
 +	return wait_for_common(x, timeout, TASK_KILLABLE);
 +}
 +EXPORT_SYMBOL(wait_for_completion_killable_timeout);
  
 -	if (rq->curr != rq->idle)
 -		return 0;
 +/**
 + *	try_wait_for_completion - try to decrement a completion without blocking
 + *	@x:	completion structure
 + *
 + *	Return: 0 if a decrement cannot be done without blocking
 + *		 1 if a decrement succeeded.
 + *
 + *	If a completion is being used as a counting completion,
 + *	attempt to decrement the counter without blocking. This
 + *	enables us to avoid waiting if the resource the completion
 + *	is protecting is not available.
 + */
 +bool try_wait_for_completion(struct completion *x)
 +{
 +	unsigned long flags;
 +	int ret = 1;
  
 -	if (rq->nr_running)
 -		return 0;
 +	spin_lock_irqsave(&x->wait.lock, flags);
 +	if (!x->done)
 +		ret = 0;
 +	else
 +		x->done--;
 +	spin_unlock_irqrestore(&x->wait.lock, flags);
 +	return ret;
 +}
 +EXPORT_SYMBOL(try_wait_for_completion);
 +
 +/**
 + *	completion_done - Test to see if a completion has any waiters
 + *	@x:	completion structure
 + *
 + *	Return: 0 if there are waiters (wait_for_completion() in progress)
 + *		 1 if there are no waiters.
 + *
 + */
 +bool completion_done(struct completion *x)
 +{
 +	unsigned long flags;
 +	int ret = 1;
 +
 +	spin_lock_irqsave(&x->wait.lock, flags);
 +	if (!x->done)
 +		ret = 0;
 +	spin_unlock_irqrestore(&x->wait.lock, flags);
 +	return ret;
 +}
 +EXPORT_SYMBOL(completion_done);
 +
 +static long __sched
 +sleep_on_common(wait_queue_head_t *q, int state, long timeout)
 +{
 +	unsigned long flags;
 +	wait_queue_t wait;
 +
 +	init_waitqueue_entry(&wait, current);
 +
 +	__set_current_state(state);
 +
 +	spin_lock_irqsave(&q->lock, flags);
 +	__add_wait_queue(q, &wait);
 +	spin_unlock(&q->lock);
 +	timeout = schedule_timeout(timeout);
 +	spin_lock_irq(&q->lock);
 +	__remove_wait_queue(q, &wait);
 +	spin_unlock_irqrestore(&q->lock, flags);
 +
 +	return timeout;
 +}
 +
 +void __sched interruptible_sleep_on(wait_queue_head_t *q)
 +{
 +	sleep_on_common(q, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
 +}
 +EXPORT_SYMBOL(interruptible_sleep_on);
 +
 +long __sched
 +interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 +{
 +	return sleep_on_common(q, TASK_INTERRUPTIBLE, timeout);
 +}
 +EXPORT_SYMBOL(interruptible_sleep_on_timeout);
 +
 +void __sched sleep_on(wait_queue_head_t *q)
 +{
 +	sleep_on_common(q, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
 +}
 +EXPORT_SYMBOL(sleep_on);
 +
 +long __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
 +{
 +	return sleep_on_common(q, TASK_UNINTERRUPTIBLE, timeout);
 +}
 +EXPORT_SYMBOL(sleep_on_timeout);
 +
 +#ifdef CONFIG_RT_MUTEXES
 +
 +/*
 + * rt_mutex_setprio - set the current priority of a task
 + * @p: task
 + * @prio: prio value (kernel-internal form)
 + *
 + * This function changes the 'effective' priority of a task. It does
 + * not touch ->normal_prio like __setscheduler().
 + *
 + * Used by the rt_mutex code to implement priority inheritance logic.
 + */
 +void rt_mutex_setprio(struct task_struct *p, int prio)
 +{
 +	int oldprio, on_rq, running, enqueue_flag = 0;
 +	struct rq *rq;
 +	const struct sched_class *prev_class;
 +
 +	BUG_ON(prio > MAX_PRIO);
 +
 +	rq = __task_rq_lock(p);
 +
 +	/*
 +	 * Idle task boosting is a nono in general. There is one
 +	 * exception, when PREEMPT_RT and NOHZ is active:
 +	 *
 +	 * The idle task calls get_next_timer_interrupt() and holds
 +	 * the timer wheel base->lock on the CPU and another CPU wants
 +	 * to access the timer (probably to cancel it). We can safely
 +	 * ignore the boosting request, as the idle CPU runs this code
 +	 * with interrupts disabled and will complete the lock
 +	 * protected section without being interrupted. So there is no
 +	 * real need to boost.
 +	 */
 +	if (unlikely(p == rq->idle)) {
 +		WARN_ON(p != rq->curr);
 +		WARN_ON(p->pi_blocked_on);
 +		goto out_unlock;
 +	}
 +
 +	trace_sched_pi_setprio(p, prio);
 +	oldprio = p->prio;
 +	prev_class = p->sched_class;
 +	on_rq = p->on_rq;
 +	running = task_current(rq, p);
 +	if (on_rq)
 +		dequeue_task(rq, p, 0);
 +	if (running)
 +		p->sched_class->put_prev_task(rq, p);
 +
 +	/*
 +	 * Boosting condition are:
 +	 * 1. -rt task is running and holds mutex A
 +	 *      --> -dl task blocks on mutex A
 +	 *
 +	 * 2. -dl task is running and holds mutex A
 +	 *      --> -dl task blocks on mutex A and could preempt the
 +	 *          running task
 +	 */
 +	if (dl_prio(prio)) {
 +		struct task_struct *pi_task = rt_mutex_get_top_task(p);
 +		if (!dl_prio(p->normal_prio) ||
 +			(pi_task && dl_entity_preempt(&pi_task->dl, &p->dl))) {
 +			p->dl.dl_boosted = 1;
 +			enqueue_flag = ENQUEUE_REPLENISH;
 +		} else
 +			p->dl.dl_boosted = 0;
 +		p->sched_class = &dl_sched_class;
 +	} else if (rt_prio(prio)) {
 +		if (dl_prio(oldprio))
 +			p->dl.dl_boosted = 0;
 +		if (oldprio < prio)
 +			enqueue_flag = ENQUEUE_HEAD;
 +		p->sched_class = &rt_sched_class;
 +	} else {
 +		if (dl_prio(oldprio))
 +			p->dl.dl_boosted = 0;
 +		p->sched_class = &fair_sched_class;
 +	}
 +
 +	p->prio = prio;
 +
 +	if (running)
 +		p->sched_class->set_curr_task(rq);
 +	if (on_rq)
 +		enqueue_task(rq, p, enqueue_flag);
 +
 +	check_class_changed(rq, p, prev_class, oldprio);
 +out_unlock:
 +	__task_rq_unlock(rq);
 +}
 +#endif
 +
 +void set_user_nice(struct task_struct *p, long nice)
 +{
 +	int old_prio, delta, on_rq;
 +	unsigned long flags;
 +	struct rq *rq;
 +
 +	if (TASK_NICE(p) == nice || nice < -20 || nice > 19)
 +		return;
 +	/*
 +	 * We have to be careful, if called from sys_setpriority(),
 +	 * the task might be in the middle of scheduling on another CPU.
 +	 */
 +	rq = task_rq_lock(p, &flags);
 +	/*
 +	 * The RT priorities are set via sched_setscheduler(), but we still
 +	 * allow the 'normal' nice value to be set - but as expected
 +	 * it wont have any effect on scheduling until the task is
 +	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:
 +	 */
 +	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
 +		p->static_prio = NICE_TO_PRIO(nice);
 +		goto out_unlock;
 +	}
 +	on_rq = p->on_rq;
 +	if (on_rq)
 +		dequeue_task(rq, p, 0);
 +
 +	p->static_prio = NICE_TO_PRIO(nice);
 +	set_load_weight(p);
 +	old_prio = p->prio;
 +	p->prio = effective_prio(p);
 +	delta = p->prio - old_prio;
 +
 +	if (on_rq) {
 +		enqueue_task(rq, p, 0);
 +		/*
 +		 * If the task increased its priority or is running and
 +		 * lowered its priority, then reschedule its CPU:
 +		 */
 +		if (delta < 0 || (delta > 0 && task_running(rq, p)))
 +			resched_curr(rq);
 +	}
 +out_unlock:
 +	task_rq_unlock(rq, p, &flags);
 +}
 +EXPORT_SYMBOL(set_user_nice);
 +
 +/*
 + * can_nice - check if a task can reduce its nice value
 + * @p: task
 + * @nice: nice value
 + */
 +int can_nice(const struct task_struct *p, const int nice)
 +{
 +	/* convert nice value [19,-20] to rlimit style value [1,40] */
 +	int nice_rlim = 20 - nice;
 +
 +	return (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||
 +		capable(CAP_SYS_NICE));
 +}
 +
 +#ifdef __ARCH_WANT_SYS_NICE
 +
 +/*
 + * sys_nice - change the priority of the current process.
 + * @increment: priority increment
 + *
 + * sys_setpriority is a more generic, but much slower function that
 + * does similar things.
 + */
 +SYSCALL_DEFINE1(nice, int, increment)
 +{
 +	long nice, retval;
 +
 +	/*
 +	 * Setpriority might change our priority at the same moment.
 +	 * We don't have to worry. Conceptually one call occurs first
 +	 * and we have a single winner.
 +	 */
 +	if (increment < -40)
 +		increment = -40;
 +	if (increment > 40)
 +		increment = 40;
 +
 +	nice = TASK_NICE(current) + increment;
 +	if (nice < -20)
 +		nice = -20;
 +	if (nice > 19)
 +		nice = 19;
 +
 +	if (increment < 0 && !can_nice(current, nice))
 +		return -EPERM;
 +
 +	retval = security_task_setnice(current, nice);
 +	if (retval)
 +		return retval;
 +
 +	set_user_nice(current, nice);
 +	return 0;
 +}
 +
 +#endif
 +
 +/**
 + * task_prio - return the priority value of a given task.
 + * @p: the task in question.
 + *
 + * Return: The priority value as seen by users in /proc.
 + * RT tasks are offset by -200. Normal tasks are centered
 + * around 0, value goes from -16 to +15.
 + */
 +int task_prio(const struct task_struct *p)
 +{
 +	return p->prio - MAX_RT_PRIO;
 +}
 +
 +/**
 + * task_nice - return the nice value of a given task.
 + * @p: the task in question.
 + *
 + * Return: The nice value [ -20 ... 0 ... 19 ].
 + */
 +int task_nice(const struct task_struct *p)
 +{
 +	return TASK_NICE(p);
 +}
 +EXPORT_SYMBOL(task_nice);
 +
 +/**
 + * idle_cpu - is a given cpu idle currently?
 + * @cpu: the processor in question.
 + *
 + * Return: 1 if the CPU is currently idle. 0 otherwise.
 + */
 +int idle_cpu(int cpu)
 +{
 +	struct rq *rq = cpu_rq(cpu);
 +
 +	if (rq->curr != rq->idle)
 +		return 0;
 +
 +	if (rq->nr_running)
 +		return 0;
 +
 +#ifdef CONFIG_SMP
 +	if (!llist_empty(&rq->wake_list))
 +		return 0;
 +#endif
 +
 +	return 1;
 +}
 +
 +/**
 + * idle_task - return the idle task for a given cpu.
 + * @cpu: the processor in question.
 + *
 + * Return: The idle task for the cpu @cpu.
 + */
 +struct task_struct *idle_task(int cpu)
 +{
 +	return cpu_rq(cpu)->idle;
 +}
 +
 +/**
 + * find_process_by_pid - find a process with a matching PID value.
 + * @pid: the pid in question.
 + *
 + * The task of @pid, if found. %NULL otherwise.
 + */
 +static struct task_struct *find_process_by_pid(pid_t pid)
 +{
 +	return pid ? find_task_by_vpid(pid) : current;
 +}
 +
 +/*
 + * This function initializes the sched_dl_entity of a newly becoming
 + * SCHED_DEADLINE task.
 + *
 + * Only the static values are considered here, the actual runtime and the
 + * absolute deadline will be properly calculated when the task is enqueued
 + * for the first time with its new policy.
 + */
 +static void
 +__setparam_dl(struct task_struct *p, const struct sched_attr *attr)
 +{
 +	struct sched_dl_entity *dl_se = &p->dl;
 +
 +	dl_se->dl_runtime = attr->sched_runtime;
 +	dl_se->dl_deadline = attr->sched_deadline;
 +	dl_se->dl_period = attr->sched_period ?: dl_se->dl_deadline;
 +	dl_se->flags = attr->sched_flags;
 +	dl_se->dl_bw = to_ratio(dl_se->dl_period, dl_se->dl_runtime);
 +	dl_se->dl_density = to_ratio(dl_se->dl_deadline, dl_se->dl_runtime);
 +
 +	/*
 +	 * Changing the parameters of a task is 'tricky' and we're not doing
 +	 * the correct thing -- also see task_dead_dl() and switched_from_dl().
 +	 *
 +	 * What we SHOULD do is delay the bandwidth release until the 0-lag
 +	 * point. This would include retaining the task_struct until that time
 +	 * and change dl_overflow() to not immediately decrement the current
 +	 * amount.
 +	 *
 +	 * Instead we retain the current runtime/deadline and let the new
 +	 * parameters take effect after the current reservation period lapses.
 +	 * This is safe (albeit pessimistic) because the 0-lag point is always
 +	 * before the current scheduling deadline.
 +	 *
 +	 * We can still have temporary overloads because we do not delay the
 +	 * change in bandwidth until that time; so admission control is
 +	 * not on the safe side. It does however guarantee tasks will never
 +	 * consume more than promised.
 +	 */
 +}
 +
 +/* Actually do priority change: must hold pi & rq lock. */
 +static void __setscheduler(struct rq *rq, struct task_struct *p,
 +			   const struct sched_attr *attr)
 +{
 +	int policy = attr->sched_policy;
 +
 +	if (policy == -1) /* setparam */
 +		policy = p->policy;
 +
 +	p->policy = policy;
 +
 +	if (dl_policy(policy))
 +		__setparam_dl(p, attr);
 +	else if (fair_policy(policy))
 +		p->static_prio = NICE_TO_PRIO(attr->sched_nice);
 +
 +	/*
 +	 * __sched_setscheduler() ensures attr->sched_priority == 0 when
 +	 * !rt_policy. Always setting this ensures that things like
 +	 * getparam()/getattr() don't report silly values for !rt tasks.
 +	 */
 +	p->rt_priority = attr->sched_priority;
 +
 +	p->normal_prio = normal_prio(p);
 +	p->prio = rt_mutex_getprio(p);
 +
 +	if (dl_prio(p->prio))
 +		p->sched_class = &dl_sched_class;
 +	else if (rt_prio(p->prio))
 +		p->sched_class = &rt_sched_class;
 +	else
 +		p->sched_class = &fair_sched_class;
 +
 +	set_load_weight(p);
 +}
 +
 +static void
 +__getparam_dl(struct task_struct *p, struct sched_attr *attr)
 +{
 +	struct sched_dl_entity *dl_se = &p->dl;
 +
 +	attr->sched_priority = p->rt_priority;
 +	attr->sched_runtime = dl_se->dl_runtime;
 +	attr->sched_deadline = dl_se->dl_deadline;
 +	attr->sched_period = dl_se->dl_period;
 +	attr->sched_flags = dl_se->flags;
 +}
 +
 +/*
 + * This function validates the new parameters of a -deadline task.
 + * We ask for the deadline not being zero, and greater or equal
 + * than the runtime, as well as the period of being zero or
 + * greater than deadline. Furthermore, we have to be sure that
 + * user parameters are above the internal resolution of 1us (we
 + * check sched_runtime only since it is always the smaller one) and
 + * below 2^63 ns (we have to check both sched_deadline and
 + * sched_period, as the latter can be zero).
 + */
 +static bool
 +__checkparam_dl(const struct sched_attr *attr)
 +{
 +	/* deadline != 0 */
 +	if (attr->sched_deadline == 0)
 +		return false;
 +
 +	/*
 +	 * Since we truncate DL_SCALE bits, make sure we're at least
 +	 * that big.
 +	 */
 +	if (attr->sched_runtime < (1ULL << DL_SCALE))
 +		return false;
 +
 +	/*
 +	 * Since we use the MSB for wrap-around and sign issues, make
 +	 * sure it's not set (mind that period can be equal to zero).
 +	 */
 +	if (attr->sched_deadline & (1ULL << 63) ||
 +	    attr->sched_period & (1ULL << 63))
 +		return false;
 +
 +	/* runtime <= deadline <= period (if period != 0) */
 +	if ((attr->sched_period != 0 &&
 +	     attr->sched_period < attr->sched_deadline) ||
 +	    attr->sched_deadline < attr->sched_runtime)
 +		return false;
 +
 +	return true;
 +}
 +
 +/*
 + * check the target process has a UID that matches the current process's
 + */
 +static bool check_same_owner(struct task_struct *p)
 +{
 +	const struct cred *cred = current_cred(), *pcred;
 +	bool match;
 +
 +	rcu_read_lock();
 +	pcred = __task_cred(p);
 +	match = (uid_eq(cred->euid, pcred->euid) ||
 +		 uid_eq(cred->euid, pcred->uid));
 +	rcu_read_unlock();
 +	return match;
 +}
 +
 +static int __sched_setscheduler(struct task_struct *p,
 +				const struct sched_attr *attr,
 +				bool user)
 +{
 +	int retval, oldprio, oldpolicy = -1, on_rq, running;
 +	int policy = attr->sched_policy;
 +	unsigned long flags;
 +	const struct sched_class *prev_class;
 +	struct rq *rq;
 +	int reset_on_fork;
 +
 +	/* may grab non-irq protected spin_locks */
 +	BUG_ON(in_interrupt());
 +recheck:
 +	/* double check policy once rq lock held */
 +	if (policy < 0) {
 +		reset_on_fork = p->sched_reset_on_fork;
 +		policy = oldpolicy = p->policy;
 +	} else {
 +		reset_on_fork = !!(attr->sched_flags & SCHED_FLAG_RESET_ON_FORK);
 +
 +		if (policy != SCHED_DEADLINE &&
 +				policy != SCHED_FIFO && policy != SCHED_RR &&
 +				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
 +				policy != SCHED_IDLE)
 +			return -EINVAL;
 +	}
 +
 +	if (attr->sched_flags &
 +		~(SCHED_FLAG_RESET_ON_FORK | SCHED_FLAG_RECLAIM))
 +		return -EINVAL;
 +
 +	/*
 +	 * Valid priorities for SCHED_FIFO and SCHED_RR are
 +	 * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,
 +	 * SCHED_BATCH and SCHED_IDLE is 0.
 +	 */
 +	if ((p->mm && attr->sched_priority > MAX_USER_RT_PRIO-1) ||
 +	    (!p->mm && attr->sched_priority > MAX_RT_PRIO-1))
 +		return -EINVAL;
 +	if ((dl_policy(policy) && !__checkparam_dl(attr)) ||
 +	    (rt_policy(policy) != (attr->sched_priority != 0)))
 +		return -EINVAL;
 +
 +	/*
 +	 * Allow unprivileged RT tasks to decrease priority:
 +	 */
 +	if (user && !capable(CAP_SYS_NICE)) {
 +		if (fair_policy(policy)) {
 +			if (attr->sched_nice < TASK_NICE(p) &&
 +			    !can_nice(p, attr->sched_nice))
 +				return -EPERM;
 +		}
 +
 +		if (rt_policy(policy)) {
 +			unsigned long rlim_rtprio =
 +					task_rlimit(p, RLIMIT_RTPRIO);
 +
 +			/* can't set/change the rt policy */
 +			if (policy != p->policy && !rlim_rtprio)
 +				return -EPERM;
 +
 +			/* can't increase priority */
 +			if (attr->sched_priority > p->rt_priority &&
 +			    attr->sched_priority > rlim_rtprio)
 +				return -EPERM;
 +		}
 +
 +		 /*
 +		  * Can't set/change SCHED_DEADLINE policy at all for now
 +		  * (safest behavior); in the future we would like to allow
 +		  * unprivileged DL tasks to increase their relative deadline
 +		  * or reduce their runtime (both ways reducing utilization)
 +		  */
 +		if (dl_policy(policy))
 +			return -EPERM;
 +
 +		/*
 +		 * Treat SCHED_IDLE as nice 20. Only allow a switch to
 +		 * SCHED_NORMAL if the RLIMIT_NICE would normally permit it.
 +		 */
 +		if (p->policy == SCHED_IDLE && policy != SCHED_IDLE) {
 +			if (!can_nice(p, TASK_NICE(p)))
 +				return -EPERM;
 +		}
 +
 +		/* can't change other user's priorities */
 +		if (!check_same_owner(p))
 +			return -EPERM;
 +
 +		/* Normal users shall not reset the sched_reset_on_fork flag */
 +		if (p->sched_reset_on_fork && !reset_on_fork)
 +			return -EPERM;
 +	}
 +
 +	if (user) {
 +		retval = security_task_setscheduler(p);
 +		if (retval)
 +			return retval;
 +	}
 +
 +	/*
 +	 * make sure no PI-waiters arrive (or leave) while we are
 +	 * changing the priority of the task:
 +	 *
 +	 * To be able to change p->policy safely, the appropriate
 +	 * runqueue lock must be held.
 +	 */
 +	rq = task_rq_lock(p, &flags);
 +
 +	/*
 +	 * Changing the policy of the stop threads its a very bad idea
 +	 */
 +	if (p == rq->stop) {
 +		task_rq_unlock(rq, p, &flags);
 +		return -EINVAL;
 +	}
 +
 +	/*
 +	 * If not changing anything there's no need to proceed further:
 +	 */
 +	if (unlikely(policy == p->policy)) {
 +		if (fair_policy(policy) && attr->sched_nice != TASK_NICE(p))
 +			goto change;
 +		if (rt_policy(policy) && attr->sched_priority != p->rt_priority)
 +			goto change;
 +		if (dl_policy(policy))
 +			goto change;
 +
 +		task_rq_unlock(rq, p, &flags);
 +		return 0;
 +	}
 +change:
 +
 +	if (user) {
 +#ifdef CONFIG_RT_GROUP_SCHED
 +		/*
 +		 * Do not allow realtime tasks into groups that have no runtime
 +		 * assigned.
 +		 */
 +		if (rt_bandwidth_enabled() && rt_policy(policy) &&
 +				task_group(p)->rt_bandwidth.rt_runtime == 0 &&
 +				!task_group_is_autogroup(task_group(p))) {
 +			task_rq_unlock(rq, p, &flags);
 +			return -EPERM;
 +		}
 +#endif
 +#ifdef CONFIG_SMP
 +		if (dl_bandwidth_enabled() && dl_policy(policy)) {
 +			cpumask_t *span = rq->rd->span;
 +
 +			/*
 +			 * Don't allow tasks with an affinity mask smaller than
 +			 * the entire root_domain to become SCHED_DEADLINE. We
 +			 * will also fail if there's no bandwidth available.
 +			 */
 +			if (!cpumask_subset(span, &p->cpus_allowed) ||
 +			    rq->rd->dl_bw.bw == 0) {
 +				task_rq_unlock(rq, p, &flags);
 +				return -EPERM;
 +			}
 +		}
 +#endif
 +	}
 +
 +	/* recheck policy now with rq lock held */
 +	if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {
 +		policy = oldpolicy = -1;
 +		task_rq_unlock(rq, p, &flags);
 +		goto recheck;
 +	}
 +
 +	/*
 +	 * If setscheduling to SCHED_DEADLINE (or changing the parameters
 +	 * of a SCHED_DEADLINE task) we need to check if enough bandwidth
 +	 * is available.
 +	 */
 +	if ((dl_policy(policy) || dl_task(p)) && dl_overflow(p, policy, attr)) {
 +		task_rq_unlock(rq, p, &flags);
 +		return -EBUSY;
 +	}
 +
 +	on_rq = p->on_rq;
 +	running = task_current(rq, p);
 +	if (on_rq)
 +		dequeue_task(rq, p, 0);
 +	if (running)
 +		p->sched_class->put_prev_task(rq, p);
 +
 +	p->sched_reset_on_fork = reset_on_fork;
 +
 +	oldprio = p->prio;
 +	prev_class = p->sched_class;
 +	__setscheduler(rq, p, attr);
 +
 +	if (running)
 +		p->sched_class->set_curr_task(rq);
 +	if (on_rq)
 +		enqueue_task(rq, p, 0);
 +
 +	check_class_changed(rq, p, prev_class, oldprio);
 +	task_rq_unlock(rq, p, &flags);
 +
 +	rt_mutex_adjust_pi(p);
 +
 +	return 0;
 +}
 +
 +static int _sched_setscheduler(struct task_struct *p, int policy,
 +			       const struct sched_param *param, bool check)
 +{
 +	struct sched_attr attr = {
 +		.sched_policy   = policy,
 +		.sched_priority = param->sched_priority,
 +		.sched_nice	= PRIO_TO_NICE(p->static_prio),
 +	};
 +
 +	/*
 +	 * Fixup the legacy SCHED_RESET_ON_FORK hack, except if
 +	 * the policy=-1 was passed by sched_setparam().
 +	 */
 +	if ((policy != -1) && (policy & SCHED_RESET_ON_FORK)) {
 +		attr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;
 +		policy &= ~SCHED_RESET_ON_FORK;
 +		attr.sched_policy = policy;
 +	}
 +
 +	return __sched_setscheduler(p, &attr, check);
 +}
 +/**
 + * sched_setscheduler - change the scheduling policy and/or RT priority of a thread.
 + * @p: the task in question.
 + * @policy: new policy.
 + * @param: structure containing the new RT priority.
 + *
 + * Return: 0 on success. An error code otherwise.
 + *
 + * NOTE that the task may be already dead.
 + */
 +int sched_setscheduler(struct task_struct *p, int policy,
 +		       const struct sched_param *param)
 +{
 +	return _sched_setscheduler(p, policy, param, true);
 +}
 +EXPORT_SYMBOL_GPL(sched_setscheduler);
 +
 +int sched_setattr(struct task_struct *p, const struct sched_attr *attr)
 +{
 +	return __sched_setscheduler(p, attr, true);
 +}
 +EXPORT_SYMBOL_GPL(sched_setattr);
 +
 +/**
 + * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.
 + * @p: the task in question.
 + * @policy: new policy.
 + * @param: structure containing the new RT priority.
 + *
 + * Just like sched_setscheduler, only don't bother checking if the
 + * current context has permission.  For example, this is needed in
 + * stop_machine(): we create temporary high priority worker threads,
 + * but our caller might not have that capability.
 + *
 + * Return: 0 on success. An error code otherwise.
 + */
 +int sched_setscheduler_nocheck(struct task_struct *p, int policy,
 +			       const struct sched_param *param)
 +{
 +	return _sched_setscheduler(p, policy, param, false);
 +}
 +EXPORT_SYMBOL_GPL(sched_setscheduler_nocheck);
 +
 +static int
 +do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
 +{
 +	struct sched_param lparam;
 +	struct task_struct *p;
 +	int retval;
 +
 +	if (!param || pid < 0)
 +		return -EINVAL;
 +	if (copy_from_user(&lparam, param, sizeof(struct sched_param)))
 +		return -EFAULT;
 +
 +	rcu_read_lock();
 +	retval = -ESRCH;
 +	p = find_process_by_pid(pid);
 +	if (p != NULL)
 +		retval = sched_setscheduler(p, policy, &lparam);
 +	rcu_read_unlock();
 +
 +	return retval;
 +}
 +
 +/*
 + * Mimics kernel/events/core.c perf_copy_attr().
 + */
 +static int sched_copy_attr(struct sched_attr __user *uattr,
 +			   struct sched_attr *attr)
 +{
 +	u32 size;
 +	int ret;
 +
 +	if (!access_ok(VERIFY_WRITE, uattr, SCHED_ATTR_SIZE_VER0))
 +		return -EFAULT;
 +
 +	/*
 +	 * zero the full structure, so that a short copy will be nice.
 +	 */
 +	memset(attr, 0, sizeof(*attr));
 +
 +	ret = get_user(size, &uattr->size);
 +	if (ret)
 +		return ret;
 +
 +	if (size > PAGE_SIZE)	/* silly large */
 +		goto err_size;
 +
 +	if (!size)		/* abi compat */
 +		size = SCHED_ATTR_SIZE_VER0;
 +
 +	if (size < SCHED_ATTR_SIZE_VER0)
 +		goto err_size;
 +
 +	/*
 +	 * If we're handed a bigger struct than we know of,
 +	 * ensure all the unknown bits are 0 - i.e. new
 +	 * user-space does not rely on any kernel feature
 +	 * extensions we dont know about yet.
 +	 */
 +	if (size > sizeof(*attr)) {
 +		unsigned char __user *addr;
 +		unsigned char __user *end;
 +		unsigned char val;
 +
 +		addr = (void __user *)uattr + sizeof(*attr);
 +		end  = (void __user *)uattr + size;
 +
 +		for (; addr < end; addr++) {
 +			ret = get_user(val, addr);
 +			if (ret)
 +				return ret;
 +			if (val)
 +				goto err_size;
 +		}
 +		size = sizeof(*attr);
 +	}
 +
 +	ret = copy_from_user(attr, uattr, size);
 +	if (ret)
 +		return -EFAULT;
 +
 +	/*
 +	 * XXX: do we want to be lenient like existing syscalls; or do we want
 +	 * to be strict and return an error on out-of-bounds values?
 +	 */
 +	attr->sched_nice = clamp(attr->sched_nice, -20, 19);
 +
 +out:
 +	return ret;
 +
 +err_size:
 +	put_user(sizeof(*attr), &uattr->size);
 +	ret = -E2BIG;
 +	goto out;
 +}
 +
 +/**
 + * sys_sched_setscheduler - set/change the scheduler policy and RT priority
 + * @pid: the pid in question.
 + * @policy: new policy.
 + * @param: structure containing the new RT priority.
 + *
 + * Return: 0 on success. An error code otherwise.
 + */
 +SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,
 +		struct sched_param __user *, param)
 +{
 +	/* negative values for policy are not valid */
 +	if (policy < 0)
 +		return -EINVAL;
 +
 +	return do_sched_setscheduler(pid, policy, param);
 +}
 +
 +/**
 + * sys_sched_setparam - set/change the RT priority of a thread
 + * @pid: the pid in question.
 + * @param: structure containing the new RT priority.
 + *
 + * Return: 0 on success. An error code otherwise.
 + */
 +SYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)
 +{
 +	return do_sched_setscheduler(pid, -1, param);
 +}
 +
 +static const bool sched_deadline_enable;
 +
 +/**
 + * sys_sched_setattr - same as above, but with extended sched_attr
 + * @pid: the pid in question.
 + * @uattr: structure containing the extended parameters.
 + * @flags: for future extension.
 + */
 +SYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr,
 +			       unsigned int, flags)
 +{
 +	struct sched_attr attr;
 +	struct task_struct *p;
 +	int retval;
 +
 +	if (!sched_deadline_enable)
 +		return -ENOSYS;
 +
 +	if (!uattr || pid < 0 || flags)
 +		return -EINVAL;
 +
 +	retval = sched_copy_attr(uattr, &attr);
 +	if (retval)
 +		return retval;
 +
 +	if ((int)attr.sched_policy < 0)
 +		return -EINVAL;
 +
 +	rcu_read_lock();
 +	retval = -ESRCH;
 +	p = find_process_by_pid(pid);
 +	if (p != NULL)
 +		retval = sched_setattr(p, &attr);
 +	rcu_read_unlock();
 +
 +	return retval;
 +}
 +
 +/**
 + * sys_sched_getscheduler - get the policy (scheduling class) of a thread
 + * @pid: the pid in question.
 + *
 + * Return: On success, the policy of the thread. Otherwise, a negative error
 + * code.
 + */
 +SYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)
 +{
 +	struct task_struct *p;
 +	int retval;
 +
 +	if (pid < 0)
 +		return -EINVAL;
 +
 +	retval = -ESRCH;
 +	rcu_read_lock();
 +	p = find_process_by_pid(pid);
 +	if (p) {
 +		retval = security_task_getscheduler(p);
 +		if (!retval)
 +			retval = p->policy
 +				| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);
 +	}
 +	rcu_read_unlock();
 +	return retval;
 +}
 +
 +/**
 + * sys_sched_getparam - get the RT priority of a thread
 + * @pid: the pid in question.
 + * @param: structure containing the RT priority.
 + *
 + * Return: On success, 0 and the RT priority is in @param. Otherwise, an error
 + * code.
 + */
 +SYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)
 +{
 +	struct sched_param lp;
 +	struct task_struct *p;
 +	int retval;
 +
 +	if (!param || pid < 0)
 +		return -EINVAL;
 +
 +	rcu_read_lock();
 +	p = find_process_by_pid(pid);
 +	retval = -ESRCH;
 +	if (!p)
 +		goto out_unlock;
 +
 +	retval = security_task_getscheduler(p);
 +	if (retval)
 +		goto out_unlock;
 +
 +	if (task_has_dl_policy(p)) {
 +		retval = -EINVAL;
 +		goto out_unlock;
 +	}
 +	lp.sched_priority = p->rt_priority;
 +	rcu_read_unlock();
 +
 +	/*
 +	 * This one might sleep, we cannot do it with a spinlock held ...
 +	 */
 +	retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
 +
 +	return retval;
 +
 +out_unlock:
 +	rcu_read_unlock();
 +	return retval;
 +}
 +
 +static int sched_read_attr(struct sched_attr __user *uattr,
 +			   struct sched_attr *attr,
 +			   unsigned int usize)
 +{
 +	int ret;
 +
 +	if (!access_ok(VERIFY_WRITE, uattr, usize))
 +		return -EFAULT;
 +
 +	/*
 +	 * If we're handed a smaller struct than we know of,
 +	 * ensure all the unknown bits are 0 - i.e. old
 +	 * user-space does not get uncomplete information.
 +	 */
 +	if (usize < sizeof(*attr)) {
 +		unsigned char *addr;
 +		unsigned char *end;
 +
 +		addr = (void *)attr + usize;
 +		end  = (void *)attr + sizeof(*attr);
 +
 +		for (; addr < end; addr++) {
 +			if (*addr)
 +				goto err_size;
 +		}
 +
 +		attr->size = usize;
 +	}
 +
 +	ret = copy_to_user(uattr, attr, attr->size);
 +	if (ret)
 +		return -EFAULT;
 +
 +out:
 +	return ret;
 +
 +err_size:
 +	ret = -E2BIG;
 +	goto out;
 +}
 +
 +/**
 + * sys_sched_getattr - similar to sched_getparam, but with sched_attr
 + * @pid: the pid in question.
 + * @uattr: structure containing the extended parameters.
 + * @size: sizeof(attr) for fwd/bwd comp.
 + * @flags: for future extension.
 + */
 +SYSCALL_DEFINE4(sched_getattr, pid_t, pid, struct sched_attr __user *, uattr,
 +		unsigned int, size, unsigned int, flags)
 +{
 +	struct sched_attr attr = {
 +		.size = sizeof(struct sched_attr),
 +	};
 +	struct task_struct *p;
 +	int retval;
 +
 +	if (!sched_deadline_enable)
 +		return -ENOSYS;
 +
 +	if (!uattr || pid < 0 || size > PAGE_SIZE ||
 +	    size < SCHED_ATTR_SIZE_VER0 || flags)
 +		return -EINVAL;
 +
 +	rcu_read_lock();
 +	p = find_process_by_pid(pid);
 +	retval = -ESRCH;
 +	if (!p)
 +		goto out_unlock;
 +
 +	retval = security_task_getscheduler(p);
 +	if (retval)
 +		goto out_unlock;
 +
 +	attr.sched_policy = p->policy;
 +	if (p->sched_reset_on_fork)
 +		attr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;
 +	if (task_has_dl_policy(p))
 +		__getparam_dl(p, &attr);
 +	else if (task_has_rt_policy(p))
 +		attr.sched_priority = p->rt_priority;
 +	else
 +		attr.sched_nice = TASK_NICE(p);
 +
 +	rcu_read_unlock();
 +
 +	retval = sched_read_attr(uattr, &attr, size);
 +	return retval;
 +
 +out_unlock:
 +	rcu_read_unlock();
 +	return retval;
 +}
 +
 +long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 +{
 +	cpumask_var_t cpus_allowed, new_mask;
 +	struct task_struct *p;
 +	int retval;
 +
 +	rcu_read_lock();
 +
 +	p = find_process_by_pid(pid);
 +	if (!p) {
 +		rcu_read_unlock();
 +		return -ESRCH;
 +	}
 +
 +	/* Prevent p going away */
 +	get_task_struct(p);
 +	rcu_read_unlock();
 +
 +	if (p->flags & PF_NO_SETAFFINITY) {
 +		retval = -EINVAL;
 +		goto out_put_task;
 +	}
 +	if (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) {
 +		retval = -ENOMEM;
 +		goto out_put_task;
 +	}
 +	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {
 +		retval = -ENOMEM;
 +		goto out_free_cpus_allowed;
 +	}
 +	retval = -EPERM;
 +	if (!check_same_owner(p)) {
 +		rcu_read_lock();
 +		if (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE)) {
 +			rcu_read_unlock();
 +			goto out_unlock;
 +		}
 +		rcu_read_unlock();
 +	}
 +
 +	retval = security_task_setscheduler(p);
 +	if (retval)
 +		goto out_unlock;
 +
 +
 +	cpuset_cpus_allowed(p, cpus_allowed);
 +	cpumask_and(new_mask, in_mask, cpus_allowed);
 +
 +	/*
 +	 * Since bandwidth control happens on root_domain basis,
 +	 * if admission test is enabled, we only admit -deadline
 +	 * tasks allowed to run on all the CPUs in the task's
 +	 * root_domain.
 +	 */
 +#ifdef CONFIG_SMP
 +	if (task_has_dl_policy(p)) {
 +		const struct cpumask *span = task_rq(p)->rd->span;
 +
 +		if (dl_bandwidth_enabled() && !cpumask_subset(span, new_mask)) {
 +			retval = -EBUSY;
 +			goto out_unlock;
 +		}
 +	}
 +#endif
 +again:
 +	retval = set_cpus_allowed_ptr(p, new_mask);
 +
 +	if (!retval) {
 +		cpuset_cpus_allowed(p, cpus_allowed);
 +		if (!cpumask_subset(new_mask, cpus_allowed)) {
 +			/*
 +			 * We must have raced with a concurrent cpuset
 +			 * update. Just reset the cpus_allowed to the
 +			 * cpuset's cpus_allowed
 +			 */
 +			cpumask_copy(new_mask, cpus_allowed);
 +			goto again;
 +		}
 +	}
 +out_unlock:
 +	free_cpumask_var(new_mask);
 +out_free_cpus_allowed:
 +	free_cpumask_var(cpus_allowed);
 +out_put_task:
 +	put_task_struct(p);
 +	return retval;
 +}
 +
 +static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
 +			     struct cpumask *new_mask)
 +{
 +	if (len < cpumask_size())
 +		cpumask_clear(new_mask);
 +	else if (len > cpumask_size())
 +		len = cpumask_size();
 +
 +	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
 +}
 +
 +/**
 + * sys_sched_setaffinity - set the cpu affinity of a process
 + * @pid: pid of the process
 + * @len: length in bytes of the bitmask pointed to by user_mask_ptr
 + * @user_mask_ptr: user-space pointer to the new cpu mask
 + *
 + * Return: 0 on success. An error code otherwise.
 + */
 +SYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,
 +		unsigned long __user *, user_mask_ptr)
 +{
 +	cpumask_var_t new_mask;
 +	int retval;
 +
 +	if (!alloc_cpumask_var(&new_mask, GFP_KERNEL))
 +		return -ENOMEM;
 +
 +	retval = get_user_cpu_mask(user_mask_ptr, len, new_mask);
 +	if (retval == 0)
 +		retval = sched_setaffinity(pid, new_mask);
 +	free_cpumask_var(new_mask);
 +	return retval;
 +}
 +
 +long sched_getaffinity(pid_t pid, struct cpumask *mask)
 +{
 +	struct task_struct *p;
 +	unsigned long flags;
 +	int retval;
 +
 +	rcu_read_lock();
 +
 +	retval = -ESRCH;
 +	p = find_process_by_pid(pid);
 +	if (!p)
 +		goto out_unlock;
 +
 +	retval = security_task_getscheduler(p);
 +	if (retval)
 +		goto out_unlock;
 +
 +	raw_spin_lock_irqsave(&p->pi_lock, flags);
 +	cpumask_and(mask, &p->cpus_allowed, cpu_active_mask);
 +	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 +
 +out_unlock:
 +	rcu_read_unlock();
 +
 +	return retval;
 +}
 +
 +/**
 + * sys_sched_getaffinity - get the cpu affinity of a process
 + * @pid: pid of the process
 + * @len: length in bytes of the bitmask pointed to by user_mask_ptr
 + * @user_mask_ptr: user-space pointer to hold the current cpu mask
 + *
 + * Return: 0 on success. An error code otherwise.
 + */
 +SYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,
 +		unsigned long __user *, user_mask_ptr)
 +{
 +	int ret;
 +	cpumask_var_t mask;
 +
 +	if ((len * BITS_PER_BYTE) < nr_cpu_ids)
 +		return -EINVAL;
 +	if (len & (sizeof(unsigned long)-1))
 +		return -EINVAL;
 +
 +	if (!alloc_cpumask_var(&mask, GFP_KERNEL))
 +		return -ENOMEM;
 +
 +	ret = sched_getaffinity(pid, mask);
 +	if (ret == 0) {
 +		size_t retlen = min_t(size_t, len, cpumask_size());
 +
 +		if (copy_to_user(user_mask_ptr, mask, retlen))
 +			ret = -EFAULT;
 +		else
 +			ret = retlen;
 +	}
 +	free_cpumask_var(mask);
 +
 +	return ret;
 +}
 +
 +/**
 + * sys_sched_yield - yield the current processor to other threads.
 + *
 + * This function yields the current CPU to other tasks. If there are no
 + * other threads running on this CPU then this function will return.
 + *
 + * Return: 0.
 + */
 +SYSCALL_DEFINE0(sched_yield)
 +{
 +	struct rq *rq = this_rq_lock();
 +
 +	schedstat_inc(rq, yld_count);
 +	current->sched_class->yield_task(rq);
 +
 +	/*
 +	 * Since we are going to call schedule() anyway, there's
 +	 * no need to preempt or enable interrupts:
 +	 */
 +	__release(rq->lock);
 +	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
 +	do_raw_spin_unlock(&rq->lock);
 +	sched_preempt_enable_no_resched();
 +
 +	schedule();
 +
 +	return 0;
 +}
 +
 +static inline int should_resched(void)
 +{
 +	return need_resched() && !(preempt_count() & PREEMPT_ACTIVE);
 +}
 +
 +static void __cond_resched(void)
 +{
 +	add_preempt_count(PREEMPT_ACTIVE);
 +	__schedule();
 +	sub_preempt_count(PREEMPT_ACTIVE);
 +}
 +
 +int __sched _cond_resched(void)
 +{
 +	if (should_resched()) {
 +		__cond_resched();
 +		return 1;
 +	}
 +	return 0;
 +}
 +EXPORT_SYMBOL(_cond_resched);
 +
 +/*
 + * __cond_resched_lock() - if a reschedule is pending, drop the given lock,
 + * call schedule, and on return reacquire the lock.
 + *
 + * This works OK both with and without CONFIG_PREEMPT. We do strange low-level
 + * operations here to prevent schedule() from being called twice (once via
 + * spin_unlock(), once by hand).
 + */
 +int __cond_resched_lock(spinlock_t *lock)
 +{
 +	int resched = should_resched();
 +	int ret = 0;
 +
 +	lockdep_assert_held(lock);
 +
 +	if (spin_needbreak(lock) || resched) {
 +		spin_unlock(lock);
 +		if (resched)
 +			__cond_resched();
 +		else
 +			cpu_relax();
 +		ret = 1;
 +		spin_lock(lock);
 +	}
 +	return ret;
 +}
 +EXPORT_SYMBOL(__cond_resched_lock);
 +
 +int __sched __cond_resched_softirq(void)
 +{
 +	BUG_ON(!in_softirq());
 +
 +	if (should_resched()) {
 +		local_bh_enable();
 +		__cond_resched();
 +		local_bh_disable();
 +		return 1;
 +	}
 +	return 0;
 +}
 +EXPORT_SYMBOL(__cond_resched_softirq);
 +
 +/**
 + * yield - yield the current processor to other threads.
 + *
 + * Do not ever use this function, there's a 99% chance you're doing it wrong.
 + *
 + * The scheduler is at all times free to pick the calling task as the most
 + * eligible task to run, if removing the yield() call from your code breaks
 + * it, its already broken.
 + *
 + * Typical broken usage is:
 + *
 + * while (!event)
 + * 	yield();
 + *
 + * where one assumes that yield() will let 'the other' process run that will
 + * make event true. If the current task is a SCHED_FIFO task that will never
 + * happen. Never use yield() as a progress guarantee!!
 + *
 + * If you want to use yield() to wait for something, use wait_event().
 + * If you want to use yield() to be 'nice' for others, use cond_resched().
 + * If you still want to use yield(), do not!
 + */
 +void __sched yield(void)
 +{
 +	set_current_state(TASK_RUNNING);
 +	sys_sched_yield();
 +}
 +EXPORT_SYMBOL(yield);
 +
 +/**
 + * yield_to - yield the current processor to another thread in
 + * your thread group, or accelerate that thread toward the
 + * processor it's on.
 + * @p: target task
 + * @preempt: whether task preemption is allowed or not
 + *
 + * It's the caller's job to ensure that the target task struct
 + * can't go away on us before we can do any checks.
 + *
 + * Return:
 + *	true (>0) if we indeed boosted the target task.
 + *	false (0) if we failed to boost the target.
 + *	-ESRCH if there's no task to yield to.
 + */
 +int __sched yield_to(struct task_struct *p, bool preempt)
 +{
 +	struct task_struct *curr = current;
 +	struct rq *rq, *p_rq;
 +	unsigned long flags;
 +	int yielded = 0;
 +
 +	local_irq_save(flags);
 +	rq = this_rq();
 +
 +again:
 +	p_rq = task_rq(p);
 +	/*
 +	 * If we're the only runnable task on the rq and target rq also
 +	 * has only one task, there's absolutely no point in yielding.
 +	 */
 +	if (rq->nr_running == 1 && p_rq->nr_running == 1) {
 +		yielded = -ESRCH;
 +		goto out_irq;
 +	}
 +
 +	double_rq_lock(rq, p_rq);
 +	if (task_rq(p) != p_rq) {
 +		double_rq_unlock(rq, p_rq);
 +		goto again;
 +	}
 +
 +	if (!curr->sched_class->yield_to_task)
 +		goto out_unlock;
 +
 +	if (curr->sched_class != p->sched_class)
 +		goto out_unlock;
 +
 +	if (task_running(p_rq, p) || p->state)
 +		goto out_unlock;
 +
 +	yielded = curr->sched_class->yield_to_task(rq, p, preempt);
 +	if (yielded) {
 +		schedstat_inc(rq, yld_count);
 +		/*
 +		 * Make p's CPU reschedule; pick_next_entity takes care of
 +		 * fairness.
 +		 */
 +		if (preempt && rq != p_rq)
 +			resched_curr(p_rq);
 +	}
 +
 +out_unlock:
 +	double_rq_unlock(rq, p_rq);
 +out_irq:
 +	local_irq_restore(flags);
 +
 +	if (yielded > 0)
 +		schedule();
 +
 +	return yielded;
 +}
 +EXPORT_SYMBOL_GPL(yield_to);
 +
 +/*
 + * This task is about to go to sleep on IO. Increment rq->nr_iowait so
 + * that process accounting knows that this is a task in IO wait state.
 + */
 +void __sched io_schedule(void)
 +{
 +	io_schedule_timeout(MAX_SCHEDULE_TIMEOUT);
 +}
 +EXPORT_SYMBOL(io_schedule);
 +
 +long __sched io_schedule_timeout(long timeout)
 +{
 +	int old_iowait = current->in_iowait;
 +	struct rq *rq;
 +	long ret;
 +
 +	current->in_iowait = 1;
 +	if (old_iowait)
 +		blk_schedule_flush_plug(current);
 +	else
 +		blk_flush_plug(current);
 +
 +	delayacct_blkio_start();
 +	rq = raw_rq();
 +	atomic_inc(&rq->nr_iowait);
 +	ret = schedule_timeout(timeout);
 +	current->in_iowait = old_iowait;
 +	atomic_dec(&rq->nr_iowait);
 +	delayacct_blkio_end();
 +
 +	return ret;
 +}
 +EXPORT_SYMBOL(io_schedule_timeout);
 +
 +/**
 + * sys_sched_get_priority_max - return maximum RT priority.
 + * @policy: scheduling class.
 + *
 + * Return: On success, this syscall returns the maximum
 + * rt_priority that can be used by a given scheduling class.
 + * On failure, a negative error code is returned.
 + */
 +SYSCALL_DEFINE1(sched_get_priority_max, int, policy)
 +{
 +	int ret = -EINVAL;
 +
 +	switch (policy) {
 +	case SCHED_FIFO:
 +	case SCHED_RR:
 +		ret = MAX_USER_RT_PRIO-1;
 +		break;
 +	case SCHED_DEADLINE:
 +	case SCHED_NORMAL:
 +	case SCHED_BATCH:
 +	case SCHED_IDLE:
 +		ret = 0;
 +		break;
 +	}
 +	return ret;
 +}
 +
 +/**
 + * sys_sched_get_priority_min - return minimum RT priority.
 + * @policy: scheduling class.
 + *
 + * Return: On success, this syscall returns the minimum
 + * rt_priority that can be used by a given scheduling class.
 + * On failure, a negative error code is returned.
 + */
 +SYSCALL_DEFINE1(sched_get_priority_min, int, policy)
 +{
 +	int ret = -EINVAL;
 +
 +	switch (policy) {
 +	case SCHED_FIFO:
 +	case SCHED_RR:
 +		ret = 1;
 +		break;
 +	case SCHED_DEADLINE:
 +	case SCHED_NORMAL:
 +	case SCHED_BATCH:
 +	case SCHED_IDLE:
 +		ret = 0;
 +	}
 +	return ret;
 +}
 +
 +/**
 + * sys_sched_rr_get_interval - return the default timeslice of a process.
 + * @pid: pid of the process.
 + * @interval: userspace pointer to the timeslice value.
 + *
 + * this syscall writes the default timeslice value of a given process
 + * into the user-space timespec buffer. A value of '0' means infinity.
 + *
 + * Return: On success, 0 and the timeslice is in @interval. Otherwise,
 + * an error code.
 + */
 +SYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,
 +		struct timespec __user *, interval)
 +{
 +	struct task_struct *p;
 +	unsigned int time_slice;
 +	unsigned long flags;
 +	struct rq *rq;
 +	int retval;
 +	struct timespec t;
 +
 +	if (pid < 0)
 +		return -EINVAL;
 +
 +	retval = -ESRCH;
 +	rcu_read_lock();
 +	p = find_process_by_pid(pid);
 +	if (!p)
 +		goto out_unlock;
 +
 +	retval = security_task_getscheduler(p);
 +	if (retval)
 +		goto out_unlock;
 +
 +	rq = task_rq_lock(p, &flags);
 +	time_slice = p->sched_class->get_rr_interval(rq, p);
 +	task_rq_unlock(rq, p, &flags);
 +
 +	rcu_read_unlock();
 +	jiffies_to_timespec(time_slice, &t);
 +	retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;
 +	return retval;
 +
 +out_unlock:
 +	rcu_read_unlock();
 +	return retval;
 +}
 +
 +static const char stat_nam[] = TASK_STATE_TO_CHAR_STR;
 +
 +void sched_show_task(struct task_struct *p)
 +{
 +	unsigned long free = 0;
 +	int ppid;
 +	unsigned state;
 +
 +	state = p->state ? __ffs(p->state) + 1 : 0;
 +	printk(KERN_INFO "%-15.15s %c", p->comm,
 +		state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');
 +#if BITS_PER_LONG == 32
 +	if (state == TASK_RUNNING)
 +		printk(KERN_CONT " running  ");
 +	else
 +		printk(KERN_CONT " %08lx ", thread_saved_pc(p));
 +#else
 +	if (state == TASK_RUNNING)
 +		printk(KERN_CONT "  running task    ");
 +	else
 +		printk(KERN_CONT " %016lx ", thread_saved_pc(p));
 +#endif
 +#ifdef CONFIG_DEBUG_STACK_USAGE
 +	free = stack_not_used(p);
 +#endif
 +	rcu_read_lock();
 +	ppid = task_pid_nr(rcu_dereference(p->real_parent));
 +	rcu_read_unlock();
 +	printk(KERN_CONT "%5lu %5d %6d 0x%08lx\n", free,
 +		task_pid_nr(p), ppid,
 +		(unsigned long)task_thread_info(p)->flags);
 +
 +	print_worker_info(KERN_INFO, p);
 +	show_stack(p, NULL);
 +}
 +
 +void show_state_filter(unsigned long state_filter)
 +{
 +	struct task_struct *g, *p;
 +
 +#if BITS_PER_LONG == 32
 +	printk(KERN_INFO
 +		"  task                PC stack   pid father\n");
 +#else
 +	printk(KERN_INFO
 +		"  task                        PC stack   pid father\n");
 +#endif
 +	rcu_read_lock();
 +	do_each_thread(g, p) {
 +		/*
 +		 * reset the NMI-timeout, listing all files on a slow
 +		 * console might take a lot of time:
 +		 */
 +		touch_nmi_watchdog();
 +		if (!state_filter || (p->state & state_filter))
 +			sched_show_task(p);
 +	} while_each_thread(g, p);
 +
 +	touch_all_softlockup_watchdogs();
 +
 +#ifdef CONFIG_SCHED_DEBUG
 +	sysrq_sched_debug_show();
 +#endif
 +	rcu_read_unlock();
 +	/*
 +	 * Only show locks if all tasks are dumped:
 +	 */
 +	if (!state_filter)
 +		debug_show_all_locks();
 +}
 +
 +void init_idle_bootup_task(struct task_struct *idle)
 +{
 +	idle->sched_class = &idle_sched_class;
 +}
 +
 +/**
 + * init_idle - set up an idle thread for a given CPU
 + * @idle: task in question
 + * @cpu: cpu the idle task belongs to
 + *
 + * NOTE: this function does not set the idle thread's NEED_RESCHED
 + * flag, to make booting more robust.
 + */
 +void init_idle(struct task_struct *idle, int cpu)
 +{
 +	struct rq *rq = cpu_rq(cpu);
 +	unsigned long flags;
 +
 +	raw_spin_lock_irqsave(&rq->lock, flags);
 +
 +	__sched_fork(0, idle);
 +	idle->state = TASK_RUNNING;
 +	idle->se.exec_start = sched_clock();
 +
 +	do_set_cpus_allowed(idle, cpumask_of(cpu));
 +	/*
 +	 * We're having a chicken and egg problem, even though we are
 +	 * holding rq->lock, the cpu isn't yet set to this cpu so the
 +	 * lockdep check in task_group() will fail.
 +	 *
 +	 * Similar case to sched_fork(). / Alternatively we could
 +	 * use task_rq_lock() here and obtain the other rq->lock.
 +	 *
 +	 * Silence PROVE_RCU
 +	 */
 +	rcu_read_lock();
 +	__set_task_cpu(idle, cpu);
 +	rcu_read_unlock();
 +
 +	rq->curr = rq->idle = idle;
 +#if defined(CONFIG_SMP)
 +	idle->on_cpu = 1;
 +#endif
 +	raw_spin_unlock_irqrestore(&rq->lock, flags);
 +
 +	/* Set the preempt count _outside_ the spinlocks! */
 +	task_thread_info(idle)->preempt_count = 0;
 +
 +	/*
 +	 * The idle tasks have their own, simple scheduling class:
 +	 */
 +	idle->sched_class = &idle_sched_class;
 +	ftrace_graph_init_idle_task(idle, cpu);
 +	vtime_init_idle(idle, cpu);
 +#if defined(CONFIG_SMP)
 +	sprintf(idle->comm, "%s/%d", INIT_TASK_COMM, cpu);
 +#endif
 +}
 +
 +int cpuset_cpumask_can_shrink(const struct cpumask *cur,
 +			      const struct cpumask *trial)
 +{
 +	int ret = 1, trial_cpus;
 +	struct dl_bw *cur_dl_b;
 +	unsigned long flags;
 +
 +	if (!cpumask_weight(cur))
 +		return ret;
 +
 +	rcu_read_lock_sched();
 +	cur_dl_b = dl_bw_of(cpumask_any(cur));
 +	trial_cpus = cpumask_weight(trial);
 +
 +	raw_spin_lock_irqsave(&cur_dl_b->lock, flags);
 +	if (cur_dl_b->bw != -1 &&
 +	    cur_dl_b->bw * trial_cpus < cur_dl_b->total_bw)
 +		ret = 0;
 +	raw_spin_unlock_irqrestore(&cur_dl_b->lock, flags);
 +	rcu_read_unlock_sched();
 +
 +	return ret;
 +}
 +
 +int task_can_attach(struct task_struct *p,
 +		    const struct cpumask *cs_cpus_allowed)
 +{
 +	int ret = 0;
 +
 +	/*
 +	 * Kthreads which disallow setaffinity shouldn't be moved
 +	 * to a new cpuset; we don't want to change their cpu
 +	 * affinity and isolating such threads by their set of
 +	 * allowed nodes is unnecessary.  Thus, cpusets are not
 +	 * applicable for such threads.  This prevents checking for
 +	 * success of set_cpus_allowed_ptr() on all attached tasks
 +	 * before cpus_allowed may be changed.
 +	 */
 +	if (p->flags & PF_NO_SETAFFINITY) {
 +		ret = -EINVAL;
 +		goto out;
 +	}
 +
 +#ifdef CONFIG_SMP
 +	if (dl_task(p) && !cpumask_intersects(task_rq(p)->rd->span,
 +					      cs_cpus_allowed)) {
 +		unsigned int dest_cpu = cpumask_any_and(cpu_active_mask,
 +							cs_cpus_allowed);
 +		struct dl_bw *dl_b;
 +		bool overflow;
 +		int cpus;
 +		unsigned long flags;
 +
 +		rcu_read_lock_sched();
 +		dl_b = dl_bw_of(dest_cpu);
 +		raw_spin_lock_irqsave(&dl_b->lock, flags);
 +		cpus = dl_bw_cpus(dest_cpu);
 +		overflow = __dl_overflow(dl_b, cpus, 0, p->dl.dl_bw);
 +		if (overflow)
 +			ret = -EBUSY;
 +		else {
 +			/*
 +			 * We reserve space for this task in the destination
 +			 * root_domain, as we can't fail after this point.
 +			 * We will free resources in the source root_domain
 +			 * later on (see set_cpus_allowed_dl()).
 +			 */
- 			__dl_add(dl_b, p->dl.dl_bw);
++			__dl_add(dl_b, p->dl.dl_bw, cpus);
 +		}
 +		raw_spin_unlock_irqrestore(&dl_b->lock, flags);
 +		rcu_read_unlock_sched();
  
 -#ifdef CONFIG_SMP
 -	if (!llist_empty(&rq->wake_list))
 -		return 0;
 +	}
  #endif
 -
 -	return 1;
 +out:
 +	return ret;
  }
  
 -/**
 - * idle_task - return the idle task for a given CPU.
 - * @cpu: the processor in question.
 - *
 - * Return: The idle task for the CPU @cpu.
 - */
 -struct task_struct *idle_task(int cpu)
 +#ifdef CONFIG_SMP
 +void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
  {
 -	return cpu_rq(cpu)->idle;
 -}
 +	if (p->sched_class && p->sched_class->set_cpus_allowed)
 +		p->sched_class->set_cpus_allowed(p, new_mask);
  
 -/**
 - * find_process_by_pid - find a process with a matching PID value.
 - * @pid: the pid in question.
 - *
 - * The task of @pid, if found. %NULL otherwise.
 - */
 -static struct task_struct *find_process_by_pid(pid_t pid)
 -{
 -	return pid ? find_task_by_vpid(pid) : current;
 +	cpumask_copy(&p->cpus_allowed, new_mask);
 +	p->nr_cpus_allowed = cpumask_weight(new_mask);
  }
  
  /*
@@@ -9104,6 -6760,19 +9123,22 @@@ static int sched_dl_global_validate(voi
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ void init_dl_rq_bw_ratio(struct dl_rq *dl_rq)
+ {
+ 	if (global_rt_runtime() == RUNTIME_INF) {
+ 		dl_rq->bw_ratio = 1 << RATIO_SHIFT;
+ 		dl_rq->extra_bw = 1 << BW_SHIFT;
+ 	} else {
+ 		dl_rq->bw_ratio = to_ratio(global_rt_runtime(),
+ 			  global_rt_period()) >> (BW_SHIFT - RATIO_SHIFT);
+ 		dl_rq->extra_bw = to_ratio(global_rt_period(),
+ 						    global_rt_runtime());
+ 	}
+ }
+ 
++>>>>>>> daec57983670 (sched/deadline: Reclaim bandwidth not used by dl tasks)
  static void sched_dl_do_global(void)
  {
  	u64 new_bw = -1;
diff --cc kernel/sched/deadline.c
index fc0d2f428765,e3b25dfb74f3..000000000000
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@@ -65,6 -66,199 +65,202 @@@ void sub_running_bw(u64 dl_bw, struct d
  		dl_rq->running_bw = 0;
  }
  
++<<<<<<< HEAD
++=======
+ static inline
+ void add_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)
+ {
+ 	u64 old = dl_rq->this_bw;
+ 
+ 	lockdep_assert_held(&(rq_of_dl_rq(dl_rq))->lock);
+ 	dl_rq->this_bw += dl_bw;
+ 	SCHED_WARN_ON(dl_rq->this_bw < old); /* overflow */
+ }
+ 
+ static inline
+ void sub_rq_bw(u64 dl_bw, struct dl_rq *dl_rq)
+ {
+ 	u64 old = dl_rq->this_bw;
+ 
+ 	lockdep_assert_held(&(rq_of_dl_rq(dl_rq))->lock);
+ 	dl_rq->this_bw -= dl_bw;
+ 	SCHED_WARN_ON(dl_rq->this_bw > old); /* underflow */
+ 	if (dl_rq->this_bw > old)
+ 		dl_rq->this_bw = 0;
+ 	SCHED_WARN_ON(dl_rq->running_bw > dl_rq->this_bw);
+ }
+ 
+ void dl_change_utilization(struct task_struct *p, u64 new_bw)
+ {
+ 	struct rq *rq;
+ 
+ 	if (task_on_rq_queued(p))
+ 		return;
+ 
+ 	rq = task_rq(p);
+ 	if (p->dl.dl_non_contending) {
+ 		sub_running_bw(p->dl.dl_bw, &rq->dl);
+ 		p->dl.dl_non_contending = 0;
+ 		/*
+ 		 * If the timer handler is currently running and the
+ 		 * timer cannot be cancelled, inactive_task_timer()
+ 		 * will see that dl_not_contending is not set, and
+ 		 * will not touch the rq's active utilization,
+ 		 * so we are still safe.
+ 		 */
+ 		if (hrtimer_try_to_cancel(&p->dl.inactive_timer) == 1)
+ 			put_task_struct(p);
+ 	}
+ 	sub_rq_bw(p->dl.dl_bw, &rq->dl);
+ 	add_rq_bw(new_bw, &rq->dl);
+ }
+ 
+ /*
+  * The utilization of a task cannot be immediately removed from
+  * the rq active utilization (running_bw) when the task blocks.
+  * Instead, we have to wait for the so called "0-lag time".
+  *
+  * If a task blocks before the "0-lag time", a timer (the inactive
+  * timer) is armed, and running_bw is decreased when the timer
+  * fires.
+  *
+  * If the task wakes up again before the inactive timer fires,
+  * the timer is cancelled, whereas if the task wakes up after the
+  * inactive timer fired (and running_bw has been decreased) the
+  * task's utilization has to be added to running_bw again.
+  * A flag in the deadline scheduling entity (dl_non_contending)
+  * is used to avoid race conditions between the inactive timer handler
+  * and task wakeups.
+  *
+  * The following diagram shows how running_bw is updated. A task is
+  * "ACTIVE" when its utilization contributes to running_bw; an
+  * "ACTIVE contending" task is in the TASK_RUNNING state, while an
+  * "ACTIVE non contending" task is a blocked task for which the "0-lag time"
+  * has not passed yet. An "INACTIVE" task is a task for which the "0-lag"
+  * time already passed, which does not contribute to running_bw anymore.
+  *                              +------------------+
+  *             wakeup           |    ACTIVE        |
+  *          +------------------>+   contending     |
+  *          | add_running_bw    |                  |
+  *          |                   +----+------+------+
+  *          |                        |      ^
+  *          |                dequeue |      |
+  * +--------+-------+                |      |
+  * |                |   t >= 0-lag   |      | wakeup
+  * |    INACTIVE    |<---------------+      |
+  * |                | sub_running_bw |      |
+  * +--------+-------+                |      |
+  *          ^                        |      |
+  *          |              t < 0-lag |      |
+  *          |                        |      |
+  *          |                        V      |
+  *          |                   +----+------+------+
+  *          | sub_running_bw    |    ACTIVE        |
+  *          +-------------------+                  |
+  *            inactive timer    |  non contending  |
+  *            fired             +------------------+
+  *
+  * The task_non_contending() function is invoked when a task
+  * blocks, and checks if the 0-lag time already passed or
+  * not (in the first case, it directly updates running_bw;
+  * in the second case, it arms the inactive timer).
+  *
+  * The task_contending() function is invoked when a task wakes
+  * up, and checks if the task is still in the "ACTIVE non contending"
+  * state or not (in the second case, it updates running_bw).
+  */
+ static void task_non_contending(struct task_struct *p)
+ {
+ 	struct sched_dl_entity *dl_se = &p->dl;
+ 	struct hrtimer *timer = &dl_se->inactive_timer;
+ 	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+ 	struct rq *rq = rq_of_dl_rq(dl_rq);
+ 	s64 zerolag_time;
+ 
+ 	/*
+ 	 * If this is a non-deadline task that has been boosted,
+ 	 * do nothing
+ 	 */
+ 	if (dl_se->dl_runtime == 0)
+ 		return;
+ 
+ 	WARN_ON(hrtimer_active(&dl_se->inactive_timer));
+ 	WARN_ON(dl_se->dl_non_contending);
+ 
+ 	zerolag_time = dl_se->deadline -
+ 		 div64_long((dl_se->runtime * dl_se->dl_period),
+ 			dl_se->dl_runtime);
+ 
+ 	/*
+ 	 * Using relative times instead of the absolute "0-lag time"
+ 	 * allows to simplify the code
+ 	 */
+ 	zerolag_time -= rq_clock(rq);
+ 
+ 	/*
+ 	 * If the "0-lag time" already passed, decrease the active
+ 	 * utilization now, instead of starting a timer
+ 	 */
+ 	if (zerolag_time < 0) {
+ 		if (dl_task(p))
+ 			sub_running_bw(dl_se->dl_bw, dl_rq);
+ 		if (!dl_task(p) || p->state == TASK_DEAD) {
+ 			struct dl_bw *dl_b = dl_bw_of(task_cpu(p));
+ 
+ 			if (p->state == TASK_DEAD)
+ 				sub_rq_bw(p->dl.dl_bw, &rq->dl);
+ 			raw_spin_lock(&dl_b->lock);
+ 			__dl_clear(dl_b, p->dl.dl_bw, dl_bw_cpus(task_cpu(p)));
+ 			__dl_clear_params(p);
+ 			raw_spin_unlock(&dl_b->lock);
+ 		}
+ 
+ 		return;
+ 	}
+ 
+ 	dl_se->dl_non_contending = 1;
+ 	get_task_struct(p);
+ 	hrtimer_start(timer, ns_to_ktime(zerolag_time), HRTIMER_MODE_REL);
+ }
+ 
+ static void task_contending(struct sched_dl_entity *dl_se, int flags)
+ {
+ 	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+ 
+ 	/*
+ 	 * If this is a non-deadline task that has been boosted,
+ 	 * do nothing
+ 	 */
+ 	if (dl_se->dl_runtime == 0)
+ 		return;
+ 
+ 	if (flags & ENQUEUE_MIGRATED)
+ 		add_rq_bw(dl_se->dl_bw, dl_rq);
+ 
+ 	if (dl_se->dl_non_contending) {
+ 		dl_se->dl_non_contending = 0;
+ 		/*
+ 		 * If the timer handler is currently running and the
+ 		 * timer cannot be cancelled, inactive_task_timer()
+ 		 * will see that dl_not_contending is not set, and
+ 		 * will not touch the rq's active utilization,
+ 		 * so we are still safe.
+ 		 */
+ 		if (hrtimer_try_to_cancel(&dl_se->inactive_timer) == 1)
+ 			put_task_struct(dl_task_of(dl_se));
+ 	} else {
+ 		/*
+ 		 * Since "dl_non_contending" is not set, the
+ 		 * task's utilization has already been removed from
+ 		 * active utilization (either when the task blocked,
+ 		 * when the "inactive timer" fired).
+ 		 * So, add it back.
+ 		 */
+ 		add_running_bw(dl_se->dl_bw, dl_rq);
+ 	}
+ }
+ 
++>>>>>>> daec57983670 (sched/deadline: Reclaim bandwidth not used by dl tasks)
  static inline int is_leftmost(struct task_struct *p, struct dl_rq *dl_rq)
  {
  	struct sched_dl_entity *dl_se = &p->dl;
@@@ -802,17 -955,42 +998,56 @@@ extern bool sched_rt_bandwidth_account(
  /*
   * This function implements the GRUB accounting rule:
   * according to the GRUB reclaiming algorithm, the runtime is
++<<<<<<< HEAD
 + * not decreased as "dq = -dt", but as "dq = -Uact dt", where
 + * Uact is the (per-runqueue) active utilization.
 + * Since rq->dl.running_bw contains Uact * 2^BW_SHIFT, the result
 + * has to be shifted right by BW_SHIFT.
++=======
+  * not decreased as "dq = -dt", but as
+  * "dq = -max{u / Umax, (1 - Uinact - Uextra)} dt",
+  * where u is the utilization of the task, Umax is the maximum reclaimable
+  * utilization, Uinact is the (per-runqueue) inactive utilization, computed
+  * as the difference between the "total runqueue utilization" and the
+  * runqueue active utilization, and Uextra is the (per runqueue) extra
+  * reclaimable utilization.
+  * Since rq->dl.running_bw and rq->dl.this_bw contain utilizations
+  * multiplied by 2^BW_SHIFT, the result has to be shifted right by
+  * BW_SHIFT.
+  * Since rq->dl.bw_ratio contains 1 / Umax multipled by 2^RATIO_SHIFT,
+  * dl_bw is multiped by rq->dl.bw_ratio and shifted right by RATIO_SHIFT.
+  * Since delta is a 64 bit variable, to have an overflow its value
+  * should be larger than 2^(64 - 20 - 8), which is more than 64 seconds.
+  * So, overflow is not an issue here.
++>>>>>>> daec57983670 (sched/deadline: Reclaim bandwidth not used by dl tasks)
   */
 -u64 grub_reclaim(u64 delta, struct rq *rq, struct sched_dl_entity *dl_se)
 +u64 grub_reclaim(u64 delta, struct rq *rq)
  {
++<<<<<<< HEAD
 +	delta *= rq->dl.running_bw;
 +	delta >>= BW_SHIFT;
 +
 +	return delta;
++=======
+ 	u64 u_inact = rq->dl.this_bw - rq->dl.running_bw; /* Utot - Uact */
+ 	u64 u_act;
+ 	u64 u_act_min = (dl_se->dl_bw * rq->dl.bw_ratio) >> RATIO_SHIFT;
+ 
+ 	/*
+ 	 * Instead of computing max{u * bw_ratio, (1 - u_inact - u_extra)},
+ 	 * we compare u_inact + rq->dl.extra_bw with
+ 	 * 1 - (u * rq->dl.bw_ratio >> RATIO_SHIFT), because
+ 	 * u_inact + rq->dl.extra_bw can be larger than
+ 	 * 1 * (so, 1 - u_inact - rq->dl.extra_bw would be negative
+ 	 * leading to wrong results)
+ 	 */
+ 	if (u_inact + rq->dl.extra_bw > BW_UNIT - u_act_min)
+ 		u_act = u_act_min;
+ 	else
+ 		u_act = BW_UNIT - u_inact - rq->dl.extra_bw;
+ 
+ 	return (delta * u_act) >> BW_SHIFT;
++>>>>>>> daec57983670 (sched/deadline: Reclaim bandwidth not used by dl tasks)
  }
  
  /*
@@@ -899,6 -1076,56 +1134,59 @@@ throttle
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static enum hrtimer_restart inactive_task_timer(struct hrtimer *timer)
+ {
+ 	struct sched_dl_entity *dl_se = container_of(timer,
+ 						     struct sched_dl_entity,
+ 						     inactive_timer);
+ 	struct task_struct *p = dl_task_of(dl_se);
+ 	struct rq_flags rf;
+ 	struct rq *rq;
+ 
+ 	rq = task_rq_lock(p, &rf);
+ 
+ 	if (!dl_task(p) || p->state == TASK_DEAD) {
+ 		struct dl_bw *dl_b = dl_bw_of(task_cpu(p));
+ 
+ 		if (p->state == TASK_DEAD && dl_se->dl_non_contending) {
+ 			sub_running_bw(p->dl.dl_bw, dl_rq_of_se(&p->dl));
+ 			sub_rq_bw(p->dl.dl_bw, dl_rq_of_se(&p->dl));
+ 			dl_se->dl_non_contending = 0;
+ 		}
+ 
+ 		raw_spin_lock(&dl_b->lock);
+ 		__dl_clear(dl_b, p->dl.dl_bw, dl_bw_cpus(task_cpu(p)));
+ 		raw_spin_unlock(&dl_b->lock);
+ 		__dl_clear_params(p);
+ 
+ 		goto unlock;
+ 	}
+ 	if (dl_se->dl_non_contending == 0)
+ 		goto unlock;
+ 
+ 	sched_clock_tick();
+ 	update_rq_clock(rq);
+ 
+ 	sub_running_bw(dl_se->dl_bw, &rq->dl);
+ 	dl_se->dl_non_contending = 0;
+ unlock:
+ 	task_rq_unlock(rq, p, &rf);
+ 	put_task_struct(p);
+ 
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se)
+ {
+ 	struct hrtimer *timer = &dl_se->inactive_timer;
+ 
+ 	hrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+ 	timer->function = inactive_task_timer;
+ }
+ 
++>>>>>>> daec57983670 (sched/deadline: Reclaim bandwidth not used by dl tasks)
  #ifdef CONFIG_SMP
  
  static void inc_dl_deadline(struct dl_rq *dl_rq, u64 deadline)
diff --cc kernel/sched/sched.h
index d6fd5ead3b99,f1e400c6403c..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -509,6 -570,24 +514,27 @@@ struct dl_rq 
  	 * task blocks
  	 */
  	u64 running_bw;
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Utilization of the tasks "assigned" to this runqueue (including
+ 	 * the tasks that are in runqueue and the tasks that executed on this
+ 	 * CPU and blocked). Increased when a task moves to this runqueue, and
+ 	 * decreased when the task moves away (migrates, changes scheduling
+ 	 * policy, or terminates).
+ 	 * This is needed to compute the "inactive utilization" for the
+ 	 * runqueue (inactive utilization = this_bw - running_bw).
+ 	 */
+ 	u64 this_bw;
+ 	u64 extra_bw;
+ 
+ 	/*
+ 	 * Inverse of the fraction of CPU utilization that can be reclaimed
+ 	 * by the GRUB algorithm.
+ 	 */
+ 	u64 bw_ratio;
++>>>>>>> daec57983670 (sched/deadline: Reclaim bandwidth not used by dl tasks)
  };
  
  #ifdef CONFIG_SMP
@@@ -1650,55 -1958,67 +1676,82 @@@ enum rq_nohz_flag_bits 
  };
  
  #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
 -
 -extern void nohz_balance_exit_idle(unsigned int cpu);
 -#else
 -static inline void nohz_balance_exit_idle(unsigned int cpu) { }
  #endif
  
+ 
+ #ifdef CONFIG_SMP
+ static inline
+ void __dl_update(struct dl_bw *dl_b, s64 bw)
+ {
+ 	struct root_domain *rd = container_of(dl_b, struct root_domain, dl_bw);
+ 	int i;
+ 
+ 	RCU_LOCKDEP_WARN(!rcu_read_lock_sched_held(),
+ 			 "sched RCU must be held");
+ 	for_each_cpu_and(i, rd->span, cpu_active_mask) {
+ 		struct rq *rq = cpu_rq(i);
+ 
+ 		rq->dl.extra_bw += bw;
+ 	}
+ }
+ #else
+ static inline
+ void __dl_update(struct dl_bw *dl_b, s64 bw)
+ {
+ 	struct dl_rq *dl = container_of(dl_b, struct dl_rq, dl_bw);
+ 
+ 	dl->extra_bw += bw;
+ }
+ #endif
+ 
+ 
  #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 -struct irqtime {
 -	u64			total;
 -	u64			tick_delta;
 -	u64			irq_start_time;
 -	struct u64_stats_sync	sync;
 -};
  
 -DECLARE_PER_CPU(struct irqtime, cpu_irqtime);
 +DECLARE_PER_CPU(u64, cpu_hardirq_time);
 +DECLARE_PER_CPU(u64, cpu_softirq_time);
 +
 +#ifndef CONFIG_64BIT
 +DECLARE_PER_CPU(seqcount_t, irq_time_seq);
 +
 +static inline void irq_time_write_begin(void)
 +{
 +	__this_cpu_inc(irq_time_seq.sequence);
 +	smp_wmb();
 +}
 +
 +static inline void irq_time_write_end(void)
 +{
 +	smp_wmb();
 +	__this_cpu_inc(irq_time_seq.sequence);
 +}
  
 -/*
 - * Returns the irqtime minus the softirq time computed by ksoftirqd.
 - * Otherwise ksoftirqd's sum_exec_runtime is substracted its own runtime
 - * and never move forward.
 - */
  static inline u64 irq_time_read(int cpu)
  {
 -	struct irqtime *irqtime = &per_cpu(cpu_irqtime, cpu);
 -	unsigned int seq;
 -	u64 total;
 +	u64 irq_time;
 +	unsigned seq;
  
  	do {
 -		seq = __u64_stats_fetch_begin(&irqtime->sync);
 -		total = irqtime->total;
 -	} while (__u64_stats_fetch_retry(&irqtime->sync, seq));
 +		seq = read_seqcount_begin(&per_cpu(irq_time_seq, cpu));
 +		irq_time = per_cpu(cpu_softirq_time, cpu) +
 +			   per_cpu(cpu_hardirq_time, cpu);
 +	} while (read_seqcount_retry(&per_cpu(irq_time_seq, cpu), seq));
 +
 +	return irq_time;
 +}
 +#else /* CONFIG_64BIT */
 +static inline void irq_time_write_begin(void)
 +{
 +}
  
 -	return total;
 +static inline void irq_time_write_end(void)
 +{
 +}
 +
 +static inline u64 irq_time_read(int cpu)
 +{
 +	return per_cpu(cpu_softirq_time, cpu) + per_cpu(cpu_hardirq_time, cpu);
  }
 +#endif /* CONFIG_64BIT */
  #endif /* CONFIG_IRQ_TIME_ACCOUNTING */
  
  #ifdef CONFIG_CPU_FREQ
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/deadline.c
* Unmerged path kernel/sched/sched.h
