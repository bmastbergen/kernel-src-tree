xdp: base API for new XDP rx-queue info concept

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jesper Dangaard Brouer <brouer@redhat.com>
commit aecd67b60722dd24353b0bc50e78a55b30707dcd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/aecd67b6.failed

This patch only introduce the core data structures and API functions.
All XDP enabled drivers must use the API before this info can used.

There is a need for XDP to know more about the RX-queue a given XDP
frames have arrived on.  For both the XDP bpf-prog and kernel side.

Instead of extending xdp_buff each time new info is needed, the patch
creates a separate read-mostly struct xdp_rxq_info, that contains this
info.  We stress this data/cache-line is for read-only info.  This is
NOT for dynamic per packet info, use the data_meta for such use-cases.

The performance advantage is this info can be setup at RX-ring init
time, instead of updating N-members in xdp_buff.  A possible (driver
level) micro optimization is that xdp_buff->rxq assignment could be
done once per XDP/NAPI loop.  The extra pointer deref only happens for
program needing access to this info (thus, no slowdown to existing
use-cases).

	Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit aecd67b60722dd24353b0bc50e78a55b30707dcd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/filter.h
#	net/core/Makefile
diff --cc include/linux/filter.h
index d322ed880333,425056c7f96c..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -4,20 -5,445 +4,36 @@@
  #ifndef __LINUX_FILTER_H__
  #define __LINUX_FILTER_H__
  
 -#include <stdarg.h>
 -
  #include <linux/atomic.h>
 -#include <linux/refcount.h>
  #include <linux/compat.h>
++<<<<<<< HEAD
++=======
+ #include <linux/skbuff.h>
+ #include <linux/linkage.h>
+ #include <linux/printk.h>
+ #include <linux/workqueue.h>
+ #include <linux/sched.h>
+ #include <linux/capability.h>
+ #include <linux/cryptohash.h>
+ #include <linux/set_memory.h>
+ #include <linux/kallsyms.h>
+ 
+ #include <net/xdp.h>
+ #include <net/sch_generic.h>
+ 
++>>>>>>> aecd67b60722 (xdp: base API for new XDP rx-queue info concept)
  #include <uapi/linux/filter.h>
 -#include <uapi/linux/bpf.h>
 -
 -struct sk_buff;
 -struct sock;
 -struct seccomp_data;
 -struct bpf_prog_aux;
 -
 -/* ArgX, context and stack frame pointer register positions. Note,
 - * Arg1, Arg2, Arg3, etc are used as argument mappings of function
 - * calls in BPF_CALL instruction.
 - */
 -#define BPF_REG_ARG1	BPF_REG_1
 -#define BPF_REG_ARG2	BPF_REG_2
 -#define BPF_REG_ARG3	BPF_REG_3
 -#define BPF_REG_ARG4	BPF_REG_4
 -#define BPF_REG_ARG5	BPF_REG_5
 -#define BPF_REG_CTX	BPF_REG_6
 -#define BPF_REG_FP	BPF_REG_10
 -
 -/* Additional register mappings for converted user programs. */
 -#define BPF_REG_A	BPF_REG_0
 -#define BPF_REG_X	BPF_REG_7
 -#define BPF_REG_TMP	BPF_REG_8
 -
 -/* Kernel hidden auxiliary/helper register for hardening step.
 - * Only used by eBPF JITs. It's nothing more than a temporary
 - * register that JITs use internally, only that here it's part
 - * of eBPF instructions that have been rewritten for blinding
 - * constants. See JIT pre-step in bpf_jit_blind_constants().
 - */
 -#define BPF_REG_AX		MAX_BPF_REG
 -#define MAX_BPF_JIT_REG		(MAX_BPF_REG + 1)
 -
 -/* unused opcode to mark special call to bpf_tail_call() helper */
 -#define BPF_TAIL_CALL	0xf0
 -
 -/* unused opcode to mark call to interpreter with arguments */
 -#define BPF_CALL_ARGS	0xe0
 -
 -/* As per nm, we expose JITed images as text (code) section for
 - * kallsyms. That way, tools like perf can find it to match
 - * addresses.
 - */
 -#define BPF_SYM_ELF_TYPE	't'
 -
 -/* BPF program can access up to 512 bytes of stack space. */
 -#define MAX_BPF_STACK	512
 -
 -/* Helper macros for filter block array initializers. */
 -
 -/* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
 -
 -#define BPF_ALU64_REG(OP, DST, SRC)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -#define BPF_ALU32_REG(OP, DST, SRC)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
 -
 -#define BPF_ALU64_IMM(OP, DST, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_ALU32_IMM(OP, DST, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
 -
 -#define BPF_ENDIAN(TYPE, DST, LEN)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = LEN })
 -
 -/* Short form of mov, dst_reg = src_reg */
 -
 -#define BPF_MOV64_REG(DST, SRC)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -#define BPF_MOV32_REG(DST, SRC)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* Short form of mov, dst_reg = imm32 */
 -
 -#define BPF_MOV64_IMM(DST, IMM)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_MOV32_IMM(DST, IMM)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
 -#define BPF_LD_IMM64(DST, IMM)					\
 -	BPF_LD_IMM64_RAW(DST, 0, IMM)
 -
 -#define BPF_LD_IMM64_RAW(DST, SRC, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LD | BPF_DW | BPF_IMM,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = (__u32) (IMM) }),			\
 -	((struct bpf_insn) {					\
 -		.code  = 0, /* zero is reserved opcode */	\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = ((__u64) (IMM)) >> 32 })
 -
 -/* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */
 -#define BPF_LD_MAP_FD(DST, MAP_FD)				\
 -	BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)
 -
 -/* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
 -
 -#define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
 -
 -#define BPF_LD_ABS(SIZE, IMM)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
 -
 -#define BPF_LD_IND(SIZE, SRC, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
 -		.dst_reg = 0,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Memory load, dst_reg = *(uint *) (src_reg + off16) */
 -
 -#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Memory store, *(uint *) (dst_reg + off16) = src_reg */
 -
 -#define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
 -
 -#define BPF_STX_XADD(SIZE, DST, SRC, OFF)			\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_XADD,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Memory store, *(uint *) (dst_reg + off16) = imm32 */
 -
 -#define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
 -
 -#define BPF_JMP_REG(OP, DST, SRC, OFF)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
 -
 -#define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Unconditional jumps, goto pc + off16 */
 -
 -#define BPF_JMP_A(OFF)						\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_JA,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Function call */
 -
 -#define BPF_EMIT_CALL(FUNC)					\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_CALL,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = ((FUNC) - __bpf_call_base) })
 -
 -/* Raw code statement block */
 -
 -#define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
 -	((struct bpf_insn) {					\
 -		.code  = CODE,					\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Program exit */
 -
 -#define BPF_EXIT_INSN()						\
 -	((struct bpf_insn) {					\
 -		.code  = BPF_JMP | BPF_EXIT,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* Internal classic blocks for direct assignment */
 -
 -#define __BPF_STMT(CODE, K)					\
 -	((struct sock_filter) BPF_STMT(CODE, K))
 -
 -#define __BPF_JUMP(CODE, K, JT, JF)				\
 -	((struct sock_filter) BPF_JUMP(CODE, K, JT, JF))
 -
 -#define bytes_to_bpf_size(bytes)				\
 -({								\
 -	int bpf_size = -EINVAL;					\
 -								\
 -	if (bytes == sizeof(u8))				\
 -		bpf_size = BPF_B;				\
 -	else if (bytes == sizeof(u16))				\
 -		bpf_size = BPF_H;				\
 -	else if (bytes == sizeof(u32))				\
 -		bpf_size = BPF_W;				\
 -	else if (bytes == sizeof(u64))				\
 -		bpf_size = BPF_DW;				\
 -								\
 -	bpf_size;						\
 -})
 -
 -#define bpf_size_to_bytes(bpf_size)				\
 -({								\
 -	int bytes = -EINVAL;					\
 -								\
 -	if (bpf_size == BPF_B)					\
 -		bytes = sizeof(u8);				\
 -	else if (bpf_size == BPF_H)				\
 -		bytes = sizeof(u16);				\
 -	else if (bpf_size == BPF_W)				\
 -		bytes = sizeof(u32);				\
 -	else if (bpf_size == BPF_DW)				\
 -		bytes = sizeof(u64);				\
 -								\
 -	bytes;							\
 -})
 -
 -#define BPF_SIZEOF(type)					\
 -	({							\
 -		const int __size = bytes_to_bpf_size(sizeof(type)); \
 -		BUILD_BUG_ON(__size < 0);			\
 -		__size;						\
 -	})
 -
 -#define BPF_FIELD_SIZEOF(type, field)				\
 -	({							\
 -		const int __size = bytes_to_bpf_size(FIELD_SIZEOF(type, field)); \
 -		BUILD_BUG_ON(__size < 0);			\
 -		__size;						\
 -	})
 -
 -#define BPF_LDST_BYTES(insn)					\
 -	({							\
 -		const int __size = bpf_size_to_bytes(BPF_SIZE(insn->code)); \
 -		WARN_ON(__size < 0);				\
 -		__size;						\
 -	})
 -
 -#define __BPF_MAP_0(m, v, ...) v
 -#define __BPF_MAP_1(m, v, t, a, ...) m(t, a)
 -#define __BPF_MAP_2(m, v, t, a, ...) m(t, a), __BPF_MAP_1(m, v, __VA_ARGS__)
 -#define __BPF_MAP_3(m, v, t, a, ...) m(t, a), __BPF_MAP_2(m, v, __VA_ARGS__)
 -#define __BPF_MAP_4(m, v, t, a, ...) m(t, a), __BPF_MAP_3(m, v, __VA_ARGS__)
 -#define __BPF_MAP_5(m, v, t, a, ...) m(t, a), __BPF_MAP_4(m, v, __VA_ARGS__)
 -
 -#define __BPF_REG_0(...) __BPF_PAD(5)
 -#define __BPF_REG_1(...) __BPF_MAP(1, __VA_ARGS__), __BPF_PAD(4)
 -#define __BPF_REG_2(...) __BPF_MAP(2, __VA_ARGS__), __BPF_PAD(3)
 -#define __BPF_REG_3(...) __BPF_MAP(3, __VA_ARGS__), __BPF_PAD(2)
 -#define __BPF_REG_4(...) __BPF_MAP(4, __VA_ARGS__), __BPF_PAD(1)
 -#define __BPF_REG_5(...) __BPF_MAP(5, __VA_ARGS__)
 -
 -#define __BPF_MAP(n, ...) __BPF_MAP_##n(__VA_ARGS__)
 -#define __BPF_REG(n, ...) __BPF_REG_##n(__VA_ARGS__)
 -
 -#define __BPF_CAST(t, a)						       \
 -	(__force t)							       \
 -	(__force							       \
 -	 typeof(__builtin_choose_expr(sizeof(t) == sizeof(unsigned long),      \
 -				      (unsigned long)0, (t)0))) a
 -#define __BPF_V void
 -#define __BPF_N
 -
 -#define __BPF_DECL_ARGS(t, a) t   a
 -#define __BPF_DECL_REGS(t, a) u64 a
 -
 -#define __BPF_PAD(n)							       \
 -	__BPF_MAP(n, __BPF_DECL_ARGS, __BPF_N, u64, __ur_1, u64, __ur_2,       \
 -		  u64, __ur_3, u64, __ur_4, u64, __ur_5)
 -
 -#define BPF_CALL_x(x, name, ...)					       \
 -	static __always_inline						       \
 -	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__));   \
 -	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__));	       \
 -	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__))	       \
 -	{								       \
 -		return ____##name(__BPF_MAP(x,__BPF_CAST,__BPF_N,__VA_ARGS__));\
 -	}								       \
 -	static __always_inline						       \
 -	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__))
 -
 -#define BPF_CALL_0(name, ...)	BPF_CALL_x(0, name, __VA_ARGS__)
 -#define BPF_CALL_1(name, ...)	BPF_CALL_x(1, name, __VA_ARGS__)
 -#define BPF_CALL_2(name, ...)	BPF_CALL_x(2, name, __VA_ARGS__)
 -#define BPF_CALL_3(name, ...)	BPF_CALL_x(3, name, __VA_ARGS__)
 -#define BPF_CALL_4(name, ...)	BPF_CALL_x(4, name, __VA_ARGS__)
 -#define BPF_CALL_5(name, ...)	BPF_CALL_x(5, name, __VA_ARGS__)
 -
 -#define bpf_ctx_range(TYPE, MEMBER)						\
 -	offsetof(TYPE, MEMBER) ... offsetofend(TYPE, MEMBER) - 1
 -#define bpf_ctx_range_till(TYPE, MEMBER1, MEMBER2)				\
 -	offsetof(TYPE, MEMBER1) ... offsetofend(TYPE, MEMBER2) - 1
 -
 -#define bpf_target_off(TYPE, MEMBER, SIZE, PTR_SIZE)				\
 -	({									\
 -		BUILD_BUG_ON(FIELD_SIZEOF(TYPE, MEMBER) != (SIZE));		\
 -		*(PTR_SIZE) = (SIZE);						\
 -		offsetof(TYPE, MEMBER);						\
 -	})
 +#ifndef __GENKSYMS__
 +#include <net/sch_generic.h>
 +#endif
  
  #ifdef CONFIG_COMPAT
 -/* A struct sock_filter is architecture independent. */
 +/*
 + * A struct sock_filter is architecture independent.
 + */
  struct compat_sock_fprog {
  	u16		len;
 -	compat_uptr_t	filter;	/* struct sock_filter * */
 +	compat_uptr_t	filter;		/* struct sock_filter * */
  };
  #endif
  
@@@ -43,20 -502,181 +59,21 @@@ struct sk_filte
  struct xdp_buff {
  	void *data;
  	void *data_end;
 -	void *data_meta;
  	void *data_hard_start;
+ 	struct xdp_rxq_info *rxq;
  };
  
 -/* Compute the linear packet data range [data, data_end) which
 - * will be accessed by various program types (cls_bpf, act_bpf,
 - * lwt, ...). Subsystems allowing direct data access must (!)
 - * ensure that cb[] area can be written to when BPF program is
 - * invoked (otherwise cb[] save/restore is necessary).
 +/* compute the linear packet data range [data, data_end) which
 + * will be accessed by cls_bpf and act_bpf programs
   */
 -static inline void bpf_compute_data_pointers(struct sk_buff *skb)
 -{
 -	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;
 -
 -	BUILD_BUG_ON(sizeof(*cb) > FIELD_SIZEOF(struct sk_buff, cb));
 -	cb->data_meta = skb->data - skb_metadata_len(skb);
 -	cb->data_end  = skb->data + skb_headlen(skb);
 -}
 -
 -static inline u8 *bpf_skb_cb(struct sk_buff *skb)
 -{
 -	/* eBPF programs may read/write skb->cb[] area to transfer meta
 -	 * data between tail calls. Since this also needs to work with
 -	 * tc, that scratch memory is mapped to qdisc_skb_cb's data area.
 -	 *
 -	 * In some socket filter cases, the cb unfortunately needs to be
 -	 * saved/restored so that protocol specific skb->cb[] data won't
 -	 * be lost. In any case, due to unpriviledged eBPF programs
 -	 * attached to sockets, we need to clear the bpf_skb_cb() area
 -	 * to not leak previous contents to user space.
 -	 */
 -	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) != BPF_SKB_CB_LEN);
 -	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) !=
 -		     FIELD_SIZEOF(struct qdisc_skb_cb, data));
 -
 -	return qdisc_skb_cb(skb)->data;
 -}
 -
 -static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
 -				       struct sk_buff *skb)
 -{
 -	u8 *cb_data = bpf_skb_cb(skb);
 -	u8 cb_saved[BPF_SKB_CB_LEN];
 -	u32 res;
 -
 -	if (unlikely(prog->cb_access)) {
 -		memcpy(cb_saved, cb_data, sizeof(cb_saved));
 -		memset(cb_data, 0, sizeof(cb_saved));
 -	}
 -
 -	res = BPF_PROG_RUN(prog, skb);
 -
 -	if (unlikely(prog->cb_access))
 -		memcpy(cb_data, cb_saved, sizeof(cb_saved));
 -
 -	return res;
 -}
 -
 -static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 -					struct sk_buff *skb)
 -{
 -	u8 *cb_data = bpf_skb_cb(skb);
 -
 -	if (unlikely(prog->cb_access))
 -		memset(cb_data, 0, BPF_SKB_CB_LEN);
 -
 -	return BPF_PROG_RUN(prog, skb);
 -}
 -
 -static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 -					    struct xdp_buff *xdp)
 -{
 -	/* Caller needs to hold rcu_read_lock() (!), otherwise program
 -	 * can be released while still running, or map elements could be
 -	 * freed early while still having concurrent users. XDP fastpath
 -	 * already takes rcu_read_lock() when fetching the program, so
 -	 * it's not necessary here anymore.
 -	 */
 -	return BPF_PROG_RUN(prog, xdp);
 -}
 -
 -static inline u32 bpf_prog_insn_size(const struct bpf_prog *prog)
 -{
 -	return prog->len * sizeof(struct bpf_insn);
 -}
 -
 -static inline u32 bpf_prog_tag_scratch_size(const struct bpf_prog *prog)
 -{
 -	return round_up(bpf_prog_insn_size(prog) +
 -			sizeof(__be64) + 1, SHA_MESSAGE_BYTES);
 -}
 -
 -static inline unsigned int bpf_prog_size(unsigned int proglen)
 -{
 -	return max(sizeof(struct bpf_prog),
 -		   offsetof(struct bpf_prog, insns[proglen]));
 -}
 -
 -static inline bool bpf_prog_was_classic(const struct bpf_prog *prog)
 -{
 -	/* When classic BPF programs have been loaded and the arch
 -	 * does not have a classic BPF JIT (anymore), they have been
 -	 * converted via bpf_migrate_filter() to eBPF and thus always
 -	 * have an unspec program type.
 -	 */
 -	return prog->type == BPF_PROG_TYPE_UNSPEC;
 -}
 -
 -static inline bool
 -bpf_ctx_narrow_access_ok(u32 off, u32 size, const u32 size_default)
 -{
 -	bool off_ok;
 -#ifdef __LITTLE_ENDIAN
 -	off_ok = (off & (size_default - 1)) == 0;
 -#else
 -	off_ok = (off & (size_default - 1)) + size == size_default;
 -#endif
 -	return off_ok && size <= size_default && (size & (size - 1)) == 0;
 -}
 -
 -#define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 -
 -#ifdef CONFIG_ARCH_HAS_SET_MEMORY
 -static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 -{
 -	fp->locked = 1;
 -	WARN_ON_ONCE(set_memory_ro((unsigned long)fp, fp->pages));
 -}
 -
 -static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 -{
 -	if (fp->locked) {
 -		WARN_ON_ONCE(set_memory_rw((unsigned long)fp, fp->pages));
 -		/* In case set_memory_rw() fails, we want to be the first
 -		 * to crash here instead of some random place later on.
 -		 */
 -		fp->locked = 0;
 -	}
 -}
 -
 -static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 -{
 -	WARN_ON_ONCE(set_memory_ro((unsigned long)hdr, hdr->pages));
 -}
 -
 -static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
 -{
 -	WARN_ON_ONCE(set_memory_rw((unsigned long)hdr, hdr->pages));
 -}
 -#else
 -static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 -{
 -}
 -
 -static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 -{
 -}
 -
 -static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 +static inline unsigned int sk_filter_len(const struct sk_filter *fp)
  {
 +	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
  }
  
 -static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
 +static inline void bpf_compute_data_end(struct sk_buff *skb)
  {
 -}
 -#endif /* CONFIG_ARCH_HAS_SET_MEMORY */
 -
 -static inline struct bpf_binary_header *
 -bpf_jit_binary_hdr(const struct bpf_prog *fp)
 -{
 -	unsigned long real_start = (unsigned long)fp->bpf_func;
 -	unsigned long addr = real_start & PAGE_MASK;
 -
 -	return (void *)addr;
 +	return;
  }
  
  int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
diff --cc net/core/Makefile
index 149101a792d7,6dbbba8c57ae..000000000000
--- a/net/core/Makefile
+++ b/net/core/Makefile
@@@ -10,9 -11,8 +10,13 @@@ obj-$(CONFIG_SYSCTL) += sysctl_net_core
  obj-y		     += dev.o ethtool.o dev_addr_lists.o dst.o netevent.o \
  			neighbour.o rtnetlink.o utils.o link_watch.o filter.o \
  			sock_diag.o dev_ioctl.o tso.o sock_reuseport.o \
++<<<<<<< HEAD
 +			fib_notifier.o rh_wrap.o
++=======
+ 			fib_notifier.o xdp.o
++>>>>>>> aecd67b60722 (xdp: base API for new XDP rx-queue info concept)
  
 +obj-$(CONFIG_XFRM) += flow.o
  obj-y += net-sysfs.o
  obj-$(CONFIG_PROC_FS) += net-procfs.o
  obj-$(CONFIG_NET_PKTGEN) += pktgen.o
* Unmerged path include/linux/filter.h
diff --git a/include/net/xdp.h b/include/net/xdp.h
new file mode 100644
index 000000000000..86c41631a908
--- /dev/null
+++ b/include/net/xdp.h
@@ -0,0 +1,47 @@
+/* include/net/xdp.h
+ *
+ * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.
+ * Released under terms in GPL version 2.  See COPYING.
+ */
+#ifndef __LINUX_NET_XDP_H__
+#define __LINUX_NET_XDP_H__
+
+/**
+ * DOC: XDP RX-queue information
+ *
+ * The XDP RX-queue info (xdp_rxq_info) is associated with the driver
+ * level RX-ring queues.  It is information that is specific to how
+ * the driver have configured a given RX-ring queue.
+ *
+ * Each xdp_buff frame received in the driver carry a (pointer)
+ * reference to this xdp_rxq_info structure.  This provides the XDP
+ * data-path read-access to RX-info for both kernel and bpf-side
+ * (limited subset).
+ *
+ * For now, direct access is only safe while running in NAPI/softirq
+ * context.  Contents is read-mostly and must not be updated during
+ * driver NAPI/softirq poll.
+ *
+ * The driver usage API is a register and unregister API.
+ *
+ * The struct is not directly tied to the XDP prog.  A new XDP prog
+ * can be attached as long as it doesn't change the underlying
+ * RX-ring.  If the RX-ring does change significantly, the NIC driver
+ * naturally need to stop the RX-ring before purging and reallocating
+ * memory.  In that process the driver MUST call unregistor (which
+ * also apply for driver shutdown and unload).  The register API is
+ * also mandatory during RX-ring setup.
+ */
+
+struct xdp_rxq_info {
+	struct net_device *dev;
+	u32 queue_index;
+	u32 reg_state;
+} ____cacheline_aligned; /* perf critical, avoid false-sharing */
+
+int xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,
+		     struct net_device *dev, u32 queue_index);
+void xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq);
+void xdp_rxq_info_unused(struct xdp_rxq_info *xdp_rxq);
+
+#endif /* __LINUX_NET_XDP_H__ */
* Unmerged path net/core/Makefile
diff --git a/net/core/xdp.c b/net/core/xdp.c
new file mode 100644
index 000000000000..229bc5a0ee04
--- /dev/null
+++ b/net/core/xdp.c
@@ -0,0 +1,67 @@
+/* net/core/xdp.c
+ *
+ * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.
+ * Released under terms in GPL version 2.  See COPYING.
+ */
+#include <linux/types.h>
+#include <linux/mm.h>
+
+#include <net/xdp.h>
+
+#define REG_STATE_NEW		0x0
+#define REG_STATE_REGISTERED	0x1
+#define REG_STATE_UNREGISTERED	0x2
+#define REG_STATE_UNUSED	0x3
+
+void xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq)
+{
+	/* Simplify driver cleanup code paths, allow unreg "unused" */
+	if (xdp_rxq->reg_state == REG_STATE_UNUSED)
+		return;
+
+	WARN(!(xdp_rxq->reg_state == REG_STATE_REGISTERED), "Driver BUG");
+
+	xdp_rxq->reg_state = REG_STATE_UNREGISTERED;
+	xdp_rxq->dev = NULL;
+}
+EXPORT_SYMBOL_GPL(xdp_rxq_info_unreg);
+
+static void xdp_rxq_info_init(struct xdp_rxq_info *xdp_rxq)
+{
+	memset(xdp_rxq, 0, sizeof(*xdp_rxq));
+}
+
+/* Returns 0 on success, negative on failure */
+int xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,
+		     struct net_device *dev, u32 queue_index)
+{
+	if (xdp_rxq->reg_state == REG_STATE_UNUSED) {
+		WARN(1, "Driver promised not to register this");
+		return -EINVAL;
+	}
+
+	if (xdp_rxq->reg_state == REG_STATE_REGISTERED) {
+		WARN(1, "Missing unregister, handled but fix driver");
+		xdp_rxq_info_unreg(xdp_rxq);
+	}
+
+	if (!dev) {
+		WARN(1, "Missing net_device from driver");
+		return -ENODEV;
+	}
+
+	/* State either UNREGISTERED or NEW */
+	xdp_rxq_info_init(xdp_rxq);
+	xdp_rxq->dev = dev;
+	xdp_rxq->queue_index = queue_index;
+
+	xdp_rxq->reg_state = REG_STATE_REGISTERED;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xdp_rxq_info_reg);
+
+void xdp_rxq_info_unused(struct xdp_rxq_info *xdp_rxq)
+{
+	xdp_rxq->reg_state = REG_STATE_UNUSED;
+}
+EXPORT_SYMBOL_GPL(xdp_rxq_info_unused);
