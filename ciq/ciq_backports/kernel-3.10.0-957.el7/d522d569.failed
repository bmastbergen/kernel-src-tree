xfs: consolidate the various page fault handlers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Christoph Hellwig <hch@lst.de>
commit d522d569d6adf72ceda90153a086e089e6c2fbc6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/d522d569.failed

Add a new __xfs_filemap_fault helper that implements all four page fault
callouts, and make these methods themselves small stubs that set the
correct write_fault flag, and exit early for the non-DAX case for the
hugepage related ones.

Also remove the extra size checking in the pfn_fault path, which is now
handled in the core DAX code.

Life would be so much simpler if we only had one method for all this.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
(cherry picked from commit d522d569d6adf72ceda90153a086e089e6c2fbc6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_file.c
index cea567087acc,0debbc7e3f03..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -1107,81 -1011,11 +1107,87 @@@ xfs_file_llseek
   *       page_lock (MM)
   *         i_lock (XFS - extent map serialisation)
   */
++<<<<<<< HEAD
 +
 +/*
 + * mmap()d file has taken write protection fault and is being made writable. We
 + * can set the page state up correctly for a writable page, which means we can
 + * do correct delalloc accounting (ENOSPC checking!) and unwritten extent
 + * mapping.
 + */
 +STATIC int
 +xfs_filemap_page_mkwrite(
 +	struct vm_area_struct	*vma,
 +	struct vm_fault		*vmf)
 +{
 +	struct inode		*inode = file_inode(vma->vm_file);
 +	int			ret;
 +
 +	trace_xfs_filemap_page_mkwrite(XFS_I(inode));
 +
 +	sb_start_pagefault(inode->i_sb);
 +	file_update_time(vma->vm_file);
 +	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 +
 +	if (IS_DAX(inode)) {
 +		ret = dax_iomap_fault(vmf, PE_SIZE_PTE, &xfs_iomap_ops);
 +	} else {
 +		ret = iomap_page_mkwrite(vma, vmf, &xfs_iomap_ops);
 +		ret = block_page_mkwrite_return(ret);
 +	}
 +
 +	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 +	sb_end_pagefault(inode->i_sb);
 +
 +	return ret;
 +}
 +
 +STATIC int
 +xfs_filemap_fault(
 +	struct vm_area_struct	*vma,
 +	struct vm_fault		*vmf)
 +{
 +	struct inode		*inode = file_inode(vma->vm_file);
 +	int			ret;
 +
 +	trace_xfs_filemap_fault(XFS_I(inode));
 +
 +	/* DAX can shortcut the normal fault path on write faults! */
 +	if ((vmf->flags & FAULT_FLAG_WRITE) && IS_DAX(inode))
 +		return xfs_filemap_page_mkwrite(vma, vmf);
 +
 +	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 +	if (IS_DAX(inode)) {
 +		/*
 +		 * we do not want to trigger unwritten extent conversion on read
 +		 * faults - that is unnecessary overhead and would also require
 +		 * changes to xfs_get_blocks_direct() to map unwritten extent
 +		 * ioend for conversion on read-only mappings.
 +		 */
 +		ret = dax_iomap_fault(vmf, PE_SIZE_PTE, &xfs_iomap_ops);
 +	} else
 +		ret = filemap_fault(vma, vmf);
 +	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 +
 +	return ret;
 +}
 +
 +/*
 + * Similar to xfs_filemap_fault(), the DAX fault path can call into here on
 + * both read and write faults. Hence we need to handle both cases. There is no
 + * ->huge_mkwrite callout for huge pages, so we have a single function here to
 + * handle both cases here. @flags carries the information on the type of fault
 + * occuring.
 + */
 +STATIC int
 +xfs_filemap_huge_fault(
++=======
+ static int
+ __xfs_filemap_fault(
++>>>>>>> d522d569d6ad (xfs: consolidate the various page fault handlers)
  	struct vm_fault		*vmf,
- 	enum page_entry_size	pe_size)
+ 	enum page_entry_size	pe_size,
+ 	bool			write_fault)
  {
  	struct inode		*inode = file_inode(vmf->vma->vm_file);
  	struct xfs_inode	*ip = XFS_I(inode);
* Unmerged path fs/xfs/xfs_file.c
diff --git a/fs/xfs/xfs_trace.h b/fs/xfs/xfs_trace.h
index 9b67d179b1aa..656824be9062 100644
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@ -686,11 +686,34 @@ DEFINE_INODE_EVENT(xfs_inode_set_eofblocks_tag);
 DEFINE_INODE_EVENT(xfs_inode_clear_eofblocks_tag);
 DEFINE_INODE_EVENT(xfs_inode_free_eofblocks_invalid);
 
-DEFINE_INODE_EVENT(xfs_filemap_fault);
-DEFINE_INODE_EVENT(xfs_filemap_huge_fault);
-DEFINE_INODE_EVENT(xfs_filemap_page_mkwrite);
 DEFINE_INODE_EVENT(xfs_filemap_pfn_mkwrite);
 
+TRACE_EVENT(xfs_filemap_fault,
+	TP_PROTO(struct xfs_inode *ip, enum page_entry_size pe_size,
+		 bool write_fault),
+	TP_ARGS(ip, pe_size, write_fault),
+	TP_STRUCT__entry(
+		__field(dev_t, dev)
+		__field(xfs_ino_t, ino)
+		__field(enum page_entry_size, pe_size)
+		__field(bool, write_fault)
+	),
+	TP_fast_assign(
+		__entry->dev = VFS_I(ip)->i_sb->s_dev;
+		__entry->ino = ip->i_ino;
+		__entry->pe_size = pe_size;
+		__entry->write_fault = write_fault;
+	),
+	TP_printk("dev %d:%d ino 0x%llx %s write_fault %d",
+		  MAJOR(__entry->dev), MINOR(__entry->dev),
+		  __entry->ino,
+		  __print_symbolic(__entry->pe_size,
+			{ PE_SIZE_PTE,	"PTE" },
+			{ PE_SIZE_PMD,	"PMD" },
+			{ PE_SIZE_PUD,	"PUD" }),
+		  __entry->write_fault)
+)
+
 DECLARE_EVENT_CLASS(xfs_iref_class,
 	TP_PROTO(struct xfs_inode *ip, unsigned long caller_ip),
 	TP_ARGS(ip, caller_ip),
