x86/speculation/l1tf: Disallow non privileged high MMIO PROT_NONE mappings

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] l1tf: disallow non privileged high mmio prot_none mappings (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 87.88%
commit-author Andi Kleen <ak@linux.intel.com>
commit 42e4089c7890725fcd329999252dc489b72f2921
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/42e4089c.failed

For L1TF PROT_NONE mappings are protected by inverting the PFN in the page
table entry. This sets the high bits in the CPU's address space, thus
making sure to point to not point an unmapped entry to valid cached memory.

Some server system BIOSes put the MMIO mappings high up in the physical
address space. If such an high mapping was mapped to unprivileged users
they could attack low memory by setting such a mapping to PROT_NONE. This
could happen through a special device driver which is not access
protected. Normal /dev/mem is of course access protected.

To avoid this forbid PROT_NONE mappings or mprotect for high MMIO mappings.

Valid page mappings are allowed because the system is then unsafe anyways.

It's not expected that users commonly use PROT_NONE on MMIO. But to
minimize any impact this is only enforced if the mapping actually refers to
a high MMIO address (defined as the MAX_PA-1 bit being set), and also skip
the check for root.

For mmaps this is straight forward and can be handled in vm_insert_pfn and
in remap_pfn_range().

For mprotect it's a bit trickier. At the point where the actual PTEs are
accessed a lot of state has been changed and it would be difficult to undo
on an error. Since this is a uncommon case use a separate early page talk
walk pass for MMIO PROT_NONE mappings that checks for this condition
early. For non MMIO and non PROT_NONE there are no changes.

	Signed-off-by: Andi Kleen <ak@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
	Acked-by: Dave Hansen <dave.hansen@intel.com>


(cherry picked from commit 42e4089c7890725fcd329999252dc489b72f2921)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/pgtable.h
#	include/asm-generic/pgtable.h
#	mm/memory.c
diff --cc arch/x86/include/asm/pgtable.h
index 6a32515b7486,6a090a76fdca..000000000000
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@@ -1154,6 -1290,62 +1154,65 @@@ static inline u16 pte_flags_pkey(unsign
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool __pkru_allows_pkey(u16 pkey, bool write)
+ {
+ 	u32 pkru = read_pkru();
+ 
+ 	if (!__pkru_allows_read(pkru, pkey))
+ 		return false;
+ 	if (write && !__pkru_allows_write(pkru, pkey))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * 'pteval' can come from a PTE, PMD or PUD.  We only check
+  * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the
+  * same value on all 3 types.
+  */
+ static inline bool __pte_access_permitted(unsigned long pteval, bool write)
+ {
+ 	unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;
+ 
+ 	if (write)
+ 		need_pte_bits |= _PAGE_RW;
+ 
+ 	if ((pteval & need_pte_bits) != need_pte_bits)
+ 		return 0;
+ 
+ 	return __pkru_allows_pkey(pte_flags_pkey(pteval), write);
+ }
+ 
+ #define pte_access_permitted pte_access_permitted
+ static inline bool pte_access_permitted(pte_t pte, bool write)
+ {
+ 	return __pte_access_permitted(pte_val(pte), write);
+ }
+ 
+ #define pmd_access_permitted pmd_access_permitted
+ static inline bool pmd_access_permitted(pmd_t pmd, bool write)
+ {
+ 	return __pte_access_permitted(pmd_val(pmd), write);
+ }
+ 
+ #define pud_access_permitted pud_access_permitted
+ static inline bool pud_access_permitted(pud_t pud, bool write)
+ {
+ 	return __pte_access_permitted(pud_val(pud), write);
+ }
+ 
+ #define __HAVE_ARCH_PFN_MODIFY_ALLOWED 1
+ extern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);
+ 
+ static inline bool arch_has_pfn_modify_check(void)
+ {
+ 	return boot_cpu_has_bug(X86_BUG_L1TF);
+ }
+ 
++>>>>>>> 42e4089c7890 (x86/speculation/l1tf: Disallow non privileged high MMIO PROT_NONE mappings)
  #include <asm-generic/pgtable.h>
  #endif	/* __ASSEMBLY__ */
  
diff --cc include/asm-generic/pgtable.h
index 8aa445dbab59,0ecc1197084b..000000000000
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@@ -942,8 -1046,67 +942,35 @@@ static inline int pmd_clear_huge(pmd_t 
  {
  	return 0;
  }
 -static inline int pud_free_pmd_page(pud_t *pud)
 -{
 -	return 0;
 -}
 -static inline int pmd_free_pte_page(pmd_t *pmd)
 -{
 -	return 0;
 -}
  #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
  
 -#ifndef __HAVE_ARCH_FLUSH_PMD_TLB_RANGE
 -#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -/*
 - * ARCHes with special requirements for evicting THP backing TLB entries can
 - * implement this. Otherwise also, it can help optimize normal TLB flush in
 - * THP regime. stock flush_tlb_range() typically has optimization to nuke the
 - * entire TLB TLB if flush span is greater than a threshold, which will
 - * likely be true for a single huge page. Thus a single thp flush will
 - * invalidate the entire TLB which is not desitable.
 - * e.g. see arch/arc: flush_pmd_tlb_range
 - */
 -#define flush_pmd_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
 -#define flush_pud_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
 -#else
 -#define flush_pmd_tlb_range(vma, addr, end)	BUILD_BUG()
 -#define flush_pud_tlb_range(vma, addr, end)	BUILD_BUG()
 -#endif
 -#endif
 -
 -struct file;
 -int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,
 -			unsigned long size, pgprot_t *vma_prot);
 -
 -#ifndef CONFIG_X86_ESPFIX64
 -static inline void init_espfix_bsp(void) { }
 -#endif
 -
  #endif /* !__ASSEMBLY__ */
  
++<<<<<<< HEAD
++=======
+ #ifndef io_remap_pfn_range
+ #define io_remap_pfn_range remap_pfn_range
+ #endif
+ 
+ #ifndef has_transparent_hugepage
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ #define has_transparent_hugepage() 1
+ #else
+ #define has_transparent_hugepage() 0
+ #endif
+ #endif
+ 
+ #ifndef __HAVE_ARCH_PFN_MODIFY_ALLOWED
+ static inline bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
+ {
+ 	return true;
+ }
+ 
+ static inline bool arch_has_pfn_modify_check(void)
+ {
+ 	return false;
+ }
+ #endif
+ 
++>>>>>>> 42e4089c7890 (x86/speculation/l1tf: Disallow non privileged high MMIO PROT_NONE mappings)
  #endif /* _ASM_GENERIC_PGTABLE_H */
diff --cc mm/memory.c
index 03948e8ee2f3,3ba81b44a542..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -1715,8 -1885,11 +1715,16 @@@ int vm_insert_pfn(struct vm_area_struc
  
  	if (addr < vma->vm_start || addr >= vma->vm_end)
  		return -EFAULT;
++<<<<<<< HEAD
 +	if (track_pfn_insert(vma, &pgprot, __pfn_to_pfn_t(pfn, PFN_DEV)))
 +		return -EINVAL;
++=======
+ 
+ 	if (!pfn_modify_allowed(pfn, pgprot))
+ 		return -EACCES;
+ 
+ 	track_pfn_insert(vma, &pgprot, __pfn_to_pfn_t(pfn, PFN_DEV));
++>>>>>>> 42e4089c7890 (x86/speculation/l1tf: Disallow non privileged high MMIO PROT_NONE mappings)
  
  	ret = insert_pfn(vma, addr, __pfn_to_pfn_t(pfn, PFN_DEV), pgprot,
  			false);
@@@ -1748,9 -1921,12 +1756,12 @@@ static int __vm_insert_mixed(struct vm_
  
  	if (addr < vma->vm_start || addr >= vma->vm_end)
  		return -EFAULT;
 -
 -	track_pfn_insert(vma, &pgprot, pfn);
 +	if (track_pfn_insert(vma, &pgprot, pfn))
 +		return -EINVAL;
  
+ 	if (!pfn_modify_allowed(pfn_t_to_pfn(pfn), pgprot))
+ 		return -EACCES;
+ 
  	/*
  	 * If we don't have pte special, then we have to use the pfn_valid()
  	 * based VM_MIXEDMAP scheme (see vm_normal_page), and thus we *must*
@@@ -1840,9 -2037,10 +1858,10 @@@ static inline int remap_pud_range(struc
  {
  	pud_t *pud;
  	unsigned long next;
+ 	int err;
  
  	pfn -= addr >> PAGE_SHIFT;
 -	pud = pud_alloc(mm, p4d, addr);
 +	pud = pud_alloc(mm, pgd, addr);
  	if (!pud)
  		return -ENOMEM;
  	do {
@@@ -1854,6 -2053,28 +1874,31 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,
+ 			unsigned long addr, unsigned long end,
+ 			unsigned long pfn, pgprot_t prot)
+ {
+ 	p4d_t *p4d;
+ 	unsigned long next;
+ 	int err;
+ 
+ 	pfn -= addr >> PAGE_SHIFT;
+ 	p4d = p4d_alloc(mm, pgd, addr);
+ 	if (!p4d)
+ 		return -ENOMEM;
+ 	do {
+ 		next = p4d_addr_end(addr, end);
+ 		err = remap_pud_range(mm, p4d, addr, next,
+ 				pfn + (addr >> PAGE_SHIFT), prot);
+ 		if (err)
+ 			return err;
+ 	} while (p4d++, addr = next, addr != end);
+ 	return 0;
+ }
+ 
++>>>>>>> 42e4089c7890 (x86/speculation/l1tf: Disallow non privileged high MMIO PROT_NONE mappings)
  /**
   * remap_pfn_range - remap kernel memory to userspace
   * @vma: user vma to map to
* Unmerged path arch/x86/include/asm/pgtable.h
diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
index 98a074bd8cfc..7e7c434946f3 100644
--- a/arch/x86/mm/mmap.c
+++ b/arch/x86/mm/mmap.c
@@ -116,3 +116,24 @@ void arch_pick_mmap_layout(struct mm_struct *mm)
 		mm->unmap_area = arch_unmap_area_topdown;
 	}
 }
+
+/*
+ * Only allow root to set high MMIO mappings to PROT_NONE.
+ * This prevents an unpriv. user to set them to PROT_NONE and invert
+ * them, then pointing to valid memory for L1TF speculation.
+ *
+ * Note: for locked down kernels may want to disable the root override.
+ */
+bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
+{
+	if (!boot_cpu_has_bug(X86_BUG_L1TF))
+		return true;
+	if (!__pte_needs_invert(pgprot_val(prot)))
+		return true;
+	/* If it's real memory always allow */
+	if (pfn_valid(pfn))
+		return true;
+	if (pfn > l1tf_pfn_limit() && !capable(CAP_SYS_ADMIN))
+		return false;
+	return true;
+}
* Unmerged path include/asm-generic/pgtable.h
* Unmerged path mm/memory.c
diff --git a/mm/mprotect.c b/mm/mprotect.c
index c5ef273dd1ee..7ffbe1cb7a35 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -256,6 +256,42 @@ unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 	return pages;
 }
 
+static int prot_none_pte_entry(pte_t *pte, unsigned long addr,
+			       unsigned long next, struct mm_walk *walk)
+{
+	return pfn_modify_allowed(pte_pfn(*pte), *(pgprot_t *)(walk->private)) ?
+		0 : -EACCES;
+}
+
+static int prot_none_hugetlb_entry(pte_t *pte, unsigned long hmask,
+				   unsigned long addr, unsigned long next,
+				   struct mm_walk *walk)
+{
+	return pfn_modify_allowed(pte_pfn(*pte), *(pgprot_t *)(walk->private)) ?
+		0 : -EACCES;
+}
+
+static int prot_none_test(unsigned long addr, unsigned long next,
+			  struct mm_walk *walk)
+{
+	return 0;
+}
+
+static int prot_none_walk(struct vm_area_struct *vma, unsigned long start,
+			   unsigned long end, unsigned long newflags)
+{
+	pgprot_t new_pgprot = vm_get_page_prot(newflags);
+	struct mm_walk prot_none_walk = {
+		.pte_entry = prot_none_pte_entry,
+		.hugetlb_entry = prot_none_hugetlb_entry,
+		.test_walk = prot_none_test,
+		.mm = current->mm,
+		.private = &new_pgprot,
+	};
+
+	return walk_page_range(start, end, &prot_none_walk);
+}
+
 int
 mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 	unsigned long start, unsigned long end, unsigned long newflags)
@@ -273,6 +309,19 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 		return 0;
 	}
 
+	/*
+	 * Do PROT_NONE PFN permission checks here when we can still
+	 * bail out without undoing a lot of state. This is a rather
+	 * uncommon case, so doesn't need to be very optimized.
+	 */
+	if (arch_has_pfn_modify_check() &&
+	    (vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) &&
+	    (newflags & (VM_READ|VM_WRITE|VM_EXEC)) == 0) {
+		error = prot_none_walk(vma, start, end, newflags);
+		if (error)
+			return error;
+	}
+
 	/*
 	 * If we make a private mapping writable we increase our commit;
 	 * but (without finer accounting) cannot reduce our commit if we
