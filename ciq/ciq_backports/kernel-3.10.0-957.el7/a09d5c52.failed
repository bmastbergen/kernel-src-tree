nfp: bpf: flag jump destination to guide insn combine optimizations

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jiong Wang <jiong.wang@netronome.com>
commit a09d5c52c42129adbac2d1e39bd0e49a92729e3e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a09d5c52.failed

NFP eBPF offload JIT engine is doing some instruction combine based
optimizations which however must not be safe if the combined sequences
are across basic block boarders.

Currently, there are post checks during fixing jump destinations. If the
jump destination is found to be eBPF insn that has been combined into
another one, then JIT engine will raise error and abort.

This is not optimal. The JIT engine ought to disable the optimization on
such cross-bb-border sequences instead of abort.

As there is no control flow information in eBPF infrastructure that we
can't do basic block based optimizations, this patch extends the existing
jump destination record pass to also flag the jump destination, then in
instruction combine passes we could skip the optimizations if insns in the
sequence are jump targets.

	Suggested-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
	Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit a09d5c52c42129adbac2d1e39bd0e49a92729e3e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/offload.c
index de79faf0874b,377976ce92dd..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/offload.c
@@@ -51,63 -51,41 +51,79 @@@
  #include "../nfp_net_ctrl.h"
  #include "../nfp_net.h"
  
 +void nfp_net_filter_stats_timer(unsigned long data)
 +{
 +	struct nfp_net *nn = (void *)data;
 +	struct nfp_net_bpf_priv *priv;
 +	struct nfp_stat_pair latest;
 +
 +	priv = nn->app_priv;
 +
 +	spin_lock_bh(&priv->rx_filter_lock);
 +
 +	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 +		mod_timer(&priv->rx_filter_stats_timer,
 +			  jiffies + NFP_NET_STAT_POLL_IVL);
 +
 +	spin_unlock_bh(&priv->rx_filter_lock);
 +
 +	latest.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	latest.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +
 +	if (latest.pkts != priv->rx_filter.pkts)
 +		priv->rx_filter_change = jiffies;
 +
 +	priv->rx_filter = latest;
 +}
 +
 +#if 0 /* Not in RHEL7 */
 +static void nfp_net_bpf_stats_reset(struct nfp_net *nn)
 +{
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +
 +	priv->rx_filter.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	priv->rx_filter.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +	priv->rx_filter_prev = priv->rx_filter;
 +	priv->rx_filter_change = jiffies;
 +}
 +
  static int
 -nfp_prog_prepare(struct nfp_prog *nfp_prog, const struct bpf_insn *prog,
 -		 unsigned int cnt)
 +nfp_net_bpf_stats_update(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
  {
 -	struct nfp_insn_meta *meta;
 -	unsigned int i;
 +	struct tc_action *a;
 +	LIST_HEAD(actions);
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bytes, pkts;
  
 -	for (i = 0; i < cnt; i++) {
 -		meta = kzalloc(sizeof(*meta), GFP_KERNEL);
 -		if (!meta)
 -			return -ENOMEM;
 +	pkts = priv->rx_filter.pkts - priv->rx_filter_prev.pkts;
 +	bytes = priv->rx_filter.bytes - priv->rx_filter_prev.bytes;
 +	bytes -= pkts * ETH_HLEN;
  
 -		meta->insn = prog[i];
 -		meta->n = i;
 +	priv->rx_filter_prev = priv->rx_filter;
  
 -		list_add_tail(&meta->l, &nfp_prog->insns);
 -	}
 +	preempt_disable();
  
 -	/* Another pass to record jump information. */
 -	list_for_each_entry(meta, &nfp_prog->insns, l) {
 -		u64 code = meta->insn.code;
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list)
 +		tcf_action_stats_update(a, bytes, pkts, nn->rx_filter_change);
  
++<<<<<<< HEAD
 +	preempt_enable();
++=======
+ 		if (BPF_CLASS(code) == BPF_JMP && BPF_OP(code) != BPF_EXIT &&
+ 		    BPF_OP(code) != BPF_CALL) {
+ 			struct nfp_insn_meta *dst_meta;
+ 			unsigned short dst_indx;
+ 
+ 			dst_indx = meta->n + 1 + meta->insn.off;
+ 			dst_meta = nfp_bpf_goto_meta(nfp_prog, meta, dst_indx,
+ 						     cnt);
+ 
+ 			meta->jmp_dst = dst_meta;
+ 			dst_meta->flags |= FLAG_INSN_IS_JUMP_DST;
+ 		}
+ 	}
++>>>>>>> a09d5c52c421 (nfp: bpf: flag jump destination to guide insn combine optimizations)
  
  	return 0;
  }
diff --git a/drivers/net/ethernet/netronome/nfp/bpf/main.h b/drivers/net/ethernet/netronome/nfp/bpf/main.h
index 5212b54abaf7..ce96f1a21f97 100644
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.h
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.h
@@ -96,12 +96,15 @@ typedef int (*instr_cb_t)(struct nfp_prog *, struct nfp_insn_meta *);
 #define nfp_meta_next(meta)	list_next_entry(meta, l)
 #define nfp_meta_prev(meta)	list_prev_entry(meta, l)
 
+#define FLAG_INSN_IS_JUMP_DST	BIT(0)
+
 /**
  * struct nfp_insn_meta - BPF instruction wrapper
  * @insn: BPF instruction
  * @ptr: pointer type for memory operations
  * @off: index of first generated machine instruction (in nfp_prog.prog)
  * @n: eBPF instruction number
+ * @flags: eBPF instruction extra optimization flags
  * @skip: skip this instruction (optimized out)
  * @double_cb: callback for second part of the instruction
  * @l: link on nfp_prog->insns list
@@ -111,6 +114,7 @@ struct nfp_insn_meta {
 	struct bpf_reg_state ptr;
 	unsigned int off;
 	unsigned short n;
+	unsigned short flags;
 	bool skip;
 	instr_cb_t double_cb;
 
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/offload.c
