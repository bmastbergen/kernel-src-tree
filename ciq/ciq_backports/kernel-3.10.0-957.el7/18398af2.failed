nvme-rdma: disable the controller on resets

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Sagi Grimberg <sagi@grimberg.me>
commit 18398af2dcca3044c74fd2697f1d5b624b7848fe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/18398af2.failed

Mimic the pci driver as a controller disable might be more lightweight
than a shutdown.

	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 18398af2dcca3044c74fd2697f1d5b624b7848fe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index 6d8e34242c5f,b1e67cd41c81..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -1563,99 -1655,7 +1563,103 @@@ static struct blk_mq_ops nvme_rdma_admi
  	.timeout	= nvme_rdma_timeout,
  };
  
++<<<<<<< HEAD
 +static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl)
 +{
 +	int error;
 +
 +	error = nvme_rdma_init_queue(ctrl, 0, NVME_AQ_DEPTH);
 +	if (error)
 +		return error;
 +
 +	ctrl->device = ctrl->queues[0].device;
 +
 +	/*
 +	 * We need a reference on the device as long as the tag_set is alive,
 +	 * as the MRs in the request structures need a valid ib_device.
 +	 */
 +	error = -EINVAL;
 +	if (!nvme_rdma_dev_get(ctrl->device))
 +		goto out_free_queue;
 +
 +	ctrl->max_fr_pages = min_t(u32, NVME_RDMA_MAX_SEGMENTS,
 +		ctrl->device->dev->attrs.max_fast_reg_page_list_len);
 +
 +	memset(&ctrl->admin_tag_set, 0, sizeof(ctrl->admin_tag_set));
 +	ctrl->admin_tag_set.ops = &nvme_rdma_admin_mq_ops;
 +	ctrl->admin_tag_set.queue_depth = NVME_RDMA_AQ_BLKMQ_DEPTH;
 +	ctrl->admin_tag_set.reserved_tags = 2; /* connect + keep-alive */
 +	ctrl->admin_tag_set.numa_node = NUMA_NO_NODE;
 +	ctrl->admin_tag_set.cmd_size = sizeof(struct nvme_rdma_request) +
 +		SG_CHUNK_SIZE * sizeof(struct scatterlist);
 +	ctrl->admin_tag_set.driver_data = ctrl;
 +	ctrl->admin_tag_set.nr_hw_queues = 1;
 +	ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
 +
 +	error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
 +	if (error)
 +		goto out_put_dev;
 +
 +	ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
 +	if (IS_ERR(ctrl->ctrl.admin_q)) {
 +		error = PTR_ERR(ctrl->ctrl.admin_q);
 +		goto out_free_tagset;
 +	}
 +	ctrl->ctrl.admin_q->tail_queue = 1;
 +
 +	error = nvmf_connect_admin_queue(&ctrl->ctrl);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	set_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[0].flags);
 +
 +	error = nvmf_reg_read64(&ctrl->ctrl, NVME_REG_CAP,
 +			&ctrl->ctrl.cap);
 +	if (error) {
 +		dev_err(ctrl->ctrl.device,
 +			"prop_get NVME_REG_CAP failed\n");
 +		goto out_cleanup_queue;
 +	}
 +
 +	ctrl->ctrl.sqsize =
 +		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
 +
 +	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	ctrl->ctrl.max_hw_sectors =
 +		(ctrl->max_fr_pages - 1) << (ilog2(SZ_4K) - 9);
 +
 +	error = nvme_init_identify(&ctrl->ctrl);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	error = nvme_rdma_alloc_qe(ctrl->queues[0].device->dev,
 +			&ctrl->async_event_sqe, sizeof(struct nvme_command),
 +			DMA_TO_DEVICE);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	return 0;
 +
 +out_cleanup_queue:
 +	blk_cleanup_queue(ctrl->ctrl.admin_q);
 +out_free_tagset:
 +	/* disconnect and drain the queue before freeing the tagset */
 +	nvme_rdma_stop_queue(&ctrl->queues[0]);
 +	blk_mq_free_tag_set(&ctrl->admin_tag_set);
 +out_put_dev:
 +	nvme_rdma_dev_put(ctrl->device);
 +out_free_queue:
 +	nvme_rdma_free_queue(&ctrl->queues[0]);
 +	return error;
 +}
 +
 +static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl)
++=======
+ static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
++>>>>>>> 18398af2dcca (nvme-rdma: disable the controller on resets)
  {
  	cancel_work_sync(&ctrl->err_work);
  	cancel_delayed_work_sync(&ctrl->reconnect_work);
@@@ -1667,12 -1667,15 +1671,14 @@@
  		nvme_rdma_free_io_queues(ctrl);
  	}
  
- 	if (test_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[0].flags))
+ 	if (shutdown)
  		nvme_shutdown_ctrl(&ctrl->ctrl);
+ 	else
+ 		nvme_disable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
  
 -	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
 +	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
  	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
  				nvme_cancel_request, &ctrl->ctrl);
 -	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
  	nvme_rdma_destroy_admin_queue(ctrl);
  }
  
@@@ -1746,14 -1748,8 +1752,14 @@@ static void nvme_rdma_reset_ctrl_work(s
  	bool changed;
  
  	nvme_stop_ctrl(&ctrl->ctrl);
- 	nvme_rdma_shutdown_ctrl(ctrl);
+ 	nvme_rdma_shutdown_ctrl(ctrl, false);
  
 +	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RECONNECTING)) {
 +		/* state change failure should never happen */
 +		WARN_ON_ONCE(1);
 +		return;
 +	}
 +
  	ret = nvme_rdma_configure_admin_queue(ctrl);
  	if (ret) {
  		/* ctrl is already shutdown, just remove the ctrl */
* Unmerged path drivers/nvme/host/rdma.c
