md/raid10: Refactor raid10_make_request

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] raid10: Refactor raid10_make_request (Nigel Croxon) [1494474]
Rebuild_FUZZ: 96.00%
commit-author Robert LeBlanc <robert@leblancnet.us>
commit bb5f1ed70bc3bbbce510907da3432dab267ff508
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/bb5f1ed7.failed

Refactor raid10_make_request into seperate read and write functions to
clean up the code.

Shaohua: add the recovery check back to read path

	Signed-off-by: Robert LeBlanc <robert@leblancnet.us>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit bb5f1ed70bc3bbbce510907da3432dab267ff508)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid10.c
diff --cc drivers/md/raid10.c
index 6f29cb6cf257,1920756828df..000000000000
--- a/drivers/md/raid10.c
+++ b/drivers/md/raid10.c
@@@ -1162,81 -1087,123 +1162,191 @@@ static void raid10_unplug(struct blk_pl
  	kfree(plug);
  }
  
++<<<<<<< HEAD
 +static bool raid10_make_request(struct mddev *mddev, struct bio * bio)
++=======
+ static void raid10_read_request(struct mddev *mddev, struct bio *bio,
+ 				struct r10bio *r10_bio)
++>>>>>>> bb5f1ed70bc3 (md/raid10: Refactor raid10_make_request)
  {
  	struct r10conf *conf = mddev->private;
- 	struct r10bio *r10_bio;
  	struct bio *read_bio;
+ 	const int op = bio_op(bio);
+ 	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
+ 	int sectors_handled;
+ 	int max_sectors;
+ 	sector_t sectors;
+ 	struct md_rdev *rdev;
+ 	int slot;
+ 
+ 	/*
+ 	 * Register the new request and wait if the reconstruction
+ 	 * thread has put up a bar for new requests.
+ 	 * Continue immediately if no resync is active currently.
+ 	 */
+ 	wait_barrier(conf);
+ 
+ 	sectors = bio_sectors(bio);
+ 	while (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
+ 	    bio->bi_iter.bi_sector < conf->reshape_progress &&
+ 	    bio->bi_iter.bi_sector + sectors > conf->reshape_progress) {
+ 		/*
+ 		 * IO spans the reshape position.  Need to wait for reshape to
+ 		 * pass
+ 		 */
+ 		raid10_log(conf->mddev, "wait reshape");
+ 		allow_barrier(conf);
+ 		wait_event(conf->wait_barrier,
+ 			   conf->reshape_progress <= bio->bi_iter.bi_sector ||
+ 			   conf->reshape_progress >= bio->bi_iter.bi_sector +
+ 			   sectors);
+ 		wait_barrier(conf);
+ 	}
+ 
+ read_again:
+ 	rdev = read_balance(conf, r10_bio, &max_sectors);
+ 	if (!rdev) {
+ 		raid_end_bio_io(r10_bio);
+ 		return;
+ 	}
+ 	slot = r10_bio->read_slot;
+ 
+ 	read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
+ 	bio_trim(read_bio, r10_bio->sector - bio->bi_iter.bi_sector,
+ 		 max_sectors);
+ 
+ 	r10_bio->devs[slot].bio = read_bio;
+ 	r10_bio->devs[slot].rdev = rdev;
+ 
+ 	read_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +
+ 		choose_data_offset(r10_bio, rdev);
+ 	read_bio->bi_bdev = rdev->bdev;
+ 	read_bio->bi_end_io = raid10_end_read_request;
+ 	bio_set_op_attrs(read_bio, op, do_sync);
+ 	if (test_bit(FailFast, &rdev->flags) &&
+ 	    test_bit(R10BIO_FailFast, &r10_bio->state))
+ 	        read_bio->bi_opf |= MD_FAILFAST;
+ 	read_bio->bi_private = r10_bio;
+ 
+ 	if (mddev->gendisk)
+ 	        trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
+ 	                              read_bio, disk_devt(mddev->gendisk),
+ 	                              r10_bio->sector);
+ 	if (max_sectors < r10_bio->sectors) {
+ 		/*
+ 		 * Could not read all from this device, so we will need another
+ 		 * r10_bio.
+ 		 */
+ 		sectors_handled = (r10_bio->sector + max_sectors
+ 				   - bio->bi_iter.bi_sector);
+ 		r10_bio->sectors = max_sectors;
+ 		spin_lock_irq(&conf->device_lock);
+ 		if (bio->bi_phys_segments == 0)
+ 			bio->bi_phys_segments = 2;
+ 		else
+ 			bio->bi_phys_segments++;
+ 		spin_unlock_irq(&conf->device_lock);
+ 		/*
+ 		 * Cannot call generic_make_request directly as that will be
+ 		 * queued in __generic_make_request and subsequent
+ 		 * mempool_alloc might block waiting for it.  so hand bio over
+ 		 * to raid10d.
+ 		 */
+ 		reschedule_retry(r10_bio);
+ 
+ 		r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
+ 
+ 		r10_bio->master_bio = bio;
+ 		r10_bio->sectors = bio_sectors(bio) - sectors_handled;
+ 		r10_bio->state = 0;
+ 		r10_bio->mddev = mddev;
+ 		r10_bio->sector = bio->bi_iter.bi_sector + sectors_handled;
+ 		goto read_again;
+ 	} else
+ 		generic_make_request(read_bio);
+ 	return;
+ }
+ 
+ static void raid10_write_request(struct mddev *mddev, struct bio *bio,
+ 				 struct r10bio *r10_bio)
+ {
+ 	struct r10conf *conf = mddev->private;
  	int i;
++<<<<<<< HEAD
 +	sector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);
 +	int chunk_sects = chunk_mask + 1;
 +	const int rw = bio_data_dir(bio);
 +	const unsigned long do_sync = (bio->bi_rw & REQ_SYNC);
 +	const unsigned long do_fua = (bio->bi_rw & REQ_FUA);
 +	const unsigned long do_discard = (bio->bi_rw
 +					  & (REQ_DISCARD | REQ_SECURE));
 +	const unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);
++=======
+ 	const int op = bio_op(bio);
+ 	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
+ 	const unsigned long do_fua = (bio->bi_opf & REQ_FUA);
++>>>>>>> bb5f1ed70bc3 (md/raid10: Refactor raid10_make_request)
  	unsigned long flags;
  	struct md_rdev *blocked_rdev;
  	struct blk_plug_cb *cb;
  	struct raid10_plug_cb *plug = NULL;
+ 	sector_t sectors;
  	int sectors_handled;
  	int max_sectors;
- 	int sectors;
  
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
 +		md_flush_request(mddev, bio);
 +		return true;
 +	}
 +
 +	/* If this request crosses a chunk boundary, we need to
 +	 * split it.  This will only happen for 1 PAGE (or less) requests.
 +	 */
 +	if (unlikely((bio->bi_sector & chunk_mask) + bio_sectors(bio)
 +		     > chunk_sects
 +		     && (conf->geo.near_copies < conf->geo.raid_disks
 +			 || conf->prev.near_copies < conf->prev.raid_disks))) {
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		bp = bio_split(bio,
 +			       chunk_sects - (bio->bi_sector & (chunk_sects - 1)) );
 +
 +		/* Each of these 'make_request' calls will call 'wait_barrier'.
 +		 * If the first succeeds but the second blocks due to the resync
 +		 * thread raising the barrier, we will deadlock because the
 +		 * IO to the underlying device will be queued in generic_make_request
 +		 * and will never complete, so will never reduce nr_pending.
 +		 * So increment nr_waiting here so no new raise_barriers will
 +		 * succeed, and so the second wait_barrier cannot block.
 +		 */
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting++;
 +		spin_unlock_irq(&conf->resync_lock);
 +
 +		raid10_make_request(mddev, &bp->bio1);
 +		raid10_make_request(mddev, &bp->bio2);
 +
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting--;
 +		wake_up(&conf->wait_barrier);
 +		spin_unlock_irq(&conf->resync_lock);
 +
 +		bio_pair_release(bp);
 +		return true;
 +	bad_map:
 +		printk("md/raid10:%s: make_request bug: can't convert block across chunks"
 +		       " or bigger than %dk %llu %d\n", mdname(mddev), chunk_sects/2,
 +		       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
 +
 +		bio_io_error(bio);
 +		return true;
 +	}
 +
  	md_write_start(mddev, bio);
  
  	/*
@@@ -1248,25 -1215,27 +1358,33 @@@
  
  	sectors = bio_sectors(bio);
  	while (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
++<<<<<<< HEAD
 +	    bio->bi_sector < conf->reshape_progress &&
 +	    bio->bi_sector + sectors > conf->reshape_progress) {
 +		/* IO spans the reshape position.  Need to wait for
 +		 * reshape to pass
++=======
+ 	    bio->bi_iter.bi_sector < conf->reshape_progress &&
+ 	    bio->bi_iter.bi_sector + sectors > conf->reshape_progress) {
+ 		/*
+ 		 * IO spans the reshape position.  Need to wait for reshape to
+ 		 * pass
++>>>>>>> bb5f1ed70bc3 (md/raid10: Refactor raid10_make_request)
  		 */
 -		raid10_log(conf->mddev, "wait reshape");
  		allow_barrier(conf);
  		wait_event(conf->wait_barrier,
 -			   conf->reshape_progress <= bio->bi_iter.bi_sector ||
 -			   conf->reshape_progress >= bio->bi_iter.bi_sector +
 -			   sectors);
 +			   conf->reshape_progress <= bio->bi_sector ||
 +			   conf->reshape_progress >= bio->bi_sector + sectors);
  		wait_barrier(conf);
  	}
+ 
  	if (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
- 	    bio_data_dir(bio) == WRITE &&
  	    (mddev->reshape_backwards
 -	     ? (bio->bi_iter.bi_sector < conf->reshape_safe &&
 -		bio->bi_iter.bi_sector + sectors > conf->reshape_progress)
 -	     : (bio->bi_iter.bi_sector + sectors > conf->reshape_safe &&
 -		bio->bi_iter.bi_sector < conf->reshape_progress))) {
 +	     ? (bio->bi_sector < conf->reshape_safe &&
 +		bio->bi_sector + sectors > conf->reshape_progress)
 +	     : (bio->bi_sector + sectors > conf->reshape_safe &&
 +		bio->bi_sector < conf->reshape_progress))) {
 +		gmb();
  		/* Need to update reshape_position in metadata */
  		mddev->reshape_position = conf->reshape_progress;
  		set_mask_bits(&mddev->sb_flags, 0,
@@@ -1278,95 -1248,9 +1396,98 @@@
  		conf->reshape_safe = mddev->reshape_position;
  	}
  
++<<<<<<< HEAD
 +	r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
 +
 +	r10_bio->master_bio = bio;
 +	r10_bio->sectors = sectors;
 +
 +	r10_bio->mddev = mddev;
 +	r10_bio->sector = bio->bi_sector;
 +	r10_bio->state = 0;
 +
 +	/* We might need to issue multiple reads to different
 +	 * devices if there are bad blocks around, so we keep
 +	 * track of the number of reads in bio->bi_phys_segments.
 +	 * If this is 0, there is only one r10_bio and no locking
 +	 * will be needed when the request completes.  If it is
 +	 * non-zero, then it is the number of not-completed requests.
 +	 */
 +	bio->bi_phys_segments = 0;
 +	clear_bit(BIO_SEG_VALID, &bio->bi_flags);
 +
 +	if (rw == READ) {
 +		/*
 +		 * read balancing logic:
 +		 */
 +		struct md_rdev *rdev;
 +		int slot;
 +
 +read_again:
 +		rdev = read_balance(conf, r10_bio, &max_sectors);
 +		if (!rdev) {
 +			raid_end_bio_io(r10_bio);
 +			return true;
 +		}
 +		slot = r10_bio->read_slot;
 +
 +		read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
 +		bio_trim(read_bio, r10_bio->sector - bio->bi_sector,
 +			 max_sectors);
 +
 +		r10_bio->devs[slot].bio = read_bio;
 +		r10_bio->devs[slot].rdev = rdev;
 +
 +		read_bio->bi_sector = r10_bio->devs[slot].addr +
 +			choose_data_offset(r10_bio, rdev);
 +		read_bio->bi_bdev = rdev->bdev;
 +		read_bio->bi_end_io = raid10_end_read_request;
 +		read_bio->bi_rw = READ | do_sync;
 +		if (test_bit(FailFast, &rdev->flags) &&
 +		    test_bit(R10BIO_FailFast, &r10_bio->state))
 +			read_bio->bi_rw |= MD_FAILFAST;
 +		read_bio->bi_private = r10_bio;
 +
 +		if (max_sectors < r10_bio->sectors) {
 +			/* Could not read all from this device, so we will
 +			 * need another r10_bio.
 +			 */
 +			sectors_handled = (r10_bio->sector + max_sectors
 +					   - bio->bi_sector);
 +			r10_bio->sectors = max_sectors;
 +			spin_lock_irq(&conf->device_lock);
 +			if (bio->bi_phys_segments == 0)
 +				bio->bi_phys_segments = 2;
 +			else
 +				bio->bi_phys_segments++;
 +			spin_unlock_irq(&conf->device_lock);
 +			/* Cannot call generic_make_request directly
 +			 * as that will be queued in __generic_make_request
 +			 * and subsequent mempool_alloc might block
 +			 * waiting for it.  so hand bio over to raid10d.
 +			 */
 +			reschedule_retry(r10_bio);
 +
 +			r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
 +
 +			r10_bio->master_bio = bio;
 +			r10_bio->sectors = bio_sectors(bio) - sectors_handled;
 +			r10_bio->state = 0;
 +			r10_bio->mddev = mddev;
 +			r10_bio->sector = bio->bi_sector + sectors_handled;
 +			goto read_again;
 +		} else
 +			generic_make_request(read_bio);
 +		return true;
 +	}
 +
 +	/*
 +	 * WRITE:
 +	 */
++=======
++>>>>>>> bb5f1ed70bc3 (md/raid10: Refactor raid10_make_request)
  	if (conf->pending_count >= max_queued_requests) {
  		md_wakeup_thread(mddev->thread);
 -		raid10_log(mddev, "wait queued");
  		wait_event(conf->wait_barrier,
  			   conf->pending_count < max_queued_requests);
  	}
@@@ -1518,18 -1411,23 +1638,23 @@@ retry_write
  				 max_sectors);
  			r10_bio->devs[i].bio = mbio;
  
++<<<<<<< HEAD
 +			mbio->bi_sector	= (r10_bio->devs[i].addr+
 +					   choose_data_offset(r10_bio,
 +							      rdev));
++=======
+ 			mbio->bi_iter.bi_sector	= (r10_bio->devs[i].addr+
+ 					   choose_data_offset(r10_bio, rdev));
++>>>>>>> bb5f1ed70bc3 (md/raid10: Refactor raid10_make_request)
  			mbio->bi_bdev = rdev->bdev;
  			mbio->bi_end_io	= raid10_end_write_request;
 -			bio_set_op_attrs(mbio, op, do_sync | do_fua);
 +			mbio->bi_rw =
 +				WRITE | do_sync | do_fua | do_discard | do_same;
  			if (test_bit(FailFast, &conf->mirrors[d].rdev->flags) &&
  			    enough(conf, d))
 -				mbio->bi_opf |= MD_FAILFAST;
 +				mbio->bi_rw |= MD_FAILFAST;
  			mbio->bi_private = r10_bio;
  
 -			if (conf->mddev->gendisk)
 -				trace_block_bio_remap(bdev_get_queue(mbio->bi_bdev),
 -						      mbio, disk_devt(conf->mddev->gendisk),
 -						      r10_bio->sector);
 -			/* flush_pending_writes() needs access to the rdev so...*/
 -			mbio->bi_bdev = (void*)rdev;
 -
  			atomic_inc(&r10_bio->remaining);
  
  			cb = blk_check_plugged(raid10_unplug, mddev,
@@@ -1564,15 -1462,20 +1689,20 @@@
  				 max_sectors);
  			r10_bio->devs[i].repl_bio = mbio;
  
++<<<<<<< HEAD
 +			mbio->bi_sector	= (r10_bio->devs[i].addr +
 +					   choose_data_offset(
 +						   r10_bio, rdev));
++=======
+ 			mbio->bi_iter.bi_sector	= (r10_bio->devs[i].addr +
+ 					   choose_data_offset(r10_bio, rdev));
++>>>>>>> bb5f1ed70bc3 (md/raid10: Refactor raid10_make_request)
  			mbio->bi_bdev = rdev->bdev;
  			mbio->bi_end_io	= raid10_end_write_request;
 -			bio_set_op_attrs(mbio, op, do_sync | do_fua);
 +			mbio->bi_rw =
 +				WRITE | do_sync | do_fua | do_discard | do_same;
  			mbio->bi_private = r10_bio;
  
 -			if (conf->mddev->gendisk)
 -				trace_block_bio_remap(bdev_get_queue(mbio->bi_bdev),
 -						      mbio, disk_devt(conf->mddev->gendisk),
 -						      r10_bio->sector);
 -			/* flush_pending_writes() needs access to the rdev so...*/
 -			mbio->bi_bdev = (void*)rdev;
 -
  			atomic_inc(&r10_bio->remaining);
  			spin_lock_irqsave(&conf->device_lock, flags);
  			bio_list_add(&conf->pending_bio_list, mbio);
@@@ -1609,6 -1506,73 +1739,76 @@@
  		goto retry_write;
  	}
  	one_write_done(r10_bio);
++<<<<<<< HEAD
++=======
+ }
+ 
+ static void __make_request(struct mddev *mddev, struct bio *bio)
+ {
+ 	struct r10conf *conf = mddev->private;
+ 	struct r10bio *r10_bio;
+ 
+ 	r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
+ 
+ 	r10_bio->master_bio = bio;
+ 	r10_bio->sectors = bio_sectors(bio);
+ 
+ 	r10_bio->mddev = mddev;
+ 	r10_bio->sector = bio->bi_iter.bi_sector;
+ 	r10_bio->state = 0;
+ 
+ 	/*
+ 	 * We might need to issue multiple reads to different devices if there
+ 	 * are bad blocks around, so we keep track of the number of reads in
+ 	 * bio->bi_phys_segments.  If this is 0, there is only one r10_bio and
+ 	 * no locking will be needed when the request completes.  If it is
+ 	 * non-zero, then it is the number of not-completed requests.
+ 	 */
+ 	bio->bi_phys_segments = 0;
+ 	bio_clear_flag(bio, BIO_SEG_VALID);
+ 
+ 	if (bio_data_dir(bio) == READ)
+ 		raid10_read_request(mddev, bio, r10_bio);
+ 	else
+ 		raid10_write_request(mddev, bio, r10_bio);
+ }
+ 
+ static void raid10_make_request(struct mddev *mddev, struct bio *bio)
+ {
+ 	struct r10conf *conf = mddev->private;
+ 	sector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);
+ 	int chunk_sects = chunk_mask + 1;
+ 
+ 	struct bio *split;
+ 
+ 	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
+ 		md_flush_request(mddev, bio);
+ 		return;
+ 	}
+ 
+ 	do {
+ 
+ 		/*
+ 		 * If this request crosses a chunk boundary, we need to split
+ 		 * it.
+ 		 */
+ 		if (unlikely((bio->bi_iter.bi_sector & chunk_mask) +
+ 			     bio_sectors(bio) > chunk_sects
+ 			     && (conf->geo.near_copies < conf->geo.raid_disks
+ 				 || conf->prev.near_copies <
+ 				 conf->prev.raid_disks))) {
+ 			split = bio_split(bio, chunk_sects -
+ 					  (bio->bi_iter.bi_sector &
+ 					   (chunk_sects - 1)),
+ 					  GFP_NOIO, fs_bio_set);
+ 			bio_chain(split, bio);
+ 		} else {
+ 			split = bio;
+ 		}
+ 
+ 		__make_request(mddev, split);
+ 	} while (split != bio);
++>>>>>>> bb5f1ed70bc3 (md/raid10: Refactor raid10_make_request)
  
  	/* In case raid10d snuck in to freeze_array */
  	wake_up(&conf->wait_barrier);
* Unmerged path drivers/md/raid10.c
