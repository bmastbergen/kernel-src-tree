iommu/iova: Add locking to Flush-Queues

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iommu] iova: Add locking to Flush-Queues (Jerry Snitselaar) [1519117]
Rebuild_FUZZ: 91.67%
commit-author Joerg Roedel <jroedel@suse.de>
commit 8109c2a2f8463852dddd6a1c3fcf262047c0c124
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/8109c2a2.failed

The lock is taken from the same CPU most of the time. But
having it allows to flush the queue also from another CPU if
necessary.

This will be used by a timer to regularily flush any pending
IOVAs from the Flush-Queues.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 8109c2a2f8463852dddd6a1c3fcf262047c0c124)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
#	include/linux/iova.h
diff --cc drivers/iommu/iova.c
index f106fd9782bf,749d39533e0b..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -53,6 -57,48 +53,51 @@@ init_iova_domain(struct iova_domain *io
  }
  EXPORT_SYMBOL_GPL(init_iova_domain);
  
++<<<<<<< HEAD
++=======
+ static void free_iova_flush_queue(struct iova_domain *iovad)
+ {
+ 	if (!iovad->fq)
+ 		return;
+ 
+ 	fq_destroy_all_entries(iovad);
+ 	free_percpu(iovad->fq);
+ 
+ 	iovad->fq         = NULL;
+ 	iovad->flush_cb   = NULL;
+ 	iovad->entry_dtor = NULL;
+ }
+ 
+ int init_iova_flush_queue(struct iova_domain *iovad,
+ 			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor)
+ {
+ 	int cpu;
+ 
+ 	atomic64_set(&iovad->fq_flush_start_cnt,  0);
+ 	atomic64_set(&iovad->fq_flush_finish_cnt, 0);
+ 
+ 	iovad->fq = alloc_percpu(struct iova_fq);
+ 	if (!iovad->fq)
+ 		return -ENOMEM;
+ 
+ 	iovad->flush_cb   = flush_cb;
+ 	iovad->entry_dtor = entry_dtor;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq;
+ 
+ 		fq = per_cpu_ptr(iovad->fq, cpu);
+ 		fq->head = 0;
+ 		fq->tail = 0;
+ 
+ 		spin_lock_init(&fq->lock);
+ 	}
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(init_iova_flush_queue);
+ 
++>>>>>>> 8109c2a2f846 (iommu/iova: Add locking to Flush-Queues)
  static struct rb_node *
  __get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)
  {
@@@ -449,6 -468,111 +494,114 @@@ free_iova_fast(struct iova_domain *iova
  }
  EXPORT_SYMBOL_GPL(free_iova_fast);
  
++<<<<<<< HEAD
++=======
+ #define fq_ring_for_each(i, fq) \
+ 	for ((i) = (fq)->head; (i) != (fq)->tail; (i) = ((i) + 1) % IOVA_FQ_SIZE)
+ 
+ static inline bool fq_full(struct iova_fq *fq)
+ {
+ 	assert_spin_locked(&fq->lock);
+ 	return (((fq->tail + 1) % IOVA_FQ_SIZE) == fq->head);
+ }
+ 
+ static inline unsigned fq_ring_add(struct iova_fq *fq)
+ {
+ 	unsigned idx = fq->tail;
+ 
+ 	assert_spin_locked(&fq->lock);
+ 
+ 	fq->tail = (idx + 1) % IOVA_FQ_SIZE;
+ 
+ 	return idx;
+ }
+ 
+ static void fq_ring_free(struct iova_domain *iovad, struct iova_fq *fq)
+ {
+ 	u64 counter = atomic64_read(&iovad->fq_flush_finish_cnt);
+ 	unsigned idx;
+ 
+ 	assert_spin_locked(&fq->lock);
+ 
+ 	fq_ring_for_each(idx, fq) {
+ 
+ 		if (fq->entries[idx].counter >= counter)
+ 			break;
+ 
+ 		if (iovad->entry_dtor)
+ 			iovad->entry_dtor(fq->entries[idx].data);
+ 
+ 		free_iova_fast(iovad,
+ 			       fq->entries[idx].iova_pfn,
+ 			       fq->entries[idx].pages);
+ 
+ 		fq->head = (fq->head + 1) % IOVA_FQ_SIZE;
+ 	}
+ }
+ 
+ static void iova_domain_flush(struct iova_domain *iovad)
+ {
+ 	atomic64_inc(&iovad->fq_flush_start_cnt);
+ 	iovad->flush_cb(iovad);
+ 	atomic64_inc(&iovad->fq_flush_finish_cnt);
+ }
+ 
+ static void fq_destroy_all_entries(struct iova_domain *iovad)
+ {
+ 	int cpu;
+ 
+ 	/*
+ 	 * This code runs when the iova_domain is being detroyed, so don't
+ 	 * bother to free iovas, just call the entry_dtor on all remaining
+ 	 * entries.
+ 	 */
+ 	if (!iovad->entry_dtor)
+ 		return;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq = per_cpu_ptr(iovad->fq, cpu);
+ 		int idx;
+ 
+ 		fq_ring_for_each(idx, fq)
+ 			iovad->entry_dtor(fq->entries[idx].data);
+ 	}
+ }
+ 
+ void queue_iova(struct iova_domain *iovad,
+ 		unsigned long pfn, unsigned long pages,
+ 		unsigned long data)
+ {
+ 	struct iova_fq *fq = get_cpu_ptr(iovad->fq);
+ 	unsigned long flags;
+ 	unsigned idx;
+ 
+ 	spin_lock_irqsave(&fq->lock, flags);
+ 
+ 	/*
+ 	 * First remove all entries from the flush queue that have already been
+ 	 * flushed out on another CPU. This makes the fq_full() check below less
+ 	 * likely to be true.
+ 	 */
+ 	fq_ring_free(iovad, fq);
+ 
+ 	if (fq_full(fq)) {
+ 		iova_domain_flush(iovad);
+ 		fq_ring_free(iovad, fq);
+ 	}
+ 
+ 	idx = fq_ring_add(fq);
+ 
+ 	fq->entries[idx].iova_pfn = pfn;
+ 	fq->entries[idx].pages    = pages;
+ 	fq->entries[idx].data     = data;
+ 	fq->entries[idx].counter  = atomic64_read(&iovad->fq_flush_start_cnt);
+ 
+ 	spin_unlock_irqrestore(&fq->lock, flags);
+ 	put_cpu_ptr(iovad->fq);
+ }
+ EXPORT_SYMBOL_GPL(queue_iova);
+ 
++>>>>>>> 8109c2a2f846 (iommu/iova: Add locking to Flush-Queues)
  /**
   * put_iova_domain - destroys the iova doamin
   * @iovad: - iova domain in question.
diff --cc include/linux/iova.h
index f27bb2c62fca,913a690cd4b0..000000000000
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@@ -36,6 -37,32 +36,35 @@@ struct iova_rcache 
  	struct iova_cpu_rcache __percpu *cpu_rcaches;
  };
  
++<<<<<<< HEAD
++=======
+ struct iova_domain;
+ 
+ /* Call-Back from IOVA code into IOMMU drivers */
+ typedef void (* iova_flush_cb)(struct iova_domain *domain);
+ 
+ /* Destructor for per-entry data */
+ typedef void (* iova_entry_dtor)(unsigned long data);
+ 
+ /* Number of entries per Flush Queue */
+ #define IOVA_FQ_SIZE	256
+ 
+ /* Flush Queue entry for defered flushing */
+ struct iova_fq_entry {
+ 	unsigned long iova_pfn;
+ 	unsigned long pages;
+ 	unsigned long data;
+ 	u64 counter; /* Flush counter when this entrie was added */
+ };
+ 
+ /* Per-CPU Flush Queue structure */
+ struct iova_fq {
+ 	struct iova_fq_entry entries[IOVA_FQ_SIZE];
+ 	unsigned head, tail;
+ 	spinlock_t lock;
+ };
+ 
++>>>>>>> 8109c2a2f846 (iommu/iova: Add locking to Flush-Queues)
  /* holds all the iova translations for a domain */
  struct iova_domain {
  	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
* Unmerged path drivers/iommu/iova.c
* Unmerged path include/linux/iova.h
