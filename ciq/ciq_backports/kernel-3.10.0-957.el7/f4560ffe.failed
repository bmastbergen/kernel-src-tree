blk-mq: use QUEUE_FLAG_QUIESCED to quiesce queue

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [blk-mq] use QUEUE_FLAG_QUIESCED to quiesce queue (Ewan Milne) [1549232]
Rebuild_FUZZ: 90.91%
commit-author Ming Lei <ming.lei@redhat.com>
commit f4560ffe8cec1361b1021d81aca6a4173f8e7c87
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/f4560ffe.failed

It is required that no dispatch can happen any more once
blk_mq_quiesce_queue() returns, and we don't have such requirement
on APIs of stopping queue.

But blk_mq_quiesce_queue() still may not block/drain dispatch in the
the case of BLK_MQ_S_START_ON_RUN, so use the new introduced flag of
QUEUE_FLAG_QUIESCED and evaluate it inside RCU read-side critical
sections for fixing this issue.

Also blk_mq_quiesce_queue() is implemented via stopping queue, which
limits its uses, and easy to cause race, because any queue restart in
other paths may break blk_mq_quiesce_queue(). With the introduced
flag of QUEUE_FLAG_QUIESCED, we don't need to depend on stopping queue
for quiescing any more.

	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit f4560ffe8cec1361b1021d81aca6a4173f8e7c87)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/blkdev.h
diff --cc include/linux/blkdev.h
index 2b624b97bb35,22cfba64ce81..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -598,14 -609,17 +598,28 @@@ struct request_queue 
  #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
  #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
  #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
++<<<<<<< HEAD
 +#define QUEUE_FLAG_UNPRIV_SGIO 21	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NO_SG_MERGE 22	/* don't attempt to merge SG segments*/
 +#define QUEUE_FLAG_SG_GAPS     23	/* queue doesn't support SG gaps */
 +#define QUEUE_FLAG_DAX         24	/* device supports DAX */
 +#define QUEUE_FLAG_REGISTERED  25	/* queue has been registered to a disk */
 +#define QUEUE_FLAG_STATS       26	/* track rq completion times */
 +#define QUEUE_FLAG_POLL_STATS  27	/* collecting stats for hybrid polling */
 +#define QUEUE_FLAG_PREEMPT_ONLY	28	/* only process REQ_PREEMPT requests */
++=======
+ #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
+ #define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
+ #define QUEUE_FLAG_WC	       23	/* Write back caching */
+ #define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
+ #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
+ #define QUEUE_FLAG_DAX         26	/* device supports DAX */
+ #define QUEUE_FLAG_STATS       27	/* track rq completion times */
+ #define QUEUE_FLAG_POLL_STATS  28	/* collecting stats for hybrid polling */
+ #define QUEUE_FLAG_REGISTERED  29	/* queue has been registered to a disk */
+ #define QUEUE_FLAG_SCSI_PASSTHROUGH 30	/* queue supports SCSI commands */
+ #define QUEUE_FLAG_QUIESCED    31	/* queue has been quiesced */
++>>>>>>> f4560ffe8cec (blk-mq: use QUEUE_FLAG_QUIESCED to quiesce queue)
  
  #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
  				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@@ -706,14 -716,12 +720,15 @@@ extern void blk_clear_preempt_only(stru
  #define blk_noretry_request(rq) \
  	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
  			     REQ_FAILFAST_DRIVER))
+ #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
  
 -static inline bool blk_account_rq(struct request *rq)
 -{
 -	return (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);
 -}
 +#define blk_account_rq(rq) \
 +	(((rq)->cmd_flags & REQ_STARTED) && \
 +	 ((rq)->cmd_type == REQ_TYPE_FS))
 +
 +#define blk_pm_request(rq)	\
 +	((rq)->cmd_type == REQ_TYPE_PM_SUSPEND || \
 +	 (rq)->cmd_type == REQ_TYPE_PM_RESUME)
  
  #define blk_rq_cpu_valid(rq)	((rq)->cpu != -1)
  #define blk_bidi_rq(rq)		((rq)->next_rq != NULL)
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 6673285afaf1..0d5cbe005db8 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -268,7 +268,8 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	const bool has_sched_dispatch = e && e->aux->ops.mq.dispatch_request;
 	LIST_HEAD(rq_list);
 
-	if (unlikely(blk_mq_hctx_stopped(hctx)))
+	/* RCU or SRCU read lock is needed before checking quiesced flag */
+	if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
 		return;
 
 	hctx->run++;
diff --git a/block/blk-mq.c b/block/blk-mq.c
index ba64dbbf5824..4cb867f5b2ca 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -151,6 +151,10 @@ void blk_mq_quiesce_queue(struct request_queue *q)
 
 	blk_mq_stop_hw_queues(q);
 
+	spin_lock_irq(q->queue_lock);
+	queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+	spin_unlock_irq(q->queue_lock);
+
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (hctx->flags & BLK_MQ_F_BLOCKING)
 			synchronize_srcu(&hctx->queue_rq_srcu);
@@ -171,6 +175,10 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);
  */
 void blk_mq_unquiesce_queue(struct request_queue *q)
 {
+	spin_lock_irq(q->queue_lock);
+	queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+	spin_unlock_irq(q->queue_lock);
+
 	blk_mq_start_stopped_hw_queues(q, true);
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
@@ -1513,7 +1521,8 @@ static void __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	int ret;
 	bool run_queue = true;
 
-	if (blk_mq_hctx_stopped(hctx)) {
+	/* RCU or SRCU read lock is needed before checking quiesced flag */
+	if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
 		run_queue = false;
 		goto insert;
 	}
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 5369c1639875..1461abfb1795 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -346,6 +346,10 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);
  */
 static inline void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
+	spin_lock_irq(q->queue_lock);
+	queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+	spin_unlock_irq(q->queue_lock);
+
 	blk_mq_stop_hw_queues(q);
 }
 
* Unmerged path include/linux/blkdev.h
