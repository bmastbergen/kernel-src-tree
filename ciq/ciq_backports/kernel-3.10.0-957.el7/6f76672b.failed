crypto: chelsio - Remove declaration of static function from header

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [crypto] chelsio - Remove declaration of static function from header (Arjun Vynipadath) [1523191]
Rebuild_FUZZ: 93.65%
commit-author Harsh Jain <harsh@chelsio.com>
commit 6f76672bd65039d68197be12653473cb4529741f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6f76672b.failed

It fixes compilation warning introduced in commit

Fixes: 5110e65536f3 ("crypto: chelsio - Split Hash requests for...")
	Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 6f76672bd65039d68197be12653473cb4529741f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/chelsio/chcr_crypto.h
diff --cc drivers/crypto/chelsio/chcr_algo.c
index fb08cbb8fadb,752ed9b25284..000000000000
mode 100755,100644..100755
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -101,22 -128,52 +101,63 @@@ static inline struct uld_ctx *ULD_CTX(s
  
  static inline int is_ofld_imm(const struct sk_buff *skb)
  {
 -	return (skb->len <= SGE_MAX_WR_LEN);
 +	return (skb->len <= CRYPTO_MAX_IMM_TX_PKT_LEN);
  }
  
 -static inline void chcr_init_hctx_per_wr(struct chcr_ahash_req_ctx *reqctx)
 +/*
 + *	sgl_len - calculates the size of an SGL of the given capacity
 + *	@n: the number of SGL entries
 + *	Calculates the number of flits needed for a scatter/gather list that
 + *	can hold the given number of entries.
 + */
 +static inline unsigned int sgl_len(unsigned int n)
  {
 -	memset(&reqctx->hctx_wr, 0, sizeof(struct chcr_hctx_per_wr));
 +	n--;
 +	return (3 * n) / 2 + (n & 1) + 2;
  }
  
++<<<<<<< HEAD
 +static void chcr_verify_tag(struct aead_request *req, u8 *input, int *err)
++=======
+ static int sg_nents_xlen(struct scatterlist *sg, unsigned int reqlen,
+ 			 unsigned int entlen,
+ 			 unsigned int skip)
+ {
+ 	int nents = 0;
+ 	unsigned int less;
+ 	unsigned int skip_len = 0;
+ 
+ 	while (sg && skip) {
+ 		if (sg_dma_len(sg) <= skip) {
+ 			skip -= sg_dma_len(sg);
+ 			skip_len = 0;
+ 			sg = sg_next(sg);
+ 		} else {
+ 			skip_len = skip;
+ 			skip = 0;
+ 		}
+ 	}
+ 
+ 	while (sg && reqlen) {
+ 		less = min(reqlen, sg_dma_len(sg) - skip_len);
+ 		nents += DIV_ROUND_UP(less, entlen);
+ 		reqlen -= less;
+ 		skip_len = 0;
+ 		sg = sg_next(sg);
+ 	}
+ 	return nents;
+ }
+ 
+ static inline int get_aead_subtype(struct crypto_aead *aead)
+ {
+ 	struct aead_alg *alg = crypto_aead_alg(aead);
+ 	struct chcr_alg_template *chcr_crypto_alg =
+ 		container_of(alg, struct chcr_alg_template, alg.aead);
+ 	return chcr_crypto_alg->type & CRYPTO_ALG_SUB_TYPE_MASK;
+ }
+ 
+ void chcr_verify_tag(struct aead_request *req, u8 *input, int *err)
++>>>>>>> 6f76672bd650 (crypto: chelsio - Remove declaration of static function from header)
  {
  	u8 temp[SHA512_DIGEST_SIZE];
  	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
@@@ -140,100 -198,26 +181,123 @@@
  		*err = 0;
  }
  
++<<<<<<< HEAD
 +/*
 + *	chcr_handle_resp - Unmap the DMA buffers associated with the request
 + *	@req: crypto request
 + */
 +int chcr_handle_resp(struct crypto_async_request *req, unsigned char *input,
 +			 int err)
 +{
 +	struct crypto_tfm *tfm = req->tfm;
 +	struct chcr_context *ctx = crypto_tfm_ctx(tfm);
 +	struct uld_ctx *u_ctx = ULD_CTX(ctx);
 +	struct chcr_req_ctx ctx_req;
 +	unsigned int digestsize, updated_digestsize;
 +	struct adapter *adap = padap(ctx->dev);
 +
 +	switch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {
 +	case CRYPTO_ALG_TYPE_AEAD:
 +		ctx_req.req.aead_req = aead_request_cast(req);
 +		ctx_req.ctx.reqctx = aead_request_ctx(ctx_req.req.aead_req);
 +		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.ctx.reqctx->dst,
 +			     ctx_req.ctx.reqctx->dst_nents, DMA_FROM_DEVICE);
 +		if (ctx_req.ctx.reqctx->skb) {
 +			kfree_skb(ctx_req.ctx.reqctx->skb);
 +			ctx_req.ctx.reqctx->skb = NULL;
 +		}
 +		free_new_sg(ctx_req.ctx.reqctx->newdstsg);
 +		ctx_req.ctx.reqctx->newdstsg = NULL;
 +		if (ctx_req.ctx.reqctx->verify == VERIFY_SW) {
 +			chcr_verify_tag(ctx_req.req.aead_req, input,
 +					&err);
 +			ctx_req.ctx.reqctx->verify = VERIFY_HW;
 +		}
 +		ctx_req.req.aead_req->base.complete(req, err);
 +		break;
 +
 +	case CRYPTO_ALG_TYPE_ABLKCIPHER:
 +		 err = chcr_handle_cipher_resp(ablkcipher_request_cast(req),
 +					       input, err);
 +		break;
 +
 +	case CRYPTO_ALG_TYPE_AHASH:
 +		ctx_req.req.ahash_req = ahash_request_cast(req);
 +		ctx_req.ctx.ahash_ctx =
 +			ahash_request_ctx(ctx_req.req.ahash_req);
 +		digestsize =
 +			crypto_ahash_digestsize(crypto_ahash_reqtfm(
 +							ctx_req.req.ahash_req));
 +		updated_digestsize = digestsize;
 +		if (digestsize == SHA224_DIGEST_SIZE)
 +			updated_digestsize = SHA256_DIGEST_SIZE;
 +		else if (digestsize == SHA384_DIGEST_SIZE)
 +			updated_digestsize = SHA512_DIGEST_SIZE;
 +		if (ctx_req.ctx.ahash_ctx->skb) {
 +			kfree_skb(ctx_req.ctx.ahash_ctx->skb);
 +			ctx_req.ctx.ahash_ctx->skb = NULL;
 +		}
 +		if (ctx_req.ctx.ahash_ctx->result == 1) {
 +			ctx_req.ctx.ahash_ctx->result = 0;
 +			memcpy(ctx_req.req.ahash_req->result, input +
 +			       sizeof(struct cpl_fw6_pld),
 +			       digestsize);
 +		} else {
 +			memcpy(ctx_req.ctx.ahash_ctx->partial_hash, input +
 +			       sizeof(struct cpl_fw6_pld),
 +			       updated_digestsize);
 +		}
 +		ctx_req.req.ahash_req->base.complete(req, err);
 +		break;
 +	}
 +	atomic_inc(&adap->chcr_stats.complete);
 +	return err;
 +}
 +
 +/*
 + *	calc_tx_flits_ofld - calculate # of flits for an offload packet
 + *	@skb: the packet
 + *	Returns the number of flits needed for the given offload packet.
 + *	These packets are already fully constructed and no additional headers
 + *	will be added.
 + */
 +static inline unsigned int calc_tx_flits_ofld(const struct sk_buff *skb)
 +{
 +	unsigned int flits, cnt;
 +
 +	if (is_ofld_imm(skb))
 +		return DIV_ROUND_UP(skb->len, 8);
 +
 +	flits = skb_transport_offset(skb) / 8;   /* headers */
 +	cnt = skb_shinfo(skb)->nr_frags;
 +	if (skb_tail_pointer(skb) != skb_transport_header(skb))
 +		cnt++;
 +	return flits + sgl_len(cnt);
 +}
 +
 +static inline void get_aes_decrypt_key(unsigned char *dec_key,
++=======
+ static inline void chcr_handle_aead_resp(struct aead_request *req,
+ 					 unsigned char *input,
+ 					 int err)
+ {
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct uld_ctx *u_ctx = ULD_CTX(a_ctx(tfm));
+ 
+ 	chcr_aead_dma_unmap(&u_ctx->lldi.pdev->dev, req, reqctx->op);
+ 	if (reqctx->b0_dma)
+ 		dma_unmap_single(&u_ctx->lldi.pdev->dev, reqctx->b0_dma,
+ 				 reqctx->b0_len, DMA_BIDIRECTIONAL);
+ 	if (reqctx->verify == VERIFY_SW) {
+ 		chcr_verify_tag(req, input, &err);
+ 		reqctx->verify = VERIFY_HW;
+ 	}
+ 	req->base.complete(&req->base, err);
+ }
+ 
+ static void get_aes_decrypt_key(unsigned char *dec_key,
++>>>>>>> 6f76672bd650 (crypto: chelsio - Remove declaration of static function from header)
  				       const unsigned char *key,
  				       unsigned int keylength)
  {
@@@ -1636,15 -1848,165 +1700,160 @@@ static int chcr_ahash_digest(struct aha
  	}
  
  	skb = create_hash_wr(req, &params);
 -	if (IS_ERR(skb)) {
 -		error = PTR_ERR(skb);
 -		goto unmap;
 -	}
 -	req_ctx->hctx_wr.processed += params.sg_len;
 +	if (IS_ERR(skb))
 +		return PTR_ERR(skb);
 +
  	skb->dev = u_ctx->lldi.ports[0];
 -	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
 +	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
  	chcr_send_wr(skb);
  	return -EINPROGRESS;
 -unmap:
 -	chcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);
 -	return error;
  }
  
+ static int chcr_ahash_continue(struct ahash_request *req)
+ {
+ 	struct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);
+ 	struct chcr_hctx_per_wr *hctx_wr = &reqctx->hctx_wr;
+ 	struct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);
+ 	struct uld_ctx *u_ctx = NULL;
+ 	struct sk_buff *skb;
+ 	struct hash_wr_param params;
+ 	u8  bs;
+ 	int error;
+ 
+ 	bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));
+ 	u_ctx = ULD_CTX(h_ctx(rtfm));
+ 	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 					    h_ctx(rtfm)->tx_qidx))) {
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
+ 			return -EBUSY;
+ 	}
+ 	get_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));
+ 	params.kctx_len = roundup(params.alg_prm.result_size, 16);
+ 	if (is_hmac(crypto_ahash_tfm(rtfm))) {
+ 		params.kctx_len *= 2;
+ 		params.opad_needed = 1;
+ 	} else {
+ 		params.opad_needed = 0;
+ 	}
+ 	params.sg_len = chcr_hash_ent_in_wr(hctx_wr->srcsg, 0,
+ 					    HASH_SPACE_LEFT(params.kctx_len),
+ 					    hctx_wr->src_ofst);
+ 	if ((params.sg_len + hctx_wr->processed) > req->nbytes)
+ 		params.sg_len = req->nbytes - hctx_wr->processed;
+ 	if (!hctx_wr->result ||
+ 	    ((params.sg_len + hctx_wr->processed) < req->nbytes)) {
+ 		if (is_hmac(crypto_ahash_tfm(rtfm))) {
+ 			params.kctx_len /= 2;
+ 			params.opad_needed = 0;
+ 		}
+ 		params.last = 0;
+ 		params.more = 1;
+ 		params.sg_len = rounddown(params.sg_len, bs);
+ 		params.hash_size = params.alg_prm.result_size;
+ 		params.scmd1 = 0;
+ 	} else {
+ 		params.last = 1;
+ 		params.more = 0;
+ 		params.hash_size = crypto_ahash_digestsize(rtfm);
+ 		params.scmd1 = reqctx->data_len + params.sg_len;
+ 	}
+ 	params.bfr_len = 0;
+ 	reqctx->data_len += params.sg_len;
+ 	skb = create_hash_wr(req, &params);
+ 	if (IS_ERR(skb)) {
+ 		error = PTR_ERR(skb);
+ 		goto err;
+ 	}
+ 	hctx_wr->processed += params.sg_len;
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
+ 	chcr_send_wr(skb);
+ 	return 0;
+ err:
+ 	return error;
+ }
+ 
+ static inline void chcr_handle_ahash_resp(struct ahash_request *req,
+ 					  unsigned char *input,
+ 					  int err)
+ {
+ 	struct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);
+ 	struct chcr_hctx_per_wr *hctx_wr = &reqctx->hctx_wr;
+ 	int digestsize, updated_digestsize;
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct uld_ctx *u_ctx = ULD_CTX(h_ctx(tfm));
+ 
+ 	if (input == NULL)
+ 		goto out;
+ 	digestsize = crypto_ahash_digestsize(crypto_ahash_reqtfm(req));
+ 	updated_digestsize = digestsize;
+ 	if (digestsize == SHA224_DIGEST_SIZE)
+ 		updated_digestsize = SHA256_DIGEST_SIZE;
+ 	else if (digestsize == SHA384_DIGEST_SIZE)
+ 		updated_digestsize = SHA512_DIGEST_SIZE;
+ 
+ 	if (hctx_wr->dma_addr) {
+ 		dma_unmap_single(&u_ctx->lldi.pdev->dev, hctx_wr->dma_addr,
+ 				 hctx_wr->dma_len, DMA_TO_DEVICE);
+ 		hctx_wr->dma_addr = 0;
+ 	}
+ 	if (hctx_wr->isfinal || ((hctx_wr->processed + reqctx->reqlen) ==
+ 				 req->nbytes)) {
+ 		if (hctx_wr->result == 1) {
+ 			hctx_wr->result = 0;
+ 			memcpy(req->result, input + sizeof(struct cpl_fw6_pld),
+ 			       digestsize);
+ 		} else {
+ 			memcpy(reqctx->partial_hash,
+ 			       input + sizeof(struct cpl_fw6_pld),
+ 			       updated_digestsize);
+ 
+ 		}
+ 		goto unmap;
+ 	}
+ 	memcpy(reqctx->partial_hash, input + sizeof(struct cpl_fw6_pld),
+ 	       updated_digestsize);
+ 
+ 	err = chcr_ahash_continue(req);
+ 	if (err)
+ 		goto unmap;
+ 	return;
+ unmap:
+ 	if (hctx_wr->is_sg_map)
+ 		chcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);
+ 
+ 
+ out:
+ 	req->base.complete(&req->base, err);
+ }
+ 
+ /*
+  *	chcr_handle_resp - Unmap the DMA buffers associated with the request
+  *	@req: crypto request
+  */
+ int chcr_handle_resp(struct crypto_async_request *req, unsigned char *input,
+ 			 int err)
+ {
+ 	struct crypto_tfm *tfm = req->tfm;
+ 	struct chcr_context *ctx = crypto_tfm_ctx(tfm);
+ 	struct adapter *adap = padap(ctx->dev);
+ 
+ 	switch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {
+ 	case CRYPTO_ALG_TYPE_AEAD:
+ 		chcr_handle_aead_resp(aead_request_cast(req), input, err);
+ 		break;
+ 
+ 	case CRYPTO_ALG_TYPE_ABLKCIPHER:
+ 		 err = chcr_handle_cipher_resp(ablkcipher_request_cast(req),
+ 					       input, err);
+ 		break;
+ 
+ 	case CRYPTO_ALG_TYPE_AHASH:
+ 		chcr_handle_ahash_resp(ahash_request_cast(req), input, err);
+ 		}
+ 	atomic_inc(&adap->chcr_stats.complete);
+ 	return err;
+ }
  static int chcr_ahash_export(struct ahash_request *areq, void *out)
  {
  	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);
diff --cc drivers/crypto/chelsio/chcr_crypto.h
index ee637a7f2f30,c8e8972af283..000000000000
--- a/drivers/crypto/chelsio/chcr_crypto.h
+++ b/drivers/crypto/chelsio/chcr_crypto.h
@@@ -283,15 -315,29 +283,43 @@@ typedef struct sk_buff *(*create_wr_t)(
  				       int size,
  				       unsigned short op_type);
  
++<<<<<<< HEAD
 +static int chcr_aead_op(struct aead_request *req_base,
 +			  unsigned short op_type,
 +			  int size,
 +			  create_wr_t create_wr_fn);
 +static inline int get_aead_subtype(struct crypto_aead *aead);
 +static int is_newsg(struct scatterlist *sgl, unsigned int *newents);
 +static struct scatterlist *alloc_new_sg(struct scatterlist *sgl,
 +					unsigned int nents);
 +static inline void free_new_sg(struct scatterlist *sgl);
 +static int chcr_handle_cipher_resp(struct ablkcipher_request *req,
 +				   unsigned char *input, int err);
++=======
+ void chcr_verify_tag(struct aead_request *req, u8 *input, int *err);
+ int chcr_aead_dma_map(struct device *dev, struct aead_request *req,
+ 		      unsigned short op_type);
+ void chcr_aead_dma_unmap(struct device *dev, struct aead_request *req,
+ 			 unsigned short op_type);
+ void chcr_add_aead_dst_ent(struct aead_request *req,
+ 			   struct cpl_rx_phys_dsgl *phys_cpl,
+ 			   unsigned int assoclen, unsigned short op_type,
+ 			   unsigned short qid);
+ void chcr_add_aead_src_ent(struct aead_request *req, struct ulptx_sgl *ulptx,
+ 			   unsigned int assoclen, unsigned short op_type);
+ void chcr_add_cipher_src_ent(struct ablkcipher_request *req,
+ 			     struct ulptx_sgl *ulptx,
+ 			     struct  cipher_wr_param *wrparam);
+ int chcr_cipher_dma_map(struct device *dev, struct ablkcipher_request *req);
+ void chcr_cipher_dma_unmap(struct device *dev, struct ablkcipher_request *req);
+ void chcr_add_cipher_dst_ent(struct ablkcipher_request *req,
+ 			     struct cpl_rx_phys_dsgl *phys_cpl,
+ 			     struct  cipher_wr_param *wrparam,
+ 			     unsigned short qid);
+ int sg_nents_len_skip(struct scatterlist *sg, u64 len, u64 skip);
+ void chcr_add_hash_src_ent(struct ahash_request *req, struct ulptx_sgl *ulptx,
+ 			   struct hash_wr_param *param);
+ int chcr_hash_dma_map(struct device *dev, struct ahash_request *req);
+ void chcr_hash_dma_unmap(struct device *dev, struct ahash_request *req);
++>>>>>>> 6f76672bd650 (crypto: chelsio - Remove declaration of static function from header)
  #endif /* __CHCR_CRYPTO_H__ */
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/chelsio/chcr_crypto.h
