membarrier: Provide core serializing command, *_SYNC_CORE

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
commit 70216e18e519a54a2f13569e8caff99a092a92d6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/70216e18.failed

Provide core serializing membarrier command to support memory reclaim
by JIT.

Each architecture needs to explicitly opt into that support by
documenting in their architecture code how they provide the core
serializing instructions required when returning from the membarrier
IPI, and after the scheduler has updated the curr->mm pointer (before
going back to user-space). They should then select
ARCH_HAS_MEMBARRIER_SYNC_CORE to enable support for that command on
their architecture.

Architectures selecting this feature need to either document that
they issue core serializing instructions when returning to user-space,
or implement their architecture-specific sync_core_before_usermode().

	Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
	Acked-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrea Parri <parri.andrea@gmail.com>
	Cc: Andrew Hunter <ahh@google.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Avi Kivity <avi@scylladb.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Boqun Feng <boqun.feng@gmail.com>
	Cc: Dave Watson <davejwatson@fb.com>
	Cc: David Sehr <sehr@google.com>
	Cc: Greg Hackmann <ghackmann@google.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Maged Michael <maged.michael@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: linux-api@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
Link: http://lkml.kernel.org/r/20180129202020.8515-9-mathieu.desnoyers@efficios.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 70216e18e519a54a2f13569e8caff99a092a92d6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched/mm.h
#	include/uapi/linux/membarrier.h
#	kernel/sched/core.c
#	kernel/sched/membarrier.c
diff --cc include/linux/sched/mm.h
index a7adba1cd0a9,03a169087a18..000000000000
--- a/include/linux/sched/mm.h
+++ b/include/linux/sched/mm.h
@@@ -1,6 -1,267 +1,267 @@@
  #ifndef _LINUX_SCHED_MM_H
  #define _LINUX_SCHED_MM_H
  
 -#include <linux/kernel.h>
 -#include <linux/atomic.h>
  #include <linux/sched.h>
++<<<<<<< HEAD
++=======
+ #include <linux/mm_types.h>
+ #include <linux/gfp.h>
+ #include <linux/sync_core.h>
+ 
+ /*
+  * Routines for handling mm_structs
+  */
+ extern struct mm_struct * mm_alloc(void);
+ 
+ /**
+  * mmgrab() - Pin a &struct mm_struct.
+  * @mm: The &struct mm_struct to pin.
+  *
+  * Make sure that @mm will not get freed even after the owning task
+  * exits. This doesn't guarantee that the associated address space
+  * will still exist later on and mmget_not_zero() has to be used before
+  * accessing it.
+  *
+  * This is a preferred way to to pin @mm for a longer/unbounded amount
+  * of time.
+  *
+  * Use mmdrop() to release the reference acquired by mmgrab().
+  *
+  * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
+  * of &mm_struct.mm_count vs &mm_struct.mm_users.
+  */
+ static inline void mmgrab(struct mm_struct *mm)
+ {
+ 	atomic_inc(&mm->mm_count);
+ }
+ 
+ /* mmdrop drops the mm and the page tables */
+ extern void __mmdrop(struct mm_struct *);
+ static inline void mmdrop(struct mm_struct *mm)
+ {
+ 	/*
+ 	 * The implicit full barrier implied by atomic_dec_and_test() is
+ 	 * required by the membarrier system call before returning to
+ 	 * user-space, after storing to rq->curr.
+ 	 */
+ 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
+ 		__mmdrop(mm);
+ }
+ 
+ static inline void mmdrop_async_fn(struct work_struct *work)
+ {
+ 	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
+ 	__mmdrop(mm);
+ }
+ 
+ static inline void mmdrop_async(struct mm_struct *mm)
+ {
+ 	if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
+ 		INIT_WORK(&mm->async_put_work, mmdrop_async_fn);
+ 		schedule_work(&mm->async_put_work);
+ 	}
+ }
+ 
+ /**
+  * mmget() - Pin the address space associated with a &struct mm_struct.
+  * @mm: The address space to pin.
+  *
+  * Make sure that the address space of the given &struct mm_struct doesn't
+  * go away. This does not protect against parts of the address space being
+  * modified or freed, however.
+  *
+  * Never use this function to pin this address space for an
+  * unbounded/indefinite amount of time.
+  *
+  * Use mmput() to release the reference acquired by mmget().
+  *
+  * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
+  * of &mm_struct.mm_count vs &mm_struct.mm_users.
+  */
+ static inline void mmget(struct mm_struct *mm)
+ {
+ 	atomic_inc(&mm->mm_users);
+ }
+ 
+ static inline bool mmget_not_zero(struct mm_struct *mm)
+ {
+ 	return atomic_inc_not_zero(&mm->mm_users);
+ }
+ 
+ /* mmput gets rid of the mappings and all user-space */
+ extern void mmput(struct mm_struct *);
+ #ifdef CONFIG_MMU
+ /* same as above but performs the slow path from the async context. Can
+  * be called from the atomic context as well
+  */
+ void mmput_async(struct mm_struct *);
+ #endif
+ 
+ /* Grab a reference to a task's mm, if it is not already going away */
+ extern struct mm_struct *get_task_mm(struct task_struct *task);
+ /*
+  * Grab a reference to a task's mm, if it is not already going away
+  * and ptrace_may_access with the mode parameter passed to it
+  * succeeds.
+  */
+ extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
+ /* Remove the current tasks stale references to the old mm_struct */
+ extern void mm_release(struct task_struct *, struct mm_struct *);
+ 
+ #ifdef CONFIG_MEMCG
+ extern void mm_update_next_owner(struct mm_struct *mm);
+ #else
+ static inline void mm_update_next_owner(struct mm_struct *mm)
+ {
+ }
+ #endif /* CONFIG_MEMCG */
+ 
+ #ifdef CONFIG_MMU
+ extern void arch_pick_mmap_layout(struct mm_struct *mm);
+ extern unsigned long
+ arch_get_unmapped_area(struct file *, unsigned long, unsigned long,
+ 		       unsigned long, unsigned long);
+ extern unsigned long
+ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
+ 			  unsigned long len, unsigned long pgoff,
+ 			  unsigned long flags);
+ #else
+ static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
+ #endif
+ 
+ static inline bool in_vfork(struct task_struct *tsk)
+ {
+ 	bool ret;
+ 
+ 	/*
+ 	 * need RCU to access ->real_parent if CLONE_VM was used along with
+ 	 * CLONE_PARENT.
+ 	 *
+ 	 * We check real_parent->mm == tsk->mm because CLONE_VFORK does not
+ 	 * imply CLONE_VM
+ 	 *
+ 	 * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus
+ 	 * ->real_parent is not necessarily the task doing vfork(), so in
+ 	 * theory we can't rely on task_lock() if we want to dereference it.
+ 	 *
+ 	 * And in this case we can't trust the real_parent->mm == tsk->mm
+ 	 * check, it can be false negative. But we do not care, if init or
+ 	 * another oom-unkillable task does this it should blame itself.
+ 	 */
+ 	rcu_read_lock();
+ 	ret = tsk->vfork_done && tsk->real_parent->mm == tsk->mm;
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Applies per-task gfp context to the given allocation flags.
+  * PF_MEMALLOC_NOIO implies GFP_NOIO
+  * PF_MEMALLOC_NOFS implies GFP_NOFS
+  */
+ static inline gfp_t current_gfp_context(gfp_t flags)
+ {
+ 	/*
+ 	 * NOIO implies both NOIO and NOFS and it is a weaker context
+ 	 * so always make sure it makes precendence
+ 	 */
+ 	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
+ 		flags &= ~(__GFP_IO | __GFP_FS);
+ 	else if (unlikely(current->flags & PF_MEMALLOC_NOFS))
+ 		flags &= ~__GFP_FS;
+ 	return flags;
+ }
+ 
+ #ifdef CONFIG_LOCKDEP
+ extern void fs_reclaim_acquire(gfp_t gfp_mask);
+ extern void fs_reclaim_release(gfp_t gfp_mask);
+ #else
+ static inline void fs_reclaim_acquire(gfp_t gfp_mask) { }
+ static inline void fs_reclaim_release(gfp_t gfp_mask) { }
+ #endif
+ 
+ static inline unsigned int memalloc_noio_save(void)
+ {
+ 	unsigned int flags = current->flags & PF_MEMALLOC_NOIO;
+ 	current->flags |= PF_MEMALLOC_NOIO;
+ 	return flags;
+ }
+ 
+ static inline void memalloc_noio_restore(unsigned int flags)
+ {
+ 	current->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;
+ }
+ 
+ static inline unsigned int memalloc_nofs_save(void)
+ {
+ 	unsigned int flags = current->flags & PF_MEMALLOC_NOFS;
+ 	current->flags |= PF_MEMALLOC_NOFS;
+ 	return flags;
+ }
+ 
+ static inline void memalloc_nofs_restore(unsigned int flags)
+ {
+ 	current->flags = (current->flags & ~PF_MEMALLOC_NOFS) | flags;
+ }
+ 
+ static inline unsigned int memalloc_noreclaim_save(void)
+ {
+ 	unsigned int flags = current->flags & PF_MEMALLOC;
+ 	current->flags |= PF_MEMALLOC;
+ 	return flags;
+ }
+ 
+ static inline void memalloc_noreclaim_restore(unsigned int flags)
+ {
+ 	current->flags = (current->flags & ~PF_MEMALLOC) | flags;
+ }
+ 
+ #ifdef CONFIG_MEMBARRIER
+ enum {
+ 	MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY		= (1U << 0),
+ 	MEMBARRIER_STATE_PRIVATE_EXPEDITED			= (1U << 1),
+ 	MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY			= (1U << 2),
+ 	MEMBARRIER_STATE_GLOBAL_EXPEDITED			= (1U << 3),
+ 	MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY	= (1U << 4),
+ 	MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE		= (1U << 5),
+ };
+ 
+ enum {
+ 	MEMBARRIER_FLAG_SYNC_CORE	= (1U << 0),
+ };
+ 
+ #ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS
+ #include <asm/membarrier.h>
+ #endif
+ 
+ static inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)
+ {
+ 	if (likely(!(atomic_read(&mm->membarrier_state) &
+ 		     MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE)))
+ 		return;
+ 	sync_core_before_usermode();
+ }
+ 
+ static inline void membarrier_execve(struct task_struct *t)
+ {
+ 	atomic_set(&t->mm->membarrier_state, 0);
+ }
+ #else
+ #ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS
+ static inline void membarrier_arch_switch_mm(struct mm_struct *prev,
+ 					     struct mm_struct *next,
+ 					     struct task_struct *tsk)
+ {
+ }
+ #endif
+ static inline void membarrier_execve(struct task_struct *t)
+ {
+ }
+ static inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)
+ {
+ }
+ #endif
++>>>>>>> 70216e18e519 (membarrier: Provide core serializing command, *_SYNC_CORE)
  
  #endif /* _LINUX_SCHED_MM_H */
diff --cc kernel/sched/core.c
index 1a5e18b224eb,ee420d78e674..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -2400,11 -2703,23 +2400,29 @@@ static void finish_task_switch(struct r
  	finish_arch_post_lock_switch();
  
  	fire_sched_in_preempt_notifiers(current);
++<<<<<<< HEAD
 +	if (mm)
++=======
+ 	/*
+ 	 * When switching through a kernel thread, the loop in
+ 	 * membarrier_{private,global}_expedited() may have observed that
+ 	 * kernel thread and not issued an IPI. It is therefore possible to
+ 	 * schedule between user->kernel->user threads without passing though
+ 	 * switch_mm(). Membarrier requires a barrier after storing to
+ 	 * rq->curr, before returning to userspace, so provide them here:
+ 	 *
+ 	 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly
+ 	 *   provided by mmdrop(),
+ 	 * - a sync_core for SYNC_CORE.
+ 	 */
+ 	if (mm) {
+ 		membarrier_mm_sync_core_before_usermode(mm);
++>>>>>>> 70216e18e519 (membarrier: Provide core serializing command, *_SYNC_CORE)
  		mmdrop(mm);
+ 	}
  	if (unlikely(prev_state == TASK_DEAD)) {
 +		task_numa_free(prev);
 +
  		if (prev->sched_class->task_dead)
  			prev->sched_class->task_dead(prev);
  
* Unmerged path include/uapi/linux/membarrier.h
* Unmerged path kernel/sched/membarrier.c
* Unmerged path include/linux/sched/mm.h
* Unmerged path include/uapi/linux/membarrier.h
diff --git a/init/Kconfig b/init/Kconfig
index 598b4d05d99c..71ab4f26ef82 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1464,6 +1464,9 @@ config PCI_QUIRKS
 	  bugs/quirks. Disable this only if your target machine is
 	  unaffected by PCI quirks.
 
+config ARCH_HAS_MEMBARRIER_SYNC_CORE
+	bool
+
 config EMBEDDED
 	bool "Embedded system"
 	option allnoconfig_y
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/membarrier.c
