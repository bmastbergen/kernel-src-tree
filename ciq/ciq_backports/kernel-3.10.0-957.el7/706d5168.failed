x86/speculation: Support Enhanced IBRS on future CPUs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] speculation: Support Enhanced IBRS on future CPUs (Waiman Long) [1614143]
Rebuild_FUZZ: 96.08%
commit-author Sai Praneeth <sai.praneeth.prakhya@intel.com>
commit 706d51681d636a0c4a5ef53395ec3b803e45ed4d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/706d5168.failed

Future Intel processors will support "Enhanced IBRS" which is an "always
on" mode i.e. IBRS bit in SPEC_CTRL MSR is enabled once and never
disabled.

From the specification [1]:

 "With enhanced IBRS, the predicted targets of indirect branches
  executed cannot be controlled by software that was executed in a less
  privileged predictor mode or on another logical processor. As a
  result, software operating on a processor with enhanced IBRS need not
  use WRMSR to set IA32_SPEC_CTRL.IBRS after every transition to a more
  privileged predictor mode. Software can isolate predictor modes
  effectively simply by setting the bit once. Software need not disable
  enhanced IBRS prior to entering a sleep state such as MWAIT or HLT."

If Enhanced IBRS is supported by the processor then use it as the
preferred spectre v2 mitigation mechanism instead of Retpoline. Intel's
Retpoline white paper [2] states:

 "Retpoline is known to be an effective branch target injection (Spectre
  variant 2) mitigation on Intel processors belonging to family 6
  (enumerated by the CPUID instruction) that do not have support for
  enhanced IBRS. On processors that support enhanced IBRS, it should be
  used for mitigation instead of retpoline."

The reason why Enhanced IBRS is the recommended mitigation on processors
which support it is that these processors also support CET which
provides a defense against ROP attacks. Retpoline is very similar to ROP
techniques and might trigger false positives in the CET defense.

If Enhanced IBRS is selected as the mitigation technique for spectre v2,
the IBRS bit in SPEC_CTRL MSR is set once at boot time and never
cleared. Kernel also has to make sure that IBRS bit remains set after
VMEXIT because the guest might have cleared the bit. This is already
covered by the existing x86_spec_ctrl_set_guest() and
x86_spec_ctrl_restore_host() speculation control functions.

Enhanced IBRS still requires IBPB for full mitigation.

[1] Speculative-Execution-Side-Channel-Mitigations.pdf
[2] Retpoline-A-Branch-Target-Injection-Mitigation.pdf
Both documents are available at:
https://bugzilla.kernel.org/show_bug.cgi?id=199511

Originally-by: David Woodhouse <dwmw@amazon.co.uk>
	Signed-off-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tim C Chen <tim.c.chen@intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Ravi Shankar <ravi.v.shankar@intel.com>
Link: https://lkml.kernel.org/r/1533148945-24095-1-git-send-email-sai.praneeth.prakhya@intel.com

(cherry picked from commit 706d51681d636a0c4a5ef53395ec3b803e45ed4d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
diff --cc arch/x86/include/asm/cpufeatures.h
index 7a3e9c71ed7d,2687cd8e8d58..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -190,126 -190,151 +190,159 @@@
   *
   * Reuse free bits when adding new feature flags!
   */
++<<<<<<< HEAD
 +
 +#define X86_FEATURE_RING3MWAIT	(7*32+ 0) /* Ring 3 MONITOR/MWAIT */
 +#define X86_FEATURE_CPB		(7*32+ 2) /* AMD Core Performance Boost */
 +#define X86_FEATURE_EPB		(7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 +#define X86_FEATURE_CAT_L3	(7*32+ 4) /* Cache Allocation Technology L3 */
 +#define X86_FEATURE_CAT_L2	(7*32+ 5) /* Cache Allocation Technology L2 */
 +#define X86_FEATURE_CDP_L3	(7*32+ 6) /* Code and Data Prioritization L3 */
 +
 +#define X86_FEATURE_HW_PSTATE	(7*32+ 8) /* AMD HW-PState */
 +#define X86_FEATURE_PROC_FEEDBACK (7*32+ 9) /* AMD ProcFeedbackInterface */
 +#define X86_FEATURE_SME		( 7*32+10) /* AMD Secure Memory Encryption */
 +#define X86_FEATURE_RETPOLINE_AMD (7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */
 +#define X86_FEATURE_INTEL_PPIN	( 7*32+14) /* Intel Processor Inventory Number */
 +#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */
 +
 +#define X86_FEATURE_MBA		( 7*32+18) /* Memory Bandwidth Allocation */
 +#define X86_FEATURE_IBP_DISABLE ( 7*32+21) /* Old AMD Indirect Branch Predictor Disable */
++=======
+ #define X86_FEATURE_RING3MWAIT		( 7*32+ 0) /* Ring 3 MONITOR/MWAIT instructions */
+ #define X86_FEATURE_CPUID_FAULT		( 7*32+ 1) /* Intel CPUID faulting */
+ #define X86_FEATURE_CPB			( 7*32+ 2) /* AMD Core Performance Boost */
+ #define X86_FEATURE_EPB			( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
+ #define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
+ #define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
+ #define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
+ #define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID && CR4.PCIDE=1 */
+ #define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
+ #define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
+ #define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
+ #define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
+ #define X86_FEATURE_RETPOLINE		( 7*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
+ #define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* "" AMD Retpoline mitigation for Spectre variant 2 */
+ #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
+ #define X86_FEATURE_CDP_L2		( 7*32+15) /* Code and Data Prioritization L2 */
+ #define X86_FEATURE_MSR_SPEC_CTRL	( 7*32+16) /* "" MSR SPEC_CTRL is implemented */
+ #define X86_FEATURE_SSBD		( 7*32+17) /* Speculative Store Bypass Disable */
+ #define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
+ #define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* "" Fill RSB on context switches */
+ #define X86_FEATURE_SEV			( 7*32+20) /* AMD Secure Encrypted Virtualization */
+ #define X86_FEATURE_USE_IBPB		( 7*32+21) /* "" Indirect Branch Prediction Barrier enabled */
+ #define X86_FEATURE_USE_IBRS_FW		( 7*32+22) /* "" Use IBRS during runtime firmware calls */
+ #define X86_FEATURE_SPEC_STORE_BYPASS_DISABLE	( 7*32+23) /* "" Disable Speculative Store Bypass. */
+ #define X86_FEATURE_LS_CFG_SSBD		( 7*32+24)  /* "" AMD SSBD implementation via LS_CFG MSR */
+ #define X86_FEATURE_IBRS		( 7*32+25) /* Indirect Branch Restricted Speculation */
+ #define X86_FEATURE_IBPB		( 7*32+26) /* Indirect Branch Prediction Barrier */
+ #define X86_FEATURE_STIBP		( 7*32+27) /* Single Thread Indirect Branch Predictors */
+ #define X86_FEATURE_ZEN			( 7*32+28) /* "" CPU is AMD family 0x17 (Zen) */
+ #define X86_FEATURE_IBRS_ENHANCED	( 7*32+29) /* Enhanced IBRS */
++>>>>>>> 706d51681d63 (x86/speculation: Support Enhanced IBRS on future CPUs)
  
  /* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 -
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 -
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
  
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (EDX), word 11 */
 -#define X86_FEATURE_CQM_LLC		(11*32+ 1) /* LLC QoS if 1 */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (EDX), word 12 */
 -#define X86_FEATURE_CQM_OCCUP_LLC	(12*32+ 0) /* LLC occupancy monitoring */
 -#define X86_FEATURE_CQM_MBM_TOTAL	(12*32+ 1) /* LLC Total MBM monitoring */
 -#define X86_FEATURE_CQM_MBM_LOCAL	(12*32+ 2) /* LLC Local MBM monitoring */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
 -#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 -#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 -#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
 -#define X86_FEATURE_AMD_IBPB		(13*32+12) /* "" Indirect Branch Prediction Barrier */
 -#define X86_FEATURE_AMD_IBRS		(13*32+14) /* "" Indirect Branch Restricted Speculation */
 -#define X86_FEATURE_AMD_STIBP		(13*32+15) /* "" Single Thread Indirect Branch Predictors */
 -#define X86_FEATURE_AMD_SSBD		(13*32+24) /* "" Speculative Store Bypass Disable */
 -#define X86_FEATURE_VIRT_SSBD		(13*32+25) /* Virtualized Speculative Store Bypass Disable */
 -#define X86_FEATURE_AMD_SSB_NO		(13*32+26) /* "" Speculative Store Bypass is fixed in hardware. */
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
 +#define X86_FEATURE_IBPB		(13*32+12) /* Indirect Branch Prediction Barrier */
 +#define X86_FEATURE_IBRS		(13*32+14) /* Indirect Branch Restricted Speculation */
 +#define X86_FEATURE_STIBP		(13*32+15) /* Single Thread Indirect Branch Predictors */
  
 -/* Thermal and Power Management Leaf, CPUID level 0x00000006 (EAX), word 14 */
 -#define X86_FEATURE_DTHERM		(14*32+ 0) /* Digital Thermal Sensor */
 -#define X86_FEATURE_IDA			(14*32+ 1) /* Intel Dynamic Acceleration */
 -#define X86_FEATURE_ARAT		(14*32+ 2) /* Always Running APIC Timer */
 -#define X86_FEATURE_PLN			(14*32+ 4) /* Intel Power Limit Notification */
 -#define X86_FEATURE_PTS			(14*32+ 6) /* Intel Package Thermal Status */
 -#define X86_FEATURE_HWP			(14*32+ 7) /* Intel Hardware P-states */
 -#define X86_FEATURE_HWP_NOTIFY		(14*32+ 8) /* HWP Notification */
 -#define X86_FEATURE_HWP_ACT_WINDOW	(14*32+ 9) /* HWP Activity Window */
 -#define X86_FEATURE_HWP_EPP		(14*32+10) /* HWP Energy Perf. Preference */
 -#define X86_FEATURE_HWP_PKG_REQ		(14*32+11) /* HWP Package Level Request */
 +/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */
 +#define X86_FEATURE_OVERFLOW_RECOV (17*32+0) /* MCA overflow recovery support */
 +#define X86_FEATURE_SUCCOR	(17*32+1) /* Uncorrectable error containment and recovery */
 +#define X86_FEATURE_SMCA	(17*32+3) /* Scalable MCA */
  
 -/* AMD SVM Feature Identification, CPUID level 0x8000000a (EDX), word 15 */
 -#define X86_FEATURE_NPT			(15*32+ 0) /* Nested Page Table support */
 -#define X86_FEATURE_LBRV		(15*32+ 1) /* LBR Virtualization support */
 -#define X86_FEATURE_SVML		(15*32+ 2) /* "svm_lock" SVM locking MSR */
 -#define X86_FEATURE_NRIPS		(15*32+ 3) /* "nrip_save" SVM next_rip save */
 -#define X86_FEATURE_TSCRATEMSR		(15*32+ 4) /* "tsc_scale" TSC scaling support */
 -#define X86_FEATURE_VMCBCLEAN		(15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 -#define X86_FEATURE_FLUSHBYASID		(15*32+ 6) /* flush-by-ASID support */
 -#define X86_FEATURE_DECODEASSISTS	(15*32+ 7) /* Decode Assists support */
 -#define X86_FEATURE_PAUSEFILTER		(15*32+10) /* filtered pause intercept */
 -#define X86_FEATURE_PFTHRESHOLD		(15*32+12) /* pause filter threshold */
 -#define X86_FEATURE_AVIC		(15*32+13) /* Virtual Interrupt Controller */
 -#define X86_FEATURE_V_VMSAVE_VMLOAD	(15*32+15) /* Virtual VMSAVE VMLOAD */
 -#define X86_FEATURE_VGIF		(15*32+16) /* Virtual GIF */
 +/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */
 +#define X86_FEATURE_DTHERM	(14*32+ 0) /* Digital Thermal Sensor */
 +#define X86_FEATURE_IDA		(14*32+ 1) /* Intel Dynamic Acceleration */
 +#define X86_FEATURE_ARAT	(14*32+ 2) /* Always Running APIC Timer */
 +#define X86_FEATURE_PLN		(14*32+ 4) /* Intel Power Limit Notification */
 +#define X86_FEATURE_PTS		(14*32+ 6) /* Intel Package Thermal Status */
 +#define X86_FEATURE_HWP		(14*32+ 7) /* Intel Hardware P-states */
 +#define X86_FEATURE_HWP_NOTIFY	(14*32+ 8) /* HWP Notification */
 +#define X86_FEATURE_HWP_ACT_WINDOW (14*32+ 9) /* HWP Activity Window */
 +#define X86_FEATURE_HWP_EPP	(14*32+10) /* HWP Energy Perf. Preference */
 +#define X86_FEATURE_HWP_PKG_REQ (14*32+11) /* HWP Package Level Request */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (ECX), word 16 */
 -#define X86_FEATURE_AVX512VBMI		(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 -#define X86_FEATURE_UMIP		(16*32+ 2) /* User Mode Instruction Protection */
 -#define X86_FEATURE_PKU			(16*32+ 3) /* Protection Keys for Userspace */
 -#define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */
 -#define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 -#define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */
 -#define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */
 -#define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */
 -#define X86_FEATURE_AVX512_VNNI		(16*32+11) /* Vector Neural Network Instructions */
 -#define X86_FEATURE_AVX512_BITALG	(16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB instructions */
 -#define X86_FEATURE_TME			(16*32+13) /* Intel Total Memory Encryption */
 -#define X86_FEATURE_AVX512_VPOPCNTDQ	(16*32+14) /* POPCNT for vectors of DW/QW */
 -#define X86_FEATURE_LA57		(16*32+16) /* 5-level page tables */
 -#define X86_FEATURE_RDPID		(16*32+22) /* RDPID instruction */
 -#define X86_FEATURE_CLDEMOTE		(16*32+25) /* CLDEMOTE instruction */
 +/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */
 +#define X86_FEATURE_NPT		(15*32+ 0) /* Nested Page Table support */
 +#define X86_FEATURE_LBRV	(15*32+ 1) /* LBR Virtualization support */
 +#define X86_FEATURE_SVML	(15*32+ 2) /* "svm_lock" SVM locking MSR */
 +#define X86_FEATURE_NRIPS	(15*32+ 3) /* "nrip_save" SVM next_rip save */
 +#define X86_FEATURE_TSCRATEMSR  (15*32+ 4) /* "tsc_scale" TSC scaling support */
 +#define X86_FEATURE_VMCBCLEAN   (15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 +#define X86_FEATURE_FLUSHBYASID (15*32+ 6) /* flush-by-ASID support */
 +#define X86_FEATURE_DECODEASSISTS (15*32+ 7) /* Decode Assists support */
 +#define X86_FEATURE_PAUSEFILTER (15*32+10) /* filtered pause intercept */
 +#define X86_FEATURE_PFTHRESHOLD (15*32+12) /* pause filter threshold */
 +#define X86_FEATURE_AVIC	(15*32+13) /* Virtual Interrupt Controller */
 +#define X86_FEATURE_V_VMSAVE_VMLOAD (15*32+15) /* Virtual VMSAVE VMLOAD */
 +#define X86_FEATURE_VGIF	(15*32+16) /* Virtual GIF */
  
 -/* AMD-defined CPU features, CPUID level 0x80000007 (EBX), word 17 */
 -#define X86_FEATURE_OVERFLOW_RECOV	(17*32+ 0) /* MCA overflow recovery support */
 -#define X86_FEATURE_SUCCOR		(17*32+ 1) /* Uncorrectable error containment and recovery */
 -#define X86_FEATURE_SMCA		(17*32+ 3) /* Scalable MCA */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */
 +#define X86_FEATURE_AVX512VBMI	(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 +#define X86_FEATURE_PKU		(16*32+ 3) /* Protection Keys for Userspace */
 +#define X86_FEATURE_OSPKE	(16*32+ 4) /* OS Protection Keys Enable */
 +#define X86_FEATURE_AVX512_VBMI2 (16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 +#define X86_FEATURE_GFNI	(16*32+ 8) /* Galois Field New Instructions */
 +#define X86_FEATURE_VAES	(16*32+ 9) /* Vector AES */
 +#define X86_FEATURE_VPCLMULQDQ	(16*32+ 10) /* Carry-Less Multiplication Double Quadword */
 +#define X86_FEATURE_AVX512_VNNI (16*32+ 11) /* Vector Neural Network Instructions */
 +#define X86_FEATURE_AVX512_BITALG (16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB */
 +#define X86_FEATURE_AVX512_VPOPCNTDQ (16*32+14) /* POPCNT for vectors of DW/QW */
  
  /* Intel-defined CPU features, CPUID level 0x00000007:0 (EDX), word 18 */
  #define X86_FEATURE_AVX512_4VNNIW	(18*32+ 2) /* AVX-512 Neural Network Instructions */
diff --cc arch/x86/include/asm/nospec-branch.h
index d5b6abbaaa24,fd2a8c1b88bc..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -169,27 -211,22 +169,34 @@@
  enum spectre_v2_mitigation {
  	SPECTRE_V2_NONE,
  	SPECTRE_V2_RETPOLINE_MINIMAL,
++<<<<<<< HEAD
 +	SPECTRE_V2_RETPOLINE_NO_IBPB,
 +	SPECTRE_V2_RETPOLINE_SKYLAKE,
 +	SPECTRE_V2_RETPOLINE_UNSAFE_MODULE,
 +	SPECTRE_V2_RETPOLINE,
 +	SPECTRE_V2_RETPOLINE_IBRS_USER,
 +	SPECTRE_V2_IBRS,
 +	SPECTRE_V2_IBRS_ALWAYS,
 +	SPECTRE_V2_IBP_DISABLED,
++=======
+ 	SPECTRE_V2_RETPOLINE_MINIMAL_AMD,
+ 	SPECTRE_V2_RETPOLINE_GENERIC,
+ 	SPECTRE_V2_RETPOLINE_AMD,
+ 	SPECTRE_V2_IBRS_ENHANCED,
++>>>>>>> 706d51681d63 (x86/speculation: Support Enhanced IBRS on future CPUs)
  };
  
 -/* The Speculative Store Bypass disable variants */
 -enum ssb_mitigation {
 -	SPEC_STORE_BYPASS_NONE,
 -	SPEC_STORE_BYPASS_DISABLE,
 -	SPEC_STORE_BYPASS_PRCTL,
 -	SPEC_STORE_BYPASS_SECCOMP,
 -};
 +void __spectre_v2_select_mitigation(void);
 +void spectre_v2_print_mitigation(void);
  
 -extern char __indirect_thunk_start[];
 -extern char __indirect_thunk_end[];
 +static inline bool retp_compiler(void)
 +{
 +#ifdef RETPOLINE
 +	return true;
 +#else
 +	return false;
 +#endif
 +}
  
  /*
   * On VMEXIT we must ensure that no RSB predictions learned in the guest
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,405a9a61bb89..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -88,19 -126,13 +88,27 @@@ enum spectre_v2_mitigation_cmd 
  
  static const char *spectre_v2_strings[] = {
  	[SPECTRE_V2_NONE]			= "Vulnerable",
++<<<<<<< HEAD
 +	[SPECTRE_V2_RETPOLINE_MINIMAL]		= "Vulnerable: Minimal ASM retpoline",
 +	[SPECTRE_V2_RETPOLINE_NO_IBPB]		= "Vulnerable: Retpoline without IBPB",
 +	[SPECTRE_V2_RETPOLINE_SKYLAKE]		= "Vulnerable: Retpoline on Skylake+",
 +	[SPECTRE_V2_RETPOLINE_UNSAFE_MODULE]	= "Vulnerable: Retpoline with unsafe module(s)",
 +	[SPECTRE_V2_RETPOLINE]			= "Mitigation: Full retpoline",
 +	[SPECTRE_V2_RETPOLINE_IBRS_USER]	= "Mitigation: Full retpoline and IBRS (user space)",
 +	[SPECTRE_V2_IBRS]			= "Mitigation: IBRS (kernel)",
 +	[SPECTRE_V2_IBRS_ALWAYS]		= "Mitigation: IBRS (kernel and user space)",
 +	[SPECTRE_V2_IBP_DISABLED]		= "Mitigation: IBP disabled",
++=======
+ 	[SPECTRE_V2_RETPOLINE_MINIMAL]		= "Vulnerable: Minimal generic ASM retpoline",
+ 	[SPECTRE_V2_RETPOLINE_MINIMAL_AMD]	= "Vulnerable: Minimal AMD ASM retpoline",
+ 	[SPECTRE_V2_RETPOLINE_GENERIC]		= "Mitigation: Full generic retpoline",
+ 	[SPECTRE_V2_RETPOLINE_AMD]		= "Mitigation: Full AMD retpoline",
+ 	[SPECTRE_V2_IBRS_ENHANCED]		= "Mitigation: Enhanced IBRS",
++>>>>>>> 706d51681d63 (x86/speculation: Support Enhanced IBRS on future CPUs)
  };
  
 +enum spectre_v2_mitigation_cmd spectre_v2_cmd = SPECTRE_V2_CMD_AUTO;
 +
  #undef pr_fmt
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
@@@ -153,49 -333,86 +161,93 @@@ void __spectre_v2_select_mitigation(voi
  
  	case SPECTRE_V2_CMD_FORCE:
  	case SPECTRE_V2_CMD_AUTO:
++<<<<<<< HEAD
++=======
+ 		if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+ 			mode = SPECTRE_V2_IBRS_ENHANCED;
+ 			/* Force it so VMEXIT will restore correctly */
+ 			x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+ 			wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ 			goto specv2_set_mode;
+ 		}
+ 		if (IS_ENABLED(CONFIG_RETPOLINE))
+ 			goto retpoline_auto;
+ 		break;
+ 	case SPECTRE_V2_CMD_RETPOLINE_AMD:
+ 		if (IS_ENABLED(CONFIG_RETPOLINE))
+ 			goto retpoline_amd;
+ 		break;
+ 	case SPECTRE_V2_CMD_RETPOLINE_GENERIC:
+ 		if (IS_ENABLED(CONFIG_RETPOLINE))
+ 			goto retpoline_generic;
++>>>>>>> 706d51681d63 (x86/speculation: Support Enhanced IBRS on future CPUs)
  		break;
 +
  	case SPECTRE_V2_CMD_RETPOLINE:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_auto;
 +		spec_ctrl_enable_retpoline();
 +		return;
 +
 +	case SPECTRE_V2_CMD_IBRS:
 +		if (spec_ctrl_force_enable_ibrs())
 +			return;
 +		break;
 +
 +	case SPECTRE_V2_CMD_IBRS_ALWAYS:
 +		if (spec_ctrl_enable_ibrs_always() ||
 +		    spec_ctrl_force_enable_ibp_disabled())
 +			return;
 +		break;
 +
 +	case SPECTRE_V2_CMD_RETPOLINE_IBRS_USER:
 +		if (spec_ctrl_enable_retpoline_ibrs_user())
 +			return;
  		break;
  	}
 -	pr_err("Spectre mitigation: kernel not compiled with retpoline; no mitigation available!");
 -	return;
 -
 -retpoline_auto:
 -	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {
 -	retpoline_amd:
 -		if (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {
 -			pr_err("Spectre mitigation: LFENCE not serializing, switching to generic retpoline\n");
 -			goto retpoline_generic;
 -		}
 -		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :
 -					 SPECTRE_V2_RETPOLINE_MINIMAL_AMD;
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 -	} else {
 -	retpoline_generic:
 -		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_GENERIC :
 -					 SPECTRE_V2_RETPOLINE_MINIMAL;
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 -	}
  
 +	if (spec_ctrl_cond_enable_ibrs(full_retpoline))
 +		return;
 +
++<<<<<<< HEAD
 +	if (spec_ctrl_cond_enable_ibp_disabled())
 +		return;
++=======
+ specv2_set_mode:
+ 	spectre_v2_enabled = mode;
+ 	pr_info("%s\n", spectre_v2_strings[mode]);
++>>>>>>> 706d51681d63 (x86/speculation: Support Enhanced IBRS on future CPUs)
  
 -	/*
 -	 * If spectre v2 protection has been enabled, unconditionally fill
 -	 * RSB during a context switch; this protects against two independent
 -	 * issues:
 -	 *
 -	 *	- RSB underflow (and switch to BTB) on Skylake+
 -	 *	- SpectreRSB variant of spectre v2 on X86_BUG_SPECTRE_V2 CPUs
 -	 */
 -	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 -	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
 +	spec_ctrl_enable_retpoline();
 +}
  
 -	/* Initialize Indirect Branch Prediction Barrier if supported */
 -	if (boot_cpu_has(X86_FEATURE_IBPB)) {
 -		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 -		pr_info("Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\n");
 -	}
 +void spectre_v2_print_mitigation(void)
 +{
 +
++<<<<<<< HEAD
 +	pr_info("%s\n", spectre_v2_strings[spec_ctrl_get_mitigation()]);
 +}
  
 +static void __init spectre_v2_select_mitigation(void)
 +{
 +	spectre_v2_cmd = spectre_v2_parse_cmdline();
 +	__spectre_v2_select_mitigation();
 +	spectre_v2_print_mitigation();
++=======
+ 	/*
+ 	 * Retpoline means the kernel is safe because it has no indirect
+ 	 * branches. Enhanced IBRS protects firmware too, so, enable restricted
+ 	 * speculation around firmware calls only when Enhanced IBRS isn't
+ 	 * supported.
+ 	 *
+ 	 * Use "mode" to check Enhanced IBRS instead of boot_cpu_has(), because
+ 	 * the user might select retpoline on the kernel command line and if
+ 	 * the CPU supports Enhanced IBRS, kernel might un-intentionally not
+ 	 * enable IBRS around firmware calls.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_IBRS) && mode != SPECTRE_V2_IBRS_ENHANCED) {
+ 		setup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);
+ 		pr_info("Enabling Restricted Speculation for firmware calls\n");
+ 	}
++>>>>>>> 706d51681d63 (x86/speculation: Support Enhanced IBRS on future CPUs)
  }
  
  #undef pr_fmt
diff --cc arch/x86/kernel/cpu/common.c
index 49cb90f121df,df28e931d732..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -853,6 -953,71 +853,74 @@@ static void identify_cpu_without_cpuid(
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ static const __initconst struct x86_cpu_id cpu_no_speculation[] = {
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CEDARVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CLOVERVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_LINCROFT,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PENWELL,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PINEVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_CENTAUR,	5 },
+ 	{ X86_VENDOR_INTEL,	5 },
+ 	{ X86_VENDOR_NSC,	5 },
+ 	{ X86_VENDOR_ANY,	4 },
+ 	{}
+ };
+ 
+ static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
+ 	{ X86_VENDOR_AMD },
+ 	{}
+ };
+ 
+ /* Only list CPUs which speculate but are non susceptible to SSB */
+ static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_CORE_YONAH		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
+ 	{ X86_VENDOR_AMD,	0x12,					},
+ 	{ X86_VENDOR_AMD,	0x11,					},
+ 	{ X86_VENDOR_AMD,	0x10,					},
+ 	{ X86_VENDOR_AMD,	0xf,					},
+ 	{}
+ };
+ 
+ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
+ {
+ 	u64 ia32_cap = 0;
+ 
+ 	if (x86_match_cpu(cpu_no_speculation))
+ 		return;
+ 
+ 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+ 	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+ 
+ 	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
+ 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
+ 
+ 	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
+ 	   !(ia32_cap & ARCH_CAP_SSB_NO) &&
+ 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
+ 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
+ 
+ 	if (ia32_cap & ARCH_CAP_IBRS_ALL)
+ 		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+ 
+ 	if (x86_match_cpu(cpu_no_meltdown))
+ 		return;
+ 
+ 	/* Rogue Data Cache Load? No! */
+ 	if (ia32_cap & ARCH_CAP_RDCL_NO)
+ 		return;
+ 
+ 	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
+ }
+ 
++>>>>>>> 706d51681d63 (x86/speculation: Support Enhanced IBRS on future CPUs)
  /*
   * Do minimum CPU detection early.
   * Fields really needed: vendor, cpuid_level, family, model, mask,
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
