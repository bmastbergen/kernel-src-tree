s390: run user space and KVM guests with modified branch prediction

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [s390] run user space and KVM guests with modified branch prediction (Hendrik Brueckner) [1558325]
Rebuild_FUZZ: 95.31%
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit 6b73044b2b0081ee3dd1cd6eaab7dee552601efb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6b73044b.failed

Define TIF_ISOLATE_BP and TIF_ISOLATE_BP_GUEST and add the necessary
plumbing in entry.S to be able to run user space and KVM guests with
limited branch prediction.

To switch a user space process to limited branch prediction the
s390_isolate_bp() function has to be call, and to run a vCPU of a KVM
guest associated with the current task with limited branch prediction
call s390_isolate_bp_guest().

	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 6b73044b2b0081ee3dd1cd6eaab7dee552601efb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/thread_info.h
#	arch/s390/kernel/entry.S
#	arch/s390/kernel/processor.c
diff --cc arch/s390/include/asm/thread_info.h
index de589f5fc0d4,83ba57533ce6..000000000000
--- a/arch/s390/include/asm/thread_info.h
+++ b/arch/s390/include/asm/thread_info.h
@@@ -82,42 -56,39 +82,66 @@@ static inline struct thread_info *curre
  #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
  #define TIF_UPROBE		3	/* breakpointed or single-stepping */
  #define TIF_GUARDED_STORAGE	4	/* load guarded storage control block */
++<<<<<<< HEAD
 +#define TIF_ASCE		5	/* primary asce needs fixup / uaccess */
 +#define TIF_PER_TRAP		6	/* deliver sigtrap on return to user */
 +#define TIF_MCCK_PENDING	7	/* machine check handling is pending */
 +#define TIF_SYSCALL_TRACE	8	/* syscall trace active */
 +#define TIF_SYSCALL_AUDIT	9	/* syscall auditing active */
 +#define TIF_SECCOMP		10	/* secure computing */
 +#define TIF_SYSCALL_TRACEPOINT	11	/* syscall tracepoint instrumentation */
 +#define TIF_SYSCALL		12	/* inside a system call */
 +#define TIF_31BIT		17	/* 32bit process */
 +#define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 +#define TIF_RESTORE_SIGMASK	19	/* restore signal mask in do_signal() */
 +#define TIF_SINGLE_STEP		20	/* This task is single stepped */
++=======
+ #define TIF_PATCH_PENDING	5	/* pending live patching update */
+ #define TIF_PGSTE		6	/* New mm's will use 4K page tables */
+ #define TIF_ISOLATE_BP		8	/* Run process with isolated BP */
+ #define TIF_ISOLATE_BP_GUEST	9	/* Run KVM guests with isolated BP */
+ 
+ #define TIF_31BIT		16	/* 32bit process */
+ #define TIF_MEMDIE		17	/* is terminating due to OOM killer */
+ #define TIF_RESTORE_SIGMASK	18	/* restore signal mask in do_signal() */
+ #define TIF_SINGLE_STEP		19	/* This task is single stepped */
+ #define TIF_BLOCK_STEP		20	/* This task is block stepped */
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  #define TIF_UPROBE_SINGLESTEP	21	/* This task is uprobe single stepped */
  
 -/* _TIF_TRACE bits */
 -#define TIF_SYSCALL_TRACE	24	/* syscall trace active */
 -#define TIF_SYSCALL_AUDIT	25	/* syscall auditing active */
 -#define TIF_SECCOMP		26	/* secure computing */
 -#define TIF_SYSCALL_TRACEPOINT	27	/* syscall tracepoint instrumentation */
 -
 +#define _TIF_SYSCALL		(1<<TIF_SYSCALL)
 +#define _TIF_NOTIFY_RESUME	(1<<TIF_NOTIFY_RESUME)
 +#define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
 +#define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 +#define _TIF_ASCE		(1<<TIF_ASCE)
 +#define _TIF_PER_TRAP		(1<<TIF_PER_TRAP)
 +#define _TIF_MCCK_PENDING	(1<<TIF_MCCK_PENDING)
 +#define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
 +#define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 +#define _TIF_SECCOMP		(1<<TIF_SECCOMP)
 +#define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
 +#define _TIF_UPROBE		(1<<TIF_UPROBE)
 +#define _TIF_31BIT		(1<<TIF_31BIT)
 +#define _TIF_SINGLE_STEP	(1<<TIF_SINGLE_STEP)
 +#define _TIF_GUARDED_STORAGE	(1<<TIF_GUARDED_STORAGE)
 +
++<<<<<<< HEAD
 +#ifdef CONFIG_64BIT
 +#define is_32bit_task()		(test_thread_flag(TIF_31BIT))
 +#else
 +#define is_32bit_task()		(1)
 +#endif
++=======
+ #define _TIF_NOTIFY_RESUME	_BITUL(TIF_NOTIFY_RESUME)
+ #define _TIF_SIGPENDING		_BITUL(TIF_SIGPENDING)
+ #define _TIF_NEED_RESCHED	_BITUL(TIF_NEED_RESCHED)
+ #define _TIF_UPROBE		_BITUL(TIF_UPROBE)
+ #define _TIF_GUARDED_STORAGE	_BITUL(TIF_GUARDED_STORAGE)
+ #define _TIF_PATCH_PENDING	_BITUL(TIF_PATCH_PENDING)
+ #define _TIF_ISOLATE_BP		_BITUL(TIF_ISOLATE_BP)
+ #define _TIF_ISOLATE_BP_GUEST	_BITUL(TIF_ISOLATE_BP_GUEST)
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  
 -#define _TIF_31BIT		_BITUL(TIF_31BIT)
 -#define _TIF_SINGLE_STEP	_BITUL(TIF_SINGLE_STEP)
 -
 -#define _TIF_SYSCALL_TRACE	_BITUL(TIF_SYSCALL_TRACE)
 -#define _TIF_SYSCALL_AUDIT	_BITUL(TIF_SYSCALL_AUDIT)
 -#define _TIF_SECCOMP		_BITUL(TIF_SECCOMP)
 -#define _TIF_SYSCALL_TRACEPOINT	_BITUL(TIF_SYSCALL_TRACEPOINT)
 +#define PREEMPT_ACTIVE		0x4000000
  
  #endif /* _ASM_THREAD_INFO_H */
diff --cc arch/s390/kernel/entry.S
index be8edbeb24eb,53145b5d987d..000000000000
--- a/arch/s390/kernel/entry.S
+++ b/arch/s390/kernel/entry.S
@@@ -82,58 -88,76 +82,72 @@@ STACK_INIT  = STACK_SIZE - STACK_FRAME_
  #endif
  	.endm
  
 -	.macro	SWITCH_ASYNC savearea,timer
 -	tmhh	%r8,0x0001		# interrupting from user ?
 +	.macro	SWITCH_ASYNC savearea,stack,shift
 +	tmh	%r8,0x0001		# interrupting from user ?
  	jnz	1f
 -	lgr	%r14,%r9
 -	slg	%r14,BASED(.Lcritical_start)
 -	clg	%r14,BASED(.Lcritical_length)
 +	lr	%r14,%r9
 +	sl	%r14,BASED(.Lcritical_start)
 +	cl	%r14,BASED(.Lcritical_length)
  	jhe	0f
 -	lghi	%r11,\savearea		# inside critical section, do cleanup
 -	brasl	%r14,cleanup_critical
 -	tmhh	%r8,0x0001		# retest problem state after cleanup
 +	la	%r11,\savearea		# inside critical section, do cleanup
 +	bras	%r14,cleanup_critical
 +	tmh	%r8,0x0001		# retest problem state after cleanup
 +	jnz	1f
++<<<<<<< HEAD
 +0:	l	%r14,\stack		# are we already on the target stack?
 +	slr	%r14,%r15
 +	sra	%r14,\shift
  	jnz	1f
 +	CHECK_STACK 1<<\shift,\savearea
 +	ahi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
 +	j	2f
 +1:	l	%r15,\stack		# load target stack
 +2:	la	%r11,STACK_FRAME_OVERHEAD(%r15)
++=======
+ 0:	lg	%r14,__LC_ASYNC_STACK	# are we already on the async stack?
+ 	slgr	%r14,%r15
+ 	srag	%r14,%r14,STACK_SHIFT
+ 	jnz	2f
+ 	CHECK_STACK 1<<STACK_SHIFT,\savearea
+ 	aghi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
+ 	j	3f
+ 1:	UPDATE_VTIME %r14,%r15,\timer
+ 	BPENTER __TI_flags(%r12),_TIF_ISOLATE_BP
+ 2:	lg	%r15,__LC_ASYNC_STACK	# load async stack
+ 3:	la	%r11,STACK_FRAME_OVERHEAD(%r15)
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  	.endm
  
 -	.macro UPDATE_VTIME w1,w2,enter_timer
 -	lg	\w1,__LC_EXIT_TIMER
 -	lg	\w2,__LC_LAST_UPDATE_TIMER
 -	slg	\w1,\enter_timer
 -	slg	\w2,__LC_EXIT_TIMER
 -	alg	\w1,__LC_USER_TIMER
 -	alg	\w2,__LC_SYSTEM_TIMER
 -	stg	\w1,__LC_USER_TIMER
 -	stg	\w2,__LC_SYSTEM_TIMER
 -	mvc	__LC_LAST_UPDATE_TIMER(8),\enter_timer
 +	.macro	ADD64 high,low,timer
 +	al	\high,\timer
 +	al	\low,4+\timer
 +	brc	12,.+8
 +	ahi	\high,1
  	.endm
  
 -	.macro REENABLE_IRQS
 -	stg	%r8,__LC_RETURN_PSW
 -	ni	__LC_RETURN_PSW,0xbf
 -	ssm	__LC_RETURN_PSW
 +	.macro	SUB64 high,low,timer
 +	sl	\high,\timer
 +	sl	\low,4+\timer
 +	brc	3,.+8
 +	ahi	\high,-1
  	.endm
  
 -	.macro STCK savearea
 -#ifdef CONFIG_HAVE_MARCH_Z9_109_FEATURES
 -	.insn	s,0xb27c0000,\savearea		# store clock fast
 -#else
 -	.insn	s,0xb2050000,\savearea		# store clock
 -#endif
 +	.macro	UPDATE_VTIME high,low,enter_timer
 +	lm	\high,\low,__LC_EXIT_TIMER
 +	SUB64	\high,\low,\enter_timer
 +	ADD64	\high,\low,__LC_USER_TIMER
 +	stm	\high,\low,__LC_USER_TIMER
 +	lm	\high,\low,__LC_LAST_UPDATE_TIMER
 +	SUB64	\high,\low,__LC_EXIT_TIMER
 +	ADD64	\high,\low,__LC_SYSTEM_TIMER
 +	stm	\high,\low,__LC_SYSTEM_TIMER
 +	mvc	__LC_LAST_UPDATE_TIMER(8),\enter_timer
  	.endm
  
 -	/*
 -	 * The TSTMSK macro generates a test-under-mask instruction by
 -	 * calculating the memory offset for the specified mask value.
 -	 * Mask value can be any constant.  The macro shifts the mask
 -	 * value to calculate the memory offset for the test-under-mask
 -	 * instruction.
 -	 */
 -	.macro TSTMSK addr, mask, size=8, bytepos=0
 -		.if (\bytepos < \size) && (\mask >> 8)
 -			.if (\mask & 0xff)
 -				.error "Mask exceeds byte boundary"
 -			.endif
 -			TSTMSK \addr, "(\mask >> 8)", \size, "(\bytepos + 1)"
 -			.exitm
 -		.endif
 -		.ifeq \mask
 -			.error "Mask must not be zero"
 -		.endif
 -		off = \size - \bytepos - 1
 -		tm	off+\addr, \mask
 +	.macro REENABLE_IRQS
 +	st	%r8,__LC_RETURN_PSW
 +	ni	__LC_RETURN_PSW,0xbf
 +	ssm	__LC_RETURN_PSW
  	.endm
  
  	.macro BPOFF
@@@ -164,7 -188,51 +178,41 @@@
  	.popsection
  	.endm
  
+ 	.macro BPENTER tif_ptr,tif_mask
+ 	.pushsection .altinstr_replacement, "ax"
+ 662:	.word	0xc004, 0x0000, 0x0000	# 6 byte nop
+ 	.word	0xc004, 0x0000, 0x0000	# 6 byte nop
+ 	.popsection
+ 664:	TSTMSK	\tif_ptr,\tif_mask
+ 	jz	. + 8
+ 	.long	0xb2e8d000
+ 	.pushsection .altinstructions, "a"
+ 	.long 664b - .
+ 	.long 662b - .
+ 	.word 82
+ 	.byte 12
+ 	.byte 12
+ 	.popsection
+ 	.endm
+ 
+ 	.macro BPEXIT tif_ptr,tif_mask
+ 	TSTMSK	\tif_ptr,\tif_mask
+ 	.pushsection .altinstr_replacement, "ax"
+ 662:	jnz	. + 8
+ 	.long	0xb2e8d000
+ 	.popsection
+ 664:	jz	. + 8
+ 	.long	0xb2e8c000
+ 	.pushsection .altinstructions, "a"
+ 	.long 664b - .
+ 	.long 662b - .
+ 	.word 82
+ 	.byte 8
+ 	.byte 8
+ 	.popsection
+ 	.endm
+ 
  	.section .kprobes.text, "ax"
 -.Ldummy:
 -	/*
 -	 * This nop exists only in order to avoid that __switch_to starts at
 -	 * the beginning of the kprobes text section. In that case we would
 -	 * have several symbols at the same address. E.g. objdump would take
 -	 * an arbitrary symbol name when disassembling this code.
 -	 * With the added nop in between the __switch_to symbol is unique
 -	 * again.
 -	 */
 -	nop	0
  
  ENTRY(__bpon)
  	.globl __bpon
@@@ -179,26 -247,103 +227,108 @@@
   *  gpr2 = prev
   */
  ENTRY(__switch_to)
 -	stmg	%r6,%r15,__SF_GPRS(%r15)	# store gprs of prev task
 -	lghi	%r4,__TASK_stack
 -	lghi	%r1,__TASK_thread
 -	lg	%r5,0(%r4,%r3)			# start of kernel stack of next
 -	stg	%r15,__THREAD_ksp(%r1,%r2)	# store kernel stack of prev
 -	lgr	%r15,%r5
 -	aghi	%r15,STACK_INIT			# end of kernel stack of next
 -	stg	%r3,__LC_CURRENT		# store task struct of next
 -	stg	%r15,__LC_KERNEL_STACK		# store end of kernel stack
 -	lg	%r15,__THREAD_ksp(%r1,%r3)	# load kernel stack of next
 -	aghi	%r3,__TASK_pid
 -	mvc	__LC_CURRENT_PID(4,%r0),0(%r3)	# store pid of next
 -	lmg	%r6,%r15,__SF_GPRS(%r15)	# load gprs of next task
 -	TSTMSK	__LC_MACHINE_FLAGS,MACHINE_FLAG_LPP
 -	bzr	%r14
 -	.insn	s,0xb2800000,__LC_LPP		# set program parameter
 +	stm	%r6,%r15,__SF_GPRS(%r15)	# store gprs of prev task
 +	st	%r15,__THREAD_ksp(%r2)		# store kernel stack of prev
 +	l	%r4,__THREAD_info(%r2)		# get thread_info of prev
 +	l	%r5,__THREAD_info(%r3)		# get thread_info of next
 +	lr	%r15,%r5
 +	ahi	%r15,STACK_INIT			# end of kernel stack of next
 +	st	%r3,__LC_CURRENT		# store task struct of next
 +	st	%r5,__LC_THREAD_INFO		# store thread info of next
 +	st	%r15,__LC_KERNEL_STACK		# store end of kernel stack
 +	lctl	%c4,%c4,__TASK_pid(%r3)		# load pid to control reg. 4
 +	mvc	__LC_CURRENT_PID(4,%r0),__TASK_pid(%r3)	# store pid of next
 +	l	%r15,__THREAD_ksp(%r3)		# load kernel stack of next
 +	tm	__TI_flags+3(%r4),_TIF_MCCK_PENDING # machine check pending?
 +	jz	0f
 +	ni	__TI_flags+3(%r4),255-_TIF_MCCK_PENDING	# clear flag in prev
 +	oi	__TI_flags+3(%r5),_TIF_MCCK_PENDING	# set it in next
 +0:	lm	%r6,%r15,__SF_GPRS(%r15)	# load gprs of next task
  	br	%r14
  
++<<<<<<< HEAD
 +__critical_start:
++=======
+ .L__critical_start:
+ 
+ #if IS_ENABLED(CONFIG_KVM)
+ /*
+  * sie64a calling convention:
+  * %r2 pointer to sie control block
+  * %r3 guest register save area
+  */
+ ENTRY(sie64a)
+ 	stmg	%r6,%r14,__SF_GPRS(%r15)	# save kernel registers
+ 	lg	%r12,__LC_CURRENT
+ 	stg	%r2,__SF_EMPTY(%r15)		# save control block pointer
+ 	stg	%r3,__SF_EMPTY+8(%r15)		# save guest register save area
+ 	xc	__SF_EMPTY+16(8,%r15),__SF_EMPTY+16(%r15) # reason code = 0
+ 	mvc	__SF_EMPTY+24(8,%r15),__TI_flags(%r12) # copy thread flags
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU		# load guest fp/vx registers ?
+ 	jno	.Lsie_load_guest_gprs
+ 	brasl	%r14,load_fpu_regs		# load guest fp/vx regs
+ .Lsie_load_guest_gprs:
+ 	lmg	%r0,%r13,0(%r3)			# load guest gprs 0-13
+ 	lg	%r14,__LC_GMAP			# get gmap pointer
+ 	ltgr	%r14,%r14
+ 	jz	.Lsie_gmap
+ 	lctlg	%c1,%c1,__GMAP_ASCE(%r14)	# load primary asce
+ .Lsie_gmap:
+ 	lg	%r14,__SF_EMPTY(%r15)		# get control block pointer
+ 	oi	__SIE_PROG0C+3(%r14),1		# we are going into SIE now
+ 	tm	__SIE_PROG20+3(%r14),3		# last exit...
+ 	jnz	.Lsie_skip
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU
+ 	jo	.Lsie_skip			# exit if fp/vx regs changed
+ 	BPEXIT	__SF_EMPTY+24(%r15),(_TIF_ISOLATE_BP|_TIF_ISOLATE_BP_GUEST)
+ .Lsie_entry:
+ 	sie	0(%r14)
+ .Lsie_exit:
+ 	BPOFF
+ 	BPENTER	__SF_EMPTY+24(%r15),(_TIF_ISOLATE_BP|_TIF_ISOLATE_BP_GUEST)
+ .Lsie_skip:
+ 	ni	__SIE_PROG0C+3(%r14),0xfe	# no longer in SIE
+ 	lctlg	%c1,%c1,__LC_USER_ASCE		# load primary asce
+ .Lsie_done:
+ # some program checks are suppressing. C code (e.g. do_protection_exception)
+ # will rewind the PSW by the ILC, which is often 4 bytes in case of SIE. There
+ # are some corner cases (e.g. runtime instrumentation) where ILC is unpredictable.
+ # Other instructions between sie64a and .Lsie_done should not cause program
+ # interrupts. So lets use 3 nops as a landing pad for all possible rewinds.
+ # See also .Lcleanup_sie
+ .Lrewind_pad6:
+ 	nopr	7
+ .Lrewind_pad4:
+ 	nopr	7
+ .Lrewind_pad2:
+ 	nopr	7
+ 	.globl sie_exit
+ sie_exit:
+ 	lg	%r14,__SF_EMPTY+8(%r15)		# load guest register save area
+ 	stmg	%r0,%r13,0(%r14)		# save guest gprs 0-13
+ 	xgr	%r0,%r0				# clear guest registers to
+ 	xgr	%r1,%r1				# prevent speculative use
+ 	xgr	%r2,%r2
+ 	xgr	%r3,%r3
+ 	xgr	%r4,%r4
+ 	xgr	%r5,%r5
+ 	lmg	%r6,%r14,__SF_GPRS(%r15)	# restore kernel registers
+ 	lg	%r2,__SF_EMPTY+16(%r15)		# return exit reason code
+ 	br	%r14
+ .Lsie_fault:
+ 	lghi	%r14,-EFAULT
+ 	stg	%r14,__SF_EMPTY+16(%r15)	# set exit reason code
+ 	j	sie_exit
+ 
+ 	EX_TABLE(.Lrewind_pad6,.Lsie_fault)
+ 	EX_TABLE(.Lrewind_pad4,.Lsie_fault)
+ 	EX_TABLE(.Lrewind_pad2,.Lsie_fault)
+ 	EX_TABLE(sie_exit,.Lsie_fault)
+ EXPORT_SYMBOL(sie64a)
+ EXPORT_SYMBOL(sie_exit)
+ #endif
+ 
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  /*
   * SVC interrupt handler routine. System calls are synchronous events and
   * are executed with interrupts enabled.
@@@ -206,57 -351,67 +336,82 @@@
  
  ENTRY(system_call)
  	stpt	__LC_SYNC_ENTER_TIMER
 -.Lsysc_stmg:
 -	stmg	%r8,%r15,__LC_SAVE_AREA_SYNC
 +sysc_stm:
 +	stm	%r8,%r15,__LC_SAVE_AREA_SYNC
  	BPOFF
 -	lg	%r12,__LC_CURRENT
 -	lghi	%r13,__TASK_thread
 -	lghi	%r14,_PIF_SYSCALL
 -.Lsysc_per:
 -	lg	%r15,__LC_KERNEL_STACK
 +	l	%r12,__LC_THREAD_INFO
 +	l	%r13,__LC_SVC_NEW_PSW+4
 +sysc_per:
 +	l	%r15,__LC_KERNEL_STACK
  	la	%r11,STACK_FRAME_OVERHEAD(%r15)	# pointer to pt_regs
 -.Lsysc_vtime:
 +sysc_vtime:
  	UPDATE_VTIME %r8,%r9,__LC_SYNC_ENTER_TIMER
++<<<<<<< HEAD
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_SAVE_AREA_SYNC
 +	mvc	__PT_PSW(8,%r11),__LC_SVC_OLD_PSW
++=======
+ 	BPENTER __TI_flags(%r12),_TIF_ISOLATE_BP
+ 	stmg	%r0,%r7,__PT_R0(%r11)
+ 	# clear user controlled register to prevent speculative use
+ 	xgr	%r0,%r0
+ 	mvc	__PT_R8(64,%r11),__LC_SAVE_AREA_SYNC
+ 	mvc	__PT_PSW(16,%r11),__LC_SVC_OLD_PSW
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  	mvc	__PT_INT_CODE(4,%r11),__LC_SVC_ILC
 -	stg	%r14,__PT_FLAGS(%r11)
 -.Lsysc_do_svc:
 -	# load address of system call table
 -	lg	%r10,__THREAD_sysc_table(%r13,%r12)
 -	llgh	%r8,__PT_INT_CODE+2(%r11)
 -	slag	%r8,%r8,2			# shift and test for svc 0
 -	jnz	.Lsysc_nr_ok
 +sysc_do_svc:
 +	oi	__TI_flags+2(%r12),_TIF_SYSCALL>>8
 +	l	%r10,__TI_sysc_table(%r12)	# 31 bit system call table
 +	lh	%r8,__PT_INT_CODE+2(%r11)
 +	sla	%r8,2				# shift and test for svc0
 +	jnz	sysc_nr_ok
  	# svc 0: system call number in %r1
 -	llgfr	%r1,%r1				# clear high word in r1
 -	cghi	%r1,NR_syscalls
 -	jnl	.Lsysc_nr_ok
 +	cl	%r1,BASED(.Lnr_syscalls)
 +	jnl	sysc_nr_ok
  	sth	%r1,__PT_INT_CODE+2(%r11)
 -	slag	%r8,%r1,2
 -.Lsysc_nr_ok:
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	stg	%r2,__PT_ORIG_GPR2(%r11)
 -	stg	%r7,STACK_FRAME_OVERHEAD(%r15)
 -	lgf	%r9,0(%r8,%r10)			# get system call add.
 -	TSTMSK	__TI_flags(%r12),_TIF_TRACE
 -	jnz	.Lsysc_tracesys
 +	lr	%r8,%r1
 +	sla	%r8,2
 +sysc_nr_ok:
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	st	%r2,__PT_ORIG_GPR2(%r11)
 +	st	%r7,STACK_FRAME_OVERHEAD(%r15)
 +	l	%r9,0(%r8,%r10)			# get system call addr.
 +	tm	__TI_flags+2(%r12),_TIF_TRACE >> 8
 +	jnz	sysc_tracesys
  	basr	%r14,%r9			# call sys_xxxx
 -	stg	%r2,__PT_R2(%r11)		# store return value
 +	st	%r2,__PT_R2(%r11)		# store return value
  
 -.Lsysc_return:
 +sysc_return:
  	LOCKDEP_SYS_EXIT
++<<<<<<< HEAD
 +sysc_tif:
 +	tm	__PT_PSW+1(%r11),0x01		# returning to user ?
 +	jno	sysc_restore
 +	tm	__TI_flags+3(%r12),_TIF_WORK_SVC
 +	jnz	sysc_work			# check for work
 +	ni	__TI_flags+2(%r12),255-_TIF_SYSCALL>>8
 +	BPON
 +sysc_restore:
 +	mvc	__LC_RETURN_PSW(8),__PT_PSW(%r11)
++=======
+ .Lsysc_tif:
+ 	TSTMSK	__PT_FLAGS(%r11),_PIF_WORK
+ 	jnz	.Lsysc_work
+ 	TSTMSK	__TI_flags(%r12),_TIF_WORK
+ 	jnz	.Lsysc_work			# check for work
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_WORK
+ 	jnz	.Lsysc_work
+ 	BPEXIT	__TI_flags(%r12),_TIF_ISOLATE_BP
+ .Lsysc_restore:
+ 	lg	%r14,__LC_VDSO_PER_CPU
+ 	lmg	%r0,%r10,__PT_R0(%r11)
+ 	mvc	__LC_RETURN_PSW(16),__PT_PSW(%r11)
+ .Lsysc_exit_timer:
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  	stpt	__LC_EXIT_TIMER
 -	mvc	__VDSO_ECTG_BASE(16,%r14),__LC_EXIT_TIMER
 -	lmg	%r11,%r15,__PT_R11(%r11)
 -	lpswe	__LC_RETURN_PSW
 -.Lsysc_done:
 +	lm	%r0,%r15,__PT_R0(%r11)
 +	lpsw	__LC_RETURN_PSW
 +sysc_done:
  
  #
  # One of the work bits is on. Find out which one.
@@@ -395,51 -608,86 +550,82 @@@ ENTRY(kernel_thread_starter
  ENTRY(pgm_check_handler)
  	stpt	__LC_SYNC_ENTER_TIMER
  	BPOFF
 -	stmg	%r8,%r15,__LC_SAVE_AREA_SYNC
 -	lg	%r10,__LC_LAST_BREAK
 -	lg	%r12,__LC_CURRENT
 -	lghi	%r11,0
 -	larl	%r13,cleanup_critical
 -	lmg	%r8,%r9,__LC_PGM_OLD_PSW
 -	tmhh	%r8,0x0001		# test problem state bit
 -	jnz	2f			# -> fault in user space
 -#if IS_ENABLED(CONFIG_KVM)
 -	# cleanup critical section for program checks in sie64a
 -	lgr	%r14,%r9
 -	slg	%r14,BASED(.Lsie_critical_start)
 -	clg	%r14,BASED(.Lsie_critical_length)
 -	jhe	0f
 -	lg	%r14,__SF_EMPTY(%r15)		# get control block pointer
 -	ni	__SIE_PROG0C+3(%r14),0xfe	# no longer in SIE
 -	lctlg	%c1,%c1,__LC_USER_ASCE		# load primary asce
 -	larl	%r9,sie_exit			# skip forward to sie_exit
 -	lghi	%r11,_PIF_GUEST_FAULT
 -#endif
 -0:	tmhh	%r8,0x4000		# PER bit set in old PSW ?
 -	jnz	1f			# -> enabled, can't be a double fault
 +	stm	%r8,%r15,__LC_SAVE_AREA_SYNC
 +	l	%r12,__LC_THREAD_INFO
 +	l	%r13,__LC_SVC_NEW_PSW+4
 +	lm	%r8,%r9,__LC_PGM_OLD_PSW
 +	tmh	%r8,0x0001		# test problem state bit
 +	jnz	1f			# -> fault in user space
 +	tmh	%r8,0x4000		# PER bit set in old PSW ?
 +	jnz	0f			# -> enabled, can't be a double fault
  	tm	__LC_PGM_ILC+3,0x80	# check for per exception
++<<<<<<< HEAD
 +	jnz	pgm_svcper		# -> single stepped svc
 +0:	CHECK_STACK STACK_SIZE,__LC_SAVE_AREA_SYNC
 +	ahi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
 +	j	2f
 +1:	UPDATE_VTIME %r14,%r15,__LC_SYNC_ENTER_TIMER
 +	l	%r15,__LC_KERNEL_STACK
 +2:	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_SAVE_AREA_SYNC
 +	stm	%r8,%r9,__PT_PSW(%r11)
++=======
+ 	jnz	.Lpgm_svcper		# -> single stepped svc
+ 1:	CHECK_STACK STACK_SIZE,__LC_SAVE_AREA_SYNC
+ 	aghi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
+ 	j	4f
+ 2:	UPDATE_VTIME %r14,%r15,__LC_SYNC_ENTER_TIMER
+ 	BPENTER __TI_flags(%r12),_TIF_ISOLATE_BP
+ 	lg	%r15,__LC_KERNEL_STACK
+ 	lgr	%r14,%r12
+ 	aghi	%r14,__TASK_thread	# pointer to thread_struct
+ 	lghi	%r13,__LC_PGM_TDB
+ 	tm	__LC_PGM_ILC+2,0x02	# check for transaction abort
+ 	jz	3f
+ 	mvc	__THREAD_trap_tdb(256,%r14),0(%r13)
+ 3:	stg	%r10,__THREAD_last_break(%r14)
+ 4:	lgr	%r13,%r11
+ 	la	%r11,STACK_FRAME_OVERHEAD(%r15)
+ 	stmg	%r0,%r7,__PT_R0(%r11)
+ 	# clear user controlled registers to prevent speculative use
+ 	xgr	%r0,%r0
+ 	xgr	%r1,%r1
+ 	xgr	%r2,%r2
+ 	xgr	%r3,%r3
+ 	xgr	%r4,%r4
+ 	xgr	%r5,%r5
+ 	xgr	%r6,%r6
+ 	xgr	%r7,%r7
+ 	mvc	__PT_R8(64,%r11),__LC_SAVE_AREA_SYNC
+ 	stmg	%r8,%r9,__PT_PSW(%r11)
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  	mvc	__PT_INT_CODE(4,%r11),__LC_PGM_ILC
 -	mvc	__PT_INT_PARM_LONG(8,%r11),__LC_TRANS_EXC_CODE
 -	stg	%r13,__PT_FLAGS(%r11)
 -	stg	%r10,__PT_ARGS(%r11)
 +	mvc	__PT_INT_PARM_LONG(4,%r11),__LC_TRANS_EXC_CODE
  	tm	__LC_PGM_ILC+3,0x80	# check for per exception
 -	jz	5f
 -	tmhh	%r8,0x0001		# kernel per event ?
 -	jz	.Lpgm_kprobe
 -	oi	__PT_FLAGS+7(%r11),_PIF_PER_TRAP
 -	mvc	__THREAD_per_address(8,%r14),__LC_PER_ADDRESS
 -	mvc	__THREAD_per_cause(2,%r14),__LC_PER_CODE
 -	mvc	__THREAD_per_paid(1,%r14),__LC_PER_ACCESS_ID
 -5:	REENABLE_IRQS
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	larl	%r1,pgm_check_table
 -	llgh	%r10,__PT_INT_CODE+2(%r11)
 -	nill	%r10,0x007f
 +	jz	0f
 +	l	%r1,__TI_task(%r12)
 +	tmh	%r8,0x0001		# kernel per event ?
 +	jz	pgm_kprobe
 +	oi	__TI_flags+3(%r12),_TIF_PER_TRAP
 +	mvc	__THREAD_per_address(4,%r1),__LC_PER_ADDRESS
 +	mvc	__THREAD_per_cause(2,%r1),__LC_PER_CAUSE
 +	mvc	__THREAD_per_paid(1,%r1),__LC_PER_PAID
 +0:	REENABLE_IRQS
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	l	%r1,BASED(.Ljump_table)
 +	la	%r10,0x7f
 +	n	%r10,__PT_INT_CODE(%r11)
 +	je	pgm_exit
  	sll	%r10,2
 -	je	.Lpgm_return
 -	lgf	%r1,0(%r10,%r1)		# load address of handler routine
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 +	l	%r1,0(%r10,%r1)		# load address of handler routine
 +	lr	%r2,%r11		# pass pointer to pt_regs
  	basr	%r14,%r1		# branch to interrupt-handler
 -.Lpgm_return:
 +pgm_exit:
  	LOCKDEP_SYS_EXIT
  	tm	__PT_PSW+1(%r11),0x01	# returning to user ?
 -	jno	.Lsysc_restore
 -	TSTMSK	__PT_FLAGS(%r11),_PIF_SYSCALL
 -	jo	.Lsysc_do_syscall
 -	j	.Lsysc_tif
 +	jno	sysc_restore
 +	j	sysc_tif
  
  #
  # PER event in supervisor state, must be kprobes
@@@ -464,44 -713,70 +650,50 @@@ pgm_svcper
  /*
   * IO interrupt handler routine
   */
 +
  ENTRY(io_int_handler)
 -	STCK	__LC_INT_CLOCK
 +	stck	__LC_INT_CLOCK
  	stpt	__LC_ASYNC_ENTER_TIMER
  	BPOFF
 -	stmg	%r8,%r15,__LC_SAVE_AREA_ASYNC
 -	lg	%r12,__LC_CURRENT
 -	larl	%r13,cleanup_critical
 -	lmg	%r8,%r9,__LC_IO_OLD_PSW
 -	SWITCH_ASYNC __LC_SAVE_AREA_ASYNC,__LC_ASYNC_ENTER_TIMER
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	# clear user controlled registers to prevent speculative use
 -	xgr	%r0,%r0
 -	xgr	%r1,%r1
 -	xgr	%r2,%r2
 -	xgr	%r3,%r3
 -	xgr	%r4,%r4
 -	xgr	%r5,%r5
 -	xgr	%r6,%r6
 -	xgr	%r7,%r7
 -	xgr	%r10,%r10
 -	mvc	__PT_R8(64,%r11),__LC_SAVE_AREA_ASYNC
 -	stmg	%r8,%r9,__PT_PSW(%r11)
 -	mvc	__PT_INT_CODE(12,%r11),__LC_SUBCHANNEL_ID
 -	xc	__PT_FLAGS(8,%r11),__PT_FLAGS(%r11)
 -	TSTMSK	__LC_CPU_FLAGS,_CIF_IGNORE_IRQ
 -	jo	.Lio_restore
 +	stm	%r8,%r15,__LC_SAVE_AREA_ASYNC
 +	l	%r12,__LC_THREAD_INFO
 +	l	%r13,__LC_SVC_NEW_PSW+4
 +	lm	%r8,%r9,__LC_IO_OLD_PSW
 +	tmh	%r8,0x0001		# interrupting from user ?
 +	jz	io_skip
 +	UPDATE_VTIME %r14,%r15,__LC_ASYNC_ENTER_TIMER
 +io_skip:
 +	SWITCH_ASYNC __LC_SAVE_AREA_ASYNC,__LC_ASYNC_STACK,STACK_SHIFT
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_SAVE_AREA_ASYNC
 +	stm	%r8,%r9,__PT_PSW(%r11)
  	TRACE_IRQS_OFF
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -.Lio_loop:
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 -	lghi	%r3,IO_INTERRUPT
 -	tm	__PT_INT_CODE+8(%r11),0x80	# adapter interrupt ?
 -	jz	.Lio_call
 -	lghi	%r3,THIN_INTERRUPT
 -.Lio_call:
 -	brasl	%r14,do_IRQ
 -	TSTMSK	__LC_MACHINE_FLAGS,MACHINE_FLAG_LPAR
 -	jz	.Lio_return
 -	tpi	0
 -	jz	.Lio_return
 -	mvc	__PT_INT_CODE(12,%r11),__LC_SUBCHANNEL_ID
 -	j	.Lio_loop
 -.Lio_return:
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	l	%r1,BASED(.Ldo_IRQ)
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	basr	%r14,%r1		# call do_IRQ
 +io_return:
  	LOCKDEP_SYS_EXIT
  	TRACE_IRQS_ON
 -.Lio_tif:
 -	TSTMSK	__TI_flags(%r12),_TIF_WORK
 -	jnz	.Lio_work		# there is work to do (signals etc.)
 -	TSTMSK	__LC_CPU_FLAGS,_CIF_WORK
 -	jnz	.Lio_work
 -.Lio_restore:
 -	lg	%r14,__LC_VDSO_PER_CPU
 -	lmg	%r0,%r10,__PT_R0(%r11)
 -	mvc	__LC_RETURN_PSW(16),__PT_PSW(%r11)
 +io_tif:
 +	tm	__TI_flags+3(%r12),_TIF_WORK_INT
 +	jnz	io_work			# there is work to do (signals etc.)
 +io_restore:
 +	mvc	__LC_RETURN_PSW(8),__PT_PSW(%r11)
  	tm	__PT_PSW+1(%r11),0x01	# returning to user ?
++<<<<<<< HEAD
 +	jno	io_exit_kernel
 +	BPON
++=======
+ 	jno	.Lio_exit_kernel
+ 	BPEXIT	__TI_flags(%r12),_TIF_ISOLATE_BP
+ .Lio_exit_timer:
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  	stpt	__LC_EXIT_TIMER
 -	mvc	__VDSO_ECTG_BASE(16,%r14),__LC_EXIT_TIMER
 -.Lio_exit_kernel:
 -	lmg	%r11,%r15,__PT_R11(%r11)
 -	lpswe	__LC_RETURN_PSW
 -.Lio_done:
 +io_exit_kernel:
 +	lm	%r0,%r15,__PT_R0(%r11)
 +	lpsw	__LC_RETURN_PSW
 +io_done:
  
  #
  # There is work todo, find out in which context we have been interrupted:
@@@ -697,52 -1160,61 +889,52 @@@ ENTRY(mcck_int_handler
  	la	%r14,__LC_LAST_UPDATE_TIMER
  2:	spt	0(%r14)
  	mvc	__LC_MCCK_ENTER_TIMER(8),0(%r14)
 -3:	TSTMSK	__LC_MCCK_CODE,MCCK_CODE_PSW_MWP_VALID
 -	jno	.Lmcck_panic
 -	tmhh	%r8,0x0001		# interrupting from user ?
 -	jnz	4f
 -	TSTMSK	__LC_MCCK_CODE,MCCK_CODE_PSW_IA_VALID
 -	jno	.Lmcck_panic
 -4:	SWITCH_ASYNC __LC_GPREGS_SAVE_AREA+64,__LC_MCCK_ENTER_TIMER
 -.Lmcck_skip:
 -	lghi	%r14,__LC_GPREGS_SAVE_AREA+64
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	# clear user controlled registers to prevent speculative use
 -	xgr	%r0,%r0
 -	xgr	%r1,%r1
 -	xgr	%r2,%r2
 -	xgr	%r3,%r3
 -	xgr	%r4,%r4
 -	xgr	%r5,%r5
 -	xgr	%r6,%r6
 -	xgr	%r7,%r7
 -	xgr	%r10,%r10
 -	mvc	__PT_R8(64,%r11),0(%r14)
 -	stmg	%r8,%r9,__PT_PSW(%r11)
 -	xc	__PT_FLAGS(8,%r11),__PT_FLAGS(%r11)
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 -	brasl	%r14,s390_do_machine_check
 +3:	tm	__LC_MCCK_CODE+2,0x09	# mwp + ia of old psw valid?
 +	jno	mcck_panic		# no -> skip cleanup critical
 +	tm	%r8,0x0001		# interrupting from user ?
 +	jz	mcck_skip
 +	UPDATE_VTIME %r14,%r15,__LC_MCCK_ENTER_TIMER
 +mcck_skip:
 +	SWITCH_ASYNC __LC_GPREGS_SAVE_AREA+32,__LC_PANIC_STACK,PAGE_SHIFT
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_GPREGS_SAVE_AREA+32
 +	stm	%r8,%r9,__PT_PSW(%r11)
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	l	%r1,BASED(.Ldo_machine_check)
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	basr	%r14,%r1		# call s390_do_machine_check
  	tm	__PT_PSW+1(%r11),0x01	# returning to user ?
 -	jno	.Lmcck_return
 -	lg	%r1,__LC_KERNEL_STACK	# switch to kernel stack
 +	jno	mcck_return
 +	l	%r1,__LC_KERNEL_STACK	# switch to kernel stack
  	mvc	STACK_FRAME_OVERHEAD(__PT_SIZE,%r1),0(%r11)
 -	xc	__SF_BACKCHAIN(8,%r1),__SF_BACKCHAIN(%r1)
 -	la	%r11,STACK_FRAME_OVERHEAD(%r1)
 -	lgr	%r15,%r1
 +	xc	__SF_BACKCHAIN(4,%r1),__SF_BACKCHAIN(%r1)
 +	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 +	lr	%r15,%r1
  	ssm	__LC_PGM_NEW_PSW	# turn dat on, keep irqs off
 -	TSTMSK	__LC_CPU_FLAGS,_CIF_MCCK_PENDING
 -	jno	.Lmcck_return
 +	tm	__TI_flags+3(%r12),_TIF_MCCK_PENDING
 +	jno	mcck_return
  	TRACE_IRQS_OFF
 -	brasl	%r14,s390_handle_mcck
 +	l	%r1,BASED(.Lhandle_mcck)
 +	basr	%r14,%r1		# call s390_handle_mcck
  	TRACE_IRQS_ON
 -.Lmcck_return:
 -	lg	%r14,__LC_VDSO_PER_CPU
 -	lmg	%r0,%r10,__PT_R0(%r11)
 -	mvc	__LC_RETURN_MCCK_PSW(16),__PT_PSW(%r11) # move return PSW
 +mcck_return:
 +	mvc	__LC_RETURN_MCCK_PSW(8),__PT_PSW(%r11) # move return PSW
 +	lm	%r0,%r15,__PT_R0(%r11)
  	tm	__LC_RETURN_MCCK_PSW+1,0x01 # returning to user ?
  	jno	0f
- 	BPON
+ 	BPEXIT	__TI_flags(%r12),_TIF_ISOLATE_BP
  	stpt	__LC_EXIT_TIMER
 -	mvc	__VDSO_ECTG_BASE(16,%r14),__LC_EXIT_TIMER
 -0:	lmg	%r11,%r15,__PT_R11(%r11)
 -	lpswe	__LC_RETURN_MCCK_PSW
 +0:	lpsw	__LC_RETURN_MCCK_PSW
  
 -.Lmcck_panic:
 -	lg	%r15,__LC_PANIC_STACK
 -	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 -	j	.Lmcck_skip
 +mcck_panic:
 +	l	%r14,__LC_PANIC_STACK
 +	slr	%r14,%r15
 +	sra	%r14,PAGE_SHIFT
 +	jz	0f
 +	l	%r15,__LC_PANIC_STACK
 +	j	mcck_skip
 +0:	ahi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
 +	j	mcck_skip
  
  #
  # PSW restart interrupt handler
@@@ -780,141 -1255,198 +972,182 @@@ ENTRY(restart_int_handler
   * Setup a pt_regs so that show_trace can provide a good call trace.
   */
  stack_overflow:
 -	lg	%r15,__LC_PANIC_STACK	# change to panic stack
 +	l	%r15,__LC_PANIC_STACK	# change to panic stack
  	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	stmg	%r8,%r9,__PT_PSW(%r11)
 -	mvc	__PT_R8(64,%r11),0(%r14)
 -	stg	%r10,__PT_ORIG_GPR2(%r11) # store last break to orig_gpr2
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 -	jg	kernel_stack_overflow
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	stm	%r8,%r9,__PT_PSW(%r11)
 +	mvc	__PT_R8(32,%r11),0(%r14)
 +	l	%r1,BASED(1f)
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	br	%r1			# branch to kernel_stack_overflow
 +1:	.long	kernel_stack_overflow
  #endif
  
 +cleanup_table:
 +	.long	system_call + 0x80000000
 +	.long	sysc_do_svc + 0x80000000
 +	.long	sysc_tif + 0x80000000
 +	.long	sysc_restore + 0x80000000
 +	.long	sysc_done + 0x80000000
 +	.long	io_tif + 0x80000000
 +	.long	io_restore + 0x80000000
 +	.long	io_done + 0x80000000
 +	.long	psw_idle + 0x80000000
 +	.long	psw_idle_end + 0x80000000
 +
  cleanup_critical:
 -#if IS_ENABLED(CONFIG_KVM)
 -	clg	%r9,BASED(.Lcleanup_table_sie)	# .Lsie_gmap
 -	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table_sie+8)# .Lsie_done
 -	jl	.Lcleanup_sie
 -#endif
 -	clg	%r9,BASED(.Lcleanup_table)	# system_call
 -	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+8)	# .Lsysc_do_svc
 -	jl	.Lcleanup_system_call
 -	clg	%r9,BASED(.Lcleanup_table+16)	# .Lsysc_tif
 +	cl	%r9,BASED(cleanup_table)	# system_call
  	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+24)	# .Lsysc_restore
 -	jl	.Lcleanup_sysc_tif
 -	clg	%r9,BASED(.Lcleanup_table+32)	# .Lsysc_done
 -	jl	.Lcleanup_sysc_restore
 -	clg	%r9,BASED(.Lcleanup_table+40)	# .Lio_tif
 +	cl	%r9,BASED(cleanup_table+4)	# sysc_do_svc
 +	jl	cleanup_system_call
 +	cl	%r9,BASED(cleanup_table+8)	# sysc_tif
  	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+48)	# .Lio_restore
 -	jl	.Lcleanup_io_tif
 -	clg	%r9,BASED(.Lcleanup_table+56)	# .Lio_done
 -	jl	.Lcleanup_io_restore
 -	clg	%r9,BASED(.Lcleanup_table+64)	# psw_idle
 +	cl	%r9,BASED(cleanup_table+12)	# sysc_restore
 +	jl	cleanup_sysc_tif
 +	cl	%r9,BASED(cleanup_table+16)	# sysc_done
 +	jl	cleanup_sysc_restore
 +	cl	%r9,BASED(cleanup_table+20)	# io_tif
  	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+72)	# .Lpsw_idle_end
 -	jl	.Lcleanup_idle
 -	clg	%r9,BASED(.Lcleanup_table+80)	# save_fpu_regs
 +	cl	%r9,BASED(cleanup_table+24)	# io_restore
 +	jl	cleanup_io_tif
 +	cl	%r9,BASED(cleanup_table+28)	# io_done
 +	jl	cleanup_io_restore
 +	cl	%r9,BASED(cleanup_table+32)	# psw_idle
  	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+88)	# .Lsave_fpu_regs_end
 -	jl	.Lcleanup_save_fpu_regs
 -	clg	%r9,BASED(.Lcleanup_table+96)	# load_fpu_regs
 -	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+104)	# .Lload_fpu_regs_end
 -	jl	.Lcleanup_load_fpu_regs
 +	cl	%r9,BASED(cleanup_table+36)	# psw_idle_end
 +	jl	cleanup_idle
  0:	br	%r14
  
++<<<<<<< HEAD
 +cleanup_system_call:
++=======
+ 	.align	8
+ .Lcleanup_table:
+ 	.quad	system_call
+ 	.quad	.Lsysc_do_svc
+ 	.quad	.Lsysc_tif
+ 	.quad	.Lsysc_restore
+ 	.quad	.Lsysc_done
+ 	.quad	.Lio_tif
+ 	.quad	.Lio_restore
+ 	.quad	.Lio_done
+ 	.quad	psw_idle
+ 	.quad	.Lpsw_idle_end
+ 	.quad	save_fpu_regs
+ 	.quad	.Lsave_fpu_regs_end
+ 	.quad	load_fpu_regs
+ 	.quad	.Lload_fpu_regs_end
+ 
+ #if IS_ENABLED(CONFIG_KVM)
+ .Lcleanup_table_sie:
+ 	.quad	.Lsie_gmap
+ 	.quad	.Lsie_done
+ 
+ .Lcleanup_sie:
+ 	cghi    %r11,__LC_SAVE_AREA_ASYNC 	#Is this in normal interrupt?
+ 	je      1f
+ 	slg     %r9,BASED(.Lsie_crit_mcck_start)
+ 	clg     %r9,BASED(.Lsie_crit_mcck_length)
+ 	jh      1f
+ 	oi      __LC_CPU_FLAGS+7, _CIF_MCCK_GUEST
+ 1:	BPENTER __SF_EMPTY+24(%r15),(_TIF_ISOLATE_BP|_TIF_ISOLATE_BP_GUEST)
+ 	lg	%r9,__SF_EMPTY(%r15)		# get control block pointer
+ 	ni	__SIE_PROG0C+3(%r9),0xfe	# no longer in SIE
+ 	lctlg	%c1,%c1,__LC_USER_ASCE		# load primary asce
+ 	larl	%r9,sie_exit			# skip forward to sie_exit
+ 	br	%r14
+ #endif
+ 
+ .Lcleanup_system_call:
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
  	# check if stpt has been executed
 -	clg	%r9,BASED(.Lcleanup_system_call_insn)
 +	cl	%r9,BASED(cleanup_system_call_insn)
  	jh	0f
  	mvc	__LC_SYNC_ENTER_TIMER(8),__LC_ASYNC_ENTER_TIMER
 -	cghi	%r11,__LC_SAVE_AREA_ASYNC
 +	chi	%r11,__LC_SAVE_AREA_ASYNC
  	je	0f
  	mvc	__LC_SYNC_ENTER_TIMER(8),__LC_MCCK_ENTER_TIMER
 -0:	# check if stmg has been executed
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+8)
 +0:	# check if stm has been executed
 +	cl	%r9,BASED(cleanup_system_call_insn+4)
  	jh	0f
 -	mvc	__LC_SAVE_AREA_SYNC(64),0(%r11)
 -0:	# check if base register setup + TIF bit load has been done
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+16)
 -	jhe	0f
 -	# set up saved register r12 task struct pointer
 -	stg	%r12,32(%r11)
 -	# set up saved register r13 __TASK_thread offset
 -	mvc	40(8,%r11),BASED(.Lcleanup_system_call_const)
 -0:	# check if the user time update has been done
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+24)
 +	mvc	__LC_SAVE_AREA_SYNC(32),0(%r11)
 +0:	# set up saved registers r12, and r13
 +	st	%r12,16(%r11)		# r12 thread-info pointer
 +	st	%r13,20(%r11)		# r13 literal-pool pointer
 +	# check if the user time calculation has been done
 +	cl	%r9,BASED(cleanup_system_call_insn+8)
  	jh	0f
 -	lg	%r15,__LC_EXIT_TIMER
 -	slg	%r15,__LC_SYNC_ENTER_TIMER
 -	alg	%r15,__LC_USER_TIMER
 -	stg	%r15,__LC_USER_TIMER
 -0:	# check if the system time update has been done
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+32)
 +	l	%r10,__LC_EXIT_TIMER
 +	l	%r15,__LC_EXIT_TIMER+4
 +	SUB64	%r10,%r15,__LC_SYNC_ENTER_TIMER
 +	ADD64	%r10,%r15,__LC_USER_TIMER
 +	st	%r10,__LC_USER_TIMER
 +	st	%r15,__LC_USER_TIMER+4
 +0:	# check if the system time calculation has been done
 +	cl	%r9,BASED(cleanup_system_call_insn+12)
  	jh	0f
 -	lg	%r15,__LC_LAST_UPDATE_TIMER
 -	slg	%r15,__LC_EXIT_TIMER
 -	alg	%r15,__LC_SYSTEM_TIMER
 -	stg	%r15,__LC_SYSTEM_TIMER
 +	l	%r10,__LC_LAST_UPDATE_TIMER
 +	l	%r15,__LC_LAST_UPDATE_TIMER+4
 +	SUB64	%r10,%r15,__LC_EXIT_TIMER
 +	ADD64	%r10,%r15,__LC_SYSTEM_TIMER
 +	st	%r10,__LC_SYSTEM_TIMER
 +	st	%r15,__LC_SYSTEM_TIMER+4
  0:	# update accounting time stamp
  	mvc	__LC_LAST_UPDATE_TIMER(8),__LC_SYNC_ENTER_TIMER
 -	# set up saved register r11
 -	lg	%r15,__LC_KERNEL_STACK
 +	# set up saved register 11
 +	l	%r15,__LC_KERNEL_STACK
  	la	%r9,STACK_FRAME_OVERHEAD(%r15)
 -	stg	%r9,24(%r11)		# r11 pt_regs pointer
 +	st	%r9,12(%r11)		# r11 pt_regs pointer
  	# fill pt_regs
 -	mvc	__PT_R8(64,%r9),__LC_SAVE_AREA_SYNC
 -	stmg	%r0,%r7,__PT_R0(%r9)
 -	mvc	__PT_PSW(16,%r9),__LC_SVC_OLD_PSW
 +	mvc	__PT_R8(32,%r9),__LC_SAVE_AREA_SYNC
 +	stm	%r0,%r7,__PT_R0(%r9)
 +	mvc	__PT_PSW(8,%r9),__LC_SVC_OLD_PSW
  	mvc	__PT_INT_CODE(4,%r9),__LC_SVC_ILC
 -	xc	__PT_FLAGS(8,%r9),__PT_FLAGS(%r9)
 -	mvi	__PT_FLAGS+7(%r9),_PIF_SYSCALL
 -	# setup saved register r15
 -	stg	%r15,56(%r11)		# r15 stack pointer
 +	# setup saved register 15
 +	st	%r15,28(%r11)		# r15 stack pointer
  	# set new psw address and exit
 -	larl	%r9,.Lsysc_do_svc
 +	l	%r9,BASED(cleanup_table+4)	# sysc_do_svc + 0x80000000
  	br	%r14
 -.Lcleanup_system_call_insn:
 -	.quad	system_call
 -	.quad	.Lsysc_stmg
 -	.quad	.Lsysc_per
 -	.quad	.Lsysc_vtime+36
 -	.quad	.Lsysc_vtime+42
 -.Lcleanup_system_call_const:
 -	.quad	__TASK_thread
 -
 -.Lcleanup_sysc_tif:
 -	larl	%r9,.Lsysc_tif
 +cleanup_system_call_insn:
 +	.long	system_call + 0x80000000
 +	.long	sysc_stm + 0x80000000
 +	.long	sysc_vtime + 0x80000000 + 36
 +	.long	sysc_vtime + 0x80000000 + 76
 +
 +cleanup_sysc_tif:
 +	l	%r9,BASED(cleanup_table+8)	# sysc_tif + 0x80000000
  	br	%r14
  
 -.Lcleanup_sysc_restore:
 -	# check if stpt has been executed
 -	clg	%r9,BASED(.Lcleanup_sysc_restore_insn)
 -	jh	0f
 -	mvc	__LC_EXIT_TIMER(8),__LC_ASYNC_ENTER_TIMER
 -	cghi	%r11,__LC_SAVE_AREA_ASYNC
 -	je	0f
 -	mvc	__LC_EXIT_TIMER(8),__LC_MCCK_ENTER_TIMER
 -0:	clg	%r9,BASED(.Lcleanup_sysc_restore_insn+8)
 -	je	1f
 -	lg	%r9,24(%r11)		# get saved pointer to pt_regs
 -	mvc	__LC_RETURN_PSW(16),__PT_PSW(%r9)
 -	mvc	0(64,%r11),__PT_R8(%r9)
 -	lmg	%r0,%r7,__PT_R0(%r9)
 -1:	lmg	%r8,%r9,__LC_RETURN_PSW
 +cleanup_sysc_restore:
 +	cl	%r9,BASED(cleanup_sysc_restore_insn)
 +	jhe	0f
 +	l	%r9,12(%r11)		# get saved pointer to pt_regs
 +	mvc	__LC_RETURN_PSW(8),__PT_PSW(%r9)
 +	mvc	0(32,%r11),__PT_R8(%r9)
 +	lm	%r0,%r7,__PT_R0(%r9)
 +0:	lm	%r8,%r9,__LC_RETURN_PSW
  	br	%r14
 -.Lcleanup_sysc_restore_insn:
 -	.quad	.Lsysc_exit_timer
 -	.quad	.Lsysc_done - 4
 +cleanup_sysc_restore_insn:
 +	.long	sysc_done - 4 + 0x80000000
  
 -.Lcleanup_io_tif:
 -	larl	%r9,.Lio_tif
 +cleanup_io_tif:
 +	l	%r9,BASED(cleanup_table+20)	# io_tif + 0x80000000
  	br	%r14
  
 -.Lcleanup_io_restore:
 -	# check if stpt has been executed
 -	clg	%r9,BASED(.Lcleanup_io_restore_insn)
 -	jh	0f
 -	mvc	__LC_EXIT_TIMER(8),__LC_MCCK_ENTER_TIMER
 -0:	clg	%r9,BASED(.Lcleanup_io_restore_insn+8)
 -	je	1f
 -	lg	%r9,24(%r11)		# get saved r11 pointer to pt_regs
 -	mvc	__LC_RETURN_PSW(16),__PT_PSW(%r9)
 -	mvc	0(64,%r11),__PT_R8(%r9)
 -	lmg	%r0,%r7,__PT_R0(%r9)
 -1:	lmg	%r8,%r9,__LC_RETURN_PSW
 +cleanup_io_restore:
 +	cl	%r9,BASED(cleanup_io_restore_insn)
 +	jhe	0f
 +	l	%r9,12(%r11)		# get saved r11 pointer to pt_regs
 +	mvc	__LC_RETURN_PSW(8),__PT_PSW(%r9)
 +	mvc	0(32,%r11),__PT_R8(%r9)
 +	lm	%r0,%r7,__PT_R0(%r9)
 +0:	lm	%r8,%r9,__LC_RETURN_PSW
  	br	%r14
 -.Lcleanup_io_restore_insn:
 -	.quad	.Lio_exit_timer
 -	.quad	.Lio_done - 4
 +cleanup_io_restore_insn:
 +	.long	io_done - 4 + 0x80000000
  
 -.Lcleanup_idle:
 -	ni	__LC_CPU_FLAGS+7,255-_CIF_ENABLED_WAIT
 +cleanup_idle:
  	# copy interrupt clock & cpu timer
  	mvc	__CLOCK_IDLE_EXIT(8,%r2),__LC_INT_CLOCK
  	mvc	__TIMER_IDLE_EXIT(8,%r2),__LC_ASYNC_ENTER_TIMER
diff --cc arch/s390/kernel/processor.c
index 3ed66b0373dc,6fe2e1875058..000000000000
--- a/arch/s390/kernel/processor.c
+++ b/arch/s390/kernel/processor.c
@@@ -125,3 -198,20 +125,23 @@@ const struct seq_operations cpuinfo_op 
  	.show	= show_cpuinfo,
  };
  
++<<<<<<< HEAD
++=======
+ int s390_isolate_bp(void)
+ {
+ 	if (!test_facility(82))
+ 		return -EOPNOTSUPP;
+ 	set_thread_flag(TIF_ISOLATE_BP);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(s390_isolate_bp);
+ 
+ int s390_isolate_bp_guest(void)
+ {
+ 	if (!test_facility(82))
+ 		return -EOPNOTSUPP;
+ 	set_thread_flag(TIF_ISOLATE_BP_GUEST);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(s390_isolate_bp_guest);
++>>>>>>> 6b73044b2b00 (s390: run user space and KVM guests with modified branch prediction)
diff --git a/arch/s390/include/asm/processor.h b/arch/s390/include/asm/processor.h
index 36c10ab6af98..36053350f050 100644
--- a/arch/s390/include/asm/processor.h
+++ b/arch/s390/include/asm/processor.h
@@ -396,6 +396,9 @@ extern void memcpy_absolute(void *, void *, size_t);
 	.long	(_target) - . ;		\
 	.previous
 
+extern int s390_isolate_bp(void);
+extern int s390_isolate_bp_guest(void);
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* __ASM_S390_PROCESSOR_H */
* Unmerged path arch/s390/include/asm/thread_info.h
* Unmerged path arch/s390/kernel/entry.S
* Unmerged path arch/s390/kernel/processor.c
