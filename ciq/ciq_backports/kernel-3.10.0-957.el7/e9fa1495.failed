ipv6: Reflect MTU changes on PMTU of exceptions for MTU-less routes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Stefano Brivio <sbrivio@redhat.com>
commit e9fa1495d738e34fcec88a3d2ec9101a9ee5b310
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/e9fa1495.failed

Currently, administrative MTU changes on a given netdevice are
not reflected on route exceptions for MTU-less routes, with a
set PMTU value, for that device:

 # ip -6 route get 2001:db8::b
 2001:db8::b from :: dev vti_a proto kernel src 2001:db8::a metric 256 pref medium
 # ping6 -c 1 -q -s10000 2001:db8::b > /dev/null
 # ip netns exec a ip -6 route get 2001:db8::b
 2001:db8::b from :: dev vti_a src 2001:db8::a metric 0
     cache expires 571sec mtu 4926 pref medium
 # ip link set dev vti_a mtu 3000
 # ip -6 route get 2001:db8::b
 2001:db8::b from :: dev vti_a src 2001:db8::a metric 0
     cache expires 571sec mtu 4926 pref medium
 # ip link set dev vti_a mtu 9000
 # ip -6 route get 2001:db8::b
 2001:db8::b from :: dev vti_a src 2001:db8::a metric 0
     cache expires 571sec mtu 4926 pref medium

The first issue is that since commit fb56be83e43d ("net-ipv6: on
device mtu change do not add mtu to mtu-less routes") we don't
call rt6_exceptions_update_pmtu() from rt6_mtu_change_route(),
which handles administrative MTU changes, if the regular route
is MTU-less.

However, PMTU exceptions should be always updated, as long as
RTAX_MTU is not locked. Keep the check for MTU-less main route,
as introduced by that commit, but, for exceptions,
call rt6_exceptions_update_pmtu() regardless of that check.

Once that is fixed, one problem remains: MTU changes are not
reflected if the new MTU is higher than the previous one,
because rt6_exceptions_update_pmtu() doesn't allow that. We
should instead allow PMTU increase if the old PMTU matches the
local MTU, as that implies that the old MTU was the lowest in the
path, and PMTU discovery might lead to different results.

The existing check in rt6_mtu_change_route() correctly took that
case into account (for regular routes only), so factor it out
and re-use it also in rt6_exceptions_update_pmtu().

While at it, fix comments style and grammar, and try to be a bit
more descriptive.

	Reported-by: Xiumei Mu <xmu@redhat.com>
Fixes: fb56be83e43d ("net-ipv6: on device mtu change do not add mtu to mtu-less routes")
Fixes: f5bbe7ee79c2 ("ipv6: prepare rt6_mtu_change() for exception table")
	Signed-off-by: Stefano Brivio <sbrivio@redhat.com>
	Acked-by: David Ahern <dsahern@gmail.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e9fa1495d738e34fcec88a3d2ec9101a9ee5b310)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv6/route.c
diff --cc net/ipv6/route.c
index d1226c42fbb2,0db4218c9186..000000000000
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@@ -1078,11 -1134,548 +1078,553 @@@ static struct rt6_info *rt6_make_pcpu_r
  	return pcpu_rt;
  }
  
++<<<<<<< HEAD
 +static struct rt6_info *ip6_pol_route(struct net *net, struct fib6_table *table, int oif,
 +				      struct flowi6 *fl6, int flags)
++=======
+ /* exception hash table implementation
+  */
+ static DEFINE_SPINLOCK(rt6_exception_lock);
+ 
+ /* Remove rt6_ex from hash table and free the memory
+  * Caller must hold rt6_exception_lock
+  */
+ static void rt6_remove_exception(struct rt6_exception_bucket *bucket,
+ 				 struct rt6_exception *rt6_ex)
+ {
+ 	struct net *net;
+ 
+ 	if (!bucket || !rt6_ex)
+ 		return;
+ 
+ 	net = dev_net(rt6_ex->rt6i->dst.dev);
+ 	rt6_ex->rt6i->rt6i_node = NULL;
+ 	hlist_del_rcu(&rt6_ex->hlist);
+ 	rt6_release(rt6_ex->rt6i);
+ 	kfree_rcu(rt6_ex, rcu);
+ 	WARN_ON_ONCE(!bucket->depth);
+ 	bucket->depth--;
+ 	net->ipv6.rt6_stats->fib_rt_cache--;
+ }
+ 
+ /* Remove oldest rt6_ex in bucket and free the memory
+  * Caller must hold rt6_exception_lock
+  */
+ static void rt6_exception_remove_oldest(struct rt6_exception_bucket *bucket)
+ {
+ 	struct rt6_exception *rt6_ex, *oldest = NULL;
+ 
+ 	if (!bucket)
+ 		return;
+ 
+ 	hlist_for_each_entry(rt6_ex, &bucket->chain, hlist) {
+ 		if (!oldest || time_before(rt6_ex->stamp, oldest->stamp))
+ 			oldest = rt6_ex;
+ 	}
+ 	rt6_remove_exception(bucket, oldest);
+ }
+ 
+ static u32 rt6_exception_hash(const struct in6_addr *dst,
+ 			      const struct in6_addr *src)
+ {
+ 	static u32 seed __read_mostly;
+ 	u32 val;
+ 
+ 	net_get_random_once(&seed, sizeof(seed));
+ 	val = jhash(dst, sizeof(*dst), seed);
+ 
+ #ifdef CONFIG_IPV6_SUBTREES
+ 	if (src)
+ 		val = jhash(src, sizeof(*src), val);
+ #endif
+ 	return hash_32(val, FIB6_EXCEPTION_BUCKET_SIZE_SHIFT);
+ }
+ 
+ /* Helper function to find the cached rt in the hash table
+  * and update bucket pointer to point to the bucket for this
+  * (daddr, saddr) pair
+  * Caller must hold rt6_exception_lock
+  */
+ static struct rt6_exception *
+ __rt6_find_exception_spinlock(struct rt6_exception_bucket **bucket,
+ 			      const struct in6_addr *daddr,
+ 			      const struct in6_addr *saddr)
+ {
+ 	struct rt6_exception *rt6_ex;
+ 	u32 hval;
+ 
+ 	if (!(*bucket) || !daddr)
+ 		return NULL;
+ 
+ 	hval = rt6_exception_hash(daddr, saddr);
+ 	*bucket += hval;
+ 
+ 	hlist_for_each_entry(rt6_ex, &(*bucket)->chain, hlist) {
+ 		struct rt6_info *rt6 = rt6_ex->rt6i;
+ 		bool matched = ipv6_addr_equal(daddr, &rt6->rt6i_dst.addr);
+ 
+ #ifdef CONFIG_IPV6_SUBTREES
+ 		if (matched && saddr)
+ 			matched = ipv6_addr_equal(saddr, &rt6->rt6i_src.addr);
+ #endif
+ 		if (matched)
+ 			return rt6_ex;
+ 	}
+ 	return NULL;
+ }
+ 
+ /* Helper function to find the cached rt in the hash table
+  * and update bucket pointer to point to the bucket for this
+  * (daddr, saddr) pair
+  * Caller must hold rcu_read_lock()
+  */
+ static struct rt6_exception *
+ __rt6_find_exception_rcu(struct rt6_exception_bucket **bucket,
+ 			 const struct in6_addr *daddr,
+ 			 const struct in6_addr *saddr)
+ {
+ 	struct rt6_exception *rt6_ex;
+ 	u32 hval;
+ 
+ 	WARN_ON_ONCE(!rcu_read_lock_held());
+ 
+ 	if (!(*bucket) || !daddr)
+ 		return NULL;
+ 
+ 	hval = rt6_exception_hash(daddr, saddr);
+ 	*bucket += hval;
+ 
+ 	hlist_for_each_entry_rcu(rt6_ex, &(*bucket)->chain, hlist) {
+ 		struct rt6_info *rt6 = rt6_ex->rt6i;
+ 		bool matched = ipv6_addr_equal(daddr, &rt6->rt6i_dst.addr);
+ 
+ #ifdef CONFIG_IPV6_SUBTREES
+ 		if (matched && saddr)
+ 			matched = ipv6_addr_equal(saddr, &rt6->rt6i_src.addr);
+ #endif
+ 		if (matched)
+ 			return rt6_ex;
+ 	}
+ 	return NULL;
+ }
+ 
+ static int rt6_insert_exception(struct rt6_info *nrt,
+ 				struct rt6_info *ort)
+ {
+ 	struct net *net = dev_net(ort->dst.dev);
+ 	struct rt6_exception_bucket *bucket;
+ 	struct in6_addr *src_key = NULL;
+ 	struct rt6_exception *rt6_ex;
+ 	int err = 0;
+ 
+ 	/* ort can't be a cache or pcpu route */
+ 	if (ort->rt6i_flags & (RTF_CACHE | RTF_PCPU))
+ 		ort = ort->from;
+ 	WARN_ON_ONCE(ort->rt6i_flags & (RTF_CACHE | RTF_PCPU));
+ 
+ 	spin_lock_bh(&rt6_exception_lock);
+ 
+ 	if (ort->exception_bucket_flushed) {
+ 		err = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	bucket = rcu_dereference_protected(ort->rt6i_exception_bucket,
+ 					lockdep_is_held(&rt6_exception_lock));
+ 	if (!bucket) {
+ 		bucket = kcalloc(FIB6_EXCEPTION_BUCKET_SIZE, sizeof(*bucket),
+ 				 GFP_ATOMIC);
+ 		if (!bucket) {
+ 			err = -ENOMEM;
+ 			goto out;
+ 		}
+ 		rcu_assign_pointer(ort->rt6i_exception_bucket, bucket);
+ 	}
+ 
+ #ifdef CONFIG_IPV6_SUBTREES
+ 	/* rt6i_src.plen != 0 indicates ort is in subtree
+ 	 * and exception table is indexed by a hash of
+ 	 * both rt6i_dst and rt6i_src.
+ 	 * Otherwise, the exception table is indexed by
+ 	 * a hash of only rt6i_dst.
+ 	 */
+ 	if (ort->rt6i_src.plen)
+ 		src_key = &nrt->rt6i_src.addr;
+ #endif
+ 
+ 	/* Update rt6i_prefsrc as it could be changed
+ 	 * in rt6_remove_prefsrc()
+ 	 */
+ 	nrt->rt6i_prefsrc = ort->rt6i_prefsrc;
+ 	/* rt6_mtu_change() might lower mtu on ort.
+ 	 * Only insert this exception route if its mtu
+ 	 * is less than ort's mtu value.
+ 	 */
+ 	if (nrt->rt6i_pmtu >= dst_mtu(&ort->dst)) {
+ 		err = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	rt6_ex = __rt6_find_exception_spinlock(&bucket, &nrt->rt6i_dst.addr,
+ 					       src_key);
+ 	if (rt6_ex)
+ 		rt6_remove_exception(bucket, rt6_ex);
+ 
+ 	rt6_ex = kzalloc(sizeof(*rt6_ex), GFP_ATOMIC);
+ 	if (!rt6_ex) {
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 	rt6_ex->rt6i = nrt;
+ 	rt6_ex->stamp = jiffies;
+ 	atomic_inc(&nrt->rt6i_ref);
+ 	nrt->rt6i_node = ort->rt6i_node;
+ 	hlist_add_head_rcu(&rt6_ex->hlist, &bucket->chain);
+ 	bucket->depth++;
+ 	net->ipv6.rt6_stats->fib_rt_cache++;
+ 
+ 	if (bucket->depth > FIB6_MAX_DEPTH)
+ 		rt6_exception_remove_oldest(bucket);
+ 
+ out:
+ 	spin_unlock_bh(&rt6_exception_lock);
+ 
+ 	/* Update fn->fn_sernum to invalidate all cached dst */
+ 	if (!err) {
+ 		spin_lock_bh(&ort->rt6i_table->tb6_lock);
+ 		fib6_update_sernum(ort);
+ 		spin_unlock_bh(&ort->rt6i_table->tb6_lock);
+ 		fib6_force_start_gc(net);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ void rt6_flush_exceptions(struct rt6_info *rt)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct rt6_exception *rt6_ex;
+ 	struct hlist_node *tmp;
+ 	int i;
+ 
+ 	spin_lock_bh(&rt6_exception_lock);
+ 	/* Prevent rt6_insert_exception() to recreate the bucket list */
+ 	rt->exception_bucket_flushed = 1;
+ 
+ 	bucket = rcu_dereference_protected(rt->rt6i_exception_bucket,
+ 				    lockdep_is_held(&rt6_exception_lock));
+ 	if (!bucket)
+ 		goto out;
+ 
+ 	for (i = 0; i < FIB6_EXCEPTION_BUCKET_SIZE; i++) {
+ 		hlist_for_each_entry_safe(rt6_ex, tmp, &bucket->chain, hlist)
+ 			rt6_remove_exception(bucket, rt6_ex);
+ 		WARN_ON_ONCE(bucket->depth);
+ 		bucket++;
+ 	}
+ 
+ out:
+ 	spin_unlock_bh(&rt6_exception_lock);
+ }
+ 
+ /* Find cached rt in the hash table inside passed in rt
+  * Caller has to hold rcu_read_lock()
+  */
+ static struct rt6_info *rt6_find_cached_rt(struct rt6_info *rt,
+ 					   struct in6_addr *daddr,
+ 					   struct in6_addr *saddr)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct in6_addr *src_key = NULL;
+ 	struct rt6_exception *rt6_ex;
+ 	struct rt6_info *res = NULL;
+ 
+ 	bucket = rcu_dereference(rt->rt6i_exception_bucket);
+ 
+ #ifdef CONFIG_IPV6_SUBTREES
+ 	/* rt6i_src.plen != 0 indicates rt is in subtree
+ 	 * and exception table is indexed by a hash of
+ 	 * both rt6i_dst and rt6i_src.
+ 	 * Otherwise, the exception table is indexed by
+ 	 * a hash of only rt6i_dst.
+ 	 */
+ 	if (rt->rt6i_src.plen)
+ 		src_key = saddr;
+ #endif
+ 	rt6_ex = __rt6_find_exception_rcu(&bucket, daddr, src_key);
+ 
+ 	if (rt6_ex && !rt6_check_expired(rt6_ex->rt6i))
+ 		res = rt6_ex->rt6i;
+ 
+ 	return res;
+ }
+ 
+ /* Remove the passed in cached rt from the hash table that contains it */
+ int rt6_remove_exception_rt(struct rt6_info *rt)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct rt6_info *from = rt->from;
+ 	struct in6_addr *src_key = NULL;
+ 	struct rt6_exception *rt6_ex;
+ 	int err;
+ 
+ 	if (!from ||
+ 	    !(rt->rt6i_flags & RTF_CACHE))
+ 		return -EINVAL;
+ 
+ 	if (!rcu_access_pointer(from->rt6i_exception_bucket))
+ 		return -ENOENT;
+ 
+ 	spin_lock_bh(&rt6_exception_lock);
+ 	bucket = rcu_dereference_protected(from->rt6i_exception_bucket,
+ 				    lockdep_is_held(&rt6_exception_lock));
+ #ifdef CONFIG_IPV6_SUBTREES
+ 	/* rt6i_src.plen != 0 indicates 'from' is in subtree
+ 	 * and exception table is indexed by a hash of
+ 	 * both rt6i_dst and rt6i_src.
+ 	 * Otherwise, the exception table is indexed by
+ 	 * a hash of only rt6i_dst.
+ 	 */
+ 	if (from->rt6i_src.plen)
+ 		src_key = &rt->rt6i_src.addr;
+ #endif
+ 	rt6_ex = __rt6_find_exception_spinlock(&bucket,
+ 					       &rt->rt6i_dst.addr,
+ 					       src_key);
+ 	if (rt6_ex) {
+ 		rt6_remove_exception(bucket, rt6_ex);
+ 		err = 0;
+ 	} else {
+ 		err = -ENOENT;
+ 	}
+ 
+ 	spin_unlock_bh(&rt6_exception_lock);
+ 	return err;
+ }
+ 
+ /* Find rt6_ex which contains the passed in rt cache and
+  * refresh its stamp
+  */
+ static void rt6_update_exception_stamp_rt(struct rt6_info *rt)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct rt6_info *from = rt->from;
+ 	struct in6_addr *src_key = NULL;
+ 	struct rt6_exception *rt6_ex;
+ 
+ 	if (!from ||
+ 	    !(rt->rt6i_flags & RTF_CACHE))
+ 		return;
+ 
+ 	rcu_read_lock();
+ 	bucket = rcu_dereference(from->rt6i_exception_bucket);
+ 
+ #ifdef CONFIG_IPV6_SUBTREES
+ 	/* rt6i_src.plen != 0 indicates 'from' is in subtree
+ 	 * and exception table is indexed by a hash of
+ 	 * both rt6i_dst and rt6i_src.
+ 	 * Otherwise, the exception table is indexed by
+ 	 * a hash of only rt6i_dst.
+ 	 */
+ 	if (from->rt6i_src.plen)
+ 		src_key = &rt->rt6i_src.addr;
+ #endif
+ 	rt6_ex = __rt6_find_exception_rcu(&bucket,
+ 					  &rt->rt6i_dst.addr,
+ 					  src_key);
+ 	if (rt6_ex)
+ 		rt6_ex->stamp = jiffies;
+ 
+ 	rcu_read_unlock();
+ }
+ 
+ static void rt6_exceptions_remove_prefsrc(struct rt6_info *rt)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct rt6_exception *rt6_ex;
+ 	int i;
+ 
+ 	bucket = rcu_dereference_protected(rt->rt6i_exception_bucket,
+ 					lockdep_is_held(&rt6_exception_lock));
+ 
+ 	if (bucket) {
+ 		for (i = 0; i < FIB6_EXCEPTION_BUCKET_SIZE; i++) {
+ 			hlist_for_each_entry(rt6_ex, &bucket->chain, hlist) {
+ 				rt6_ex->rt6i->rt6i_prefsrc.plen = 0;
+ 			}
+ 			bucket++;
+ 		}
+ 	}
+ }
+ 
+ static bool rt6_mtu_change_route_allowed(struct inet6_dev *idev,
+ 					 struct rt6_info *rt, int mtu)
+ {
+ 	/* If the new MTU is lower than the route PMTU, this new MTU will be the
+ 	 * lowest MTU in the path: always allow updating the route PMTU to
+ 	 * reflect PMTU decreases.
+ 	 *
+ 	 * If the new MTU is higher, and the route PMTU is equal to the local
+ 	 * MTU, this means the old MTU is the lowest in the path, so allow
+ 	 * updating it: if other nodes now have lower MTUs, PMTU discovery will
+ 	 * handle this.
+ 	 */
+ 
+ 	if (dst_mtu(&rt->dst) >= mtu)
+ 		return true;
+ 
+ 	if (dst_mtu(&rt->dst) == idev->cnf.mtu6)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static void rt6_exceptions_update_pmtu(struct inet6_dev *idev,
+ 				       struct rt6_info *rt, int mtu)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct rt6_exception *rt6_ex;
+ 	int i;
+ 
+ 	bucket = rcu_dereference_protected(rt->rt6i_exception_bucket,
+ 					lockdep_is_held(&rt6_exception_lock));
+ 
+ 	if (!bucket)
+ 		return;
+ 
+ 	for (i = 0; i < FIB6_EXCEPTION_BUCKET_SIZE; i++) {
+ 		hlist_for_each_entry(rt6_ex, &bucket->chain, hlist) {
+ 			struct rt6_info *entry = rt6_ex->rt6i;
+ 
+ 			/* For RTF_CACHE with rt6i_pmtu == 0 (i.e. a redirected
+ 			 * route), the metrics of its rt->dst.from have already
+ 			 * been updated.
+ 			 */
+ 			if (entry->rt6i_pmtu &&
+ 			    rt6_mtu_change_route_allowed(idev, entry, mtu))
+ 				entry->rt6i_pmtu = mtu;
+ 		}
+ 		bucket++;
+ 	}
+ }
+ 
+ #define RTF_CACHE_GATEWAY	(RTF_GATEWAY | RTF_CACHE)
+ 
+ static void rt6_exceptions_clean_tohost(struct rt6_info *rt,
+ 					struct in6_addr *gateway)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct rt6_exception *rt6_ex;
+ 	struct hlist_node *tmp;
+ 	int i;
+ 
+ 	if (!rcu_access_pointer(rt->rt6i_exception_bucket))
+ 		return;
+ 
+ 	spin_lock_bh(&rt6_exception_lock);
+ 	bucket = rcu_dereference_protected(rt->rt6i_exception_bucket,
+ 				     lockdep_is_held(&rt6_exception_lock));
+ 
+ 	if (bucket) {
+ 		for (i = 0; i < FIB6_EXCEPTION_BUCKET_SIZE; i++) {
+ 			hlist_for_each_entry_safe(rt6_ex, tmp,
+ 						  &bucket->chain, hlist) {
+ 				struct rt6_info *entry = rt6_ex->rt6i;
+ 
+ 				if ((entry->rt6i_flags & RTF_CACHE_GATEWAY) ==
+ 				    RTF_CACHE_GATEWAY &&
+ 				    ipv6_addr_equal(gateway,
+ 						    &entry->rt6i_gateway)) {
+ 					rt6_remove_exception(bucket, rt6_ex);
+ 				}
+ 			}
+ 			bucket++;
+ 		}
+ 	}
+ 
+ 	spin_unlock_bh(&rt6_exception_lock);
+ }
+ 
+ static void rt6_age_examine_exception(struct rt6_exception_bucket *bucket,
+ 				      struct rt6_exception *rt6_ex,
+ 				      struct fib6_gc_args *gc_args,
+ 				      unsigned long now)
+ {
+ 	struct rt6_info *rt = rt6_ex->rt6i;
+ 
+ 	/* we are pruning and obsoleting aged-out and non gateway exceptions
+ 	 * even if others have still references to them, so that on next
+ 	 * dst_check() such references can be dropped.
+ 	 * EXPIRES exceptions - e.g. pmtu-generated ones are pruned when
+ 	 * expired, independently from their aging, as per RFC 8201 section 4
+ 	 */
+ 	if (!(rt->rt6i_flags & RTF_EXPIRES)) {
+ 		if (time_after_eq(now, rt->dst.lastuse + gc_args->timeout)) {
+ 			RT6_TRACE("aging clone %p\n", rt);
+ 			rt6_remove_exception(bucket, rt6_ex);
+ 			return;
+ 		}
+ 	} else if (time_after(jiffies, rt->dst.expires)) {
+ 		RT6_TRACE("purging expired route %p\n", rt);
+ 		rt6_remove_exception(bucket, rt6_ex);
+ 		return;
+ 	}
+ 
+ 	if (rt->rt6i_flags & RTF_GATEWAY) {
+ 		struct neighbour *neigh;
+ 		__u8 neigh_flags = 0;
+ 
+ 		neigh = dst_neigh_lookup(&rt->dst, &rt->rt6i_gateway);
+ 		if (neigh) {
+ 			neigh_flags = neigh->flags;
+ 			neigh_release(neigh);
+ 		}
+ 		if (!(neigh_flags & NTF_ROUTER)) {
+ 			RT6_TRACE("purging route %p via non-router but gateway\n",
+ 				  rt);
+ 			rt6_remove_exception(bucket, rt6_ex);
+ 			return;
+ 		}
+ 	}
+ 
+ 	gc_args->more++;
+ }
+ 
+ void rt6_age_exceptions(struct rt6_info *rt,
+ 			struct fib6_gc_args *gc_args,
+ 			unsigned long now)
+ {
+ 	struct rt6_exception_bucket *bucket;
+ 	struct rt6_exception *rt6_ex;
+ 	struct hlist_node *tmp;
+ 	int i;
+ 
+ 	if (!rcu_access_pointer(rt->rt6i_exception_bucket))
+ 		return;
+ 
+ 	spin_lock_bh(&rt6_exception_lock);
+ 	bucket = rcu_dereference_protected(rt->rt6i_exception_bucket,
+ 				    lockdep_is_held(&rt6_exception_lock));
+ 
+ 	if (bucket) {
+ 		for (i = 0; i < FIB6_EXCEPTION_BUCKET_SIZE; i++) {
+ 			hlist_for_each_entry_safe(rt6_ex, tmp,
+ 						  &bucket->chain, hlist) {
+ 				rt6_age_examine_exception(bucket, rt6_ex,
+ 							  gc_args, now);
+ 			}
+ 			bucket++;
+ 		}
+ 	}
+ 	spin_unlock_bh(&rt6_exception_lock);
+ }
+ 
+ struct rt6_info *ip6_pol_route(struct net *net, struct fib6_table *table,
+ 			       int oif, struct flowi6 *fl6, int flags)
++>>>>>>> e9fa1495d738 (ipv6: Reflect MTU changes on PMTU of exceptions for MTU-less routes)
  {
  	struct fib6_node *fn, *saved_fn;
 -	struct rt6_info *rt, *rt_cache;
 +	struct rt6_info *rt;
  	int strict = 0;
  
  	strict |= flags & RT6_LOOKUP_F_IFACE;
@@@ -2701,30 -3834,14 +3243,41 @@@ static int rt6_mtu_change_route(struct 
  	   Since RFC 1981 doesn't include administrative MTU increase
  	   update PMTU increase is a MUST. (i.e. jumbo frame)
  	 */
++<<<<<<< HEAD
 +	/*
 +	   If new MTU is less than route PMTU, this new MTU will be the
 +	   lowest MTU in the path, update the route PMTU to reflect PMTU
 +	   decreases; if new MTU is greater than route PMTU, and the
 +	   old MTU is the lowest MTU in the path, update the route PMTU
 +	   to reflect the increase. In this case if the other nodes' MTU
 +	   also have the lowest MTU, TOO BIG MESSAGE will be lead to
 +	   PMTU discouvery.
 +	 */
 +	if (rt->dst.dev == arg->dev &&
 +	    !dst_metric_locked(&rt->dst, RTAX_MTU)) {
 +		if (rt->rt6i_flags & RTF_CACHE) {
 +			/* For RTF_CACHE with rt6i_pmtu == 0
 +			 * (i.e. a redirected route),
 +			 * the metrics of its rt->dst.from has already
 +			 * been updated.
 +			 */
 +			if (rt->rt6i_pmtu && rt->rt6i_pmtu > arg->mtu)
 +				rt->rt6i_pmtu = arg->mtu;
 +		} else if (dst_mtu(&rt->dst) >= arg->mtu ||
 +			   (dst_mtu(&rt->dst) < arg->mtu &&
 +			    dst_mtu(&rt->dst) == idev->cnf.mtu6)) {
 +			dst_metric_set(&rt->dst, RTAX_MTU, arg->mtu);
 +		}
++=======
+ 	if (rt->dst.dev == arg->dev &&
+ 	    !dst_metric_locked(&rt->dst, RTAX_MTU)) {
+ 		spin_lock_bh(&rt6_exception_lock);
+ 		if (dst_metric_raw(&rt->dst, RTAX_MTU) &&
+ 		    rt6_mtu_change_route_allowed(idev, rt, arg->mtu))
+ 			dst_metric_set(&rt->dst, RTAX_MTU, arg->mtu);
+ 		rt6_exceptions_update_pmtu(idev, rt, arg->mtu);
+ 		spin_unlock_bh(&rt6_exception_lock);
++>>>>>>> e9fa1495d738 (ipv6: Reflect MTU changes on PMTU of exceptions for MTU-less routes)
  	}
  	return 0;
  }
* Unmerged path net/ipv6/route.c
