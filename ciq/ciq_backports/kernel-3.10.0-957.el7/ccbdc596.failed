nfp: bpf: don't allow changing MTU above BPF offload limit when active

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit ccbdc596f4f6f6795956d46bb4b5f58c7e4bc3c8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/ccbdc596.failed

When BPF offload is active we need may need to restrict the MTU
changes more than just to the limitation of the kernel XDP datapath.
Allow the BPF code to veto a MTU change.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit ccbdc596f4f6f6795956d46bb4b5f58c7e4bc3c8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/main.c
#	drivers/net/ethernet/netronome/nfp/nfp_app.h
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.c
index 4ca14182b178,978086580ca0..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.c
@@@ -141,10 -194,143 +141,149 @@@ static bool nfp_bpf_tc_busy(struct nfp_
  	return nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ nfp_bpf_change_mtu(struct nfp_app *app, struct net_device *netdev, int new_mtu)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 	unsigned int max_mtu;
+ 
+ 	if (~nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
+ 		return 0;
+ 
+ 	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
+ 	if (new_mtu > max_mtu) {
+ 		nn_info(nn, "BPF offload active, MTU over %u not supported\n",
+ 			max_mtu);
+ 		return -EBUSY;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_parse_cap_adjust_head(struct nfp_app_bpf *bpf, void __iomem *value,
+ 			      u32 length)
+ {
+ 	struct nfp_bpf_cap_tlv_adjust_head __iomem *cap = value;
+ 	struct nfp_cpp *cpp = bpf->app->pf->cpp;
+ 
+ 	if (length < sizeof(*cap)) {
+ 		nfp_err(cpp, "truncated adjust_head TLV: %d\n", length);
+ 		return -EINVAL;
+ 	}
+ 
+ 	bpf->adjust_head.flags = readl(&cap->flags);
+ 	bpf->adjust_head.off_min = readl(&cap->off_min);
+ 	bpf->adjust_head.off_max = readl(&cap->off_max);
+ 	bpf->adjust_head.guaranteed_sub = readl(&cap->guaranteed_sub);
+ 	bpf->adjust_head.guaranteed_add = readl(&cap->guaranteed_add);
+ 
+ 	if (bpf->adjust_head.off_min > bpf->adjust_head.off_max) {
+ 		nfp_err(cpp, "invalid adjust_head TLV: min > max\n");
+ 		return -EINVAL;
+ 	}
+ 	if (!FIELD_FIT(UR_REG_IMM_MAX, bpf->adjust_head.off_min) ||
+ 	    !FIELD_FIT(UR_REG_IMM_MAX, bpf->adjust_head.off_max)) {
+ 		nfp_warn(cpp, "disabling adjust_head - driver expects min/max to fit in as immediates\n");
+ 		memset(&bpf->adjust_head, 0, sizeof(bpf->adjust_head));
+ 		return 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_bpf_parse_capabilities(struct nfp_app *app)
+ {
+ 	struct nfp_cpp *cpp = app->pf->cpp;
+ 	struct nfp_cpp_area *area;
+ 	u8 __iomem *mem, *start;
+ 
+ 	mem = nfp_rtsym_map(app->pf->rtbl, "_abi_bpf_capabilities", "bpf.cap",
+ 			    8, &area);
+ 	if (IS_ERR(mem))
+ 		return PTR_ERR(mem) == -ENOENT ? 0 : PTR_ERR(mem);
+ 
+ 	start = mem;
+ 	while (mem - start + 8 < nfp_cpp_area_size(area)) {
+ 		u8 __iomem *value;
+ 		u32 type, length;
+ 
+ 		type = readl(mem);
+ 		length = readl(mem + 4);
+ 		value = mem + 8;
+ 
+ 		mem += 8 + length;
+ 		if (mem - start > nfp_cpp_area_size(area))
+ 			goto err_release_free;
+ 
+ 		switch (type) {
+ 		case NFP_BPF_CAP_TYPE_ADJUST_HEAD:
+ 			if (nfp_bpf_parse_cap_adjust_head(app->priv, value,
+ 							  length))
+ 				goto err_release_free;
+ 			break;
+ 		default:
+ 			nfp_dbg(cpp, "unknown BPF capability: %d\n", type);
+ 			break;
+ 		}
+ 	}
+ 	if (mem - start != nfp_cpp_area_size(area)) {
+ 		nfp_err(cpp, "BPF capabilities left after parsing, parsed:%zd total length:%zu\n",
+ 			mem - start, nfp_cpp_area_size(area));
+ 		goto err_release_free;
+ 	}
+ 
+ 	nfp_cpp_area_release_free(area);
+ 
+ 	return 0;
+ 
+ err_release_free:
+ 	nfp_err(cpp, "invalid BPF capabilities at offset:%zd\n", mem - start);
+ 	nfp_cpp_area_release_free(area);
+ 	return -EINVAL;
+ }
+ 
+ static int nfp_bpf_init(struct nfp_app *app)
+ {
+ 	struct nfp_app_bpf *bpf;
+ 	int err;
+ 
+ 	bpf = kzalloc(sizeof(*bpf), GFP_KERNEL);
+ 	if (!bpf)
+ 		return -ENOMEM;
+ 	bpf->app = app;
+ 	app->priv = bpf;
+ 
+ 	err = nfp_bpf_parse_capabilities(app);
+ 	if (err)
+ 		goto err_free_bpf;
+ 
+ 	return 0;
+ 
+ err_free_bpf:
+ 	kfree(bpf);
+ 	return err;
+ }
+ 
+ static void nfp_bpf_clean(struct nfp_app *app)
+ {
+ 	kfree(app->priv);
+ }
+ 
++>>>>>>> ccbdc596f4f6 (nfp: bpf: don't allow changing MTU above BPF offload limit when active)
  const struct nfp_app_type app_bpf = {
  	.id		= NFP_APP_BPF_NIC,
  	.name		= "ebpf",
  
++<<<<<<< HEAD
++=======
+ 	.init		= nfp_bpf_init,
+ 	.clean		= nfp_bpf_clean,
+ 
+ 	.change_mtu	= nfp_bpf_change_mtu,
+ 
++>>>>>>> ccbdc596f4f6 (nfp: bpf: don't allow changing MTU above BPF offload limit when active)
  	.extra_cap	= nfp_bpf_extra_cap,
  
  	.vnic_alloc	= nfp_bpf_vnic_alloc,
diff --cc drivers/net/ethernet/netronome/nfp/nfp_app.h
index b6035aad75b0,e6b59c28c4ca..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_app.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_app.h
@@@ -76,8 -76,14 +76,10 @@@ extern const struct nfp_app_type app_fl
   * @vnic_free:	free up app's vNIC state
   * @vnic_init:	vNIC netdev was registered
   * @vnic_clean:	vNIC netdev about to be unregistered
 - * @repr_init:	representor about to be registered
 - * @repr_preclean:	representor about to unregistered, executed before app
 - *			reference to the it is removed
 - * @repr_clean:	representor about to be unregistered
   * @repr_open:	representor netdev open callback
   * @repr_stop:	representor netdev stop callback
+  * @change_mtu:	MTU change on a netdev has been requested (veto-only, change
+  *		is not guaranteed to be committed)
   * @start:	start application logic
   * @stop:	stop application logic
   * @ctrl_msg_rx:    control message handler
@@@ -203,6 -225,36 +208,39 @@@ static inline int nfp_app_repr_stop(str
  	return app->type->repr_stop(app, repr);
  }
  
++<<<<<<< HEAD
++=======
+ static inline int
+ nfp_app_repr_init(struct nfp_app *app, struct net_device *netdev)
+ {
+ 	if (!app->type->repr_init)
+ 		return 0;
+ 	return app->type->repr_init(app, netdev);
+ }
+ 
+ static inline void
+ nfp_app_repr_preclean(struct nfp_app *app, struct net_device *netdev)
+ {
+ 	if (app->type->repr_preclean)
+ 		app->type->repr_preclean(app, netdev);
+ }
+ 
+ static inline void
+ nfp_app_repr_clean(struct nfp_app *app, struct net_device *netdev)
+ {
+ 	if (app->type->repr_clean)
+ 		app->type->repr_clean(app, netdev);
+ }
+ 
+ static inline int
+ nfp_app_change_mtu(struct nfp_app *app, struct net_device *netdev, int new_mtu)
+ {
+ 	if (!app || !app->type->change_mtu)
+ 		return 0;
+ 	return app->type->change_mtu(app, netdev, new_mtu);
+ }
+ 
++>>>>>>> ccbdc596f4f6 (nfp: bpf: don't allow changing MTU above BPF offload limit when active)
  static inline int nfp_app_start(struct nfp_app *app, struct nfp_net *ctrl)
  {
  	app->ctrl = ctrl;
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.c
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_app.h
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 51e5c398d8b2..25d6ffa0b6f5 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@ -3060,6 +3060,11 @@ static int nfp_net_change_mtu(struct net_device *netdev, int new_mtu)
 {
 	struct nfp_net *nn = netdev_priv(netdev);
 	struct nfp_net_dp *dp;
+	int err;
+
+	err = nfp_app_change_mtu(nn->app, netdev, new_mtu);
+	if (err)
+		return err;
 
 	dp = nfp_net_clone_dp(nn);
 	if (!dp)
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net_repr.c b/drivers/net/ethernet/netronome/nfp/nfp_net_repr.c
index 90bc09b8021b..89bde6502a64 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_repr.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_repr.c
@@ -185,6 +185,13 @@ nfp_repr_get_offload_stats(int attr_id, const struct net_device *dev,
 	return -EINVAL;
 }
 
+static int nfp_repr_change_mtu(struct net_device *netdev, int new_mtu)
+{
+	struct nfp_repr *repr = netdev_priv(netdev);
+
+	return nfp_app_change_mtu(repr->app, netdev, new_mtu);
+}
+
 static netdev_tx_t nfp_repr_xmit(struct sk_buff *skb, struct net_device *netdev)
 {
 	struct nfp_repr *repr = netdev_priv(netdev);
@@ -240,6 +247,7 @@ const struct net_device_ops nfp_repr_netdev_ops = {
 	.ndo_open		= nfp_repr_open,
 	.ndo_stop		= nfp_repr_stop,
 	.ndo_start_xmit		= nfp_repr_xmit,
+	.ndo_change_mtu		= nfp_repr_change_mtu,
 	.ndo_get_stats64	= nfp_repr_get_stats64,
 	.extended.ndo_has_offload_stats	= nfp_repr_has_offload_stats,
 	.extended.ndo_get_offload_stats	= nfp_repr_get_offload_stats,
