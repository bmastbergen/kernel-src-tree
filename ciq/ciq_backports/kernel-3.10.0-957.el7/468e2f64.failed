bpf: introduce BPF_PROG_QUERY command

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Alexei Starovoitov <ast@fb.com>
commit 468e2f64d220fe2dc11caa2bcb9b3a1e50fc7321
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/468e2f64.failed

introduce BPF_PROG_QUERY command to retrieve a set of either
attached programs to given cgroup or a set of effective programs
that will execute for events within a cgroup

	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Martin KaFai Lau <kafai@fb.com>
for cgroup bits
	Acked-by: Tejun Heo <tj@kernel.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 468e2f64d220fe2dc11caa2bcb9b3a1e50fc7321)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf-cgroup.h
#	include/linux/bpf.h
#	include/uapi/linux/bpf.h
#	kernel/bpf/cgroup.c
#	kernel/bpf/core.c
#	kernel/bpf/syscall.c
#	kernel/cgroup/cgroup.c
diff --cc include/linux/bpf.h
index d4c1f9049ad3,a67daea731ab..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -89,28 -154,311 +89,296 @@@ struct bpf_func_proto 
  struct bpf_verifier_ops {
  	/* return eBPF function prototype for verification */
  	const struct bpf_func_proto *(*get_func_proto)(enum bpf_func_id func_id);
 +};
  
 -	/* return true if 'size' wide access at offset 'off' within bpf_context
 -	 * with 'type' (read or write) is allowed
 -	 */
 -	bool (*is_valid_access)(int off, int size, enum bpf_access_type type,
 -				struct bpf_insn_access_aux *info);
 -	int (*gen_prologue)(struct bpf_insn *insn, bool direct_write,
 -			    const struct bpf_prog *prog);
 -	u32 (*convert_ctx_access)(enum bpf_access_type type,
 -				  const struct bpf_insn *src,
 -				  struct bpf_insn *dst,
 -				  struct bpf_prog *prog, u32 *target_size);
 -	int (*test_run)(struct bpf_prog *prog, const union bpf_attr *kattr,
 -			union bpf_attr __user *uattr);
 +struct bpf_prog_type_list {
 +	struct list_head list_node;
 +	struct bpf_verifier_ops *ops;
 +	enum bpf_prog_type type;
  };
  
 +void bpf_register_prog_type(struct bpf_prog_type_list *tl);
 +
 +struct bpf_prog;
 +
  struct bpf_prog_aux {
  	atomic_t refcnt;
 -	u32 used_map_cnt;
 -	u32 max_ctx_offset;
 -	u32 stack_depth;
 +	bool is_gpl_compatible;
 +	enum bpf_prog_type prog_type;
 +	struct bpf_verifier_ops *ops;
  	u32 id;
 -	struct latch_tree_node ksym_tnode;
 -	struct list_head ksym_lnode;
 -	const struct bpf_verifier_ops *ops;
  	struct bpf_map **used_maps;
 +	u32 used_map_cnt;
  	struct bpf_prog *prog;
 -	struct user_struct *user;
 -	u64 load_time; /* ns since boottime */
 -	u8 name[BPF_OBJ_NAME_LEN];
 -	union {
 -		struct work_struct work;
 -		struct rcu_head	rcu;
 -	};
 +	struct work_struct work;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_array {
+ 	struct bpf_map map;
+ 	u32 elem_size;
+ 	/* 'ownership' of prog_array is claimed by the first program that
+ 	 * is going to use this map or by the first program which FD is stored
+ 	 * in the map to make sure that all callers and callees have the same
+ 	 * prog_type and JITed flag
+ 	 */
+ 	enum bpf_prog_type owner_prog_type;
+ 	bool owner_jited;
+ 	union {
+ 		char value[0] __aligned(8);
+ 		void *ptrs[0] __aligned(8);
+ 		void __percpu *pptrs[0] __aligned(8);
+ 	};
+ };
+ 
+ #define MAX_TAIL_CALL_CNT 32
+ 
+ struct bpf_event_entry {
+ 	struct perf_event *event;
+ 	struct file *perf_file;
+ 	struct file *map_file;
+ 	struct rcu_head rcu;
+ };
+ 
+ u64 bpf_tail_call(u64 ctx, u64 r2, u64 index, u64 r4, u64 r5);
+ u64 bpf_get_stackid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
+ bool bpf_prog_array_compatible(struct bpf_array *array, const struct bpf_prog *fp);
+ int bpf_prog_calc_tag(struct bpf_prog *fp);
+ 
+ const struct bpf_func_proto *bpf_get_trace_printk_proto(void);
+ 
+ typedef unsigned long (*bpf_ctx_copy_t)(void *dst, const void *src,
+ 					unsigned long off, unsigned long len);
+ 
+ u64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,
+ 		     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy);
+ 
+ int bpf_prog_test_run_xdp(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ int bpf_prog_test_run_skb(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ 
+ /* an array of programs to be executed under rcu_lock.
+  *
+  * Typical usage:
+  * ret = BPF_PROG_RUN_ARRAY(&bpf_prog_array, ctx, BPF_PROG_RUN);
+  *
+  * the structure returned by bpf_prog_array_alloc() should be populated
+  * with program pointers and the last pointer must be NULL.
+  * The user has to keep refcnt on the program and make sure the program
+  * is removed from the array before bpf_prog_put().
+  * The 'struct bpf_prog_array *' should only be replaced with xchg()
+  * since other cpus are walking the array of pointers in parallel.
+  */
+ struct bpf_prog_array {
+ 	struct rcu_head rcu;
+ 	struct bpf_prog *progs[0];
+ };
+ 
+ struct bpf_prog_array __rcu *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags);
+ void bpf_prog_array_free(struct bpf_prog_array __rcu *progs);
+ int bpf_prog_array_length(struct bpf_prog_array __rcu *progs);
+ int bpf_prog_array_copy_to_user(struct bpf_prog_array __rcu *progs,
+ 				__u32 __user *prog_ids, u32 cnt);
+ 
+ #define BPF_PROG_RUN_ARRAY(array, ctx, func)		\
+ 	({						\
+ 		struct bpf_prog **_prog;		\
+ 		u32 _ret = 1;				\
+ 		rcu_read_lock();			\
+ 		_prog = rcu_dereference(array)->progs;	\
+ 		for (; *_prog; _prog++)			\
+ 			_ret &= func(*_prog, ctx);	\
+ 		rcu_read_unlock();			\
+ 		_ret;					\
+ 	 })
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ DECLARE_PER_CPU(int, bpf_prog_active);
+ 
+ #define BPF_PROG_TYPE(_id, _ops) \
+ 	extern const struct bpf_verifier_ops _ops;
+ #define BPF_MAP_TYPE(_id, _ops) \
+ 	extern const struct bpf_map_ops _ops;
+ #include <linux/bpf_types.h>
+ #undef BPF_PROG_TYPE
+ #undef BPF_MAP_TYPE
+ 
+ struct bpf_prog *bpf_prog_get(u32 ufd);
+ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type);
+ struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog, int i);
+ void bpf_prog_sub(struct bpf_prog *prog, int i);
+ struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog);
+ struct bpf_prog * __must_check bpf_prog_inc_not_zero(struct bpf_prog *prog);
+ void bpf_prog_put(struct bpf_prog *prog);
+ int __bpf_prog_charge(struct user_struct *user, u32 pages);
+ void __bpf_prog_uncharge(struct user_struct *user, u32 pages);
+ 
+ struct bpf_map *bpf_map_get_with_uref(u32 ufd);
+ struct bpf_map *__bpf_map_get(struct fd f);
+ struct bpf_map * __must_check bpf_map_inc(struct bpf_map *map, bool uref);
+ void bpf_map_put_with_uref(struct bpf_map *map);
+ void bpf_map_put(struct bpf_map *map);
+ int bpf_map_precharge_memlock(u32 pages);
+ void *bpf_map_area_alloc(size_t size, int numa_node);
+ void bpf_map_area_free(void *base);
+ 
+ extern int sysctl_unprivileged_bpf_disabled;
+ 
+ int bpf_map_new_fd(struct bpf_map *map);
+ int bpf_prog_new_fd(struct bpf_prog *prog);
+ 
+ int bpf_obj_pin_user(u32 ufd, const char __user *pathname);
+ int bpf_obj_get_user(const char __user *pathname);
+ 
+ int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,
+ 			   u64 flags);
+ int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,
+ 			    u64 flags);
+ 
+ int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value);
+ 
+ int bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				 void *key, void *value, u64 map_flags);
+ int bpf_fd_array_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ void bpf_fd_array_map_clear(struct bpf_map *map);
+ int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				void *key, void *value, u64 map_flags);
+ int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ 
+ /* memcpy that is used with 8-byte aligned pointers, power-of-8 size and
+  * forced to use 'long' read/writes to try to atomically copy long counters.
+  * Best-effort only.  No barriers here, since it _will_ race with concurrent
+  * updates from BPF programs. Called from bpf syscall and mostly used with
+  * size 8 or 16 bytes, so ask compiler to inline it.
+  */
+ static inline void bpf_long_memcpy(void *dst, const void *src, u32 size)
+ {
+ 	const long *lsrc = src;
+ 	long *ldst = dst;
+ 
+ 	size /= sizeof(long);
+ 	while (size--)
+ 		*ldst++ = *lsrc++;
+ }
+ 
+ /* verify correctness of eBPF program */
+ int bpf_check(struct bpf_prog **fp, union bpf_attr *attr);
+ 
+ /* Map specifics */
+ struct net_device  *__dev_map_lookup_elem(struct bpf_map *map, u32 key);
+ void __dev_map_insert_ctx(struct bpf_map *map, u32 index);
+ void __dev_map_flush(struct bpf_map *map);
+ 
+ /* Return map's numa specified by userspace */
+ static inline int bpf_map_attr_numa_node(const union bpf_attr *attr)
+ {
+ 	return (attr->map_flags & BPF_F_NUMA_NODE) ?
+ 		attr->numa_node : NUMA_NO_NODE;
+ }
+ 
+ #else
+ static inline struct bpf_prog *bpf_prog_get(u32 ufd)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get_type(u32 ufd,
+ 						 enum bpf_prog_type type)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ static inline struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog,
+ 							  int i)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void bpf_prog_sub(struct bpf_prog *prog, int i)
+ {
+ }
+ 
+ static inline void bpf_prog_put(struct bpf_prog *prog)
+ {
+ }
+ 
+ static inline struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *__must_check
+ bpf_prog_inc_not_zero(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline int __bpf_prog_charge(struct user_struct *user, u32 pages)
+ {
+ 	return 0;
+ }
+ 
+ static inline void __bpf_prog_uncharge(struct user_struct *user, u32 pages)
+ {
+ }
+ 
+ static inline struct net_device  *__dev_map_lookup_elem(struct bpf_map *map,
+ 						       u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void __dev_map_insert_ctx(struct bpf_map *map, u32 index)
+ {
+ }
+ 
+ static inline void __dev_map_flush(struct bpf_map *map)
+ {
+ }
+ #endif /* CONFIG_BPF_SYSCALL */
+ 
+ #if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_BPF_SYSCALL)
+ struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key);
+ int sock_map_prog(struct bpf_map *map, struct bpf_prog *prog, u32 type);
+ #else
+ static inline struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline int sock_map_prog(struct bpf_map *map,
+ 				struct bpf_prog *prog,
+ 				u32 type)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ #endif
+ 
+ /* verifier prototypes for helper functions called from eBPF programs */
+ extern const struct bpf_func_proto bpf_map_lookup_elem_proto;
+ extern const struct bpf_func_proto bpf_map_update_elem_proto;
+ extern const struct bpf_func_proto bpf_map_delete_elem_proto;
+ 
+ extern const struct bpf_func_proto bpf_get_prandom_u32_proto;
+ extern const struct bpf_func_proto bpf_get_smp_processor_id_proto;
+ extern const struct bpf_func_proto bpf_get_numa_node_id_proto;
+ extern const struct bpf_func_proto bpf_tail_call_proto;
+ extern const struct bpf_func_proto bpf_ktime_get_ns_proto;
+ extern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;
+ extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
+ extern const struct bpf_func_proto bpf_get_current_comm_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_push_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_pop_proto;
+ extern const struct bpf_func_proto bpf_get_stackid_proto;
+ extern const struct bpf_func_proto bpf_sock_map_update_proto;
+ 
+ /* Shared helpers among cBPF and eBPF. */
+ void bpf_user_rnd_init_once(void);
+ u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
++>>>>>>> 468e2f64d220 (bpf: introduce BPF_PROG_QUERY command)
  #endif /* _LINUX_BPF_H */
diff --cc include/uapi/linux/bpf.h
index e369860b690e,cb2b9f95160a..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -62,50 -68,31 +62,64 @@@ struct bpf_insn 
  	__s32	imm;		/* signed immediate constant */
  };
  
 -/* Key of an a BPF_MAP_TYPE_LPM_TRIE entry */
 -struct bpf_lpm_trie_key {
 -	__u32	prefixlen;	/* up to 32 for AF_INET, 128 for AF_INET6 */
 -	__u8	data[0];	/* Arbitrary size */
 -};
 -
 -/* BPF syscall commands, see bpf(2) man-page for details. */
 +/* BPF syscall commands */
  enum bpf_cmd {
 +	/* create a map with given type and attributes
 +	 * fd = bpf(BPF_MAP_CREATE, union bpf_attr *, u32 size)
 +	 * returns fd or negative error
 +	 * map is deleted when fd is closed
 +	 */
  	BPF_MAP_CREATE,
 +
 +	/* verify and load eBPF program
 +	 * prog_fd = bpf(BPF_PROG_LOAD, union bpf_attr *attr, u32 size)
 +	 * Using attr->prog_type, attr->insns, attr->license
 +	 * returns fd or negative error
 +	 */
 +	BPF_PROG_LOAD,
++<<<<<<< HEAD
 +
 +	/* lookup key in a given map
 +	 * err = bpf(BPF_MAP_LOOKUP_ELEM, union bpf_attr *attr, u32 size)
 +	 * Using attr->map_fd, attr->key, attr->value
 +	 * returns zero and stores found elem into value
 +	 * or negative error
 +	 */
  	BPF_MAP_LOOKUP_ELEM,
 +
 +	/* create or update key/value pair in a given map
 +	 * err = bpf(BPF_MAP_UPDATE_ELEM, union bpf_attr *attr, u32 size)
 +	 * Using attr->map_fd, attr->key, attr->value
 +	 * returns zero or negative error
 +	 */
  	BPF_MAP_UPDATE_ELEM,
 +
 +	/* find and delete elem by key in a given map
 +	 * err = bpf(BPF_MAP_DELETE_ELEM, union bpf_attr *attr, u32 size)
 +	 * Using attr->map_fd, attr->key
 +	 * returns zero or negative error
 +	 */
  	BPF_MAP_DELETE_ELEM,
 +
 +	/* lookup key in a given map and return next key
 +	 * err = bpf(BPF_MAP_GET_NEXT_KEY, union bpf_attr *attr, u32 size)
 +	 * Using attr->map_fd, attr->key, attr->next_key
 +	 * returns zero and stores next key or negative error
 +	 */
  	BPF_MAP_GET_NEXT_KEY,
 -	BPF_PROG_LOAD,
++=======
+ 	BPF_OBJ_PIN,
+ 	BPF_OBJ_GET,
+ 	BPF_PROG_ATTACH,
+ 	BPF_PROG_DETACH,
+ 	BPF_PROG_TEST_RUN,
+ 	BPF_PROG_GET_NEXT_ID,
+ 	BPF_MAP_GET_NEXT_ID,
+ 	BPF_PROG_GET_FD_BY_ID,
+ 	BPF_MAP_GET_FD_BY_ID,
+ 	BPF_OBJ_GET_INFO_BY_FD,
+ 	BPF_PROG_QUERY,
++>>>>>>> 468e2f64d220 (bpf: introduce BPF_PROG_QUERY command)
  };
  
  enum bpf_map_type {
@@@ -115,8 -116,107 +129,96 @@@
  
  enum bpf_prog_type {
  	BPF_PROG_TYPE_UNSPEC,
 -	BPF_PROG_TYPE_SOCKET_FILTER,
 -	BPF_PROG_TYPE_KPROBE,
 -	BPF_PROG_TYPE_SCHED_CLS,
 -	BPF_PROG_TYPE_SCHED_ACT,
 -	BPF_PROG_TYPE_TRACEPOINT,
 -	BPF_PROG_TYPE_XDP,
 -	BPF_PROG_TYPE_PERF_EVENT,
 -	BPF_PROG_TYPE_CGROUP_SKB,
 -	BPF_PROG_TYPE_CGROUP_SOCK,
 -	BPF_PROG_TYPE_LWT_IN,
 -	BPF_PROG_TYPE_LWT_OUT,
 -	BPF_PROG_TYPE_LWT_XMIT,
 -	BPF_PROG_TYPE_SOCK_OPS,
 -	BPF_PROG_TYPE_SK_SKB,
  };
  
++<<<<<<< HEAD
++=======
+ enum bpf_attach_type {
+ 	BPF_CGROUP_INET_INGRESS,
+ 	BPF_CGROUP_INET_EGRESS,
+ 	BPF_CGROUP_INET_SOCK_CREATE,
+ 	BPF_CGROUP_SOCK_OPS,
+ 	BPF_SK_SKB_STREAM_PARSER,
+ 	BPF_SK_SKB_STREAM_VERDICT,
+ 	__MAX_BPF_ATTACH_TYPE
+ };
+ 
+ #define MAX_BPF_ATTACH_TYPE __MAX_BPF_ATTACH_TYPE
+ 
+ /* cgroup-bpf attach flags used in BPF_PROG_ATTACH command
+  *
+  * NONE(default): No further bpf programs allowed in the subtree.
+  *
+  * BPF_F_ALLOW_OVERRIDE: If a sub-cgroup installs some bpf program,
+  * the program in this cgroup yields to sub-cgroup program.
+  *
+  * BPF_F_ALLOW_MULTI: If a sub-cgroup installs some bpf program,
+  * that cgroup program gets run in addition to the program in this cgroup.
+  *
+  * Only one program is allowed to be attached to a cgroup with
+  * NONE or BPF_F_ALLOW_OVERRIDE flag.
+  * Attaching another program on top of NONE or BPF_F_ALLOW_OVERRIDE will
+  * release old program and attach the new one. Attach flags has to match.
+  *
+  * Multiple programs are allowed to be attached to a cgroup with
+  * BPF_F_ALLOW_MULTI flag. They are executed in FIFO order
+  * (those that were attached first, run first)
+  * The programs of sub-cgroup are executed first, then programs of
+  * this cgroup and then programs of parent cgroup.
+  * When children program makes decision (like picking TCP CA or sock bind)
+  * parent program has a chance to override it.
+  *
+  * A cgroup with MULTI or OVERRIDE flag allows any attach flags in sub-cgroups.
+  * A cgroup with NONE doesn't allow any programs in sub-cgroups.
+  * Ex1:
+  * cgrp1 (MULTI progs A, B) ->
+  *    cgrp2 (OVERRIDE prog C) ->
+  *      cgrp3 (MULTI prog D) ->
+  *        cgrp4 (OVERRIDE prog E) ->
+  *          cgrp5 (NONE prog F)
+  * the event in cgrp5 triggers execution of F,D,A,B in that order.
+  * if prog F is detached, the execution is E,D,A,B
+  * if prog F and D are detached, the execution is E,A,B
+  * if prog F, E and D are detached, the execution is C,A,B
+  *
+  * All eligible programs are executed regardless of return code from
+  * earlier programs.
+  */
+ #define BPF_F_ALLOW_OVERRIDE	(1U << 0)
+ #define BPF_F_ALLOW_MULTI	(1U << 1)
+ 
+ /* If BPF_F_STRICT_ALIGNMENT is used in BPF_PROG_LOAD command, the
+  * verifier will perform strict alignment checking as if the kernel
+  * has been built with CONFIG_EFFICIENT_UNALIGNED_ACCESS not set,
+  * and NET_IP_ALIGN defined to 2.
+  */
+ #define BPF_F_STRICT_ALIGNMENT	(1U << 0)
+ 
+ #define BPF_PSEUDO_MAP_FD	1
+ 
+ /* flags for BPF_MAP_UPDATE_ELEM command */
+ #define BPF_ANY		0 /* create new element or update existing */
+ #define BPF_NOEXIST	1 /* create new element if it didn't exist */
+ #define BPF_EXIST	2 /* update existing element */
+ 
+ /* flags for BPF_MAP_CREATE command */
+ #define BPF_F_NO_PREALLOC	(1U << 0)
+ /* Instead of having one common LRU list in the
+  * BPF_MAP_TYPE_LRU_[PERCPU_]HASH map, use a percpu LRU list
+  * which can scale and perform better.
+  * Note, the LRU nodes (including free nodes) cannot be moved
+  * across different LRU lists.
+  */
+ #define BPF_F_NO_COMMON_LRU	(1U << 1)
+ /* Specify numa node during map creation */
+ #define BPF_F_NUMA_NODE		(1U << 2)
+ 
+ /* flags for BPF_PROG_QUERY */
+ #define BPF_F_QUERY_EFFECTIVE	(1U << 0)
+ 
+ #define BPF_OBJ_NAME_LEN 16U
+ 
++>>>>>>> 468e2f64d220 (bpf: introduce BPF_PROG_QUERY command)
  union bpf_attr {
  	struct { /* anonymous struct used by BPF_MAP_CREATE command */
  		__u32	map_type;	/* one of enum bpf_map_type */
@@@ -132,13 -240,597 +234,76 @@@
  			__aligned_u64 value;
  			__aligned_u64 next_key;
  		};
 -		__u64		flags;
  	};
++<<<<<<< HEAD
++=======
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_LOAD command */
+ 		__u32		prog_type;	/* one of enum bpf_prog_type */
+ 		__u32		insn_cnt;
+ 		__aligned_u64	insns;
+ 		__aligned_u64	license;
+ 		__u32		log_level;	/* verbosity level of verifier */
+ 		__u32		log_size;	/* size of user buffer */
+ 		__aligned_u64	log_buf;	/* user supplied buffer */
+ 		__u32		kern_version;	/* checked when prog_type=kprobe */
+ 		__u32		prog_flags;
+ 		__u8		prog_name[BPF_OBJ_NAME_LEN];
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_OBJ_* commands */
+ 		__aligned_u64	pathname;
+ 		__u32		bpf_fd;
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_ATTACH/DETACH commands */
+ 		__u32		target_fd;	/* container object to attach to */
+ 		__u32		attach_bpf_fd;	/* eBPF program to attach */
+ 		__u32		attach_type;
+ 		__u32		attach_flags;
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_TEST_RUN command */
+ 		__u32		prog_fd;
+ 		__u32		retval;
+ 		__u32		data_size_in;
+ 		__u32		data_size_out;
+ 		__aligned_u64	data_in;
+ 		__aligned_u64	data_out;
+ 		__u32		repeat;
+ 		__u32		duration;
+ 	} test;
+ 
+ 	struct { /* anonymous struct used by BPF_*_GET_*_ID */
+ 		union {
+ 			__u32		start_id;
+ 			__u32		prog_id;
+ 			__u32		map_id;
+ 		};
+ 		__u32		next_id;
+ 	};
+ 
+ 	struct { /* anonymous struct used by BPF_OBJ_GET_INFO_BY_FD */
+ 		__u32		bpf_fd;
+ 		__u32		info_len;
+ 		__aligned_u64	info;
+ 	} info;
+ 
+ 	struct { /* anonymous struct used by BPF_PROG_QUERY command */
+ 		__u32		target_fd;	/* container object to query */
+ 		__u32		attach_type;
+ 		__u32		query_flags;
+ 		__u32		attach_flags;
+ 		__aligned_u64	prog_ids;
+ 		__u32		prog_cnt;
+ 	} query;
++>>>>>>> 468e2f64d220 (bpf: introduce BPF_PROG_QUERY command)
  } __attribute__((aligned(8)));
  
 -/* BPF helper function descriptions:
 - *
 - * void *bpf_map_lookup_elem(&map, &key)
 - *     Return: Map value or NULL
 - *
 - * int bpf_map_update_elem(&map, &key, &value, flags)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_map_delete_elem(&map, &key)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_probe_read(void *dst, int size, void *src)
 - *     Return: 0 on success or negative error
 - *
 - * u64 bpf_ktime_get_ns(void)
 - *     Return: current ktime
 - *
 - * int bpf_trace_printk(const char *fmt, int fmt_size, ...)
 - *     Return: length of buffer written or negative error
 - *
 - * u32 bpf_prandom_u32(void)
 - *     Return: random value
 - *
 - * u32 bpf_raw_smp_processor_id(void)
 - *     Return: SMP processor ID
 - *
 - * int bpf_skb_store_bytes(skb, offset, from, len, flags)
 - *     store bytes into packet
 - *     @skb: pointer to skb
 - *     @offset: offset within packet from skb->mac_header
 - *     @from: pointer where to copy bytes from
 - *     @len: number of bytes to store into packet
 - *     @flags: bit 0 - if true, recompute skb->csum
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_l3_csum_replace(skb, offset, from, to, flags)
 - *     recompute IP checksum
 - *     @skb: pointer to skb
 - *     @offset: offset within packet where IP checksum is located
 - *     @from: old value of header field
 - *     @to: new value of header field
 - *     @flags: bits 0-3 - size of header field
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_l4_csum_replace(skb, offset, from, to, flags)
 - *     recompute TCP/UDP checksum
 - *     @skb: pointer to skb
 - *     @offset: offset within packet where TCP/UDP checksum is located
 - *     @from: old value of header field
 - *     @to: new value of header field
 - *     @flags: bits 0-3 - size of header field
 - *             bit 4 - is pseudo header
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_tail_call(ctx, prog_array_map, index)
 - *     jump into another BPF program
 - *     @ctx: context pointer passed to next program
 - *     @prog_array_map: pointer to map which type is BPF_MAP_TYPE_PROG_ARRAY
 - *     @index: index inside array that selects specific program to run
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_clone_redirect(skb, ifindex, flags)
 - *     redirect to another netdev
 - *     @skb: pointer to skb
 - *     @ifindex: ifindex of the net device
 - *     @flags: bit 0 - if set, redirect to ingress instead of egress
 - *             other bits - reserved
 - *     Return: 0 on success or negative error
 - *
 - * u64 bpf_get_current_pid_tgid(void)
 - *     Return: current->tgid << 32 | current->pid
 - *
 - * u64 bpf_get_current_uid_gid(void)
 - *     Return: current_gid << 32 | current_uid
 - *
 - * int bpf_get_current_comm(char *buf, int size_of_buf)
 - *     stores current->comm into buf
 - *     Return: 0 on success or negative error
 - *
 - * u32 bpf_get_cgroup_classid(skb)
 - *     retrieve a proc's classid
 - *     @skb: pointer to skb
 - *     Return: classid if != 0
 - *
 - * int bpf_skb_vlan_push(skb, vlan_proto, vlan_tci)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_vlan_pop(skb)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_get_tunnel_key(skb, key, size, flags)
 - * int bpf_skb_set_tunnel_key(skb, key, size, flags)
 - *     retrieve or populate tunnel metadata
 - *     @skb: pointer to skb
 - *     @key: pointer to 'struct bpf_tunnel_key'
 - *     @size: size of 'struct bpf_tunnel_key'
 - *     @flags: room for future extensions
 - *     Return: 0 on success or negative error
 - *
 - * u64 bpf_perf_event_read(map, flags)
 - *     read perf event counter value
 - *     @map: pointer to perf_event_array map
 - *     @flags: index of event in the map or bitmask flags
 - *     Return: value of perf event counter read or error code
 - *
 - * int bpf_redirect(ifindex, flags)
 - *     redirect to another netdev
 - *     @ifindex: ifindex of the net device
 - *     @flags:
 - *	  cls_bpf:
 - *          bit 0 - if set, redirect to ingress instead of egress
 - *          other bits - reserved
 - *	  xdp_bpf:
 - *	    all bits - reserved
 - *     Return: cls_bpf: TC_ACT_REDIRECT on success or TC_ACT_SHOT on error
 - *	       xdp_bfp: XDP_REDIRECT on success or XDP_ABORT on error
 - * int bpf_redirect_map(map, key, flags)
 - *     redirect to endpoint in map
 - *     @map: pointer to dev map
 - *     @key: index in map to lookup
 - *     @flags: --
 - *     Return: XDP_REDIRECT on success or XDP_ABORT on error
 - *
 - * u32 bpf_get_route_realm(skb)
 - *     retrieve a dst's tclassid
 - *     @skb: pointer to skb
 - *     Return: realm if != 0
 - *
 - * int bpf_perf_event_output(ctx, map, flags, data, size)
 - *     output perf raw sample
 - *     @ctx: struct pt_regs*
 - *     @map: pointer to perf_event_array map
 - *     @flags: index of event in the map or bitmask flags
 - *     @data: data on stack to be output as raw data
 - *     @size: size of data
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_get_stackid(ctx, map, flags)
 - *     walk user or kernel stack and return id
 - *     @ctx: struct pt_regs*
 - *     @map: pointer to stack_trace map
 - *     @flags: bits 0-7 - numer of stack frames to skip
 - *             bit 8 - collect user stack instead of kernel
 - *             bit 9 - compare stacks by hash only
 - *             bit 10 - if two different stacks hash into the same stackid
 - *                      discard old
 - *             other bits - reserved
 - *     Return: >= 0 stackid on success or negative error
 - *
 - * s64 bpf_csum_diff(from, from_size, to, to_size, seed)
 - *     calculate csum diff
 - *     @from: raw from buffer
 - *     @from_size: length of from buffer
 - *     @to: raw to buffer
 - *     @to_size: length of to buffer
 - *     @seed: optional seed
 - *     Return: csum result or negative error code
 - *
 - * int bpf_skb_get_tunnel_opt(skb, opt, size)
 - *     retrieve tunnel options metadata
 - *     @skb: pointer to skb
 - *     @opt: pointer to raw tunnel option data
 - *     @size: size of @opt
 - *     Return: option size
 - *
 - * int bpf_skb_set_tunnel_opt(skb, opt, size)
 - *     populate tunnel options metadata
 - *     @skb: pointer to skb
 - *     @opt: pointer to raw tunnel option data
 - *     @size: size of @opt
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_change_proto(skb, proto, flags)
 - *     Change protocol of the skb. Currently supported is v4 -> v6,
 - *     v6 -> v4 transitions. The helper will also resize the skb. eBPF
 - *     program is expected to fill the new headers via skb_store_bytes
 - *     and lX_csum_replace.
 - *     @skb: pointer to skb
 - *     @proto: new skb->protocol type
 - *     @flags: reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_change_type(skb, type)
 - *     Change packet type of skb.
 - *     @skb: pointer to skb
 - *     @type: new skb->pkt_type type
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_under_cgroup(skb, map, index)
 - *     Check cgroup2 membership of skb
 - *     @skb: pointer to skb
 - *     @map: pointer to bpf_map in BPF_MAP_TYPE_CGROUP_ARRAY type
 - *     @index: index of the cgroup in the bpf_map
 - *     Return:
 - *       == 0 skb failed the cgroup2 descendant test
 - *       == 1 skb succeeded the cgroup2 descendant test
 - *        < 0 error
 - *
 - * u32 bpf_get_hash_recalc(skb)
 - *     Retrieve and possibly recalculate skb->hash.
 - *     @skb: pointer to skb
 - *     Return: hash
 - *
 - * u64 bpf_get_current_task(void)
 - *     Returns current task_struct
 - *     Return: current
 - *
 - * int bpf_probe_write_user(void *dst, void *src, int len)
 - *     safely attempt to write to a location
 - *     @dst: destination address in userspace
 - *     @src: source address on stack
 - *     @len: number of bytes to copy
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_current_task_under_cgroup(map, index)
 - *     Check cgroup2 membership of current task
 - *     @map: pointer to bpf_map in BPF_MAP_TYPE_CGROUP_ARRAY type
 - *     @index: index of the cgroup in the bpf_map
 - *     Return:
 - *       == 0 current failed the cgroup2 descendant test
 - *       == 1 current succeeded the cgroup2 descendant test
 - *        < 0 error
 - *
 - * int bpf_skb_change_tail(skb, len, flags)
 - *     The helper will resize the skb to the given new size, to be used f.e.
 - *     with control messages.
 - *     @skb: pointer to skb
 - *     @len: new skb length
 - *     @flags: reserved
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_skb_pull_data(skb, len)
 - *     The helper will pull in non-linear data in case the skb is non-linear
 - *     and not all of len are part of the linear section. Only needed for
 - *     read/write with direct packet access.
 - *     @skb: pointer to skb
 - *     @len: len to make read/writeable
 - *     Return: 0 on success or negative error
 - *
 - * s64 bpf_csum_update(skb, csum)
 - *     Adds csum into skb->csum in case of CHECKSUM_COMPLETE.
 - *     @skb: pointer to skb
 - *     @csum: csum to add
 - *     Return: csum on success or negative error
 - *
 - * void bpf_set_hash_invalid(skb)
 - *     Invalidate current skb->hash.
 - *     @skb: pointer to skb
 - *
 - * int bpf_get_numa_node_id()
 - *     Return: Id of current NUMA node.
 - *
 - * int bpf_skb_change_head()
 - *     Grows headroom of skb and adjusts MAC header offset accordingly.
 - *     Will extends/reallocae as required automatically.
 - *     May change skb data pointer and will thus invalidate any check
 - *     performed for direct packet access.
 - *     @skb: pointer to skb
 - *     @len: length of header to be pushed in front
 - *     @flags: Flags (unused for now)
 - *     Return: 0 on success or negative error
 - *
 - * int bpf_xdp_adjust_head(xdp_md, delta)
 - *     Adjust the xdp_md.data by delta
 - *     @xdp_md: pointer to xdp_md
 - *     @delta: An positive/negative integer to be added to xdp_md.data
 - *     Return: 0 on success or negative on error
 - *
 - * int bpf_probe_read_str(void *dst, int size, const void *unsafe_ptr)
 - *     Copy a NUL terminated string from unsafe address. In case the string
 - *     length is smaller than size, the target is not padded with further NUL
 - *     bytes. In case the string length is larger than size, just count-1
 - *     bytes are copied and the last byte is set to NUL.
 - *     @dst: destination address
 - *     @size: maximum number of bytes to copy, including the trailing NUL
 - *     @unsafe_ptr: unsafe address
 - *     Return:
 - *       > 0 length of the string including the trailing NUL on success
 - *       < 0 error
 - *
 - * u64 bpf_get_socket_cookie(skb)
 - *     Get the cookie for the socket stored inside sk_buff.
 - *     @skb: pointer to skb
 - *     Return: 8 Bytes non-decreasing number on success or 0 if the socket
 - *     field is missing inside sk_buff
 - *
 - * u32 bpf_get_socket_uid(skb)
 - *     Get the owner uid of the socket stored inside sk_buff.
 - *     @skb: pointer to skb
 - *     Return: uid of the socket owner on success or overflowuid if failed.
 - *
 - * u32 bpf_set_hash(skb, hash)
 - *     Set full skb->hash.
 - *     @skb: pointer to skb
 - *     @hash: hash to set
 - *
 - * int bpf_setsockopt(bpf_socket, level, optname, optval, optlen)
 - *     Calls setsockopt. Not all opts are available, only those with
 - *     integer optvals plus TCP_CONGESTION.
 - *     Supported levels: SOL_SOCKET and IPROTO_TCP
 - *     @bpf_socket: pointer to bpf_socket
 - *     @level: SOL_SOCKET or IPROTO_TCP
 - *     @optname: option name
 - *     @optval: pointer to option value
 - *     @optlen: length of optval in byes
 - *     Return: 0 or negative error
 - *
 - * int bpf_skb_adjust_room(skb, len_diff, mode, flags)
 - *     Grow or shrink room in sk_buff.
 - *     @skb: pointer to skb
 - *     @len_diff: (signed) amount of room to grow/shrink
 - *     @mode: operation mode (enum bpf_adj_room_mode)
 - *     @flags: reserved for future use
 - *     Return: 0 on success or negative error code
 - *
 - * int bpf_sk_redirect_map(map, key, flags)
 - *     Redirect skb to a sock in map using key as a lookup key for the
 - *     sock in map.
 - *     @map: pointer to sockmap
 - *     @key: key to lookup sock in map
 - *     @flags: reserved for future use
 - *     Return: SK_REDIRECT
 - *
 - * int bpf_sock_map_update(skops, map, key, flags)
 - *	@skops: pointer to bpf_sock_ops
 - *	@map: pointer to sockmap to update
 - *	@key: key to insert/update sock in map
 - *	@flags: same flags as map update elem
 - *
 - * int bpf_xdp_adjust_meta(xdp_md, delta)
 - *     Adjust the xdp_md.data_meta by delta
 - *     @xdp_md: pointer to xdp_md
 - *     @delta: An positive/negative integer to be added to xdp_md.data_meta
 - *     Return: 0 on success or negative on error
 - */
 -#define __BPF_FUNC_MAPPER(FN)		\
 -	FN(unspec),			\
 -	FN(map_lookup_elem),		\
 -	FN(map_update_elem),		\
 -	FN(map_delete_elem),		\
 -	FN(probe_read),			\
 -	FN(ktime_get_ns),		\
 -	FN(trace_printk),		\
 -	FN(get_prandom_u32),		\
 -	FN(get_smp_processor_id),	\
 -	FN(skb_store_bytes),		\
 -	FN(l3_csum_replace),		\
 -	FN(l4_csum_replace),		\
 -	FN(tail_call),			\
 -	FN(clone_redirect),		\
 -	FN(get_current_pid_tgid),	\
 -	FN(get_current_uid_gid),	\
 -	FN(get_current_comm),		\
 -	FN(get_cgroup_classid),		\
 -	FN(skb_vlan_push),		\
 -	FN(skb_vlan_pop),		\
 -	FN(skb_get_tunnel_key),		\
 -	FN(skb_set_tunnel_key),		\
 -	FN(perf_event_read),		\
 -	FN(redirect),			\
 -	FN(get_route_realm),		\
 -	FN(perf_event_output),		\
 -	FN(skb_load_bytes),		\
 -	FN(get_stackid),		\
 -	FN(csum_diff),			\
 -	FN(skb_get_tunnel_opt),		\
 -	FN(skb_set_tunnel_opt),		\
 -	FN(skb_change_proto),		\
 -	FN(skb_change_type),		\
 -	FN(skb_under_cgroup),		\
 -	FN(get_hash_recalc),		\
 -	FN(get_current_task),		\
 -	FN(probe_write_user),		\
 -	FN(current_task_under_cgroup),	\
 -	FN(skb_change_tail),		\
 -	FN(skb_pull_data),		\
 -	FN(csum_update),		\
 -	FN(set_hash_invalid),		\
 -	FN(get_numa_node_id),		\
 -	FN(skb_change_head),		\
 -	FN(xdp_adjust_head),		\
 -	FN(probe_read_str),		\
 -	FN(get_socket_cookie),		\
 -	FN(get_socket_uid),		\
 -	FN(set_hash),			\
 -	FN(setsockopt),			\
 -	FN(skb_adjust_room),		\
 -	FN(redirect_map),		\
 -	FN(sk_redirect_map),		\
 -	FN(sock_map_update),		\
 -	FN(xdp_adjust_meta),
 -
 -/* integer value in 'imm' field of BPF_CALL instruction selects which helper
 - * function eBPF program intends to call
 - */
 -#define __BPF_ENUM_FN(x) BPF_FUNC_ ## x
 -enum bpf_func_id {
 -	__BPF_FUNC_MAPPER(__BPF_ENUM_FN)
 -	__BPF_FUNC_MAX_ID,
 -};
 -#undef __BPF_ENUM_FN
 -
 -/* All flags used by eBPF helper functions, placed here. */
 -
 -/* BPF_FUNC_skb_store_bytes flags. */
 -#define BPF_F_RECOMPUTE_CSUM		(1ULL << 0)
 -#define BPF_F_INVALIDATE_HASH		(1ULL << 1)
 -
 -/* BPF_FUNC_l3_csum_replace and BPF_FUNC_l4_csum_replace flags.
 - * First 4 bits are for passing the header field size.
 - */
 -#define BPF_F_HDR_FIELD_MASK		0xfULL
 -
 -/* BPF_FUNC_l4_csum_replace flags. */
 -#define BPF_F_PSEUDO_HDR		(1ULL << 4)
 -#define BPF_F_MARK_MANGLED_0		(1ULL << 5)
 -#define BPF_F_MARK_ENFORCE		(1ULL << 6)
 -
 -/* BPF_FUNC_clone_redirect and BPF_FUNC_redirect flags. */
 -#define BPF_F_INGRESS			(1ULL << 0)
 -
 -/* BPF_FUNC_skb_set_tunnel_key and BPF_FUNC_skb_get_tunnel_key flags. */
 -#define BPF_F_TUNINFO_IPV6		(1ULL << 0)
 -
 -/* BPF_FUNC_get_stackid flags. */
 -#define BPF_F_SKIP_FIELD_MASK		0xffULL
 -#define BPF_F_USER_STACK		(1ULL << 8)
 -#define BPF_F_FAST_STACK_CMP		(1ULL << 9)
 -#define BPF_F_REUSE_STACKID		(1ULL << 10)
 -
 -/* BPF_FUNC_skb_set_tunnel_key flags. */
 -#define BPF_F_ZERO_CSUM_TX		(1ULL << 1)
 -#define BPF_F_DONT_FRAGMENT		(1ULL << 2)
 -
 -/* BPF_FUNC_perf_event_output and BPF_FUNC_perf_event_read flags. */
 -#define BPF_F_INDEX_MASK		0xffffffffULL
 -#define BPF_F_CURRENT_CPU		BPF_F_INDEX_MASK
 -/* BPF_FUNC_perf_event_output for sk_buff input context. */
 -#define BPF_F_CTXLEN_MASK		(0xfffffULL << 32)
 -
 -/* Mode for BPF_FUNC_skb_adjust_room helper. */
 -enum bpf_adj_room_mode {
 -	BPF_ADJ_ROOM_NET,
 -};
 -
 -/* user accessible mirror of in-kernel sk_buff.
 - * new fields can only be added to the end of this structure
 - */
 -struct __sk_buff {
 -	__u32 len;
 -	__u32 pkt_type;
 -	__u32 mark;
 -	__u32 queue_mapping;
 -	__u32 protocol;
 -	__u32 vlan_present;
 -	__u32 vlan_tci;
 -	__u32 vlan_proto;
 -	__u32 priority;
 -	__u32 ingress_ifindex;
 -	__u32 ifindex;
 -	__u32 tc_index;
 -	__u32 cb[5];
 -	__u32 hash;
 -	__u32 tc_classid;
 -	__u32 data;
 -	__u32 data_end;
 -	__u32 napi_id;
 -
 -	/* Accessed by BPF_PROG_TYPE_sk_skb types from here to ... */
 -	__u32 family;
 -	__u32 remote_ip4;	/* Stored in network byte order */
 -	__u32 local_ip4;	/* Stored in network byte order */
 -	__u32 remote_ip6[4];	/* Stored in network byte order */
 -	__u32 local_ip6[4];	/* Stored in network byte order */
 -	__u32 remote_port;	/* Stored in network byte order */
 -	__u32 local_port;	/* stored in host byte order */
 -	/* ... here. */
 -
 -	__u32 data_meta;
 -};
 -
 -struct bpf_tunnel_key {
 -	__u32 tunnel_id;
 -	union {
 -		__u32 remote_ipv4;
 -		__u32 remote_ipv6[4];
 -	};
 -	__u8 tunnel_tos;
 -	__u8 tunnel_ttl;
 -	__u16 tunnel_ext;
 -	__u32 tunnel_label;
 -};
 -
 -/* Generic BPF return codes which all BPF program types may support.
 - * The values are binary compatible with their TC_ACT_* counter-part to
 - * provide backwards compatibility with existing SCHED_CLS and SCHED_ACT
 - * programs.
 - *
 - * XDP is handled seprately, see XDP_*.
 - */
 -enum bpf_ret_code {
 -	BPF_OK = 0,
 -	/* 1 reserved */
 -	BPF_DROP = 2,
 -	/* 3-6 reserved */
 -	BPF_REDIRECT = 7,
 -	/* >127 are reserved for prog type specific return codes */
 -};
 -
 -struct bpf_sock {
 -	__u32 bound_dev_if;
 -	__u32 family;
 -	__u32 type;
 -	__u32 protocol;
 -	__u32 mark;
 -	__u32 priority;
 -};
 -
 -#define XDP_PACKET_HEADROOM 256
 -
  /* User return codes for XDP prog type.
   * A valid XDP program must return one of these defined values. All other
 - * return codes are reserved for future use. Unknown return codes will
 - * result in packet drops and a warning via bpf_warn_invalid_xdp_action().
 + * return codes are reserved for future use. Unknown return codes will result
 + * in packet drop.
   */
  enum xdp_action {
  	XDP_ABORTED = 0,
* Unmerged path include/linux/bpf-cgroup.h
* Unmerged path kernel/bpf/cgroup.c
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/syscall.c
* Unmerged path kernel/cgroup/cgroup.c
* Unmerged path include/linux/bpf-cgroup.h
* Unmerged path include/linux/bpf.h
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/bpf/cgroup.c
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/syscall.c
* Unmerged path kernel/cgroup/cgroup.c
