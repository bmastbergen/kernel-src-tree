x86/KVM/VMX: Extend add_atomic_switch_msr() to allow VMENTER only MSRs

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] kvm/vmx: extend add_atomic_switch_msr() to allow vmenter only msrs (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 97.06%
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 989e3992d2eca32c3f1404f2bc91acda3aa122d8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/989e3992.failed

The IA32_FLUSH_CMD MSR needs only to be written on VMENTER. Extend
add_atomic_switch_msr() with an entry_only parameter to allow storing the
MSR only in the guest (ENTRY) MSR array.

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 989e3992d2eca32c3f1404f2bc91acda3aa122d8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index e2f48f8aba96,fd80f93ba98b..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -1902,9 -2484,9 +1902,13 @@@ static void add_atomic_switch_msr_speci
  }
  
  static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
- 				  u64 guest_val, u64 host_val)
+ 				  u64 guest_val, u64 host_val, bool entry_only)
  {
++<<<<<<< HEAD
 +	unsigned i;
++=======
+ 	int i, j = 0;
++>>>>>>> 989e3992d2ec (x86/KVM/VMX: Extend add_atomic_switch_msr() to allow VMENTER only MSRs)
  	struct msr_autoload *m = &vmx->msr_autoload;
  
  	switch (msr) {
@@@ -1939,37 -2521,31 +1943,64 @@@
  		wrmsrl(MSR_IA32_PEBS_ENABLE, 0);
  	}
  
++<<<<<<< HEAD
 +	for (i = 0; i < m->nr; ++i)
 +		if (m->guest[i].index == msr)
 +			break;
 +
 +	if (i == NR_AUTOLOAD_MSRS) {
++=======
+ 	i = find_msr(&m->guest, msr);
+ 	if (!entry_only)
+ 		j = find_msr(&m->host, msr);
+ 
+ 	if (i == NR_AUTOLOAD_MSRS || j == NR_AUTOLOAD_MSRS) {
++>>>>>>> 989e3992d2ec (x86/KVM/VMX: Extend add_atomic_switch_msr() to allow VMENTER only MSRs)
  		printk_once(KERN_WARNING "Not enough msr switch entries. "
  				"Can't add msr %x\n", msr);
  		return;
 +	} else if (i == m->nr) {
 +		++m->nr;
 +		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);
 +		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);
  	}
++<<<<<<< HEAD
 +
 +	m->guest[i].index = msr;
 +	m->guest[i].value = guest_val;
 +	m->host[i].index = msr;
 +	m->host[i].value = host_val;
 +}
 +
 +static void reload_tss(void)
 +{
 +	/*
 +	 * VT restores TR but not its size.  Useless.
 +	 */
 +	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
 +	struct desc_struct *descs;
 +
 +	descs = (void *)gdt->address;
 +	descs[GDT_ENTRY_TSS].type = 9; /* available TSS */
 +	load_TR_desc();
++=======
+ 	if (i < 0) {
+ 		i = m->guest.nr++;
+ 		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);
+ 	}
+ 	m->guest.val[i].index = msr;
+ 	m->guest.val[i].value = guest_val;
+ 
+ 	if (entry_only)
+ 		return;
+ 
+ 	if (j < 0) {
+ 		j = m->host.nr++;
+ 		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
+ 	}
+ 	m->host.val[j].index = msr;
+ 	m->host.val[j].value = host_val;
++>>>>>>> 989e3992d2ec (x86/KVM/VMX: Extend add_atomic_switch_msr() to allow VMENTER only MSRs)
  }
  
  static bool update_transition_efer(struct vcpu_vmx *vmx, int efer_offset)
@@@ -3156,7 -4026,27 +3187,31 @@@ static int vmx_set_msr(struct kvm_vcpu 
  			vmx_leave_nested(vcpu);
  		break;
  	case MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:
++<<<<<<< HEAD
 +		return 1; /* they are read-only */
++=======
+ 		if (!msr_info->host_initiated)
+ 			return 1; /* they are read-only */
+ 		if (!nested_vmx_allowed(vcpu))
+ 			return 1;
+ 		return vmx_set_vmx_msr(vcpu, msr_index, data);
+ 	case MSR_IA32_XSS:
+ 		if (!vmx_xsaves_supported())
+ 			return 1;
+ 		/*
+ 		 * The only supported bit as of Skylake is bit 8, but
+ 		 * it is not supported on KVM.
+ 		 */
+ 		if (data != 0)
+ 			return 1;
+ 		vcpu->arch.ia32_xss = data;
+ 		if (vcpu->arch.ia32_xss != host_xss)
+ 			add_atomic_switch_msr(vmx, MSR_IA32_XSS,
+ 				vcpu->arch.ia32_xss, host_xss, false);
+ 		else
+ 			clear_atomic_switch_msr(vmx, MSR_IA32_XSS);
+ 		break;
++>>>>>>> 989e3992d2ec (x86/KVM/VMX: Extend add_atomic_switch_msr() to allow VMENTER only MSRs)
  	case MSR_TSC_AUX:
  		if (!msr_info->host_initiated &&
  		    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))
* Unmerged path arch/x86/kvm/vmx.c
