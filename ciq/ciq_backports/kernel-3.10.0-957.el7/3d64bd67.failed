crypto: chelsio - Add authenc versions of ctr and sha

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [crypto] chelsio - Add authenc versions of ctr and sha (Arjun Vynipadath) [1523191]
Rebuild_FUZZ: 91.84%
commit-author Harsh Jain <harsh@chelsio.com>
commit 3d64bd670269b1391c924a04722441fc5cb3fc3a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3d64bd67.failed

Add ctr and sha combination of algo in authenc mode.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 3d64bd670269b1391c924a04722441fc5cb3fc3a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
diff --cc drivers/crypto/chelsio/chcr_algo.c
index 3d0dcd08d9d8,5cc84c459526..000000000000
mode 100755,100644..100644
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -1934,40 -2088,24 +1934,53 @@@ static struct sk_buff *create_authenc_w
  	struct sk_buff *skb = NULL;
  	struct chcr_wr *chcr_req;
  	struct cpl_rx_phys_dsgl *phys_cpl;
++<<<<<<< HEAD
 +	struct phys_sge_parm sg_param;
 +	unsigned int frags = 0, transhdr_len;
 +	unsigned int ivsize = crypto_aead_ivsize(tfm), dst_size = 0;
 +	unsigned int   kctx_len = 0, nents;
 +	unsigned short stop_offset = 0;
++=======
+ 	struct ulptx_sgl *ulptx;
+ 	unsigned int transhdr_len;
+ 	unsigned int dst_size = 0, temp, subtype = get_aead_subtype(tfm);
+ 	unsigned int   kctx_len = 0, dnents;
++>>>>>>> 3d64bd670269 (crypto: chelsio - Add authenc versions of ctr and sha)
  	unsigned int  assoclen = req->assoclen;
  	unsigned int  authsize = crypto_aead_authsize(tfm);
 -	int error = -EINVAL;
 +	int error = -EINVAL, src_nent;
  	int null = 0;
  	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
  		GFP_ATOMIC;
 -	struct adapter *adap = padap(a_ctx(tfm)->dev);
 +	struct adapter *adap = padap(ctx->dev);
  
 -	if (req->cryptlen == 0)
 -		return NULL;
 +	reqctx->newdstsg = NULL;
 +	dst_size = req->cryptlen + (op_type ? -authsize :
 +					      authsize);
 +	if (aeadctx->enckey_len == 0 || (req->cryptlen <= 0))
 +		goto err;
  
++<<<<<<< HEAD
 +	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
 +		goto err;
 +	src_nent = sg_nents_for_len(req->src, req->cryptlen);
 +	if (src_nent < 0)
 +		goto err;
 +
 +	if (dst_size && is_newsg(req->dst, &nents)) {
 +		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
 +		if (IS_ERR(reqctx->newdstsg))
 +			return ERR_CAST(reqctx->newdstsg);
 +		reqctx->dst = reqctx->newdstsg;
 +	} else {
 +		reqctx->dst = req->dst;
 +	}
 +	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {
++=======
+ 	reqctx->b0_dma = 0;
+ 	if (subtype == CRYPTO_ALG_SUB_TYPE_CBC_NULL ||
+ 	subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {
++>>>>>>> 3d64bd670269 (crypto: chelsio - Add authenc versions of ctr and sha)
  		null = 1;
  		assoclen = 0;
  	}
@@@ -2012,27 -2159,34 +2025,40 @@@
  	 * to the hardware spec
  	 */
  	chcr_req->sec_cpl.op_ivinsrtofst =
 -		FILL_SEC_CPL_OP_IVINSR(a_ctx(tfm)->dev->rx_channel_id, 2,
 -				       assoclen + 1);
 -	chcr_req->sec_cpl.pldlen = htonl(assoclen + IV + req->cryptlen);
 +		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,
 +				       (ivsize ? (assoclen + 1) : 0));
 +	chcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);
  	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
  					assoclen ? 1 : 0, assoclen,
 -					assoclen + IV + 1,
 -					(temp & 0x1F0) >> 4);
 +					assoclen + ivsize + 1,
 +					(stop_offset & 0x1F0) >> 4);
  	chcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(
++<<<<<<< HEAD
 +					stop_offset & 0xF,
 +					null ? 0 : assoclen + 1,
 +					stop_offset, stop_offset);
++=======
+ 					temp & 0xF,
+ 					null ? 0 : assoclen + IV + 1,
+ 					temp, temp);
+ 	if (subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL ||
+ 	    subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA)
+ 		temp = CHCR_SCMD_CIPHER_MODE_AES_CTR;
+ 	else
+ 		temp = CHCR_SCMD_CIPHER_MODE_AES_CBC;
++>>>>>>> 3d64bd670269 (crypto: chelsio - Add authenc versions of ctr and sha)
  	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,
  					(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,
- 					CHCR_SCMD_CIPHER_MODE_AES_CBC,
+ 					temp,
  					actx->auth_mode, aeadctx->hmac_ctrl,
 -					IV >> 1);
 +					ivsize >> 1);
  	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
 -					 0, 0, dst_size);
 +					 0, 1, dst_size);
  
  	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
- 	if (op_type == CHCR_ENCRYPT_OP)
+ 	if (op_type == CHCR_ENCRYPT_OP ||
+ 		subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||
+ 		subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL)
  		memcpy(chcr_req->key_ctx.key, aeadctx->key,
  		       aeadctx->enckey_len);
  	else
@@@ -2042,40 -2196,316 +2068,53 @@@
  	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<
  					4), actx->h_iopad, kctx_len -
  				(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));
++<<<<<<< HEAD
 +
++=======
+ 	if (subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||
+ 	    subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {
+ 		memcpy(reqctx->iv, aeadctx->nonce, CTR_RFC3686_NONCE_SIZE);
+ 		memcpy(reqctx->iv + CTR_RFC3686_NONCE_SIZE, req->iv,
+ 				CTR_RFC3686_IV_SIZE);
+ 		*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +
+ 			CTR_RFC3686_IV_SIZE) = cpu_to_be32(1);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, IV);
+ 	}
++>>>>>>> 3d64bd670269 (crypto: chelsio - Add authenc versions of ctr and sha)
  	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
 -	ulptx = (struct ulptx_sgl *)((u8 *)(phys_cpl + 1) + dst_size);
 -	chcr_add_aead_dst_ent(req, phys_cpl, assoclen, op_type, qid);
 -	chcr_add_aead_src_ent(req, ulptx, assoclen, op_type);
 -	atomic_inc(&adap->chcr_stats.cipher_rqst);
 -	temp = sizeof(struct cpl_rx_phys_dsgl) + dst_size +
 -		kctx_len + (reqctx->imm ? (assoclen + IV + req->cryptlen) : 0);
 -	create_wreq(a_ctx(tfm), chcr_req, &req->base, reqctx->imm, size,
 -		   transhdr_len, temp, 0);
 -	reqctx->skb = skb;
 -	reqctx->op = op_type;
 -
 -	return skb;
 -err:
 -	chcr_aead_dma_unmap(&ULD_CTX(a_ctx(tfm))->lldi.pdev->dev, req,
 -			    op_type);
 -
 -	return ERR_PTR(error);
 -}
 -
 -int chcr_aead_dma_map(struct device *dev,
 -		      struct aead_request *req,
 -		      unsigned short op_type)
 -{
 -	int error;
 -	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
 -	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 -	unsigned int authsize = crypto_aead_authsize(tfm);
 -	int dst_size;
 -
 -	dst_size = req->assoclen + req->cryptlen + (op_type ?
 -				-authsize : authsize);
 -	if (!req->cryptlen || !dst_size)
 -		return 0;
 -	reqctx->iv_dma = dma_map_single(dev, reqctx->iv, IV,
 -					DMA_BIDIRECTIONAL);
 -	if (dma_mapping_error(dev, reqctx->iv_dma))
 -		return -ENOMEM;
 -
 -	if (req->src == req->dst) {
 -		error = dma_map_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_BIDIRECTIONAL);
 -		if (!error)
 -			goto err;
 -	} else {
 -		error = dma_map_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_TO_DEVICE);
 -		if (!error)
 -			goto err;
 -		error = dma_map_sg(dev, req->dst, sg_nents(req->dst),
 -				   DMA_FROM_DEVICE);
 -		if (!error) {
 -			dma_unmap_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_TO_DEVICE);
 -			goto err;
 -		}
 -	}
 -
 -	return 0;
 -err:
 -	dma_unmap_single(dev, reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);
 -	return -ENOMEM;
 -}
 -
 -void chcr_aead_dma_unmap(struct device *dev,
 -			 struct aead_request *req,
 -			 unsigned short op_type)
 -{
 -	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
 -	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 -	unsigned int authsize = crypto_aead_authsize(tfm);
 -	int dst_size;
 -
 -	dst_size = req->assoclen + req->cryptlen + (op_type ?
 -					-authsize : authsize);
 -	if (!req->cryptlen || !dst_size)
 -		return;
 -
 -	dma_unmap_single(dev, reqctx->iv_dma, IV,
 -					DMA_BIDIRECTIONAL);
 -	if (req->src == req->dst) {
 -		dma_unmap_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_BIDIRECTIONAL);
 -	} else {
 -		dma_unmap_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_TO_DEVICE);
 -		dma_unmap_sg(dev, req->dst, sg_nents(req->dst),
 -				   DMA_FROM_DEVICE);
 -	}
 -}
 -
 -void chcr_add_aead_src_ent(struct aead_request *req,
 -			   struct ulptx_sgl *ulptx,
 -			   unsigned int assoclen,
 -			   unsigned short op_type)
 -{
 -	struct ulptx_walk ulp_walk;
 -	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
 -
 -	if (reqctx->imm) {
 -		u8 *buf = (u8 *)ulptx;
 -
 -		if (reqctx->b0_dma) {
 -			memcpy(buf, reqctx->scratch_pad, reqctx->b0_len);
 -			buf += reqctx->b0_len;
 -		}
 -		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
 -				   buf, assoclen, 0);
 -		buf += assoclen;
 -		memcpy(buf, reqctx->iv, IV);
 -		buf += IV;
 -		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
 -				   buf, req->cryptlen, req->assoclen);
 -	} else {
 -		ulptx_walk_init(&ulp_walk, ulptx);
 -		if (reqctx->b0_dma)
 -			ulptx_walk_add_page(&ulp_walk, reqctx->b0_len,
 -					    &reqctx->b0_dma);
 -		ulptx_walk_add_sg(&ulp_walk, req->src, assoclen, 0);
 -		ulptx_walk_add_page(&ulp_walk, IV, &reqctx->iv_dma);
 -		ulptx_walk_add_sg(&ulp_walk, req->src, req->cryptlen,
 -				  req->assoclen);
 -		ulptx_walk_end(&ulp_walk);
 -	}
 -}
 -
 -void chcr_add_aead_dst_ent(struct aead_request *req,
 -			   struct cpl_rx_phys_dsgl *phys_cpl,
 -			   unsigned int assoclen,
 -			   unsigned short op_type,
 -			   unsigned short qid)
 -{
 -	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
 -	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 -	struct dsgl_walk dsgl_walk;
 -	unsigned int authsize = crypto_aead_authsize(tfm);
 -	u32 temp;
 -
 -	dsgl_walk_init(&dsgl_walk, phys_cpl);
 -	if (reqctx->b0_dma)
 -		dsgl_walk_add_page(&dsgl_walk, reqctx->b0_len, &reqctx->b0_dma);
 -	dsgl_walk_add_sg(&dsgl_walk, req->dst, assoclen, 0);
 -	dsgl_walk_add_page(&dsgl_walk, IV, &reqctx->iv_dma);
 -	temp = req->cryptlen + (op_type ? -authsize : authsize);
 -	dsgl_walk_add_sg(&dsgl_walk, req->dst, temp, req->assoclen);
 -	dsgl_walk_end(&dsgl_walk, qid);
 -}
 -
 -void chcr_add_cipher_src_ent(struct ablkcipher_request *req,
 -			     struct ulptx_sgl *ulptx,
 -			     struct  cipher_wr_param *wrparam)
 -{
 -	struct ulptx_walk ulp_walk;
 -	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
 -
 -	if (reqctx->imm) {
 -		u8 *buf = (u8 *)ulptx;
 -
 -		memcpy(buf, reqctx->iv, IV);
 -		buf += IV;
 -		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
 -				   buf, wrparam->bytes, reqctx->processed);
 -	} else {
 -		ulptx_walk_init(&ulp_walk, ulptx);
 -		ulptx_walk_add_page(&ulp_walk, IV, &reqctx->iv_dma);
 -		ulptx_walk_add_sg(&ulp_walk, reqctx->srcsg, wrparam->bytes,
 -				  reqctx->src_ofst);
 -		reqctx->srcsg = ulp_walk.last_sg;
 -		reqctx->src_ofst = ulp_walk.last_sg_len;
 -		ulptx_walk_end(&ulp_walk);
 -	}
 -}
 -
 -void chcr_add_cipher_dst_ent(struct ablkcipher_request *req,
 -			     struct cpl_rx_phys_dsgl *phys_cpl,
 -			     struct  cipher_wr_param *wrparam,
 -			     unsigned short qid)
 -{
 -	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
 -	struct dsgl_walk dsgl_walk;
 -
 -	dsgl_walk_init(&dsgl_walk, phys_cpl);
 -	dsgl_walk_add_page(&dsgl_walk, IV, &reqctx->iv_dma);
 -	dsgl_walk_add_sg(&dsgl_walk, reqctx->dstsg, wrparam->bytes,
 -			 reqctx->dst_ofst);
 -	reqctx->dstsg = dsgl_walk.last_sg;
 -	reqctx->dst_ofst = dsgl_walk.last_sg_len;
 -
 -	dsgl_walk_end(&dsgl_walk, qid);
 -}
 -
 -void chcr_add_hash_src_ent(struct ahash_request *req,
 -			   struct ulptx_sgl *ulptx,
 -			   struct hash_wr_param *param)
 -{
 -	struct ulptx_walk ulp_walk;
 -	struct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);
 -
 -	if (reqctx->imm) {
 -		u8 *buf = (u8 *)ulptx;
 -
 -		if (param->bfr_len) {
 -			memcpy(buf, reqctx->reqbfr, param->bfr_len);
 -			buf += param->bfr_len;
 -		}
 -		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
 -				   buf, param->sg_len, 0);
 -	} else {
 -		ulptx_walk_init(&ulp_walk, ulptx);
 -		if (param->bfr_len)
 -			ulptx_walk_add_page(&ulp_walk, param->bfr_len,
 -					    &reqctx->dma_addr);
 -		ulptx_walk_add_sg(&ulp_walk, req->src, param->sg_len,
 -				  0);
 -		ulptx_walk_end(&ulp_walk);
 -	}
 -}
 -
 -int chcr_hash_dma_map(struct device *dev,
 -		      struct ahash_request *req)
 -{
 -	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);
 -	int error = 0;
 -
 -	if (!req->nbytes)
 -		return 0;
 -	error = dma_map_sg(dev, req->src, sg_nents(req->src),
 -			   DMA_TO_DEVICE);
 -	if (!error)
 -		return -ENOMEM;
 -	req_ctx->is_sg_map = 1;
 -	return 0;
 -}
 -
 -void chcr_hash_dma_unmap(struct device *dev,
 -			 struct ahash_request *req)
 -{
 -	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);
 -
 -	if (!req->nbytes)
 -		return;
 -
 -	dma_unmap_sg(dev, req->src, sg_nents(req->src),
 -			   DMA_TO_DEVICE);
 -	req_ctx->is_sg_map = 0;
 -
 -}
 +	sg_param.nents = reqctx->dst_nents;
 +	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
 +	sg_param.qid = qid;
 +	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
 +					reqctx->dst, &sg_param);
 +	if (error)
 +		goto dstmap_fail;
  
 -int chcr_cipher_dma_map(struct device *dev,
 -			struct ablkcipher_request *req)
 -{
 -	int error;
 -	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
 +	skb_set_transport_header(skb, transhdr_len);
  
 -	reqctx->iv_dma = dma_map_single(dev, reqctx->iv, IV,
 -					DMA_BIDIRECTIONAL);
 -	if (dma_mapping_error(dev, reqctx->iv_dma))
 -		return -ENOMEM;
 +	if (assoclen) {
 +		/* AAD buffer in */
 +		write_sg_to_skb(skb, &frags, req->assoc, assoclen);
  
 -	if (req->src == req->dst) {
 -		error = dma_map_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_BIDIRECTIONAL);
 -		if (!error)
 -			goto err;
 -	} else {
 -		error = dma_map_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_TO_DEVICE);
 -		if (!error)
 -			goto err;
 -		error = dma_map_sg(dev, req->dst, sg_nents(req->dst),
 -				   DMA_FROM_DEVICE);
 -		if (!error) {
 -			dma_unmap_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_TO_DEVICE);
 -			goto err;
 -		}
  	}
 +	memcpy(reqctx->iv, req->iv, ivsize);
 +	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
 +	write_sg_to_skb(skb, &frags, req->src, req->cryptlen);
 +	atomic_inc(&adap->chcr_stats.cipher_rqst);
 +	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
 +		   sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
 +	reqctx->skb = skb;
 +	skb_get(skb);
  
 -	return 0;
 +	return skb;
 +dstmap_fail:
 +	/* ivmap_fail: */
 +	kfree_skb(skb);
  err:
 -	dma_unmap_single(dev, reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);
 -	return -ENOMEM;
 -}
 -
 -void chcr_cipher_dma_unmap(struct device *dev,
 -			   struct ablkcipher_request *req)
 -{
 -	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
 -
 -	dma_unmap_single(dev, reqctx->iv_dma, IV,
 -					DMA_BIDIRECTIONAL);
 -	if (req->src == req->dst) {
 -		dma_unmap_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_BIDIRECTIONAL);
 -	} else {
 -		dma_unmap_sg(dev, req->src, sg_nents(req->src),
 -				   DMA_TO_DEVICE);
 -		dma_unmap_sg(dev, req->dst, sg_nents(req->dst),
 -				   DMA_FROM_DEVICE);
 -	}
 +	free_new_sg(reqctx->newdstsg);
 +	reqctx->newdstsg = NULL;
 +	return ERR_PTR(error);
  }
  
  static int set_msg_len(u8 *block, unsigned int msglen, int csize)
@@@ -3382,132 -3794,258 +3450,259 @@@ static struct chcr_alg_template driver_
  		}
  	},
  	{
- 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC,
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,
  		.is_registered = 0,
 -		.alg.aead = {
 -			.base = {
 -				.cra_name = "authenc(hmac(sha1),cbc(aes))",
 -				.cra_driver_name =
 -					"authenc-hmac-sha1-cbc-aes-chcr",
 -				.cra_blocksize	 = AES_BLOCK_SIZE,
 -				.cra_priority = CHCR_AEAD_PRIORITY,
 -				.cra_ctxsize =	sizeof(struct chcr_context) +
 -						sizeof(struct chcr_aead_ctx) +
 -						sizeof(struct chcr_authenc_ctx),
 -
 -			},
 -			.ivsize = AES_BLOCK_SIZE,
 -			.maxauthsize = SHA1_DIGEST_SIZE,
 -			.setkey = chcr_authenc_setkey,
 -			.setauthsize = chcr_authenc_setauthsize,
 +		.alg.crypto = {
 +			.cra_name = "authenc(hmac(sha1),cbc(aes))",
 +			.cra_driver_name =
 +				"authenc-hmac-sha1-cbc-aes-chcr",
 +			.cra_blocksize	 = AES_BLOCK_SIZE,
 +			.cra_priority = CHCR_AEAD_PRIORITY,
 +			.cra_ctxsize =	sizeof(struct chcr_context) +
 +					sizeof(struct chcr_aead_ctx) +
 +					sizeof(struct chcr_authenc_ctx),
 +
 +			.cra_u.aead = {
 +				.ivsize = AES_BLOCK_SIZE,
 +				.maxauthsize = SHA1_DIGEST_SIZE,
 +				.setkey = chcr_authenc_setkey,
 +				.setauthsize = chcr_authenc_setauthsize,
 +			}
  		}
  	},
  	{
- 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC,
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,
  		.is_registered = 0,
 -		.alg.aead = {
 -			.base = {
 -
 -				.cra_name = "authenc(hmac(sha256),cbc(aes))",
 -				.cra_driver_name =
 -					"authenc-hmac-sha256-cbc-aes-chcr",
 -				.cra_blocksize	 = AES_BLOCK_SIZE,
 -				.cra_priority = CHCR_AEAD_PRIORITY,
 -				.cra_ctxsize =	sizeof(struct chcr_context) +
 -						sizeof(struct chcr_aead_ctx) +
 -						sizeof(struct chcr_authenc_ctx),
 +		.alg.crypto = {
  
 -			},
 -			.ivsize = AES_BLOCK_SIZE,
 -			.maxauthsize	= SHA256_DIGEST_SIZE,
 -			.setkey = chcr_authenc_setkey,
 -			.setauthsize = chcr_authenc_setauthsize,
 +			.cra_name = "authenc(hmac(sha256),cbc(aes))",
 +			.cra_driver_name =
 +				"authenc-hmac-sha256-cbc-aes-chcr",
 +			.cra_blocksize	 = AES_BLOCK_SIZE,
 +			.cra_priority = CHCR_AEAD_PRIORITY,
 +			.cra_ctxsize =	sizeof(struct chcr_context) +
 +					sizeof(struct chcr_aead_ctx) +
 +					sizeof(struct chcr_authenc_ctx),
 +
 +			.cra_u.aead = {
 +				.ivsize = AES_BLOCK_SIZE,
 +				.maxauthsize	= SHA256_DIGEST_SIZE,
 +				.setkey = chcr_authenc_setkey,
 +				.setauthsize = chcr_authenc_setauthsize,
 +			}
  		}
  	},
  	{
- 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC,
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,
  		.is_registered = 0,
 -		.alg.aead = {
 -			.base = {
 -				.cra_name = "authenc(hmac(sha224),cbc(aes))",
 -				.cra_driver_name =
 -					"authenc-hmac-sha224-cbc-aes-chcr",
 -				.cra_blocksize	 = AES_BLOCK_SIZE,
 -				.cra_priority = CHCR_AEAD_PRIORITY,
 -				.cra_ctxsize =	sizeof(struct chcr_context) +
 -						sizeof(struct chcr_aead_ctx) +
 -						sizeof(struct chcr_authenc_ctx),
 -			},
 -			.ivsize = AES_BLOCK_SIZE,
 -			.maxauthsize = SHA224_DIGEST_SIZE,
 -			.setkey = chcr_authenc_setkey,
 -			.setauthsize = chcr_authenc_setauthsize,
 +		.alg.crypto = {
 +			.cra_name = "authenc(hmac(sha224),cbc(aes))",
 +			.cra_driver_name =
 +				"authenc-hmac-sha224-cbc-aes-chcr",
 +			.cra_blocksize	 = AES_BLOCK_SIZE,
 +			.cra_priority = CHCR_AEAD_PRIORITY,
 +			.cra_ctxsize =	sizeof(struct chcr_context) +
 +					sizeof(struct chcr_aead_ctx) +
 +					sizeof(struct chcr_authenc_ctx),
 +
 +			.cra_u.aead = {
 +				.ivsize = AES_BLOCK_SIZE,
 +				.maxauthsize = SHA224_DIGEST_SIZE,
 +				.setkey = chcr_authenc_setkey,
 +				.setauthsize = chcr_authenc_setauthsize,
 +			}
  		}
  	},
  	{
- 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC,
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,
  		.is_registered = 0,
 -		.alg.aead = {
 -			.base = {
 -				.cra_name = "authenc(hmac(sha384),cbc(aes))",
 -				.cra_driver_name =
 -					"authenc-hmac-sha384-cbc-aes-chcr",
 -				.cra_blocksize	 = AES_BLOCK_SIZE,
 -				.cra_priority = CHCR_AEAD_PRIORITY,
 -				.cra_ctxsize =	sizeof(struct chcr_context) +
 -						sizeof(struct chcr_aead_ctx) +
 -						sizeof(struct chcr_authenc_ctx),
 -
 -			},
 -			.ivsize = AES_BLOCK_SIZE,
 -			.maxauthsize = SHA384_DIGEST_SIZE,
 -			.setkey = chcr_authenc_setkey,
 -			.setauthsize = chcr_authenc_setauthsize,
 +		.alg.crypto = {
 +			.cra_name = "authenc(hmac(sha384),cbc(aes))",
 +			.cra_driver_name =
 +				"authenc-hmac-sha384-cbc-aes-chcr",
 +			.cra_blocksize	 = AES_BLOCK_SIZE,
 +			.cra_priority = CHCR_AEAD_PRIORITY,
 +			.cra_ctxsize =	sizeof(struct chcr_context) +
 +					sizeof(struct chcr_aead_ctx) +
 +					sizeof(struct chcr_authenc_ctx),
 +
 +			.cra_u.aead = {
 +				.ivsize = AES_BLOCK_SIZE,
 +				.maxauthsize = SHA384_DIGEST_SIZE,
 +				.setkey = chcr_authenc_setkey,
 +				.setauthsize = chcr_authenc_setauthsize,
 +			}
  		}
  	},
  	{
- 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC,
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_SHA,
  		.is_registered = 0,
 -		.alg.aead = {
 -			.base = {
 -				.cra_name = "authenc(hmac(sha512),cbc(aes))",
 -				.cra_driver_name =
 -					"authenc-hmac-sha512-cbc-aes-chcr",
 -				.cra_blocksize	 = AES_BLOCK_SIZE,
 -				.cra_priority = CHCR_AEAD_PRIORITY,
 -				.cra_ctxsize =	sizeof(struct chcr_context) +
 -						sizeof(struct chcr_aead_ctx) +
 -						sizeof(struct chcr_authenc_ctx),
 -
 -			},
 -			.ivsize = AES_BLOCK_SIZE,
 -			.maxauthsize = SHA512_DIGEST_SIZE,
 -			.setkey = chcr_authenc_setkey,
 -			.setauthsize = chcr_authenc_setauthsize,
 +		.alg.crypto = {
 +			.cra_name = "authenc(hmac(sha512),cbc(aes))",
 +			.cra_driver_name =
 +				"authenc-hmac-sha512-cbc-aes-chcr",
 +			.cra_blocksize	 = AES_BLOCK_SIZE,
 +			.cra_priority = CHCR_AEAD_PRIORITY,
 +			.cra_ctxsize =	sizeof(struct chcr_context) +
 +					sizeof(struct chcr_aead_ctx) +
 +					sizeof(struct chcr_authenc_ctx),
 +
 +			.cra_u.aead = {
 +				.ivsize = AES_BLOCK_SIZE,
 +				.maxauthsize = SHA512_DIGEST_SIZE,
 +				.setkey = chcr_authenc_setkey,
 +				.setauthsize = chcr_authenc_setauthsize,
 +			}
  		}
  	},
  	{
- 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_AEAD_NULL,
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CBC_NULL,
  		.is_registered = 0,
 -		.alg.aead = {
 -			.base = {
 -				.cra_name = "authenc(digest_null,cbc(aes))",
 -				.cra_driver_name =
 -					"authenc-digest_null-cbc-aes-chcr",
 -				.cra_blocksize	 = AES_BLOCK_SIZE,
 -				.cra_priority = CHCR_AEAD_PRIORITY,
 -				.cra_ctxsize =	sizeof(struct chcr_context) +
 -						sizeof(struct chcr_aead_ctx) +
 -						sizeof(struct chcr_authenc_ctx),
 -
 -			},
 -			.ivsize  = AES_BLOCK_SIZE,
 -			.maxauthsize = 0,
 -			.setkey  = chcr_aead_digest_null_setkey,
 -			.setauthsize = chcr_authenc_null_setauthsize,
 +		.alg.crypto = {
 +			.cra_name = "authenc(digest_null,cbc(aes))",
 +			.cra_driver_name =
 +				"authenc-digest_null-cbc-aes-chcr",
 +			.cra_blocksize	 = AES_BLOCK_SIZE,
 +			.cra_priority = CHCR_AEAD_PRIORITY,
 +			.cra_ctxsize =	sizeof(struct chcr_context) +
 +					sizeof(struct chcr_aead_ctx) +
 +					sizeof(struct chcr_authenc_ctx),
 +
 +			.cra_u.aead = {
 +				.ivsize  = AES_BLOCK_SIZE,
 +				.maxauthsize = 0,
 +				.setkey  = chcr_aead_digest_null_setkey,
 +				.setauthsize = chcr_authenc_null_setauthsize,
 +			}
  		}
  	},
+ 	{
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,
+ 		.is_registered = 0,
+ 		.alg.aead = {
+ 			.base = {
+ 				.cra_name = "authenc(hmac(sha1),rfc3686(ctr(aes)))",
+ 				.cra_driver_name =
+ 				"authenc-hmac-sha1-rfc3686-ctr-aes-chcr",
+ 				.cra_blocksize	 = 1,
+ 				.cra_priority = CHCR_AEAD_PRIORITY,
+ 				.cra_ctxsize =	sizeof(struct chcr_context) +
+ 						sizeof(struct chcr_aead_ctx) +
+ 						sizeof(struct chcr_authenc_ctx),
+ 
+ 			},
+ 			.ivsize = CTR_RFC3686_IV_SIZE,
+ 			.maxauthsize = SHA1_DIGEST_SIZE,
+ 			.setkey = chcr_authenc_setkey,
+ 			.setauthsize = chcr_authenc_setauthsize,
+ 		}
+ 	},
+ 	{
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,
+ 		.is_registered = 0,
+ 		.alg.aead = {
+ 			.base = {
+ 
+ 				.cra_name = "authenc(hmac(sha256),rfc3686(ctr(aes)))",
+ 				.cra_driver_name =
+ 				"authenc-hmac-sha256-rfc3686-ctr-aes-chcr",
+ 				.cra_blocksize	 = 1,
+ 				.cra_priority = CHCR_AEAD_PRIORITY,
+ 				.cra_ctxsize =	sizeof(struct chcr_context) +
+ 						sizeof(struct chcr_aead_ctx) +
+ 						sizeof(struct chcr_authenc_ctx),
+ 
+ 			},
+ 			.ivsize = CTR_RFC3686_IV_SIZE,
+ 			.maxauthsize	= SHA256_DIGEST_SIZE,
+ 			.setkey = chcr_authenc_setkey,
+ 			.setauthsize = chcr_authenc_setauthsize,
+ 		}
+ 	},
+ 	{
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,
+ 		.is_registered = 0,
+ 		.alg.aead = {
+ 			.base = {
+ 				.cra_name = "authenc(hmac(sha224),rfc3686(ctr(aes)))",
+ 				.cra_driver_name =
+ 				"authenc-hmac-sha224-rfc3686-ctr-aes-chcr",
+ 				.cra_blocksize	 = 1,
+ 				.cra_priority = CHCR_AEAD_PRIORITY,
+ 				.cra_ctxsize =	sizeof(struct chcr_context) +
+ 						sizeof(struct chcr_aead_ctx) +
+ 						sizeof(struct chcr_authenc_ctx),
+ 			},
+ 			.ivsize = CTR_RFC3686_IV_SIZE,
+ 			.maxauthsize = SHA224_DIGEST_SIZE,
+ 			.setkey = chcr_authenc_setkey,
+ 			.setauthsize = chcr_authenc_setauthsize,
+ 		}
+ 	},
+ 	{
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,
+ 		.is_registered = 0,
+ 		.alg.aead = {
+ 			.base = {
+ 				.cra_name = "authenc(hmac(sha384),rfc3686(ctr(aes)))",
+ 				.cra_driver_name =
+ 				"authenc-hmac-sha384-rfc3686-ctr-aes-chcr",
+ 				.cra_blocksize	 = 1,
+ 				.cra_priority = CHCR_AEAD_PRIORITY,
+ 				.cra_ctxsize =	sizeof(struct chcr_context) +
+ 						sizeof(struct chcr_aead_ctx) +
+ 						sizeof(struct chcr_authenc_ctx),
+ 
+ 			},
+ 			.ivsize = CTR_RFC3686_IV_SIZE,
+ 			.maxauthsize = SHA384_DIGEST_SIZE,
+ 			.setkey = chcr_authenc_setkey,
+ 			.setauthsize = chcr_authenc_setauthsize,
+ 		}
+ 	},
+ 	{
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_SHA,
+ 		.is_registered = 0,
+ 		.alg.aead = {
+ 			.base = {
+ 				.cra_name = "authenc(hmac(sha512),rfc3686(ctr(aes)))",
+ 				.cra_driver_name =
+ 				"authenc-hmac-sha512-rfc3686-ctr-aes-chcr",
+ 				.cra_blocksize	 = 1,
+ 				.cra_priority = CHCR_AEAD_PRIORITY,
+ 				.cra_ctxsize =	sizeof(struct chcr_context) +
+ 						sizeof(struct chcr_aead_ctx) +
+ 						sizeof(struct chcr_authenc_ctx),
+ 
+ 			},
+ 			.ivsize = CTR_RFC3686_IV_SIZE,
+ 			.maxauthsize = SHA512_DIGEST_SIZE,
+ 			.setkey = chcr_authenc_setkey,
+ 			.setauthsize = chcr_authenc_setauthsize,
+ 		}
+ 	},
+ 	{
+ 		.type = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_SUB_TYPE_CTR_NULL,
+ 		.is_registered = 0,
+ 		.alg.aead = {
+ 			.base = {
+ 				.cra_name = "authenc(digest_null,rfc3686(ctr(aes)))",
+ 				.cra_driver_name =
+ 				"authenc-digest_null-rfc3686-ctr-aes-chcr",
+ 				.cra_blocksize	 = 1,
+ 				.cra_priority = CHCR_AEAD_PRIORITY,
+ 				.cra_ctxsize =	sizeof(struct chcr_context) +
+ 						sizeof(struct chcr_aead_ctx) +
+ 						sizeof(struct chcr_authenc_ctx),
+ 
+ 			},
+ 			.ivsize  = CTR_RFC3686_IV_SIZE,
+ 			.maxauthsize = 0,
+ 			.setkey  = chcr_aead_digest_null_setkey,
+ 			.setauthsize = chcr_authenc_null_setauthsize,
+ 		}
+ 	},
+ 
  };
  
  /*
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
diff --git a/drivers/crypto/chelsio/chcr_crypto.h b/drivers/crypto/chelsio/chcr_crypto.h
index ee637a7f2f30..40bb7fa18ab1 100644
--- a/drivers/crypto/chelsio/chcr_crypto.h
+++ b/drivers/crypto/chelsio/chcr_crypto.h
@@ -134,14 +134,16 @@
 #define CRYPTO_ALG_SUB_TYPE_HASH_HMAC       0x01000000
 #define CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106    0x02000000
 #define CRYPTO_ALG_SUB_TYPE_AEAD_GCM	    0x03000000
-#define CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC    0x04000000
+#define CRYPTO_ALG_SUB_TYPE_CBC_SHA	    0x04000000
 #define CRYPTO_ALG_SUB_TYPE_AEAD_CCM        0x05000000
 #define CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309    0x06000000
-#define CRYPTO_ALG_SUB_TYPE_AEAD_NULL       0x07000000
+#define CRYPTO_ALG_SUB_TYPE_CBC_NULL	    0x07000000
 #define CRYPTO_ALG_SUB_TYPE_CTR             0x08000000
 #define CRYPTO_ALG_SUB_TYPE_CTR_RFC3686     0x09000000
 #define CRYPTO_ALG_SUB_TYPE_XTS		    0x0a000000
 #define CRYPTO_ALG_SUB_TYPE_CBC		    0x0b000000
+#define CRYPTO_ALG_SUB_TYPE_CTR_SHA	    0x0c000000
+#define CRYPTO_ALG_SUB_TYPE_CTR_NULL   0x0d000000
 #define CRYPTO_ALG_TYPE_HMAC (CRYPTO_ALG_TYPE_AHASH |\
 			      CRYPTO_ALG_SUB_TYPE_HASH_HMAC)
 
@@ -196,6 +198,7 @@ struct chcr_aead_ctx {
 	struct crypto_aead *sw_cipher;
 	u8 salt[MAX_SALT];
 	u8 key[CHCR_AES_MAX_KEY_LEN];
+	u8 nonce[4];
 	u16 hmac_ctrl;
 	u16 mayverify;
 	struct	__aead_ctx ctx[0];
