mm: pass the vmem_altmap to vmemmap_free

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] pass the vmem_altmap to vmemmap_free (Jeff Moyer) [1505291]
Rebuild_FUZZ: 94.74%
commit-author Christoph Hellwig <hch@lst.de>
commit 24b6d4164348370c6b6a58b4248babd85ff9e982
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/24b6d416.failed

We can just pass this on instead of having to do a radix tree lookup
without proper locking a few levels into the callchain.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 24b6d4164348370c6b6a58b4248babd85ff9e982)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/mm/init_64.c
#	arch/sparc/mm/init_64.c
#	arch/x86/mm/init_64.c
#	mm/sparse.c
diff --cc arch/powerpc/mm/init_64.c
index bc11f9ce7444,db7d4e092157..000000000000
--- a/arch/powerpc/mm/init_64.c
+++ b/arch/powerpc/mm/init_64.c
@@@ -363,16 -254,20 +363,23 @@@ static unsigned long vmemmap_list_free(
  	return vmem_back->phys;
  }
  
- void __ref vmemmap_free(unsigned long start, unsigned long end)
+ void __ref vmemmap_free(unsigned long start, unsigned long end,
+ 		struct vmem_altmap *altmap)
  {
  	unsigned long page_size = 1 << mmu_psize_defs[mmu_vmemmap_psize].shift;
 -	unsigned long page_order = get_order(page_size);
  
  	start = _ALIGN_DOWN(start, page_size);
  
  	pr_debug("vmemmap_free %lx...%lx\n", start, end);
  
  	for (; start < end; start += page_size) {
++<<<<<<< HEAD
 +		unsigned long addr;
++=======
+ 		unsigned long nr_pages, addr;
+ 		struct page *section_base;
+ 		struct page *page;
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  
  		/*
  		 * the section has already be marked as invalid, so
@@@ -383,29 -278,32 +390,48 @@@
  			continue;
  
  		addr = vmemmap_list_free(start);
 -		if (!addr)
 -			continue;
 -
 -		page = pfn_to_page(addr >> PAGE_SHIFT);
 -		section_base = pfn_to_page(vmemmap_section_start(start));
 -		nr_pages = 1 << page_order;
 -
 +		if (addr) {
 +			struct page *page = pfn_to_page(addr >> PAGE_SHIFT);
 +
 +			if (PageReserved(page)) {
 +				/* allocated from bootmem */
 +				if (page_size < PAGE_SIZE) {
 +					/*
 +					 * this shouldn't happen, but if it is
 +					 * the case, leave the memory there
 +					 */
 +					WARN_ON_ONCE(1);
 +				} else {
 +					unsigned int nr_pages =
 +						1 << get_order(page_size);
 +					while (nr_pages--)
 +						free_reserved_page(page++);
 +				}
 +			} else
 +				free_pages((unsigned long)(__va(addr)),
 +							get_order(page_size));
 +
++<<<<<<< HEAD
 +			vmemmap_remove_mapping(start, page_size);
++=======
+ 		if (altmap) {
+ 			vmem_altmap_free(altmap, nr_pages);
+ 		} else if (PageReserved(page)) {
+ 			/* allocated from bootmem */
+ 			if (page_size < PAGE_SIZE) {
+ 				/*
+ 				 * this shouldn't happen, but if it is
+ 				 * the case, leave the memory there
+ 				 */
+ 				WARN_ON_ONCE(1);
+ 			} else {
+ 				while (nr_pages--)
+ 					free_reserved_page(page++);
+ 			}
+ 		} else {
+ 			free_pages((unsigned long)(__va(addr)), page_order);
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  		}
 -
 -		vmemmap_remove_mapping(start, page_size);
  	}
  }
  #endif
diff --cc arch/sparc/mm/init_64.c
index 1e1aed064009,995f9490334d..000000000000
--- a/arch/sparc/mm/init_64.c
+++ b/arch/sparc/mm/init_64.c
@@@ -2213,21 -2670,11 +2213,26 @@@ int __meminit vmemmap_populate(unsigne
  	return 0;
  }
  
++<<<<<<< HEAD
 +void __meminit vmemmap_populate_print_last(void)
 +{
 +	if (addr_start) {
 +		printk(KERN_DEBUG " [%lx-%lx] on node %d\n",
 +		       addr_start, addr_end-1, node_start);
 +		addr_start = 0;
 +		addr_end = 0;
 +		node_start = 0;
 +	}
 +}
 +
 +void vmemmap_free(unsigned long start, unsigned long end)
++=======
+ void vmemmap_free(unsigned long start, unsigned long end,
+ 		struct vmem_altmap *altmap)
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  {
  }
 +
  #endif /* CONFIG_SPARSEMEM_VMEMMAP */
  
  static void prot_init_common(unsigned long page_none,
diff --cc arch/x86/mm/init_64.c
index 25c65b6af83e,0cab4b5b59ba..000000000000
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@@ -759,9 -864,28 +761,31 @@@ static void __meminit free_pmd_table(pm
  	spin_unlock(&init_mm.page_table_lock);
  }
  
++<<<<<<< HEAD
++=======
+ static void __meminit free_pud_table(pud_t *pud_start, p4d_t *p4d,
+ 		struct vmem_altmap *altmap)
+ {
+ 	pud_t *pud;
+ 	int i;
+ 
+ 	for (i = 0; i < PTRS_PER_PUD; i++) {
+ 		pud = pud_start + i;
+ 		if (!pud_none(*pud))
+ 			return;
+ 	}
+ 
+ 	/* free a pud talbe */
+ 	free_pagetable(p4d_page(*p4d), 0, altmap);
+ 	spin_lock(&init_mm.page_table_lock);
+ 	p4d_clear(p4d);
+ 	spin_unlock(&init_mm.page_table_lock);
+ }
+ 
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  static void __meminit
  remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,
- 		 bool direct)
+ 		 struct vmem_altmap *altmap, bool direct)
  {
  	unsigned long next, pages = 0;
  	pte_t *pte;
@@@ -933,18 -1060,51 +961,60 @@@ remove_pud_table(pud_t *pud_start, unsi
  			continue;
  		}
  
++<<<<<<< HEAD
 +		pmd_base = (pmd_t *)pud_page_vaddr(*pud);
 +		remove_pmd_table(pmd_base, addr, next, direct);
 +		free_pmd_table(pmd_base, pud);
++=======
+ 		pmd_base = pmd_offset(pud, 0);
+ 		remove_pmd_table(pmd_base, addr, next, direct, altmap);
+ 		free_pmd_table(pmd_base, pud, altmap);
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  	}
  
  	if (direct)
  		update_page_count(PG_LEVEL_1G, -pages);
  }
  
++<<<<<<< HEAD
++=======
+ static void __meminit
+ remove_p4d_table(p4d_t *p4d_start, unsigned long addr, unsigned long end,
+ 		 struct vmem_altmap *altmap, bool direct)
+ {
+ 	unsigned long next, pages = 0;
+ 	pud_t *pud_base;
+ 	p4d_t *p4d;
+ 
+ 	p4d = p4d_start + p4d_index(addr);
+ 	for (; addr < end; addr = next, p4d++) {
+ 		next = p4d_addr_end(addr, end);
+ 
+ 		if (!p4d_present(*p4d))
+ 			continue;
+ 
+ 		BUILD_BUG_ON(p4d_large(*p4d));
+ 
+ 		pud_base = pud_offset(p4d, 0);
+ 		remove_pud_table(pud_base, addr, next, altmap, direct);
+ 		/*
+ 		 * For 4-level page tables we do not want to free PUDs, but in the
+ 		 * 5-level case we should free them. This code will have to change
+ 		 * to adapt for boot-time switching between 4 and 5 level page tables.
+ 		 */
+ 		if (CONFIG_PGTABLE_LEVELS == 5)
+ 			free_pud_table(pud_base, p4d, altmap);
+ 	}
+ 
+ 	if (direct)
+ 		update_page_count(PG_LEVEL_512G, -pages);
+ }
+ 
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  /* start and end are both virtual address. */
  static void __meminit
- remove_pagetable(unsigned long start, unsigned long end, bool direct)
+ remove_pagetable(unsigned long start, unsigned long end, bool direct,
+ 		struct vmem_altmap *altmap)
  {
  	unsigned long next;
  	unsigned long addr;
@@@ -958,8 -1118,8 +1028,13 @@@
  		if (!pgd_present(*pgd))
  			continue;
  
++<<<<<<< HEAD
 +		pud = (pud_t *)pgd_page_vaddr(*pgd);
 +		remove_pud_table(pud, addr, next, direct);
++=======
+ 		p4d = p4d_offset(pgd, 0);
+ 		remove_p4d_table(p4d, addr, next, altmap, direct);
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  	}
  
  	flush_tlb_all();
@@@ -977,10 -1138,10 +1053,10 @@@ kernel_physical_mapping_remove(unsigne
  	start = (unsigned long)__va(start);
  	end = (unsigned long)__va(end);
  
- 	remove_pagetable(start, end, true);
+ 	remove_pagetable(start, end, true, NULL);
  }
  
 -int __ref arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap)
 +int __ref arch_remove_memory(u64 start, u64 size)
  {
  	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
diff --cc mm/sparse.c
index 443cfc7e80e7,06130c13dc99..000000000000
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@@ -609,13 -636,57 +609,14 @@@ void __init sparse_init(void
  }
  
  #ifdef CONFIG_MEMORY_HOTPLUG
 -
 -/* Mark all memory sections within the pfn range as online */
 -void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn)
 -{
 -	unsigned long pfn;
 -
 -	for (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {
 -		unsigned long section_nr = pfn_to_section_nr(pfn);
 -		struct mem_section *ms;
 -
 -		/* onlining code should never touch invalid ranges */
 -		if (WARN_ON(!valid_section_nr(section_nr)))
 -			continue;
 -
 -		ms = __nr_to_section(section_nr);
 -		ms->section_mem_map |= SECTION_IS_ONLINE;
 -	}
 -}
 -
 -#ifdef CONFIG_MEMORY_HOTREMOVE
 -/* Mark all memory sections within the pfn range as online */
 -void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn)
 -{
 -	unsigned long pfn;
 -
 -	for (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {
 -		unsigned long section_nr = pfn_to_section_nr(start_pfn);
 -		struct mem_section *ms;
 -
 -		/*
 -		 * TODO this needs some double checking. Offlining code makes
 -		 * sure to check pfn_valid but those checks might be just bogus
 -		 */
 -		if (WARN_ON(!valid_section_nr(section_nr)))
 -			continue;
 -
 -		ms = __nr_to_section(section_nr);
 -		ms->section_mem_map &= ~SECTION_IS_ONLINE;
 -	}
 -}
 -#endif
 -
  #ifdef CONFIG_SPARSEMEM_VMEMMAP
 -static inline struct page *kmalloc_section_memmap(unsigned long pnum, int nid,
 -		struct vmem_altmap *altmap)
 +static inline struct page *kmalloc_section_memmap(unsigned long pnum, int nid)
  {
  	/* This will make the necessary allocations eventually. */
 -	return sparse_mem_map_populate(pnum, nid, altmap);
 +	return sparse_mem_map_populate(pnum, nid);
  }
- static void __kfree_section_memmap(struct page *memmap)
+ static void __kfree_section_memmap(struct page *memmap,
+ 		struct vmem_altmap *altmap)
  {
  	unsigned long start = (unsigned long)memmap;
  	unsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);
@@@ -775,8 -849,8 +777,13 @@@ static inline void clear_hwpoisoned_pag
  }
  #endif
  
++<<<<<<< HEAD
 +#ifdef CONFIG_MEMORY_HOTREMOVE
 +static void free_section_usemap(struct page *memmap, unsigned long *usemap)
++=======
+ static void free_section_usemap(struct page *memmap, unsigned long *usemap,
+ 		struct vmem_altmap *altmap)
++>>>>>>> 24b6d4164348 (mm: pass the vmem_altmap to vmemmap_free)
  {
  	struct page *usemap_page;
  
diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
index eeecc9c8ed68..12a32f65f042 100644
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@ -431,7 +431,8 @@ int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)
 	return 0;
 }
 #endif	/* CONFIG_ARM64_64K_PAGES */
-void vmemmap_free(unsigned long start, unsigned long end)
+void vmemmap_free(unsigned long start, unsigned long end,
+		struct vmem_altmap *altmap)
 {
 }
 #endif	/* CONFIG_SPARSEMEM_VMEMMAP */
diff --git a/arch/ia64/mm/discontig.c b/arch/ia64/mm/discontig.c
index ae4db4bd6d97..2fa34caf47cb 100644
--- a/arch/ia64/mm/discontig.c
+++ b/arch/ia64/mm/discontig.c
@@ -824,7 +824,8 @@ int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)
 	return vmemmap_populate_basepages(start, end, node);
 }
 
-void vmemmap_free(unsigned long start, unsigned long end)
+void vmemmap_free(unsigned long start, unsigned long end,
+		struct vmem_altmap *altmap)
 {
 }
 #endif
* Unmerged path arch/powerpc/mm/init_64.c
diff --git a/arch/s390/mm/vmem.c b/arch/s390/mm/vmem.c
index c0b7a029c72a..f8a0a02b2761 100644
--- a/arch/s390/mm/vmem.c
+++ b/arch/s390/mm/vmem.c
@@ -281,7 +281,8 @@ out:
 	return ret;
 }
 
-void vmemmap_free(unsigned long start, unsigned long end)
+void vmemmap_free(unsigned long start, unsigned long end,
+		struct vmem_altmap *altmap)
 {
 }
 
* Unmerged path arch/sparc/mm/init_64.c
* Unmerged path arch/x86/mm/init_64.c
diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index d0cd6c1954bf..39ef2075ea50 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -272,7 +272,7 @@ extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
-		unsigned long map_offset);
+		unsigned long map_offset, struct vmem_altmap *altmap);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
 int add_pages(int nid, unsigned long start,
diff --git a/include/linux/mm.h b/include/linux/mm.h
index a228f1a787bf..41373cb4dd23 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2381,7 +2381,8 @@ int vmemmap_populate_basepages(unsigned long start, unsigned long end,
 int vmemmap_populate(unsigned long start, unsigned long end, int node);
 void vmemmap_populate_print_last(void);
 #ifdef CONFIG_MEMORY_HOTPLUG
-void vmemmap_free(unsigned long start, unsigned long end);
+void vmemmap_free(unsigned long start, unsigned long end,
+		struct vmem_altmap *altmap);
 #endif
 void register_page_bootmem_memmap(unsigned long section_nr, struct page *map,
 				  unsigned long size);
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index 4ad6525f2bd5..80594a20062d 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -765,7 +765,7 @@ static void __remove_zone(struct zone *zone, unsigned long start_pfn)
 }
 
 static int __remove_section(struct zone *zone, struct mem_section *ms,
-		unsigned long map_offset)
+		unsigned long map_offset, struct vmem_altmap *altmap)
 {
 	unsigned long start_pfn;
 	int scn_nr;
@@ -782,7 +782,7 @@ static int __remove_section(struct zone *zone, struct mem_section *ms,
 	start_pfn = section_nr_to_pfn((unsigned long)scn_nr);
 	__remove_zone(zone, start_pfn);
 
-	sparse_remove_one_section(zone, ms, map_offset);
+	sparse_remove_one_section(zone, ms, map_offset, altmap);
 	return 0;
 }
 
@@ -838,7 +838,8 @@ int __remove_pages(struct zone *zone, unsigned long phys_start_pfn,
 	for (i = 0; i < sections_to_remove; i++) {
 		unsigned long pfn = phys_start_pfn + i*PAGES_PER_SECTION;
 
-		ret = __remove_section(zone, __pfn_to_section(pfn), map_offset);
+		ret = __remove_section(zone, __pfn_to_section(pfn), map_offset,
+				altmap);
 		map_offset = 0;
 		if (ret)
 			break;
* Unmerged path mm/sparse.c
