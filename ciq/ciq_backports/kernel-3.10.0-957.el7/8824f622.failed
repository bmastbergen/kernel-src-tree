blk-mq: fail the request in case issue failure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ming Lei <ming.lei@redhat.com>
commit 8824f62246bef288173a6624a363352f0d4d3b09
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/8824f622.failed

Inside blk_mq_try_issue_list_directly(), if the request is issued as
failed, we shouldn't try to do it again, otherwise the warning in
blk_mq_start_request() will be triggered. This change is aligned to
behaviour of other ways of request issue & dispatch.

Fixes: 6ce3dd6eec1 ("blk-mq: issue directly if hw queue isn't busy in case of 'none'")
	Cc: Kashyap Desai <kashyap.desai@broadcom.com>
	Cc: Laurence Oberman <loberman@redhat.com>
	Cc: Omar Sandoval <osandov@fb.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Bart Van Assche <bart.vanassche@wdc.com>
	Cc: Hannes Reinecke <hare@suse.de>
	Cc: Kashyap Desai <kashyap.desai@broadcom.com>
	Cc: kernel test robot <rong.a.chen@intel.com>
	Cc: LKP <lkp@01.org>
	Reported-by: kernel test robot <rong.a.chen@intel.com>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 8824f62246bef288173a6624a363352f0d4d3b09)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index a8e551c0c631,e13bdc2707ce..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1577,20 -1750,57 +1577,67 @@@ insert
  }
  
  static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 -		struct request *rq, blk_qc_t *cookie)
 +				      struct request *rq)
  {
 -	blk_status_t ret;
 -	int srcu_idx;
 +	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 +		rcu_read_lock();
 +		__blk_mq_try_issue_directly(hctx, rq, false);
 +		rcu_read_unlock();
 +	} else {
 +		unsigned int srcu_idx;
  
 -	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 +		might_sleep();
  
++<<<<<<< HEAD
 +		srcu_idx = srcu_read_lock(&hctx->queue_rq_srcu);
 +		__blk_mq_try_issue_directly(hctx, rq, true);
 +		srcu_read_unlock(&hctx->queue_rq_srcu, srcu_idx);
++=======
+ 	hctx_lock(hctx, &srcu_idx);
+ 
+ 	ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false);
+ 	if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE)
+ 		blk_mq_sched_insert_request(rq, false, true, false);
+ 	else if (ret != BLK_STS_OK)
+ 		blk_mq_end_request(rq, ret);
+ 
+ 	hctx_unlock(hctx, srcu_idx);
+ }
+ 
+ blk_status_t blk_mq_request_issue_directly(struct request *rq)
+ {
+ 	blk_status_t ret;
+ 	int srcu_idx;
+ 	blk_qc_t unused_cookie;
+ 	struct blk_mq_ctx *ctx = rq->mq_ctx;
+ 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(rq->q, ctx->cpu);
+ 
+ 	hctx_lock(hctx, &srcu_idx);
+ 	ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true);
+ 	hctx_unlock(hctx, srcu_idx);
+ 
+ 	return ret;
+ }
+ 
+ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
+ 		struct list_head *list)
+ {
+ 	while (!list_empty(list)) {
+ 		blk_status_t ret;
+ 		struct request *rq = list_first_entry(list, struct request,
+ 				queuelist);
+ 
+ 		list_del_init(&rq->queuelist);
+ 		ret = blk_mq_request_issue_directly(rq);
+ 		if (ret != BLK_STS_OK) {
+ 			if (ret == BLK_STS_RESOURCE ||
+ 					ret == BLK_STS_DEV_RESOURCE) {
+ 				list_add(&rq->queuelist, list);
+ 				break;
+ 			}
+ 			blk_mq_end_request(rq, ret);
+ 		}
++>>>>>>> 8824f62246be (blk-mq: fail the request in case issue failure)
  	}
  }
  
* Unmerged path block/blk-mq.c
