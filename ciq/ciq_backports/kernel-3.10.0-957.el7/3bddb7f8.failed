md/r5cache: handle FLUSH and FUA

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] r5cache: handle FLUSH and FUA (Nigel Croxon) [1494474]
Rebuild_FUZZ: 95.08%
commit-author Song Liu <songliubraving@fb.com>
commit 3bddb7f8f264ec58dc86e11ca97341c24f9d38f6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3bddb7f8.failed

With raid5 cache, we committing data from journal device. When
there is flush request, we need to flush journal device's cache.
This was not needed in raid5 journal, because we will flush the
journal before committing data to raid disks.

This is similar to FUA, except that we also need flush journal for
FUA. Otherwise, corruptions in earlier meta data will stop recovery
from reaching FUA data.

slightly changed the code by Shaohua

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 3bddb7f8f264ec58dc86e11ca97341c24f9d38f6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5-cache.c
diff --cc drivers/md/raid5-cache.c
index 04e5f78a9fc3,8cb79fc0eed9..000000000000
--- a/drivers/md/raid5-cache.c
+++ b/drivers/md/raid5-cache.c
@@@ -149,8 -160,8 +149,13 @@@ struct r5l_log 
  	spinlock_t stripe_in_journal_lock;
  	atomic_t stripe_in_journal_count;
  
++<<<<<<< HEAD
 +	/* to disable write back during in degraded mode */
 +	struct work_struct disable_writeback_work;
++=======
+ 	/* to submit async io_units, to fulfill ordering of flush */
+ 	struct work_struct deferred_io_work;
++>>>>>>> 3bddb7f8f264 (md/r5cache: handle FLUSH and FUA)
  };
  
  /*
@@@ -178,6 -187,19 +183,22 @@@ struct r5l_io_unit 
  	struct list_head stripe_list; /* stripes added to the io_unit */
  
  	int state;
++<<<<<<< HEAD
++=======
+ 	bool need_split_bio;
+ 	struct bio *split_bio;
+ 
+ 	unsigned int has_flush:1;      /* include flush request */
+ 	unsigned int has_fua:1;        /* include fua request */
+ 	unsigned int has_null_flush:1; /* include empty flush request */
+ 	/*
+ 	 * io isn't sent yet, flush/fua request can only be submitted till it's
+ 	 * the first IO in running_ios list
+ 	 */
+ 	unsigned int io_deferred:1;
+ 
+ 	struct bio_list flush_barriers;   /* size == 0 flush bios */
++>>>>>>> 3bddb7f8f264 (md/r5cache: handle FLUSH and FUA)
  };
  
  /* r5l_io_unit state */
@@@ -500,9 -509,11 +521,15 @@@ static void r5l_move_to_end_ios(struct 
  	}
  }
  
++<<<<<<< HEAD
 +static void r5l_log_endio(struct bio *bio, int error)
++=======
+ static void __r5l_stripe_write_finished(struct r5l_io_unit *io);
+ static void r5l_log_endio(struct bio *bio)
++>>>>>>> 3bddb7f8f264 (md/r5cache: handle FLUSH and FUA)
  {
  	struct r5l_io_unit *io = bio->bi_private;
+ 	struct r5l_io_unit *io_deferred;
  	struct r5l_log *log = io->log;
  	unsigned long flags;
  
@@@ -524,41 -544,74 +562,101 @@@
  
  	if (log->need_cache_flush)
  		md_wakeup_thread(log->rdev->mddev->thread);
+ 
+ 	if (io->has_null_flush) {
+ 		struct bio *bi;
+ 
+ 		WARN_ON(bio_list_empty(&io->flush_barriers));
+ 		while ((bi = bio_list_pop(&io->flush_barriers)) != NULL) {
+ 			bio_endio(bi);
+ 			atomic_dec(&io->pending_stripe);
+ 		}
+ 		if (atomic_read(&io->pending_stripe) == 0)
+ 			__r5l_stripe_write_finished(io);
+ 	}
+ }
+ 
+ static void r5l_do_submit_io(struct r5l_log *log, struct r5l_io_unit *io)
+ {
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&log->io_list_lock, flags);
+ 	__r5l_set_io_unit_state(io, IO_UNIT_IO_START);
+ 	spin_unlock_irqrestore(&log->io_list_lock, flags);
+ 
+ 	if (io->has_flush)
+ 		bio_set_op_attrs(io->current_bio, REQ_OP_WRITE, WRITE_FLUSH);
+ 	if (io->has_fua)
+ 		bio_set_op_attrs(io->current_bio, REQ_OP_WRITE, WRITE_FUA);
+ 	submit_bio(io->current_bio);
+ 
+ 	if (!io->split_bio)
+ 		return;
+ 
+ 	if (io->has_flush)
+ 		bio_set_op_attrs(io->split_bio, REQ_OP_WRITE, WRITE_FLUSH);
+ 	if (io->has_fua)
+ 		bio_set_op_attrs(io->split_bio, REQ_OP_WRITE, WRITE_FUA);
+ 	submit_bio(io->split_bio);
+ }
+ 
+ /* deferred io_unit will be dispatched here */
+ static void r5l_submit_io_async(struct work_struct *work)
+ {
+ 	struct r5l_log *log = container_of(work, struct r5l_log,
+ 					   deferred_io_work);
+ 	struct r5l_io_unit *io = NULL;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&log->io_list_lock, flags);
+ 	if (!list_empty(&log->running_ios)) {
+ 		io = list_first_entry(&log->running_ios, struct r5l_io_unit,
+ 				      log_sibling);
+ 		if (!io->io_deferred)
+ 			io = NULL;
+ 		else
+ 			io->io_deferred = 0;
+ 	}
+ 	spin_unlock_irqrestore(&log->io_list_lock, flags);
+ 	if (io)
+ 		r5l_do_submit_io(log, io);
  }
  
 +static void r5c_disable_writeback_async(struct work_struct *work)
 +{
 +	struct r5l_log *log = container_of(work, struct r5l_log,
 +					   disable_writeback_work);
 +	struct mddev *mddev = log->rdev->mddev;
 +	struct r5conf *conf = mddev->private;
 +	int locked = 0;
 +
 +	if (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)
 +		return;
 +	pr_info("md/raid:%s: Disabling writeback cache for degraded array.\n",
 +		mdname(mddev));
 +
 +	/* wait superblock change before suspend */
 +	wait_event(mddev->sb_wait,
 +		   conf->log == NULL ||
 +		   (!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags) &&
 +		    (locked = mddev_trylock(mddev))));
 +	if (locked) {
 +		mddev_suspend(mddev);
 +		log->r5c_journal_mode = R5C_JOURNAL_MODE_WRITE_THROUGH;
 +		mddev_resume(mddev);
 +		mddev_unlock(mddev);
 +	}
 +}
 +
  static void r5l_submit_current_io(struct r5l_log *log)
  {
  	struct r5l_io_unit *io = log->current_io;
+ 	struct bio *bio;
  	struct r5l_meta_block *block;
 +	struct bio *bio;
  	unsigned long flags;
  	u32 crc;
+ 	bool do_submit = true;
  
  	if (!io)
  		return;
@@@ -570,14 -624,19 +669,25 @@@
  
  	log->current_io = NULL;
  	spin_lock_irqsave(&log->io_list_lock, flags);
- 	__r5l_set_io_unit_state(io, IO_UNIT_IO_START);
+ 	if (io->has_flush || io->has_fua) {
+ 		if (io != list_first_entry(&log->running_ios,
+ 					   struct r5l_io_unit, log_sibling)) {
+ 			io->io_deferred = 1;
+ 			do_submit = false;
+ 		}
+ 	}
  	spin_unlock_irqrestore(&log->io_list_lock, flags);
++<<<<<<< HEAD
 +
 +	while ((bio = bio_list_pop(&io->bios)))
 +		submit_bio(WRITE, bio);
++=======
+ 	if (do_submit)
+ 		r5l_do_submit_io(log, io);
++>>>>>>> 3bddb7f8f264 (md/r5cache: handle FLUSH and FUA)
  }
  
 -static struct bio *r5l_bio_alloc(struct r5l_log *log)
 +static struct bio *r5l_bio_alloc(struct r5l_log *log, struct r5l_io_unit *io)
  {
  	struct bio *bio = bio_alloc_bioset(GFP_NOIO, BIO_MAX_PAGES, log->bs);
  
@@@ -621,9 -676,9 +731,10 @@@ static struct r5l_io_unit *r5l_new_meta
  	memset(io, 0, sizeof(*io));
  
  	io->log = log;
 +	bio_list_init(&io->bios);
  	INIT_LIST_HEAD(&io->log_sibling);
  	INIT_LIST_HEAD(&io->stripe_list);
+ 	bio_list_init(&io->flush_barriers);
  	io->state = IO_UNIT_RUNNING;
  
  	io->meta_page = mempool_alloc(log->meta_pool, GFP_NOIO);
@@@ -691,15 -748,17 +802,24 @@@ static void r5l_append_payload_page(str
  {
  	struct r5l_io_unit *io = log->current_io;
  
++<<<<<<< HEAD
 +alloc_bio:
 +	if (!io->current_bio)
 +		io->current_bio = r5l_bio_alloc(log, io);
 +
 +	if (!bio_add_page(io->current_bio, page, PAGE_SIZE, 0)) {
 +		io->current_bio = NULL;
 +		goto alloc_bio;
++=======
+ 	if (io->need_split_bio) {
+ 		BUG_ON(io->split_bio);
+ 		io->split_bio = io->current_bio;
+ 		io->current_bio = r5l_bio_alloc(log);
+ 		bio_chain(io->current_bio, io->split_bio);
+ 		io->need_split_bio = false;
++>>>>>>> 3bddb7f8f264 (md/r5cache: handle FLUSH and FUA)
  	}
  
 -	if (!bio_add_page(io->current_bio, page, PAGE_SIZE, 0))
 -		BUG();
 -
  	r5_reserve_log_entry(log, io);
  }
  
@@@ -892,17 -963,34 +1024,48 @@@ int r5l_handle_flush_request(struct r5l
  {
  	if (!log)
  		return -ENODEV;
++<<<<<<< HEAD
 +	/*
 +	 * we flush log disk cache first, then write stripe data to raid disks.
 +	 * So if bio is finished, the log disk cache is flushed already. The
 +	 * recovery guarantees we can recovery the bio from log disk, so we
 +	 * don't need to flush again
 +	 */
 +	if (bio->bi_size == 0) {
 +		bio_endio(bio, 0);
 +		return 0;
 +	}
 +	bio->bi_rw &= ~REQ_FLUSH;
++=======
+ 
+ 	if (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH) {
+ 		/*
+ 		 * in write through (journal only)
+ 		 * we flush log disk cache first, then write stripe data to
+ 		 * raid disks. So if bio is finished, the log disk cache is
+ 		 * flushed already. The recovery guarantees we can recovery
+ 		 * the bio from log disk, so we don't need to flush again
+ 		 */
+ 		if (bio->bi_iter.bi_size == 0) {
+ 			bio_endio(bio);
+ 			return 0;
+ 		}
+ 		bio->bi_opf &= ~REQ_PREFLUSH;
+ 	} else {
+ 		/* write back (with cache) */
+ 		if (bio->bi_iter.bi_size == 0) {
+ 			mutex_lock(&log->io_mutex);
+ 			r5l_get_meta(log, 0);
+ 			bio_list_add(&log->current_io->flush_barriers, bio);
+ 			log->current_io->has_flush = 1;
+ 			log->current_io->has_null_flush = 1;
+ 			atomic_inc(&log->current_io->pending_stripe);
+ 			r5l_submit_current_io(log);
+ 			mutex_unlock(&log->io_mutex);
+ 			return 0;
+ 		}
+ 	}
++>>>>>>> 3bddb7f8f264 (md/r5cache: handle FLUSH and FUA)
  	return -EAGAIN;
  }
  
@@@ -2590,7 -2594,7 +2753,11 @@@ int r5l_init_log(struct r5conf *conf, s
  	INIT_LIST_HEAD(&log->no_space_stripes);
  	spin_lock_init(&log->no_space_stripes_lock);
  
++<<<<<<< HEAD
 +	INIT_WORK(&log->disable_writeback_work, r5c_disable_writeback_async);
++=======
+ 	INIT_WORK(&log->deferred_io_work, r5l_submit_io_async);
++>>>>>>> 3bddb7f8f264 (md/r5cache: handle FLUSH and FUA)
  
  	log->r5c_journal_mode = R5C_JOURNAL_MODE_WRITE_THROUGH;
  	INIT_LIST_HEAD(&log->stripe_in_journal_list);
* Unmerged path drivers/md/raid5-cache.c
diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index 5c35f05eceef..0601a25c3f02 100644
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@ -5370,6 +5370,7 @@ static bool raid5_make_request(struct mddev *mddev, struct bio * bi)
 	int remaining;
 	DEFINE_WAIT(w);
 	bool do_prepare;
+	bool do_flush = false;
 
 	if (unlikely(bi->bi_rw & REQ_FLUSH)) {
 		int ret = r5l_handle_flush_request(conf->log, bi);
@@ -5381,6 +5382,11 @@ static bool raid5_make_request(struct mddev *mddev, struct bio * bi)
 			return true;
 		}
 		/* ret == -EAGAIN, fallback */
+		/*
+		 * if r5l_handle_flush_request() didn't clear REQ_PREFLUSH,
+		 * we need to flush journal device
+		 */
+		do_flush = bi->bi_opf & REQ_PREFLUSH;
 	}
 
 	if (!md_write_start(mddev, bi))
@@ -5500,6 +5506,12 @@ static bool raid5_make_request(struct mddev *mddev, struct bio * bi)
 				do_prepare = true;
 				goto retry;
 			}
+			if (do_flush) {
+				set_bit(STRIPE_R5C_PREFLUSH, &sh->state);
+				/* we only need flush for one stripe */
+				do_flush = false;
+			}
+
 			set_bit(STRIPE_HANDLE, &sh->state);
 			clear_bit(STRIPE_DELAYED, &sh->state);
 			if ((!sh->batch_head || sh == sh->batch_head) &&
diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 1c4637f3b1c7..785c045d146b 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -382,6 +382,7 @@ enum {
 	STRIPE_R5C_FULL_STRIPE,	/* in r5c cache (to-be/being handled or
 				 * in conf->r5c_full_stripe_list)
 				 */
+	STRIPE_R5C_PREFLUSH,	/* need to flush journal device */
 };
 
 #define STRIPE_EXPAND_SYNC_FLAGS \
