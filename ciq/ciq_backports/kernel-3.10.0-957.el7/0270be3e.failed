x86/speculation: Rework speculative_store_bypass_update()

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] speculation: Rework speculative_store_bypass_update() (Waiman Long) [1584569] {CVE-2018-3639}
Rebuild_FUZZ: 96.36%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 0270be3e34efb05a88bc4c422572ece038ef3608
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/0270be3e.failed

The upcoming support for the virtual SPEC_CTRL MSR on AMD needs to reuse
speculative_store_bypass_update() to avoid code duplication. Add an
argument for supplying a thread info (TIF) value and create a wrapper
speculative_store_bypass_update_current() which is used at the existing
call site.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit 0270be3e34efb05a88bc4c422572ece038ef3608)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/spec-ctrl.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/process.c
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,f2f0c1b3bf50..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -199,15 -428,269 +199,249 @@@ static void __init spectre_v2_select_mi
  }
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
+ 
+ static enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;
+ 
+ /* The kernel command line selection */
+ enum ssb_mitigation_cmd {
+ 	SPEC_STORE_BYPASS_CMD_NONE,
+ 	SPEC_STORE_BYPASS_CMD_AUTO,
+ 	SPEC_STORE_BYPASS_CMD_ON,
+ 	SPEC_STORE_BYPASS_CMD_PRCTL,
+ 	SPEC_STORE_BYPASS_CMD_SECCOMP,
+ };
+ 
+ static const char *ssb_strings[] = {
+ 	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
+ 	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled",
+ 	[SPEC_STORE_BYPASS_PRCTL]	= "Mitigation: Speculative Store Bypass disabled via prctl",
+ 	[SPEC_STORE_BYPASS_SECCOMP]	= "Mitigation: Speculative Store Bypass disabled via prctl and seccomp",
+ };
+ 
+ static const struct {
+ 	const char *option;
+ 	enum ssb_mitigation_cmd cmd;
+ } ssb_mitigation_options[] = {
+ 	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO },    /* Platform decides */
+ 	{ "on",		SPEC_STORE_BYPASS_CMD_ON },      /* Disable Speculative Store Bypass */
+ 	{ "off",	SPEC_STORE_BYPASS_CMD_NONE },    /* Don't touch Speculative Store Bypass */
+ 	{ "prctl",	SPEC_STORE_BYPASS_CMD_PRCTL },   /* Disable Speculative Store Bypass via prctl */
+ 	{ "seccomp",	SPEC_STORE_BYPASS_CMD_SECCOMP }, /* Disable Speculative Store Bypass via prctl and seccomp */
+ };
+ 
+ static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
+ {
+ 	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
+ 		return SPEC_STORE_BYPASS_CMD_NONE;
+ 	} else {
+ 		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
+ 					  arg, sizeof(arg));
+ 		if (ret < 0)
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
+ 			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
+ 				continue;
+ 
+ 			cmd = ssb_mitigation_options[i].cmd;
+ 			break;
+ 		}
+ 
+ 		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
+ 			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 		}
+ 	}
+ 
+ 	return cmd;
+ }
+ 
+ static enum ssb_mitigation __init __ssb_select_mitigation(void)
+ {
+ 	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
+ 	enum ssb_mitigation_cmd cmd;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_SSBD))
+ 		return mode;
+ 
+ 	cmd = ssb_parse_cmdline();
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
+ 	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
+ 	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
+ 		return mode;
+ 
+ 	switch (cmd) {
+ 	case SPEC_STORE_BYPASS_CMD_AUTO:
+ 	case SPEC_STORE_BYPASS_CMD_SECCOMP:
+ 		/*
+ 		 * Choose prctl+seccomp as the default mode if seccomp is
+ 		 * enabled.
+ 		 */
+ 		if (IS_ENABLED(CONFIG_SECCOMP))
+ 			mode = SPEC_STORE_BYPASS_SECCOMP;
+ 		else
+ 			mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_ON:
+ 		mode = SPEC_STORE_BYPASS_DISABLE;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_PRCTL:
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_NONE:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * We have three CPU feature flags that are in play here:
+ 	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.
+ 	 *  - X86_FEATURE_SSBD - CPU is able to turn off speculative store bypass
+ 	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation
+ 	 */
+ 	if (mode == SPEC_STORE_BYPASS_DISABLE) {
+ 		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
+ 		/*
+ 		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD uses
+ 		 * a completely different MSR and bit dependent on family.
+ 		 */
+ 		switch (boot_cpu_data.x86_vendor) {
+ 		case X86_VENDOR_INTEL:
+ 			x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
+ 			x86_spec_ctrl_mask &= ~SPEC_CTRL_SSBD;
+ 			x86_spec_ctrl_set(SPEC_CTRL_SSBD);
+ 			break;
+ 		case X86_VENDOR_AMD:
+ 			x86_amd_ssb_disable();
+ 			break;
+ 		}
+ 	}
+ 
+ 	return mode;
+ }
+ 
+ static void ssb_select_mitigation(void)
+ {
+ 	ssb_mode = __ssb_select_mitigation();
+ 
+ 	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		pr_info("%s\n", ssb_strings[ssb_mode]);
+ }
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Speculation prctl: " fmt
+ 
+ static int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)
+ {
+ 	bool update;
+ 
+ 	if (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&
+ 	    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)
+ 		return -ENXIO;
+ 
+ 	switch (ctrl) {
+ 	case PR_SPEC_ENABLE:
+ 		/* If speculation is force disabled, enable is not allowed */
+ 		if (task_spec_ssb_force_disable(task))
+ 			return -EPERM;
+ 		task_clear_spec_ssb_disable(task);
+ 		update = test_and_clear_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	case PR_SPEC_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	case PR_SPEC_FORCE_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		task_set_spec_ssb_force_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	default:
+ 		return -ERANGE;
+ 	}
+ 
+ 	/*
+ 	 * If being set on non-current task, delay setting the CPU
+ 	 * mitigation until it is next scheduled.
+ 	 */
+ 	if (task == current && update)
+ 		speculative_store_bypass_update_current();
+ 
+ 	return 0;
+ }
+ 
+ int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
+ 			     unsigned long ctrl)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_set(task, ctrl);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ #ifdef CONFIG_SECCOMP
+ void arch_seccomp_spec_mitigate(struct task_struct *task)
+ {
+ 	if (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)
+ 		ssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);
+ }
+ #endif
+ 
+ static int ssb_prctl_get(struct task_struct *task)
+ {
+ 	switch (ssb_mode) {
+ 	case SPEC_STORE_BYPASS_DISABLE:
+ 		return PR_SPEC_DISABLE;
+ 	case SPEC_STORE_BYPASS_SECCOMP:
+ 	case SPEC_STORE_BYPASS_PRCTL:
+ 		if (task_spec_ssb_force_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;
+ 		if (task_spec_ssb_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_DISABLE;
+ 		return PR_SPEC_PRCTL | PR_SPEC_ENABLE;
+ 	default:
+ 		if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 			return PR_SPEC_ENABLE;
+ 		return PR_SPEC_NOT_AFFECTED;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_get(task);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ void x86_spec_ctrl_setup_ap(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
+ 		x86_spec_ctrl_set(x86_spec_ctrl_base & ~x86_spec_ctrl_mask);
+ 
+ 	if (ssb_mode == SPEC_STORE_BYPASS_DISABLE)
+ 		x86_amd_ssb_disable();
+ }
++>>>>>>> 0270be3e34ef (x86/speculation: Rework speculative_store_bypass_update())
  
  #ifdef CONFIG_SYSFS
 -
 -static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			       char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
 -
 -	switch (bug) {
 -	case X86_BUG_CPU_MELTDOWN:
 -		if (boot_cpu_has(X86_FEATURE_PTI))
 -			return sprintf(buf, "Mitigation: PTI\n");
 -
 -		break;
 -
 -	case X86_BUG_SPECTRE_V1:
 -		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 -
 -	case X86_BUG_SPECTRE_V2:
 -		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
 -			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
 -			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 -			       spectre_v2_module_string());
 -
 -	case X86_BUG_SPEC_STORE_BYPASS:
 -		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
 -
 -	default:
 -		break;
 -	}
 -
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
  	return sprintf(buf, "Vulnerable\n");
  }
  
diff --cc arch/x86/kernel/process.c
index f741d66041de,30ca2d1a9231..000000000000
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@@ -237,7 -277,184 +237,167 @@@ void __switch_to_xtra(struct task_struc
  		 */
  		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
  	}
++<<<<<<< HEAD
++=======
+ }
+ 
+ #ifdef CONFIG_SMP
+ 
+ struct ssb_state {
+ 	struct ssb_state	*shared_state;
+ 	raw_spinlock_t		lock;
+ 	unsigned int		disable_state;
+ 	unsigned long		local_state;
+ };
+ 
+ #define LSTATE_SSB	0
+ 
+ static DEFINE_PER_CPU(struct ssb_state, ssb_state);
+ 
+ void speculative_store_bypass_ht_init(void)
+ {
+ 	struct ssb_state *st = this_cpu_ptr(&ssb_state);
+ 	unsigned int this_cpu = smp_processor_id();
+ 	unsigned int cpu;
+ 
+ 	st->local_state = 0;
+ 
+ 	/*
+ 	 * Shared state setup happens once on the first bringup
+ 	 * of the CPU. It's not destroyed on CPU hotunplug.
+ 	 */
+ 	if (st->shared_state)
+ 		return;
+ 
+ 	raw_spin_lock_init(&st->lock);
+ 
+ 	/*
+ 	 * Go over HT siblings and check whether one of them has set up the
+ 	 * shared state pointer already.
+ 	 */
+ 	for_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {
+ 		if (cpu == this_cpu)
+ 			continue;
+ 
+ 		if (!per_cpu(ssb_state, cpu).shared_state)
+ 			continue;
+ 
+ 		/* Link it to the state of the sibling: */
+ 		st->shared_state = per_cpu(ssb_state, cpu).shared_state;
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * First HT sibling to come up on the core.  Link shared state of
+ 	 * the first HT sibling to itself. The siblings on the same core
+ 	 * which come up later will see the shared state pointer and link
+ 	 * themself to the state of this CPU.
+ 	 */
+ 	st->shared_state = st;
+ }
+ 
+ /*
+  * Logic is: First HT sibling enables SSBD for both siblings in the core
+  * and last sibling to disable it, disables it for the whole core. This how
+  * MSR_SPEC_CTRL works in "hardware":
+  *
+  *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL
+  */
+ static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
+ {
+ 	struct ssb_state *st = this_cpu_ptr(&ssb_state);
+ 	u64 msr = x86_amd_ls_cfg_base;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_ZEN)) {
+ 		msr |= ssbd_tif_to_amd_ls_cfg(tifn);
+ 		wrmsrl(MSR_AMD64_LS_CFG, msr);
+ 		return;
+ 	}
+ 
+ 	if (tifn & _TIF_SSBD) {
+ 		/*
+ 		 * Since this can race with prctl(), block reentry on the
+ 		 * same CPU.
+ 		 */
+ 		if (__test_and_set_bit(LSTATE_SSB, &st->local_state))
+ 			return;
+ 
+ 		msr |= x86_amd_ls_cfg_ssbd_mask;
+ 
+ 		raw_spin_lock(&st->shared_state->lock);
+ 		/* First sibling enables SSBD: */
+ 		if (!st->shared_state->disable_state)
+ 			wrmsrl(MSR_AMD64_LS_CFG, msr);
+ 		st->shared_state->disable_state++;
+ 		raw_spin_unlock(&st->shared_state->lock);
+ 	} else {
+ 		if (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))
+ 			return;
+ 
+ 		raw_spin_lock(&st->shared_state->lock);
+ 		st->shared_state->disable_state--;
+ 		if (!st->shared_state->disable_state)
+ 			wrmsrl(MSR_AMD64_LS_CFG, msr);
+ 		raw_spin_unlock(&st->shared_state->lock);
+ 	}
+ }
+ #else
+ static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
+ {
+ 	u64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
+ 
+ 	wrmsrl(MSR_AMD64_LS_CFG, msr);
+ }
+ #endif
+ 
+ static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
+ {
+ 	/*
+ 	 * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,
+ 	 * so ssbd_tif_to_spec_ctrl() just works.
+ 	 */
+ 	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));
+ }
+ 
+ static __always_inline void intel_set_ssb_state(unsigned long tifn)
+ {
+ 	u64 msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
+ 
+ 	wrmsrl(MSR_IA32_SPEC_CTRL, msr);
+ }
+ 
+ static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+ {
+ 	if (static_cpu_has(X86_FEATURE_VIRT_SSBD))
+ 		amd_set_ssb_virt_state(tifn);
+ 	else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
+ 		amd_set_core_ssb_state(tifn);
+ 	else
+ 		intel_set_ssb_state(tifn);
+ }
+ 
+ void speculative_store_bypass_update(unsigned long tif)
+ {
+ 	preempt_disable();
+ 	__speculative_store_bypass_update(tif);
+ 	preempt_enable();
+ }
+ 
+ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
+ 		      struct tss_struct *tss)
+ {
+ 	struct thread_struct *prev, *next;
+ 	unsigned long tifp, tifn;
+ 
+ 	prev = &prev_p->thread;
+ 	next = &next_p->thread;
+ 
+ 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
+ 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
+ 	switch_to_bitmap(tss, prev, next, tifp, tifn);
+ 
++>>>>>>> 0270be3e34ef (x86/speculation: Rework speculative_store_bypass_update())
  	propagate_user_return_notify(prev_p, next_p);
 -
 -	if ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&
 -	    arch_has_block_step()) {
 -		unsigned long debugctl, msk;
 -
 -		rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 -		debugctl &= ~DEBUGCTLMSR_BTF;
 -		msk = tifn & _TIF_BLOCKSTEP;
 -		debugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;
 -		wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 -	}
 -
 -	if ((tifp ^ tifn) & _TIF_NOTSC)
 -		cr4_toggle_bits_irqsoff(X86_CR4_TSD);
 -
 -	if ((tifp ^ tifn) & _TIF_NOCPUID)
 -		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 -
 -	if ((tifp ^ tifn) & _TIF_SSBD)
 -		__speculative_store_bypass_update(tifn);
  }
  
  /*
* Unmerged path arch/x86/include/asm/spec-ctrl.h
* Unmerged path arch/x86/include/asm/spec-ctrl.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/process.c
