bcache: remove unused parameter

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [crypto] chelsio: Remove unused parameter (Arjun Vynipadath) [1548047]
Rebuild_FUZZ: 88.89%
commit-author Yijing Wang <wangyijing@huawei.com>
commit 238501027abf0386fa5f5dcaf589f406eb187bc3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/23850102.failed

Parameter bio is no longer used, clean it.

	Signed-off-by: Yijing Wang <wangyijing@huawei.com>
	Reviewed-by: Coly Li <colyli@suse.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 238501027abf0386fa5f5dcaf589f406eb187bc3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/bcache/request.c
diff --cc drivers/md/bcache/request.c
index a30a0f8a41c0,163a17a80874..000000000000
--- a/drivers/md/bcache/request.c
+++ b/drivers/md/bcache/request.c
@@@ -26,174 -24,15 +26,174 @@@
  
  struct kmem_cache *bch_search_cache;
  
 -static void bch_data_insert_start(struct closure *);
 +static void check_should_skip(struct cached_dev *, struct search *);
 +
 +/* Cgroup interface */
 +
 +#ifdef CONFIG_CGROUP_BCACHE
 +static struct bch_cgroup bcache_default_cgroup = { .cache_mode = -1 };
 +
 +static struct bch_cgroup *cgroup_to_bcache(struct cgroup *cgroup)
 +{
 +	struct cgroup_subsys_state *css;
 +	return cgroup &&
 +		(css = cgroup_subsys_state(cgroup, bcache_subsys_id))
 +		? container_of(css, struct bch_cgroup, css)
 +		: &bcache_default_cgroup;
 +}
 +
 +struct bch_cgroup *bch_bio_to_cgroup(struct bio *bio)
 +{
 +	struct cgroup_subsys_state *css = bio->bi_css
 +		? cgroup_subsys_state(bio->bi_css->cgroup, bcache_subsys_id)
 +		: task_subsys_state(current, bcache_subsys_id);
 +
 +	return css
 +		? container_of(css, struct bch_cgroup, css)
 +		: &bcache_default_cgroup;
 +}
 +
 +static ssize_t cache_mode_read(struct cgroup *cgrp, struct cftype *cft,
 +			struct file *file,
 +			char __user *buf, size_t nbytes, loff_t *ppos)
 +{
 +	char tmp[1024];
 +	int len = bch_snprint_string_list(tmp, PAGE_SIZE, bch_cache_modes,
 +					  cgroup_to_bcache(cgrp)->cache_mode + 1);
 +
 +	if (len < 0)
 +		return len;
 +
 +	return simple_read_from_buffer(buf, nbytes, ppos, tmp, len);
 +}
 +
 +static int cache_mode_write(struct cgroup *cgrp, struct cftype *cft,
 +			    const char *buf)
 +{
 +	int v = bch_read_string_list(buf, bch_cache_modes);
 +	if (v < 0)
 +		return v;
 +
 +	cgroup_to_bcache(cgrp)->cache_mode = v - 1;
 +	return 0;
 +}
 +
 +static u64 bch_verify_read(struct cgroup *cgrp, struct cftype *cft)
 +{
 +	return cgroup_to_bcache(cgrp)->verify;
 +}
 +
 +static int bch_verify_write(struct cgroup *cgrp, struct cftype *cft, u64 val)
 +{
 +	cgroup_to_bcache(cgrp)->verify = val;
 +	return 0;
 +}
 +
 +static u64 bch_cache_hits_read(struct cgroup *cgrp, struct cftype *cft)
 +{
 +	struct bch_cgroup *bcachecg = cgroup_to_bcache(cgrp);
 +	return atomic_read(&bcachecg->stats.cache_hits);
 +}
 +
 +static u64 bch_cache_misses_read(struct cgroup *cgrp, struct cftype *cft)
 +{
 +	struct bch_cgroup *bcachecg = cgroup_to_bcache(cgrp);
 +	return atomic_read(&bcachecg->stats.cache_misses);
 +}
 +
 +static u64 bch_cache_bypass_hits_read(struct cgroup *cgrp,
 +					 struct cftype *cft)
 +{
 +	struct bch_cgroup *bcachecg = cgroup_to_bcache(cgrp);
 +	return atomic_read(&bcachecg->stats.cache_bypass_hits);
 +}
 +
 +static u64 bch_cache_bypass_misses_read(struct cgroup *cgrp,
 +					   struct cftype *cft)
 +{
 +	struct bch_cgroup *bcachecg = cgroup_to_bcache(cgrp);
 +	return atomic_read(&bcachecg->stats.cache_bypass_misses);
 +}
 +
 +static struct cftype bch_files[] = {
 +	{
 +		.name		= "cache_mode",
 +		.read		= cache_mode_read,
 +		.write_string	= cache_mode_write,
 +	},
 +	{
 +		.name		= "verify",
 +		.read_u64	= bch_verify_read,
 +		.write_u64	= bch_verify_write,
 +	},
 +	{
 +		.name		= "cache_hits",
 +		.read_u64	= bch_cache_hits_read,
 +	},
 +	{
 +		.name		= "cache_misses",
 +		.read_u64	= bch_cache_misses_read,
 +	},
 +	{
 +		.name		= "cache_bypass_hits",
 +		.read_u64	= bch_cache_bypass_hits_read,
 +	},
 +	{
 +		.name		= "cache_bypass_misses",
 +		.read_u64	= bch_cache_bypass_misses_read,
 +	},
 +	{ }	/* terminate */
 +};
 +
 +static void init_bch_cgroup(struct bch_cgroup *cg)
 +{
 +	cg->cache_mode = -1;
 +}
 +
 +static struct cgroup_subsys_state *bcachecg_create(struct cgroup *cgroup)
 +{
 +	struct bch_cgroup *cg;
 +
 +	cg = kzalloc(sizeof(*cg), GFP_KERNEL);
 +	if (!cg)
 +		return ERR_PTR(-ENOMEM);
 +	init_bch_cgroup(cg);
 +	return &cg->css;
 +}
 +
 +static void bcachecg_destroy(struct cgroup *cgroup)
 +{
 +	struct bch_cgroup *cg = cgroup_to_bcache(cgroup);
 +	free_css_id(&bcache_subsys, &cg->css);
 +	kfree(cg);
 +}
 +
 +struct cgroup_subsys bcache_subsys = {
 +	.create		= bcachecg_create,
 +	.destroy	= bcachecg_destroy,
 +	.subsys_id	= bcache_subsys_id,
 +	.name		= "bcache",
 +	.module		= THIS_MODULE,
 +};
 +EXPORT_SYMBOL_GPL(bcache_subsys);
 +#endif
  
- static unsigned cache_mode(struct cached_dev *dc, struct bio *bio)
+ static unsigned cache_mode(struct cached_dev *dc)
  {
 +#ifdef CONFIG_CGROUP_BCACHE
 +	int r = bch_bio_to_cgroup(bio)->cache_mode;
 +	if (r >= 0)
 +		return r;
 +#endif
  	return BDEV_CACHE_MODE(&dc->sb);
  }
  
- static bool verify(struct cached_dev *dc, struct bio *bio)
+ static bool verify(struct cached_dev *dc)
  {
 +#ifdef CONFIG_CGROUP_BCACHE
 +	if (bch_bio_to_cgroup(bio)->verify)
 +		return true;
 +#endif
  	return dc->verify;
  }
  
@@@ -593,56 -305,179 +593,125 @@@ err
   * data is written it calls bch_journal, and after the keys have been added to
   * the next journal write they're inserted into the btree.
   *
 - * It inserts the data in s->cache_bio; bi_sector is used for the key offset,
 + * It inserts the data in op->cache_bio; bi_sector is used for the key offset,
   * and op->inode is used for the key inode.
   *
 - * If s->bypass is true, instead of inserting the data it invalidates the
 - * region of the cache represented by s->cache_bio and op->inode.
 + * If op->skip is true, instead of inserting the data it invalidates the region
 + * of the cache represented by op->cache_bio and op->inode.
   */
 -void bch_data_insert(struct closure *cl)
 +void bch_insert_data(struct closure *cl)
  {
 -	struct data_insert_op *op = container_of(cl, struct data_insert_op, cl);
 -
 -	trace_bcache_write(op->c, op->inode, op->bio,
 -			   op->writeback, op->bypass);
 +	struct btree_op *op = container_of(cl, struct btree_op, cl);
  
 -	bch_keylist_init(&op->insert_keys);
 -	bio_get(op->bio);
 -	bch_data_insert_start(cl);
 +	bch_keylist_init(&op->keys);
 +	bio_get(op->cache_bio);
 +	bch_insert_data_loop(cl);
  }
  
 -/* Congested? */
 -
 -unsigned bch_get_congested(struct cache_set *c)
 +void bch_btree_insert_async(struct closure *cl)
  {
 -	int i;
 -	long rand;
 +	struct btree_op *op = container_of(cl, struct btree_op, cl);
 +	struct search *s = container_of(op, struct search, op);
  
++<<<<<<< HEAD
 +	if (bch_btree_insert(op, op->c)) {
 +		s->error		= -ENOMEM;
 +		op->insert_data_done	= true;
++=======
+ 	if (!c->congested_read_threshold_us &&
+ 	    !c->congested_write_threshold_us)
+ 		return 0;
+ 
+ 	i = (local_clock_us() - c->congested_last_us) / 1024;
+ 	if (i < 0)
+ 		return 0;
+ 
+ 	i += atomic_read(&c->congested);
+ 	if (i >= 0)
+ 		return 0;
+ 
+ 	i += CONGESTED_MAX;
+ 
+ 	if (i > 0)
+ 		i = fract_exp_two(i, 6);
+ 
+ 	rand = get_random_int();
+ 	i -= bitmap_weight(&rand, BITS_PER_LONG);
+ 
+ 	return i > 0 ? i : 1;
+ }
+ 
+ static void add_sequential(struct task_struct *t)
+ {
+ 	ewma_add(t->sequential_io_avg,
+ 		 t->sequential_io, 8, 0);
+ 
+ 	t->sequential_io = 0;
+ }
+ 
+ static struct hlist_head *iohash(struct cached_dev *dc, uint64_t k)
+ {
+ 	return &dc->io_hash[hash_64(k, RECENT_IO_BITS)];
+ }
+ 
+ static bool check_should_bypass(struct cached_dev *dc, struct bio *bio)
+ {
+ 	struct cache_set *c = dc->disk.c;
+ 	unsigned mode = cache_mode(dc);
+ 	unsigned sectors, congested = bch_get_congested(c);
+ 	struct task_struct *task = current;
+ 	struct io *i;
+ 
+ 	if (test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags) ||
+ 	    c->gc_stats.in_use > CUTOFF_CACHE_ADD ||
+ 	    (bio_op(bio) == REQ_OP_DISCARD))
+ 		goto skip;
+ 
+ 	if (mode == CACHE_MODE_NONE ||
+ 	    (mode == CACHE_MODE_WRITEAROUND &&
+ 	     op_is_write(bio_op(bio))))
+ 		goto skip;
+ 
+ 	/*
+ 	 * Flag for bypass if the IO is for read-ahead or background,
+ 	 * unless the read-ahead request is for metadata (eg, for gfs2).
+ 	 */
+ 	if (bio->bi_opf & (REQ_RAHEAD|REQ_BACKGROUND) &&
+ 	    !(bio->bi_opf & REQ_META))
+ 		goto skip;
+ 
+ 	if (bio->bi_iter.bi_sector & (c->sb.block_size - 1) ||
+ 	    bio_sectors(bio) & (c->sb.block_size - 1)) {
+ 		pr_debug("skipping unaligned io");
+ 		goto skip;
++>>>>>>> 238501027abf (bcache: remove unused parameter)
  	}
  
 -	if (bypass_torture_test(dc)) {
 -		if ((get_random_int() & 3) == 3)
 -			goto skip;
 -		else
 -			goto rescale;
 -	}
 -
 -	if (!congested && !dc->sequential_cutoff)
 -		goto rescale;
 -
 -	spin_lock(&dc->io_lock);
 -
 -	hlist_for_each_entry(i, iohash(dc, bio->bi_iter.bi_sector), hash)
 -		if (i->last == bio->bi_iter.bi_sector &&
 -		    time_before(jiffies, i->jiffies))
 -			goto found;
 -
 -	i = list_first_entry(&dc->io_lru, struct io, lru);
 -
 -	add_sequential(task);
 -	i->sequential = 0;
 -found:
 -	if (i->sequential + bio->bi_iter.bi_size > i->sequential)
 -		i->sequential	+= bio->bi_iter.bi_size;
 -
 -	i->last			 = bio_end_sector(bio);
 -	i->jiffies		 = jiffies + msecs_to_jiffies(5000);
 -	task->sequential_io	 = i->sequential;
 -
 -	hlist_del(&i->hash);
 -	hlist_add_head(&i->hash, iohash(dc, i->last));
 -	list_move_tail(&i->lru, &dc->io_lru);
 -
 -	spin_unlock(&dc->io_lock);
 +	if (op->insert_data_done) {
 +		bch_keylist_free(&op->keys);
 +		closure_return(cl);
 +	} else
 +		continue_at(cl, bch_insert_data_loop, bcache_wq);
 +}
  
 -	sectors = max(task->sequential_io,
 -		      task->sequential_io_avg) >> 9;
 +/* Common code for the make_request functions */
  
 -	if (dc->sequential_cutoff &&
 -	    sectors >= dc->sequential_cutoff >> 9) {
 -		trace_bcache_bypass_sequential(bio);
 -		goto skip;
 -	}
 +static void request_endio(struct bio *bio, int error)
 +{
 +	struct closure *cl = bio->bi_private;
  
 -	if (congested && sectors >= congested) {
 -		trace_bcache_bypass_congested(bio);
 -		goto skip;
 +	if (error) {
 +		struct search *s = container_of(cl, struct search, cl);
 +		s->error = error;
 +		/* Only cache read errors are recoverable */
 +		s->recoverable = false;
  	}
  
 -rescale:
 -	bch_rescale_priorities(c, bio_sectors(bio));
 -	return false;
 -skip:
 -	bch_mark_sectors_bypassed(c, dc, bio_sectors(bio));
 -	return true;
 +	bio_put(bio);
 +	closure_put(cl);
  }
  
 -/* Cache lookup */
 -
 -struct search {
 -	/* Stack frame for bio_complete */
 -	struct closure		cl;
 -
 -	struct bbio		bio;
 -	struct bio		*orig_bio;
 -	struct bio		*cache_miss;
 -	struct bcache_device	*d;
 -
 -	unsigned		insert_bio_sectors;
 -	unsigned		recoverable:1;
 -	unsigned		write:1;
 -	unsigned		read_dirty_data:1;
 -
 -	unsigned long		start_time;
 -
 -	struct btree_op		op;
 -	struct data_insert_op	iop;
 -};
 -
 -static void bch_cache_read_endio(struct bio *bio)
 +void bch_cache_read_endio(struct bio *bio, int error)
  {
  	struct bbio *b = container_of(bio, struct bbio, bio);
  	struct closure *cl = bio->bi_private;
@@@ -885,8 -747,8 +954,13 @@@ static void request_read_done(struct cl
  		s->cache_miss = NULL;
  	}
  
++<<<<<<< HEAD
 +	if (verify(dc, &s->bio.bio) && s->recoverable)
 +		bch_data_verify(s);
++=======
+ 	if (verify(dc) && s->recoverable && !s->read_dirty_data)
+ 		bch_data_verify(dc, s->orig_bio);
++>>>>>>> 238501027abf (bcache: remove unused parameter)
  
  	bio_complete(s);
  
@@@ -904,14 -766,16 +978,21 @@@ static void request_read_done_bh(struc
  	struct search *s = container_of(cl, struct search, cl);
  	struct cached_dev *dc = container_of(s->d, struct cached_dev, disk);
  
 -	bch_mark_cache_accounting(s->iop.c, s->d,
 -				  !s->cache_miss, s->iop.bypass);
 -	trace_bcache_read(s->orig_bio, !s->cache_miss, s->iop.bypass);
 +	bch_mark_cache_accounting(s, !s->cache_miss, s->op.skip);
  
++<<<<<<< HEAD
 +	if (s->error)
 +		continue_at_nobarrier(cl, request_read_error, bcache_wq);
 +	else if (s->op.cache_bio || verify(dc, &s->bio.bio))
 +		continue_at_nobarrier(cl, request_read_done, bcache_wq);
++=======
+ 	if (s->iop.status)
+ 		continue_at_nobarrier(cl, cached_dev_read_error, bcache_wq);
+ 	else if (s->iop.bio || verify(dc))
+ 		continue_at_nobarrier(cl, cached_dev_read_done, bcache_wq);
++>>>>>>> 238501027abf (bcache: remove unused parameter)
  	else
 -		continue_at_nobarrier(cl, cached_dev_bio_complete, NULL);
 +		continue_at_nobarrier(cl, cached_dev_read_complete, NULL);
  }
  
  static int cached_dev_cache_miss(struct btree *b, struct search *s,
@@@ -1023,41 -873,50 +1104,50 @@@ static void request_write(struct cached
  {
  	struct closure *cl = &s->cl;
  	struct bio *bio = &s->bio.bio;
 -	struct bkey start = KEY(dc->disk.id, bio->bi_iter.bi_sector, 0);
 -	struct bkey end = KEY(dc->disk.id, bio_end_sector(bio), 0);
 +	struct bkey start, end;
 +	start = KEY(dc->disk.id, bio->bi_sector, 0);
 +	end = KEY(dc->disk.id, bio_end(bio), 0);
  
 -	bch_keybuf_check_overlapping(&s->iop.c->moving_gc_keys, &start, &end);
 +	bch_keybuf_check_overlapping(&s->op.c->moving_gc_keys, &start, &end);
  
 +	check_should_skip(dc, s);
  	down_read_non_owner(&dc->writeback_lock);
 +
  	if (bch_keybuf_check_overlapping(&dc->writeback_keys, &start, &end)) {
 -		/*
 -		 * We overlap with some dirty data undergoing background
 -		 * writeback, force this write to writeback
 -		 */
 -		s->iop.bypass = false;
 -		s->iop.writeback = true;
 +		s->op.skip	= false;
 +		s->writeback	= true;
  	}
  
 -	/*
 -	 * Discards aren't _required_ to do anything, so skipping if
 -	 * check_overlapping returned true is ok
 -	 *
 -	 * But check_overlapping drops dirty keys for which io hasn't started,
 -	 * so we still want to call it.
 -	 */
 -	if (bio_op(bio) == REQ_OP_DISCARD)
 -		s->iop.bypass = true;
 +	if (bio->bi_rw & REQ_DISCARD)
 +		goto skip;
  
++<<<<<<< HEAD
 +	if (s->op.skip)
 +		goto skip;
++=======
+ 	if (should_writeback(dc, s->orig_bio,
+ 			     cache_mode(dc),
+ 			     s->iop.bypass)) {
+ 		s->iop.bypass = false;
+ 		s->iop.writeback = true;
+ 	}
++>>>>>>> 238501027abf (bcache: remove unused parameter)
 +
 +	if (should_writeback(dc, s->orig_bio))
 +		s->writeback = true;
  
 -	if (s->iop.bypass) {
 -		s->iop.bio = s->orig_bio;
 -		bio_get(s->iop.bio);
 +	if (!s->writeback) {
 +		s->op.cache_bio = bio_clone_bioset(bio, GFP_NOIO,
 +						   dc->disk.bio_split);
  
 -		if ((bio_op(bio) != REQ_OP_DISCARD) ||
 -		    blk_queue_discard(bdev_get_queue(dc->bdev)))
 -			closure_bio_submit(bio, cl);
 -	} else if (s->iop.writeback) {
 -		bch_writeback_add(dc);
 -		s->iop.bio = bio;
 +		trace_bcache_writethrough(s->orig_bio);
 +		closure_bio_submit(bio, cl, s->d);
 +	} else {
 +		trace_bcache_writeback(s->orig_bio);
 +		bch_writeback_add(dc, bio_sectors(bio));
 +		s->op.cache_bio = bio;
  
 -		if (bio->bi_opf & REQ_PREFLUSH) {
 +		if (bio->bi_rw & REQ_FLUSH) {
  			/* Also need to send a flush to the backing device */
  			struct bio *flush = bio_alloc_bioset(GFP_NOIO, 0,
  							     dc->disk.bio_split);
* Unmerged path drivers/md/bcache/request.c
