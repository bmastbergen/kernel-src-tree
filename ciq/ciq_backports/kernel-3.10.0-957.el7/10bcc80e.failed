membarrier/x86: Provide core serializing command

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] membarrier: provide core serializing command (Rafael Aquini) [1560024]
Rebuild_FUZZ: 95.65%
commit-author Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
commit 10bcc80e9dbced128e3b4aa86e4737e5486a45d0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/10bcc80e.failed

There are two places where core serialization is needed by membarrier:

1) When returning from the membarrier IPI,
2) After scheduler updates curr to a thread with a different mm, before
   going back to user-space, since the curr->mm is used by membarrier to
   check whether it needs to send an IPI to that CPU.

x86-32 uses IRET as return from interrupt, and both IRET and SYSEXIT to go
back to user-space. The IRET instruction is core serializing, but not
SYSEXIT.

x86-64 uses IRET as return from interrupt, which takes care of the IPI.
However, it can return to user-space through either SYSRETL (compat
code), SYSRETQ, or IRET. Given that SYSRET{L,Q} is not core serializing,
we rely instead on write_cr3() performed by switch_mm() to provide core
serialization after changing the current mm, and deal with the special
case of kthread -> uthread (temporarily keeping current mm into
active_mm) by adding a sync_core() in that specific case.

Use the new sync_core_before_usermode() to guarantee this.

	Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
	Acked-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrea Parri <parri.andrea@gmail.com>
	Cc: Andrew Hunter <ahh@google.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Avi Kivity <avi@scylladb.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Boqun Feng <boqun.feng@gmail.com>
	Cc: Dave Watson <davejwatson@fb.com>
	Cc: David Sehr <sehr@google.com>
	Cc: Greg Hackmann <ghackmann@google.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Maged Michael <maged.michael@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: linux-api@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
Link: http://lkml.kernel.org/r/20180129202020.8515-10-mathieu.desnoyers@efficios.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 10bcc80e9dbced128e3b4aa86e4737e5486a45d0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/entry/entry_32.S
#	arch/x86/entry/entry_64.S
#	arch/x86/mm/tlb.c
diff --cc arch/x86/Kconfig
index 48ae09959d87,e095bdb9afdf..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -15,132 -22,175 +15,152 @@@ config X86_3
  config X86_64
  	def_bool y
  	depends on 64BIT
 -	# Options that are inherently 64-bit kernel only:
 -	select ARCH_HAS_GIGANTIC_PAGE if (MEMORY_ISOLATION && COMPACTION) || CMA
 -	select ARCH_SUPPORTS_INT128
 -	select ARCH_USE_CMPXCHG_LOCKREF
 -	select HAVE_ARCH_SOFT_DIRTY
 -	select MODULES_USE_ELF_RELA
  	select X86_DEV_DMA_OPS
 +	select ARCH_USE_CMPXCHG_LOCKREF
 +	select HAVE_LIVEPATCH
  
 -#
 -# Arch settings
 -#
 -# ( Note that options that are marked 'if X86_64' could in principle be
 -#   ported to 32-bit as well. )
 -#
 +### Arch settings
  config X86
  	def_bool y
++<<<<<<< HEAD
 +	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
++=======
+ 	#
+ 	# Note: keep this list sorted alphabetically
+ 	#
+ 	select ACPI_LEGACY_TABLES_LOOKUP	if ACPI
+ 	select ACPI_SYSTEM_POWER_STATES_SUPPORT	if ACPI
+ 	select ANON_INODES
+ 	select ARCH_CLOCKSOURCE_DATA
+ 	select ARCH_DISCARD_MEMBLOCK
+ 	select ARCH_HAS_ACPI_TABLE_UPGRADE	if ACPI
+ 	select ARCH_HAS_DEBUG_VIRTUAL
+ 	select ARCH_HAS_DEVMEM_IS_ALLOWED
+ 	select ARCH_HAS_ELF_RANDOMIZE
+ 	select ARCH_HAS_FAST_MULTIPLIER
+ 	select ARCH_HAS_FORTIFY_SOURCE
+ 	select ARCH_HAS_GCOV_PROFILE_ALL
+ 	select ARCH_HAS_KCOV			if X86_64
+ 	select ARCH_HAS_MEMBARRIER_SYNC_CORE
++>>>>>>> 10bcc80e9dbc (membarrier/x86: Provide core serializing command)
  	select ARCH_HAS_PMEM_API		if X86_64
 -	select ARCH_HAS_REFCOUNT
  	select ARCH_HAS_UACCESS_FLUSHCACHE	if X86_64
 -	select ARCH_HAS_SET_MEMORY
 -	select ARCH_HAS_SG_CHAIN
 -	select ARCH_HAS_STRICT_KERNEL_RWX
 -	select ARCH_HAS_STRICT_MODULE_RWX
 -	select ARCH_HAS_SYNC_CORE_BEFORE_USERMODE
 -	select ARCH_HAS_UBSAN_SANITIZE_ALL
 -	select ARCH_HAS_ZONE_DEVICE		if X86_64
 -	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 -	select ARCH_MIGHT_HAVE_ACPI_PDC		if ACPI
 -	select ARCH_MIGHT_HAVE_PC_PARPORT
 -	select ARCH_MIGHT_HAVE_PC_SERIO
 -	select ARCH_SUPPORTS_ATOMIC_RMW
 +	select ARCH_HAS_MMIO_FLUSH
 +	select HAVE_AOUT if X86_32
 +	select HAVE_UNSTABLE_SCHED_CLOCK
 +	select ARCH_SUPPORTS_NUMA_BALANCING
 +	select ARCH_SUPPORTS_INT128 if X86_64
 +	select ARCH_WANTS_PROT_NUMA_PROT_NONE
 +	select HAVE_IDE
 +	select HAVE_OPROFILE
 +	select HAVE_PCSPKR_PLATFORM
 +	select HAVE_PERF_EVENTS
 +	select HAVE_IOREMAP_PROT
 +	select HAVE_KPROBES
 +	select HAVE_MEMBLOCK
 +	select HAVE_MEMBLOCK_NODE_MAP
 +	select ARCH_DISCARD_MEMBLOCK
  	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 -	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
 -	select ARCH_USE_BUILTIN_BSWAP
 -	select ARCH_USE_QUEUED_RWLOCKS
 -	select ARCH_USE_QUEUED_SPINLOCKS
 -	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 -	select ARCH_WANTS_DYNAMIC_TASK_STRUCT
 -	select ARCH_WANTS_THP_SWAP		if X86_64
 -	select BUILDTIME_EXTABLE_SORT
 -	select CLKEVT_I8253
 -	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
 -	select CLOCKSOURCE_WATCHDOG
 -	select DCACHE_WORD_ACCESS
 -	select EDAC_ATOMIC_SCRUB
 -	select EDAC_SUPPORT
 -	select GENERIC_CLOCKEVENTS
 -	select GENERIC_CLOCKEVENTS_BROADCAST	if X86_64 || (X86_32 && X86_LOCAL_APIC)
 -	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 -	select GENERIC_CMOS_UPDATE
 -	select GENERIC_CPU_AUTOPROBE
 -	select GENERIC_CPU_VULNERABILITIES
 -	select GENERIC_EARLY_IOREMAP
 -	select GENERIC_FIND_FIRST_BIT
 -	select GENERIC_IOMAP
 -	select GENERIC_IRQ_EFFECTIVE_AFF_MASK	if SMP
 -	select GENERIC_IRQ_MATRIX_ALLOCATOR	if X86_LOCAL_APIC
 -	select GENERIC_IRQ_MIGRATION		if SMP
 -	select GENERIC_IRQ_PROBE
 -	select GENERIC_IRQ_RESERVATION_MODE
 -	select GENERIC_IRQ_SHOW
 -	select GENERIC_PENDING_IRQ		if SMP
 -	select GENERIC_SMP_IDLE_THREAD
 -	select GENERIC_STRNCPY_FROM_USER
 -	select GENERIC_STRNLEN_USER
 -	select GENERIC_TIME_VSYSCALL
 -	select HARDLOCKUP_CHECK_TIMESTAMP	if X86_64
 -	select HAVE_ACPI_APEI			if ACPI
 -	select HAVE_ACPI_APEI_NMI		if ACPI
 -	select HAVE_ALIGNED_STRUCT_PAGE		if SLUB
 -	select HAVE_ARCH_AUDITSYSCALL
 -	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
 -	select HAVE_ARCH_JUMP_LABEL
 -	select HAVE_ARCH_KASAN			if X86_64
 -	select HAVE_ARCH_KGDB
 +	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
 +	select ARCH_WANT_OPTIONAL_GPIOLIB
 +	select ARCH_WANT_FRAME_POINTERS
 +	select HAVE_DMA_CONTIGUOUS if !SWIOTLB
 +	select HAVE_KRETPROBES
 +	select HAVE_OPTPROBES
 +	select HAVE_KPROBES_ON_FTRACE
 +	select HAVE_FTRACE_MCOUNT_RECORD
 +	select HAVE_FENTRY if X86_64
  	select HAVE_ARCH_MMAP_RND_BITS		if MMU
  	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
 -	select HAVE_ARCH_COMPAT_MMAP_BASES	if MMU && COMPAT
 -	select HAVE_ARCH_SECCOMP_FILTER
 -	select HAVE_ARCH_TRACEHOOK
 -	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 -	select HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD if X86_64
 -	select HAVE_ARCH_VMAP_STACK		if X86_64
 -	select HAVE_ARCH_WITHIN_STACK_FRAMES
 -	select HAVE_CC_STACKPROTECTOR
 -	select HAVE_CMPXCHG_DOUBLE
 -	select HAVE_CMPXCHG_LOCAL
 -	select HAVE_CONTEXT_TRACKING		if X86_64
 -	select HAVE_COPY_THREAD_TLS
  	select HAVE_C_RECORDMCOUNT
 -	select HAVE_DEBUG_KMEMLEAK
 -	select HAVE_DEBUG_STACKOVERFLOW
 -	select HAVE_DMA_API_DEBUG
 -	select HAVE_DMA_CONTIGUOUS
  	select HAVE_DYNAMIC_FTRACE
  	select HAVE_DYNAMIC_FTRACE_WITH_REGS
 -	select HAVE_EBPF_JIT			if X86_64
 -	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 -	select HAVE_EXIT_THREAD
 -	select HAVE_FENTRY			if X86_64 || DYNAMIC_FTRACE
 -	select HAVE_FTRACE_MCOUNT_RECORD
 -	select HAVE_FUNCTION_GRAPH_TRACER
  	select HAVE_FUNCTION_TRACER
 -	select HAVE_GCC_PLUGINS
 -	select HAVE_HW_BREAKPOINT
 -	select HAVE_IDE
 -	select HAVE_IOREMAP_PROT
 -	select HAVE_IRQ_EXIT_ON_IRQ_STACK	if X86_64
 -	select HAVE_IRQ_TIME_ACCOUNTING
 -	select HAVE_KERNEL_BZIP2
 +	select HAVE_FUNCTION_GRAPH_TRACER
 +	select HAVE_SYSCALL_TRACEPOINTS
 +	select SYSCTL_EXCEPTION_TRACE
 +	select HAVE_KVM
 +	select HAVE_ARCH_KGDB
 +	select HAVE_ARCH_TRACEHOOK
 +	select HAVE_GENERIC_DMA_COHERENT if X86_32
 +	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 +	select USER_STACKTRACE_SUPPORT
 +	select HAVE_REGS_AND_STACK_ACCESS_API
 +	select HAVE_DMA_API_DEBUG
  	select HAVE_KERNEL_GZIP
 -	select HAVE_KERNEL_LZ4
 +	select HAVE_KERNEL_BZIP2
  	select HAVE_KERNEL_LZMA
 -	select HAVE_KERNEL_LZO
  	select HAVE_KERNEL_XZ
 -	select HAVE_KPROBES
 -	select HAVE_KPROBES_ON_FTRACE
 -	select HAVE_KRETPROBES
 -	select HAVE_KVM
 -	select HAVE_LIVEPATCH			if X86_64
 -	select HAVE_MEMBLOCK
 -	select HAVE_MEMBLOCK_NODE_MAP
 +	select HAVE_KERNEL_LZO
 +	select HAVE_HW_BREAKPOINT
  	select HAVE_MIXED_BREAKPOINTS_REGS
 -	select HAVE_MOD_ARCH_SPECIFIC
 -	select HAVE_NMI
 -	select HAVE_OPROFILE
 -	select HAVE_OPTPROBES
 -	select HAVE_PCSPKR_PLATFORM
 -	select HAVE_PERF_EVENTS
 +	select PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
 -	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
 +	select HAVE_DEBUG_KMEMLEAK
 +	select ANON_INODES
 +	select HAVE_ALIGNED_STRUCT_PAGE if SLUB
 +	select HAVE_CMPXCHG_LOCAL
 +	select HAVE_CMPXCHG_DOUBLE
 +	select HAVE_ARCH_KMEMCHECK
  	select HAVE_RCU_TABLE_FREE
 -	select HAVE_REGS_AND_STACK_ACCESS_API
 -	select HAVE_RELIABLE_STACKTRACE		if X86_64 && UNWINDER_FRAME_POINTER && STACK_VALIDATION
 -	select HAVE_STACK_VALIDATION		if X86_64
 -	select HAVE_SYSCALL_TRACEPOINTS
 -	select HAVE_UNSTABLE_SCHED_CLOCK
  	select HAVE_USER_RETURN_NOTIFIER
 -	select IRQ_FORCED_THREADING
 -	select PCI_LOCKLESS_CONFIG
 -	select PERF_EVENTS
 -	select RTC_LIB
 -	select RTC_MC146818_LIB
 +	select ARCH_HAS_ELF_RANDOMIZE
 +	select HAVE_ARCH_JUMP_LABEL
 +	select HAVE_TEXT_POKE_SMP
 +	select HAVE_GENERIC_HARDIRQS
 +	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
  	select SPARSE_IRQ
 -	select SRCU
 -	select SYSCTL_EXCEPTION_TRACE
 -	select THREAD_INFO_IN_TASK
 -	select USER_STACKTRACE_SUPPORT
 +	select GENERIC_FIND_FIRST_BIT
 +	select GENERIC_IRQ_PROBE
 +	select GENERIC_PENDING_IRQ if SMP
 +	select GENERIC_IRQ_SHOW
 +	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 +	select IRQ_FORCED_THREADING
 +	select USE_GENERIC_SMP_HELPERS if SMP
 +	select HAVE_BPF_JIT if X86_64
 +	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 +	select HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD if X86_64
 +	select HAVE_ARCH_HUGE_VMAP if X86_64 || (X86_32 && X86_PAE)
 +	select CLKEVT_I8253
 +	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 +	select GENERIC_IOMAP
 +	select DCACHE_WORD_ACCESS
 +	select GENERIC_SMP_IDLE_THREAD
 +	select ARCH_WANT_IPC_PARSE_VERSION if X86_32
 +	select HAVE_ARCH_SECCOMP_FILTER
 +	select BUILDTIME_EXTABLE_SORT
 +	select GENERIC_CMOS_UPDATE
 +	select HAVE_ARCH_SOFT_DIRTY
 +	select GENERIC_CLOCKEVENTS
 +	select ARCH_CLOCKSOURCE_DATA if X86_64
 +	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
 +	select GENERIC_CLOCKEVENTS_BROADCAST if X86_64 || (X86_32 && X86_LOCAL_APIC)
 +	select GENERIC_TIME_VSYSCALL if X86_64
 +	select HARDLOCKUP_CHECK_TIMESTAMP if X86_64
 +	select KTIME_SCALAR if X86_32
 +	select GENERIC_STRNCPY_FROM_USER
 +	select GENERIC_STRNLEN_USER
 +	select HAVE_CONTEXT_TRACKING if X86_64
 +	select HAVE_IRQ_TIME_ACCOUNTING
  	select VIRT_TO_BUS
 -	select X86_FEATURE_NAMES		if PROC_FS
 +	select MODULES_USE_ELF_REL if X86_32
 +	select MODULES_USE_ELF_RELA if X86_64
 +	select CLONE_BACKWARDS if X86_32
 +	select ARCH_USE_BUILTIN_BSWAP
 +	select ARCH_USE_QUEUED_SPINLOCKS
 +	select ARCH_USE_QUEUED_RWLOCKS
 +	select OLD_SIGSUSPEND3 if X86_32 || IA32_EMULATION
 +	select OLD_SIGACTION if X86_32
 +	select COMPAT_OLD_SIGACTION if IA32_EMULATION
 +	select RTC_LIB
 +	select HAVE_CC_STACKPROTECTOR
 +	select HAVE_STACK_VALIDATION if X86_64
 +	select HAVE_RELIABLE_STACKTRACE		if X86_64 && FRAME_POINTER && STACK_VALIDATION
 +	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS
 +	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS
 +	select GENERIC_CPU_VULNERABILITIES
  
  config INSTRUCTION_DECODER
  	def_bool y
diff --cc arch/x86/mm/tlb.c
index bbb1a2b21ef2,9b34121c8f05..000000000000
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@@ -53,72 -141,389 +53,239 @@@ void leave_mm(int cpu
  }
  EXPORT_SYMBOL_GPL(leave_mm);
  
++<<<<<<< HEAD
 +/*
 + * The flush IPI assumes that a thread switch happens in this order:
 + * [cpu0: the cpu that switches]
 + * 1) switch_mm() either 1a) or 1b)
 + * 1a) thread switch to a different mm
 + * 1a1) set cpu_tlbstate to TLBSTATE_OK
 + *	Now the tlb flush NMI handler flush_tlb_func won't call leave_mm
 + *	if cpu0 was in lazy tlb mode.
 + * 1a2) update cpu active_mm
 + *	Now cpu0 accepts tlb flushes for the new mm.
 + * 1a3) cpu_set(cpu, new_mm->cpu_vm_mask);
 + *	Now the other cpus will send tlb flush ipis.
 + * 1a4) change cr3.
 + * 1a5) cpu_clear(cpu, old_mm->cpu_vm_mask);
 + *	Stop ipi delivery for the old mm. This is not synchronized with
 + *	the other cpus, but flush_tlb_func ignore flush ipis for the wrong
 + *	mm, and in the worst case we perform a superfluous tlb flush.
 + * 1b) thread switch without mm change
 + *	cpu active_mm is correct, cpu0 already handles flush ipis.
 + * 1b1) set cpu_tlbstate to TLBSTATE_OK
 + * 1b2) test_and_set the cpu bit in cpu_vm_mask.
 + *	Atomically set the bit [other cpus will start sending flush ipis],
 + *	and test the bit.
 + * 1b3) if the bit was 0: leave_mm was called, flush the tlb.
 + * 2) switch %%esp, ie current
 + *
 + * The interrupt must handle 2 special cases:
 + * - cr3 is changed before %%esp, ie. it cannot use current->{active_,}mm.
 + * - the cpu performs speculative tlb reads, i.e. even if the cpu only
 + *   runs in kernel space, the cpu could load tlb entries for user space
 + *   pages.
 + *
 + * The good news is that cpu_tlbstate is local to each cpu, no
 + * write/read ordering problems.
 + */
++=======
+ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
+ 	       struct task_struct *tsk)
+ {
+ 	unsigned long flags;
+ 
+ 	local_irq_save(flags);
+ 	switch_mm_irqs_off(prev, next, tsk);
+ 	local_irq_restore(flags);
+ }
+ 
+ static void sync_current_stack_to_mm(struct mm_struct *mm)
+ {
+ 	unsigned long sp = current_stack_pointer;
+ 	pgd_t *pgd = pgd_offset(mm, sp);
+ 
+ 	if (CONFIG_PGTABLE_LEVELS > 4) {
+ 		if (unlikely(pgd_none(*pgd))) {
+ 			pgd_t *pgd_ref = pgd_offset_k(sp);
+ 
+ 			set_pgd(pgd, *pgd_ref);
+ 		}
+ 	} else {
+ 		/*
+ 		 * "pgd" is faked.  The top level entries are "p4d"s, so sync
+ 		 * the p4d.  This compiles to approximately the same code as
+ 		 * the 5-level case.
+ 		 */
+ 		p4d_t *p4d = p4d_offset(pgd, sp);
+ 
+ 		if (unlikely(p4d_none(*p4d))) {
+ 			pgd_t *pgd_ref = pgd_offset_k(sp);
+ 			p4d_t *p4d_ref = p4d_offset(pgd_ref, sp);
+ 
+ 			set_p4d(p4d, *p4d_ref);
+ 		}
+ 	}
+ }
+ 
+ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
+ 			struct task_struct *tsk)
+ {
+ 	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
+ 	u16 prev_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
+ 	unsigned cpu = smp_processor_id();
+ 	u64 next_tlb_gen;
+ 
+ 	/*
+ 	 * NB: The scheduler will call us with prev == next when switching
+ 	 * from lazy TLB mode to normal mode if active_mm isn't changing.
+ 	 * When this happens, we don't assume that CR3 (and hence
+ 	 * cpu_tlbstate.loaded_mm) matches next.
+ 	 *
+ 	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
+ 	 */
+ 
+ 	/* We don't want flush_tlb_func_* to run concurrently with us. */
+ 	if (IS_ENABLED(CONFIG_PROVE_LOCKING))
+ 		WARN_ON_ONCE(!irqs_disabled());
+ 
+ 	/*
+ 	 * Verify that CR3 is what we think it is.  This will catch
+ 	 * hypothetical buggy code that directly switches to swapper_pg_dir
+ 	 * without going through leave_mm() / switch_mm_irqs_off() or that
+ 	 * does something like write_cr3(read_cr3_pa()).
+ 	 *
+ 	 * Only do this check if CONFIG_DEBUG_VM=y because __read_cr3()
+ 	 * isn't free.
+ 	 */
+ #ifdef CONFIG_DEBUG_VM
+ 	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev->pgd, prev_asid))) {
+ 		/*
+ 		 * If we were to BUG here, we'd be very likely to kill
+ 		 * the system so hard that we don't see the call trace.
+ 		 * Try to recover instead by ignoring the error and doing
+ 		 * a global flush to minimize the chance of corruption.
+ 		 *
+ 		 * (This is far from being a fully correct recovery.
+ 		 *  Architecturally, the CPU could prefetch something
+ 		 *  back into an incorrect ASID slot and leave it there
+ 		 *  to cause trouble down the road.  It's better than
+ 		 *  nothing, though.)
+ 		 */
+ 		__flush_tlb_all();
+ 	}
+ #endif
+ 	this_cpu_write(cpu_tlbstate.is_lazy, false);
+ 
+ 	/*
+ 	 * The membarrier system call requires a full memory barrier and
+ 	 * core serialization before returning to user-space, after
+ 	 * storing to rq->curr. Writing to CR3 provides that full
+ 	 * memory barrier and core serializing instruction.
+ 	 */
+ 	if (real_prev == next) {
+ 		VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
+ 			   next->context.ctx_id);
+ 
+ 		/*
+ 		 * We don't currently support having a real mm loaded without
+ 		 * our cpu set in mm_cpumask().  We have all the bookkeeping
+ 		 * in place to figure out whether we would need to flush
+ 		 * if our cpu were cleared in mm_cpumask(), but we don't
+ 		 * currently use it.
+ 		 */
+ 		if (WARN_ON_ONCE(real_prev != &init_mm &&
+ 				 !cpumask_test_cpu(cpu, mm_cpumask(next))))
+ 			cpumask_set_cpu(cpu, mm_cpumask(next));
+ 
+ 		return;
+ 	} else {
+ 		u16 new_asid;
+ 		bool need_flush;
+ 
+ 		if (IS_ENABLED(CONFIG_VMAP_STACK)) {
+ 			/*
+ 			 * If our current stack is in vmalloc space and isn't
+ 			 * mapped in the new pgd, we'll double-fault.  Forcibly
+ 			 * map it.
+ 			 */
+ 			sync_current_stack_to_mm(next);
+ 		}
+ 
+ 		/* Stop remote flushes for the previous mm */
+ 		VM_WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &&
+ 				real_prev != &init_mm);
+ 		cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
+ 
+ 		/*
+ 		 * Start remote flushes and then read tlb_gen.
+ 		 */
+ 		cpumask_set_cpu(cpu, mm_cpumask(next));
+ 		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
+ 
+ 		choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush);
+ 
+ 		if (need_flush) {
+ 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
+ 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
+ 			load_new_mm_cr3(next->pgd, new_asid, true);
+ 
+ 			/*
+ 			 * NB: This gets called via leave_mm() in the idle path
+ 			 * where RCU functions differently.  Tracing normally
+ 			 * uses RCU, so we need to use the _rcuidle variant.
+ 			 *
+ 			 * (There is no good reason for this.  The idle code should
+ 			 *  be rearranged to call this before rcu_idle_enter().)
+ 			 */
+ 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
+ 		} else {
+ 			/* The new ASID is already up to date. */
+ 			load_new_mm_cr3(next->pgd, new_asid, false);
+ 
+ 			/* See above wrt _rcuidle. */
+ 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
+ 		}
+ 
+ 		this_cpu_write(cpu_tlbstate.loaded_mm, next);
+ 		this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
+ 	}
+ 
+ 	load_mm_cr4(next);
+ 	switch_ldt(real_prev, next);
+ }
++>>>>>>> 10bcc80e9dbc (membarrier/x86: Provide core serializing command)
  
  /*
 - * Please ignore the name of this function.  It should be called
 - * switch_to_kernel_thread().
 - *
 - * enter_lazy_tlb() is a hint from the scheduler that we are entering a
 - * kernel thread or other context without an mm.  Acceptable implementations
 - * include doing nothing whatsoever, switching to init_mm, or various clever
 - * lazy tricks to try to minimize TLB flushes.
 - *
 - * The scheduler reserves the right to call enter_lazy_tlb() several times
 - * in a row.  It will notify us that we're going back to a real mm by
 - * calling switch_mm_irqs_off().
 + * TLB flush funcation:
 + * 1) Flush the tlb entries if the cpu uses the mm that's being flushed.
 + * 2) Leave the mm if we are in the lazy tlb mode.
   */
 -void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
 +static void flush_tlb_func(void *info)
  {
 -	if (this_cpu_read(cpu_tlbstate.loaded_mm) == &init_mm)
 -		return;
 -
 -	if (tlb_defer_switch_to_init_mm()) {
 -		/*
 -		 * There's a significant optimization that may be possible
 -		 * here.  We have accurate enough TLB flush tracking that we
 -		 * don't need to maintain coherence of TLB per se when we're
 -		 * lazy.  We do, however, need to maintain coherence of
 -		 * paging-structure caches.  We could, in principle, leave our
 -		 * old mm loaded and only switch to init_mm when
 -		 * tlb_remove_page() happens.
 -		 */
 -		this_cpu_write(cpu_tlbstate.is_lazy, true);
 -	} else {
 -		switch_mm(NULL, &init_mm, NULL);
 -	}
 -}
 -
 -/*
 - * Call this when reinitializing a CPU.  It fixes the following potential
 - * problems:
 - *
 - * - The ASID changed from what cpu_tlbstate thinks it is (most likely
 - *   because the CPU was taken down and came back up with CR3's PCID
 - *   bits clear.  CPU hotplug can do this.
 - *
 - * - The TLB contains junk in slots corresponding to inactive ASIDs.
 - *
 - * - The CPU went so far out to lunch that it may have missed a TLB
 - *   flush.
 - */
 -void initialize_tlbstate_and_flush(void)
 -{
 -	int i;
 -	struct mm_struct *mm = this_cpu_read(cpu_tlbstate.loaded_mm);
 -	u64 tlb_gen = atomic64_read(&init_mm.context.tlb_gen);
 -	unsigned long cr3 = __read_cr3();
 -
 -	/* Assert that CR3 already references the right mm. */
 -	WARN_ON((cr3 & CR3_ADDR_MASK) != __pa(mm->pgd));
 -
 -	/*
 -	 * Assert that CR4.PCIDE is set if needed.  (CR4.PCIDE initialization
 -	 * doesn't work like other CR4 bits because it can only be set from
 -	 * long mode.)
 -	 */
 -	WARN_ON(boot_cpu_has(X86_FEATURE_PCID) &&
 -		!(cr4_read_shadow() & X86_CR4_PCIDE));
 -
 -	/* Force ASID 0 and force a TLB flush. */
 -	write_cr3(build_cr3(mm->pgd, 0));
 -
 -	/* Reinitialize tlbstate. */
 -	this_cpu_write(cpu_tlbstate.loaded_mm_asid, 0);
 -	this_cpu_write(cpu_tlbstate.next_asid, 1);
 -	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, mm->context.ctx_id);
 -	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, tlb_gen);
 -
 -	for (i = 1; i < TLB_NR_DYN_ASIDS; i++)
 -		this_cpu_write(cpu_tlbstate.ctxs[i].ctx_id, 0);
 -}
 -
 -/*
 - * flush_tlb_func_common()'s memory ordering requirement is that any
 - * TLB fills that happen after we flush the TLB are ordered after we
 - * read active_mm's tlb_gen.  We don't need any explicit barriers
 - * because all x86 flush operations are serializing and the
 - * atomic64_read operation won't be reordered by the compiler.
 - */
 -static void flush_tlb_func_common(const struct flush_tlb_info *f,
 -				  bool local, enum tlb_flush_reason reason)
 -{
 -	/*
 -	 * We have three different tlb_gen values in here.  They are:
 -	 *
 -	 * - mm_tlb_gen:     the latest generation.
 -	 * - local_tlb_gen:  the generation that this CPU has already caught
 -	 *                   up to.
 -	 * - f->new_tlb_gen: the generation that the requester of the flush
 -	 *                   wants us to catch up to.
 -	 */
 -	struct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);
 -	u32 loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
 -	u64 mm_tlb_gen = atomic64_read(&loaded_mm->context.tlb_gen);
 -	u64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen);
 +	struct flush_tlb_info *f = info;
  
 -	/* This code cannot presently handle being reentered. */
 -	VM_WARN_ON(!irqs_disabled());
 +	inc_irq_stat(irq_tlb_count);
  
 -	if (unlikely(loaded_mm == &init_mm))
 +	if (f->flush_mm != this_cpu_read(cpu_tlbstate.active_mm))
  		return;
  
 -	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=
 -		   loaded_mm->context.ctx_id);
 -
 -	if (this_cpu_read(cpu_tlbstate.is_lazy)) {
 -		/*
 -		 * We're in lazy mode.  We need to at least flush our
 -		 * paging-structure cache to avoid speculatively reading
 -		 * garbage into our TLB.  Since switching to init_mm is barely
 -		 * slower than a minimal flush, just switch to init_mm.
 -		 */
 -		switch_mm_irqs_off(NULL, &init_mm, NULL);
 -		return;
 -	}
 -
 -	if (unlikely(local_tlb_gen == mm_tlb_gen)) {
 -		/*
 -		 * There's nothing to do: we're already up to date.  This can
 -		 * happen if two concurrent flushes happen -- the first flush to
 -		 * be handled can catch us all the way up, leaving no work for
 -		 * the second flush.
 -		 */
 -		trace_tlb_flush(reason, 0);
 -		return;
 -	}
 -
 -	WARN_ON_ONCE(local_tlb_gen > mm_tlb_gen);
 -	WARN_ON_ONCE(f->new_tlb_gen > mm_tlb_gen);
 -
 -	/*
 -	 * If we get to this point, we know that our TLB is out of date.
 -	 * This does not strictly imply that we need to flush (it's
 -	 * possible that f->new_tlb_gen <= local_tlb_gen), but we're
 -	 * going to need to flush in the very near future, so we might
 -	 * as well get it over with.
 -	 *
 -	 * The only question is whether to do a full or partial flush.
 -	 *
 -	 * We do a partial flush if requested and two extra conditions
 -	 * are met:
 -	 *
 -	 * 1. f->new_tlb_gen == local_tlb_gen + 1.  We have an invariant that
 -	 *    we've always done all needed flushes to catch up to
 -	 *    local_tlb_gen.  If, for example, local_tlb_gen == 2 and
 -	 *    f->new_tlb_gen == 3, then we know that the flush needed to bring
 -	 *    us up to date for tlb_gen 3 is the partial flush we're
 -	 *    processing.
 -	 *
 -	 *    As an example of why this check is needed, suppose that there
 -	 *    are two concurrent flushes.  The first is a full flush that
 -	 *    changes context.tlb_gen from 1 to 2.  The second is a partial
 -	 *    flush that changes context.tlb_gen from 2 to 3.  If they get
 -	 *    processed on this CPU in reverse order, we'll see
 -	 *     local_tlb_gen == 1, mm_tlb_gen == 3, and end != TLB_FLUSH_ALL.
 -	 *    If we were to use __flush_tlb_single() and set local_tlb_gen to
 -	 *    3, we'd be break the invariant: we'd update local_tlb_gen above
 -	 *    1 without the full flush that's needed for tlb_gen 2.
 -	 *
 -	 * 2. f->new_tlb_gen == mm_tlb_gen.  This is purely an optimiation.
 -	 *    Partial TLB flushes are not all that much cheaper than full TLB
 -	 *    flushes, so it seems unlikely that it would be a performance win
 -	 *    to do a partial flush if that won't bring our TLB fully up to
 -	 *    date.  By doing a full flush instead, we can increase
 -	 *    local_tlb_gen all the way to mm_tlb_gen and we can probably
 -	 *    avoid another flush in the very near future.
 -	 */
 -	if (f->end != TLB_FLUSH_ALL &&
 -	    f->new_tlb_gen == local_tlb_gen + 1 &&
 -	    f->new_tlb_gen == mm_tlb_gen) {
 -		/* Partial flush */
 -		unsigned long addr;
 -		unsigned long nr_pages = (f->end - f->start) >> PAGE_SHIFT;
 -
 -		addr = f->start;
 -		while (addr < f->end) {
 -			__flush_tlb_single(addr);
 -			addr += PAGE_SIZE;
 +	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK) {
 +		if (f->flush_end == TLB_FLUSH_ALL)
 +			local_flush_tlb();
 +		else if (!f->flush_end)
 +			__flush_tlb_single(f->flush_start);
 +		else {
 +			unsigned long addr;
 +			addr = f->flush_start;
 +			while (addr < f->flush_end) {
 +				__flush_tlb_single(addr);
 +				addr += PAGE_SIZE;
 +			}
  		}
 -		if (local)
 -			count_vm_tlb_events(NR_TLB_LOCAL_FLUSH_ONE, nr_pages);
 -		trace_tlb_flush(reason, nr_pages);
 -	} else {
 -		/* Full flush. */
 -		local_flush_tlb();
 -		if (local)
 -			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
 -		trace_tlb_flush(reason, TLB_FLUSH_ALL);
 -	}
 +	} else
 +		leave_mm(smp_processor_id());
  
 -	/* Both paths above update our state to mm_tlb_gen. */
 -	this_cpu_write(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen, mm_tlb_gen);
 -}
 -
 -static void flush_tlb_func_local(void *info, enum tlb_flush_reason reason)
 -{
 -	const struct flush_tlb_info *f = info;
 -
 -	flush_tlb_func_common(f, true, reason);
 -}
 -
 -static void flush_tlb_func_remote(void *info)
 -{
 -	const struct flush_tlb_info *f = info;
 -
 -	inc_irq_stat(irq_tlb_count);
 -
 -	if (f->mm && f->mm != this_cpu_read(cpu_tlbstate.loaded_mm))
 -		return;
 -
 -	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);
 -	flush_tlb_func_common(f, false, TLB_REMOTE_SHOOTDOWN);
  }
  
  void native_flush_tlb_others(const struct cpumask *cpumask,
* Unmerged path arch/x86/entry/entry_32.S
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/entry/entry_32.S
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/mm/tlb.c
