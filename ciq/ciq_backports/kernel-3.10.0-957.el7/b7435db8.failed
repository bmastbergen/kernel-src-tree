blk-mq: Add locking annotations to hctx_lock() and hctx_unlock()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Bart Van Assche <bart.vanassche@wdc.com>
commit b7435db8b8d11df94453708295c2ea5b09caff5f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/b7435db8.failed

This patch avoids that sparse reports the following:

block/blk-mq.c:637:33: warning: context imbalance in 'hctx_unlock' - unexpected unlock
block/blk-mq.c:642:9: warning: context imbalance in 'hctx_lock' - wrong count at exit

	Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
	Cc: Tejun Heo <tj@kernel.org>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit b7435db8b8d11df94453708295c2ea5b09caff5f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 1eaa154c3ecb,8000ba6db07d..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -449,27 -558,54 +449,77 @@@ static void blk_mq_ipi_complete_request
  	put_cpu();
  }
  
++<<<<<<< HEAD
 +static void blk_mq_stat_add(struct request *rq)
++=======
+ static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
+ 	__releases(hctx->srcu)
++>>>>>>> b7435db8b8d1 (blk-mq: Add locking annotations to hctx_lock() and hctx_unlock())
  {
 -	if (!(hctx->flags & BLK_MQ_F_BLOCKING))
 -		rcu_read_unlock();
 +	if (rq->cmd_flags & REQ_STATS) {
 +		blk_mq_poll_stats_start(rq->q);
 +		blk_stat_add(rq);
 +	}
 +}
 +
 +static void __blk_mq_complete_request(struct request *rq)
 +{
 +	struct request_queue *q = rq->q;
 +
 +	if (rq_aux(rq)->internal_tag != -1)
 +		blk_mq_sched_completed_request(rq);
 +
 +	blk_mq_stat_add(rq);
 +
 +	if (!q->softirq_done_fn)
 +		blk_mq_end_request(rq, rq->errors);
  	else
++<<<<<<< HEAD
 +		blk_mq_ipi_complete_request(rq);
++=======
+ 		srcu_read_unlock(hctx->srcu, srcu_idx);
+ }
+ 
+ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
+ 	__acquires(hctx->srcu)
+ {
+ 	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
+ 		/* shut up gcc false positive */
+ 		*srcu_idx = 0;
+ 		rcu_read_lock();
+ 	} else
+ 		*srcu_idx = srcu_read_lock(hctx->srcu);
+ }
+ 
+ static void blk_mq_rq_update_aborted_gstate(struct request *rq, u64 gstate)
+ {
+ 	unsigned long flags;
+ 
+ 	/*
+ 	 * blk_mq_rq_aborted_gstate() is used from the completion path and
+ 	 * can thus be called from irq context.  u64_stats_fetch in the
+ 	 * middle of update on the same CPU leads to lockup.  Disable irq
+ 	 * while updating.
+ 	 */
+ 	local_irq_save(flags);
+ 	u64_stats_update_begin(&rq->aborted_gstate_sync);
+ 	rq->aborted_gstate = gstate;
+ 	u64_stats_update_end(&rq->aborted_gstate_sync);
+ 	local_irq_restore(flags);
+ }
+ 
+ static u64 blk_mq_rq_aborted_gstate(struct request *rq)
+ {
+ 	unsigned int start;
+ 	u64 aborted_gstate;
+ 
+ 	do {
+ 		start = u64_stats_fetch_begin(&rq->aborted_gstate_sync);
+ 		aborted_gstate = rq->aborted_gstate;
+ 	} while (u64_stats_fetch_retry(&rq->aborted_gstate_sync, start));
+ 
+ 	return aborted_gstate;
++>>>>>>> b7435db8b8d1 (blk-mq: Add locking annotations to hctx_lock() and hctx_unlock())
  }
  
  /**
* Unmerged path block/blk-mq.c
