perf/bpf: Extend the perf_event_read_local() interface, a.k.a. "bpf: perf event change needed for subsequent bpf helpers"

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Yonghong Song <yhs@fb.com>
commit 7d9285e82db5defca4d9674ba089429eeca0c697
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/7d9285e8.failed

eBPF programs would like access to the (perf) event enabled and
running times along with the event value, such that they can deal with
event multiplexing (among other things).

This patch extends the interface; a future eBPF patch will utilize
the new functionality.

[ Note, there's a same-content commit with a poor changelog and a meaningless
  title in the networking tree as well - but we need this change for subsequent
  perf work, so apply it here as well, with a proper changelog. Hopefully Git
  will be able to sort out this somewhat messy workflow, if there are no other,
  conflicting changes to these files. ]

	Signed-off-by: Yonghong Song <yhs@fb.com>
[ Rewrote the changelog. ]
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: <ast@fb.com>
	Cc: <daniel@iogearbox.net>
	Cc: <rostedt@goodmis.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: David S. Miller <davem@davemloft.net>
Link: http://lkml.kernel.org/r/20171005161923.332790-2-yhs@fb.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 7d9285e82db5defca4d9674ba089429eeca0c697)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/perf_event.h
#	kernel/bpf/arraymap.c
#	kernel/events/core.c
#	kernel/trace/bpf_trace.c
diff --cc include/linux/perf_event.h
index c93e5f6b30d7,79b18a20cf5d..000000000000
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@@ -700,9 -803,14 +700,18 @@@ struct perf_output_handle 
  	int				page;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_perf_event_data_kern {
+ 	struct pt_regs *regs;
+ 	struct perf_sample_data *data;
+ 	struct perf_event *event;
+ };
+ 
++>>>>>>> 7d9285e82db5 (perf/bpf: Extend the perf_event_read_local() interface, a.k.a. "bpf: perf event change needed for subsequent bpf helpers")
  #ifdef CONFIG_CGROUP_PERF
  
 +#ifndef __GENKSYMS__
  /*
   * perf_cgroup_info keeps track of time_enabled for a cgroup.
   * This is a per-cpu dynamically allocated data structure.
@@@ -775,7 -885,8 +784,12 @@@ perf_event_create_kernel_counter(struc
  				void *context);
  extern void perf_pmu_migrate_context(struct pmu *pmu,
  				int src_cpu, int dst_cpu);
++<<<<<<< HEAD
 +extern u64 perf_event_read_local(struct perf_event *event);
++=======
+ int perf_event_read_local(struct perf_event *event, u64 *value,
+ 			  u64 *enabled, u64 *running);
++>>>>>>> 7d9285e82db5 (perf/bpf: Extend the perf_event_read_local() interface, a.k.a. "bpf: perf event change needed for subsequent bpf helpers")
  extern u64 perf_event_read_value(struct perf_event *event,
  				 u64 *enabled, u64 *running);
  
@@@ -1132,7 -1288,11 +1146,15 @@@ static inline const struct perf_event_a
  {
  	return ERR_PTR(-EINVAL);
  }
++<<<<<<< HEAD
 +static inline u64 perf_event_read_local(struct perf_event *event)	{ return -EINVAL; }
++=======
+ static inline int perf_event_read_local(struct perf_event *event, u64 *value,
+ 					u64 *enabled, u64 *running)
+ {
+ 	return -EINVAL;
+ }
++>>>>>>> 7d9285e82db5 (perf/bpf: Extend the perf_event_read_local() interface, a.k.a. "bpf: perf event change needed for subsequent bpf helpers")
  static inline void perf_event_print_debug(void)				{ }
  static inline int perf_event_task_disable(void)				{ return -EINVAL; }
  static inline int perf_event_task_enable(void)				{ return -EINVAL; }
diff --cc kernel/events/core.c
index e490cd411934,74671e101b92..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -3638,10 -3684,12 +3638,19 @@@ static inline u64 perf_event_count(stru
   *     will not be local and we cannot read them atomically
   *   - must not have a pmu::count method
   */
++<<<<<<< HEAD
 +u64 perf_event_read_local(struct perf_event *event)
 +{
 +	unsigned long flags;
 +	u64 val;
++=======
+ int perf_event_read_local(struct perf_event *event, u64 *value,
+ 			  u64 *enabled, u64 *running)
+ {
+ 	unsigned long flags;
+ 	int ret = 0;
+ 	u64 now;
++>>>>>>> 7d9285e82db5 (perf/bpf: Extend the perf_event_read_local() interface, a.k.a. "bpf: perf event change needed for subsequent bpf helpers")
  
  	/*
  	 * Disabling interrupts avoids all counter scheduling (context
@@@ -3661,20 -3701,46 +3670,28 @@@
  	 * It must not be an event with inherit set, we cannot read
  	 * all child counters from atomic context.
  	 */
 -	if (event->attr.inherit) {
 -		ret = -EOPNOTSUPP;
 -		goto out;
 -	}
 -
 -	/* If this is a per-task event, it must be for current */
 -	if ((event->attach_state & PERF_ATTACH_TASK) &&
 -	    event->hw.target != current) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	/* If this is a per-CPU event, it must be for this CPU */
 -	if (!(event->attach_state & PERF_ATTACH_TASK) &&
 -	    event->cpu != smp_processor_id()) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 +	WARN_ON_ONCE(event->attr.inherit);
  
+ 	now = event->shadow_ctx_time + perf_clock();
+ 	if (enabled)
+ 		*enabled = now - event->tstamp_enabled;
  	/*
  	 * If the event is currently on this CPU, its either a per-task event,
  	 * or local to this CPU. Furthermore it means its ACTIVE (otherwise
  	 * oncpu == -1).
  	 */
- 	if (event->oncpu == smp_processor_id())
+ 	if (event->oncpu == smp_processor_id()) {
  		event->pmu->read(event);
+ 		if (running)
+ 			*running = now - event->tstamp_running;
+ 	} else if (running) {
+ 		*running = event->total_time_running;
+ 	}
  
 -	*value = local64_read(&event->count);
 -out:
 +	val = local64_read(&event->count);
  	local_irq_restore(flags);
  
 -	return ret;
 +	return val;
  }
  
  static int perf_event_read(struct perf_event *event, bool group)
@@@ -7739,6 -8074,135 +7756,138 @@@ static void perf_event_free_filter(stru
  	ftrace_profile_free_filter(event);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_BPF_SYSCALL
+ static void bpf_overflow_handler(struct perf_event *event,
+ 				 struct perf_sample_data *data,
+ 				 struct pt_regs *regs)
+ {
+ 	struct bpf_perf_event_data_kern ctx = {
+ 		.data = data,
+ 		.regs = regs,
+ 		.event = event,
+ 	};
+ 	int ret = 0;
+ 
+ 	preempt_disable();
+ 	if (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1))
+ 		goto out;
+ 	rcu_read_lock();
+ 	ret = BPF_PROG_RUN(event->prog, &ctx);
+ 	rcu_read_unlock();
+ out:
+ 	__this_cpu_dec(bpf_prog_active);
+ 	preempt_enable();
+ 	if (!ret)
+ 		return;
+ 
+ 	event->orig_overflow_handler(event, data, regs);
+ }
+ 
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	struct bpf_prog *prog;
+ 
+ 	if (event->overflow_handler_context)
+ 		/* hw breakpoint or kernel counter */
+ 		return -EINVAL;
+ 
+ 	if (event->prog)
+ 		return -EEXIST;
+ 
+ 	prog = bpf_prog_get_type(prog_fd, BPF_PROG_TYPE_PERF_EVENT);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	event->prog = prog;
+ 	event->orig_overflow_handler = READ_ONCE(event->overflow_handler);
+ 	WRITE_ONCE(event->overflow_handler, bpf_overflow_handler);
+ 	return 0;
+ }
+ 
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ 	struct bpf_prog *prog = event->prog;
+ 
+ 	if (!prog)
+ 		return;
+ 
+ 	WRITE_ONCE(event->overflow_handler, event->orig_overflow_handler);
+ 	event->prog = NULL;
+ 	bpf_prog_put(prog);
+ }
+ #else
+ static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ static void perf_event_free_bpf_handler(struct perf_event *event)
+ {
+ }
+ #endif
+ 
+ static int perf_event_set_bpf_prog(struct perf_event *event, u32 prog_fd)
+ {
+ 	bool is_kprobe, is_tracepoint, is_syscall_tp;
+ 	struct bpf_prog *prog;
+ 
+ 	if (event->attr.type != PERF_TYPE_TRACEPOINT)
+ 		return perf_event_set_bpf_handler(event, prog_fd);
+ 
+ 	if (event->tp_event->prog)
+ 		return -EEXIST;
+ 
+ 	is_kprobe = event->tp_event->flags & TRACE_EVENT_FL_UKPROBE;
+ 	is_tracepoint = event->tp_event->flags & TRACE_EVENT_FL_TRACEPOINT;
+ 	is_syscall_tp = is_syscall_trace_event(event->tp_event);
+ 	if (!is_kprobe && !is_tracepoint && !is_syscall_tp)
+ 		/* bpf programs can only be attached to u/kprobe or tracepoint */
+ 		return -EINVAL;
+ 
+ 	prog = bpf_prog_get(prog_fd);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	if ((is_kprobe && prog->type != BPF_PROG_TYPE_KPROBE) ||
+ 	    (is_tracepoint && prog->type != BPF_PROG_TYPE_TRACEPOINT) ||
+ 	    (is_syscall_tp && prog->type != BPF_PROG_TYPE_TRACEPOINT)) {
+ 		/* valid fd, but invalid bpf program type */
+ 		bpf_prog_put(prog);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (is_tracepoint || is_syscall_tp) {
+ 		int off = trace_event_get_offsets(event->tp_event);
+ 
+ 		if (prog->aux->max_ctx_offset > off) {
+ 			bpf_prog_put(prog);
+ 			return -EACCES;
+ 		}
+ 	}
+ 	event->tp_event->prog = prog;
+ 	event->tp_event->bpf_prog_owner = event;
+ 
+ 	return 0;
+ }
+ 
+ static void perf_event_free_bpf_prog(struct perf_event *event)
+ {
+ 	struct bpf_prog *prog;
+ 
+ 	perf_event_free_bpf_handler(event);
+ 
+ 	if (!event->tp_event)
+ 		return;
+ 
+ 	prog = event->tp_event->prog;
+ 	if (prog && event->tp_event->bpf_prog_owner == event) {
+ 		event->tp_event->prog = NULL;
+ 		bpf_prog_put(prog);
+ 	}
+ }
+ 
++>>>>>>> 7d9285e82db5 (perf/bpf: Extend the perf_event_read_local() interface, a.k.a. "bpf: perf event change needed for subsequent bpf helpers")
  #else
  
  static inline void perf_tp_register(void)
* Unmerged path kernel/bpf/arraymap.c
* Unmerged path kernel/trace/bpf_trace.c
* Unmerged path include/linux/perf_event.h
* Unmerged path kernel/bpf/arraymap.c
* Unmerged path kernel/events/core.c
* Unmerged path kernel/trace/bpf_trace.c
