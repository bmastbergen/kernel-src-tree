md: remove 'idx' from 'struct resync_pages'

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] remove 'idx' from 'struct resync_pages' (Nigel Croxon) [1494474]
Rebuild_FUZZ: 95.12%
commit-author Ming Lei <ming.lei@redhat.com>
commit 022e510fcbda79183fd2cdc01abb01b4be80d03f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/022e510f.failed

bio_add_page() won't fail for resync bio, and the page index for each
bio is same, so remove it.

More importantly the 'idx' of 'struct resync_pages' is initialized in
mempool allocator function, the current way is wrong since mempool is
only responsible for allocation, we can't use that for initialization.

	Suggested-by: NeilBrown <neilb@suse.com>
	Reported-by: NeilBrown <neilb@suse.com>
Reported-and-tested-by: Patrick <dto@gmx.net>
Fixes: f0250618361d(md: raid10: don't use bio's vec table to manage resync pages)
Fixes: 98d30c5812c3(md: raid1: don't use bio's vec table to manage resync pages)
	Cc: stable@vger.kernel.org (4.12+)
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 022e510fcbda79183fd2cdc01abb01b4be80d03f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/md.h
#	drivers/md/raid1.c
#	drivers/md/raid10.c
diff --cc drivers/md/md.h
index 6f7f96f635fa,991769cc3615..000000000000
--- a/drivers/md/md.h
+++ b/drivers/md/md.h
@@@ -725,4 -717,71 +725,74 @@@ static inline void mddev_clear_unsuppor
  {
  	mddev->flags &= ~unsupported_flags;
  }
++<<<<<<< HEAD
++=======
+ 
+ static inline void mddev_check_writesame(struct mddev *mddev, struct bio *bio)
+ {
+ 	if (bio_op(bio) == REQ_OP_WRITE_SAME &&
+ 	    !bdev_get_queue(bio->bi_bdev)->limits.max_write_same_sectors)
+ 		mddev->queue->limits.max_write_same_sectors = 0;
+ }
+ 
+ static inline void mddev_check_write_zeroes(struct mddev *mddev, struct bio *bio)
+ {
+ 	if (bio_op(bio) == REQ_OP_WRITE_ZEROES &&
+ 	    !bdev_get_queue(bio->bi_bdev)->limits.max_write_zeroes_sectors)
+ 		mddev->queue->limits.max_write_zeroes_sectors = 0;
+ }
+ 
+ /* Maximum size of each resync request */
+ #define RESYNC_BLOCK_SIZE (64*1024)
+ #define RESYNC_PAGES ((RESYNC_BLOCK_SIZE + PAGE_SIZE-1) / PAGE_SIZE)
+ 
+ /* for managing resync I/O pages */
+ struct resync_pages {
+ 	void		*raid_bio;
+ 	struct page	*pages[RESYNC_PAGES];
+ };
+ 
+ static inline int resync_alloc_pages(struct resync_pages *rp,
+ 				     gfp_t gfp_flags)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < RESYNC_PAGES; i++) {
+ 		rp->pages[i] = alloc_page(gfp_flags);
+ 		if (!rp->pages[i])
+ 			goto out_free;
+ 	}
+ 
+ 	return 0;
+ 
+ out_free:
+ 	while (--i >= 0)
+ 		put_page(rp->pages[i]);
+ 	return -ENOMEM;
+ }
+ 
+ static inline void resync_free_pages(struct resync_pages *rp)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < RESYNC_PAGES; i++)
+ 		put_page(rp->pages[i]);
+ }
+ 
+ static inline void resync_get_all_pages(struct resync_pages *rp)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < RESYNC_PAGES; i++)
+ 		get_page(rp->pages[i]);
+ }
+ 
+ static inline struct page *resync_fetch_page(struct resync_pages *rp,
+ 					     unsigned idx)
+ {
+ 	if (WARN_ON_ONCE(idx >= RESYNC_PAGES))
+ 		return NULL;
+ 	return rp->pages[idx];
+ }
++>>>>>>> 022e510fcbda (md: remove 'idx' from 'struct resync_pages')
  #endif /* _MD_MD_H */
diff --cc drivers/md/raid1.c
index e54d0416ed5a,0896c772a560..000000000000
--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@@ -127,22 -157,21 +127,35 @@@ static void * r1buf_pool_alloc(gfp_t gf
  		need_pages = pi->raid_disks;
  	else
  		need_pages = 1;
 -	for (j = 0; j < pi->raid_disks; j++) {
 -		struct resync_pages *rp = &rps[j];
 -
 +	for (j = 0; j < need_pages; j++) {
  		bio = r1_bio->bios[j];
 -
 +		bio->bi_vcnt = RESYNC_PAGES;
 +
++<<<<<<< HEAD
 +		if (bio_alloc_pages(bio, gfp_flags))
 +			goto out_free_pages;
 +	}
 +	/* If not user-requests, copy the page pointers to all bios */
 +	if (!test_bit(MD_RECOVERY_REQUESTED, &pi->mddev->recovery)) {
 +		for (i = 0; i< RESYNC_PAGES; i++)
 +			for (j = 1; j < pi->raid_disks; j++) {
 +				struct page *page =
 +					r1_bio->bios[0]->bi_io_vec[i].bv_page;
 +				get_page(page);
 +				r1_bio->bios[j]->bi_io_vec[i].bv_page = page;
 +			}
++=======
+ 		if (j < need_pages) {
+ 			if (resync_alloc_pages(rp, gfp_flags))
+ 				goto out_free_pages;
+ 		} else {
+ 			memcpy(rp, &rps[0], sizeof(*rp));
+ 			resync_get_all_pages(rp);
+ 		}
+ 
+ 		rp->raid_bio = r1_bio;
+ 		bio->bi_private = rp;
++>>>>>>> 022e510fcbda (md: remove 'idx' from 'struct resync_pages')
  	}
  
  	r1_bio->master_bio = NULL;
@@@ -2834,33 -2841,37 +2848,48 @@@ static sector_t raid1_sync_request(stru
  		}
  
  		for (i = 0 ; i < conf->raid_disks * 2; i++) {
 -			struct resync_pages *rp;
 -
  			bio = r1_bio->bios[i];
 -			rp = get_resync_pages(bio);
  			if (bio->bi_end_io) {
++<<<<<<< HEAD
 +				page = bio->bi_io_vec[bio->bi_vcnt].bv_page;
 +				if (bio_add_page(bio, page, len, 0) == 0) {
 +					/* stop here */
 +					bio->bi_io_vec[bio->bi_vcnt].bv_page = page;
 +					while (i > 0) {
 +						i--;
 +						bio = r1_bio->bios[i];
 +						if (bio->bi_end_io==NULL)
 +							continue;
 +						/* remove last page from this bio */
 +						bio->bi_vcnt--;
 +						bio->bi_size -= len;
 +						__clear_bit(BIO_SEG_VALID, &bio->bi_flags);
 +					}
 +					goto bio_full;
 +				}
++=======
+ 				page = resync_fetch_page(rp, page_idx);
+ 
+ 				/*
+ 				 * won't fail because the vec table is big
+ 				 * enough to hold all these pages
+ 				 */
+ 				bio_add_page(bio, page, len, 0);
++>>>>>>> 022e510fcbda (md: remove 'idx' from 'struct resync_pages')
  			}
  		}
  		nr_sectors += len>>9;
  		sector_nr += len>>9;
  		sync_blocks -= (len>>9);
++<<<<<<< HEAD
 +	} while (r1_bio->bios[disk]->bi_vcnt < RESYNC_PAGES);
 + bio_full:
++=======
+ 	} while (++page_idx < RESYNC_PAGES);
+ 
++>>>>>>> 022e510fcbda (md: remove 'idx' from 'struct resync_pages')
  	r1_bio->sectors = nr_sectors;
  
 -	if (mddev_is_clustered(mddev) &&
 -			conf->cluster_sync_high < sector_nr + nr_sectors) {
 -		conf->cluster_sync_low = mddev->curr_resync_completed;
 -		conf->cluster_sync_high = conf->cluster_sync_low + CLUSTER_RESYNC_WINDOW_SECTORS;
 -		/* Send resync message */
 -		md_cluster_ops->resync_info_update(mddev,
 -				conf->cluster_sync_low,
 -				conf->cluster_sync_high);
 -	}
 -
  	/* For a user-requested sync, we read all readable devices and do a
  	 * compare
  	 */
diff --cc drivers/md/raid10.c
index 17d84aee79e2,fa8bcf04e791..000000000000
--- a/drivers/md/raid10.c
+++ b/drivers/md/raid10.c
@@@ -174,25 -202,30 +174,33 @@@ static void * r10buf_pool_alloc(gfp_t g
  	 * Allocate RESYNC_PAGES data pages and attach them
  	 * where needed.
  	 */
 -	for (j = 0; j < nalloc; j++) {
 +	for (j = 0 ; j < nalloc; j++) {
  		struct bio *rbio = r10_bio->devs[j].repl_bio;
 -		struct resync_pages *rp, *rp_repl;
 -
 -		rp = &rps[j];
 -		if (rbio)
 -			rp_repl = &rps[nalloc + j];
 -
  		bio = r10_bio->devs[j].bio;
 -
 -		if (!j || test_bit(MD_RECOVERY_SYNC,
 -				   &conf->mddev->recovery)) {
 -			if (resync_alloc_pages(rp, gfp_flags))
 +		for (i = 0; i < RESYNC_PAGES; i++) {
 +			if (j > 0 && !test_bit(MD_RECOVERY_SYNC,
 +					       &conf->mddev->recovery)) {
 +				/* we can share bv_page's during recovery
 +				 * and reshape */
 +				struct bio *rbio = r10_bio->devs[0].bio;
 +				page = rbio->bi_io_vec[i].bv_page;
 +				get_page(page);
 +			} else
 +				page = alloc_page(gfp_flags);
 +			if (unlikely(!page))
  				goto out_free_pages;
 -		} else {
 -			memcpy(rp, &rps[0], sizeof(*rp));
 -			resync_get_all_pages(rp);
 -		}
  
++<<<<<<< HEAD
 +			bio->bi_io_vec[i].bv_page = page;
 +			if (rbio)
 +				rbio->bi_io_vec[i].bv_page = page;
++=======
+ 		rp->raid_bio = r10_bio;
+ 		bio->bi_private = rp;
+ 		if (rbio) {
+ 			memcpy(rp_repl, rp, sizeof(*rp));
+ 			rbio->bi_private = rp_repl;
++>>>>>>> 022e510fcbda (md: remove 'idx' from 'struct resync_pages')
  		}
  	}
  
@@@ -3511,27 -3354,17 +3520,41 @@@ static sector_t raid10_sync_request(str
  		if (len == 0)
  			break;
  		for (bio= biolist ; bio ; bio=bio->bi_next) {
++<<<<<<< HEAD
 +			struct bio *bio2;
 +			page = bio->bi_io_vec[bio->bi_vcnt].bv_page;
 +			if (bio_add_page(bio, page, len, 0))
 +				continue;
 +
 +			/* stop here */
 +			bio->bi_io_vec[bio->bi_vcnt].bv_page = page;
 +			for (bio2 = biolist;
 +			     bio2 && bio2 != bio;
 +			     bio2 = bio2->bi_next) {
 +				/* remove last page from this bio */
 +				bio2->bi_vcnt--;
 +				bio2->bi_size -= len;
 +				__clear_bit(BIO_SEG_VALID, &bio2->bi_flags);
 +			}
 +			goto bio_full;
 +		}
 +		nr_sectors += len>>9;
 +		sector_nr += len>>9;
 +	} while (biolist->bi_vcnt < RESYNC_PAGES);
 + bio_full:
++=======
+ 			struct resync_pages *rp = get_resync_pages(bio);
+ 			page = resync_fetch_page(rp, page_idx);
+ 			/*
+ 			 * won't fail because the vec table is big enough
+ 			 * to hold all these pages
+ 			 */
+ 			bio_add_page(bio, page, len, 0);
+ 		}
+ 		nr_sectors += len>>9;
+ 		sector_nr += len>>9;
+ 	} while (++page_idx < RESYNC_PAGES);
++>>>>>>> 022e510fcbda (md: remove 'idx' from 'struct resync_pages')
  	r10_bio->sectors = nr_sectors;
  
  	while (biolist) {
* Unmerged path drivers/md/md.h
* Unmerged path drivers/md/raid1.c
* Unmerged path drivers/md/raid10.c
