tcp: increment sk_drops for dropped rx packets

jira LE-1907
cve CVE-2018-5390
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 532182cd610782db8c18230c2747626562032205
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/532182cd.failed

Now ss can report sk_drops, we can instruct TCP to increment
this per socket counter when it drops an incoming frame, to refine
monitoring and debugging.

Following patch takes care of listeners drops.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 532182cd610782db8c18230c2747626562032205)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/sock.h
#	net/ipv4/tcp_ipv4.c
#	net/ipv6/tcp_ipv6.c
diff --cc include/net/sock.h
index 91144cd664b8,310c4367ea83..000000000000
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@@ -2291,13 -1992,37 +2291,27 @@@ static inline int sock_intr_errno(long 
  static inline void
  sock_skb_set_dropcount(const struct sock *sk, struct sk_buff *skb)
  {
 -	SOCK_SKB_CB(skb)->dropcount = atomic_read(&sk->sk_drops);
 +	skb->dropcount = atomic_read(&sk->sk_drops);
  }
  
++<<<<<<< HEAD
 +extern void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
 +	struct sk_buff *skb);
 +extern void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
 +	struct sk_buff *skb);
++=======
+ static inline void sk_drops_add(struct sock *sk, const struct sk_buff *skb)
+ {
+ 	int segs = max_t(u16, 1, skb_shinfo(skb)->gso_segs);
+ 
+ 	atomic_add(segs, &sk->sk_drops);
+ }
+ 
+ void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
+ 			   struct sk_buff *skb);
+ void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
+ 			     struct sk_buff *skb);
++>>>>>>> 532182cd6107 (tcp: increment sk_drops for dropped rx packets)
  
  static inline void
  sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
diff --cc net/ipv4/tcp_ipv4.c
index b635ea161b80,059a98f5e7e1..000000000000
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@@ -1746,7 -1694,9 +1746,13 @@@ discard_it
  	return 0;
  
  discard_and_relse:
++<<<<<<< HEAD
 +	sock_put(sk);
++=======
+ 	sk_drops_add(sk, skb);
+ 	if (refcounted)
+ 		sock_put(sk);
++>>>>>>> 532182cd6107 (tcp: increment sk_drops for dropped rx packets)
  	goto discard_it;
  
  do_time_wait:
diff --cc net/ipv6/tcp_ipv6.c
index 90cee4fdd27b,5fa8fea394c9..000000000000
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@@ -1488,7 -1484,9 +1488,13 @@@ discard_it
  	return 0;
  
  discard_and_relse:
++<<<<<<< HEAD
 +	sock_put(sk);
++=======
+ 	sk_drops_add(sk, skb);
+ 	if (refcounted)
+ 		sock_put(sk);
++>>>>>>> 532182cd6107 (tcp: increment sk_drops for dropped rx packets)
  	goto discard_it;
  
  do_time_wait:
* Unmerged path include/net/sock.h
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index 4ec1e4f8ab44..ca9dbb902ab3 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -4226,6 +4226,12 @@ static bool tcp_try_coalesce(struct sock *sk,
 	return true;
 }
 
+static void tcp_drop(struct sock *sk, struct sk_buff *skb)
+{
+	sk_drops_add(sk, skb);
+	__kfree_skb(skb);
+}
+
 /* This one checks to see if we can put data from the
  * out_of_order queue into the receive_queue.
  */
@@ -4250,7 +4256,7 @@ static void tcp_ofo_queue(struct sock *sk)
 		__skb_unlink(skb, &tp->out_of_order_queue);
 		if (!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt)) {
 			SOCK_DEBUG(sk, "ofo packet was already received\n");
-			__kfree_skb(skb);
+			tcp_drop(sk, skb);
 			continue;
 		}
 		SOCK_DEBUG(sk, "ofo requeuing : rcv_next %X seq %X - %X\n",
@@ -4302,7 +4308,7 @@ static void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)
 
 	if (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {
 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFODROP);
-		__kfree_skb(skb);
+		tcp_drop(sk, skb);
 		return;
 	}
 
@@ -4366,7 +4372,7 @@ static void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)
 		if (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {
 			/* All the bits are present. Drop. */
 			NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFOMERGE);
-			__kfree_skb(skb);
+			tcp_drop(sk, skb);
 			skb = NULL;
 			tcp_dsack_set(sk, seq, end_seq);
 			goto add_sack;
@@ -4405,7 +4411,7 @@ static void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)
 		tcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,
 				 TCP_SKB_CB(skb1)->end_seq);
 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPOFOMERGE);
-		__kfree_skb(skb1);
+		tcp_drop(sk, skb1);
 	}
 
 add_sack:
@@ -4472,12 +4478,13 @@ err:
 static void tcp_data_queue(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
-	int eaten = -1;
 	bool fragstolen = false;
+	int eaten = -1;
 
-	if (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq)
-		goto drop;
-
+	if (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq) {
+		__kfree_skb(skb);
+		return;
+	}
 	skb_dst_drop(skb);
 	__skb_pull(skb, tcp_hdr(skb)->doff * 4);
 
@@ -4559,7 +4566,7 @@ out_of_window:
 		tcp_enter_quickack_mode(sk);
 		inet_csk_schedule_ack(sk);
 drop:
-		__kfree_skb(skb);
+		tcp_drop(sk, skb);
 		return;
 	}
 
@@ -5135,7 +5142,7 @@ syn_challenge:
 	return true;
 
 discard:
-	__kfree_skb(skb);
+	tcp_drop(sk, skb);
 	return false;
 }
 
@@ -5353,7 +5360,7 @@ csum_error:
 	TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
 
 discard:
-	__kfree_skb(skb);
+	tcp_drop(sk, skb);
 }
 EXPORT_SYMBOL(tcp_rcv_established);
 
@@ -5581,7 +5588,7 @@ static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 						  TCP_DELACK_MAX, TCP_RTO_MAX);
 
 discard:
-			__kfree_skb(skb);
+			tcp_drop(sk, skb);
 			return 0;
 		} else {
 			tcp_send_ack(sk);
@@ -5940,7 +5947,7 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 
 	if (!queued) {
 discard:
-		__kfree_skb(skb);
+		tcp_drop(sk, skb);
 	}
 	return 0;
 }
* Unmerged path net/ipv4/tcp_ipv4.c
* Unmerged path net/ipv6/tcp_ipv6.c
