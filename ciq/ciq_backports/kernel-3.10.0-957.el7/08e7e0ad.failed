iio: buffer: Allocate standard attributes in the core

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iio] buffer: Allocate standard attributes in the core (Tony Camuso) [1559170]
Rebuild_FUZZ: 95.05%
commit-author Lars-Peter Clausen <lars@metafoo.de>
commit 08e7e0adaa17205f86894157d86c4bee3c714330
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/08e7e0ad.failed

All buffers want at least the length and the enable attribute. Move the
creation of those attributes to the core instead of having to do this in
each individual buffer implementation. This allows us to get rid of some
boiler-plate code.

	Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
	Signed-off-by: Jonathan Cameron <jic23@kernel.org>
(cherry picked from commit 08e7e0adaa17205f86894157d86c4bee3c714330)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iio/industrialio-buffer.c
#	drivers/iio/kfifo_buf.c
#	include/linux/iio/buffer.h
diff --cc drivers/iio/industrialio-buffer.c
index 5ef822987c05,ba89357fc096..000000000000
--- a/drivers/iio/industrialio-buffer.c
+++ b/drivers/iio/industrialio-buffer.c
@@@ -386,99 -383,9 +386,105 @@@ error_ret
  	return ret;
  }
  
++<<<<<<< HEAD
 +static const char * const iio_scan_elements_group_name = "scan_elements";
 +
 +int iio_buffer_register(struct iio_dev *indio_dev,
 +			const struct iio_chan_spec *channels,
 +			int num_channels)
 +{
 +	struct iio_dev_attr *p;
 +	struct attribute **attr;
 +	struct iio_buffer *buffer = indio_dev->buffer;
 +	int ret, i, attrn, attrcount, attrcount_orig = 0;
 +
 +	if (buffer->attrs)
 +		indio_dev->groups[indio_dev->groupcounter++] = buffer->attrs;
 +
 +	if (buffer->scan_el_attrs != NULL) {
 +		attr = buffer->scan_el_attrs->attrs;
 +		while (*attr++ != NULL)
 +			attrcount_orig++;
 +	}
 +	attrcount = attrcount_orig;
 +	INIT_LIST_HEAD(&buffer->scan_el_dev_attr_list);
 +	if (channels) {
 +		/* new magic */
 +		for (i = 0; i < num_channels; i++) {
 +			if (channels[i].scan_index < 0)
 +				continue;
 +
 +			/* Establish necessary mask length */
 +			if (channels[i].scan_index >
 +			    (int)indio_dev->masklength - 1)
 +				indio_dev->masklength
 +					= channels[i].scan_index + 1;
 +
 +			ret = iio_buffer_add_channel_sysfs(indio_dev,
 +							 &channels[i]);
 +			if (ret < 0)
 +				goto error_cleanup_dynamic;
 +			attrcount += ret;
 +			if (channels[i].type == IIO_TIMESTAMP)
 +				indio_dev->scan_index_timestamp =
 +					channels[i].scan_index;
 +		}
 +		if (indio_dev->masklength && buffer->scan_mask == NULL) {
 +			buffer->scan_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),
 +						    sizeof(*buffer->scan_mask),
 +						    GFP_KERNEL);
 +			if (buffer->scan_mask == NULL) {
 +				ret = -ENOMEM;
 +				goto error_cleanup_dynamic;
 +			}
 +		}
 +	}
 +
 +	buffer->scan_el_group.name = iio_scan_elements_group_name;
 +
 +	buffer->scan_el_group.attrs = kcalloc(attrcount + 1,
 +					      sizeof(buffer->scan_el_group.attrs[0]),
 +					      GFP_KERNEL);
 +	if (buffer->scan_el_group.attrs == NULL) {
 +		ret = -ENOMEM;
 +		goto error_free_scan_mask;
 +	}
 +	if (buffer->scan_el_attrs)
 +		memcpy(buffer->scan_el_group.attrs, buffer->scan_el_attrs,
 +		       sizeof(buffer->scan_el_group.attrs[0])*attrcount_orig);
 +	attrn = attrcount_orig;
 +
 +	list_for_each_entry(p, &buffer->scan_el_dev_attr_list, l)
 +		buffer->scan_el_group.attrs[attrn++] = &p->dev_attr.attr;
 +	indio_dev->groups[indio_dev->groupcounter++] = &buffer->scan_el_group;
 +
 +	return 0;
 +
 +error_free_scan_mask:
 +	kfree(buffer->scan_mask);
 +error_cleanup_dynamic:
 +	iio_free_chan_devattr_list(&buffer->scan_el_dev_attr_list);
 +
 +	return ret;
 +}
 +EXPORT_SYMBOL(iio_buffer_register);
 +
 +void iio_buffer_unregister(struct iio_dev *indio_dev)
 +{
 +	kfree(indio_dev->buffer->scan_mask);
 +	kfree(indio_dev->buffer->scan_el_group.attrs);
 +	iio_free_chan_devattr_list(&indio_dev->buffer->scan_el_dev_attr_list);
 +}
 +EXPORT_SYMBOL(iio_buffer_unregister);
 +
 +ssize_t iio_buffer_read_length(struct device *dev,
 +			       struct device_attribute *attr,
 +			       char *buf)
++=======
+ static ssize_t iio_buffer_read_length(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
++>>>>>>> 08e7e0adaa17 (iio: buffer: Allocate standard attributes in the core)
  {
  	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
  	struct iio_buffer *buffer = indio_dev->buffer;
@@@ -835,8 -755,126 +837,129 @@@ done
  	mutex_unlock(&indio_dev->mlock);
  	return (ret < 0) ? ret : len;
  }
- EXPORT_SYMBOL(iio_buffer_store_enable);
  
++<<<<<<< HEAD
++=======
+ static const char * const iio_scan_elements_group_name = "scan_elements";
+ 
+ static DEVICE_ATTR(length, S_IRUGO | S_IWUSR, iio_buffer_read_length,
+ 		   iio_buffer_write_length);
+ static DEVICE_ATTR(enable, S_IRUGO | S_IWUSR,
+ 		   iio_buffer_show_enable, iio_buffer_store_enable);
+ 
+ int iio_buffer_alloc_sysfs_and_mask(struct iio_dev *indio_dev)
+ {
+ 	struct iio_dev_attr *p;
+ 	struct attribute **attr;
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 	int ret, i, attrn, attrcount, attrcount_orig = 0;
+ 	const struct iio_chan_spec *channels;
+ 
+ 	if (!buffer)
+ 		return 0;
+ 
+ 	attrcount = 0;
+ 	if (buffer->attrs) {
+ 		while (buffer->attrs[attrcount] != NULL)
+ 			attrcount++;
+ 	}
+ 
+ 	buffer->buffer_group.name = "buffer";
+ 	buffer->buffer_group.attrs = kcalloc(attrcount + 3,
+ 			sizeof(*buffer->buffer_group.attrs), GFP_KERNEL);
+ 	if (!buffer->buffer_group.attrs)
+ 		return -ENOMEM;
+ 
+ 	buffer->buffer_group.attrs[0] = &dev_attr_length.attr;
+ 	buffer->buffer_group.attrs[1] = &dev_attr_enable.attr;
+ 	if (buffer->attrs)
+ 		memcpy(&buffer->buffer_group.attrs[2], buffer->attrs,
+ 			sizeof(*&buffer->buffer_group.attrs) * (attrcount - 2));
+ 	buffer->buffer_group.attrs[attrcount+2] = NULL;
+ 
+ 	indio_dev->groups[indio_dev->groupcounter++] = &buffer->buffer_group;
+ 
+ 	if (buffer->scan_el_attrs != NULL) {
+ 		attr = buffer->scan_el_attrs->attrs;
+ 		while (*attr++ != NULL)
+ 			attrcount_orig++;
+ 	}
+ 	attrcount = attrcount_orig;
+ 	INIT_LIST_HEAD(&buffer->scan_el_dev_attr_list);
+ 	channels = indio_dev->channels;
+ 	if (channels) {
+ 		/* new magic */
+ 		for (i = 0; i < indio_dev->num_channels; i++) {
+ 			if (channels[i].scan_index < 0)
+ 				continue;
+ 
+ 			/* Establish necessary mask length */
+ 			if (channels[i].scan_index >
+ 			    (int)indio_dev->masklength - 1)
+ 				indio_dev->masklength
+ 					= channels[i].scan_index + 1;
+ 
+ 			ret = iio_buffer_add_channel_sysfs(indio_dev,
+ 							 &channels[i]);
+ 			if (ret < 0)
+ 				goto error_cleanup_dynamic;
+ 			attrcount += ret;
+ 			if (channels[i].type == IIO_TIMESTAMP)
+ 				indio_dev->scan_index_timestamp =
+ 					channels[i].scan_index;
+ 		}
+ 		if (indio_dev->masklength && buffer->scan_mask == NULL) {
+ 			buffer->scan_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),
+ 						    sizeof(*buffer->scan_mask),
+ 						    GFP_KERNEL);
+ 			if (buffer->scan_mask == NULL) {
+ 				ret = -ENOMEM;
+ 				goto error_cleanup_dynamic;
+ 			}
+ 		}
+ 	}
+ 
+ 	buffer->scan_el_group.name = iio_scan_elements_group_name;
+ 
+ 	buffer->scan_el_group.attrs = kcalloc(attrcount + 1,
+ 					      sizeof(buffer->scan_el_group.attrs[0]),
+ 					      GFP_KERNEL);
+ 	if (buffer->scan_el_group.attrs == NULL) {
+ 		ret = -ENOMEM;
+ 		goto error_free_scan_mask;
+ 	}
+ 	if (buffer->scan_el_attrs)
+ 		memcpy(buffer->scan_el_group.attrs, buffer->scan_el_attrs,
+ 		       sizeof(buffer->scan_el_group.attrs[0])*attrcount_orig);
+ 	attrn = attrcount_orig;
+ 
+ 	list_for_each_entry(p, &buffer->scan_el_dev_attr_list, l)
+ 		buffer->scan_el_group.attrs[attrn++] = &p->dev_attr.attr;
+ 	indio_dev->groups[indio_dev->groupcounter++] = &buffer->scan_el_group;
+ 
+ 	return 0;
+ 
+ error_free_scan_mask:
+ 	kfree(buffer->scan_mask);
+ error_cleanup_dynamic:
+ 	iio_free_chan_devattr_list(&buffer->scan_el_dev_attr_list);
+ 	kfree(indio_dev->buffer->buffer_group.attrs);
+ 
+ 	return ret;
+ }
+ 
+ void iio_buffer_free_sysfs_and_mask(struct iio_dev *indio_dev)
+ {
+ 	if (!indio_dev->buffer)
+ 		return;
+ 
+ 	kfree(indio_dev->buffer->scan_mask);
+ 	kfree(indio_dev->buffer->buffer_group.attrs);
+ 	kfree(indio_dev->buffer->scan_el_group.attrs);
+ 	iio_free_chan_devattr_list(&indio_dev->buffer->scan_el_dev_attr_list);
+ }
+ 
++>>>>>>> 08e7e0adaa17 (iio: buffer: Allocate standard attributes in the core)
  /**
   * iio_validate_scan_mask_onehot() - Validates that exactly one channel is selected
   * @indio_dev: the iio device
diff --cc drivers/iio/kfifo_buf.c
index 1bea41bcbdc6,3b0a3bc4f0ad..000000000000
--- a/drivers/iio/kfifo_buf.c
+++ b/drivers/iio/kfifo_buf.c
@@@ -47,25 -52,6 +47,28 @@@ static int iio_get_length_kfifo(struct 
  	return r->length;
  }
  
++<<<<<<< HEAD
 +static IIO_BUFFER_ENABLE_ATTR;
 +static IIO_BUFFER_LENGTH_ATTR;
 +
 +static struct attribute *iio_kfifo_attributes[] = {
 +	&dev_attr_length.attr,
 +	&dev_attr_enable.attr,
 +	NULL,
 +};
 +
 +static struct attribute_group iio_kfifo_attribute_group = {
 +	.attrs = iio_kfifo_attributes,
 +	.name = "buffer",
 +};
 +
 +static int iio_get_bytes_per_datum_kfifo(struct iio_buffer *r)
 +{
 +	return r->bytes_per_datum;
 +}
 +
++=======
++>>>>>>> 08e7e0adaa17 (iio: buffer: Allocate standard attributes in the core)
  static int iio_mark_update_needed_kfifo(struct iio_buffer *r)
  {
  	struct iio_kfifo *kf = iio_to_kfifo(r);
@@@ -149,9 -155,9 +152,8 @@@ struct iio_buffer *iio_kfifo_allocate(s
  		return NULL;
  	kf->update_needed = true;
  	iio_buffer_init(&kf->buffer);
- 	kf->buffer.attrs = &iio_kfifo_attribute_group;
  	kf->buffer.access = &kfifo_access_funcs;
  	kf->buffer.length = 2;
 -	mutex_init(&kf->user_lock);
  	return &kf->buffer;
  }
  EXPORT_SYMBOL(iio_kfifo_allocate);
diff --cc include/linux/iio/buffer.h
index 26890e4a025c,16b7663036f2..000000000000
--- a/include/linux/iio/buffer.h
+++ b/include/linux/iio/buffer.h
@@@ -82,10 -87,11 +83,10 @@@ struct iio_buffer 
  	struct attribute_group			scan_el_group;
  	wait_queue_head_t			pollq;
  	bool					stufftoread;
- 	const struct attribute_group *attrs;
+ 	const struct attribute			**attrs;
  	struct list_head			demux_list;
 -	void					*demux_bounce;
 +	unsigned char				*demux_bounce;
  	struct list_head			buffer_list;
 -	struct kref				ref;
  };
  
  /**
@@@ -143,56 -149,6 +144,59 @@@ static inline int iio_push_to_buffers_w
  
  int iio_update_demux(struct iio_dev *indio_dev);
  
++<<<<<<< HEAD
 +/**
 + * iio_buffer_register() - register the buffer with IIO core
 + * @indio_dev:		device with the buffer to be registered
 + * @channels:		the channel descriptions used to construct buffer
 + * @num_channels:	the number of channels
 + **/
 +int iio_buffer_register(struct iio_dev *indio_dev,
 +			const struct iio_chan_spec *channels,
 +			int num_channels);
 +
 +/**
 + * iio_buffer_unregister() - unregister the buffer from IIO core
 + * @indio_dev:		the device with the buffer to be unregistered
 + **/
 +void iio_buffer_unregister(struct iio_dev *indio_dev);
 +
 +/**
 + * iio_buffer_read_length() - attr func to get number of datums in the buffer
 + **/
 +ssize_t iio_buffer_read_length(struct device *dev,
 +			       struct device_attribute *attr,
 +			       char *buf);
 +/**
 + * iio_buffer_write_length() - attr func to set number of datums in the buffer
 + **/
 +ssize_t iio_buffer_write_length(struct device *dev,
 +			      struct device_attribute *attr,
 +			      const char *buf,
 +			      size_t len);
 +/**
 + * iio_buffer_store_enable() - attr to turn the buffer on
 + **/
 +ssize_t iio_buffer_store_enable(struct device *dev,
 +				struct device_attribute *attr,
 +				const char *buf,
 +				size_t len);
 +/**
 + * iio_buffer_show_enable() - attr to see if the buffer is on
 + **/
 +ssize_t iio_buffer_show_enable(struct device *dev,
 +			       struct device_attribute *attr,
 +			       char *buf);
 +#define IIO_BUFFER_LENGTH_ATTR DEVICE_ATTR(length, S_IRUGO | S_IWUSR,	\
 +					   iio_buffer_read_length,	\
 +					   iio_buffer_write_length)
 +
 +#define IIO_BUFFER_ENABLE_ATTR DEVICE_ATTR(enable, S_IRUGO | S_IWUSR,	\
 +					   iio_buffer_show_enable,	\
 +					   iio_buffer_store_enable)
 +
++=======
++>>>>>>> 08e7e0adaa17 (iio: buffer: Allocate standard attributes in the core)
  bool iio_validate_scan_mask_onehot(struct iio_dev *indio_dev,
  	const unsigned long *mask);
  
* Unmerged path drivers/iio/industrialio-buffer.c
* Unmerged path drivers/iio/kfifo_buf.c
diff --git a/drivers/staging/iio/accel/sca3000_ring.c b/drivers/staging/iio/accel/sca3000_ring.c
index 3e5e860aa38e..5c2ce4961e80 100644
--- a/drivers/staging/iio/accel/sca3000_ring.c
+++ b/drivers/staging/iio/accel/sca3000_ring.c
@@ -141,9 +141,6 @@ static int sca3000_ring_get_bytes_per_datum(struct iio_buffer *r)
 	return 6;
 }
 
-static IIO_BUFFER_ENABLE_ATTR;
-static IIO_BUFFER_LENGTH_ATTR;
-
 /**
  * sca3000_query_ring_int() is the hardware ring status interrupt enabled
  **/
@@ -233,20 +230,13 @@ static IIO_DEVICE_ATTR(in_accel_scale,
  * only apply to the ring buffer.  At all times full rate and accuracy
  * is available via direct reading from registers.
  */
-static struct attribute *sca3000_ring_attributes[] = {
-	&dev_attr_length.attr,
-	&dev_attr_enable.attr,
+static const struct attribute *sca3000_ring_attributes[] = {
 	&iio_dev_attr_50_percent.dev_attr.attr,
 	&iio_dev_attr_75_percent.dev_attr.attr,
 	&iio_dev_attr_in_accel_scale.dev_attr.attr,
 	NULL,
 };
 
-static struct attribute_group sca3000_ring_attr = {
-	.attrs = sca3000_ring_attributes,
-	.name = "buffer",
-};
-
 static struct iio_buffer *sca3000_rb_allocate(struct iio_dev *indio_dev)
 {
 	struct iio_buffer *buf;
@@ -259,7 +249,7 @@ static struct iio_buffer *sca3000_rb_allocate(struct iio_dev *indio_dev)
 	ring->private = indio_dev;
 	buf = &ring->buf;
 	buf->stufftoread = 0;
-	buf->attrs = &sca3000_ring_attr;
+	buf->attrs = sca3000_ring_attributes;
 	iio_buffer_init(buf);
 
 	return buf;
* Unmerged path include/linux/iio/buffer.h
