x86/spectre_v1: Disable compiler optimizations over array_index_mask_nospec()

jira LE-1907
cve CVE-2018-3693
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] spectre_v1: Disable compiler optimizations over array_index_mask_nospec() (Lauro Ramos Venancio) [1589035] {CVE-2018-3693}
Rebuild_FUZZ: 97.33%
commit-author Dan Williams <dan.j.williams@intel.com>
commit eab6870fee877258122a042bfd99ee7908c40280
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/eab6870f.failed

Mark Rutland noticed that GCC optimization passes have the potential to elide
necessary invocations of the array_index_mask_nospec() instruction sequence,
so mark the asm() volatile.

Mark explains:

"The volatile will inhibit *some* cases where the compiler could lift the
 array_index_nospec() call out of a branch, e.g. where there are multiple
 invocations of array_index_nospec() with the same arguments:

        if (idx < foo) {
                idx1 = array_idx_nospec(idx, foo)
                do_something(idx1);
        }

        < some other code >

        if (idx < foo) {
                idx2 = array_idx_nospec(idx, foo);
                do_something_else(idx2);
        }

 ... since the compiler can determine that the two invocations yield the same
 result, and reuse the first result (likely the same register as idx was in
 originally) for the second branch, effectively re-writing the above as:

        if (idx < foo) {
                idx = array_idx_nospec(idx, foo);
                do_something(idx);
        }

        < some other code >

        if (idx < foo) {
                do_something_else(idx);
        }

 ... if we don't take the first branch, then speculatively take the second, we
 lose the nospec protection.

 There's more info on volatile asm in the GCC docs:

   https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html#Volatile
 "

	Reported-by: Mark Rutland <mark.rutland@arm.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Acked-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: <stable@vger.kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
Fixes: babdde2698d4 ("x86: Implement array_index_mask_nospec")
Link: https://lkml.kernel.org/lkml/152838798950.14521.4893346294059739135.stgit@dwillia2-desk3.amr.corp.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit eab6870fee877258122a042bfd99ee7908c40280)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/barrier.h
diff --cc arch/x86/include/asm/barrier.h
index 4bf86772da50,14de0432d288..000000000000
--- a/arch/x86/include/asm/barrier.h
+++ b/arch/x86/include/asm/barrier.h
@@@ -24,55 -24,47 +24,85 @@@
  #define wmb()	asm volatile("sfence" ::: "memory")
  #endif
  
++<<<<<<< HEAD
 +#define gmb() alternative(ASM_NOP3, "lfence", X86_FEATURE_LFENCE_RDTSC)
++=======
+ /**
+  * array_index_mask_nospec() - generate a mask that is ~0UL when the
+  * 	bounds check succeeds and 0 otherwise
+  * @index: array element index
+  * @size: number of elements in array
+  *
+  * Returns:
+  *     0 - (index < size)
+  */
+ static inline unsigned long array_index_mask_nospec(unsigned long index,
+ 		unsigned long size)
+ {
+ 	unsigned long mask;
+ 
+ 	asm volatile ("cmp %1,%2; sbb %0,%0;"
+ 			:"=r" (mask)
+ 			:"g"(size),"r" (index)
+ 			:"cc");
+ 	return mask;
+ }
+ 
+ /* Override the default implementation from linux/nospec.h. */
+ #define array_index_mask_nospec array_index_mask_nospec
+ 
+ /* Prevent speculative execution past this barrier. */
+ #define barrier_nospec() alternative_2("", "mfence", X86_FEATURE_MFENCE_RDTSC, \
+ 					   "lfence", X86_FEATURE_LFENCE_RDTSC)
++>>>>>>> eab6870fee87 (x86/spectre_v1: Disable compiler optimizations over array_index_mask_nospec())
  
 +#ifdef CONFIG_X86_PPRO_FENCE
 +#define dma_rmb()	rmb()
 +#else
  #define dma_rmb()	barrier()
 +#endif
  #define dma_wmb()	barrier()
  
 -#ifdef CONFIG_X86_32
 -#define __smp_mb()	asm volatile("lock; addl $0,-4(%%esp)" ::: "memory", "cc")
 -#else
 -#define __smp_mb()	asm volatile("lock; addl $0,-4(%%rsp)" ::: "memory", "cc")
 -#endif
 -#define __smp_rmb()	dma_rmb()
 -#define __smp_wmb()	barrier()
 -#define __smp_store_mb(var, value) do { (void)xchg(&var, value); } while (0)
 +#ifdef CONFIG_SMP
 +#define smp_mb()	mb()
 +#define smp_rmb()	dma_rmb()
 +#define smp_wmb()	barrier()
 +#define set_mb(var, value) do { (void)xchg(&var, value); } while (0)
 +#else /* !SMP */
 +#define smp_mb()	barrier()
 +#define smp_rmb()	barrier()
 +#define smp_wmb()	barrier()
 +#define set_mb(var, value) do { var = value; barrier(); } while (0)
 +#endif /* SMP */
 +
 +#define read_barrier_depends()		do { } while (0)
 +#define smp_read_barrier_depends()	do { } while (0)
  
 -#define __smp_store_release(p, v)					\
 +#if defined(CONFIG_X86_PPRO_FENCE)
 +
 +/*
 + * For either of these options x86 doesn't have a strong TSO memory
 + * model and we should fall back to full barriers.
 + */
 +
 +#define smp_store_release(p, v)						\
 +do {									\
 +	compiletime_assert_atomic_type(*p);				\
 +	smp_mb();							\
 +	ACCESS_ONCE(*p) = (v);						\
 +} while (0)
 +
 +#define smp_load_acquire(p)						\
 +({									\
 +	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
 +	compiletime_assert_atomic_type(*p);				\
 +	smp_mb();							\
 +	___p1;								\
 +})
 +
 +#else /* regular x86 TSO memory ordering */
 +
 +#define smp_store_release(p, v)						\
  do {									\
  	compiletime_assert_atomic_type(*p);				\
  	barrier();							\
* Unmerged path arch/x86/include/asm/barrier.h
