dm crypt: use shifts instead of sector_div

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit ff3af92b4461be773337111daea80bb91b2cd545
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/ff3af92b.failed

sector_div is very slow, so we introduce a variable sector_shift and
use shift instead of sector_div.

	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit ff3af92b4461be773337111daea80bb91b2cd545)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-crypt.c
diff --cc drivers/md/dm-crypt.c
index 91e9c1daa9b3,ccbc7f36bb2e..000000000000
--- a/drivers/md/dm-crypt.c
+++ b/drivers/md/dm-crypt.c
@@@ -151,6 -172,8 +151,11 @@@ struct crypt_config 
  	} iv_gen_private;
  	sector_t iv_offset;
  	unsigned int iv_size;
++<<<<<<< HEAD
++=======
+ 	unsigned short int sector_size;
+ 	unsigned char sector_shift;
++>>>>>>> ff3af92b4461 (dm crypt: use shifts instead of sector_div)
  
  	/* ESSIV: struct crypto_cipher *essiv_tfm */
  	void *iv_private;
@@@ -838,61 -997,215 +843,163 @@@ static struct ablkcipher_request *req_o
  static u8 *iv_of_dmreq(struct crypt_config *cc,
  		       struct dm_crypt_request *dmreq)
  {
 -	if (crypt_integrity_aead(cc))
 -		return (u8 *)ALIGN((unsigned long)(dmreq + 1),
 -			crypto_aead_alignmask(any_tfm_aead(cc)) + 1);
 -	else
 -		return (u8 *)ALIGN((unsigned long)(dmreq + 1),
 -			crypto_skcipher_alignmask(any_tfm(cc)) + 1);
 -}
 -
 -static u8 *org_iv_of_dmreq(struct crypt_config *cc,
 -		       struct dm_crypt_request *dmreq)
 -{
 -	return iv_of_dmreq(cc, dmreq) + cc->iv_size;
 -}
 -
 -static uint64_t *org_sector_of_dmreq(struct crypt_config *cc,
 -		       struct dm_crypt_request *dmreq)
 -{
 -	u8 *ptr = iv_of_dmreq(cc, dmreq) + cc->iv_size + cc->iv_size;
 -	return (uint64_t*) ptr;
 -}
 -
 -static unsigned int *org_tag_of_dmreq(struct crypt_config *cc,
 -		       struct dm_crypt_request *dmreq)
 -{
 -	u8 *ptr = iv_of_dmreq(cc, dmreq) + cc->iv_size +
 -		  cc->iv_size + sizeof(uint64_t);
 -	return (unsigned int*)ptr;
 -}
 -
 -static void *tag_from_dmreq(struct crypt_config *cc,
 -				struct dm_crypt_request *dmreq)
 -{
 -	struct convert_context *ctx = dmreq->ctx;
 -	struct dm_crypt_io *io = container_of(ctx, struct dm_crypt_io, ctx);
 -
 -	return &io->integrity_metadata[*org_tag_of_dmreq(cc, dmreq) *
 -		cc->on_disk_tag_size];
 -}
 -
 -static void *iv_tag_from_dmreq(struct crypt_config *cc,
 -			       struct dm_crypt_request *dmreq)
 -{
 -	return tag_from_dmreq(cc, dmreq) + cc->integrity_tag_size;
 +	return (u8 *)ALIGN((unsigned long)(dmreq + 1),
 +		crypto_ablkcipher_alignmask(any_tfm(cc)) + 1);
  }
  
 -static int crypt_convert_block_aead(struct crypt_config *cc,
 -				     struct convert_context *ctx,
 -				     struct aead_request *req,
 -				     unsigned int tag_offset)
 +static int crypt_convert_block(struct crypt_config *cc,
 +			       struct convert_context *ctx,
 +			       struct ablkcipher_request *req)
  {
 -	struct bio_vec bv_in = bio_iter_iovec(ctx->bio_in, ctx->iter_in);
 -	struct bio_vec bv_out = bio_iter_iovec(ctx->bio_out, ctx->iter_out);
 +	struct bio_vec *bv_in = bio_iovec_idx(ctx->bio_in, ctx->idx_in);
 +	struct bio_vec *bv_out = bio_iovec_idx(ctx->bio_out, ctx->idx_out);
  	struct dm_crypt_request *dmreq;
 -	u8 *iv, *org_iv, *tag_iv, *tag;
 -	uint64_t *sector;
 -	int r = 0;
 -
 -	BUG_ON(cc->integrity_iv_size && cc->integrity_iv_size != cc->iv_size);
 -
 -	/* Reject unexpected unaligned bio. */
 -	if (unlikely(bv_in.bv_offset & (cc->sector_size - 1)))
 -		return -EIO;
 +	u8 *iv;
 +	int r;
  
  	dmreq = dmreq_of_req(cc, req);
++<<<<<<< HEAD
++=======
+ 	dmreq->iv_sector = ctx->cc_sector;
+ 	if (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))
+ 		dmreq->iv_sector >>= cc->sector_shift;
+ 	dmreq->ctx = ctx;
+ 
+ 	*org_tag_of_dmreq(cc, dmreq) = tag_offset;
+ 
+ 	sector = org_sector_of_dmreq(cc, dmreq);
+ 	*sector = cpu_to_le64(ctx->cc_sector - cc->iv_offset);
+ 
++>>>>>>> ff3af92b4461 (dm crypt: use shifts instead of sector_div)
  	iv = iv_of_dmreq(cc, dmreq);
 -	org_iv = org_iv_of_dmreq(cc, dmreq);
 -	tag = tag_from_dmreq(cc, dmreq);
 -	tag_iv = iv_tag_from_dmreq(cc, dmreq);
  
 -	/* AEAD request:
 -	 *  |----- AAD -------|------ DATA -------|-- AUTH TAG --|
 -	 *  | (authenticated) | (auth+encryption) |              |
 -	 *  | sector_LE |  IV |  sector in/out    |  tag in/out  |
 -	 */
 -	sg_init_table(dmreq->sg_in, 4);
 -	sg_set_buf(&dmreq->sg_in[0], sector, sizeof(uint64_t));
 -	sg_set_buf(&dmreq->sg_in[1], org_iv, cc->iv_size);
 -	sg_set_page(&dmreq->sg_in[2], bv_in.bv_page, cc->sector_size, bv_in.bv_offset);
 -	sg_set_buf(&dmreq->sg_in[3], tag, cc->integrity_tag_size);
 -
 -	sg_init_table(dmreq->sg_out, 4);
 -	sg_set_buf(&dmreq->sg_out[0], sector, sizeof(uint64_t));
 -	sg_set_buf(&dmreq->sg_out[1], org_iv, cc->iv_size);
 -	sg_set_page(&dmreq->sg_out[2], bv_out.bv_page, cc->sector_size, bv_out.bv_offset);
 -	sg_set_buf(&dmreq->sg_out[3], tag, cc->integrity_tag_size);
 +	dmreq->iv_sector = ctx->cc_sector;
 +	dmreq->ctx = ctx;
 +	sg_init_table(&dmreq->sg_in, 1);
 +	sg_set_page(&dmreq->sg_in, bv_in->bv_page, 1 << SECTOR_SHIFT,
 +		    bv_in->bv_offset + ctx->offset_in);
 +
 +	sg_init_table(&dmreq->sg_out, 1);
 +	sg_set_page(&dmreq->sg_out, bv_out->bv_page, 1 << SECTOR_SHIFT,
 +		    bv_out->bv_offset + ctx->offset_out);
 +
 +	ctx->offset_in += 1 << SECTOR_SHIFT;
 +	if (ctx->offset_in >= bv_in->bv_len) {
 +		ctx->offset_in = 0;
 +		ctx->idx_in++;
 +	}
 +
 +	ctx->offset_out += 1 << SECTOR_SHIFT;
 +	if (ctx->offset_out >= bv_out->bv_len) {
 +		ctx->offset_out = 0;
 +		ctx->idx_out++;
 +	}
  
  	if (cc->iv_gen_ops) {
 -		/* For READs use IV stored in integrity metadata */
 -		if (cc->integrity_iv_size && bio_data_dir(ctx->bio_in) != WRITE) {
 -			memcpy(org_iv, tag_iv, cc->iv_size);
 -		} else {
 -			r = cc->iv_gen_ops->generator(cc, org_iv, dmreq);
 -			if (r < 0)
 -				return r;
 -			/* Store generated IV in integrity metadata */
 -			if (cc->integrity_iv_size)
 -				memcpy(tag_iv, org_iv, cc->iv_size);
 -		}
 -		/* Working copy of IV, to be modified in crypto API */
 -		memcpy(iv, org_iv, cc->iv_size);
 +		r = cc->iv_gen_ops->generator(cc, iv, dmreq);
 +		if (r < 0)
 +			return r;
  	}
  
++<<<<<<< HEAD
 +	ablkcipher_request_set_crypt(req, &dmreq->sg_in, &dmreq->sg_out,
 +				     1 << SECTOR_SHIFT, iv);
++=======
+ 	aead_request_set_ad(req, sizeof(uint64_t) + cc->iv_size);
+ 	if (bio_data_dir(ctx->bio_in) == WRITE) {
+ 		aead_request_set_crypt(req, dmreq->sg_in, dmreq->sg_out,
+ 				       cc->sector_size, iv);
+ 		r = crypto_aead_encrypt(req);
+ 		if (cc->integrity_tag_size + cc->integrity_iv_size != cc->on_disk_tag_size)
+ 			memset(tag + cc->integrity_tag_size + cc->integrity_iv_size, 0,
+ 			       cc->on_disk_tag_size - (cc->integrity_tag_size + cc->integrity_iv_size));
+ 	} else {
+ 		aead_request_set_crypt(req, dmreq->sg_in, dmreq->sg_out,
+ 				       cc->sector_size + cc->integrity_tag_size, iv);
+ 		r = crypto_aead_decrypt(req);
+ 	}
+ 
+ 	if (r == -EBADMSG)
+ 		DMERR_LIMIT("INTEGRITY AEAD ERROR, sector %llu",
+ 			    (unsigned long long)le64_to_cpu(*sector));
+ 
+ 	if (!r && cc->iv_gen_ops && cc->iv_gen_ops->post)
+ 		r = cc->iv_gen_ops->post(cc, org_iv, dmreq);
+ 
+ 	bio_advance_iter(ctx->bio_in, &ctx->iter_in, cc->sector_size);
+ 	bio_advance_iter(ctx->bio_out, &ctx->iter_out, cc->sector_size);
+ 
+ 	return r;
+ }
+ 
+ static int crypt_convert_block_skcipher(struct crypt_config *cc,
+ 					struct convert_context *ctx,
+ 					struct skcipher_request *req,
+ 					unsigned int tag_offset)
+ {
+ 	struct bio_vec bv_in = bio_iter_iovec(ctx->bio_in, ctx->iter_in);
+ 	struct bio_vec bv_out = bio_iter_iovec(ctx->bio_out, ctx->iter_out);
+ 	struct scatterlist *sg_in, *sg_out;
+ 	struct dm_crypt_request *dmreq;
+ 	u8 *iv, *org_iv, *tag_iv;
+ 	uint64_t *sector;
+ 	int r = 0;
+ 
+ 	/* Reject unexpected unaligned bio. */
+ 	if (unlikely(bv_in.bv_offset & (cc->sector_size - 1)))
+ 		return -EIO;
+ 
+ 	dmreq = dmreq_of_req(cc, req);
+ 	dmreq->iv_sector = ctx->cc_sector;
+ 	if (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))
+ 		dmreq->iv_sector >>= cc->sector_shift;
+ 	dmreq->ctx = ctx;
+ 
+ 	*org_tag_of_dmreq(cc, dmreq) = tag_offset;
+ 
+ 	iv = iv_of_dmreq(cc, dmreq);
+ 	org_iv = org_iv_of_dmreq(cc, dmreq);
+ 	tag_iv = iv_tag_from_dmreq(cc, dmreq);
+ 
+ 	sector = org_sector_of_dmreq(cc, dmreq);
+ 	*sector = cpu_to_le64(ctx->cc_sector - cc->iv_offset);
+ 
+ 	/* For skcipher we use only the first sg item */
+ 	sg_in  = &dmreq->sg_in[0];
+ 	sg_out = &dmreq->sg_out[0];
+ 
+ 	sg_init_table(sg_in, 1);
+ 	sg_set_page(sg_in, bv_in.bv_page, cc->sector_size, bv_in.bv_offset);
+ 
+ 	sg_init_table(sg_out, 1);
+ 	sg_set_page(sg_out, bv_out.bv_page, cc->sector_size, bv_out.bv_offset);
+ 
+ 	if (cc->iv_gen_ops) {
+ 		/* For READs use IV stored in integrity metadata */
+ 		if (cc->integrity_iv_size && bio_data_dir(ctx->bio_in) != WRITE) {
+ 			memcpy(org_iv, tag_iv, cc->integrity_iv_size);
+ 		} else {
+ 			r = cc->iv_gen_ops->generator(cc, org_iv, dmreq);
+ 			if (r < 0)
+ 				return r;
+ 			/* Store generated IV in integrity metadata */
+ 			if (cc->integrity_iv_size)
+ 				memcpy(tag_iv, org_iv, cc->integrity_iv_size);
+ 		}
+ 		/* Working copy of IV, to be modified in crypto API */
+ 		memcpy(iv, org_iv, cc->iv_size);
+ 	}
+ 
+ 	skcipher_request_set_crypt(req, sg_in, sg_out, cc->sector_size, iv);
++>>>>>>> ff3af92b4461 (dm crypt: use shifts instead of sector_div)
  
  	if (bio_data_dir(ctx->bio_in) == WRITE)
 -		r = crypto_skcipher_encrypt(req);
 +		r = crypto_ablkcipher_encrypt(req);
  	else
 -		r = crypto_skcipher_decrypt(req);
 +		r = crypto_ablkcipher_decrypt(req);
  
  	if (!r && cc->iv_gen_ops && cc->iv_gen_ops->post)
 -		r = cc->iv_gen_ops->post(cc, org_iv, dmreq);
 -
 -	bio_advance_iter(ctx->bio_in, &ctx->iter_in, cc->sector_size);
 -	bio_advance_iter(ctx->bio_out, &ctx->iter_out, cc->sector_size);
 +		r = cc->iv_gen_ops->post(cc, iv, dmreq);
  
  	return r;
  }
@@@ -929,6 -1290,8 +1036,11 @@@ static void crypt_free_req(struct crypt
  static int crypt_convert(struct crypt_config *cc,
  			 struct convert_context *ctx)
  {
++<<<<<<< HEAD
++=======
+ 	unsigned int tag_offset = 0;
+ 	unsigned int sector_step = cc->sector_size >> SECTOR_SHIFT;
++>>>>>>> ff3af92b4461 (dm crypt: use shifts instead of sector_div)
  	int r;
  
  	atomic_set(&ctx->cc_pending, 1);
@@@ -1696,14 -2521,78 +1808,86 @@@ static int crypt_ctr_cipher(struct dm_t
  		}
  	}
  
 +	ret = 0;
 +bad:
 +	kfree(cipher_api);
  	return ret;
 -}
  
++<<<<<<< HEAD
 +bad_mem:
 +	ti->error = "Cannot allocate cipher strings";
 +	return -ENOMEM;
++=======
+ static int crypt_ctr_optional(struct dm_target *ti, unsigned int argc, char **argv)
+ {
+ 	struct crypt_config *cc = ti->private;
+ 	struct dm_arg_set as;
+ 	static struct dm_arg _args[] = {
+ 		{0, 6, "Invalid number of feature args"},
+ 	};
+ 	unsigned int opt_params, val;
+ 	const char *opt_string, *sval;
+ 	char dummy;
+ 	int ret;
+ 
+ 	/* Optional parameters */
+ 	as.argc = argc;
+ 	as.argv = argv;
+ 
+ 	ret = dm_read_arg_group(_args, &as, &opt_params, &ti->error);
+ 	if (ret)
+ 		return ret;
+ 
+ 	while (opt_params--) {
+ 		opt_string = dm_shift_arg(&as);
+ 		if (!opt_string) {
+ 			ti->error = "Not enough feature arguments";
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (!strcasecmp(opt_string, "allow_discards"))
+ 			ti->num_discard_bios = 1;
+ 
+ 		else if (!strcasecmp(opt_string, "same_cpu_crypt"))
+ 			set_bit(DM_CRYPT_SAME_CPU, &cc->flags);
+ 
+ 		else if (!strcasecmp(opt_string, "submit_from_crypt_cpus"))
+ 			set_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags);
+ 		else if (sscanf(opt_string, "integrity:%u:", &val) == 1) {
+ 			if (val == 0 || val > MAX_TAG_SIZE) {
+ 				ti->error = "Invalid integrity arguments";
+ 				return -EINVAL;
+ 			}
+ 			cc->on_disk_tag_size = val;
+ 			sval = strchr(opt_string + strlen("integrity:"), ':') + 1;
+ 			if (!strcasecmp(sval, "aead")) {
+ 				set_bit(CRYPT_MODE_INTEGRITY_AEAD, &cc->cipher_flags);
+ 			} else  if (strcasecmp(sval, "none")) {
+ 				ti->error = "Unknown integrity profile";
+ 				return -EINVAL;
+ 			}
+ 
+ 			cc->cipher_auth = kstrdup(sval, GFP_KERNEL);
+ 			if (!cc->cipher_auth)
+ 				return -ENOMEM;
+ 		} else if (sscanf(opt_string, "sector_size:%hu%c", &cc->sector_size, &dummy) == 1) {
+ 			if (cc->sector_size < (1 << SECTOR_SHIFT) ||
+ 			    cc->sector_size > 4096 ||
+ 			    (cc->sector_size & (cc->sector_size - 1))) {
+ 				ti->error = "Invalid feature value for sector_size";
+ 				return -EINVAL;
+ 			}
+ 			cc->sector_shift = __ffs(cc->sector_size) - SECTOR_SHIFT;
+ 		} else if (!strcasecmp(opt_string, "iv_large_sectors"))
+ 			set_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags);
+ 		else {
+ 			ti->error = "Invalid feature arguments";
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	return 0;
++>>>>>>> ff3af92b4461 (dm crypt: use shifts instead of sector_div)
  }
  
  /*
@@@ -1738,8 -2626,18 +1922,13 @@@ static int crypt_ctr(struct dm_target *
  		return -ENOMEM;
  	}
  	cc->key_size = key_size;
++<<<<<<< HEAD
++=======
+ 	cc->sector_size = (1 << SECTOR_SHIFT);
+ 	cc->sector_shift = 0;
++>>>>>>> ff3af92b4461 (dm crypt: use shifts instead of sector_div)
  
  	ti->private = cc;
 -
 -	/* Optional parameters need to be read before cipher constructor */
 -	if (argc > 5) {
 -		ret = crypt_ctr_optional(ti, argc - 5, &argv[5]);
 -		if (ret)
 -			goto bad;
 -	}
 -
  	ret = crypt_ctr_cipher(ti, argv[0], argv[1]);
  	if (ret < 0)
  		goto bad;
@@@ -1941,6 -2873,10 +2130,13 @@@ static void crypt_status(struct dm_targ
  		num_feature_args += !!ti->num_discard_bios;
  		num_feature_args += test_bit(DM_CRYPT_SAME_CPU, &cc->flags);
  		num_feature_args += test_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags);
++<<<<<<< HEAD
++=======
+ 		num_feature_args += cc->sector_size != (1 << SECTOR_SHIFT);
+ 		num_feature_args += test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags);
+ 		if (cc->on_disk_tag_size)
+ 			num_feature_args++;
++>>>>>>> ff3af92b4461 (dm crypt: use shifts instead of sector_div)
  		if (num_feature_args) {
  			DMEMIT(" %d", num_feature_args);
  			if (ti->num_discard_bios)
* Unmerged path drivers/md/dm-crypt.c
