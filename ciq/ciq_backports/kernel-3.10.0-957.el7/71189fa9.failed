bpf: free up BPF_JMP | BPF_CALL | BPF_X opcode

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Alexei Starovoitov <ast@fb.com>
commit 71189fa9b092ef125ee741eccb2f5fa916798afd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/71189fa9.failed

free up BPF_JMP | BPF_CALL | BPF_X opcode to be used by actual
indirect call by register and use kernel internal opcode to
mark call instruction into bpf_tail_call() helper.

	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 71189fa9b092ef125ee741eccb2f5fa916798afd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/net/bpf_jit_comp.c
#	arch/powerpc/net/bpf_jit_comp64.c
#	arch/s390/net/bpf_jit_comp.c
#	arch/sparc/net/bpf_jit_comp_64.c
#	arch/x86/net/bpf_jit_comp.c
#	include/linux/filter.h
#	kernel/bpf/core.c
#	kernel/bpf/verifier.c
diff --cc arch/s390/net/bpf_jit_comp.c
index 600b1e5c8c9d,42ad3832586c..000000000000
--- a/arch/s390/net/bpf_jit_comp.c
+++ b/arch/s390/net/bpf_jit_comp.c
@@@ -222,606 -476,864 +222,971 @@@ static void bpf_jit_epilogue(struct bpf
  }
  
  /*
 - * Compile one eBPF instruction into s390x code
 - *
 - * NOTE: Use noinline because for gcov (-fprofile-arcs) gcc allocates a lot of
 - * stack space for the large switch statement.
 + * make sure we dont leak kernel information to user
   */
 -static noinline int bpf_jit_insn(struct bpf_jit *jit, struct bpf_prog *fp, int i)
 +static void bpf_jit_noleaks(struct bpf_jit *jit, struct sock_filter *filter)
  {
 -	struct bpf_insn *insn = &fp->insnsi[i];
 -	int jmp_off, last, insn_count = 1;
 -	unsigned int func_addr, mask;
 -	u32 dst_reg = insn->dst_reg;
 -	u32 src_reg = insn->src_reg;
 -	u32 *addrs = jit->addrs;
 -	s32 imm = insn->imm;
 -	s16 off = insn->off;
 -
 -	if (dst_reg == BPF_REG_AX || src_reg == BPF_REG_AX)
 -		jit->seen |= SEEN_REG_AX;
 -	switch (insn->code) {
 -	/*
 -	 * BPF_MOV
 -	 */
 -	case BPF_ALU | BPF_MOV | BPF_X: /* dst = (u32) src */
 -		/* llgfr %dst,%src */
 -		EMIT4(0xb9160000, dst_reg, src_reg);
 -		break;
 -	case BPF_ALU64 | BPF_MOV | BPF_X: /* dst = src */
 -		/* lgr %dst,%src */
 -		EMIT4(0xb9040000, dst_reg, src_reg);
 -		break;
 -	case BPF_ALU | BPF_MOV | BPF_K: /* dst = (u32) imm */
 -		/* llilf %dst,imm */
 -		EMIT6_IMM(0xc00f0000, dst_reg, imm);
 -		break;
 -	case BPF_ALU64 | BPF_MOV | BPF_K: /* dst = imm */
 -		/* lgfi %dst,imm */
 -		EMIT6_IMM(0xc0010000, dst_reg, imm);
 -		break;
 -	/*
 -	 * BPF_LD 64
 -	 */
 -	case BPF_LD | BPF_IMM | BPF_DW: /* dst = (u64) imm */
 -	{
 -		/* 16 byte instruction that uses two 'struct bpf_insn' */
 -		u64 imm64;
 -
 -		imm64 = (u64)(u32) insn[0].imm | ((u64)(u32) insn[1].imm) << 32;
 -		/* lg %dst,<d(imm)>(%l) */
 -		EMIT6_DISP_LH(0xe3000000, 0x0004, dst_reg, REG_0, REG_L,
 -			      EMIT_CONST_U64(imm64));
 -		insn_count = 2;
 -		break;
 +	/* Clear temporary memory if (seen & SEEN_MEM) */
 +	if (jit->seen & SEEN_MEM)
 +		/* xc 0(64,%r15),0(%r15) */
 +		EMIT6(0xd73ff000, 0xf000);
 +	/* Clear X if (seen & SEEN_XREG) */
 +	if (jit->seen & SEEN_XREG)
 +		/* lhi %r12,0 */
 +		EMIT4(0xa7c80000);
 +	/* Clear A if the first register does not set it. */
 +	switch (filter[0].code) {
 +	case BPF_S_LD_W_ABS:
 +	case BPF_S_LD_H_ABS:
 +	case BPF_S_LD_B_ABS:
 +	case BPF_S_LD_W_LEN:
 +	case BPF_S_LD_W_IND:
 +	case BPF_S_LD_H_IND:
 +	case BPF_S_LD_B_IND:
 +	case BPF_S_LD_IMM:
 +	case BPF_S_LD_MEM:
 +	case BPF_S_MISC_TXA:
 +	case BPF_S_ANC_PROTOCOL:
 +	case BPF_S_ANC_PKTTYPE:
 +	case BPF_S_ANC_IFINDEX:
 +	case BPF_S_ANC_MARK:
 +	case BPF_S_ANC_QUEUE:
 +	case BPF_S_ANC_HATYPE:
 +	case BPF_S_ANC_RXHASH:
 +	case BPF_S_ANC_CPU:
 +	case BPF_S_ANC_VLAN_TAG:
 +	case BPF_S_ANC_VLAN_TAG_PRESENT:
 +	case BPF_S_RET_K:
 +		/* first instruction sets A register */
 +		break;
 +	default: /* A = 0 */
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
  	}
 -	/*
 -	 * BPF_ADD
 -	 */
 -	case BPF_ALU | BPF_ADD | BPF_X: /* dst = (u32) dst + (u32) src */
 -		/* ar %dst,%src */
 -		EMIT2(0x1a00, dst_reg, src_reg);
 -		EMIT_ZERO(dst_reg);
 -		break;
 -	case BPF_ALU64 | BPF_ADD | BPF_X: /* dst = dst + src */
 -		/* agr %dst,%src */
 -		EMIT4(0xb9080000, dst_reg, src_reg);
 -		break;
 -	case BPF_ALU | BPF_ADD | BPF_K: /* dst = (u32) dst + (u32) imm */
 -		if (!imm)
 -			break;
 -		/* alfi %dst,imm */
 -		EMIT6_IMM(0xc20b0000, dst_reg, imm);
 -		EMIT_ZERO(dst_reg);
 -		break;
 -	case BPF_ALU64 | BPF_ADD | BPF_K: /* dst = dst + imm */
 -		if (!imm)
 -			break;
 -		/* agfi %dst,imm */
 -		EMIT6_IMM(0xc2080000, dst_reg, imm);
 -		break;
 -	/*
 -	 * BPF_SUB
 -	 */
 -	case BPF_ALU | BPF_SUB | BPF_X: /* dst = (u32) dst - (u32) src */
 -		/* sr %dst,%src */
 -		EMIT2(0x1b00, dst_reg, src_reg);
 -		EMIT_ZERO(dst_reg);
 -		break;
 -	case BPF_ALU64 | BPF_SUB | BPF_X: /* dst = dst - src */
 -		/* sgr %dst,%src */
 -		EMIT4(0xb9090000, dst_reg, src_reg);
 -		break;
 -	case BPF_ALU | BPF_SUB | BPF_K: /* dst = (u32) dst - (u32) imm */
 -		if (!imm)
 -			break;
 -		/* alfi %dst,-imm */
 -		EMIT6_IMM(0xc20b0000, dst_reg, -imm);
 -		EMIT_ZERO(dst_reg);
 -		break;
 -	case BPF_ALU64 | BPF_SUB | BPF_K: /* dst = dst - imm */
 -		if (!imm)
 -			break;
 -		/* agfi %dst,-imm */
 -		EMIT6_IMM(0xc2080000, dst_reg, -imm);
 -		break;
 -	/*
 -	 * BPF_MUL
 -	 */
 -	case BPF_ALU | BPF_MUL | BPF_X: /* dst = (u32) dst * (u32) src */
 -		/* msr %dst,%src */
 -		EMIT4(0xb2520000, dst_reg, src_reg);
 -		EMIT_ZERO(dst_reg);
 -		break;
 -	case BPF_ALU64 | BPF_MUL | BPF_X: /* dst = dst * src */
 -		/* msgr %dst,%src */
 -		EMIT4(0xb90c0000, dst_reg, src_reg);
 -		break;
 -	case BPF_ALU | BPF_MUL | BPF_K: /* dst = (u32) dst * (u32) imm */
 -		if (imm == 1)
 -			break;
 -		/* msfi %r5,imm */
 -		EMIT6_IMM(0xc2010000, dst_reg, imm);
 -		EMIT_ZERO(dst_reg);
 -		break;
 -	case BPF_ALU64 | BPF_MUL | BPF_K: /* dst = dst * imm */
 -		if (imm == 1)
 -			break;
 -		/* msgfi %dst,imm */
 -		EMIT6_IMM(0xc2000000, dst_reg, imm);
 -		break;
 -	/*
 -	 * BPF_DIV / BPF_MOD
 -	 */
 -	case BPF_ALU | BPF_DIV | BPF_X: /* dst = (u32) dst / (u32) src */
 -	case BPF_ALU | BPF_MOD | BPF_X: /* dst = (u32) dst % (u32) src */
 -	{
 -		int rc_reg = BPF_OP(insn->code) == BPF_DIV ? REG_W1 : REG_W0;
 -
 -		jit->seen |= SEEN_RET0;
 -		/* ltr %src,%src (if src == 0 goto fail) */
 -		EMIT2(0x1200, src_reg, src_reg);
 -		/* jz <ret0> */
 -		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 -		/* lhi %w0,0 */
 -		EMIT4_IMM(0xa7080000, REG_W0, 0);
 -		/* lr %w1,%dst */
 -		EMIT2(0x1800, REG_W1, dst_reg);
 -		/* dlr %w0,%src */
 -		EMIT4(0xb9970000, REG_W0, src_reg);
 -		/* llgfr %dst,%rc */
 -		EMIT4(0xb9160000, dst_reg, rc_reg);
 -		break;
 -	}
 -	case BPF_ALU64 | BPF_DIV | BPF_X: /* dst = dst / src */
 -	case BPF_ALU64 | BPF_MOD | BPF_X: /* dst = dst % src */
 -	{
 -		int rc_reg = BPF_OP(insn->code) == BPF_DIV ? REG_W1 : REG_W0;
 -
 -		jit->seen |= SEEN_RET0;
 -		/* ltgr %src,%src (if src == 0 goto fail) */
 -		EMIT4(0xb9020000, src_reg, src_reg);
 -		/* jz <ret0> */
 -		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 -		/* lghi %w0,0 */
 -		EMIT4_IMM(0xa7090000, REG_W0, 0);
 -		/* lgr %w1,%dst */
 -		EMIT4(0xb9040000, REG_W1, dst_reg);
 -		/* dlgr %w0,%dst */
 -		EMIT4(0xb9870000, REG_W0, src_reg);
 -		/* lgr %dst,%rc */
 -		EMIT4(0xb9040000, dst_reg, rc_reg);
 -		break;
 -	}
 -	case BPF_ALU | BPF_DIV | BPF_K: /* dst = (u32) dst / (u32) imm */
 -	case BPF_ALU | BPF_MOD | BPF_K: /* dst = (u32) dst % (u32) imm */
 -	{
 -		int rc_reg = BPF_OP(insn->code) == BPF_DIV ? REG_W1 : REG_W0;
 +}
  
 -		if (imm == 1) {
 -			if (BPF_OP(insn->code) == BPF_MOD)
 -				/* lhgi %dst,0 */
 -				EMIT4_IMM(0xa7090000, dst_reg, 0);
 +static int bpf_jit_insn(struct bpf_jit *jit, struct sock_filter *filter,
 +			unsigned int *addrs, int i, int last)
 +{
 +	unsigned int K;
 +	int offset;
 +	unsigned int mask;
 +
 +	K = filter->k;
 +	switch (filter->code) {
 +	case BPF_S_ALU_ADD_X: /* A += X */
 +		jit->seen |= SEEN_XREG;
 +		/* ar %r5,%r12 */
 +		EMIT2(0x1a5c);
 +		break;
 +	case BPF_S_ALU_ADD_K: /* A += K */
 +		if (!K)
  			break;
++<<<<<<< HEAD
 +		if (K <= 16383)
 +			/* ahi %r5,<K> */
 +			EMIT4_IMM(0xa75a0000, K);
 +		else if (test_facility(21))
 +			/* alfi %r5,<K> */
 +			EMIT6_IMM(0xc25b0000, K);
++=======
+ 		}
+ 		/* lhi %w0,0 */
+ 		EMIT4_IMM(0xa7080000, REG_W0, 0);
+ 		/* lr %w1,%dst */
+ 		EMIT2(0x1800, REG_W1, dst_reg);
+ 		/* dl %w0,<d(imm)>(%l) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0097, REG_W0, REG_0, REG_L,
+ 			      EMIT_CONST_U32(imm));
+ 		/* llgfr %dst,%rc */
+ 		EMIT4(0xb9160000, dst_reg, rc_reg);
+ 		break;
+ 	}
+ 	case BPF_ALU64 | BPF_DIV | BPF_K: /* dst = dst / imm */
+ 	case BPF_ALU64 | BPF_MOD | BPF_K: /* dst = dst % imm */
+ 	{
+ 		int rc_reg = BPF_OP(insn->code) == BPF_DIV ? REG_W1 : REG_W0;
+ 
+ 		if (imm == 1) {
+ 			if (BPF_OP(insn->code) == BPF_MOD)
+ 				/* lhgi %dst,0 */
+ 				EMIT4_IMM(0xa7090000, dst_reg, 0);
+ 			break;
+ 		}
+ 		/* lghi %w0,0 */
+ 		EMIT4_IMM(0xa7090000, REG_W0, 0);
+ 		/* lgr %w1,%dst */
+ 		EMIT4(0xb9040000, REG_W1, dst_reg);
+ 		/* dlg %w0,<d(imm)>(%l) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0087, REG_W0, REG_0, REG_L,
+ 			      EMIT_CONST_U64(imm));
+ 		/* lgr %dst,%rc */
+ 		EMIT4(0xb9040000, dst_reg, rc_reg);
+ 		break;
+ 	}
+ 	/*
+ 	 * BPF_AND
+ 	 */
+ 	case BPF_ALU | BPF_AND | BPF_X: /* dst = (u32) dst & (u32) src */
+ 		/* nr %dst,%src */
+ 		EMIT2(0x1400, dst_reg, src_reg);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_AND | BPF_X: /* dst = dst & src */
+ 		/* ngr %dst,%src */
+ 		EMIT4(0xb9800000, dst_reg, src_reg);
+ 		break;
+ 	case BPF_ALU | BPF_AND | BPF_K: /* dst = (u32) dst & (u32) imm */
+ 		/* nilf %dst,imm */
+ 		EMIT6_IMM(0xc00b0000, dst_reg, imm);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_AND | BPF_K: /* dst = dst & imm */
+ 		/* ng %dst,<d(imm)>(%l) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0080, dst_reg, REG_0, REG_L,
+ 			      EMIT_CONST_U64(imm));
+ 		break;
+ 	/*
+ 	 * BPF_OR
+ 	 */
+ 	case BPF_ALU | BPF_OR | BPF_X: /* dst = (u32) dst | (u32) src */
+ 		/* or %dst,%src */
+ 		EMIT2(0x1600, dst_reg, src_reg);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_OR | BPF_X: /* dst = dst | src */
+ 		/* ogr %dst,%src */
+ 		EMIT4(0xb9810000, dst_reg, src_reg);
+ 		break;
+ 	case BPF_ALU | BPF_OR | BPF_K: /* dst = (u32) dst | (u32) imm */
+ 		/* oilf %dst,imm */
+ 		EMIT6_IMM(0xc00d0000, dst_reg, imm);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_OR | BPF_K: /* dst = dst | imm */
+ 		/* og %dst,<d(imm)>(%l) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0081, dst_reg, REG_0, REG_L,
+ 			      EMIT_CONST_U64(imm));
+ 		break;
+ 	/*
+ 	 * BPF_XOR
+ 	 */
+ 	case BPF_ALU | BPF_XOR | BPF_X: /* dst = (u32) dst ^ (u32) src */
+ 		/* xr %dst,%src */
+ 		EMIT2(0x1700, dst_reg, src_reg);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_XOR | BPF_X: /* dst = dst ^ src */
+ 		/* xgr %dst,%src */
+ 		EMIT4(0xb9820000, dst_reg, src_reg);
+ 		break;
+ 	case BPF_ALU | BPF_XOR | BPF_K: /* dst = (u32) dst ^ (u32) imm */
+ 		if (!imm)
+ 			break;
+ 		/* xilf %dst,imm */
+ 		EMIT6_IMM(0xc0070000, dst_reg, imm);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_XOR | BPF_K: /* dst = dst ^ imm */
+ 		/* xg %dst,<d(imm)>(%l) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0082, dst_reg, REG_0, REG_L,
+ 			      EMIT_CONST_U64(imm));
+ 		break;
+ 	/*
+ 	 * BPF_LSH
+ 	 */
+ 	case BPF_ALU | BPF_LSH | BPF_X: /* dst = (u32) dst << (u32) src */
+ 		/* sll %dst,0(%src) */
+ 		EMIT4_DISP(0x89000000, dst_reg, src_reg, 0);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_LSH | BPF_X: /* dst = dst << src */
+ 		/* sllg %dst,%dst,0(%src) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x000d, dst_reg, dst_reg, src_reg, 0);
+ 		break;
+ 	case BPF_ALU | BPF_LSH | BPF_K: /* dst = (u32) dst << (u32) imm */
+ 		if (imm == 0)
+ 			break;
+ 		/* sll %dst,imm(%r0) */
+ 		EMIT4_DISP(0x89000000, dst_reg, REG_0, imm);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_LSH | BPF_K: /* dst = dst << imm */
+ 		if (imm == 0)
+ 			break;
+ 		/* sllg %dst,%dst,imm(%r0) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x000d, dst_reg, dst_reg, REG_0, imm);
+ 		break;
+ 	/*
+ 	 * BPF_RSH
+ 	 */
+ 	case BPF_ALU | BPF_RSH | BPF_X: /* dst = (u32) dst >> (u32) src */
+ 		/* srl %dst,0(%src) */
+ 		EMIT4_DISP(0x88000000, dst_reg, src_reg, 0);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_RSH | BPF_X: /* dst = dst >> src */
+ 		/* srlg %dst,%dst,0(%src) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x000c, dst_reg, dst_reg, src_reg, 0);
+ 		break;
+ 	case BPF_ALU | BPF_RSH | BPF_K: /* dst = (u32) dst >> (u32) imm */
+ 		if (imm == 0)
+ 			break;
+ 		/* srl %dst,imm(%r0) */
+ 		EMIT4_DISP(0x88000000, dst_reg, REG_0, imm);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_RSH | BPF_K: /* dst = dst >> imm */
+ 		if (imm == 0)
+ 			break;
+ 		/* srlg %dst,%dst,imm(%r0) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x000c, dst_reg, dst_reg, REG_0, imm);
+ 		break;
+ 	/*
+ 	 * BPF_ARSH
+ 	 */
+ 	case BPF_ALU64 | BPF_ARSH | BPF_X: /* ((s64) dst) >>= src */
+ 		/* srag %dst,%dst,0(%src) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x000a, dst_reg, dst_reg, src_reg, 0);
+ 		break;
+ 	case BPF_ALU64 | BPF_ARSH | BPF_K: /* ((s64) dst) >>= imm */
+ 		if (imm == 0)
+ 			break;
+ 		/* srag %dst,%dst,imm(%r0) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x000a, dst_reg, dst_reg, REG_0, imm);
+ 		break;
+ 	/*
+ 	 * BPF_NEG
+ 	 */
+ 	case BPF_ALU | BPF_NEG: /* dst = (u32) -dst */
+ 		/* lcr %dst,%dst */
+ 		EMIT2(0x1300, dst_reg, dst_reg);
+ 		EMIT_ZERO(dst_reg);
+ 		break;
+ 	case BPF_ALU64 | BPF_NEG: /* dst = -dst */
+ 		/* lcgr %dst,%dst */
+ 		EMIT4(0xb9130000, dst_reg, dst_reg);
+ 		break;
+ 	/*
+ 	 * BPF_FROM_BE/LE
+ 	 */
+ 	case BPF_ALU | BPF_END | BPF_FROM_BE:
+ 		/* s390 is big endian, therefore only clear high order bytes */
+ 		switch (imm) {
+ 		case 16: /* dst = (u16) cpu_to_be16(dst) */
+ 			/* llghr %dst,%dst */
+ 			EMIT4(0xb9850000, dst_reg, dst_reg);
+ 			break;
+ 		case 32: /* dst = (u32) cpu_to_be32(dst) */
+ 			/* llgfr %dst,%dst */
+ 			EMIT4(0xb9160000, dst_reg, dst_reg);
+ 			break;
+ 		case 64: /* dst = (u64) cpu_to_be64(dst) */
+ 			break;
+ 		}
+ 		break;
+ 	case BPF_ALU | BPF_END | BPF_FROM_LE:
+ 		switch (imm) {
+ 		case 16: /* dst = (u16) cpu_to_le16(dst) */
+ 			/* lrvr %dst,%dst */
+ 			EMIT4(0xb91f0000, dst_reg, dst_reg);
+ 			/* srl %dst,16(%r0) */
+ 			EMIT4_DISP(0x88000000, dst_reg, REG_0, 16);
+ 			/* llghr %dst,%dst */
+ 			EMIT4(0xb9850000, dst_reg, dst_reg);
+ 			break;
+ 		case 32: /* dst = (u32) cpu_to_le32(dst) */
+ 			/* lrvr %dst,%dst */
+ 			EMIT4(0xb91f0000, dst_reg, dst_reg);
+ 			/* llgfr %dst,%dst */
+ 			EMIT4(0xb9160000, dst_reg, dst_reg);
+ 			break;
+ 		case 64: /* dst = (u64) cpu_to_le64(dst) */
+ 			/* lrvgr %dst,%dst */
+ 			EMIT4(0xb90f0000, dst_reg, dst_reg);
+ 			break;
+ 		}
+ 		break;
+ 	/*
+ 	 * BPF_ST(X)
+ 	 */
+ 	case BPF_STX | BPF_MEM | BPF_B: /* *(u8 *)(dst + off) = src_reg */
+ 		/* stcy %src,off(%dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0072, src_reg, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_STX | BPF_MEM | BPF_H: /* (u16 *)(dst + off) = src */
+ 		/* sthy %src,off(%dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0070, src_reg, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_STX | BPF_MEM | BPF_W: /* *(u32 *)(dst + off) = src */
+ 		/* sty %src,off(%dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0050, src_reg, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_STX | BPF_MEM | BPF_DW: /* (u64 *)(dst + off) = src */
+ 		/* stg %src,off(%dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0024, src_reg, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_ST | BPF_MEM | BPF_B: /* *(u8 *)(dst + off) = imm */
+ 		/* lhi %w0,imm */
+ 		EMIT4_IMM(0xa7080000, REG_W0, (u8) imm);
+ 		/* stcy %w0,off(dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0072, REG_W0, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_ST | BPF_MEM | BPF_H: /* (u16 *)(dst + off) = imm */
+ 		/* lhi %w0,imm */
+ 		EMIT4_IMM(0xa7080000, REG_W0, (u16) imm);
+ 		/* sthy %w0,off(dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0070, REG_W0, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_ST | BPF_MEM | BPF_W: /* *(u32 *)(dst + off) = imm */
+ 		/* llilf %w0,imm  */
+ 		EMIT6_IMM(0xc00f0000, REG_W0, (u32) imm);
+ 		/* sty %w0,off(%dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0050, REG_W0, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_ST | BPF_MEM | BPF_DW: /* *(u64 *)(dst + off) = imm */
+ 		/* lgfi %w0,imm */
+ 		EMIT6_IMM(0xc0010000, REG_W0, imm);
+ 		/* stg %w0,off(%dst) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0024, REG_W0, dst_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	/*
+ 	 * BPF_STX XADD (atomic_add)
+ 	 */
+ 	case BPF_STX | BPF_XADD | BPF_W: /* *(u32 *)(dst + off) += src */
+ 		/* laal %w0,%src,off(%dst) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x00fa, REG_W0, src_reg,
+ 			      dst_reg, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_STX | BPF_XADD | BPF_DW: /* *(u64 *)(dst + off) += src */
+ 		/* laalg %w0,%src,off(%dst) */
+ 		EMIT6_DISP_LH(0xeb000000, 0x00ea, REG_W0, src_reg,
+ 			      dst_reg, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	/*
+ 	 * BPF_LDX
+ 	 */
+ 	case BPF_LDX | BPF_MEM | BPF_B: /* dst = *(u8 *)(ul) (src + off) */
+ 		/* llgc %dst,0(off,%src) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0090, dst_reg, src_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_LDX | BPF_MEM | BPF_H: /* dst = *(u16 *)(ul) (src + off) */
+ 		/* llgh %dst,0(off,%src) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0091, dst_reg, src_reg, REG_0, off);
+ 		jit->seen |= SEEN_MEM;
+ 		break;
+ 	case BPF_LDX | BPF_MEM | BPF_W: /* dst = *(u32 *)(ul) (src + off) */
+ 		/* llgf %dst,off(%src) */
+ 		jit->seen |= SEEN_MEM;
+ 		EMIT6_DISP_LH(0xe3000000, 0x0016, dst_reg, src_reg, REG_0, off);
+ 		break;
+ 	case BPF_LDX | BPF_MEM | BPF_DW: /* dst = *(u64 *)(ul) (src + off) */
+ 		/* lg %dst,0(off,%src) */
+ 		jit->seen |= SEEN_MEM;
+ 		EMIT6_DISP_LH(0xe3000000, 0x0004, dst_reg, src_reg, REG_0, off);
+ 		break;
+ 	/*
+ 	 * BPF_JMP / CALL
+ 	 */
+ 	case BPF_JMP | BPF_CALL:
+ 	{
+ 		/*
+ 		 * b0 = (__bpf_call_base + imm)(b1, b2, b3, b4, b5)
+ 		 */
+ 		const u64 func = (u64)__bpf_call_base + imm;
+ 
+ 		REG_SET_SEEN(BPF_REG_5);
+ 		jit->seen |= SEEN_FUNC;
+ 		/* lg %w1,<d(imm)>(%l) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0004, REG_W1, REG_0, REG_L,
+ 			      EMIT_CONST_U64(func));
+ 		/* basr %r14,%w1 */
+ 		EMIT2(0x0d00, REG_14, REG_W1);
+ 		/* lgr %b0,%r2: load return value into %b0 */
+ 		EMIT4(0xb9040000, BPF_REG_0, REG_2);
+ 		if (bpf_helper_changes_pkt_data((void *)func)) {
+ 			jit->seen |= SEEN_SKB_CHANGE;
+ 			/* lg %b1,ST_OFF_SKBP(%r15) */
+ 			EMIT6_DISP_LH(0xe3000000, 0x0004, BPF_REG_1, REG_0,
+ 				      REG_15, STK_OFF_SKBP);
+ 			emit_load_skb_data_hlen(jit);
+ 		}
+ 		break;
+ 	}
+ 	case BPF_JMP | BPF_TAIL_CALL:
+ 		/*
+ 		 * Implicit input:
+ 		 *  B1: pointer to ctx
+ 		 *  B2: pointer to bpf_array
+ 		 *  B3: index in bpf_array
+ 		 */
+ 		jit->seen |= SEEN_TAIL_CALL;
+ 
+ 		/*
+ 		 * if (index >= array->map.max_entries)
+ 		 *         goto out;
+ 		 */
+ 
+ 		/* llgf %w1,map.max_entries(%b2) */
+ 		EMIT6_DISP_LH(0xe3000000, 0x0016, REG_W1, REG_0, BPF_REG_2,
+ 			      offsetof(struct bpf_array, map.max_entries));
+ 		/* clgrj %b3,%w1,0xa,label0: if %b3 >= %w1 goto out */
+ 		EMIT6_PCREL_LABEL(0xec000000, 0x0065, BPF_REG_3,
+ 				  REG_W1, 0, 0xa);
+ 
+ 		/*
+ 		 * if (tail_call_cnt++ > MAX_TAIL_CALL_CNT)
+ 		 *         goto out;
+ 		 */
+ 
+ 		if (jit->seen & SEEN_STACK)
+ 			off = STK_OFF_TCCNT + STK_OFF;
++>>>>>>> 71189fa9b092 (bpf: free up BPF_JMP | BPF_CALL | BPF_X opcode)
  		else
 -			off = STK_OFF_TCCNT;
 -		/* lhi %w0,1 */
 -		EMIT4_IMM(0xa7080000, REG_W0, 1);
 -		/* laal %w1,%w0,off(%r15) */
 -		EMIT6_DISP_LH(0xeb000000, 0x00fa, REG_W1, REG_W0, REG_15, off);
 -		/* clij %w1,MAX_TAIL_CALL_CNT,0x2,label0 */
 -		EMIT6_PCREL_IMM_LABEL(0xec000000, 0x007f, REG_W1,
 -				      MAX_TAIL_CALL_CNT, 0, 0x2);
 -
 -		/*
 -		 * prog = array->ptrs[index];
 -		 * if (prog == NULL)
 -		 *         goto out;
 -		 */
 -
 -		/* sllg %r1,%b3,3: %r1 = index * 8 */
 -		EMIT6_DISP_LH(0xeb000000, 0x000d, REG_1, BPF_REG_3, REG_0, 3);
 -		/* lg %r1,prog(%b2,%r1) */
 -		EMIT6_DISP_LH(0xe3000000, 0x0004, REG_1, BPF_REG_2,
 -			      REG_1, offsetof(struct bpf_array, ptrs));
 -		/* clgij %r1,0,0x8,label0 */
 -		EMIT6_PCREL_IMM_LABEL(0xec000000, 0x007d, REG_1, 0, 0, 0x8);
 -
 -		/*
 -		 * Restore registers before calling function
 -		 */
 -		save_restore_regs(jit, REGS_RESTORE);
 -
 -		/*
 -		 * goto *(prog->bpf_func + tail_call_start);
 -		 */
 -
 -		/* lg %r1,bpf_func(%r1) */
 -		EMIT6_DISP_LH(0xe3000000, 0x0004, REG_1, REG_1, REG_0,
 -			      offsetof(struct bpf_prog, bpf_func));
 -		/* bc 0xf,tail_call_start(%r1) */
 -		_EMIT4(0x47f01000 + jit->tail_call_start);
 -		/* out: */
 -		jit->labels[0] = jit->prg;
 -		break;
 -	case BPF_JMP | BPF_EXIT: /* return b0 */
 -		last = (i == fp->len - 1) ? 1 : 0;
 -		if (last && !(jit->seen & SEEN_RET0))
 -			break;
 -		/* j <exit> */
 -		EMIT4_PCREL(0xa7f40000, jit->exit_ip - jit->prg);
 +			/* a %r5,<d(K)>(%r13) */
 +			EMIT4_DISP(0x5a50d000, EMIT_CONST(K));
  		break;
 -	/*
 -	 * Branch relative (number of skipped instructions) to offset on
 -	 * condition.
 -	 *
 -	 * Condition code to mask mapping:
 -	 *
 -	 * CC | Description	   | Mask
 -	 * ------------------------------
 -	 * 0  | Operands equal	   |	8
 -	 * 1  | First operand low  |	4
 -	 * 2  | First operand high |	2
 -	 * 3  | Unused		   |	1
 -	 *
 -	 * For s390x relative branches: ip = ip + off_bytes
 -	 * For BPF relative branches:	insn = insn + off_insns + 1
 -	 *
 -	 * For example for s390x with offset 0 we jump to the branch
 -	 * instruction itself (loop) and for BPF with offset 0 we
 -	 * branch to the instruction behind the branch.
 -	 */
 -	case BPF_JMP | BPF_JA: /* if (true) */
 -		mask = 0xf000; /* j */
 -		goto branch_oc;
 -	case BPF_JMP | BPF_JSGT | BPF_K: /* ((s64) dst > (s64) imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JSGE | BPF_K: /* ((s64) dst >= (s64) imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JGT | BPF_K: /* (dst_reg > imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JGE | BPF_K: /* (dst_reg >= imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JNE | BPF_K: /* (dst_reg != imm) */
 -		mask = 0x7000; /* jne */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JEQ | BPF_K: /* (dst_reg == imm) */
 -		mask = 0x8000; /* je */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JSET | BPF_K: /* (dst_reg & imm) */
 -		mask = 0x7000; /* jnz */
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* ngr %w1,%dst */
 -		EMIT4(0xb9800000, REG_W1, dst_reg);
 -		goto branch_oc;
 -
 -	case BPF_JMP | BPF_JSGT | BPF_X: /* ((s64) dst > (s64) src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JSGE | BPF_X: /* ((s64) dst >= (s64) src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JGT | BPF_X: /* (dst > src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JGE | BPF_X: /* (dst >= src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JNE | BPF_X: /* (dst != src) */
 -		mask = 0x7000; /* jne */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JEQ | BPF_X: /* (dst == src) */
 -		mask = 0x8000; /* je */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JSET | BPF_X: /* (dst & src) */
 -		mask = 0x7000; /* jnz */
 -		/* ngrk %w1,%dst,%src */
 -		EMIT4_RRF(0xb9e40000, REG_W1, dst_reg, src_reg);
 -		goto branch_oc;
 -branch_ks:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* cgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_ku:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* clgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_xs:
 -		/* cgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, src_reg, i, off, mask);
 -		break;
 -branch_xu:
 -		/* clgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, src_reg, i, off, mask);
 -		break;
 -branch_oc:
 -		/* brc mask,jmp_off (branch instruction needs 4 bytes) */
 -		jmp_off = addrs[i + off + 1] - (addrs[i + 1] - 4);
 -		EMIT4_PCREL(0xa7040000 | mask << 8, jmp_off);
 +	case BPF_S_ALU_SUB_X: /* A -= X */
 +		jit->seen |= SEEN_XREG;
 +		/* sr %r5,%r12 */
 +		EMIT2(0x1b5c);
  		break;
 -	/*
 -	 * BPF_LD
 -	 */
 -	case BPF_LD | BPF_ABS | BPF_B: /* b0 = *(u8 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_B: /* b0 = *(u8 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_byte_pos);
 +	case BPF_S_ALU_SUB_K: /* A -= K */
 +		if (!K)
 +			break;
 +		if (K <= 16384)
 +			/* ahi %r5,-K */
 +			EMIT4_IMM(0xa75a0000, -K);
 +		else if (test_facility(21))
 +			/* alfi %r5,-K */
 +			EMIT6_IMM(0xc25b0000, -K);
  		else
 -			func_addr = __pa(sk_load_byte);
 -		goto call_fn;
 -	case BPF_LD | BPF_ABS | BPF_H: /* b0 = *(u16 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_H: /* b0 = *(u16 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_half_pos);
 +			/* s %r5,<d(K)>(%r13) */
 +			EMIT4_DISP(0x5b50d000, EMIT_CONST(K));
 +		break;
 +	case BPF_S_ALU_MUL_X: /* A *= X */
 +		jit->seen |= SEEN_XREG;
 +		/* msr %r5,%r12 */
 +		EMIT4(0xb252005c);
 +		break;
 +	case BPF_S_ALU_MUL_K: /* A *= K */
 +		if (K <= 16383)
 +			/* mhi %r5,K */
 +			EMIT4_IMM(0xa75c0000, K);
 +		else if (test_facility(34))
 +			/* msfi %r5,<K> */
 +			EMIT6_IMM(0xc2510000, K);
  		else
 -			func_addr = __pa(sk_load_half);
 -		goto call_fn;
 -	case BPF_LD | BPF_ABS | BPF_W: /* b0 = *(u32 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_W: /* b0 = *(u32 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_word_pos);
 +			/* ms %r5,<d(K)>(%r13) */
 +			EMIT4_DISP(0x7150d000, EMIT_CONST(K));
 +		break;
 +	case BPF_S_ALU_DIV_X: /* A /= X */
 +		jit->seen |= SEEN_XREG | SEEN_RET0;
 +		/* ltr %r12,%r12 */
 +		EMIT2(0x12cc);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, (jit->ret0_ip - jit->prg));
 +		/* lhi %r4,0 */
 +		EMIT4(0xa7480000);
 +		/* dlr %r4,%r12 */
 +		EMIT4(0xb997004c);
 +		break;
 +	case BPF_S_ALU_DIV_K: /* A /= K */
 +		if (K == 1)
 +			break;
 +		/* lhi %r4,0 */
 +		EMIT4(0xa7480000);
 +		/* dl %r4,<d(K)>(%r13) */
 +		EMIT6_DISP(0xe340d000, 0x0097, EMIT_CONST(K));
 +		break;
 +	case BPF_S_ALU_MOD_X: /* A %= X */
 +		jit->seen |= SEEN_XREG | SEEN_RET0;
 +		/* ltr %r12,%r12 */
 +		EMIT2(0x12cc);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, (jit->ret0_ip - jit->prg));
 +		/* lhi %r4,0 */
 +		EMIT4(0xa7480000);
 +		/* dlr %r4,%r12 */
 +		EMIT4(0xb997004c);
 +		/* lr %r5,%r4 */
 +		EMIT2(0x1854);
 +		break;
 +	case BPF_S_ALU_MOD_K: /* A %= K */
 +		if (K == 1) {
 +			/* lhi %r5,0 */
 +			EMIT4(0xa7580000);
 +			break;
 +		}
 +		/* lhi %r4,0 */
 +		EMIT4(0xa7480000);
 +		/* dl %r4,<d(K)>(%r13) */
 +		EMIT6_DISP(0xe340d000, 0x0097, EMIT_CONST(K));
 +		/* lr %r5,%r4 */
 +		EMIT2(0x1854);
 +		break;
 +	case BPF_S_ALU_AND_X: /* A &= X */
 +		jit->seen |= SEEN_XREG;
 +		/* nr %r5,%r12 */
 +		EMIT2(0x145c);
 +		break;
 +	case BPF_S_ALU_AND_K: /* A &= K */
 +		if (test_facility(21))
 +			/* nilf %r5,<K> */
 +			EMIT6_IMM(0xc05b0000, K);
  		else
 -			func_addr = __pa(sk_load_word);
 -		goto call_fn;
 -call_fn:
 -		jit->seen |= SEEN_SKB | SEEN_RET0 | SEEN_FUNC;
 -		REG_SET_SEEN(REG_14); /* Return address of possible func call */
 -
 -		/*
 -		 * Implicit input:
 -		 *  BPF_REG_6	 (R7) : skb pointer
 -		 *  REG_SKB_DATA (R12): skb data pointer (if no BPF_REG_AX)
 -		 *
 -		 * Calculated input:
 -		 *  BPF_REG_2	 (R3) : offset of byte(s) to fetch in skb
 -		 *  BPF_REG_5	 (R6) : return address
 -		 *
 -		 * Output:
 -		 *  BPF_REG_0	 (R14): data read from skb
 -		 *
 -		 * Scratch registers (BPF_REG_1-5)
 -		 */
 -
 -		/* Call function: llilf %w1,func_addr  */
 -		EMIT6_IMM(0xc00f0000, REG_W1, func_addr);
 -
 -		/* Offset: lgfi %b2,imm */
 -		EMIT6_IMM(0xc0010000, BPF_REG_2, imm);
 -		if (BPF_MODE(insn->code) == BPF_IND)
 -			/* agfr %b2,%src (%src is s32 here) */
 -			EMIT4(0xb9180000, BPF_REG_2, src_reg);
 -
 -		/* Reload REG_SKB_DATA if BPF_REG_AX is used */
 -		if (jit->seen & SEEN_REG_AX)
 -			/* lg %skb_data,data_off(%b6) */
 -			EMIT6_DISP_LH(0xe3000000, 0x0004, REG_SKB_DATA, REG_0,
 -				      BPF_REG_6, offsetof(struct sk_buff, data));
 -		/* basr %b5,%w1 (%b5 is call saved) */
 -		EMIT2(0x0d00, BPF_REG_5, REG_W1);
 -
 -		/*
 -		 * Note: For fast access we jump directly after the
 -		 * jnz instruction from bpf_jit.S
 -		 */
 +			/* n %r5,<d(K)>(%r13) */
 +			EMIT4_DISP(0x5450d000, EMIT_CONST(K));
 +		break;
 +	case BPF_S_ALU_OR_X: /* A |= X */
 +		jit->seen |= SEEN_XREG;
 +		/* or %r5,%r12 */
 +		EMIT2(0x165c);
 +		break;
 +	case BPF_S_ALU_OR_K: /* A |= K */
 +		if (test_facility(21))
 +			/* oilf %r5,<K> */
 +			EMIT6_IMM(0xc05d0000, K);
 +		else
 +			/* o %r5,<d(K)>(%r13) */
 +			EMIT4_DISP(0x5650d000, EMIT_CONST(K));
 +		break;
 +	case BPF_S_ANC_ALU_XOR_X: /* A ^= X; */
 +	case BPF_S_ALU_XOR_X:
 +		jit->seen |= SEEN_XREG;
 +		/* xr %r5,%r12 */
 +		EMIT2(0x175c);
 +		break;
 +	case BPF_S_ALU_XOR_K: /* A ^= K */
 +		if (!K)
 +			break;
 +		/* x %r5,<d(K)>(%r13) */
 +		EMIT4_DISP(0x5750d000, EMIT_CONST(K));
 +		break;
 +	case BPF_S_ALU_LSH_X: /* A <<= X; */
 +		jit->seen |= SEEN_XREG;
 +		/* sll %r5,0(%r12) */
 +		EMIT4(0x8950c000);
 +		break;
 +	case BPF_S_ALU_LSH_K: /* A <<= K */
 +		if (K == 0)
 +			break;
 +		/* sll %r5,K */
 +		EMIT4_DISP(0x89500000, K);
 +		break;
 +	case BPF_S_ALU_RSH_X: /* A >>= X; */
 +		jit->seen |= SEEN_XREG;
 +		/* srl %r5,0(%r12) */
 +		EMIT4(0x8850c000);
 +		break;
 +	case BPF_S_ALU_RSH_K: /* A >>= K; */
 +		if (K == 0)
 +			break;
 +		/* srl %r5,K */
 +		EMIT4_DISP(0x88500000, K);
 +		break;
 +	case BPF_S_ALU_NEG: /* A = -A */
 +		/* lcr %r5,%r5 */
 +		EMIT2(0x1355);
 +		break;
 +	case BPF_S_JMP_JA: /* ip += K */
 +		offset = addrs[i + K] + jit->start - jit->prg;
 +		EMIT4_PCREL(0xa7f40000, offset);
 +		break;
 +	case BPF_S_JMP_JGT_K: /* ip += (A > K) ? jt : jf */
 +		mask = 0x200000; /* jh */
 +		goto kbranch;
 +	case BPF_S_JMP_JGE_K: /* ip += (A >= K) ? jt : jf */
 +		mask = 0xa00000; /* jhe */
 +		goto kbranch;
 +	case BPF_S_JMP_JEQ_K: /* ip += (A == K) ? jt : jf */
 +		mask = 0x800000; /* je */
 +kbranch:	/* Emit compare if the branch targets are different */
 +		if (filter->jt != filter->jf) {
 +			if (test_facility(21))
 +				/* clfi %r5,<K> */
 +				EMIT6_IMM(0xc25f0000, K);
 +			else
 +				/* cl %r5,<d(K)>(%r13) */
 +				EMIT4_DISP(0x5550d000, EMIT_CONST(K));
 +		}
 +branch:		if (filter->jt == filter->jf) {
 +			if (filter->jt == 0)
 +				break;
 +			/* j <jt> */
 +			offset = addrs[i + filter->jt] + jit->start - jit->prg;
 +			EMIT4_PCREL(0xa7f40000, offset);
 +			break;
 +		}
 +		if (filter->jt != 0) {
 +			/* brc	<mask>,<jt> */
 +			offset = addrs[i + filter->jt] + jit->start - jit->prg;
 +			EMIT4_PCREL(0xa7040000 | mask, offset);
 +		}
 +		if (filter->jf != 0) {
 +			/* brc	<mask^15>,<jf> */
 +			offset = addrs[i + filter->jf] + jit->start - jit->prg;
 +			EMIT4_PCREL(0xa7040000 | (mask ^ 0xf00000), offset);
 +		}
 +		break;
 +	case BPF_S_JMP_JSET_K: /* ip += (A & K) ? jt : jf */
 +		mask = 0x700000; /* jnz */
 +		/* Emit test if the branch targets are different */
 +		if (filter->jt != filter->jf) {
 +			if (K > 65535) {
 +				/* lr %r4,%r5 */
 +				EMIT2(0x1845);
 +				/* n %r4,<d(K)>(%r13) */
 +				EMIT4_DISP(0x5440d000, EMIT_CONST(K));
 +			} else
 +				/* tmll %r5,K */
 +				EMIT4_IMM(0xa7510000, K);
 +		}
 +		goto branch;
 +	case BPF_S_JMP_JGT_X: /* ip += (A > X) ? jt : jf */
 +		mask = 0x200000; /* jh */
 +		goto xbranch;
 +	case BPF_S_JMP_JGE_X: /* ip += (A >= X) ? jt : jf */
 +		mask = 0xa00000; /* jhe */
 +		goto xbranch;
 +	case BPF_S_JMP_JEQ_X: /* ip += (A == X) ? jt : jf */
 +		mask = 0x800000; /* je */
 +xbranch:	/* Emit compare if the branch targets are different */
 +		if (filter->jt != filter->jf) {
 +			jit->seen |= SEEN_XREG;
 +			/* clr %r5,%r12 */
 +			EMIT2(0x155c);
 +		}
 +		goto branch;
 +	case BPF_S_JMP_JSET_X: /* ip += (A & X) ? jt : jf */
 +		mask = 0x700000; /* jnz */
 +		/* Emit test if the branch targets are different */
 +		if (filter->jt != filter->jf) {
 +			jit->seen |= SEEN_XREG;
 +			/* lr %r4,%r5 */
 +			EMIT2(0x1845);
 +			/* nr %r4,%r12 */
 +			EMIT2(0x144c);
 +		}
 +		goto branch;
 +	case BPF_S_LD_W_ABS: /* A = *(u32 *) (skb->data+K) */
 +		jit->seen |= SEEN_DATAREF | SEEN_RET0 | SEEN_LOAD_WORD;
 +		offset = jit->off_load_word;
 +		goto load_abs;
 +	case BPF_S_LD_H_ABS: /* A = *(u16 *) (skb->data+K) */
 +		jit->seen |= SEEN_DATAREF | SEEN_RET0 | SEEN_LOAD_HALF;
 +		offset = jit->off_load_half;
 +		goto load_abs;
 +	case BPF_S_LD_B_ABS: /* A = *(u8 *) (skb->data+K) */
 +		jit->seen |= SEEN_DATAREF | SEEN_RET0 | SEEN_LOAD_BYTE;
 +		offset = jit->off_load_byte;
 +load_abs:	if ((int) K < 0)
 +			goto out;
 +call_fn:	/* lg %r1,<d(function)>(%r13) */
 +		EMIT6_DISP(0xe310d000, 0x0004, offset);
 +		/* l %r3,<d(K)>(%r13) */
 +		EMIT4_DISP(0x5830d000, EMIT_CONST(K));
 +		/* basr %r8,%r1 */
 +		EMIT2(0x0d81);
  		/* jnz <ret0> */
 -		EMIT4_PCREL(0xa7740000, jit->ret0_ip - jit->prg);
 +		EMIT4_PCREL(0xa7740000, (jit->ret0_ip - jit->prg));
 +		break;
 +	case BPF_S_LD_W_IND: /* A = *(u32 *) (skb->data+K+X) */
 +		jit->seen |= SEEN_DATAREF | SEEN_RET0 | SEEN_LOAD_IWORD;
 +		offset = jit->off_load_iword;
 +		goto call_fn;
 +	case BPF_S_LD_H_IND: /* A = *(u16 *) (skb->data+K+X) */
 +		jit->seen |= SEEN_DATAREF | SEEN_RET0 | SEEN_LOAD_IHALF;
 +		offset = jit->off_load_ihalf;
 +		goto call_fn;
 +	case BPF_S_LD_B_IND: /* A = *(u8 *) (skb->data+K+X) */
 +		jit->seen |= SEEN_DATAREF | SEEN_RET0 | SEEN_LOAD_IBYTE;
 +		offset = jit->off_load_ibyte;
 +		goto call_fn;
 +	case BPF_S_LDX_B_MSH:
 +		/* X = (*(u8 *)(skb->data+K) & 0xf) << 2 */
 +		jit->seen |= SEEN_RET0;
 +		if ((int) K < 0) {
 +			/* j <ret0> */
 +			EMIT4_PCREL(0xa7f40000, (jit->ret0_ip - jit->prg));
 +			break;
 +		}
 +		jit->seen |= SEEN_DATAREF | SEEN_LOAD_BMSH;
 +		offset = jit->off_load_bmsh;
 +		goto call_fn;
 +	case BPF_S_LD_W_LEN: /*	A = skb->len; */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);
 +		/* l %r5,<d(len)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, len));
 +		break;
 +	case BPF_S_LDX_W_LEN: /* X = skb->len; */
 +		jit->seen |= SEEN_XREG;
 +		/* l %r12,<d(len)>(%r2) */
 +		EMIT4_DISP(0x58c02000, offsetof(struct sk_buff, len));
 +		break;
 +	case BPF_S_LD_IMM: /* A = K */
 +		if (K <= 16383)
 +			/* lhi %r5,K */
 +			EMIT4_IMM(0xa7580000, K);
 +		else if (test_facility(21))
 +			/* llilf %r5,<K> */
 +			EMIT6_IMM(0xc05f0000, K);
 +		else
 +			/* l %r5,<d(K)>(%r13) */
 +			EMIT4_DISP(0x5850d000, EMIT_CONST(K));
 +		break;
 +	case BPF_S_LDX_IMM: /* X = K */
 +		jit->seen |= SEEN_XREG;
 +		if (K <= 16383)
 +			/* lhi %r12,<K> */
 +			EMIT4_IMM(0xa7c80000, K);
 +		else if (test_facility(21))
 +			/* llilf %r12,<K> */
 +			EMIT6_IMM(0xc0cf0000, K);
 +		else
 +			/* l %r12,<d(K)>(%r13) */
 +			EMIT4_DISP(0x58c0d000, EMIT_CONST(K));
 +		break;
 +	case BPF_S_LD_MEM: /* A = mem[K] */
 +		jit->seen |= SEEN_MEM;
 +		/* l %r5,<K>(%r15) */
 +		EMIT4_DISP(0x5850f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_LDX_MEM: /* X = mem[K] */
 +		jit->seen |= SEEN_XREG | SEEN_MEM;
 +		/* l %r12,<K>(%r15) */
 +		EMIT4_DISP(0x58c0f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_MISC_TAX: /* X = A */
 +		jit->seen |= SEEN_XREG;
 +		/* lr %r12,%r5 */
 +		EMIT2(0x18c5);
 +		break;
 +	case BPF_S_MISC_TXA: /* A = X */
 +		jit->seen |= SEEN_XREG;
 +		/* lr %r5,%r12 */
 +		EMIT2(0x185c);
 +		break;
 +	case BPF_S_RET_K:
 +		if (K == 0) {
 +			jit->seen |= SEEN_RET0;
 +			if (last)
 +				break;
 +			/* j <ret0> */
 +			EMIT4_PCREL(0xa7f40000, jit->ret0_ip - jit->prg);
 +		} else {
 +			if (K <= 16383)
 +				/* lghi %r2,K */
 +				EMIT4_IMM(0xa7290000, K);
 +			else
 +				/* llgf %r2,<K>(%r13) */
 +				EMIT6_DISP(0xe320d000, 0x0016, EMIT_CONST(K));
 +			/* j <exit> */
 +			if (last && !(jit->seen & SEEN_RET0))
 +				break;
 +			EMIT4_PCREL(0xa7f40000, jit->exit_ip - jit->prg);
 +		}
 +		break;
 +	case BPF_S_RET_A:
 +		/* llgfr %r2,%r5 */
 +		EMIT4(0xb9160025);
 +		/* j <exit> */
 +		EMIT4_PCREL(0xa7f40000, jit->exit_ip - jit->prg);
 +		break;
 +	case BPF_S_ST: /* mem[K] = A */
 +		jit->seen |= SEEN_MEM;
 +		/* st %r5,<K>(%r15) */
 +		EMIT4_DISP(0x5050f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_STX: /* mem[K] = X : mov %ebx,off8(%rbp) */
 +		jit->seen |= SEEN_XREG | SEEN_MEM;
 +		/* st %r12,<K>(%r15) */
 +		EMIT4_DISP(0x50c0f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_ANC_PROTOCOL: /* A = ntohs(skb->protocol); */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(protocol)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, protocol));
 +		break;
 +	case BPF_S_ANC_IFINDEX:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->ifindex */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* l %r5,<d(ifindex)>(%r1) */
 +		EMIT4_DISP(0x58501000, offsetof(struct net_device, ifindex));
 +		break;
 +	case BPF_S_ANC_MARK: /* A = skb->mark */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
 +		/* l %r5,<d(mark)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, mark));
 +		break;
 +	case BPF_S_ANC_QUEUE: /* A = skb->queue_mapping */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(queue_mapping)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, queue_mapping));
 +		break;
 +	case BPF_S_ANC_HATYPE:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->type */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(type)>(%r1) */
 +		EMIT4_DISP(0xbf531000, offsetof(struct net_device, type));
 +		break;
 +	case BPF_S_ANC_RXHASH: /* A = skb->hash */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
 +		/* l %r5,<d(hash)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, hash));
 +		break;
 +	case BPF_S_ANC_VLAN_TAG:
 +	case BPF_S_ANC_VLAN_TAG_PRESENT:
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
 +		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(vlan_tci)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, vlan_tci));
 +		if (filter->code == BPF_S_ANC_VLAN_TAG) {
 +			/* nill %r5,0xefff */
 +			EMIT4_IMM(0xa5570000, ~VLAN_TAG_PRESENT);
 +		} else {
 +			/* nill %r5,0x1000 */
 +			EMIT4_IMM(0xa5570000, VLAN_TAG_PRESENT);
 +			/* srl %r5,12 */
 +			EMIT4_DISP(0x88500000, 12);
 +		}
 +		break;
 +	case BPF_S_ANC_CPU: /* A = smp_processor_id() */
 +#ifdef CONFIG_SMP
 +		/* l %r5,<d(cpu_nr)> */
 +		EMIT4_DISP(0x58500000, offsetof(struct _lowcore, cpu_nr));
 +#else
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +#endif
  		break;
  	default: /* too complex, give up */
 -		pr_err("Unknown opcode %02x\n", insn->code);
 -		return -1;
 -	}
 -	return insn_count;
 -}
 -
 -/*
 - * Compile eBPF program into s390x code
 - */
 -static int bpf_jit_prog(struct bpf_jit *jit, struct bpf_prog *fp)
 -{
 -	int i, insn_count;
 -
 -	jit->lit = jit->lit_start;
 -	jit->prg = 0;
 -
 -	bpf_jit_prologue(jit);
 -	for (i = 0; i < fp->len; i += insn_count) {
 -		insn_count = bpf_jit_insn(jit, fp, i);
 -		if (insn_count < 0)
 -			return -1;
 -		jit->addrs[i + 1] = jit->prg; /* Next instruction address */
 +		goto out;
  	}
 -	bpf_jit_epilogue(jit);
 -
 -	jit->lit_start = jit->prg;
 -	jit->size = jit->lit;
 -	jit->size_prg = jit->prg;
 +	addrs[i] = jit->prg - jit->start;
  	return 0;
 +out:
 +	return -1;
  }
  
 -/*
 - * Compile eBPF program "fp"
 - */
 -struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *fp)
 +void bpf_jit_compile(struct sk_filter *fp)
  {
 -	struct bpf_prog *tmp, *orig_fp = fp;
 -	struct bpf_binary_header *header;
 -	bool tmp_blinded = false;
 -	struct bpf_jit jit;
 -	int pass;
 +	unsigned long size, prg_len, lit_len;
 +	struct bpf_jit jit, cjit;
 +	unsigned int *addrs;
 +	int pass, i;
  
  	if (!bpf_jit_enable)
 -		return orig_fp;
 -
 -	tmp = bpf_jit_blind_constants(fp);
 -	/*
 -	 * If blinding was requested and we failed during blinding,
 -	 * we must fall back to the interpreter.
 -	 */
 -	if (IS_ERR(tmp))
 -		return orig_fp;
 -	if (tmp != fp) {
 -		tmp_blinded = true;
 -		fp = tmp;
 -	}
 -
 -	memset(&jit, 0, sizeof(jit));
 -	jit.addrs = kcalloc(fp->len + 1, sizeof(*jit.addrs), GFP_KERNEL);
 -	if (jit.addrs == NULL) {
 -		fp = orig_fp;
 -		goto out;
 -	}
 -	/*
 -	 * Three initial passes:
 -	 *   - 1/2: Determine clobbered registers
 -	 *   - 3:   Calculate program size and addrs arrray
 -	 */
 -	for (pass = 1; pass <= 3; pass++) {
 -		if (bpf_jit_prog(&jit, fp)) {
 -			fp = orig_fp;
 -			goto free_addrs;
 +		return;
 +	addrs = kcalloc(fp->len, sizeof(*addrs), GFP_KERNEL);
 +	if (addrs == NULL)
 +		return;
 +	memset(&jit, 0, sizeof(cjit));
 +	memset(&cjit, 0, sizeof(cjit));
 +
 +	for (pass = 0; pass < 10; pass++) {
 +		jit.prg = jit.start;
 +		jit.lit = jit.mid;
 +
 +		bpf_jit_prologue(&jit);
 +		bpf_jit_noleaks(&jit, fp->insns);
 +		for (i = 0; i < fp->len; i++) {
 +			if (bpf_jit_insn(&jit, fp->insns + i, addrs, i,
 +					 i == fp->len - 1))
 +				goto out;
  		}
 -	}
 -	/*
 -	 * Final pass: Allocate and generate program
 -	 */
 -	if (jit.size >= BPF_SIZE_MAX) {
 -		fp = orig_fp;
 -		goto free_addrs;
 -	}
 -	header = bpf_jit_binary_alloc(jit.size, &jit.prg_buf, 2, jit_fill_hole);
 -	if (!header) {
 -		fp = orig_fp;
 -		goto free_addrs;
 -	}
 -	if (bpf_jit_prog(&jit, fp)) {
 -		fp = orig_fp;
 -		goto free_addrs;
 +		bpf_jit_epilogue(&jit);
 +		if (jit.start) {
 +			WARN_ON(jit.prg > cjit.prg || jit.lit > cjit.lit);
 +			if (memcmp(&jit, &cjit, sizeof(jit)) == 0)
 +				break;
 +		} else if (jit.prg == cjit.prg && jit.lit == cjit.lit) {
 +			prg_len = jit.prg - jit.start;
 +			lit_len = jit.lit - jit.mid;
 +			size = max_t(unsigned long, prg_len + lit_len,
 +				     sizeof(struct work_struct));
 +			if (size >= BPF_SIZE_MAX)
 +				goto out;
 +			jit.start = module_alloc(size);
 +			if (!jit.start)
 +				goto out;
 +			jit.prg = jit.mid = jit.start + prg_len;
 +			jit.lit = jit.end = jit.start + prg_len + lit_len;
 +			jit.base_ip += (unsigned long) jit.start;
 +			jit.exit_ip += (unsigned long) jit.start;
 +			jit.ret0_ip += (unsigned long) jit.start;
 +		}
 +		cjit = jit;
  	}
  	if (bpf_jit_enable > 1) {
 -		bpf_jit_dump(fp->len, jit.size, pass, jit.prg_buf);
 -		print_fn_code(jit.prg_buf, jit.size_prg);
 +		pr_err("flen=%d proglen=%lu pass=%d image=%p\n",
 +		       fp->len, jit.end - jit.start, pass, jit.start);
 +		if (jit.start) {
 +			printk(KERN_ERR "JIT code:\n");
 +			print_fn_code(jit.start, jit.mid - jit.start);
 +			print_hex_dump(KERN_ERR, "JIT literals:\n",
 +				       DUMP_PREFIX_ADDRESS, 16, 1,
 +				       jit.mid, jit.end - jit.mid, false);
 +		}
  	}
 -	bpf_jit_binary_lock_ro(header);
 -	fp->bpf_func = (void *) jit.prg_buf;
 -	fp->jited = 1;
 -free_addrs:
 -	kfree(jit.addrs);
 +	if (jit.start)
 +		fp->bpf_func = (void *) jit.start;
  out:
 -	if (tmp_blinded)
 -		bpf_jit_prog_release_other(fp, fp == orig_fp ?
 -					   tmp : orig_fp);
 -	return fp;
 +	kfree(addrs);
 +}
 +
 +static void jit_free_defer(struct work_struct *arg)
 +{
 +	module_free(NULL, arg);
 +}
 +
 +/* run from softirq, we must use a work_struct to call
 + * module_free() from process context
 + */
 +void bpf_jit_free(struct sk_filter *fp)
 +{
 +	struct work_struct *work;
 +
 +	if (fp->bpf_func == sk_run_filter)
 +		return;
 +	work = (struct work_struct *)fp->bpf_func;
 +	INIT_WORK(work, jit_free_defer);
 +	schedule_work(work);
  }
diff --cc arch/x86/net/bpf_jit_comp.c
index 76c7b3a140ad,fec12eaa0dec..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -124,48 -110,1006 +124,982 @@@ static inline void bpf_flush_icache(voi
  #define CHOOSE_LOAD_FUNC(K, func) \
  	((int)K < 0 ? ((int)K >= SKF_LL_OFF ? func##_negative_offset : func) : func##_positive_offset)
  
 -/* pick a register outside of BPF range for JIT internal work */
 -#define AUX_REG (MAX_BPF_JIT_REG + 1)
 -
 -/* The following table maps BPF registers to x64 registers.
 - *
 - * x64 register r12 is unused, since if used as base address
 - * register in load/store instructions, it always needs an
 - * extra byte of encoding and is callee saved.
 - *
 - *  r9 caches skb->len - skb->data_len
 - * r10 caches skb->data, and used for blinding (if enabled)
 - */
 -static const int reg2hex[] = {
 -	[BPF_REG_0] = 0,  /* rax */
 -	[BPF_REG_1] = 7,  /* rdi */
 -	[BPF_REG_2] = 6,  /* rsi */
 -	[BPF_REG_3] = 2,  /* rdx */
 -	[BPF_REG_4] = 1,  /* rcx */
 -	[BPF_REG_5] = 0,  /* r8 */
 -	[BPF_REG_6] = 3,  /* rbx callee saved */
 -	[BPF_REG_7] = 5,  /* r13 callee saved */
 -	[BPF_REG_8] = 6,  /* r14 callee saved */
 -	[BPF_REG_9] = 7,  /* r15 callee saved */
 -	[BPF_REG_FP] = 5, /* rbp readonly */
 -	[BPF_REG_AX] = 2, /* r10 temp register */
 -	[AUX_REG] = 3,    /* r11 temp register */
 -};
 -
 -/* is_ereg() == true if BPF register 'reg' maps to x64 r8..r15
 - * which need extra byte of encoding.
 - * rax,rcx,...,rbp have simpler encoding
 +/* Helper to find the offset of pkt_type in sk_buff
 + * We want to make sure its still a 3bit field starting at a byte boundary.
   */
 -static bool is_ereg(u32 reg)
 +#define PKT_TYPE_MAX 7
 +static int pkt_type_offset(void)
  {
++<<<<<<< HEAD
 +	struct sk_buff skb_probe = {
 +		.pkt_type = ~0,
 +	};
 +	char *ct = (char *)&skb_probe;
 +	unsigned int off;
 +
 +	for (off = 0; off < sizeof(struct sk_buff); off++) {
 +		if (ct[off] == PKT_TYPE_MAX)
 +			return off;
++=======
+ 	return (1 << reg) & (BIT(BPF_REG_5) |
+ 			     BIT(AUX_REG) |
+ 			     BIT(BPF_REG_7) |
+ 			     BIT(BPF_REG_8) |
+ 			     BIT(BPF_REG_9) |
+ 			     BIT(BPF_REG_AX));
+ }
+ 
+ /* add modifiers if 'reg' maps to x64 registers r8..r15 */
+ static u8 add_1mod(u8 byte, u32 reg)
+ {
+ 	if (is_ereg(reg))
+ 		byte |= 1;
+ 	return byte;
+ }
+ 
+ static u8 add_2mod(u8 byte, u32 r1, u32 r2)
+ {
+ 	if (is_ereg(r1))
+ 		byte |= 1;
+ 	if (is_ereg(r2))
+ 		byte |= 4;
+ 	return byte;
+ }
+ 
+ /* encode 'dst_reg' register into x64 opcode 'byte' */
+ static u8 add_1reg(u8 byte, u32 dst_reg)
+ {
+ 	return byte + reg2hex[dst_reg];
+ }
+ 
+ /* encode 'dst_reg' and 'src_reg' registers into x64 opcode 'byte' */
+ static u8 add_2reg(u8 byte, u32 dst_reg, u32 src_reg)
+ {
+ 	return byte + reg2hex[dst_reg] + (reg2hex[src_reg] << 3);
+ }
+ 
+ static void jit_fill_hole(void *area, unsigned int size)
+ {
+ 	/* fill whole space with int3 instructions */
+ 	memset(area, 0xcc, size);
+ }
+ 
+ struct jit_context {
+ 	int cleanup_addr; /* epilogue code offset */
+ 	bool seen_ld_abs;
+ 	bool seen_ax_reg;
+ };
+ 
+ /* maximum number of bytes emitted while JITing one eBPF insn */
+ #define BPF_MAX_INSN_SIZE	128
+ #define BPF_INSN_SAFETY		64
+ 
+ #define STACKSIZE \
+ 	(MAX_BPF_STACK + \
+ 	 32 /* space for rbx, r13, r14, r15 */ + \
+ 	 8 /* space for skb_copy_bits() buffer */)
+ 
+ #define PROLOGUE_SIZE 48
+ 
+ /* emit x64 prologue code for BPF program and check it's size.
+  * bpf_tail_call helper will skip it while jumping into another program
+  */
+ static void emit_prologue(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	EMIT1(0x55); /* push rbp */
+ 	EMIT3(0x48, 0x89, 0xE5); /* mov rbp,rsp */
+ 
+ 	/* sub rsp, STACKSIZE */
+ 	EMIT3_off32(0x48, 0x81, 0xEC, STACKSIZE);
+ 
+ 	/* all classic BPF filters use R6(rbx) save it */
+ 
+ 	/* mov qword ptr [rbp-X],rbx */
+ 	EMIT3_off32(0x48, 0x89, 0x9D, -STACKSIZE);
+ 
+ 	/* bpf_convert_filter() maps classic BPF register X to R7 and uses R8
+ 	 * as temporary, so all tcpdump filters need to spill/fill R7(r13) and
+ 	 * R8(r14). R9(r15) spill could be made conditional, but there is only
+ 	 * one 'bpf_error' return path out of helper functions inside bpf_jit.S
+ 	 * The overhead of extra spill is negligible for any filter other
+ 	 * than synthetic ones. Therefore not worth adding complexity.
+ 	 */
+ 
+ 	/* mov qword ptr [rbp-X],r13 */
+ 	EMIT3_off32(0x4C, 0x89, 0xAD, -STACKSIZE + 8);
+ 	/* mov qword ptr [rbp-X],r14 */
+ 	EMIT3_off32(0x4C, 0x89, 0xB5, -STACKSIZE + 16);
+ 	/* mov qword ptr [rbp-X],r15 */
+ 	EMIT3_off32(0x4C, 0x89, 0xBD, -STACKSIZE + 24);
+ 
+ 	/* Clear the tail call counter (tail_call_cnt): for eBPF tail calls
+ 	 * we need to reset the counter to 0. It's done in two instructions,
+ 	 * resetting rax register to 0 (xor on eax gets 0 extended), and
+ 	 * moving it to the counter location.
+ 	 */
+ 
+ 	/* xor eax, eax */
+ 	EMIT2(0x31, 0xc0);
+ 	/* mov qword ptr [rbp-X], rax */
+ 	EMIT3_off32(0x48, 0x89, 0x85, -STACKSIZE + 32);
+ 
+ 	BUILD_BUG_ON(cnt != PROLOGUE_SIZE);
+ 	*pprog = prog;
+ }
+ 
+ /* generate the following code:
+  * ... bpf_tail_call(void *ctx, struct bpf_array *array, u64 index) ...
+  *   if (index >= array->map.max_entries)
+  *     goto out;
+  *   if (++tail_call_cnt > MAX_TAIL_CALL_CNT)
+  *     goto out;
+  *   prog = array->ptrs[index];
+  *   if (prog == NULL)
+  *     goto out;
+  *   goto *(prog->bpf_func + prologue_size);
+  * out:
+  */
+ static void emit_bpf_tail_call(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int label1, label2, label3;
+ 	int cnt = 0;
+ 
+ 	/* rdi - pointer to ctx
+ 	 * rsi - pointer to bpf_array
+ 	 * rdx - index in bpf_array
+ 	 */
+ 
+ 	/* if (index >= array->map.max_entries)
+ 	 *   goto out;
+ 	 */
+ 	EMIT4(0x48, 0x8B, 0x46,                   /* mov rax, qword ptr [rsi + 16] */
+ 	      offsetof(struct bpf_array, map.max_entries));
+ 	EMIT3(0x48, 0x39, 0xD0);                  /* cmp rax, rdx */
+ #define OFFSET1 47 /* number of bytes to jump */
+ 	EMIT2(X86_JBE, OFFSET1);                  /* jbe out */
+ 	label1 = cnt;
+ 
+ 	/* if (tail_call_cnt > MAX_TAIL_CALL_CNT)
+ 	 *   goto out;
+ 	 */
+ 	EMIT2_off32(0x8B, 0x85, -STACKSIZE + 36); /* mov eax, dword ptr [rbp - 516] */
+ 	EMIT3(0x83, 0xF8, MAX_TAIL_CALL_CNT);     /* cmp eax, MAX_TAIL_CALL_CNT */
+ #define OFFSET2 36
+ 	EMIT2(X86_JA, OFFSET2);                   /* ja out */
+ 	label2 = cnt;
+ 	EMIT3(0x83, 0xC0, 0x01);                  /* add eax, 1 */
+ 	EMIT2_off32(0x89, 0x85, -STACKSIZE + 36); /* mov dword ptr [rbp - 516], eax */
+ 
+ 	/* prog = array->ptrs[index]; */
+ 	EMIT4_off32(0x48, 0x8D, 0x84, 0xD6,       /* lea rax, [rsi + rdx * 8 + offsetof(...)] */
+ 		    offsetof(struct bpf_array, ptrs));
+ 	EMIT3(0x48, 0x8B, 0x00);                  /* mov rax, qword ptr [rax] */
+ 
+ 	/* if (prog == NULL)
+ 	 *   goto out;
+ 	 */
+ 	EMIT4(0x48, 0x83, 0xF8, 0x00);            /* cmp rax, 0 */
+ #define OFFSET3 10
+ 	EMIT2(X86_JE, OFFSET3);                   /* je out */
+ 	label3 = cnt;
+ 
+ 	/* goto *(prog->bpf_func + prologue_size); */
+ 	EMIT4(0x48, 0x8B, 0x40,                   /* mov rax, qword ptr [rax + 32] */
+ 	      offsetof(struct bpf_prog, bpf_func));
+ 	EMIT4(0x48, 0x83, 0xC0, PROLOGUE_SIZE);   /* add rax, prologue_size */
+ 
+ 	/* now we're ready to jump into next BPF program
+ 	 * rdi == ctx (1st arg)
+ 	 * rax == prog->bpf_func + prologue_size
+ 	 */
+ 	EMIT2(0xFF, 0xE0);                        /* jmp rax */
+ 
+ 	/* out: */
+ 	BUILD_BUG_ON(cnt - label1 != OFFSET1);
+ 	BUILD_BUG_ON(cnt - label2 != OFFSET2);
+ 	BUILD_BUG_ON(cnt - label3 != OFFSET3);
+ 	*pprog = prog;
+ }
+ 
+ 
+ static void emit_load_skb_data_hlen(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	/* r9d = skb->len - skb->data_len (headlen)
+ 	 * r10 = skb->data
+ 	 */
+ 	/* mov %r9d, off32(%rdi) */
+ 	EMIT3_off32(0x44, 0x8b, 0x8f, offsetof(struct sk_buff, len));
+ 
+ 	/* sub %r9d, off32(%rdi) */
+ 	EMIT3_off32(0x44, 0x2b, 0x8f, offsetof(struct sk_buff, data_len));
+ 
+ 	/* mov %r10, off32(%rdi) */
+ 	EMIT3_off32(0x4c, 0x8b, 0x97, offsetof(struct sk_buff, data));
+ 	*pprog = prog;
+ }
+ 
+ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,
+ 		  int oldproglen, struct jit_context *ctx)
+ {
+ 	struct bpf_insn *insn = bpf_prog->insnsi;
+ 	int insn_cnt = bpf_prog->len;
+ 	bool seen_ld_abs = ctx->seen_ld_abs | (oldproglen == 0);
+ 	bool seen_ax_reg = ctx->seen_ax_reg | (oldproglen == 0);
+ 	bool seen_exit = false;
+ 	u8 temp[BPF_MAX_INSN_SIZE + BPF_INSN_SAFETY];
+ 	int i, cnt = 0;
+ 	int proglen = 0;
+ 	u8 *prog = temp;
+ 
+ 	emit_prologue(&prog);
+ 
+ 	if (seen_ld_abs)
+ 		emit_load_skb_data_hlen(&prog);
+ 
+ 	for (i = 0; i < insn_cnt; i++, insn++) {
+ 		const s32 imm32 = insn->imm;
+ 		u32 dst_reg = insn->dst_reg;
+ 		u32 src_reg = insn->src_reg;
+ 		u8 b1 = 0, b2 = 0, b3 = 0;
+ 		s64 jmp_offset;
+ 		u8 jmp_cond;
+ 		bool reload_skb_data;
+ 		int ilen;
+ 		u8 *func;
+ 
+ 		if (dst_reg == BPF_REG_AX || src_reg == BPF_REG_AX)
+ 			ctx->seen_ax_reg = seen_ax_reg = true;
+ 
+ 		switch (insn->code) {
+ 			/* ALU */
+ 		case BPF_ALU | BPF_ADD | BPF_X:
+ 		case BPF_ALU | BPF_SUB | BPF_X:
+ 		case BPF_ALU | BPF_AND | BPF_X:
+ 		case BPF_ALU | BPF_OR | BPF_X:
+ 		case BPF_ALU | BPF_XOR | BPF_X:
+ 		case BPF_ALU64 | BPF_ADD | BPF_X:
+ 		case BPF_ALU64 | BPF_SUB | BPF_X:
+ 		case BPF_ALU64 | BPF_AND | BPF_X:
+ 		case BPF_ALU64 | BPF_OR | BPF_X:
+ 		case BPF_ALU64 | BPF_XOR | BPF_X:
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_ADD: b2 = 0x01; break;
+ 			case BPF_SUB: b2 = 0x29; break;
+ 			case BPF_AND: b2 = 0x21; break;
+ 			case BPF_OR: b2 = 0x09; break;
+ 			case BPF_XOR: b2 = 0x31; break;
+ 			}
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_2mod(0x48, dst_reg, src_reg));
+ 			else if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT1(add_2mod(0x40, dst_reg, src_reg));
+ 			EMIT2(b2, add_2reg(0xC0, dst_reg, src_reg));
+ 			break;
+ 
+ 			/* mov dst, src */
+ 		case BPF_ALU64 | BPF_MOV | BPF_X:
+ 			EMIT_mov(dst_reg, src_reg);
+ 			break;
+ 
+ 			/* mov32 dst, src */
+ 		case BPF_ALU | BPF_MOV | BPF_X:
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT1(add_2mod(0x40, dst_reg, src_reg));
+ 			EMIT2(0x89, add_2reg(0xC0, dst_reg, src_reg));
+ 			break;
+ 
+ 			/* neg dst */
+ 		case BPF_ALU | BPF_NEG:
+ 		case BPF_ALU64 | BPF_NEG:
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 			EMIT2(0xF7, add_1reg(0xD8, dst_reg));
+ 			break;
+ 
+ 		case BPF_ALU | BPF_ADD | BPF_K:
+ 		case BPF_ALU | BPF_SUB | BPF_K:
+ 		case BPF_ALU | BPF_AND | BPF_K:
+ 		case BPF_ALU | BPF_OR | BPF_K:
+ 		case BPF_ALU | BPF_XOR | BPF_K:
+ 		case BPF_ALU64 | BPF_ADD | BPF_K:
+ 		case BPF_ALU64 | BPF_SUB | BPF_K:
+ 		case BPF_ALU64 | BPF_AND | BPF_K:
+ 		case BPF_ALU64 | BPF_OR | BPF_K:
+ 		case BPF_ALU64 | BPF_XOR | BPF_K:
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_ADD: b3 = 0xC0; break;
+ 			case BPF_SUB: b3 = 0xE8; break;
+ 			case BPF_AND: b3 = 0xE0; break;
+ 			case BPF_OR: b3 = 0xC8; break;
+ 			case BPF_XOR: b3 = 0xF0; break;
+ 			}
+ 
+ 			if (is_imm8(imm32))
+ 				EMIT3(0x83, add_1reg(b3, dst_reg), imm32);
+ 			else
+ 				EMIT2_off32(0x81, add_1reg(b3, dst_reg), imm32);
+ 			break;
+ 
+ 		case BPF_ALU64 | BPF_MOV | BPF_K:
+ 			/* optimization: if imm32 is positive,
+ 			 * use 'mov eax, imm32' (which zero-extends imm32)
+ 			 * to save 2 bytes
+ 			 */
+ 			if (imm32 < 0) {
+ 				/* 'mov rax, imm32' sign extends imm32 */
+ 				b1 = add_1mod(0x48, dst_reg);
+ 				b2 = 0xC7;
+ 				b3 = 0xC0;
+ 				EMIT3_off32(b1, b2, add_1reg(b3, dst_reg), imm32);
+ 				break;
+ 			}
+ 
+ 		case BPF_ALU | BPF_MOV | BPF_K:
+ 			/* optimization: if imm32 is zero, use 'xor <dst>,<dst>'
+ 			 * to save 3 bytes.
+ 			 */
+ 			if (imm32 == 0) {
+ 				if (is_ereg(dst_reg))
+ 					EMIT1(add_2mod(0x40, dst_reg, dst_reg));
+ 				b2 = 0x31; /* xor */
+ 				b3 = 0xC0;
+ 				EMIT2(b2, add_2reg(b3, dst_reg, dst_reg));
+ 				break;
+ 			}
+ 
+ 			/* mov %eax, imm32 */
+ 			if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 			EMIT1_off32(add_1reg(0xB8, dst_reg), imm32);
+ 			break;
+ 
+ 		case BPF_LD | BPF_IMM | BPF_DW:
+ 			/* optimization: if imm64 is zero, use 'xor <dst>,<dst>'
+ 			 * to save 7 bytes.
+ 			 */
+ 			if (insn[0].imm == 0 && insn[1].imm == 0) {
+ 				b1 = add_2mod(0x48, dst_reg, dst_reg);
+ 				b2 = 0x31; /* xor */
+ 				b3 = 0xC0;
+ 				EMIT3(b1, b2, add_2reg(b3, dst_reg, dst_reg));
+ 
+ 				insn++;
+ 				i++;
+ 				break;
+ 			}
+ 
+ 			/* movabsq %rax, imm64 */
+ 			EMIT2(add_1mod(0x48, dst_reg), add_1reg(0xB8, dst_reg));
+ 			EMIT(insn[0].imm, 4);
+ 			EMIT(insn[1].imm, 4);
+ 
+ 			insn++;
+ 			i++;
+ 			break;
+ 
+ 			/* dst %= src, dst /= src, dst %= imm32, dst /= imm32 */
+ 		case BPF_ALU | BPF_MOD | BPF_X:
+ 		case BPF_ALU | BPF_DIV | BPF_X:
+ 		case BPF_ALU | BPF_MOD | BPF_K:
+ 		case BPF_ALU | BPF_DIV | BPF_K:
+ 		case BPF_ALU64 | BPF_MOD | BPF_X:
+ 		case BPF_ALU64 | BPF_DIV | BPF_X:
+ 		case BPF_ALU64 | BPF_MOD | BPF_K:
+ 		case BPF_ALU64 | BPF_DIV | BPF_K:
+ 			EMIT1(0x50); /* push rax */
+ 			EMIT1(0x52); /* push rdx */
+ 
+ 			if (BPF_SRC(insn->code) == BPF_X)
+ 				/* mov r11, src_reg */
+ 				EMIT_mov(AUX_REG, src_reg);
+ 			else
+ 				/* mov r11, imm32 */
+ 				EMIT3_off32(0x49, 0xC7, 0xC3, imm32);
+ 
+ 			/* mov rax, dst_reg */
+ 			EMIT_mov(BPF_REG_0, dst_reg);
+ 
+ 			/* xor edx, edx
+ 			 * equivalent to 'xor rdx, rdx', but one byte less
+ 			 */
+ 			EMIT2(0x31, 0xd2);
+ 
+ 			if (BPF_SRC(insn->code) == BPF_X) {
+ 				/* if (src_reg == 0) return 0 */
+ 
+ 				/* cmp r11, 0 */
+ 				EMIT4(0x49, 0x83, 0xFB, 0x00);
+ 
+ 				/* jne .+9 (skip over pop, pop, xor and jmp) */
+ 				EMIT2(X86_JNE, 1 + 1 + 2 + 5);
+ 				EMIT1(0x5A); /* pop rdx */
+ 				EMIT1(0x58); /* pop rax */
+ 				EMIT2(0x31, 0xc0); /* xor eax, eax */
+ 
+ 				/* jmp cleanup_addr
+ 				 * addrs[i] - 11, because there are 11 bytes
+ 				 * after this insn: div, mov, pop, pop, mov
+ 				 */
+ 				jmp_offset = ctx->cleanup_addr - (addrs[i] - 11);
+ 				EMIT1_off32(0xE9, jmp_offset);
+ 			}
+ 
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				/* div r11 */
+ 				EMIT3(0x49, 0xF7, 0xF3);
+ 			else
+ 				/* div r11d */
+ 				EMIT3(0x41, 0xF7, 0xF3);
+ 
+ 			if (BPF_OP(insn->code) == BPF_MOD)
+ 				/* mov r11, rdx */
+ 				EMIT3(0x49, 0x89, 0xD3);
+ 			else
+ 				/* mov r11, rax */
+ 				EMIT3(0x49, 0x89, 0xC3);
+ 
+ 			EMIT1(0x5A); /* pop rdx */
+ 			EMIT1(0x58); /* pop rax */
+ 
+ 			/* mov dst_reg, r11 */
+ 			EMIT_mov(dst_reg, AUX_REG);
+ 			break;
+ 
+ 		case BPF_ALU | BPF_MUL | BPF_K:
+ 		case BPF_ALU | BPF_MUL | BPF_X:
+ 		case BPF_ALU64 | BPF_MUL | BPF_K:
+ 		case BPF_ALU64 | BPF_MUL | BPF_X:
+ 			EMIT1(0x50); /* push rax */
+ 			EMIT1(0x52); /* push rdx */
+ 
+ 			/* mov r11, dst_reg */
+ 			EMIT_mov(AUX_REG, dst_reg);
+ 
+ 			if (BPF_SRC(insn->code) == BPF_X)
+ 				/* mov rax, src_reg */
+ 				EMIT_mov(BPF_REG_0, src_reg);
+ 			else
+ 				/* mov rax, imm32 */
+ 				EMIT3_off32(0x48, 0xC7, 0xC0, imm32);
+ 
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, AUX_REG));
+ 			else if (is_ereg(AUX_REG))
+ 				EMIT1(add_1mod(0x40, AUX_REG));
+ 			/* mul(q) r11 */
+ 			EMIT2(0xF7, add_1reg(0xE0, AUX_REG));
+ 
+ 			/* mov r11, rax */
+ 			EMIT_mov(AUX_REG, BPF_REG_0);
+ 
+ 			EMIT1(0x5A); /* pop rdx */
+ 			EMIT1(0x58); /* pop rax */
+ 
+ 			/* mov dst_reg, r11 */
+ 			EMIT_mov(dst_reg, AUX_REG);
+ 			break;
+ 
+ 			/* shifts */
+ 		case BPF_ALU | BPF_LSH | BPF_K:
+ 		case BPF_ALU | BPF_RSH | BPF_K:
+ 		case BPF_ALU | BPF_ARSH | BPF_K:
+ 		case BPF_ALU64 | BPF_LSH | BPF_K:
+ 		case BPF_ALU64 | BPF_RSH | BPF_K:
+ 		case BPF_ALU64 | BPF_ARSH | BPF_K:
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_LSH: b3 = 0xE0; break;
+ 			case BPF_RSH: b3 = 0xE8; break;
+ 			case BPF_ARSH: b3 = 0xF8; break;
+ 			}
+ 			EMIT3(0xC1, add_1reg(b3, dst_reg), imm32);
+ 			break;
+ 
+ 		case BPF_ALU | BPF_LSH | BPF_X:
+ 		case BPF_ALU | BPF_RSH | BPF_X:
+ 		case BPF_ALU | BPF_ARSH | BPF_X:
+ 		case BPF_ALU64 | BPF_LSH | BPF_X:
+ 		case BPF_ALU64 | BPF_RSH | BPF_X:
+ 		case BPF_ALU64 | BPF_ARSH | BPF_X:
+ 
+ 			/* check for bad case when dst_reg == rcx */
+ 			if (dst_reg == BPF_REG_4) {
+ 				/* mov r11, dst_reg */
+ 				EMIT_mov(AUX_REG, dst_reg);
+ 				dst_reg = AUX_REG;
+ 			}
+ 
+ 			if (src_reg != BPF_REG_4) { /* common case */
+ 				EMIT1(0x51); /* push rcx */
+ 
+ 				/* mov rcx, src_reg */
+ 				EMIT_mov(BPF_REG_4, src_reg);
+ 			}
+ 
+ 			/* shl %rax, %cl | shr %rax, %cl | sar %rax, %cl */
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_LSH: b3 = 0xE0; break;
+ 			case BPF_RSH: b3 = 0xE8; break;
+ 			case BPF_ARSH: b3 = 0xF8; break;
+ 			}
+ 			EMIT2(0xD3, add_1reg(b3, dst_reg));
+ 
+ 			if (src_reg != BPF_REG_4)
+ 				EMIT1(0x59); /* pop rcx */
+ 
+ 			if (insn->dst_reg == BPF_REG_4)
+ 				/* mov dst_reg, r11 */
+ 				EMIT_mov(insn->dst_reg, AUX_REG);
+ 			break;
+ 
+ 		case BPF_ALU | BPF_END | BPF_FROM_BE:
+ 			switch (imm32) {
+ 			case 16:
+ 				/* emit 'ror %ax, 8' to swap lower 2 bytes */
+ 				EMIT1(0x66);
+ 				if (is_ereg(dst_reg))
+ 					EMIT1(0x41);
+ 				EMIT3(0xC1, add_1reg(0xC8, dst_reg), 8);
+ 
+ 				/* emit 'movzwl eax, ax' */
+ 				if (is_ereg(dst_reg))
+ 					EMIT3(0x45, 0x0F, 0xB7);
+ 				else
+ 					EMIT2(0x0F, 0xB7);
+ 				EMIT1(add_2reg(0xC0, dst_reg, dst_reg));
+ 				break;
+ 			case 32:
+ 				/* emit 'bswap eax' to swap lower 4 bytes */
+ 				if (is_ereg(dst_reg))
+ 					EMIT2(0x41, 0x0F);
+ 				else
+ 					EMIT1(0x0F);
+ 				EMIT1(add_1reg(0xC8, dst_reg));
+ 				break;
+ 			case 64:
+ 				/* emit 'bswap rax' to swap 8 bytes */
+ 				EMIT3(add_1mod(0x48, dst_reg), 0x0F,
+ 				      add_1reg(0xC8, dst_reg));
+ 				break;
+ 			}
+ 			break;
+ 
+ 		case BPF_ALU | BPF_END | BPF_FROM_LE:
+ 			switch (imm32) {
+ 			case 16:
+ 				/* emit 'movzwl eax, ax' to zero extend 16-bit
+ 				 * into 64 bit
+ 				 */
+ 				if (is_ereg(dst_reg))
+ 					EMIT3(0x45, 0x0F, 0xB7);
+ 				else
+ 					EMIT2(0x0F, 0xB7);
+ 				EMIT1(add_2reg(0xC0, dst_reg, dst_reg));
+ 				break;
+ 			case 32:
+ 				/* emit 'mov eax, eax' to clear upper 32-bits */
+ 				if (is_ereg(dst_reg))
+ 					EMIT1(0x45);
+ 				EMIT2(0x89, add_2reg(0xC0, dst_reg, dst_reg));
+ 				break;
+ 			case 64:
+ 				/* nop */
+ 				break;
+ 			}
+ 			break;
+ 
+ 			/* ST: *(u8*)(dst_reg + off) = imm */
+ 		case BPF_ST | BPF_MEM | BPF_B:
+ 			if (is_ereg(dst_reg))
+ 				EMIT2(0x41, 0xC6);
+ 			else
+ 				EMIT1(0xC6);
+ 			goto st;
+ 		case BPF_ST | BPF_MEM | BPF_H:
+ 			if (is_ereg(dst_reg))
+ 				EMIT3(0x66, 0x41, 0xC7);
+ 			else
+ 				EMIT2(0x66, 0xC7);
+ 			goto st;
+ 		case BPF_ST | BPF_MEM | BPF_W:
+ 			if (is_ereg(dst_reg))
+ 				EMIT2(0x41, 0xC7);
+ 			else
+ 				EMIT1(0xC7);
+ 			goto st;
+ 		case BPF_ST | BPF_MEM | BPF_DW:
+ 			EMIT2(add_1mod(0x48, dst_reg), 0xC7);
+ 
+ st:			if (is_imm8(insn->off))
+ 				EMIT2(add_1reg(0x40, dst_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_1reg(0x80, dst_reg), insn->off);
+ 
+ 			EMIT(imm32, bpf_size_to_x86_bytes(BPF_SIZE(insn->code)));
+ 			break;
+ 
+ 			/* STX: *(u8*)(dst_reg + off) = src_reg */
+ 		case BPF_STX | BPF_MEM | BPF_B:
+ 			/* emit 'mov byte ptr [rax + off], al' */
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg) ||
+ 			    /* have to add extra byte for x86 SIL, DIL regs */
+ 			    src_reg == BPF_REG_1 || src_reg == BPF_REG_2)
+ 				EMIT2(add_2mod(0x40, dst_reg, src_reg), 0x88);
+ 			else
+ 				EMIT1(0x88);
+ 			goto stx;
+ 		case BPF_STX | BPF_MEM | BPF_H:
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT3(0x66, add_2mod(0x40, dst_reg, src_reg), 0x89);
+ 			else
+ 				EMIT2(0x66, 0x89);
+ 			goto stx;
+ 		case BPF_STX | BPF_MEM | BPF_W:
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT2(add_2mod(0x40, dst_reg, src_reg), 0x89);
+ 			else
+ 				EMIT1(0x89);
+ 			goto stx;
+ 		case BPF_STX | BPF_MEM | BPF_DW:
+ 			EMIT2(add_2mod(0x48, dst_reg, src_reg), 0x89);
+ stx:			if (is_imm8(insn->off))
+ 				EMIT2(add_2reg(0x40, dst_reg, src_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_2reg(0x80, dst_reg, src_reg),
+ 					    insn->off);
+ 			break;
+ 
+ 			/* LDX: dst_reg = *(u8*)(src_reg + off) */
+ 		case BPF_LDX | BPF_MEM | BPF_B:
+ 			/* emit 'movzx rax, byte ptr [rax + off]' */
+ 			EMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB6);
+ 			goto ldx;
+ 		case BPF_LDX | BPF_MEM | BPF_H:
+ 			/* emit 'movzx rax, word ptr [rax + off]' */
+ 			EMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB7);
+ 			goto ldx;
+ 		case BPF_LDX | BPF_MEM | BPF_W:
+ 			/* emit 'mov eax, dword ptr [rax+0x14]' */
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT2(add_2mod(0x40, src_reg, dst_reg), 0x8B);
+ 			else
+ 				EMIT1(0x8B);
+ 			goto ldx;
+ 		case BPF_LDX | BPF_MEM | BPF_DW:
+ 			/* emit 'mov rax, qword ptr [rax+0x14]' */
+ 			EMIT2(add_2mod(0x48, src_reg, dst_reg), 0x8B);
+ ldx:			/* if insn->off == 0 we can save one extra byte, but
+ 			 * special case of x86 r13 which always needs an offset
+ 			 * is not worth the hassle
+ 			 */
+ 			if (is_imm8(insn->off))
+ 				EMIT2(add_2reg(0x40, src_reg, dst_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_2reg(0x80, src_reg, dst_reg),
+ 					    insn->off);
+ 			break;
+ 
+ 			/* STX XADD: lock *(u32*)(dst_reg + off) += src_reg */
+ 		case BPF_STX | BPF_XADD | BPF_W:
+ 			/* emit 'lock add dword ptr [rax + off], eax' */
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT3(0xF0, add_2mod(0x40, dst_reg, src_reg), 0x01);
+ 			else
+ 				EMIT2(0xF0, 0x01);
+ 			goto xadd;
+ 		case BPF_STX | BPF_XADD | BPF_DW:
+ 			EMIT3(0xF0, add_2mod(0x48, dst_reg, src_reg), 0x01);
+ xadd:			if (is_imm8(insn->off))
+ 				EMIT2(add_2reg(0x40, dst_reg, src_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_2reg(0x80, dst_reg, src_reg),
+ 					    insn->off);
+ 			break;
+ 
+ 			/* call */
+ 		case BPF_JMP | BPF_CALL:
+ 			func = (u8 *) __bpf_call_base + imm32;
+ 			jmp_offset = func - (image + addrs[i]);
+ 			if (seen_ld_abs) {
+ 				reload_skb_data = bpf_helper_changes_pkt_data(func);
+ 				if (reload_skb_data) {
+ 					EMIT1(0x57); /* push %rdi */
+ 					jmp_offset += 22; /* pop, mov, sub, mov */
+ 				} else {
+ 					EMIT2(0x41, 0x52); /* push %r10 */
+ 					EMIT2(0x41, 0x51); /* push %r9 */
+ 					/* need to adjust jmp offset, since
+ 					 * pop %r9, pop %r10 take 4 bytes after call insn
+ 					 */
+ 					jmp_offset += 4;
+ 				}
+ 			}
+ 			if (!imm32 || !is_simm32(jmp_offset)) {
+ 				pr_err("unsupported bpf func %d addr %p image %p\n",
+ 				       imm32, func, image);
+ 				return -EINVAL;
+ 			}
+ 			EMIT1_off32(0xE8, jmp_offset);
+ 			if (seen_ld_abs) {
+ 				if (reload_skb_data) {
+ 					EMIT1(0x5F); /* pop %rdi */
+ 					emit_load_skb_data_hlen(&prog);
+ 				} else {
+ 					EMIT2(0x41, 0x59); /* pop %r9 */
+ 					EMIT2(0x41, 0x5A); /* pop %r10 */
+ 				}
+ 			}
+ 			break;
+ 
+ 		case BPF_JMP | BPF_TAIL_CALL:
+ 			emit_bpf_tail_call(&prog);
+ 			break;
+ 
+ 			/* cond jump */
+ 		case BPF_JMP | BPF_JEQ | BPF_X:
+ 		case BPF_JMP | BPF_JNE | BPF_X:
+ 		case BPF_JMP | BPF_JGT | BPF_X:
+ 		case BPF_JMP | BPF_JGE | BPF_X:
+ 		case BPF_JMP | BPF_JSGT | BPF_X:
+ 		case BPF_JMP | BPF_JSGE | BPF_X:
+ 			/* cmp dst_reg, src_reg */
+ 			EMIT3(add_2mod(0x48, dst_reg, src_reg), 0x39,
+ 			      add_2reg(0xC0, dst_reg, src_reg));
+ 			goto emit_cond_jmp;
+ 
+ 		case BPF_JMP | BPF_JSET | BPF_X:
+ 			/* test dst_reg, src_reg */
+ 			EMIT3(add_2mod(0x48, dst_reg, src_reg), 0x85,
+ 			      add_2reg(0xC0, dst_reg, src_reg));
+ 			goto emit_cond_jmp;
+ 
+ 		case BPF_JMP | BPF_JSET | BPF_K:
+ 			/* test dst_reg, imm32 */
+ 			EMIT1(add_1mod(0x48, dst_reg));
+ 			EMIT2_off32(0xF7, add_1reg(0xC0, dst_reg), imm32);
+ 			goto emit_cond_jmp;
+ 
+ 		case BPF_JMP | BPF_JEQ | BPF_K:
+ 		case BPF_JMP | BPF_JNE | BPF_K:
+ 		case BPF_JMP | BPF_JGT | BPF_K:
+ 		case BPF_JMP | BPF_JGE | BPF_K:
+ 		case BPF_JMP | BPF_JSGT | BPF_K:
+ 		case BPF_JMP | BPF_JSGE | BPF_K:
+ 			/* cmp dst_reg, imm8/32 */
+ 			EMIT1(add_1mod(0x48, dst_reg));
+ 
+ 			if (is_imm8(imm32))
+ 				EMIT3(0x83, add_1reg(0xF8, dst_reg), imm32);
+ 			else
+ 				EMIT2_off32(0x81, add_1reg(0xF8, dst_reg), imm32);
+ 
+ emit_cond_jmp:		/* convert BPF opcode to x86 */
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_JEQ:
+ 				jmp_cond = X86_JE;
+ 				break;
+ 			case BPF_JSET:
+ 			case BPF_JNE:
+ 				jmp_cond = X86_JNE;
+ 				break;
+ 			case BPF_JGT:
+ 				/* GT is unsigned '>', JA in x86 */
+ 				jmp_cond = X86_JA;
+ 				break;
+ 			case BPF_JGE:
+ 				/* GE is unsigned '>=', JAE in x86 */
+ 				jmp_cond = X86_JAE;
+ 				break;
+ 			case BPF_JSGT:
+ 				/* signed '>', GT in x86 */
+ 				jmp_cond = X86_JG;
+ 				break;
+ 			case BPF_JSGE:
+ 				/* signed '>=', GE in x86 */
+ 				jmp_cond = X86_JGE;
+ 				break;
+ 			default: /* to silence gcc warning */
+ 				return -EFAULT;
+ 			}
+ 			jmp_offset = addrs[i + insn->off] - addrs[i];
+ 			if (is_imm8(jmp_offset)) {
+ 				EMIT2(jmp_cond, jmp_offset);
+ 			} else if (is_simm32(jmp_offset)) {
+ 				EMIT2_off32(0x0F, jmp_cond + 0x10, jmp_offset);
+ 			} else {
+ 				pr_err("cond_jmp gen bug %llx\n", jmp_offset);
+ 				return -EFAULT;
+ 			}
+ 
+ 			break;
+ 
+ 		case BPF_JMP | BPF_JA:
+ 			jmp_offset = addrs[i + insn->off] - addrs[i];
+ 			if (!jmp_offset)
+ 				/* optimize out nop jumps */
+ 				break;
+ emit_jmp:
+ 			if (is_imm8(jmp_offset)) {
+ 				EMIT2(0xEB, jmp_offset);
+ 			} else if (is_simm32(jmp_offset)) {
+ 				EMIT1_off32(0xE9, jmp_offset);
+ 			} else {
+ 				pr_err("jmp gen bug %llx\n", jmp_offset);
+ 				return -EFAULT;
+ 			}
+ 			break;
+ 
+ 		case BPF_LD | BPF_IND | BPF_W:
+ 			func = sk_load_word;
+ 			goto common_load;
+ 		case BPF_LD | BPF_ABS | BPF_W:
+ 			func = CHOOSE_LOAD_FUNC(imm32, sk_load_word);
+ common_load:
+ 			ctx->seen_ld_abs = seen_ld_abs = true;
+ 			jmp_offset = func - (image + addrs[i]);
+ 			if (!func || !is_simm32(jmp_offset)) {
+ 				pr_err("unsupported bpf func %d addr %p image %p\n",
+ 				       imm32, func, image);
+ 				return -EINVAL;
+ 			}
+ 			if (BPF_MODE(insn->code) == BPF_ABS) {
+ 				/* mov %esi, imm32 */
+ 				EMIT1_off32(0xBE, imm32);
+ 			} else {
+ 				/* mov %rsi, src_reg */
+ 				EMIT_mov(BPF_REG_2, src_reg);
+ 				if (imm32) {
+ 					if (is_imm8(imm32))
+ 						/* add %esi, imm8 */
+ 						EMIT3(0x83, 0xC6, imm32);
+ 					else
+ 						/* add %esi, imm32 */
+ 						EMIT2_off32(0x81, 0xC6, imm32);
+ 				}
+ 			}
+ 			/* skb pointer is in R6 (%rbx), it will be copied into
+ 			 * %rdi if skb_copy_bits() call is necessary.
+ 			 * sk_load_* helpers also use %r10 and %r9d.
+ 			 * See bpf_jit.S
+ 			 */
+ 			if (seen_ax_reg)
+ 				/* r10 = skb->data, mov %r10, off32(%rbx) */
+ 				EMIT3_off32(0x4c, 0x8b, 0x93,
+ 					    offsetof(struct sk_buff, data));
+ 			EMIT1_off32(0xE8, jmp_offset); /* call */
+ 			break;
+ 
+ 		case BPF_LD | BPF_IND | BPF_H:
+ 			func = sk_load_half;
+ 			goto common_load;
+ 		case BPF_LD | BPF_ABS | BPF_H:
+ 			func = CHOOSE_LOAD_FUNC(imm32, sk_load_half);
+ 			goto common_load;
+ 		case BPF_LD | BPF_IND | BPF_B:
+ 			func = sk_load_byte;
+ 			goto common_load;
+ 		case BPF_LD | BPF_ABS | BPF_B:
+ 			func = CHOOSE_LOAD_FUNC(imm32, sk_load_byte);
+ 			goto common_load;
+ 
+ 		case BPF_JMP | BPF_EXIT:
+ 			if (seen_exit) {
+ 				jmp_offset = ctx->cleanup_addr - addrs[i];
+ 				goto emit_jmp;
+ 			}
+ 			seen_exit = true;
+ 			/* update cleanup_addr */
+ 			ctx->cleanup_addr = proglen;
+ 			/* mov rbx, qword ptr [rbp-X] */
+ 			EMIT3_off32(0x48, 0x8B, 0x9D, -STACKSIZE);
+ 			/* mov r13, qword ptr [rbp-X] */
+ 			EMIT3_off32(0x4C, 0x8B, 0xAD, -STACKSIZE + 8);
+ 			/* mov r14, qword ptr [rbp-X] */
+ 			EMIT3_off32(0x4C, 0x8B, 0xB5, -STACKSIZE + 16);
+ 			/* mov r15, qword ptr [rbp-X] */
+ 			EMIT3_off32(0x4C, 0x8B, 0xBD, -STACKSIZE + 24);
+ 
+ 			EMIT1(0xC9); /* leave */
+ 			EMIT1(0xC3); /* ret */
+ 			break;
+ 
+ 		default:
+ 			/* By design x64 JIT should support all BPF instructions
+ 			 * This error will be seen if new instruction was added
+ 			 * to interpreter, but not to JIT
+ 			 * or if there is junk in bpf_prog
+ 			 */
+ 			pr_err("bpf_jit: unknown opcode %02x\n", insn->code);
+ 			return -EINVAL;
+ 		}
+ 
+ 		ilen = prog - temp;
+ 		if (ilen > BPF_MAX_INSN_SIZE) {
+ 			pr_err("bpf_jit: fatal insn size error\n");
+ 			return -EFAULT;
+ 		}
+ 
+ 		if (image) {
+ 			if (unlikely(proglen + ilen > oldproglen)) {
+ 				pr_err("bpf_jit: fatal error\n");
+ 				return -EFAULT;
+ 			}
+ 			memcpy(image + proglen, temp, ilen);
+ 		}
+ 		proglen += ilen;
+ 		addrs[i] = proglen;
+ 		prog = temp;
++>>>>>>> 71189fa9b092 (bpf: free up BPF_JMP | BPF_CALL | BPF_X opcode)
  	}
 -	return proglen;
 +	pr_err_once("Please fix pkt_type_offset(), as pkt_type couldn't be found\n");
 +	return -1;
  }
  
 -struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog)
 +void bpf_jit_compile(struct sk_filter *fp)
  {
 -	struct bpf_binary_header *header = NULL;
 -	struct bpf_prog *tmp, *orig_prog = prog;
 -	int proglen, oldproglen = 0;
 -	struct jit_context ctx = {};
 -	bool tmp_blinded = false;
 +	u8 temp[64];
 +	u8 *prog;
 +	unsigned int proglen, oldproglen = 0;
 +	int ilen, i;
 +	int t_offset, f_offset;
 +	u8 t_op, f_op, seen = 0, pass;
  	u8 *image = NULL;
 -	int *addrs;
 -	int pass;
 -	int i;
 +	u8 *func;
 +	int pc_ret0 = -1; /* bpf index of first RET #0 instruction (if any) */
 +	unsigned int cleanup_addr; /* epilogue code offset */
 +	unsigned int *addrs;
 +	const struct sock_filter *filter = fp->insns;
 +	int flen = fp->len;
  
  	if (!bpf_jit_enable)
 -		return orig_prog;
 +		return;
  
 -	tmp = bpf_jit_blind_constants(prog);
 -	/* If blinding was requested and we failed during blinding,
 -	 * we must fall back to the interpreter.
 -	 */
 -	if (IS_ERR(tmp))
 -		return orig_prog;
 -	if (tmp != prog) {
 -		tmp_blinded = true;
 -		prog = tmp;
 -	}
 -
 -	addrs = kmalloc(prog->len * sizeof(*addrs), GFP_KERNEL);
 -	if (!addrs) {
 -		prog = orig_prog;
 -		goto out;
 -	}
 +	addrs = kmalloc(flen * sizeof(*addrs), GFP_KERNEL);
 +	if (addrs == NULL)
 +		return;
  
  	/* Before first pass, make a rough estimation of addrs[]
  	 * each bpf instruction is translated to less than 64 bytes
diff --cc include/linux/filter.h
index d322ed880333,a20ba40fcb73..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -23,21 -28,435 +23,407 @@@ struct compat_sock_fprog 
  
  struct sk_buff;
  struct sock;
 -struct seccomp_data;
  struct bpf_prog_aux;
  
++<<<<<<< HEAD
 +struct bpf_prog
 +{
 +	struct bpf_prog_aux	*aux;	/* Auxiliary fields */
++=======
+ /* ArgX, context and stack frame pointer register positions. Note,
+  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
+  * calls in BPF_CALL instruction.
+  */
+ #define BPF_REG_ARG1	BPF_REG_1
+ #define BPF_REG_ARG2	BPF_REG_2
+ #define BPF_REG_ARG3	BPF_REG_3
+ #define BPF_REG_ARG4	BPF_REG_4
+ #define BPF_REG_ARG5	BPF_REG_5
+ #define BPF_REG_CTX	BPF_REG_6
+ #define BPF_REG_FP	BPF_REG_10
+ 
+ /* Additional register mappings for converted user programs. */
+ #define BPF_REG_A	BPF_REG_0
+ #define BPF_REG_X	BPF_REG_7
+ #define BPF_REG_TMP	BPF_REG_8
+ 
+ /* Kernel hidden auxiliary/helper register for hardening step.
+  * Only used by eBPF JITs. It's nothing more than a temporary
+  * register that JITs use internally, only that here it's part
+  * of eBPF instructions that have been rewritten for blinding
+  * constants. See JIT pre-step in bpf_jit_blind_constants().
+  */
+ #define BPF_REG_AX		MAX_BPF_REG
+ #define MAX_BPF_JIT_REG		(MAX_BPF_REG + 1)
+ 
+ /* unused opcode to mark special call to bpf_tail_call() helper */
+ #define BPF_TAIL_CALL	0xf0
+ 
+ /* As per nm, we expose JITed images as text (code) section for
+  * kallsyms. That way, tools like perf can find it to match
+  * addresses.
+  */
+ #define BPF_SYM_ELF_TYPE	't'
+ 
+ /* BPF program can access up to 512 bytes of stack space. */
+ #define MAX_BPF_STACK	512
+ 
+ #define BPF_TAG_SIZE	8
+ 
+ /* Helper macros for filter block array initializers. */
+ 
+ /* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
+ 
+ #define BPF_ALU64_REG(OP, DST, SRC)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ #define BPF_ALU32_REG(OP, DST, SRC)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ /* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
+ 
+ #define BPF_ALU64_IMM(OP, DST, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ #define BPF_ALU32_IMM(OP, DST, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
+ 
+ #define BPF_ENDIAN(TYPE, DST, LEN)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = LEN })
+ 
+ /* Short form of mov, dst_reg = src_reg */
+ 
+ #define BPF_MOV64_REG(DST, SRC)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ #define BPF_MOV32_REG(DST, SRC)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ /* Short form of mov, dst_reg = imm32 */
+ 
+ #define BPF_MOV64_IMM(DST, IMM)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ #define BPF_MOV32_IMM(DST, IMM)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
+ #define BPF_LD_IMM64(DST, IMM)					\
+ 	BPF_LD_IMM64_RAW(DST, 0, IMM)
+ 
+ #define BPF_LD_IMM64_RAW(DST, SRC, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_LD | BPF_DW | BPF_IMM,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = (__u32) (IMM) }),			\
+ 	((struct bpf_insn) {					\
+ 		.code  = 0, /* zero is reserved opcode */	\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = ((__u64) (IMM)) >> 32 })
+ 
+ /* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */
+ #define BPF_LD_MAP_FD(DST, MAP_FD)				\
+ 	BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)
+ 
+ /* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
+ 
+ #define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ #define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
+ 
+ #define BPF_LD_ABS(SIZE, IMM)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
+ 
+ #define BPF_LD_IND(SIZE, SRC, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
+ 		.dst_reg = 0,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Memory load, dst_reg = *(uint *) (src_reg + off16) */
+ 
+ #define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Memory store, *(uint *) (dst_reg + off16) = src_reg */
+ 
+ #define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
+ 
+ #define BPF_STX_XADD(SIZE, DST, SRC, OFF)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_XADD,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Memory store, *(uint *) (dst_reg + off16) = imm32 */
+ 
+ #define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = OFF,					\
+ 		.imm   = IMM })
+ 
+ /* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
+ 
+ #define BPF_JMP_REG(OP, DST, SRC, OFF)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
+ 
+ #define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = OFF,					\
+ 		.imm   = IMM })
+ 
+ /* Unconditional jumps, goto pc + off16 */
+ 
+ #define BPF_JMP_A(OFF)						\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_JA,			\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Function call */
+ 
+ #define BPF_EMIT_CALL(FUNC)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_CALL,			\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = ((FUNC) - __bpf_call_base) })
+ 
+ /* Raw code statement block */
+ 
+ #define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = CODE,					\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = IMM })
+ 
+ /* Program exit */
+ 
+ #define BPF_EXIT_INSN()						\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_EXIT,			\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ /* Internal classic blocks for direct assignment */
+ 
+ #define __BPF_STMT(CODE, K)					\
+ 	((struct sock_filter) BPF_STMT(CODE, K))
+ 
+ #define __BPF_JUMP(CODE, K, JT, JF)				\
+ 	((struct sock_filter) BPF_JUMP(CODE, K, JT, JF))
+ 
+ #define bytes_to_bpf_size(bytes)				\
+ ({								\
+ 	int bpf_size = -EINVAL;					\
+ 								\
+ 	if (bytes == sizeof(u8))				\
+ 		bpf_size = BPF_B;				\
+ 	else if (bytes == sizeof(u16))				\
+ 		bpf_size = BPF_H;				\
+ 	else if (bytes == sizeof(u32))				\
+ 		bpf_size = BPF_W;				\
+ 	else if (bytes == sizeof(u64))				\
+ 		bpf_size = BPF_DW;				\
+ 								\
+ 	bpf_size;						\
+ })
+ 
+ #define BPF_SIZEOF(type)					\
+ 	({							\
+ 		const int __size = bytes_to_bpf_size(sizeof(type)); \
+ 		BUILD_BUG_ON(__size < 0);			\
+ 		__size;						\
+ 	})
+ 
+ #define BPF_FIELD_SIZEOF(type, field)				\
+ 	({							\
+ 		const int __size = bytes_to_bpf_size(FIELD_SIZEOF(type, field)); \
+ 		BUILD_BUG_ON(__size < 0);			\
+ 		__size;						\
+ 	})
+ 
+ #define __BPF_MAP_0(m, v, ...) v
+ #define __BPF_MAP_1(m, v, t, a, ...) m(t, a)
+ #define __BPF_MAP_2(m, v, t, a, ...) m(t, a), __BPF_MAP_1(m, v, __VA_ARGS__)
+ #define __BPF_MAP_3(m, v, t, a, ...) m(t, a), __BPF_MAP_2(m, v, __VA_ARGS__)
+ #define __BPF_MAP_4(m, v, t, a, ...) m(t, a), __BPF_MAP_3(m, v, __VA_ARGS__)
+ #define __BPF_MAP_5(m, v, t, a, ...) m(t, a), __BPF_MAP_4(m, v, __VA_ARGS__)
+ 
+ #define __BPF_REG_0(...) __BPF_PAD(5)
+ #define __BPF_REG_1(...) __BPF_MAP(1, __VA_ARGS__), __BPF_PAD(4)
+ #define __BPF_REG_2(...) __BPF_MAP(2, __VA_ARGS__), __BPF_PAD(3)
+ #define __BPF_REG_3(...) __BPF_MAP(3, __VA_ARGS__), __BPF_PAD(2)
+ #define __BPF_REG_4(...) __BPF_MAP(4, __VA_ARGS__), __BPF_PAD(1)
+ #define __BPF_REG_5(...) __BPF_MAP(5, __VA_ARGS__)
+ 
+ #define __BPF_MAP(n, ...) __BPF_MAP_##n(__VA_ARGS__)
+ #define __BPF_REG(n, ...) __BPF_REG_##n(__VA_ARGS__)
+ 
+ #define __BPF_CAST(t, a)						       \
+ 	(__force t)							       \
+ 	(__force							       \
+ 	 typeof(__builtin_choose_expr(sizeof(t) == sizeof(unsigned long),      \
+ 				      (unsigned long)0, (t)0))) a
+ #define __BPF_V void
+ #define __BPF_N
+ 
+ #define __BPF_DECL_ARGS(t, a) t   a
+ #define __BPF_DECL_REGS(t, a) u64 a
+ 
+ #define __BPF_PAD(n)							       \
+ 	__BPF_MAP(n, __BPF_DECL_ARGS, __BPF_N, u64, __ur_1, u64, __ur_2,       \
+ 		  u64, __ur_3, u64, __ur_4, u64, __ur_5)
+ 
+ #define BPF_CALL_x(x, name, ...)					       \
+ 	static __always_inline						       \
+ 	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__));   \
+ 	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__));	       \
+ 	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__))	       \
+ 	{								       \
+ 		return ____##name(__BPF_MAP(x,__BPF_CAST,__BPF_N,__VA_ARGS__));\
+ 	}								       \
+ 	static __always_inline						       \
+ 	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__))
+ 
+ #define BPF_CALL_0(name, ...)	BPF_CALL_x(0, name, __VA_ARGS__)
+ #define BPF_CALL_1(name, ...)	BPF_CALL_x(1, name, __VA_ARGS__)
+ #define BPF_CALL_2(name, ...)	BPF_CALL_x(2, name, __VA_ARGS__)
+ #define BPF_CALL_3(name, ...)	BPF_CALL_x(3, name, __VA_ARGS__)
+ #define BPF_CALL_4(name, ...)	BPF_CALL_x(4, name, __VA_ARGS__)
+ #define BPF_CALL_5(name, ...)	BPF_CALL_x(5, name, __VA_ARGS__)
+ 
+ #ifdef CONFIG_COMPAT
+ /* A struct sock_filter is architecture independent. */
+ struct compat_sock_fprog {
+ 	u16		len;
+ 	compat_uptr_t	filter;	/* struct sock_filter * */
+ };
+ #endif
+ 
+ struct sock_fprog_kern {
+ 	u16			len;
+ 	struct sock_filter	*filter;
++>>>>>>> 71189fa9b092 (bpf: free up BPF_JMP | BPF_CALL | BPF_X opcode)
  };
  
 -struct bpf_binary_header {
 -	unsigned int pages;
 -	u8 image[];
 -};
 -
 -struct bpf_prog {
 -	u16			pages;		/* Number of allocated pages */
 -	kmemcheck_bitfield_begin(meta);
 -	u16			jited:1,	/* Is our filter JIT'ed? */
 -				locked:1,	/* Program image locked? */
 -				gpl_compatible:1, /* Is filter GPL compatible? */
 -				cb_access:1,	/* Is control block accessed? */
 -				dst_needed:1;	/* Do we need dst entry? */
 -	kmemcheck_bitfield_end(meta);
 -	enum bpf_prog_type	type;		/* Type of BPF program */
 -	u32			len;		/* Number of filter blocks */
 -	u8			tag[BPF_TAG_SIZE];
 -	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
 -	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 -	unsigned int		(*bpf_func)(const void *ctx,
 -					    const struct bpf_insn *insn);
 -	/* Instructions for interpreter */
 -	union {
 -		struct sock_filter	insns[0];
 -		struct bpf_insn		insnsi[0];
 -	};
 -};
 -
 -struct sk_filter {
 -	refcount_t	refcnt;
 -	struct rcu_head	rcu;
 -	struct bpf_prog	*prog;
 -};
 -
 -#define BPF_PROG_RUN(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 -
 -#define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 -
 -struct bpf_skb_data_end {
 -	struct qdisc_skb_cb qdisc_cb;
 -	void *data_end;
 +struct sk_filter
 +{
 +	atomic_t		refcnt;
 +	unsigned int         	len;	/* Number of filter blocks */
 +	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 +					    const struct sock_filter *filter);
 +	struct rcu_head		rcu;
 +	struct sock_filter     	insns[0];
  };
  
  struct xdp_buff {
* Unmerged path arch/arm64/net/bpf_jit_comp.c
* Unmerged path arch/powerpc/net/bpf_jit_comp64.c
* Unmerged path arch/sparc/net/bpf_jit_comp_64.c
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/verifier.c
* Unmerged path arch/arm64/net/bpf_jit_comp.c
* Unmerged path arch/powerpc/net/bpf_jit_comp64.c
* Unmerged path arch/s390/net/bpf_jit_comp.c
* Unmerged path arch/sparc/net/bpf_jit_comp_64.c
* Unmerged path arch/x86/net/bpf_jit_comp.c
* Unmerged path include/linux/filter.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/verifier.c
