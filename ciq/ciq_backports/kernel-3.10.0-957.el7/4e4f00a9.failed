x86, dax, libnvdimm: remove wb_cache_pmem() indirection

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [fs] dax, libnvdimm: remove wb_cache_pmem() indirection (Jeff Moyer) [1471712]
Rebuild_FUZZ: 95.24%
commit-author Dan Williams <dan.j.williams@intel.com>
commit 4e4f00a9b51a1c52ebdd728a1caeb3b9fe48c39d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/4e4f00a9.failed

With all handling of the CONFIG_ARCH_HAS_PMEM_API case being moved to
libnvdimm and the pmem driver directly we do not need to provide global
wrappers and fallbacks in the CONFIG_ARCH_HAS_PMEM_API=n case. The pmem
driver will simply not link to arch_wb_cache_pmem() in that case.  Same
as before, pmem flushing is only defined for x86_64, via
clean_cache_range(), but it is straightforward to add other archs in the
future.

arch_wb_cache_pmem() is an exported function since the pmem module needs
to find it, but it is privately declared in drivers/nvdimm/pmem.h because
there are no consumers outside of the pmem driver.

	Cc: <x86@kernel.org>
	Cc: Jan Kara <jack@suse.cz>
	Cc: Jeff Moyer <jmoyer@redhat.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Oliver O'Halloran <oohall@gmail.com>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Suggested-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 4e4f00a9b51a1c52ebdd728a1caeb3b9fe48c39d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/pmem.h
diff --cc arch/x86/include/asm/pmem.h
index ab4983df7bff,4759a179aa52..000000000000
--- a/arch/x86/include/asm/pmem.h
+++ b/arch/x86/include/asm/pmem.h
@@@ -44,26 -44,6 +44,29 @@@ static inline void arch_memcpy_to_pmem(
  		BUG();
  }
  
++<<<<<<< HEAD
 +/**
 + * arch_wb_cache_pmem - write back a cache range with CLWB
 + * @vaddr:	virtual start address
 + * @size:	number of bytes to write back
 + *
 + * Write back a cache range using the CLWB (cache line write back)
 + * instruction.
 + */
 +static inline void arch_wb_cache_pmem(void *addr, size_t size)
 +{
 +	u16 x86_clflush_size = boot_cpu_data.x86_clflush_size;
 +	unsigned long clflush_mask = x86_clflush_size - 1;
 +	void *vend = addr + size;
 +	void *p;
 +
 +	for (p = (void *)((unsigned long)addr & ~clflush_mask);
 +	     p < vend; p += x86_clflush_size)
 +		clwb(p);
 +}
 +
++=======
++>>>>>>> 4e4f00a9b51a (x86, dax, libnvdimm: remove wb_cache_pmem() indirection)
  static inline void arch_invalidate_pmem(void *addr, size_t size)
  {
  	clflush_cache_range(addr, size);
* Unmerged path arch/x86/include/asm/pmem.h
diff --git a/arch/x86/lib/usercopy_64.c b/arch/x86/lib/usercopy_64.c
index 8ff245e9a964..1c147d30aade 100644
--- a/arch/x86/lib/usercopy_64.c
+++ b/arch/x86/lib/usercopy_64.c
@@ -111,6 +111,12 @@ static void clean_cache_range(void *addr, size_t size)
 		clwb(p);
 }
 
+void arch_wb_cache_pmem(void *addr, size_t size)
+{
+	clean_cache_range(addr, size);
+}
+EXPORT_SYMBOL_GPL(arch_wb_cache_pmem);
+
 long __copy_user_flushcache(void *dst, const void __user *src, unsigned size)
 {
 	unsigned long flushed, dest = (unsigned long) dst;
diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c
index 49eeb1950ba6..24b75e62b378 100644
--- a/drivers/nvdimm/pmem.c
+++ b/drivers/nvdimm/pmem.c
@@ -233,7 +233,7 @@ static long pmem_dax_direct_access(struct dax_device *dax_dev,
 static void pmem_dax_flush(struct dax_device *dax_dev, pgoff_t pgoff,
 		void *addr, size_t size)
 {
-	wb_cache_pmem(addr, size);
+	arch_wb_cache_pmem(addr, size);
 }
 
 static const struct dax_operations pmem_dax_ops = {
diff --git a/drivers/nvdimm/pmem.h b/drivers/nvdimm/pmem.h
index c5917f040fa7..7d79ae46e307 100644
--- a/drivers/nvdimm/pmem.h
+++ b/drivers/nvdimm/pmem.h
@@ -5,6 +5,14 @@
 #include <linux/pfn_t.h>
 #include <linux/fs.h>
 
+#ifdef CONFIG_ARCH_HAS_PMEM_API
+void arch_wb_cache_pmem(void *addr, size_t size);
+#else
+static inline void arch_wb_cache_pmem(void *addr, size_t size)
+{
+}
+#endif
+
 /* this definition is in it's own header for tools/testing/nvdimm to consume */
 struct pmem_device {
 	/* One contiguous memory region per device */
diff --git a/include/linux/pmem.h b/include/linux/pmem.h
index e34add5f758c..79266c7f8e2d 100644
--- a/include/linux/pmem.h
+++ b/include/linux/pmem.h
@@ -32,11 +32,6 @@ static inline void arch_memcpy_to_pmem(void *dst, const void *src,
 	BUG();
 }
 
-static inline void arch_wb_cache_pmem(void *addr, size_t size)
-{
-	BUG();
-}
-
 static inline void arch_invalidate_pmem(void *addr, size_t size)
 {
 	BUG();
@@ -102,18 +97,4 @@ static inline void invalidate_pmem(void *addr, size_t size)
 	if (arch_has_pmem_api())
 		arch_invalidate_pmem(addr, size);
 }
-
-/**
- * wb_cache_pmem - write back processor cache for PMEM memory range
- * @addr:	virtual start address
- * @size:	number of bytes to write back
- *
- * Write back the processor cache range starting at 'addr' for 'size' bytes.
- * See blkdev_issue_flush() note for memcpy_to_pmem().
- */
-static inline void wb_cache_pmem(void *addr, size_t size)
-{
-	if (arch_has_pmem_api())
-		arch_wb_cache_pmem(addr, size);
-}
 #endif /* __PMEM_H__ */
