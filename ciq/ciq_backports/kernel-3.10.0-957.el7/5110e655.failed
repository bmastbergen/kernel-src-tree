crypto: chelsio -Split Hash requests for large scatter gather list

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [crypto] chelsio -Split Hash requests for large scatter gather list (Arjun Vynipadath) [1523191]
Rebuild_FUZZ: 93.55%
commit-author Harsh Jain <harsh@chelsio.com>
commit 5110e65536f35e854f5d520b913505dbdbe22787
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/5110e655.failed

Send multiple WRs to H/W when No. of entries received in scatter list
cannot be sent in single request.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 5110e65536f35e854f5d520b913505dbdbe22787)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/chelsio/chcr_algo.h
#	drivers/crypto/chelsio/chcr_crypto.h
diff --cc drivers/crypto/chelsio/chcr_algo.c
index fb08cbb8fadb,4617c7acf4da..000000000000
mode 100755,100644..100755
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -101,22 -128,106 +101,99 @@@ static inline struct uld_ctx *ULD_CTX(s
  
  static inline int is_ofld_imm(const struct sk_buff *skb)
  {
 -	return (skb->len <= SGE_MAX_WR_LEN);
 +	return (skb->len <= CRYPTO_MAX_IMM_TX_PKT_LEN);
  }
  
++<<<<<<< HEAD
 +/*
 + *	sgl_len - calculates the size of an SGL of the given capacity
 + *	@n: the number of SGL entries
 + *	Calculates the number of flits needed for a scatter/gather list that
 + *	can hold the given number of entries.
 + */
 +static inline unsigned int sgl_len(unsigned int n)
++=======
+ static inline void chcr_init_hctx_per_wr(struct chcr_ahash_req_ctx *reqctx)
+ {
+ 	memset(&reqctx->hctx_wr, 0, sizeof(struct chcr_hctx_per_wr));
+ }
+ 
+ static int sg_nents_xlen(struct scatterlist *sg, unsigned int reqlen,
+ 			 unsigned int entlen,
+ 			 unsigned int skip)
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  {
 -	int nents = 0;
 -	unsigned int less;
 -	unsigned int skip_len = 0;
 -
 -	while (sg && skip) {
 -		if (sg_dma_len(sg) <= skip) {
 -			skip -= sg_dma_len(sg);
 -			skip_len = 0;
 -			sg = sg_next(sg);
 -		} else {
 -			skip_len = skip;
 -			skip = 0;
 -		}
 -	}
 -
 -	while (sg && reqlen) {
 -		less = min(reqlen, sg_dma_len(sg) - skip_len);
 -		nents += DIV_ROUND_UP(less, entlen);
 -		reqlen -= less;
 -		skip_len = 0;
 -		sg = sg_next(sg);
 -	}
 -	return nents;
 +	n--;
 +	return (3 * n) / 2 + (n & 1) + 2;
  }
  
++<<<<<<< HEAD
 +static void chcr_verify_tag(struct aead_request *req, u8 *input, int *err)
++=======
+ static inline void chcr_handle_ahash_resp(struct ahash_request *req,
+ 					  unsigned char *input,
+ 					  int err)
+ {
+ 	struct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);
+ 	struct chcr_hctx_per_wr *hctx_wr = &reqctx->hctx_wr;
+ 	int digestsize, updated_digestsize;
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct uld_ctx *u_ctx = ULD_CTX(h_ctx(tfm));
+ 
+ 	if (input == NULL)
+ 		goto out;
+ 	digestsize = crypto_ahash_digestsize(crypto_ahash_reqtfm(req));
+ 	updated_digestsize = digestsize;
+ 	if (digestsize == SHA224_DIGEST_SIZE)
+ 		updated_digestsize = SHA256_DIGEST_SIZE;
+ 	else if (digestsize == SHA384_DIGEST_SIZE)
+ 		updated_digestsize = SHA512_DIGEST_SIZE;
+ 
+ 	if (hctx_wr->dma_addr) {
+ 		dma_unmap_single(&u_ctx->lldi.pdev->dev, hctx_wr->dma_addr,
+ 				 hctx_wr->dma_len, DMA_TO_DEVICE);
+ 		hctx_wr->dma_addr = 0;
+ 	}
+ 	if (hctx_wr->isfinal || ((hctx_wr->processed + reqctx->reqlen) ==
+ 				 req->nbytes)) {
+ 		if (hctx_wr->result == 1) {
+ 			hctx_wr->result = 0;
+ 			memcpy(req->result, input + sizeof(struct cpl_fw6_pld),
+ 			       digestsize);
+ 		} else {
+ 			memcpy(reqctx->partial_hash,
+ 			       input + sizeof(struct cpl_fw6_pld),
+ 			       updated_digestsize);
+ 
+ 		}
+ 		goto unmap;
+ 	}
+ 	memcpy(reqctx->partial_hash, input + sizeof(struct cpl_fw6_pld),
+ 	       updated_digestsize);
+ 
+ 	err = chcr_ahash_continue(req);
+ 	if (err)
+ 		goto unmap;
+ 	return;
+ unmap:
+ 	if (hctx_wr->is_sg_map)
+ 		chcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);
+ 
+ 
+ out:
+ 	req->base.complete(&req->base, err);
+ }
+ 
+ static inline int get_aead_subtype(struct crypto_aead *aead)
+ {
+ 	struct aead_alg *alg = crypto_aead_alg(aead);
+ 	struct chcr_alg_template *chcr_crypto_alg =
+ 		container_of(alg, struct chcr_alg_template, alg.aead);
+ 	return chcr_crypto_alg->type & CRYPTO_ALG_SUB_TYPE_MASK;
+ }
+ 
+ void chcr_verify_tag(struct aead_request *req, u8 *input, int *err)
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  {
  	u8 temp[SHA512_DIGEST_SIZE];
  	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
@@@ -396,47 -468,171 +473,98 @@@ static void write_phys_cpl(struct cpl_r
  		      CPL_RX_PHYS_DSGL_PCITPHNTENB_V(0) |
  		      CPL_RX_PHYS_DSGL_PCITPHNT_V(0) |
  		      CPL_RX_PHYS_DSGL_DCAID_V(0) |
 -		      CPL_RX_PHYS_DSGL_NOOFSGENTR_V(walk->nents));
 +		      CPL_RX_PHYS_DSGL_NOOFSGENTR_V(nents));
  	phys_cpl->rss_hdr_int.opcode = CPL_RX_PHYS_ADDR;
 -	phys_cpl->rss_hdr_int.qid = htons(qid);
 +	phys_cpl->rss_hdr_int.qid = htons(sg_param->qid);
  	phys_cpl->rss_hdr_int.hash_val = 0;
 -}
 -
 -static inline void dsgl_walk_add_page(struct dsgl_walk *walk,
 -					size_t size,
 -					dma_addr_t *addr)
 -{
 -	int j;
 -
 -	if (!size)
 -		return;
 -	j = walk->nents;
 -	walk->to->len[j % 8] = htons(size);
 -	walk->to->addr[j % 8] = cpu_to_be64(*addr);
 -	j++;
 -	if ((j % 8) == 0)
 -		walk->to++;
 -	walk->nents = j;
 -}
 -
 -static void  dsgl_walk_add_sg(struct dsgl_walk *walk,
 -			   struct scatterlist *sg,
 -			      unsigned int slen,
 -			      unsigned int skip)
 -{
 -	int skip_len = 0;
 -	unsigned int left_size = slen, len = 0;
 -	unsigned int j = walk->nents;
 -	int offset, ent_len;
 -
 -	if (!slen)
 -		return;
 -	while (sg && skip) {
 -		if (sg_dma_len(sg) <= skip) {
 -			skip -= sg_dma_len(sg);
 -			skip_len = 0;
 +	to = (struct phys_sge_pairs *)((unsigned char *)phys_cpl +
 +				       sizeof(struct cpl_rx_phys_dsgl));
 +	for (i = 0; nents && left_size; to++) {
 +		for (j = 0; j < 8 && nents && left_size; j++, nents--) {
 +			len = min(left_size, sg_dma_len(sg));
 +			to->len[j] = htons(len);
 +			to->addr[j] = cpu_to_be64(sg_dma_address(sg));
 +			left_size -= len;
  			sg = sg_next(sg);
 -		} else {
 -			skip_len = skip;
 -			skip = 0;
  		}
  	}
 -
 -	while (left_size && sg) {
 -		len = min_t(u32, left_size, sg_dma_len(sg) - skip_len);
 -		offset = 0;
 -		while (len) {
 -			ent_len =  min_t(u32, len, CHCR_DST_SG_SIZE);
 -			walk->to->len[j % 8] = htons(ent_len);
 -			walk->to->addr[j % 8] = cpu_to_be64(sg_dma_address(sg) +
 -						      offset + skip_len);
 -			offset += ent_len;
 -			len -= ent_len;
 -			j++;
 -			if ((j % 8) == 0)
 -				walk->to++;
 -		}
 -		walk->last_sg = sg;
 -		walk->last_sg_len = min_t(u32, left_size, sg_dma_len(sg) -
 -					  skip_len) + skip_len;
 -		left_size -= min_t(u32, left_size, sg_dma_len(sg) - skip_len);
 -		skip_len = 0;
 -		sg = sg_next(sg);
 -	}
 -	walk->nents = j;
 -}
 -
 -static inline void ulptx_walk_init(struct ulptx_walk *walk,
 -				   struct ulptx_sgl *ulp)
 -{
 -	walk->sgl = ulp;
 -	walk->nents = 0;
 -	walk->pair_idx = 0;
 -	walk->pair = ulp->sge;
 -	walk->last_sg = NULL;
 -	walk->last_sg_len = 0;
  }
  
 -static inline void ulptx_walk_end(struct ulptx_walk *walk)
 -{
 -	walk->sgl->cmd_nsge = htonl(ULPTX_CMD_V(ULP_TX_SC_DSGL) |
 -			      ULPTX_NSGE_V(walk->nents));
 -}
 -
 -
 -static inline void ulptx_walk_add_page(struct ulptx_walk *walk,
 -					size_t size,
 -					dma_addr_t *addr)
 +static inline int map_writesg_phys_cpl(struct device *dev,
 +					struct cpl_rx_phys_dsgl *phys_cpl,
 +					struct scatterlist *sg,
 +					struct phys_sge_parm *sg_param)
  {
 -	if (!size)
 -		return;
 +	if (!sg || !sg_param->nents)
 +		return -EINVAL;
  
 -	if (walk->nents == 0) {
 -		walk->sgl->len0 = cpu_to_be32(size);
 -		walk->sgl->addr0 = cpu_to_be64(*addr);
 -	} else {
 -		walk->pair->addr[walk->pair_idx] = cpu_to_be64(*addr);
 -		walk->pair->len[walk->pair_idx] = cpu_to_be32(size);
 -		walk->pair_idx = !walk->pair_idx;
 -		if (!walk->pair_idx)
 -			walk->pair++;
++<<<<<<< HEAD
 +	sg_param->nents = dma_map_sg(dev, sg, sg_param->nents, DMA_FROM_DEVICE);
 +	if (sg_param->nents == 0) {
 +		pr_err("CHCR : DMA mapping failed\n");
 +		return -EINVAL;
  	}
 -	walk->nents++;
 +	write_phys_cpl(phys_cpl, sg, sg_param);
 +	return 0;
  }
  
 -static void  ulptx_walk_add_sg(struct ulptx_walk *walk,
 -					struct scatterlist *sg,
 -			       unsigned int len,
 -			       unsigned int skip)
 +static inline int get_aead_subtype(struct crypto_aead *aead)
  {
 -	int small;
 -	int skip_len = 0;
 -	unsigned int sgmin;
 -
 +	struct crypto_tfm *tfm = crypto_aead_tfm(aead);
 +	struct crypto_alg *alg = tfm->__crt_alg;
 +	struct chcr_alg_template *chcr_crypto_alg =
 +		container_of(alg, struct chcr_alg_template, alg.crypto);
 +	return chcr_crypto_alg->type & CRYPTO_ALG_SUB_TYPE_MASK;
++=======
+ 	if (!len)
+ 		return;
+ 	while (sg && skip) {
+ 		if (sg_dma_len(sg) <= skip) {
+ 			skip -= sg_dma_len(sg);
+ 			skip_len = 0;
+ 			sg = sg_next(sg);
+ 		} else {
+ 			skip_len = skip;
+ 			skip = 0;
+ 		}
+ 	}
+ 	WARN(!sg, "SG should not be null here\n");
+ 	if (sg && (walk->nents == 0)) {
+ 		small = min_t(unsigned int, sg_dma_len(sg) - skip_len, len);
+ 		sgmin = min_t(unsigned int, small, CHCR_SRC_SG_SIZE);
+ 		walk->sgl->len0 = cpu_to_be32(sgmin);
+ 		walk->sgl->addr0 = cpu_to_be64(sg_dma_address(sg) + skip_len);
+ 		walk->nents++;
+ 		len -= sgmin;
+ 		walk->last_sg = sg;
+ 		walk->last_sg_len = sgmin + skip_len;
+ 		skip_len += sgmin;
+ 		if (sg_dma_len(sg) == skip_len) {
+ 			sg = sg_next(sg);
+ 			skip_len = 0;
+ 		}
+ 	}
+ 
+ 	while (sg && len) {
+ 		small = min(sg_dma_len(sg) - skip_len, len);
+ 		sgmin = min_t(unsigned int, small, CHCR_SRC_SG_SIZE);
+ 		walk->pair->len[walk->pair_idx] = cpu_to_be32(sgmin);
+ 		walk->pair->addr[walk->pair_idx] =
+ 			cpu_to_be64(sg_dma_address(sg) + skip_len);
+ 		walk->pair_idx = !walk->pair_idx;
+ 		walk->nents++;
+ 		if (!walk->pair_idx)
+ 			walk->pair++;
+ 		len -= sgmin;
+ 		skip_len += sgmin;
+ 		walk->last_sg = sg;
+ 		walk->last_sg_len = skip_len;
+ 		if (sg_dma_len(sg) == skip_len) {
+ 			sg = sg_next(sg);
+ 			skip_len = 0;
+ 		}
+ 	}
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  }
  
  static inline int get_cryptoalg_subtype(struct crypto_tfm *tfm)
@@@ -1020,33 -1203,35 +1177,47 @@@ static int chcr_handle_cipher_resp(stru
  		}
  
  	}
++<<<<<<< HEAD
 +	wrparam.srcsg = scatterwalk_ffwd(reqctx->srcffwd, req->src,
 +				       reqctx->processed);
 +	reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, reqctx->dstsg,
 +					 reqctx->processed);
 +	if (!wrparam.srcsg || !reqctx->dst) {
 +		pr_err("Input sg list length less that nbytes\n");
 +		err = -EINVAL;
 +		goto complete;
++=======
+ 	if (!reqctx->imm) {
+ 		bytes = chcr_sg_ent_in_wr(reqctx->srcsg, reqctx->dstsg, 1,
+ 					  CIP_SPACE_LEFT(ablkctx->enckey_len),
+ 					  reqctx->src_ofst, reqctx->dst_ofst);
+ 		if ((bytes + reqctx->processed) >= req->nbytes)
+ 			bytes  = req->nbytes - reqctx->processed;
+ 		else
+ 			bytes = rounddown(bytes, 16);
+ 	} else {
+ 		/*CTR mode counter overfloa*/
+ 		bytes  = req->nbytes - reqctx->processed;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	}
 -	dma_sync_single_for_cpu(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev,
 -				reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);
 +	bytes = chcr_sg_ent_in_wr(wrparam.srcsg, reqctx->dst, 1,
 +				 SPACE_LEFT(ablkctx->enckey_len),
 +				 &wrparam.snent, &reqctx->dst_nents);
 +	if ((bytes + reqctx->processed) >= req->nbytes)
 +		bytes  = req->nbytes - reqctx->processed;
 +	else
 +		bytes = ROUND_16(bytes);
  	err = chcr_update_cipher_iv(req, fw6_pld, reqctx->iv);
 -	dma_sync_single_for_device(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev,
 -				   reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);
  	if (err)
 -		goto unmap;
 +		goto complete;
  
  	if (unlikely(bytes == 0)) {
 -		chcr_cipher_dma_unmap(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev,
 -				      req);
  		err = chcr_cipher_fallback(ablkctx->sw_cipher,
  				     req->base.flags,
 -				     req->src,
 -				     req->dst,
 -				     req->nbytes,
 -				     req->info,
 +				     wrparam.srcsg,
 +				     reqctx->dst,
 +				     req->nbytes - reqctx->processed,
 +				     reqctx->iv,
  				     reqctx->op);
  		goto complete;
  	}
@@@ -1100,27 -1283,43 +1271,41 @@@ static int process_cipher(struct ablkci
  		       ablkctx->enckey_len, req->nbytes, ivsize);
  		goto error;
  	}
 -	chcr_cipher_dma_map(&ULD_CTX(c_ctx(tfm))->lldi.pdev->dev, req);
 -	if (req->nbytes < (SGE_MAX_WR_LEN - (sizeof(struct chcr_wr) +
 -					    AES_MIN_KEY_SIZE +
 -					    sizeof(struct cpl_rx_phys_dsgl) +
 -					/*Min dsgl size*/
 -					    32))) {
 -		/* Can be sent as Imm*/
 -		unsigned int dnents = 0, transhdr_len, phys_dsgl, kctx_len;
 -
 -		dnents = sg_nents_xlen(req->dst, req->nbytes,
 -				       CHCR_DST_SG_SIZE, 0);
 -		dnents += 1; // IV
 -		phys_dsgl = get_space_for_phys_dsgl(dnents);
 -		kctx_len = roundup(ablkctx->enckey_len, 16);
 -		transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, phys_dsgl);
 -		reqctx->imm = (transhdr_len + IV + req->nbytes) <=
 -			SGE_MAX_WR_LEN;
 -		bytes = IV + req->nbytes;
 -
 +	wrparam.srcsg = req->src;
 +	if (is_newsg(req->dst, &nents)) {
 +		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
 +		if (IS_ERR(reqctx->newdstsg))
 +			return PTR_ERR(reqctx->newdstsg);
 +		reqctx->dstsg = reqctx->newdstsg;
  	} else {
 -		reqctx->imm = 0;
 +		reqctx->dstsg = req->dst;
  	}
++<<<<<<< HEAD
 +	bytes = chcr_sg_ent_in_wr(wrparam.srcsg, reqctx->dstsg, MIN_CIPHER_SG,
 +				 SPACE_LEFT(ablkctx->enckey_len),
 +				 &wrparam.snent,
 +				 &reqctx->dst_nents);
 +	if ((bytes + reqctx->processed) >= req->nbytes)
 +		bytes  = req->nbytes - reqctx->processed;
 +	else
 +		bytes = ROUND_16(bytes);
 +	if (unlikely(bytes > req->nbytes))
++=======
+ 
+ 	if (!reqctx->imm) {
+ 		bytes = chcr_sg_ent_in_wr(req->src, req->dst,
+ 					  MIN_CIPHER_SG,
+ 					  CIP_SPACE_LEFT(ablkctx->enckey_len),
+ 					  0, 0);
+ 		if ((bytes + reqctx->processed) >= req->nbytes)
+ 			bytes  = req->nbytes - reqctx->processed;
+ 		else
+ 			bytes = rounddown(bytes, 16);
+ 	} else {
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  		bytes = req->nbytes;
 -	}
  	if (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==
 -	    CRYPTO_ALG_SUB_TYPE_CTR) {
 +				  CRYPTO_ALG_SUB_TYPE_CTR) {
  		bytes = adjust_ctr_overflow(req->info, bytes);
  	}
  	if (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==
@@@ -1358,38 -1560,35 +1543,58 @@@ static struct sk_buff *create_hash_wr(s
  {
  	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);
  	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
 -	struct hmac_ctx *hmacctx = HMAC_CTX(h_ctx(tfm));
 +	struct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
 +	struct hmac_ctx *hmacctx = HMAC_CTX(ctx);
  	struct sk_buff *skb = NULL;
 -	struct uld_ctx *u_ctx = ULD_CTX(h_ctx(tfm));
  	struct chcr_wr *chcr_req;
++<<<<<<< HEAD
 +	unsigned int frags = 0, transhdr_len, iopad_alignment = 0;
 +	unsigned int digestsize = crypto_ahash_digestsize(tfm);
 +	unsigned int kctx_len = 0;
 +	u8 hash_size_in_response = 0;
++=======
+ 	struct ulptx_sgl *ulptx;
+ 	unsigned int nents = 0, transhdr_len;
+ 	unsigned int temp = 0;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
  		GFP_ATOMIC;
 -	struct adapter *adap = padap(h_ctx(tfm)->dev);
 -	int error = 0;
 +	struct adapter *adap = padap(ctx->dev);
 +
++<<<<<<< HEAD
 +	iopad_alignment = KEYCTX_ALIGN_PAD(digestsize);
 +	kctx_len = param->alg_prm.result_size + iopad_alignment;
 +	if (param->opad_needed)
 +		kctx_len += param->alg_prm.result_size + iopad_alignment;
  
 +	if (req_ctx->result)
 +		hash_size_in_response = digestsize;
 +	else
 +		hash_size_in_response = param->alg_prm.result_size;
 +	transhdr_len = HASH_TRANSHDR_SIZE(kctx_len);
 +	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
++=======
+ 	transhdr_len = HASH_TRANSHDR_SIZE(param->kctx_len);
+ 	req_ctx->hctx_wr.imm = (transhdr_len + param->bfr_len +
+ 				param->sg_len) <= SGE_MAX_WR_LEN;
+ 	nents = sg_nents_xlen(req_ctx->hctx_wr.srcsg, param->sg_len,
+ 		      CHCR_SRC_SG_SIZE, req_ctx->hctx_wr.src_ofst);
+ 	nents += param->bfr_len ? 1 : 0;
+ 	transhdr_len += req_ctx->hctx_wr.imm ? roundup(param->bfr_len +
+ 				param->sg_len, 16) : (sgl_len(nents) * 8);
+ 	transhdr_len = roundup(transhdr_len, 16);
+ 
+ 	skb = alloc_skb(transhdr_len, flags);
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	if (!skb)
 -		return ERR_PTR(-ENOMEM);
 -	chcr_req = __skb_put_zero(skb, transhdr_len);
 +		return skb;
 +
 +	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
 +	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
 +	memset(chcr_req, 0, transhdr_len);
  
  	chcr_req->sec_cpl.op_ivinsrtofst =
 -		FILL_SEC_CPL_OP_IVINSR(h_ctx(tfm)->dev->rx_channel_id, 2, 0);
 +		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2, 0);
  	chcr_req->sec_cpl.pldlen = htonl(param->bfr_len + param->sg_len);
  
  	chcr_req->sec_cpl.aadstart_cipherstop_hi =
@@@ -1415,22 -1614,37 +1620,49 @@@
  	chcr_req->key_ctx.ctx_hdr = FILL_KEY_CTX_HDR(CHCR_KEYCTX_NO_KEY,
  					    param->alg_prm.mk_size, 0,
  					    param->opad_needed,
- 					    ((kctx_len +
+ 					    ((param->kctx_len +
  					     sizeof(chcr_req->key_ctx)) >> 4));
  	chcr_req->sec_cpl.scmd1 = cpu_to_be64((u64)param->scmd1);
++<<<<<<< HEAD
 +
 +	skb_set_transport_header(skb, transhdr_len);
 +	if (param->bfr_len != 0)
 +		write_buffer_to_skb(skb, &frags, req_ctx->reqbfr,
 +				    param->bfr_len);
 +	if (param->sg_len != 0)
 +		write_sg_to_skb(skb, &frags, req->src, param->sg_len);
 +	atomic_inc(&adap->chcr_stats.digest_rqst);
 +	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len,
 +		    hash_size_in_response, 0, DUMMY_BYTES, 0);
 +	req_ctx->skb = skb;
 +	skb_get(skb);
++=======
+ 	ulptx = (struct ulptx_sgl *)((u8 *)(chcr_req + 1) + param->kctx_len +
+ 				     DUMMY_BYTES);
+ 	if (param->bfr_len != 0) {
+ 		req_ctx->hctx_wr.dma_addr =
+ 			dma_map_single(&u_ctx->lldi.pdev->dev, req_ctx->reqbfr,
+ 				       param->bfr_len, DMA_TO_DEVICE);
+ 		if (dma_mapping_error(&u_ctx->lldi.pdev->dev,
+ 				       req_ctx->hctx_wr. dma_addr)) {
+ 			error = -ENOMEM;
+ 			goto err;
+ 		}
+ 		req_ctx->hctx_wr.dma_len = param->bfr_len;
+ 	} else {
+ 		req_ctx->hctx_wr.dma_addr = 0;
+ 	}
+ 	chcr_add_hash_src_ent(req, ulptx, param);
+ 	/* Request upto max wr size */
+ 	temp = param->kctx_len + DUMMY_BYTES + (req_ctx->hctx_wr.imm ?
+ 				(param->sg_len + param->bfr_len) : 0);
+ 	atomic_inc(&adap->chcr_stats.digest_rqst);
+ 	create_wreq(h_ctx(tfm), chcr_req, &req->base, req_ctx->hctx_wr.imm,
+ 		    param->hash_size, transhdr_len,
+ 		    temp,  0);
+ 	req_ctx->hctx_wr.skb = skb;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	return skb;
 -err:
 -	kfree_skb(skb);
 -	return  ERR_PTR(error);
  }
  
  static int chcr_ahash_update(struct ahash_request *req)
@@@ -1443,12 -1656,12 +1675,16 @@@
  	u8 remainder = 0, bs;
  	unsigned int nbytes = req->nbytes;
  	struct hash_wr_param params;
 -	int error;
  
  	bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));
++<<<<<<< HEAD
 +
 +	u_ctx = ULD_CTX(ctx);
++=======
+ 	u_ctx = ULD_CTX(h_ctx(rtfm));
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	if (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
 -					    h_ctx(rtfm)->tx_qidx))) {
 +					    ctx->tx_qidx))) {
  		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
  			return -EBUSY;
  	}
@@@ -1462,20 -1675,34 +1698,36 @@@
  		req_ctx->reqlen += nbytes;
  		return 0;
  	}
++<<<<<<< HEAD
 +
++=======
+ 	chcr_init_hctx_per_wr(req_ctx);
+ 	error = chcr_hash_dma_map(&u_ctx->lldi.pdev->dev, req);
+ 	if (error)
+ 		return -ENOMEM;
+ 	get_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));
+ 	params.kctx_len = roundup(params.alg_prm.result_size, 16);
+ 	params.sg_len = chcr_hash_ent_in_wr(req->src, !!req_ctx->reqlen,
+ 				     HASH_SPACE_LEFT(params.kctx_len), 0);
+ 	if (params.sg_len > req->nbytes)
+ 		params.sg_len = req->nbytes;
+ 	params.sg_len = rounddown(params.sg_len + req_ctx->reqlen, bs) -
+ 			req_ctx->reqlen;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	params.opad_needed = 0;
  	params.more = 1;
  	params.last = 0;
- 	params.sg_len = nbytes - req_ctx->reqlen;
  	params.bfr_len = req_ctx->reqlen;
  	params.scmd1 = 0;
- 	get_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));
- 	req_ctx->result = 0;
+ 	req_ctx->hctx_wr.srcsg = req->src;
+ 
+ 	params.hash_size = params.alg_prm.result_size;
  	req_ctx->data_len += params.sg_len + params.bfr_len;
  	skb = create_hash_wr(req, &params);
 -	if (IS_ERR(skb)) {
 -		error = PTR_ERR(skb);
 -		goto unmap;
 -	}
 +	if (IS_ERR(skb))
 +		return PTR_ERR(skb);
  
+ 	req_ctx->hctx_wr.processed += params.sg_len;
  	if (remainder) {
  		/* Swap buffers */
  		swap(req_ctx->reqbfr, req_ctx->skbfr);
@@@ -1511,7 -1740,8 +1763,12 @@@ static int chcr_ahash_final(struct ahas
  	struct uld_ctx *u_ctx = NULL;
  	u8 bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));
  
++<<<<<<< HEAD
 +	u_ctx = ULD_CTX(ctx);
++=======
+ 	chcr_init_hctx_per_wr(req_ctx);
+ 	u_ctx = ULD_CTX(h_ctx(rtfm));
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	if (is_hmac(crypto_ahash_tfm(rtfm)))
  		params.opad_needed = 1;
  	else
@@@ -1536,9 -1777,9 +1804,9 @@@
  	skb = create_hash_wr(req, &params);
  	if (IS_ERR(skb))
  		return PTR_ERR(skb);
- 
+ 	req_ctx->reqlen = 0;
  	skb->dev = u_ctx->lldi.ports[0];
 -	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
 +	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
  	chcr_send_wr(skb);
  	return -EINPROGRESS;
  }
@@@ -1578,18 -1911,16 +1938,24 @@@ static int chcr_ahash_finup(struct ahas
  		params.more = 1;
  		params.scmd1 = 0;
  		params.bfr_len = bs;
- 	} else {
- 		params.scmd1 = req_ctx->data_len;
- 		params.last = 1;
- 		params.more = 0;
  	}
++<<<<<<< HEAD
 +
 +	skb = create_hash_wr(req, &params);
 +	if (IS_ERR(skb))
 +		return PTR_ERR(skb);
 +
++=======
+ 	skb = create_hash_wr(req, &params);
+ 	if (IS_ERR(skb)) {
+ 		error = PTR_ERR(skb);
+ 		goto unmap;
+ 	}
+ 	req_ctx->reqlen = 0;
+ 	req_ctx->hctx_wr.processed += params.sg_len;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	skb->dev = u_ctx->lldi.ports[0];
 -	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
 +	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
  	chcr_send_wr(skb);
  
  	return -EINPROGRESS;
@@@ -1615,18 -1949,42 +1981,49 @@@ static int chcr_ahash_digest(struct aha
  			return -EBUSY;
  	}
  
++<<<<<<< HEAD
 +	if (is_hmac(crypto_ahash_tfm(rtfm)))
 +		params.opad_needed = 1;
 +	else
 +		params.opad_needed = 0;
++=======
+ 	chcr_init_hctx_per_wr(req_ctx);
+ 	error = chcr_hash_dma_map(&u_ctx->lldi.pdev->dev, req);
+ 	if (error)
+ 		return -ENOMEM;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  
- 	params.last = 0;
- 	params.more = 0;
- 	params.sg_len = req->nbytes;
- 	params.bfr_len = 0;
- 	params.scmd1 = 0;
  	get_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));
- 	req_ctx->result = 1;
+ 	params.kctx_len = roundup(params.alg_prm.result_size, 16);
+ 	if (is_hmac(crypto_ahash_tfm(rtfm))) {
+ 		params.kctx_len *= 2;
+ 		params.opad_needed = 1;
+ 	} else {
+ 		params.opad_needed = 0;
+ 	}
+ 	params.sg_len = chcr_hash_ent_in_wr(req->src, !!req_ctx->reqlen,
+ 				HASH_SPACE_LEFT(params.kctx_len), 0);
+ 	if (params.sg_len < req->nbytes) {
+ 		if (is_hmac(crypto_ahash_tfm(rtfm))) {
+ 			params.kctx_len /= 2;
+ 			params.opad_needed = 0;
+ 		}
+ 		params.last = 0;
+ 		params.more = 1;
+ 		params.scmd1 = 0;
+ 		params.sg_len = rounddown(params.sg_len, bs);
+ 		params.hash_size = params.alg_prm.result_size;
+ 	} else {
+ 		params.sg_len = req->nbytes;
+ 		params.hash_size = crypto_ahash_digestsize(rtfm);
+ 		params.last = 1;
+ 		params.more = 0;
+ 		params.scmd1 = req->nbytes + req_ctx->data_len;
+ 
+ 	}
+ 	params.bfr_len = 0;
+ 	req_ctx->hctx_wr.result = 1;
+ 	req_ctx->hctx_wr.srcsg = req->src;
  	req_ctx->data_len += params.bfr_len + params.sg_len;
  
  	if (req->nbytes == 0) {
@@@ -1636,13 -1994,18 +2033,21 @@@
  	}
  
  	skb = create_hash_wr(req, &params);
++<<<<<<< HEAD
 +	if (IS_ERR(skb))
 +		return PTR_ERR(skb);
 +
++=======
+ 	if (IS_ERR(skb)) {
+ 		error = PTR_ERR(skb);
+ 		goto unmap;
+ 	}
+ 	req_ctx->hctx_wr.processed += params.sg_len;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	skb->dev = u_ctx->lldi.ports[0];
 -	set_wr_txq(skb, CPL_PRIORITY_DATA, h_ctx(rtfm)->tx_qidx);
 +	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
  	chcr_send_wr(skb);
  	return -EINPROGRESS;
 -unmap:
 -	chcr_hash_dma_unmap(&u_ctx->lldi.pdev->dev, req);
 -	return error;
  }
  
  static int chcr_ahash_export(struct ahash_request *areq, void *out)
@@@ -1768,9 -2131,8 +2175,13 @@@ static int chcr_sha_init(struct ahash_r
  	req_ctx->reqlen = 0;
  	req_ctx->reqbfr = req_ctx->bfr1;
  	req_ctx->skbfr = req_ctx->bfr2;
++<<<<<<< HEAD
 +	req_ctx->skb = NULL;
 +	req_ctx->result = 0;
++=======
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  	copy_hash_init_values(req_ctx->partial_hash, digestsize);
+ 
  	return 0;
  }
  
@@@ -2078,6 -2395,292 +2489,295 @@@ err
  	return ERR_PTR(error);
  }
  
++<<<<<<< HEAD
++=======
+ int chcr_aead_dma_map(struct device *dev,
+ 		      struct aead_request *req,
+ 		      unsigned short op_type)
+ {
+ 	int error;
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int dst_size;
+ 
+ 	dst_size = req->assoclen + req->cryptlen + (op_type ?
+ 				-authsize : authsize);
+ 	if (!req->cryptlen || !dst_size)
+ 		return 0;
+ 	reqctx->iv_dma = dma_map_single(dev, reqctx->iv, IV,
+ 					DMA_BIDIRECTIONAL);
+ 	if (dma_mapping_error(dev, reqctx->iv_dma))
+ 		return -ENOMEM;
+ 
+ 	if (req->src == req->dst) {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 		if (!error)
+ 			goto err;
+ 	} else {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		if (!error)
+ 			goto err;
+ 		error = dma_map_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 		if (!error) {
+ 			dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ err:
+ 	dma_unmap_single(dev, reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);
+ 	return -ENOMEM;
+ }
+ 
+ void chcr_aead_dma_unmap(struct device *dev,
+ 			 struct aead_request *req,
+ 			 unsigned short op_type)
+ {
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int dst_size;
+ 
+ 	dst_size = req->assoclen + req->cryptlen + (op_type ?
+ 					-authsize : authsize);
+ 	if (!req->cryptlen || !dst_size)
+ 		return;
+ 
+ 	dma_unmap_single(dev, reqctx->iv_dma, IV,
+ 					DMA_BIDIRECTIONAL);
+ 	if (req->src == req->dst) {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 	} else {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		dma_unmap_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 	}
+ }
+ 
+ void chcr_add_aead_src_ent(struct aead_request *req,
+ 			   struct ulptx_sgl *ulptx,
+ 			   unsigned int assoclen,
+ 			   unsigned short op_type)
+ {
+ 	struct ulptx_walk ulp_walk;
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 
+ 	if (reqctx->imm) {
+ 		u8 *buf = (u8 *)ulptx;
+ 
+ 		if (reqctx->b0_dma) {
+ 			memcpy(buf, reqctx->scratch_pad, reqctx->b0_len);
+ 			buf += reqctx->b0_len;
+ 		}
+ 		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
+ 				   buf, assoclen, 0);
+ 		buf += assoclen;
+ 		memcpy(buf, reqctx->iv, IV);
+ 		buf += IV;
+ 		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
+ 				   buf, req->cryptlen, req->assoclen);
+ 	} else {
+ 		ulptx_walk_init(&ulp_walk, ulptx);
+ 		if (reqctx->b0_dma)
+ 			ulptx_walk_add_page(&ulp_walk, reqctx->b0_len,
+ 					    &reqctx->b0_dma);
+ 		ulptx_walk_add_sg(&ulp_walk, req->src, assoclen, 0);
+ 		ulptx_walk_add_page(&ulp_walk, IV, &reqctx->iv_dma);
+ 		ulptx_walk_add_sg(&ulp_walk, req->src, req->cryptlen,
+ 				  req->assoclen);
+ 		ulptx_walk_end(&ulp_walk);
+ 	}
+ }
+ 
+ void chcr_add_aead_dst_ent(struct aead_request *req,
+ 			   struct cpl_rx_phys_dsgl *phys_cpl,
+ 			   unsigned int assoclen,
+ 			   unsigned short op_type,
+ 			   unsigned short qid)
+ {
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct dsgl_walk dsgl_walk;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	u32 temp;
+ 
+ 	dsgl_walk_init(&dsgl_walk, phys_cpl);
+ 	if (reqctx->b0_dma)
+ 		dsgl_walk_add_page(&dsgl_walk, reqctx->b0_len, &reqctx->b0_dma);
+ 	dsgl_walk_add_sg(&dsgl_walk, req->dst, assoclen, 0);
+ 	dsgl_walk_add_page(&dsgl_walk, IV, &reqctx->iv_dma);
+ 	temp = req->cryptlen + (op_type ? -authsize : authsize);
+ 	dsgl_walk_add_sg(&dsgl_walk, req->dst, temp, req->assoclen);
+ 	dsgl_walk_end(&dsgl_walk, qid);
+ }
+ 
+ void chcr_add_cipher_src_ent(struct ablkcipher_request *req,
+ 			     struct ulptx_sgl *ulptx,
+ 			     struct  cipher_wr_param *wrparam)
+ {
+ 	struct ulptx_walk ulp_walk;
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 
+ 	if (reqctx->imm) {
+ 		u8 *buf = (u8 *)ulptx;
+ 
+ 		memcpy(buf, reqctx->iv, IV);
+ 		buf += IV;
+ 		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
+ 				   buf, wrparam->bytes, reqctx->processed);
+ 	} else {
+ 		ulptx_walk_init(&ulp_walk, ulptx);
+ 		ulptx_walk_add_page(&ulp_walk, IV, &reqctx->iv_dma);
+ 		ulptx_walk_add_sg(&ulp_walk, reqctx->srcsg, wrparam->bytes,
+ 				  reqctx->src_ofst);
+ 		reqctx->srcsg = ulp_walk.last_sg;
+ 		reqctx->src_ofst = ulp_walk.last_sg_len;
+ 		ulptx_walk_end(&ulp_walk);
+ 	}
+ }
+ 
+ void chcr_add_cipher_dst_ent(struct ablkcipher_request *req,
+ 			     struct cpl_rx_phys_dsgl *phys_cpl,
+ 			     struct  cipher_wr_param *wrparam,
+ 			     unsigned short qid)
+ {
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 	struct dsgl_walk dsgl_walk;
+ 
+ 	dsgl_walk_init(&dsgl_walk, phys_cpl);
+ 	dsgl_walk_add_page(&dsgl_walk, IV, &reqctx->iv_dma);
+ 	dsgl_walk_add_sg(&dsgl_walk, reqctx->dstsg, wrparam->bytes,
+ 			 reqctx->dst_ofst);
+ 	reqctx->dstsg = dsgl_walk.last_sg;
+ 	reqctx->dst_ofst = dsgl_walk.last_sg_len;
+ 
+ 	dsgl_walk_end(&dsgl_walk, qid);
+ }
+ 
+ void chcr_add_hash_src_ent(struct ahash_request *req,
+ 			   struct ulptx_sgl *ulptx,
+ 			   struct hash_wr_param *param)
+ {
+ 	struct ulptx_walk ulp_walk;
+ 	struct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);
+ 
+ 	if (reqctx->hctx_wr.imm) {
+ 		u8 *buf = (u8 *)ulptx;
+ 
+ 		if (param->bfr_len) {
+ 			memcpy(buf, reqctx->reqbfr, param->bfr_len);
+ 			buf += param->bfr_len;
+ 		}
+ 
+ 		sg_pcopy_to_buffer(reqctx->hctx_wr.srcsg,
+ 				   sg_nents(reqctx->hctx_wr.srcsg), buf,
+ 				   param->sg_len, 0);
+ 	} else {
+ 		ulptx_walk_init(&ulp_walk, ulptx);
+ 		if (param->bfr_len)
+ 			ulptx_walk_add_page(&ulp_walk, param->bfr_len,
+ 					    &reqctx->hctx_wr.dma_addr);
+ 		ulptx_walk_add_sg(&ulp_walk, reqctx->hctx_wr.srcsg,
+ 				  param->sg_len, reqctx->hctx_wr.src_ofst);
+ 		reqctx->hctx_wr.srcsg = ulp_walk.last_sg;
+ 		reqctx->hctx_wr.src_ofst = ulp_walk.last_sg_len;
+ 		ulptx_walk_end(&ulp_walk);
+ 	}
+ }
+ 
+ int chcr_hash_dma_map(struct device *dev,
+ 		      struct ahash_request *req)
+ {
+ 	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);
+ 	int error = 0;
+ 
+ 	if (!req->nbytes)
+ 		return 0;
+ 	error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 			   DMA_TO_DEVICE);
+ 	if (!error)
+ 		return -ENOMEM;
+ 	req_ctx->hctx_wr.is_sg_map = 1;
+ 	return 0;
+ }
+ 
+ void chcr_hash_dma_unmap(struct device *dev,
+ 			 struct ahash_request *req)
+ {
+ 	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);
+ 
+ 	if (!req->nbytes)
+ 		return;
+ 
+ 	dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 			   DMA_TO_DEVICE);
+ 	req_ctx->hctx_wr.is_sg_map = 0;
+ 
+ }
+ 
+ int chcr_cipher_dma_map(struct device *dev,
+ 			struct ablkcipher_request *req)
+ {
+ 	int error;
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 
+ 	reqctx->iv_dma = dma_map_single(dev, reqctx->iv, IV,
+ 					DMA_BIDIRECTIONAL);
+ 	if (dma_mapping_error(dev, reqctx->iv_dma))
+ 		return -ENOMEM;
+ 
+ 	if (req->src == req->dst) {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 		if (!error)
+ 			goto err;
+ 	} else {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		if (!error)
+ 			goto err;
+ 		error = dma_map_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 		if (!error) {
+ 			dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ err:
+ 	dma_unmap_single(dev, reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);
+ 	return -ENOMEM;
+ }
+ 
+ void chcr_cipher_dma_unmap(struct device *dev,
+ 			   struct ablkcipher_request *req)
+ {
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 
+ 	dma_unmap_single(dev, reqctx->iv_dma, IV,
+ 					DMA_BIDIRECTIONAL);
+ 	if (req->src == req->dst) {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 	} else {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		dma_unmap_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 	}
+ }
+ 
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  static int set_msg_len(u8 *block, unsigned int msglen, int csize)
  {
  	__be32 data;
diff --cc drivers/crypto/chelsio/chcr_algo.h
index 583008de51a3,6748c293ccd4..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.h
+++ b/drivers/crypto/chelsio/chcr_algo.h
@@@ -214,31 -214,18 +214,39 @@@
  					   calc_tx_flits_ofld(skb) * 8), 16)))
  
  #define FILL_CMD_MORE(immdatalen) htonl(ULPTX_CMD_V(ULP_TX_SC_IMM) |\
 -					ULP_TX_SC_MORE_V((immdatalen)))
 +					ULP_TX_SC_MORE_V((immdatalen) ? 0 : 1))
 +
  #define MAX_NK 8
 +#define CRYPTO_MAX_IMM_TX_PKT_LEN 256
 +#define MAX_WR_SIZE			512
 +#define ROUND_16(bytes)		((bytes) & 0xFFFFFFF0)
  #define MAX_DSGL_ENT			32
 +#define MAX_DIGEST_SKB_SGE	(MAX_SKB_FRAGS - 2)
  #define MIN_CIPHER_SG			1 /* IV */
 -#define MIN_AUTH_SG			1 /* IV */
 -#define MIN_GCM_SG			1 /* IV */
 +#define MIN_AUTH_SG			2 /*IV + AAD*/
 +#define MIN_GCM_SG			2 /* IV + AAD*/
  #define MIN_DIGEST_SG			1 /*Partial Buffer*/
++<<<<<<< HEAD
 +#define MIN_CCM_SG			3 /*IV+AAD+B0*/
 +#define SPACE_LEFT(len) \
 +	((MAX_WR_SIZE - WR_MIN_LEN - (len)))
 +
 +unsigned int sgl_ent_len[] = {0, 0, 16, 24, 40,
 +				48, 64, 72, 88,
 +				96, 112, 120, 136,
 +				144, 160, 168, 184,
 +				192};
 +unsigned int dsgl_ent_len[] = {0, 32, 32, 48, 48, 64, 64, 80, 80,
 +				112, 112, 128, 128, 144, 144, 160, 160,
 +				192, 192, 208, 208, 224, 224, 240, 240,
 +				272, 272, 288, 288, 304, 304, 320, 320};
++=======
+ #define MIN_CCM_SG			2 /*IV+B0*/
+ #define CIP_SPACE_LEFT(len) \
+ 	((SGE_MAX_WR_LEN - CIP_WR_MIN_LEN - (len)))
+ #define HASH_SPACE_LEFT(len) \
+ 	((SGE_MAX_WR_LEN - HASH_WR_MIN_LEN - (len)))
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  
  struct algo_param {
  	unsigned int auth_mode;
diff --cc drivers/crypto/chelsio/chcr_crypto.h
index ee637a7f2f30,71025ea9c3db..000000000000
--- a/drivers/crypto/chelsio/chcr_crypto.h
+++ b/drivers/crypto/chelsio/chcr_crypto.h
@@@ -222,18 -258,32 +222,45 @@@ struct chcr_context 
  	struct __crypto_ctx crypto_ctx[0];
  };
  
++<<<<<<< HEAD
 +struct chcr_ahash_req_ctx {
 +	u32 result;
 +	u8 bfr1[CHCR_HASH_MAX_BLOCK_SIZE_128];
 +	u8 bfr2[CHCR_HASH_MAX_BLOCK_SIZE_128];
 +	u8 *reqbfr;
 +	u8 *skbfr;
 +	u8 reqlen;
 +	/* DMA the partial hash in it */
 +	u8 partial_hash[CHCR_HASH_MAX_DIGEST_SIZE];
 +	u64 data_len;  /* Data len till time */
- 	/* SKB which is being sent to the hardware for processing */
++=======
+ struct chcr_hctx_per_wr {
+ 	struct scatterlist *srcsg;
  	struct sk_buff *skb;
+ 	dma_addr_t dma_addr;
+ 	u32 dma_len;
+ 	unsigned int src_ofst;
+ 	unsigned int processed;
+ 	u32 result;
+ 	u8 is_sg_map;
+ 	u8 imm;
+ 	/*Final callback called. Driver cannot rely on nbytes to decide
+ 	 * final call
+ 	 */
+ 	u8 isfinal;
+ };
+ 
+ struct chcr_ahash_req_ctx {
+ 	struct chcr_hctx_per_wr hctx_wr;
+ 	u8 *reqbfr;
+ 	u8 *skbfr;
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
+ 	/* SKB which is being sent to the hardware for processing */
+ 	u64 data_len;  /* Data len till time */
+ 	u8 reqlen;
+ 	u8 partial_hash[CHCR_HASH_MAX_DIGEST_SIZE];
+ 	u8 bfr1[CHCR_HASH_MAX_BLOCK_SIZE_128];
+ 	u8 bfr2[CHCR_HASH_MAX_BLOCK_SIZE_128];
  };
  
  struct chcr_blkcipher_req_ctx {
@@@ -283,15 -315,30 +310,44 @@@ typedef struct sk_buff *(*create_wr_t)(
  				       int size,
  				       unsigned short op_type);
  
++<<<<<<< HEAD
 +static int chcr_aead_op(struct aead_request *req_base,
 +			  unsigned short op_type,
 +			  int size,
 +			  create_wr_t create_wr_fn);
 +static inline int get_aead_subtype(struct crypto_aead *aead);
 +static int is_newsg(struct scatterlist *sgl, unsigned int *newents);
 +static struct scatterlist *alloc_new_sg(struct scatterlist *sgl,
 +					unsigned int nents);
 +static inline void free_new_sg(struct scatterlist *sgl);
 +static int chcr_handle_cipher_resp(struct ablkcipher_request *req,
 +				   unsigned char *input, int err);
++=======
+ void chcr_verify_tag(struct aead_request *req, u8 *input, int *err);
+ int chcr_aead_dma_map(struct device *dev, struct aead_request *req,
+ 		      unsigned short op_type);
+ void chcr_aead_dma_unmap(struct device *dev, struct aead_request *req,
+ 			 unsigned short op_type);
+ void chcr_add_aead_dst_ent(struct aead_request *req,
+ 			   struct cpl_rx_phys_dsgl *phys_cpl,
+ 			   unsigned int assoclen, unsigned short op_type,
+ 			   unsigned short qid);
+ void chcr_add_aead_src_ent(struct aead_request *req, struct ulptx_sgl *ulptx,
+ 			   unsigned int assoclen, unsigned short op_type);
+ void chcr_add_cipher_src_ent(struct ablkcipher_request *req,
+ 			     struct ulptx_sgl *ulptx,
+ 			     struct  cipher_wr_param *wrparam);
+ int chcr_cipher_dma_map(struct device *dev, struct ablkcipher_request *req);
+ void chcr_cipher_dma_unmap(struct device *dev, struct ablkcipher_request *req);
+ void chcr_add_cipher_dst_ent(struct ablkcipher_request *req,
+ 			     struct cpl_rx_phys_dsgl *phys_cpl,
+ 			     struct  cipher_wr_param *wrparam,
+ 			     unsigned short qid);
+ int sg_nents_len_skip(struct scatterlist *sg, u64 len, u64 skip);
+ void chcr_add_hash_src_ent(struct ahash_request *req, struct ulptx_sgl *ulptx,
+ 			   struct hash_wr_param *param);
+ int chcr_hash_dma_map(struct device *dev, struct ahash_request *req);
+ void chcr_hash_dma_unmap(struct device *dev, struct ahash_request *req);
+ static int chcr_ahash_continue(struct ahash_request *req);
++>>>>>>> 5110e65536f3 (crypto: chelsio -Split Hash requests for large scatter gather list)
  #endif /* __CHCR_CRYPTO_H__ */
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/chelsio/chcr_algo.h
diff --git a/drivers/crypto/chelsio/chcr_core.h b/drivers/crypto/chelsio/chcr_core.h
index c9a19b2a1e9f..06da7a792ab5 100644
--- a/drivers/crypto/chelsio/chcr_core.h
+++ b/drivers/crypto/chelsio/chcr_core.h
@@ -53,10 +53,14 @@
 #define MAC_ERROR_BIT		0
 #define CHK_MAC_ERR_BIT(x)	(((x) >> MAC_ERROR_BIT) & 1)
 #define MAX_SALT                4
-#define WR_MIN_LEN (sizeof(struct chcr_wr) + \
+#define CIP_WR_MIN_LEN (sizeof(struct chcr_wr) + \
 		    sizeof(struct cpl_rx_phys_dsgl) + \
 		    sizeof(struct ulptx_sgl))
 
+#define HASH_WR_MIN_LEN (sizeof(struct chcr_wr) + \
+			DUMMY_BYTES + \
+		    sizeof(struct ulptx_sgl))
+
 #define padap(dev) pci_get_drvdata(dev->u_ctx->lldi.pdev)
 
 struct uld_ctx;
* Unmerged path drivers/crypto/chelsio/chcr_crypto.h
