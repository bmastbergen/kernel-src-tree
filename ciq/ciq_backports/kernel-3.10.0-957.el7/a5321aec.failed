x86/microcode: Synchronize late microcode loading

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] microcode: Synchronize late microcode loading (Prarit Bhargava) [1568249]
Rebuild_FUZZ: 95.74%
commit-author Ashok Raj <ashok.raj@intel.com>
commit a5321aec6412b20b5ad15db2d6b916c05349dbff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a5321aec.failed

Original idea by Ashok, completely rewritten by Borislav.

Before you read any further: the early loading method is still the
preferred one and you should always do that. The following patch is
improving the late loading mechanism for long running jobs and cloud use
cases.

Gather all cores and serialize the microcode update on them by doing it
one-by-one to make the late update process as reliable as possible and
avoid potential issues caused by the microcode update.

[ Borislav: Rewrite completely. ]

Co-developed-by: Borislav Petkov <bp@suse.de>
	Signed-off-by: Ashok Raj <ashok.raj@intel.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Tom Lendacky <thomas.lendacky@amd.com>
	Tested-by: Ashok Raj <ashok.raj@intel.com>
	Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Arjan Van De Ven <arjan.van.de.ven@intel.com>
Link: https://lkml.kernel.org/r/20180228102846.13447-8-bp@alien8.de

(cherry picked from commit a5321aec6412b20b5ad15db2d6b916c05349dbff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/microcode/core.c
diff --cc arch/x86/kernel/cpu/microcode/core.c
index 1b81185d9250,70ecbc8099c9..000000000000
--- a/arch/x86/kernel/cpu/microcode/core.c
+++ b/arch/x86/kernel/cpu/microcode/core.c
@@@ -60,12 -67,12 +63,17 @@@ static bool dis_ucode_ldr
   */
  static DEFINE_MUTEX(microcode_mutex);
  
+ /*
+  * Serialize late loading so that CPUs get updated one-by-one.
+  */
+ static DEFINE_SPINLOCK(update_lock);
+ 
  struct ucode_cpu_info		ucode_cpu_info[NR_CPUS];
 +EXPORT_SYMBOL_GPL(ucode_cpu_info);
 +
 +/*
 + * Operations that are run on a target cpu:
 + */
  
  struct cpu_info_ctx {
  	struct cpu_signature	*cpu_sig;
@@@ -369,31 -494,110 +377,133 @@@ static void __exit microcode_dev_exit(v
  /* fake device for request_firmware */
  static struct platform_device	*microcode_pdev;
  
++<<<<<<< HEAD
 +static int reload_for_cpu(int cpu)
 +{
 +	struct ucode_cpu_info *uci = ucode_cpu_info + cpu;
 +	enum ucode_state ustate;
 +	int err = 0;
 +
 +	if (!uci->valid)
 +		return err;
 +
 +	ustate = microcode_ops->request_microcode_fw(cpu, &microcode_pdev->dev, true);
 +	if (ustate == UCODE_OK)
 +		apply_microcode_on_target(cpu);
 +	else
 +		if (ustate == UCODE_ERROR)
 +			err = -EINVAL;
 +	return err;
++=======
+ /*
+  * Late loading dance. Why the heavy-handed stomp_machine effort?
+  *
+  * - HT siblings must be idle and not execute other code while the other sibling
+  *   is loading microcode in order to avoid any negative interactions caused by
+  *   the loading.
+  *
+  * - In addition, microcode update on the cores must be serialized until this
+  *   requirement can be relaxed in the future. Right now, this is conservative
+  *   and good.
+  */
+ #define SPINUNIT 100 /* 100 nsec */
+ 
+ static int check_online_cpus(void)
+ {
+ 	if (num_online_cpus() == num_present_cpus())
+ 		return 0;
+ 
+ 	pr_err("Not all CPUs online, aborting microcode update.\n");
+ 
+ 	return -EINVAL;
+ }
+ 
+ static atomic_t late_cpus;
+ 
+ /*
+  * Returns:
+  * < 0 - on error
+  *   0 - no update done
+  *   1 - microcode was updated
+  */
+ static int __reload_late(void *info)
+ {
+ 	unsigned int timeout = NSEC_PER_SEC;
+ 	int all_cpus = num_online_cpus();
+ 	int cpu = smp_processor_id();
+ 	enum ucode_state err;
+ 	int ret = 0;
+ 
+ 	atomic_dec(&late_cpus);
+ 
+ 	/*
+ 	 * Wait for all CPUs to arrive. A load will not be attempted unless all
+ 	 * CPUs show up.
+ 	 * */
+ 	while (atomic_read(&late_cpus)) {
+ 		if (timeout < SPINUNIT) {
+ 			pr_err("Timeout while waiting for CPUs rendezvous, remaining: %d\n",
+ 				atomic_read(&late_cpus));
+ 			return -1;
+ 		}
+ 
+ 		ndelay(SPINUNIT);
+ 		timeout -= SPINUNIT;
+ 
+ 		touch_nmi_watchdog();
+ 	}
+ 
+ 	spin_lock(&update_lock);
+ 	apply_microcode_local(&err);
+ 	spin_unlock(&update_lock);
+ 
+ 	if (err > UCODE_NFOUND) {
+ 		pr_warn("Error reloading microcode on CPU %d\n", cpu);
+ 		ret = -1;
+ 	} else if (err == UCODE_UPDATED) {
+ 		ret = 1;
+ 	}
+ 
+ 	atomic_inc(&late_cpus);
+ 
+ 	while (atomic_read(&late_cpus) != all_cpus)
+ 		cpu_relax();
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Reload microcode late on all CPUs. Wait for a sec until they
+  * all gather together.
+  */
+ static int microcode_reload_late(void)
+ {
+ 	int ret;
+ 
+ 	atomic_set(&late_cpus, num_online_cpus());
+ 
+ 	ret = stop_machine_cpuslocked(__reload_late, NULL, cpu_online_mask);
+ 	if (ret < 0)
+ 		return ret;
+ 	else if (ret > 0)
+ 		microcode_check();
+ 
+ 	return ret;
++>>>>>>> a5321aec6412 (x86/microcode: Synchronize late microcode loading)
  }
  
  static ssize_t reload_store(struct device *dev,
  			    struct device_attribute *attr,
  			    const char *buf, size_t size)
  {
++<<<<<<< HEAD
++=======
+ 	enum ucode_state tmp_ret = UCODE_OK;
+ 	int bsp = boot_cpu_data.cpu_index;
++>>>>>>> a5321aec6412 (x86/microcode: Synchronize late microcode loading)
  	unsigned long val;
 -	ssize_t ret = 0;
 +	int cpu;
 +	ssize_t ret = 0, tmp_ret;
  
  	ret = kstrtoul(buf, 0, &val);
  	if (ret)
@@@ -402,25 -606,24 +512,29 @@@
  	if (val != 1)
  		return size;
  
 -	tmp_ret = microcode_ops->request_microcode_fw(bsp, &microcode_pdev->dev, true);
 -	if (tmp_ret != UCODE_OK)
 -		return size;
 -
  	get_online_cpus();
 -
 -	ret = check_online_cpus();
 -	if (ret)
 -		goto put;
 -
  	mutex_lock(&microcode_mutex);
++<<<<<<< HEAD
 +	for_each_online_cpu(cpu) {
 +		tmp_ret = reload_for_cpu(cpu);
 +		if (tmp_ret != 0)
 +			pr_warn("Error reloading microcode on CPU %d\n", cpu);
 +
 +		/* save retval of the first encountered reload error */
 +		if (!ret)
 +			ret = tmp_ret;
 +	}
 +	if (!ret) {
 +		perf_check_microcode();
 +		spec_ctrl_rescan_cpuid();
 +	}
++=======
+ 	ret = microcode_reload_late();
++>>>>>>> a5321aec6412 (x86/microcode: Synchronize late microcode loading)
  	mutex_unlock(&microcode_mutex);
 -
 -put:
  	put_online_cpus();
  
- 	if (!ret)
+ 	if (ret >= 0)
  		ret = size;
  
  	return ret;
* Unmerged path arch/x86/kernel/cpu/microcode/core.c
