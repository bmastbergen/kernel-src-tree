ixgbe: deleting dfwd stations out of order can cause null ptr deref

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author John Fastabend <john.fastabend@gmail.com>
commit 51f3773bdeecf6ec48647dbfea335be4e507da0b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/51f3773b.failed

The number of stations in use is kept in the num_rx_pools counter
in the ixgbe_adapter structure. This is in turn used by the queue
allocation scheme to determine how many queues are needed to support
the number of pools in use with the current feature set.

This works as long as the pools are added and destroyed in order
because (num_rx_pools * queues_per_pool) is equal to the last
queue in use by a pool. But as soon as you delete a pool out of
order this is no longer the case. So the above multiplication
allocates to few queues and a pool may reference a ring that has
not been allocated/initialized.

To resolve use the bit mask of in use pools to determine the final
pool being used and allocate enough queues so that we don't
inadvertently remove its queues.

# ip link add link eth2 \
	numtxqueues 4 numrxqueues 4 txqueuelen 50 type macvlan
# ip link set dev macvlan0 up
# ip link add link eth2 \
	numtxqueues 4 numrxqueues 4 txqueuelen 50 type macvlan
# ip link set dev macvlan1 up
# for i in {0..100}; do
  ip link set dev macvlan0 down; ip link set dev macvlan0 up;
  done;

	Signed-off-by: John Fastabend <john.r.fastabend@intel.com>
	Acked-by: Neil Horman <nhorman@tuxdriver.com>
	Acked-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 51f3773bdeecf6ec48647dbfea335be4e507da0b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 6af8bd8690a7,ec1bf3edb063..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -9553,19 -7536,18 +9553,23 @@@ static void *ixgbe_fwd_add(struct net_d
  {
  	struct ixgbe_fwd_adapter *fwd_adapter = NULL;
  	struct ixgbe_adapter *adapter = netdev_priv(pdev);
++<<<<<<< HEAD
 +	int used_pools = adapter->num_vfs + adapter->num_rx_pools;
++=======
+ 	unsigned int limit;
++>>>>>>> 51f3773bdeec (ixgbe: deleting dfwd stations out of order can cause null ptr deref)
  	int pool, err;
  
 -#ifdef CONFIG_RPS
 -	if (vdev->num_rx_queues != vdev->num_tx_queues) {
 -		netdev_info(pdev, "%s: Only supports a single queue count for TX and RX\n",
 -			    vdev->name);
 +	/* Hardware has a limited number of available pools. Each VF, and the
 +	 * PF require a pool. Check to ensure we don't attempt to use more
 +	 * then the available number of pools.
 +	 */
 +	if (used_pools >= IXGBE_MAX_VF_FUNCTIONS)
  		return ERR_PTR(-EINVAL);
 -	}
 -#endif
 +
  	/* Check for hardware restriction on number of rx/tx queues */
 -	if (vdev->num_tx_queues > IXGBE_MAX_L2A_QUEUES ||
 +	if (vdev->num_rx_queues != vdev->num_tx_queues ||
 +	    vdev->num_tx_queues > IXGBE_MAX_L2A_QUEUES ||
  	    vdev->num_tx_queues == IXGBE_BAD_L2A_QUEUE) {
  		netdev_info(pdev,
  			    "%s: Supports RX/TX Queue counts 1,2, and 4\n",
@@@ -9588,8 -7571,8 +9593,13 @@@
  
  	/* Enable VMDq flag so device will be set in VM mode */
  	adapter->flags |= IXGBE_FLAG_VMDQ_ENABLED | IXGBE_FLAG_SRIOV_ENABLED;
++<<<<<<< HEAD
 +	adapter->ring_feature[RING_F_VMDQ].limit = adapter->num_rx_pools;
 +	adapter->ring_feature[RING_F_RSS].limit = vdev->num_rx_queues;
++=======
+ 	adapter->ring_feature[RING_F_VMDQ].limit = limit + 1;
+ 	adapter->ring_feature[RING_F_RSS].limit = vdev->num_tx_queues;
++>>>>>>> 51f3773bdeec (ixgbe: deleting dfwd stations out of order can cause null ptr deref)
  
  	/* Force reinit of ring allocation with VMDQ enabled */
  	err = ixgbe_setup_tc(pdev, netdev_get_num_tc(pdev));
@@@ -9624,19 -7604,9 +9635,20 @@@ static void ixgbe_fwd_del(struct net_de
  	clear_bit(fwd_adapter->pool, &adapter->fwd_bitmask);
  	adapter->num_rx_pools--;
  
- 	adapter->ring_feature[RING_F_VMDQ].limit = adapter->num_rx_pools;
+ 	limit = find_last_bit(&adapter->fwd_bitmask, 32);
+ 	adapter->ring_feature[RING_F_VMDQ].limit = limit + 1;
  	ixgbe_fwd_ring_down(fwd_adapter->netdev, fwd_adapter);
 +
 +	/* go back to full RSS if we're done with our VMQs */
 +	if (adapter->ring_feature[RING_F_VMDQ].limit == 1) {
 +		int rss = min_t(int, ixgbe_max_rss_indices(adapter),
 +				num_online_cpus());
 +
 +		adapter->flags &= ~IXGBE_FLAG_VMDQ_ENABLED;
 +		adapter->flags &= ~IXGBE_FLAG_SRIOV_ENABLED;
 +		adapter->ring_feature[RING_F_RSS].limit = rss;
 +	}
 +
  	ixgbe_setup_tc(pdev, netdev_get_num_tc(pdev));
  	netdev_dbg(pdev, "pool %i:%i queues %i:%i VSI bitmask %lx\n",
  		   fwd_adapter->pool, adapter->num_rx_pools,
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
