blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ming Lei <ming.lei@redhat.com>
commit 396eaf21ee17c476e8f66249fb1f4a39003d0ab4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/396eaf21.failed

blk_insert_cloned_request() is called in the fast path of a dm-rq driver
(e.g. blk-mq request-based DM mpath).  blk_insert_cloned_request() uses
blk_mq_request_bypass_insert() to directly append the request to the
blk-mq hctx->dispatch_list of the underlying queue.

1) This way isn't efficient enough because the hctx spinlock is always
used.

2) With blk_insert_cloned_request(), we completely bypass underlying
queue's elevator and depend on the upper-level dm-rq driver's elevator
to schedule IO.  But dm-rq currently can't get the underlying queue's
dispatch feedback at all.  Without knowing whether a request was issued
or not (e.g. due to underlying queue being busy) the dm-rq elevator will
not be able to provide effective IO merging (as a side-effect of dm-rq
currently blindly destaging a request from its elevator only to requeue
it after a delay, which kills any opportunity for merging).  This
obviously causes very bad sequential IO performance.

Fix this by updating blk_insert_cloned_request() to use
blk_mq_request_direct_issue().  blk_mq_request_direct_issue() allows a
request to be issued directly to the underlying queue and returns the
dispatch feedback (blk_status_t).  If blk_mq_request_direct_issue()
returns BLK_SYS_RESOURCE the dm-rq driver will now use DM_MAPIO_REQUEUE
to _not_ destage the request.  Whereby preserving the opportunity to
merge IO.

With this, request-based DM's blk-mq sequential IO performance is vastly
improved (as much as 3X in mpath/virtio-scsi testing).

	Signed-off-by: Ming Lei <ming.lei@redhat.com>
[blk-mq.c changes heavily influenced by Ming Lei's initial solution, but
they were refactored to make them less fragile and easier to read/review]
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 396eaf21ee17c476e8f66249fb1f4a39003d0ab4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	block/blk-mq.c
#	drivers/md/dm-rq.c
diff --cc block/blk-core.c
index 3c1e8c52cafa,55f338020254..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -2327,8 -2500,7 +2327,12 @@@ int blk_insert_cloned_request(struct re
  		 * bypass a potential scheduler on the bottom device for
  		 * insert.
  		 */
++<<<<<<< HEAD
 +		blk_mq_request_bypass_insert(rq, true);
 +		return 0;
++=======
+ 		return blk_mq_request_direct_issue(rq);
++>>>>>>> 396eaf21ee17 (blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback)
  	}
  
  	spin_lock_irqsave(q->queue_lock, flags);
diff --cc block/blk-mq.c
index 1eaa154c3ecb,e383a20809f4..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1535,13 -1745,55 +1535,59 @@@ static void __blk_mq_try_issue_directly
  	struct request_queue *q = rq->q;
  	struct blk_mq_queue_data bd = {
  		.rq = rq,
 +		.list = NULL,
  		.last = true,
  	};
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	blk_qc_t new_cookie;
+ 	blk_status_t ret;
+ 
+ 	new_cookie = request_to_qc_t(hctx, rq);
+ 
+ 	/*
+ 	 * For OK queue, we are done. For error, caller may kill it.
+ 	 * Any other error (busy), just add it to our list as we
+ 	 * previously would have done.
+ 	 */
+ 	ret = q->mq_ops->queue_rq(hctx, &bd);
+ 	switch (ret) {
+ 	case BLK_STS_OK:
+ 		*cookie = new_cookie;
+ 		break;
+ 	case BLK_STS_RESOURCE:
+ 		__blk_mq_requeue_request(rq);
+ 		break;
+ 	default:
+ 		*cookie = BLK_QC_T_NONE;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void __blk_mq_fallback_to_insert(struct blk_mq_hw_ctx *hctx,
+ 					struct request *rq,
+ 					bool run_queue, bool bypass_insert)
+ {
+ 	if (!bypass_insert)
+ 		blk_mq_sched_insert_request(rq, false, run_queue, false,
+ 					    hctx->flags & BLK_MQ_F_BLOCKING);
+ 	else
+ 		blk_mq_request_bypass_insert(rq, run_queue);
+ }
+ 
+ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
+ 						struct request *rq,
+ 						blk_qc_t *cookie,
+ 						bool bypass_insert)
+ {
+ 	struct request_queue *q = rq->q;
++>>>>>>> 396eaf21ee17 (blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback)
  	bool run_queue = true;
  
 -	/* RCU or SRCU read lock is needed before checking quiesced flag */
 -	if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
 +	if (blk_mq_hctx_stopped(hctx)) {
  		run_queue = false;
  		goto insert;
  	}
@@@ -1557,48 -1809,53 +1603,86 @@@
  		goto insert;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * For OK queue, we are done. For error, kill it. Any other
 +	 * error (busy), just add it to our list as we previously
 +	 * would have done
 +	 */
 +	ret = q->mq_ops->queue_rq(hctx, &bd);
 +	if (ret == BLK_MQ_RQ_QUEUE_OK)
 +		return;
++=======
+ 	return __blk_mq_issue_directly(hctx, rq, cookie);
+ insert:
+ 	__blk_mq_fallback_to_insert(hctx, rq, run_queue, bypass_insert);
+ 	if (bypass_insert)
+ 		return BLK_STS_RESOURCE;
++>>>>>>> 396eaf21ee17 (blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback)
  
 -	return BLK_STS_OK;
 +	if (ret == BLK_MQ_RQ_QUEUE_ERROR) {
 +		rq->errors = -EIO;
 +		blk_mq_end_request(rq, rq->errors);
 +		return;
 +	}
 +
 +	__blk_mq_requeue_request(rq);
 +insert:
 +	blk_mq_sched_insert_request(rq, false, run_queue, false, may_sleep);
  }
  
  static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 -		struct request *rq, blk_qc_t *cookie)
 +				      struct request *rq)
  {
 -	blk_status_t ret;
 -	int srcu_idx;
 +	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 +		rcu_read_lock();
 +		__blk_mq_try_issue_directly(hctx, rq, false);
 +		rcu_read_unlock();
 +	} else {
 +		unsigned int srcu_idx;
 +
 +		might_sleep();
  
 -	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
++<<<<<<< HEAD
 +		srcu_idx = srcu_read_lock(&hctx->queue_rq_srcu);
 +		__blk_mq_try_issue_directly(hctx, rq, true);
 +		srcu_read_unlock(&hctx->queue_rq_srcu, srcu_idx);
 +	}
 +}
  
 +static void blk_mq_make_request(struct request_queue *q, struct bio *bio)
++=======
+ 	hctx_lock(hctx, &srcu_idx);
+ 
+ 	ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false);
+ 	if (ret == BLK_STS_RESOURCE)
+ 		__blk_mq_fallback_to_insert(hctx, rq, true, false);
+ 	else if (ret != BLK_STS_OK)
+ 		blk_mq_end_request(rq, ret);
+ 
+ 	hctx_unlock(hctx, srcu_idx);
+ }
+ 
+ blk_status_t blk_mq_request_direct_issue(struct request *rq)
+ {
+ 	blk_status_t ret;
+ 	int srcu_idx;
+ 	blk_qc_t unused_cookie;
+ 	struct blk_mq_ctx *ctx = rq->mq_ctx;
+ 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(rq->q, ctx->cpu);
+ 
+ 	hctx_lock(hctx, &srcu_idx);
+ 	ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true);
+ 	hctx_unlock(hctx, srcu_idx);
+ 
+ 	return ret;
+ }
+ 
+ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
++>>>>>>> 396eaf21ee17 (blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback)
  {
 -	const int is_sync = op_is_sync(bio->bi_opf);
 -	const int is_flush_fua = op_is_flush(bio->bi_opf);
 +	const int is_sync = rw_is_sync(bio->bi_rw);
 +	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
  	struct blk_mq_alloc_data data = { .flags = 0 };
  	struct request *rq;
  	unsigned int request_count = 0;
diff --cc drivers/md/dm-rq.c
index d5df417cac04,b7d175e94a02..000000000000
--- a/drivers/md/dm-rq.c
+++ b/drivers/md/dm-rq.c
@@@ -468,12 -395,12 +468,12 @@@ static void end_clone_request(struct re
  	dm_complete_request(tio->orig, error);
  }
  
- static void dm_dispatch_clone_request(struct request *clone, struct request *rq)
+ static blk_status_t dm_dispatch_clone_request(struct request *clone, struct request *rq)
  {
 -	blk_status_t r;
 +	int r;
  
  	if (blk_queue_io_stat(clone->q))
 -		clone->rq_flags |= RQF_IO_STAT;
 +		clone->cmd_flags |= REQ_IO_STAT;
  
  	clone->start_time = jiffies;
  	r = blk_insert_cloned_request(clone->q, clone);
@@@ -629,27 -477,10 +630,33 @@@ static int map_request(struct dm_rq_tar
  	struct mapped_device *md = tio->md;
  	struct request *rq = tio->orig;
  	struct request *clone = NULL;
+ 	blk_status_t ret;
  
++<<<<<<< HEAD
 +	if (tio->clone) {
 +		clone = tio->clone;
 +		r = ti->type->map_rq(ti, clone, &tio->info);
 +		if (r == DM_MAPIO_DELAY_REQUEUE)
 +			return DM_MAPIO_REQUEUE; /* .request_fn requeue is always immediate */
 +	} else {
 +		r = ti->type->clone_and_map_rq(ti, rq, &tio->info, &clone);
 +		if (r < 0) {
 +			/* The target wants to complete the I/O */
 +			dm_kill_unmapped_request(rq, r);
 +			return r;
 +		}
 +		if (r == DM_MAPIO_REMAPPED &&
 +		    setup_clone(clone, rq, tio, GFP_ATOMIC)) {
 +			/* -ENOMEM */
 +			ti->type->release_clone_rq(clone);
 +			return DM_MAPIO_REQUEUE;
 +		}
 +	}
 +
++=======
+ 	r = ti->type->clone_and_map_rq(ti, rq, &tio->info, &clone);
+ check_again:
++>>>>>>> 396eaf21ee17 (blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback)
  	switch (r) {
  	case DM_MAPIO_SUBMITTED:
  		/* The target has taken the I/O to submit by itself later */
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq.c
diff --git a/block/blk-mq.h b/block/blk-mq.h
index a1edc1df16f2..e06b582f1e98 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -75,6 +75,9 @@ void blk_mq_cpu_init(void);
 void blk_mq_enable_hotplug(void);
 void blk_mq_disable_hotplug(void);
 
+/* Used by blk_insert_cloned_request() to issue request directly */
+blk_status_t blk_mq_request_direct_issue(struct request *rq);
+
 /*
  * CPU -> queue mappings
  */
* Unmerged path drivers/md/dm-rq.c
