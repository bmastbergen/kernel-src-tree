md/raid5: remove over-loading of ->bi_phys_segments.

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] raid5: remove over-loading of ->bi_phys_segments (Nigel Croxon) [1494474]
Rebuild_FUZZ: 96.00%
commit-author NeilBrown <neilb@suse.com>
commit 0472a42ba1f89ec85f070c731f4440d7cc38c44c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/0472a42b.failed

When a read request, which bypassed the cache, fails, we need to retry
it through the cache.
This involves attaching it to a sequence of stripe_heads, and it may not
be possible to get all the stripe_heads we need at once.
We do what we can, and record how far we got in ->bi_phys_segments so
we can pick up again later.

There is only ever one bio which may have a non-zero offset stored in
->bi_phys_segments, the one that is either active in the single thread
which calls retry_aligned_read(), or is in conf->retry_read_aligned
waiting for retry_aligned_read() to be called again.

So we only need to store one offset value.  This can be in a local
variable passed between remove_bio_from_retry() and
retry_aligned_read(), or in the r5conf structure next to the
->retry_read_aligned pointer.

Storing it there allows the last usage of ->bi_phys_segments to be
removed from md/raid5.c.

	Signed-off-by: NeilBrown <neilb@suse.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 0472a42ba1f89ec85f070c731f4440d7cc38c44c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5.c
#	drivers/md/raid5.h
diff --cc drivers/md/raid5.c
index b47f960eee74,1c8be667e9a9..000000000000
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@@ -5054,11 -5097,7 +5056,15 @@@ static struct bio *remove_bio_from_retr
  	if(bi) {
  		conf->retry_read_aligned_list = bi->bi_next;
  		bi->bi_next = NULL;
++<<<<<<< HEAD
 +		/*
 +		 * this sets the active strip count to 1 and the processed
 +		 * strip count to zero (upper 8 bits)
 +		 */
 +		raid5_set_bi_stripes(bi, 1); /* biased count of active stripes */
++=======
+ 		*offset = 0;
++>>>>>>> 0472a42ba1f8 (md/raid5: remove over-loading of ->bi_phys_segments.)
  	}
  
  	return bi;
diff --cc drivers/md/raid5.h
index 4c5b46f68dc8,cdc7f92e1806..000000000000
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@@ -486,50 -487,6 +486,53 @@@ static inline struct bio *r5_next_bio(s
  		return NULL;
  }
  
++<<<<<<< HEAD
 +/*
 + * We maintain a biased count of active stripes in the bottom 16 bits of
 + * bi_phys_segments, and a count of processed stripes in the upper 16 bits
 + */
 +static inline int raid5_bi_processed_stripes(struct bio *bio)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +
 +	return (atomic_read(segments) >> 16) & 0xffff;
 +}
 +
 +static inline int raid5_dec_bi_active_stripes(struct bio *bio)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +
 +	return atomic_sub_return(1, segments) & 0xffff;
 +}
 +
 +static inline void raid5_inc_bi_active_stripes(struct bio *bio)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +
 +	atomic_inc(segments);
 +}
 +
 +static inline void raid5_set_bi_processed_stripes(struct bio *bio,
 +	unsigned int cnt)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +	int old, new;
 +
 +	do {
 +		old = atomic_read(segments);
 +		new = (old & 0xffff) | (cnt << 16);
 +	} while (atomic_cmpxchg(segments, old, new) != old);
 +}
 +
 +static inline void raid5_set_bi_stripes(struct bio *bio, unsigned int cnt)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +
 +	atomic_set(segments, cnt);
 +}
 +
++=======
++>>>>>>> 0472a42ba1f8 (md/raid5: remove over-loading of ->bi_phys_segments.)
  /* NOTE NR_STRIPE_HASH_LOCKS must remain below 64.
   * This is because we sometimes take all the spinlocks
   * and creating that much locking depth can cause
* Unmerged path drivers/md/raid5.c
* Unmerged path drivers/md/raid5.h
