drm/nouveau/gem: tie deferred unmapping of buffers to VMA fence completion

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ben Skeggs <bskeggs@redhat.com>
commit 470db8b78186efe840b6452c6c4934178058059e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/470db8b7.failed

As VMAs are per-client, unlike buffers, this allows us to avoid referencing
foreign fences (those that belong to another client/driver) from the client
deferred work handler, and prevent some not-fun race conditions that can be
triggered when a fence stalls.

	Signed-off-by: Ben Skeggs <bskeggs@redhat.com>
(cherry picked from commit 470db8b78186efe840b6452c6c4934178058059e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/nouveau/nouveau_gem.c
diff --cc drivers/gpu/drm/nouveau/nouveau_gem.c
index 2170534101ca,300daee74209..000000000000
--- a/drivers/gpu/drm/nouveau/nouveau_gem.c
+++ b/drivers/gpu/drm/nouveau/nouveau_gem.c
@@@ -114,33 -104,36 +114,44 @@@ nouveau_gem_object_delete(void *data
  }
  
  static void
 -nouveau_gem_object_delete_work(struct nouveau_cli_work *w)
 -{
 -	struct nouveau_gem_object_unmap *work =
 -		container_of(w, typeof(*work), work);
 -	nouveau_gem_object_delete(work->vma);
 -	kfree(work);
 -}
 -
 -static void
 -nouveau_gem_object_unmap(struct nouveau_bo *nvbo, struct nouveau_vma *vma)
 +nouveau_gem_object_unmap(struct nouveau_bo *nvbo, struct nvkm_vma *vma)
  {
++<<<<<<< HEAD
 +	const bool mapped = nvbo->bo.mem.mem_type != TTM_PL_SYSTEM;
 +	struct reservation_object *resv = nvbo->bo.resv;
 +	struct reservation_object_list *fobj;
 +	struct dma_fence *fence = NULL;
 +
 +	fobj = reservation_object_get_list(resv);
++=======
+ 	struct dma_fence *fence = vma->fence ? &vma->fence->base : NULL;
+ 	struct nouveau_gem_object_unmap *work;
++>>>>>>> 470db8b78186 (drm/nouveau/gem: tie deferred unmapping of buffers to VMA fence completion)
  
 -	list_del_init(&vma->head);
 +	list_del(&vma->head);
  
 -	if (!fence) {
 -		nouveau_gem_object_delete(vma);
 -		return;
 -	}
++<<<<<<< HEAD
 +	if (fobj && fobj->shared_count > 1)
 +		ttm_bo_wait(&nvbo->bo, false, false);
 +	else if (fobj && fobj->shared_count == 1)
 +		fence = rcu_dereference_protected(fobj->shared[0],
 +						reservation_object_held(resv));
 +	else
 +		fence = reservation_object_get_excl(nvbo->bo.resv);
  
 -	if (!(work = kmalloc(sizeof(*work), GFP_KERNEL))) {
 -		WARN_ON(dma_fence_wait_timeout(fence, false, 2 * HZ) <= 0);
 +	if (fence && mapped) {
 +		nouveau_fence_work(fence, nouveau_gem_object_delete, vma);
 +	} else {
 +		if (mapped)
 +			nvkm_vm_unmap(vma);
 +		nvkm_vm_put(vma);
 +		kfree(vma);
++=======
++	if (!fence) {
+ 		nouveau_gem_object_delete(vma);
+ 		return;
++>>>>>>> 470db8b78186 (drm/nouveau/gem: tie deferred unmapping of buffers to VMA fence completion)
  	}
 -
 -	work->work.func = nouveau_gem_object_delete_work;
 -	work->vma = vma;
 -	nouveau_cli_work_queue(vma->vmm->cli, fence, &work->work);
  }
  
  void
* Unmerged path drivers/gpu/drm/nouveau/nouveau_gem.c
