x86/bugs: Rename _RDS to _SSBD

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] bugs: Rename _RDS to _SSBD (Waiman Long) [1566905] {CVE-2018-3639}
Rebuild_FUZZ: 92.86%
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 9f65fb29374ee37856dbad847b4e121aab72b510
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9f65fb29.failed

Intel collateral will reference the SSB mitigation bit in IA32_SPEC_CTL[2]
as SSBD (Speculative Store Bypass Disable).

Hence changing it.

It is unclear yet what the MSR_IA32_ARCH_CAPABILITIES (0x10a) Bit(4) name
is going to be. Following the rename it would be SSBD_NO but that rolls out
to Speculative Store Bypass Disable No.

Also fixed the missing space in X86_FEATURE_AMD_SSBD.

[ tglx: Fixup x86_amd_rds_enable() and rds_tif_to_amd_ls_cfg() as well ]

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 9f65fb29374ee37856dbad847b4e121aab72b510)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/msr-index.h
#	arch/x86/include/asm/spec-ctrl.h
#	arch/x86/include/asm/thread_info.h
#	arch/x86/kernel/cpu/amd.c
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
#	arch/x86/kernel/cpu/intel.c
#	arch/x86/kernel/process.c
#	arch/x86/kvm/cpuid.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/include/asm/cpufeatures.h
index 9284f1bf3f7a,4e1c747acbf8..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -190,79 -190,92 +190,86 @@@
   *
   * Reuse free bits when adding new feature flags!
   */
 -#define X86_FEATURE_RING3MWAIT		( 7*32+ 0) /* Ring 3 MONITOR/MWAIT instructions */
 -#define X86_FEATURE_CPUID_FAULT		( 7*32+ 1) /* Intel CPUID faulting */
 -#define X86_FEATURE_CPB			( 7*32+ 2) /* AMD Core Performance Boost */
 -#define X86_FEATURE_EPB			( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 -#define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
 -#define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
 -#define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
 -#define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID && CR4.PCIDE=1 */
 -
 -#define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
 -#define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 -#define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
 -#define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
 -#define X86_FEATURE_RETPOLINE		( 7*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* "" AMD Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 -#define X86_FEATURE_CDP_L2		( 7*32+15) /* Code and Data Prioritization L2 */
 -
 -#define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
 -#define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* "" Fill RSB on context switches */
 -#define X86_FEATURE_SEV			( 7*32+20) /* AMD Secure Encrypted Virtualization */
  
 +#define X86_FEATURE_RING3MWAIT	(7*32+ 0) /* Ring 3 MONITOR/MWAIT */
 +#define X86_FEATURE_CPB		(7*32+ 2) /* AMD Core Performance Boost */
 +#define X86_FEATURE_EPB		(7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 +#define X86_FEATURE_CAT_L3	(7*32+ 4) /* Cache Allocation Technology L3 */
 +#define X86_FEATURE_CAT_L2	(7*32+ 5) /* Cache Allocation Technology L2 */
 +#define X86_FEATURE_CDP_L3	(7*32+ 6) /* Code and Data Prioritization L3 */
 +
 +#define X86_FEATURE_HW_PSTATE	(7*32+ 8) /* AMD HW-PState */
 +#define X86_FEATURE_PROC_FEEDBACK (7*32+ 9) /* AMD ProcFeedbackInterface */
 +#define X86_FEATURE_SME		( 7*32+10) /* AMD Secure Memory Encryption */
 +#define X86_FEATURE_RETPOLINE_AMD (7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */
 +#define X86_FEATURE_INTEL_PPIN	( 7*32+14) /* Intel Processor Inventory Number */
 +#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */
 +
++<<<<<<< HEAD
 +#define X86_FEATURE_MBA		( 7*32+18) /* Memory Bandwidth Allocation */
 +#define X86_FEATURE_IBP_DISABLE ( 7*32+21) /* Old AMD Indirect Branch Predictor Disable */
++=======
+ #define X86_FEATURE_USE_IBPB		( 7*32+21) /* "" Indirect Branch Prediction Barrier enabled */
+ #define X86_FEATURE_USE_IBRS_FW		( 7*32+22) /* "" Use IBRS during runtime firmware calls */
+ #define X86_FEATURE_SPEC_STORE_BYPASS_DISABLE	( 7*32+23) /* "" Disable Speculative Store Bypass. */
+ #define X86_FEATURE_AMD_SSBD		( 7*32+24)  /* "" AMD SSBD implementation */
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  
  /* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 -
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 -
 -
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 -
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 -
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (EDX), word 11 */
 -#define X86_FEATURE_CQM_LLC		(11*32+ 1) /* LLC QoS if 1 */
 -
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (EDX), word 12 */
 -#define X86_FEATURE_CQM_OCCUP_LLC	(12*32+ 0) /* LLC occupancy monitoring */
 -#define X86_FEATURE_CQM_MBM_TOTAL	(12*32+ 1) /* LLC Total MBM monitoring */
 -#define X86_FEATURE_CQM_MBM_LOCAL	(12*32+ 2) /* LLC Local MBM monitoring */
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
 +
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
 +
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
 +
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
 -#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 -#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 -#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
  #define X86_FEATURE_IBPB		(13*32+12) /* Indirect Branch Prediction Barrier */
  #define X86_FEATURE_IBRS		(13*32+14) /* Indirect Branch Restricted Speculation */
  #define X86_FEATURE_STIBP		(13*32+15) /* Single Thread Indirect Branch Predictors */
@@@ -308,9 -332,11 +315,13 @@@
  /* Intel-defined CPU features, CPUID level 0x00000007:0 (EDX), word 18 */
  #define X86_FEATURE_AVX512_4VNNIW	(18*32+ 2) /* AVX-512 Neural Network Instructions */
  #define X86_FEATURE_AVX512_4FMAPS	(18*32+ 3) /* AVX-512 Multiply Accumulation Single precision */
 -#define X86_FEATURE_PCONFIG		(18*32+18) /* Intel PCONFIG */
 -#define X86_FEATURE_SPEC_CTRL		(18*32+26) /* "" Speculation Control (IBRS + IBPB) */
 -#define X86_FEATURE_INTEL_STIBP		(18*32+27) /* "" Single Thread Indirect Branch Predictors */
 +#define X86_FEATURE_SPEC_CTRL		(18*32+26) /* Speculation Control (IBRS + IBPB) */
 +#define X86_FEATURE_INTEL_STIBP		(18*32+27) /* Single Thread Indirect Branch Predictors */
  #define X86_FEATURE_ARCH_CAPABILITIES	(18*32+29) /* IA32_ARCH_CAPABILITIES MSR (Intel) */
++<<<<<<< HEAD
++=======
+ #define X86_FEATURE_SSBD		(18*32+31) /* Speculative Store Bypass Disable */
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  
  /*
   * BUG word(s)
diff --cc arch/x86/include/asm/msr-index.h
index e2029f6c9386,0da3ca260b06..000000000000
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@@ -33,8 -39,14 +33,19 @@@
  
  /* Intel MSRs. Some also available on other CPUs */
  
++<<<<<<< HEAD
 +#define MSR_IA32_SPEC_CTRL		0x00000048
 +#define MSR_IA32_PRED_CMD		0x00000049
++=======
+ #define MSR_IA32_SPEC_CTRL		0x00000048 /* Speculation Control */
+ #define SPEC_CTRL_IBRS			(1 << 0)   /* Indirect Branch Restricted Speculation */
+ #define SPEC_CTRL_STIBP			(1 << 1)   /* Single Thread Indirect Branch Predictors */
+ #define SPEC_CTRL_SSBD_SHIFT		2	   /* Speculative Store Bypass Disable bit */
+ #define SPEC_CTRL_SSBD			(1 << SPEC_CTRL_SSBD_SHIFT)   /* Speculative Store Bypass Disable */
+ 
+ #define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */
+ #define PRED_CMD_IBPB			(1 << 0)   /* Indirect Branch Prediction Barrier */
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  
  #define MSR_PPIN_CTL			0x0000004e
  #define MSR_PPIN			0x0000004f
@@@ -52,8 -65,17 +63,21 @@@
  #define SNB_C1_AUTO_UNDEMOTE		(1UL << 27)
  #define SNB_C3_AUTO_UNDEMOTE		(1UL << 28)
  
 +#define MSR_PLATFORM_INFO		0x000000ce
  #define MSR_MTRRcap			0x000000fe
++<<<<<<< HEAD
++=======
+ 
+ #define MSR_IA32_ARCH_CAPABILITIES	0x0000010a
+ #define ARCH_CAP_RDCL_NO		(1 << 0)   /* Not susceptible to Meltdown */
+ #define ARCH_CAP_IBRS_ALL		(1 << 1)   /* Enhanced IBRS support */
+ #define ARCH_CAP_SSBD_NO		(1 << 4)   /*
+ 						    * Not susceptible to Speculative Store Bypass
+ 						    * attack, so no Speculative Store Bypass
+ 						    * control required.
+ 						    */
+ 
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  #define MSR_IA32_BBL_CR_CTL		0x00000119
  #define MSR_IA32_BBL_CR_CTL3		0x0000011e
  
diff --cc arch/x86/include/asm/thread_info.h
index 2fcd0a3d04a2,2ff2a30a264f..000000000000
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@@ -75,6 -79,7 +75,10 @@@ struct thread_info 
  #define TIF_SIGPENDING		2	/* signal pending */
  #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
  #define TIF_SINGLESTEP		4	/* reenable singlestep on user return*/
++<<<<<<< HEAD
++=======
+ #define TIF_SSBD			5	/* Reduced data speculation */
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
  #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
  #define TIF_SECCOMP		8	/* secure computing */
@@@ -101,6 -106,7 +105,10 @@@
  #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
  #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
  #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
++<<<<<<< HEAD
++=======
+ #define _TIF_SSBD		(1 << TIF_SSBD)
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
  #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
  #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
@@@ -152,16 -146,13 +160,20 @@@
  
  /* flags to check in __switch_to() */
  #define _TIF_WORK_CTXSW							\
++<<<<<<< HEAD
 +	(_TIF_IO_BITMAP|_TIF_NOTSC|_TIF_BLOCKSTEP)
++=======
+ 	(_TIF_IO_BITMAP|_TIF_NOCPUID|_TIF_NOTSC|_TIF_BLOCKSTEP|_TIF_SSBD)
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  
  #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 -#define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
 +#define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)
  
 -#define STACK_WARN		(THREAD_SIZE/8)
 +#define PREEMPT_ACTIVE		0x10000000
  
 +#ifdef CONFIG_X86_32
 +
 +#define STACK_WARN	(THREAD_SIZE/8)
  /*
   * macros/functions for gaining access to the thread information structure
   *
diff --cc arch/x86/kernel/cpu/amd.c
index 8e108518b7dc,7bde990b0385..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -512,6 -540,86 +512,89 @@@ static void bsp_init_amd(struct cpuinfo
  		/* A random value per boot for bit slice [12:upper_bit) */
  		va_align.bits = get_random_int() & va_align.mask;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	if (cpu_has(c, X86_FEATURE_MWAITX))
+ 		use_mwaitx_delay();
+ 
+ 	if (boot_cpu_has(X86_FEATURE_TOPOEXT)) {
+ 		u32 ecx;
+ 
+ 		ecx = cpuid_ecx(0x8000001e);
+ 		nodes_per_socket = ((ecx >> 8) & 7) + 1;
+ 	} else if (boot_cpu_has(X86_FEATURE_NODEID_MSR)) {
+ 		u64 value;
+ 
+ 		rdmsrl(MSR_FAM10H_NODE_ID, value);
+ 		nodes_per_socket = ((value >> 3) & 7) + 1;
+ 	}
+ 
+ 	if (c->x86 >= 0x15 && c->x86 <= 0x17) {
+ 		unsigned int bit;
+ 
+ 		switch (c->x86) {
+ 		case 0x15: bit = 54; break;
+ 		case 0x16: bit = 33; break;
+ 		case 0x17: bit = 10; break;
+ 		default: return;
+ 		}
+ 		/*
+ 		 * Try to cache the base value so further operations can
+ 		 * avoid RMW. If that faults, do not enable SSBD.
+ 		 */
+ 		if (!rdmsrl_safe(MSR_AMD64_LS_CFG, &x86_amd_ls_cfg_base)) {
+ 			setup_force_cpu_cap(X86_FEATURE_SSBD);
+ 			setup_force_cpu_cap(X86_FEATURE_AMD_SSBD);
+ 			x86_amd_ls_cfg_ssbd_mask = 1ULL << bit;
+ 		}
+ 	}
+ }
+ 
+ static void early_detect_mem_encrypt(struct cpuinfo_x86 *c)
+ {
+ 	u64 msr;
+ 
+ 	/*
+ 	 * BIOS support is required for SME and SEV.
+ 	 *   For SME: If BIOS has enabled SME then adjust x86_phys_bits by
+ 	 *	      the SME physical address space reduction value.
+ 	 *	      If BIOS has not enabled SME then don't advertise the
+ 	 *	      SME feature (set in scattered.c).
+ 	 *   For SEV: If BIOS has not enabled SEV then don't advertise the
+ 	 *            SEV feature (set in scattered.c).
+ 	 *
+ 	 *   In all cases, since support for SME and SEV requires long mode,
+ 	 *   don't advertise the feature under CONFIG_X86_32.
+ 	 */
+ 	if (cpu_has(c, X86_FEATURE_SME) || cpu_has(c, X86_FEATURE_SEV)) {
+ 		/* Check if memory encryption is enabled */
+ 		rdmsrl(MSR_K8_SYSCFG, msr);
+ 		if (!(msr & MSR_K8_SYSCFG_MEM_ENCRYPT))
+ 			goto clear_all;
+ 
+ 		/*
+ 		 * Always adjust physical address bits. Even though this
+ 		 * will be a value above 32-bits this is still done for
+ 		 * CONFIG_X86_32 so that accurate values are reported.
+ 		 */
+ 		c->x86_phys_bits -= (cpuid_ebx(0x8000001f) >> 6) & 0x3f;
+ 
+ 		if (IS_ENABLED(CONFIG_X86_32))
+ 			goto clear_all;
+ 
+ 		rdmsrl(MSR_K7_HWCR, msr);
+ 		if (!(msr & MSR_K7_HWCR_SMMLOCK))
+ 			goto clear_sev;
+ 
+ 		return;
+ 
+ clear_all:
+ 		clear_cpu_cap(c, X86_FEATURE_SME);
+ clear_sev:
+ 		clear_cpu_cap(c, X86_FEATURE_SEV);
+ 	}
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  }
  
  static void early_init_amd(struct cpuinfo_x86 *c)
@@@ -780,51 -911,19 +863,57 @@@ static void init_amd(struct cpuinfo_x8
  	if (c->x86 > 0x11)
  		set_cpu_cap(c, X86_FEATURE_ARAT);
  
 -	/* 3DNow or LM implies PREFETCHW */
 -	if (!cpu_has(c, X86_FEATURE_3DNOWPREFETCH))
 -		if (cpu_has(c, X86_FEATURE_3DNOW) || cpu_has(c, X86_FEATURE_LM))
 -			set_cpu_cap(c, X86_FEATURE_3DNOWPREFETCH);
 +	if (c->x86 == 0x10) {
 +		/*
 +		 * Disable GART TLB Walk Errors on Fam10h. We do this here
 +		 * because this is always needed when GART is enabled, even in a
 +		 * kernel which has no MCE support built in.
 +		 * BIOS should disable GartTlbWlk Errors themself. If
 +		 * it doesn't do it here as suggested by the BKDG.
 +		 *
 +		 * Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=33012
 +		 */
 +		u64 mask;
 +		int err;
 +
 +		err = rdmsrl_safe(MSR_AMD64_MCx_MASK(4), &mask);
 +		if (err == 0) {
 +			mask |= (1 << 10);
 +			wrmsrl_safe(MSR_AMD64_MCx_MASK(4), mask);
 +		}
 +
++<<<<<<< HEAD
 +		/*
 +		 * On family 10h BIOS may not have properly enabled WC+ support,
 +		 * causing it to be converted to CD memtype. This may result in
 +		 * performance degradation for certain nested-paging guests.
 +		 * Prevent this conversion by clearing bit 24 in
 +		 * MSR_AMD64_BU_CFG2.
 +		 *
 +		 * NOTE: we want to use the _safe accessors so as not to #GP kvm
 +		 * guests on older kvm hosts.
 +		 */
  
 -	/* AMD CPUs don't reset SS attributes on SYSRET, Xen does. */
 -	if (!cpu_has(c, X86_FEATURE_XENPV))
 -		set_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
 +		rdmsrl_safe(MSR_AMD64_BU_CFG2, &value);
 +		value &= ~(1ULL << 24);
 +		wrmsrl_safe(MSR_AMD64_BU_CFG2, value);
  
 +		if (cpu_has_amd_erratum(c, amd_erratum_383))
 +			set_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);
++=======
+ 	if (boot_cpu_has(X86_FEATURE_AMD_SSBD)) {
+ 		set_cpu_cap(c, X86_FEATURE_SSBD);
+ 		set_cpu_cap(c, X86_FEATURE_AMD_SSBD);
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  	}
 +
 +	if (cpu_has_amd_erratum(c, amd_erratum_400))
 +		set_cpu_bug(c, X86_BUG_AMD_APIC_C1E);
 +
 +	if (c->x86 == 0x10 || c->x86 == 0x12)
 +		set_cpu_cap(c, X86_FEATURE_IBP_DISABLE);
 +
 +	set_cpu_cap(c, X86_FEATURE_RETPOLINE_AMD);
  }
  
  #ifdef CONFIG_X86_32
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,09b116b7f3bf..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -21,10 -25,30 +21,33 @@@
  #include <asm/paravirt.h>
  #include <asm/alternative.h>
  #include <asm/pgtable.h>
 -#include <asm/set_memory.h>
 -#include <asm/intel-family.h>
 +#include <asm/cacheflush.h>
 +#include <asm/spec_ctrl.h>
  
  static void __init spectre_v2_select_mitigation(void);
++<<<<<<< HEAD
++=======
+ static void __init ssb_select_mitigation(void);
+ 
+ /*
+  * Our boot-time value of the SPEC_CTRL MSR. We read it once so that any
+  * writes to SPEC_CTRL contain whatever reserved bits have been set.
+  */
+ u64 __ro_after_init x86_spec_ctrl_base;
+ 
+ /*
+  * The vendor and possibly platform specific bits which can be modified in
+  * x86_spec_ctrl_base.
+  */
+ static u64 __ro_after_init x86_spec_ctrl_mask = ~SPEC_CTRL_IBRS;
+ 
+ /*
+  * AMD specific MSR info for Speculative Store Bypass control.
+  * x86_amd_ls_cfg_ssbd_mask is initialized in identify_boot_cpu().
+  */
+ u64 __ro_after_init x86_amd_ls_cfg_base;
+ u64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  
  void __init check_bugs(void)
  {
@@@ -104,6 -129,104 +127,107 @@@ enum spectre_v2_mitigation_cmd spectre_
  #undef pr_fmt
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
++<<<<<<< HEAD
++=======
+ static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
+ 	SPECTRE_V2_NONE;
+ 
+ void x86_spec_ctrl_set(u64 val)
+ {
+ 	if (val & x86_spec_ctrl_mask)
+ 		WARN_ONCE(1, "SPEC_CTRL MSR value 0x%16llx is unknown.\n", val);
+ 	else
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base | val);
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_set);
+ 
+ u64 x86_spec_ctrl_get_default(void)
+ {
+ 	u64 msrval = x86_spec_ctrl_base;
+ 
+ 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
+ 		msrval |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
+ 	return msrval;
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_get_default);
+ 
+ void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl)
+ {
+ 	u64 host = x86_spec_ctrl_base;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_IBRS))
+ 		return;
+ 
+ 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
+ 		host |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
+ 
+ 	if (host != guest_spec_ctrl)
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, guest_spec_ctrl);
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_set_guest);
+ 
+ void x86_spec_ctrl_restore_host(u64 guest_spec_ctrl)
+ {
+ 	u64 host = x86_spec_ctrl_base;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_IBRS))
+ 		return;
+ 
+ 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
+ 		host |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
+ 
+ 	if (host != guest_spec_ctrl)
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, host);
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_restore_host);
+ 
+ static void x86_amd_ssb_disable(void)
+ {
+ 	u64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_ssbd_mask;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_AMD_SSBD))
+ 		wrmsrl(MSR_AMD64_LS_CFG, msrval);
+ }
+ 
+ #ifdef RETPOLINE
+ static bool spectre_v2_bad_module;
+ 
+ bool retpoline_module_ok(bool has_retpoline)
+ {
+ 	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
+ 		return true;
+ 
+ 	pr_err("System may be vulnerable to spectre v2\n");
+ 	spectre_v2_bad_module = true;
+ 	return false;
+ }
+ 
+ static inline const char *spectre_v2_module_string(void)
+ {
+ 	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
+ }
+ #else
+ static inline const char *spectre_v2_module_string(void) { return ""; }
+ #endif
+ 
+ static void __init spec2_print_if_insecure(const char *reason)
+ {
+ 	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static void __init spec2_print_if_secure(const char *reason)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static inline bool retp_compiler(void)
+ {
+ 	return __is_defined(RETPOLINE);
+ }
+ 
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  static inline bool match_option(const char *arg, int arglen, const char *opt)
  {
  	int len = strlen(opt);
@@@ -199,15 -406,269 +323,249 @@@ static void __init spectre_v2_select_mi
  }
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
+ 
+ static enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;
+ 
+ /* The kernel command line selection */
+ enum ssb_mitigation_cmd {
+ 	SPEC_STORE_BYPASS_CMD_NONE,
+ 	SPEC_STORE_BYPASS_CMD_AUTO,
+ 	SPEC_STORE_BYPASS_CMD_ON,
+ 	SPEC_STORE_BYPASS_CMD_PRCTL,
+ 	SPEC_STORE_BYPASS_CMD_SECCOMP,
+ };
+ 
+ static const char *ssb_strings[] = {
+ 	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
+ 	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled",
+ 	[SPEC_STORE_BYPASS_PRCTL]	= "Mitigation: Speculative Store Bypass disabled via prctl",
+ 	[SPEC_STORE_BYPASS_SECCOMP]	= "Mitigation: Speculative Store Bypass disabled via prctl and seccomp",
+ };
+ 
+ static const struct {
+ 	const char *option;
+ 	enum ssb_mitigation_cmd cmd;
+ } ssb_mitigation_options[] = {
+ 	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO },    /* Platform decides */
+ 	{ "on",		SPEC_STORE_BYPASS_CMD_ON },      /* Disable Speculative Store Bypass */
+ 	{ "off",	SPEC_STORE_BYPASS_CMD_NONE },    /* Don't touch Speculative Store Bypass */
+ 	{ "prctl",	SPEC_STORE_BYPASS_CMD_PRCTL },   /* Disable Speculative Store Bypass via prctl */
+ 	{ "seccomp",	SPEC_STORE_BYPASS_CMD_SECCOMP }, /* Disable Speculative Store Bypass via prctl and seccomp */
+ };
+ 
+ static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
+ {
+ 	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
+ 		return SPEC_STORE_BYPASS_CMD_NONE;
+ 	} else {
+ 		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
+ 					  arg, sizeof(arg));
+ 		if (ret < 0)
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
+ 			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
+ 				continue;
+ 
+ 			cmd = ssb_mitigation_options[i].cmd;
+ 			break;
+ 		}
+ 
+ 		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
+ 			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 		}
+ 	}
+ 
+ 	return cmd;
+ }
+ 
+ static enum ssb_mitigation_cmd __init __ssb_select_mitigation(void)
+ {
+ 	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
+ 	enum ssb_mitigation_cmd cmd;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_SSBD))
+ 		return mode;
+ 
+ 	cmd = ssb_parse_cmdline();
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
+ 	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
+ 	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
+ 		return mode;
+ 
+ 	switch (cmd) {
+ 	case SPEC_STORE_BYPASS_CMD_AUTO:
+ 	case SPEC_STORE_BYPASS_CMD_SECCOMP:
+ 		/*
+ 		 * Choose prctl+seccomp as the default mode if seccomp is
+ 		 * enabled.
+ 		 */
+ 		if (IS_ENABLED(CONFIG_SECCOMP))
+ 			mode = SPEC_STORE_BYPASS_SECCOMP;
+ 		else
+ 			mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_ON:
+ 		mode = SPEC_STORE_BYPASS_DISABLE;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_PRCTL:
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_NONE:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * We have three CPU feature flags that are in play here:
+ 	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.
+ 	 *  - X86_FEATURE_SSBD - CPU is able to turn off speculative store bypass
+ 	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation
+ 	 */
+ 	if (mode == SPEC_STORE_BYPASS_DISABLE) {
+ 		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
+ 		/*
+ 		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD uses
+ 		 * a completely different MSR and bit dependent on family.
+ 		 */
+ 		switch (boot_cpu_data.x86_vendor) {
+ 		case X86_VENDOR_INTEL:
+ 			x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
+ 			x86_spec_ctrl_mask &= ~SPEC_CTRL_SSBD;
+ 			x86_spec_ctrl_set(SPEC_CTRL_SSBD);
+ 			break;
+ 		case X86_VENDOR_AMD:
+ 			x86_amd_ssb_disable();
+ 			break;
+ 		}
+ 	}
+ 
+ 	return mode;
+ }
+ 
+ static void ssb_select_mitigation()
+ {
+ 	ssb_mode = __ssb_select_mitigation();
+ 
+ 	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		pr_info("%s\n", ssb_strings[ssb_mode]);
+ }
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Speculation prctl: " fmt
+ 
+ static int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)
+ {
+ 	bool update;
+ 
+ 	if (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&
+ 	    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)
+ 		return -ENXIO;
+ 
+ 	switch (ctrl) {
+ 	case PR_SPEC_ENABLE:
+ 		/* If speculation is force disabled, enable is not allowed */
+ 		if (task_spec_ssb_force_disable(task))
+ 			return -EPERM;
+ 		task_clear_spec_ssb_disable(task);
+ 		update = test_and_clear_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	case PR_SPEC_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	case PR_SPEC_FORCE_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		task_set_spec_ssb_force_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_SSBD);
+ 		break;
+ 	default:
+ 		return -ERANGE;
+ 	}
+ 
+ 	/*
+ 	 * If being set on non-current task, delay setting the CPU
+ 	 * mitigation until it is next scheduled.
+ 	 */
+ 	if (task == current && update)
+ 		speculative_store_bypass_update();
+ 
+ 	return 0;
+ }
+ 
+ int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
+ 			     unsigned long ctrl)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_set(task, ctrl);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ #ifdef CONFIG_SECCOMP
+ void arch_seccomp_spec_mitigate(struct task_struct *task)
+ {
+ 	if (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)
+ 		ssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);
+ }
+ #endif
+ 
+ static int ssb_prctl_get(struct task_struct *task)
+ {
+ 	switch (ssb_mode) {
+ 	case SPEC_STORE_BYPASS_DISABLE:
+ 		return PR_SPEC_DISABLE;
+ 	case SPEC_STORE_BYPASS_SECCOMP:
+ 	case SPEC_STORE_BYPASS_PRCTL:
+ 		if (task_spec_ssb_force_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;
+ 		if (task_spec_ssb_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_DISABLE;
+ 		return PR_SPEC_PRCTL | PR_SPEC_ENABLE;
+ 	default:
+ 		if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 			return PR_SPEC_ENABLE;
+ 		return PR_SPEC_NOT_AFFECTED;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_get(task);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ void x86_spec_ctrl_setup_ap(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_IBRS))
+ 		x86_spec_ctrl_set(x86_spec_ctrl_base & ~x86_spec_ctrl_mask);
+ 
+ 	if (ssb_mode == SPEC_STORE_BYPASS_DISABLE)
+ 		x86_amd_ssb_disable();
+ }
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  
  #ifdef CONFIG_SYSFS
 -
 -ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
 -
 -	switch (bug) {
 -	case X86_BUG_CPU_MELTDOWN:
 -		if (boot_cpu_has(X86_FEATURE_PTI))
 -			return sprintf(buf, "Mitigation: PTI\n");
 -
 -		break;
 -
 -	case X86_BUG_SPECTRE_V1:
 -		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 -
 -	case X86_BUG_SPECTRE_V2:
 -		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
 -			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
 -			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 -			       spectre_v2_module_string());
 -
 -	case X86_BUG_SPEC_STORE_BYPASS:
 -		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
 -
 -	default:
 -		break;
 -	}
 -
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
  	return sprintf(buf, "Vulnerable\n");
  }
  
diff --cc arch/x86/kernel/cpu/common.c
index 49cb90f121df,9fbb388fadac..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -853,6 -909,75 +853,78 @@@ static void identify_cpu_without_cpuid(
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ static const __initconst struct x86_cpu_id cpu_no_speculation[] = {
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CEDARVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_CLOVERVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_LINCROFT,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PENWELL,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_INTEL,	6, INTEL_FAM6_ATOM_PINEVIEW,	X86_FEATURE_ANY },
+ 	{ X86_VENDOR_CENTAUR,	5 },
+ 	{ X86_VENDOR_INTEL,	5 },
+ 	{ X86_VENDOR_NSC,	5 },
+ 	{ X86_VENDOR_ANY,	4 },
+ 	{}
+ };
+ 
+ static const __initconst struct x86_cpu_id cpu_no_meltdown[] = {
+ 	{ X86_VENDOR_AMD },
+ 	{}
+ };
+ 
+ static const __initconst struct x86_cpu_id cpu_no_spec_store_bypass[] = {
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_PINEVIEW	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_LINCROFT	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_PENWELL		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_CLOVERVIEW	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_CEDARVIEW	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT1	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_AIRMONT		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_SILVERMONT2	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_ATOM_MERRIFIELD	},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_CORE_YONAH		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNL		},
+ 	{ X86_VENDOR_INTEL,	6,	INTEL_FAM6_XEON_PHI_KNM		},
+ 	{ X86_VENDOR_CENTAUR,	5,					},
+ 	{ X86_VENDOR_INTEL,	5,					},
+ 	{ X86_VENDOR_NSC,	5,					},
+ 	{ X86_VENDOR_AMD,	0x12,					},
+ 	{ X86_VENDOR_AMD,	0x11,					},
+ 	{ X86_VENDOR_AMD,	0x10,					},
+ 	{ X86_VENDOR_AMD,	0xf,					},
+ 	{ X86_VENDOR_ANY,	4,					},
+ 	{}
+ };
+ 
+ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
+ {
+ 	u64 ia32_cap = 0;
+ 
+ 	if (cpu_has(c, X86_FEATURE_ARCH_CAPABILITIES))
+ 		rdmsrl(MSR_IA32_ARCH_CAPABILITIES, ia32_cap);
+ 
+ 	if (!x86_match_cpu(cpu_no_spec_store_bypass) &&
+ 	   !(ia32_cap & ARCH_CAP_SSBD_NO))
+ 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
+ 
+ 	if (x86_match_cpu(cpu_no_speculation))
+ 		return;
+ 
+ 	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
+ 	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+ 
+ 	if (x86_match_cpu(cpu_no_meltdown))
+ 		return;
+ 
+ 	/* Rogue Data Cache Load? No! */
+ 	if (ia32_cap & ARCH_CAP_RDCL_NO)
+ 		return;
+ 
+ 	setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
+ }
+ 
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  /*
   * Do minimum CPU detection early.
   * Fields really needed: vendor, cpuid_level, family, model, mask,
diff --cc arch/x86/kernel/cpu/intel.c
index 9ac0d10cb29f,0eab6c89c8d9..000000000000
--- a/arch/x86/kernel/cpu/intel.c
+++ b/arch/x86/kernel/cpu/intel.c
@@@ -123,13 -175,21 +123,28 @@@ static void early_init_intel(struct cpu
  		(c->x86 == 0x6 && c->x86_model >= 0x0e))
  		set_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);
  
 -	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64))
 -		c->microcode = intel_get_microcode_revision();
 +	if (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64)) {
 +		unsigned lower_word;
  
++<<<<<<< HEAD
 +		wrmsr(MSR_IA32_UCODE_REV, 0, 0);
 +		/* Required by the SDM */
 +		sync_core();
 +		rdmsr(MSR_IA32_UCODE_REV, lower_word, c->microcode);
++=======
+ 	/* Now if any of them are set, check the blacklist and clear the lot */
+ 	if ((cpu_has(c, X86_FEATURE_SPEC_CTRL) ||
+ 	     cpu_has(c, X86_FEATURE_INTEL_STIBP) ||
+ 	     cpu_has(c, X86_FEATURE_IBRS) || cpu_has(c, X86_FEATURE_IBPB) ||
+ 	     cpu_has(c, X86_FEATURE_STIBP)) && bad_spectre_microcode(c)) {
+ 		pr_warn("Intel Spectre v2 broken microcode detected; disabling Speculation Control\n");
+ 		setup_clear_cpu_cap(X86_FEATURE_IBRS);
+ 		setup_clear_cpu_cap(X86_FEATURE_IBPB);
+ 		setup_clear_cpu_cap(X86_FEATURE_STIBP);
+ 		setup_clear_cpu_cap(X86_FEATURE_SPEC_CTRL);
+ 		setup_clear_cpu_cap(X86_FEATURE_INTEL_STIBP);
+ 		setup_clear_cpu_cap(X86_FEATURE_SSBD);
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  	}
  
  	/*
diff --cc arch/x86/kernel/process.c
index f741d66041de,b77a091bf3b8..000000000000
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@@ -237,7 -277,60 +237,64 @@@ void __switch_to_xtra(struct task_struc
  		 */
  		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
  	}
++<<<<<<< HEAD
 +	propagate_user_return_notify(prev_p, next_p);
++=======
+ }
+ 
+ static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+ {
+ 	u64 msr;
+ 
+ 	if (static_cpu_has(X86_FEATURE_AMD_SSBD)) {
+ 		msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
+ 		wrmsrl(MSR_AMD64_LS_CFG, msr);
+ 	} else {
+ 		msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
+ 	}
+ }
+ 
+ void speculative_store_bypass_update(void)
+ {
+ 	__speculative_store_bypass_update(current_thread_info()->flags);
+ }
+ 
+ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
+ 		      struct tss_struct *tss)
+ {
+ 	struct thread_struct *prev, *next;
+ 	unsigned long tifp, tifn;
+ 
+ 	prev = &prev_p->thread;
+ 	next = &next_p->thread;
+ 
+ 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
+ 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
+ 	switch_to_bitmap(tss, prev, next, tifp, tifn);
+ 
+ 	propagate_user_return_notify(prev_p, next_p);
+ 
+ 	if ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&
+ 	    arch_has_block_step()) {
+ 		unsigned long debugctl, msk;
+ 
+ 		rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
+ 		debugctl &= ~DEBUGCTLMSR_BTF;
+ 		msk = tifn & _TIF_BLOCKSTEP;
+ 		debugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;
+ 		wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
+ 	}
+ 
+ 	if ((tifp ^ tifn) & _TIF_NOTSC)
+ 		cr4_toggle_bits_irqsoff(X86_CR4_TSD);
+ 
+ 	if ((tifp ^ tifn) & _TIF_NOCPUID)
+ 		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
+ 
+ 	if ((tifp ^ tifn) & _TIF_SSBD)
+ 		__speculative_store_bypass_update(tifn);
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  }
  
  /*
diff --cc arch/x86/kvm/cpuid.c
index 7bb663af32bb,865c9a769864..000000000000
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@@ -383,12 -407,8 +383,17 @@@ static inline int __do_cpuid_ent(struc
  
  	/* cpuid 7.0.edx*/
  	const u32 kvm_cpuid_7_0_edx_x86_features =
++<<<<<<< HEAD
 +		F(AVX512_4VNNIW) | F(AVX512_4FMAPS) |
 +		F(SPEC_CTRL) | F(INTEL_STIBP);
 +
 +	/* cpuid 0x80000008.ebx */
 +	const u32 kvm_cpuid_8000_0008_ebx_x86_features =
 +		F(IBPB) | F(IBRS) | F(STIBP);
++=======
+ 		F(AVX512_4VNNIW) | F(AVX512_4FMAPS) | F(SPEC_CTRL) | F(SSBD) |
+ 		F(ARCH_CAPABILITIES);
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  
  	/* all calls to cpuid_count() should be made on the same cpu */
  	get_cpu();
diff --cc arch/x86/kvm/vmx.c
index c3d0093f5126,9b8d80bf3889..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -2959,7 -3524,8 +2959,12 @@@ static int vmx_get_msr(struct kvm_vcpu 
  	case MSR_IA32_SPEC_CTRL:
  		if (!msr_info->host_initiated &&
  		    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &&
++<<<<<<< HEAD
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
++=======
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL) &&
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_SSBD))
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  			return 1;
  
  		msr_info->data = to_vmx(vcpu)->spec_ctrl;
@@@ -3070,11 -3644,12 +3075,20 @@@ static int vmx_set_msr(struct kvm_vcpu 
  	case MSR_IA32_SPEC_CTRL:
  		if (!msr_info->host_initiated &&
  		    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &&
++<<<<<<< HEAD
 +		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 +			return 1;
 +
 +		/* The STIBP bit doesn't fault even if it's not advertised */
 +		if (data & ~(FEATURE_ENABLE_IBRS | FEATURE_ENABLE_STIBP))
++=======
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL) &&
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_SSBD))
+ 			return 1;
+ 
+ 		/* The STIBP bit doesn't fault even if it's not advertised */
+ 		if (data & ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP | SPEC_CTRL_SSBD))
++>>>>>>> 9f65fb29374e (x86/bugs: Rename _RDS to _SSBD)
  			return 1;
  
  		vmx->spec_ctrl = data;
* Unmerged path arch/x86/include/asm/spec-ctrl.h
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/include/asm/msr-index.h
* Unmerged path arch/x86/include/asm/spec-ctrl.h
* Unmerged path arch/x86/include/asm/thread_info.h
* Unmerged path arch/x86/kernel/cpu/amd.c
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
* Unmerged path arch/x86/kernel/cpu/intel.c
* Unmerged path arch/x86/kernel/process.c
* Unmerged path arch/x86/kvm/cpuid.c
* Unmerged path arch/x86/kvm/vmx.c
