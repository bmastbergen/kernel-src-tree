nvme-rdma: Fix possible double free in reconnect flow

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Sagi Grimberg <sagi@grimberg.me>
commit bd9f07590a17f3158b51fb869dca723f1f606bdc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/bd9f0759.failed

The fact that we free the async event buffer in
nvme_rdma_destroy_admin_queue can cause us to free it
more than once because this happens in every reconnect
attempt since commit 31fdf1840170. we rely on the queue
state flags DELETING to avoid this for other resources.

A more complete fix is to not destroy the admin/io queues
unconditionally on every reconnect attempt, but its a bit
more extensive and will go in the next release.

Fixes: 31fdf1840170 ("nvme-rdma: reuse configure/destroy_admin_queue")
	Reported-by: Yi Zhang <yi.zhang@redhat.com>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit bd9f07590a17f3158b51fb869dca723f1f606bdc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index a9e45e7b65a3,4dbee893a047..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -597,6 -568,15 +597,18 @@@ static void nvme_rdma_stop_queue(struc
  
  static void nvme_rdma_free_queue(struct nvme_rdma_queue *queue)
  {
++<<<<<<< HEAD
++=======
+ 	if (test_and_set_bit(NVME_RDMA_Q_DELETING, &queue->flags))
+ 		return;
+ 
+ 	if (nvme_rdma_queue_idx(queue) == 0) {
+ 		nvme_rdma_free_qe(queue->device->dev,
+ 			&queue->ctrl->async_event_sqe,
+ 			sizeof(struct nvme_command), DMA_TO_DEVICE);
+ 	}
+ 
++>>>>>>> bd9f07590a17 (nvme-rdma: Fix possible double free in reconnect flow)
  	nvme_rdma_destroy_queue_ib(queue);
  	rdma_destroy_id(queue->cm_id);
  }
@@@ -675,14 -676,213 +687,155 @@@ out_free_queues
  	return ret;
  }
  
 -static void nvme_rdma_free_tagset(struct nvme_ctrl *nctrl, bool admin)
 +static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl)
  {
 -	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
 -	struct blk_mq_tag_set *set = admin ?
 -			&ctrl->admin_tag_set : &ctrl->tag_set;
 -
 -	blk_mq_free_tag_set(set);
++<<<<<<< HEAD
 +	nvme_rdma_free_qe(ctrl->queues[0].device->dev, &ctrl->async_event_sqe,
 +			sizeof(struct nvme_command), DMA_TO_DEVICE);
 +	nvme_rdma_stop_and_free_queue(&ctrl->queues[0]);
 +	blk_cleanup_queue(ctrl->ctrl.admin_q);
 +	blk_mq_free_tag_set(&ctrl->admin_tag_set);
  	nvme_rdma_dev_put(ctrl->device);
 -}
 -
 -static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
 -		bool admin)
 -{
 -	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
 -	struct blk_mq_tag_set *set;
 -	int ret;
 -
 -	if (admin) {
 -		set = &ctrl->admin_tag_set;
 -		memset(set, 0, sizeof(*set));
 -		set->ops = &nvme_rdma_admin_mq_ops;
 -		set->queue_depth = NVME_RDMA_AQ_BLKMQ_DEPTH;
 -		set->reserved_tags = 2; /* connect + keep-alive */
 -		set->numa_node = NUMA_NO_NODE;
 -		set->cmd_size = sizeof(struct nvme_rdma_request) +
 -			SG_CHUNK_SIZE * sizeof(struct scatterlist);
 -		set->driver_data = ctrl;
 -		set->nr_hw_queues = 1;
 -		set->timeout = ADMIN_TIMEOUT;
 -	} else {
 -		set = &ctrl->tag_set;
 -		memset(set, 0, sizeof(*set));
 -		set->ops = &nvme_rdma_mq_ops;
 -		set->queue_depth = nctrl->opts->queue_size;
 -		set->reserved_tags = 1; /* fabric connect */
 -		set->numa_node = NUMA_NO_NODE;
 -		set->flags = BLK_MQ_F_SHOULD_MERGE;
 -		set->cmd_size = sizeof(struct nvme_rdma_request) +
 -			SG_CHUNK_SIZE * sizeof(struct scatterlist);
 -		set->driver_data = ctrl;
 -		set->nr_hw_queues = nctrl->queue_count - 1;
 -		set->timeout = NVME_IO_TIMEOUT;
 -	}
 -
 -	ret = blk_mq_alloc_tag_set(set);
 -	if (ret)
 -		goto out;
 -
 -	/*
 -	 * We need a reference on the device as long as the tag_set is alive,
 -	 * as the MRs in the request structures need a valid ib_device.
 -	 */
 -	ret = nvme_rdma_dev_get(ctrl->device);
 -	if (!ret) {
 -		ret = -EINVAL;
 -		goto out_free_tagset;
 -	}
 -
 -	return set;
 -
 -out_free_tagset:
 -	blk_mq_free_tag_set(set);
 -out:
 -	return ERR_PTR(ret);
 -}
 -
 -static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
 -		bool remove)
 -{
++=======
+ 	nvme_rdma_stop_queue(&ctrl->queues[0]);
+ 	if (remove) {
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, true);
+ 	}
+ 	nvme_rdma_free_queue(&ctrl->queues[0]);
+ }
+ 
+ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
+ 		bool new)
+ {
+ 	int error;
+ 
+ 	error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ 	if (error)
+ 		return error;
+ 
+ 	ctrl->device = ctrl->queues[0].device;
+ 
+ 	ctrl->max_fr_pages = min_t(u32, NVME_RDMA_MAX_SEGMENTS,
+ 		ctrl->device->dev->attrs.max_fast_reg_page_list_len);
+ 
+ 	if (new) {
+ 		ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ 		if (IS_ERR(ctrl->ctrl.admin_tagset))
+ 			goto out_free_queue;
+ 
+ 		ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ 		if (IS_ERR(ctrl->ctrl.admin_q)) {
+ 			error = PTR_ERR(ctrl->ctrl.admin_q);
+ 			goto out_free_tagset;
+ 		}
+ 	} else {
+ 		error = blk_mq_reinit_tagset(&ctrl->admin_tag_set,
+ 					     nvme_rdma_reinit_request);
+ 		if (error)
+ 			goto out_free_queue;
+ 	}
+ 
+ 	error = nvme_rdma_start_queue(ctrl, 0);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	error = ctrl->ctrl.ops->reg_read64(&ctrl->ctrl, NVME_REG_CAP,
+ 			&ctrl->ctrl.cap);
+ 	if (error) {
+ 		dev_err(ctrl->ctrl.device,
+ 			"prop_get NVME_REG_CAP failed\n");
+ 		goto out_cleanup_queue;
+ 	}
+ 
+ 	ctrl->ctrl.sqsize =
+ 		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
+ 
+ 	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	ctrl->ctrl.max_hw_sectors =
+ 		(ctrl->max_fr_pages - 1) << (ilog2(SZ_4K) - 9);
+ 
+ 	error = nvme_init_identify(&ctrl->ctrl);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	error = nvme_rdma_alloc_qe(ctrl->queues[0].device->dev,
+ 			&ctrl->async_event_sqe, sizeof(struct nvme_command),
+ 			DMA_TO_DEVICE);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	return 0;
+ 
+ out_cleanup_queue:
+ 	if (new)
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ out_free_tagset:
+ 	if (new)
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, true);
+ out_free_queue:
+ 	nvme_rdma_free_queue(&ctrl->queues[0]);
+ 	return error;
+ }
+ 
+ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
+ 		bool remove)
+ {
+ 	nvme_rdma_stop_io_queues(ctrl);
+ 	if (remove) {
+ 		blk_cleanup_queue(ctrl->ctrl.connect_q);
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, false);
+ 	}
+ 	nvme_rdma_free_io_queues(ctrl);
+ }
+ 
+ static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
+ {
+ 	int ret;
+ 
+ 	ret = nvme_rdma_alloc_io_queues(ctrl);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (new) {
+ 		ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ 		if (IS_ERR(ctrl->ctrl.tagset))
+ 			goto out_free_io_queues;
+ 
+ 		ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ 		if (IS_ERR(ctrl->ctrl.connect_q)) {
+ 			ret = PTR_ERR(ctrl->ctrl.connect_q);
+ 			goto out_free_tag_set;
+ 		}
+ 	} else {
+ 		ret = blk_mq_reinit_tagset(&ctrl->tag_set,
+ 					   nvme_rdma_reinit_request);
+ 		if (ret)
+ 			goto out_free_io_queues;
+ 
+ 		blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ 			ctrl->ctrl.queue_count - 1);
+ 	}
+ 
+ 	ret = nvme_rdma_start_io_queues(ctrl);
+ 	if (ret)
+ 		goto out_cleanup_connect_q;
+ 
+ 	return 0;
+ 
+ out_cleanup_connect_q:
+ 	if (new)
+ 		blk_cleanup_queue(ctrl->ctrl.connect_q);
+ out_free_tag_set:
+ 	if (new)
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, false);
+ out_free_io_queues:
+ 	nvme_rdma_free_io_queues(ctrl);
+ 	return ret;
++>>>>>>> bd9f07590a17 (nvme-rdma: Fix possible double free in reconnect flow)
  }
  
  static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
* Unmerged path drivers/nvme/host/rdma.c
