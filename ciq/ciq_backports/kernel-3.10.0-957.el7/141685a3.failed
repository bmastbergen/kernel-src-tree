tcmu: Add dynamic growing data area feature support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Xiubo Li <lixiubo@cmss.chinamobile.com>
commit 141685a39151aea95eb56562d2953e919c6c73da
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/141685a3.failed

Currently for the TCMU, the ring buffer size is fixed to 64K cmd
area + 1M data area, and this will be bottlenecks for high iops.

The struct tcmu_cmd_entry {} size is fixed about 112 bytes with
iovec[N] & N <= 4, and the size of struct iovec is about 16 bytes.

If N == 0, the ratio will be sizeof(cmd entry) : sizeof(datas) ==
112Bytes : (N * 4096)Bytes = 28 : 0, no data area is need.

If 0 < N <=4, the ratio will be sizeof(cmd entry) : sizeof(datas)
== 112Bytes : (N * 4096)Bytes = 28 : (N * 1024), so the max will
be 28 : 1024.

If N > 4, the sizeof(cmd entry) will be [(N - 4) *16 + 112] bytes,
and its corresponding data size will be [N * 4096], so the ratio
of sizeof(cmd entry) : sizeof(datas) == [(N - 4) * 16 + 112)Bytes
: (N * 4096)Bytes == 4/1024 - 12/(N * 1024), so the max is about
4 : 1024.

When N is bigger, the ratio will be smaller.

As the initial patch, we will set the cmd area size to 2M, and
the cmd area size to 32M. The TCMU will dynamically grows the data
area from 0 to max 32M size as needed.

The cmd area memory will be allocated through vmalloc(), and the
data area's blocks will be allocated individually later when needed.

The allocated data area block memory will be managed via radix tree.
For now the bitmap still be the most efficient way to search and
manage the block index, this could be update later.

	Signed-off-by: Xiubo Li <lixiubo@cmss.chinamobile.com>
	Signed-off-by: Jianfei Hu <hujianfei@cmss.chinamobile.com>
	Acked-by: Mike Christie <mchristi@redhat.com>
	Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>
(cherry picked from commit 141685a39151aea95eb56562d2953e919c6c73da)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/target/target_core_user.c
diff --cc drivers/target/target_core_user.c
index 2ac4515b6a67,02cf543a888d..000000000000
--- a/drivers/target/target_core_user.c
+++ b/drivers/target/target_core_user.c
@@@ -25,11 -26,14 +26,12 @@@
  #include <linux/parser.h>
  #include <linux/vmalloc.h>
  #include <linux/uio_driver.h>
+ #include <linux/radix-tree.h>
  #include <linux/stringify.h>
  #include <linux/bitops.h>
 -#include <linux/highmem.h>
  #include <linux/configfs.h>
  #include <net/genetlink.h>
 -#include <scsi/scsi_common.h>
 -#include <scsi/scsi_proto.h>
 +#include <asm/unaligned.h>
  #include <target/target_core_base.h>
  #include <target/target_core_fabric.h>
  #include <target/target_core_backend.h>
@@@ -61,19 -65,18 +63,31 @@@
   * this may have a 'UAM' comment.
   */
  
- 
  #define TCMU_TIME_OUT (30 * MSEC_PER_SEC)
  
++<<<<<<< HEAD
 +#define DATA_BLOCK_BITS_DEF 2048
 +#define DATA_BLOCKS_BITS_MAX 65536
 +#define DATA_BLOCK_SIZE PAGE_SIZE
 +#define DATA_BLOCK_SHIFT PAGE_SHIFT
 +#define TCMU_MBS_TO_BLOCKS(_mbs) (_mbs << (20 - DATA_BLOCK_SHIFT))
 +#define TCMU_BLOCKS_TO_MBS(_blocks) (_blocks >> (20 - DATA_BLOCK_SHIFT))
 +
 +#define CMDR_SIZE (16 * 4096)
 +
 +static u8 tcmu_kern_cmd_reply_supported;
++=======
+ /* For cmd area, the size is fixed 2M */
+ #define CMDR_SIZE (2 * 1024 * 1024)
+ 
+ /* For data area, the size is fixed 32M */
+ #define DATA_BLOCK_BITS (8 * 1024)
+ #define DATA_BLOCK_SIZE 4096
+ #define DATA_SIZE (DATA_BLOCK_BITS * DATA_BLOCK_SIZE)
+ 
+ /* The ring buffer size is 34M */
+ #define TCMU_RING_SIZE (CMDR_SIZE + DATA_SIZE)
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
  static struct device *tcmu_root_device;
  
@@@ -111,11 -106,7 +125,14 @@@ struct tcmu_dev 
  	/* Must add data_off and mb_addr to get the address */
  	size_t data_off;
  	size_t data_size;
 +	uint32_t max_blocks;
 +	size_t ring_size;
  
++<<<<<<< HEAD
 +	unsigned long *data_bitmap;
 +
++=======
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  	wait_queue_head_t wait_cmdr;
  	/* TODO should this be a mutex? */
  	spinlock_t cmdr_lock;
@@@ -147,7 -136,9 +168,13 @@@ struct tcmu_cmd 
  
  	/* Can't use se_cmd when cleaning up expired cmds, because if
  	   cmd has been completed then accessing se_cmd is off limits */
++<<<<<<< HEAD
 +	unsigned long *data_bitmap;
++=======
+ 	uint32_t dbi_cnt;
+ 	uint32_t dbi_cur;
+ 	uint32_t *dbi;
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
  	unsigned long deadline;
  
@@@ -288,10 -167,65 +315,70 @@@ static struct genl_family tcmu_genl_fam
  	.mcgrps = tcmu_mcgrps,
  	.n_mcgrps = ARRAY_SIZE(tcmu_mcgrps),
  	.netnsok = true,
 +	.ops = tcmu_genl_ops,
 +	.n_ops = ARRAY_SIZE(tcmu_genl_ops),
  };
  
++<<<<<<< HEAD
++=======
+ #define tcmu_cmd_set_dbi_cur(cmd, index) ((cmd)->dbi_cur = (index))
+ #define tcmu_cmd_reset_dbi_cur(cmd) tcmu_cmd_set_dbi_cur(cmd, 0)
+ #define tcmu_cmd_set_dbi(cmd, index) ((cmd)->dbi[(cmd)->dbi_cur++] = (index))
+ #define tcmu_cmd_get_dbi(cmd) ((cmd)->dbi[(cmd)->dbi_cur++])
+ 
+ static void tcmu_cmd_free_data(struct tcmu_cmd *tcmu_cmd)
+ {
+ 	struct tcmu_dev *udev = tcmu_cmd->tcmu_dev;
+ 	uint32_t i;
+ 
+ 	for (i = 0; i < tcmu_cmd->dbi_cnt; i++)
+ 		clear_bit(tcmu_cmd->dbi[i], udev->data_bitmap);
+ }
+ 
+ static int tcmu_get_empty_block(struct tcmu_dev *udev, void **addr)
+ {
+ 	void *p;
+ 	uint32_t dbi;
+ 	int ret;
+ 
+ 	dbi = find_first_zero_bit(udev->data_bitmap, DATA_BLOCK_BITS);
+ 	if (dbi > udev->dbi_max)
+ 		udev->dbi_max = dbi;
+ 
+ 	set_bit(dbi, udev->data_bitmap);
+ 
+ 	p = radix_tree_lookup(&udev->data_blocks, dbi);
+ 	if (!p) {
+ 		p = kzalloc(DATA_BLOCK_SIZE, GFP_ATOMIC);
+ 		if (!p) {
+ 			clear_bit(dbi, udev->data_bitmap);
+ 			return -ENOMEM;
+ 		}
+ 
+ 		ret = radix_tree_insert(&udev->data_blocks, dbi, p);
+ 		if (ret) {
+ 			kfree(p);
+ 			clear_bit(dbi, udev->data_bitmap);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	*addr = p;
+ 	return dbi;
+ }
+ 
+ static void *tcmu_get_block_addr(struct tcmu_dev *udev, uint32_t dbi)
+ {
+ 	return radix_tree_lookup(&udev->data_blocks, dbi);
+ }
+ 
+ static inline void tcmu_free_cmd(struct tcmu_cmd *tcmu_cmd)
+ {
+ 	kfree(tcmu_cmd->dbi);
+ 	kmem_cache_free(tcmu_cmd_cache, tcmu_cmd);
+ }
+ 
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  static inline size_t tcmu_cmd_get_data_length(struct tcmu_cmd *tcmu_cmd)
  {
  	struct se_cmd *se_cmd = tcmu_cmd->se_cmd;
@@@ -313,12 -247,6 +400,15 @@@ static inline uint32_t tcmu_cmd_get_blo
  	return data_length / DATA_BLOCK_SIZE;
  }
  
++<<<<<<< HEAD
 +static void tcmu_free_cmd(struct tcmu_cmd *tcmu_cmd)
 +{
 +	kfree(tcmu_cmd->data_bitmap);
 +	kmem_cache_free(tcmu_cmd_cache, tcmu_cmd);
 +}
 +
++=======
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  static struct tcmu_cmd *tcmu_alloc_cmd(struct se_cmd *se_cmd)
  {
  	struct se_device *se_dev = se_cmd->se_dev;
@@@ -348,9 -280,10 +447,16 @@@
  	spin_unlock_irq(&udev->commands_lock);
  	idr_preload_end();
  
++<<<<<<< HEAD
 +	if (cmd_id < 0)
 +		goto free_bitmap;
 +
++=======
+ 	if (cmd_id < 0) {
+ 		tcmu_free_cmd(tcmu_cmd);
+ 		return NULL;
+ 	}
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  	tcmu_cmd->cmd_id = cmd_id;
  
  	return tcmu_cmd;
@@@ -428,15 -355,15 +534,22 @@@ static inline size_t iov_tail(struct tc
  	return (size_t)iov->iov_base + iov->iov_len;
  }
  
++<<<<<<< HEAD
 +static void alloc_and_scatter_data_area(struct tcmu_dev *udev,
 +	unsigned long *cmd_bitmap, struct scatterlist *data_sg,
 +	unsigned int data_nents, struct iovec **iov, int *iov_cnt,
 +	bool copy_data)
++=======
+ static int alloc_and_scatter_data_area(struct tcmu_dev *udev,
+ 	struct tcmu_cmd *tcmu_cmd, struct scatterlist *data_sg,
+ 	unsigned int data_nents, struct iovec **iov,
+ 	int *iov_cnt, bool copy_data)
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  {
- 	int i, block;
+ 	int i, dbi;
  	int block_remaining = 0;
- 	void *from, *to;
- 	size_t copy_bytes, to_offset;
+ 	void *from, *to = NULL;
+ 	size_t copy_bytes, to_offset, offset;
  	struct scatterlist *sg;
  
  	for_each_sg(data_sg, sg, data_nents, i) {
@@@ -444,17 -371,22 +557,30 @@@
  		from = kmap_atomic(sg_page(sg)) + sg->offset;
  		while (sg_remaining > 0) {
  			if (block_remaining == 0) {
++<<<<<<< HEAD
 +				block = find_first_zero_bit(udev->data_bitmap,
 +							    udev->max_blocks);
 +				block_remaining = DATA_BLOCK_SIZE;
 +				set_bit(block, udev->data_bitmap);
 +				set_bit(block, cmd_bitmap);
++=======
+ 				block_remaining = DATA_BLOCK_SIZE;
+ 				dbi = tcmu_get_empty_block(udev, &to);
+ 				if (dbi < 0) {
+ 					kunmap_atomic(from - sg->offset);
+ 					return dbi;
+ 				}
+ 				tcmu_cmd_set_dbi(tcmu_cmd, dbi);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  			}
+ 
  			copy_bytes = min_t(size_t, sg_remaining,
  					block_remaining);
- 			to_offset = get_block_offset(udev, block,
+ 			to_offset = get_block_offset_user(udev, dbi,
  					block_remaining);
- 			to = (void *)udev->mb_addr + to_offset;
+ 			offset = DATA_BLOCK_SIZE - block_remaining;
+ 			to = (void *)(unsigned long)to + offset;
+ 
  			if (*iov_cnt != 0 &&
  			    to_offset == iov_tail(udev, *iov)) {
  				(*iov)->iov_len += copy_bytes;
@@@ -473,33 -405,48 +599,71 @@@
  		}
  		kunmap_atomic(from - sg->offset);
  	}
- }
  
++<<<<<<< HEAD
 +static void free_data_area(struct tcmu_dev *udev, struct tcmu_cmd *cmd)
 +{
 +	bitmap_xor(udev->data_bitmap, udev->data_bitmap, cmd->data_bitmap,
 +		   udev->max_blocks);
++=======
+ 	return 0;
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  }
  
 -static void gather_data_area(struct tcmu_dev *udev, struct tcmu_cmd *cmd,
 -			     bool bidi)
 +static void gather_data_area(struct tcmu_dev *udev, unsigned long *cmd_bitmap,
 +		struct scatterlist *data_sg, unsigned int data_nents)
  {
++<<<<<<< HEAD
 +	int i, block;
 +	int block_remaining = 0;
 +	void *from, *to;
 +	size_t copy_bytes, from_offset;
 +	struct scatterlist *sg;
++=======
+ 	struct se_cmd *se_cmd = cmd->se_cmd;
+ 	int i, dbi;
+ 	int block_remaining = 0;
+ 	void *from, *to;
+ 	size_t copy_bytes, offset;
+ 	struct scatterlist *sg, *data_sg;
+ 	unsigned int data_nents;
+ 	uint32_t count = 0;
+ 
+ 	if (!bidi) {
+ 		data_sg = se_cmd->t_data_sg;
+ 		data_nents = se_cmd->t_data_nents;
+ 	} else {
+ 
+ 		/*
+ 		 * For bidi case, the first count blocks are for Data-Out
+ 		 * buffer blocks, and before gathering the Data-In buffer
+ 		 * the Data-Out buffer blocks should be discarded.
+ 		 */
+ 		count = DIV_ROUND_UP(se_cmd->data_length, DATA_BLOCK_SIZE);
+ 
+ 		data_sg = se_cmd->t_bidi_data_sg;
+ 		data_nents = se_cmd->t_bidi_data_nents;
+ 	}
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
+ 
+ 	tcmu_cmd_set_dbi_cur(cmd, count);
  
  	for_each_sg(data_sg, sg, data_nents, i) {
  		int sg_remaining = sg->length;
  		to = kmap_atomic(sg_page(sg)) + sg->offset;
  		while (sg_remaining > 0) {
  			if (block_remaining == 0) {
++<<<<<<< HEAD
 +				block = find_first_bit(cmd_bitmap,
 +						       udev->max_blocks);
 +				block_remaining = DATA_BLOCK_SIZE;
 +				clear_bit(block, udev->data_bitmap);
 +				clear_bit(block, cmd_bitmap);
++=======
+ 				block_remaining = DATA_BLOCK_SIZE;
+ 				dbi = tcmu_cmd_get_dbi(cmd);
+ 				from = tcmu_get_block_addr(udev, dbi);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  			}
  			copy_bytes = min_t(size_t, sg_remaining,
  					block_remaining);
@@@ -566,27 -511,6 +729,30 @@@ static bool is_ring_space_avail(struct 
  	return true;
  }
  
++<<<<<<< HEAD
 +static inline size_t tcmu_cmd_get_base_cmd_size(size_t iov_cnt)
 +{
 +	return max(offsetof(struct tcmu_cmd_entry, req.iov[iov_cnt]),
 +			sizeof(struct tcmu_cmd_entry));
 +}
 +
 +static inline size_t tcmu_cmd_get_cmd_size(struct tcmu_cmd *tcmu_cmd,
 +					   size_t base_command_size)
 +{
 +	struct se_cmd *se_cmd = tcmu_cmd->se_cmd;
 +	size_t command_size;
 +
 +	command_size = base_command_size +
 +		round_up(scsi_command_size(se_cmd->t_task_cdb),
 +				TCMU_OP_ALIGN_SIZE);
 +
 +	WARN_ON(command_size & (TCMU_OP_ALIGN_SIZE-1));
 +
 +	return command_size;
 +}
 +
++=======
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  static sense_reason_t
  tcmu_queue_cmd_ring(struct tcmu_cmd *tcmu_cmd)
  {
@@@ -600,7 -524,7 +766,11 @@@
  	uint32_t cmd_head;
  	uint64_t cdb_off;
  	bool copy_to_data_area;
++<<<<<<< HEAD
 +	size_t data_length;
++=======
+ 	size_t data_length = tcmu_cmd_get_data_length(tcmu_cmd);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
  	if (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags))
  		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
@@@ -696,37 -601,44 +866,70 @@@
  	}
  
  	entry = (void *) mb + CMDR_OFF + cmd_head;
 -	tcmu_flush_dcache_range(entry, sizeof(*entry));
 +	memset(entry, 0, command_size);
  	tcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_CMD);
 -	tcmu_hdr_set_len(&entry->hdr.len_op, command_size);
  	entry->hdr.cmd_id = tcmu_cmd->cmd_id;
++<<<<<<< HEAD
++=======
+ 	entry->hdr.kflags = 0;
+ 	entry->hdr.uflags = 0;
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
  	/* Handle allocating space from the data area */
  	iov = &entry->req.iov[0];
  	iov_cnt = 0;
  	copy_to_data_area = (se_cmd->data_direction == DMA_TO_DEVICE
  		|| se_cmd->se_cmd_flags & SCF_BIDI);
++<<<<<<< HEAD
 +	alloc_and_scatter_data_area(udev, tcmu_cmd->data_bitmap,
 +		se_cmd->t_data_sg, se_cmd->t_data_nents, &iov, &iov_cnt,
 +		copy_to_data_area);
++=======
+ 	ret = alloc_and_scatter_data_area(udev, tcmu_cmd,
+ 			se_cmd->t_data_sg, se_cmd->t_data_nents,
+ 			&iov, &iov_cnt, copy_to_data_area);
+ 	if (ret) {
+ 		spin_unlock_irq(&udev->cmdr_lock);
+ 		pr_err("tcmu: alloc and scatter data failed\n");
+ 		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+ 	}
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  	entry->req.iov_cnt = iov_cnt;
  	entry->req.iov_dif_cnt = 0;
  
  	/* Handle BIDI commands */
++<<<<<<< HEAD
 +	iov_cnt = 0;
 +	alloc_and_scatter_data_area(udev, tcmu_cmd->data_bitmap,
 +		se_cmd->t_bidi_data_sg, se_cmd->t_bidi_data_nents, &iov,
 +		&iov_cnt, false);
 +	entry->req.iov_bidi_cnt = iov_cnt;
 +
 +	/*
 +	 * Recalaulate the command's base size and size according
 +	 * to the actual needs
 +	 */
 +	base_command_size = tcmu_cmd_get_base_cmd_size(entry->req.iov_cnt +
 +						       entry->req.iov_bidi_cnt);
 +	command_size = tcmu_cmd_get_cmd_size(tcmu_cmd, base_command_size);
 +
 +	tcmu_hdr_set_len(&entry->hdr.len_op, command_size);
++=======
+ 	if (se_cmd->se_cmd_flags & SCF_BIDI) {
+ 		iov_cnt = 0;
+ 		iov++;
+ 		ret = alloc_and_scatter_data_area(udev, tcmu_cmd,
+ 					se_cmd->t_bidi_data_sg,
+ 					se_cmd->t_bidi_data_nents,
+ 					&iov, &iov_cnt, false);
+ 		if (ret) {
+ 			spin_unlock_irq(&udev->cmdr_lock);
+ 			pr_err("tcmu: alloc and scatter bidi data failed\n");
+ 			return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+ 		}
+ 		entry->req.iov_bidi_cnt = iov_cnt;
+ 	}
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
  	/* All offsets relative to mb_addr, not start of entry! */
  	cdb_off = CMDR_OFF + cmd_head + base_command_size;
@@@ -778,42 -691,39 +981,60 @@@ static void tcmu_handle_completion(stru
  	struct se_cmd *se_cmd = cmd->se_cmd;
  	struct tcmu_dev *udev = cmd->tcmu_dev;
  
- 	if (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags)) {
- 		/*
- 		 * cmd has been completed already from timeout, just reclaim
- 		 * data area space and free cmd
- 		 */
- 		free_data_area(udev, cmd);
+ 	/*
+ 	 * cmd has been completed already from timeout, just reclaim
+ 	 * data area space and free cmd
+ 	 */
+ 	if (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags))
+ 		goto out;
  
++<<<<<<< HEAD
 +		tcmu_free_cmd(cmd);
 +		return;
 +	}
++=======
+ 	tcmu_cmd_reset_dbi_cur(cmd);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
  	if (entry->hdr.uflags & TCMU_UFLAG_UNKNOWN_OP) {
- 		free_data_area(udev, cmd);
  		pr_warn("TCMU: Userspace set UNKNOWN_OP flag on se_cmd %p\n",
  			cmd->se_cmd);
  		entry->rsp.scsi_status = SAM_STAT_CHECK_CONDITION;
  	} else if (entry->rsp.scsi_status == SAM_STAT_CHECK_CONDITION) {
++<<<<<<< HEAD
 +		transport_copy_sense_to_cmd(se_cmd, entry->rsp.sense_buffer);
 +		free_data_area(udev, cmd);
 +	} else if (se_cmd->se_cmd_flags & SCF_BIDI) {
 +		/* Get Data-In buffer before clean up */
 +		gather_data_area(udev, cmd->data_bitmap,
 +			se_cmd->t_bidi_data_sg, se_cmd->t_bidi_data_nents);
 +	} else if (se_cmd->data_direction == DMA_FROM_DEVICE) {
 +		gather_data_area(udev, cmd->data_bitmap,
 +			se_cmd->t_data_sg, se_cmd->t_data_nents);
++=======
+ 		memcpy(se_cmd->sense_buffer, entry->rsp.sense_buffer,
+ 			       se_cmd->scsi_sense_length);
+ 	} else if (se_cmd->se_cmd_flags & SCF_BIDI) {
+ 		/* Get Data-In buffer before clean up */
+ 		gather_data_area(udev, cmd, true);
+ 	} else if (se_cmd->data_direction == DMA_FROM_DEVICE) {
+ 		gather_data_area(udev, cmd, false);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  	} else if (se_cmd->data_direction == DMA_TO_DEVICE) {
- 		free_data_area(udev, cmd);
+ 		/* TODO: */
  	} else if (se_cmd->data_direction != DMA_NONE) {
  		pr_warn("TCMU: data direction was %d!\n",
  			se_cmd->data_direction);
  	}
  
  	target_complete_cmd(cmd->se_cmd, entry->rsp.scsi_status);
- 	cmd->se_cmd = NULL;
  
++<<<<<<< HEAD
++=======
+ out:
+ 	cmd->se_cmd = NULL;
+ 	tcmu_cmd_free_data(cmd);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  	tcmu_free_cmd(cmd);
  }
  
@@@ -1211,13 -1094,7 +1491,17 @@@ static int tcmu_configure_device(struc
  
  	info->name = str;
  
++<<<<<<< HEAD
 +	udev->data_bitmap = kzalloc(BITS_TO_LONGS(udev->max_blocks) *
 +				    sizeof(unsigned long), GFP_KERNEL);
 +	if (!udev->data_bitmap)
 +		goto err_bitmap_alloc;
 +
 +	udev->ring_size = CMDR_SIZE + (udev->max_blocks * DATA_BLOCK_SIZE);
 +	udev->mb_addr = vzalloc(udev->ring_size);
++=======
+ 	udev->mb_addr = vzalloc(CMDR_SIZE);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  	if (!udev->mb_addr) {
  		ret = -ENOMEM;
  		goto err_vzalloc;
@@@ -1226,8 -1103,9 +1510,13 @@@
  	/* mailbox fits in first part of CMDR space */
  	udev->cmdr_size = CMDR_SIZE - CMDR_OFF;
  	udev->data_off = CMDR_SIZE;
++<<<<<<< HEAD
 +	udev->data_size = udev->ring_size - CMDR_SIZE;
++=======
+ 	udev->data_size = DATA_SIZE;
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
+ 	/* Initialise the mailbox of the ring buffer */
  	mb = udev->mb_addr;
  	mb->version = TCMU_MAILBOX_VERSION;
  	mb->flags = TCMU_MAILBOX_FLAG_CAP_OOOC;
@@@ -1242,8 -1122,8 +1533,13 @@@
  
  	info->mem[0].name = "tcm-user command & data buffer";
  	info->mem[0].addr = (phys_addr_t)(uintptr_t)udev->mb_addr;
++<<<<<<< HEAD
 +	info->mem[0].size = udev->ring_size;
 +	info->mem[0].memtype = UIO_MEM_VIRTUAL;
++=======
+ 	info->mem[0].size = TCMU_RING_SIZE;
+ 	info->mem[0].memtype = UIO_MEM_NONE;
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  
  	info->irqcontrol = tcmu_irqcontrol;
  	info->irq = UIO_IRQ_CUSTOM;
@@@ -1345,7 -1203,18 +1641,22 @@@ static void tcmu_destroy_device(struct 
  	idr_destroy(&udev->commands);
  	spin_unlock_irq(&udev->commands_lock);
  	WARN_ON(!all_expired);
++<<<<<<< HEAD
 +	kfree(udev->data_bitmap);
++=======
+ 
+ 	tcmu_blocks_release(udev, true);
+ 
+ 	if (tcmu_dev_configured(udev)) {
+ 		tcmu_netlink_event(TCMU_CMD_REMOVED_DEVICE, udev->uio_info.name,
+ 				   udev->uio_info.uio_dev->minor);
+ 
+ 		uio_unregister_device(&udev->uio_info);
+ 		kfree(udev->uio_info.name);
+ 		kfree(udev->name);
+ 	}
+ 	call_rcu(&dev->rcu_head, tcmu_dev_call_rcu);
++>>>>>>> 141685a39151 (tcmu: Add dynamic growing data area feature support)
  }
  
  enum {
* Unmerged path drivers/target/target_core_user.c
