dm crypt: optionally support larger encryption sector size

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Milan Broz <gmazyland@gmail.com>
commit 8f0009a225171cc1b76a6b443de5137b26e1374b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/8f0009a2.failed

Add  optional "sector_size"  parameter that specifies encryption sector
size (atomic unit of block device encryption).

Parameter can be in range 512 - 4096 bytes and must be power of two.
For compatibility reasons, the maximal IO must fit into the page limit,
so the limit is set to the minimal page size possible (4096 bytes).

NOTE: this device cannot yet be handled by cryptsetup if this parameter
is set.

IV for the sector is calculated from the 512 bytes sector offset unless
the iv_large_sectors option is used.

Test script using dmsetup:

  DEV="/dev/sdb"
  DEV_SIZE=$(blockdev --getsz $DEV)
  KEY="9c1185a5c5e9fc54612808977ee8f548b2258d31ddadef707ba62c166051b9e3cd0294c27515f2bccee924e8823ca6e124b8fc3167ed478bca702babe4e130ac"
  BLOCK_SIZE=4096

  # dmsetup create test_crypt --table "0 $DEV_SIZE crypt aes-xts-plain64 $KEY 0 $DEV 0 1 sector_size:$BLOCK_SIZE"
  # dmsetup table --showkeys test_crypt

	Signed-off-by: Milan Broz <gmazyland@gmail.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 8f0009a225171cc1b76a6b443de5137b26e1374b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/device-mapper/dm-crypt.txt
#	drivers/md/dm-crypt.c
diff --cc Documentation/device-mapper/dm-crypt.txt
index 692171fe9da0,3b3e1de21c9c..000000000000
--- a/Documentation/device-mapper/dm-crypt.txt
+++ b/Documentation/device-mapper/dm-crypt.txt
@@@ -76,6 -110,32 +76,35 @@@ submit_from_crypt_cpu
      thread because it benefits CFQ to have writes submitted using the
      same context.
  
++<<<<<<< HEAD
++=======
+ integrity:<bytes>:<type>
+     The device requires additional <bytes> metadata per-sector stored
+     in per-bio integrity structure. This metadata must by provided
+     by underlying dm-integrity target.
+ 
+     The <type> can be "none" if metadata is used only for persistent IV.
+ 
+     For Authenticated Encryption with Additional Data (AEAD)
+     the <type> is "aead". An AEAD mode additionally calculates and verifies
+     integrity for the encrypted device. The additional space is then
+     used for storing authentication tag (and persistent IV if needed).
+ 
+ sector_size:<bytes>
+     Use <bytes> as the encryption unit instead of 512 bytes sectors.
+     This option can be in range 512 - 4096 bytes and must be power of two.
+     Virtual device will announce this size as a minimal IO and logical sector.
+ 
+ iv_large_sectors
+    IV generators will use sector number counted in <sector_size> units
+    instead of default 512 bytes sectors.
+ 
+    For example, if <sector_size> is 4096 bytes, plain64 IV for the second
+    sector will be 8 (without flag) and 1 if iv_large_sectors is present.
+    The <iv_offset> must be multiple of <sector_size> (in 512 bytes units)
+    if this flag is specified.
+ 
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  Example scripts
  ===============
  LUKS (Linux Unified Key Setup) is now the preferred way to set up disk
diff --cc drivers/md/dm-crypt.c
index 91e9c1daa9b3,05acc42bdd38..000000000000
--- a/drivers/md/dm-crypt.c
+++ b/drivers/md/dm-crypt.c
@@@ -116,6 -127,11 +116,14 @@@ struct iv_tcw_private 
  enum flags { DM_CRYPT_SUSPENDED, DM_CRYPT_KEY_VALID,
  	     DM_CRYPT_SAME_CPU, DM_CRYPT_NO_OFFLOAD };
  
++<<<<<<< HEAD
++=======
+ enum cipher_flags {
+ 	CRYPT_MODE_INTEGRITY_AEAD,	/* Use authenticated mode for cihper */
+ 	CRYPT_IV_LARGE_SECTORS,		/* Calculate IV from sector_size, not 512B sectors */
+ };
+ 
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  /*
   * The fields in here must be read only after initialization.
   */
@@@ -838,61 -996,215 +857,195 @@@ static struct ablkcipher_request *req_o
  static u8 *iv_of_dmreq(struct crypt_config *cc,
  		       struct dm_crypt_request *dmreq)
  {
 -	if (crypt_integrity_aead(cc))
 -		return (u8 *)ALIGN((unsigned long)(dmreq + 1),
 -			crypto_aead_alignmask(any_tfm_aead(cc)) + 1);
 -	else
 -		return (u8 *)ALIGN((unsigned long)(dmreq + 1),
 -			crypto_skcipher_alignmask(any_tfm(cc)) + 1);
 -}
 -
 -static u8 *org_iv_of_dmreq(struct crypt_config *cc,
 -		       struct dm_crypt_request *dmreq)
 -{
 -	return iv_of_dmreq(cc, dmreq) + cc->iv_size;
 -}
 -
 -static uint64_t *org_sector_of_dmreq(struct crypt_config *cc,
 -		       struct dm_crypt_request *dmreq)
 -{
 -	u8 *ptr = iv_of_dmreq(cc, dmreq) + cc->iv_size + cc->iv_size;
 -	return (uint64_t*) ptr;
 -}
 -
 -static unsigned int *org_tag_of_dmreq(struct crypt_config *cc,
 -		       struct dm_crypt_request *dmreq)
 -{
 -	u8 *ptr = iv_of_dmreq(cc, dmreq) + cc->iv_size +
 -		  cc->iv_size + sizeof(uint64_t);
 -	return (unsigned int*)ptr;
 -}
 -
 -static void *tag_from_dmreq(struct crypt_config *cc,
 -				struct dm_crypt_request *dmreq)
 -{
 -	struct convert_context *ctx = dmreq->ctx;
 -	struct dm_crypt_io *io = container_of(ctx, struct dm_crypt_io, ctx);
 -
 -	return &io->integrity_metadata[*org_tag_of_dmreq(cc, dmreq) *
 -		cc->on_disk_tag_size];
 -}
 -
 -static void *iv_tag_from_dmreq(struct crypt_config *cc,
 -			       struct dm_crypt_request *dmreq)
 -{
 -	return tag_from_dmreq(cc, dmreq) + cc->integrity_tag_size;
 +	return (u8 *)ALIGN((unsigned long)(dmreq + 1),
 +		crypto_ablkcipher_alignmask(any_tfm(cc)) + 1);
  }
  
 -static int crypt_convert_block_aead(struct crypt_config *cc,
 -				     struct convert_context *ctx,
 -				     struct aead_request *req,
 -				     unsigned int tag_offset)
 +static int crypt_convert_block(struct crypt_config *cc,
 +			       struct convert_context *ctx,
 +			       struct ablkcipher_request *req)
  {
 -	struct bio_vec bv_in = bio_iter_iovec(ctx->bio_in, ctx->iter_in);
 -	struct bio_vec bv_out = bio_iter_iovec(ctx->bio_out, ctx->iter_out);
 +	struct bio_vec *bv_in = bio_iovec_idx(ctx->bio_in, ctx->idx_in);
 +	struct bio_vec *bv_out = bio_iovec_idx(ctx->bio_out, ctx->idx_out);
  	struct dm_crypt_request *dmreq;
++<<<<<<< HEAD
 +	u8 *iv;
 +	int r;
++=======
+ 	u8 *iv, *org_iv, *tag_iv, *tag;
+ 	uint64_t *sector;
+ 	int r = 0;
+ 
+ 	BUG_ON(cc->integrity_iv_size && cc->integrity_iv_size != cc->iv_size);
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
+ 
+ 	/* Reject unexpected unaligned bio. */
+ 	if (unlikely(bv_in.bv_offset & (cc->sector_size - 1)))
+ 		return -EIO;
  
  	dmreq = dmreq_of_req(cc, req);
 +	iv = iv_of_dmreq(cc, dmreq);
 +
  	dmreq->iv_sector = ctx->cc_sector;
+ 	if (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))
+ 		sector_div(dmreq->iv_sector, cc->sector_size >> SECTOR_SHIFT);
  	dmreq->ctx = ctx;
 +	sg_init_table(&dmreq->sg_in, 1);
 +	sg_set_page(&dmreq->sg_in, bv_in->bv_page, 1 << SECTOR_SHIFT,
 +		    bv_in->bv_offset + ctx->offset_in);
  
 -	*org_tag_of_dmreq(cc, dmreq) = tag_offset;
 +	sg_init_table(&dmreq->sg_out, 1);
 +	sg_set_page(&dmreq->sg_out, bv_out->bv_page, 1 << SECTOR_SHIFT,
 +		    bv_out->bv_offset + ctx->offset_out);
  
 -	sector = org_sector_of_dmreq(cc, dmreq);
 -	*sector = cpu_to_le64(ctx->cc_sector - cc->iv_offset);
 +	ctx->offset_in += 1 << SECTOR_SHIFT;
 +	if (ctx->offset_in >= bv_in->bv_len) {
 +		ctx->offset_in = 0;
 +		ctx->idx_in++;
 +	}
  
++<<<<<<< HEAD
 +	ctx->offset_out += 1 << SECTOR_SHIFT;
 +	if (ctx->offset_out >= bv_out->bv_len) {
 +		ctx->offset_out = 0;
 +		ctx->idx_out++;
 +	}
++=======
+ 	iv = iv_of_dmreq(cc, dmreq);
+ 	org_iv = org_iv_of_dmreq(cc, dmreq);
+ 	tag = tag_from_dmreq(cc, dmreq);
+ 	tag_iv = iv_tag_from_dmreq(cc, dmreq);
+ 
+ 	/* AEAD request:
+ 	 *  |----- AAD -------|------ DATA -------|-- AUTH TAG --|
+ 	 *  | (authenticated) | (auth+encryption) |              |
+ 	 *  | sector_LE |  IV |  sector in/out    |  tag in/out  |
+ 	 */
+ 	sg_init_table(dmreq->sg_in, 4);
+ 	sg_set_buf(&dmreq->sg_in[0], sector, sizeof(uint64_t));
+ 	sg_set_buf(&dmreq->sg_in[1], org_iv, cc->iv_size);
+ 	sg_set_page(&dmreq->sg_in[2], bv_in.bv_page, cc->sector_size, bv_in.bv_offset);
+ 	sg_set_buf(&dmreq->sg_in[3], tag, cc->integrity_tag_size);
+ 
+ 	sg_init_table(dmreq->sg_out, 4);
+ 	sg_set_buf(&dmreq->sg_out[0], sector, sizeof(uint64_t));
+ 	sg_set_buf(&dmreq->sg_out[1], org_iv, cc->iv_size);
+ 	sg_set_page(&dmreq->sg_out[2], bv_out.bv_page, cc->sector_size, bv_out.bv_offset);
+ 	sg_set_buf(&dmreq->sg_out[3], tag, cc->integrity_tag_size);
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  
  	if (cc->iv_gen_ops) {
 -		/* For READs use IV stored in integrity metadata */
 -		if (cc->integrity_iv_size && bio_data_dir(ctx->bio_in) != WRITE) {
 -			memcpy(org_iv, tag_iv, cc->iv_size);
 -		} else {
 -			r = cc->iv_gen_ops->generator(cc, org_iv, dmreq);
 -			if (r < 0)
 -				return r;
 -			/* Store generated IV in integrity metadata */
 -			if (cc->integrity_iv_size)
 -				memcpy(tag_iv, org_iv, cc->iv_size);
 -		}
 -		/* Working copy of IV, to be modified in crypto API */
 -		memcpy(iv, org_iv, cc->iv_size);
 +		r = cc->iv_gen_ops->generator(cc, iv, dmreq);
 +		if (r < 0)
 +			return r;
  	}
  
++<<<<<<< HEAD
 +	ablkcipher_request_set_crypt(req, &dmreq->sg_in, &dmreq->sg_out,
 +				     1 << SECTOR_SHIFT, iv);
++=======
+ 	aead_request_set_ad(req, sizeof(uint64_t) + cc->iv_size);
+ 	if (bio_data_dir(ctx->bio_in) == WRITE) {
+ 		aead_request_set_crypt(req, dmreq->sg_in, dmreq->sg_out,
+ 				       cc->sector_size, iv);
+ 		r = crypto_aead_encrypt(req);
+ 		if (cc->integrity_tag_size + cc->integrity_iv_size != cc->on_disk_tag_size)
+ 			memset(tag + cc->integrity_tag_size + cc->integrity_iv_size, 0,
+ 			       cc->on_disk_tag_size - (cc->integrity_tag_size + cc->integrity_iv_size));
+ 	} else {
+ 		aead_request_set_crypt(req, dmreq->sg_in, dmreq->sg_out,
+ 				       cc->sector_size + cc->integrity_tag_size, iv);
+ 		r = crypto_aead_decrypt(req);
+ 	}
+ 
+ 	if (r == -EBADMSG)
+ 		DMERR_LIMIT("INTEGRITY AEAD ERROR, sector %llu",
+ 			    (unsigned long long)le64_to_cpu(*sector));
+ 
+ 	if (!r && cc->iv_gen_ops && cc->iv_gen_ops->post)
+ 		r = cc->iv_gen_ops->post(cc, org_iv, dmreq);
+ 
+ 	bio_advance_iter(ctx->bio_in, &ctx->iter_in, cc->sector_size);
+ 	bio_advance_iter(ctx->bio_out, &ctx->iter_out, cc->sector_size);
+ 
+ 	return r;
+ }
+ 
+ static int crypt_convert_block_skcipher(struct crypt_config *cc,
+ 					struct convert_context *ctx,
+ 					struct skcipher_request *req,
+ 					unsigned int tag_offset)
+ {
+ 	struct bio_vec bv_in = bio_iter_iovec(ctx->bio_in, ctx->iter_in);
+ 	struct bio_vec bv_out = bio_iter_iovec(ctx->bio_out, ctx->iter_out);
+ 	struct scatterlist *sg_in, *sg_out;
+ 	struct dm_crypt_request *dmreq;
+ 	u8 *iv, *org_iv, *tag_iv;
+ 	uint64_t *sector;
+ 	int r = 0;
+ 
+ 	/* Reject unexpected unaligned bio. */
+ 	if (unlikely(bv_in.bv_offset & (cc->sector_size - 1)))
+ 		return -EIO;
+ 
+ 	dmreq = dmreq_of_req(cc, req);
+ 	dmreq->iv_sector = ctx->cc_sector;
+ 	if (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))
+ 		sector_div(dmreq->iv_sector, cc->sector_size >> SECTOR_SHIFT);
+ 	dmreq->ctx = ctx;
+ 
+ 	*org_tag_of_dmreq(cc, dmreq) = tag_offset;
+ 
+ 	iv = iv_of_dmreq(cc, dmreq);
+ 	org_iv = org_iv_of_dmreq(cc, dmreq);
+ 	tag_iv = iv_tag_from_dmreq(cc, dmreq);
+ 
+ 	sector = org_sector_of_dmreq(cc, dmreq);
+ 	*sector = cpu_to_le64(ctx->cc_sector - cc->iv_offset);
+ 
+ 	/* For skcipher we use only the first sg item */
+ 	sg_in  = &dmreq->sg_in[0];
+ 	sg_out = &dmreq->sg_out[0];
+ 
+ 	sg_init_table(sg_in, 1);
+ 	sg_set_page(sg_in, bv_in.bv_page, cc->sector_size, bv_in.bv_offset);
+ 
+ 	sg_init_table(sg_out, 1);
+ 	sg_set_page(sg_out, bv_out.bv_page, cc->sector_size, bv_out.bv_offset);
+ 
+ 	if (cc->iv_gen_ops) {
+ 		/* For READs use IV stored in integrity metadata */
+ 		if (cc->integrity_iv_size && bio_data_dir(ctx->bio_in) != WRITE) {
+ 			memcpy(org_iv, tag_iv, cc->integrity_iv_size);
+ 		} else {
+ 			r = cc->iv_gen_ops->generator(cc, org_iv, dmreq);
+ 			if (r < 0)
+ 				return r;
+ 			/* Store generated IV in integrity metadata */
+ 			if (cc->integrity_iv_size)
+ 				memcpy(tag_iv, org_iv, cc->integrity_iv_size);
+ 		}
+ 		/* Working copy of IV, to be modified in crypto API */
+ 		memcpy(iv, org_iv, cc->iv_size);
+ 	}
+ 
+ 	skcipher_request_set_crypt(req, sg_in, sg_out, cc->sector_size, iv);
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  
  	if (bio_data_dir(ctx->bio_in) == WRITE)
 -		r = crypto_skcipher_encrypt(req);
 +		r = crypto_ablkcipher_encrypt(req);
  	else
 -		r = crypto_skcipher_decrypt(req);
 +		r = crypto_ablkcipher_decrypt(req);
  
  	if (!r && cc->iv_gen_ops && cc->iv_gen_ops->post)
++<<<<<<< HEAD
 +		r = cc->iv_gen_ops->post(cc, iv, dmreq);
++=======
+ 		r = cc->iv_gen_ops->post(cc, org_iv, dmreq);
+ 
+ 	bio_advance_iter(ctx->bio_in, &ctx->iter_in, cc->sector_size);
+ 	bio_advance_iter(ctx->bio_out, &ctx->iter_out, cc->sector_size);
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  
  	return r;
  }
@@@ -929,41 -1289,61 +1082,56 @@@ static void crypt_free_req(struct crypt
  static int crypt_convert(struct crypt_config *cc,
  			 struct convert_context *ctx)
  {
++<<<<<<< HEAD
++=======
+ 	unsigned int tag_offset = 0;
+ 	unsigned int sector_step = cc->sector_size / (1 << SECTOR_SHIFT);
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  	int r;
  
  	atomic_set(&ctx->cc_pending, 1);
  
 -	while (ctx->iter_in.bi_size && ctx->iter_out.bi_size) {
 +	while(ctx->idx_in < ctx->bio_in->bi_vcnt &&
 +	      ctx->idx_out < ctx->bio_out->bi_vcnt) {
  
  		crypt_alloc_req(cc, ctx);
- 
  		atomic_inc(&ctx->cc_pending);
  
 -		if (crypt_integrity_aead(cc))
 -			r = crypt_convert_block_aead(cc, ctx, ctx->r.req_aead, tag_offset);
 -		else
 -			r = crypt_convert_block_skcipher(cc, ctx, ctx->r.req, tag_offset);
 +		r = crypt_convert_block(cc, ctx, ctx->req);
  
  		switch (r) {
 -		/*
 -		 * The request was queued by a crypto driver
 -		 * but the driver request queue is full, let's wait.
 -		 */
 +		/* async */
  		case -EBUSY:
  			wait_for_completion(&ctx->restart);
 -			reinit_completion(&ctx->restart);
 -			/* fall through */
 -		/*
 -		 * The request is queued and processed asynchronously,
 -		 * completion function kcryptd_async_done() will be called.
 -		 */
 +			INIT_COMPLETION(ctx->restart);
 +			/* fall through*/
  		case -EINPROGRESS:
++<<<<<<< HEAD
 +			ctx->req = NULL;
 +			ctx->cc_sector++;
++=======
+ 			ctx->r.req = NULL;
+ 			ctx->cc_sector += sector_step;
+ 			tag_offset += sector_step;
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  			continue;
 -		/*
 -		 * The request was already processed (synchronously).
 -		 */
 +
 +		/* sync */
  		case 0:
  			atomic_dec(&ctx->cc_pending);
++<<<<<<< HEAD
 +			ctx->cc_sector++;
++=======
+ 			ctx->cc_sector += sector_step;
+ 			tag_offset += sector_step;
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  			cond_resched();
  			continue;
 -		/*
 -		 * There was a data integrity error.
 -		 */
 -		case -EBADMSG:
 -			atomic_dec(&ctx->cc_pending);
 -			return -EILSEQ;
 -		/*
 -		 * There was an error while processing the request.
 -		 */
 +
 +		/* error */
  		default:
  			atomic_dec(&ctx->cc_pending);
 -			return -EIO;
 +			return r;
  		}
  	}
  
@@@ -1696,14 -2520,77 +1864,85 @@@ static int crypt_ctr_cipher(struct dm_t
  		}
  	}
  
 +	ret = 0;
 +bad:
 +	kfree(cipher_api);
  	return ret;
 -}
  
++<<<<<<< HEAD
 +bad_mem:
 +	ti->error = "Cannot allocate cipher strings";
 +	return -ENOMEM;
++=======
+ static int crypt_ctr_optional(struct dm_target *ti, unsigned int argc, char **argv)
+ {
+ 	struct crypt_config *cc = ti->private;
+ 	struct dm_arg_set as;
+ 	static struct dm_arg _args[] = {
+ 		{0, 6, "Invalid number of feature args"},
+ 	};
+ 	unsigned int opt_params, val;
+ 	const char *opt_string, *sval;
+ 	char dummy;
+ 	int ret;
+ 
+ 	/* Optional parameters */
+ 	as.argc = argc;
+ 	as.argv = argv;
+ 
+ 	ret = dm_read_arg_group(_args, &as, &opt_params, &ti->error);
+ 	if (ret)
+ 		return ret;
+ 
+ 	while (opt_params--) {
+ 		opt_string = dm_shift_arg(&as);
+ 		if (!opt_string) {
+ 			ti->error = "Not enough feature arguments";
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (!strcasecmp(opt_string, "allow_discards"))
+ 			ti->num_discard_bios = 1;
+ 
+ 		else if (!strcasecmp(opt_string, "same_cpu_crypt"))
+ 			set_bit(DM_CRYPT_SAME_CPU, &cc->flags);
+ 
+ 		else if (!strcasecmp(opt_string, "submit_from_crypt_cpus"))
+ 			set_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags);
+ 		else if (sscanf(opt_string, "integrity:%u:", &val) == 1) {
+ 			if (val == 0 || val > MAX_TAG_SIZE) {
+ 				ti->error = "Invalid integrity arguments";
+ 				return -EINVAL;
+ 			}
+ 			cc->on_disk_tag_size = val;
+ 			sval = strchr(opt_string + strlen("integrity:"), ':') + 1;
+ 			if (!strcasecmp(sval, "aead")) {
+ 				set_bit(CRYPT_MODE_INTEGRITY_AEAD, &cc->cipher_flags);
+ 			} else  if (strcasecmp(sval, "none")) {
+ 				ti->error = "Unknown integrity profile";
+ 				return -EINVAL;
+ 			}
+ 
+ 			cc->cipher_auth = kstrdup(sval, GFP_KERNEL);
+ 			if (!cc->cipher_auth)
+ 				return -ENOMEM;
+ 		} else if (sscanf(opt_string, "sector_size:%u%c", &cc->sector_size, &dummy) == 1) {
+ 			if (cc->sector_size < (1 << SECTOR_SHIFT) ||
+ 			    cc->sector_size > 4096 ||
+ 			    (1 << ilog2(cc->sector_size) != cc->sector_size)) {
+ 				ti->error = "Invalid feature value for sector_size";
+ 				return -EINVAL;
+ 			}
+ 		} else if (!strcasecmp(opt_string, "iv_large_sectors"))
+ 			set_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags);
+ 		else {
+ 			ti->error = "Invalid feature arguments";
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	return 0;
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  }
  
  /*
@@@ -1738,8 -2624,17 +1977,9 @@@ static int crypt_ctr(struct dm_target *
  		return -ENOMEM;
  	}
  	cc->key_size = key_size;
+ 	cc->sector_size = (1 << SECTOR_SHIFT);
  
  	ti->private = cc;
 -
 -	/* Optional parameters need to be read before cipher constructor */
 -	if (argc > 5) {
 -		ret = crypt_ctr_optional(ti, argc - 5, &argv[5]);
 -		if (ret)
 -			goto bad;
 -	}
 -
  	ret = crypt_ctr_cipher(ti, argv[0], argv[1]);
  	if (ret < 0)
  		goto bad;
@@@ -1901,9 -2792,44 +2142,50 @@@ static int crypt_map(struct dm_target *
  		return DM_MAPIO_REMAPPED;
  	}
  
++<<<<<<< HEAD
 +	io = dm_per_bio_data(bio, cc->per_bio_data_size);
 +	crypt_io_init(io, cc, bio, dm_target_offset(ti, bio->bi_sector));
 +	io->ctx.req = (struct ablkcipher_request *)(io + 1);
++=======
+ 	/*
+ 	 * Check if bio is too large, split as needed.
+ 	 */
+ 	if (unlikely(bio->bi_iter.bi_size > (BIO_MAX_PAGES << PAGE_SHIFT)) &&
+ 	    (bio_data_dir(bio) == WRITE || cc->on_disk_tag_size))
+ 		dm_accept_partial_bio(bio, ((BIO_MAX_PAGES << PAGE_SHIFT) >> SECTOR_SHIFT));
+ 
+ 	/*
+ 	 * Ensure that bio is a multiple of internal sector encryption size
+ 	 * and is aligned to this size as defined in IO hints.
+ 	 */
+ 	if (unlikely((bio->bi_iter.bi_sector & ((cc->sector_size >> SECTOR_SHIFT) - 1)) != 0))
+ 		return -EIO;
+ 
+ 	if (unlikely(bio->bi_iter.bi_size & (cc->sector_size - 1)))
+ 		return -EIO;
+ 
+ 	io = dm_per_bio_data(bio, cc->per_bio_data_size);
+ 	crypt_io_init(io, cc, bio, dm_target_offset(ti, bio->bi_iter.bi_sector));
+ 
+ 	if (cc->on_disk_tag_size) {
+ 		unsigned tag_len = cc->on_disk_tag_size * bio_sectors(bio);
+ 
+ 		if (unlikely(tag_len > KMALLOC_MAX_SIZE) ||
+ 		    unlikely(!(io->integrity_metadata = kzalloc(tag_len,
+ 				GFP_NOIO | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN)))) {
+ 			if (bio_sectors(bio) > cc->tag_pool_max_sectors)
+ 				dm_accept_partial_bio(bio, cc->tag_pool_max_sectors);
+ 			io->integrity_metadata = mempool_alloc(cc->tag_pool, GFP_NOIO);
+ 			io->integrity_metadata_from_pool = true;
+ 			memset(io->integrity_metadata, 0, cc->tag_pool_max_sectors * (1 << SECTOR_SHIFT));
+ 		}
+ 	}
+ 
+ 	if (crypt_integrity_aead(cc))
+ 		io->ctx.r.req_aead = (struct aead_request *)(io + 1);
+ 	else
+ 		io->ctx.r.req = (struct skcipher_request *)(io + 1);
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  
  	if (bio_data_dir(io->base_bio) == READ) {
  		if (kcryptd_io_read(io, GFP_NOWAIT))
@@@ -1941,6 -2870,10 +2223,13 @@@ static void crypt_status(struct dm_targ
  		num_feature_args += !!ti->num_discard_bios;
  		num_feature_args += test_bit(DM_CRYPT_SAME_CPU, &cc->flags);
  		num_feature_args += test_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags);
++<<<<<<< HEAD
++=======
+ 		num_feature_args += (cc->sector_size != (1 << SECTOR_SHIFT)) ? 1 : 0;
+ 		num_feature_args += test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags);
+ 		if (cc->on_disk_tag_size)
+ 			num_feature_args++;
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  		if (num_feature_args) {
  			DMEMIT(" %d", num_feature_args);
  			if (ti->num_discard_bios)
@@@ -1949,6 -2882,12 +2238,15 @@@
  				DMEMIT(" same_cpu_crypt");
  			if (test_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags))
  				DMEMIT(" submit_from_crypt_cpus");
++<<<<<<< HEAD
++=======
+ 			if (cc->on_disk_tag_size)
+ 				DMEMIT(" integrity:%u:%s", cc->on_disk_tag_size, cc->cipher_auth);
+ 			if (cc->sector_size != (1 << SECTOR_SHIFT))
+ 				DMEMIT(" sector_size:%d", cc->sector_size);
+ 			if (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))
+ 				DMEMIT(" iv_large_sectors");
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  		}
  
  		break;
@@@ -2057,7 -2996,7 +2363,11 @@@ static void crypt_io_hints(struct dm_ta
  
  static struct target_type crypt_target = {
  	.name   = "crypt",
++<<<<<<< HEAD
 +	.version = {1, 14, 1},
++=======
+ 	.version = {1, 17, 0},
++>>>>>>> 8f0009a22517 (dm crypt: optionally support larger encryption sector size)
  	.module = THIS_MODULE,
  	.ctr    = crypt_ctr,
  	.dtr    = crypt_dtr,
* Unmerged path Documentation/device-mapper/dm-crypt.txt
* Unmerged path drivers/md/dm-crypt.c
