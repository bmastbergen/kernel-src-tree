x86/uaccess: Enable hardened usercopy

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [s390] uaccess: Enable hardened usercopy (Christoph von Recklinghausen) [1562140]
Rebuild_FUZZ: 94.29%
commit-author Kees Cook <keescook@chromium.org>
commit 5b710f34e194c6b7710f69fdb5d798fdf35b98c1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/5b710f34.failed

Enables CONFIG_HARDENED_USERCOPY checks on x86. This is done both in
copy_*_user() and __copy_*_user() because copy_*_user() actually calls
down to _copy_*_user() and not __copy_*_user().

Based on code from PaX and grsecurity.

	Signed-off-by: Kees Cook <keescook@chromium.org>
	Tested-by: Valdis Kletnieks <valdis.kletnieks@vt.edu>
(cherry picked from commit 5b710f34e194c6b7710f69fdb5d798fdf35b98c1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/include/asm/uaccess.h
#	arch/x86/include/asm/uaccess_32.h
#	arch/x86/include/asm/uaccess_64.h
diff --cc arch/x86/Kconfig
index 48ae09959d87,9640942b68b9..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -22,59 -17,127 +22,146 @@@ config X86_6
  ### Arch settings
  config X86
  	def_bool y
 -	select ACPI_LEGACY_TABLES_LOOKUP	if ACPI
 -	select ACPI_SYSTEM_POWER_STATES_SUPPORT	if ACPI
 -	select ANON_INODES
 -	select ARCH_CLOCKSOURCE_DATA
 -	select ARCH_DISCARD_MEMBLOCK
 -	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
  	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
 -	select ARCH_HAS_DEVMEM_IS_ALLOWED
 -	select ARCH_HAS_ELF_RANDOMIZE
 -	select ARCH_HAS_FAST_MULTIPLIER
 -	select ARCH_HAS_GCOV_PROFILE_ALL
 -	select ARCH_HAS_KCOV			if X86_64
  	select ARCH_HAS_PMEM_API		if X86_64
 +	select ARCH_HAS_UACCESS_FLUSHCACHE	if X86_64
  	select ARCH_HAS_MMIO_FLUSH
++<<<<<<< HEAD
 +	select HAVE_AOUT if X86_32
 +	select HAVE_UNSTABLE_SCHED_CLOCK
 +	select ARCH_SUPPORTS_NUMA_BALANCING
 +	select ARCH_SUPPORTS_INT128 if X86_64
 +	select ARCH_WANTS_PROT_NUMA_PROT_NONE
++=======
+ 	select ARCH_HAS_SG_CHAIN
+ 	select ARCH_HAS_UBSAN_SANITIZE_ALL
+ 	select ARCH_HAVE_NMI_SAFE_CMPXCHG
+ 	select ARCH_MIGHT_HAVE_ACPI_PDC		if ACPI
+ 	select ARCH_MIGHT_HAVE_PC_PARPORT
+ 	select ARCH_MIGHT_HAVE_PC_SERIO
+ 	select ARCH_SUPPORTS_ATOMIC_RMW
+ 	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	select ARCH_SUPPORTS_INT128		if X86_64
+ 	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
+ 	select ARCH_USE_BUILTIN_BSWAP
+ 	select ARCH_USE_CMPXCHG_LOCKREF		if X86_64
+ 	select ARCH_USE_QUEUED_RWLOCKS
+ 	select ARCH_USE_QUEUED_SPINLOCKS
+ 	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
+ 	select ARCH_WANTS_DYNAMIC_TASK_STRUCT
+ 	select ARCH_WANT_FRAME_POINTERS
+ 	select ARCH_WANT_IPC_PARSE_VERSION	if X86_32
+ 	select ARCH_WANT_OPTIONAL_GPIOLIB
+ 	select BUILDTIME_EXTABLE_SORT
+ 	select CLKEVT_I8253
+ 	select CLKSRC_I8253			if X86_32
+ 	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
+ 	select CLOCKSOURCE_WATCHDOG
+ 	select CLONE_BACKWARDS			if X86_32
+ 	select COMPAT_OLD_SIGACTION		if IA32_EMULATION
+ 	select DCACHE_WORD_ACCESS
+ 	select EDAC_ATOMIC_SCRUB
+ 	select EDAC_SUPPORT
+ 	select GENERIC_CLOCKEVENTS
+ 	select GENERIC_CLOCKEVENTS_BROADCAST	if X86_64 || (X86_32 && X86_LOCAL_APIC)
+ 	select GENERIC_CLOCKEVENTS_MIN_ADJUST
+ 	select GENERIC_CMOS_UPDATE
+ 	select GENERIC_CPU_AUTOPROBE
+ 	select GENERIC_EARLY_IOREMAP
+ 	select GENERIC_FIND_FIRST_BIT
+ 	select GENERIC_IOMAP
+ 	select GENERIC_IRQ_PROBE
+ 	select GENERIC_IRQ_SHOW
+ 	select GENERIC_PENDING_IRQ		if SMP
+ 	select GENERIC_SMP_IDLE_THREAD
+ 	select GENERIC_STRNCPY_FROM_USER
+ 	select GENERIC_STRNLEN_USER
+ 	select GENERIC_TIME_VSYSCALL
+ 	select HAVE_ACPI_APEI			if ACPI
+ 	select HAVE_ACPI_APEI_NMI		if ACPI
+ 	select HAVE_ALIGNED_STRUCT_PAGE		if SLUB
+ 	select HAVE_AOUT			if X86_32
+ 	select HAVE_ARCH_AUDITSYSCALL
+ 	select HAVE_ARCH_HARDENED_USERCOPY
+ 	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
+ 	select HAVE_ARCH_JUMP_LABEL
+ 	select HAVE_ARCH_KASAN			if X86_64 && SPARSEMEM_VMEMMAP
+ 	select HAVE_ARCH_KGDB
+ 	select HAVE_ARCH_KMEMCHECK
+ 	select HAVE_ARCH_MMAP_RND_BITS		if MMU
+ 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
+ 	select HAVE_ARCH_SECCOMP_FILTER
+ 	select HAVE_ARCH_SOFT_DIRTY		if X86_64
+ 	select HAVE_ARCH_TRACEHOOK
+ 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
+ 	select HAVE_ARCH_WITHIN_STACK_FRAMES
+ 	select HAVE_EBPF_JIT			if X86_64
+ 	select HAVE_CC_STACKPROTECTOR
+ 	select HAVE_CMPXCHG_DOUBLE
+ 	select HAVE_CMPXCHG_LOCAL
+ 	select HAVE_CONTEXT_TRACKING		if X86_64
+ 	select HAVE_COPY_THREAD_TLS
+ 	select HAVE_C_RECORDMCOUNT
+ 	select HAVE_DEBUG_KMEMLEAK
+ 	select HAVE_DEBUG_STACKOVERFLOW
+ 	select HAVE_DMA_API_DEBUG
+ 	select HAVE_DMA_CONTIGUOUS
+ 	select HAVE_DYNAMIC_FTRACE
+ 	select HAVE_DYNAMIC_FTRACE_WITH_REGS
+ 	select HAVE_EFFICIENT_UNALIGNED_ACCESS
+ 	select HAVE_EXIT_THREAD
+ 	select HAVE_FENTRY			if X86_64
+ 	select HAVE_FTRACE_MCOUNT_RECORD
+ 	select HAVE_FUNCTION_GRAPH_FP_TEST
+ 	select HAVE_FUNCTION_GRAPH_TRACER
+ 	select HAVE_FUNCTION_TRACER
+ 	select HAVE_GENERIC_DMA_COHERENT	if X86_32
+ 	select HAVE_HW_BREAKPOINT
++>>>>>>> 5b710f34e194 (x86/uaccess: Enable hardened usercopy)
  	select HAVE_IDE
 +	select HAVE_OPROFILE
 +	select HAVE_PCSPKR_PLATFORM
 +	select HAVE_PERF_EVENTS
  	select HAVE_IOREMAP_PROT
 -	select HAVE_IRQ_EXIT_ON_IRQ_STACK	if X86_64
 -	select HAVE_IRQ_TIME_ACCOUNTING
 -	select HAVE_KERNEL_BZIP2
 -	select HAVE_KERNEL_GZIP
 -	select HAVE_KERNEL_LZ4
 -	select HAVE_KERNEL_LZMA
 -	select HAVE_KERNEL_LZO
 -	select HAVE_KERNEL_XZ
  	select HAVE_KPROBES
 -	select HAVE_KPROBES_ON_FTRACE
 -	select HAVE_KRETPROBES
 -	select HAVE_KVM
 -	select HAVE_LIVEPATCH			if X86_64
  	select HAVE_MEMBLOCK
  	select HAVE_MEMBLOCK_NODE_MAP
 -	select HAVE_MIXED_BREAKPOINTS_REGS
 -	select HAVE_NMI
 -	select HAVE_OPROFILE
 +	select ARCH_DISCARD_MEMBLOCK
 +	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 +	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
 +	select ARCH_WANT_OPTIONAL_GPIOLIB
 +	select ARCH_WANT_FRAME_POINTERS
 +	select HAVE_DMA_CONTIGUOUS if !SWIOTLB
 +	select HAVE_KRETPROBES
  	select HAVE_OPTPROBES
 -	select HAVE_PCSPKR_PLATFORM
 -	select HAVE_PERF_EVENTS
 +	select HAVE_KPROBES_ON_FTRACE
 +	select HAVE_FTRACE_MCOUNT_RECORD
 +	select HAVE_FENTRY if X86_64
 +	select HAVE_ARCH_MMAP_RND_BITS		if MMU
 +	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
 +	select HAVE_C_RECORDMCOUNT
 +	select HAVE_DYNAMIC_FTRACE
 +	select HAVE_DYNAMIC_FTRACE_WITH_REGS
 +	select HAVE_FUNCTION_TRACER
 +	select HAVE_FUNCTION_GRAPH_TRACER
 +	select HAVE_SYSCALL_TRACEPOINTS
 +	select SYSCTL_EXCEPTION_TRACE
 +	select HAVE_KVM
 +	select HAVE_ARCH_KGDB
 +	select HAVE_ARCH_TRACEHOOK
 +	select HAVE_GENERIC_DMA_COHERENT if X86_32
 +	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 +	select USER_STACKTRACE_SUPPORT
 +	select HAVE_REGS_AND_STACK_ACCESS_API
 +	select HAVE_DMA_API_DEBUG
 +	select HAVE_KERNEL_GZIP
 +	select HAVE_KERNEL_BZIP2
 +	select HAVE_KERNEL_LZMA
 +	select HAVE_KERNEL_XZ
 +	select HAVE_KERNEL_LZO
 +	select HAVE_HW_BREAKPOINT
 +	select HAVE_MIXED_BREAKPOINTS_REGS
 +	select PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
diff --cc arch/x86/include/asm/uaccess.h
index 46d1d9482490,d3312f0fcdfc..000000000000
--- a/arch/x86/include/asm/uaccess.h
+++ b/arch/x86/include/asm/uaccess.h
@@@ -642,5 -673,143 +642,146 @@@ extern struct movsl_mask 
  # include <asm/uaccess_64.h>
  #endif
  
++<<<<<<< HEAD
++=======
+ unsigned long __must_check _copy_from_user(void *to, const void __user *from,
+ 					   unsigned n);
+ unsigned long __must_check _copy_to_user(void __user *to, const void *from,
+ 					 unsigned n);
+ 
+ #ifdef CONFIG_DEBUG_STRICT_USER_COPY_CHECKS
+ # define copy_user_diag __compiletime_error
+ #else
+ # define copy_user_diag __compiletime_warning
+ #endif
+ 
+ extern void copy_user_diag("copy_from_user() buffer size is too small")
+ copy_from_user_overflow(void);
+ extern void copy_user_diag("copy_to_user() buffer size is too small")
+ copy_to_user_overflow(void) __asm__("copy_from_user_overflow");
+ 
+ #undef copy_user_diag
+ 
+ #ifdef CONFIG_DEBUG_STRICT_USER_COPY_CHECKS
+ 
+ extern void
+ __compiletime_warning("copy_from_user() buffer size is not provably correct")
+ __copy_from_user_overflow(void) __asm__("copy_from_user_overflow");
+ #define __copy_from_user_overflow(size, count) __copy_from_user_overflow()
+ 
+ extern void
+ __compiletime_warning("copy_to_user() buffer size is not provably correct")
+ __copy_to_user_overflow(void) __asm__("copy_from_user_overflow");
+ #define __copy_to_user_overflow(size, count) __copy_to_user_overflow()
+ 
+ #else
+ 
+ static inline void
+ __copy_from_user_overflow(int size, unsigned long count)
+ {
+ 	WARN(1, "Buffer overflow detected (%d < %lu)!\n", size, count);
+ }
+ 
+ #define __copy_to_user_overflow __copy_from_user_overflow
+ 
+ #endif
+ 
+ static inline unsigned long __must_check
+ copy_from_user(void *to, const void __user *from, unsigned long n)
+ {
+ 	int sz = __compiletime_object_size(to);
+ 
+ 	might_fault();
+ 
+ 	kasan_check_write(to, n);
+ 
+ 	/*
+ 	 * While we would like to have the compiler do the checking for us
+ 	 * even in the non-constant size case, any false positives there are
+ 	 * a problem (especially when DEBUG_STRICT_USER_COPY_CHECKS, but even
+ 	 * without - the [hopefully] dangerous looking nature of the warning
+ 	 * would make people go look at the respecitive call sites over and
+ 	 * over again just to find that there's no problem).
+ 	 *
+ 	 * And there are cases where it's just not realistic for the compiler
+ 	 * to prove the count to be in range. For example when multiple call
+ 	 * sites of a helper function - perhaps in different source files -
+ 	 * all doing proper range checking, yet the helper function not doing
+ 	 * so again.
+ 	 *
+ 	 * Therefore limit the compile time checking to the constant size
+ 	 * case, and do only runtime checking for non-constant sizes.
+ 	 */
+ 
+ 	if (likely(sz < 0 || sz >= n)) {
+ 		check_object_size(to, n, false);
+ 		n = _copy_from_user(to, from, n);
+ 	} else if (__builtin_constant_p(n))
+ 		copy_from_user_overflow();
+ 	else
+ 		__copy_from_user_overflow(sz, n);
+ 
+ 	return n;
+ }
+ 
+ static inline unsigned long __must_check
+ copy_to_user(void __user *to, const void *from, unsigned long n)
+ {
+ 	int sz = __compiletime_object_size(from);
+ 
+ 	kasan_check_read(from, n);
+ 
+ 	might_fault();
+ 
+ 	/* See the comment in copy_from_user() above. */
+ 	if (likely(sz < 0 || sz >= n)) {
+ 		check_object_size(from, n, true);
+ 		n = _copy_to_user(to, from, n);
+ 	} else if (__builtin_constant_p(n))
+ 		copy_to_user_overflow();
+ 	else
+ 		__copy_to_user_overflow(sz, n);
+ 
+ 	return n;
+ }
+ 
+ #undef __copy_from_user_overflow
+ #undef __copy_to_user_overflow
+ 
+ /*
+  * We rely on the nested NMI work to allow atomic faults from the NMI path; the
+  * nested NMI paths are careful to preserve CR2.
+  *
+  * Caller must use pagefault_enable/disable, or run in interrupt context,
+  * and also do a uaccess_ok() check
+  */
+ #define __copy_from_user_nmi __copy_from_user_inatomic
+ 
+ /*
+  * The "unsafe" user accesses aren't really "unsafe", but the naming
+  * is a big fat warning: you have to not only do the access_ok()
+  * checking before using them, but you have to surround them with the
+  * user_access_begin/end() pair.
+  */
+ #define user_access_begin()	__uaccess_begin()
+ #define user_access_end()	__uaccess_end()
+ 
+ #define unsafe_put_user(x, ptr)						\
+ ({										\
+ 	int __pu_err;								\
+ 	__put_user_size((x), (ptr), sizeof(*(ptr)), __pu_err, -EFAULT);		\
+ 	__builtin_expect(__pu_err, 0);						\
+ })
+ 
+ #define unsafe_get_user(x, ptr)						\
+ ({										\
+ 	int __gu_err;								\
+ 	unsigned long __gu_val;							\
+ 	__get_user_size(__gu_val, (ptr), sizeof(*(ptr)), __gu_err, -EFAULT);	\
+ 	(x) = (__force __typeof__(*(ptr)))__gu_val;				\
+ 	__builtin_expect(__gu_err, 0);						\
+ })
+ 
++>>>>>>> 5b710f34e194 (x86/uaccess: Enable hardened usercopy)
  #endif /* _ASM_X86_UACCESS_H */
  
diff --cc arch/x86/include/asm/uaccess_32.h
index 7f760a9f1f61,7d3bdd1ed697..000000000000
--- a/arch/x86/include/asm/uaccess_32.h
+++ b/arch/x86/include/asm/uaccess_32.h
@@@ -43,24 -37,7 +43,28 @@@ unsigned long __must_check __copy_from_
  static __always_inline unsigned long __must_check
  __copy_to_user_inatomic(void __user *to, const void *from, unsigned long n)
  {
++<<<<<<< HEAD
 +	if (__builtin_constant_p(n)) {
 +		unsigned long ret;
 +
 +		switch (n) {
 +		case 1:
 +			__put_user_size(*(u8 *)from, (u8 __user *)to,
 +					1, ret, 1);
 +			return ret;
 +		case 2:
 +			__put_user_size(*(u16 *)from, (u16 __user *)to,
 +					2, ret, 2);
 +			return ret;
 +		case 4:
 +			__put_user_size(*(u32 *)from, (u32 __user *)to,
 +					4, ret, 4);
 +			return ret;
 +		}
 +	}
++=======
+ 	check_object_size(from, n, true);
++>>>>>>> 5b710f34e194 (x86/uaccess: Enable hardened usercopy)
  	return __copy_to_user_ll(to, from, n);
  }
  
diff --cc arch/x86/include/asm/uaccess_64.h
index 109207779f98,673059a109fe..000000000000
--- a/arch/x86/include/asm/uaccess_64.h
+++ b/arch/x86/include/asm/uaccess_64.h
@@@ -52,72 -49,70 +52,130 @@@ _copy_from_user(void *to, const void __
  __must_check unsigned long
  copy_in_user(void __user *to, const void __user *from, unsigned len);
  
 +static inline unsigned long __must_check copy_from_user(void *to,
 +					  const void __user *from,
 +					  unsigned long n)
 +{
 +	int sz = __compiletime_object_size(to);
 +
++<<<<<<< HEAD
 +	might_fault();
 +	if (likely(sz == -1 || sz >= n))
 +		n = _copy_from_user(to, from, n);
 +#ifdef CONFIG_DEBUG_VM
 +	else
 +		WARN(1, "Buffer overflow detected!\n");
 +#endif
 +	return n;
 +}
 +
  static __always_inline __must_check
 -int __copy_from_user_nocheck(void *dst, const void __user *src, unsigned size)
 +int copy_to_user(void __user *dst, const void *src, unsigned size)
  {
 -	int ret = 0;
 +	might_fault();
  
 +	return _copy_to_user(dst, src, size);
++=======
+ 	check_object_size(dst, size, false);
+ 	if (!__builtin_constant_p(size))
+ 		return copy_user_generic(dst, (__force void *)src, size);
+ 	switch (size) {
+ 	case 1:
+ 		__uaccess_begin();
+ 		__get_user_asm(*(u8 *)dst, (u8 __user *)src,
+ 			      ret, "b", "b", "=q", 1);
+ 		__uaccess_end();
+ 		return ret;
+ 	case 2:
+ 		__uaccess_begin();
+ 		__get_user_asm(*(u16 *)dst, (u16 __user *)src,
+ 			      ret, "w", "w", "=r", 2);
+ 		__uaccess_end();
+ 		return ret;
+ 	case 4:
+ 		__uaccess_begin();
+ 		__get_user_asm(*(u32 *)dst, (u32 __user *)src,
+ 			      ret, "l", "k", "=r", 4);
+ 		__uaccess_end();
+ 		return ret;
+ 	case 8:
+ 		__uaccess_begin();
+ 		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
+ 			      ret, "q", "", "=r", 8);
+ 		__uaccess_end();
+ 		return ret;
+ 	case 10:
+ 		__uaccess_begin();
+ 		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
+ 			       ret, "q", "", "=r", 10);
+ 		if (likely(!ret))
+ 			__get_user_asm(*(u16 *)(8 + (char *)dst),
+ 				       (u16 __user *)(8 + (char __user *)src),
+ 				       ret, "w", "w", "=r", 2);
+ 		__uaccess_end();
+ 		return ret;
+ 	case 16:
+ 		__uaccess_begin();
+ 		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
+ 			       ret, "q", "", "=r", 16);
+ 		if (likely(!ret))
+ 			__get_user_asm(*(u64 *)(8 + (char *)dst),
+ 				       (u64 __user *)(8 + (char __user *)src),
+ 				       ret, "q", "", "=r", 8);
+ 		__uaccess_end();
+ 		return ret;
+ 	default:
+ 		return copy_user_generic(dst, (__force void *)src, size);
+ 	}
++>>>>>>> 5b710f34e194 (x86/uaccess: Enable hardened usercopy)
  }
  
  static __always_inline __must_check
  int __copy_from_user(void *dst, const void __user *src, unsigned size)
  {
 +	int ret = 0;
 +
++<<<<<<< HEAD
  	might_fault();
 -	kasan_check_write(dst, size);
 -	return __copy_from_user_nocheck(dst, src, size);
++=======
++	check_object_size(src, size, true);
++>>>>>>> 5b710f34e194 (x86/uaccess: Enable hardened usercopy)
 +	if (!__builtin_constant_p(size))
 +		return copy_user_generic(dst, (__force void *)src, size);
 +	switch (size) {
 +	case 1:__get_user_asm(*(u8 *)dst, (u8 __user *)src,
 +			      ret, "b", "b", "=q", 1);
 +		return ret;
 +	case 2:__get_user_asm(*(u16 *)dst, (u16 __user *)src,
 +			      ret, "w", "w", "=r", 2);
 +		return ret;
 +	case 4:__get_user_asm(*(u32 *)dst, (u32 __user *)src,
 +			      ret, "l", "k", "=r", 4);
 +		return ret;
 +	case 8:__get_user_asm(*(u64 *)dst, (u64 __user *)src,
 +			      ret, "q", "", "=r", 8);
 +		return ret;
 +	case 10:
 +		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
 +			       ret, "q", "", "=r", 10);
 +		if (unlikely(ret))
 +			return ret;
 +		__get_user_asm(*(u16 *)(8 + (char *)dst),
 +			       (u16 __user *)(8 + (char __user *)src),
 +			       ret, "w", "w", "=r", 2);
 +		return ret;
 +	case 16:
 +		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
 +			       ret, "q", "", "=r", 16);
 +		if (unlikely(ret))
 +			return ret;
 +		__get_user_asm(*(u64 *)(8 + (char *)dst),
 +			       (u64 __user *)(8 + (char __user *)src),
 +			       ret, "q", "", "=r", 8);
 +		return ret;
 +	default:
 +		return copy_user_generic(dst, (__force void *)src, size);
 +	}
  }
  
  static __always_inline __must_check
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/include/asm/uaccess.h
* Unmerged path arch/x86/include/asm/uaccess_32.h
* Unmerged path arch/x86/include/asm/uaccess_64.h
