net: bpf: make eBPF interpreter images read-only

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [kernel] bpf: make eBPF interpreter images read-only (Jiri Olsa) [1311586]
Rebuild_FUZZ: 94.51%
commit-author Daniel Borkmann <dborkman@redhat.com>
commit 60a3b2253c413cf601783b070507d7dd6620c954
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/60a3b225.failed

With eBPF getting more extended and exposure to user space is on it's way,
hardening the memory range the interpreter uses to steer its command flow
seems appropriate.  This patch moves the to be interpreted bytecode to
read-only pages.

In case we execute a corrupted BPF interpreter image for some reason e.g.
caused by an attacker which got past a verifier stage, it would not only
provide arbitrary read/write memory access but arbitrary function calls
as well. After setting up the BPF interpreter image, its contents do not
change until destruction time, thus we can setup the image on immutable
made pages in order to mitigate modifications to that code. The idea
is derived from commit 314beb9bcabf ("x86: bpf_jit_comp: secure bpf jit
against spraying attacks").

This is possible because bpf_prog is not part of sk_filter anymore.
After setup bpf_prog cannot be altered during its life-time. This prevents
any modifications to the entire bpf_prog structure (incl. function/JIT
image pointer).

Every eBPF program (including classic BPF that are migrated) have to call
bpf_prog_select_runtime() to select either interpreter or a JIT image
as a last setup step, and they all are being freed via bpf_prog_free(),
including non-JIT. Therefore, we can easily integrate this into the
eBPF life-time, plus since we directly allocate a bpf_prog, we have no
performance penalty.

Tested with seccomp and test_bpf testsuite in JIT/non-JIT mode and manual
inspection of kernel_page_tables.  Brad Spengler proposed the same idea
via Twitter during development of this patch.

Joint work with Hannes Frederic Sowa.

	Suggested-by: Brad Spengler <spender@grsecurity.net>
	Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
	Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Cc: Alexei Starovoitov <ast@plumgrid.com>
	Cc: Kees Cook <keescook@chromium.org>
	Acked-by: Alexei Starovoitov <ast@plumgrid.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 60a3b2253c413cf601783b070507d7dd6620c954)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/net/bpf_jit_32.c
#	arch/mips/net/bpf_jit.c
#	arch/powerpc/net/bpf_jit_comp.c
#	arch/s390/net/bpf_jit_comp.c
#	arch/sparc/net/bpf_jit_comp.c
#	arch/x86/net/bpf_jit_comp.c
#	include/linux/filter.h
#	kernel/bpf/core.c
#	kernel/seccomp.c
#	lib/test_bpf.c
#	net/core/filter.c
diff --cc arch/arm/net/bpf_jit_32.c
index 09420d09618b,a76623bcf722..000000000000
--- a/arch/arm/net/bpf_jit_32.c
+++ b/arch/arm/net/bpf_jit_32.c
@@@ -927,19 -926,10 +927,26 @@@ out
  	return;
  }
  
 -void bpf_jit_free(struct bpf_prog *fp)
 +static void bpf_jit_free_worker(struct work_struct *work)
 +{
++<<<<<<< HEAD
 +	module_free(NULL, work);
 +}
 +
 +void bpf_jit_free(struct sk_filter *fp)
  {
 +	struct work_struct *work;
 +
 +	if (fp->bpf_func != sk_run_filter) {
 +		work = (struct work_struct *)fp->bpf_func;
 +
 +		INIT_WORK(work, bpf_jit_free_worker);
 +		schedule_work(work);
 +	}
++=======
+ 	if (fp->jited)
+ 		module_free(NULL, fp->bpf_func);
+ 
+ 	bpf_prog_unlock_free(fp);
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  }
diff --cc arch/powerpc/net/bpf_jit_comp.c
index 3c4d59c26d16,40c53ff59124..000000000000
--- a/arch/powerpc/net/bpf_jit_comp.c
+++ b/arch/powerpc/net/bpf_jit_comp.c
@@@ -700,20 -693,10 +700,27 @@@ out
  	return;
  }
  
 -void bpf_jit_free(struct bpf_prog *fp)
 +static void jit_free_defer(struct work_struct *arg)
  {
++<<<<<<< HEAD
 +	module_free(NULL, arg);
 +}
 +
 +/* run from softirq, we must use a work_struct to call
 + * module_free() from process context
 + */
 +void bpf_jit_free(struct sk_filter *fp)
 +{
 +	if (fp->bpf_func != sk_run_filter) {
 +		struct work_struct *work = (struct work_struct *)fp->bpf_func;
 +
 +		INIT_WORK(work, jit_free_defer);
 +		schedule_work(work);
 +	}
++=======
+ 	if (fp->jited)
+ 		module_free(NULL, fp->bpf_func);
+ 
+ 	bpf_prog_unlock_free(fp);
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  }
diff --cc arch/s390/net/bpf_jit_comp.c
index 600b1e5c8c9d,f2833c5b218a..000000000000
--- a/arch/s390/net/bpf_jit_comp.c
+++ b/arch/s390/net/bpf_jit_comp.c
@@@ -807,21 -875,17 +807,35 @@@ out
  	kfree(addrs);
  }
  
 -void bpf_jit_free(struct bpf_prog *fp)
 +static void jit_free_defer(struct work_struct *arg)
  {
++<<<<<<< HEAD
 +	module_free(NULL, arg);
 +}
 +
 +/* run from softirq, we must use a work_struct to call
 + * module_free() from process context
 + */
 +void bpf_jit_free(struct sk_filter *fp)
 +{
 +	struct work_struct *work;
 +
 +	if (fp->bpf_func == sk_run_filter)
 +		return;
 +	work = (struct work_struct *)fp->bpf_func;
 +	INIT_WORK(work, jit_free_defer);
 +	schedule_work(work);
++=======
+ 	unsigned long addr = (unsigned long)fp->bpf_func & PAGE_MASK;
+ 	struct bpf_binary_header *header = (void *)addr;
+ 
+ 	if (!fp->jited)
+ 		goto free_filter;
+ 
+ 	set_memory_rw(addr, header->pages);
+ 	module_free(NULL, header);
+ 
+ free_filter:
+ 	bpf_prog_unlock_free(fp);
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  }
diff --cc arch/sparc/net/bpf_jit_comp.c
index d32340c1e7af,f7a736b645e8..000000000000
--- a/arch/sparc/net/bpf_jit_comp.c
+++ b/arch/sparc/net/bpf_jit_comp.c
@@@ -817,20 -808,10 +817,27 @@@ out
  	return;
  }
  
 -void bpf_jit_free(struct bpf_prog *fp)
 +static void jit_free_defer(struct work_struct *arg)
  {
++<<<<<<< HEAD
 +	module_free(NULL, arg);
 +}
 +
 +/* run from softirq, we must use a work_struct to call
 + * module_free() from process context
 + */
 +void bpf_jit_free(struct sk_filter *fp)
 +{
 +	if (fp->bpf_func != sk_run_filter) {
 +		struct work_struct *work = (struct work_struct *)fp->bpf_func;
 +
 +		INIT_WORK(work, jit_free_defer);
 +		schedule_work(work);
 +	}
++=======
+ 	if (fp->jited)
+ 		module_free(NULL, fp->bpf_func);
+ 
+ 	bpf_prog_unlock_free(fp);
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  }
diff --cc arch/x86/net/bpf_jit_comp.c
index 76c7b3a140ad,39ccfbb4a723..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -746,23 -970,19 +746,39 @@@ cond_branch:			f_offset = addrs[i + fil
  	}
  out:
  	kfree(addrs);
 +	return;
  }
  
++<<<<<<< HEAD
 +static void jit_free_defer(struct work_struct *arg)
 +{
 +	module_free(NULL, arg);
 +}
 +
 +/* run from softirq, we must use a work_struct to call
 + * module_free() from process context
 + */
 +void bpf_jit_free(struct sk_filter *fp)
 +{
 +	if (fp->bpf_func != sk_run_filter) {
 +		struct work_struct *work = (struct work_struct *)fp->bpf_func;
 +
 +		INIT_WORK(work, jit_free_defer);
 +		schedule_work(work);
 +	}
++=======
+ void bpf_jit_free(struct bpf_prog *fp)
+ {
+ 	unsigned long addr = (unsigned long)fp->bpf_func & PAGE_MASK;
+ 	struct bpf_binary_header *header = (void *)addr;
+ 
+ 	if (!fp->jited)
+ 		goto free_filter;
+ 
+ 	set_memory_rw(addr, header->pages);
+ 	module_free(NULL, header);
+ 
+ free_filter:
+ 	bpf_prog_unlock_free(fp);
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  }
diff --cc include/linux/filter.h
index d322ed880333,c78994593355..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -6,85 -6,461 +6,479 @@@
  
  #include <linux/atomic.h>
  #include <linux/compat.h>
 -#include <linux/skbuff.h>
 -#include <linux/workqueue.h>
  #include <uapi/linux/filter.h>
++<<<<<<< HEAD
 +#ifndef __GENKSYMS__
 +#include <net/sch_generic.h>
 +#endif
++=======
+ #include <asm/cacheflush.h>
+ 
+ struct sk_buff;
+ struct sock;
+ struct seccomp_data;
+ 
+ /* Internally used and optimized filter representation with extended
+  * instruction set based on top of classic BPF.
+  */
+ 
+ /* instruction classes */
+ #define BPF_ALU64	0x07	/* alu mode in double word width */
+ 
+ /* ld/ldx fields */
+ #define BPF_DW		0x18	/* double word */
+ #define BPF_XADD	0xc0	/* exclusive add */
+ 
+ /* alu/jmp fields */
+ #define BPF_MOV		0xb0	/* mov reg to reg */
+ #define BPF_ARSH	0xc0	/* sign extending arithmetic shift right */
+ 
+ /* change endianness of a register */
+ #define BPF_END		0xd0	/* flags for endianness conversion: */
+ #define BPF_TO_LE	0x00	/* convert to little-endian */
+ #define BPF_TO_BE	0x08	/* convert to big-endian */
+ #define BPF_FROM_LE	BPF_TO_LE
+ #define BPF_FROM_BE	BPF_TO_BE
+ 
+ #define BPF_JNE		0x50	/* jump != */
+ #define BPF_JSGT	0x60	/* SGT is signed '>', GT in x86 */
+ #define BPF_JSGE	0x70	/* SGE is signed '>=', GE in x86 */
+ #define BPF_CALL	0x80	/* function call */
+ #define BPF_EXIT	0x90	/* function return */
+ 
+ /* Register numbers */
+ enum {
+ 	BPF_REG_0 = 0,
+ 	BPF_REG_1,
+ 	BPF_REG_2,
+ 	BPF_REG_3,
+ 	BPF_REG_4,
+ 	BPF_REG_5,
+ 	BPF_REG_6,
+ 	BPF_REG_7,
+ 	BPF_REG_8,
+ 	BPF_REG_9,
+ 	BPF_REG_10,
+ 	__MAX_BPF_REG,
+ };
+ 
+ /* BPF has 10 general purpose 64-bit registers and stack frame. */
+ #define MAX_BPF_REG	__MAX_BPF_REG
+ 
+ /* ArgX, context and stack frame pointer register positions. Note,
+  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
+  * calls in BPF_CALL instruction.
+  */
+ #define BPF_REG_ARG1	BPF_REG_1
+ #define BPF_REG_ARG2	BPF_REG_2
+ #define BPF_REG_ARG3	BPF_REG_3
+ #define BPF_REG_ARG4	BPF_REG_4
+ #define BPF_REG_ARG5	BPF_REG_5
+ #define BPF_REG_CTX	BPF_REG_6
+ #define BPF_REG_FP	BPF_REG_10
+ 
+ /* Additional register mappings for converted user programs. */
+ #define BPF_REG_A	BPF_REG_0
+ #define BPF_REG_X	BPF_REG_7
+ #define BPF_REG_TMP	BPF_REG_8
+ 
+ /* BPF program can access up to 512 bytes of stack space. */
+ #define MAX_BPF_STACK	512
+ 
+ /* Helper macros for filter block array initializers. */
+ 
+ /* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
+ 
+ #define BPF_ALU64_REG(OP, DST, SRC)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ #define BPF_ALU32_REG(OP, DST, SRC)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ /* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
+ 
+ #define BPF_ALU64_IMM(OP, DST, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ #define BPF_ALU32_IMM(OP, DST, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
+ 
+ #define BPF_ENDIAN(TYPE, DST, LEN)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = LEN })
+ 
+ /* Short form of mov, dst_reg = src_reg */
+ 
+ #define BPF_MOV64_REG(DST, SRC)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ #define BPF_MOV32_REG(DST, SRC)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ /* Short form of mov, dst_reg = imm32 */
+ 
+ #define BPF_MOV64_IMM(DST, IMM)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ #define BPF_MOV32_IMM(DST, IMM)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
+ 
+ #define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ #define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
+ 
+ #define BPF_LD_ABS(SIZE, IMM)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
+ 
+ #define BPF_LD_IND(SIZE, SRC, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
+ 		.dst_reg = 0,					\
+ 		.src_reg = SRC,					\
+ 		.off   = 0,					\
+ 		.imm   = IMM })
+ 
+ /* Memory load, dst_reg = *(uint *) (src_reg + off16) */
+ 
+ #define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Memory store, *(uint *) (dst_reg + off16) = src_reg */
+ 
+ #define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Memory store, *(uint *) (dst_reg + off16) = imm32 */
+ 
+ #define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,	\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = OFF,					\
+ 		.imm   = IMM })
+ 
+ /* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
+ 
+ #define BPF_JMP_REG(OP, DST, SRC, OFF)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = 0 })
+ 
+ /* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
+ 
+ #define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
+ 		.dst_reg = DST,					\
+ 		.src_reg = 0,					\
+ 		.off   = OFF,					\
+ 		.imm   = IMM })
+ 
+ /* Function call */
+ 
+ #define BPF_EMIT_CALL(FUNC)					\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_CALL,			\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = ((FUNC) - __bpf_call_base) })
+ 
+ /* Raw code statement block */
+ 
+ #define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
+ 	((struct bpf_insn) {					\
+ 		.code  = CODE,					\
+ 		.dst_reg = DST,					\
+ 		.src_reg = SRC,					\
+ 		.off   = OFF,					\
+ 		.imm   = IMM })
+ 
+ /* Program exit */
+ 
+ #define BPF_EXIT_INSN()						\
+ 	((struct bpf_insn) {					\
+ 		.code  = BPF_JMP | BPF_EXIT,			\
+ 		.dst_reg = 0,					\
+ 		.src_reg = 0,					\
+ 		.off   = 0,					\
+ 		.imm   = 0 })
+ 
+ #define bytes_to_bpf_size(bytes)				\
+ ({								\
+ 	int bpf_size = -EINVAL;					\
+ 								\
+ 	if (bytes == sizeof(u8))				\
+ 		bpf_size = BPF_B;				\
+ 	else if (bytes == sizeof(u16))				\
+ 		bpf_size = BPF_H;				\
+ 	else if (bytes == sizeof(u32))				\
+ 		bpf_size = BPF_W;				\
+ 	else if (bytes == sizeof(u64))				\
+ 		bpf_size = BPF_DW;				\
+ 								\
+ 	bpf_size;						\
+ })
+ 
+ /* Macro to invoke filter function. */
+ #define SK_RUN_FILTER(filter, ctx) \
+ 	(*filter->prog->bpf_func)(ctx, filter->prog->insnsi)
+ 
+ struct bpf_insn {
+ 	__u8	code;		/* opcode */
+ 	__u8	dst_reg:4;	/* dest register */
+ 	__u8	src_reg:4;	/* source register */
+ 	__s16	off;		/* signed offset */
+ 	__s32	imm;		/* signed immediate constant */
+ };
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  
  #ifdef CONFIG_COMPAT
 -/* A struct sock_filter is architecture independent. */
 +/*
 + * A struct sock_filter is architecture independent.
 + */
  struct compat_sock_fprog {
  	u16		len;
 -	compat_uptr_t	filter;	/* struct sock_filter * */
 +	compat_uptr_t	filter;		/* struct sock_filter * */
  };
  #endif
  
++<<<<<<< HEAD
 +struct sk_buff;
 +struct sock;
 +struct bpf_prog_aux;
 +
 +struct bpf_prog
 +{
 +	struct bpf_prog_aux	*aux;	/* Auxiliary fields */
 +};
 +
 +struct sk_filter
 +{
 +	atomic_t		refcnt;
 +	unsigned int         	len;	/* Number of filter blocks */
 +	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 +					    const struct sock_filter *filter);
 +	struct rcu_head		rcu;
 +	struct sock_filter     	insns[0];
++=======
+ struct sock_fprog_kern {
+ 	u16			len;
+ 	struct sock_filter	*filter;
+ };
+ 
+ struct bpf_work_struct {
+ 	struct bpf_prog *prog;
+ 	struct work_struct work;
+ };
+ 
+ struct bpf_prog {
+ 	u32			pages;		/* Number of allocated pages */
+ 	u32			jited:1,	/* Is our filter JIT'ed? */
+ 				len:31;		/* Number of filter blocks */
+ 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
+ 	struct bpf_work_struct	*work;		/* Deferred free work struct */
+ 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
+ 					    const struct bpf_insn *filter);
+ 	/* Instructions for interpreter */
+ 	union {
+ 		struct sock_filter	insns[0];
+ 		struct bpf_insn		insnsi[0];
+ 	};
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  };
  
 -struct sk_filter {
 -	atomic_t	refcnt;
 -	struct rcu_head	rcu;
 -	struct bpf_prog	*prog;
 +struct xdp_buff {
 +	void *data;
 +	void *data_end;
 +	void *data_hard_start;
  };
  
 -#define BPF_PROG_RUN(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 -
 -static inline unsigned int bpf_prog_size(unsigned int proglen)
 +/* compute the linear packet data range [data, data_end) which
 + * will be accessed by cls_bpf and act_bpf programs
 + */
 +static inline unsigned int sk_filter_len(const struct sk_filter *fp)
  {
 -	return max(sizeof(struct bpf_prog),
 -		   offsetof(struct bpf_prog, insns[proglen]));
 +	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
  }
  
++<<<<<<< HEAD
 +static inline void bpf_compute_data_end(struct sk_buff *skb)
++=======
+ #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
+ 
+ #ifdef CONFIG_DEBUG_SET_MODULE_RONX
+ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
+ {
+ 	set_memory_ro((unsigned long)fp, fp->pages);
+ }
+ 
+ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
+ {
+ 	set_memory_rw((unsigned long)fp, fp->pages);
+ }
+ #else
+ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
+ {
+ }
+ 
+ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
+ {
+ }
+ #endif /* CONFIG_DEBUG_SET_MODULE_RONX */
+ 
+ int sk_filter(struct sock *sk, struct sk_buff *skb);
+ 
+ void bpf_prog_select_runtime(struct bpf_prog *fp);
+ void bpf_prog_free(struct bpf_prog *fp);
+ 
+ int bpf_convert_filter(struct sock_filter *prog, int len,
+ 		       struct bpf_insn *new_prog, int *new_len);
+ 
+ struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
+ struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
+ 				  gfp_t gfp_extra_flags);
+ void __bpf_prog_free(struct bpf_prog *fp);
+ 
+ static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
+ {
+ 	bpf_prog_unlock_ro(fp);
+ 	__bpf_prog_free(fp);
+ }
+ 
+ int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
+ void bpf_prog_destroy(struct bpf_prog *fp);
+ 
+ int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+ int sk_detach_filter(struct sock *sk);
+ 
+ int bpf_check_classic(const struct sock_filter *filter, unsigned int flen);
+ int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
+ 		  unsigned int len);
+ 
+ bool sk_filter_charge(struct sock *sk, struct sk_filter *fp);
+ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
+ 
+ u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ void bpf_int_jit_compile(struct bpf_prog *fp);
+ 
+ #define BPF_ANC		BIT(15)
+ 
+ static inline u16 bpf_anc_helper(const struct sock_filter *ftest)
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  {
 -	BUG_ON(ftest->code & BPF_ANC);
 -
 -	switch (ftest->code) {
 -	case BPF_LD | BPF_W | BPF_ABS:
 -	case BPF_LD | BPF_H | BPF_ABS:
 -	case BPF_LD | BPF_B | BPF_ABS:
 -#define BPF_ANCILLARY(CODE)	case SKF_AD_OFF + SKF_AD_##CODE:	\
 -				return BPF_ANC | SKF_AD_##CODE
 -		switch (ftest->k) {
 -		BPF_ANCILLARY(PROTOCOL);
 -		BPF_ANCILLARY(PKTTYPE);
 -		BPF_ANCILLARY(IFINDEX);
 -		BPF_ANCILLARY(NLATTR);
 -		BPF_ANCILLARY(NLATTR_NEST);
 -		BPF_ANCILLARY(MARK);
 -		BPF_ANCILLARY(QUEUE);
 -		BPF_ANCILLARY(HATYPE);
 -		BPF_ANCILLARY(RXHASH);
 -		BPF_ANCILLARY(CPU);
 -		BPF_ANCILLARY(ALU_XOR_X);
 -		BPF_ANCILLARY(VLAN_TAG);
 -		BPF_ANCILLARY(VLAN_TAG_PRESENT);
 -		BPF_ANCILLARY(PAY_OFFSET);
 -		BPF_ANCILLARY(RANDOM);
 -		}
 -		/* Fallthrough. */
 -	default:
 -		return ftest->code;
 -	}
 +	return;
  }
  
 -void *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb,
 -					   int k, unsigned int size);
 +int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
 +static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 +{
 +	return sk_filter_trim_cap(sk, skb, 1);
 +}
  
 -static inline void *bpf_load_pointer(const struct sk_buff *skb, int k,
 -				     unsigned int size, void *buffer)
 +extern unsigned int sk_run_filter(const struct sk_buff *skb,
 +				  const struct sock_filter *filter);
 +extern int sk_unattached_filter_create(struct sk_filter **pfp,
 +				       struct sock_fprog *fprog);
 +extern void sk_unattached_filter_destroy(struct sk_filter *fp);
 +extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 +extern int sk_detach_filter(struct sock *sk);
 +extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 +extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 +extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
 +
 +static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 +				   struct xdp_buff *xdp)
  {
 -	if (k >= 0)
 -		return skb_header_pointer(skb, k, size, buffer);
 +	return 0;
 +}
  
 -	return bpf_internal_load_pointer_neg_helper(skb, k, size);
 +static inline void bpf_warn_invalid_xdp_action(u32 act)
 +{
 +	return;
  }
  
  #ifdef CONFIG_BPF_JIT
@@@ -98,22 -474,24 +492,26 @@@ extern void bpf_jit_free(struct sk_filt
  static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
  				u32 pass, void *image)
  {
 -	pr_err("flen=%u proglen=%u pass=%u image=%pK\n",
 +	pr_err("flen=%u proglen=%u pass=%u image=%p\n",
  	       flen, proglen, pass, image);
  	if (image)
 -		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_OFFSET,
 +		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_ADDRESS,
  			       16, 1, image, proglen, false);
  }
 +#define SK_RUN_FILTER(FILTER, SKB) (*FILTER->bpf_func)(SKB, FILTER->insns)
  #else
 -#include <linux/slab.h>
 -
 -static inline void bpf_jit_compile(struct bpf_prog *fp)
 +static inline void bpf_jit_compile(struct sk_filter *fp)
  {
  }
 -
 -static inline void bpf_jit_free(struct bpf_prog *fp)
 +static inline void bpf_jit_free(struct sk_filter *fp)
  {
++<<<<<<< HEAD
++=======
+ 	bpf_prog_unlock_free(fp);
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  }
 -#endif /* CONFIG_BPF_JIT */
 +#define SK_RUN_FILTER(FILTER, SKB) sk_run_filter(SKB, FILTER->insns)
 +#endif
  
  static inline int bpf_tell_extensions(void)
  {
diff --cc kernel/seccomp.c
index 42e55449c1a5,84922befea84..000000000000
--- a/kernel/seccomp.c
+++ b/kernel/seccomp.c
@@@ -397,22 -370,53 +397,58 @@@ static struct seccomp_filter *seccomp_p
  
  	/* Copy the instructions from fprog. */
  	ret = -EFAULT;
 -	if (copy_from_user(fp, fprog->filter, fp_size))
 -		goto free_prog;
 +	if (copy_from_user(filter->insns, fprog->filter, fp_size))
 +		goto fail;
  
  	/* Check and rewrite the fprog via the skb checker */
 -	ret = bpf_check_classic(fp, fprog->len);
 +	ret = sk_chk_filter(filter->insns, filter->len);
  	if (ret)
 -		goto free_prog;
 +		goto fail;
  
  	/* Check and rewrite the fprog for seccomp use */
 -	ret = seccomp_check_filter(fp, fprog->len);
 +	ret = seccomp_check_filter(filter->insns, filter->len);
  	if (ret)
++<<<<<<< HEAD
 +		goto fail;
 +
 +	return filter;
 +fail:
++=======
+ 		goto free_prog;
+ 
+ 	/* Convert 'sock_filter' insns to 'bpf_insn' insns */
+ 	ret = bpf_convert_filter(fp, fprog->len, NULL, &new_len);
+ 	if (ret)
+ 		goto free_prog;
+ 
+ 	/* Allocate a new seccomp_filter */
+ 	ret = -ENOMEM;
+ 	filter = kzalloc(sizeof(struct seccomp_filter),
+ 			 GFP_KERNEL|__GFP_NOWARN);
+ 	if (!filter)
+ 		goto free_prog;
+ 
+ 	filter->prog = bpf_prog_alloc(bpf_prog_size(new_len), __GFP_NOWARN);
+ 	if (!filter->prog)
+ 		goto free_filter;
+ 
+ 	ret = bpf_convert_filter(fp, fprog->len, filter->prog->insnsi, &new_len);
+ 	if (ret)
+ 		goto free_filter_prog;
+ 
+ 	kfree(fp);
+ 	atomic_set(&filter->usage, 1);
+ 	filter->prog->len = new_len;
+ 
+ 	bpf_prog_select_runtime(filter->prog);
+ 
+ 	return filter;
+ 
+ free_filter_prog:
+ 	__bpf_prog_free(filter->prog);
+ free_filter:
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  	kfree(filter);
 -free_prog:
 -	kfree(fp);
  	return ERR_PTR(ret);
  }
  
diff --cc net/core/filter.c
index 060ed5f86613,37f8eb06fdee..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -647,12 -861,113 +647,119 @@@ void sk_filter_release_rcu(struct rcu_h
  {
  	struct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);
  
 -	__sk_filter_release(fp);
 +	bpf_jit_free(fp);
 +	kfree(fp);
  }
 +EXPORT_SYMBOL(sk_filter_release_rcu);
  
++<<<<<<< HEAD
 +static int __sk_prepare_filter(struct sk_filter *fp)
++=======
+ /**
+  *	sk_filter_release - release a socket filter
+  *	@fp: filter to remove
+  *
+  *	Remove a filter from a socket and release its resources.
+  */
+ static void sk_filter_release(struct sk_filter *fp)
+ {
+ 	if (atomic_dec_and_test(&fp->refcnt))
+ 		call_rcu(&fp->rcu, sk_filter_release_rcu);
+ }
+ 
+ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)
+ {
+ 	u32 filter_size = bpf_prog_size(fp->prog->len);
+ 
+ 	atomic_sub(filter_size, &sk->sk_omem_alloc);
+ 	sk_filter_release(fp);
+ }
+ 
+ /* try to charge the socket memory if there is space available
+  * return true on success
+  */
+ bool sk_filter_charge(struct sock *sk, struct sk_filter *fp)
+ {
+ 	u32 filter_size = bpf_prog_size(fp->prog->len);
+ 
+ 	/* same check as in sock_kmalloc() */
+ 	if (filter_size <= sysctl_optmem_max &&
+ 	    atomic_read(&sk->sk_omem_alloc) + filter_size < sysctl_optmem_max) {
+ 		atomic_inc(&fp->refcnt);
+ 		atomic_add(filter_size, &sk->sk_omem_alloc);
+ 		return true;
+ 	}
+ 	return false;
+ }
+ 
+ static struct bpf_prog *bpf_migrate_filter(struct bpf_prog *fp)
+ {
+ 	struct sock_filter *old_prog;
+ 	struct bpf_prog *old_fp;
+ 	int err, new_len, old_len = fp->len;
+ 
+ 	/* We are free to overwrite insns et al right here as it
+ 	 * won't be used at this point in time anymore internally
+ 	 * after the migration to the internal BPF instruction
+ 	 * representation.
+ 	 */
+ 	BUILD_BUG_ON(sizeof(struct sock_filter) !=
+ 		     sizeof(struct bpf_insn));
+ 
+ 	/* Conversion cannot happen on overlapping memory areas,
+ 	 * so we need to keep the user BPF around until the 2nd
+ 	 * pass. At this time, the user BPF is stored in fp->insns.
+ 	 */
+ 	old_prog = kmemdup(fp->insns, old_len * sizeof(struct sock_filter),
+ 			   GFP_KERNEL);
+ 	if (!old_prog) {
+ 		err = -ENOMEM;
+ 		goto out_err;
+ 	}
+ 
+ 	/* 1st pass: calculate the new program length. */
+ 	err = bpf_convert_filter(old_prog, old_len, NULL, &new_len);
+ 	if (err)
+ 		goto out_err_free;
+ 
+ 	/* Expand fp for appending the new filter representation. */
+ 	old_fp = fp;
+ 	fp = bpf_prog_realloc(old_fp, bpf_prog_size(new_len), 0);
+ 	if (!fp) {
+ 		/* The old_fp is still around in case we couldn't
+ 		 * allocate new memory, so uncharge on that one.
+ 		 */
+ 		fp = old_fp;
+ 		err = -ENOMEM;
+ 		goto out_err_free;
+ 	}
+ 
+ 	fp->len = new_len;
+ 
+ 	/* 2nd pass: remap sock_filter insns into bpf_insn insns. */
+ 	err = bpf_convert_filter(old_prog, old_len, fp->insnsi, &new_len);
+ 	if (err)
+ 		/* 2nd bpf_convert_filter() can fail only if it fails
+ 		 * to allocate memory, remapping must succeed. Note,
+ 		 * that at this time old_fp has already been released
+ 		 * by krealloc().
+ 		 */
+ 		goto out_err_free;
+ 
+ 	bpf_prog_select_runtime(fp);
+ 
+ 	kfree(old_prog);
+ 	return fp;
+ 
+ out_err_free:
+ 	kfree(old_prog);
+ out_err:
+ 	__bpf_prog_release(fp);
+ 	return ERR_PTR(err);
+ }
+ 
+ static struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp)
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  {
  	int err;
  
@@@ -687,17 -1013,25 +794,21 @@@ int sk_unattached_filter_create(struct 
  	if (fprog->filter == NULL)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	fp = kmalloc(fsize + sizeof(*fp), GFP_KERNEL);
++=======
+ 	fp = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  	if (!fp)
  		return -ENOMEM;
 -
  	memcpy(fp->insns, fprog->filter, fsize);
  
 +	atomic_set(&fp->refcnt, 1);
  	fp->len = fprog->len;
 -	/* Since unattached filters are not copied back to user
 -	 * space through sk_get_filter(), we do not need to hold
 -	 * a copy here, and can spare us the work.
 -	 */
 -	fp->orig_prog = NULL;
  
 -	/* bpf_prepare_filter() already takes care of freeing
 -	 * memory in case something goes wrong.
 -	 */
 -	fp = bpf_prepare_filter(fp);
 -	if (IS_ERR(fp))
 -		return PTR_ERR(fp);
 +	err = __sk_prepare_filter(fp);
 +	if (err)
 +		goto free_mem;
  
  	*pfp = fp;
  	return 0;
@@@ -736,11 -1069,12 +847,16 @@@ int sk_attach_filter(struct sock_fprog 
  	if (fprog->filter == NULL)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	fp = sock_kmalloc(sk, fsize+sizeof(*fp), GFP_KERNEL);
 +	if (!fp)
++=======
+ 	prog = bpf_prog_alloc(bpf_fsize, 0);
+ 	if (!prog)
++>>>>>>> 60a3b2253c41 (net: bpf: make eBPF interpreter images read-only)
  		return -ENOMEM;
 -
 -	if (copy_from_user(prog->insns, fprog->filter, fsize)) {
 -		kfree(prog);
 +	if (copy_from_user(fp->insns, fprog->filter, fsize)) {
 +		sock_kfree_s(sk, fp, fsize+sizeof(*fp));
  		return -EFAULT;
  	}
  
* Unmerged path arch/mips/net/bpf_jit.c
* Unmerged path kernel/bpf/core.c
* Unmerged path lib/test_bpf.c
* Unmerged path arch/arm/net/bpf_jit_32.c
* Unmerged path arch/mips/net/bpf_jit.c
* Unmerged path arch/powerpc/net/bpf_jit_comp.c
* Unmerged path arch/s390/net/bpf_jit_comp.c
* Unmerged path arch/sparc/net/bpf_jit_comp.c
* Unmerged path arch/x86/net/bpf_jit_comp.c
* Unmerged path include/linux/filter.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/seccomp.c
* Unmerged path lib/test_bpf.c
* Unmerged path net/core/filter.c
