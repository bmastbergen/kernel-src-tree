scsi: qla2xxx: Fix inconsistent DMA mem alloc/free

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [scsi] qla2xxx: Fix inconsistent DMA mem alloc/free (Himanshu Madhani) [1596344]
Rebuild_FUZZ: 93.62%
commit-author Quinn Tran <quin.tran@cavium.com>
commit b5f3bc39a0e815a30005da246dd4ad47fd2f88ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/b5f3bc39.failed

GPNFT command allocates 2 buffer for switch query. On completion, the same
buffers were freed using different size, instead of using original size at
the time of allocation.

This patch saves the size of the request and response buffers and uses that
to free them.

Following stack trace can be seen when using debug kernel

dump_stack+0x19/0x1b
__warn+0xd8/0x100
warn_slowpath_fmt+0x5f/0x80
check_unmap+0xfb/0xa20
debug_dma_free_coherent+0x110/0x160
qla24xx_sp_unmap+0x131/0x1e0 [qla2xxx]
qla24xx_async_gnnft_done+0xb6/0x550 [qla2xxx]
qla2x00_do_work+0x1ec/0x9f0 [qla2xxx]

	Cc: <stable@vger.kernel.org> # v4.17+
Fixes: 33b28357dd00 ("scsi: qla2xxx: Fix Async GPN_FT for FCP and FC-NVMe scan")
	Reported-by: Ewan D. Milne <emilne@redhat.com>
	Signed-off-by: Quinn Tran <quinn.tran@cavium.com>
	Signed-off-by: Himanshu Madhani <himanshu.madhani@cavium.com>
	Signed-off-by: Himanshu Madhani <hmadhani@redhat.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit b5f3bc39a0e815a30005da246dd4ad47fd2f88ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/qla2xxx/qla_gs.c
diff --cc drivers/scsi/qla2xxx/qla_gs.c
index 985af1dbf875,2c35b0b2baa0..000000000000
--- a/drivers/scsi/qla2xxx/qla_gs.c
+++ b/drivers/scsi/qla2xxx/qla_gs.c
@@@ -509,6 -513,72 +509,75 @@@ qla2x00_gnn_id(scsi_qla_host_t *vha, sw
  	return (rval);
  }
  
++<<<<<<< HEAD
++=======
+ static void qla2x00_async_sns_sp_done(void *s, int rc)
+ {
+ 	struct srb *sp = s;
+ 	struct scsi_qla_host *vha = sp->vha;
+ 	struct ct_sns_pkt *ct_sns;
+ 	struct qla_work_evt *e;
+ 
+ 	sp->rc = rc;
+ 	if (rc == QLA_SUCCESS) {
+ 		ql_dbg(ql_dbg_disc, vha, 0x204f,
+ 		    "Async done-%s exiting normally.\n",
+ 		    sp->name);
+ 	} else if (rc == QLA_FUNCTION_TIMEOUT) {
+ 		ql_dbg(ql_dbg_disc, vha, 0x204f,
+ 		    "Async done-%s timeout\n", sp->name);
+ 	} else {
+ 		ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.rsp;
+ 		memset(ct_sns, 0, sizeof(*ct_sns));
+ 		sp->retry_count++;
+ 		if (sp->retry_count > 3)
+ 			goto err;
+ 
+ 		ql_dbg(ql_dbg_disc, vha, 0x204f,
+ 		    "Async done-%s fail rc %x.  Retry count %d\n",
+ 		    sp->name, rc, sp->retry_count);
+ 
+ 		e = qla2x00_alloc_work(vha, QLA_EVT_SP_RETRY);
+ 		if (!e)
+ 			goto err2;
+ 
+ 		del_timer(&sp->u.iocb_cmd.timer);
+ 		e->u.iosb.sp = sp;
+ 		qla2x00_post_work(vha, e);
+ 		return;
+ 	}
+ 
+ err:
+ 	e = qla2x00_alloc_work(vha, QLA_EVT_UNMAP);
+ err2:
+ 	if (!e) {
+ 		/* please ignore kernel warning. otherwise, we have mem leak. */
+ 		if (sp->u.iocb_cmd.u.ctarg.req) {
+ 			dma_free_coherent(&vha->hw->pdev->dev,
+ 			    sp->u.iocb_cmd.u.ctarg.req_allocated_size,
+ 			    sp->u.iocb_cmd.u.ctarg.req,
+ 			    sp->u.iocb_cmd.u.ctarg.req_dma);
+ 			sp->u.iocb_cmd.u.ctarg.req = NULL;
+ 		}
+ 
+ 		if (sp->u.iocb_cmd.u.ctarg.rsp) {
+ 			dma_free_coherent(&vha->hw->pdev->dev,
+ 			    sp->u.iocb_cmd.u.ctarg.rsp_allocated_size,
+ 			    sp->u.iocb_cmd.u.ctarg.rsp,
+ 			    sp->u.iocb_cmd.u.ctarg.rsp_dma);
+ 			sp->u.iocb_cmd.u.ctarg.rsp = NULL;
+ 		}
+ 
+ 		sp->free(sp);
+ 
+ 		return;
+ 	}
+ 
+ 	e->u.iosb.sp = sp;
+ 	qla2x00_post_work(vha, e);
+ }
+ 
++>>>>>>> b5f3bc39a0e8 (scsi: qla2xxx: Fix inconsistent DMA mem alloc/free)
  /**
   * qla2x00_rft_id() - SNS Register FC-4 TYPEs (RFT_ID) supported by the HBA.
   * @vha: HA context
@@@ -528,21 -593,54 +597,66 @@@ qla2x00_rft_id(scsi_qla_host_t *vha
  	if (IS_QLA2100(ha) || IS_QLA2200(ha))
  		return qla2x00_sns_rft_id(vha);
  
 -	return qla_async_rftid(vha, &vha->d_id);
 -}
 +	arg.iocb = ha->ms_iocb;
 +	arg.req_dma = ha->ct_sns_dma;
 +	arg.rsp_dma = ha->ct_sns_dma;
 +	arg.req_size = RFT_ID_REQ_SIZE;
 +	arg.rsp_size = RFT_ID_RSP_SIZE;
 +	arg.nport_handle = NPH_SNS;
  
++<<<<<<< HEAD
 +	/* Issue RFT_ID */
 +	/* Prepare common MS IOCB */
 +	ms_pkt = ha->isp_ops->prep_ms_iocb(vha, &arg);
++=======
+ static int qla_async_rftid(scsi_qla_host_t *vha, port_id_t *d_id)
+ {
+ 	int rval = QLA_MEMORY_ALLOC_FAILED;
+ 	struct ct_sns_req *ct_req;
+ 	srb_t *sp;
+ 	struct ct_sns_pkt *ct_sns;
+ 
+ 	if (!vha->flags.online)
+ 		goto done;
+ 
+ 	sp = qla2x00_get_sp(vha, NULL, GFP_KERNEL);
+ 	if (!sp)
+ 		goto done;
+ 
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "rft_id";
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	sp->u.iocb_cmd.u.ctarg.req = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.req_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.req_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.req) {
+ 		ql_log(ql_log_warn, vha, 0xd041,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	sp->u.iocb_cmd.u.ctarg.rsp = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.rsp_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.rsp_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.rsp) {
+ 		ql_log(ql_log_warn, vha, 0xd042,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.rsp;
+ 	memset(ct_sns, 0, sizeof(*ct_sns));
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.req;
++>>>>>>> b5f3bc39a0e8 (scsi: qla2xxx: Fix inconsistent DMA mem alloc/free)
  
  	/* Prepare CT request */
 -	ct_req = qla2x00_prep_ct_req(ct_sns, RFT_ID_CMD, RFT_ID_RSP_SIZE);
 +	ct_req = qla2x00_prep_ct_req(ha->ct_sns, RFT_ID_CMD,
 +	    RFT_ID_RSP_SIZE);
 +	ct_rsp = &ha->ct_sns->p.rsp;
  
  	/* Prepare CT arguments -- port_id, FC-4 types */
  	ct_req->req.rft_id.port_id[0] = vha->d_id.b.domain;
@@@ -592,47 -691,83 +706,90 @@@ qla2x00_rff_id(scsi_qla_host_t *vha
  		return (QLA_SUCCESS);
  	}
  
 -	return qla_async_rffid(vha, &vha->d_id, qlt_rff_id(vha),
 -	    FC4_TYPE_FCP_SCSI);
 -}
 +	arg.iocb = ha->ms_iocb;
 +	arg.req_dma = ha->ct_sns_dma;
 +	arg.rsp_dma = ha->ct_sns_dma;
 +	arg.req_size = RFF_ID_REQ_SIZE;
 +	arg.rsp_size = RFF_ID_RSP_SIZE;
 +	arg.nport_handle = NPH_SNS;
  
++<<<<<<< HEAD
 +	/* Issue RFF_ID */
 +	/* Prepare common MS IOCB */
 +	ms_pkt = ha->isp_ops->prep_ms_iocb(vha, &arg);
++=======
+ static int qla_async_rffid(scsi_qla_host_t *vha, port_id_t *d_id,
+     u8 fc4feature, u8 fc4type)
+ {
+ 	int rval = QLA_MEMORY_ALLOC_FAILED;
+ 	struct ct_sns_req *ct_req;
+ 	srb_t *sp;
+ 	struct ct_sns_pkt *ct_sns;
+ 
+ 	sp = qla2x00_get_sp(vha, NULL, GFP_KERNEL);
+ 	if (!sp)
+ 		goto done;
+ 
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "rff_id";
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	sp->u.iocb_cmd.u.ctarg.req = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.req_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.req_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.req) {
+ 		ql_log(ql_log_warn, vha, 0xd041,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	sp->u.iocb_cmd.u.ctarg.rsp = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.rsp_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.rsp_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.rsp) {
+ 		ql_log(ql_log_warn, vha, 0xd042,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.rsp;
+ 	memset(ct_sns, 0, sizeof(*ct_sns));
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.req;
++>>>>>>> b5f3bc39a0e8 (scsi: qla2xxx: Fix inconsistent DMA mem alloc/free)
  
  	/* Prepare CT request */
 -	ct_req = qla2x00_prep_ct_req(ct_sns, RFF_ID_CMD, RFF_ID_RSP_SIZE);
 +	ct_req = qla2x00_prep_ct_req(ha->ct_sns, RFF_ID_CMD,
 +	    RFF_ID_RSP_SIZE);
 +	ct_rsp = &ha->ct_sns->p.rsp;
  
  	/* Prepare CT arguments -- port_id, FC-4 feature, FC-4 type */
 -	ct_req->req.rff_id.port_id[0] = d_id->b.domain;
 -	ct_req->req.rff_id.port_id[1] = d_id->b.area;
 -	ct_req->req.rff_id.port_id[2] = d_id->b.al_pa;
 -	ct_req->req.rff_id.fc4_feature = fc4feature;
 -	ct_req->req.rff_id.fc4_type = fc4type;		/* SCSI - FCP */
 -
 -	sp->u.iocb_cmd.u.ctarg.req_size = RFF_ID_REQ_SIZE;
 -	sp->u.iocb_cmd.u.ctarg.rsp_size = RFF_ID_RSP_SIZE;
 -	sp->u.iocb_cmd.u.ctarg.nport_handle = NPH_SNS;
 -	sp->u.iocb_cmd.timeout = qla2x00_async_iocb_timeout;
 -	sp->done = qla2x00_async_sns_sp_done;
 +	ct_req->req.rff_id.port_id[0] = vha->d_id.b.domain;
 +	ct_req->req.rff_id.port_id[1] = vha->d_id.b.area;
 +	ct_req->req.rff_id.port_id[2] = vha->d_id.b.al_pa;
  
 -	rval = qla2x00_start_sp(sp);
 +	qlt_rff_id(vha, ct_req);
 +
 +	ct_req->req.rff_id.fc4_type = 0x08;		/* SCSI - FCP */
 +
 +	/* Execute MS IOCB */
 +	rval = qla2x00_issue_iocb(vha, ha->ms_iocb, ha->ms_iocb_dma,
 +	    sizeof(ms_iocb_entry_t));
  	if (rval != QLA_SUCCESS) {
 +		/*EMPTY*/
  		ql_dbg(ql_dbg_disc, vha, 0x2047,
  		    "RFF_ID issue IOCB failed (%d).\n", rval);
 -		goto done_free_sp;
 +	} else if (qla2x00_chk_ms_status(vha, ms_pkt, ct_rsp, "RFF_ID") !=
 +	    QLA_SUCCESS) {
 +		rval = QLA_FUNCTION_FAILED;
 +	} else {
 +		ql_dbg(ql_dbg_disc, vha, 0x2048,
 +		    "RFF_ID exiting normally.\n");
  	}
  
 -	ql_dbg(ql_dbg_disc, vha, 0xffff,
 -	    "Async-%s - hdl=%x portid %06x feature %x type %x.\n",
 -	    sp->name, sp->handle, d_id->b24, fc4feature, fc4type);
 -	return rval;
 -
 -done_free_sp:
 -	sp->free(sp);
 -done:
 -	return rval;
 +	return (rval);
  }
  
  /**
@@@ -654,20 -784,52 +811,63 @@@ qla2x00_rnn_id(scsi_qla_host_t *vha
  	if (IS_QLA2100(ha) || IS_QLA2200(ha))
  		return qla2x00_sns_rnn_id(vha);
  
 -	return  qla_async_rnnid(vha, &vha->d_id, vha->node_name);
 -}
 +	arg.iocb = ha->ms_iocb;
 +	arg.req_dma = ha->ct_sns_dma;
 +	arg.rsp_dma = ha->ct_sns_dma;
 +	arg.req_size = RNN_ID_REQ_SIZE;
 +	arg.rsp_size = RNN_ID_RSP_SIZE;
 +	arg.nport_handle = NPH_SNS;
  
++<<<<<<< HEAD
 +	/* Issue RNN_ID */
 +	/* Prepare common MS IOCB */
 +	ms_pkt = ha->isp_ops->prep_ms_iocb(vha, &arg);
++=======
+ static int qla_async_rnnid(scsi_qla_host_t *vha, port_id_t *d_id,
+ 	u8 *node_name)
+ {
+ 	int rval = QLA_MEMORY_ALLOC_FAILED;
+ 	struct ct_sns_req *ct_req;
+ 	srb_t *sp;
+ 	struct ct_sns_pkt *ct_sns;
+ 
+ 	sp = qla2x00_get_sp(vha, NULL, GFP_KERNEL);
+ 	if (!sp)
+ 		goto done;
+ 
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "rnid";
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	sp->u.iocb_cmd.u.ctarg.req = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.req_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.req_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.req) {
+ 		ql_log(ql_log_warn, vha, 0xd041,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	sp->u.iocb_cmd.u.ctarg.rsp = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.rsp_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.rsp_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.rsp) {
+ 		ql_log(ql_log_warn, vha, 0xd042,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.rsp;
+ 	memset(ct_sns, 0, sizeof(*ct_sns));
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.req;
++>>>>>>> b5f3bc39a0e8 (scsi: qla2xxx: Fix inconsistent DMA mem alloc/free)
  
  	/* Prepare CT request */
 -	ct_req = qla2x00_prep_ct_req(ct_sns, RNN_ID_CMD, RNN_ID_RSP_SIZE);
 +	ct_req = qla2x00_prep_ct_req(ha->ct_sns, RNN_ID_CMD, RNN_ID_RSP_SIZE);
 +	ct_rsp = &ha->ct_sns->p.rsp;
  
  	/* Prepare CT arguments -- port_id, node_name */
  	ct_req->req.rnn_id.port_id[0] = vha->d_id.b.domain;
@@@ -731,22 -894,51 +931,64 @@@ qla2x00_rsnn_nn(scsi_qla_host_t *vha
  		return (QLA_SUCCESS);
  	}
  
 -	return qla_async_rsnn_nn(vha);
 -}
 +	arg.iocb = ha->ms_iocb;
 +	arg.req_dma = ha->ct_sns_dma;
 +	arg.rsp_dma = ha->ct_sns_dma;
 +	arg.req_size = 0;
 +	arg.rsp_size = RSNN_NN_RSP_SIZE;
 +	arg.nport_handle = NPH_SNS;
  
++<<<<<<< HEAD
 +	/* Issue RSNN_NN */
 +	/* Prepare common MS IOCB */
 +	/*   Request size adjusted after CT preparation */
 +	ms_pkt = ha->isp_ops->prep_ms_iocb(vha, &arg);
++=======
+ static int qla_async_rsnn_nn(scsi_qla_host_t *vha)
+ {
+ 	int rval = QLA_MEMORY_ALLOC_FAILED;
+ 	struct ct_sns_req *ct_req;
+ 	srb_t *sp;
+ 	struct ct_sns_pkt *ct_sns;
+ 
+ 	sp = qla2x00_get_sp(vha, NULL, GFP_KERNEL);
+ 	if (!sp)
+ 		goto done;
+ 
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "rsnn_nn";
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	sp->u.iocb_cmd.u.ctarg.req = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.req_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.req_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.req) {
+ 		ql_log(ql_log_warn, vha, 0xd041,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	sp->u.iocb_cmd.u.ctarg.rsp = dma_alloc_coherent(&vha->hw->pdev->dev,
+ 	    sizeof(struct ct_sns_pkt), &sp->u.iocb_cmd.u.ctarg.rsp_dma,
+ 	    GFP_KERNEL);
+ 	sp->u.iocb_cmd.u.ctarg.rsp_allocated_size = sizeof(struct ct_sns_pkt);
+ 	if (!sp->u.iocb_cmd.u.ctarg.rsp) {
+ 		ql_log(ql_log_warn, vha, 0xd042,
+ 		    "%s: Failed to allocate ct_sns request.\n",
+ 		    __func__);
+ 		goto done_free_sp;
+ 	}
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.rsp;
+ 	memset(ct_sns, 0, sizeof(*ct_sns));
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.req;
++>>>>>>> b5f3bc39a0e8 (scsi: qla2xxx: Fix inconsistent DMA mem alloc/free)
  
  	/* Prepare CT request */
 -	ct_req = qla2x00_prep_ct_req(ct_sns, RSNN_NN_CMD, RSNN_NN_RSP_SIZE);
 +	ct_req = qla2x00_prep_ct_req(ha->ct_sns, RSNN_NN_CMD,
 +	    RSNN_NN_RSP_SIZE);
 +	ct_rsp = &ha->ct_sns->p.rsp;
  
  	/* Prepare CT arguments -- node_name, symbolic node_name, size */
  	memcpy(ct_req->req.rsnn_nn.node_name, vha->node_name, WWN_SIZE);
@@@ -3326,3 -3727,995 +3570,998 @@@ done_free_sp
  done:
  	return rval;
  }
++<<<<<<< HEAD
++=======
+ 
+ void qla24xx_handle_gffid_event(scsi_qla_host_t *vha, struct event_arg *ea)
+ {
+        fc_port_t *fcport = ea->fcport;
+ 
+        qla24xx_post_gnl_work(vha, fcport);
+ }
+ 
+ void qla24xx_async_gffid_sp_done(void *s, int res)
+ {
+        struct srb *sp = s;
+        struct scsi_qla_host *vha = sp->vha;
+        fc_port_t *fcport = sp->fcport;
+        struct ct_sns_rsp *ct_rsp;
+        struct event_arg ea;
+ 
+        ql_dbg(ql_dbg_disc, vha, 0x2133,
+ 	   "Async done-%s res %x ID %x. %8phC\n",
+ 	   sp->name, res, fcport->d_id.b24, fcport->port_name);
+ 
+        fcport->flags &= ~FCF_ASYNC_SENT;
+        ct_rsp = &fcport->ct_desc.ct_sns->p.rsp;
+        /*
+ 	* FC-GS-7, 5.2.3.12 FC-4 Features - format
+ 	* The format of the FC-4 Features object, as defined by the FC-4,
+ 	* Shall be an array of 4-bit values, one for each type code value
+ 	*/
+        if (!res) {
+ 	       if (ct_rsp->rsp.gff_id.fc4_features[GFF_FCP_SCSI_OFFSET] & 0xf) {
+ 		       /* w1 b00:03 */
+ 		       fcport->fc4_type =
+ 			   ct_rsp->rsp.gff_id.fc4_features[GFF_FCP_SCSI_OFFSET];
+ 		       fcport->fc4_type &= 0xf;
+ 	       }
+ 
+ 	       if (ct_rsp->rsp.gff_id.fc4_features[GFF_NVME_OFFSET] & 0xf) {
+ 		       /* w5 [00:03]/28h */
+ 		       fcport->fc4f_nvme =
+ 			   ct_rsp->rsp.gff_id.fc4_features[GFF_NVME_OFFSET];
+ 		       fcport->fc4f_nvme &= 0xf;
+ 	       }
+        }
+ 
+        memset(&ea, 0, sizeof(ea));
+        ea.sp = sp;
+        ea.fcport = sp->fcport;
+        ea.rc = res;
+        ea.event = FCME_GFFID_DONE;
+ 
+        qla2x00_fcport_event_handler(vha, &ea);
+        sp->free(sp);
+ }
+ 
+ /* Get FC4 Feature with Nport ID. */
+ int qla24xx_async_gffid(scsi_qla_host_t *vha, fc_port_t *fcport)
+ {
+ 	int rval = QLA_FUNCTION_FAILED;
+ 	struct ct_sns_req       *ct_req;
+ 	srb_t *sp;
+ 
+ 	if (!vha->flags.online || (fcport->flags & FCF_ASYNC_SENT))
+ 		return rval;
+ 
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_KERNEL);
+ 	if (!sp)
+ 		return rval;
+ 
+ 	fcport->flags |= FCF_ASYNC_SENT;
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "gffid";
+ 	sp->gen1 = fcport->rscn_gen;
+ 	sp->gen2 = fcport->login_gen;
+ 
+ 	sp->u.iocb_cmd.timeout = qla2x00_async_iocb_timeout;
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	/* CT_IU preamble  */
+ 	ct_req = qla2x00_prep_ct_req(fcport->ct_desc.ct_sns, GFF_ID_CMD,
+ 	    GFF_ID_RSP_SIZE);
+ 
+ 	ct_req->req.gff_id.port_id[0] = fcport->d_id.b.domain;
+ 	ct_req->req.gff_id.port_id[1] = fcport->d_id.b.area;
+ 	ct_req->req.gff_id.port_id[2] = fcport->d_id.b.al_pa;
+ 
+ 	sp->u.iocb_cmd.u.ctarg.req = fcport->ct_desc.ct_sns;
+ 	sp->u.iocb_cmd.u.ctarg.req_dma = fcport->ct_desc.ct_sns_dma;
+ 	sp->u.iocb_cmd.u.ctarg.rsp = fcport->ct_desc.ct_sns;
+ 	sp->u.iocb_cmd.u.ctarg.rsp_dma = fcport->ct_desc.ct_sns_dma;
+ 	sp->u.iocb_cmd.u.ctarg.req_size = GFF_ID_REQ_SIZE;
+ 	sp->u.iocb_cmd.u.ctarg.rsp_size = GFF_ID_RSP_SIZE;
+ 	sp->u.iocb_cmd.u.ctarg.nport_handle = NPH_SNS;
+ 
+ 	sp->done = qla24xx_async_gffid_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 	if (rval != QLA_SUCCESS)
+ 		goto done_free_sp;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0x2132,
+ 	    "Async-%s hdl=%x  %8phC.\n", sp->name,
+ 	    sp->handle, fcport->port_name);
+ 
+ 	return rval;
+ done_free_sp:
+ 	sp->free(sp);
+ 	fcport->flags &= ~FCF_ASYNC_SENT;
+ 	return rval;
+ }
+ 
+ /* GPN_FT + GNN_FT*/
+ static int qla2x00_is_a_vp(scsi_qla_host_t *vha, u64 wwn)
+ {
+ 	struct qla_hw_data *ha = vha->hw;
+ 	scsi_qla_host_t *vp;
+ 	unsigned long flags;
+ 	u64 twwn;
+ 	int rc = 0;
+ 
+ 	if (!ha->num_vhosts)
+ 		return 0;
+ 
+ 	spin_lock_irqsave(&ha->vport_slock, flags);
+ 	list_for_each_entry(vp, &ha->vp_list, list) {
+ 		twwn = wwn_to_u64(vp->port_name);
+ 		if (wwn == twwn) {
+ 			rc = 1;
+ 			break;
+ 		}
+ 	}
+ 	spin_unlock_irqrestore(&ha->vport_slock, flags);
+ 
+ 	return rc;
+ }
+ 
+ void qla24xx_async_gnnft_done(scsi_qla_host_t *vha, srb_t *sp)
+ {
+ 	fc_port_t *fcport;
+ 	u32 i, rc;
+ 	bool found;
+ 	struct fab_scan_rp *rp;
+ 	unsigned long flags;
+ 	u8 recheck = 0;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "%s enter\n", __func__);
+ 
+ 	if (sp->gen1 != vha->hw->base_qpair->chip_reset) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 		    "%s scan stop due to chip reset %x/%x\n",
+ 		    sp->name, sp->gen1, vha->hw->base_qpair->chip_reset);
+ 		goto out;
+ 	}
+ 
+ 	rc = sp->rc;
+ 	if (rc) {
+ 		vha->scan.scan_retry++;
+ 		if (vha->scan.scan_retry < MAX_SCAN_RETRIES) {
+ 			set_bit(LOCAL_LOOP_UPDATE, &vha->dpc_flags);
+ 			set_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags);
+ 		} else {
+ 			ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 			    "Fabric scan failed on all retries.\n");
+ 		}
+ 		goto out;
+ 	}
+ 	vha->scan.scan_retry = 0;
+ 
+ 	list_for_each_entry(fcport, &vha->vp_fcports, list)
+ 		fcport->scan_state = QLA_FCPORT_SCAN;
+ 
+ 	for (i = 0; i < vha->hw->max_fibre_devices; i++) {
+ 		u64 wwn;
+ 
+ 		rp = &vha->scan.l[i];
+ 		found = false;
+ 
+ 		wwn = wwn_to_u64(rp->port_name);
+ 		if (wwn == 0)
+ 			continue;
+ 
+ 		if (!memcmp(rp->port_name, vha->port_name, WWN_SIZE))
+ 			continue;
+ 
+ 		/* Bypass reserved domain fields. */
+ 		if ((rp->id.b.domain & 0xf0) == 0xf0)
+ 			continue;
+ 
+ 		/* Bypass virtual ports of the same host. */
+ 		if (qla2x00_is_a_vp(vha, wwn))
+ 			continue;
+ 
+ 		list_for_each_entry(fcport, &vha->vp_fcports, list) {
+ 			if (memcmp(rp->port_name, fcport->port_name, WWN_SIZE))
+ 				continue;
+ 			fcport->rscn_rcvd = 0;
+ 			fcport->scan_state = QLA_FCPORT_FOUND;
+ 			found = true;
+ 			/*
+ 			 * If device was not a fabric device before.
+ 			 */
+ 			if ((fcport->flags & FCF_FABRIC_DEVICE) == 0) {
+ 				qla2x00_clear_loop_id(fcport);
+ 				fcport->flags |= FCF_FABRIC_DEVICE;
+ 			} else if (fcport->d_id.b24 != rp->id.b24) {
+ 				qlt_schedule_sess_for_deletion(fcport);
+ 			}
+ 			fcport->d_id.b24 = rp->id.b24;
+ 			break;
+ 		}
+ 
+ 		if (!found) {
+ 			ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 			    "%s %d %8phC post new sess\n",
+ 			    __func__, __LINE__, rp->port_name);
+ 			qla24xx_post_newsess_work(vha, &rp->id, rp->port_name,
+ 			    rp->node_name, NULL, rp->fc4type);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Logout all previous fabric dev marked lost, except FCP2 devices.
+ 	 */
+ 	list_for_each_entry(fcport, &vha->vp_fcports, list) {
+ 		if ((fcport->flags & FCF_FABRIC_DEVICE) == 0) {
+ 			fcport->rscn_rcvd = 0;
+ 			continue;
+ 		}
+ 
+ 		if (fcport->scan_state != QLA_FCPORT_FOUND) {
+ 			fcport->rscn_rcvd = 0;
+ 			if ((qla_dual_mode_enabled(vha) ||
+ 				qla_ini_mode_enabled(vha)) &&
+ 			    atomic_read(&fcport->state) == FCS_ONLINE) {
+ 				qla2x00_mark_device_lost(vha, fcport,
+ 				    ql2xplogiabsentdevice, 0);
+ 
+ 				if (fcport->loop_id != FC_NO_LOOP_ID &&
+ 				    (fcport->flags & FCF_FCP2_DEVICE) == 0) {
+ 					ql_dbg(ql_dbg_disc, vha, 0x20f0,
+ 					    "%s %d %8phC post del sess\n",
+ 					    __func__, __LINE__,
+ 					    fcport->port_name);
+ 
+ 					qlt_schedule_sess_for_deletion(fcport);
+ 					continue;
+ 				}
+ 			}
+ 		} else {
+ 			if (fcport->rscn_rcvd ||
+ 			    fcport->disc_state != DSC_LOGIN_COMPLETE) {
+ 				fcport->rscn_rcvd = 0;
+ 				qla24xx_fcport_handle_login(vha, fcport);
+ 			}
+ 		}
+ 	}
+ 
+ 	recheck = 1;
+ out:
+ 	qla24xx_sp_unmap(vha, sp);
+ 	spin_lock_irqsave(&vha->work_lock, flags);
+ 	vha->scan.scan_flags &= ~SF_SCANNING;
+ 	spin_unlock_irqrestore(&vha->work_lock, flags);
+ 
+ 	if (recheck) {
+ 		list_for_each_entry(fcport, &vha->vp_fcports, list) {
+ 			if (fcport->rscn_rcvd) {
+ 				set_bit(LOCAL_LOOP_UPDATE, &vha->dpc_flags);
+ 				set_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags);
+ 				break;
+ 			}
+ 		}
+ 	}
+ }
+ 
+ static void qla2x00_find_free_fcp_nvme_slot(struct scsi_qla_host *vha,
+ 	struct srb *sp)
+ {
+ 	struct qla_hw_data *ha = vha->hw;
+ 	int num_fibre_dev = ha->max_fibre_devices;
+ 	struct ct_sns_req *ct_req =
+ 		(struct ct_sns_req *)sp->u.iocb_cmd.u.ctarg.req;
+ 	struct ct_sns_gpnft_rsp *ct_rsp =
+ 		(struct ct_sns_gpnft_rsp *)sp->u.iocb_cmd.u.ctarg.rsp;
+ 	struct ct_sns_gpn_ft_data *d;
+ 	struct fab_scan_rp *rp;
+ 	u16 cmd = be16_to_cpu(ct_req->command);
+ 	u8 fc4_type = sp->gen2;
+ 	int i, j, k;
+ 	port_id_t id;
+ 	u8 found;
+ 	u64 wwn;
+ 
+ 	j = 0;
+ 	for (i = 0; i < num_fibre_dev; i++) {
+ 		d  = &ct_rsp->entries[i];
+ 
+ 		id.b.rsvd_1 = 0;
+ 		id.b.domain = d->port_id[0];
+ 		id.b.area   = d->port_id[1];
+ 		id.b.al_pa  = d->port_id[2];
+ 		wwn = wwn_to_u64(d->port_name);
+ 
+ 		if (id.b24 == 0 || wwn == 0)
+ 			continue;
+ 
+ 		if (fc4_type == FC4_TYPE_FCP_SCSI) {
+ 			if (cmd == GPN_FT_CMD) {
+ 				rp = &vha->scan.l[j];
+ 				rp->id = id;
+ 				memcpy(rp->port_name, d->port_name, 8);
+ 				j++;
+ 				rp->fc4type = FS_FC4TYPE_FCP;
+ 			} else {
+ 				for (k = 0; k < num_fibre_dev; k++) {
+ 					rp = &vha->scan.l[k];
+ 					if (id.b24 == rp->id.b24) {
+ 						memcpy(rp->node_name,
+ 						    d->port_name, 8);
+ 						break;
+ 					}
+ 				}
+ 			}
+ 		} else {
+ 			/* Search if the fibre device supports FC4_TYPE_NVME */
+ 			if (cmd == GPN_FT_CMD) {
+ 				found = 0;
+ 
+ 				for (k = 0; k < num_fibre_dev; k++) {
+ 					rp = &vha->scan.l[k];
+ 					if (!memcmp(rp->port_name,
+ 					    d->port_name, 8)) {
+ 						/*
+ 						 * Supports FC-NVMe & FCP
+ 						 */
+ 						rp->fc4type |= FS_FC4TYPE_NVME;
+ 						found = 1;
+ 						break;
+ 					}
+ 				}
+ 
+ 				/* We found new FC-NVMe only port */
+ 				if (!found) {
+ 					for (k = 0; k < num_fibre_dev; k++) {
+ 						rp = &vha->scan.l[k];
+ 						if (wwn_to_u64(rp->port_name)) {
+ 							continue;
+ 						} else {
+ 							rp->id = id;
+ 							memcpy(rp->port_name,
+ 							    d->port_name, 8);
+ 							rp->fc4type =
+ 							    FS_FC4TYPE_NVME;
+ 							break;
+ 						}
+ 					}
+ 				}
+ 			} else {
+ 				for (k = 0; k < num_fibre_dev; k++) {
+ 					rp = &vha->scan.l[k];
+ 					if (id.b24 == rp->id.b24) {
+ 						memcpy(rp->node_name,
+ 						    d->port_name, 8);
+ 						break;
+ 					}
+ 				}
+ 			}
+ 		}
+ 	}
+ }
+ 
+ static void qla2x00_async_gpnft_gnnft_sp_done(void *s, int res)
+ {
+ 	struct srb *sp = s;
+ 	struct scsi_qla_host *vha = sp->vha;
+ 	struct qla_work_evt *e;
+ 	struct ct_sns_req *ct_req =
+ 		(struct ct_sns_req *)sp->u.iocb_cmd.u.ctarg.req;
+ 	u16 cmd = be16_to_cpu(ct_req->command);
+ 	u8 fc4_type = sp->gen2;
+ 	unsigned long flags;
+ 
+ 	/* gen2 field is holding the fc4type */
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "Async done-%s res %x FC4Type %x\n",
+ 	    sp->name, res, sp->gen2);
+ 
+ 	if (res) {
+ 		unsigned long flags;
+ 
+ 		sp->free(sp);
+ 		spin_lock_irqsave(&vha->work_lock, flags);
+ 		vha->scan.scan_flags &= ~SF_SCANNING;
+ 		vha->scan.scan_retry++;
+ 		spin_unlock_irqrestore(&vha->work_lock, flags);
+ 
+ 		if (vha->scan.scan_retry < MAX_SCAN_RETRIES) {
+ 			set_bit(LOCAL_LOOP_UPDATE, &vha->dpc_flags);
+ 			set_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags);
+ 			qla2xxx_wake_dpc(vha);
+ 		} else {
+ 			ql_dbg(ql_dbg_disc, sp->vha, 0xffff,
+ 			    "Async done-%s rescan failed on all retries\n",
+ 			    sp->name);
+ 		}
+ 		return;
+ 	}
+ 
+ 	if (!res)
+ 		qla2x00_find_free_fcp_nvme_slot(vha, sp);
+ 
+ 	if ((fc4_type == FC4_TYPE_FCP_SCSI) && vha->flags.nvme_enabled &&
+ 	    cmd == GNN_FT_CMD) {
+ 		del_timer(&sp->u.iocb_cmd.timer);
+ 		spin_lock_irqsave(&vha->work_lock, flags);
+ 		vha->scan.scan_flags &= ~SF_SCANNING;
+ 		spin_unlock_irqrestore(&vha->work_lock, flags);
+ 
+ 		e = qla2x00_alloc_work(vha, QLA_EVT_GPNFT);
+ 		if (!e) {
+ 			/*
+ 			 * please ignore kernel warning. Otherwise,
+ 			 * we have mem leak.
+ 			 */
+ 			if (sp->u.iocb_cmd.u.ctarg.req) {
+ 				dma_free_coherent(&vha->hw->pdev->dev,
+ 				    sp->u.iocb_cmd.u.ctarg.req_allocated_size,
+ 				    sp->u.iocb_cmd.u.ctarg.req,
+ 				    sp->u.iocb_cmd.u.ctarg.req_dma);
+ 				sp->u.iocb_cmd.u.ctarg.req = NULL;
+ 			}
+ 			if (sp->u.iocb_cmd.u.ctarg.rsp) {
+ 				dma_free_coherent(&vha->hw->pdev->dev,
+ 				    sp->u.iocb_cmd.u.ctarg.rsp_allocated_size,
+ 				    sp->u.iocb_cmd.u.ctarg.rsp,
+ 				    sp->u.iocb_cmd.u.ctarg.rsp_dma);
+ 				sp->u.iocb_cmd.u.ctarg.rsp = NULL;
+ 			}
+ 
+ 			ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 			    "Async done-%s unable to alloc work element\n",
+ 			    sp->name);
+ 			sp->free(sp);
+ 			set_bit(LOCAL_LOOP_UPDATE, &vha->dpc_flags);
+ 			set_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags);
+ 			return;
+ 		}
+ 		e->u.gpnft.fc4_type = FC4_TYPE_NVME;
+ 		sp->rc = res;
+ 		e->u.gpnft.sp = sp;
+ 
+ 		qla2x00_post_work(vha, e);
+ 		return;
+ 	}
+ 
+ 	if (cmd == GPN_FT_CMD)
+ 		e = qla2x00_alloc_work(vha, QLA_EVT_GPNFT_DONE);
+ 	else
+ 		e = qla2x00_alloc_work(vha, QLA_EVT_GNNFT_DONE);
+ 	if (!e) {
+ 		/* please ignore kernel warning. Otherwise, we have mem leak. */
+ 		if (sp->u.iocb_cmd.u.ctarg.req) {
+ 			dma_free_coherent(&vha->hw->pdev->dev,
+ 			    sp->u.iocb_cmd.u.ctarg.req_allocated_size,
+ 			    sp->u.iocb_cmd.u.ctarg.req,
+ 			    sp->u.iocb_cmd.u.ctarg.req_dma);
+ 			sp->u.iocb_cmd.u.ctarg.req = NULL;
+ 		}
+ 		if (sp->u.iocb_cmd.u.ctarg.rsp) {
+ 			dma_free_coherent(&vha->hw->pdev->dev,
+ 			    sp->u.iocb_cmd.u.ctarg.rsp_allocated_size,
+ 			    sp->u.iocb_cmd.u.ctarg.rsp,
+ 			    sp->u.iocb_cmd.u.ctarg.rsp_dma);
+ 			sp->u.iocb_cmd.u.ctarg.rsp = NULL;
+ 		}
+ 
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 		    "Async done-%s unable to alloc work element\n",
+ 		    sp->name);
+ 		sp->free(sp);
+ 		set_bit(LOCAL_LOOP_UPDATE, &vha->dpc_flags);
+ 		set_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags);
+ 		return;
+ 	}
+ 
+ 	sp->rc = res;
+ 	e->u.iosb.sp = sp;
+ 
+ 	qla2x00_post_work(vha, e);
+ }
+ 
+ /*
+  * Get WWNN list for fc4_type
+  *
+  * It is assumed the same SRB is re-used from GPNFT to avoid
+  * mem free & re-alloc
+  */
+ static int qla24xx_async_gnnft(scsi_qla_host_t *vha, struct srb *sp,
+     u8 fc4_type)
+ {
+ 	int rval = QLA_FUNCTION_FAILED;
+ 	struct ct_sns_req *ct_req;
+ 	struct ct_sns_pkt *ct_sns;
+ 	unsigned long flags;
+ 
+ 	if (!vha->flags.online) {
+ 		spin_lock_irqsave(&vha->work_lock, flags);
+ 		vha->scan.scan_flags &= ~SF_SCANNING;
+ 		spin_unlock_irqrestore(&vha->work_lock, flags);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	if (!sp->u.iocb_cmd.u.ctarg.req || !sp->u.iocb_cmd.u.ctarg.rsp) {
+ 		ql_log(ql_log_warn, vha, 0xffff,
+ 		    "%s: req %p rsp %p are not setup\n",
+ 		    __func__, sp->u.iocb_cmd.u.ctarg.req,
+ 		    sp->u.iocb_cmd.u.ctarg.rsp);
+ 		spin_lock_irqsave(&vha->work_lock, flags);
+ 		vha->scan.scan_flags &= ~SF_SCANNING;
+ 		spin_unlock_irqrestore(&vha->work_lock, flags);
+ 		WARN_ON(1);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xfffff,
+ 	    "%s: FC4Type %x, CT-PASSTRHU %s command ctarg rsp size %d, ctarg req size %d\n",
+ 	    __func__, fc4_type, sp->name, sp->u.iocb_cmd.u.ctarg.rsp_size,
+ 	     sp->u.iocb_cmd.u.ctarg.req_size);
+ 
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "gnnft";
+ 	sp->gen1 = vha->hw->base_qpair->chip_reset;
+ 	sp->gen2 = fc4_type;
+ 
+ 	sp->u.iocb_cmd.timeout = qla2x00_async_iocb_timeout;
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	memset(sp->u.iocb_cmd.u.ctarg.rsp, 0, sp->u.iocb_cmd.u.ctarg.rsp_size);
+ 	memset(sp->u.iocb_cmd.u.ctarg.req, 0, sp->u.iocb_cmd.u.ctarg.req_size);
+ 
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.req;
+ 	/* CT_IU preamble  */
+ 	ct_req = qla2x00_prep_ct_req(ct_sns, GNN_FT_CMD,
+ 	    sp->u.iocb_cmd.u.ctarg.rsp_size);
+ 
+ 	/* GPN_FT req */
+ 	ct_req->req.gpn_ft.port_type = fc4_type;
+ 
+ 	sp->u.iocb_cmd.u.ctarg.req_size = GNN_FT_REQ_SIZE;
+ 	sp->u.iocb_cmd.u.ctarg.nport_handle = NPH_SNS;
+ 
+ 	sp->done = qla2x00_async_gpnft_gnnft_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 	if (rval != QLA_SUCCESS)
+ 		goto done_free_sp;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "Async-%s hdl=%x FC4Type %x.\n", sp->name,
+ 	    sp->handle, ct_req->req.gpn_ft.port_type);
+ 	return rval;
+ 
+ done_free_sp:
+ 	if (sp->u.iocb_cmd.u.ctarg.req) {
+ 		dma_free_coherent(&vha->hw->pdev->dev,
+ 		    sp->u.iocb_cmd.u.ctarg.req_allocated_size,
+ 		    sp->u.iocb_cmd.u.ctarg.req,
+ 		    sp->u.iocb_cmd.u.ctarg.req_dma);
+ 		sp->u.iocb_cmd.u.ctarg.req = NULL;
+ 	}
+ 	if (sp->u.iocb_cmd.u.ctarg.rsp) {
+ 		dma_free_coherent(&vha->hw->pdev->dev,
+ 		    sp->u.iocb_cmd.u.ctarg.rsp_allocated_size,
+ 		    sp->u.iocb_cmd.u.ctarg.rsp,
+ 		    sp->u.iocb_cmd.u.ctarg.rsp_dma);
+ 		sp->u.iocb_cmd.u.ctarg.rsp = NULL;
+ 	}
+ 
+ 	sp->free(sp);
+ 
+ 	return rval;
+ } /* GNNFT */
+ 
+ void qla24xx_async_gpnft_done(scsi_qla_host_t *vha, srb_t *sp)
+ {
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "%s enter\n", __func__);
+ 	del_timer(&sp->u.iocb_cmd.timer);
+ 	qla24xx_async_gnnft(vha, sp, sp->gen2);
+ }
+ 
+ /* Get WWPN list for certain fc4_type */
+ int qla24xx_async_gpnft(scsi_qla_host_t *vha, u8 fc4_type, srb_t *sp)
+ {
+ 	int rval = QLA_FUNCTION_FAILED;
+ 	struct ct_sns_req       *ct_req;
+ 	struct ct_sns_pkt *ct_sns;
+ 	u32 rspsz;
+ 	unsigned long flags;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "%s enter\n", __func__);
+ 
+ 	if (!vha->flags.online)
+ 		return rval;
+ 
+ 	spin_lock_irqsave(&vha->work_lock, flags);
+ 	if (vha->scan.scan_flags & SF_SCANNING) {
+ 		spin_unlock_irqrestore(&vha->work_lock, flags);
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff, "scan active\n");
+ 		return rval;
+ 	}
+ 	vha->scan.scan_flags |= SF_SCANNING;
+ 	spin_unlock_irqrestore(&vha->work_lock, flags);
+ 
+ 	if (fc4_type == FC4_TYPE_FCP_SCSI) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 		    "%s: Performing FCP Scan\n", __func__);
+ 
+ 		if (sp)
+ 			sp->free(sp); /* should not happen */
+ 
+ 		sp = qla2x00_get_sp(vha, NULL, GFP_KERNEL);
+ 		if (!sp) {
+ 			spin_lock_irqsave(&vha->work_lock, flags);
+ 			vha->scan.scan_flags &= ~SF_SCANNING;
+ 			spin_unlock_irqrestore(&vha->work_lock, flags);
+ 			return rval;
+ 		}
+ 
+ 		sp->u.iocb_cmd.u.ctarg.req = dma_zalloc_coherent(
+ 			&vha->hw->pdev->dev, sizeof(struct ct_sns_pkt),
+ 			&sp->u.iocb_cmd.u.ctarg.req_dma, GFP_KERNEL);
+ 		sp->u.iocb_cmd.u.ctarg.req_allocated_size = sizeof(struct ct_sns_pkt);
+ 		if (!sp->u.iocb_cmd.u.ctarg.req) {
+ 			ql_log(ql_log_warn, vha, 0xffff,
+ 			    "Failed to allocate ct_sns request.\n");
+ 			spin_lock_irqsave(&vha->work_lock, flags);
+ 			vha->scan.scan_flags &= ~SF_SCANNING;
+ 			spin_unlock_irqrestore(&vha->work_lock, flags);
+ 			goto done_free_sp;
+ 		}
+ 		sp->u.iocb_cmd.u.ctarg.req_size = GPN_FT_REQ_SIZE;
+ 
+ 		rspsz = sizeof(struct ct_sns_gpnft_rsp) +
+ 			((vha->hw->max_fibre_devices - 1) *
+ 			    sizeof(struct ct_sns_gpn_ft_data));
+ 
+ 		sp->u.iocb_cmd.u.ctarg.rsp = dma_zalloc_coherent(
+ 			&vha->hw->pdev->dev, rspsz,
+ 			&sp->u.iocb_cmd.u.ctarg.rsp_dma, GFP_KERNEL);
+ 		sp->u.iocb_cmd.u.ctarg.rsp_allocated_size = sizeof(struct ct_sns_pkt);
+ 		if (!sp->u.iocb_cmd.u.ctarg.rsp) {
+ 			ql_log(ql_log_warn, vha, 0xffff,
+ 			    "Failed to allocate ct_sns request.\n");
+ 			spin_lock_irqsave(&vha->work_lock, flags);
+ 			vha->scan.scan_flags &= ~SF_SCANNING;
+ 			spin_unlock_irqrestore(&vha->work_lock, flags);
+ 			goto done_free_sp;
+ 		}
+ 		sp->u.iocb_cmd.u.ctarg.rsp_size = rspsz;
+ 
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 		    "%s scan list size %d\n", __func__, vha->scan.size);
+ 
+ 		memset(vha->scan.l, 0, vha->scan.size);
+ 	} else if (!sp) {
+ 		ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 		    "NVME scan did not provide SP\n");
+ 		return rval;
+ 	}
+ 
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "gpnft";
+ 	sp->gen1 = vha->hw->base_qpair->chip_reset;
+ 	sp->gen2 = fc4_type;
+ 
+ 	sp->u.iocb_cmd.timeout = qla2x00_async_iocb_timeout;
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	rspsz = sizeof(struct ct_sns_gpnft_rsp) +
+ 		((vha->hw->max_fibre_devices - 1) *
+ 		    sizeof(struct ct_sns_gpn_ft_data));
+ 
+ 	ct_sns = (struct ct_sns_pkt *)sp->u.iocb_cmd.u.ctarg.req;
+ 	/* CT_IU preamble  */
+ 	ct_req = qla2x00_prep_ct_req(ct_sns, GPN_FT_CMD, rspsz);
+ 
+ 	/* GPN_FT req */
+ 	ct_req->req.gpn_ft.port_type = fc4_type;
+ 
+ 	sp->u.iocb_cmd.u.ctarg.nport_handle = NPH_SNS;
+ 
+ 	sp->done = qla2x00_async_gpnft_gnnft_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 	if (rval != QLA_SUCCESS) {
+ 		spin_lock_irqsave(&vha->work_lock, flags);
+ 		vha->scan.scan_flags &= ~SF_SCANNING;
+ 		spin_unlock_irqrestore(&vha->work_lock, flags);
+ 		goto done_free_sp;
+ 	}
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "Async-%s hdl=%x FC4Type %x.\n", sp->name,
+ 	    sp->handle, ct_req->req.gpn_ft.port_type);
+ 	return rval;
+ 
+ done_free_sp:
+ 	if (sp->u.iocb_cmd.u.ctarg.req) {
+ 		dma_free_coherent(&vha->hw->pdev->dev,
+ 		    sp->u.iocb_cmd.u.ctarg.req_allocated_size,
+ 		    sp->u.iocb_cmd.u.ctarg.req,
+ 		    sp->u.iocb_cmd.u.ctarg.req_dma);
+ 		sp->u.iocb_cmd.u.ctarg.req = NULL;
+ 	}
+ 	if (sp->u.iocb_cmd.u.ctarg.rsp) {
+ 		dma_free_coherent(&vha->hw->pdev->dev,
+ 		    sp->u.iocb_cmd.u.ctarg.rsp_allocated_size,
+ 		    sp->u.iocb_cmd.u.ctarg.rsp,
+ 		    sp->u.iocb_cmd.u.ctarg.rsp_dma);
+ 		sp->u.iocb_cmd.u.ctarg.rsp = NULL;
+ 	}
+ 
+ 	sp->free(sp);
+ 
+ 	return rval;
+ }
+ 
+ void qla_scan_work_fn(struct work_struct *work)
+ {
+ 	struct fab_scan *s = container_of(to_delayed_work(work),
+ 	    struct fab_scan, scan_work);
+ 	struct scsi_qla_host *vha = container_of(s, struct scsi_qla_host,
+ 	    scan);
+ 	unsigned long flags;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "%s: schedule loop resync\n", __func__);
+ 	set_bit(LOCAL_LOOP_UPDATE, &vha->dpc_flags);
+ 	set_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags);
+ 	qla2xxx_wake_dpc(vha);
+ 	spin_lock_irqsave(&vha->work_lock, flags);
+ 	vha->scan.scan_flags &= ~SF_QUEUED;
+ 	spin_unlock_irqrestore(&vha->work_lock, flags);
+ }
+ 
+ /* GNN_ID */
+ void qla24xx_handle_gnnid_event(scsi_qla_host_t *vha, struct event_arg *ea)
+ {
+ 	qla24xx_post_gnl_work(vha, ea->fcport);
+ }
+ 
+ static void qla2x00_async_gnnid_sp_done(void *s, int res)
+ {
+ 	struct srb *sp = s;
+ 	struct scsi_qla_host *vha = sp->vha;
+ 	fc_port_t *fcport = sp->fcport;
+ 	u8 *node_name = fcport->ct_desc.ct_sns->p.rsp.rsp.gnn_id.node_name;
+ 	struct event_arg ea;
+ 	u64 wwnn;
+ 
+ 	fcport->flags &= ~FCF_ASYNC_SENT;
+ 	wwnn = wwn_to_u64(node_name);
+ 	if (wwnn)
+ 		memcpy(fcport->node_name, node_name, WWN_SIZE);
+ 
+ 	memset(&ea, 0, sizeof(ea));
+ 	ea.fcport = fcport;
+ 	ea.sp = sp;
+ 	ea.rc = res;
+ 	ea.event = FCME_GNNID_DONE;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0x204f,
+ 	    "Async done-%s res %x, WWPN %8phC %8phC\n",
+ 	    sp->name, res, fcport->port_name, fcport->node_name);
+ 
+ 	qla2x00_fcport_event_handler(vha, &ea);
+ 
+ 	sp->free(sp);
+ }
+ 
+ int qla24xx_async_gnnid(scsi_qla_host_t *vha, fc_port_t *fcport)
+ {
+ 	int rval = QLA_FUNCTION_FAILED;
+ 	struct ct_sns_req       *ct_req;
+ 	srb_t *sp;
+ 
+ 	if (!vha->flags.online || (fcport->flags & FCF_ASYNC_SENT))
+ 		return rval;
+ 
+ 	fcport->disc_state = DSC_GNN_ID;
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_ATOMIC);
+ 	if (!sp)
+ 		goto done;
+ 
+ 	fcport->flags |= FCF_ASYNC_SENT;
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "gnnid";
+ 	sp->gen1 = fcport->rscn_gen;
+ 	sp->gen2 = fcport->login_gen;
+ 
+ 	sp->u.iocb_cmd.timeout = qla2x00_async_iocb_timeout;
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	/* CT_IU preamble  */
+ 	ct_req = qla2x00_prep_ct_req(fcport->ct_desc.ct_sns, GNN_ID_CMD,
+ 	    GNN_ID_RSP_SIZE);
+ 
+ 	/* GNN_ID req */
+ 	ct_req->req.port_id.port_id[0] = fcport->d_id.b.domain;
+ 	ct_req->req.port_id.port_id[1] = fcport->d_id.b.area;
+ 	ct_req->req.port_id.port_id[2] = fcport->d_id.b.al_pa;
+ 
+ 
+ 	/* req & rsp use the same buffer */
+ 	sp->u.iocb_cmd.u.ctarg.req = fcport->ct_desc.ct_sns;
+ 	sp->u.iocb_cmd.u.ctarg.req_dma = fcport->ct_desc.ct_sns_dma;
+ 	sp->u.iocb_cmd.u.ctarg.rsp = fcport->ct_desc.ct_sns;
+ 	sp->u.iocb_cmd.u.ctarg.rsp_dma = fcport->ct_desc.ct_sns_dma;
+ 	sp->u.iocb_cmd.u.ctarg.req_size = GNN_ID_REQ_SIZE;
+ 	sp->u.iocb_cmd.u.ctarg.rsp_size = GNN_ID_RSP_SIZE;
+ 	sp->u.iocb_cmd.u.ctarg.nport_handle = NPH_SNS;
+ 
+ 	sp->done = qla2x00_async_gnnid_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 	if (rval != QLA_SUCCESS)
+ 		goto done_free_sp;
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "Async-%s - %8phC hdl=%x loopid=%x portid %06x.\n",
+ 	    sp->name, fcport->port_name,
+ 	    sp->handle, fcport->loop_id, fcport->d_id.b24);
+ 	return rval;
+ 
+ done_free_sp:
+ 	sp->free(sp);
+ done:
+ 	return rval;
+ }
+ 
+ int qla24xx_post_gnnid_work(struct scsi_qla_host *vha, fc_port_t *fcport)
+ {
+ 	struct qla_work_evt *e;
+ 	int ls;
+ 
+ 	ls = atomic_read(&vha->loop_state);
+ 	if (((ls != LOOP_READY) && (ls != LOOP_UP)) ||
+ 		test_bit(UNLOADING, &vha->dpc_flags))
+ 		return 0;
+ 
+ 	e = qla2x00_alloc_work(vha, QLA_EVT_GNNID);
+ 	if (!e)
+ 		return QLA_FUNCTION_FAILED;
+ 
+ 	e->u.fcport.fcport = fcport;
+ 	return qla2x00_post_work(vha, e);
+ }
+ 
+ /* GPFN_ID */
+ void qla24xx_handle_gfpnid_event(scsi_qla_host_t *vha, struct event_arg *ea)
+ {
+ 	fc_port_t *fcport = ea->fcport;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "%s %8phC DS %d LS %d rc %d login %d|%d rscn %d|%d fcpcnt %d\n",
+ 	    __func__, fcport->port_name, fcport->disc_state,
+ 	    fcport->fw_login_state, ea->rc, fcport->login_gen, ea->sp->gen2,
+ 	    fcport->rscn_gen, ea->sp->gen1, vha->fcport_count);
+ 
+ 	if (fcport->disc_state == DSC_DELETE_PEND)
+ 		return;
+ 
+ 	if (ea->sp->gen2 != fcport->login_gen) {
+ 		/* target side must have changed it. */
+ 		ql_dbg(ql_dbg_disc, vha, 0x20d3,
+ 		    "%s %8phC generation changed\n",
+ 		    __func__, fcport->port_name);
+ 		return;
+ 	} else if (ea->sp->gen1 != fcport->rscn_gen) {
+ 		ql_dbg(ql_dbg_disc, vha, 0x20d4, "%s %d %8phC post gidpn\n",
+ 		    __func__, __LINE__, fcport->port_name);
+ 		qla24xx_post_gidpn_work(vha, fcport);
+ 		return;
+ 	}
+ 
+ 	qla24xx_post_gpsc_work(vha, fcport);
+ }
+ 
+ static void qla2x00_async_gfpnid_sp_done(void *s, int res)
+ {
+ 	struct srb *sp = s;
+ 	struct scsi_qla_host *vha = sp->vha;
+ 	fc_port_t *fcport = sp->fcport;
+ 	u8 *fpn = fcport->ct_desc.ct_sns->p.rsp.rsp.gfpn_id.port_name;
+ 	struct event_arg ea;
+ 	u64 wwn;
+ 
+ 	wwn = wwn_to_u64(fpn);
+ 	if (wwn)
+ 		memcpy(fcport->fabric_port_name, fpn, WWN_SIZE);
+ 
+ 	memset(&ea, 0, sizeof(ea));
+ 	ea.fcport = fcport;
+ 	ea.sp = sp;
+ 	ea.rc = res;
+ 	ea.event = FCME_GFPNID_DONE;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0x204f,
+ 	    "Async done-%s res %x, WWPN %8phC %8phC\n",
+ 	    sp->name, res, fcport->port_name, fcport->fabric_port_name);
+ 
+ 	qla2x00_fcport_event_handler(vha, &ea);
+ 
+ 	sp->free(sp);
+ }
+ 
+ int qla24xx_async_gfpnid(scsi_qla_host_t *vha, fc_port_t *fcport)
+ {
+ 	int rval = QLA_FUNCTION_FAILED;
+ 	struct ct_sns_req       *ct_req;
+ 	srb_t *sp;
+ 
+ 	if (!vha->flags.online || (fcport->flags & FCF_ASYNC_SENT))
+ 		return rval;
+ 
+ 	sp = qla2x00_get_sp(vha, fcport, GFP_ATOMIC);
+ 	if (!sp)
+ 		goto done;
+ 
+ 	sp->type = SRB_CT_PTHRU_CMD;
+ 	sp->name = "gfpnid";
+ 	sp->gen1 = fcport->rscn_gen;
+ 	sp->gen2 = fcport->login_gen;
+ 
+ 	sp->u.iocb_cmd.timeout = qla2x00_async_iocb_timeout;
+ 	qla2x00_init_timer(sp, qla2x00_get_async_timeout(vha) + 2);
+ 
+ 	/* CT_IU preamble  */
+ 	ct_req = qla2x00_prep_ct_req(fcport->ct_desc.ct_sns, GFPN_ID_CMD,
+ 	    GFPN_ID_RSP_SIZE);
+ 
+ 	/* GFPN_ID req */
+ 	ct_req->req.port_id.port_id[0] = fcport->d_id.b.domain;
+ 	ct_req->req.port_id.port_id[1] = fcport->d_id.b.area;
+ 	ct_req->req.port_id.port_id[2] = fcport->d_id.b.al_pa;
+ 
+ 
+ 	/* req & rsp use the same buffer */
+ 	sp->u.iocb_cmd.u.ctarg.req = fcport->ct_desc.ct_sns;
+ 	sp->u.iocb_cmd.u.ctarg.req_dma = fcport->ct_desc.ct_sns_dma;
+ 	sp->u.iocb_cmd.u.ctarg.rsp = fcport->ct_desc.ct_sns;
+ 	sp->u.iocb_cmd.u.ctarg.rsp_dma = fcport->ct_desc.ct_sns_dma;
+ 	sp->u.iocb_cmd.u.ctarg.req_size = GFPN_ID_REQ_SIZE;
+ 	sp->u.iocb_cmd.u.ctarg.rsp_size = GFPN_ID_RSP_SIZE;
+ 	sp->u.iocb_cmd.u.ctarg.nport_handle = NPH_SNS;
+ 
+ 	sp->done = qla2x00_async_gfpnid_sp_done;
+ 
+ 	rval = qla2x00_start_sp(sp);
+ 	if (rval != QLA_SUCCESS)
+ 		goto done_free_sp;
+ 
+ 	ql_dbg(ql_dbg_disc, vha, 0xffff,
+ 	    "Async-%s - %8phC hdl=%x loopid=%x portid %06x.\n",
+ 	    sp->name, fcport->port_name,
+ 	    sp->handle, fcport->loop_id, fcport->d_id.b24);
+ 	return rval;
+ 
+ done_free_sp:
+ 	sp->free(sp);
+ 	fcport->flags &= ~FCF_ASYNC_SENT;
+ done:
+ 	return rval;
+ }
+ 
+ int qla24xx_post_gfpnid_work(struct scsi_qla_host *vha, fc_port_t *fcport)
+ {
+ 	struct qla_work_evt *e;
+ 	int ls;
+ 
+ 	ls = atomic_read(&vha->loop_state);
+ 	if (((ls != LOOP_READY) && (ls != LOOP_UP)) ||
+ 		test_bit(UNLOADING, &vha->dpc_flags))
+ 		return 0;
+ 
+ 	e = qla2x00_alloc_work(vha, QLA_EVT_GFPNID);
+ 	if (!e)
+ 		return QLA_FUNCTION_FAILED;
+ 
+ 	e->u.fcport.fcport = fcport;
+ 	return qla2x00_post_work(vha, e);
+ }
++>>>>>>> b5f3bc39a0e8 (scsi: qla2xxx: Fix inconsistent DMA mem alloc/free)
diff --git a/drivers/scsi/qla2xxx/qla_def.h b/drivers/scsi/qla2xxx/qla_def.h
index d0397ca42fad..f7544f0d0ef7 100644
--- a/drivers/scsi/qla2xxx/qla_def.h
+++ b/drivers/scsi/qla2xxx/qla_def.h
@@ -339,6 +339,8 @@ struct ct_arg {
 	dma_addr_t	rsp_dma;
 	u32		req_size;
 	u32		rsp_size;
+	u32		req_allocated_size;
+	u32		rsp_allocated_size;
 	void		*req;
 	void		*rsp;
 };
* Unmerged path drivers/scsi/qla2xxx/qla_gs.c
