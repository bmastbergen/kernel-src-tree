crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [crypto] chelsio: Remove separate buffer used for DMA map B0 block in CCM (Arjun Vynipadath) [1595086]
Rebuild_FUZZ: 91.97%
commit-author Harsh Jain <harsh@chelsio.com>
commit 4262c98aab95119ec0810b5ec4be521dda1b28b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/4262c98a.failed

Extends memory required for IV to include B0 Block and DMA map in
single operation.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 4262c98aab95119ec0810b5ec4be521dda1b28b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/chelsio/chcr_crypto.h
diff --cc drivers/crypto/chelsio/chcr_algo.c
index 65f5b237b3c9,b916c4eb608c..000000000000
mode 100755,100644..100755
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -140,100 -198,21 +140,109 @@@ static void chcr_verify_tag(struct aead
  		*err = 0;
  }
  
 -static inline void chcr_handle_aead_resp(struct aead_request *req,
 -					 unsigned char *input,
 -					 int err)
 +/*
 + *	chcr_handle_resp - Unmap the DMA buffers associated with the request
 + *	@req: crypto request
 + */
 +int chcr_handle_resp(struct crypto_async_request *req, unsigned char *input,
 +			 int err)
  {
++<<<<<<< HEAD
 +	struct crypto_tfm *tfm = req->tfm;
 +	struct chcr_context *ctx = crypto_tfm_ctx(tfm);
 +	struct uld_ctx *u_ctx = ULD_CTX(ctx);
 +	struct chcr_req_ctx ctx_req;
 +	unsigned int digestsize, updated_digestsize;
 +	struct adapter *adap = padap(ctx->dev);
 +
 +	switch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {
 +	case CRYPTO_ALG_TYPE_AEAD:
 +		ctx_req.req.aead_req = aead_request_cast(req);
 +		ctx_req.ctx.reqctx = aead_request_ctx(ctx_req.req.aead_req);
 +		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.ctx.reqctx->dst,
 +			     ctx_req.ctx.reqctx->dst_nents, DMA_FROM_DEVICE);
 +		if (ctx_req.ctx.reqctx->skb) {
 +			kfree_skb(ctx_req.ctx.reqctx->skb);
 +			ctx_req.ctx.reqctx->skb = NULL;
 +		}
 +		free_new_sg(ctx_req.ctx.reqctx->newdstsg);
 +		ctx_req.ctx.reqctx->newdstsg = NULL;
 +		if (ctx_req.ctx.reqctx->verify == VERIFY_SW) {
 +			chcr_verify_tag(ctx_req.req.aead_req, input,
 +					&err);
 +			ctx_req.ctx.reqctx->verify = VERIFY_HW;
 +		}
 +		ctx_req.req.aead_req->base.complete(req, err);
 +		break;
 +
 +	case CRYPTO_ALG_TYPE_ABLKCIPHER:
 +		 err = chcr_handle_cipher_resp(ablkcipher_request_cast(req),
 +					       input, err);
 +		break;
 +
 +	case CRYPTO_ALG_TYPE_AHASH:
 +		ctx_req.req.ahash_req = ahash_request_cast(req);
 +		ctx_req.ctx.ahash_ctx =
 +			ahash_request_ctx(ctx_req.req.ahash_req);
 +		digestsize =
 +			crypto_ahash_digestsize(crypto_ahash_reqtfm(
 +							ctx_req.req.ahash_req));
 +		updated_digestsize = digestsize;
 +		if (digestsize == SHA224_DIGEST_SIZE)
 +			updated_digestsize = SHA256_DIGEST_SIZE;
 +		else if (digestsize == SHA384_DIGEST_SIZE)
 +			updated_digestsize = SHA512_DIGEST_SIZE;
 +		if (ctx_req.ctx.ahash_ctx->skb) {
 +			kfree_skb(ctx_req.ctx.ahash_ctx->skb);
 +			ctx_req.ctx.ahash_ctx->skb = NULL;
 +		}
 +		if (ctx_req.ctx.ahash_ctx->result == 1) {
 +			ctx_req.ctx.ahash_ctx->result = 0;
 +			memcpy(ctx_req.req.ahash_req->result, input +
 +			       sizeof(struct cpl_fw6_pld),
 +			       digestsize);
 +		} else {
 +			memcpy(ctx_req.ctx.ahash_ctx->partial_hash, input +
 +			       sizeof(struct cpl_fw6_pld),
 +			       updated_digestsize);
 +		}
 +		ctx_req.req.ahash_req->base.complete(req, err);
 +		break;
++=======
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 
+ 	chcr_aead_common_exit(req);
+ 	if (reqctx->verify == VERIFY_SW) {
+ 		chcr_verify_tag(req, input, &err);
+ 		reqctx->verify = VERIFY_HW;
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	}
 -	req->base.complete(&req->base, err);
 +	atomic_inc(&adap->chcr_stats.complete);
 +	return err;
 +}
 +
 +/*
 + *	calc_tx_flits_ofld - calculate # of flits for an offload packet
 + *	@skb: the packet
 + *	Returns the number of flits needed for the given offload packet.
 + *	These packets are already fully constructed and no additional headers
 + *	will be added.
 + */
 +static inline unsigned int calc_tx_flits_ofld(const struct sk_buff *skb)
 +{
 +	unsigned int flits, cnt;
 +
 +	if (is_ofld_imm(skb))
 +		return DIV_ROUND_UP(skb->len, 8);
 +
 +	flits = skb_transport_offset(skb) / 8;   /* headers */
 +	cnt = skb_shinfo(skb)->nr_frags;
 +	if (skb_tail_pointer(skb) != skb_transport_header(skb))
 +		cnt++;
 +	return flits + sgl_len(cnt);
  }
  
 -static void get_aes_decrypt_key(unsigned char *dec_key,
 +static inline void get_aes_decrypt_key(unsigned char *dec_key,
  				       const unsigned char *key,
  				       unsigned int keylength)
  {
@@@ -1832,64 -2173,49 +1841,99 @@@ static void chcr_hmac_cra_exit(struct c
  	}
  }
  
++<<<<<<< HEAD
 +static int is_newsg(struct scatterlist *sgl, unsigned int *newents)
 +{
 +	int nents = 0;
 +	int ret = 0;
 +
 +	while (sgl) {
 +		if (sgl->length > CHCR_SG_SIZE)
 +			ret = 1;
 +		nents += DIV_ROUND_UP(sgl->length, CHCR_SG_SIZE);
 +		sgl = sg_next(sgl);
++=======
+ inline void chcr_aead_common_exit(struct aead_request *req)
+ {
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct uld_ctx *u_ctx = ULD_CTX(a_ctx(tfm));
+ 
+ 	chcr_aead_dma_unmap(&u_ctx->lldi.pdev->dev, req, reqctx->op);
+ }
+ 
+ static int chcr_aead_common_init(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL;
+ 
+ 	/* validate key size */
+ 	if (aeadctx->enckey_len == 0)
+ 		goto err;
+ 	if (reqctx->op && req->cryptlen < authsize)
+ 		goto err;
+ 	if (reqctx->b0_len)
+ 		reqctx->scratch_pad = reqctx->iv + IV;
+ 	else
+ 		reqctx->scratch_pad = NULL;
+ 
+ 	error = chcr_aead_dma_map(&ULD_CTX(a_ctx(tfm))->lldi.pdev->dev, req,
+ 				  reqctx->op);
+ 	if (error) {
+ 		error = -ENOMEM;
+ 		goto err;
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	}
 -	reqctx->aad_nents = sg_nents_xlen(req->src, req->assoclen,
 -					  CHCR_SRC_SG_SIZE, 0);
 -	reqctx->src_nents = sg_nents_xlen(req->src, req->cryptlen,
 -					  CHCR_SRC_SG_SIZE, req->assoclen);
 -	return 0;
 -err:
 -	return error;
 +	*newents = nents;
 +	return ret;
 +}
 +
 +static inline void free_new_sg(struct scatterlist *sgl)
 +{
 +	kfree(sgl);
 +}
 +
 +static struct scatterlist *alloc_new_sg(struct scatterlist *sgl,
 +				       unsigned int nents)
 +{
 +	struct scatterlist *newsg, *sg;
 +	int i, len, processed = 0;
 +	struct page *spage;
 +	int offset;
 +
 +	newsg = kmalloc_array(nents, sizeof(struct scatterlist), GFP_KERNEL);
 +	if (!newsg)
 +		return ERR_PTR(-ENOMEM);
 +	sg = newsg;
 +	sg_init_table(sg, nents);
 +	offset = sgl->offset;
 +	spage = sg_page(sgl);
 +	for (i = 0; i < nents; i++) {
 +		len = min_t(u32, sgl->length - processed, CHCR_SG_SIZE);
 +		sg_set_page(sg, spage, len, offset);
 +		processed += len;
 +		offset += len;
 +		if (offset >= PAGE_SIZE) {
 +			offset = offset % PAGE_SIZE;
 +			spage++;
 +		}
 +		if (processed == sgl->length) {
 +			processed = 0;
 +			sgl = sg_next(sgl);
 +			if (!sgl)
 +				break;
 +			spage = sg_page(sgl);
 +			offset = sgl->offset;
 +		}
 +		sg = sg_next(sg);
 +	}
 +	return newsg;
  }
  
 -static int chcr_aead_need_fallback(struct aead_request *req, int dst_nents,
 +static int chcr_aead_need_fallback(struct aead_request *req, int src_nent,
  				   int aadmax, int wrlen,
  				   unsigned short op_type)
  {
@@@ -1922,13 -2247,10 +1966,12 @@@ static int chcr_aead_fallback(struct ae
  
  static struct sk_buff *create_authenc_wr(struct aead_request *req,
  					 unsigned short qid,
- 					 int size,
- 					 unsigned short op_type)
+ 					 int size)
  {
  	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 -	struct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));
 +	struct chcr_context *ctx = crypto_aead_ctx(tfm);
 +	struct uld_ctx *u_ctx = ULD_CTX(ctx);
 +	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
  	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
  	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
  	struct sk_buff *skb = NULL;
@@@ -1945,66 -2266,55 +1988,107 @@@
  	int null = 0;
  	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
  		GFP_ATOMIC;
 -	struct adapter *adap = padap(a_ctx(tfm)->dev);
 +	struct adapter *adap = padap(ctx->dev);
 +
 +	reqctx->newdstsg = NULL;
 +	dst_size = req->cryptlen + (op_type ? -authsize :
 +					      authsize);
 +	if (aeadctx->enckey_len == 0 || (req->cryptlen <= 0))
 +		goto err;
  
 -	if (req->cryptlen == 0)
 -		return NULL;
++<<<<<<< HEAD
 +	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
 +		goto err;
 +	src_nent = sg_nents_for_len(req->src, req->cryptlen);
 +	if (src_nent < 0)
 +		goto err;
  
 +	if (dst_size && is_newsg(req->dst, &nents)) {
 +		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
 +		if (IS_ERR(reqctx->newdstsg))
 +			return ERR_CAST(reqctx->newdstsg);
 +		reqctx->dst = reqctx->newdstsg;
 +	} else {
 +		reqctx->dst = req->dst;
 +	}
 +	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {
 +		null = 1;
 +		assoclen = 0;
 +	}
 +	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
 +					     (op_type ? -authsize : authsize));
 +	if (reqctx->dst_nents < 0) {
 +		pr_err("AUTHENC:Invalid Destination sg entries\n");
 +		error = -EINVAL;
 +		goto err;
 +	}
 +	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
 +	kctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)
 +		- sizeof(chcr_req->key_ctx);
 +	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
 +	if (chcr_aead_need_fallback(req, src_nent + MIN_AUTH_SG,
 +			T6_MAX_AAD_SIZE,
 +			transhdr_len + (sgl_len(src_nent + MIN_AUTH_SG) * 8),
 +				op_type)) {
 +		atomic_inc(&adap->chcr_stats.fallback);
 +		free_new_sg(reqctx->newdstsg);
 +		reqctx->newdstsg = NULL;
 +		return ERR_PTR(chcr_aead_fallback(req, op_type));
++=======
+ 	reqctx->b0_len = 0;
+ 	error = chcr_aead_common_init(req);
+ 	if (error)
+ 		return ERR_PTR(error);
+ 
+ 	if (subtype == CRYPTO_ALG_SUB_TYPE_CBC_NULL ||
+ 		subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {
+ 		null = 1;
+ 		assoclen = 0;
+ 		reqctx->aad_nents = 0;
+ 	}
+ 	dnents = sg_nents_xlen(req->dst, assoclen, CHCR_DST_SG_SIZE, 0);
+ 	dnents += sg_nents_xlen(req->dst, req->cryptlen +
+ 		(reqctx->op ? -authsize : authsize), CHCR_DST_SG_SIZE,
+ 		req->assoclen);
+ 	dnents += MIN_AUTH_SG; // For IV
+ 
+ 	dst_size = get_space_for_phys_dsgl(dnents);
+ 	kctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)
+ 		- sizeof(chcr_req->key_ctx);
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	reqctx->imm = (transhdr_len + assoclen + IV + req->cryptlen) <
+ 			SGE_MAX_WR_LEN;
+ 	temp = reqctx->imm ? roundup(assoclen + IV + req->cryptlen, 16)
+ 			: (sgl_len(reqctx->src_nents + reqctx->aad_nents
+ 			+ MIN_GCM_SG) * 8);
+ 	transhdr_len += temp;
+ 	transhdr_len = roundup(transhdr_len, 16);
+ 
+ 	if (chcr_aead_need_fallback(req, dnents, T6_MAX_AAD_SIZE,
+ 				    transhdr_len, reqctx->op)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		chcr_aead_common_exit(req);
+ 		return ERR_PTR(chcr_aead_fallback(req, reqctx->op));
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	}
 -	skb = alloc_skb(SGE_MAX_WR_LEN, flags);
 +	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
  	if (!skb) {
  		error = -ENOMEM;
  		goto err;
  	}
  
 -	chcr_req = __skb_put_zero(skb, transhdr_len);
 +	/* LLD is going to write the sge hdr. */
 +	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
  
++<<<<<<< HEAD
 +	/* Write WR */
 +	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
 +	memset(chcr_req, 0, transhdr_len);
 +
 +	stop_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
++=======
+ 	temp  = (reqctx->op == CHCR_ENCRYPT_OP) ? 0 : authsize;
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  
  	/*
  	 * Input order	is AAD,IV and Payload. where IV should be included as
@@@ -2012,72 -2322,343 +2096,382 @@@
  	 * to the hardware spec
  	 */
  	chcr_req->sec_cpl.op_ivinsrtofst =
 -		FILL_SEC_CPL_OP_IVINSR(a_ctx(tfm)->dev->rx_channel_id, 2,
 -				       assoclen + 1);
 -	chcr_req->sec_cpl.pldlen = htonl(assoclen + IV + req->cryptlen);
 +		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,
 +				       (ivsize ? (assoclen + 1) : 0));
 +	chcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);
  	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
  					assoclen ? 1 : 0, assoclen,
 -					assoclen + IV + 1,
 -					(temp & 0x1F0) >> 4);
 +					assoclen + ivsize + 1,
 +					(stop_offset & 0x1F0) >> 4);
  	chcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(
++<<<<<<< HEAD
 +					stop_offset & 0xF,
 +					null ? 0 : assoclen + 1,
 +					stop_offset, stop_offset);
 +	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,
 +					(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,
 +					CHCR_SCMD_CIPHER_MODE_AES_CBC,
++=======
+ 					temp & 0xF,
+ 					null ? 0 : assoclen + IV + 1,
+ 					temp, temp);
+ 	if (subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL ||
+ 	    subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA)
+ 		temp = CHCR_SCMD_CIPHER_MODE_AES_CTR;
+ 	else
+ 		temp = CHCR_SCMD_CIPHER_MODE_AES_CBC;
+ 	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(reqctx->op,
+ 					(reqctx->op == CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					temp,
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  					actx->auth_mode, aeadctx->hmac_ctrl,
 -					IV >> 1);
 +					ivsize >> 1);
  	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
 -					 0, 0, dst_size);
 +					 0, 1, dst_size);
  
  	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
++<<<<<<< HEAD
 +	if (op_type == CHCR_ENCRYPT_OP)
++=======
+ 	if (reqctx->op == CHCR_ENCRYPT_OP ||
+ 		subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||
+ 		subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL)
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  		memcpy(chcr_req->key_ctx.key, aeadctx->key,
  		       aeadctx->enckey_len);
  	else
  		memcpy(chcr_req->key_ctx.key, actx->dec_rrkey,
  		       aeadctx->enckey_len);
  
 -	memcpy(chcr_req->key_ctx.key + roundup(aeadctx->enckey_len, 16),
 -	       actx->h_iopad, kctx_len - roundup(aeadctx->enckey_len, 16));
 -	if (subtype == CRYPTO_ALG_SUB_TYPE_CTR_SHA ||
 -	    subtype == CRYPTO_ALG_SUB_TYPE_CTR_NULL) {
 -		memcpy(reqctx->iv, aeadctx->nonce, CTR_RFC3686_NONCE_SIZE);
 -		memcpy(reqctx->iv + CTR_RFC3686_NONCE_SIZE, req->iv,
 -				CTR_RFC3686_IV_SIZE);
 -		*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +
 -			CTR_RFC3686_IV_SIZE) = cpu_to_be32(1);
 -	} else {
 -		memcpy(reqctx->iv, req->iv, IV);
 -	}
 +	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<
 +					4), actx->h_iopad, kctx_len -
 +				(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));
 +
  	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
++<<<<<<< HEAD
 +	sg_param.nents = reqctx->dst_nents;
 +	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
 +	sg_param.qid = qid;
 +	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
 +					reqctx->dst, &sg_param);
 +	if (error)
 +		goto dstmap_fail;
 +
 +	skb_set_transport_header(skb, transhdr_len);
 +
 +	if (assoclen) {
 +		/* AAD buffer in */
 +		write_sg_to_skb(skb, &frags, req->assoc, assoclen);
 +
 +	}
 +	memcpy(reqctx->iv, req->iv, ivsize);
 +	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
 +	write_sg_to_skb(skb, &frags, req->src, req->cryptlen);
++=======
+ 	ulptx = (struct ulptx_sgl *)((u8 *)(phys_cpl + 1) + dst_size);
+ 	chcr_add_aead_dst_ent(req, phys_cpl, assoclen, qid);
+ 	chcr_add_aead_src_ent(req, ulptx, assoclen);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	atomic_inc(&adap->chcr_stats.cipher_rqst);
 -	temp = sizeof(struct cpl_rx_phys_dsgl) + dst_size +
 -		kctx_len + (reqctx->imm ? (assoclen + IV + req->cryptlen) : 0);
 -	create_wreq(a_ctx(tfm), chcr_req, &req->base, reqctx->imm, size,
 -		   transhdr_len, temp, 0);
 +	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
 +		   sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
  	reqctx->skb = skb;
++<<<<<<< HEAD
 +	skb_get(skb);
++=======
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  
  	return skb;
 +dstmap_fail:
 +	/* ivmap_fail: */
 +	kfree_skb(skb);
  err:
++<<<<<<< HEAD
 +	free_new_sg(reqctx->newdstsg);
 +	reqctx->newdstsg = NULL;
 +	return ERR_PTR(error);
 +}
 +
++=======
+ 	chcr_aead_common_exit(req);
+ 
+ 	return ERR_PTR(error);
+ }
+ 
+ int chcr_aead_dma_map(struct device *dev,
+ 		      struct aead_request *req,
+ 		      unsigned short op_type)
+ {
+ 	int error;
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int dst_size;
+ 
+ 	dst_size = req->assoclen + req->cryptlen + (op_type ?
+ 				-authsize : authsize);
+ 	if (!req->cryptlen || !dst_size)
+ 		return 0;
+ 	reqctx->iv_dma = dma_map_single(dev, reqctx->iv, (IV + reqctx->b0_len),
+ 					DMA_BIDIRECTIONAL);
+ 	if (dma_mapping_error(dev, reqctx->iv_dma))
+ 		return -ENOMEM;
+ 	if (reqctx->b0_len)
+ 		reqctx->b0_dma = reqctx->iv_dma + IV;
+ 	else
+ 		reqctx->b0_dma = 0;
+ 	if (req->src == req->dst) {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 		if (!error)
+ 			goto err;
+ 	} else {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		if (!error)
+ 			goto err;
+ 		error = dma_map_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 		if (!error) {
+ 			dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ err:
+ 	dma_unmap_single(dev, reqctx->iv_dma, IV, DMA_BIDIRECTIONAL);
+ 	return -ENOMEM;
+ }
+ 
+ void chcr_aead_dma_unmap(struct device *dev,
+ 			 struct aead_request *req,
+ 			 unsigned short op_type)
+ {
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int dst_size;
+ 
+ 	dst_size = req->assoclen + req->cryptlen + (op_type ?
+ 					-authsize : authsize);
+ 	if (!req->cryptlen || !dst_size)
+ 		return;
+ 
+ 	dma_unmap_single(dev, reqctx->iv_dma, (IV + reqctx->b0_len),
+ 					DMA_BIDIRECTIONAL);
+ 	if (req->src == req->dst) {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 	} else {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		dma_unmap_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 	}
+ }
+ 
+ void chcr_add_aead_src_ent(struct aead_request *req,
+ 			   struct ulptx_sgl *ulptx,
+ 			   unsigned int assoclen)
+ {
+ 	struct ulptx_walk ulp_walk;
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 
+ 	if (reqctx->imm) {
+ 		u8 *buf = (u8 *)ulptx;
+ 
+ 		if (reqctx->b0_len) {
+ 			memcpy(buf, reqctx->scratch_pad, reqctx->b0_len);
+ 			buf += reqctx->b0_len;
+ 		}
+ 		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
+ 				   buf, assoclen, 0);
+ 		buf += assoclen;
+ 		memcpy(buf, reqctx->iv, IV);
+ 		buf += IV;
+ 		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
+ 				   buf, req->cryptlen, req->assoclen);
+ 	} else {
+ 		ulptx_walk_init(&ulp_walk, ulptx);
+ 		if (reqctx->b0_len)
+ 			ulptx_walk_add_page(&ulp_walk, reqctx->b0_len,
+ 					    &reqctx->b0_dma);
+ 		ulptx_walk_add_sg(&ulp_walk, req->src, assoclen, 0);
+ 		ulptx_walk_add_page(&ulp_walk, IV, &reqctx->iv_dma);
+ 		ulptx_walk_add_sg(&ulp_walk, req->src, req->cryptlen,
+ 				  req->assoclen);
+ 		ulptx_walk_end(&ulp_walk);
+ 	}
+ }
+ 
+ void chcr_add_aead_dst_ent(struct aead_request *req,
+ 			   struct cpl_rx_phys_dsgl *phys_cpl,
+ 			   unsigned int assoclen,
+ 			   unsigned short qid)
+ {
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct dsgl_walk dsgl_walk;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	u32 temp;
+ 
+ 	dsgl_walk_init(&dsgl_walk, phys_cpl);
+ 	if (reqctx->b0_len)
+ 		dsgl_walk_add_page(&dsgl_walk, reqctx->b0_len, &reqctx->b0_dma);
+ 	dsgl_walk_add_sg(&dsgl_walk, req->dst, assoclen, 0);
+ 	dsgl_walk_add_page(&dsgl_walk, IV, &reqctx->iv_dma);
+ 	temp = req->cryptlen + (reqctx->op ? -authsize : authsize);
+ 	dsgl_walk_add_sg(&dsgl_walk, req->dst, temp, req->assoclen);
+ 	dsgl_walk_end(&dsgl_walk, qid);
+ }
+ 
+ void chcr_add_cipher_src_ent(struct ablkcipher_request *req,
+ 			     void *ulptx,
+ 			     struct  cipher_wr_param *wrparam)
+ {
+ 	struct ulptx_walk ulp_walk;
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 	u8 *buf = ulptx;
+ 
+ 	memcpy(buf, reqctx->iv, IV);
+ 	buf += IV;
+ 	if (reqctx->imm) {
+ 		sg_pcopy_to_buffer(req->src, sg_nents(req->src),
+ 				   buf, wrparam->bytes, reqctx->processed);
+ 	} else {
+ 		ulptx_walk_init(&ulp_walk, (struct ulptx_sgl *)buf);
+ 		ulptx_walk_add_sg(&ulp_walk, reqctx->srcsg, wrparam->bytes,
+ 				  reqctx->src_ofst);
+ 		reqctx->srcsg = ulp_walk.last_sg;
+ 		reqctx->src_ofst = ulp_walk.last_sg_len;
+ 		ulptx_walk_end(&ulp_walk);
+ 	}
+ }
+ 
+ void chcr_add_cipher_dst_ent(struct ablkcipher_request *req,
+ 			     struct cpl_rx_phys_dsgl *phys_cpl,
+ 			     struct  cipher_wr_param *wrparam,
+ 			     unsigned short qid)
+ {
+ 	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
+ 	struct dsgl_walk dsgl_walk;
+ 
+ 	dsgl_walk_init(&dsgl_walk, phys_cpl);
+ 	dsgl_walk_add_sg(&dsgl_walk, reqctx->dstsg, wrparam->bytes,
+ 			 reqctx->dst_ofst);
+ 	reqctx->dstsg = dsgl_walk.last_sg;
+ 	reqctx->dst_ofst = dsgl_walk.last_sg_len;
+ 
+ 	dsgl_walk_end(&dsgl_walk, qid);
+ }
+ 
+ void chcr_add_hash_src_ent(struct ahash_request *req,
+ 			   struct ulptx_sgl *ulptx,
+ 			   struct hash_wr_param *param)
+ {
+ 	struct ulptx_walk ulp_walk;
+ 	struct chcr_ahash_req_ctx *reqctx = ahash_request_ctx(req);
+ 
+ 	if (reqctx->hctx_wr.imm) {
+ 		u8 *buf = (u8 *)ulptx;
+ 
+ 		if (param->bfr_len) {
+ 			memcpy(buf, reqctx->reqbfr, param->bfr_len);
+ 			buf += param->bfr_len;
+ 		}
+ 
+ 		sg_pcopy_to_buffer(reqctx->hctx_wr.srcsg,
+ 				   sg_nents(reqctx->hctx_wr.srcsg), buf,
+ 				   param->sg_len, 0);
+ 	} else {
+ 		ulptx_walk_init(&ulp_walk, ulptx);
+ 		if (param->bfr_len)
+ 			ulptx_walk_add_page(&ulp_walk, param->bfr_len,
+ 					    &reqctx->hctx_wr.dma_addr);
+ 		ulptx_walk_add_sg(&ulp_walk, reqctx->hctx_wr.srcsg,
+ 				  param->sg_len, reqctx->hctx_wr.src_ofst);
+ 		reqctx->hctx_wr.srcsg = ulp_walk.last_sg;
+ 		reqctx->hctx_wr.src_ofst = ulp_walk.last_sg_len;
+ 		ulptx_walk_end(&ulp_walk);
+ 	}
+ }
+ 
+ int chcr_hash_dma_map(struct device *dev,
+ 		      struct ahash_request *req)
+ {
+ 	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);
+ 	int error = 0;
+ 
+ 	if (!req->nbytes)
+ 		return 0;
+ 	error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 			   DMA_TO_DEVICE);
+ 	if (!error)
+ 		return -ENOMEM;
+ 	req_ctx->hctx_wr.is_sg_map = 1;
+ 	return 0;
+ }
+ 
+ void chcr_hash_dma_unmap(struct device *dev,
+ 			 struct ahash_request *req)
+ {
+ 	struct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);
+ 
+ 	if (!req->nbytes)
+ 		return;
+ 
+ 	dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 			   DMA_TO_DEVICE);
+ 	req_ctx->hctx_wr.is_sg_map = 0;
+ 
+ }
+ 
+ int chcr_cipher_dma_map(struct device *dev,
+ 			struct ablkcipher_request *req)
+ {
+ 	int error;
+ 
+ 	if (req->src == req->dst) {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 		if (!error)
+ 			goto err;
+ 	} else {
+ 		error = dma_map_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		if (!error)
+ 			goto err;
+ 		error = dma_map_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 		if (!error) {
+ 			dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ err:
+ 	return -ENOMEM;
+ }
+ 
+ void chcr_cipher_dma_unmap(struct device *dev,
+ 			   struct ablkcipher_request *req)
+ {
+ 	if (req->src == req->dst) {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_BIDIRECTIONAL);
+ 	} else {
+ 		dma_unmap_sg(dev, req->src, sg_nents(req->src),
+ 				   DMA_TO_DEVICE);
+ 		dma_unmap_sg(dev, req->dst, sg_nents(req->dst),
+ 				   DMA_FROM_DEVICE);
+ 	}
+ }
+ 
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  static int set_msg_len(u8 *block, unsigned int msglen, int csize)
  {
  	__be32 data;
@@@ -2149,8 -2731,9 +2544,14 @@@ static int ccm_format_packet(struct aea
  	} else {
  		memcpy(reqctx->iv, req->iv, 16);
  	}
++<<<<<<< HEAD
 +	*((unsigned short *)(reqctx->scratch_pad + 16)) =
 +				htons(req->assoclen);
++=======
+ 	if (assoclen)
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 				htons(assoclen);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  
  	generate_b0(req, aeadctx, op_type);
  	/* zero the ctr value */
@@@ -2227,234 -2814,171 +2628,330 @@@ static int aead_ccm_validate_input(unsi
  	return 0;
  }
  
 +unsigned int fill_aead_req_fields(struct sk_buff *skb,
 +				  struct aead_request *req,
 +				  struct scatterlist *src,
 +				  unsigned int ivsize,
 +				  struct chcr_aead_ctx *aeadctx)
 +{
 +	unsigned int frags = 0;
 +	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
 +	/* b0 and aad length(if available) */
 +
 +	write_buffer_to_skb(skb, &frags, reqctx->scratch_pad, CCM_B0_SIZE +
 +				(req->assoclen ?  CCM_AAD_FIELD_SIZE : 0));
 +	if (req->assoclen)
 +		write_sg_to_skb(skb, &frags, req->assoc, req->assoclen);
 +	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
 +	if (req->cryptlen)
 +		write_sg_to_skb(skb, &frags, src, req->cryptlen);
 +
 +	return frags;
 +}
 +
  static struct sk_buff *create_aead_ccm_wr(struct aead_request *req,
  					  unsigned short qid,
- 					  int size,
- 					  unsigned short op_type)
+ 					  int size)
  {
  	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 -	struct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));
 +	struct chcr_context *ctx = crypto_aead_ctx(tfm);
 +	struct uld_ctx *u_ctx = ULD_CTX(ctx);
 +	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
  	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
  	struct sk_buff *skb = NULL;
  	struct chcr_wr *chcr_req;
  	struct cpl_rx_phys_dsgl *phys_cpl;
 -	struct ulptx_sgl *ulptx;
 -	unsigned int transhdr_len;
 -	unsigned int dst_size = 0, kctx_len, dnents, temp;
 -	unsigned int sub_type, assoclen = req->assoclen;
 +	struct phys_sge_parm sg_param;
 +	unsigned int frags = 0, transhdr_len, ivsize = AES_BLOCK_SIZE;
 +	unsigned int dst_size = 0, kctx_len, nents;
 +	unsigned int sub_type;
  	unsigned int authsize = crypto_aead_authsize(tfm);
 -	int error = -EINVAL;
 +	int error = -EINVAL, src_nent;
  	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
  		GFP_ATOMIC;
 -	struct adapter *adap = padap(a_ctx(tfm)->dev);
 +	struct adapter *adap = padap(ctx->dev);
 +
 +	dst_size = req->cryptlen + (op_type ? -authsize :
 +					      authsize);
 +	reqctx->newdstsg = NULL;
 +	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
 +		goto err;
 +	src_nent = sg_nents_for_len(req->src, req->cryptlen);
 +	if (src_nent < 0)
 +		goto err;
  
  	sub_type = get_aead_subtype(tfm);
++<<<<<<< HEAD
 +	if (dst_size && is_newsg(req->dst, &nents)) {
 +		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
 +		if (IS_ERR(reqctx->newdstsg))
 +			return ERR_CAST(reqctx->newdstsg);
 +		reqctx->dst = reqctx->newdstsg;
 +	} else {
 +		reqctx->dst = req->dst;
 +	}
 +	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
 +					     (op_type ? -authsize : authsize));
 +	if (reqctx->dst_nents < 0) {
 +		pr_err("CCM:Invalid Destination sg entries\n");
 +		error = -EINVAL;
 +		goto err;
 +	}
 +	error = aead_ccm_validate_input(op_type, req, aeadctx, sub_type);
 +	if (error)
 +		goto err;
 +
 +	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
 +	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) * 2;
 +	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
 +	if (chcr_aead_need_fallback(req, src_nent + MIN_CCM_SG,
 +			    T6_MAX_AAD_SIZE - 18,
 +			    transhdr_len + (sgl_len(src_nent + MIN_CCM_SG) * 8),
 +			    op_type)) {
 +		atomic_inc(&adap->chcr_stats.fallback);
 +		free_new_sg(reqctx->newdstsg);
 +		reqctx->newdstsg = NULL;
 +		return ERR_PTR(chcr_aead_fallback(req, op_type));
++=======
+ 	if (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 		assoclen -= 8;
+ 	reqctx->b0_len = CCM_B0_SIZE + (assoclen ? CCM_AAD_FIELD_SIZE : 0);
+ 	error = chcr_aead_common_init(req);
+ 	if (error)
+ 		return ERR_PTR(error);
+ 
+ 	error = aead_ccm_validate_input(reqctx->op, req, aeadctx, sub_type);
+ 	if (error)
+ 		goto err;
+ 	dnents = sg_nents_xlen(req->dst, assoclen, CHCR_DST_SG_SIZE, 0);
+ 	dnents += sg_nents_xlen(req->dst, req->cryptlen
+ 			+ (reqctx->op ? -authsize : authsize),
+ 			CHCR_DST_SG_SIZE, req->assoclen);
+ 	dnents += MIN_CCM_SG; // For IV and B0
+ 	dst_size = get_space_for_phys_dsgl(dnents);
+ 	kctx_len = roundup(aeadctx->enckey_len, 16) * 2;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	reqctx->imm = (transhdr_len + assoclen + IV + req->cryptlen +
+ 		       reqctx->b0_len) <= SGE_MAX_WR_LEN;
+ 	temp = reqctx->imm ? roundup(assoclen + IV + req->cryptlen +
+ 				     reqctx->b0_len, 16) :
+ 		(sgl_len(reqctx->src_nents + reqctx->aad_nents +
+ 				    MIN_CCM_SG) *  8);
+ 	transhdr_len += temp;
+ 	transhdr_len = roundup(transhdr_len, 16);
+ 
+ 	if (chcr_aead_need_fallback(req, dnents, T6_MAX_AAD_SIZE -
+ 				    reqctx->b0_len, transhdr_len, reqctx->op)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		chcr_aead_common_exit(req);
+ 		return ERR_PTR(chcr_aead_fallback(req, reqctx->op));
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	}
 -	skb = alloc_skb(SGE_MAX_WR_LEN,  flags);
 +
 +	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)),  flags);
  
  	if (!skb) {
  		error = -ENOMEM;
  		goto err;
  	}
  
 -	chcr_req = (struct chcr_wr *) __skb_put_zero(skb, transhdr_len);
 +	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
  
++<<<<<<< HEAD
 +	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
 +	memset(chcr_req, 0, transhdr_len);
 +
 +	fill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, op_type, ctx);
++=======
+ 	fill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, reqctx->op);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  
  	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
  	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
 -	memcpy(chcr_req->key_ctx.key + roundup(aeadctx->enckey_len, 16),
 -			aeadctx->key, aeadctx->enckey_len);
 +	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
 +					16), aeadctx->key, aeadctx->enckey_len);
  
  	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
++<<<<<<< HEAD
 +	error = ccm_format_packet(req, aeadctx, sub_type, op_type);
 +	if (error)
 +		goto dstmap_fail;
 +
 +	sg_param.nents = reqctx->dst_nents;
 +	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
 +	sg_param.qid = qid;
 +	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
 +				 reqctx->dst, &sg_param);
 +	if (error)
 +		goto dstmap_fail;
++=======
+ 	ulptx = (struct ulptx_sgl *)((u8 *)(phys_cpl + 1) + dst_size);
+ 	error = ccm_format_packet(req, aeadctx, sub_type, reqctx->op, assoclen);
+ 	if (error)
+ 		goto dstmap_fail;
+ 	chcr_add_aead_dst_ent(req, phys_cpl, assoclen, qid);
+ 	chcr_add_aead_src_ent(req, ulptx, assoclen);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  
 +	skb_set_transport_header(skb, transhdr_len);
 +	frags = fill_aead_req_fields(skb, req, req->src, ivsize, aeadctx);
  	atomic_inc(&adap->chcr_stats.aead_rqst);
 -	temp = sizeof(struct cpl_rx_phys_dsgl) + dst_size +
 -		kctx_len + (reqctx->imm ? (assoclen + IV + req->cryptlen +
 -		reqctx->b0_len) : 0);
 -	create_wreq(a_ctx(tfm), chcr_req, &req->base, reqctx->imm, 0,
 -		    transhdr_len, temp, 0);
 +	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, 0, 1,
 +		    sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
  	reqctx->skb = skb;
++<<<<<<< HEAD
 +	skb_get(skb);
++=======
+ 
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	return skb;
  dstmap_fail:
  	kfree_skb(skb);
  err:
++<<<<<<< HEAD
 +	free_new_sg(reqctx->newdstsg);
 +	reqctx->newdstsg = NULL;
++=======
+ 	chcr_aead_common_exit(req);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	return ERR_PTR(error);
  }
  
  static struct sk_buff *create_gcm_wr(struct aead_request *req,
  				     unsigned short qid,
- 				     int size,
- 				     unsigned short op_type)
+ 				     int size)
  {
  	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 -	struct chcr_aead_ctx *aeadctx = AEAD_CTX(a_ctx(tfm));
 +	struct chcr_context *ctx = crypto_aead_ctx(tfm);
 +	struct uld_ctx *u_ctx = ULD_CTX(ctx);
 +	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
  	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
  	struct sk_buff *skb = NULL;
  	struct chcr_wr *chcr_req;
  	struct cpl_rx_phys_dsgl *phys_cpl;
 -	struct ulptx_sgl *ulptx;
 -	unsigned int transhdr_len, dnents = 0;
 -	unsigned int dst_size = 0, temp = 0, kctx_len, assoclen = req->assoclen;
 +	struct phys_sge_parm sg_param;
 +	unsigned int frags = 0, transhdr_len;
 +	unsigned int ivsize = AES_BLOCK_SIZE;
 +	unsigned int dst_size = 0, kctx_len, nents, assoclen = req->assoclen;
 +	unsigned char tag_offset = 0;
  	unsigned int authsize = crypto_aead_authsize(tfm);
 -	int error = -EINVAL;
 +	int error = -EINVAL, src_nent;
  	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
  		GFP_ATOMIC;
 -	struct adapter *adap = padap(a_ctx(tfm)->dev);
 +	struct adapter *adap = padap(ctx->dev);
 +
 +	reqctx->newdstsg = NULL;
 +	dst_size = req->cryptlen + (op_type ? -authsize :
 +					      authsize);
 +	/* validate key size */
 +	if (aeadctx->enckey_len == 0)
 +		goto err;
 +
++<<<<<<< HEAD
 +	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
 +		goto err;
 +	src_nent = sg_nents_for_len(req->src, req->cryptlen);
 +	if (src_nent < 0)
 +		goto err;
 +
 +	if (dst_size && is_newsg(req->dst, &nents)) {
 +		reqctx->newdstsg = alloc_new_sg(req->dst, nents);
 +		if (IS_ERR(reqctx->newdstsg))
 +			return ERR_CAST(reqctx->newdstsg);
 +		reqctx->dst = reqctx->newdstsg;
 +	} else {
 +		reqctx->dst = req->dst;
 +	}
 +
 +	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
 +					     (op_type ? -authsize : authsize));
 +	if (reqctx->dst_nents < 0) {
 +		pr_err("GCM:Invalid Destination sg entries\n");
 +		error = -EINVAL;
 +		goto err;
 +	}
  
 -	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)
 -		assoclen = req->assoclen - 8;
  
 +	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
 +	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) +
 +		AEAD_H_SIZE;
 +	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
 +	if (chcr_aead_need_fallback(req, src_nent + MIN_GCM_SG,
 +			    T6_MAX_AAD_SIZE,
 +			    transhdr_len + (sgl_len(src_nent + MIN_GCM_SG) * 8),
 +			    op_type)) {
 +		atomic_inc(&adap->chcr_stats.fallback);
 +		free_new_sg(reqctx->newdstsg);
 +		reqctx->newdstsg = NULL;
 +		return ERR_PTR(chcr_aead_fallback(req, op_type));
++=======
+ 	reqctx->b0_len = 0;
+ 	error = chcr_aead_common_init(req);
+ 	if (error)
+ 		return ERR_PTR(error);
+ 	dnents = sg_nents_xlen(req->dst, assoclen, CHCR_DST_SG_SIZE, 0);
+ 	dnents += sg_nents_xlen(req->dst, req->cryptlen +
+ 				(reqctx->op ? -authsize : authsize),
+ 				CHCR_DST_SG_SIZE, req->assoclen);
+ 	dnents += MIN_GCM_SG; // For IV
+ 	dst_size = get_space_for_phys_dsgl(dnents);
+ 	kctx_len = roundup(aeadctx->enckey_len, 16) + AEAD_H_SIZE;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	reqctx->imm = (transhdr_len + assoclen + IV + req->cryptlen) <=
+ 			SGE_MAX_WR_LEN;
+ 	temp = reqctx->imm ? roundup(assoclen + IV + req->cryptlen, 16) :
+ 		(sgl_len(reqctx->src_nents +
+ 		reqctx->aad_nents + MIN_GCM_SG) * 8);
+ 	transhdr_len += temp;
+ 	transhdr_len = roundup(transhdr_len, 16);
+ 	if (chcr_aead_need_fallback(req, dnents, T6_MAX_AAD_SIZE,
+ 			    transhdr_len, reqctx->op)) {
+ 
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		chcr_aead_common_exit(req);
+ 		return ERR_PTR(chcr_aead_fallback(req, reqctx->op));
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	}
 -	skb = alloc_skb(SGE_MAX_WR_LEN, flags);
 +	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
  	if (!skb) {
  		error = -ENOMEM;
  		goto err;
  	}
  
 -	chcr_req = __skb_put_zero(skb, transhdr_len);
 +	/* NIC driver is going to write the sge hdr. */
 +	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
 +
++<<<<<<< HEAD
 +	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
 +	memset(chcr_req, 0, transhdr_len);
  
 +	tag_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
++=======
+ 	//Offset of tag from end
+ 	temp = (reqctx->op == CHCR_ENCRYPT_OP) ? 0 : authsize;
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	chcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(
 -					a_ctx(tfm)->dev->rx_channel_id, 2,
 -					(assoclen + 1));
 +					ctx->dev->rx_channel_id, 2, (ivsize ?
 +					(assoclen + 1) : 0));
  	chcr_req->sec_cpl.pldlen =
 -		htonl(assoclen + IV + req->cryptlen);
 +		htonl(assoclen + ivsize + req->cryptlen);
  	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
  					assoclen ? 1 : 0, assoclen,
++<<<<<<< HEAD
 +					assoclen + ivsize + 1, 0);
 +		chcr_req->sec_cpl.cipherstop_lo_authinsert =
 +			FILL_SEC_CPL_AUTHINSERT(0, assoclen + ivsize + 1,
 +						tag_offset, tag_offset);
 +		chcr_req->sec_cpl.seqno_numivs =
 +			FILL_SEC_CPL_SCMD0_SEQNO(op_type, (op_type ==
++=======
+ 					assoclen + IV + 1, 0);
+ 	chcr_req->sec_cpl.cipherstop_lo_authinsert =
+ 			FILL_SEC_CPL_AUTHINSERT(0, assoclen + IV + 1,
+ 						temp, temp);
+ 	chcr_req->sec_cpl.seqno_numivs =
+ 			FILL_SEC_CPL_SCMD0_SEQNO(reqctx->op, (reqctx->op ==
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  					CHCR_ENCRYPT_OP) ? 1 : 0,
  					CHCR_SCMD_CIPHER_MODE_AES_GCM,
  					CHCR_SCMD_AUTH_MODE_GHASH,
@@@ -2478,32 -3002,20 +2975,44 @@@
  	*((unsigned int *)(reqctx->iv + 12)) = htonl(0x01);
  
  	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
 -	ulptx = (struct ulptx_sgl *)((u8 *)(phys_cpl + 1) + dst_size);
 +	sg_param.nents = reqctx->dst_nents;
 +	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
 +	sg_param.qid = qid;
 +	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
 +					  reqctx->dst, &sg_param);
 +	if (error)
 +		goto dstmap_fail;
  
++<<<<<<< HEAD
 +	skb_set_transport_header(skb, transhdr_len);
 +	write_sg_to_skb(skb, &frags, req->assoc, assoclen);
 +	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
 +	write_sg_to_skb(skb, &frags, req->src, req->cryptlen);
++=======
+ 	chcr_add_aead_dst_ent(req, phys_cpl, assoclen, qid);
+ 	chcr_add_aead_src_ent(req, ulptx, assoclen);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	atomic_inc(&adap->chcr_stats.aead_rqst);
 -	temp = sizeof(struct cpl_rx_phys_dsgl) + dst_size +
 -		kctx_len + (reqctx->imm ? (assoclen + IV + req->cryptlen) : 0);
 -	create_wreq(a_ctx(tfm), chcr_req, &req->base, reqctx->imm, size,
 -		    transhdr_len, temp, reqctx->verify);
 +	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
 +			sizeof(struct cpl_rx_phys_dsgl) + dst_size,
 +			reqctx->verify);
  	reqctx->skb = skb;
++<<<<<<< HEAD
 +	skb_get(skb);
++=======
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	return skb;
  
 +dstmap_fail:
 +	/* ivmap_fail: */
 +	kfree_skb(skb);
  err:
++<<<<<<< HEAD
 +	free_new_sg(reqctx->newdstsg);
 +	reqctx->newdstsg = NULL;
++=======
+ 	chcr_aead_common_exit(req);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	return ERR_PTR(error);
  }
  
@@@ -3018,25 -3547,59 +3527,69 @@@ out
  	memzero_explicit(&keys, sizeof(keys));
  	return -EINVAL;
  }
++<<<<<<< HEAD
++=======
+ 
+ static int chcr_aead_op(struct aead_request *req,
+ 			int size,
+ 			create_wr_t create_wr_fn)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct uld_ctx *u_ctx;
+ 	struct sk_buff *skb;
+ 	int isfull = 0;
+ 
+ 	if (!a_ctx(tfm)->dev) {
+ 		pr_err("chcr : %s : No crypto device.\n", __func__);
+ 		return -ENXIO;
+ 	}
+ 	u_ctx = ULD_CTX(a_ctx(tfm));
+ 	if (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 				   a_ctx(tfm)->tx_qidx)) {
+ 		isfull = 1;
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
+ 			return -ENOSPC;
+ 	}
+ 
+ 	/* Form a WR from req */
+ 	skb = create_wr_fn(req, u_ctx->lldi.rxq_ids[a_ctx(tfm)->rx_qidx], size);
+ 
+ 	if (IS_ERR(skb) || !skb)
+ 		return PTR_ERR(skb);
+ 
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, a_ctx(tfm)->tx_qidx);
+ 	chcr_send_wr(skb);
+ 	return isfull ? -EBUSY : -EINPROGRESS;
+ }
+ 
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  static int chcr_aead_encrypt(struct aead_request *req)
  {
  	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
  	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
  
  	reqctx->verify = VERIFY_HW;
+ 	reqctx->op = CHCR_ENCRYPT_OP;
  
  	switch (get_aead_subtype(tfm)) {
++<<<<<<< HEAD
 +	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
 +	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
 +		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
 +				    create_authenc_wr);
++=======
+ 	case CRYPTO_ALG_SUB_TYPE_CTR_SHA:
+ 	case CRYPTO_ALG_SUB_TYPE_CBC_SHA:
+ 	case CRYPTO_ALG_SUB_TYPE_CBC_NULL:
+ 	case CRYPTO_ALG_SUB_TYPE_CTR_NULL:
+ 		return chcr_aead_op(req, 0, create_authenc_wr);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
  	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
- 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
- 				    create_aead_ccm_wr);
+ 		return chcr_aead_op(req, 0, create_aead_ccm_wr);
  	default:
- 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
- 				    create_gcm_wr);
+ 		return chcr_aead_op(req, 0, create_gcm_wr);
  	}
  }
  
@@@ -3054,19 -3617,18 +3607,25 @@@ static int chcr_aead_decrypt(struct aea
  		size = 0;
  		reqctx->verify = VERIFY_HW;
  	}
- 
+ 	reqctx->op = CHCR_DECRYPT_OP;
  	switch (get_aead_subtype(tfm)) {
++<<<<<<< HEAD
 +	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
 +	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
 +		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
 +				    create_authenc_wr);
++=======
+ 	case CRYPTO_ALG_SUB_TYPE_CBC_SHA:
+ 	case CRYPTO_ALG_SUB_TYPE_CTR_SHA:
+ 	case CRYPTO_ALG_SUB_TYPE_CBC_NULL:
+ 	case CRYPTO_ALG_SUB_TYPE_CTR_NULL:
+ 		return chcr_aead_op(req, size, create_authenc_wr);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
  	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
- 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
- 				    create_aead_ccm_wr);
+ 		return chcr_aead_op(req, size, create_aead_ccm_wr);
  	default:
- 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
- 				    create_gcm_wr);
+ 		return chcr_aead_op(req, size, create_gcm_wr);
  	}
  }
  
diff --cc drivers/crypto/chelsio/chcr_crypto.h
index ee637a7f2f30,54835cb109e5..000000000000
--- a/drivers/crypto/chelsio/chcr_crypto.h
+++ b/drivers/crypto/chelsio/chcr_crypto.h
@@@ -165,14 -181,37 +165,14 @@@ struct ablk_ctx 
  };
  struct chcr_aead_reqctx {
  	struct	sk_buff	*skb;
 -	dma_addr_t iv_dma;
 -	dma_addr_t b0_dma;
 -	unsigned int b0_len;
 -	unsigned int op;
 -	short int aad_nents;
 -	short int src_nents;
 +	struct scatterlist *dst;
 +	struct scatterlist *newdstsg;
  	short int dst_nents;
 -	u16 imm;
  	u16 verify;
- 	u8 iv[CHCR_MAX_CRYPTO_IV_LEN];
- 	unsigned char scratch_pad[MAX_SCRATCH_PAD_SIZE];
+ 	u8 iv[CHCR_MAX_CRYPTO_IV_LEN + MAX_SCRATCH_PAD_SIZE];
+ 	u8 *scratch_pad;
  };
  
 -struct ulptx_walk {
 -	struct ulptx_sgl *sgl;
 -	unsigned int nents;
 -	unsigned int pair_idx;
 -	unsigned int last_sg_len;
 -	struct scatterlist *last_sg;
 -	struct ulptx_sge_pair *pair;
 -
 -};
 -
 -struct dsgl_walk {
 -	unsigned int nents;
 -	unsigned int last_sg_len;
 -	struct scatterlist *last_sg;
 -	struct cpl_rx_phys_dsgl *dsgl;
 -	struct phys_sge_pairs *to;
 -};
 -
  struct chcr_gcm_ctx {
  	u8 ghash_h[AEAD_H_SIZE];
  };
@@@ -260,38 -309,34 +260,66 @@@ struct chcr_alg_template 
  	} alg;
  };
  
 +struct chcr_req_ctx {
 +	union {
 +		struct ahash_request *ahash_req;
 +		struct aead_request *aead_req;
 +		struct ablkcipher_request *ablk_req;
 +	} req;
 +	union {
 +		struct chcr_ahash_req_ctx *ahash_ctx;
 +		struct chcr_aead_reqctx *reqctx;
 +		struct chcr_blkcipher_req_ctx *ablk_ctx;
 +	} ctx;
 +};
 +
 +struct sge_opaque_hdr {
 +	void *dev;
 +	dma_addr_t addr[MAX_SKB_FRAGS + 1];
 +};
 +
  typedef struct sk_buff *(*create_wr_t)(struct aead_request *req,
  				       unsigned short qid,
- 				       int size,
- 				       unsigned short op_type);
+ 				       int size);
  
++<<<<<<< HEAD
 +static int chcr_aead_op(struct aead_request *req_base,
 +			  unsigned short op_type,
 +			  int size,
 +			  create_wr_t create_wr_fn);
 +static inline int get_aead_subtype(struct crypto_aead *aead);
 +static int is_newsg(struct scatterlist *sgl, unsigned int *newents);
 +static struct scatterlist *alloc_new_sg(struct scatterlist *sgl,
 +					unsigned int nents);
 +static inline void free_new_sg(struct scatterlist *sgl);
 +static int chcr_handle_cipher_resp(struct ablkcipher_request *req,
 +				   unsigned char *input, int err);
++=======
+ void chcr_verify_tag(struct aead_request *req, u8 *input, int *err);
+ int chcr_aead_dma_map(struct device *dev, struct aead_request *req,
+ 		      unsigned short op_type);
+ void chcr_aead_dma_unmap(struct device *dev, struct aead_request *req,
+ 			 unsigned short op_type);
+ void chcr_add_aead_dst_ent(struct aead_request *req,
+ 			   struct cpl_rx_phys_dsgl *phys_cpl,
+ 			   unsigned int assoclen,
+ 			   unsigned short qid);
+ void chcr_add_aead_src_ent(struct aead_request *req, struct ulptx_sgl *ulptx,
+ 			   unsigned int assoclen);
+ void chcr_add_cipher_src_ent(struct ablkcipher_request *req,
+ 			     void *ulptx,
+ 			     struct  cipher_wr_param *wrparam);
+ int chcr_cipher_dma_map(struct device *dev, struct ablkcipher_request *req);
+ void chcr_cipher_dma_unmap(struct device *dev, struct ablkcipher_request *req);
+ void chcr_add_cipher_dst_ent(struct ablkcipher_request *req,
+ 			     struct cpl_rx_phys_dsgl *phys_cpl,
+ 			     struct  cipher_wr_param *wrparam,
+ 			     unsigned short qid);
+ int sg_nents_len_skip(struct scatterlist *sg, u64 len, u64 skip);
+ void chcr_add_hash_src_ent(struct ahash_request *req, struct ulptx_sgl *ulptx,
+ 			   struct hash_wr_param *param);
+ int chcr_hash_dma_map(struct device *dev, struct ahash_request *req);
+ void chcr_hash_dma_unmap(struct device *dev, struct ahash_request *req);
+ void chcr_aead_common_exit(struct aead_request *req);
++>>>>>>> 4262c98aab95 (crypto: chelsio - Remove separate buffer used for DMA map B0 block in CCM)
  #endif /* __CHCR_CRYPTO_H__ */
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/chelsio/chcr_crypto.h
