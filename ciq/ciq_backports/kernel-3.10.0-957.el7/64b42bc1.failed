mm/madvise.c: free swp_entry in madvise_free

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] madvise: free swp_entry in madvise_free (Rafael Aquini) [1562137]
Rebuild_FUZZ: 93.98%
commit-author Minchan Kim <minchan@kernel.org>
commit 64b42bc1cfdf6e2c3ab7315f2ff56c31cd257370
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/64b42bc1.failed

When I test below piece of code with 12 processes(ie, 512M * 12 = 6G
consume) on my (3G ram + 12 cpu + 8G swap, the madvise_free is
siginficat slower (ie, 2x times) than madvise_dontneed.

     loop = 5;
     mmap(512M);
     while (loop--) {
             memset(512M);
             madvise(MADV_FREE or MADV_DONTNEED);
     }

The reason is lots of swapin.

1) dontneed: 1,612 swapin
2) madvfree: 879,585 swapin

If we find hinted pages were already swapped out when syscall is called,
it's pointless to keep the swapped-out pages in pte.  Instead, let's
free the cold page because swapin is more expensive than (alloc page +
zeroing).

With this patch, it reduced swapin from 879,585 to 1,878 so elapsed time

1) dontneed: 6.10user 233.50system 0:50.44elapsed
2) madvfree: 6.03user 401.17system 1:30.67elapsed
2) madvfree + below patch: 6.70user 339.14system 1:04.45elapsed

	Signed-off-by: Minchan Kim <minchan@kernel.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Hugh Dickins <hughd@google.com>
	Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Cc: Shaohua Li <shli@kernel.org>
	Cc: <yalin.wang2010@gmail.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Chen Gang <gang.chen.5i5j@gmail.com>
	Cc: Chris Zankel <chris@zankel.net>
	Cc: Daniel Micay <danielmicay@gmail.com>
	Cc: Darrick J. Wong <darrick.wong@oracle.com>
	Cc: David S. Miller <davem@davemloft.net>
	Cc: Helge Deller <deller@gmx.de>
	Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
	Cc: Jason Evans <je@fb.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Cc: Kirill A. Shutemov <kirill@shutemov.name>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Max Filippov <jcmvbkbc@gmail.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Michael Kerrisk <mtk.manpages@gmail.com>
	Cc: Mika Penttil <mika.penttila@nextfour.com>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: Richard Henderson <rth@twiddle.net>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Roland Dreier <roland@kernel.org>
	Cc: Russell King <rmk@arm.linux.org.uk>
	Cc: Shaohua Li <shli@kernel.org>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: Wu Fengguang <fengguang.wu@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 64b42bc1cfdf6e2c3ab7315f2ff56c31cd257370)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/madvise.c
diff --cc mm/madvise.c
index 896fc122d177,98e28e777ccb..000000000000
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@@ -253,6 -260,186 +253,189 @@@ static long madvise_willneed(struct vm_
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
+ 				unsigned long end, struct mm_walk *walk)
+ 
+ {
+ 	struct mmu_gather *tlb = walk->private;
+ 	struct mm_struct *mm = tlb->mm;
+ 	struct vm_area_struct *vma = walk->vma;
+ 	spinlock_t *ptl;
+ 	pte_t *orig_pte, *pte, ptent;
+ 	struct page *page;
+ 	int nr_swap = 0;
+ 
+ 	split_huge_pmd(vma, pmd, addr);
+ 	if (pmd_trans_unstable(pmd))
+ 		return 0;
+ 
+ 	orig_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
+ 	arch_enter_lazy_mmu_mode();
+ 	for (; addr != end; pte++, addr += PAGE_SIZE) {
+ 		ptent = *pte;
+ 
+ 		if (pte_none(ptent))
+ 			continue;
+ 		/*
+ 		 * If the pte has swp_entry, just clear page table to
+ 		 * prevent swap-in which is more expensive rather than
+ 		 * (page allocation + zeroing).
+ 		 */
+ 		if (!pte_present(ptent)) {
+ 			swp_entry_t entry;
+ 
+ 			entry = pte_to_swp_entry(ptent);
+ 			if (non_swap_entry(entry))
+ 				continue;
+ 			nr_swap--;
+ 			free_swap_and_cache(entry);
+ 			pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
+ 			continue;
+ 		}
+ 
+ 		page = vm_normal_page(vma, addr, ptent);
+ 		if (!page)
+ 			continue;
+ 
+ 		/*
+ 		 * If pmd isn't transhuge but the page is THP and
+ 		 * is owned by only this process, split it and
+ 		 * deactivate all pages.
+ 		 */
+ 		if (PageTransCompound(page)) {
+ 			if (page_mapcount(page) != 1)
+ 				goto out;
+ 			get_page(page);
+ 			if (!trylock_page(page)) {
+ 				put_page(page);
+ 				goto out;
+ 			}
+ 			pte_unmap_unlock(orig_pte, ptl);
+ 			if (split_huge_page(page)) {
+ 				unlock_page(page);
+ 				put_page(page);
+ 				pte_offset_map_lock(mm, pmd, addr, &ptl);
+ 				goto out;
+ 			}
+ 			put_page(page);
+ 			unlock_page(page);
+ 			pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
+ 			pte--;
+ 			addr -= PAGE_SIZE;
+ 			continue;
+ 		}
+ 
+ 		VM_BUG_ON_PAGE(PageTransCompound(page), page);
+ 
+ 		if (PageSwapCache(page) || PageDirty(page)) {
+ 			if (!trylock_page(page))
+ 				continue;
+ 			/*
+ 			 * If page is shared with others, we couldn't clear
+ 			 * PG_dirty of the page.
+ 			 */
+ 			if (page_mapcount(page) != 1) {
+ 				unlock_page(page);
+ 				continue;
+ 			}
+ 
+ 			if (PageSwapCache(page) && !try_to_free_swap(page)) {
+ 				unlock_page(page);
+ 				continue;
+ 			}
+ 
+ 			ClearPageDirty(page);
+ 			unlock_page(page);
+ 		}
+ 
+ 		if (pte_young(ptent) || pte_dirty(ptent)) {
+ 			/*
+ 			 * Some of architecture(ex, PPC) don't update TLB
+ 			 * with set_pte_at and tlb_remove_tlb_entry so for
+ 			 * the portability, remap the pte with old|clean
+ 			 * after pte clearing.
+ 			 */
+ 			ptent = ptep_get_and_clear_full(mm, addr, pte,
+ 							tlb->fullmm);
+ 
+ 			ptent = pte_mkold(ptent);
+ 			ptent = pte_mkclean(ptent);
+ 			set_pte_at(mm, addr, pte, ptent);
+ 			tlb_remove_tlb_entry(tlb, pte, addr);
+ 		}
+ 	}
+ out:
+ 	if (nr_swap) {
+ 		if (current->mm == mm)
+ 			sync_mm_rss(mm);
+ 
+ 		add_mm_counter(mm, MM_SWAPENTS, nr_swap);
+ 	}
+ 	arch_leave_lazy_mmu_mode();
+ 	pte_unmap_unlock(orig_pte, ptl);
+ 	cond_resched();
+ 	return 0;
+ }
+ 
+ static void madvise_free_page_range(struct mmu_gather *tlb,
+ 			     struct vm_area_struct *vma,
+ 			     unsigned long addr, unsigned long end)
+ {
+ 	struct mm_walk free_walk = {
+ 		.pmd_entry = madvise_free_pte_range,
+ 		.mm = vma->vm_mm,
+ 		.private = tlb,
+ 	};
+ 
+ 	tlb_start_vma(tlb, vma);
+ 	walk_page_range(addr, end, &free_walk);
+ 	tlb_end_vma(tlb, vma);
+ }
+ 
+ static int madvise_free_single_vma(struct vm_area_struct *vma,
+ 			unsigned long start_addr, unsigned long end_addr)
+ {
+ 	unsigned long start, end;
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	struct mmu_gather tlb;
+ 
+ 	if (vma->vm_flags & (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))
+ 		return -EINVAL;
+ 
+ 	/* MADV_FREE works for only anon vma at the moment */
+ 	if (!vma_is_anonymous(vma))
+ 		return -EINVAL;
+ 
+ 	start = max(vma->vm_start, start_addr);
+ 	if (start >= vma->vm_end)
+ 		return -EINVAL;
+ 	end = min(vma->vm_end, end_addr);
+ 	if (end <= vma->vm_start)
+ 		return -EINVAL;
+ 
+ 	lru_add_drain();
+ 	tlb_gather_mmu(&tlb, mm, start, end);
+ 	update_hiwater_rss(mm);
+ 
+ 	mmu_notifier_invalidate_range_start(mm, start, end);
+ 	madvise_free_page_range(&tlb, vma, start, end);
+ 	mmu_notifier_invalidate_range_end(mm, start, end);
+ 	tlb_finish_mmu(&tlb, start, end);
+ 
+ 	return 0;
+ }
+ 
+ static long madvise_free(struct vm_area_struct *vma,
+ 			     struct vm_area_struct **prev,
+ 			     unsigned long start, unsigned long end)
+ {
+ 	*prev = vma;
+ 	return madvise_free_single_vma(vma, start, end);
+ }
+ 
++>>>>>>> 64b42bc1cfdf (mm/madvise.c: free swp_entry in madvise_free)
  /*
   * Application no longer needs these pages.  If the pages are dirty,
   * it's OK to just throw them away.  The app will be more careful about
* Unmerged path mm/madvise.c
