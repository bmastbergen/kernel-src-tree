nvme-rdma: Use unlikely macro in the fast path

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Max Gurtovoy <maxg@mellanox.com>
commit a7b7c7a105a528e6c2a0a2581b814a5acacb4c38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a7b7c7a1.failed

This patch slightly improves performance (mainly for small block sizes).

	Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit a7b7c7a105a528e6c2a0a2581b814a5acacb4c38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index 6d8e34242c5f,6a7682620d87..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -956,12 -1111,8 +956,17 @@@ static int nvme_rdma_map_sg_fr(struct n
  	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
  	int nr;
  
++<<<<<<< HEAD
 +	/*
 +	 * Align the MR to a 4K page size to match the ctrl page size and
 +	 * the block virtual boundary.
 +	 */
 +	nr = ib_map_mr_sg(req->mr, req->sg_table.sgl, count, NULL, SZ_4K);
 +	if (nr < count) {
++=======
+ 	nr = ib_map_mr_sg(req->mr, req->sg_table.sgl, count, NULL, PAGE_SIZE);
+ 	if (unlikely(nr < count)) {
++>>>>>>> a7b7c7a105a5 (nvme-rdma: Use unlikely macro in the fast path)
  		if (nr < 0)
  			return nr;
  		return -EINVAL;
@@@ -1504,11 -1633,10 +1509,16 @@@ static int nvme_rdma_queue_rq(struct bl
  
  	blk_mq_start_request(rq);
  
++<<<<<<< HEAD
 +	map_len = nvme_map_len(rq);
 +	ret = nvme_rdma_map_data(queue, rq, map_len, c);
 +	if (ret < 0) {
++=======
+ 	err = nvme_rdma_map_data(queue, rq, c);
+ 	if (unlikely(err < 0)) {
++>>>>>>> a7b7c7a105a5 (nvme-rdma: Use unlikely macro in the fast path)
  		dev_err(queue->ctrl->ctrl.device,
 -			     "Failed to map data (%d)\n", err);
 +			     "Failed to map data (%d)\n", ret);
  		nvme_cleanup_cmd(rq);
  		goto err;
  	}
@@@ -1516,11 -1644,11 +1526,15 @@@
  	ib_dma_sync_single_for_device(dev, sqe->dma,
  			sizeof(struct nvme_command), DMA_TO_DEVICE);
  
 -	if (req_op(rq) == REQ_OP_FLUSH)
 +	if (rq->cmd_type == REQ_TYPE_FS && rq->cmd_flags & REQ_FLUSH)
  		flush = true;
 -	err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
 +	ret = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
  			req->mr->need_inval ? &req->reg_wr.wr : NULL, flush);
++<<<<<<< HEAD
 +	if (ret) {
++=======
+ 	if (unlikely(err)) {
++>>>>>>> a7b7c7a105a5 (nvme-rdma: Use unlikely macro in the fast path)
  		nvme_rdma_unmap_data(queue, rq);
  		goto err;
  	}
* Unmerged path drivers/nvme/host/rdma.c
