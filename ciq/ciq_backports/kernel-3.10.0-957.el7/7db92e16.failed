x86/kvm: Move l1tf setup function

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] kvm: move l1tf setup function (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 93.55%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 7db92e165ac814487264632ab2624e832f20ae38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/7db92e16.failed

In preparation of allowing run time control for L1D flushing, move the
setup code to the module parameter handler.

In case of pre module init parsing, just store the value and let vmx_init()
do the actual setup after running kvm_init() so that enable_ept is having
the correct state.

During run-time invoke it directly from the parameter setter to prepare for
run-time control.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Jiri Kosina <jkosina@suse.cz>
	Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
Link: https://lkml.kernel.org/r/20180713142322.694063239@linutronix.de

(cherry picked from commit 7db92e165ac814487264632ab2624e832f20ae38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index e2f48f8aba96,ce06701916ae..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -175,6 -191,109 +175,112 @@@ module_param(ple_window_max, uint, 0444
  
  extern const ulong vmx_return;
  
++<<<<<<< HEAD
++=======
+ static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+ 
+ /* Storage for pre module init parameter parsing */
+ static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
+ 
+ static const struct {
+ 	const char *option;
+ 	enum vmx_l1d_flush_state cmd;
+ } vmentry_l1d_param[] = {
+ 	{"auto",	VMENTER_L1D_FLUSH_AUTO},
+ 	{"never",	VMENTER_L1D_FLUSH_NEVER},
+ 	{"cond",	VMENTER_L1D_FLUSH_COND},
+ 	{"always",	VMENTER_L1D_FLUSH_ALWAYS},
+ };
+ 
+ #define L1D_CACHE_ORDER 4
+ static void *vmx_l1d_flush_pages;
+ 
+ static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
+ {
+ 	struct page *page;
+ 
+ 	/* If set to 'auto' select 'cond' */
+ 	if (l1tf == VMENTER_L1D_FLUSH_AUTO)
+ 		l1tf = VMENTER_L1D_FLUSH_COND;
+ 
+ 	if (!enable_ept) {
+ 		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+ 		return 0;
+ 	}
+ 
+ 	if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+ 	    !boot_cpu_has(X86_FEATURE_FLUSH_L1D)) {
+ 		page = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);
+ 		if (!page)
+ 			return -ENOMEM;
+ 		vmx_l1d_flush_pages = page_address(page);
+ 	}
+ 
+ 	l1tf_vmx_mitigation = l1tf;
+ 
+ 	if (l1tf != VMENTER_L1D_FLUSH_NEVER)
+ 		static_branch_enable(&vmx_l1d_should_flush);
+ 	return 0;
+ }
+ 
+ static int vmentry_l1d_flush_parse(const char *s)
+ {
+ 	unsigned int i;
+ 
+ 	if (s) {
+ 		for (i = 0; i < ARRAY_SIZE(vmentry_l1d_param); i++) {
+ 			if (!strcmp(s, vmentry_l1d_param[i].option))
+ 				return vmentry_l1d_param[i].cmd;
+ 		}
+ 	}
+ 	return -EINVAL;
+ }
+ 
+ static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
+ {
+ 	int l1tf;
+ 
+ 	if (!boot_cpu_has(X86_BUG_L1TF))
+ 		return 0;
+ 
+ 	l1tf = vmentry_l1d_flush_parse(s);
+ 	if (l1tf < 0)
+ 		return l1tf;
+ 
+ 	/*
+ 	 * Has vmx_init() run already? If not then this is the pre init
+ 	 * parameter parsing. In that case just store the value and let
+ 	 * vmx_init() do the proper setup after enable_ept has been
+ 	 * established.
+ 	 */
+ 	if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+ 		vmentry_l1d_flush_param = l1tf;
+ 		return 0;
+ 	}
+ 
+ 	return vmx_setup_l1d_flush(l1tf);
+ }
+ 
+ static int vmentry_l1d_flush_get(char *s, const struct kernel_param *kp)
+ {
+ 	return sprintf(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+ }
+ 
+ static const struct kernel_param_ops vmentry_l1d_flush_ops = {
+ 	.set = vmentry_l1d_flush_set,
+ 	.get = vmentry_l1d_flush_get,
+ };
+ module_param_cb(vmentry_l1d_flush, &vmentry_l1d_flush_ops, NULL, S_IRUGO);
+ 
+ struct kvm_vmx {
+ 	struct kvm kvm;
+ 
+ 	unsigned int tss_addr;
+ 	bool ept_identity_pagetable_done;
+ 	gpa_t ept_identity_map_addr;
+ };
+ 
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
  #define NR_AUTOLOAD_MSRS 8
  
  struct vmcs {
@@@ -8434,6 -9635,65 +8540,68 @@@ static int vmx_handle_exit(struct kvm_v
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Software based L1D cache flush which is used when microcode providing
+  * the cache control MSR is not loaded.
+  *
+  * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
+  * flush it is required to read in 64 KiB because the replacement algorithm
+  * is not exactly LRU. This could be sized at runtime via topology
+  * information but as all relevant affected CPUs have 32KiB L1D cache size
+  * there is no point in doing so.
+  */
+ #define L1D_CACHE_ORDER 4
+ static void *vmx_l1d_flush_pages;
+ 
+ static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
+ {
+ 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
+ 	bool always;
+ 
+ 	/*
+ 	 * This code is only executed when the the flush mode is 'cond' or
+ 	 * 'always'
+ 	 *
+ 	 * If 'flush always', keep the flush bit set, otherwise clear
+ 	 * it. The flush bit gets set again either from vcpu_run() or from
+ 	 * one of the unsafe VMEXIT handlers.
+ 	 */
+ 	always = l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_ALWAYS;
+ 	vcpu->arch.l1tf_flush_l1d = always;
+ 
+ 	vcpu->stat.l1d_flush++;
+ 
+ 	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
+ 		wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
+ 		return;
+ 	}
+ 
+ 	asm volatile(
+ 		/* First ensure the pages are in the TLB */
+ 		"xorl	%%eax, %%eax\n"
+ 		".Lpopulate_tlb:\n\t"
+ 		"movzbl	(%[empty_zp], %%" _ASM_AX "), %%ecx\n\t"
+ 		"addl	$4096, %%eax\n\t"
+ 		"cmpl	%%eax, %[size]\n\t"
+ 		"jne	.Lpopulate_tlb\n\t"
+ 		"xorl	%%eax, %%eax\n\t"
+ 		"cpuid\n\t"
+ 		/* Now fill the cache */
+ 		"xorl	%%eax, %%eax\n"
+ 		".Lfill_cache:\n"
+ 		"movzbl	(%[empty_zp], %%" _ASM_AX "), %%ecx\n\t"
+ 		"addl	$64, %%eax\n\t"
+ 		"cmpl	%%eax, %[size]\n\t"
+ 		"jne	.Lfill_cache\n\t"
+ 		"lfence\n"
+ 		:: [empty_zp] "r" (vmx_l1d_flush_pages),
+ 		    [size] "r" (size)
+ 		: "eax", "ebx", "ecx", "edx");
+ }
+ 
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
  static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
  {
  	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@@ -11663,22 -13250,17 +11831,36 @@@ static struct kvm_x86_ops vmx_x86_ops 
  	.enable_smi_window = enable_smi_window,
  };
  
++<<<<<<< HEAD
 +static int __init vmx_init(void)
 +{
 +	int r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
 +                     __alignof__(struct vcpu_vmx), THIS_MODULE);
 +	if (r)
 +		return r;
 +
 +#ifdef CONFIG_KEXEC_CORE
 +	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
 +			   crash_vmclear_local_loaded_vmcss);
 +#endif
 +
 +	return 0;
 +}
 +
 +static void __exit vmx_exit(void)
++=======
+ static void vmx_cleanup_l1d_flush(void)
+ {
+ 	if (vmx_l1d_flush_pages) {
+ 		free_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ 		vmx_l1d_flush_pages = NULL;
+ 	}
+ 	/* Restore state so sysfs ignores VMX */
+ 	l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+ }
+ 
+ static void vmx_exit(void)
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
  {
  #ifdef CONFIG_KEXEC_CORE
  	RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
@@@ -11686,7 -13268,92 +11868,72 @@@
  #endif
  
  	kvm_exit();
 -
 -#if IS_ENABLED(CONFIG_HYPERV)
 -	if (static_branch_unlikely(&enable_evmcs)) {
 -		int cpu;
 -		struct hv_vp_assist_page *vp_ap;
 -		/*
 -		 * Reset everything to support using non-enlightened VMCS
 -		 * access later (e.g. when we reload the module with
 -		 * enlightened_vmcs=0)
 -		 */
 -		for_each_online_cpu(cpu) {
 -			vp_ap =	hv_get_vp_assist_page(cpu);
 -
 -			if (!vp_ap)
 -				continue;
 -
 -			vp_ap->current_nested_vmcs = 0;
 -			vp_ap->enlighten_vmentry = 0;
 -		}
 -
 -		static_branch_disable(&enable_evmcs);
 -	}
 -#endif
 -	vmx_cleanup_l1d_flush();
  }
 -module_exit(vmx_exit);
  
++<<<<<<< HEAD
 +module_init(vmx_init)
 +module_exit(vmx_exit)
++=======
+ static int __init vmx_init(void)
+ {
+ 	int r;
+ 
+ #if IS_ENABLED(CONFIG_HYPERV)
+ 	/*
+ 	 * Enlightened VMCS usage should be recommended and the host needs
+ 	 * to support eVMCS v1 or above. We can also disable eVMCS support
+ 	 * with module parameter.
+ 	 */
+ 	if (enlightened_vmcs &&
+ 	    ms_hyperv.hints & HV_X64_ENLIGHTENED_VMCS_RECOMMENDED &&
+ 	    (ms_hyperv.nested_features & HV_X64_ENLIGHTENED_VMCS_VERSION) >=
+ 	    KVM_EVMCS_VERSION) {
+ 		int cpu;
+ 
+ 		/* Check that we have assist pages on all online CPUs */
+ 		for_each_online_cpu(cpu) {
+ 			if (!hv_get_vp_assist_page(cpu)) {
+ 				enlightened_vmcs = false;
+ 				break;
+ 			}
+ 		}
+ 
+ 		if (enlightened_vmcs) {
+ 			pr_info("KVM: vmx: using Hyper-V Enlightened VMCS\n");
+ 			static_branch_enable(&enable_evmcs);
+ 		}
+ 	} else {
+ 		enlightened_vmcs = false;
+ 	}
+ #endif
+ 
+ 	r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ 		     __alignof__(struct vcpu_vmx), THIS_MODULE);
+ 	if (r)
+ 		return r;
+ 
+ 	/*
+ 	 * Must be called after kvm_init() so enable_ept is properly set
+ 	 * up. Hand the parameter mitigation value in which was stored in
+ 	 * the pre module init parser. If no parameter was given, it will
+ 	 * contain 'auto' which will be turned into the default 'cond'
+ 	 * mitigation mode.
+ 	 */
+ 	if (boot_cpu_has(X86_BUG_L1TF)) {
+ 		r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ 		if (r) {
+ 			vmx_exit();
+ 			return r;
+ 		}
+ 	}
+ 
+ #ifdef CONFIG_KEXEC_CORE
+ 	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
+ 			   crash_vmclear_local_loaded_vmcss);
+ #endif
+ 	vmx_check_vmcs12_offsets();
+ 
+ 	return 0;
+ }
+ module_init(vmx_init);
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
* Unmerged path arch/x86/kvm/vmx.c
