md/r5cache: improve recovery with read ahead page pool

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] r5cache: improve recovery with read ahead page pool (Nigel Croxon) [1494474]
Rebuild_FUZZ: 97.14%
commit-author Song Liu <songliubraving@fb.com>
commit effe6ee7523aa50d0517bd7da141e112b44d89fa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/effe6ee7.failed

In r5cache recovery, the journal device is scanned page by page.
Currently, we use sync_page_io() to read journal device. This is
not efficient when we have to recovery many stripes from the journal.

To improve the speed of recovery, this patch introduces a read ahead
page pool (ra_pool) to recovery_ctx. With ra_pool, multiple consecutive
pages are read in one IO. Then the recovery code read the journal from
ra_pool.

With ra_pool, r5l_recovery_ctx has become much bigger. Therefore,
r5l_recovery_log() is refactored so r5l_recovery_ctx is not using
stack space.

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit effe6ee7523aa50d0517bd7da141e112b44d89fa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5-cache.c
diff --cc drivers/md/raid5-cache.c
index 45f5446ca2a5,5c8640c86b90..000000000000
--- a/drivers/md/raid5-cache.c
+++ b/drivers/md/raid5-cache.c
@@@ -1406,9 -1683,11 +1521,16 @@@ static int r5l_recovery_read_meta_block
  	struct page *page = ctx->meta_page;
  	struct r5l_meta_block *mb;
  	u32 crc, stored_crc;
+ 	int ret;
  
++<<<<<<< HEAD
 +	if (!sync_page_io(log->rdev, ctx->pos, PAGE_SIZE, page, READ, false))
 +		return -EIO;
++=======
+ 	ret = r5l_recovery_read_page(log, ctx, page, ctx->pos);
+ 	if (ret != 0)
+ 		return ret;
++>>>>>>> effe6ee7523a (md/r5cache: improve recovery with read ahead page pool)
  
  	mb = page_address(page);
  	stored_crc = le32_to_cpu(mb->checksum);
@@@ -1490,8 -1769,7 +1612,12 @@@ static void r5l_recovery_load_data(stru
  	raid5_compute_sector(conf,
  			     le64_to_cpu(payload->location), 0,
  			     &dd_idx, sh);
++<<<<<<< HEAD
 +	sync_page_io(log->rdev, log_offset, PAGE_SIZE,
 +		     sh->dev[dd_idx].page, READ, false);
++=======
+ 	r5l_recovery_read_page(log, ctx, sh->dev[dd_idx].page, log_offset);
++>>>>>>> effe6ee7523a (md/r5cache: improve recovery with read ahead page pool)
  	sh->dev[dd_idx].log_checksum =
  		le32_to_cpu(payload->checksum[0]);
  	ctx->meta_total_blocks += BLOCK_SECTORS;
@@@ -1510,17 -1788,15 +1636,27 @@@ static void r5l_recovery_load_parity(st
  	struct r5conf *conf = mddev->private;
  
  	ctx->meta_total_blocks += BLOCK_SECTORS * conf->max_degraded;
++<<<<<<< HEAD
 +	sync_page_io(log->rdev, log_offset, PAGE_SIZE,
 +		     sh->dev[sh->pd_idx].page, READ, false);
++=======
+ 	r5l_recovery_read_page(log, ctx, sh->dev[sh->pd_idx].page, log_offset);
++>>>>>>> effe6ee7523a (md/r5cache: improve recovery with read ahead page pool)
  	sh->dev[sh->pd_idx].log_checksum =
  		le32_to_cpu(payload->checksum[0]);
  	set_bit(R5_Wantwrite, &sh->dev[sh->pd_idx].flags);
  
  	if (sh->qd_idx >= 0) {
++<<<<<<< HEAD
 +		sync_page_io(log->rdev,
 +			     r5l_ring_add(log, log_offset, BLOCK_SECTORS),
 +			     PAGE_SIZE, sh->dev[sh->qd_idx].page,
 +			     READ, false);
++=======
+ 		r5l_recovery_read_page(
+ 			log, ctx, sh->dev[sh->qd_idx].page,
+ 			r5l_ring_add(log, log_offset, BLOCK_SECTORS));
++>>>>>>> effe6ee7523a (md/r5cache: improve recovery with read ahead page pool)
  		sh->dev[sh->qd_idx].log_checksum =
  			le32_to_cpu(payload->checksum[1]);
  		set_bit(R5_Wantwrite, &sh->dev[sh->qd_idx].flags);
@@@ -1657,8 -1935,7 +1795,12 @@@ r5l_recovery_verify_data_checksum(struc
  	void *addr;
  	u32 checksum;
  
++<<<<<<< HEAD
 +	sync_page_io(log->rdev, log_offset, PAGE_SIZE,
 +		     page, READ, false);
++=======
+ 	r5l_recovery_read_page(log, ctx, page, log_offset);
++>>>>>>> effe6ee7523a (md/r5cache: improve recovery with read ahead page pool)
  	addr = kmap_atomic(page);
  	checksum = crc32c_le(log->uuid_checksum, addr, PAGE_SIZE);
  	kunmap_atomic(addr);
* Unmerged path drivers/md/raid5-cache.c
