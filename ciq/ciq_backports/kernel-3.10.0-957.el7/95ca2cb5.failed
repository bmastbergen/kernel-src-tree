KVM: s390: Add sthyi emulation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Janosch Frank <frankja@linux.vnet.ibm.com>
commit 95ca2cb57985b07f5b136405f80a5106f5b06641
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/95ca2cb5.failed

Store Hypervisor Information is an emulated z/VM instruction that
provides a guest with basic information about the layers it is running
on. This includes information about the cpu configuration of both the
machine and the lpar, as well as their names, machine model and
machine type. This information enables an application to determine the
maximum capacity of CPs and IFLs available to software.

The instruction is available whenever the facility bit 74 is set,
otherwise executing it results in an operation exception.

It is important to check the validity flags in the sections before
using data from any structure member. It is not guaranteed that all
members will be valid on all machines / machine configurations.

	Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
	Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
	Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
(cherry picked from commit 95ca2cb57985b07f5b136405f80a5106f5b06641)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/diag.h
#	arch/s390/include/asm/kvm_host.h
#	arch/s390/include/uapi/asm/sie.h
#	arch/s390/kvm/Makefile
#	arch/s390/kvm/intercept.c
#	arch/s390/kvm/kvm-s390.c
#	arch/s390/kvm/trace.h
diff --cc arch/s390/include/asm/diag.h
index 244335e704b3,82211998ccf7..000000000000
--- a/arch/s390/include/asm/diag.h
+++ b/arch/s390/include/asm/diag.h
@@@ -51,29 -78,153 +51,137 @@@ struct diag210 
  
  extern int diag210(struct diag210 *addr);
  
 -/* bit is set in flags, when physical cpu info is included in diag 204 data */
 -#define DIAG204_LPAR_PHYS_FLG 0x80
 -#define DIAG204_LPAR_NAME_LEN 8		/* lpar name len in diag 204 data */
 -#define DIAG204_CPU_NAME_LEN 16		/* type name len of cpus in diag224 name table */
 -
 -/* diag 204 subcodes */
 -enum diag204_sc {
 -	DIAG204_SUBC_STIB4 = 4,
 -	DIAG204_SUBC_RSI = 5,
 -	DIAG204_SUBC_STIB6 = 6,
 -	DIAG204_SUBC_STIB7 = 7
 +enum diag26c_sc {
 +	DIAG26C_MAC_SERVICES = 0x00000030
  };
  
 -/* The two available diag 204 data formats */
 -enum diag204_format {
 -	DIAG204_INFO_SIMPLE = 0,
 -	DIAG204_INFO_EXT = 0x00010000
 +enum diag26c_version {
 +	DIAG26C_VERSION2 = 0x00000002	/* z/VM 5.4.0 */
  };
  
 -enum diag204_cpu_flags {
 -	DIAG204_CPU_ONLINE = 0x20,
 -	DIAG204_CPU_CAPPED = 0x40,
 +#define DIAG26C_GET_MAC	0x0000
 +struct diag26c_mac_req {
 +	u32	resp_buf_len;
 +	u32	resp_version;
 +	u16	op_code;
 +	u16	devno;
 +	u8	res[4];
  };
  
 -struct diag204_info_blk_hdr {
 -	__u8  npar;
 -	__u8  flags;
 -	__u16 tslice;
 -	__u16 phys_cpus;
 -	__u16 this_part;
 -	__u64 curtod;
 -} __packed;
 +struct diag26c_mac_resp {
 +	u32	version;
 +	u8	mac[ETH_ALEN];
 +	u8	res[2];
 +} __aligned(8);
  
 -struct diag204_x_info_blk_hdr {
 -	__u8  npar;
 -	__u8  flags;
 -	__u16 tslice;
 -	__u16 phys_cpus;
 -	__u16 this_part;
 -	__u64 curtod1;
 -	__u64 curtod2;
 -	char reserved[40];
 -} __packed;
 +int diag26c(void *req, void *resp, enum diag26c_sc subcode);
  
++<<<<<<< HEAD
++=======
+ struct diag204_part_hdr {
+ 	__u8 pn;
+ 	__u8 cpus;
+ 	char reserved[6];
+ 	char part_name[DIAG204_LPAR_NAME_LEN];
+ } __packed;
+ 
+ struct diag204_x_part_hdr {
+ 	__u8  pn;
+ 	__u8  cpus;
+ 	__u8  rcpus;
+ 	__u8  pflag;
+ 	__u32 mlu;
+ 	char  part_name[DIAG204_LPAR_NAME_LEN];
+ 	char  lpc_name[8];
+ 	char  os_name[8];
+ 	__u64 online_cs;
+ 	__u64 online_es;
+ 	__u8  upid;
+ 	__u8  reserved:3;
+ 	__u8  mtid:5;
+ 	char  reserved1[2];
+ 	__u32 group_mlu;
+ 	char  group_name[8];
+ 	char  hardware_group_name[8];
+ 	char  reserved2[24];
+ } __packed;
+ 
+ struct diag204_cpu_info {
+ 	__u16 cpu_addr;
+ 	char  reserved1[2];
+ 	__u8  ctidx;
+ 	__u8  cflag;
+ 	__u16 weight;
+ 	__u64 acc_time;
+ 	__u64 lp_time;
+ } __packed;
+ 
+ struct diag204_x_cpu_info {
+ 	__u16 cpu_addr;
+ 	char  reserved1[2];
+ 	__u8  ctidx;
+ 	__u8  cflag;
+ 	__u16 weight;
+ 	__u64 acc_time;
+ 	__u64 lp_time;
+ 	__u16 min_weight;
+ 	__u16 cur_weight;
+ 	__u16 max_weight;
+ 	char  reseved2[2];
+ 	__u64 online_time;
+ 	__u64 wait_time;
+ 	__u32 pma_weight;
+ 	__u32 polar_weight;
+ 	__u32 cpu_type_cap;
+ 	__u32 group_cpu_type_cap;
+ 	char  reserved3[32];
+ } __packed;
+ 
+ struct diag204_phys_hdr {
+ 	char reserved1[1];
+ 	__u8 cpus;
+ 	char reserved2[6];
+ 	char mgm_name[8];
+ } __packed;
+ 
+ struct diag204_x_phys_hdr {
+ 	char reserved1[1];
+ 	__u8 cpus;
+ 	char reserved2[6];
+ 	char mgm_name[8];
+ 	char reserved3[80];
+ } __packed;
+ 
+ struct diag204_phys_cpu {
+ 	__u16 cpu_addr;
+ 	char  reserved1[2];
+ 	__u8  ctidx;
+ 	char  reserved2[3];
+ 	__u64 mgm_time;
+ 	char  reserved3[8];
+ } __packed;
+ 
+ struct diag204_x_phys_cpu {
+ 	__u16 cpu_addr;
+ 	char  reserved1[2];
+ 	__u8  ctidx;
+ 	char  reserved2[1];
+ 	__u16 weight;
+ 	__u64 mgm_time;
+ 	char  reserved3[80];
+ } __packed;
+ 
+ struct diag204_x_part_block {
+ 	struct diag204_x_part_hdr hdr;
+ 	struct diag204_x_cpu_info cpus[];
+ } __packed;
+ 
+ struct diag204_x_phys_block {
+ 	struct diag204_x_phys_hdr hdr;
+ 	struct diag204_x_phys_cpu cpus[];
+ } __packed;
+ 
+ int diag204(unsigned long subcode, unsigned long size, void *addr);
+ int diag224(void *ptr);
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  #endif /* _ASM_S390_DIAG_H */
diff --cc arch/s390/include/asm/kvm_host.h
index 0aaab12c8aec,7233b1c49964..000000000000
--- a/arch/s390/include/asm/kvm_host.h
+++ b/arch/s390/include/asm/kvm_host.h
@@@ -90,11 -151,27 +90,22 @@@ struct kvm_s390_sie_block 
  #define LCTL_CR14	0x0002
  	__u16   lctl;			/* 0x0044 */
  	__s16	icpua;			/* 0x0046 */
++<<<<<<< HEAD
 +#define ICTL_LPSW 0x00400000
++=======
+ #define ICTL_OPEREXC	0x80000000
+ #define ICTL_PINT	0x20000000
+ #define ICTL_LPSW	0x00400000
+ #define ICTL_STCTL	0x00040000
+ #define ICTL_ISKE	0x00004000
+ #define ICTL_SSKE	0x00002000
+ #define ICTL_RRBE	0x00001000
+ #define ICTL_TPROT	0x00000200
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  	__u32	ictl;			/* 0x0048 */
  	__u32	eca;			/* 0x004c */
 -#define ICPT_INST	0x04
 -#define ICPT_PROGI	0x08
 -#define ICPT_INSTPROGI	0x0C
 -#define ICPT_OPEREXC	0x2C
 -#define ICPT_PARTEXEC	0x38
 -#define ICPT_IOINST	0x40
  	__u8	icptcode;		/* 0x0050 */
 -	__u8	icptstatus;		/* 0x0051 */
 +	__u8	reserved51;		/* 0x0051 */
  	__u16	ihcpu;			/* 0x0052 */
  	__u8	reserved54[2];		/* 0x0054 */
  	__u16	ipa;			/* 0x0056 */
@@@ -118,97 -215,227 +129,164 @@@
  	__u64	gbea;			/* 0x0180 */
  	__u8	reserved188[24];	/* 0x0188 */
  	__u32	fac;			/* 0x01a0 */
 -	__u8	reserved1a4[20];	/* 0x01a4 */
 -	__u64	cbrlo;			/* 0x01b8 */
 -	__u8	reserved1c0[8];		/* 0x01c0 */
 -	__u32	ecd;			/* 0x01c8 */
 -	__u8	reserved1cc[18];	/* 0x01cc */
 -	__u64	pp;			/* 0x01de */
 -	__u8	reserved1e6[2];		/* 0x01e6 */
 -	__u64	itdba;			/* 0x01e8 */
 -	__u64   riccbd;			/* 0x01f0 */
 -	__u8    reserved1f8[8];		/* 0x01f8 */
 +	__u8	reserved1a4[92];	/* 0x01a4 */
  } __attribute__((packed));
  
 -struct kvm_s390_itdb {
 -	__u8	data[256];
 -} __packed;
 -
 -struct sie_page {
 -	struct kvm_s390_sie_block sie_block;
 -	__u8 reserved200[1024];		/* 0x0200 */
 -	struct kvm_s390_itdb itdb;	/* 0x0600 */
 -	__u8 reserved700[2304];		/* 0x0700 */
 -} __packed;
 -
  struct kvm_vcpu_stat {
++<<<<<<< HEAD
 +	u64 exit_userspace;
 +	u64 exit_null;
 +	u64 exit_external_request;
 +	u64 exit_external_interrupt;
 +	u64 exit_stop_request;
 +	u64 exit_validity;
 +	u64 exit_instruction;
 +	u64 halt_successful_poll;
 +	u64 halt_attempted_poll;
 +	u64 instruction_lctl;
 +	u64 instruction_lctlg;
 +	u64 exit_program_interruption;
 +	u64 exit_instr_and_program;
 +	u64 deliver_external_call;
 +	u64 deliver_emergency_signal;
 +	u64 deliver_service_signal;
 +	u64 deliver_virtio_interrupt;
 +	u64 deliver_stop_signal;
 +	u64 deliver_prefix_signal;
 +	u64 deliver_restart_signal;
 +	u64 deliver_program_int;
 +	u64 deliver_io_int;
 +	u64 exit_wait_state;
 +	u64 instruction_pfmf;
 +	u64 instruction_stidp;
 +	u64 instruction_spx;
 +	u64 instruction_stpx;
 +	u64 instruction_stap;
 +	u64 instruction_storage_key;
 +	u64 instruction_stsch;
 +	u64 instruction_chsc;
 +	u64 instruction_stsi;
 +	u64 instruction_stfl;
 +	u64 instruction_tprot;
 +	u64 instruction_sigp_sense;
 +	u64 instruction_sigp_sense_running;
 +	u64 instruction_sigp_external_call;
 +	u64 instruction_sigp_emergency;
 +	u64 instruction_sigp_stop;
 +	u64 instruction_sigp_arch;
 +	u64 instruction_sigp_prefix;
 +	u64 instruction_sigp_restart;
 +	u64 diagnose_10;
 +	u64 diagnose_44;
 +	u64 diagnose_9c;
++=======
+ 	u32 exit_userspace;
+ 	u32 exit_null;
+ 	u32 exit_external_request;
+ 	u32 exit_external_interrupt;
+ 	u32 exit_stop_request;
+ 	u32 exit_validity;
+ 	u32 exit_instruction;
+ 	u32 halt_successful_poll;
+ 	u32 halt_attempted_poll;
+ 	u32 halt_poll_invalid;
+ 	u32 halt_wakeup;
+ 	u32 instruction_lctl;
+ 	u32 instruction_lctlg;
+ 	u32 instruction_stctl;
+ 	u32 instruction_stctg;
+ 	u32 exit_program_interruption;
+ 	u32 exit_instr_and_program;
+ 	u32 exit_operation_exception;
+ 	u32 deliver_external_call;
+ 	u32 deliver_emergency_signal;
+ 	u32 deliver_service_signal;
+ 	u32 deliver_virtio_interrupt;
+ 	u32 deliver_stop_signal;
+ 	u32 deliver_prefix_signal;
+ 	u32 deliver_restart_signal;
+ 	u32 deliver_program_int;
+ 	u32 deliver_io_int;
+ 	u32 exit_wait_state;
+ 	u32 instruction_pfmf;
+ 	u32 instruction_stidp;
+ 	u32 instruction_spx;
+ 	u32 instruction_stpx;
+ 	u32 instruction_stap;
+ 	u32 instruction_storage_key;
+ 	u32 instruction_ipte_interlock;
+ 	u32 instruction_stsch;
+ 	u32 instruction_chsc;
+ 	u32 instruction_stsi;
+ 	u32 instruction_stfl;
+ 	u32 instruction_tprot;
+ 	u32 instruction_essa;
+ 	u32 instruction_sthyi;
+ 	u32 instruction_sigp_sense;
+ 	u32 instruction_sigp_sense_running;
+ 	u32 instruction_sigp_external_call;
+ 	u32 instruction_sigp_emergency;
+ 	u32 instruction_sigp_cond_emergency;
+ 	u32 instruction_sigp_start;
+ 	u32 instruction_sigp_stop;
+ 	u32 instruction_sigp_stop_store_status;
+ 	u32 instruction_sigp_store_status;
+ 	u32 instruction_sigp_store_adtl_status;
+ 	u32 instruction_sigp_arch;
+ 	u32 instruction_sigp_prefix;
+ 	u32 instruction_sigp_restart;
+ 	u32 instruction_sigp_init_cpu_reset;
+ 	u32 instruction_sigp_cpu_reset;
+ 	u32 instruction_sigp_unknown;
+ 	u32 diagnose_10;
+ 	u32 diagnose_44;
+ 	u32 diagnose_9c;
+ 	u32 diagnose_258;
+ 	u32 diagnose_308;
+ 	u32 diagnose_500;
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  };
  
 -#define PGM_OPERATION			0x01
 -#define PGM_PRIVILEGED_OP		0x02
 -#define PGM_EXECUTE			0x03
 -#define PGM_PROTECTION			0x04
 -#define PGM_ADDRESSING			0x05
 -#define PGM_SPECIFICATION		0x06
 -#define PGM_DATA			0x07
 -#define PGM_FIXED_POINT_OVERFLOW	0x08
 -#define PGM_FIXED_POINT_DIVIDE		0x09
 -#define PGM_DECIMAL_OVERFLOW		0x0a
 -#define PGM_DECIMAL_DIVIDE		0x0b
 -#define PGM_HFP_EXPONENT_OVERFLOW	0x0c
 -#define PGM_HFP_EXPONENT_UNDERFLOW	0x0d
 -#define PGM_HFP_SIGNIFICANCE		0x0e
 -#define PGM_HFP_DIVIDE			0x0f
 -#define PGM_SEGMENT_TRANSLATION		0x10
 -#define PGM_PAGE_TRANSLATION		0x11
 -#define PGM_TRANSLATION_SPEC		0x12
 -#define PGM_SPECIAL_OPERATION		0x13
 -#define PGM_OPERAND			0x15
 -#define PGM_TRACE_TABEL			0x16
 -#define PGM_VECTOR_PROCESSING		0x1b
 -#define PGM_SPACE_SWITCH		0x1c
 -#define PGM_HFP_SQUARE_ROOT		0x1d
 -#define PGM_PC_TRANSLATION_SPEC		0x1f
 -#define PGM_AFX_TRANSLATION		0x20
 -#define PGM_ASX_TRANSLATION		0x21
 -#define PGM_LX_TRANSLATION		0x22
 -#define PGM_EX_TRANSLATION		0x23
 -#define PGM_PRIMARY_AUTHORITY		0x24
 -#define PGM_SECONDARY_AUTHORITY		0x25
 -#define PGM_LFX_TRANSLATION		0x26
 -#define PGM_LSX_TRANSLATION		0x27
 -#define PGM_ALET_SPECIFICATION		0x28
 -#define PGM_ALEN_TRANSLATION		0x29
 -#define PGM_ALE_SEQUENCE		0x2a
 -#define PGM_ASTE_VALIDITY		0x2b
 -#define PGM_ASTE_SEQUENCE		0x2c
 -#define PGM_EXTENDED_AUTHORITY		0x2d
 -#define PGM_LSTE_SEQUENCE		0x2e
 -#define PGM_ASTE_INSTANCE		0x2f
 -#define PGM_STACK_FULL			0x30
 -#define PGM_STACK_EMPTY			0x31
 -#define PGM_STACK_SPECIFICATION		0x32
 -#define PGM_STACK_TYPE			0x33
 -#define PGM_STACK_OPERATION		0x34
 -#define PGM_ASCE_TYPE			0x38
 -#define PGM_REGION_FIRST_TRANS		0x39
 -#define PGM_REGION_SECOND_TRANS		0x3a
 -#define PGM_REGION_THIRD_TRANS		0x3b
 -#define PGM_MONITOR			0x40
 -#define PGM_PER				0x80
 -#define PGM_CRYPTO_OPERATION		0x119
 -
 -/* irq types in order of priority */
 -enum irq_types {
 -	IRQ_PEND_MCHK_EX = 0,
 -	IRQ_PEND_SVC,
 -	IRQ_PEND_PROG,
 -	IRQ_PEND_MCHK_REP,
 -	IRQ_PEND_EXT_IRQ_KEY,
 -	IRQ_PEND_EXT_MALFUNC,
 -	IRQ_PEND_EXT_EMERGENCY,
 -	IRQ_PEND_EXT_EXTERNAL,
 -	IRQ_PEND_EXT_CLOCK_COMP,
 -	IRQ_PEND_EXT_CPU_TIMER,
 -	IRQ_PEND_EXT_TIMING,
 -	IRQ_PEND_EXT_SERVICE,
 -	IRQ_PEND_EXT_HOST,
 -	IRQ_PEND_PFAULT_INIT,
 -	IRQ_PEND_PFAULT_DONE,
 -	IRQ_PEND_VIRTIO,
 -	IRQ_PEND_IO_ISC_0,
 -	IRQ_PEND_IO_ISC_1,
 -	IRQ_PEND_IO_ISC_2,
 -	IRQ_PEND_IO_ISC_3,
 -	IRQ_PEND_IO_ISC_4,
 -	IRQ_PEND_IO_ISC_5,
 -	IRQ_PEND_IO_ISC_6,
 -	IRQ_PEND_IO_ISC_7,
 -	IRQ_PEND_SIGP_STOP,
 -	IRQ_PEND_RESTART,
 -	IRQ_PEND_SET_PREFIX,
 -	IRQ_PEND_COUNT
 +struct kvm_s390_io_info {
 +	__u16        subchannel_id;            /* 0x0b8 */
 +	__u16        subchannel_nr;            /* 0x0ba */
 +	__u32        io_int_parm;              /* 0x0bc */
 +	__u32        io_int_word;              /* 0x0c0 */
  };
  
 -/* We have 2M for virtio device descriptor pages. Smallest amount of
 - * memory per page is 24 bytes (1 queue), so (2048*1024) / 24 = 87381
 - */
 -#define KVM_S390_MAX_VIRTIO_IRQS 87381
 +struct kvm_s390_ext_info {
 +	__u32 ext_params;
 +	__u64 ext_params2;
 +};
  
 -/*
 - * Repressible (non-floating) machine check interrupts
 - * subclass bits in MCIC
 - */
 -#define MCHK_EXTD_BIT 58
 -#define MCHK_DEGR_BIT 56
 -#define MCHK_WARN_BIT 55
 -#define MCHK_REP_MASK ((1UL << MCHK_DEGR_BIT) | \
 -		       (1UL << MCHK_EXTD_BIT) | \
 -		       (1UL << MCHK_WARN_BIT))
 -
 -/* Exigent machine check interrupts subclass bits in MCIC */
 -#define MCHK_SD_BIT 63
 -#define MCHK_PD_BIT 62
 -#define MCHK_EX_MASK ((1UL << MCHK_SD_BIT) | (1UL << MCHK_PD_BIT))
 -
 -#define IRQ_PEND_EXT_MASK ((1UL << IRQ_PEND_EXT_IRQ_KEY)    | \
 -			   (1UL << IRQ_PEND_EXT_CLOCK_COMP) | \
 -			   (1UL << IRQ_PEND_EXT_CPU_TIMER)  | \
 -			   (1UL << IRQ_PEND_EXT_MALFUNC)    | \
 -			   (1UL << IRQ_PEND_EXT_EMERGENCY)  | \
 -			   (1UL << IRQ_PEND_EXT_EXTERNAL)   | \
 -			   (1UL << IRQ_PEND_EXT_TIMING)     | \
 -			   (1UL << IRQ_PEND_EXT_HOST)       | \
 -			   (1UL << IRQ_PEND_EXT_SERVICE)    | \
 -			   (1UL << IRQ_PEND_VIRTIO)         | \
 -			   (1UL << IRQ_PEND_PFAULT_INIT)    | \
 -			   (1UL << IRQ_PEND_PFAULT_DONE))
 -
 -#define IRQ_PEND_IO_MASK ((1UL << IRQ_PEND_IO_ISC_0) | \
 -			  (1UL << IRQ_PEND_IO_ISC_1) | \
 -			  (1UL << IRQ_PEND_IO_ISC_2) | \
 -			  (1UL << IRQ_PEND_IO_ISC_3) | \
 -			  (1UL << IRQ_PEND_IO_ISC_4) | \
 -			  (1UL << IRQ_PEND_IO_ISC_5) | \
 -			  (1UL << IRQ_PEND_IO_ISC_6) | \
 -			  (1UL << IRQ_PEND_IO_ISC_7))
 -
 -#define IRQ_PEND_MCHK_MASK ((1UL << IRQ_PEND_MCHK_REP) | \
 -			    (1UL << IRQ_PEND_MCHK_EX))
 +#define PGM_OPERATION            0x01
 +#define PGM_PRIVILEGED_OP	 0x02
 +#define PGM_EXECUTE              0x03
 +#define PGM_PROTECTION           0x04
 +#define PGM_ADDRESSING           0x05
 +#define PGM_SPECIFICATION        0x06
 +#define PGM_DATA                 0x07
 +
 +struct kvm_s390_pgm_info {
 +	__u16 code;
 +};
 +
 +struct kvm_s390_prefix_info {
 +	__u32 address;
 +};
 +
 +struct kvm_s390_extcall_info {
 +	__u16 code;
 +};
 +
 +struct kvm_s390_emerg_info {
 +	__u16 code;
 +};
 +
 +struct kvm_s390_mchk_info {
 +	__u64 cr14;
 +	__u64 mcic;
 +};
  
  struct kvm_s390_interrupt_info {
  	struct list_head list;
diff --cc arch/s390/kvm/Makefile
index 40b4c6470f88,82e73e2b953d..000000000000
--- a/arch/s390/kvm/Makefile
+++ b/arch/s390/kvm/Makefile
@@@ -11,5 -11,7 +11,11 @@@ common-objs = $(KVM)/kvm_main.o $(KVM)/
  
  ccflags-y := -Ivirt/kvm -Iarch/s390/kvm
  
++<<<<<<< HEAD
 +kvm-objs := $(common-objs) kvm-s390.o intercept.o interrupt.o priv.o sigp.o diag.o
++=======
+ kvm-objs := $(common-objs) kvm-s390.o intercept.o interrupt.o priv.o sigp.o
+ kvm-objs += diag.o gaccess.o guestdbg.o sthyi.o
+ 
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  obj-$(CONFIG_KVM) += kvm.o
diff --cc arch/s390/kvm/intercept.c
index 5ee56e5acc23,9359f65c8634..000000000000
--- a/arch/s390/kvm/intercept.c
+++ b/arch/s390/kvm/intercept.c
@@@ -115,40 -136,231 +115,150 @@@ static int handle_instruction(struct kv
  	return -EOPNOTSUPP;
  }
  
 -static int inject_prog_on_prog_intercept(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_pgm_info pgm_info = {
 -		.code = vcpu->arch.sie_block->iprcc,
 -		/* the PSW has already been rewound */
 -		.flags = KVM_S390_PGM_FLAGS_NO_REWIND,
 -	};
 -
 -	switch (vcpu->arch.sie_block->iprcc & ~PGM_PER) {
 -	case PGM_AFX_TRANSLATION:
 -	case PGM_ASX_TRANSLATION:
 -	case PGM_EX_TRANSLATION:
 -	case PGM_LFX_TRANSLATION:
 -	case PGM_LSTE_SEQUENCE:
 -	case PGM_LSX_TRANSLATION:
 -	case PGM_LX_TRANSLATION:
 -	case PGM_PRIMARY_AUTHORITY:
 -	case PGM_SECONDARY_AUTHORITY:
 -	case PGM_SPACE_SWITCH:
 -		pgm_info.trans_exc_code = vcpu->arch.sie_block->tecmc;
 -		break;
 -	case PGM_ALEN_TRANSLATION:
 -	case PGM_ALE_SEQUENCE:
 -	case PGM_ASTE_INSTANCE:
 -	case PGM_ASTE_SEQUENCE:
 -	case PGM_ASTE_VALIDITY:
 -	case PGM_EXTENDED_AUTHORITY:
 -		pgm_info.exc_access_id = vcpu->arch.sie_block->eai;
 -		break;
 -	case PGM_ASCE_TYPE:
 -	case PGM_PAGE_TRANSLATION:
 -	case PGM_REGION_FIRST_TRANS:
 -	case PGM_REGION_SECOND_TRANS:
 -	case PGM_REGION_THIRD_TRANS:
 -	case PGM_SEGMENT_TRANSLATION:
 -		pgm_info.trans_exc_code = vcpu->arch.sie_block->tecmc;
 -		pgm_info.exc_access_id  = vcpu->arch.sie_block->eai;
 -		pgm_info.op_access_id  = vcpu->arch.sie_block->oai;
 -		break;
 -	case PGM_MONITOR:
 -		pgm_info.mon_class_nr = vcpu->arch.sie_block->mcn;
 -		pgm_info.mon_code = vcpu->arch.sie_block->tecmc;
 -		break;
 -	case PGM_VECTOR_PROCESSING:
 -	case PGM_DATA:
 -		pgm_info.data_exc_code = vcpu->arch.sie_block->dxc;
 -		break;
 -	case PGM_PROTECTION:
 -		pgm_info.trans_exc_code = vcpu->arch.sie_block->tecmc;
 -		pgm_info.exc_access_id  = vcpu->arch.sie_block->eai;
 -		break;
 -	default:
 -		break;
 -	}
 -
 -	if (vcpu->arch.sie_block->iprcc & PGM_PER) {
 -		pgm_info.per_code = vcpu->arch.sie_block->perc;
 -		pgm_info.per_atmid = vcpu->arch.sie_block->peratmid;
 -		pgm_info.per_address = vcpu->arch.sie_block->peraddr;
 -		pgm_info.per_access_id = vcpu->arch.sie_block->peraid;
 -	}
 -	return kvm_s390_inject_prog_irq(vcpu, &pgm_info);
 -}
 -
 -/*
 - * restore ITDB to program-interruption TDB in guest lowcore
 - * and set TX abort indication if required
 -*/
 -static int handle_itdb(struct kvm_vcpu *vcpu)
 +static int handle_prog(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_s390_itdb *itdb;
 -	int rc;
 -
 -	if (!IS_TE_ENABLED(vcpu) || !IS_ITDB_VALID(vcpu))
 -		return 0;
 -	if (current->thread.per_flags & PER_FLAG_NO_TE)
 -		return 0;
 -	itdb = (struct kvm_s390_itdb *)vcpu->arch.sie_block->itdba;
 -	rc = write_guest_lc(vcpu, __LC_PGM_TDB, itdb, sizeof(*itdb));
 -	if (rc)
 -		return rc;
 -	memset(itdb, 0, sizeof(*itdb));
 -
 -	return 0;
 +	vcpu->stat.exit_program_interruption++;
 +	trace_kvm_s390_intercept_prog(vcpu, vcpu->arch.sie_block->iprcc);
 +	return kvm_s390_inject_program_int(vcpu, vcpu->arch.sie_block->iprcc);
  }
  
 -#define per_event(vcpu) (vcpu->arch.sie_block->iprcc & PGM_PER)
 -
 -static int handle_prog(struct kvm_vcpu *vcpu)
 +static int handle_instruction_and_prog(struct kvm_vcpu *vcpu)
  {
 -	psw_t psw;
 -	int rc;
 +	int rc, rc2;
  
 -	vcpu->stat.exit_program_interruption++;
 +	vcpu->stat.exit_instr_and_program++;
 +	rc = handle_instruction(vcpu);
 +	rc2 = handle_prog(vcpu);
  
 -	if (guestdbg_enabled(vcpu) && per_event(vcpu)) {
 -		kvm_s390_handle_per_event(vcpu);
 -		/* the interrupt might have been filtered out completely */
 -		if (vcpu->arch.sie_block->iprcc == 0)
 -			return 0;
 -	}
 -
 -	trace_kvm_s390_intercept_prog(vcpu, vcpu->arch.sie_block->iprcc);
 -	if (vcpu->arch.sie_block->iprcc == PGM_SPECIFICATION) {
 -		rc = read_guest_lc(vcpu, __LC_PGM_NEW_PSW, &psw, sizeof(psw_t));
 -		if (rc)
 -			return rc;
 -		/* Avoid endless loops of specification exceptions */
 -		if (!is_valid_psw(&psw))
 -			return -EOPNOTSUPP;
 -	}
 -	rc = handle_itdb(vcpu);
 +	if (rc == -EOPNOTSUPP)
 +		vcpu->arch.sie_block->icptcode = 0x04;
  	if (rc)
  		return rc;
 -
 -	return inject_prog_on_prog_intercept(vcpu);
 +	return rc2;
  }
  
++<<<<<<< HEAD
 +static const intercept_handler_t intercept_funcs[] = {
 +	[0x00 >> 2] = handle_noop,
 +	[0x04 >> 2] = handle_instruction,
 +	[0x08 >> 2] = handle_prog,
 +	[0x0C >> 2] = handle_instruction_and_prog,
 +	[0x10 >> 2] = handle_noop,
 +	[0x14 >> 2] = handle_noop,
 +	[0x18 >> 2] = handle_noop,
 +	[0x1C >> 2] = kvm_s390_handle_wait,
 +	[0x20 >> 2] = handle_validity,
 +	[0x28 >> 2] = handle_stop,
 +};
++=======
+ /**
+  * handle_external_interrupt - used for external interruption interceptions
+  *
+  * This interception only occurs if the CPUSTAT_EXT_INT bit was set, or if
+  * the new PSW does not have external interrupts disabled. In the first case,
+  * we've got to deliver the interrupt manually, and in the second case, we
+  * drop to userspace to handle the situation there.
+  */
+ static int handle_external_interrupt(struct kvm_vcpu *vcpu)
+ {
+ 	u16 eic = vcpu->arch.sie_block->eic;
+ 	struct kvm_s390_irq irq;
+ 	psw_t newpsw;
+ 	int rc;
+ 
+ 	vcpu->stat.exit_external_interrupt++;
+ 
+ 	rc = read_guest_lc(vcpu, __LC_EXT_NEW_PSW, &newpsw, sizeof(psw_t));
+ 	if (rc)
+ 		return rc;
+ 	/* We can not handle clock comparator or timer interrupt with bad PSW */
+ 	if ((eic == EXT_IRQ_CLK_COMP || eic == EXT_IRQ_CPU_TIMER) &&
+ 	    (newpsw.mask & PSW_MASK_EXT))
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (eic) {
+ 	case EXT_IRQ_CLK_COMP:
+ 		irq.type = KVM_S390_INT_CLOCK_COMP;
+ 		break;
+ 	case EXT_IRQ_CPU_TIMER:
+ 		irq.type = KVM_S390_INT_CPU_TIMER;
+ 		break;
+ 	case EXT_IRQ_EXTERNAL_CALL:
+ 		irq.type = KVM_S390_INT_EXTERNAL_CALL;
+ 		irq.u.extcall.code = vcpu->arch.sie_block->extcpuaddr;
+ 		rc = kvm_s390_inject_vcpu(vcpu, &irq);
+ 		/* ignore if another external call is already pending */
+ 		if (rc == -EBUSY)
+ 			return 0;
+ 		return rc;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	return kvm_s390_inject_vcpu(vcpu, &irq);
+ }
+ 
+ /**
+  * Handle MOVE PAGE partial execution interception.
+  *
+  * This interception can only happen for guests with DAT disabled and
+  * addresses that are currently not mapped in the host. Thus we try to
+  * set up the mappings for the corresponding user pages here (or throw
+  * addressing exceptions in case of illegal guest addresses).
+  */
+ static int handle_mvpg_pei(struct kvm_vcpu *vcpu)
+ {
+ 	unsigned long srcaddr, dstaddr;
+ 	int reg1, reg2, rc;
+ 
+ 	kvm_s390_get_regs_rre(vcpu, &reg1, &reg2);
+ 
+ 	/* Make sure that the source is paged-in */
+ 	rc = guest_translate_address(vcpu, vcpu->run->s.regs.gprs[reg2],
+ 				     reg2, &srcaddr, GACC_FETCH);
+ 	if (rc)
+ 		return kvm_s390_inject_prog_cond(vcpu, rc);
+ 	rc = kvm_arch_fault_in_page(vcpu, srcaddr, 0);
+ 	if (rc != 0)
+ 		return rc;
+ 
+ 	/* Make sure that the destination is paged-in */
+ 	rc = guest_translate_address(vcpu, vcpu->run->s.regs.gprs[reg1],
+ 				     reg1, &dstaddr, GACC_STORE);
+ 	if (rc)
+ 		return kvm_s390_inject_prog_cond(vcpu, rc);
+ 	rc = kvm_arch_fault_in_page(vcpu, dstaddr, 1);
+ 	if (rc != 0)
+ 		return rc;
+ 
+ 	kvm_s390_retry_instr(vcpu);
+ 
+ 	return 0;
+ }
+ 
+ static int handle_partial_execution(struct kvm_vcpu *vcpu)
+ {
+ 	if (vcpu->arch.sie_block->ipa == 0xb254)	/* MVPG */
+ 		return handle_mvpg_pei(vcpu);
+ 	if (vcpu->arch.sie_block->ipa >> 8 == 0xae)	/* SIGP */
+ 		return kvm_s390_handle_sigp_pei(vcpu);
+ 
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static int handle_operexc(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->stat.exit_operation_exception++;
+ 	trace_kvm_s390_handle_operexc(vcpu, vcpu->arch.sie_block->ipa,
+ 				      vcpu->arch.sie_block->ipb);
+ 
+ 	if (vcpu->arch.sie_block->ipa == 0xb256 &&
+ 	    test_kvm_facility(vcpu->kvm, 74))
+ 		return handle_sthyi(vcpu);
+ 
+ 	return kvm_s390_inject_program_int(vcpu, PGM_OPERATION);
+ }
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  
  int kvm_handle_sie_intercept(struct kvm_vcpu *vcpu)
  {
diff --cc arch/s390/kvm/kvm-s390.c
index 9b2d6973d202,1c10254119b3..000000000000
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@@ -263,122 -1159,368 +264,154 @@@ int kvm_arch_init_vm(struct kvm *kvm, u
  
  	sprintf(debug_name, "kvm-%u", current->pid);
  
 -	kvm->arch.dbf = debug_register(debug_name, 32, 1, 7 * sizeof(long));
 +	kvm->arch.dbf = debug_register(debug_name, 8, 2, 8 * sizeof(long));
  	if (!kvm->arch.dbf)
++<<<<<<< HEAD
 +		goto out_nodbf;
++=======
+ 		goto out_err;
+ 
+ 	kvm->arch.sie_page2 =
+ 	     (struct sie_page2 *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
+ 	if (!kvm->arch.sie_page2)
+ 		goto out_err;
+ 
+ 	/* Populate the facility mask initially. */
+ 	memcpy(kvm->arch.model.fac_mask, S390_lowcore.stfle_fac_list,
+ 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+ 	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
+ 		if (i < kvm_s390_fac_list_mask_size())
+ 			kvm->arch.model.fac_mask[i] &= kvm_s390_fac_list_mask[i];
+ 		else
+ 			kvm->arch.model.fac_mask[i] = 0UL;
+ 	}
+ 
+ 	/* Populate the facility list initially. */
+ 	kvm->arch.model.fac_list = kvm->arch.sie_page2->fac_list;
+ 	memcpy(kvm->arch.model.fac_list, kvm->arch.model.fac_mask,
+ 	       S390_ARCH_FAC_LIST_SIZE_BYTE);
+ 
+ 	set_kvm_facility(kvm->arch.model.fac_mask, 74);
+ 	set_kvm_facility(kvm->arch.model.fac_list, 74);
+ 
+ 	kvm->arch.model.cpuid = kvm_s390_get_initial_cpuid();
+ 	kvm->arch.model.ibc = sclp.ibc & 0x0fff;
+ 
+ 	kvm_s390_crypto_init(kvm);
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  
  	spin_lock_init(&kvm->arch.float_int.lock);
 -	for (i = 0; i < FIRQ_LIST_COUNT; i++)
 -		INIT_LIST_HEAD(&kvm->arch.float_int.lists[i]);
 -	init_waitqueue_head(&kvm->arch.ipte_wq);
 -	mutex_init(&kvm->arch.ipte_mutex);
 +	INIT_LIST_HEAD(&kvm->arch.float_int.list);
  
  	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 -	VM_EVENT(kvm, 3, "vm created with type %lu", type);
 -
 -	if (type & KVM_VM_S390_UCONTROL) {
 -		kvm->arch.gmap = NULL;
 -		kvm->arch.mem_limit = KVM_S390_NO_MEM_LIMIT;
 -	} else {
 -		if (sclp.hamax == U64_MAX)
 -			kvm->arch.mem_limit = TASK_MAX_SIZE;
 -		else
 -			kvm->arch.mem_limit = min_t(unsigned long, TASK_MAX_SIZE,
 -						    sclp.hamax + 1);
 -		kvm->arch.gmap = gmap_alloc(current->mm, kvm->arch.mem_limit - 1);
 -		if (!kvm->arch.gmap)
 -			goto out_err;
 -		kvm->arch.gmap->private = kvm;
 -		kvm->arch.gmap->pfault_enabled = 0;
 -	}
 -
 -	kvm->arch.css_support = 0;
 -	kvm->arch.use_irqchip = 0;
 -	kvm->arch.epoch = 0;
 -
 -	spin_lock_init(&kvm->arch.start_stop_lock);
 -	KVM_EVENT(3, "vm 0x%pK created by pid %u", kvm, current->pid);
 -
 -	return 0;
 -out_err:
 -	free_page((unsigned long)kvm->arch.sie_page2);
 -	debug_unregister(kvm->arch.dbf);
 -	sca_dispose(kvm);
 -	KVM_EVENT(3, "creation of vm failed: %d", rc);
 -	return rc;
 -}
 -
 -void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 -{
 -	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
 -	trace_kvm_s390_destroy_vcpu(vcpu->vcpu_id);
 -	kvm_s390_clear_local_irqs(vcpu);
 -	kvm_clear_async_pf_completion_queue(vcpu);
 -	if (!kvm_is_ucontrol(vcpu->kvm))
 -		sca_del_vcpu(vcpu);
 -
 -	if (kvm_is_ucontrol(vcpu->kvm))
 -		gmap_free(vcpu->arch.gmap);
 -
 -	if (vcpu->kvm->arch.use_cmma)
 -		kvm_s390_vcpu_unsetup_cmma(vcpu);
 -	free_page((unsigned long)(vcpu->arch.sie_block));
 -
 -	kvm_vcpu_uninit(vcpu);
 -	kmem_cache_free(kvm_vcpu_cache, vcpu);
 -}
 -
 -static void kvm_free_vcpus(struct kvm *kvm)
 -{
 -	unsigned int i;
 -	struct kvm_vcpu *vcpu;
 -
 -	kvm_for_each_vcpu(i, vcpu, kvm)
 -		kvm_arch_vcpu_destroy(vcpu);
 -
 -	mutex_lock(&kvm->lock);
 -	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
 -		kvm->vcpus[i] = NULL;
 -
 -	atomic_set(&kvm->online_vcpus, 0);
 -	mutex_unlock(&kvm->lock);
 -}
 -
 -void kvm_arch_destroy_vm(struct kvm *kvm)
 -{
 -	kvm_free_vcpus(kvm);
 -	sca_dispose(kvm);
 -	debug_unregister(kvm->arch.dbf);
 -	free_page((unsigned long)kvm->arch.sie_page2);
 -	if (!kvm_is_ucontrol(kvm))
 -		gmap_free(kvm->arch.gmap);
 -	kvm_s390_destroy_adapters(kvm);
 -	kvm_s390_clear_float_irqs(kvm);
 -	KVM_EVENT(3, "vm 0x%pK destroyed", kvm);
 -}
 -
 -/* Section: vcpu related */
 -static int __kvm_ucontrol_vcpu_init(struct kvm_vcpu *vcpu)
 -{
 -	vcpu->arch.gmap = gmap_alloc(current->mm, -1UL);
 -	if (!vcpu->arch.gmap)
 -		return -ENOMEM;
 -	vcpu->arch.gmap->private = vcpu->kvm;
 -
 -	return 0;
 -}
 -
 -static void sca_del_vcpu(struct kvm_vcpu *vcpu)
 -{
 -	read_lock(&vcpu->kvm->arch.sca_lock);
 -	if (vcpu->kvm->arch.use_esca) {
 -		struct esca_block *sca = vcpu->kvm->arch.sca;
 -
 -		clear_bit_inv(vcpu->vcpu_id, (unsigned long *) sca->mcn);
 -		sca->cpu[vcpu->vcpu_id].sda = 0;
 -	} else {
 -		struct bsca_block *sca = vcpu->kvm->arch.sca;
 -
 -		clear_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
 -		sca->cpu[vcpu->vcpu_id].sda = 0;
 -	}
 -	read_unlock(&vcpu->kvm->arch.sca_lock);
 -}
 -
 -static void sca_add_vcpu(struct kvm_vcpu *vcpu)
 -{
 -	read_lock(&vcpu->kvm->arch.sca_lock);
 -	if (vcpu->kvm->arch.use_esca) {
 -		struct esca_block *sca = vcpu->kvm->arch.sca;
 -
 -		sca->cpu[vcpu->vcpu_id].sda = (__u64) vcpu->arch.sie_block;
 -		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 -		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca & ~0x3fU;
 -		vcpu->arch.sie_block->ecb2 |= 0x04U;
 -		set_bit_inv(vcpu->vcpu_id, (unsigned long *) sca->mcn);
 -	} else {
 -		struct bsca_block *sca = vcpu->kvm->arch.sca;
 -
 -		sca->cpu[vcpu->vcpu_id].sda = (__u64) vcpu->arch.sie_block;
 -		vcpu->arch.sie_block->scaoh = (__u32)(((__u64)sca) >> 32);
 -		vcpu->arch.sie_block->scaol = (__u32)(__u64)sca;
 -		set_bit_inv(vcpu->vcpu_id, (unsigned long *) &sca->mcn);
 -	}
 -	read_unlock(&vcpu->kvm->arch.sca_lock);
 -}
 -
 -/* Basic SCA to Extended SCA data copy routines */
 -static inline void sca_copy_entry(struct esca_entry *d, struct bsca_entry *s)
 -{
 -	d->sda = s->sda;
 -	d->sigp_ctrl.c = s->sigp_ctrl.c;
 -	d->sigp_ctrl.scn = s->sigp_ctrl.scn;
 -}
 -
 -static void sca_copy_b_to_e(struct esca_block *d, struct bsca_block *s)
 -{
 -	int i;
 -
 -	d->ipte_control = s->ipte_control;
 -	d->mcn[0] = s->mcn;
 -	for (i = 0; i < KVM_S390_BSCA_CPU_SLOTS; i++)
 -		sca_copy_entry(&d->cpu[i], &s->cpu[i]);
 -}
 -
 -static int sca_switch_to_extended(struct kvm *kvm)
 -{
 -	struct bsca_block *old_sca = kvm->arch.sca;
 -	struct esca_block *new_sca;
 -	struct kvm_vcpu *vcpu;
 -	unsigned int vcpu_idx;
 -	u32 scaol, scaoh;
 -
 -	new_sca = alloc_pages_exact(sizeof(*new_sca), GFP_KERNEL|__GFP_ZERO);
 -	if (!new_sca)
 -		return -ENOMEM;
 -
 -	scaoh = (u32)((u64)(new_sca) >> 32);
 -	scaol = (u32)(u64)(new_sca) & ~0x3fU;
 -
 -	kvm_s390_vcpu_block_all(kvm);
 -	write_lock(&kvm->arch.sca_lock);
 -
 -	sca_copy_b_to_e(new_sca, old_sca);
 -
 -	kvm_for_each_vcpu(vcpu_idx, vcpu, kvm) {
 -		vcpu->arch.sie_block->scaoh = scaoh;
 -		vcpu->arch.sie_block->scaol = scaol;
 -		vcpu->arch.sie_block->ecb2 |= 0x04U;
 -	}
 -	kvm->arch.sca = new_sca;
 -	kvm->arch.use_esca = 1;
 -
 -	write_unlock(&kvm->arch.sca_lock);
 -	kvm_s390_vcpu_unblock_all(kvm);
 -
 -	free_page((unsigned long)old_sca);
 -
 -	VM_EVENT(kvm, 2, "Switched to ESCA (0x%pK -> 0x%pK)",
 -		 old_sca, kvm->arch.sca);
 -	return 0;
 -}
 -
 -static int sca_can_add_vcpu(struct kvm *kvm, unsigned int id)
 -{
 -	int rc;
 -
 -	if (id < KVM_S390_BSCA_CPU_SLOTS)
 -		return true;
 -	if (!sclp.has_esca)
 -		return false;
 -
 -	mutex_lock(&kvm->lock);
 -	rc = kvm->arch.use_esca ? 0 : sca_switch_to_extended(kvm);
 -	mutex_unlock(&kvm->lock);
 -
 -	return rc == 0 && id < KVM_S390_ESCA_CPU_SLOTS;
 -}
 +	VM_EVENT(kvm, 3, "%s", "vm created");
  
 -int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 -{
 -	vcpu->arch.pfault_token = KVM_S390_PFAULT_TOKEN_INVALID;
 -	kvm_clear_async_pf_completion_queue(vcpu);
 -	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
 -				    KVM_SYNC_GPRS |
 -				    KVM_SYNC_ACRS |
 -				    KVM_SYNC_CRS |
 -				    KVM_SYNC_ARCH0 |
 -				    KVM_SYNC_PFAULT;
 -	if (test_kvm_facility(vcpu->kvm, 64))
 -		vcpu->run->kvm_valid_regs |= KVM_SYNC_RICCB;
 -	/* fprs can be synchronized via vrs, even if the guest has no vx. With
 -	 * MACHINE_HAS_VX, (load|store)_fpu_regs() will work with vrs format.
 -	 */
 -	if (MACHINE_HAS_VX)
 -		vcpu->run->kvm_valid_regs |= KVM_SYNC_VRS;
 -	else
 -		vcpu->run->kvm_valid_regs |= KVM_SYNC_FPRS;
 +	if (type & KVM_VM_S390_UCONTROL) {
 +		kvm->arch.gmap = NULL;
 +	} else {
 +		kvm->arch.gmap = gmap_alloc(current->mm);
 +		if (!kvm->arch.gmap)
 +			goto out_nogmap;
 +		kvm->arch.gmap->private = kvm;
 +	}
  
 -	if (kvm_is_ucontrol(vcpu->kvm))
 -		return __kvm_ucontrol_vcpu_init(vcpu);
 +	kvm->arch.css_support = 0;
  
  	return 0;
 +out_nogmap:
 +	debug_unregister(kvm->arch.dbf);
 +out_nodbf:
 +	free_page((unsigned long)(kvm->arch.sca));
 +out_err:
 +	return rc;
  }
  
 -/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
 -static void __start_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 +bool kvm_arch_has_vcpu_debugfs(void)
  {
 -	WARN_ON_ONCE(vcpu->arch.cputm_start != 0);
 -	raw_write_seqcount_begin(&vcpu->arch.cputm_seqcount);
 -	vcpu->arch.cputm_start = get_tod_clock_fast();
 -	raw_write_seqcount_end(&vcpu->arch.cputm_seqcount);
 +	return false;
  }
  
 -/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
 -static void __stop_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 +int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
  {
 -	WARN_ON_ONCE(vcpu->arch.cputm_start == 0);
 -	raw_write_seqcount_begin(&vcpu->arch.cputm_seqcount);
 -	vcpu->arch.sie_block->cputm -= get_tod_clock_fast() - vcpu->arch.cputm_start;
 -	vcpu->arch.cputm_start = 0;
 -	raw_write_seqcount_end(&vcpu->arch.cputm_seqcount);
 +	return 0;
  }
  
 -/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
 -static void __enable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 +void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
  {
 -	WARN_ON_ONCE(vcpu->arch.cputm_enabled);
 -	vcpu->arch.cputm_enabled = true;
 -	__start_cpu_timer_accounting(vcpu);
 +	VCPU_EVENT(vcpu, 3, "%s", "free cpu");
 +	trace_kvm_s390_destroy_vcpu(vcpu->vcpu_id);
 +	if (!kvm_is_ucontrol(vcpu->kvm)) {
 +		clear_bit(63 - vcpu->vcpu_id,
 +			  (unsigned long *) &vcpu->kvm->arch.sca->mcn);
 +		if (vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda ==
 +		    (__u64) vcpu->arch.sie_block)
 +			vcpu->kvm->arch.sca->cpu[vcpu->vcpu_id].sda = 0;
 +	}
 +	smp_mb();
 +
 +	if (kvm_is_ucontrol(vcpu->kvm))
 +		gmap_free(vcpu->arch.gmap);
 +
 +	free_page((unsigned long)(vcpu->arch.sie_block));
 +	kvm_vcpu_uninit(vcpu);
 +	kmem_cache_free(kvm_vcpu_cache, vcpu);
  }
  
 -/* needs disabled preemption to protect from TOD sync and vcpu_load/put */
 -static void __disable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 +static void kvm_free_vcpus(struct kvm *kvm)
  {
 -	WARN_ON_ONCE(!vcpu->arch.cputm_enabled);
 -	__stop_cpu_timer_accounting(vcpu);
 -	vcpu->arch.cputm_enabled = false;
 +	unsigned int i;
 +	struct kvm_vcpu *vcpu;
 +
 +	kvm_for_each_vcpu(i, vcpu, kvm)
 +		kvm_arch_vcpu_destroy(vcpu);
 +
 +	mutex_lock(&kvm->lock);
 +	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
 +		kvm->vcpus[i] = NULL;
 +
 +	atomic_set(&kvm->online_vcpus, 0);
 +	mutex_unlock(&kvm->lock);
  }
  
 -static void enable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 +void kvm_arch_sync_events(struct kvm *kvm)
  {
 -	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
 -	__enable_cpu_timer_accounting(vcpu);
 -	preempt_enable();
  }
  
 -static void disable_cpu_timer_accounting(struct kvm_vcpu *vcpu)
 +void kvm_arch_destroy_vm(struct kvm *kvm)
  {
 -	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
 -	__disable_cpu_timer_accounting(vcpu);
 -	preempt_enable();
 +	kvm_free_vcpus(kvm);
 +	free_page((unsigned long)(kvm->arch.sca));
 +	debug_unregister(kvm->arch.dbf);
 +	if (!kvm_is_ucontrol(kvm))
 +		gmap_free(kvm->arch.gmap);
  }
  
 -/* set the cpu timer - may only be called from the VCPU thread itself */
 -void kvm_s390_set_cpu_timer(struct kvm_vcpu *vcpu, __u64 cputm)
 +/* Section: vcpu related */
 +int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
  {
 -	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
 -	raw_write_seqcount_begin(&vcpu->arch.cputm_seqcount);
 -	if (vcpu->arch.cputm_enabled)
 -		vcpu->arch.cputm_start = get_tod_clock_fast();
 -	vcpu->arch.sie_block->cputm = cputm;
 -	raw_write_seqcount_end(&vcpu->arch.cputm_seqcount);
 -	preempt_enable();
 +	if (kvm_is_ucontrol(vcpu->kvm)) {
 +		vcpu->arch.gmap = gmap_alloc(current->mm);
 +		if (!vcpu->arch.gmap)
 +			return -ENOMEM;
 +		vcpu->arch.gmap->private = vcpu->kvm;
 +		return 0;
 +	}
 +
 +	vcpu->arch.gmap = vcpu->kvm->arch.gmap;
 +	vcpu->run->kvm_valid_regs = KVM_SYNC_PREFIX |
 +				    KVM_SYNC_GPRS |
 +				    KVM_SYNC_ACRS |
 +				    KVM_SYNC_CRS;
 +	return 0;
  }
  
 -/* update and get the cpu timer - can also be called from other VCPU threads */
 -__u64 kvm_s390_get_cpu_timer(struct kvm_vcpu *vcpu)
 +void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
  {
 -	unsigned int seq;
 -	__u64 value;
 -
 -	if (unlikely(!vcpu->arch.cputm_enabled))
 -		return vcpu->arch.sie_block->cputm;
 +	/* Nothing todo */
 +}
  
 -	preempt_disable(); /* protect from TOD sync and vcpu_load/put */
 -	do {
 -		seq = raw_read_seqcount(&vcpu->arch.cputm_seqcount);
 -		/*
 -		 * If the writer would ever execute a read in the critical
 -		 * section, e.g. in irq context, we have a deadlock.
 -		 */
 -		WARN_ON_ONCE((seq & 1) && smp_processor_id() == vcpu->cpu);
 -		value = vcpu->arch.sie_block->cputm;
 -		/* if cputm_start is 0, accounting is being started/stopped */
 -		if (likely(vcpu->arch.cputm_start))
 -			value -= get_tod_clock_fast() - vcpu->arch.cputm_start;
 -	} while (read_seqcount_retry(&vcpu->arch.cputm_seqcount, seq & ~1));
 -	preempt_enable();
 -	return value;
 +void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 +{
  }
  
  void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
@@@ -429,23 -1649,54 +462,45 @@@ void kvm_arch_vcpu_postcreate(struct kv
  
  int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
  {
 -	int rc = 0;
 -
  	atomic_set(&vcpu->arch.sie_block->cpuflags, CPUSTAT_ZARCH |
  						    CPUSTAT_SM |
 -						    CPUSTAT_STOPPED);
 -
 -	if (test_kvm_facility(vcpu->kvm, 78))
 -		atomic_or(CPUSTAT_GED2, &vcpu->arch.sie_block->cpuflags);
 -	else if (test_kvm_facility(vcpu->kvm, 8))
 -		atomic_or(CPUSTAT_GED, &vcpu->arch.sie_block->cpuflags);
 -
 -	kvm_s390_vcpu_setup_model(vcpu);
 -
 -	vcpu->arch.sie_block->ecb = 0x02;
 -	if (test_kvm_facility(vcpu->kvm, 9))
 -		vcpu->arch.sie_block->ecb |= 0x04;
 -	if (test_kvm_facility(vcpu->kvm, 50) && test_kvm_facility(vcpu->kvm, 73))
 -		vcpu->arch.sie_block->ecb |= 0x10;
 -
 -	if (test_kvm_facility(vcpu->kvm, 8))
 -		vcpu->arch.sie_block->ecb2 |= 0x08;
 +						    CPUSTAT_STOPPED |
 +						    CPUSTAT_GED);
 +	vcpu->arch.sie_block->ecb   = 6;
 +	vcpu->arch.sie_block->ecb2  = 8;
  	vcpu->arch.sie_block->eca   = 0xC1002000U;
 -	if (sclp.has_siif)
 +	if (sclp_has_siif())
  		vcpu->arch.sie_block->eca |= 1;
++<<<<<<< HEAD
 +	vcpu->arch.sie_block->fac   = (int) (long) vfacilities;
 +	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 +	tasklet_init(&vcpu->arch.tasklet, kvm_s390_tasklet,
 +		     (unsigned long) vcpu);
++=======
+ 	if (sclp.has_sigpif)
+ 		vcpu->arch.sie_block->eca |= 0x10000000U;
+ 	if (test_kvm_facility(vcpu->kvm, 64))
+ 		vcpu->arch.sie_block->ecb3 |= 0x01;
+ 	if (test_kvm_facility(vcpu->kvm, 129)) {
+ 		vcpu->arch.sie_block->eca |= 0x00020000;
+ 		vcpu->arch.sie_block->ecd |= 0x20000000;
+ 	}
+ 	vcpu->arch.sie_block->riccbd = (unsigned long) &vcpu->run->s.regs.riccb;
+ 	vcpu->arch.sie_block->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;
+ 	if (test_kvm_facility(vcpu->kvm, 74))
+ 		vcpu->arch.sie_block->ictl |= ICTL_OPEREXC;
+ 
+ 	if (vcpu->kvm->arch.use_cmma) {
+ 		rc = kvm_s390_vcpu_setup_cmma(vcpu);
+ 		if (rc)
+ 			return rc;
+ 	}
+ 	hrtimer_init(&vcpu->arch.ckc_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  	vcpu->arch.ckc_timer.function = kvm_s390_idle_wakeup;
 -
 -	kvm_s390_vcpu_crypto_setup(vcpu);
 -
 -	return rc;
 +	get_cpu_id(&vcpu->arch.cpu_id);
 +	vcpu->arch.cpu_id.version = 0xff;
 +	return 0;
  }
  
  struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm,
diff --cc arch/s390/kvm/trace.h
index 0c991c6748ab,a429ef9b0d30..000000000000
--- a/arch/s390/kvm/trace.h
+++ b/arch/s390/kvm/trace.h
@@@ -336,6 -412,47 +336,50 @@@ TRACE_EVENT(kvm_s390_handle_stsi
  			   __entry->addr)
  	);
  
++<<<<<<< HEAD
++=======
+ TRACE_EVENT(kvm_s390_handle_operexc,
+ 	    TP_PROTO(VCPU_PROTO_COMMON, __u16 ipa, __u32 ipb),
+ 	    TP_ARGS(VCPU_ARGS_COMMON, ipa, ipb),
+ 
+ 	    TP_STRUCT__entry(
+ 		    VCPU_FIELD_COMMON
+ 		    __field(__u64, instruction)
+ 		    ),
+ 
+ 	    TP_fast_assign(
+ 		    VCPU_ASSIGN_COMMON
+ 		    __entry->instruction = ((__u64)ipa << 48) |
+ 		    ((__u64)ipb << 16);
+ 		    ),
+ 
+ 	    VCPU_TP_PRINTK("operation exception on instruction %016llx (%s)",
+ 			   __entry->instruction,
+ 			   __print_symbolic(icpt_insn_decoder(__entry->instruction),
+ 					    icpt_insn_codes))
+ 	);
+ 
+ TRACE_EVENT(kvm_s390_handle_sthyi,
+ 	    TP_PROTO(VCPU_PROTO_COMMON, u64 code, u64 addr),
+ 	    TP_ARGS(VCPU_ARGS_COMMON, code, addr),
+ 
+ 	    TP_STRUCT__entry(
+ 		    VCPU_FIELD_COMMON
+ 		    __field(u64, code)
+ 		    __field(u64, addr)
+ 		    ),
+ 
+ 	    TP_fast_assign(
+ 		    VCPU_ASSIGN_COMMON
+ 		    __entry->code = code;
+ 		    __entry->addr = addr;
+ 		    ),
+ 
+ 	    VCPU_TP_PRINTK("STHYI fc: %llu addr: %016llx",
+ 			   __entry->code, __entry->addr)
+ 	);
+ 
++>>>>>>> 95ca2cb57985 (KVM: s390: Add sthyi emulation)
  #endif /* _TRACE_KVM_H */
  
  /* This part must be outside protection */
* Unmerged path arch/s390/include/uapi/asm/sie.h
* Unmerged path arch/s390/include/asm/diag.h
* Unmerged path arch/s390/include/asm/kvm_host.h
* Unmerged path arch/s390/include/uapi/asm/sie.h
* Unmerged path arch/s390/kvm/Makefile
* Unmerged path arch/s390/kvm/intercept.c
* Unmerged path arch/s390/kvm/kvm-s390.c
diff --git a/arch/s390/kvm/kvm-s390.h b/arch/s390/kvm/kvm-s390.h
index dc99f1ca4267..106845b9c445 100644
--- a/arch/s390/kvm/kvm-s390.h
+++ b/arch/s390/kvm/kvm-s390.h
@@ -148,6 +148,9 @@ int kvm_s390_handle_eb(struct kvm_vcpu *vcpu);
 /* implemented in sigp.c */
 int kvm_s390_handle_sigp(struct kvm_vcpu *vcpu);
 
+/* implemented in sthyi.c */
+int handle_sthyi(struct kvm_vcpu *vcpu);
+
 /* implemented in kvm-s390.c */
 int kvm_s390_vcpu_store_status(struct kvm_vcpu *vcpu,
 				 unsigned long addr);
diff --git a/arch/s390/kvm/sthyi.c b/arch/s390/kvm/sthyi.c
new file mode 100644
index 000000000000..894d5626f18d
--- /dev/null
+++ b/arch/s390/kvm/sthyi.c
@@ -0,0 +1,460 @@
+/*
+ * store hypervisor information instruction emulation functions.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License (version 2 only)
+ * as published by the Free Software Foundation.
+ *
+ * Copyright IBM Corp. 2016
+ * Author(s): Janosch Frank <frankja@linux.vnet.ibm.com>
+ */
+#include <linux/kvm_host.h>
+#include <linux/errno.h>
+#include <linux/pagemap.h>
+#include <linux/vmalloc.h>
+
+#include <asm/kvm_host.h>
+#include <asm/asm-offsets.h>
+#include <asm/sclp.h>
+#include <asm/diag.h>
+#include <asm/sysinfo.h>
+#include <asm/ebcdic.h>
+
+#include "kvm-s390.h"
+#include "gaccess.h"
+#include "trace.h"
+
+#define DED_WEIGHT 0xffff
+/*
+ * CP and IFL as EBCDIC strings, SP/0x40 determines the end of string
+ * as they are justified with spaces.
+ */
+#define CP  0xc3d7404040404040UL
+#define IFL 0xc9c6d34040404040UL
+
+enum hdr_flags {
+	HDR_NOT_LPAR   = 0x10,
+	HDR_STACK_INCM = 0x20,
+	HDR_STSI_UNAV  = 0x40,
+	HDR_PERF_UNAV  = 0x80,
+};
+
+enum mac_validity {
+	MAC_NAME_VLD = 0x20,
+	MAC_ID_VLD   = 0x40,
+	MAC_CNT_VLD  = 0x80,
+};
+
+enum par_flag {
+	PAR_MT_EN = 0x80,
+};
+
+enum par_validity {
+	PAR_GRP_VLD  = 0x08,
+	PAR_ID_VLD   = 0x10,
+	PAR_ABS_VLD  = 0x20,
+	PAR_WGHT_VLD = 0x40,
+	PAR_PCNT_VLD  = 0x80,
+};
+
+struct hdr_sctn {
+	u8 infhflg1;
+	u8 infhflg2; /* reserved */
+	u8 infhval1; /* reserved */
+	u8 infhval2; /* reserved */
+	u8 reserved[3];
+	u8 infhygct;
+	u16 infhtotl;
+	u16 infhdln;
+	u16 infmoff;
+	u16 infmlen;
+	u16 infpoff;
+	u16 infplen;
+	u16 infhoff1;
+	u16 infhlen1;
+	u16 infgoff1;
+	u16 infglen1;
+	u16 infhoff2;
+	u16 infhlen2;
+	u16 infgoff2;
+	u16 infglen2;
+	u16 infhoff3;
+	u16 infhlen3;
+	u16 infgoff3;
+	u16 infglen3;
+	u8 reserved2[4];
+} __packed;
+
+struct mac_sctn {
+	u8 infmflg1; /* reserved */
+	u8 infmflg2; /* reserved */
+	u8 infmval1;
+	u8 infmval2; /* reserved */
+	u16 infmscps;
+	u16 infmdcps;
+	u16 infmsifl;
+	u16 infmdifl;
+	char infmname[8];
+	char infmtype[4];
+	char infmmanu[16];
+	char infmseq[16];
+	char infmpman[4];
+	u8 reserved[4];
+} __packed;
+
+struct par_sctn {
+	u8 infpflg1;
+	u8 infpflg2; /* reserved */
+	u8 infpval1;
+	u8 infpval2; /* reserved */
+	u16 infppnum;
+	u16 infpscps;
+	u16 infpdcps;
+	u16 infpsifl;
+	u16 infpdifl;
+	u16 reserved;
+	char infppnam[8];
+	u32 infpwbcp;
+	u32 infpabcp;
+	u32 infpwbif;
+	u32 infpabif;
+	char infplgnm[8];
+	u32 infplgcp;
+	u32 infplgif;
+} __packed;
+
+struct sthyi_sctns {
+	struct hdr_sctn hdr;
+	struct mac_sctn mac;
+	struct par_sctn par;
+} __packed;
+
+struct cpu_inf {
+	u64 lpar_cap;
+	u64 lpar_grp_cap;
+	u64 lpar_weight;
+	u64 all_weight;
+	int cpu_num_ded;
+	int cpu_num_shd;
+};
+
+struct lpar_cpu_inf {
+	struct cpu_inf cp;
+	struct cpu_inf ifl;
+};
+
+static inline u64 cpu_id(u8 ctidx, void *diag224_buf)
+{
+	return *((u64 *)(diag224_buf + (ctidx + 1) * DIAG204_CPU_NAME_LEN));
+}
+
+/*
+ * Scales the cpu capping from the lpar range to the one expected in
+ * sthyi data.
+ *
+ * diag204 reports a cap in hundredths of processor units.
+ * z/VM's range for one core is 0 - 0x10000.
+ */
+static u32 scale_cap(u32 in)
+{
+	return (0x10000 * in) / 100;
+}
+
+static void fill_hdr(struct sthyi_sctns *sctns)
+{
+	sctns->hdr.infhdln = sizeof(sctns->hdr);
+	sctns->hdr.infmoff = sizeof(sctns->hdr);
+	sctns->hdr.infmlen = sizeof(sctns->mac);
+	sctns->hdr.infplen = sizeof(sctns->par);
+	sctns->hdr.infpoff = sctns->hdr.infhdln + sctns->hdr.infmlen;
+	sctns->hdr.infhtotl = sctns->hdr.infpoff + sctns->hdr.infplen;
+}
+
+static void fill_stsi_mac(struct sthyi_sctns *sctns,
+			  struct sysinfo_1_1_1 *sysinfo)
+{
+	if (stsi(sysinfo, 1, 1, 1))
+		return;
+
+	sclp_ocf_cpc_name_copy(sctns->mac.infmname);
+
+	memcpy(sctns->mac.infmtype, sysinfo->type, sizeof(sctns->mac.infmtype));
+	memcpy(sctns->mac.infmmanu, sysinfo->manufacturer, sizeof(sctns->mac.infmmanu));
+	memcpy(sctns->mac.infmpman, sysinfo->plant, sizeof(sctns->mac.infmpman));
+	memcpy(sctns->mac.infmseq, sysinfo->sequence, sizeof(sctns->mac.infmseq));
+
+	sctns->mac.infmval1 |= MAC_ID_VLD | MAC_NAME_VLD;
+}
+
+static void fill_stsi_par(struct sthyi_sctns *sctns,
+			  struct sysinfo_2_2_2 *sysinfo)
+{
+	if (stsi(sysinfo, 2, 2, 2))
+		return;
+
+	sctns->par.infppnum = sysinfo->lpar_number;
+	memcpy(sctns->par.infppnam, sysinfo->name, sizeof(sctns->par.infppnam));
+
+	sctns->par.infpval1 |= PAR_ID_VLD;
+}
+
+static void fill_stsi(struct sthyi_sctns *sctns)
+{
+	void *sysinfo;
+
+	/* Errors are handled through the validity bits in the response. */
+	sysinfo = (void *)__get_free_page(GFP_KERNEL);
+	if (!sysinfo)
+		return;
+
+	fill_stsi_mac(sctns, sysinfo);
+	fill_stsi_par(sctns, sysinfo);
+
+	free_pages((unsigned long)sysinfo, 0);
+}
+
+static void fill_diag_mac(struct sthyi_sctns *sctns,
+			  struct diag204_x_phys_block *block,
+			  void *diag224_buf)
+{
+	int i;
+
+	for (i = 0; i < block->hdr.cpus; i++) {
+		switch (cpu_id(block->cpus[i].ctidx, diag224_buf)) {
+		case CP:
+			if (block->cpus[i].weight == DED_WEIGHT)
+				sctns->mac.infmdcps++;
+			else
+				sctns->mac.infmscps++;
+			break;
+		case IFL:
+			if (block->cpus[i].weight == DED_WEIGHT)
+				sctns->mac.infmdifl++;
+			else
+				sctns->mac.infmsifl++;
+			break;
+		}
+	}
+	sctns->mac.infmval1 |= MAC_CNT_VLD;
+}
+
+/* Returns a pointer to the the next partition block. */
+static struct diag204_x_part_block *lpar_cpu_inf(struct lpar_cpu_inf *part_inf,
+						 bool this_lpar,
+						 void *diag224_buf,
+						 struct diag204_x_part_block *block)
+{
+	int i, capped = 0, weight_cp = 0, weight_ifl = 0;
+	struct cpu_inf *cpu_inf;
+
+	for (i = 0; i < block->hdr.rcpus; i++) {
+		if (!(block->cpus[i].cflag & DIAG204_CPU_ONLINE))
+			continue;
+
+		switch (cpu_id(block->cpus[i].ctidx, diag224_buf)) {
+		case CP:
+			cpu_inf = &part_inf->cp;
+			if (block->cpus[i].cur_weight < DED_WEIGHT)
+				weight_cp |= block->cpus[i].cur_weight;
+			break;
+		case IFL:
+			cpu_inf = &part_inf->ifl;
+			if (block->cpus[i].cur_weight < DED_WEIGHT)
+				weight_ifl |= block->cpus[i].cur_weight;
+			break;
+		default:
+			continue;
+		}
+
+		if (!this_lpar)
+			continue;
+
+		capped |= block->cpus[i].cflag & DIAG204_CPU_CAPPED;
+		cpu_inf->lpar_cap |= block->cpus[i].cpu_type_cap;
+		cpu_inf->lpar_grp_cap |= block->cpus[i].group_cpu_type_cap;
+
+		if (block->cpus[i].weight == DED_WEIGHT)
+			cpu_inf->cpu_num_ded += 1;
+		else
+			cpu_inf->cpu_num_shd += 1;
+	}
+
+	if (this_lpar && capped) {
+		part_inf->cp.lpar_weight = weight_cp;
+		part_inf->ifl.lpar_weight = weight_ifl;
+	}
+	part_inf->cp.all_weight += weight_cp;
+	part_inf->ifl.all_weight += weight_ifl;
+	return (struct diag204_x_part_block *)&block->cpus[i];
+}
+
+static void fill_diag(struct sthyi_sctns *sctns)
+{
+	int i, r, pages;
+	bool this_lpar;
+	void *diag204_buf;
+	void *diag224_buf = NULL;
+	struct diag204_x_info_blk_hdr *ti_hdr;
+	struct diag204_x_part_block *part_block;
+	struct diag204_x_phys_block *phys_block;
+	struct lpar_cpu_inf lpar_inf = {};
+
+	/* Errors are handled through the validity bits in the response. */
+	pages = diag204((unsigned long)DIAG204_SUBC_RSI |
+			(unsigned long)DIAG204_INFO_EXT, 0, NULL);
+	if (pages <= 0)
+		return;
+
+	diag204_buf = vmalloc(PAGE_SIZE * pages);
+	if (!diag204_buf)
+		return;
+
+	r = diag204((unsigned long)DIAG204_SUBC_STIB7 |
+		    (unsigned long)DIAG204_INFO_EXT, pages, diag204_buf);
+	if (r < 0)
+		goto out;
+
+	diag224_buf = kmalloc(PAGE_SIZE, GFP_KERNEL | GFP_DMA);
+	if (!diag224_buf || diag224(diag224_buf))
+		goto out;
+
+	ti_hdr = diag204_buf;
+	part_block = diag204_buf + sizeof(*ti_hdr);
+
+	for (i = 0; i < ti_hdr->npar; i++) {
+		/*
+		 * For the calling lpar we also need to get the cpu
+		 * caps and weights. The time information block header
+		 * specifies the offset to the partition block of the
+		 * caller lpar, so we know when we process its data.
+		 */
+		this_lpar = (void *)part_block - diag204_buf == ti_hdr->this_part;
+		part_block = lpar_cpu_inf(&lpar_inf, this_lpar, diag224_buf,
+					  part_block);
+	}
+
+	phys_block = (struct diag204_x_phys_block *)part_block;
+	part_block = diag204_buf + ti_hdr->this_part;
+	if (part_block->hdr.mtid)
+		sctns->par.infpflg1 = PAR_MT_EN;
+
+	sctns->par.infpval1 |= PAR_GRP_VLD;
+	sctns->par.infplgcp = scale_cap(lpar_inf.cp.lpar_grp_cap);
+	sctns->par.infplgif = scale_cap(lpar_inf.ifl.lpar_grp_cap);
+	memcpy(sctns->par.infplgnm, part_block->hdr.hardware_group_name,
+	       sizeof(sctns->par.infplgnm));
+
+	sctns->par.infpscps = lpar_inf.cp.cpu_num_shd;
+	sctns->par.infpdcps = lpar_inf.cp.cpu_num_ded;
+	sctns->par.infpsifl = lpar_inf.ifl.cpu_num_shd;
+	sctns->par.infpdifl = lpar_inf.ifl.cpu_num_ded;
+	sctns->par.infpval1 |= PAR_PCNT_VLD;
+
+	sctns->par.infpabcp = scale_cap(lpar_inf.cp.lpar_cap);
+	sctns->par.infpabif = scale_cap(lpar_inf.ifl.lpar_cap);
+	sctns->par.infpval1 |= PAR_ABS_VLD;
+
+	/*
+	 * Everything below needs global performance data to be
+	 * meaningful.
+	 */
+	if (!(ti_hdr->flags & DIAG204_LPAR_PHYS_FLG)) {
+		sctns->hdr.infhflg1 |= HDR_PERF_UNAV;
+		goto out;
+	}
+
+	fill_diag_mac(sctns, phys_block, diag224_buf);
+
+	if (lpar_inf.cp.lpar_weight) {
+		sctns->par.infpwbcp = sctns->mac.infmscps * 0x10000 *
+			lpar_inf.cp.lpar_weight / lpar_inf.cp.all_weight;
+	}
+
+	if (lpar_inf.ifl.lpar_weight) {
+		sctns->par.infpwbif = sctns->mac.infmsifl * 0x10000 *
+			lpar_inf.ifl.lpar_weight / lpar_inf.ifl.all_weight;
+	}
+	sctns->par.infpval1 |= PAR_WGHT_VLD;
+
+out:
+	kfree(diag224_buf);
+	vfree(diag204_buf);
+}
+
+static int sthyi(u64 vaddr)
+{
+	register u64 code asm("0") = 0;
+	register u64 addr asm("2") = vaddr;
+	int cc;
+
+	asm volatile(
+		".insn   rre,0xB2560000,%[code],%[addr]\n"
+		"ipm     %[cc]\n"
+		"srl     %[cc],28\n"
+		: [cc] "=d" (cc)
+		: [code] "d" (code), [addr] "a" (addr)
+		: "memory", "cc");
+	return cc;
+}
+
+int handle_sthyi(struct kvm_vcpu *vcpu)
+{
+	int reg1, reg2, r = 0;
+	u64 code, addr, cc = 0;
+	struct sthyi_sctns *sctns = NULL;
+
+	kvm_s390_get_regs_rre(vcpu, &reg1, &reg2);
+	code = vcpu->run->s.regs.gprs[reg1];
+	addr = vcpu->run->s.regs.gprs[reg2];
+
+	vcpu->stat.instruction_sthyi++;
+	VCPU_EVENT(vcpu, 3, "STHYI: fc: %llu addr: 0x%016llx", code, addr);
+	trace_kvm_s390_handle_sthyi(vcpu, code, addr);
+
+	if (reg1 == reg2 || reg1 & 1 || reg2 & 1 || addr & ~PAGE_MASK)
+		return kvm_s390_inject_program_int(vcpu, PGM_SPECIFICATION);
+
+	if (code & 0xffff) {
+		cc = 3;
+		goto out;
+	}
+
+	/*
+	 * If the page has not yet been faulted in, we want to do that
+	 * now and not after all the expensive calculations.
+	 */
+	r = write_guest(vcpu, addr, reg2, &cc, 1);
+	if (r)
+		return kvm_s390_inject_prog_cond(vcpu, r);
+
+	sctns = (void *)get_zeroed_page(GFP_KERNEL);
+	if (!sctns)
+		return -ENOMEM;
+
+	/*
+	 * If we are a guest, we don't want to emulate an emulated
+	 * instruction. We ask the hypervisor to provide the data.
+	 */
+	if (test_facility(74)) {
+		cc = sthyi((u64)sctns);
+		goto out;
+	}
+
+	fill_hdr(sctns);
+	fill_stsi(sctns);
+	fill_diag(sctns);
+
+out:
+	if (!cc) {
+		r = write_guest(vcpu, addr, reg2, sctns, PAGE_SIZE);
+		if (r) {
+			free_page((unsigned long)sctns);
+			return kvm_s390_inject_prog_cond(vcpu, r);
+		}
+	}
+
+	free_page((unsigned long)sctns);
+	vcpu->run->s.regs.gprs[reg2 + 1] = cc ? 4 : 0;
+	kvm_s390_set_psw_cc(vcpu, cc);
+	return r;
+}
* Unmerged path arch/s390/kvm/trace.h
