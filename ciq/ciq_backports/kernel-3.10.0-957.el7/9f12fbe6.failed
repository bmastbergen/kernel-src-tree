net: filter: move load_pointer() into filter.h

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [kernel] filter: move load_pointer() into filter.h (Jiri Olsa) [1311586]
Rebuild_FUZZ: 94.25%
commit-author Zi Shen Lim <zlim.lnx@gmail.com>
commit 9f12fbe603f7ae346b2b46008e325f0c9a68e55d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9f12fbe6.failed

load_pointer() is already a static inline function.
Let's move it into filter.h so BPF JIT implementations can reuse this
function.

Since we're exporting this function, let's also rename it to
bpf_load_pointer() for clarity.

	Signed-off-by: Zi Shen Lim <zlim.lnx@gmail.com>
	Reviewed-by: Daniel Borkmann <dborkman@redhat.com>
	Acked-by: Alexei Starovoitov <ast@plumgrid.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 9f12fbe603f7ae346b2b46008e325f0c9a68e55d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/filter.h
#	net/core/filter.c
diff --cc include/linux/filter.h
index d322ed880333,b885dcb7eaca..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -6,87 -6,419 +6,104 @@@
  
  #include <linux/atomic.h>
  #include <linux/compat.h>
++<<<<<<< HEAD
++=======
+ #include <linux/skbuff.h>
+ #include <linux/workqueue.h>
++>>>>>>> 9f12fbe603f7 (net: filter: move load_pointer() into filter.h)
  #include <uapi/linux/filter.h>
 -
 -/* Internally used and optimized filter representation with extended
 - * instruction set based on top of classic BPF.
 - */
 -
 -/* instruction classes */
 -#define BPF_ALU64	0x07	/* alu mode in double word width */
 -
 -/* ld/ldx fields */
 -#define BPF_DW		0x18	/* double word */
 -#define BPF_XADD	0xc0	/* exclusive add */
 -
 -/* alu/jmp fields */
 -#define BPF_MOV		0xb0	/* mov reg to reg */
 -#define BPF_ARSH	0xc0	/* sign extending arithmetic shift right */
 -
 -/* change endianness of a register */
 -#define BPF_END		0xd0	/* flags for endianness conversion: */
 -#define BPF_TO_LE	0x00	/* convert to little-endian */
 -#define BPF_TO_BE	0x08	/* convert to big-endian */
 -#define BPF_FROM_LE	BPF_TO_LE
 -#define BPF_FROM_BE	BPF_TO_BE
 -
 -#define BPF_JNE		0x50	/* jump != */
 -#define BPF_JSGT	0x60	/* SGT is signed '>', GT in x86 */
 -#define BPF_JSGE	0x70	/* SGE is signed '>=', GE in x86 */
 -#define BPF_CALL	0x80	/* function call */
 -#define BPF_EXIT	0x90	/* function return */
 -
 -/* Register numbers */
 -enum {
 -	BPF_REG_0 = 0,
 -	BPF_REG_1,
 -	BPF_REG_2,
 -	BPF_REG_3,
 -	BPF_REG_4,
 -	BPF_REG_5,
 -	BPF_REG_6,
 -	BPF_REG_7,
 -	BPF_REG_8,
 -	BPF_REG_9,
 -	BPF_REG_10,
 -	__MAX_BPF_REG,
 -};
 -
 -/* BPF has 10 general purpose 64-bit registers and stack frame. */
 -#define MAX_BPF_REG	__MAX_BPF_REG
 -
 -/* ArgX, context and stack frame pointer register positions. Note,
 - * Arg1, Arg2, Arg3, etc are used as argument mappings of function
 - * calls in BPF_CALL instruction.
 - */
 -#define BPF_REG_ARG1	BPF_REG_1
 -#define BPF_REG_ARG2	BPF_REG_2
 -#define BPF_REG_ARG3	BPF_REG_3
 -#define BPF_REG_ARG4	BPF_REG_4
 -#define BPF_REG_ARG5	BPF_REG_5
 -#define BPF_REG_CTX	BPF_REG_6
 -#define BPF_REG_FP	BPF_REG_10
 -
 -/* Additional register mappings for converted user programs. */
 -#define BPF_REG_A	BPF_REG_0
 -#define BPF_REG_X	BPF_REG_7
 -#define BPF_REG_TMP	BPF_REG_8
 -
 -/* BPF program can access up to 512 bytes of stack space. */
 -#define MAX_BPF_STACK	512
 -
 -/* Helper macros for filter block array initializers. */
 -
 -/* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
 -
 -#define BPF_ALU64_REG(OP, DST, SRC)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -#define BPF_ALU32_REG(OP, DST, SRC)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
 -
 -#define BPF_ALU64_IMM(OP, DST, IMM)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_ALU32_IMM(OP, DST, IMM)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
 -
 -#define BPF_ENDIAN(TYPE, DST, LEN)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = LEN })
 -
 -/* Short form of mov, dst_reg = src_reg */
 -
 -#define BPF_MOV64_REG(DST, SRC)					\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -#define BPF_MOV32_REG(DST, SRC)					\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -/* Short form of mov, dst_reg = imm32 */
 -
 -#define BPF_MOV64_IMM(DST, IMM)					\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_MOV32_IMM(DST, IMM)					\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
 -
 -#define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -#define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)			\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
 -
 -#define BPF_LD_ABS(SIZE, IMM)					\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
 -
 -#define BPF_LD_IND(SIZE, SRC, IMM)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
 -		.dst_reg = 0,					\
 -		.src_reg = SRC,					\
 -		.off   = 0,					\
 -		.imm   = IMM })
 -
 -/* Memory load, dst_reg = *(uint *) (src_reg + off16) */
 -
 -#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Memory store, *(uint *) (dst_reg + off16) = src_reg */
 -
 -#define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Memory store, *(uint *) (dst_reg + off16) = imm32 */
 -
 -#define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,	\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
 -
 -#define BPF_JMP_REG(OP, DST, SRC, OFF)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = 0 })
 -
 -/* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
 -
 -#define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
 -		.dst_reg = DST,					\
 -		.src_reg = 0,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Function call */
 -
 -#define BPF_EMIT_CALL(FUNC)					\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_JMP | BPF_CALL,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = ((FUNC) - __bpf_call_base) })
 -
 -/* Raw code statement block */
 -
 -#define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
 -	((struct sock_filter_int) {				\
 -		.code  = CODE,					\
 -		.dst_reg = DST,					\
 -		.src_reg = SRC,					\
 -		.off   = OFF,					\
 -		.imm   = IMM })
 -
 -/* Program exit */
 -
 -#define BPF_EXIT_INSN()						\
 -	((struct sock_filter_int) {				\
 -		.code  = BPF_JMP | BPF_EXIT,			\
 -		.dst_reg = 0,					\
 -		.src_reg = 0,					\
 -		.off   = 0,					\
 -		.imm   = 0 })
 -
 -#define bytes_to_bpf_size(bytes)				\
 -({								\
 -	int bpf_size = -EINVAL;					\
 -								\
 -	if (bytes == sizeof(u8))				\
 -		bpf_size = BPF_B;				\
 -	else if (bytes == sizeof(u16))				\
 -		bpf_size = BPF_H;				\
 -	else if (bytes == sizeof(u32))				\
 -		bpf_size = BPF_W;				\
 -	else if (bytes == sizeof(u64))				\
 -		bpf_size = BPF_DW;				\
 -								\
 -	bpf_size;						\
 -})
 -
 -/* Macro to invoke filter function. */
 -#define SK_RUN_FILTER(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 -
 -struct sock_filter_int {
 -	__u8	code;		/* opcode */
 -	__u8	dst_reg:4;	/* dest register */
 -	__u8	src_reg:4;	/* source register */
 -	__s16	off;		/* signed offset */
 -	__s32	imm;		/* signed immediate constant */
 -};
 +#ifndef __GENKSYMS__
 +#include <net/sch_generic.h>
 +#endif
  
  #ifdef CONFIG_COMPAT
 -/* A struct sock_filter is architecture independent. */
 +/*
 + * A struct sock_filter is architecture independent.
 + */
  struct compat_sock_fprog {
  	u16		len;
 -	compat_uptr_t	filter;	/* struct sock_filter * */
 +	compat_uptr_t	filter;		/* struct sock_filter * */
  };
  #endif
  
 -struct sock_fprog_kern {
 -	u16			len;
 -	struct sock_filter	*filter;
 -};
 -
  struct sk_buff;
  struct sock;
 -struct seccomp_data;
 +struct bpf_prog_aux;
  
 -struct sk_filter {
 +struct bpf_prog
 +{
 +	struct bpf_prog_aux	*aux;	/* Auxiliary fields */
 +};
 +
 +struct sk_filter
 +{
  	atomic_t		refcnt;
 -	u32			jited:1,	/* Is our filter JIT'ed? */
 -				len:31;		/* Number of filter blocks */
 -	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 -	struct rcu_head		rcu;
 +	unsigned int         	len;	/* Number of filter blocks */
  	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 -					    const struct sock_filter_int *filter);
 -	union {
 -		struct sock_filter	insns[0];
 -		struct sock_filter_int	insnsi[0];
 -		struct work_struct	work;
 -	};
 +					    const struct sock_filter *filter);
 +	struct rcu_head		rcu;
 +	struct sock_filter     	insns[0];
  };
  
 -static inline unsigned int sk_filter_size(unsigned int proglen)
 +struct xdp_buff {
 +	void *data;
 +	void *data_end;
 +	void *data_hard_start;
 +};
 +
 +/* compute the linear packet data range [data, data_end) which
 + * will be accessed by cls_bpf and act_bpf programs
 + */
 +static inline unsigned int sk_filter_len(const struct sk_filter *fp)
  {
 -	return max(sizeof(struct sk_filter),
 -		   offsetof(struct sk_filter, insns[proglen]));
 +	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
  }
  
 -#define sk_filter_proglen(fprog)			\
 -		(fprog->len * sizeof(fprog->filter[0]))
 -
 -int sk_filter(struct sock *sk, struct sk_buff *skb);
 -
 -void sk_filter_select_runtime(struct sk_filter *fp);
 -void sk_filter_free(struct sk_filter *fp);
 -
 -int sk_convert_filter(struct sock_filter *prog, int len,
 -		      struct sock_filter_int *new_prog, int *new_len);
 -
 -int sk_unattached_filter_create(struct sk_filter **pfp,
 -				struct sock_fprog_kern *fprog);
 -void sk_unattached_filter_destroy(struct sk_filter *fp);
 -
 -int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 -int sk_detach_filter(struct sock *sk);
 -
 -int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 -int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 -		  unsigned int len);
 -
 -void sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 -void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 -
 -u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 -void bpf_int_jit_compile(struct sk_filter *fp);
 +static inline void bpf_compute_data_end(struct sk_buff *skb)
 +{
 +	return;
 +}
  
 -#define BPF_ANC		BIT(15)
 +int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
 +static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 +{
 +	return sk_filter_trim_cap(sk, skb, 1);
 +}
  
 -static inline u16 bpf_anc_helper(const struct sock_filter *ftest)
 +extern unsigned int sk_run_filter(const struct sk_buff *skb,
 +				  const struct sock_filter *filter);
 +extern int sk_unattached_filter_create(struct sk_filter **pfp,
 +				       struct sock_fprog *fprog);
 +extern void sk_unattached_filter_destroy(struct sk_filter *fp);
 +extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 +extern int sk_detach_filter(struct sock *sk);
 +extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 +extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 +extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
 +
 +static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 +				   struct xdp_buff *xdp)
  {
 -	BUG_ON(ftest->code & BPF_ANC);
 +	return 0;
 +}
  
 -	switch (ftest->code) {
 -	case BPF_LD | BPF_W | BPF_ABS:
 -	case BPF_LD | BPF_H | BPF_ABS:
 -	case BPF_LD | BPF_B | BPF_ABS:
 -#define BPF_ANCILLARY(CODE)	case SKF_AD_OFF + SKF_AD_##CODE:	\
 -				return BPF_ANC | SKF_AD_##CODE
 -		switch (ftest->k) {
 -		BPF_ANCILLARY(PROTOCOL);
 -		BPF_ANCILLARY(PKTTYPE);
 -		BPF_ANCILLARY(IFINDEX);
 -		BPF_ANCILLARY(NLATTR);
 -		BPF_ANCILLARY(NLATTR_NEST);
 -		BPF_ANCILLARY(MARK);
 -		BPF_ANCILLARY(QUEUE);
 -		BPF_ANCILLARY(HATYPE);
 -		BPF_ANCILLARY(RXHASH);
 -		BPF_ANCILLARY(CPU);
 -		BPF_ANCILLARY(ALU_XOR_X);
 -		BPF_ANCILLARY(VLAN_TAG);
 -		BPF_ANCILLARY(VLAN_TAG_PRESENT);
 -		BPF_ANCILLARY(PAY_OFFSET);
 -		BPF_ANCILLARY(RANDOM);
 -		}
 -		/* Fallthrough. */
 -	default:
 -		return ftest->code;
 -	}
 +static inline void bpf_warn_invalid_xdp_action(u32 act)
 +{
 +	return;
  }
  
+ void *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb,
+ 					   int k, unsigned int size);
+ 
+ static inline void *bpf_load_pointer(const struct sk_buff *skb, int k,
+ 				     unsigned int size, void *buffer)
+ {
+ 	if (k >= 0)
+ 		return skb_header_pointer(skb, k, size, buffer);
+ 
+ 	return bpf_internal_load_pointer_neg_helper(skb, k, size);
+ }
+ 
  #ifdef CONFIG_BPF_JIT
  #include <stdarg.h>
  #include <linux/linkage.h>
diff --cc net/core/filter.c
index 060ed5f86613,87af1e3e56c0..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -59,19 -83,11 +59,22 @@@ void *bpf_internal_load_pointer_neg_hel
  	return NULL;
  }
  
++<<<<<<< HEAD
 +static inline void *load_pointer(const struct sk_buff *skb, int k,
 +				 unsigned int size, void *buffer)
 +{
 +	if (k >= 0)
 +		return skb_header_pointer(skb, k, size, buffer);
 +	return bpf_internal_load_pointer_neg_helper(skb, k, size);
 +}
 +
++=======
++>>>>>>> 9f12fbe603f7 (net: filter: move load_pointer() into filter.h)
  /**
 - *	sk_filter - run a packet through a socket filter
 + *	sk_filter_trim_cap - run a packet through a socket filter
   *	@sk: sock associated with &sk_buff
   *	@skb: buffer to filter
 + *	@cap: limit on how short the eBPF program may trim the packet
   *
   * Run the filter code and then cut skb->data to correct size returned by
   * sk_run_filter. If pkt_len is 0 we toss packet. If skb->len is smaller
@@@ -108,314 -124,962 +111,645 @@@ int sk_filter_trim_cap(struct sock *sk
  
  	return err;
  }
 -EXPORT_SYMBOL(sk_filter);
 -
 -/* Base function for offset calculation. Needs to go into .text section,
 - * therefore keeping it non-static as well; will also be used by JITs
 - * anyway later on, so do not let the compiler omit it.
 - */
 -noinline u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
 -{
 -	return 0;
 -}
 +EXPORT_SYMBOL(sk_filter_trim_cap);
  
  /**
 - *	__sk_run_filter - run a filter on a given context
 - *	@ctx: buffer to run the filter on
 - *	@insn: filter to apply
 + *	sk_run_filter - run a filter on a socket
 + *	@skb: buffer to run the filter on
 + *	@fentry: filter to apply
   *
 - * Decode and apply filter instructions to the skb->data. Return length to
 - * keep, 0 for none. @ctx is the data we are operating on, @insn is the
 - * array of filter instructions.
 + * Decode and apply filter instructions to the skb->data.
 + * Return length to keep, 0 for none. @skb is the data we are
 + * filtering, @filter is the array of filter instructions.
 + * Because all jumps are guaranteed to be before last instruction,
 + * and last instruction guaranteed to be a RET, we dont need to check
 + * flen. (We used to pass to this function the length of filter)
   */
 -static unsigned int __sk_run_filter(void *ctx, const struct sock_filter_int *insn)
 +unsigned int sk_run_filter(const struct sk_buff *skb,
 +			   const struct sock_filter *fentry)
  {
 -	u64 stack[MAX_BPF_STACK / sizeof(u64)];
 -	u64 regs[MAX_BPF_REG], tmp;
 -	static const void *jumptable[256] = {
 -		[0 ... 255] = &&default_label,
 -		/* Now overwrite non-defaults ... */
 -		/* 32 bit ALU operations */
 -		[BPF_ALU | BPF_ADD | BPF_X] = &&ALU_ADD_X,
 -		[BPF_ALU | BPF_ADD | BPF_K] = &&ALU_ADD_K,
 -		[BPF_ALU | BPF_SUB | BPF_X] = &&ALU_SUB_X,
 -		[BPF_ALU | BPF_SUB | BPF_K] = &&ALU_SUB_K,
 -		[BPF_ALU | BPF_AND | BPF_X] = &&ALU_AND_X,
 -		[BPF_ALU | BPF_AND | BPF_K] = &&ALU_AND_K,
 -		[BPF_ALU | BPF_OR | BPF_X]  = &&ALU_OR_X,
 -		[BPF_ALU | BPF_OR | BPF_K]  = &&ALU_OR_K,
 -		[BPF_ALU | BPF_LSH | BPF_X] = &&ALU_LSH_X,
 -		[BPF_ALU | BPF_LSH | BPF_K] = &&ALU_LSH_K,
 -		[BPF_ALU | BPF_RSH | BPF_X] = &&ALU_RSH_X,
 -		[BPF_ALU | BPF_RSH | BPF_K] = &&ALU_RSH_K,
 -		[BPF_ALU | BPF_XOR | BPF_X] = &&ALU_XOR_X,
 -		[BPF_ALU | BPF_XOR | BPF_K] = &&ALU_XOR_K,
 -		[BPF_ALU | BPF_MUL | BPF_X] = &&ALU_MUL_X,
 -		[BPF_ALU | BPF_MUL | BPF_K] = &&ALU_MUL_K,
 -		[BPF_ALU | BPF_MOV | BPF_X] = &&ALU_MOV_X,
 -		[BPF_ALU | BPF_MOV | BPF_K] = &&ALU_MOV_K,
 -		[BPF_ALU | BPF_DIV | BPF_X] = &&ALU_DIV_X,
 -		[BPF_ALU | BPF_DIV | BPF_K] = &&ALU_DIV_K,
 -		[BPF_ALU | BPF_MOD | BPF_X] = &&ALU_MOD_X,
 -		[BPF_ALU | BPF_MOD | BPF_K] = &&ALU_MOD_K,
 -		[BPF_ALU | BPF_NEG] = &&ALU_NEG,
 -		[BPF_ALU | BPF_END | BPF_TO_BE] = &&ALU_END_TO_BE,
 -		[BPF_ALU | BPF_END | BPF_TO_LE] = &&ALU_END_TO_LE,
 -		/* 64 bit ALU operations */
 -		[BPF_ALU64 | BPF_ADD | BPF_X] = &&ALU64_ADD_X,
 -		[BPF_ALU64 | BPF_ADD | BPF_K] = &&ALU64_ADD_K,
 -		[BPF_ALU64 | BPF_SUB | BPF_X] = &&ALU64_SUB_X,
 -		[BPF_ALU64 | BPF_SUB | BPF_K] = &&ALU64_SUB_K,
 -		[BPF_ALU64 | BPF_AND | BPF_X] = &&ALU64_AND_X,
 -		[BPF_ALU64 | BPF_AND | BPF_K] = &&ALU64_AND_K,
 -		[BPF_ALU64 | BPF_OR | BPF_X] = &&ALU64_OR_X,
 -		[BPF_ALU64 | BPF_OR | BPF_K] = &&ALU64_OR_K,
 -		[BPF_ALU64 | BPF_LSH | BPF_X] = &&ALU64_LSH_X,
 -		[BPF_ALU64 | BPF_LSH | BPF_K] = &&ALU64_LSH_K,
 -		[BPF_ALU64 | BPF_RSH | BPF_X] = &&ALU64_RSH_X,
 -		[BPF_ALU64 | BPF_RSH | BPF_K] = &&ALU64_RSH_K,
 -		[BPF_ALU64 | BPF_XOR | BPF_X] = &&ALU64_XOR_X,
 -		[BPF_ALU64 | BPF_XOR | BPF_K] = &&ALU64_XOR_K,
 -		[BPF_ALU64 | BPF_MUL | BPF_X] = &&ALU64_MUL_X,
 -		[BPF_ALU64 | BPF_MUL | BPF_K] = &&ALU64_MUL_K,
 -		[BPF_ALU64 | BPF_MOV | BPF_X] = &&ALU64_MOV_X,
 -		[BPF_ALU64 | BPF_MOV | BPF_K] = &&ALU64_MOV_K,
 -		[BPF_ALU64 | BPF_ARSH | BPF_X] = &&ALU64_ARSH_X,
 -		[BPF_ALU64 | BPF_ARSH | BPF_K] = &&ALU64_ARSH_K,
 -		[BPF_ALU64 | BPF_DIV | BPF_X] = &&ALU64_DIV_X,
 -		[BPF_ALU64 | BPF_DIV | BPF_K] = &&ALU64_DIV_K,
 -		[BPF_ALU64 | BPF_MOD | BPF_X] = &&ALU64_MOD_X,
 -		[BPF_ALU64 | BPF_MOD | BPF_K] = &&ALU64_MOD_K,
 -		[BPF_ALU64 | BPF_NEG] = &&ALU64_NEG,
 -		/* Call instruction */
 -		[BPF_JMP | BPF_CALL] = &&JMP_CALL,
 -		/* Jumps */
 -		[BPF_JMP | BPF_JA] = &&JMP_JA,
 -		[BPF_JMP | BPF_JEQ | BPF_X] = &&JMP_JEQ_X,
 -		[BPF_JMP | BPF_JEQ | BPF_K] = &&JMP_JEQ_K,
 -		[BPF_JMP | BPF_JNE | BPF_X] = &&JMP_JNE_X,
 -		[BPF_JMP | BPF_JNE | BPF_K] = &&JMP_JNE_K,
 -		[BPF_JMP | BPF_JGT | BPF_X] = &&JMP_JGT_X,
 -		[BPF_JMP | BPF_JGT | BPF_K] = &&JMP_JGT_K,
 -		[BPF_JMP | BPF_JGE | BPF_X] = &&JMP_JGE_X,
 -		[BPF_JMP | BPF_JGE | BPF_K] = &&JMP_JGE_K,
 -		[BPF_JMP | BPF_JSGT | BPF_X] = &&JMP_JSGT_X,
 -		[BPF_JMP | BPF_JSGT | BPF_K] = &&JMP_JSGT_K,
 -		[BPF_JMP | BPF_JSGE | BPF_X] = &&JMP_JSGE_X,
 -		[BPF_JMP | BPF_JSGE | BPF_K] = &&JMP_JSGE_K,
 -		[BPF_JMP | BPF_JSET | BPF_X] = &&JMP_JSET_X,
 -		[BPF_JMP | BPF_JSET | BPF_K] = &&JMP_JSET_K,
 -		/* Program return */
 -		[BPF_JMP | BPF_EXIT] = &&JMP_EXIT,
 -		/* Store instructions */
 -		[BPF_STX | BPF_MEM | BPF_B] = &&STX_MEM_B,
 -		[BPF_STX | BPF_MEM | BPF_H] = &&STX_MEM_H,
 -		[BPF_STX | BPF_MEM | BPF_W] = &&STX_MEM_W,
 -		[BPF_STX | BPF_MEM | BPF_DW] = &&STX_MEM_DW,
 -		[BPF_STX | BPF_XADD | BPF_W] = &&STX_XADD_W,
 -		[BPF_STX | BPF_XADD | BPF_DW] = &&STX_XADD_DW,
 -		[BPF_ST | BPF_MEM | BPF_B] = &&ST_MEM_B,
 -		[BPF_ST | BPF_MEM | BPF_H] = &&ST_MEM_H,
 -		[BPF_ST | BPF_MEM | BPF_W] = &&ST_MEM_W,
 -		[BPF_ST | BPF_MEM | BPF_DW] = &&ST_MEM_DW,
 -		/* Load instructions */
 -		[BPF_LDX | BPF_MEM | BPF_B] = &&LDX_MEM_B,
 -		[BPF_LDX | BPF_MEM | BPF_H] = &&LDX_MEM_H,
 -		[BPF_LDX | BPF_MEM | BPF_W] = &&LDX_MEM_W,
 -		[BPF_LDX | BPF_MEM | BPF_DW] = &&LDX_MEM_DW,
 -		[BPF_LD | BPF_ABS | BPF_W] = &&LD_ABS_W,
 -		[BPF_LD | BPF_ABS | BPF_H] = &&LD_ABS_H,
 -		[BPF_LD | BPF_ABS | BPF_B] = &&LD_ABS_B,
 -		[BPF_LD | BPF_IND | BPF_W] = &&LD_IND_W,
 -		[BPF_LD | BPF_IND | BPF_H] = &&LD_IND_H,
 -		[BPF_LD | BPF_IND | BPF_B] = &&LD_IND_B,
 -	};
  	void *ptr;
 -	int off;
 +	u32 A = 0;			/* Accumulator */
 +	u32 X = 0;			/* Index Register */
 +	u32 mem[BPF_MEMWORDS];		/* Scratch Memory Store */
 +	u32 tmp;
 +	int k;
  
++<<<<<<< HEAD
 +	/*
 +	 * Process array of filter instructions.
 +	 */
 +	for (;; fentry++) {
 +#if defined(CONFIG_X86_32)
 +#define	K (fentry->k)
++=======
+ #define CONT	 ({ insn++; goto select_insn; })
+ #define CONT_JMP ({ insn++; goto select_insn; })
+ 
+ 	FP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)];
+ 	ARG1 = (u64) (unsigned long) ctx;
+ 
+ 	/* Registers used in classic BPF programs need to be reset first. */
+ 	regs[BPF_REG_A] = 0;
+ 	regs[BPF_REG_X] = 0;
+ 
+ select_insn:
+ 	goto *jumptable[insn->code];
+ 
+ 	/* ALU */
+ #define ALU(OPCODE, OP)			\
+ 	ALU64_##OPCODE##_X:		\
+ 		DST = DST OP SRC;	\
+ 		CONT;			\
+ 	ALU_##OPCODE##_X:		\
+ 		DST = (u32) DST OP (u32) SRC;	\
+ 		CONT;			\
+ 	ALU64_##OPCODE##_K:		\
+ 		DST = DST OP IMM;		\
+ 		CONT;			\
+ 	ALU_##OPCODE##_K:		\
+ 		DST = (u32) DST OP (u32) IMM;	\
+ 		CONT;
+ 
+ 	ALU(ADD,  +)
+ 	ALU(SUB,  -)
+ 	ALU(AND,  &)
+ 	ALU(OR,   |)
+ 	ALU(LSH, <<)
+ 	ALU(RSH, >>)
+ 	ALU(XOR,  ^)
+ 	ALU(MUL,  *)
+ #undef ALU
+ 	ALU_NEG:
+ 		DST = (u32) -DST;
+ 		CONT;
+ 	ALU64_NEG:
+ 		DST = -DST;
+ 		CONT;
+ 	ALU_MOV_X:
+ 		DST = (u32) SRC;
+ 		CONT;
+ 	ALU_MOV_K:
+ 		DST = (u32) IMM;
+ 		CONT;
+ 	ALU64_MOV_X:
+ 		DST = SRC;
+ 		CONT;
+ 	ALU64_MOV_K:
+ 		DST = IMM;
+ 		CONT;
+ 	ALU64_ARSH_X:
+ 		(*(s64 *) &DST) >>= SRC;
+ 		CONT;
+ 	ALU64_ARSH_K:
+ 		(*(s64 *) &DST) >>= IMM;
+ 		CONT;
+ 	ALU64_MOD_X:
+ 		if (unlikely(SRC == 0))
+ 			return 0;
+ 		tmp = DST;
+ 		DST = do_div(tmp, SRC);
+ 		CONT;
+ 	ALU_MOD_X:
+ 		if (unlikely(SRC == 0))
+ 			return 0;
+ 		tmp = (u32) DST;
+ 		DST = do_div(tmp, (u32) SRC);
+ 		CONT;
+ 	ALU64_MOD_K:
+ 		tmp = DST;
+ 		DST = do_div(tmp, IMM);
+ 		CONT;
+ 	ALU_MOD_K:
+ 		tmp = (u32) DST;
+ 		DST = do_div(tmp, (u32) IMM);
+ 		CONT;
+ 	ALU64_DIV_X:
+ 		if (unlikely(SRC == 0))
+ 			return 0;
+ 		do_div(DST, SRC);
+ 		CONT;
+ 	ALU_DIV_X:
+ 		if (unlikely(SRC == 0))
+ 			return 0;
+ 		tmp = (u32) DST;
+ 		do_div(tmp, (u32) SRC);
+ 		DST = (u32) tmp;
+ 		CONT;
+ 	ALU64_DIV_K:
+ 		do_div(DST, IMM);
+ 		CONT;
+ 	ALU_DIV_K:
+ 		tmp = (u32) DST;
+ 		do_div(tmp, (u32) IMM);
+ 		DST = (u32) tmp;
+ 		CONT;
+ 	ALU_END_TO_BE:
+ 		switch (IMM) {
+ 		case 16:
+ 			DST = (__force u16) cpu_to_be16(DST);
+ 			break;
+ 		case 32:
+ 			DST = (__force u32) cpu_to_be32(DST);
+ 			break;
+ 		case 64:
+ 			DST = (__force u64) cpu_to_be64(DST);
+ 			break;
+ 		}
+ 		CONT;
+ 	ALU_END_TO_LE:
+ 		switch (IMM) {
+ 		case 16:
+ 			DST = (__force u16) cpu_to_le16(DST);
+ 			break;
+ 		case 32:
+ 			DST = (__force u32) cpu_to_le32(DST);
+ 			break;
+ 		case 64:
+ 			DST = (__force u64) cpu_to_le64(DST);
+ 			break;
+ 		}
+ 		CONT;
+ 
+ 	/* CALL */
+ 	JMP_CALL:
+ 		/* Function call scratches BPF_R1-BPF_R5 registers,
+ 		 * preserves BPF_R6-BPF_R9, and stores return value
+ 		 * into BPF_R0.
+ 		 */
+ 		BPF_R0 = (__bpf_call_base + insn->imm)(BPF_R1, BPF_R2, BPF_R3,
+ 						       BPF_R4, BPF_R5);
+ 		CONT;
+ 
+ 	/* JMP */
+ 	JMP_JA:
+ 		insn += insn->off;
+ 		CONT;
+ 	JMP_JEQ_X:
+ 		if (DST == SRC) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JEQ_K:
+ 		if (DST == IMM) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JNE_X:
+ 		if (DST != SRC) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JNE_K:
+ 		if (DST != IMM) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGT_X:
+ 		if (DST > SRC) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGT_K:
+ 		if (DST > IMM) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGE_X:
+ 		if (DST >= SRC) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGE_K:
+ 		if (DST >= IMM) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGT_X:
+ 		if (((s64) DST) > ((s64) SRC)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGT_K:
+ 		if (((s64) DST) > ((s64) IMM)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGE_X:
+ 		if (((s64) DST) >= ((s64) SRC)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGE_K:
+ 		if (((s64) DST) >= ((s64) IMM)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSET_X:
+ 		if (DST & SRC) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSET_K:
+ 		if (DST & IMM) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_EXIT:
+ 		return BPF_R0;
+ 
+ 	/* STX and ST and LDX*/
+ #define LDST(SIZEOP, SIZE)						\
+ 	STX_MEM_##SIZEOP:						\
+ 		*(SIZE *)(unsigned long) (DST + insn->off) = SRC;	\
+ 		CONT;							\
+ 	ST_MEM_##SIZEOP:						\
+ 		*(SIZE *)(unsigned long) (DST + insn->off) = IMM;	\
+ 		CONT;							\
+ 	LDX_MEM_##SIZEOP:						\
+ 		DST = *(SIZE *)(unsigned long) (SRC + insn->off);	\
+ 		CONT;
+ 
+ 	LDST(B,   u8)
+ 	LDST(H,  u16)
+ 	LDST(W,  u32)
+ 	LDST(DW, u64)
+ #undef LDST
+ 	STX_XADD_W: /* lock xadd *(u32 *)(dst_reg + off16) += src_reg */
+ 		atomic_add((u32) SRC, (atomic_t *)(unsigned long)
+ 			   (DST + insn->off));
+ 		CONT;
+ 	STX_XADD_DW: /* lock xadd *(u64 *)(dst_reg + off16) += src_reg */
+ 		atomic64_add((u64) SRC, (atomic64_t *)(unsigned long)
+ 			     (DST + insn->off));
+ 		CONT;
+ 	LD_ABS_W: /* BPF_R0 = ntohl(*(u32 *) (skb->data + imm32)) */
+ 		off = IMM;
+ load_word:
+ 		/* BPF_LD + BPD_ABS and BPF_LD + BPF_IND insns are
+ 		 * only appearing in the programs where ctx ==
+ 		 * skb. All programs keep 'ctx' in regs[BPF_REG_CTX]
+ 		 * == BPF_R6, sk_convert_filter() saves it in BPF_R6,
+ 		 * internal BPF verifier will check that BPF_R6 ==
+ 		 * ctx.
+ 		 *
+ 		 * BPF_ABS and BPF_IND are wrappers of function calls,
+ 		 * so they scratch BPF_R1-BPF_R5 registers, preserve
+ 		 * BPF_R6-BPF_R9, and store return value into BPF_R0.
+ 		 *
+ 		 * Implicit input:
+ 		 *   ctx == skb == BPF_R6 == CTX
+ 		 *
+ 		 * Explicit input:
+ 		 *   SRC == any register
+ 		 *   IMM == 32-bit immediate
+ 		 *
+ 		 * Output:
+ 		 *   BPF_R0 - 8/16/32-bit skb data converted to cpu endianness
+ 		 */
+ 
+ 		ptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 4, &tmp);
+ 		if (likely(ptr != NULL)) {
+ 			BPF_R0 = get_unaligned_be32(ptr);
+ 			CONT;
+ 		}
+ 
+ 		return 0;
+ 	LD_ABS_H: /* BPF_R0 = ntohs(*(u16 *) (skb->data + imm32)) */
+ 		off = IMM;
+ load_half:
+ 		ptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 2, &tmp);
+ 		if (likely(ptr != NULL)) {
+ 			BPF_R0 = get_unaligned_be16(ptr);
+ 			CONT;
+ 		}
+ 
+ 		return 0;
+ 	LD_ABS_B: /* BPF_R0 = *(u8 *) (skb->data + imm32) */
+ 		off = IMM;
+ load_byte:
+ 		ptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 1, &tmp);
+ 		if (likely(ptr != NULL)) {
+ 			BPF_R0 = *(u8 *)ptr;
+ 			CONT;
+ 		}
+ 
+ 		return 0;
+ 	LD_IND_W: /* BPF_R0 = ntohl(*(u32 *) (skb->data + src_reg + imm32)) */
+ 		off = IMM + SRC;
+ 		goto load_word;
+ 	LD_IND_H: /* BPF_R0 = ntohs(*(u16 *) (skb->data + src_reg + imm32)) */
+ 		off = IMM + SRC;
+ 		goto load_half;
+ 	LD_IND_B: /* BPF_R0 = *(u8 *) (skb->data + src_reg + imm32) */
+ 		off = IMM + SRC;
+ 		goto load_byte;
+ 
+ 	default_label:
+ 		/* If we ever reach this, we have a bug somewhere. */
+ 		WARN_RATELIMIT(1, "unknown opcode %02x\n", insn->code);
+ 		return 0;
+ }
+ 
+ /* Helper to find the offset of pkt_type in sk_buff structure. We want
+  * to make sure its still a 3bit field starting at a byte boundary;
+  * taken from arch/x86/net/bpf_jit_comp.c.
+  */
+ #ifdef __BIG_ENDIAN_BITFIELD
+ #define PKT_TYPE_MAX	(7 << 5)
++>>>>>>> 9f12fbe603f7 (net: filter: move load_pointer() into filter.h)
  #else
 -#define PKT_TYPE_MAX	7
 +		const u32 K = fentry->k;
  #endif
 -static unsigned int pkt_type_offset(void)
 -{
 -	struct sk_buff skb_probe = { .pkt_type = ~0, };
 -	u8 *ct = (u8 *) &skb_probe;
 -	unsigned int off;
 -
 -	for (off = 0; off < sizeof(struct sk_buff); off++) {
 -		if (ct[off] == PKT_TYPE_MAX)
 -			return off;
 -	}
 -
 -	pr_err_once("Please fix %s, as pkt_type couldn't be found!\n", __func__);
 -	return -1;
 -}
 -
 -static u64 __skb_get_pay_offset(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
 -{
 -	return __skb_get_poff((struct sk_buff *)(unsigned long) ctx);
 -}
 -
 -static u64 __skb_get_nlattr(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
 -{
 -	struct sk_buff *skb = (struct sk_buff *)(unsigned long) ctx;
 -	struct nlattr *nla;
 -
 -	if (skb_is_nonlinear(skb))
 -		return 0;
 -
 -	if (skb->len < sizeof(struct nlattr))
 -		return 0;
 -
 -	if (a > skb->len - sizeof(struct nlattr))
 -		return 0;
 -
 -	nla = nla_find((struct nlattr *) &skb->data[a], skb->len - a, x);
 -	if (nla)
 -		return (void *) nla - (void *) skb->data;
  
 -	return 0;
 -}
 -
 -static u64 __skb_get_nlattr_nest(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
 -{
 -	struct sk_buff *skb = (struct sk_buff *)(unsigned long) ctx;
 -	struct nlattr *nla;
 -
 -	if (skb_is_nonlinear(skb))
 -		return 0;
 -
 -	if (skb->len < sizeof(struct nlattr))
 -		return 0;
 -
 -	if (a > skb->len - sizeof(struct nlattr))
 -		return 0;
 -
 -	nla = (struct nlattr *) &skb->data[a];
 -	if (nla->nla_len > skb->len - a)
 -		return 0;
 -
 -	nla = nla_find_nested(nla, x);
 -	if (nla)
 -		return (void *) nla - (void *) skb->data;
 -
 -	return 0;
 -}
 -
 -static u64 __get_raw_cpu_id(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
 -{
 -	return raw_smp_processor_id();
 -}
 -
 -/* note that this only generates 32-bit random numbers */
 -static u64 __get_random_u32(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
 -{
 -	return prandom_u32();
 -}
 -
 -static bool convert_bpf_extensions(struct sock_filter *fp,
 -				   struct sock_filter_int **insnp)
 -{
 -	struct sock_filter_int *insn = *insnp;
 -
 -	switch (fp->k) {
 -	case SKF_AD_OFF + SKF_AD_PROTOCOL:
 -		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
 -
 -		/* A = *(u16 *) (CTX + offsetof(protocol)) */
 -		*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
 -				      offsetof(struct sk_buff, protocol));
 -		/* A = ntohs(A) [emitting a nop or swap16] */
 -		*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_PKTTYPE:
 -		*insn = BPF_LDX_MEM(BPF_B, BPF_REG_A, BPF_REG_CTX,
 -				    pkt_type_offset());
 -		if (insn->off < 0)
 -			return false;
 -		insn++;
 -		*insn = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, PKT_TYPE_MAX);
 -#ifdef __BIG_ENDIAN_BITFIELD
 -		insn++;
 -                *insn = BPF_ALU32_IMM(BPF_RSH, BPF_REG_A, 5);
 -#endif
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_IFINDEX:
 -	case SKF_AD_OFF + SKF_AD_HATYPE:
 -		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
 -		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
 -		BUILD_BUG_ON(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)) < 0);
 -
 -		*insn++ = BPF_LDX_MEM(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)),
 -				      BPF_REG_TMP, BPF_REG_CTX,
 -				      offsetof(struct sk_buff, dev));
 -		/* if (tmp != 0) goto pc + 1 */
 -		*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_TMP, 0, 1);
 -		*insn++ = BPF_EXIT_INSN();
 -		if (fp->k == SKF_AD_OFF + SKF_AD_IFINDEX)
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_TMP,
 -					    offsetof(struct net_device, ifindex));
 -		else
 -			*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_TMP,
 -					    offsetof(struct net_device, type));
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_MARK:
 -		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
 -
 -		*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,
 -				    offsetof(struct sk_buff, mark));
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_RXHASH:
 -		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
 -
 -		*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,
 -				    offsetof(struct sk_buff, hash));
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_QUEUE:
 -		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
 -
 -		*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
 -				    offsetof(struct sk_buff, queue_mapping));
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_VLAN_TAG:
 -	case SKF_AD_OFF + SKF_AD_VLAN_TAG_PRESENT:
 -		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
 -		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
 -
 -		/* A = *(u16 *) (CTX + offsetof(vlan_tci)) */
 -		*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
 -				      offsetof(struct sk_buff, vlan_tci));
 -		if (fp->k == SKF_AD_OFF + SKF_AD_VLAN_TAG) {
 -			*insn = BPF_ALU32_IMM(BPF_AND, BPF_REG_A,
 -					      ~VLAN_TAG_PRESENT);
 -		} else {
 -			/* A >>= 12 */
 -			*insn++ = BPF_ALU32_IMM(BPF_RSH, BPF_REG_A, 12);
 -			/* A &= 1 */
 -			*insn = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 1);
 -		}
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
 -	case SKF_AD_OFF + SKF_AD_NLATTR:
 -	case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
 -	case SKF_AD_OFF + SKF_AD_CPU:
 -	case SKF_AD_OFF + SKF_AD_RANDOM:
 -		/* arg1 = CTX */
 -		*insn++ = BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_CTX);
 -		/* arg2 = A */
 -		*insn++ = BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_A);
 -		/* arg3 = X */
 -		*insn++ = BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_X);
 -		/* Emit call(arg1=CTX, arg2=A, arg3=X) */
 -		switch (fp->k) {
 -		case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
 -			*insn = BPF_EMIT_CALL(__skb_get_pay_offset);
 -			break;
 -		case SKF_AD_OFF + SKF_AD_NLATTR:
 -			*insn = BPF_EMIT_CALL(__skb_get_nlattr);
 -			break;
 -		case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
 -			*insn = BPF_EMIT_CALL(__skb_get_nlattr_nest);
 -			break;
 -		case SKF_AD_OFF + SKF_AD_CPU:
 -			*insn = BPF_EMIT_CALL(__get_raw_cpu_id);
 -			break;
 -		case SKF_AD_OFF + SKF_AD_RANDOM:
 -			*insn = BPF_EMIT_CALL(__get_random_u32);
 -			break;
 -		}
 -		break;
 -
 -	case SKF_AD_OFF + SKF_AD_ALU_XOR_X:
 -		/* A ^= X */
 -		*insn = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_X);
 -		break;
 -
 -	default:
 -		/* This is just a dummy call to avoid letting the compiler
 -		 * evict __bpf_call_base() as an optimization. Placed here
 -		 * where no-one bothers.
 -		 */
 -		BUG_ON(__bpf_call_base(0, 0, 0, 0, 0) != 0);
 -		return false;
 -	}
 -
 -	*insnp = insn;
 -	return true;
 -}
 -
 -/**
 - *	sk_convert_filter - convert filter program
 - *	@prog: the user passed filter program
 - *	@len: the length of the user passed filter program
 - *	@new_prog: buffer where converted program will be stored
 - *	@new_len: pointer to store length of converted program
 - *
 - * Remap 'sock_filter' style BPF instruction set to 'sock_filter_ext' style.
 - * Conversion workflow:
 - *
 - * 1) First pass for calculating the new program length:
 - *   sk_convert_filter(old_prog, old_len, NULL, &new_len)
 - *
 - * 2) 2nd pass to remap in two passes: 1st pass finds new
 - *    jump offsets, 2nd pass remapping:
 - *   new_prog = kmalloc(sizeof(struct sock_filter_int) * new_len);
 - *   sk_convert_filter(old_prog, old_len, new_prog, &new_len);
 - *
 - * User BPF's register A is mapped to our BPF register 6, user BPF
 - * register X is mapped to BPF register 7; frame pointer is always
 - * register 10; Context 'void *ctx' is stored in register 1, that is,
 - * for socket filters: ctx == 'struct sk_buff *', for seccomp:
 - * ctx == 'struct seccomp_data *'.
 - */
 -int sk_convert_filter(struct sock_filter *prog, int len,
 -		      struct sock_filter_int *new_prog, int *new_len)
 -{
 -	int new_flen = 0, pass = 0, target, i;
 -	struct sock_filter_int *new_insn;
 -	struct sock_filter *fp;
 -	int *addrs = NULL;
 -	u8 bpf_src;
 -
 -	BUILD_BUG_ON(BPF_MEMWORDS * sizeof(u32) > MAX_BPF_STACK);
 -	BUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);
 -
 -	if (len <= 0 || len > BPF_MAXINSNS)
 -		return -EINVAL;
 -
 -	if (new_prog) {
 -		addrs = kcalloc(len, sizeof(*addrs), GFP_KERNEL);
 -		if (!addrs)
 -			return -ENOMEM;
 -	}
 -
 -do_pass:
 -	new_insn = new_prog;
 -	fp = prog;
 -
 -	if (new_insn)
 -		*new_insn = BPF_MOV64_REG(BPF_REG_CTX, BPF_REG_ARG1);
 -	new_insn++;
 -
 -	for (i = 0; i < len; fp++, i++) {
 -		struct sock_filter_int tmp_insns[6] = { };
 -		struct sock_filter_int *insn = tmp_insns;
 -
 -		if (addrs)
 -			addrs[i] = new_insn - new_prog;
 -
 -		switch (fp->code) {
 -		/* All arithmetic insns and skb loads map as-is. */
 -		case BPF_ALU | BPF_ADD | BPF_X:
 -		case BPF_ALU | BPF_ADD | BPF_K:
 -		case BPF_ALU | BPF_SUB | BPF_X:
 -		case BPF_ALU | BPF_SUB | BPF_K:
 -		case BPF_ALU | BPF_AND | BPF_X:
 -		case BPF_ALU | BPF_AND | BPF_K:
 -		case BPF_ALU | BPF_OR | BPF_X:
 -		case BPF_ALU | BPF_OR | BPF_K:
 -		case BPF_ALU | BPF_LSH | BPF_X:
 -		case BPF_ALU | BPF_LSH | BPF_K:
 -		case BPF_ALU | BPF_RSH | BPF_X:
 -		case BPF_ALU | BPF_RSH | BPF_K:
 -		case BPF_ALU | BPF_XOR | BPF_X:
 -		case BPF_ALU | BPF_XOR | BPF_K:
 -		case BPF_ALU | BPF_MUL | BPF_X:
 -		case BPF_ALU | BPF_MUL | BPF_K:
 -		case BPF_ALU | BPF_DIV | BPF_X:
 -		case BPF_ALU | BPF_DIV | BPF_K:
 -		case BPF_ALU | BPF_MOD | BPF_X:
 -		case BPF_ALU | BPF_MOD | BPF_K:
 -		case BPF_ALU | BPF_NEG:
 -		case BPF_LD | BPF_ABS | BPF_W:
 -		case BPF_LD | BPF_ABS | BPF_H:
 -		case BPF_LD | BPF_ABS | BPF_B:
 -		case BPF_LD | BPF_IND | BPF_W:
 -		case BPF_LD | BPF_IND | BPF_H:
 -		case BPF_LD | BPF_IND | BPF_B:
 -			/* Check for overloaded BPF extension and
 -			 * directly convert it if found, otherwise
 -			 * just move on with mapping.
 -			 */
 -			if (BPF_CLASS(fp->code) == BPF_LD &&
 -			    BPF_MODE(fp->code) == BPF_ABS &&
 -			    convert_bpf_extensions(fp, &insn))
 -				break;
 -
 -			*insn = BPF_RAW_INSN(fp->code, BPF_REG_A, BPF_REG_X, 0, fp->k);
 -			break;
 -
 -		/* Jump transformation cannot use BPF block macros
 -		 * everywhere as offset calculation and target updates
 -		 * require a bit more work than the rest, i.e. jump
 -		 * opcodes map as-is, but offsets need adjustment.
 -		 */
 -
 -#define BPF_EMIT_JMP							\
 -	do {								\
 -		if (target >= len || target < 0)			\
 -			goto err;					\
 -		insn->off = addrs ? addrs[target] - addrs[i] - 1 : 0;	\
 -		/* Adjust pc relative offset for 2nd or 3rd insn. */	\
 -		insn->off -= insn - tmp_insns;				\
 -	} while (0)
 -
 -		case BPF_JMP | BPF_JA:
 -			target = i + fp->k + 1;
 -			insn->code = fp->code;
 -			BPF_EMIT_JMP;
 -			break;
 -
 -		case BPF_JMP | BPF_JEQ | BPF_K:
 -		case BPF_JMP | BPF_JEQ | BPF_X:
 -		case BPF_JMP | BPF_JSET | BPF_K:
 -		case BPF_JMP | BPF_JSET | BPF_X:
 -		case BPF_JMP | BPF_JGT | BPF_K:
 -		case BPF_JMP | BPF_JGT | BPF_X:
 -		case BPF_JMP | BPF_JGE | BPF_K:
 -		case BPF_JMP | BPF_JGE | BPF_X:
 -			if (BPF_SRC(fp->code) == BPF_K && (int) fp->k < 0) {
 -				/* BPF immediates are signed, zero extend
 -				 * immediate into tmp register and use it
 -				 * in compare insn.
 -				 */
 -				*insn++ = BPF_MOV32_IMM(BPF_REG_TMP, fp->k);
 -
 -				insn->dst_reg = BPF_REG_A;
 -				insn->src_reg = BPF_REG_TMP;
 -				bpf_src = BPF_X;
 -			} else {
 -				insn->dst_reg = BPF_REG_A;
 -				insn->src_reg = BPF_REG_X;
 -				insn->imm = fp->k;
 -				bpf_src = BPF_SRC(fp->code);
 +		switch (fentry->code) {
 +		case BPF_S_ALU_ADD_X:
 +			A += X;
 +			continue;
 +		case BPF_S_ALU_ADD_K:
 +			A += K;
 +			continue;
 +		case BPF_S_ALU_SUB_X:
 +			A -= X;
 +			continue;
 +		case BPF_S_ALU_SUB_K:
 +			A -= K;
 +			continue;
 +		case BPF_S_ALU_MUL_X:
 +			A *= X;
 +			continue;
 +		case BPF_S_ALU_MUL_K:
 +			A *= K;
 +			continue;
 +		case BPF_S_ALU_DIV_X:
 +			if (X == 0)
 +				return 0;
 +			A /= X;
 +			continue;
 +		case BPF_S_ALU_DIV_K:
 +			A /= K;
 +			continue;
 +		case BPF_S_ALU_MOD_X:
 +			if (X == 0)
 +				return 0;
 +			A %= X;
 +			continue;
 +		case BPF_S_ALU_MOD_K:
 +			A %= K;
 +			continue;
 +		case BPF_S_ALU_AND_X:
 +			A &= X;
 +			continue;
 +		case BPF_S_ALU_AND_K:
 +			A &= K;
 +			continue;
 +		case BPF_S_ALU_OR_X:
 +			A |= X;
 +			continue;
 +		case BPF_S_ALU_OR_K:
 +			A |= K;
 +			continue;
 +		case BPF_S_ANC_ALU_XOR_X:
 +		case BPF_S_ALU_XOR_X:
 +			A ^= X;
 +			continue;
 +		case BPF_S_ALU_XOR_K:
 +			A ^= K;
 +			continue;
 +		case BPF_S_ALU_LSH_X:
 +			A <<= X;
 +			continue;
 +		case BPF_S_ALU_LSH_K:
 +			A <<= K;
 +			continue;
 +		case BPF_S_ALU_RSH_X:
 +			A >>= X;
 +			continue;
 +		case BPF_S_ALU_RSH_K:
 +			A >>= K;
 +			continue;
 +		case BPF_S_ALU_NEG:
 +			A = -A;
 +			continue;
 +		case BPF_S_JMP_JA:
 +			fentry += K;
 +			continue;
 +		case BPF_S_JMP_JGT_K:
 +			fentry += (A > K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_K:
 +			fentry += (A >= K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_K:
 +			fentry += (A == K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_K:
 +			fentry += (A & K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGT_X:
 +			fentry += (A > X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_X:
 +			fentry += (A >= X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_X:
 +			fentry += (A == X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_X:
 +			fentry += (A & X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_LD_W_ABS:
 +			k = K;
 +load_w:
 +			ptr = load_pointer(skb, k, 4, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be32(ptr);
 +				continue;
  			}
 -
 -			/* Common case where 'jump_false' is next insn. */
 -			if (fp->jf == 0) {
 -				insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -				target = i + fp->jt + 1;
 -				BPF_EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_H_ABS:
 +			k = K;
 +load_h:
 +			ptr = load_pointer(skb, k, 2, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be16(ptr);
 +				continue;
  			}
 -
 -			/* Convert JEQ into JNE when 'jump_true' is next insn. */
 -			if (fp->jt == 0 && BPF_OP(fp->code) == BPF_JEQ) {
 -				insn->code = BPF_JMP | BPF_JNE | bpf_src;
 -				target = i + fp->jf + 1;
 -				BPF_EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_B_ABS:
 +			k = K;
 +load_b:
 +			ptr = load_pointer(skb, k, 1, &tmp);
 +			if (ptr != NULL) {
 +				A = *(u8 *)ptr;
 +				continue;
  			}
 +			return 0;
 +		case BPF_S_LD_W_LEN:
 +			A = skb->len;
 +			continue;
 +		case BPF_S_LDX_W_LEN:
 +			X = skb->len;
 +			continue;
 +		case BPF_S_LD_W_IND:
 +			k = X + K;
 +			goto load_w;
 +		case BPF_S_LD_H_IND:
 +			k = X + K;
 +			goto load_h;
 +		case BPF_S_LD_B_IND:
 +			k = X + K;
 +			goto load_b;
 +		case BPF_S_LDX_B_MSH:
 +			ptr = load_pointer(skb, K, 1, &tmp);
 +			if (ptr != NULL) {
 +				X = (*(u8 *)ptr & 0xf) << 2;
 +				continue;
 +			}
 +			return 0;
 +		case BPF_S_LD_IMM:
 +			A = K;
 +			continue;
 +		case BPF_S_LDX_IMM:
 +			X = K;
 +			continue;
 +		case BPF_S_LD_MEM:
 +			A = mem[K];
 +			continue;
 +		case BPF_S_LDX_MEM:
 +			X = mem[K];
 +			continue;
 +		case BPF_S_MISC_TAX:
 +			X = A;
 +			continue;
 +		case BPF_S_MISC_TXA:
 +			A = X;
 +			continue;
 +		case BPF_S_RET_K:
 +			return K;
 +		case BPF_S_RET_A:
 +			return A;
 +		case BPF_S_ST:
 +			mem[K] = A;
 +			continue;
 +		case BPF_S_STX:
 +			mem[K] = X;
 +			continue;
 +		case BPF_S_ANC_PROTOCOL:
 +			A = ntohs(skb->protocol);
 +			continue;
 +		case BPF_S_ANC_PKTTYPE:
 +			A = skb->pkt_type;
 +			continue;
 +		case BPF_S_ANC_IFINDEX:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->ifindex;
 +			continue;
 +		case BPF_S_ANC_MARK:
 +			A = skb->mark;
 +			continue;
 +		case BPF_S_ANC_QUEUE:
 +			A = skb->queue_mapping;
 +			continue;
 +		case BPF_S_ANC_HATYPE:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->type;
 +			continue;
 +		case BPF_S_ANC_RXHASH:
 +			A = skb->hash;
 +			continue;
 +		case BPF_S_ANC_CPU:
 +			A = raw_smp_processor_id();
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG:
 +			A = skb_vlan_tag_get(skb);
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG_PRESENT:
 +			A = !!skb_vlan_tag_present(skb);
 +			continue;
 +		case BPF_S_ANC_PAY_OFFSET:
 +			A = skb_get_poff(skb);
 +			continue;
 +		case BPF_S_ANC_NLATTR: {
 +			struct nlattr *nla;
 +
 +			if (skb_is_nonlinear(skb))
 +				return 0;
 +
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
 +
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
 +
 +			nla = nla_find((struct nlattr *)&skb->data[A],
 +				       skb->len - A, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +		case BPF_S_ANC_NLATTR_NEST: {
 +			struct nlattr *nla;
  
 -			/* Other jumps are mapped into two insns: Jxx and JA. */
 -			target = i + fp->jt + 1;
 -			insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -			BPF_EMIT_JMP;
 -			insn++;
 -
 -			insn->code = BPF_JMP | BPF_JA;
 -			target = i + fp->jf + 1;
 -			BPF_EMIT_JMP;
 -			break;
 -
 -		/* ldxb 4 * ([14] & 0xf) is remaped into 6 insns. */
 -		case BPF_LDX | BPF_MSH | BPF_B:
 -			/* tmp = A */
 -			*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_A);
 -			/* A = BPF_R0 = *(u8 *) (skb->data + K) */
 -			*insn++ = BPF_LD_ABS(BPF_B, fp->k);
 -			/* A &= 0xf */
 -			*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 0xf);
 -			/* A <<= 2 */
 -			*insn++ = BPF_ALU32_IMM(BPF_LSH, BPF_REG_A, 2);
 -			/* X = A */
 -			*insn++ = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);
 -			/* A = tmp */
 -			*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_TMP);
 -			break;
 -
 -		/* RET_K, RET_A are remaped into 2 insns. */
 -		case BPF_RET | BPF_A:
 -		case BPF_RET | BPF_K:
 -			*insn++ = BPF_MOV32_RAW(BPF_RVAL(fp->code) == BPF_K ?
 -						BPF_K : BPF_X, BPF_REG_0,
 -						BPF_REG_A, fp->k);
 -			*insn = BPF_EXIT_INSN();
 -			break;
 -
 -		/* Store to stack. */
 -		case BPF_ST:
 -		case BPF_STX:
 -			*insn = BPF_STX_MEM(BPF_W, BPF_REG_FP, BPF_CLASS(fp->code) ==
 -					    BPF_ST ? BPF_REG_A : BPF_REG_X,
 -					    -(BPF_MEMWORDS - fp->k) * 4);
 -			break;
 -
 -		/* Load from stack. */
 -		case BPF_LD | BPF_MEM:
 -		case BPF_LDX | BPF_MEM:
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD  ?
 -					    BPF_REG_A : BPF_REG_X, BPF_REG_FP,
 -					    -(BPF_MEMWORDS - fp->k) * 4);
 -			break;
 -
 -		/* A = K or X = K */
 -		case BPF_LD | BPF_IMM:
 -		case BPF_LDX | BPF_IMM:
 -			*insn = BPF_MOV32_IMM(BPF_CLASS(fp->code) == BPF_LD ?
 -					      BPF_REG_A : BPF_REG_X, fp->k);
 -			break;
 -
 -		/* X = A */
 -		case BPF_MISC | BPF_TAX:
 -			*insn = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);
 -			break;
 +			if (skb_is_nonlinear(skb))
 +				return 0;
  
 -		/* A = X */
 -		case BPF_MISC | BPF_TXA:
 -			*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_X);
 -			break;
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
  
 -		/* A = skb->len or X = skb->len */
 -		case BPF_LD | BPF_W | BPF_LEN:
 -		case BPF_LDX | BPF_W | BPF_LEN:
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD ?
 -					    BPF_REG_A : BPF_REG_X, BPF_REG_CTX,
 -					    offsetof(struct sk_buff, len));
 -			break;
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
  
 -		/* Access seccomp_data fields. */
 -		case BPF_LDX | BPF_ABS | BPF_W:
 -			/* A = *(u32 *) (ctx + K) */
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX, fp->k);
 -			break;
 +			nla = (struct nlattr *)&skb->data[A];
 +			if (nla->nla_len > skb->len - A)
 +				return 0;
  
 -		/* Unkown instruction. */
 +			nla = nla_find_nested(nla, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +#ifdef CONFIG_SECCOMP_FILTER
 +		case BPF_S_ANC_SECCOMP_LD_W:
 +			A = seccomp_bpf_load(fentry->k);
 +			continue;
 +#endif
  		default:
 -			goto err;
 +			WARN_RATELIMIT(1, "Unknown code:%u jt:%u tf:%u k:%u\n",
 +				       fentry->code, fentry->jt,
 +				       fentry->jf, fentry->k);
 +			return 0;
  		}
 -
 -		insn++;
 -		if (new_prog)
 -			memcpy(new_insn, tmp_insns,
 -			       sizeof(*insn) * (insn - tmp_insns));
 -		new_insn += insn - tmp_insns;
 -	}
 -
 -	if (!new_prog) {
 -		/* Only calculating new length. */
 -		*new_len = new_insn - new_prog;
 -		return 0;
 -	}
 -
 -	pass++;
 -	if (new_flen != new_insn - new_prog) {
 -		new_flen = new_insn - new_prog;
 -		if (pass > 2)
 -			goto err;
 -		goto do_pass;
  	}
  
 -	kfree(addrs);
 -	BUG_ON(*new_len != new_flen);
  	return 0;
 -err:
 -	kfree(addrs);
 -	return -EINVAL;
  }
 +EXPORT_SYMBOL(sk_run_filter);
  
 -/* Security:
 - *
 +/*
 + * Security :
   * A BPF program is able to use 16 cells of memory to store intermediate
 - * values (check u32 mem[BPF_MEMWORDS] in sk_run_filter()).
 - *
 + * values (check u32 mem[BPF_MEMWORDS] in sk_run_filter())
   * As we dont want to clear mem[] array for each packet going through
   * sk_run_filter(), we check that filter loaded by user never try to read
   * a cell if not previously written, and we check all branches to be sure
* Unmerged path include/linux/filter.h
* Unmerged path net/core/filter.c
