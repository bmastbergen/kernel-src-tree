blk-mq: factor out a few helpers from __blk_mq_try_issue_directly

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Mike Snitzer <snitzer@redhat.com>
commit 0f95549c0ea1e8075ae049202088b2c6a0cb40ad
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/0f95549c.failed

No functional change.  Just makes code flow more logically.

In following commit, __blk_mq_try_issue_directly() will be used to
return the dispatch result (blk_status_t) to DM.  DM needs this
information to improve IO merging.

	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 0f95549c0ea1e8075ae049202088b2c6a0cb40ad)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 1eaa154c3ecb,ddc46f215bfa..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1529,19 -1730,66 +1529,75 @@@ static inline void blk_mq_queue_io(stru
  	spin_unlock(&ctx->lock);
  }
  
++<<<<<<< HEAD
 +static void __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 +					struct request *rq, bool may_sleep)
++=======
+ static blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx, struct request *rq)
+ {
+ 	if (rq->tag != -1)
+ 		return blk_tag_to_qc_t(rq->tag, hctx->queue_num, false);
+ 
+ 	return blk_tag_to_qc_t(rq->internal_tag, hctx->queue_num, true);
+ }
+ 
+ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
+ 					    struct request *rq,
+ 					    blk_qc_t *cookie)
++>>>>>>> 0f95549c0ea1 (blk-mq: factor out a few helpers from __blk_mq_try_issue_directly)
  {
  	struct request_queue *q = rq->q;
  	struct blk_mq_queue_data bd = {
  		.rq = rq,
 +		.list = NULL,
  		.last = true,
  	};
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	blk_qc_t new_cookie;
+ 	blk_status_t ret;
+ 
+ 	new_cookie = request_to_qc_t(hctx, rq);
+ 
+ 	/*
+ 	 * For OK queue, we are done. For error, caller may kill it.
+ 	 * Any other error (busy), just add it to our list as we
+ 	 * previously would have done.
+ 	 */
+ 	ret = q->mq_ops->queue_rq(hctx, &bd);
+ 	switch (ret) {
+ 	case BLK_STS_OK:
+ 		*cookie = new_cookie;
+ 		break;
+ 	case BLK_STS_RESOURCE:
+ 		__blk_mq_requeue_request(rq);
+ 		break;
+ 	default:
+ 		*cookie = BLK_QC_T_NONE;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void __blk_mq_fallback_to_insert(struct blk_mq_hw_ctx *hctx,
+ 					struct request *rq,
+ 					bool run_queue)
+ {
+ 	blk_mq_sched_insert_request(rq, false, run_queue, false,
+ 					hctx->flags & BLK_MQ_F_BLOCKING);
+ }
+ 
+ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
+ 						struct request *rq,
+ 						blk_qc_t *cookie)
+ {
+ 	struct request_queue *q = rq->q;
++>>>>>>> 0f95549c0ea1 (blk-mq: factor out a few helpers from __blk_mq_try_issue_directly)
  	bool run_queue = true;
  
 -	/* RCU or SRCU read lock is needed before checking quiesced flag */
 -	if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
 +	if (blk_mq_hctx_stopped(hctx)) {
  		run_queue = false;
  		goto insert;
  	}
@@@ -1557,48 -1805,36 +1613,73 @@@
  		goto insert;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * For OK queue, we are done. For error, kill it. Any other
 +	 * error (busy), just add it to our list as we previously
 +	 * would have done
 +	 */
 +	ret = q->mq_ops->queue_rq(hctx, &bd);
 +	if (ret == BLK_MQ_RQ_QUEUE_OK)
 +		return;
 +
 +	if (ret == BLK_MQ_RQ_QUEUE_ERROR) {
 +		rq->errors = -EIO;
 +		blk_mq_end_request(rq, rq->errors);
 +		return;
 +	}
 +
 +	__blk_mq_requeue_request(rq);
 +insert:
 +	blk_mq_sched_insert_request(rq, false, run_queue, false, may_sleep);
++=======
+ 	return __blk_mq_issue_directly(hctx, rq, cookie);
+ insert:
+ 	__blk_mq_fallback_to_insert(hctx, rq, run_queue);
+ 
+ 	return BLK_STS_OK;
++>>>>>>> 0f95549c0ea1 (blk-mq: factor out a few helpers from __blk_mq_try_issue_directly)
  }
  
  static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 -		struct request *rq, blk_qc_t *cookie)
 +				      struct request *rq)
  {
++<<<<<<< HEAD
 +	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 +		rcu_read_lock();
 +		__blk_mq_try_issue_directly(hctx, rq, false);
 +		rcu_read_unlock();
 +	} else {
 +		unsigned int srcu_idx;
++=======
+ 	blk_status_t ret;
+ 	int srcu_idx;
++>>>>>>> 0f95549c0ea1 (blk-mq: factor out a few helpers from __blk_mq_try_issue_directly)
  
 -	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 +		might_sleep();
  
++<<<<<<< HEAD
 +		srcu_idx = srcu_read_lock(&hctx->queue_rq_srcu);
 +		__blk_mq_try_issue_directly(hctx, rq, true);
 +		srcu_read_unlock(&hctx->queue_rq_srcu, srcu_idx);
 +	}
++=======
+ 	hctx_lock(hctx, &srcu_idx);
+ 
+ 	ret = __blk_mq_try_issue_directly(hctx, rq, cookie);
+ 	if (ret == BLK_STS_RESOURCE)
+ 		__blk_mq_fallback_to_insert(hctx, rq, true);
+ 	else if (ret != BLK_STS_OK)
+ 		blk_mq_end_request(rq, ret);
+ 
+ 	hctx_unlock(hctx, srcu_idx);
++>>>>>>> 0f95549c0ea1 (blk-mq: factor out a few helpers from __blk_mq_try_issue_directly)
  }
  
 -static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 +static void blk_mq_make_request(struct request_queue *q, struct bio *bio)
  {
 -	const int is_sync = op_is_sync(bio->bi_opf);
 -	const int is_flush_fua = op_is_flush(bio->bi_opf);
 +	const int is_sync = rw_is_sync(bio->bi_rw);
 +	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
  	struct blk_mq_alloc_data data = { .flags = 0 };
  	struct request *rq;
  	unsigned int request_count = 0;
* Unmerged path block/blk-mq.c
