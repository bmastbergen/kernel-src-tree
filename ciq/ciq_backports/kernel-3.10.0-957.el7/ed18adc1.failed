mm: SLUB hardened usercopy support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] SLUB hardened usercopy support (Christoph von Recklinghausen) [1562140]
Rebuild_FUZZ: 93.75%
commit-author Kees Cook <keescook@chromium.org>
commit ed18adc1cdd00a5c55a20fbdaed4804660772281
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/ed18adc1.failed

Under CONFIG_HARDENED_USERCOPY, this adds object size checking to the
SLUB allocator to catch any copies that may span objects. Includes a
redzone handling fix discovered by Michael Ellerman.

Based on code from PaX and grsecurity.

	Signed-off-by: Kees Cook <keescook@chromium.org>
	Tested-by: Michael Ellerman <mpe@ellerman.id.au>
Reviwed-by: Laura Abbott <labbott@redhat.com>
(cherry picked from commit ed18adc1cdd00a5c55a20fbdaed4804660772281)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 20b1a0c59b8d,256a8efd165e..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -3535,7 -3614,47 +3535,51 @@@ void *__kmalloc_node(size_t size, gfp_
  EXPORT_SYMBOL(__kmalloc_node);
  #endif
  
++<<<<<<< HEAD
 +size_t ksize(const void *object)
++=======
+ #ifdef CONFIG_HARDENED_USERCOPY
+ /*
+  * Rejects objects that are incorrectly sized.
+  *
+  * Returns NULL if check passes, otherwise const char * to name of cache
+  * to indicate an error.
+  */
+ const char *__check_heap_object(const void *ptr, unsigned long n,
+ 				struct page *page)
+ {
+ 	struct kmem_cache *s;
+ 	unsigned long offset;
+ 	size_t object_size;
+ 
+ 	/* Find object and usable object size. */
+ 	s = page->slab_cache;
+ 	object_size = slab_ksize(s);
+ 
+ 	/* Reject impossible pointers. */
+ 	if (ptr < page_address(page))
+ 		return s->name;
+ 
+ 	/* Find offset within object. */
+ 	offset = (ptr - page_address(page)) % s->size;
+ 
+ 	/* Adjust for redzone and reject if within the redzone. */
+ 	if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE) {
+ 		if (offset < s->red_left_pad)
+ 			return s->name;
+ 		offset -= s->red_left_pad;
+ 	}
+ 
+ 	/* Allow address range falling entirely within object size. */
+ 	if (offset <= object_size && n <= object_size - offset)
+ 		return NULL;
+ 
+ 	return s->name;
+ }
+ #endif /* CONFIG_HARDENED_USERCOPY */
+ 
+ static size_t __ksize(const void *object)
++>>>>>>> ed18adc1cdd0 (mm: SLUB hardened usercopy support)
  {
  	struct page *page;
  
diff --git a/init/Kconfig b/init/Kconfig
index 6fb20bde9404..a76c196cb1d1 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1576,6 +1576,7 @@ config SLAB
 
 config SLUB
 	bool "SLUB (Unqueued Allocator)"
+	select HAVE_HARDENED_USERCOPY_ALLOCATOR
 	help
 	   SLUB is a slab allocator that minimizes cache line usage
 	   instead of managing queues of cached objects (SLAB approach).
* Unmerged path mm/slub.c
