mmc: block: remove req back pointer

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mmc] block: remove req back pointer (Gopal Tiwari) [1456570]
Rebuild_FUZZ: 92.31%
commit-author Linus Walleij <linus.walleij@linaro.org>
commit 67e69d5220c904238f94bb2e6001d7c590f5a0bb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/67e69d52.failed

Just as we can use blk_mq_rq_from_pdu() to get the per-request
tag we can use blk_mq_rq_to_pdu() to get a request from a tag.
Introduce a static inline helper so we are on the clear what
is happening.

	Suggested-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
	Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
(cherry picked from commit 67e69d5220c904238f94bb2e6001d7c590f5a0bb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/mmc/core/block.c
#	drivers/mmc/core/queue.c
#	drivers/mmc/core/queue.h
diff --cc drivers/mmc/core/block.c
index 97e814713e97,553ab4d1db94..000000000000
--- a/drivers/mmc/core/block.c
+++ b/drivers/mmc/core/block.c
@@@ -1322,9 -1364,9 +1322,9 @@@ static enum mmc_blk_status mmc_blk_err_
  					     struct mmc_async_req *areq)
  {
  	struct mmc_queue_req *mq_mrq = container_of(areq, struct mmc_queue_req,
 -						    areq);
 +						    mmc_active);
  	struct mmc_blk_request *brq = &mq_mrq->brq;
- 	struct request *req = mq_mrq->req;
+ 	struct request *req = mmc_queue_req_to_req(mq_mrq);
  	int need_retune = card->host->need_retune;
  	bool ecc_err = false;
  	bool gen_err = false;
@@@ -1424,16 -1466,14 +1424,20 @@@
  	return MMC_BLK_SUCCESS;
  }
  
 -static void mmc_blk_data_prep(struct mmc_queue *mq, struct mmc_queue_req *mqrq,
 -			      int disable_multi, bool *do_rel_wr,
 -			      bool *do_data_tag)
 +static void mmc_blk_rw_rq_prep(struct mmc_queue_req *mqrq,
 +			       struct mmc_card *card,
 +			       int disable_multi,
 +			       struct mmc_queue *mq)
  {
 -	struct mmc_blk_data *md = mq->blkdata;
 -	struct mmc_card *card = md->queue.card;
 +	u32 readcmd, writecmd;
  	struct mmc_blk_request *brq = &mqrq->brq;
++<<<<<<< HEAD
 +	struct request *req = mqrq->req;
 +	struct mmc_blk_data *md = mq->blkdata;
 +	bool do_data_tag;
++=======
+ 	struct request *req = mmc_queue_req_to_req(mqrq);
++>>>>>>> 67e69d5220c9 (mmc: block: remove req back pointer)
  
  	/*
  	 * Reliable writes are used to implement Forced Unit Access and
@@@ -1484,6 -1529,68 +1488,71 @@@
  						brq->data.blocks);
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (*do_rel_wr)
+ 		mmc_apply_rel_rw(brq, card, req);
+ 
+ 	/*
+ 	 * Data tag is used only during writing meta data to speed
+ 	 * up write and any subsequent read of this meta data
+ 	 */
+ 	*do_data_tag = card->ext_csd.data_tag_unit_size &&
+ 		       (req->cmd_flags & REQ_META) &&
+ 		       (rq_data_dir(req) == WRITE) &&
+ 		       ((brq->data.blocks * brq->data.blksz) >=
+ 			card->ext_csd.data_tag_unit_size);
+ 
+ 	mmc_set_data_timeout(&brq->data, card);
+ 
+ 	brq->data.sg = mqrq->sg;
+ 	brq->data.sg_len = mmc_queue_map_sg(mq, mqrq);
+ 
+ 	/*
+ 	 * Adjust the sg list so it is the same size as the
+ 	 * request.
+ 	 */
+ 	if (brq->data.blocks != blk_rq_sectors(req)) {
+ 		int i, data_size = brq->data.blocks << 9;
+ 		struct scatterlist *sg;
+ 
+ 		for_each_sg(brq->data.sg, sg, brq->data.sg_len, i) {
+ 			data_size -= sg->length;
+ 			if (data_size <= 0) {
+ 				sg->length += data_size;
+ 				i++;
+ 				break;
+ 			}
+ 		}
+ 		brq->data.sg_len = i;
+ 	}
+ 
+ 	mqrq->areq.mrq = &brq->mrq;
+ 
+ 	mmc_queue_bounce_pre(mqrq);
+ }
+ 
+ static void mmc_blk_rw_rq_prep(struct mmc_queue_req *mqrq,
+ 			       struct mmc_card *card,
+ 			       int disable_multi,
+ 			       struct mmc_queue *mq)
+ {
+ 	u32 readcmd, writecmd;
+ 	struct mmc_blk_request *brq = &mqrq->brq;
+ 	struct request *req = mmc_queue_req_to_req(mqrq);
+ 	struct mmc_blk_data *md = mq->blkdata;
+ 	bool do_rel_wr, do_data_tag;
+ 
+ 	mmc_blk_data_prep(mq, mqrq, disable_multi, &do_rel_wr, &do_data_tag);
+ 
+ 	brq->mrq.cmd = &brq->cmd;
+ 
+ 	brq->cmd.arg = blk_rq_pos(req);
+ 	if (!mmc_card_blockaddr(card))
+ 		brq->cmd.arg <<= 9;
+ 	brq->cmd.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_ADTC;
+ 
++>>>>>>> 67e69d5220c9 (mmc: block: remove req back pointer)
  	if (brq->data.blocks > 1 || do_rel_wr) {
  		/* SPI multiblock writes terminate using a special
  		 * token, not a STOP_TRANSMISSION request.
@@@ -1663,10 -1758,10 +1732,15 @@@ static int mmc_blk_issue_rw_rq(struct m
  		 * An asynchronous request has been completed and we proceed
  		 * to handle the result of it.
  		 */
 -		mq_rq =	container_of(old_areq, struct mmc_queue_req, areq);
 +		mq_rq =	container_of(old_areq, struct mmc_queue_req, mmc_active);
  		brq = &mq_rq->brq;
++<<<<<<< HEAD
 +		req = mq_rq->req;
 +		type = rq_data_dir(req) == READ ? MMC_BLK_READ : MMC_BLK_WRITE;
++=======
+ 		old_req = mmc_queue_req_to_req(mq_rq);
+ 		type = rq_data_dir(old_req) == READ ? MMC_BLK_READ : MMC_BLK_WRITE;
++>>>>>>> 67e69d5220c9 (mmc: block: remove req back pointer)
  		mmc_queue_bounce_post(mq_rq);
  
  		switch (status) {
diff --cc drivers/mmc/core/queue.c
index b0ae9d688e28,ba689a2ffc51..000000000000
--- a/drivers/mmc/core/queue.c
+++ b/drivers/mmc/core/queue.c
@@@ -179,83 -147,80 +179,110 @@@ static void mmc_queue_setup_discard(str
  	if (card->pref_erase > max_discard)
  		q->limits.discard_granularity = 0;
  	if (mmc_can_secure_erase_trim(card))
 -		queue_flag_set_unlocked(QUEUE_FLAG_SECERASE, q);
 +		queue_flag_set_unlocked(QUEUE_FLAG_SECDISCARD, q);
  }
  
 -static unsigned int mmc_queue_calc_bouncesz(struct mmc_host *host)
 +#ifdef CONFIG_MMC_BLOCK_BOUNCE
 +static bool mmc_queue_alloc_bounce_bufs(struct mmc_queue *mq,
 +					unsigned int bouncesz)
  {
 -	unsigned int bouncesz = MMC_QUEUE_BOUNCESZ;
 +	int i;
 +
 +	for (i = 0; i < mq->qdepth; i++) {
 +		mq->mqrq[i].bounce_buf = kmalloc(bouncesz, GFP_KERNEL);
 +		if (!mq->mqrq[i].bounce_buf)
 +			goto out_err;
 +	}
  
 -	if (host->max_segs != 1 || (host->caps & MMC_CAP_NO_BOUNCE_BUFF))
 -		return 0;
 +	return true;
  
 -	if (bouncesz > host->max_req_size)
 -		bouncesz = host->max_req_size;
 -	if (bouncesz > host->max_seg_size)
 -		bouncesz = host->max_seg_size;
 -	if (bouncesz > host->max_blk_count * 512)
 -		bouncesz = host->max_blk_count * 512;
 +out_err:
 +	while (--i >= 0) {
 +		kfree(mq->mqrq[i].bounce_buf);
 +		mq->mqrq[i].bounce_buf = NULL;
 +	}
 +	pr_warn("%s: unable to allocate bounce buffers\n",
 +		mmc_card_name(mq->card));
 +	return false;
 +}
  
 -	if (bouncesz <= 512)
 -		return 0;
 +static int mmc_queue_alloc_bounce_sgs(struct mmc_queue *mq,
 +				      unsigned int bouncesz)
 +{
 +	int i, ret;
 +
++<<<<<<< HEAD
 +	for (i = 0; i < mq->qdepth; i++) {
 +		mq->mqrq[i].sg = mmc_alloc_sg(1, &ret);
 +		if (ret)
 +			return ret;
 +
 +		mq->mqrq[i].bounce_sg = mmc_alloc_sg(bouncesz / 512, &ret);
 +		if (ret)
 +			return ret;
 +	}
  
 -	return bouncesz;
 +	return 0;
  }
 +#endif
  
 -/**
 - * mmc_init_request() - initialize the MMC-specific per-request data
 - * @q: the request queue
 - * @req: the request
 - * @gfp: memory allocation policy
 - */
 -static int mmc_init_request(struct request_queue *q, struct request *req,
 -			    gfp_t gfp)
 +static int mmc_queue_alloc_sgs(struct mmc_queue *mq, int max_segs)
  {
 -	struct mmc_queue_req *mq_rq = req_to_mmc_queue_req(req);
 -	struct mmc_queue *mq = q->queuedata;
 -	struct mmc_card *card = mq->card;
 -	struct mmc_host *host = card->host;
 +	int i, ret;
  
 +	for (i = 0; i < mq->qdepth; i++) {
 +		mq->mqrq[i].sg = mmc_alloc_sg(max_segs, &ret);
 +		if (ret)
 +			return ret;
++=======
+ 	if (card->bouncesz) {
+ 		mq_rq->bounce_buf = kmalloc(card->bouncesz, gfp);
+ 		if (!mq_rq->bounce_buf)
+ 			return -ENOMEM;
+ 		if (card->bouncesz > 512) {
+ 			mq_rq->sg = mmc_alloc_sg(1, gfp);
+ 			if (!mq_rq->sg)
+ 				return -ENOMEM;
+ 			mq_rq->bounce_sg = mmc_alloc_sg(card->bouncesz / 512,
+ 							gfp);
+ 			if (!mq_rq->bounce_sg)
+ 				return -ENOMEM;
+ 		}
+ 	} else {
+ 		mq_rq->bounce_buf = NULL;
+ 		mq_rq->bounce_sg = NULL;
+ 		mq_rq->sg = mmc_alloc_sg(host->max_segs, gfp);
+ 		if (!mq_rq->sg)
+ 			return -ENOMEM;
++>>>>>>> 67e69d5220c9 (mmc: block: remove req back pointer)
  	}
  
  	return 0;
  }
  
 -static void mmc_exit_request(struct request_queue *q, struct request *req)
 +static void mmc_queue_req_free_bufs(struct mmc_queue_req *mqrq)
  {
 -	struct mmc_queue_req *mq_rq = req_to_mmc_queue_req(req);
 +	kfree(mqrq->bounce_sg);
 +	mqrq->bounce_sg = NULL;
  
 -	/* It is OK to kfree(NULL) so this will be smooth */
 -	kfree(mq_rq->bounce_sg);
 -	mq_rq->bounce_sg = NULL;
 +	kfree(mqrq->sg);
 +	mqrq->sg = NULL;
  
 -	kfree(mq_rq->bounce_buf);
 -	mq_rq->bounce_buf = NULL;
 +	kfree(mqrq->bounce_buf);
 +	mqrq->bounce_buf = NULL;
 +}
 +
++<<<<<<< HEAD
 +static void mmc_queue_reqs_free_bufs(struct mmc_queue *mq)
 +{
 +	int i;
  
 +	for (i = 0; i < mq->qdepth; i++)
 +		mmc_queue_req_free_bufs(&mq->mqrq[i]);
++=======
+ 	kfree(mq_rq->sg);
+ 	mq_rq->sg = NULL;
++>>>>>>> 67e69d5220c9 (mmc: block: remove req back pointer)
  }
  
  /**
diff --cc drivers/mmc/core/queue.h
index a61f88199573,2793020a3c8c..000000000000
--- a/drivers/mmc/core/queue.h
+++ b/drivers/mmc/core/queue.h
@@@ -1,11 -1,27 +1,21 @@@
  #ifndef MMC_QUEUE_H
  #define MMC_QUEUE_H
  
 -#include <linux/types.h>
 -#include <linux/blkdev.h>
 -#include <linux/blk-mq.h>
 -#include <linux/mmc/core.h>
 -#include <linux/mmc/host.h>
 -
 -static inline struct mmc_queue_req *req_to_mmc_queue_req(struct request *rq)
 -{
 -	return blk_mq_rq_to_pdu(rq);
 -}
 +#define MMC_REQ_SPECIAL_MASK	(REQ_DISCARD | REQ_FLUSH)
  
++<<<<<<< HEAD
 +struct request;
++=======
+ struct mmc_queue_req;
+ 
+ static inline struct request *mmc_queue_req_to_req(struct mmc_queue_req *mqr)
+ {
+ 	return blk_mq_rq_from_pdu(mqr);
+ }
+ 
++>>>>>>> 67e69d5220c9 (mmc: block: remove req back pointer)
  struct task_struct;
  struct mmc_blk_data;
 -struct mmc_blk_ioc_data;
  
  struct mmc_blk_request {
  	struct mmc_request	mrq;
* Unmerged path drivers/mmc/core/block.c
* Unmerged path drivers/mmc/core/queue.c
* Unmerged path drivers/mmc/core/queue.h
