mm: initialize pages on demand during boot

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] initialize pages on demand during boot (Masayoshi Mizuma) [1496330]
Rebuild_FUZZ: 95.00%
commit-author Pavel Tatashin <pasha.tatashin@oracle.com>
commit c9e97a1997fbf3a1d18d4065c2ca381f0704d7e5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/c9e97a19.failed

Deferred page initialization allows the boot cpu to initialize a small
subset of the system's pages early in boot, with other cpus doing the
rest later on.

It is, however, problematic to know how many pages the kernel needs
during boot.  Different modules and kernel parameters may change the
requirement, so the boot cpu either initializes too many pages or runs
out of memory.

To fix that, initialize early pages on demand.  This ensures the kernel
does the minimum amount of work to initialize pages during boot and
leaves the rest to be divided in the multithreaded initialization path
(deferred_init_memmap).

The on-demand code is permanently disabled using static branching once
deferred pages are initialized.  After the static branch is changed to
false, the overhead is up-to two branch-always instructions if the zone
watermark check fails or if rmqueue fails.

Sergey Senozhatsky noticed that while deferred pages currently make
sense only on NUMA machines (we start one thread per latency node),
CONFIG_NUMA is not a requirement for CONFIG_DEFERRED_STRUCT_PAGE_INIT,
so that is also must be addressed in the patch.

[akpm@linux-foundation.org: fix typo in comment, make deferred_pages static]
[pasha.tatashin@oracle.com: fix min() type mismatch warning]
  Link: http://lkml.kernel.org/r/20180212164543.26592-1-pasha.tatashin@oracle.com
[pasha.tatashin@oracle.com: use zone_to_nid() in deferred_grow_zone()]
  Link: http://lkml.kernel.org/r/20180214163343.21234-2-pasha.tatashin@oracle.com
[pasha.tatashin@oracle.com: might_sleep warning]
  Link: http://lkml.kernel.org/r/20180306192022.28289-1-pasha.tatashin@oracle.com
[akpm@linux-foundation.org: s/spin_lock/spin_lock_irq/ in page_alloc_init_late()]
[pasha.tatashin@oracle.com: v5]
  Link: http://lkml.kernel.org/r/20180309220807.24961-3-pasha.tatashin@oracle.com
[akpm@linux-foundation.org: tweak comments]
[pasha.tatashin@oracle.com: v6]
  Link: http://lkml.kernel.org/r/20180313182355.17669-3-pasha.tatashin@oracle.com
[akpm@linux-foundation.org: coding-style fixes]
Link: http://lkml.kernel.org/r/20180209192216.20509-2-pasha.tatashin@oracle.com
	Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
	Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
	Reviewed-by: Steven Sistare <steven.sistare@oracle.com>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Tested-by: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
	Acked-by: Mel Gorman <mgorman@suse.de>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
	Cc: Gioh Kim <gi-oh.kim@profitbricks.com>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
	Cc: Wei Yang <richard.weiyang@gmail.com>
	Cc: Paul Burton <paul.burton@mips.com>
	Cc: Miles Chen <miles.chen@mediatek.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c9e97a1997fbf3a1d18d4065c2ca381f0704d7e5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memblock.h
#	mm/page_alloc.c
diff --cc include/linux/memblock.h
index 5a439c937c3c,0257aee7ab4b..000000000000
--- a/include/linux/memblock.h
+++ b/include/linux/memblock.h
@@@ -384,21 -404,23 +384,27 @@@ static inline unsigned long memblock_re
  	     region < (memblock.memblock_type.regions + memblock.memblock_type.cnt);	\
  	     region++)
  
 -#define for_each_memblock_type(i, memblock_type, rgn)			\
 -	for (i = 0, rgn = &memblock_type->regions[0];			\
 -	     i < memblock_type->cnt;					\
 -	     i++, rgn = &memblock_type->regions[i])
  
 -#ifdef CONFIG_MEMTEST
 -extern void early_memtest(phys_addr_t start, phys_addr_t end);
 +#ifdef CONFIG_ARCH_DISCARD_MEMBLOCK
 +#define __init_memblock __meminit
 +#define __initdata_memblock __meminitdata
  #else
 -static inline void early_memtest(phys_addr_t start, phys_addr_t end)
 -{
 -}
 +#define __init_memblock
 +#define __initdata_memblock
  #endif
++<<<<<<< HEAD
 +
++=======
++>>>>>>> c9e97a1997fb (mm: initialize pages on demand during boot)
  #else
  static inline phys_addr_t memblock_alloc(phys_addr_t size, phys_addr_t align)
  {
  	return 0;
  }
++<<<<<<< HEAD
 +
++=======
++>>>>>>> c9e97a1997fb (mm: initialize pages on demand during boot)
  #endif /* CONFIG_HAVE_MEMBLOCK */
  
  #endif /* __KERNEL__ */
diff --cc mm/page_alloc.c
index 0f8b31880f9c,3183eb2f579c..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -242,11 -292,6 +242,14 @@@ EXPORT_SYMBOL(nr_online_nodes)
  int page_group_by_mobility_disabled __read_mostly;
  
  #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
++<<<<<<< HEAD
 +static inline void reset_deferred_meminit(pg_data_t *pgdat)
 +{
 +	pgdat->first_deferred_pfn = ULONG_MAX;
 +}
 +
++=======
++>>>>>>> c9e97a1997fb (mm: initialize pages on demand during boot)
  /* Returns true if the struct page for the pfn is uninitialised */
  static inline bool __meminit early_page_uninitialised(unsigned long pfn)
  {
@@@ -1236,6 -1573,118 +1235,121 @@@ free_range
  	pgdat_init_report_one_done();
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * During boot we initialize deferred pages on-demand, as needed, but once
+  * page_alloc_init_late() has finished, the deferred pages are all initialized,
+  * and we can permanently disable that path.
+  */
+ static DEFINE_STATIC_KEY_TRUE(deferred_pages);
+ 
+ /*
+  * If this zone has deferred pages, try to grow it by initializing enough
+  * deferred pages to satisfy the allocation specified by order, rounded up to
+  * the nearest PAGES_PER_SECTION boundary.  So we're adding memory in increments
+  * of SECTION_SIZE bytes by initializing struct pages in increments of
+  * PAGES_PER_SECTION * sizeof(struct page) bytes.
+  *
+  * Return true when zone was grown, otherwise return false. We return true even
+  * when we grow less than requested, to let the caller decide if there are
+  * enough pages to satisfy the allocation.
+  *
+  * Note: We use noinline because this function is needed only during boot, and
+  * it is called from a __ref function _deferred_grow_zone. This way we are
+  * making sure that it is not inlined into permanent text section.
+  */
+ static noinline bool __init
+ deferred_grow_zone(struct zone *zone, unsigned int order)
+ {
+ 	int zid = zone_idx(zone);
+ 	int nid = zone_to_nid(zone);
+ 	pg_data_t *pgdat = NODE_DATA(nid);
+ 	unsigned long nr_pages_needed = ALIGN(1 << order, PAGES_PER_SECTION);
+ 	unsigned long nr_pages = 0;
+ 	unsigned long first_init_pfn, spfn, epfn, t, flags;
+ 	unsigned long first_deferred_pfn = pgdat->first_deferred_pfn;
+ 	phys_addr_t spa, epa;
+ 	u64 i;
+ 
+ 	/* Only the last zone may have deferred pages */
+ 	if (zone_end_pfn(zone) != pgdat_end_pfn(pgdat))
+ 		return false;
+ 
+ 	pgdat_resize_lock(pgdat, &flags);
+ 
+ 	/*
+ 	 * If deferred pages have been initialized while we were waiting for
+ 	 * the lock, return true, as the zone was grown.  The caller will retry
+ 	 * this zone.  We won't return to this function since the caller also
+ 	 * has this static branch.
+ 	 */
+ 	if (!static_branch_unlikely(&deferred_pages)) {
+ 		pgdat_resize_unlock(pgdat, &flags);
+ 		return true;
+ 	}
+ 
+ 	/*
+ 	 * If someone grew this zone while we were waiting for spinlock, return
+ 	 * true, as there might be enough pages already.
+ 	 */
+ 	if (first_deferred_pfn != pgdat->first_deferred_pfn) {
+ 		pgdat_resize_unlock(pgdat, &flags);
+ 		return true;
+ 	}
+ 
+ 	first_init_pfn = max(zone->zone_start_pfn, first_deferred_pfn);
+ 
+ 	if (first_init_pfn >= pgdat_end_pfn(pgdat)) {
+ 		pgdat_resize_unlock(pgdat, &flags);
+ 		return false;
+ 	}
+ 
+ 	for_each_free_mem_range(i, nid, MEMBLOCK_NONE, &spa, &epa, NULL) {
+ 		spfn = max_t(unsigned long, first_init_pfn, PFN_UP(spa));
+ 		epfn = min_t(unsigned long, zone_end_pfn(zone), PFN_DOWN(epa));
+ 
+ 		while (spfn < epfn && nr_pages < nr_pages_needed) {
+ 			t = ALIGN(spfn + PAGES_PER_SECTION, PAGES_PER_SECTION);
+ 			first_deferred_pfn = min(t, epfn);
+ 			nr_pages += deferred_init_pages(nid, zid, spfn,
+ 							first_deferred_pfn);
+ 			spfn = first_deferred_pfn;
+ 		}
+ 
+ 		if (nr_pages >= nr_pages_needed)
+ 			break;
+ 	}
+ 
+ 	for_each_free_mem_range(i, nid, MEMBLOCK_NONE, &spa, &epa, NULL) {
+ 		spfn = max_t(unsigned long, first_init_pfn, PFN_UP(spa));
+ 		epfn = min_t(unsigned long, first_deferred_pfn, PFN_DOWN(epa));
+ 		deferred_free_pages(nid, zid, spfn, epfn);
+ 
+ 		if (first_deferred_pfn == epfn)
+ 			break;
+ 	}
+ 	pgdat->first_deferred_pfn = first_deferred_pfn;
+ 	pgdat_resize_unlock(pgdat, &flags);
+ 
+ 	return nr_pages > 0;
+ }
+ 
+ /*
+  * deferred_grow_zone() is __init, but it is called from
+  * get_page_from_freelist() during early boot until deferred_pages permanently
+  * disables this call. This is why we have refdata wrapper to avoid warning,
+  * and to ensure that the function body gets unloaded.
+  */
+ static bool __ref
+ _deferred_grow_zone(struct zone *zone, unsigned int order)
+ {
+ 	return deferred_grow_zone(zone, order);
+ }
+ 
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
++>>>>>>> c9e97a1997fb (mm: initialize pages on demand during boot)
  
  void __init page_alloc_init_late(void)
  {
@@@ -1250,13 -1702,26 +1364,19 @@@
  	/* Block until all are initialised */
  	wait_for_completion(&pgdat_init_all_done_comp);
  
+ 	/*
+ 	 * We initialized the rest of the deferred pages.  Permanently disable
+ 	 * on-demand struct page initialization.
+ 	 */
+ 	static_branch_disable(&deferred_pages);
+ 
  	/* Reinit limits that are based on free pages after the kernel is up */
  	files_maxfiles_init();
 -#endif
 -#ifdef CONFIG_ARCH_DISCARD_MEMBLOCK
 -	/* Discard memblock private memory */
 -	memblock_discard();
 -#endif
 -
 -	for_each_populated_zone(zone)
 -		set_zone_contiguous(zone);
  }
 +#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
  
  #ifdef CONFIG_CMA
 -/* Free whole pageblock and set its migration type to MIGRATE_CMA. */
 +/* Free whole pageblock and set it's migration type to MIGRATE_CMA. */
  void __init init_cma_reserved_pageblock(struct page *page)
  {
  	unsigned i = pageblock_nr_pages;
@@@ -2419,66 -3263,55 +2539,82 @@@ zonelist_scan
  		 * should be able to balance it without having to
  		 * write pages from its LRU list.
  		 *
 +		 * This may look like it could increase pressure on
 +		 * lower zones by failing allocations in higher zones
 +		 * before they are full.  But the pages that do spill
 +		 * over are limited as the lower zones are protected
 +		 * by this very same mechanism.  It should not become
 +		 * a practical burden to them.
 +		 *
  		 * XXX: For now, allow allocations to potentially
 -		 * exceed the per-node dirty limit in the slowpath
 -		 * (spread_dirty_pages unset) before going into reclaim,
 +		 * exceed the per-zone dirty limit in the slowpath
 +		 * (ALLOC_WMARK_LOW unset) before going into reclaim,
  		 * which is important when on a NUMA setup the allowed
 -		 * nodes are together not big enough to reach the
 +		 * zones are together not big enough to reach the
  		 * global limit.  The proper fix for these situations
 -		 * will require awareness of nodes in the
 +		 * will require awareness of zones in the
  		 * dirty-throttling and the flusher threads.
  		 */
 -		if (ac->spread_dirty_pages) {
 -			if (last_pgdat_dirty_limit == zone->zone_pgdat)
 -				continue;
 -
 -			if (!node_dirty_ok(zone->zone_pgdat)) {
 -				last_pgdat_dirty_limit = zone->zone_pgdat;
 -				continue;
 -			}
 -		}
 +		if ((alloc_flags & ALLOC_WMARK_LOW) &&
 +		    (gfp_mask & __GFP_WRITE) && !zone_dirty_ok(zone))
 +			goto this_zone_full;
  
 -		mark = zone->watermark[alloc_flags & ALLOC_WMARK_MASK];
 -		if (!zone_watermark_fast(zone, order, mark,
 -				       ac_classzone_idx(ac), alloc_flags)) {
 +		BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
 +		if (!(alloc_flags & ALLOC_NO_WATERMARKS)) {
 +			unsigned long mark;
  			int ret;
  
++<<<<<<< HEAD
 +			mark = zone->watermark[alloc_flags & ALLOC_WMARK_MASK];
 +			if (zone_watermark_ok(zone, order, mark,
 +				    classzone_idx, alloc_flags))
++=======
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ 			/*
+ 			 * Watermark failed for this zone, but see if we can
+ 			 * grow this zone if it contains deferred pages.
+ 			 */
+ 			if (static_branch_unlikely(&deferred_pages)) {
+ 				if (_deferred_grow_zone(zone, order))
+ 					goto try_this_zone;
+ 			}
+ #endif
+ 			/* Checked here to keep the fast path fast */
+ 			BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
+ 			if (alloc_flags & ALLOC_NO_WATERMARKS)
++>>>>>>> c9e97a1997fb (mm: initialize pages on demand during boot)
  				goto try_this_zone;
  
 -			if (node_reclaim_mode == 0 ||
 -			    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))
 +			if (IS_ENABLED(CONFIG_NUMA) &&
 +					!did_zlc_setup && nr_online_nodes > 1) {
 +				/*
 +				 * we do zlc_setup if there are multiple nodes
 +				 * and before considering the first zone allowed
 +				 * by the cpuset.
 +				 */
 +				allowednodes = zlc_setup(zonelist, alloc_flags);
 +				zlc_active = 1;
 +				did_zlc_setup = 1;
 +			}
 +
 +			if (zone_reclaim_mode == 0 ||
 +			    !zone_allows_reclaim(preferred_zone, zone))
 +				goto this_zone_full;
 +
 +			/*
 +			 * As we may have just activated ZLC, check if the first
 +			 * eligible zone has failed zone_reclaim recently.
 +			 */
 +			if (IS_ENABLED(CONFIG_NUMA) && zlc_active &&
 +				!zlc_zone_worth_trying(zonelist, z, allowednodes))
  				continue;
  
 -			ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);
 +			ret = zone_reclaim(zone, gfp_mask, order);
  			switch (ret) {
 -			case NODE_RECLAIM_NOSCAN:
 +			case ZONE_RECLAIM_NOSCAN:
  				/* did not scan */
  				continue;
 -			case NODE_RECLAIM_FULL:
 +			case ZONE_RECLAIM_FULL:
  				/* scanned but unreclaimable */
  				continue;
  			default:
@@@ -2505,36 -3325,31 +2641,61 @@@
  		}
  
  try_this_zone:
++<<<<<<< HEAD
 +		page = buffered_rmqueue(preferred_zone, zone, order,
 +						gfp_mask, migratetype);
 +		if (page)
 +			break;
 +this_zone_full:
 +		if (IS_ENABLED(CONFIG_NUMA))
 +			zlc_mark_zone_full(zonelist, z);
++=======
+ 		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
+ 				gfp_mask, alloc_flags, ac->migratetype);
+ 		if (page) {
+ 			prep_new_page(page, order, gfp_mask, alloc_flags);
+ 
+ 			/*
+ 			 * If this is a high-order atomic allocation then check
+ 			 * if the pageblock should be reserved for the future
+ 			 */
+ 			if (unlikely(order && (alloc_flags & ALLOC_HARDER)))
+ 				reserve_highatomic_pageblock(page, zone, order);
+ 
+ 			return page;
+ 		} else {
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ 			/* Try again if zone has deferred pages */
+ 			if (static_branch_unlikely(&deferred_pages)) {
+ 				if (_deferred_grow_zone(zone, order))
+ 					goto try_this_zone;
+ 			}
+ #endif
+ 		}
++>>>>>>> c9e97a1997fb (mm: initialize pages on demand during boot)
  	}
  
 -	return NULL;
 +	if (unlikely(IS_ENABLED(CONFIG_NUMA) && page == NULL && zlc_active)) {
 +		/* Disable zlc cache for second zonelist scan */
 +		zlc_active = 0;
 +		goto zonelist_scan;
 +	}
 +
 +	if (page) {
 +		/*
 +		 * page is set pfmemalloc when ALLOC_NO_WATERMARKS was
 +		 * necessary to allocate the page. The expectation is
 +		 * that the caller is taking steps that will free more
 +		 * memory. The caller should avoid the page being used
 +		 * for !PFMEMALLOC purposes.
 +		 */
 +		if (alloc_flags & ALLOC_NO_WATERMARKS)
 +			set_page_pfmemalloc(page);
 +		else
 +			clear_page_pfmemalloc(page);
 +	}
 +
 +	return page;
  }
  
  /*
@@@ -5705,15 -6340,59 +5866,28 @@@ void __init_refok free_area_init_node(i
  				  zones_size, zholes_size);
  
  	alloc_node_mem_map(pgdat);
 +#ifdef CONFIG_FLAT_NODE_MEM_MAP
 +	printk(KERN_DEBUG "free_area_init_node: node %d, pgdat %08lx, node_mem_map %08lx\n",
 +		nid, (unsigned long)pgdat,
 +		(unsigned long)pgdat->node_mem_map);
 +#endif
  
++<<<<<<< HEAD
 +	free_area_init_core(pgdat, start_pfn, end_pfn);
++=======
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ 	/*
+ 	 * We start only with one section of pages, more pages are added as
+ 	 * needed until the rest of deferred pages are initialized.
+ 	 */
+ 	pgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,
+ 					 pgdat->node_spanned_pages);
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ #endif
+ 	free_area_init_core(pgdat);
++>>>>>>> c9e97a1997fb (mm: initialize pages on demand during boot)
  }
  
 -#ifdef CONFIG_HAVE_MEMBLOCK
 -/*
 - * Only struct pages that are backed by physical memory are zeroed and
 - * initialized by going through __init_single_page(). But, there are some
 - * struct pages which are reserved in memblock allocator and their fields
 - * may be accessed (for example page_to_pfn() on some configuration accesses
 - * flags). We must explicitly zero those struct pages.
 - */
 -void __paginginit zero_resv_unavail(void)
 -{
 -	phys_addr_t start, end;
 -	unsigned long pfn;
 -	u64 i, pgcnt;
 -
 -	/*
 -	 * Loop through ranges that are reserved, but do not have reported
 -	 * physical memory backing.
 -	 */
 -	pgcnt = 0;
 -	for_each_resv_unavail_range(i, &start, &end) {
 -		for (pfn = PFN_DOWN(start); pfn < PFN_UP(end); pfn++) {
 -			if (!pfn_valid(ALIGN_DOWN(pfn, pageblock_nr_pages)))
 -				continue;
 -			mm_zero_struct_page(pfn_to_page(pfn));
 -			pgcnt++;
 -		}
 -	}
 -
 -	/*
 -	 * Struct pages that do not have backing memory. This could be because
 -	 * firmware is using some of this memory, or for some other reasons.
 -	 * Once memblock is changed so such behaviour is not allowed: i.e.
 -	 * list of "reserved" memory must be a subset of list of "memory", then
 -	 * this code can be removed.
 -	 */
 -	if (pgcnt)
 -		pr_info("Reserved but unavailable: %lld pages", pgcnt);
 -}
 -#endif /* CONFIG_HAVE_MEMBLOCK */
 -
  #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
  
  #if MAX_NUMNODES > 1
* Unmerged path include/linux/memblock.h
* Unmerged path mm/page_alloc.c
