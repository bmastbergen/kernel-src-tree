tcmu: remove commands_lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Mike Christie <mchristi@redhat.com>
commit 6fddcb775477bb2213bd76ab62145645eb570f33
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6fddcb77.failed

No need for the commands_lock. The cmdr_lock is already held during
idr addition and deletion, so just grab it during traversal.

Note: This also fixes a issue where we should have been using at
least _bh locking in tcmu_handle_completions when taking the commands
lock to prevent the case where tcmu_handle_completions could be
interrupted by a timer softirq while the commands_lock is held.

	Signed-off-by: Mike Christie <mchristi@redhat.com>
	Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>
(cherry picked from commit 6fddcb775477bb2213bd76ab62145645eb570f33)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/target/target_core_user.c
diff --cc drivers/target/target_core_user.c
index 8fa83807dcdc,43583a792439..000000000000
--- a/drivers/target/target_core_user.c
+++ b/drivers/target/target_core_user.c
@@@ -111,17 -128,17 +111,16 @@@ struct tcmu_dev 
  	/* Must add data_off and mb_addr to get the address */
  	size_t data_off;
  	size_t data_size;
 +	uint32_t max_blocks;
 +	size_t ring_size;
  
 -	wait_queue_head_t wait_cmdr;
 -	struct mutex cmdr_lock;
 +	unsigned long *data_bitmap;
  
 -	bool waiting_global;
 -	uint32_t dbi_max;
 -	uint32_t dbi_thresh;
 -	DECLARE_BITMAP(data_bitmap, DATA_BLOCK_BITS);
 -	struct radix_tree_root data_blocks;
 +	wait_queue_head_t wait_cmdr;
 +	/* TODO should this be a mutex? */
 +	spinlock_t cmdr_lock;
  
  	struct idr commands;
- 	spinlock_t commands_lock;
  
  	struct timer_list timeout;
  	unsigned int cmd_time_out;
@@@ -849,12 -1013,7 +848,16 @@@ static unsigned int tcmu_handle_complet
  		}
  		WARN_ON(tcmu_hdr_get_op(entry->hdr.len_op) != TCMU_OP_CMD);
  
++<<<<<<< HEAD
 +		spin_lock(&udev->commands_lock);
 +		cmd = idr_find(&udev->commands, entry->hdr.cmd_id);
 +		if (cmd)
 +			idr_remove(&udev->commands, cmd->cmd_id);
 +		spin_unlock(&udev->commands_lock);
 +
++=======
+ 		cmd = idr_remove(&udev->commands, entry->hdr.cmd_id);
++>>>>>>> 6fddcb775477 (tcmu: remove commands_lock)
  		if (!cmd) {
  			pr_err("cmd_id not found, ring is broken\n");
  			set_bit(TCMU_DEV_BIT_BROKEN, &udev->flags);
@@@ -953,18 -1105,14 +956,17 @@@ static struct se_device *tcmu_alloc_dev
  
  	udev->hba = hba;
  	udev->cmd_time_out = TCMU_TIME_OUT;
 +	udev->qfull_time_out = -1;
 +
 +	udev->max_blocks = DATA_BLOCK_BITS_DEF;
  
  	init_waitqueue_head(&udev->wait_cmdr);
 -	mutex_init(&udev->cmdr_lock);
 +	spin_lock_init(&udev->cmdr_lock);
  
 -	INIT_LIST_HEAD(&udev->timedout_entry);
  	idr_init(&udev->commands);
- 	spin_lock_init(&udev->commands_lock);
  
 -	timer_setup(&udev->timeout, tcmu_device_timedout, 0);
 +	setup_timer(&udev->timeout, tcmu_device_timedout,
 +		(unsigned long)udev);
  
  	init_waitqueue_head(&udev->nl_cmd_wq);
  	spin_lock_init(&udev->nl_cmd_lock);
@@@ -1059,6 -1277,71 +1061,74 @@@ static int tcmu_open(struct uio_info *i
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static void tcmu_dev_call_rcu(struct rcu_head *p)
+ {
+ 	struct se_device *dev = container_of(p, struct se_device, rcu_head);
+ 	struct tcmu_dev *udev = TCMU_DEV(dev);
+ 
+ 	kfree(udev->uio_info.name);
+ 	kfree(udev->name);
+ 	kfree(udev);
+ }
+ 
+ static int tcmu_check_and_free_pending_cmd(struct tcmu_cmd *cmd)
+ {
+ 	if (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags)) {
+ 		kmem_cache_free(tcmu_cmd_cache, cmd);
+ 		return 0;
+ 	}
+ 	return -EINVAL;
+ }
+ 
+ static void tcmu_blocks_release(struct radix_tree_root *blocks,
+ 				int start, int end)
+ {
+ 	int i;
+ 	struct page *page;
+ 
+ 	for (i = start; i < end; i++) {
+ 		page = radix_tree_delete(blocks, i);
+ 		if (page) {
+ 			__free_page(page);
+ 			atomic_dec(&global_db_count);
+ 		}
+ 	}
+ }
+ 
+ static void tcmu_dev_kref_release(struct kref *kref)
+ {
+ 	struct tcmu_dev *udev = container_of(kref, struct tcmu_dev, kref);
+ 	struct se_device *dev = &udev->se_dev;
+ 	struct tcmu_cmd *cmd;
+ 	bool all_expired = true;
+ 	int i;
+ 
+ 	vfree(udev->mb_addr);
+ 	udev->mb_addr = NULL;
+ 
+ 	spin_lock_bh(&timed_out_udevs_lock);
+ 	if (!list_empty(&udev->timedout_entry))
+ 		list_del(&udev->timedout_entry);
+ 	spin_unlock_bh(&timed_out_udevs_lock);
+ 
+ 	/* Upper layer should drain all requests before calling this */
+ 	mutex_lock(&udev->cmdr_lock);
+ 	idr_for_each_entry(&udev->commands, cmd, i) {
+ 		if (tcmu_check_and_free_pending_cmd(cmd) != 0)
+ 			all_expired = false;
+ 	}
+ 	idr_destroy(&udev->commands);
+ 	WARN_ON(!all_expired);
+ 
+ 	tcmu_blocks_release(&udev->data_blocks, 0, udev->dbi_max + 1);
+ 	mutex_unlock(&udev->cmdr_lock);
+ 
+ 	call_rcu(&dev->rcu_head, tcmu_dev_call_rcu);
+ }
+ 
++>>>>>>> 6fddcb775477 (tcmu: remove commands_lock)
  static int tcmu_release(struct uio_info *info, struct inode *inode)
  {
  	struct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);
@@@ -1654,6 -1970,106 +1724,109 @@@ static struct target_backend_ops tcmu_o
  	.tb_dev_attrib_attrs	= NULL,
  };
  
++<<<<<<< HEAD
++=======
+ 
+ static void find_free_blocks(void)
+ {
+ 	struct tcmu_dev *udev;
+ 	loff_t off;
+ 	uint32_t start, end, block;
+ 
+ 	mutex_lock(&root_udev_mutex);
+ 	list_for_each_entry(udev, &root_udev, node) {
+ 		mutex_lock(&udev->cmdr_lock);
+ 
+ 		/* Try to complete the finished commands first */
+ 		tcmu_handle_completions(udev);
+ 
+ 		/* Skip the udevs waiting the global pool or in idle */
+ 		if (udev->waiting_global || !udev->dbi_thresh) {
+ 			mutex_unlock(&udev->cmdr_lock);
+ 			continue;
+ 		}
+ 
+ 		end = udev->dbi_max + 1;
+ 		block = find_last_bit(udev->data_bitmap, end);
+ 		if (block == udev->dbi_max) {
+ 			/*
+ 			 * The last bit is dbi_max, so there is
+ 			 * no need to shrink any blocks.
+ 			 */
+ 			mutex_unlock(&udev->cmdr_lock);
+ 			continue;
+ 		} else if (block == end) {
+ 			/* The current udev will goto idle state */
+ 			udev->dbi_thresh = start = 0;
+ 			udev->dbi_max = 0;
+ 		} else {
+ 			udev->dbi_thresh = start = block + 1;
+ 			udev->dbi_max = block;
+ 		}
+ 
+ 		/* Here will truncate the data area from off */
+ 		off = udev->data_off + start * DATA_BLOCK_SIZE;
+ 		unmap_mapping_range(udev->inode->i_mapping, off, 0, 1);
+ 
+ 		/* Release the block pages */
+ 		tcmu_blocks_release(&udev->data_blocks, start, end);
+ 		mutex_unlock(&udev->cmdr_lock);
+ 	}
+ 	mutex_unlock(&root_udev_mutex);
+ }
+ 
+ static void run_cmdr_queues(void)
+ {
+ 	struct tcmu_dev *udev;
+ 
+ 	/*
+ 	 * Try to wake up the udevs who are waiting
+ 	 * for the global data block pool.
+ 	 */
+ 	mutex_lock(&root_udev_mutex);
+ 	list_for_each_entry(udev, &root_udev, node) {
+ 		mutex_lock(&udev->cmdr_lock);
+ 		if (!udev->waiting_global) {
+ 			mutex_unlock(&udev->cmdr_lock);
+ 			break;
+ 		}
+ 		mutex_unlock(&udev->cmdr_lock);
+ 
+ 		wake_up(&udev->wait_cmdr);
+ 	}
+ 	mutex_unlock(&root_udev_mutex);
+ }
+ 
+ static void check_timedout_devices(void)
+ {
+ 	struct tcmu_dev *udev, *tmp_dev;
+ 	LIST_HEAD(devs);
+ 
+ 	spin_lock_bh(&timed_out_udevs_lock);
+ 	list_splice_init(&timed_out_udevs, &devs);
+ 
+ 	list_for_each_entry_safe(udev, tmp_dev, &devs, timedout_entry) {
+ 		list_del_init(&udev->timedout_entry);
+ 		spin_unlock_bh(&timed_out_udevs_lock);
+ 
+ 		mutex_lock(&udev->cmdr_lock);
+ 		idr_for_each(&udev->commands, tcmu_check_expired_cmd, NULL);
+ 		mutex_unlock(&udev->cmdr_lock);
+ 
+ 		spin_lock_bh(&timed_out_udevs_lock);
+ 	}
+ 
+ 	spin_unlock_bh(&timed_out_udevs_lock);
+ }
+ 
+ static void tcmu_unmap_work_fn(struct work_struct *work)
+ {
+ 	check_timedout_devices();
+ 	find_free_blocks();
+ 	run_cmdr_queues();
+ }
+ 
++>>>>>>> 6fddcb775477 (tcmu: remove commands_lock)
  static int __init tcmu_module_init(void)
  {
  	int ret, i, k, len = 0;
* Unmerged path drivers/target/target_core_user.c
