x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] l1tf: protect prot_none ptes against speculation (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 85.71%
commit-author Andi Kleen <ak@linux.intel.com>
commit 6b28baca9b1f0d4a42b865da7a05b1c81424bd5c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6b28baca.failed

When PTEs are set to PROT_NONE the kernel just clears the Present bit and
preserves the PFN, which creates attack surface for L1TF speculation
speculation attacks.

This is important inside guests, because L1TF speculation bypasses physical
page remapping. While the host has its own migitations preventing leaking
data from other VMs into the guest, this would still risk leaking the wrong
page inside the current guest.

This uses the same technique as Linus' swap entry patch: while an entry is
is in PROTNONE state invert the complete PFN part part of it. This ensures
that the the highest bit will point to non existing memory.

The invert is done by pte/pmd_modify and pfn/pmd/pud_pte for PROTNONE and
pte/pmd/pud_pfn undo it.

This assume that no code path touches the PFN part of a PTE directly
without using these primitives.

This doesn't handle the case that MMIO is on the top of the CPU physical
memory. If such an MMIO region was exposed by an unpriviledged driver for
mmap it would be possible to attack some real memory.  However this
situation is all rather unlikely.

For 32bit non PAE the inversion is not done because there are really not
enough bits to protect anything.

Q: Why does the guest need to be protected when the HyperVisor already has
   L1TF mitigations?

A: Here's an example:

   Physical pages 1 2 get mapped into a guest as
   GPA 1 -> PA 2
   GPA 2 -> PA 1
   through EPT.

   The L1TF speculation ignores the EPT remapping.

   Now the guest kernel maps GPA 1 to process A and GPA 2 to process B, and
   they belong to different users and should be isolated.

   A sets the GPA 1 PA 2 PTE to PROT_NONE to bypass the EPT remapping and
   gets read access to the underlying physical page. Which in this case
   points to PA 2, so it can read process B's data, if it happened to be in
   L1, so isolation inside the guest is broken.

   There's nothing the hypervisor can do about this. This mitigation has to
   be done in the guest itself.

[ tglx: Massaged changelog ]

	Signed-off-by: Andi Kleen <ak@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Acked-by: Dave Hansen <dave.hansen@intel.com>



(cherry picked from commit 6b28baca9b1f0d4a42b865da7a05b1c81424bd5c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/pgtable-3level.h
#	arch/x86/include/asm/pgtable.h
#	arch/x86/include/asm/pgtable_64.h
diff --cc arch/x86/include/asm/pgtable-3level.h
index 2e1ec3324a0b,76ab26a99e6e..000000000000
--- a/arch/x86/include/asm/pgtable-3level.h
+++ b/arch/x86/include/asm/pgtable-3level.h
@@@ -239,4 -248,53 +239,56 @@@ static inline pud_t native_pudp_get_and
  #define __pte_to_swp_entry(pte)		((swp_entry_t){ (pte).pte_high })
  #define __swp_entry_to_pte(x)		((pte_t){ { .pte_high = (x).val } })
  
++<<<<<<< HEAD
++=======
+ #define gup_get_pte gup_get_pte
+ /*
+  * WARNING: only to be used in the get_user_pages_fast() implementation.
+  *
+  * With get_user_pages_fast(), we walk down the pagetables without taking
+  * any locks.  For this we would like to load the pointers atomically,
+  * but that is not possible (without expensive cmpxchg8b) on PAE.  What
+  * we do have is the guarantee that a PTE will only either go from not
+  * present to present, or present to not present or both -- it will not
+  * switch to a completely different present page without a TLB flush in
+  * between; something that we are blocking by holding interrupts off.
+  *
+  * Setting ptes from not present to present goes:
+  *
+  *   ptep->pte_high = h;
+  *   smp_wmb();
+  *   ptep->pte_low = l;
+  *
+  * And present to not present goes:
+  *
+  *   ptep->pte_low = 0;
+  *   smp_wmb();
+  *   ptep->pte_high = 0;
+  *
+  * We must ensure here that the load of pte_low sees 'l' iff pte_high
+  * sees 'h'. We load pte_high *after* loading pte_low, which ensures we
+  * don't see an older value of pte_high.  *Then* we recheck pte_low,
+  * which ensures that we haven't picked up a changed pte high. We might
+  * have gotten rubbish values from pte_low and pte_high, but we are
+  * guaranteed that pte_low will not have the present bit set *unless*
+  * it is 'l'. Because get_user_pages_fast() only operates on present ptes
+  * we're safe.
+  */
+ static inline pte_t gup_get_pte(pte_t *ptep)
+ {
+ 	pte_t pte;
+ 
+ 	do {
+ 		pte.pte_low = ptep->pte_low;
+ 		smp_rmb();
+ 		pte.pte_high = ptep->pte_high;
+ 		smp_rmb();
+ 	} while (unlikely(pte.pte_low != ptep->pte_low));
+ 
+ 	return pte;
+ }
+ 
+ #include <asm/pgtable-invert.h>
+ 
++>>>>>>> 6b28baca9b1f (x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation)
  #endif /* _ASM_X86_PGTABLE_3LEVEL_H */
diff --cc arch/x86/include/asm/pgtable.h
index 6a32515b7486,a9c89cb1a9c5..000000000000
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@@ -184,9 -205,16 +192,11 @@@ static inline unsigned long pmd_pfn(pmd
  
  static inline unsigned long pud_pfn(pud_t pud)
  {
- 	return (pud_val(pud) & pud_pfn_mask(pud)) >> PAGE_SHIFT;
+ 	unsigned long pfn = pud_val(pud);
+ 	pfn ^= protnone_mask(pfn);
+ 	return (pfn & pud_pfn_mask(pud)) >> PAGE_SHIFT;
  }
  
 -static inline unsigned long p4d_pfn(p4d_t p4d)
 -{
 -	return (p4d_val(p4d) & p4d_pfn_mask(p4d)) >> PAGE_SHIFT;
 -}
 -
  static inline unsigned long pgd_pfn(pgd_t pgd)
  {
  	return (pgd_val(pgd) & PTE_PFN_MASK) >> PAGE_SHIFT;
@@@ -518,45 -536,70 +528,78 @@@ static inline pgprotval_t massage_pgpro
  	return protval;
  }
  
 -static inline pgprotval_t check_pgprot(pgprot_t pgprot)
 -{
 -	pgprotval_t massaged_val = massage_pgprot(pgprot);
 -
 -	/* mmdebug.h can not be included here because of dependencies */
 -#ifdef CONFIG_DEBUG_VM
 -	WARN_ONCE(pgprot_val(pgprot) != massaged_val,
 -		  "attempted to set unsupported pgprot: %016llx "
 -		  "bits: %016llx supported: %016llx\n",
 -		  (u64)pgprot_val(pgprot),
 -		  (u64)pgprot_val(pgprot) ^ massaged_val,
 -		  (u64)__supported_pte_mask);
 -#endif
 -
 -	return massaged_val;
 -}
 -
  static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
  {
++<<<<<<< HEAD
 +	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
 +		     massage_pgprot(pgprot));
++=======
+ 	phys_addr_t pfn = page_nr << PAGE_SHIFT;
+ 	pfn ^= protnone_mask(pgprot_val(pgprot));
+ 	pfn &= PTE_PFN_MASK;
+ 	return __pte(pfn | check_pgprot(pgprot));
++>>>>>>> 6b28baca9b1f (x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation)
  }
  
  static inline pmd_t pfn_pmd(unsigned long page_nr, pgprot_t pgprot)
  {
++<<<<<<< HEAD
 +	return __pmd(((phys_addr_t)page_nr << PAGE_SHIFT) |
 +		     massage_pgprot(pgprot));
++=======
+ 	phys_addr_t pfn = page_nr << PAGE_SHIFT;
+ 	pfn ^= protnone_mask(pgprot_val(pgprot));
+ 	pfn &= PHYSICAL_PMD_PAGE_MASK;
+ 	return __pmd(pfn | check_pgprot(pgprot));
++>>>>>>> 6b28baca9b1f (x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation)
  }
  
  static inline pud_t pfn_pud(unsigned long page_nr, pgprot_t pgprot)
  {
++<<<<<<< HEAD
 +	return __pud(((phys_addr_t)page_nr << PAGE_SHIFT) |
 +		     massage_pgprot(pgprot));
++=======
+ 	phys_addr_t pfn = page_nr << PAGE_SHIFT;
+ 	pfn ^= protnone_mask(pgprot_val(pgprot));
+ 	pfn &= PHYSICAL_PUD_PAGE_MASK;
+ 	return __pud(pfn | check_pgprot(pgprot));
++>>>>>>> 6b28baca9b1f (x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation)
  }
  
+ static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask);
+ 
  static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
  {
- 	pteval_t val = pte_val(pte);
+ 	pteval_t val = pte_val(pte), oldval = val;
  
  	/*
  	 * Chop off the NX bit (if present), and add the NX portion of
  	 * the newprot (if present):
  	 */
  	val &= _PAGE_CHG_MASK;
++<<<<<<< HEAD
 +	val |= massage_pgprot(newprot) & ~_PAGE_CHG_MASK;
 +
++=======
+ 	val |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;
+ 	val = flip_protnone_guard(oldval, val, PTE_PFN_MASK);
++>>>>>>> 6b28baca9b1f (x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation)
  	return __pte(val);
  }
  
  static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
  {
- 	pmdval_t val = pmd_val(pmd);
+ 	pmdval_t val = pmd_val(pmd), oldval = val;
  
  	val &= _HPAGE_CHG_MASK;
++<<<<<<< HEAD
 +	val |= massage_pgprot(newprot) & ~_HPAGE_CHG_MASK;
 +
++=======
+ 	val |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;
+ 	val = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);
++>>>>>>> 6b28baca9b1f (x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation)
  	return __pmd(val);
  }
  
diff --cc arch/x86/include/asm/pgtable_64.h
index 1c528b8b0542,3c687e9a95a4..000000000000
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@@ -384,6 -342,22 +384,26 @@@ extern void cleanup_highmap(void)
  extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);
  extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);
  
++<<<<<<< HEAD
++=======
+ #define gup_fast_permitted gup_fast_permitted
+ static inline bool gup_fast_permitted(unsigned long start, int nr_pages,
+ 		int write)
+ {
+ 	unsigned long len, end;
+ 
+ 	len = (unsigned long)nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 	if (end < start)
+ 		return false;
+ 	if (end >> __VIRTUAL_MASK_SHIFT)
+ 		return false;
+ 	return true;
+ }
+ 
+ #include <asm/pgtable-invert.h>
+ 
++>>>>>>> 6b28baca9b1f (x86/speculation/l1tf: Protect PROT_NONE PTEs against speculation)
  #endif /* !__ASSEMBLY__ */
 +
  #endif /* _ASM_X86_PGTABLE_64_H */
diff --git a/arch/x86/include/asm/pgtable-2level.h b/arch/x86/include/asm/pgtable-2level.h
index 2fc9daa29902..f6f051e58a5d 100644
--- a/arch/x86/include/asm/pgtable-2level.h
+++ b/arch/x86/include/asm/pgtable-2level.h
@@ -176,4 +176,21 @@ static inline pud_t native_pudp_get_and_clear(pud_t *xp)
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { (pte).pte_low })
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
 
+/* No inverted PFNs on 2 level page tables */
+
+static inline u64 protnone_mask(u64 val)
+{
+	return 0;
+}
+
+static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask)
+{
+	return val;
+}
+
+static inline bool __pte_needs_invert(u64 val)
+{
+	return false;
+}
+
 #endif /* _ASM_X86_PGTABLE_2LEVEL_H */
* Unmerged path arch/x86/include/asm/pgtable-3level.h
diff --git a/arch/x86/include/asm/pgtable-invert.h b/arch/x86/include/asm/pgtable-invert.h
new file mode 100644
index 000000000000..177564187fc0
--- /dev/null
+++ b/arch/x86/include/asm/pgtable-invert.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_PGTABLE_INVERT_H
+#define _ASM_PGTABLE_INVERT_H 1
+
+#ifndef __ASSEMBLY__
+
+static inline bool __pte_needs_invert(u64 val)
+{
+	return (val & (_PAGE_PRESENT|_PAGE_PROTNONE)) == _PAGE_PROTNONE;
+}
+
+/* Get a mask to xor with the page table entry to get the correct pfn. */
+static inline u64 protnone_mask(u64 val)
+{
+	return __pte_needs_invert(val) ?  ~0ull : 0;
+}
+
+static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask)
+{
+	/*
+	 * When a PTE transitions from NONE to !NONE or vice-versa
+	 * invert the PFN part to stop speculation.
+	 * pte_pfn undoes this when needed.
+	 */
+	if (__pte_needs_invert(oldval) != __pte_needs_invert(val))
+		val = (val & ~mask) | (~val & mask);
+	return val;
+}
+
+#endif /* __ASSEMBLY__ */
+
+#endif
* Unmerged path arch/x86/include/asm/pgtable.h
* Unmerged path arch/x86/include/asm/pgtable_64.h
