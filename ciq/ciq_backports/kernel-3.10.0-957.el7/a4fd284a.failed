ip: process in-order fragments efficiently

jira LE-1907
cve CVE-2018-5391
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Peter Oskolkov <posk@google.com>
commit a4fd284a1f8fd4b6c59aa59db2185b1e17c5c11c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a4fd284a.failed

This patch changes the runtime behavior of IP defrag queue:
incoming in-order fragments are added to the end of the current
list/"run" of in-order fragments at the tail.

On some workloads, UDP stream performance is substantially improved:

RX: ./udp_stream -F 10 -T 2 -l 60
TX: ./udp_stream -c -H <host> -F 10 -T 5 -l 60

with this patchset applied on a 10Gbps receiver:

  throughput=9524.18
  throughput_units=Mbit/s

upstream (net-next):

  throughput=4608.93
  throughput_units=Mbit/s

	Reported-by: Willem de Bruijn <willemb@google.com>
	Signed-off-by: Peter Oskolkov <posk@google.com>
	Cc: Eric Dumazet <edumazet@google.com>
	Cc: Florian Westphal <fw@strlen.de>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a4fd284a1f8fd4b6c59aa59db2185b1e17c5c11c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/inet_fragment.c
#	net/ipv4/ip_fragment.c
diff --cc net/ipv4/inet_fragment.c
index e5c94fababe6,bcb11f3a27c0..000000000000
--- a/net/ipv4/inet_fragment.c
+++ b/net/ipv4/inet_fragment.c
@@@ -176,22 -136,23 +176,31 @@@ void inet_frag_destroy(struct inet_frag
  	/* Release all fragment data. */
  	fp = q->fragments;
  	nf = q->net;
 -	f = nf->f;
 -	if (fp) {
 -		do {
 -			struct sk_buff *xp = fp->next;
 -
 +	while (fp) {
 +		struct sk_buff *xp = fp->next;
 +
++<<<<<<< HEAD
 +		sum_truesize += fp->truesize;
 +		frag_kfree_skb(nf, f, fp);
 +		fp = xp;
++=======
+ 			sum_truesize += fp->truesize;
+ 			kfree_skb(fp);
+ 			fp = xp;
+ 		} while (fp);
+ 	} else {
+ 		sum_truesize = inet_frag_rbtree_purge(&q->rb_fragments);
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  	}
  	sum = sum_truesize + f->qsize;
 +	if (work)
 +		*work -= sum;
 +	sub_frag_mem_limit(q, sum);
  
 -	call_rcu(&q->rcu, inet_frag_destroy_rcu);
 +	if (f->destructor)
 +		f->destructor(q);
 +	kfree(q);
  
 -	sub_frag_mem_limit(nf, sum);
  }
  EXPORT_SYMBOL(inet_frag_destroy);
  
diff --cc net/ipv4/ip_fragment.c
index da6190718c34,88281fbce88c..000000000000
--- a/net/ipv4/ip_fragment.c
+++ b/net/ipv4/ip_fragment.c
@@@ -87,54 -126,11 +87,59 @@@ static inline u8 ip4_frag_ecn(u8 tos
  
  static struct inet_frags ip4_frags;
  
++<<<<<<< HEAD
 +int ip_frag_nqueues(struct net *net)
 +{
 +	return net->ipv4.frags.nqueues;
 +}
 +
 +int ip_frag_mem(struct net *net)
 +{
 +	return sum_frag_mem_limit(&net->ipv4.frags);
 +}
 +
 +static int ip_frag_reasm(struct ipq *qp, struct sk_buff *prev,
 +			 struct net_device *dev);
++=======
+ static int ip_frag_reasm(struct ipq *qp, struct sk_buff *skb,
+ 			 struct sk_buff *prev_tail, struct net_device *dev);
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
 +
 +struct ip4_create_arg {
 +	struct iphdr *iph;
 +	u32 user;
 +};
 +
 +static unsigned int ipqhashfn(__be16 id, __be32 saddr, __be32 daddr, u8 prot)
 +{
 +	net_get_random_once(&ip4_frags.rnd, sizeof(ip4_frags.rnd));
 +	return jhash_3words((__force u32)id << 16 | prot,
 +			    (__force u32)saddr, (__force u32)daddr,
 +			    ip4_frags.rnd) & (INETFRAGS_HASHSZ - 1);
 +}
 +
 +static unsigned int ip4_hashfn(struct inet_frag_queue *q)
 +{
 +	struct ipq *ipq;
 +
 +	ipq = container_of(q, struct ipq, q);
 +	return ipqhashfn(ipq->id, ipq->saddr, ipq->daddr, ipq->protocol);
 +}
 +
 +static bool ip4_frag_match(struct inet_frag_queue *q, void *a)
 +{
 +	struct ipq *qp;
 +	struct ip4_create_arg *arg = a;
  
 +	qp = container_of(q, struct ipq, q);
 +	return	qp->id == arg->iph->id &&
 +		qp->saddr == arg->iph->saddr &&
 +		qp->daddr == arg->iph->daddr &&
 +		qp->protocol == arg->iph->protocol &&
 +		qp->user == arg->user;
 +}
  
 -static void ip4_frag_init(struct inet_frag_queue *q, const void *a)
 +static void ip4_frag_init(struct inet_frag_queue *q, void *a)
  {
  	struct ipq *qp = container_of(q, struct ipq, q);
  	struct netns_ipv4 *ipv4 = container_of(q->net, struct netns_ipv4,
@@@ -207,44 -202,66 +212,69 @@@ static void ip_expire(unsigned long arg
  		goto out;
  
  	ipq_kill(qp);
 -	__IP_INC_STATS(net, IPSTATS_MIB_REASMFAILS);
 -	__IP_INC_STATS(net, IPSTATS_MIB_REASMTIMEOUT);
  
 -	if (!(qp->q.flags & INET_FRAG_FIRST_IN))
 -		goto out;
 +	IP_INC_STATS_BH(net, IPSTATS_MIB_REASMTIMEOUT);
 +	IP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);
  
++<<<<<<< HEAD
 +	if ((qp->q.last_in & INET_FRAG_FIRST_IN) && qp->q.fragments != NULL) {
 +		struct sk_buff *head = qp->q.fragments;
 +		const struct iphdr *iph;
 +		int err;
++=======
+ 	/* sk_buff::dev and sk_buff::rbnode are unionized. So we
+ 	 * pull the head out of the tree in order to be able to
+ 	 * deal with head->dev.
+ 	 */
+ 	if (qp->q.fragments) {
+ 		head = qp->q.fragments;
+ 		qp->q.fragments = head->next;
+ 	} else {
+ 		head = skb_rb_first(&qp->q.rb_fragments);
+ 		if (!head)
+ 			goto out;
+ 		if (FRAG_CB(head)->next_frag)
+ 			rb_replace_node(&head->rbnode,
+ 					&FRAG_CB(head)->next_frag->rbnode,
+ 					&qp->q.rb_fragments);
+ 		else
+ 			rb_erase(&head->rbnode, &qp->q.rb_fragments);
+ 		memset(&head->rbnode, 0, sizeof(head->rbnode));
+ 		barrier();
+ 	}
+ 	if (head == qp->q.fragments_tail)
+ 		qp->q.fragments_tail = NULL;
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  
 -	sub_frag_mem_limit(qp->q.net, head->truesize);
 -
 -	head->dev = dev_get_by_index_rcu(net, qp->iif);
 -	if (!head->dev)
 -		goto out;
 +		rcu_read_lock();
 +		head->dev = dev_get_by_index_rcu(net, qp->iif);
 +		if (!head->dev)
 +			goto out_rcu_unlock;
  
 -
 -	/* skb has no dst, perform route lookup again */
 -	iph = ip_hdr(head);
 -	err = ip_route_input_noref(head, iph->daddr, iph->saddr,
 +		/* skb has no dst, perform route lookup again */
 +		iph = ip_hdr(head);
 +		err = ip_route_input_noref(head, iph->daddr, iph->saddr,
  					   iph->tos, head->dev);
 -	if (err)
 -		goto out;
 +		if (err)
 +			goto out_rcu_unlock;
  
 -	/* Only an end host needs to send an ICMP
 -	 * "Fragment Reassembly Timeout" message, per RFC792.
 -	 */
 -	if (frag_expire_skip_icmp(qp->q.key.v4.user) &&
 -	    (skb_rtable(head)->rt_type != RTN_LOCAL))
 -		goto out;
 +		/*
 +		 * Only an end host needs to send an ICMP
 +		 * "Fragment Reassembly Timeout" message, per RFC792.
 +		 */
 +		if (qp->user == IP_DEFRAG_AF_PACKET ||
 +		    (qp->user == IP_DEFRAG_CONNTRACK_IN &&
 +		     skb_rtable(head)->rt_type != RTN_LOCAL))
 +			goto out_rcu_unlock;
  
 -	spin_unlock(&qp->q.lock);
 -	icmp_send(head, ICMP_TIME_EXCEEDED, ICMP_EXC_FRAGTIME, 0);
 -	goto out_rcu_unlock;
  
 +		/* Send an ICMP "Fragment Reassembly Timeout" message. */
 +		icmp_send(head, ICMP_TIME_EXCEEDED, ICMP_EXC_FRAGTIME, 0);
 +out_rcu_unlock:
 +		rcu_read_unlock();
 +	}
  out:
  	spin_unlock(&qp->q.lock);
 -out_rcu_unlock:
 -	rcu_read_unlock();
 -	if (head)
 -		kfree_skb(head);
  	ipq_put(qp);
  }
  
@@@ -309,21 -325,16 +339,27 @@@ static int ip_frag_reinit(struct ipq *q
  		return -ETIMEDOUT;
  	}
  
++<<<<<<< HEAD
 +	fp = qp->q.fragments;
 +	do {
 +		struct sk_buff *xp = fp->next;
++=======
+ 	sum_truesize = inet_frag_rbtree_purge(&qp->q.rb_fragments);
+ 	sub_frag_mem_limit(qp->q.net, sum_truesize);
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  
 -	qp->q.flags = 0;
 +		sum_truesize += fp->truesize;
 +		kfree_skb(fp);
 +		fp = xp;
 +	} while (fp);
 +	sub_frag_mem_limit(&qp->q, sum_truesize);
 +
 +	qp->q.last_in = 0;
  	qp->q.len = 0;
  	qp->q.meat = 0;
  	qp->q.fragments = NULL;
 -	qp->q.rb_fragments = RB_ROOT;
  	qp->q.fragments_tail = NULL;
+ 	qp->q.last_run_head = NULL;
  	qp->iif = 0;
  	qp->ecn = 0;
  
@@@ -333,7 -344,9 +369,13 @@@
  /* Add new segment to existing queue. */
  static int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)
  {
++<<<<<<< HEAD
 +	struct sk_buff *prev, *next;
++=======
+ 	struct net *net = container_of(qp->q.net, struct net, ipv4.frags);
+ 	struct rb_node **rbn, *parent;
+ 	struct sk_buff *skb1, *prev_tail;
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  	struct net_device *dev;
  	unsigned int fragsize;
  	int flags, offset;
@@@ -396,94 -409,61 +438,133 @@@
  	if (err)
  		goto err;
  
 -	/* Note : skb->rbnode and skb->dev share the same location. */
 -	dev = skb->dev;
 -	/* Makes sure compiler wont do silly aliasing games */
 -	barrier();
 -
 -	/* RFC5722, Section 4, amended by Errata ID : 3089
 -	 *                          When reassembling an IPv6 datagram, if
 -	 *   one or more its constituent fragments is determined to be an
 -	 *   overlapping fragment, the entire datagram (and any constituent
 -	 *   fragments) MUST be silently discarded.
 -	 *
 -	 * We do the same here for IPv4 (and increment an snmp counter).
 +	/* Find out which fragments are in front and at the back of us
 +	 * in the chain of fragments so far.  We must know where to put
 +	 * this fragment, right?
  	 */
++<<<<<<< HEAD
 +	prev = qp->q.fragments_tail;
 +	if (!prev || FRAG_CB(prev)->offset < offset) {
 +		next = NULL;
 +		goto found;
 +	}
 +	prev = NULL;
 +	for (next = qp->q.fragments; next != NULL; next = next->next) {
 +		if (FRAG_CB(next)->offset >= offset)
 +			break;	/* bingo! */
 +		prev = next;
++=======
+ 
+ 	/* Find out where to put this fragment.  */
+ 	prev_tail = qp->q.fragments_tail;
+ 	if (!prev_tail)
+ 		ip4_frag_create_run(&qp->q, skb);  /* First fragment. */
+ 	else if (prev_tail->ip_defrag_offset + prev_tail->len < end) {
+ 		/* This is the common case: skb goes to the end. */
+ 		/* Detect and discard overlaps. */
+ 		if (offset < prev_tail->ip_defrag_offset + prev_tail->len)
+ 			goto discard_qp;
+ 		if (offset == prev_tail->ip_defrag_offset + prev_tail->len)
+ 			ip4_frag_append_to_last_run(&qp->q, skb);
+ 		else
+ 			ip4_frag_create_run(&qp->q, skb);
+ 	} else {
+ 		/* Binary search. Note that skb can become the first fragment,
+ 		 * but not the last (covered above).
+ 		 */
+ 		rbn = &qp->q.rb_fragments.rb_node;
+ 		do {
+ 			parent = *rbn;
+ 			skb1 = rb_to_skb(parent);
+ 			if (end <= skb1->ip_defrag_offset)
+ 				rbn = &parent->rb_left;
+ 			else if (offset >= skb1->ip_defrag_offset +
+ 						FRAG_CB(skb1)->frag_run_len)
+ 				rbn = &parent->rb_right;
+ 			else /* Found an overlap with skb1. */
+ 				goto discard_qp;
+ 		} while (*rbn);
+ 		/* Here we have parent properly set, and rbn pointing to
+ 		 * one of its NULL left/right children. Insert skb.
+ 		 */
+ 		ip4_frag_init_run(skb);
+ 		rb_link_node(&skb->rbnode, parent, rbn);
+ 		rb_insert_color(&skb->rbnode, &qp->q.rb_fragments);
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  	}
  
 -	if (dev)
 -		qp->iif = dev->ifindex;
 -	skb->ip_defrag_offset = offset;
 +found:
 +	/* We found where to put this one.  Check for overlap with
 +	 * preceding fragment, and, if needed, align things so that
 +	 * any overlaps are eliminated.
 +	 */
 +	if (prev) {
 +		int i = (FRAG_CB(prev)->offset + prev->len) - offset;
 +
 +		if (i > 0) {
 +			offset += i;
 +			err = -EINVAL;
 +			if (end <= offset)
 +				goto err;
 +			err = -ENOMEM;
 +			if (!pskb_pull(skb, i))
 +				goto err;
 +			if (skb->ip_summed != CHECKSUM_UNNECESSARY)
 +				skb->ip_summed = CHECKSUM_NONE;
 +		}
 +	}
 +
 +	err = -ENOMEM;
 +
 +	while (next && FRAG_CB(next)->offset < end) {
 +		int i = end - FRAG_CB(next)->offset; /* overlap is 'i' bytes */
 +
 +		if (i < next->len) {
 +			/* Eat head of the next overlapped fragment
 +			 * and leave the loop. The next ones cannot overlap.
 +			 */
 +			if (!pskb_pull(next, i))
 +				goto err;
 +			FRAG_CB(next)->offset += i;
 +			qp->q.meat -= i;
 +			if (next->ip_summed != CHECKSUM_UNNECESSARY)
 +				next->ip_summed = CHECKSUM_NONE;
 +			break;
 +		} else {
 +			struct sk_buff *free_it = next;
 +
 +			/* Old fragment is completely overridden with
 +			 * new one drop it.
 +			 */
 +			next = next->next;
 +
 +			if (prev)
 +				prev->next = next;
 +			else
 +				qp->q.fragments = next;
 +
 +			qp->q.meat -= free_it->len;
 +			sub_frag_mem_limit(&qp->q, free_it->truesize);
 +			kfree_skb(free_it);
 +		}
 +	}
  
 +	FRAG_CB(skb)->offset = offset;
 +
 +	/* Insert this fragment in the chain of fragments. */
 +	skb->next = next;
 +	if (!next)
 +		qp->q.fragments_tail = skb;
 +	if (prev)
 +		prev->next = skb;
 +	else
 +		qp->q.fragments = skb;
 +
 +	dev = skb->dev;
 +	if (dev) {
 +		qp->iif = dev->ifindex;
 +		skb->dev = NULL;
 +	}
  	qp->q.stamp = skb->tstamp;
  	qp->q.meat += skb->len;
  	qp->ecn |= ecn;
@@@ -505,7 -485,7 +586,11 @@@
  		unsigned long orefdst = skb->_skb_refdst;
  
  		skb->_skb_refdst = 0UL;
++<<<<<<< HEAD
 +		err = ip_frag_reasm(qp, prev, dev);
++=======
+ 		err = ip_frag_reasm(qp, skb, prev_tail, dev);
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  		skb->_skb_refdst = orefdst;
  		return err;
  	}
@@@ -519,11 -502,9 +604,16 @@@ err
  	return err;
  }
  
 +
  /* Build a new IP datagram from all its fragments. */
++<<<<<<< HEAD
 +
 +static int ip_frag_reasm(struct ipq *qp, struct sk_buff *prev,
 +			 struct net_device *dev)
++=======
+ static int ip_frag_reasm(struct ipq *qp, struct sk_buff *skb,
+ 			 struct sk_buff *prev_tail, struct net_device *dev)
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  {
  	struct net *net = container_of(qp->q.net, struct net, ipv4.frags);
  	struct iphdr *iph;
@@@ -541,26 -524,27 +631,43 @@@
  		goto out_fail;
  	}
  	/* Make the one we just received the head. */
 -	if (head != skb) {
 -		fp = skb_clone(skb, GFP_ATOMIC);
 +	if (prev) {
 +		head = prev->next;
 +		fp = skb_clone(head, GFP_ATOMIC);
  		if (!fp)
  			goto out_nomem;
++<<<<<<< HEAD
 +
 +		fp->next = head->next;
 +		if (!fp->next)
 +			qp->q.fragments_tail = fp;
 +		prev->next = fp;
 +
 +		skb_morph(head, qp->q.fragments);
 +		head->next = qp->q.fragments->next;
 +
 +		consume_skb(qp->q.fragments);
 +		qp->q.fragments = head;
++=======
+ 		FRAG_CB(fp)->next_frag = FRAG_CB(skb)->next_frag;
+ 		if (RB_EMPTY_NODE(&skb->rbnode))
+ 			FRAG_CB(prev_tail)->next_frag = fp;
+ 		else
+ 			rb_replace_node(&skb->rbnode, &fp->rbnode,
+ 					&qp->q.rb_fragments);
+ 		if (qp->q.fragments_tail == skb)
+ 			qp->q.fragments_tail = fp;
+ 		skb_morph(skb, head);
+ 		FRAG_CB(skb)->next_frag = FRAG_CB(head)->next_frag;
+ 		rb_replace_node(&head->rbnode, &skb->rbnode,
+ 				&qp->q.rb_fragments);
+ 		consume_skb(head);
+ 		head = skb;
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  	}
  
 -	WARN_ON(head->ip_defrag_offset != 0);
 +	WARN_ON(head == NULL);
 +	WARN_ON(FRAG_CB(head)->offset != 0);
  
  	/* Allocate a new buffer for the datagram. */
  	ihlen = ip_hdrlen(head);
@@@ -590,28 -573,55 +697,66 @@@
  		for (i = 0; i < skb_shinfo(head)->nr_frags; i++)
  			plen += skb_frag_size(&skb_shinfo(head)->frags[i]);
  		clone->len = clone->data_len = head->data_len - plen;
++<<<<<<< HEAD
 +		head->data_len -= clone->len;
 +		head->len -= clone->len;
++=======
+ 		head->truesize += clone->truesize;
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  		clone->csum = 0;
  		clone->ip_summed = head->ip_summed;
 -		add_frag_mem_limit(qp->q.net, clone->truesize);
 -		skb_shinfo(head)->frag_list = clone;
 -		nextp = &clone->next;
 -	} else {
 -		nextp = &skb_shinfo(head)->frag_list;
 +		add_frag_mem_limit(&qp->q, clone->truesize);
  	}
  
 +	skb_shinfo(head)->frag_list = head->next;
  	skb_push(head, head->data - skb_network_header(head));
  
++<<<<<<< HEAD
 +	for (fp=head->next; fp; fp = fp->next) {
 +		head->data_len += fp->len;
 +		head->len += fp->len;
 +		if (head->ip_summed != fp->ip_summed)
 +			head->ip_summed = CHECKSUM_NONE;
 +		else if (head->ip_summed == CHECKSUM_COMPLETE)
 +			head->csum = csum_add(head->csum, fp->csum);
 +		head->truesize += fp->truesize;
++=======
+ 	/* Traverse the tree in order, to build frag_list. */
+ 	fp = FRAG_CB(head)->next_frag;
+ 	rbn = rb_next(&head->rbnode);
+ 	rb_erase(&head->rbnode, &qp->q.rb_fragments);
+ 	while (rbn || fp) {
+ 		/* fp points to the next sk_buff in the current run;
+ 		 * rbn points to the next run.
+ 		 */
+ 		/* Go through the current run. */
+ 		while (fp) {
+ 			*nextp = fp;
+ 			nextp = &fp->next;
+ 			fp->prev = NULL;
+ 			memset(&fp->rbnode, 0, sizeof(fp->rbnode));
+ 			head->data_len += fp->len;
+ 			head->len += fp->len;
+ 			if (head->ip_summed != fp->ip_summed)
+ 				head->ip_summed = CHECKSUM_NONE;
+ 			else if (head->ip_summed == CHECKSUM_COMPLETE)
+ 				head->csum = csum_add(head->csum, fp->csum);
+ 			head->truesize += fp->truesize;
+ 			fp = FRAG_CB(fp)->next_frag;
+ 		}
+ 		/* Move to the next run. */
+ 		if (rbn) {
+ 			struct rb_node *rbnext = rb_next(rbn);
+ 
+ 			fp = rb_to_skb(rbn);
+ 			rb_erase(rbn, &qp->q.rb_fragments);
+ 			rbn = rbnext;
+ 		}
++>>>>>>> a4fd284a1f8f (ip: process in-order fragments efficiently)
  	}
 -	sub_frag_mem_limit(qp->q.net, head->truesize);
 +	sub_frag_mem_limit(&qp->q, head->truesize);
  
 -	*nextp = NULL;
  	head->next = NULL;
 -	head->prev = NULL;
  	head->dev = dev;
  	head->tstamp = qp->q.stamp;
  	IPCB(head)->frag_max_size = max(qp->max_df_size, qp->q.max_size);
@@@ -637,9 -647,11 +782,10 @@@
  
  	ip_send_check(iph);
  
 -	__IP_INC_STATS(net, IPSTATS_MIB_REASMOKS);
 +	IP_INC_STATS_BH(net, IPSTATS_MIB_REASMOKS);
  	qp->q.fragments = NULL;
 -	qp->q.rb_fragments = RB_ROOT;
  	qp->q.fragments_tail = NULL;
+ 	qp->q.last_run_head = NULL;
  	return 0;
  
  out_nomem:
* Unmerged path net/ipv4/inet_fragment.c
* Unmerged path net/ipv4/ip_fragment.c
