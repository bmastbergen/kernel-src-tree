mm/pagewalk.c: prevent positive return value of walk_page_test() from being passed to callers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] pagewalk: prevent positive return value of walk_page_test() from being passed to callers (Rafael Aquini) [1562137]
Rebuild_FUZZ: 97.24%
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit f683739539e819e9b821a197d80e52258510837b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/f6837395.failed

walk_page_test() is purely pagewalk's internal stuff, and its positive
return values are not intended to be passed to the callers of pagewalk.

However, in the current code if the last vma in the do-while loop in
walk_page_range() happens to return a positive value, it leaks outside
walk_page_range().  So the user visible effect is invalid/unexpected
return value (according to the reporter, mbind() causes it.)

This patch fixes it simply by reinitializing the return value after
checked.

Another exposed interface, walk_page_vma(), already returns 0 for such
cases so no problem.

Fixes: fafaa4264eba ("pagewalk: improve vma handling")
	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Kazutomo Yoshii <kazutomo.yoshii@gmail.com>
	Reported-by: Kazutomo Yoshii <kazutomo.yoshii@gmail.com>
	Acked-by: David Rientjes <rientjes@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f683739539e819e9b821a197d80e52258510837b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/pagewalk.c
diff --cc mm/pagewalk.c
index fc0ccb7f9250,29f2f8b853ae..000000000000
--- a/mm/pagewalk.c
+++ b/mm/pagewalk.c
@@@ -194,76 -249,39 +194,91 @@@ int walk_page_range(unsigned long addr
  	if (!walk->mm)
  		return -EINVAL;
  
 -	VM_BUG_ON_MM(!rwsem_is_locked(&walk->mm->mmap_sem), walk->mm);
 +	VM_BUG_ON(!rwsem_is_locked(&walk->mm->mmap_sem));
  
 -	vma = find_vma(walk->mm, start);
 +	pgd = pgd_offset(walk->mm, addr);
  	do {
 -		if (!vma) { /* after the last vma */
 -			walk->vma = NULL;
 -			next = end;
 -		} else if (start < vma->vm_start) { /* outside vma */
 -			walk->vma = NULL;
 -			next = min(end, vma->vm_start);
 -		} else { /* inside vma */
 -			walk->vma = vma;
 -			next = min(end, vma->vm_end);
 -			vma = vma->vm_next;
 +		struct vm_area_struct *vma = NULL;
 +
++<<<<<<< HEAD
 +		next = pgd_addr_end(addr, end);
  
 +		/*
 +		 * This function was not intended to be vma based.
 +		 * But there are vma special cases to be handled:
 +		 * - hugetlb vma's
 +		 * - VM_PFNMAP vma's
 +		 */
 +		vma = find_vma(walk->mm, addr);
 +		if (vma) {
 +			/*
 +			 * There are no page structures backing a VM_PFNMAP
 +			 * range, so do not allow split_huge_page_pmd().
 +			 */
 +			if ((vma->vm_start <= addr) &&
 +			    (vma->vm_flags & VM_PFNMAP)) {
 +				if (walk->pte_hole)
 +					err = walk->pte_hole(addr, next, walk);
 +				if (err)
 +					break;
 +				pgd = pgd_offset(walk->mm, next);
 +				continue;
 +			}
 +			/*
 +			 * Handle hugetlb vma individually because pagetable
 +			 * walk for the hugetlb page is dependent on the
 +			 * architecture and we can't handled it in the same
 +			 * manner as non-huge pages.
 +			 */
 +			if (walk->hugetlb_entry && (vma->vm_start <= addr) &&
 +			    is_vm_hugetlb_page(vma)) {
 +				if (vma->vm_end < next)
 +					next = vma->vm_end;
 +				/*
 +				 * Hugepage is very tightly coupled with vma,
 +				 * so walk through hugetlb entries within a
 +				 * given vma.
 +				 */
 +				err = walk_hugetlb_range(vma, addr, next, walk);
 +				if (err)
 +					break;
 +				pgd = pgd_offset(walk->mm, next);
 +				continue;
 +			}
++=======
+ 			err = walk_page_test(start, next, walk);
+ 			if (err > 0) {
+ 				/*
+ 				 * positive return values are purely for
+ 				 * controlling the pagewalk, so should never
+ 				 * be passed to the callers.
+ 				 */
+ 				err = 0;
+ 				continue;
+ 			}
+ 			if (err < 0)
+ 				break;
++>>>>>>> f683739539e8 (mm/pagewalk.c: prevent positive return value of walk_page_test() from being passed to callers)
 +		}
 +
 +		if (pgd_none_or_clear_bad(pgd)) {
 +			if (walk->pte_hole)
 +				err = walk->pte_hole(addr, next, walk);
 +			if (err)
 +				break;
 +			pgd++;
 +			continue;
  		}
 -		if (walk->vma || walk->pte_hole)
 -			err = __walk_page_range(start, next, walk);
 +		if (walk->pgd_entry)
 +			err = walk->pgd_entry(pgd, addr, next, walk);
 +		if (!err &&
 +		    (walk->pud_entry || walk->pmd_entry || walk->pte_entry))
 +			err = walk_pud_range(pgd, addr, next, walk);
  		if (err)
  			break;
 -	} while (start = next, start < end);
 +		pgd++;
 +	} while (addr = next, addr < end);
 +
  	return err;
  }
  
* Unmerged path mm/pagewalk.c
