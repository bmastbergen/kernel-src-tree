nfp: bpf: add support for reading map memory

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 3dd43c3319cb0ba17cec9a989124176b409da326
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3dd43c33.failed

Map memory needs to use 40 bit addressing.  Add handling of such
accesses.  Since 40 bit addresses are formed by using both 32 bit
operands we need to pre-calculate the actual address instead of
adding in the offset inside the instruction, like we did in 32 bit
mode.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 3dd43c3319cb0ba17cec9a989124176b409da326)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/jit.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/jit.c
index 2a38b8e187f7,cdc949fabe98..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/jit.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/jit.c
@@@ -498,6 -540,171 +498,174 @@@ static void wrp_reg_mov(struct nfp_pro
  	wrp_mov(nfp_prog, reg_both(dst), reg_b(src));
  }
  
++<<<<<<< HEAD
++=======
+ /* wrp_reg_subpart() - load @field_len bytes from @offset of @src, write the
+  * result to @dst from low end.
+  */
+ static void
+ wrp_reg_subpart(struct nfp_prog *nfp_prog, swreg dst, swreg src, u8 field_len,
+ 		u8 offset)
+ {
+ 	enum shf_sc sc = offset ? SHF_SC_R_SHF : SHF_SC_NONE;
+ 	u8 mask = (1 << field_len) - 1;
+ 
+ 	emit_ld_field_any(nfp_prog, dst, mask, src, sc, offset * 8, true);
+ }
+ 
+ static void
+ addr40_offset(struct nfp_prog *nfp_prog, u8 src_gpr, swreg offset,
+ 	      swreg *rega, swreg *regb)
+ {
+ 	if (offset == reg_imm(0)) {
+ 		*rega = reg_a(src_gpr);
+ 		*regb = reg_b(src_gpr + 1);
+ 		return;
+ 	}
+ 
+ 	emit_alu(nfp_prog, imm_a(nfp_prog), reg_a(src_gpr), ALU_OP_ADD, offset);
+ 	emit_alu(nfp_prog, imm_b(nfp_prog), reg_b(src_gpr + 1), ALU_OP_ADD_C,
+ 		 reg_imm(0));
+ 	*rega = imm_a(nfp_prog);
+ 	*regb = imm_b(nfp_prog);
+ }
+ 
+ /* NFP has Command Push Pull bus which supports bluk memory operations. */
+ static int nfp_cpp_memcpy(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta)
+ {
+ 	bool descending_seq = meta->ldst_gather_len < 0;
+ 	s16 len = abs(meta->ldst_gather_len);
+ 	swreg src_base, off;
+ 	bool src_40bit_addr;
+ 	unsigned int i;
+ 	u8 xfer_num;
+ 
+ 	off = re_load_imm_any(nfp_prog, meta->insn.off, imm_b(nfp_prog));
+ 	src_40bit_addr = meta->ptr.type == PTR_TO_MAP_VALUE;
+ 	src_base = reg_a(meta->insn.src_reg * 2);
+ 	xfer_num = round_up(len, 4) / 4;
+ 
+ 	if (src_40bit_addr)
+ 		addr40_offset(nfp_prog, meta->insn.src_reg, off, &src_base,
+ 			      &off);
+ 
+ 	/* Setup PREV_ALU fields to override memory read length. */
+ 	if (len > 32)
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, xfer_num - 1));
+ 
+ 	/* Memory read from source addr into transfer-in registers. */
+ 	emit_cmd_any(nfp_prog, CMD_TGT_READ32_SWAP,
+ 		     src_40bit_addr ? CMD_MODE_40b_BA : CMD_MODE_32b, 0,
+ 		     src_base, off, xfer_num - 1, true, len > 32);
+ 
+ 	/* Move from transfer-in to transfer-out. */
+ 	for (i = 0; i < xfer_num; i++)
+ 		wrp_mov(nfp_prog, reg_xfer(i), reg_xfer(i));
+ 
+ 	off = re_load_imm_any(nfp_prog, meta->paired_st->off, imm_b(nfp_prog));
+ 
+ 	if (len <= 8) {
+ 		/* Use single direct_ref write8. */
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 0,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, len - 1,
+ 			 true);
+ 	} else if (len <= 32 && IS_ALIGNED(len, 4)) {
+ 		/* Use single direct_ref write32. */
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, xfer_num - 1,
+ 			 true);
+ 	} else if (len <= 32) {
+ 		/* Use single indirect_ref write8. */
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, len - 1));
+ 		emit_cmd_indir(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 0,
+ 			       reg_a(meta->paired_st->dst_reg * 2), off,
+ 			       len - 1, true);
+ 	} else if (IS_ALIGNED(len, 4)) {
+ 		/* Use single indirect_ref write32. */
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, xfer_num - 1));
+ 		emit_cmd_indir(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			       reg_a(meta->paired_st->dst_reg * 2), off,
+ 			       xfer_num - 1, true);
+ 	} else if (len <= 40) {
+ 		/* Use one direct_ref write32 to write the first 32-bytes, then
+ 		 * another direct_ref write8 to write the remaining bytes.
+ 		 */
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, 7,
+ 			 true);
+ 
+ 		off = re_load_imm_any(nfp_prog, meta->paired_st->off + 32,
+ 				      imm_b(nfp_prog));
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 8,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, len - 33,
+ 			 true);
+ 	} else {
+ 		/* Use one indirect_ref write32 to write 4-bytes aligned length,
+ 		 * then another direct_ref write8 to write the remaining bytes.
+ 		 */
+ 		u8 new_off;
+ 
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, xfer_num - 2));
+ 		emit_cmd_indir(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			       reg_a(meta->paired_st->dst_reg * 2), off,
+ 			       xfer_num - 2, true);
+ 		new_off = meta->paired_st->off + (xfer_num - 1) * 4;
+ 		off = re_load_imm_any(nfp_prog, new_off, imm_b(nfp_prog));
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b,
+ 			 xfer_num - 1, reg_a(meta->paired_st->dst_reg * 2), off,
+ 			 (len & 0x3) - 1, true);
+ 	}
+ 
+ 	/* TODO: The following extra load is to make sure data flow be identical
+ 	 *  before and after we do memory copy optimization.
+ 	 *
+ 	 *  The load destination register is not guaranteed to be dead, so we
+ 	 *  need to make sure it is loaded with the value the same as before
+ 	 *  this transformation.
+ 	 *
+ 	 *  These extra loads could be removed once we have accurate register
+ 	 *  usage information.
+ 	 */
+ 	if (descending_seq)
+ 		xfer_num = 0;
+ 	else if (BPF_SIZE(meta->insn.code) != BPF_DW)
+ 		xfer_num = xfer_num - 1;
+ 	else
+ 		xfer_num = xfer_num - 2;
+ 
+ 	switch (BPF_SIZE(meta->insn.code)) {
+ 	case BPF_B:
+ 		wrp_reg_subpart(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 				reg_xfer(xfer_num), 1,
+ 				IS_ALIGNED(len, 4) ? 3 : (len & 3) - 1);
+ 		break;
+ 	case BPF_H:
+ 		wrp_reg_subpart(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 				reg_xfer(xfer_num), 2, (len & 3) ^ 2);
+ 		break;
+ 	case BPF_W:
+ 		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 			reg_xfer(0));
+ 		break;
+ 	case BPF_DW:
+ 		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 			reg_xfer(xfer_num));
+ 		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2 + 1),
+ 			reg_xfer(xfer_num + 1));
+ 		break;
+ 	}
+ 
+ 	if (BPF_SIZE(meta->insn.code) != BPF_DW)
+ 		wrp_immed(nfp_prog, reg_both(meta->insn.dst_reg * 2 + 1), 0);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 3dd43c3319cb (nfp: bpf: add support for reading map memory)
  static int
  data_ld(struct nfp_prog *nfp_prog, swreg offset, u8 dst_gpr, int size)
  {
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/jit.c
diff --git a/drivers/net/ethernet/netronome/nfp/bpf/verifier.c b/drivers/net/ethernet/netronome/nfp/bpf/verifier.c
index 0f4ae869a0f1..bcd6d3145230 100644
--- a/drivers/net/ethernet/netronome/nfp/bpf/verifier.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/verifier.c
@@ -155,6 +155,7 @@ nfp_bpf_check_ptr(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,
 
 	if (reg->type != PTR_TO_CTX &&
 	    reg->type != PTR_TO_STACK &&
+	    reg->type != PTR_TO_MAP_VALUE &&
 	    reg->type != PTR_TO_PACKET) {
 		pr_info("unsupported ptr type: %d\n", reg->type);
 		return -EINVAL;
@@ -166,6 +167,13 @@ nfp_bpf_check_ptr(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,
 			return err;
 	}
 
+	if (reg->type == PTR_TO_MAP_VALUE) {
+		if (is_mbpf_store(meta)) {
+			pr_info("map writes not supported\n");
+			return -EOPNOTSUPP;
+		}
+	}
+
 	if (meta->ptr.type != NOT_INIT && meta->ptr.type != reg->type) {
 		pr_info("ptr type changed for instruction %d -> %d\n",
 			meta->ptr.type, reg->type);
