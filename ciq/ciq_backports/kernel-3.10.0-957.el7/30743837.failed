net: filter: make register naming more comprehensible

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [kernel] filter: make register naming more comprehensible (Jiri Olsa) [1311586]
Rebuild_FUZZ: 95.05%
commit-author Daniel Borkmann <dborkman@redhat.com>
commit 30743837dd204d2b04fd4e9d3db78cc7b118c81a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/30743837.failed

The current code is a bit hard to parse on which registers can be used,
how they are mapped and all play together. It makes much more sense to
define this a bit more clearly so that the code is a bit more intuitive.
This patch cleans this up, and makes naming a bit more consistent among
the code. This also allows for moving some of the defines into the header
file. Clearing of A and X registers in __sk_run_filter() do not get a
particular register name assigned as they have not an 'official' function,
but rather just result from the concrete initial mapping of old BPF
programs. Since for BPF helper functions for BPF_CALL we already use
small letters, so be consistent here as well. No functional changes.

	Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
	Acked-by: Alexei Starovoitov <ast@plumgrid.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 30743837dd204d2b04fd4e9d3db78cc7b118c81a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/filter.h
#	net/core/filter.c
diff --cc include/linux/filter.h
index d322ed880333,ed1efab10b8f..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -6,86 -6,155 +6,174 @@@
  
  #include <linux/atomic.h>
  #include <linux/compat.h>
 -#include <linux/workqueue.h>
  #include <uapi/linux/filter.h>
++<<<<<<< HEAD
 +#ifndef __GENKSYMS__
 +#include <net/sch_generic.h>
 +#endif
++=======
+ 
+ /* Internally used and optimized filter representation with extended
+  * instruction set based on top of classic BPF.
+  */
+ 
+ /* instruction classes */
+ #define BPF_ALU64	0x07	/* alu mode in double word width */
+ 
+ /* ld/ldx fields */
+ #define BPF_DW		0x18	/* double word */
+ #define BPF_XADD	0xc0	/* exclusive add */
+ 
+ /* alu/jmp fields */
+ #define BPF_MOV		0xb0	/* mov reg to reg */
+ #define BPF_ARSH	0xc0	/* sign extending arithmetic shift right */
+ 
+ /* change endianness of a register */
+ #define BPF_END		0xd0	/* flags for endianness conversion: */
+ #define BPF_TO_LE	0x00	/* convert to little-endian */
+ #define BPF_TO_BE	0x08	/* convert to big-endian */
+ #define BPF_FROM_LE	BPF_TO_LE
+ #define BPF_FROM_BE	BPF_TO_BE
+ 
+ #define BPF_JNE		0x50	/* jump != */
+ #define BPF_JSGT	0x60	/* SGT is signed '>', GT in x86 */
+ #define BPF_JSGE	0x70	/* SGE is signed '>=', GE in x86 */
+ #define BPF_CALL	0x80	/* function call */
+ #define BPF_EXIT	0x90	/* function return */
+ 
+ /* Placeholder/dummy for 0 */
+ #define BPF_0		0
+ 
+ /* Register numbers */
+ enum {
+ 	BPF_REG_0 = 0,
+ 	BPF_REG_1,
+ 	BPF_REG_2,
+ 	BPF_REG_3,
+ 	BPF_REG_4,
+ 	BPF_REG_5,
+ 	BPF_REG_6,
+ 	BPF_REG_7,
+ 	BPF_REG_8,
+ 	BPF_REG_9,
+ 	BPF_REG_10,
+ 	__MAX_BPF_REG,
+ };
+ 
+ /* BPF has 10 general purpose 64-bit registers and stack frame. */
+ #define MAX_BPF_REG	__MAX_BPF_REG
+ 
+ /* ArgX, context and stack frame pointer register positions. Note,
+  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
+  * calls in BPF_CALL instruction.
+  */
+ #define BPF_REG_ARG1	BPF_REG_1
+ #define BPF_REG_ARG2	BPF_REG_2
+ #define BPF_REG_ARG3	BPF_REG_3
+ #define BPF_REG_ARG4	BPF_REG_4
+ #define BPF_REG_ARG5	BPF_REG_5
+ #define BPF_REG_CTX	BPF_REG_6
+ #define BPF_REG_FP	BPF_REG_10
+ 
+ /* Additional register mappings for converted user programs. */
+ #define BPF_REG_A	BPF_REG_0
+ #define BPF_REG_X	BPF_REG_7
+ #define BPF_REG_TMP	BPF_REG_8
+ 
+ /* BPF program can access up to 512 bytes of stack space. */
+ #define MAX_BPF_STACK	512
+ 
+ /* Macro to invoke filter function. */
+ #define SK_RUN_FILTER(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
+ 
+ struct sock_filter_int {
+ 	__u8	code;		/* opcode */
+ 	__u8	a_reg:4;	/* dest register */
+ 	__u8	x_reg:4;	/* source register */
+ 	__s16	off;		/* signed offset */
+ 	__s32	imm;		/* signed immediate constant */
+ };
++>>>>>>> 30743837dd20 (net: filter: make register naming more comprehensible)
  
  #ifdef CONFIG_COMPAT
 -/* A struct sock_filter is architecture independent. */
 +/*
 + * A struct sock_filter is architecture independent.
 + */
  struct compat_sock_fprog {
  	u16		len;
 -	compat_uptr_t	filter;	/* struct sock_filter * */
 +	compat_uptr_t	filter;		/* struct sock_filter * */
  };
  #endif
  
 -struct sock_fprog_kern {
 -	u16			len;
 -	struct sock_filter	*filter;
 -};
 -
  struct sk_buff;
  struct sock;
 -struct seccomp_data;
 +struct bpf_prog_aux;
 +
 +struct bpf_prog
 +{
 +	struct bpf_prog_aux	*aux;	/* Auxiliary fields */
 +};
  
 -struct sk_filter {
 +struct sk_filter
 +{
  	atomic_t		refcnt;
 -	u32			jited:1,	/* Is our filter JIT'ed? */
 -				len:31;		/* Number of filter blocks */
 -	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 -	struct rcu_head		rcu;
 +	unsigned int         	len;	/* Number of filter blocks */
  	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 -					    const struct sock_filter_int *filter);
 -	union {
 -		struct sock_filter	insns[0];
 -		struct sock_filter_int	insnsi[0];
 -		struct work_struct	work;
 -	};
 +					    const struct sock_filter *filter);
 +	struct rcu_head		rcu;
 +	struct sock_filter     	insns[0];
  };
  
 -static inline unsigned int sk_filter_size(unsigned int proglen)
 +struct xdp_buff {
 +	void *data;
 +	void *data_end;
 +	void *data_hard_start;
 +};
 +
 +/* compute the linear packet data range [data, data_end) which
 + * will be accessed by cls_bpf and act_bpf programs
 + */
 +static inline unsigned int sk_filter_len(const struct sk_filter *fp)
  {
 -	return max(sizeof(struct sk_filter),
 -		   offsetof(struct sk_filter, insns[proglen]));
 +	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
  }
  
 -#define sk_filter_proglen(fprog)			\
 -		(fprog->len * sizeof(fprog->filter[0]))
 -
 -int sk_filter(struct sock *sk, struct sk_buff *skb);
 -
 -u32 sk_run_filter_int_seccomp(const struct seccomp_data *ctx,
 -			      const struct sock_filter_int *insni);
 -u32 sk_run_filter_int_skb(const struct sk_buff *ctx,
 -			  const struct sock_filter_int *insni);
 -
 -int sk_convert_filter(struct sock_filter *prog, int len,
 -		      struct sock_filter_int *new_prog, int *new_len);
 +static inline void bpf_compute_data_end(struct sk_buff *skb)
 +{
 +	return;
 +}
  
 -int sk_unattached_filter_create(struct sk_filter **pfp,
 -				struct sock_fprog *fprog);
 -void sk_unattached_filter_destroy(struct sk_filter *fp);
++<<<<<<< HEAD
 +int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
 +static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 +{
 +	return sk_filter_trim_cap(sk, skb, 1);
 +}
  
 -int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 -int sk_detach_filter(struct sock *sk);
 +extern unsigned int sk_run_filter(const struct sk_buff *skb,
 +				  const struct sock_filter *filter);
 +extern int sk_unattached_filter_create(struct sk_filter **pfp,
 +				       struct sock_fprog *fprog);
 +extern void sk_unattached_filter_destroy(struct sk_filter *fp);
 +extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 +extern int sk_detach_filter(struct sock *sk);
 +extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 +extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 +extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
++=======
++int sk_filter(struct sock *sk, struct sk_buff *skb);
++>>>>>>> 30743837dd20 (net: filter: make register naming more comprehensible)
  
 -int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 -int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 -		  unsigned int len);
 -void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
 +static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 +				   struct xdp_buff *xdp)
 +{
 +	return 0;
 +}
  
 -void sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 -void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 +static inline void bpf_warn_invalid_xdp_action(u32 act)
 +{
 +	return;
 +}
  
  #ifdef CONFIG_BPF_JIT
  #include <stdarg.h>
diff --cc net/core/filter.c
index 060ed5f86613,c93ca8d66f37..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -108,314 -132,1047 +129,1282 @@@ int sk_filter_trim_cap(struct sock *sk
  
  	return err;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(sk_filter_trim_cap);
++=======
+ EXPORT_SYMBOL(sk_filter);
+ 
+ /* Base function for offset calculation. Needs to go into .text section,
+  * therefore keeping it non-static as well; will also be used by JITs
+  * anyway later on, so do not let the compiler omit it.
+  */
+ noinline u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	return 0;
+ }
++>>>>>>> 30743837dd20 (net: filter: make register naming more comprehensible)
  
  /**
 - *	__sk_run_filter - run a filter on a given context
 - *	@ctx: buffer to run the filter on
 - *	@insn: filter to apply
 + *	sk_run_filter - run a filter on a socket
 + *	@skb: buffer to run the filter on
 + *	@fentry: filter to apply
   *
 - * Decode and apply filter instructions to the skb->data. Return length to
 - * keep, 0 for none. @ctx is the data we are operating on, @insn is the
 - * array of filter instructions.
 + * Decode and apply filter instructions to the skb->data.
 + * Return length to keep, 0 for none. @skb is the data we are
 + * filtering, @filter is the array of filter instructions.
 + * Because all jumps are guaranteed to be before last instruction,
 + * and last instruction guaranteed to be a RET, we dont need to check
 + * flen. (We used to pass to this function the length of filter)
   */
 -unsigned int __sk_run_filter(void *ctx, const struct sock_filter_int *insn)
 +unsigned int sk_run_filter(const struct sk_buff *skb,
 +			   const struct sock_filter *fentry)
  {
++<<<<<<< HEAD
 +	void *ptr;
 +	u32 A = 0;			/* Accumulator */
 +	u32 X = 0;			/* Index Register */
 +	u32 mem[BPF_MEMWORDS];		/* Scratch Memory Store */
 +	u32 tmp;
 +	int k;
 +
 +	/*
 +	 * Process array of filter instructions.
 +	 */
 +	for (;; fentry++) {
 +#if defined(CONFIG_X86_32)
 +#define	K (fentry->k)
 +#else
 +		const u32 K = fentry->k;
 +#endif
 +
 +		switch (fentry->code) {
 +		case BPF_S_ALU_ADD_X:
 +			A += X;
 +			continue;
 +		case BPF_S_ALU_ADD_K:
 +			A += K;
 +			continue;
 +		case BPF_S_ALU_SUB_X:
 +			A -= X;
 +			continue;
 +		case BPF_S_ALU_SUB_K:
 +			A -= K;
 +			continue;
 +		case BPF_S_ALU_MUL_X:
 +			A *= X;
 +			continue;
 +		case BPF_S_ALU_MUL_K:
 +			A *= K;
 +			continue;
 +		case BPF_S_ALU_DIV_X:
 +			if (X == 0)
 +				return 0;
 +			A /= X;
 +			continue;
 +		case BPF_S_ALU_DIV_K:
 +			A /= K;
 +			continue;
 +		case BPF_S_ALU_MOD_X:
 +			if (X == 0)
 +				return 0;
 +			A %= X;
 +			continue;
 +		case BPF_S_ALU_MOD_K:
 +			A %= K;
 +			continue;
 +		case BPF_S_ALU_AND_X:
 +			A &= X;
 +			continue;
 +		case BPF_S_ALU_AND_K:
 +			A &= K;
 +			continue;
 +		case BPF_S_ALU_OR_X:
 +			A |= X;
 +			continue;
 +		case BPF_S_ALU_OR_K:
 +			A |= K;
 +			continue;
 +		case BPF_S_ANC_ALU_XOR_X:
 +		case BPF_S_ALU_XOR_X:
 +			A ^= X;
 +			continue;
 +		case BPF_S_ALU_XOR_K:
 +			A ^= K;
 +			continue;
 +		case BPF_S_ALU_LSH_X:
 +			A <<= X;
 +			continue;
 +		case BPF_S_ALU_LSH_K:
 +			A <<= K;
 +			continue;
 +		case BPF_S_ALU_RSH_X:
 +			A >>= X;
 +			continue;
 +		case BPF_S_ALU_RSH_K:
 +			A >>= K;
 +			continue;
 +		case BPF_S_ALU_NEG:
 +			A = -A;
 +			continue;
 +		case BPF_S_JMP_JA:
 +			fentry += K;
 +			continue;
 +		case BPF_S_JMP_JGT_K:
 +			fentry += (A > K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_K:
 +			fentry += (A >= K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_K:
 +			fentry += (A == K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_K:
 +			fentry += (A & K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGT_X:
 +			fentry += (A > X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_X:
 +			fentry += (A >= X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_X:
 +			fentry += (A == X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_X:
 +			fentry += (A & X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_LD_W_ABS:
 +			k = K;
 +load_w:
 +			ptr = load_pointer(skb, k, 4, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be32(ptr);
 +				continue;
++=======
+ 	u64 stack[MAX_BPF_STACK / sizeof(u64)];
+ 	u64 regs[MAX_BPF_REG], tmp;
+ 	static const void *jumptable[256] = {
+ 		[0 ... 255] = &&default_label,
+ 		/* Now overwrite non-defaults ... */
+ #define DL(A, B, C)	[BPF_##A|BPF_##B|BPF_##C] = &&A##_##B##_##C
+ 		DL(ALU, ADD, X),
+ 		DL(ALU, ADD, K),
+ 		DL(ALU, SUB, X),
+ 		DL(ALU, SUB, K),
+ 		DL(ALU, AND, X),
+ 		DL(ALU, AND, K),
+ 		DL(ALU, OR, X),
+ 		DL(ALU, OR, K),
+ 		DL(ALU, LSH, X),
+ 		DL(ALU, LSH, K),
+ 		DL(ALU, RSH, X),
+ 		DL(ALU, RSH, K),
+ 		DL(ALU, XOR, X),
+ 		DL(ALU, XOR, K),
+ 		DL(ALU, MUL, X),
+ 		DL(ALU, MUL, K),
+ 		DL(ALU, MOV, X),
+ 		DL(ALU, MOV, K),
+ 		DL(ALU, DIV, X),
+ 		DL(ALU, DIV, K),
+ 		DL(ALU, MOD, X),
+ 		DL(ALU, MOD, K),
+ 		DL(ALU, NEG, 0),
+ 		DL(ALU, END, TO_BE),
+ 		DL(ALU, END, TO_LE),
+ 		DL(ALU64, ADD, X),
+ 		DL(ALU64, ADD, K),
+ 		DL(ALU64, SUB, X),
+ 		DL(ALU64, SUB, K),
+ 		DL(ALU64, AND, X),
+ 		DL(ALU64, AND, K),
+ 		DL(ALU64, OR, X),
+ 		DL(ALU64, OR, K),
+ 		DL(ALU64, LSH, X),
+ 		DL(ALU64, LSH, K),
+ 		DL(ALU64, RSH, X),
+ 		DL(ALU64, RSH, K),
+ 		DL(ALU64, XOR, X),
+ 		DL(ALU64, XOR, K),
+ 		DL(ALU64, MUL, X),
+ 		DL(ALU64, MUL, K),
+ 		DL(ALU64, MOV, X),
+ 		DL(ALU64, MOV, K),
+ 		DL(ALU64, ARSH, X),
+ 		DL(ALU64, ARSH, K),
+ 		DL(ALU64, DIV, X),
+ 		DL(ALU64, DIV, K),
+ 		DL(ALU64, MOD, X),
+ 		DL(ALU64, MOD, K),
+ 		DL(ALU64, NEG, 0),
+ 		DL(JMP, CALL, 0),
+ 		DL(JMP, JA, 0),
+ 		DL(JMP, JEQ, X),
+ 		DL(JMP, JEQ, K),
+ 		DL(JMP, JNE, X),
+ 		DL(JMP, JNE, K),
+ 		DL(JMP, JGT, X),
+ 		DL(JMP, JGT, K),
+ 		DL(JMP, JGE, X),
+ 		DL(JMP, JGE, K),
+ 		DL(JMP, JSGT, X),
+ 		DL(JMP, JSGT, K),
+ 		DL(JMP, JSGE, X),
+ 		DL(JMP, JSGE, K),
+ 		DL(JMP, JSET, X),
+ 		DL(JMP, JSET, K),
+ 		DL(JMP, EXIT, 0),
+ 		DL(STX, MEM, B),
+ 		DL(STX, MEM, H),
+ 		DL(STX, MEM, W),
+ 		DL(STX, MEM, DW),
+ 		DL(STX, XADD, W),
+ 		DL(STX, XADD, DW),
+ 		DL(ST, MEM, B),
+ 		DL(ST, MEM, H),
+ 		DL(ST, MEM, W),
+ 		DL(ST, MEM, DW),
+ 		DL(LDX, MEM, B),
+ 		DL(LDX, MEM, H),
+ 		DL(LDX, MEM, W),
+ 		DL(LDX, MEM, DW),
+ 		DL(LD, ABS, W),
+ 		DL(LD, ABS, H),
+ 		DL(LD, ABS, B),
+ 		DL(LD, IND, W),
+ 		DL(LD, IND, H),
+ 		DL(LD, IND, B),
+ #undef DL
+ 	};
+ 	void *ptr;
+ 	int off;
+ 
+ #define CONT	 ({ insn++; goto select_insn; })
+ #define CONT_JMP ({ insn++; goto select_insn; })
+ 
+ 	FP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)];
+ 	ARG1 = (u64) (unsigned long) ctx;
+ 
+ 	/* Register for user BPF programs need to be reset first. */
+ 	regs[BPF_REG_A] = 0;
+ 	regs[BPF_REG_X] = 0;
+ 
+ select_insn:
+ 	goto *jumptable[insn->code];
+ 
+ 	/* ALU */
+ #define ALU(OPCODE, OP)			\
+ 	ALU64_##OPCODE##_X:		\
+ 		A = A OP X;		\
+ 		CONT;			\
+ 	ALU_##OPCODE##_X:		\
+ 		A = (u32) A OP (u32) X;	\
+ 		CONT;			\
+ 	ALU64_##OPCODE##_K:		\
+ 		A = A OP K;		\
+ 		CONT;			\
+ 	ALU_##OPCODE##_K:		\
+ 		A = (u32) A OP (u32) K;	\
+ 		CONT;
+ 
+ 	ALU(ADD,  +)
+ 	ALU(SUB,  -)
+ 	ALU(AND,  &)
+ 	ALU(OR,   |)
+ 	ALU(LSH, <<)
+ 	ALU(RSH, >>)
+ 	ALU(XOR,  ^)
+ 	ALU(MUL,  *)
+ #undef ALU
+ 	ALU_NEG_0:
+ 		A = (u32) -A;
+ 		CONT;
+ 	ALU64_NEG_0:
+ 		A = -A;
+ 		CONT;
+ 	ALU_MOV_X:
+ 		A = (u32) X;
+ 		CONT;
+ 	ALU_MOV_K:
+ 		A = (u32) K;
+ 		CONT;
+ 	ALU64_MOV_X:
+ 		A = X;
+ 		CONT;
+ 	ALU64_MOV_K:
+ 		A = K;
+ 		CONT;
+ 	ALU64_ARSH_X:
+ 		(*(s64 *) &A) >>= X;
+ 		CONT;
+ 	ALU64_ARSH_K:
+ 		(*(s64 *) &A) >>= K;
+ 		CONT;
+ 	ALU64_MOD_X:
+ 		if (unlikely(X == 0))
+ 			return 0;
+ 		tmp = A;
+ 		A = do_div(tmp, X);
+ 		CONT;
+ 	ALU_MOD_X:
+ 		if (unlikely(X == 0))
+ 			return 0;
+ 		tmp = (u32) A;
+ 		A = do_div(tmp, (u32) X);
+ 		CONT;
+ 	ALU64_MOD_K:
+ 		tmp = A;
+ 		A = do_div(tmp, K);
+ 		CONT;
+ 	ALU_MOD_K:
+ 		tmp = (u32) A;
+ 		A = do_div(tmp, (u32) K);
+ 		CONT;
+ 	ALU64_DIV_X:
+ 		if (unlikely(X == 0))
+ 			return 0;
+ 		do_div(A, X);
+ 		CONT;
+ 	ALU_DIV_X:
+ 		if (unlikely(X == 0))
+ 			return 0;
+ 		tmp = (u32) A;
+ 		do_div(tmp, (u32) X);
+ 		A = (u32) tmp;
+ 		CONT;
+ 	ALU64_DIV_K:
+ 		do_div(A, K);
+ 		CONT;
+ 	ALU_DIV_K:
+ 		tmp = (u32) A;
+ 		do_div(tmp, (u32) K);
+ 		A = (u32) tmp;
+ 		CONT;
+ 	ALU_END_TO_BE:
+ 		switch (K) {
+ 		case 16:
+ 			A = (__force u16) cpu_to_be16(A);
+ 			break;
+ 		case 32:
+ 			A = (__force u32) cpu_to_be32(A);
+ 			break;
+ 		case 64:
+ 			A = (__force u64) cpu_to_be64(A);
+ 			break;
+ 		}
+ 		CONT;
+ 	ALU_END_TO_LE:
+ 		switch (K) {
+ 		case 16:
+ 			A = (__force u16) cpu_to_le16(A);
+ 			break;
+ 		case 32:
+ 			A = (__force u32) cpu_to_le32(A);
+ 			break;
+ 		case 64:
+ 			A = (__force u64) cpu_to_le64(A);
+ 			break;
+ 		}
+ 		CONT;
+ 
+ 	/* CALL */
+ 	JMP_CALL_0:
+ 		/* Function call scratches R1-R5 registers, preserves R6-R9,
+ 		 * and stores return value into R0.
+ 		 */
+ 		R0 = (__bpf_call_base + insn->imm)(R1, R2, R3, R4, R5);
+ 		CONT;
+ 
+ 	/* JMP */
+ 	JMP_JA_0:
+ 		insn += insn->off;
+ 		CONT;
+ 	JMP_JEQ_X:
+ 		if (A == X) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JEQ_K:
+ 		if (A == K) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JNE_X:
+ 		if (A != X) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JNE_K:
+ 		if (A != K) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGT_X:
+ 		if (A > X) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGT_K:
+ 		if (A > K) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGE_X:
+ 		if (A >= X) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JGE_K:
+ 		if (A >= K) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGT_X:
+ 		if (((s64) A) > ((s64) X)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGT_K:
+ 		if (((s64) A) > ((s64) K)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGE_X:
+ 		if (((s64) A) >= ((s64) X)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSGE_K:
+ 		if (((s64) A) >= ((s64) K)) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSET_X:
+ 		if (A & X) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_JSET_K:
+ 		if (A & K) {
+ 			insn += insn->off;
+ 			CONT_JMP;
+ 		}
+ 		CONT;
+ 	JMP_EXIT_0:
+ 		return R0;
+ 
+ 	/* STX and ST and LDX*/
+ #define LDST(SIZEOP, SIZE)					\
+ 	STX_MEM_##SIZEOP:					\
+ 		*(SIZE *)(unsigned long) (A + insn->off) = X;	\
+ 		CONT;						\
+ 	ST_MEM_##SIZEOP:					\
+ 		*(SIZE *)(unsigned long) (A + insn->off) = K;	\
+ 		CONT;						\
+ 	LDX_MEM_##SIZEOP:					\
+ 		A = *(SIZE *)(unsigned long) (X + insn->off);	\
+ 		CONT;
+ 
+ 	LDST(B,   u8)
+ 	LDST(H,  u16)
+ 	LDST(W,  u32)
+ 	LDST(DW, u64)
+ #undef LDST
+ 	STX_XADD_W: /* lock xadd *(u32 *)(A + insn->off) += X */
+ 		atomic_add((u32) X, (atomic_t *)(unsigned long)
+ 			   (A + insn->off));
+ 		CONT;
+ 	STX_XADD_DW: /* lock xadd *(u64 *)(A + insn->off) += X */
+ 		atomic64_add((u64) X, (atomic64_t *)(unsigned long)
+ 			     (A + insn->off));
+ 		CONT;
+ 	LD_ABS_W: /* R0 = ntohl(*(u32 *) (skb->data + K)) */
+ 		off = K;
+ load_word:
+ 		/* BPF_LD + BPD_ABS and BPF_LD + BPF_IND insns are only
+ 		 * appearing in the programs where ctx == skb. All programs
+ 		 * keep 'ctx' in regs[BPF_REG_CTX] == R6, sk_convert_filter()
+ 		 * saves it in R6, internal BPF verifier will check that
+ 		 * R6 == ctx.
+ 		 *
+ 		 * BPF_ABS and BPF_IND are wrappers of function calls, so
+ 		 * they scratch R1-R5 registers, preserve R6-R9, and store
+ 		 * return value into R0.
+ 		 *
+ 		 * Implicit input:
+ 		 *   ctx
+ 		 *
+ 		 * Explicit input:
+ 		 *   X == any register
+ 		 *   K == 32-bit immediate
+ 		 *
+ 		 * Output:
+ 		 *   R0 - 8/16/32-bit skb data converted to cpu endianness
+ 		 */
+ 		ptr = load_pointer((struct sk_buff *) ctx, off, 4, &tmp);
+ 		if (likely(ptr != NULL)) {
+ 			R0 = get_unaligned_be32(ptr);
+ 			CONT;
+ 		}
+ 		return 0;
+ 	LD_ABS_H: /* R0 = ntohs(*(u16 *) (skb->data + K)) */
+ 		off = K;
+ load_half:
+ 		ptr = load_pointer((struct sk_buff *) ctx, off, 2, &tmp);
+ 		if (likely(ptr != NULL)) {
+ 			R0 = get_unaligned_be16(ptr);
+ 			CONT;
+ 		}
+ 		return 0;
+ 	LD_ABS_B: /* R0 = *(u8 *) (ctx + K) */
+ 		off = K;
+ load_byte:
+ 		ptr = load_pointer((struct sk_buff *) ctx, off, 1, &tmp);
+ 		if (likely(ptr != NULL)) {
+ 			R0 = *(u8 *)ptr;
+ 			CONT;
+ 		}
+ 		return 0;
+ 	LD_IND_W: /* R0 = ntohl(*(u32 *) (skb->data + X + K)) */
+ 		off = K + X;
+ 		goto load_word;
+ 	LD_IND_H: /* R0 = ntohs(*(u16 *) (skb->data + X + K)) */
+ 		off = K + X;
+ 		goto load_half;
+ 	LD_IND_B: /* R0 = *(u8 *) (skb->data + X + K) */
+ 		off = K + X;
+ 		goto load_byte;
+ 
+ 	default_label:
+ 		/* If we ever reach this, we have a bug somewhere. */
+ 		WARN_RATELIMIT(1, "unknown opcode %02x\n", insn->code);
+ 		return 0;
+ }
+ 
+ u32 sk_run_filter_int_seccomp(const struct seccomp_data *ctx,
+ 			      const struct sock_filter_int *insni)
+     __attribute__ ((alias ("__sk_run_filter")));
+ 
+ u32 sk_run_filter_int_skb(const struct sk_buff *ctx,
+ 			  const struct sock_filter_int *insni)
+     __attribute__ ((alias ("__sk_run_filter")));
+ EXPORT_SYMBOL_GPL(sk_run_filter_int_skb);
+ 
+ /* Helper to find the offset of pkt_type in sk_buff structure. We want
+  * to make sure its still a 3bit field starting at a byte boundary;
+  * taken from arch/x86/net/bpf_jit_comp.c.
+  */
+ #define PKT_TYPE_MAX	7
+ static unsigned int pkt_type_offset(void)
+ {
+ 	struct sk_buff skb_probe = { .pkt_type = ~0, };
+ 	u8 *ct = (u8 *) &skb_probe;
+ 	unsigned int off;
+ 
+ 	for (off = 0; off < sizeof(struct sk_buff); off++) {
+ 		if (ct[off] == PKT_TYPE_MAX)
+ 			return off;
+ 	}
+ 
+ 	pr_err_once("Please fix %s, as pkt_type couldn't be found!\n", __func__);
+ 	return -1;
+ }
+ 
+ static u64 __skb_get_pay_offset(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *)(long) ctx;
+ 
+ 	return __skb_get_poff(skb);
+ }
+ 
+ static u64 __skb_get_nlattr(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *)(long) ctx;
+ 	struct nlattr *nla;
+ 
+ 	if (skb_is_nonlinear(skb))
+ 		return 0;
+ 
+ 	if (skb->len < sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	if (a > skb->len - sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	nla = nla_find((struct nlattr *) &skb->data[a], skb->len - a, x);
+ 	if (nla)
+ 		return (void *) nla - (void *) skb->data;
+ 
+ 	return 0;
+ }
+ 
+ static u64 __skb_get_nlattr_nest(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *)(long) ctx;
+ 	struct nlattr *nla;
+ 
+ 	if (skb_is_nonlinear(skb))
+ 		return 0;
+ 
+ 	if (skb->len < sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	if (a > skb->len - sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	nla = (struct nlattr *) &skb->data[a];
+ 	if (nla->nla_len > skb->len - a)
+ 		return 0;
+ 
+ 	nla = nla_find_nested(nla, x);
+ 	if (nla)
+ 		return (void *) nla - (void *) skb->data;
+ 
+ 	return 0;
+ }
+ 
+ static u64 __get_raw_cpu_id(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	return raw_smp_processor_id();
+ }
+ 
+ /* note that this only generates 32-bit random numbers */
+ static u64 __get_random_u32(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	return (u64)prandom_u32();
+ }
+ 
+ static bool convert_bpf_extensions(struct sock_filter *fp,
+ 				   struct sock_filter_int **insnp)
+ {
+ 	struct sock_filter_int *insn = *insnp;
+ 
+ 	switch (fp->k) {
+ 	case SKF_AD_OFF + SKF_AD_PROTOCOL:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
+ 
+ 		insn->code = BPF_LDX | BPF_MEM | BPF_H;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn->off = offsetof(struct sk_buff, protocol);
+ 		insn++;
+ 
+ 		/* A = ntohs(A) [emitting a nop or swap16] */
+ 		insn->code = BPF_ALU | BPF_END | BPF_FROM_BE;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->imm = 16;
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_PKTTYPE:
+ 		insn->code = BPF_LDX | BPF_MEM | BPF_B;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn->off = pkt_type_offset();
+ 		if (insn->off < 0)
+ 			return false;
+ 		insn++;
+ 
+ 		insn->code = BPF_ALU | BPF_AND | BPF_K;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->imm = PKT_TYPE_MAX;
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_IFINDEX:
+ 	case SKF_AD_OFF + SKF_AD_HATYPE:
+ 		if (FIELD_SIZEOF(struct sk_buff, dev) == 8)
+ 			insn->code = BPF_LDX | BPF_MEM | BPF_DW;
+ 		else
+ 			insn->code = BPF_LDX | BPF_MEM | BPF_W;
+ 		insn->a_reg = BPF_REG_TMP;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn->off = offsetof(struct sk_buff, dev);
+ 		insn++;
+ 
+ 		insn->code = BPF_JMP | BPF_JNE | BPF_K;
+ 		insn->a_reg = BPF_REG_TMP;
+ 		insn->imm = 0;
+ 		insn->off = 1;
+ 		insn++;
+ 
+ 		insn->code = BPF_JMP | BPF_EXIT;
+ 		insn++;
+ 
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
+ 
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_TMP;
+ 
+ 		if (fp->k == SKF_AD_OFF + SKF_AD_IFINDEX) {
+ 			insn->code = BPF_LDX | BPF_MEM | BPF_W;
+ 			insn->off = offsetof(struct net_device, ifindex);
+ 		} else {
+ 			insn->code = BPF_LDX | BPF_MEM | BPF_H;
+ 			insn->off = offsetof(struct net_device, type);
+ 		}
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_MARK:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
+ 
+ 		insn->code = BPF_LDX | BPF_MEM | BPF_W;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn->off = offsetof(struct sk_buff, mark);
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_RXHASH:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
+ 
+ 		insn->code = BPF_LDX | BPF_MEM | BPF_W;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn->off = offsetof(struct sk_buff, hash);
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_QUEUE:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
+ 
+ 		insn->code = BPF_LDX | BPF_MEM | BPF_H;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn->off = offsetof(struct sk_buff, queue_mapping);
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_VLAN_TAG:
+ 	case SKF_AD_OFF + SKF_AD_VLAN_TAG_PRESENT:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
+ 
+ 		insn->code = BPF_LDX | BPF_MEM | BPF_H;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn->off = offsetof(struct sk_buff, vlan_tci);
+ 		insn++;
+ 
+ 		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
+ 
+ 		if (fp->k == SKF_AD_OFF + SKF_AD_VLAN_TAG) {
+ 			insn->code = BPF_ALU | BPF_AND | BPF_K;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->imm = ~VLAN_TAG_PRESENT;
+ 		} else {
+ 			insn->code = BPF_ALU | BPF_RSH | BPF_K;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->imm = 12;
+ 			insn++;
+ 
+ 			insn->code = BPF_ALU | BPF_AND | BPF_K;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->imm = 1;
+ 		}
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
+ 	case SKF_AD_OFF + SKF_AD_NLATTR:
+ 	case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
+ 	case SKF_AD_OFF + SKF_AD_CPU:
+ 	case SKF_AD_OFF + SKF_AD_RANDOM:
+ 		/* arg1 = ctx */
+ 		insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 		insn->a_reg = BPF_REG_ARG1;
+ 		insn->x_reg = BPF_REG_CTX;
+ 		insn++;
+ 
+ 		/* arg2 = A */
+ 		insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 		insn->a_reg = BPF_REG_ARG2;
+ 		insn->x_reg = BPF_REG_A;
+ 		insn++;
+ 
+ 		/* arg3 = X */
+ 		insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 		insn->a_reg = BPF_REG_ARG3;
+ 		insn->x_reg = BPF_REG_X;
+ 		insn++;
+ 
+ 		/* Emit call(ctx, arg2=A, arg3=X) */
+ 		insn->code = BPF_JMP | BPF_CALL;
+ 		switch (fp->k) {
+ 		case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
+ 			insn->imm = __skb_get_pay_offset - __bpf_call_base;
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_NLATTR:
+ 			insn->imm = __skb_get_nlattr - __bpf_call_base;
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
+ 			insn->imm = __skb_get_nlattr_nest - __bpf_call_base;
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_CPU:
+ 			insn->imm = __get_raw_cpu_id - __bpf_call_base;
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_RANDOM:
+ 			insn->imm = __get_random_u32 - __bpf_call_base;
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_ALU_XOR_X:
+ 		insn->code = BPF_ALU | BPF_XOR | BPF_X;
+ 		insn->a_reg = BPF_REG_A;
+ 		insn->x_reg = BPF_REG_X;
+ 		break;
+ 
+ 	default:
+ 		/* This is just a dummy call to avoid letting the compiler
+ 		 * evict __bpf_call_base() as an optimization. Placed here
+ 		 * where no-one bothers.
+ 		 */
+ 		BUG_ON(__bpf_call_base(0, 0, 0, 0, 0) != 0);
+ 		return false;
+ 	}
+ 
+ 	*insnp = insn;
+ 	return true;
+ }
+ 
+ /**
+  *	sk_convert_filter - convert filter program
+  *	@prog: the user passed filter program
+  *	@len: the length of the user passed filter program
+  *	@new_prog: buffer where converted program will be stored
+  *	@new_len: pointer to store length of converted program
+  *
+  * Remap 'sock_filter' style BPF instruction set to 'sock_filter_ext' style.
+  * Conversion workflow:
+  *
+  * 1) First pass for calculating the new program length:
+  *   sk_convert_filter(old_prog, old_len, NULL, &new_len)
+  *
+  * 2) 2nd pass to remap in two passes: 1st pass finds new
+  *    jump offsets, 2nd pass remapping:
+  *   new_prog = kmalloc(sizeof(struct sock_filter_int) * new_len);
+  *   sk_convert_filter(old_prog, old_len, new_prog, &new_len);
+  *
+  * User BPF's register A is mapped to our BPF register 6, user BPF
+  * register X is mapped to BPF register 7; frame pointer is always
+  * register 10; Context 'void *ctx' is stored in register 1, that is,
+  * for socket filters: ctx == 'struct sk_buff *', for seccomp:
+  * ctx == 'struct seccomp_data *'.
+  */
+ int sk_convert_filter(struct sock_filter *prog, int len,
+ 		      struct sock_filter_int *new_prog, int *new_len)
+ {
+ 	int new_flen = 0, pass = 0, target, i;
+ 	struct sock_filter_int *new_insn;
+ 	struct sock_filter *fp;
+ 	int *addrs = NULL;
+ 	u8 bpf_src;
+ 
+ 	BUILD_BUG_ON(BPF_MEMWORDS * sizeof(u32) > MAX_BPF_STACK);
+ 	BUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);
+ 
+ 	if (len <= 0 || len >= BPF_MAXINSNS)
+ 		return -EINVAL;
+ 
+ 	if (new_prog) {
+ 		addrs = kzalloc(len * sizeof(*addrs), GFP_KERNEL);
+ 		if (!addrs)
+ 			return -ENOMEM;
+ 	}
+ 
+ do_pass:
+ 	new_insn = new_prog;
+ 	fp = prog;
+ 
+ 	if (new_insn) {
+ 		new_insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 		new_insn->a_reg = BPF_REG_CTX;
+ 		new_insn->x_reg = BPF_REG_ARG1;
+ 	}
+ 	new_insn++;
+ 
+ 	for (i = 0; i < len; fp++, i++) {
+ 		struct sock_filter_int tmp_insns[6] = { };
+ 		struct sock_filter_int *insn = tmp_insns;
+ 
+ 		if (addrs)
+ 			addrs[i] = new_insn - new_prog;
+ 
+ 		switch (fp->code) {
+ 		/* All arithmetic insns and skb loads map as-is. */
+ 		case BPF_ALU | BPF_ADD | BPF_X:
+ 		case BPF_ALU | BPF_ADD | BPF_K:
+ 		case BPF_ALU | BPF_SUB | BPF_X:
+ 		case BPF_ALU | BPF_SUB | BPF_K:
+ 		case BPF_ALU | BPF_AND | BPF_X:
+ 		case BPF_ALU | BPF_AND | BPF_K:
+ 		case BPF_ALU | BPF_OR | BPF_X:
+ 		case BPF_ALU | BPF_OR | BPF_K:
+ 		case BPF_ALU | BPF_LSH | BPF_X:
+ 		case BPF_ALU | BPF_LSH | BPF_K:
+ 		case BPF_ALU | BPF_RSH | BPF_X:
+ 		case BPF_ALU | BPF_RSH | BPF_K:
+ 		case BPF_ALU | BPF_XOR | BPF_X:
+ 		case BPF_ALU | BPF_XOR | BPF_K:
+ 		case BPF_ALU | BPF_MUL | BPF_X:
+ 		case BPF_ALU | BPF_MUL | BPF_K:
+ 		case BPF_ALU | BPF_DIV | BPF_X:
+ 		case BPF_ALU | BPF_DIV | BPF_K:
+ 		case BPF_ALU | BPF_MOD | BPF_X:
+ 		case BPF_ALU | BPF_MOD | BPF_K:
+ 		case BPF_ALU | BPF_NEG:
+ 		case BPF_LD | BPF_ABS | BPF_W:
+ 		case BPF_LD | BPF_ABS | BPF_H:
+ 		case BPF_LD | BPF_ABS | BPF_B:
+ 		case BPF_LD | BPF_IND | BPF_W:
+ 		case BPF_LD | BPF_IND | BPF_H:
+ 		case BPF_LD | BPF_IND | BPF_B:
+ 			/* Check for overloaded BPF extension and
+ 			 * directly convert it if found, otherwise
+ 			 * just move on with mapping.
+ 			 */
+ 			if (BPF_CLASS(fp->code) == BPF_LD &&
+ 			    BPF_MODE(fp->code) == BPF_ABS &&
+ 			    convert_bpf_extensions(fp, &insn))
+ 				break;
+ 
+ 			insn->code = fp->code;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->x_reg = BPF_REG_X;
+ 			insn->imm = fp->k;
+ 			break;
+ 
+ 		/* Jump opcodes map as-is, but offsets need adjustment. */
+ 		case BPF_JMP | BPF_JA:
+ 			target = i + fp->k + 1;
+ 			insn->code = fp->code;
+ #define EMIT_JMP							\
+ 	do {								\
+ 		if (target >= len || target < 0)			\
+ 			goto err;					\
+ 		insn->off = addrs ? addrs[target] - addrs[i] - 1 : 0;	\
+ 		/* Adjust pc relative offset for 2nd or 3rd insn. */	\
+ 		insn->off -= insn - tmp_insns;				\
+ 	} while (0)
+ 
+ 			EMIT_JMP;
+ 			break;
+ 
+ 		case BPF_JMP | BPF_JEQ | BPF_K:
+ 		case BPF_JMP | BPF_JEQ | BPF_X:
+ 		case BPF_JMP | BPF_JSET | BPF_K:
+ 		case BPF_JMP | BPF_JSET | BPF_X:
+ 		case BPF_JMP | BPF_JGT | BPF_K:
+ 		case BPF_JMP | BPF_JGT | BPF_X:
+ 		case BPF_JMP | BPF_JGE | BPF_K:
+ 		case BPF_JMP | BPF_JGE | BPF_X:
+ 			if (BPF_SRC(fp->code) == BPF_K && (int) fp->k < 0) {
+ 				/* BPF immediates are signed, zero extend
+ 				 * immediate into tmp register and use it
+ 				 * in compare insn.
+ 				 */
+ 				insn->code = BPF_ALU | BPF_MOV | BPF_K;
+ 				insn->a_reg = BPF_REG_TMP;
+ 				insn->imm = fp->k;
+ 				insn++;
+ 
+ 				insn->a_reg = BPF_REG_A;
+ 				insn->x_reg = BPF_REG_TMP;
+ 				bpf_src = BPF_X;
+ 			} else {
+ 				insn->a_reg = BPF_REG_A;
+ 				insn->x_reg = BPF_REG_X;
+ 				insn->imm = fp->k;
+ 				bpf_src = BPF_SRC(fp->code);
++>>>>>>> 30743837dd20 (net: filter: make register naming more comprehensible)
  			}
 -
 -			/* Common case where 'jump_false' is next insn. */
 -			if (fp->jf == 0) {
 -				insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -				target = i + fp->jt + 1;
 -				EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_H_ABS:
 +			k = K;
 +load_h:
 +			ptr = load_pointer(skb, k, 2, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be16(ptr);
 +				continue;
  			}
 -
 -			/* Convert JEQ into JNE when 'jump_true' is next insn. */
 -			if (fp->jt == 0 && BPF_OP(fp->code) == BPF_JEQ) {
 -				insn->code = BPF_JMP | BPF_JNE | bpf_src;
 -				target = i + fp->jf + 1;
 -				EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_B_ABS:
 +			k = K;
 +load_b:
 +			ptr = load_pointer(skb, k, 1, &tmp);
 +			if (ptr != NULL) {
 +				A = *(u8 *)ptr;
 +				continue;
  			}
 +			return 0;
 +		case BPF_S_LD_W_LEN:
 +			A = skb->len;
 +			continue;
 +		case BPF_S_LDX_W_LEN:
 +			X = skb->len;
 +			continue;
 +		case BPF_S_LD_W_IND:
 +			k = X + K;
 +			goto load_w;
 +		case BPF_S_LD_H_IND:
 +			k = X + K;
 +			goto load_h;
 +		case BPF_S_LD_B_IND:
 +			k = X + K;
 +			goto load_b;
 +		case BPF_S_LDX_B_MSH:
 +			ptr = load_pointer(skb, K, 1, &tmp);
 +			if (ptr != NULL) {
 +				X = (*(u8 *)ptr & 0xf) << 2;
 +				continue;
 +			}
 +			return 0;
 +		case BPF_S_LD_IMM:
 +			A = K;
 +			continue;
 +		case BPF_S_LDX_IMM:
 +			X = K;
 +			continue;
 +		case BPF_S_LD_MEM:
 +			A = mem[K];
 +			continue;
 +		case BPF_S_LDX_MEM:
 +			X = mem[K];
 +			continue;
 +		case BPF_S_MISC_TAX:
 +			X = A;
 +			continue;
 +		case BPF_S_MISC_TXA:
 +			A = X;
 +			continue;
 +		case BPF_S_RET_K:
 +			return K;
 +		case BPF_S_RET_A:
 +			return A;
 +		case BPF_S_ST:
 +			mem[K] = A;
 +			continue;
 +		case BPF_S_STX:
 +			mem[K] = X;
 +			continue;
 +		case BPF_S_ANC_PROTOCOL:
 +			A = ntohs(skb->protocol);
 +			continue;
 +		case BPF_S_ANC_PKTTYPE:
 +			A = skb->pkt_type;
 +			continue;
 +		case BPF_S_ANC_IFINDEX:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->ifindex;
 +			continue;
 +		case BPF_S_ANC_MARK:
 +			A = skb->mark;
 +			continue;
 +		case BPF_S_ANC_QUEUE:
 +			A = skb->queue_mapping;
 +			continue;
 +		case BPF_S_ANC_HATYPE:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->type;
 +			continue;
 +		case BPF_S_ANC_RXHASH:
 +			A = skb->hash;
 +			continue;
 +		case BPF_S_ANC_CPU:
 +			A = raw_smp_processor_id();
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG:
 +			A = skb_vlan_tag_get(skb);
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG_PRESENT:
 +			A = !!skb_vlan_tag_present(skb);
 +			continue;
 +		case BPF_S_ANC_PAY_OFFSET:
 +			A = skb_get_poff(skb);
 +			continue;
 +		case BPF_S_ANC_NLATTR: {
 +			struct nlattr *nla;
 +
 +			if (skb_is_nonlinear(skb))
 +				return 0;
 +
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
 +
++<<<<<<< HEAD
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
 +
 +			nla = nla_find((struct nlattr *)&skb->data[A],
 +				       skb->len - A, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +		case BPF_S_ANC_NLATTR_NEST: {
 +			struct nlattr *nla;
  
 -			/* Other jumps are mapped into two insns: Jxx and JA. */
 -			target = i + fp->jt + 1;
 -			insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -			EMIT_JMP;
 -			insn++;
 +			if (skb_is_nonlinear(skb))
 +				return 0;
  
 -			insn->code = BPF_JMP | BPF_JA;
 -			target = i + fp->jf + 1;
 -			EMIT_JMP;
 -			break;
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
  
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
 +
 +			nla = (struct nlattr *)&skb->data[A];
 +			if (nla->nla_len > skb->len - A)
 +				return 0;
++=======
+ 		/* ldxb 4 * ([14] & 0xf) is remaped into 6 insns. */
+ 		case BPF_LDX | BPF_MSH | BPF_B:
+ 			insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 			insn->a_reg = BPF_REG_TMP;
+ 			insn->x_reg = BPF_REG_A;
+ 			insn++;
+ 
+ 			insn->code = BPF_LD | BPF_ABS | BPF_B;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->imm = fp->k;
+ 			insn++;
+ 
+ 			insn->code = BPF_ALU | BPF_AND | BPF_K;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->imm = 0xf;
+ 			insn++;
+ 
+ 			insn->code = BPF_ALU | BPF_LSH | BPF_K;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->imm = 2;
+ 			insn++;
+ 
+ 			insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 			insn->a_reg = BPF_REG_X;
+ 			insn->x_reg = BPF_REG_A;
+ 			insn++;
+ 
+ 			insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->x_reg = BPF_REG_TMP;
+ 			break;
+ 
+ 		/* RET_K, RET_A are remaped into 2 insns. */
+ 		case BPF_RET | BPF_A:
+ 		case BPF_RET | BPF_K:
+ 			insn->code = BPF_ALU | BPF_MOV |
+ 				     (BPF_RVAL(fp->code) == BPF_K ?
+ 				      BPF_K : BPF_X);
+ 			insn->a_reg = 0;
+ 			insn->x_reg = BPF_REG_A;
+ 			insn->imm = fp->k;
+ 			insn++;
+ 
+ 			insn->code = BPF_JMP | BPF_EXIT;
+ 			break;
+ 
+ 		/* Store to stack. */
+ 		case BPF_ST:
+ 		case BPF_STX:
+ 			insn->code = BPF_STX | BPF_MEM | BPF_W;
+ 			insn->a_reg = BPF_REG_FP;
+ 			insn->x_reg = fp->code == BPF_ST ?
+ 				      BPF_REG_A : BPF_REG_X;
+ 			insn->off = -(BPF_MEMWORDS - fp->k) * 4;
+ 			break;
+ 
+ 		/* Load from stack. */
+ 		case BPF_LD | BPF_MEM:
+ 		case BPF_LDX | BPF_MEM:
+ 			insn->code = BPF_LDX | BPF_MEM | BPF_W;
+ 			insn->a_reg = BPF_CLASS(fp->code) == BPF_LD ?
+ 				      BPF_REG_A : BPF_REG_X;
+ 			insn->x_reg = BPF_REG_FP;
+ 			insn->off = -(BPF_MEMWORDS - fp->k) * 4;
+ 			break;
+ 
+ 		/* A = K or X = K */
+ 		case BPF_LD | BPF_IMM:
+ 		case BPF_LDX | BPF_IMM:
+ 			insn->code = BPF_ALU | BPF_MOV | BPF_K;
+ 			insn->a_reg = BPF_CLASS(fp->code) == BPF_LD ?
+ 				      BPF_REG_A : BPF_REG_X;
+ 			insn->imm = fp->k;
+ 			break;
+ 
+ 		/* X = A */
+ 		case BPF_MISC | BPF_TAX:
+ 			insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 			insn->a_reg = BPF_REG_X;
+ 			insn->x_reg = BPF_REG_A;
+ 			break;
+ 
+ 		/* A = X */
+ 		case BPF_MISC | BPF_TXA:
+ 			insn->code = BPF_ALU64 | BPF_MOV | BPF_X;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->x_reg = BPF_REG_X;
+ 			break;
+ 
+ 		/* A = skb->len or X = skb->len */
+ 		case BPF_LD | BPF_W | BPF_LEN:
+ 		case BPF_LDX | BPF_W | BPF_LEN:
+ 			insn->code = BPF_LDX | BPF_MEM | BPF_W;
+ 			insn->a_reg = BPF_CLASS(fp->code) == BPF_LD ?
+ 				      BPF_REG_A : BPF_REG_X;
+ 			insn->x_reg = BPF_REG_CTX;
+ 			insn->off = offsetof(struct sk_buff, len);
+ 			break;
+ 
+ 		/* access seccomp_data fields */
+ 		case BPF_LDX | BPF_ABS | BPF_W:
+ 			insn->code = BPF_LDX | BPF_MEM | BPF_W;
+ 			insn->a_reg = BPF_REG_A;
+ 			insn->x_reg = BPF_REG_CTX;
+ 			insn->off = fp->k;
+ 			break;
 -
++>>>>>>> 30743837dd20 (net: filter: make register naming more comprehensible)
 +
 +			nla = nla_find_nested(nla, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +#ifdef CONFIG_SECCOMP_FILTER
 +		case BPF_S_ANC_SECCOMP_LD_W:
 +			A = seccomp_bpf_load(fentry->k);
 +			continue;
 +#endif
  		default:
 -			goto err;
 +			WARN_RATELIMIT(1, "Unknown code:%u jt:%u tf:%u k:%u\n",
 +				       fentry->code, fentry->jt,
 +				       fentry->jf, fentry->k);
 +			return 0;
  		}
 -
 -		insn++;
 -		if (new_prog)
 -			memcpy(new_insn, tmp_insns,
 -			       sizeof(*insn) * (insn - tmp_insns));
 -
 -		new_insn += insn - tmp_insns;
  	}
  
 -	if (!new_prog) {
 -		/* Only calculating new length. */
 -		*new_len = new_insn - new_prog;
 -		return 0;
 -	}
 -
 -	pass++;
 -	if (new_flen != new_insn - new_prog) {
 -		new_flen = new_insn - new_prog;
 -		if (pass > 2)
 -			goto err;
 -
 -		goto do_pass;
 -	}
 -
 -	kfree(addrs);
 -	BUG_ON(*new_len != new_flen);
  	return 0;
 -err:
 -	kfree(addrs);
 -	return -EINVAL;
  }
 +EXPORT_SYMBOL(sk_run_filter);
  
 -/* Security:
 - *
 +/*
 + * Security :
   * A BPF program is able to use 16 cells of memory to store intermediate
 - * values (check u32 mem[BPF_MEMWORDS] in sk_run_filter()).
 - *
 + * values (check u32 mem[BPF_MEMWORDS] in sk_run_filter())
   * As we dont want to clear mem[] array for each packet going through
   * sk_run_filter(), we check that filter loaded by user never try to read
   * a cell if not previously written, and we check all branches to be sure
* Unmerged path include/linux/filter.h
* Unmerged path net/core/filter.c
