x86/mm: Add support for gbpages to kernel_ident_mapping_init()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] mm: Add support for gbpages to kernel_ident_mapping_init() (Pingfan Liu) [1503400]
Rebuild_FUZZ: 96.67%
commit-author Xunlei Pang <xlpang@redhat.com>
commit 66aad4fdf2bf0af29c7decb4433dc5ec6c7c5451
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/66aad4fd.failed

Kernel identity mappings on x86-64 kernels are created in two
ways: by the early x86 boot code, or by kernel_ident_mapping_init().

Native kernels (which is the dominant usecase) use the former,
but the kexec and the hibernation code uses kernel_ident_mapping_init().

There's a subtle difference between these two ways of how identity
mappings are created, the current kernel_ident_mapping_init() code
creates identity mappings always using 2MB page(PMD level) - while
the native kernel boot path also utilizes gbpages where available.

This difference is suboptimal both for performance and for memory
usage: kernel_ident_mapping_init() needs to allocate pages for the
page tables when creating the new identity mappings.

This patch adds 1GB page(PUD level) support to kernel_ident_mapping_init()
to address these concerns.

The primary advantage would be better TLB coverage/performance,
because we'd utilize 1GB TLBs instead of 2MB ones.

It is also useful for machines with large number of memory to
save paging structure allocations(around 4MB/TB using 2MB page)
when setting identity mappings for all the memory, after using
1GB page it will consume only 8KB/TB.

( Note that this change alone does not activate gbpages in kexec,
  we are doing that in a separate patch. )

	Signed-off-by: Xunlei Pang <xlpang@redhat.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: Eric Biederman <ebiederm@xmission.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Yinghai Lu <yinghai@kernel.org>
	Cc: akpm@linux-foundation.org
	Cc: kexec@lists.infradead.org
Link: http://lkml.kernel.org/r/1493862171-8799-1-git-send-email-xlpang@redhat.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 66aad4fdf2bf0af29c7decb4433dc5ec6c7c5451)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/init.h
#	arch/x86/kernel/machine_kexec_64.c
diff --cc arch/x86/include/asm/init.h
index b2ec511e2da9,474eb8c66fee..000000000000
--- a/arch/x86/include/asm/init.h
+++ b/arch/x86/include/asm/init.h
@@@ -4,9 -4,9 +4,13 @@@
  struct x86_mapping_info {
  	void *(*alloc_pgt_page)(void *); /* allocate buf for page table */
  	void *context;			 /* context for alloc_pgt_page */
- 	unsigned long pmd_flag;		 /* page flag for PMD entry */
+ 	unsigned long page_flag;	 /* page flag for PMD or PUD entry */
  	unsigned long offset;		 /* ident mapping offset */
++<<<<<<< HEAD
 +	unsigned long kernpg_flag;	 /* kernel pagetable flag override */
++=======
+ 	bool direct_gbpages;		 /* PUD level 1GB page support */
++>>>>>>> 66aad4fdf2bf (x86/mm: Add support for gbpages to kernel_ident_mapping_init())
  };
  
  int kernel_ident_mapping_init(struct x86_mapping_info *info, pgd_t *pgd_page,
diff --cc arch/x86/kernel/machine_kexec_64.c
index 9e197c3a1cb6,1d4f2b076545..000000000000
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@@ -101,8 -113,7 +101,12 @@@ static int init_pgtable(struct kimage *
  	struct x86_mapping_info info = {
  		.alloc_pgt_page	= alloc_pgt_page,
  		.context	= image,
++<<<<<<< HEAD
 +		.pmd_flag	= __PAGE_KERNEL_LARGE_EXEC,
 +		.kernpg_flag	= _KERNPG_TABLE_NOENC,
++=======
+ 		.page_flag	= __PAGE_KERNEL_LARGE_EXEC,
++>>>>>>> 66aad4fdf2bf (x86/mm: Add support for gbpages to kernel_ident_mapping_init())
  	};
  	unsigned long mstart, mend;
  	pgd_t *level4p;
diff --git a/arch/x86/boot/compressed/pagetable.c b/arch/x86/boot/compressed/pagetable.c
index b22d3a478861..db4fa615b2b4 100644
--- a/arch/x86/boot/compressed/pagetable.c
+++ b/arch/x86/boot/compressed/pagetable.c
@@ -87,7 +87,7 @@ static unsigned long level4p;
  * Due to relocation, pointers must be assigned at run time not build time.
  */
 static struct x86_mapping_info mapping_info = {
-	.pmd_flag       = __PAGE_KERNEL_LARGE_EXEC,
+	.page_flag       = __PAGE_KERNEL_LARGE_EXEC,
 };
 
 /* Locates and clears a region for a new top level page table. */
* Unmerged path arch/x86/include/asm/init.h
* Unmerged path arch/x86/kernel/machine_kexec_64.c
diff --git a/arch/x86/mm/ident_map.c b/arch/x86/mm/ident_map.c
index 7cfceaddfb16..717428f96699 100644
--- a/arch/x86/mm/ident_map.c
+++ b/arch/x86/mm/ident_map.c
@@ -13,7 +13,7 @@ static void ident_pmd_init(struct x86_mapping_info *info, pmd_t *pmd_page,
 		if (pmd_present(*pmd))
 			continue;
 
-		set_pmd(pmd, __pmd((addr - info->offset) | info->pmd_flag));
+		set_pmd(pmd, __pmd((addr - info->offset) | info->page_flag));
 	}
 }
 
@@ -30,6 +30,18 @@ static int ident_pud_init(struct x86_mapping_info *info, pud_t *pud_page,
 		if (next > end)
 			next = end;
 
+		if (info->direct_gbpages) {
+			pud_t pudval;
+
+			if (pud_present(*pud))
+				continue;
+
+			addr &= PUD_MASK;
+			pudval = __pud((addr - info->offset) | info->page_flag);
+			set_pud(pud, pudval);
+			continue;
+		}
+
 		if (pud_present(*pud)) {
 			pmd = pmd_offset(pud, 0);
 			ident_pmd_init(info, pmd, addr, next);
diff --git a/arch/x86/power/hibernate_64.c b/arch/x86/power/hibernate_64.c
index c9ab0127f8f4..f0956827aa17 100644
--- a/arch/x86/power/hibernate_64.c
+++ b/arch/x86/power/hibernate_64.c
@@ -88,7 +88,7 @@ static int set_up_temporary_mappings(void)
 {
 	struct x86_mapping_info info = {
 		.alloc_pgt_page	= alloc_pgt_page,
-		.pmd_flag	= __PAGE_KERNEL_LARGE_EXEC,
+		.page_flag	= __PAGE_KERNEL_LARGE_EXEC,
 		.offset		= __PAGE_OFFSET,
 	};
 	unsigned long mstart, mend;
