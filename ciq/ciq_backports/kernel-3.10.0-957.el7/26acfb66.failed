x86/KVM: Warn user if KVM is loaded SMT and L1TF CPU bug being present

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 26acfb666a473d960f0fd971fe68f3e3ad16c70b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/26acfb66.failed

If the L1TF CPU bug is present we allow the KVM module to be loaded as the
major of users that use Linux and KVM have trusted guests and do not want a
broken setup.

Cloud vendors are the ones that are uncomfortable with CVE 2018-3620 and as
such they are the ones that should set nosmt to one.

Setting 'nosmt' means that the system administrator also needs to disable
SMT (Hyper-threading) in the BIOS, or via the 'nosmt' command line
parameter, or via the /sys/devices/system/cpu/smt/control. See commit
05736e4ac13c ("cpu/hotplug: Provide knobs to control SMT").

Other mitigations are to use task affinity, cpu sets, interrupt binding,
etc - anything to make sure that _only_ the same guests vCPUs are running
on sibling threads.

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 26acfb666a473d960f0fd971fe68f3e3ad16c70b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/kernel-parameters.txt
#	arch/x86/kvm/vmx.c
#	kernel/cpu.c
diff --cc Documentation/kernel-parameters.txt
index a0f1f1428af0,298f1b38dc89..000000000000
--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@@ -1553,6 -1930,28 +1553,31 @@@ bytes respectively. Such letter suffixe
  			for all guests.
  			Default is 1 (enabled) if in 64-bit or 32-bit PAE mode.
  
++<<<<<<< HEAD:Documentation/kernel-parameters.txt
++=======
+ 	kvm-arm.vgic_v3_group0_trap=
+ 			[KVM,ARM] Trap guest accesses to GICv3 group-0
+ 			system registers
+ 
+ 	kvm-arm.vgic_v3_group1_trap=
+ 			[KVM,ARM] Trap guest accesses to GICv3 group-1
+ 			system registers
+ 
+ 	kvm-arm.vgic_v3_common_trap=
+ 			[KVM,ARM] Trap guest accesses to GICv3 common
+ 			system registers
+ 
+ 	kvm-arm.vgic_v4_enable=
+ 			[KVM,ARM] Allow use of GICv4 for direct injection of
+ 			LPIs.
+ 
+ 	kvm-intel.nosmt=[KVM,Intel] If the L1TF CPU bug is present (CVE-2018-3620)
+ 			and the system has SMT (aka Hyper-Threading) enabled then
+ 			don't allow guests to be created.
+ 
+ 			Default is 0 (allow guests to be created).
+ 
++>>>>>>> 26acfb666a47 (x86/KVM: Warn user if KVM is loaded SMT and L1TF CPU bug being present):Documentation/admin-guide/kernel-parameters.txt
  	kvm-intel.ept=	[KVM,Intel] Disable extended page tables
  			(virtualized MMU) support on capable Intel chips.
  			Default is 1 (enabled)
diff --cc arch/x86/kvm/vmx.c
index e2f48f8aba96,f2e7b6d016c9..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -9247,6 -10373,23 +9250,26 @@@ free_vcpu
  	return ERR_PTR(err);
  }
  
++<<<<<<< HEAD
++=======
+ #define L1TF_MSG "SMT enabled with L1TF CPU bug present. Refer to CVE-2018-3620 for details.\n"
+ 
+ static int vmx_vm_init(struct kvm *kvm)
+ {
+ 	if (!ple_gap)
+ 		kvm->arch.pause_in_guest = true;
+ 
+ 	if (boot_cpu_has(X86_BUG_L1TF) && cpu_smt_control == CPU_SMT_ENABLED) {
+ 		if (nosmt) {
+ 			pr_err(L1TF_MSG);
+ 			return -EOPNOTSUPP;
+ 		}
+ 		pr_warn(L1TF_MSG);
+ 	}
+ 	return 0;
+ }
+ 
++>>>>>>> 26acfb666a47 (x86/KVM: Warn user if KVM is loaded SMT and L1TF CPU bug being present)
  static void __init vmx_check_processor_compat(void *rtn)
  {
  	struct vmcs_config vmcs_conf;
diff --cc kernel/cpu.c
index 0d9e250d0ea0,5a00ebdf98c6..000000000000
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@@ -216,22 -337,111 +216,28 @@@ void cpu_hotplug_disable(void
  void cpu_hotplug_enable(void)
  {
  	cpu_maps_update_begin();
 -	__cpu_hotplug_enable();
 +	cpu_hotplug_disabled = 0;
  	cpu_maps_update_done();
  }
 -EXPORT_SYMBOL_GPL(cpu_hotplug_enable);
 -#endif	/* CONFIG_HOTPLUG_CPU */
  
++<<<<<<< HEAD
 +#else /* #if CONFIG_HOTPLUG_CPU */
 +static void cpu_hotplug_begin(void) {}
 +static void cpu_hotplug_done(void) {}
 +#endif	/* #else #if CONFIG_HOTPLUG_CPU */
++=======
+ #ifdef CONFIG_HOTPLUG_SMT
+ enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
+ EXPORT_SYMBOL_GPL(cpu_smt_control);
++>>>>>>> 26acfb666a47 (x86/KVM: Warn user if KVM is loaded SMT and L1TF CPU bug being present)
  
 -static int __init smt_cmdline_disable(char *str)
 -{
 -	cpu_smt_control = CPU_SMT_DISABLED;
 -	if (str && !strcmp(str, "force")) {
 -		pr_info("SMT: Force disabled\n");
 -		cpu_smt_control = CPU_SMT_FORCE_DISABLED;
 -	}
 -	return 0;
 -}
 -early_param("nosmt", smt_cmdline_disable);
 -
 -static inline bool cpu_smt_allowed(unsigned int cpu)
 -{
 -	if (cpu_smt_control == CPU_SMT_ENABLED)
 -		return true;
 -
 -	if (topology_is_primary_thread(cpu))
 -		return true;
 -
 -	/*
 -	 * On x86 it's required to boot all logical CPUs at least once so
 -	 * that the init code can get a chance to set CR4.MCE on each
 -	 * CPU. Otherwise, a broadacasted MCE observing CR4.MCE=0b on any
 -	 * core will shutdown the machine.
 -	 */
 -	return !per_cpu(cpuhp_state, cpu).booted_once;
 -}
 -#else
 -static inline bool cpu_smt_allowed(unsigned int cpu) { return true; }
 -#endif
 -
 -static inline enum cpuhp_state
 -cpuhp_set_state(struct cpuhp_cpu_state *st, enum cpuhp_state target)
 -{
 -	enum cpuhp_state prev_state = st->state;
 -
 -	st->rollback = false;
 -	st->last = NULL;
 -
 -	st->target = target;
 -	st->single = false;
 -	st->bringup = st->state < target;
 -
 -	return prev_state;
 -}
 -
 -static inline void
 -cpuhp_reset_state(struct cpuhp_cpu_state *st, enum cpuhp_state prev_state)
 -{
 -	st->rollback = true;
 -
 -	/*
 -	 * If we have st->last we need to undo partial multi_instance of this
 -	 * state first. Otherwise start undo at the previous state.
 -	 */
 -	if (!st->last) {
 -		if (st->bringup)
 -			st->state--;
 -		else
 -			st->state++;
 -	}
 -
 -	st->target = prev_state;
 -	st->bringup = !st->bringup;
 -}
 -
 -/* Regular hotplug invocation of the AP hotplug thread */
 -static void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)
 -{
 -	if (!st->single && st->state == st->target)
 -		return;
 -
 -	st->result = 0;
 -	/*
 -	 * Make sure the above stores are visible before should_run becomes
 -	 * true. Paired with the mb() above in cpuhp_thread_fun()
 -	 */
 -	smp_mb();
 -	st->should_run = true;
 -	wake_up_process(st->thread);
 -	wait_for_ap_thread(st, st->bringup);
 -}
 -
 -static int cpuhp_kick_ap(struct cpuhp_cpu_state *st, enum cpuhp_state target)
 +/* Need to know about CPUs going up/down? */
 +int __ref register_cpu_notifier(struct notifier_block *nb)
  {
 -	enum cpuhp_state prev_state;
  	int ret;
 -
 -	prev_state = cpuhp_set_state(st, target);
 -	__cpuhp_kick_ap(st);
 -	if ((ret = st->result)) {
 -		cpuhp_reset_state(st, prev_state);
 -		__cpuhp_kick_ap(st);
 -	}
 -
 +	cpu_maps_update_begin();
 +	ret = raw_notifier_chain_register(&cpu_chain, nb);
 +	cpu_maps_update_done();
  	return ret;
  }
  
* Unmerged path Documentation/kernel-parameters.txt
* Unmerged path arch/x86/kvm/vmx.c
* Unmerged path kernel/cpu.c
