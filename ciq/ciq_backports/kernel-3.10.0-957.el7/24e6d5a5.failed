mm: pass the vmem_altmap to arch_add_memory and __add_pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] pass the vmem_altmap to arch_add_memory and __add_pages (Jeff Moyer) [1505291]
Rebuild_FUZZ: 96.49%
commit-author Christoph Hellwig <hch@lst.de>
commit 24e6d5a59ac7d31adc0322de2d0117dfa370936f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/24e6d5a5.failed

We can just pass this on instead of having to do a radix tree lookup
without proper locking 2 levels into the callchain.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 24e6d5a59ac7d31adc0322de2d0117dfa370936f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/ia64/mm/init.c
#	arch/powerpc/mm/mem.c
#	arch/s390/mm/init.c
#	arch/sh/mm/init.c
#	arch/x86/mm/init_32.c
#	arch/x86/mm/init_64.c
#	include/linux/memory_hotplug.h
#	kernel/memremap.c
#	mm/hmm.c
#	mm/memory_hotplug.c
diff --cc arch/ia64/mm/init.c
index d1fe4b402601,2e2e4f532204..000000000000
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@@ -654,19 -647,14 +654,28 @@@ mem_init (void
  }
  
  #ifdef CONFIG_MEMORY_HOTPLUG
++<<<<<<< HEAD
 +int arch_add_memory(int nid, u64 start, u64 size)
++=======
+ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+ 		bool want_memblock)
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  {
 +	pg_data_t *pgdat;
 +	struct zone *zone;
  	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  	int ret;
  
++<<<<<<< HEAD
 +	pgdat = NODE_DATA(nid);
 +
 +	zone = pgdat->node_zones + ZONE_NORMAL;
 +	ret = __add_pages(nid, zone, start_pfn, nr_pages);
 +
++=======
+ 	ret = __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  	if (ret)
  		printk("%s: Problem encountered in __add_pages() as ret=%d\n",
  		       __func__,  ret);
diff --cc arch/powerpc/mm/mem.c
index 7bbac9df162a,e670cfc2766e..000000000000
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@@ -115,26 -117,35 +115,45 @@@ int memory_add_physaddr_to_nid(u64 star
  }
  #endif
  
++<<<<<<< HEAD
 +int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
++=======
+ int __weak create_section_mapping(unsigned long start, unsigned long end)
+ {
+ 	return -ENODEV;
+ }
+ 
+ int __weak remove_section_mapping(unsigned long start, unsigned long end)
+ {
+ 	return -ENODEV;
+ }
+ 
+ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+ 		bool want_memblock)
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  {
 +	struct pglist_data *pgdata;
 +	struct zone *zone;
  	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
 -	int rc;
  
  	resize_hpt_for_hotplug(memblock_phys_mem_size());
  
 +	pgdata = NODE_DATA(nid);
 +
++<<<<<<< HEAD
  	start = (unsigned long)__va(start);
 -	rc = create_section_mapping(start, start + size);
 -	if (rc) {
 -		pr_warning(
 -			"Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
 -			start, start + size, rc);
 -		return -EFAULT;
 -	}
 +	if (create_section_mapping(start, start + size))
 +		return -EINVAL;
  
 +	/* this should work for most non-highmem platforms */
 +	zone = pgdata->node_zones +
 +		zone_for_memory(nid, start, size, 0, for_device);
 +
 +	return __add_pages(nid, zone, start_pfn, nr_pages);
++=======
+ 	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  }
  
  #ifdef CONFIG_MEMORY_HOTREMOVE
diff --cc arch/s390/mm/init.c
index e383ffc4ecb6,e12c5af50cd7..000000000000
--- a/arch/s390/mm/init.c
+++ b/arch/s390/mm/init.c
@@@ -237,7 -166,77 +237,81 @@@ unsigned long memory_block_size_bytes(v
  	 * Make sure the memory block size is always greater
  	 * or equal than the memory increment size.
  	 */
++<<<<<<< HEAD
 +	return max_t(unsigned long, MIN_MEMORY_BLOCK_SIZE, sclp_get_rzm());
++=======
+ 	return max_t(unsigned long, MIN_MEMORY_BLOCK_SIZE, sclp.rzm);
+ }
+ 
+ #ifdef CONFIG_MEMORY_HOTPLUG
+ 
+ #ifdef CONFIG_CMA
+ 
+ /* Prevent memory blocks which contain cma regions from going offline */
+ 
+ struct s390_cma_mem_data {
+ 	unsigned long start;
+ 	unsigned long end;
+ };
+ 
+ static int s390_cma_check_range(struct cma *cma, void *data)
+ {
+ 	struct s390_cma_mem_data *mem_data;
+ 	unsigned long start, end;
+ 
+ 	mem_data = data;
+ 	start = cma_get_base(cma);
+ 	end = start + cma_get_size(cma);
+ 	if (end < mem_data->start)
+ 		return 0;
+ 	if (start >= mem_data->end)
+ 		return 0;
+ 	return -EBUSY;
+ }
+ 
+ static int s390_cma_mem_notifier(struct notifier_block *nb,
+ 				 unsigned long action, void *data)
+ {
+ 	struct s390_cma_mem_data mem_data;
+ 	struct memory_notify *arg;
+ 	int rc = 0;
+ 
+ 	arg = data;
+ 	mem_data.start = arg->start_pfn << PAGE_SHIFT;
+ 	mem_data.end = mem_data.start + (arg->nr_pages << PAGE_SHIFT);
+ 	if (action == MEM_GOING_OFFLINE)
+ 		rc = cma_for_each_area(s390_cma_check_range, &mem_data);
+ 	return notifier_from_errno(rc);
+ }
+ 
+ static struct notifier_block s390_cma_mem_nb = {
+ 	.notifier_call = s390_cma_mem_notifier,
+ };
+ 
+ static int __init s390_cma_mem_init(void)
+ {
+ 	return register_memory_notifier(&s390_cma_mem_nb);
+ }
+ device_initcall(s390_cma_mem_init);
+ 
+ #endif /* CONFIG_CMA */
+ 
+ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+ 		bool want_memblock)
+ {
+ 	unsigned long start_pfn = PFN_DOWN(start);
+ 	unsigned long size_pages = PFN_DOWN(size);
+ 	int rc;
+ 
+ 	rc = vmem_add_mapping(start, size);
+ 	if (rc)
+ 		return rc;
+ 
+ 	rc = __add_pages(nid, start_pfn, size_pages, altmap, want_memblock);
+ 	if (rc)
+ 		vmem_remove_mapping(start, size);
+ 	return rc;
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  }
  
  #ifdef CONFIG_MEMORY_HOTREMOVE
diff --cc arch/sh/mm/init.c
index 20f9ead650d3,552afbf55bad..000000000000
--- a/arch/sh/mm/init.c
+++ b/arch/sh/mm/init.c
@@@ -510,18 -485,15 +510,27 @@@ void free_initrd_mem(unsigned long star
  #endif
  
  #ifdef CONFIG_MEMORY_HOTPLUG
++<<<<<<< HEAD
 +int arch_add_memory(int nid, u64 start, u64 size)
++=======
+ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+ 		bool want_memblock)
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  {
 -	unsigned long start_pfn = PFN_DOWN(start);
 +	pg_data_t *pgdat;
 +	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  	int ret;
  
 +	pgdat = NODE_DATA(nid);
 +
  	/* We only have ZONE_NORMAL, so this is easy.. */
++<<<<<<< HEAD
 +	ret = __add_pages(nid, pgdat->node_zones + ZONE_NORMAL,
 +				start_pfn, nr_pages);
++=======
+ 	ret = __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  	if (unlikely(ret))
  		printk("%s: Failed, __add_pages() == %d\n", __func__, ret);
  
diff --cc arch/x86/mm/init_32.c
index 0b994ce85c46,8a3091511a71..000000000000
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@@ -844,14 -829,13 +844,23 @@@ void __init mem_init(void
  }
  
  #ifdef CONFIG_MEMORY_HOTPLUG
++<<<<<<< HEAD
 +int arch_add_memory(int nid, u64 start, u64 size)
++=======
+ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+ 		bool want_memblock)
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  {
 +	struct pglist_data *pgdata = NODE_DATA(nid);
 +	struct zone *zone = pgdata->node_zones + ZONE_HIGHMEM;
  	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  
++<<<<<<< HEAD
 +	return __add_pages(nid, zone, start_pfn, nr_pages);
++=======
+ 	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  }
  
  #ifdef CONFIG_MEMORY_HOTREMOVE
diff --cc arch/x86/mm/init_64.c
index 25c65b6af83e,e80bb4189254..000000000000
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@@ -661,24 -772,12 +661,33 @@@ static void  update_end_of_memory_vars(
  	}
  }
  
++<<<<<<< HEAD
 +/*
 + * Memory is added always to NORMAL zone. This means you will never get
 + * additional DMA/DMA32 memory.
 + */
 +int add_pages(int nid, unsigned long start,
 +	      unsigned long size, bool for_device)
++=======
+ int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
+ 		struct vmem_altmap *altmap, bool want_memblock)
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  {
 +	struct pglist_data *pgdat = NODE_DATA(nid);
 +	int zoneid = zone_for_memory(nid, start, size, ZONE_NORMAL, for_device);
 +	struct zone *zone = pgdat->node_zones + zoneid;
  	int ret;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_ZONE_DEVICE
 +	if (zoneid == ZONE_DEVICE)
 +		zone = pgdat->zone_device;
 +#endif
 +
 +	ret = __add_pages(nid, zone, start >> PAGE_SHIFT, size >> PAGE_SHIFT);
++=======
+ 	ret = __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  	WARN_ON_ONCE(ret);
  
  	/* update max_pfn, max_low_pfn and high_memory */
@@@ -687,13 -787,16 +696,22 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
 +int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
++=======
+ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+ 		bool want_memblock)
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  {
 -	unsigned long start_pfn = start >> PAGE_SHIFT;
 -	unsigned long nr_pages = size >> PAGE_SHIFT;
 -
  	init_memory_mapping(start, start + size);
  
++<<<<<<< HEAD
 +	return add_pages(nid, start, size, for_device);
++=======
+ 	return add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  }
 +EXPORT_SYMBOL_GPL(arch_add_memory);
  
  #define PAGE_INUSE 0xFD
  
diff --cc include/linux/memory_hotplug.h
index d0cd6c1954bf,db276afbefcc..000000000000
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@@ -11,8 -12,24 +11,13 @@@ struct zone
  struct pglist_data;
  struct mem_section;
  struct memory_block;
++<<<<<<< HEAD
++=======
+ struct resource;
+ struct vmem_altmap;
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  
  #ifdef CONFIG_MEMORY_HOTPLUG
 -/*
 - * Return page for the valid pfn only if the page is online. All pfn
 - * walkers which rely on the fully initialized page->flags and others
 - * should use this rather than pfn_valid && pfn_to_page
 - */
 -#define pfn_to_online_page(pfn)				\
 -({							\
 -	struct page *___page = NULL;			\
 -	unsigned long ___nr = pfn_to_section_nr(pfn);	\
 -							\
 -	if (___nr < NR_MEM_SECTIONS && online_section_nr(___nr))\
 -		___page = pfn_to_page(pfn);		\
 -	___page;					\
 -})
  
  /*
   * Types for free bootmem stored in page->lru.next. These have to be in
@@@ -103,9 -131,21 +108,27 @@@ extern int __remove_pages(struct zone *
  	unsigned long nr_pages);
  #endif /* CONFIG_MEMORY_HOTREMOVE */
  
++<<<<<<< HEAD
 +/* reasonably generic interface to expand the physical pages in a zone  */
 +extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,
 +	unsigned long nr_pages);
++=======
+ /* reasonably generic interface to expand the physical pages */
+ extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
+ 		struct vmem_altmap *altmap, bool want_memblock);
+ 
+ #ifndef CONFIG_ARCH_HAS_ADD_PAGES
+ static inline int add_pages(int nid, unsigned long start_pfn,
+ 		unsigned long nr_pages, struct vmem_altmap *altmap,
+ 		bool want_memblock)
+ {
+ 	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
+ }
+ #else /* ARCH_HAS_ADD_PAGES */
+ int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
+ 		struct vmem_altmap *altmap, bool want_memblock);
+ #endif /* ARCH_HAS_ADD_PAGES */
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  
  #ifdef CONFIG_NUMA
  extern int memory_add_physaddr_to_nid(u64 start);
@@@ -264,9 -319,11 +287,17 @@@ static inline void remove_memory(int ni
  extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
  		void *arg, int (*func)(struct memory_block *, void *));
  extern int add_memory(int nid, u64 start, u64 size);
++<<<<<<< HEAD
 +extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
 +		bool for_device);
 +extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);
++=======
+ extern int add_memory_resource(int nid, struct resource *resource, bool online);
+ extern int arch_add_memory(int nid, u64 start, u64 size,
+ 		struct vmem_altmap *altmap, bool want_memblock);
+ extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
+ 		unsigned long nr_pages);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
  extern bool is_memblock_offlined(struct memory_block *mem);
  extern void remove_memory(int nid, u64 start, u64 size);
diff --cc kernel/memremap.c
index 00f3d3b53574,8488cdeead16..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -449,7 -428,11 +450,15 @@@ void *devm_memremap_pages(struct devic
  		goto err_pfn_remap;
  
  	mem_hotplug_begin();
++<<<<<<< HEAD
 +	error = arch_add_memory(nid, align_start, align_size, true);
++=======
+ 	error = arch_add_memory(nid, align_start, align_size, altmap, false);
+ 	if (!error)
+ 		move_pfn_range_to_zone(&NODE_DATA(nid)->node_zones[ZONE_DEVICE],
+ 					align_start >> PAGE_SHIFT,
+ 					align_size >> PAGE_SHIFT);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  	mem_hotplug_done();
  	if (error)
  		goto err_add_memory;
diff --cc mm/hmm.c
index 125cbd4521ca,231aaacd1997..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -907,9 -919,31 +907,37 @@@ static int hmm_devmem_pages_create(stru
  	if (nid < 0)
  		nid = numa_mem_id();
  
++<<<<<<< HEAD
 +	ret = add_pages(nid, align_start, align_size, true);
 +	if (ret)
 +		goto error_radix;
++=======
+ 	mem_hotplug_begin();
+ 	/*
+ 	 * For device private memory we call add_pages() as we only need to
+ 	 * allocate and initialize struct page for the device memory. More-
+ 	 * over the device memory is un-accessible thus we do not want to
+ 	 * create a linear mapping for the memory like arch_add_memory()
+ 	 * would do.
+ 	 *
+ 	 * For device public memory, which is accesible by the CPU, we do
+ 	 * want the linear mapping and thus use arch_add_memory().
+ 	 */
+ 	if (devmem->pagemap.type == MEMORY_DEVICE_PUBLIC)
+ 		ret = arch_add_memory(nid, align_start, align_size, NULL,
+ 				false);
+ 	else
+ 		ret = add_pages(nid, align_start >> PAGE_SHIFT,
+ 				align_size >> PAGE_SHIFT, NULL, false);
+ 	if (ret) {
+ 		mem_hotplug_done();
+ 		goto error_add_memory;
+ 	}
+ 	move_pfn_range_to_zone(&NODE_DATA(nid)->node_zones[ZONE_DEVICE],
+ 				align_start >> PAGE_SHIFT,
+ 				align_size >> PAGE_SHIFT);
+ 	mem_hotplug_done();
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  
  	for (pfn = devmem->pfn_first; pfn < devmem->pfn_last; pfn++) {
  		struct page *page = pfn_to_page(pfn);
diff --cc mm/memory_hotplug.c
index 4ad6525f2bd5,fc0485dcece1..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -515,8 -291,9 +515,14 @@@ static int __meminit __add_section(int 
   * call this function after deciding the zone to which to
   * add the new pages.
   */
++<<<<<<< HEAD
 +int __ref __add_pages(int nid, struct zone *zone, unsigned long phys_start_pfn,
 +			unsigned long nr_pages)
++=======
+ int __ref __add_pages(int nid, unsigned long phys_start_pfn,
+ 		unsigned long nr_pages, struct vmem_altmap *altmap,
+ 		bool want_memblock)
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  {
  	unsigned long i;
  	int err = 0;
@@@ -1316,7 -1147,7 +1320,11 @@@ int __ref add_memory(int nid, u64 start
  	}
  
  	/* call arch's memory hotadd */
++<<<<<<< HEAD
 +	ret = arch_add_memory(nid, start, size, false);
++=======
+ 	ret = arch_add_memory(nid, start, size, NULL, true);
++>>>>>>> 24e6d5a59ac7 (mm: pass the vmem_altmap to arch_add_memory and __add_pages)
  
  	if (ret < 0)
  		goto error;
* Unmerged path arch/ia64/mm/init.c
* Unmerged path arch/powerpc/mm/mem.c
* Unmerged path arch/s390/mm/init.c
* Unmerged path arch/sh/mm/init.c
* Unmerged path arch/x86/mm/init_32.c
* Unmerged path arch/x86/mm/init_64.c
* Unmerged path include/linux/memory_hotplug.h
* Unmerged path kernel/memremap.c
* Unmerged path mm/hmm.c
* Unmerged path mm/memory_hotplug.c
