nvme: expand nvmf_check_if_ready checks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [nvme] expand nvmf_check_if_ready checks (Ewan Milne) [1584753]
Rebuild_FUZZ: 91.67%
commit-author James Smart <jsmart2021@gmail.com>
commit bb06ec31452fb2da1594f88035c2ecea4e0652f4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/bb06ec31.failed

The nvmf_check_if_ready() checks that were added are very simplistic.
As such, the routine allows a lot of cases to fail ios during windows
of reset or re-connection. In cases where there are not multi-path
options present, the error goes back to the callee - the filesystem
or application. Not good.

The common routine was rewritten and calling syntax slightly expanded
so that per-transport is_ready routines don't need to be present.
The transports now call the routine directly. The routine is now a
fabrics routine rather than an inline function.

The routine now looks at controller state to decide the action to
take. Some states mandate io failure. Others define the condition where
a command can be accepted.  When the decision is unclear, a generic
queue-or-reject check is made to look for failfast or multipath ios and
only fails the io if it is so marked. Otherwise, the io will be queued
and wait for the controller state to resolve.

Admin commands issued via ioctl share a live admin queue with commands
from the transport for controller init. The ioctls could be intermixed
with the initialization commands. It's possible for the ioctl cmd to
be issued prior to the controller being enabled. To block this, the
ioctl admin commands need to be distinguished from admin commands used
for controller init. Added a USERCMD nvme_req(req)->rq_flags bit to
reflect this division and set it on ioctls requests.  As the
nvmf_check_if_ready() routine is called prior to nvme_setup_cmd(),
ensure that commands allocated by the ioctl path (actually anything
in core.c) preps the nvme_req(req) before starting the io. This will
preserve the USERCMD flag during execution and/or retry.

	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Sagi Grimberg <sagi@grimberg.e>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit bb06ec31452fb2da1594f88035c2ecea4e0652f4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/core.c
#	drivers/nvme/host/fabrics.h
#	drivers/nvme/host/fc.c
#	drivers/nvme/host/rdma.c
#	drivers/nvme/target/loop.c
diff --cc drivers/nvme/host/core.c
index d0c03595e82f,9df4f71e58ca..000000000000
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@@ -269,32 -376,19 +269,46 @@@ static void nvme_put_ns(struct nvme_ns 
  	kref_put(&ns->kref, nvme_free_ns);
  }
  
++<<<<<<< HEAD
 +static struct nvme_ns *nvme_get_ns_from_disk(struct gendisk *disk)
 +{
 +	struct nvme_ns *ns;
 +
 +	spin_lock(&dev_list_lock);
 +	ns = disk->private_data;
 +	if (ns) {
 +		if (!kref_get_unless_zero(&ns->kref))
 +			goto fail;
 +		if (!try_module_get(ns->ctrl->ops->module))
 +			goto fail_put_ns;
 +	}
 +	spin_unlock(&dev_list_lock);
 +
 +	return ns;
 +
 +fail_put_ns:
 +	kref_put(&ns->kref, nvme_free_ns);
 +fail:
 +	spin_unlock(&dev_list_lock);
 +	return NULL;
 +}
 +
 +struct request *nvme_alloc_request(struct request_queue *q,
 +		struct nvme_command *cmd, unsigned int flags, int qid)
++=======
+ static inline void nvme_clear_nvme_request(struct request *req)
+ {
+ 	if (!(req->rq_flags & RQF_DONTPREP)) {
+ 		nvme_req(req)->retries = 0;
+ 		nvme_req(req)->flags = 0;
+ 		req->rq_flags |= RQF_DONTPREP;
+ 	}
+ }
+ 
+ struct request *nvme_alloc_request(struct request_queue *q,
+ 		struct nvme_command *cmd, blk_mq_req_flags_t flags, int qid)
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  {
 -	unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
  	struct request *req;
  
  	if (qid == NVME_QID_ANY) {
@@@ -306,12 -400,8 +320,16 @@@
  	if (IS_ERR(req))
  		return req;
  
 +	req->cmd_type = REQ_TYPE_DRV_PRIV;
  	req->cmd_flags |= REQ_FAILFAST_DRIVER;
++<<<<<<< HEAD
 +	req->__data_len = 0;
 +	req->__sector = (sector_t) -1;
 +	req->bio = req->biotail = NULL;
 +
++=======
+ 	nvme_clear_nvme_request(req);
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  	nvme_req(req)->cmd = cmd;
  
  	return req;
@@@ -406,27 -613,40 +424,31 @@@ static inline int nvme_setup_rw(struct 
  	return 0;
  }
  
 -blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 +int nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
  		struct nvme_command *cmd)
  {
 -	blk_status_t ret = BLK_STS_OK;
 +	int ret = BLK_MQ_RQ_QUEUE_OK;
  
++<<<<<<< HEAD
 +	if (!(req->cmd_flags & REQ_DONTPREP)) {
 +		nvme_req(req)->retries = 0;
 +		req->cmd_flags |= REQ_DONTPREP;
 +	}
++=======
+ 	nvme_clear_nvme_request(req);
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  
 -	switch (req_op(req)) {
 -	case REQ_OP_DRV_IN:
 -	case REQ_OP_DRV_OUT:
 +	if (req->cmd_type == REQ_TYPE_DRV_PRIV)
  		memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
 -		break;
 -	case REQ_OP_FLUSH:
 +	else if (req->cmd_flags & REQ_FLUSH)
  		nvme_setup_flush(ns, cmd);
 -		break;
 -	case REQ_OP_WRITE_ZEROES:
 -		/* currently only aliased to deallocate for a few ctrls: */
 -	case REQ_OP_DISCARD:
 +	else if (req->cmd_flags & REQ_DISCARD)
  		ret = nvme_setup_discard(ns, req, cmd);
 -		break;
 -	case REQ_OP_READ:
 -	case REQ_OP_WRITE:
 +	else
  		ret = nvme_setup_rw(ns, req, cmd);
 -		break;
 -	default:
 -		WARN_ON_ONCE(1);
 -		return BLK_STS_IOERR;
 -	}
  
  	cmd->common.command_id = req->tag;
 -	if (ns)
 -		trace_nvme_setup_nvm_cmd(req->q->id, cmd);
 -	else
 -		trace_nvme_setup_admin_cmd(cmd);
 +
  	return ret;
  }
  EXPORT_SYMBOL_GPL(nvme_setup_cmd);
@@@ -493,9 -748,11 +515,10 @@@ int __nvme_submit_user_cmd(struct reque
  		return PTR_ERR(req);
  
  	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+ 	nvme_req(req)->flags |= NVME_REQ_USERCMD;
  
  	if (ubuffer && bufflen) {
 -		ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
 -				GFP_KERNEL);
 +		ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen, __GFP_WAIT);
  		if (ret)
  			goto out;
  		bio = req->bio;
diff --cc drivers/nvme/host/fabrics.h
index 5a20948e5609,ef46c915b7b5..000000000000
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@@ -155,5 -157,7 +155,10 @@@ void nvmf_unregister_transport(struct n
  void nvmf_free_options(struct nvmf_ctrl_options *opts);
  int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size);
  bool nvmf_should_reconnect(struct nvme_ctrl *ctrl);
++<<<<<<< HEAD
++=======
+ blk_status_t nvmf_check_if_ready(struct nvme_ctrl *ctrl,
+ 	struct request *rq, bool queue_live, bool is_connected);
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  
  #endif /* _NVME_FABRICS_H */
diff --cc drivers/nvme/host/fc.c
index d165c0527376,6cb26bcf6ec0..000000000000
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@@ -2327,21 -2269,15 +2327,25 @@@ nvme_fc_start_fcp_op(struct nvme_fc_ctr
  
  		if (ctrl->rport->remoteport.port_state == FC_OBJSTATE_ONLINE &&
  				ret != -EBUSY)
 -			return BLK_STS_IOERR;
 +			return BLK_MQ_RQ_QUEUE_ERROR;
  
 -		return BLK_STS_RESOURCE;
 +		goto busy;
  	}
  
 -	return BLK_STS_OK;
 +	return BLK_MQ_RQ_QUEUE_OK;
 +
 +busy:
 +	if (!(op->flags & FCOP_FLAGS_AEN) && queue->hctx)
 +		blk_mq_delay_run_hw_queue(queue->hctx, NVMEFC_QUEUE_DELAY);
 +
 +	return BLK_MQ_RQ_QUEUE_BUSY;
  }
  
++<<<<<<< HEAD
 +static int
++=======
+ static blk_status_t
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  nvme_fc_queue_rq(struct blk_mq_hw_ctx *hctx,
  			const struct blk_mq_queue_data *bd)
  {
@@@ -2354,7 -2290,13 +2358,17 @@@
  	struct nvme_command *sqe = &cmdiu->sqe;
  	enum nvmefc_fcp_datadir	io_dir;
  	u32 data_len;
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	blk_status_t ret;
+ 
+ 	ret = nvmf_check_if_ready(&queue->ctrl->ctrl, rq,
+ 		test_bit(NVME_FC_Q_LIVE, &queue->flags),
+ 		ctrl->rport->remoteport.port_state == FC_OBJSTATE_ONLINE);
+ 	if (unlikely(ret))
+ 		return ret;
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  
  	ret = nvme_setup_cmd(ns, rq, sqe);
  	if (ret)
diff --cc drivers/nvme/host/rdma.c
index 215f7f62fdfb,1eb4438a8763..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -1452,36 -1601,7 +1452,40 @@@ nvme_rdma_timeout(struct request *rq, b
  	return BLK_EH_HANDLED;
  }
  
++<<<<<<< HEAD
 +/*
 + * We cannot accept any other command until the Connect command has completed.
 + */
 +static inline int nvme_rdma_queue_is_ready(struct nvme_rdma_queue *queue,
 +		struct request *rq)
 +{
 +	if (unlikely(!test_bit(NVME_RDMA_Q_LIVE, &queue->flags))) {
 +		struct nvme_command *cmd = nvme_req(rq)->cmd;
 +
 +		if (rq->cmd_type != REQ_TYPE_DRV_PRIV ||
 +		    cmd->common.opcode != nvme_fabrics_command ||
 +		    cmd->fabrics.fctype != nvme_fabrics_type_connect) {
 +			/*
 +			 * reconnecting state means transport disruption, which
 +			 * can take a long time and even might fail permanently,
 +			 * so we can't let incoming I/O be requeued forever.
 +			 * fail it fast to allow upper layers a chance to
 +			 * failover.
 +			 */
 +			if (queue->ctrl->ctrl.state == NVME_CTRL_RECONNECTING)
 +				return -EIO;
 +			else
 +				return -EAGAIN;
 +		}
 +	}
 +
 +	return 0;
 +}
 +
 +static int nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
++=======
+ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  		const struct blk_mq_queue_data *bd)
  {
  	struct nvme_ns *ns = hctx->queue->queuedata;
@@@ -1497,9 -1616,10 +1501,14 @@@
  
  	WARN_ON_ONCE(rq->tag < 0);
  
++<<<<<<< HEAD
 +	ret = nvme_rdma_queue_is_ready(queue, rq);
++=======
+ 	ret = nvmf_check_if_ready(&queue->ctrl->ctrl, rq,
+ 		test_bit(NVME_RDMA_Q_LIVE, &queue->flags), true);
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  	if (unlikely(ret))
 -		return ret;
 +		goto err;
  
  	dev = queue->device->dev;
  	ib_dma_sync_single_for_cpu(dev, sqe->dma,
diff --cc drivers/nvme/target/loop.c
index 21dcbdab74e4,31fdfba556a8..000000000000
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@@ -153,19 -149,25 +153,32 @@@ nvme_loop_timeout(struct request *rq, b
  	return BLK_EH_HANDLED;
  }
  
++<<<<<<< HEAD
 +static int nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
++=======
+ static blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  		const struct blk_mq_queue_data *bd)
  {
  	struct nvme_ns *ns = hctx->queue->queuedata;
  	struct nvme_loop_queue *queue = hctx->driver_data;
  	struct request *req = bd->rq;
  	struct nvme_loop_iod *iod = blk_mq_rq_to_pdu(req);
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	blk_status_t ret;
+ 
+ 	ret = nvmf_check_if_ready(&queue->ctrl->ctrl, req,
+ 		test_bit(NVME_LOOP_Q_LIVE, &queue->flags), true);
+ 	if (unlikely(ret))
+ 		return ret;
++>>>>>>> bb06ec31452f (nvme: expand nvmf_check_if_ready checks)
  
  	ret = nvme_setup_cmd(ns, req, &iod->cmd);
 -	if (ret)
 +	if (ret != BLK_MQ_RQ_QUEUE_OK)
  		return ret;
  
 -	blk_mq_start_request(req);
  	iod->cmd.common.flags |= NVME_CMD_SGL_METABUF;
  	iod->req.port = nvmet_loop_port;
  	if (!nvmet_req_init(&iod->req, &queue->nvme_cq,
* Unmerged path drivers/nvme/host/core.c
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 86f00fb96ad0..cba6a1fb4633 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -536,6 +536,85 @@ static struct nvmf_transport_ops *nvmf_lookup_transport(
 	return NULL;
 }
 
+blk_status_t nvmf_check_if_ready(struct nvme_ctrl *ctrl, struct request *rq,
+		bool queue_live, bool is_connected)
+{
+	struct nvme_command *cmd = nvme_req(rq)->cmd;
+
+	if (likely(ctrl->state == NVME_CTRL_LIVE && is_connected))
+		return BLK_STS_OK;
+
+	switch (ctrl->state) {
+	case NVME_CTRL_DELETING:
+		goto reject_io;
+
+	case NVME_CTRL_NEW:
+	case NVME_CTRL_CONNECTING:
+		if (!is_connected)
+			/*
+			 * This is the case of starting a new
+			 * association but connectivity was lost
+			 * before it was fully created. We need to
+			 * error the commands used to initialize the
+			 * controller so the reconnect can go into a
+			 * retry attempt. The commands should all be
+			 * marked REQ_FAILFAST_DRIVER, which will hit
+			 * the reject path below. Anything else will
+			 * be queued while the state settles.
+			 */
+			goto reject_or_queue_io;
+
+		if ((queue_live &&
+		     !(nvme_req(rq)->flags & NVME_REQ_USERCMD)) ||
+		    (!queue_live && blk_rq_is_passthrough(rq) &&
+		     cmd->common.opcode == nvme_fabrics_command &&
+		     cmd->fabrics.fctype == nvme_fabrics_type_connect))
+			/*
+			 * If queue is live, allow only commands that
+			 * are internally generated pass through. These
+			 * are commands on the admin queue to initialize
+			 * the controller. This will reject any ioctl
+			 * admin cmds received while initializing.
+			 *
+			 * If the queue is not live, allow only a
+			 * connect command. This will reject any ioctl
+			 * admin cmd as well as initialization commands
+			 * if the controller reverted the queue to non-live.
+			 */
+			return BLK_STS_OK;
+
+		/*
+		 * fall-thru to the reject_or_queue_io clause
+		 */
+		break;
+
+	/* these cases fall-thru
+	 * case NVME_CTRL_LIVE:
+	 * case NVME_CTRL_RESETTING:
+	 */
+	default:
+		break;
+	}
+
+reject_or_queue_io:
+	/*
+	 * Any other new io is something we're not in a state to send
+	 * to the device. Default action is to busy it and retry it
+	 * after the controller state is recovered. However, anything
+	 * marked for failfast or nvme multipath is immediately failed.
+	 * Note: commands used to initialize the controller will be
+	 *  marked for failfast.
+	 * Note: nvme cli/ioctl commands are marked for failfast.
+	 */
+	if (!blk_noretry_request(rq) && !(rq->cmd_flags & REQ_NVME_MPATH))
+		return BLK_STS_RESOURCE;
+
+reject_io:
+	nvme_req(rq)->status = NVME_SC_ABORT_REQ;
+	return BLK_STS_IOERR;
+}
+EXPORT_SYMBOL_GPL(nvmf_check_if_ready);
+
 static const match_table_t opt_tokens = {
 	{ NVMF_OPT_TRANSPORT,		"transport=%s"		},
 	{ NVMF_OPT_TRADDR,		"traddr=%s"		},
* Unmerged path drivers/nvme/host/fabrics.h
* Unmerged path drivers/nvme/host/fc.c
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 516a12852b11..cefbdff00812 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -85,6 +85,7 @@ struct nvme_request {
 
 enum {
 	NVME_REQ_CANCELLED		= (1 << 0),
+	NVME_REQ_USERCMD		= (1 << 1),
 };
 
 static inline struct nvme_request *nvme_req(struct request *req)
* Unmerged path drivers/nvme/host/rdma.c
* Unmerged path drivers/nvme/target/loop.c
