nfp: bpf: move translation prepare to offload.c

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 9314c442d7ddf749d29c09ab48ffa5333d2bf48e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9314c442.failed

struct nfp_prog is currently only used internally by the translator.
This means there is a lot of parameter passing going on, between
the translator and different stages of offload.  Simplify things
by allocating nfp_prog in offload.c already.

We will now use kmalloc() to allocate the program area and only
DMA map it for the time of loading (instead of allocating DMA
coherent memory upfront).

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 9314c442d7ddf749d29c09ab48ffa5333d2bf48e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/jit.c
#	drivers/net/ethernet/netronome/nfp/bpf/main.h
#	drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/jit.c
index a86508b776ac,eae7a137a7a8..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/jit.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/jit.c
@@@ -2202,64 -2245,27 +2202,79 @@@ static int nfp_bpf_ustore_calc(struct n
  
  /**
   * nfp_bpf_jit() - translate BPF code into NFP assembly
+  * @nfp_prog:	nfp_prog prepared based on @filter
   * @filter:	kernel BPF filter struct
++<<<<<<< HEAD
 + * @prog_mem:	memory to store assembler instructions
 + * @act:	action attached to this eBPF program
 + * @prog_start:	offset of the first instruction when loaded
 + * @prog_done:	where to jump on exit
 + * @prog_sz:	size of @prog_mem in instructions
 + * @res:	achieved parameters of translation results
 + */
 +int
 +nfp_bpf_jit(struct bpf_prog *filter, void *prog_mem,
 +	    enum nfp_bpf_action_type act,
 +	    unsigned int prog_start, unsigned int prog_done,
 +	    unsigned int prog_sz, struct nfp_bpf_result *res)
++=======
+  */
+ int nfp_bpf_jit(struct nfp_prog *nfp_prog, struct bpf_prog *filter)
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  {
- 	struct nfp_prog *nfp_prog;
  	int ret;
  
++<<<<<<< HEAD
 +	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
 +	if (!nfp_prog)
 +		return -ENOMEM;
 +
 +	INIT_LIST_HEAD(&nfp_prog->insns);
 +	nfp_prog->act = act;
 +	nfp_prog->start_off = prog_start;
 +	nfp_prog->tgt_done = prog_done;
 +
 +	ret = nfp_prog_prepare(nfp_prog, filter->insnsi, filter->len);
 +	if (ret)
 +		goto out;
 +
++=======
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  	ret = nfp_prog_verify(nfp_prog, filter);
  	if (ret)
- 		goto out;
+ 		return ret;
  
  	ret = nfp_bpf_optimize(nfp_prog);
  	if (ret)
++<<<<<<< HEAD
 +		goto out;
 +
 +	nfp_prog->num_regs = MAX_BPF_REG;
 +	nfp_prog->regs_per_thread = 32;
 +
 +	nfp_prog->prog = prog_mem;
 +	nfp_prog->__prog_alloc_len = prog_sz;
++=======
+ 		return ret;
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  
  	ret = nfp_translate(nfp_prog);
  	if (ret) {
  		pr_err("Translation failed with error %d (translated: %u)\n",
  		       ret, nfp_prog->n_translated);
- 		ret = -EINVAL;
- 		goto out;
+ 		return -EINVAL;
  	}
  
++<<<<<<< HEAD
 +	ret = nfp_bpf_ustore_calc(nfp_prog, (__force __le64 *)prog_mem);
 +
 +	res->n_instr = nfp_prog->prog_len;
 +	res->dense_mode = false;
 +out:
 +	nfp_prog_free(nfp_prog);
 +
 +	return ret;
++=======
+ 	return nfp_bpf_ustore_calc(nfp_prog, (__force __le64 *)nfp_prog->prog);
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  }
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.h
index cc2a5beba757,36b4eda2d3f8..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.h
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.h
@@@ -181,15 -169,7 +181,19 @@@ struct nfp_prog 
  	struct list_head insns;
  };
  
++<<<<<<< HEAD
 +struct nfp_bpf_result {
 +	unsigned int n_instr;
 +	bool dense_mode;
 +};
 +
 +int
 +nfp_bpf_jit(struct bpf_prog *filter, void *prog, enum nfp_bpf_action_type act,
 +	    unsigned int prog_start, unsigned int prog_done,
 +	    unsigned int prog_sz, struct nfp_bpf_result *res);
++=======
+ int nfp_bpf_jit(struct nfp_prog *nfp_prog, struct bpf_prog *filter);
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  
  int nfp_prog_verify(struct nfp_prog *nfp_prog, struct bpf_prog *prog);
  
diff --cc drivers/net/ethernet/netronome/nfp/bpf/offload.c
index de79faf0874b,c5546c0e87d8..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/offload.c
@@@ -51,149 -51,124 +51,235 @@@
  #include "../nfp_net_ctrl.h"
  #include "../nfp_net.h"
  
++<<<<<<< HEAD
 +void nfp_net_filter_stats_timer(unsigned long data)
++=======
+ static int
+ nfp_prog_prepare(struct nfp_prog *nfp_prog, const struct bpf_insn *prog,
+ 		 unsigned int cnt)
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  {
 -	unsigned int i;
 +	struct nfp_net *nn = (void *)data;
 +	struct nfp_net_bpf_priv *priv;
 +	struct nfp_stat_pair latest;
  
 -	for (i = 0; i < cnt; i++) {
 -		struct nfp_insn_meta *meta;
 +	priv = nn->app_priv;
  
 -		meta = kzalloc(sizeof(*meta), GFP_KERNEL);
 -		if (!meta)
 -			return -ENOMEM;
 +	spin_lock_bh(&priv->rx_filter_lock);
  
 -		meta->insn = prog[i];
 -		meta->n = i;
 +	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 +		mod_timer(&priv->rx_filter_stats_timer,
 +			  jiffies + NFP_NET_STAT_POLL_IVL);
  
 -		list_add_tail(&meta->l, &nfp_prog->insns);
 -	}
 +	spin_unlock_bh(&priv->rx_filter_lock);
 +
 +	latest.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	latest.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +
 +	if (latest.pkts != priv->rx_filter.pkts)
 +		priv->rx_filter_change = jiffies;
 +
 +	priv->rx_filter = latest;
 +}
 +
 +#if 0 /* Not in RHEL7 */
 +static void nfp_net_bpf_stats_reset(struct nfp_net *nn)
 +{
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +
 +	priv->rx_filter.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	priv->rx_filter.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +	priv->rx_filter_prev = priv->rx_filter;
 +	priv->rx_filter_change = jiffies;
 +}
 +
 +static int
 +nfp_net_bpf_stats_update(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
 +{
 +	struct tc_action *a;
 +	LIST_HEAD(actions);
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bytes, pkts;
 +
 +	pkts = priv->rx_filter.pkts - priv->rx_filter_prev.pkts;
 +	bytes = priv->rx_filter.bytes - priv->rx_filter_prev.bytes;
 +	bytes -= pkts * ETH_HLEN;
 +
 +	priv->rx_filter_prev = priv->rx_filter;
 +
 +	preempt_disable();
 +
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list)
 +		tcf_action_stats_update(a, bytes, pkts, nn->rx_filter_change);
 +
 +	preempt_enable();
  
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int
 +nfp_net_bpf_get_act(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
++=======
+ static void nfp_prog_free(struct nfp_prog *nfp_prog)
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  {
 -	struct nfp_insn_meta *meta, *tmp;
 +	const struct tc_action *a;
 +	LIST_HEAD(actions);
 +
 +	if (!cls_bpf->exts)
 +		return NN_ACT_XDP;
 +
 +	/* TC direct action */
 +	if (cls_bpf->exts_integrated) {
 +		if (tc_no_actions(cls_bpf->exts))
 +			return NN_ACT_DIRECT;
 +
 +		return -EOPNOTSUPP;
 +	}
 +
 +	/* TC legacy mode */
 +	if (!tc_single_action(cls_bpf->exts))
 +		return -EOPNOTSUPP;
  
 -	list_for_each_entry_safe(meta, tmp, &nfp_prog->insns, l) {
 -		list_del(&meta->l);
 -		kfree(meta);
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list) {
 +		if (is_tcf_gact_shot(a))
 +			return NN_ACT_TC_DROP;
 +
 +		if (is_tcf_mirred_egress_redirect(a) &&
 +		    tcf_mirred_ifindex(a) == nn->dp.netdev->ifindex)
 +			return NN_ACT_TC_REDIR;
  	}
 -	kfree(nfp_prog);
 +
 +	return -EOPNOTSUPP;
  }
  
++<<<<<<< HEAD
 +static int
 +nfp_net_bpf_offload_prepare(struct nfp_net *nn,
 +			    struct tc_cls_bpf_offload *cls_bpf,
 +			    struct nfp_bpf_result *res,
 +			    void **code, dma_addr_t *dma_addr, u16 max_instr)
 +{
 +	unsigned int code_sz = max_instr * sizeof(u64);
 +	enum nfp_bpf_action_type act;
 +	unsigned int stack_size;
 +	u16 start_off, done_off;
 +	unsigned int max_mtu;
 +	int ret;
 +
 +	ret = nfp_net_bpf_get_act(nn, cls_bpf);
 +	if (ret < 0)
 +		return ret;
 +	act = ret;
 +
 +	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
 +	if (max_mtu < nn->dp.netdev->mtu) {
 +		nn_info(nn, "BPF offload not supported with MTU larger than HW packet split boundary\n");
 +		return -EOPNOTSUPP;
 +	}
++=======
+ static struct nfp_prog *nfp_bpf_verifier_prep(struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog;
+ 	int ret;
+ 
+ 	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
+ 	if (!nfp_prog)
+ 		return NULL;
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
+ 
+ 	INIT_LIST_HEAD(&nfp_prog->insns);
+ 	nfp_prog->type = prog->type;
+ 
+ 	ret = nfp_prog_prepare(nfp_prog, prog->insnsi, prog->len);
+ 	if (ret)
+ 		goto err_free;
+ 
+ 	return nfp_prog;
+ 
+ err_free:
+ 	nfp_prog_free(nfp_prog);
  
- 	start_off = nn_readw(nn, NFP_NET_CFG_BPF_START);
- 	done_off = nn_readw(nn, NFP_NET_CFG_BPF_DONE);
+ 	return NULL;
+ }
+ 
+ static int
+ nfp_bpf_translate(struct nfp_net *nn, struct nfp_prog *nfp_prog,
+ 		  struct bpf_prog *prog)
+ {
+ 	unsigned int stack_size;
+ 	unsigned int max_instr;
  
 +	if (cls_bpf->prog->aux->stack_depth > 64) {
 +		nn_info(nn, "large stack not supported: program %dB > 64B\n",
 +			cls_bpf->prog->aux->stack_depth);
 +		return -EOPNOTSUPP;
 +	}
 +
  	stack_size = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;
 -	if (prog->aux->stack_depth > stack_size) {
 +	if (cls_bpf->prog->aux->stack_depth > stack_size) {
  		nn_info(nn, "stack too large: program %dB > FW stack %dB\n",
 -			prog->aux->stack_depth, stack_size);
 +			cls_bpf->prog->aux->stack_depth, stack_size);
  		return -EOPNOTSUPP;
  	}
  
- 	*code = dma_zalloc_coherent(nn->dp.dev, code_sz, dma_addr, GFP_KERNEL);
- 	if (!*code)
+ 	nfp_prog->stack_depth = prog->aux->stack_depth;
+ 	nfp_prog->start_off = nn_readw(nn, NFP_NET_CFG_BPF_START);
+ 	nfp_prog->tgt_done = nn_readw(nn, NFP_NET_CFG_BPF_DONE);
+ 
+ 	max_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);
+ 	nfp_prog->__prog_alloc_len = max_instr * sizeof(u64);
+ 
+ 	nfp_prog->prog = kmalloc(nfp_prog->__prog_alloc_len, GFP_KERNEL);
+ 	if (!nfp_prog->prog)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	ret = nfp_bpf_jit(cls_bpf->prog, *code, act, start_off, done_off,
 +			  max_instr, res);
 +	if (ret)
 +		goto out;
++=======
+ 	return nfp_bpf_jit(nfp_prog, prog);
+ }
+ 
+ static void nfp_bpf_destroy(struct nfp_prog *nfp_prog)
+ {
+ 	kfree(nfp_prog->prog);
+ 	nfp_prog_free(nfp_prog);
+ }
+ 
+ static struct nfp_prog *
+ nfp_net_bpf_offload_prepare(struct nfp_net *nn, struct bpf_prog *prog,
+ 			    dma_addr_t *dma_addr)
+ {
+ 	struct nfp_prog *nfp_prog;
+ 	unsigned int max_mtu;
+ 	int err;
+ 
+ 	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
+ 	if (max_mtu < nn->dp.netdev->mtu) {
+ 		nn_info(nn, "BPF offload not supported with MTU larger than HW packet split boundary\n");
+ 		return NULL;
+ 	}
+ 
+ 	nfp_prog = nfp_bpf_verifier_prep(prog);
+ 	if (!nfp_prog)
+ 		return NULL;
+ 
+ 	err = nfp_bpf_translate(nn, nfp_prog, prog);
+ 	if (err)
+ 		goto err_destroy_prog;
+ 
+ 	*dma_addr = dma_map_single(nn->dp.dev, nfp_prog->prog,
+ 				   nfp_prog->prog_len * sizeof(u64),
+ 				   DMA_TO_DEVICE);
+ 	if (dma_mapping_error(nn->dp.dev, *dma_addr))
+ 		goto err_destroy_prog;
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  
  	return 0;
  
@@@ -203,28 -178,28 +289,50 @@@ err_destroy_prog
  }
  
  static void
++<<<<<<< HEAD
 +nfp_net_bpf_load_and_start(struct nfp_net *nn, u32 tc_flags,
 +			   void *code, dma_addr_t dma_addr,
 +			   unsigned int code_sz, unsigned int n_instr,
 +			   bool dense_mode)
++=======
+ nfp_net_bpf_load(struct nfp_net *nn, struct nfp_prog *nfp_prog,
+ 		 dma_addr_t dma_addr)
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  {
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bpf_addr = dma_addr;
  	int err;
  
++<<<<<<< HEAD
 +	nn->dp.bpf_offload_skip_sw = !!(tc_flags & TCA_CLS_FLAGS_SKIP_SW);
 +
 +	if (dense_mode)
 +		bpf_addr |= NFP_NET_CFG_BPF_CFG_8CTX;
 +
 +	nn_writew(nn, NFP_NET_CFG_BPF_SIZE, n_instr);
 +	nn_writeq(nn, NFP_NET_CFG_BPF_ADDR, bpf_addr);
++=======
+ 	nn_writew(nn, NFP_NET_CFG_BPF_SIZE, nfp_prog->prog_len);
+ 	nn_writeq(nn, NFP_NET_CFG_BPF_ADDR, dma_addr);
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  
  	/* Load up the JITed code */
  	err = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_BPF);
  	if (err)
  		nn_err(nn, "FW command error while loading BPF: %d\n", err);
  
++<<<<<<< HEAD
++=======
+ 	dma_unmap_single(nn->dp.dev, dma_addr, nfp_prog->prog_len * sizeof(u64),
+ 			 DMA_TO_DEVICE);
+ 	nfp_bpf_destroy(nfp_prog);
+ }
+ 
+ static void nfp_net_bpf_start(struct nfp_net *nn)
+ {
+ 	int err;
+ 
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  	/* Enable passing packets through BPF function */
  	nn->dp.ctrl |= NFP_NET_CFG_CTRL_BPF;
  	nn_writel(nn, NFP_NET_CFG_CTRL, nn->dp.ctrl);
@@@ -246,76 -213,42 +354,104 @@@ static int nfp_net_bpf_stop(struct nfp_
  	if (!(nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF))
  		return 0;
  
 +	spin_lock_bh(&priv->rx_filter_lock);
  	nn->dp.ctrl &= ~NFP_NET_CFG_CTRL_BPF;
 +	spin_unlock_bh(&priv->rx_filter_lock);
  	nn_writel(nn, NFP_NET_CFG_CTRL, nn->dp.ctrl);
  
 +	del_timer_sync(&priv->rx_filter_stats_timer);
 +	nn->dp.bpf_offload_skip_sw = 0;
 +
  	return nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);
  }
 +#endif
  
 -int nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,
 -			bool old_prog)
 +int nfp_net_bpf_offload(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
  {
++<<<<<<< HEAD
 +#if 0 /* Not in RHEL7 */
 +	struct nfp_bpf_result res;
++=======
+ 	struct nfp_prog *nfp_prog;
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  	dma_addr_t dma_addr;
- 	u16 max_instr;
- 	void *code;
- 	int err;
  
++<<<<<<< HEAD
 +	max_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);
 +
 +	switch (cls_bpf->command) {
 +	case TC_CLSBPF_REPLACE:
 +		/* There is nothing stopping us from implementing seamless
 +		 * replace but the simple method of loading I adopted in
 +		 * the firmware does not handle atomic replace (i.e. we have to
 +		 * stop the BPF offload and re-enable it).  Leaking-in a few
 +		 * frames which didn't have BPF applied in the hardware should
 +		 * be fine if software fallback is available, though.
 +		 */
 +		if (nn->dp.bpf_offload_skip_sw)
 +			return -EBUSY;
 +
 +		err = nfp_net_bpf_offload_prepare(nn, cls_bpf, &res, &code,
 +						  &dma_addr, max_instr);
 +		if (err)
 +			return err;
++=======
+ 	if (prog && old_prog) {
+ 		u8 cap;
+ 
+ 		cap = nn_readb(nn, NFP_NET_CFG_BPF_CAP);
+ 		if (!(cap & NFP_NET_BPF_CAP_RELO)) {
+ 			nn_err(nn, "FW does not support live reload\n");
+ 			return -EBUSY;
+ 		}
+ 	}
+ 
+ 	/* Something else is loaded, different program type? */
+ 	if (!old_prog && nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
+ 		return -EBUSY;
+ 
+ 	if (old_prog && !prog)
+ 		return nfp_net_bpf_stop(nn);
+ 
+ 	nfp_prog = nfp_net_bpf_offload_prepare(nn, prog, &dma_addr);
+ 	if (!nfp_prog)
+ 		return -EINVAL;
+ 
+ 	nfp_net_bpf_load(nn, nfp_prog, dma_addr);
+ 	if (!old_prog)
+ 		nfp_net_bpf_start(nn);
++>>>>>>> 9314c442d7dd (nfp: bpf: move translation prepare to offload.c)
  
 -	return 0;
 +		nfp_net_bpf_stop(nn);
 +		nfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,
 +					   dma_addr, max_instr * sizeof(u64),
 +					   res.n_instr, res.dense_mode);
 +		return 0;
 +
 +	case TC_CLSBPF_ADD:
 +		if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 +			return -EBUSY;
 +
 +		err = nfp_net_bpf_offload_prepare(nn, cls_bpf, &res, &code,
 +						  &dma_addr, max_instr);
 +		if (err)
 +			return err;
 +
 +		nfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,
 +					   dma_addr, max_instr * sizeof(u64),
 +					   res.n_instr, res.dense_mode);
 +		return 0;
 +
 +	case TC_CLSBPF_DESTROY:
 +		return nfp_net_bpf_stop(nn);
 +
 +	case TC_CLSBPF_STATS:
 +		return nfp_net_bpf_stats_update(nn, cls_bpf);
 +
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +#else
 +	return -EOPNOTSUPP;
 +#endif
  }
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/jit.c
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.h
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/offload.c
