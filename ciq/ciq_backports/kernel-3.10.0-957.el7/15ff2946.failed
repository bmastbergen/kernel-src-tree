mmc: block: make function mmc_cqe_issue_type static

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mmc] block: make function mmc_cqe_issue_type static (Gopal Tiwari) [1456570]
Rebuild_FUZZ: 94.85%
commit-author Colin Ian King <colin.king@canonical.com>
commit 15ff2946b3c9661b14fc5123902dad28e1f13f3e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/15ff2946.failed

The function mmc_cqe_issue_type is local to the source and does
not need to be in global scope, so make it static.

Cleans up sparse warning:
drivers/mmc/core/queue.c:62:21: warning: symbol 'mmc_cqe_issue_type'
was not declared. Should it be static?

	Signed-off-by: Colin Ian King <colin.king@canonical.com>
	Acked-by: Adrian Hunter <adrian.hunter@intel.com>
	Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
(cherry picked from commit 15ff2946b3c9661b14fc5123902dad28e1f13f3e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/mmc/core/queue.c
diff --cc drivers/mmc/core/queue.c
index 8704591ee805,5db388081789..000000000000
--- a/drivers/mmc/core/queue.c
+++ b/drivers/mmc/core/queue.c
@@@ -47,6 -40,147 +47,150 @@@ static int mmc_prep_request(struct requ
  	return BLKPREP_OK;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool mmc_cqe_dcmd_busy(struct mmc_queue *mq)
+ {
+ 	/* Allow only 1 DCMD at a time */
+ 	return mq->in_flight[MMC_ISSUE_DCMD];
+ }
+ 
+ void mmc_cqe_check_busy(struct mmc_queue *mq)
+ {
+ 	if ((mq->cqe_busy & MMC_CQE_DCMD_BUSY) && !mmc_cqe_dcmd_busy(mq))
+ 		mq->cqe_busy &= ~MMC_CQE_DCMD_BUSY;
+ 
+ 	mq->cqe_busy &= ~MMC_CQE_QUEUE_FULL;
+ }
+ 
+ static inline bool mmc_cqe_can_dcmd(struct mmc_host *host)
+ {
+ 	return host->caps2 & MMC_CAP2_CQE_DCMD;
+ }
+ 
+ static enum mmc_issue_type mmc_cqe_issue_type(struct mmc_host *host,
+ 					      struct request *req)
+ {
+ 	switch (req_op(req)) {
+ 	case REQ_OP_DRV_IN:
+ 	case REQ_OP_DRV_OUT:
+ 	case REQ_OP_DISCARD:
+ 	case REQ_OP_SECURE_ERASE:
+ 		return MMC_ISSUE_SYNC;
+ 	case REQ_OP_FLUSH:
+ 		return mmc_cqe_can_dcmd(host) ? MMC_ISSUE_DCMD : MMC_ISSUE_SYNC;
+ 	default:
+ 		return MMC_ISSUE_ASYNC;
+ 	}
+ }
+ 
+ enum mmc_issue_type mmc_issue_type(struct mmc_queue *mq, struct request *req)
+ {
+ 	struct mmc_host *host = mq->card->host;
+ 
+ 	if (mq->use_cqe)
+ 		return mmc_cqe_issue_type(host, req);
+ 
+ 	if (req_op(req) == REQ_OP_READ || req_op(req) == REQ_OP_WRITE)
+ 		return MMC_ISSUE_ASYNC;
+ 
+ 	return MMC_ISSUE_SYNC;
+ }
+ 
+ static void __mmc_cqe_recovery_notifier(struct mmc_queue *mq)
+ {
+ 	if (!mq->recovery_needed) {
+ 		mq->recovery_needed = true;
+ 		schedule_work(&mq->recovery_work);
+ 	}
+ }
+ 
+ void mmc_cqe_recovery_notifier(struct mmc_request *mrq)
+ {
+ 	struct mmc_queue_req *mqrq = container_of(mrq, struct mmc_queue_req,
+ 						  brq.mrq);
+ 	struct request *req = mmc_queue_req_to_req(mqrq);
+ 	struct request_queue *q = req->q;
+ 	struct mmc_queue *mq = q->queuedata;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(q->queue_lock, flags);
+ 	__mmc_cqe_recovery_notifier(mq);
+ 	spin_unlock_irqrestore(q->queue_lock, flags);
+ }
+ 
+ static enum blk_eh_timer_return mmc_cqe_timed_out(struct request *req)
+ {
+ 	struct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);
+ 	struct mmc_request *mrq = &mqrq->brq.mrq;
+ 	struct mmc_queue *mq = req->q->queuedata;
+ 	struct mmc_host *host = mq->card->host;
+ 	enum mmc_issue_type issue_type = mmc_issue_type(mq, req);
+ 	bool recovery_needed = false;
+ 
+ 	switch (issue_type) {
+ 	case MMC_ISSUE_ASYNC:
+ 	case MMC_ISSUE_DCMD:
+ 		if (host->cqe_ops->cqe_timeout(host, mrq, &recovery_needed)) {
+ 			if (recovery_needed)
+ 				__mmc_cqe_recovery_notifier(mq);
+ 			return BLK_EH_RESET_TIMER;
+ 		}
+ 		/* No timeout */
+ 		return BLK_EH_HANDLED;
+ 	default:
+ 		/* Timeout is handled by mmc core */
+ 		return BLK_EH_RESET_TIMER;
+ 	}
+ }
+ 
+ static enum blk_eh_timer_return mmc_mq_timed_out(struct request *req,
+ 						 bool reserved)
+ {
+ 	struct request_queue *q = req->q;
+ 	struct mmc_queue *mq = q->queuedata;
+ 	unsigned long flags;
+ 	int ret;
+ 
+ 	spin_lock_irqsave(q->queue_lock, flags);
+ 
+ 	if (mq->recovery_needed || !mq->use_cqe)
+ 		ret = BLK_EH_RESET_TIMER;
+ 	else
+ 		ret = mmc_cqe_timed_out(req);
+ 
+ 	spin_unlock_irqrestore(q->queue_lock, flags);
+ 
+ 	return ret;
+ }
+ 
+ static void mmc_mq_recovery_handler(struct work_struct *work)
+ {
+ 	struct mmc_queue *mq = container_of(work, struct mmc_queue,
+ 					    recovery_work);
+ 	struct request_queue *q = mq->queue;
+ 
+ 	mmc_get_card(mq->card, &mq->ctx);
+ 
+ 	mq->in_recovery = true;
+ 
+ 	if (mq->use_cqe)
+ 		mmc_blk_cqe_recovery(mq);
+ 	else
+ 		mmc_blk_mq_recovery(mq);
+ 
+ 	mq->in_recovery = false;
+ 
+ 	spin_lock_irq(q->queue_lock);
+ 	mq->recovery_needed = false;
+ 	spin_unlock_irq(q->queue_lock);
+ 
+ 	mmc_put_card(mq->card, &mq->ctx);
+ 
+ 	blk_mq_run_hw_queues(q, true);
+ }
+ 
++>>>>>>> 15ff2946b3c9 (mmc: block: make function mmc_cqe_issue_type static)
  static int mmc_queue_thread(void *d)
  {
  	struct mmc_queue *mq = d;
* Unmerged path drivers/mmc/core/queue.c
