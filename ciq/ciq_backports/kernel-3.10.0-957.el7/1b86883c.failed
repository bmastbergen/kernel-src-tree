x86/bugs: Read SPEC_CTRL MSR during boot and re-use reserved bits

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] bugs: Read SPEC_CTRL MSR during boot and re-use (Waiman Long) [1566905] {CVE-2018-3639}
Rebuild_FUZZ: 83.93%
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 1b86883ccb8d5d9506529d42dbe1a5257cb30b18
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/1b86883c.failed

The 336996-Speculative-Execution-Side-Channel-Mitigations.pdf refers to all
the other bits as reserved. The Intel SDM glossary defines reserved as
implementation specific - aka unknown.

As such at bootup this must be taken it into account and proper masking for
the bits in use applied.

A copy of this document is available at
https://bugzilla.kernel.org/show_bug.cgi?id=199511

[ tglx: Made x86_spec_ctrl_base __ro_after_init ]

	Suggested-by: Jon Masters <jcm@redhat.com>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 1b86883ccb8d5d9506529d42dbe1a5257cb30b18)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
diff --cc arch/x86/include/asm/nospec-branch.h
index d5b6abbaaa24,9ec3d4d448cd..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -169,43 -211,125 +169,104 @@@
  enum spectre_v2_mitigation {
  	SPECTRE_V2_NONE,
  	SPECTRE_V2_RETPOLINE_MINIMAL,
 -	SPECTRE_V2_RETPOLINE_MINIMAL_AMD,
 -	SPECTRE_V2_RETPOLINE_GENERIC,
 -	SPECTRE_V2_RETPOLINE_AMD,
 +	SPECTRE_V2_RETPOLINE_NO_IBPB,
 +	SPECTRE_V2_RETPOLINE_SKYLAKE,
 +	SPECTRE_V2_RETPOLINE_UNSAFE_MODULE,
 +	SPECTRE_V2_RETPOLINE,
 +	SPECTRE_V2_RETPOLINE_IBRS_USER,
  	SPECTRE_V2_IBRS,
 +	SPECTRE_V2_IBRS_ALWAYS,
 +	SPECTRE_V2_IBP_DISABLED,
  };
  
++<<<<<<< HEAD
 +void __spectre_v2_select_mitigation(void);
 +void spectre_v2_print_mitigation(void);
 +
 +static inline bool retp_compiler(void)
 +{
 +#ifdef RETPOLINE
 +	return true;
 +#else
 +	return false;
 +#endif
 +}
++=======
+ /*
+  * The Intel specification for the SPEC_CTRL MSR requires that we
+  * preserve any already set reserved bits at boot time (e.g. for
+  * future additions that this kernel is not currently aware of).
+  * We then set any additional mitigation bits that we want
+  * ourselves and always use this as the base for SPEC_CTRL.
+  * We also use this when handling guest entry/exit as below.
+  */
+ extern void x86_spec_ctrl_set(u64);
+ extern u64 x86_spec_ctrl_get_default(void);
+ 
+ extern char __indirect_thunk_start[];
+ extern char __indirect_thunk_end[];
++>>>>>>> 1b86883ccb8d (x86/bugs: Read SPEC_CTRL MSR during boot and re-use reserved bits)
  
  /*
   * On VMEXIT we must ensure that no RSB predictions learned in the guest
   * can be followed in the host, by overwriting the RSB completely. Both
   * retpoline and IBRS mitigations for Spectre v2 need this; only on future
 - * CPUs with IBRS_ALL *might* it be avoided.
 + * CPUs with IBRS_ATT *might* it be avoided.
   */
 -static inline void vmexit_fill_RSB(void)
 +static inline void fill_RSB(void)
  {
 -#ifdef CONFIG_RETPOLINE
  	unsigned long loops;
 +	register unsigned long sp asm(_ASM_SP);
  
 -	asm volatile (ANNOTATE_NOSPEC_ALTERNATIVE
 -		      ALTERNATIVE("jmp 910f",
 -				  __stringify(__FILL_RETURN_BUFFER(%0, RSB_CLEAR_LOOPS, %1)),
 -				  X86_FEATURE_RETPOLINE)
 -		      "910:"
 -		      : "=r" (loops), ASM_CALL_CONSTRAINT
 +	asm volatile (__stringify(__FILL_RETURN_BUFFER(%0, RSB_CLEAR_LOOPS, %1))
 +		      : "=r" (loops), "+r" (sp)
  		      : : "memory" );
 -#endif
  }
  
++<<<<<<< HEAD
++=======
+ static __always_inline
+ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
+ {
+ 	asm volatile(ALTERNATIVE("", "wrmsr", %c[feature])
+ 		: : "c" (msr),
+ 		    "a" (val),
+ 		    "d" (val >> 32),
+ 		    [feature] "i" (feature)
+ 		: "memory");
+ }
+ 
+ static inline void indirect_branch_prediction_barrier(void)
+ {
+ 	u64 val = PRED_CMD_IBPB;
+ 
+ 	alternative_msr_write(MSR_IA32_PRED_CMD, val, X86_FEATURE_USE_IBPB);
+ }
+ 
+ /*
+  * With retpoline, we must use IBRS to restrict branch prediction
+  * before calling into firmware.
+  *
+  * (Implemented as CPP macros due to header hell.)
+  */
+ #define firmware_restrict_branch_speculation_start()			\
+ do {									\
+ 	u64 val = x86_spec_ctrl_get_default() | SPEC_CTRL_IBRS;		\
+ 									\
+ 	preempt_disable();						\
+ 	alternative_msr_write(MSR_IA32_SPEC_CTRL, val,			\
+ 			      X86_FEATURE_USE_IBRS_FW);			\
+ } while (0)
+ 
+ #define firmware_restrict_branch_speculation_end()			\
+ do {									\
+ 	u64 val = x86_spec_ctrl_get_default();				\
+ 									\
+ 	alternative_msr_write(MSR_IA32_SPEC_CTRL, val,			\
+ 			      X86_FEATURE_USE_IBRS_FW);			\
+ 	preempt_enable();						\
+ } while (0)
+ 
++>>>>>>> 1b86883ccb8d (x86/bugs: Read SPEC_CTRL MSR during boot and re-use reserved bits)
  #endif /* __ASSEMBLY__ */
 -
 -/*
 - * Below is used in the eBPF JIT compiler and emits the byte sequence
 - * for the following assembly:
 - *
 - * With retpolines configured:
 - *
 - *    callq do_rop
 - *  spec_trap:
 - *    pause
 - *    lfence
 - *    jmp spec_trap
 - *  do_rop:
 - *    mov %rax,(%rsp)
 - *    retq
 - *
 - * Without retpolines configured:
 - *
 - *    jmp *%rax
 - */
 -#ifdef CONFIG_RETPOLINE
 -# define RETPOLINE_RAX_BPF_JIT_SIZE	17
 -# define RETPOLINE_RAX_BPF_JIT()				\
 -	EMIT1_off32(0xE8, 7);	 /* callq do_rop */		\
 -	/* spec_trap: */					\
 -	EMIT2(0xF3, 0x90);       /* pause */			\
 -	EMIT3(0x0F, 0xAE, 0xE8); /* lfence */			\
 -	EMIT2(0xEB, 0xF9);       /* jmp spec_trap */		\
 -	/* do_rop: */						\
 -	EMIT4(0x48, 0x89, 0x04, 0x24); /* mov %rax,(%rsp) */	\
 -	EMIT1(0xC3);             /* retq */
 -#else
 -# define RETPOLINE_RAX_BPF_JIT_SIZE	2
 -# define RETPOLINE_RAX_BPF_JIT()				\
 -	EMIT2(0xFF, 0xE0);	 /* jmp *%rax */
 -#endif
 -
 -#endif /* _ASM_X86_NOSPEC_BRANCH_H_ */
 +#endif /* __NOSPEC_BRANCH_H__ */
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,6ed84f50d74a..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -35,9 -43,15 +41,20 @@@ void __init check_bugs(void
  		print_cpu_info(&boot_cpu_data);
  	}
  
++<<<<<<< HEAD
 +	spec_ctrl_init();
++=======
+ 	/*
+ 	 * Read the SPEC_CTRL MSR to account for reserved bits which may
+ 	 * have unknown values.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_IBRS))
+ 		rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ 
+ 	/* Select the proper spectre mitigation before patching alternatives */
++>>>>>>> 1b86883ccb8d (x86/bugs: Read SPEC_CTRL MSR during boot and re-use reserved bits)
  	spectre_v2_select_mitigation();
 +	spec_ctrl_cpu_init();
  
  #ifdef CONFIG_X86_32
  	/*
@@@ -104,6 -106,61 +121,64 @@@ enum spectre_v2_mitigation_cmd spectre_
  #undef pr_fmt
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
++<<<<<<< HEAD
++=======
+ static enum spectre_v2_mitigation spectre_v2_enabled = SPECTRE_V2_NONE;
+ 
+ void x86_spec_ctrl_set(u64 val)
+ {
+ 	if (val & ~SPEC_CTRL_IBRS)
+ 		WARN_ONCE(1, "SPEC_CTRL MSR value 0x%16llx is unknown.\n", val);
+ 	else
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base | val);
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_set);
+ 
+ u64 x86_spec_ctrl_get_default(void)
+ {
+ 	return x86_spec_ctrl_base;
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_get_default);
+ 
+ #ifdef RETPOLINE
+ static bool spectre_v2_bad_module;
+ 
+ bool retpoline_module_ok(bool has_retpoline)
+ {
+ 	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
+ 		return true;
+ 
+ 	pr_err("System may be vulnerable to spectre v2\n");
+ 	spectre_v2_bad_module = true;
+ 	return false;
+ }
+ 
+ static inline const char *spectre_v2_module_string(void)
+ {
+ 	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
+ }
+ #else
+ static inline const char *spectre_v2_module_string(void) { return ""; }
+ #endif
+ 
+ static void __init spec2_print_if_insecure(const char *reason)
+ {
+ 	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static void __init spec2_print_if_secure(const char *reason)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static inline bool retp_compiler(void)
+ {
+ 	return __is_defined(RETPOLINE);
+ }
+ 
++>>>>>>> 1b86883ccb8d (x86/bugs: Read SPEC_CTRL MSR during boot and re-use reserved bits)
  static inline bool match_option(const char *arg, int arglen, const char *opt)
  {
  	int len = strlen(opt);
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
