md/raid5-cache: bump flush stripe batch size

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] raid5-cache: bump flush stripe batch size (Nigel Croxon) [1494474]
Rebuild_FUZZ: 96.47%
commit-author Shaohua Li <shli@fb.com>
commit 84890c03b6c5d7e8d76ea5e20b6aaf7e7ad410f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/84890c03.failed

Bump the flush stripe batch size to 2048. For my 12 disks raid
array, the stripes takes:
12 * 4k * 2048 = 96MB

This is still quite small. A hardware raid card generally has 1GB size,
which we suggest the raid5-cache has similar cache size.

The advantage of a big batch size is we can dispatch a lot of IO in the
same time, then we can do some scheduling to make better IO pattern.

Last patch prioritizes stripes, so we don't worry about a big flush
stripe batch will starve normal stripes.

	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 84890c03b6c5d7e8d76ea5e20b6aaf7e7ad410f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5-cache.c
diff --cc drivers/md/raid5-cache.c
index 45f5446ca2a5,738e180d3740..000000000000
--- a/drivers/md/raid5-cache.c
+++ b/drivers/md/raid5-cache.c
@@@ -1230,8 -1392,8 +1230,13 @@@ static void r5c_do_reclaim(struct r5con
  		 */
  		stripes_to_flush = R5C_RECLAIM_STRIPE_GROUP;
  	else if (total_cached > conf->min_nr_stripes * 1 / 2 ||
++<<<<<<< HEAD
 +		 atomic_read(&conf->r5c_cached_full_stripes) >
 +		 R5C_FULL_STRIPE_FLUSH_BATCH)
++=======
+ 		 atomic_read(&conf->r5c_cached_full_stripes) - flushing_full >
+ 		 R5C_FULL_STRIPE_FLUSH_BATCH(conf))
++>>>>>>> 84890c03b6c5 (md/raid5-cache: bump flush stripe batch size)
  		/*
  		 * if stripe cache pressure moderate, or if there is many full
  		 * stripes,flush all full stripes
* Unmerged path drivers/md/raid5-cache.c
