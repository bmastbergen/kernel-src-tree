tcp: refine tcp_prune_ofo_queue() to not drop all packets

jira LE-1907
cve CVE-2018-5390
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 36a6503feddadbbad415fb3891e80f94c10a9b21
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/36a6503f.failed

Over the years, TCP BDP has increased a lot, and is typically
in the order of ~10 Mbytes with help of clever Congestion Control
modules.

In presence of packet losses, TCP stores incoming packets into an out of
order queue, and number of skbs sitting there waiting for the missing
packets to be received can match the BDP (~10 Mbytes)

In some cases, TCP needs to make room for incoming skbs, and current
strategy can simply remove all skbs in the out of order queue as a last
resort, incurring a huge penalty, both for receiver and sender.

Unfortunately these 'last resort events' are quite frequent, forcing
sender to send all packets again, stalling the flow and wasting a lot of
resources.

This patch cleans only a part of the out of order queue in order
to meet the memory constraints.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Neal Cardwell <ncardwell@google.com>
	Cc: Yuchung Cheng <ycheng@google.com>
	Cc: Soheil Hassas Yeganeh <soheil@google.com>
	Cc: C. Stephen Gun <csg@google.com>
	Cc: Van Jacobson <vanj@google.com>
	Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
	Acked-by: Yuchung Cheng <ycheng@google.com>
	Acked-by: Neal Cardwell <ncardwell@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 36a6503feddadbbad415fb3891e80f94c10a9b21)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_input.c
diff --cc net/ipv4/tcp_input.c
index 4ec1e4f8ab44,8cd02c0b056c..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -4754,23 -4883,29 +4757,35 @@@ static void tcp_collapse_ofo_queue(stru
  static bool tcp_prune_ofo_queue(struct sock *sk)
  {
  	struct tcp_sock *tp = tcp_sk(sk);
- 	bool res = false;
+ 	struct sk_buff *skb;
  
++<<<<<<< HEAD
 +	if (!skb_queue_empty(&tp->out_of_order_queue)) {
 +		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_OFOPRUNED);
 +		__skb_queue_purge(&tp->out_of_order_queue);
++=======
+ 	if (skb_queue_empty(&tp->out_of_order_queue))
+ 		return false;
++>>>>>>> 36a6503fedda (tcp: refine tcp_prune_ofo_queue() to not drop all packets)
  
- 		/* Reset SACK state.  A conforming SACK implementation will
- 		 * do the same at a timeout based retransmit.  When a connection
- 		 * is in a sad state like this, we care only about integrity
- 		 * of the connection not performance.
- 		 */
- 		if (tp->rx_opt.sack_ok)
- 			tcp_sack_reset(&tp->rx_opt);
+ 	NET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);
+ 
+ 	while ((skb = __skb_dequeue_tail(&tp->out_of_order_queue)) != NULL) {
+ 		tcp_drop(sk, skb);
  		sk_mem_reclaim(sk);
- 		res = true;
+ 		if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&
+ 		    !tcp_under_memory_pressure(sk))
+ 			break;
  	}
- 	return res;
+ 
+ 	/* Reset SACK state.  A conforming SACK implementation will
+ 	 * do the same at a timeout based retransmit.  When a connection
+ 	 * is in a sad state like this, we care only about integrity
+ 	 * of the connection not performance.
+ 	 */
+ 	if (tp->rx_opt.sack_ok)
+ 		tcp_sack_reset(&tp->rx_opt);
+ 	return true;
  }
  
  /* Reduce allocated memory if we can, trying to get
* Unmerged path net/ipv4/tcp_input.c
