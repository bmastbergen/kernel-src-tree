cpu/hotplug: Boot HT siblings at least once

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [kernel] cpu/hotplug: boot ht siblings at least once, part 2 (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 91.49%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 0cc3cd21657be04cb0559fe8063f2130493f92cf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/0cc3cd21.failed

Due to the way Machine Check Exceptions work on X86 hyperthreads it's
required to boot up _all_ logical cores at least once in order to set the
CR4.MCE bit.

So instead of ignoring the sibling threads right away, let them boot up
once so they can configure themselves. After they came out of the initial
boot stage check whether its a "secondary" sibling and cancel the operation
which puts the CPU back into offline state.

	Reported-by: Dave Hansen <dave.hansen@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Tony Luck <tony.luck@intel.com>

(cherry picked from commit 0cc3cd21657be04cb0559fe8063f2130493f92cf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cpu.c
diff --cc kernel/cpu.c
index 0d9e250d0ea0,6f3a3cde8b83..000000000000
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@@ -24,9 -26,240 +24,213 @@@
  
  #include "smpboot.h"
  
 -/**
 - * cpuhp_cpu_state - Per cpu hotplug state storage
 - * @state:	The current cpu state
 - * @target:	The target state
 - * @thread:	Pointer to the hotplug thread
 - * @should_run:	Thread should execute
 - * @rollback:	Perform a rollback
 - * @single:	Single callback invocation
 - * @bringup:	Single callback bringup or teardown selector
 - * @cb_state:	The state for a single callback (install/uninstall)
 - * @result:	Result of the operation
 - * @done_up:	Signal completion to the issuer of the task for cpu-up
 - * @done_down:	Signal completion to the issuer of the task for cpu-down
 - */
 -struct cpuhp_cpu_state {
 -	enum cpuhp_state	state;
 -	enum cpuhp_state	target;
 -	enum cpuhp_state	fail;
  #ifdef CONFIG_SMP
++<<<<<<< HEAD
++=======
+ 	struct task_struct	*thread;
+ 	bool			should_run;
+ 	bool			rollback;
+ 	bool			single;
+ 	bool			bringup;
+ 	bool			booted_once;
+ 	struct hlist_node	*node;
+ 	struct hlist_node	*last;
+ 	enum cpuhp_state	cb_state;
+ 	int			result;
+ 	struct completion	done_up;
+ 	struct completion	done_down;
+ #endif
+ };
+ 
+ static DEFINE_PER_CPU(struct cpuhp_cpu_state, cpuhp_state) = {
+ 	.fail = CPUHP_INVALID,
+ };
+ 
+ #if defined(CONFIG_LOCKDEP) && defined(CONFIG_SMP)
+ static struct lockdep_map cpuhp_state_up_map =
+ 	STATIC_LOCKDEP_MAP_INIT("cpuhp_state-up", &cpuhp_state_up_map);
+ static struct lockdep_map cpuhp_state_down_map =
+ 	STATIC_LOCKDEP_MAP_INIT("cpuhp_state-down", &cpuhp_state_down_map);
+ 
+ 
+ static inline void cpuhp_lock_acquire(bool bringup)
+ {
+ 	lock_map_acquire(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);
+ }
+ 
+ static inline void cpuhp_lock_release(bool bringup)
+ {
+ 	lock_map_release(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);
+ }
+ #else
+ 
+ static inline void cpuhp_lock_acquire(bool bringup) { }
+ static inline void cpuhp_lock_release(bool bringup) { }
+ 
+ #endif
+ 
+ /**
+  * cpuhp_step - Hotplug state machine step
+  * @name:	Name of the step
+  * @startup:	Startup function of the step
+  * @teardown:	Teardown function of the step
+  * @skip_onerr:	Do not invoke the functions on error rollback
+  *		Will go away once the notifiers	are gone
+  * @cant_stop:	Bringup/teardown can't be stopped at this step
+  */
+ struct cpuhp_step {
+ 	const char		*name;
+ 	union {
+ 		int		(*single)(unsigned int cpu);
+ 		int		(*multi)(unsigned int cpu,
+ 					 struct hlist_node *node);
+ 	} startup;
+ 	union {
+ 		int		(*single)(unsigned int cpu);
+ 		int		(*multi)(unsigned int cpu,
+ 					 struct hlist_node *node);
+ 	} teardown;
+ 	struct hlist_head	list;
+ 	bool			skip_onerr;
+ 	bool			cant_stop;
+ 	bool			multi_instance;
+ };
+ 
+ static DEFINE_MUTEX(cpuhp_state_mutex);
+ static struct cpuhp_step cpuhp_hp_states[];
+ 
+ static struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)
+ {
+ 	return cpuhp_hp_states + state;
+ }
+ 
+ /**
+  * cpuhp_invoke_callback _ Invoke the callbacks for a given state
+  * @cpu:	The cpu for which the callback should be invoked
+  * @state:	The state to do callbacks for
+  * @bringup:	True if the bringup callback should be invoked
+  * @node:	For multi-instance, do a single entry callback for install/remove
+  * @lastp:	For multi-instance rollback, remember how far we got
+  *
+  * Called from cpu hotplug and from the state register machinery.
+  */
+ static int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,
+ 				 bool bringup, struct hlist_node *node,
+ 				 struct hlist_node **lastp)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 	struct cpuhp_step *step = cpuhp_get_step(state);
+ 	int (*cbm)(unsigned int cpu, struct hlist_node *node);
+ 	int (*cb)(unsigned int cpu);
+ 	int ret, cnt;
+ 
+ 	if (st->fail == state) {
+ 		st->fail = CPUHP_INVALID;
+ 
+ 		if (!(bringup ? step->startup.single : step->teardown.single))
+ 			return 0;
+ 
+ 		return -EAGAIN;
+ 	}
+ 
+ 	if (!step->multi_instance) {
+ 		WARN_ON_ONCE(lastp && *lastp);
+ 		cb = bringup ? step->startup.single : step->teardown.single;
+ 		if (!cb)
+ 			return 0;
+ 		trace_cpuhp_enter(cpu, st->target, state, cb);
+ 		ret = cb(cpu);
+ 		trace_cpuhp_exit(cpu, st->state, state, ret);
+ 		return ret;
+ 	}
+ 	cbm = bringup ? step->startup.multi : step->teardown.multi;
+ 	if (!cbm)
+ 		return 0;
+ 
+ 	/* Single invocation for instance add/remove */
+ 	if (node) {
+ 		WARN_ON_ONCE(lastp && *lastp);
+ 		trace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);
+ 		ret = cbm(cpu, node);
+ 		trace_cpuhp_exit(cpu, st->state, state, ret);
+ 		return ret;
+ 	}
+ 
+ 	/* State transition. Invoke on all instances */
+ 	cnt = 0;
+ 	hlist_for_each(node, &step->list) {
+ 		if (lastp && node == *lastp)
+ 			break;
+ 
+ 		trace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);
+ 		ret = cbm(cpu, node);
+ 		trace_cpuhp_exit(cpu, st->state, state, ret);
+ 		if (ret) {
+ 			if (!lastp)
+ 				goto err;
+ 
+ 			*lastp = node;
+ 			return ret;
+ 		}
+ 		cnt++;
+ 	}
+ 	if (lastp)
+ 		*lastp = NULL;
+ 	return 0;
+ err:
+ 	/* Rollback the instances if one failed */
+ 	cbm = !bringup ? step->startup.multi : step->teardown.multi;
+ 	if (!cbm)
+ 		return ret;
+ 
+ 	hlist_for_each(node, &step->list) {
+ 		if (!cnt--)
+ 			break;
+ 
+ 		trace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);
+ 		ret = cbm(cpu, node);
+ 		trace_cpuhp_exit(cpu, st->state, state, ret);
+ 		/*
+ 		 * Rollback must not fail,
+ 		 */
+ 		WARN_ON_ONCE(ret);
+ 	}
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_SMP
+ static bool cpuhp_is_ap_state(enum cpuhp_state state)
+ {
+ 	/*
+ 	 * The extra check for CPUHP_TEARDOWN_CPU is only for documentation
+ 	 * purposes as that state is handled explicitly in cpu_down.
+ 	 */
+ 	return state > CPUHP_BRINGUP_CPU && state != CPUHP_TEARDOWN_CPU;
+ }
+ 
+ static inline void wait_for_ap_thread(struct cpuhp_cpu_state *st, bool bringup)
+ {
+ 	struct completion *done = bringup ? &st->done_up : &st->done_down;
+ 	wait_for_completion(done);
+ }
+ 
+ static inline void complete_ap_thread(struct cpuhp_cpu_state *st, bool bringup)
+ {
+ 	struct completion *done = bringup ? &st->done_up : &st->done_down;
+ 	complete(done);
+ }
+ 
+ /*
+  * The former STARTING/DYING states, ran with IRQs disabled and must not fail.
+  */
+ static bool cpuhp_is_atomic_state(enum cpuhp_state state)
+ {
+ 	return CPUHP_AP_IDLE_DEAD <= state && state < CPUHP_AP_ONLINE;
+ }
+ 
++>>>>>>> 0cc3cd21657b (cpu/hotplug: Boot HT siblings at least once)
  /* Serializes the updates to cpu_online_mask, cpu_present_mask */
  static DEFINE_MUTEX(cpu_add_remove_lock);
 -bool cpuhp_tasks_frozen;
 -EXPORT_SYMBOL_GPL(cpuhp_tasks_frozen);
  
  /*
   * The following two APIs (cpu_maps_update_begin/done) must be used when
@@@ -216,69 -337,385 +420,136 @@@ void cpu_hotplug_disable(void
  void cpu_hotplug_enable(void)
  {
  	cpu_maps_update_begin();
 -	__cpu_hotplug_enable();
 +	cpu_hotplug_disabled = 0;
  	cpu_maps_update_done();
  }
 -EXPORT_SYMBOL_GPL(cpu_hotplug_enable);
 -#endif	/* CONFIG_HOTPLUG_CPU */
  
++<<<<<<< HEAD
 +#else /* #if CONFIG_HOTPLUG_CPU */
 +static void cpu_hotplug_begin(void) {}
 +static void cpu_hotplug_done(void) {}
 +#endif	/* #else #if CONFIG_HOTPLUG_CPU */
 +
 +/* Need to know about CPUs going up/down? */
 +int __ref register_cpu_notifier(struct notifier_block *nb)
++=======
+ #ifdef CONFIG_HOTPLUG_SMT
+ enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
+ 
+ static int __init smt_cmdline_disable(char *str)
+ {
+ 	cpu_smt_control = CPU_SMT_DISABLED;
+ 	if (str && !strcmp(str, "force")) {
+ 		pr_info("SMT: Force disabled\n");
+ 		cpu_smt_control = CPU_SMT_FORCE_DISABLED;
+ 	}
+ 	return 0;
+ }
+ early_param("nosmt", smt_cmdline_disable);
+ 
+ static inline bool cpu_smt_allowed(unsigned int cpu)
+ {
+ 	if (cpu_smt_control == CPU_SMT_ENABLED)
+ 		return true;
+ 
+ 	if (topology_is_primary_thread(cpu))
+ 		return true;
+ 
+ 	/*
+ 	 * On x86 it's required to boot all logical CPUs at least once so
+ 	 * that the init code can get a chance to set CR4.MCE on each
+ 	 * CPU. Otherwise, a broadacasted MCE observing CR4.MCE=0b on any
+ 	 * core will shutdown the machine.
+ 	 */
+ 	return !per_cpu(cpuhp_state, cpu).booted_once;
+ }
+ #else
+ static inline bool cpu_smt_allowed(unsigned int cpu) { return true; }
+ #endif
+ 
+ static inline enum cpuhp_state
+ cpuhp_set_state(struct cpuhp_cpu_state *st, enum cpuhp_state target)
++>>>>>>> 0cc3cd21657b (cpu/hotplug: Boot HT siblings at least once)
  {
 -	enum cpuhp_state prev_state = st->state;
 -
 -	st->rollback = false;
 -	st->last = NULL;
 -
 -	st->target = target;
 -	st->single = false;
 -	st->bringup = st->state < target;
 -
 -	return prev_state;
 -}
 -
 -static inline void
 -cpuhp_reset_state(struct cpuhp_cpu_state *st, enum cpuhp_state prev_state)
 -{
 -	st->rollback = true;
 -
 -	/*
 -	 * If we have st->last we need to undo partial multi_instance of this
 -	 * state first. Otherwise start undo at the previous state.
 -	 */
 -	if (!st->last) {
 -		if (st->bringup)
 -			st->state--;
 -		else
 -			st->state++;
 -	}
 -
 -	st->target = prev_state;
 -	st->bringup = !st->bringup;
 -}
 -
 -/* Regular hotplug invocation of the AP hotplug thread */
 -static void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)
 -{
 -	if (!st->single && st->state == st->target)
 -		return;
 -
 -	st->result = 0;
 -	/*
 -	 * Make sure the above stores are visible before should_run becomes
 -	 * true. Paired with the mb() above in cpuhp_thread_fun()
 -	 */
 -	smp_mb();
 -	st->should_run = true;
 -	wake_up_process(st->thread);
 -	wait_for_ap_thread(st, st->bringup);
 -}
 -
 -static int cpuhp_kick_ap(struct cpuhp_cpu_state *st, enum cpuhp_state target)
 -{
 -	enum cpuhp_state prev_state;
  	int ret;
 -
 -	prev_state = cpuhp_set_state(st, target);
 -	__cpuhp_kick_ap(st);
 -	if ((ret = st->result)) {
 -		cpuhp_reset_state(st, prev_state);
 -		__cpuhp_kick_ap(st);
 -	}
 -
 +	cpu_maps_update_begin();
 +	ret = raw_notifier_chain_register(&cpu_chain, nb);
 +	cpu_maps_update_done();
  	return ret;
  }
  
 -static int bringup_wait_for_ap(unsigned int cpu)
 +int __ref __register_cpu_notifier(struct notifier_block *nb)
  {
++<<<<<<< HEAD
 +	return raw_notifier_chain_register(&cpu_chain, nb);
++=======
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
+ 
+ 	/* Wait for the CPU to reach CPUHP_AP_ONLINE_IDLE */
+ 	wait_for_ap_thread(st, true);
+ 	if (WARN_ON_ONCE((!cpu_online(cpu))))
+ 		return -ECANCELED;
+ 
+ 	/* Unpark the stopper thread and the hotplug thread of the target cpu */
+ 	stop_machine_unpark(cpu);
+ 	kthread_unpark(st->thread);
+ 
+ 	/*
+ 	 * SMT soft disabling on X86 requires to bring the CPU out of the
+ 	 * BIOS 'wait for SIPI' state in order to set the CR4.MCE bit.  The
+ 	 * CPU marked itself as booted_once in cpu_notify_starting() so the
+ 	 * cpu_smt_allowed() check will now return false if this is not the
+ 	 * primary sibling.
+ 	 */
+ 	if (!cpu_smt_allowed(cpu))
+ 		return -ECANCELED;
+ 
+ 	if (st->target <= CPUHP_AP_ONLINE_IDLE)
+ 		return 0;
+ 
+ 	return cpuhp_kick_ap(st, st->target);
++>>>>>>> 0cc3cd21657b (cpu/hotplug: Boot HT siblings at least once)
  }
  
 -static int bringup_cpu(unsigned int cpu)
 +static int __cpu_notify(unsigned long val, void *v, int nr_to_call,
 +			int *nr_calls)
  {
 -	struct task_struct *idle = idle_thread_get(cpu);
  	int ret;
  
 -	/*
 -	 * Some architectures have to walk the irq descriptors to
 -	 * setup the vector space for the cpu which comes online.
 -	 * Prevent irq alloc/free across the bringup.
 -	 */
 -	irq_lock_sparse();
 -
 -	/* Arch-specific enabling code. */
 -	ret = __cpu_up(cpu, idle);
 -	irq_unlock_sparse();
 -	if (ret)
 -		return ret;
 -	return bringup_wait_for_ap(cpu);
 -}
 -
 -/*
 - * Hotplug state machine related functions
 - */
 -
 -static void undo_cpu_up(unsigned int cpu, struct cpuhp_cpu_state *st)
 -{
 -	for (st->state--; st->state > st->target; st->state--) {
 -		struct cpuhp_step *step = cpuhp_get_step(st->state);
 -
 -		if (!step->skip_onerr)
 -			cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);
 -	}
 -}
 -
 -static int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
 -			      enum cpuhp_state target)
 -{
 -	enum cpuhp_state prev_state = st->state;
 -	int ret = 0;
 -
 -	while (st->state < target) {
 -		st->state++;
 -		ret = cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
 -		if (ret) {
 -			st->target = prev_state;
 -			undo_cpu_up(cpu, st);
 -			break;
 -		}
 -	}
 -	return ret;
 -}
 -
 -/*
 - * The cpu hotplug threads manage the bringup and teardown of the cpus
 - */
 -static void cpuhp_create(unsigned int cpu)
 -{
 -	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 +	ret = __raw_notifier_call_chain(&cpu_chain, val, v, nr_to_call,
 +					nr_calls);
  
 -	init_completion(&st->done_up);
 -	init_completion(&st->done_down);
 +	return notifier_to_errno(ret);
  }
  
 -static int cpuhp_should_run(unsigned int cpu)
 +static int cpu_notify(unsigned long val, void *v)
  {
 -	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
 -
 -	return st->should_run;
 +	return __cpu_notify(val, v, -1, NULL);
  }
  
 -/*
 - * Execute teardown/startup callbacks on the plugged cpu. Also used to invoke
 - * callbacks when a state gets [un]installed at runtime.
 - *
 - * Each invocation of this function by the smpboot thread does a single AP
 - * state callback.
 - *
 - * It has 3 modes of operation:
 - *  - single: runs st->cb_state
 - *  - up:     runs ++st->state, while st->state < st->target
 - *  - down:   runs st->state--, while st->state > st->target
 - *
 - * When complete or on error, should_run is cleared and the completion is fired.
 - */
 -static void cpuhp_thread_fun(unsigned int cpu)
 -{
 -	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
 -	bool bringup = st->bringup;
 -	enum cpuhp_state state;
 -
 -	/*
 -	 * ACQUIRE for the cpuhp_should_run() load of ->should_run. Ensures
 -	 * that if we see ->should_run we also see the rest of the state.
 -	 */
 -	smp_mb();
 -
 -	if (WARN_ON_ONCE(!st->should_run))
 -		return;
 -
 -	cpuhp_lock_acquire(bringup);
 -
 -	if (st->single) {
 -		state = st->cb_state;
 -		st->should_run = false;
 -	} else {
 -		if (bringup) {
 -			st->state++;
 -			state = st->state;
 -			st->should_run = (st->state < st->target);
 -			WARN_ON_ONCE(st->state > st->target);
 -		} else {
 -			state = st->state;
 -			st->state--;
 -			st->should_run = (st->state > st->target);
 -			WARN_ON_ONCE(st->state < st->target);
 -		}
 -	}
 -
 -	WARN_ON_ONCE(!cpuhp_is_ap_state(state));
 -
 -	if (st->rollback) {
 -		struct cpuhp_step *step = cpuhp_get_step(state);
 -		if (step->skip_onerr)
 -			goto next;
 -	}
 -
 -	if (cpuhp_is_atomic_state(state)) {
 -		local_irq_disable();
 -		st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);
 -		local_irq_enable();
 -
 -		/*
 -		 * STARTING/DYING must not fail!
 -		 */
 -		WARN_ON_ONCE(st->result);
 -	} else {
 -		st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);
 -	}
 -
 -	if (st->result) {
 -		/*
 -		 * If we fail on a rollback, we're up a creek without no
 -		 * paddle, no way forward, no way back. We loose, thanks for
 -		 * playing.
 -		 */
 -		WARN_ON_ONCE(st->rollback);
 -		st->should_run = false;
 -	}
 -
 -next:
 -	cpuhp_lock_release(bringup);
 -
 -	if (!st->should_run)
 -		complete_ap_thread(st, bringup);
 -}
 +#ifdef CONFIG_HOTPLUG_CPU
  
 -/* Invoke a single callback on a remote cpu */
 -static int
 -cpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state, bool bringup,
 -			 struct hlist_node *node)
 +static void cpu_notify_nofail(unsigned long val, void *v)
  {
 -	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 -	int ret;
 -
 -	if (!cpu_online(cpu))
 -		return 0;
 -
 -	cpuhp_lock_acquire(false);
 -	cpuhp_lock_release(false);
 -
 -	cpuhp_lock_acquire(true);
 -	cpuhp_lock_release(true);
 -
 -	/*
 -	 * If we are up and running, use the hotplug thread. For early calls
 -	 * we invoke the thread function directly.
 -	 */
 -	if (!st->thread)
 -		return cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
 -
 -	st->rollback = false;
 -	st->last = NULL;
 -
 -	st->node = node;
 -	st->bringup = bringup;
 -	st->cb_state = state;
 -	st->single = true;
 -
 -	__cpuhp_kick_ap(st);
 -
 -	/*
 -	 * If we failed and did a partial, do a rollback.
 -	 */
 -	if ((ret = st->result) && st->last) {
 -		st->rollback = true;
 -		st->bringup = !bringup;
 -
 -		__cpuhp_kick_ap(st);
 -	}
 -
 -	/*
 -	 * Clean up the leftovers so the next hotplug operation wont use stale
 -	 * data.
 -	 */
 -	st->node = st->last = NULL;
 -	return ret;
 +	BUG_ON(cpu_notify(val, v));
  }
 +EXPORT_SYMBOL(register_cpu_notifier);
 +EXPORT_SYMBOL(__register_cpu_notifier);
  
 -static int cpuhp_kick_ap_work(unsigned int cpu)
 +void __ref unregister_cpu_notifier(struct notifier_block *nb)
  {
 -	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 -	enum cpuhp_state prev_state = st->state;
 -	int ret;
 -
 -	cpuhp_lock_acquire(false);
 -	cpuhp_lock_release(false);
 -
 -	cpuhp_lock_acquire(true);
 -	cpuhp_lock_release(true);
 -
 -	trace_cpuhp_enter(cpu, st->target, prev_state, cpuhp_kick_ap_work);
 -	ret = cpuhp_kick_ap(st, st->target);
 -	trace_cpuhp_exit(cpu, st->state, prev_state, ret);
 -
 -	return ret;
 +	cpu_maps_update_begin();
 +	raw_notifier_chain_unregister(&cpu_chain, nb);
 +	cpu_maps_update_done();
  }
 +EXPORT_SYMBOL(unregister_cpu_notifier);
  
 -static struct smp_hotplug_thread cpuhp_threads = {
 -	.store			= &cpuhp_state.thread,
 -	.create			= &cpuhp_create,
 -	.thread_should_run	= cpuhp_should_run,
 -	.thread_fn		= cpuhp_thread_fun,
 -	.thread_comm		= "cpuhp/%u",
 -	.selfparking		= true,
 -};
 -
 -void __init cpuhp_threads_init(void)
 +void __ref __unregister_cpu_notifier(struct notifier_block *nb)
  {
 -	BUG_ON(smpboot_register_percpu_thread(&cpuhp_threads));
 -	kthread_unpark(this_cpu_read(cpuhp_state.thread));
 +	raw_notifier_chain_unregister(&cpu_chain, nb);
  }
 +EXPORT_SYMBOL(__unregister_cpu_notifier);
  
 -#ifdef CONFIG_HOTPLUG_CPU
  /**
   * clear_tasks_mm_cpumask - Safely clear tasks' mm_cpumask for a CPU
   * @cpu: a CPU id
@@@ -432,49 -836,200 +703,94 @@@ static int __ref _cpu_down(unsigned in
  	/* This actually kills the CPU. */
  	__cpu_die(cpu);
  
 -	tick_cleanup_dead_cpu(cpu);
 -	rcutree_migrate_callbacks(cpu);
 -	return 0;
 -}
 +	/* CPU is completely dead: tell everyone.  Too late to complain. */
 +	cpu_notify_nofail(CPU_DEAD | mod, hcpu);
  
 -static void cpuhp_complete_idle_dead(void *arg)
 -{
 -	struct cpuhp_cpu_state *st = arg;
 +	check_for_tasks(cpu);
  
 -	complete_ap_thread(st, false);
 +out_release:
 +	cpu_hotplug_done();
 +	if (!err)
 +		cpu_notify_nofail(CPU_POST_DEAD | mod, hcpu);
 +	return err;
  }
  
 -void cpuhp_report_idle_dead(void)
 +int __ref cpu_down(unsigned int cpu)
  {
 -	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
 -
 -	BUG_ON(st->state != CPUHP_AP_OFFLINE);
 -	rcu_report_dead(smp_processor_id());
 -	st->state = CPUHP_AP_IDLE_DEAD;
 -	/*
 -	 * We cannot call complete after rcu_report_dead() so we delegate it
 -	 * to an online cpu.
 -	 */
 -	smp_call_function_single(cpumask_first(cpu_online_mask),
 -				 cpuhp_complete_idle_dead, st, 0);
 -}
 +	int err;
  
 -static void undo_cpu_down(unsigned int cpu, struct cpuhp_cpu_state *st)
 -{
 -	for (st->state++; st->state < st->target; st->state++) {
 -		struct cpuhp_step *step = cpuhp_get_step(st->state);
 +	cpu_maps_update_begin();
  
 -		if (!step->skip_onerr)
 -			cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
 +	if (cpu_hotplug_disabled) {
 +		err = -EBUSY;
 +		goto out;
  	}
 -}
  
 -static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
 -				enum cpuhp_state target)
 -{
 -	enum cpuhp_state prev_state = st->state;
 -	int ret = 0;
 +	err = _cpu_down(cpu, 0);
  
 -	for (; st->state > target; st->state--) {
 -		ret = cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);
 -		if (ret) {
 -			st->target = prev_state;
 -			undo_cpu_down(cpu, st);
 -			break;
 -		}
 -	}
 -	return ret;
 +out:
 +	cpu_maps_update_done();
 +	return err;
  }
 +EXPORT_SYMBOL(cpu_down);
 +#endif /*CONFIG_HOTPLUG_CPU*/
  
 -/* Requires cpu_add_remove_lock to be held */
 -static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,
 -			   enum cpuhp_state target)
++<<<<<<< HEAD
++=======
++/**
++ * notify_cpu_starting(cpu) - Invoke the callbacks on the starting CPU
++ * @cpu: cpu that just started
++ *
++ * It must be called by the arch code on the new cpu, before the new cpu
++ * enables interrupts and before the "boot" cpu returns from __cpu_up().
++ */
++void notify_cpu_starting(unsigned int cpu)
+ {
+ 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 -	int prev_state, ret = 0;
 -
 -	if (num_online_cpus() == 1)
 -		return -EBUSY;
 -
 -	if (!cpu_present(cpu))
 -		return -EINVAL;
 -
 -	cpus_write_lock();
 -
 -	cpuhp_tasks_frozen = tasks_frozen;
 -
 -	prev_state = cpuhp_set_state(st, target);
 -	/*
 -	 * If the current CPU state is in the range of the AP hotplug thread,
 -	 * then we need to kick the thread.
 -	 */
 -	if (st->state > CPUHP_TEARDOWN_CPU) {
 -		st->target = max((int)target, CPUHP_TEARDOWN_CPU);
 -		ret = cpuhp_kick_ap_work(cpu);
 -		/*
 -		 * The AP side has done the error rollback already. Just
 -		 * return the error code..
 -		 */
 -		if (ret)
 -			goto out;
 -
 -		/*
 -		 * We might have stopped still in the range of the AP hotplug
 -		 * thread. Nothing to do anymore.
 -		 */
 -		if (st->state > CPUHP_TEARDOWN_CPU)
 -			goto out;
 -
 -		st->target = target;
 -	}
 -	/*
 -	 * The AP brought itself down to CPUHP_TEARDOWN_CPU. So we need
 -	 * to do the further cleanups.
 -	 */
 -	ret = cpuhp_down_callbacks(cpu, st, target);
 -	if (ret && st->state > CPUHP_TEARDOWN_CPU && st->state < prev_state) {
 -		cpuhp_reset_state(st, prev_state);
 -		__cpuhp_kick_ap(st);
 -	}
 -
 -out:
 -	cpus_write_unlock();
 -	/*
 -	 * Do post unplug cleanup. This is still protected against
 -	 * concurrent CPU hotplug via cpu_add_remove_lock.
 -	 */
 -	lockup_detector_cleanup();
 -	return ret;
 -}
 -
 -static int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)
 -{
 -	if (cpu_hotplug_disabled)
 -		return -EBUSY;
 -	return _cpu_down(cpu, 0, target);
 -}
 -
 -static int do_cpu_down(unsigned int cpu, enum cpuhp_state target)
 -{
 -	int err;
 -
 -	cpu_maps_update_begin();
 -	err = cpu_down_maps_locked(cpu, target);
 -	cpu_maps_update_done();
 -	return err;
 -}
 -
 -int cpu_down(unsigned int cpu)
 -{
 -	return do_cpu_down(cpu, CPUHP_OFFLINE);
 -}
 -EXPORT_SYMBOL(cpu_down);
 -
 -#else
 -#define takedown_cpu		NULL
 -#endif /*CONFIG_HOTPLUG_CPU*/
 -
 -/**
 - * notify_cpu_starting(cpu) - Invoke the callbacks on the starting CPU
 - * @cpu: cpu that just started
 - *
 - * It must be called by the arch code on the new cpu, before the new cpu
 - * enables interrupts and before the "boot" cpu returns from __cpu_up().
 - */
 -void notify_cpu_starting(unsigned int cpu)
 -{
 -	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 -	enum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);
 -	int ret;
++	enum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);
++	int ret;
+ 
+ 	rcu_cpu_starting(cpu);	/* Enables RCU usage on this CPU. */
+ 	st->booted_once = true;
+ 	while (st->state < target) {
+ 		st->state++;
+ 		ret = cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);
+ 		/*
+ 		 * STARTING must not fail!
+ 		 */
+ 		WARN_ON_ONCE(ret);
+ 	}
+ }
+ 
+ /*
+  * Called from the idle task. Wake up the controlling task which brings the
+  * stopper and the hotplug thread of the upcoming CPU up and then delegates
+  * the rest of the online bringup to the hotplug thread.
+  */
+ void cpuhp_online_idle(enum cpuhp_state state)
+ {
+ 	struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
+ 
+ 	/* Happens for the boot cpu */
+ 	if (state != CPUHP_AP_ONLINE_IDLE)
+ 		return;
+ 
+ 	st->state = CPUHP_AP_ONLINE_IDLE;
+ 	complete_ap_thread(st, true);
+ }
+ 
++>>>>>>> 0cc3cd21657b (cpu/hotplug: Boot HT siblings at least once)
  /* Requires cpu_add_remove_lock to be held */
 -static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)
 +static int _cpu_up(unsigned int cpu, int tasks_frozen)
  {
 -	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 +	int ret, nr_calls = 0;
 +	void *hcpu = (void *)(long)cpu;
 +	unsigned long mod = tasks_frozen ? CPU_TASKS_FROZEN : 0;
  	struct task_struct *idle;
 -	int ret = 0;
  
 -	cpus_write_lock();
 +	cpu_hotplug_begin();
  
 -	if (!cpu_present(cpu)) {
 +	if (cpu_online(cpu) || !cpu_present(cpu)) {
  		ret = -EINVAL;
  		goto out;
  	}
@@@ -805,5 -2177,32 +1121,36 @@@ void init_cpu_possible(const struct cpu
  
  void init_cpu_online(const struct cpumask *src)
  {
++<<<<<<< HEAD
 +	cpumask_copy(to_cpumask(cpu_online_bits), src);
++=======
+ 	cpumask_copy(&__cpu_online_mask, src);
+ }
+ 
+ /*
+  * Activate the first processor.
+  */
+ void __init boot_cpu_init(void)
+ {
+ 	int cpu = smp_processor_id();
+ 
+ 	/* Mark the boot cpu "present", "online" etc for SMP and UP case */
+ 	set_cpu_online(cpu, true);
+ 	set_cpu_active(cpu, true);
+ 	set_cpu_present(cpu, true);
+ 	set_cpu_possible(cpu, true);
+ 
+ #ifdef CONFIG_SMP
+ 	__boot_cpu_id = cpu;
+ #endif
+ }
+ 
+ /*
+  * Must be called _AFTER_ setting up the per_cpu areas
+  */
+ void __init boot_cpu_state_init(void)
+ {
+ 	this_cpu_write(cpuhp_state.booted_once, true);
+ 	this_cpu_write(cpuhp_state.state, CPUHP_ONLINE);
++>>>>>>> 0cc3cd21657b (cpu/hotplug: Boot HT siblings at least once)
  }
* Unmerged path kernel/cpu.c
