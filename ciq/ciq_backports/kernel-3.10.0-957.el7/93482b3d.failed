mmc: block: Prepare CQE data

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mmc] block: Prepare CQE data (Gopal Tiwari) [1456570]
Rebuild_FUZZ: 90.20%
commit-author Adrian Hunter <adrian.hunter@intel.com>
commit 93482b3d70c2120aadb0f1d1281a59199866e70a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/93482b3d.failed

Enhance mmc_blk_data_prep() to support CQE requests. That means adding
some things that for non-CQE requests would be encoded into the command
arguments - such as the block address, reliable-write flag, and data tag
flag. Also the request tag is needed to provide the command queue task id,
and a comment is added to explain the future possibility of defining a
priority.

	Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
	Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
	Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
(cherry picked from commit 93482b3d70c2120aadb0f1d1281a59199866e70a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/mmc/core/block.c
diff --cc drivers/mmc/core/block.c
index bf17147e2808,ea80ff4cd7f9..000000000000
--- a/drivers/mmc/core/block.c
+++ b/drivers/mmc/core/block.c
@@@ -1477,22 -1588,36 +1477,31 @@@ static void mmc_blk_rw_rq_prep(struct m
  	 * Reliable writes are used to implement Forced Unit Access and
  	 * are supported only on MMCs.
  	 */
 -	do_rel_wr = (req->cmd_flags & REQ_FUA) &&
 -		    rq_data_dir(req) == WRITE &&
 -		    (md->flags & MMC_BLK_REL_WR);
 +	bool do_rel_wr = (req->cmd_flags & REQ_FUA) &&
 +		(rq_data_dir(req) == WRITE) &&
 +		(md->flags & MMC_BLK_REL_WR);
  
  	memset(brq, 0, sizeof(struct mmc_blk_request));
 -
 +	brq->mrq.cmd = &brq->cmd;
  	brq->mrq.data = &brq->data;
+ 	brq->mrq.tag = req->tag;
  
 +	brq->cmd.arg = blk_rq_pos(req);
 +	if (!mmc_card_blockaddr(card))
 +		brq->cmd.arg <<= 9;
 +	brq->cmd.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_ADTC;
 +	brq->data.blksz = 512;
  	brq->stop.opcode = MMC_STOP_TRANSMISSION;
  	brq->stop.arg = 0;
 -
 -	if (rq_data_dir(req) == READ) {
 -		brq->data.flags = MMC_DATA_READ;
 -		brq->stop.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_AC;
 -	} else {
 -		brq->data.flags = MMC_DATA_WRITE;
 -		brq->stop.flags = MMC_RSP_SPI_R1B | MMC_RSP_R1B | MMC_CMD_AC;
 -	}
 -
 -	brq->data.blksz = 512;
  	brq->data.blocks = blk_rq_sectors(req);
+ 	brq->data.blk_addr = blk_rq_pos(req);
+ 
+ 	/*
+ 	 * The command queue supports 2 priorities: "high" (1) and "simple" (0).
+ 	 * The eMMC will give "high" priority tasks priority over "simple"
+ 	 * priority tasks. Here we always set "simple" priority by not setting
+ 	 * MMC_DATA_PRIO.
+ 	 */
  
  	/*
  	 * The block layer doesn't support all sector count
@@@ -1522,6 -1647,77 +1531,80 @@@
  						brq->data.blocks);
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (do_rel_wr) {
+ 		mmc_apply_rel_rw(brq, card, req);
+ 		brq->data.flags |= MMC_DATA_REL_WR;
+ 	}
+ 
+ 	/*
+ 	 * Data tag is used only during writing meta data to speed
+ 	 * up write and any subsequent read of this meta data
+ 	 */
+ 	do_data_tag = card->ext_csd.data_tag_unit_size &&
+ 		      (req->cmd_flags & REQ_META) &&
+ 		      (rq_data_dir(req) == WRITE) &&
+ 		      ((brq->data.blocks * brq->data.blksz) >=
+ 		       card->ext_csd.data_tag_unit_size);
+ 
+ 	if (do_data_tag)
+ 		brq->data.flags |= MMC_DATA_DAT_TAG;
+ 
+ 	mmc_set_data_timeout(&brq->data, card);
+ 
+ 	brq->data.sg = mqrq->sg;
+ 	brq->data.sg_len = mmc_queue_map_sg(mq, mqrq);
+ 
+ 	/*
+ 	 * Adjust the sg list so it is the same size as the
+ 	 * request.
+ 	 */
+ 	if (brq->data.blocks != blk_rq_sectors(req)) {
+ 		int i, data_size = brq->data.blocks << 9;
+ 		struct scatterlist *sg;
+ 
+ 		for_each_sg(brq->data.sg, sg, brq->data.sg_len, i) {
+ 			data_size -= sg->length;
+ 			if (data_size <= 0) {
+ 				sg->length += data_size;
+ 				i++;
+ 				break;
+ 			}
+ 		}
+ 		brq->data.sg_len = i;
+ 	}
+ 
+ 	mqrq->areq.mrq = &brq->mrq;
+ 
+ 	if (do_rel_wr_p)
+ 		*do_rel_wr_p = do_rel_wr;
+ 
+ 	if (do_data_tag_p)
+ 		*do_data_tag_p = do_data_tag;
+ }
+ 
+ static void mmc_blk_rw_rq_prep(struct mmc_queue_req *mqrq,
+ 			       struct mmc_card *card,
+ 			       int disable_multi,
+ 			       struct mmc_queue *mq)
+ {
+ 	u32 readcmd, writecmd;
+ 	struct mmc_blk_request *brq = &mqrq->brq;
+ 	struct request *req = mmc_queue_req_to_req(mqrq);
+ 	struct mmc_blk_data *md = mq->blkdata;
+ 	bool do_rel_wr, do_data_tag;
+ 
+ 	mmc_blk_data_prep(mq, mqrq, disable_multi, &do_rel_wr, &do_data_tag);
+ 
+ 	brq->mrq.cmd = &brq->cmd;
+ 
+ 	brq->cmd.arg = blk_rq_pos(req);
+ 	if (!mmc_card_blockaddr(card))
+ 		brq->cmd.arg <<= 9;
+ 	brq->cmd.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_ADTC;
+ 
++>>>>>>> 93482b3d70c2 (mmc: block: Prepare CQE data)
  	if (brq->data.blocks > 1 || do_rel_wr) {
  		/* SPI multiblock writes terminate using a special
  		 * token, not a STOP_TRANSMISSION request.
* Unmerged path drivers/mmc/core/block.c
