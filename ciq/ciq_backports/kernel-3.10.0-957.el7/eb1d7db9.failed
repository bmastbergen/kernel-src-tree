nfp: bpf: set new jit info fields

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jiong Wang <jiong.wang@netronome.com>
commit eb1d7db927a9653f1402473c777839e0456a7836
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/eb1d7db9.failed

This patch set those new jit info fields introduced in this patch set.

	Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit eb1d7db927a9653f1402473c777839e0456a7836)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/offload.c
index de79faf0874b,c452bf9462e0..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/offload.c
@@@ -147,22 -91,201 +147,213 @@@ nfp_net_bpf_get_act(struct nfp_net *nn
  }
  
  static int
 -nfp_bpf_verifier_prep(struct nfp_app *app, struct nfp_net *nn,
 -		      struct netdev_bpf *bpf)
 +nfp_net_bpf_offload_prepare(struct nfp_net *nn,
 +			    struct tc_cls_bpf_offload *cls_bpf,
 +			    struct nfp_bpf_result *res,
 +			    void **code, dma_addr_t *dma_addr, u16 max_instr)
  {
 -	struct bpf_prog *prog = bpf->verifier.prog;
 -	struct nfp_prog *nfp_prog;
 +	unsigned int code_sz = max_instr * sizeof(u64);
 +	enum nfp_bpf_action_type act;
 +	unsigned int stack_size;
 +	u16 start_off, done_off;
 +	unsigned int max_mtu;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = nfp_net_bpf_get_act(nn, cls_bpf);
 +	if (ret < 0)
 +		return ret;
 +	act = ret;
++=======
+ 	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
+ 	if (!nfp_prog)
+ 		return -ENOMEM;
+ 	prog->aux->offload->dev_priv = nfp_prog;
+ 
+ 	INIT_LIST_HEAD(&nfp_prog->insns);
+ 	nfp_prog->type = prog->type;
+ 	nfp_prog->bpf = app->priv;
+ 
+ 	ret = nfp_prog_prepare(nfp_prog, prog->insnsi, prog->len);
+ 	if (ret)
+ 		goto err_free;
+ 
+ 	nfp_prog->verifier_meta = nfp_prog_first_meta(nfp_prog);
+ 	bpf->verifier.ops = &nfp_bpf_analyzer_ops;
+ 
+ 	return 0;
+ 
+ err_free:
+ 	nfp_prog_free(nfp_prog);
+ 
+ 	return ret;
+ }
+ 
+ static int nfp_bpf_translate(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 	unsigned int stack_size;
+ 	unsigned int max_instr;
+ 	int err;
+ 
+ 	stack_size = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;
+ 	if (prog->aux->stack_depth > stack_size) {
+ 		nn_info(nn, "stack too large: program %dB > FW stack %dB\n",
+ 			prog->aux->stack_depth, stack_size);
+ 		return -EOPNOTSUPP;
+ 	}
+ 	nfp_prog->stack_depth = round_up(prog->aux->stack_depth, 4);
+ 
+ 	max_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);
+ 	nfp_prog->__prog_alloc_len = max_instr * sizeof(u64);
+ 
+ 	nfp_prog->prog = kvmalloc(nfp_prog->__prog_alloc_len, GFP_KERNEL);
+ 	if (!nfp_prog->prog)
+ 		return -ENOMEM;
+ 
+ 	err = nfp_bpf_jit(nfp_prog);
+ 	if (err)
+ 		return err;
+ 
+ 	prog->aux->offload->jited_len = nfp_prog->prog_len * sizeof(u64);
+ 	prog->aux->offload->jited_image = nfp_prog->prog;
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_bpf_destroy(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 
+ 	kvfree(nfp_prog->prog);
+ 	nfp_prog_free(nfp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_map_get_next_key(struct bpf_offloaded_map *offmap,
+ 			 void *key, void *next_key)
+ {
+ 	if (!key)
+ 		return nfp_bpf_ctrl_getfirst_entry(offmap, next_key);
+ 	return nfp_bpf_ctrl_getnext_entry(offmap, key, next_key);
+ }
+ 
+ static int
+ nfp_bpf_map_delete_elem(struct bpf_offloaded_map *offmap, void *key)
+ {
+ 	return nfp_bpf_ctrl_del_entry(offmap, key);
+ }
+ 
+ static const struct bpf_map_dev_ops nfp_bpf_map_ops = {
+ 	.map_get_next_key	= nfp_bpf_map_get_next_key,
+ 	.map_lookup_elem	= nfp_bpf_ctrl_lookup_entry,
+ 	.map_update_elem	= nfp_bpf_ctrl_update_entry,
+ 	.map_delete_elem	= nfp_bpf_map_delete_elem,
+ };
+ 
+ static int
+ nfp_bpf_map_alloc(struct nfp_app_bpf *bpf, struct bpf_offloaded_map *offmap)
+ {
+ 	struct nfp_bpf_map *nfp_map;
+ 	long long int res;
+ 
+ 	if (!bpf->maps.types)
+ 		return -EOPNOTSUPP;
+ 
+ 	if (offmap->map.map_flags ||
+ 	    offmap->map.numa_node != NUMA_NO_NODE) {
+ 		pr_info("map flags are not supported\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!(bpf->maps.types & 1 << offmap->map.map_type)) {
+ 		pr_info("map type not supported\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 	if (bpf->maps.max_maps == bpf->maps_in_use) {
+ 		pr_info("too many maps for a device\n");
+ 		return -ENOMEM;
+ 	}
+ 	if (bpf->maps.max_elems - bpf->map_elems_in_use <
+ 	    offmap->map.max_entries) {
+ 		pr_info("map with too many elements: %u, left: %u\n",
+ 			offmap->map.max_entries,
+ 			bpf->maps.max_elems - bpf->map_elems_in_use);
+ 		return -ENOMEM;
+ 	}
+ 	if (offmap->map.key_size > bpf->maps.max_key_sz ||
+ 	    offmap->map.value_size > bpf->maps.max_val_sz ||
+ 	    round_up(offmap->map.key_size, 8) +
+ 	    round_up(offmap->map.value_size, 8) > bpf->maps.max_elem_sz) {
+ 		pr_info("elements don't fit in device constraints\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	nfp_map = kzalloc(sizeof(*nfp_map), GFP_USER);
+ 	if (!nfp_map)
+ 		return -ENOMEM;
+ 
+ 	offmap->dev_priv = nfp_map;
+ 	nfp_map->offmap = offmap;
+ 	nfp_map->bpf = bpf;
+ 
+ 	res = nfp_bpf_ctrl_alloc_map(bpf, &offmap->map);
+ 	if (res < 0) {
+ 		kfree(nfp_map);
+ 		return res;
+ 	}
+ 
+ 	nfp_map->tid = res;
+ 	offmap->dev_ops = &nfp_bpf_map_ops;
+ 	bpf->maps_in_use++;
+ 	bpf->map_elems_in_use += offmap->map.max_entries;
+ 	list_add_tail(&nfp_map->l, &bpf->map_list);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_map_free(struct nfp_app_bpf *bpf, struct bpf_offloaded_map *offmap)
+ {
+ 	struct nfp_bpf_map *nfp_map = offmap->dev_priv;
+ 
+ 	nfp_bpf_ctrl_free_map(bpf, nfp_map);
+ 	list_del_init(&nfp_map->l);
+ 	bpf->map_elems_in_use -= offmap->map.max_entries;
+ 	bpf->maps_in_use--;
+ 	kfree(nfp_map);
+ 
+ 	return 0;
+ }
+ 
+ int nfp_ndo_bpf(struct nfp_app *app, struct nfp_net *nn, struct netdev_bpf *bpf)
+ {
+ 	switch (bpf->command) {
+ 	case BPF_OFFLOAD_VERIFIER_PREP:
+ 		return nfp_bpf_verifier_prep(app, nn, bpf);
+ 	case BPF_OFFLOAD_TRANSLATE:
+ 		return nfp_bpf_translate(nn, bpf->offload.prog);
+ 	case BPF_OFFLOAD_DESTROY:
+ 		return nfp_bpf_destroy(nn, bpf->offload.prog);
+ 	case BPF_OFFLOAD_MAP_ALLOC:
+ 		return nfp_bpf_map_alloc(app->priv, bpf->offmap);
+ 	case BPF_OFFLOAD_MAP_FREE:
+ 		return nfp_bpf_map_free(app->priv, bpf->offmap);
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
+ static int nfp_net_bpf_load(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 	unsigned int max_mtu;
+ 	dma_addr_t dma_addr;
+ 	void *img;
+ 	int err;
++>>>>>>> eb1d7db927a9 (nfp: bpf: set new jit info fields)
  
  	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
  	if (max_mtu < nn->dp.netdev->mtu) {
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/offload.c
