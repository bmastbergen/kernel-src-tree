mm: move get_dev_pagemap out of line

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 0822acb86cf340cd45b3af6436cec7e3bb24ebd2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/0822acb8.failed

This is a pretty big function, which should be out of line in general,
and a no-op stub if CONFIG_ZONE_DEVICÐ• is not set.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 0822acb86cf340cd45b3af6436cec7e3bb24ebd2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/memremap.c
diff --cc kernel/memremap.c
index 00f3d3b53574,3df6cd4ffb40..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -516,5 -501,61 +516,66 @@@ struct vmem_altmap *to_vmem_altmap(unsi
  
  	return pgmap ? pgmap->altmap : NULL;
  }
++<<<<<<< HEAD
 +#endif /* CONFIG_SPARSEMEM_VMEMMAP */
 +#endif /* CONFIG_ZONE_DEVICE */
++=======
+ 
+ /**
+  * get_dev_pagemap() - take a new live reference on the dev_pagemap for @pfn
+  * @pfn: page frame number to lookup page_map
+  * @pgmap: optional known pgmap that already has a reference
+  *
+  * @pgmap allows the overhead of a lookup to be bypassed when @pfn lands in the
+  * same mapping.
+  */
+ struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
+ 		struct dev_pagemap *pgmap)
+ {
+ 	const struct resource *res = pgmap ? pgmap->res : NULL;
+ 	resource_size_t phys = PFN_PHYS(pfn);
+ 
+ 	/*
+ 	 * In the cached case we're already holding a live reference so
+ 	 * we can simply do a blind increment
+ 	 */
+ 	if (res && phys >= res->start && phys <= res->end) {
+ 		percpu_ref_get(pgmap->ref);
+ 		return pgmap;
+ 	}
+ 
+ 	/* fall back to slow path lookup */
+ 	rcu_read_lock();
+ 	pgmap = find_dev_pagemap(phys);
+ 	if (pgmap && !percpu_ref_tryget_live(pgmap->ref))
+ 		pgmap = NULL;
+ 	rcu_read_unlock();
+ 
+ 	return pgmap;
+ }
+ #endif /* CONFIG_ZONE_DEVICE */
+ 
+ #if IS_ENABLED(CONFIG_DEVICE_PRIVATE) ||  IS_ENABLED(CONFIG_DEVICE_PUBLIC)
+ void put_zone_device_private_or_public_page(struct page *page)
+ {
+ 	int count = page_ref_dec_return(page);
+ 
+ 	/*
+ 	 * If refcount is 1 then page is freed and refcount is stable as nobody
+ 	 * holds a reference on the page.
+ 	 */
+ 	if (count == 1) {
+ 		/* Clear Active bit in case of parallel mark_page_accessed */
+ 		__ClearPageActive(page);
+ 		__ClearPageWaiters(page);
+ 
+ 		page->mapping = NULL;
+ 		mem_cgroup_uncharge(page);
+ 
+ 		page->pgmap->page_free(page, page->pgmap->data);
+ 	} else if (!count)
+ 		__put_page(page);
+ }
+ EXPORT_SYMBOL(put_zone_device_private_or_public_page);
+ #endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */
++>>>>>>> 0822acb86cf3 (mm: move get_dev_pagemap out of line)
diff --git a/include/linux/memremap.h b/include/linux/memremap.h
index c4c41ebb44e1..c5b7224e70a2 100644
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@ -121,7 +121,8 @@ struct dev_pagemap {
 #ifdef CONFIG_ZONE_DEVICE
 void *devm_memremap_pages(struct device *dev, struct resource *res,
 		struct percpu_ref *ref, struct vmem_altmap *altmap);
-struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
+struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
+		struct dev_pagemap *pgmap);
 
 static inline bool is_hmm_page(const struct page *page)
 {
@@ -143,7 +144,8 @@ static inline void *devm_memremap_pages(struct device *dev,
 	return ERR_PTR(-ENXIO);
 }
 
-static inline struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
+static inline struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
+		struct dev_pagemap *pgmap)
 {
 	return NULL;
 }
@@ -154,39 +156,6 @@ static inline bool is_hmm_page(const struct page *page)
 }
 #endif
 
-/**
- * get_dev_pagemap() - take a new live reference on the dev_pagemap for @pfn
- * @pfn: page frame number to lookup page_map
- * @pgmap: optional known pgmap that already has a reference
- *
- * @pgmap allows the overhead of a lookup to be bypassed when @pfn lands in the
- * same mapping.
- */
-static inline struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
-		struct dev_pagemap *pgmap)
-{
-	const struct resource *res = pgmap ? pgmap->res : NULL;
-	resource_size_t phys = PFN_PHYS(pfn);
-
-	/*
-	 * In the cached case we're already holding a live reference so
-	 * we can simply do a blind increment
-	 */
-	if (res && phys >= res->start && phys <= res->end) {
-		percpu_ref_get(pgmap->ref);
-		return pgmap;
-	}
-
-	/* fall back to slow path lookup */
-	rcu_read_lock();
-	pgmap = find_dev_pagemap(phys);
-	if (pgmap && !percpu_ref_tryget_live(pgmap->ref))
-		pgmap = NULL;
-	rcu_read_unlock();
-
-	return pgmap;
-}
-
 static inline void put_dev_pagemap(struct dev_pagemap *pgmap)
 {
 	if (pgmap) {
* Unmerged path kernel/memremap.c
