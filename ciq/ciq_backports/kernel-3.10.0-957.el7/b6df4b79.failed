tcmu: Add global data block pool support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Xiubo Li <lixiubo@cmss.chinamobile.com>
commit b6df4b79a5514a9c6c53533436704129ef45bf76
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/b6df4b79.failed

For each target there will be one ring, when the target number
grows larger and larger, it could eventually runs out of the
system memories.

In this patch for each target ring, currently for the cmd area
the size will be fixed to 8MB and for the data area the size
will grow from 0 to max 256K * PAGE_SIZE(1G for 4K page size).

For all the targets' data areas, they will get empty blocks
from the "global data block pool", which has limited to 512K *
PAGE_SIZE(2G for 4K page size) for now.

When the "global data block pool" has been used up, then any
target could wake up the unmap thread routine to shrink other
targets' data area memories. And the unmap thread routine will
always try to truncate the ring vma from the last using block
offset.

When user space has touched the data blocks out of tcmu_cmd
iov[], the tcmu_page_fault() will try to return one zeroed blocks.

Here we move the timeout's tcmu_handle_completions() into unmap
thread routine, that's to say when the timeout fired, it will
only do the tcmu_check_expired_cmd() and then wake up the unmap
thread to do the completions() and then try to shrink its idle
memories. Then the cmdr_lock could be a mutex and could simplify
this patch because the unmap_mapping_range() or zap_* may go to
sleep.

	Signed-off-by: Xiubo Li <lixiubo@cmss.chinamobile.com>
	Signed-off-by: Jianfei Hu <hujianfei@cmss.chinamobile.com>
	Acked-by: Mike Christie <mchristi@redhat.com>
	Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>
(cherry picked from commit b6df4b79a5514a9c6c53533436704129ef45bf76)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/target/target_core_user.c
diff --cc drivers/target/target_core_user.c
index 2ac4515b6a67,0b29e4f00bce..000000000000
--- a/drivers/target/target_core_user.c
+++ b/drivers/target/target_core_user.c
@@@ -25,11 -26,16 +25,13 @@@
  #include <linux/parser.h>
  #include <linux/vmalloc.h>
  #include <linux/uio_driver.h>
 -#include <linux/radix-tree.h>
  #include <linux/stringify.h>
  #include <linux/bitops.h>
 -#include <linux/highmem.h>
  #include <linux/configfs.h>
+ #include <linux/mutex.h>
+ #include <linux/kthread.h>
  #include <net/genetlink.h>
 -#include <scsi/scsi_common.h>
 -#include <scsi/scsi_proto.h>
 +#include <asm/unaligned.h>
  #include <target/target_core_base.h>
  #include <target/target_core_fabric.h>
  #include <target/target_core_backend.h>
@@@ -61,19 -67,25 +63,39 @@@
   * this may have a 'UAM' comment.
   */
  
 +
  #define TCMU_TIME_OUT (30 * MSEC_PER_SEC)
  
++<<<<<<< HEAD
 +#define DATA_BLOCK_BITS_DEF 2048
 +#define DATA_BLOCKS_BITS_MAX 65536
 +#define DATA_BLOCK_SIZE PAGE_SIZE
 +#define DATA_BLOCK_SHIFT PAGE_SHIFT
 +#define TCMU_MBS_TO_BLOCKS(_mbs) (_mbs << (20 - DATA_BLOCK_SHIFT))
 +#define TCMU_BLOCKS_TO_MBS(_blocks) (_blocks >> (20 - DATA_BLOCK_SHIFT))
 +
 +#define CMDR_SIZE (16 * 4096)
 +
 +static u8 tcmu_kern_cmd_reply_supported;
++=======
+ /* For cmd area, the size is fixed 8MB */
+ #define CMDR_SIZE (8 * 1024 * 1024)
+ 
+ /*
+  * For data area, the block size is PAGE_SIZE and
+  * the total size is 256K * PAGE_SIZE.
+  */
+ #define DATA_BLOCK_SIZE PAGE_SIZE
+ #define DATA_BLOCK_BITS (256 * 1024)
+ #define DATA_SIZE (DATA_BLOCK_BITS * DATA_BLOCK_SIZE)
+ #define DATA_BLOCK_INIT_BITS 128
+ 
+ /* The total size of the ring is 8M + 256K * PAGE_SIZE */
+ #define TCMU_RING_SIZE (CMDR_SIZE + DATA_SIZE)
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
+ 
+ /* Default maximum of the global data blocks(512K * PAGE_SIZE) */
+ #define TCMU_GLOBAL_MAX_BLOCKS (512 * 1024)
  
  static struct device *tcmu_root_device;
  
@@@ -83,16 -95,10 +105,18 @@@ struct tcmu_hba 
  
  #define TCMU_CONFIG_LEN 256
  
 +struct tcmu_nl_cmd {
 +	/* wake up thread waiting for reply */
 +	struct completion complete;
 +	int cmd;
 +	int status;
 +};
 +
  struct tcmu_dev {
+ 	struct list_head node;
+ 
  	struct se_device se_dev;
 +	int dev_index;
  
  	char *name;
  	struct se_hba *hba;
@@@ -111,15 -119,16 +137,23 @@@
  	/* Must add data_off and mb_addr to get the address */
  	size_t data_off;
  	size_t data_size;
 +	uint32_t max_blocks;
 +	size_t ring_size;
 +
 +	unsigned long *data_bitmap;
  
  	wait_queue_head_t wait_cmdr;
- 	/* TODO should this be a mutex? */
- 	spinlock_t cmdr_lock;
+ 	struct mutex cmdr_lock;
+ 
++<<<<<<< HEAD
++=======
+ 	bool waiting_global;
+ 	uint32_t dbi_max;
+ 	uint32_t dbi_thresh;
+ 	DECLARE_BITMAP(data_bitmap, DATA_BLOCK_BITS);
+ 	struct radix_tree_root data_blocks;
  
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  	struct idr commands;
  	spinlock_t commands_lock;
  
@@@ -155,11 -160,15 +189,18 @@@ struct tcmu_cmd 
  	unsigned long flags;
  };
  
+ static struct task_struct *unmap_thread;
+ static wait_queue_head_t unmap_wait;
+ static DEFINE_MUTEX(root_udev_mutex);
+ static LIST_HEAD(root_udev);
+ 
+ static atomic_t global_db_count = ATOMIC_INIT(0);
+ 
  static struct kmem_cache *tcmu_cmd_cache;
  
 +static DEFINE_IDR(devices_idr);
 +static DEFINE_MUTEX(device_mutex);
 +
  /* multicast group */
  enum tcmu_multicast_groups {
  	TCMU_MCGRP_CONFIG,
@@@ -288,10 -188,95 +329,100 @@@ static struct genl_family tcmu_genl_fam
  	.mcgrps = tcmu_mcgrps,
  	.n_mcgrps = ARRAY_SIZE(tcmu_mcgrps),
  	.netnsok = true,
 +	.ops = tcmu_genl_ops,
 +	.n_ops = ARRAY_SIZE(tcmu_genl_ops),
  };
  
++<<<<<<< HEAD
++=======
+ #define tcmu_cmd_set_dbi_cur(cmd, index) ((cmd)->dbi_cur = (index))
+ #define tcmu_cmd_reset_dbi_cur(cmd) tcmu_cmd_set_dbi_cur(cmd, 0)
+ #define tcmu_cmd_set_dbi(cmd, index) ((cmd)->dbi[(cmd)->dbi_cur++] = (index))
+ #define tcmu_cmd_get_dbi(cmd) ((cmd)->dbi[(cmd)->dbi_cur++])
+ 
+ static void tcmu_cmd_free_data(struct tcmu_cmd *tcmu_cmd, uint32_t len)
+ {
+ 	struct tcmu_dev *udev = tcmu_cmd->tcmu_dev;
+ 	uint32_t i;
+ 
+ 	for (i = 0; i < len; i++)
+ 		clear_bit(tcmu_cmd->dbi[i], udev->data_bitmap);
+ }
+ 
+ static inline bool tcmu_get_empty_block(struct tcmu_dev *udev,
+ 					struct tcmu_cmd *tcmu_cmd)
+ {
+ 	struct page *page;
+ 	int ret, dbi;
+ 
+ 	dbi = find_first_zero_bit(udev->data_bitmap, udev->dbi_thresh);
+ 	if (dbi == udev->dbi_thresh)
+ 		return false;
+ 
+ 	page = radix_tree_lookup(&udev->data_blocks, dbi);
+ 	if (!page) {
+ 
+ 		if (atomic_add_return(1, &global_db_count) >
+ 					TCMU_GLOBAL_MAX_BLOCKS) {
+ 			atomic_dec(&global_db_count);
+ 			return false;
+ 		}
+ 
+ 		/* try to get new page from the mm */
+ 		page = alloc_page(GFP_KERNEL);
+ 		if (!page)
+ 			return false;
+ 
+ 		ret = radix_tree_insert(&udev->data_blocks, dbi, page);
+ 		if (ret) {
+ 			__free_page(page);
+ 			return false;
+ 		}
+ 
+ 	}
+ 
+ 	if (dbi > udev->dbi_max)
+ 		udev->dbi_max = dbi;
+ 
+ 	set_bit(dbi, udev->data_bitmap);
+ 	tcmu_cmd_set_dbi(tcmu_cmd, dbi);
+ 
+ 	return true;
+ }
+ 
+ static bool tcmu_get_empty_blocks(struct tcmu_dev *udev,
+ 				  struct tcmu_cmd *tcmu_cmd)
+ {
+ 	int i;
+ 
+ 	udev->waiting_global = false;
+ 
+ 	for (i = tcmu_cmd->dbi_cur; i < tcmu_cmd->dbi_cnt; i++) {
+ 		if (!tcmu_get_empty_block(udev, tcmu_cmd))
+ 			goto err;
+ 	}
+ 	return true;
+ 
+ err:
+ 	udev->waiting_global = true;
+ 	/* Try to wake up the unmap thread */
+ 	wake_up(&unmap_wait);
+ 	return false;
+ }
+ 
+ static inline struct page *
+ tcmu_get_block_page(struct tcmu_dev *udev, uint32_t dbi)
+ {
+ 	return radix_tree_lookup(&udev->data_blocks, dbi);
+ }
+ 
+ static inline void tcmu_free_cmd(struct tcmu_cmd *tcmu_cmd)
+ {
+ 	kfree(tcmu_cmd->dbi);
+ 	kmem_cache_free(tcmu_cmd_cache, tcmu_cmd);
+ }
+ 
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  static inline size_t tcmu_cmd_get_data_length(struct tcmu_cmd *tcmu_cmd)
  {
  	struct se_cmd *se_cmd = tcmu_cmd->se_cmd;
@@@ -428,33 -406,39 +559,51 @@@ static inline size_t iov_tail(struct tc
  	return (size_t)iov->iov_base + iov->iov_len;
  }
  
++<<<<<<< HEAD
 +static void alloc_and_scatter_data_area(struct tcmu_dev *udev,
 +	unsigned long *cmd_bitmap, struct scatterlist *data_sg,
 +	unsigned int data_nents, struct iovec **iov, int *iov_cnt,
 +	bool copy_data)
++=======
+ static int scatter_data_area(struct tcmu_dev *udev,
+ 	struct tcmu_cmd *tcmu_cmd, struct scatterlist *data_sg,
+ 	unsigned int data_nents, struct iovec **iov,
+ 	int *iov_cnt, bool copy_data)
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  {
 -	int i, dbi;
 +	int i, block;
  	int block_remaining = 0;
 -	void *from, *to = NULL;
 -	size_t copy_bytes, to_offset, offset;
 +	void *from, *to;
 +	size_t copy_bytes, to_offset;
  	struct scatterlist *sg;
+ 	struct page *page;
  
  	for_each_sg(data_sg, sg, data_nents, i) {
  		int sg_remaining = sg->length;
  		from = kmap_atomic(sg_page(sg)) + sg->offset;
  		while (sg_remaining > 0) {
  			if (block_remaining == 0) {
++<<<<<<< HEAD
 +				block = find_first_zero_bit(udev->data_bitmap,
 +							    udev->max_blocks);
 +				block_remaining = DATA_BLOCK_SIZE;
 +				set_bit(block, udev->data_bitmap);
 +				set_bit(block, cmd_bitmap);
++=======
+ 				if (to)
+ 					kunmap_atomic(to);
+ 
+ 				block_remaining = DATA_BLOCK_SIZE;
+ 				dbi = tcmu_cmd_get_dbi(tcmu_cmd);
+ 				page = tcmu_get_block_page(udev, dbi);
+ 				to = kmap_atomic(page);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  			}
 -
  			copy_bytes = min_t(size_t, sg_remaining,
  					block_remaining);
 -			to_offset = get_block_offset_user(udev, dbi,
 +			to_offset = get_block_offset(udev, block,
  					block_remaining);
 -			offset = DATA_BLOCK_SIZE - block_remaining;
 -			to = (void *)(unsigned long)to + offset;
 -
 +			to = (void *)udev->mb_addr + to_offset;
  			if (*iov_cnt != 0 &&
  			    to_offset == iov_tail(udev, *iov)) {
  				(*iov)->iov_len += copy_bytes;
@@@ -473,33 -457,55 +622,77 @@@
  		}
  		kunmap_atomic(from - sg->offset);
  	}
++<<<<<<< HEAD
++=======
+ 	if (to)
+ 		kunmap_atomic(to);
+ 
+ 	return 0;
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  }
  
 -static void gather_data_area(struct tcmu_dev *udev, struct tcmu_cmd *cmd,
 -			     bool bidi)
 +static void free_data_area(struct tcmu_dev *udev, struct tcmu_cmd *cmd)
  {
 -	struct se_cmd *se_cmd = cmd->se_cmd;
 -	int i, dbi;
 +	bitmap_xor(udev->data_bitmap, udev->data_bitmap, cmd->data_bitmap,
 +		   udev->max_blocks);
 +}
 +
 +static void gather_data_area(struct tcmu_dev *udev, unsigned long *cmd_bitmap,
 +		struct scatterlist *data_sg, unsigned int data_nents)
 +{
 +	int i, block;
  	int block_remaining = 0;
++<<<<<<< HEAD
 +	void *from, *to;
 +	size_t copy_bytes, from_offset;
 +	struct scatterlist *sg;
++=======
+ 	void *from = NULL, *to;
+ 	size_t copy_bytes, offset;
+ 	struct scatterlist *sg, *data_sg;
+ 	struct page *page;
+ 	unsigned int data_nents;
+ 	uint32_t count = 0;
+ 
+ 	if (!bidi) {
+ 		data_sg = se_cmd->t_data_sg;
+ 		data_nents = se_cmd->t_data_nents;
+ 	} else {
+ 
+ 		/*
+ 		 * For bidi case, the first count blocks are for Data-Out
+ 		 * buffer blocks, and before gathering the Data-In buffer
+ 		 * the Data-Out buffer blocks should be discarded.
+ 		 */
+ 		count = DIV_ROUND_UP(se_cmd->data_length, DATA_BLOCK_SIZE);
+ 
+ 		data_sg = se_cmd->t_bidi_data_sg;
+ 		data_nents = se_cmd->t_bidi_data_nents;
+ 	}
+ 
+ 	tcmu_cmd_set_dbi_cur(cmd, count);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  
  	for_each_sg(data_sg, sg, data_nents, i) {
  		int sg_remaining = sg->length;
  		to = kmap_atomic(sg_page(sg)) + sg->offset;
  		while (sg_remaining > 0) {
  			if (block_remaining == 0) {
++<<<<<<< HEAD
 +				block = find_first_bit(cmd_bitmap,
 +						       udev->max_blocks);
 +				block_remaining = DATA_BLOCK_SIZE;
 +				clear_bit(block, udev->data_bitmap);
 +				clear_bit(block, cmd_bitmap);
++=======
+ 				if (from)
+ 					kunmap_atomic(from);
+ 
+ 				block_remaining = DATA_BLOCK_SIZE;
+ 				dbi = tcmu_cmd_get_dbi(cmd);
+ 				page = tcmu_get_block_page(udev, dbi);
+ 				from = kmap_atomic(page);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  			}
  			copy_bytes = min_t(size_t, sg_remaining,
  					block_remaining);
@@@ -515,13 -520,13 +708,21 @@@
  		}
  		kunmap_atomic(to - sg->offset);
  	}
+ 	if (from)
+ 		kunmap_atomic(from);
  }
  
++<<<<<<< HEAD
 +static inline size_t spc_bitmap_free(unsigned long *bitmap,
 +				     unsigned int max_blocks)
 +{
 +	return DATA_BLOCK_SIZE *
 +			(max_blocks - bitmap_weight(bitmap, max_blocks));
++=======
+ static inline size_t spc_bitmap_free(unsigned long *bitmap, uint32_t thresh)
+ {
+ 	return DATA_BLOCK_SIZE * (thresh - bitmap_weight(bitmap, thresh));
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  }
  
  /*
@@@ -556,13 -564,41 +760,45 @@@ static bool is_ring_space_avail(struct 
  		return false;
  	}
  
++<<<<<<< HEAD
 +	space = spc_bitmap_free(udev->data_bitmap, udev->max_blocks);
++=======
+ 	/* try to check and get the data blocks as needed */
+ 	space = spc_bitmap_free(udev->data_bitmap, udev->dbi_thresh);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  	if (space < data_needed) {
- 		pr_debug("no data space: only %zu available, but ask for %zu\n",
- 				space, data_needed);
- 		return false;
+ 		unsigned long blocks_left = DATA_BLOCK_BITS - udev->dbi_thresh;
+ 		unsigned long grow;
+ 
+ 		if (blocks_left < blocks_needed) {
+ 			pr_debug("no data space: only %lu available, but ask for %zu\n",
+ 					blocks_left * DATA_BLOCK_SIZE,
+ 					data_needed);
+ 			return false;
+ 		}
+ 
+ 		/* Try to expand the thresh */
+ 		if (!udev->dbi_thresh) {
+ 			/* From idle state */
+ 			uint32_t init_thresh = DATA_BLOCK_INIT_BITS;
+ 
+ 			udev->dbi_thresh = max(blocks_needed, init_thresh);
+ 		} else {
+ 			/*
+ 			 * Grow the data area by max(blocks needed,
+ 			 * dbi_thresh / 2), but limited to the max
+ 			 * DATA_BLOCK_BITS size.
+ 			 */
+ 			grow = max(blocks_needed, udev->dbi_thresh / 2);
+ 			udev->dbi_thresh += grow;
+ 			if (udev->dbi_thresh > DATA_BLOCK_BITS)
+ 				udev->dbi_thresh = DATA_BLOCK_BITS;
+ 		}
  	}
  
+ 	if (!tcmu_get_empty_blocks(udev, cmd))
+ 		return false;
+ 
  	return true;
  }
  
@@@ -608,19 -623,19 +844,19 @@@ tcmu_queue_cmd_ring(struct tcmu_cmd *tc
  	/*
  	 * Must be a certain minimum size for response sense info, but
  	 * also may be larger if the iov array is large.
 +	 * We prepare as many iovs as possbile for potential uses here,
 +	 * because it's expensive to tell how many regions are freed in
 +	 * the bitmap & global data pool, as the size calculated here
 +	 * will only be used to do the checks.
  	 *
 -	 * We prepare way too many iovs for potential uses here, because it's
 -	 * expensive to tell how many regions are freed in the bitmap
 -	*/
 -	base_command_size = max(offsetof(struct tcmu_cmd_entry,
 -				req.iov[tcmu_cmd_get_block_cnt(tcmu_cmd)]),
 -				sizeof(struct tcmu_cmd_entry));
 -	command_size = base_command_size
 -		+ round_up(scsi_command_size(se_cmd->t_task_cdb), TCMU_OP_ALIGN_SIZE);
 -
 -	WARN_ON(command_size & (TCMU_OP_ALIGN_SIZE-1));
 +	 * The size will be recalculated later as actually needed to save
 +	 * cmd area memories.
 +	 */
 +	base_command_size = tcmu_cmd_get_base_cmd_size(
 +					tcmu_cmd_get_block_cnt(tcmu_cmd));
 +	command_size = tcmu_cmd_get_cmd_size(tcmu_cmd, base_command_size);
  
- 	spin_lock_irq(&udev->cmdr_lock);
+ 	mutex_lock(&udev->cmdr_lock);
  
  	mb = udev->mb_addr;
  	cmd_head = mb->cmd_head % udev->cmdr_size; /* UAM */
@@@ -650,27 -655,19 +886,32 @@@
  		prepare_to_wait(&udev->wait_cmdr, &__wait, TASK_INTERRUPTIBLE);
  
  		pr_debug("sleeping for ring space\n");
++<<<<<<< HEAD
 +		/*
 +		 * For backwards compat if qfull_time_out is not set
 +		 * use cmd_time_out and if that's not set use the default
 +		 * time out.
 +		 */
 +		spin_unlock_irq(&udev->cmdr_lock);
 +		if (udev->qfull_time_out > 0)
 +			ret = schedule_timeout(
 +					msecs_to_jiffies(udev->qfull_time_out));
 +		else if (udev->cmd_time_out)
++=======
+ 		mutex_unlock(&udev->cmdr_lock);
+ 		if (udev->cmd_time_out)
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  			ret = schedule_timeout(
  					msecs_to_jiffies(udev->cmd_time_out));
  		else
  			ret = schedule_timeout(msecs_to_jiffies(TCMU_TIME_OUT));
  		finish_wait(&udev->wait_cmdr, &__wait);
  		if (!ret) {
 -			pr_warn("tcmu: command timed out\n");
 -			return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
 +			pr_debug("tcmu: command timed out waiting for ring space.\n");
 +			return TCM_OUT_OF_RESOURCES;
  		}
  
- 		spin_lock_irq(&udev->cmdr_lock);
+ 		mutex_lock(&udev->cmdr_lock);
  
  		/* We dropped cmdr_lock, cmd_head is stale */
  		cmd_head = mb->cmd_head % udev->cmdr_size; /* UAM */
@@@ -696,37 -692,49 +937,70 @@@
  	}
  
  	entry = (void *) mb + CMDR_OFF + cmd_head;
 -	tcmu_flush_dcache_range(entry, sizeof(*entry));
 +	memset(entry, 0, command_size);
  	tcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_CMD);
 -	tcmu_hdr_set_len(&entry->hdr.len_op, command_size);
  	entry->hdr.cmd_id = tcmu_cmd->cmd_id;
 -	entry->hdr.kflags = 0;
 -	entry->hdr.uflags = 0;
  
  	/* Handle allocating space from the data area */
+ 	tcmu_cmd_reset_dbi_cur(tcmu_cmd);
  	iov = &entry->req.iov[0];
  	iov_cnt = 0;
  	copy_to_data_area = (se_cmd->data_direction == DMA_TO_DEVICE
  		|| se_cmd->se_cmd_flags & SCF_BIDI);
++<<<<<<< HEAD
 +	alloc_and_scatter_data_area(udev, tcmu_cmd->data_bitmap,
 +		se_cmd->t_data_sg, se_cmd->t_data_nents, &iov, &iov_cnt,
 +		copy_to_data_area);
++=======
+ 	ret = scatter_data_area(udev, tcmu_cmd, se_cmd->t_data_sg,
+ 				se_cmd->t_data_nents, &iov, &iov_cnt,
+ 				copy_to_data_area);
+ 	if (ret) {
+ 		tcmu_cmd_free_data(tcmu_cmd, tcmu_cmd->dbi_cnt);
+ 		mutex_unlock(&udev->cmdr_lock);
+ 
+ 		pr_err("tcmu: alloc and scatter data failed\n");
+ 		return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+ 	}
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  	entry->req.iov_cnt = iov_cnt;
  	entry->req.iov_dif_cnt = 0;
  
  	/* Handle BIDI commands */
++<<<<<<< HEAD
 +	iov_cnt = 0;
 +	alloc_and_scatter_data_area(udev, tcmu_cmd->data_bitmap,
 +		se_cmd->t_bidi_data_sg, se_cmd->t_bidi_data_nents, &iov,
 +		&iov_cnt, false);
 +	entry->req.iov_bidi_cnt = iov_cnt;
 +
 +	/*
 +	 * Recalaulate the command's base size and size according
 +	 * to the actual needs
 +	 */
 +	base_command_size = tcmu_cmd_get_base_cmd_size(entry->req.iov_cnt +
 +						       entry->req.iov_bidi_cnt);
 +	command_size = tcmu_cmd_get_cmd_size(tcmu_cmd, base_command_size);
 +
 +	tcmu_hdr_set_len(&entry->hdr.len_op, command_size);
++=======
+ 	if (se_cmd->se_cmd_flags & SCF_BIDI) {
+ 		iov_cnt = 0;
+ 		iov++;
+ 		ret = scatter_data_area(udev, tcmu_cmd,
+ 					se_cmd->t_bidi_data_sg,
+ 					se_cmd->t_bidi_data_nents,
+ 					&iov, &iov_cnt, false);
+ 		if (ret) {
+ 			tcmu_cmd_free_data(tcmu_cmd, tcmu_cmd->dbi_cnt);
+ 			mutex_unlock(&udev->cmdr_lock);
+ 
+ 			pr_err("tcmu: alloc and scatter bidi data failed\n");
+ 			return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;
+ 		}
+ 		entry->req.iov_bidi_cnt = iov_cnt;
+ 	}
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  
  	/* All offsets relative to mb_addr, not start of entry! */
  	cdb_off = CMDR_OFF + cmd_head + base_command_size;
@@@ -812,8 -815,10 +1085,12 @@@ static void tcmu_handle_completion(stru
  	}
  
  	target_complete_cmd(cmd->se_cmd, entry->rsp.scsi_status);
 -
 -out:
  	cmd->se_cmd = NULL;
++<<<<<<< HEAD
 +
++=======
+ 	tcmu_cmd_free_data(cmd, cmd->dbi_cnt);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  	tcmu_free_cmd(cmd);
  }
  
@@@ -952,12 -948,9 +1222,12 @@@ static struct se_device *tcmu_alloc_dev
  
  	udev->hba = hba;
  	udev->cmd_time_out = TCMU_TIME_OUT;
 +	udev->qfull_time_out = -1;
 +
 +	udev->max_blocks = DATA_BLOCK_BITS_DEF;
  
  	init_waitqueue_head(&udev->wait_cmdr);
- 	spin_lock_init(&udev->cmdr_lock);
+ 	mutex_init(&udev->cmdr_lock);
  
  	idr_init(&udev->commands);
  	spin_lock_init(&udev->commands_lock);
@@@ -997,9 -989,63 +1269,67 @@@ static int tcmu_find_mem_index(struct v
  	return -1;
  }
  
++<<<<<<< HEAD
 +static int tcmu_vma_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
++=======
+ static struct page *tcmu_try_get_block_page(struct tcmu_dev *udev, uint32_t dbi)
+ {
+ 	struct page *page;
+ 	int ret;
+ 
+ 	mutex_lock(&udev->cmdr_lock);
+ 	page = tcmu_get_block_page(udev, dbi);
+ 	if (likely(page)) {
+ 		mutex_unlock(&udev->cmdr_lock);
+ 		return page;
+ 	}
+ 
+ 	/*
+ 	 * Normally it shouldn't be here:
+ 	 * Only when the userspace has touched the blocks which
+ 	 * are out of the tcmu_cmd's data iov[], and will return
+ 	 * one zeroed page.
+ 	 */
+ 	pr_warn("Block(%u) out of cmd's iov[] has been touched!\n", dbi);
+ 	pr_warn("Mostly it will be a bug of userspace, please have a check!\n");
+ 
+ 	if (dbi >= udev->dbi_thresh) {
+ 		/* Extern the udev->dbi_thresh to dbi + 1 */
+ 		udev->dbi_thresh = dbi + 1;
+ 		udev->dbi_max = dbi;
+ 	}
+ 
+ 	page = radix_tree_lookup(&udev->data_blocks, dbi);
+ 	if (!page) {
+ 		page = alloc_page(GFP_KERNEL | __GFP_ZERO);
+ 		if (!page) {
+ 			mutex_unlock(&udev->cmdr_lock);
+ 			return NULL;
+ 		}
+ 
+ 		ret = radix_tree_insert(&udev->data_blocks, dbi, page);
+ 		if (ret) {
+ 			mutex_unlock(&udev->cmdr_lock);
+ 			__free_page(page);
+ 			return NULL;
+ 		}
+ 
+ 		/*
+ 		 * Since this case is rare in page fault routine, here we
+ 		 * will allow the global_db_count >= TCMU_GLOBAL_MAX_BLOCKS
+ 		 * to reduce possible page fault call trace.
+ 		 */
+ 		atomic_inc(&global_db_count);
+ 	}
+ 	mutex_unlock(&udev->cmdr_lock);
+ 
+ 	return page;
+ }
+ 
+ static int tcmu_vma_fault(struct vm_fault *vmf)
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  {
 -	struct tcmu_dev *udev = vmf->vma->vm_private_data;
 +	struct tcmu_dev *udev = vma->vm_private_data;
  	struct uio_info *info = &udev->uio_info;
  	struct page *page;
  	unsigned long offset;
@@@ -1015,11 -1061,20 +1345,28 @@@
  	 */
  	offset = (vmf->pgoff - mi) << PAGE_SHIFT;
  
++<<<<<<< HEAD
 +	addr = (void *)(unsigned long)info->mem[mi].addr + offset;
 +	if (info->mem[mi].memtype == UIO_MEM_LOGICAL)
 +		page = virt_to_page(addr);
 +	else
 +		page = vmalloc_to_page(addr);
++=======
+ 	if (offset < udev->data_off) {
+ 		/* For the vmalloc()ed cmd area pages */
+ 		addr = (void *)(unsigned long)info->mem[mi].addr + offset;
+ 		page = vmalloc_to_page(addr);
+ 	} else {
+ 		uint32_t dbi;
+ 
+ 		/* For the dynamically growing data area pages */
+ 		dbi = (offset - udev->data_off) / DATA_BLOCK_SIZE;
+ 		page = tcmu_try_get_block_page(udev, dbi);
+ 		if (!page)
+ 			return VM_FAULT_NOPAGE;
+ 	}
+ 
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  	get_page(page);
  	vmf->page = page;
  	return 0;
@@@ -1226,8 -1199,11 +1575,14 @@@ static int tcmu_configure_device(struc
  	/* mailbox fits in first part of CMDR space */
  	udev->cmdr_size = CMDR_SIZE - CMDR_OFF;
  	udev->data_off = CMDR_SIZE;
++<<<<<<< HEAD
 +	udev->data_size = udev->ring_size - CMDR_SIZE;
++=======
+ 	udev->data_size = DATA_SIZE;
+ 	udev->dbi_thresh = 0; /* Default in Idle state */
+ 	udev->waiting_global = false;
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  
 -	/* Initialise the mailbox of the ring buffer */
  	mb = udev->mb_addr;
  	mb->version = TCMU_MAILBOX_VERSION;
  	mb->flags = TCMU_MAILBOX_FLAG_CAP_OOOC;
@@@ -1238,6 -1214,8 +1593,11 @@@
  	WARN_ON(udev->data_size % PAGE_SIZE);
  	WARN_ON(udev->data_size % DATA_BLOCK_SIZE);
  
++<<<<<<< HEAD
++=======
+ 	INIT_RADIX_TREE(&udev->data_blocks, GFP_KERNEL);
+ 
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  	info->version = __stringify(TCMU_MAILBOX_VERSION);
  
  	info->mem[0].name = "tcm-user command & data buffer";
@@@ -1313,7 -1285,24 +1677,28 @@@ static bool tcmu_dev_configured(struct 
  	return udev->uio_info.uio_dev ? true : false;
  }
  
++<<<<<<< HEAD
 +static void tcmu_destroy_device(struct se_device *dev)
++=======
+ static void tcmu_blocks_release(struct tcmu_dev *udev)
+ {
+ 	int i;
+ 	struct page *page;
+ 
+ 	/* Try to release all block pages */
+ 	mutex_lock(&udev->cmdr_lock);
+ 	for (i = 0; i <= udev->dbi_max; i++) {
+ 		page = radix_tree_delete(&udev->data_blocks, i);
+ 		if (page) {
+ 			__free_page(page);
+ 			atomic_dec(&global_db_count);
+ 		}
+ 	}
+ 	mutex_unlock(&udev->cmdr_lock);
+ }
+ 
+ static void tcmu_free_device(struct se_device *dev)
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  {
  	struct tcmu_dev *udev = TCMU_DEV(dev);
  	struct tcmu_cmd *cmd;
@@@ -1322,17 -1311,9 +1707,23 @@@
  
  	del_timer_sync(&udev->timeout);
  
++<<<<<<< HEAD
 +	if (tcmu_dev_configured(udev)) {
 +		tcmu_netlink_event(udev, TCMU_CMD_REMOVED_DEVICE, 0, NULL);
 +
 +		uio_unregister_device(&udev->uio_info);
 +		kfree(udev->uio_info.name);
 +		kfree(udev->name);
 +
 +		mutex_lock(&device_mutex);
 +		idr_remove(&devices_idr, udev->dev_index);
 +		mutex_unlock(&device_mutex);
 +	}
++=======
+ 	mutex_lock(&root_udev_mutex);
+ 	list_del(&udev->node);
+ 	mutex_unlock(&root_udev_mutex);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  
  	vfree(udev->mb_addr);
  
@@@ -1345,7 -1326,18 +1736,22 @@@
  	idr_destroy(&udev->commands);
  	spin_unlock_irq(&udev->commands_lock);
  	WARN_ON(!all_expired);
++<<<<<<< HEAD
 +	kfree(udev->data_bitmap);
++=======
+ 
+ 	tcmu_blocks_release(udev);
+ 
+ 	if (tcmu_dev_configured(udev)) {
+ 		tcmu_netlink_event(TCMU_CMD_REMOVED_DEVICE, udev->uio_info.name,
+ 				   udev->uio_info.uio_dev->minor);
+ 
+ 		uio_unregister_device(&udev->uio_info);
+ 		kfree(udev->uio_info.name);
+ 		kfree(udev->name);
+ 	}
+ 	call_rcu(&dev->rcu_head, tcmu_dev_call_rcu);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  }
  
  enum {
@@@ -1650,9 -1515,84 +2056,84 @@@ static struct target_backend_ops tcmu_o
  	.tb_dev_attrib_attrs	= NULL,
  };
  
+ static int unmap_thread_fn(void *data)
+ {
+ 	struct tcmu_dev *udev;
+ 	loff_t off;
+ 	uint32_t start, end, block;
+ 	struct page *page;
+ 	int i;
+ 
+ 	while (1) {
+ 		DEFINE_WAIT(__wait);
+ 
+ 		prepare_to_wait(&unmap_wait, &__wait, TASK_INTERRUPTIBLE);
+ 		schedule();
+ 		finish_wait(&unmap_wait, &__wait);
+ 
+ 		mutex_lock(&root_udev_mutex);
+ 		list_for_each_entry(udev, &root_udev, node) {
+ 			mutex_lock(&udev->cmdr_lock);
+ 
+ 			/* Try to complete the finished commands first */
+ 			tcmu_handle_completions(udev);
+ 
+ 			/* Skip the udevs waiting the global pool or in idle */
+ 			if (udev->waiting_global || !udev->dbi_thresh) {
+ 				mutex_unlock(&udev->cmdr_lock);
+ 				continue;
+ 			}
+ 
+ 			end = udev->dbi_max + 1;
+ 			block = find_last_bit(udev->data_bitmap, end);
+ 			if (block == udev->dbi_max) {
+ 				/*
+ 				 * The last bit is dbi_max, so there is
+ 				 * no need to shrink any blocks.
+ 				 */
+ 				mutex_unlock(&udev->cmdr_lock);
+ 				continue;
+ 			} else if (block == end) {
+ 				/* The current udev will goto idle state */
+ 				udev->dbi_thresh = start = 0;
+ 				udev->dbi_max = 0;
+ 			} else {
+ 				udev->dbi_thresh = start = block + 1;
+ 				udev->dbi_max = block;
+ 			}
+ 
+ 			/* Here will truncate the data area from off */
+ 			off = udev->data_off + start * DATA_BLOCK_SIZE;
+ 			unmap_mapping_range(udev->inode->i_mapping, off, 0, 1);
+ 
+ 			/* Release the block pages */
+ 			for (i = start; i < end; i++) {
+ 				page = radix_tree_delete(&udev->data_blocks, i);
+ 				if (page) {
+ 					__free_page(page);
+ 					atomic_dec(&global_db_count);
+ 				}
+ 			}
+ 			mutex_unlock(&udev->cmdr_lock);
+ 		}
+ 
+ 		/*
+ 		 * Try to wake up the udevs who are waiting
+ 		 * for the global data pool.
+ 		 */
+ 		list_for_each_entry(udev, &root_udev, node) {
+ 			if (udev->waiting_global)
+ 				wake_up(&udev->wait_cmdr);
+ 		}
+ 		mutex_unlock(&root_udev_mutex);
+ 	}
+ 
+ 	return 0;
+ }
+ 
  static int __init tcmu_module_init(void)
  {
 -	int ret, i, len = 0;
 +	int ret, i, k, len = 0;
  
  	BUILD_BUG_ON((sizeof(struct tcmu_cmd_entry) % TCMU_OP_ALIGN_SIZE) != 0);
  
@@@ -1701,7 -1635,12 +2182,16 @@@
  	if (ret)
  		goto out_attrs;
  
++<<<<<<< HEAD
 +	idr_init(&devices_idr);
++=======
+ 	init_waitqueue_head(&unmap_wait);
+ 	unmap_thread = kthread_run(unmap_thread_fn, NULL, "tcmu_unmap");
+ 	if (IS_ERR(unmap_thread)) {
+ 		ret = PTR_ERR(unmap_thread);
+ 		goto out_unreg_transport;
+ 	}
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  
  	return 0;
  
@@@ -1719,7 -1660,7 +2211,11 @@@ out_free_cache
  
  static void __exit tcmu_module_exit(void)
  {
++<<<<<<< HEAD
 +	idr_destroy(&devices_idr);
++=======
+ 	kthread_stop(unmap_thread);
++>>>>>>> b6df4b79a551 (tcmu: Add global data block pool support)
  	target_backend_unregister(&tcmu_ops);
  	kfree(tcmu_attrs);
  	genl_unregister_family(&tcmu_genl_family);
* Unmerged path drivers/target/target_core_user.c
