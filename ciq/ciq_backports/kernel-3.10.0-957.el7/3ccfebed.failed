powerpc, membarrier: Skip memory barrier in switch_mm()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [powerpc] membarrier: skip memory barrier in switch_mm() (Rafael Aquini) [1560024]
Rebuild_FUZZ: 91.09%
commit-author Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
commit 3ccfebedd8cf54e291c809c838d8ad5cc00f5688
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3ccfebed.failed

Allow PowerPC to skip the full memory barrier in switch_mm(), and
only issue the barrier when scheduling into a task belonging to a
process that has registered to use expedited private.

Threads targeting the same VM but which belong to different thread
groups is a tricky case. It has a few consequences:

It turns out that we cannot rely on get_nr_threads(p) to count the
number of threads using a VM. We can use
(atomic_read(&mm->mm_users) == 1 && get_nr_threads(p) == 1)
instead to skip the synchronize_sched() for cases where the VM only has
a single user, and that user only has a single thread.

It also turns out that we cannot use for_each_thread() to set
thread flags in all threads using a VM, as it only iterates on the
thread group.

Therefore, test the membarrier state variable directly rather than
relying on thread flags. This means
membarrier_register_private_expedited() needs to set the
MEMBARRIER_STATE_PRIVATE_EXPEDITED flag, issue synchronize_sched(), and
only then set MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY which allows
private expedited membarrier commands to succeed.
membarrier_arch_switch_mm() now tests for the
MEMBARRIER_STATE_PRIVATE_EXPEDITED flag.

	Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
	Acked-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Alan Stern <stern@rowland.harvard.edu>
	Cc: Alexander Viro <viro@zeniv.linux.org.uk>
	Cc: Andrea Parri <parri.andrea@gmail.com>
	Cc: Andrew Hunter <ahh@google.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Avi Kivity <avi@scylladb.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Boqun Feng <boqun.feng@gmail.com>
	Cc: Dave Watson <davejwatson@fb.com>
	Cc: David Sehr <sehr@google.com>
	Cc: Greg Hackmann <ghackmann@google.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Maged Michael <maged.michael@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Nicholas Piggin <npiggin@gmail.com>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: linux-api@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
	Cc: linuxppc-dev@lists.ozlabs.org
Link: http://lkml.kernel.org/r/20180129202020.8515-3-mathieu.desnoyers@efficios.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 3ccfebedd8cf54e291c809c838d8ad5cc00f5688)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	MAINTAINERS
#	arch/powerpc/Kconfig
#	arch/powerpc/mm/mmu_context.c
#	include/linux/sched/mm.h
#	init/Kconfig
#	kernel/sched/core.c
#	kernel/sched/membarrier.c
diff --cc MAINTAINERS
index 20d5d87b6e53,8e96d4e9677b..000000000000
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@@ -5639,14 -8865,86 +5639,75 @@@ W:	http://www.mellanox.co
  Q:	http://patchwork.ozlabs.org/project/netdev/list/
  F:	drivers/net/ethernet/mellanox/mlxfw/
  
 -MELLANOX MLX CPLD HOTPLUG DRIVER
 -M:	Vadim Pasternak <vadimp@mellanox.com>
 -L:	platform-driver-x86@vger.kernel.org
 -S:	Supported
 -F:	drivers/platform/x86/mlxcpld-hotplug.c
 -F:	include/linux/platform_data/mlxcpld-hotplug.h
 -
 -MELLANOX MLX4 core VPI driver
 -M:	Tariq Toukan <tariqt@mellanox.com>
 -L:	netdev@vger.kernel.org
 +SOFT-ROCE DRIVER (rxe)
 +M:	Moni Shoua <monis@mellanox.com>
  L:	linux-rdma@vger.kernel.org
 -W:	http://www.mellanox.com
 -Q:	http://patchwork.ozlabs.org/project/netdev/list/
  S:	Supported
 -F:	drivers/net/ethernet/mellanox/mlx4/
 -F:	include/linux/mlx4/
 -
 -MELLANOX MLX4 IB driver
 -M:	Yishai Hadas <yishaih@mellanox.com>
 -L:	linux-rdma@vger.kernel.org
 -W:	http://www.mellanox.com
 +W:	https://github.com/SoftRoCE/rxe-dev/wiki/rxe-dev:-Home
  Q:	http://patchwork.kernel.org/project/linux-rdma/list/
++<<<<<<< HEAD
 +F:	drivers/infiniband/hw/rxe/
 +F:	include/uapi/rdma/rdma_user_rxe.h
++=======
+ S:	Supported
+ F:	drivers/infiniband/hw/mlx4/
+ F:	include/linux/mlx4/
+ F:	include/uapi/rdma/mlx4-abi.h
+ 
+ MELLANOX MLX5 core VPI driver
+ M:	Saeed Mahameed <saeedm@mellanox.com>
+ M:	Matan Barak <matanb@mellanox.com>
+ M:	Leon Romanovsky <leonro@mellanox.com>
+ L:	netdev@vger.kernel.org
+ L:	linux-rdma@vger.kernel.org
+ W:	http://www.mellanox.com
+ Q:	http://patchwork.ozlabs.org/project/netdev/list/
+ S:	Supported
+ F:	drivers/net/ethernet/mellanox/mlx5/core/
+ F:	include/linux/mlx5/
+ 
+ MELLANOX MLX5 IB driver
+ M:	Matan Barak <matanb@mellanox.com>
+ M:	Leon Romanovsky <leonro@mellanox.com>
+ L:	linux-rdma@vger.kernel.org
+ W:	http://www.mellanox.com
+ Q:	http://patchwork.kernel.org/project/linux-rdma/list/
+ S:	Supported
+ F:	drivers/infiniband/hw/mlx5/
+ F:	include/linux/mlx5/
+ F:	include/uapi/rdma/mlx5-abi.h
+ 
+ MELLANOX MLXCPLD I2C AND MUX DRIVER
+ M:	Vadim Pasternak <vadimp@mellanox.com>
+ M:	Michael Shych <michaelsh@mellanox.com>
+ L:	linux-i2c@vger.kernel.org
+ S:	Supported
+ F:	drivers/i2c/busses/i2c-mlxcpld.c
+ F:	drivers/i2c/muxes/i2c-mux-mlxcpld.c
+ F:	Documentation/i2c/busses/i2c-mlxcpld
+ 
+ MELLANOX MLXCPLD LED DRIVER
+ M:	Vadim Pasternak <vadimp@mellanox.com>
+ L:	linux-leds@vger.kernel.org
+ S:	Supported
+ F:	drivers/leds/leds-mlxcpld.c
+ F:	Documentation/leds/leds-mlxcpld.txt
+ 
+ MELLANOX PLATFORM DRIVER
+ M:	Vadim Pasternak <vadimp@mellanox.com>
+ L:	platform-driver-x86@vger.kernel.org
+ S:	Supported
+ F:	drivers/platform/x86/mlx-platform.c
+ 
+ MEMBARRIER SUPPORT
+ M:	Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
+ M:	"Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
+ L:	linux-kernel@vger.kernel.org
+ S:	Supported
+ F:	kernel/sched/membarrier.c
+ F:	include/uapi/linux/membarrier.h
+ F:	arch/powerpc/include/asm/membarrier.h
++>>>>>>> 3ccfebedd8cf (powerpc, membarrier: Skip memory barrier in switch_mm())
  
  MEMORY MANAGEMENT
  L:	linux-mm@kvack.org
diff --cc arch/powerpc/Kconfig
index 32232f9bee79,a2380de50878..000000000000
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@@ -126,72 -131,113 +126,92 @@@ config ARCH_HAS_DMA_SET_COHERENT_MAS
  config PPC
  	bool
  	default y
++<<<<<<< HEAD
++=======
+ 	#
+ 	# Please keep this list sorted alphabetically.
+ 	#
+ 	select ARCH_HAS_DEVMEM_IS_ALLOWED
+ 	select ARCH_HAS_DMA_SET_COHERENT_MASK
+ 	select ARCH_HAS_ELF_RANDOMIZE
+ 	select ARCH_HAS_FORTIFY_SOURCE
+ 	select ARCH_HAS_GCOV_PROFILE_ALL
+ 	select ARCH_HAS_PMEM_API                if PPC64
+ 	select ARCH_HAS_MEMBARRIER_CALLBACKS
+ 	select ARCH_HAS_SCALED_CPUTIME		if VIRT_CPU_ACCOUNTING_NATIVE
+ 	select ARCH_HAS_SG_CHAIN
+ 	select ARCH_HAS_TICK_BROADCAST		if GENERIC_CLOCKEVENTS_BROADCAST
+ 	select ARCH_HAS_UACCESS_FLUSHCACHE	if PPC64
+ 	select ARCH_HAS_UBSAN_SANITIZE_ALL
+ 	select ARCH_HAS_ZONE_DEVICE		if PPC_BOOK3S_64
+ 	select ARCH_HAVE_NMI_SAFE_CMPXCHG
++>>>>>>> 3ccfebedd8cf (powerpc, membarrier: Skip memory barrier in switch_mm())
  	select ARCH_MIGHT_HAVE_PC_PARPORT
 -	select ARCH_MIGHT_HAVE_PC_SERIO
 -	select ARCH_SUPPORTS_ATOMIC_RMW
 -	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 -	select ARCH_USE_BUILTIN_BSWAP
 -	select ARCH_USE_CMPXCHG_LOCKREF		if PPC64
 -	select ARCH_WANT_IPC_PARSE_VERSION
 -	select ARCH_WEAK_RELEASE_ACQUIRE
  	select BINFMT_ELF
 -	select BUILDTIME_EXTABLE_SORT
 -	select CLONE_BACKWARDS
 -	select DCACHE_WORD_ACCESS		if PPC64 && CPU_LITTLE_ENDIAN
 -	select EDAC_ATOMIC_SCRUB
 -	select EDAC_SUPPORT
 -	select GENERIC_ATOMIC64			if PPC32
 -	select GENERIC_CLOCKEVENTS
 -	select GENERIC_CLOCKEVENTS_BROADCAST	if SMP
 -	select GENERIC_CMOS_UPDATE
 -	select GENERIC_CPU_AUTOPROBE
 -	select GENERIC_CPU_VULNERABILITIES	if PPC_BOOK3S_64
 -	select GENERIC_IRQ_SHOW
 -	select GENERIC_IRQ_SHOW_LEVEL
 -	select GENERIC_SMP_IDLE_THREAD
 -	select GENERIC_STRNCPY_FROM_USER
 -	select GENERIC_STRNLEN_USER
 -	select GENERIC_TIME_VSYSCALL
 -	select HAVE_ARCH_AUDITSYSCALL
 -	select HAVE_ARCH_JUMP_LABEL
 -	select HAVE_ARCH_KGDB
 +	select ARCH_HAS_ELF_RANDOMIZE
 +	select OF
 +	select OF_EARLY_FLATTREE
 +	select HAVE_FTRACE_MCOUNT_RECORD
  	select HAVE_ARCH_MMAP_RND_BITS
  	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if COMPAT
 -	select HAVE_ARCH_SECCOMP_FILTER
 -	select HAVE_ARCH_TRACEHOOK
 -	select ARCH_HAS_STRICT_KERNEL_RWX	if ((PPC_BOOK3S_64 || PPC32) && !RELOCATABLE && !HIBERNATION)
 -	select ARCH_OPTIONAL_KERNEL_RWX		if ARCH_HAS_STRICT_KERNEL_RWX
 -	select HAVE_CBPF_JIT			if !PPC64
 -	select HAVE_CONTEXT_TRACKING		if PPC64
 -	select HAVE_DEBUG_KMEMLEAK
 -	select HAVE_DEBUG_STACKOVERFLOW
 -	select HAVE_DMA_API_DEBUG
  	select HAVE_DYNAMIC_FTRACE
 -	select HAVE_DYNAMIC_FTRACE_WITH_REGS	if MPROFILE_KERNEL
 -	select HAVE_EBPF_JIT			if PPC64
 -	select HAVE_EFFICIENT_UNALIGNED_ACCESS	if !(CPU_LITTLE_ENDIAN && POWER7_CPU)
 -	select HAVE_FTRACE_MCOUNT_RECORD
 -	select HAVE_FUNCTION_GRAPH_TRACER
  	select HAVE_FUNCTION_TRACER
 -	select HAVE_GCC_PLUGINS
 -	select HAVE_GENERIC_GUP
 -	select HAVE_HW_BREAKPOINT		if PERF_EVENTS && (PPC_BOOK3S || PPC_8xx)
 +	select HAVE_FUNCTION_GRAPH_TRACER
 +	select SYSCTL_EXCEPTION_TRACE
 +	select ARCH_WANT_OPTIONAL_GPIOLIB
 +	select VIRT_TO_BUS if !PPC64
  	select HAVE_IDE
  	select HAVE_IOREMAP_PROT
 -	select HAVE_IRQ_EXIT_ON_IRQ_STACK
 -	select HAVE_KERNEL_GZIP
 +	select HAVE_EFFICIENT_UNALIGNED_ACCESS if !CPU_LITTLE_ENDIAN
  	select HAVE_KPROBES
 -	select HAVE_KPROBES_ON_FTRACE
 +	select HAVE_ARCH_KGDB
  	select HAVE_KRETPROBES
 -	select HAVE_LIVEPATCH			if HAVE_DYNAMIC_FTRACE_WITH_REGS
 +	select HAVE_ARCH_TRACEHOOK
  	select HAVE_MEMBLOCK
  	select HAVE_MEMBLOCK_NODE_MAP
 -	select HAVE_MOD_ARCH_SPECIFIC
 -	select HAVE_NMI				if PERF_EVENTS || (PPC64 && PPC_BOOK3S)
 -	select HAVE_HARDLOCKUP_DETECTOR_ARCH	if (PPC64 && PPC_BOOK3S)
 +	select HAVE_DMA_API_DEBUG
 +	select USE_GENERIC_SMP_HELPERS if SMP
  	select HAVE_OPROFILE
 -	select HAVE_OPTPROBES			if PPC64
 +	select HAVE_DEBUG_KMEMLEAK
 +	select GENERIC_ATOMIC64 if PPC32
 +	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
  	select HAVE_PERF_EVENTS
 -	select HAVE_PERF_EVENTS_NMI		if PPC64
 -	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI && !HAVE_HARDLOCKUP_DETECTOR_ARCH
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
 -	select HAVE_RCU_TABLE_FREE		if SMP
  	select HAVE_REGS_AND_STACK_ACCESS_API
 -	select HAVE_SYSCALL_TRACEPOINTS
 -	select HAVE_VIRT_CPU_ACCOUNTING
 -	select HAVE_IRQ_TIME_ACCOUNTING
 +	select HAVE_HW_BREAKPOINT if PERF_EVENTS && PPC_BOOK3S_64
 +	select HAVE_GENERIC_HARDIRQS
 +	select ARCH_WANT_IPC_PARSE_VERSION
 +	select SPARSE_IRQ
  	select IRQ_DOMAIN
 +	select GENERIC_IRQ_SHOW
 +	select GENERIC_IRQ_SHOW_LEVEL
  	select IRQ_FORCED_THREADING
 +	select HAVE_RCU_TABLE_FREE if SMP
 +	select HAVE_SYSCALL_TRACEPOINTS
 +	select HAVE_BPF_JIT if (PPC64 && CPU_BIG_ENDIAN)
 +	select HAVE_ARCH_JUMP_LABEL
 +	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 +	select GENERIC_SMP_IDLE_THREAD
 +	select GENERIC_CMOS_UPDATE
 +	select GENERIC_TIME_VSYSCALL_OLD
 +	select GENERIC_CLOCKEVENTS
 +	select GENERIC_CLOCKEVENTS_BROADCAST if SMP
 +	select ARCH_HAS_TICK_BROADCAST if GENERIC_CLOCKEVENTS_BROADCAST
 +	select GENERIC_STRNCPY_FROM_USER
 +	select GENERIC_STRNLEN_USER
 +	select HAVE_MOD_ARCH_SPECIFIC
  	select MODULES_USE_ELF_RELA
 -	select NO_BOOTMEM
 -	select OF
 -	select OF_EARLY_FLATTREE
 -	select OF_RESERVED_MEM
 -	select OLD_SIGACTION			if PPC32
 +	select CLONE_BACKWARDS
 +	select ARCH_USE_BUILTIN_BSWAP
  	select OLD_SIGSUSPEND
 -	select SPARSE_IRQ
 -	select SYSCTL_EXCEPTION_TRACE
 -	select VIRT_TO_BUS			if !PPC64
 -	#
 -	# Please keep this list sorted alphabetically.
 -	#
 +	select OLD_SIGACTION if PPC32
 +	select HAVE_IRQ_EXIT_ON_IRQ_STACK
 +	select ARCH_USE_CMPXCHG_LOCKREF if PPC64
 +	select ARCH_HAS_DMA_SET_COHERENT_MASK
 +	select HAVE_ARCH_SECCOMP_FILTER
 +	select HAVE_PERF_EVENTS_NMI if PPC64
 +	select GENERIC_CPU_VULNERABILITIES	if PPC_BOOK3S_64
  
  config GENERIC_CSUM
  	def_bool n
diff --cc include/linux/sched/mm.h
index a7adba1cd0a9,26307cdc3969..000000000000
--- a/include/linux/sched/mm.h
+++ b/include/linux/sched/mm.h
@@@ -1,6 -1,242 +1,242 @@@
  #ifndef _LINUX_SCHED_MM_H
  #define _LINUX_SCHED_MM_H
  
 -#include <linux/kernel.h>
 -#include <linux/atomic.h>
  #include <linux/sched.h>
++<<<<<<< HEAD
++=======
+ #include <linux/mm_types.h>
+ #include <linux/gfp.h>
+ 
+ /*
+  * Routines for handling mm_structs
+  */
+ extern struct mm_struct * mm_alloc(void);
+ 
+ /**
+  * mmgrab() - Pin a &struct mm_struct.
+  * @mm: The &struct mm_struct to pin.
+  *
+  * Make sure that @mm will not get freed even after the owning task
+  * exits. This doesn't guarantee that the associated address space
+  * will still exist later on and mmget_not_zero() has to be used before
+  * accessing it.
+  *
+  * This is a preferred way to to pin @mm for a longer/unbounded amount
+  * of time.
+  *
+  * Use mmdrop() to release the reference acquired by mmgrab().
+  *
+  * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
+  * of &mm_struct.mm_count vs &mm_struct.mm_users.
+  */
+ static inline void mmgrab(struct mm_struct *mm)
+ {
+ 	atomic_inc(&mm->mm_count);
+ }
+ 
+ /* mmdrop drops the mm and the page tables */
+ extern void __mmdrop(struct mm_struct *);
+ static inline void mmdrop(struct mm_struct *mm)
+ {
+ 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
+ 		__mmdrop(mm);
+ }
+ 
+ static inline void mmdrop_async_fn(struct work_struct *work)
+ {
+ 	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
+ 	__mmdrop(mm);
+ }
+ 
+ static inline void mmdrop_async(struct mm_struct *mm)
+ {
+ 	if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
+ 		INIT_WORK(&mm->async_put_work, mmdrop_async_fn);
+ 		schedule_work(&mm->async_put_work);
+ 	}
+ }
+ 
+ /**
+  * mmget() - Pin the address space associated with a &struct mm_struct.
+  * @mm: The address space to pin.
+  *
+  * Make sure that the address space of the given &struct mm_struct doesn't
+  * go away. This does not protect against parts of the address space being
+  * modified or freed, however.
+  *
+  * Never use this function to pin this address space for an
+  * unbounded/indefinite amount of time.
+  *
+  * Use mmput() to release the reference acquired by mmget().
+  *
+  * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
+  * of &mm_struct.mm_count vs &mm_struct.mm_users.
+  */
+ static inline void mmget(struct mm_struct *mm)
+ {
+ 	atomic_inc(&mm->mm_users);
+ }
+ 
+ static inline bool mmget_not_zero(struct mm_struct *mm)
+ {
+ 	return atomic_inc_not_zero(&mm->mm_users);
+ }
+ 
+ /* mmput gets rid of the mappings and all user-space */
+ extern void mmput(struct mm_struct *);
+ #ifdef CONFIG_MMU
+ /* same as above but performs the slow path from the async context. Can
+  * be called from the atomic context as well
+  */
+ void mmput_async(struct mm_struct *);
+ #endif
+ 
+ /* Grab a reference to a task's mm, if it is not already going away */
+ extern struct mm_struct *get_task_mm(struct task_struct *task);
+ /*
+  * Grab a reference to a task's mm, if it is not already going away
+  * and ptrace_may_access with the mode parameter passed to it
+  * succeeds.
+  */
+ extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
+ /* Remove the current tasks stale references to the old mm_struct */
+ extern void mm_release(struct task_struct *, struct mm_struct *);
+ 
+ #ifdef CONFIG_MEMCG
+ extern void mm_update_next_owner(struct mm_struct *mm);
+ #else
+ static inline void mm_update_next_owner(struct mm_struct *mm)
+ {
+ }
+ #endif /* CONFIG_MEMCG */
+ 
+ #ifdef CONFIG_MMU
+ extern void arch_pick_mmap_layout(struct mm_struct *mm);
+ extern unsigned long
+ arch_get_unmapped_area(struct file *, unsigned long, unsigned long,
+ 		       unsigned long, unsigned long);
+ extern unsigned long
+ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
+ 			  unsigned long len, unsigned long pgoff,
+ 			  unsigned long flags);
+ #else
+ static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
+ #endif
+ 
+ static inline bool in_vfork(struct task_struct *tsk)
+ {
+ 	bool ret;
+ 
+ 	/*
+ 	 * need RCU to access ->real_parent if CLONE_VM was used along with
+ 	 * CLONE_PARENT.
+ 	 *
+ 	 * We check real_parent->mm == tsk->mm because CLONE_VFORK does not
+ 	 * imply CLONE_VM
+ 	 *
+ 	 * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus
+ 	 * ->real_parent is not necessarily the task doing vfork(), so in
+ 	 * theory we can't rely on task_lock() if we want to dereference it.
+ 	 *
+ 	 * And in this case we can't trust the real_parent->mm == tsk->mm
+ 	 * check, it can be false negative. But we do not care, if init or
+ 	 * another oom-unkillable task does this it should blame itself.
+ 	 */
+ 	rcu_read_lock();
+ 	ret = tsk->vfork_done && tsk->real_parent->mm == tsk->mm;
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Applies per-task gfp context to the given allocation flags.
+  * PF_MEMALLOC_NOIO implies GFP_NOIO
+  * PF_MEMALLOC_NOFS implies GFP_NOFS
+  */
+ static inline gfp_t current_gfp_context(gfp_t flags)
+ {
+ 	/*
+ 	 * NOIO implies both NOIO and NOFS and it is a weaker context
+ 	 * so always make sure it makes precendence
+ 	 */
+ 	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
+ 		flags &= ~(__GFP_IO | __GFP_FS);
+ 	else if (unlikely(current->flags & PF_MEMALLOC_NOFS))
+ 		flags &= ~__GFP_FS;
+ 	return flags;
+ }
+ 
+ #ifdef CONFIG_LOCKDEP
+ extern void fs_reclaim_acquire(gfp_t gfp_mask);
+ extern void fs_reclaim_release(gfp_t gfp_mask);
+ #else
+ static inline void fs_reclaim_acquire(gfp_t gfp_mask) { }
+ static inline void fs_reclaim_release(gfp_t gfp_mask) { }
+ #endif
+ 
+ static inline unsigned int memalloc_noio_save(void)
+ {
+ 	unsigned int flags = current->flags & PF_MEMALLOC_NOIO;
+ 	current->flags |= PF_MEMALLOC_NOIO;
+ 	return flags;
+ }
+ 
+ static inline void memalloc_noio_restore(unsigned int flags)
+ {
+ 	current->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;
+ }
+ 
+ static inline unsigned int memalloc_nofs_save(void)
+ {
+ 	unsigned int flags = current->flags & PF_MEMALLOC_NOFS;
+ 	current->flags |= PF_MEMALLOC_NOFS;
+ 	return flags;
+ }
+ 
+ static inline void memalloc_nofs_restore(unsigned int flags)
+ {
+ 	current->flags = (current->flags & ~PF_MEMALLOC_NOFS) | flags;
+ }
+ 
+ static inline unsigned int memalloc_noreclaim_save(void)
+ {
+ 	unsigned int flags = current->flags & PF_MEMALLOC;
+ 	current->flags |= PF_MEMALLOC;
+ 	return flags;
+ }
+ 
+ static inline void memalloc_noreclaim_restore(unsigned int flags)
+ {
+ 	current->flags = (current->flags & ~PF_MEMALLOC) | flags;
+ }
+ 
+ #ifdef CONFIG_MEMBARRIER
+ enum {
+ 	MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY	= (1U << 0),
+ 	MEMBARRIER_STATE_PRIVATE_EXPEDITED		= (1U << 1),
+ };
+ 
+ #ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS
+ #include <asm/membarrier.h>
+ #endif
+ 
+ static inline void membarrier_execve(struct task_struct *t)
+ {
+ 	atomic_set(&t->mm->membarrier_state, 0);
+ }
+ #else
+ #ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS
+ static inline void membarrier_arch_switch_mm(struct mm_struct *prev,
+ 					     struct mm_struct *next,
+ 					     struct task_struct *tsk)
+ {
+ }
+ #endif
+ static inline void membarrier_execve(struct task_struct *t)
+ {
+ }
+ #endif
++>>>>>>> 3ccfebedd8cf (powerpc, membarrier: Skip memory barrier in switch_mm())
  
  #endif /* _LINUX_SCHED_MM_H */
diff --cc init/Kconfig
index 6ec689c4589d,837adcf075d9..000000000000
--- a/init/Kconfig
+++ b/init/Kconfig
@@@ -1453,16 -1412,8 +1453,21 @@@ config USERFAULTF
  	  Enable the userfaultfd() system call that allows to intercept and
  	  handle page faults in userland.
  
++<<<<<<< HEAD
 +	  If unsure, say Y.
 +
 +config PCI_QUIRKS
 +	default y
 +	bool "Enable PCI quirk workarounds" if EXPERT
 +	depends on PCI
 +	help
 +	  This enables workarounds for various PCI chipset
 +	  bugs/quirks. Disable this only if your target machine is
 +	  unaffected by PCI quirks.
++=======
+ config ARCH_HAS_MEMBARRIER_CALLBACKS
+ 	bool
++>>>>>>> 3ccfebedd8cf (powerpc, membarrier: Skip memory barrier in switch_mm())
  
  config EMBEDDED
  	bool "Embedded system"
diff --cc kernel/sched/core.c
index 1a5e18b224eb,ead0c2135d47..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -2394,9 -2697,9 +2394,14 @@@ static void finish_task_switch(struct r
  	 */
  	prev_state = prev->state;
  	vtime_task_switch(prev);
 +	finish_arch_switch(prev);
  	perf_event_task_sched_in(prev, current);
++<<<<<<< HEAD
 +	finish_lock_switch(rq, prev);
++=======
+ 	finish_task(prev);
+ 	finish_lock_switch(rq);
++>>>>>>> 3ccfebedd8cf (powerpc, membarrier: Skip memory barrier in switch_mm())
  	finish_arch_post_lock_switch();
  
  	fire_sched_in_preempt_notifiers(current);
* Unmerged path arch/powerpc/mm/mmu_context.c
* Unmerged path kernel/sched/membarrier.c
* Unmerged path MAINTAINERS
* Unmerged path arch/powerpc/Kconfig
diff --git a/arch/powerpc/include/asm/membarrier.h b/arch/powerpc/include/asm/membarrier.h
new file mode 100644
index 000000000000..98ff4f1fcf2b
--- /dev/null
+++ b/arch/powerpc/include/asm/membarrier.h
@@ -0,0 +1,26 @@
+#ifndef _ASM_POWERPC_MEMBARRIER_H
+#define _ASM_POWERPC_MEMBARRIER_H
+
+static inline void membarrier_arch_switch_mm(struct mm_struct *prev,
+					     struct mm_struct *next,
+					     struct task_struct *tsk)
+{
+	/*
+	 * Only need the full barrier when switching between processes.
+	 * Barrier when switching from kernel to userspace is not
+	 * required here, given that it is implied by mmdrop(). Barrier
+	 * when switching from userspace to kernel is not needed after
+	 * store to rq->curr.
+	 */
+	if (likely(!(atomic_read(&next->membarrier_state) &
+		     MEMBARRIER_STATE_PRIVATE_EXPEDITED) || !prev))
+		return;
+
+	/*
+	 * The membarrier system call requires a full memory barrier
+	 * after storing to rq->curr, before going back to user-space.
+	 */
+	smp_mb();
+}
+
+#endif /* _ASM_POWERPC_MEMBARRIER_H */
* Unmerged path arch/powerpc/mm/mmu_context.c
* Unmerged path include/linux/sched/mm.h
* Unmerged path init/Kconfig
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/membarrier.c
