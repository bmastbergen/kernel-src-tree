iio: Replace printk in __iio_update_buffers with dev_dbg

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iio] Replace printk in __iio_update_buffers with dev_dbg (Tony Camuso) [1559170]
Rebuild_FUZZ: 95.33%
commit-author Lars-Peter Clausen <lars@metafoo.de>
commit 63223c5f5c11b832586edca28cfc7d2850bc3e44
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/63223c5f.failed

While more verbose error messages are useful for debugging we should really
not put those error messages into the kernel log for normal errors that are
already reported to the application via the error code, when running in
non-debug mode.

Otherwise application authors might expect that this is part of the ABI and
to get the error they should scan the kernel log. Which would be rather
error prone itself since there is no direct mapping between a operation and
the error message so it is impossible to find out which error message
belongs to which error.

	Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
	Signed-off-by: Jonathan Cameron <jic23@kernel.org>
(cherry picked from commit 63223c5f5c11b832586edca28cfc7d2850bc3e44)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iio/industrialio-buffer.c
diff --cc drivers/iio/industrialio-buffer.c
index ed6b8aa675ce,1f91031aa460..000000000000
--- a/drivers/iio/industrialio-buffer.c
+++ b/drivers/iio/industrialio-buffer.c
@@@ -386,11 -441,446 +386,388 @@@ error_ret
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t iio_buffer_read_length(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 
+ 	return sprintf(buf, "%d\n", buffer->length);
+ }
+ 
+ static ssize_t iio_buffer_write_length(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf, size_t len)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 	unsigned int val;
+ 	int ret;
+ 
+ 	ret = kstrtouint(buf, 10, &val);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (val == buffer->length)
+ 		return len;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 	if (iio_buffer_is_active(indio_dev->buffer)) {
+ 		ret = -EBUSY;
+ 	} else {
+ 		buffer->access->set_length(buffer, val);
+ 		ret = 0;
+ 	}
+ 	if (ret)
+ 		goto out;
+ 	if (buffer->length && buffer->length < buffer->watermark)
+ 		buffer->watermark = buffer->length;
+ out:
+ 	mutex_unlock(&indio_dev->mlock);
+ 
+ 	return ret ? ret : len;
+ }
+ 
+ static ssize_t iio_buffer_show_enable(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	return sprintf(buf, "%d\n", iio_buffer_is_active(indio_dev->buffer));
+ }
+ 
+ static int iio_compute_scan_bytes(struct iio_dev *indio_dev,
+ 				const unsigned long *mask, bool timestamp)
+ {
+ 	const struct iio_chan_spec *ch;
+ 	unsigned bytes = 0;
+ 	int length, i;
+ 
+ 	/* How much space will the demuxed element take? */
+ 	for_each_set_bit(i, mask,
+ 			 indio_dev->masklength) {
+ 		ch = iio_find_channel_from_si(indio_dev, i);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	if (timestamp) {
+ 		ch = iio_find_channel_from_si(indio_dev,
+ 					      indio_dev->scan_index_timestamp);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	return bytes;
+ }
+ 
+ static void iio_buffer_activate(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	iio_buffer_get(buffer);
+ 	list_add(&buffer->buffer_list, &indio_dev->buffer_list);
+ }
+ 
+ static void iio_buffer_deactivate(struct iio_buffer *buffer)
+ {
+ 	list_del_init(&buffer->buffer_list);
+ 	wake_up_interruptible(&buffer->pollq);
+ 	iio_buffer_put(buffer);
+ }
+ 
+ void iio_disable_all_buffers(struct iio_dev *indio_dev)
+ {
+ 	struct iio_buffer *buffer, *_buffer;
+ 
+ 	if (list_empty(&indio_dev->buffer_list))
+ 		return;
+ 
+ 	if (indio_dev->setup_ops->predisable)
+ 		indio_dev->setup_ops->predisable(indio_dev);
+ 
+ 	list_for_each_entry_safe(buffer, _buffer,
+ 			&indio_dev->buffer_list, buffer_list)
+ 		iio_buffer_deactivate(buffer);
+ 
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 	if (indio_dev->setup_ops->postdisable)
+ 		indio_dev->setup_ops->postdisable(indio_dev);
+ 
+ 	if (indio_dev->available_scan_masks == NULL)
+ 		kfree(indio_dev->active_scan_mask);
+ }
+ 
+ static void iio_buffer_update_bytes_per_datum(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	unsigned int bytes;
+ 
+ 	if (!buffer->access->set_bytes_per_datum)
+ 		return;
+ 
+ 	bytes = iio_compute_scan_bytes(indio_dev, buffer->scan_mask,
+ 		buffer->scan_timestamp);
+ 
+ 	buffer->access->set_bytes_per_datum(buffer, bytes);
+ }
+ 
+ static int __iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 	int success = 0;
+ 	struct iio_buffer *buffer;
+ 	unsigned long *compound_mask;
+ 	const unsigned long *old_mask;
+ 
+ 	/* Wind down existing buffers - iff there are any */
+ 	if (!list_empty(&indio_dev->buffer_list)) {
+ 		if (indio_dev->setup_ops->predisable) {
+ 			ret = indio_dev->setup_ops->predisable(indio_dev);
+ 			if (ret)
+ 				return ret;
+ 		}
+ 		indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 		if (indio_dev->setup_ops->postdisable) {
+ 			ret = indio_dev->setup_ops->postdisable(indio_dev);
+ 			if (ret)
+ 				return ret;
+ 		}
+ 	}
+ 	/* Keep a copy of current setup to allow roll back */
+ 	old_mask = indio_dev->active_scan_mask;
+ 	if (!indio_dev->available_scan_masks)
+ 		indio_dev->active_scan_mask = NULL;
+ 
+ 	if (remove_buffer)
+ 		iio_buffer_deactivate(remove_buffer);
+ 	if (insert_buffer)
+ 		iio_buffer_activate(indio_dev, insert_buffer);
+ 
+ 	/* If no buffers in list, we are done */
+ 	if (list_empty(&indio_dev->buffer_list)) {
+ 		indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 		if (indio_dev->available_scan_masks == NULL)
+ 			kfree(old_mask);
+ 		return 0;
+ 	}
+ 
+ 	/* What scan mask do we actually have? */
+ 	compound_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),
+ 				sizeof(long), GFP_KERNEL);
+ 	if (compound_mask == NULL) {
+ 		if (indio_dev->available_scan_masks == NULL)
+ 			kfree(old_mask);
+ 		return -ENOMEM;
+ 	}
+ 	indio_dev->scan_timestamp = 0;
+ 
+ 	list_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {
+ 		bitmap_or(compound_mask, compound_mask, buffer->scan_mask,
+ 			  indio_dev->masklength);
+ 		indio_dev->scan_timestamp |= buffer->scan_timestamp;
+ 	}
+ 	if (indio_dev->available_scan_masks) {
+ 		indio_dev->active_scan_mask =
+ 			iio_scan_mask_match(indio_dev->available_scan_masks,
+ 					    indio_dev->masklength,
+ 					    compound_mask);
+ 		if (indio_dev->active_scan_mask == NULL) {
+ 			/*
+ 			 * Roll back.
+ 			 * Note can only occur when adding a buffer.
+ 			 */
+ 			iio_buffer_deactivate(insert_buffer);
+ 			if (old_mask) {
+ 				indio_dev->active_scan_mask = old_mask;
+ 				success = -EINVAL;
+ 			}
+ 			else {
+ 				kfree(compound_mask);
+ 				ret = -EINVAL;
+ 				return ret;
+ 			}
+ 		}
+ 	} else {
+ 		indio_dev->active_scan_mask = compound_mask;
+ 	}
+ 
+ 	iio_update_demux(indio_dev);
+ 
+ 	/* Wind up again */
+ 	if (indio_dev->setup_ops->preenable) {
+ 		ret = indio_dev->setup_ops->preenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer preenable failed (%d)\n", ret);
+ 			goto error_remove_inserted;
+ 		}
+ 	}
+ 	indio_dev->scan_bytes =
+ 		iio_compute_scan_bytes(indio_dev,
+ 				       indio_dev->active_scan_mask,
+ 				       indio_dev->scan_timestamp);
+ 	list_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {
+ 		iio_buffer_update_bytes_per_datum(indio_dev, buffer);
+ 		if (buffer->access->request_update) {
+ 			ret = buffer->access->request_update(buffer);
+ 			if (ret) {
+ 				dev_dbg(&indio_dev->dev,
+ 				       "Buffer not started: buffer parameter update failed (%d)\n", ret);
+ 				goto error_run_postdisable;
+ 			}
+ 		}
+ 	}
+ 	if (indio_dev->info->update_scan_mode) {
+ 		ret = indio_dev->info
+ 			->update_scan_mode(indio_dev,
+ 					   indio_dev->active_scan_mask);
+ 		if (ret < 0) {
+ 			dev_dbg(&indio_dev->dev,
+ 				"Buffer not started: update scan mode failed (%d)\n",
+ 				ret);
+ 			goto error_run_postdisable;
+ 		}
+ 	}
+ 	/* Definitely possible for devices to support both of these. */
+ 	if ((indio_dev->modes & INDIO_BUFFER_TRIGGERED) && indio_dev->trig) {
+ 		indio_dev->currentmode = INDIO_BUFFER_TRIGGERED;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_HARDWARE) {
+ 		indio_dev->currentmode = INDIO_BUFFER_HARDWARE;
+ 	} else if (indio_dev->modes & INDIO_BUFFER_SOFTWARE) {
+ 		indio_dev->currentmode = INDIO_BUFFER_SOFTWARE;
+ 	} else { /* Should never be reached */
+ 		/* Can only occur on first buffer */
+ 		if (indio_dev->modes & INDIO_BUFFER_TRIGGERED)
+ 			dev_dbg(&indio_dev->dev, "Buffer not started: no trigger\n");
+ 		ret = -EINVAL;
+ 		goto error_run_postdisable;
+ 	}
+ 
+ 	if (indio_dev->setup_ops->postenable) {
+ 		ret = indio_dev->setup_ops->postenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: postenable failed (%d)\n", ret);
+ 			indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 			if (indio_dev->setup_ops->postdisable)
+ 				indio_dev->setup_ops->postdisable(indio_dev);
+ 			goto error_disable_all_buffers;
+ 		}
+ 	}
+ 
+ 	if (indio_dev->available_scan_masks)
+ 		kfree(compound_mask);
+ 	else
+ 		kfree(old_mask);
+ 
+ 	return success;
+ 
+ error_disable_all_buffers:
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ error_run_postdisable:
+ 	if (indio_dev->setup_ops->postdisable)
+ 		indio_dev->setup_ops->postdisable(indio_dev);
+ error_remove_inserted:
+ 	if (insert_buffer)
+ 		iio_buffer_deactivate(insert_buffer);
+ 	indio_dev->active_scan_mask = old_mask;
+ 	kfree(compound_mask);
+ 	return ret;
+ }
+ 
+ int iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 
+ 	if (insert_buffer == remove_buffer)
+ 		return 0;
+ 
+ 	mutex_lock(&indio_dev->info_exist_lock);
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	if (insert_buffer && iio_buffer_is_active(insert_buffer))
+ 		insert_buffer = NULL;
+ 
+ 	if (remove_buffer && !iio_buffer_is_active(remove_buffer))
+ 		remove_buffer = NULL;
+ 
+ 	if (!insert_buffer && !remove_buffer) {
+ 		ret = 0;
+ 		goto out_unlock;
+ 	}
+ 
+ 	if (indio_dev->info == NULL) {
+ 		ret = -ENODEV;
+ 		goto out_unlock;
+ 	}
+ 
+ 	ret = __iio_update_buffers(indio_dev, insert_buffer, remove_buffer);
+ 
+ out_unlock:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	mutex_unlock(&indio_dev->info_exist_lock);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(iio_update_buffers);
+ 
+ static ssize_t iio_buffer_store_enable(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf,
+ 				       size_t len)
+ {
+ 	int ret;
+ 	bool requested_state;
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	bool inlist;
+ 
+ 	ret = strtobool(buf, &requested_state);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	/* Find out if it is in the list */
+ 	inlist = iio_buffer_is_active(indio_dev->buffer);
+ 	/* Already in desired state */
+ 	if (inlist == requested_state)
+ 		goto done;
+ 
+ 	if (requested_state)
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 indio_dev->buffer, NULL);
+ 	else
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 NULL, indio_dev->buffer);
+ 
+ 	if (ret < 0)
+ 		goto done;
+ done:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	return (ret < 0) ? ret : len;
+ }
+ 
++>>>>>>> 63223c5f5c11 (iio: Replace printk in __iio_update_buffers with dev_dbg)
  static const char * const iio_scan_elements_group_name = "scan_elements";
  
 -static ssize_t iio_buffer_show_watermark(struct device *dev,
 -					 struct device_attribute *attr,
 -					 char *buf)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -
 -	return sprintf(buf, "%u\n", buffer->watermark);
 -}
 -
 -static ssize_t iio_buffer_store_watermark(struct device *dev,
 -					  struct device_attribute *attr,
 -					  const char *buf,
 -					  size_t len)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -	unsigned int val;
 -	int ret;
 -
 -	ret = kstrtouint(buf, 10, &val);
 -	if (ret)
 -		return ret;
 -	if (!val)
 -		return -EINVAL;
 -
 -	mutex_lock(&indio_dev->mlock);
 -
 -	if (val > buffer->length) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	if (iio_buffer_is_active(indio_dev->buffer)) {
 -		ret = -EBUSY;
 -		goto out;
 -	}
 -
 -	buffer->watermark = val;
 -
 -	if (indio_dev->info->hwfifo_set_watermark)
 -		indio_dev->info->hwfifo_set_watermark(indio_dev, val);
 -out:
 -	mutex_unlock(&indio_dev->mlock);
 -
 -	return ret ? ret : len;
 -}
 -
 -static DEVICE_ATTR(length, S_IRUGO | S_IWUSR, iio_buffer_read_length,
 -		   iio_buffer_write_length);
 -static struct device_attribute dev_attr_length_ro = __ATTR(length,
 -	S_IRUGO, iio_buffer_read_length, NULL);
 -static DEVICE_ATTR(enable, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_enable, iio_buffer_store_enable);
 -static DEVICE_ATTR(watermark, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_watermark, iio_buffer_store_watermark);
 -
 -static struct attribute *iio_buffer_attrs[] = {
 -	&dev_attr_length.attr,
 -	&dev_attr_enable.attr,
 -	&dev_attr_watermark.attr,
 -};
 -
 -int iio_buffer_alloc_sysfs_and_mask(struct iio_dev *indio_dev)
 +int iio_buffer_register(struct iio_dev *indio_dev,
 +			const struct iio_chan_spec *channels,
 +			int num_channels)
  {
  	struct iio_dev_attr *p;
  	struct attribute **attr;
* Unmerged path drivers/iio/industrialio-buffer.c
