dm, dax: Make sure dm_dax_flush() is called if device supports it

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Vivek Goyal <vgoyal@redhat.com>
commit 273752c9ff03eb83856601b2a3458218bb949e46
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/273752c9.failed

Currently dm_dax_flush() is not being called, even if underlying dax
device supports write cache, because DAXDEV_WRITE_CACHE is not being
propagated up to the DM dax device.

If the underlying dax device supports write cache, set
DAXDEV_WRITE_CACHE on the DM dax device.  This will cause dm_dax_flush()
to be called.

Fixes: abebfbe2f7 ("dm: add ->flush() dax operation support")
	Signed-off-by: Vivek Goyal <vgoyal@redhat.com>
	Acked-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 273752c9ff03eb83856601b2a3458218bb949e46)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/super.c
#	drivers/md/dm-table.c
#	include/linux/dax.h
diff --cc drivers/dax/super.c
index 41efc362f7f7,938eb4868f7f..000000000000
--- a/drivers/dax/super.c
+++ b/drivers/dax/super.c
@@@ -184,6 -245,45 +184,48 @@@ long dax_direct_access(struct dax_devic
  }
  EXPORT_SYMBOL_GPL(dax_direct_access);
  
++<<<<<<< HEAD
++=======
+ size_t dax_copy_from_iter(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t bytes, struct iov_iter *i)
+ {
+ 	if (!dax_alive(dax_dev))
+ 		return 0;
+ 
+ 	return dax_dev->ops->copy_from_iter(dax_dev, pgoff, addr, bytes, i);
+ }
+ EXPORT_SYMBOL_GPL(dax_copy_from_iter);
+ 
+ void dax_flush(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t size)
+ {
+ 	if (!dax_alive(dax_dev))
+ 		return;
+ 
+ 	if (!test_bit(DAXDEV_WRITE_CACHE, &dax_dev->flags))
+ 		return;
+ 
+ 	if (dax_dev->ops->flush)
+ 		dax_dev->ops->flush(dax_dev, pgoff, addr, size);
+ }
+ EXPORT_SYMBOL_GPL(dax_flush);
+ 
+ void dax_write_cache(struct dax_device *dax_dev, bool wc)
+ {
+ 	if (wc)
+ 		set_bit(DAXDEV_WRITE_CACHE, &dax_dev->flags);
+ 	else
+ 		clear_bit(DAXDEV_WRITE_CACHE, &dax_dev->flags);
+ }
+ EXPORT_SYMBOL_GPL(dax_write_cache);
+ 
+ bool dax_write_cache_enabled(struct dax_device *dax_dev)
+ {
+ 	return test_bit(DAXDEV_WRITE_CACHE, &dax_dev->flags);
+ }
+ EXPORT_SYMBOL_GPL(dax_write_cache_enabled);
+ 
++>>>>>>> 273752c9ff03 (dm, dax: Make sure dm_dax_flush() is called if device supports it)
  bool dax_alive(struct dax_device *dax_dev)
  {
  	lockdep_assert_held(&dax_srcu);
diff --cc drivers/md/dm-table.c
index d9f3215786ed,28a4071cdf85..000000000000
--- a/drivers/md/dm-table.c
+++ b/drivers/md/dm-table.c
@@@ -1467,20 -1631,35 +1468,52 @@@ static bool dm_table_supports_flush(str
  	return false;
  }
  
++<<<<<<< HEAD
 +static bool dm_table_discard_zeroes_data(struct dm_table *t)
 +{
 +	struct dm_target *ti;
 +	unsigned i = 0;
 +
 +	/* Ensure that all targets supports discard_zeroes_data. */
 +	while (i < dm_table_get_num_targets(t)) {
 +		ti = dm_table_get_target(t, i++);
 +
 +		if (ti->discard_zeroes_data_unsupported)
 +			return false;
 +	}
 +
 +	return true;
++=======
+ static int device_dax_write_cache_enabled(struct dm_target *ti,
+ 					  struct dm_dev *dev, sector_t start,
+ 					  sector_t len, void *data)
+ {
+ 	struct dax_device *dax_dev = dev->dax_dev;
+ 
+ 	if (!dax_dev)
+ 		return false;
+ 
+ 	if (dax_write_cache_enabled(dax_dev))
+ 		return true;
+ 	return false;
+ }
+ 
+ static int dm_table_supports_dax_write_cache(struct dm_table *t)
+ {
+ 	struct dm_target *ti;
+ 	unsigned i;
+ 
+ 	for (i = 0; i < dm_table_get_num_targets(t); i++) {
+ 		ti = dm_table_get_target(t, i);
+ 
+ 		if (ti->type->iterate_devices &&
+ 		    ti->type->iterate_devices(ti,
+ 				device_dax_write_cache_enabled, NULL))
+ 			return true;
+ 	}
+ 
+ 	return false;
++>>>>>>> 273752c9ff03 (dm, dax: Make sure dm_dax_flush() is called if device supports it)
  }
  
  static int device_is_nonrot(struct dm_target *ti, struct dm_dev *dev,
@@@ -1594,29 -1804,22 +1627,32 @@@ void dm_table_set_restrictions(struct d
  	 * Copy table's limits to the DM device's request_queue
  	 */
  	q->limits = *limits;
 +	memcpy(limits_aux, limits->limits_aux, sizeof(struct queue_limits_aux));
 +	q->limits.limits_aux = limits_aux;
  
 -	if (!dm_table_supports_discards(t))
 +	if (!dm_table_supports_discards(t)) {
  		queue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);
 -	else
 +		/* Must also clear discard limits... */
 +		q->limits.max_discard_sectors = 0;
 +		q->limits.discard_granularity = 0;
 +		q->limits.discard_alignment = 0;
 +		q->limits.discard_misaligned = 0;
 +	} else
  		queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);
  
 -	if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {
 -		wc = true;
 -		if (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))
 -			fua = true;
 +	if (dm_table_supports_flush(t, REQ_FLUSH)) {
 +		flush |= REQ_FLUSH;
 +		if (dm_table_supports_flush(t, REQ_FUA))
 +			flush |= REQ_FUA;
  	}
 -	blk_queue_write_cache(q, wc, fua);
 +	blk_queue_flush(q, flush);
 +
 +	if (!dm_table_discard_zeroes_data(t))
 +		q->limits.discard_zeroes_data = 0;
  
+ 	if (dm_table_supports_dax_write_cache(t))
+ 		dax_write_cache(t->md->dax_dev, true);
+ 
  	/* Ensure that all underlying devices are non-rotational. */
  	if (dm_table_all_devices_attribute(t, device_is_nonrot))
  		queue_flag_set_unlocked(QUEUE_FLAG_NONROT, q);
diff --cc include/linux/dax.h
index b7b81d6cc271,df97b7af7e2c..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -6,6 -6,89 +6,92 @@@
  #include <linux/radix-tree.h>
  #include <asm/pgtable.h>
  
++<<<<<<< HEAD
++=======
+ struct iomap_ops;
+ struct dax_device;
+ struct dax_operations {
+ 	/*
+ 	 * direct_access: translate a device-relative
+ 	 * logical-page-offset into an absolute physical pfn. Return the
+ 	 * number of pages available for DAX at that pfn.
+ 	 */
+ 	long (*direct_access)(struct dax_device *, pgoff_t, long,
+ 			void **, pfn_t *);
+ 	/* copy_from_iter: required operation for fs-dax direct-i/o */
+ 	size_t (*copy_from_iter)(struct dax_device *, pgoff_t, void *, size_t,
+ 			struct iov_iter *);
+ 	/* flush: optional driver-specific cache management after writes */
+ 	void (*flush)(struct dax_device *, pgoff_t, void *, size_t);
+ };
+ 
+ extern struct attribute_group dax_attribute_group;
+ 
+ #if IS_ENABLED(CONFIG_DAX)
+ struct dax_device *dax_get_by_host(const char *host);
+ void put_dax(struct dax_device *dax_dev);
+ #else
+ static inline struct dax_device *dax_get_by_host(const char *host)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void put_dax(struct dax_device *dax_dev)
+ {
+ }
+ #endif
+ 
+ int bdev_dax_pgoff(struct block_device *, sector_t, size_t, pgoff_t *pgoff);
+ #if IS_ENABLED(CONFIG_FS_DAX)
+ int __bdev_dax_supported(struct super_block *sb, int blocksize);
+ static inline int bdev_dax_supported(struct super_block *sb, int blocksize)
+ {
+ 	return __bdev_dax_supported(sb, blocksize);
+ }
+ 
+ static inline struct dax_device *fs_dax_get_by_host(const char *host)
+ {
+ 	return dax_get_by_host(host);
+ }
+ 
+ static inline void fs_put_dax(struct dax_device *dax_dev)
+ {
+ 	put_dax(dax_dev);
+ }
+ 
+ #else
+ static inline int bdev_dax_supported(struct super_block *sb, int blocksize)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static inline struct dax_device *fs_dax_get_by_host(const char *host)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void fs_put_dax(struct dax_device *dax_dev)
+ {
+ }
+ #endif
+ 
+ int dax_read_lock(void);
+ void dax_read_unlock(int id);
+ struct dax_device *alloc_dax(void *private, const char *host,
+ 		const struct dax_operations *ops);
+ bool dax_alive(struct dax_device *dax_dev);
+ void kill_dax(struct dax_device *dax_dev);
+ void *dax_get_private(struct dax_device *dax_dev);
+ long dax_direct_access(struct dax_device *dax_dev, pgoff_t pgoff, long nr_pages,
+ 		void **kaddr, pfn_t *pfn);
+ size_t dax_copy_from_iter(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t bytes, struct iov_iter *i);
+ void dax_flush(struct dax_device *dax_dev, pgoff_t pgoff, void *addr,
+ 		size_t size);
+ void dax_write_cache(struct dax_device *dax_dev, bool wc);
+ bool dax_write_cache_enabled(struct dax_device *dax_dev);
+ 
++>>>>>>> 273752c9ff03 (dm, dax: Make sure dm_dax_flush() is called if device supports it)
  /*
   * We use lowest available bit in exceptional entry for locking, one bit for
   * the entry size (PMD) and two more to tell us if the entry is a huge zero
* Unmerged path drivers/dax/super.c
* Unmerged path drivers/md/dm-table.c
* Unmerged path include/linux/dax.h
