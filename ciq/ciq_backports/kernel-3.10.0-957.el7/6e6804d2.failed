perf/core: Simpify perf_event_groups_for_each()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 6e6804d2fa0eff6520f3a2b48ff52bcb9dc25a9d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6e6804d2.failed

The last argument is, and always must be, the same.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Mark Rutland <mark.rutland@arm.com>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Alexey Budankov <alexey.budankov@linux.intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: David Carrillo-Cisneros <davidcc@google.com>
	Cc: Dmitri Prokhorov <Dmitry.Prohorov@intel.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Kan Liang <kan.liang@intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Valery Cherepennikov <valery.cherepennikov@intel.com>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 6e6804d2fa0eff6520f3a2b48ff52bcb9dc25a9d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/events/core.c
diff --cc kernel/events/core.c
index e490cd411934,fc5dd072c194..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -1440,6 -1483,172 +1440,175 @@@ ctx_group_list(struct perf_event *event
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Helper function to initializes perf_event_group trees.
+  */
+ static void perf_event_groups_init(struct perf_event_groups *groups)
+ {
+ 	groups->tree = RB_ROOT;
+ 	groups->index = 0;
+ }
+ 
+ /*
+  * Compare function for event groups;
+  *
+  * Implements complex key that first sorts by CPU and then by virtual index
+  * which provides ordering when rotating groups for the same CPU.
+  */
+ static bool
+ perf_event_groups_less(struct perf_event *left, struct perf_event *right)
+ {
+ 	if (left->cpu < right->cpu)
+ 		return true;
+ 	if (left->cpu > right->cpu)
+ 		return false;
+ 
+ 	if (left->group_index < right->group_index)
+ 		return true;
+ 	if (left->group_index > right->group_index)
+ 		return false;
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Insert @event into @groups' tree; using {@event->cpu, ++@groups->index} for
+  * key (see perf_event_groups_less). This places it last inside the CPU
+  * subtree.
+  */
+ static void
+ perf_event_groups_insert(struct perf_event_groups *groups,
+ 			 struct perf_event *event)
+ {
+ 	struct perf_event *node_event;
+ 	struct rb_node *parent;
+ 	struct rb_node **node;
+ 
+ 	event->group_index = ++groups->index;
+ 
+ 	node = &groups->tree.rb_node;
+ 	parent = *node;
+ 
+ 	while (*node) {
+ 		parent = *node;
+ 		node_event = container_of(*node, struct perf_event, group_node);
+ 
+ 		if (perf_event_groups_less(event, node_event))
+ 			node = &parent->rb_left;
+ 		else
+ 			node = &parent->rb_right;
+ 	}
+ 
+ 	rb_link_node(&event->group_node, parent, node);
+ 	rb_insert_color(&event->group_node, &groups->tree);
+ }
+ 
+ /*
+  * Helper function to insert event into the pinned or flexible groups.
+  */
+ static void
+ add_event_to_groups(struct perf_event *event, struct perf_event_context *ctx)
+ {
+ 	struct perf_event_groups *groups;
+ 
+ 	groups = get_event_groups(event, ctx);
+ 	perf_event_groups_insert(groups, event);
+ }
+ 
+ /*
+  * Delete a group from a tree.
+  */
+ static void
+ perf_event_groups_delete(struct perf_event_groups *groups,
+ 			 struct perf_event *event)
+ {
+ 	WARN_ON_ONCE(RB_EMPTY_NODE(&event->group_node) ||
+ 		     RB_EMPTY_ROOT(&groups->tree));
+ 
+ 	rb_erase(&event->group_node, &groups->tree);
+ 	init_event_group(event);
+ }
+ 
+ /*
+  * Helper function to delete event from its groups.
+  */
+ static void
+ del_event_from_groups(struct perf_event *event, struct perf_event_context *ctx)
+ {
+ 	struct perf_event_groups *groups;
+ 
+ 	groups = get_event_groups(event, ctx);
+ 	perf_event_groups_delete(groups, event);
+ }
+ 
+ /*
+  * Get the leftmost event in the @cpu subtree.
+  */
+ static struct perf_event *
+ perf_event_groups_first(struct perf_event_groups *groups, int cpu)
+ {
+ 	struct perf_event *node_event = NULL, *match = NULL;
+ 	struct rb_node *node = groups->tree.rb_node;
+ 
+ 	while (node) {
+ 		node_event = container_of(node, struct perf_event, group_node);
+ 
+ 		if (cpu < node_event->cpu) {
+ 			node = node->rb_left;
+ 		} else if (cpu > node_event->cpu) {
+ 			node = node->rb_right;
+ 		} else {
+ 			match = node_event;
+ 			node = node->rb_left;
+ 		}
+ 	}
+ 
+ 	return match;
+ }
+ 
+ /*
+  * Like rb_entry_next_safe() for the @cpu subtree.
+  */
+ static struct perf_event *
+ perf_event_groups_next(struct perf_event *event)
+ {
+ 	struct perf_event *next;
+ 
+ 	next = rb_entry_safe(rb_next(&event->group_node), typeof(*event), group_node);
+ 	if (next && next->cpu == event->cpu)
+ 		return next;
+ 
+ 	return NULL;
+ }
+ 
+ /*
+  * Rotate the @cpu subtree.
+  *
+  * Re-insert the leftmost event at the tail of the subtree.
+  */
+ static void
+ perf_event_groups_rotate(struct perf_event_groups *groups, int cpu)
+ {
+ 	struct perf_event *event = perf_event_groups_first(groups, cpu);
+ 
+ 	if (event) {
+ 		perf_event_groups_delete(groups, event);
+ 		perf_event_groups_insert(groups, event);
+ 	}
+ }
+ 
+ /*
+  * Iterate through the whole groups tree.
+  */
+ #define perf_event_groups_for_each(event, groups)			\
+ 	for (event = rb_entry_safe(rb_first(&((groups)->tree)),		\
+ 				typeof(*event), group_node); event;	\
+ 		event = rb_entry_safe(rb_next(&event->group_node),	\
+ 				typeof(*event), group_node))
+ 
+ /*
++>>>>>>> 6e6804d2fa0e (perf/core: Simpify perf_event_groups_for_each())
   * Add a event from the lists for its context.
   * Must be called with ctx->mutex and ctx->lock held.
   */
@@@ -10456,7 -11345,7 +10625,11 @@@ static int perf_event_init_context(stru
  	 * We dont have to disable NMIs - we are only looking at
  	 * the list, not manipulating it:
  	 */
++<<<<<<< HEAD
 +	list_for_each_entry(event, &parent_ctx->pinned_groups, group_entry) {
++=======
+ 	perf_event_groups_for_each(event, &parent_ctx->pinned_groups) {
++>>>>>>> 6e6804d2fa0e (perf/core: Simpify perf_event_groups_for_each())
  		ret = inherit_task_group(event, parent, parent_ctx,
  					 child, ctxn, &inherited_all);
  		if (ret)
@@@ -10472,7 -11361,7 +10645,11 @@@
  	parent_ctx->rotate_disable = 1;
  	raw_spin_unlock_irqrestore(&parent_ctx->lock, flags);
  
++<<<<<<< HEAD
 +	list_for_each_entry(event, &parent_ctx->flexible_groups, group_entry) {
++=======
+ 	perf_event_groups_for_each(event, &parent_ctx->flexible_groups) {
++>>>>>>> 6e6804d2fa0e (perf/core: Simpify perf_event_groups_for_each())
  		ret = inherit_task_group(event, parent, parent_ctx,
  					 child, ctxn, &inherited_all);
  		if (ret)
* Unmerged path kernel/events/core.c
