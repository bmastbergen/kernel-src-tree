nvme-rdma: Fix command completion race at error recovery

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [nvme] rdma: Fix command completion race at error recovery (David Milburn) [1610641]
Rebuild_FUZZ: 95.33%
commit-author Israel Rukshin <israelr@mellanox.com>
commit c947657b15379505a9bba36a02005882b66abe57
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/c947657b.failed

The race is between completing the request at error recovery work and
rdma completions.  If we cancel the request before getting the good
rdma completion we get a NULL deref of the request MR at
nvme_rdma_process_nvme_rsp().

When Canceling the request we return its mr to the mr pool (set mr to
NULL) and also unmap its data.  Canceling the requests while the rdma
queues are active is not safe.  Because rdma queues are active and we
get good rdma completions that can use the mr pointer which may be NULL.
Completing the request too soon may lead also to performing DMA to/from
user buffers which might have been already unmapped.

The commit fixes the race by draining the QP before starting the abort
commands mechanism.

	Signed-off-by: Israel Rukshin <israelr@mellanox.com>
	Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit c947657b15379505a9bba36a02005882b66abe57)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index 215f7f62fdfb,2815749f4dfb..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -673,14 -659,218 +673,226 @@@ out_free_queues
  	return ret;
  }
  
 -static void nvme_rdma_free_tagset(struct nvme_ctrl *nctrl,
 -		struct blk_mq_tag_set *set)
 +static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl)
  {
++<<<<<<< HEAD
 +	nvme_rdma_free_qe(ctrl->queues[0].device->dev, &ctrl->async_event_sqe,
 +			sizeof(struct nvme_command), DMA_TO_DEVICE);
 +	nvme_rdma_stop_and_free_queue(&ctrl->queues[0]);
 +	blk_cleanup_queue(ctrl->ctrl.admin_q);
 +	blk_mq_free_tag_set(&ctrl->admin_tag_set);
 +	nvme_rdma_dev_put(ctrl->device);
++=======
+ 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ 
+ 	blk_mq_free_tag_set(set);
+ 	nvme_rdma_dev_put(ctrl->device);
+ }
+ 
+ static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
+ 		bool admin)
+ {
+ 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ 	struct blk_mq_tag_set *set;
+ 	int ret;
+ 
+ 	if (admin) {
+ 		set = &ctrl->admin_tag_set;
+ 		memset(set, 0, sizeof(*set));
+ 		set->ops = &nvme_rdma_admin_mq_ops;
+ 		set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+ 		set->reserved_tags = 2; /* connect + keep-alive */
+ 		set->numa_node = NUMA_NO_NODE;
+ 		set->cmd_size = sizeof(struct nvme_rdma_request) +
+ 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+ 		set->driver_data = ctrl;
+ 		set->nr_hw_queues = 1;
+ 		set->timeout = ADMIN_TIMEOUT;
+ 		set->flags = BLK_MQ_F_NO_SCHED;
+ 	} else {
+ 		set = &ctrl->tag_set;
+ 		memset(set, 0, sizeof(*set));
+ 		set->ops = &nvme_rdma_mq_ops;
+ 		set->queue_depth = nctrl->opts->queue_size;
+ 		set->reserved_tags = 1; /* fabric connect */
+ 		set->numa_node = NUMA_NO_NODE;
+ 		set->flags = BLK_MQ_F_SHOULD_MERGE;
+ 		set->cmd_size = sizeof(struct nvme_rdma_request) +
+ 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+ 		set->driver_data = ctrl;
+ 		set->nr_hw_queues = nctrl->queue_count - 1;
+ 		set->timeout = NVME_IO_TIMEOUT;
+ 	}
+ 
+ 	ret = blk_mq_alloc_tag_set(set);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/*
+ 	 * We need a reference on the device as long as the tag_set is alive,
+ 	 * as the MRs in the request structures need a valid ib_device.
+ 	 */
+ 	ret = nvme_rdma_dev_get(ctrl->device);
+ 	if (!ret) {
+ 		ret = -EINVAL;
+ 		goto out_free_tagset;
+ 	}
+ 
+ 	return set;
+ 
+ out_free_tagset:
+ 	blk_mq_free_tag_set(set);
+ out:
+ 	return ERR_PTR(ret);
+ }
+ 
+ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
+ 		bool remove)
+ {
+ 	if (remove) {
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.admin_tagset);
+ 	}
+ 	nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ 		sizeof(struct nvme_command), DMA_TO_DEVICE);
+ 	nvme_rdma_free_queue(&ctrl->queues[0]);
+ }
+ 
+ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
+ 		bool new)
+ {
+ 	int error;
+ 
+ 	error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ 	if (error)
+ 		return error;
+ 
+ 	ctrl->device = ctrl->queues[0].device;
+ 
+ 	ctrl->max_fr_pages = nvme_rdma_get_max_fr_pages(ctrl->device->dev);
+ 
+ 	error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ 			sizeof(struct nvme_command), DMA_TO_DEVICE);
+ 	if (error)
+ 		goto out_free_queue;
+ 
+ 	if (new) {
+ 		ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ 		if (IS_ERR(ctrl->ctrl.admin_tagset)) {
+ 			error = PTR_ERR(ctrl->ctrl.admin_tagset);
+ 			goto out_free_async_qe;
+ 		}
+ 
+ 		ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ 		if (IS_ERR(ctrl->ctrl.admin_q)) {
+ 			error = PTR_ERR(ctrl->ctrl.admin_q);
+ 			goto out_free_tagset;
+ 		}
+ 	}
+ 
+ 	error = nvme_rdma_start_queue(ctrl, 0);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	error = ctrl->ctrl.ops->reg_read64(&ctrl->ctrl, NVME_REG_CAP,
+ 			&ctrl->ctrl.cap);
+ 	if (error) {
+ 		dev_err(ctrl->ctrl.device,
+ 			"prop_get NVME_REG_CAP failed\n");
+ 		goto out_stop_queue;
+ 	}
+ 
+ 	ctrl->ctrl.sqsize =
+ 		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
+ 
+ 	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ 	if (error)
+ 		goto out_stop_queue;
+ 
+ 	ctrl->ctrl.max_hw_sectors =
+ 		(ctrl->max_fr_pages - 1) << (ilog2(SZ_4K) - 9);
+ 
+ 	error = nvme_init_identify(&ctrl->ctrl);
+ 	if (error)
+ 		goto out_stop_queue;
+ 
+ 	return 0;
+ 
+ out_stop_queue:
+ 	nvme_rdma_stop_queue(&ctrl->queues[0]);
+ out_cleanup_queue:
+ 	if (new)
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ out_free_tagset:
+ 	if (new)
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.admin_tagset);
+ out_free_async_qe:
+ 	nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ 		sizeof(struct nvme_command), DMA_TO_DEVICE);
+ out_free_queue:
+ 	nvme_rdma_free_queue(&ctrl->queues[0]);
+ 	return error;
+ }
+ 
+ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
+ 		bool remove)
+ {
+ 	if (remove) {
+ 		blk_cleanup_queue(ctrl->ctrl.connect_q);
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.tagset);
+ 	}
+ 	nvme_rdma_free_io_queues(ctrl);
+ }
+ 
+ static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
+ {
+ 	int ret;
+ 
+ 	ret = nvme_rdma_alloc_io_queues(ctrl);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (new) {
+ 		ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ 		if (IS_ERR(ctrl->ctrl.tagset)) {
+ 			ret = PTR_ERR(ctrl->ctrl.tagset);
+ 			goto out_free_io_queues;
+ 		}
+ 
+ 		ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ 		if (IS_ERR(ctrl->ctrl.connect_q)) {
+ 			ret = PTR_ERR(ctrl->ctrl.connect_q);
+ 			goto out_free_tag_set;
+ 		}
+ 	} else {
+ 		blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ 			ctrl->ctrl.queue_count - 1);
+ 	}
+ 
+ 	ret = nvme_rdma_start_io_queues(ctrl);
+ 	if (ret)
+ 		goto out_cleanup_connect_q;
+ 
+ 	return 0;
+ 
+ out_cleanup_connect_q:
+ 	if (new)
+ 		blk_cleanup_queue(ctrl->ctrl.connect_q);
+ out_free_tag_set:
+ 	if (new)
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.tagset);
+ out_free_io_queues:
+ 	nvme_rdma_free_io_queues(ctrl);
+ 	return ret;
+ }
+ 
+ static void nvme_rdma_stop_ctrl(struct nvme_ctrl *nctrl)
+ {
+ 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ 
+ 	cancel_work_sync(&ctrl->err_work);
+ 	cancel_delayed_work_sync(&ctrl->reconnect_work);
++>>>>>>> c947657b1537 (nvme-rdma: Fix command completion race at error recovery)
  }
  
  static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
@@@ -785,6 -944,9 +997,12 @@@ static void nvme_rdma_reconnect_ctrl_wo
  
  	return;
  
++<<<<<<< HEAD
++=======
+ destroy_admin:
+ 	nvme_rdma_stop_queue(&ctrl->queues[0]);
+ 	nvme_rdma_destroy_admin_queue(ctrl, false);
++>>>>>>> c947657b1537 (nvme-rdma: Fix command completion race at error recovery)
  requeue:
  	dev_info(ctrl->ctrl.device, "Failed reconnect attempt %d\n",
  			ctrl->ctrl.nr_reconnects);
@@@ -799,19 -960,19 +1017,30 @@@ static void nvme_rdma_error_recovery_wo
  
  	nvme_stop_keep_alive(&ctrl->ctrl);
  
 -	if (ctrl->ctrl.queue_count > 1) {
 +	for (i = 0; i < ctrl->ctrl.queue_count; i++)
 +		clear_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[i].flags);
 +
 +	if (ctrl->ctrl.queue_count > 1)
  		nvme_stop_queues(&ctrl->ctrl);
++<<<<<<< HEAD
 +	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
 +
 +	/* We must take care of fastfail/requeue all our inflight requests */
 +	if (ctrl->ctrl.queue_count > 1)
 +		blk_mq_tagset_busy_iter(&ctrl->tag_set,
 +					nvme_cancel_request, &ctrl->ctrl);
++=======
+ 		nvme_rdma_stop_io_queues(ctrl);
+ 		blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ 					nvme_cancel_request, &ctrl->ctrl);
+ 		nvme_rdma_destroy_io_queues(ctrl, false);
+ 	}
+ 
+ 	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ 	nvme_rdma_stop_queue(&ctrl->queues[0]);
++>>>>>>> c947657b1537 (nvme-rdma: Fix command completion race at error recovery)
  	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
  				nvme_cancel_request, &ctrl->ctrl);
 -	nvme_rdma_destroy_admin_queue(ctrl, false);
  
  	/*
  	 * queues are not a live anymore, so restart the queues to fail fast
@@@ -1570,179 -1731,32 +1799,185 @@@ static struct blk_mq_ops nvme_rdma_admi
  	.timeout	= nvme_rdma_timeout,
  };
  
 -static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 +static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl)
 +{
 +	int error;
 +
 +	error = nvme_rdma_init_queue(ctrl, 0, NVME_AQ_DEPTH);
 +	if (error)
 +		return error;
 +
 +	ctrl->device = ctrl->queues[0].device;
 +
 +	/*
 +	 * We need a reference on the device as long as the tag_set is alive,
 +	 * as the MRs in the request structures need a valid ib_device.
 +	 */
 +	error = -EINVAL;
 +	if (!nvme_rdma_dev_get(ctrl->device))
 +		goto out_free_queue;
 +
 +	ctrl->max_fr_pages = min_t(u32, NVME_RDMA_MAX_SEGMENTS,
 +		ctrl->device->dev->attrs.max_fast_reg_page_list_len);
 +
 +	memset(&ctrl->admin_tag_set, 0, sizeof(ctrl->admin_tag_set));
 +	ctrl->admin_tag_set.ops = &nvme_rdma_admin_mq_ops;
 +	ctrl->admin_tag_set.queue_depth = NVME_RDMA_AQ_BLKMQ_DEPTH;
 +	ctrl->admin_tag_set.reserved_tags = 2; /* connect + keep-alive */
 +	ctrl->admin_tag_set.numa_node = NUMA_NO_NODE;
 +	ctrl->admin_tag_set.cmd_size = sizeof(struct nvme_rdma_request) +
 +		SG_CHUNK_SIZE * sizeof(struct scatterlist);
 +	ctrl->admin_tag_set.driver_data = ctrl;
 +	ctrl->admin_tag_set.nr_hw_queues = 1;
 +	ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
 +
 +	error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
 +	if (error)
 +		goto out_put_dev;
 +
 +	ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
 +	if (IS_ERR(ctrl->ctrl.admin_q)) {
 +		error = PTR_ERR(ctrl->ctrl.admin_q);
 +		goto out_free_tagset;
 +	}
 +	ctrl->ctrl.admin_q->tail_queue = 1;
 +
 +	error = nvmf_connect_admin_queue(&ctrl->ctrl);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	set_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[0].flags);
 +
 +	error = nvmf_reg_read64(&ctrl->ctrl, NVME_REG_CAP,
 +			&ctrl->ctrl.cap);
 +	if (error) {
 +		dev_err(ctrl->ctrl.device,
 +			"prop_get NVME_REG_CAP failed\n");
 +		goto out_cleanup_queue;
 +	}
 +
 +	ctrl->ctrl.sqsize =
 +		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
 +
 +	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	ctrl->ctrl.max_hw_sectors =
 +		(ctrl->max_fr_pages - 1) << (ilog2(SZ_4K) - 9);
 +
 +	error = nvme_init_identify(&ctrl->ctrl);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	error = nvme_rdma_alloc_qe(ctrl->queues[0].device->dev,
 +			&ctrl->async_event_sqe, sizeof(struct nvme_command),
 +			DMA_TO_DEVICE);
 +	if (error)
 +		goto out_cleanup_queue;
 +
 +	return 0;
 +
 +out_cleanup_queue:
 +	blk_cleanup_queue(ctrl->ctrl.admin_q);
 +out_free_tagset:
 +	/* disconnect and drain the queue before freeing the tagset */
 +	nvme_rdma_stop_queue(&ctrl->queues[0]);
 +	blk_mq_free_tag_set(&ctrl->admin_tag_set);
 +out_put_dev:
 +	nvme_rdma_dev_put(ctrl->device);
 +out_free_queue:
 +	nvme_rdma_free_queue(&ctrl->queues[0]);
 +	return error;
 +}
 +
 +static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl)
  {
 +	cancel_work_sync(&ctrl->err_work);
 +	cancel_delayed_work_sync(&ctrl->reconnect_work);
 +
  	if (ctrl->ctrl.queue_count > 1) {
  		nvme_stop_queues(&ctrl->ctrl);
+ 		nvme_rdma_stop_io_queues(ctrl);
  		blk_mq_tagset_busy_iter(&ctrl->tag_set,
  					nvme_cancel_request, &ctrl->ctrl);
 -		nvme_rdma_destroy_io_queues(ctrl, shutdown);
 +		nvme_rdma_free_io_queues(ctrl);
  	}
  
 -	if (shutdown)
 +	if (test_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[0].flags))
  		nvme_shutdown_ctrl(&ctrl->ctrl);
 -	else
 -		nvme_disable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
  
++<<<<<<< HEAD
 +	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
++=======
+ 	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ 	nvme_rdma_stop_queue(&ctrl->queues[0]);
++>>>>>>> c947657b1537 (nvme-rdma: Fix command completion race at error recovery)
  	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
  				nvme_cancel_request, &ctrl->ctrl);
 -	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
 -	nvme_rdma_destroy_admin_queue(ctrl, shutdown);
 +	nvme_rdma_destroy_admin_queue(ctrl);
 +}
 +
 +static void __nvme_rdma_remove_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 +{
 +	nvme_stop_ctrl(&ctrl->ctrl);
 +	nvme_remove_namespaces(&ctrl->ctrl);
 +	if (shutdown)
 +		nvme_rdma_shutdown_ctrl(ctrl);
 +
 +	nvme_uninit_ctrl(&ctrl->ctrl);
 +	if (ctrl->ctrl.tagset) {
 +		blk_cleanup_queue(ctrl->ctrl.connect_q);
 +		blk_mq_free_tag_set(&ctrl->tag_set);
 +		nvme_rdma_dev_put(ctrl->device);
 +	}
 +
 +	nvme_put_ctrl(&ctrl->ctrl);
 +}
 +
 +static void nvme_rdma_del_ctrl_work(struct work_struct *work)
 +{
 +	struct nvme_rdma_ctrl *ctrl = container_of(work,
 +				struct nvme_rdma_ctrl, delete_work);
 +
 +	__nvme_rdma_remove_ctrl(ctrl, true);
 +}
 +
 +static int __nvme_rdma_del_ctrl(struct nvme_rdma_ctrl *ctrl)
 +{
 +	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_DELETING))
 +		return -EBUSY;
 +
 +	if (!queue_work(nvme_wq, &ctrl->delete_work))
 +		return -EBUSY;
 +
 +	return 0;
 +}
 +
 +static int nvme_rdma_del_ctrl(struct nvme_ctrl *nctrl)
 +{
 +	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
 +	int ret = 0;
 +
 +	/*
 +	 * Keep a reference until all work is flushed since
 +	 * __nvme_rdma_del_ctrl can free the ctrl mem
 +	 */
 +	if (!kref_get_unless_zero(&ctrl->ctrl.kref))
 +		return -EBUSY;
 +	ret = __nvme_rdma_del_ctrl(ctrl);
 +	if (!ret)
 +		flush_work(&ctrl->delete_work);
 +	nvme_put_ctrl(&ctrl->ctrl);
 +	return ret;
  }
  
 -static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
 +static void nvme_rdma_remove_ctrl_work(struct work_struct *work)
  {
 -	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
 +	struct nvme_rdma_ctrl *ctrl = container_of(work,
 +				struct nvme_rdma_ctrl, delete_work);
 +
 +	__nvme_rdma_remove_ctrl(ctrl, false);
  }
  
  static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
@@@ -2087,9 -2014,8 +2322,14 @@@ static struct nvme_ctrl *nvme_rdma_crea
  	return &ctrl->ctrl;
  
  out_remove_admin_queue:
++<<<<<<< HEAD
 +	nvme_rdma_destroy_admin_queue(ctrl);
 +out_kfree_queues:
 +	kfree(ctrl->queues);
++=======
+ 	nvme_rdma_stop_queue(&ctrl->queues[0]);
+ 	nvme_rdma_destroy_admin_queue(ctrl, true);
++>>>>>>> c947657b1537 (nvme-rdma: Fix command completion race at error recovery)
  out_uninit_ctrl:
  	nvme_uninit_ctrl(&ctrl->ctrl);
  	nvme_put_ctrl(&ctrl->ctrl);
* Unmerged path drivers/nvme/host/rdma.c
