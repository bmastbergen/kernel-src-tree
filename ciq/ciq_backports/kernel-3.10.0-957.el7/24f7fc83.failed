x86/bugs: Provide boot parameters for the spec_store_bypass_disable mitigation

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] bugs: Provide boot parameters for the spec_store_bypass_disable mitigation (Waiman Long) [1566905] {CVE-2018-3639}
Rebuild_FUZZ: 97.37%
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 24f7fc83b9204d20f878c57cb77d261ae825e033
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/24f7fc83.failed

Contemporary high performance processors use a common industry-wide
optimization known as "Speculative Store Bypass" in which loads from
addresses to which a recent store has occurred may (speculatively) see an
older value. Intel refers to this feature as "Memory Disambiguation" which
is part of their "Smart Memory Access" capability.

Memory Disambiguation can expose a cache side-channel attack against such
speculatively read values. An attacker can create exploit code that allows
them to read memory outside of a sandbox environment (for example,
malicious JavaScript in a web page), or to perform more complex attacks
against code running within the same privilege level, e.g. via the stack.

As a first step to mitigate against such attacks, provide two boot command
line control knobs:

 nospec_store_bypass_disable
 spec_store_bypass_disable=[off,auto,on]

By default affected x86 processors will power on with Speculative
Store Bypass enabled. Hence the provided kernel parameters are written
from the point of view of whether to enable a mitigation or not.
The parameters are as follows:

 - auto - Kernel detects whether your CPU model contains an implementation
	  of Speculative Store Bypass and picks the most appropriate
	  mitigation.

 - on   - disable Speculative Store Bypass
 - off  - enable Speculative Store Bypass

[ tglx: Reordered the checks so that the whole evaluation is not done
  	when the CPU does not support RDS ]

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Ingo Molnar <mingo@kernel.org>

(cherry picked from commit 24f7fc83b9204d20f878c57cb77d261ae825e033)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
diff --cc arch/x86/include/asm/cpufeatures.h
index 9284f1bf3f7a,58689ed969bb..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -190,79 -190,91 +190,85 @@@
   *
   * Reuse free bits when adding new feature flags!
   */
 -#define X86_FEATURE_RING3MWAIT		( 7*32+ 0) /* Ring 3 MONITOR/MWAIT instructions */
 -#define X86_FEATURE_CPUID_FAULT		( 7*32+ 1) /* Intel CPUID faulting */
 -#define X86_FEATURE_CPB			( 7*32+ 2) /* AMD Core Performance Boost */
 -#define X86_FEATURE_EPB			( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 -#define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
 -#define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
 -#define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
 -#define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID && CR4.PCIDE=1 */
 -
 -#define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
 -#define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 -#define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
 -#define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
 -#define X86_FEATURE_RETPOLINE		( 7*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* "" AMD Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 -#define X86_FEATURE_CDP_L2		( 7*32+15) /* Code and Data Prioritization L2 */
 -
 -#define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
 -#define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* "" Fill RSB on context switches */
 -#define X86_FEATURE_SEV			( 7*32+20) /* AMD Secure Encrypted Virtualization */
  
 +#define X86_FEATURE_RING3MWAIT	(7*32+ 0) /* Ring 3 MONITOR/MWAIT */
 +#define X86_FEATURE_CPB		(7*32+ 2) /* AMD Core Performance Boost */
 +#define X86_FEATURE_EPB		(7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 +#define X86_FEATURE_CAT_L3	(7*32+ 4) /* Cache Allocation Technology L3 */
 +#define X86_FEATURE_CAT_L2	(7*32+ 5) /* Cache Allocation Technology L2 */
 +#define X86_FEATURE_CDP_L3	(7*32+ 6) /* Code and Data Prioritization L3 */
 +
 +#define X86_FEATURE_HW_PSTATE	(7*32+ 8) /* AMD HW-PState */
 +#define X86_FEATURE_PROC_FEEDBACK (7*32+ 9) /* AMD ProcFeedbackInterface */
 +#define X86_FEATURE_SME		( 7*32+10) /* AMD Secure Memory Encryption */
 +#define X86_FEATURE_RETPOLINE_AMD (7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */
 +#define X86_FEATURE_INTEL_PPIN	( 7*32+14) /* Intel Processor Inventory Number */
 +#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */
 +
++<<<<<<< HEAD
 +#define X86_FEATURE_MBA		( 7*32+18) /* Memory Bandwidth Allocation */
 +#define X86_FEATURE_IBP_DISABLE ( 7*32+21) /* Old AMD Indirect Branch Predictor Disable */
++=======
+ #define X86_FEATURE_USE_IBPB		( 7*32+21) /* "" Indirect Branch Prediction Barrier enabled */
+ #define X86_FEATURE_USE_IBRS_FW		( 7*32+22) /* "" Use IBRS during runtime firmware calls */
+ #define X86_FEATURE_SPEC_STORE_BYPASS_DISABLE	( 7*32+23) /* "" Disable Speculative Store Bypass. */
++>>>>>>> 24f7fc83b920 (x86/bugs: Provide boot parameters for the spec_store_bypass_disable mitigation)
  
  /* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 -
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 -
 -
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 -
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 -
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (EDX), word 11 */
 -#define X86_FEATURE_CQM_LLC		(11*32+ 1) /* LLC QoS if 1 */
 -
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (EDX), word 12 */
 -#define X86_FEATURE_CQM_OCCUP_LLC	(12*32+ 0) /* LLC occupancy monitoring */
 -#define X86_FEATURE_CQM_MBM_TOTAL	(12*32+ 1) /* LLC Total MBM monitoring */
 -#define X86_FEATURE_CQM_MBM_LOCAL	(12*32+ 2) /* LLC Local MBM monitoring */
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
 +
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
 +
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
 +
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
 -#define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
 -#define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
 -#define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
  #define X86_FEATURE_IBPB		(13*32+12) /* Indirect Branch Prediction Barrier */
  #define X86_FEATURE_IBRS		(13*32+14) /* Indirect Branch Restricted Speculation */
  #define X86_FEATURE_STIBP		(13*32+15) /* Single Thread Indirect Branch Predictors */
diff --cc arch/x86/include/asm/nospec-branch.h
index d5b6abbaaa24,7b9eacfc03f3..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -169,27 -211,41 +169,48 @@@
  enum spectre_v2_mitigation {
  	SPECTRE_V2_NONE,
  	SPECTRE_V2_RETPOLINE_MINIMAL,
 -	SPECTRE_V2_RETPOLINE_MINIMAL_AMD,
 -	SPECTRE_V2_RETPOLINE_GENERIC,
 -	SPECTRE_V2_RETPOLINE_AMD,
 +	SPECTRE_V2_RETPOLINE_NO_IBPB,
 +	SPECTRE_V2_RETPOLINE_SKYLAKE,
 +	SPECTRE_V2_RETPOLINE_UNSAFE_MODULE,
 +	SPECTRE_V2_RETPOLINE,
 +	SPECTRE_V2_RETPOLINE_IBRS_USER,
  	SPECTRE_V2_IBRS,
 +	SPECTRE_V2_IBRS_ALWAYS,
 +	SPECTRE_V2_IBP_DISABLED,
  };
  
 -/*
 - * The Intel specification for the SPEC_CTRL MSR requires that we
 - * preserve any already set reserved bits at boot time (e.g. for
 - * future additions that this kernel is not currently aware of).
 - * We then set any additional mitigation bits that we want
 - * ourselves and always use this as the base for SPEC_CTRL.
 - * We also use this when handling guest entry/exit as below.
 - */
 -extern void x86_spec_ctrl_set(u64);
 -extern u64 x86_spec_ctrl_get_default(void);
 +void __spectre_v2_select_mitigation(void);
 +void spectre_v2_print_mitigation(void);
  
++<<<<<<< HEAD
 +static inline bool retp_compiler(void)
 +{
 +#ifdef RETPOLINE
 +	return true;
 +#else
 +	return false;
 +#endif
 +}
++=======
+ /*
+  * On VMENTER we must preserve whatever view of the SPEC_CTRL MSR
+  * the guest has, while on VMEXIT we restore the host view. This
+  * would be easier if SPEC_CTRL were architecturally maskable or
+  * shadowable for guests but this is not (currently) the case.
+  * Takes the guest view of SPEC_CTRL MSR as a parameter.
+  */
+ extern void x86_spec_ctrl_set_guest(u64);
+ extern void x86_spec_ctrl_restore_host(u64);
+ 
+ /* The Speculative Store Bypass disable variants */
+ enum ssb_mitigation {
+ 	SPEC_STORE_BYPASS_NONE,
+ 	SPEC_STORE_BYPASS_DISABLE,
+ };
+ 
+ extern char __indirect_thunk_start[];
+ extern char __indirect_thunk_end[];
++>>>>>>> 24f7fc83b920 (x86/bugs: Provide boot parameters for the spec_store_bypass_disable mitigation)
  
  /*
   * On VMEXIT we must ensure that no RSB predictions learned in the guest
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,2d1f1cb604aa..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -21,11 -23,18 +21,12 @@@
  #include <asm/paravirt.h>
  #include <asm/alternative.h>
  #include <asm/pgtable.h>
 -#include <asm/set_memory.h>
 -#include <asm/intel-family.h>
 +#include <asm/cacheflush.h>
 +#include <asm/spec_ctrl.h>
  
  static void __init spectre_v2_select_mitigation(void);
+ static void __init ssb_select_mitigation(void);
  
 -/*
 - * Our boot-time value of the SPEC_CTRL MSR. We read it once so that any
 - * writes to SPEC_CTRL contain whatever reserved bits have been set.
 - */
 -static u64 __ro_after_init x86_spec_ctrl_base;
 -
  void __init check_bugs(void)
  {
  	identify_boot_cpu();
@@@ -35,10 -44,22 +36,16 @@@
  		print_cpu_info(&boot_cpu_data);
  	}
  
 -	/*
 -	 * Read the SPEC_CTRL MSR to account for reserved bits which may
 -	 * have unknown values.
 -	 */
 -	if (boot_cpu_has(X86_FEATURE_IBRS))
 -		rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
 -
 -	/* Select the proper spectre mitigation before patching alternatives */
 +	spec_ctrl_init();
  	spectre_v2_select_mitigation();
 +	spec_ctrl_cpu_init();
  
+ 	/*
+ 	 * Select proper mitigation for any exposure to the Speculative Store
+ 	 * Bypass vulnerability.
+ 	 */
+ 	ssb_select_mitigation();
+ 
  #ifdef CONFIG_X86_32
  	/*
  	 * Check whether we are able to run this kernel safely on SMP.
@@@ -153,61 -290,206 +160,181 @@@ void __spectre_v2_select_mitigation(voi
  
  	case SPECTRE_V2_CMD_FORCE:
  	case SPECTRE_V2_CMD_AUTO:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_auto;
  		break;
 -	case SPECTRE_V2_CMD_RETPOLINE_AMD:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_amd;
 +
 +	case SPECTRE_V2_CMD_RETPOLINE:
 +		spec_ctrl_enable_retpoline();
 +		return;
 +
 +	case SPECTRE_V2_CMD_IBRS:
 +		if (spec_ctrl_force_enable_ibrs())
 +			return;
  		break;
 -	case SPECTRE_V2_CMD_RETPOLINE_GENERIC:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_generic;
 +
 +	case SPECTRE_V2_CMD_IBRS_ALWAYS:
 +		if (spec_ctrl_enable_ibrs_always() ||
 +		    spec_ctrl_force_enable_ibp_disabled())
 +			return;
  		break;
 -	case SPECTRE_V2_CMD_RETPOLINE:
 -		if (IS_ENABLED(CONFIG_RETPOLINE))
 -			goto retpoline_auto;
 +
 +	case SPECTRE_V2_CMD_RETPOLINE_IBRS_USER:
 +		if (spec_ctrl_enable_retpoline_ibrs_user())
 +			return;
  		break;
  	}
 -	pr_err("Spectre mitigation: kernel not compiled with retpoline; no mitigation available!");
 -	return;
 -
 -retpoline_auto:
 -	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {
 -	retpoline_amd:
 -		if (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {
 -			pr_err("Spectre mitigation: LFENCE not serializing, switching to generic retpoline\n");
 -			goto retpoline_generic;
 -		}
 -		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :
 -					 SPECTRE_V2_RETPOLINE_MINIMAL_AMD;
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 -	} else {
 -	retpoline_generic:
 -		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_GENERIC :
 -					 SPECTRE_V2_RETPOLINE_MINIMAL;
 -		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 -	}
  
 -	spectre_v2_enabled = mode;
 -	pr_info("%s\n", spectre_v2_strings[mode]);
 +	if (spec_ctrl_cond_enable_ibrs(full_retpoline))
 +		return;
  
 -	/*
 -	 * If neither SMEP nor PTI are available, there is a risk of
 -	 * hitting userspace addresses in the RSB after a context switch
 -	 * from a shallow call stack to a deeper one. To prevent this fill
 -	 * the entire RSB, even when using IBRS.
 -	 *
 -	 * Skylake era CPUs have a separate issue with *underflow* of the
 -	 * RSB, when they will predict 'ret' targets from the generic BTB.
 -	 * The proper mitigation for this is IBRS. If IBRS is not supported
 -	 * or deactivated in favour of retpolines the RSB fill on context
 -	 * switch is required.
 -	 */
 -	if ((!boot_cpu_has(X86_FEATURE_PTI) &&
 -	     !boot_cpu_has(X86_FEATURE_SMEP)) || is_skylake_era()) {
 -		setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 -		pr_info("Spectre v2 mitigation: Filling RSB on context switch\n");
 -	}
 +	if (spec_ctrl_cond_enable_ibp_disabled())
 +		return;
  
 -	/* Initialize Indirect Branch Prediction Barrier if supported */
 -	if (boot_cpu_has(X86_FEATURE_IBPB)) {
 -		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 -		pr_info("Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\n");
 -	}
 +	spec_ctrl_enable_retpoline();
 +}
  
 -	/*
 -	 * Retpoline means the kernel is safe because it has no indirect
 -	 * branches. But firmware isn't, so use IBRS to protect that.
 -	 */
 -	if (boot_cpu_has(X86_FEATURE_IBRS)) {
 -		setup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);
 -		pr_info("Enabling Restricted Speculation for firmware calls\n");
 -	}
 +void spectre_v2_print_mitigation(void)
 +{
 +
 +	pr_info("%s\n", spectre_v2_strings[spec_ctrl_get_mitigation()]);
 +}
 +
 +static void __init spectre_v2_select_mitigation(void)
 +{
 +	spectre_v2_cmd = spectre_v2_parse_cmdline();
 +	__spectre_v2_select_mitigation();
 +	spectre_v2_print_mitigation();
  }
  
+ #undef pr_fmt
+ #define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
+ 
+ static enum ssb_mitigation ssb_mode = SPEC_STORE_BYPASS_NONE;
+ 
+ /* The kernel command line selection */
+ enum ssb_mitigation_cmd {
+ 	SPEC_STORE_BYPASS_CMD_NONE,
+ 	SPEC_STORE_BYPASS_CMD_AUTO,
+ 	SPEC_STORE_BYPASS_CMD_ON,
+ };
+ 
+ static const char *ssb_strings[] = {
+ 	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
+ 	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled"
+ };
+ 
+ static const struct {
+ 	const char *option;
+ 	enum ssb_mitigation_cmd cmd;
+ } ssb_mitigation_options[] = {
+ 	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO }, /* Platform decides */
+ 	{ "on",		SPEC_STORE_BYPASS_CMD_ON },   /* Disable Speculative Store Bypass */
+ 	{ "off",	SPEC_STORE_BYPASS_CMD_NONE }, /* Don't touch Speculative Store Bypass */
+ };
+ 
+ static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
+ {
+ 	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
+ 		return SPEC_STORE_BYPASS_CMD_NONE;
+ 	} else {
+ 		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
+ 					  arg, sizeof(arg));
+ 		if (ret < 0)
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
+ 			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
+ 				continue;
+ 
+ 			cmd = ssb_mitigation_options[i].cmd;
+ 			break;
+ 		}
+ 
+ 		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
+ 			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 		}
+ 	}
+ 
+ 	return cmd;
+ }
+ 
+ static enum ssb_mitigation_cmd __init __ssb_select_mitigation(void)
+ {
+ 	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
+ 	enum ssb_mitigation_cmd cmd;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_RDS))
+ 		return mode;
+ 
+ 	cmd = ssb_parse_cmdline();
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
+ 	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
+ 	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
+ 		return mode;
+ 
+ 	switch (cmd) {
+ 	case SPEC_STORE_BYPASS_CMD_AUTO:
+ 	case SPEC_STORE_BYPASS_CMD_ON:
+ 		mode = SPEC_STORE_BYPASS_DISABLE;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_NONE:
+ 		break;
+ 	}
+ 
+ 	if (mode != SPEC_STORE_BYPASS_NONE)
+ 		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
+ 	return mode;
+ }
+ 
+ static void ssb_select_mitigation()
+ {
+ 	ssb_mode = __ssb_select_mitigation();
+ 
+ 	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		pr_info("%s\n", ssb_strings[ssb_mode]);
+ }
+ 
  #undef pr_fmt
  
  #ifdef CONFIG_SYSFS
 -
 -ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
++<<<<<<< HEAD
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
++=======
+ 
+ 	switch (bug) {
+ 	case X86_BUG_CPU_MELTDOWN:
+ 		if (boot_cpu_has(X86_FEATURE_PTI))
+ 			return sprintf(buf, "Mitigation: PTI\n");
+ 
+ 		break;
+ 
+ 	case X86_BUG_SPECTRE_V1:
+ 		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
+ 
+ 	case X86_BUG_SPECTRE_V2:
+ 		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
+ 			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
+ 			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
+ 			       spectre_v2_module_string());
+ 
+ 	case X86_BUG_SPEC_STORE_BYPASS:
+ 		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
+ 
+ 	default:
+ 		break;
+ 	}
+ 
++>>>>>>> 24f7fc83b920 (x86/bugs: Provide boot parameters for the spec_store_bypass_disable mitigation)
  	return sprintf(buf, "Vulnerable\n");
  }
  
diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt
index a0f1f1428af0..072eae3d4ada 100644
--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@ -2208,6 +2208,9 @@ bytes respectively. Such letter suffixes can also be entirely omitted.
 			allow data leaks with this option, which is equivalent
 			to spectre_v2=off.
 
+	nospec_store_bypass_disable
+			[HW] Disable all mitigations for the Speculative Store Bypass vulnerability
+
 	noxsave		[BUGS=X86] Disables x86 extended register state save
 			and restore using xsave. The kernel will fallback to
 			enabling legacy floating-point and sse state.
@@ -3222,6 +3225,36 @@ bytes respectively. Such letter suffixes can also be entirely omitted.
 			Not specifying this option is equivalent to
 			spectre_v2=auto.
 
+	spec_store_bypass_disable=
+			[HW] Control Speculative Store Bypass (SSB) Disable mitigation
+			(Speculative Store Bypass vulnerability)
+
+			Certain CPUs are vulnerable to an exploit against a
+			a common industry wide performance optimization known
+			as "Speculative Store Bypass" in which recent stores
+			to the same memory location may not be observed by
+			later loads during speculative execution. The idea
+			is that such stores are unlikely and that they can
+			be detected prior to instruction retirement at the
+			end of a particular speculation execution window.
+
+			In vulnerable processors, the speculatively forwarded
+			store can be used in a cache side channel attack, for
+			example to read memory to which the attacker does not
+			directly have access (e.g. inside sandboxed code).
+
+			This parameter controls whether the Speculative Store
+			Bypass optimization is used.
+
+			on     - Unconditionally disable Speculative Store Bypass
+			off    - Unconditionally enable Speculative Store Bypass
+			auto   - Kernel detects whether the CPU model contains an
+				 implementation of Speculative Store Bypass and
+				 picks the most appropriate mitigation
+
+			Not specifying this option is equivalent to
+			spec_store_bypass_disable=auto.
+
 	spia_io_base=	[HW,MTD]
 	spia_fio_base=
 	spia_pedr=
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
