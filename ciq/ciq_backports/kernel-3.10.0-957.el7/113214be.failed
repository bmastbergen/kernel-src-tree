bpf: refactor bpf_prog_get and type check into helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 113214be7f6c98dd6d0435e4765aea8dea91662c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/113214be.failed

Since bpf_prog_get() and program type check is used in a couple of places,
refactor this into a small helper function that we can make use of. Since
the non RO prog->aux part is not used in performance critical paths and a
program destruction via RCU is rather very unlikley when doing the put, we
shouldn't have an issue just doing the bpf_prog_get() + prog->type != type
check, but actually not taking the ref at all (due to being in fdget() /
fdput() section of the bpf fd) is even cleaner and makes the diff smaller
as well, so just go for that. Callsites are changed to make use of the new
helper where possible.

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 113214be7f6c98dd6d0435e4765aea8dea91662c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	kernel/bpf/syscall.c
#	net/core/filter.c
#	net/kcm/kcmsock.c
#	net/packet/af_packet.c
#	net/sched/act_bpf.c
#	net/sched/cls_bpf.c
diff --cc include/linux/bpf.h
index d4c1f9049ad3,b3336b4f5d04..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -97,20 -163,150 +97,155 @@@ struct bpf_prog_type_list 
  	enum bpf_prog_type type;
  };
  
 +void bpf_register_prog_type(struct bpf_prog_type_list *tl);
 +
 +struct bpf_prog;
 +
  struct bpf_prog_aux {
  	atomic_t refcnt;
 -	u32 used_map_cnt;
 -	u32 max_ctx_offset;
 -	const struct bpf_verifier_ops *ops;
 +	bool is_gpl_compatible;
 +	enum bpf_prog_type prog_type;
 +	struct bpf_verifier_ops *ops;
 +	u32 id;
  	struct bpf_map **used_maps;
 +	u32 used_map_cnt;
  	struct bpf_prog *prog;
 -	struct user_struct *user;
 -	union {
 -		struct work_struct work;
 -		struct rcu_head	rcu;
 -	};
 +	struct work_struct work;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_array {
+ 	struct bpf_map map;
+ 	u32 elem_size;
+ 	/* 'ownership' of prog_array is claimed by the first program that
+ 	 * is going to use this map or by the first program which FD is stored
+ 	 * in the map to make sure that all callers and callees have the same
+ 	 * prog_type and JITed flag
+ 	 */
+ 	enum bpf_prog_type owner_prog_type;
+ 	bool owner_jited;
+ 	union {
+ 		char value[0] __aligned(8);
+ 		void *ptrs[0] __aligned(8);
+ 		void __percpu *pptrs[0] __aligned(8);
+ 	};
+ };
+ 
+ #define MAX_TAIL_CALL_CNT 32
+ 
+ struct bpf_event_entry {
+ 	struct perf_event *event;
+ 	struct file *perf_file;
+ 	struct file *map_file;
+ 	struct rcu_head rcu;
+ };
+ 
+ u64 bpf_tail_call(u64 ctx, u64 r2, u64 index, u64 r4, u64 r5);
+ u64 bpf_get_stackid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
+ bool bpf_prog_array_compatible(struct bpf_array *array, const struct bpf_prog *fp);
+ 
+ const struct bpf_func_proto *bpf_get_trace_printk_proto(void);
+ const struct bpf_func_proto *bpf_get_event_output_proto(void);
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ DECLARE_PER_CPU(int, bpf_prog_active);
+ 
+ void bpf_register_prog_type(struct bpf_prog_type_list *tl);
+ void bpf_register_map_type(struct bpf_map_type_list *tl);
+ 
+ struct bpf_prog *bpf_prog_get(u32 ufd);
+ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type);
+ struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog);
+ void bpf_prog_put(struct bpf_prog *prog);
+ 
+ struct bpf_map *bpf_map_get_with_uref(u32 ufd);
+ struct bpf_map *__bpf_map_get(struct fd f);
+ struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref);
+ void bpf_map_put_with_uref(struct bpf_map *map);
+ void bpf_map_put(struct bpf_map *map);
+ int bpf_map_precharge_memlock(u32 pages);
+ 
+ extern int sysctl_unprivileged_bpf_disabled;
+ 
+ int bpf_map_new_fd(struct bpf_map *map);
+ int bpf_prog_new_fd(struct bpf_prog *prog);
+ 
+ int bpf_obj_pin_user(u32 ufd, const char __user *pathname);
+ int bpf_obj_get_user(const char __user *pathname);
+ 
+ int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,
+ 			   u64 flags);
+ int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,
+ 			    u64 flags);
+ 
+ int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value);
+ 
+ int bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				 void *key, void *value, u64 map_flags);
+ void bpf_fd_array_map_clear(struct bpf_map *map);
+ 
+ /* memcpy that is used with 8-byte aligned pointers, power-of-8 size and
+  * forced to use 'long' read/writes to try to atomically copy long counters.
+  * Best-effort only.  No barriers here, since it _will_ race with concurrent
+  * updates from BPF programs. Called from bpf syscall and mostly used with
+  * size 8 or 16 bytes, so ask compiler to inline it.
+  */
+ static inline void bpf_long_memcpy(void *dst, const void *src, u32 size)
+ {
+ 	const long *lsrc = src;
+ 	long *ldst = dst;
+ 
+ 	size /= sizeof(long);
+ 	while (size--)
+ 		*ldst++ = *lsrc++;
+ }
+ 
+ /* verify correctness of eBPF program */
+ int bpf_check(struct bpf_prog **fp, union bpf_attr *attr);
+ #else
+ static inline void bpf_register_prog_type(struct bpf_prog_type_list *tl)
+ {
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get(u32 ufd)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get_type(u32 ufd,
+ 						 enum bpf_prog_type type)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void bpf_prog_put(struct bpf_prog *prog)
+ {
+ }
+ #endif /* CONFIG_BPF_SYSCALL */
+ 
+ /* verifier prototypes for helper functions called from eBPF programs */
+ extern const struct bpf_func_proto bpf_map_lookup_elem_proto;
+ extern const struct bpf_func_proto bpf_map_update_elem_proto;
+ extern const struct bpf_func_proto bpf_map_delete_elem_proto;
+ 
+ extern const struct bpf_func_proto bpf_get_prandom_u32_proto;
+ extern const struct bpf_func_proto bpf_get_smp_processor_id_proto;
+ extern const struct bpf_func_proto bpf_tail_call_proto;
+ extern const struct bpf_func_proto bpf_ktime_get_ns_proto;
+ extern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;
+ extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
+ extern const struct bpf_func_proto bpf_get_current_comm_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_push_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_pop_proto;
+ extern const struct bpf_func_proto bpf_get_stackid_proto;
+ 
+ /* Shared helpers among cBPF and eBPF. */
+ void bpf_user_rnd_init_once(void);
+ u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
++>>>>>>> 113214be7f6c (bpf: refactor bpf_prog_get and type check into helper)
  #endif /* _LINUX_BPF_H */
diff --cc net/core/filter.c
index 060ed5f86613,76fee35da244..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -763,6 -1282,1291 +763,1294 @@@ int sk_attach_filter(struct sock_fprog 
  }
  EXPORT_SYMBOL_GPL(sk_attach_filter);
  
++<<<<<<< HEAD
++=======
+ int sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk)
+ {
+ 	struct bpf_prog *prog = __get_filter(fprog, sk);
+ 	int err;
+ 
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	err = __reuseport_attach_prog(prog, sk);
+ 	if (err < 0) {
+ 		__bpf_prog_release(prog);
+ 		return err;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static struct bpf_prog *__get_bpf(u32 ufd, struct sock *sk)
+ {
+ 	if (sock_flag(sk, SOCK_FILTER_LOCKED))
+ 		return ERR_PTR(-EPERM);
+ 
+ 	return bpf_prog_get_type(ufd, BPF_PROG_TYPE_SOCKET_FILTER);
+ }
+ 
+ int sk_attach_bpf(u32 ufd, struct sock *sk)
+ {
+ 	struct bpf_prog *prog = __get_bpf(ufd, sk);
+ 	int err;
+ 
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	err = __sk_attach_prog(prog, sk);
+ 	if (err < 0) {
+ 		bpf_prog_put(prog);
+ 		return err;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int sk_reuseport_attach_bpf(u32 ufd, struct sock *sk)
+ {
+ 	struct bpf_prog *prog = __get_bpf(ufd, sk);
+ 	int err;
+ 
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	err = __reuseport_attach_prog(prog, sk);
+ 	if (err < 0) {
+ 		bpf_prog_put(prog);
+ 		return err;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ struct bpf_scratchpad {
+ 	union {
+ 		__be32 diff[MAX_BPF_STACK / sizeof(__be32)];
+ 		u8     buff[MAX_BPF_STACK];
+ 	};
+ };
+ 
+ static DEFINE_PER_CPU(struct bpf_scratchpad, bpf_sp);
+ 
+ static inline int bpf_try_make_writable(struct sk_buff *skb,
+ 					unsigned int write_len)
+ {
+ 	int err;
+ 
+ 	if (!skb_cloned(skb))
+ 		return 0;
+ 	if (skb_clone_writable(skb, write_len))
+ 		return 0;
+ 	err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+ 	if (!err)
+ 		bpf_compute_data_end(skb);
+ 	return err;
+ }
+ 
+ static u64 bpf_skb_store_bytes(u64 r1, u64 r2, u64 r3, u64 r4, u64 flags)
+ {
+ 	struct bpf_scratchpad *sp = this_cpu_ptr(&bpf_sp);
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	int offset = (int) r2;
+ 	void *from = (void *) (long) r3;
+ 	unsigned int len = (unsigned int) r4;
+ 	void *ptr;
+ 
+ 	if (unlikely(flags & ~(BPF_F_RECOMPUTE_CSUM | BPF_F_INVALIDATE_HASH)))
+ 		return -EINVAL;
+ 
+ 	/* bpf verifier guarantees that:
+ 	 * 'from' pointer points to bpf program stack
+ 	 * 'len' bytes of it were initialized
+ 	 * 'len' > 0
+ 	 * 'skb' is a valid pointer to 'struct sk_buff'
+ 	 *
+ 	 * so check for invalid 'offset' and too large 'len'
+ 	 */
+ 	if (unlikely((u32) offset > 0xffff || len > sizeof(sp->buff)))
+ 		return -EFAULT;
+ 	if (unlikely(bpf_try_make_writable(skb, offset + len)))
+ 		return -EFAULT;
+ 
+ 	ptr = skb_header_pointer(skb, offset, len, sp->buff);
+ 	if (unlikely(!ptr))
+ 		return -EFAULT;
+ 
+ 	if (flags & BPF_F_RECOMPUTE_CSUM)
+ 		skb_postpull_rcsum(skb, ptr, len);
+ 
+ 	memcpy(ptr, from, len);
+ 
+ 	if (ptr == sp->buff)
+ 		/* skb_store_bits cannot return -EFAULT here */
+ 		skb_store_bits(skb, offset, ptr, len);
+ 
+ 	if (flags & BPF_F_RECOMPUTE_CSUM)
+ 		skb_postpush_rcsum(skb, ptr, len);
+ 	if (flags & BPF_F_INVALIDATE_HASH)
+ 		skb_clear_hash(skb);
+ 
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_store_bytes_proto = {
+ 	.func		= bpf_skb_store_bytes,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_PTR_TO_STACK,
+ 	.arg4_type	= ARG_CONST_STACK_SIZE,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_skb_load_bytes(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	const struct sk_buff *skb = (const struct sk_buff *)(unsigned long) r1;
+ 	int offset = (int) r2;
+ 	void *to = (void *)(unsigned long) r3;
+ 	unsigned int len = (unsigned int) r4;
+ 	void *ptr;
+ 
+ 	if (unlikely((u32) offset > 0xffff))
+ 		goto err_clear;
+ 
+ 	ptr = skb_header_pointer(skb, offset, len, to);
+ 	if (unlikely(!ptr))
+ 		goto err_clear;
+ 	if (ptr != to)
+ 		memcpy(to, ptr, len);
+ 
+ 	return 0;
+ err_clear:
+ 	memset(to, 0, len);
+ 	return -EFAULT;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_load_bytes_proto = {
+ 	.func		= bpf_skb_load_bytes,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_PTR_TO_RAW_STACK,
+ 	.arg4_type	= ARG_CONST_STACK_SIZE,
+ };
+ 
+ static u64 bpf_l3_csum_replace(u64 r1, u64 r2, u64 from, u64 to, u64 flags)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	int offset = (int) r2;
+ 	__sum16 sum, *ptr;
+ 
+ 	if (unlikely(flags & ~(BPF_F_HDR_FIELD_MASK)))
+ 		return -EINVAL;
+ 	if (unlikely((u32) offset > 0xffff))
+ 		return -EFAULT;
+ 	if (unlikely(bpf_try_make_writable(skb, offset + sizeof(sum))))
+ 		return -EFAULT;
+ 
+ 	ptr = skb_header_pointer(skb, offset, sizeof(sum), &sum);
+ 	if (unlikely(!ptr))
+ 		return -EFAULT;
+ 
+ 	switch (flags & BPF_F_HDR_FIELD_MASK) {
+ 	case 0:
+ 		if (unlikely(from != 0))
+ 			return -EINVAL;
+ 
+ 		csum_replace_by_diff(ptr, to);
+ 		break;
+ 	case 2:
+ 		csum_replace2(ptr, from, to);
+ 		break;
+ 	case 4:
+ 		csum_replace4(ptr, from, to);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (ptr == &sum)
+ 		/* skb_store_bits guaranteed to not return -EFAULT here */
+ 		skb_store_bits(skb, offset, ptr, sizeof(sum));
+ 
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_l3_csum_replace_proto = {
+ 	.func		= bpf_l3_csum_replace,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_ANYTHING,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_l4_csum_replace(u64 r1, u64 r2, u64 from, u64 to, u64 flags)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	bool is_pseudo = flags & BPF_F_PSEUDO_HDR;
+ 	bool is_mmzero = flags & BPF_F_MARK_MANGLED_0;
+ 	int offset = (int) r2;
+ 	__sum16 sum, *ptr;
+ 
+ 	if (unlikely(flags & ~(BPF_F_MARK_MANGLED_0 | BPF_F_PSEUDO_HDR |
+ 			       BPF_F_HDR_FIELD_MASK)))
+ 		return -EINVAL;
+ 	if (unlikely((u32) offset > 0xffff))
+ 		return -EFAULT;
+ 	if (unlikely(bpf_try_make_writable(skb, offset + sizeof(sum))))
+ 		return -EFAULT;
+ 
+ 	ptr = skb_header_pointer(skb, offset, sizeof(sum), &sum);
+ 	if (unlikely(!ptr))
+ 		return -EFAULT;
+ 	if (is_mmzero && !*ptr)
+ 		return 0;
+ 
+ 	switch (flags & BPF_F_HDR_FIELD_MASK) {
+ 	case 0:
+ 		if (unlikely(from != 0))
+ 			return -EINVAL;
+ 
+ 		inet_proto_csum_replace_by_diff(ptr, skb, to, is_pseudo);
+ 		break;
+ 	case 2:
+ 		inet_proto_csum_replace2(ptr, skb, from, to, is_pseudo);
+ 		break;
+ 	case 4:
+ 		inet_proto_csum_replace4(ptr, skb, from, to, is_pseudo);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (is_mmzero && !*ptr)
+ 		*ptr = CSUM_MANGLED_0;
+ 	if (ptr == &sum)
+ 		/* skb_store_bits guaranteed to not return -EFAULT here */
+ 		skb_store_bits(skb, offset, ptr, sizeof(sum));
+ 
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_l4_csum_replace_proto = {
+ 	.func		= bpf_l4_csum_replace,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_ANYTHING,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_csum_diff(u64 r1, u64 from_size, u64 r3, u64 to_size, u64 seed)
+ {
+ 	struct bpf_scratchpad *sp = this_cpu_ptr(&bpf_sp);
+ 	u64 diff_size = from_size + to_size;
+ 	__be32 *from = (__be32 *) (long) r1;
+ 	__be32 *to   = (__be32 *) (long) r3;
+ 	int i, j = 0;
+ 
+ 	/* This is quite flexible, some examples:
+ 	 *
+ 	 * from_size == 0, to_size > 0,  seed := csum --> pushing data
+ 	 * from_size > 0,  to_size == 0, seed := csum --> pulling data
+ 	 * from_size > 0,  to_size > 0,  seed := 0    --> diffing data
+ 	 *
+ 	 * Even for diffing, from_size and to_size don't need to be equal.
+ 	 */
+ 	if (unlikely(((from_size | to_size) & (sizeof(__be32) - 1)) ||
+ 		     diff_size > sizeof(sp->diff)))
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < from_size / sizeof(__be32); i++, j++)
+ 		sp->diff[j] = ~from[i];
+ 	for (i = 0; i <   to_size / sizeof(__be32); i++, j++)
+ 		sp->diff[j] = to[i];
+ 
+ 	return csum_partial(sp->diff, diff_size, seed);
+ }
+ 
+ static const struct bpf_func_proto bpf_csum_diff_proto = {
+ 	.func		= bpf_csum_diff,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_STACK,
+ 	.arg2_type	= ARG_CONST_STACK_SIZE_OR_ZERO,
+ 	.arg3_type	= ARG_PTR_TO_STACK,
+ 	.arg4_type	= ARG_CONST_STACK_SIZE_OR_ZERO,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ static inline int __bpf_rx_skb(struct net_device *dev, struct sk_buff *skb)
+ {
+ 	if (skb_at_tc_ingress(skb))
+ 		skb_postpush_rcsum(skb, skb_mac_header(skb), skb->mac_len);
+ 
+ 	return dev_forward_skb(dev, skb);
+ }
+ 
+ static inline int __bpf_tx_skb(struct net_device *dev, struct sk_buff *skb)
+ {
+ 	int ret;
+ 
+ 	if (unlikely(__this_cpu_read(xmit_recursion) > XMIT_RECURSION_LIMIT)) {
+ 		net_crit_ratelimited("bpf: recursion limit reached on datapath, buggy bpf program?\n");
+ 		kfree_skb(skb);
+ 		return -ENETDOWN;
+ 	}
+ 
+ 	skb->dev = dev;
+ 
+ 	__this_cpu_inc(xmit_recursion);
+ 	ret = dev_queue_xmit(skb);
+ 	__this_cpu_dec(xmit_recursion);
+ 
+ 	return ret;
+ }
+ 
+ static u64 bpf_clone_redirect(u64 r1, u64 ifindex, u64 flags, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	struct net_device *dev;
+ 
+ 	if (unlikely(flags & ~(BPF_F_INGRESS)))
+ 		return -EINVAL;
+ 
+ 	dev = dev_get_by_index_rcu(dev_net(skb->dev), ifindex);
+ 	if (unlikely(!dev))
+ 		return -EINVAL;
+ 
+ 	skb = skb_clone(skb, GFP_ATOMIC);
+ 	if (unlikely(!skb))
+ 		return -ENOMEM;
+ 
+ 	return flags & BPF_F_INGRESS ?
+ 	       __bpf_rx_skb(dev, skb) : __bpf_tx_skb(dev, skb);
+ }
+ 
+ static const struct bpf_func_proto bpf_clone_redirect_proto = {
+ 	.func           = bpf_clone_redirect,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_ANYTHING,
+ 	.arg3_type      = ARG_ANYTHING,
+ };
+ 
+ struct redirect_info {
+ 	u32 ifindex;
+ 	u32 flags;
+ };
+ 
+ static DEFINE_PER_CPU(struct redirect_info, redirect_info);
+ 
+ static u64 bpf_redirect(u64 ifindex, u64 flags, u64 r3, u64 r4, u64 r5)
+ {
+ 	struct redirect_info *ri = this_cpu_ptr(&redirect_info);
+ 
+ 	if (unlikely(flags & ~(BPF_F_INGRESS)))
+ 		return TC_ACT_SHOT;
+ 
+ 	ri->ifindex = ifindex;
+ 	ri->flags = flags;
+ 
+ 	return TC_ACT_REDIRECT;
+ }
+ 
+ int skb_do_redirect(struct sk_buff *skb)
+ {
+ 	struct redirect_info *ri = this_cpu_ptr(&redirect_info);
+ 	struct net_device *dev;
+ 
+ 	dev = dev_get_by_index_rcu(dev_net(skb->dev), ri->ifindex);
+ 	ri->ifindex = 0;
+ 	if (unlikely(!dev)) {
+ 		kfree_skb(skb);
+ 		return -EINVAL;
+ 	}
+ 
+ 	return ri->flags & BPF_F_INGRESS ?
+ 	       __bpf_rx_skb(dev, skb) : __bpf_tx_skb(dev, skb);
+ }
+ 
+ static const struct bpf_func_proto bpf_redirect_proto = {
+ 	.func           = bpf_redirect,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_ANYTHING,
+ 	.arg2_type      = ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_get_cgroup_classid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	return task_get_classid((struct sk_buff *) (unsigned long) r1);
+ }
+ 
+ static const struct bpf_func_proto bpf_get_cgroup_classid_proto = {
+ 	.func           = bpf_get_cgroup_classid,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ };
+ 
+ static u64 bpf_get_route_realm(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	return dst_tclassid((struct sk_buff *) (unsigned long) r1);
+ }
+ 
+ static const struct bpf_func_proto bpf_get_route_realm_proto = {
+ 	.func           = bpf_get_route_realm,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ };
+ 
+ static u64 bpf_skb_vlan_push(u64 r1, u64 r2, u64 vlan_tci, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	__be16 vlan_proto = (__force __be16) r2;
+ 	int ret;
+ 
+ 	if (unlikely(vlan_proto != htons(ETH_P_8021Q) &&
+ 		     vlan_proto != htons(ETH_P_8021AD)))
+ 		vlan_proto = htons(ETH_P_8021Q);
+ 
+ 	ret = skb_vlan_push(skb, vlan_proto, vlan_tci);
+ 	bpf_compute_data_end(skb);
+ 	return ret;
+ }
+ 
+ const struct bpf_func_proto bpf_skb_vlan_push_proto = {
+ 	.func           = bpf_skb_vlan_push,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_ANYTHING,
+ 	.arg3_type      = ARG_ANYTHING,
+ };
+ EXPORT_SYMBOL_GPL(bpf_skb_vlan_push_proto);
+ 
+ static u64 bpf_skb_vlan_pop(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	int ret;
+ 
+ 	ret = skb_vlan_pop(skb);
+ 	bpf_compute_data_end(skb);
+ 	return ret;
+ }
+ 
+ const struct bpf_func_proto bpf_skb_vlan_pop_proto = {
+ 	.func           = bpf_skb_vlan_pop,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ };
+ EXPORT_SYMBOL_GPL(bpf_skb_vlan_pop_proto);
+ 
+ static int bpf_skb_generic_push(struct sk_buff *skb, u32 off, u32 len)
+ {
+ 	/* Caller already did skb_cow() with len as headroom,
+ 	 * so no need to do it here.
+ 	 */
+ 	skb_push(skb, len);
+ 	memmove(skb->data, skb->data + len, off);
+ 	memset(skb->data + off, 0, len);
+ 
+ 	/* No skb_postpush_rcsum(skb, skb->data + off, len)
+ 	 * needed here as it does not change the skb->csum
+ 	 * result for checksum complete when summing over
+ 	 * zeroed blocks.
+ 	 */
+ 	return 0;
+ }
+ 
+ static int bpf_skb_generic_pop(struct sk_buff *skb, u32 off, u32 len)
+ {
+ 	/* skb_ensure_writable() is not needed here, as we're
+ 	 * already working on an uncloned skb.
+ 	 */
+ 	if (unlikely(!pskb_may_pull(skb, off + len)))
+ 		return -ENOMEM;
+ 
+ 	skb_postpull_rcsum(skb, skb->data + off, len);
+ 	memmove(skb->data + len, skb->data, off);
+ 	__skb_pull(skb, len);
+ 
+ 	return 0;
+ }
+ 
+ static int bpf_skb_net_hdr_push(struct sk_buff *skb, u32 off, u32 len)
+ {
+ 	bool trans_same = skb->transport_header == skb->network_header;
+ 	int ret;
+ 
+ 	/* There's no need for __skb_push()/__skb_pull() pair to
+ 	 * get to the start of the mac header as we're guaranteed
+ 	 * to always start from here under eBPF.
+ 	 */
+ 	ret = bpf_skb_generic_push(skb, off, len);
+ 	if (likely(!ret)) {
+ 		skb->mac_header -= len;
+ 		skb->network_header -= len;
+ 		if (trans_same)
+ 			skb->transport_header = skb->network_header;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int bpf_skb_net_hdr_pop(struct sk_buff *skb, u32 off, u32 len)
+ {
+ 	bool trans_same = skb->transport_header == skb->network_header;
+ 	int ret;
+ 
+ 	/* Same here, __skb_push()/__skb_pull() pair not needed. */
+ 	ret = bpf_skb_generic_pop(skb, off, len);
+ 	if (likely(!ret)) {
+ 		skb->mac_header += len;
+ 		skb->network_header += len;
+ 		if (trans_same)
+ 			skb->transport_header = skb->network_header;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int bpf_skb_proto_4_to_6(struct sk_buff *skb)
+ {
+ 	const u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);
+ 	u32 off = skb->network_header - skb->mac_header;
+ 	int ret;
+ 
+ 	ret = skb_cow(skb, len_diff);
+ 	if (unlikely(ret < 0))
+ 		return ret;
+ 
+ 	ret = bpf_skb_net_hdr_push(skb, off, len_diff);
+ 	if (unlikely(ret < 0))
+ 		return ret;
+ 
+ 	if (skb_is_gso(skb)) {
+ 		/* SKB_GSO_UDP stays as is. SKB_GSO_TCPV4 needs to
+ 		 * be changed into SKB_GSO_TCPV6.
+ 		 */
+ 		if (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {
+ 			skb_shinfo(skb)->gso_type &= ~SKB_GSO_TCPV4;
+ 			skb_shinfo(skb)->gso_type |=  SKB_GSO_TCPV6;
+ 		}
+ 
+ 		/* Due to IPv6 header, MSS needs to be downgraded. */
+ 		skb_shinfo(skb)->gso_size -= len_diff;
+ 		/* Header must be checked, and gso_segs recomputed. */
+ 		skb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;
+ 		skb_shinfo(skb)->gso_segs = 0;
+ 	}
+ 
+ 	skb->protocol = htons(ETH_P_IPV6);
+ 	skb_clear_hash(skb);
+ 
+ 	return 0;
+ }
+ 
+ static int bpf_skb_proto_6_to_4(struct sk_buff *skb)
+ {
+ 	const u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);
+ 	u32 off = skb->network_header - skb->mac_header;
+ 	int ret;
+ 
+ 	ret = skb_unclone(skb, GFP_ATOMIC);
+ 	if (unlikely(ret < 0))
+ 		return ret;
+ 
+ 	ret = bpf_skb_net_hdr_pop(skb, off, len_diff);
+ 	if (unlikely(ret < 0))
+ 		return ret;
+ 
+ 	if (skb_is_gso(skb)) {
+ 		/* SKB_GSO_UDP stays as is. SKB_GSO_TCPV6 needs to
+ 		 * be changed into SKB_GSO_TCPV4.
+ 		 */
+ 		if (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6) {
+ 			skb_shinfo(skb)->gso_type &= ~SKB_GSO_TCPV6;
+ 			skb_shinfo(skb)->gso_type |=  SKB_GSO_TCPV4;
+ 		}
+ 
+ 		/* Due to IPv4 header, MSS can be upgraded. */
+ 		skb_shinfo(skb)->gso_size += len_diff;
+ 		/* Header must be checked, and gso_segs recomputed. */
+ 		skb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;
+ 		skb_shinfo(skb)->gso_segs = 0;
+ 	}
+ 
+ 	skb->protocol = htons(ETH_P_IP);
+ 	skb_clear_hash(skb);
+ 
+ 	return 0;
+ }
+ 
+ static int bpf_skb_proto_xlat(struct sk_buff *skb, __be16 to_proto)
+ {
+ 	__be16 from_proto = skb->protocol;
+ 
+ 	if (from_proto == htons(ETH_P_IP) &&
+ 	      to_proto == htons(ETH_P_IPV6))
+ 		return bpf_skb_proto_4_to_6(skb);
+ 
+ 	if (from_proto == htons(ETH_P_IPV6) &&
+ 	      to_proto == htons(ETH_P_IP))
+ 		return bpf_skb_proto_6_to_4(skb);
+ 
+ 	return -ENOTSUPP;
+ }
+ 
+ static u64 bpf_skb_change_proto(u64 r1, u64 r2, u64 flags, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	__be16 proto = (__force __be16) r2;
+ 	int ret;
+ 
+ 	if (unlikely(flags))
+ 		return -EINVAL;
+ 
+ 	/* General idea is that this helper does the basic groundwork
+ 	 * needed for changing the protocol, and eBPF program fills the
+ 	 * rest through bpf_skb_store_bytes(), bpf_lX_csum_replace()
+ 	 * and other helpers, rather than passing a raw buffer here.
+ 	 *
+ 	 * The rationale is to keep this minimal and without a need to
+ 	 * deal with raw packet data. F.e. even if we would pass buffers
+ 	 * here, the program still needs to call the bpf_lX_csum_replace()
+ 	 * helpers anyway. Plus, this way we keep also separation of
+ 	 * concerns, since f.e. bpf_skb_store_bytes() should only take
+ 	 * care of stores.
+ 	 *
+ 	 * Currently, additional options and extension header space are
+ 	 * not supported, but flags register is reserved so we can adapt
+ 	 * that. For offloads, we mark packet as dodgy, so that headers
+ 	 * need to be verified first.
+ 	 */
+ 	ret = bpf_skb_proto_xlat(skb, proto);
+ 	bpf_compute_data_end(skb);
+ 	return ret;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_change_proto_proto = {
+ 	.func		= bpf_skb_change_proto,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_skb_change_type(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	u32 pkt_type = r2;
+ 
+ 	/* We only allow a restricted subset to be changed for now. */
+ 	if (unlikely(skb->pkt_type > PACKET_OTHERHOST ||
+ 		     pkt_type > PACKET_OTHERHOST))
+ 		return -EINVAL;
+ 
+ 	skb->pkt_type = pkt_type;
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_change_type_proto = {
+ 	.func		= bpf_skb_change_type,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ };
+ 
+ bool bpf_helper_changes_skb_data(void *func)
+ {
+ 	if (func == bpf_skb_vlan_push)
+ 		return true;
+ 	if (func == bpf_skb_vlan_pop)
+ 		return true;
+ 	if (func == bpf_skb_store_bytes)
+ 		return true;
+ 	if (func == bpf_skb_change_proto)
+ 		return true;
+ 	if (func == bpf_l3_csum_replace)
+ 		return true;
+ 	if (func == bpf_l4_csum_replace)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static unsigned short bpf_tunnel_key_af(u64 flags)
+ {
+ 	return flags & BPF_F_TUNINFO_IPV6 ? AF_INET6 : AF_INET;
+ }
+ 
+ static u64 bpf_skb_get_tunnel_key(u64 r1, u64 r2, u64 size, u64 flags, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	struct bpf_tunnel_key *to = (struct bpf_tunnel_key *) (long) r2;
+ 	const struct ip_tunnel_info *info = skb_tunnel_info(skb);
+ 	u8 compat[sizeof(struct bpf_tunnel_key)];
+ 	void *to_orig = to;
+ 	int err;
+ 
+ 	if (unlikely(!info || (flags & ~(BPF_F_TUNINFO_IPV6)))) {
+ 		err = -EINVAL;
+ 		goto err_clear;
+ 	}
+ 	if (ip_tunnel_info_af(info) != bpf_tunnel_key_af(flags)) {
+ 		err = -EPROTO;
+ 		goto err_clear;
+ 	}
+ 	if (unlikely(size != sizeof(struct bpf_tunnel_key))) {
+ 		err = -EINVAL;
+ 		switch (size) {
+ 		case offsetof(struct bpf_tunnel_key, tunnel_label):
+ 		case offsetof(struct bpf_tunnel_key, tunnel_ext):
+ 			goto set_compat;
+ 		case offsetof(struct bpf_tunnel_key, remote_ipv6[1]):
+ 			/* Fixup deprecated structure layouts here, so we have
+ 			 * a common path later on.
+ 			 */
+ 			if (ip_tunnel_info_af(info) != AF_INET)
+ 				goto err_clear;
+ set_compat:
+ 			to = (struct bpf_tunnel_key *)compat;
+ 			break;
+ 		default:
+ 			goto err_clear;
+ 		}
+ 	}
+ 
+ 	to->tunnel_id = be64_to_cpu(info->key.tun_id);
+ 	to->tunnel_tos = info->key.tos;
+ 	to->tunnel_ttl = info->key.ttl;
+ 
+ 	if (flags & BPF_F_TUNINFO_IPV6) {
+ 		memcpy(to->remote_ipv6, &info->key.u.ipv6.src,
+ 		       sizeof(to->remote_ipv6));
+ 		to->tunnel_label = be32_to_cpu(info->key.label);
+ 	} else {
+ 		to->remote_ipv4 = be32_to_cpu(info->key.u.ipv4.src);
+ 	}
+ 
+ 	if (unlikely(size != sizeof(struct bpf_tunnel_key)))
+ 		memcpy(to_orig, to, size);
+ 
+ 	return 0;
+ err_clear:
+ 	memset(to_orig, 0, size);
+ 	return err;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_get_tunnel_key_proto = {
+ 	.func		= bpf_skb_get_tunnel_key,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_RAW_STACK,
+ 	.arg3_type	= ARG_CONST_STACK_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_skb_get_tunnel_opt(u64 r1, u64 r2, u64 size, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	u8 *to = (u8 *) (long) r2;
+ 	const struct ip_tunnel_info *info = skb_tunnel_info(skb);
+ 	int err;
+ 
+ 	if (unlikely(!info ||
+ 		     !(info->key.tun_flags & TUNNEL_OPTIONS_PRESENT))) {
+ 		err = -ENOENT;
+ 		goto err_clear;
+ 	}
+ 	if (unlikely(size < info->options_len)) {
+ 		err = -ENOMEM;
+ 		goto err_clear;
+ 	}
+ 
+ 	ip_tunnel_info_opts_get(to, info);
+ 	if (size > info->options_len)
+ 		memset(to + info->options_len, 0, size - info->options_len);
+ 
+ 	return info->options_len;
+ err_clear:
+ 	memset(to, 0, size);
+ 	return err;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_get_tunnel_opt_proto = {
+ 	.func		= bpf_skb_get_tunnel_opt,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_RAW_STACK,
+ 	.arg3_type	= ARG_CONST_STACK_SIZE,
+ };
+ 
+ static struct metadata_dst __percpu *md_dst;
+ 
+ static u64 bpf_skb_set_tunnel_key(u64 r1, u64 r2, u64 size, u64 flags, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	struct bpf_tunnel_key *from = (struct bpf_tunnel_key *) (long) r2;
+ 	struct metadata_dst *md = this_cpu_ptr(md_dst);
+ 	u8 compat[sizeof(struct bpf_tunnel_key)];
+ 	struct ip_tunnel_info *info;
+ 
+ 	if (unlikely(flags & ~(BPF_F_TUNINFO_IPV6 | BPF_F_ZERO_CSUM_TX |
+ 			       BPF_F_DONT_FRAGMENT)))
+ 		return -EINVAL;
+ 	if (unlikely(size != sizeof(struct bpf_tunnel_key))) {
+ 		switch (size) {
+ 		case offsetof(struct bpf_tunnel_key, tunnel_label):
+ 		case offsetof(struct bpf_tunnel_key, tunnel_ext):
+ 		case offsetof(struct bpf_tunnel_key, remote_ipv6[1]):
+ 			/* Fixup deprecated structure layouts here, so we have
+ 			 * a common path later on.
+ 			 */
+ 			memcpy(compat, from, size);
+ 			memset(compat + size, 0, sizeof(compat) - size);
+ 			from = (struct bpf_tunnel_key *)compat;
+ 			break;
+ 		default:
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	if (unlikely((!(flags & BPF_F_TUNINFO_IPV6) && from->tunnel_label) ||
+ 		     from->tunnel_ext))
+ 		return -EINVAL;
+ 
+ 	skb_dst_drop(skb);
+ 	dst_hold((struct dst_entry *) md);
+ 	skb_dst_set(skb, (struct dst_entry *) md);
+ 
+ 	info = &md->u.tun_info;
+ 	info->mode = IP_TUNNEL_INFO_TX;
+ 
+ 	info->key.tun_flags = TUNNEL_KEY | TUNNEL_CSUM | TUNNEL_NOCACHE;
+ 	if (flags & BPF_F_DONT_FRAGMENT)
+ 		info->key.tun_flags |= TUNNEL_DONT_FRAGMENT;
+ 
+ 	info->key.tun_id = cpu_to_be64(from->tunnel_id);
+ 	info->key.tos = from->tunnel_tos;
+ 	info->key.ttl = from->tunnel_ttl;
+ 
+ 	if (flags & BPF_F_TUNINFO_IPV6) {
+ 		info->mode |= IP_TUNNEL_INFO_IPV6;
+ 		memcpy(&info->key.u.ipv6.dst, from->remote_ipv6,
+ 		       sizeof(from->remote_ipv6));
+ 		info->key.label = cpu_to_be32(from->tunnel_label) &
+ 				  IPV6_FLOWLABEL_MASK;
+ 	} else {
+ 		info->key.u.ipv4.dst = cpu_to_be32(from->remote_ipv4);
+ 		if (flags & BPF_F_ZERO_CSUM_TX)
+ 			info->key.tun_flags &= ~TUNNEL_CSUM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_set_tunnel_key_proto = {
+ 	.func		= bpf_skb_set_tunnel_key,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_STACK,
+ 	.arg3_type	= ARG_CONST_STACK_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_skb_set_tunnel_opt(u64 r1, u64 r2, u64 size, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	u8 *from = (u8 *) (long) r2;
+ 	struct ip_tunnel_info *info = skb_tunnel_info(skb);
+ 	const struct metadata_dst *md = this_cpu_ptr(md_dst);
+ 
+ 	if (unlikely(info != &md->u.tun_info || (size & (sizeof(u32) - 1))))
+ 		return -EINVAL;
+ 	if (unlikely(size > IP_TUNNEL_OPTS_MAX))
+ 		return -ENOMEM;
+ 
+ 	ip_tunnel_info_opts_set(info, from, size);
+ 
+ 	return 0;
+ }
+ 
+ static const struct bpf_func_proto bpf_skb_set_tunnel_opt_proto = {
+ 	.func		= bpf_skb_set_tunnel_opt,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_STACK,
+ 	.arg3_type	= ARG_CONST_STACK_SIZE,
+ };
+ 
+ static const struct bpf_func_proto *
+ bpf_get_skb_set_tunnel_proto(enum bpf_func_id which)
+ {
+ 	if (!md_dst) {
+ 		/* Race is not possible, since it's called from verifier
+ 		 * that is holding verifier mutex.
+ 		 */
+ 		md_dst = metadata_dst_alloc_percpu(IP_TUNNEL_OPTS_MAX,
+ 						   GFP_KERNEL);
+ 		if (!md_dst)
+ 			return NULL;
+ 	}
+ 
+ 	switch (which) {
+ 	case BPF_FUNC_skb_set_tunnel_key:
+ 		return &bpf_skb_set_tunnel_key_proto;
+ 	case BPF_FUNC_skb_set_tunnel_opt:
+ 		return &bpf_skb_set_tunnel_opt_proto;
+ 	default:
+ 		return NULL;
+ 	}
+ }
+ 
+ static const struct bpf_func_proto *
+ sk_filter_func_proto(enum bpf_func_id func_id)
+ {
+ 	switch (func_id) {
+ 	case BPF_FUNC_map_lookup_elem:
+ 		return &bpf_map_lookup_elem_proto;
+ 	case BPF_FUNC_map_update_elem:
+ 		return &bpf_map_update_elem_proto;
+ 	case BPF_FUNC_map_delete_elem:
+ 		return &bpf_map_delete_elem_proto;
+ 	case BPF_FUNC_get_prandom_u32:
+ 		return &bpf_get_prandom_u32_proto;
+ 	case BPF_FUNC_get_smp_processor_id:
+ 		return &bpf_get_raw_smp_processor_id_proto;
+ 	case BPF_FUNC_tail_call:
+ 		return &bpf_tail_call_proto;
+ 	case BPF_FUNC_ktime_get_ns:
+ 		return &bpf_ktime_get_ns_proto;
+ 	case BPF_FUNC_trace_printk:
+ 		if (capable(CAP_SYS_ADMIN))
+ 			return bpf_get_trace_printk_proto();
+ 	default:
+ 		return NULL;
+ 	}
+ }
+ 
+ static const struct bpf_func_proto *
+ tc_cls_act_func_proto(enum bpf_func_id func_id)
+ {
+ 	switch (func_id) {
+ 	case BPF_FUNC_skb_store_bytes:
+ 		return &bpf_skb_store_bytes_proto;
+ 	case BPF_FUNC_skb_load_bytes:
+ 		return &bpf_skb_load_bytes_proto;
+ 	case BPF_FUNC_csum_diff:
+ 		return &bpf_csum_diff_proto;
+ 	case BPF_FUNC_l3_csum_replace:
+ 		return &bpf_l3_csum_replace_proto;
+ 	case BPF_FUNC_l4_csum_replace:
+ 		return &bpf_l4_csum_replace_proto;
+ 	case BPF_FUNC_clone_redirect:
+ 		return &bpf_clone_redirect_proto;
+ 	case BPF_FUNC_get_cgroup_classid:
+ 		return &bpf_get_cgroup_classid_proto;
+ 	case BPF_FUNC_skb_vlan_push:
+ 		return &bpf_skb_vlan_push_proto;
+ 	case BPF_FUNC_skb_vlan_pop:
+ 		return &bpf_skb_vlan_pop_proto;
+ 	case BPF_FUNC_skb_change_proto:
+ 		return &bpf_skb_change_proto_proto;
+ 	case BPF_FUNC_skb_change_type:
+ 		return &bpf_skb_change_type_proto;
+ 	case BPF_FUNC_skb_get_tunnel_key:
+ 		return &bpf_skb_get_tunnel_key_proto;
+ 	case BPF_FUNC_skb_set_tunnel_key:
+ 		return bpf_get_skb_set_tunnel_proto(func_id);
+ 	case BPF_FUNC_skb_get_tunnel_opt:
+ 		return &bpf_skb_get_tunnel_opt_proto;
+ 	case BPF_FUNC_skb_set_tunnel_opt:
+ 		return bpf_get_skb_set_tunnel_proto(func_id);
+ 	case BPF_FUNC_redirect:
+ 		return &bpf_redirect_proto;
+ 	case BPF_FUNC_get_route_realm:
+ 		return &bpf_get_route_realm_proto;
+ 	case BPF_FUNC_perf_event_output:
+ 		return bpf_get_event_output_proto();
+ 	case BPF_FUNC_get_smp_processor_id:
+ 		return &bpf_get_smp_processor_id_proto;
+ 	default:
+ 		return sk_filter_func_proto(func_id);
+ 	}
+ }
+ 
+ static bool __is_valid_access(int off, int size, enum bpf_access_type type)
+ {
+ 	if (off < 0 || off >= sizeof(struct __sk_buff))
+ 		return false;
+ 	/* The verifier guarantees that size > 0. */
+ 	if (off % size != 0)
+ 		return false;
+ 	if (size != sizeof(__u32))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static bool sk_filter_is_valid_access(int off, int size,
+ 				      enum bpf_access_type type,
+ 				      enum bpf_reg_type *reg_type)
+ {
+ 	switch (off) {
+ 	case offsetof(struct __sk_buff, tc_classid):
+ 	case offsetof(struct __sk_buff, data):
+ 	case offsetof(struct __sk_buff, data_end):
+ 		return false;
+ 	}
+ 
+ 	if (type == BPF_WRITE) {
+ 		switch (off) {
+ 		case offsetof(struct __sk_buff, cb[0]) ...
+ 		     offsetof(struct __sk_buff, cb[4]):
+ 			break;
+ 		default:
+ 			return false;
+ 		}
+ 	}
+ 
+ 	return __is_valid_access(off, size, type);
+ }
+ 
+ static bool tc_cls_act_is_valid_access(int off, int size,
+ 				       enum bpf_access_type type,
+ 				       enum bpf_reg_type *reg_type)
+ {
+ 	if (type == BPF_WRITE) {
+ 		switch (off) {
+ 		case offsetof(struct __sk_buff, mark):
+ 		case offsetof(struct __sk_buff, tc_index):
+ 		case offsetof(struct __sk_buff, priority):
+ 		case offsetof(struct __sk_buff, cb[0]) ...
+ 		     offsetof(struct __sk_buff, cb[4]):
+ 		case offsetof(struct __sk_buff, tc_classid):
+ 			break;
+ 		default:
+ 			return false;
+ 		}
+ 	}
+ 
+ 	switch (off) {
+ 	case offsetof(struct __sk_buff, data):
+ 		*reg_type = PTR_TO_PACKET;
+ 		break;
+ 	case offsetof(struct __sk_buff, data_end):
+ 		*reg_type = PTR_TO_PACKET_END;
+ 		break;
+ 	}
+ 
+ 	return __is_valid_access(off, size, type);
+ }
+ 
+ static u32 bpf_net_convert_ctx_access(enum bpf_access_type type, int dst_reg,
+ 				      int src_reg, int ctx_off,
+ 				      struct bpf_insn *insn_buf,
+ 				      struct bpf_prog *prog)
+ {
+ 	struct bpf_insn *insn = insn_buf;
+ 
+ 	switch (ctx_off) {
+ 	case offsetof(struct __sk_buff, len):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, len));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, protocol):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, protocol));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, vlan_proto):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_proto) != 2);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, vlan_proto));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, priority):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, priority) != 4);
+ 
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_W, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, priority));
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, priority));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, ingress_ifindex):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, skb_iif) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, skb_iif));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, ifindex):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)),
+ 				      dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, dev));
+ 		*insn++ = BPF_JMP_IMM(BPF_JEQ, dst_reg, 0, 1);
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, dst_reg,
+ 				      offsetof(struct net_device, ifindex));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, hash):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, hash));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, mark):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
+ 
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_W, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, mark));
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, mark));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, pkt_type):
+ 		return convert_skb_access(SKF_AD_PKTTYPE, dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, queue_mapping):
+ 		return convert_skb_access(SKF_AD_QUEUE, dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, vlan_present):
+ 		return convert_skb_access(SKF_AD_VLAN_TAG_PRESENT,
+ 					  dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, vlan_tci):
+ 		return convert_skb_access(SKF_AD_VLAN_TAG,
+ 					  dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, cb[0]) ...
+ 		offsetof(struct __sk_buff, cb[4]):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct qdisc_skb_cb, data) < 20);
+ 
+ 		prog->cb_access = 1;
+ 		ctx_off -= offsetof(struct __sk_buff, cb[0]);
+ 		ctx_off += offsetof(struct sk_buff, cb);
+ 		ctx_off += offsetof(struct qdisc_skb_cb, data);
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_W, dst_reg, src_reg, ctx_off);
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg, ctx_off);
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, tc_classid):
+ 		ctx_off -= offsetof(struct __sk_buff, tc_classid);
+ 		ctx_off += offsetof(struct sk_buff, cb);
+ 		ctx_off += offsetof(struct qdisc_skb_cb, tc_classid);
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_H, dst_reg, src_reg, ctx_off);
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg, ctx_off);
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, data):
+ 		*insn++ = BPF_LDX_MEM(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, data)),
+ 				      dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, data));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, data_end):
+ 		ctx_off -= offsetof(struct __sk_buff, data_end);
+ 		ctx_off += offsetof(struct sk_buff, cb);
+ 		ctx_off += offsetof(struct bpf_skb_data_end, data_end);
+ 		*insn++ = BPF_LDX_MEM(bytes_to_bpf_size(sizeof(void *)),
+ 				      dst_reg, src_reg, ctx_off);
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, tc_index):
+ #ifdef CONFIG_NET_SCHED
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, tc_index) != 2);
+ 
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_H, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, tc_index));
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, tc_index));
+ 		break;
+ #else
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_MOV64_REG(dst_reg, dst_reg);
+ 		else
+ 			*insn++ = BPF_MOV64_IMM(dst_reg, 0);
+ 		break;
+ #endif
+ 	}
+ 
+ 	return insn - insn_buf;
+ }
+ 
+ static const struct bpf_verifier_ops sk_filter_ops = {
+ 	.get_func_proto		= sk_filter_func_proto,
+ 	.is_valid_access	= sk_filter_is_valid_access,
+ 	.convert_ctx_access	= bpf_net_convert_ctx_access,
+ };
+ 
+ static const struct bpf_verifier_ops tc_cls_act_ops = {
+ 	.get_func_proto		= tc_cls_act_func_proto,
+ 	.is_valid_access	= tc_cls_act_is_valid_access,
+ 	.convert_ctx_access	= bpf_net_convert_ctx_access,
+ };
+ 
+ static struct bpf_prog_type_list sk_filter_type __read_mostly = {
+ 	.ops	= &sk_filter_ops,
+ 	.type	= BPF_PROG_TYPE_SOCKET_FILTER,
+ };
+ 
+ static struct bpf_prog_type_list sched_cls_type __read_mostly = {
+ 	.ops	= &tc_cls_act_ops,
+ 	.type	= BPF_PROG_TYPE_SCHED_CLS,
+ };
+ 
+ static struct bpf_prog_type_list sched_act_type __read_mostly = {
+ 	.ops	= &tc_cls_act_ops,
+ 	.type	= BPF_PROG_TYPE_SCHED_ACT,
+ };
+ 
+ static int __init register_sk_filter_ops(void)
+ {
+ 	bpf_register_prog_type(&sk_filter_type);
+ 	bpf_register_prog_type(&sched_cls_type);
+ 	bpf_register_prog_type(&sched_act_type);
+ 
+ 	return 0;
+ }
+ late_initcall(register_sk_filter_ops);
+ 
++>>>>>>> 113214be7f6c (bpf: refactor bpf_prog_get and type check into helper)
  int sk_detach_filter(struct sock *sk)
  {
  	int ret = -ENOENT;
diff --cc net/packet/af_packet.c
index 133714a0ec2f,48b58957adf4..000000000000
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@@ -1357,12 -1517,105 +1357,109 @@@ static void __fanout_unlink(struct soc
  	spin_unlock(&f->lock);
  }
  
 -static bool match_fanout_group(struct packet_type *ptype, struct sock *sk)
 +static bool match_fanout_group(struct packet_type *ptype, struct sock * sk)
  {
 -	if (sk->sk_family != PF_PACKET)
 -		return false;
 +	if (ptype->af_packet_priv == (void*)((struct packet_sock *)sk)->fanout)
 +		return true;
  
++<<<<<<< HEAD
 +	return false;
++=======
+ 	return ptype->af_packet_priv == pkt_sk(sk)->fanout;
+ }
+ 
+ static void fanout_init_data(struct packet_fanout *f)
+ {
+ 	switch (f->type) {
+ 	case PACKET_FANOUT_LB:
+ 		atomic_set(&f->rr_cur, 0);
+ 		break;
+ 	case PACKET_FANOUT_CBPF:
+ 	case PACKET_FANOUT_EBPF:
+ 		RCU_INIT_POINTER(f->bpf_prog, NULL);
+ 		break;
+ 	}
+ }
+ 
+ static void __fanout_set_data_bpf(struct packet_fanout *f, struct bpf_prog *new)
+ {
+ 	struct bpf_prog *old;
+ 
+ 	spin_lock(&f->lock);
+ 	old = rcu_dereference_protected(f->bpf_prog, lockdep_is_held(&f->lock));
+ 	rcu_assign_pointer(f->bpf_prog, new);
+ 	spin_unlock(&f->lock);
+ 
+ 	if (old) {
+ 		synchronize_net();
+ 		bpf_prog_destroy(old);
+ 	}
+ }
+ 
+ static int fanout_set_data_cbpf(struct packet_sock *po, char __user *data,
+ 				unsigned int len)
+ {
+ 	struct bpf_prog *new;
+ 	struct sock_fprog fprog;
+ 	int ret;
+ 
+ 	if (sock_flag(&po->sk, SOCK_FILTER_LOCKED))
+ 		return -EPERM;
+ 	if (len != sizeof(fprog))
+ 		return -EINVAL;
+ 	if (copy_from_user(&fprog, data, len))
+ 		return -EFAULT;
+ 
+ 	ret = bpf_prog_create_from_user(&new, &fprog, NULL, false);
+ 	if (ret)
+ 		return ret;
+ 
+ 	__fanout_set_data_bpf(po->fanout, new);
+ 	return 0;
+ }
+ 
+ static int fanout_set_data_ebpf(struct packet_sock *po, char __user *data,
+ 				unsigned int len)
+ {
+ 	struct bpf_prog *new;
+ 	u32 fd;
+ 
+ 	if (sock_flag(&po->sk, SOCK_FILTER_LOCKED))
+ 		return -EPERM;
+ 	if (len != sizeof(fd))
+ 		return -EINVAL;
+ 	if (copy_from_user(&fd, data, len))
+ 		return -EFAULT;
+ 
+ 	new = bpf_prog_get_type(fd, BPF_PROG_TYPE_SOCKET_FILTER);
+ 	if (IS_ERR(new))
+ 		return PTR_ERR(new);
+ 
+ 	__fanout_set_data_bpf(po->fanout, new);
+ 	return 0;
+ }
+ 
+ static int fanout_set_data(struct packet_sock *po, char __user *data,
+ 			   unsigned int len)
+ {
+ 	switch (po->fanout->type) {
+ 	case PACKET_FANOUT_CBPF:
+ 		return fanout_set_data_cbpf(po, data, len);
+ 	case PACKET_FANOUT_EBPF:
+ 		return fanout_set_data_ebpf(po, data, len);
+ 	default:
+ 		return -EINVAL;
+ 	};
+ }
+ 
+ static void fanout_release_data(struct packet_fanout *f)
+ {
+ 	switch (f->type) {
+ 	case PACKET_FANOUT_CBPF:
+ 	case PACKET_FANOUT_EBPF:
+ 		__fanout_set_data_bpf(f, NULL);
+ 	};
++>>>>>>> 113214be7f6c (bpf: refactor bpf_prog_get and type check into helper)
  }
  
  static int fanout_add(struct sock *sk, u16 id, u16 type_flags)
diff --cc net/sched/cls_bpf.c
index c7a7c00a2b7c,c3002c2c68bb..000000000000
--- a/net/sched/cls_bpf.c
+++ b/net/sched/cls_bpf.c
@@@ -212,24 -246,103 +212,109 @@@ static int cls_bpf_set_parms(struct ne
  
  	memcpy(bpf_ops, nla_data(tb[TCA_BPF_OPS]), bpf_size);
  
 -	fprog_tmp.len = bpf_num_ops;
 -	fprog_tmp.filter = bpf_ops;
 +	tmp.len = bpf_len;
 +	tmp.filter = (struct sock_filter __user *) bpf_ops;
  
 -	ret = bpf_prog_create(&fp, &fprog_tmp);
 -	if (ret < 0) {
 -		kfree(bpf_ops);
 -		return ret;
 -	}
 +	ret = sk_unattached_filter_create(&fp, &tmp);
 +	if (ret)
 +		goto errout_free;
  
 +	prog->bpf_len = bpf_len;
  	prog->bpf_ops = bpf_ops;
 -	prog->bpf_num_ops = bpf_num_ops;
 -	prog->bpf_name = NULL;
  	prog->filter = fp;
 +	prog->res.classid = classid;
 +
 +	tcf_bind_filter(tp, &prog->res, base);
  
  	return 0;
++<<<<<<< HEAD
 +errout_free:
 +	kfree(bpf_ops);
 +	return ret;
++=======
+ }
+ 
+ static int cls_bpf_prog_from_efd(struct nlattr **tb, struct cls_bpf_prog *prog,
+ 				 const struct tcf_proto *tp)
+ {
+ 	struct bpf_prog *fp;
+ 	char *name = NULL;
+ 	u32 bpf_fd;
+ 
+ 	bpf_fd = nla_get_u32(tb[TCA_BPF_FD]);
+ 
+ 	fp = bpf_prog_get_type(bpf_fd, BPF_PROG_TYPE_SCHED_CLS);
+ 	if (IS_ERR(fp))
+ 		return PTR_ERR(fp);
+ 
+ 	if (tb[TCA_BPF_NAME]) {
+ 		name = kmemdup(nla_data(tb[TCA_BPF_NAME]),
+ 			       nla_len(tb[TCA_BPF_NAME]),
+ 			       GFP_KERNEL);
+ 		if (!name) {
+ 			bpf_prog_put(fp);
+ 			return -ENOMEM;
+ 		}
+ 	}
+ 
+ 	prog->bpf_ops = NULL;
+ 	prog->bpf_fd = bpf_fd;
+ 	prog->bpf_name = name;
+ 	prog->filter = fp;
+ 
+ 	if (fp->dst_needed && !(tp->q->flags & TCQ_F_INGRESS))
+ 		netif_keep_dst(qdisc_dev(tp->q));
+ 
+ 	return 0;
+ }
+ 
+ static int cls_bpf_modify_existing(struct net *net, struct tcf_proto *tp,
+ 				   struct cls_bpf_prog *prog,
+ 				   unsigned long base, struct nlattr **tb,
+ 				   struct nlattr *est, bool ovr)
+ {
+ 	bool is_bpf, is_ebpf, have_exts = false;
+ 	struct tcf_exts exts;
+ 	int ret;
+ 
+ 	is_bpf = tb[TCA_BPF_OPS_LEN] && tb[TCA_BPF_OPS];
+ 	is_ebpf = tb[TCA_BPF_FD];
+ 	if ((!is_bpf && !is_ebpf) || (is_bpf && is_ebpf))
+ 		return -EINVAL;
+ 
+ 	tcf_exts_init(&exts, TCA_BPF_ACT, TCA_BPF_POLICE);
+ 	ret = tcf_exts_validate(net, tp, tb, est, &exts, ovr);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (tb[TCA_BPF_FLAGS]) {
+ 		u32 bpf_flags = nla_get_u32(tb[TCA_BPF_FLAGS]);
+ 
+ 		if (bpf_flags & ~TCA_BPF_FLAG_ACT_DIRECT) {
+ 			tcf_exts_destroy(&exts);
+ 			return -EINVAL;
+ 		}
+ 
+ 		have_exts = bpf_flags & TCA_BPF_FLAG_ACT_DIRECT;
+ 	}
+ 
+ 	prog->exts_integrated = have_exts;
+ 
+ 	ret = is_bpf ? cls_bpf_prog_from_ops(tb, prog) :
+ 		       cls_bpf_prog_from_efd(tb, prog, tp);
+ 	if (ret < 0) {
+ 		tcf_exts_destroy(&exts);
+ 		return ret;
+ 	}
+ 
+ 	if (tb[TCA_BPF_CLASSID]) {
+ 		prog->res.classid = nla_get_u32(tb[TCA_BPF_CLASSID]);
+ 		tcf_bind_filter(tp, &prog->res, base);
+ 	}
+ 
+ 	tcf_exts_change(tp, &prog->exts, &exts);
+ 	return 0;
++>>>>>>> 113214be7f6c (bpf: refactor bpf_prog_get and type check into helper)
  }
  
  static u32 cls_bpf_grab_new_handle(struct tcf_proto *tp,
* Unmerged path kernel/bpf/syscall.c
* Unmerged path net/kcm/kcmsock.c
* Unmerged path net/sched/act_bpf.c
* Unmerged path include/linux/bpf.h
* Unmerged path kernel/bpf/syscall.c
* Unmerged path net/core/filter.c
* Unmerged path net/kcm/kcmsock.c
* Unmerged path net/packet/af_packet.c
* Unmerged path net/sched/act_bpf.c
* Unmerged path net/sched/cls_bpf.c
