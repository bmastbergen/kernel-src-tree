sched: Add wrapper for checking task_struct::on_rq

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Kirill Tkhai <ktkhai@parallels.com>
commit da0c1e65b51a289540159663aa4b90ba2366bc21
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/da0c1e65.failed

Implement task_on_rq_queued() and use it everywhere instead of
on_rq check. No functional changes.

The only exception is we do not use the wrapper in
check_for_tasks(), because it requires to export
task_on_rq_queued() in global header files. Next patch in series
would return it back, so we do not twist it from here to there.

	Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Paul Turner <pjt@google.com>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
	Cc: Kirill Tkhai <tkhai@yandex.ru>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
Link: http://lkml.kernel.org/r/1408528052.23412.87.camel@tkhai
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit da0c1e65b51a289540159663aa4b90ba2366bc21)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/deadline.c
#	kernel/sched/fair.c
#	kernel/sched/rt.c
#	kernel/sched/sched.h
#	kernel/sched/stop_task.c
diff --cc kernel/sched/core.c
index 1a5e18b224eb,a02b624fee6c..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -1588,7 -1537,9 +1588,13 @@@ static int ttwu_remote(struct task_stru
  	int ret = 0;
  
  	rq = __task_rq_lock(p);
++<<<<<<< HEAD
 +	if (p->on_rq) {
++=======
+ 	if (task_on_rq_queued(p)) {
+ 		/* check_preempt_curr() may use rq clock */
+ 		update_rq_clock(rq);
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  		ttwu_do_wakeup(rq, p, wake_flags);
  		ret = 1;
  	}
@@@ -1723,29 -1678,7 +1729,33 @@@ try_to_wake_up(struct task_struct *p, u
  	success = 1; /* we're going to change ->state */
  	cpu = task_cpu(p);
  
++<<<<<<< HEAD
 +	/*
 +	 * Ensure we load p->on_rq _after_ p->state, otherwise it would
 +	 * be possible to, falsely, observe p->on_rq == 0 and get stuck
 +	 * in smp_cond_load_acquire() below.
 +	 *
 +	 * sched_ttwu_pending()                 try_to_wake_up()
 +	 *   [S] p->on_rq = 1;                  [L] P->state
 +	 *       UNLOCK rq->lock  -----.
 +	 *                              \
 +	 *				 +---   RMB
 +	 * schedule()                   /
 +	 *       LOCK rq->lock    -----'
 +	 *       UNLOCK rq->lock
 +	 *
 +	 * [task p]
 +	 *   [S] p->state = UNINTERRUPTIBLE     [L] p->on_rq
 +	 *
 +	 * Pairs with the UNLOCK+LOCK on rq->lock from the
 +	 * last wakeup of our task and the schedule that got our task
 +	 * current.
 +	 */
 +	smp_rmb();
 +	if (p->on_rq && ttwu_remote(p, wake_flags))
++=======
+ 	if (task_on_rq_queued(p) && ttwu_remote(p, wake_flags))
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  		goto stat;
  
  #ifdef CONFIG_SMP
@@@ -3196,6 -2430,44 +3206,47 @@@ EXPORT_PER_CPU_SYMBOL(kstat)
  EXPORT_PER_CPU_SYMBOL(kernel_cpustat);
  
  /*
++<<<<<<< HEAD
++=======
+  * Return any ns on the sched_clock that have not yet been accounted in
+  * @p in case that task is currently running.
+  *
+  * Called with task_rq_lock() held on @rq.
+  */
+ static u64 do_task_delta_exec(struct task_struct *p, struct rq *rq)
+ {
+ 	u64 ns = 0;
+ 
+ 	/*
+ 	 * Must be ->curr _and_ ->on_rq.  If dequeued, we would
+ 	 * project cycles that may never be accounted to this
+ 	 * thread, breaking clock_gettime().
+ 	 */
+ 	if (task_current(rq, p) && task_on_rq_queued(p)) {
+ 		update_rq_clock(rq);
+ 		ns = rq_clock_task(rq) - p->se.exec_start;
+ 		if ((s64)ns < 0)
+ 			ns = 0;
+ 	}
+ 
+ 	return ns;
+ }
+ 
+ unsigned long long task_delta_exec(struct task_struct *p)
+ {
+ 	unsigned long flags;
+ 	struct rq *rq;
+ 	u64 ns = 0;
+ 
+ 	rq = task_rq_lock(p, &flags);
+ 	ns = do_task_delta_exec(p, rq);
+ 	task_rq_unlock(rq, p, &flags);
+ 
+ 	return ns;
+ }
+ 
+ /*
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
   * Return accounted runtime for the task.
   * In case the task is currently running, return the runtime plus current's
   * pending runtime that have not been accounted yet.
@@@ -3219,20 -2490,12 +3270,20 @@@ unsigned long long task_sched_runtime(s
  	 * If we see ->on_cpu without ->on_rq, the task is leaving, and has
  	 * been accounted, so we're correct here as well.
  	 */
- 	if (!p->on_cpu || !p->on_rq)
+ 	if (!p->on_cpu || !task_on_rq_queued(p))
  		return p->se.sum_exec_runtime;
  #endif
 -
  	rq = task_rq_lock(p, &flags);
 -	ns = p->se.sum_exec_runtime + do_task_delta_exec(p, rq);
 +	/*
 +	 * Must be ->curr _and_ ->on_rq.  If dequeued, we would
 +	 * project cycles that may never be accounted to this
 +	 * thread, breaking clock_gettime().
 +	 */
 +	if (task_current(rq, p) && p->on_rq) {
 +		update_rq_clock(rq);
 +		p->sched_class->update_curr(rq);
 +	}
 +	ns = p->se.sum_exec_runtime;
  	task_rq_unlock(rq, p, &flags);
  
  	return ns;
@@@ -3522,14 -2794,12 +3573,19 @@@ need_resched
  		switch_count = &prev->nvcsw;
  	}
  
++<<<<<<< HEAD
 +	pre_schedule(rq, prev);
++=======
+ 	if (task_on_rq_queued(prev) || rq->skip_clock_update < 0)
+ 		update_rq_clock(rq);
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
 +
 +	if (unlikely(!rq->nr_running))
 +		idle_balance(cpu, rq);
  
 -	next = pick_next_task(rq, prev);
 +	put_prev_task(rq, prev);
 +	next = pick_next_task(rq);
  	clear_tsk_need_resched(prev);
 -	clear_preempt_need_resched();
  	rq->skip_clock_update = 0;
  
  	if (likely(prev != next)) {
@@@ -4530,7 -3342,9 +4586,13 @@@ static int __sched_setscheduler(struct 
  				const struct sched_attr *attr,
  				bool user)
  {
++<<<<<<< HEAD
 +	int retval, oldprio, oldpolicy = -1, on_rq, running;
++=======
+ 	int newprio = dl_policy(attr->sched_policy) ? MAX_DL_PRIO - 1 :
+ 		      MAX_RT_PRIO - 1 - attr->sched_priority;
+ 	int retval, oldprio, oldpolicy = -1, queued, running;
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  	int policy = attr->sched_policy;
  	unsigned long flags;
  	const struct sched_class *prev_class;
@@@ -4707,9 -3523,27 +4769,31 @@@ change
  		return -EBUSY;
  	}
  
++<<<<<<< HEAD
 +	on_rq = p->on_rq;
++=======
+ 	p->sched_reset_on_fork = reset_on_fork;
+ 	oldprio = p->prio;
+ 
+ 	/*
+ 	 * Special case for priority boosted tasks.
+ 	 *
+ 	 * If the new priority is lower or equal (user space view)
+ 	 * than the current (boosted) priority, we just store the new
+ 	 * normal parameters and do not touch the scheduler class and
+ 	 * the runqueue. This will be done when the task deboost
+ 	 * itself.
+ 	 */
+ 	if (rt_mutex_check_prio(p, newprio)) {
+ 		__setscheduler_params(p, attr);
+ 		task_rq_unlock(rq, p, &flags);
+ 		return 0;
+ 	}
+ 
+ 	queued = task_on_rq_queued(p);
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  	running = task_current(rq, p);
- 	if (on_rq)
+ 	if (queued)
  		dequeue_task(rq, p, 0);
  	if (running)
  		p->sched_class->put_prev_task(rq, p);
@@@ -4722,8 -3553,13 +4806,18 @@@
  
  	if (running)
  		p->sched_class->set_curr_task(rq);
++<<<<<<< HEAD
 +	if (on_rq)
 +		enqueue_task(rq, p, 0);
++=======
+ 	if (queued) {
+ 		/*
+ 		 * We enqueue to tail when the priority of a task is
+ 		 * increased (user space view).
+ 		 */
+ 		enqueue_task(rq, p, oldprio <= p->prio ? ENQUEUE_HEAD : 0);
+ 	}
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  
  	check_class_changed(rq, p, prev_class, oldprio);
  	task_rq_unlock(rq, p, &flags);
@@@ -5756,6 -4568,7 +5850,10 @@@ void init_idle(struct task_struct *idle
  	rcu_read_unlock();
  
  	rq->curr = rq->idle = idle;
++<<<<<<< HEAD
++=======
+ 	idle->on_rq = TASK_ON_RQ_QUEUED;
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  #if defined(CONFIG_SMP)
  	idle->on_cpu = 1;
  #endif
diff --cc kernel/sched/deadline.c
index 77a5fcf8fde2,d21a8e0259d2..000000000000
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@@ -664,64 -528,25 +664,83 @@@ static enum hrtimer_restart dl_task_tim
  
  	sched_clock_tick();
  	update_rq_clock(rq);
++<<<<<<< HEAD
 +
 +	/*
 +	 * If the throttle happened during sched-out; like:
 +	 *
 +	 *   schedule()
 +	 *     deactivate_task()
 +	 *       dequeue_task_dl()
 +	 *         update_curr_dl()
 +	 *           start_dl_timer()
 +	 *         __dequeue_task_dl()
 +	 *     prev->on_rq = 0;
 +	 *
 +	 * We can be both throttled and !queued. Replenish the counter
 +	 * but do not enqueue -- wait for our wakeup to do that.
 +	 */
 +	if (!p->on_rq) {
 +		replenish_dl_entity(dl_se, dl_se);
 +		goto unlock;
++=======
+ 	dl_se->dl_throttled = 0;
+ 	dl_se->dl_yielded = 0;
+ 	if (task_on_rq_queued(p)) {
+ 		enqueue_task_dl(rq, p, ENQUEUE_REPLENISH);
+ 		if (task_has_dl_policy(rq->curr))
+ 			check_preempt_curr_dl(rq, p, 0);
+ 		else
+ 			resched_curr(rq);
+ #ifdef CONFIG_SMP
+ 		/*
+ 		 * Queueing this task back might have overloaded rq,
+ 		 * check if we need to kick someone away.
+ 		 */
+ 		if (has_pushable_dl_tasks(rq))
+ 			push_dl_task(rq);
+ #endif
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
 +	}
 +
 +	enqueue_task_dl(rq, p, ENQUEUE_REPLENISH);
 +	if (dl_task(rq->curr))
 +		check_preempt_curr_dl(rq, p, 0);
 +	else
 +		resched_curr(rq);
 +
 +#ifdef CONFIG_SMP
 +	/*
 +	 * Perform balancing operations here; after the replenishments.  We
 +	 * cannot drop rq->lock before this, otherwise the assertion in
 +	 * start_dl_timer() about not missing updates is not true.
 +	 *
 +	 * If we find that the rq the task was on is no longer available, we
 +	 * need to select a new rq.
 +	 *
 +	 * XXX figure out if select_task_rq_dl() deals with offline cpus.
 +	 */
 +	if (unlikely(!rq->online)) {
 +		rq = dl_task_offline_migration(rq, p);
 +		update_rq_clock(rq);
  	}
 +
 +	/*
 +	 * Queueing this task back might have overloaded rq, check if we need
 +	 * to kick someone away.
 +	 */
 +	if (has_pushable_dl_tasks(rq))
 +		push_dl_task(rq);
 +#endif
 +
  unlock:
 -	raw_spin_unlock(&rq->lock);
 +	task_rq_unlock(rq, p, &flags);
 +
 +	/*
 +	 * This can free the task_struct, including this hrtimer, do not touch
 +	 * anything related to that after this.
 +	 */
 +	put_task_struct(p);
  
  	return HRTIMER_NORESTART;
  }
@@@ -1213,6 -1023,24 +1232,27 @@@ struct task_struct *pick_next_task_dl(s
  
  	dl_rq = &rq->dl;
  
++<<<<<<< HEAD
++=======
+ 	if (need_pull_dl_task(rq, prev)) {
+ 		pull_dl_task(rq);
+ 		/*
+ 		 * pull_rt_task() can drop (and re-acquire) rq->lock; this
+ 		 * means a stop task can slip in, in which case we need to
+ 		 * re-start task selection.
+ 		 */
+ 		if (rq->stop && task_on_rq_queued(rq->stop))
+ 			return RETRY_TASK;
+ 	}
+ 
+ 	/*
+ 	 * When prev is DL, we may throttle it in put_prev_task().
+ 	 * So, we update time before we check for dl_nr_running.
+ 	 */
+ 	if (prev->sched_class == &dl_sched_class)
+ 		update_curr_dl(rq);
+ 
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  	if (unlikely(!dl_rq->dl_nr_running))
  		return NULL;
  
@@@ -1815,13 -1589,17 +1856,13 @@@ static void switched_to_dl(struct rq *r
  {
  	int check_resched = 1;
  
 -	/*
 -	 * If p is throttled, don't consider the possibility
 -	 * of preempting rq->curr, the check will be done right
 -	 * after its runtime will get replenished.
 -	 */
 -	if (unlikely(p->dl.dl_throttled))
 -		return;
 +	if (dl_time_before(p->dl.deadline, rq_clock(rq)))
 +		setup_new_dl_entity(&p->dl, &p->dl);
  
- 	if (p->on_rq && rq->curr != p) {
+ 	if (task_on_rq_queued(p) && rq->curr != p) {
  #ifdef CONFIG_SMP
 -		if (rq->dl.overloaded && push_dl_task(rq) && rq != task_rq(p))
 +		if (p->nr_cpus_allowed > 1 && rq->dl.overloaded &&
 +		    push_dl_task(rq) && rq != task_rq(p))
  			/* Only reschedule if pushing failed */
  			check_resched = 0;
  #endif /* CONFIG_SMP */
diff --cc kernel/sched/fair.c
index a455c3157828,9e6ca0d88f51..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -7422,7 -7494,7 +7422,11 @@@ static void task_fork_fair(struct task_
  static void
  prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
  {
++<<<<<<< HEAD
 +	if (!p->se.on_rq)
++=======
+ 	if (!task_on_rq_queued(p))
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  		return;
  
  	/*
@@@ -7480,7 -7550,15 +7484,19 @@@ static void switched_from_fair(struct r
   */
  static void switched_to_fair(struct rq *rq, struct task_struct *p)
  {
++<<<<<<< HEAD
 +	if (!p->se.on_rq)
++=======
+ #ifdef CONFIG_FAIR_GROUP_SCHED
+ 	struct sched_entity *se = &p->se;
+ 	/*
+ 	 * Since the real-depth could have been changed (only FAIR
+ 	 * class maintain depth value), reset depth properly.
+ 	 */
+ 	se->depth = se->parent ? se->parent->depth + 1 : 0;
+ #endif
+ 	if (!task_on_rq_queued(p))
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  		return;
  
  	/*
@@@ -7526,9 -7604,11 +7542,9 @@@ void init_cfs_rq(struct cfs_rq *cfs_rq
  }
  
  #ifdef CONFIG_FAIR_GROUP_SCHED
- static void task_move_group_fair(struct task_struct *p, int on_rq)
+ static void task_move_group_fair(struct task_struct *p, int queued)
  {
 -	struct sched_entity *se = &p->se;
  	struct cfs_rq *cfs_rq;
 -
  	/*
  	 * If the task was not on the rq at the time of this cgroup movement
  	 * it must have been asleep, sleeping tasks keep their ->vruntime
@@@ -7554,15 -7634,16 +7570,28 @@@
  	 * To prevent boost or penalty in the new cfs_rq caused by delta
  	 * min_vruntime between the two cfs_rqs, we skip vruntime adjustment.
  	 */
++<<<<<<< HEAD
 +	if (!on_rq && (!p->se.sum_exec_runtime || p->state == TASK_WAKING))
 +		on_rq = 1;
 +
 +	if (!on_rq)
 +		p->se.vruntime -= cfs_rq_of(&p->se)->min_vruntime;
 +	set_task_rq(p, task_cpu(p));
 +	if (!on_rq) {
 +		cfs_rq = cfs_rq_of(&p->se);
 +		p->se.vruntime += cfs_rq->min_vruntime;
++=======
+ 	if (!queued && (!se->sum_exec_runtime || p->state == TASK_WAKING))
+ 		queued = 1;
+ 
+ 	if (!queued)
+ 		se->vruntime -= cfs_rq_of(se)->min_vruntime;
+ 	set_task_rq(p, task_cpu(p));
+ 	se->depth = se->parent ? se->parent->depth + 1 : 0;
+ 	if (!queued) {
+ 		cfs_rq = cfs_rq_of(se);
+ 		se->vruntime += cfs_rq->min_vruntime;
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  #ifdef CONFIG_SMP
  		/*
  		 * migrate_task_rq_fair() will have removed our previous
diff --cc kernel/sched/rt.c
index bdcb285e7261,4feac8fcb47f..000000000000
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@@ -1370,9 -1435,37 +1370,40 @@@ static struct task_struct *_pick_next_t
  	return p;
  }
  
 -static struct task_struct *
 -pick_next_task_rt(struct rq *rq, struct task_struct *prev)
 +static struct task_struct *pick_next_task_rt(struct rq *rq)
  {
++<<<<<<< HEAD
 +	struct task_struct *p = _pick_next_task_rt(rq);
++=======
+ 	struct task_struct *p;
+ 	struct rt_rq *rt_rq = &rq->rt;
+ 
+ 	if (need_pull_rt_task(rq, prev)) {
+ 		pull_rt_task(rq);
+ 		/*
+ 		 * pull_rt_task() can drop (and re-acquire) rq->lock; this
+ 		 * means a dl or stop task can slip in, in which case we need
+ 		 * to re-start task selection.
+ 		 */
+ 		if (unlikely((rq->stop && task_on_rq_queued(rq->stop)) ||
+ 			     rq->dl.dl_nr_running))
+ 			return RETRY_TASK;
+ 	}
+ 
+ 	/*
+ 	 * We may dequeue prev's rt_rq in put_prev_task().
+ 	 * So, we update time before rt_nr_running check.
+ 	 */
+ 	if (prev->sched_class == &rt_sched_class)
+ 		update_curr_rt(rq);
+ 
+ 	if (!rt_rq->rt_queued)
+ 		return NULL;
+ 
+ 	put_prev_task(rq, prev);
+ 
+ 	p = _pick_next_task_rt(rq);
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  
  	/* The running task is never eligible for pushing */
  	if (p)
@@@ -1914,11 -1970,11 +1945,11 @@@ static void switched_to_rt(struct rq *r
  	 * If that current running task is also an RT task
  	 * then see if we can move to another run queue.
  	 */
- 	if (p->on_rq && rq->curr != p) {
+ 	if (task_on_rq_queued(p) && rq->curr != p) {
  #ifdef CONFIG_SMP
 -		if (p->nr_cpus_allowed > 1 && rq->rt.overloaded &&
 +		if (rq->rt.overloaded && push_rt_task(rq) &&
  		    /* Don't resched if we changed runqueues */
 -		    push_rt_task(rq) && rq != task_rq(p))
 +		    rq != task_rq(p))
  			check_resched = 0;
  #endif /* CONFIG_SMP */
  		if (check_resched && p->prio < rq->curr->prio)
diff --cc kernel/sched/sched.h
index 98cec6922405,26566d0c67ac..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -16,25 -13,18 +16,33 @@@
  #include "cpudeadline.h"
  #include "cpuacct.h"
  
++<<<<<<< HEAD
++=======
+ struct rq;
+ 
+ /* task_struct::on_rq states: */
+ #define TASK_ON_RQ_QUEUED	1
+ 
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  extern __read_mostly int scheduler_running;
  
 -extern unsigned long calc_load_update;
 -extern atomic_long_t calc_load_tasks;
 +/*
 + * Convert user-nice values [ -20 ... 0 ... 19 ]
 + * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
 + * and back.
 + */
 +#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
 +#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
 +#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
  
 -extern long calc_load_fold_active(struct rq *this_rq);
 -extern void update_cpu_load_active(struct rq *this_rq);
 +/*
 + * 'User priority' is the nice value converted to something we
 + * can work with better when scaling various scheduler parameters,
 + * it's a [ 0 ... 39 ] range.
 + */
 +#define USER_PRIO(p)		((p)-MAX_RT_PRIO)
 +#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
 +#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
  
  /*
   * Helpers for converting nanosecond timing to jiffy resolution
diff --cc kernel/sched/stop_task.c
index 04f14f4514f8,67426e529f59..000000000000
--- a/kernel/sched/stop_task.c
+++ b/kernel/sched/stop_task.c
@@@ -27,12 -28,14 +27,17 @@@ static struct task_struct *pick_next_ta
  {
  	struct task_struct *stop = rq->stop;
  
++<<<<<<< HEAD
 +	if (stop && stop->on_rq) {
 +		stop->se.exec_start = rq_clock_task(rq);
 +		return stop;
 +	}
++=======
+ 	if (!stop || !task_on_rq_queued(stop))
+ 		return NULL;
++>>>>>>> da0c1e65b51a (sched: Add wrapper for checking task_struct::on_rq)
  
 -	put_prev_task(rq, prev);
 -
 -	stop->se.exec_start = rq_clock_task(rq);
 -
 -	return stop;
 +	return NULL;
  }
  
  static void
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/deadline.c
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/rt.c
* Unmerged path kernel/sched/sched.h
* Unmerged path kernel/sched/stop_task.c
