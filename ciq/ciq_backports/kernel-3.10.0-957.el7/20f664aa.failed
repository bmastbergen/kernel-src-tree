mm: pmd dirty emulation in page fault handler

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] pmd dirty emulation in page fault handler (Rafael Aquini) [1562137]
Rebuild_FUZZ: 95.35%
commit-author Minchan Kim <minchan@kernel.org>
commit 20f664aabeb88d582b623a625f83b0454fa34f07
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/20f664aa.failed

Andreas reported [1] made a test in jemalloc hang in THP mode in arm64:

  http://lkml.kernel.org/r/mvmmvfy37g1.fsf@hawking.suse.de

The problem is currently page fault handler doesn't supports dirty bit
emulation of pmd for non-HW dirty-bit architecture so that application
stucks until VM marked the pmd dirty.

How the emulation work depends on the architecture.  In case of arm64,
when it set up pte firstly, it sets pte PTE_RDONLY to get a chance to
mark the pte dirty via triggering page fault when store access happens.
Once the page fault occurs, VM marks the pmd dirty and arch code for
setting pmd will clear PTE_RDONLY for application to proceed.

IOW, if VM doesn't mark the pmd dirty, application hangs forever by
repeated fault(i.e., store op but the pmd is PTE_RDONLY).

This patch enables pmd dirty-bit emulation for those architectures.

[1] b8d3c4c3009d, mm/huge_memory.c: don't split THP page when MADV_FREE syscall is called

Fixes: b8d3c4c3009d ("mm/huge_memory.c: don't split THP page when MADV_FREE syscall is called")
Link: http://lkml.kernel.org/r/1482506098-6149-1-git-send-email-minchan@kernel.org
	Signed-off-by: Minchan Kim <minchan@kernel.org>
	Reported-by: Andreas Schwab <schwab@suse.de>
	Tested-by: Andreas Schwab <schwab@suse.de>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Jason Evans <je@fb.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: <stable@vger.kernel.org> [4.5+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 20f664aabeb88d582b623a625f83b0454fa34f07)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index 075f651aabed,9a6bd6c8d55a..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1139,240 -879,39 +1139,249 @@@ out
  	return ret;
  }
  
 -void huge_pmd_set_accessed(struct vm_fault *vmf, pmd_t orig_pmd)
 +#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
 +static void touch_pud(struct vm_area_struct *vma, unsigned long addr,
 +		pud_t *pud)
  {
 -	pmd_t entry;
 -	unsigned long haddr;
 -	bool write = vmf->flags & FAULT_FLAG_WRITE;
 -
 -	vmf->ptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);
 -	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
 -		goto unlock;
 -
 -	entry = pmd_mkyoung(orig_pmd);
 -	if (write)
 -		entry = pmd_mkdirty(entry);
 -	haddr = vmf->address & HPAGE_PMD_MASK;
 -	if (pmdp_set_access_flags(vmf->vma, haddr, vmf->pmd, entry, write))
 -		update_mmu_cache_pmd(vmf->vma, vmf->address, vmf->pmd);
 +	pud_t _pud;
  
 -unlock:
 -	spin_unlock(vmf->ptl);
 +	/*
 +	 * We should set the dirty bit only for FOLL_WRITE but for now
 +	 * the dirty bit in the pud is meaningless.  And if the dirty
 +	 * bit will become meaningful and we'll only set it with
 +	 * FOLL_WRITE, an atomic set_bit will be required on the pud to
 +	 * set the young bit, instead of the current set_pud_at.
 +	 */
 +	_pud = pud_mkyoung(pud_mkdirty(*pud));
 +	if (pudp_set_access_flags(vma, addr & HPAGE_PUD_MASK,
 +				pud, _pud,  1))
 +		update_mmu_cache_pud(vma, addr, pud);
  }
  
 -static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,
 -		struct page *page)
 +struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,
 +		pud_t *pud, int flags)
  {
 -	struct vm_area_struct *vma = vmf->vma;
 -	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
 -	struct mem_cgroup *memcg;
 -	pgtable_t pgtable;
 -	pmd_t _pmd;
 -	int ret = 0, i;
 -	struct page **pages;
 -	unsigned long mmun_start;	/* For mmu_notifiers */
 -	unsigned long mmun_end;		/* For mmu_notifiers */
 +	unsigned long pfn = pud_pfn(*pud);
 +	struct mm_struct *mm = vma->vm_mm;
 +	struct dev_pagemap *pgmap;
 +	struct page *page;
 +
 +	assert_spin_locked(pud_lockptr(mm, pud));
 +
 +	if (flags & FOLL_WRITE && !pud_write(*pud))
 +		return NULL;
 +
 +	if (pud_present(*pud) && pud_devmap(*pud))
 +		/* pass */;
 +	else
 +		return NULL;
 +
 +	if (flags & FOLL_TOUCH)
 +		touch_pud(vma, addr, pud);
 +
 +	/*
 +	 * device mapped pages can only be returned if the
 +	 * caller will manage the page reference count.
 +	 */
 +	if (!(flags & FOLL_GET))
 +		return ERR_PTR(-EEXIST);
 +
 +	pfn += (addr & ~PUD_MASK) >> PAGE_SHIFT;
 +	pgmap = get_dev_pagemap(pfn, NULL);
 +	if (!pgmap)
 +		return ERR_PTR(-EFAULT);
 +	page = pfn_to_page(pfn);
 +	get_page(page);
 +	put_dev_pagemap(pgmap);
 +
 +	return page;
 +}
 +
 +int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 +		  pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
 +		  struct vm_area_struct *vma)
 +{
 +	spinlock_t *dst_ptl, *src_ptl;
 +	pud_t pud;
 +	int ret;
 +
 +	dst_ptl = pud_lock(dst_mm, dst_pud);
 +	src_ptl = pud_lockptr(src_mm, src_pud);
 +	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
 +
 +	ret = -EAGAIN;
 +	pud = *src_pud;
 +	if (unlikely(!pud_trans_huge(pud) && !pud_devmap(pud)))
 +		goto out_unlock;
 +
 +	/*
 +	 * When page table lock is held, the huge zero pud should not be
 +	 * under splitting since we don't split the page itself, only pud to
 +	 * a page table.
 +	 */
 +	if (is_huge_zero_pud(pud)) {
 +		/* No huge zero pud yet */
 +	}
 +
 +	pudp_set_wrprotect(src_mm, addr, src_pud);
 +	pud = pud_mkold(pud_wrprotect(pud));
 +	set_pud_at(dst_mm, addr, dst_pud, pud);
 +
 +	ret = 0;
 +out_unlock:
 +	spin_unlock(src_ptl);
 +	spin_unlock(dst_ptl);
 +	return ret;
 +}
 +
 +void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud)
 +{
 +	pud_t entry;
 +	unsigned long address = (unsigned long)vmf->virtual_address;
 +	unsigned long haddr;
 +	bool write = vmf->flags & FAULT_FLAG_WRITE;
 +	spinlock_t *ptl;
 +
 +	ptl = pud_lock(vmf->vma->vm_mm, vmf->pud);
 +	if (unlikely(!pud_same(*vmf->pud, orig_pud)))
 +		goto unlock;
 +
 +	entry = pud_mkyoung(orig_pud);
 +	if (write)
 +		entry = pud_mkdirty(entry);
 +	haddr = address & HPAGE_PUD_MASK;
 +	if (pudp_set_access_flags(vmf->vma, haddr, vmf->pud, entry, write))
 +		update_mmu_cache_pud(vmf->vma, address, vmf->pud);
 +
 +unlock:
 +	spin_unlock(ptl);
 +}
 +#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
 +
 +void huge_pmd_set_accessed(struct vm_fault *vmf, pmd_t orig_pmd)
 +{
 +	struct vm_area_struct *vma = vmf->vma;
 +	unsigned long address = (unsigned long)vmf->virtual_address;
 +	spinlock_t *ptl;
 +	pmd_t entry;
 +	unsigned long haddr;
++	bool write = vmf->flags & FAULT_FLAG_WRITE;
 +
 +	ptl = pmd_lock(vma->vm_mm, vmf->pmd);
 +	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
 +		goto unlock;
 +
 +	entry = pmd_mkyoung(orig_pmd);
++<<<<<<< HEAD
 +	haddr = address & HPAGE_PMD_MASK;
 +	if (pmdp_set_access_flags(vmf->vma, haddr, vmf->pmd, entry,
 +				vmf->flags & FAULT_FLAG_WRITE))
 +		update_mmu_cache_pmd(vma, address, vmf->pmd);
++=======
++	if (write)
++		entry = pmd_mkdirty(entry);
++	haddr = vmf->address & HPAGE_PMD_MASK;
++	if (pmdp_set_access_flags(vmf->vma, haddr, vmf->pmd, entry, write))
++		update_mmu_cache_pmd(vmf->vma, vmf->address, vmf->pmd);
++>>>>>>> 20f664aabeb8 (mm: pmd dirty emulation in page fault handler)
 +
 +unlock:
 +	spin_unlock(ptl);
 +}
 +
 +static int do_huge_pmd_wp_zero_page_fallback(struct vm_fault *vmf,
 +		pmd_t orig_pmd)
 +{
 +	struct vm_area_struct *vma = vmf->vma;
 +	struct mm_struct *mm = vma->vm_mm;
 +	unsigned long address = (unsigned long)vmf->virtual_address;
 +	unsigned long haddr = address & HPAGE_PMD_MASK;
 +	spinlock_t *ptl;
 +	pgtable_t pgtable;
 +	pmd_t _pmd;
 +	struct page *page;
 +	int i, ret = 0;
 +	unsigned long mmun_start;	/* For mmu_notifiers */
 +	unsigned long mmun_end;		/* For mmu_notifiers */
 +
 +	page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
 +	if (!page) {
 +		ret |= VM_FAULT_OOM;
 +		goto out;
 +	}
 +
 +	if (mem_cgroup_newpage_charge(page, mm, GFP_KERNEL)) {
 +		put_page(page);
 +		ret |= VM_FAULT_OOM;
 +		goto out;
 +	}
 +
 +	clear_user_highpage(page, address);
 +	__SetPageUptodate(page);
 +
 +	mmun_start = haddr;
 +	mmun_end   = haddr + HPAGE_PMD_SIZE;
 +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 +
 +	ptl = pmd_lock(mm, vmf->pmd);
 +	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
 +		goto out_free_page;
 +
 +	pmdp_clear_flush_notify(vma, haddr, vmf->pmd);
 +	/* leave pmd empty until pte is filled */
 +
 +	pgtable = pgtable_trans_huge_withdraw(mm, vmf->pmd);
 +	pmd_populate(mm, &_pmd, pgtable);
 +
 +	for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
 +		pte_t *pte, entry;
 +		if (haddr == (address & PAGE_MASK)) {
 +			entry = mk_pte(page, vma->vm_page_prot);
 +			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +			page_add_new_anon_rmap(page, vma, haddr);
 +		} else {
 +			entry = pfn_pte(my_zero_pfn(haddr), vma->vm_page_prot);
 +			entry = pte_mkspecial(entry);
 +		}
 +		pte = pte_offset_map(&_pmd, haddr);
 +		VM_BUG_ON(!pte_none(*pte));
 +		set_pte_at(mm, haddr, pte, entry);
 +		pte_unmap(pte);
 +	}
 +	smp_wmb(); /* make pte visible before pmd */
 +	pmd_populate(mm, vmf->pmd, pgtable);
 +	spin_unlock(ptl);
 +	put_huge_zero_page();
 +	inc_mm_counter(mm, MM_ANONPAGES);
 +
 +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 +
 +	ret |= VM_FAULT_WRITE;
 +out:
 +	return ret;
 +out_free_page:
 +	spin_unlock(ptl);
 +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 +	mem_cgroup_uncharge_page(page);
 +	put_page(page);
 +	goto out;
 +}
 +
 +static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 +					pmd_t orig_pmd,
 +					struct page *page)
 +{
 +	struct vm_area_struct *vma = vmf->vma;
 +	struct mm_struct *mm = vma->vm_mm;
 +	unsigned long address = (unsigned long)vmf->virtual_address;
 +	unsigned long haddr = address & HPAGE_PMD_MASK;
 +	spinlock_t *ptl;
 +	pgtable_t pgtable;
 +	pmd_t _pmd = {0};
 +	int ret = 0, i;
 +	struct page **pages;
 +	unsigned long mmun_start;	/* For mmu_notifiers */
 +	unsigned long mmun_end;		/* For mmu_notifiers */
  
  	pages = kmalloc(sizeof(struct page *) * HPAGE_PMD_NR,
  			GFP_KERNEL);
* Unmerged path mm/huge_memory.c
