bpf: split state from prandom_u32() and consolidate {c, e}BPF prngs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [kernel] bpf: split state from prandom_u32() and consolidate c/eBPF prngs (Jiri Olsa) [1311586]
Rebuild_FUZZ: 96.18%
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 3ad0040573b0c00f88488bc31958acd07a55ee2e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3ad00405.failed

While recently arguing on a seccomp discussion that raw prandom_u32()
access shouldn't be exposed to unpriviledged user space, I forgot the
fact that SKF_AD_RANDOM extension actually already does it for some time
in cBPF via commit 4cd3675ebf74 ("filter: added BPF random opcode").

Since prandom_u32() is being used in a lot of critical networking code,
lets be more conservative and split their states. Furthermore, consolidate
eBPF and cBPF prandom handlers to use the new internal PRNG. For eBPF,
bpf_get_prandom_u32() was only accessible for priviledged users, but
should that change one day, we also don't want to leak raw sequences
through things like eBPF maps.

One thought was also to have own per bpf_prog states, but due to ABI
reasons this is not easily possible, i.e. the program code currently
cannot access bpf_prog itself, and copying the rnd_state to/from the
stack scratch space whenever a program uses the prng seems not really
worth the trouble and seems too hacky. If needed, taus113 could in such
cases be implemented within eBPF using a map entry to keep the state
space, or get_random_bytes() could become a second helper in cases where
performance would not be critical.

Both sides can trigger a one-time late init via prandom_init_once() on
the shared state. Performance-wise, there should even be a tiny gain
as bpf_user_rnd_u32() saves one function call. The PRNG needs to live
inside the BPF core since kernels could have a NET-less config as well.

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Acked-by: Alexei Starovoitov <ast@plumgrid.com>
	Cc: Chema Gonzalez <chema@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3ad0040573b0c00f88488bc31958acd07a55ee2e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	kernel/bpf/core.c
#	kernel/bpf/helpers.c
#	kernel/bpf/syscall.c
#	net/core/filter.c
diff --cc include/linux/bpf.h
index d4c1f9049ad3,3697ad563899..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -103,14 -124,84 +103,87 @@@ struct bpf_prog
  
  struct bpf_prog_aux {
  	atomic_t refcnt;
 -	u32 used_map_cnt;
 -	const struct bpf_verifier_ops *ops;
 +	bool is_gpl_compatible;
 +	enum bpf_prog_type prog_type;
 +	struct bpf_verifier_ops *ops;
 +	u32 id;
  	struct bpf_map **used_maps;
 +	u32 used_map_cnt;
  	struct bpf_prog *prog;
 -	union {
 -		struct work_struct work;
 -		struct rcu_head	rcu;
 -	};
 +	struct work_struct work;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_array {
+ 	struct bpf_map map;
+ 	u32 elem_size;
+ 	/* 'ownership' of prog_array is claimed by the first program that
+ 	 * is going to use this map or by the first program which FD is stored
+ 	 * in the map to make sure that all callers and callees have the same
+ 	 * prog_type and JITed flag
+ 	 */
+ 	enum bpf_prog_type owner_prog_type;
+ 	bool owner_jited;
+ 	union {
+ 		char value[0] __aligned(8);
+ 		void *ptrs[0] __aligned(8);
+ 	};
+ };
+ #define MAX_TAIL_CALL_CNT 32
+ 
+ u64 bpf_tail_call(u64 ctx, u64 r2, u64 index, u64 r4, u64 r5);
+ void bpf_fd_array_map_clear(struct bpf_map *map);
+ bool bpf_prog_array_compatible(struct bpf_array *array, const struct bpf_prog *fp);
+ const struct bpf_func_proto *bpf_get_trace_printk_proto(void);
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ void bpf_register_prog_type(struct bpf_prog_type_list *tl);
+ void bpf_register_map_type(struct bpf_map_type_list *tl);
+ 
+ struct bpf_prog *bpf_prog_get(u32 ufd);
+ void bpf_prog_put(struct bpf_prog *prog);
+ void bpf_prog_put_rcu(struct bpf_prog *prog);
+ 
+ struct bpf_map *bpf_map_get(struct fd f);
+ void bpf_map_put(struct bpf_map *map);
+ 
+ /* verify correctness of eBPF program */
+ int bpf_check(struct bpf_prog **fp, union bpf_attr *attr);
+ #else
+ static inline void bpf_register_prog_type(struct bpf_prog_type_list *tl)
+ {
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get(u32 ufd)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void bpf_prog_put(struct bpf_prog *prog)
+ {
+ }
+ #endif /* CONFIG_BPF_SYSCALL */
+ 
+ /* verifier prototypes for helper functions called from eBPF programs */
+ extern const struct bpf_func_proto bpf_map_lookup_elem_proto;
+ extern const struct bpf_func_proto bpf_map_update_elem_proto;
+ extern const struct bpf_func_proto bpf_map_delete_elem_proto;
+ 
+ extern const struct bpf_func_proto bpf_perf_event_read_proto;
+ extern const struct bpf_func_proto bpf_get_prandom_u32_proto;
+ extern const struct bpf_func_proto bpf_get_smp_processor_id_proto;
+ extern const struct bpf_func_proto bpf_tail_call_proto;
+ extern const struct bpf_func_proto bpf_ktime_get_ns_proto;
+ extern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;
+ extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
+ extern const struct bpf_func_proto bpf_get_current_comm_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_push_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_pop_proto;
+ 
+ /* Shared helpers among cBPF and eBPF. */
+ void bpf_user_rnd_init_once(void);
+ u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
++>>>>>>> 3ad0040573b0 (bpf: split state from prandom_u32() and consolidate {c, e}BPF prngs)
  #endif /* _LINUX_BPF_H */
diff --cc net/core/filter.c
index 060ed5f86613,342e6c8fc415..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -108,316 -91,531 +108,558 @@@ int sk_filter_trim_cap(struct sock *sk
  
  	return err;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(sk_filter_trim_cap);
++=======
+ EXPORT_SYMBOL(sk_filter);
+ 
+ static u64 __skb_get_pay_offset(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	return skb_get_poff((struct sk_buff *)(unsigned long) ctx);
+ }
+ 
+ static u64 __skb_get_nlattr(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *)(unsigned long) ctx;
+ 	struct nlattr *nla;
+ 
+ 	if (skb_is_nonlinear(skb))
+ 		return 0;
+ 
+ 	if (skb->len < sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	if (a > skb->len - sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	nla = nla_find((struct nlattr *) &skb->data[a], skb->len - a, x);
+ 	if (nla)
+ 		return (void *) nla - (void *) skb->data;
+ 
+ 	return 0;
+ }
+ 
+ static u64 __skb_get_nlattr_nest(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *)(unsigned long) ctx;
+ 	struct nlattr *nla;
+ 
+ 	if (skb_is_nonlinear(skb))
+ 		return 0;
+ 
+ 	if (skb->len < sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	if (a > skb->len - sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	nla = (struct nlattr *) &skb->data[a];
+ 	if (nla->nla_len > skb->len - a)
+ 		return 0;
+ 
+ 	nla = nla_find_nested(nla, x);
+ 	if (nla)
+ 		return (void *) nla - (void *) skb->data;
+ 
+ 	return 0;
+ }
+ 
+ static u64 __get_raw_cpu_id(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	return raw_smp_processor_id();
+ }
+ 
+ static u32 convert_skb_access(int skb_field, int dst_reg, int src_reg,
+ 			      struct bpf_insn *insn_buf)
+ {
+ 	struct bpf_insn *insn = insn_buf;
+ 
+ 	switch (skb_field) {
+ 	case SKF_AD_MARK:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, mark));
+ 		break;
+ 
+ 	case SKF_AD_PKTTYPE:
+ 		*insn++ = BPF_LDX_MEM(BPF_B, dst_reg, src_reg, PKT_TYPE_OFFSET());
+ 		*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg, PKT_TYPE_MAX);
+ #ifdef __BIG_ENDIAN_BITFIELD
+ 		*insn++ = BPF_ALU32_IMM(BPF_RSH, dst_reg, 5);
+ #endif
+ 		break;
+ 
+ 	case SKF_AD_QUEUE:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, queue_mapping));
+ 		break;
+ 
+ 	case SKF_AD_VLAN_TAG:
+ 	case SKF_AD_VLAN_TAG_PRESENT:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
+ 		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
+ 
+ 		/* dst_reg = *(u16 *) (src_reg + offsetof(vlan_tci)) */
+ 		*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, vlan_tci));
+ 		if (skb_field == SKF_AD_VLAN_TAG) {
+ 			*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg,
+ 						~VLAN_TAG_PRESENT);
+ 		} else {
+ 			/* dst_reg >>= 12 */
+ 			*insn++ = BPF_ALU32_IMM(BPF_RSH, dst_reg, 12);
+ 			/* dst_reg &= 1 */
+ 			*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg, 1);
+ 		}
+ 		break;
+ 	}
+ 
+ 	return insn - insn_buf;
+ }
+ 
+ static bool convert_bpf_extensions(struct sock_filter *fp,
+ 				   struct bpf_insn **insnp)
+ {
+ 	struct bpf_insn *insn = *insnp;
+ 	u32 cnt;
+ 
+ 	switch (fp->k) {
+ 	case SKF_AD_OFF + SKF_AD_PROTOCOL:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
+ 
+ 		/* A = *(u16 *) (CTX + offsetof(protocol)) */
+ 		*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
+ 				      offsetof(struct sk_buff, protocol));
+ 		/* A = ntohs(A) [emitting a nop or swap16] */
+ 		*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_PKTTYPE:
+ 		cnt = convert_skb_access(SKF_AD_PKTTYPE, BPF_REG_A, BPF_REG_CTX, insn);
+ 		insn += cnt - 1;
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_IFINDEX:
+ 	case SKF_AD_OFF + SKF_AD_HATYPE:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
+ 		BUILD_BUG_ON(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)) < 0);
+ 
+ 		*insn++ = BPF_LDX_MEM(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)),
+ 				      BPF_REG_TMP, BPF_REG_CTX,
+ 				      offsetof(struct sk_buff, dev));
+ 		/* if (tmp != 0) goto pc + 1 */
+ 		*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_TMP, 0, 1);
+ 		*insn++ = BPF_EXIT_INSN();
+ 		if (fp->k == SKF_AD_OFF + SKF_AD_IFINDEX)
+ 			*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_TMP,
+ 					    offsetof(struct net_device, ifindex));
+ 		else
+ 			*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_TMP,
+ 					    offsetof(struct net_device, type));
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_MARK:
+ 		cnt = convert_skb_access(SKF_AD_MARK, BPF_REG_A, BPF_REG_CTX, insn);
+ 		insn += cnt - 1;
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_RXHASH:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
+ 
+ 		*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,
+ 				    offsetof(struct sk_buff, hash));
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_QUEUE:
+ 		cnt = convert_skb_access(SKF_AD_QUEUE, BPF_REG_A, BPF_REG_CTX, insn);
+ 		insn += cnt - 1;
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_VLAN_TAG:
+ 		cnt = convert_skb_access(SKF_AD_VLAN_TAG,
+ 					 BPF_REG_A, BPF_REG_CTX, insn);
+ 		insn += cnt - 1;
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_VLAN_TAG_PRESENT:
+ 		cnt = convert_skb_access(SKF_AD_VLAN_TAG_PRESENT,
+ 					 BPF_REG_A, BPF_REG_CTX, insn);
+ 		insn += cnt - 1;
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_VLAN_TPID:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_proto) != 2);
+ 
+ 		/* A = *(u16 *) (CTX + offsetof(vlan_proto)) */
+ 		*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
+ 				      offsetof(struct sk_buff, vlan_proto));
+ 		/* A = ntohs(A) [emitting a nop or swap16] */
+ 		*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
+ 	case SKF_AD_OFF + SKF_AD_NLATTR:
+ 	case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
+ 	case SKF_AD_OFF + SKF_AD_CPU:
+ 	case SKF_AD_OFF + SKF_AD_RANDOM:
+ 		/* arg1 = CTX */
+ 		*insn++ = BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_CTX);
+ 		/* arg2 = A */
+ 		*insn++ = BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_A);
+ 		/* arg3 = X */
+ 		*insn++ = BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_X);
+ 		/* Emit call(arg1=CTX, arg2=A, arg3=X) */
+ 		switch (fp->k) {
+ 		case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
+ 			*insn = BPF_EMIT_CALL(__skb_get_pay_offset);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_NLATTR:
+ 			*insn = BPF_EMIT_CALL(__skb_get_nlattr);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
+ 			*insn = BPF_EMIT_CALL(__skb_get_nlattr_nest);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_CPU:
+ 			*insn = BPF_EMIT_CALL(__get_raw_cpu_id);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_RANDOM:
+ 			*insn = BPF_EMIT_CALL(bpf_user_rnd_u32);
+ 			bpf_user_rnd_init_once();
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_ALU_XOR_X:
+ 		/* A ^= X */
+ 		*insn = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_X);
+ 		break;
+ 
+ 	default:
+ 		/* This is just a dummy call to avoid letting the compiler
+ 		 * evict __bpf_call_base() as an optimization. Placed here
+ 		 * where no-one bothers.
+ 		 */
+ 		BUG_ON(__bpf_call_base(0, 0, 0, 0, 0) != 0);
+ 		return false;
+ 	}
+ 
+ 	*insnp = insn;
+ 	return true;
+ }
++>>>>>>> 3ad0040573b0 (bpf: split state from prandom_u32() and consolidate {c, e}BPF prngs)
  
  /**
 - *	bpf_convert_filter - convert filter program
 - *	@prog: the user passed filter program
 - *	@len: the length of the user passed filter program
 - *	@new_prog: buffer where converted program will be stored
 - *	@new_len: pointer to store length of converted program
 + *	sk_run_filter - run a filter on a socket
 + *	@skb: buffer to run the filter on
 + *	@fentry: filter to apply
   *
 - * Remap 'sock_filter' style BPF instruction set to 'sock_filter_ext' style.
 - * Conversion workflow:
 - *
 - * 1) First pass for calculating the new program length:
 - *   bpf_convert_filter(old_prog, old_len, NULL, &new_len)
 - *
 - * 2) 2nd pass to remap in two passes: 1st pass finds new
 - *    jump offsets, 2nd pass remapping:
 - *   new_prog = kmalloc(sizeof(struct bpf_insn) * new_len);
 - *   bpf_convert_filter(old_prog, old_len, new_prog, &new_len);
 - *
 - * User BPF's register A is mapped to our BPF register 6, user BPF
 - * register X is mapped to BPF register 7; frame pointer is always
 - * register 10; Context 'void *ctx' is stored in register 1, that is,
 - * for socket filters: ctx == 'struct sk_buff *', for seccomp:
 - * ctx == 'struct seccomp_data *'.
 + * Decode and apply filter instructions to the skb->data.
 + * Return length to keep, 0 for none. @skb is the data we are
 + * filtering, @filter is the array of filter instructions.
 + * Because all jumps are guaranteed to be before last instruction,
 + * and last instruction guaranteed to be a RET, we dont need to check
 + * flen. (We used to pass to this function the length of filter)
   */
 -static int bpf_convert_filter(struct sock_filter *prog, int len,
 -			      struct bpf_insn *new_prog, int *new_len)
 +unsigned int sk_run_filter(const struct sk_buff *skb,
 +			   const struct sock_filter *fentry)
  {
 -	int new_flen = 0, pass = 0, target, i;
 -	struct bpf_insn *new_insn;
 -	struct sock_filter *fp;
 -	int *addrs = NULL;
 -	u8 bpf_src;
 -
 -	BUILD_BUG_ON(BPF_MEMWORDS * sizeof(u32) > MAX_BPF_STACK);
 -	BUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);
 -
 -	if (len <= 0 || len > BPF_MAXINSNS)
 -		return -EINVAL;
 -
 -	if (new_prog) {
 -		addrs = kcalloc(len, sizeof(*addrs),
 -				GFP_KERNEL | __GFP_NOWARN);
 -		if (!addrs)
 -			return -ENOMEM;
 -	}
 -
 -do_pass:
 -	new_insn = new_prog;
 -	fp = prog;
 -
 -	if (new_insn)
 -		*new_insn = BPF_MOV64_REG(BPF_REG_CTX, BPF_REG_ARG1);
 -	new_insn++;
 -
 -	for (i = 0; i < len; fp++, i++) {
 -		struct bpf_insn tmp_insns[6] = { };
 -		struct bpf_insn *insn = tmp_insns;
 -
 -		if (addrs)
 -			addrs[i] = new_insn - new_prog;
 -
 -		switch (fp->code) {
 -		/* All arithmetic insns and skb loads map as-is. */
 -		case BPF_ALU | BPF_ADD | BPF_X:
 -		case BPF_ALU | BPF_ADD | BPF_K:
 -		case BPF_ALU | BPF_SUB | BPF_X:
 -		case BPF_ALU | BPF_SUB | BPF_K:
 -		case BPF_ALU | BPF_AND | BPF_X:
 -		case BPF_ALU | BPF_AND | BPF_K:
 -		case BPF_ALU | BPF_OR | BPF_X:
 -		case BPF_ALU | BPF_OR | BPF_K:
 -		case BPF_ALU | BPF_LSH | BPF_X:
 -		case BPF_ALU | BPF_LSH | BPF_K:
 -		case BPF_ALU | BPF_RSH | BPF_X:
 -		case BPF_ALU | BPF_RSH | BPF_K:
 -		case BPF_ALU | BPF_XOR | BPF_X:
 -		case BPF_ALU | BPF_XOR | BPF_K:
 -		case BPF_ALU | BPF_MUL | BPF_X:
 -		case BPF_ALU | BPF_MUL | BPF_K:
 -		case BPF_ALU | BPF_DIV | BPF_X:
 -		case BPF_ALU | BPF_DIV | BPF_K:
 -		case BPF_ALU | BPF_MOD | BPF_X:
 -		case BPF_ALU | BPF_MOD | BPF_K:
 -		case BPF_ALU | BPF_NEG:
 -		case BPF_LD | BPF_ABS | BPF_W:
 -		case BPF_LD | BPF_ABS | BPF_H:
 -		case BPF_LD | BPF_ABS | BPF_B:
 -		case BPF_LD | BPF_IND | BPF_W:
 -		case BPF_LD | BPF_IND | BPF_H:
 -		case BPF_LD | BPF_IND | BPF_B:
 -			/* Check for overloaded BPF extension and
 -			 * directly convert it if found, otherwise
 -			 * just move on with mapping.
 -			 */
 -			if (BPF_CLASS(fp->code) == BPF_LD &&
 -			    BPF_MODE(fp->code) == BPF_ABS &&
 -			    convert_bpf_extensions(fp, &insn))
 -				break;
 -
 -			*insn = BPF_RAW_INSN(fp->code, BPF_REG_A, BPF_REG_X, 0, fp->k);
 -			break;
 -
 -		/* Jump transformation cannot use BPF block macros
 -		 * everywhere as offset calculation and target updates
 -		 * require a bit more work than the rest, i.e. jump
 -		 * opcodes map as-is, but offsets need adjustment.
 -		 */
 +	void *ptr;
 +	u32 A = 0;			/* Accumulator */
 +	u32 X = 0;			/* Index Register */
 +	u32 mem[BPF_MEMWORDS];		/* Scratch Memory Store */
 +	u32 tmp;
 +	int k;
  
 -#define BPF_EMIT_JMP							\
 -	do {								\
 -		if (target >= len || target < 0)			\
 -			goto err;					\
 -		insn->off = addrs ? addrs[target] - addrs[i] - 1 : 0;	\
 -		/* Adjust pc relative offset for 2nd or 3rd insn. */	\
 -		insn->off -= insn - tmp_insns;				\
 -	} while (0)
 -
 -		case BPF_JMP | BPF_JA:
 -			target = i + fp->k + 1;
 -			insn->code = fp->code;
 -			BPF_EMIT_JMP;
 -			break;
 +	/*
 +	 * Process array of filter instructions.
 +	 */
 +	for (;; fentry++) {
 +#if defined(CONFIG_X86_32)
 +#define	K (fentry->k)
 +#else
 +		const u32 K = fentry->k;
 +#endif
  
 -		case BPF_JMP | BPF_JEQ | BPF_K:
 -		case BPF_JMP | BPF_JEQ | BPF_X:
 -		case BPF_JMP | BPF_JSET | BPF_K:
 -		case BPF_JMP | BPF_JSET | BPF_X:
 -		case BPF_JMP | BPF_JGT | BPF_K:
 -		case BPF_JMP | BPF_JGT | BPF_X:
 -		case BPF_JMP | BPF_JGE | BPF_K:
 -		case BPF_JMP | BPF_JGE | BPF_X:
 -			if (BPF_SRC(fp->code) == BPF_K && (int) fp->k < 0) {
 -				/* BPF immediates are signed, zero extend
 -				 * immediate into tmp register and use it
 -				 * in compare insn.
 -				 */
 -				*insn++ = BPF_MOV32_IMM(BPF_REG_TMP, fp->k);
 -
 -				insn->dst_reg = BPF_REG_A;
 -				insn->src_reg = BPF_REG_TMP;
 -				bpf_src = BPF_X;
 -			} else {
 -				insn->dst_reg = BPF_REG_A;
 -				insn->imm = fp->k;
 -				bpf_src = BPF_SRC(fp->code);
 -				insn->src_reg = bpf_src == BPF_X ? BPF_REG_X : 0;
 +		switch (fentry->code) {
 +		case BPF_S_ALU_ADD_X:
 +			A += X;
 +			continue;
 +		case BPF_S_ALU_ADD_K:
 +			A += K;
 +			continue;
 +		case BPF_S_ALU_SUB_X:
 +			A -= X;
 +			continue;
 +		case BPF_S_ALU_SUB_K:
 +			A -= K;
 +			continue;
 +		case BPF_S_ALU_MUL_X:
 +			A *= X;
 +			continue;
 +		case BPF_S_ALU_MUL_K:
 +			A *= K;
 +			continue;
 +		case BPF_S_ALU_DIV_X:
 +			if (X == 0)
 +				return 0;
 +			A /= X;
 +			continue;
 +		case BPF_S_ALU_DIV_K:
 +			A /= K;
 +			continue;
 +		case BPF_S_ALU_MOD_X:
 +			if (X == 0)
 +				return 0;
 +			A %= X;
 +			continue;
 +		case BPF_S_ALU_MOD_K:
 +			A %= K;
 +			continue;
 +		case BPF_S_ALU_AND_X:
 +			A &= X;
 +			continue;
 +		case BPF_S_ALU_AND_K:
 +			A &= K;
 +			continue;
 +		case BPF_S_ALU_OR_X:
 +			A |= X;
 +			continue;
 +		case BPF_S_ALU_OR_K:
 +			A |= K;
 +			continue;
 +		case BPF_S_ANC_ALU_XOR_X:
 +		case BPF_S_ALU_XOR_X:
 +			A ^= X;
 +			continue;
 +		case BPF_S_ALU_XOR_K:
 +			A ^= K;
 +			continue;
 +		case BPF_S_ALU_LSH_X:
 +			A <<= X;
 +			continue;
 +		case BPF_S_ALU_LSH_K:
 +			A <<= K;
 +			continue;
 +		case BPF_S_ALU_RSH_X:
 +			A >>= X;
 +			continue;
 +		case BPF_S_ALU_RSH_K:
 +			A >>= K;
 +			continue;
 +		case BPF_S_ALU_NEG:
 +			A = -A;
 +			continue;
 +		case BPF_S_JMP_JA:
 +			fentry += K;
 +			continue;
 +		case BPF_S_JMP_JGT_K:
 +			fentry += (A > K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_K:
 +			fentry += (A >= K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_K:
 +			fentry += (A == K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_K:
 +			fentry += (A & K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGT_X:
 +			fentry += (A > X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_X:
 +			fentry += (A >= X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_X:
 +			fentry += (A == X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_X:
 +			fentry += (A & X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_LD_W_ABS:
 +			k = K;
 +load_w:
 +			ptr = load_pointer(skb, k, 4, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be32(ptr);
 +				continue;
  			}
 -
 -			/* Common case where 'jump_false' is next insn. */
 -			if (fp->jf == 0) {
 -				insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -				target = i + fp->jt + 1;
 -				BPF_EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_H_ABS:
 +			k = K;
 +load_h:
 +			ptr = load_pointer(skb, k, 2, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be16(ptr);
 +				continue;
  			}
 -
 -			/* Convert JEQ into JNE when 'jump_true' is next insn. */
 -			if (fp->jt == 0 && BPF_OP(fp->code) == BPF_JEQ) {
 -				insn->code = BPF_JMP | BPF_JNE | bpf_src;
 -				target = i + fp->jf + 1;
 -				BPF_EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_B_ABS:
 +			k = K;
 +load_b:
 +			ptr = load_pointer(skb, k, 1, &tmp);
 +			if (ptr != NULL) {
 +				A = *(u8 *)ptr;
 +				continue;
  			}
 +			return 0;
 +		case BPF_S_LD_W_LEN:
 +			A = skb->len;
 +			continue;
 +		case BPF_S_LDX_W_LEN:
 +			X = skb->len;
 +			continue;
 +		case BPF_S_LD_W_IND:
 +			k = X + K;
 +			goto load_w;
 +		case BPF_S_LD_H_IND:
 +			k = X + K;
 +			goto load_h;
 +		case BPF_S_LD_B_IND:
 +			k = X + K;
 +			goto load_b;
 +		case BPF_S_LDX_B_MSH:
 +			ptr = load_pointer(skb, K, 1, &tmp);
 +			if (ptr != NULL) {
 +				X = (*(u8 *)ptr & 0xf) << 2;
 +				continue;
 +			}
 +			return 0;
 +		case BPF_S_LD_IMM:
 +			A = K;
 +			continue;
 +		case BPF_S_LDX_IMM:
 +			X = K;
 +			continue;
 +		case BPF_S_LD_MEM:
 +			A = mem[K];
 +			continue;
 +		case BPF_S_LDX_MEM:
 +			X = mem[K];
 +			continue;
 +		case BPF_S_MISC_TAX:
 +			X = A;
 +			continue;
 +		case BPF_S_MISC_TXA:
 +			A = X;
 +			continue;
 +		case BPF_S_RET_K:
 +			return K;
 +		case BPF_S_RET_A:
 +			return A;
 +		case BPF_S_ST:
 +			mem[K] = A;
 +			continue;
 +		case BPF_S_STX:
 +			mem[K] = X;
 +			continue;
 +		case BPF_S_ANC_PROTOCOL:
 +			A = ntohs(skb->protocol);
 +			continue;
 +		case BPF_S_ANC_PKTTYPE:
 +			A = skb->pkt_type;
 +			continue;
 +		case BPF_S_ANC_IFINDEX:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->ifindex;
 +			continue;
 +		case BPF_S_ANC_MARK:
 +			A = skb->mark;
 +			continue;
 +		case BPF_S_ANC_QUEUE:
 +			A = skb->queue_mapping;
 +			continue;
 +		case BPF_S_ANC_HATYPE:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->type;
 +			continue;
 +		case BPF_S_ANC_RXHASH:
 +			A = skb->hash;
 +			continue;
 +		case BPF_S_ANC_CPU:
 +			A = raw_smp_processor_id();
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG:
 +			A = skb_vlan_tag_get(skb);
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG_PRESENT:
 +			A = !!skb_vlan_tag_present(skb);
 +			continue;
 +		case BPF_S_ANC_PAY_OFFSET:
 +			A = skb_get_poff(skb);
 +			continue;
 +		case BPF_S_ANC_NLATTR: {
 +			struct nlattr *nla;
 +
 +			if (skb_is_nonlinear(skb))
 +				return 0;
 +
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
 +
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
 +
 +			nla = nla_find((struct nlattr *)&skb->data[A],
 +				       skb->len - A, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +		case BPF_S_ANC_NLATTR_NEST: {
 +			struct nlattr *nla;
  
 -			/* Other jumps are mapped into two insns: Jxx and JA. */
 -			target = i + fp->jt + 1;
 -			insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -			BPF_EMIT_JMP;
 -			insn++;
 -
 -			insn->code = BPF_JMP | BPF_JA;
 -			target = i + fp->jf + 1;
 -			BPF_EMIT_JMP;
 -			break;
 -
 -		/* ldxb 4 * ([14] & 0xf) is remaped into 6 insns. */
 -		case BPF_LDX | BPF_MSH | BPF_B:
 -			/* tmp = A */
 -			*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_A);
 -			/* A = BPF_R0 = *(u8 *) (skb->data + K) */
 -			*insn++ = BPF_LD_ABS(BPF_B, fp->k);
 -			/* A &= 0xf */
 -			*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 0xf);
 -			/* A <<= 2 */
 -			*insn++ = BPF_ALU32_IMM(BPF_LSH, BPF_REG_A, 2);
 -			/* X = A */
 -			*insn++ = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);
 -			/* A = tmp */
 -			*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_TMP);
 -			break;
 -
 -		/* RET_K, RET_A are remaped into 2 insns. */
 -		case BPF_RET | BPF_A:
 -		case BPF_RET | BPF_K:
 -			*insn++ = BPF_MOV32_RAW(BPF_RVAL(fp->code) == BPF_K ?
 -						BPF_K : BPF_X, BPF_REG_0,
 -						BPF_REG_A, fp->k);
 -			*insn = BPF_EXIT_INSN();
 -			break;
 -
 -		/* Store to stack. */
 -		case BPF_ST:
 -		case BPF_STX:
 -			*insn = BPF_STX_MEM(BPF_W, BPF_REG_FP, BPF_CLASS(fp->code) ==
 -					    BPF_ST ? BPF_REG_A : BPF_REG_X,
 -					    -(BPF_MEMWORDS - fp->k) * 4);
 -			break;
 -
 -		/* Load from stack. */
 -		case BPF_LD | BPF_MEM:
 -		case BPF_LDX | BPF_MEM:
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD  ?
 -					    BPF_REG_A : BPF_REG_X, BPF_REG_FP,
 -					    -(BPF_MEMWORDS - fp->k) * 4);
 -			break;
 -
 -		/* A = K or X = K */
 -		case BPF_LD | BPF_IMM:
 -		case BPF_LDX | BPF_IMM:
 -			*insn = BPF_MOV32_IMM(BPF_CLASS(fp->code) == BPF_LD ?
 -					      BPF_REG_A : BPF_REG_X, fp->k);
 -			break;
 -
 -		/* X = A */
 -		case BPF_MISC | BPF_TAX:
 -			*insn = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);
 -			break;
 +			if (skb_is_nonlinear(skb))
 +				return 0;
  
 -		/* A = X */
 -		case BPF_MISC | BPF_TXA:
 -			*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_X);
 -			break;
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
  
 -		/* A = skb->len or X = skb->len */
 -		case BPF_LD | BPF_W | BPF_LEN:
 -		case BPF_LDX | BPF_W | BPF_LEN:
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD ?
 -					    BPF_REG_A : BPF_REG_X, BPF_REG_CTX,
 -					    offsetof(struct sk_buff, len));
 -			break;
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
  
 -		/* Access seccomp_data fields. */
 -		case BPF_LDX | BPF_ABS | BPF_W:
 -			/* A = *(u32 *) (ctx + K) */
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX, fp->k);
 -			break;
 +			nla = (struct nlattr *)&skb->data[A];
 +			if (nla->nla_len > skb->len - A)
 +				return 0;
  
 -		/* Unknown instruction. */
 +			nla = nla_find_nested(nla, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +#ifdef CONFIG_SECCOMP_FILTER
 +		case BPF_S_ANC_SECCOMP_LD_W:
 +			A = seccomp_bpf_load(fentry->k);
 +			continue;
 +#endif
  		default:
 -			goto err;
 +			WARN_RATELIMIT(1, "Unknown code:%u jt:%u tf:%u k:%u\n",
 +				       fentry->code, fentry->jt,
 +				       fentry->jf, fentry->k);
 +			return 0;
  		}
 -
 -		insn++;
 -		if (new_prog)
 -			memcpy(new_insn, tmp_insns,
 -			       sizeof(*insn) * (insn - tmp_insns));
 -		new_insn += insn - tmp_insns;
 -	}
 -
 -	if (!new_prog) {
 -		/* Only calculating new length. */
 -		*new_len = new_insn - new_prog;
 -		return 0;
  	}
  
 -	pass++;
 -	if (new_flen != new_insn - new_prog) {
 -		new_flen = new_insn - new_prog;
 -		if (pass > 2)
 -			goto err;
 -		goto do_pass;
 -	}
 -
 -	kfree(addrs);
 -	BUG_ON(*new_len != new_flen);
  	return 0;
 -err:
 -	kfree(addrs);
 -	return -EINVAL;
  }
 +EXPORT_SYMBOL(sk_run_filter);
  
 -/* Security:
 - *
 +/*
 + * Security :
 + * A BPF program is able to use 16 cells of memory to store intermediate
 + * values (check u32 mem[BPF_MEMWORDS] in sk_run_filter())
   * As we dont want to clear mem[] array for each packet going through
 - * __bpf_prog_run(), we check that filter loaded by user never try to read
 + * sk_run_filter(), we check that filter loaded by user never try to read
   * a cell if not previously written, and we check all branches to be sure
   * a malicious user doesn't try to abuse us.
   */
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/helpers.c
* Unmerged path kernel/bpf/syscall.c
* Unmerged path include/linux/bpf.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/helpers.c
* Unmerged path kernel/bpf/syscall.c
* Unmerged path net/core/filter.c
