iommu/iova: Add flush counters to Flush-Queue implementation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iommu] iova: Add flush counters to Flush-Queue implementation (Jerry Snitselaar) [1519117]
Rebuild_FUZZ: 94.74%
commit-author Joerg Roedel <jroedel@suse.de>
commit fb418dab8a4f01dde0c025d15145c589ec02796b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/fb418dab.failed

There are two counters:

	* fq_flush_start_cnt  - Increased when a TLB flush
	                        is started.

	* fq_flush_finish_cnt - Increased when a TLB flush
				is finished.

The fq_flush_start_cnt is assigned to every Flush-Queue
entry on its creation. When freeing entries from the
Flush-Queue, the value in the entry is compared to the
fq_flush_finish_cnt. The entry can only be freed when its
value is less than the value of fq_flush_finish_cnt.

The reason for these counters it to take advantage of IOMMU
TLB flushes that happened on other CPUs. These already
flushed the TLB for Flush-Queue entries on other CPUs so
that they can already be freed without flushing the TLB
again.

This makes it less likely that the Flush-Queue is full and
saves IOMMU TLB flushes.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit fb418dab8a4f01dde0c025d15145c589ec02796b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
#	include/linux/iova.h
diff --cc drivers/iommu/iova.c
index f106fd9782bf,47b144e417ad..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -53,6 -57,46 +53,49 @@@ init_iova_domain(struct iova_domain *io
  }
  EXPORT_SYMBOL_GPL(init_iova_domain);
  
++<<<<<<< HEAD
++=======
+ static void free_iova_flush_queue(struct iova_domain *iovad)
+ {
+ 	if (!iovad->fq)
+ 		return;
+ 
+ 	fq_destroy_all_entries(iovad);
+ 	free_percpu(iovad->fq);
+ 
+ 	iovad->fq         = NULL;
+ 	iovad->flush_cb   = NULL;
+ 	iovad->entry_dtor = NULL;
+ }
+ 
+ int init_iova_flush_queue(struct iova_domain *iovad,
+ 			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor)
+ {
+ 	int cpu;
+ 
+ 	atomic64_set(&iovad->fq_flush_start_cnt,  0);
+ 	atomic64_set(&iovad->fq_flush_finish_cnt, 0);
+ 
+ 	iovad->fq = alloc_percpu(struct iova_fq);
+ 	if (!iovad->fq)
+ 		return -ENOMEM;
+ 
+ 	iovad->flush_cb   = flush_cb;
+ 	iovad->entry_dtor = entry_dtor;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq;
+ 
+ 		fq = per_cpu_ptr(iovad->fq, cpu);
+ 		fq->head = 0;
+ 		fq->tail = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(init_iova_flush_queue);
+ 
++>>>>>>> fb418dab8a4f (iommu/iova: Add flush counters to Flush-Queue implementation)
  static struct rb_node *
  __get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)
  {
@@@ -449,6 -466,102 +492,105 @@@ free_iova_fast(struct iova_domain *iova
  }
  EXPORT_SYMBOL_GPL(free_iova_fast);
  
++<<<<<<< HEAD
++=======
+ #define fq_ring_for_each(i, fq) \
+ 	for ((i) = (fq)->head; (i) != (fq)->tail; (i) = ((i) + 1) % IOVA_FQ_SIZE)
+ 
+ static inline bool fq_full(struct iova_fq *fq)
+ {
+ 	return (((fq->tail + 1) % IOVA_FQ_SIZE) == fq->head);
+ }
+ 
+ static inline unsigned fq_ring_add(struct iova_fq *fq)
+ {
+ 	unsigned idx = fq->tail;
+ 
+ 	fq->tail = (idx + 1) % IOVA_FQ_SIZE;
+ 
+ 	return idx;
+ }
+ 
+ static void fq_ring_free(struct iova_domain *iovad, struct iova_fq *fq)
+ {
+ 	u64 counter = atomic64_read(&iovad->fq_flush_finish_cnt);
+ 	unsigned idx;
+ 
+ 	fq_ring_for_each(idx, fq) {
+ 
+ 		if (fq->entries[idx].counter >= counter)
+ 			break;
+ 
+ 		if (iovad->entry_dtor)
+ 			iovad->entry_dtor(fq->entries[idx].data);
+ 
+ 		free_iova_fast(iovad,
+ 			       fq->entries[idx].iova_pfn,
+ 			       fq->entries[idx].pages);
+ 
+ 		fq->head = (fq->head + 1) % IOVA_FQ_SIZE;
+ 	}
+ }
+ 
+ static void iova_domain_flush(struct iova_domain *iovad)
+ {
+ 	atomic64_inc(&iovad->fq_flush_start_cnt);
+ 	iovad->flush_cb(iovad);
+ 	atomic64_inc(&iovad->fq_flush_finish_cnt);
+ }
+ 
+ static void fq_destroy_all_entries(struct iova_domain *iovad)
+ {
+ 	int cpu;
+ 
+ 	/*
+ 	 * This code runs when the iova_domain is being detroyed, so don't
+ 	 * bother to free iovas, just call the entry_dtor on all remaining
+ 	 * entries.
+ 	 */
+ 	if (!iovad->entry_dtor)
+ 		return;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq = per_cpu_ptr(iovad->fq, cpu);
+ 		int idx;
+ 
+ 		fq_ring_for_each(idx, fq)
+ 			iovad->entry_dtor(fq->entries[idx].data);
+ 	}
+ }
+ 
+ void queue_iova(struct iova_domain *iovad,
+ 		unsigned long pfn, unsigned long pages,
+ 		unsigned long data)
+ {
+ 	struct iova_fq *fq = get_cpu_ptr(iovad->fq);
+ 	unsigned idx;
+ 
+ 	/*
+ 	 * First remove all entries from the flush queue that have already been
+ 	 * flushed out on another CPU. This makes the fq_full() check below less
+ 	 * likely to be true.
+ 	 */
+ 	fq_ring_free(iovad, fq);
+ 
+ 	if (fq_full(fq)) {
+ 		iova_domain_flush(iovad);
+ 		fq_ring_free(iovad, fq);
+ 	}
+ 
+ 	idx = fq_ring_add(fq);
+ 
+ 	fq->entries[idx].iova_pfn = pfn;
+ 	fq->entries[idx].pages    = pages;
+ 	fq->entries[idx].data     = data;
+ 	fq->entries[idx].counter  = atomic64_read(&iovad->fq_flush_start_cnt);
+ 
+ 	put_cpu_ptr(iovad->fq);
+ }
+ EXPORT_SYMBOL_GPL(queue_iova);
+ 
++>>>>>>> fb418dab8a4f (iommu/iova: Add flush counters to Flush-Queue implementation)
  /**
   * put_iova_domain - destroys the iova doamin
   * @iovad: - iova domain in question.
diff --cc include/linux/iova.h
index f27bb2c62fca,985b8008999e..000000000000
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@@ -36,6 -37,31 +37,34 @@@ struct iova_rcache 
  	struct iova_cpu_rcache __percpu *cpu_rcaches;
  };
  
++<<<<<<< HEAD
++=======
+ struct iova_domain;
+ 
+ /* Call-Back from IOVA code into IOMMU drivers */
+ typedef void (* iova_flush_cb)(struct iova_domain *domain);
+ 
+ /* Destructor for per-entry data */
+ typedef void (* iova_entry_dtor)(unsigned long data);
+ 
+ /* Number of entries per Flush Queue */
+ #define IOVA_FQ_SIZE	256
+ 
+ /* Flush Queue entry for defered flushing */
+ struct iova_fq_entry {
+ 	unsigned long iova_pfn;
+ 	unsigned long pages;
+ 	unsigned long data;
+ 	u64 counter; /* Flush counter when this entrie was added */
+ };
+ 
+ /* Per-CPU Flush Queue structure */
+ struct iova_fq {
+ 	struct iova_fq_entry entries[IOVA_FQ_SIZE];
+ 	unsigned head, tail;
+ };
+ 
++>>>>>>> fb418dab8a4f (iommu/iova: Add flush counters to Flush-Queue implementation)
  /* holds all the iova translations for a domain */
  struct iova_domain {
  	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
@@@ -45,6 -71,20 +74,23 @@@
  	unsigned long	start_pfn;	/* Lower limit for this domain */
  	unsigned long	dma_32bit_pfn;
  	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
++<<<<<<< HEAD
++=======
+ 
+ 	iova_flush_cb	flush_cb;	/* Call-Back function to flush IOMMU
+ 					   TLBs */
+ 
+ 	iova_entry_dtor entry_dtor;	/* IOMMU driver specific destructor for
+ 					   iova entry */
+ 
+ 	struct iova_fq __percpu *fq;	/* Flush Queue */
+ 
+ 	atomic64_t	fq_flush_start_cnt;	/* Number of TLB flushes that
+ 						   have been started */
+ 
+ 	atomic64_t	fq_flush_finish_cnt;	/* Number of TLB flushes that
+ 						   have been finished */
++>>>>>>> fb418dab8a4f (iommu/iova: Add flush counters to Flush-Queue implementation)
  };
  
  static inline unsigned long iova_size(struct iova *iova)
* Unmerged path drivers/iommu/iova.c
* Unmerged path include/linux/iova.h
