mm: extract code to fault in a page from __get_user_pages()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] extract code to fault in a page from __get_user_pages() (Rafael Aquini) [1560030]
Rebuild_FUZZ: 96.49%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit 1674448345cdb56e724483a2a26622771f4e3a10
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/16744483.failed

Nesting level in __get_user_pages() is just insane. Let's try to fix it
a bit.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1674448345cdb56e724483a2a26622771f4e3a10)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/gup.c
diff --cc mm/gup.c
index 20b926f646c5,28e370068ffe..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -269,21 -119,200 +269,182 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -/**
 - * follow_page_mask - look up a page descriptor from a user-virtual address
 - * @vma: vm_area_struct mapping @address
 - * @address: virtual address to look up
 - * @flags: flags modifying lookup behaviour
 - * @page_mask: on output, *page_mask is set according to the size of the page
 - *
 - * @flags can have FOLL_ flags set, defined in <linux/mm.h>
 - *
 - * Returns the mapped (struct page *), %NULL if no mapping exists, or
 - * an error pointer if there is a mapping to something not represented
 - * by a page descriptor (see also vm_normal_page()).
 - */
 -struct page *follow_page_mask(struct vm_area_struct *vma,
 -			      unsigned long address, unsigned int flags,
 -			      unsigned int *page_mask)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	*page_mask = 0;
 -
 -	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 -	if (!IS_ERR(page)) {
 -		BUG_ON(flags & FOLL_GET);
  		return page;
 -	}
  
++<<<<<<< HEAD
 +no_page_table:
 +	/*
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
 +	 */
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
 +	return page;
++=======
+ 	pgd = pgd_offset(mm, address);
+ 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pud = pud_offset(pgd, address);
+ 	if (pud_none(*pud))
+ 		return no_page_table(vma, flags);
+ 	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
+ 		if (flags & FOLL_GET)
+ 			return NULL;
+ 		page = follow_huge_pud(mm, address, pud, flags & FOLL_WRITE);
+ 		return page;
+ 	}
+ 	if (unlikely(pud_bad(*pud)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pmd(mm, address, pmd, flags & FOLL_WRITE);
+ 		if (flags & FOLL_GET) {
+ 			/*
+ 			 * Refcount on tail pages are not well-defined and
+ 			 * shouldn't be taken. The caller should handle a NULL
+ 			 * return when trying to follow tail pages.
+ 			 */
+ 			if (PageHead(page))
+ 				get_page(page);
+ 			else
+ 				page = NULL;
+ 		}
+ 		return page;
+ 	}
+ 	if ((flags & FOLL_NUMA) && pmd_numa(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_trans_huge(*pmd)) {
+ 		if (flags & FOLL_SPLIT) {
+ 			split_huge_page_pmd(vma, address, pmd);
+ 			return follow_page_pte(vma, address, pmd, flags);
+ 		}
+ 		ptl = pmd_lock(mm, pmd);
+ 		if (likely(pmd_trans_huge(*pmd))) {
+ 			if (unlikely(pmd_trans_splitting(*pmd))) {
+ 				spin_unlock(ptl);
+ 				wait_split_huge_page(vma->anon_vma, pmd);
+ 			} else {
+ 				page = follow_trans_huge_pmd(vma, address,
+ 							     pmd, flags);
+ 				spin_unlock(ptl);
+ 				*page_mask = HPAGE_PMD_NR - 1;
+ 				return page;
+ 			}
+ 		} else
+ 			spin_unlock(ptl);
+ 	}
+ 	return follow_page_pte(vma, address, pmd, flags);
+ }
+ 
+ static int get_gate_page(struct mm_struct *mm, unsigned long address,
+ 		unsigned int gup_flags, struct vm_area_struct **vma,
+ 		struct page **page)
+ {
+ 	pgd_t *pgd;
+ 	pud_t *pud;
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 	int ret = -EFAULT;
+ 
+ 	/* user gate pages are read-only */
+ 	if (gup_flags & FOLL_WRITE)
+ 		return -EFAULT;
+ 	if (address > TASK_SIZE)
+ 		pgd = pgd_offset_k(address);
+ 	else
+ 		pgd = pgd_offset_gate(mm, address);
+ 	BUG_ON(pgd_none(*pgd));
+ 	pud = pud_offset(pgd, address);
+ 	BUG_ON(pud_none(*pud));
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return -EFAULT;
+ 	VM_BUG_ON(pmd_trans_huge(*pmd));
+ 	pte = pte_offset_map(pmd, address);
+ 	if (pte_none(*pte))
+ 		goto unmap;
+ 	*vma = get_gate_vma(mm);
+ 	if (!page)
+ 		goto out;
+ 	*page = vm_normal_page(*vma, address, *pte);
+ 	if (!*page) {
+ 		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
+ 			goto unmap;
+ 		*page = pte_page(*pte);
+ 	}
+ 	get_page(*page);
+ out:
+ 	ret = 0;
+ unmap:
+ 	pte_unmap(pte);
+ 	return ret;
++>>>>>>> 1674448345cd (mm: extract code to fault in a page from __get_user_pages())
+ }
+ 
+ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
+ 		unsigned long address, unsigned int *flags, int *nonblocking)
+ {
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned int fault_flags = 0;
+ 	int ret;
+ 
+ 	/* For mlock, just skip the stack guard page. */
+ 	if ((*flags & FOLL_MLOCK) &&
+ 			(stack_guard_page_start(vma, address) ||
+ 			 stack_guard_page_end(vma, address + PAGE_SIZE)))
+ 		return -ENOENT;
+ 	if (*flags & FOLL_WRITE)
+ 		fault_flags |= FAULT_FLAG_WRITE;
+ 	if (nonblocking)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
+ 	if (*flags & FOLL_NOWAIT)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
+ 
+ 	ret = handle_mm_fault(mm, vma, address, fault_flags);
+ 	if (ret & VM_FAULT_ERROR) {
+ 		if (ret & VM_FAULT_OOM)
+ 			return -ENOMEM;
+ 		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
+ 			return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;
+ 		if (ret & VM_FAULT_SIGBUS)
+ 			return -EFAULT;
+ 		BUG();
+ 	}
+ 
+ 	if (tsk) {
+ 		if (ret & VM_FAULT_MAJOR)
+ 			tsk->maj_flt++;
+ 		else
+ 			tsk->min_flt++;
+ 	}
+ 
+ 	if (ret & VM_FAULT_RETRY) {
+ 		if (nonblocking)
+ 			*nonblocking = 0;
+ 		return -EBUSY;
+ 	}
+ 
+ 	/*
+ 	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
+ 	 * necessary, even if maybe_mkwrite decided not to set pte_write. We
+ 	 * can thus safely do subsequent page lookups as if they were reads.
+ 	 * But only do so when looping for pte_write is futile: in some cases
+ 	 * userspace may also be wanting to write to the gotten user page,
+ 	 * which a read fault here might prevent (a readonly page might get
+ 	 * reCOWed by userspace write).
+ 	 */
+ 	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
+ 		*flags &= ~FOLL_WRITE;
+ 	return 0;
  }
  
  /**
@@@ -461,82 -461,26 +622,101 @@@ long __get_user_pages(struct task_struc
  			while (!(page = follow_page_mask(vma, start,
  						foll_flags, &page_mask))) {
  				int ret;
++<<<<<<< HEAD
 +				unsigned int fault_flags = 0;
 +
 +				if (foll_flags & FOLL_WRITE)
 +					fault_flags |= FAULT_FLAG_WRITE;
 +				if (foll_flags & FOLL_REMOTE)
 +					fault_flags |= FAULT_FLAG_REMOTE;
 +				if (nonblocking)
 +					fault_flags |= FAULT_FLAG_ALLOW_RETRY;
 +				if (foll_flags & FOLL_NOWAIT)
 +					fault_flags |= (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT);
 +				if (foll_flags & FOLL_TRIED) {
 +					WARN_ON_ONCE(fault_flags &
 +						     FAULT_FLAG_ALLOW_RETRY);
 +					fault_flags |= FAULT_FLAG_TRIED;
 +				}
 +
 +				ret = handle_mm_fault(vma, start,
 +							fault_flags);
 +
 +				if (ret & VM_FAULT_ERROR) {
 +					if (ret & VM_FAULT_OOM)
 +						return i ? i : -ENOMEM;
 +					if (ret & (VM_FAULT_HWPOISON |
 +						   VM_FAULT_HWPOISON_LARGE)) {
 +						if (i)
 +							return i;
 +						else if (gup_flags & FOLL_HWPOISON)
 +							return -EHWPOISON;
 +						else
 +							return -EFAULT;
 +					}
 +					if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
 +						return i ? i : -EFAULT;
 +					BUG();
 +				}
 +
 +				if (tsk) {
 +					if (ret & VM_FAULT_MAJOR)
 +						tsk->maj_flt++;
 +					else
 +						tsk->min_flt++;
 +				}
 +
 +				if (ret & VM_FAULT_RETRY) {
 +					if (nonblocking)
 +						*nonblocking = 0;
 +					return i;
 +				}
 +
 +				/*
 +				 * The VM_FAULT_WRITE bit tells us that
 +				 * do_wp_page has broken COW when necessary,
 +				 * even if maybe_mkwrite decided not to set
 +				 * pte_write. We can thus safely do subsequent
 +				 * page lookups as if they were reads. But only
 +				 * do so when looping for pte_write is futile:
 +				 * in some cases userspace may also be wanting
 +				 * to write to the gotten user page, which a
 +				 * read fault here might prevent (a readonly
 +				 * page might get reCOWed by userspace write).
 +				 */
 +				if ((ret & VM_FAULT_WRITE) &&
 +				    !(vma->vm_flags & VM_WRITE))
 +					foll_flags |= FOLL_COW;
 +
++=======
+ 				ret = faultin_page(tsk, vma, start, &foll_flags,
+ 						nonblocking);
+ 				switch (ret) {
+ 				case 0:
+ 					break;
+ 				case -EFAULT:
+ 				case -ENOMEM:
+ 				case -EHWPOISON:
+ 					return i ? i : ret;
+ 				case -EBUSY:
+ 					return i;
+ 				case -ENOENT:
+ 					goto next_page;
+ 				default:
+ 					BUG();
+ 				}
++>>>>>>> 1674448345cd (mm: extract code to fault in a page from __get_user_pages())
  				cond_resched();
  			}
 -			if (IS_ERR(page))
 +			if (PTR_ERR(page) == -EEXIST) {
 +				/*
 +				 * Proper page table entry exists, but
 +				 * no corresponding struct page.
 +				 */
 +				goto next_page;
 +			} else if (IS_ERR(page)) {
  				return i ? i : PTR_ERR(page);
 +			}
  			if (pages) {
  				pages[i] = page;
  
* Unmerged path mm/gup.c
