x86/l1tf: Handle EPT disabled state proper

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] l1tf: handle ept disabled state proper (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 95.00%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit a7b9020b06ec6d7c3f3b0d4ef1a9eba12654f4f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a7b9020b.failed

If Extended Page Tables (EPT) are disabled or not supported, no L1D
flushing is required. The setup function can just avoid setting up the L1D
flush for the EPT=n case.

Invoke it after the hardware setup has be done and enable_ept has the
correct state and expose the EPT disabled state in the mitigation status as
well.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Jiri Kosina <jkosina@suse.cz>
	Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
Link: https://lkml.kernel.org/r/20180713142322.612160168@linutronix.de

(cherry picked from commit a7b9020b06ec6d7c3f3b0d4ef1a9eba12654f4f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/vmx.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/include/asm/vmx.h
index 6bba1de91efd,94a8547d915b..000000000000
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@@ -526,4 -573,14 +526,17 @@@ enum vm_instruction_error_number 
  	VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID = 28,
  };
  
++<<<<<<< HEAD
++=======
+ enum vmx_l1d_flush_state {
+ 	VMENTER_L1D_FLUSH_AUTO,
+ 	VMENTER_L1D_FLUSH_NEVER,
+ 	VMENTER_L1D_FLUSH_COND,
+ 	VMENTER_L1D_FLUSH_ALWAYS,
+ 	VMENTER_L1D_FLUSH_EPT_DISABLED,
+ };
+ 
+ extern enum vmx_l1d_flush_state l1tf_vmx_mitigation;
+ 
++>>>>>>> a7b9020b06ec (x86/l1tf: Handle EPT disabled state proper)
  #endif
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,1b3dc586d29d..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -201,13 -659,100 +201,29 @@@ static void __init spectre_v2_select_mi
  #undef pr_fmt
  
  #ifdef CONFIG_SYSFS
++<<<<<<< HEAD
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
++=======
+ 
+ #define L1TF_DEFAULT_MSG "Mitigation: PTE Inversion"
+ 
+ #if IS_ENABLED(CONFIG_KVM_INTEL)
+ static const char *l1tf_vmx_states[] = {
+ 	[VMENTER_L1D_FLUSH_AUTO]		= "auto",
+ 	[VMENTER_L1D_FLUSH_NEVER]		= "vulnerable",
+ 	[VMENTER_L1D_FLUSH_COND]		= "conditional cache flushes",
+ 	[VMENTER_L1D_FLUSH_ALWAYS]		= "cache flushes",
+ 	[VMENTER_L1D_FLUSH_EPT_DISABLED]	= "EPT disabled",
+ };
+ 
+ static ssize_t l1tf_show_state(char *buf)
++>>>>>>> a7b9020b06ec (x86/l1tf: Handle EPT disabled state proper)
  {
 -	if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)
 -		return sprintf(buf, "%s\n", L1TF_DEFAULT_MSG);
 -
 -	return sprintf(buf, "%s; VMX: SMT %s, L1D %s\n", L1TF_DEFAULT_MSG,
 -		       cpu_smt_control == CPU_SMT_ENABLED ? "vulnerable" : "disabled",
 -		       l1tf_vmx_states[l1tf_vmx_mitigation]);
 -}
 -#else
 -static ssize_t l1tf_show_state(char *buf)
 -{
 -	return sprintf(buf, "%s\n", L1TF_DEFAULT_MSG);
 -}
 -#endif
 -
 -static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			       char *buf, unsigned int bug)
 -{
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
 -
 -	switch (bug) {
 -	case X86_BUG_CPU_MELTDOWN:
 -		if (boot_cpu_has(X86_FEATURE_PTI))
 -			return sprintf(buf, "Mitigation: PTI\n");
 -
 -		break;
 -
 -	case X86_BUG_SPECTRE_V1:
 -		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 -
 -	case X86_BUG_SPECTRE_V2:
 -		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
 -			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
 -			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 -			       spectre_v2_module_string());
 -
 -	case X86_BUG_SPEC_STORE_BYPASS:
 -		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
 -
 -	case X86_BUG_L1TF:
 -		if (boot_cpu_has(X86_FEATURE_L1TF_PTEINV))
 -			return l1tf_show_state(buf);
 -		break;
 -	default:
 -		break;
 -	}
 -
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
  	return sprintf(buf, "Vulnerable\n");
  }
  
diff --cc arch/x86/kvm/vmx.c
index e2f48f8aba96,78593550059b..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -11663,13 -13197,126 +11663,136 @@@ static struct kvm_x86_ops vmx_x86_ops 
  	.enable_smi_window = enable_smi_window,
  };
  
++<<<<<<< HEAD
 +static int __init vmx_init(void)
 +{
 +	int r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
 +                     __alignof__(struct vcpu_vmx), THIS_MODULE);
 +	if (r)
 +		return r;
 +
++=======
+ static int __init vmx_setup_l1d_flush(void)
+ {
+ 	struct page *page;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_L1TF))
+ 		return 0;
+ 
+ 	if (!enable_ept) {
+ 		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+ 		return 0;
+ 	}
+ 
+ 	l1tf_vmx_mitigation = vmentry_l1d_flush;
+ 
+ 	if (vmentry_l1d_flush == VMENTER_L1D_FLUSH_NEVER)
+ 		return 0;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_FLUSH_L1D)) {
+ 		page = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);
+ 		if (!page)
+ 			return -ENOMEM;
+ 		vmx_l1d_flush_pages = page_address(page);
+ 	}
+ 
+ 	static_branch_enable(&vmx_l1d_should_flush);
+ 	return 0;
+ }
+ 
+ static void vmx_cleanup_l1d_flush(void)
+ {
+ 	if (vmx_l1d_flush_pages) {
+ 		free_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ 		vmx_l1d_flush_pages = NULL;
+ 	}
+ 	/* Restore state so sysfs ignores VMX */
+ 	l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+ }
+ 
+ static void vmx_exit(void)
+ {
+ #ifdef CONFIG_KEXEC_CORE
+ 	RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+ 	synchronize_rcu();
+ #endif
+ 
+ 	kvm_exit();
+ 
+ #if IS_ENABLED(CONFIG_HYPERV)
+ 	if (static_branch_unlikely(&enable_evmcs)) {
+ 		int cpu;
+ 		struct hv_vp_assist_page *vp_ap;
+ 		/*
+ 		 * Reset everything to support using non-enlightened VMCS
+ 		 * access later (e.g. when we reload the module with
+ 		 * enlightened_vmcs=0)
+ 		 */
+ 		for_each_online_cpu(cpu) {
+ 			vp_ap =	hv_get_vp_assist_page(cpu);
+ 
+ 			if (!vp_ap)
+ 				continue;
+ 
+ 			vp_ap->current_nested_vmcs = 0;
+ 			vp_ap->enlighten_vmentry = 0;
+ 		}
+ 
+ 		static_branch_disable(&enable_evmcs);
+ 	}
+ #endif
+ 	vmx_cleanup_l1d_flush();
+ }
+ module_exit(vmx_exit);
+ 
+ static int __init vmx_init(void)
+ {
+ 	int r;
+ 
+ #if IS_ENABLED(CONFIG_HYPERV)
+ 	/*
+ 	 * Enlightened VMCS usage should be recommended and the host needs
+ 	 * to support eVMCS v1 or above. We can also disable eVMCS support
+ 	 * with module parameter.
+ 	 */
+ 	if (enlightened_vmcs &&
+ 	    ms_hyperv.hints & HV_X64_ENLIGHTENED_VMCS_RECOMMENDED &&
+ 	    (ms_hyperv.nested_features & HV_X64_ENLIGHTENED_VMCS_VERSION) >=
+ 	    KVM_EVMCS_VERSION) {
+ 		int cpu;
+ 
+ 		/* Check that we have assist pages on all online CPUs */
+ 		for_each_online_cpu(cpu) {
+ 			if (!hv_get_vp_assist_page(cpu)) {
+ 				enlightened_vmcs = false;
+ 				break;
+ 			}
+ 		}
+ 
+ 		if (enlightened_vmcs) {
+ 			pr_info("KVM: vmx: using Hyper-V Enlightened VMCS\n");
+ 			static_branch_enable(&enable_evmcs);
+ 		}
+ 	} else {
+ 		enlightened_vmcs = false;
+ 	}
+ #endif
+ 
+ 	r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ 		     __alignof__(struct vcpu_vmx), THIS_MODULE);
+ 	if (r)
+ 		return r;
+ 
+ 	/*
+ 	 * Must be called after kvm_init() so enable_ept is properly set up
+ 	 */
+ 	r = vmx_setup_l1d_flush();
+ 	if (r) {
+ 		vmx_exit();
+ 		return r;
+ 	}
+ 
++>>>>>>> a7b9020b06ec (x86/l1tf: Handle EPT disabled state proper)
  #ifdef CONFIG_KEXEC_CORE
  	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
  			   crash_vmclear_local_loaded_vmcss);
@@@ -11677,16 -13324,5 +11800,20 @@@
  
  	return 0;
  }
++<<<<<<< HEAD
 +
 +static void __exit vmx_exit(void)
 +{
 +#ifdef CONFIG_KEXEC_CORE
 +	RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
 +	synchronize_rcu();
 +#endif
 +
 +	kvm_exit();
 +}
 +
 +module_init(vmx_init)
 +module_exit(vmx_exit)
++=======
+ module_init(vmx_init);
++>>>>>>> a7b9020b06ec (x86/l1tf: Handle EPT disabled state proper)
* Unmerged path arch/x86/include/asm/vmx.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kvm/vmx.c
