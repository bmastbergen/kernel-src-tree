x86/speculation/l1tf: Unbreak !__HAVE_ARCH_PFN_MODIFY_ALLOWED architectures

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jiri Kosina <jkosina@suse.cz>
commit 6c26fcd2abfe0a56bbd95271fce02df2896cfd24
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6c26fcd2.failed

pfn_modify_allowed() and arch_has_pfn_modify_check() are outside of the 
!__ASSEMBLY__ section in include/asm-generic/pgtable.h, which confuses 
assembler on archs that don't have __HAVE_ARCH_PFN_MODIFY_ALLOWED (e.g. 
ia64) and breaks build:

    include/asm-generic/pgtable.h: Assembler messages:
    include/asm-generic/pgtable.h:538: Error: Unknown opcode `static inline bool pfn_modify_allowed(unsigned long pfn,pgprot_t prot)'
    include/asm-generic/pgtable.h:540: Error: Unknown opcode `return true'
    include/asm-generic/pgtable.h:543: Error: Unknown opcode `static inline bool arch_has_pfn_modify_check(void)'
    include/asm-generic/pgtable.h:545: Error: Unknown opcode `return false'
    arch/ia64/kernel/entry.S:69: Error: `mov' does not fit into bundle

Move those two static inlines into the !__ASSEMBLY__ section so that they 
don't confuse the asm build pass.

Fixes: 42e4089c7890 ("x86/speculation/l1tf: Disallow non privileged high MMIO PROT_NONE mappings")
	Signed-off-by: Jiri Kosina <jkosina@suse.cz>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 6c26fcd2abfe0a56bbd95271fce02df2896cfd24)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/asm-generic/pgtable.h
diff --cc include/asm-generic/pgtable.h
index 8aa445dbab59,26ca0276b503..000000000000
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@@ -942,8 -1046,67 +942,64 @@@ static inline int pmd_clear_huge(pmd_t 
  {
  	return 0;
  }
 -static inline int pud_free_pmd_page(pud_t *pud)
 -{
 -	return 0;
 -}
 -static inline int pmd_free_pte_page(pmd_t *pmd)
 -{
 -	return 0;
 -}
  #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
  
++<<<<<<< HEAD
 +#endif /* !__ASSEMBLY__ */
 +
++=======
+ #ifndef __HAVE_ARCH_FLUSH_PMD_TLB_RANGE
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ /*
+  * ARCHes with special requirements for evicting THP backing TLB entries can
+  * implement this. Otherwise also, it can help optimize normal TLB flush in
+  * THP regime. stock flush_tlb_range() typically has optimization to nuke the
+  * entire TLB TLB if flush span is greater than a threshold, which will
+  * likely be true for a single huge page. Thus a single thp flush will
+  * invalidate the entire TLB which is not desitable.
+  * e.g. see arch/arc: flush_pmd_tlb_range
+  */
+ #define flush_pmd_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
+ #define flush_pud_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
+ #else
+ #define flush_pmd_tlb_range(vma, addr, end)	BUILD_BUG()
+ #define flush_pud_tlb_range(vma, addr, end)	BUILD_BUG()
+ #endif
+ #endif
+ 
+ struct file;
+ int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,
+ 			unsigned long size, pgprot_t *vma_prot);
+ 
+ #ifndef CONFIG_X86_ESPFIX64
+ static inline void init_espfix_bsp(void) { }
+ #endif
+ 
+ #ifndef __HAVE_ARCH_PFN_MODIFY_ALLOWED
+ static inline bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
+ {
+ 	return true;
+ }
+ 
+ static inline bool arch_has_pfn_modify_check(void)
+ {
+ 	return false;
+ }
+ #endif /* !_HAVE_ARCH_PFN_MODIFY_ALLOWED */
+ 
+ #endif /* !__ASSEMBLY__ */
+ 
+ #ifndef io_remap_pfn_range
+ #define io_remap_pfn_range remap_pfn_range
+ #endif
+ 
+ #ifndef has_transparent_hugepage
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ #define has_transparent_hugepage() 1
+ #else
+ #define has_transparent_hugepage() 0
+ #endif
+ #endif
+ 
++>>>>>>> 6c26fcd2abfe (x86/speculation/l1tf: Unbreak !__HAVE_ARCH_PFN_MODIFY_ALLOWED architectures)
  #endif /* _ASM_GENERIC_PGTABLE_H */
* Unmerged path include/asm-generic/pgtable.h
