mm: pagewalk: fix misbehavior of walk_page_range for vma(VM_PFNMAP)

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] pagewalk: fix misbehavior of walk_page_range for vma(VM_PFNMAP) (Rafael Aquini) [1562137]
Rebuild_FUZZ: 96.92%
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit 48684a65b4e3ff544d62532c1b78962c9677b632
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/48684a65.failed

walk_page_range() silently skips vma having VM_PFNMAP set, which leads to
undesirable behaviour at client end (who called walk_page_range).  For
example for pagemap_read(), when no callbacks are called against VM_PFNMAP
vma, pagemap_read() may prepare pagemap data for next virtual address
range at wrong index.  That could confuse and/or break userspace
applications.

This patch avoid this misbehavior caused by vma(VM_PFNMAP) like follows:
- for pagemap_read() which has its own ->pte_hole(), call the ->pte_hole()
  over vma(VM_PFNMAP),
- for clear_refs and queue_pages which have their own ->tests_walk,
  just return 1 and skip vma(VM_PFNMAP). This is no problem because
  these are not interested in hole regions,
- for other callers, just skip the vma(VM_PFNMAP) as a default behavior.

	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Shiraz Hashim <shashim@codeaurora.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 48684a65b4e3ff544d62532c1b78962c9677b632)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/task_mmu.c
#	mm/mempolicy.c
#	mm/pagewalk.c
diff --cc fs/proc/task_mmu.c
index 2a2e2958ee0d,f5ca96524f5f..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -888,6 -800,28 +888,31 @@@ static int clear_refs_pte_range(pmd_t *
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int clear_refs_test_walk(unsigned long start, unsigned long end,
+ 				struct mm_walk *walk)
+ {
+ 	struct clear_refs_private *cp = walk->private;
+ 	struct vm_area_struct *vma = walk->vma;
+ 
+ 	if (vma->vm_flags & VM_PFNMAP)
+ 		return 1;
+ 
+ 	/*
+ 	 * Writing 1 to /proc/pid/clear_refs affects all pages.
+ 	 * Writing 2 to /proc/pid/clear_refs only affects anonymous pages.
+ 	 * Writing 3 to /proc/pid/clear_refs only affects file mapped pages.
+ 	 * Writing 4 to /proc/pid/clear_refs affects all pages.
+ 	 */
+ 	if (cp->type == CLEAR_REFS_ANON && vma->vm_file)
+ 		return 1;
+ 	if (cp->type == CLEAR_REFS_MAPPED && !vma->vm_file)
+ 		return 1;
+ 	return 0;
+ }
+ 
++>>>>>>> 48684a65b4e3 (mm: pagewalk: fix misbehavior of walk_page_range for vma(VM_PFNMAP))
  static ssize_t clear_refs_write(struct file *file, const char __user *buf,
  				size_t count, loff_t *ppos)
  {
diff --cc mm/mempolicy.c
index 1b95545cab88,f1bd23803576..000000000000
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@@ -642,7 -581,50 +642,54 @@@ static unsigned long change_prot_numa(s
  {
  	return 0;
  }
++<<<<<<< HEAD
 +#endif /* CONFIG_ARCH_NUMA_BALANCING */
++=======
+ #endif /* CONFIG_NUMA_BALANCING */
+ 
+ static int queue_pages_test_walk(unsigned long start, unsigned long end,
+ 				struct mm_walk *walk)
+ {
+ 	struct vm_area_struct *vma = walk->vma;
+ 	struct queue_pages *qp = walk->private;
+ 	unsigned long endvma = vma->vm_end;
+ 	unsigned long flags = qp->flags;
+ 
+ 	if (vma->vm_flags & VM_PFNMAP)
+ 		return 1;
+ 
+ 	if (endvma > end)
+ 		endvma = end;
+ 	if (vma->vm_start > start)
+ 		start = vma->vm_start;
+ 
+ 	if (!(flags & MPOL_MF_DISCONTIG_OK)) {
+ 		if (!vma->vm_next && vma->vm_end < end)
+ 			return -EFAULT;
+ 		if (qp->prev && qp->prev->vm_end < vma->vm_start)
+ 			return -EFAULT;
+ 	}
+ 
+ 	qp->prev = vma;
+ 
+ 	if (vma->vm_flags & VM_PFNMAP)
+ 		return 1;
+ 
+ 	if (flags & MPOL_MF_LAZY) {
+ 		/* Similar to task_numa_work, skip inaccessible VMAs */
+ 		if (vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))
+ 			change_prot_numa(vma, start, endvma);
+ 		return 1;
+ 	}
+ 
+ 	if ((flags & MPOL_MF_STRICT) ||
+ 	    ((flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) &&
+ 	     vma_migratable(vma)))
+ 		/* queue pages from current vma */
+ 		return 0;
+ 	return 1;
+ }
++>>>>>>> 48684a65b4e3 (mm: pagewalk: fix misbehavior of walk_page_range for vma(VM_PFNMAP))
  
  /*
   * Walk through page tables and collect pages to be migrated.
diff --cc mm/pagewalk.c
index fc0ccb7f9250,75c1f2878519..000000000000
--- a/mm/pagewalk.c
+++ b/mm/pagewalk.c
@@@ -154,42 -160,91 +154,90 @@@ static int walk_hugetlb_range(struct vm
  
  #endif /* CONFIG_HUGETLB_PAGE */
  
++<<<<<<< HEAD
++
++=======
+ /*
+  * Decide whether we really walk over the current vma on [@start, @end)
+  * or skip it via the returned value. Return 0 if we do walk over the
+  * current vma, and return 1 if we skip the vma. Negative values means
+  * error, where we abort the current walk.
+  */
+ static int walk_page_test(unsigned long start, unsigned long end,
+ 			struct mm_walk *walk)
+ {
+ 	struct vm_area_struct *vma = walk->vma;
+ 
+ 	if (walk->test_walk)
+ 		return walk->test_walk(start, end, walk);
+ 
+ 	/*
+ 	 * vma(VM_PFNMAP) doesn't have any valid struct pages behind VM_PFNMAP
+ 	 * range, so we don't walk over it as we do for normal vmas. However,
+ 	 * Some callers are interested in handling hole range and they don't
+ 	 * want to just ignore any single address range. Such users certainly
+ 	 * define their ->pte_hole() callbacks, so let's delegate them to handle
+ 	 * vma(VM_PFNMAP).
+ 	 */
+ 	if (vma->vm_flags & VM_PFNMAP) {
+ 		int err = 1;
+ 		if (walk->pte_hole)
+ 			err = walk->pte_hole(start, end, walk);
+ 		return err ? err : 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static int __walk_page_range(unsigned long start, unsigned long end,
+ 			struct mm_walk *walk)
+ {
+ 	int err = 0;
+ 	struct vm_area_struct *vma = walk->vma;
+ 
+ 	if (vma && is_vm_hugetlb_page(vma)) {
+ 		if (walk->hugetlb_entry)
+ 			err = walk_hugetlb_range(start, end, walk);
+ 	} else
+ 		err = walk_pgd_range(start, end, walk);
  
+ 	return err;
+ }
++>>>>>>> 48684a65b4e3 (mm: pagewalk: fix misbehavior of walk_page_range for vma(VM_PFNMAP))
  
  /**
 - * walk_page_range - walk page table with caller specific callbacks
 + * walk_page_range - walk a memory map's page tables with a callback
 + * @addr: starting address
 + * @end: ending address
 + * @walk: set of callbacks to invoke for each level of the tree
 + *
 + * Recursively walk the page table for the memory area in a VMA,
 + * calling supplied callbacks. Callbacks are called in-order (first
 + * PGD, first PUD, first PMD, first PTE, second PTE... second PMD,
 + * etc.). If lower-level callbacks are omitted, walking depth is reduced.
   *
 - * Recursively walk the page table tree of the process represented by @walk->mm
 - * within the virtual address range [@start, @end). During walking, we can do
 - * some caller-specific works for each entry, by setting up pmd_entry(),
 - * pte_entry(), and/or hugetlb_entry(). If you don't set up for some of these
 - * callbacks, the associated entries/pages are just ignored.
 - * The return values of these callbacks are commonly defined like below:
 - *  - 0  : succeeded to handle the current entry, and if you don't reach the
 - *         end address yet, continue to walk.
 - *  - >0 : succeeded to handle the current entry, and return to the caller
 - *         with caller specific value.
 - *  - <0 : failed to handle the current entry, and return to the caller
 - *         with error code.
 + * Each callback receives an entry pointer and the start and end of the
 + * associated range, and a copy of the original mm_walk for access to
 + * the ->private or ->mm fields.
   *
 - * Before starting to walk page table, some callers want to check whether
 - * they really want to walk over the current vma, typically by checking
 - * its vm_flags. walk_page_test() and @walk->test_walk() are used for this
 - * purpose.
 + * Usually no locks are taken, but splitting transparent huge page may
 + * take page table lock. And the bottom level iterator will map PTE
 + * directories from highmem if necessary.
   *
 - * struct mm_walk keeps current values of some common data like vma and pmd,
 - * which are useful for the access from callbacks. If you want to pass some
 - * caller-specific data to callbacks, @walk->private should be helpful.
 + * If any callback returns a non-zero value, the walk is aborted and
 + * the return value is propagated back to the caller. Otherwise 0 is returned.
   *
 - * Locking:
 - *   Callers of walk_page_range() and walk_page_vma() should hold
 - *   @walk->mm->mmap_sem, because these function traverse vma list and/or
 - *   access to vma's data.
 + * walk->mm->mmap_sem must be held for at least read if walk->hugetlb_entry
 + * is !NULL.
   */
 -int walk_page_range(unsigned long start, unsigned long end,
 +int walk_page_range(unsigned long addr, unsigned long end,
  		    struct mm_walk *walk)
  {
 -	int err = 0;
 +	pgd_t *pgd;
  	unsigned long next;
 -	struct vm_area_struct *vma;
 +	int err = 0;
  
 -	if (start >= end)
 -		return -EINVAL;
 +	if (addr >= end)
 +		return err;
  
  	if (!walk->mm)
  		return -EINVAL;
* Unmerged path fs/proc/task_mmu.c
* Unmerged path mm/mempolicy.c
* Unmerged path mm/pagewalk.c
