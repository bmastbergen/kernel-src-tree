blk-mq: introduce BLK_STS_DEV_RESOURCE

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ming Lei <ming.lei@redhat.com>
commit 86ff7c2a80cd357f6156a53b354f6a0b357dc0c9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/86ff7c2a.failed

This status is returned from driver to block layer if device related
resource is unavailable, but driver can guarantee that IO dispatch
will be triggered in future when the resource is available.

Convert some drivers to return BLK_STS_DEV_RESOURCE.  Also, if driver
returns BLK_STS_RESOURCE and SCHED_RESTART is set, rerun queue after
a delay (BLK_MQ_DELAY_QUEUE) to avoid IO stalls.  BLK_MQ_DELAY_QUEUE is
3 ms because both scsi-mq and nvmefc are using that magic value.

If a driver can make sure there is in-flight IO, it is safe to return
BLK_STS_DEV_RESOURCE because:

1) If all in-flight IOs complete before examining SCHED_RESTART in
blk_mq_dispatch_rq_list(), SCHED_RESTART must be cleared, so queue
is run immediately in this case by blk_mq_dispatch_rq_list();

2) if there is any in-flight IO after/when examining SCHED_RESTART
in blk_mq_dispatch_rq_list():
- if SCHED_RESTART isn't set, queue is run immediately as handled in 1)
- otherwise, this request will be dispatched after any in-flight IO is
  completed via blk_mq_sched_restart()

3) if SCHED_RESTART is set concurently in context because of
BLK_STS_RESOURCE, blk_mq_delay_run_hw_queue() will cover the above two
cases and make sure IO hang can be avoided.

One invariant is that queue will be rerun if SCHED_RESTART is set.

	Suggested-by: Jens Axboe <axboe@kernel.dk>
	Tested-by: Laurence Oberman <loberman@redhat.com>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 86ff7c2a80cd357f6156a53b354f6a0b357dc0c9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	block/blk-mq.c
#	drivers/block/null_blk.c
#	drivers/block/virtio_blk.c
#	drivers/block/xen-blkfront.c
#	drivers/md/dm-rq.c
#	drivers/nvme/host/fc.c
#	drivers/scsi/scsi_lib.c
#	include/linux/blk_types.h
diff --cc block/blk-core.c
index 3c1e8c52cafa,134fd34b681f..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -123,16 -129,76 +123,79 @@@ void blk_rq_init(struct request_queue *
  }
  EXPORT_SYMBOL(blk_rq_init);
  
++<<<<<<< HEAD
++=======
+ static const struct {
+ 	int		errno;
+ 	const char	*name;
+ } blk_errors[] = {
+ 	[BLK_STS_OK]		= { 0,		"" },
+ 	[BLK_STS_NOTSUPP]	= { -EOPNOTSUPP, "operation not supported" },
+ 	[BLK_STS_TIMEOUT]	= { -ETIMEDOUT,	"timeout" },
+ 	[BLK_STS_NOSPC]		= { -ENOSPC,	"critical space allocation" },
+ 	[BLK_STS_TRANSPORT]	= { -ENOLINK,	"recoverable transport" },
+ 	[BLK_STS_TARGET]	= { -EREMOTEIO,	"critical target" },
+ 	[BLK_STS_NEXUS]		= { -EBADE,	"critical nexus" },
+ 	[BLK_STS_MEDIUM]	= { -ENODATA,	"critical medium" },
+ 	[BLK_STS_PROTECTION]	= { -EILSEQ,	"protection" },
+ 	[BLK_STS_RESOURCE]	= { -ENOMEM,	"kernel resource" },
+ 	[BLK_STS_DEV_RESOURCE]	= { -EBUSY,	"device resource" },
+ 	[BLK_STS_AGAIN]		= { -EAGAIN,	"nonblocking retry" },
+ 
+ 	/* device mapper special case, should not leak out: */
+ 	[BLK_STS_DM_REQUEUE]	= { -EREMCHG, "dm internal retry" },
+ 
+ 	/* everything else not covered above: */
+ 	[BLK_STS_IOERR]		= { -EIO,	"I/O" },
+ };
+ 
+ blk_status_t errno_to_blk_status(int errno)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(blk_errors); i++) {
+ 		if (blk_errors[i].errno == errno)
+ 			return (__force blk_status_t)i;
+ 	}
+ 
+ 	return BLK_STS_IOERR;
+ }
+ EXPORT_SYMBOL_GPL(errno_to_blk_status);
+ 
+ int blk_status_to_errno(blk_status_t status)
+ {
+ 	int idx = (__force int)status;
+ 
+ 	if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
+ 		return -EIO;
+ 	return blk_errors[idx].errno;
+ }
+ EXPORT_SYMBOL_GPL(blk_status_to_errno);
+ 
+ static void print_req_error(struct request *req, blk_status_t status)
+ {
+ 	int idx = (__force int)status;
+ 
+ 	if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
+ 		return;
+ 
+ 	printk_ratelimited(KERN_ERR "%s: %s error, dev %s, sector %llu\n",
+ 			   __func__, blk_errors[idx].name, req->rq_disk ?
+ 			   req->rq_disk->disk_name : "?",
+ 			   (unsigned long long)blk_rq_pos(req));
+ }
+ 
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  static void req_bio_endio(struct request *rq, struct bio *bio,
 -			  unsigned int nbytes, blk_status_t error)
 +			  unsigned int nbytes, int error)
  {
  	if (error)
 -		bio->bi_status = error;
 +		clear_bit(BIO_UPTODATE, &bio->bi_flags);
 +	else if (!test_bit(BIO_UPTODATE, &bio->bi_flags))
 +		error = -EIO;
  
 -	if (unlikely(rq->rq_flags & RQF_QUIET))
 -		bio_set_flag(bio, BIO_QUIET);
 +	if (unlikely(rq->cmd_flags & REQ_QUIET))
 +		set_bit(BIO_QUIET, &bio->bi_flags);
  
  	bio_advance(bio, nbytes);
  
diff --cc block/blk-mq.c
index 1eaa154c3ecb,df93102e2149..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -978,40 -1145,33 +978,47 @@@ static bool blk_mq_mark_tag_wait(struc
  	 * queue.
  	 */
  	ret = blk_mq_get_driver_tag(rq, hctx, false);
 -	if (!ret) {
 -		spin_unlock(&this_hctx->lock);
 -		return false;
 -	}
  
 -	/*
 -	 * We got a tag, remove ourselves from the wait queue to ensure
 -	 * someone else gets the wakeup.
 -	 */
 -	spin_lock_irq(&ws->wait.lock);
 -	list_del_init(&wait->entry);
 -	spin_unlock_irq(&ws->wait.lock);
 -	spin_unlock(&this_hctx->lock);
 +	if (!shared_tags) {
 +		/*
 +		 * Don't clear RESTART here, someone else could have set it.
 +		 * At most this will cost an extra queue run.
 +		 */
 +		return ret;
 +	} else {
 +		if (!ret) {
 +			spin_unlock(&this_hctx->lock);
 +			return false;
 +		}
  
 -	return true;
 +		/*
 +		 * We got a tag, remove ourselves from the wait queue to ensure
 +		 * someone else gets the wakeup.
 +		 */
 +		spin_lock_irq(&ws->wait.lock);
 +		list_del_init(&wait->task_list);
 +		spin_unlock_irq(&ws->wait.lock);
 +		spin_unlock(&this_hctx->lock);
 +		return true;
 +	}
  }
  
+ #define BLK_MQ_RESOURCE_DELAY	3		/* ms units */
+ 
  bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
  			     bool got_budget)
  {
  	struct blk_mq_hw_ctx *hctx;
 -	struct request *rq, *nxt;
  	bool no_tag = false;
++<<<<<<< HEAD
 +	struct request *rq, *nxt;
 +	LIST_HEAD(driver_list);
 +	struct list_head *dptr;
 +	int errors, queued, ret = BLK_MQ_RQ_QUEUE_OK;
++=======
+ 	int errors, queued;
+ 	blk_status_t ret = BLK_STS_OK;
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  
  	if (list_empty(list))
  		return false;
@@@ -1075,14 -1228,11 +1082,18 @@@
  		}
  
  		ret = q->mq_ops->queue_rq(hctx, &bd);
++<<<<<<< HEAD
 +		switch (ret) {
 +		case BLK_MQ_RQ_QUEUE_OK:
 +			queued++;
 +			break;
 +		case BLK_MQ_RQ_QUEUE_BUSY:
++=======
+ 		if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE) {
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  			/*
  			 * If an I/O scheduler has been configured and we got a
 -			 * driver tag for the next request already, free it
 -			 * again.
 +			 * driver tag for the next request already, free it again.
  			 */
  			if (!list_empty(list)) {
  				nxt = list_first_entry(list, struct request, queuelist);
@@@ -1138,10 -1274,27 +1151,33 @@@
  		 * a driver tag with an I/O scheduler attached. If our dispatch
  		 * waitqueue is no longer active, ensure that we run the queue
  		 * AFTER adding our entries back to the list.
++<<<<<<< HEAD
 +		 */
 +		if (!blk_mq_sched_needs_restart(hctx) ||
 +		    (no_tag && list_empty_careful(&hctx->dispatch_wait.task_list)))
++=======
+ 		 *
+ 		 * If no I/O scheduler has been configured it is possible that
+ 		 * the hardware queue got stopped and restarted before requests
+ 		 * were pushed back onto the dispatch list. Rerun the queue to
+ 		 * avoid starvation. Notes:
+ 		 * - blk_mq_run_hw_queue() checks whether or not a queue has
+ 		 *   been stopped before rerunning a queue.
+ 		 * - Some but not all block drivers stop a queue before
+ 		 *   returning BLK_STS_RESOURCE. Two exceptions are scsi-mq
+ 		 *   and dm-rq.
+ 		 *
+ 		 * If driver returns BLK_STS_RESOURCE and SCHED_RESTART
+ 		 * bit is set, run queue after a delay to avoid IO stalls
+ 		 * that could otherwise occur if the queue is idle.
+ 		 */
+ 		needs_restart = blk_mq_sched_needs_restart(hctx);
+ 		if (!needs_restart ||
+ 		    (no_tag && list_empty_careful(&hctx->dispatch_wait.entry)))
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  			blk_mq_run_hw_queue(hctx, true);
+ 		else if (needs_restart && (ret == BLK_STS_RESOURCE))
+ 			blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);
  	}
  
  	return (queued + errors) != 0;
@@@ -1535,14 -1757,53 +1571,50 @@@ static void __blk_mq_try_issue_directly
  	struct request_queue *q = rq->q;
  	struct blk_mq_queue_data bd = {
  		.rq = rq,
 +		.list = NULL,
  		.last = true,
  	};
++<<<<<<< HEAD
 +	int ret;
++=======
+ 	blk_qc_t new_cookie;
+ 	blk_status_t ret;
+ 
+ 	new_cookie = request_to_qc_t(hctx, rq);
+ 
+ 	/*
+ 	 * For OK queue, we are done. For error, caller may kill it.
+ 	 * Any other error (busy), just add it to our list as we
+ 	 * previously would have done.
+ 	 */
+ 	ret = q->mq_ops->queue_rq(hctx, &bd);
+ 	switch (ret) {
+ 	case BLK_STS_OK:
+ 		*cookie = new_cookie;
+ 		break;
+ 	case BLK_STS_RESOURCE:
+ 	case BLK_STS_DEV_RESOURCE:
+ 		__blk_mq_requeue_request(rq);
+ 		break;
+ 	default:
+ 		*cookie = BLK_QC_T_NONE;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
+ 						struct request *rq,
+ 						blk_qc_t *cookie,
+ 						bool bypass_insert)
+ {
+ 	struct request_queue *q = rq->q;
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  	bool run_queue = true;
  
 -	/*
 -	 * RCU or SRCU read lock is needed before checking quiesced flag.
 -	 *
 -	 * When queue is stopped or quiesced, ignore 'bypass_insert' from
 -	 * blk_mq_request_issue_directly(), and return BLK_STS_OK to caller,
 -	 * and avoid driver to try to dispatch again.
 -	 */
 -	if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
 +	if (blk_mq_hctx_stopped(hctx)) {
  		run_queue = false;
 -		bypass_insert = false;
  		goto insert;
  	}
  
@@@ -1578,27 -1828,43 +1650,39 @@@ insert
  }
  
  static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 -		struct request *rq, blk_qc_t *cookie)
 +				      struct request *rq)
  {
 -	blk_status_t ret;
 -	int srcu_idx;
 +	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 +		rcu_read_lock();
 +		__blk_mq_try_issue_directly(hctx, rq, false);
 +		rcu_read_unlock();
 +	} else {
 +		unsigned int srcu_idx;
  
 -	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 +		might_sleep();
  
++<<<<<<< HEAD
 +		srcu_idx = srcu_read_lock(&hctx->queue_rq_srcu);
 +		__blk_mq_try_issue_directly(hctx, rq, true);
 +		srcu_read_unlock(&hctx->queue_rq_srcu, srcu_idx);
 +	}
++=======
+ 	hctx_lock(hctx, &srcu_idx);
+ 
+ 	ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false);
+ 	if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE)
+ 		blk_mq_sched_insert_request(rq, false, true, false);
+ 	else if (ret != BLK_STS_OK)
+ 		blk_mq_end_request(rq, ret);
+ 
+ 	hctx_unlock(hctx, srcu_idx);
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  }
  
 -blk_status_t blk_mq_request_issue_directly(struct request *rq)
 -{
 -	blk_status_t ret;
 -	int srcu_idx;
 -	blk_qc_t unused_cookie;
 -	struct blk_mq_ctx *ctx = rq->mq_ctx;
 -	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(rq->q, ctx->cpu);
 -
 -	hctx_lock(hctx, &srcu_idx);
 -	ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true);
 -	hctx_unlock(hctx, srcu_idx);
 -
 -	return ret;
 -}
 -
 -static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 +static void blk_mq_make_request(struct request_queue *q, struct bio *bio)
  {
 -	const int is_sync = op_is_sync(bio->bi_opf);
 -	const int is_flush_fua = op_is_flush(bio->bi_opf);
 +	const int is_sync = rw_is_sync(bio->bi_rw);
 +	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
  	struct blk_mq_alloc_data data = { .flags = 0 };
  	struct request *rq;
  	unsigned int request_count = 0;
diff --cc drivers/block/null_blk.c
index 974f570db9ee,287a09611c0f..000000000000
--- a/drivers/block/null_blk.c
+++ b/drivers/block/null_blk.c
@@@ -233,14 -716,569 +233,572 @@@ static void null_softirq_done_fn(struc
  		end_cmd(rq->special);
  }
  
 -static struct nullb_page *null_alloc_page(gfp_t gfp_flags)
 +static inline void null_handle_cmd(struct nullb_cmd *cmd)
  {
++<<<<<<< HEAD
++=======
+ 	struct nullb_page *t_page;
+ 
+ 	t_page = kmalloc(sizeof(struct nullb_page), gfp_flags);
+ 	if (!t_page)
+ 		goto out;
+ 
+ 	t_page->page = alloc_pages(gfp_flags, 0);
+ 	if (!t_page->page)
+ 		goto out_freepage;
+ 
+ 	t_page->bitmap = 0;
+ 	return t_page;
+ out_freepage:
+ 	kfree(t_page);
+ out:
+ 	return NULL;
+ }
+ 
+ static void null_free_page(struct nullb_page *t_page)
+ {
+ 	__set_bit(NULLB_PAGE_FREE, &t_page->bitmap);
+ 	if (test_bit(NULLB_PAGE_LOCK, &t_page->bitmap))
+ 		return;
+ 	__free_page(t_page->page);
+ 	kfree(t_page);
+ }
+ 
+ static void null_free_sector(struct nullb *nullb, sector_t sector,
+ 	bool is_cache)
+ {
+ 	unsigned int sector_bit;
+ 	u64 idx;
+ 	struct nullb_page *t_page, *ret;
+ 	struct radix_tree_root *root;
+ 
+ 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
+ 	idx = sector >> PAGE_SECTORS_SHIFT;
+ 	sector_bit = (sector & SECTOR_MASK);
+ 
+ 	t_page = radix_tree_lookup(root, idx);
+ 	if (t_page) {
+ 		__clear_bit(sector_bit, &t_page->bitmap);
+ 
+ 		if (!t_page->bitmap) {
+ 			ret = radix_tree_delete_item(root, idx, t_page);
+ 			WARN_ON(ret != t_page);
+ 			null_free_page(ret);
+ 			if (is_cache)
+ 				nullb->dev->curr_cache -= PAGE_SIZE;
+ 		}
+ 	}
+ }
+ 
+ static struct nullb_page *null_radix_tree_insert(struct nullb *nullb, u64 idx,
+ 	struct nullb_page *t_page, bool is_cache)
+ {
+ 	struct radix_tree_root *root;
+ 
+ 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
+ 
+ 	if (radix_tree_insert(root, idx, t_page)) {
+ 		null_free_page(t_page);
+ 		t_page = radix_tree_lookup(root, idx);
+ 		WARN_ON(!t_page || t_page->page->index != idx);
+ 	} else if (is_cache)
+ 		nullb->dev->curr_cache += PAGE_SIZE;
+ 
+ 	return t_page;
+ }
+ 
+ static void null_free_device_storage(struct nullb_device *dev, bool is_cache)
+ {
+ 	unsigned long pos = 0;
+ 	int nr_pages;
+ 	struct nullb_page *ret, *t_pages[FREE_BATCH];
+ 	struct radix_tree_root *root;
+ 
+ 	root = is_cache ? &dev->cache : &dev->data;
+ 
+ 	do {
+ 		int i;
+ 
+ 		nr_pages = radix_tree_gang_lookup(root,
+ 				(void **)t_pages, pos, FREE_BATCH);
+ 
+ 		for (i = 0; i < nr_pages; i++) {
+ 			pos = t_pages[i]->page->index;
+ 			ret = radix_tree_delete_item(root, pos, t_pages[i]);
+ 			WARN_ON(ret != t_pages[i]);
+ 			null_free_page(ret);
+ 		}
+ 
+ 		pos++;
+ 	} while (nr_pages == FREE_BATCH);
+ 
+ 	if (is_cache)
+ 		dev->curr_cache = 0;
+ }
+ 
+ static struct nullb_page *__null_lookup_page(struct nullb *nullb,
+ 	sector_t sector, bool for_write, bool is_cache)
+ {
+ 	unsigned int sector_bit;
+ 	u64 idx;
+ 	struct nullb_page *t_page;
+ 	struct radix_tree_root *root;
+ 
+ 	idx = sector >> PAGE_SECTORS_SHIFT;
+ 	sector_bit = (sector & SECTOR_MASK);
+ 
+ 	root = is_cache ? &nullb->dev->cache : &nullb->dev->data;
+ 	t_page = radix_tree_lookup(root, idx);
+ 	WARN_ON(t_page && t_page->page->index != idx);
+ 
+ 	if (t_page && (for_write || test_bit(sector_bit, &t_page->bitmap)))
+ 		return t_page;
+ 
+ 	return NULL;
+ }
+ 
+ static struct nullb_page *null_lookup_page(struct nullb *nullb,
+ 	sector_t sector, bool for_write, bool ignore_cache)
+ {
+ 	struct nullb_page *page = NULL;
+ 
+ 	if (!ignore_cache)
+ 		page = __null_lookup_page(nullb, sector, for_write, true);
+ 	if (page)
+ 		return page;
+ 	return __null_lookup_page(nullb, sector, for_write, false);
+ }
+ 
+ static struct nullb_page *null_insert_page(struct nullb *nullb,
+ 	sector_t sector, bool ignore_cache)
+ {
+ 	u64 idx;
+ 	struct nullb_page *t_page;
+ 
+ 	t_page = null_lookup_page(nullb, sector, true, ignore_cache);
+ 	if (t_page)
+ 		return t_page;
+ 
+ 	spin_unlock_irq(&nullb->lock);
+ 
+ 	t_page = null_alloc_page(GFP_NOIO);
+ 	if (!t_page)
+ 		goto out_lock;
+ 
+ 	if (radix_tree_preload(GFP_NOIO))
+ 		goto out_freepage;
+ 
+ 	spin_lock_irq(&nullb->lock);
+ 	idx = sector >> PAGE_SECTORS_SHIFT;
+ 	t_page->page->index = idx;
+ 	t_page = null_radix_tree_insert(nullb, idx, t_page, !ignore_cache);
+ 	radix_tree_preload_end();
+ 
+ 	return t_page;
+ out_freepage:
+ 	null_free_page(t_page);
+ out_lock:
+ 	spin_lock_irq(&nullb->lock);
+ 	return null_lookup_page(nullb, sector, true, ignore_cache);
+ }
+ 
+ static int null_flush_cache_page(struct nullb *nullb, struct nullb_page *c_page)
+ {
+ 	int i;
+ 	unsigned int offset;
+ 	u64 idx;
+ 	struct nullb_page *t_page, *ret;
+ 	void *dst, *src;
+ 
+ 	idx = c_page->page->index;
+ 
+ 	t_page = null_insert_page(nullb, idx << PAGE_SECTORS_SHIFT, true);
+ 
+ 	__clear_bit(NULLB_PAGE_LOCK, &c_page->bitmap);
+ 	if (test_bit(NULLB_PAGE_FREE, &c_page->bitmap)) {
+ 		null_free_page(c_page);
+ 		if (t_page && t_page->bitmap == 0) {
+ 			ret = radix_tree_delete_item(&nullb->dev->data,
+ 				idx, t_page);
+ 			null_free_page(t_page);
+ 		}
+ 		return 0;
+ 	}
+ 
+ 	if (!t_page)
+ 		return -ENOMEM;
+ 
+ 	src = kmap_atomic(c_page->page);
+ 	dst = kmap_atomic(t_page->page);
+ 
+ 	for (i = 0; i < PAGE_SECTORS;
+ 			i += (nullb->dev->blocksize >> SECTOR_SHIFT)) {
+ 		if (test_bit(i, &c_page->bitmap)) {
+ 			offset = (i << SECTOR_SHIFT);
+ 			memcpy(dst + offset, src + offset,
+ 				nullb->dev->blocksize);
+ 			__set_bit(i, &t_page->bitmap);
+ 		}
+ 	}
+ 
+ 	kunmap_atomic(dst);
+ 	kunmap_atomic(src);
+ 
+ 	ret = radix_tree_delete_item(&nullb->dev->cache, idx, c_page);
+ 	null_free_page(ret);
+ 	nullb->dev->curr_cache -= PAGE_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int null_make_cache_space(struct nullb *nullb, unsigned long n)
+ {
+ 	int i, err, nr_pages;
+ 	struct nullb_page *c_pages[FREE_BATCH];
+ 	unsigned long flushed = 0, one_round;
+ 
+ again:
+ 	if ((nullb->dev->cache_size * 1024 * 1024) >
+ 	     nullb->dev->curr_cache + n || nullb->dev->curr_cache == 0)
+ 		return 0;
+ 
+ 	nr_pages = radix_tree_gang_lookup(&nullb->dev->cache,
+ 			(void **)c_pages, nullb->cache_flush_pos, FREE_BATCH);
+ 	/*
+ 	 * nullb_flush_cache_page could unlock before using the c_pages. To
+ 	 * avoid race, we don't allow page free
+ 	 */
+ 	for (i = 0; i < nr_pages; i++) {
+ 		nullb->cache_flush_pos = c_pages[i]->page->index;
+ 		/*
+ 		 * We found the page which is being flushed to disk by other
+ 		 * threads
+ 		 */
+ 		if (test_bit(NULLB_PAGE_LOCK, &c_pages[i]->bitmap))
+ 			c_pages[i] = NULL;
+ 		else
+ 			__set_bit(NULLB_PAGE_LOCK, &c_pages[i]->bitmap);
+ 	}
+ 
+ 	one_round = 0;
+ 	for (i = 0; i < nr_pages; i++) {
+ 		if (c_pages[i] == NULL)
+ 			continue;
+ 		err = null_flush_cache_page(nullb, c_pages[i]);
+ 		if (err)
+ 			return err;
+ 		one_round++;
+ 	}
+ 	flushed += one_round << PAGE_SHIFT;
+ 
+ 	if (n > flushed) {
+ 		if (nr_pages == 0)
+ 			nullb->cache_flush_pos = 0;
+ 		if (one_round == 0) {
+ 			/* give other threads a chance */
+ 			spin_unlock_irq(&nullb->lock);
+ 			spin_lock_irq(&nullb->lock);
+ 		}
+ 		goto again;
+ 	}
+ 	return 0;
+ }
+ 
+ static int copy_to_nullb(struct nullb *nullb, struct page *source,
+ 	unsigned int off, sector_t sector, size_t n, bool is_fua)
+ {
+ 	size_t temp, count = 0;
+ 	unsigned int offset;
+ 	struct nullb_page *t_page;
+ 	void *dst, *src;
+ 
+ 	while (count < n) {
+ 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
+ 
+ 		if (null_cache_active(nullb) && !is_fua)
+ 			null_make_cache_space(nullb, PAGE_SIZE);
+ 
+ 		offset = (sector & SECTOR_MASK) << SECTOR_SHIFT;
+ 		t_page = null_insert_page(nullb, sector,
+ 			!null_cache_active(nullb) || is_fua);
+ 		if (!t_page)
+ 			return -ENOSPC;
+ 
+ 		src = kmap_atomic(source);
+ 		dst = kmap_atomic(t_page->page);
+ 		memcpy(dst + offset, src + off + count, temp);
+ 		kunmap_atomic(dst);
+ 		kunmap_atomic(src);
+ 
+ 		__set_bit(sector & SECTOR_MASK, &t_page->bitmap);
+ 
+ 		if (is_fua)
+ 			null_free_sector(nullb, sector, true);
+ 
+ 		count += temp;
+ 		sector += temp >> SECTOR_SHIFT;
+ 	}
+ 	return 0;
+ }
+ 
+ static int copy_from_nullb(struct nullb *nullb, struct page *dest,
+ 	unsigned int off, sector_t sector, size_t n)
+ {
+ 	size_t temp, count = 0;
+ 	unsigned int offset;
+ 	struct nullb_page *t_page;
+ 	void *dst, *src;
+ 
+ 	while (count < n) {
+ 		temp = min_t(size_t, nullb->dev->blocksize, n - count);
+ 
+ 		offset = (sector & SECTOR_MASK) << SECTOR_SHIFT;
+ 		t_page = null_lookup_page(nullb, sector, false,
+ 			!null_cache_active(nullb));
+ 
+ 		dst = kmap_atomic(dest);
+ 		if (!t_page) {
+ 			memset(dst + off + count, 0, temp);
+ 			goto next;
+ 		}
+ 		src = kmap_atomic(t_page->page);
+ 		memcpy(dst + off + count, src + offset, temp);
+ 		kunmap_atomic(src);
+ next:
+ 		kunmap_atomic(dst);
+ 
+ 		count += temp;
+ 		sector += temp >> SECTOR_SHIFT;
+ 	}
+ 	return 0;
+ }
+ 
+ static void null_handle_discard(struct nullb *nullb, sector_t sector, size_t n)
+ {
+ 	size_t temp;
+ 
+ 	spin_lock_irq(&nullb->lock);
+ 	while (n > 0) {
+ 		temp = min_t(size_t, n, nullb->dev->blocksize);
+ 		null_free_sector(nullb, sector, false);
+ 		if (null_cache_active(nullb))
+ 			null_free_sector(nullb, sector, true);
+ 		sector += temp >> SECTOR_SHIFT;
+ 		n -= temp;
+ 	}
+ 	spin_unlock_irq(&nullb->lock);
+ }
+ 
+ static int null_handle_flush(struct nullb *nullb)
+ {
+ 	int err;
+ 
+ 	if (!null_cache_active(nullb))
+ 		return 0;
+ 
+ 	spin_lock_irq(&nullb->lock);
+ 	while (true) {
+ 		err = null_make_cache_space(nullb,
+ 			nullb->dev->cache_size * 1024 * 1024);
+ 		if (err || nullb->dev->curr_cache == 0)
+ 			break;
+ 	}
+ 
+ 	WARN_ON(!radix_tree_empty(&nullb->dev->cache));
+ 	spin_unlock_irq(&nullb->lock);
+ 	return err;
+ }
+ 
+ static int null_transfer(struct nullb *nullb, struct page *page,
+ 	unsigned int len, unsigned int off, bool is_write, sector_t sector,
+ 	bool is_fua)
+ {
+ 	int err = 0;
+ 
+ 	if (!is_write) {
+ 		err = copy_from_nullb(nullb, page, off, sector, len);
+ 		flush_dcache_page(page);
+ 	} else {
+ 		flush_dcache_page(page);
+ 		err = copy_to_nullb(nullb, page, off, sector, len, is_fua);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int null_handle_rq(struct nullb_cmd *cmd)
+ {
+ 	struct request *rq = cmd->rq;
+ 	struct nullb *nullb = cmd->nq->dev->nullb;
+ 	int err;
+ 	unsigned int len;
+ 	sector_t sector;
+ 	struct req_iterator iter;
+ 	struct bio_vec bvec;
+ 
+ 	sector = blk_rq_pos(rq);
+ 
+ 	if (req_op(rq) == REQ_OP_DISCARD) {
+ 		null_handle_discard(nullb, sector, blk_rq_bytes(rq));
+ 		return 0;
+ 	}
+ 
+ 	spin_lock_irq(&nullb->lock);
+ 	rq_for_each_segment(bvec, rq, iter) {
+ 		len = bvec.bv_len;
+ 		err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ 				     op_is_write(req_op(rq)), sector,
+ 				     req_op(rq) & REQ_FUA);
+ 		if (err) {
+ 			spin_unlock_irq(&nullb->lock);
+ 			return err;
+ 		}
+ 		sector += len >> SECTOR_SHIFT;
+ 	}
+ 	spin_unlock_irq(&nullb->lock);
+ 
+ 	return 0;
+ }
+ 
+ static int null_handle_bio(struct nullb_cmd *cmd)
+ {
+ 	struct bio *bio = cmd->bio;
+ 	struct nullb *nullb = cmd->nq->dev->nullb;
+ 	int err;
+ 	unsigned int len;
+ 	sector_t sector;
+ 	struct bio_vec bvec;
+ 	struct bvec_iter iter;
+ 
+ 	sector = bio->bi_iter.bi_sector;
+ 
+ 	if (bio_op(bio) == REQ_OP_DISCARD) {
+ 		null_handle_discard(nullb, sector,
+ 			bio_sectors(bio) << SECTOR_SHIFT);
+ 		return 0;
+ 	}
+ 
+ 	spin_lock_irq(&nullb->lock);
+ 	bio_for_each_segment(bvec, bio, iter) {
+ 		len = bvec.bv_len;
+ 		err = null_transfer(nullb, bvec.bv_page, len, bvec.bv_offset,
+ 				     op_is_write(bio_op(bio)), sector,
+ 				     bio_op(bio) & REQ_FUA);
+ 		if (err) {
+ 			spin_unlock_irq(&nullb->lock);
+ 			return err;
+ 		}
+ 		sector += len >> SECTOR_SHIFT;
+ 	}
+ 	spin_unlock_irq(&nullb->lock);
+ 	return 0;
+ }
+ 
+ static void null_stop_queue(struct nullb *nullb)
+ {
+ 	struct request_queue *q = nullb->q;
+ 
+ 	if (nullb->dev->queue_mode == NULL_Q_MQ)
+ 		blk_mq_stop_hw_queues(q);
+ 	else {
+ 		spin_lock_irq(q->queue_lock);
+ 		blk_stop_queue(q);
+ 		spin_unlock_irq(q->queue_lock);
+ 	}
+ }
+ 
+ static void null_restart_queue_async(struct nullb *nullb)
+ {
+ 	struct request_queue *q = nullb->q;
+ 	unsigned long flags;
+ 
+ 	if (nullb->dev->queue_mode == NULL_Q_MQ)
+ 		blk_mq_start_stopped_hw_queues(q, true);
+ 	else {
+ 		spin_lock_irqsave(q->queue_lock, flags);
+ 		blk_start_queue_async(q);
+ 		spin_unlock_irqrestore(q->queue_lock, flags);
+ 	}
+ }
+ 
+ static blk_status_t null_handle_cmd(struct nullb_cmd *cmd)
+ {
+ 	struct nullb_device *dev = cmd->nq->dev;
+ 	struct nullb *nullb = dev->nullb;
+ 	int err = 0;
+ 
+ 	if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+ 		struct request *rq = cmd->rq;
+ 
+ 		if (!hrtimer_active(&nullb->bw_timer))
+ 			hrtimer_restart(&nullb->bw_timer);
+ 
+ 		if (atomic_long_sub_return(blk_rq_bytes(rq),
+ 				&nullb->cur_bytes) < 0) {
+ 			null_stop_queue(nullb);
+ 			/* race with timer */
+ 			if (atomic_long_read(&nullb->cur_bytes) > 0)
+ 				null_restart_queue_async(nullb);
+ 			if (dev->queue_mode == NULL_Q_RQ) {
+ 				struct request_queue *q = nullb->q;
+ 
+ 				spin_lock_irq(q->queue_lock);
+ 				rq->rq_flags |= RQF_DONTPREP;
+ 				blk_requeue_request(q, rq);
+ 				spin_unlock_irq(q->queue_lock);
+ 				return BLK_STS_OK;
+ 			} else
+ 				/* requeue request */
+ 				return BLK_STS_DEV_RESOURCE;
+ 		}
+ 	}
+ 
+ 	if (nullb->dev->badblocks.shift != -1) {
+ 		int bad_sectors;
+ 		sector_t sector, size, first_bad;
+ 		bool is_flush = true;
+ 
+ 		if (dev->queue_mode == NULL_Q_BIO &&
+ 				bio_op(cmd->bio) != REQ_OP_FLUSH) {
+ 			is_flush = false;
+ 			sector = cmd->bio->bi_iter.bi_sector;
+ 			size = bio_sectors(cmd->bio);
+ 		}
+ 		if (dev->queue_mode != NULL_Q_BIO &&
+ 				req_op(cmd->rq) != REQ_OP_FLUSH) {
+ 			is_flush = false;
+ 			sector = blk_rq_pos(cmd->rq);
+ 			size = blk_rq_sectors(cmd->rq);
+ 		}
+ 		if (!is_flush && badblocks_check(&nullb->dev->badblocks, sector,
+ 				size, &first_bad, &bad_sectors)) {
+ 			cmd->error = BLK_STS_IOERR;
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	if (dev->memory_backed) {
+ 		if (dev->queue_mode == NULL_Q_BIO) {
+ 			if (bio_op(cmd->bio) == REQ_OP_FLUSH)
+ 				err = null_handle_flush(nullb);
+ 			else
+ 				err = null_handle_bio(cmd);
+ 		} else {
+ 			if (req_op(cmd->rq) == REQ_OP_FLUSH)
+ 				err = null_handle_flush(nullb);
+ 			else
+ 				err = null_handle_rq(cmd);
+ 		}
+ 	}
+ 	cmd->error = errno_to_blk_status(err);
+ out:
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  	/* Complete IO by inline, softirq or timer */
 -	switch (dev->irqmode) {
 +	switch (irqmode) {
  	case NULL_IRQ_SOFTIRQ:
 -		switch (dev->queue_mode)  {
 +		switch (queue_mode)  {
  		case NULL_Q_MQ:
 -			blk_mq_complete_request(cmd->rq);
 +			blk_mq_complete_request(cmd->rq, cmd->rq->errors);
  			break;
  		case NULL_Q_RQ:
  			blk_complete_request(cmd->rq);
diff --cc drivers/block/virtio_blk.c
index 12e2f30eeaac,79908e6ddbf2..000000000000
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@@ -273,6 -137,181 +273,184 @@@ static int virtblk_ioctl(struct block_d
  	return scsi_cmd_blk_ioctl(bdev, mode, cmd,
  				  (void __user *)data);
  }
++<<<<<<< HEAD
++=======
+ #else
+ static inline int virtblk_add_req_scsi(struct virtqueue *vq,
+ 		struct virtblk_req *vbr, struct scatterlist *data_sg,
+ 		bool have_data)
+ {
+ 	return -EIO;
+ }
+ static inline void virtblk_scsi_request_done(struct request *req)
+ {
+ }
+ #define virtblk_ioctl	NULL
+ #endif /* CONFIG_VIRTIO_BLK_SCSI */
+ 
+ static int virtblk_add_req(struct virtqueue *vq, struct virtblk_req *vbr,
+ 		struct scatterlist *data_sg, bool have_data)
+ {
+ 	struct scatterlist hdr, status, *sgs[3];
+ 	unsigned int num_out = 0, num_in = 0;
+ 
+ 	sg_init_one(&hdr, &vbr->out_hdr, sizeof(vbr->out_hdr));
+ 	sgs[num_out++] = &hdr;
+ 
+ 	if (have_data) {
+ 		if (vbr->out_hdr.type & cpu_to_virtio32(vq->vdev, VIRTIO_BLK_T_OUT))
+ 			sgs[num_out++] = data_sg;
+ 		else
+ 			sgs[num_out + num_in++] = data_sg;
+ 	}
+ 
+ 	sg_init_one(&status, &vbr->status, sizeof(vbr->status));
+ 	sgs[num_out + num_in++] = &status;
+ 
+ 	return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
+ }
+ 
+ static inline void virtblk_request_done(struct request *req)
+ {
+ 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ 
+ 	switch (req_op(req)) {
+ 	case REQ_OP_SCSI_IN:
+ 	case REQ_OP_SCSI_OUT:
+ 		virtblk_scsi_request_done(req);
+ 		break;
+ 	}
+ 
+ 	blk_mq_end_request(req, virtblk_result(vbr));
+ }
+ 
+ static void virtblk_done(struct virtqueue *vq)
+ {
+ 	struct virtio_blk *vblk = vq->vdev->priv;
+ 	bool req_done = false;
+ 	int qid = vq->index;
+ 	struct virtblk_req *vbr;
+ 	unsigned long flags;
+ 	unsigned int len;
+ 
+ 	spin_lock_irqsave(&vblk->vqs[qid].lock, flags);
+ 	do {
+ 		virtqueue_disable_cb(vq);
+ 		while ((vbr = virtqueue_get_buf(vblk->vqs[qid].vq, &len)) != NULL) {
+ 			struct request *req = blk_mq_rq_from_pdu(vbr);
+ 
+ 			blk_mq_complete_request(req);
+ 			req_done = true;
+ 		}
+ 		if (unlikely(virtqueue_is_broken(vq)))
+ 			break;
+ 	} while (!virtqueue_enable_cb(vq));
+ 
+ 	/* In case queue is stopped waiting for more buffers. */
+ 	if (req_done)
+ 		blk_mq_start_stopped_hw_queues(vblk->disk->queue, true);
+ 	spin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);
+ }
+ 
+ static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
+ 			   const struct blk_mq_queue_data *bd)
+ {
+ 	struct virtio_blk *vblk = hctx->queue->queuedata;
+ 	struct request *req = bd->rq;
+ 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ 	unsigned long flags;
+ 	unsigned int num;
+ 	int qid = hctx->queue_num;
+ 	int err;
+ 	bool notify = false;
+ 	u32 type;
+ 
+ 	BUG_ON(req->nr_phys_segments + 2 > vblk->sg_elems);
+ 
+ 	switch (req_op(req)) {
+ 	case REQ_OP_READ:
+ 	case REQ_OP_WRITE:
+ 		type = 0;
+ 		break;
+ 	case REQ_OP_FLUSH:
+ 		type = VIRTIO_BLK_T_FLUSH;
+ 		break;
+ 	case REQ_OP_SCSI_IN:
+ 	case REQ_OP_SCSI_OUT:
+ 		type = VIRTIO_BLK_T_SCSI_CMD;
+ 		break;
+ 	case REQ_OP_DRV_IN:
+ 		type = VIRTIO_BLK_T_GET_ID;
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		return BLK_STS_IOERR;
+ 	}
+ 
+ 	vbr->out_hdr.type = cpu_to_virtio32(vblk->vdev, type);
+ 	vbr->out_hdr.sector = type ?
+ 		0 : cpu_to_virtio64(vblk->vdev, blk_rq_pos(req));
+ 	vbr->out_hdr.ioprio = cpu_to_virtio32(vblk->vdev, req_get_ioprio(req));
+ 
+ 	blk_mq_start_request(req);
+ 
+ 	num = blk_rq_map_sg(hctx->queue, req, vbr->sg);
+ 	if (num) {
+ 		if (rq_data_dir(req) == WRITE)
+ 			vbr->out_hdr.type |= cpu_to_virtio32(vblk->vdev, VIRTIO_BLK_T_OUT);
+ 		else
+ 			vbr->out_hdr.type |= cpu_to_virtio32(vblk->vdev, VIRTIO_BLK_T_IN);
+ 	}
+ 
+ 	spin_lock_irqsave(&vblk->vqs[qid].lock, flags);
+ 	if (blk_rq_is_scsi(req))
+ 		err = virtblk_add_req_scsi(vblk->vqs[qid].vq, vbr, vbr->sg, num);
+ 	else
+ 		err = virtblk_add_req(vblk->vqs[qid].vq, vbr, vbr->sg, num);
+ 	if (err) {
+ 		virtqueue_kick(vblk->vqs[qid].vq);
+ 		blk_mq_stop_hw_queue(hctx);
+ 		spin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);
+ 		/* Out of mem doesn't actually happen, since we fall back
+ 		 * to direct descriptors */
+ 		if (err == -ENOMEM || err == -ENOSPC)
+ 			return BLK_STS_DEV_RESOURCE;
+ 		return BLK_STS_IOERR;
+ 	}
+ 
+ 	if (bd->last && virtqueue_kick_prepare(vblk->vqs[qid].vq))
+ 		notify = true;
+ 	spin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);
+ 
+ 	if (notify)
+ 		virtqueue_notify(vblk->vqs[qid].vq);
+ 	return BLK_STS_OK;
+ }
+ 
+ /* return id (s/n) string for *disk to *id_str
+  */
+ static int virtblk_get_id(struct gendisk *disk, char *id_str)
+ {
+ 	struct virtio_blk *vblk = disk->private_data;
+ 	struct request_queue *q = vblk->disk->queue;
+ 	struct request *req;
+ 	int err;
+ 
+ 	req = blk_get_request(q, REQ_OP_DRV_IN, GFP_KERNEL);
+ 	if (IS_ERR(req))
+ 		return PTR_ERR(req);
+ 
+ 	err = blk_rq_map_kern(q, req, id_str, VIRTIO_BLK_ID_BYTES, GFP_KERNEL);
+ 	if (err)
+ 		goto out;
+ 
+ 	blk_execute_rq(vblk->disk->queue, vblk->disk, req, false);
+ 	err = blk_status_to_errno(virtblk_result(blk_mq_rq_to_pdu(req)));
+ out:
+ 	blk_put_request(req);
+ 	return err;
+ }
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  
  /* We provide getgeo only to please some old bootloader/partitioning tools */
  static int virtblk_getgeo(struct block_device *bd, struct hd_geometry *geo)
diff --cc drivers/block/xen-blkfront.c
index 5cb25b62d3a8,e126e4cac2ca..000000000000
--- a/drivers/block/xen-blkfront.c
+++ b/drivers/block/xen-blkfront.c
@@@ -596,71 -872,64 +596,78 @@@ static inline void flush_requests(struc
  static inline bool blkif_request_flush_invalid(struct request *req,
  					       struct blkfront_info *info)
  {
 -	return (blk_rq_is_passthrough(req) ||
 -		((req_op(req) == REQ_OP_FLUSH) &&
 -		 !info->feature_flush) ||
 +	return ((req->cmd_type != REQ_TYPE_FS) ||
 +		((req->cmd_flags & REQ_FLUSH) &&
 +		 !(info->feature_flush & REQ_FLUSH)) ||
  		((req->cmd_flags & REQ_FUA) &&
 -		 !info->feature_fua));
 +		 !(info->feature_flush & REQ_FUA)));
  }
  
 -static blk_status_t blkif_queue_rq(struct blk_mq_hw_ctx *hctx,
 -			  const struct blk_mq_queue_data *qd)
 +/*
 + * do_blkif_request
 + *  read a block; request is in a request queue
 + */
 +static void do_blkif_request(struct request_queue *rq)
  {
 -	unsigned long flags;
 -	int qid = hctx->queue_num;
 -	struct blkfront_info *info = hctx->queue->queuedata;
 -	struct blkfront_ring_info *rinfo = NULL;
 +	struct blkfront_info *info = NULL;
 +	struct request *req;
 +	int queued;
  
 -	BUG_ON(info->nr_rings <= qid);
 -	rinfo = &info->rinfo[qid];
 -	blk_mq_start_request(qd->rq);
 -	spin_lock_irqsave(&rinfo->ring_lock, flags);
 -	if (RING_FULL(&rinfo->ring))
 -		goto out_busy;
 +	pr_debug("Entered do_blkif_request\n");
  
 -	if (blkif_request_flush_invalid(qd->rq, rinfo->dev_info))
 -		goto out_err;
 +	queued = 0;
  
 -	if (blkif_queue_request(qd->rq, rinfo))
 -		goto out_busy;
 +	while ((req = blk_peek_request(rq)) != NULL) {
 +		info = req->rq_disk->private_data;
  
 -	flush_requests(rinfo);
 -	spin_unlock_irqrestore(&rinfo->ring_lock, flags);
 -	return BLK_STS_OK;
 +		if (RING_FULL(&info->ring))
 +			goto wait;
  
 -out_err:
 -	spin_unlock_irqrestore(&rinfo->ring_lock, flags);
 -	return BLK_STS_IOERR;
 +		blk_start_request(req);
 +
++<<<<<<< HEAD
 +		if (blkif_request_flush_invalid(req, info)) {
 +			__blk_end_request_all(req, -EOPNOTSUPP);
 +			continue;
 +		}
 +
 +		pr_debug("do_blk_req %p: cmd %p, sec %lx, "
 +			 "(%u/%u) buffer:%p [%s]\n",
 +			 req, req->cmd, (unsigned long)blk_rq_pos(req),
 +			 blk_rq_cur_sectors(req), blk_rq_sectors(req),
 +			 req->buffer, rq_data_dir(req) ? "write" : "read");
 +
 +		if (blkif_queue_request(req)) {
 +			blk_requeue_request(rq, req);
 +wait:
 +			/* Avoid pointless unplugs. */
 +			blk_stop_queue(rq);
 +			break;
 +		}
  
 +		queued++;
 +	}
 +
 +	if (queued != 0)
 +		flush_requests(info);
++=======
+ out_busy:
+ 	blk_mq_stop_hw_queue(hctx);
+ 	spin_unlock_irqrestore(&rinfo->ring_lock, flags);
+ 	return BLK_STS_DEV_RESOURCE;
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  }
  
 -static void blkif_complete_rq(struct request *rq)
 +static int xlvbd_init_blk_queue(struct gendisk *gd, u16 sector_size,
 +				unsigned int physical_sector_size,
 +				unsigned int segments)
  {
 -	blk_mq_end_request(rq, blkif_req(rq)->error);
 -}
 -
 -static const struct blk_mq_ops blkfront_mq_ops = {
 -	.queue_rq = blkif_queue_rq,
 -	.complete = blkif_complete_rq,
 -};
 +	struct request_queue *rq;
 +	struct blkfront_info *info = gd->private_data;
  
 -static void blkif_set_queue_limits(struct blkfront_info *info)
 -{
 -	struct request_queue *rq = info->rq;
 -	struct gendisk *gd = info->gd;
 -	unsigned int segments = info->max_indirect_segments ? :
 -				BLKIF_MAX_SEGMENTS_PER_REQUEST;
 +	rq = blk_init_queue(do_blkif_request, &info->io_lock);
 +	if (rq == NULL)
 +		return -1;
  
  	queue_flag_set_unlocked(QUEUE_FLAG_VIRT, rq);
  
diff --cc drivers/md/dm-rq.c
index d5df417cac04,348a0cb6963a..000000000000
--- a/drivers/md/dm-rq.c
+++ b/drivers/md/dm-rq.c
@@@ -477,9 -404,10 +477,13 @@@ static void dm_dispatch_clone_request(s
  
  	clone->start_time = jiffies;
  	r = blk_insert_cloned_request(clone->q, clone);
++<<<<<<< HEAD
 +	if (r)
++=======
+ 	if (r != BLK_STS_OK && r != BLK_STS_RESOURCE && r != BLK_STS_DEV_RESOURCE)
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  		/* must complete clone in terms of original request */
  		dm_complete_request(rq, r);
 -	return r;
  }
  
  static int dm_rq_bio_constructor(struct bio *bio, struct bio *bio_orig,
@@@ -658,7 -495,17 +662,21 @@@ static int map_request(struct dm_rq_tar
  		/* The target has remapped the I/O so dispatch it */
  		trace_block_rq_remap(clone->q, clone, disk_devt(dm_disk(md)),
  				     blk_rq_pos(rq));
++<<<<<<< HEAD
 +		dm_dispatch_clone_request(clone, rq);
++=======
+ 		ret = dm_dispatch_clone_request(clone, rq);
+ 		if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE) {
+ 			blk_rq_unprep_clone(clone);
+ 			tio->ti->type->release_clone_rq(clone);
+ 			tio->clone = NULL;
+ 			if (!rq->q->mq_ops)
+ 				r = DM_MAPIO_DELAY_REQUEUE;
+ 			else
+ 				r = DM_MAPIO_REQUEUE;
+ 			goto check_again;
+ 		}
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  		break;
  	case DM_MAPIO_REQUEUE:
  		/* The target wants to requeue the I/O */
@@@ -901,13 -769,13 +919,17 @@@ static int dm_mq_queue_rq(struct blk_mq
  		/* Undo dm_start_request() before requeuing */
  		rq_end_stats(md, rq);
  		rq_completed(md, rq_data_dir(rq), false);
++<<<<<<< HEAD
 +		return BLK_MQ_RQ_QUEUE_BUSY;
++=======
+ 		return BLK_STS_RESOURCE;
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  	}
  
 -	return BLK_STS_OK;
 +	return BLK_MQ_RQ_QUEUE_OK;
  }
  
 -static const struct blk_mq_ops dm_mq_ops = {
 +static struct blk_mq_ops dm_mq_ops = {
  	.queue_rq = dm_mq_queue_rq,
  	.complete = dm_softirq_done,
  	.init_request = dm_mq_init_request,
diff --cc drivers/nvme/host/fc.c
index d165c0527376,b856d7c919d2..000000000000
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@@ -30,21 -30,11 +30,19 @@@
  /* *************************** Data Structures/Defines ****************** */
  
  
 +/*
 + * We handle AEN commands ourselves and don't even let the
 + * block layer know about them.
 + */
 +#define NVME_FC_NR_AEN_COMMANDS	1
 +#define NVME_FC_AQ_BLKMQ_DEPTH	\
 +	(NVME_AQ_DEPTH - NVME_FC_NR_AEN_COMMANDS)
 +#define AEN_CMDID_BASE		(NVME_FC_AQ_BLKMQ_DEPTH + 1)
 +
  enum nvme_fc_queue_flags {
 -	NVME_FC_Q_CONNECTED = 0,
 -	NVME_FC_Q_LIVE,
 +	NVME_FC_Q_CONNECTED = (1 << 0),
  };
  
- #define NVMEFC_QUEUE_DELAY	3		/* ms units */
- 
  #define NVME_FC_DEFAULT_DEV_LOSS_TMO	60	/* seconds */
  
  struct nvme_fc_queue {
@@@ -2247,10 -2229,10 +2245,10 @@@ nvme_fc_start_fcp_op(struct nvme_fc_ctr
  	 * the target device is present
  	 */
  	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE)
- 		goto busy;
+ 		return BLK_STS_RESOURCE;
  
  	if (!nvme_fc_ctrl_get(ctrl))
 -		return BLK_STS_IOERR;
 +		return BLK_MQ_RQ_QUEUE_ERROR;
  
  	/* format the FC-NVME CMD IU and fcp_req */
  	cmdiu->connection_id = cpu_to_be64(queue->connection_id);
@@@ -2327,21 -2307,23 +2325,25 @@@
  
  		if (ctrl->rport->remoteport.port_state == FC_OBJSTATE_ONLINE &&
  				ret != -EBUSY)
 -			return BLK_STS_IOERR;
 +			return BLK_MQ_RQ_QUEUE_ERROR;
  
- 		goto busy;
+ 		return BLK_STS_RESOURCE;
  	}
  
 -	return BLK_STS_OK;
 -}
++<<<<<<< HEAD
 +	return BLK_MQ_RQ_QUEUE_OK;
  
 -static inline blk_status_t nvme_fc_is_ready(struct nvme_fc_queue *queue,
 -		struct request *rq)
 -{
 -	if (unlikely(!test_bit(NVME_FC_Q_LIVE, &queue->flags)))
 -		return nvmf_check_init_req(&queue->ctrl->ctrl, rq);
 +busy:
 +	if (!(op->flags & FCOP_FLAGS_AEN) && queue->hctx)
 +		blk_mq_delay_run_hw_queue(queue->hctx, NVMEFC_QUEUE_DELAY);
 +
 +	return BLK_MQ_RQ_QUEUE_BUSY;
++=======
+ 	return BLK_STS_OK;
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  }
  
 -static blk_status_t
 +static int
  nvme_fc_queue_rq(struct blk_mq_hw_ctx *hctx,
  			const struct blk_mq_queue_data *bd)
  {
diff --cc drivers/scsi/scsi_lib.c
index e0f2e99b8896,55be2550c555..000000000000
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@@ -1892,12 -2027,14 +1892,21 @@@ out_dec_target_busy
  out_put_budget:
  	scsi_mq_put_budget(hctx);
  	switch (ret) {
++<<<<<<< HEAD
 +	case BLK_MQ_RQ_QUEUE_BUSY:
 +		if (atomic_read(&sdev->device_busy) == 0 &&
 +		    !scsi_device_blocked(sdev))
 +			blk_mq_delay_run_hw_queue(hctx, SCSI_QUEUE_DELAY);
++=======
+ 	case BLK_STS_OK:
+ 		break;
+ 	case BLK_STS_RESOURCE:
+ 		if (atomic_read(&sdev->device_busy) ||
+ 		    scsi_device_blocked(sdev))
+ 			ret = BLK_STS_DEV_RESOURCE;
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
  		break;
 -	default:
 +	case BLK_MQ_RQ_QUEUE_ERROR:
  		/*
  		 * Make sure to release all allocated ressources when
  		 * we hit an error, as we will never see this command
diff --cc include/linux/blk_types.h
index a78cedbfc79d,bf18b95ed92d..000000000000
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@@ -16,37 -16,79 +16,106 @@@ struct page
  struct block_device;
  struct io_context;
  struct cgroup_subsys_state;
 -typedef void (bio_end_io_t) (struct bio *);
 +typedef void (bio_end_io_t) (struct bio *, int);
 +typedef void (bio_destructor_t) (struct bio *);
  
  /*
 - * Block error status values.  See block/blk-core:blk_errors for the details.
 + * was unsigned short, but we might as well be ready for > 64kB I/O pages
   */
++<<<<<<< HEAD
 +struct bio_vec {
 +	struct page	*bv_page;
 +	unsigned int	bv_len;
 +	unsigned int	bv_offset;
++=======
+ typedef u8 __bitwise blk_status_t;
+ #define	BLK_STS_OK 0
+ #define BLK_STS_NOTSUPP		((__force blk_status_t)1)
+ #define BLK_STS_TIMEOUT		((__force blk_status_t)2)
+ #define BLK_STS_NOSPC		((__force blk_status_t)3)
+ #define BLK_STS_TRANSPORT	((__force blk_status_t)4)
+ #define BLK_STS_TARGET		((__force blk_status_t)5)
+ #define BLK_STS_NEXUS		((__force blk_status_t)6)
+ #define BLK_STS_MEDIUM		((__force blk_status_t)7)
+ #define BLK_STS_PROTECTION	((__force blk_status_t)8)
+ #define BLK_STS_RESOURCE	((__force blk_status_t)9)
+ #define BLK_STS_IOERR		((__force blk_status_t)10)
+ 
+ /* hack for device mapper, don't use elsewhere: */
+ #define BLK_STS_DM_REQUEUE    ((__force blk_status_t)11)
+ 
+ #define BLK_STS_AGAIN		((__force blk_status_t)12)
+ 
+ /*
+  * BLK_STS_DEV_RESOURCE is returned from the driver to the block layer if
+  * device related resources are unavailable, but the driver can guarantee
+  * that the queue will be rerun in the future once resources become
+  * available again. This is typically the case for device specific
+  * resources that are consumed for IO. If the driver fails allocating these
+  * resources, we know that inflight (or pending) IO will free these
+  * resource upon completion.
+  *
+  * This is different from BLK_STS_RESOURCE in that it explicitly references
+  * a device specific resource. For resources of wider scope, allocation
+  * failure can happen without having pending IO. This means that we can't
+  * rely on request completions freeing these resources, as IO may not be in
+  * flight. Examples of that are kernel memory allocations, DMA mappings, or
+  * any other system wide resources.
+  */
+ #define BLK_STS_DEV_RESOURCE	((__force blk_status_t)13)
+ 
+ /**
+  * blk_path_error - returns true if error may be path related
+  * @error: status the request was completed with
+  *
+  * Description:
+  *     This classifies block error status into non-retryable errors and ones
+  *     that may be successful if retried on a failover path.
+  *
+  * Return:
+  *     %false - retrying failover path will not help
+  *     %true  - may succeed if retried
+  */
+ static inline bool blk_path_error(blk_status_t error)
+ {
+ 	switch (error) {
+ 	case BLK_STS_NOTSUPP:
+ 	case BLK_STS_NOSPC:
+ 	case BLK_STS_TARGET:
+ 	case BLK_STS_NEXUS:
+ 	case BLK_STS_MEDIUM:
+ 	case BLK_STS_PROTECTION:
+ 		return false;
+ 	}
+ 
+ 	/* Anything else could be a path failure, so should be retried */
+ 	return true;
+ }
+ 
+ struct blk_issue_stat {
+ 	u64 stat;
++>>>>>>> 86ff7c2a80cd (blk-mq: introduce BLK_STS_DEV_RESOURCE)
 +};
 +
 +/*
 + * RHEL7 auxillary shadow structure used to extend 'struct bio' without
 + * breaking RHEL kABI -- bio_init_aux() must be used to set bio->bio_aux
 + */
 +struct bio_aux {
 +	unsigned long	bi_flags;
 +	atomic_t	__bi_remaining;
 +
 +	/*
 +	 * IMPORTANT: adding any new members to this struct will require a more
 +	 * comprehensive audit (e.g. all bio_init() callers checked to see if
 +	 * they'll need to make use of the new bio_aux member(s) you're adding).
 +	 */
  };
  
 +#define BIO_AUX_CHAIN	0	/* chained bio, ->bi_remaining in effect */
 +
 +#define bio_aux_flagged(bio, flag)	((bio)->bio_aux && (bio)->bio_aux->bi_flags & (1 << (flag)))
 +
  /*
   * main unit of I/O for the block layer and lower layers (ie drivers and
   * stacking drivers)
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq.c
* Unmerged path drivers/block/null_blk.c
* Unmerged path drivers/block/virtio_blk.c
* Unmerged path drivers/block/xen-blkfront.c
* Unmerged path drivers/md/dm-rq.c
* Unmerged path drivers/nvme/host/fc.c
* Unmerged path drivers/scsi/scsi_lib.c
* Unmerged path include/linux/blk_types.h
