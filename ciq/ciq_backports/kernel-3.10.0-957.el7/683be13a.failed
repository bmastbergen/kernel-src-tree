timer: Minimize nohz off overhead

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 683be13a284720205228e29207ef11a1c3c322b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/683be13a.failed

If nohz is disabled on the kernel command line the [hr]timer code
still calls wake_up_nohz_cpu() and tick_nohz_full_cpu(), a pretty
pointless exercise. Cache nohz_active in [hr]timer per cpu bases and
avoid the overhead.

Before:
  48.10%  hog       [.] main
  15.25%  [kernel]  [k] _raw_spin_lock_irqsave
   9.76%  [kernel]  [k] _raw_spin_unlock_irqrestore
   6.50%  [kernel]  [k] mod_timer
   6.44%  [kernel]  [k] lock_timer_base.isra.38
   3.87%  [kernel]  [k] detach_if_pending
   3.80%  [kernel]  [k] del_timer
   2.67%  [kernel]  [k] internal_add_timer
   1.33%  [kernel]  [k] __internal_add_timer
   0.73%  [kernel]  [k] timerfn
   0.54%  [kernel]  [k] wake_up_nohz_cpu

After:
  48.73%  hog       [.] main
  15.36%  [kernel]  [k] _raw_spin_lock_irqsave
   9.77%  [kernel]  [k] _raw_spin_unlock_irqrestore
   6.61%  [kernel]  [k] lock_timer_base.isra.38
   6.42%  [kernel]  [k] mod_timer
   3.90%  [kernel]  [k] detach_if_pending
   3.76%  [kernel]  [k] del_timer
   2.41%  [kernel]  [k] internal_add_timer
   1.39%  [kernel]  [k] __internal_add_timer
   0.76%  [kernel]  [k] timerfn

We probably should have a cached value for nohz full in the per cpu
bases as well to avoid the cpumask check. The base cache line is hot
already, the cpumask not necessarily.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Eric Dumazet <edumazet@google.com>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
	Cc: John Stultz <john.stultz@linaro.org>
	Cc: Joonwoo Park <joonwoop@codeaurora.org>
	Cc: Wenbo Wang <wenbo.wang@memblaze.com>
Link: http://lkml.kernel.org/r/20150526224512.207378134@linutronix.de
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 683be13a284720205228e29207ef11a1c3c322b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hrtimer.h
#	kernel/time/tick-internal.h
#	kernel/time/tick-sched.c
#	kernel/timer.c
diff --cc include/linux/hrtimer.h
index cc981460a2c7,76dd4f0da5ca..000000000000
--- a/include/linux/hrtimer.h
+++ b/include/linux/hrtimer.h
@@@ -164,12 -158,17 +164,18 @@@ enum  hrtimer_base_type 
   * struct hrtimer_cpu_base - the per cpu clock bases
   * @lock:		lock protecting the base and associated clock bases
   *			and timers
 - * @seq:		seqcount around __run_hrtimer
 - * @running:		pointer to the currently running hrtimer
 - * @cpu:		cpu number
   * @active_bases:	Bitfield to mark bases with active timers
++<<<<<<< HEAD
 + * @clock_was_set:	Sequence counter of clock was set events
 + *			Note that in RHEL7 clock_was_set is upstream's
 + *			clock_was_set_seq (KABI).
++=======
+  * @clock_was_set_seq:	Sequence counter of clock was set events
+  * @migration_enabled:	The migration of hrtimers to other cpus is enabled
+  * @nohz_active:	The nohz functionality is enabled
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead)
   * @expires_next:	absolute time of the next event which was scheduled
   *			via clock_set_next_event()
 - * @next_timer:		Pointer to the first expiring timer
 - * @in_hrtirq:		hrtimer_interrupt() is currently executing
   * @hres_active:	State of high resolution mode
   * @hang_detected:	The last hrtimer interrupt detected a hang
   * @nr_events:		Total number of hrtimer interrupt events
@@@ -182,21 -183,26 +188,27 @@@
   */
  struct hrtimer_cpu_base {
  	raw_spinlock_t			lock;
 -	seqcount_t			seq;
 -	struct hrtimer			*running;
 -	unsigned int			cpu;
  	unsigned int			active_bases;
++<<<<<<< HEAD
 +	unsigned int			clock_was_set; /* clock_was_set_seq */
++=======
+ 	unsigned int			clock_was_set_seq;
+ 	bool				migration_enabled;
+ 	bool				nohz_active;
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead)
  #ifdef CONFIG_HIGH_RES_TIMERS
 -	unsigned int			in_hrtirq	: 1,
 -					hres_active	: 1,
 -					hang_detected	: 1;
  	ktime_t				expires_next;
 -	struct hrtimer			*next_timer;
 -	unsigned int			nr_events;
 -	unsigned int			nr_retries;
 -	unsigned int			nr_hangs;
 -	unsigned int			max_hang_time;
 +	int				hres_active;
 +	int				hang_detected;
 +	unsigned long			nr_events;
 +	unsigned long			nr_retries;
 +	unsigned long			nr_hangs;
 +	ktime_t				max_hang_time;
  #endif
  	struct hrtimer_clock_base	clock_base[HRTIMER_MAX_CLOCK_BASES];
 -} ____cacheline_aligned;
 +	RH_KABI_EXTEND(int cpu)
 +	RH_KABI_EXTEND(int in_hrtirq)
 +};
  
  static inline void hrtimer_set_expires(struct hrtimer *timer, ktime_t time)
  {
diff --cc kernel/time/tick-internal.h
index 74ad669ea929,966a5a6fdd0a..000000000000
--- a/kernel/time/tick-internal.h
+++ b/kernel/time/tick-internal.h
@@@ -147,20 -89,78 +147,36 @@@ static inline void tick_set_periodic_ha
  {
  	dev->event_handler = tick_handle_periodic;
  }
 -# endif /* !CONFIG_GENERIC_CLOCKEVENTS_BROADCAST */
 +#endif /* !BROADCAST */
  
 -#else /* !GENERIC_CLOCKEVENTS: */
 -static inline void tick_suspend(void) { }
 -static inline void tick_resume(void) { }
 -#endif /* !GENERIC_CLOCKEVENTS */
 +/*
 + * Check, if the device is functional or a dummy for broadcast
 + */
 +static inline int tick_device_is_functional(struct clock_event_device *dev)
 +{
 +	return !(dev->features & CLOCK_EVT_FEAT_DUMMY);
 +}
  
 -/* Oneshot related functions */
 -#ifdef CONFIG_TICK_ONESHOT
 -extern void tick_setup_oneshot(struct clock_event_device *newdev,
 -			       void (*handler)(struct clock_event_device *),
 -			       ktime_t nextevt);
 -extern int tick_program_event(ktime_t expires, int force);
 -extern void tick_oneshot_notify(void);
 -extern int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *));
 -extern void tick_resume_oneshot(void);
 -static inline bool tick_oneshot_possible(void) { return true; }
 -extern int tick_oneshot_mode_active(void);
 -extern void tick_clock_notify(void);
 -extern int tick_check_oneshot_change(int allow_nohz);
 -extern int tick_init_highres(void);
 -#else /* !CONFIG_TICK_ONESHOT: */
 -static inline
 -void tick_setup_oneshot(struct clock_event_device *newdev,
 -			void (*handler)(struct clock_event_device *),
 -			ktime_t nextevt) { BUG(); }
 -static inline void tick_resume_oneshot(void) { BUG(); }
 -static inline int tick_program_event(ktime_t expires, int force) { return 0; }
 -static inline void tick_oneshot_notify(void) { }
 -static inline bool tick_oneshot_possible(void) { return false; }
 -static inline int tick_oneshot_mode_active(void) { return 0; }
 -static inline void tick_clock_notify(void) { }
 -static inline int tick_check_oneshot_change(int allow_nohz) { return 0; }
 -#endif /* !CONFIG_TICK_ONESHOT */
 -
 -/* Functions related to oneshot broadcasting */
 -#if defined(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST) && defined(CONFIG_TICK_ONESHOT)
 -extern void tick_broadcast_setup_oneshot(struct clock_event_device *bc);
 -extern void tick_broadcast_switch_to_oneshot(void);
 -extern void tick_shutdown_broadcast_oneshot(unsigned int cpu);
 -extern int tick_broadcast_oneshot_active(void);
 -extern void tick_check_oneshot_broadcast_this_cpu(void);
 -bool tick_broadcast_oneshot_available(void);
 -extern struct cpumask *tick_get_broadcast_oneshot_mask(void);
 -#else /* !(BROADCAST && ONESHOT): */
 -static inline void tick_broadcast_setup_oneshot(struct clock_event_device *bc) { BUG(); }
 -static inline void tick_broadcast_switch_to_oneshot(void) { }
 -static inline void tick_shutdown_broadcast_oneshot(unsigned int cpu) { }
 -static inline int tick_broadcast_oneshot_active(void) { return 0; }
 -static inline void tick_check_oneshot_broadcast_this_cpu(void) { }
 -static inline bool tick_broadcast_oneshot_available(void) { return tick_oneshot_possible(); }
 -#endif /* !(BROADCAST && ONESHOT) */
 -
 -/* NO_HZ_FULL internal */
 -#ifdef CONFIG_NO_HZ_FULL
 -extern void tick_nohz_init(void);
 -# else
 -static inline void tick_nohz_init(void) { }
  #endif
  
++<<<<<<< HEAD
 +int __clockevents_update_freq(struct clock_event_device *dev, u32 freq);
 +extern void do_timer(unsigned long ticks);
 +extern void update_wall_time(void);
++=======
+ #ifdef CONFIG_NO_HZ_COMMON
+ extern unsigned long tick_nohz_active;
+ #else
+ #define tick_nohz_active (0)
+ #endif
+ 
+ #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
+ extern void timers_update_migration(bool update_nohz);
+ #else
+ static inline void timers_update_migration(bool update_nohz) { }
+ #endif
+ 
+ DECLARE_PER_CPU(struct hrtimer_cpu_base, hrtimer_bases);
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead)
  
  extern u64 get_next_timer_interrupt(unsigned long basej, u64 basem);
diff --cc kernel/time/tick-sched.c
index 85c7fe06eace,c792429e98c6..000000000000
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@@ -966,6 -956,16 +966,19 @@@ static void tick_nohz_handler(struct cl
  	tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
  }
  
++<<<<<<< HEAD
++=======
+ static inline void tick_nohz_activate(struct tick_sched *ts, int mode)
+ {
+ 	if (!tick_nohz_enabled)
+ 		return;
+ 	ts->nohz_mode = mode;
+ 	/* One update is enough */
+ 	if (!test_and_set_bit(0, &tick_nohz_active))
+ 		timers_update_migration(true);
+ }
+ 
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead)
  /**
   * tick_nohz_switch_to_nohz - switch to nohz mode
   */
diff --cc kernel/timer.c
index 032a9a53e430,520499dd85af..000000000000
--- a/kernel/timer.c
+++ b/kernel/timer.c
@@@ -81,41 -83,71 +81,85 @@@ struct tvec_base 
  	unsigned long timer_jiffies;
  	unsigned long next_timer;
  	unsigned long active_timers;
++<<<<<<< HEAD:kernel/timer.c
++=======
+ 	unsigned long all_timers;
+ 	int cpu;
+ 	bool migration_enabled;
+ 	bool nohz_active;
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead):kernel/time/timer.c
  	struct tvec_root tv1;
  	struct tvec tv2;
  	struct tvec tv3;
  	struct tvec tv4;
  	struct tvec tv5;
 +	RH_KABI_EXTEND(unsigned long all_timers)
  } ____cacheline_aligned;
  
 +struct tvec_base boot_tvec_bases;
 +EXPORT_SYMBOL(boot_tvec_bases);
 +static DEFINE_PER_CPU(struct tvec_base *, tvec_bases) = &boot_tvec_bases;
  
++<<<<<<< HEAD:kernel/timer.c
 +/* Functions below help us manage 'deferrable' flag */
 +static inline unsigned int tbase_get_deferrable(struct tvec_base *base)
 +{
 +	return ((unsigned int)(unsigned long)base & TIMER_DEFERRABLE);
++=======
+ static DEFINE_PER_CPU(struct tvec_base, tvec_bases);
+ 
+ #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
+ unsigned int sysctl_timer_migration = 1;
+ 
+ void timers_update_migration(bool update_nohz)
+ {
+ 	bool on = sysctl_timer_migration && tick_nohz_active;
+ 	unsigned int cpu;
+ 
+ 	/* Avoid the loop, if nothing to update */
+ 	if (this_cpu_read(tvec_bases.migration_enabled) == on)
+ 		return;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		per_cpu(tvec_bases.migration_enabled, cpu) = on;
+ 		per_cpu(hrtimer_bases.migration_enabled, cpu) = on;
+ 		if (!update_nohz)
+ 			continue;
+ 		per_cpu(tvec_bases.nohz_active, cpu) = true;
+ 		per_cpu(hrtimer_bases.nohz_active, cpu) = true;
+ 	}
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead):kernel/time/timer.c
  }
  
 -int timer_migration_handler(struct ctl_table *table, int write,
 -			    void __user *buffer, size_t *lenp,
 -			    loff_t *ppos)
 +static inline unsigned int tbase_get_irqsafe(struct tvec_base *base)
  {
++<<<<<<< HEAD:kernel/timer.c
 +	return ((unsigned int)(unsigned long)base & TIMER_IRQSAFE);
++=======
+ 	static DEFINE_MUTEX(mutex);
+ 	int ret;
+ 
+ 	mutex_lock(&mutex);
+ 	ret = proc_dointvec(table, write, buffer, lenp, ppos);
+ 	if (!ret && write)
+ 		timers_update_migration(false);
+ 	mutex_unlock(&mutex);
+ 	return ret;
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead):kernel/time/timer.c
  }
  
 -static inline struct tvec_base *get_target_base(struct tvec_base *base,
 -						int pinned)
 +static inline struct tvec_base *tbase_get_base(struct tvec_base *base)
  {
 -	if (pinned || !base->migration_enabled)
 -		return this_cpu_ptr(&tvec_bases);
 -	return per_cpu_ptr(&tvec_bases, get_nohz_timer_target());
 +	return ((struct tvec_base *)((unsigned long)base & ~TIMER_FLAG_MASK));
  }
 -#else
 -static inline struct tvec_base *get_target_base(struct tvec_base *base,
 -						int pinned)
 +
 +static inline void
 +timer_set_base(struct timer_list *timer, struct tvec_base *new_base)
  {
 -	return this_cpu_ptr(&tvec_bases);
 +	unsigned long flags = (unsigned long)timer->base & TIMER_FLAG_MASK;
 +
 +	timer->base = (struct tvec_base *)((unsigned long)(new_base) | flags);
  }
 -#endif
  
  static unsigned long round_jiffies_common(unsigned long j, int cpu,
  		bool force_up)
@@@ -409,8 -441,11 +453,16 @@@ static void internal_add_timer(struct t
  	 * require special care against races with idle_cpu(), lets deal
  	 * with that later.
  	 */
++<<<<<<< HEAD:kernel/timer.c
 +	if (!tbase_get_deferrable(timer->base) || tick_nohz_full_cpu(base->cpu))
 +		wake_up_nohz_cpu(base->cpu);
++=======
+ 	if (base->nohz_active) {
+ 		if (!(timer->flags & TIMER_DEFERRABLE) ||
+ 		    tick_nohz_full_cpu(base->cpu))
+ 			wake_up_nohz_cpu(base->cpu);
+ 	}
++>>>>>>> 683be13a2847 (timer: Minimize nohz off overhead):kernel/time/timer.c
  }
  
  #ifdef CONFIG_TIMER_STATS
* Unmerged path include/linux/hrtimer.h
diff --git a/kernel/hrtimer.c b/kernel/hrtimer.c
index ecbe27cbadd0..ad334450e824 100644
--- a/kernel/hrtimer.c
+++ b/kernel/hrtimer.c
@@ -1012,7 +1012,8 @@ int hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
 		 * Kick to reschedule the next tick to handle the new timer
 		 * on dynticks target.
 		 */
-		wake_up_nohz_cpu(new_base->cpu_base->cpu);
+		if (new_base->cpu_base->nohz_active)
+			wake_up_nohz_cpu(new_base->cpu_base->cpu);
 	} else {
 		hrtimer_reprogram(timer, new_base);
 	}
* Unmerged path kernel/time/tick-internal.h
* Unmerged path kernel/time/tick-sched.c
* Unmerged path kernel/timer.c
