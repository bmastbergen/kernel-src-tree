libceph: rados pool namespace support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Yan, Zheng <zyan@redhat.com>
commit 30c156d9951e0aa88202707d80c583b0a09d3167
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/30c156d9.failed

Add pool namesapce pointer to struct ceph_file_layout and struct
ceph_object_locator. Pool namespace is used by when mapping object
to PG, it's also used when composing OSD request.

The namespace pointer in struct ceph_file_layout is RCU protected.
So libceph can read namespace without taking lock.

	Signed-off-by: Yan, Zheng <zyan@redhat.com>
[idryomov@gmail.com: ceph_oloc_destroy(), misc minor changes]
	Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
(cherry picked from commit 30c156d9951e0aa88202707d80c583b0a09d3167)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/rbd.c
#	include/linux/ceph/ceph_fs.h
#	net/ceph/debugfs.c
#	net/ceph/osd_client.c
diff --cc drivers/block/rbd.c
index c4cb261ecbf7,58fd02d4e534..000000000000
--- a/drivers/block/rbd.c
+++ b/drivers/block/rbd.c
@@@ -4905,49 -3991,25 +4905,69 @@@ static struct rbd_device *__rbd_dev_cre
  
  	rbd_dev->rbd_client = rbdc;
  	rbd_dev->spec = spec;
++<<<<<<< HEAD
++=======
+ 	rbd_dev->opts = opts;
+ 
+ 	/* Initialize the layout used for all rbd requests */
+ 
+ 	rbd_dev->layout.stripe_unit = 1 << RBD_MAX_OBJ_ORDER;
+ 	rbd_dev->layout.stripe_count = 1;
+ 	rbd_dev->layout.object_size = 1 << RBD_MAX_OBJ_ORDER;
+ 	rbd_dev->layout.pool_id = spec->pool_id;
+ 	RCU_INIT_POINTER(rbd_dev->layout.pool_ns, NULL);
+ 
+ 	/*
+ 	 * If this is a mapping rbd_dev (as opposed to a parent one),
+ 	 * pin our module.  We have a ref from do_rbd_add(), so use
+ 	 * __module_get().
+ 	 */
+ 	if (rbd_dev->opts)
+ 		__module_get(THIS_MODULE);
++>>>>>>> 30c156d9951e (libceph: rados pool namespace support)
 +
 +	return rbd_dev;
 +}
 +
 +/*
 + * Create a mapping rbd_dev.
 + */
 +static struct rbd_device *rbd_dev_create(struct rbd_client *rbdc,
 +					 struct rbd_spec *spec,
 +					 struct rbd_options *opts)
 +{
 +	struct rbd_device *rbd_dev;
 +
 +	rbd_dev = __rbd_dev_create(rbdc, spec);
 +	if (!rbd_dev)
 +		return NULL;
 +
 +	rbd_dev->opts = opts;
 +
 +	/* get an id and fill in device name */
 +	rbd_dev->dev_id = ida_simple_get(&rbd_dev_id_ida, 0,
 +					 minor_to_rbd_dev_id(1 << MINORBITS),
 +					 GFP_KERNEL);
 +	if (rbd_dev->dev_id < 0)
 +		goto fail_rbd_dev;
 +
 +	sprintf(rbd_dev->name, RBD_DRV_NAME "%d", rbd_dev->dev_id);
 +	rbd_dev->task_wq = alloc_ordered_workqueue("%s-tasks", WQ_MEM_RECLAIM,
 +						   rbd_dev->name);
 +	if (!rbd_dev->task_wq)
 +		goto fail_dev_id;
 +
 +	/* we have a ref from do_rbd_add() */
 +	__module_get(THIS_MODULE);
  
 +	dout("%s rbd_dev %p dev_id %d\n", __func__, rbd_dev, rbd_dev->dev_id);
  	return rbd_dev;
 +
 +fail_dev_id:
 +	ida_simple_remove(&rbd_dev_id_ida, rbd_dev->dev_id);
 +fail_rbd_dev:
 +	rbd_dev_free(rbd_dev);
 +	return NULL;
  }
  
  static void rbd_dev_destroy(struct rbd_device *rbd_dev)
diff --cc include/linux/ceph/ceph_fs.h
index 45fc5ba3262d,08b8fd72261e..000000000000
--- a/include/linux/ceph/ceph_fs.h
+++ b/include/linux/ceph/ceph_fs.h
@@@ -53,28 -53,24 +53,43 @@@ struct ceph_file_layout 
  	__le32 fl_pg_pool;      /* namespace, crush ruleset, rep level */
  } __attribute__ ((packed));
  
++<<<<<<< HEAD
 +#define ceph_file_layout_su(l) ((__s32)le32_to_cpu((l).fl_stripe_unit))
 +#define ceph_file_layout_stripe_count(l) \
 +	((__s32)le32_to_cpu((l).fl_stripe_count))
 +#define ceph_file_layout_object_size(l) ((__s32)le32_to_cpu((l).fl_object_size))
 +#define ceph_file_layout_cas_hash(l) ((__s32)le32_to_cpu((l).fl_cas_hash))
 +#define ceph_file_layout_object_su(l) \
 +	((__s32)le32_to_cpu((l).fl_object_stripe_unit))
 +#define ceph_file_layout_pg_pool(l) \
 +	((__s32)le32_to_cpu((l).fl_pg_pool))
++=======
+ struct ceph_string;
+ /*
+  * ceph_file_layout - describe data layout for a file/inode
+  */
+ struct ceph_file_layout {
+ 	/* file -> object mapping */
+ 	u32 stripe_unit;   /* stripe unit, in bytes */
+ 	u32 stripe_count;  /* over this many objects */
+ 	u32 object_size;   /* until objects are this big */
+ 	s64 pool_id;        /* rados pool id */
+ 	struct ceph_string __rcu *pool_ns; /* rados pool namespace */
+ };
++>>>>>>> 30c156d9951e (libceph: rados pool namespace support)
 +
 +static inline unsigned ceph_file_layout_stripe_width(struct ceph_file_layout *l)
 +{
 +	return le32_to_cpu(l->fl_stripe_unit) *
 +		le32_to_cpu(l->fl_stripe_count);
 +}
  
 -extern int ceph_file_layout_is_valid(const struct ceph_file_layout *layout);
 -extern void ceph_file_layout_from_legacy(struct ceph_file_layout *fl,
 -				struct ceph_file_layout_legacy *legacy);
 -extern void ceph_file_layout_to_legacy(struct ceph_file_layout *fl,
 -				struct ceph_file_layout_legacy *legacy);
 +/* "period" == bytes before i start on a new set of objects */
 +static inline unsigned ceph_file_layout_period(struct ceph_file_layout *l)
 +{
 +	return le32_to_cpu(l->fl_object_size) *
 +		le32_to_cpu(l->fl_stripe_count);
 +}
  
  #define CEPH_MIN_STRIPE_UNIT 65536
  
diff --cc net/ceph/debugfs.c
index 8507949a54ea,c62b2b029a6e..000000000000
--- a/net/ceph/debugfs.c
+++ b/net/ceph/debugfs.c
@@@ -189,8 -156,16 +189,21 @@@ static void dump_target(struct seq_fil
  	seq_printf(s, "]/%d\t[", t->up.primary);
  	for (i = 0; i < t->acting.size; i++)
  		seq_printf(s, "%s%d", (!i ? "" : ","), t->acting.osds[i]);
++<<<<<<< HEAD
 +	seq_printf(s, "]/%d\te%u\t%s\t0x%x", t->acting.primary, t->epoch,
 +		   t->target_oid.name, t->flags);
++=======
+ 	seq_printf(s, "]/%d\t", t->acting.primary);
+ 	if (t->target_oloc.pool_ns) {
+ 		seq_printf(s, "%*pE/%*pE\t0x%x",
+ 			(int)t->target_oloc.pool_ns->len,
+ 			t->target_oloc.pool_ns->str,
+ 			t->target_oid.name_len, t->target_oid.name, t->flags);
+ 	} else {
+ 		seq_printf(s, "%*pE\t0x%x", t->target_oid.name_len,
+ 			t->target_oid.name, t->flags);
+ 	}
++>>>>>>> 30c156d9951e (libceph: rados pool namespace support)
  	if (t->paused)
  		seq_puts(s, "\tP");
  }
diff --cc net/ceph/osd_client.c
index 105036300de0,b68cc15e9cdc..000000000000
--- a/net/ceph/osd_client.c
+++ b/net/ceph/osd_client.c
@@@ -541,9 -535,9 +543,15 @@@ struct ceph_osd_request *ceph_osdc_allo
  }
  EXPORT_SYMBOL(ceph_osdc_alloc_request);
  
++<<<<<<< HEAD
 +static int ceph_oloc_encoding_size(const struct ceph_object_locator *oloc)
 +{
 +	return 8 + 4 + 4 + 4;
++=======
+ static int ceph_oloc_encoding_size(struct ceph_object_locator *oloc)
+ {
+ 	return 8 + 4 + 4 + 4 + (oloc->pool_ns ? oloc->pool_ns->len : 0);
++>>>>>>> 30c156d9951e (libceph: rados pool namespace support)
  }
  
  int ceph_osdc_alloc_messages(struct ceph_osd_request *req, gfp_t gfp)
@@@ -553,17 -547,14 +561,26 @@@
  	int msg_size;
  
  	WARN_ON(ceph_oid_empty(&req->r_base_oid));
+ 	WARN_ON(ceph_oloc_empty(&req->r_base_oloc));
  
  	/* create request message */
++<<<<<<< HEAD
 +	msg_size = CEPH_ENCODING_START_BLK_LEN +
 +			CEPH_PGID_ENCODING_LEN + 1; /* spgid */
 +	msg_size += 4 + 4 + 4; /* hash, osdmap_epoch, flags */
 +	msg_size += CEPH_ENCODING_START_BLK_LEN +
 +			sizeof(struct ceph_osd_reqid); /* reqid */
 +	msg_size += sizeof(struct ceph_blkin_trace_info); /* trace */
 +	msg_size += 4 + sizeof(struct ceph_timespec); /* client_inc, mtime */
 +	msg_size += CEPH_ENCODING_START_BLK_LEN +
 +			ceph_oloc_encoding_size(&req->r_base_oloc); /* oloc */
++=======
+ 	msg_size = 4 + 4 + 4; /* client_inc, osdmap_epoch, flags */
+ 	msg_size += 4 + 4 + 4 + 8; /* mtime, reassert_version */
+ 	msg_size += CEPH_ENCODING_START_BLK_LEN +
+ 			ceph_oloc_encoding_size(&req->r_base_oloc); /* oloc */
+ 	msg_size += 1 + 8 + 4 + 4; /* pgid */
++>>>>>>> 30c156d9951e (libceph: rados pool namespace support)
  	msg_size += 4 + req->r_base_oid.name_len; /* oid */
  	msg_size += 2 + req->r_num_ops * sizeof(struct ceph_osd_op);
  	msg_size += 8; /* snapid */
@@@ -963,9 -956,9 +980,14 @@@ struct ceph_osd_request *ceph_osdc_new_
  				       truncate_size, truncate_seq);
  	}
  
 +	req->r_abort_on_full = true;
  	req->r_flags = flags;
++<<<<<<< HEAD
 +	req->r_base_oloc.pool = ceph_file_layout_pg_pool(*layout);
++=======
+ 	req->r_base_oloc.pool = layout->pool_id;
+ 	req->r_base_oloc.pool_ns = ceph_try_get_string(layout->pool_ns);
++>>>>>>> 30c156d9951e (libceph: rados pool namespace support)
  	ceph_oid_printf(&req->r_base_oid, "%llx.%08llx", vino.ino, objnum);
  
  	req->r_snapid = vino.snap;
@@@ -1864,27 -1489,37 +1886,52 @@@ static void encode_request_partial(stru
  
  	setup_request_data(req, msg);
  
 -	ceph_encode_32(&p, 1); /* client_inc, always 1 */
 +	encode_spgid(&p, &req->r_t.spgid); /* actual spg */
 +	ceph_encode_32(&p, req->r_t.pgid.seed); /* raw hash */
  	ceph_encode_32(&p, req->r_osdc->osdmap->epoch);
  	ceph_encode_32(&p, req->r_flags);
 +
 +	/* reqid */
 +	ceph_start_encoding(&p, 2, 2, sizeof(struct ceph_osd_reqid));
 +	memset(p, 0, sizeof(struct ceph_osd_reqid));
 +	p += sizeof(struct ceph_osd_reqid);
 +
 +	/* trace */
 +	memset(p, 0, sizeof(struct ceph_blkin_trace_info));
 +	p += sizeof(struct ceph_blkin_trace_info);
 +
 +	ceph_encode_32(&p, 0); /* client_inc, always 0 */
  	ceph_encode_timespec(p, &req->r_mtime);
  	p += sizeof(struct ceph_timespec);
 -	/* aka reassert_version */
 -	memcpy(p, &req->r_replay_version, sizeof(req->r_replay_version));
 -	p += sizeof(req->r_replay_version);
  
++<<<<<<< HEAD
 +	encode_oloc(&p, end, &req->r_t.target_oloc);
 +	ceph_encode_string(&p, end, req->r_t.target_oid.name,
 +			   req->r_t.target_oid.name_len);
++=======
+ 	/* oloc */
+ 	ceph_start_encoding(&p, 5, 4,
+ 			    ceph_oloc_encoding_size(&req->r_t.target_oloc));
+ 	ceph_encode_64(&p, req->r_t.target_oloc.pool);
+ 	ceph_encode_32(&p, -1); /* preferred */
+ 	ceph_encode_32(&p, 0); /* key len */
+ 	if (req->r_t.target_oloc.pool_ns)
+ 		ceph_encode_string(&p, end, req->r_t.target_oloc.pool_ns->str,
+ 				   req->r_t.target_oloc.pool_ns->len);
+ 	else
+ 		ceph_encode_32(&p, 0);
+ 
+ 	/* pgid */
+ 	ceph_encode_8(&p, 1);
+ 	ceph_encode_64(&p, req->r_t.pgid.pool);
+ 	ceph_encode_32(&p, req->r_t.pgid.seed);
+ 	ceph_encode_32(&p, -1); /* preferred */
+ 
+ 	/* oid */
+ 	ceph_encode_32(&p, req->r_t.target_oid.name_len);
+ 	memcpy(p, req->r_t.target_oid.name, req->r_t.target_oid.name_len);
+ 	p += req->r_t.target_oid.name_len;
++>>>>>>> 30c156d9951e (libceph: rados pool namespace support)
  
  	/* ops, can imply data */
  	ceph_encode_16(&p, req->r_num_ops);
* Unmerged path drivers/block/rbd.c
diff --git a/fs/ceph/inode.c b/fs/ceph/inode.c
index e8aea0cbac25..12036c1ad2d5 100644
--- a/fs/ceph/inode.c
+++ b/fs/ceph/inode.c
@@ -441,6 +441,7 @@ struct inode *ceph_alloc_inode(struct super_block *sb)
 	ci->i_symlink = NULL;
 
 	memset(&ci->i_dir_layout, 0, sizeof(ci->i_dir_layout));
+	RCU_INIT_POINTER(ci->i_layout.pool_ns, NULL);
 	ci->i_pool_ns_len = 0;
 
 	ci->i_fragtree = RB_ROOT;
@@ -561,6 +562,8 @@ void ceph_destroy_inode(struct inode *inode)
 	if (ci->i_xattrs.prealloc_blob)
 		ceph_buffer_put(ci->i_xattrs.prealloc_blob);
 
+	ceph_put_string(ci->i_layout.pool_ns);
+
 	call_rcu(&inode->i_rcu, ceph_i_callback);
 }
 
* Unmerged path include/linux/ceph/ceph_fs.h
diff --git a/include/linux/ceph/osdmap.h b/include/linux/ceph/osdmap.h
index c104d9166429..af3444a5bfdd 100644
--- a/include/linux/ceph/osdmap.h
+++ b/include/linux/ceph/osdmap.h
@@ -71,11 +71,13 @@ static inline bool ceph_can_shift_osds(struct ceph_pg_pool_info *pool)
 
 struct ceph_object_locator {
 	s64 pool;
+	struct ceph_string *pool_ns;
 };
 
 static inline void ceph_oloc_init(struct ceph_object_locator *oloc)
 {
 	oloc->pool = -1;
+	oloc->pool_ns = NULL;
 }
 
 static inline bool ceph_oloc_empty(const struct ceph_object_locator *oloc)
@@ -83,11 +85,9 @@ static inline bool ceph_oloc_empty(const struct ceph_object_locator *oloc)
 	return oloc->pool == -1;
 }
 
-static inline void ceph_oloc_copy(struct ceph_object_locator *dest,
-				  const struct ceph_object_locator *src)
-{
-	dest->pool = src->pool;
-}
+void ceph_oloc_copy(struct ceph_object_locator *dest,
+		    const struct ceph_object_locator *src);
+void ceph_oloc_destroy(struct ceph_object_locator *oloc);
 
 /*
  * 51-char inline_name is long enough for all cephfs and all but one
* Unmerged path net/ceph/debugfs.c
* Unmerged path net/ceph/osd_client.c
diff --git a/net/ceph/osdmap.c b/net/ceph/osdmap.c
index f24224ba4cf0..eab3a325570c 100644
--- a/net/ceph/osdmap.c
+++ b/net/ceph/osdmap.c
@@ -1870,6 +1870,24 @@ bad:
 	return ERR_PTR(err);
 }
 
+void ceph_oloc_copy(struct ceph_object_locator *dest,
+		    const struct ceph_object_locator *src)
+{
+	WARN_ON(!ceph_oloc_empty(dest));
+	WARN_ON(dest->pool_ns); /* empty() only covers ->pool */
+
+	dest->pool = src->pool;
+	if (src->pool_ns)
+		dest->pool_ns = ceph_get_string(src->pool_ns);
+}
+EXPORT_SYMBOL(ceph_oloc_copy);
+
+void ceph_oloc_destroy(struct ceph_object_locator *oloc)
+{
+	ceph_put_string(oloc->pool_ns);
+}
+EXPORT_SYMBOL(ceph_oloc_destroy);
+
 void ceph_oid_copy(struct ceph_object_id *dest,
 		   const struct ceph_object_id *src)
 {
@@ -2203,12 +2221,34 @@ int __ceph_object_locator_to_pg(struct ceph_pg_pool_info *pi,
 {
 	WARN_ON(pi->id != oloc->pool);
 
-	raw_pgid->pool = oloc->pool;
-	raw_pgid->seed = ceph_str_hash(pi->object_hash, oid->name,
-				       oid->name_len);
-
-	dout("%s %s -> raw_pgid %llu.%x\n", __func__, oid->name,
-	     raw_pgid->pool, raw_pgid->seed);
+	if (!oloc->pool_ns) {
+		raw_pgid->pool = oloc->pool;
+		raw_pgid->seed = ceph_str_hash(pi->object_hash, oid->name,
+					     oid->name_len);
+		dout("%s %s -> raw_pgid %llu.%x\n", __func__, oid->name,
+		     raw_pgid->pool, raw_pgid->seed);
+	} else {
+		char stack_buf[256];
+		char *buf = stack_buf;
+		int nsl = oloc->pool_ns->len;
+		size_t total = nsl + 1 + oid->name_len;
+
+		if (total > sizeof(stack_buf)) {
+			buf = kmalloc(total, GFP_NOIO);
+			if (!buf)
+				return -ENOMEM;
+		}
+		memcpy(buf, oloc->pool_ns->str, nsl);
+		buf[nsl] = '\037';
+		memcpy(buf + nsl + 1, oid->name, oid->name_len);
+		raw_pgid->pool = oloc->pool;
+		raw_pgid->seed = ceph_str_hash(pi->object_hash, buf, total);
+		if (buf != stack_buf)
+			kfree(buf);
+		dout("%s %s ns %.*s -> raw_pgid %llu.%x\n", __func__,
+		     oid->name, nsl, oloc->pool_ns->str,
+		     raw_pgid->pool, raw_pgid->seed);
+	}
 	return 0;
 }
 
