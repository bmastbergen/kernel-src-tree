s390/sthyi: add cache to store hypervisor info

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [s390] sthyi: add cache to store hypervisor info (Hendrik Brueckner) [1519343]
Rebuild_FUZZ: 94.25%
commit-author QingFeng Hao <haoqf@linux.vnet.ibm.com>
commit 9fb6c9b3fea1b1d1c6f14178373e8f7235f3b681
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9fb6c9b3.failed

STHYI requires extensive locking in the higher hypervisors and is
very computational/memory expensive. Therefore we cache the retrieved
hypervisor info whose valid period is 1s with mutex to allow concurrent
access. rw semaphore can't benefit here due to cache line bounce.

	Signed-off-by: QingFeng Hao <haoqf@linux.vnet.ibm.com>
	Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 9fb6c9b3fea1b1d1c6f14178373e8f7235f3b681)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/kvm_host.h
#	arch/s390/kernel/sthyi.c
#	arch/s390/kvm/intercept.c
#	arch/s390/kvm/kvm-s390.c
diff --cc arch/s390/include/asm/kvm_host.h
index 0aaab12c8aec,fd006a272024..000000000000
--- a/arch/s390/include/asm/kvm_host.h
+++ b/arch/s390/include/asm/kvm_host.h
@@@ -273,12 -639,113 +273,35 @@@ struct kvm_vm_stat 
  struct kvm_arch_memory_slot {
  };
  
 -struct s390_map_info {
 -	struct list_head list;
 -	__u64 guest_addr;
 -	__u64 addr;
 -	struct page *page;
 -};
 -
 -struct s390_io_adapter {
 -	unsigned int id;
 -	int isc;
 -	bool maskable;
 -	bool masked;
 -	bool swap;
 -	bool suppressible;
 -	struct rw_semaphore maps_lock;
 -	struct list_head maps;
 -	atomic_t nr_maps;
 -};
 -
 -#define MAX_S390_IO_ADAPTERS ((MAX_ISC + 1) * 8)
 -#define MAX_S390_ADAPTER_MAPS 256
 -
 -/* maximum size of facilities and facility mask is 2k bytes */
 -#define S390_ARCH_FAC_LIST_SIZE_BYTE (1<<11)
 -#define S390_ARCH_FAC_LIST_SIZE_U64 \
 -	(S390_ARCH_FAC_LIST_SIZE_BYTE / sizeof(u64))
 -#define S390_ARCH_FAC_MASK_SIZE_BYTE S390_ARCH_FAC_LIST_SIZE_BYTE
 -#define S390_ARCH_FAC_MASK_SIZE_U64 \
 -	(S390_ARCH_FAC_MASK_SIZE_BYTE / sizeof(u64))
 -
 -struct kvm_s390_cpu_model {
 -	/* facility mask supported by kvm & hosting machine */
 -	__u64 fac_mask[S390_ARCH_FAC_LIST_SIZE_U64];
 -	/* facility list requested by guest (in dma page) */
 -	__u64 *fac_list;
 -	u64 cpuid;
 -	unsigned short ibc;
 -};
 -
 -struct kvm_s390_crypto {
 -	struct kvm_s390_crypto_cb *crycb;
 -	__u32 crycbd;
 -	__u8 aes_kw;
 -	__u8 dea_kw;
 -};
 -
 -struct kvm_s390_crypto_cb {
 -	__u8    reserved00[72];                 /* 0x0000 */
 -	__u8    dea_wrapping_key_mask[24];      /* 0x0048 */
 -	__u8    aes_wrapping_key_mask[32];      /* 0x0060 */
 -	__u8    reserved80[128];                /* 0x0080 */
 -};
 -
 -/*
 - * sie_page2 has to be allocated as DMA because fac_list and crycb need
 - * 31bit addresses in the sie control block.
 - */
 -struct sie_page2 {
 -	__u64 fac_list[S390_ARCH_FAC_LIST_SIZE_U64];	/* 0x0000 */
 -	struct kvm_s390_crypto_cb crycb;		/* 0x0800 */
 -	u8 reserved900[0x1000 - 0x900];			/* 0x0900 */
 -};
 -
 -struct kvm_s390_vsie {
 -	struct mutex mutex;
 -	struct radix_tree_root addr_to_page;
 -	int page_count;
 -	int next;
 -	struct page *pages[KVM_MAX_VCPUS];
 -};
 -
 -struct kvm_s390_migration_state {
 -	unsigned long bitmap_size;	/* in bits (number of guest pages) */
 -	atomic64_t dirty_pages;		/* number of dirty pages */
 -	unsigned long *pgste_bitmap;
 -};
 -
  struct kvm_arch{
 -	void *sca;
 -	int use_esca;
 -	rwlock_t sca_lock;
 +	struct sca_block *sca;
  	debug_info_t *dbf;
  	struct kvm_s390_float_interrupt float_int;
 -	struct kvm_device *flic;
  	struct gmap *gmap;
 -	unsigned long mem_limit;
  	int css_support;
++<<<<<<< HEAD
++=======
+ 	int use_irqchip;
+ 	int use_cmma;
+ 	int user_cpu_state_ctrl;
+ 	int user_sigp;
+ 	int user_stsi;
+ 	int user_instr0;
+ 	struct s390_io_adapter *adapters[MAX_S390_IO_ADAPTERS];
+ 	wait_queue_head_t ipte_wq;
+ 	int ipte_lock_count;
+ 	struct mutex ipte_mutex;
+ 	spinlock_t start_stop_lock;
+ 	struct sie_page2 *sie_page2;
+ 	struct kvm_s390_cpu_model model;
+ 	struct kvm_s390_crypto crypto;
+ 	struct kvm_s390_vsie vsie;
+ 	u8 epdx;
+ 	u64 epoch;
+ 	struct kvm_s390_migration_state *migration_state;
+ 	/* subset of available cpu features enabled by user space */
+ 	DECLARE_BITMAP(cpu_feat, KVM_S390_VM_CPU_FEAT_NR_BITS);
++>>>>>>> 9fb6c9b3fea1 (s390/sthyi: add cache to store hypervisor info)
  };
  
  #define KVM_HVA_ERR_BAD		(-1UL)
diff --cc arch/s390/kvm/intercept.c
index 5ee56e5acc23,8fe034beb623..000000000000
--- a/arch/s390/kvm/intercept.c
+++ b/arch/s390/kvm/intercept.c
@@@ -115,40 -144,313 +115,230 @@@ static int handle_instruction(struct kv
  	return -EOPNOTSUPP;
  }
  
 -static int inject_prog_on_prog_intercept(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_pgm_info pgm_info = {
 -		.code = vcpu->arch.sie_block->iprcc,
 -		/* the PSW has already been rewound */
 -		.flags = KVM_S390_PGM_FLAGS_NO_REWIND,
 -	};
 -
 -	switch (vcpu->arch.sie_block->iprcc & ~PGM_PER) {
 -	case PGM_AFX_TRANSLATION:
 -	case PGM_ASX_TRANSLATION:
 -	case PGM_EX_TRANSLATION:
 -	case PGM_LFX_TRANSLATION:
 -	case PGM_LSTE_SEQUENCE:
 -	case PGM_LSX_TRANSLATION:
 -	case PGM_LX_TRANSLATION:
 -	case PGM_PRIMARY_AUTHORITY:
 -	case PGM_SECONDARY_AUTHORITY:
 -	case PGM_SPACE_SWITCH:
 -		pgm_info.trans_exc_code = vcpu->arch.sie_block->tecmc;
 -		break;
 -	case PGM_ALEN_TRANSLATION:
 -	case PGM_ALE_SEQUENCE:
 -	case PGM_ASTE_INSTANCE:
 -	case PGM_ASTE_SEQUENCE:
 -	case PGM_ASTE_VALIDITY:
 -	case PGM_EXTENDED_AUTHORITY:
 -		pgm_info.exc_access_id = vcpu->arch.sie_block->eai;
 -		break;
 -	case PGM_ASCE_TYPE:
 -	case PGM_PAGE_TRANSLATION:
 -	case PGM_REGION_FIRST_TRANS:
 -	case PGM_REGION_SECOND_TRANS:
 -	case PGM_REGION_THIRD_TRANS:
 -	case PGM_SEGMENT_TRANSLATION:
 -		pgm_info.trans_exc_code = vcpu->arch.sie_block->tecmc;
 -		pgm_info.exc_access_id  = vcpu->arch.sie_block->eai;
 -		pgm_info.op_access_id  = vcpu->arch.sie_block->oai;
 -		break;
 -	case PGM_MONITOR:
 -		pgm_info.mon_class_nr = vcpu->arch.sie_block->mcn;
 -		pgm_info.mon_code = vcpu->arch.sie_block->tecmc;
 -		break;
 -	case PGM_VECTOR_PROCESSING:
 -	case PGM_DATA:
 -		pgm_info.data_exc_code = vcpu->arch.sie_block->dxc;
 -		break;
 -	case PGM_PROTECTION:
 -		pgm_info.trans_exc_code = vcpu->arch.sie_block->tecmc;
 -		pgm_info.exc_access_id  = vcpu->arch.sie_block->eai;
 -		break;
 -	default:
 -		break;
 -	}
 -
 -	if (vcpu->arch.sie_block->iprcc & PGM_PER) {
 -		pgm_info.per_code = vcpu->arch.sie_block->perc;
 -		pgm_info.per_atmid = vcpu->arch.sie_block->peratmid;
 -		pgm_info.per_address = vcpu->arch.sie_block->peraddr;
 -		pgm_info.per_access_id = vcpu->arch.sie_block->peraid;
 -	}
 -	return kvm_s390_inject_prog_irq(vcpu, &pgm_info);
 -}
 -
 -/*
 - * restore ITDB to program-interruption TDB in guest lowcore
 - * and set TX abort indication if required
 -*/
 -static int handle_itdb(struct kvm_vcpu *vcpu)
 +static int handle_prog(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_s390_itdb *itdb;
 -	int rc;
 -
 -	if (!IS_TE_ENABLED(vcpu) || !IS_ITDB_VALID(vcpu))
 -		return 0;
 -	if (current->thread.per_flags & PER_FLAG_NO_TE)
 -		return 0;
 -	itdb = (struct kvm_s390_itdb *)vcpu->arch.sie_block->itdba;
 -	rc = write_guest_lc(vcpu, __LC_PGM_TDB, itdb, sizeof(*itdb));
 -	if (rc)
 -		return rc;
 -	memset(itdb, 0, sizeof(*itdb));
 -
 -	return 0;
 +	vcpu->stat.exit_program_interruption++;
 +	trace_kvm_s390_intercept_prog(vcpu, vcpu->arch.sie_block->iprcc);
 +	return kvm_s390_inject_program_int(vcpu, vcpu->arch.sie_block->iprcc);
  }
  
 -#define per_event(vcpu) (vcpu->arch.sie_block->iprcc & PGM_PER)
 -
 -static int handle_prog(struct kvm_vcpu *vcpu)
 +static int handle_instruction_and_prog(struct kvm_vcpu *vcpu)
  {
 -	psw_t psw;
 -	int rc;
 -
 -	vcpu->stat.exit_program_interruption++;
 +	int rc, rc2;
  
 -	if (guestdbg_enabled(vcpu) && per_event(vcpu)) {
 -		rc = kvm_s390_handle_per_event(vcpu);
 -		if (rc)
 -			return rc;
 -		/* the interrupt might have been filtered out completely */
 -		if (vcpu->arch.sie_block->iprcc == 0)
 -			return 0;
 -	}
 +	vcpu->stat.exit_instr_and_program++;
 +	rc = handle_instruction(vcpu);
 +	rc2 = handle_prog(vcpu);
  
 -	trace_kvm_s390_intercept_prog(vcpu, vcpu->arch.sie_block->iprcc);
 -	if (vcpu->arch.sie_block->iprcc == PGM_SPECIFICATION) {
 -		rc = read_guest_lc(vcpu, __LC_PGM_NEW_PSW, &psw, sizeof(psw_t));
 -		if (rc)
 -			return rc;
 -		/* Avoid endless loops of specification exceptions */
 -		if (!is_valid_psw(&psw))
 -			return -EOPNOTSUPP;
 -	}
 -	rc = handle_itdb(vcpu);
 +	if (rc == -EOPNOTSUPP)
 +		vcpu->arch.sie_block->icptcode = 0x04;
  	if (rc)
  		return rc;
 -
 -	return inject_prog_on_prog_intercept(vcpu);
 +	return rc2;
  }
  
++<<<<<<< HEAD
 +static const intercept_handler_t intercept_funcs[] = {
 +	[0x00 >> 2] = handle_noop,
 +	[0x04 >> 2] = handle_instruction,
 +	[0x08 >> 2] = handle_prog,
 +	[0x0C >> 2] = handle_instruction_and_prog,
 +	[0x10 >> 2] = handle_noop,
 +	[0x14 >> 2] = handle_noop,
 +	[0x18 >> 2] = handle_noop,
 +	[0x1C >> 2] = kvm_s390_handle_wait,
 +	[0x20 >> 2] = handle_validity,
 +	[0x28 >> 2] = handle_stop,
 +};
++=======
+ /**
+  * handle_external_interrupt - used for external interruption interceptions
+  *
+  * This interception only occurs if the CPUSTAT_EXT_INT bit was set, or if
+  * the new PSW does not have external interrupts disabled. In the first case,
+  * we've got to deliver the interrupt manually, and in the second case, we
+  * drop to userspace to handle the situation there.
+  */
+ static int handle_external_interrupt(struct kvm_vcpu *vcpu)
+ {
+ 	u16 eic = vcpu->arch.sie_block->eic;
+ 	struct kvm_s390_irq irq;
+ 	psw_t newpsw;
+ 	int rc;
+ 
+ 	vcpu->stat.exit_external_interrupt++;
+ 
+ 	rc = read_guest_lc(vcpu, __LC_EXT_NEW_PSW, &newpsw, sizeof(psw_t));
+ 	if (rc)
+ 		return rc;
+ 	/* We can not handle clock comparator or timer interrupt with bad PSW */
+ 	if ((eic == EXT_IRQ_CLK_COMP || eic == EXT_IRQ_CPU_TIMER) &&
+ 	    (newpsw.mask & PSW_MASK_EXT))
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (eic) {
+ 	case EXT_IRQ_CLK_COMP:
+ 		irq.type = KVM_S390_INT_CLOCK_COMP;
+ 		break;
+ 	case EXT_IRQ_CPU_TIMER:
+ 		irq.type = KVM_S390_INT_CPU_TIMER;
+ 		break;
+ 	case EXT_IRQ_EXTERNAL_CALL:
+ 		irq.type = KVM_S390_INT_EXTERNAL_CALL;
+ 		irq.u.extcall.code = vcpu->arch.sie_block->extcpuaddr;
+ 		rc = kvm_s390_inject_vcpu(vcpu, &irq);
+ 		/* ignore if another external call is already pending */
+ 		if (rc == -EBUSY)
+ 			return 0;
+ 		return rc;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	return kvm_s390_inject_vcpu(vcpu, &irq);
+ }
+ 
+ /**
+  * Handle MOVE PAGE partial execution interception.
+  *
+  * This interception can only happen for guests with DAT disabled and
+  * addresses that are currently not mapped in the host. Thus we try to
+  * set up the mappings for the corresponding user pages here (or throw
+  * addressing exceptions in case of illegal guest addresses).
+  */
+ static int handle_mvpg_pei(struct kvm_vcpu *vcpu)
+ {
+ 	unsigned long srcaddr, dstaddr;
+ 	int reg1, reg2, rc;
+ 
+ 	kvm_s390_get_regs_rre(vcpu, &reg1, &reg2);
+ 
+ 	/* Make sure that the source is paged-in */
+ 	rc = guest_translate_address(vcpu, vcpu->run->s.regs.gprs[reg2],
+ 				     reg2, &srcaddr, GACC_FETCH);
+ 	if (rc)
+ 		return kvm_s390_inject_prog_cond(vcpu, rc);
+ 	rc = kvm_arch_fault_in_page(vcpu, srcaddr, 0);
+ 	if (rc != 0)
+ 		return rc;
+ 
+ 	/* Make sure that the destination is paged-in */
+ 	rc = guest_translate_address(vcpu, vcpu->run->s.regs.gprs[reg1],
+ 				     reg1, &dstaddr, GACC_STORE);
+ 	if (rc)
+ 		return kvm_s390_inject_prog_cond(vcpu, rc);
+ 	rc = kvm_arch_fault_in_page(vcpu, dstaddr, 1);
+ 	if (rc != 0)
+ 		return rc;
+ 
+ 	kvm_s390_retry_instr(vcpu);
+ 
+ 	return 0;
+ }
+ 
+ static int handle_partial_execution(struct kvm_vcpu *vcpu)
+ {
+ 	vcpu->stat.exit_pei++;
+ 
+ 	if (vcpu->arch.sie_block->ipa == 0xb254)	/* MVPG */
+ 		return handle_mvpg_pei(vcpu);
+ 	if (vcpu->arch.sie_block->ipa >> 8 == 0xae)	/* SIGP */
+ 		return kvm_s390_handle_sigp_pei(vcpu);
+ 
+ 	return -EOPNOTSUPP;
+ }
+ 
+ /*
+  * Handle the sthyi instruction that provides the guest with system
+  * information, like current CPU resources available at each level of
+  * the machine.
+  */
+ int handle_sthyi(struct kvm_vcpu *vcpu)
+ {
+ 	int reg1, reg2, r = 0;
+ 	u64 code, addr, cc = 0, rc = 0;
+ 	struct sthyi_sctns *sctns = NULL;
+ 
+ 	if (!test_kvm_facility(vcpu->kvm, 74))
+ 		return kvm_s390_inject_program_int(vcpu, PGM_OPERATION);
+ 
+ 	kvm_s390_get_regs_rre(vcpu, &reg1, &reg2);
+ 	code = vcpu->run->s.regs.gprs[reg1];
+ 	addr = vcpu->run->s.regs.gprs[reg2];
+ 
+ 	vcpu->stat.instruction_sthyi++;
+ 	VCPU_EVENT(vcpu, 3, "STHYI: fc: %llu addr: 0x%016llx", code, addr);
+ 	trace_kvm_s390_handle_sthyi(vcpu, code, addr);
+ 
+ 	if (reg1 == reg2 || reg1 & 1 || reg2 & 1)
+ 		return kvm_s390_inject_program_int(vcpu, PGM_SPECIFICATION);
+ 
+ 	if (code & 0xffff) {
+ 		cc = 3;
+ 		rc = 4;
+ 		goto out;
+ 	}
+ 
+ 	if (addr & ~PAGE_MASK)
+ 		return kvm_s390_inject_program_int(vcpu, PGM_SPECIFICATION);
+ 
+ 	sctns = (void *)get_zeroed_page(GFP_KERNEL);
+ 	if (!sctns)
+ 		return -ENOMEM;
+ 
+ 	cc = sthyi_fill(sctns, &rc);
+ 
+ out:
+ 	if (!cc) {
+ 		r = write_guest(vcpu, addr, reg2, sctns, PAGE_SIZE);
+ 		if (r) {
+ 			free_page((unsigned long)sctns);
+ 			return kvm_s390_inject_prog_cond(vcpu, r);
+ 		}
+ 	}
+ 
+ 	free_page((unsigned long)sctns);
+ 	vcpu->run->s.regs.gprs[reg2 + 1] = rc;
+ 	kvm_s390_set_psw_cc(vcpu, cc);
+ 	return r;
+ }
+ 
+ static int handle_operexc(struct kvm_vcpu *vcpu)
+ {
+ 	psw_t oldpsw, newpsw;
+ 	int rc;
+ 
+ 	vcpu->stat.exit_operation_exception++;
+ 	trace_kvm_s390_handle_operexc(vcpu, vcpu->arch.sie_block->ipa,
+ 				      vcpu->arch.sie_block->ipb);
+ 
+ 	if (vcpu->arch.sie_block->ipa == 0xb256)
+ 		return handle_sthyi(vcpu);
+ 
+ 	if (vcpu->arch.sie_block->ipa == 0 && vcpu->kvm->arch.user_instr0)
+ 		return -EOPNOTSUPP;
+ 	rc = read_guest_lc(vcpu, __LC_PGM_NEW_PSW, &newpsw, sizeof(psw_t));
+ 	if (rc)
+ 		return rc;
+ 	/*
+ 	 * Avoid endless loops of operation exceptions, if the pgm new
+ 	 * PSW will cause a new operation exception.
+ 	 * The heuristic checks if the pgm new psw is within 6 bytes before
+ 	 * the faulting psw address (with same DAT, AS settings) and the
+ 	 * new psw is not a wait psw and the fault was not triggered by
+ 	 * problem state.
+ 	 */
+ 	oldpsw = vcpu->arch.sie_block->gpsw;
+ 	if (oldpsw.addr - newpsw.addr <= 6 &&
+ 	    !(newpsw.mask & PSW_MASK_WAIT) &&
+ 	    !(oldpsw.mask & PSW_MASK_PSTATE) &&
+ 	    (newpsw.mask & PSW_MASK_ASC) == (oldpsw.mask & PSW_MASK_ASC) &&
+ 	    (newpsw.mask & PSW_MASK_DAT) == (oldpsw.mask & PSW_MASK_DAT))
+ 		return -EOPNOTSUPP;
+ 
+ 	return kvm_s390_inject_program_int(vcpu, PGM_OPERATION);
+ }
++>>>>>>> 9fb6c9b3fea1 (s390/sthyi: add cache to store hypervisor info)
  
  int kvm_handle_sie_intercept(struct kvm_vcpu *vcpu)
  {
diff --cc arch/s390/kvm/kvm-s390.c
index 9b2d6973d202,de6a5b790da0..000000000000
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@@ -229,77 -700,183 +229,85 @@@ long kvm_arch_vm_ioctl(struct file *fil
  		break;
  	}
  	default:
 -		ret = -ENXIO;
 -		break;
 +		r = -ENOTTY;
  	}
 -	return ret;
 -}
  
 -static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu);
 +	return r;
 +}
  
 -static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 +int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
  {
 -	struct kvm_vcpu *vcpu;
 -	int i;
 -
 -	if (!test_kvm_facility(kvm, 76))
 -		return -EINVAL;
 -
 -	mutex_lock(&kvm->lock);
 -	switch (attr->attr) {
 -	case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
 -		get_random_bytes(
 -			kvm->arch.crypto.crycb->aes_wrapping_key_mask,
 -			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 -		kvm->arch.crypto.aes_kw = 1;
 -		VM_EVENT(kvm, 3, "%s", "ENABLE: AES keywrapping support");
 -		break;
 -	case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
 -		get_random_bytes(
 -			kvm->arch.crypto.crycb->dea_wrapping_key_mask,
 -			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 -		kvm->arch.crypto.dea_kw = 1;
 -		VM_EVENT(kvm, 3, "%s", "ENABLE: DEA keywrapping support");
 -		break;
 -	case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
 -		kvm->arch.crypto.aes_kw = 0;
 -		memset(kvm->arch.crypto.crycb->aes_wrapping_key_mask, 0,
 -			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 -		VM_EVENT(kvm, 3, "%s", "DISABLE: AES keywrapping support");
 -		break;
 -	case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
 -		kvm->arch.crypto.dea_kw = 0;
 -		memset(kvm->arch.crypto.crycb->dea_wrapping_key_mask, 0,
 -			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 -		VM_EVENT(kvm, 3, "%s", "DISABLE: DEA keywrapping support");
 -		break;
 -	default:
 -		mutex_unlock(&kvm->lock);
 -		return -ENXIO;
 -	}
 +	int rc;
 +	char debug_name[16];
  
 -	kvm_for_each_vcpu(i, vcpu, kvm) {
 -		kvm_s390_vcpu_crypto_setup(vcpu);
 -		exit_sie(vcpu);
 -	}
 -	mutex_unlock(&kvm->lock);
 -	return 0;
 -}
 +	rc = -EINVAL;
 +#ifdef CONFIG_KVM_S390_UCONTROL
 +	if (type & ~KVM_VM_S390_UCONTROL)
 +		goto out_err;
 +	if ((type & KVM_VM_S390_UCONTROL) && (!capable(CAP_SYS_ADMIN)))
 +		goto out_err;
 +#else
 +	if (type)
 +		goto out_err;
 +#endif
  
 -static void kvm_s390_sync_request_broadcast(struct kvm *kvm, int req)
 -{
 -	int cx;
 -	struct kvm_vcpu *vcpu;
 +	rc = s390_enable_sie();
 +	if (rc)
 +		goto out_err;
  
 -	kvm_for_each_vcpu(cx, vcpu, kvm)
 -		kvm_s390_sync_request(req, vcpu);
 -}
 +	rc = -ENOMEM;
  
 -/*
 - * Must be called with kvm->srcu held to avoid races on memslots, and with
 - * kvm->lock to avoid races with ourselves and kvm_s390_vm_stop_migration.
 - */
 -static int kvm_s390_vm_start_migration(struct kvm *kvm)
 -{
 -	struct kvm_s390_migration_state *mgs;
 -	struct kvm_memory_slot *ms;
 -	/* should be the only one */
 -	struct kvm_memslots *slots;
 -	unsigned long ram_pages;
 -	int slotnr;
++<<<<<<< HEAD
 +	kvm->arch.sca = (struct sca_block *) get_zeroed_page(GFP_KERNEL);
++=======
++	kvm->arch.use_esca = 0; /* start with basic SCA */
++	if (!sclp.has_64bscao)
++		alloc_flags |= GFP_DMA;
++	rwlock_init(&kvm->arch.sca_lock);
++	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(alloc_flags);
++>>>>>>> 9fb6c9b3fea1 (s390/sthyi: add cache to store hypervisor info)
 +	if (!kvm->arch.sca)
 +		goto out_err;
  
 -	/* migration mode already enabled */
 -	if (kvm->arch.migration_state)
 -		return 0;
 +	sprintf(debug_name, "kvm-%u", current->pid);
  
 -	slots = kvm_memslots(kvm);
 -	if (!slots || !slots->used_slots)
 -		return -EINVAL;
 +	kvm->arch.dbf = debug_register(debug_name, 8, 2, 8 * sizeof(long));
 +	if (!kvm->arch.dbf)
 +		goto out_nodbf;
  
 -	mgs = kzalloc(sizeof(*mgs), GFP_KERNEL);
 -	if (!mgs)
 -		return -ENOMEM;
 -	kvm->arch.migration_state = mgs;
 -
 -	if (kvm->arch.use_cmma) {
 -		/*
 -		 * Get the last slot. They should be sorted by base_gfn, so the
 -		 * last slot is also the one at the end of the address space.
 -		 * We have verified above that at least one slot is present.
 -		 */
 -		ms = slots->memslots + slots->used_slots - 1;
 -		/* round up so we only use full longs */
 -		ram_pages = roundup(ms->base_gfn + ms->npages, BITS_PER_LONG);
 -		/* allocate enough bytes to store all the bits */
 -		mgs->pgste_bitmap = vmalloc(ram_pages / 8);
 -		if (!mgs->pgste_bitmap) {
 -			kfree(mgs);
 -			kvm->arch.migration_state = NULL;
 -			return -ENOMEM;
 -		}
 +	spin_lock_init(&kvm->arch.float_int.lock);
 +	INIT_LIST_HEAD(&kvm->arch.float_int.list);
  
 -		mgs->bitmap_size = ram_pages;
 -		atomic64_set(&mgs->dirty_pages, ram_pages);
 -		/* mark all the pages in active slots as dirty */
 -		for (slotnr = 0; slotnr < slots->used_slots; slotnr++) {
 -			ms = slots->memslots + slotnr;
 -			bitmap_set(mgs->pgste_bitmap, ms->base_gfn, ms->npages);
 -		}
 +	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 +	VM_EVENT(kvm, 3, "%s", "vm created");
  
 -		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_START_MIGRATION);
 +	if (type & KVM_VM_S390_UCONTROL) {
 +		kvm->arch.gmap = NULL;
 +	} else {
 +		kvm->arch.gmap = gmap_alloc(current->mm);
 +		if (!kvm->arch.gmap)
 +			goto out_nogmap;
 +		kvm->arch.gmap->private = kvm;
  	}
 -	return 0;
 -}
 -
 -/*
 - * Must be called with kvm->lock to avoid races with ourselves and
 - * kvm_s390_vm_start_migration.
 - */
 -static int kvm_s390_vm_stop_migration(struct kvm *kvm)
 -{
 -	struct kvm_s390_migration_state *mgs;
  
 -	/* migration mode already disabled */
 -	if (!kvm->arch.migration_state)
 -		return 0;
 -	mgs = kvm->arch.migration_state;
 -	kvm->arch.migration_state = NULL;
 +	kvm->arch.css_support = 0;
  
 -	if (kvm->arch.use_cmma) {
 -		kvm_s390_sync_request_broadcast(kvm, KVM_REQ_STOP_MIGRATION);
 -		vfree(mgs->pgste_bitmap);
 -	}
 -	kfree(mgs);
  	return 0;
 +out_nogmap:
 +	debug_unregister(kvm->arch.dbf);
 +out_nodbf:
 +	free_page((unsigned long)(kvm->arch.sca));
 +out_err:
 +	return rc;
  }
  
 -static int kvm_s390_vm_set_migration(struct kvm *kvm,
 -				     struct kvm_device_attr *attr)
 +bool kvm_arch_has_vcpu_debugfs(void)
  {
 -	int idx, res = -ENXIO;
 -
 -	mutex_lock(&kvm->lock);
 -	switch (attr->attr) {
 -	case KVM_S390_VM_MIGRATION_START:
 -		idx = srcu_read_lock(&kvm->srcu);
 -		res = kvm_s390_vm_start_migration(kvm);
 -		srcu_read_unlock(&kvm->srcu, idx);
 -		break;
 -	case KVM_S390_VM_MIGRATION_STOP:
 -		res = kvm_s390_vm_stop_migration(kvm);
 -		break;
 -	default:
 -		break;
 -	}
 -	mutex_unlock(&kvm->lock);
 -
 -	return res;
 +	return false;
  }
  
 -static int kvm_s390_vm_get_migration(struct kvm *kvm,
 -				     struct kvm_device_attr *attr)
 +int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
  {
 -	u64 mig = (kvm->arch.migration_state != NULL);
 -
 -	if (attr->attr != KVM_S390_VM_MIGRATION_STATUS)
 -		return -ENXIO;
 -
 -	if (copy_to_user((void __user *)attr->addr, &mig, sizeof(mig)))
 -		return -EFAULT;
  	return 0;
  }
  
* Unmerged path arch/s390/kernel/sthyi.c
* Unmerged path arch/s390/include/asm/kvm_host.h
* Unmerged path arch/s390/kernel/sthyi.c
* Unmerged path arch/s390/kvm/intercept.c
* Unmerged path arch/s390/kvm/kvm-s390.c
