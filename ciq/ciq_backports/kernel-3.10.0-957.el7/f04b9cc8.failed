nvme-rdma: Fix error status return in tagset allocation failure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Sagi Grimberg <sagi@grimberg.me>
commit f04b9cc87b5fc466b1b7231ba7b078e885956c5b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/f04b9cc8.failed

We should make sure to escelate allocation failures to prevent a
use-after-free in nvmf_create_ctrl.

Fixes: b28a308ee777 ("nvme-rdma: move tagset allocation to a dedicated routine")
	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit f04b9cc87b5fc466b1b7231ba7b078e885956c5b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/rdma.c
diff --cc drivers/nvme/host/rdma.c
index a9e45e7b65a3,87bac27ec64b..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -685,6 -686,209 +685,212 @@@ static void nvme_rdma_destroy_admin_que
  	nvme_rdma_dev_put(ctrl->device);
  }
  
++<<<<<<< HEAD
++=======
+ static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
+ 		bool admin)
+ {
+ 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ 	struct blk_mq_tag_set *set;
+ 	int ret;
+ 
+ 	if (admin) {
+ 		set = &ctrl->admin_tag_set;
+ 		memset(set, 0, sizeof(*set));
+ 		set->ops = &nvme_rdma_admin_mq_ops;
+ 		set->queue_depth = NVME_RDMA_AQ_BLKMQ_DEPTH;
+ 		set->reserved_tags = 2; /* connect + keep-alive */
+ 		set->numa_node = NUMA_NO_NODE;
+ 		set->cmd_size = sizeof(struct nvme_rdma_request) +
+ 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+ 		set->driver_data = ctrl;
+ 		set->nr_hw_queues = 1;
+ 		set->timeout = ADMIN_TIMEOUT;
+ 	} else {
+ 		set = &ctrl->tag_set;
+ 		memset(set, 0, sizeof(*set));
+ 		set->ops = &nvme_rdma_mq_ops;
+ 		set->queue_depth = nctrl->opts->queue_size;
+ 		set->reserved_tags = 1; /* fabric connect */
+ 		set->numa_node = NUMA_NO_NODE;
+ 		set->flags = BLK_MQ_F_SHOULD_MERGE;
+ 		set->cmd_size = sizeof(struct nvme_rdma_request) +
+ 			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+ 		set->driver_data = ctrl;
+ 		set->nr_hw_queues = nctrl->queue_count - 1;
+ 		set->timeout = NVME_IO_TIMEOUT;
+ 	}
+ 
+ 	ret = blk_mq_alloc_tag_set(set);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/*
+ 	 * We need a reference on the device as long as the tag_set is alive,
+ 	 * as the MRs in the request structures need a valid ib_device.
+ 	 */
+ 	ret = nvme_rdma_dev_get(ctrl->device);
+ 	if (!ret) {
+ 		ret = -EINVAL;
+ 		goto out_free_tagset;
+ 	}
+ 
+ 	return set;
+ 
+ out_free_tagset:
+ 	blk_mq_free_tag_set(set);
+ out:
+ 	return ERR_PTR(ret);
+ }
+ 
+ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
+ 		bool remove)
+ {
+ 	nvme_rdma_stop_queue(&ctrl->queues[0]);
+ 	if (remove) {
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, true);
+ 	}
+ 	nvme_rdma_free_queue(&ctrl->queues[0]);
+ }
+ 
+ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
+ 		bool new)
+ {
+ 	int error;
+ 
+ 	error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ 	if (error)
+ 		return error;
+ 
+ 	ctrl->device = ctrl->queues[0].device;
+ 
+ 	ctrl->max_fr_pages = min_t(u32, NVME_RDMA_MAX_SEGMENTS,
+ 		ctrl->device->dev->attrs.max_fast_reg_page_list_len);
+ 
+ 	if (new) {
+ 		ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ 		if (IS_ERR(ctrl->ctrl.admin_tagset)) {
+ 			error = PTR_ERR(ctrl->ctrl.admin_tagset);
+ 			goto out_free_queue;
+ 		}
+ 
+ 		ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ 		if (IS_ERR(ctrl->ctrl.admin_q)) {
+ 			error = PTR_ERR(ctrl->ctrl.admin_q);
+ 			goto out_free_tagset;
+ 		}
+ 	} else {
+ 		error = blk_mq_reinit_tagset(&ctrl->admin_tag_set,
+ 					     nvme_rdma_reinit_request);
+ 		if (error)
+ 			goto out_free_queue;
+ 	}
+ 
+ 	error = nvme_rdma_start_queue(ctrl, 0);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	error = ctrl->ctrl.ops->reg_read64(&ctrl->ctrl, NVME_REG_CAP,
+ 			&ctrl->ctrl.cap);
+ 	if (error) {
+ 		dev_err(ctrl->ctrl.device,
+ 			"prop_get NVME_REG_CAP failed\n");
+ 		goto out_cleanup_queue;
+ 	}
+ 
+ 	ctrl->ctrl.sqsize =
+ 		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
+ 
+ 	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	ctrl->ctrl.max_hw_sectors =
+ 		(ctrl->max_fr_pages - 1) << (ilog2(SZ_4K) - 9);
+ 
+ 	error = nvme_init_identify(&ctrl->ctrl);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	error = nvme_rdma_alloc_qe(ctrl->queues[0].device->dev,
+ 			&ctrl->async_event_sqe, sizeof(struct nvme_command),
+ 			DMA_TO_DEVICE);
+ 	if (error)
+ 		goto out_cleanup_queue;
+ 
+ 	return 0;
+ 
+ out_cleanup_queue:
+ 	if (new)
+ 		blk_cleanup_queue(ctrl->ctrl.admin_q);
+ out_free_tagset:
+ 	if (new)
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, true);
+ out_free_queue:
+ 	nvme_rdma_free_queue(&ctrl->queues[0]);
+ 	return error;
+ }
+ 
+ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
+ 		bool remove)
+ {
+ 	nvme_rdma_stop_io_queues(ctrl);
+ 	if (remove) {
+ 		blk_cleanup_queue(ctrl->ctrl.connect_q);
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, false);
+ 	}
+ 	nvme_rdma_free_io_queues(ctrl);
+ }
+ 
+ static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
+ {
+ 	int ret;
+ 
+ 	ret = nvme_rdma_alloc_io_queues(ctrl);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (new) {
+ 		ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ 		if (IS_ERR(ctrl->ctrl.tagset)) {
+ 			ret = PTR_ERR(ctrl->ctrl.tagset);
+ 			goto out_free_io_queues;
+ 		}
+ 
+ 		ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ 		if (IS_ERR(ctrl->ctrl.connect_q)) {
+ 			ret = PTR_ERR(ctrl->ctrl.connect_q);
+ 			goto out_free_tag_set;
+ 		}
+ 	} else {
+ 		ret = blk_mq_reinit_tagset(&ctrl->tag_set,
+ 					   nvme_rdma_reinit_request);
+ 		if (ret)
+ 			goto out_free_io_queues;
+ 
+ 		blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ 			ctrl->ctrl.queue_count - 1);
+ 	}
+ 
+ 	ret = nvme_rdma_start_io_queues(ctrl);
+ 	if (ret)
+ 		goto out_cleanup_connect_q;
+ 
+ 	return 0;
+ 
+ out_cleanup_connect_q:
+ 	if (new)
+ 		blk_cleanup_queue(ctrl->ctrl.connect_q);
+ out_free_tag_set:
+ 	if (new)
+ 		nvme_rdma_free_tagset(&ctrl->ctrl, false);
+ out_free_io_queues:
+ 	nvme_rdma_free_io_queues(ctrl);
+ 	return ret;
+ }
+ 
++>>>>>>> f04b9cc87b5f (nvme-rdma: Fix error status return in tagset allocation failure)
  static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
  {
  	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
* Unmerged path drivers/nvme/host/rdma.c
