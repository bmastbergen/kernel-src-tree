prctl: Add force disable speculation

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 356e4bfff2c5489e016fdb925adbf12a1e3950ee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/356e4bff.failed

For certain use cases it is desired to enforce mitigations so they cannot
be undone afterwards. That's important for loader stubs which want to
prevent a child from disabling the mitigation again. Will also be used for
seccomp(). The extra state preserving of the prctl state for SSB is a
preparatory step for EBPF dymanic speculation control.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 356e4bfff2c5489e016fdb925adbf12a1e3950ee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/userspace-api/spec_ctrl.rst
#	arch/x86/kernel/cpu/bugs.c
#	fs/proc/array.c
#	include/linux/sched.h
#	include/uapi/linux/prctl.h
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,7e0f28160e5e..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -199,15 -406,248 +199,228 @@@ static void __init spectre_v2_select_mi
  }
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
+ 
+ static enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;
+ 
+ /* The kernel command line selection */
+ enum ssb_mitigation_cmd {
+ 	SPEC_STORE_BYPASS_CMD_NONE,
+ 	SPEC_STORE_BYPASS_CMD_AUTO,
+ 	SPEC_STORE_BYPASS_CMD_ON,
+ 	SPEC_STORE_BYPASS_CMD_PRCTL,
+ };
+ 
+ static const char *ssb_strings[] = {
+ 	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
+ 	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled",
+ 	[SPEC_STORE_BYPASS_PRCTL]	= "Mitigation: Speculative Store Bypass disabled via prctl"
+ };
+ 
+ static const struct {
+ 	const char *option;
+ 	enum ssb_mitigation_cmd cmd;
+ } ssb_mitigation_options[] = {
+ 	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO },  /* Platform decides */
+ 	{ "on",		SPEC_STORE_BYPASS_CMD_ON },    /* Disable Speculative Store Bypass */
+ 	{ "off",	SPEC_STORE_BYPASS_CMD_NONE },  /* Don't touch Speculative Store Bypass */
+ 	{ "prctl",	SPEC_STORE_BYPASS_CMD_PRCTL }, /* Disable Speculative Store Bypass via prctl */
+ };
+ 
+ static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
+ {
+ 	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
+ 		return SPEC_STORE_BYPASS_CMD_NONE;
+ 	} else {
+ 		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
+ 					  arg, sizeof(arg));
+ 		if (ret < 0)
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
+ 			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
+ 				continue;
+ 
+ 			cmd = ssb_mitigation_options[i].cmd;
+ 			break;
+ 		}
+ 
+ 		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
+ 			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 		}
+ 	}
+ 
+ 	return cmd;
+ }
+ 
+ static enum ssb_mitigation_cmd __init __ssb_select_mitigation(void)
+ {
+ 	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
+ 	enum ssb_mitigation_cmd cmd;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_RDS))
+ 		return mode;
+ 
+ 	cmd = ssb_parse_cmdline();
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
+ 	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
+ 	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
+ 		return mode;
+ 
+ 	switch (cmd) {
+ 	case SPEC_STORE_BYPASS_CMD_AUTO:
+ 		/* Choose prctl as the default mode */
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_ON:
+ 		mode = SPEC_STORE_BYPASS_DISABLE;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_PRCTL:
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_NONE:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * We have three CPU feature flags that are in play here:
+ 	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.
+ 	 *  - X86_FEATURE_RDS - CPU is able to turn off speculative store bypass
+ 	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation
+ 	 */
+ 	if (mode == SPEC_STORE_BYPASS_DISABLE) {
+ 		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
+ 		/*
+ 		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD uses
+ 		 * a completely different MSR and bit dependent on family.
+ 		 */
+ 		switch (boot_cpu_data.x86_vendor) {
+ 		case X86_VENDOR_INTEL:
+ 			x86_spec_ctrl_base |= SPEC_CTRL_RDS;
+ 			x86_spec_ctrl_mask &= ~SPEC_CTRL_RDS;
+ 			x86_spec_ctrl_set(SPEC_CTRL_RDS);
+ 			break;
+ 		case X86_VENDOR_AMD:
+ 			x86_amd_rds_enable();
+ 			break;
+ 		}
+ 	}
+ 
+ 	return mode;
+ }
+ 
+ static void ssb_select_mitigation()
+ {
+ 	ssb_mode = __ssb_select_mitigation();
+ 
+ 	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		pr_info("%s\n", ssb_strings[ssb_mode]);
+ }
+ 
+ #undef pr_fmt
+ 
+ static int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)
+ {
+ 	bool update;
+ 
+ 	if (ssb_mode != SPEC_STORE_BYPASS_PRCTL)
+ 		return -ENXIO;
+ 
+ 	switch (ctrl) {
+ 	case PR_SPEC_ENABLE:
+ 		/* If speculation is force disabled, enable is not allowed */
+ 		if (task_spec_ssb_force_disable(task))
+ 			return -EPERM;
+ 		task_clear_spec_ssb_disable(task);
+ 		update = test_and_clear_tsk_thread_flag(task, TIF_RDS);
+ 		break;
+ 	case PR_SPEC_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_RDS);
+ 		break;
+ 	case PR_SPEC_FORCE_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		task_set_spec_ssb_force_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_RDS);
+ 		break;
+ 	default:
+ 		return -ERANGE;
+ 	}
+ 
+ 	/*
+ 	 * If being set on non-current task, delay setting the CPU
+ 	 * mitigation until it is next scheduled.
+ 	 */
+ 	if (task == current && update)
+ 		speculative_store_bypass_update();
+ 
+ 	return 0;
+ }
+ 
+ static int ssb_prctl_get(struct task_struct *task)
+ {
+ 	switch (ssb_mode) {
+ 	case SPEC_STORE_BYPASS_DISABLE:
+ 		return PR_SPEC_DISABLE;
+ 	case SPEC_STORE_BYPASS_PRCTL:
+ 		if (task_spec_ssb_force_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;
+ 		if (task_spec_ssb_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_DISABLE;
+ 		return PR_SPEC_PRCTL | PR_SPEC_ENABLE;
+ 	default:
+ 		if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 			return PR_SPEC_ENABLE;
+ 		return PR_SPEC_NOT_AFFECTED;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
+ 			     unsigned long ctrl)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_set(task, ctrl);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_get(task);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ void x86_spec_ctrl_setup_ap(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_IBRS))
+ 		x86_spec_ctrl_set(x86_spec_ctrl_base & ~x86_spec_ctrl_mask);
+ 
+ 	if (ssb_mode == SPEC_STORE_BYPASS_DISABLE)
+ 		x86_amd_rds_enable();
+ }
++>>>>>>> 356e4bfff2c5 (prctl: Add force disable speculation)
  
  #ifdef CONFIG_SYSFS
 -
 -ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
 -
 -	switch (bug) {
 -	case X86_BUG_CPU_MELTDOWN:
 -		if (boot_cpu_has(X86_FEATURE_PTI))
 -			return sprintf(buf, "Mitigation: PTI\n");
 -
 -		break;
 -
 -	case X86_BUG_SPECTRE_V1:
 -		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 -
 -	case X86_BUG_SPECTRE_V2:
 -		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
 -			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
 -			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 -			       spectre_v2_module_string());
 -
 -	case X86_BUG_SPEC_STORE_BYPASS:
 -		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
 -
 -	default:
 -		break;
 -	}
 -
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
  	return sprintf(buf, "Vulnerable\n");
  }
  
diff --cc fs/proc/array.c
index d7c97e89352e,d178a0236514..000000000000
--- a/fs/proc/array.c
+++ b/fs/proc/array.c
@@@ -362,9 -332,35 +362,37 @@@ static inline void task_cap(struct seq_
  
  static inline void task_seccomp(struct seq_file *m, struct task_struct *p)
  {
 -	seq_put_decimal_ull(m, "NoNewPrivs:\t", task_no_new_privs(p));
  #ifdef CONFIG_SECCOMP
 -	seq_put_decimal_ull(m, "\nSeccomp:\t", p->seccomp.mode);
 +	seq_printf(m, "Seccomp:\t%d\n", p->seccomp.mode);
  #endif
++<<<<<<< HEAD
++=======
+ 	seq_printf(m, "\nSpeculation Store Bypass:\t");
+ 	switch (arch_prctl_spec_ctrl_get(p, PR_SPEC_STORE_BYPASS)) {
+ 	case -EINVAL:
+ 		seq_printf(m, "unknown");
+ 		break;
+ 	case PR_SPEC_NOT_AFFECTED:
+ 		seq_printf(m, "not vulnerable");
+ 		break;
+ 	case PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE:
+ 		seq_printf(m, "thread force mitigated");
+ 		break;
+ 	case PR_SPEC_PRCTL | PR_SPEC_DISABLE:
+ 		seq_printf(m, "thread mitigated");
+ 		break;
+ 	case PR_SPEC_PRCTL | PR_SPEC_ENABLE:
+ 		seq_printf(m, "thread vulnerable");
+ 		break;
+ 	case PR_SPEC_DISABLE:
+ 		seq_printf(m, "globally mitigated");
+ 		break;
+ 	default:
+ 		seq_printf(m, "vulnerable");
+ 		break;
+ 	}
+ 	seq_putc(m, '\n');
++>>>>>>> 356e4bfff2c5 (prctl: Add force disable speculation)
  }
  
  static inline void task_context_switch_counts(struct seq_file *m,
diff --cc include/linux/sched.h
index 0646138be0ad,e4218d4deba0..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -2095,34 -1362,51 +2095,54 @@@ extern void thread_group_cputime_adjust
   * child is not running and in turn not changing child->flags
   * at the same time the parent does it.
   */
 -#define clear_stopped_child_used_math(child)	do { (child)->flags &= ~PF_USED_MATH; } while (0)
 -#define set_stopped_child_used_math(child)	do { (child)->flags |= PF_USED_MATH; } while (0)
 -#define clear_used_math()			clear_stopped_child_used_math(current)
 -#define set_used_math()				set_stopped_child_used_math(current)
 -
 +#define clear_stopped_child_used_math(child) do { (child)->flags &= ~PF_USED_MATH; } while (0)
 +#define set_stopped_child_used_math(child) do { (child)->flags |= PF_USED_MATH; } while (0)
 +#define clear_used_math() clear_stopped_child_used_math(current)
 +#define set_used_math() set_stopped_child_used_math(current)
  #define conditional_stopped_child_used_math(condition, child) \
  	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)
 -
 -#define conditional_used_math(condition)	conditional_stopped_child_used_math(condition, current)
 -
 +#define conditional_used_math(condition) \
 +	conditional_stopped_child_used_math(condition, current)
  #define copy_to_stopped_child_used_math(child) \
  	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)
 -
  /* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */
 -#define tsk_used_math(p)			((p)->flags & PF_USED_MATH)
 -#define used_math()				tsk_used_math(current)
 -
 -static inline bool is_percpu_thread(void)
 -{
 -#ifdef CONFIG_SMP
 -	return (current->flags & PF_NO_SETAFFINITY) &&
 -		(current->nr_cpus_allowed  == 1);
 -#else
 -	return true;
 -#endif
 -}
 +#define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 +#define used_math() tsk_used_math(current)
  
  /* Per-process atomic flags. */
++<<<<<<< HEAD
 +#define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
 +#define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */
 +#define PFA_SPREAD_SLAB  2      /* Spread some slab caches over cpuset */
 +
 +#define TASK_PFA_TEST(name, func)                                      \
 +       static inline bool task_##func(struct task_struct *p)           \
 +       { return test_bit(PFA_##name, &p->atomic_flags); }
 +#define TASK_PFA_SET(name, func)                                       \
 +       static inline void task_set_##func(struct task_struct *p)       \
 +       { set_bit(PFA_##name, &p->atomic_flags); }
 +#define TASK_PFA_CLEAR(name, func)                                     \
 +       static inline void task_clear_##func(struct task_struct *p)     \
 +       { clear_bit(PFA_##name, &p->atomic_flags); }
++=======
+ #define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */
+ #define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */
+ #define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */
+ #define PFA_SPEC_SSB_DISABLE		3	/* Speculative Store Bypass disabled */
+ #define PFA_SPEC_SSB_FORCE_DISABLE	4	/* Speculative Store Bypass force disabled*/
+ 
+ #define TASK_PFA_TEST(name, func)					\
+ 	static inline bool task_##func(struct task_struct *p)		\
+ 	{ return test_bit(PFA_##name, &p->atomic_flags); }
+ 
+ #define TASK_PFA_SET(name, func)					\
+ 	static inline void task_set_##func(struct task_struct *p)	\
+ 	{ set_bit(PFA_##name, &p->atomic_flags); }
+ 
+ #define TASK_PFA_CLEAR(name, func)					\
+ 	static inline void task_clear_##func(struct task_struct *p)	\
+ 	{ clear_bit(PFA_##name, &p->atomic_flags); }
++>>>>>>> 356e4bfff2c5 (prctl: Add force disable speculation)
  
  TASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)
  TASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)
@@@ -2135,702 -1419,156 +2155,714 @@@ TASK_PFA_TEST(SPREAD_SLAB, spread_slab
  TASK_PFA_SET(SPREAD_SLAB, spread_slab)
  TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
  
++<<<<<<< HEAD
 +/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags */
 +static inline gfp_t memalloc_noio_flags(gfp_t flags)
++=======
+ TASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ssb_disable)
+ TASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)
+ TASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)
+ 
 -TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 -TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
++TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
++TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
++
++static inline void
++current_restore_flags(unsigned long orig_flags, unsigned long flags)
++>>>>>>> 356e4bfff2c5 (prctl: Add force disable speculation)
 +{
 +	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
 +		flags &= ~__GFP_IO;
 +	return flags;
 +}
 +
 +static inline unsigned int memalloc_noio_save(void)
 +{
 +	unsigned int flags = current->flags & PF_MEMALLOC_NOIO;
 +	current->flags |= PF_MEMALLOC_NOIO;
 +	return flags;
 +}
 +
 +static inline void memalloc_noio_restore(unsigned int flags)
 +{
 +	current->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;
 +}
 +
 +/*
 + * task->jobctl flags
 + */
 +#define JOBCTL_STOP_SIGMASK	0xffff	/* signr of the last group stop */
 +
 +#define JOBCTL_STOP_DEQUEUED_BIT 16	/* stop signal dequeued */
 +#define JOBCTL_STOP_PENDING_BIT	17	/* task should stop for group stop */
 +#define JOBCTL_STOP_CONSUME_BIT	18	/* consume group stop count */
 +#define JOBCTL_TRAP_STOP_BIT	19	/* trap for STOP */
 +#define JOBCTL_TRAP_NOTIFY_BIT	20	/* trap for NOTIFY */
 +#define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
 +#define JOBCTL_LISTENING_BIT	22	/* ptracer is listening for events */
 +
 +#define JOBCTL_STOP_DEQUEUED	(1 << JOBCTL_STOP_DEQUEUED_BIT)
 +#define JOBCTL_STOP_PENDING	(1 << JOBCTL_STOP_PENDING_BIT)
 +#define JOBCTL_STOP_CONSUME	(1 << JOBCTL_STOP_CONSUME_BIT)
 +#define JOBCTL_TRAP_STOP	(1 << JOBCTL_TRAP_STOP_BIT)
 +#define JOBCTL_TRAP_NOTIFY	(1 << JOBCTL_TRAP_NOTIFY_BIT)
 +#define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
 +#define JOBCTL_LISTENING	(1 << JOBCTL_LISTENING_BIT)
 +
 +#define JOBCTL_TRAP_MASK	(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)
 +#define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)
 +
 +extern bool task_set_jobctl_pending(struct task_struct *task,
 +				    unsigned int mask);
 +extern void task_clear_jobctl_trapping(struct task_struct *task);
 +extern void task_clear_jobctl_pending(struct task_struct *task,
 +				      unsigned int mask);
 +
 +#ifdef CONFIG_PREEMPT_RCU
 +
 +#define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
 +#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
 +
 +static inline void rcu_copy_process(struct task_struct *p)
 +{
 +	p->rcu_read_lock_nesting = 0;
 +	p->rcu_read_unlock_special = 0;
 +#ifdef CONFIG_TREE_PREEMPT_RCU
 +	p->rcu_blocked_node = NULL;
 +#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 +#ifdef CONFIG_RCU_BOOST
 +	p->rcu_boost_mutex = NULL;
 +#endif /* #ifdef CONFIG_RCU_BOOST */
 +	INIT_LIST_HEAD(&p->rcu_node_entry);
 +}
 +
 +#else
 +
 +static inline void rcu_copy_process(struct task_struct *p)
 +{
 +}
 +
 +#endif
 +
 +static inline void tsk_restore_flags(struct task_struct *task,
 +				unsigned long orig_flags, unsigned long flags)
 +{
 +	task->flags &= ~flags;
 +	task->flags |= orig_flags & flags;
 +}
 +
 +extern int cpuset_cpumask_can_shrink(const struct cpumask *cur,
 +				     const struct cpumask *trial);
 +extern int task_can_attach(struct task_struct *p,
 +			   const struct cpumask *cs_cpus_allowed);
 +#ifdef CONFIG_SMP
 +extern void do_set_cpus_allowed(struct task_struct *p,
 +			       const struct cpumask *new_mask);
 +
 +extern int set_cpus_allowed_ptr(struct task_struct *p,
 +				const struct cpumask *new_mask);
 +#else
 +static inline void do_set_cpus_allowed(struct task_struct *p,
 +				      const struct cpumask *new_mask)
 +{
 +}
 +static inline int set_cpus_allowed_ptr(struct task_struct *p,
 +				       const struct cpumask *new_mask)
 +{
 +	if (!cpumask_test_cpu(0, new_mask))
 +		return -EINVAL;
 +	return 0;
 +}
 +#endif
 +
 +#ifdef CONFIG_NO_HZ_COMMON
 +void calc_load_enter_idle(void);
 +void calc_load_exit_idle(void);
 +#else
 +static inline void calc_load_enter_idle(void) { }
 +static inline void calc_load_exit_idle(void) { }
 +#endif /* CONFIG_NO_HZ_COMMON */
 +
 +#ifndef CONFIG_CPUMASK_OFFSTACK
 +static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 +{
 +	return set_cpus_allowed_ptr(p, &new_mask);
 +}
 +#endif
 +
 +/*
 + * Do not use outside of architecture code which knows its limitations.
 + *
 + * sched_clock() has no promise of monotonicity or bounded drift between
 + * CPUs, use (which you should not) requires disabling IRQs.
 + *
 + * Please use one of the three interfaces below.
 + */
 +extern unsigned long long notrace sched_clock(void);
 +/*
 + * See the comment in kernel/sched/clock.c
 + */
 +extern u64 cpu_clock(int cpu);
 +extern u64 local_clock(void);
 +extern u64 running_clock(void);
 +extern u64 sched_clock_cpu(int cpu);
 +
 +
 +extern void sched_clock_init(void);
 +
 +#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
 +static inline void sched_clock_tick(void)
 +{
 +}
 +
 +static inline void sched_clock_idle_sleep_event(void)
 +{
 +}
 +
 +static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 +{
 +}
 +#else
 +/*
 + * Architectures can set this to 1 if they have specified
 + * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
 + * but then during bootup it turns out that sched_clock()
 + * is reliable after all:
 + */
 +extern int sched_clock_stable(void);
 +extern void set_sched_clock_stable(void);
 +extern void clear_sched_clock_stable(void);
 +
 +extern void sched_clock_tick(void);
 +extern void sched_clock_idle_sleep_event(void);
 +extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 +#endif
 +
 +#ifdef CONFIG_IRQ_TIME_ACCOUNTING
 +/*
 + * An i/f to runtime opt-in for irq time accounting based off of sched_clock.
 + * The reason for this explicit opt-in is not to have perf penalty with
 + * slow sched_clocks.
 + */
 +extern void enable_sched_clock_irqtime(void);
 +extern void disable_sched_clock_irqtime(void);
 +#else
 +static inline void enable_sched_clock_irqtime(void) {}
 +static inline void disable_sched_clock_irqtime(void) {}
 +#endif
 +
 +extern unsigned long long
 +task_sched_runtime(struct task_struct *task);
 +
 +/* sched_exec is called by processes performing an exec */
 +#ifdef CONFIG_SMP
 +extern void sched_exec(void);
 +#else
 +#define sched_exec()   {}
 +#endif
 +
 +extern void sched_clock_idle_sleep_event(void);
 +extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 +
 +#ifdef CONFIG_HOTPLUG_CPU
 +extern void idle_task_exit(void);
 +#else
 +static inline void idle_task_exit(void) {}
 +#endif
 +
 +#if defined(CONFIG_NO_HZ_COMMON) && defined(CONFIG_SMP)
 +extern void wake_up_nohz_cpu(int cpu);
 +#else
 +static inline void wake_up_nohz_cpu(int cpu) { }
 +#endif
 +
 +#ifdef CONFIG_NO_HZ_FULL
 +extern bool sched_can_stop_tick(void);
 +extern u64 scheduler_tick_max_deferment(void);
 +#else
 +static inline bool sched_can_stop_tick(void) { return false; }
 +#endif
 +
 +#ifdef CONFIG_SCHED_AUTOGROUP
 +extern void sched_autogroup_create_attach(struct task_struct *p);
 +extern void sched_autogroup_detach(struct task_struct *p);
 +extern void sched_autogroup_fork(struct signal_struct *sig);
 +extern void sched_autogroup_exit(struct signal_struct *sig);
 +extern void sched_autogroup_exit_task(struct task_struct *p);
 +#ifdef CONFIG_PROC_FS
 +extern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);
 +extern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);
 +#endif
 +#else
 +static inline void sched_autogroup_create_attach(struct task_struct *p) { }
 +static inline void sched_autogroup_detach(struct task_struct *p) { }
 +static inline void sched_autogroup_fork(struct signal_struct *sig) { }
 +static inline void sched_autogroup_exit(struct signal_struct *sig) { }
 +static inline void sched_autogroup_exit_task(struct task_struct *p) { }
 +#endif
 +
 +extern int yield_to(struct task_struct *p, bool preempt);
 +extern void set_user_nice(struct task_struct *p, long nice);
 +extern int task_prio(const struct task_struct *p);
 +extern int task_nice(const struct task_struct *p);
 +extern int can_nice(const struct task_struct *p, const int nice);
 +extern int task_curr(const struct task_struct *p);
 +extern int idle_cpu(int cpu);
 +extern int sched_setscheduler(struct task_struct *, int,
 +			      const struct sched_param *);
 +extern int sched_setscheduler_nocheck(struct task_struct *, int,
 +				      const struct sched_param *);
 +extern int sched_setattr(struct task_struct *,
 +			 const struct sched_attr *);
 +extern struct task_struct *idle_task(int cpu);
 +/**
 + * is_idle_task - is the specified task an idle task?
 + * @p: the task in question.
 + *
 + * Return: 1 if @p is an idle task. 0 otherwise.
 + */
 +static inline bool is_idle_task(const struct task_struct *p)
 +{
 +	return p->pid == 0;
 +}
 +extern struct task_struct *curr_task(int cpu);
 +extern void set_curr_task(int cpu, struct task_struct *p);
 +
 +void yield(void);
 +
 +/*
 + * The default (Linux) execution domain.
 + */
 +extern struct exec_domain	default_exec_domain;
 +
 +union thread_union {
 +	struct thread_info thread_info;
 +	unsigned long stack[THREAD_SIZE/sizeof(long)];
 +};
 +
 +#ifndef __HAVE_ARCH_KSTACK_END
 +static inline int kstack_end(void *addr)
 +{
 +	/* Reliable end of stack detection:
 +	 * Some APM bios versions misalign the stack
 +	 */
 +	return !(((unsigned long)addr+sizeof(void*)-1) & (THREAD_SIZE-sizeof(void*)));
 +}
 +#endif
 +
 +extern union thread_union init_thread_union;
 +extern struct task_struct init_task;
 +
 +extern struct   mm_struct init_mm;
 +
 +extern struct pid_namespace init_pid_ns;
 +
 +/*
 + * find a task by one of its numerical ids
 + *
 + * find_task_by_pid_ns():
 + *      finds a task by its pid in the specified namespace
 + * find_task_by_vpid():
 + *      finds a task by its virtual pid
 + *
 + * see also find_vpid() etc in include/linux/pid.h
 + */
 +
 +extern struct task_struct *find_task_by_vpid(pid_t nr);
 +extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 +		struct pid_namespace *ns);
 +
 +extern void __set_special_pids(struct pid *pid);
 +
 +/* per-UID process charging. */
 +extern struct user_struct * alloc_uid(kuid_t);
 +static inline struct user_struct *get_uid(struct user_struct *u)
 +{
 +	atomic_inc(&u->__count);
 +	return u;
 +}
 +extern void free_uid(struct user_struct *);
 +
 +#include <asm/current.h>
 +
 +extern void xtime_update(unsigned long ticks);
 +
 +extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 +extern int wake_up_process(struct task_struct *tsk);
 +extern void wake_up_new_task(struct task_struct *tsk);
 +#ifdef CONFIG_SMP
 + extern void kick_process(struct task_struct *tsk);
 +#else
 + static inline void kick_process(struct task_struct *tsk) { }
 +#endif
 +extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
 +extern void sched_dead(struct task_struct *p);
 +
 +extern void proc_caches_init(void);
 +extern void flush_signals(struct task_struct *);
 +extern void __flush_signals(struct task_struct *);
 +extern void ignore_signals(struct task_struct *);
 +extern void flush_signal_handlers(struct task_struct *, int force_default);
 +extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);
 +
 +static inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
 +{
 +	unsigned long flags;
 +	int ret;
 +
 +	spin_lock_irqsave(&tsk->sighand->siglock, flags);
 +	ret = dequeue_signal(tsk, mask, info);
 +	spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
 +
 +	return ret;
 +}
 +
 +extern void block_all_signals(int (*notifier)(void *priv), void *priv,
 +			      sigset_t *mask);
 +extern void unblock_all_signals(void);
 +extern void release_task(struct task_struct * p);
 +extern int send_sig_info(int, struct siginfo *, struct task_struct *);
 +extern int force_sigsegv(int, struct task_struct *);
 +extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 +extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
 +extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
 +extern int kill_pid_info_as_cred(int, struct siginfo *, struct pid *,
 +				const struct cred *, u32);
 +extern int kill_pgrp(struct pid *pid, int sig, int priv);
 +extern int kill_pid(struct pid *pid, int sig, int priv);
 +extern int kill_proc_info(int, struct siginfo *, pid_t);
 +extern __must_check bool do_notify_parent(struct task_struct *, int);
 +extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
 +extern void force_sig(int, struct task_struct *);
 +extern int send_sig(int, struct task_struct *, int);
 +extern int zap_other_threads(struct task_struct *p);
 +extern struct sigqueue *sigqueue_alloc(void);
 +extern void sigqueue_free(struct sigqueue *);
 +extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
 +extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
 +
 +static inline void restore_saved_sigmask(void)
 +{
 +	if (test_and_clear_restore_sigmask())
 +		__set_current_blocked(&current->saved_sigmask);
 +}
 +
 +static inline sigset_t *sigmask_to_save(void)
 +{
 +	sigset_t *res = &current->blocked;
 +	if (unlikely(test_restore_sigmask()))
 +		res = &current->saved_sigmask;
 +	return res;
 +}
 +
 +static inline int kill_cad_pid(int sig, int priv)
 +{
 +	return kill_pid(cad_pid, sig, priv);
 +}
 +
 +/* These can be the second arg to send_sig_info/send_group_sig_info.  */
 +#define SEND_SIG_NOINFO ((struct siginfo *) 0)
 +#define SEND_SIG_PRIV	((struct siginfo *) 1)
 +#define SEND_SIG_FORCED	((struct siginfo *) 2)
 +
 +/*
 + * True if we are on the alternate signal stack.
 + */
 +static inline int on_sig_stack(unsigned long sp)
 +{
 +#ifdef CONFIG_STACK_GROWSUP
 +	return sp >= current->sas_ss_sp &&
 +		sp - current->sas_ss_sp < current->sas_ss_size;
 +#else
 +	return sp > current->sas_ss_sp &&
 +		sp - current->sas_ss_sp <= current->sas_ss_size;
 +#endif
 +}
 +
 +static inline int sas_ss_flags(unsigned long sp)
 +{
 +	return (current->sas_ss_size == 0 ? SS_DISABLE
 +		: on_sig_stack(sp) ? SS_ONSTACK : 0);
 +}
 +
 +static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
 +{
 +	if (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))
 +#ifdef CONFIG_STACK_GROWSUP
 +		return current->sas_ss_sp;
 +#else
 +		return current->sas_ss_sp + current->sas_ss_size;
 +#endif
 +	return sp;
 +}
 +
 +/*
 + * Routines for handling mm_structs
 + */
 +extern struct mm_struct * mm_alloc(void);
 +
 +/* mmdrop drops the mm and the page tables */
 +extern void __mmdrop(struct mm_struct *);
 +static inline void mmdrop(struct mm_struct *mm)
 +{
 +	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
 +		__mmdrop(mm);
 +}
 +
 +static inline bool mmget_not_zero(struct mm_struct *mm)
 +{
 +	return atomic_inc_not_zero(&mm->mm_users);
 +}
 +
 +/* mmput gets rid of the mappings and all user-space */
 +extern void mmput(struct mm_struct *);
 +/* Grab a reference to a task's mm, if it is not already going away */
 +extern struct mm_struct *get_task_mm(struct task_struct *task);
 +/*
 + * Grab a reference to a task's mm, if it is not already going away
 + * and ptrace_may_access with the mode parameter passed to it
 + * succeeds.
 + */
 +extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
 +/* Remove the current tasks stale references to the old mm_struct */
 +extern void mm_release(struct task_struct *, struct mm_struct *);
 +/* Allocate a new mm structure and copy contents from tsk->mm */
 +extern struct mm_struct *dup_mm(struct task_struct *tsk);
 +
 +extern int copy_thread(unsigned long, unsigned long, unsigned long,
 +			struct task_struct *);
 +extern void flush_thread(void);
 +extern void exit_thread(void);
 +
 +extern void exit_files(struct task_struct *);
 +extern void __cleanup_sighand(struct sighand_struct *);
 +
 +extern void exit_itimers(struct signal_struct *);
 +extern void flush_itimer_signals(void);
 +
 +extern void do_group_exit(int);
 +
 +extern int allow_signal(int);
 +extern int disallow_signal(int);
 +
 +extern int do_execve(struct filename *,
 +		     const char __user * const __user *,
 +		     const char __user * const __user *);
 +extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
 +struct task_struct *fork_idle(int);
 +extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 +
 +extern void __set_task_comm(struct task_struct *tsk, char *from, bool exec);
 +static inline void set_task_comm(struct task_struct *tsk, char *from)
 +{
 +	__set_task_comm(tsk, from, false);
 +}
 +extern char *get_task_comm(char *to, struct task_struct *tsk);
 +
 +#ifdef CONFIG_SMP
 +void scheduler_ipi(void);
 +extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 +#else
 +static inline void scheduler_ipi(void) { }
 +static inline unsigned long wait_task_inactive(struct task_struct *p,
 +					       long match_state)
 +{
 +	return 1;
 +}
 +#endif
 +
 +#define next_task(p) \
 +	list_entry_rcu((p)->tasks.next, struct task_struct, tasks)
 +
 +#define for_each_process(p) \
 +	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
 +
 +extern bool current_is_single_threaded(void);
 +
 +/*
 + * Careful: do_each_thread/while_each_thread is a double loop so
 + *          'break' will not work as expected - use goto instead.
 + */
 +#define do_each_thread(g, t) \
 +	for (g = t = &init_task ; (g = t = next_task(g)) != &init_task ; ) do
 +
 +#define while_each_thread(g, t) \
 +	while ((t = next_thread(t)) != g)
 +
 +#define __for_each_thread(signal, t)	\
 +	list_for_each_entry_rcu(t, &(signal)->thread_head, thread_node)
 +
 +#define for_each_thread(p, t)		\
 +	__for_each_thread((p)->signal, t)
  
 -static inline void
 -current_restore_flags(unsigned long orig_flags, unsigned long flags)
 +/* Careful: this is a double loop, 'break' won't work as expected. */
 +#define for_each_process_thread(p, t)	\
 +	for_each_process(p) for_each_thread(p, t)
 +
 +static inline int get_nr_threads(struct task_struct *tsk)
  {
 -	current->flags &= ~flags;
 -	current->flags |= orig_flags & flags;
 +	return tsk->signal->nr_threads;
  }
  
 -extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
 -extern int task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);
 -#ifdef CONFIG_SMP
 -extern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);
 -extern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);
 -#else
 -static inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 +static inline bool thread_group_leader(struct task_struct *p)
  {
 +	return p->exit_signal >= 0;
  }
 -static inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 +
 +/* Do to the insanities of de_thread it is possible for a process
 + * to have the pid of the thread group leader without actually being
 + * the thread group leader.  For iteration through the pids in proc
 + * all we care about is that we have a task with the appropriate
 + * pid, we don't actually care if we have the right task.
 + */
 +static inline int has_group_leader_pid(struct task_struct *p)
  {
 -	if (!cpumask_test_cpu(0, new_mask))
 -		return -EINVAL;
 -	return 0;
 +	return p->pid == p->tgid;
  }
 -#endif
  
 -#ifndef cpu_relax_yield
 -#define cpu_relax_yield() cpu_relax()
 -#endif
 +static inline
 +int same_thread_group(struct task_struct *p1, struct task_struct *p2)
 +{
 +	return p1->tgid == p2->tgid;
 +}
  
 -extern int yield_to(struct task_struct *p, bool preempt);
 -extern void set_user_nice(struct task_struct *p, long nice);
 -extern int task_prio(const struct task_struct *p);
 +static inline struct task_struct *next_thread(const struct task_struct *p)
 +{
 +	return list_entry_rcu(p->thread_group.next,
 +			      struct task_struct, thread_group);
 +}
  
 -/**
 - * task_nice - return the nice value of a given task.
 - * @p: the task in question.
 - *
 - * Return: The nice value [ -20 ... 0 ... 19 ].
 - */
 -static inline int task_nice(const struct task_struct *p)
 +static inline int thread_group_empty(struct task_struct *p)
  {
 -	return PRIO_TO_NICE((p)->static_prio);
 +	return list_empty(&p->thread_group);
  }
  
 -extern int can_nice(const struct task_struct *p, const int nice);
 -extern int task_curr(const struct task_struct *p);
 -extern int idle_cpu(int cpu);
 -extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);
 -extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);
 -extern int sched_setattr(struct task_struct *, const struct sched_attr *);
 -extern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);
 -extern struct task_struct *idle_task(int cpu);
 +#define delay_group_leader(p) \
 +		(thread_group_leader(p) && !thread_group_empty(p))
  
 -/**
 - * is_idle_task - is the specified task an idle task?
 - * @p: the task in question.
 +/*
 + * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
 + * subscriptions and synchronises with wait4().  Also used in procfs.  Also
 + * pins the final release of task.io_context.  Also protects ->cpuset and
 + * ->cgroup.subsys[]. And ->vfork_done.
   *
 - * Return: 1 if @p is an idle task. 0 otherwise.
 + * Nests both inside and outside of qread_lock(&tasklist_lock).
 + * It must not be nested with qwrite_lock_irq(&tasklist_lock),
 + * neither inside nor outside.
   */
 -static inline bool is_idle_task(const struct task_struct *p)
 +static inline void task_lock(struct task_struct *p)
  {
 -	return !!(p->flags & PF_IDLE);
 +	spin_lock(&p->alloc_lock);
  }
  
 -extern struct task_struct *curr_task(int cpu);
 -extern void ia64_set_curr_task(int cpu, struct task_struct *p);
 +static inline void task_unlock(struct task_struct *p)
 +{
 +	spin_unlock(&p->alloc_lock);
 +}
  
 -void yield(void);
 +extern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
 +							unsigned long *flags);
  
 -union thread_union {
 -#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK
 -	struct task_struct task;
 -#endif
 -#ifndef CONFIG_THREAD_INFO_IN_TASK
 -	struct thread_info thread_info;
 -#endif
 -	unsigned long stack[THREAD_SIZE/sizeof(long)];
 -};
 +static inline struct sighand_struct *lock_task_sighand(struct task_struct *tsk,
 +						       unsigned long *flags)
 +{
 +	struct sighand_struct *ret;
  
 -#ifndef CONFIG_THREAD_INFO_IN_TASK
 -extern struct thread_info init_thread_info;
 -#endif
 +	ret = __lock_task_sighand(tsk, flags);
 +	(void)__cond_lock(&tsk->sighand->siglock, ret);
 +	return ret;
 +}
  
 -extern unsigned long init_stack[THREAD_SIZE / sizeof(unsigned long)];
 +static inline void unlock_task_sighand(struct task_struct *tsk,
 +						unsigned long *flags)
 +{
 +	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
 +}
  
 -#ifdef CONFIG_THREAD_INFO_IN_TASK
 -static inline struct thread_info *task_thread_info(struct task_struct *task)
 +#ifdef CONFIG_CGROUPS
 +static inline void threadgroup_change_begin(struct task_struct *tsk)
  {
 -	return &task->thread_info;
 +	down_read(&tsk->signal->group_rwsem);
 +}
 +static inline void threadgroup_change_end(struct task_struct *tsk)
 +{
 +	up_read(&tsk->signal->group_rwsem);
  }
 -#elif !defined(__HAVE_THREAD_FUNCTIONS)
 -# define task_thread_info(task)	((struct thread_info *)(task)->stack)
 -#endif
  
 -/*
 - * find a task by one of its numerical ids
 +/**
 + * threadgroup_lock - lock threadgroup
 + * @tsk: member task of the threadgroup to lock
   *
 - * find_task_by_pid_ns():
 - *      finds a task by its pid in the specified namespace
 - * find_task_by_vpid():
 - *      finds a task by its virtual pid
 + * Lock the threadgroup @tsk belongs to.  No new task is allowed to enter
 + * and member tasks aren't allowed to exit (as indicated by PF_EXITING) or
 + * change ->group_leader/pid.  This is useful for cases where the threadgroup
 + * needs to stay stable across blockable operations.
   *
 - * see also find_vpid() etc in include/linux/pid.h
 + * fork and exit paths explicitly call threadgroup_change_{begin|end}() for
 + * synchronization.  While held, no new task will be added to threadgroup
 + * and no existing live task will have its PF_EXITING set.
 + *
 + * de_thread() does threadgroup_change_{begin|end}() when a non-leader
 + * sub-thread becomes a new leader.
   */
 +static inline void threadgroup_lock(struct task_struct *tsk)
 +{
 +	down_write(&tsk->signal->group_rwsem);
 +}
  
 -extern struct task_struct *find_task_by_vpid(pid_t nr);
 -extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);
 -
 -/*
 - * find a task by its virtual pid and get the task struct
 +/**
 + * threadgroup_unlock - unlock threadgroup
 + * @tsk: member task of the threadgroup to unlock
 + *
 + * Reverse threadgroup_lock().
   */
 -extern struct task_struct *find_get_task_by_vpid(pid_t nr);
 -
 -extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 -extern int wake_up_process(struct task_struct *tsk);
 -extern void wake_up_new_task(struct task_struct *tsk);
 -
 -#ifdef CONFIG_SMP
 -extern void kick_process(struct task_struct *tsk);
 +static inline void threadgroup_unlock(struct task_struct *tsk)
 +{
 +	up_write(&tsk->signal->group_rwsem);
 +}
  #else
 -static inline void kick_process(struct task_struct *tsk) { }
 +static inline void threadgroup_change_begin(struct task_struct *tsk) {}
 +static inline void threadgroup_change_end(struct task_struct *tsk) {}
 +static inline void threadgroup_lock(struct task_struct *tsk) {}
 +static inline void threadgroup_unlock(struct task_struct *tsk) {}
  #endif
  
 -extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
 +#ifndef __HAVE_THREAD_FUNCTIONS
  
 -static inline void set_task_comm(struct task_struct *tsk, const char *from)
 +#define task_thread_info(task)	((struct thread_info *)(task)->stack)
 +#define task_stack_page(task)	((task)->stack)
 +
 +static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
  {
 -	__set_task_comm(tsk, from, false);
 +	*task_thread_info(p) = *task_thread_info(org);
 +	task_thread_info(p)->task = p;
  }
  
 -extern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);
 -#define get_task_comm(buf, tsk) ({			\
 -	BUILD_BUG_ON(sizeof(buf) != TASK_COMM_LEN);	\
 -	__get_task_comm(buf, sizeof(buf), tsk);		\
 -})
 +static inline unsigned long *end_of_stack(struct task_struct *p)
 +{
 +	return (unsigned long *)(task_thread_info(p) + 1);
 +}
  
 -#ifdef CONFIG_SMP
 -void scheduler_ipi(void);
 -extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 -#else
 -static inline void scheduler_ipi(void) { }
 -static inline unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 +#endif
 +
 +static inline int object_is_on_stack(void *obj)
  {
 -	return 1;
 +	void *stack = task_stack_page(current);
 +
 +	return (obj >= stack) && (obj < (stack + THREAD_SIZE));
 +}
 +
 +extern void thread_info_cache_init(void);
 +
 +#ifdef CONFIG_DEBUG_STACK_USAGE
 +static inline unsigned long stack_not_used(struct task_struct *p)
 +{
 +	unsigned long *n = end_of_stack(p);
 +
 +	do { 	/* Skip over canary */
 +		n++;
 +	} while (!*n);
 +
 +	return (unsigned long)n - (unsigned long)end_of_stack(p);
  }
  #endif
  
diff --cc include/uapi/linux/prctl.h
index f818d081129b,db9f15f5db04..000000000000
--- a/include/uapi/linux/prctl.h
+++ b/include/uapi/linux/prctl.h
@@@ -192,4 -198,25 +192,28 @@@ struct prctl_mm_map 
  # define PR_CAP_AMBIENT_LOWER		3
  # define PR_CAP_AMBIENT_CLEAR_ALL	4
  
++<<<<<<< HEAD
++=======
+ /* arm64 Scalable Vector Extension controls */
+ /* Flag values must be kept in sync with ptrace NT_ARM_SVE interface */
+ #define PR_SVE_SET_VL			50	/* set task vector length */
+ # define PR_SVE_SET_VL_ONEXEC		(1 << 18) /* defer effect until exec */
+ #define PR_SVE_GET_VL			51	/* get task vector length */
+ /* Bits common to PR_SVE_SET_VL and PR_SVE_GET_VL */
+ # define PR_SVE_VL_LEN_MASK		0xffff
+ # define PR_SVE_VL_INHERIT		(1 << 17) /* inherit across exec */
+ 
+ /* Per task speculation control */
+ #define PR_GET_SPECULATION_CTRL		52
+ #define PR_SET_SPECULATION_CTRL		53
+ /* Speculation control variants */
+ # define PR_SPEC_STORE_BYPASS		0
+ /* Return and control values for PR_SET/GET_SPECULATION_CTRL */
+ # define PR_SPEC_NOT_AFFECTED		0
+ # define PR_SPEC_PRCTL			(1UL << 0)
+ # define PR_SPEC_ENABLE			(1UL << 1)
+ # define PR_SPEC_DISABLE		(1UL << 2)
+ # define PR_SPEC_FORCE_DISABLE		(1UL << 3)
+ 
++>>>>>>> 356e4bfff2c5 (prctl: Add force disable speculation)
  #endif /* _LINUX_PRCTL_H */
* Unmerged path Documentation/userspace-api/spec_ctrl.rst
* Unmerged path Documentation/userspace-api/spec_ctrl.rst
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path fs/proc/array.c
* Unmerged path include/linux/sched.h
* Unmerged path include/uapi/linux/prctl.h
