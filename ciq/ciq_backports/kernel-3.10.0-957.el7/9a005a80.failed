iommu/iova: Add flush timer

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iommu] iova: Add flush timer (Jerry Snitselaar) [1519117]
Rebuild_FUZZ: 87.50%
commit-author Joerg Roedel <jroedel@suse.de>
commit 9a005a800ae817c2c90ef117d7cd77614d866777
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9a005a80.failed

Add a timer to flush entries from the Flush-Queues every
10ms. This makes sure that no stale TLB entries remain for
too long after an IOVA has been unmapped.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 9a005a800ae817c2c90ef117d7cd77614d866777)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
#	include/linux/iova.h
diff --cc drivers/iommu/iova.c
index f106fd9782bf,33edfa794ae9..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -31,6 -32,8 +31,11 @@@ static unsigned long iova_rcache_get(st
  				     unsigned long limit_pfn);
  static void init_iova_rcaches(struct iova_domain *iovad);
  static void free_iova_rcaches(struct iova_domain *iovad);
++<<<<<<< HEAD
++=======
+ static void fq_destroy_all_entries(struct iova_domain *iovad);
+ static void fq_flush_timeout(unsigned long data);
++>>>>>>> 9a005a800ae8 (iommu/iova: Add flush timer)
  
  void
  init_iova_domain(struct iova_domain *iovad, unsigned long granule,
@@@ -53,6 -58,55 +58,58 @@@
  }
  EXPORT_SYMBOL_GPL(init_iova_domain);
  
++<<<<<<< HEAD
++=======
+ static void free_iova_flush_queue(struct iova_domain *iovad)
+ {
+ 	if (!iovad->fq)
+ 		return;
+ 
+ 	if (timer_pending(&iovad->fq_timer))
+ 		del_timer(&iovad->fq_timer);
+ 
+ 	fq_destroy_all_entries(iovad);
+ 
+ 	free_percpu(iovad->fq);
+ 
+ 	iovad->fq         = NULL;
+ 	iovad->flush_cb   = NULL;
+ 	iovad->entry_dtor = NULL;
+ }
+ 
+ int init_iova_flush_queue(struct iova_domain *iovad,
+ 			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor)
+ {
+ 	int cpu;
+ 
+ 	atomic64_set(&iovad->fq_flush_start_cnt,  0);
+ 	atomic64_set(&iovad->fq_flush_finish_cnt, 0);
+ 
+ 	iovad->fq = alloc_percpu(struct iova_fq);
+ 	if (!iovad->fq)
+ 		return -ENOMEM;
+ 
+ 	iovad->flush_cb   = flush_cb;
+ 	iovad->entry_dtor = entry_dtor;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq;
+ 
+ 		fq = per_cpu_ptr(iovad->fq, cpu);
+ 		fq->head = 0;
+ 		fq->tail = 0;
+ 
+ 		spin_lock_init(&fq->lock);
+ 	}
+ 
+ 	setup_timer(&iovad->fq_timer, fq_flush_timeout, (unsigned long)iovad);
+ 	atomic_set(&iovad->fq_timer_on, 0);
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(init_iova_flush_queue);
+ 
++>>>>>>> 9a005a800ae8 (iommu/iova: Add flush timer)
  static struct rb_node *
  __get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)
  {
@@@ -449,6 -476,135 +506,138 @@@ free_iova_fast(struct iova_domain *iova
  }
  EXPORT_SYMBOL_GPL(free_iova_fast);
  
++<<<<<<< HEAD
++=======
+ #define fq_ring_for_each(i, fq) \
+ 	for ((i) = (fq)->head; (i) != (fq)->tail; (i) = ((i) + 1) % IOVA_FQ_SIZE)
+ 
+ static inline bool fq_full(struct iova_fq *fq)
+ {
+ 	assert_spin_locked(&fq->lock);
+ 	return (((fq->tail + 1) % IOVA_FQ_SIZE) == fq->head);
+ }
+ 
+ static inline unsigned fq_ring_add(struct iova_fq *fq)
+ {
+ 	unsigned idx = fq->tail;
+ 
+ 	assert_spin_locked(&fq->lock);
+ 
+ 	fq->tail = (idx + 1) % IOVA_FQ_SIZE;
+ 
+ 	return idx;
+ }
+ 
+ static void fq_ring_free(struct iova_domain *iovad, struct iova_fq *fq)
+ {
+ 	u64 counter = atomic64_read(&iovad->fq_flush_finish_cnt);
+ 	unsigned idx;
+ 
+ 	assert_spin_locked(&fq->lock);
+ 
+ 	fq_ring_for_each(idx, fq) {
+ 
+ 		if (fq->entries[idx].counter >= counter)
+ 			break;
+ 
+ 		if (iovad->entry_dtor)
+ 			iovad->entry_dtor(fq->entries[idx].data);
+ 
+ 		free_iova_fast(iovad,
+ 			       fq->entries[idx].iova_pfn,
+ 			       fq->entries[idx].pages);
+ 
+ 		fq->head = (fq->head + 1) % IOVA_FQ_SIZE;
+ 	}
+ }
+ 
+ static void iova_domain_flush(struct iova_domain *iovad)
+ {
+ 	atomic64_inc(&iovad->fq_flush_start_cnt);
+ 	iovad->flush_cb(iovad);
+ 	atomic64_inc(&iovad->fq_flush_finish_cnt);
+ }
+ 
+ static void fq_destroy_all_entries(struct iova_domain *iovad)
+ {
+ 	int cpu;
+ 
+ 	/*
+ 	 * This code runs when the iova_domain is being detroyed, so don't
+ 	 * bother to free iovas, just call the entry_dtor on all remaining
+ 	 * entries.
+ 	 */
+ 	if (!iovad->entry_dtor)
+ 		return;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq = per_cpu_ptr(iovad->fq, cpu);
+ 		int idx;
+ 
+ 		fq_ring_for_each(idx, fq)
+ 			iovad->entry_dtor(fq->entries[idx].data);
+ 	}
+ }
+ 
+ static void fq_flush_timeout(unsigned long data)
+ {
+ 	struct iova_domain *iovad = (struct iova_domain *)data;
+ 	int cpu;
+ 
+ 	atomic_set(&iovad->fq_timer_on, 0);
+ 	iova_domain_flush(iovad);
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		unsigned long flags;
+ 		struct iova_fq *fq;
+ 
+ 		fq = per_cpu_ptr(iovad->fq, cpu);
+ 		spin_lock_irqsave(&fq->lock, flags);
+ 		fq_ring_free(iovad, fq);
+ 		spin_unlock_irqrestore(&fq->lock, flags);
+ 	}
+ }
+ 
+ void queue_iova(struct iova_domain *iovad,
+ 		unsigned long pfn, unsigned long pages,
+ 		unsigned long data)
+ {
+ 	struct iova_fq *fq = get_cpu_ptr(iovad->fq);
+ 	unsigned long flags;
+ 	unsigned idx;
+ 
+ 	spin_lock_irqsave(&fq->lock, flags);
+ 
+ 	/*
+ 	 * First remove all entries from the flush queue that have already been
+ 	 * flushed out on another CPU. This makes the fq_full() check below less
+ 	 * likely to be true.
+ 	 */
+ 	fq_ring_free(iovad, fq);
+ 
+ 	if (fq_full(fq)) {
+ 		iova_domain_flush(iovad);
+ 		fq_ring_free(iovad, fq);
+ 	}
+ 
+ 	idx = fq_ring_add(fq);
+ 
+ 	fq->entries[idx].iova_pfn = pfn;
+ 	fq->entries[idx].pages    = pages;
+ 	fq->entries[idx].data     = data;
+ 	fq->entries[idx].counter  = atomic64_read(&iovad->fq_flush_start_cnt);
+ 
+ 	spin_unlock_irqrestore(&fq->lock, flags);
+ 
+ 	if (atomic_cmpxchg(&iovad->fq_timer_on, 0, 1) == 0)
+ 		mod_timer(&iovad->fq_timer,
+ 			  jiffies + msecs_to_jiffies(IOVA_FQ_TIMEOUT));
+ 
+ 	put_cpu_ptr(iovad->fq);
+ }
+ EXPORT_SYMBOL_GPL(queue_iova);
+ 
++>>>>>>> 9a005a800ae8 (iommu/iova: Add flush timer)
  /**
   * put_iova_domain - destroys the iova doamin
   * @iovad: - iova domain in question.
diff --cc include/linux/iova.h
index f27bb2c62fca,d179b9bf7814..000000000000
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@@ -36,6 -37,35 +36,38 @@@ struct iova_rcache 
  	struct iova_cpu_rcache __percpu *cpu_rcaches;
  };
  
++<<<<<<< HEAD
++=======
+ struct iova_domain;
+ 
+ /* Call-Back from IOVA code into IOMMU drivers */
+ typedef void (* iova_flush_cb)(struct iova_domain *domain);
+ 
+ /* Destructor for per-entry data */
+ typedef void (* iova_entry_dtor)(unsigned long data);
+ 
+ /* Number of entries per Flush Queue */
+ #define IOVA_FQ_SIZE	256
+ 
+ /* Timeout (in ms) after which entries are flushed from the Flush-Queue */
+ #define IOVA_FQ_TIMEOUT	10
+ 
+ /* Flush Queue entry for defered flushing */
+ struct iova_fq_entry {
+ 	unsigned long iova_pfn;
+ 	unsigned long pages;
+ 	unsigned long data;
+ 	u64 counter; /* Flush counter when this entrie was added */
+ };
+ 
+ /* Per-CPU Flush Queue structure */
+ struct iova_fq {
+ 	struct iova_fq_entry entries[IOVA_FQ_SIZE];
+ 	unsigned head, tail;
+ 	spinlock_t lock;
+ };
+ 
++>>>>>>> 9a005a800ae8 (iommu/iova: Add flush timer)
  /* holds all the iova translations for a domain */
  struct iova_domain {
  	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
@@@ -45,6 -75,25 +77,28 @@@
  	unsigned long	start_pfn;	/* Lower limit for this domain */
  	unsigned long	dma_32bit_pfn;
  	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
++<<<<<<< HEAD
++=======
+ 
+ 	iova_flush_cb	flush_cb;	/* Call-Back function to flush IOMMU
+ 					   TLBs */
+ 
+ 	iova_entry_dtor entry_dtor;	/* IOMMU driver specific destructor for
+ 					   iova entry */
+ 
+ 	struct iova_fq __percpu *fq;	/* Flush Queue */
+ 
+ 	atomic64_t	fq_flush_start_cnt;	/* Number of TLB flushes that
+ 						   have been started */
+ 
+ 	atomic64_t	fq_flush_finish_cnt;	/* Number of TLB flushes that
+ 						   have been finished */
+ 
+ 	struct timer_list fq_timer;		/* Timer to regularily empty the
+ 						   flush-queues */
+ 	atomic_t fq_timer_on;			/* 1 when timer is active, 0
+ 						   when not */
++>>>>>>> 9a005a800ae8 (iommu/iova: Add flush timer)
  };
  
  static inline unsigned long iova_size(struct iova *iova)
* Unmerged path drivers/iommu/iova.c
* Unmerged path include/linux/iova.h
