cpu/hotplug: Expose SMT control init function

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jiri Kosina <jkosina@suse.cz>
commit 8e1b706b6e819bed215c0db16345568864660393
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/8e1b706b.failed

The L1TF mitigation will gain a commend line parameter which allows to set
a combination of hypervisor mitigation and SMT control.

Expose cpu_smt_disable() so the command line parser can tweak SMT settings.

[ tglx: Split out of larger patch and made it preserve an already existing
  	force off state ]

	Signed-off-by: Jiri Kosina <jkosina@suse.cz>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Jiri Kosina <jkosina@suse.cz>
	Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
Link: https://lkml.kernel.org/r/20180713142323.039715135@linutronix.de

(cherry picked from commit 8e1b706b6e819bed215c0db16345568864660393)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/cpu.h
#	kernel/cpu.c
diff --cc include/linux/cpu.h
index 1cc4c784a301,c9b23ad27b38..000000000000
--- a/include/linux/cpu.h
+++ b/include/linux/cpu.h
@@@ -290,6 -161,26 +290,24 @@@ void cpu_set_state_online(int cpu)
  #ifdef CONFIG_HOTPLUG_CPU
  bool cpu_wait_death(unsigned int cpu, int seconds);
  bool cpu_report_death(void);
 -void cpuhp_report_idle_dead(void);
 -#else
 -static inline void cpuhp_report_idle_dead(void) { }
  #endif /* #ifdef CONFIG_HOTPLUG_CPU */
  
++<<<<<<< HEAD
++=======
+ enum cpuhp_smt_control {
+ 	CPU_SMT_ENABLED,
+ 	CPU_SMT_DISABLED,
+ 	CPU_SMT_FORCE_DISABLED,
+ 	CPU_SMT_NOT_SUPPORTED,
+ };
+ 
+ #if defined(CONFIG_SMP) && defined(CONFIG_HOTPLUG_SMT)
+ extern enum cpuhp_smt_control cpu_smt_control;
+ extern void cpu_smt_disable(bool force);
+ #else
+ # define cpu_smt_control		(CPU_SMT_ENABLED)
+ static inline void cpu_smt_disable(bool force) { }
+ #endif
+ 
++>>>>>>> 8e1b706b6e81 (cpu/hotplug: Expose SMT control init function)
  #endif /* _LINUX_CPU_H_ */
diff --cc kernel/cpu.c
index 0d9e250d0ea0,8453e31f2d1a..000000000000
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@@ -216,22 -337,121 +216,121 @@@ void cpu_hotplug_disable(void
  void cpu_hotplug_enable(void)
  {
  	cpu_maps_update_begin();
 -	__cpu_hotplug_enable();
 +	cpu_hotplug_disabled = 0;
  	cpu_maps_update_done();
  }
 -EXPORT_SYMBOL_GPL(cpu_hotplug_enable);
 -#endif	/* CONFIG_HOTPLUG_CPU */
  
 -#ifdef CONFIG_HOTPLUG_SMT
 -enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
 -EXPORT_SYMBOL_GPL(cpu_smt_control);
 +#else /* #if CONFIG_HOTPLUG_CPU */
 +static void cpu_hotplug_begin(void) {}
 +static void cpu_hotplug_done(void) {}
 +#endif	/* #else #if CONFIG_HOTPLUG_CPU */
  
++<<<<<<< HEAD
 +/* Need to know about CPUs going up/down? */
 +int __ref register_cpu_notifier(struct notifier_block *nb)
 +{
++=======
+ void __init cpu_smt_disable(bool force)
+ {
+ 	if (cpu_smt_control == CPU_SMT_FORCE_DISABLED ||
+ 		cpu_smt_control == CPU_SMT_NOT_SUPPORTED)
+ 		return;
+ 
+ 	if (force) {
+ 		pr_info("SMT: Force disabled\n");
+ 		cpu_smt_control = CPU_SMT_FORCE_DISABLED;
+ 	} else {
+ 		cpu_smt_control = CPU_SMT_DISABLED;
+ 	}
+ }
+ 
+ static int __init smt_cmdline_disable(char *str)
+ {
+ 	cpu_smt_disable(str && !strcmp(str, "force"));
+ 	return 0;
+ }
+ early_param("nosmt", smt_cmdline_disable);
+ 
+ static inline bool cpu_smt_allowed(unsigned int cpu)
+ {
+ 	if (cpu_smt_control == CPU_SMT_ENABLED)
+ 		return true;
+ 
+ 	if (topology_is_primary_thread(cpu))
+ 		return true;
+ 
+ 	/*
+ 	 * On x86 it's required to boot all logical CPUs at least once so
+ 	 * that the init code can get a chance to set CR4.MCE on each
+ 	 * CPU. Otherwise, a broadacasted MCE observing CR4.MCE=0b on any
+ 	 * core will shutdown the machine.
+ 	 */
+ 	return !per_cpu(cpuhp_state, cpu).booted_once;
+ }
+ #else
+ static inline bool cpu_smt_allowed(unsigned int cpu) { return true; }
+ #endif
+ 
+ static inline enum cpuhp_state
+ cpuhp_set_state(struct cpuhp_cpu_state *st, enum cpuhp_state target)
+ {
+ 	enum cpuhp_state prev_state = st->state;
+ 
+ 	st->rollback = false;
+ 	st->last = NULL;
+ 
+ 	st->target = target;
+ 	st->single = false;
+ 	st->bringup = st->state < target;
+ 
+ 	return prev_state;
+ }
+ 
+ static inline void
+ cpuhp_reset_state(struct cpuhp_cpu_state *st, enum cpuhp_state prev_state)
+ {
+ 	st->rollback = true;
+ 
+ 	/*
+ 	 * If we have st->last we need to undo partial multi_instance of this
+ 	 * state first. Otherwise start undo at the previous state.
+ 	 */
+ 	if (!st->last) {
+ 		if (st->bringup)
+ 			st->state--;
+ 		else
+ 			st->state++;
+ 	}
+ 
+ 	st->target = prev_state;
+ 	st->bringup = !st->bringup;
+ }
+ 
+ /* Regular hotplug invocation of the AP hotplug thread */
+ static void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)
+ {
+ 	if (!st->single && st->state == st->target)
+ 		return;
+ 
+ 	st->result = 0;
+ 	/*
+ 	 * Make sure the above stores are visible before should_run becomes
+ 	 * true. Paired with the mb() above in cpuhp_thread_fun()
+ 	 */
+ 	smp_mb();
+ 	st->should_run = true;
+ 	wake_up_process(st->thread);
+ 	wait_for_ap_thread(st, st->bringup);
+ }
+ 
+ static int cpuhp_kick_ap(struct cpuhp_cpu_state *st, enum cpuhp_state target)
+ {
+ 	enum cpuhp_state prev_state;
++>>>>>>> 8e1b706b6e81 (cpu/hotplug: Expose SMT control init function)
  	int ret;
 -
 -	prev_state = cpuhp_set_state(st, target);
 -	__cpuhp_kick_ap(st);
 -	if ((ret = st->result)) {
 -		cpuhp_reset_state(st, prev_state);
 -		__cpuhp_kick_ap(st);
 -	}
 -
 +	cpu_maps_update_begin();
 +	ret = raw_notifier_chain_register(&cpu_chain, nb);
 +	cpu_maps_update_done();
  	return ret;
  }
  
* Unmerged path include/linux/cpu.h
* Unmerged path kernel/cpu.c
