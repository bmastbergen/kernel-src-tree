x86/KVM/VMX: Separate the VMX AUTOLOAD guest/host number accounting

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] kvm/vmx: separate the vmx autoload guest/host number accounting (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 96.92%
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 3190709335dd31fe1aeeebfe4ffb6c7624ef971f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/31907093.failed

This allows to load a different number of MSRs depending on the context:
VMEXIT or VMENTER.

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 3190709335dd31fe1aeeebfe4ffb6c7624ef971f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index e2f48f8aba96,bde1602a94f5..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -1876,18 -2455,21 +1876,36 @@@ static void clear_atomic_switch_msr(str
  		}
  		break;
  	}
++<<<<<<< HEAD
 +
 +	for (i = 0; i < m->nr; ++i)
 +		if (m->guest[i].index == msr)
 +			break;
 +
 +	if (i == m->nr)
 +		return;
 +	--m->nr;
 +	m->guest[i] = m->guest[m->nr];
 +	m->host[i] = m->host[m->nr];
 +	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);
 +	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);
++=======
+ 	i = find_msr(&m->guest, msr);
+ 	if (i < 0)
+ 		goto skip_guest;
+ 	--m->guest.nr;
+ 	m->guest.val[i] = m->guest.val[m->guest.nr];
+ 	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);
+ 
+ skip_guest:
+ 	i = find_msr(&m->host, msr);
+ 	if (i < 0)
+ 		return;
+ 
+ 	--m->host.nr;
+ 	m->host.val[i] = m->host.val[m->host.nr];
+ 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
++>>>>>>> 3190709335dd (x86/KVM/VMX: Separate the VMX AUTOLOAD guest/host number accounting)
  }
  
  static void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
@@@ -1904,7 -2486,7 +1922,11 @@@
  static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
  				  u64 guest_val, u64 host_val)
  {
++<<<<<<< HEAD
 +	unsigned i;
++=======
+ 	int i, j;
++>>>>>>> 3190709335dd (x86/KVM/VMX: Separate the VMX AUTOLOAD guest/host number accounting)
  	struct msr_autoload *m = &vmx->msr_autoload;
  
  	switch (msr) {
@@@ -1939,37 -2521,25 +1961,59 @@@
  		wrmsrl(MSR_IA32_PEBS_ENABLE, 0);
  	}
  
++<<<<<<< HEAD
 +	for (i = 0; i < m->nr; ++i)
 +		if (m->guest[i].index == msr)
 +			break;
 +
 +	if (i == NR_AUTOLOAD_MSRS) {
 +		printk_once(KERN_WARNING "Not enough msr switch entries. "
 +				"Can't add msr %x\n", msr);
 +		return;
 +	} else if (i == m->nr) {
 +		++m->nr;
 +		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);
 +		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);
 +	}
 +
 +	m->guest[i].index = msr;
 +	m->guest[i].value = guest_val;
 +	m->host[i].index = msr;
 +	m->host[i].value = host_val;
 +}
 +
 +static void reload_tss(void)
 +{
 +	/*
 +	 * VT restores TR but not its size.  Useless.
 +	 */
 +	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
 +	struct desc_struct *descs;
 +
 +	descs = (void *)gdt->address;
 +	descs[GDT_ENTRY_TSS].type = 9; /* available TSS */
 +	load_TR_desc();
++=======
+ 	i = find_msr(&m->guest, msr);
+ 	j = find_msr(&m->host, msr);
+ 	if (i == NR_AUTOLOAD_MSRS || j == NR_AUTOLOAD_MSRS) {
+ 		printk_once(KERN_WARNING "Not enough msr switch entries. "
+ 				"Can't add msr %x\n", msr);
+ 		return;
+ 	}
+ 	if (i < 0) {
+ 		i = m->guest.nr++;
+ 		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);
+ 	}
+ 	if (j < 0) {
+ 		j = m->host.nr++;
+ 		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
+ 	}
+ 	m->guest.val[i].index = msr;
+ 	m->guest.val[i].value = guest_val;
+ 	m->host.val[j].index = msr;
+ 	m->host.val[j].value = host_val;
++>>>>>>> 3190709335dd (x86/KVM/VMX: Separate the VMX AUTOLOAD guest/host number accounting)
  }
  
  static bool update_transition_efer(struct vcpu_vmx *vmx, int efer_offset)
* Unmerged path arch/x86/kvm/vmx.c
