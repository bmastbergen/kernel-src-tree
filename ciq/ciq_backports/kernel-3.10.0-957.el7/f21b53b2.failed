x86/speculation: Make "seccomp" the default mode for Speculative Store Bypass

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] speculation: Make "seccomp" the default mode for Speculative Store Bypass (Waiman Long) [1584569] {CVE-2018-3639}
Rebuild_FUZZ: 97.33%
commit-author Kees Cook <keescook@chromium.org>
commit f21b53b20c754021935ea43364dbf53778eeba32
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/f21b53b2.failed

Unless explicitly opted out of, anything running under seccomp will have
SSB mitigations enabled. Choosing the "prctl" mode will disable this.

[ tglx: Adjusted it to the new arch_seccomp_spec_mitigate() mechanism ]

	Signed-off-by: Kees Cook <keescook@chromium.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit f21b53b20c754021935ea43364dbf53778eeba32)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/kernel-parameters.txt
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
diff --cc Documentation/kernel-parameters.txt
index a0f1f1428af0,f2040d46f095..000000000000
--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@@ -3222,6 -4028,48 +3222,51 @@@ bytes respectively. Such letter suffixe
  			Not specifying this option is equivalent to
  			spectre_v2=auto.
  
++<<<<<<< HEAD:Documentation/kernel-parameters.txt
++=======
+ 	spec_store_bypass_disable=
+ 			[HW] Control Speculative Store Bypass (SSB) Disable mitigation
+ 			(Speculative Store Bypass vulnerability)
+ 
+ 			Certain CPUs are vulnerable to an exploit against a
+ 			a common industry wide performance optimization known
+ 			as "Speculative Store Bypass" in which recent stores
+ 			to the same memory location may not be observed by
+ 			later loads during speculative execution. The idea
+ 			is that such stores are unlikely and that they can
+ 			be detected prior to instruction retirement at the
+ 			end of a particular speculation execution window.
+ 
+ 			In vulnerable processors, the speculatively forwarded
+ 			store can be used in a cache side channel attack, for
+ 			example to read memory to which the attacker does not
+ 			directly have access (e.g. inside sandboxed code).
+ 
+ 			This parameter controls whether the Speculative Store
+ 			Bypass optimization is used.
+ 
+ 			on      - Unconditionally disable Speculative Store Bypass
+ 			off     - Unconditionally enable Speculative Store Bypass
+ 			auto    - Kernel detects whether the CPU model contains an
+ 				  implementation of Speculative Store Bypass and
+ 				  picks the most appropriate mitigation. If the
+ 				  CPU is not vulnerable, "off" is selected. If the
+ 				  CPU is vulnerable the default mitigation is
+ 				  architecture and Kconfig dependent. See below.
+ 			prctl   - Control Speculative Store Bypass per thread
+ 				  via prctl. Speculative Store Bypass is enabled
+ 				  for a process by default. The state of the control
+ 				  is inherited on fork.
+ 			seccomp - Same as "prctl" above, but all seccomp threads
+ 				  will disable SSB unless they explicitly opt out.
+ 
+ 			Not specifying this option is equivalent to
+ 			spec_store_bypass_disable=auto.
+ 
+ 			Default mitigations:
+ 			X86:	If CONFIG_SECCOMP=y "seccomp", otherwise "prctl"
+ 
++>>>>>>> f21b53b20c75 (x86/speculation: Make "seccomp" the default mode for Speculative Store Bypass):Documentation/admin-guide/kernel-parameters.txt
  	spia_io_base=	[HW,MTD]
  	spia_fio_base=
  	spia_pedr=
diff --cc arch/x86/include/asm/nospec-branch.h
index d5b6abbaaa24,328ea3cb769f..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -169,27 -211,33 +169,40 @@@
  enum spectre_v2_mitigation {
  	SPECTRE_V2_NONE,
  	SPECTRE_V2_RETPOLINE_MINIMAL,
 -	SPECTRE_V2_RETPOLINE_MINIMAL_AMD,
 -	SPECTRE_V2_RETPOLINE_GENERIC,
 -	SPECTRE_V2_RETPOLINE_AMD,
 +	SPECTRE_V2_RETPOLINE_NO_IBPB,
 +	SPECTRE_V2_RETPOLINE_SKYLAKE,
 +	SPECTRE_V2_RETPOLINE_UNSAFE_MODULE,
 +	SPECTRE_V2_RETPOLINE,
 +	SPECTRE_V2_RETPOLINE_IBRS_USER,
  	SPECTRE_V2_IBRS,
 +	SPECTRE_V2_IBRS_ALWAYS,
 +	SPECTRE_V2_IBP_DISABLED,
  };
  
 -/*
 - * The Intel specification for the SPEC_CTRL MSR requires that we
 - * preserve any already set reserved bits at boot time (e.g. for
 - * future additions that this kernel is not currently aware of).
 - * We then set any additional mitigation bits that we want
 - * ourselves and always use this as the base for SPEC_CTRL.
 - * We also use this when handling guest entry/exit as below.
 - */
 -extern void x86_spec_ctrl_set(u64);
 -extern u64 x86_spec_ctrl_get_default(void);
 +void __spectre_v2_select_mitigation(void);
 +void spectre_v2_print_mitigation(void);
  
++<<<<<<< HEAD
 +static inline bool retp_compiler(void)
 +{
 +#ifdef RETPOLINE
 +	return true;
 +#else
 +	return false;
 +#endif
 +}
++=======
+ /* The Speculative Store Bypass disable variants */
+ enum ssb_mitigation {
+ 	SPEC_STORE_BYPASS_NONE,
+ 	SPEC_STORE_BYPASS_DISABLE,
+ 	SPEC_STORE_BYPASS_PRCTL,
+ 	SPEC_STORE_BYPASS_SECCOMP,
+ };
+ 
+ extern char __indirect_thunk_start[];
+ extern char __indirect_thunk_end[];
++>>>>>>> f21b53b20c75 (x86/speculation: Make "seccomp" the default mode for Speculative Store Bypass)
  
  /*
   * On VMEXIT we must ensure that no RSB predictions learned in the guest
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,563d8e54c863..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -199,15 -406,269 +199,249 @@@ static void __init spectre_v2_select_mi
  }
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
+ 
+ static enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;
+ 
+ /* The kernel command line selection */
+ enum ssb_mitigation_cmd {
+ 	SPEC_STORE_BYPASS_CMD_NONE,
+ 	SPEC_STORE_BYPASS_CMD_AUTO,
+ 	SPEC_STORE_BYPASS_CMD_ON,
+ 	SPEC_STORE_BYPASS_CMD_PRCTL,
+ 	SPEC_STORE_BYPASS_CMD_SECCOMP,
+ };
+ 
+ static const char *ssb_strings[] = {
+ 	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
+ 	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled",
+ 	[SPEC_STORE_BYPASS_PRCTL]	= "Mitigation: Speculative Store Bypass disabled via prctl",
+ 	[SPEC_STORE_BYPASS_SECCOMP]	= "Mitigation: Speculative Store Bypass disabled via prctl and seccomp",
+ };
+ 
+ static const struct {
+ 	const char *option;
+ 	enum ssb_mitigation_cmd cmd;
+ } ssb_mitigation_options[] = {
+ 	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO },    /* Platform decides */
+ 	{ "on",		SPEC_STORE_BYPASS_CMD_ON },      /* Disable Speculative Store Bypass */
+ 	{ "off",	SPEC_STORE_BYPASS_CMD_NONE },    /* Don't touch Speculative Store Bypass */
+ 	{ "prctl",	SPEC_STORE_BYPASS_CMD_PRCTL },   /* Disable Speculative Store Bypass via prctl */
+ 	{ "seccomp",	SPEC_STORE_BYPASS_CMD_SECCOMP }, /* Disable Speculative Store Bypass via prctl and seccomp */
+ };
+ 
+ static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
+ {
+ 	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
+ 		return SPEC_STORE_BYPASS_CMD_NONE;
+ 	} else {
+ 		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
+ 					  arg, sizeof(arg));
+ 		if (ret < 0)
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
+ 			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
+ 				continue;
+ 
+ 			cmd = ssb_mitigation_options[i].cmd;
+ 			break;
+ 		}
+ 
+ 		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
+ 			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 		}
+ 	}
+ 
+ 	return cmd;
+ }
+ 
+ static enum ssb_mitigation_cmd __init __ssb_select_mitigation(void)
+ {
+ 	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
+ 	enum ssb_mitigation_cmd cmd;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_RDS))
+ 		return mode;
+ 
+ 	cmd = ssb_parse_cmdline();
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
+ 	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
+ 	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
+ 		return mode;
+ 
+ 	switch (cmd) {
+ 	case SPEC_STORE_BYPASS_CMD_AUTO:
+ 	case SPEC_STORE_BYPASS_CMD_SECCOMP:
+ 		/*
+ 		 * Choose prctl+seccomp as the default mode if seccomp is
+ 		 * enabled.
+ 		 */
+ 		if (IS_ENABLED(CONFIG_SECCOMP))
+ 			mode = SPEC_STORE_BYPASS_SECCOMP;
+ 		else
+ 			mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_ON:
+ 		mode = SPEC_STORE_BYPASS_DISABLE;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_PRCTL:
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_NONE:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * We have three CPU feature flags that are in play here:
+ 	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.
+ 	 *  - X86_FEATURE_RDS - CPU is able to turn off speculative store bypass
+ 	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation
+ 	 */
+ 	if (mode == SPEC_STORE_BYPASS_DISABLE) {
+ 		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
+ 		/*
+ 		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD uses
+ 		 * a completely different MSR and bit dependent on family.
+ 		 */
+ 		switch (boot_cpu_data.x86_vendor) {
+ 		case X86_VENDOR_INTEL:
+ 			x86_spec_ctrl_base |= SPEC_CTRL_RDS;
+ 			x86_spec_ctrl_mask &= ~SPEC_CTRL_RDS;
+ 			x86_spec_ctrl_set(SPEC_CTRL_RDS);
+ 			break;
+ 		case X86_VENDOR_AMD:
+ 			x86_amd_rds_enable();
+ 			break;
+ 		}
+ 	}
+ 
+ 	return mode;
+ }
+ 
+ static void ssb_select_mitigation()
+ {
+ 	ssb_mode = __ssb_select_mitigation();
+ 
+ 	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		pr_info("%s\n", ssb_strings[ssb_mode]);
+ }
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "Speculation prctl: " fmt
+ 
+ static int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)
+ {
+ 	bool update;
+ 
+ 	if (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&
+ 	    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)
+ 		return -ENXIO;
+ 
+ 	switch (ctrl) {
+ 	case PR_SPEC_ENABLE:
+ 		/* If speculation is force disabled, enable is not allowed */
+ 		if (task_spec_ssb_force_disable(task))
+ 			return -EPERM;
+ 		task_clear_spec_ssb_disable(task);
+ 		update = test_and_clear_tsk_thread_flag(task, TIF_RDS);
+ 		break;
+ 	case PR_SPEC_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_RDS);
+ 		break;
+ 	case PR_SPEC_FORCE_DISABLE:
+ 		task_set_spec_ssb_disable(task);
+ 		task_set_spec_ssb_force_disable(task);
+ 		update = !test_and_set_tsk_thread_flag(task, TIF_RDS);
+ 		break;
+ 	default:
+ 		return -ERANGE;
+ 	}
+ 
+ 	/*
+ 	 * If being set on non-current task, delay setting the CPU
+ 	 * mitigation until it is next scheduled.
+ 	 */
+ 	if (task == current && update)
+ 		speculative_store_bypass_update();
+ 
+ 	return 0;
+ }
+ 
+ int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
+ 			     unsigned long ctrl)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_set(task, ctrl);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ #ifdef CONFIG_SECCOMP
+ void arch_seccomp_spec_mitigate(struct task_struct *task)
+ {
+ 	if (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)
+ 		ssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);
+ }
+ #endif
+ 
+ static int ssb_prctl_get(struct task_struct *task)
+ {
+ 	switch (ssb_mode) {
+ 	case SPEC_STORE_BYPASS_DISABLE:
+ 		return PR_SPEC_DISABLE;
+ 	case SPEC_STORE_BYPASS_SECCOMP:
+ 	case SPEC_STORE_BYPASS_PRCTL:
+ 		if (task_spec_ssb_force_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;
+ 		if (task_spec_ssb_disable(task))
+ 			return PR_SPEC_PRCTL | PR_SPEC_DISABLE;
+ 		return PR_SPEC_PRCTL | PR_SPEC_ENABLE;
+ 	default:
+ 		if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 			return PR_SPEC_ENABLE;
+ 		return PR_SPEC_NOT_AFFECTED;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_get(task);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ void x86_spec_ctrl_setup_ap(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_IBRS))
+ 		x86_spec_ctrl_set(x86_spec_ctrl_base & ~x86_spec_ctrl_mask);
+ 
+ 	if (ssb_mode == SPEC_STORE_BYPASS_DISABLE)
+ 		x86_amd_rds_enable();
+ }
++>>>>>>> f21b53b20c75 (x86/speculation: Make "seccomp" the default mode for Speculative Store Bypass)
  
  #ifdef CONFIG_SYSFS
 -
 -ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
 -
 -	switch (bug) {
 -	case X86_BUG_CPU_MELTDOWN:
 -		if (boot_cpu_has(X86_FEATURE_PTI))
 -			return sprintf(buf, "Mitigation: PTI\n");
 -
 -		break;
 -
 -	case X86_BUG_SPECTRE_V1:
 -		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 -
 -	case X86_BUG_SPECTRE_V2:
 -		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
 -			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
 -			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 -			       spectre_v2_module_string());
 -
 -	case X86_BUG_SPEC_STORE_BYPASS:
 -		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
 -
 -	default:
 -		break;
 -	}
 -
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
  	return sprintf(buf, "Vulnerable\n");
  }
  
* Unmerged path Documentation/kernel-parameters.txt
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
