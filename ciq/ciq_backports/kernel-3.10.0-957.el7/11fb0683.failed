x86/speculation: Add virtualized speculative store bypass disable support

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] speculation: Add virtualized speculative store bypass disable support (Waiman Long) [1584569] {CVE-2018-3639}
Rebuild_FUZZ: 97.18%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit 11fb0683493b2da112cd64c9dada221b52463bf7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/11fb0683.failed

Some AMD processors only support a non-architectural means of enabling
speculative store bypass disable (SSBD).  To allow a simplified view of
this to a guest, an architectural definition has been created through a new
CPUID bit, 0x80000008_EBX[25], and a new MSR, 0xc001011f.  With this, a
hypervisor can virtualize the existence of this definition and provide an
architectural method for using SSBD to a guest.

Add the new CPUID feature, the new MSR and update the existing SSBD
support to use this MSR when present.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit 11fb0683493b2da112cd64c9dada221b52463bf7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/msr-index.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/process.c
diff --cc arch/x86/include/asm/cpufeatures.h
index 7a3e9c71ed7d,fb00a2fca990..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -190,126 -190,148 +190,136 @@@
   *
   * Reuse free bits when adding new feature flags!
   */
 -#define X86_FEATURE_RING3MWAIT		( 7*32+ 0) /* Ring 3 MONITOR/MWAIT instructions */
 -#define X86_FEATURE_CPUID_FAULT		( 7*32+ 1) /* Intel CPUID faulting */
 -#define X86_FEATURE_CPB			( 7*32+ 2) /* AMD Core Performance Boost */
 -#define X86_FEATURE_EPB			( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 -#define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
 -#define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
 -#define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
 -#define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID && CR4.PCIDE=1 */
 -#define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
 -#define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 -#define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
 -#define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
 -#define X86_FEATURE_RETPOLINE		( 7*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* "" AMD Retpoline mitigation for Spectre variant 2 */
 -#define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 -#define X86_FEATURE_CDP_L2		( 7*32+15) /* Code and Data Prioritization L2 */
 -#define X86_FEATURE_MSR_SPEC_CTRL	( 7*32+16) /* "" MSR SPEC_CTRL is implemented */
 -#define X86_FEATURE_SSBD		( 7*32+17) /* Speculative Store Bypass Disable */
 -#define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
 -#define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* "" Fill RSB on context switches */
 -#define X86_FEATURE_SEV			( 7*32+20) /* AMD Secure Encrypted Virtualization */
 -#define X86_FEATURE_USE_IBPB		( 7*32+21) /* "" Indirect Branch Prediction Barrier enabled */
 -#define X86_FEATURE_USE_IBRS_FW		( 7*32+22) /* "" Use IBRS during runtime firmware calls */
 -#define X86_FEATURE_SPEC_STORE_BYPASS_DISABLE	( 7*32+23) /* "" Disable Speculative Store Bypass. */
 -#define X86_FEATURE_LS_CFG_SSBD		( 7*32+24)  /* "" AMD SSBD implementation via LS_CFG MSR */
 -#define X86_FEATURE_IBRS		( 7*32+25) /* Indirect Branch Restricted Speculation */
 -#define X86_FEATURE_IBPB		( 7*32+26) /* Indirect Branch Prediction Barrier */
 -#define X86_FEATURE_STIBP		( 7*32+27) /* Single Thread Indirect Branch Predictors */
 -#define X86_FEATURE_ZEN			( 7*32+28) /* "" CPU is AMD family 0x17 (Zen) */
  
 -/* Virtualization flags: Linux defined, word 8 */
 -#define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
 -#define X86_FEATURE_VNMI		( 8*32+ 1) /* Intel Virtual NMI */
 -#define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 -#define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 -#define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_RING3MWAIT	(7*32+ 0) /* Ring 3 MONITOR/MWAIT */
 +#define X86_FEATURE_CPB		(7*32+ 2) /* AMD Core Performance Boost */
 +#define X86_FEATURE_EPB		(7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
 +#define X86_FEATURE_CAT_L3	(7*32+ 4) /* Cache Allocation Technology L3 */
 +#define X86_FEATURE_CAT_L2	(7*32+ 5) /* Cache Allocation Technology L2 */
 +#define X86_FEATURE_CDP_L3	(7*32+ 6) /* Code and Data Prioritization L3 */
  
 -#define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 -#define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
 +#define X86_FEATURE_HW_PSTATE	(7*32+ 8) /* AMD HW-PState */
 +#define X86_FEATURE_PROC_FEEDBACK (7*32+ 9) /* AMD ProcFeedbackInterface */
 +#define X86_FEATURE_SME		( 7*32+10) /* AMD Secure Memory Encryption */
 +#define X86_FEATURE_RETPOLINE_AMD (7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */
 +#define X86_FEATURE_INTEL_PPIN	( 7*32+14) /* Intel Processor Inventory Number */
 +#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */
  
 +#define X86_FEATURE_MBA		( 7*32+18) /* Memory Bandwidth Allocation */
 +#define X86_FEATURE_IBP_DISABLE ( 7*32+21) /* Old AMD Indirect Branch Predictor Disable */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (EBX), word 9 */
 -#define X86_FEATURE_FSGSBASE		( 9*32+ 0) /* RDFSBASE, WRFSBASE, RDGSBASE, WRGSBASE instructions*/
 -#define X86_FEATURE_TSC_ADJUST		( 9*32+ 1) /* TSC adjustment MSR 0x3B */
 -#define X86_FEATURE_BMI1		( 9*32+ 3) /* 1st group bit manipulation extensions */
 -#define X86_FEATURE_HLE			( 9*32+ 4) /* Hardware Lock Elision */
 -#define X86_FEATURE_AVX2		( 9*32+ 5) /* AVX2 instructions */
 -#define X86_FEATURE_SMEP		( 9*32+ 7) /* Supervisor Mode Execution Protection */
 -#define X86_FEATURE_BMI2		( 9*32+ 8) /* 2nd group bit manipulation extensions */
 -#define X86_FEATURE_ERMS		( 9*32+ 9) /* Enhanced REP MOVSB/STOSB instructions */
 -#define X86_FEATURE_INVPCID		( 9*32+10) /* Invalidate Processor Context ID */
 -#define X86_FEATURE_RTM			( 9*32+11) /* Restricted Transactional Memory */
 -#define X86_FEATURE_CQM			( 9*32+12) /* Cache QoS Monitoring */
 -#define X86_FEATURE_MPX			( 9*32+14) /* Memory Protection Extension */
 -#define X86_FEATURE_RDT_A		( 9*32+15) /* Resource Director Technology Allocation */
 -#define X86_FEATURE_AVX512F		( 9*32+16) /* AVX-512 Foundation */
 -#define X86_FEATURE_AVX512DQ		( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 -#define X86_FEATURE_RDSEED		( 9*32+18) /* RDSEED instruction */
 -#define X86_FEATURE_ADX			( 9*32+19) /* ADCX and ADOX instructions */
 -#define X86_FEATURE_SMAP		( 9*32+20) /* Supervisor Mode Access Prevention */
 -#define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 -#define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 -#define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
 -#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */
 -#define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 -#define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 -#define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
 -#define X86_FEATURE_SHA_NI		( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 -#define X86_FEATURE_AVX512BW		( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 -#define X86_FEATURE_AVX512VL		( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
 +/* Virtualization flags: Linux defined, word 8 */
 +#define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
 +#define X86_FEATURE_VNMI        (8*32+ 1) /* Intel Virtual NMI */
 +#define X86_FEATURE_FLEXPRIORITY (8*32+ 2) /* Intel FlexPriority */
 +#define X86_FEATURE_EPT         (8*32+ 3) /* Intel Extended Page Table */
 +#define X86_FEATURE_VPID        (8*32+ 4) /* Intel Virtual Processor ID */
 +#define X86_FEATURE_VMMCALL     (8*32+15) /* Prefer vmmcall to vmcall */
 +
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
 +#define X86_FEATURE_FSGSBASE	(9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/
 +#define X86_FEATURE_TSC_ADJUST	(9*32+ 1) /* TSC adjustment MSR 0x3b */
 +#define X86_FEATURE_BMI1	(9*32+ 3) /* 1st group bit manipulation extensions */
 +#define X86_FEATURE_HLE		(9*32+ 4) /* Hardware Lock Elision */
 +#define X86_FEATURE_AVX2	(9*32+ 5) /* AVX2 instructions */
 +#define X86_FEATURE_SMEP	(9*32+ 7) /* Supervisor Mode Execution Protection */
 +#define X86_FEATURE_BMI2	(9*32+ 8) /* 2nd group bit manipulation extensions */
 +#define X86_FEATURE_ERMS	(9*32+ 9) /* Enhanced REP MOVSB/STOSB */
 +#define X86_FEATURE_INVPCID	(9*32+10) /* Invalidate Processor Context ID */
 +#define X86_FEATURE_RTM		(9*32+11) /* Restricted Transactional Memory */
 +#define X86_FEATURE_CQM		(9*32+12) /* Cache QoS Monitoring */
 +#define X86_FEATURE_MPX		(9*32+14) /* Memory Protection Extension */
 +#define X86_FEATURE_RDT_A	(9*32+15) /* Resource Director Technology Allocation */
 +#define X86_FEATURE_AVX512F	(9*32+16) /* AVX-512 Foundation */
 +#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */
 +#define X86_FEATURE_RDSEED	(9*32+18) /* The RDSEED instruction */
 +#define X86_FEATURE_ADX		(9*32+19) /* The ADCX and ADOX instructions */
 +#define X86_FEATURE_SMAP	(9*32+20) /* Supervisor Mode Access Prevention */
 +#define X86_FEATURE_AVX512IFMA	( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 +#define X86_FEATURE_CLFLUSHOPT	(9*32+23) /* CLFLUSHOPT instruction */
 +#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */
 +#define X86_FEATURE_AVX512PF	(9*32+26) /* AVX-512 Prefetch */
 +#define X86_FEATURE_AVX512ER	(9*32+27) /* AVX-512 Exponential and Reciprocal */
 +#define X86_FEATURE_AVX512CD	(9*32+28) /* AVX-512 Conflict Detection */
 +#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */
 +#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */
 +#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */
  
 -/* Extended state features, CPUID level 0x0000000d:1 (EAX), word 10 */
 -#define X86_FEATURE_XSAVEOPT		(10*32+ 0) /* XSAVEOPT instruction */
 -#define X86_FEATURE_XSAVEC		(10*32+ 1) /* XSAVEC instruction */
 -#define X86_FEATURE_XGETBV1		(10*32+ 2) /* XGETBV with ECX = 1 instruction */
 -#define X86_FEATURE_XSAVES		(10*32+ 3) /* XSAVES/XRSTORS instructions */
 +/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */
 +#define X86_FEATURE_XSAVEOPT   (10*32+ 0) /* XSAVEOPT */
 +#define X86_FEATURE_XSAVEC     (10*32+ 1) /* XSAVEC */
 +#define X86_FEATURE_XGETBV1    (10*32+ 2) /* XGETBV with ECX = 1 */
 +#define X86_FEATURE_XSAVES     (10*32+ 3) /* XSAVES/XRSTORS */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (EDX), word 11 */
 -#define X86_FEATURE_CQM_LLC		(11*32+ 1) /* LLC QoS if 1 */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */
 +#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */
  
 -/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (EDX), word 12 */
 -#define X86_FEATURE_CQM_OCCUP_LLC	(12*32+ 0) /* LLC occupancy monitoring */
 -#define X86_FEATURE_CQM_MBM_TOTAL	(12*32+ 1) /* LLC Total MBM monitoring */
 -#define X86_FEATURE_CQM_MBM_LOCAL	(12*32+ 2) /* LLC Local MBM monitoring */
 +/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */
 +#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */
 +#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */
 +#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */
  
  /* AMD-defined CPU features, CPUID level 0x80000008 (EBX), word 13 */
++<<<<<<< HEAD
 +#define X86_FEATURE_CLZERO              (13*32+ 0) /* CLZERO instruction */
 +#define X86_FEATURE_IRPERF              (13*32+ 1) /* Instructions Retired Count */
 +#define X86_FEATURE_XSAVEERPTR          (13*32+ 2) /* Always save/restore FP error pointers */
 +#define X86_FEATURE_IBPB		(13*32+12) /* Indirect Branch Prediction Barrier */
 +#define X86_FEATURE_IBRS		(13*32+14) /* Indirect Branch Restricted Speculation */
 +#define X86_FEATURE_STIBP		(13*32+15) /* Single Thread Indirect Branch Predictors */
++=======
+ #define X86_FEATURE_CLZERO		(13*32+ 0) /* CLZERO instruction */
+ #define X86_FEATURE_IRPERF		(13*32+ 1) /* Instructions Retired Count */
+ #define X86_FEATURE_XSAVEERPTR		(13*32+ 2) /* Always save/restore FP error pointers */
+ #define X86_FEATURE_AMD_IBPB		(13*32+12) /* "" Indirect Branch Prediction Barrier */
+ #define X86_FEATURE_AMD_IBRS		(13*32+14) /* "" Indirect Branch Restricted Speculation */
+ #define X86_FEATURE_AMD_STIBP		(13*32+15) /* "" Single Thread Indirect Branch Predictors */
+ #define X86_FEATURE_VIRT_SSBD		(13*32+25) /* Virtualized Speculative Store Bypass Disable */
++>>>>>>> 11fb0683493b (x86/speculation: Add virtualized speculative store bypass disable support)
  
 -/* Thermal and Power Management Leaf, CPUID level 0x00000006 (EAX), word 14 */
 -#define X86_FEATURE_DTHERM		(14*32+ 0) /* Digital Thermal Sensor */
 -#define X86_FEATURE_IDA			(14*32+ 1) /* Intel Dynamic Acceleration */
 -#define X86_FEATURE_ARAT		(14*32+ 2) /* Always Running APIC Timer */
 -#define X86_FEATURE_PLN			(14*32+ 4) /* Intel Power Limit Notification */
 -#define X86_FEATURE_PTS			(14*32+ 6) /* Intel Package Thermal Status */
 -#define X86_FEATURE_HWP			(14*32+ 7) /* Intel Hardware P-states */
 -#define X86_FEATURE_HWP_NOTIFY		(14*32+ 8) /* HWP Notification */
 -#define X86_FEATURE_HWP_ACT_WINDOW	(14*32+ 9) /* HWP Activity Window */
 -#define X86_FEATURE_HWP_EPP		(14*32+10) /* HWP Energy Perf. Preference */
 -#define X86_FEATURE_HWP_PKG_REQ		(14*32+11) /* HWP Package Level Request */
 +/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */
 +#define X86_FEATURE_OVERFLOW_RECOV (17*32+0) /* MCA overflow recovery support */
 +#define X86_FEATURE_SUCCOR	(17*32+1) /* Uncorrectable error containment and recovery */
 +#define X86_FEATURE_SMCA	(17*32+3) /* Scalable MCA */
  
 -/* AMD SVM Feature Identification, CPUID level 0x8000000a (EDX), word 15 */
 -#define X86_FEATURE_NPT			(15*32+ 0) /* Nested Page Table support */
 -#define X86_FEATURE_LBRV		(15*32+ 1) /* LBR Virtualization support */
 -#define X86_FEATURE_SVML		(15*32+ 2) /* "svm_lock" SVM locking MSR */
 -#define X86_FEATURE_NRIPS		(15*32+ 3) /* "nrip_save" SVM next_rip save */
 -#define X86_FEATURE_TSCRATEMSR		(15*32+ 4) /* "tsc_scale" TSC scaling support */
 -#define X86_FEATURE_VMCBCLEAN		(15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 -#define X86_FEATURE_FLUSHBYASID		(15*32+ 6) /* flush-by-ASID support */
 -#define X86_FEATURE_DECODEASSISTS	(15*32+ 7) /* Decode Assists support */
 -#define X86_FEATURE_PAUSEFILTER		(15*32+10) /* filtered pause intercept */
 -#define X86_FEATURE_PFTHRESHOLD		(15*32+12) /* pause filter threshold */
 -#define X86_FEATURE_AVIC		(15*32+13) /* Virtual Interrupt Controller */
 -#define X86_FEATURE_V_VMSAVE_VMLOAD	(15*32+15) /* Virtual VMSAVE VMLOAD */
 -#define X86_FEATURE_VGIF		(15*32+16) /* Virtual GIF */
 +/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */
 +#define X86_FEATURE_DTHERM	(14*32+ 0) /* Digital Thermal Sensor */
 +#define X86_FEATURE_IDA		(14*32+ 1) /* Intel Dynamic Acceleration */
 +#define X86_FEATURE_ARAT	(14*32+ 2) /* Always Running APIC Timer */
 +#define X86_FEATURE_PLN		(14*32+ 4) /* Intel Power Limit Notification */
 +#define X86_FEATURE_PTS		(14*32+ 6) /* Intel Package Thermal Status */
 +#define X86_FEATURE_HWP		(14*32+ 7) /* Intel Hardware P-states */
 +#define X86_FEATURE_HWP_NOTIFY	(14*32+ 8) /* HWP Notification */
 +#define X86_FEATURE_HWP_ACT_WINDOW (14*32+ 9) /* HWP Activity Window */
 +#define X86_FEATURE_HWP_EPP	(14*32+10) /* HWP Energy Perf. Preference */
 +#define X86_FEATURE_HWP_PKG_REQ (14*32+11) /* HWP Package Level Request */
  
 -/* Intel-defined CPU features, CPUID level 0x00000007:0 (ECX), word 16 */
 -#define X86_FEATURE_AVX512VBMI		(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 -#define X86_FEATURE_UMIP		(16*32+ 2) /* User Mode Instruction Protection */
 -#define X86_FEATURE_PKU			(16*32+ 3) /* Protection Keys for Userspace */
 -#define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */
 -#define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 -#define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */
 -#define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */
 -#define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */
 -#define X86_FEATURE_AVX512_VNNI		(16*32+11) /* Vector Neural Network Instructions */
 -#define X86_FEATURE_AVX512_BITALG	(16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB instructions */
 -#define X86_FEATURE_TME			(16*32+13) /* Intel Total Memory Encryption */
 -#define X86_FEATURE_AVX512_VPOPCNTDQ	(16*32+14) /* POPCNT for vectors of DW/QW */
 -#define X86_FEATURE_LA57		(16*32+16) /* 5-level page tables */
 -#define X86_FEATURE_RDPID		(16*32+22) /* RDPID instruction */
 -#define X86_FEATURE_CLDEMOTE		(16*32+25) /* CLDEMOTE instruction */
 +/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */
 +#define X86_FEATURE_NPT		(15*32+ 0) /* Nested Page Table support */
 +#define X86_FEATURE_LBRV	(15*32+ 1) /* LBR Virtualization support */
 +#define X86_FEATURE_SVML	(15*32+ 2) /* "svm_lock" SVM locking MSR */
 +#define X86_FEATURE_NRIPS	(15*32+ 3) /* "nrip_save" SVM next_rip save */
 +#define X86_FEATURE_TSCRATEMSR  (15*32+ 4) /* "tsc_scale" TSC scaling support */
 +#define X86_FEATURE_VMCBCLEAN   (15*32+ 5) /* "vmcb_clean" VMCB clean bits support */
 +#define X86_FEATURE_FLUSHBYASID (15*32+ 6) /* flush-by-ASID support */
 +#define X86_FEATURE_DECODEASSISTS (15*32+ 7) /* Decode Assists support */
 +#define X86_FEATURE_PAUSEFILTER (15*32+10) /* filtered pause intercept */
 +#define X86_FEATURE_PFTHRESHOLD (15*32+12) /* pause filter threshold */
 +#define X86_FEATURE_AVIC	(15*32+13) /* Virtual Interrupt Controller */
 +#define X86_FEATURE_V_VMSAVE_VMLOAD (15*32+15) /* Virtual VMSAVE VMLOAD */
 +#define X86_FEATURE_VGIF	(15*32+16) /* Virtual GIF */
  
 -/* AMD-defined CPU features, CPUID level 0x80000007 (EBX), word 17 */
 -#define X86_FEATURE_OVERFLOW_RECOV	(17*32+ 0) /* MCA overflow recovery support */
 -#define X86_FEATURE_SUCCOR		(17*32+ 1) /* Uncorrectable error containment and recovery */
 -#define X86_FEATURE_SMCA		(17*32+ 3) /* Scalable MCA */
 +/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */
 +#define X86_FEATURE_AVX512VBMI	(16*32+ 1) /* AVX512 Vector Bit Manipulation instructions*/
 +#define X86_FEATURE_PKU		(16*32+ 3) /* Protection Keys for Userspace */
 +#define X86_FEATURE_OSPKE	(16*32+ 4) /* OS Protection Keys Enable */
 +#define X86_FEATURE_AVX512_VBMI2 (16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
 +#define X86_FEATURE_GFNI	(16*32+ 8) /* Galois Field New Instructions */
 +#define X86_FEATURE_VAES	(16*32+ 9) /* Vector AES */
 +#define X86_FEATURE_VPCLMULQDQ	(16*32+ 10) /* Carry-Less Multiplication Double Quadword */
 +#define X86_FEATURE_AVX512_VNNI (16*32+ 11) /* Vector Neural Network Instructions */
 +#define X86_FEATURE_AVX512_BITALG (16*32+12) /* Support for VPOPCNT[B,W] and VPSHUF-BITQMB */
 +#define X86_FEATURE_AVX512_VPOPCNTDQ (16*32+14) /* POPCNT for vectors of DW/QW */
  
  /* Intel-defined CPU features, CPUID level 0x00000007:0 (EDX), word 18 */
  #define X86_FEATURE_AVX512_4VNNIW	(18*32+ 2) /* AVX-512 Neural Network Instructions */
diff --cc arch/x86/include/asm/msr-index.h
index e2029f6c9386,562414d5b834..000000000000
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@@ -318,6 -343,14 +318,17 @@@
  #define MSR_AMD64_IBSBRTARGET		0xc001103b
  #define MSR_AMD64_IBSOPDATA4		0xc001103d
  #define MSR_AMD64_IBS_REG_COUNT_MAX	8 /* includes MSR_AMD64_IBSBRTARGET */
++<<<<<<< HEAD
++=======
+ #define MSR_AMD64_SEV			0xc0010131
+ #define MSR_AMD64_SEV_ENABLED_BIT	0
+ #define MSR_AMD64_SEV_ENABLED		BIT_ULL(MSR_AMD64_SEV_ENABLED_BIT)
+ 
+ #define MSR_AMD64_VIRT_SPEC_CTRL	0xc001011f
+ 
+ /* Fam 17h MSRs */
+ #define MSR_F17H_IRPERF			0xc00000e9
++>>>>>>> 11fb0683493b (x86/speculation: Add virtualized speculative store bypass disable support)
  
  /* Fam 16h MSRs */
  #define MSR_F16H_L2I_PERF_CTL		0xc0010230
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,82422a04b506..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -104,6 -129,126 +104,129 @@@ enum spectre_v2_mitigation_cmd spectre_
  #undef pr_fmt
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
++<<<<<<< HEAD
++=======
+ static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
+ 	SPECTRE_V2_NONE;
+ 
+ void x86_spec_ctrl_set(u64 val)
+ {
+ 	if (val & x86_spec_ctrl_mask)
+ 		WARN_ONCE(1, "SPEC_CTRL MSR value 0x%16llx is unknown.\n", val);
+ 	else
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base | val);
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_set);
+ 
+ u64 x86_spec_ctrl_get_default(void)
+ {
+ 	u64 msrval = x86_spec_ctrl_base;
+ 
+ 	if (static_cpu_has(X86_FEATURE_SPEC_CTRL))
+ 		msrval |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
+ 	return msrval;
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_get_default);
+ 
+ /**
+  * x86_spec_ctrl_set_guest - Set speculation control registers for the guest
+  * @guest_spec_ctrl:		The guest content of MSR_SPEC_CTRL
+  * @guest_virt_spec_ctrl:	The guest controlled bits of MSR_VIRT_SPEC_CTRL
+  *				(may get translated to MSR_AMD64_LS_CFG bits)
+  *
+  * Avoids writing to the MSR if the content/bits are the same
+  */
+ void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
+ {
+ 	u64 host = x86_spec_ctrl_base;
+ 
+ 	/* Is MSR_SPEC_CTRL implemented ? */
+ 	if (!static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
+ 		return;
+ 
+ 	/* SSBD controlled in MSR_SPEC_CTRL */
+ 	if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD))
+ 		host |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
+ 
+ 	if (host != guest_spec_ctrl)
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, guest_spec_ctrl);
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_set_guest);
+ 
+ /**
+  * x86_spec_ctrl_restore_host - Restore host speculation control registers
+  * @guest_spec_ctrl:		The guest content of MSR_SPEC_CTRL
+  * @guest_virt_spec_ctrl:	The guest controlled bits of MSR_VIRT_SPEC_CTRL
+  *				(may get translated to MSR_AMD64_LS_CFG bits)
+  *
+  * Avoids writing to the MSR if the content/bits are the same
+  */
+ void x86_spec_ctrl_restore_host(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
+ {
+ 	u64 host = x86_spec_ctrl_base;
+ 
+ 	/* Is MSR_SPEC_CTRL implemented ? */
+ 	if (!static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
+ 		return;
+ 
+ 	/* SSBD controlled in MSR_SPEC_CTRL */
+ 	if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD))
+ 		host |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
+ 
+ 	if (host != guest_spec_ctrl)
+ 		wrmsrl(MSR_IA32_SPEC_CTRL, host);
+ }
+ EXPORT_SYMBOL_GPL(x86_spec_ctrl_restore_host);
+ 
+ static void x86_amd_ssb_disable(void)
+ {
+ 	u64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_ssbd_mask;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_VIRT_SSBD))
+ 		wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, SPEC_CTRL_SSBD);
+ 	else if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD))
+ 		wrmsrl(MSR_AMD64_LS_CFG, msrval);
+ }
+ 
+ #ifdef RETPOLINE
+ static bool spectre_v2_bad_module;
+ 
+ bool retpoline_module_ok(bool has_retpoline)
+ {
+ 	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
+ 		return true;
+ 
+ 	pr_err("System may be vulnerable to spectre v2\n");
+ 	spectre_v2_bad_module = true;
+ 	return false;
+ }
+ 
+ static inline const char *spectre_v2_module_string(void)
+ {
+ 	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
+ }
+ #else
+ static inline const char *spectre_v2_module_string(void) { return ""; }
+ #endif
+ 
+ static void __init spec2_print_if_insecure(const char *reason)
+ {
+ 	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static void __init spec2_print_if_secure(const char *reason)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+ 		pr_info("%s selected on command line.\n", reason);
+ }
+ 
+ static inline bool retp_compiler(void)
+ {
+ 	return __is_defined(RETPOLINE);
+ }
+ 
++>>>>>>> 11fb0683493b (x86/speculation: Add virtualized speculative store bypass disable support)
  static inline bool match_option(const char *arg, int arglen, const char *opt)
  {
  	int len = strlen(opt);
diff --cc arch/x86/kernel/process.c
index f741d66041de,91c3398286d8..000000000000
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@@ -237,7 -277,184 +237,167 @@@ void __switch_to_xtra(struct task_struc
  		 */
  		memset(tss->io_bitmap, 0xff, prev->io_bitmap_max);
  	}
++<<<<<<< HEAD
++=======
+ }
+ 
+ #ifdef CONFIG_SMP
+ 
+ struct ssb_state {
+ 	struct ssb_state	*shared_state;
+ 	raw_spinlock_t		lock;
+ 	unsigned int		disable_state;
+ 	unsigned long		local_state;
+ };
+ 
+ #define LSTATE_SSB	0
+ 
+ static DEFINE_PER_CPU(struct ssb_state, ssb_state);
+ 
+ void speculative_store_bypass_ht_init(void)
+ {
+ 	struct ssb_state *st = this_cpu_ptr(&ssb_state);
+ 	unsigned int this_cpu = smp_processor_id();
+ 	unsigned int cpu;
+ 
+ 	st->local_state = 0;
+ 
+ 	/*
+ 	 * Shared state setup happens once on the first bringup
+ 	 * of the CPU. It's not destroyed on CPU hotunplug.
+ 	 */
+ 	if (st->shared_state)
+ 		return;
+ 
+ 	raw_spin_lock_init(&st->lock);
+ 
+ 	/*
+ 	 * Go over HT siblings and check whether one of them has set up the
+ 	 * shared state pointer already.
+ 	 */
+ 	for_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {
+ 		if (cpu == this_cpu)
+ 			continue;
+ 
+ 		if (!per_cpu(ssb_state, cpu).shared_state)
+ 			continue;
+ 
+ 		/* Link it to the state of the sibling: */
+ 		st->shared_state = per_cpu(ssb_state, cpu).shared_state;
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * First HT sibling to come up on the core.  Link shared state of
+ 	 * the first HT sibling to itself. The siblings on the same core
+ 	 * which come up later will see the shared state pointer and link
+ 	 * themself to the state of this CPU.
+ 	 */
+ 	st->shared_state = st;
+ }
+ 
+ /*
+  * Logic is: First HT sibling enables SSBD for both siblings in the core
+  * and last sibling to disable it, disables it for the whole core. This how
+  * MSR_SPEC_CTRL works in "hardware":
+  *
+  *  CORE_SPEC_CTRL = THREAD0_SPEC_CTRL | THREAD1_SPEC_CTRL
+  */
+ static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
+ {
+ 	struct ssb_state *st = this_cpu_ptr(&ssb_state);
+ 	u64 msr = x86_amd_ls_cfg_base;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_ZEN)) {
+ 		msr |= ssbd_tif_to_amd_ls_cfg(tifn);
+ 		wrmsrl(MSR_AMD64_LS_CFG, msr);
+ 		return;
+ 	}
+ 
+ 	if (tifn & _TIF_SSBD) {
+ 		/*
+ 		 * Since this can race with prctl(), block reentry on the
+ 		 * same CPU.
+ 		 */
+ 		if (__test_and_set_bit(LSTATE_SSB, &st->local_state))
+ 			return;
+ 
+ 		msr |= x86_amd_ls_cfg_ssbd_mask;
+ 
+ 		raw_spin_lock(&st->shared_state->lock);
+ 		/* First sibling enables SSBD: */
+ 		if (!st->shared_state->disable_state)
+ 			wrmsrl(MSR_AMD64_LS_CFG, msr);
+ 		st->shared_state->disable_state++;
+ 		raw_spin_unlock(&st->shared_state->lock);
+ 	} else {
+ 		if (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))
+ 			return;
+ 
+ 		raw_spin_lock(&st->shared_state->lock);
+ 		st->shared_state->disable_state--;
+ 		if (!st->shared_state->disable_state)
+ 			wrmsrl(MSR_AMD64_LS_CFG, msr);
+ 		raw_spin_unlock(&st->shared_state->lock);
+ 	}
+ }
+ #else
+ static __always_inline void amd_set_core_ssb_state(unsigned long tifn)
+ {
+ 	u64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);
+ 
+ 	wrmsrl(MSR_AMD64_LS_CFG, msr);
+ }
+ #endif
+ 
+ static __always_inline void amd_set_ssb_virt_state(unsigned long tifn)
+ {
+ 	/*
+ 	 * SSBD has the same definition in SPEC_CTRL and VIRT_SPEC_CTRL,
+ 	 * so ssbd_tif_to_spec_ctrl() just works.
+ 	 */
+ 	wrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));
+ }
+ 
+ static __always_inline void intel_set_ssb_state(unsigned long tifn)
+ {
+ 	u64 msr = x86_spec_ctrl_base | ssbd_tif_to_spec_ctrl(tifn);
+ 
+ 	wrmsrl(MSR_IA32_SPEC_CTRL, msr);
+ }
+ 
+ static __always_inline void __speculative_store_bypass_update(unsigned long tifn)
+ {
+ 	if (static_cpu_has(X86_FEATURE_VIRT_SSBD))
+ 		amd_set_ssb_virt_state(tifn);
+ 	else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD))
+ 		amd_set_core_ssb_state(tifn);
+ 	else
+ 		intel_set_ssb_state(tifn);
+ }
+ 
+ void speculative_store_bypass_update(void)
+ {
+ 	preempt_disable();
+ 	__speculative_store_bypass_update(current_thread_info()->flags);
+ 	preempt_enable();
+ }
+ 
+ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
+ 		      struct tss_struct *tss)
+ {
+ 	struct thread_struct *prev, *next;
+ 	unsigned long tifp, tifn;
+ 
+ 	prev = &prev_p->thread;
+ 	next = &next_p->thread;
+ 
+ 	tifn = READ_ONCE(task_thread_info(next_p)->flags);
+ 	tifp = READ_ONCE(task_thread_info(prev_p)->flags);
+ 	switch_to_bitmap(tss, prev, next, tifp, tifn);
+ 
++>>>>>>> 11fb0683493b (x86/speculation: Add virtualized speculative store bypass disable support)
  	propagate_user_return_notify(prev_p, next_p);
 -
 -	if ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&
 -	    arch_has_block_step()) {
 -		unsigned long debugctl, msk;
 -
 -		rdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 -		debugctl &= ~DEBUGCTLMSR_BTF;
 -		msk = tifn & _TIF_BLOCKSTEP;
 -		debugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;
 -		wrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);
 -	}
 -
 -	if ((tifp ^ tifn) & _TIF_NOTSC)
 -		cr4_toggle_bits_irqsoff(X86_CR4_TSD);
 -
 -	if ((tifp ^ tifn) & _TIF_NOCPUID)
 -		set_cpuid_faulting(!!(tifn & _TIF_NOCPUID));
 -
 -	if ((tifp ^ tifn) & _TIF_SSBD)
 -		__speculative_store_bypass_update(tifn);
  }
  
  /*
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/include/asm/msr-index.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/process.c
