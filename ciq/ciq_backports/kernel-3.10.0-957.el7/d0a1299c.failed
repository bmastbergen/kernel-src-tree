cxgb4: add support for vxlan segmentation offload

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ganesh Goudar <ganeshgr@chelsio.com>
commit d0a1299c6bf7d80c8bb8e181f36a7c407a4cabca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/d0a1299c.failed

add changes to t4_eth_xmit to enable vxlan segmentation
offload support.

Original work by: Santosh Rastapur <santosh@chelsio.com>
	Signed-off-by: Ganesh Goudar <ganeshgr@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit d0a1299c6bf7d80c8bb8e181f36a7c407a4cabca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/chelsio/cxgb4/sge.c
diff --cc drivers/net/ethernet/chelsio/cxgb4/sge.c
index a64f3863f5c9,eab781fab2a8..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@@ -1119,6 -1138,134 +1135,137 @@@ static inline void txq_advance(struct s
  		q->pidx -= q->size;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_CHELSIO_T4_FCOE
+ static inline int
+ cxgb_fcoe_offload(struct sk_buff *skb, struct adapter *adap,
+ 		  const struct port_info *pi, u64 *cntrl)
+ {
+ 	const struct cxgb_fcoe *fcoe = &pi->fcoe;
+ 
+ 	if (!(fcoe->flags & CXGB_FCOE_ENABLED))
+ 		return 0;
+ 
+ 	if (skb->protocol != htons(ETH_P_FCOE))
+ 		return 0;
+ 
+ 	skb_reset_mac_header(skb);
+ 	skb->mac_len = sizeof(struct ethhdr);
+ 
+ 	skb_set_network_header(skb, skb->mac_len);
+ 	skb_set_transport_header(skb, skb->mac_len + sizeof(struct fcoe_hdr));
+ 
+ 	if (!cxgb_fcoe_sof_eof_supported(adap, skb))
+ 		return -ENOTSUPP;
+ 
+ 	/* FC CRC offload */
+ 	*cntrl = TXPKT_CSUM_TYPE_V(TX_CSUM_FCOE) |
+ 		     TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F |
+ 		     TXPKT_CSUM_START_V(CXGB_FCOE_TXPKT_CSUM_START) |
+ 		     TXPKT_CSUM_END_V(CXGB_FCOE_TXPKT_CSUM_END) |
+ 		     TXPKT_CSUM_LOC_V(CXGB_FCOE_TXPKT_CSUM_END);
+ 	return 0;
+ }
+ #endif /* CONFIG_CHELSIO_T4_FCOE */
+ 
+ /* Returns tunnel type if hardware supports offloading of the same.
+  * It is called only for T5 and onwards.
+  */
+ enum cpl_tx_tnl_lso_type cxgb_encap_offload_supported(struct sk_buff *skb)
+ {
+ 	u8 l4_hdr = 0;
+ 	enum cpl_tx_tnl_lso_type tnl_type = TX_TNL_TYPE_OPAQUE;
+ 	struct port_info *pi = netdev_priv(skb->dev);
+ 	struct adapter *adapter = pi->adapter;
+ 
+ 	if (skb->inner_protocol_type != ENCAP_TYPE_ETHER ||
+ 	    skb->inner_protocol != htons(ETH_P_TEB))
+ 		return tnl_type;
+ 
+ 	switch (vlan_get_protocol(skb)) {
+ 	case htons(ETH_P_IP):
+ 		l4_hdr = ip_hdr(skb)->protocol;
+ 		break;
+ 	case htons(ETH_P_IPV6):
+ 		l4_hdr = ipv6_hdr(skb)->nexthdr;
+ 		break;
+ 	default:
+ 		return tnl_type;
+ 	}
+ 
+ 	switch (l4_hdr) {
+ 	case IPPROTO_UDP:
+ 		if (adapter->vxlan_port == udp_hdr(skb)->dest)
+ 			tnl_type = TX_TNL_TYPE_VXLAN;
+ 		break;
+ 	default:
+ 		return tnl_type;
+ 	}
+ 
+ 	return tnl_type;
+ }
+ 
+ static inline void t6_fill_tnl_lso(struct sk_buff *skb,
+ 				   struct cpl_tx_tnl_lso *tnl_lso,
+ 				   enum cpl_tx_tnl_lso_type tnl_type)
+ {
+ 	u32 val;
+ 	int in_eth_xtra_len;
+ 	int l3hdr_len = skb_network_header_len(skb);
+ 	int eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;
+ 	const struct skb_shared_info *ssi = skb_shinfo(skb);
+ 	bool v6 = (ip_hdr(skb)->version == 6);
+ 
+ 	val = CPL_TX_TNL_LSO_OPCODE_V(CPL_TX_TNL_LSO) |
+ 	      CPL_TX_TNL_LSO_FIRST_F |
+ 	      CPL_TX_TNL_LSO_LAST_F |
+ 	      (v6 ? CPL_TX_TNL_LSO_IPV6OUT_F : 0) |
+ 	      CPL_TX_TNL_LSO_ETHHDRLENOUT_V(eth_xtra_len / 4) |
+ 	      CPL_TX_TNL_LSO_IPHDRLENOUT_V(l3hdr_len / 4) |
+ 	      (v6 ? 0 : CPL_TX_TNL_LSO_IPHDRCHKOUT_F) |
+ 	      CPL_TX_TNL_LSO_IPLENSETOUT_F |
+ 	      (v6 ? 0 : CPL_TX_TNL_LSO_IPIDINCOUT_F);
+ 	tnl_lso->op_to_IpIdSplitOut = htonl(val);
+ 
+ 	tnl_lso->IpIdOffsetOut = 0;
+ 
+ 	/* Get the tunnel header length */
+ 	val = skb_inner_mac_header(skb) - skb_mac_header(skb);
+ 	in_eth_xtra_len = skb_inner_network_header(skb) -
+ 			  skb_inner_mac_header(skb) - ETH_HLEN;
+ 
+ 	switch (tnl_type) {
+ 	case TX_TNL_TYPE_VXLAN:
+ 		tnl_lso->UdpLenSetOut_to_TnlHdrLen =
+ 			htons(CPL_TX_TNL_LSO_UDPCHKCLROUT_F |
+ 			CPL_TX_TNL_LSO_UDPLENSETOUT_F);
+ 		break;
+ 	default:
+ 		tnl_lso->UdpLenSetOut_to_TnlHdrLen = 0;
+ 		break;
+ 	}
+ 
+ 	tnl_lso->UdpLenSetOut_to_TnlHdrLen |=
+ 		 htons(CPL_TX_TNL_LSO_TNLHDRLEN_V(val) |
+ 		       CPL_TX_TNL_LSO_TNLTYPE_V(tnl_type));
+ 
+ 	tnl_lso->r1 = 0;
+ 
+ 	val = CPL_TX_TNL_LSO_ETHHDRLEN_V(in_eth_xtra_len / 4) |
+ 	      CPL_TX_TNL_LSO_IPV6_V(inner_ip_hdr(skb)->version == 6) |
+ 	      CPL_TX_TNL_LSO_IPHDRLEN_V(skb_inner_network_header_len(skb) / 4) |
+ 	      CPL_TX_TNL_LSO_TCPHDRLEN_V(inner_tcp_hdrlen(skb) / 4);
+ 	tnl_lso->Flow_to_TcpHdrLen = htonl(val);
+ 
+ 	tnl_lso->IpIdOffset = htons(0);
+ 
+ 	tnl_lso->IpIdSplit_to_Mss = htons(CPL_TX_TNL_LSO_MSS_V(ssi->gso_size));
+ 	tnl_lso->TCPSeqOffset = htonl(0);
+ 	tnl_lso->EthLenOffset_Size = htonl(CPL_TX_TNL_LSO_SIZE_V(skb->len));
+ }
+ 
++>>>>>>> d0a1299c6bf7 (cxgb4: add support for vxlan segmentation offload)
  /**
   *	t4_eth_xmit - add a packet to an Ethernet Tx queue
   *	@skb: the packet
@@@ -1142,6 -1289,12 +1289,15 @@@ netdev_tx_t t4_eth_xmit(struct sk_buff 
  	bool immediate = false;
  	int len, max_pkt_len;
  	bool ptp_enabled = is_ptp_enabled(skb, dev);
++<<<<<<< HEAD
++=======
+ 	unsigned int chip_ver;
+ 	enum cpl_tx_tnl_lso_type tnl_type = TX_TNL_TYPE_OPAQUE;
+ 
+ #ifdef CONFIG_CHELSIO_T4_FCOE
+ 	int err;
+ #endif /* CONFIG_CHELSIO_T4_FCOE */
++>>>>>>> d0a1299c6bf7 (cxgb4: add support for vxlan segmentation offload)
  
  	/*
  	 * The chip min packet length is 10 octets but play safe and reject
@@@ -1178,8 -1331,19 +1334,9 @@@ out_free:	dev_kfree_skb_any(skb)
  	skb_tx_timestamp(skb);
  
  	reclaim_completed_tx(adap, &q->q, true);
 -	cntrl = TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F;
 -
 -#ifdef CONFIG_CHELSIO_T4_FCOE
 -	err = cxgb_fcoe_offload(skb, adap, pi, &cntrl);
 -	if (unlikely(err == -ENOTSUPP)) {
 -		if (ptp_enabled)
 -			spin_unlock(&adap->ptp_lock);
 -		goto out_free;
 -	}
 -#endif /* CONFIG_CHELSIO_T4_FCOE */
  
- 	flits = calc_tx_flits(skb);
+ 	chip_ver = CHELSIO_CHIP_VERSION(adap->params.chip);
+ 	flits = calc_tx_flits(skb, chip_ver);
  	ndesc = flits_to_desc(flits);
  	credits = txq_avail(&q->q) - ndesc;
  
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index 28baa6f532a1..96f55af88db2 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@ -1288,6 +1288,7 @@ void t4_sge_start(struct adapter *adap);
 void t4_sge_stop(struct adapter *adap);
 void cxgb4_set_ethtool_ops(struct net_device *netdev);
 int cxgb4_write_rss(const struct port_info *pi, const u16 *queues);
+enum cpl_tx_tnl_lso_type cxgb_encap_offload_supported(struct sk_buff *skb);
 extern int dbfifo_int_thresh;
 
 #define for_each_port(adapter, iter) \
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
index 78a568c4dc20..c295778fc523 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
@@ -5029,6 +5029,10 @@ static int init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
 			NETIF_F_RXCSUM | NETIF_F_RXHASH |
 			NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |
 			NETIF_F_HW_TC;
+
+		if (CHELSIO_CHIP_VERSION(chip) > CHELSIO_T5)
+			netdev->hw_features |= NETIF_F_GSO_UDP_TUNNEL;
+
 		if (highdma)
 			netdev->hw_features |= NETIF_F_HIGHDMA;
 		netdev->features |= netdev->hw_features;
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/sge.c
