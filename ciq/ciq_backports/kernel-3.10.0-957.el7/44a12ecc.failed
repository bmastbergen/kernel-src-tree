nfp: bpf: don't depend on high order allocations for program image

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 44a12ecc1cab7dcf4647dfef7d94f5c559c01407
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/44a12ecc.failed

The translator pre-allocates a buffer of maximal program size.
Due to HW/FW limitations the program buffer can't currently be
longer than 128Kb, so we used to kmalloc() it, and then map for
DMA directly.

Now that the late branch resolution is copying the program image
anyway, we can just kvmalloc() the buffer.  While at it, after
translation reallocate the buffer to save space.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 44a12ecc1cab7dcf4647dfef7d94f5c559c01407)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/jit.c
#	drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/jit.c
index 0a5af8620ac1,5deebbc18cfd..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/jit.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/jit.c
@@@ -2464,66 -2676,36 +2464,88 @@@ static int nfp_bpf_ustore_calc(struct n
  	return 0;
  }
  
++<<<<<<< HEAD
 +/**
 + * nfp_bpf_jit() - translate BPF code into NFP assembly
 + * @filter:	kernel BPF filter struct
 + * @prog_mem:	memory to store assembler instructions
 + * @act:	action attached to this eBPF program
 + * @prog_start:	offset of the first instruction when loaded
 + * @prog_done:	where to jump on exit
 + * @prog_sz:	size of @prog_mem in instructions
 + * @res:	achieved parameters of translation results
 + */
 +int
 +nfp_bpf_jit(struct bpf_prog *filter, void *prog_mem,
 +	    enum nfp_bpf_action_type act,
 +	    unsigned int prog_start, unsigned int prog_done,
 +	    unsigned int prog_sz, struct nfp_bpf_result *res)
++=======
+ static void nfp_bpf_prog_trim(struct nfp_prog *nfp_prog)
+ {
+ 	void *prog;
+ 
+ 	prog = kvmalloc_array(nfp_prog->prog_len, sizeof(u64), GFP_KERNEL);
+ 	if (!prog)
+ 		return;
+ 
+ 	nfp_prog->__prog_alloc_len = nfp_prog->prog_len * sizeof(u64);
+ 	memcpy(prog, nfp_prog->prog, nfp_prog->__prog_alloc_len);
+ 	kvfree(nfp_prog->prog);
+ 	nfp_prog->prog = prog;
+ }
+ 
+ int nfp_bpf_jit(struct nfp_prog *nfp_prog)
++>>>>>>> 44a12ecc1cab (nfp: bpf: don't depend on high order allocations for program image)
  {
 +	struct nfp_prog *nfp_prog;
  	int ret;
  
 +	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
 +	if (!nfp_prog)
 +		return -ENOMEM;
 +
 +	INIT_LIST_HEAD(&nfp_prog->insns);
 +	nfp_prog->act = act;
 +	nfp_prog->start_off = prog_start;
 +	nfp_prog->tgt_done = prog_done;
 +
 +	ret = nfp_prog_prepare(nfp_prog, filter->insnsi, filter->len);
 +	if (ret)
 +		goto out;
 +
 +	ret = nfp_prog_verify(nfp_prog, filter);
 +	if (ret)
 +		goto out;
 +
  	ret = nfp_bpf_optimize(nfp_prog);
  	if (ret)
 -		return ret;
 +		goto out;
 +
 +	nfp_prog->num_regs = MAX_BPF_REG;
 +	nfp_prog->regs_per_thread = 32;
 +
 +	nfp_prog->prog = prog_mem;
 +	nfp_prog->__prog_alloc_len = prog_sz;
  
  	ret = nfp_translate(nfp_prog);
  	if (ret) {
  		pr_err("Translation failed with error %d (translated: %u)\n",
  		       ret, nfp_prog->n_translated);
 -		return -EINVAL;
 +		ret = -EINVAL;
 +		goto out;
  	}
  
++<<<<<<< HEAD
 +	ret = nfp_bpf_ustore_calc(nfp_prog, (__force __le64 *)prog_mem);
 +
 +	res->n_instr = nfp_prog->prog_len;
 +	res->dense_mode = false;
 +out:
 +	nfp_prog_free(nfp_prog);
++=======
+ 	nfp_bpf_prog_trim(nfp_prog);
++>>>>>>> 44a12ecc1cab (nfp: bpf: don't depend on high order allocations for program image)
  
  	return ret;
  }
diff --cc drivers/net/ethernet/netronome/nfp/bpf/offload.c
index de79faf0874b,f63560550753..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/offload.c
@@@ -51,118 -52,115 +52,138 @@@
  #include "../nfp_net_ctrl.h"
  #include "../nfp_net.h"
  
 -static int
 -nfp_prog_prepare(struct nfp_prog *nfp_prog, const struct bpf_insn *prog,
 -		 unsigned int cnt)
 +void nfp_net_filter_stats_timer(unsigned long data)
  {
 -	struct nfp_insn_meta *meta;
 -	unsigned int i;
 +	struct nfp_net *nn = (void *)data;
 +	struct nfp_net_bpf_priv *priv;
 +	struct nfp_stat_pair latest;
  
 -	for (i = 0; i < cnt; i++) {
 -		meta = kzalloc(sizeof(*meta), GFP_KERNEL);
 -		if (!meta)
 -			return -ENOMEM;
 +	priv = nn->app_priv;
  
 -		meta->insn = prog[i];
 -		meta->n = i;
 +	spin_lock_bh(&priv->rx_filter_lock);
  
 -		list_add_tail(&meta->l, &nfp_prog->insns);
 -	}
 +	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 +		mod_timer(&priv->rx_filter_stats_timer,
 +			  jiffies + NFP_NET_STAT_POLL_IVL);
  
 -	nfp_bpf_jit_prepare(nfp_prog, cnt);
 +	spin_unlock_bh(&priv->rx_filter_lock);
  
 -	return 0;
 +	latest.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	latest.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +
 +	if (latest.pkts != priv->rx_filter.pkts)
 +		priv->rx_filter_change = jiffies;
 +
 +	priv->rx_filter = latest;
  }
  
 -static void nfp_prog_free(struct nfp_prog *nfp_prog)
 +#if 0 /* Not in RHEL7 */
 +static void nfp_net_bpf_stats_reset(struct nfp_net *nn)
  {
 -	struct nfp_insn_meta *meta, *tmp;
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
  
 -	list_for_each_entry_safe(meta, tmp, &nfp_prog->insns, l) {
 -		list_del(&meta->l);
 -		kfree(meta);
 -	}
 -	kfree(nfp_prog);
 +	priv->rx_filter.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	priv->rx_filter.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +	priv->rx_filter_prev = priv->rx_filter;
 +	priv->rx_filter_change = jiffies;
  }
  
 -int nfp_bpf_verifier_prep(struct nfp_app *app, struct nfp_net *nn,
 -			  struct netdev_bpf *bpf)
 +static int
 +nfp_net_bpf_stats_update(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
  {
 -	struct bpf_prog *prog = bpf->verifier.prog;
 -	struct nfp_prog *nfp_prog;
 -	int ret;
 +	struct tc_action *a;
 +	LIST_HEAD(actions);
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bytes, pkts;
  
 -	nfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);
 -	if (!nfp_prog)
 -		return -ENOMEM;
 -	prog->aux->offload->dev_priv = nfp_prog;
 +	pkts = priv->rx_filter.pkts - priv->rx_filter_prev.pkts;
 +	bytes = priv->rx_filter.bytes - priv->rx_filter_prev.bytes;
 +	bytes -= pkts * ETH_HLEN;
  
 -	INIT_LIST_HEAD(&nfp_prog->insns);
 -	nfp_prog->type = prog->type;
 -	nfp_prog->bpf = app->priv;
 +	priv->rx_filter_prev = priv->rx_filter;
  
 -	ret = nfp_prog_prepare(nfp_prog, prog->insnsi, prog->len);
 -	if (ret)
 -		goto err_free;
 +	preempt_disable();
  
 -	nfp_prog->verifier_meta = nfp_prog_first_meta(nfp_prog);
 -	bpf->verifier.ops = &nfp_bpf_analyzer_ops;
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list)
 +		tcf_action_stats_update(a, bytes, pkts, nn->rx_filter_change);
  
 -	return 0;
 -
 -err_free:
 -	nfp_prog_free(nfp_prog);
 +	preempt_enable();
  
 -	return ret;
 +	return 0;
  }
  
 -int nfp_bpf_translate(struct nfp_app *app, struct nfp_net *nn,
 -		      struct bpf_prog *prog)
 +static int
 +nfp_net_bpf_get_act(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
  {
 -	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
 -	unsigned int stack_size;
 -	unsigned int max_instr;
 +	const struct tc_action *a;
 +	LIST_HEAD(actions);
 +
 +	if (!cls_bpf->exts)
 +		return NN_ACT_XDP;
 +
 +	/* TC direct action */
 +	if (cls_bpf->exts_integrated) {
 +		if (tc_no_actions(cls_bpf->exts))
 +			return NN_ACT_DIRECT;
  
 -	stack_size = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;
 -	if (prog->aux->stack_depth > stack_size) {
 -		nn_info(nn, "stack too large: program %dB > FW stack %dB\n",
 -			prog->aux->stack_depth, stack_size);
  		return -EOPNOTSUPP;
  	}
 -	nfp_prog->stack_depth = round_up(prog->aux->stack_depth, 4);
  
 -	max_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);
 -	nfp_prog->__prog_alloc_len = max_instr * sizeof(u64);
 +	/* TC legacy mode */
 +	if (!tc_single_action(cls_bpf->exts))
 +		return -EOPNOTSUPP;
  
++<<<<<<< HEAD
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list) {
 +		if (is_tcf_gact_shot(a))
 +			return NN_ACT_TC_DROP;
++=======
+ 	nfp_prog->prog = kvmalloc(nfp_prog->__prog_alloc_len, GFP_KERNEL);
+ 	if (!nfp_prog->prog)
+ 		return -ENOMEM;
++>>>>>>> 44a12ecc1cab (nfp: bpf: don't depend on high order allocations for program image)
  
 -	return nfp_bpf_jit(nfp_prog);
 +		if (is_tcf_mirred_egress_redirect(a) &&
 +		    tcf_mirred_ifindex(a) == nn->dp.netdev->ifindex)
 +			return NN_ACT_TC_REDIR;
 +	}
 +
 +	return -EOPNOTSUPP;
  }
  
 -int nfp_bpf_destroy(struct nfp_app *app, struct nfp_net *nn,
 -		    struct bpf_prog *prog)
 +static int
 +nfp_net_bpf_offload_prepare(struct nfp_net *nn,
 +			    struct tc_cls_bpf_offload *cls_bpf,
 +			    struct nfp_bpf_result *res,
 +			    void **code, dma_addr_t *dma_addr, u16 max_instr)
  {
++<<<<<<< HEAD
 +	unsigned int code_sz = max_instr * sizeof(u64);
 +	enum nfp_bpf_action_type act;
 +	unsigned int stack_size;
 +	u16 start_off, done_off;
++=======
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
+ 
+ 	kvfree(nfp_prog->prog);
+ 	nfp_prog_free(nfp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_net_bpf_load(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;
++>>>>>>> 44a12ecc1cab (nfp: bpf: don't depend on high order allocations for program image)
  	unsigned int max_mtu;
 -	dma_addr_t dma_addr;
 -	void *img;
 -	int err;
 +	int ret;
 +
 +	ret = nfp_net_bpf_get_act(nn, cls_bpf);
 +	if (ret < 0)
 +		return ret;
 +	act = ret;
  
  	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
  	if (max_mtu < nn->dp.netdev->mtu) {
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/jit.c
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/offload.c
