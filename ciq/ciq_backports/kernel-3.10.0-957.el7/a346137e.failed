powerpc/numa: Use ibm,max-associativity-domains to discover possible nodes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [powerpc] numa: Use ibm, max-associativity-domains to discover possible nodes (Serhii Popovych) [1507765]
Rebuild_FUZZ: 93.62%
commit-author Michael Bringmann <mwb@linux.vnet.ibm.com>
commit a346137e9142b039fd13af2e59696e3d40c487ef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a346137e.failed

On powerpc systems which allow 'hot-add' of CPU or memory resources,
it may occur that the new resources are to be inserted into nodes that
were not used for these resources at bootup. In the kernel, any node
that is used must be defined and initialized. These empty nodes may
occur when,

* Dedicated vs. shared resources. Shared resources require information
  such as the VPHN hcall for CPU assignment to nodes. Associativity
  decisions made based on dedicated resource rules, such as
  associativity properties in the device tree, may vary from decisions
  made using the values returned by the VPHN hcall.

* memoryless nodes at boot. Nodes need to be defined as 'possible' at
  boot for operation with other code modules. Previously, the powerpc
  code would limit the set of possible nodes to those which have
  memory assigned at boot, and were thus online. Subsequent add/remove
  of CPUs or memory would only work with this subset of possible
  nodes.

* memoryless nodes with CPUs at boot. Due to the previous restriction
  on nodes, nodes that had CPUs but no memory were being collapsed
  into other nodes that did have memory at boot. In practice this
  meant that the node assignment presented by the runtime kernel
  differed from the affinity and associativity attributes presented by
  the device tree or VPHN hcalls. Nodes that might be known to the
  pHyp were not 'possible' in the runtime kernel because they did not
  have memory at boot.

This patch ensures that sufficient nodes are defined to support
configuration requirements after boot, as well as at boot. This patch
set fixes a couple of problems.

* Nodes known to powerpc to be memoryless at boot, but to have CPUs in
  them are allowed to be 'possible' and 'online'. Memory allocations
  for those nodes are taken from another node that does have memory
  until and if memory is hot-added to the node. * Nodes which have no
  resources assigned at boot, but which may still be referenced
  subsequently by affinity or associativity attributes, are kept in
  the list of 'possible' nodes for powerpc. Hot-add of memory or CPUs
  to the system can reference these nodes and bring them online
  instead of redirecting to one of the set of nodes that were known to
  have memory at boot.

This patch extracts the value of the lowest domain level (number of
allocable resources) from the device tree property
"ibm,max-associativity-domains" to use as the maximum number of nodes
to setup as possibly available in the system. This new setting will
override the instruction:

    nodes_and(node_possible_map, node_possible_map, node_online_map);

presently seen in the function arch/powerpc/mm/numa.c:initmem_init().

If the "ibm,max-associativity-domains" property is not present at
boot, no operation will be performed to define or enable additional
nodes, or enable the above 'nodes_and()'.

	Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
	Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit a346137e9142b039fd13af2e59696e3d40c487ef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/mm/numa.c
diff --cc arch/powerpc/mm/numa.c
index 4542d324c7e4,f9cd40cd5485..000000000000
--- a/arch/powerpc/mm/numa.c
+++ b/arch/powerpc/mm/numa.c
@@@ -894,165 -784,61 +894,197 @@@ void __init dump_numa_cpu_topology(void
  	}
  }
  
 -/* Initialize NODE_DATA for a node on the local memory */
 -static void __init setup_node_data(int nid, u64 start_pfn, u64 end_pfn)
 +static void __init dump_numa_memory_topology(void)
  {
 -	u64 spanned_pages = end_pfn - start_pfn;
 -	const size_t nd_size = roundup(sizeof(pg_data_t), SMP_CACHE_BYTES);
 -	u64 nd_pa;
 -	void *nd;
 -	int tnid;
 +	unsigned int node;
 +	unsigned int count;
  
 -	nd_pa = memblock_alloc_try_nid(nd_size, SMP_CACHE_BYTES, nid);
 -	nd = __va(nd_pa);
 +	if (min_common_depth == -1 || !numa_enabled)
 +		return;
  
 -	/* report and initialize */
 -	pr_info("  NODE_DATA [mem %#010Lx-%#010Lx]\n",
 -		nd_pa, nd_pa + nd_size - 1);
 -	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
 -	if (tnid != nid)
 -		pr_info("    NODE_DATA(%d) on node %d\n", nid, tnid);
 +	for_each_online_node(node) {
 +		unsigned long i;
 +
 +		printk(KERN_DEBUG "Node %d Memory:", node);
 +
 +		count = 0;
 +
 +		for (i = 0; i < memblock_end_of_DRAM();
 +		     i += (1 << SECTION_SIZE_BITS)) {
 +			if (early_pfn_to_nid(i >> PAGE_SHIFT) == node) {
 +				if (count == 0)
 +					printk(" 0x%lx", i);
 +				++count;
 +			} else {
 +				if (count > 0)
 +					printk("-0x%lx", i);
 +				count = 0;
 +			}
 +		}
  
 -	node_data[nid] = nd;
 -	memset(NODE_DATA(nid), 0, sizeof(pg_data_t));
 -	NODE_DATA(nid)->node_id = nid;
 -	NODE_DATA(nid)->node_start_pfn = start_pfn;
 -	NODE_DATA(nid)->node_spanned_pages = spanned_pages;
 +		if (count > 0)
 +			printk("-0x%lx", i);
 +		printk("\n");
 +	}
  }
  
++<<<<<<< HEAD
 +/*
 + * Allocate some memory, satisfying the memblock or bootmem allocator where
 + * required. nid is the preferred node and end is the physical address of
 + * the highest address in the node.
 + *
 + * Returns the virtual address of the memory.
 + */
 +static void __init *careful_zallocation(int nid, unsigned long size,
 +				       unsigned long align,
 +				       unsigned long end_pfn)
 +{
 +	void *ret;
 +	int new_nid;
 +	unsigned long ret_paddr;
 +
 +	ret_paddr = __memblock_alloc_base(size, align, end_pfn << PAGE_SHIFT);
 +
 +	/* retry over all memory */
 +	if (!ret_paddr)
 +		ret_paddr = __memblock_alloc_base(size, align, memblock_end_of_DRAM());
 +
 +	if (!ret_paddr)
 +		panic("numa.c: cannot allocate %lu bytes for node %d",
 +		      size, nid);
 +
 +	ret = __va(ret_paddr);
 +
 +	/*
 +	 * We initialize the nodes in numeric order: 0, 1, 2...
 +	 * and hand over control from the MEMBLOCK allocator to the
 +	 * bootmem allocator.  If this function is called for
 +	 * node 5, then we know that all nodes <5 are using the
 +	 * bootmem allocator instead of the MEMBLOCK allocator.
 +	 *
 +	 * So, check the nid from which this allocation came
 +	 * and double check to see if we need to use bootmem
 +	 * instead of the MEMBLOCK.  We don't free the MEMBLOCK memory
 +	 * since it would be useless.
 +	 */
 +	new_nid = early_pfn_to_nid(ret_paddr >> PAGE_SHIFT);
 +	if (new_nid < nid) {
 +		ret = __alloc_bootmem_node(NODE_DATA(new_nid),
 +				size, align, 0);
 +
 +		dbg("alloc_bootmem %p %lx\n", ret, size);
 +	}
 +
 +	memset(ret, 0, size);
 +	return ret;
 +}
 +
 +static struct notifier_block ppc64_numa_nb = {
 +	.notifier_call = cpu_numa_callback,
 +	.priority = 1 /* Must run before sched domains notifier. */
 +};
 +
 +static void __init mark_reserved_regions_for_nid(int nid)
 +{
 +	struct pglist_data *node = NODE_DATA(nid);
 +	struct memblock_region *reg;
 +
 +	for_each_memblock(reserved, reg) {
 +		unsigned long physbase = reg->base;
 +		unsigned long size = reg->size;
 +		unsigned long start_pfn = physbase >> PAGE_SHIFT;
 +		unsigned long end_pfn = PFN_UP(physbase + size);
 +		struct node_active_region node_ar;
 +		unsigned long node_end_pfn = node->node_start_pfn +
 +					     node->node_spanned_pages;
 +
 +		/*
 +		 * Check to make sure that this memblock.reserved area is
 +		 * within the bounds of the node that we care about.
 +		 * Checking the nid of the start and end points is not
 +		 * sufficient because the reserved area could span the
 +		 * entire node.
 +		 */
 +		if (end_pfn <= node->node_start_pfn ||
 +		    start_pfn >= node_end_pfn)
 +			continue;
 +
 +		get_node_active_region(start_pfn, &node_ar);
 +		while (start_pfn < end_pfn &&
 +			node_ar.start_pfn < node_ar.end_pfn) {
 +			unsigned long reserve_size = size;
 +			/*
 +			 * if reserved region extends past active region
 +			 * then trim size to active region
 +			 */
 +			if (end_pfn > node_ar.end_pfn)
 +				reserve_size = (node_ar.end_pfn << PAGE_SHIFT)
 +					- physbase;
 +			/*
 +			 * Only worry about *this* node, others may not
 +			 * yet have valid NODE_DATA().
 +			 */
 +			if (node_ar.nid == nid) {
 +				dbg("reserve_bootmem %lx %lx nid=%d\n",
 +					physbase, reserve_size, node_ar.nid);
 +				reserve_bootmem_node(NODE_DATA(node_ar.nid),
 +						physbase, reserve_size,
 +						BOOTMEM_DEFAULT);
 +			}
 +			/*
 +			 * if reserved region is contained in the active region
 +			 * then done.
 +			 */
 +			if (end_pfn <= node_ar.end_pfn)
 +				break;
 +
 +			/*
 +			 * reserved region extends past the active region
 +			 *   get next active region that contains this
 +			 *   reserved region
 +			 */
 +			start_pfn = node_ar.end_pfn;
 +			physbase = start_pfn << PAGE_SHIFT;
 +			size = size - reserve_size;
 +			get_node_active_region(start_pfn, &node_ar);
 +		}
 +	}
 +}
 +
 +
 +void __init do_init_bootmem(void)
++=======
+ static void __init find_possible_nodes(void)
+ {
+ 	struct device_node *rtas;
+ 	u32 numnodes, i;
+ 
+ 	if (min_common_depth <= 0)
+ 		return;
+ 
+ 	rtas = of_find_node_by_path("/rtas");
+ 	if (!rtas)
+ 		return;
+ 
+ 	if (of_property_read_u32_index(rtas,
+ 				"ibm,max-associativity-domains",
+ 				min_common_depth, &numnodes))
+ 		goto out;
+ 
+ 	for (i = 0; i < numnodes; i++) {
+ 		if (!node_possible(i)) {
+ 			setup_node_data(i, 0, 0);
+ 			node_set(i, node_possible_map);
+ 		}
+ 	}
+ 
+ out:
+ 	of_node_put(rtas);
+ }
+ 
+ void __init initmem_init(void)
++>>>>>>> a346137e9142 (powerpc/numa: Use ibm,max-associativity-domains to discover possible nodes)
  {
  	int nid, cpu;
  
@@@ -1062,8 -847,18 +1094,23 @@@
  
  	if (parse_numa_properties())
  		setup_nonnuma();
++<<<<<<< HEAD
 +	else
 +		dump_numa_memory_topology();
++=======
+ 
+ 	memblock_dump_all();
+ 
+ 	/*
+ 	 * Modify the set of possible NUMA nodes to reflect information
+ 	 * available about the set of online nodes, and the set of nodes
+ 	 * that we expect to make use of for this platform's affinity
+ 	 * calculations.
+ 	 */
+ 	nodes_and(node_possible_map, node_possible_map, node_online_map);
++>>>>>>> a346137e9142 (powerpc/numa: Use ibm,max-associativity-domains to discover possible nodes)
+ 
+ 	find_possible_nodes();
  
  	for_each_online_node(nid) {
  		unsigned long start_pfn, end_pfn;
* Unmerged path arch/powerpc/mm/numa.c
