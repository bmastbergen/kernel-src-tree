powerpc/numa: Ensure nodes initialized for hotplug

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [powerpc] numa: Ensure nodes initialized for hotplug (Serhii Popovych) [1507765]
Rebuild_FUZZ: 91.30%
commit-author Michael Bringmann <mwb@linux.vnet.ibm.com>
commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/ea05ba7c.failed

This patch fixes some problems encountered at runtime with
configurations that support memory-less nodes, or that hot-add CPUs
into nodes that are memoryless during system execution after boot. The
problems of interest include:

* Nodes known to powerpc to be memoryless at boot, but to have CPUs in
  them are allowed to be 'possible' and 'online'. Memory allocations
  for those nodes are taken from another node that does have memory
  until and if memory is hot-added to the node.

* Nodes which have no resources assigned at boot, but which may still
  be referenced subsequently by affinity or associativity attributes,
  are kept in the list of 'possible' nodes for powerpc. Hot-add of
  memory or CPUs to the system can reference these nodes and bring
  them online instead of redirecting the references to one of the set
  of nodes known to have memory at boot.

Note that this software operates under the context of CPU hotplug. We
are not doing memory hotplug in this code, but rather updating the
kernel's CPU topology (i.e. arch_update_cpu_topology /
numa_update_cpu_topology). We are initializing a node that may be used
by CPUs or memory before it can be referenced as invalid by a CPU
hotplug operation. CPU hotplug operations are protected by a range of
APIs including cpu_maps_update_begin/cpu_maps_update_done,
cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
Memory hotplug operations, including try_online_node, are protected by
mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
case of CPUs being hot-added to a previously memoryless node, the
try_online_node operation occurs wholly within the CPU locks with no
overlap. Using HMC hot-add/hot-remove operations, we have been able to
add and remove CPUs to any possible node without failures. HMC
operations involve a degree self-serialization, though.

	Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
	Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/mm/numa.c
diff --cc arch/powerpc/mm/numa.c
index 4542d324c7e4,1bead2c67272..000000000000
--- a/arch/powerpc/mm/numa.c
+++ b/arch/powerpc/mm/numa.c
@@@ -894,165 -784,59 +894,171 @@@ void __init dump_numa_cpu_topology(void
  	}
  }
  
 -/* Initialize NODE_DATA for a node on the local memory */
 -static void __init setup_node_data(int nid, u64 start_pfn, u64 end_pfn)
 +static void __init dump_numa_memory_topology(void)
  {
 -	u64 spanned_pages = end_pfn - start_pfn;
 -	const size_t nd_size = roundup(sizeof(pg_data_t), SMP_CACHE_BYTES);
 -	u64 nd_pa;
 -	void *nd;
 -	int tnid;
 +	unsigned int node;
 +	unsigned int count;
  
 -	nd_pa = memblock_alloc_try_nid(nd_size, SMP_CACHE_BYTES, nid);
 -	nd = __va(nd_pa);
 +	if (min_common_depth == -1 || !numa_enabled)
 +		return;
  
 -	/* report and initialize */
 -	pr_info("  NODE_DATA [mem %#010Lx-%#010Lx]\n",
 -		nd_pa, nd_pa + nd_size - 1);
 -	tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);
 -	if (tnid != nid)
 -		pr_info("    NODE_DATA(%d) on node %d\n", nid, tnid);
 +	for_each_online_node(node) {
 +		unsigned long i;
  
 -	node_data[nid] = nd;
 -	memset(NODE_DATA(nid), 0, sizeof(pg_data_t));
 -	NODE_DATA(nid)->node_id = nid;
 -	NODE_DATA(nid)->node_start_pfn = start_pfn;
 -	NODE_DATA(nid)->node_spanned_pages = spanned_pages;
 +		printk(KERN_DEBUG "Node %d Memory:", node);
 +
 +		count = 0;
 +
 +		for (i = 0; i < memblock_end_of_DRAM();
 +		     i += (1 << SECTION_SIZE_BITS)) {
 +			if (early_pfn_to_nid(i >> PAGE_SHIFT) == node) {
 +				if (count == 0)
 +					printk(" 0x%lx", i);
 +				++count;
 +			} else {
 +				if (count > 0)
 +					printk("-0x%lx", i);
 +				count = 0;
 +			}
 +		}
 +
 +		if (count > 0)
 +			printk("-0x%lx", i);
 +		printk("\n");
 +	}
  }
  
 -static void __init find_possible_nodes(void)
 +/*
 + * Allocate some memory, satisfying the memblock or bootmem allocator where
 + * required. nid is the preferred node and end is the physical address of
 + * the highest address in the node.
 + *
 + * Returns the virtual address of the memory.
 + */
 +static void __init *careful_zallocation(int nid, unsigned long size,
 +				       unsigned long align,
 +				       unsigned long end_pfn)
  {
 -	struct device_node *rtas;
 -	u32 numnodes, i;
 +	void *ret;
 +	int new_nid;
 +	unsigned long ret_paddr;
  
 -	if (min_common_depth <= 0)
 -		return;
 +	ret_paddr = __memblock_alloc_base(size, align, end_pfn << PAGE_SHIFT);
  
 -	rtas = of_find_node_by_path("/rtas");
 -	if (!rtas)
 -		return;
 +	/* retry over all memory */
 +	if (!ret_paddr)
 +		ret_paddr = __memblock_alloc_base(size, align, memblock_end_of_DRAM());
  
 -	if (of_property_read_u32_index(rtas,
 -				"ibm,max-associativity-domains",
 -				min_common_depth, &numnodes))
 -		goto out;
 +	if (!ret_paddr)
 +		panic("numa.c: cannot allocate %lu bytes for node %d",
 +		      size, nid);
 +
++<<<<<<< HEAD
 +	ret = __va(ret_paddr);
 +
 +	/*
 +	 * We initialize the nodes in numeric order: 0, 1, 2...
 +	 * and hand over control from the MEMBLOCK allocator to the
 +	 * bootmem allocator.  If this function is called for
 +	 * node 5, then we know that all nodes <5 are using the
 +	 * bootmem allocator instead of the MEMBLOCK allocator.
 +	 *
 +	 * So, check the nid from which this allocation came
 +	 * and double check to see if we need to use bootmem
 +	 * instead of the MEMBLOCK.  We don't free the MEMBLOCK memory
 +	 * since it would be useless.
 +	 */
 +	new_nid = early_pfn_to_nid(ret_paddr >> PAGE_SHIFT);
 +	if (new_nid < nid) {
 +		ret = __alloc_bootmem_node(NODE_DATA(new_nid),
 +				size, align, 0);
 +
 +		dbg("alloc_bootmem %p %lx\n", ret, size);
 +	}
 +
 +	memset(ret, 0, size);
 +	return ret;
 +}
 +
 +static struct notifier_block ppc64_numa_nb = {
 +	.notifier_call = cpu_numa_callback,
 +	.priority = 1 /* Must run before sched domains notifier. */
 +};
  
 +static void __init mark_reserved_regions_for_nid(int nid)
 +{
 +	struct pglist_data *node = NODE_DATA(nid);
 +	struct memblock_region *reg;
 +
 +	for_each_memblock(reserved, reg) {
 +		unsigned long physbase = reg->base;
 +		unsigned long size = reg->size;
 +		unsigned long start_pfn = physbase >> PAGE_SHIFT;
 +		unsigned long end_pfn = PFN_UP(physbase + size);
 +		struct node_active_region node_ar;
 +		unsigned long node_end_pfn = node->node_start_pfn +
 +					     node->node_spanned_pages;
 +
 +		/*
 +		 * Check to make sure that this memblock.reserved area is
 +		 * within the bounds of the node that we care about.
 +		 * Checking the nid of the start and end points is not
 +		 * sufficient because the reserved area could span the
 +		 * entire node.
 +		 */
 +		if (end_pfn <= node->node_start_pfn ||
 +		    start_pfn >= node_end_pfn)
 +			continue;
 +
 +		get_node_active_region(start_pfn, &node_ar);
 +		while (start_pfn < end_pfn &&
 +			node_ar.start_pfn < node_ar.end_pfn) {
 +			unsigned long reserve_size = size;
 +			/*
 +			 * if reserved region extends past active region
 +			 * then trim size to active region
 +			 */
 +			if (end_pfn > node_ar.end_pfn)
 +				reserve_size = (node_ar.end_pfn << PAGE_SHIFT)
 +					- physbase;
 +			/*
 +			 * Only worry about *this* node, others may not
 +			 * yet have valid NODE_DATA().
 +			 */
 +			if (node_ar.nid == nid) {
 +				dbg("reserve_bootmem %lx %lx nid=%d\n",
 +					physbase, reserve_size, node_ar.nid);
 +				reserve_bootmem_node(NODE_DATA(node_ar.nid),
 +						physbase, reserve_size,
 +						BOOTMEM_DEFAULT);
 +			}
 +			/*
 +			 * if reserved region is contained in the active region
 +			 * then done.
 +			 */
 +			if (end_pfn <= node_ar.end_pfn)
 +				break;
 +
 +			/*
 +			 * reserved region extends past the active region
 +			 *   get next active region that contains this
 +			 *   reserved region
 +			 */
 +			start_pfn = node_ar.end_pfn;
 +			physbase = start_pfn << PAGE_SHIFT;
 +			size = size - reserve_size;
 +			get_node_active_region(start_pfn, &node_ar);
 +		}
++=======
+ 	for (i = 0; i < numnodes; i++) {
+ 		if (!node_possible(i))
+ 			node_set(i, node_possible_map);
++>>>>>>> ea05ba7c559c (powerpc/numa: Ensure nodes initialized for hotplug)
  	}
 -
 -out:
 -	of_node_put(rtas);
  }
  
 -void __init initmem_init(void)
 +
 +void __init do_init_bootmem(void)
  {
  	int nid, cpu;
  
* Unmerged path arch/powerpc/mm/numa.c
