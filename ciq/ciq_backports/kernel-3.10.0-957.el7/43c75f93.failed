x86/mm: Streamline and restore probe_memory_block_size()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] revert "mm: Streamline and restore probe_memory_block_size()" (Baoquan He) [1625143]
Rebuild_FUZZ: 88.89%
commit-author Seth Jennings <sjennings@variantweb.net>
commit 43c75f933be26422f166d6d869a19997312f4732
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/43c75f93.failed

The cumulative effect of the following two commits:

  bdee237c0343 ("x86: mm: Use 2GB memory block size on large-memory x86-64 systems")
  982792c782ef ("x86, mm: probe memory block size for generic x86 64bit")

... is some pretty convoluted code.

The first commit also removed code for the UV case without stated reason,
which might lead to unexpected change in behavior.

This commit has no other (intended) functional change; just seeks to simplify
and make the code more understandable, beyond restoring the UV behavior.

The whole section with the "tail size" doesn't seem to be
reachable, since both the >= 64GB and < 64GB case return, so it
was removed.

	Signed-off-by: Seth Jennings <sjennings@variantweb.net>
	Cc: Daniel J Blueman <daniel@numascale.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Yinghai Lu <yinghai@kernel.org>
Link: http://lkml.kernel.org/r/1448902063-18885-1-git-send-email-sjennings@variantweb.net
[ Rewrote the title and changelog. ]
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 43c75f933be26422f166d6d869a19997312f4732)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/init_64.c
diff --cc arch/x86/mm/init_64.c
index 25c65b6af83e,8f18fec74e67..000000000000
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@@ -1172,67 -1192,28 +1172,79 @@@ int kern_addr_valid(unsigned long addr
  	return pfn_valid(pte_pfn(*pte));
  }
  
 -static unsigned long probe_memory_block_size(void)
 +/*
 + * A pseudo VMA to allow ptrace access for the vsyscall page.  This only
 + * covers the 64bit vsyscall page now. 32bit has a real VMA now and does
 + * not need special handling anymore:
 + */
 +static struct vm_area_struct gate_vma = {
 +	.vm_start	= VSYSCALL_START,
 +	.vm_end		= VSYSCALL_START + (VSYSCALL_MAPPED_PAGES * PAGE_SIZE),
 +	.vm_page_prot	= PAGE_READONLY_EXEC,
 +	.vm_flags	= VM_READ | VM_EXEC
 +};
 +
 +struct vm_area_struct *get_gate_vma(struct mm_struct *mm)
  {
++<<<<<<< HEAD
 +#ifdef CONFIG_IA32_EMULATION
 +	if (!mm || mm->context.ia32_compat)
 +		return NULL;
 +#endif
 +	return &gate_vma;
++=======
+ 	unsigned long bz = MIN_MEMORY_BLOCK_SIZE;
+ 
+ 	/* if system is UV or has 64GB of RAM or more, use large blocks */
+ 	if (is_uv_system() || ((max_pfn << PAGE_SHIFT) >= (64UL << 30)))
+ 		bz = 2UL << 30; /* 2GB */
+ 
+ 	pr_info("x86/mm: Memory block size: %ldMB\n", bz >> 20);
+ 
+ 	return bz;
++>>>>>>> 43c75f933be2 (x86/mm: Streamline and restore probe_memory_block_size())
  }
  
 -static unsigned long memory_block_size_probed;
 -unsigned long memory_block_size_bytes(void)
 +int in_gate_area(struct mm_struct *mm, unsigned long addr)
 +{
 +	struct vm_area_struct *vma = get_gate_vma(mm);
 +
 +	if (!vma)
 +		return 0;
 +
 +	return (addr >= vma->vm_start) && (addr < vma->vm_end);
 +}
 +
 +/*
 + * Use this when you have no reliable mm, typically from interrupt
 + * context. It is less reliable than using a task's mm and may give
 + * false positives.
 + */
 +int in_gate_area_no_mm(unsigned long addr)
  {
 -	if (!memory_block_size_probed)
 -		memory_block_size_probed = probe_memory_block_size();
 +	return (addr >= VSYSCALL_START) && (addr < VSYSCALL_END);
 +}
  
 -	return memory_block_size_probed;
 +const char *arch_vma_name(struct vm_area_struct *vma)
 +{
 +	if (vma->vm_mm && vma->vm_start == (long)vma->vm_mm->context.vdso)
 +		return "[vdso]";
 +	if (vma == &gate_vma)
 +		return "[vsyscall]";
 +	return NULL;
  }
  
 +#ifdef CONFIG_X86_UV
 +unsigned long memory_block_size_bytes(void)
 +{
 +	if (is_uv_system()) {
 +		printk(KERN_INFO "UV: memory block size 2GB\n");
 +		return 2UL * 1024 * 1024 * 1024;
 +	}
 +	return MIN_MEMORY_BLOCK_SIZE;
 +}
 +#endif
 +
  #ifdef CONFIG_SPARSEMEM_VMEMMAP
  /*
   * Initialise the sparsemem vmemmap using huge-pages at the PMD level.
* Unmerged path arch/x86/mm/init_64.c
