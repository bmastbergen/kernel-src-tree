bpf/tracing: fix a deadlock in perf_event_detach_bpf_prog

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Yonghong Song <yhs@fb.com>
commit 3a38bb98d9abdc3856f26b5ed4332803065cd7cf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/3a38bb98.failed

syzbot reported a possible deadlock in perf_event_detach_bpf_prog.
The error details:
  ======================================================
  WARNING: possible circular locking dependency detected
  4.16.0-rc7+ #3 Not tainted
  ------------------------------------------------------
  syz-executor7/24531 is trying to acquire lock:
   (bpf_event_mutex){+.+.}, at: [<000000008a849b07>] perf_event_detach_bpf_prog+0x92/0x3d0 kernel/trace/bpf_trace.c:854

  but task is already holding lock:
   (&mm->mmap_sem){++++}, at: [<0000000038768f87>] vm_mmap_pgoff+0x198/0x280 mm/util.c:353

  which lock already depends on the new lock.

  the existing dependency chain (in reverse order) is:

  -> #1 (&mm->mmap_sem){++++}:
       __might_fault+0x13a/0x1d0 mm/memory.c:4571
       _copy_to_user+0x2c/0xc0 lib/usercopy.c:25
       copy_to_user include/linux/uaccess.h:155 [inline]
       bpf_prog_array_copy_info+0xf2/0x1c0 kernel/bpf/core.c:1694
       perf_event_query_prog_array+0x1c7/0x2c0 kernel/trace/bpf_trace.c:891
       _perf_ioctl kernel/events/core.c:4750 [inline]
       perf_ioctl+0x3e1/0x1480 kernel/events/core.c:4770
       vfs_ioctl fs/ioctl.c:46 [inline]
       do_vfs_ioctl+0x1b1/0x1520 fs/ioctl.c:686
       SYSC_ioctl fs/ioctl.c:701 [inline]
       SyS_ioctl+0x8f/0xc0 fs/ioctl.c:692
       do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
       entry_SYSCALL_64_after_hwframe+0x42/0xb7

  -> #0 (bpf_event_mutex){+.+.}:
       lock_acquire+0x1d5/0x580 kernel/locking/lockdep.c:3920
       __mutex_lock_common kernel/locking/mutex.c:756 [inline]
       __mutex_lock+0x16f/0x1a80 kernel/locking/mutex.c:893
       mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:908
       perf_event_detach_bpf_prog+0x92/0x3d0 kernel/trace/bpf_trace.c:854
       perf_event_free_bpf_prog kernel/events/core.c:8147 [inline]
       _free_event+0xbdb/0x10f0 kernel/events/core.c:4116
       put_event+0x24/0x30 kernel/events/core.c:4204
       perf_mmap_close+0x60d/0x1010 kernel/events/core.c:5172
       remove_vma+0xb4/0x1b0 mm/mmap.c:172
       remove_vma_list mm/mmap.c:2490 [inline]
       do_munmap+0x82a/0xdf0 mm/mmap.c:2731
       mmap_region+0x59e/0x15a0 mm/mmap.c:1646
       do_mmap+0x6c0/0xe00 mm/mmap.c:1483
       do_mmap_pgoff include/linux/mm.h:2223 [inline]
       vm_mmap_pgoff+0x1de/0x280 mm/util.c:355
       SYSC_mmap_pgoff mm/mmap.c:1533 [inline]
       SyS_mmap_pgoff+0x462/0x5f0 mm/mmap.c:1491
       SYSC_mmap arch/x86/kernel/sys_x86_64.c:100 [inline]
       SyS_mmap+0x16/0x20 arch/x86/kernel/sys_x86_64.c:91
       do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
       entry_SYSCALL_64_after_hwframe+0x42/0xb7

  other info that might help us debug this:

   Possible unsafe locking scenario:

         CPU0                    CPU1
         ----                    ----
    lock(&mm->mmap_sem);
                                 lock(bpf_event_mutex);
                                 lock(&mm->mmap_sem);
    lock(bpf_event_mutex);

   *** DEADLOCK ***
  ======================================================

The bug is introduced by Commit f371b304f12e ("bpf/tracing: allow
user space to query prog array on the same tp") where copy_to_user,
which requires mm->mmap_sem, is called inside bpf_event_mutex lock.
At the same time, during perf_event file descriptor close,
mm->mmap_sem is held first and then subsequent
perf_event_detach_bpf_prog needs bpf_event_mutex lock.
Such a senario caused a deadlock.

As suggested by Daniel, moving copy_to_user out of the
bpf_event_mutex lock should fix the problem.

Fixes: f371b304f12e ("bpf/tracing: allow user space to query prog array on the same tp")
	Reported-by: syzbot+dc5ca0e4c9bfafaf2bae@syzkaller.appspotmail.com
	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 3a38bb98d9abdc3856f26b5ed4332803065cd7cf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	kernel/bpf/core.c
#	kernel/trace/bpf_trace.c
diff --cc include/linux/bpf.h
index d4c1f9049ad3,486e65e3db26..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -103,14 -244,442 +103,429 @@@ struct bpf_prog
  
  struct bpf_prog_aux {
  	atomic_t refcnt;
 -	u32 used_map_cnt;
 -	u32 max_ctx_offset;
 -	u32 stack_depth;
 +	bool is_gpl_compatible;
 +	enum bpf_prog_type prog_type;
 +	struct bpf_verifier_ops *ops;
  	u32 id;
 -	u32 func_cnt;
 -	bool offload_requested;
 -	struct bpf_prog **func;
 -	void *jit_data; /* JIT specific data. arch dependent */
 -	struct latch_tree_node ksym_tnode;
 -	struct list_head ksym_lnode;
 -	const struct bpf_prog_ops *ops;
  	struct bpf_map **used_maps;
 +	u32 used_map_cnt;
  	struct bpf_prog *prog;
 -	struct user_struct *user;
 -	u64 load_time; /* ns since boottime */
 -	char name[BPF_OBJ_NAME_LEN];
 -#ifdef CONFIG_SECURITY
 -	void *security;
 -#endif
 -	struct bpf_prog_offload *offload;
 -	union {
 -		struct work_struct work;
 -		struct rcu_head	rcu;
 -	};
 +	struct work_struct work;
  };
  
++<<<<<<< HEAD
++=======
+ struct bpf_array {
+ 	struct bpf_map map;
+ 	u32 elem_size;
+ 	u32 index_mask;
+ 	/* 'ownership' of prog_array is claimed by the first program that
+ 	 * is going to use this map or by the first program which FD is stored
+ 	 * in the map to make sure that all callers and callees have the same
+ 	 * prog_type and JITed flag
+ 	 */
+ 	enum bpf_prog_type owner_prog_type;
+ 	bool owner_jited;
+ 	union {
+ 		char value[0] __aligned(8);
+ 		void *ptrs[0] __aligned(8);
+ 		void __percpu *pptrs[0] __aligned(8);
+ 	};
+ };
+ 
+ #define MAX_TAIL_CALL_CNT 32
+ 
+ struct bpf_event_entry {
+ 	struct perf_event *event;
+ 	struct file *perf_file;
+ 	struct file *map_file;
+ 	struct rcu_head rcu;
+ };
+ 
+ bool bpf_prog_array_compatible(struct bpf_array *array, const struct bpf_prog *fp);
+ int bpf_prog_calc_tag(struct bpf_prog *fp);
+ 
+ const struct bpf_func_proto *bpf_get_trace_printk_proto(void);
+ 
+ typedef unsigned long (*bpf_ctx_copy_t)(void *dst, const void *src,
+ 					unsigned long off, unsigned long len);
+ 
+ u64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,
+ 		     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy);
+ 
+ int bpf_prog_test_run_xdp(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ int bpf_prog_test_run_skb(struct bpf_prog *prog, const union bpf_attr *kattr,
+ 			  union bpf_attr __user *uattr);
+ 
+ /* an array of programs to be executed under rcu_lock.
+  *
+  * Typical usage:
+  * ret = BPF_PROG_RUN_ARRAY(&bpf_prog_array, ctx, BPF_PROG_RUN);
+  *
+  * the structure returned by bpf_prog_array_alloc() should be populated
+  * with program pointers and the last pointer must be NULL.
+  * The user has to keep refcnt on the program and make sure the program
+  * is removed from the array before bpf_prog_put().
+  * The 'struct bpf_prog_array *' should only be replaced with xchg()
+  * since other cpus are walking the array of pointers in parallel.
+  */
+ struct bpf_prog_array {
+ 	struct rcu_head rcu;
+ 	struct bpf_prog *progs[0];
+ };
+ 
+ struct bpf_prog_array __rcu *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags);
+ void bpf_prog_array_free(struct bpf_prog_array __rcu *progs);
+ int bpf_prog_array_length(struct bpf_prog_array __rcu *progs);
+ int bpf_prog_array_copy_to_user(struct bpf_prog_array __rcu *progs,
+ 				__u32 __user *prog_ids, u32 cnt);
+ 
+ void bpf_prog_array_delete_safe(struct bpf_prog_array __rcu *progs,
+ 				struct bpf_prog *old_prog);
+ int bpf_prog_array_copy_info(struct bpf_prog_array __rcu *array,
+ 			     u32 *prog_ids, u32 request_cnt,
+ 			     u32 *prog_cnt);
+ int bpf_prog_array_copy(struct bpf_prog_array __rcu *old_array,
+ 			struct bpf_prog *exclude_prog,
+ 			struct bpf_prog *include_prog,
+ 			struct bpf_prog_array **new_array);
+ 
+ #define __BPF_PROG_RUN_ARRAY(array, ctx, func, check_non_null)	\
+ 	({						\
+ 		struct bpf_prog **_prog, *__prog;	\
+ 		struct bpf_prog_array *_array;		\
+ 		u32 _ret = 1;				\
+ 		rcu_read_lock();			\
+ 		_array = rcu_dereference(array);	\
+ 		if (unlikely(check_non_null && !_array))\
+ 			goto _out;			\
+ 		_prog = _array->progs;			\
+ 		while ((__prog = READ_ONCE(*_prog))) {	\
+ 			_ret &= func(__prog, ctx);	\
+ 			_prog++;			\
+ 		}					\
+ _out:							\
+ 		rcu_read_unlock();			\
+ 		_ret;					\
+ 	 })
+ 
+ #define BPF_PROG_RUN_ARRAY(array, ctx, func)		\
+ 	__BPF_PROG_RUN_ARRAY(array, ctx, func, false)
+ 
+ #define BPF_PROG_RUN_ARRAY_CHECK(array, ctx, func)	\
+ 	__BPF_PROG_RUN_ARRAY(array, ctx, func, true)
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ DECLARE_PER_CPU(int, bpf_prog_active);
+ 
+ extern const struct file_operations bpf_map_fops;
+ extern const struct file_operations bpf_prog_fops;
+ 
+ #define BPF_PROG_TYPE(_id, _name) \
+ 	extern const struct bpf_prog_ops _name ## _prog_ops; \
+ 	extern const struct bpf_verifier_ops _name ## _verifier_ops;
+ #define BPF_MAP_TYPE(_id, _ops) \
+ 	extern const struct bpf_map_ops _ops;
+ #include <linux/bpf_types.h>
+ #undef BPF_PROG_TYPE
+ #undef BPF_MAP_TYPE
+ 
+ extern const struct bpf_prog_ops bpf_offload_prog_ops;
+ extern const struct bpf_verifier_ops tc_cls_act_analyzer_ops;
+ extern const struct bpf_verifier_ops xdp_analyzer_ops;
+ 
+ struct bpf_prog *bpf_prog_get(u32 ufd);
+ struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
+ 				       bool attach_drv);
+ struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog, int i);
+ void bpf_prog_sub(struct bpf_prog *prog, int i);
+ struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog);
+ struct bpf_prog * __must_check bpf_prog_inc_not_zero(struct bpf_prog *prog);
+ void bpf_prog_put(struct bpf_prog *prog);
+ int __bpf_prog_charge(struct user_struct *user, u32 pages);
+ void __bpf_prog_uncharge(struct user_struct *user, u32 pages);
+ 
+ void bpf_prog_free_id(struct bpf_prog *prog, bool do_idr_lock);
+ void bpf_map_free_id(struct bpf_map *map, bool do_idr_lock);
+ 
+ struct bpf_map *bpf_map_get_with_uref(u32 ufd);
+ struct bpf_map *__bpf_map_get(struct fd f);
+ struct bpf_map * __must_check bpf_map_inc(struct bpf_map *map, bool uref);
+ void bpf_map_put_with_uref(struct bpf_map *map);
+ void bpf_map_put(struct bpf_map *map);
+ int bpf_map_precharge_memlock(u32 pages);
+ void *bpf_map_area_alloc(size_t size, int numa_node);
+ void bpf_map_area_free(void *base);
+ void bpf_map_init_from_attr(struct bpf_map *map, union bpf_attr *attr);
+ 
+ extern int sysctl_unprivileged_bpf_disabled;
+ 
+ int bpf_map_new_fd(struct bpf_map *map, int flags);
+ int bpf_prog_new_fd(struct bpf_prog *prog);
+ 
+ int bpf_obj_pin_user(u32 ufd, const char __user *pathname);
+ int bpf_obj_get_user(const char __user *pathname, int flags);
+ 
+ int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
+ int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,
+ 			   u64 flags);
+ int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,
+ 			    u64 flags);
+ 
+ int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value);
+ 
+ int bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				 void *key, void *value, u64 map_flags);
+ int bpf_fd_array_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ void bpf_fd_array_map_clear(struct bpf_map *map);
+ int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,
+ 				void *key, void *value, u64 map_flags);
+ int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);
+ 
+ int bpf_get_file_flag(int flags);
+ 
+ /* memcpy that is used with 8-byte aligned pointers, power-of-8 size and
+  * forced to use 'long' read/writes to try to atomically copy long counters.
+  * Best-effort only.  No barriers here, since it _will_ race with concurrent
+  * updates from BPF programs. Called from bpf syscall and mostly used with
+  * size 8 or 16 bytes, so ask compiler to inline it.
+  */
+ static inline void bpf_long_memcpy(void *dst, const void *src, u32 size)
+ {
+ 	const long *lsrc = src;
+ 	long *ldst = dst;
+ 
+ 	size /= sizeof(long);
+ 	while (size--)
+ 		*ldst++ = *lsrc++;
+ }
+ 
+ /* verify correctness of eBPF program */
+ int bpf_check(struct bpf_prog **fp, union bpf_attr *attr);
+ void bpf_patch_call_args(struct bpf_insn *insn, u32 stack_depth);
+ 
+ /* Map specifics */
+ struct net_device  *__dev_map_lookup_elem(struct bpf_map *map, u32 key);
+ void __dev_map_insert_ctx(struct bpf_map *map, u32 index);
+ void __dev_map_flush(struct bpf_map *map);
+ 
+ struct bpf_cpu_map_entry *__cpu_map_lookup_elem(struct bpf_map *map, u32 key);
+ void __cpu_map_insert_ctx(struct bpf_map *map, u32 index);
+ void __cpu_map_flush(struct bpf_map *map);
+ struct xdp_buff;
+ int cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_buff *xdp,
+ 		    struct net_device *dev_rx);
+ 
+ /* Return map's numa specified by userspace */
+ static inline int bpf_map_attr_numa_node(const union bpf_attr *attr)
+ {
+ 	return (attr->map_flags & BPF_F_NUMA_NODE) ?
+ 		attr->numa_node : NUMA_NO_NODE;
+ }
+ 
+ struct bpf_prog *bpf_prog_get_type_path(const char *name, enum bpf_prog_type type);
+ 
+ #else /* !CONFIG_BPF_SYSCALL */
+ static inline struct bpf_prog *bpf_prog_get(u32 ufd)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get_type_dev(u32 ufd,
+ 						     enum bpf_prog_type type,
+ 						     bool attach_drv)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog * __must_check bpf_prog_add(struct bpf_prog *prog,
+ 							  int i)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void bpf_prog_sub(struct bpf_prog *prog, int i)
+ {
+ }
+ 
+ static inline void bpf_prog_put(struct bpf_prog *prog)
+ {
+ }
+ 
+ static inline struct bpf_prog * __must_check bpf_prog_inc(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline struct bpf_prog *__must_check
+ bpf_prog_inc_not_zero(struct bpf_prog *prog)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline int __bpf_prog_charge(struct user_struct *user, u32 pages)
+ {
+ 	return 0;
+ }
+ 
+ static inline void __bpf_prog_uncharge(struct user_struct *user, u32 pages)
+ {
+ }
+ 
+ static inline int bpf_obj_get_user(const char __user *pathname, int flags)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static inline struct net_device  *__dev_map_lookup_elem(struct bpf_map *map,
+ 						       u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void __dev_map_insert_ctx(struct bpf_map *map, u32 index)
+ {
+ }
+ 
+ static inline void __dev_map_flush(struct bpf_map *map)
+ {
+ }
+ 
+ static inline
+ struct bpf_cpu_map_entry *__cpu_map_lookup_elem(struct bpf_map *map, u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline void __cpu_map_insert_ctx(struct bpf_map *map, u32 index)
+ {
+ }
+ 
+ static inline void __cpu_map_flush(struct bpf_map *map)
+ {
+ }
+ 
+ struct xdp_buff;
+ static inline int cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu,
+ 				  struct xdp_buff *xdp,
+ 				  struct net_device *dev_rx)
+ {
+ 	return 0;
+ }
+ 
+ static inline struct bpf_prog *bpf_prog_get_type_path(const char *name,
+ 				enum bpf_prog_type type)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ #endif /* CONFIG_BPF_SYSCALL */
+ 
+ static inline struct bpf_prog *bpf_prog_get_type(u32 ufd,
+ 						 enum bpf_prog_type type)
+ {
+ 	return bpf_prog_get_type_dev(ufd, type, false);
+ }
+ 
+ bool bpf_prog_get_ok(struct bpf_prog *, enum bpf_prog_type *, bool);
+ 
+ int bpf_prog_offload_compile(struct bpf_prog *prog);
+ void bpf_prog_offload_destroy(struct bpf_prog *prog);
+ int bpf_prog_offload_info_fill(struct bpf_prog_info *info,
+ 			       struct bpf_prog *prog);
+ 
+ int bpf_map_offload_info_fill(struct bpf_map_info *info, struct bpf_map *map);
+ 
+ int bpf_map_offload_lookup_elem(struct bpf_map *map, void *key, void *value);
+ int bpf_map_offload_update_elem(struct bpf_map *map,
+ 				void *key, void *value, u64 flags);
+ int bpf_map_offload_delete_elem(struct bpf_map *map, void *key);
+ int bpf_map_offload_get_next_key(struct bpf_map *map,
+ 				 void *key, void *next_key);
+ 
+ bool bpf_offload_dev_match(struct bpf_prog *prog, struct bpf_map *map);
+ 
+ #if defined(CONFIG_NET) && defined(CONFIG_BPF_SYSCALL)
+ int bpf_prog_offload_init(struct bpf_prog *prog, union bpf_attr *attr);
+ 
+ static inline bool bpf_prog_is_dev_bound(struct bpf_prog_aux *aux)
+ {
+ 	return aux->offload_requested;
+ }
+ 
+ static inline bool bpf_map_is_dev_bound(struct bpf_map *map)
+ {
+ 	return unlikely(map->ops == &bpf_map_offload_ops);
+ }
+ 
+ struct bpf_map *bpf_map_offload_map_alloc(union bpf_attr *attr);
+ void bpf_map_offload_map_free(struct bpf_map *map);
+ #else
+ static inline int bpf_prog_offload_init(struct bpf_prog *prog,
+ 					union bpf_attr *attr)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static inline bool bpf_prog_is_dev_bound(struct bpf_prog_aux *aux)
+ {
+ 	return false;
+ }
+ 
+ static inline bool bpf_map_is_dev_bound(struct bpf_map *map)
+ {
+ 	return false;
+ }
+ 
+ static inline struct bpf_map *bpf_map_offload_map_alloc(union bpf_attr *attr)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void bpf_map_offload_map_free(struct bpf_map *map)
+ {
+ }
+ #endif /* CONFIG_NET && CONFIG_BPF_SYSCALL */
+ 
+ #if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_INET)
+ struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key);
+ int sock_map_prog(struct bpf_map *map, struct bpf_prog *prog, u32 type);
+ #else
+ static inline struct sock  *__sock_map_lookup_elem(struct bpf_map *map, u32 key)
+ {
+ 	return NULL;
+ }
+ 
+ static inline int sock_map_prog(struct bpf_map *map,
+ 				struct bpf_prog *prog,
+ 				u32 type)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ #endif
+ 
+ /* verifier prototypes for helper functions called from eBPF programs */
+ extern const struct bpf_func_proto bpf_map_lookup_elem_proto;
+ extern const struct bpf_func_proto bpf_map_update_elem_proto;
+ extern const struct bpf_func_proto bpf_map_delete_elem_proto;
+ 
+ extern const struct bpf_func_proto bpf_get_prandom_u32_proto;
+ extern const struct bpf_func_proto bpf_get_smp_processor_id_proto;
+ extern const struct bpf_func_proto bpf_get_numa_node_id_proto;
+ extern const struct bpf_func_proto bpf_tail_call_proto;
+ extern const struct bpf_func_proto bpf_ktime_get_ns_proto;
+ extern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;
+ extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
+ extern const struct bpf_func_proto bpf_get_current_comm_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_push_proto;
+ extern const struct bpf_func_proto bpf_skb_vlan_pop_proto;
+ extern const struct bpf_func_proto bpf_get_stackid_proto;
+ extern const struct bpf_func_proto bpf_sock_map_update_proto;
+ 
+ /* Shared helpers among cBPF and eBPF. */
+ void bpf_user_rnd_init_once(void);
+ u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+ 
++>>>>>>> 3a38bb98d9ab (bpf/tracing: fix a deadlock in perf_event_detach_bpf_prog)
  #endif /* _LINUX_BPF_H */
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/trace/bpf_trace.c
* Unmerged path include/linux/bpf.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/trace/bpf_trace.c
