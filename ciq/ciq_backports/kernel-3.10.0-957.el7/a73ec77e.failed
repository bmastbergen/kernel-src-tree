x86/speculation: Add prctl for Speculative Store Bypass mitigation

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] speculation: Add prctl for Speculative Store Bypass mitigation (Waiman Long) [1566905] {CVE-2018-3639}
Rebuild_FUZZ: 96.88%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit a73ec77ee17ec556fe7f165d00314cb7c047b1ac
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a73ec77e.failed

Add prctl based control for Speculative Store Bypass mitigation and make it
the default mitigation for Intel and AMD.

Andi Kleen provided the following rationale (slightly redacted):

 There are multiple levels of impact of Speculative Store Bypass:

 1) JITed sandbox.
    It cannot invoke system calls, but can do PRIME+PROBE and may have call
    interfaces to other code

 2) Native code process.
    No protection inside the process at this level.

 3) Kernel.

 4) Between processes. 

 The prctl tries to protect against case (1) doing attacks.

 If the untrusted code can do random system calls then control is already
 lost in a much worse way. So there needs to be system call protection in
 some way (using a JIT not allowing them or seccomp). Or rather if the
 process can subvert its environment somehow to do the prctl it can already
 execute arbitrary code, which is much worse than SSB.

 To put it differently, the point of the prctl is to not allow JITed code
 to read data it shouldn't read from its JITed sandbox. If it already has
 escaped its sandbox then it can already read everything it wants in its
 address space, and do much worse.

 The ability to control Speculative Store Bypass allows to enable the
 protection selectively without affecting overall system performance.

Based on an initial patch from Tim Chen. Completely rewritten.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit a73ec77ee17ec556fe7f165d00314cb7c047b1ac)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/kernel-parameters.txt
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
diff --cc Documentation/kernel-parameters.txt
index a0f1f1428af0,a8d2ae1e335b..000000000000
--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@@ -3222,6 -4028,40 +3222,43 @@@ bytes respectively. Such letter suffixe
  			Not specifying this option is equivalent to
  			spectre_v2=auto.
  
++<<<<<<< HEAD:Documentation/kernel-parameters.txt
++=======
+ 	spec_store_bypass_disable=
+ 			[HW] Control Speculative Store Bypass (SSB) Disable mitigation
+ 			(Speculative Store Bypass vulnerability)
+ 
+ 			Certain CPUs are vulnerable to an exploit against a
+ 			a common industry wide performance optimization known
+ 			as "Speculative Store Bypass" in which recent stores
+ 			to the same memory location may not be observed by
+ 			later loads during speculative execution. The idea
+ 			is that such stores are unlikely and that they can
+ 			be detected prior to instruction retirement at the
+ 			end of a particular speculation execution window.
+ 
+ 			In vulnerable processors, the speculatively forwarded
+ 			store can be used in a cache side channel attack, for
+ 			example to read memory to which the attacker does not
+ 			directly have access (e.g. inside sandboxed code).
+ 
+ 			This parameter controls whether the Speculative Store
+ 			Bypass optimization is used.
+ 
+ 			on     - Unconditionally disable Speculative Store Bypass
+ 			off    - Unconditionally enable Speculative Store Bypass
+ 			auto   - Kernel detects whether the CPU model contains an
+ 				 implementation of Speculative Store Bypass and
+ 				 picks the most appropriate mitigation.
+ 			prctl  - Control Speculative Store Bypass per thread
+ 				 via prctl. Speculative Store Bypass is enabled
+ 				 for a process by default. The state of the control
+ 				 is inherited on fork.
+ 
+ 			Not specifying this option is equivalent to
+ 			spec_store_bypass_disable=auto.
+ 
++>>>>>>> a73ec77ee17e (x86/speculation: Add prctl for Speculative Store Bypass mitigation):Documentation/admin-guide/kernel-parameters.txt
  	spia_io_base=	[HW,MTD]
  	spia_fio_base=
  	spia_pedr=
diff --cc arch/x86/include/asm/nospec-branch.h
index d5b6abbaaa24,71ad01422655..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -169,27 -211,32 +169,39 @@@
  enum spectre_v2_mitigation {
  	SPECTRE_V2_NONE,
  	SPECTRE_V2_RETPOLINE_MINIMAL,
 -	SPECTRE_V2_RETPOLINE_MINIMAL_AMD,
 -	SPECTRE_V2_RETPOLINE_GENERIC,
 -	SPECTRE_V2_RETPOLINE_AMD,
 +	SPECTRE_V2_RETPOLINE_NO_IBPB,
 +	SPECTRE_V2_RETPOLINE_SKYLAKE,
 +	SPECTRE_V2_RETPOLINE_UNSAFE_MODULE,
 +	SPECTRE_V2_RETPOLINE,
 +	SPECTRE_V2_RETPOLINE_IBRS_USER,
  	SPECTRE_V2_IBRS,
 +	SPECTRE_V2_IBRS_ALWAYS,
 +	SPECTRE_V2_IBP_DISABLED,
  };
  
 -/*
 - * The Intel specification for the SPEC_CTRL MSR requires that we
 - * preserve any already set reserved bits at boot time (e.g. for
 - * future additions that this kernel is not currently aware of).
 - * We then set any additional mitigation bits that we want
 - * ourselves and always use this as the base for SPEC_CTRL.
 - * We also use this when handling guest entry/exit as below.
 - */
 -extern void x86_spec_ctrl_set(u64);
 -extern u64 x86_spec_ctrl_get_default(void);
 +void __spectre_v2_select_mitigation(void);
 +void spectre_v2_print_mitigation(void);
  
++<<<<<<< HEAD
 +static inline bool retp_compiler(void)
 +{
 +#ifdef RETPOLINE
 +	return true;
 +#else
 +	return false;
 +#endif
 +}
++=======
+ /* The Speculative Store Bypass disable variants */
+ enum ssb_mitigation {
+ 	SPEC_STORE_BYPASS_NONE,
+ 	SPEC_STORE_BYPASS_DISABLE,
+ 	SPEC_STORE_BYPASS_PRCTL,
+ };
+ 
+ extern char __indirect_thunk_start[];
+ extern char __indirect_thunk_end[];
++>>>>>>> a73ec77ee17e (x86/speculation: Add prctl for Speculative Store Bypass mitigation)
  
  /*
   * On VMEXIT we must ensure that no RSB predictions learned in the guest
diff --cc arch/x86/kernel/cpu/bugs.c
index 29b8876b1f95,fc9187b6fae7..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -10,8 -11,11 +10,14 @@@
  #include <linux/init.h>
  #include <linux/utsname.h>
  #include <linux/cpu.h>
++<<<<<<< HEAD
++=======
+ #include <linux/module.h>
+ #include <linux/nospec.h>
+ #include <linux/prctl.h>
++>>>>>>> a73ec77ee17e (x86/speculation: Add prctl for Speculative Store Bypass mitigation)
  
 -#include <asm/spec-ctrl.h>
 +#include <asm/nospec-branch.h>
  #include <asm/cmdline.h>
  #include <asm/bugs.h>
  #include <asm/processor.h>
@@@ -199,15 -405,228 +205,208 @@@ static void __init spectre_v2_select_mi
  }
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Store Bypass: " fmt
+ 
+ static enum ssb_mitigation ssb_mode = SPEC_STORE_BYPASS_NONE;
+ 
+ /* The kernel command line selection */
+ enum ssb_mitigation_cmd {
+ 	SPEC_STORE_BYPASS_CMD_NONE,
+ 	SPEC_STORE_BYPASS_CMD_AUTO,
+ 	SPEC_STORE_BYPASS_CMD_ON,
+ 	SPEC_STORE_BYPASS_CMD_PRCTL,
+ };
+ 
+ static const char *ssb_strings[] = {
+ 	[SPEC_STORE_BYPASS_NONE]	= "Vulnerable",
+ 	[SPEC_STORE_BYPASS_DISABLE]	= "Mitigation: Speculative Store Bypass disabled",
+ 	[SPEC_STORE_BYPASS_PRCTL]	= "Mitigation: Speculative Store Bypass disabled via prctl"
+ };
+ 
+ static const struct {
+ 	const char *option;
+ 	enum ssb_mitigation_cmd cmd;
+ } ssb_mitigation_options[] = {
+ 	{ "auto",	SPEC_STORE_BYPASS_CMD_AUTO },  /* Platform decides */
+ 	{ "on",		SPEC_STORE_BYPASS_CMD_ON },    /* Disable Speculative Store Bypass */
+ 	{ "off",	SPEC_STORE_BYPASS_CMD_NONE },  /* Don't touch Speculative Store Bypass */
+ 	{ "prctl",	SPEC_STORE_BYPASS_CMD_PRCTL }, /* Disable Speculative Store Bypass via prctl */
+ };
+ 
+ static enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)
+ {
+ 	enum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;
+ 	char arg[20];
+ 	int ret, i;
+ 
+ 	if (cmdline_find_option_bool(boot_command_line, "nospec_store_bypass_disable")) {
+ 		return SPEC_STORE_BYPASS_CMD_NONE;
+ 	} else {
+ 		ret = cmdline_find_option(boot_command_line, "spec_store_bypass_disable",
+ 					  arg, sizeof(arg));
+ 		if (ret < 0)
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 
+ 		for (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {
+ 			if (!match_option(arg, ret, ssb_mitigation_options[i].option))
+ 				continue;
+ 
+ 			cmd = ssb_mitigation_options[i].cmd;
+ 			break;
+ 		}
+ 
+ 		if (i >= ARRAY_SIZE(ssb_mitigation_options)) {
+ 			pr_err("unknown option (%s). Switching to AUTO select\n", arg);
+ 			return SPEC_STORE_BYPASS_CMD_AUTO;
+ 		}
+ 	}
+ 
+ 	return cmd;
+ }
+ 
+ static enum ssb_mitigation_cmd __init __ssb_select_mitigation(void)
+ {
+ 	enum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;
+ 	enum ssb_mitigation_cmd cmd;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_RDS))
+ 		return mode;
+ 
+ 	cmd = ssb_parse_cmdline();
+ 	if (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&
+ 	    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||
+ 	     cmd == SPEC_STORE_BYPASS_CMD_AUTO))
+ 		return mode;
+ 
+ 	switch (cmd) {
+ 	case SPEC_STORE_BYPASS_CMD_AUTO:
+ 		/* Choose prctl as the default mode */
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_ON:
+ 		mode = SPEC_STORE_BYPASS_DISABLE;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_PRCTL:
+ 		mode = SPEC_STORE_BYPASS_PRCTL;
+ 		break;
+ 	case SPEC_STORE_BYPASS_CMD_NONE:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * We have three CPU feature flags that are in play here:
+ 	 *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.
+ 	 *  - X86_FEATURE_RDS - CPU is able to turn off speculative store bypass
+ 	 *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation
+ 	 */
+ 	if (mode == SPEC_STORE_BYPASS_DISABLE) {
+ 		setup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);
+ 		/*
+ 		 * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD uses
+ 		 * a completely different MSR and bit dependent on family.
+ 		 */
+ 		switch (boot_cpu_data.x86_vendor) {
+ 		case X86_VENDOR_INTEL:
+ 			x86_spec_ctrl_base |= SPEC_CTRL_RDS;
+ 			x86_spec_ctrl_mask &= ~SPEC_CTRL_RDS;
+ 			x86_spec_ctrl_set(SPEC_CTRL_RDS);
+ 			break;
+ 		case X86_VENDOR_AMD:
+ 			x86_amd_rds_enable();
+ 			break;
+ 		}
+ 	}
+ 
+ 	return mode;
+ }
+ 
+ static void ssb_select_mitigation()
+ {
+ 	ssb_mode = __ssb_select_mitigation();
+ 
+ 	if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 		pr_info("%s\n", ssb_strings[ssb_mode]);
+ }
+ 
+ #undef pr_fmt
+ 
+ static int ssb_prctl_set(unsigned long ctrl)
+ {
+ 	bool rds = !!test_tsk_thread_flag(current, TIF_RDS);
+ 
+ 	if (ssb_mode != SPEC_STORE_BYPASS_PRCTL)
+ 		return -ENXIO;
+ 
+ 	if (ctrl == PR_SPEC_ENABLE)
+ 		clear_tsk_thread_flag(current, TIF_RDS);
+ 	else
+ 		set_tsk_thread_flag(current, TIF_RDS);
+ 
+ 	if (rds != !!test_tsk_thread_flag(current, TIF_RDS))
+ 		speculative_store_bypass_update();
+ 
+ 	return 0;
+ }
+ 
+ static int ssb_prctl_get(void)
+ {
+ 	switch (ssb_mode) {
+ 	case SPEC_STORE_BYPASS_DISABLE:
+ 		return PR_SPEC_DISABLE;
+ 	case SPEC_STORE_BYPASS_PRCTL:
+ 		if (test_tsk_thread_flag(current, TIF_RDS))
+ 			return PR_SPEC_PRCTL | PR_SPEC_DISABLE;
+ 		return PR_SPEC_PRCTL | PR_SPEC_ENABLE;
+ 	default:
+ 		if (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))
+ 			return PR_SPEC_ENABLE;
+ 		return PR_SPEC_NOT_AFFECTED;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_set(unsigned long which, unsigned long ctrl)
+ {
+ 	if (ctrl != PR_SPEC_ENABLE && ctrl != PR_SPEC_DISABLE)
+ 		return -ERANGE;
+ 
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_set(ctrl);
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ int arch_prctl_spec_ctrl_get(unsigned long which)
+ {
+ 	switch (which) {
+ 	case PR_SPEC_STORE_BYPASS:
+ 		return ssb_prctl_get();
+ 	default:
+ 		return -ENODEV;
+ 	}
+ }
+ 
+ void x86_spec_ctrl_setup_ap(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_IBRS))
+ 		x86_spec_ctrl_set(x86_spec_ctrl_base & ~x86_spec_ctrl_mask);
+ 
+ 	if (ssb_mode == SPEC_STORE_BYPASS_DISABLE)
+ 		x86_amd_rds_enable();
+ }
++>>>>>>> a73ec77ee17e (x86/speculation: Add prctl for Speculative Store Bypass mitigation)
  
  #ifdef CONFIG_SYSFS
 -
 -ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
 -			char *buf, unsigned int bug)
 +ssize_t cpu_show_meltdown(struct device *dev,
 +			  struct device_attribute *attr, char *buf)
  {
 -	if (!boot_cpu_has_bug(bug))
 +	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
  		return sprintf(buf, "Not affected\n");
 -
 -	switch (bug) {
 -	case X86_BUG_CPU_MELTDOWN:
 -		if (boot_cpu_has(X86_FEATURE_PTI))
 -			return sprintf(buf, "Mitigation: PTI\n");
 -
 -		break;
 -
 -	case X86_BUG_SPECTRE_V1:
 -		return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 -
 -	case X86_BUG_SPECTRE_V2:
 -		return sprintf(buf, "%s%s%s%s\n", spectre_v2_strings[spectre_v2_enabled],
 -			       boot_cpu_has(X86_FEATURE_USE_IBPB) ? ", IBPB" : "",
 -			       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 -			       spectre_v2_module_string());
 -
 -	case X86_BUG_SPEC_STORE_BYPASS:
 -		return sprintf(buf, "%s\n", ssb_strings[ssb_mode]);
 -
 -	default:
 -		break;
 -	}
 -
 +	if (kaiser_enabled)
 +		return sprintf(buf, "Mitigation: PTI\n");
  	return sprintf(buf, "Vulnerable\n");
  }
  
* Unmerged path Documentation/kernel-parameters.txt
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
