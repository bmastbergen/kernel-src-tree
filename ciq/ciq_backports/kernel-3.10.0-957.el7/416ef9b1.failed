net: sched: red: don't reset the backlog on every stat dump

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [net] sched: red: don't reset the backlog on every stat dump (Ivan Vecera) [1583702]
Rebuild_FUZZ: 95.58%
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 416ef9b15c688b91edbf654ebe7bc349c9151147
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/416ef9b1.failed

Commit 0dfb33a0d7e2 ("sch_red: report backlog information") copied
child's backlog into RED's backlog.  Back then RED did not maintain
its own backlog counts.  This has changed after commit 2ccccf5fb43f
("net_sched: update hierarchical backlog too") and commit d7f4f332f082
("sch_red: update backlog as well").  Copying is no longer necessary.

Tested:

$ tc -s qdisc show dev veth0
qdisc red 1: root refcnt 2 limit 400000b min 30000b max 30000b ecn
 Sent 20942 bytes 221 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 1260b 14p requeues 14
  marked 0 early 0 pdrop 0 other 0
qdisc tbf 2: parent 1: rate 1Kbit burst 15000b lat 3585.0s
 Sent 20942 bytes 221 pkt (dropped 0, overlimits 138 requeues 0)
 backlog 1260b 14p requeues 14

Recently RED offload was added.  We need to make sure drivers don't
depend on resetting the stats.  This means backlog should be treated
like any other statistic:

  total_stat = new_hw_stat - prev_hw_stat;

Adjust mlxsw.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Acked-by: Nogah Frankel <nogahf@mellanox.com>
	Acked-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 416ef9b15c688b91edbf654ebe7bc349c9151147)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_qdisc.c
#	include/net/pkt_cls.h
#	net/sched/sch_red.c
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum_qdisc.c
index b5397da94d7f,0b7670459051..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_qdisc.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_qdisc.c
@@@ -90,24 -234,21 +90,31 @@@ mlxsw_sp_setup_tc_qdisc_clean_stats(str
  
  	xstats = &mlxsw_sp_port->periodic_hw_stats.xstats;
  	stats = &mlxsw_sp_port->periodic_hw_stats.stats;
 -	stats_base = &mlxsw_sp_qdisc->stats_base;
 -	red_base = &mlxsw_sp_qdisc->xstats_base.red;
 -
 -	stats_base->tx_packets = stats->tx_packets;
 -	stats_base->tx_bytes = stats->tx_bytes;
 -
 -	red_base->prob_mark = xstats->ecn;
 -	red_base->prob_drop = xstats->wred_drop[tclass_num];
 -	red_base->pdrop = xstats->tail_drop[tclass_num];
  
 +	mlxsw_sp_qdisc->tx_packets = stats->tx_packets;
 +	mlxsw_sp_qdisc->tx_bytes = stats->tx_bytes;
 +
 +	switch (mlxsw_sp_qdisc->type) {
 +	case MLXSW_SP_QDISC_RED:
 +		xstats_base->prob_mark = xstats->ecn;
 +		xstats_base->prob_drop = xstats->wred_drop[tclass_num];
 +		xstats_base->pdrop = xstats->tail_drop[tclass_num];
 +
++<<<<<<< HEAD
 +		mlxsw_sp_qdisc->overlimits = xstats_base->prob_drop +
 +					     xstats_base->prob_mark;
 +		mlxsw_sp_qdisc->drops = xstats_base->prob_drop +
 +					xstats_base->pdrop;
 +		break;
 +	default:
 +		break;
 +	}
++=======
+ 	stats_base->overlimits = red_base->prob_drop + red_base->prob_mark;
+ 	stats_base->drops = red_base->prob_drop + red_base->pdrop;
+ 
+ 	stats_base->backlog = 0;
++>>>>>>> 416ef9b15c68 (net: sched: red: don't reset the backlog on every stat dump)
  }
  
  static int
@@@ -162,87 -304,103 +169,140 @@@ mlxsw_sp_qdisc_red_replace(struct mlxsw
  	prob = DIV_ROUND_UP(prob, 1 << 16);
  	min = mlxsw_sp_bytes_cells(mlxsw_sp, p->min);
  	max = mlxsw_sp_bytes_cells(mlxsw_sp, p->max);
 -	return mlxsw_sp_tclass_congestion_enable(mlxsw_sp_port, tclass_num, min,
 -						 max, prob, p->is_ecn);
 +	err = mlxsw_sp_tclass_congestion_enable(mlxsw_sp_port, tclass_num, min,
 +						max, prob, p->is_ecn);
 +	if (err)
 +		goto err_config;
 +
 +	mlxsw_sp_qdisc->type = MLXSW_SP_QDISC_RED;
 +	if (mlxsw_sp_qdisc->handle != handle)
 +		mlxsw_sp_setup_tc_qdisc_clean_stats(mlxsw_sp_port,
 +						    mlxsw_sp_qdisc,
 +						    tclass_num);
 +
 +	mlxsw_sp_qdisc->handle = handle;
 +	return 0;
 +
 +err_bad_param:
 +	err = -EINVAL;
 +err_config:
 +	mlxsw_sp_qdisc_red_destroy(mlxsw_sp_port, mlxsw_sp_qdisc->handle,
 +				   mlxsw_sp_qdisc, tclass_num);
 +	return err;
  }
  
+ static void
+ mlxsw_sp_qdisc_red_unoffload(struct mlxsw_sp_port *mlxsw_sp_port,
+ 			     struct mlxsw_sp_qdisc *mlxsw_sp_qdisc,
+ 			     void *params)
+ {
+ 	struct tc_red_qopt_offload_params *p = params;
+ 	u64 backlog;
+ 
+ 	backlog = mlxsw_sp_cells_bytes(mlxsw_sp_port->mlxsw_sp,
+ 				       mlxsw_sp_qdisc->stats_base.backlog);
+ 	p->qstats->backlog -= backlog;
+ }
+ 
  static int
 -mlxsw_sp_qdisc_get_red_xstats(struct mlxsw_sp_port *mlxsw_sp_port,
 +mlxsw_sp_qdisc_get_red_xstats(struct mlxsw_sp_port *mlxsw_sp_port, u32 handle,
  			      struct mlxsw_sp_qdisc *mlxsw_sp_qdisc,
 -			      void *xstats_ptr)
 +			      int tclass_num, struct red_stats *res)
  {
 -	struct red_stats *xstats_base = &mlxsw_sp_qdisc->xstats_base.red;
 -	u8 tclass_num = mlxsw_sp_qdisc->tclass_num;
 +	struct red_stats *xstats_base = &mlxsw_sp_qdisc->xstats_base;
  	struct mlxsw_sp_port_xstats *xstats;
 -	struct red_stats *res = xstats_ptr;
 -	int early_drops, marks, pdrops;
  
 -	xstats = &mlxsw_sp_port->periodic_hw_stats.xstats;
 -
 -	early_drops = xstats->wred_drop[tclass_num] - xstats_base->prob_drop;
 -	marks = xstats->ecn - xstats_base->prob_mark;
 -	pdrops = xstats->tail_drop[tclass_num] - xstats_base->pdrop;
 +	if (mlxsw_sp_qdisc->handle != handle ||
 +	    mlxsw_sp_qdisc->type != MLXSW_SP_QDISC_RED)
 +		return -EOPNOTSUPP;
  
 -	res->pdrop += pdrops;
 -	res->prob_drop += early_drops;
 -	res->prob_mark += marks;
 +	xstats = &mlxsw_sp_port->periodic_hw_stats.xstats;
  
 -	xstats_base->pdrop += pdrops;
 -	xstats_base->prob_drop += early_drops;
 -	xstats_base->prob_mark += marks;
 +	res->prob_drop = xstats->wred_drop[tclass_num] - xstats_base->prob_drop;
 +	res->prob_mark = xstats->ecn - xstats_base->prob_mark;
 +	res->pdrop = xstats->tail_drop[tclass_num] - xstats_base->pdrop;
  	return 0;
  }
  
  static int
 -mlxsw_sp_qdisc_get_red_stats(struct mlxsw_sp_port *mlxsw_sp_port,
 +mlxsw_sp_qdisc_get_red_stats(struct mlxsw_sp_port *mlxsw_sp_port, u32 handle,
  			     struct mlxsw_sp_qdisc *mlxsw_sp_qdisc,
 -			     struct tc_qopt_offload_stats *stats_ptr)
 +			     int tclass_num,
 +			     struct tc_red_qopt_offload_stats *res)
  {
++<<<<<<< HEAD
 +	u64 tx_bytes, tx_packets, overlimits, drops;
++=======
+ 	u64 tx_bytes, tx_packets, overlimits, drops, backlog;
+ 	u8 tclass_num = mlxsw_sp_qdisc->tclass_num;
+ 	struct mlxsw_sp_qdisc_stats *stats_base;
++>>>>>>> 416ef9b15c68 (net: sched: red: don't reset the backlog on every stat dump)
  	struct mlxsw_sp_port_xstats *xstats;
  	struct rtnl_link_stats64 *stats;
  
 +	if (mlxsw_sp_qdisc->handle != handle ||
 +	    mlxsw_sp_qdisc->type != MLXSW_SP_QDISC_RED)
 +		return -EOPNOTSUPP;
 +
  	xstats = &mlxsw_sp_port->periodic_hw_stats.xstats;
  	stats = &mlxsw_sp_port->periodic_hw_stats.stats;
 -	stats_base = &mlxsw_sp_qdisc->stats_base;
  
 -	tx_bytes = stats->tx_bytes - stats_base->tx_bytes;
 -	tx_packets = stats->tx_packets - stats_base->tx_packets;
 +	tx_bytes = stats->tx_bytes - mlxsw_sp_qdisc->tx_bytes;
 +	tx_packets = stats->tx_packets - mlxsw_sp_qdisc->tx_packets;
  	overlimits = xstats->wred_drop[tclass_num] + xstats->ecn -
 -		     stats_base->overlimits;
 +		     mlxsw_sp_qdisc->overlimits;
  	drops = xstats->wred_drop[tclass_num] + xstats->tail_drop[tclass_num] -
++<<<<<<< HEAD
 +		mlxsw_sp_qdisc->drops;
 +
 +	_bstats_update(res->bstats, tx_bytes, tx_packets);
 +	res->qstats->overlimits += overlimits;
 +	res->qstats->drops += drops;
 +	res->qstats->backlog += mlxsw_sp_cells_bytes(mlxsw_sp_port->mlxsw_sp,
 +						xstats->backlog[tclass_num]);
 +
 +	mlxsw_sp_qdisc->drops +=  drops;
 +	mlxsw_sp_qdisc->overlimits += overlimits;
 +	mlxsw_sp_qdisc->tx_bytes += tx_bytes;
 +	mlxsw_sp_qdisc->tx_packets += tx_packets;
++=======
+ 		stats_base->drops;
+ 	backlog = xstats->backlog[tclass_num];
+ 
+ 	_bstats_update(stats_ptr->bstats, tx_bytes, tx_packets);
+ 	stats_ptr->qstats->overlimits += overlimits;
+ 	stats_ptr->qstats->drops += drops;
+ 	stats_ptr->qstats->backlog +=
+ 				mlxsw_sp_cells_bytes(mlxsw_sp_port->mlxsw_sp,
+ 						     backlog) -
+ 				mlxsw_sp_cells_bytes(mlxsw_sp_port->mlxsw_sp,
+ 						     stats_base->backlog);
+ 
+ 	stats_base->backlog = backlog;
+ 	stats_base->drops +=  drops;
+ 	stats_base->overlimits += overlimits;
+ 	stats_base->tx_bytes += tx_bytes;
+ 	stats_base->tx_packets += tx_packets;
++>>>>>>> 416ef9b15c68 (net: sched: red: don't reset the backlog on every stat dump)
  	return 0;
  }
  
  #define MLXSW_SP_PORT_DEFAULT_TCLASS 0
  
++<<<<<<< HEAD
++=======
+ static struct mlxsw_sp_qdisc_ops mlxsw_sp_qdisc_ops_red = {
+ 	.type = MLXSW_SP_QDISC_RED,
+ 	.check_params = mlxsw_sp_qdisc_red_check_params,
+ 	.replace = mlxsw_sp_qdisc_red_replace,
+ 	.unoffload = mlxsw_sp_qdisc_red_unoffload,
+ 	.destroy = mlxsw_sp_qdisc_red_destroy,
+ 	.get_stats = mlxsw_sp_qdisc_get_red_stats,
+ 	.get_xstats = mlxsw_sp_qdisc_get_red_xstats,
+ 	.clean_stats = mlxsw_sp_setup_tc_qdisc_red_clean_stats,
+ };
+ 
++>>>>>>> 416ef9b15c68 (net: sched: red: don't reset the backlog on every stat dump)
  int mlxsw_sp_setup_tc_red(struct mlxsw_sp_port *mlxsw_sp_port,
  			  struct tc_red_qopt_offload *p)
  {
diff --cc include/net/pkt_cls.h
index aade8feaa726,cc23c041a6d7..000000000000
--- a/include/net/pkt_cls.h
+++ b/include/net/pkt_cls.h
@@@ -597,18 -731,60 +597,65 @@@ struct tc_cookie 
  	u32 len;
  };
  
 -struct tc_qopt_offload_stats {
 -	struct gnet_stats_basic_packed *bstats;
 -	struct gnet_stats_queue *qstats;
 +enum tc_clsbpf_command {
 +	TC_CLSBPF_ADD,
 +	TC_CLSBPF_REPLACE,
 +	TC_CLSBPF_DESTROY,
  };
  
 -enum tc_red_command {
 -	TC_RED_REPLACE,
 -	TC_RED_DESTROY,
 -	TC_RED_STATS,
 -	TC_RED_XSTATS,
 +struct tc_cls_bpf_offload {
 +	enum tc_clsbpf_command command;
 +	struct tcf_exts *exts;
 +	struct bpf_prog *prog;
 +	const char *name;
 +	bool exts_integrated;
  };
  
++<<<<<<< HEAD
++=======
+ struct tc_red_qopt_offload_params {
+ 	u32 min;
+ 	u32 max;
+ 	u32 probability;
+ 	bool is_ecn;
+ 	struct gnet_stats_queue *qstats;
+ };
+ 
+ struct tc_red_qopt_offload {
+ 	enum tc_red_command command;
+ 	u32 handle;
+ 	u32 parent;
+ 	union {
+ 		struct tc_red_qopt_offload_params set;
+ 		struct tc_qopt_offload_stats stats;
+ 		struct red_stats *xstats;
+ 	};
+ };
+ 
+ enum tc_prio_command {
+ 	TC_PRIO_REPLACE,
+ 	TC_PRIO_DESTROY,
+ 	TC_PRIO_STATS,
+ };
+ 
+ struct tc_prio_qopt_offload_params {
+ 	int bands;
+ 	u8 priomap[TC_PRIO_MAX + 1];
+ 	/* In case that a prio qdisc is offloaded and now is changed to a
+ 	 * non-offloadedable config, it needs to update the backlog & qlen
+ 	 * values to negate the HW backlog & qlen values (and only them).
+ 	 */
+ 	struct gnet_stats_queue *qstats;
+ };
+ 
+ struct tc_prio_qopt_offload {
+ 	enum tc_prio_command command;
+ 	u32 handle;
+ 	u32 parent;
+ 	union {
+ 		struct tc_prio_qopt_offload_params replace_params;
+ 		struct tc_qopt_offload_stats stats;
+ 	};
+ };
++>>>>>>> 416ef9b15c68 (net: sched: red: don't reset the backlog on every stat dump)
  #endif
diff --cc net/sched/sch_red.c
index 5e2e228d6957,16644b3d2362..000000000000
--- a/net/sched/sch_red.c
+++ b/net/sched/sch_red.c
@@@ -147,6 -149,32 +147,35 @@@ static void red_reset(struct Qdisc *sch
  	red_restart(&q->vars);
  }
  
++<<<<<<< HEAD
++=======
+ static int red_offload(struct Qdisc *sch, bool enable)
+ {
+ 	struct red_sched_data *q = qdisc_priv(sch);
+ 	struct net_device *dev = qdisc_dev(sch);
+ 	struct tc_red_qopt_offload opt = {
+ 		.handle = sch->handle,
+ 		.parent = sch->parent,
+ 	};
+ 
+ 	if (!tc_can_offload(dev) || !dev->netdev_ops->ndo_setup_tc)
+ 		return -EOPNOTSUPP;
+ 
+ 	if (enable) {
+ 		opt.command = TC_RED_REPLACE;
+ 		opt.set.min = q->parms.qth_min >> q->parms.Wlog;
+ 		opt.set.max = q->parms.qth_max >> q->parms.Wlog;
+ 		opt.set.probability = q->parms.max_P;
+ 		opt.set.is_ecn = red_use_ecn(q);
+ 		opt.set.qstats = &sch->qstats;
+ 	} else {
+ 		opt.command = TC_RED_DESTROY;
+ 	}
+ 
+ 	return dev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_QDISC_RED, &opt);
+ }
+ 
++>>>>>>> 416ef9b15c68 (net: sched: red: don't reset the backlog on every stat dump)
  static void red_destroy(struct Qdisc *sch)
  {
  	struct red_sched_data *q = qdisc_priv(sch);
@@@ -258,8 -321,12 +287,15 @@@ static int red_dump(struct Qdisc *sch, 
  		.Plog		= q->parms.Plog,
  		.Scell_log	= q->parms.Scell_log,
  	};
 -	int err;
  
++<<<<<<< HEAD
 +	sch->qstats.backlog = q->qdisc->qstats.backlog;
++=======
+ 	err = red_dump_offload_stats(sch, &opt);
+ 	if (err)
+ 		goto nla_put_failure;
+ 
++>>>>>>> 416ef9b15c68 (net: sched: red: don't reset the backlog on every stat dump)
  	opts = nla_nest_start(skb, TCA_OPTIONS);
  	if (opts == NULL)
  		goto nla_put_failure;
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_qdisc.c
* Unmerged path include/net/pkt_cls.h
* Unmerged path net/sched/sch_red.c
