iio: Specify supported modes for buffers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [iio] Specify supported modes for buffers (Tony Camuso) [1559170]
Rebuild_FUZZ: 93.33%
commit-author Lars-Peter Clausen <lars@metafoo.de>
commit 225d59adf1c899176cce0fc80e42b1d1c12f109f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/225d59ad.failed

For each buffer type specify the supported device modes for this buffer.
This allows us for devices which support multiple different operating modes
to pick the correct operating mode based on the modes supported by the
attached buffers.

It also prevents that buffers with conflicting modes are attached
to a device at the same time or that a buffer with a non-supported mode is
attached to a device (e.g. in-kernel callback buffer to a device only
supporting hardware mode).

	Signed-off-by: Lars-Peter Clausen <lars@metafoo.de>
	Signed-off-by: Jonathan Cameron <jic23@kernel.org>
(cherry picked from commit 225d59adf1c899176cce0fc80e42b1d1c12f109f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iio/buffer_cb.c
#	drivers/iio/industrialio-buffer.c
#	drivers/iio/kfifo_buf.c
#	drivers/staging/iio/accel/sca3000_ring.c
#	include/linux/iio/buffer.h
diff --cc drivers/iio/buffer_cb.c
index 0578f74fa23c,1648e6e5a848..000000000000
--- a/drivers/iio/buffer_cb.c
+++ b/drivers/iio/buffer_cb.c
@@@ -21,8 -20,21 +21,14 @@@ static int iio_buffer_cb_store_to(struc
  	return cb_buff->cb(data, cb_buff->private);
  }
  
 -static void iio_buffer_cb_release(struct iio_buffer *buffer)
 -{
 -	struct iio_cb_buffer *cb_buff = buffer_to_cb_buffer(buffer);
 -	kfree(cb_buff->buffer.scan_mask);
 -	kfree(cb_buff);
 -}
 -
  static const struct iio_buffer_access_funcs iio_cb_access = {
  	.store_to = &iio_buffer_cb_store_to,
++<<<<<<< HEAD
++=======
+ 	.release = &iio_buffer_cb_release,
+ 
+ 	.modes = INDIO_BUFFER_SOFTWARE | INDIO_BUFFER_TRIGGERED,
++>>>>>>> 225d59adf1c8 (iio: Specify supported modes for buffers)
  };
  
  struct iio_cb_buffer *iio_channel_get_all_cb(struct device *dev,
diff --cc drivers/iio/industrialio-buffer.c
index ed6b8aa675ce,0a14bd825fe0..000000000000
--- a/drivers/iio/industrialio-buffer.c
+++ b/drivers/iio/industrialio-buffer.c
@@@ -386,11 -441,536 +386,478 @@@ error_ret
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t iio_buffer_read_length(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 
+ 	return sprintf(buf, "%d\n", buffer->length);
+ }
+ 
+ static ssize_t iio_buffer_write_length(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf, size_t len)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	struct iio_buffer *buffer = indio_dev->buffer;
+ 	unsigned int val;
+ 	int ret;
+ 
+ 	ret = kstrtouint(buf, 10, &val);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (val == buffer->length)
+ 		return len;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 	if (iio_buffer_is_active(indio_dev->buffer)) {
+ 		ret = -EBUSY;
+ 	} else {
+ 		buffer->access->set_length(buffer, val);
+ 		ret = 0;
+ 	}
+ 	if (ret)
+ 		goto out;
+ 	if (buffer->length && buffer->length < buffer->watermark)
+ 		buffer->watermark = buffer->length;
+ out:
+ 	mutex_unlock(&indio_dev->mlock);
+ 
+ 	return ret ? ret : len;
+ }
+ 
+ static ssize_t iio_buffer_show_enable(struct device *dev,
+ 				      struct device_attribute *attr,
+ 				      char *buf)
+ {
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	return sprintf(buf, "%d\n", iio_buffer_is_active(indio_dev->buffer));
+ }
+ 
+ static int iio_compute_scan_bytes(struct iio_dev *indio_dev,
+ 				const unsigned long *mask, bool timestamp)
+ {
+ 	const struct iio_chan_spec *ch;
+ 	unsigned bytes = 0;
+ 	int length, i;
+ 
+ 	/* How much space will the demuxed element take? */
+ 	for_each_set_bit(i, mask,
+ 			 indio_dev->masklength) {
+ 		ch = iio_find_channel_from_si(indio_dev, i);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	if (timestamp) {
+ 		ch = iio_find_channel_from_si(indio_dev,
+ 					      indio_dev->scan_index_timestamp);
+ 		if (ch->scan_type.repeat > 1)
+ 			length = ch->scan_type.storagebits / 8 *
+ 				ch->scan_type.repeat;
+ 		else
+ 			length = ch->scan_type.storagebits / 8;
+ 		bytes = ALIGN(bytes, length);
+ 		bytes += length;
+ 	}
+ 	return bytes;
+ }
+ 
+ static void iio_buffer_activate(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	iio_buffer_get(buffer);
+ 	list_add(&buffer->buffer_list, &indio_dev->buffer_list);
+ }
+ 
+ static void iio_buffer_deactivate(struct iio_buffer *buffer)
+ {
+ 	list_del_init(&buffer->buffer_list);
+ 	wake_up_interruptible(&buffer->pollq);
+ 	iio_buffer_put(buffer);
+ }
+ 
+ static void iio_buffer_deactivate_all(struct iio_dev *indio_dev)
+ {
+ 	struct iio_buffer *buffer, *_buffer;
+ 
+ 	list_for_each_entry_safe(buffer, _buffer,
+ 			&indio_dev->buffer_list, buffer_list)
+ 		iio_buffer_deactivate(buffer);
+ }
+ 
+ static void iio_buffer_update_bytes_per_datum(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	unsigned int bytes;
+ 
+ 	if (!buffer->access->set_bytes_per_datum)
+ 		return;
+ 
+ 	bytes = iio_compute_scan_bytes(indio_dev, buffer->scan_mask,
+ 		buffer->scan_timestamp);
+ 
+ 	buffer->access->set_bytes_per_datum(buffer, bytes);
+ }
+ 
+ static int iio_buffer_request_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *buffer)
+ {
+ 	int ret;
+ 
+ 	iio_buffer_update_bytes_per_datum(indio_dev, buffer);
+ 	if (buffer->access->request_update) {
+ 		ret = buffer->access->request_update(buffer);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer parameter update failed (%d)\n",
+ 				ret);
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void iio_free_scan_mask(struct iio_dev *indio_dev,
+ 	const unsigned long *mask)
+ {
+ 	/* If the mask is dynamically allocated free it, otherwise do nothing */
+ 	if (!indio_dev->available_scan_masks)
+ 		kfree(mask);
+ }
+ 
+ struct iio_device_config {
+ 	unsigned int mode;
+ 	const unsigned long *scan_mask;
+ 	unsigned int scan_bytes;
+ 	bool scan_timestamp;
+ };
+ 
+ static int iio_verify_update(struct iio_dev *indio_dev,
+ 	struct iio_buffer *insert_buffer, struct iio_buffer *remove_buffer,
+ 	struct iio_device_config *config)
+ {
+ 	unsigned long *compound_mask;
+ 	const unsigned long *scan_mask;
+ 	struct iio_buffer *buffer;
+ 	bool scan_timestamp;
+ 	unsigned int modes;
+ 
+ 	memset(config, 0, sizeof(*config));
+ 
+ 	/*
+ 	 * If there is just one buffer and we are removing it there is nothing
+ 	 * to verify.
+ 	 */
+ 	if (remove_buffer && !insert_buffer &&
+ 		list_is_singular(&indio_dev->buffer_list))
+ 			return 0;
+ 
+ 	modes = indio_dev->modes;
+ 
+ 	list_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {
+ 		if (buffer == remove_buffer)
+ 			continue;
+ 		modes &= buffer->access->modes;
+ 	}
+ 
+ 	if (insert_buffer)
+ 		modes &= insert_buffer->access->modes;
+ 
+ 	/* Definitely possible for devices to support both of these. */
+ 	if ((modes & INDIO_BUFFER_TRIGGERED) && indio_dev->trig) {
+ 		config->mode = INDIO_BUFFER_TRIGGERED;
+ 	} else if (modes & INDIO_BUFFER_HARDWARE) {
+ 		config->mode = INDIO_BUFFER_HARDWARE;
+ 	} else if (modes & INDIO_BUFFER_SOFTWARE) {
+ 		config->mode = INDIO_BUFFER_SOFTWARE;
+ 	} else {
+ 		/* Can only occur on first buffer */
+ 		if (indio_dev->modes & INDIO_BUFFER_TRIGGERED)
+ 			dev_dbg(&indio_dev->dev, "Buffer not started: no trigger\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* What scan mask do we actually have? */
+ 	compound_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),
+ 				sizeof(long), GFP_KERNEL);
+ 	if (compound_mask == NULL)
+ 		return -ENOMEM;
+ 
+ 	scan_timestamp = false;
+ 
+ 	list_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {
+ 		if (buffer == remove_buffer)
+ 			continue;
+ 		bitmap_or(compound_mask, compound_mask, buffer->scan_mask,
+ 			  indio_dev->masklength);
+ 		scan_timestamp |= buffer->scan_timestamp;
+ 	}
+ 
+ 	if (insert_buffer) {
+ 		bitmap_or(compound_mask, compound_mask,
+ 			  insert_buffer->scan_mask, indio_dev->masklength);
+ 		scan_timestamp |= insert_buffer->scan_timestamp;
+ 	}
+ 
+ 	if (indio_dev->available_scan_masks) {
+ 		scan_mask = iio_scan_mask_match(indio_dev->available_scan_masks,
+ 				    indio_dev->masklength,
+ 				    compound_mask);
+ 		kfree(compound_mask);
+ 		if (scan_mask == NULL)
+ 			return -EINVAL;
+ 	} else {
+ 	    scan_mask = compound_mask;
+ 	}
+ 
+ 	config->scan_bytes = iio_compute_scan_bytes(indio_dev,
+ 				    scan_mask, scan_timestamp);
+ 	config->scan_mask = scan_mask;
+ 	config->scan_timestamp = scan_timestamp;
+ 
+ 	return 0;
+ }
+ 
+ static int iio_enable_buffers(struct iio_dev *indio_dev,
+ 	struct iio_device_config *config)
+ {
+ 	int ret;
+ 
+ 	indio_dev->active_scan_mask = config->scan_mask;
+ 	indio_dev->scan_timestamp = config->scan_timestamp;
+ 	indio_dev->scan_bytes = config->scan_bytes;
+ 
+ 	iio_update_demux(indio_dev);
+ 
+ 	/* Wind up again */
+ 	if (indio_dev->setup_ops->preenable) {
+ 		ret = indio_dev->setup_ops->preenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: buffer preenable failed (%d)\n", ret);
+ 			goto err_undo_config;
+ 		}
+ 	}
+ 
+ 	if (indio_dev->info->update_scan_mode) {
+ 		ret = indio_dev->info
+ 			->update_scan_mode(indio_dev,
+ 					   indio_dev->active_scan_mask);
+ 		if (ret < 0) {
+ 			dev_dbg(&indio_dev->dev,
+ 				"Buffer not started: update scan mode failed (%d)\n",
+ 				ret);
+ 			goto err_run_postdisable;
+ 		}
+ 	}
+ 
+ 	indio_dev->currentmode = config->mode;
+ 
+ 	if (indio_dev->setup_ops->postenable) {
+ 		ret = indio_dev->setup_ops->postenable(indio_dev);
+ 		if (ret) {
+ 			dev_dbg(&indio_dev->dev,
+ 			       "Buffer not started: postenable failed (%d)\n", ret);
+ 			goto err_run_postdisable;
+ 		}
+ 	}
+ 
+ 	return 0;
+ 
+ err_run_postdisable:
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 	if (indio_dev->setup_ops->postdisable)
+ 		indio_dev->setup_ops->postdisable(indio_dev);
+ err_undo_config:
+ 	indio_dev->active_scan_mask = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static int iio_disable_buffers(struct iio_dev *indio_dev)
+ {
+ 	int ret = 0;
+ 	int ret2;
+ 
+ 	/* Wind down existing buffers - iff there are any */
+ 	if (list_empty(&indio_dev->buffer_list))
+ 		return 0;
+ 
+ 	/*
+ 	 * If things go wrong at some step in disable we still need to continue
+ 	 * to perform the other steps, otherwise we leave the device in a
+ 	 * inconsistent state. We return the error code for the first error we
+ 	 * encountered.
+ 	 */
+ 
+ 	if (indio_dev->setup_ops->predisable) {
+ 		ret2 = indio_dev->setup_ops->predisable(indio_dev);
+ 		if (ret2 && !ret)
+ 			ret = ret2;
+ 	}
+ 
+ 	indio_dev->currentmode = INDIO_DIRECT_MODE;
+ 
+ 	if (indio_dev->setup_ops->postdisable) {
+ 		ret2 = indio_dev->setup_ops->postdisable(indio_dev);
+ 		if (ret2 && !ret)
+ 			ret = ret2;
+ 	}
+ 
+ 	iio_free_scan_mask(indio_dev, indio_dev->active_scan_mask);
+ 	indio_dev->active_scan_mask = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static int __iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	struct iio_device_config new_config;
+ 	int ret;
+ 
+ 	ret = iio_verify_update(indio_dev, insert_buffer, remove_buffer,
+ 		&new_config);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (insert_buffer) {
+ 		ret = iio_buffer_request_update(indio_dev, insert_buffer);
+ 		if (ret)
+ 			goto err_free_config;
+ 	}
+ 
+ 	ret = iio_disable_buffers(indio_dev);
+ 	if (ret)
+ 		goto err_deactivate_all;
+ 
+ 	if (remove_buffer)
+ 		iio_buffer_deactivate(remove_buffer);
+ 	if (insert_buffer)
+ 		iio_buffer_activate(indio_dev, insert_buffer);
+ 
+ 	/* If no buffers in list, we are done */
+ 	if (list_empty(&indio_dev->buffer_list))
+ 		return 0;
+ 
+ 	ret = iio_enable_buffers(indio_dev, &new_config);
+ 	if (ret)
+ 		goto err_deactivate_all;
+ 
+ 	return 0;
+ 
+ err_deactivate_all:
+ 	/*
+ 	 * We've already verified that the config is valid earlier. If things go
+ 	 * wrong in either enable or disable the most likely reason is an IO
+ 	 * error from the device. In this case there is no good recovery
+ 	 * strategy. Just make sure to disable everything and leave the device
+ 	 * in a sane state.  With a bit of luck the device might come back to
+ 	 * life again later and userspace can try again.
+ 	 */
+ 	iio_buffer_deactivate_all(indio_dev);
+ 
+ err_free_config:
+ 	iio_free_scan_mask(indio_dev, new_config.scan_mask);
+ 	return ret;
+ }
+ 
+ int iio_update_buffers(struct iio_dev *indio_dev,
+ 		       struct iio_buffer *insert_buffer,
+ 		       struct iio_buffer *remove_buffer)
+ {
+ 	int ret;
+ 
+ 	if (insert_buffer == remove_buffer)
+ 		return 0;
+ 
+ 	mutex_lock(&indio_dev->info_exist_lock);
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	if (insert_buffer && iio_buffer_is_active(insert_buffer))
+ 		insert_buffer = NULL;
+ 
+ 	if (remove_buffer && !iio_buffer_is_active(remove_buffer))
+ 		remove_buffer = NULL;
+ 
+ 	if (!insert_buffer && !remove_buffer) {
+ 		ret = 0;
+ 		goto out_unlock;
+ 	}
+ 
+ 	if (indio_dev->info == NULL) {
+ 		ret = -ENODEV;
+ 		goto out_unlock;
+ 	}
+ 
+ 	ret = __iio_update_buffers(indio_dev, insert_buffer, remove_buffer);
+ 
+ out_unlock:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	mutex_unlock(&indio_dev->info_exist_lock);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(iio_update_buffers);
+ 
+ void iio_disable_all_buffers(struct iio_dev *indio_dev)
+ {
+ 	iio_disable_buffers(indio_dev);
+ 	iio_buffer_deactivate_all(indio_dev);
+ }
+ 
+ static ssize_t iio_buffer_store_enable(struct device *dev,
+ 				       struct device_attribute *attr,
+ 				       const char *buf,
+ 				       size_t len)
+ {
+ 	int ret;
+ 	bool requested_state;
+ 	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+ 	bool inlist;
+ 
+ 	ret = strtobool(buf, &requested_state);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	mutex_lock(&indio_dev->mlock);
+ 
+ 	/* Find out if it is in the list */
+ 	inlist = iio_buffer_is_active(indio_dev->buffer);
+ 	/* Already in desired state */
+ 	if (inlist == requested_state)
+ 		goto done;
+ 
+ 	if (requested_state)
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 indio_dev->buffer, NULL);
+ 	else
+ 		ret = __iio_update_buffers(indio_dev,
+ 					 NULL, indio_dev->buffer);
+ 
+ done:
+ 	mutex_unlock(&indio_dev->mlock);
+ 	return (ret < 0) ? ret : len;
+ }
+ 
++>>>>>>> 225d59adf1c8 (iio: Specify supported modes for buffers)
  static const char * const iio_scan_elements_group_name = "scan_elements";
  
 -static ssize_t iio_buffer_show_watermark(struct device *dev,
 -					 struct device_attribute *attr,
 -					 char *buf)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -
 -	return sprintf(buf, "%u\n", buffer->watermark);
 -}
 -
 -static ssize_t iio_buffer_store_watermark(struct device *dev,
 -					  struct device_attribute *attr,
 -					  const char *buf,
 -					  size_t len)
 -{
 -	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
 -	struct iio_buffer *buffer = indio_dev->buffer;
 -	unsigned int val;
 -	int ret;
 -
 -	ret = kstrtouint(buf, 10, &val);
 -	if (ret)
 -		return ret;
 -	if (!val)
 -		return -EINVAL;
 -
 -	mutex_lock(&indio_dev->mlock);
 -
 -	if (val > buffer->length) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	if (iio_buffer_is_active(indio_dev->buffer)) {
 -		ret = -EBUSY;
 -		goto out;
 -	}
 -
 -	buffer->watermark = val;
 -
 -	if (indio_dev->info->hwfifo_set_watermark)
 -		indio_dev->info->hwfifo_set_watermark(indio_dev, val);
 -out:
 -	mutex_unlock(&indio_dev->mlock);
 -
 -	return ret ? ret : len;
 -}
 -
 -static DEVICE_ATTR(length, S_IRUGO | S_IWUSR, iio_buffer_read_length,
 -		   iio_buffer_write_length);
 -static struct device_attribute dev_attr_length_ro = __ATTR(length,
 -	S_IRUGO, iio_buffer_read_length, NULL);
 -static DEVICE_ATTR(enable, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_enable, iio_buffer_store_enable);
 -static DEVICE_ATTR(watermark, S_IRUGO | S_IWUSR,
 -		   iio_buffer_show_watermark, iio_buffer_store_watermark);
 -
 -static struct attribute *iio_buffer_attrs[] = {
 -	&dev_attr_length.attr,
 -	&dev_attr_enable.attr,
 -	&dev_attr_watermark.attr,
 -};
 -
 -int iio_buffer_alloc_sysfs_and_mask(struct iio_dev *indio_dev)
 +int iio_buffer_register(struct iio_dev *indio_dev,
 +			const struct iio_chan_spec *channels,
 +			int num_channels)
  {
  	struct iio_dev_attr *p;
  	struct attribute **attr;
diff --cc drivers/iio/kfifo_buf.c
index 1bea41bcbdc6,db15684a4731..000000000000
--- a/drivers/iio/kfifo_buf.c
+++ b/drivers/iio/kfifo_buf.c
@@@ -133,14 -130,16 +133,20 @@@ static int iio_read_first_n_kfifo(struc
  static const struct iio_buffer_access_funcs kfifo_access_funcs = {
  	.store_to = &iio_store_to_kfifo,
  	.read_first_n = &iio_read_first_n_kfifo,
 -	.data_available = iio_kfifo_buf_data_available,
  	.request_update = &iio_request_update_kfifo,
 +	.get_bytes_per_datum = &iio_get_bytes_per_datum_kfifo,
  	.set_bytes_per_datum = &iio_set_bytes_per_datum_kfifo,
 +	.get_length = &iio_get_length_kfifo,
  	.set_length = &iio_set_length_kfifo,
++<<<<<<< HEAD
++=======
+ 	.release = &iio_kfifo_buffer_release,
+ 
+ 	.modes = INDIO_BUFFER_SOFTWARE | INDIO_BUFFER_TRIGGERED,
++>>>>>>> 225d59adf1c8 (iio: Specify supported modes for buffers)
  };
  
 -struct iio_buffer *iio_kfifo_allocate(void)
 +struct iio_buffer *iio_kfifo_allocate(struct iio_dev *indio_dev)
  {
  	struct iio_kfifo *kf;
  
diff --cc drivers/staging/iio/accel/sca3000_ring.c
index 3e5e860aa38e,23685e74917e..000000000000
--- a/drivers/staging/iio/accel/sca3000_ring.c
+++ b/drivers/staging/iio/accel/sca3000_ring.c
@@@ -272,8 -256,10 +272,15 @@@ static inline void sca3000_rb_free(stru
  
  static const struct iio_buffer_access_funcs sca3000_ring_access_funcs = {
  	.read_first_n = &sca3000_read_first_n_hw_rb,
++<<<<<<< HEAD
 +	.get_length = &sca3000_ring_get_length,
 +	.get_bytes_per_datum = &sca3000_ring_get_bytes_per_datum,
++=======
+ 	.data_available = sca3000_ring_buf_data_available,
+ 	.release = sca3000_ring_release,
+ 
+ 	.modes = INDIO_BUFFER_HARDWARE,
++>>>>>>> 225d59adf1c8 (iio: Specify supported modes for buffers)
  };
  
  int sca3000_configure_ring(struct iio_dev *indio_dev)
diff --cc include/linux/iio/buffer.h
index 26890e4a025c,1600c55828e0..000000000000
--- a/include/linux/iio/buffer.h
+++ b/include/linux/iio/buffer.h
@@@ -20,14 -21,15 +20,20 @@@ struct iio_buffer
   * struct iio_buffer_access_funcs - access functions for buffers.
   * @store_to:		actually store stuff to the buffer
   * @read_first_n:	try to get a specified number of bytes (must exist)
 - * @data_available:	indicates how much data is available for reading from
 - *			the buffer.
 + * @data_available:	indicates whether data for reading from the buffer is
 + *			available.
   * @request_update:	if a parameter change has been marked, update underlying
   *			storage.
 + * @get_bytes_per_datum:get current bytes per datum
   * @set_bytes_per_datum:set number of bytes per datum
 + * @get_length:		get number of datums in buffer
   * @set_length:		set number of datums in buffer
++<<<<<<< HEAD
++=======
+  * @release:		called when the last reference to the buffer is dropped,
+  *			should free all resources allocated by the buffer.
+  * @modes:		Supported operating modes by this buffer type
++>>>>>>> 225d59adf1c8 (iio: Specify supported modes for buffers)
   *
   * The purpose of this structure is to make the buffer element
   * modular as event for a given driver, different usecases may require
@@@ -46,10 -48,12 +52,17 @@@ struct iio_buffer_access_funcs 
  
  	int (*request_update)(struct iio_buffer *buffer);
  
 +	int (*get_bytes_per_datum)(struct iio_buffer *buffer);
  	int (*set_bytes_per_datum)(struct iio_buffer *buffer, size_t bpd);
 +	int (*get_length)(struct iio_buffer *buffer);
  	int (*set_length)(struct iio_buffer *buffer, int length);
++<<<<<<< HEAD
++=======
+ 
+ 	void (*release)(struct iio_buffer *buffer);
+ 
+ 	unsigned int modes;
++>>>>>>> 225d59adf1c8 (iio: Specify supported modes for buffers)
  };
  
  /**
* Unmerged path drivers/iio/buffer_cb.c
* Unmerged path drivers/iio/industrialio-buffer.c
* Unmerged path drivers/iio/kfifo_buf.c
* Unmerged path drivers/staging/iio/accel/sca3000_ring.c
* Unmerged path include/linux/iio/buffer.h
