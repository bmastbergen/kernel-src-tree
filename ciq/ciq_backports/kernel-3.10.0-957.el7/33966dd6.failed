x86/KVM/VMX: Split the VMX MSR LOAD structures to have an host/guest numbers

jira LE-1907
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] kvm/vmx: split the vmx msr load structures to have an host/guest numbers (Christoph von Recklinghausen) [1593384] {CVE-2018-3620}
Rebuild_FUZZ: 97.30%
commit-author Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
commit 33966dd6b2d2c352fae55412db2ea8cfff5df13a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/33966dd6.failed

There is no semantic change but this change allows an unbalanced amount of
MSRs to be loaded on VMEXIT and VMENTER, i.e. the number of MSRs to save or
restore on VMEXIT or VMENTER may be different.

	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 33966dd6b2d2c352fae55412db2ea8cfff5df13a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index e2f48f8aba96,3425238b0808..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -1947,31 -2515,19 +1951,32 @@@ static void add_atomic_switch_msr(struc
  		printk_once(KERN_WARNING "Not enough msr switch entries. "
  				"Can't add msr %x\n", msr);
  		return;
- 	} else if (i == m->nr) {
- 		++m->nr;
- 		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);
- 		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);
+ 	} else if (i == m->guest.nr) {
+ 		++m->guest.nr;
+ 		++m->host.nr;
+ 		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);
+ 		vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
  	}
  
- 	m->guest[i].index = msr;
- 	m->guest[i].value = guest_val;
- 	m->host[i].index = msr;
- 	m->host[i].value = host_val;
+ 	m->guest.val[i].index = msr;
+ 	m->guest.val[i].value = guest_val;
+ 	m->host.val[i].index = msr;
+ 	m->host.val[i].value = host_val;
  }
  
 +static void reload_tss(void)
 +{
 +	/*
 +	 * VT restores TR but not its size.  Useless.
 +	 */
 +	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
 +	struct desc_struct *descs;
 +
 +	descs = (void *)gdt->address;
 +	descs[GDT_ENTRY_TSS].type = 9; /* available TSS */
 +	load_TR_desc();
 +}
 +
  static bool update_transition_efer(struct vcpu_vmx *vmx, int efer_offset)
  {
  	u64 guest_efer = vmx->vcpu.arch.efer;
@@@ -5246,11 -6290,14 +5251,11 @@@ static int vmx_vcpu_setup(struct vcpu_v
  	vmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */
  #endif
  
 -	if (cpu_has_vmx_vmfunc())
 -		vmcs_write64(VM_FUNCTION_CONTROL, 0);
 -
  	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
  	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
- 	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host));
+ 	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));
  	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
- 	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest));
+ 	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));
  
  	if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)
  		vmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);
@@@ -10060,6 -11332,144 +10065,147 @@@ static int prepare_vmcs02(struct kvm_vc
  	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH,
  		enable_ept ? vmcs12->page_fault_error_code_match : 0);
  
++<<<<<<< HEAD
++=======
+ 	/* All VMFUNCs are currently emulated through L0 vmexits.  */
+ 	if (cpu_has_vmx_vmfunc())
+ 		vmcs_write64(VM_FUNCTION_CONTROL, 0);
+ 
+ 	if (cpu_has_vmx_apicv()) {
+ 		vmcs_write64(EOI_EXIT_BITMAP0, vmcs12->eoi_exit_bitmap0);
+ 		vmcs_write64(EOI_EXIT_BITMAP1, vmcs12->eoi_exit_bitmap1);
+ 		vmcs_write64(EOI_EXIT_BITMAP2, vmcs12->eoi_exit_bitmap2);
+ 		vmcs_write64(EOI_EXIT_BITMAP3, vmcs12->eoi_exit_bitmap3);
+ 	}
+ 
+ 	/*
+ 	 * Set host-state according to L0's settings (vmcs12 is irrelevant here)
+ 	 * Some constant fields are set here by vmx_set_constant_host_state().
+ 	 * Other fields are different per CPU, and will be set later when
+ 	 * vmx_vcpu_load() is called, and when vmx_save_host_state() is called.
+ 	 */
+ 	vmx_set_constant_host_state(vmx);
+ 
+ 	/*
+ 	 * Set the MSR load/store lists to match L0's settings.
+ 	 */
+ 	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
+ 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);
+ 	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));
+ 	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);
+ 	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));
+ 
+ 	set_cr4_guest_host_mask(vmx);
+ 
+ 	if (vmx_mpx_supported())
+ 		vmcs_write64(GUEST_BNDCFGS, vmcs12->guest_bndcfgs);
+ 
+ 	if (enable_vpid) {
+ 		if (nested_cpu_has_vpid(vmcs12) && vmx->nested.vpid02)
+ 			vmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->nested.vpid02);
+ 		else
+ 			vmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);
+ 	}
+ 
+ 	/*
+ 	 * L1 may access the L2's PDPTR, so save them to construct vmcs12
+ 	 */
+ 	if (enable_ept) {
+ 		vmcs_write64(GUEST_PDPTR0, vmcs12->guest_pdptr0);
+ 		vmcs_write64(GUEST_PDPTR1, vmcs12->guest_pdptr1);
+ 		vmcs_write64(GUEST_PDPTR2, vmcs12->guest_pdptr2);
+ 		vmcs_write64(GUEST_PDPTR3, vmcs12->guest_pdptr3);
+ 	}
+ 
+ 	if (cpu_has_vmx_msr_bitmap())
+ 		vmcs_write64(MSR_BITMAP, __pa(vmx->nested.vmcs02.msr_bitmap));
+ }
+ 
+ /*
+  * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested
+  * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function "merges" it
+  * with L0's requirements for its guest (a.k.a. vmcs01), so we can run the L2
+  * guest in a way that will both be appropriate to L1's requests, and our
+  * needs. In addition to modifying the active vmcs (which is vmcs02), this
+  * function also has additional necessary side-effects, like setting various
+  * vcpu->arch fields.
+  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
+  * is assigned to entry_failure_code on failure.
+  */
+ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
+ 			  u32 *entry_failure_code)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 	u32 exec_control, vmcs12_exec_ctrl;
+ 
+ 	if (vmx->nested.dirty_vmcs12) {
+ 		prepare_vmcs02_full(vcpu, vmcs12);
+ 		vmx->nested.dirty_vmcs12 = false;
+ 	}
+ 
+ 	/*
+ 	 * First, the fields that are shadowed.  This must be kept in sync
+ 	 * with vmx_shadow_fields.h.
+ 	 */
+ 
+ 	vmcs_write16(GUEST_CS_SELECTOR, vmcs12->guest_cs_selector);
+ 	vmcs_write32(GUEST_CS_LIMIT, vmcs12->guest_cs_limit);
+ 	vmcs_write32(GUEST_CS_AR_BYTES, vmcs12->guest_cs_ar_bytes);
+ 	vmcs_writel(GUEST_ES_BASE, vmcs12->guest_es_base);
+ 	vmcs_writel(GUEST_CS_BASE, vmcs12->guest_cs_base);
+ 
+ 	/*
+ 	 * Not in vmcs02: GUEST_PML_INDEX, HOST_FS_SELECTOR, HOST_GS_SELECTOR,
+ 	 * HOST_FS_BASE, HOST_GS_BASE.
+ 	 */
+ 
+ 	if (vmx->nested.nested_run_pending &&
+ 	    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS)) {
+ 		kvm_set_dr(vcpu, 7, vmcs12->guest_dr7);
+ 		vmcs_write64(GUEST_IA32_DEBUGCTL, vmcs12->guest_ia32_debugctl);
+ 	} else {
+ 		kvm_set_dr(vcpu, 7, vcpu->arch.dr7);
+ 		vmcs_write64(GUEST_IA32_DEBUGCTL, vmx->nested.vmcs01_debugctl);
+ 	}
+ 	if (vmx->nested.nested_run_pending) {
+ 		vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
+ 			     vmcs12->vm_entry_intr_info_field);
+ 		vmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE,
+ 			     vmcs12->vm_entry_exception_error_code);
+ 		vmcs_write32(VM_ENTRY_INSTRUCTION_LEN,
+ 			     vmcs12->vm_entry_instruction_len);
+ 		vmcs_write32(GUEST_INTERRUPTIBILITY_INFO,
+ 			     vmcs12->guest_interruptibility_info);
+ 		vmx->loaded_vmcs->nmi_known_unmasked =
+ 			!(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);
+ 	} else {
+ 		vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
+ 	}
+ 	vmx_set_rflags(vcpu, vmcs12->guest_rflags);
+ 
+ 	exec_control = vmcs12->pin_based_vm_exec_control;
+ 
+ 	/* Preemption timer setting is only taken from vmcs01.  */
+ 	exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
+ 	exec_control |= vmcs_config.pin_based_exec_ctrl;
+ 	if (vmx->hv_deadline_tsc == -1)
+ 		exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
+ 
+ 	/* Posted interrupts setting is only taken from vmcs12.  */
+ 	if (nested_cpu_has_posted_intr(vmcs12)) {
+ 		vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+ 		vmx->nested.pi_pending = false;
+ 	} else {
+ 		exec_control &= ~PIN_BASED_POSTED_INTR;
+ 	}
+ 
+ 	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, exec_control);
+ 
+ 	vmx->nested.preemption_timer_expired = false;
+ 	if (nested_cpu_has_preemption_timer(vmcs12))
+ 		vmx_start_preemption_timer(vcpu);
+ 
++>>>>>>> 33966dd6b2d2 (x86/KVM/VMX: Split the VMX MSR LOAD structures to have an host/guest numbers)
  	if (cpu_has_secondary_exec_ctrls()) {
  		exec_control = vmx->secondary_exec_control;
  
@@@ -11020,9 -12461,9 +11166,14 @@@ static void nested_vmx_vmexit(struct kv
  	vm_exit_controls_reset_shadow(vmx);
  	vmx_segment_cache_clear(vmx);
  
 +	load_vmcs12_host_state(vcpu, vmcs12);
 +
  	/* Update any VMCS fields that might have changed while L2 ran */
++<<<<<<< HEAD
++=======
+ 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);
+ 	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);
++>>>>>>> 33966dd6b2d2 (x86/KVM/VMX: Split the VMX MSR LOAD structures to have an host/guest numbers)
  	vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
  	if (vmx->hv_deadline_tsc == -1)
  		vmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,
* Unmerged path arch/x86/kvm/vmx.c
