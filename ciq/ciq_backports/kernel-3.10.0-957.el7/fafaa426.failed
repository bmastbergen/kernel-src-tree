pagewalk: improve vma handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit fafaa4264eba49fd10695c193a82760558d093f4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/fafaa426.failed

Current implementation of page table walker has a fundamental problem in
vma handling, which started when we tried to handle vma(VM_HUGETLB).
Because it's done in pgd loop, considering vma boundary makes code
complicated and bug-prone.

From the users viewpoint, some user checks some vma-related condition to
determine whether the user really does page walk over the vma.

In order to solve these, this patch moves vma check outside pgd loop and
introduce a new callback ->test_walk().

	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Cyrill Gorcunov <gorcunov@openvz.org>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Pavel Emelyanov <xemul@parallels.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fafaa4264eba49fd10695c193a82760558d093f4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/pagewalk.c
diff --cc mm/pagewalk.c
index 33631e2e5f18,d9cc3caae802..000000000000
--- a/mm/pagewalk.c
+++ b/mm/pagewalk.c
@@@ -194,75 -244,31 +263,39 @@@ int walk_page_range(unsigned long start
  	if (!walk->mm)
  		return -EINVAL;
  
 -	VM_BUG_ON_MM(!rwsem_is_locked(&walk->mm->mmap_sem), walk->mm);
 +	VM_BUG_ON(!rwsem_is_locked(&walk->mm->mmap_sem));
  
- 	pgd = pgd_offset(walk->mm, addr);
+ 	vma = find_vma(walk->mm, start);
  	do {
- 		struct vm_area_struct *vma = NULL;
- 
- 		next = pgd_addr_end(addr, end);
- 
- 		/*
- 		 * This function was not intended to be vma based.
- 		 * But there are vma special cases to be handled:
- 		 * - hugetlb vma's
- 		 * - VM_PFNMAP vma's
- 		 */
- 		vma = find_vma(walk->mm, addr);
- 		if (vma) {
- 			/*
- 			 * There are no page structures backing a VM_PFNMAP
- 			 * range, so do not allow split_huge_page_pmd().
- 			 */
- 			if ((vma->vm_start <= addr) &&
- 			    (vma->vm_flags & VM_PFNMAP)) {
- 				if (walk->pte_hole)
- 					err = walk->pte_hole(addr, next, walk);
- 				if (err)
- 					break;
- 				pgd = pgd_offset(walk->mm, next);
- 				continue;
- 			}
- 			/*
- 			 * Handle hugetlb vma individually because pagetable
- 			 * walk for the hugetlb page is dependent on the
- 			 * architecture and we can't handled it in the same
- 			 * manner as non-huge pages.
- 			 */
- 			if (walk->hugetlb_entry && (vma->vm_start <= addr) &&
- 			    is_vm_hugetlb_page(vma)) {
- 				if (vma->vm_end < next)
- 					next = vma->vm_end;
- 				/*
- 				 * Hugepage is very tightly coupled with vma,
- 				 * so walk through hugetlb entries within a
- 				 * given vma.
- 				 */
- 				err = walk_hugetlb_range(vma, addr, next, walk);
- 				if (err)
- 					break;
- 				pgd = pgd_offset(walk->mm, next);
+ 		if (!vma) { /* after the last vma */
+ 			walk->vma = NULL;
+ 			next = end;
+ 		} else if (start < vma->vm_start) { /* outside vma */
+ 			walk->vma = NULL;
+ 			next = min(end, vma->vm_start);
+ 		} else { /* inside vma */
+ 			walk->vma = vma;
+ 			next = min(end, vma->vm_end);
+ 			vma = vma->vm_next;
+ 
+ 			err = walk_page_test(start, next, walk);
+ 			if (err > 0)
  				continue;
- 			}
- 		}
- 
- 		if (pgd_none_or_clear_bad(pgd)) {
- 			if (walk->pte_hole)
- 				err = walk->pte_hole(addr, next, walk);
- 			if (err)
+ 			if (err < 0)
  				break;
- 			pgd++;
- 			continue;
  		}
++<<<<<<< HEAD
 +		if (walk->pgd_entry)
 +			err = walk->pgd_entry(pgd, addr, next, walk);
 +		if (!err &&
 +		    (walk->pud_entry || walk->pmd_entry || walk->pte_entry))
 +			err = walk_pud_range(pgd, addr, next, walk);
++=======
+ 		if (walk->vma || walk->pte_hole)
+ 			err = __walk_page_range(start, next, walk);
++>>>>>>> fafaa4264eba (pagewalk: improve vma handling)
  		if (err)
  			break;
- 		pgd++;
- 	} while (addr = next, addr < end);
- 
+ 	} while (start = next, start < end);
  	return err;
  }
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 8e43fc8aaa30..2354a6b1588a 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1289,10 +1289,16 @@ void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
  * @pte_entry: if set, called for each non-empty PTE (4th-level) entry
  * @pte_hole: if set, called for each hole at all levels
  * @hugetlb_entry: if set, called for each hugetlb entry
- *		   *Caution*: The caller must hold mmap_sem() if @hugetlb_entry
- * 			      is used.
+ * @test_walk: caller specific callback function to determine whether
+ *             we walk over the current vma or not. A positive returned
+ *             value means "do page table walk over the current vma,"
+ *             and a negative one means "abort current page table walk
+ *             right now." 0 means "skip the current vma."
+ * @mm:        mm_struct representing the target process of page table walk
+ * @vma:       vma currently walked (NULL if walking outside vmas)
+ * @private:   private data for callbacks' usage
  *
- * (see walk_page_range for more details)
+ * (see the comment on walk_page_range() for more details)
  */
 struct mm_walk {
 	int (*pgd_entry)(pgd_t *pgd, unsigned long addr,
@@ -1308,7 +1314,10 @@ struct mm_walk {
 	int (*hugetlb_entry)(pte_t *pte, unsigned long hmask,
 			     unsigned long addr, unsigned long next,
 			     struct mm_walk *walk);
+	int (*test_walk)(unsigned long addr, unsigned long next,
+			struct mm_walk *walk);
 	struct mm_struct *mm;
+	struct vm_area_struct *vma;
 	void *private;
 };
 
* Unmerged path mm/pagewalk.c
