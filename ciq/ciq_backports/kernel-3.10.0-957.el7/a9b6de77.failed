mm: fix __gup_device_huge vs unmap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit a9b6de77b1a3ff729f7bfc54b2e17711776a416c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a9b6de77.failed

get_user_pages_fast() for device pages is missing the typical validation
that all page references have been taken while the mapping was valid.
Without this validation truncate operations can not reliably coordinate
against new page reference events like O_DIRECT.

	Cc: <stable@vger.kernel.org>
Fixes: 3565fce3a659 ("mm, x86: get_user_pages() for dax mappings")
	Reported-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit a9b6de77b1a3ff729f7bfc54b2e17711776a416c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/gup.c
diff --cc mm/gup.c
index 20b926f646c5,84dd2063ca3d..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -932,3 -1294,568 +932,571 @@@ struct page *get_dump_page(unsigned lon
  	return page;
  }
  #endif /* CONFIG_ELF_CORE */
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Generic Fast GUP
+  *
+  * get_user_pages_fast attempts to pin user pages by walking the page
+  * tables directly and avoids taking locks. Thus the walker needs to be
+  * protected from page table pages being freed from under it, and should
+  * block any THP splits.
+  *
+  * One way to achieve this is to have the walker disable interrupts, and
+  * rely on IPIs from the TLB flushing code blocking before the page table
+  * pages are freed. This is unsuitable for architectures that do not need
+  * to broadcast an IPI when invalidating TLBs.
+  *
+  * Another way to achieve this is to batch up page table containing pages
+  * belonging to more than one mm_user, then rcu_sched a callback to free those
+  * pages. Disabling interrupts will allow the fast_gup walker to both block
+  * the rcu_sched callback, and an IPI that we broadcast for splitting THPs
+  * (which is a relatively rare event). The code below adopts this strategy.
+  *
+  * Before activating this code, please be aware that the following assumptions
+  * are currently made:
+  *
+  *  *) Either HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table() is used to
+  *  free pages containing page tables or TLB flushing requires IPI broadcast.
+  *
+  *  *) ptes can be read atomically by the architecture.
+  *
+  *  *) access_ok is sufficient to validate userspace address ranges.
+  *
+  * The last two assumptions can be relaxed by the addition of helper functions.
+  *
+  * This code is based heavily on the PowerPC implementation by Nick Piggin.
+  */
+ #ifdef CONFIG_HAVE_GENERIC_GUP
+ 
+ #ifndef gup_get_pte
+ /*
+  * We assume that the PTE can be read atomically. If this is not the case for
+  * your architecture, please provide the helper.
+  */
+ static inline pte_t gup_get_pte(pte_t *ptep)
+ {
+ 	return READ_ONCE(*ptep);
+ }
+ #endif
+ 
+ static void undo_dev_pagemap(int *nr, int nr_start, struct page **pages)
+ {
+ 	while ((*nr) - nr_start) {
+ 		struct page *page = pages[--(*nr)];
+ 
+ 		ClearPageReferenced(page);
+ 		put_page(page);
+ 	}
+ }
+ 
+ #ifdef __HAVE_ARCH_PTE_SPECIAL
+ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	struct dev_pagemap *pgmap = NULL;
+ 	int nr_start = *nr, ret = 0;
+ 	pte_t *ptep, *ptem;
+ 
+ 	ptem = ptep = pte_offset_map(&pmd, addr);
+ 	do {
+ 		pte_t pte = gup_get_pte(ptep);
+ 		struct page *head, *page;
+ 
+ 		/*
+ 		 * Similar to the PMD case below, NUMA hinting must take slow
+ 		 * path using the pte_protnone check.
+ 		 */
+ 		if (pte_protnone(pte))
+ 			goto pte_unmap;
+ 
+ 		if (!pte_access_permitted(pte, write))
+ 			goto pte_unmap;
+ 
+ 		if (pte_devmap(pte)) {
+ 			pgmap = get_dev_pagemap(pte_pfn(pte), pgmap);
+ 			if (unlikely(!pgmap)) {
+ 				undo_dev_pagemap(nr, nr_start, pages);
+ 				goto pte_unmap;
+ 			}
+ 		} else if (pte_special(pte))
+ 			goto pte_unmap;
+ 
+ 		VM_BUG_ON(!pfn_valid(pte_pfn(pte)));
+ 		page = pte_page(pte);
+ 		head = compound_head(page);
+ 
+ 		if (!page_cache_get_speculative(head))
+ 			goto pte_unmap;
+ 
+ 		if (unlikely(pte_val(pte) != pte_val(*ptep))) {
+ 			put_page(head);
+ 			goto pte_unmap;
+ 		}
+ 
+ 		VM_BUG_ON_PAGE(compound_head(page) != head, page);
+ 
+ 		SetPageReferenced(page);
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 
+ 	} while (ptep++, addr += PAGE_SIZE, addr != end);
+ 
+ 	ret = 1;
+ 
+ pte_unmap:
+ 	if (pgmap)
+ 		put_dev_pagemap(pgmap);
+ 	pte_unmap(ptem);
+ 	return ret;
+ }
+ #else
+ 
+ /*
+  * If we can't determine whether or not a pte is special, then fail immediately
+  * for ptes. Note, we can still pin HugeTLB and THP as these are guaranteed not
+  * to be special.
+  *
+  * For a futex to be placed on a THP tail page, get_futex_key requires a
+  * __get_user_pages_fast implementation that can pin pages. Thus it's still
+  * useful to have gup_huge_pmd even if we can't operate on ptes.
+  */
+ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	return 0;
+ }
+ #endif /* __HAVE_ARCH_PTE_SPECIAL */
+ 
+ #if defined(__HAVE_ARCH_PTE_DEVMAP) && defined(CONFIG_TRANSPARENT_HUGEPAGE)
+ static int __gup_device_huge(unsigned long pfn, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	int nr_start = *nr;
+ 	struct dev_pagemap *pgmap = NULL;
+ 
+ 	do {
+ 		struct page *page = pfn_to_page(pfn);
+ 
+ 		pgmap = get_dev_pagemap(pfn, pgmap);
+ 		if (unlikely(!pgmap)) {
+ 			undo_dev_pagemap(nr, nr_start, pages);
+ 			return 0;
+ 		}
+ 		SetPageReferenced(page);
+ 		pages[*nr] = page;
+ 		get_page(page);
+ 		(*nr)++;
+ 		pfn++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	if (pgmap)
+ 		put_dev_pagemap(pgmap);
+ 	return 1;
+ }
+ 
+ static int __gup_device_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	unsigned long fault_pfn;
+ 	int nr_start = *nr;
+ 
+ 	fault_pfn = pmd_pfn(orig) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 	if (!__gup_device_huge(fault_pfn, addr, end, pages, nr))
+ 		return 0;
+ 
+ 	if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {
+ 		undo_dev_pagemap(nr, nr_start, pages);
+ 		return 0;
+ 	}
+ 	return 1;
+ }
+ 
+ static int __gup_device_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	unsigned long fault_pfn;
+ 	int nr_start = *nr;
+ 
+ 	fault_pfn = pud_pfn(orig) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+ 	if (!__gup_device_huge(fault_pfn, addr, end, pages, nr))
+ 		return 0;
+ 
+ 	if (unlikely(pud_val(orig) != pud_val(*pudp))) {
+ 		undo_dev_pagemap(nr, nr_start, pages);
+ 		return 0;
+ 	}
+ 	return 1;
+ }
+ #else
+ static int __gup_device_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	BUILD_BUG();
+ 	return 0;
+ }
+ 
+ static int __gup_device_huge_pud(pud_t pud, pud_t *pudp, unsigned long addr,
+ 		unsigned long end, struct page **pages, int *nr)
+ {
+ 	BUILD_BUG();
+ 	return 0;
+ }
+ #endif
+ 
+ static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
+ 		unsigned long end, int write, struct page **pages, int *nr)
+ {
+ 	struct page *head, *page;
+ 	int refs;
+ 
+ 	if (!pmd_access_permitted(orig, write))
+ 		return 0;
+ 
+ 	if (pmd_devmap(orig))
+ 		return __gup_device_huge_pmd(orig, pmdp, addr, end, pages, nr);
+ 
+ 	refs = 0;
+ 	page = pmd_page(orig) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	head = compound_head(pmd_page(orig));
+ 	if (!page_cache_add_speculative(head, refs)) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	SetPageReferenced(head);
+ 	return 1;
+ }
+ 
+ static int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,
+ 		unsigned long end, int write, struct page **pages, int *nr)
+ {
+ 	struct page *head, *page;
+ 	int refs;
+ 
+ 	if (!pud_access_permitted(orig, write))
+ 		return 0;
+ 
+ 	if (pud_devmap(orig))
+ 		return __gup_device_huge_pud(orig, pudp, addr, end, pages, nr);
+ 
+ 	refs = 0;
+ 	page = pud_page(orig) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	head = compound_head(pud_page(orig));
+ 	if (!page_cache_add_speculative(head, refs)) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pud_val(orig) != pud_val(*pudp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	SetPageReferenced(head);
+ 	return 1;
+ }
+ 
+ static int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,
+ 			unsigned long end, int write,
+ 			struct page **pages, int *nr)
+ {
+ 	int refs;
+ 	struct page *head, *page;
+ 
+ 	if (!pgd_access_permitted(orig, write))
+ 		return 0;
+ 
+ 	BUILD_BUG_ON(pgd_devmap(orig));
+ 	refs = 0;
+ 	page = pgd_page(orig) + ((addr & ~PGDIR_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	head = compound_head(pgd_page(orig));
+ 	if (!page_cache_add_speculative(head, refs)) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pgd_val(orig) != pgd_val(*pgdp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	SetPageReferenced(head);
+ 	return 1;
+ }
+ 
+ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,
+ 		int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pmd_t *pmdp;
+ 
+ 	pmdp = pmd_offset(&pud, addr);
+ 	do {
+ 		pmd_t pmd = READ_ONCE(*pmdp);
+ 
+ 		next = pmd_addr_end(addr, end);
+ 		if (!pmd_present(pmd))
+ 			return 0;
+ 
+ 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
+ 			/*
+ 			 * NUMA hinting faults need to be handled in the GUP
+ 			 * slowpath for accounting purposes and so that they
+ 			 * can be serialised against THP migration.
+ 			 */
+ 			if (pmd_protnone(pmd))
+ 				return 0;
+ 
+ 			if (!gup_huge_pmd(pmd, pmdp, addr, next, write,
+ 				pages, nr))
+ 				return 0;
+ 
+ 		} else if (unlikely(is_hugepd(__hugepd(pmd_val(pmd))))) {
+ 			/*
+ 			 * architecture have different format for hugetlbfs
+ 			 * pmd format and THP pmd format
+ 			 */
+ 			if (!gup_huge_pd(__hugepd(pmd_val(pmd)), addr,
+ 					 PMD_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pte_range(pmd, addr, next, write, pages, nr))
+ 			return 0;
+ 	} while (pmdp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ static int gup_pud_range(p4d_t p4d, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pud_t *pudp;
+ 
+ 	pudp = pud_offset(&p4d, addr);
+ 	do {
+ 		pud_t pud = READ_ONCE(*pudp);
+ 
+ 		next = pud_addr_end(addr, end);
+ 		if (pud_none(pud))
+ 			return 0;
+ 		if (unlikely(pud_huge(pud))) {
+ 			if (!gup_huge_pud(pud, pudp, addr, next, write,
+ 					  pages, nr))
+ 				return 0;
+ 		} else if (unlikely(is_hugepd(__hugepd(pud_val(pud))))) {
+ 			if (!gup_huge_pd(__hugepd(pud_val(pud)), addr,
+ 					 PUD_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pmd_range(pud, addr, next, write, pages, nr))
+ 			return 0;
+ 	} while (pudp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ static int gup_p4d_range(pgd_t pgd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	p4d_t *p4dp;
+ 
+ 	p4dp = p4d_offset(&pgd, addr);
+ 	do {
+ 		p4d_t p4d = READ_ONCE(*p4dp);
+ 
+ 		next = p4d_addr_end(addr, end);
+ 		if (p4d_none(p4d))
+ 			return 0;
+ 		BUILD_BUG_ON(p4d_huge(p4d));
+ 		if (unlikely(is_hugepd(__hugepd(p4d_val(p4d))))) {
+ 			if (!gup_huge_pd(__hugepd(p4d_val(p4d)), addr,
+ 					 P4D_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pud_range(p4d, addr, next, write, pages, nr))
+ 			return 0;
+ 	} while (p4dp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ static void gup_pgd_range(unsigned long addr, unsigned long end,
+ 		int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pgd_t *pgdp;
+ 
+ 	pgdp = pgd_offset(current->mm, addr);
+ 	do {
+ 		pgd_t pgd = READ_ONCE(*pgdp);
+ 
+ 		next = pgd_addr_end(addr, end);
+ 		if (pgd_none(pgd))
+ 			return;
+ 		if (unlikely(pgd_huge(pgd))) {
+ 			if (!gup_huge_pgd(pgd, pgdp, addr, next, write,
+ 					  pages, nr))
+ 				return;
+ 		} else if (unlikely(is_hugepd(__hugepd(pgd_val(pgd))))) {
+ 			if (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,
+ 					 PGDIR_SHIFT, next, write, pages, nr))
+ 				return;
+ 		} else if (!gup_p4d_range(pgd, addr, next, write, pages, nr))
+ 			return;
+ 	} while (pgdp++, addr = next, addr != end);
+ }
+ 
+ #ifndef gup_fast_permitted
+ /*
+  * Check if it's allowed to use __get_user_pages_fast() for the range, or
+  * we need to fall back to the slow version:
+  */
+ bool gup_fast_permitted(unsigned long start, int nr_pages, int write)
+ {
+ 	unsigned long len, end;
+ 
+ 	len = (unsigned long) nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 	return end >= start;
+ }
+ #endif
+ 
+ /*
+  * Like get_user_pages_fast() except it's IRQ-safe in that it won't fall back to
+  * the regular GUP.
+  * Note a difference with get_user_pages_fast: this always returns the
+  * number of pages pinned, 0 if no pages were pinned.
+  */
+ int __get_user_pages_fast(unsigned long start, int nr_pages, int write,
+ 			  struct page **pages)
+ {
+ 	unsigned long addr, len, end;
+ 	unsigned long flags;
+ 	int nr = 0;
+ 
+ 	start &= PAGE_MASK;
+ 	addr = start;
+ 	len = (unsigned long) nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 
+ 	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
+ 					(void __user *)start, len)))
+ 		return 0;
+ 
+ 	/*
+ 	 * Disable interrupts.  We use the nested form as we can already have
+ 	 * interrupts disabled by get_futex_key.
+ 	 *
+ 	 * With interrupts disabled, we block page table pages from being
+ 	 * freed from under us. See mmu_gather_tlb in asm-generic/tlb.h
+ 	 * for more details.
+ 	 *
+ 	 * We do not adopt an rcu_read_lock(.) here as we also want to
+ 	 * block IPIs that come from THPs splitting.
+ 	 */
+ 
+ 	if (gup_fast_permitted(start, nr_pages, write)) {
+ 		local_irq_save(flags);
+ 		gup_pgd_range(addr, end, write, pages, &nr);
+ 		local_irq_restore(flags);
+ 	}
+ 
+ 	return nr;
+ }
+ 
+ /**
+  * get_user_pages_fast() - pin user pages in memory
+  * @start:	starting user address
+  * @nr_pages:	number of pages from start to pin
+  * @write:	whether pages will be written to
+  * @pages:	array that receives pointers to the pages pinned.
+  *		Should be at least nr_pages long.
+  *
+  * Attempt to pin user pages in memory without taking mm->mmap_sem.
+  * If not successful, it will fall back to taking the lock and
+  * calling get_user_pages().
+  *
+  * Returns number of pages pinned. This may be fewer than the number
+  * requested. If nr_pages is 0 or negative, returns 0. If no pages
+  * were pinned, returns -errno.
+  */
+ int get_user_pages_fast(unsigned long start, int nr_pages, int write,
+ 			struct page **pages)
+ {
+ 	unsigned long addr, len, end;
+ 	int nr = 0, ret = 0;
+ 
+ 	start &= PAGE_MASK;
+ 	addr = start;
+ 	len = (unsigned long) nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 
+ 	if (nr_pages <= 0)
+ 		return 0;
+ 
+ 	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
+ 					(void __user *)start, len)))
+ 		return -EFAULT;
+ 
+ 	if (gup_fast_permitted(start, nr_pages, write)) {
+ 		local_irq_disable();
+ 		gup_pgd_range(addr, end, write, pages, &nr);
+ 		local_irq_enable();
+ 		ret = nr;
+ 	}
+ 
+ 	if (nr < nr_pages) {
+ 		/* Try to get the remaining pages with get_user_pages */
+ 		start += nr << PAGE_SHIFT;
+ 		pages += nr;
+ 
+ 		ret = get_user_pages_unlocked(start, nr_pages - nr, pages,
+ 				write ? FOLL_WRITE : 0);
+ 
+ 		/* Have to be a bit careful with return values */
+ 		if (nr > 0) {
+ 			if (ret < 0)
+ 				ret = nr;
+ 			else
+ 				ret += nr;
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ #endif /* CONFIG_HAVE_GENERIC_GUP */
++>>>>>>> a9b6de77b1a3 (mm: fix __gup_device_huge vs unmap)
* Unmerged path mm/gup.c
