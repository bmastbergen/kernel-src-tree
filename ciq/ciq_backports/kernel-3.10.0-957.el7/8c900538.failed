nfp: bpf: implement memory bulk copy for length bigger than 32-bytes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jiong Wang <jiong.wang@netronome.com>
commit 8c90053858fce1ca60fab7be03bb61d314ea5c1c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/8c900538.failed

When the gathered copy length is bigger than 32-bytes and within 128-bytes
(the maximum length a single CPP Pull/Push request can finish), the
strategy of read/write are changeed into:

  * Read.
      - use direct reference mode when length is within 32-bytes.
      - use indirect mode when length is bigger than 32-bytes.

  * Write.
      - length <= 8-bytes
        use write8 (direct_ref).
      - length <= 32-byte and 4-bytes aligned
        use write32 (direct_ref).
      - length <= 32-bytes but not 4-bytes aligned
        use write8 (indirect_ref).
      - length > 32-bytes and 4-bytes aligned
        use write32 (indirect_ref).
      - length > 32-bytes and not 4-bytes aligned and <= 40-bytes
        use write32 (direct_ref) to finish the first 32-bytes.
        use write8 (direct_ref) to finish all remaining hanging part.
      - length > 32-bytes and not 4-bytes aligned
        use write32 (indirect_ref) to finish those 4-byte aligned parts.
        use write8 (direct_ref) to finish all remaining hanging part.

	Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
	Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 8c90053858fce1ca60fab7be03bb61d314ea5c1c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/jit.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/jit.c
index 35648b968309,1b98ef239605..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/jit.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/jit.c
@@@ -498,6 -522,147 +498,150 @@@ static void wrp_reg_mov(struct nfp_pro
  	wrp_mov(nfp_prog, reg_both(dst), reg_b(src));
  }
  
++<<<<<<< HEAD
++=======
+ /* wrp_reg_subpart() - load @field_len bytes from @offset of @src, write the
+  * result to @dst from low end.
+  */
+ static void
+ wrp_reg_subpart(struct nfp_prog *nfp_prog, swreg dst, swreg src, u8 field_len,
+ 		u8 offset)
+ {
+ 	enum shf_sc sc = offset ? SHF_SC_R_SHF : SHF_SC_NONE;
+ 	u8 mask = (1 << field_len) - 1;
+ 
+ 	emit_ld_field_any(nfp_prog, dst, mask, src, sc, offset * 8, true);
+ }
+ 
+ /* NFP has Command Push Pull bus which supports bluk memory operations. */
+ static int nfp_cpp_memcpy(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta)
+ {
+ 	bool descending_seq = meta->ldst_gather_len < 0;
+ 	s16 len = abs(meta->ldst_gather_len);
+ 	swreg src_base, off;
+ 	unsigned int i;
+ 	u8 xfer_num;
+ 
+ 	off = re_load_imm_any(nfp_prog, meta->insn.off, imm_b(nfp_prog));
+ 	src_base = reg_a(meta->insn.src_reg * 2);
+ 	xfer_num = round_up(len, 4) / 4;
+ 
+ 	/* Setup PREV_ALU fields to override memory read length. */
+ 	if (len > 32)
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, xfer_num - 1));
+ 
+ 	/* Memory read from source addr into transfer-in registers. */
+ 	emit_cmd_any(nfp_prog, CMD_TGT_READ32_SWAP, CMD_MODE_32b, 0, src_base,
+ 		     off, xfer_num - 1, true, len > 32);
+ 
+ 	/* Move from transfer-in to transfer-out. */
+ 	for (i = 0; i < xfer_num; i++)
+ 		wrp_mov(nfp_prog, reg_xfer(i), reg_xfer(i));
+ 
+ 	off = re_load_imm_any(nfp_prog, meta->paired_st->off, imm_b(nfp_prog));
+ 
+ 	if (len <= 8) {
+ 		/* Use single direct_ref write8. */
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 0,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, len - 1,
+ 			 true);
+ 	} else if (len <= 32 && IS_ALIGNED(len, 4)) {
+ 		/* Use single direct_ref write32. */
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, xfer_num - 1,
+ 			 true);
+ 	} else if (len <= 32) {
+ 		/* Use single indirect_ref write8. */
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, len - 1));
+ 		emit_cmd_indir(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 0,
+ 			       reg_a(meta->paired_st->dst_reg * 2), off,
+ 			       len - 1, true);
+ 	} else if (IS_ALIGNED(len, 4)) {
+ 		/* Use single indirect_ref write32. */
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, xfer_num - 1));
+ 		emit_cmd_indir(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			       reg_a(meta->paired_st->dst_reg * 2), off,
+ 			       xfer_num - 1, true);
+ 	} else if (len <= 40) {
+ 		/* Use one direct_ref write32 to write the first 32-bytes, then
+ 		 * another direct_ref write8 to write the remaining bytes.
+ 		 */
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, 7,
+ 			 true);
+ 
+ 		off = re_load_imm_any(nfp_prog, meta->paired_st->off + 32,
+ 				      imm_b(nfp_prog));
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 8,
+ 			 reg_a(meta->paired_st->dst_reg * 2), off, len - 33,
+ 			 true);
+ 	} else {
+ 		/* Use one indirect_ref write32 to write 4-bytes aligned length,
+ 		 * then another direct_ref write8 to write the remaining bytes.
+ 		 */
+ 		u8 new_off;
+ 
+ 		wrp_immed(nfp_prog, reg_none(),
+ 			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, xfer_num - 2));
+ 		emit_cmd_indir(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+ 			       reg_a(meta->paired_st->dst_reg * 2), off,
+ 			       xfer_num - 2, true);
+ 		new_off = meta->paired_st->off + (xfer_num - 1) * 4;
+ 		off = re_load_imm_any(nfp_prog, new_off, imm_b(nfp_prog));
+ 		emit_cmd(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b,
+ 			 xfer_num - 1, reg_a(meta->paired_st->dst_reg * 2), off,
+ 			 (len & 0x3) - 1, true);
+ 	}
+ 
+ 	/* TODO: The following extra load is to make sure data flow be identical
+ 	 *  before and after we do memory copy optimization.
+ 	 *
+ 	 *  The load destination register is not guaranteed to be dead, so we
+ 	 *  need to make sure it is loaded with the value the same as before
+ 	 *  this transformation.
+ 	 *
+ 	 *  These extra loads could be removed once we have accurate register
+ 	 *  usage information.
+ 	 */
+ 	if (descending_seq)
+ 		xfer_num = 0;
+ 	else if (BPF_SIZE(meta->insn.code) != BPF_DW)
+ 		xfer_num = xfer_num - 1;
+ 	else
+ 		xfer_num = xfer_num - 2;
+ 
+ 	switch (BPF_SIZE(meta->insn.code)) {
+ 	case BPF_B:
+ 		wrp_reg_subpart(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 				reg_xfer(xfer_num), 1,
+ 				IS_ALIGNED(len, 4) ? 3 : (len & 3) - 1);
+ 		break;
+ 	case BPF_H:
+ 		wrp_reg_subpart(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 				reg_xfer(xfer_num), 2, (len & 3) ^ 2);
+ 		break;
+ 	case BPF_W:
+ 		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 			reg_xfer(0));
+ 		break;
+ 	case BPF_DW:
+ 		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+ 			reg_xfer(xfer_num));
+ 		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2 + 1),
+ 			reg_xfer(xfer_num + 1));
+ 		break;
+ 	}
+ 
+ 	if (BPF_SIZE(meta->insn.code) != BPF_DW)
+ 		wrp_immed(nfp_prog, reg_both(meta->insn.dst_reg * 2 + 1), 0);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 8c90053858fc (nfp: bpf: implement memory bulk copy for length bigger than 32-bytes)
  static int
  data_ld(struct nfp_prog *nfp_prog, swreg offset, u8 dst_gpr, int size)
  {
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/jit.c
