mm: rename FOLL_MLOCK to FOLL_POPULATE

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] rename FOLL_MLOCK to FOLL_POPULATE (Rafael Aquini) [1560030]
Rebuild_FUZZ: 94.44%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit 84d33df279e0380995b0e03fb8aad04cef2bc29f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/84d33df2.failed

After commit a1fde08c74e9 ("VM: skip the stack guard page lookup in
get_user_pages only for mlock") FOLL_MLOCK has lost its original
meaning: we don't necessarily mlock the page if the flags is set -- we
also take VM_LOCKED into consideration.

Since we use the same codepath for __mm_populate(), let's rename
FOLL_MLOCK to FOLL_POPULATE.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Acked-by: David Rientjes <rientjes@google.com>
	Cc: Michel Lespinasse <walken@google.com>
	Cc: Rik van Riel <riel@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 84d33df279e0380995b0e03fb8aad04cef2bc29f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/gup.c
#	mm/huge_memory.c
diff --cc mm/gup.c
index 20b926f646c5,1b114ba9aebf..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -269,21 -123,238 +269,171 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -/**
 - * follow_page_mask - look up a page descriptor from a user-virtual address
 - * @vma: vm_area_struct mapping @address
 - * @address: virtual address to look up
 - * @flags: flags modifying lookup behaviour
 - * @page_mask: on output, *page_mask is set according to the size of the page
 - *
 - * @flags can have FOLL_ flags set, defined in <linux/mm.h>
 - *
 - * Returns the mapped (struct page *), %NULL if no mapping exists, or
 - * an error pointer if there is a mapping to something not represented
 - * by a page descriptor (see also vm_normal_page()).
 - */
 -struct page *follow_page_mask(struct vm_area_struct *vma,
 -			      unsigned long address, unsigned int flags,
 -			      unsigned int *page_mask)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	*page_mask = 0;
 -
 -	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 -	if (!IS_ERR(page)) {
 -		BUG_ON(flags & FOLL_GET);
  		return page;
++<<<<<<< HEAD
++=======
+ 	}
+ 
+ 	pgd = pgd_offset(mm, address);
+ 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pud = pud_offset(pgd, address);
+ 	if (pud_none(*pud))
+ 		return no_page_table(vma, flags);
+ 	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pud(mm, address, pud, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if (unlikely(pud_bad(*pud)))
+ 		return no_page_table(vma, flags);
+ 
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
+ 		page = follow_huge_pmd(mm, address, pmd, flags);
+ 		if (page)
+ 			return page;
+ 		return no_page_table(vma, flags);
+ 	}
+ 	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
+ 		return no_page_table(vma, flags);
+ 	if (pmd_trans_huge(*pmd)) {
+ 		if (flags & FOLL_SPLIT) {
+ 			split_huge_page_pmd(vma, address, pmd);
+ 			return follow_page_pte(vma, address, pmd, flags);
+ 		}
+ 		ptl = pmd_lock(mm, pmd);
+ 		if (likely(pmd_trans_huge(*pmd))) {
+ 			if (unlikely(pmd_trans_splitting(*pmd))) {
+ 				spin_unlock(ptl);
+ 				wait_split_huge_page(vma->anon_vma, pmd);
+ 			} else {
+ 				page = follow_trans_huge_pmd(vma, address,
+ 							     pmd, flags);
+ 				spin_unlock(ptl);
+ 				*page_mask = HPAGE_PMD_NR - 1;
+ 				return page;
+ 			}
+ 		} else
+ 			spin_unlock(ptl);
+ 	}
+ 	return follow_page_pte(vma, address, pmd, flags);
+ }
+ 
+ static int get_gate_page(struct mm_struct *mm, unsigned long address,
+ 		unsigned int gup_flags, struct vm_area_struct **vma,
+ 		struct page **page)
+ {
+ 	pgd_t *pgd;
+ 	pud_t *pud;
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 	int ret = -EFAULT;
+ 
+ 	/* user gate pages are read-only */
+ 	if (gup_flags & FOLL_WRITE)
+ 		return -EFAULT;
+ 	if (address > TASK_SIZE)
+ 		pgd = pgd_offset_k(address);
+ 	else
+ 		pgd = pgd_offset_gate(mm, address);
+ 	BUG_ON(pgd_none(*pgd));
+ 	pud = pud_offset(pgd, address);
+ 	BUG_ON(pud_none(*pud));
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return -EFAULT;
+ 	VM_BUG_ON(pmd_trans_huge(*pmd));
+ 	pte = pte_offset_map(pmd, address);
+ 	if (pte_none(*pte))
+ 		goto unmap;
+ 	*vma = get_gate_vma(mm);
+ 	if (!page)
+ 		goto out;
+ 	*page = vm_normal_page(*vma, address, *pte);
+ 	if (!*page) {
+ 		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
+ 			goto unmap;
+ 		*page = pte_page(*pte);
+ 	}
+ 	get_page(*page);
+ out:
+ 	ret = 0;
+ unmap:
+ 	pte_unmap(pte);
+ 	return ret;
+ }
+ 
+ /*
+  * mmap_sem must be held on entry.  If @nonblocking != NULL and
+  * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.
+  * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
+  */
+ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
+ 		unsigned long address, unsigned int *flags, int *nonblocking)
+ {
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned int fault_flags = 0;
+ 	int ret;
+ 
+ 	/* For mm_populate(), just skip the stack guard page. */
+ 	if ((*flags & FOLL_POPULATE) &&
+ 			(stack_guard_page_start(vma, address) ||
+ 			 stack_guard_page_end(vma, address + PAGE_SIZE)))
+ 		return -ENOENT;
+ 	if (*flags & FOLL_WRITE)
+ 		fault_flags |= FAULT_FLAG_WRITE;
+ 	if (nonblocking)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
+ 	if (*flags & FOLL_NOWAIT)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
+ 	if (*flags & FOLL_TRIED) {
+ 		VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
+ 		fault_flags |= FAULT_FLAG_TRIED;
+ 	}
+ 
+ 	ret = handle_mm_fault(mm, vma, address, fault_flags);
+ 	if (ret & VM_FAULT_ERROR) {
+ 		if (ret & VM_FAULT_OOM)
+ 			return -ENOMEM;
+ 		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
+ 			return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;
+ 		if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
+ 			return -EFAULT;
+ 		BUG();
+ 	}
+ 
+ 	if (tsk) {
+ 		if (ret & VM_FAULT_MAJOR)
+ 			tsk->maj_flt++;
+ 		else
+ 			tsk->min_flt++;
+ 	}
+ 
+ 	if (ret & VM_FAULT_RETRY) {
+ 		if (nonblocking)
+ 			*nonblocking = 0;
+ 		return -EBUSY;
+ 	}
++>>>>>>> 84d33df279e0 (mm: rename FOLL_MLOCK to FOLL_POPULATE)
  
 +no_page_table:
  	/*
 -	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
 -	 * necessary, even if maybe_mkwrite decided not to set pte_write. We
 -	 * can thus safely do subsequent page lookups as if they were reads.
 -	 * But only do so when looping for pte_write is futile: in some cases
 -	 * userspace may also be wanting to write to the gotten user page,
 -	 * which a read fault here might prevent (a readonly page might get
 -	 * reCOWed by userspace write).
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
  	 */
 -	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
 -		*flags &= ~FOLL_WRITE;
 -	return 0;
 -}
 -
 -static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 -{
 -	vm_flags_t vm_flags = vma->vm_flags;
 -
 -	if (vm_flags & (VM_IO | VM_PFNMAP))
 -		return -EFAULT;
 -
 -	if (gup_flags & FOLL_WRITE) {
 -		if (!(vm_flags & VM_WRITE)) {
 -			if (!(gup_flags & FOLL_FORCE))
 -				return -EFAULT;
 -			/*
 -			 * We used to let the write,force case do COW in a
 -			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
 -			 * set a breakpoint in a read-only mapping of an
 -			 * executable, without corrupting the file (yet only
 -			 * when that file had been opened for writing!).
 -			 * Anon pages in shared mappings are surprising: now
 -			 * just reject it.
 -			 */
 -			if (!is_cow_mapping(vm_flags)) {
 -				WARN_ON_ONCE(vm_flags & VM_MAYWRITE);
 -				return -EFAULT;
 -			}
 -		}
 -	} else if (!(vm_flags & VM_READ)) {
 -		if (!(gup_flags & FOLL_FORCE))
 -			return -EFAULT;
 -		/*
 -		 * Is there actually any vma we can reach here which does not
 -		 * have VM_MAYREAD set?
 -		 */
 -		if (!(vm_flags & VM_MAYREAD))
 -			return -EFAULT;
 -	}
 -	return 0;
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
 +	return page;
  }
  
  /**
diff --cc mm/huge_memory.c
index 14da2891fc9a,10a4b6cea0d1..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1605,9 -1216,22 +1605,28 @@@ struct page *follow_trans_huge_pmd(stru
  
  	page = pmd_page(*pmd);
  	VM_BUG_ON_PAGE(!PageHead(page), page);
++<<<<<<< HEAD
 +	if (flags & FOLL_TOUCH)
 +		touch_pmd(vma, addr, pmd);
 +	if ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
++=======
+ 	if (flags & FOLL_TOUCH) {
+ 		pmd_t _pmd;
+ 		/*
+ 		 * We should set the dirty bit only for FOLL_WRITE but
+ 		 * for now the dirty bit in the pmd is meaningless.
+ 		 * And if the dirty bit will become meaningful and
+ 		 * we'll only set it with FOLL_WRITE, an atomic
+ 		 * set_bit will be required on the pmd to set the
+ 		 * young bit, instead of the current set_pmd_at.
+ 		 */
+ 		_pmd = pmd_mkyoung(pmd_mkdirty(*pmd));
+ 		if (pmdp_set_access_flags(vma, addr & HPAGE_PMD_MASK,
+ 					  pmd, _pmd,  1))
+ 			update_mmu_cache_pmd(vma, addr, pmd);
+ 	}
+ 	if ((flags & FOLL_POPULATE) && (vma->vm_flags & VM_LOCKED)) {
++>>>>>>> 84d33df279e0 (mm: rename FOLL_MLOCK to FOLL_POPULATE)
  		if (page->mapping && trylock_page(page)) {
  			lru_add_drain();
  			if (page->mapping)
diff --git a/include/linux/mm.h b/include/linux/mm.h
index e9281aafa708..9b039e81b4ab 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2270,7 +2270,7 @@ static inline struct page *follow_page(struct vm_area_struct *vma,
 #define FOLL_FORCE	0x10	/* get_user_pages read/write w/o permission */
 #define FOLL_NOWAIT	0x20	/* if a disk transfer is needed, start the IO
 				 * and return without waiting upon it */
-#define FOLL_MLOCK	0x40	/* mark page as mlocked */
+#define FOLL_POPULATE	0x40	/* fault in page */
 #define FOLL_SPLIT	0x80	/* don't return transhuge pages, split them */
 #define FOLL_HWPOISON	0x100	/* check page is hwpoisoned */
 #define FOLL_NUMA	0x200	/* force NUMA hinting page fault */
* Unmerged path mm/gup.c
* Unmerged path mm/huge_memory.c
diff --git a/mm/mlock.c b/mm/mlock.c
index 85f2de22baef..63b7cfba8a1c 100644
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@ -192,7 +192,7 @@ long __mlock_vma_pages_range(struct vm_area_struct *vma,
 	VM_BUG_ON(end   > vma->vm_end);
 	VM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));
 
-	gup_flags = FOLL_TOUCH | FOLL_MLOCK;
+	gup_flags = FOLL_TOUCH | FOLL_POPULATE;
 	/*
 	 * We want to touch writable mappings with a write fault in order
 	 * to break COW, except for shared mappings because these don't COW
