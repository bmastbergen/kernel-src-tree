mm/rmap: extend rmap_walk_xxx() to cope with different cases

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] rmap: extend rmap_walk_xxx() to cope with different cases (Rafael Aquini) [1562137]
Rebuild_FUZZ: 97.44%
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit 0dd1c7bbce8d1d142bb25aefaa50262dfd77cb78
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/0dd1c7bb.failed

There are a lot of common parts in traversing functions, but there are
also a little of uncommon parts in it.  By assigning proper function
pointer on each rmap_walker_control, we can handle these difference
correctly.

Following are differences we should handle.

1. difference of lock function in anon mapping case
2. nonlinear handling in file mapping case
3. prechecked condition:
	checking memcg in page_referenced(),
	checking VM_SHARE in page_mkclean()
	checking temporary vma in try_to_unmap()
4. exit condition:
	checking page_mapped() in try_to_unmap()

So, in this patch, I introduce 4 function pointers to handle above
differences.

	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: Hillf Danton <dhillf@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0dd1c7bbce8d1d142bb25aefaa50262dfd77cb78)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rmap.h
#	mm/ksm.c
#	mm/rmap.c
diff --cc include/linux/rmap.h
index 4fef883ac5c3,616aa4d05f0a..000000000000
--- a/include/linux/rmap.h
+++ b/include/linux/rmap.h
@@@ -245,6 -235,27 +245,30 @@@ struct anon_vma *page_lock_anon_vma_rea
  void page_unlock_anon_vma_read(struct anon_vma *anon_vma);
  int page_mapped_in_vma(struct page *page, struct vm_area_struct *vma);
  
++<<<<<<< HEAD
++=======
+ /*
+  * rmap_walk_control: To control rmap traversing for specific needs
+  *
+  * arg: passed to rmap_one() and invalid_vma()
+  * rmap_one: executed on each vma where page is mapped
+  * done: for checking traversing termination condition
+  * file_nonlinear: for handling file nonlinear mapping
+  * anon_lock: for getting anon_lock by optimized way rather than default
+  * invalid_vma: for skipping uninterested vma
+  */
+ struct rmap_walk_control {
+ 	void *arg;
+ 	int (*rmap_one)(struct page *page, struct vm_area_struct *vma,
+ 					unsigned long addr, void *arg);
+ 	int (*done)(struct page *page);
+ 	int (*file_nonlinear)(struct page *, struct address_space *,
+ 					struct vm_area_struct *vma);
+ 	struct anon_vma *(*anon_lock)(struct page *page);
+ 	bool (*invalid_vma)(struct vm_area_struct *vma, void *arg);
+ };
+ 
++>>>>>>> 0dd1c7bbce8d (mm/rmap: extend rmap_walk_xxx() to cope with different cases)
  /*
   * Called by migrate.c to remove migration ptes, but might be used more later.
   */
diff --cc mm/ksm.c
index 1813339bf866,91b8cb35f7cc..000000000000
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@@ -2644,7 -2032,11 +2644,15 @@@ again
  			if ((rmap_item->mm == vma->vm_mm) == search_new_forks)
  				continue;
  
++<<<<<<< HEAD
 +			ret = rmap_one(page, vma, rmap_item->address, arg);
++=======
+ 			if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
+ 				continue;
+ 
+ 			ret = rwc->rmap_one(page, vma,
+ 					rmap_item->address, rwc->arg);
++>>>>>>> 0dd1c7bbce8d (mm/rmap: extend rmap_walk_xxx() to cope with different cases)
  			if (ret != SWAP_AGAIN) {
  				anon_vma_unlock_read(anon_vma);
  				goto out;
diff --cc mm/rmap.c
index 794c49685fc7,97bf8f0396f8..000000000000
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@@ -1895,18 -1682,16 +1895,26 @@@ void __put_anon_vma(struct anon_vma *an
  }
  
  #ifdef CONFIG_MIGRATION
++<<<<<<< HEAD
 +/*
 + * rmap_walk() and its helpers rmap_walk_anon() and rmap_walk_file():
 + * Called by migrate.c to remove migration ptes, but might be used more later.
 + */
 +static int rmap_walk_anon(struct page *page, int (*rmap_one)(struct page *,
 +		struct vm_area_struct *, unsigned long, void *), void *arg)
++=======
+ static struct anon_vma *rmap_walk_anon_lock(struct page *page,
+ 					struct rmap_walk_control *rwc)
++>>>>>>> 0dd1c7bbce8d (mm/rmap: extend rmap_walk_xxx() to cope with different cases)
  {
  	struct anon_vma *anon_vma;
 +	pgoff_t pgoff;
 +	struct anon_vma_chain *avc;
 +	int ret = SWAP_AGAIN;
  
+ 	if (rwc->anon_lock)
+ 		return rwc->anon_lock(page);
+ 
  	/*
  	 * Note: remove_migration_ptes() cannot use page_lock_anon_vma_read()
  	 * because that depends on page_mapped(); but not all its usages
@@@ -1915,17 -1700,39 +1923,51 @@@
  	 */
  	anon_vma = page_anon_vma(page);
  	if (!anon_vma)
++<<<<<<< HEAD
++=======
+ 		return NULL;
+ 
+ 	anon_vma_lock_read(anon_vma);
+ 	return anon_vma;
+ }
+ 
+ /*
+  * rmap_walk() and its helpers rmap_walk_anon() and rmap_walk_file():
+  * Called by migrate.c to remove migration ptes, but might be used more later.
+  */
+ static int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)
+ {
+ 	struct anon_vma *anon_vma;
+ 	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
+ 	struct anon_vma_chain *avc;
+ 	int ret = SWAP_AGAIN;
+ 
+ 	anon_vma = rmap_walk_anon_lock(page, rwc);
+ 	if (!anon_vma)
++>>>>>>> 0dd1c7bbce8d (mm/rmap: extend rmap_walk_xxx() to cope with different cases)
  		return ret;
 -
 +	anon_vma_lock_read(anon_vma);
 +	pgoff = page_to_pgoff(page);
  	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
  		struct vm_area_struct *vma = avc->vma;
  		unsigned long address = vma_address(page, vma);
++<<<<<<< HEAD
 +		ret = rmap_one(page, vma, address, arg);
 +		if (ret != SWAP_AGAIN)
 +			break;
 +
 +		cond_resched();
++=======
+ 
+ 		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
+ 			continue;
+ 
+ 		ret = rwc->rmap_one(page, vma, address, rwc->arg);
+ 		if (ret != SWAP_AGAIN)
+ 			break;
+ 		if (rwc->done && rwc->done(page))
+ 			break;
++>>>>>>> 0dd1c7bbce8d (mm/rmap: extend rmap_walk_xxx() to cope with different cases)
  	}
  	anon_vma_unlock_read(anon_vma);
  	return ret;
@@@ -1945,17 -1750,26 +1987,34 @@@ static int rmap_walk_file(struct page *
  	mutex_lock(&mapping->i_mmap_mutex);
  	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
  		unsigned long address = vma_address(page, vma);
++<<<<<<< HEAD
 +		ret = rmap_one(page, vma, address, arg);
 +		if (ret != SWAP_AGAIN)
 +			break;
 +
 +		cond_resched();
++=======
+ 
+ 		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
+ 			continue;
+ 
+ 		ret = rwc->rmap_one(page, vma, address, rwc->arg);
+ 		if (ret != SWAP_AGAIN)
+ 			goto done;
+ 		if (rwc->done && rwc->done(page))
+ 			goto done;
++>>>>>>> 0dd1c7bbce8d (mm/rmap: extend rmap_walk_xxx() to cope with different cases)
  	}
- 	/*
- 	 * No nonlinear handling: being always shared, nonlinear vmas
- 	 * never contain migration ptes.  Decide what to do about this
- 	 * limitation to linear when we need rmap_walk() on nonlinear.
- 	 */
+ 
+ 	if (!rwc->file_nonlinear)
+ 		goto done;
+ 
+ 	if (list_empty(&mapping->i_mmap_nonlinear))
+ 		goto done;
+ 
+ 	ret = rwc->file_nonlinear(page, mapping, vma);
+ 
+ done:
  	mutex_unlock(&mapping->i_mmap_mutex);
  	return ret;
  }
* Unmerged path include/linux/rmap.h
* Unmerged path mm/ksm.c
* Unmerged path mm/rmap.c
