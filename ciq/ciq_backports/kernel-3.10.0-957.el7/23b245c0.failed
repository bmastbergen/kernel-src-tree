md/raid1/10: avoid unnecessary locking

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [md] raid1/10: avoid unnecessary locking (Nigel Croxon) [1494474]
Rebuild_FUZZ: 95.89%
commit-author Shaohua Li <shli@fb.com>
commit 23b245c04d0ef408087430dd4d1b214a5da1eb78
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/23b245c0.failed

If we add bios to block plugging list, locking is unnecessry, since the block
unplug is guaranteed not to run at that time.

	Reviewed-by: NeilBrown <neilb@suse.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 23b245c04d0ef408087430dd4d1b214a5da1eb78)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid10.c
diff --cc drivers/md/raid10.c
index 17d84aee79e2,4343d7ff9916..000000000000
--- a/drivers/md/raid10.c
+++ b/drivers/md/raid10.c
@@@ -1162,80 -1111,197 +1162,148 @@@ static void raid10_unplug(struct blk_pl
  	kfree(plug);
  }
  
 -static void raid10_read_request(struct mddev *mddev, struct bio *bio,
 -				struct r10bio *r10_bio)
 +static bool raid10_make_request(struct mddev *mddev, struct bio * bio)
  {
  	struct r10conf *conf = mddev->private;
 +	struct r10bio *r10_bio;
  	struct bio *read_bio;
 -	const int op = bio_op(bio);
 -	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
 -	int max_sectors;
 -	sector_t sectors;
 -	struct md_rdev *rdev;
 -	char b[BDEVNAME_SIZE];
 -	int slot = r10_bio->read_slot;
 -	struct md_rdev *err_rdev = NULL;
 -	gfp_t gfp = GFP_NOIO;
 -
 -	if (r10_bio->devs[slot].rdev) {
 -		/*
 -		 * This is an error retry, but we cannot
 -		 * safely dereference the rdev in the r10_bio,
 -		 * we must use the one in conf.
 -		 * If it has already been disconnected (unlikely)
 -		 * we lose the device name in error messages.
 -		 */
 -		int disk;
 -		/*
 -		 * As we are blocking raid10, it is a little safer to
 -		 * use __GFP_HIGH.
 -		 */
 -		gfp = GFP_NOIO | __GFP_HIGH;
 -
 -		rcu_read_lock();
 -		disk = r10_bio->devs[slot].devnum;
 -		err_rdev = rcu_dereference(conf->mirrors[disk].rdev);
 -		if (err_rdev)
 -			bdevname(err_rdev->bdev, b);
 -		else {
 -			strcpy(b, "???");
 -			/* This never gets dereferenced */
 -			err_rdev = r10_bio->devs[slot].rdev;
 -		}
 -		rcu_read_unlock();
 -	}
 -	/*
 -	 * Register the new request and wait if the reconstruction
 -	 * thread has put up a bar for new requests.
 -	 * Continue immediately if no resync is active currently.
 -	 */
 -	wait_barrier(conf);
 -
 -	sectors = r10_bio->sectors;
 -	while (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
 -	    bio->bi_iter.bi_sector < conf->reshape_progress &&
 -	    bio->bi_iter.bi_sector + sectors > conf->reshape_progress) {
 -		/*
 -		 * IO spans the reshape position.  Need to wait for reshape to
 -		 * pass
 -		 */
 -		raid10_log(conf->mddev, "wait reshape");
 -		allow_barrier(conf);
 -		wait_event(conf->wait_barrier,
 -			   conf->reshape_progress <= bio->bi_iter.bi_sector ||
 -			   conf->reshape_progress >= bio->bi_iter.bi_sector +
 -			   sectors);
 -		wait_barrier(conf);
 -	}
 -
 -	rdev = read_balance(conf, r10_bio, &max_sectors);
 -	if (!rdev) {
 -		if (err_rdev) {
 -			pr_crit_ratelimited("md/raid10:%s: %s: unrecoverable I/O read error for block %llu\n",
 -					    mdname(mddev), b,
 -					    (unsigned long long)r10_bio->sector);
 -		}
 -		raid_end_bio_io(r10_bio);
 -		return;
 -	}
 -	if (err_rdev)
 -		pr_err_ratelimited("md/raid10:%s: %s: redirecting sector %llu to another mirror\n",
 -				   mdname(mddev),
 -				   bdevname(rdev->bdev, b),
 -				   (unsigned long long)r10_bio->sector);
 -	if (max_sectors < bio_sectors(bio)) {
 -		struct bio *split = bio_split(bio, max_sectors,
 -					      gfp, conf->bio_split);
 -		bio_chain(split, bio);
 -		generic_make_request(bio);
 -		bio = split;
 -		r10_bio->master_bio = bio;
 -		r10_bio->sectors = max_sectors;
 -	}
 -	slot = r10_bio->read_slot;
 -
 -	read_bio = bio_clone_fast(bio, gfp, mddev->bio_set);
 -
 -	r10_bio->devs[slot].bio = read_bio;
 -	r10_bio->devs[slot].rdev = rdev;
 -
 -	read_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +
 -		choose_data_offset(r10_bio, rdev);
 -	read_bio->bi_bdev = rdev->bdev;
 -	read_bio->bi_end_io = raid10_end_read_request;
 -	bio_set_op_attrs(read_bio, op, do_sync);
 -	if (test_bit(FailFast, &rdev->flags) &&
 -	    test_bit(R10BIO_FailFast, &r10_bio->state))
 -	        read_bio->bi_opf |= MD_FAILFAST;
 -	read_bio->bi_private = r10_bio;
 -
 -	if (mddev->gendisk)
 -	        trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
 -	                              read_bio, disk_devt(mddev->gendisk),
 -	                              r10_bio->sector);
 -	generic_make_request(read_bio);
 -	return;
 -}
 -
 -static void raid10_write_one_disk(struct mddev *mddev, struct r10bio *r10_bio,
 -				  struct bio *bio, bool replacement,
 -				  int n_copy)
 -{
 -	const int op = bio_op(bio);
 -	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
 -	const unsigned long do_fua = (bio->bi_opf & REQ_FUA);
 +	int i;
 +	sector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);
 +	int chunk_sects = chunk_mask + 1;
 +	const int rw = bio_data_dir(bio);
 +	const unsigned long do_sync = (bio->bi_rw & REQ_SYNC);
 +	const unsigned long do_fua = (bio->bi_rw & REQ_FUA);
 +	const unsigned long do_discard = (bio->bi_rw
 +					  & (REQ_DISCARD | REQ_SECURE));
 +	const unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);
  	unsigned long flags;
 +	struct md_rdev *blocked_rdev;
  	struct blk_plug_cb *cb;
  	struct raid10_plug_cb *plug = NULL;
++<<<<<<< HEAD
 +	int sectors_handled;
++=======
+ 	struct r10conf *conf = mddev->private;
+ 	struct md_rdev *rdev;
+ 	int devnum = r10_bio->devs[n_copy].devnum;
+ 	struct bio *mbio;
+ 
+ 	if (replacement) {
+ 		rdev = conf->mirrors[devnum].replacement;
+ 		if (rdev == NULL) {
+ 			/* Replacement just got moved to main 'rdev' */
+ 			smp_mb();
+ 			rdev = conf->mirrors[devnum].rdev;
+ 		}
+ 	} else
+ 		rdev = conf->mirrors[devnum].rdev;
+ 
+ 	mbio = bio_clone_fast(bio, GFP_NOIO, mddev->bio_set);
+ 	if (replacement)
+ 		r10_bio->devs[n_copy].repl_bio = mbio;
+ 	else
+ 		r10_bio->devs[n_copy].bio = mbio;
+ 
+ 	mbio->bi_iter.bi_sector	= (r10_bio->devs[n_copy].addr +
+ 				   choose_data_offset(r10_bio, rdev));
+ 	mbio->bi_bdev = rdev->bdev;
+ 	mbio->bi_end_io	= raid10_end_write_request;
+ 	bio_set_op_attrs(mbio, op, do_sync | do_fua);
+ 	if (!replacement && test_bit(FailFast,
+ 				     &conf->mirrors[devnum].rdev->flags)
+ 			 && enough(conf, devnum))
+ 		mbio->bi_opf |= MD_FAILFAST;
+ 	mbio->bi_private = r10_bio;
+ 
+ 	if (conf->mddev->gendisk)
+ 		trace_block_bio_remap(bdev_get_queue(mbio->bi_bdev),
+ 				      mbio, disk_devt(conf->mddev->gendisk),
+ 				      r10_bio->sector);
+ 	/* flush_pending_writes() needs access to the rdev so...*/
+ 	mbio->bi_bdev = (void *)rdev;
+ 
+ 	atomic_inc(&r10_bio->remaining);
+ 
+ 	cb = blk_check_plugged(raid10_unplug, mddev, sizeof(*plug));
+ 	if (cb)
+ 		plug = container_of(cb, struct raid10_plug_cb, cb);
+ 	else
+ 		plug = NULL;
+ 	if (plug) {
+ 		bio_list_add(&plug->pending, mbio);
+ 		plug->pending_cnt++;
+ 	} else {
+ 		spin_lock_irqsave(&conf->device_lock, flags);
+ 		bio_list_add(&conf->pending_bio_list, mbio);
+ 		conf->pending_count++;
+ 		spin_unlock_irqrestore(&conf->device_lock, flags);
+ 		md_wakeup_thread(mddev->thread);
+ 	}
+ }
+ 
+ static void raid10_write_request(struct mddev *mddev, struct bio *bio,
+ 				 struct r10bio *r10_bio)
+ {
+ 	struct r10conf *conf = mddev->private;
+ 	int i;
+ 	struct md_rdev *blocked_rdev;
+ 	sector_t sectors;
++>>>>>>> 23b245c04d0e (md/raid1/10: avoid unnecessary locking)
  	int max_sectors;
 +	int sectors;
 +
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
 +		md_flush_request(mddev, bio);
 +		return true;
 +	}
 +
 +	/* If this request crosses a chunk boundary, we need to
 +	 * split it.  This will only happen for 1 PAGE (or less) requests.
 +	 */
 +	if (unlikely((bio->bi_sector & chunk_mask) + bio_sectors(bio)
 +		     > chunk_sects
 +		     && (conf->geo.near_copies < conf->geo.raid_disks
 +			 || conf->prev.near_copies < conf->prev.raid_disks))) {
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		bp = bio_split(bio,
 +			       chunk_sects - (bio->bi_sector & (chunk_sects - 1)) );
 +
 +		/* Each of these 'make_request' calls will call 'wait_barrier'.
 +		 * If the first succeeds but the second blocks due to the resync
 +		 * thread raising the barrier, we will deadlock because the
 +		 * IO to the underlying device will be queued in generic_make_request
 +		 * and will never complete, so will never reduce nr_pending.
 +		 * So increment nr_waiting here so no new raise_barriers will
 +		 * succeed, and so the second wait_barrier cannot block.
 +		 */
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting++;
 +		spin_unlock_irq(&conf->resync_lock);
 +
 +		raid10_make_request(mddev, &bp->bio1);
 +		raid10_make_request(mddev, &bp->bio2);
 +
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting--;
 +		wake_up(&conf->wait_barrier);
 +		spin_unlock_irq(&conf->resync_lock);
 +
 +		bio_pair_release(bp);
 +		return true;
 +	bad_map:
 +		printk("md/raid10:%s: make_request bug: can't convert block across chunks"
 +		       " or bigger than %dk %llu %d\n", mdname(mddev), chunk_sects/2,
 +		       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
 +
 +		bio_io_error(bio);
 +		return true;
 +	}
  
  	md_write_start(mddev, bio);
  
diff --git a/drivers/md/raid1.c b/drivers/md/raid1.c
index 883e7284fcdb..c52e4c51e27c 100644
--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@ -1462,17 +1462,16 @@ static bool raid1_write_request(struct mddev *mddev, struct bio *bio,
 			plug = container_of(cb, struct raid1_plug_cb, cb);
 		else
 			plug = NULL;
-		spin_lock_irqsave(&conf->device_lock, flags);
 		if (plug) {
 			bio_list_add(&plug->pending, mbio);
 			plug->pending_cnt++;
 		} else {
+			spin_lock_irqsave(&conf->device_lock, flags);
 			bio_list_add(&conf->pending_bio_list, mbio);
 			conf->pending_count++;
-		}
-		spin_unlock_irqrestore(&conf->device_lock, flags);
-		if (!plug)
+			spin_unlock_irqrestore(&conf->device_lock, flags);
 			md_wakeup_thread(mddev->thread);
+		}
 	}
 	/* Mustn't call r1_bio_write_done before this next test,
 	 * as it could result in the bio being freed.
* Unmerged path drivers/md/raid10.c
