mm: pass the vmem_altmap to memmap_init_zone

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] pass the vmem_altmap to memmap_init_zone (Jeff Moyer) [1505291]
Rebuild_FUZZ: 95.24%
commit-author Christoph Hellwig <hch@lst.de>
commit a99583e780c751003ac9c0105eec9a3b23ec3bc4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/a99583e7.failed

Pass the vmem_altmap two levels down instead of needing a lookup.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit a99583e780c751003ac9c0105eec9a3b23ec3bc4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memory_hotplug.h
#	kernel/memremap.c
#	mm/hmm.c
#	mm/memory_hotplug.c
diff --cc include/linux/memory_hotplug.h
index d0cd6c1954bf,aba5f86eb038..000000000000
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@@ -264,9 -320,11 +264,17 @@@ static inline void remove_memory(int ni
  extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
  		void *arg, int (*func)(struct memory_block *, void *));
  extern int add_memory(int nid, u64 start, u64 size);
++<<<<<<< HEAD
 +extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
 +		bool for_device);
 +extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);
++=======
+ extern int add_memory_resource(int nid, struct resource *resource, bool online);
+ extern int arch_add_memory(int nid, u64 start, u64 size,
+ 		struct vmem_altmap *altmap, bool want_memblock);
+ extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
+ 		unsigned long nr_pages, struct vmem_altmap *altmap);
++>>>>>>> a99583e780c7 (mm: pass the vmem_altmap to memmap_init_zone)
  extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
  extern bool is_memblock_offlined(struct memory_block *mem);
  extern void remove_memory(int nid, u64 start, u64 size);
diff --cc kernel/memremap.c
index 00f3d3b53574,64b12c806cc5..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -449,7 -428,11 +449,15 @@@ void *devm_memremap_pages(struct devic
  		goto err_pfn_remap;
  
  	mem_hotplug_begin();
++<<<<<<< HEAD
 +	error = arch_add_memory(nid, align_start, align_size, true);
++=======
+ 	error = arch_add_memory(nid, align_start, align_size, altmap, false);
+ 	if (!error)
+ 		move_pfn_range_to_zone(&NODE_DATA(nid)->node_zones[ZONE_DEVICE],
+ 					align_start >> PAGE_SHIFT,
+ 					align_size >> PAGE_SHIFT, altmap);
++>>>>>>> a99583e780c7 (mm: pass the vmem_altmap to memmap_init_zone)
  	mem_hotplug_done();
  	if (error)
  		goto err_add_memory;
diff --cc mm/hmm.c
index 125cbd4521ca,2f2e13c61040..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -907,9 -919,31 +907,37 @@@ static int hmm_devmem_pages_create(stru
  	if (nid < 0)
  		nid = numa_mem_id();
  
++<<<<<<< HEAD
 +	ret = add_pages(nid, align_start, align_size, true);
 +	if (ret)
 +		goto error_radix;
++=======
+ 	mem_hotplug_begin();
+ 	/*
+ 	 * For device private memory we call add_pages() as we only need to
+ 	 * allocate and initialize struct page for the device memory. More-
+ 	 * over the device memory is un-accessible thus we do not want to
+ 	 * create a linear mapping for the memory like arch_add_memory()
+ 	 * would do.
+ 	 *
+ 	 * For device public memory, which is accesible by the CPU, we do
+ 	 * want the linear mapping and thus use arch_add_memory().
+ 	 */
+ 	if (devmem->pagemap.type == MEMORY_DEVICE_PUBLIC)
+ 		ret = arch_add_memory(nid, align_start, align_size, NULL,
+ 				false);
+ 	else
+ 		ret = add_pages(nid, align_start >> PAGE_SHIFT,
+ 				align_size >> PAGE_SHIFT, NULL, false);
+ 	if (ret) {
+ 		mem_hotplug_done();
+ 		goto error_add_memory;
+ 	}
+ 	move_pfn_range_to_zone(&NODE_DATA(nid)->node_zones[ZONE_DEVICE],
+ 				align_start >> PAGE_SHIFT,
+ 				align_size >> PAGE_SHIFT, NULL);
+ 	mem_hotplug_done();
++>>>>>>> a99583e780c7 (mm: pass the vmem_altmap to memmap_init_zone)
  
  	for (pfn = devmem->pfn_first; pfn < devmem->pfn_last; pfn++) {
  		struct page *page = pfn_to_page(pfn);
diff --cc mm/memory_hotplug.c
index 4ad6525f2bd5,12df8a5fadcc..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -1021,10 -776,135 +1021,134 @@@ static void node_states_set_node(int no
  	node_set_state(node, N_MEMORY);
  }
  
 -static void __meminit resize_zone_range(struct zone *zone, unsigned long start_pfn,
 -		unsigned long nr_pages)
 -{
 -	unsigned long old_end_pfn = zone_end_pfn(zone);
  
++<<<<<<< HEAD
 +/* Must be protected by mem_hotplug_begin() */
++=======
+ 	if (zone_is_empty(zone) || start_pfn < zone->zone_start_pfn)
+ 		zone->zone_start_pfn = start_pfn;
+ 
+ 	zone->spanned_pages = max(start_pfn + nr_pages, old_end_pfn) - zone->zone_start_pfn;
+ }
+ 
+ static void __meminit resize_pgdat_range(struct pglist_data *pgdat, unsigned long start_pfn,
+                                      unsigned long nr_pages)
+ {
+ 	unsigned long old_end_pfn = pgdat_end_pfn(pgdat);
+ 
+ 	if (!pgdat->node_spanned_pages || start_pfn < pgdat->node_start_pfn)
+ 		pgdat->node_start_pfn = start_pfn;
+ 
+ 	pgdat->node_spanned_pages = max(start_pfn + nr_pages, old_end_pfn) - pgdat->node_start_pfn;
+ }
+ 
+ void __ref move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
+ 		unsigned long nr_pages, struct vmem_altmap *altmap)
+ {
+ 	struct pglist_data *pgdat = zone->zone_pgdat;
+ 	int nid = pgdat->node_id;
+ 	unsigned long flags;
+ 
+ 	if (zone_is_empty(zone))
+ 		init_currently_empty_zone(zone, start_pfn, nr_pages);
+ 
+ 	clear_zone_contiguous(zone);
+ 
+ 	/* TODO Huh pgdat is irqsave while zone is not. It used to be like that before */
+ 	pgdat_resize_lock(pgdat, &flags);
+ 	zone_span_writelock(zone);
+ 	resize_zone_range(zone, start_pfn, nr_pages);
+ 	zone_span_writeunlock(zone);
+ 	resize_pgdat_range(pgdat, start_pfn, nr_pages);
+ 	pgdat_resize_unlock(pgdat, &flags);
+ 
+ 	/*
+ 	 * TODO now we have a visible range of pages which are not associated
+ 	 * with their zone properly. Not nice but set_pfnblock_flags_mask
+ 	 * expects the zone spans the pfn range. All the pages in the range
+ 	 * are reserved so nobody should be touching them so we should be safe
+ 	 */
+ 	memmap_init_zone(nr_pages, nid, zone_idx(zone), start_pfn,
+ 			MEMMAP_HOTPLUG, altmap);
+ 
+ 	set_zone_contiguous(zone);
+ }
+ 
+ /*
+  * Returns a default kernel memory zone for the given pfn range.
+  * If no kernel zone covers this pfn range it will automatically go
+  * to the ZONE_NORMAL.
+  */
+ static struct zone *default_kernel_zone_for_pfn(int nid, unsigned long start_pfn,
+ 		unsigned long nr_pages)
+ {
+ 	struct pglist_data *pgdat = NODE_DATA(nid);
+ 	int zid;
+ 
+ 	for (zid = 0; zid <= ZONE_NORMAL; zid++) {
+ 		struct zone *zone = &pgdat->node_zones[zid];
+ 
+ 		if (zone_intersects(zone, start_pfn, nr_pages))
+ 			return zone;
+ 	}
+ 
+ 	return &pgdat->node_zones[ZONE_NORMAL];
+ }
+ 
+ static inline struct zone *default_zone_for_pfn(int nid, unsigned long start_pfn,
+ 		unsigned long nr_pages)
+ {
+ 	struct zone *kernel_zone = default_kernel_zone_for_pfn(nid, start_pfn,
+ 			nr_pages);
+ 	struct zone *movable_zone = &NODE_DATA(nid)->node_zones[ZONE_MOVABLE];
+ 	bool in_kernel = zone_intersects(kernel_zone, start_pfn, nr_pages);
+ 	bool in_movable = zone_intersects(movable_zone, start_pfn, nr_pages);
+ 
+ 	/*
+ 	 * We inherit the existing zone in a simple case where zones do not
+ 	 * overlap in the given range
+ 	 */
+ 	if (in_kernel ^ in_movable)
+ 		return (in_kernel) ? kernel_zone : movable_zone;
+ 
+ 	/*
+ 	 * If the range doesn't belong to any zone or two zones overlap in the
+ 	 * given range then we use movable zone only if movable_node is
+ 	 * enabled because we always online to a kernel zone by default.
+ 	 */
+ 	return movable_node_enabled ? movable_zone : kernel_zone;
+ }
+ 
+ struct zone * zone_for_pfn_range(int online_type, int nid, unsigned start_pfn,
+ 		unsigned long nr_pages)
+ {
+ 	if (online_type == MMOP_ONLINE_KERNEL)
+ 		return default_kernel_zone_for_pfn(nid, start_pfn, nr_pages);
+ 
+ 	if (online_type == MMOP_ONLINE_MOVABLE)
+ 		return &NODE_DATA(nid)->node_zones[ZONE_MOVABLE];
+ 
+ 	return default_zone_for_pfn(nid, start_pfn, nr_pages);
+ }
+ 
+ /*
+  * Associates the given pfn range with the given node and the zone appropriate
+  * for the given online type.
+  */
+ static struct zone * __meminit move_pfn_range(int online_type, int nid,
+ 		unsigned long start_pfn, unsigned long nr_pages)
+ {
+ 	struct zone *zone;
+ 
+ 	zone = zone_for_pfn_range(online_type, nid, start_pfn, nr_pages);
+ 	move_pfn_range_to_zone(zone, start_pfn, nr_pages, NULL);
+ 	return zone;
+ }
+ 
+ /* Must be protected by mem_hotplug_begin() or a device_lock */
++>>>>>>> a99583e780c7 (mm: pass the vmem_altmap to memmap_init_zone)
  int __ref online_pages(unsigned long pfn, unsigned long nr_pages, int online_type)
  {
 -	unsigned long flags;
  	unsigned long onlined_pages = 0;
  	struct zone *zone;
  	int need_zonelists_rebuild = 0;
diff --git a/arch/ia64/mm/init.c b/arch/ia64/mm/init.c
index d1fe4b402601..92d119c89dd7 100644
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@ -478,7 +478,7 @@ virtual_memmap_init(u64 start, u64 end, void *arg)
 	if (map_start < map_end)
 		memmap_init_zone((unsigned long)(map_end - map_start),
 				 args->nid, args->zone, page_to_pfn(map_start),
-				 MEMMAP_EARLY);
+				 MEMMAP_EARLY, NULL);
 	return 0;
 }
 
@@ -486,9 +486,10 @@ void __meminit
 memmap_init (unsigned long size, int nid, unsigned long zone,
 	     unsigned long start_pfn)
 {
-	if (!vmem_map)
-		memmap_init_zone(size, nid, zone, start_pfn, MEMMAP_EARLY);
-	else {
+	if (!vmem_map) {
+		memmap_init_zone(size, nid, zone, start_pfn, MEMMAP_EARLY,
+				NULL);
+	} else {
 		struct page *start;
 		struct memmap_init_callback_data args;
 
* Unmerged path include/linux/memory_hotplug.h
diff --git a/include/linux/mm.h b/include/linux/mm.h
index a228f1a787bf..26298552242f 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1924,8 +1924,8 @@ extern int __meminit __early_pfn_to_nid(unsigned long pfn,
 #endif
 
 extern void set_dma_reserve(unsigned long new_dma_reserve);
-extern void memmap_init_zone(unsigned long, int, unsigned long,
-				unsigned long, enum memmap_context);
+extern void memmap_init_zone(unsigned long, int, unsigned long, unsigned long,
+		enum memmap_context, struct vmem_altmap *);
 extern void setup_per_zone_wmarks(void);
 extern int __meminit init_per_zone_wmark_min(void);
 extern void mem_init(void);
* Unmerged path kernel/memremap.c
* Unmerged path mm/hmm.c
* Unmerged path mm/memory_hotplug.c
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 0f8b31880f9c..a476c6a91c5d 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -4697,9 +4697,9 @@ static void setup_zone_migrate_reserve(struct zone *zone)
  * done. Non-atomic initialization, single-pass.
  */
 void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
-		unsigned long start_pfn, enum memmap_context context)
+		unsigned long start_pfn, enum memmap_context context,
+		struct vmem_altmap *altmap)
 {
-	struct vmem_altmap *altmap = to_vmem_altmap(__pfn_to_phys(start_pfn));
 	unsigned long end_pfn = start_pfn + size;
 	pg_data_t *pgdat = NODE_DATA(nid);
 	unsigned long pfn;
@@ -4824,7 +4824,7 @@ static void __meminit zone_init_free_lists(struct zone *zone)
 
 #ifndef __HAVE_ARCH_MEMMAP_INIT
 #define memmap_init(size, nid, zone, start_pfn) \
-	memmap_init_zone((size), (nid), (zone), (start_pfn), MEMMAP_EARLY)
+	memmap_init_zone((size), (nid), (zone), (start_pfn), MEMMAP_EARLY, NULL)
 #endif
 
 static int zone_batchsize(struct zone *zone)
