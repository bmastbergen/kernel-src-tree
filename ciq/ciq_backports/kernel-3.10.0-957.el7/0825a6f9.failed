mm: use octal not symbolic permissions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [netdrv] sfc: Use octal not symbolic permissions (Jarod Wilson) [1547443]
Rebuild_FUZZ: 93.51%
commit-author Joe Perches <joe@perches.com>
commit 0825a6f98689d847ab8058c51b3a55f0abcc6563
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/0825a6f9.failed

mm/*.c files use symbolic and octal styles for permissions.

Using octal and not symbolic permissions is preferred by many as more
readable.

https://lkml.org/lkml/2016/8/2/1945

Prefer the direct use of octal for permissions.

Done using
$ scripts/checkpatch.pl -f --types=SYMBOLIC_PERMS --fix-inplace mm/*.c
and some typing.

Before:	 $ git grep -P -w "0[0-7]{3,3}" mm | wc -l
44
After:	 $ git grep -P -w "0[0-7]{3,3}" mm | wc -l
86

Miscellanea:

o Whitespace neatening around these conversions.

Link: http://lkml.kernel.org/r/2e032ef111eebcd4c5952bae86763b541d373469.1522102887.git.joe@perches.com
	Signed-off-by: Joe Perches <joe@perches.com>
	Acked-by: David Rientjes <rientjes@google.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0825a6f98689d847ab8058c51b3a55f0abcc6563)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/cma_debug.c
#	mm/memblock.c
#	mm/page_idle.c
#	mm/page_owner.c
#	mm/slab_common.c
#	mm/vmalloc.c
#	mm/zsmalloc.c
#	mm/zswap.c
diff --cc mm/memblock.c
index fbc8071d9f43,cc16d70b8333..000000000000
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@@ -1664,8 -1809,14 +1664,19 @@@ static int __init memblock_init_debugfs
  	struct dentry *root = debugfs_create_dir("memblock", NULL);
  	if (!root)
  		return -ENXIO;
++<<<<<<< HEAD
 +	debugfs_create_file("memory", S_IRUGO, root, &memblock.memory, &memblock_debug_fops);
 +	debugfs_create_file("reserved", S_IRUGO, root, &memblock.reserved, &memblock_debug_fops);
++=======
+ 	debugfs_create_file("memory", 0444, root,
+ 			    &memblock.memory, &memblock_debug_fops);
+ 	debugfs_create_file("reserved", 0444, root,
+ 			    &memblock.reserved, &memblock_debug_fops);
+ #ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP
+ 	debugfs_create_file("physmem", 0444, root,
+ 			    &memblock.physmem, &memblock_debug_fops);
+ #endif
++>>>>>>> 0825a6f98689 (mm: use octal not symbolic permissions)
  
  	return 0;
  }
diff --cc mm/slab_common.c
index d9eb94deb116,890b1f04a03a..000000000000
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@@ -580,9 -1165,90 +580,86 @@@ void __init create_kmalloc_caches(unsig
  }
  #endif /* !CONFIG_SLOB */
  
 -/*
 - * To avoid unnecessary overhead, we pass through large allocation requests
 - * directly to the page allocator. We use __GFP_COMP, because we will need to
 - * know the allocation order to free the pages properly in kfree.
 - */
 -void *kmalloc_order(size_t size, gfp_t flags, unsigned int order)
 -{
 -	void *ret;
 -	struct page *page;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_SLABINFO
 +void print_slabinfo_header(struct seq_file *m)
++=======
+ 	flags |= __GFP_COMP;
+ 	page = alloc_pages(flags, order);
+ 	ret = page ? page_address(page) : NULL;
+ 	kmemleak_alloc(ret, size, 1, flags);
+ 	kasan_kmalloc_large(ret, size, flags);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(kmalloc_order);
+ 
+ #ifdef CONFIG_TRACING
+ void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
+ {
+ 	void *ret = kmalloc_order(size, flags, order);
+ 	trace_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << order, flags);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(kmalloc_order_trace);
+ #endif
+ 
+ #ifdef CONFIG_SLAB_FREELIST_RANDOM
+ /* Randomize a generic freelist */
+ static void freelist_randomize(struct rnd_state *state, unsigned int *list,
+ 			       unsigned int count)
+ {
+ 	unsigned int rand;
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < count; i++)
+ 		list[i] = i;
+ 
+ 	/* Fisher-Yates shuffle */
+ 	for (i = count - 1; i > 0; i--) {
+ 		rand = prandom_u32_state(state);
+ 		rand %= (i + 1);
+ 		swap(list[i], list[rand]);
+ 	}
+ }
+ 
+ /* Create a random sequence per cache */
+ int cache_random_seq_create(struct kmem_cache *cachep, unsigned int count,
+ 				    gfp_t gfp)
+ {
+ 	struct rnd_state state;
+ 
+ 	if (count < 2 || cachep->random_seq)
+ 		return 0;
+ 
+ 	cachep->random_seq = kcalloc(count, sizeof(unsigned int), gfp);
+ 	if (!cachep->random_seq)
+ 		return -ENOMEM;
+ 
+ 	/* Get best entropy at this stage of boot */
+ 	prandom_seed_state(&state, get_random_long());
+ 
+ 	freelist_randomize(&state, cachep->random_seq, count);
+ 	return 0;
+ }
+ 
+ /* Destroy the per-cache random freelist sequence */
+ void cache_random_seq_destroy(struct kmem_cache *cachep)
+ {
+ 	kfree(cachep->random_seq);
+ 	cachep->random_seq = NULL;
+ }
+ #endif /* CONFIG_SLAB_FREELIST_RANDOM */
+ 
+ #if defined(CONFIG_SLAB) || defined(CONFIG_SLUB_DEBUG)
+ #ifdef CONFIG_SLAB
+ #define SLABINFO_RIGHTS (0600)
+ #else
+ #define SLABINFO_RIGHTS (0400)
+ #endif
+ 
+ static void print_slabinfo_header(struct seq_file *m)
++>>>>>>> 0825a6f98689 (mm: use octal not symbolic permissions)
  {
  	/*
  	 * Output format version, so at least we can change it
diff --cc mm/vmalloc.c
index 90de46594b5f,cfea25be7754..000000000000
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@@ -2688,35 -2738,14 +2688,44 @@@ static const struct seq_operations vmal
  	.show = s_show,
  };
  
 +static int vmalloc_open(struct inode *inode, struct file *file)
 +{
 +	unsigned int *ptr = NULL;
 +	int ret;
 +
 +	if (IS_ENABLED(CONFIG_NUMA)) {
 +		ptr = kmalloc(nr_node_ids * sizeof(unsigned int), GFP_KERNEL);
 +		if (ptr == NULL)
 +			return -ENOMEM;
 +	}
 +	ret = seq_open(file, &vmalloc_op);
 +	if (!ret) {
 +		struct seq_file *m = file->private_data;
 +		m->private = ptr;
 +	} else
 +		kfree(ptr);
 +	return ret;
 +}
 +
 +static const struct file_operations proc_vmalloc_operations = {
 +	.open		= vmalloc_open,
 +	.read		= seq_read,
 +	.llseek		= seq_lseek,
 +	.release	= seq_release_private,
 +};
 +
  static int __init proc_vmalloc_init(void)
  {
++<<<<<<< HEAD
 +	proc_create("vmallocinfo", S_IRUSR, NULL, &proc_vmalloc_operations);
++=======
+ 	if (IS_ENABLED(CONFIG_NUMA))
+ 		proc_create_seq_private("vmallocinfo", 0400, NULL,
+ 				&vmalloc_op,
+ 				nr_node_ids * sizeof(unsigned int), NULL);
+ 	else
+ 		proc_create_seq("vmallocinfo", 0400, NULL, &vmalloc_op);
++>>>>>>> 0825a6f98689 (mm: use octal not symbolic permissions)
  	return 0;
  }
  module_init(proc_vmalloc_init);
diff --cc mm/zsmalloc.c
index 252478ef0860,8d87e973a4f5..000000000000
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@@ -367,9 -541,161 +367,164 @@@ static int get_size_class_index(int siz
  		idx = DIV_ROUND_UP(size - ZS_MIN_ALLOC_SIZE,
  				ZS_SIZE_CLASS_DELTA);
  
 -	return min_t(int, ZS_SIZE_CLASSES - 1, idx);
 +	return idx;
  }
  
++<<<<<<< HEAD
++=======
+ /* type can be of enum type zs_stat_type or fullness_group */
+ static inline void zs_stat_inc(struct size_class *class,
+ 				int type, unsigned long cnt)
+ {
+ 	class->stats.objs[type] += cnt;
+ }
+ 
+ /* type can be of enum type zs_stat_type or fullness_group */
+ static inline void zs_stat_dec(struct size_class *class,
+ 				int type, unsigned long cnt)
+ {
+ 	class->stats.objs[type] -= cnt;
+ }
+ 
+ /* type can be of enum type zs_stat_type or fullness_group */
+ static inline unsigned long zs_stat_get(struct size_class *class,
+ 				int type)
+ {
+ 	return class->stats.objs[type];
+ }
+ 
+ #ifdef CONFIG_ZSMALLOC_STAT
+ 
+ static void __init zs_stat_init(void)
+ {
+ 	if (!debugfs_initialized()) {
+ 		pr_warn("debugfs not available, stat dir not created\n");
+ 		return;
+ 	}
+ 
+ 	zs_stat_root = debugfs_create_dir("zsmalloc", NULL);
+ 	if (!zs_stat_root)
+ 		pr_warn("debugfs 'zsmalloc' stat dir creation failed\n");
+ }
+ 
+ static void __exit zs_stat_exit(void)
+ {
+ 	debugfs_remove_recursive(zs_stat_root);
+ }
+ 
+ static unsigned long zs_can_compact(struct size_class *class);
+ 
+ static int zs_stats_size_show(struct seq_file *s, void *v)
+ {
+ 	int i;
+ 	struct zs_pool *pool = s->private;
+ 	struct size_class *class;
+ 	int objs_per_zspage;
+ 	unsigned long class_almost_full, class_almost_empty;
+ 	unsigned long obj_allocated, obj_used, pages_used, freeable;
+ 	unsigned long total_class_almost_full = 0, total_class_almost_empty = 0;
+ 	unsigned long total_objs = 0, total_used_objs = 0, total_pages = 0;
+ 	unsigned long total_freeable = 0;
+ 
+ 	seq_printf(s, " %5s %5s %11s %12s %13s %10s %10s %16s %8s\n",
+ 			"class", "size", "almost_full", "almost_empty",
+ 			"obj_allocated", "obj_used", "pages_used",
+ 			"pages_per_zspage", "freeable");
+ 
+ 	for (i = 0; i < ZS_SIZE_CLASSES; i++) {
+ 		class = pool->size_class[i];
+ 
+ 		if (class->index != i)
+ 			continue;
+ 
+ 		spin_lock(&class->lock);
+ 		class_almost_full = zs_stat_get(class, CLASS_ALMOST_FULL);
+ 		class_almost_empty = zs_stat_get(class, CLASS_ALMOST_EMPTY);
+ 		obj_allocated = zs_stat_get(class, OBJ_ALLOCATED);
+ 		obj_used = zs_stat_get(class, OBJ_USED);
+ 		freeable = zs_can_compact(class);
+ 		spin_unlock(&class->lock);
+ 
+ 		objs_per_zspage = class->objs_per_zspage;
+ 		pages_used = obj_allocated / objs_per_zspage *
+ 				class->pages_per_zspage;
+ 
+ 		seq_printf(s, " %5u %5u %11lu %12lu %13lu"
+ 				" %10lu %10lu %16d %8lu\n",
+ 			i, class->size, class_almost_full, class_almost_empty,
+ 			obj_allocated, obj_used, pages_used,
+ 			class->pages_per_zspage, freeable);
+ 
+ 		total_class_almost_full += class_almost_full;
+ 		total_class_almost_empty += class_almost_empty;
+ 		total_objs += obj_allocated;
+ 		total_used_objs += obj_used;
+ 		total_pages += pages_used;
+ 		total_freeable += freeable;
+ 	}
+ 
+ 	seq_puts(s, "\n");
+ 	seq_printf(s, " %5s %5s %11lu %12lu %13lu %10lu %10lu %16s %8lu\n",
+ 			"Total", "", total_class_almost_full,
+ 			total_class_almost_empty, total_objs,
+ 			total_used_objs, total_pages, "", total_freeable);
+ 
+ 	return 0;
+ }
+ DEFINE_SHOW_ATTRIBUTE(zs_stats_size);
+ 
+ static void zs_pool_stat_create(struct zs_pool *pool, const char *name)
+ {
+ 	struct dentry *entry;
+ 
+ 	if (!zs_stat_root) {
+ 		pr_warn("no root stat dir, not creating <%s> stat dir\n", name);
+ 		return;
+ 	}
+ 
+ 	entry = debugfs_create_dir(name, zs_stat_root);
+ 	if (!entry) {
+ 		pr_warn("debugfs dir <%s> creation failed\n", name);
+ 		return;
+ 	}
+ 	pool->stat_dentry = entry;
+ 
+ 	entry = debugfs_create_file("classes", S_IFREG | 0444,
+ 				    pool->stat_dentry, pool,
+ 				    &zs_stats_size_fops);
+ 	if (!entry) {
+ 		pr_warn("%s: debugfs file entry <%s> creation failed\n",
+ 				name, "classes");
+ 		debugfs_remove_recursive(pool->stat_dentry);
+ 		pool->stat_dentry = NULL;
+ 	}
+ }
+ 
+ static void zs_pool_stat_destroy(struct zs_pool *pool)
+ {
+ 	debugfs_remove_recursive(pool->stat_dentry);
+ }
+ 
+ #else /* CONFIG_ZSMALLOC_STAT */
+ static void __init zs_stat_init(void)
+ {
+ }
+ 
+ static void __exit zs_stat_exit(void)
+ {
+ }
+ 
+ static inline void zs_pool_stat_create(struct zs_pool *pool, const char *name)
+ {
+ }
+ 
+ static inline void zs_pool_stat_destroy(struct zs_pool *pool)
+ {
+ }
+ #endif
+ 
+ 
++>>>>>>> 0825a6f98689 (mm: use octal not symbolic permissions)
  /*
   * For each size class, zspages are divided into different groups
   * depending on how "full" they are. This was done so that we could
diff --cc mm/zswap.c
index 79cdd12ac453,7d34e69507e3..000000000000
--- a/mm/zswap.c
+++ b/mm/zswap.c
@@@ -799,24 -1256,26 +799,47 @@@ static int __init zswap_debugfs_init(vo
  	if (!zswap_debugfs_root)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	debugfs_create_u64("pool_limit_hit", S_IRUGO,
 +			zswap_debugfs_root, &zswap_pool_limit_hit);
 +	debugfs_create_u64("reject_reclaim_fail", S_IRUGO,
 +			zswap_debugfs_root, &zswap_reject_reclaim_fail);
 +	debugfs_create_u64("reject_alloc_fail", S_IRUGO,
 +			zswap_debugfs_root, &zswap_reject_alloc_fail);
 +	debugfs_create_u64("reject_kmemcache_fail", S_IRUGO,
 +			zswap_debugfs_root, &zswap_reject_kmemcache_fail);
 +	debugfs_create_u64("reject_compress_poor", S_IRUGO,
 +			zswap_debugfs_root, &zswap_reject_compress_poor);
 +	debugfs_create_u64("written_back_pages", S_IRUGO,
 +			zswap_debugfs_root, &zswap_written_back_pages);
 +	debugfs_create_u64("duplicate_entry", S_IRUGO,
 +			zswap_debugfs_root, &zswap_duplicate_entry);
 +	debugfs_create_u64("pool_total_size", S_IRUGO,
 +			zswap_debugfs_root, &zswap_pool_total_size);
 +	debugfs_create_atomic_t("stored_pages", S_IRUGO,
 +			zswap_debugfs_root, &zswap_stored_pages);
++=======
+ 	debugfs_create_u64("pool_limit_hit", 0444,
+ 			   zswap_debugfs_root, &zswap_pool_limit_hit);
+ 	debugfs_create_u64("reject_reclaim_fail", 0444,
+ 			   zswap_debugfs_root, &zswap_reject_reclaim_fail);
+ 	debugfs_create_u64("reject_alloc_fail", 0444,
+ 			   zswap_debugfs_root, &zswap_reject_alloc_fail);
+ 	debugfs_create_u64("reject_kmemcache_fail", 0444,
+ 			   zswap_debugfs_root, &zswap_reject_kmemcache_fail);
+ 	debugfs_create_u64("reject_compress_poor", 0444,
+ 			   zswap_debugfs_root, &zswap_reject_compress_poor);
+ 	debugfs_create_u64("written_back_pages", 0444,
+ 			   zswap_debugfs_root, &zswap_written_back_pages);
+ 	debugfs_create_u64("duplicate_entry", 0444,
+ 			   zswap_debugfs_root, &zswap_duplicate_entry);
+ 	debugfs_create_u64("pool_total_size", 0444,
+ 			   zswap_debugfs_root, &zswap_pool_total_size);
+ 	debugfs_create_atomic_t("stored_pages", 0444,
+ 				zswap_debugfs_root, &zswap_stored_pages);
+ 	debugfs_create_atomic_t("same_filled_pages", 0444,
+ 				zswap_debugfs_root, &zswap_same_filled_pages);
++>>>>>>> 0825a6f98689 (mm: use octal not symbolic permissions)
  
  	return 0;
  }
* Unmerged path mm/cma_debug.c
* Unmerged path mm/page_idle.c
* Unmerged path mm/page_owner.c
diff --git a/mm/cleancache.c b/mm/cleancache.c
index d0eac4350403..24a04e573a8b 100644
--- a/mm/cleancache.c
+++ b/mm/cleancache.c
@@ -393,12 +393,10 @@ static int __init init_cleancache(void)
 	struct dentry *root = debugfs_create_dir("cleancache", NULL);
 	if (root == NULL)
 		return -ENXIO;
-	debugfs_create_u64("succ_gets", S_IRUGO, root, &cleancache_succ_gets);
-	debugfs_create_u64("failed_gets", S_IRUGO,
-				root, &cleancache_failed_gets);
-	debugfs_create_u64("puts", S_IRUGO, root, &cleancache_puts);
-	debugfs_create_u64("invalidates", S_IRUGO,
-				root, &cleancache_invalidates);
+	debugfs_create_u64("succ_gets", 0444, root, &cleancache_succ_gets);
+	debugfs_create_u64("failed_gets", 0444, root, &cleancache_failed_gets);
+	debugfs_create_u64("puts", 0444, root, &cleancache_puts);
+	debugfs_create_u64("invalidates", 0444, root, &cleancache_invalidates);
 #endif
 	for (i = 0; i < MAX_INITIALIZABLE_FS; i++) {
 		fs_poolid_map[i] = FS_UNKNOWN;
* Unmerged path mm/cma_debug.c
diff --git a/mm/compaction.c b/mm/compaction.c
index a412633c4cf4..7feaa589756d 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -1216,7 +1216,7 @@ ssize_t sysfs_compact_node(struct device *dev,
 
 	return count;
 }
-static DEVICE_ATTR(compact, S_IWUSR, NULL, sysfs_compact_node);
+static DEVICE_ATTR(compact, 0200, NULL, sysfs_compact_node);
 
 int compaction_register_node(struct node *node)
 {
diff --git a/mm/dmapool.c b/mm/dmapool.c
index 0b33dd0bf614..c0f2a30c485a 100644
--- a/mm/dmapool.c
+++ b/mm/dmapool.c
@@ -104,7 +104,7 @@ show_pools(struct device *dev, struct device_attribute *attr, char *buf)
 	return PAGE_SIZE - size;
 }
 
-static DEVICE_ATTR(pools, S_IRUGO, show_pools, NULL);
+static DEVICE_ATTR(pools, 0444, show_pools, NULL);
 
 /**
  * dma_pool_create - Creates a pool of consistent memory blocks, for dma.
diff --git a/mm/failslab.c b/mm/failslab.c
index fefaabaab76d..f378324153c1 100644
--- a/mm/failslab.c
+++ b/mm/failslab.c
@@ -35,7 +35,7 @@ __setup("failslab=", setup_failslab);
 static int __init failslab_debugfs_init(void)
 {
 	struct dentry *dir;
-	umode_t mode = S_IFREG | S_IRUSR | S_IWUSR;
+	umode_t mode = S_IFREG | 0600;
 
 	dir = fault_create_debugfs_attr("failslab", NULL, &failslab.attr);
 	if (IS_ERR(dir))
diff --git a/mm/frontswap.c b/mm/frontswap.c
index c30eec536f03..d0b5cd5def14 100644
--- a/mm/frontswap.c
+++ b/mm/frontswap.c
@@ -442,12 +442,11 @@ static int __init init_frontswap(void)
 	struct dentry *root = debugfs_create_dir("frontswap", NULL);
 	if (root == NULL)
 		return -ENXIO;
-	debugfs_create_u64("loads", S_IRUGO, root, &frontswap_loads);
-	debugfs_create_u64("succ_stores", S_IRUGO, root, &frontswap_succ_stores);
-	debugfs_create_u64("failed_stores", S_IRUGO, root,
-				&frontswap_failed_stores);
-	debugfs_create_u64("invalidates", S_IRUGO,
-				root, &frontswap_invalidates);
+	debugfs_create_u64("loads", 0444, root, &frontswap_loads);
+	debugfs_create_u64("succ_stores", 0444, root, &frontswap_succ_stores);
+	debugfs_create_u64("failed_stores", 0444, root,
+			   &frontswap_failed_stores);
+	debugfs_create_u64("invalidates", 0444, root, &frontswap_invalidates);
 #endif
 	return 0;
 }
* Unmerged path mm/memblock.c
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 0f8b31880f9c..86418adae425 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -2109,7 +2109,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)
 
 static int __init fail_page_alloc_debugfs(void)
 {
-	umode_t mode = S_IFREG | S_IRUSR | S_IWUSR;
+	umode_t mode = S_IFREG | 0600;
 	struct dentry *dir;
 
 	dir = fault_create_debugfs_attr("fail_page_alloc", NULL,
* Unmerged path mm/page_idle.c
* Unmerged path mm/page_owner.c
diff --git a/mm/shmem.c b/mm/shmem.c
index 1f3771bdc78b..8c8c539db62b 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -2706,7 +2706,8 @@ static int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *s
 	if (len > PAGE_CACHE_SIZE)
 		return -ENAMETOOLONG;
 
-	inode = shmem_get_inode(dir->i_sb, dir, S_IFLNK|S_IRWXUGO, 0, VM_NORESERVE);
+	inode = shmem_get_inode(dir->i_sb, dir, S_IFLNK | 0777, 0,
+				VM_NORESERVE);
 	if (!inode)
 		return -ENOSPC;
 
@@ -3170,7 +3171,7 @@ static int shmem_show_options(struct seq_file *seq, struct dentry *root)
 			sbinfo->max_blocks << (PAGE_CACHE_SHIFT - 10));
 	if (sbinfo->max_inodes != shmem_default_max_inodes())
 		seq_printf(seq, ",nr_inodes=%lu", sbinfo->max_inodes);
-	if (sbinfo->mode != (S_IRWXUGO | S_ISVTX))
+	if (sbinfo->mode != (0777 | S_ISVTX))
 		seq_printf(seq, ",mode=%03ho", sbinfo->mode);
 	if (!uid_eq(sbinfo->uid, GLOBAL_ROOT_UID))
 		seq_printf(seq, ",uid=%u",
@@ -3276,7 +3277,7 @@ int shmem_fill_super(struct super_block *sb, void *data, int silent)
 	if (!sbinfo)
 		return -ENOMEM;
 
-	sbinfo->mode = S_IRWXUGO | S_ISVTX;
+	sbinfo->mode = 0777 | S_ISVTX;
 	sbinfo->uid = current_fsuid();
 	sbinfo->gid = current_fsgid();
 	sb->s_fs_info = sbinfo;
@@ -3626,7 +3627,7 @@ static struct file *__shmem_file_setup(const char *name, loff_t size,
 	path.mnt = mntget(shm_mnt);
 
 	res = ERR_PTR(-ENOSPC);
-	inode = shmem_get_inode(sb, NULL, S_IFREG | S_IRWXUGO, 0, flags);
+	inode = shmem_get_inode(sb, NULL, S_IFREG | 0777, 0, flags);
 	if (!inode)
 		goto put_dentry;
 
* Unmerged path mm/slab_common.c
* Unmerged path mm/vmalloc.c
* Unmerged path mm/zsmalloc.c
* Unmerged path mm/zswap.c
