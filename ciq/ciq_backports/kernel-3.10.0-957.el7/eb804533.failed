mm: merge vmem_altmap_alloc into altmap_alloc_block_buf

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [mm] merge vmem_altmap_alloc into altmap_alloc_block_buf (Jeff Moyer) [1505291]
Rebuild_FUZZ: 96.23%
commit-author Christoph Hellwig <hch@lst.de>
commit eb8045335c70ef8b272d2888a225b81344423139
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/eb804533.failed

There is no clear separation between the two, so merge them.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit eb8045335c70ef8b272d2888a225b81344423139)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/sparse-vmemmap.c
diff --cc mm/sparse-vmemmap.c
index b60802b3e5ea,bd0276d5f66b..000000000000
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@@ -104,33 -107,16 +104,36 @@@ static unsigned long __meminit vmem_alt
  }
  
  /**
-  * vmem_altmap_alloc - allocate pages from the vmem_altmap reservation
-  * @altmap - reserved page pool for the allocation
-  * @nr_pfns - size (in pages) of the allocation
+  * altmap_alloc_block_buf - allocate pages from the device page map
+  * @altmap:	device page map
+  * @size:	size (in bytes) of the allocation
   *
-  * Allocations are aligned to the size of the request
+  * Allocations are aligned to the size of the request.
   */
++<<<<<<< HEAD
 +static unsigned long __meminit vmem_altmap_alloc(struct vmem_altmap *altmap,
 +		unsigned long nr_pfns)
 +{
 +	unsigned long pfn = vmem_altmap_next_pfn(altmap);
 +	unsigned long nr_align;
 +
 +	nr_align = 1UL << find_first_bit(&nr_pfns, BITS_PER_LONG);
 +	nr_align = ALIGN(pfn, nr_align) - pfn;
 +
 +	if (nr_pfns + nr_align > vmem_altmap_nr_free(altmap))
 +		return ULONG_MAX;
 +	altmap->alloc += nr_pfns;
 +	altmap->align += nr_align;
 +	return pfn + nr_align;
 +}
 +
 +static void * __meminit altmap_alloc_block_buf(unsigned long size,
++=======
+ void * __meminit altmap_alloc_block_buf(unsigned long size,
++>>>>>>> eb8045335c70 (mm: merge vmem_altmap_alloc into altmap_alloc_block_buf)
  		struct vmem_altmap *altmap)
  {
- 	unsigned long pfn, nr_pfns;
- 	void *ptr;
+ 	unsigned long pfn, nr_pfns, nr_align;
  
  	if (size & ~PAGE_MASK) {
  		pr_warn_once("%s: allocations must be multiple of PAGE_SIZE (%ld)\n",
@@@ -138,27 -124,22 +141,31 @@@
  		return NULL;
  	}
  
+ 	pfn = vmem_altmap_next_pfn(altmap);
  	nr_pfns = size >> PAGE_SHIFT;
- 	pfn = vmem_altmap_alloc(altmap, nr_pfns);
- 	if (pfn < ULONG_MAX)
- 		ptr = __va(__pfn_to_phys(pfn));
- 	else
- 		ptr = NULL;
+ 	nr_align = 1UL << find_first_bit(&nr_pfns, BITS_PER_LONG);
+ 	nr_align = ALIGN(pfn, nr_align) - pfn;
+ 	if (nr_pfns + nr_align > vmem_altmap_nr_free(altmap))
+ 		return NULL;
+ 
+ 	altmap->alloc += nr_pfns;
+ 	altmap->align += nr_align;
+ 	pfn += nr_align;
+ 
  	pr_debug("%s: pfn: %#lx alloc: %ld align: %ld nr: %#lx\n",
  			__func__, pfn, altmap->alloc, altmap->align, nr_pfns);
- 
- 	return ptr;
+ 	return __va(__pfn_to_phys(pfn));
  }
  
 +/* need to make sure size is all the same during early stage */
 +void * __meminit __vmemmap_alloc_block_buf(unsigned long size, int node,
 +		struct vmem_altmap *altmap)
 +{
 +	if (altmap)
 +		return altmap_alloc_block_buf(size, altmap);
 +	return alloc_block_buf(size, node);
 +}
 +
  void __meminit vmemmap_verify(pte_t *pte, int node,
  				unsigned long start, unsigned long end)
  {
* Unmerged path mm/sparse-vmemmap.c
