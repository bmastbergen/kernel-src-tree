bpf: provide helper that indicates eBPF was migrated

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 7b36f92934e40d1ee24e5617ddedb852e10086ca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/7b36f929.failed

During recent discussions we had with Michael, we found that it would
be useful to have an indicator that tells the JIT that an eBPF program
had been migrated from classic instructions into eBPF instructions, as
only in that case A and X need to be cleared in the prologue. Such eBPF
programs do not set a particular type, but all have BPF_PROG_TYPE_UNSPEC.
Thus, introduce a small helper for cde66c2d88da ("s390/bpf: Only clear
A and X for converted BPF programs") and possibly others in future.

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Cc: Michael Holzheu <holzheu@linux.vnet.ibm.com>
	Acked-by: Alexei Starovoitov <ast@plumgrid.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 7b36f92934e40d1ee24e5617ddedb852e10086ca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/net/bpf_jit_comp.c
#	include/linux/filter.h
diff --cc arch/s390/net/bpf_jit_comp.c
index 600b1e5c8c9d,9f4bbc09bf07..000000000000
--- a/arch/s390/net/bpf_jit_comp.c
+++ b/arch/s390/net/bpf_jit_comp.c
@@@ -641,112 -1069,213 +641,140 @@@ call_fn:	/* lg %r1,<d(function)>(%r13) 
  		/* j <exit> */
  		EMIT4_PCREL(0xa7f40000, jit->exit_ip - jit->prg);
  		break;
 -	/*
 -	 * Branch relative (number of skipped instructions) to offset on
 -	 * condition.
 -	 *
 -	 * Condition code to mask mapping:
 -	 *
 -	 * CC | Description	   | Mask
 -	 * ------------------------------
 -	 * 0  | Operands equal	   |	8
 -	 * 1  | First operand low  |	4
 -	 * 2  | First operand high |	2
 -	 * 3  | Unused		   |	1
 -	 *
 -	 * For s390x relative branches: ip = ip + off_bytes
 -	 * For BPF relative branches:	insn = insn + off_insns + 1
 -	 *
 -	 * For example for s390x with offset 0 we jump to the branch
 -	 * instruction itself (loop) and for BPF with offset 0 we
 -	 * branch to the instruction behind the branch.
 -	 */
 -	case BPF_JMP | BPF_JA: /* if (true) */
 -		mask = 0xf000; /* j */
 -		goto branch_oc;
 -	case BPF_JMP | BPF_JSGT | BPF_K: /* ((s64) dst > (s64) imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JSGE | BPF_K: /* ((s64) dst >= (s64) imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JGT | BPF_K: /* (dst_reg > imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JGE | BPF_K: /* (dst_reg >= imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JNE | BPF_K: /* (dst_reg != imm) */
 -		mask = 0x7000; /* jne */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JEQ | BPF_K: /* (dst_reg == imm) */
 -		mask = 0x8000; /* je */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JSET | BPF_K: /* (dst_reg & imm) */
 -		mask = 0x7000; /* jnz */
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* ngr %w1,%dst */
 -		EMIT4(0xb9800000, REG_W1, dst_reg);
 -		goto branch_oc;
 -
 -	case BPF_JMP | BPF_JSGT | BPF_X: /* ((s64) dst > (s64) src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JSGE | BPF_X: /* ((s64) dst >= (s64) src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JGT | BPF_X: /* (dst > src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JGE | BPF_X: /* (dst >= src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JNE | BPF_X: /* (dst != src) */
 -		mask = 0x7000; /* jne */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JEQ | BPF_X: /* (dst == src) */
 -		mask = 0x8000; /* je */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JSET | BPF_X: /* (dst & src) */
 -		mask = 0x7000; /* jnz */
 -		/* ngrk %w1,%dst,%src */
 -		EMIT4_RRF(0xb9e40000, REG_W1, dst_reg, src_reg);
 -		goto branch_oc;
 -branch_ks:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* cgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_ku:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* clgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_xs:
 -		/* cgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, src_reg, i, off, mask);
 -		break;
 -branch_xu:
 -		/* clgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, src_reg, i, off, mask);
 -		break;
 -branch_oc:
 -		/* brc mask,jmp_off (branch instruction needs 4 bytes) */
 -		jmp_off = addrs[i + off + 1] - (addrs[i + 1] - 4);
 -		EMIT4_PCREL(0xa7040000 | mask << 8, jmp_off);
 +	case BPF_S_ST: /* mem[K] = A */
 +		jit->seen |= SEEN_MEM;
 +		/* st %r5,<K>(%r15) */
 +		EMIT4_DISP(0x5050f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_STX: /* mem[K] = X : mov %ebx,off8(%rbp) */
 +		jit->seen |= SEEN_XREG | SEEN_MEM;
 +		/* st %r12,<K>(%r15) */
 +		EMIT4_DISP(0x50c0f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_ANC_PROTOCOL: /* A = ntohs(skb->protocol); */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(protocol)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, protocol));
 +		break;
 +	case BPF_S_ANC_IFINDEX:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->ifindex */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* l %r5,<d(ifindex)>(%r1) */
 +		EMIT4_DISP(0x58501000, offsetof(struct net_device, ifindex));
 +		break;
 +	case BPF_S_ANC_MARK: /* A = skb->mark */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
 +		/* l %r5,<d(mark)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, mark));
 +		break;
 +	case BPF_S_ANC_QUEUE: /* A = skb->queue_mapping */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(queue_mapping)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, queue_mapping));
 +		break;
 +	case BPF_S_ANC_HATYPE:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->type */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(type)>(%r1) */
 +		EMIT4_DISP(0xbf531000, offsetof(struct net_device, type));
 +		break;
 +	case BPF_S_ANC_RXHASH: /* A = skb->hash */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
 +		/* l %r5,<d(hash)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, hash));
 +		break;
 +	case BPF_S_ANC_VLAN_TAG:
 +	case BPF_S_ANC_VLAN_TAG_PRESENT:
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
 +		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(vlan_tci)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, vlan_tci));
 +		if (filter->code == BPF_S_ANC_VLAN_TAG) {
 +			/* nill %r5,0xefff */
 +			EMIT4_IMM(0xa5570000, ~VLAN_TAG_PRESENT);
 +		} else {
 +			/* nill %r5,0x1000 */
 +			EMIT4_IMM(0xa5570000, VLAN_TAG_PRESENT);
 +			/* srl %r5,12 */
 +			EMIT4_DISP(0x88500000, 12);
 +		}
  		break;
 -	/*
 -	 * BPF_LD
 -	 */
 -	case BPF_LD | BPF_ABS | BPF_B: /* b0 = *(u8 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_B: /* b0 = *(u8 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_byte_pos);
 -		else
 -			func_addr = __pa(sk_load_byte);
 -		goto call_fn;
 -	case BPF_LD | BPF_ABS | BPF_H: /* b0 = *(u16 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_H: /* b0 = *(u16 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_half_pos);
 -		else
 -			func_addr = __pa(sk_load_half);
 -		goto call_fn;
 -	case BPF_LD | BPF_ABS | BPF_W: /* b0 = *(u32 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_W: /* b0 = *(u32 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_word_pos);
 -		else
 -			func_addr = __pa(sk_load_word);
 -		goto call_fn;
 -call_fn:
 -		jit->seen |= SEEN_SKB | SEEN_RET0 | SEEN_FUNC;
 -		REG_SET_SEEN(REG_14); /* Return address of possible func call */
 -
 -		/*
 -		 * Implicit input:
 -		 *  BPF_REG_6	 (R7) : skb pointer
 -		 *  REG_SKB_DATA (R12): skb data pointer
 -		 *
 -		 * Calculated input:
 -		 *  BPF_REG_2	 (R3) : offset of byte(s) to fetch in skb
 -		 *  BPF_REG_5	 (R6) : return address
 -		 *
 -		 * Output:
 -		 *  BPF_REG_0	 (R14): data read from skb
 -		 *
 -		 * Scratch registers (BPF_REG_1-5)
 -		 */
 -
 -		/* Call function: llilf %w1,func_addr  */
 -		EMIT6_IMM(0xc00f0000, REG_W1, func_addr);
 -
 -		/* Offset: lgfi %b2,imm */
 -		EMIT6_IMM(0xc0010000, BPF_REG_2, imm);
 -		if (BPF_MODE(insn->code) == BPF_IND)
 -			/* agfr %b2,%src (%src is s32 here) */
 -			EMIT4(0xb9180000, BPF_REG_2, src_reg);
 -
 -		/* basr %b5,%w1 (%b5 is call saved) */
 -		EMIT2(0x0d00, BPF_REG_5, REG_W1);
 -
 -		/*
 -		 * Note: For fast access we jump directly after the
 -		 * jnz instruction from bpf_jit.S
 -		 */
 -		/* jnz <ret0> */
 -		EMIT4_PCREL(0xa7740000, jit->ret0_ip - jit->prg);
 +	case BPF_S_ANC_CPU: /* A = smp_processor_id() */
 +#ifdef CONFIG_SMP
 +		/* l %r5,<d(cpu_nr)> */
 +		EMIT4_DISP(0x58500000, offsetof(struct _lowcore, cpu_nr));
 +#else
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +#endif
  		break;
  	default: /* too complex, give up */
 -		pr_err("Unknown opcode %02x\n", insn->code);
 -		return -1;
 +		goto out;
  	}
++<<<<<<< HEAD
 +	addrs[i] = jit->prg - jit->start;
++=======
+ 	return insn_count;
+ }
+ 
+ /*
+  * Compile eBPF program into s390x code
+  */
+ static int bpf_jit_prog(struct bpf_jit *jit, struct bpf_prog *fp)
+ {
+ 	int i, insn_count;
+ 
+ 	jit->lit = jit->lit_start;
+ 	jit->prg = 0;
+ 
+ 	bpf_jit_prologue(jit, bpf_prog_was_classic(fp));
+ 	for (i = 0; i < fp->len; i += insn_count) {
+ 		insn_count = bpf_jit_insn(jit, fp, i);
+ 		if (insn_count < 0)
+ 			return -1;
+ 		jit->addrs[i + 1] = jit->prg; /* Next instruction address */
+ 	}
+ 	bpf_jit_epilogue(jit);
+ 
+ 	jit->lit_start = jit->prg;
+ 	jit->size = jit->lit;
+ 	jit->size_prg = jit->prg;
++>>>>>>> 7b36f92934e4 (bpf: provide helper that indicates eBPF was migrated)
  	return 0;
 +out:
 +	return -1;
  }
  
 -/*
 - * Classic BPF function stub. BPF programs will be converted into
 - * eBPF and then bpf_int_jit_compile() will be called.
 - */
 -void bpf_jit_compile(struct bpf_prog *fp)
 -{
 -}
 -
 -/*
 - * Compile eBPF program "fp"
 - */
 -void bpf_int_jit_compile(struct bpf_prog *fp)
 +void bpf_jit_compile(struct sk_filter *fp)
  {
 -	struct bpf_binary_header *header;
 -	struct bpf_jit jit;
 -	int pass;
 +	unsigned long size, prg_len, lit_len;
 +	struct bpf_jit jit, cjit;
 +	unsigned int *addrs;
 +	int pass, i;
  
  	if (!bpf_jit_enable)
  		return;
diff --cc include/linux/filter.h
index d322ed880333,6b025491120d..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -21,79 -313,127 +21,96 @@@ struct compat_sock_fprog 
  };
  #endif
  
 -struct sock_fprog_kern {
 -	u16			len;
 -	struct sock_filter	*filter;
 -};
 +struct sk_buff;
 +struct sock;
 +struct bpf_prog_aux;
  
 -struct bpf_binary_header {
 -	unsigned int pages;
 -	u8 image[];
 +struct bpf_prog
 +{
 +	struct bpf_prog_aux	*aux;	/* Auxiliary fields */
  };
  
 -struct bpf_prog {
 -	u16			pages;		/* Number of allocated pages */
 -	bool			jited;		/* Is our filter JIT'ed? */
 -	bool			gpl_compatible;	/* Is our filter GPL compatible? */
 -	u32			len;		/* Number of filter blocks */
 -	enum bpf_prog_type	type;		/* Type of BPF program */
 -	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
 -	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 +struct sk_filter
 +{
 +	atomic_t		refcnt;
 +	unsigned int         	len;	/* Number of filter blocks */
  	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 -					    const struct bpf_insn *filter);
 -	/* Instructions for interpreter */
 -	union {
 -		struct sock_filter	insns[0];
 -		struct bpf_insn		insnsi[0];
 -	};
 +					    const struct sock_filter *filter);
 +	struct rcu_head		rcu;
 +	struct sock_filter     	insns[0];
  };
  
 -struct sk_filter {
 -	atomic_t	refcnt;
 -	struct rcu_head	rcu;
 -	struct bpf_prog	*prog;
 +struct xdp_buff {
 +	void *data;
 +	void *data_end;
 +	void *data_hard_start;
  };
  
 -#define BPF_PROG_RUN(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 -
 -static inline unsigned int bpf_prog_size(unsigned int proglen)
 +/* compute the linear packet data range [data, data_end) which
 + * will be accessed by cls_bpf and act_bpf programs
 + */
 +static inline unsigned int sk_filter_len(const struct sk_filter *fp)
  {
 -	return max(sizeof(struct bpf_prog),
 -		   offsetof(struct bpf_prog, insns[proglen]));
 +	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
  }
  
++<<<<<<< HEAD
 +static inline void bpf_compute_data_end(struct sk_buff *skb)
++=======
+ static inline bool bpf_prog_was_classic(const struct bpf_prog *prog)
+ {
+ 	/* When classic BPF programs have been loaded and the arch
+ 	 * does not have a classic BPF JIT (anymore), they have been
+ 	 * converted via bpf_migrate_filter() to eBPF and thus always
+ 	 * have an unspec program type.
+ 	 */
+ 	return prog->type == BPF_PROG_TYPE_UNSPEC;
+ }
+ 
+ #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
+ 
+ #ifdef CONFIG_DEBUG_SET_MODULE_RONX
+ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
++>>>>>>> 7b36f92934e4 (bpf: provide helper that indicates eBPF was migrated)
  {
 -	set_memory_ro((unsigned long)fp, fp->pages);
 +	return;
  }
  
 -static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 -{
 -	set_memory_rw((unsigned long)fp, fp->pages);
 -}
 -#else
 -static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 +int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
 +static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
  {
 +	return sk_filter_trim_cap(sk, skb, 1);
  }
  
 -static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 +extern unsigned int sk_run_filter(const struct sk_buff *skb,
 +				  const struct sock_filter *filter);
 +extern int sk_unattached_filter_create(struct sk_filter **pfp,
 +				       struct sock_fprog *fprog);
 +extern void sk_unattached_filter_destroy(struct sk_filter *fp);
 +extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 +extern int sk_detach_filter(struct sock *sk);
 +extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 +extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 +extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
 +
 +static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 +				   struct xdp_buff *xdp)
  {
 +	return 0;
  }
 -#endif /* CONFIG_DEBUG_SET_MODULE_RONX */
 -
 -int sk_filter(struct sock *sk, struct sk_buff *skb);
  
 -int bpf_prog_select_runtime(struct bpf_prog *fp);
 -void bpf_prog_free(struct bpf_prog *fp);
 -
 -struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
 -struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 -				  gfp_t gfp_extra_flags);
 -void __bpf_prog_free(struct bpf_prog *fp);
 -
 -static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 +static inline void bpf_warn_invalid_xdp_action(u32 act)
  {
 -	bpf_prog_unlock_ro(fp);
 -	__bpf_prog_free(fp);
 +	return;
  }
  
 -typedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,
 -				       unsigned int flen);
 -
 -int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
 -int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
 -			      bpf_aux_classic_check_t trans);
 -void bpf_prog_destroy(struct bpf_prog *fp);
 -
 -int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 -int sk_attach_bpf(u32 ufd, struct sock *sk);
 -int sk_detach_filter(struct sock *sk);
 -int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 -		  unsigned int len);
 -
 -bool sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 -void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 -
 -u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 -void bpf_int_jit_compile(struct bpf_prog *fp);
 -bool bpf_helper_changes_skb_data(void *func);
 -
  #ifdef CONFIG_BPF_JIT
 -typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
 -
 -struct bpf_binary_header *
 -bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
 -		     unsigned int alignment,
 -		     bpf_jit_fill_hole_t bpf_fill_ill_insns);
 -void bpf_jit_binary_free(struct bpf_binary_header *hdr);
 +#include <stdarg.h>
 +#include <linux/linkage.h>
 +#include <linux/printk.h>
  
 -void bpf_jit_compile(struct bpf_prog *fp);
 -void bpf_jit_free(struct bpf_prog *fp);
 +extern void bpf_jit_compile(struct sk_filter *fp);
 +extern void bpf_jit_free(struct sk_filter *fp);
  
  static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
  				u32 pass, void *image)
* Unmerged path arch/s390/net/bpf_jit_comp.c
* Unmerged path include/linux/filter.h
