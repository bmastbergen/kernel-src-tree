dax: dax_layout_busy_page() warn on !exceptional

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Ross Zwisler <ross.zwisler@linux.intel.com>
commit cdbf8897cb09b7baf2b8a7e78051a35a872b01d5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/cdbf8897.failed

Inodes using DAX should only ever have exceptional entries in their page
caches.  Make this clear by warning if the iteration in
dax_layout_busy_page() ever sees a non-exceptional entry, and by adding a
comment for the pagevec_release() call which only deals with struct page
pointers.

	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Signed-off-by: Theodore Ts'o <tytso@mit.edu>
	Reviewed-by: Jan Kara <jack@suse.cz>
(cherry picked from commit cdbf8897cb09b7baf2b8a7e78051a35a872b01d5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 879d2cfa39b7,897b51e41d8f..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -415,30 -505,97 +415,96 @@@ restart
  	return entry;
  }
  
 -/**
 - * dax_layout_busy_page - find first pinned page in @mapping
 - * @mapping: address space to scan for a page with ref count > 1
 - *
 - * DAX requires ZONE_DEVICE mapped pages. These pages are never
 - * 'onlined' to the page allocator so they are considered idle when
 - * page->count == 1. A filesystem uses this interface to determine if
 - * any page in the mapping is busy, i.e. for DMA, or other
 - * get_user_pages() usages.
 - *
 - * It is expected that the filesystem is holding locks to block the
 - * establishment of new mappings in this address_space. I.e. it expects
 - * to be able to run unmap_mapping_range() and subsequently not race
 - * mapping_mapped() becoming true.
 +/*
 + * We do not necessarily hold the mapping->tree_lock when we call this
 + * function so it is possible that 'entry' is no longer a valid item in the
 + * radix tree.  This is okay because all we really need to do is to find the
 + * correct waitqueue where tasks might be waiting for that old 'entry' and
 + * wake them.
   */
 -struct page *dax_layout_busy_page(struct address_space *mapping)
 +void dax_wake_mapping_entry_waiter(struct address_space *mapping,
 +		pgoff_t index, void *entry, bool wake_all)
  {
 -	pgoff_t	indices[PAGEVEC_SIZE];
 -	struct page *page = NULL;
 -	struct pagevec pvec;
 -	pgoff_t	index, end;
 -	unsigned i;
 +	struct exceptional_entry_key key;
 +	wait_queue_head_t *wq;
 +
 +	wq = dax_entry_waitqueue(mapping, index, entry, &key);
  
  	/*
 -	 * In the 'limited' case get_user_pages() for dax is disabled.
 +	 * Checking for locked entry and prepare_to_wait_exclusive() happens
 +	 * under mapping->tree_lock, ditto for entry handling in our callers.
 +	 * So at this point all tasks that could have seen our entry locked
 +	 * must be in the waitqueue and the following check will see them.
  	 */
++<<<<<<< HEAD
 +	if (waitqueue_active(wq))
 +		__wake_up(wq, TASK_NORMAL, wake_all ? 0 : 1, &key);
++=======
+ 	if (IS_ENABLED(CONFIG_FS_DAX_LIMITED))
+ 		return NULL;
+ 
+ 	if (!dax_mapping(mapping) || !mapping_mapped(mapping))
+ 		return NULL;
+ 
+ 	pagevec_init(&pvec);
+ 	index = 0;
+ 	end = -1;
+ 
+ 	/*
+ 	 * If we race get_user_pages_fast() here either we'll see the
+ 	 * elevated page count in the pagevec_lookup and wait, or
+ 	 * get_user_pages_fast() will see that the page it took a reference
+ 	 * against is no longer mapped in the page tables and bail to the
+ 	 * get_user_pages() slow path.  The slow path is protected by
+ 	 * pte_lock() and pmd_lock(). New references are not taken without
+ 	 * holding those locks, and unmap_mapping_range() will not zero the
+ 	 * pte or pmd without holding the respective lock, so we are
+ 	 * guaranteed to either see new references or prevent new
+ 	 * references from being established.
+ 	 */
+ 	unmap_mapping_range(mapping, 0, 0, 1);
+ 
+ 	while (index < end && pagevec_lookup_entries(&pvec, mapping, index,
+ 				min(end - index, (pgoff_t)PAGEVEC_SIZE),
+ 				indices)) {
+ 		for (i = 0; i < pagevec_count(&pvec); i++) {
+ 			struct page *pvec_ent = pvec.pages[i];
+ 			void *entry;
+ 
+ 			index = indices[i];
+ 			if (index >= end)
+ 				break;
+ 
+ 			if (WARN_ON_ONCE(
+ 			     !radix_tree_exceptional_entry(pvec_ent)))
+ 				continue;
+ 
+ 			xa_lock_irq(&mapping->i_pages);
+ 			entry = get_unlocked_mapping_entry(mapping, index, NULL);
+ 			if (entry)
+ 				page = dax_busy_page(entry);
+ 			put_unlocked_mapping_entry(mapping, index, entry);
+ 			xa_unlock_irq(&mapping->i_pages);
+ 			if (page)
+ 				break;
+ 		}
+ 
+ 		/*
+ 		 * We don't expect normal struct page entries to exist in our
+ 		 * tree, but we keep these pagevec calls so that this code is
+ 		 * consistent with the common pattern for handling pagevecs
+ 		 * throughout the kernel.
+ 		 */
+ 		pagevec_remove_exceptionals(&pvec);
+ 		pagevec_release(&pvec);
+ 		index++;
+ 
+ 		if (page)
+ 			break;
+ 	}
+ 	return page;
++>>>>>>> cdbf8897cb09 (dax: dax_layout_busy_page() warn on !exceptional)
  }
 -EXPORT_SYMBOL_GPL(dax_layout_busy_page);
  
  static int __dax_invalidate_mapping_entry(struct address_space *mapping,
  					  pgoff_t index, bool trunc)
* Unmerged path fs/dax.c
