nfp: bpf: refactor offload logic

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 9ce7a956327ad6c14e1a7eb9f4cb5300c8b61db6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9ce7a956.failed

We currently create a fake cls_bpf offload object when we want
to offload XDP.  Simplify and clarify the code by moving the
TC/XDP specific logic out of common offload code.  This is easy
now that we don't support legacy TC actions.  We only need the
bpf program and state of the skip_sw flag.

Temporarily set @code to NULL in nfp_net_bpf_offload(), compilers
seem to have trouble recognizing it's always initialized.  Next
patches will eliminate that variable.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 9ce7a956327ad6c14e1a7eb9f4cb5300c8b61db6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/main.c
#	drivers/net/ethernet/netronome/nfp/bpf/main.h
#	drivers/net/ethernet/netronome/nfp/bpf/offload.c
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.c
index 4ca14182b178,9e1286346d42..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.c
@@@ -111,7 -86,66 +108,70 @@@ static void nfp_bpf_vnic_free(struct nf
  {
  	if (nn->dp.bpf_offload_xdp)
  		nfp_bpf_xdp_offload(app, nn, NULL);
++<<<<<<< HEAD
 +	kfree(nn->app_priv);
++=======
+ }
+ 
+ static int nfp_bpf_setup_tc_block_cb(enum tc_setup_type type,
+ 				     void *type_data, void *cb_priv)
+ {
+ 	struct tc_cls_bpf_offload *cls_bpf = type_data;
+ 	struct nfp_net *nn = cb_priv;
+ 	bool skip_sw;
+ 
+ 	if (type != TC_SETUP_CLSBPF ||
+ 	    !tc_can_offload(nn->dp.netdev) ||
+ 	    !nfp_net_ebpf_capable(nn) ||
+ 	    cls_bpf->common.protocol != htons(ETH_P_ALL) ||
+ 	    cls_bpf->common.chain_index)
+ 		return -EOPNOTSUPP;
+ 	if (nn->dp.bpf_offload_xdp)
+ 		return -EBUSY;
+ 
+ 	/* Only support TC direct action */
+ 	if (!cls_bpf->exts_integrated ||
+ 	    tcf_exts_has_actions(cls_bpf->exts)) {
+ 		nn_err(nn, "only direct action with no legacy actions supported\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	skip_sw = !!(cls_bpf->gen_flags & TCA_CLS_FLAGS_SKIP_SW);
+ 
+ 	switch (cls_bpf->command) {
+ 	case TC_CLSBPF_REPLACE:
+ 		return nfp_net_bpf_offload(nn, cls_bpf->prog, true, !skip_sw);
+ 	case TC_CLSBPF_ADD:
+ 		return nfp_net_bpf_offload(nn, cls_bpf->prog, false, !skip_sw);
+ 	case TC_CLSBPF_DESTROY:
+ 		return nfp_net_bpf_offload(nn, NULL, true, !skip_sw);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ static int nfp_bpf_setup_tc_block(struct net_device *netdev,
+ 				  struct tc_block_offload *f)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 
+ 	if (f->binder_type != TCF_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (f->command) {
+ 	case TC_BLOCK_BIND:
+ 		return tcf_block_cb_register(f->block,
+ 					     nfp_bpf_setup_tc_block_cb,
+ 					     nn, nn);
+ 	case TC_BLOCK_UNBIND:
+ 		tcf_block_cb_unregister(f->block,
+ 					nfp_bpf_setup_tc_block_cb,
+ 					nn);
+ 		return 0;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
++>>>>>>> 9ce7a956327a (nfp: bpf: refactor offload logic)
  }
  
  static int nfp_bpf_setup_tc(struct nfp_app *app, struct net_device *netdev,
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.h
index cc2a5beba757,6dddab95d57a..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.h
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.h
@@@ -194,24 -181,8 +194,28 @@@ nfp_bpf_jit(struct bpf_prog *filter, vo
  int nfp_prog_verify(struct nfp_prog *nfp_prog, struct bpf_prog *prog);
  
  struct nfp_net;
- struct tc_cls_bpf_offload;
  
++<<<<<<< HEAD
 +/**
 + * struct nfp_net_bpf_priv - per-vNIC BPF private data
 + * @rx_filter:		Filter offload statistics - dropped packets/bytes
 + * @rx_filter_prev:	Filter offload statistics - values from previous update
 + * @rx_filter_change:	Jiffies when statistics last changed
 + * @rx_filter_stats_timer:  Timer for polling filter offload statistics
 + * @rx_filter_lock:	Lock protecting timer state changes (teardown)
 + */
 +struct nfp_net_bpf_priv {
 +	struct nfp_stat_pair rx_filter, rx_filter_prev;
 +	unsigned long rx_filter_change;
 +	struct timer_list rx_filter_stats_timer;
 +	spinlock_t rx_filter_lock;
 +};
 +
 +int nfp_net_bpf_offload(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf);
 +void nfp_net_filter_stats_timer(unsigned long data);
++=======
+ int nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,
+ 			bool old_prog, bool sw_fallback);
++>>>>>>> 9ce7a956327a (nfp: bpf: refactor offload logic)
  
  #endif
diff --cc drivers/net/ethernet/netronome/nfp/bpf/offload.c
index de79faf0874b,c09efa1a9649..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/offload.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/offload.c
@@@ -51,104 -51,8 +51,103 @@@
  #include "../nfp_net_ctrl.h"
  #include "../nfp_net.h"
  
 +void nfp_net_filter_stats_timer(unsigned long data)
 +{
 +	struct nfp_net *nn = (void *)data;
 +	struct nfp_net_bpf_priv *priv;
 +	struct nfp_stat_pair latest;
 +
 +	priv = nn->app_priv;
 +
 +	spin_lock_bh(&priv->rx_filter_lock);
 +
 +	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 +		mod_timer(&priv->rx_filter_stats_timer,
 +			  jiffies + NFP_NET_STAT_POLL_IVL);
 +
 +	spin_unlock_bh(&priv->rx_filter_lock);
 +
 +	latest.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	latest.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +
 +	if (latest.pkts != priv->rx_filter.pkts)
 +		priv->rx_filter_change = jiffies;
 +
 +	priv->rx_filter = latest;
 +}
 +
 +#if 0 /* Not in RHEL7 */
 +static void nfp_net_bpf_stats_reset(struct nfp_net *nn)
 +{
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +
 +	priv->rx_filter.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);
 +	priv->rx_filter.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);
 +	priv->rx_filter_prev = priv->rx_filter;
 +	priv->rx_filter_change = jiffies;
 +}
 +
 +static int
 +nfp_net_bpf_stats_update(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
 +{
 +	struct tc_action *a;
 +	LIST_HEAD(actions);
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bytes, pkts;
 +
 +	pkts = priv->rx_filter.pkts - priv->rx_filter_prev.pkts;
 +	bytes = priv->rx_filter.bytes - priv->rx_filter_prev.bytes;
 +	bytes -= pkts * ETH_HLEN;
 +
 +	priv->rx_filter_prev = priv->rx_filter;
 +
 +	preempt_disable();
 +
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list)
 +		tcf_action_stats_update(a, bytes, pkts, nn->rx_filter_change);
 +
 +	preempt_enable();
 +
 +	return 0;
 +}
 +
 +static int
 +nfp_net_bpf_get_act(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
 +{
 +	const struct tc_action *a;
 +	LIST_HEAD(actions);
 +
 +	if (!cls_bpf->exts)
 +		return NN_ACT_XDP;
 +
 +	/* TC direct action */
 +	if (cls_bpf->exts_integrated) {
 +		if (tc_no_actions(cls_bpf->exts))
 +			return NN_ACT_DIRECT;
 +
 +		return -EOPNOTSUPP;
 +	}
 +
 +	/* TC legacy mode */
 +	if (!tc_single_action(cls_bpf->exts))
 +		return -EOPNOTSUPP;
 +
 +	tcf_exts_to_list(cls_bpf->exts, &actions);
 +	list_for_each_entry(a, &actions, list) {
 +		if (is_tcf_gact_shot(a))
 +			return NN_ACT_TC_DROP;
 +
 +		if (is_tcf_mirred_egress_redirect(a) &&
 +		    tcf_mirred_ifindex(a) == nn->dp.netdev->ifindex)
 +			return NN_ACT_TC_REDIR;
 +	}
 +
 +	return -EOPNOTSUPP;
 +}
 +
  static int
- nfp_net_bpf_offload_prepare(struct nfp_net *nn,
- 			    struct tc_cls_bpf_offload *cls_bpf,
+ nfp_net_bpf_offload_prepare(struct nfp_net *nn, struct bpf_prog *prog,
  			    struct nfp_bpf_result *res,
  			    void **code, dma_addr_t *dma_addr, u16 max_instr)
  {
@@@ -173,16 -71,10 +172,16 @@@
  	start_off = nn_readw(nn, NFP_NET_CFG_BPF_START);
  	done_off = nn_readw(nn, NFP_NET_CFG_BPF_DONE);
  
 +	if (cls_bpf->prog->aux->stack_depth > 64) {
 +		nn_info(nn, "large stack not supported: program %dB > 64B\n",
 +			cls_bpf->prog->aux->stack_depth);
 +		return -EOPNOTSUPP;
 +	}
 +
  	stack_size = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;
- 	if (cls_bpf->prog->aux->stack_depth > stack_size) {
+ 	if (prog->aux->stack_depth > stack_size) {
  		nn_info(nn, "stack too large: program %dB > FW stack %dB\n",
- 			cls_bpf->prog->aux->stack_depth, stack_size);
+ 			prog->aux->stack_depth, stack_size);
  		return -EOPNOTSUPP;
  	}
  
@@@ -190,8 -82,7 +189,12 @@@
  	if (!*code)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	ret = nfp_bpf_jit(cls_bpf->prog, *code, act, start_off, done_off,
 +			  max_instr, res);
++=======
+ 	ret = nfp_bpf_jit(prog, *code, start_off, done_off, max_instr, res);
++>>>>>>> 9ce7a956327a (nfp: bpf: refactor offload logic)
  	if (ret)
  		goto out;
  
@@@ -203,22 -94,16 +206,22 @@@ out
  }
  
  static void
- nfp_net_bpf_load_and_start(struct nfp_net *nn, u32 tc_flags,
+ nfp_net_bpf_load_and_start(struct nfp_net *nn, bool sw_fallback,
  			   void *code, dma_addr_t dma_addr,
 -			   unsigned int code_sz, unsigned int n_instr)
 +			   unsigned int code_sz, unsigned int n_instr,
 +			   bool dense_mode)
  {
 +	struct nfp_net_bpf_priv *priv = nn->app_priv;
 +	u64 bpf_addr = dma_addr;
  	int err;
  
- 	nn->dp.bpf_offload_skip_sw = !!(tc_flags & TCA_CLS_FLAGS_SKIP_SW);
+ 	nn->dp.bpf_offload_skip_sw = !sw_fallback;
  
 +	if (dense_mode)
 +		bpf_addr |= NFP_NET_CFG_BPF_CFG_8CTX;
 +
  	nn_writew(nn, NFP_NET_CFG_BPF_SIZE, n_instr);
 -	nn_writeq(nn, NFP_NET_CFG_BPF_ADDR, dma_addr);
 +	nn_writeq(nn, NFP_NET_CFG_BPF_ADDR, bpf_addr);
  
  	/* Load up the JITed code */
  	err = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_BPF);
@@@ -256,11 -131,10 +259,12 @@@ static int nfp_net_bpf_stop(struct nfp_
  
  	return nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);
  }
 +#endif
  
- int nfp_net_bpf_offload(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)
+ int nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,
+ 			bool old_prog, bool sw_fallback)
  {
 +#if 0 /* Not in RHEL7 */
  	struct nfp_bpf_result res;
  	dma_addr_t dma_addr;
  	u16 max_instr;
@@@ -285,37 -163,15 +293,51 @@@
  						  &dma_addr, max_instr);
  		if (err)
  			return err;
++<<<<<<< HEAD
 +
 +		nfp_net_bpf_stop(nn);
 +		nfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,
 +					   dma_addr, max_instr * sizeof(u64),
 +					   res.n_instr, res.dense_mode);
 +		return 0;
 +
 +	case TC_CLSBPF_ADD:
 +		if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
 +			return -EBUSY;
 +
 +		err = nfp_net_bpf_offload_prepare(nn, cls_bpf, &res, &code,
 +						  &dma_addr, max_instr);
 +		if (err)
 +			return err;
 +
 +		nfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,
 +					   dma_addr, max_instr * sizeof(u64),
 +					   res.n_instr, res.dense_mode);
 +		return 0;
 +
 +	case TC_CLSBPF_DESTROY:
 +		return nfp_net_bpf_stop(nn);
 +
 +	case TC_CLSBPF_STATS:
 +		return nfp_net_bpf_stats_update(nn, cls_bpf);
 +
 +	default:
 +		return -EOPNOTSUPP;
 +	}
 +#else
 +	return -EOPNOTSUPP;
 +#endif
++=======
+ 	}
+ 
+ 	if (old_prog)
+ 		nfp_net_bpf_stop(nn);
+ 
+ 	if (prog)
+ 		nfp_net_bpf_load_and_start(nn, sw_fallback, code,
+ 					   dma_addr, max_instr * sizeof(u64),
+ 					   res.n_instr);
+ 
+ 	return 0;
++>>>>>>> 9ce7a956327a (nfp: bpf: refactor offload logic)
  }
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.c
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.h
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/offload.c
