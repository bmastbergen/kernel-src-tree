xfs, dax: introduce xfs_dax_aops

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 6e2608dfd93464bb26ba868b301ad5336c8c1df8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6e2608df.failed

In preparation for the dax implementation to start associating dax pages
to inodes via page->mapping, we need to provide a 'struct
address_space_operations' instance for dax. Otherwise, direct-I/O
triggers incorrect page cache assumptions and warnings like the
following:

 WARNING: CPU: 27 PID: 1783 at fs/xfs/xfs_aops.c:1468
 xfs_vm_set_page_dirty+0xf3/0x1b0 [xfs]
 [..]
 CPU: 27 PID: 1783 Comm: dma-collision Tainted: G           O 4.15.0-rc2+ #984
 [..]
 Call Trace:
  set_page_dirty_lock+0x40/0x60
  bio_set_pages_dirty+0x37/0x50
  iomap_dio_actor+0x2b7/0x3b0
  ? iomap_dio_zero+0x110/0x110
  iomap_apply+0xa4/0x110
  iomap_dio_rw+0x29e/0x3b0
  ? iomap_dio_zero+0x110/0x110
  ? xfs_file_dio_aio_read+0x7c/0x1a0 [xfs]
  xfs_file_dio_aio_read+0x7c/0x1a0 [xfs]
  xfs_file_read_iter+0xa0/0xc0 [xfs]
  __vfs_read+0xf9/0x170
  vfs_read+0xa6/0x150
  SyS_pread64+0x93/0xb0
  entry_SYSCALL_64_fastpath+0x1f/0x96

...where the default set_page_dirty() handler assumes that dirty state
is being tracked in 'struct page' flags.

	Cc: Jeff Moyer <jmoyer@redhat.com>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Suggested-by: Jan Kara <jack@suse.cz>
	Suggested-by: Dave Chinner <david@fromorbit.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 6e2608dfd93464bb26ba868b301ad5336c8c1df8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_aops.c
diff --cc fs/xfs/xfs_aops.c
index 27f979ff685f,e7a56c4786ff..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -1427,349 -1373,6 +1433,352 @@@ out_unlock
  	return error;
  }
  
++<<<<<<< HEAD
 +int
 +xfs_get_blocks(
 +	struct inode		*inode,
 +	sector_t		iblock,
 +	struct buffer_head	*bh_result,
 +	int			create)
 +{
 +	return __xfs_get_blocks(inode, iblock, bh_result, create, false);
 +}
 +
 +int
 +xfs_get_blocks_direct(
 +	struct inode		*inode,
 +	sector_t		iblock,
 +	struct buffer_head	*bh_result,
 +	int			create)
 +{
 +	return __xfs_get_blocks(inode, iblock, bh_result, create, true);
 +}
 +
 +static void
 +__xfs_end_io_direct_write(
 +	struct inode		*inode,
 +	struct xfs_ioend	*ioend,
 +	loff_t			offset,
 +	ssize_t			size)
 +{
 +	struct xfs_mount	*mp = XFS_I(inode)->i_mount;
 +	unsigned long		flags;
 +
 +	if (XFS_FORCED_SHUTDOWN(mp) || ioend->io_error)
 +		goto out_end_io;
 +
 +	/*
 +	 * dio completion end_io functions are only called on writes if more
 +	 * than 0 bytes was written.
 +	 */
 +	ASSERT(size > 0);
 +
 +	/*
 +	 * The ioend only maps whole blocks, while the IO may be sector aligned.
 +	 * Hence the ioend offset/size may not match the IO offset/size exactly.
 +	 * Because we don't map overwrites within EOF into the ioend, the offset
 +	 * may not match, but only if the endio spans EOF.  Either way, write
 +	 * the IO sizes into the ioend so that completion processing does the
 +	 * right thing.
 +	 */
 +	ASSERT(offset + size <= ioend->io_offset + ioend->io_size);
 +	ioend->io_size = size;
 +	ioend->io_offset = offset;
 +
 +	/*
 +	 * The ioend tells us whether we are doing unwritten extent conversion
 +	 * or an append transaction that updates the on-disk file size. These
 +	 * cases are the only cases where we should *potentially* be needing
 +	 * to update the VFS inode size.
 +	 *
 +	 * We need to update the in-core inode size here so that we don't end up
 +	 * with the on-disk inode size being outside the in-core inode size. We
 +	 * have no other method of updating EOF for AIO, so always do it here
 +	 * if necessary.
 +	 *
 +	 * We need to lock the test/set EOF update as we can be racing with
 +	 * other IO completions here to update the EOF. Failing to serialise
 +	 * here can result in EOF moving backwards and Bad Things Happen when
 +	 * that occurs.
 +	 */
 +	spin_lock_irqsave(&XFS_I(inode)->i_size_lock, flags);
 +	if (offset + size > i_size_read(inode))
 +		i_size_write(inode, offset + size);
 +	spin_unlock_irqrestore(&XFS_I(inode)->i_size_lock, flags);
 +
 +	/*
 +	 * If we are doing an append IO that needs to update the EOF on disk,
 +	 * do the transaction reserve now so we can use common end io
 +	 * processing. Stashing the error (if there is one) in the ioend will
 +	 * result in the ioend processing passing on the error if it is
 +	 * possible as we can't return it from here.
 +	 */
 +	if (ioend->io_type == XFS_IO_OVERWRITE)
 +		ioend->io_error = xfs_setfilesize_trans_alloc(ioend);
 +
 +out_end_io:
 +	xfs_end_io(&ioend->io_work);
 +	return;
 +}
 +
 +/*
 + * Complete a direct I/O write request.
 + *
 + * The ioend structure is passed from __xfs_get_blocks() to tell us what to do.
 + * If no ioend exists (i.e. @private == NULL) then the write IO is an overwrite
 + * wholly within the EOF and so there is nothing for us to do. Note that in this
 + * case the completion can be called in interrupt context, whereas if we have an
 + * ioend we will always be called in task context (i.e. from a workqueue).
 + */
 +void
 +xfs_end_io_direct_write(
 +	struct kiocb		*iocb,
 +	loff_t			offset,
 +	ssize_t			size,
 +	void			*private,
 +	int			__attribute__((unused))ret,
 +	bool			__attribute__((unused))is_async)
 +{
 +	struct inode		*inode = file_inode(iocb->ki_filp);
 +	struct xfs_ioend	*ioend = private;
 +
 +	trace_xfs_gbmap_direct_endio(XFS_I(inode), offset, size,
 +				     ioend ? ioend->io_type : 0, NULL);
 +
 +	if (!ioend) {
 +		ASSERT(offset + size <= i_size_read(inode));
 +		return;
 +	}
 +
 +	__xfs_end_io_direct_write(inode, ioend, offset, size);
 +}
 +
 +STATIC ssize_t
 +xfs_vm_direct_IO(
 +	int			rw,
 +	struct kiocb		*iocb,
 +	const struct iovec	*iov,
 +	loff_t			offset,
 +	unsigned long		nr_segs)
 +{
 +	/*
 +	 * We just need the method present so that open/fcntl allow direct I/O.
 +	 */
 +	return -EINVAL;
 +}
 +
 +/*
 + * Punch out the delalloc blocks we have already allocated.
 + *
 + * Don't bother with xfs_setattr given that nothing can have made it to disk yet
 + * as the page is still locked at this point.
 + */
 +STATIC void
 +xfs_vm_kill_delalloc_range(
 +	struct inode		*inode,
 +	loff_t			start,
 +	loff_t			end)
 +{
 +	struct xfs_inode	*ip = XFS_I(inode);
 +	xfs_fileoff_t		start_fsb;
 +	xfs_fileoff_t		end_fsb;
 +	int			error;
 +
 +	start_fsb = XFS_B_TO_FSB(ip->i_mount, start);
 +	end_fsb = XFS_B_TO_FSB(ip->i_mount, end);
 +	if (end_fsb <= start_fsb)
 +		return;
 +
 +	xfs_ilock(ip, XFS_ILOCK_EXCL);
 +	error = xfs_bmap_punch_delalloc_range(ip, start_fsb,
 +						end_fsb - start_fsb);
 +	if (error) {
 +		/* something screwed, just bail */
 +		if (!XFS_FORCED_SHUTDOWN(ip->i_mount)) {
 +			xfs_alert(ip->i_mount,
 +		"xfs_vm_write_failed: unable to clean up ino %lld",
 +					ip->i_ino);
 +		}
 +	}
 +	xfs_iunlock(ip, XFS_ILOCK_EXCL);
 +}
 +
 +STATIC void
 +xfs_vm_write_failed(
 +	struct inode		*inode,
 +	struct page		*page,
 +	loff_t			pos,
 +	unsigned		len)
 +{
 +	loff_t			block_offset;
 +	loff_t			block_start;
 +	loff_t			block_end;
 +	loff_t			from = pos & (PAGE_CACHE_SIZE - 1);
 +	loff_t			to = from + len;
 +	struct buffer_head	*bh, *head;
 +	struct xfs_mount	*mp = XFS_I(inode)->i_mount;
 +
 +	/*
 +	 * The request pos offset might be 32 or 64 bit, this is all fine
 +	 * on 64-bit platform.  However, for 64-bit pos request on 32-bit
 +	 * platform, the high 32-bit will be masked off if we evaluate the
 +	 * block_offset via (pos & PAGE_MASK) because the PAGE_MASK is
 +	 * 0xfffff000 as an unsigned long, hence the result is incorrect
 +	 * which could cause the following ASSERT failed in most cases.
 +	 * In order to avoid this, we can evaluate the block_offset of the
 +	 * start of the page by using shifts rather than masks the mismatch
 +	 * problem.
 +	 */
 +	block_offset = (pos >> PAGE_CACHE_SHIFT) << PAGE_CACHE_SHIFT;
 +
 +	ASSERT(block_offset + from == pos);
 +
 +	head = page_buffers(page);
 +	block_start = 0;
 +	for (bh = head; bh != head || !block_start;
 +	     bh = bh->b_this_page, block_start = block_end,
 +				   block_offset += bh->b_size) {
 +		block_end = block_start + bh->b_size;
 +
 +		/* skip buffers before the write */
 +		if (block_end <= from)
 +			continue;
 +
 +		/* if the buffer is after the write, we're done */
 +		if (block_start >= to)
 +			break;
 +
 +		/*
 +		 * Process delalloc and unwritten buffers beyond EOF. We can
 +		 * encounter unwritten buffers in the event that a file has
 +		 * post-EOF unwritten extents and an extending write happens to
 +		 * fail (e.g., an unaligned write that also involves a delalloc
 +		 * to the same page).
 +		 */
 +		if (!buffer_delay(bh) && !buffer_unwritten(bh))
 +			continue;
 +
 +		if (!xfs_mp_drop_writes(mp) && !buffer_new(bh) &&
 +		    block_offset < i_size_read(inode))
 +			continue;
 +
 +		if (buffer_delay(bh))
 +			xfs_vm_kill_delalloc_range(inode, block_offset,
 +						   block_offset + bh->b_size);
 +
 +		/*
 +		 * This buffer does not contain data anymore. make sure anyone
 +		 * who finds it knows that for certain.
 +		 */
 +		clear_buffer_delay(bh);
 +		clear_buffer_uptodate(bh);
 +		clear_buffer_mapped(bh);
 +		clear_buffer_new(bh);
 +		clear_buffer_dirty(bh);
 +		clear_buffer_unwritten(bh);
 +	}
 +
 +}
 +
 +/*
 + * This used to call block_write_begin(), but it unlocks and releases the page
 + * on error, and we need that page to be able to punch stale delalloc blocks out
 + * on failure. hence we copy-n-waste it here and call xfs_vm_write_failed() at
 + * the appropriate point.
 + */
 +STATIC int
 +xfs_vm_write_begin(
 +	struct file		*file,
 +	struct address_space	*mapping,
 +	loff_t			pos,
 +	unsigned		len,
 +	unsigned		flags,
 +	struct page		**pagep,
 +	void			**fsdata)
 +{
 +	pgoff_t			index = pos >> PAGE_CACHE_SHIFT;
 +	struct page		*page;
 +	int			status;
 +	struct xfs_mount	*mp = XFS_I(mapping->host)->i_mount;
 +
 +	ASSERT(len <= PAGE_CACHE_SIZE);
 +
 +	page = grab_cache_page_write_begin(mapping, index, flags);
 +	if (!page)
 +		return -ENOMEM;
 +
 +	status = __block_write_begin(page, pos, len, xfs_get_blocks);
 +	if (xfs_mp_drop_writes(mp))
 +		status = -EIO;
 +	if (unlikely(status)) {
 +		struct inode	*inode = mapping->host;
 +		size_t		isize = i_size_read(inode);
 +
 +		xfs_vm_write_failed(inode, page, pos, len);
 +		unlock_page(page);
 +
 +		/*
 +		 * If the write is beyond EOF, we only want to kill blocks
 +		 * allocated in this write, not blocks that were previously
 +		 * written successfully.
 +		 */
 +		if (xfs_mp_drop_writes(mp))
 +			isize = 0;
 +		if (pos + len > isize) {
 +			ssize_t start = max_t(ssize_t, pos, isize);
 +
 +			truncate_pagecache_range(inode, start, pos + len);
 +		}
 +
 +		page_cache_release(page);
 +		page = NULL;
 +	}
 +
 +	*pagep = page;
 +	return status;
 +}
 +
 +/*
 + * On failure, we only need to kill delalloc blocks beyond EOF in the range of
 + * this specific write because they will never be written. Previous writes
 + * beyond EOF where block allocation succeeded do not need to be trashed, so
 + * only new blocks from this write should be trashed. For blocks within
 + * EOF, generic_write_end() zeros them so they are safe to leave alone and be
 + * written with all the other valid data.
 + */
 +STATIC int
 +xfs_vm_write_end(
 +	struct file		*file,
 +	struct address_space	*mapping,
 +	loff_t			pos,
 +	unsigned		len,
 +	unsigned		copied,
 +	struct page		*page,
 +	void			*fsdata)
 +{
 +	int			ret;
 +
 +	ASSERT(len <= PAGE_CACHE_SIZE);
 +
 +	ret = generic_write_end(file, mapping, pos, len, copied, page, fsdata);
 +	if (unlikely(ret < len)) {
 +		struct inode	*inode = mapping->host;
 +		size_t		isize = i_size_read(inode);
 +		loff_t		to = pos + len;
 +
 +		if (to > isize) {
 +			/* only kill blocks in this write beyond EOF */
 +			if (pos > isize)
 +				isize = pos;
 +			xfs_vm_kill_delalloc_range(inode, isize, to);
 +			truncate_pagecache_range(inode, isize, to);
 +		}
 +	}
 +	return ret;
 +}
 +
++=======
++>>>>>>> 6e2608dfd934 (xfs, dax: introduce xfs_dax_aops)
  STATIC sector_t
  xfs_vm_bmap(
  	struct address_space	*mapping,
@@@ -1872,11 -1493,9 +1881,11 @@@ const struct address_space_operations x
  	.writepages		= xfs_vm_writepages,
  	.set_page_dirty		= xfs_vm_set_page_dirty,
  	.releasepage		= xfs_vm_releasepage,
 -	.invalidatepage		= xfs_vm_invalidatepage,
 +	.invalidatepage_range	= xfs_vm_invalidatepage,
 +	.write_begin		= xfs_vm_write_begin,
 +	.write_end		= xfs_vm_write_end,
  	.bmap			= xfs_vm_bmap,
- 	.direct_IO		= xfs_vm_direct_IO,
+ 	.direct_IO		= noop_direct_IO,
  	.migratepage		= buffer_migrate_page,
  	.is_partially_uptodate  = block_is_partially_uptodate,
  	.error_remove_page	= generic_error_remove_page,
* Unmerged path fs/xfs/xfs_aops.c
diff --git a/fs/xfs/xfs_aops.h b/fs/xfs/xfs_aops.h
index acb6dd71401d..39d1f0d9ad4b 100644
--- a/fs/xfs/xfs_aops.h
+++ b/fs/xfs/xfs_aops.h
@@ -53,6 +53,7 @@ struct xfs_ioend {
 };
 
 extern const struct address_space_operations xfs_address_space_operations;
+extern const struct address_space_operations xfs_dax_aops;
 
 int	xfs_get_blocks(struct inode *inode, sector_t offset,
 		       struct buffer_head *map_bh, int create);
diff --git a/fs/xfs/xfs_iops.c b/fs/xfs/xfs_iops.c
index 8c9150043298..ddd9a676d54c 100644
--- a/fs/xfs/xfs_iops.c
+++ b/fs/xfs/xfs_iops.c
@@ -1289,7 +1289,10 @@ xfs_setup_iops(
 	case S_IFREG:
 		inode->i_op = &xfs_inode_operations;
 		inode->i_fop = &xfs_file_operations;
-		inode->i_mapping->a_ops = &xfs_address_space_operations;
+		if (IS_DAX(inode))
+			inode->i_mapping->a_ops = &xfs_dax_aops;
+		else
+			inode->i_mapping->a_ops = &xfs_address_space_operations;
 		break;
 	case S_IFDIR:
 		if (xfs_sb_version_hasasciici(&XFS_M(inode->i_sb)->m_sb)) {
