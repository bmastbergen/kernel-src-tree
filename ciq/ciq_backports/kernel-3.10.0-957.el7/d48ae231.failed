nfp: bpf: add basic control channel communication

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit d48ae231c5e13d98e3664443c6342c2011f5df2b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/d48ae231.failed

For map support we will need to send and receive control messages.
Add basic support for sending a message to FW, and waiting for a
reply.

Control messages are tagged with a 16 bit ID.  Add a simple ID
allocator and make sure we don't allow too many messages in flight,
to avoid request <> reply mismatches.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit d48ae231c5e13d98e3664443c6342c2011f5df2b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/fw.h
#	drivers/net/ethernet/netronome/nfp/bpf/main.c
#	drivers/net/ethernet/netronome/nfp/bpf/main.h
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.c
index 60a7af297852,a14368c6449f..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.c
@@@ -143,6 -201,142 +143,145 @@@ static bool nfp_bpf_tc_busy(struct nfp_
  	return !!bv->tc_prog;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ nfp_bpf_change_mtu(struct nfp_app *app, struct net_device *netdev, int new_mtu)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 	unsigned int max_mtu;
+ 
+ 	if (~nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)
+ 		return 0;
+ 
+ 	max_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;
+ 	if (new_mtu > max_mtu) {
+ 		nn_info(nn, "BPF offload active, MTU over %u not supported\n",
+ 			max_mtu);
+ 		return -EBUSY;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ nfp_bpf_parse_cap_adjust_head(struct nfp_app_bpf *bpf, void __iomem *value,
+ 			      u32 length)
+ {
+ 	struct nfp_bpf_cap_tlv_adjust_head __iomem *cap = value;
+ 	struct nfp_cpp *cpp = bpf->app->pf->cpp;
+ 
+ 	if (length < sizeof(*cap)) {
+ 		nfp_err(cpp, "truncated adjust_head TLV: %d\n", length);
+ 		return -EINVAL;
+ 	}
+ 
+ 	bpf->adjust_head.flags = readl(&cap->flags);
+ 	bpf->adjust_head.off_min = readl(&cap->off_min);
+ 	bpf->adjust_head.off_max = readl(&cap->off_max);
+ 	bpf->adjust_head.guaranteed_sub = readl(&cap->guaranteed_sub);
+ 	bpf->adjust_head.guaranteed_add = readl(&cap->guaranteed_add);
+ 
+ 	if (bpf->adjust_head.off_min > bpf->adjust_head.off_max) {
+ 		nfp_err(cpp, "invalid adjust_head TLV: min > max\n");
+ 		return -EINVAL;
+ 	}
+ 	if (!FIELD_FIT(UR_REG_IMM_MAX, bpf->adjust_head.off_min) ||
+ 	    !FIELD_FIT(UR_REG_IMM_MAX, bpf->adjust_head.off_max)) {
+ 		nfp_warn(cpp, "disabling adjust_head - driver expects min/max to fit in as immediates\n");
+ 		memset(&bpf->adjust_head, 0, sizeof(bpf->adjust_head));
+ 		return 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_bpf_parse_capabilities(struct nfp_app *app)
+ {
+ 	struct nfp_cpp *cpp = app->pf->cpp;
+ 	struct nfp_cpp_area *area;
+ 	u8 __iomem *mem, *start;
+ 
+ 	mem = nfp_rtsym_map(app->pf->rtbl, "_abi_bpf_capabilities", "bpf.cap",
+ 			    8, &area);
+ 	if (IS_ERR(mem))
+ 		return PTR_ERR(mem) == -ENOENT ? 0 : PTR_ERR(mem);
+ 
+ 	start = mem;
+ 	while (mem - start + 8 < nfp_cpp_area_size(area)) {
+ 		u8 __iomem *value;
+ 		u32 type, length;
+ 
+ 		type = readl(mem);
+ 		length = readl(mem + 4);
+ 		value = mem + 8;
+ 
+ 		mem += 8 + length;
+ 		if (mem - start > nfp_cpp_area_size(area))
+ 			goto err_release_free;
+ 
+ 		switch (type) {
+ 		case NFP_BPF_CAP_TYPE_ADJUST_HEAD:
+ 			if (nfp_bpf_parse_cap_adjust_head(app->priv, value,
+ 							  length))
+ 				goto err_release_free;
+ 			break;
+ 		default:
+ 			nfp_dbg(cpp, "unknown BPF capability: %d\n", type);
+ 			break;
+ 		}
+ 	}
+ 	if (mem - start != nfp_cpp_area_size(area)) {
+ 		nfp_err(cpp, "BPF capabilities left after parsing, parsed:%zd total length:%zu\n",
+ 			mem - start, nfp_cpp_area_size(area));
+ 		goto err_release_free;
+ 	}
+ 
+ 	nfp_cpp_area_release_free(area);
+ 
+ 	return 0;
+ 
+ err_release_free:
+ 	nfp_err(cpp, "invalid BPF capabilities at offset:%zd\n", mem - start);
+ 	nfp_cpp_area_release_free(area);
+ 	return -EINVAL;
+ }
+ 
+ static int nfp_bpf_init(struct nfp_app *app)
+ {
+ 	struct nfp_app_bpf *bpf;
+ 	int err;
+ 
+ 	bpf = kzalloc(sizeof(*bpf), GFP_KERNEL);
+ 	if (!bpf)
+ 		return -ENOMEM;
+ 	bpf->app = app;
+ 	app->priv = bpf;
+ 
+ 	skb_queue_head_init(&bpf->cmsg_replies);
+ 	init_waitqueue_head(&bpf->cmsg_wq);
+ 	INIT_LIST_HEAD(&bpf->map_list);
+ 
+ 	err = nfp_bpf_parse_capabilities(app);
+ 	if (err)
+ 		goto err_free_bpf;
+ 
+ 	return 0;
+ 
+ err_free_bpf:
+ 	kfree(bpf);
+ 	return err;
+ }
+ 
+ static void nfp_bpf_clean(struct nfp_app *app)
+ {
+ 	struct nfp_app_bpf *bpf = app->priv;
+ 
+ 	WARN_ON(!skb_queue_empty(&bpf->cmsg_replies));
+ 	WARN_ON(!list_empty(&bpf->map_list));
+ 	kfree(bpf);
+ }
+ 
++>>>>>>> d48ae231c5e1 (nfp: bpf: add basic control channel communication)
  const struct nfp_app_type app_bpf = {
  	.id		= NFP_APP_BPF_NIC,
  	.name		= "ebpf",
@@@ -152,7 -351,10 +291,9 @@@
  	.vnic_alloc	= nfp_bpf_vnic_alloc,
  	.vnic_free	= nfp_bpf_vnic_free,
  
+ 	.ctrl_msg_rx	= nfp_bpf_ctrl_msg_rx,
+ 
  	.setup_tc	= nfp_bpf_setup_tc,
  	.tc_busy	= nfp_bpf_tc_busy,
 -	.bpf		= nfp_ndo_bpf,
  	.xdp_offload	= nfp_bpf_xdp_offload,
  };
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.h
index 5212b54abaf7,e00a7de2a9c2..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.h
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.h
@@@ -37,21 -37,36 +37,25 @@@
  #include <linux/bitfield.h>
  #include <linux/bpf.h>
  #include <linux/bpf_verifier.h>
+ #include <linux/kernel.h>
  #include <linux/list.h>
+ #include <linux/skbuff.h>
  #include <linux/types.h>
+ #include <linux/wait.h>
  
  #include "../nfp_asm.h"
+ #include "fw.h"
  
 -/* For relocation logic use up-most byte of branch instruction as scratch
 +/* For branch fixup logic use up-most byte of branch instruction as scratch
   * area.  Remember to clear this before sending instructions to HW!
   */
 -#define OP_RELO_TYPE	0xff00000000000000ULL
 -
 -enum nfp_relo_type {
 -	RELO_NONE = 0,
 -	/* standard internal jumps */
 -	RELO_BR_REL,
 -	/* internal jumps to parts of the outro */
 -	RELO_BR_GO_OUT,
 -	RELO_BR_GO_ABORT,
 -	/* external jumps to fixed addresses */
 -	RELO_BR_NEXT_PKT,
 -};
 +#define OP_BR_SPECIAL	0xff00000000000000ULL
  
 -/* To make absolute relocated branches (branches other than RELO_BR_REL)
 - * distinguishable in user space dumps from normal jumps, add a large offset
 - * to them.
 - */
 -#define BR_OFF_RELO		15000
 +enum br_special {
 +	OP_BR_NORMAL = 0,
 +	OP_BR_GO_OUT,
 +	OP_BR_GO_ABORT,
 +};
  
  enum static_regs {
  	STATIC_REG_IMM		= 21, /* Bank AB */
@@@ -85,6 -93,61 +89,64 @@@ enum nfp_bpf_action_type 
  #define NFP_BPF_ABI_FLAGS	reg_imm(0)
  #define   NFP_BPF_ABI_FLAG_MARK	1
  
++<<<<<<< HEAD
++=======
+ /**
+  * struct nfp_app_bpf - bpf app priv structure
+  * @app:		backpointer to the app
+  *
+  * @tag_allocator:	bitmap of control message tags in use
+  * @tag_alloc_next:	next tag bit to allocate
+  * @tag_alloc_last:	next tag bit to be freed
+  *
+  * @cmsg_replies:	received cmsg replies waiting to be consumed
+  * @cmsg_wq:		work queue for waiting for cmsg replies
+  *
+  * @map_list:		list of offloaded maps
+  *
+  * @adjust_head:	adjust head capability
+  * @flags:		extra flags for adjust head
+  * @off_min:		minimal packet offset within buffer required
+  * @off_max:		maximum packet offset within buffer required
+  * @guaranteed_sub:	amount of negative adjustment guaranteed possible
+  * @guaranteed_add:	amount of positive adjustment guaranteed possible
+  */
+ struct nfp_app_bpf {
+ 	struct nfp_app *app;
+ 
+ 	DECLARE_BITMAP(tag_allocator, U16_MAX + 1);
+ 	u16 tag_alloc_next;
+ 	u16 tag_alloc_last;
+ 
+ 	struct sk_buff_head cmsg_replies;
+ 	struct wait_queue_head cmsg_wq;
+ 
+ 	struct list_head map_list;
+ 
+ 	struct nfp_bpf_cap_adjust_head {
+ 		u32 flags;
+ 		int off_min;
+ 		int off_max;
+ 		int guaranteed_sub;
+ 		int guaranteed_add;
+ 	} adjust_head;
+ };
+ 
+ /**
+  * struct nfp_bpf_map - private per-map data attached to BPF maps for offload
+  * @offmap:	pointer to the offloaded BPF map
+  * @bpf:	back pointer to bpf app private structure
+  * @tid:	table id identifying map on datapath
+  * @l:		link on the nfp_app_bpf->map_list list
+  */
+ struct nfp_bpf_map {
+ 	struct bpf_offloaded_map *offmap;
+ 	struct nfp_app_bpf *bpf;
+ 	u32 tid;
+ 	struct list_head l;
+ };
+ 
++>>>>>>> d48ae231c5e1 (nfp: bpf: add basic control channel communication)
  struct nfp_prog;
  struct nfp_insn_meta;
  typedef int (*instr_cb_t)(struct nfp_prog *, struct nfp_insn_meta *);
@@@ -183,37 -271,40 +245,46 @@@ struct nfp_prog 
  	struct list_head insns;
  };
  
 -/**
 - * struct nfp_bpf_vnic - per-vNIC BPF priv structure
 - * @tc_prog:	currently loaded cls_bpf program
 - * @start_off:	address of the first instruction in the memory
 - * @tgt_done:	jump target to get the next packet
 - */
 -struct nfp_bpf_vnic {
 -	struct bpf_prog *tc_prog;
 -	unsigned int start_off;
 -	unsigned int tgt_done;
 +struct nfp_bpf_result {
 +	unsigned int n_instr;
 +	bool dense_mode;
  };
  
 -void nfp_bpf_jit_prepare(struct nfp_prog *nfp_prog, unsigned int cnt);
 -int nfp_bpf_jit(struct nfp_prog *prog);
 +int
 +nfp_bpf_jit(struct bpf_prog *filter, void *prog, enum nfp_bpf_action_type act,
 +	    unsigned int prog_start, unsigned int prog_done,
 +	    unsigned int prog_sz, struct nfp_bpf_result *res);
  
 -extern const struct bpf_prog_offload_ops nfp_bpf_analyzer_ops;
 +int nfp_prog_verify(struct nfp_prog *nfp_prog, struct bpf_prog *prog);
  
 -struct netdev_bpf;
 -struct nfp_app;
  struct nfp_net;
 +struct tc_cls_bpf_offload;
  
 -int nfp_ndo_bpf(struct nfp_app *app, struct nfp_net *nn,
 -		struct netdev_bpf *bpf);
 -int nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,
 -			bool old_prog);
 +/**
 + * struct nfp_net_bpf_priv - per-vNIC BPF private data
 + * @rx_filter:		Filter offload statistics - dropped packets/bytes
 + * @rx_filter_prev:	Filter offload statistics - values from previous update
 + * @rx_filter_change:	Jiffies when statistics last changed
 + * @rx_filter_stats_timer:  Timer for polling filter offload statistics
 + * @rx_filter_lock:	Lock protecting timer state changes (teardown)
 + */
 +struct nfp_net_bpf_priv {
 +	struct nfp_stat_pair rx_filter, rx_filter_prev;
 +	unsigned long rx_filter_change;
 +	struct timer_list rx_filter_stats_timer;
 +	spinlock_t rx_filter_lock;
 +};
  
 -struct nfp_insn_meta *
 -nfp_bpf_goto_meta(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,
 -		  unsigned int insn_idx, unsigned int n_insns);
 +int nfp_net_bpf_offload(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf);
 +void nfp_net_filter_stats_timer(unsigned long data);
  
++<<<<<<< HEAD
++=======
+ void *nfp_bpf_relo_for_vnic(struct nfp_prog *nfp_prog, struct nfp_bpf_vnic *bv);
+ 
+ struct sk_buff *
+ nfp_bpf_cmsg_communicate(struct nfp_app_bpf *bpf, struct sk_buff *skb,
+ 			 enum nfp_bpf_cmsg_type type, unsigned int reply_size);
+ void nfp_bpf_ctrl_msg_rx(struct nfp_app *app, struct sk_buff *skb);
++>>>>>>> d48ae231c5e1 (nfp: bpf: add basic control channel communication)
  #endif
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/fw.h
diff --git a/drivers/net/ethernet/netronome/nfp/Makefile b/drivers/net/ethernet/netronome/nfp/Makefile
index 4e99c659a4cb..c5287661b319 100644
--- a/drivers/net/ethernet/netronome/nfp/Makefile
+++ b/drivers/net/ethernet/netronome/nfp/Makefile
@@ -42,6 +42,7 @@ endif
 
 ifeq ($(CONFIG_BPF_SYSCALL),y)
 nfp-objs += \
+	    bpf/cmsg.o \
 	    bpf/main.o \
 	    bpf/offload.o \
 	    bpf/verifier.o \
diff --git a/drivers/net/ethernet/netronome/nfp/bpf/cmsg.c b/drivers/net/ethernet/netronome/nfp/bpf/cmsg.c
new file mode 100644
index 000000000000..46753ee9f7c5
--- /dev/null
+++ b/drivers/net/ethernet/netronome/nfp/bpf/cmsg.c
@@ -0,0 +1,238 @@
+/*
+ * Copyright (C) 2017 Netronome Systems, Inc.
+ *
+ * This software is dual licensed under the GNU General License Version 2,
+ * June 1991 as shown in the file COPYING in the top-level directory of this
+ * source tree or the BSD 2-Clause License provided below.  You have the
+ * option to license this software under the complete terms of either license.
+ *
+ * The BSD 2-Clause License:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      1. Redistributions of source code must retain the above
+ *         copyright notice, this list of conditions and the following
+ *         disclaimer.
+ *
+ *      2. Redistributions in binary form must reproduce the above
+ *         copyright notice, this list of conditions and the following
+ *         disclaimer in the documentation and/or other materials
+ *         provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/bitops.h>
+#include <linux/bug.h>
+#include <linux/jiffies.h>
+#include <linux/skbuff.h>
+#include <linux/wait.h>
+
+#include "../nfp_app.h"
+#include "../nfp_net.h"
+#include "fw.h"
+#include "main.h"
+
+#define cmsg_warn(bpf, msg...)	nn_dp_warn(&(bpf)->app->ctrl->dp, msg)
+
+#define NFP_BPF_TAG_ALLOC_SPAN	(U16_MAX / 4)
+
+static bool nfp_bpf_all_tags_busy(struct nfp_app_bpf *bpf)
+{
+	u16 used_tags;
+
+	used_tags = bpf->tag_alloc_next - bpf->tag_alloc_last;
+
+	return used_tags > NFP_BPF_TAG_ALLOC_SPAN;
+}
+
+static int nfp_bpf_alloc_tag(struct nfp_app_bpf *bpf)
+{
+	/* All FW communication for BPF is request-reply.  To make sure we
+	 * don't reuse the message ID too early after timeout - limit the
+	 * number of requests in flight.
+	 */
+	if (nfp_bpf_all_tags_busy(bpf)) {
+		cmsg_warn(bpf, "all FW request contexts busy!\n");
+		return -EAGAIN;
+	}
+
+	WARN_ON(__test_and_set_bit(bpf->tag_alloc_next, bpf->tag_allocator));
+	return bpf->tag_alloc_next++;
+}
+
+static void nfp_bpf_free_tag(struct nfp_app_bpf *bpf, u16 tag)
+{
+	WARN_ON(!__test_and_clear_bit(tag, bpf->tag_allocator));
+
+	while (!test_bit(bpf->tag_alloc_last, bpf->tag_allocator) &&
+	       bpf->tag_alloc_last != bpf->tag_alloc_next)
+		bpf->tag_alloc_last++;
+}
+
+static unsigned int nfp_bpf_cmsg_get_tag(struct sk_buff *skb)
+{
+	struct cmsg_hdr *hdr;
+
+	hdr = (struct cmsg_hdr *)skb->data;
+
+	return be16_to_cpu(hdr->tag);
+}
+
+static struct sk_buff *__nfp_bpf_reply(struct nfp_app_bpf *bpf, u16 tag)
+{
+	unsigned int msg_tag;
+	struct sk_buff *skb;
+
+	skb_queue_walk(&bpf->cmsg_replies, skb) {
+		msg_tag = nfp_bpf_cmsg_get_tag(skb);
+		if (msg_tag == tag) {
+			nfp_bpf_free_tag(bpf, tag);
+			__skb_unlink(skb, &bpf->cmsg_replies);
+			return skb;
+		}
+	}
+
+	return NULL;
+}
+
+static struct sk_buff *nfp_bpf_reply(struct nfp_app_bpf *bpf, u16 tag)
+{
+	struct sk_buff *skb;
+
+	nfp_ctrl_lock(bpf->app->ctrl);
+	skb = __nfp_bpf_reply(bpf, tag);
+	nfp_ctrl_unlock(bpf->app->ctrl);
+
+	return skb;
+}
+
+static struct sk_buff *nfp_bpf_reply_drop_tag(struct nfp_app_bpf *bpf, u16 tag)
+{
+	struct sk_buff *skb;
+
+	nfp_ctrl_lock(bpf->app->ctrl);
+	skb = __nfp_bpf_reply(bpf, tag);
+	if (!skb)
+		nfp_bpf_free_tag(bpf, tag);
+	nfp_ctrl_unlock(bpf->app->ctrl);
+
+	return skb;
+}
+
+static struct sk_buff *
+nfp_bpf_cmsg_wait_reply(struct nfp_app_bpf *bpf, enum nfp_bpf_cmsg_type type,
+			int tag)
+{
+	struct sk_buff *skb;
+	int err;
+
+	err = wait_event_interruptible_timeout(bpf->cmsg_wq,
+					       skb = nfp_bpf_reply(bpf, tag),
+					       msecs_to_jiffies(5000));
+	/* We didn't get a response - try last time and atomically drop
+	 * the tag even if no response is matched.
+	 */
+	if (!skb)
+		skb = nfp_bpf_reply_drop_tag(bpf, tag);
+	if (err < 0) {
+		cmsg_warn(bpf, "%s waiting for response to 0x%02x: %d\n",
+			  err == ERESTARTSYS ? "interrupted" : "error",
+			  type, err);
+		return ERR_PTR(err);
+	}
+	if (!skb) {
+		cmsg_warn(bpf, "timeout waiting for response to 0x%02x\n",
+			  type);
+		return ERR_PTR(-ETIMEDOUT);
+	}
+
+	return skb;
+}
+
+struct sk_buff *
+nfp_bpf_cmsg_communicate(struct nfp_app_bpf *bpf, struct sk_buff *skb,
+			 enum nfp_bpf_cmsg_type type, unsigned int reply_size)
+{
+	struct cmsg_hdr *hdr;
+	int tag;
+
+	nfp_ctrl_lock(bpf->app->ctrl);
+	tag = nfp_bpf_alloc_tag(bpf);
+	if (tag < 0) {
+		nfp_ctrl_unlock(bpf->app->ctrl);
+		dev_kfree_skb_any(skb);
+		return ERR_PTR(tag);
+	}
+
+	hdr = (void *)skb->data;
+	hdr->ver = CMSG_MAP_ABI_VERSION;
+	hdr->type = type;
+	hdr->tag = cpu_to_be16(tag);
+
+	__nfp_app_ctrl_tx(bpf->app, skb);
+
+	nfp_ctrl_unlock(bpf->app->ctrl);
+
+	skb = nfp_bpf_cmsg_wait_reply(bpf, type, tag);
+	if (IS_ERR(skb))
+		return skb;
+
+	hdr = (struct cmsg_hdr *)skb->data;
+	/* 0 reply_size means caller will do the validation */
+	if (reply_size && skb->len != reply_size) {
+		cmsg_warn(bpf, "cmsg drop - wrong size %d != %d!\n",
+			  skb->len, reply_size);
+		goto err_free;
+	}
+	if (hdr->type != __CMSG_REPLY(type)) {
+		cmsg_warn(bpf, "cmsg drop - wrong type 0x%02x != 0x%02lx!\n",
+			  hdr->type, __CMSG_REPLY(type));
+		goto err_free;
+	}
+
+	return skb;
+err_free:
+	dev_kfree_skb_any(skb);
+	return ERR_PTR(-EIO);
+}
+
+void nfp_bpf_ctrl_msg_rx(struct nfp_app *app, struct sk_buff *skb)
+{
+	struct nfp_app_bpf *bpf = app->priv;
+	unsigned int tag;
+
+	if (unlikely(skb->len < sizeof(struct cmsg_reply_map_simple))) {
+		cmsg_warn(bpf, "cmsg drop - too short %d!\n", skb->len);
+		goto err_free;
+	}
+
+	nfp_ctrl_lock(bpf->app->ctrl);
+
+	tag = nfp_bpf_cmsg_get_tag(skb);
+	if (unlikely(!test_bit(tag, bpf->tag_allocator))) {
+		cmsg_warn(bpf, "cmsg drop - no one is waiting for tag %u!\n",
+			  tag);
+		goto err_unlock;
+	}
+
+	__skb_queue_tail(&bpf->cmsg_replies, skb);
+	wake_up_interruptible_all(&bpf->cmsg_wq);
+
+	nfp_ctrl_unlock(bpf->app->ctrl);
+
+	return;
+err_unlock:
+	nfp_ctrl_unlock(bpf->app->ctrl);
+err_free:
+	dev_kfree_skb_any(skb);
+}
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/fw.h
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.c
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.h
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_app.h b/drivers/net/ethernet/netronome/nfp/nfp_app.h
index b6035aad75b0..680505012436 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_app.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_app.h
@@ -149,6 +149,7 @@ struct nfp_app {
 	void *priv;
 };
 
+bool __nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb);
 bool nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb);
 
 static inline int nfp_app_init(struct nfp_app *app)
@@ -273,6 +274,14 @@ static inline int nfp_app_xdp_offload(struct nfp_app *app, struct nfp_net *nn,
 	return app->type->xdp_offload(app, nn, prog);
 }
 
+static inline bool __nfp_app_ctrl_tx(struct nfp_app *app, struct sk_buff *skb)
+{
+	trace_devlink_hwmsg(priv_to_devlink(app->pf), false, 0,
+			    skb->data, skb->len);
+
+	return __nfp_ctrl_tx(app->ctrl, skb);
+}
+
 static inline bool nfp_app_ctrl_tx(struct nfp_app *app, struct sk_buff *skb)
 {
 	trace_devlink_hwmsg(priv_to_devlink(app->pf), false, 0,
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net.h b/drivers/net/ethernet/netronome/nfp/nfp_net.h
index 8d7985151594..b16252e46376 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net.h
@@ -839,6 +839,18 @@ static inline const char *nfp_net_name(struct nfp_net *nn)
 	return nn->dp.netdev ? nn->dp.netdev->name : "ctrl";
 }
 
+static inline void nfp_ctrl_lock(struct nfp_net *nn)
+	__acquires(&nn->r_vecs[0].lock)
+{
+	spin_lock_bh(&nn->r_vecs[0].lock);
+}
+
+static inline void nfp_ctrl_unlock(struct nfp_net *nn)
+	__releases(&nn->r_vecs[0].lock)
+{
+	spin_unlock_bh(&nn->r_vecs[0].lock);
+}
+
 /* Globals */
 extern const char nfp_driver_version[];
 
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 51e5c398d8b2..5456ffeec121 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@ -1941,6 +1941,13 @@ err_free:
 	return false;
 }
 
+bool __nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb)
+{
+	struct nfp_net_r_vector *r_vec = &nn->r_vecs[0];
+
+	return nfp_ctrl_tx_one(nn, r_vec, skb, false);
+}
+
 bool nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb)
 {
 	struct nfp_net_r_vector *r_vec = &nn->r_vecs[0];
