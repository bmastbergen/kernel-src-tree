s390: move expoline assembler macros to a header

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [s390] move expoline assembler macros to a header (Hendrik Brueckner) [1583564]
Rebuild_FUZZ: 93.33%
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit 6dd85fbb87d1d6b87a3b1f02ca28d7b2abd2e7ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/6dd85fbb.failed

To be able to use the expoline branches in different assembler
files move the associated macros from entry.S to a new header
nospec-insn.h.

While we are at it make the macros a bit nicer to use.

	Cc: stable@vger.kernel.org # 4.16
Fixes: f19fbd5ed6 ("s390: introduce execute-trampolines for branches")
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 6dd85fbb87d1d6b87a3b1f02ca28d7b2abd2e7ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/kernel/entry.S
diff --cc arch/s390/kernel/entry.S
index be8edbeb24eb,f03402efab4b..000000000000
--- a/arch/s390/kernel/entry.S
+++ b/arch/s390/kernel/entry.S
@@@ -18,36 -23,45 +18,45 @@@
  #include <asm/unistd.h>
  #include <asm/page.h>
  #include <asm/sigp.h>
++<<<<<<< HEAD
++=======
+ #include <asm/irq.h>
+ #include <asm/vx-insn.h>
+ #include <asm/setup.h>
+ #include <asm/nmi.h>
+ #include <asm/export.h>
+ #include <asm/nospec-insn.h>
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
  __PT_R0      =	__PT_GPRS
 -__PT_R1      =	__PT_GPRS + 8
 -__PT_R2      =	__PT_GPRS + 16
 -__PT_R3      =	__PT_GPRS + 24
 -__PT_R4      =	__PT_GPRS + 32
 -__PT_R5      =	__PT_GPRS + 40
 -__PT_R6      =	__PT_GPRS + 48
 -__PT_R7      =	__PT_GPRS + 56
 -__PT_R8      =	__PT_GPRS + 64
 -__PT_R9      =	__PT_GPRS + 72
 -__PT_R10     =	__PT_GPRS + 80
 -__PT_R11     =	__PT_GPRS + 88
 -__PT_R12     =	__PT_GPRS + 96
 -__PT_R13     =	__PT_GPRS + 104
 -__PT_R14     =	__PT_GPRS + 112
 -__PT_R15     =	__PT_GPRS + 120
 -
 -STACK_SHIFT = PAGE_SHIFT + THREAD_SIZE_ORDER
 +__PT_R1      =	__PT_GPRS + 4
 +__PT_R2      =	__PT_GPRS + 8
 +__PT_R3      =	__PT_GPRS + 12
 +__PT_R4      =	__PT_GPRS + 16
 +__PT_R5      =	__PT_GPRS + 20
 +__PT_R6      =	__PT_GPRS + 24
 +__PT_R7      =	__PT_GPRS + 28
 +__PT_R8      =	__PT_GPRS + 32
 +__PT_R9      =	__PT_GPRS + 36
 +__PT_R10     =	__PT_GPRS + 40
 +__PT_R11     =	__PT_GPRS + 44
 +__PT_R12     =	__PT_GPRS + 48
 +__PT_R13     =	__PT_GPRS + 524
 +__PT_R14     =	__PT_GPRS + 56
 +__PT_R15     =	__PT_GPRS + 60
 +
 +_TIF_WORK_SVC = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 +		 _TIF_MCCK_PENDING | _TIF_PER_TRAP | _TIF_ASCE)
 +_TIF_WORK_INT = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 +		 _TIF_MCCK_PENDING | _TIF_ASCE)
 +_TIF_TRACE    = (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | \
 +		 _TIF_SYSCALL_TRACEPOINT)
 +
 +STACK_SHIFT = PAGE_SHIFT + THREAD_ORDER
  STACK_SIZE  = 1 << STACK_SHIFT
 -STACK_INIT = STACK_SIZE - STACK_FRAME_OVERHEAD - __PT_SIZE
 +STACK_INIT  = STACK_SIZE - STACK_FRAME_OVERHEAD - __PT_SIZE
  
 -_TIF_WORK	= (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 -		   _TIF_UPROBE | _TIF_GUARDED_STORAGE | _TIF_PATCH_PENDING)
 -_TIF_TRACE	= (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | \
 -		   _TIF_SYSCALL_TRACEPOINT)
 -_CIF_WORK	= (_CIF_MCCK_PENDING | _CIF_ASCE_PRIMARY | \
 -		   _CIF_ASCE_SECONDARY | _CIF_FPU)
 -_PIF_WORK	= (_PIF_PER_TRAP | _PIF_SYSCALL_RESTART)
 -
 -_LPP_OFFSET	= __LC_LPP
 -
 -#define BASED(name) name-cleanup_critical(%r13)
 +#define BASED(name) name-system_call(%r13)
  
  	.macro	TRACE_IRQS_ON
  #ifdef CONFIG_TRACE_IRQFLAGS
@@@ -151,25 -170,40 +160,50 @@@
  	.endm
  
  	.macro BPON
++<<<<<<< HEAD
 +	.pushsection .altinstr_replacement, "ax"
 +662:	.long	0xb2e8d000
 +	.popsection
 +663:	.long	0x47000000
 +	.pushsection .altnobp, "a"
 +        .long 663b - .
 +	.long 662b - .
 +	.word 82
 +	.byte 4
 +	.byte 4
 +	.popsection
 +	.endm
 +
++=======
+ 	ALTERNATIVE "", ".long 0xb2e8d000", 82
+ 	.endm
+ 
+ 	.macro BPENTER tif_ptr,tif_mask
+ 	ALTERNATIVE "TSTMSK \tif_ptr,\tif_mask; jz .+8; .long 0xb2e8d000", \
+ 		    "", 82
+ 	.endm
+ 
+ 	.macro BPEXIT tif_ptr,tif_mask
+ 	TSTMSK	\tif_ptr,\tif_mask
+ 	ALTERNATIVE "jz .+8;  .long 0xb2e8c000", \
+ 		    "jnz .+8; .long 0xb2e8d000", 82
+ 	.endm
+ 
+ 	GEN_BR_THUNK %r9
+ 	GEN_BR_THUNK %r14
+ 	GEN_BR_THUNK %r14,%r11
+ 
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  	.section .kprobes.text, "ax"
 -.Ldummy:
 -	/*
 -	 * This nop exists only in order to avoid that __switch_to starts at
 -	 * the beginning of the kprobes text section. In that case we would
 -	 * have several symbols at the same address. E.g. objdump would take
 -	 * an arbitrary symbol name when disassembling this code.
 -	 * With the added nop in between the __switch_to symbol is unique
 -	 * again.
 -	 */
 -	nop	0
  
  ENTRY(__bpon)
  	.globl __bpon
  	BPON
++<<<<<<< HEAD
 +	br	%r14
++=======
+ 	BR_EX	%r14
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
  /*
   * Scheduler resume function, called by switch_to
@@@ -179,26 -213,101 +213,123 @@@
   *  gpr2 = prev
   */
  ENTRY(__switch_to)
++<<<<<<< HEAD
 +	stm	%r6,%r15,__SF_GPRS(%r15)	# store gprs of prev task
 +	st	%r15,__THREAD_ksp(%r2)		# store kernel stack of prev
 +	l	%r4,__THREAD_info(%r2)		# get thread_info of prev
 +	l	%r5,__THREAD_info(%r3)		# get thread_info of next
 +	lr	%r15,%r5
 +	ahi	%r15,STACK_INIT			# end of kernel stack of next
 +	st	%r3,__LC_CURRENT		# store task struct of next
 +	st	%r5,__LC_THREAD_INFO		# store thread info of next
 +	st	%r15,__LC_KERNEL_STACK		# store end of kernel stack
 +	lctl	%c4,%c4,__TASK_pid(%r3)		# load pid to control reg. 4
 +	mvc	__LC_CURRENT_PID(4,%r0),__TASK_pid(%r3)	# store pid of next
 +	l	%r15,__THREAD_ksp(%r3)		# load kernel stack of next
 +	tm	__TI_flags+3(%r4),_TIF_MCCK_PENDING # machine check pending?
 +	jz	0f
 +	ni	__TI_flags+3(%r4),255-_TIF_MCCK_PENDING	# clear flag in prev
 +	oi	__TI_flags+3(%r5),_TIF_MCCK_PENDING	# set it in next
 +0:	lm	%r6,%r15,__SF_GPRS(%r15)	# load gprs of next task
 +	br	%r14
++=======
+ 	stmg	%r6,%r15,__SF_GPRS(%r15)	# store gprs of prev task
+ 	lghi	%r4,__TASK_stack
+ 	lghi	%r1,__TASK_thread
+ 	lg	%r5,0(%r4,%r3)			# start of kernel stack of next
+ 	stg	%r15,__THREAD_ksp(%r1,%r2)	# store kernel stack of prev
+ 	lgr	%r15,%r5
+ 	aghi	%r15,STACK_INIT			# end of kernel stack of next
+ 	stg	%r3,__LC_CURRENT		# store task struct of next
+ 	stg	%r15,__LC_KERNEL_STACK		# store end of kernel stack
+ 	lg	%r15,__THREAD_ksp(%r1,%r3)	# load kernel stack of next
+ 	aghi	%r3,__TASK_pid
+ 	mvc	__LC_CURRENT_PID(4,%r0),0(%r3)	# store pid of next
+ 	lmg	%r6,%r15,__SF_GPRS(%r15)	# load gprs of next task
+ 	ALTERNATIVE "", ".insn s,0xb2800000,_LPP_OFFSET", 40
+ 	BR_EX	%r14
+ 
+ .L__critical_start:
+ 
+ #if IS_ENABLED(CONFIG_KVM)
+ /*
+  * sie64a calling convention:
+  * %r2 pointer to sie control block
+  * %r3 guest register save area
+  */
+ ENTRY(sie64a)
+ 	stmg	%r6,%r14,__SF_GPRS(%r15)	# save kernel registers
+ 	lg	%r12,__LC_CURRENT
+ 	stg	%r2,__SF_SIE_CONTROL(%r15)	# save control block pointer
+ 	stg	%r3,__SF_SIE_SAVEAREA(%r15)	# save guest register save area
+ 	xc	__SF_SIE_REASON(8,%r15),__SF_SIE_REASON(%r15) # reason code = 0
+ 	mvc	__SF_SIE_FLAGS(8,%r15),__TI_flags(%r12) # copy thread flags
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU		# load guest fp/vx registers ?
+ 	jno	.Lsie_load_guest_gprs
+ 	brasl	%r14,load_fpu_regs		# load guest fp/vx regs
+ .Lsie_load_guest_gprs:
+ 	lmg	%r0,%r13,0(%r3)			# load guest gprs 0-13
+ 	lg	%r14,__LC_GMAP			# get gmap pointer
+ 	ltgr	%r14,%r14
+ 	jz	.Lsie_gmap
+ 	lctlg	%c1,%c1,__GMAP_ASCE(%r14)	# load primary asce
+ .Lsie_gmap:
+ 	lg	%r14,__SF_SIE_CONTROL(%r15)	# get control block pointer
+ 	oi	__SIE_PROG0C+3(%r14),1		# we are going into SIE now
+ 	tm	__SIE_PROG20+3(%r14),3		# last exit...
+ 	jnz	.Lsie_skip
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU
+ 	jo	.Lsie_skip			# exit if fp/vx regs changed
+ 	BPEXIT	__SF_SIE_FLAGS(%r15),(_TIF_ISOLATE_BP|_TIF_ISOLATE_BP_GUEST)
+ .Lsie_entry:
+ 	sie	0(%r14)
+ .Lsie_exit:
+ 	BPOFF
+ 	BPENTER	__SF_SIE_FLAGS(%r15),(_TIF_ISOLATE_BP|_TIF_ISOLATE_BP_GUEST)
+ .Lsie_skip:
+ 	ni	__SIE_PROG0C+3(%r14),0xfe	# no longer in SIE
+ 	lctlg	%c1,%c1,__LC_USER_ASCE		# load primary asce
+ .Lsie_done:
+ # some program checks are suppressing. C code (e.g. do_protection_exception)
+ # will rewind the PSW by the ILC, which is often 4 bytes in case of SIE. There
+ # are some corner cases (e.g. runtime instrumentation) where ILC is unpredictable.
+ # Other instructions between sie64a and .Lsie_done should not cause program
+ # interrupts. So lets use 3 nops as a landing pad for all possible rewinds.
+ # See also .Lcleanup_sie
+ .Lrewind_pad6:
+ 	nopr	7
+ .Lrewind_pad4:
+ 	nopr	7
+ .Lrewind_pad2:
+ 	nopr	7
+ 	.globl sie_exit
+ sie_exit:
+ 	lg	%r14,__SF_SIE_SAVEAREA(%r15)	# load guest register save area
+ 	stmg	%r0,%r13,0(%r14)		# save guest gprs 0-13
+ 	xgr	%r0,%r0				# clear guest registers to
+ 	xgr	%r1,%r1				# prevent speculative use
+ 	xgr	%r2,%r2
+ 	xgr	%r3,%r3
+ 	xgr	%r4,%r4
+ 	xgr	%r5,%r5
+ 	lmg	%r6,%r14,__SF_GPRS(%r15)	# restore kernel registers
+ 	lg	%r2,__SF_SIE_REASON(%r15)	# return exit reason code
+ 	BR_EX	%r14
+ .Lsie_fault:
+ 	lghi	%r14,-EFAULT
+ 	stg	%r14,__SF_SIE_REASON(%r15)	# set exit reason code
+ 	j	sie_exit
+ 
+ 	EX_TABLE(.Lrewind_pad6,.Lsie_fault)
+ 	EX_TABLE(.Lrewind_pad4,.Lsie_fault)
+ 	EX_TABLE(.Lrewind_pad2,.Lsie_fault)
+ 	EX_TABLE(sie_exit,.Lsie_fault)
+ EXPORT_SYMBOL(sie64a)
+ EXPORT_SYMBOL(sie_exit)
+ #endif
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
 +__critical_start:
  /*
   * SVC interrupt handler routine. System calls are synchronous events and
   * are executed with interrupts enabled.
@@@ -206,57 -315,67 +337,70 @@@
  
  ENTRY(system_call)
  	stpt	__LC_SYNC_ENTER_TIMER
 -.Lsysc_stmg:
 -	stmg	%r8,%r15,__LC_SAVE_AREA_SYNC
 +sysc_stm:
 +	stm	%r8,%r15,__LC_SAVE_AREA_SYNC
  	BPOFF
 -	lg	%r12,__LC_CURRENT
 -	lghi	%r13,__TASK_thread
 -	lghi	%r14,_PIF_SYSCALL
 -.Lsysc_per:
 -	lg	%r15,__LC_KERNEL_STACK
 +	l	%r12,__LC_THREAD_INFO
 +	l	%r13,__LC_SVC_NEW_PSW+4
 +sysc_per:
 +	l	%r15,__LC_KERNEL_STACK
  	la	%r11,STACK_FRAME_OVERHEAD(%r15)	# pointer to pt_regs
 -.Lsysc_vtime:
 +sysc_vtime:
  	UPDATE_VTIME %r8,%r9,__LC_SYNC_ENTER_TIMER
 -	BPENTER __TI_flags(%r12),_TIF_ISOLATE_BP
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	mvc	__PT_R8(64,%r11),__LC_SAVE_AREA_SYNC
 -	mvc	__PT_PSW(16,%r11),__LC_SVC_OLD_PSW
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_SAVE_AREA_SYNC
 +	mvc	__PT_PSW(8,%r11),__LC_SVC_OLD_PSW
  	mvc	__PT_INT_CODE(4,%r11),__LC_SVC_ILC
 -	stg	%r14,__PT_FLAGS(%r11)
 -.Lsysc_do_svc:
 -	# clear user controlled register to prevent speculative use
 -	xgr	%r0,%r0
 -	# load address of system call table
 -	lg	%r10,__THREAD_sysc_table(%r13,%r12)
 -	llgh	%r8,__PT_INT_CODE+2(%r11)
 -	slag	%r8,%r8,2			# shift and test for svc 0
 -	jnz	.Lsysc_nr_ok
 +sysc_do_svc:
 +	oi	__TI_flags+2(%r12),_TIF_SYSCALL>>8
 +	l	%r10,__TI_sysc_table(%r12)	# 31 bit system call table
 +	lh	%r8,__PT_INT_CODE+2(%r11)
 +	sla	%r8,2				# shift and test for svc0
 +	jnz	sysc_nr_ok
  	# svc 0: system call number in %r1
 -	llgfr	%r1,%r1				# clear high word in r1
 -	cghi	%r1,NR_syscalls
 -	jnl	.Lsysc_nr_ok
 +	cl	%r1,BASED(.Lnr_syscalls)
 +	jnl	sysc_nr_ok
  	sth	%r1,__PT_INT_CODE+2(%r11)
++<<<<<<< HEAD
 +	lr	%r8,%r1
 +	sla	%r8,2
 +sysc_nr_ok:
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	st	%r2,__PT_ORIG_GPR2(%r11)
 +	st	%r7,STACK_FRAME_OVERHEAD(%r15)
 +	l	%r9,0(%r8,%r10)			# get system call addr.
 +	tm	__TI_flags+2(%r12),_TIF_TRACE >> 8
 +	jnz	sysc_tracesys
 +	basr	%r14,%r9			# call sys_xxxx
 +	st	%r2,__PT_R2(%r11)		# store return value
++=======
+ 	slag	%r8,%r1,2
+ .Lsysc_nr_ok:
+ 	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
+ 	stg	%r2,__PT_ORIG_GPR2(%r11)
+ 	stg	%r7,STACK_FRAME_OVERHEAD(%r15)
+ 	lgf	%r9,0(%r8,%r10)			# get system call add.
+ 	TSTMSK	__TI_flags(%r12),_TIF_TRACE
+ 	jnz	.Lsysc_tracesys
+ 	BASR_EX	%r14,%r9			# call sys_xxxx
+ 	stg	%r2,__PT_R2(%r11)		# store return value
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
 -.Lsysc_return:
 +sysc_return:
  	LOCKDEP_SYS_EXIT
 -.Lsysc_tif:
 -	TSTMSK	__PT_FLAGS(%r11),_PIF_WORK
 -	jnz	.Lsysc_work
 -	TSTMSK	__TI_flags(%r12),_TIF_WORK
 -	jnz	.Lsysc_work			# check for work
 -	TSTMSK	__LC_CPU_FLAGS,_CIF_WORK
 -	jnz	.Lsysc_work
 -	BPEXIT	__TI_flags(%r12),_TIF_ISOLATE_BP
 -.Lsysc_restore:
 -	lg	%r14,__LC_VDSO_PER_CPU
 -	lmg	%r0,%r10,__PT_R0(%r11)
 -	mvc	__LC_RETURN_PSW(16),__PT_PSW(%r11)
 -.Lsysc_exit_timer:
 +sysc_tif:
 +	tm	__PT_PSW+1(%r11),0x01		# returning to user ?
 +	jno	sysc_restore
 +	tm	__TI_flags+3(%r12),_TIF_WORK_SVC
 +	jnz	sysc_work			# check for work
 +	ni	__TI_flags+2(%r12),255-_TIF_SYSCALL>>8
 +	BPON
 +sysc_restore:
 +	mvc	__LC_RETURN_PSW(8),__PT_PSW(%r11)
  	stpt	__LC_EXIT_TIMER
 -	mvc	__VDSO_ECTG_BASE(16,%r14),__LC_EXIT_TIMER
 -	lmg	%r11,%r15,__PT_R11(%r11)
 -	lpswe	__LC_RETURN_PSW
 -.Lsysc_done:
 +	lm	%r0,%r15,__PT_R0(%r11)
 +	lpsw	__LC_RETURN_PSW
 +sysc_done:
  
  #
  # One of the work bits is on. Find out which one.
@@@ -341,32 -523,29 +485,55 @@@ sysc_singlestep
  # call tracehook_report_syscall_entry/tracehook_report_syscall_exit before
  # and after the system call
  #
 -.Lsysc_tracesys:
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 +sysc_tracesys:
 +	l	%r1,BASED(.Ltrace_enter)
 +	lr	%r2,%r11		# pass pointer to pt_regs
  	la	%r3,0
++<<<<<<< HEAD
 +	xr	%r0,%r0
 +	icm	%r0,3,__PT_INT_CODE+2(%r11)
 +	st	%r0,__PT_R2(%r11)
 +	basr	%r14,%r1		# call do_syscall_trace_enter
 +	cl	%r2,BASED(.Lnr_syscalls)
 +	jnl	sysc_tracenogo
 +	lr	%r8,%r2
 +	sll	%r8,2
 +	l	%r9,0(%r8,%r10)
 +sysc_tracego:
 +	lm	%r3,%r7,__PT_R3(%r11)
 +	st	%r7,STACK_FRAME_OVERHEAD(%r15)
 +	l	%r2,__PT_ORIG_GPR2(%r11)
 +	basr	%r14,%r9		# call sys_xxx
 +	st	%r2,__PT_R2(%r11)	# store return value
 +sysc_tracenogo:
 +	tm	__TI_flags+2(%r12),_TIF_TRACE >> 8
 +	jz	sysc_return
 +	l	%r1,BASED(.Ltrace_exit)
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	la	%r14,BASED(sysc_return)
 +	br	%r1			# call do_syscall_trace_exit
++=======
+ 	llgh	%r0,__PT_INT_CODE+2(%r11)
+ 	stg	%r0,__PT_R2(%r11)
+ 	brasl	%r14,do_syscall_trace_enter
+ 	lghi	%r0,NR_syscalls
+ 	clgr	%r0,%r2
+ 	jnh	.Lsysc_tracenogo
+ 	sllg	%r8,%r2,2
+ 	lgf	%r9,0(%r8,%r10)
+ .Lsysc_tracego:
+ 	lmg	%r3,%r7,__PT_R3(%r11)
+ 	stg	%r7,STACK_FRAME_OVERHEAD(%r15)
+ 	lg	%r2,__PT_ORIG_GPR2(%r11)
+ 	BASR_EX	%r14,%r9		# call sys_xxx
+ 	stg	%r2,__PT_R2(%r11)	# store return value
+ .Lsysc_tracenogo:
+ 	TSTMSK	__TI_flags(%r12),_TIF_TRACE
+ 	jz	.Lsysc_return
+ 	lgr	%r2,%r11		# pass pointer to pt_regs
+ 	larl	%r14,.Lsysc_return
+ 	jg	do_syscall_trace_exit
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
  #
  # a new process exits the kernel with ret_from_fork
@@@ -380,13 -557,13 +547,18 @@@ ENTRY(ret_from_fork
  	TRACE_IRQS_ON
  	ssm	__LC_SVC_NEW_PSW	# reenable interrupts
  	tm	__PT_PSW+1(%r11),0x01	# forking a kernel thread ?
 -	jne	.Lsysc_tracenogo
 +	jne	sysc_tracenogo
  	# it's a kernel thread
 -	lmg	%r9,%r10,__PT_R9(%r11)	# load gprs
 +	lm	%r9,%r10,__PT_R9(%r11)	# load gprs
  ENTRY(kernel_thread_starter)
  	la	%r2,0(%r10)
++<<<<<<< HEAD
 +	basr	%r14,%r9
 +	j	sysc_tracenogo
++=======
+ 	BASR_EX	%r14,%r9
+ 	j	.Lsysc_tracenogo
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
  /*
   * Program check handler routine
@@@ -395,51 -572,86 +567,59 @@@
  ENTRY(pgm_check_handler)
  	stpt	__LC_SYNC_ENTER_TIMER
  	BPOFF
 -	stmg	%r8,%r15,__LC_SAVE_AREA_SYNC
 -	lg	%r10,__LC_LAST_BREAK
 -	lg	%r12,__LC_CURRENT
 -	lghi	%r11,0
 -	larl	%r13,cleanup_critical
 -	lmg	%r8,%r9,__LC_PGM_OLD_PSW
 -	tmhh	%r8,0x0001		# test problem state bit
 -	jnz	2f			# -> fault in user space
 -#if IS_ENABLED(CONFIG_KVM)
 -	# cleanup critical section for program checks in sie64a
 -	lgr	%r14,%r9
 -	slg	%r14,BASED(.Lsie_critical_start)
 -	clg	%r14,BASED(.Lsie_critical_length)
 -	jhe	0f
 -	lg	%r14,__SF_SIE_CONTROL(%r15)	# get control block pointer
 -	ni	__SIE_PROG0C+3(%r14),0xfe	# no longer in SIE
 -	lctlg	%c1,%c1,__LC_USER_ASCE		# load primary asce
 -	larl	%r9,sie_exit			# skip forward to sie_exit
 -	lghi	%r11,_PIF_GUEST_FAULT
 -#endif
 -0:	tmhh	%r8,0x4000		# PER bit set in old PSW ?
 -	jnz	1f			# -> enabled, can't be a double fault
 +	stm	%r8,%r15,__LC_SAVE_AREA_SYNC
 +	l	%r12,__LC_THREAD_INFO
 +	l	%r13,__LC_SVC_NEW_PSW+4
 +	lm	%r8,%r9,__LC_PGM_OLD_PSW
 +	tmh	%r8,0x0001		# test problem state bit
 +	jnz	1f			# -> fault in user space
 +	tmh	%r8,0x4000		# PER bit set in old PSW ?
 +	jnz	0f			# -> enabled, can't be a double fault
  	tm	__LC_PGM_ILC+3,0x80	# check for per exception
 -	jnz	.Lpgm_svcper		# -> single stepped svc
 -1:	CHECK_STACK STACK_SIZE,__LC_SAVE_AREA_SYNC
 -	aghi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
 -	j	4f
 -2:	UPDATE_VTIME %r14,%r15,__LC_SYNC_ENTER_TIMER
 -	BPENTER __TI_flags(%r12),_TIF_ISOLATE_BP
 -	lg	%r15,__LC_KERNEL_STACK
 -	lgr	%r14,%r12
 -	aghi	%r14,__TASK_thread	# pointer to thread_struct
 -	lghi	%r13,__LC_PGM_TDB
 -	tm	__LC_PGM_ILC+2,0x02	# check for transaction abort
 -	jz	3f
 -	mvc	__THREAD_trap_tdb(256,%r14),0(%r13)
 -3:	stg	%r10,__THREAD_last_break(%r14)
 -4:	lgr	%r13,%r11
 -	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	# clear user controlled registers to prevent speculative use
 -	xgr	%r0,%r0
 -	xgr	%r1,%r1
 -	xgr	%r2,%r2
 -	xgr	%r3,%r3
 -	xgr	%r4,%r4
 -	xgr	%r5,%r5
 -	xgr	%r6,%r6
 -	xgr	%r7,%r7
 -	mvc	__PT_R8(64,%r11),__LC_SAVE_AREA_SYNC
 -	stmg	%r8,%r9,__PT_PSW(%r11)
 +	jnz	pgm_svcper		# -> single stepped svc
 +0:	CHECK_STACK STACK_SIZE,__LC_SAVE_AREA_SYNC
 +	ahi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
 +	j	2f
 +1:	UPDATE_VTIME %r14,%r15,__LC_SYNC_ENTER_TIMER
 +	l	%r15,__LC_KERNEL_STACK
 +2:	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_SAVE_AREA_SYNC
 +	stm	%r8,%r9,__PT_PSW(%r11)
  	mvc	__PT_INT_CODE(4,%r11),__LC_PGM_ILC
 -	mvc	__PT_INT_PARM_LONG(8,%r11),__LC_TRANS_EXC_CODE
 -	stg	%r13,__PT_FLAGS(%r11)
 -	stg	%r10,__PT_ARGS(%r11)
 +	mvc	__PT_INT_PARM_LONG(4,%r11),__LC_TRANS_EXC_CODE
  	tm	__LC_PGM_ILC+3,0x80	# check for per exception
 -	jz	5f
 -	tmhh	%r8,0x0001		# kernel per event ?
 -	jz	.Lpgm_kprobe
 -	oi	__PT_FLAGS+7(%r11),_PIF_PER_TRAP
 -	mvc	__THREAD_per_address(8,%r14),__LC_PER_ADDRESS
 -	mvc	__THREAD_per_cause(2,%r14),__LC_PER_CODE
 -	mvc	__THREAD_per_paid(1,%r14),__LC_PER_ACCESS_ID
 -5:	REENABLE_IRQS
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	larl	%r1,pgm_check_table
 -	llgh	%r10,__PT_INT_CODE+2(%r11)
 -	nill	%r10,0x007f
 +	jz	0f
 +	l	%r1,__TI_task(%r12)
 +	tmh	%r8,0x0001		# kernel per event ?
 +	jz	pgm_kprobe
 +	oi	__TI_flags+3(%r12),_TIF_PER_TRAP
 +	mvc	__THREAD_per_address(4,%r1),__LC_PER_ADDRESS
 +	mvc	__THREAD_per_cause(2,%r1),__LC_PER_CAUSE
 +	mvc	__THREAD_per_paid(1,%r1),__LC_PER_PAID
 +0:	REENABLE_IRQS
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	l	%r1,BASED(.Ljump_table)
 +	la	%r10,0x7f
 +	n	%r10,__PT_INT_CODE(%r11)
 +	je	pgm_exit
  	sll	%r10,2
++<<<<<<< HEAD
 +	l	%r1,0(%r10,%r1)		# load address of handler routine
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	basr	%r14,%r1		# branch to interrupt-handler
 +pgm_exit:
++=======
+ 	je	.Lpgm_return
+ 	lgf	%r9,0(%r10,%r1)		# load address of handler routine
+ 	lgr	%r2,%r11		# pass pointer to pt_regs
+ 	BASR_EX	%r14,%r9		# branch to interrupt-handler
+ .Lpgm_return:
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  	LOCKDEP_SYS_EXIT
  	tm	__PT_PSW+1(%r11),0x01	# returning to user ?
 -	jno	.Lsysc_restore
 -	TSTMSK	__PT_FLAGS(%r11),_PIF_SYSCALL
 -	jo	.Lsysc_do_syscall
 -	j	.Lsysc_tif
 +	jno	sysc_restore
 +	j	sysc_tif
  
  #
  # PER event in supervisor state, must be kprobes
@@@ -623,49 -905,155 +803,146 @@@ io_notify_resume
  /*
   * External interrupt handler routine
   */
 +
  ENTRY(ext_int_handler)
 -	STCK	__LC_INT_CLOCK
 +	stck	__LC_INT_CLOCK
  	stpt	__LC_ASYNC_ENTER_TIMER
  	BPOFF
 -	stmg	%r8,%r15,__LC_SAVE_AREA_ASYNC
 -	lg	%r12,__LC_CURRENT
 -	larl	%r13,cleanup_critical
 -	lmg	%r8,%r9,__LC_EXT_OLD_PSW
 -	SWITCH_ASYNC __LC_SAVE_AREA_ASYNC,__LC_ASYNC_ENTER_TIMER
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	# clear user controlled registers to prevent speculative use
 -	xgr	%r0,%r0
 -	xgr	%r1,%r1
 -	xgr	%r2,%r2
 -	xgr	%r3,%r3
 -	xgr	%r4,%r4
 -	xgr	%r5,%r5
 -	xgr	%r6,%r6
 -	xgr	%r7,%r7
 -	xgr	%r10,%r10
 -	mvc	__PT_R8(64,%r11),__LC_SAVE_AREA_ASYNC
 -	stmg	%r8,%r9,__PT_PSW(%r11)
 -	lghi	%r1,__LC_EXT_PARAMS2
 -	mvc	__PT_INT_CODE(4,%r11),__LC_EXT_CPU_ADDR
 -	mvc	__PT_INT_PARM(4,%r11),__LC_EXT_PARAMS
 -	mvc	__PT_INT_PARM_LONG(8,%r11),0(%r1)
 -	xc	__PT_FLAGS(8,%r11),__PT_FLAGS(%r11)
 -	TSTMSK	__LC_CPU_FLAGS,_CIF_IGNORE_IRQ
 -	jo	.Lio_restore
 +	stm	%r8,%r15,__LC_SAVE_AREA_ASYNC
 +	l	%r12,__LC_THREAD_INFO
 +	l	%r13,__LC_SVC_NEW_PSW+4
 +	lm	%r8,%r9,__LC_EXT_OLD_PSW
 +	tmh	%r8,0x0001		# interrupting from user ?
 +	jz	ext_skip
 +	UPDATE_VTIME %r14,%r15,__LC_ASYNC_ENTER_TIMER
 +ext_skip:
 +	SWITCH_ASYNC __LC_SAVE_AREA_ASYNC,__LC_ASYNC_STACK,STACK_SHIFT
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_SAVE_AREA_ASYNC
 +	stm	%r8,%r9,__PT_PSW(%r11)
  	TRACE_IRQS_OFF
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 -	lghi	%r3,EXT_INTERRUPT
 -	brasl	%r14,do_IRQ
 -	j	.Lio_return
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	l	%r3,__LC_EXT_CPU_ADDR	# get cpu address + interruption code
 +	l	%r4,__LC_EXT_PARAMS	# get external parameters
 +	l	%r1,BASED(.Ldo_extint)
 +	basr	%r14,%r1		# call do_extint
 +	j	io_return
  
  /*
 - * Load idle PSW. The second "half" of this function is in .Lcleanup_idle.
 + * Load idle PSW. The second "half" of this function is in cleanup_idle.
   */
  ENTRY(psw_idle)
 -	stg	%r3,__SF_EMPTY(%r15)
 -	larl	%r1,.Lpsw_idle_lpsw+4
 -	stg	%r1,__SF_EMPTY+8(%r15)
 -#ifdef CONFIG_SMP
 -	larl	%r1,smp_cpu_mtid
 -	llgf	%r1,0(%r1)
 -	ltgr	%r1,%r1
 -	jz	.Lpsw_idle_stcctm
 -	.insn	rsy,0xeb0000000017,%r1,5,__SF_EMPTY+16(%r15)
 -.Lpsw_idle_stcctm:
 -#endif
 -	oi	__LC_CPU_FLAGS+7,_CIF_ENABLED_WAIT
 +	st	%r3,__SF_EMPTY(%r15)
 +	basr	%r1,0
 +	la	%r1,psw_idle_lpsw+4-.(%r1)
 +	st	%r1,__SF_EMPTY+4(%r15)
 +	oi	__SF_EMPTY+4(%r15),0x80
  	BPON
 -	STCK	__CLOCK_IDLE_ENTER(%r2)
 +	stck	__CLOCK_IDLE_ENTER(%r2)
  	stpt	__TIMER_IDLE_ENTER(%r2)
++<<<<<<< HEAD
 +psw_idle_lpsw:
 +	lpsw	__SF_EMPTY(%r15)
 +	br	%r14
 +psw_idle_end:
 +
 +__critical_end:
++=======
+ .Lpsw_idle_lpsw:
+ 	lpswe	__SF_EMPTY(%r15)
+ 	BR_EX	%r14
+ .Lpsw_idle_end:
+ 
+ /*
+  * Store floating-point controls and floating-point or vector register
+  * depending whether the vector facility is available.	A critical section
+  * cleanup assures that the registers are stored even if interrupted for
+  * some other work.  The CIF_FPU flag is set to trigger a lazy restore
+  * of the register contents at return from io or a system call.
+  */
+ ENTRY(save_fpu_regs)
+ 	lg	%r2,__LC_CURRENT
+ 	aghi	%r2,__TASK_thread
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU
+ 	jo	.Lsave_fpu_regs_exit
+ 	stfpc	__THREAD_FPU_fpc(%r2)
+ 	lg	%r3,__THREAD_FPU_regs(%r2)
+ 	TSTMSK	__LC_MACHINE_FLAGS,MACHINE_FLAG_VX
+ 	jz	.Lsave_fpu_regs_fp	  # no -> store FP regs
+ 	VSTM	%v0,%v15,0,%r3		  # vstm 0,15,0(3)
+ 	VSTM	%v16,%v31,256,%r3	  # vstm 16,31,256(3)
+ 	j	.Lsave_fpu_regs_done	  # -> set CIF_FPU flag
+ .Lsave_fpu_regs_fp:
+ 	std	0,0(%r3)
+ 	std	1,8(%r3)
+ 	std	2,16(%r3)
+ 	std	3,24(%r3)
+ 	std	4,32(%r3)
+ 	std	5,40(%r3)
+ 	std	6,48(%r3)
+ 	std	7,56(%r3)
+ 	std	8,64(%r3)
+ 	std	9,72(%r3)
+ 	std	10,80(%r3)
+ 	std	11,88(%r3)
+ 	std	12,96(%r3)
+ 	std	13,104(%r3)
+ 	std	14,112(%r3)
+ 	std	15,120(%r3)
+ .Lsave_fpu_regs_done:
+ 	oi	__LC_CPU_FLAGS+7,_CIF_FPU
+ .Lsave_fpu_regs_exit:
+ 	BR_EX	%r14
+ .Lsave_fpu_regs_end:
+ EXPORT_SYMBOL(save_fpu_regs)
+ 
+ /*
+  * Load floating-point controls and floating-point or vector registers.
+  * A critical section cleanup assures that the register contents are
+  * loaded even if interrupted for some other work.
+  *
+  * There are special calling conventions to fit into sysc and io return work:
+  *	%r15:	<kernel stack>
+  * The function requires:
+  *	%r4
+  */
+ load_fpu_regs:
+ 	lg	%r4,__LC_CURRENT
+ 	aghi	%r4,__TASK_thread
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU
+ 	jno	.Lload_fpu_regs_exit
+ 	lfpc	__THREAD_FPU_fpc(%r4)
+ 	TSTMSK	__LC_MACHINE_FLAGS,MACHINE_FLAG_VX
+ 	lg	%r4,__THREAD_FPU_regs(%r4)	# %r4 <- reg save area
+ 	jz	.Lload_fpu_regs_fp		# -> no VX, load FP regs
+ 	VLM	%v0,%v15,0,%r4
+ 	VLM	%v16,%v31,256,%r4
+ 	j	.Lload_fpu_regs_done
+ .Lload_fpu_regs_fp:
+ 	ld	0,0(%r4)
+ 	ld	1,8(%r4)
+ 	ld	2,16(%r4)
+ 	ld	3,24(%r4)
+ 	ld	4,32(%r4)
+ 	ld	5,40(%r4)
+ 	ld	6,48(%r4)
+ 	ld	7,56(%r4)
+ 	ld	8,64(%r4)
+ 	ld	9,72(%r4)
+ 	ld	10,80(%r4)
+ 	ld	11,88(%r4)
+ 	ld	12,96(%r4)
+ 	ld	13,104(%r4)
+ 	ld	14,112(%r4)
+ 	ld	15,120(%r4)
+ .Lload_fpu_regs_done:
+ 	ni	__LC_CPU_FLAGS+7,255-_CIF_FPU
+ .Lload_fpu_regs_exit:
+ 	BR_EX	%r14
+ .Lload_fpu_regs_end:
+ 
+ .L__critical_end:
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
  /*
   * Machine check handler routines
@@@ -780,141 -1219,199 +1057,258 @@@ ENTRY(restart_int_handler
   * Setup a pt_regs so that show_trace can provide a good call trace.
   */
  stack_overflow:
 -	lg	%r15,__LC_PANIC_STACK	# change to panic stack
 +	l	%r15,__LC_PANIC_STACK	# change to panic stack
  	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	stmg	%r8,%r9,__PT_PSW(%r11)
 -	mvc	__PT_R8(64,%r11),0(%r14)
 -	stg	%r10,__PT_ORIG_GPR2(%r11) # store last break to orig_gpr2
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 -	jg	kernel_stack_overflow
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	stm	%r8,%r9,__PT_PSW(%r11)
 +	mvc	__PT_R8(32,%r11),0(%r14)
 +	l	%r1,BASED(1f)
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	br	%r1			# branch to kernel_stack_overflow
 +1:	.long	kernel_stack_overflow
  #endif
  
 +cleanup_table:
 +	.long	system_call + 0x80000000
 +	.long	sysc_do_svc + 0x80000000
 +	.long	sysc_tif + 0x80000000
 +	.long	sysc_restore + 0x80000000
 +	.long	sysc_done + 0x80000000
 +	.long	io_tif + 0x80000000
 +	.long	io_restore + 0x80000000
 +	.long	io_done + 0x80000000
 +	.long	psw_idle + 0x80000000
 +	.long	psw_idle_end + 0x80000000
 +
  cleanup_critical:
 -#if IS_ENABLED(CONFIG_KVM)
 -	clg	%r9,BASED(.Lcleanup_table_sie)	# .Lsie_gmap
 +	cl	%r9,BASED(cleanup_table)	# system_call
  	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table_sie+8)# .Lsie_done
 -	jl	.Lcleanup_sie
 -#endif
 -	clg	%r9,BASED(.Lcleanup_table)	# system_call
 +	cl	%r9,BASED(cleanup_table+4)	# sysc_do_svc
 +	jl	cleanup_system_call
 +	cl	%r9,BASED(cleanup_table+8)	# sysc_tif
  	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+8)	# .Lsysc_do_svc
 -	jl	.Lcleanup_system_call
 -	clg	%r9,BASED(.Lcleanup_table+16)	# .Lsysc_tif
 +	cl	%r9,BASED(cleanup_table+12)	# sysc_restore
 +	jl	cleanup_sysc_tif
 +	cl	%r9,BASED(cleanup_table+16)	# sysc_done
 +	jl	cleanup_sysc_restore
 +	cl	%r9,BASED(cleanup_table+20)	# io_tif
  	jl	0f
 -	clg	%r9,BASED(.Lcleanup_table+24)	# .Lsysc_restore
 -	jl	.Lcleanup_sysc_tif
 -	clg	%r9,BASED(.Lcleanup_table+32)	# .Lsysc_done
 -	jl	.Lcleanup_sysc_restore
 -	clg	%r9,BASED(.Lcleanup_table+40)	# .Lio_tif
 +	cl	%r9,BASED(cleanup_table+24)	# io_restore
 +	jl	cleanup_io_tif
 +	cl	%r9,BASED(cleanup_table+28)	# io_done
 +	jl	cleanup_io_restore
 +	cl	%r9,BASED(cleanup_table+32)	# psw_idle
  	jl	0f
++<<<<<<< HEAD
 +	cl	%r9,BASED(cleanup_table+36)	# psw_idle_end
 +	jl	cleanup_idle
 +0:	br	%r14
 +
 +cleanup_system_call:
++=======
+ 	clg	%r9,BASED(.Lcleanup_table+48)	# .Lio_restore
+ 	jl	.Lcleanup_io_tif
+ 	clg	%r9,BASED(.Lcleanup_table+56)	# .Lio_done
+ 	jl	.Lcleanup_io_restore
+ 	clg	%r9,BASED(.Lcleanup_table+64)	# psw_idle
+ 	jl	0f
+ 	clg	%r9,BASED(.Lcleanup_table+72)	# .Lpsw_idle_end
+ 	jl	.Lcleanup_idle
+ 	clg	%r9,BASED(.Lcleanup_table+80)	# save_fpu_regs
+ 	jl	0f
+ 	clg	%r9,BASED(.Lcleanup_table+88)	# .Lsave_fpu_regs_end
+ 	jl	.Lcleanup_save_fpu_regs
+ 	clg	%r9,BASED(.Lcleanup_table+96)	# load_fpu_regs
+ 	jl	0f
+ 	clg	%r9,BASED(.Lcleanup_table+104)	# .Lload_fpu_regs_end
+ 	jl	.Lcleanup_load_fpu_regs
+ 0:	BR_EX	%r14
+ 
+ 	.align	8
+ .Lcleanup_table:
+ 	.quad	system_call
+ 	.quad	.Lsysc_do_svc
+ 	.quad	.Lsysc_tif
+ 	.quad	.Lsysc_restore
+ 	.quad	.Lsysc_done
+ 	.quad	.Lio_tif
+ 	.quad	.Lio_restore
+ 	.quad	.Lio_done
+ 	.quad	psw_idle
+ 	.quad	.Lpsw_idle_end
+ 	.quad	save_fpu_regs
+ 	.quad	.Lsave_fpu_regs_end
+ 	.quad	load_fpu_regs
+ 	.quad	.Lload_fpu_regs_end
+ 
+ #if IS_ENABLED(CONFIG_KVM)
+ .Lcleanup_table_sie:
+ 	.quad	.Lsie_gmap
+ 	.quad	.Lsie_done
+ 
+ .Lcleanup_sie:
+ 	cghi    %r11,__LC_SAVE_AREA_ASYNC 	#Is this in normal interrupt?
+ 	je      1f
+ 	slg     %r9,BASED(.Lsie_crit_mcck_start)
+ 	clg     %r9,BASED(.Lsie_crit_mcck_length)
+ 	jh      1f
+ 	oi      __LC_CPU_FLAGS+7, _CIF_MCCK_GUEST
+ 1:	BPENTER __SF_SIE_FLAGS(%r15),(_TIF_ISOLATE_BP|_TIF_ISOLATE_BP_GUEST)
+ 	lg	%r9,__SF_SIE_CONTROL(%r15)	# get control block pointer
+ 	ni	__SIE_PROG0C+3(%r9),0xfe	# no longer in SIE
+ 	lctlg	%c1,%c1,__LC_USER_ASCE		# load primary asce
+ 	larl	%r9,sie_exit			# skip forward to sie_exit
+ 	BR_EX	%r14
+ #endif
+ 
+ .Lcleanup_system_call:
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  	# check if stpt has been executed
 -	clg	%r9,BASED(.Lcleanup_system_call_insn)
 +	cl	%r9,BASED(cleanup_system_call_insn)
  	jh	0f
  	mvc	__LC_SYNC_ENTER_TIMER(8),__LC_ASYNC_ENTER_TIMER
 -	cghi	%r11,__LC_SAVE_AREA_ASYNC
 +	chi	%r11,__LC_SAVE_AREA_ASYNC
  	je	0f
  	mvc	__LC_SYNC_ENTER_TIMER(8),__LC_MCCK_ENTER_TIMER
 -0:	# check if stmg has been executed
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+8)
 +0:	# check if stm has been executed
 +	cl	%r9,BASED(cleanup_system_call_insn+4)
  	jh	0f
 -	mvc	__LC_SAVE_AREA_SYNC(64),0(%r11)
 -0:	# check if base register setup + TIF bit load has been done
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+16)
 -	jhe	0f
 -	# set up saved register r12 task struct pointer
 -	stg	%r12,32(%r11)
 -	# set up saved register r13 __TASK_thread offset
 -	mvc	40(8,%r11),BASED(.Lcleanup_system_call_const)
 -0:	# check if the user time update has been done
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+24)
 +	mvc	__LC_SAVE_AREA_SYNC(32),0(%r11)
 +0:	# set up saved registers r12, and r13
 +	st	%r12,16(%r11)		# r12 thread-info pointer
 +	st	%r13,20(%r11)		# r13 literal-pool pointer
 +	# check if the user time calculation has been done
 +	cl	%r9,BASED(cleanup_system_call_insn+8)
  	jh	0f
 -	lg	%r15,__LC_EXIT_TIMER
 -	slg	%r15,__LC_SYNC_ENTER_TIMER
 -	alg	%r15,__LC_USER_TIMER
 -	stg	%r15,__LC_USER_TIMER
 -0:	# check if the system time update has been done
 -	clg	%r9,BASED(.Lcleanup_system_call_insn+32)
 +	l	%r10,__LC_EXIT_TIMER
 +	l	%r15,__LC_EXIT_TIMER+4
 +	SUB64	%r10,%r15,__LC_SYNC_ENTER_TIMER
 +	ADD64	%r10,%r15,__LC_USER_TIMER
 +	st	%r10,__LC_USER_TIMER
 +	st	%r15,__LC_USER_TIMER+4
 +0:	# check if the system time calculation has been done
 +	cl	%r9,BASED(cleanup_system_call_insn+12)
  	jh	0f
 -	lg	%r15,__LC_LAST_UPDATE_TIMER
 -	slg	%r15,__LC_EXIT_TIMER
 -	alg	%r15,__LC_SYSTEM_TIMER
 -	stg	%r15,__LC_SYSTEM_TIMER
 +	l	%r10,__LC_LAST_UPDATE_TIMER
 +	l	%r15,__LC_LAST_UPDATE_TIMER+4
 +	SUB64	%r10,%r15,__LC_EXIT_TIMER
 +	ADD64	%r10,%r15,__LC_SYSTEM_TIMER
 +	st	%r10,__LC_SYSTEM_TIMER
 +	st	%r15,__LC_SYSTEM_TIMER+4
  0:	# update accounting time stamp
  	mvc	__LC_LAST_UPDATE_TIMER(8),__LC_SYNC_ENTER_TIMER
 -	BPENTER __TI_flags(%r12),_TIF_ISOLATE_BP
 -	# set up saved register r11
 -	lg	%r15,__LC_KERNEL_STACK
 +	# set up saved register 11
 +	l	%r15,__LC_KERNEL_STACK
  	la	%r9,STACK_FRAME_OVERHEAD(%r15)
 -	stg	%r9,24(%r11)		# r11 pt_regs pointer
 +	st	%r9,12(%r11)		# r11 pt_regs pointer
  	# fill pt_regs
 -	mvc	__PT_R8(64,%r9),__LC_SAVE_AREA_SYNC
 -	stmg	%r0,%r7,__PT_R0(%r9)
 -	mvc	__PT_PSW(16,%r9),__LC_SVC_OLD_PSW
 +	mvc	__PT_R8(32,%r9),__LC_SAVE_AREA_SYNC
 +	stm	%r0,%r7,__PT_R0(%r9)
 +	mvc	__PT_PSW(8,%r9),__LC_SVC_OLD_PSW
  	mvc	__PT_INT_CODE(4,%r9),__LC_SVC_ILC
 -	xc	__PT_FLAGS(8,%r9),__PT_FLAGS(%r9)
 -	mvi	__PT_FLAGS+7(%r9),_PIF_SYSCALL
 -	# setup saved register r15
 -	stg	%r15,56(%r11)		# r15 stack pointer
 +	# setup saved register 15
 +	st	%r15,28(%r11)		# r15 stack pointer
  	# set new psw address and exit
++<<<<<<< HEAD
 +	l	%r9,BASED(cleanup_table+4)	# sysc_do_svc + 0x80000000
 +	br	%r14
 +cleanup_system_call_insn:
 +	.long	system_call + 0x80000000
 +	.long	sysc_stm + 0x80000000
 +	.long	sysc_vtime + 0x80000000 + 36
 +	.long	sysc_vtime + 0x80000000 + 76
 +
 +cleanup_sysc_tif:
 +	l	%r9,BASED(cleanup_table+8)	# sysc_tif + 0x80000000
 +	br	%r14
 +
 +cleanup_sysc_restore:
 +	cl	%r9,BASED(cleanup_sysc_restore_insn)
 +	jhe	0f
 +	l	%r9,12(%r11)		# get saved pointer to pt_regs
 +	mvc	__LC_RETURN_PSW(8),__PT_PSW(%r9)
 +	mvc	0(32,%r11),__PT_R8(%r9)
 +	lm	%r0,%r7,__PT_R0(%r9)
 +0:	lm	%r8,%r9,__LC_RETURN_PSW
 +	br	%r14
 +cleanup_sysc_restore_insn:
 +	.long	sysc_done - 4 + 0x80000000
 +
 +cleanup_io_tif:
 +	l	%r9,BASED(cleanup_table+20)	# io_tif + 0x80000000
 +	br	%r14
 +
 +cleanup_io_restore:
 +	cl	%r9,BASED(cleanup_io_restore_insn)
 +	jhe	0f
 +	l	%r9,12(%r11)		# get saved r11 pointer to pt_regs
 +	mvc	__LC_RETURN_PSW(8),__PT_PSW(%r9)
 +	mvc	0(32,%r11),__PT_R8(%r9)
 +	lm	%r0,%r7,__PT_R0(%r9)
 +0:	lm	%r8,%r9,__LC_RETURN_PSW
 +	br	%r14
 +cleanup_io_restore_insn:
 +	.long	io_done - 4 + 0x80000000
++=======
+ 	larl	%r9,.Lsysc_do_svc
+ 	BR_EX	%r14,%r11
+ .Lcleanup_system_call_insn:
+ 	.quad	system_call
+ 	.quad	.Lsysc_stmg
+ 	.quad	.Lsysc_per
+ 	.quad	.Lsysc_vtime+36
+ 	.quad	.Lsysc_vtime+42
+ .Lcleanup_system_call_const:
+ 	.quad	__TASK_thread
+ 
+ .Lcleanup_sysc_tif:
+ 	larl	%r9,.Lsysc_tif
+ 	BR_EX	%r14,%r11
+ 
+ .Lcleanup_sysc_restore:
+ 	# check if stpt has been executed
+ 	clg	%r9,BASED(.Lcleanup_sysc_restore_insn)
+ 	jh	0f
+ 	mvc	__LC_EXIT_TIMER(8),__LC_ASYNC_ENTER_TIMER
+ 	cghi	%r11,__LC_SAVE_AREA_ASYNC
+ 	je	0f
+ 	mvc	__LC_EXIT_TIMER(8),__LC_MCCK_ENTER_TIMER
+ 0:	clg	%r9,BASED(.Lcleanup_sysc_restore_insn+8)
+ 	je	1f
+ 	lg	%r9,24(%r11)		# get saved pointer to pt_regs
+ 	mvc	__LC_RETURN_PSW(16),__PT_PSW(%r9)
+ 	mvc	0(64,%r11),__PT_R8(%r9)
+ 	lmg	%r0,%r7,__PT_R0(%r9)
+ 1:	lmg	%r8,%r9,__LC_RETURN_PSW
+ 	BR_EX	%r14,%r11
+ .Lcleanup_sysc_restore_insn:
+ 	.quad	.Lsysc_exit_timer
+ 	.quad	.Lsysc_done - 4
+ 
+ .Lcleanup_io_tif:
+ 	larl	%r9,.Lio_tif
+ 	BR_EX	%r14,%r11
+ 
+ .Lcleanup_io_restore:
+ 	# check if stpt has been executed
+ 	clg	%r9,BASED(.Lcleanup_io_restore_insn)
+ 	jh	0f
+ 	mvc	__LC_EXIT_TIMER(8),__LC_MCCK_ENTER_TIMER
+ 0:	clg	%r9,BASED(.Lcleanup_io_restore_insn+8)
+ 	je	1f
+ 	lg	%r9,24(%r11)		# get saved r11 pointer to pt_regs
+ 	mvc	__LC_RETURN_PSW(16),__PT_PSW(%r9)
+ 	mvc	0(64,%r11),__PT_R8(%r9)
+ 	lmg	%r0,%r7,__PT_R0(%r9)
+ 1:	lmg	%r8,%r9,__LC_RETURN_PSW
+ 	BR_EX	%r14,%r11
+ .Lcleanup_io_restore_insn:
+ 	.quad	.Lio_exit_timer
+ 	.quad	.Lio_done - 4
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
 -.Lcleanup_idle:
 -	ni	__LC_CPU_FLAGS+7,255-_CIF_ENABLED_WAIT
 +cleanup_idle:
  	# copy interrupt clock & cpu timer
  	mvc	__CLOCK_IDLE_EXIT(8,%r2),__LC_INT_CLOCK
  	mvc	__TIMER_IDLE_EXIT(8,%r2),__LC_ASYNC_ENTER_TIMER
@@@ -922,30 -1419,56 +1316,46 @@@
  	je	0f
  	mvc	__CLOCK_IDLE_EXIT(8,%r2),__LC_MCCK_CLOCK
  	mvc	__TIMER_IDLE_EXIT(8,%r2),__LC_MCCK_ENTER_TIMER
 -0:	# check if stck & stpt have been executed
 -	clg	%r9,BASED(.Lcleanup_idle_insn)
 +0:	# check if stck has been executed
 +	cl	%r9,BASED(cleanup_idle_insn)
  	jhe	1f
  	mvc	__CLOCK_IDLE_ENTER(8,%r2),__CLOCK_IDLE_EXIT(%r2)
 -	mvc	__TIMER_IDLE_ENTER(8,%r2),__TIMER_IDLE_EXIT(%r2)
 -1:	# calculate idle cycles
 -#ifdef CONFIG_SMP
 -	clg	%r9,BASED(.Lcleanup_idle_insn)
 -	jl	3f
 -	larl	%r1,smp_cpu_mtid
 -	llgf	%r1,0(%r1)
 -	ltgr	%r1,%r1
 -	jz	3f
 -	.insn	rsy,0xeb0000000017,%r1,5,__SF_EMPTY+80(%r15)
 -	larl	%r3,mt_cycles
 -	ag	%r3,__LC_PERCPU_OFFSET
 -	la	%r4,__SF_EMPTY+16(%r15)
 -2:	lg	%r0,0(%r3)
 -	slg	%r0,0(%r4)
 -	alg	%r0,64(%r4)
 -	stg	%r0,0(%r3)
 -	la	%r3,8(%r3)
 -	la	%r4,8(%r4)
 -	brct	%r1,2b
 -#endif
 -3:	# account system time going idle
 -	lg	%r9,__LC_STEAL_TIMER
 -	alg	%r9,__CLOCK_IDLE_ENTER(%r2)
 -	slg	%r9,__LC_LAST_UPDATE_CLOCK
 -	stg	%r9,__LC_STEAL_TIMER
 +	mvc	__TIMER_IDLE_ENTER(8,%r2),__TIMER_IDLE_EXIT(%r3)
 +1:	# account system time going idle
 +	lm	%r9,%r10,__LC_STEAL_TIMER
 +	ADD64	%r9,%r10,__CLOCK_IDLE_ENTER(%r2)
 +	SUB64	%r9,%r10,__LC_LAST_UPDATE_CLOCK
 +	stm	%r9,%r10,__LC_STEAL_TIMER
  	mvc	__LC_LAST_UPDATE_CLOCK(8),__CLOCK_IDLE_EXIT(%r2)
 -	lg	%r9,__LC_SYSTEM_TIMER
 -	alg	%r9,__LC_LAST_UPDATE_TIMER
 -	slg	%r9,__TIMER_IDLE_ENTER(%r2)
 -	stg	%r9,__LC_SYSTEM_TIMER
 +	lm	%r9,%r10,__LC_SYSTEM_TIMER
 +	ADD64	%r9,%r10,__LC_LAST_UPDATE_TIMER
 +	SUB64	%r9,%r10,__TIMER_IDLE_ENTER(%r2)
 +	stm	%r9,%r10,__LC_SYSTEM_TIMER
  	mvc	__LC_LAST_UPDATE_TIMER(8),__TIMER_IDLE_EXIT(%r2)
  	# prepare return psw
++<<<<<<< HEAD
 +	n	%r8,BASED(cleanup_idle_wait)	# clear irq & wait state bits
 +	l	%r9,24(%r11)			# return from psw_idle
 +	br	%r14
 +cleanup_idle_insn:
 +	.long	psw_idle_lpsw + 0x80000000
 +cleanup_idle_wait:
 +	.long	0xfcfdffff
++=======
+ 	nihh	%r8,0xfcfd		# clear irq & wait state bits
+ 	lg	%r9,48(%r11)		# return from psw_idle
+ 	BR_EX	%r14,%r11
+ .Lcleanup_idle_insn:
+ 	.quad	.Lpsw_idle_lpsw
+ 
+ .Lcleanup_save_fpu_regs:
+ 	larl	%r9,save_fpu_regs
+ 	BR_EX	%r14,%r11
+ 
+ .Lcleanup_load_fpu_regs:
+ 	larl	%r9,load_fpu_regs
+ 	BR_EX	%r14,%r11
++>>>>>>> 6dd85fbb87d1 (s390: move expoline assembler macros to a header)
  
  /*
   * Integer constants
diff --git a/arch/s390/include/asm/nospec-insn.h b/arch/s390/include/asm/nospec-insn.h
new file mode 100644
index 000000000000..440689cbcf51
--- /dev/null
+++ b/arch/s390/include/asm/nospec-insn.h
@@ -0,0 +1,127 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_S390_NOSPEC_ASM_H
+#define _ASM_S390_NOSPEC_ASM_H
+
+#include <asm/dwarf.h>
+
+#ifdef __ASSEMBLY__
+
+#ifdef CONFIG_EXPOLINE
+
+/*
+ * The expoline macros are used to create thunks in the same format
+ * as gcc generates them. The 'comdat' section flag makes sure that
+ * the various thunks are merged into a single copy.
+ */
+	.macro __THUNK_PROLOG_NAME name
+	.pushsection .text.\name,"axG",@progbits,\name,comdat
+	.globl \name
+	.hidden \name
+	.type \name,@function
+\name:
+	CFI_STARTPROC
+	.endm
+
+	.macro __THUNK_EPILOG
+	CFI_ENDPROC
+	.popsection
+	.endm
+
+	.macro __THUNK_PROLOG_BR r1,r2
+	__THUNK_PROLOG_NAME __s390x_indirect_jump_r\r2\()use_r\r1
+	.endm
+
+	.macro __THUNK_BR r1,r2
+	jg	__s390x_indirect_jump_r\r2\()use_r\r1
+	.endm
+
+	.macro __THUNK_BRASL r1,r2,r3
+	brasl	\r1,__s390x_indirect_jump_r\r3\()use_r\r2
+	.endm
+
+	.macro	__DECODE_RR expand,reg,ruse
+	.set __decode_fail,1
+	.irp r1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
+	.ifc \reg,%r\r1
+	.irp r2,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
+	.ifc \ruse,%r\r2
+	\expand \r1,\r2
+	.set __decode_fail,0
+	.endif
+	.endr
+	.endif
+	.endr
+	.if __decode_fail == 1
+	.error "__DECODE_RR failed"
+	.endif
+	.endm
+
+	.macro	__DECODE_RRR expand,rsave,rtarget,ruse
+	.set __decode_fail,1
+	.irp r1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
+	.ifc \rsave,%r\r1
+	.irp r2,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
+	.ifc \rtarget,%r\r2
+	.irp r3,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15
+	.ifc \ruse,%r\r3
+	\expand \r1,\r2,\r3
+	.set __decode_fail,0
+	.endif
+	.endr
+	.endif
+	.endr
+	.endif
+	.endr
+	.if __decode_fail == 1
+	.error "__DECODE_RRR failed"
+	.endif
+	.endm
+
+	.macro __THUNK_EX_BR reg,ruse
+#ifdef CONFIG_HAVE_MARCH_Z10_FEATURES
+	exrl	0,555f
+	j	.
+#else
+	larl	\ruse,555f
+	ex	0,0(\ruse)
+	j	.
+#endif
+555:	br	\reg
+	.endm
+
+	.macro GEN_BR_THUNK reg,ruse=%r1
+	__DECODE_RR __THUNK_PROLOG_BR,\reg,\ruse
+	__THUNK_EX_BR \reg,\ruse
+	__THUNK_EPILOG
+	.endm
+
+	.macro BR_EX reg,ruse=%r1
+557:	__DECODE_RR __THUNK_BR,\reg,\ruse
+	.pushsection .s390_indirect_branches,"a",@progbits
+	.long	557b-.
+	.popsection
+	.endm
+
+	.macro BASR_EX rsave,rtarget,ruse=%r1
+559:	__DECODE_RRR __THUNK_BRASL,\rsave,\rtarget,\ruse
+	.pushsection .s390_indirect_branches,"a",@progbits
+	.long	559b-.
+	.popsection
+	.endm
+
+#else
+	.macro GEN_BR_THUNK reg,ruse=%r1
+	.endm
+
+	 .macro BR_EX reg,ruse=%r1
+	br	\reg
+	.endm
+
+	.macro BASR_EX rsave,rtarget,ruse=%r1
+	basr	\rsave,\rtarget
+	.endm
+#endif
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* _ASM_S390_NOSPEC_ASM_H */
* Unmerged path arch/s390/kernel/entry.S
