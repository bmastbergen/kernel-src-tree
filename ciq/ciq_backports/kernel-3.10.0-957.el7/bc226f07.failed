KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD

jira LE-1907
cve CVE-2018-3639
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit bc226f07dcd3c9ef0b7f6236fe356ea4a9cb4769
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/bc226f07.failed

Expose the new virtualized architectural mechanism, VIRT_SSBD, for using
speculative store bypass disable (SSBD) under SVM.  This will allow guests
to use SSBD on hardware that uses non-architectural mechanisms for enabling
SSBD.

[ tglx: Folded the migration fixup from Paolo Bonzini ]

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit bc226f07dcd3c9ef0b7f6236fe356ea4a9cb4769)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/common.c
#	arch/x86/kvm/cpuid.c
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kernel/cpu/common.c
index 49cb90f121df,b4247ed0c81e..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -723,9 -761,28 +723,29 @@@ static void init_speculation_control(st
  	if (cpu_has(c, X86_FEATURE_SPEC_CTRL)) {
  		set_cpu_cap(c, X86_FEATURE_IBRS);
  		set_cpu_cap(c, X86_FEATURE_IBPB);
 -		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
  	}
 -
  	if (cpu_has(c, X86_FEATURE_INTEL_STIBP))
  		set_cpu_cap(c, X86_FEATURE_STIBP);
++<<<<<<< HEAD
++=======
+ 
+ 	if (cpu_has(c, X86_FEATURE_SPEC_CTRL_SSBD) ||
+ 	    cpu_has(c, X86_FEATURE_VIRT_SSBD))
+ 		set_cpu_cap(c, X86_FEATURE_SSBD);
+ 
+ 	if (cpu_has(c, X86_FEATURE_AMD_IBRS)) {
+ 		set_cpu_cap(c, X86_FEATURE_IBRS);
+ 		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+ 	}
+ 
+ 	if (cpu_has(c, X86_FEATURE_AMD_IBPB))
+ 		set_cpu_cap(c, X86_FEATURE_IBPB);
+ 
+ 	if (cpu_has(c, X86_FEATURE_AMD_STIBP)) {
+ 		set_cpu_cap(c, X86_FEATURE_STIBP);
+ 		set_cpu_cap(c, X86_FEATURE_MSR_SPEC_CTRL);
+ 	}
++>>>>>>> bc226f07dcd3 (KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD)
  }
  
  void get_cpu_cap(struct cpuinfo_x86 *c)
diff --cc arch/x86/kvm/cpuid.c
index 7bb663af32bb,ced851169730..000000000000
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@@ -357,7 -374,12 +357,16 @@@ static inline int __do_cpuid_ent(struc
  		F(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |
  		F(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |
  		F(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |
++<<<<<<< HEAD
 +		0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM);
++=======
+ 		0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM) |
+ 		F(TOPOEXT) | F(PERFCTR_CORE);
+ 
+ 	/* cpuid 0x80000008.ebx */
+ 	const u32 kvm_cpuid_8000_0008_ebx_x86_features =
+ 		F(AMD_IBPB) | F(AMD_IBRS) | F(VIRT_SSBD);
++>>>>>>> bc226f07dcd3 (KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD)
  
  	/* cpuid 0xC0000001.edx */
  	const u32 kvm_cpuid_C000_0001_edx_x86_features =
@@@ -623,9 -646,21 +632,27 @@@
  		if (!g_phys_as)
  			g_phys_as = phys_as;
  		entry->eax = g_phys_as | (virt_as << 8);
++<<<<<<< HEAD
 +		entry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;
 +		cpuid_mask(&entry->ecx, CPUID_8000_0008_EBX);
 +		entry->edx = 0;
++=======
+ 		entry->edx = 0;
+ 		/*
+ 		 * IBRS, IBPB and VIRT_SSBD aren't necessarily present in
+ 		 * hardware cpuid
+ 		 */
+ 		if (boot_cpu_has(X86_FEATURE_AMD_IBPB))
+ 			entry->ebx |= F(AMD_IBPB);
+ 		if (boot_cpu_has(X86_FEATURE_AMD_IBRS))
+ 			entry->ebx |= F(AMD_IBRS);
+ 		if (boot_cpu_has(X86_FEATURE_VIRT_SSBD))
+ 			entry->ebx |= F(VIRT_SSBD);
+ 		entry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;
+ 		cpuid_mask(&entry->ebx, CPUID_8000_0008_EBX);
+ 		if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD))
+ 			entry->ebx |= F(VIRT_SSBD);
++>>>>>>> bc226f07dcd3 (KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD)
  		break;
  	}
  	case 0x80000019:
diff --cc arch/x86/kvm/svm.c
index 6bd91543d358,26110c202b19..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -3653,10 -4114,38 +3653,39 @@@ static int svm_get_msr(struct kvm_vcpu 
  		msr_info->data = svm->nested.vm_cr_msr;
  		break;
  	case MSR_IA32_SPEC_CTRL:
 -		if (!msr_info->host_initiated &&
 -		    !guest_cpuid_has(vcpu, X86_FEATURE_AMD_IBRS))
 -			return 1;
 -
  		msr_info->data = svm->spec_ctrl;
  		break;
++<<<<<<< HEAD
 +	case MSR_IA32_UCODE_REV:
 +		msr_info->data = 0x01000065;
++=======
+ 	case MSR_AMD64_VIRT_SPEC_CTRL:
+ 		if (!msr_info->host_initiated &&
+ 		    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))
+ 			return 1;
+ 
+ 		msr_info->data = svm->virt_spec_ctrl;
+ 		break;
+ 	case MSR_F15H_IC_CFG: {
+ 
+ 		int family, model;
+ 
+ 		family = guest_cpuid_family(vcpu);
+ 		model  = guest_cpuid_model(vcpu);
+ 
+ 		if (family < 0 || model < 0)
+ 			return kvm_get_msr_common(vcpu, msr_info);
+ 
+ 		msr_info->data = 0;
+ 
+ 		if (family == 0x15 &&
+ 		    (model >= 0x2 && model < 0x20))
+ 			msr_info->data = 0x1E;
+ 		}
+ 		break;
+ 	case MSR_F10H_DECFG:
+ 		msr_info->data = svm->msr_decfg;
++>>>>>>> bc226f07dcd3 (KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD)
  		break;
  	default:
  		return kvm_get_msr_common(vcpu, msr_info);
diff --cc arch/x86/kvm/x86.c
index 5fb8d1b72511,421a39e40d5e..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1000,6 -1055,10 +1000,13 @@@ static u32 emulated_msrs[] = 
  	MSR_IA32_MCG_CTL,
  	MSR_IA32_MCG_EXT_CTL,
  	MSR_IA32_SMBASE,
++<<<<<<< HEAD
++=======
+ 	MSR_SMI_COUNT,
+ 	MSR_PLATFORM_INFO,
+ 	MSR_MISC_FEATURES_ENABLES,
+ 	MSR_AMD64_VIRT_SPEC_CTRL,
++>>>>>>> bc226f07dcd3 (KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD)
  };
  
  static unsigned num_emulated_msrs;
@@@ -2634,11 -2904,8 +2641,11 @@@ int kvm_vm_ioctl_check_extension(struc
  		 * fringe case that is not enabled except via specific settings
  		 * of the module parameters.
  		 */
- 		r = kvm_x86_ops->cpu_has_high_real_mode_segbase();
+ 		r = kvm_x86_ops->has_emulated_msr(MSR_IA32_SMBASE);
  		break;
 +	case KVM_CAP_COALESCED_MMIO:
 +		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 +		break;
  	case KVM_CAP_VAPIC:
  		r = !kvm_x86_ops->cpu_has_accelerated_tpr();
  		break;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 081db5ad9392..b9588ae7a261 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -823,7 +823,7 @@ struct kvm_x86_ops {
 	int (*hardware_setup)(void);               /* __init */
 	void (*hardware_unsetup)(void);            /* __exit */
 	bool (*cpu_has_accelerated_tpr)(void);
-	bool (*cpu_has_high_real_mode_segbase)(void);
+	bool (*has_emulated_msr)(int index);
 	void (*cpuid_update)(struct kvm_vcpu *vcpu);
 
 	int (*vm_init)(struct kvm *kvm);
* Unmerged path arch/x86/kernel/cpu/common.c
* Unmerged path arch/x86/kvm/cpuid.c
* Unmerged path arch/x86/kvm/svm.c
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index bb5a385ea422..fec8083c7dd8 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -8674,9 +8674,21 @@ static void vmx_handle_external_intr(struct kvm_vcpu *vcpu)
 		local_irq_enable();
 }
 
-static bool vmx_has_high_real_mode_segbase(void)
+static bool vmx_has_emulated_msr(int index)
 {
-	return enable_unrestricted_guest || emulate_invalid_guest_state;
+	switch (index) {
+	case MSR_IA32_SMBASE:
+		/*
+		 * We cannot do SMM unless we can run the guest in big
+		 * real mode.
+		 */
+		return enable_unrestricted_guest || emulate_invalid_guest_state;
+	case MSR_AMD64_VIRT_SPEC_CTRL:
+		/* This is AMD only.  */
+		return false;
+	default:
+		return true;
+	}
 }
 
 static bool vmx_mpx_supported(void)
@@ -11530,7 +11542,7 @@ static struct kvm_x86_ops vmx_x86_ops = {
 	.hardware_enable = hardware_enable,
 	.hardware_disable = hardware_disable,
 	.cpu_has_accelerated_tpr = report_flexpriority,
-	.cpu_has_high_real_mode_segbase = vmx_has_high_real_mode_segbase,
+	.has_emulated_msr = vmx_has_emulated_msr,
 
 	.vcpu_create = vmx_create_vcpu,
 	.vcpu_free = vmx_free_vcpu,
* Unmerged path arch/x86/kvm/x86.c
