ip: use rb trees for IP frag queue.

jira LE-1907
cve CVE-2018-5391
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [net] ip: use rb trees for IP frag queue (Sabrina Dubroca) [1613924] {CVE-2018-5391}
Rebuild_FUZZ: 98.55%
commit-author Peter Oskolkov <posk@google.com>
commit fa0f527358bd900ef92f925878ed6bfbd51305cc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/fa0f5273.failed

Similar to TCP OOO RX queue, it makes sense to use rb trees to store
IP fragments, so that OOO fragments are inserted faster.

Tested:

- a follow-up patch contains a rather comprehensive ip defrag
  self-test (functional)
- ran neper `udp_stream -c -H <host> -F 100 -l 300 -T 20`:
    netstat --statistics
    Ip:
        282078937 total packets received
        0 forwarded
        0 incoming packets discarded
        946760 incoming packets delivered
        18743456 requests sent out
        101 fragments dropped after timeout
        282077129 reassemblies required
        944952 packets reassembled ok
        262734239 packet reassembles failed
   (The numbers/stats above are somewhat better re:
    reassemblies vs a kernel without this patchset. More
    comprehensive performance testing TBD).

	Reported-by: Jann Horn <jannh@google.com>
	Reported-by: Juha-Matti Tilli <juha-matti.tilli@iki.fi>
	Suggested-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: Peter Oskolkov <posk@google.com>
	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Florian Westphal <fw@strlen.de>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit fa0f527358bd900ef92f925878ed6bfbd51305cc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/skbuff.h
#	include/net/inet_frag.h
#	net/ipv4/inet_fragment.c
#	net/ipv4/ip_fragment.c
diff --cc include/linux/skbuff.h
index 8a4de9b1fc10,7ebdf158a795..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -677,18 -670,27 +677,37 @@@ struct sk_buff 
  			struct sk_buff		*prev;
  
  			union {
++<<<<<<< HEAD
 +				ktime_t		tstamp;
 +				struct skb_mstamp skb_mstamp;
 +				__RH_KABI_CHECK_SIZE_ALIGN(ktime_t a,
 +							   struct skb_mstamp b);
 +			};
 +		};
 +		struct rb_node	rbnode; /* used in netem & tcp stack */
 +	};
 +#endif
 +	struct sock		*sk;
 +	struct net_device	*dev;
++=======
+ 				struct net_device	*dev;
+ 				/* Some protocols might use this space to store information,
+ 				 * while device pointer would be NULL.
+ 				 * UDP receive path is one user.
+ 				 */
+ 				unsigned long		dev_scratch;
+ 			};
+ 		};
+ 		struct rb_node		rbnode; /* used in netem, ip4 defrag, and tcp stack */
+ 		struct list_head	list;
+ 	};
+ 
+ 	union {
+ 		struct sock		*sk;
+ 		int			ip_defrag_offset;
+ 	};
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  
 -	union {
 -		ktime_t		tstamp;
 -		u64		skb_mstamp;
 -	};
  	/*
  	 * This is the control buffer. It is free to use for every
  	 * layer. Please put your private variables there. If you
diff --cc include/net/inet_frag.h
index 5e18d96ee409,b86d14528188..000000000000
--- a/include/net/inet_frag.h
+++ b/include/net/inet_frag.h
@@@ -1,160 -1,139 +1,166 @@@
  #ifndef __NET_FRAG_H__
  #define __NET_FRAG_H__
  
 -#include <linux/rhashtable-types.h>
 +#include <linux/percpu_counter.h> /* Only needed for RH KABI */
  
  struct netns_frags {
 -	/* sysctls */
 -	long			high_thresh;
 -	long			low_thresh;
 -	int			timeout;
 -	int			max_dist;
 -	struct inet_frags	*f;
 +	int			nqueues;
 +	struct list_head	lru_list;
 +	spinlock_t		lru_lock;
  
 -	struct rhashtable       rhashtable ____cacheline_aligned_in_smp;
 +	/* RHEL notes: this struct is frozen by KABI as it is embedded
 +	 * into other netns structs.  Fortunately the cacheline
 +	 * alignment contains enough padding.
 +	 */
 +	RH_KABI_DEPRECATE(struct percpu_counter, mem ____cacheline_aligned_in_smp)
  
 -	/* Keep atomic mem on separate cachelines in structs that include it */
 -	atomic_long_t		mem ____cacheline_aligned_in_smp;
 -};
 -
 -/**
 - * fragment queue flags
 - *
 - * @INET_FRAG_FIRST_IN: first fragment has arrived
 - * @INET_FRAG_LAST_IN: final fragment has arrived
 - * @INET_FRAG_COMPLETE: frag queue has been processed and is due for destruction
 - */
 -enum {
 -	INET_FRAG_FIRST_IN	= BIT(0),
 -	INET_FRAG_LAST_IN	= BIT(1),
 -	INET_FRAG_COMPLETE	= BIT(2),
 -};
 -
 -struct frag_v4_compare_key {
 -	__be32		saddr;
 -	__be32		daddr;
 -	u32		user;
 -	u32		vif;
 -	__be16		id;
 -	u16		protocol;
 -};
 +	/* sysctls */
 +	int			timeout;
 +	int			high_thresh;
 +	int			low_thresh;
  
 -struct frag_v6_compare_key {
 -	struct in6_addr	saddr;
 -	struct in6_addr	daddr;
 -	u32		user;
 -	__be32		id;
 -	u32		iif;
 +	RH_KABI_EXTEND(atomic_t	mem)
  };
  
 -/**
 - * struct inet_frag_queue - fragment queue
 - *
 - * @node: rhash node
 - * @key: keys identifying this frag.
 - * @timer: queue expiration timer
 - * @lock: spinlock protecting this frag
 - * @refcnt: reference count of the queue
 - * @fragments: received fragments head
 - * @fragments_tail: received fragments tail
 - * @stamp: timestamp of the last received fragment
 - * @len: total length of the original datagram
 - * @meat: length of received fragments so far
 - * @flags: fragment queue flags
 - * @max_size: maximum received fragment size
 - * @net: namespace that this frag belongs to
 - * @rcu: rcu head for freeing deferall
 - */
  struct inet_frag_queue {
 -	struct rhash_head	node;
 -	union {
 -		struct frag_v4_compare_key v4;
 -		struct frag_v6_compare_key v6;
 -	} key;
 -	struct timer_list	timer;
  	spinlock_t		lock;
++<<<<<<< HEAD
 +	struct timer_list	timer;      /* when will this queue expire? */
 +	struct list_head	lru_list;   /* lru list member */
 +	struct hlist_node	list;
 +	atomic_t		refcnt;
 +	struct sk_buff		*fragments; /* list of received fragments */
++=======
+ 	refcount_t		refcnt;
+ 	struct sk_buff		*fragments;  /* Used in IPv6. */
+ 	struct rb_root		rb_fragments; /* Used in IPv4. */
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	struct sk_buff		*fragments_tail;
  	ktime_t			stamp;
 -	int			len;
 +	int			len;        /* total length of orig datagram */
  	int			meat;
 -	__u8			flags;
 +	__u8			last_in;    /* first/last segment arrived? */
 +
 +#define INET_FRAG_COMPLETE	4
 +#define INET_FRAG_FIRST_IN	2
 +#define INET_FRAG_LAST_IN	1
 +
  	u16			max_size;
 -	struct netns_frags      *net;
 -	struct rcu_head		rcu;
 +
 +	struct netns_frags	*net;
  };
  
 -struct inet_frags {
 -	unsigned int		qsize;
 +#define INETFRAGS_HASHSZ	1024
 +
 +/* averaged:
 + * max_depth = default ipfrag_high_thresh / INETFRAGS_HASHSZ /
 + *	       rounded up (SKB_TRUELEN(0) + sizeof(struct ipq or
 + *	       struct frag_queue))
 + */
 +#define INETFRAGS_MAXDEPTH		128
  
 +struct inet_frag_bucket {
 +	struct hlist_head	chain;
 +	spinlock_t		chain_lock;
 +};
 +
 +struct inet_frags {
 +	struct inet_frag_bucket	hash[INETFRAGS_HASHSZ];
 +	/* This rwlock is a global lock (seperate per IPv4, IPv6 and
 +	 * netfilter). Important to keep this on a seperate cacheline.
 +	 * Its primarily a rebuild protection rwlock.
 +	 */
 +	rwlock_t		lock ____cacheline_aligned_in_smp;
 +	int			secret_interval;
 +	struct timer_list	secret_timer;
 +
 +	/* The first call to hashfn is responsible to initialize
 +	 * rnd. This is best done with net_get_random_once.
 +	 */
 +	u32			rnd;
 +	int			qsize;
 +
 +	unsigned int		(*hashfn)(struct inet_frag_queue *);
 +	bool			(*match)(struct inet_frag_queue *q, void *arg);
  	void			(*constructor)(struct inet_frag_queue *q,
 -					       const void *arg);
 +						void *arg);
  	void			(*destructor)(struct inet_frag_queue *);
 -	void			(*frag_expire)(struct timer_list *t);
 -	struct kmem_cache	*frags_cachep;
 -	const char		*frags_cache_name;
 -	struct rhashtable_params rhash_params;
 +	void			(*skb_free)(struct sk_buff *);
 +	void			(*frag_expire)(unsigned long data);
  };
  
 -int inet_frags_init(struct inet_frags *);
 +void inet_frags_init(struct inet_frags *);
  void inet_frags_fini(struct inet_frags *);
  
 -static inline int inet_frags_init_net(struct netns_frags *nf)
 +void inet_frags_init_net(struct netns_frags *nf);
 +void inet_frags_exit_net(struct netns_frags *nf, struct inet_frags *f);
 +
 +void inet_frag_kill(struct inet_frag_queue *q, struct inet_frags *f);
 +void inet_frag_destroy(struct inet_frag_queue *q,
 +				struct inet_frags *f, int *work);
 +int inet_frag_evictor(struct netns_frags *nf, struct inet_frags *f, bool force);
 +struct inet_frag_queue *inet_frag_find(struct netns_frags *nf,
 +		struct inet_frags *f, void *key, unsigned int hash)
 +	__releases(&f->lock);
 +void inet_frag_maybe_warn_overflow(struct inet_frag_queue *q,
 +				   const char *prefix);
 +
 +static inline void inet_frag_put(struct inet_frag_queue *q, struct inet_frags *f)
  {
 -	atomic_long_set(&nf->mem, 0);
 -	return rhashtable_init(&nf->rhashtable, &nf->f->rhash_params);
 +	if (atomic_dec_and_test(&q->refcnt))
 +		inet_frag_destroy(q, f, NULL);
  }
 -void inet_frags_exit_net(struct netns_frags *nf);
  
 -void inet_frag_kill(struct inet_frag_queue *q);
 -void inet_frag_destroy(struct inet_frag_queue *q);
 -struct inet_frag_queue *inet_frag_find(struct netns_frags *nf, void *key);
 +/* Memory Tracking Functions. */
  
 -static inline void inet_frag_put(struct inet_frag_queue *q)
 +static inline int frag_mem_limit(struct netns_frags *nf)
  {
 -	if (refcount_dec_and_test(&q->refcnt))
 -		inet_frag_destroy(q);
 +	return atomic_read(&nf->mem);
  }
  
 -/* Memory Tracking Functions. */
 +static inline void sub_frag_mem_limit(struct inet_frag_queue *q, int i)
 +{
 +	atomic_sub(i, &q->net->mem);
 +}
 +
 +static inline void add_frag_mem_limit(struct inet_frag_queue *q, int i)
 +{
 +	atomic_add(i, &q->net->mem);
 +}
 +
 +static inline void init_frag_mem_limit(struct netns_frags *nf)
 +{
 +	atomic_set(&nf->mem, 0);
 +}
 +
 +static inline int sum_frag_mem_limit(struct netns_frags *nf)
 +{
 +	return atomic_read(&nf->mem);
 +}
  
 -static inline long frag_mem_limit(const struct netns_frags *nf)
 +static inline void inet_frag_lru_move(struct inet_frag_queue *q)
  {
 -	return atomic_long_read(&nf->mem);
 +	spin_lock(&q->net->lru_lock);
 +	if (!list_empty(&q->lru_list))
 +		list_move_tail(&q->lru_list, &q->net->lru_list);
 +	spin_unlock(&q->net->lru_lock);
  }
  
 -static inline void sub_frag_mem_limit(struct netns_frags *nf, long val)
 +static inline void inet_frag_lru_del(struct inet_frag_queue *q)
  {
 -	atomic_long_sub(val, &nf->mem);
 +	spin_lock(&q->net->lru_lock);
 +	list_del_init(&q->lru_list);
 +	q->net->nqueues--;
 +	spin_unlock(&q->net->lru_lock);
  }
  
 -static inline void add_frag_mem_limit(struct netns_frags *nf, long val)
 +static inline void inet_frag_lru_add(struct netns_frags *nf,
 +				     struct inet_frag_queue *q)
  {
 -	atomic_long_add(val, &nf->mem);
 +	spin_lock(&nf->lru_lock);
 +	list_add_tail(&q->lru_list, &nf->lru_list);
 +	q->net->nqueues++;
 +	spin_unlock(&nf->lru_lock);
  }
  
  /* RFC 3168 support :
diff --cc net/ipv4/inet_fragment.c
index e5c94fababe6,6d258a5669e7..000000000000
--- a/net/ipv4/inet_fragment.c
+++ b/net/ipv4/inet_fragment.c
@@@ -176,22 -136,23 +176,36 @@@ void inet_frag_destroy(struct inet_frag
  	/* Release all fragment data. */
  	fp = q->fragments;
  	nf = q->net;
++<<<<<<< HEAD
 +	while (fp) {
 +		struct sk_buff *xp = fp->next;
 +
 +		sum_truesize += fp->truesize;
 +		frag_kfree_skb(nf, f, fp);
 +		fp = xp;
++=======
+ 	f = nf->f;
+ 	if (fp) {
+ 		do {
+ 			struct sk_buff *xp = fp->next;
+ 
+ 			sum_truesize += fp->truesize;
+ 			kfree_skb(fp);
+ 			fp = xp;
+ 		} while (fp);
+ 	} else {
+ 		sum_truesize = skb_rbtree_purge(&q->rb_fragments);
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	}
  	sum = sum_truesize + f->qsize;
 +	if (work)
 +		*work -= sum;
 +	sub_frag_mem_limit(q, sum);
  
 -	call_rcu(&q->rcu, inet_frag_destroy_rcu);
 +	if (f->destructor)
 +		f->destructor(q);
 +	kfree(q);
  
 -	sub_frag_mem_limit(nf, sum);
  }
  EXPORT_SYMBOL(inet_frag_destroy);
  
diff --cc net/ipv4/ip_fragment.c
index da6190718c34,0e8f8de77e71..000000000000
--- a/net/ipv4/ip_fragment.c
+++ b/net/ipv4/ip_fragment.c
@@@ -193,58 -132,80 +193,115 @@@ static void ip_evictor(struct net *net
  /*
   * Oops, a fragment queue timed out.  Kill it and send an ICMP reply.
   */
 -static void ip_expire(struct timer_list *t)
 +static void ip_expire(unsigned long arg)
  {
++<<<<<<< HEAD
++=======
+ 	struct inet_frag_queue *frag = from_timer(frag, t, timer);
+ 	const struct iphdr *iph;
+ 	struct sk_buff *head = NULL;
+ 	struct net *net;
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	struct ipq *qp;
 -	int err;
 +	struct net *net;
  
 -	qp = container_of(frag, struct ipq, q);
 +	qp = container_of((struct inet_frag_queue *) arg, struct ipq, q);
  	net = container_of(qp->q.net, struct net, ipv4.frags);
  
 -	rcu_read_lock();
  	spin_lock(&qp->q.lock);
  
 -	if (qp->q.flags & INET_FRAG_COMPLETE)
 +	if (qp->q.last_in & INET_FRAG_COMPLETE)
  		goto out;
  
  	ipq_kill(qp);
++<<<<<<< HEAD
 +
 +	IP_INC_STATS_BH(net, IPSTATS_MIB_REASMTIMEOUT);
 +	IP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);
 +
 +	if ((qp->q.last_in & INET_FRAG_FIRST_IN) && qp->q.fragments != NULL) {
 +		struct sk_buff *head = qp->q.fragments;
 +		const struct iphdr *iph;
 +		int err;
 +
 +		rcu_read_lock();
 +		head->dev = dev_get_by_index_rcu(net, qp->iif);
 +		if (!head->dev)
 +			goto out_rcu_unlock;
 +
 +		/* skb has no dst, perform route lookup again */
 +		iph = ip_hdr(head);
 +		err = ip_route_input_noref(head, iph->daddr, iph->saddr,
++=======
+ 	__IP_INC_STATS(net, IPSTATS_MIB_REASMFAILS);
+ 	__IP_INC_STATS(net, IPSTATS_MIB_REASMTIMEOUT);
+ 
+ 	if (!qp->q.flags & INET_FRAG_FIRST_IN)
+ 		goto out;
+ 
+ 	/* sk_buff::dev and sk_buff::rbnode are unionized. So we
+ 	 * pull the head out of the tree in order to be able to
+ 	 * deal with head->dev.
+ 	 */
+ 	if (qp->q.fragments) {
+ 		head = qp->q.fragments;
+ 		qp->q.fragments = head->next;
+ 	} else {
+ 		head = skb_rb_first(&qp->q.rb_fragments);
+ 		if (!head)
+ 			goto out;
+ 		rb_erase(&head->rbnode, &qp->q.rb_fragments);
+ 		memset(&head->rbnode, 0, sizeof(head->rbnode));
+ 		barrier();
+ 	}
+ 	if (head == qp->q.fragments_tail)
+ 		qp->q.fragments_tail = NULL;
+ 
+ 	sub_frag_mem_limit(qp->q.net, head->truesize);
+ 
+ 	head->dev = dev_get_by_index_rcu(net, qp->iif);
+ 	if (!head->dev)
+ 		goto out;
+ 
+ 
+ 	/* skb has no dst, perform route lookup again */
+ 	iph = ip_hdr(head);
+ 	err = ip_route_input_noref(head, iph->daddr, iph->saddr,
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  					   iph->tos, head->dev);
 -	if (err)
 -		goto out;
 +		if (err)
 +			goto out_rcu_unlock;
  
 -	/* Only an end host needs to send an ICMP
 -	 * "Fragment Reassembly Timeout" message, per RFC792.
 -	 */
 -	if (frag_expire_skip_icmp(qp->q.key.v4.user) &&
 -	    (skb_rtable(head)->rt_type != RTN_LOCAL))
 -		goto out;
 +		/*
 +		 * Only an end host needs to send an ICMP
 +		 * "Fragment Reassembly Timeout" message, per RFC792.
 +		 */
 +		if (qp->user == IP_DEFRAG_AF_PACKET ||
 +		    (qp->user == IP_DEFRAG_CONNTRACK_IN &&
 +		     skb_rtable(head)->rt_type != RTN_LOCAL))
 +			goto out_rcu_unlock;
  
++<<<<<<< HEAD
++=======
+ 	spin_unlock(&qp->q.lock);
+ 	icmp_send(head, ICMP_TIME_EXCEEDED, ICMP_EXC_FRAGTIME, 0);
+ 	goto out_rcu_unlock;
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  
 +		/* Send an ICMP "Fragment Reassembly Timeout" message. */
 +		icmp_send(head, ICMP_TIME_EXCEEDED, ICMP_EXC_FRAGTIME, 0);
 +out_rcu_unlock:
 +		rcu_read_unlock();
 +	}
  out:
  	spin_unlock(&qp->q.lock);
++<<<<<<< HEAD
++=======
+ out_rcu_unlock:
+ 	rcu_read_unlock();
+ 	if (head)
+ 		kfree_skb(head);
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	ipq_put(qp);
  }
  
@@@ -309,17 -269,10 +365,22 @@@ static int ip_frag_reinit(struct ipq *q
  		return -ETIMEDOUT;
  	}
  
++<<<<<<< HEAD
 +	fp = qp->q.fragments;
 +	do {
 +		struct sk_buff *xp = fp->next;
 +
 +		sum_truesize += fp->truesize;
 +		kfree_skb(fp);
 +		fp = xp;
 +	} while (fp);
 +	sub_frag_mem_limit(&qp->q, sum_truesize);
++=======
+ 	sum_truesize = skb_rbtree_purge(&qp->q.rb_fragments);
+ 	sub_frag_mem_limit(qp->q.net, sum_truesize);
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  
 -	qp->q.flags = 0;
 +	qp->q.last_in = 0;
  	qp->q.len = 0;
  	qp->q.meat = 0;
  	qp->q.fragments = NULL;
@@@ -333,7 -287,9 +395,13 @@@
  /* Add new segment to existing queue. */
  static int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)
  {
++<<<<<<< HEAD
 +	struct sk_buff *prev, *next;
++=======
+ 	struct net *net = container_of(qp->q.net, struct net, ipv4.frags);
+ 	struct rb_node **rbn, *parent;
+ 	struct sk_buff *skb1;
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	struct net_device *dev;
  	unsigned int fragsize;
  	int flags, offset;
@@@ -396,94 -352,58 +464,150 @@@
  	if (err)
  		goto err;
  
++<<<<<<< HEAD
 +	/* Find out which fragments are in front and at the back of us
 +	 * in the chain of fragments so far.  We must know where to put
 +	 * this fragment, right?
 +	 */
 +	prev = qp->q.fragments_tail;
 +	if (!prev || FRAG_CB(prev)->offset < offset) {
 +		next = NULL;
 +		goto found;
 +	}
 +	prev = NULL;
 +	for (next = qp->q.fragments; next != NULL; next = next->next) {
 +		if (FRAG_CB(next)->offset >= offset)
 +			break;	/* bingo! */
 +		prev = next;
 +	}
 +
 +found:
 +	/* We found where to put this one.  Check for overlap with
 +	 * preceding fragment, and, if needed, align things so that
 +	 * any overlaps are eliminated.
++=======
+ 	/* Note : skb->rbnode and skb->dev share the same location. */
+ 	dev = skb->dev;
+ 	/* Makes sure compiler wont do silly aliasing games */
+ 	barrier();
+ 
+ 	/* RFC5722, Section 4, amended by Errata ID : 3089
+ 	 *                          When reassembling an IPv6 datagram, if
+ 	 *   one or more its constituent fragments is determined to be an
+ 	 *   overlapping fragment, the entire datagram (and any constituent
+ 	 *   fragments) MUST be silently discarded.
+ 	 *
+ 	 * We do the same here for IPv4 (and increment an snmp counter).
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	 */
 +	if (prev) {
 +		int i = (FRAG_CB(prev)->offset + prev->len) - offset;
 +
++<<<<<<< HEAD
 +		if (i > 0) {
 +			offset += i;
 +			err = -EINVAL;
 +			if (end <= offset)
 +				goto err;
 +			err = -ENOMEM;
 +			if (!pskb_pull(skb, i))
 +				goto err;
 +			if (skb->ip_summed != CHECKSUM_UNNECESSARY)
 +				skb->ip_summed = CHECKSUM_NONE;
 +		}
 +	}
 +
 +	err = -ENOMEM;
 +
 +	while (next && FRAG_CB(next)->offset < end) {
 +		int i = end - FRAG_CB(next)->offset; /* overlap is 'i' bytes */
 +
 +		if (i < next->len) {
 +			/* Eat head of the next overlapped fragment
 +			 * and leave the loop. The next ones cannot overlap.
 +			 */
 +			if (!pskb_pull(next, i))
 +				goto err;
 +			FRAG_CB(next)->offset += i;
 +			qp->q.meat -= i;
 +			if (next->ip_summed != CHECKSUM_UNNECESSARY)
 +				next->ip_summed = CHECKSUM_NONE;
 +			break;
 +		} else {
 +			struct sk_buff *free_it = next;
 +
 +			/* Old fragment is completely overridden with
 +			 * new one drop it.
 +			 */
 +			next = next->next;
 +
 +			if (prev)
 +				prev->next = next;
 +			else
 +				qp->q.fragments = next;
 +
 +			qp->q.meat -= free_it->len;
 +			sub_frag_mem_limit(&qp->q, free_it->truesize);
 +			kfree_skb(free_it);
 +		}
 +	}
 +
 +	FRAG_CB(skb)->offset = offset;
 +
 +	/* Insert this fragment in the chain of fragments. */
 +	skb->next = next;
 +	if (!next)
 +		qp->q.fragments_tail = skb;
 +	if (prev)
 +		prev->next = skb;
 +	else
 +		qp->q.fragments = skb;
  
 +	dev = skb->dev;
 +	if (dev) {
 +		qp->iif = dev->ifindex;
 +		skb->dev = NULL;
 +	}
++=======
+ 	/* Find out where to put this fragment.  */
+ 	skb1 = qp->q.fragments_tail;
+ 	if (!skb1) {
+ 		/* This is the first fragment we've received. */
+ 		rb_link_node(&skb->rbnode, NULL, &qp->q.rb_fragments.rb_node);
+ 		qp->q.fragments_tail = skb;
+ 	} else if ((skb1->ip_defrag_offset + skb1->len) < end) {
+ 		/* This is the common/special case: skb goes to the end. */
+ 		/* Detect and discard overlaps. */
+ 		if (offset < (skb1->ip_defrag_offset + skb1->len))
+ 			goto discard_qp;
+ 		/* Insert after skb1. */
+ 		rb_link_node(&skb->rbnode, &skb1->rbnode, &skb1->rbnode.rb_right);
+ 		qp->q.fragments_tail = skb;
+ 	} else {
+ 		/* Binary search. Note that skb can become the first fragment, but
+ 		 * not the last (covered above). */
+ 		rbn = &qp->q.rb_fragments.rb_node;
+ 		do {
+ 			parent = *rbn;
+ 			skb1 = rb_to_skb(parent);
+ 			if (end <= skb1->ip_defrag_offset)
+ 				rbn = &parent->rb_left;
+ 			else if (offset >= skb1->ip_defrag_offset + skb1->len)
+ 				rbn = &parent->rb_right;
+ 			else /* Found an overlap with skb1. */
+ 				goto discard_qp;
+ 		} while (*rbn);
+ 		/* Here we have parent properly set, and rbn pointing to
+ 		 * one of its NULL left/right children. Insert skb. */
+ 		rb_link_node(&skb->rbnode, parent, rbn);
+ 	}
+ 	rb_insert_color(&skb->rbnode, &qp->q.rb_fragments);
+ 
+ 	if (dev)
+ 		qp->iif = dev->ifindex;
+ 	skb->ip_defrag_offset = offset;
+ 
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	qp->q.stamp = skb->tstamp;
  	qp->q.meat += skb->len;
  	qp->ecn |= ecn;
@@@ -541,26 -464,21 +665,26 @@@ static int ip_frag_reasm(struct ipq *qp
  		goto out_fail;
  	}
  	/* Make the one we just received the head. */
- 	if (prev) {
- 		head = prev->next;
- 		fp = skb_clone(head, GFP_ATOMIC);
+ 	if (head != skb) {
+ 		fp = skb_clone(skb, GFP_ATOMIC);
  		if (!fp)
  			goto out_nomem;
- 
- 		fp->next = head->next;
- 		if (!fp->next)
+ 		rb_replace_node(&skb->rbnode, &fp->rbnode, &qp->q.rb_fragments);
+ 		if (qp->q.fragments_tail == skb)
  			qp->q.fragments_tail = fp;
- 		prev->next = fp;
- 
- 		skb_morph(head, qp->q.fragments);
- 		head->next = qp->q.fragments->next;
- 
- 		consume_skb(qp->q.fragments);
- 		qp->q.fragments = head;
+ 		skb_morph(skb, head);
+ 		rb_replace_node(&head->rbnode, &skb->rbnode,
+ 				&qp->q.rb_fragments);
+ 		consume_skb(head);
+ 		head = skb;
  	}
  
++<<<<<<< HEAD
 +	WARN_ON(head == NULL);
 +	WARN_ON(FRAG_CB(head)->offset != 0);
++=======
+ 	WARN_ON(head->ip_defrag_offset != 0);
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  
  	/* Allocate a new buffer for the datagram. */
  	ihlen = ip_hdrlen(head);
@@@ -581,26 -499,38 +705,41 @@@
  		struct sk_buff *clone;
  		int i, plen = 0;
  
 -		clone = alloc_skb(0, GFP_ATOMIC);
 -		if (!clone)
 +		if ((clone = alloc_skb(0, GFP_ATOMIC)) == NULL)
  			goto out_nomem;
- 		clone->next = head->next;
- 		head->next = clone;
  		skb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;
  		skb_frag_list_init(head);
  		for (i = 0; i < skb_shinfo(head)->nr_frags; i++)
  			plen += skb_frag_size(&skb_shinfo(head)->frags[i]);
  		clone->len = clone->data_len = head->data_len - plen;
- 		head->data_len -= clone->len;
- 		head->len -= clone->len;
+ 		skb->truesize += clone->truesize;
  		clone->csum = 0;
  		clone->ip_summed = head->ip_summed;
++<<<<<<< HEAD
 +		add_frag_mem_limit(&qp->q, clone->truesize);
++=======
+ 		add_frag_mem_limit(qp->q.net, clone->truesize);
+ 		skb_shinfo(head)->frag_list = clone;
+ 		nextp = &clone->next;
+ 	} else {
+ 		nextp = &skb_shinfo(head)->frag_list;
++>>>>>>> fa0f527358bd (ip: use rb trees for IP frag queue.)
  	}
  
- 	skb_shinfo(head)->frag_list = head->next;
  	skb_push(head, head->data - skb_network_header(head));
  
- 	for (fp=head->next; fp; fp = fp->next) {
+ 	/* Traverse the tree in order, to build frag_list. */
+ 	rbn = rb_next(&head->rbnode);
+ 	rb_erase(&head->rbnode, &qp->q.rb_fragments);
+ 	while (rbn) {
+ 		struct rb_node *rbnext = rb_next(rbn);
+ 		fp = rb_to_skb(rbn);
+ 		rb_erase(rbn, &qp->q.rb_fragments);
+ 		rbn = rbnext;
+ 		*nextp = fp;
+ 		nextp = &fp->next;
+ 		fp->prev = NULL;
+ 		memset(&fp->rbnode, 0, sizeof(fp->rbnode));
  		head->data_len += fp->len;
  		head->len += fp->len;
  		if (head->ip_summed != fp->ip_summed)
@@@ -609,9 -539,11 +748,11 @@@
  			head->csum = csum_add(head->csum, fp->csum);
  		head->truesize += fp->truesize;
  	}
 -	sub_frag_mem_limit(qp->q.net, head->truesize);
 +	sub_frag_mem_limit(&qp->q, head->truesize);
  
+ 	*nextp = NULL;
  	head->next = NULL;
+ 	head->prev = NULL;
  	head->dev = dev;
  	head->tstamp = qp->q.stamp;
  	IPCB(head)->frag_max_size = max(qp->max_df_size, qp->q.max_size);
@@@ -637,8 -569,9 +778,9 @@@
  
  	ip_send_check(iph);
  
 -	__IP_INC_STATS(net, IPSTATS_MIB_REASMOKS);
 +	IP_INC_STATS_BH(net, IPSTATS_MIB_REASMOKS);
  	qp->q.fragments = NULL;
+ 	qp->q.rb_fragments = RB_ROOT;
  	qp->q.fragments_tail = NULL;
  	return 0;
  
* Unmerged path include/linux/skbuff.h
* Unmerged path include/net/inet_frag.h
* Unmerged path net/ipv4/inet_fragment.c
* Unmerged path net/ipv4/ip_fragment.c
diff --git a/net/ipv6/netfilter/nf_conntrack_reasm.c b/net/ipv6/netfilter/nf_conntrack_reasm.c
index 9bbd5893ebea..1e3494d14ad3 100644
--- a/net/ipv6/netfilter/nf_conntrack_reasm.c
+++ b/net/ipv6/netfilter/nf_conntrack_reasm.c
@@ -467,6 +467,7 @@ nf_ct_frag6_reasm(struct frag_queue *fq, struct net_device *dev)
 					  head->csum);
 
 	fq->q.fragments = NULL;
+	fq->q.rb_fragments = RB_ROOT;
 	fq->q.fragments_tail = NULL;
 
 	/* all original skbs are linked into the NFCT_FRAG6_CB(head).orig */
diff --git a/net/ipv6/reassembly.c b/net/ipv6/reassembly.c
index ea7d2560bf2a..09d1f7f19f8f 100644
--- a/net/ipv6/reassembly.c
+++ b/net/ipv6/reassembly.c
@@ -504,6 +504,7 @@ static int ip6_frag_reasm(struct frag_queue *fq, struct sk_buff *prev,
 	IP6_INC_STATS_BH(net, __in6_dev_get(dev), IPSTATS_MIB_REASMOKS);
 	rcu_read_unlock();
 	fq->q.fragments = NULL;
+	fq->q.rb_fragments = RB_ROOT;
 	fq->q.fragments_tail = NULL;
 	return 1;
 
