x86/asm/memcpy_mcsafe: Remove loop unrolling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [x86] asm/memcpy_mcsafe: Remove loop unrolling (Jeff Moyer) [1608674]
Rebuild_FUZZ: 95.24%
commit-author Dan Williams <dan.j.williams@intel.com>
commit da7bc9c57eb0e91e048d05f7dbe5014a8b81ccfa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/da7bc9c5.failed

In preparation for teaching memcpy_mcsafe() to return 'bytes remaining'
rather than pass / fail, simplify the implementation to remove loop
unrolling. The unrolling complicates the fault handling for negligible
benefit given modern CPUs perform loop stream detection.

	Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Cc: Al Viro <viro@zeniv.linux.org.uk>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: hch@lst.de
	Cc: linux-fsdevel@vger.kernel.org
	Cc: linux-nvdimm@lists.01.org
Link: http://lkml.kernel.org/r/152539237092.31796.9115692316555638048.stgit@dwillia2-desk3.amr.corp.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit da7bc9c57eb0e91e048d05f7dbe5014a8b81ccfa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/string_64.h
#	arch/x86/lib/memcpy_64.S
diff --cc arch/x86/include/asm/string_64.h
index 7616dbb1c4ae,4752f8984923..000000000000
--- a/arch/x86/include/asm/string_64.h
+++ b/arch/x86/include/asm/string_64.h
@@@ -64,9 -97,28 +64,15 @@@ char *strcpy(char *dest, const char *sr
  char *strcat(char *dest, const char *src);
  int strcmp(const char *cs, const char *ct);
  
 -#if defined(CONFIG_KASAN) && !defined(__SANITIZE_ADDRESS__)
 -
 -/*
 - * For files that not instrumented (e.g. mm/slub.c) we
 - * should use not instrumented version of mem* functions.
 - */
 -
 -#undef memcpy
 -#define memcpy(dst, src, len) __memcpy(dst, src, len)
 -#define memmove(dst, src, len) __memmove(dst, src, len)
 -#define memset(s, c, n) __memset(s, c, n)
 -
 -#ifndef __NO_FORTIFY
 -#define __NO_FORTIFY /* FORTIFY_SOURCE uses __builtin_memcpy, etc. */
 -#endif
 -
 -#endif
 -
  #define __HAVE_ARCH_MEMCPY_MCSAFE 1
++<<<<<<< HEAD
 +extern struct static_key mcsafe_key;
 +__must_check int memcpy_mcsafe_unrolled(void *dst, const void *src, size_t cnt);
++=======
+ __must_check int __memcpy_mcsafe(void *dst, const void *src, size_t cnt);
+ DECLARE_STATIC_KEY_FALSE(mcsafe_key);
+ 
++>>>>>>> da7bc9c57eb0 (x86/asm/memcpy_mcsafe: Remove loop unrolling)
  /**
   * memcpy_mcsafe - copy memory with indication if a machine check happened
   *
@@@ -85,8 -137,8 +91,13 @@@ static __always_inline __must_check in
  memcpy_mcsafe(void *dst, const void *src, size_t cnt)
  {
  #ifdef CONFIG_X86_MCE
++<<<<<<< HEAD
 +	if (static_key_false(&mcsafe_key))
 +		return memcpy_mcsafe_unrolled(dst, src, cnt);
++=======
+ 	if (static_branch_unlikely(&mcsafe_key))
+ 		return __memcpy_mcsafe(dst, src, cnt);
++>>>>>>> da7bc9c57eb0 (x86/asm/memcpy_mcsafe: Remove loop unrolling)
  	else
  #endif
  		memcpy(dst, src, cnt);
diff --cc arch/x86/lib/memcpy_64.S
index 36b962df086c,54c971892db5..000000000000
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@@ -299,7 -245,8 +268,12 @@@ ENTRY(__memcpy_mcsafe
  .L_done_memcpy_trap:
  	xorq %rax, %rax
  	ret
++<<<<<<< HEAD
 +ENDPROC(memcpy_mcsafe_unrolled)
++=======
+ ENDPROC(__memcpy_mcsafe)
+ EXPORT_SYMBOL_GPL(__memcpy_mcsafe)
++>>>>>>> da7bc9c57eb0 (x86/asm/memcpy_mcsafe: Remove loop unrolling)
  
  	.section .fixup, "ax"
  	/* Return -EFAULT for any failure */
* Unmerged path arch/x86/include/asm/string_64.h
* Unmerged path arch/x86/lib/memcpy_64.S
