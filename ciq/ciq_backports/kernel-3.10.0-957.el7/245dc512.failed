net: sched: cls_u32: call block callbacks for offload

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [net] sched: cls_u32: call block callbacks for offload (Ivan Vecera) [1572720]
Rebuild_FUZZ: 95.05%
commit-author Jiri Pirko <jiri@mellanox.com>
commit 245dc5121a9bf6a0a12ac1e72f47822fc3fa8cae
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/245dc512.failed

Use the newly introduced callbacks infrastructure and call block
callbacks alongside with the existing per-netdev ndo_setup_tc.

	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 245dc5121a9bf6a0a12ac1e72f47822fc3fa8cae)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_u32.c
diff --cc net/sched/cls_u32.c
index 42766c25ee6a,d53da7968eda..000000000000
--- a/net/sched/cls_u32.c
+++ b/net/sched/cls_u32.c
@@@ -500,7 -474,9 +498,13 @@@ static void u32_clear_hw_hnode(struct t
  	cls_u32.hnode.handle = h->handle;
  	cls_u32.hnode.prio = h->prio;
  
++<<<<<<< HEAD
 +	__rh_call_ndo_setup_tc(dev, TC_SETUP_CLSU32, &cls_u32);
++=======
+ 	if (tc_can_offload(dev))
+ 		dev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_CLSU32, &cls_u32);
+ 	tc_setup_cb_call(block, NULL, TC_SETUP_CLSU32, &cls_u32, false);
++>>>>>>> 245dc5121a9b (net: sched: cls_u32: call block callbacks for offload)
  }
  
  static int u32_replace_hw_hnode(struct tcf_proto *tp, struct tc_u_hnode *h,
@@@ -519,9 -495,27 +523,32 @@@
  	cls_u32.hnode.handle = h->handle;
  	cls_u32.hnode.prio = h->prio;
  
++<<<<<<< HEAD
 +	err = __rh_call_ndo_setup_tc(dev, TC_SETUP_CLSU32, &cls_u32);
 +	if (tc_skip_sw(flags))
++=======
+ 	if (tc_can_offload(dev)) {
+ 		err = dev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_CLSU32,
+ 						    &cls_u32);
+ 		if (err) {
+ 			if (skip_sw)
+ 				return err;
+ 		} else {
+ 			offloaded = true;
+ 		}
+ 	}
+ 
+ 	err = tc_setup_cb_call(block, NULL, TC_SETUP_CLSU32, &cls_u32, skip_sw);
+ 	if (err < 0) {
+ 		u32_clear_hw_hnode(tp, h);
++>>>>>>> 245dc5121a9b (net: sched: cls_u32: call block callbacks for offload)
  		return err;
+ 	} else if (err > 0) {
+ 		offloaded = true;
+ 	}
+ 
+ 	if (skip_sw && !offloaded)
+ 		return -EINVAL;
  
  	return 0;
  }
@@@ -538,7 -530,9 +563,13 @@@ static void u32_remove_hw_knode(struct 
  	cls_u32.command = TC_CLSU32_DELETE_KNODE;
  	cls_u32.knode.handle = handle;
  
++<<<<<<< HEAD
 +	__rh_call_ndo_setup_tc(dev, TC_SETUP_CLSU32, &cls_u32);
++=======
+ 	if (tc_can_offload(dev))
+ 		dev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_CLSU32, &cls_u32);
+ 	tc_setup_cb_call(block, NULL, TC_SETUP_CLSU32, &cls_u32, false);
++>>>>>>> 245dc5121a9b (net: sched: cls_u32: call block callbacks for offload)
  }
  
  static int u32_replace_hw_knode(struct tcf_proto *tp, struct tc_u_knode *n,
@@@ -567,13 -560,28 +597,32 @@@
  	if (n->ht_down)
  		cls_u32.knode.link_handle = n->ht_down->handle;
  
++<<<<<<< HEAD
 +	err = __rh_call_ndo_setup_tc(dev, TC_SETUP_CLSU32, &cls_u32);
++=======
++>>>>>>> 245dc5121a9b (net: sched: cls_u32: call block callbacks for offload)
+ 
+ 	if (tc_can_offload(dev)) {
+ 		err = dev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_CLSU32,
+ 						    &cls_u32);
+ 		if (err) {
+ 			if (skip_sw)
+ 				return err;
+ 		} else {
+ 			n->flags |= TCA_CLS_FLAGS_IN_HW;
+ 		}
+ 	}
  
- 	if (!err)
+ 	err = tc_setup_cb_call(block, NULL, TC_SETUP_CLSU32, &cls_u32, skip_sw);
+ 	if (err < 0) {
+ 		u32_remove_hw_knode(tp, n->handle);
+ 		return err;
+ 	} else if (err > 0) {
  		n->flags |= TCA_CLS_FLAGS_IN_HW;
+ 	}
  
- 	if (tc_skip_sw(flags))
- 		return err;
+ 	if (skip_sw && !(n->flags && TCA_CLS_FLAGS_IN_HW))
+ 		return -EINVAL;
  
  	return 0;
  }
* Unmerged path net/sched/cls_u32.c
