nfp: bpf: implement memory bulk copy for length within 32-bytes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
commit-author Jiong Wang <jiong.wang@netronome.com>
commit 9879a3814beb3b1350755475e67a8d92ba1f7e4b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/9879a381.failed

For NFP, we want to re-group a sequence of load/store pairs lowered from
memcpy/memmove into single memory bulk operation which then could be
accelerated using NFP CPP bus.

This patch extends the existing load/store auxiliary information by adding
two new fields:

	struct bpf_insn *paired_st;
	s16 ldst_gather_len;

Both fields are supposed to be carried by the the load instruction at the
head of the sequence. "paired_st" is the corresponding store instruction at
the head and "ldst_gather_len" is the gathered length.

If "ldst_gather_len" is negative, then the sequence is doing memory
load/store in descending order, otherwise it is in ascending order. We need
this information to detect overlapped memory access.

This patch then optimize memory bulk copy when the copy length is within
32-bytes.

The strategy of read/write used is:

  * Read.
    Use read32 (direct_ref), always.

  * Write.
    - length <= 8-bytes
      write8 (direct_ref).
    - length <= 32-bytes and is 4-byte aligned
      write32 (direct_ref).
    - length <= 32-bytes but is not 4-byte aligned
      write8 (indirect_ref).

NOTE: the optimization should not change program semantics. The destination
register of the last load instruction should contain the same value before
and after this optimization.

	Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
	Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
(cherry picked from commit 9879a3814beb3b1350755475e67a8d92ba1f7e4b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/bpf/main.h
#	drivers/net/ethernet/netronome/nfp/nfp_asm.h
diff --cc drivers/net/ethernet/netronome/nfp/bpf/main.h
index 5212b54abaf7,5884291ddba5..000000000000
--- a/drivers/net/ethernet/netronome/nfp/bpf/main.h
+++ b/drivers/net/ethernet/netronome/nfp/bpf/main.h
@@@ -100,17 -95,31 +100,36 @@@ typedef int (*instr_cb_t)(struct nfp_pr
   * struct nfp_insn_meta - BPF instruction wrapper
   * @insn: BPF instruction
   * @ptr: pointer type for memory operations
++<<<<<<< HEAD
++=======
+  * @ldst_gather_len: memcpy length gathered from load/store sequence
+  * @paired_st: the paired store insn at the head of the sequence
+  * @ptr_not_const: pointer is not always constant
+  * @jmp_dst: destination info for jump instructions
++>>>>>>> 9879a3814beb (nfp: bpf: implement memory bulk copy for length within 32-bytes)
   * @off: index of first generated machine instruction (in nfp_prog.prog)
   * @n: eBPF instruction number
 - * @flags: eBPF instruction extra optimization flags
   * @skip: skip this instruction (optimized out)
   * @double_cb: callback for second part of the instruction
   * @l: link on nfp_prog->insns list
   */
  struct nfp_insn_meta {
  	struct bpf_insn insn;
++<<<<<<< HEAD
 +	struct bpf_reg_state ptr;
++=======
+ 	union {
+ 		struct {
+ 			struct bpf_reg_state ptr;
+ 			struct bpf_insn *paired_st;
+ 			s16 ldst_gather_len;
+ 			bool ptr_not_const;
+ 		};
+ 		struct nfp_insn_meta *jmp_dst;
+ 	};
++>>>>>>> 9879a3814beb (nfp: bpf: implement memory bulk copy for length within 32-bytes)
  	unsigned int off;
  	unsigned short n;
 -	unsigned short flags;
  	bool skip;
  	instr_cb_t double_cb;
  
diff --cc drivers/net/ethernet/netronome/nfp/nfp_asm.h
index 84fa9577808f,98803f9f40b6..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_asm.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_asm.h
@@@ -231,11 -242,16 +232,24 @@@ enum cmd_ctx_swap 
  	CMD_CTX_NO_SWAP = 3,
  };
  
++<<<<<<< HEAD
 +#define OP_LCSR_BASE	0x0fc00000000ULL
 +#define OP_LCSR_A_SRC	0x000000003ffULL
 +#define OP_LCSR_B_SRC	0x000000ffc00ULL
 +#define OP_LCSR_WRITE	0x00000200000ULL
 +#define OP_LCSR_ADDR	0x001ffc00000ULL
++=======
+ #define CMD_OVE_LEN	BIT(7)
+ #define CMD_OV_LEN	GENMASK(12, 8)
+ 
+ #define OP_LCSR_BASE		0x0fc00000000ULL
+ #define OP_LCSR_A_SRC		0x000000003ffULL
+ #define OP_LCSR_B_SRC		0x000000ffc00ULL
+ #define OP_LCSR_WRITE		0x00000200000ULL
+ #define OP_LCSR_ADDR		0x001ffc00000ULL
+ #define OP_LCSR_SRC_LMEXTN	0x40000000000ULL
+ #define OP_LCSR_DST_LMEXTN	0x80000000000ULL
++>>>>>>> 9879a3814beb (nfp: bpf: implement memory bulk copy for length within 32-bytes)
  
  enum lcsr_wr_src {
  	LCSR_WR_AREG,
diff --git a/drivers/net/ethernet/netronome/nfp/bpf/jit.c b/drivers/net/ethernet/netronome/nfp/bpf/jit.c
index 35648b968309..18c9af07ae04 100644
--- a/drivers/net/ethernet/netronome/nfp/bpf/jit.c
+++ b/drivers/net/ethernet/netronome/nfp/bpf/jit.c
@@ -157,6 +157,13 @@ emit_cmd(struct nfp_prog *nfp_prog, enum cmd_tgt_map op,
 	__emit_cmd(nfp_prog, op, mode, xfer, reg.areg, reg.breg, size, sync);
 }
 
+static void
+emit_cmd_indir(struct nfp_prog *nfp_prog, enum cmd_tgt_map op, u8 mode, u8 xfer,
+	       swreg lreg, swreg rreg, u8 size, bool sync)
+{
+	emit_cmd_any(nfp_prog, op, mode, xfer, lreg, rreg, size, sync, true);
+}
+
 static void
 __emit_br(struct nfp_prog *nfp_prog, enum br_mask mask, enum br_ev_pip ev_pip,
 	  enum br_ctx_signal_state css, u16 addr, u8 defer)
@@ -498,6 +505,109 @@ static void wrp_reg_mov(struct nfp_prog *nfp_prog, u16 dst, u16 src)
 	wrp_mov(nfp_prog, reg_both(dst), reg_b(src));
 }
 
+/* wrp_reg_subpart() - load @field_len bytes from @offset of @src, write the
+ * result to @dst from low end.
+ */
+static void
+wrp_reg_subpart(struct nfp_prog *nfp_prog, swreg dst, swreg src, u8 field_len,
+		u8 offset)
+{
+	enum shf_sc sc = offset ? SHF_SC_R_SHF : SHF_SC_NONE;
+	u8 mask = (1 << field_len) - 1;
+
+	emit_ld_field_any(nfp_prog, dst, mask, src, sc, offset * 8, true);
+}
+
+/* NFP has Command Push Pull bus which supports bluk memory operations. */
+static int nfp_cpp_memcpy(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta)
+{
+	bool descending_seq = meta->ldst_gather_len < 0;
+	s16 len = abs(meta->ldst_gather_len);
+	swreg src_base, off;
+	unsigned int i;
+	u8 xfer_num;
+
+	if (WARN_ON_ONCE(len > 32))
+		return -EOPNOTSUPP;
+
+	off = re_load_imm_any(nfp_prog, meta->insn.off, imm_b(nfp_prog));
+	src_base = reg_a(meta->insn.src_reg * 2);
+	xfer_num = round_up(len, 4) / 4;
+
+	/* Memory read from source addr into transfer-in registers. */
+	emit_cmd(nfp_prog, CMD_TGT_READ32_SWAP, CMD_MODE_32b, 0, src_base, off,
+		 xfer_num - 1, true);
+
+	/* Move from transfer-in to transfer-out. */
+	for (i = 0; i < xfer_num; i++)
+		wrp_mov(nfp_prog, reg_xfer(i), reg_xfer(i));
+
+	off = re_load_imm_any(nfp_prog, meta->paired_st->off, imm_b(nfp_prog));
+
+	if (len <= 8) {
+		/* Use single direct_ref write8. */
+		emit_cmd(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 0,
+			 reg_a(meta->paired_st->dst_reg * 2), off, len - 1,
+			 true);
+	} else if (IS_ALIGNED(len, 4)) {
+		/* Use single direct_ref write32. */
+		emit_cmd(nfp_prog, CMD_TGT_WRITE32_SWAP, CMD_MODE_32b, 0,
+			 reg_a(meta->paired_st->dst_reg * 2), off, xfer_num - 1,
+			 true);
+	} else {
+		/* Use single indirect_ref write8. */
+		wrp_immed(nfp_prog, reg_none(),
+			  CMD_OVE_LEN | FIELD_PREP(CMD_OV_LEN, len - 1));
+		emit_cmd_indir(nfp_prog, CMD_TGT_WRITE8_SWAP, CMD_MODE_32b, 0,
+			       reg_a(meta->paired_st->dst_reg * 2), off,
+			       len - 1, true);
+	}
+
+	/* TODO: The following extra load is to make sure data flow be identical
+	 *  before and after we do memory copy optimization.
+	 *
+	 *  The load destination register is not guaranteed to be dead, so we
+	 *  need to make sure it is loaded with the value the same as before
+	 *  this transformation.
+	 *
+	 *  These extra loads could be removed once we have accurate register
+	 *  usage information.
+	 */
+	if (descending_seq)
+		xfer_num = 0;
+	else if (BPF_SIZE(meta->insn.code) != BPF_DW)
+		xfer_num = xfer_num - 1;
+	else
+		xfer_num = xfer_num - 2;
+
+	switch (BPF_SIZE(meta->insn.code)) {
+	case BPF_B:
+		wrp_reg_subpart(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+				reg_xfer(xfer_num), 1,
+				IS_ALIGNED(len, 4) ? 3 : (len & 3) - 1);
+		break;
+	case BPF_H:
+		wrp_reg_subpart(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+				reg_xfer(xfer_num), 2, (len & 3) ^ 2);
+		break;
+	case BPF_W:
+		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+			reg_xfer(0));
+		break;
+	case BPF_DW:
+		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2),
+			reg_xfer(xfer_num));
+		wrp_mov(nfp_prog, reg_both(meta->insn.dst_reg * 2 + 1),
+			reg_xfer(xfer_num + 1));
+		break;
+	}
+
+	if (BPF_SIZE(meta->insn.code) != BPF_DW)
+		wrp_immed(nfp_prog, reg_both(meta->insn.dst_reg * 2 + 1), 0);
+
+	return 0;
+}
+
 static int
 data_ld(struct nfp_prog *nfp_prog, swreg offset, u8 dst_gpr, int size)
 {
@@ -1391,6 +1501,9 @@ static int
 mem_ldx(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,
 	unsigned int size)
 {
+	if (meta->ldst_gather_len)
+		return nfp_cpp_memcpy(nfp_prog, meta);
+
 	if (meta->ptr.type == PTR_TO_CTX) {
 		if (nfp_prog->act == NN_ACT_XDP)
 			return mem_ldx_xdp(nfp_prog, meta, size);
* Unmerged path drivers/net/ethernet/netronome/nfp/bpf/main.h
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_asm.c b/drivers/net/ethernet/netronome/nfp/nfp_asm.c
index 1fbe2c4bc84e..e0e6e2399c2e 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_asm.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_asm.c
@@ -41,6 +41,7 @@
 
 const struct cmd_tgt_act cmd_tgt_act[__CMD_TGT_MAP_SIZE] = {
 	[CMD_TGT_WRITE8_SWAP] =		{ 0x02, 0x42 },
+	[CMD_TGT_WRITE32_SWAP] =	{ 0x02, 0x5f },
 	[CMD_TGT_READ8] =		{ 0x01, 0x43 },
 	[CMD_TGT_READ32] =		{ 0x00, 0x5c },
 	[CMD_TGT_READ32_LE] =		{ 0x01, 0x5c },
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_asm.h
