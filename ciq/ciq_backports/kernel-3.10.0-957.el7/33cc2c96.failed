acpi, nfit: Fix scrub idle detection

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-957.el7
Rebuild_CHGLOG: - [acpi] nfit: Fix scrub idle detection (Jeff Moyer) [1616041]
Rebuild_FUZZ: 90.91%
commit-author Dan Williams <dan.j.williams@intel.com>
commit 33cc2c9667561b224215e6dfb5bf98e8fa17914e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-957.el7/33cc2c96.failed

The notification of scrub completion happens within the scrub workqueue.
That can clearly race someone running scrub_show() and work_busy()
before the workqueue has a chance to flush the recently completed work.
Add a flag to reliably indicate the idle vs busy state. Without this
change applications using poll(2) to wait for scrub-completion may
falsely wakeup and read ARS as being busy even though the thread is
going idle and then hang indefinitely.

Fixes: bc6ba8085842 ("nfit, address-range-scrub: rework and simplify ARS...")
	Cc: <stable@vger.kernel.org>
	Reported-by: Vishal Verma <vishal.l.verma@intel.com>
	Tested-by: Vishal Verma <vishal.l.verma@intel.com>
	Reported-by: Lukasz Dorau <lukasz.dorau@intel.com>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 33cc2c9667561b224215e6dfb5bf98e8fa17914e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/acpi/nfit/core.c
diff --cc drivers/acpi/nfit/core.c
index b51217b6f576,b8040fed2a69..000000000000
--- a/drivers/acpi/nfit/core.c
+++ b/drivers/acpi/nfit/core.c
@@@ -1231,7 -1275,7 +1231,11 @@@ static ssize_t scrub_show(struct devic
  
  		mutex_lock(&acpi_desc->init_mutex);
  		rc = sprintf(buf, "%d%s", acpi_desc->scrub_count,
++<<<<<<< HEAD
 +				work_busy(&acpi_desc->work)
++=======
+ 				acpi_desc->scrub_busy
++>>>>>>> 33cc2c966756 (acpi, nfit: Fix scrub idle detection)
  				&& !acpi_desc->cancel ? "+\n" : "\n");
  		mutex_unlock(&acpi_desc->init_mutex);
  	}
@@@ -2742,223 -2838,150 +2746,259 @@@ static int acpi_nfit_query_poison(struc
  	return 0;
  }
  
 -static int ars_register(struct acpi_nfit_desc *acpi_desc, struct nfit_spa *nfit_spa,
 -		int *query_rc)
 +static void acpi_nfit_async_scrub(struct acpi_nfit_desc *acpi_desc,
 +		struct nfit_spa *nfit_spa)
  {
 -	int rc = *query_rc;
 +	struct acpi_nfit_system_address *spa = nfit_spa->spa;
 +	unsigned int overflow_retry = scrub_overflow_abort;
 +	u64 init_ars_start = 0, init_ars_len = 0;
 +	struct device *dev = acpi_desc->dev;
 +	unsigned int tmo = scrub_timeout;
 +	int rc;
  
 -	if (no_init_ars)
 -		return acpi_nfit_register_region(acpi_desc, nfit_spa);
 +	if (!test_bit(ARS_REQ, &nfit_spa->ars_state) || !nfit_spa->nd_region)
 +		return;
  
 -	set_bit(ARS_REQ, &nfit_spa->ars_state);
 -	set_bit(ARS_SHORT, &nfit_spa->ars_state);
 +	rc = ars_start(acpi_desc, nfit_spa);
 +	/*
 +	 * If we timed out the initial scan we'll still be busy here,
 +	 * and will wait another timeout before giving up permanently.
 +	 */
 +	if (rc < 0 && rc != -EBUSY)
 +		return;
  
 -	switch (rc) {
 -	case 0:
 -	case -EAGAIN:
 -		rc = ars_start(acpi_desc, nfit_spa);
 -		if (rc == -EBUSY) {
 -			*query_rc = rc;
 +	do {
 +		u64 ars_start, ars_len;
 +
 +		if (acpi_desc->cancel)
  			break;
 -		} else if (rc == 0) {
 -			rc = acpi_nfit_query_poison(acpi_desc);
 -		} else {
 -			set_bit(ARS_FAILED, &nfit_spa->ars_state);
 +		rc = acpi_nfit_query_poison(acpi_desc);
 +		if (rc == -ENOTTY)
 +			break;
 +		if (rc == -EBUSY && !tmo) {
 +			dev_warn(dev, "range %d ars timeout, aborting\n",
 +					spa->range_index);
  			break;
  		}
 -		if (rc == -EAGAIN)
 -			clear_bit(ARS_SHORT, &nfit_spa->ars_state);
 -		else if (rc == 0)
 -			ars_complete(acpi_desc, nfit_spa);
 -		break;
 -	case -EBUSY:
 -	case -ENOSPC:
 -		break;
 -	default:
 -		set_bit(ARS_FAILED, &nfit_spa->ars_state);
 -		break;
 -	}
  
 -	if (test_and_clear_bit(ARS_DONE, &nfit_spa->ars_state))
 -		set_bit(ARS_REQ, &nfit_spa->ars_state);
 -
 -	return acpi_nfit_register_region(acpi_desc, nfit_spa);
 -}
 -
 -static void ars_complete_all(struct acpi_nfit_desc *acpi_desc)
 -{
 -	struct nfit_spa *nfit_spa;
 -
 -	list_for_each_entry(nfit_spa, &acpi_desc->spas, list) {
 -		if (test_bit(ARS_FAILED, &nfit_spa->ars_state))
 +		if (rc == -EBUSY) {
 +			/*
 +			 * Note, entries may be appended to the list
 +			 * while the lock is dropped, but the workqueue
 +			 * being active prevents entries being deleted /
 +			 * freed.
 +			 */
 +			mutex_unlock(&acpi_desc->init_mutex);
 +			ssleep(1);
 +			tmo--;
 +			mutex_lock(&acpi_desc->init_mutex);
  			continue;
 -		ars_complete(acpi_desc, nfit_spa);
 -	}
 -}
 -
 -static unsigned int __acpi_nfit_scrub(struct acpi_nfit_desc *acpi_desc,
 -		int query_rc)
 -{
 -	unsigned int tmo = acpi_desc->scrub_tmo;
 -	struct device *dev = acpi_desc->dev;
 -	struct nfit_spa *nfit_spa;
 -
 -	if (acpi_desc->cancel)
 -		return 0;
 +		}
  
 -	if (query_rc == -EBUSY) {
 -		dev_dbg(dev, "ARS: ARS busy\n");
 -		return min(30U * 60U, tmo * 2);
 -	}
 -	if (query_rc == -ENOSPC) {
 -		dev_dbg(dev, "ARS: ARS continue\n");
 -		ars_continue(acpi_desc);
 -		return 1;
 -	}
 -	if (query_rc && query_rc != -EAGAIN) {
 -		unsigned long long addr, end;
 +		/* we got some results, but there are more pending... */
 +		if (rc == -ENOSPC && overflow_retry--) {
 +			if (!init_ars_len) {
 +				init_ars_len = acpi_desc->ars_status->length;
 +				init_ars_start = acpi_desc->ars_status->address;
 +			}
 +			rc = ars_continue(acpi_desc);
 +		}
  
 -		addr = acpi_desc->ars_status->address;
 -		end = addr + acpi_desc->ars_status->length;
 -		dev_dbg(dev, "ARS: %llx-%llx failed (%d)\n", addr, end,
 -				query_rc);
 -	}
 +		if (rc < 0) {
 +			dev_warn(dev, "range %d ars continuation failed\n",
 +					spa->range_index);
 +			break;
 +		}
  
 -	ars_complete_all(acpi_desc);
 -	list_for_each_entry(nfit_spa, &acpi_desc->spas, list) {
 -		if (test_bit(ARS_FAILED, &nfit_spa->ars_state))
 -			continue;
 -		if (test_bit(ARS_REQ, &nfit_spa->ars_state)) {
 -			int rc = ars_start(acpi_desc, nfit_spa);
 -
 -			clear_bit(ARS_DONE, &nfit_spa->ars_state);
 -			dev = nd_region_dev(nfit_spa->nd_region);
 -			dev_dbg(dev, "ARS: range %d ARS start (%d)\n",
 -					nfit_spa->spa->range_index, rc);
 -			if (rc == 0 || rc == -EBUSY)
 -				return 1;
 -			dev_err(dev, "ARS: range %d ARS failed (%d)\n",
 -					nfit_spa->spa->range_index, rc);
 -			set_bit(ARS_FAILED, &nfit_spa->ars_state);
 +		if (init_ars_len) {
 +			ars_start = init_ars_start;
 +			ars_len = init_ars_len;
 +		} else {
 +			ars_start = acpi_desc->ars_status->address;
 +			ars_len = acpi_desc->ars_status->length;
  		}
 -	}
 -	return 0;
 +		dev_dbg(dev, "spa range: %d ars from %#llx + %#llx complete\n",
 +				spa->range_index, ars_start, ars_len);
 +		/* notify the region about new poison entries */
 +		nvdimm_region_notify(nfit_spa->nd_region,
 +				NVDIMM_REVALIDATE_POISON);
 +		break;
 +	} while (1);
  }
  
+ static void __sched_ars(struct acpi_nfit_desc *acpi_desc, unsigned int tmo)
+ {
+ 	lockdep_assert_held(&acpi_desc->init_mutex);
+ 
+ 	acpi_desc->scrub_busy = 1;
+ 	/* note this should only be set from within the workqueue */
+ 	if (tmo)
+ 		acpi_desc->scrub_tmo = tmo;
+ 	queue_delayed_work(nfit_wq, &acpi_desc->dwork, tmo * HZ);
+ }
+ 
+ static void sched_ars(struct acpi_nfit_desc *acpi_desc)
+ {
+ 	__sched_ars(acpi_desc, 0);
+ }
+ 
+ static void notify_ars_done(struct acpi_nfit_desc *acpi_desc)
+ {
+ 	lockdep_assert_held(&acpi_desc->init_mutex);
+ 
+ 	acpi_desc->scrub_busy = 0;
+ 	acpi_desc->scrub_count++;
+ 	if (acpi_desc->scrub_count_state)
+ 		sysfs_notify_dirent(acpi_desc->scrub_count_state);
+ }
+ 
  static void acpi_nfit_scrub(struct work_struct *work)
  {
 +	struct device *dev;
 +	u64 init_scrub_length = 0;
 +	struct nfit_spa *nfit_spa;
 +	u64 init_scrub_address = 0;
 +	bool init_ars_done = false;
  	struct acpi_nfit_desc *acpi_desc;
 -	unsigned int tmo;
 -	int query_rc;
 +	unsigned int tmo = scrub_timeout;
 +	unsigned int overflow_retry = scrub_overflow_abort;
  
 -	acpi_desc = container_of(work, typeof(*acpi_desc), dwork.work);
 +	acpi_desc = container_of(work, typeof(*acpi_desc), work);
 +	dev = acpi_desc->dev;
 +
 +	/*
 +	 * We scrub in 2 phases.  The first phase waits for any platform
 +	 * firmware initiated scrubs to complete and then we go search for the
 +	 * affected spa regions to mark them scanned.  In the second phase we
 +	 * initiate a directed scrub for every range that was not scrubbed in
 +	 * phase 1. If we're called for a 'rescan', we harmlessly pass through
 +	 * the first phase, but really only care about running phase 2, where
 +	 * regions can be notified of new poison.
 +	 */
 +
 +	/* process platform firmware initiated scrubs */
 + retry:
  	mutex_lock(&acpi_desc->init_mutex);
++<<<<<<< HEAD
 +	list_for_each_entry(nfit_spa, &acpi_desc->spas, list) {
 +		struct nd_cmd_ars_status *ars_status;
 +		struct acpi_nfit_system_address *spa;
 +		u64 ars_start, ars_len;
 +		int rc;
 +
 +		if (acpi_desc->cancel)
 +			break;
 +
 +		if (nfit_spa->nd_region)
 +			continue;
 +
 +		if (init_ars_done) {
 +			/*
 +			 * No need to re-query, we're now just
 +			 * reconciling all the ranges covered by the
 +			 * initial scrub
 +			 */
 +			rc = 0;
 +		} else
 +			rc = acpi_nfit_query_poison(acpi_desc);
 +
 +		if (rc == -ENOTTY) {
 +			/* no ars capability, just register spa and move on */
 +			acpi_nfit_register_region(acpi_desc, nfit_spa);
 +			continue;
 +		}
 +
 +		if (rc == -EBUSY && !tmo) {
 +			/* fallthrough to directed scrub in phase 2 */
 +			dev_warn(dev, "timeout awaiting ars results, continuing...\n");
 +			break;
 +		} else if (rc == -EBUSY) {
 +			mutex_unlock(&acpi_desc->init_mutex);
 +			ssleep(1);
 +			tmo--;
 +			goto retry;
 +		}
 +
 +		/* we got some results, but there are more pending... */
 +		if (rc == -ENOSPC && overflow_retry--) {
 +			ars_status = acpi_desc->ars_status;
 +			/*
 +			 * Record the original scrub range, so that we
 +			 * can recall all the ranges impacted by the
 +			 * initial scrub.
 +			 */
 +			if (!init_scrub_length) {
 +				init_scrub_length = ars_status->length;
 +				init_scrub_address = ars_status->address;
 +			}
 +			rc = ars_continue(acpi_desc);
 +			if (rc == 0) {
 +				mutex_unlock(&acpi_desc->init_mutex);
 +				goto retry;
 +			}
 +		}
 +
 +		if (rc < 0) {
 +			/*
 +			 * Initial scrub failed, we'll give it one more
 +			 * try below...
 +			 */
 +			break;
 +		}
 +
 +		/* We got some final results, record completed ranges */
 +		ars_status = acpi_desc->ars_status;
 +		if (init_scrub_length) {
 +			ars_start = init_scrub_address;
 +			ars_len = ars_start + init_scrub_length;
 +		} else {
 +			ars_start = ars_status->address;
 +			ars_len = ars_status->length;
 +		}
 +		spa = nfit_spa->spa;
 +
 +		if (!init_ars_done) {
 +			init_ars_done = true;
 +			dev_dbg(dev, "init scrub %#llx + %#llx complete\n",
 +					ars_start, ars_len);
 +		}
 +		if (ars_start <= spa->address && ars_start + ars_len
 +				>= spa->address + spa->length)
 +			acpi_nfit_register_region(acpi_desc, nfit_spa);
 +	}
 +
 +	/*
 +	 * For all the ranges not covered by an initial scrub we still
 +	 * want to see if there are errors, but it's ok to discover them
 +	 * asynchronously.
 +	 */
 +	list_for_each_entry(nfit_spa, &acpi_desc->spas, list) {
 +		/*
 +		 * Flag all the ranges that still need scrubbing, but
 +		 * register them now to make data available.
 +		 */
 +		if (!nfit_spa->nd_region) {
 +			set_bit(ARS_REQ, &nfit_spa->ars_state);
 +			acpi_nfit_register_region(acpi_desc, nfit_spa);
 +		}
 +	}
 +	acpi_desc->init_complete = 1;
 +
 +	list_for_each_entry(nfit_spa, &acpi_desc->spas, list)
 +		acpi_nfit_async_scrub(acpi_desc, nfit_spa);
 +	acpi_desc->scrub_count++;
 +	acpi_desc->ars_start_flags = 0;
 +	if (acpi_desc->scrub_count_state)
 +		sysfs_notify_dirent(acpi_desc->scrub_count_state);
++=======
+ 	query_rc = acpi_nfit_query_poison(acpi_desc);
+ 	tmo = __acpi_nfit_scrub(acpi_desc, query_rc);
+ 	if (tmo)
+ 		__sched_ars(acpi_desc, tmo);
+ 	else
+ 		notify_ars_done(acpi_desc);
+ 	memset(acpi_desc->ars_status, 0, acpi_desc->max_ars);
++>>>>>>> 33cc2c966756 (acpi, nfit: Fix scrub idle detection)
  	mutex_unlock(&acpi_desc->init_mutex);
  }
  
@@@ -2990,27 -3014,54 +3030,66 @@@ static void acpi_nfit_init_ars(struct a
  static int acpi_nfit_register_regions(struct acpi_nfit_desc *acpi_desc)
  {
  	struct nfit_spa *nfit_spa;
 -	int rc, query_rc;
  
  	list_for_each_entry(nfit_spa, &acpi_desc->spas, list) {
 -		set_bit(ARS_FAILED, &nfit_spa->ars_state);
 -		switch (nfit_spa_type(nfit_spa->spa)) {
 -		case NFIT_SPA_VOLATILE:
 -		case NFIT_SPA_PM:
 +		int rc, type = nfit_spa_type(nfit_spa->spa);
 +
 +		/* PMEM and VMEM will be registered by the ARS workqueue */
 +		if (type == NFIT_SPA_PM || type == NFIT_SPA_VOLATILE) {
  			acpi_nfit_init_ars(acpi_desc, nfit_spa);
 -			break;
 +			continue;
  		}
 +		/* BLK apertures belong to BLK region registration below */
 +		if (type == NFIT_SPA_BDW)
 +			continue;
 +		/* BLK regions don't need to wait for ARS results */
 +		rc = acpi_nfit_register_region(acpi_desc, nfit_spa);
 +		if (rc)
 +			return rc;
  	}
  
++<<<<<<< HEAD
 +	acpi_desc->ars_start_flags = 0;
 +	if (!acpi_desc->cancel)
 +		queue_work(nfit_wq, &acpi_desc->work);
++=======
+ 	/*
+ 	 * Reap any results that might be pending before starting new
+ 	 * short requests.
+ 	 */
+ 	query_rc = acpi_nfit_query_poison(acpi_desc);
+ 	if (query_rc == 0)
+ 		ars_complete_all(acpi_desc);
+ 
+ 	list_for_each_entry(nfit_spa, &acpi_desc->spas, list)
+ 		switch (nfit_spa_type(nfit_spa->spa)) {
+ 		case NFIT_SPA_VOLATILE:
+ 		case NFIT_SPA_PM:
+ 			/* register regions and kick off initial ARS run */
+ 			rc = ars_register(acpi_desc, nfit_spa, &query_rc);
+ 			if (rc)
+ 				return rc;
+ 			break;
+ 		case NFIT_SPA_BDW:
+ 			/* nothing to register */
+ 			break;
+ 		case NFIT_SPA_DCR:
+ 		case NFIT_SPA_VDISK:
+ 		case NFIT_SPA_VCD:
+ 		case NFIT_SPA_PDISK:
+ 		case NFIT_SPA_PCD:
+ 			/* register known regions that don't support ARS */
+ 			rc = acpi_nfit_register_region(acpi_desc, nfit_spa);
+ 			if (rc)
+ 				return rc;
+ 			break;
+ 		default:
+ 			/* don't register unknown regions */
+ 			break;
+ 		}
+ 
+ 	sched_ars(acpi_desc);
++>>>>>>> 33cc2c966756 (acpi, nfit: Fix scrub idle detection)
  	return 0;
  }
  
@@@ -3228,21 -3247,32 +3307,35 @@@ int acpi_nfit_ars_rescan(struct acpi_nf
  	}
  
  	list_for_each_entry(nfit_spa, &acpi_desc->spas, list) {
 -		int type = nfit_spa_type(nfit_spa->spa);
 +		struct acpi_nfit_system_address *spa = nfit_spa->spa;
  
 -		if (type != NFIT_SPA_PM && type != NFIT_SPA_VOLATILE)
 -			continue;
 -		if (test_bit(ARS_FAILED, &nfit_spa->ars_state))
 +		if (nfit_spa_type(spa) != NFIT_SPA_PM)
  			continue;
  
++<<<<<<< HEAD
 +		set_bit(ARS_REQ, &nfit_spa->ars_state);
++=======
+ 		if (test_and_set_bit(ARS_REQ, &nfit_spa->ars_state))
+ 			busy++;
+ 		else {
+ 			if (test_bit(ARS_SHORT, &flags))
+ 				set_bit(ARS_SHORT, &nfit_spa->ars_state);
+ 			scheduled++;
+ 		}
+ 	}
+ 	if (scheduled) {
+ 		sched_ars(acpi_desc);
+ 		dev_dbg(dev, "ars_scan triggered\n");
++>>>>>>> 33cc2c966756 (acpi, nfit: Fix scrub idle detection)
  	}
 +	acpi_desc->ars_start_flags = 0;
 +	if (test_bit(ARS_SHORT, &flags))
 +		acpi_desc->ars_start_flags |= ND_ARS_RETURN_PREV_DATA;
 +	queue_work(nfit_wq, &acpi_desc->work);
 +	dev_dbg(dev, "%s: ars_scan triggered\n", __func__);
  	mutex_unlock(&acpi_desc->init_mutex);
  
 -	if (scheduled)
 -		return 0;
 -	if (busy)
 -		return -EBUSY;
 -	return -ENOTTY;
 +	return 0;
  }
  
  void acpi_nfit_desc_init(struct acpi_nfit_desc *acpi_desc, struct device *dev)
* Unmerged path drivers/acpi/nfit/core.c
diff --git a/drivers/acpi/nfit/nfit.h b/drivers/acpi/nfit/nfit.h
index a4224dcb856c..e920e9f2afd9 100644
--- a/drivers/acpi/nfit/nfit.h
+++ b/drivers/acpi/nfit/nfit.h
@@ -178,6 +178,7 @@ struct acpi_nfit_desc {
 	unsigned int max_ars;
 	unsigned int scrub_count;
 	unsigned int scrub_mode;
+	unsigned int scrub_busy:1;
 	unsigned int cancel:1;
 	unsigned int init_complete:1;
 	unsigned long dimm_cmd_force_en;
