scsi: lpfc: NVME Target: Base modifications

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [scsi] lpfc: NVME Target: Base modifications (Ewan Milne) [1384922]
Rebuild_FUZZ: 92.50%
commit-author James Smart <jsmart2021@gmail.com>
commit f358dd0ca26c152a5e0922e269996268dcb98a9d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/f358dd0c.failed

NVME Target: Base modifications

This set of patches adds the base modifications for NVME target support

The base modifications consist of:
- Additional module parameters or configuration tuning
- Enablement of configuration mode for NVME target. Ties into the
  queueing model put into place by the initiator basemods patches.
- Target-specific buffer pools, dma pools, sgl pools

[mkp: fixed space at end of file]

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit f358dd0ca26c152a5e0922e269996268dcb98a9d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc.h
#	drivers/scsi/lpfc/lpfc_attr.c
#	drivers/scsi/lpfc/lpfc_crtn.h
#	drivers/scsi/lpfc/lpfc_debugfs.c
#	drivers/scsi/lpfc/lpfc_hw4.h
#	drivers/scsi/lpfc/lpfc_init.c
#	drivers/scsi/lpfc/lpfc_mem.c
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/lpfc/lpfc_sli.h
#	drivers/scsi/lpfc/lpfc_sli4.h
diff --cc drivers/scsi/lpfc/lpfc.h
index 79f57d03022e,b7361474880e..000000000000
--- a/drivers/scsi/lpfc/lpfc.h
+++ b/drivers/scsi/lpfc/lpfc.h
@@@ -699,6 -738,11 +699,14 @@@ struct lpfc_hba 
  	uint8_t  wwnn[8];
  	uint8_t  wwpn[8];
  	uint32_t RandomData[7];
++<<<<<<< HEAD
++=======
+ 	uint8_t  fcp_embed_io;
+ 	uint8_t  nvme_support;	/* Firmware supports NVME */
+ 	uint8_t  nvmet_support;	/* driver supports NVMET */
+ #define LPFC_NVMET_MAX_PORTS	32
+ 	uint8_t  mds_diags_support;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  
  	/* HBA Config Parameters */
  	uint32_t cfg_ack0;
@@@ -723,9 -767,13 +731,17 @@@
  	uint32_t cfg_fcp_imax;
  	uint32_t cfg_fcp_cpu_map;
  	uint32_t cfg_fcp_io_channel;
++<<<<<<< HEAD
++=======
+ 	uint32_t cfg_suppress_rsp;
+ 	uint32_t cfg_nvme_oas;
+ 	uint32_t cfg_nvme_io_channel;
+ 	uint32_t cfg_enable_nvmet;
+ 	uint32_t cfg_nvme_enable_fb;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	uint32_t cfg_total_seg_cnt;
  	uint32_t cfg_sg_seg_cnt;
 +	uint32_t cfg_prot_sg_seg_cnt;
  	uint32_t cfg_sg_dma_buf_size;
  	uint64_t cfg_soft_wwnn;
  	uint64_t cfg_soft_wwpn;
@@@ -766,6 -816,14 +782,17 @@@
  #define LPFC_FDMI_NO_SUPPORT	0	/* FDMI not supported */
  #define LPFC_FDMI_SUPPORT	1	/* FDMI supported? */
  	uint32_t cfg_enable_SmartSAN;
++<<<<<<< HEAD
++=======
+ 	uint32_t cfg_enable_mds_diags;
+ 	uint32_t cfg_enable_fc4_type;
+ 	uint32_t cfg_xri_split;
+ #define LPFC_ENABLE_FCP  1
+ #define LPFC_ENABLE_NVME 2
+ #define LPFC_ENABLE_BOTH 3
+ 	uint32_t io_channel_irqs;	/* number of irqs for io channels */
+ 	struct nvmet_fc_target_port *targetport;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	lpfc_vpd_t vpd;		/* vital product data */
  
  	struct pci_dev *pcidev;
@@@ -1020,6 -1097,53 +1047,56 @@@
  #define LPFC_TRANSGRESSION_LOW_RXPOWER		0x4000
  	uint16_t sfp_alarm;
  	uint16_t sfp_warning;
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_SCSI_LPFC_DEBUG_FS
+ #define LPFC_CHECK_CPU_CNT    32
+ 	uint32_t cpucheck_rcv_io[LPFC_CHECK_CPU_CNT];
+ 	uint32_t cpucheck_xmt_io[LPFC_CHECK_CPU_CNT];
+ 	uint32_t cpucheck_cmpl_io[LPFC_CHECK_CPU_CNT];
+ 	uint32_t cpucheck_ccmpl_io[LPFC_CHECK_CPU_CNT];
+ 	uint16_t cpucheck_on;
+ #define LPFC_CHECK_OFF		0
+ #define LPFC_CHECK_NVME_IO	1
+ #define LPFC_CHECK_NVMET_RCV	2
+ #define LPFC_CHECK_NVMET_IO	4
+ 	uint16_t ktime_on;
+ 	uint64_t ktime_data_samples;
+ 	uint64_t ktime_status_samples;
+ 	uint64_t ktime_last_cmd;
+ 	uint64_t ktime_seg1_total;
+ 	uint64_t ktime_seg1_min;
+ 	uint64_t ktime_seg1_max;
+ 	uint64_t ktime_seg2_total;
+ 	uint64_t ktime_seg2_min;
+ 	uint64_t ktime_seg2_max;
+ 	uint64_t ktime_seg3_total;
+ 	uint64_t ktime_seg3_min;
+ 	uint64_t ktime_seg3_max;
+ 	uint64_t ktime_seg4_total;
+ 	uint64_t ktime_seg4_min;
+ 	uint64_t ktime_seg4_max;
+ 	uint64_t ktime_seg5_total;
+ 	uint64_t ktime_seg5_min;
+ 	uint64_t ktime_seg5_max;
+ 	uint64_t ktime_seg6_total;
+ 	uint64_t ktime_seg6_min;
+ 	uint64_t ktime_seg6_max;
+ 	uint64_t ktime_seg7_total;
+ 	uint64_t ktime_seg7_min;
+ 	uint64_t ktime_seg7_max;
+ 	uint64_t ktime_seg8_total;
+ 	uint64_t ktime_seg8_min;
+ 	uint64_t ktime_seg8_max;
+ 	uint64_t ktime_seg9_total;
+ 	uint64_t ktime_seg9_min;
+ 	uint64_t ktime_seg9_max;
+ 	uint64_t ktime_seg10_total;
+ 	uint64_t ktime_seg10_min;
+ 	uint64_t ktime_seg10_max;
+ #endif
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  };
  
  static inline struct Scsi_Host *
diff --cc drivers/scsi/lpfc/lpfc_attr.c
index c5f7d60f9dff,835c6c1f4cd4..000000000000
--- a/drivers/scsi/lpfc/lpfc_attr.c
+++ b/drivers/scsi/lpfc/lpfc_attr.c
@@@ -41,8 -43,10 +41,13 @@@
  #include "lpfc_sli4.h"
  #include "lpfc_nl.h"
  #include "lpfc_disc.h"
 -#include "lpfc.h"
  #include "lpfc_scsi.h"
++<<<<<<< HEAD
 +#include "lpfc.h"
++=======
+ #include "lpfc_nvme.h"
+ #include "lpfc_nvmet.h"
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  #include "lpfc_logmsg.h"
  #include "lpfc_version.h"
  #include "lpfc_compat.h"
@@@ -129,6 -134,211 +134,214 @@@ lpfc_enable_fip_show(struct device *dev
  }
  
  static ssize_t
++<<<<<<< HEAD
++=======
+ lpfc_nvme_info_show(struct device *dev, struct device_attribute *attr,
+ 		    char *buf)
+ {
+ 	struct Scsi_Host *shost = class_to_shost(dev);
+ 	struct lpfc_vport *vport = shost_priv(shost);
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	struct nvme_fc_local_port *localport;
+ 	struct lpfc_nvme_lport *lport;
+ 	struct lpfc_nvme_rport *rport;
+ 	struct nvme_fc_remote_port *nrport;
+ 	char *statep;
+ 	int len = 0;
+ 
+ 	if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME)) {
+ 		len += snprintf(buf, PAGE_SIZE, "NVME Disabled\n");
+ 		return len;
+ 	}
+ 	if (phba->nvmet_support) {
+ 		if (!phba->targetport) {
+ 			len = snprintf(buf, PAGE_SIZE,
+ 					"NVME Target: x%llx is not allocated\n",
+ 					wwn_to_u64(vport->fc_portname.u.wwn));
+ 			return len;
+ 		}
+ 		/* Port state is only one of two values for now. */
+ 		if (phba->targetport->port_id)
+ 			statep = "REGISTERED";
+ 		else
+ 			statep = "INIT";
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"NVME Target: Enabled  State %s\n",
+ 				statep);
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"%s%d WWPN x%llx WWNN x%llx DID x%06x\n",
+ 				"NVME Target: lpfc",
+ 				phba->brd_no,
+ 				wwn_to_u64(vport->fc_portname.u.wwn),
+ 				wwn_to_u64(vport->fc_nodename.u.wwn),
+ 				phba->targetport->port_id);
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE,
+ 				"\nNVME Target: Statistics\n");
+ 		tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"LS: Rcv %08x Drop %08x Abort %08x\n",
+ 				atomic_read(&tgtp->rcv_ls_req_in),
+ 				atomic_read(&tgtp->rcv_ls_req_drop),
+ 				atomic_read(&tgtp->xmt_ls_abort));
+ 		if (atomic_read(&tgtp->rcv_ls_req_in) !=
+ 		    atomic_read(&tgtp->rcv_ls_req_out)) {
+ 			len += snprintf(buf+len, PAGE_SIZE-len,
+ 					"Rcv LS: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_ls_req_in),
+ 					atomic_read(&tgtp->rcv_ls_req_out));
+ 		}
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"LS: Xmt %08x Drop %08x Cmpl %08x Err %08x\n",
+ 				atomic_read(&tgtp->xmt_ls_rsp),
+ 				atomic_read(&tgtp->xmt_ls_drop),
+ 				atomic_read(&tgtp->xmt_ls_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_ls_rsp_error));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP: Rcv %08x Drop %08x\n",
+ 				atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 				atomic_read(&tgtp->rcv_fcp_cmd_drop));
+ 
+ 		if (atomic_read(&tgtp->rcv_fcp_cmd_in) !=
+ 		    atomic_read(&tgtp->rcv_fcp_cmd_out)) {
+ 			len += snprintf(buf+len, PAGE_SIZE-len,
+ 					"Rcv FCP: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 					atomic_read(&tgtp->rcv_fcp_cmd_out));
+ 		}
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP Rsp: RD %08x rsp %08x WR %08x rsp %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_read),
+ 				atomic_read(&tgtp->xmt_fcp_read_rsp),
+ 				atomic_read(&tgtp->xmt_fcp_write),
+ 				atomic_read(&tgtp->xmt_fcp_rsp));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP Rsp: abort %08x drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_abort),
+ 				atomic_read(&tgtp->xmt_fcp_drop));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP Rsp Cmpl: %08x err %08x drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_error),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_drop));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"ABORT: Xmt %08x Err %08x Cmpl %08x",
+ 				atomic_read(&tgtp->xmt_abort_rsp),
+ 				atomic_read(&tgtp->xmt_abort_rsp_error),
+ 				atomic_read(&tgtp->xmt_abort_cmpl));
+ 
+ 		len +=  snprintf(buf+len, PAGE_SIZE-len, "\n");
+ 		return len;
+ 	}
+ 
+ 	localport = vport->localport;
+ 	if (!localport) {
+ 		len = snprintf(buf, PAGE_SIZE,
+ 				"NVME Initiator x%llx is not allocated\n",
+ 				wwn_to_u64(vport->fc_portname.u.wwn));
+ 		return len;
+ 	}
+ 	len = snprintf(buf, PAGE_SIZE, "NVME Initiator Enabled\n");
+ 
+ 	spin_lock_irq(shost->host_lock);
+ 	lport = (struct lpfc_nvme_lport *)localport->private;
+ 
+ 	/* Port state is only one of two values for now. */
+ 	if (localport->port_id)
+ 		statep = "ONLINE";
+ 	else
+ 		statep = "UNKNOWN ";
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"%s%d WWPN x%llx WWNN x%llx DID x%06x %s\n",
+ 			"NVME LPORT lpfc",
+ 			phba->brd_no,
+ 			wwn_to_u64(vport->fc_portname.u.wwn),
+ 			wwn_to_u64(vport->fc_nodename.u.wwn),
+ 			localport->port_id, statep);
+ 
+ 	list_for_each_entry(rport, &lport->rport_list, list) {
+ 		/* local short-hand pointer. */
+ 		nrport = rport->remoteport;
+ 
+ 		/* Port state is only one of two values for now. */
+ 		switch (nrport->port_state) {
+ 		case FC_OBJSTATE_ONLINE:
+ 			statep = "ONLINE";
+ 			break;
+ 		case FC_OBJSTATE_UNKNOWN:
+ 			statep = "UNKNOWN ";
+ 			break;
+ 		default:
+ 			statep = "UNSUPPORTED";
+ 			break;
+ 		}
+ 
+ 		/* Tab in to show lport ownership. */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"NVME RPORT       ");
+ 		if (phba->brd_no >= 10)
+ 			len += snprintf(buf + len, PAGE_SIZE - len, " ");
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "WWPN x%llx ",
+ 				nrport->port_name);
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "WWNN x%llx ",
+ 				nrport->node_name);
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "DID x%06x ",
+ 				nrport->port_id);
+ 
+ 		switch (nrport->port_role) {
+ 		case FC_PORT_ROLE_NVME_INITIATOR:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "INITIATOR ");
+ 			break;
+ 		case FC_PORT_ROLE_NVME_TARGET:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "TARGET ");
+ 			break;
+ 		case FC_PORT_ROLE_NVME_DISCOVERY:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "DISCOVERY ");
+ 			break;
+ 		default:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "UNKNOWN_ROLE x%x",
+ 					 nrport->port_role);
+ 			break;
+ 		}
+ 		len +=  snprintf(buf + len, PAGE_SIZE - len, "%s  ", statep);
+ 		/* Terminate the string. */
+ 		len +=  snprintf(buf + len, PAGE_SIZE - len, "\n");
+ 	}
+ 	spin_unlock_irq(shost->host_lock);
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE, "\nNVME Statistics\n");
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"LS: Xmt %016llx Cmpl %016llx\n",
+ 			phba->fc4NvmeLsRequests,
+ 			phba->fc4NvmeLsCmpls);
+ 
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"FCP: Rd %016llx Wr %016llx IO %016llx\n",
+ 			phba->fc4NvmeInputRequests,
+ 			phba->fc4NvmeOutputRequests,
+ 			phba->fc4NvmeControlRequests);
+ 
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"    Cmpl %016llx\n", phba->fc4NvmeIoCmpls);
+ 
+ 	return len;
+ }
+ 
+ static ssize_t
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  lpfc_bg_info_show(struct device *dev, struct device_attribute *attr,
  		  char *buf)
  {
@@@ -3110,6 -3273,41 +3330,44 @@@ static DEVICE_ATTR(lpfc_devloss_tmo, S_
  		   lpfc_devloss_tmo_show, lpfc_devloss_tmo_store);
  
  /*
++<<<<<<< HEAD
++=======
+  * lpfc_suppress_rsp: Enable suppress rsp feature is firmware supports it
+  * lpfc_suppress_rsp = 0  Disable
+  * lpfc_suppress_rsp = 1  Enable (default)
+  *
+  */
+ LPFC_ATTR_R(suppress_rsp, 1, 0, 1,
+ 	    "Enable suppress rsp feature is firmware supports it");
+ 
+ /*
+  * lpfc_enable_fc4_type: Defines what FC4 types are supported.
+  * Supported Values:  1 - register just FCP
+  *                    3 - register both FCP and NVME
+  * Supported values are [1,3]. Default value is 3
+  */
+ LPFC_ATTR_R(enable_fc4_type, LPFC_ENABLE_BOTH,
+ 	    LPFC_ENABLE_FCP, LPFC_ENABLE_BOTH,
+ 	    "Define fc4 type to register with fabric.");
+ 
+ /*
+  * lpfc_xri_split: Defines the division of XRI resources between SCSI and NVME
+  * This parameter is only used if:
+  *     lpfc_enable_fc4_type is 3 - register both FCP and NVME and
+  *     port is not configured for NVMET.
+  *
+  * ELS/CT always get 10% of XRIs, up to a maximum of 250
+  * The remaining XRIs get split up based on lpfc_xri_split per port:
+  *
+  * Supported Values are in percentages
+  * the xri_split value is the percentage the SCSI port will get. The remaining
+  * percentage will go to NVME.
+  */
+ LPFC_ATTR_R(xri_split, 50, 10, 90,
+ 	     "Division of XRI resources between SCSI and NVME");
+ 
+ /*
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  # lpfc_log_verbose: Only turn this flag on if you are willing to risk being
  # deluged with LOTS of information.
  # You can set a bit mask to record specific types of verbose messages:
@@@ -4640,14 -4859,53 +4898,40 @@@ LPFC_ATTR_R(use_msi, 2, 0, 2, "Use Mess
  	    "MSI-X (2), if possible");
  
  /*
++<<<<<<< HEAD
 +# lpfc_fcp_io_channel: Set the number of FCP EQ/CQ/WQ IO channels
 +#
 +# Value range is [1,7]. Default value is 4.
 +*/
 +LPFC_ATTR_R(fcp_io_channel, LPFC_FCP_IO_CHAN_DEF, LPFC_FCP_IO_CHAN_MIN,
 +	    LPFC_FCP_IO_CHAN_MAX,
++=======
+  * lpfc_nvme_oas: Use the oas bit when sending NVME/NVMET IOs
+  *
+  *      0  = NVME OAS disabled
+  *      1  = NVME OAS enabled
+  *
+  * Value range is [0,1]. Default value is 0.
+  */
+ LPFC_ATTR_RW(nvme_oas, 0, 0, 1,
+ 	     "Use OAS bit on NVME IOs");
+ 
+ /*
+  * lpfc_fcp_io_channel: Set the number of FCP IO channels the driver
+  * will advertise it supports to the SCSI layer. This also will map to
+  * the number of WQs the driver will create.
+  *
+  *      0    = Configure the number of io channels to the number of active CPUs.
+  *      1,32 = Manually specify how many io channels to use.
+  *
+  * Value range is [0,32]. Default value is 4.
+  */
+ LPFC_ATTR_R(fcp_io_channel,
+ 	    LPFC_FCP_IO_CHAN_DEF,
+ 	    LPFC_HBA_IO_CHAN_MIN, LPFC_HBA_IO_CHAN_MAX,
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	    "Set the number of FCP I/O channels");
  
 -/*
 - * lpfc_nvme_io_channel: Set the number of IO hardware queues the driver
 - * will advertise it supports to the NVME layer. This also will map to
 - * the number of WQs the driver will create.
 - *
 - * This module parameter is valid when lpfc_enable_fc4_type is set
 - * to support NVME.
 - *
 - * The NVME Layer will try to create this many, plus 1 administrative
 - * hardware queue. The administrative queue will always map to WQ 0
 - * A hardware IO queue maps (qidx) to a specific driver WQ.
 - *
 - *      0    = Configure the number of io channels to the number of active CPUs.
 - *      1,32 = Manually specify how many io channels to use.
 - *
 - * Value range is [0,32]. Default value is 0.
 - */
 -LPFC_ATTR_R(nvme_io_channel,
 -	    LPFC_NVME_IO_CHAN_DEF,
 -	    LPFC_HBA_IO_CHAN_MIN, LPFC_HBA_IO_CHAN_MAX,
 -	    "Set the number of NVME I/O channels");
 -
  /*
  # lpfc_enable_hba_reset: Allow or prevent HBA resets to the hardware.
  #       0  = HBA resets disabled
@@@ -4832,6 -5096,10 +5116,12 @@@ struct device_attribute *lpfc_hba_attrs
  	&dev_attr_lpfc_fcp_imax,
  	&dev_attr_lpfc_fcp_cpu_map,
  	&dev_attr_lpfc_fcp_io_channel,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_lpfc_suppress_rsp,
+ 	&dev_attr_lpfc_nvme_io_channel,
+ 	&dev_attr_lpfc_nvme_enable_fb,
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	&dev_attr_lpfc_enable_bg,
  	&dev_attr_lpfc_soft_wwnn,
  	&dev_attr_lpfc_soft_wwpn,
@@@ -5862,11 -6133,46 +6152,49 @@@ lpfc_get_cfgparam(struct lpfc_hba *phba
  		phba->cfg_poll = 0;
  	else
  		phba->cfg_poll = lpfc_poll;
+ 	lpfc_suppress_rsp_init(phba, lpfc_suppress_rsp);
  
++<<<<<<< HEAD
++=======
+ 	lpfc_enable_fc4_type_init(phba, lpfc_enable_fc4_type);
+ 
+ 	/* Initialize first burst. Target vs Initiator are different. */
+ 	lpfc_nvme_enable_fb_init(phba, lpfc_nvme_enable_fb);
+ 	lpfc_fcp_io_channel_init(phba, lpfc_fcp_io_channel);
+ 	lpfc_nvme_io_channel_init(phba, lpfc_nvme_io_channel);
+ 
+ 	if (phba->sli_rev != LPFC_SLI_REV4) {
+ 		/* NVME only supported on SLI4 */
+ 		phba->nvmet_support = 0;
+ 		phba->cfg_enable_fc4_type = LPFC_ENABLE_FCP;
+ 	} else {
+ 		/* We MUST have FCP support */
+ 		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP))
+ 			phba->cfg_enable_fc4_type |= LPFC_ENABLE_FCP;
+ 	}
+ 
+ 	/* A value of 0 means use the number of CPUs found in the system */
+ 	if (phba->cfg_fcp_io_channel == 0)
+ 		phba->cfg_fcp_io_channel = phba->sli4_hba.num_present_cpu;
+ 	if (phba->cfg_nvme_io_channel == 0)
+ 		phba->cfg_nvme_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_enable_fc4_type == LPFC_ENABLE_NVME)
+ 		phba->cfg_fcp_io_channel = 0;
+ 
+ 	if (phba->cfg_enable_fc4_type == LPFC_ENABLE_FCP)
+ 		phba->cfg_nvme_io_channel = 0;
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->cfg_nvme_io_channel)
+ 		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
+ 	else
+ 		phba->io_channel_irqs = phba->cfg_nvme_io_channel;
+ 
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	phba->cfg_soft_wwnn = 0L;
  	phba->cfg_soft_wwpn = 0L;
 -	lpfc_xri_split_init(phba, lpfc_xri_split);
  	lpfc_sg_seg_cnt_init(phba, lpfc_sg_seg_cnt);
 +	lpfc_prot_sg_seg_cnt_init(phba, lpfc_prot_sg_seg_cnt);
  	lpfc_hba_queue_depth_init(phba, lpfc_hba_queue_depth);
  	lpfc_hba_log_verbose_init(phba, lpfc_log_verbose);
  	lpfc_aer_support_init(phba, lpfc_aer_support);
@@@ -5881,6 -6188,34 +6209,37 @@@
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_nvme_mod_param_dep - Adjust module parameter value based on
+  * dependencies between protocols and roles.
+  * @phba: lpfc_hba pointer.
+  **/
+ void
+ lpfc_nvme_mod_param_dep(struct lpfc_hba *phba)
+ {
+ 	if (phba->cfg_nvme_io_channel > phba->sli4_hba.num_present_cpu)
+ 		phba->cfg_nvme_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->sli4_hba.num_present_cpu)
+ 		phba->cfg_fcp_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME &&
+ 	    phba->nvmet_support) {
+ 		phba->cfg_enable_fc4_type &= ~LPFC_ENABLE_FCP;
+ 		phba->cfg_fcp_io_channel = 0;
+ 	} else
+ 		/* Not NVME Target mode.  Turn off Target parameters. */
+ 		phba->nvmet_support = 0;
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->cfg_nvme_io_channel)
+ 		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
+ 	else
+ 		phba->io_channel_irqs = phba->cfg_nvme_io_channel;
+ }
+ 
+ /**
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
   * lpfc_get_vport_cfgparam - Used during port create, init the vport structure
   * @vport: lpfc_vport pointer.
   **/
diff --cc drivers/scsi/lpfc/lpfc_crtn.h
index 5d7cb958427c,1e97b1a106c5..000000000000
--- a/drivers/scsi/lpfc/lpfc_crtn.h
+++ b/drivers/scsi/lpfc/lpfc_crtn.h
@@@ -231,8 -240,15 +231,10 @@@ struct hbq_dmabuf *lpfc_els_hbq_alloc(s
  void lpfc_els_hbq_free(struct lpfc_hba *, struct hbq_dmabuf *);
  struct hbq_dmabuf *lpfc_sli4_rb_alloc(struct lpfc_hba *);
  void lpfc_sli4_rb_free(struct lpfc_hba *, struct hbq_dmabuf *);
+ struct rqb_dmabuf *lpfc_sli4_nvmet_alloc(struct lpfc_hba *phba);
+ void lpfc_sli4_nvmet_free(struct lpfc_hba *phba, struct rqb_dmabuf *dmab);
  void lpfc_sli4_build_dflt_fcf_record(struct lpfc_hba *, struct fcf_record *,
  			uint16_t);
 -int lpfc_sli4_rq_put(struct lpfc_queue *hq, struct lpfc_queue *dq,
 -		     struct lpfc_rqe *hrqe, struct lpfc_rqe *drqe);
 -int lpfc_post_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *hq,
 -			struct lpfc_queue *dq, int count);
 -int lpfc_free_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *hq);
  void lpfc_unregister_fcf(struct lpfc_hba *);
  void lpfc_unregister_fcf_rescan(struct lpfc_hba *);
  void lpfc_unregister_unused_fcf(struct lpfc_hba *);
@@@ -287,6 -303,11 +289,14 @@@ void lpfc_sli_def_mbox_cmpl(struct lpfc
  void lpfc_sli4_unreg_rpi_cmpl_clr(struct lpfc_hba *, LPFC_MBOXQ_t *);
  int lpfc_sli_issue_iocb(struct lpfc_hba *, uint32_t,
  			struct lpfc_iocbq *, uint32_t);
++<<<<<<< HEAD
++=======
+ int lpfc_sli4_issue_wqe(struct lpfc_hba *phba, uint32_t rnum,
+ 			struct lpfc_iocbq *iocbq);
+ struct lpfc_sglq *__lpfc_clear_active_sglq(struct lpfc_hba *phba, uint16_t xri);
+ struct lpfc_sglq *__lpfc_sli_get_nvmet_sglq(struct lpfc_hba *phba,
+ 					    struct lpfc_iocbq *piocbq);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  void lpfc_sli_pcimem_bcopy(void *, void *, uint32_t);
  void lpfc_sli_bemem_bcopy(void *, void *, uint32_t);
  void lpfc_sli_abort_iocb_ring(struct lpfc_hba *, struct lpfc_sli_ring *);
@@@ -473,7 -498,10 +486,14 @@@ int lpfc_issue_unreg_vfi(struct lpfc_vp
  int lpfc_selective_reset(struct lpfc_hba *);
  int lpfc_sli4_read_config(struct lpfc_hba *);
  void lpfc_sli4_node_prep(struct lpfc_hba *);
++<<<<<<< HEAD
 +int lpfc_sli4_xri_sgl_update(struct lpfc_hba *);
++=======
+ int lpfc_sli4_els_sgl_update(struct lpfc_hba *phba);
+ int lpfc_sli4_nvmet_sgl_update(struct lpfc_hba *phba);
+ int lpfc_sli4_scsi_sgl_update(struct lpfc_hba *phba);
+ int lpfc_sli4_nvme_sgl_update(struct lpfc_hba *phba);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  void lpfc_free_sgl_list(struct lpfc_hba *, struct list_head *);
  uint32_t lpfc_sli_port_speed_get(struct lpfc_hba *);
  int lpfc_sli4_request_firmware_update(struct lpfc_hba *, uint8_t);
@@@ -498,3 -526,18 +518,21 @@@ bool lpfc_find_next_oas_lun(struct lpfc
  			    uint32_t *, uint32_t *);
  int lpfc_sli4_dump_page_a0(struct lpfc_hba *phba, struct lpfcMboxq *mbox);
  void lpfc_mbx_cmpl_rdp_page_a0(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmb);
++<<<<<<< HEAD
++=======
+ 
+ /* NVME interfaces. */
+ void lpfc_nvme_unregister_port(struct lpfc_vport *vport,
+ 			struct lpfc_nodelist *ndlp);
+ int lpfc_nvme_register_port(struct lpfc_vport *vport,
+ 			struct lpfc_nodelist *ndlp);
+ int lpfc_nvme_create_localport(struct lpfc_vport *vport);
+ void lpfc_nvme_destroy_localport(struct lpfc_vport *vport);
+ void lpfc_nvme_update_localport(struct lpfc_vport *vport);
+ void lpfc_nvme_mod_param_dep(struct lpfc_hba *phba);
+ void lpfc_nvme_abort_fcreq_cmpl(struct lpfc_hba *phba,
+ 				struct lpfc_iocbq *cmdiocb,
+ 				struct lpfc_wcqe_complete *abts_cmpl);
+ extern int lpfc_enable_nvmet_cnt;
+ extern unsigned long long lpfc_enable_nvmet[];
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
diff --cc drivers/scsi/lpfc/lpfc_debugfs.c
index 4dc8eba541b9,e54307dff8be..000000000000
--- a/drivers/scsi/lpfc/lpfc_debugfs.c
+++ b/drivers/scsi/lpfc/lpfc_debugfs.c
@@@ -611,8 -625,361 +611,291 @@@ lpfc_debugfs_nodelist_data(struct lpfc_
  		len +=  snprintf(buf+len, size-len, "\n");
  	}
  	spin_unlock_irq(shost->host_lock);
 -
 -	len += snprintf(buf + len, size - len,
 -				"\nNVME Lport/Rport Entries ...\n");
 -
 -	localport = vport->localport;
 -	if (!localport)
 -		goto out_exit;
 -
 -	spin_lock_irq(shost->host_lock);
 -	lport = (struct lpfc_nvme_lport *)localport->private;
 -
 -	/* Port state is only one of two values for now. */
 -	if (localport->port_id)
 -		statep = "ONLINE";
 -	else
 -		statep = "UNKNOWN ";
 -
 -	len += snprintf(buf + len, size - len,
 -			"Lport DID x%06x PortState %s\n",
 -			localport->port_id, statep);
 -
 -	len += snprintf(buf + len, size - len, "\tRport List:\n");
 -	list_for_each_entry(rport, &lport->rport_list, list) {
 -		/* local short-hand pointer. */
 -		nrport = rport->remoteport;
 -
 -		/* Port state is only one of two values for now. */
 -		switch (nrport->port_state) {
 -		case FC_OBJSTATE_ONLINE:
 -			statep = "ONLINE";
 -			break;
 -		case FC_OBJSTATE_UNKNOWN:
 -			statep = "UNKNOWN ";
 -			break;
 -		default:
 -			statep = "UNSUPPORTED";
 -			break;
 -		}
 -
 -		/* Tab in to show lport ownership. */
 -		len += snprintf(buf + len, size - len,
 -				"\t%s Port ID:x%06x ",
 -				statep, nrport->port_id);
 -		len += snprintf(buf + len, size - len, "WWPN x%llx ",
 -				nrport->port_name);
 -		len += snprintf(buf + len, size - len, "WWNN x%llx ",
 -				nrport->node_name);
 -		switch (nrport->port_role) {
 -		case FC_PORT_ROLE_NVME_INITIATOR:
 -			len +=  snprintf(buf + len, size - len,
 -					 "NVME INITIATOR ");
 -			break;
 -		case FC_PORT_ROLE_NVME_TARGET:
 -			len +=  snprintf(buf + len, size - len,
 -					 "NVME TARGET ");
 -			break;
 -		case FC_PORT_ROLE_NVME_DISCOVERY:
 -			len +=  snprintf(buf + len, size - len,
 -					 "NVME DISCOVERY ");
 -			break;
 -		default:
 -			len +=  snprintf(buf + len, size - len,
 -					 "UNKNOWN ROLE x%x",
 -					 nrport->port_role);
 -			break;
 -		}
 -
 -		/* Terminate the string. */
 -		len +=  snprintf(buf + len, size - len, "\n");
 -	}
 -
 -	spin_unlock_irq(shost->host_lock);
 - out_exit:
  	return len;
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * lpfc_debugfs_nvmestat_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmestat_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	int len = 0;
+ 
+ 	if (phba->nvmet_support == 0) {
+ 		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME))
+ 			return len;
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"\nNVME Lport Statistics\n");
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"LS: Xmt %016llx Cmpl %016llx\n",
+ 				phba->fc4NvmeLsRequests,
+ 				phba->fc4NvmeLsCmpls);
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP: Rd %016llx Wr %016llx IO %016llx\n",
+ 				phba->fc4NvmeInputRequests,
+ 				phba->fc4NvmeOutputRequests,
+ 				phba->fc4NvmeControlRequests);
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"    Cmpl %016llx\n", phba->fc4NvmeIoCmpls);
+ 	}
+ 
+ 	return len;
+ }
+ 
+ 
+ /**
+  * lpfc_debugfs_nvmektime_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmektime_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	int len = 0;
+ 
+ 	if (phba->nvmet_support == 0) {
+ 		/* NVME Initiator */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"ktime %s: Total Samples: %lld\n",
+ 				(phba->ktime_on ?  "Enabled" : "Disabled"),
+ 				phba->ktime_data_samples);
+ 		if (phba->ktime_data_samples == 0)
+ 			return len;
+ 
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 1: Last NVME Cmd cmpl "
+ 			"done -to- Start of next NVME cnd (in driver)\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			phba->ktime_seg1_total /
+ 			phba->ktime_data_samples,
+ 			phba->ktime_seg1_min,
+ 			phba->ktime_seg1_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 2: Driver start of NVME cmd "
+ 			"-to- Firmware WQ doorbell\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			phba->ktime_seg2_total /
+ 			phba->ktime_data_samples,
+ 			phba->ktime_seg2_min,
+ 			phba->ktime_seg2_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 3: Firmware WQ doorbell -to- "
+ 			"MSI-X ISR cmpl\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			phba->ktime_seg3_total /
+ 			phba->ktime_data_samples,
+ 			phba->ktime_seg3_min,
+ 			phba->ktime_seg3_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 4: MSI-X ISR cmpl -to- "
+ 			"NVME cmpl done\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			phba->ktime_seg4_total /
+ 			phba->ktime_data_samples,
+ 			phba->ktime_seg4_min,
+ 			phba->ktime_seg4_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Total IO avg time: %08lld\n",
+ 			((phba->ktime_seg1_total +
+ 			phba->ktime_seg2_total  +
+ 			phba->ktime_seg3_total +
+ 			phba->ktime_seg4_total) /
+ 			phba->ktime_data_samples));
+ 		return len;
+ 	}
+ 	return len;
+ }
+ 
+ /**
+  * lpfc_debugfs_nvmeio_trc_data - Dump NVME IO trace list to a buffer
+  * @phba: The phba to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME IO trace associated with @phba
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmeio_trc_data(struct lpfc_hba *phba, char *buf, int size)
+ {
+ 	struct lpfc_debugfs_nvmeio_trc *dtp;
+ 	int i, state, index, skip;
+ 	int len = 0;
+ 
+ 	state = phba->nvmeio_trc_on;
+ 
+ 	index = (atomic_read(&phba->nvmeio_trc_cnt) + 1) &
+ 		(phba->nvmeio_trc_size - 1);
+ 	skip = phba->nvmeio_trc_output_idx;
+ 
+ 	len += snprintf(buf + len, size - len,
+ 			"%s IO Trace %s: next_idx %d skip %d size %d\n",
+ 			(phba->nvmet_support ? "NVME" : "NVMET"),
+ 			(state ? "Enabled" : "Disabled"),
+ 			index, skip, phba->nvmeio_trc_size);
+ 
+ 	if (!phba->nvmeio_trc || state)
+ 		return len;
+ 
+ 	/* trace MUST bhe off to continue */
+ 
+ 	for (i = index; i < phba->nvmeio_trc_size; i++) {
+ 		if (skip) {
+ 			skip--;
+ 			continue;
+ 		}
+ 		dtp = phba->nvmeio_trc + i;
+ 		phba->nvmeio_trc_output_idx++;
+ 
+ 		if (!dtp->fmt)
+ 			continue;
+ 
+ 		len +=  snprintf(buf + len, size - len, dtp->fmt,
+ 			dtp->data1, dtp->data2, dtp->data3);
+ 
+ 		if (phba->nvmeio_trc_output_idx >= phba->nvmeio_trc_size) {
+ 			phba->nvmeio_trc_output_idx = 0;
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Complete\n");
+ 			goto out;
+ 		}
+ 
+ 		if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Continue (%d of %d)\n",
+ 					phba->nvmeio_trc_output_idx,
+ 					phba->nvmeio_trc_size);
+ 			goto out;
+ 		}
+ 	}
+ 	for (i = 0; i < index; i++) {
+ 		if (skip) {
+ 			skip--;
+ 			continue;
+ 		}
+ 		dtp = phba->nvmeio_trc + i;
+ 		phba->nvmeio_trc_output_idx++;
+ 
+ 		if (!dtp->fmt)
+ 			continue;
+ 
+ 		len +=  snprintf(buf + len, size - len, dtp->fmt,
+ 			dtp->data1, dtp->data2, dtp->data3);
+ 
+ 		if (phba->nvmeio_trc_output_idx >= phba->nvmeio_trc_size) {
+ 			phba->nvmeio_trc_output_idx = 0;
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Complete\n");
+ 			goto out;
+ 		}
+ 
+ 		if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Continue (%d of %d)\n",
+ 					phba->nvmeio_trc_output_idx,
+ 					phba->nvmeio_trc_size);
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	len += snprintf(buf + len, size - len,
+ 			"Trace Done\n");
+ out:
+ 	return len;
+ }
+ 
+ /**
+  * lpfc_debugfs_cpucheck_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_cpucheck_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	int i;
+ 	int len = 0;
+ 	uint32_t tot_xmt = 0;
+ 	uint32_t tot_cmpl = 0;
+ 
+ 	if (phba->nvmet_support == 0) {
+ 		/* NVME Initiator */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"CPUcheck %s\n",
+ 				(phba->cpucheck_on & LPFC_CHECK_NVME_IO ?
+ 					"Enabled" : "Disabled"));
+ 		for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
+ 			if (i >= LPFC_CHECK_CPU_CNT)
+ 				break;
+ 			len += snprintf(buf + len, PAGE_SIZE - len,
+ 					"%02d: xmit x%08x cmpl x%08x\n",
+ 					i, phba->cpucheck_xmt_io[i],
+ 					phba->cpucheck_cmpl_io[i]);
+ 			tot_xmt += phba->cpucheck_xmt_io[i];
+ 			tot_cmpl += phba->cpucheck_cmpl_io[i];
+ 		}
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"tot:xmit x%08x cmpl x%08x\n",
+ 				tot_xmt, tot_cmpl);
+ 		return len;
+ 	}
+ 
+ 	return len;
+ }
+ 
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  #endif
  
  /**
@@@ -1243,6 -1630,360 +1526,363 @@@ lpfc_debugfs_dumpDataDif_release(struc
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ 
+ static int
+ lpfc_debugfs_nvmestat_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_vport *vport = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_NVMESTAT_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_nvmestat_data(vport, debug->buffer,
+ 		LPFC_NVMESTAT_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static int
+ lpfc_debugfs_nvmektime_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_vport *vport = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_NVMEKTIME_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_nvmektime_data(vport, debug->buffer,
+ 		LPFC_NVMEKTIME_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static ssize_t
+ lpfc_debugfs_nvmektime_write(struct file *file, const char __user *buf,
+ 			     size_t nbytes, loff_t *ppos)
+ {
+ 	struct lpfc_debug *debug = file->private_data;
+ 	struct lpfc_vport *vport = (struct lpfc_vport *)debug->i_private;
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	char mybuf[64];
+ 	char *pbuf;
+ 
+ 	if (nbytes > 64)
+ 		nbytes = 64;
+ 
+ 	/* Protect copy from user */
+ 	if (!access_ok(VERIFY_READ, buf, nbytes))
+ 		return -EFAULT;
+ 
+ 	memset(mybuf, 0, sizeof(mybuf));
+ 
+ 	if (copy_from_user(mybuf, buf, nbytes))
+ 		return -EFAULT;
+ 	pbuf = &mybuf[0];
+ 
+ 	if ((strncmp(pbuf, "on", sizeof("on") - 1) == 0)) {
+ 		phba->ktime_data_samples = 0;
+ 		phba->ktime_status_samples = 0;
+ 		phba->ktime_seg1_total = 0;
+ 		phba->ktime_seg1_max = 0;
+ 		phba->ktime_seg1_min = 0xffffffff;
+ 		phba->ktime_seg2_total = 0;
+ 		phba->ktime_seg2_max = 0;
+ 		phba->ktime_seg2_min = 0xffffffff;
+ 		phba->ktime_seg3_total = 0;
+ 		phba->ktime_seg3_max = 0;
+ 		phba->ktime_seg3_min = 0xffffffff;
+ 		phba->ktime_seg4_total = 0;
+ 		phba->ktime_seg4_max = 0;
+ 		phba->ktime_seg4_min = 0xffffffff;
+ 		phba->ktime_seg5_total = 0;
+ 		phba->ktime_seg5_max = 0;
+ 		phba->ktime_seg5_min = 0xffffffff;
+ 		phba->ktime_seg6_total = 0;
+ 		phba->ktime_seg6_max = 0;
+ 		phba->ktime_seg6_min = 0xffffffff;
+ 		phba->ktime_seg7_total = 0;
+ 		phba->ktime_seg7_max = 0;
+ 		phba->ktime_seg7_min = 0xffffffff;
+ 		phba->ktime_seg8_total = 0;
+ 		phba->ktime_seg8_max = 0;
+ 		phba->ktime_seg8_min = 0xffffffff;
+ 		phba->ktime_seg9_total = 0;
+ 		phba->ktime_seg9_max = 0;
+ 		phba->ktime_seg9_min = 0xffffffff;
+ 		phba->ktime_seg10_total = 0;
+ 		phba->ktime_seg10_max = 0;
+ 		phba->ktime_seg10_min = 0xffffffff;
+ 
+ 		phba->ktime_on = 1;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "off",
+ 		   sizeof("off") - 1) == 0)) {
+ 		phba->ktime_on = 0;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "zero",
+ 		   sizeof("zero") - 1) == 0)) {
+ 		phba->ktime_data_samples = 0;
+ 		phba->ktime_status_samples = 0;
+ 		phba->ktime_seg1_total = 0;
+ 		phba->ktime_seg1_max = 0;
+ 		phba->ktime_seg1_min = 0xffffffff;
+ 		phba->ktime_seg2_total = 0;
+ 		phba->ktime_seg2_max = 0;
+ 		phba->ktime_seg2_min = 0xffffffff;
+ 		phba->ktime_seg3_total = 0;
+ 		phba->ktime_seg3_max = 0;
+ 		phba->ktime_seg3_min = 0xffffffff;
+ 		phba->ktime_seg4_total = 0;
+ 		phba->ktime_seg4_max = 0;
+ 		phba->ktime_seg4_min = 0xffffffff;
+ 		phba->ktime_seg5_total = 0;
+ 		phba->ktime_seg5_max = 0;
+ 		phba->ktime_seg5_min = 0xffffffff;
+ 		phba->ktime_seg6_total = 0;
+ 		phba->ktime_seg6_max = 0;
+ 		phba->ktime_seg6_min = 0xffffffff;
+ 		phba->ktime_seg7_total = 0;
+ 		phba->ktime_seg7_max = 0;
+ 		phba->ktime_seg7_min = 0xffffffff;
+ 		phba->ktime_seg8_total = 0;
+ 		phba->ktime_seg8_max = 0;
+ 		phba->ktime_seg8_min = 0xffffffff;
+ 		phba->ktime_seg9_total = 0;
+ 		phba->ktime_seg9_max = 0;
+ 		phba->ktime_seg9_min = 0xffffffff;
+ 		phba->ktime_seg10_total = 0;
+ 		phba->ktime_seg10_max = 0;
+ 		phba->ktime_seg10_min = 0xffffffff;
+ 		return strlen(pbuf);
+ 	}
+ 	return -EINVAL;
+ }
+ 
+ static int
+ lpfc_debugfs_nvmeio_trc_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_hba *phba = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_NVMEIO_TRC_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_nvmeio_trc_data(phba, debug->buffer,
+ 		LPFC_NVMEIO_TRC_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static ssize_t
+ lpfc_debugfs_nvmeio_trc_write(struct file *file, const char __user *buf,
+ 			      size_t nbytes, loff_t *ppos)
+ {
+ 	struct lpfc_debug *debug = file->private_data;
+ 	struct lpfc_hba *phba = (struct lpfc_hba *)debug->i_private;
+ 	int i;
+ 	unsigned long sz;
+ 	char mybuf[64];
+ 	char *pbuf;
+ 
+ 	if (nbytes > 64)
+ 		nbytes = 64;
+ 
+ 	/* Protect copy from user */
+ 	if (!access_ok(VERIFY_READ, buf, nbytes))
+ 		return -EFAULT;
+ 
+ 	memset(mybuf, 0, sizeof(mybuf));
+ 
+ 	if (copy_from_user(mybuf, buf, nbytes))
+ 		return -EFAULT;
+ 	pbuf = &mybuf[0];
+ 
+ 	if ((strncmp(pbuf, "off", sizeof("off") - 1) == 0)) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0570 nvmeio_trc_off\n");
+ 		phba->nvmeio_trc_output_idx = 0;
+ 		phba->nvmeio_trc_on = 0;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "on", sizeof("on") - 1) == 0)) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0571 nvmeio_trc_on\n");
+ 		phba->nvmeio_trc_output_idx = 0;
+ 		phba->nvmeio_trc_on = 1;
+ 		return strlen(pbuf);
+ 	}
+ 
+ 	/* We must be off to allocate the trace buffer */
+ 	if (phba->nvmeio_trc_on != 0)
+ 		return -EINVAL;
+ 
+ 	/* If not on or off, the parameter is the trace buffer size */
+ 	i = kstrtoul(pbuf, 0, &sz);
+ 	if (i)
+ 		return -EINVAL;
+ 	phba->nvmeio_trc_size = (uint32_t)sz;
+ 
+ 	/* It must be a power of 2 - round down */
+ 	i = 0;
+ 	while (sz > 1) {
+ 		sz = sz >> 1;
+ 		i++;
+ 	}
+ 	sz = (1 << i);
+ 	if (phba->nvmeio_trc_size != sz)
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0572 nvmeio_trc_size changed to %ld\n",
+ 				sz);
+ 	phba->nvmeio_trc_size = (uint32_t)sz;
+ 
+ 	/* If one previously exists, free it */
+ 	kfree(phba->nvmeio_trc);
+ 
+ 	/* Allocate new trace buffer and initialize */
+ 	phba->nvmeio_trc = kmalloc((sizeof(struct lpfc_debugfs_nvmeio_trc) *
+ 				    sz), GFP_KERNEL);
+ 	if (!phba->nvmeio_trc) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0573 Cannot create debugfs "
+ 				"nvmeio_trc buffer\n");
+ 		return -ENOMEM;
+ 	}
+ 	memset(phba->nvmeio_trc, 0,
+ 	       (sizeof(struct lpfc_debugfs_nvmeio_trc) * sz));
+ 	atomic_set(&phba->nvmeio_trc_cnt, 0);
+ 	phba->nvmeio_trc_on = 0;
+ 	phba->nvmeio_trc_output_idx = 0;
+ 
+ 	return strlen(pbuf);
+ }
+ 
+ static int
+ lpfc_debugfs_cpucheck_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_vport *vport = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_CPUCHECK_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_cpucheck_data(vport, debug->buffer,
+ 		LPFC_NVMEKTIME_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static ssize_t
+ lpfc_debugfs_cpucheck_write(struct file *file, const char __user *buf,
+ 			    size_t nbytes, loff_t *ppos)
+ {
+ 	struct lpfc_debug *debug = file->private_data;
+ 	struct lpfc_vport *vport = (struct lpfc_vport *)debug->i_private;
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	char mybuf[64];
+ 	char *pbuf;
+ 	int i;
+ 
+ 	if (nbytes > 64)
+ 		nbytes = 64;
+ 
+ 	/* Protect copy from user */
+ 	if (!access_ok(VERIFY_READ, buf, nbytes))
+ 		return -EFAULT;
+ 
+ 	memset(mybuf, 0, sizeof(mybuf));
+ 
+ 	if (copy_from_user(mybuf, buf, nbytes))
+ 		return -EFAULT;
+ 	pbuf = &mybuf[0];
+ 
+ 	if ((strncmp(pbuf, "on", sizeof("on") - 1) == 0)) {
+ 		phba->cpucheck_on |= LPFC_CHECK_NVME_IO;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "rcv",
+ 		   sizeof("rcv") - 1) == 0)) {
+ 		if (phba->nvmet_support)
+ 			phba->cpucheck_on |= LPFC_CHECK_NVMET_RCV;
+ 		else
+ 			return -EINVAL;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "off",
+ 		   sizeof("off") - 1) == 0)) {
+ 		phba->cpucheck_on = LPFC_CHECK_OFF;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "zero",
+ 		   sizeof("zero") - 1) == 0)) {
+ 		for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
+ 			if (i >= LPFC_CHECK_CPU_CNT)
+ 				break;
+ 			phba->cpucheck_rcv_io[i] = 0;
+ 			phba->cpucheck_xmt_io[i] = 0;
+ 			phba->cpucheck_cmpl_io[i] = 0;
+ 			phba->cpucheck_ccmpl_io[i] = 0;
+ 		}
+ 		return strlen(pbuf);
+ 	}
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  /*
   * ---------------------------------
   * iDiag debugfs file access methods
diff --cc drivers/scsi/lpfc/lpfc_hw4.h
index 554ba749ed19,9bee888b7dc4..000000000000
--- a/drivers/scsi/lpfc/lpfc_hw4.h
+++ b/drivers/scsi/lpfc/lpfc_hw4.h
@@@ -3974,6 -4099,12 +4016,15 @@@ union lpfc_wqe 
  union lpfc_wqe128 {
  	uint32_t words[32];
  	struct lpfc_wqe_generic generic;
++<<<<<<< HEAD
++=======
+ 	struct fcp_icmnd64_wqe fcp_icmd;
+ 	struct fcp_iread64_wqe fcp_iread;
+ 	struct fcp_iwrite64_wqe fcp_iwrite;
+ 	struct fcp_trsp64_wqe fcp_trsp;
+ 	struct fcp_tsend64_wqe fcp_tsend;
+ 	struct fcp_treceive64_wqe fcp_treceive;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	struct xmit_seq64_wqe xmit_sequence;
  	struct gen_req64_wqe gen_req;
  };
diff --cc drivers/scsi/lpfc/lpfc_init.c
index bbd4080d8122,03fa3f02e33e..000000000000
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@@ -1013,12 -1025,18 +1015,25 @@@ lpfc_hba_down_post_s4(struct lpfc_hba *
  	list_for_each_entry(sglq_entry,
  		&phba->sli4_hba.lpfc_abts_els_sgl_list, list)
  		sglq_entry->state = SGL_FREED;
+ 	list_for_each_entry(sglq_entry,
+ 		&phba->sli4_hba.lpfc_abts_nvmet_sgl_list, list)
+ 		sglq_entry->state = SGL_FREED;
  
 +	spin_lock(&pring->ring_lock);
  	list_splice_init(&phba->sli4_hba.lpfc_abts_els_sgl_list,
++<<<<<<< HEAD
 +			&phba->sli4_hba.lpfc_sgl_list);
 +	spin_unlock(&pring->ring_lock);
 +	spin_unlock(&phba->sli4_hba.abts_sgl_list_lock);
++=======
+ 			&phba->sli4_hba.lpfc_els_sgl_list);
+ 
+ 	if (phba->sli4_hba.nvme_wq)
+ 		list_splice_init(&phba->sli4_hba.lpfc_abts_nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 
+ 	spin_unlock(&phba->sli4_hba.sgl_list_lock);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	/* abts_scsi_buf_list_lock required because worker thread uses this
  	 * list.
  	 */
@@@ -3252,6 -3322,160 +3267,163 @@@ lpfc_sli4_xri_sgl_update(struct lpfc_hb
  		sglq_entry->sli4_lxritag = lxri;
  		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
  	}
++<<<<<<< HEAD
++=======
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_els_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_nvmet_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_sglq *sglq_entry = NULL, *sglq_entry_next = NULL;
+ 	uint16_t i, lxri, xri_cnt, els_xri_cnt;
+ 	uint16_t nvmet_xri_cnt, tot_cnt;
+ 	LIST_HEAD(nvmet_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's nvmet xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	nvmet_xri_cnt = 0;
+ 	tot_cnt = phba->sli4_hba.max_cfg_param.max_xri - els_xri_cnt;
+ 
+ 	if (nvmet_xri_cnt > phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* els xri-sgl expanded */
+ 		xri_cnt = nvmet_xri_cnt - phba->sli4_hba.nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6302 NVMET xri-sgl cnt grew from %d to %d\n",
+ 				phba->sli4_hba.nvmet_xri_cnt, nvmet_xri_cnt);
+ 		/* allocate the additional nvmet sgls */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			sglq_entry = kzalloc(sizeof(struct lpfc_sglq),
+ 					     GFP_KERNEL);
+ 			if (sglq_entry == NULL) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6303 Failure to allocate an "
+ 						"NVMET sgl entry:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->buff_type = NVMET_BUFF_TYPE;
+ 			sglq_entry->virt = lpfc_nvmet_buf_alloc(phba, 0,
+ 							   &sglq_entry->phys);
+ 			if (sglq_entry->virt == NULL) {
+ 				kfree(sglq_entry);
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6304 Failure to allocate an "
+ 						"NVMET buf:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->sgl = sglq_entry->virt;
+ 			memset(sglq_entry->sgl, 0,
+ 			       phba->cfg_sg_dma_buf_size);
+ 			sglq_entry->state = SGL_FREED;
+ 			list_add_tail(&sglq_entry->list, &nvmet_sgl_list);
+ 		}
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else if (nvmet_xri_cnt < phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* nvmet xri-sgl shrunk */
+ 		xri_cnt = phba->sli4_hba.nvmet_xri_cnt - nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6305 NVMET xri-sgl count decreased from "
+ 				"%d to %d\n", phba->sli4_hba.nvmet_xri_cnt,
+ 				nvmet_xri_cnt);
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&phba->sli4_hba.lpfc_nvmet_sgl_list,
+ 				 &nvmet_sgl_list);
+ 		/* release extra nvmet sgls from list */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			list_remove_head(&nvmet_sgl_list,
+ 					 sglq_entry, struct lpfc_sglq, list);
+ 			if (sglq_entry) {
+ 				lpfc_nvmet_buf_free(phba, sglq_entry->virt,
+ 						    sglq_entry->phys);
+ 				kfree(sglq_entry);
+ 			}
+ 		}
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6306 NVMET xri-sgl count unchanged: %d\n",
+ 				nvmet_xri_cnt);
+ 	phba->sli4_hba.nvmet_xri_cnt = nvmet_xri_cnt;
+ 
+ 	/* update xris to nvmet sgls on the list */
+ 	sglq_entry = NULL;
+ 	sglq_entry_next = NULL;
+ 	list_for_each_entry_safe(sglq_entry, sglq_entry_next,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list, list) {
+ 		lxri = lpfc_sli4_next_xritag(phba);
+ 		if (lxri == NO_XRI) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 					"6307 Failed to allocate xri for "
+ 					"NVMET sgl\n");
+ 			rc = -ENOMEM;
+ 			goto out_free_mem;
+ 		}
+ 		sglq_entry->sli4_lxritag = lxri;
+ 		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
+ 	}
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_nvmet_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_scsi_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_scsi_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_scsi_buf *psb, *psb_next;
+ 	uint16_t i, lxri, els_xri_cnt, scsi_xri_cnt;
+ 	LIST_HEAD(scsi_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's els xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	phba->total_scsi_bufs = 0;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  
  	/*
  	 * update on pci function's allocated scsi xri-sgl list
@@@ -4989,32 -5327,75 +5161,82 @@@ lpfc_sli_probe_sriov_nr_virtfn(struct l
  }
  
  /**
 - * lpfc_setup_driver_resource_phase1 - Phase1 etup driver internal resources.
 + * lpfc_sli_driver_resource_setup - Setup driver internal resources for SLI3 dev.
   * @phba: pointer to lpfc hba data structure.
   *
 - * This routine is invoked to set up the driver internal resources before the
 - * device specific resource setup to support the HBA device it attached to.
 + * This routine is invoked to set up the driver internal resources specific to
 + * support the SLI-3 HBA device it attached to.
   *
   * Return codes
 - *	0 - successful
 - *	other values - error
 + * 	0 - successful
 + * 	other values - error
   **/
  static int
 -lpfc_setup_driver_resource_phase1(struct lpfc_hba *phba)
 +lpfc_sli_driver_resource_setup(struct lpfc_hba *phba)
  {
 -	struct lpfc_sli *psli = &phba->sli;
 +	struct lpfc_sli *psli;
 +	int rc;
  
  	/*
 -	 * Driver resources common to all SLI revisions
 +	 * Initialize timers used by driver
  	 */
 -	atomic_set(&phba->fast_event_count, 0);
 -	spin_lock_init(&phba->hbalock);
  
++<<<<<<< HEAD
 +	/* Heartbeat timer */
 +	init_timer(&phba->hb_tmofunc);
 +	phba->hb_tmofunc.function = lpfc_hb_timeout;
 +	phba->hb_tmofunc.data = (unsigned long)phba;
++=======
+ 	/* Initialize ndlp management spinlock */
+ 	spin_lock_init(&phba->ndlp_lock);
+ 
+ 	INIT_LIST_HEAD(&phba->port_list);
+ 	INIT_LIST_HEAD(&phba->work_list);
+ 	init_waitqueue_head(&phba->wait_4_mlo_m_q);
+ 
+ 	/* Initialize the wait queue head for the kernel thread */
+ 	init_waitqueue_head(&phba->work_waitq);
+ 
+ 	lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
+ 			"1403 Protocols supported %s %s %s\n",
+ 			((phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) ?
+ 				"SCSI" : " "),
+ 			((phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) ?
+ 				"NVME" : " "),
+ 			(phba->nvmet_support ? "NVMET" : " "));
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
+ 		/* Initialize the scsi buffer list used by driver for scsi IO */
+ 		spin_lock_init(&phba->scsi_buf_list_get_lock);
+ 		INIT_LIST_HEAD(&phba->lpfc_scsi_buf_list_get);
+ 		spin_lock_init(&phba->scsi_buf_list_put_lock);
+ 		INIT_LIST_HEAD(&phba->lpfc_scsi_buf_list_put);
+ 	}
+ 
+ 	if ((phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) &&
+ 		(phba->nvmet_support == 0)) {
+ 		/* Initialize the NVME buffer list used by driver for NVME IO */
+ 		spin_lock_init(&phba->nvme_buf_list_get_lock);
+ 		INIT_LIST_HEAD(&phba->lpfc_nvme_buf_list_get);
+ 		spin_lock_init(&phba->nvme_buf_list_put_lock);
+ 		INIT_LIST_HEAD(&phba->lpfc_nvme_buf_list_put);
+ 	}
+ 
+ 	/* Initialize the fabric iocb list */
+ 	INIT_LIST_HEAD(&phba->fabric_iocb_list);
+ 
+ 	/* Initialize list to save ELS buffers */
+ 	INIT_LIST_HEAD(&phba->elsbuf);
+ 
+ 	/* Initialize FCF connection rec list */
+ 	INIT_LIST_HEAD(&phba->fcf_conn_rec_list);
+ 
+ 	/* Initialize OAS configuration list */
+ 	spin_lock_init(&phba->devicelock);
+ 	INIT_LIST_HEAD(&phba->luns);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  
 +	psli = &phba->sli;
  	/* MBOX heartbeat timer */
  	init_timer(&psli->mbox_tmo);
  	psli->mbox_tmo.function = lpfc_mbox_timeout;
@@@ -5163,15 -5578,19 +5385,21 @@@ lpfc_sli_driver_resource_unset(struct l
  static int
  lpfc_sli4_driver_resource_setup(struct lpfc_hba *phba)
  {
 +	struct lpfc_vector_map_info *cpup;
 +	struct lpfc_sli *psli;
  	LPFC_MBOXQ_t *mboxq;
++<<<<<<< HEAD
 +	int rc, i, hbq_count, max_buf_size;
++=======
+ 	MAILBOX_t *mb;
+ 	int rc, i, max_buf_size;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	uint8_t pn_page[LPFC_MAX_SUPPORTED_PAGES] = {0};
  	struct lpfc_mqe *mqe;
  	int longs;
  	int fof_vectors = 0;
+ 	uint64_t wwn;
  
 -	phba->sli4_hba.num_online_cpu = num_online_cpus();
 -	phba->sli4_hba.num_present_cpu = lpfc_present_cpu;
 -	phba->sli4_hba.curr_disp_cpu = 0;
 -
  	/* Get all the module params for configuring this host */
  	lpfc_get_cfgparam(phba);
  
@@@ -5316,11 -5717,21 +5544,16 @@@
  	/*
  	 * Initialize the SLI Layer to run with lpfc SLI4 HBAs.
  	 */
 -	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
 -		/* Initialize the Abort scsi buffer list used by driver */
 -		spin_lock_init(&phba->sli4_hba.abts_scsi_buf_list_lock);
 -		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
 -	}
 -
 -	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {
 -		/* Initialize the Abort nvme buffer list used by driver */
 -		spin_lock_init(&phba->sli4_hba.abts_nvme_buf_list_lock);
 -		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_nvme_buf_list);
 -	}
 -
 +	/* Initialize the Abort scsi buffer list used by driver */
 +	spin_lock_init(&phba->sli4_hba.abts_scsi_buf_list_lock);
 +	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
  	/* This abort list used by worker thread */
++<<<<<<< HEAD
 +	spin_lock_init(&phba->sli4_hba.abts_sgl_list_lock);
++=======
+ 	spin_lock_init(&phba->sli4_hba.sgl_list_lock);
+ 	spin_lock_init(&phba->sli4_hba.nvmet_io_lock);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  
  	/*
  	 * Initialize driver internal slow-path work queues
@@@ -5399,6 -5808,46 +5632,49 @@@
  		goto out_free_bsmbx;
  	}
  
++<<<<<<< HEAD
++=======
+ 	/* Check for NVMET being configured */
+ 	phba->nvmet_support = 0;
+ 	if (lpfc_enable_nvmet_cnt) {
+ 
+ 		/* First get WWN of HBA instance */
+ 		lpfc_read_nv(phba, mboxq);
+ 		rc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);
+ 		if (rc != MBX_SUCCESS) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 					"6016 Mailbox failed , mbxCmd x%x "
+ 					"READ_NV, mbxStatus x%x\n",
+ 					bf_get(lpfc_mqe_command, &mboxq->u.mqe),
+ 					bf_get(lpfc_mqe_status, &mboxq->u.mqe));
+ 			rc = -EIO;
+ 			goto out_free_bsmbx;
+ 		}
+ 		mb = &mboxq->u.mb;
+ 		memcpy(&wwn, (char *)mb->un.varRDnvp.nodename,
+ 		       sizeof(uint64_t));
+ 		wwn = cpu_to_be64(wwn);
+ 		phba->sli4_hba.wwnn.u.name = wwn;
+ 		memcpy(&wwn, (char *)mb->un.varRDnvp.portname,
+ 		       sizeof(uint64_t));
+ 		/* wwn is WWPN of HBA instance */
+ 		wwn = cpu_to_be64(wwn);
+ 		phba->sli4_hba.wwpn.u.name = wwn;
+ 
+ 		/* Check to see if it matches any module parameter */
+ 		for (i = 0; i < lpfc_enable_nvmet_cnt; i++) {
+ 			if (wwn == lpfc_enable_nvmet[i]) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"6017 NVME Target %016llx\n",
+ 						wwn);
+ 				phba->nvmet_support = 1; /* a match */
+ 			}
+ 		}
+ 	}
+ 
+ 	lpfc_nvme_mod_param_dep(phba);
+ 
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	/* Get the Supported Pages if PORT_CAPABILITIES is supported by port. */
  	lpfc_supported_pages(mboxq);
  	rc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);
@@@ -5936,8 -6335,10 +6240,10 @@@ static voi
  lpfc_init_sgl_list(struct lpfc_hba *phba)
  {
  	/* Initialize and populate the sglq list per host/VF. */
 -	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_els_sgl_list);
 +	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_sgl_list);
  	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_els_sgl_list);
+ 	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_nvmet_sgl_list);
  
  	/* els xri-sgl book keeping */
  	phba->sli4_hba.els_xri_cnt = 0;
@@@ -6214,6 -6616,23 +6520,26 @@@ lpfc_create_shost(struct lpfc_hba *phba
  
  	shost = lpfc_shost_from_vport(vport);
  	phba->pport = vport;
++<<<<<<< HEAD
++=======
+ 
+ 	if (phba->nvmet_support) {
+ 		/* Only 1 vport (pport) will support NVME target */
+ 		if (phba->txrdy_payload_pool == NULL) {
+ 			phba->txrdy_payload_pool = pci_pool_create(
+ 				"txrdy_pool", phba->pcidev,
+ 				TXRDY_PAYLOAD_LEN, 16, 0);
+ 			if (phba->txrdy_payload_pool) {
+ 				phba->targetport = NULL;
+ 				phba->cfg_enable_fc4_type = LPFC_ENABLE_NVME;
+ 				lpfc_printf_log(phba, KERN_INFO,
+ 						LOG_INIT | LOG_NVME_DISC,
+ 						"6076 NVME Target Found\n");
+ 			}
+ 		}
+ 	}
+ 
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	lpfc_debugfs_initialize(vport);
  	/* Put reference to SCSI host to driver's device private data */
  	pci_set_drvdata(phba->pcidev, shost);
@@@ -7266,14 -7662,23 +7592,29 @@@ lpfc_sli4_queue_verify(struct lpfc_hba 
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
  				"2575 Reducing IO channels to match number of "
  				"available EQs: from %d to %d\n",
 -				io_channel,
 +				cfg_fcp_io_channel,
  				phba->sli4_hba.max_cfg_param.max_eq);
 -		io_channel = phba->sli4_hba.max_cfg_param.max_eq - fof_vectors;
 +		cfg_fcp_io_channel = phba->sli4_hba.max_cfg_param.max_eq -
 +			fof_vectors;
  	}
  
++<<<<<<< HEAD
 +	/* The actual number of FCP event queues adopted */
 +	phba->cfg_fcp_io_channel = cfg_fcp_io_channel;
++=======
+ 	/* The actual number of FCP / NVME event queues adopted */
+ 	if (io_channel != phba->io_channel_irqs)
+ 		phba->io_channel_irqs = io_channel;
+ 	if (phba->cfg_fcp_io_channel > io_channel)
+ 		phba->cfg_fcp_io_channel = io_channel;
+ 	if (phba->cfg_nvme_io_channel > io_channel)
+ 		phba->cfg_nvme_io_channel = io_channel;
+ 
+ 	lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 			"2574 IO channels: irqs %d fcp %d nvme %d\n",
+ 			phba->io_channel_irqs, phba->cfg_fcp_io_channel,
+ 			phba->cfg_nvme_io_channel);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  
  	/* Get EQ depth from module parameter, fake the default for now */
  	phba->sli4_hba.eq_esize = LPFC_EQE_SIZE_4B;
@@@ -9058,34 -9377,19 +9399,41 @@@ lpfc_sli4_enable_msix(struct lpfc_hba *
  	int vectors, rc, index;
  
  	/* Set up MSI-X multi-message vectors */
 -	vectors = phba->io_channel_irqs;
 -	if (phba->cfg_fof)
 -		vectors++;
 +	for (index = 0; index < phba->cfg_fcp_io_channel; index++)
 +		phba->sli4_hba.msix_entries[index].entry = index;
  
++<<<<<<< HEAD
 +	/* Configure MSI-X capability structure */
 +	vectors = phba->cfg_fcp_io_channel;
 +	if (phba->cfg_fof) {
 +		phba->sli4_hba.msix_entries[index].entry = index;
 +		vectors++;
 +	}
 +enable_msix_vectors:
 +	rc = pci_enable_msix(phba->pcidev, phba->sli4_hba.msix_entries,
 +			     vectors);
 +	if (rc > 1) {
 +		vectors = rc;
 +		goto enable_msix_vectors;
 +	} else if (rc) {
++=======
+ 	rc = pci_alloc_irq_vectors(phba->pcidev,
+ 				(phba->nvmet_support) ? 1 : 2,
+ 				vectors, PCI_IRQ_MSIX | PCI_IRQ_AFFINITY);
+ 	if (rc < 0) {
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
  				"0484 PCI enable MSI-X failed (%d)\n", rc);
 -		goto vec_fail_out;
 +		goto msi_fail_out;
  	}
 -	vectors = rc;
 +
 +	/* Log MSI-X vector assignment */
 +	for (index = 0; index < vectors; index++)
 +		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 +				"0489 MSI-X entry[%d]: vector=x%x "
 +				"message=%d\n", index,
 +				phba->sli4_hba.msix_entries[index].vector,
 +				phba->sli4_hba.msix_entries[index].entry);
  
  	/* Assign MSI-X vectors to interrupt handlers */
  	for (index = 0; index < vectors; index++) {
@@@ -9393,11 -9662,27 +9741,25 @@@ static voi
  lpfc_sli4_xri_exchange_busy_wait(struct lpfc_hba *phba)
  {
  	int wait_time = 0;
 -	int nvme_xri_cmpl = 1;
 -	int fcp_xri_cmpl = 1;
 +	int fcp_xri_cmpl = list_empty(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
  	int els_xri_cmpl = list_empty(&phba->sli4_hba.lpfc_abts_els_sgl_list);
+ 	int nvmet_xri_cmpl =
+ 			list_empty(&phba->sli4_hba.lpfc_abts_nvmet_sgl_list);
  
++<<<<<<< HEAD
 +	while (!fcp_xri_cmpl || !els_xri_cmpl) {
++=======
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP)
+ 		fcp_xri_cmpl =
+ 			list_empty(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME)
+ 		nvme_xri_cmpl =
+ 			list_empty(&phba->sli4_hba.lpfc_abts_nvme_buf_list);
+ 
+ 	while (!fcp_xri_cmpl || !els_xri_cmpl || !nvme_xri_cmpl ||
+ 	       !nvmet_xri_cmpl) {
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  		if (wait_time > LPFC_XRI_EXCH_BUSY_WAIT_TMO) {
 -			if (!nvme_xri_cmpl)
 -				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -						"6100 NVME XRI exchange busy "
 -						"wait time: %d seconds.\n",
 -						wait_time/1000);
  			if (!fcp_xri_cmpl)
  				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
  						"2877 FCP XRI exchange busy "
@@@ -9414,10 -9699,19 +9776,13 @@@
  			msleep(LPFC_XRI_EXCH_BUSY_WAIT_T1);
  			wait_time += LPFC_XRI_EXCH_BUSY_WAIT_T1;
  		}
 -		if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME)
 -			nvme_xri_cmpl = list_empty(
 -				&phba->sli4_hba.lpfc_abts_nvme_buf_list);
 -
 -		if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP)
 -			fcp_xri_cmpl = list_empty(
 -				&phba->sli4_hba.lpfc_abts_scsi_buf_list);
 -
 +		fcp_xri_cmpl =
 +			list_empty(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
  		els_xri_cmpl =
  			list_empty(&phba->sli4_hba.lpfc_abts_els_sgl_list);
+ 
+ 		nvmet_xri_cmpl =
+ 			list_empty(&phba->sli4_hba.lpfc_abts_nvmet_sgl_list);
  	}
  }
  
@@@ -9633,7 -9928,30 +9998,10 @@@ lpfc_get_sli4_parameters(struct lpfc_hb
  					   mbx_sli4_parameters);
  	phba->sli4_hba.extents_in_use = bf_get(cfg_ext, mbx_sli4_parameters);
  	phba->sli4_hba.rpi_hdrs_in_use = bf_get(cfg_hdrr, mbx_sli4_parameters);
 -	phba->nvme_support = (bf_get(cfg_nvme, mbx_sli4_parameters) &&
 -			      bf_get(cfg_xib, mbx_sli4_parameters));
 -
 -	if ((phba->cfg_enable_fc4_type == LPFC_ENABLE_FCP) ||
 -	    !phba->nvme_support) {
 -		phba->nvme_support = 0;
 -		phba->nvmet_support = 0;
 -		phba->cfg_nvme_io_channel = 0;
 -		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
 -		lpfc_printf_log(phba, KERN_ERR, LOG_INIT | LOG_NVME,
 -				"6101 Disabling NVME support: "
 -				"Not supported by firmware: %d %d\n",
 -				bf_get(cfg_nvme, mbx_sli4_parameters),
 -				bf_get(cfg_xib, mbx_sli4_parameters));
 -
 -		/* If firmware doesn't support NVME, just use SCSI support */
 -		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP))
 -			return -ENODEV;
 -		phba->cfg_enable_fc4_type = LPFC_ENABLE_FCP;
 -	}
  
+ 	if (bf_get(cfg_xib, mbx_sli4_parameters) && phba->cfg_suppress_rsp)
+ 		phba->sli.sli_flag |= LPFC_SLI_SUPPRESS_RSP;
+ 
  	/* Make sure that sge_supp_len can be handled by the driver */
  	if (sli4_params->sge_supp_len > LPFC_MAX_SGE_SIZE)
  		sli4_params->sge_supp_len = LPFC_MAX_SGE_SIZE;
@@@ -10275,6 -10600,23 +10643,26 @@@ lpfc_sli4_get_els_iocb_cnt(struct lpfc_
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_sli4_get_iocb_cnt - Calculate the # of total IOCBs to reserve
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * returns the number of ELS/CT + NVMET IOCBs to reserve
+  **/
+ int
+ lpfc_sli4_get_iocb_cnt(struct lpfc_hba *phba)
+ {
+ 	int max_xri = lpfc_sli4_get_els_iocb_cnt(phba);
+ 
+ 	if (phba->nvmet_support)
+ 		max_xri += LPFC_NVMET_BUF_POST;
+ 	return max_xri;
+ }
+ 
+ 
+ /**
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
   * lpfc_write_firmware - attempt to write a firmware image to the port
   * @fw: pointer to firmware image returned from request_firmware.
   * @phba: pointer to lpfc hba data structure.
@@@ -10628,8 -10984,12 +11016,9 @@@ lpfc_pci_remove_one_s4(struct pci_dev *
  	/* Remove FC host and then SCSI host with the physical port */
  	fc_remove_host(shost);
  	scsi_remove_host(shost);
+ 	/* todo: tgt: remove targetport */
  
 -	/* Perform ndlp cleanup on the physical port.  The nvme localport
 -	 * is destroyed after to ensure all rports are io-disabled.
 -	 */
 -	lpfc_nvme_destroy_localport(vport);
 +	/* Perform cleanup on the physical port */
  	lpfc_cleanup(vport);
  
  	/*
diff --cc drivers/scsi/lpfc/lpfc_mem.c
index 3fa65338d3f5,32db255f5216..000000000000
--- a/drivers/scsi/lpfc/lpfc_mem.c
+++ b/drivers/scsi/lpfc/lpfc_mem.c
@@@ -35,8 -37,10 +35,13 @@@
  #include "lpfc_sli4.h"
  #include "lpfc_nl.h"
  #include "lpfc_disc.h"
 -#include "lpfc.h"
  #include "lpfc_scsi.h"
++<<<<<<< HEAD
 +#include "lpfc.h"
++=======
+ #include "lpfc_nvme.h"
+ #include "lpfc_nvmet.h"
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  #include "lpfc_crtn.h"
  #include "lpfc_logmsg.h"
  
@@@ -540,9 -590,136 +583,137 @@@ lpfc_sli4_rb_free(struct lpfc_hba *phba
  	pci_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
  	pci_pool_free(phba->lpfc_drb_pool, dmab->dbuf.virt, dmab->dbuf.phys);
  	kfree(dmab);
 +	return;
  }
  
+ /**
+  * lpfc_sli4_nvmet_alloc - Allocate an SLI4 Receive buffer
+  * @phba: HBA to allocate a receive buffer for
+  *
+  * Description: Allocates a DMA-mapped receive buffer from the lpfc_hrb_pool PCI
+  * pool along a non-DMA-mapped container for it.
+  *
+  * Notes: Not interrupt-safe.  Must be called with no locks held.
+  *
+  * Returns:
+  *   pointer to HBQ on success
+  *   NULL on failure
+  **/
+ struct rqb_dmabuf *
+ lpfc_sli4_nvmet_alloc(struct lpfc_hba *phba)
+ {
+ 	struct rqb_dmabuf *dma_buf;
+ 	struct lpfc_iocbq *nvmewqe;
+ 	union lpfc_wqe128 *wqe;
+ 
+ 	dma_buf = kzalloc(sizeof(struct rqb_dmabuf), GFP_KERNEL);
+ 	if (!dma_buf)
+ 		return NULL;
+ 
+ 	dma_buf->hbuf.virt = pci_pool_alloc(phba->lpfc_hrb_pool, GFP_KERNEL,
+ 					    &dma_buf->hbuf.phys);
+ 	if (!dma_buf->hbuf.virt) {
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->dbuf.virt = pci_pool_alloc(phba->lpfc_drb_pool, GFP_KERNEL,
+ 					    &dma_buf->dbuf.phys);
+ 	if (!dma_buf->dbuf.virt) {
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->total_size = LPFC_DATA_BUF_SIZE;
+ 
+ 	dma_buf->context = kzalloc(sizeof(struct lpfc_nvmet_rcv_ctx),
+ 				   GFP_KERNEL);
+ 	if (!dma_buf->context) {
+ 		pci_pool_free(phba->lpfc_drb_pool, dma_buf->dbuf.virt,
+ 			      dma_buf->dbuf.phys);
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 
+ 	dma_buf->iocbq = lpfc_sli_get_iocbq(phba);
+ 	dma_buf->iocbq->iocb_flag = LPFC_IO_NVMET;
+ 	if (!dma_buf->iocbq) {
+ 		kfree(dma_buf->context);
+ 		pci_pool_free(phba->lpfc_drb_pool, dma_buf->dbuf.virt,
+ 			      dma_buf->dbuf.phys);
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_NVME,
+ 				"2621 Ran out of nvmet iocb/WQEs\n");
+ 		return NULL;
+ 	}
+ 	nvmewqe = dma_buf->iocbq;
+ 	wqe = (union lpfc_wqe128 *)&nvmewqe->wqe;
+ 	/* Initialize WQE */
+ 	memset(wqe, 0, sizeof(union lpfc_wqe));
+ 	/* Word 7 */
+ 	bf_set(wqe_ct, &wqe->generic.wqe_com, SLI4_CT_RPI);
+ 	bf_set(wqe_class, &wqe->generic.wqe_com, CLASS3);
+ 	bf_set(wqe_pu, &wqe->generic.wqe_com, 1);
+ 	/* Word 10 */
+ 	bf_set(wqe_nvme, &wqe->fcp_tsend.wqe_com, 1);
+ 	bf_set(wqe_ebde_cnt, &wqe->generic.wqe_com, 0);
+ 	bf_set(wqe_qosd, &wqe->generic.wqe_com, 0);
+ 
+ 	dma_buf->iocbq->context1 = NULL;
+ 	spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 	dma_buf->sglq = __lpfc_sli_get_nvmet_sglq(phba, dma_buf->iocbq);
+ 	spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 	if (!dma_buf->sglq) {
+ 		lpfc_sli_release_iocbq(phba, dma_buf->iocbq);
+ 		kfree(dma_buf->context);
+ 		pci_pool_free(phba->lpfc_drb_pool, dma_buf->dbuf.virt,
+ 			      dma_buf->dbuf.phys);
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_NVME,
+ 				"6132 Ran out of nvmet XRIs\n");
+ 		return NULL;
+ 	}
+ 	return dma_buf;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_free - Frees a receive buffer
+  * @phba: HBA buffer was allocated for
+  * @dmab: DMA Buffer container returned by lpfc_sli4_rbq_alloc
+  *
+  * Description: Frees both the container and the DMA-mapped buffers returned by
+  * lpfc_sli4_nvmet_alloc.
+  *
+  * Notes: Can be called with or without locks held.
+  *
+  * Returns: None
+  **/
+ void
+ lpfc_sli4_nvmet_free(struct lpfc_hba *phba, struct rqb_dmabuf *dmab)
+ {
+ 	unsigned long flags;
+ 
+ 	__lpfc_clear_active_sglq(phba, dmab->sglq->sli4_lxritag);
+ 	dmab->sglq->state = SGL_FREED;
+ 	dmab->sglq->ndlp = NULL;
+ 
+ 	spin_lock_irqsave(&phba->sli4_hba.sgl_list_lock, flags);
+ 	list_add_tail(&dmab->sglq->list, &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 	spin_unlock_irqrestore(&phba->sli4_hba.sgl_list_lock, flags);
+ 
+ 	lpfc_sli_release_iocbq(phba, dmab->iocbq);
+ 	kfree(dmab->context);
+ 	pci_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
+ 	pci_pool_free(phba->lpfc_drb_pool, dmab->dbuf.virt, dmab->dbuf.phys);
+ 	kfree(dmab);
+ }
+ 
  /**
   * lpfc_in_buf_free - Free a DMA buffer
   * @phba: HBA buffer is associated with
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index c1522c6b2e42,f2cd0f37e03f..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -39,8 -42,10 +39,13 @@@
  #include "lpfc_sli4.h"
  #include "lpfc_nl.h"
  #include "lpfc_disc.h"
 -#include "lpfc.h"
  #include "lpfc_scsi.h"
++<<<<<<< HEAD
 +#include "lpfc.h"
++=======
+ #include "lpfc_nvme.h"
+ #include "lpfc_nvmet.h"
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  #include "lpfc_crtn.h"
  #include "lpfc_logmsg.h"
  #include "lpfc_compat.h"
@@@ -1013,16 -1060,30 +1046,32 @@@ __lpfc_sli_release_iocbq_s4(struct lpfc
  
  
  	if (sglq)  {
++<<<<<<< HEAD
++=======
+ 		if (iocbq->iocb_flag & LPFC_IO_NVMET) {
+ 			spin_lock_irqsave(&phba->sli4_hba.sgl_list_lock,
+ 					  iflag);
+ 			sglq->state = SGL_FREED;
+ 			sglq->ndlp = NULL;
+ 			list_add_tail(&sglq->list,
+ 				      &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 			spin_unlock_irqrestore(
+ 				&phba->sli4_hba.sgl_list_lock, iflag);
+ 			goto out;
+ 		}
+ 
+ 		pring = phba->sli4_hba.els_wq->pring;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  		if ((iocbq->iocb_flag & LPFC_EXCHANGE_BUSY) &&
  			(sglq->state != SGL_XRI_ABORTED)) {
 -			spin_lock_irqsave(&phba->sli4_hba.sgl_list_lock,
 -					  iflag);
 +			spin_lock_irqsave(&phba->sli4_hba.abts_sgl_list_lock,
 +					iflag);
  			list_add(&sglq->list,
 -				 &phba->sli4_hba.lpfc_abts_els_sgl_list);
 +				&phba->sli4_hba.lpfc_abts_els_sgl_list);
  			spin_unlock_irqrestore(
 -				&phba->sli4_hba.sgl_list_lock, iflag);
 +				&phba->sli4_hba.abts_sgl_list_lock, iflag);
  		} else {
 -			spin_lock_irqsave(&phba->sli4_hba.sgl_list_lock,
 -					  iflag);
 +			spin_lock_irqsave(&pring->ring_lock, iflag);
  			sglq->state = SGL_FREED;
  			sglq->ndlp = NULL;
  			list_add_tail(&sglq->list,
@@@ -1035,13 -1097,15 +1084,22 @@@
  		}
  	}
  
++<<<<<<< HEAD
 +
++=======
+ out:
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	/*
  	 * Clean all volatile data fields, preserve iotag and node struct.
  	 */
  	memset((char *)iocbq + start_clean, 0, sizeof(*iocbq) - start_clean);
  	iocbq->sli4_lxritag = NO_XRI;
  	iocbq->sli4_xritag = NO_XRI;
++<<<<<<< HEAD
++=======
+ 	iocbq->iocb_flag &= ~(LPFC_IO_NVME | LPFC_IO_NVMET |
+ 			      LPFC_IO_NVME_LS);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	list_add_tail(&iocbq->list, &phba->lpfc_iocb_list);
  }
  
@@@ -6584,19 -6808,82 +6650,68 @@@ lpfc_sli4_hba_setup(struct lpfc_hba *ph
  				"0582 Error %d during els sgl post "
  				"operation\n", rc);
  		rc = -ENODEV;
++<<<<<<< HEAD
 +		goto out_free_mbox;
++=======
+ 		goto out_destroy_queue;
+ 	}
+ 	phba->sli4_hba.els_xri_cnt = rc;
+ 
+ 	if (phba->nvmet_support) {
+ 		/* update host nvmet xri-sgl sizes and mappings */
+ 		rc = lpfc_sli4_nvmet_sgl_update(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6308 Failed to update nvmet-sgl size "
+ 					"and mapping: %d\n", rc);
+ 			goto out_destroy_queue;
+ 		}
+ 
+ 		/* register the nvmet sgl pool to the port */
+ 		rc = lpfc_sli4_repost_sgl_list(
+ 			phba,
+ 			&phba->sli4_hba.lpfc_nvmet_sgl_list,
+ 			phba->sli4_hba.nvmet_xri_cnt);
+ 		if (unlikely(rc < 0)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"3117 Error %d during nvmet "
+ 					"sgl post\n", rc);
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
+ 		phba->sli4_hba.nvmet_xri_cnt = rc;
+ 		/* todo: tgt: create targetport */
+ 	} else {
+ 		/* update host scsi xri-sgl sizes and mappings */
+ 		rc = lpfc_sli4_scsi_sgl_update(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6309 Failed to update scsi-sgl size "
+ 					"and mapping: %d\n", rc);
+ 			goto out_destroy_queue;
+ 		}
+ 
+ 		/* update host nvme xri-sgl sizes and mappings */
+ 		rc = lpfc_sli4_nvme_sgl_update(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6082 Failed to update nvme-sgl size "
+ 					"and mapping: %d\n", rc);
+ 			goto out_destroy_queue;
+ 		}
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	}
  
 -	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
 -		/* register the allocated scsi sgl pool to the port */
 -		rc = lpfc_sli4_repost_scsi_sgl_list(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"0383 Error %d during scsi sgl post "
 -					"operation\n", rc);
 -			/* Some Scsi buffers were moved to abort scsi list */
 -			/* A pci function reset will repost them */
 -			rc = -ENODEV;
 -			goto out_destroy_queue;
 -		}
 -	}
 -
 -	if ((phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) &&
 -	    (phba->nvmet_support == 0)) {
 -
 -		/* register the allocated nvme sgl pool to the port */
 -		rc = lpfc_repost_nvme_sgl_list(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"6116 Error %d during nvme sgl post "
 -					"operation\n", rc);
 -			/* Some NVME buffers were moved to abort nvme list */
 -			/* A pci function reset will repost them */
 -			rc = -ENODEV;
 -			goto out_destroy_queue;
 -		}
 +	/* register the allocated scsi sgl pool to the port */
 +	rc = lpfc_sli4_repost_scsi_sgl_list(phba);
 +	if (unlikely(rc)) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 +				"0383 Error %d during scsi sgl post "
 +				"operation\n", rc);
 +		/* Some Scsi buffers were moved to the abort scsi list */
 +		/* A pci function reset will repost them */
 +		rc = -ENODEV;
 +		goto out_free_mbox;
  	}
  
  	/* Post the rpi header region to the device. */
@@@ -12400,25 -13079,29 +12515,51 @@@ lpfc_sli4_hba_handle_eqe(struct lpfc_hb
  	/* Get the reference to the corresponding CQ */
  	cqid = bf_get_le32(lpfc_eqe_resource_id, eqe);
  
++<<<<<<< HEAD
 +	/* Check if this is a Slow path event */
 +	if (unlikely(cqid != phba->sli4_hba.fcp_cq_map[qidx])) {
 +		lpfc_sli4_sp_handle_eqe(phba, eqe,
 +			phba->sli4_hba.hba_eq[qidx]);
 +		return;
 +	}
 +
 +	if (unlikely(!phba->sli4_hba.fcp_cq)) {
 +		lpfc_printf_log(phba, KERN_WARNING, LOG_SLI,
 +				"3146 Fast-path completion queues "
 +				"does not exist\n");
 +		return;
 +	}
 +	cq = phba->sli4_hba.fcp_cq[qidx];
 +	if (unlikely(!cq)) {
 +		if (phba->sli.sli_flag & LPFC_SLI_ACTIVE)
 +			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
 +					"0367 Fast-path completion queue "
 +					"(%d) does not exist\n", qidx);
++=======
+ 	if (phba->sli4_hba.nvme_cq_map &&
+ 	    (cqid == phba->sli4_hba.nvme_cq_map[qidx])) {
+ 		/* Process NVME / NVMET command completion */
+ 		cq = phba->sli4_hba.nvme_cq[qidx];
+ 		goto  process_cq;
+ 	}
+ 
+ 	if (phba->sli4_hba.fcp_cq_map &&
+ 	    (cqid == phba->sli4_hba.fcp_cq_map[qidx])) {
+ 		/* Process FCP command completion */
+ 		cq = phba->sli4_hba.fcp_cq[qidx];
+ 		goto  process_cq;
+ 	}
+ 
+ 	if (phba->sli4_hba.nvmels_cq &&
+ 	    (cqid == phba->sli4_hba.nvmels_cq->queue_id)) {
+ 		/* Process NVME unsol rcv */
+ 		cq = phba->sli4_hba.nvmels_cq;
+ 	}
+ 
+ 	/* Otherwise this is a Slow path event */
+ 	if (cq == NULL) {
+ 		lpfc_sli4_sp_handle_eqe(phba, eqe, phba->sli4_hba.hba_eq[qidx]);
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  		return;
  	}
  
@@@ -17134,3 -17850,217 +17275,220 @@@ lpfc_drain_txq(struct lpfc_hba *phba
  
  	return txq_cnt;
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * lpfc_wqe_bpl2sgl - Convert the bpl/bde to a sgl.
+  * @phba: Pointer to HBA context object.
+  * @pwqe: Pointer to command WQE.
+  * @sglq: Pointer to the scatter gather queue object.
+  *
+  * This routine converts the bpl or bde that is in the WQE
+  * to a sgl list for the sli4 hardware. The physical address
+  * of the bpl/bde is converted back to a virtual address.
+  * If the WQE contains a BPL then the list of BDE's is
+  * converted to sli4_sge's. If the WQE contains a single
+  * BDE then it is converted to a single sli_sge.
+  * The WQE is still in cpu endianness so the contents of
+  * the bpl can be used without byte swapping.
+  *
+  * Returns valid XRI = Success, NO_XRI = Failure.
+  */
+ static uint16_t
+ lpfc_wqe_bpl2sgl(struct lpfc_hba *phba, struct lpfc_iocbq *pwqeq,
+ 		 struct lpfc_sglq *sglq)
+ {
+ 	uint16_t xritag = NO_XRI;
+ 	struct ulp_bde64 *bpl = NULL;
+ 	struct ulp_bde64 bde;
+ 	struct sli4_sge *sgl  = NULL;
+ 	struct lpfc_dmabuf *dmabuf;
+ 	union lpfc_wqe *wqe;
+ 	int numBdes = 0;
+ 	int i = 0;
+ 	uint32_t offset = 0; /* accumulated offset in the sg request list */
+ 	int inbound = 0; /* number of sg reply entries inbound from firmware */
+ 	uint32_t cmd;
+ 
+ 	if (!pwqeq || !sglq)
+ 		return xritag;
+ 
+ 	sgl  = (struct sli4_sge *)sglq->sgl;
+ 	wqe = &pwqeq->wqe;
+ 	pwqeq->iocb.ulpIoTag = pwqeq->iotag;
+ 
+ 	cmd = bf_get(wqe_cmnd, &wqe->generic.wqe_com);
+ 	if (cmd == CMD_XMIT_BLS_RSP64_WQE)
+ 		return sglq->sli4_xritag;
+ 	numBdes = pwqeq->rsvd2;
+ 	if (numBdes) {
+ 		/* The addrHigh and addrLow fields within the WQE
+ 		 * have not been byteswapped yet so there is no
+ 		 * need to swap them back.
+ 		 */
+ 		if (pwqeq->context3)
+ 			dmabuf = (struct lpfc_dmabuf *)pwqeq->context3;
+ 		else
+ 			return xritag;
+ 
+ 		bpl  = (struct ulp_bde64 *)dmabuf->virt;
+ 		if (!bpl)
+ 			return xritag;
+ 
+ 		for (i = 0; i < numBdes; i++) {
+ 			/* Should already be byte swapped. */
+ 			sgl->addr_hi = bpl->addrHigh;
+ 			sgl->addr_lo = bpl->addrLow;
+ 
+ 			sgl->word2 = le32_to_cpu(sgl->word2);
+ 			if ((i+1) == numBdes)
+ 				bf_set(lpfc_sli4_sge_last, sgl, 1);
+ 			else
+ 				bf_set(lpfc_sli4_sge_last, sgl, 0);
+ 			/* swap the size field back to the cpu so we
+ 			 * can assign it to the sgl.
+ 			 */
+ 			bde.tus.w = le32_to_cpu(bpl->tus.w);
+ 			sgl->sge_len = cpu_to_le32(bde.tus.f.bdeSize);
+ 			/* The offsets in the sgl need to be accumulated
+ 			 * separately for the request and reply lists.
+ 			 * The request is always first, the reply follows.
+ 			 */
+ 			switch (cmd) {
+ 			case CMD_GEN_REQUEST64_WQE:
+ 				/* add up the reply sg entries */
+ 				if (bpl->tus.f.bdeFlags == BUFF_TYPE_BDE_64I)
+ 					inbound++;
+ 				/* first inbound? reset the offset */
+ 				if (inbound == 1)
+ 					offset = 0;
+ 				bf_set(lpfc_sli4_sge_offset, sgl, offset);
+ 				bf_set(lpfc_sli4_sge_type, sgl,
+ 					LPFC_SGE_TYPE_DATA);
+ 				offset += bde.tus.f.bdeSize;
+ 				break;
+ 			case CMD_FCP_TRSP64_WQE:
+ 				bf_set(lpfc_sli4_sge_offset, sgl, 0);
+ 				bf_set(lpfc_sli4_sge_type, sgl,
+ 					LPFC_SGE_TYPE_DATA);
+ 				break;
+ 			case CMD_FCP_TSEND64_WQE:
+ 			case CMD_FCP_TRECEIVE64_WQE:
+ 				bf_set(lpfc_sli4_sge_type, sgl,
+ 					bpl->tus.f.bdeFlags);
+ 				if (i < 3)
+ 					offset = 0;
+ 				else
+ 					offset += bde.tus.f.bdeSize;
+ 				bf_set(lpfc_sli4_sge_offset, sgl, offset);
+ 				break;
+ 			}
+ 			sgl->word2 = cpu_to_le32(sgl->word2);
+ 			bpl++;
+ 			sgl++;
+ 		}
+ 	} else if (wqe->gen_req.bde.tus.f.bdeFlags == BUFF_TYPE_BDE_64) {
+ 		/* The addrHigh and addrLow fields of the BDE have not
+ 		 * been byteswapped yet so they need to be swapped
+ 		 * before putting them in the sgl.
+ 		 */
+ 		sgl->addr_hi = cpu_to_le32(wqe->gen_req.bde.addrHigh);
+ 		sgl->addr_lo = cpu_to_le32(wqe->gen_req.bde.addrLow);
+ 		sgl->word2 = le32_to_cpu(sgl->word2);
+ 		bf_set(lpfc_sli4_sge_last, sgl, 1);
+ 		sgl->word2 = cpu_to_le32(sgl->word2);
+ 		sgl->sge_len = cpu_to_le32(wqe->gen_req.bde.tus.f.bdeSize);
+ 	}
+ 	return sglq->sli4_xritag;
+ }
+ 
+ /**
+  * lpfc_sli4_issue_wqe - Issue an SLI4 Work Queue Entry (WQE)
+  * @phba: Pointer to HBA context object.
+  * @ring_number: Base sli ring number
+  * @pwqe: Pointer to command WQE.
+  **/
+ int
+ lpfc_sli4_issue_wqe(struct lpfc_hba *phba, uint32_t ring_number,
+ 		    struct lpfc_iocbq *pwqe)
+ {
+ 	union lpfc_wqe *wqe = &pwqe->wqe;
+ 	struct lpfc_nvmet_rcv_ctx *ctxp;
+ 	struct lpfc_queue *wq;
+ 	struct lpfc_sglq *sglq;
+ 	struct lpfc_sli_ring *pring;
+ 	unsigned long iflags;
+ 
+ 	/* NVME_LS and NVME_LS ABTS requests. */
+ 	if (pwqe->iocb_flag & LPFC_IO_NVME_LS) {
+ 		pring =  phba->sli4_hba.nvmels_wq->pring;
+ 		spin_lock_irqsave(&pring->ring_lock, iflags);
+ 		sglq = __lpfc_sli_get_els_sglq(phba, pwqe);
+ 		if (!sglq) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_BUSY;
+ 		}
+ 		pwqe->sli4_lxritag = sglq->sli4_lxritag;
+ 		pwqe->sli4_xritag = sglq->sli4_xritag;
+ 		if (lpfc_wqe_bpl2sgl(phba, pwqe, sglq) == NO_XRI) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		bf_set(wqe_xri_tag, &pwqe->wqe.xmit_bls_rsp.wqe_com,
+ 		       pwqe->sli4_xritag);
+ 		if (lpfc_sli4_wq_put(phba->sli4_hba.nvmels_wq, wqe)) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		lpfc_sli_ringtxcmpl_put(phba, pring, pwqe);
+ 		spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 		return 0;
+ 	}
+ 
+ 	/* NVME_FCREQ and NVME_ABTS requests */
+ 	if (pwqe->iocb_flag & LPFC_IO_NVME) {
+ 		/* Get the IO distribution (hba_wqidx) for WQ assignment. */
+ 		pring = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx]->pring;
+ 
+ 		spin_lock_irqsave(&pring->ring_lock, iflags);
+ 		wq = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx];
+ 		bf_set(wqe_cqid, &wqe->generic.wqe_com,
+ 		      phba->sli4_hba.nvme_cq[pwqe->hba_wqidx]->queue_id);
+ 		if (lpfc_sli4_wq_put(wq, wqe)) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		lpfc_sli_ringtxcmpl_put(phba, pring, pwqe);
+ 		spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 		return 0;
+ 	}
+ 
+ 	/* NVMET requests */
+ 	if (pwqe->iocb_flag & LPFC_IO_NVMET) {
+ 		/* Get the IO distribution (hba_wqidx) for WQ assignment. */
+ 		pring = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx]->pring;
+ 
+ 		spin_lock_irqsave(&pring->ring_lock, iflags);
+ 		ctxp = pwqe->context2;
+ 		sglq = ctxp->rqb_buffer->sglq;
+ 		if (pwqe->sli4_xritag ==  NO_XRI) {
+ 			pwqe->sli4_lxritag = sglq->sli4_lxritag;
+ 			pwqe->sli4_xritag = sglq->sli4_xritag;
+ 		}
+ 		bf_set(wqe_xri_tag, &pwqe->wqe.xmit_bls_rsp.wqe_com,
+ 		       pwqe->sli4_xritag);
+ 		wq = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx];
+ 		bf_set(wqe_cqid, &wqe->generic.wqe_com,
+ 		      phba->sli4_hba.nvme_cq[pwqe->hba_wqidx]->queue_id);
+ 		if (lpfc_sli4_wq_put(wq, wqe)) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		lpfc_sli_ringtxcmpl_put(phba, pring, pwqe);
+ 		spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 		return 0;
+ 	}
+ 	return WQE_ERROR;
+ }
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
diff --cc drivers/scsi/lpfc/lpfc_sli.h
index 74227a28bd56,f6cea02adc7c..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.h
+++ b/drivers/scsi/lpfc/lpfc_sli.h
@@@ -82,9 -89,13 +82,17 @@@ struct lpfc_iocbq 
  #define LPFC_IO_OAS		0x10000 /* OAS FCP IO */
  #define LPFC_IO_FOF		0x20000 /* FOF FCP IO */
  #define LPFC_IO_LOOPBACK	0x40000 /* Loopback IO */
++<<<<<<< HEAD
++=======
+ #define LPFC_PRLI_NVME_REQ	0x80000 /* This is an NVME PRLI. */
+ #define LPFC_PRLI_FCP_REQ	0x100000 /* This is an NVME PRLI. */
+ #define LPFC_IO_NVME	        0x200000 /* NVME FCP command */
+ #define LPFC_IO_NVME_LS		0x400000 /* NVME LS command */
+ #define LPFC_IO_NVMET		0x800000 /* NVMET command */
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  
  	uint32_t drvrTimeout;	/* driver timeout in seconds */
 +	uint32_t fcp_wqidx;	/* index to FCP work queue */
  	struct lpfc_vport *vport;/* virtual port pointer */
  	void *context1;		/* caller context information */
  	void *context2;		/* caller context information */
@@@ -297,12 -318,9 +305,13 @@@ struct lpfc_sli 
  #define LPFC_BLOCK_MGMT_IO        0x800	/* Don't allow mgmt mbx or iocb cmds */
  #define LPFC_MENLO_MAINT          0x1000 /* need for menl fw download */
  #define LPFC_SLI_ASYNC_MBX_BLK    0x2000 /* Async mailbox is blocked */
+ #define LPFC_SLI_SUPPRESS_RSP     0x4000 /* Suppress RSP feature is supported */
  
 -	struct lpfc_sli_ring *sli3_ring;
 +	struct lpfc_sli_ring *ring;
 +	int fcp_ring;		/* ring used for FCP initiator commands */
 +	int next_ring;
 +
 +	int extra_ring;		/* extra ring used for other protocols */
  
  	struct lpfc_sli_stat slistat;	/* SLI statistical info */
  	struct list_head mboxq;
diff --cc drivers/scsi/lpfc/lpfc_sli4.h
index 0b88b5703e0f,99546eaef087..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@@ -107,6 -108,9 +107,12 @@@ enum lpfc_sli4_queue_subtype 
  	LPFC_MBOX,
  	LPFC_FCP,
  	LPFC_ELS,
++<<<<<<< HEAD
++=======
+ 	LPFC_NVME,
+ 	LPFC_NVMET,
+ 	LPFC_NVME_LS,
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	LPFC_USOL
  };
  
@@@ -568,14 -604,20 +574,21 @@@ struct lpfc_sli4_hba 
  	uint16_t rpi_hdrs_in_use; /* must post rpi hdrs if set. */
  	uint16_t next_xri; /* last_xri - max_cfg_param.xri_base = used */
  	uint16_t next_rpi;
 -	uint16_t nvme_xri_max;
 -	uint16_t nvme_xri_cnt;
 -	uint16_t nvme_xri_start;
  	uint16_t scsi_xri_max;
  	uint16_t scsi_xri_cnt;
 -	uint16_t scsi_xri_start;
  	uint16_t els_xri_cnt;
++<<<<<<< HEAD
 +	uint16_t scsi_xri_start;
 +	struct list_head lpfc_free_sgl_list;
 +	struct list_head lpfc_sgl_list;
++=======
+ 	uint16_t nvmet_xri_cnt;
+ 	struct list_head lpfc_els_sgl_list;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	struct list_head lpfc_abts_els_sgl_list;
+ 	struct list_head lpfc_nvmet_sgl_list;
+ 	struct list_head lpfc_abts_nvmet_sgl_list;
  	struct list_head lpfc_abts_scsi_buf_list;
 -	struct list_head lpfc_abts_nvme_buf_list;
  	struct lpfc_sglq **lpfc_sglq_active_list;
  	struct list_head lpfc_rpi_hdr_list;
  	unsigned long *rpi_bmask;
@@@ -602,8 -644,10 +615,13 @@@
  #define LPFC_SLI4_PPNAME_NON	0
  #define LPFC_SLI4_PPNAME_GET	1
  	struct lpfc_iov iov;
 -	spinlock_t abts_nvme_buf_list_lock; /* list of aborted SCSI IOs */
  	spinlock_t abts_scsi_buf_list_lock; /* list of aborted SCSI IOs */
++<<<<<<< HEAD
 +	spinlock_t abts_sgl_list_lock; /* list of aborted els IOs */
++=======
+ 	spinlock_t sgl_list_lock; /* list of aborted els IOs */
+ 	spinlock_t nvmet_io_lock;
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  	uint32_t physical_port;
  
  	/* CPU to vector mapping information */
@@@ -615,7 -659,8 +633,12 @@@
  
  enum lpfc_sge_type {
  	GEN_BUFF_TYPE,
++<<<<<<< HEAD
 +	SCSI_BUFF_TYPE
++=======
+ 	SCSI_BUFF_TYPE,
+ 	NVMET_BUFF_TYPE
++>>>>>>> f358dd0ca26c (scsi: lpfc: NVME Target: Base modifications)
  };
  
  enum lpfc_sgl_state {
* Unmerged path drivers/scsi/lpfc/lpfc.h
* Unmerged path drivers/scsi/lpfc/lpfc_attr.c
* Unmerged path drivers/scsi/lpfc/lpfc_crtn.h
* Unmerged path drivers/scsi/lpfc/lpfc_debugfs.c
diff --git a/drivers/scsi/lpfc/lpfc_disc.h b/drivers/scsi/lpfc/lpfc_disc.h
index 361f5b3d9d93..1d07a5f3907e 100644
--- a/drivers/scsi/lpfc/lpfc_disc.h
+++ b/drivers/scsi/lpfc/lpfc_disc.h
@@ -133,6 +133,7 @@ struct lpfc_node_rrq {
 /* Defines for nlp_flag (uint32) */
 #define NLP_IGNR_REG_CMPL  0x00000001 /* Rcvd rscn before we cmpl reg login */
 #define NLP_REG_LOGIN_SEND 0x00000002   /* sent reglogin to adapter */
+#define NLP_SUPPRESS_RSP   0x00000010	/* Remote NPort supports suppress rsp */
 #define NLP_PLOGI_SND      0x00000020	/* sent PLOGI request for this entry */
 #define NLP_PRLI_SND       0x00000040	/* sent PRLI request for this entry */
 #define NLP_ADISC_SND      0x00000080	/* sent ADISC request for this entry */
* Unmerged path drivers/scsi/lpfc/lpfc_hw4.h
* Unmerged path drivers/scsi/lpfc/lpfc_init.c
* Unmerged path drivers/scsi/lpfc/lpfc_mem.c
diff --git a/drivers/scsi/lpfc/lpfc_nvmet.h b/drivers/scsi/lpfc/lpfc_nvmet.h
new file mode 100644
index 000000000000..f7b6a3f374a4
--- /dev/null
+++ b/drivers/scsi/lpfc/lpfc_nvmet.h
@@ -0,0 +1,101 @@
+/*******************************************************************
+ * This file is part of the Emulex Linux Device Driver for         *
+ * Fibre Channel Host Bus Adapters.                                *
+ * Copyright (C) 2004-2016 Emulex.  All rights reserved.           *
+ * EMULEX and SLI are trademarks of Emulex.                        *
+ * www.emulex.com                                                  *
+ * Portions Copyright (C) 2004-2005 Christoph Hellwig              *
+ *                                                                 *
+ * This program is free software; you can redistribute it and/or   *
+ * modify it under the terms of version 2 of the GNU General       *
+ * Public License as published by the Free Software Foundation.    *
+ * This program is distributed in the hope that it will be useful. *
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND          *
+ * WARRANTIES, INCLUDING ANY IMPLIED WARRANTY OF MERCHANTABILITY,  *
+ * FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT, ARE      *
+ * DISCLAIMED, EXCEPT TO THE EXTENT THAT SUCH DISCLAIMERS ARE HELD *
+ * TO BE LEGALLY INVALID.  See the GNU General Public License for  *
+ * more details, a copy of which can be found in the file COPYING  *
+ * included with this package.                                     *
+ ********************************************************************/
+
+#define LPFC_NVMET_MIN_SEGS		16
+#define LPFC_NVMET_DEFAULT_SEGS		64	/* 256K IOs */
+#define LPFC_NVMET_MAX_SEGS		510
+#define LPFC_NVMET_SUCCESS_LEN	12
+
+/* Used for NVME Target */
+struct lpfc_nvmet_tgtport {
+	struct lpfc_hba *phba;
+	struct completion tport_unreg_done;
+
+	/* Stats counters - lpfc_nvmet_unsol_ls_buffer */
+	atomic_t rcv_ls_req_in;
+	atomic_t rcv_ls_req_out;
+	atomic_t rcv_ls_req_drop;
+	atomic_t xmt_ls_abort;
+
+	/* Stats counters - lpfc_nvmet_xmt_ls_rsp */
+	atomic_t xmt_ls_rsp;
+	atomic_t xmt_ls_drop;
+
+	/* Stats counters - lpfc_nvmet_xmt_ls_rsp_cmp */
+	atomic_t xmt_ls_rsp_error;
+	atomic_t xmt_ls_rsp_cmpl;
+
+	/* Stats counters - lpfc_nvmet_unsol_fcp_buffer */
+	atomic_t rcv_fcp_cmd_in;
+	atomic_t rcv_fcp_cmd_out;
+	atomic_t rcv_fcp_cmd_drop;
+
+	/* Stats counters - lpfc_nvmet_xmt_fcp_op */
+	atomic_t xmt_fcp_abort;
+	atomic_t xmt_fcp_drop;
+	atomic_t xmt_fcp_read_rsp;
+	atomic_t xmt_fcp_read;
+	atomic_t xmt_fcp_write;
+	atomic_t xmt_fcp_rsp;
+
+	/* Stats counters - lpfc_nvmet_xmt_fcp_op_cmp */
+	atomic_t xmt_fcp_rsp_cmpl;
+	atomic_t xmt_fcp_rsp_error;
+	atomic_t xmt_fcp_rsp_drop;
+
+
+	/* Stats counters - lpfc_nvmet_unsol_issue_abort */
+	atomic_t xmt_abort_rsp;
+	atomic_t xmt_abort_rsp_error;
+
+	/* Stats counters - lpfc_nvmet_xmt_abort_cmp */
+	atomic_t xmt_abort_cmpl;
+};
+
+struct lpfc_nvmet_rcv_ctx {
+	union {
+		struct nvmefc_tgt_ls_req ls_req;
+		struct nvmefc_tgt_fcp_req fcp_req;
+	} ctx;
+	struct lpfc_hba *phba;
+	struct lpfc_iocbq *wqeq;
+	struct lpfc_iocbq *abort_wqeq;
+	dma_addr_t txrdy_phys;
+	uint32_t *txrdy;
+	uint32_t sid;
+	uint32_t offset;
+	uint16_t oxid;
+	uint16_t size;
+	uint16_t entry_cnt;
+	uint16_t cpu;
+	uint16_t state;
+	/* States */
+#define LPFC_NVMET_STE_FREE		0
+#define LPFC_NVMET_STE_RCV		1
+#define LPFC_NVMET_STE_DATA		2
+#define LPFC_NVMET_STE_ABORT		3
+#define LPFC_NVMET_STE_RSP		4
+#define LPFC_NVMET_STE_DONE		5
+	uint16_t flag;
+#define LPFC_NVMET_IO_INP		1
+#define LPFC_NVMET_ABORT_OP		2
+	struct rqb_dmabuf *rqb_buffer;
+};
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli.h
* Unmerged path drivers/scsi/lpfc/lpfc_sli4.h
