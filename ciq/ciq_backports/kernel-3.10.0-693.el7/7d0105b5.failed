xen-netfront: request Tx response events more often

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Malcolm Crossley <malcolm.crossley@citrix.com>
commit 7d0105b5334b9722b7d33acad613096dfcf3330e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/7d0105b5.failed

Trying to batch Tx response events results in poor performance because
this delays freeing the transmitted skbs.

Instead use the standard RING_FINAL_CHECK_FOR_RESPONSES() macro to be
notified once the next Tx response is placed on the ring.

	Signed-off-by: Malcolm Crossley <malcolm.crossley@citrix.com>
	Signed-off-by: David Vrabel <david.vrabel@citrix.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 7d0105b5334b9722b7d33acad613096dfcf3330e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/xen-netfront.c
diff --cc drivers/net/xen-netfront.c
index 8896052a2ee0,96ccd4e943db..000000000000
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@@ -368,19 -363,19 +368,20 @@@ static void xennet_tx_buf_gc(struct net
  {
  	RING_IDX cons, prod;
  	unsigned short id;
 +	struct netfront_info *np = netdev_priv(dev);
  	struct sk_buff *skb;
+ 	bool more_to_do;
  
 -	BUG_ON(!netif_carrier_ok(queue->info->netdev));
 +	BUG_ON(!netif_carrier_ok(dev));
  
  	do {
 -		prod = queue->tx.sring->rsp_prod;
 +		prod = np->tx.sring->rsp_prod;
  		rmb(); /* Ensure we see responses up to 'rp'. */
  
 -		for (cons = queue->tx.rsp_cons; cons != prod; cons++) {
 +		for (cons = np->tx.rsp_cons; cons != prod; cons++) {
  			struct xen_netif_tx_response *txrsp;
  
 -			txrsp = RING_GET_RESPONSE(&queue->tx, cons);
 +			txrsp = RING_GET_RESPONSE(&np->tx, cons);
  			if (txrsp->status == XEN_NETIF_RSP_NULL)
  				continue;
  
@@@ -402,118 -397,111 +403,123 @@@
  			dev_kfree_skb_irq(skb);
  		}
  
 -		queue->tx.rsp_cons = prod;
 +		np->tx.rsp_cons = prod;
  
++<<<<<<< HEAD
 +		/*
 +		 * Set a new event, then check for race with update of tx_cons.
 +		 * Note that it is essential to schedule a callback, no matter
 +		 * how few buffers are pending. Even if there is space in the
 +		 * transmit ring, higher layers may be blocked because too much
 +		 * data is outstanding: in such cases notification from Xen is
 +		 * likely to be the only kick that we'll get.
 +		 */
 +		np->tx.sring->rsp_event =
 +			prod + ((np->tx.sring->req_prod - prod) >> 1) + 1;
 +		mb();		/* update shared area */
 +	} while ((cons == prod) && (prod != np->tx.sring->rsp_prod));
++=======
+ 		RING_FINAL_CHECK_FOR_RESPONSES(&queue->tx, more_to_do);
+ 	} while (more_to_do);
++>>>>>>> 7d0105b5334b (xen-netfront: request Tx response events more often)
  
 -	xennet_maybe_wake_tx(queue);
 +	xennet_maybe_wake_tx(dev);
  }
  
 -struct xennet_gnttab_make_txreq {
 -	struct netfront_queue *queue;
 -	struct sk_buff *skb;
 -	struct page *page;
 -	struct xen_netif_tx_request *tx; /* Last request */
 -	unsigned int size;
 -};
 -
 -static void xennet_tx_setup_grant(unsigned long gfn, unsigned int offset,
 -				  unsigned int len, void *data)
 +static void xennet_make_frags(struct sk_buff *skb, struct net_device *dev,
 +			      struct xen_netif_tx_request *tx)
  {
 -	struct xennet_gnttab_make_txreq *info = data;
 +	struct netfront_info *np = netdev_priv(dev);
 +	char *data = skb->data;
 +	unsigned long mfn;
 +	RING_IDX prod = np->tx.req_prod_pvt;
 +	int frags = skb_shinfo(skb)->nr_frags;
 +	unsigned int offset = offset_in_page(data);
 +	unsigned int len = skb_headlen(skb);
  	unsigned int id;
 -	struct xen_netif_tx_request *tx;
  	grant_ref_t ref;
 -	/* convenient aliases */
 -	struct page *page = info->page;
 -	struct netfront_queue *queue = info->queue;
 -	struct sk_buff *skb = info->skb;
 -
 -	id = get_id_from_freelist(&queue->tx_skb_freelist, queue->tx_skbs);
 -	tx = RING_GET_REQUEST(&queue->tx, queue->tx.req_prod_pvt++);
 -	ref = gnttab_claim_grant_reference(&queue->gref_tx_head);
 -	BUG_ON((signed short)ref < 0);
 -
 -	gnttab_grant_foreign_access_ref(ref, queue->info->xbdev->otherend_id,
 -					gfn, GNTMAP_readonly);
 -
 -	queue->tx_skbs[id].skb = skb;
 -	queue->grant_tx_page[id] = page;
 -	queue->grant_tx_ref[id] = ref;
 -
 -	tx->id = id;
 -	tx->gref = ref;
 -	tx->offset = offset;
 -	tx->size = len;
 -	tx->flags = 0;
 -
 -	info->tx = tx;
 -	info->size += tx->size;
 -}
 -
 -static struct xen_netif_tx_request *xennet_make_first_txreq(
 -	struct netfront_queue *queue, struct sk_buff *skb,
 -	struct page *page, unsigned int offset, unsigned int len)
 -{
 -	struct xennet_gnttab_make_txreq info = {
 -		.queue = queue,
 -		.skb = skb,
 -		.page = page,
 -		.size = 0,
 -	};
 -
 -	gnttab_for_one_grant(page, offset, len, xennet_tx_setup_grant, &info);
 +	int i;
  
 -	return info.tx;
 -}
 +	/* While the header overlaps a page boundary (including being
 +	   larger than a page), split it it into page-sized chunks. */
 +	while (len > PAGE_SIZE - offset) {
 +		tx->size = PAGE_SIZE - offset;
 +		tx->flags |= XEN_NETTXF_more_data;
 +		len -= tx->size;
 +		data += tx->size;
 +		offset = 0;
  
 -static void xennet_make_one_txreq(unsigned long gfn, unsigned int offset,
 -				  unsigned int len, void *data)
 -{
 -	struct xennet_gnttab_make_txreq *info = data;
 +		id = get_id_from_freelist(&np->tx_skb_freelist, np->tx_skbs);
 +		np->tx_skbs[id].skb = skb_get(skb);
 +		tx = RING_GET_REQUEST(&np->tx, prod++);
 +		tx->id = id;
 +		ref = gnttab_claim_grant_reference(&np->gref_tx_head);
 +		BUG_ON((signed short)ref < 0);
  
 -	info->tx->flags |= XEN_NETTXF_more_data;
 -	skb_get(info->skb);
 -	xennet_tx_setup_grant(gfn, offset, len, data);
 -}
 +		mfn = virt_to_mfn(data);
 +		gnttab_grant_foreign_access_ref(ref, np->xbdev->otherend_id,
 +						mfn, GNTMAP_readonly);
  
 -static struct xen_netif_tx_request *xennet_make_txreqs(
 -	struct netfront_queue *queue, struct xen_netif_tx_request *tx,
 -	struct sk_buff *skb, struct page *page,
 -	unsigned int offset, unsigned int len)
 -{
 -	struct xennet_gnttab_make_txreq info = {
 -		.queue = queue,
 -		.skb = skb,
 -		.tx = tx,
 -	};
 +		np->grant_tx_page[id] = virt_to_page(data);
 +		tx->gref = np->grant_tx_ref[id] = ref;
 +		tx->offset = offset;
 +		tx->size = len;
 +		tx->flags = 0;
 +	}
  
 -	/* Skip unused frames from start of page */
 -	page += offset >> PAGE_SHIFT;
 -	offset &= ~PAGE_MASK;
 +	/* Grant backend access to each skb fragment page. */
 +	for (i = 0; i < frags; i++) {
 +		skb_frag_t *frag = skb_shinfo(skb)->frags + i;
 +		struct page *page = skb_frag_page(frag);
  
 -	while (len) {
 -		info.page = page;
 -		info.size = 0;
 +		len = skb_frag_size(frag);
 +		offset = frag->page_offset;
  
 -		gnttab_foreach_grant_in_range(page, offset, len,
 -					      xennet_make_one_txreq,
 -					      &info);
 +		/* Skip unused frames from start of page */
 +		page += offset >> PAGE_SHIFT;
 +		offset &= ~PAGE_MASK;
  
 -		page++;
 -		offset = 0;
 -		len -= info.size;
 +		while (len > 0) {
 +			unsigned long bytes;
 +
 +			bytes = PAGE_SIZE - offset;
 +			if (bytes > len)
 +				bytes = len;
 +
 +			tx->flags |= XEN_NETTXF_more_data;
 +
 +			id = get_id_from_freelist(&np->tx_skb_freelist,
 +						  np->tx_skbs);
 +			np->tx_skbs[id].skb = skb_get(skb);
 +			tx = RING_GET_REQUEST(&np->tx, prod++);
 +			tx->id = id;
 +			ref = gnttab_claim_grant_reference(&np->gref_tx_head);
 +			BUG_ON((signed short)ref < 0);
 +
 +			mfn = pfn_to_mfn(page_to_pfn(page));
 +			gnttab_grant_foreign_access_ref(ref,
 +							np->xbdev->otherend_id,
 +							mfn, GNTMAP_readonly);
 +
 +			np->grant_tx_page[id] = page;
 +			tx->gref = np->grant_tx_ref[id] = ref;
 +			tx->offset = offset;
 +			tx->size = bytes;
 +			tx->flags = 0;
 +
 +			offset += bytes;
 +			len -= bytes;
 +
 +			/* Next frame */
 +			if (offset == PAGE_SIZE && len) {
 +				BUG_ON(!PageCompound(page));
 +				page++;
 +				offset = 0;
 +			}
 +		}
  	}
  
 -	return info.tx;
 +	np->tx.req_prod_pvt = prod;
  }
  
  /*
* Unmerged path drivers/net/xen-netfront.c
