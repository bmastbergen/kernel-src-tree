Btrfs: use root when checking need_async_flush

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Josef Bacik <jbacik@fb.com>
commit 87241c2e68451c4d50b60af84f9a3ab119001b4c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/87241c2e.failed

Instead of doing fs_info->fs_root in need_async_flush, which may not be set
during recovery when mounting, just pass the root itself in, which makes more
sense as thats what btrfs_calc_reclaim_metadata_size takes.

	Signed-off-by: Josef Bacik <jbacik@fb.com>
	Reported-by: David Sterba <dsterba@suse.com>
	Signed-off-by: David Sterba <dsterba@suse.com>
(cherry picked from commit 87241c2e68451c4d50b60af84f9a3ab119001b4c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/btrfs/extent-tree.c
diff --cc fs/btrfs/extent-tree.c
index b28ead068286,9ef8f99d5a2a..000000000000
--- a/fs/btrfs/extent-tree.c
+++ b/fs/btrfs/extent-tree.c
@@@ -4844,43 -4944,34 +4844,44 @@@ static inline int need_do_async_reclaim
  	if ((space_info->bytes_used + space_info->bytes_reserved) >= thresh)
  		return 0;
  
- 	if (!btrfs_calc_reclaim_metadata_size(fs_info->fs_root, space_info))
+ 	if (!btrfs_calc_reclaim_metadata_size(root, space_info))
  		return 0;
  
- 	return (used >= thresh && !btrfs_fs_closing(fs_info) &&
- 		!test_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state));
+ 	return (used >= thresh && !btrfs_fs_closing(root->fs_info) &&
+ 		!test_bit(BTRFS_FS_STATE_REMOUNTING,
+ 			  &root->fs_info->fs_state));
  }
  
 -static void wake_all_tickets(struct list_head *head)
 +static int btrfs_need_do_async_reclaim(struct btrfs_space_info *space_info,
 +				       struct btrfs_fs_info *fs_info,
 +				       int flush_state)
  {
 -	struct reserve_ticket *ticket;
 +	u64 used;
  
 -	while (!list_empty(head)) {
 -		ticket = list_first_entry(head, struct reserve_ticket, list);
 -		list_del_init(&ticket->list);
 -		ticket->error = -ENOSPC;
 -		wake_up(&ticket->wait);
 +	spin_lock(&space_info->lock);
 +	/*
 +	 * We run out of space and have not got any free space via flush_space,
 +	 * so don't bother doing async reclaim.
 +	 */
 +	if (flush_state > COMMIT_TRANS && space_info->full) {
 +		spin_unlock(&space_info->lock);
 +		return 0;
  	}
 +
 +	used = space_info->bytes_used + space_info->bytes_reserved +
 +	       space_info->bytes_pinned + space_info->bytes_readonly +
 +	       space_info->bytes_may_use;
 +	if (need_do_async_reclaim(space_info, fs_info, used)) {
 +		spin_unlock(&space_info->lock);
 +		return 1;
 +	}
 +	spin_unlock(&space_info->lock);
 +
 +	return 0;
  }
  
 -/*
 - * This is for normal flushers, we can wait all goddamned day if we want to.  We
 - * will loop and continuously try to flush as long as we are making progress.
 - * We count progress as clearing off tickets each time we have to loop.
 - */
  static void btrfs_async_reclaim_metadata_space(struct work_struct *work)
  {
 -	struct reserve_ticket *last_ticket = NULL;
  	struct btrfs_fs_info *fs_info;
  	struct btrfs_space_info *space_info;
  	u64 to_reclaim;
@@@ -4910,6 -5037,201 +4911,204 @@@ void btrfs_init_async_reclaim_work(stru
  	INIT_WORK(work, btrfs_async_reclaim_metadata_space);
  }
  
++<<<<<<< HEAD
++=======
+ static void priority_reclaim_metadata_space(struct btrfs_fs_info *fs_info,
+ 					    struct btrfs_space_info *space_info,
+ 					    struct reserve_ticket *ticket)
+ {
+ 	u64 to_reclaim;
+ 	int flush_state = FLUSH_DELAYED_ITEMS_NR;
+ 
+ 	spin_lock(&space_info->lock);
+ 	to_reclaim = btrfs_calc_reclaim_metadata_size(fs_info->fs_root,
+ 						      space_info);
+ 	if (!to_reclaim) {
+ 		spin_unlock(&space_info->lock);
+ 		return;
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 
+ 	do {
+ 		flush_space(fs_info->fs_root, space_info, to_reclaim,
+ 			    to_reclaim, flush_state);
+ 		flush_state++;
+ 		spin_lock(&space_info->lock);
+ 		if (ticket->bytes == 0) {
+ 			spin_unlock(&space_info->lock);
+ 			return;
+ 		}
+ 		spin_unlock(&space_info->lock);
+ 
+ 		/*
+ 		 * Priority flushers can't wait on delalloc without
+ 		 * deadlocking.
+ 		 */
+ 		if (flush_state == FLUSH_DELALLOC ||
+ 		    flush_state == FLUSH_DELALLOC_WAIT)
+ 			flush_state = ALLOC_CHUNK;
+ 	} while (flush_state < COMMIT_TRANS);
+ }
+ 
+ static int wait_reserve_ticket(struct btrfs_fs_info *fs_info,
+ 			       struct btrfs_space_info *space_info,
+ 			       struct reserve_ticket *ticket, u64 orig_bytes)
+ 
+ {
+ 	DEFINE_WAIT(wait);
+ 	int ret = 0;
+ 
+ 	spin_lock(&space_info->lock);
+ 	while (ticket->bytes > 0 && ticket->error == 0) {
+ 		ret = prepare_to_wait_event(&ticket->wait, &wait, TASK_KILLABLE);
+ 		if (ret) {
+ 			ret = -EINTR;
+ 			break;
+ 		}
+ 		spin_unlock(&space_info->lock);
+ 
+ 		schedule();
+ 
+ 		finish_wait(&ticket->wait, &wait);
+ 		spin_lock(&space_info->lock);
+ 	}
+ 	if (!ret)
+ 		ret = ticket->error;
+ 	if (!list_empty(&ticket->list))
+ 		list_del_init(&ticket->list);
+ 	if (ticket->bytes && ticket->bytes < orig_bytes) {
+ 		u64 num_bytes = orig_bytes - ticket->bytes;
+ 		space_info->bytes_may_use -= num_bytes;
+ 		trace_btrfs_space_reservation(fs_info, "space_info",
+ 					      space_info->flags, num_bytes, 0);
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * reserve_metadata_bytes - try to reserve bytes from the block_rsv's space
+  * @root - the root we're allocating for
+  * @space_info - the space info we want to allocate from
+  * @orig_bytes - the number of bytes we want
+  * @flush - whether or not we can flush to make our reservation
+  *
+  * This will reserve orig_bytes number of bytes from the space info associated
+  * with the block_rsv.  If there is not enough space it will make an attempt to
+  * flush out space to make room.  It will do this by flushing delalloc if
+  * possible or committing the transaction.  If flush is 0 then no attempts to
+  * regain reservations will be made and this will fail if there is not enough
+  * space already.
+  */
+ static int __reserve_metadata_bytes(struct btrfs_root *root,
+ 				    struct btrfs_space_info *space_info,
+ 				    u64 orig_bytes,
+ 				    enum btrfs_reserve_flush_enum flush)
+ {
+ 	struct reserve_ticket ticket;
+ 	u64 used;
+ 	int ret = 0;
+ 
+ 	ASSERT(orig_bytes);
+ 	spin_lock(&space_info->lock);
+ 	ret = -ENOSPC;
+ 	used = space_info->bytes_used + space_info->bytes_reserved +
+ 		space_info->bytes_pinned + space_info->bytes_readonly +
+ 		space_info->bytes_may_use;
+ 
+ 	/*
+ 	 * If we have enough space then hooray, make our reservation and carry
+ 	 * on.  If not see if we can overcommit, and if we can, hooray carry on.
+ 	 * If not things get more complicated.
+ 	 */
+ 	if (used + orig_bytes <= space_info->total_bytes) {
+ 		space_info->bytes_may_use += orig_bytes;
+ 		trace_btrfs_space_reservation(root->fs_info, "space_info",
+ 					      space_info->flags, orig_bytes,
+ 					      1);
+ 		ret = 0;
+ 	} else if (can_overcommit(root, space_info, orig_bytes, flush)) {
+ 		space_info->bytes_may_use += orig_bytes;
+ 		trace_btrfs_space_reservation(root->fs_info, "space_info",
+ 					      space_info->flags, orig_bytes,
+ 					      1);
+ 		ret = 0;
+ 	}
+ 
+ 	/*
+ 	 * If we couldn't make a reservation then setup our reservation ticket
+ 	 * and kick the async worker if it's not already running.
+ 	 *
+ 	 * If we are a priority flusher then we just need to add our ticket to
+ 	 * the list and we will do our own flushing further down.
+ 	 */
+ 	if (ret && flush != BTRFS_RESERVE_NO_FLUSH) {
+ 		ticket.bytes = orig_bytes;
+ 		ticket.error = 0;
+ 		init_waitqueue_head(&ticket.wait);
+ 		if (flush == BTRFS_RESERVE_FLUSH_ALL) {
+ 			list_add_tail(&ticket.list, &space_info->tickets);
+ 			if (!space_info->flush) {
+ 				space_info->flush = 1;
+ 				trace_btrfs_trigger_flush(root->fs_info,
+ 							  space_info->flags,
+ 							  orig_bytes, flush,
+ 							  "enospc");
+ 				queue_work(system_unbound_wq,
+ 					   &root->fs_info->async_reclaim_work);
+ 			}
+ 		} else {
+ 			list_add_tail(&ticket.list,
+ 				      &space_info->priority_tickets);
+ 		}
+ 	} else if (!ret && space_info->flags & BTRFS_BLOCK_GROUP_METADATA) {
+ 		used += orig_bytes;
+ 		/*
+ 		 * We will do the space reservation dance during log replay,
+ 		 * which means we won't have fs_info->fs_root set, so don't do
+ 		 * the async reclaim as we will panic.
+ 		 */
+ 		if (!root->fs_info->log_root_recovering &&
+ 		    need_do_async_reclaim(space_info, root, used) &&
+ 		    !work_busy(&root->fs_info->async_reclaim_work)) {
+ 			trace_btrfs_trigger_flush(root->fs_info,
+ 						  space_info->flags,
+ 						  orig_bytes, flush,
+ 						  "preempt");
+ 			queue_work(system_unbound_wq,
+ 				   &root->fs_info->async_reclaim_work);
+ 		}
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 	if (!ret || flush == BTRFS_RESERVE_NO_FLUSH)
+ 		return ret;
+ 
+ 	if (flush == BTRFS_RESERVE_FLUSH_ALL)
+ 		return wait_reserve_ticket(root->fs_info, space_info, &ticket,
+ 					   orig_bytes);
+ 
+ 	ret = 0;
+ 	priority_reclaim_metadata_space(root->fs_info, space_info, &ticket);
+ 	spin_lock(&space_info->lock);
+ 	if (ticket.bytes) {
+ 		if (ticket.bytes < orig_bytes) {
+ 			u64 num_bytes = orig_bytes - ticket.bytes;
+ 			space_info->bytes_may_use -= num_bytes;
+ 			trace_btrfs_space_reservation(root->fs_info,
+ 					"space_info", space_info->flags,
+ 					num_bytes, 0);
+ 
+ 		}
+ 		list_del_init(&ticket.list);
+ 		ret = -ENOSPC;
+ 	}
+ 	spin_unlock(&space_info->lock);
+ 	ASSERT(list_empty(&ticket.list));
+ 	return ret;
+ }
+ 
++>>>>>>> 87241c2e6845 (Btrfs: use root when checking need_async_flush)
  /**
   * reserve_metadata_bytes - try to reserve bytes from the block_rsv's space
   * @root - the root we're allocating for
* Unmerged path fs/btrfs/extent-tree.c
