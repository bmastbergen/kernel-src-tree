rtnl: use the new API to align IFLA_STATS*

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Nicolas Dichtel <nicolas.dichtel@6wind.com>
commit 58414d32a37e4c2f79da91aebc2d2365918a1562
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/58414d32.failed

	Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 58414d32a37e4c2f79da91aebc2d2365918a1562)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/rtnetlink.c
diff --cc net/core/rtnetlink.c
index 8209decfe5a9,5ec059d52823..000000000000
--- a/net/core/rtnetlink.c
+++ b/net/core/rtnetlink.c
@@@ -1024,6 -1005,188 +1024,191 @@@ static int rtnl_phys_port_id_fill(struc
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int rtnl_phys_port_name_fill(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	char name[IFNAMSIZ];
+ 	int err;
+ 
+ 	err = dev_get_phys_port_name(dev, name, sizeof(name));
+ 	if (err) {
+ 		if (err == -EOPNOTSUPP)
+ 			return 0;
+ 		return err;
+ 	}
+ 
+ 	if (nla_put(skb, IFLA_PHYS_PORT_NAME, strlen(name), name))
+ 		return -EMSGSIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int rtnl_phys_switch_id_fill(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	int err;
+ 	struct switchdev_attr attr = {
+ 		.orig_dev = dev,
+ 		.id = SWITCHDEV_ATTR_ID_PORT_PARENT_ID,
+ 		.flags = SWITCHDEV_F_NO_RECURSE,
+ 	};
+ 
+ 	err = switchdev_port_attr_get(dev, &attr);
+ 	if (err) {
+ 		if (err == -EOPNOTSUPP)
+ 			return 0;
+ 		return err;
+ 	}
+ 
+ 	if (nla_put(skb, IFLA_PHYS_SWITCH_ID, attr.u.ppid.id_len,
+ 		    attr.u.ppid.id))
+ 		return -EMSGSIZE;
+ 
+ 	return 0;
+ }
+ 
+ static noinline_for_stack int rtnl_fill_stats(struct sk_buff *skb,
+ 					      struct net_device *dev)
+ {
+ 	struct rtnl_link_stats64 *sp;
+ 	struct nlattr *attr;
+ 
+ 	attr = nla_reserve_64bit(skb, IFLA_STATS64,
+ 				 sizeof(struct rtnl_link_stats64), IFLA_PAD);
+ 	if (!attr)
+ 		return -EMSGSIZE;
+ 
+ 	sp = nla_data(attr);
+ 	dev_get_stats(dev, sp);
+ 
+ 	attr = nla_reserve(skb, IFLA_STATS,
+ 			   sizeof(struct rtnl_link_stats));
+ 	if (!attr)
+ 		return -EMSGSIZE;
+ 
+ 	copy_rtnl_link_stats(nla_data(attr), sp);
+ 
+ 	return 0;
+ }
+ 
+ static noinline_for_stack int rtnl_fill_vfinfo(struct sk_buff *skb,
+ 					       struct net_device *dev,
+ 					       int vfs_num,
+ 					       struct nlattr *vfinfo)
+ {
+ 	struct ifla_vf_rss_query_en vf_rss_query_en;
+ 	struct ifla_vf_link_state vf_linkstate;
+ 	struct ifla_vf_spoofchk vf_spoofchk;
+ 	struct ifla_vf_tx_rate vf_tx_rate;
+ 	struct ifla_vf_stats vf_stats;
+ 	struct ifla_vf_trust vf_trust;
+ 	struct ifla_vf_vlan vf_vlan;
+ 	struct ifla_vf_rate vf_rate;
+ 	struct nlattr *vf, *vfstats;
+ 	struct ifla_vf_mac vf_mac;
+ 	struct ifla_vf_info ivi;
+ 
+ 	/* Not all SR-IOV capable drivers support the
+ 	 * spoofcheck and "RSS query enable" query.  Preset to
+ 	 * -1 so the user space tool can detect that the driver
+ 	 * didn't report anything.
+ 	 */
+ 	ivi.spoofchk = -1;
+ 	ivi.rss_query_en = -1;
+ 	ivi.trusted = -1;
+ 	memset(ivi.mac, 0, sizeof(ivi.mac));
+ 	/* The default value for VF link state is "auto"
+ 	 * IFLA_VF_LINK_STATE_AUTO which equals zero
+ 	 */
+ 	ivi.linkstate = 0;
+ 	if (dev->netdev_ops->ndo_get_vf_config(dev, vfs_num, &ivi))
+ 		return 0;
+ 
+ 	vf_mac.vf =
+ 		vf_vlan.vf =
+ 		vf_rate.vf =
+ 		vf_tx_rate.vf =
+ 		vf_spoofchk.vf =
+ 		vf_linkstate.vf =
+ 		vf_rss_query_en.vf =
+ 		vf_trust.vf = ivi.vf;
+ 
+ 	memcpy(vf_mac.mac, ivi.mac, sizeof(ivi.mac));
+ 	vf_vlan.vlan = ivi.vlan;
+ 	vf_vlan.qos = ivi.qos;
+ 	vf_tx_rate.rate = ivi.max_tx_rate;
+ 	vf_rate.min_tx_rate = ivi.min_tx_rate;
+ 	vf_rate.max_tx_rate = ivi.max_tx_rate;
+ 	vf_spoofchk.setting = ivi.spoofchk;
+ 	vf_linkstate.link_state = ivi.linkstate;
+ 	vf_rss_query_en.setting = ivi.rss_query_en;
+ 	vf_trust.setting = ivi.trusted;
+ 	vf = nla_nest_start(skb, IFLA_VF_INFO);
+ 	if (!vf) {
+ 		nla_nest_cancel(skb, vfinfo);
+ 		return -EMSGSIZE;
+ 	}
+ 	if (nla_put(skb, IFLA_VF_MAC, sizeof(vf_mac), &vf_mac) ||
+ 	    nla_put(skb, IFLA_VF_VLAN, sizeof(vf_vlan), &vf_vlan) ||
+ 	    nla_put(skb, IFLA_VF_RATE, sizeof(vf_rate),
+ 		    &vf_rate) ||
+ 	    nla_put(skb, IFLA_VF_TX_RATE, sizeof(vf_tx_rate),
+ 		    &vf_tx_rate) ||
+ 	    nla_put(skb, IFLA_VF_SPOOFCHK, sizeof(vf_spoofchk),
+ 		    &vf_spoofchk) ||
+ 	    nla_put(skb, IFLA_VF_LINK_STATE, sizeof(vf_linkstate),
+ 		    &vf_linkstate) ||
+ 	    nla_put(skb, IFLA_VF_RSS_QUERY_EN,
+ 		    sizeof(vf_rss_query_en),
+ 		    &vf_rss_query_en) ||
+ 	    nla_put(skb, IFLA_VF_TRUST,
+ 		    sizeof(vf_trust), &vf_trust))
+ 		return -EMSGSIZE;
+ 	memset(&vf_stats, 0, sizeof(vf_stats));
+ 	if (dev->netdev_ops->ndo_get_vf_stats)
+ 		dev->netdev_ops->ndo_get_vf_stats(dev, vfs_num,
+ 						&vf_stats);
+ 	vfstats = nla_nest_start(skb, IFLA_VF_STATS);
+ 	if (!vfstats) {
+ 		nla_nest_cancel(skb, vf);
+ 		nla_nest_cancel(skb, vfinfo);
+ 		return -EMSGSIZE;
+ 	}
+ 	if (nla_put_u64(skb, IFLA_VF_STATS_RX_PACKETS,
+ 			vf_stats.rx_packets) ||
+ 	    nla_put_u64(skb, IFLA_VF_STATS_TX_PACKETS,
+ 			vf_stats.tx_packets) ||
+ 	    nla_put_u64(skb, IFLA_VF_STATS_RX_BYTES,
+ 			vf_stats.rx_bytes) ||
+ 	    nla_put_u64(skb, IFLA_VF_STATS_TX_BYTES,
+ 			vf_stats.tx_bytes) ||
+ 	    nla_put_u64(skb, IFLA_VF_STATS_BROADCAST,
+ 			vf_stats.broadcast) ||
+ 	    nla_put_u64(skb, IFLA_VF_STATS_MULTICAST,
+ 			vf_stats.multicast))
+ 		return -EMSGSIZE;
+ 	nla_nest_end(skb, vfstats);
+ 	nla_nest_end(skb, vf);
+ 	return 0;
+ }
+ 
+ static int rtnl_fill_link_ifmap(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	struct rtnl_link_ifmap map = {
+ 		.mem_start   = dev->mem_start,
+ 		.mem_end     = dev->mem_end,
+ 		.base_addr   = dev->base_addr,
+ 		.irq         = dev->irq,
+ 		.dma         = dev->dma,
+ 		.port        = dev->if_port,
+ 	};
+ 	if (nla_put(skb, IFLA_MAP, sizeof(map), &map))
+ 		return -EMSGSIZE;
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 58414d32a37e (rtnl: use the new API to align IFLA_STATS*)
  static int rtnl_fill_ifinfo(struct sk_buff *skb, struct net_device *dev,
  			    int type, u32 pid, u32 seq, u32 change,
  			    unsigned int flags, u32 ext_filter_mask)
@@@ -3213,6 -3444,154 +3398,157 @@@ out
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static int rtnl_fill_statsinfo(struct sk_buff *skb, struct net_device *dev,
+ 			       int type, u32 pid, u32 seq, u32 change,
+ 			       unsigned int flags, unsigned int filter_mask)
+ {
+ 	struct if_stats_msg *ifsm;
+ 	struct nlmsghdr *nlh;
+ 	struct nlattr *attr;
+ 
+ 	ASSERT_RTNL();
+ 
+ 	nlh = nlmsg_put(skb, pid, seq, type, sizeof(*ifsm), flags);
+ 	if (!nlh)
+ 		return -EMSGSIZE;
+ 
+ 	ifsm = nlmsg_data(nlh);
+ 	ifsm->ifindex = dev->ifindex;
+ 	ifsm->filter_mask = filter_mask;
+ 
+ 	if (filter_mask & IFLA_STATS_FILTER_BIT(IFLA_STATS_LINK_64)) {
+ 		struct rtnl_link_stats64 *sp;
+ 
+ 		attr = nla_reserve_64bit(skb, IFLA_STATS_LINK_64,
+ 					 sizeof(struct rtnl_link_stats64),
+ 					 IFLA_STATS_UNSPEC);
+ 		if (!attr)
+ 			goto nla_put_failure;
+ 
+ 		sp = nla_data(attr);
+ 		dev_get_stats(dev, sp);
+ 	}
+ 
+ 	nlmsg_end(skb, nlh);
+ 
+ 	return 0;
+ 
+ nla_put_failure:
+ 	nlmsg_cancel(skb, nlh);
+ 
+ 	return -EMSGSIZE;
+ }
+ 
+ static const struct nla_policy ifla_stats_policy[IFLA_STATS_MAX + 1] = {
+ 	[IFLA_STATS_LINK_64]	= { .len = sizeof(struct rtnl_link_stats64) },
+ };
+ 
+ static size_t if_nlmsg_stats_size(const struct net_device *dev,
+ 				  u32 filter_mask)
+ {
+ 	size_t size = 0;
+ 
+ 	if (filter_mask & IFLA_STATS_FILTER_BIT(IFLA_STATS_LINK_64))
+ 		size += nla_total_size_64bit(sizeof(struct rtnl_link_stats64));
+ 
+ 	return size;
+ }
+ 
+ static int rtnl_stats_get(struct sk_buff *skb, struct nlmsghdr *nlh)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct if_stats_msg *ifsm;
+ 	struct net_device *dev = NULL;
+ 	struct sk_buff *nskb;
+ 	u32 filter_mask;
+ 	int err;
+ 
+ 	ifsm = nlmsg_data(nlh);
+ 	if (ifsm->ifindex > 0)
+ 		dev = __dev_get_by_index(net, ifsm->ifindex);
+ 	else
+ 		return -EINVAL;
+ 
+ 	if (!dev)
+ 		return -ENODEV;
+ 
+ 	filter_mask = ifsm->filter_mask;
+ 	if (!filter_mask)
+ 		return -EINVAL;
+ 
+ 	nskb = nlmsg_new(if_nlmsg_stats_size(dev, filter_mask), GFP_KERNEL);
+ 	if (!nskb)
+ 		return -ENOBUFS;
+ 
+ 	err = rtnl_fill_statsinfo(nskb, dev, RTM_NEWSTATS,
+ 				  NETLINK_CB(skb).portid, nlh->nlmsg_seq, 0,
+ 				  0, filter_mask);
+ 	if (err < 0) {
+ 		/* -EMSGSIZE implies BUG in if_nlmsg_stats_size */
+ 		WARN_ON(err == -EMSGSIZE);
+ 		kfree_skb(nskb);
+ 	} else {
+ 		err = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int rtnl_stats_dump(struct sk_buff *skb, struct netlink_callback *cb)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct if_stats_msg *ifsm;
+ 	int h, s_h;
+ 	int idx = 0, s_idx;
+ 	struct net_device *dev;
+ 	struct hlist_head *head;
+ 	unsigned int flags = NLM_F_MULTI;
+ 	u32 filter_mask = 0;
+ 	int err;
+ 
+ 	s_h = cb->args[0];
+ 	s_idx = cb->args[1];
+ 
+ 	cb->seq = net->dev_base_seq;
+ 
+ 	ifsm = nlmsg_data(cb->nlh);
+ 	filter_mask = ifsm->filter_mask;
+ 	if (!filter_mask)
+ 		return -EINVAL;
+ 
+ 	for (h = s_h; h < NETDEV_HASHENTRIES; h++, s_idx = 0) {
+ 		idx = 0;
+ 		head = &net->dev_index_head[h];
+ 		hlist_for_each_entry(dev, head, index_hlist) {
+ 			if (idx < s_idx)
+ 				goto cont;
+ 			err = rtnl_fill_statsinfo(skb, dev, RTM_NEWSTATS,
+ 						  NETLINK_CB(cb->skb).portid,
+ 						  cb->nlh->nlmsg_seq, 0,
+ 						  flags, filter_mask);
+ 			/* If we ran out of room on the first message,
+ 			 * we're in trouble
+ 			 */
+ 			WARN_ON((err == -EMSGSIZE) && (skb->len == 0));
+ 
+ 			if (err < 0)
+ 				goto out;
+ 
+ 			nl_dump_check_consistent(cb, nlmsg_hdr(skb));
+ cont:
+ 			idx++;
+ 		}
+ 	}
+ out:
+ 	cb->args[1] = idx;
+ 	cb->args[0] = h;
+ 
+ 	return skb->len;
+ }
+ 
++>>>>>>> 58414d32a37e (rtnl: use the new API to align IFLA_STATS*)
  /* Process one rtnetlink message. */
  
  static int rtnetlink_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)
* Unmerged path net/core/rtnetlink.c
