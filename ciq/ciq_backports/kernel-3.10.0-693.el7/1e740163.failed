x86/platform/UV: Clean up the NMI code to match current coding style

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] platform/uv: Clean up the NMI code to match current coding style (Frank Ramsay) [1416460]
Rebuild_FUZZ: 96.97%
commit-author travis@sgi.com <travis@sgi.com>
commit 1e74016370ec3d552a7f5df18bb2b0f1c80b5a9f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/1e740163.failed

Update UV NMI to current coding style.

	Signed-off-by: Mike Travis <travis@sgi.com>
	Acked-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Russ Anderson <rja@hpe.com>
Link: http://lkml.kernel.org/r/20170125163518.419094259@asylum.americas.sgi.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 1e74016370ec3d552a7f5df18bb2b0f1c80b5a9f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/platform/uv/uv_nmi.c
diff --cc arch/x86/platform/uv/uv_nmi.c
index c6fd36338c09,0ecd7bf7d2d3..000000000000
--- a/arch/x86/platform/uv/uv_nmi.c
+++ b/arch/x86/platform/uv/uv_nmi.c
@@@ -191,8 -269,200 +191,205 @@@ static inline void uv_local_mmr_clear_n
  }
  
  /*
++<<<<<<< HEAD
 + * If first cpu in on this hub, set hub_nmi "in_nmi" and "owner" values and
 + * return true.  If first cpu in on the system, set global "in_nmi" flag.
++=======
+  * UV hubless NMI handler functions
+  */
+ static inline void uv_reassert_nmi(void)
+ {
+ 	/* (from arch/x86/include/asm/mach_traps.h) */
+ 	outb(0x8f, NMI_CONTROL_PORT);
+ 	inb(NMI_DUMMY_PORT);		/* dummy read */
+ 	outb(0x0f, NMI_CONTROL_PORT);
+ 	inb(NMI_DUMMY_PORT);		/* dummy read */
+ }
+ 
+ static void uv_init_hubless_pch_io(int offset, int mask, int data)
+ {
+ 	int *addr = PCH_PCR_GPIO_ADDRESS(offset);
+ 	int readd = readl(addr);
+ 
+ 	if (mask) {			/* OR in new data */
+ 		int writed = (readd & ~mask) | data;
+ 
+ 		nmi_debug("UV:PCH: %p = %x & %x | %x (%x)\n",
+ 			addr, readd, ~mask, data, writed);
+ 		writel(writed, addr);
+ 	} else if (readd & data) {	/* clear status bit */
+ 		nmi_debug("UV:PCH: %p = %x\n", addr, data);
+ 		writel(data, addr);
+ 	}
+ 
+ 	(void)readl(addr);		/* flush write data */
+ }
+ 
+ static void uv_nmi_setup_hubless_intr(void)
+ {
+ 	uv_pch_intr_now_enabled = uv_pch_intr_enable;
+ 
+ 	uv_init_hubless_pch_io(
+ 		PAD_CFG_DW0_GPP_D_0, GPIROUTNMI,
+ 		uv_pch_intr_now_enabled ? GPIROUTNMI : 0);
+ 
+ 	nmi_debug("UV:NMI: GPP_D_0 interrupt %s\n",
+ 		uv_pch_intr_now_enabled ? "enabled" : "disabled");
+ }
+ 
+ static struct init_nmi {
+ 	unsigned int	offset;
+ 	unsigned int	mask;
+ 	unsigned int	data;
+ } init_nmi[] = {
+ 	{	/* HOSTSW_OWN_GPP_D_0 */
+ 	.offset = 0x84,
+ 	.mask = 0x1,
+ 	.data = 0x0,	/* ACPI Mode */
+ 	},
+ 
+ /* Clear status: */
+ 	{	/* GPI_INT_STS_GPP_D_0 */
+ 	.offset = 0x104,
+ 	.mask = 0x0,
+ 	.data = 0x1,	/* Clear Status */
+ 	},
+ 	{	/* GPI_GPE_STS_GPP_D_0 */
+ 	.offset = 0x124,
+ 	.mask = 0x0,
+ 	.data = 0x1,	/* Clear Status */
+ 	},
+ 	{	/* GPI_SMI_STS_GPP_D_0 */
+ 	.offset = 0x144,
+ 	.mask = 0x0,
+ 	.data = 0x1,	/* Clear Status */
+ 	},
+ 	{	/* GPI_NMI_STS_GPP_D_0 */
+ 	.offset = 0x164,
+ 	.mask = 0x0,
+ 	.data = 0x1,	/* Clear Status */
+ 	},
+ 
+ /* Disable interrupts: */
+ 	{	/* GPI_INT_EN_GPP_D_0 */
+ 	.offset = 0x114,
+ 	.mask = 0x1,
+ 	.data = 0x0,	/* Disable interrupt generation */
+ 	},
+ 	{	/* GPI_GPE_EN_GPP_D_0 */
+ 	.offset = 0x134,
+ 	.mask = 0x1,
+ 	.data = 0x0,	/* Disable interrupt generation */
+ 	},
+ 	{	/* GPI_SMI_EN_GPP_D_0 */
+ 	.offset = 0x154,
+ 	.mask = 0x1,
+ 	.data = 0x0,	/* Disable interrupt generation */
+ 	},
+ 	{	/* GPI_NMI_EN_GPP_D_0 */
+ 	.offset = 0x174,
+ 	.mask = 0x1,
+ 	.data = 0x0,	/* Disable interrupt generation */
+ 	},
+ 
+ /* Setup GPP_D_0 Pad Config: */
+ 	{	/* PAD_CFG_DW0_GPP_D_0 */
+ 	.offset = 0x4c0,
+ 	.mask = 0xffffffff,
+ 	.data = 0x82020100,
+ /*
+  *  31:30 Pad Reset Config (PADRSTCFG): = 2h  # PLTRST# (default)
+  *
+  *  29    RX Pad State Select (RXPADSTSEL): = 0 # Raw RX pad state directly
+  *                                                from RX buffer (default)
+  *
+  *  28    RX Raw Override to '1' (RXRAW1): = 0 # No Override
+  *
+  *  26:25 RX Level/Edge Configuration (RXEVCFG):
+  *      = 0h # Level
+  *      = 1h # Edge
+  *
+  *  23    RX Invert (RXINV): = 0 # No Inversion (signal active high)
+  *
+  *  20    GPIO Input Route IOxAPIC (GPIROUTIOXAPIC):
+  * = 0 # Routing does not cause peripheral IRQ...
+  *     # (we want an NMI not an IRQ)
+  *
+  *  19    GPIO Input Route SCI (GPIROUTSCI): = 0 # Routing does not cause SCI.
+  *  18    GPIO Input Route SMI (GPIROUTSMI): = 0 # Routing does not cause SMI.
+  *  17    GPIO Input Route NMI (GPIROUTNMI): = 1 # Routing can cause NMI.
+  *
+  *  11:10 Pad Mode (PMODE1/0): = 0h = GPIO control the Pad.
+  *   9    GPIO RX Disable (GPIORXDIS):
+  * = 0 # Enable the input buffer (active low enable)
+  *
+  *   8    GPIO TX Disable (GPIOTXDIS):
+  * = 1 # Disable the output buffer; i.e. Hi-Z
+  *
+  *   1 GPIO RX State (GPIORXSTATE): This is the current internal RX pad state..
+  *   0 GPIO TX State (GPIOTXSTATE):
+  * = 0 # (Leave at default)
+  */
+ 	},
+ 
+ /* Pad Config DW1 */
+ 	{	/* PAD_CFG_DW1_GPP_D_0 */
+ 	.offset = 0x4c4,
+ 	.mask = 0x3c00,
+ 	.data = 0,	/* Termination = none (default) */
+ 	},
+ };
+ 
+ static void uv_init_hubless_pch_d0(void)
+ {
+ 	int i, read;
+ 
+ 	read = *PCH_PCR_GPIO_ADDRESS(PAD_OWN_GPP_D_0);
+ 	if (read != 0) {
+ 		pr_info("UV: Hubless NMI already configured\n");
+ 		return;
+ 	}
+ 
+ 	nmi_debug("UV: Initializing UV Hubless NMI on PCH\n");
+ 	for (i = 0; i < ARRAY_SIZE(init_nmi); i++) {
+ 		uv_init_hubless_pch_io(init_nmi[i].offset,
+ 					init_nmi[i].mask,
+ 					init_nmi[i].data);
+ 	}
+ }
+ 
+ static int uv_nmi_test_hubless(struct uv_hub_nmi_s *hub_nmi)
+ {
+ 	int *pstat = PCH_PCR_GPIO_ADDRESS(GPI_NMI_STS_GPP_D_0);
+ 	int status = *pstat;
+ 
+ 	hub_nmi->nmi_value = status;
+ 	atomic_inc(&hub_nmi->read_mmr_count);
+ 
+ 	if (!(status & STS_GPP_D_0_MASK))	/* Not a UV external NMI */
+ 		return 0;
+ 
+ 	*pstat = STS_GPP_D_0_MASK;	/* Is a UV NMI: clear GPP_D_0 status */
+ 	(void)*pstat;			/* Flush write */
+ 
+ 	return 1;
+ }
+ 
+ static int uv_test_nmi(struct uv_hub_nmi_s *hub_nmi)
+ {
+ 	if (hub_nmi->hub_present)
+ 		return uv_nmi_test_mmr(hub_nmi);
+ 
+ 	if (hub_nmi->pch_owner)		/* Only PCH owner can check status */
+ 		return uv_nmi_test_hubless(hub_nmi);
+ 
+ 	return -1;
+ }
+ 
+ /*
+  * If first CPU in on this hub, set hub_nmi "in_nmi" and "owner" values and
+  * return true.  If first CPU in on the system, set global "in_nmi" flag.
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
   */
  static int uv_set_in_nmi(int cpu, struct uv_hub_nmi_s *hub_nmi)
  {
@@@ -223,9 -494,10 +420,14 @@@ static int uv_check_nmi(struct uv_hub_n
  			break;
  
  		if (raw_spin_trylock(&hub_nmi->nmi_lock)) {
 -			nmi_detected = uv_test_nmi(hub_nmi);
  
++<<<<<<< HEAD
 +			/* check hub MMR NMI flag */
 +			if (uv_nmi_test_mmr(hub_nmi)) {
++=======
+ 			/* Check flag for UV external NMI */
+ 			if (nmi_detected > 0) {
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  				uv_set_in_nmi(cpu, hub_nmi);
  				nmi = 1;
  				break;
@@@ -235,11 -511,12 +437,11 @@@
  			raw_spin_unlock(&hub_nmi->nmi_lock);
  
  		} else {
 -
 -			/* Wait a moment for the HUB NMI locker to set flag */
 -slave_wait:		cpu_relax();
 +			/* wait a moment for the hub nmi locker to set flag */
 +			cpu_relax();
  			udelay(uv_nmi_slave_delay);
  
- 			/* re-check hub in_nmi flag */
+ 			/* Re-check hub in_nmi flag */
  			nmi = atomic_read(&hub_nmi->in_nmi);
  			if (nmi)
  				break;
@@@ -273,21 -560,7 +475,25 @@@ static inline void uv_clear_nmi(int cpu
  	}
  }
  
++<<<<<<< HEAD
 +/* Print non-responding cpus */
 +static void uv_nmi_nr_cpus_pr(char *fmt)
 +{
 +	static char cpu_list[1024];
 +	int len = sizeof(cpu_list);
 +	int c = cpumask_weight(uv_nmi_cpu_mask);
 +	int n = cpulist_scnprintf(cpu_list, len, uv_nmi_cpu_mask);
 +
 +	if (n >= len-1)
 +		strcpy(&cpu_list[len - 6], "...\n");
 +
 +	printk(fmt, c, cpu_list);
 +}
 +
 +/* Ping non-responding cpus attemping to force them into the NMI handler */
++=======
+ /* Ping non-responding CPU's attemping to force them into the NMI handler */
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  static void uv_nmi_nr_cpus_ping(void)
  {
  	int cpu;
@@@ -310,7 -583,7 +516,11 @@@ static void uv_nmi_cleanup_mask(void
  	}
  }
  
++<<<<<<< HEAD
 +/* Loop waiting as cpus enter nmi handler */
++=======
+ /* Loop waiting as CPU's enter NMI handler */
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  static int uv_nmi_wait_cpus(int first)
  {
  	int i, j, k, n = num_online_cpus();
@@@ -323,6 -597,12 +533,15 @@@
  		k = n - cpumask_weight(uv_nmi_cpu_mask);
  	}
  
++<<<<<<< HEAD
++=======
+ 	/* PCH NMI causes only one CPU to respond */
+ 	if (first && uv_pch_intr_now_enabled) {
+ 		cpumask_clear_cpu(cpu, uv_nmi_cpu_mask);
+ 		return n - k - 1;
+ 	}
+ 
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  	udelay(uv_nmi_initial_delay);
  	for (i = 0; i < uv_nmi_retry_count; i++) {
  		int loop_delay = uv_nmi_loop_delay;
@@@ -355,13 -635,13 +574,18 @@@
  	return n - k;
  }
  
- /* Wait until all slave cpus have entered UV NMI handler */
+ /* Wait until all slave CPU's have entered UV NMI handler */
  static void uv_nmi_wait(int master)
  {
++<<<<<<< HEAD
 +	/* indicate this cpu is in */
 +	atomic_set(&uv_cpu_nmi.state, UV_NMI_STATE_IN);
++=======
+ 	/* Indicate this CPU is in: */
+ 	this_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_IN);
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  
- 	/* if not the first cpu in (the master), then we are a slave cpu */
+ 	/* If not the first CPU in (the master), then we are a slave CPU */
  	if (!master)
  		return;
  
@@@ -370,12 -650,14 +594,20 @@@
  		if (!uv_nmi_wait_cpus(1))
  			break;
  
++<<<<<<< HEAD
 +		/* if not all made it in, send IPI NMI to them */
 +		uv_nmi_nr_cpus_pr(KERN_ALERT
 +			"UV: Sending NMI IPI to %d non-responding CPUs: %s\n");
++=======
+ 		/* If not all made it in, send IPI NMI to them */
+ 		pr_alert("UV: Sending NMI IPI to %d CPUs: %*pbl\n",
+ 			 cpumask_weight(uv_nmi_cpu_mask),
+ 			 cpumask_pr_args(uv_nmi_cpu_mask));
+ 
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  		uv_nmi_nr_cpus_ping();
  
- 		/* if all cpus are in, then done */
+ 		/* If all CPU's are in, then done */
  		if (!uv_nmi_wait_cpus(0))
  			break;
  
@@@ -407,22 -695,21 +639,22 @@@ static void uv_nmi_dump_state_cpu(int c
  {
  	const char *dots = " ................................. ";
  
 -	if (cpu == 0)
 -		uv_nmi_dump_cpu_ip_hdr();
 +	if (uv_nmi_action_is("ips")) {
 +		if (cpu == 0)
 +			uv_nmi_dump_cpu_ip_hdr();
  
 -	if (current->pid != 0 || !uv_nmi_action_is("ips"))
 -		uv_nmi_dump_cpu_ip(cpu, regs);
 +		if (current->pid != 0)
 +			uv_nmi_dump_cpu_ip(cpu, regs);
  
 -	if (uv_nmi_action_is("dump")) {
 -		pr_info("UV:%sNMI process trace for CPU %d\n", dots, cpu);
 +	} else if (uv_nmi_action_is("dump")) {
 +		printk(KERN_DEFAULT
 +			"UV:%sNMI process trace for CPU %d\n", dots, cpu);
  		show_regs(regs);
  	}
 -
 -	this_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_DUMP_DONE);
 +	atomic_set(&uv_cpu_nmi.state, UV_NMI_STATE_DUMP_DONE);
  }
  
- /* Trigger a slave cpu to dump it's state */
+ /* Trigger a slave CPU to dump it's state */
  static void uv_nmi_trigger_dump(int cpu)
  {
  	int retry = uv_nmi_trigger_delay;
@@@ -440,10 -727,10 +672,10 @@@
  	} while (--retry > 0);
  
  	pr_crit("UV: CPU %d stuck in process dump function\n", cpu);
 -	uv_cpu_nmi_per(cpu).state = UV_NMI_STATE_DUMP_DONE;
 +	atomic_set(&uv_cpu_nmi_per(cpu).state, UV_NMI_STATE_DUMP_DONE);
  }
  
- /* Wait until all cpus ready to exit */
+ /* Wait until all CPU's ready to exit */
  static void uv_nmi_sync_exit(int master)
  {
  	atomic_dec(&uv_nmi_cpus_in_nmi);
@@@ -457,7 -744,23 +689,27 @@@
  	}
  }
  
++<<<<<<< HEAD
 +/* Walk through cpu list and dump state of each */
++=======
+ /* Current "health" check is to check which CPU's are responsive */
+ static void uv_nmi_action_health(int cpu, struct pt_regs *regs, int master)
+ {
+ 	if (master) {
+ 		int in = atomic_read(&uv_nmi_cpus_in_nmi);
+ 		int out = num_online_cpus() - in;
+ 
+ 		pr_alert("UV: NMI CPU health check (non-responding:%d)\n", out);
+ 		atomic_set(&uv_nmi_slave_continue, SLAVE_EXIT);
+ 	} else {
+ 		while (!atomic_read(&uv_nmi_slave_continue))
+ 			cpu_relax();
+ 	}
+ 	uv_nmi_sync_exit(master);
+ }
+ 
+ /* Walk through CPU list and dump state of each */
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  static void uv_nmi_dump_state(int cpu, struct pt_regs *regs, int master)
  {
  	if (master) {
@@@ -624,22 -924,32 +876,31 @@@ int uv_handle_nmi(unsigned int reason, 
  	master = (atomic_read(&uv_nmi_cpu) == cpu);
  
  	/* If NMI action is "kdump", then attempt to do it */
 -	if (uv_nmi_action_is("kdump")) {
 +	if (uv_nmi_action_is("kdump"))
  		uv_nmi_kdump(cpu, master, regs);
  
++<<<<<<< HEAD
 +	/* Pause as all cpus enter the NMI handler */
++=======
+ 		/* Unexpected return, revert action to "dump" */
+ 		if (master)
+ 			strncpy(uv_nmi_action, "dump", strlen(uv_nmi_action));
+ 	}
+ 
+ 	/* Pause as all CPU's enter the NMI handler */
++>>>>>>> 1e74016370ec (x86/platform/UV: Clean up the NMI code to match current coding style)
  	uv_nmi_wait(master);
  
 -	/* Process actions other than "kdump": */
 -	if (uv_nmi_action_is("health")) {
 -		uv_nmi_action_health(cpu, regs, master);
 -	} else if (uv_nmi_action_is("ips") || uv_nmi_action_is("dump")) {
 +	/* Dump state of each cpu */
 +	if (uv_nmi_action_is("ips") || uv_nmi_action_is("dump"))
  		uv_nmi_dump_state(cpu, regs, master);
 -	} else if (uv_nmi_action_is("kdb") || uv_nmi_action_is("kgdb")) {
 +
 +	/* Call KGDB/KDB if enabled */
 +	else if (uv_nmi_action_is("kdb") || uv_nmi_action_is("kgdb"))
  		uv_call_kgdb_kdb(cpu, regs, master);
 -	} else {
 -		if (master)
 -			pr_alert("UV: unknown NMI action: %s\n", uv_nmi_action);
 -		uv_nmi_sync_exit(master);
 -	}
  
 -	/* Clear per_cpu "in_nmi" flag */
 -	this_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_OUT);
 +	/* Clear per_cpu "in nmi" flag */
 +	atomic_set(&uv_cpu_nmi.state, UV_NMI_STATE_OUT);
  
  	/* Clear MMR NMI flag on each hub */
  	uv_clear_nmi(cpu);
* Unmerged path arch/x86/platform/uv/uv_nmi.c
