net/mlx5e: Create UMR MKey per RQ

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Create UMR MKey per RQ (Don Dutile) [1385330 1417286]
Rebuild_FUZZ: 93.55%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit ec8b9981ad3f3eeb5dcc4f237266e897c363f896
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ec8b9981.failed

In Striding RQ implementation, we used a single UMR
(User-Mode Memory Registration) memory key for all RQs.
When the product of RQs number*size gets high, we hit a
limitation of u16 field size in FW.

Here we move to using a UMR memory key per RQ, so we can
scale to any number of rings, with the maximum buffer
size in each.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ec8b9981ad3f3eeb5dcc4f237266e897c363f896)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index e0b0c17e5832,63dd6390b161..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -69,8 -75,12 +69,17 @@@
  #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
  #define MLX5_MPWRQ_STRIDES_PER_PAGE		(MLX5_MPWRQ_NUM_STRIDES >> \
  						 MLX5_MPWRQ_WQE_PAGE_ORDER)
++<<<<<<< HEAD
 +#define MLX5_CHANNEL_MAX_NUM_MTTS (ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8) * \
 +				   BIT(MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW))
++=======
+ 
+ #define MLX5_MTT_OCTW(npages) (ALIGN(npages, 8) / 2)
+ #define MLX5E_REQUIRED_MTTS(wqes)		\
+ 	(wqes * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8))
+ #define MLX5E_VALID_NUM_MTTS(num_mtts) (MLX5_MTT_OCTW(num_mtts) - 1 <= U16_MAX)
+ 
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  #define MLX5_UMR_ALIGN				(2048)
  #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
  
@@@ -248,11 -305,56 +257,27 @@@ struct mlx5e_dma_info 
  struct mlx5e_rq {
  	/* data path */
  	struct mlx5_wq_ll      wq;
++<<<<<<< HEAD
 +	u32                    wqe_sz;
 +	struct sk_buff       **skb;
 +	struct mlx5e_mpw_info *wqe_info;
++=======
+ 
+ 	union {
+ 		struct mlx5e_dma_info *dma_info;
+ 		struct {
+ 			struct mlx5e_mpw_info *info;
+ 			void                  *mtt_no_align;
+ 		} mpwqe;
+ 	};
+ 	struct {
+ 		u8             page_order;
+ 		u32            wqe_sz;    /* wqe data buffer size */
+ 		u8             map_dir;   /* dma map direction */
+ 	} buff;
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  	__be32                 mkey_be;
 +	__be32                 umr_mkey_be;
  
  	struct device         *pdev;
  	struct net_device     *netdev;
@@@ -486,11 -689,6 +512,14 @@@ struct mlx5e_priv 
  
  	unsigned long              state;
  	struct mutex               state_lock; /* Protects Interface state */
++<<<<<<< HEAD
 +	struct mlx5_uar            cq_uar;
 +	u32                        pdn;
 +	u32                        tdn;
 +	struct mlx5_core_mkey      mkey;
 +	struct mlx5_core_mkey      umr_mkey;
++=======
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  	struct mlx5e_rq            drop_rq;
  
  	struct mlx5e_channel     **channel;
@@@ -628,6 -835,11 +657,14 @@@ static inline void mlx5e_cq_arm(struct 
  	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, NULL, cq->wq.cc);
  }
  
++<<<<<<< HEAD
++=======
+ static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)
+ {
+ 	return wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
+ }
+ 
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
  {
  	return min_t(int, mdev->priv.eq_table.num_comp_vectors,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
index fa6e9bbc23fb,352462af8d51..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
@@@ -406,18 -478,35 +406,30 @@@ static int mlx5e_set_ringparam(struct n
  			    __func__);
  		return -EINVAL;
  	}
 -
 -	min_rq_size = mlx5e_rx_wqes_to_packets(priv, rq_wq_type,
 -					       1 << mlx5_min_log_rq_size(rq_wq_type));
 -	max_rq_size = mlx5e_rx_wqes_to_packets(priv, rq_wq_type,
 -					       1 << mlx5_max_log_rq_size(rq_wq_type));
 -	rx_pending_wqes = mlx5e_packets_to_rx_wqes(priv, rq_wq_type,
 -						   param->rx_pending);
 -
 -	if (param->rx_pending < min_rq_size) {
 +	if (param->rx_pending < (1 << mlx5_min_log_rq_size(rq_wq_type))) {
  		netdev_info(dev, "%s: rx_pending (%d) < min (%d)\n",
  			    __func__, param->rx_pending,
 -			    min_rq_size);
 +			    1 << mlx5_min_log_rq_size(rq_wq_type));
  		return -EINVAL;
  	}
 -	if (param->rx_pending > max_rq_size) {
 +	if (param->rx_pending > (1 << mlx5_max_log_rq_size(rq_wq_type))) {
  		netdev_info(dev, "%s: rx_pending (%d) > max (%d)\n",
  			    __func__, param->rx_pending,
 -			    max_rq_size);
 +			    1 << mlx5_max_log_rq_size(rq_wq_type));
  		return -EINVAL;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	num_mtts = MLX5E_REQUIRED_MTTS(rx_pending_wqes);
+ 	if (priv->params.rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ &&
+ 	    !MLX5E_VALID_NUM_MTTS(num_mtts)) {
+ 		netdev_info(dev, "%s: rx_pending (%d) request can't be satisfied, try to reduce.\n",
+ 			    __func__, param->rx_pending);
+ 		return -EINVAL;
+ 	}
+ 
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  	if (param->tx_pending < (1 << MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE)) {
  		netdev_info(dev, "%s: tx_pending (%d) < min (%d)\n",
  			    __func__, param->tx_pending,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 2aca61ab5499,84a4adb7bbb0..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -280,6 -370,153 +280,156 @@@ static void mlx5e_disable_async_events(
  #define MLX5E_HW2SW_MTU(hwmtu) (hwmtu - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  #define MLX5E_SW2HW_MTU(swmtu) (swmtu + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  
++<<<<<<< HEAD
++=======
+ static inline int mlx5e_get_wqe_mtt_sz(void)
+ {
+ 	/* UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
+ 	 * To avoid copying garbage after the mtt array, we allocate
+ 	 * a little more.
+ 	 */
+ 	return ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(__be64),
+ 		     MLX5_UMR_MTT_ALIGNMENT);
+ }
+ 
+ static inline void mlx5e_build_umr_wqe(struct mlx5e_rq *rq, struct mlx5e_sq *sq,
+ 				       struct mlx5e_umr_wqe *wqe, u16 ix)
+ {
+ 	struct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;
+ 	struct mlx5_wqe_data_seg      *dseg = &wqe->data;
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	u8 ds_cnt = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS);
+ 	u32 umr_wqe_mtt_offset = mlx5e_get_wqe_mtt_offset(rq, ix);
+ 
+ 	cseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |
+ 				      ds_cnt);
+ 	cseg->fm_ce_se  = MLX5_WQE_CTRL_CQ_UPDATE;
+ 	cseg->imm       = rq->mkey_be;
+ 
+ 	ucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN;
+ 	ucseg->klm_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(MLX5_MPWRQ_PAGES_PER_WQE));
+ 	ucseg->bsf_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(umr_wqe_mtt_offset));
+ 	ucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ 
+ 	dseg->lkey = sq->mkey_be;
+ 	dseg->addr = cpu_to_be64(wi->umr.mtt_addr);
+ }
+ 
+ static int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq,
+ 				     struct mlx5e_channel *c)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int mtt_alloc = mtt_sz + MLX5_UMR_ALIGN - 1;
+ 	int i;
+ 
+ 	rq->mpwqe.info = kzalloc_node(wq_sz * sizeof(*rq->mpwqe.info),
+ 				      GFP_KERNEL, cpu_to_node(c->cpu));
+ 	if (!rq->mpwqe.info)
+ 		goto err_out;
+ 
+ 	/* We allocate more than mtt_sz as we will align the pointer */
+ 	rq->mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz, GFP_KERNEL,
+ 					cpu_to_node(c->cpu));
+ 	if (unlikely(!rq->mpwqe.mtt_no_align))
+ 		goto err_free_wqe_info;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		wi->umr.mtt = PTR_ALIGN(rq->mpwqe.mtt_no_align + i * mtt_alloc,
+ 					MLX5_UMR_ALIGN);
+ 		wi->umr.mtt_addr = dma_map_single(c->pdev, wi->umr.mtt, mtt_sz,
+ 						  PCI_DMA_TODEVICE);
+ 		if (unlikely(dma_mapping_error(c->pdev, wi->umr.mtt_addr)))
+ 			goto err_unmap_mtts;
+ 
+ 		mlx5e_build_umr_wqe(rq, &c->icosq, &wi->umr.wqe, i);
+ 	}
+ 
+ 	return 0;
+ 
+ err_unmap_mtts:
+ 	while (--i >= 0) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(c->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ err_free_wqe_info:
+ 	kfree(rq->mpwqe.info);
+ 
+ err_out:
+ 	return -ENOMEM;
+ }
+ 
+ static void mlx5e_rq_free_mpwqe_info(struct mlx5e_rq *rq)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int i;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ 	kfree(rq->mpwqe.info);
+ }
+ 
+ static int mlx5e_create_umr_mkey(struct mlx5e_priv *priv,
+ 				 u64 npages, u8 page_shift,
+ 				 struct mlx5_core_mkey *umr_mkey)
+ {
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	int inlen = MLX5_ST_SZ_BYTES(create_mkey_in);
+ 	void *mkc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	if (!MLX5E_VALID_NUM_MTTS(npages))
+ 		return -EINVAL;
+ 
+ 	in = mlx5_vzalloc(inlen);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	mkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);
+ 
+ 	MLX5_SET(mkc, mkc, free, 1);
+ 	MLX5_SET(mkc, mkc, umr_en, 1);
+ 	MLX5_SET(mkc, mkc, lw, 1);
+ 	MLX5_SET(mkc, mkc, lr, 1);
+ 	MLX5_SET(mkc, mkc, access_mode, MLX5_MKC_ACCESS_MODE_MTT);
+ 
+ 	MLX5_SET(mkc, mkc, qpn, 0xffffff);
+ 	MLX5_SET(mkc, mkc, pd, mdev->mlx5e_res.pdn);
+ 	MLX5_SET64(mkc, mkc, len, npages << page_shift);
+ 	MLX5_SET(mkc, mkc, translations_octword_size,
+ 		 MLX5_MTT_OCTW(npages));
+ 	MLX5_SET(mkc, mkc, log_page_size, page_shift);
+ 
+ 	err = mlx5_core_create_mkey(mdev, umr_mkey, in, inlen);
+ 
+ 	kvfree(in);
+ 	return err;
+ }
+ 
+ static int mlx5e_create_rq_umr_mkey(struct mlx5e_rq *rq)
+ {
+ 	struct mlx5e_priv *priv = rq->priv;
+ 	u64 num_mtts = MLX5E_REQUIRED_MTTS(BIT(priv->params.log_rq_size));
+ 
+ 	return mlx5e_create_umr_mkey(priv, num_mtts, PAGE_SHIFT, &rq->umr_mkey);
+ }
+ 
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  static int mlx5e_create_rq(struct mlx5e_channel *c,
  			   struct mlx5e_rq_param *param,
  			   struct mlx5e_rq *rq)
@@@ -353,12 -550,99 +503,102 @@@
  	rq->channel = c;
  	rq->ix      = c->ix;
  	rq->priv    = c->priv;
++<<<<<<< HEAD
 +	rq->mkey_be = c->mkey_be;
 +	rq->umr_mkey_be = cpu_to_be32(c->priv->umr_mkey.key);
++=======
+ 
+ 	rq->xdp_prog = priv->xdp_prog ? bpf_prog_inc(priv->xdp_prog) : NULL;
+ 	if (IS_ERR(rq->xdp_prog)) {
+ 		err = PTR_ERR(rq->xdp_prog);
+ 		rq->xdp_prog = NULL;
+ 		goto err_rq_wq_destroy;
+ 	}
+ 
+ 	rq->buff.map_dir = DMA_FROM_DEVICE;
+ 	if (rq->xdp_prog)
+ 		rq->buff.map_dir = DMA_BIDIRECTIONAL;
+ 
+ 	switch (priv->params.rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		if (mlx5e_is_vf_vport_rep(priv)) {
+ 			err = -EINVAL;
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		rq->handle_rx_cqe = mlx5e_handle_rx_cqe_mpwrq;
+ 		rq->alloc_wqe = mlx5e_alloc_rx_mpwqe;
+ 		rq->dealloc_wqe = mlx5e_dealloc_rx_mpwqe;
+ 
+ 		rq->mpwqe_stride_sz = BIT(priv->params.mpwqe_log_stride_sz);
+ 		rq->mpwqe_num_strides = BIT(priv->params.mpwqe_log_num_strides);
+ 
+ 		rq->buff.wqe_sz = rq->mpwqe_stride_sz * rq->mpwqe_num_strides;
+ 		byte_count = rq->buff.wqe_sz;
+ 
+ 		err = mlx5e_create_rq_umr_mkey(rq);
+ 		if (err)
+ 			goto err_rq_wq_destroy;
+ 		rq->mkey_be = cpu_to_be32(rq->umr_mkey.key);
+ 
+ 		err = mlx5e_rq_alloc_mpwqe_info(rq, c);
+ 		if (err)
+ 			goto err_destroy_umr_mkey;
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_LINKED_LIST */
+ 		rq->dma_info = kzalloc_node(wq_sz * sizeof(*rq->dma_info),
+ 					    GFP_KERNEL, cpu_to_node(c->cpu));
+ 		if (!rq->dma_info) {
+ 			err = -ENOMEM;
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		if (mlx5e_is_vf_vport_rep(priv))
+ 			rq->handle_rx_cqe = mlx5e_handle_rx_cqe_rep;
+ 		else
+ 			rq->handle_rx_cqe = mlx5e_handle_rx_cqe;
+ 
+ 		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
+ 		rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
+ 
+ 		rq->buff.wqe_sz = (priv->params.lro_en) ?
+ 				priv->params.lro_wqe_sz :
+ 				MLX5E_SW2HW_MTU(priv->netdev->mtu);
+ 		byte_count = rq->buff.wqe_sz;
+ 
+ 		/* calc the required page order */
+ 		frag_sz = MLX5_RX_HEADROOM +
+ 			  byte_count /* packet data */ +
+ 			  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+ 		frag_sz = SKB_DATA_ALIGN(frag_sz);
+ 
+ 		npages = DIV_ROUND_UP(frag_sz, PAGE_SIZE);
+ 		rq->buff.page_order = order_base_2(npages);
+ 
+ 		byte_count |= MLX5_HW_START_PADDING;
+ 		rq->mkey_be = c->mkey_be;
+ 	}
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(&rq->wq, i);
+ 
+ 		wqe->data.byte_count = cpu_to_be32(byte_count);
+ 		wqe->data.lkey = rq->mkey_be;
+ 	}
+ 
+ 	INIT_WORK(&rq->am.work, mlx5e_rx_am_work);
+ 	rq->am.mode = priv->params.rx_cq_period_mode;
+ 
+ 	rq->page_cache.head = 0;
+ 	rq->page_cache.tail = 0;
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  
  	return 0;
  
+ err_destroy_umr_mkey:
+ 	mlx5_core_destroy_mkey(mdev, &rq->umr_mkey);
+ 
  err_rq_wq_destroy:
 -	if (rq->xdp_prog)
 -		bpf_prog_put(rq->xdp_prog);
  	mlx5_wq_destroy(&rq->wq_ctrl);
  
  	return err;
@@@ -366,14 -650,26 +606,19 @@@
  
  static void mlx5e_destroy_rq(struct mlx5e_rq *rq)
  {
 -	int i;
 -
 -	if (rq->xdp_prog)
 -		bpf_prog_put(rq->xdp_prog);
 -
  	switch (rq->wq_type) {
  	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
++<<<<<<< HEAD
 +		kfree(rq->wqe_info);
++=======
+ 		mlx5e_rq_free_mpwqe_info(rq);
+ 		mlx5_core_destroy_mkey(rq->priv->mdev, &rq->umr_mkey);
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  		break;
  	default: /* MLX5_WQ_TYPE_LINKED_LIST */
 -		kfree(rq->dma_info);
 +		kfree(rq->skb);
  	}
  
 -	for (i = rq->page_cache.head; i != rq->page_cache.tail;
 -	     i = (i + 1) & (MLX5E_CACHE_SIZE - 1)) {
 -		struct mlx5e_dma_info *dma_info = &rq->page_cache.page_cache[i];
 -
 -		mlx5e_page_release(rq, dma_info, false);
 -	}
  	mlx5_wq_destroy(&rq->wq_ctrl);
  }
  
@@@ -3030,43 -3861,31 +3275,49 @@@ static void *mlx5e_create_netdev(struc
  
  	priv->wq = create_singlethread_workqueue("mlx5e");
  	if (!priv->wq)
 -		goto err_cleanup_nic;
 +		goto err_free_netdev;
  
 -	return netdev;
 +	err = mlx5_alloc_map_uar(mdev, &priv->cq_uar, false);
 +	if (err) {
 +		mlx5_core_err(mdev, "alloc_map uar failed, %d\n", err);
 +		goto err_destroy_wq;
 +	}
  
 -err_cleanup_nic:
 -	profile->cleanup(priv);
 -	free_netdev(netdev);
 +	err = mlx5_core_alloc_pd(mdev, &priv->pdn);
 +	if (err) {
 +		mlx5_core_err(mdev, "alloc pd failed, %d\n", err);
 +		goto err_unmap_free_uar;
 +	}
  
 -	return NULL;
 -}
 +	err = mlx5_core_alloc_transport_domain(mdev, &priv->tdn);
 +	if (err) {
 +		mlx5_core_err(mdev, "alloc td failed, %d\n", err);
 +		goto err_dealloc_pd;
 +	}
  
 -int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev)
 -{
 -	const struct mlx5e_profile *profile;
 -	struct mlx5e_priv *priv;
 -	u16 max_mtu;
 -	int err;
 +	err = mlx5e_create_mkey(priv, priv->pdn, &priv->mkey);
 +	if (err) {
 +		mlx5_core_err(mdev, "create mkey failed, %d\n", err);
 +		goto err_dealloc_transport_domain;
 +	}
  
 -	priv = netdev_priv(netdev);
 -	profile = priv->profile;
 -	clear_bit(MLX5E_STATE_DESTROYING, &priv->state);
++<<<<<<< HEAD
 +	err = mlx5e_create_umr_mkey(priv);
 +	if (err) {
 +		mlx5_core_err(mdev, "create umr mkey failed, %d\n", err);
 +		goto err_destroy_mkey;
 +	}
  
 +	err = mlx5e_create_tises(priv);
 +	if (err) {
 +		mlx5_core_warn(mdev, "create tises failed, %d\n", err);
 +		goto err_destroy_umr_mkey;
 +	}
++=======
+ 	err = profile->init_tx(priv);
+ 	if (err)
+ 		goto out;
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  
  	err = mlx5e_open_drop_rq(priv);
  	if (err) {
@@@ -3104,6 -3908,144 +3355,147 @@@
  
  	mlx5e_set_dev_port_mtu(netdev);
  
++<<<<<<< HEAD
++=======
+ 	if (profile->enable)
+ 		profile->enable(priv);
+ 
+ 	rtnl_lock();
+ 	if (netif_running(netdev))
+ 		mlx5e_open(netdev);
+ 	netif_device_attach(netdev);
+ 	rtnl_unlock();
+ 
+ 	return 0;
+ 
+ err_close_drop_rq:
+ 	mlx5e_close_drop_rq(priv);
+ 
+ err_cleanup_tx:
+ 	profile->cleanup_tx(priv);
+ 
+ out:
+ 	return err;
+ }
+ 
+ static void mlx5e_register_vport_rep(struct mlx5_core_dev *mdev)
+ {
+ 	struct mlx5_eswitch *esw = mdev->priv.eswitch;
+ 	int total_vfs = MLX5_TOTAL_VPORTS(mdev);
+ 	int vport;
+ 	u8 mac[ETH_ALEN];
+ 
+ 	if (!MLX5_CAP_GEN(mdev, vport_group_manager))
+ 		return;
+ 
+ 	mlx5_query_nic_vport_mac_address(mdev, 0, mac);
+ 
+ 	for (vport = 1; vport < total_vfs; vport++) {
+ 		struct mlx5_eswitch_rep rep;
+ 
+ 		rep.load = mlx5e_vport_rep_load;
+ 		rep.unload = mlx5e_vport_rep_unload;
+ 		rep.vport = vport;
+ 		ether_addr_copy(rep.hw_id, mac);
+ 		mlx5_eswitch_register_vport_rep(esw, vport, &rep);
+ 	}
+ }
+ 
+ void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(netdev);
+ 	const struct mlx5e_profile *profile = priv->profile;
+ 
+ 	set_bit(MLX5E_STATE_DESTROYING, &priv->state);
+ 	if (profile->disable)
+ 		profile->disable(priv);
+ 
+ 	flush_workqueue(priv->wq);
+ 
+ 	rtnl_lock();
+ 	if (netif_running(netdev))
+ 		mlx5e_close(netdev);
+ 	netif_device_detach(netdev);
+ 	rtnl_unlock();
+ 
+ 	mlx5e_destroy_q_counter(priv);
+ 	profile->cleanup_rx(priv);
+ 	mlx5e_close_drop_rq(priv);
+ 	profile->cleanup_tx(priv);
+ 	cancel_delayed_work_sync(&priv->update_stats_work);
+ }
+ 
+ /* mlx5e_attach and mlx5e_detach scope should be only creating/destroying
+  * hardware contexts and to connect it to the current netdev.
+  */
+ static int mlx5e_attach(struct mlx5_core_dev *mdev, void *vpriv)
+ {
+ 	struct mlx5e_priv *priv = vpriv;
+ 	struct net_device *netdev = priv->netdev;
+ 	int err;
+ 
+ 	if (netif_device_present(netdev))
+ 		return 0;
+ 
+ 	err = mlx5e_create_mdev_resources(mdev);
+ 	if (err)
+ 		return err;
+ 
+ 	err = mlx5e_attach_netdev(mdev, netdev);
+ 	if (err) {
+ 		mlx5e_destroy_mdev_resources(mdev);
+ 		return err;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void mlx5e_detach(struct mlx5_core_dev *mdev, void *vpriv)
+ {
+ 	struct mlx5e_priv *priv = vpriv;
+ 	struct net_device *netdev = priv->netdev;
+ 
+ 	if (!netif_device_present(netdev))
+ 		return;
+ 
+ 	mlx5e_detach_netdev(mdev, netdev);
+ 	mlx5e_destroy_mdev_resources(mdev);
+ }
+ 
+ static void *mlx5e_add(struct mlx5_core_dev *mdev)
+ {
+ 	struct mlx5_eswitch *esw = mdev->priv.eswitch;
+ 	int total_vfs = MLX5_TOTAL_VPORTS(mdev);
+ 	void *ppriv = NULL;
+ 	void *priv;
+ 	int vport;
+ 	int err;
+ 	struct net_device *netdev;
+ 
+ 	err = mlx5e_check_required_hca_cap(mdev);
+ 	if (err)
+ 		return NULL;
+ 
+ 	mlx5e_register_vport_rep(mdev);
+ 
+ 	if (MLX5_CAP_GEN(mdev, vport_group_manager))
+ 		ppriv = &esw->offloads.vport_reps[0];
+ 
+ 	netdev = mlx5e_create_netdev(mdev, &mlx5e_nic_profile, ppriv);
+ 	if (!netdev) {
+ 		mlx5_core_err(mdev, "mlx5e_create_netdev failed\n");
+ 		goto err_unregister_reps;
+ 	}
+ 
+ 	priv = netdev_priv(netdev);
+ 
+ 	err = mlx5e_attach(mdev, priv);
+ 	if (err) {
+ 		mlx5_core_err(mdev, "mlx5e_attach failed, %d\n", err);
+ 		goto err_destroy_netdev;
+ 	}
+ 
++>>>>>>> ec8b9981ad3f (net/mlx5e: Create UMR MKey per RQ)
  	err = register_netdev(netdev);
  	if (err) {
  		mlx5_core_err(mdev, "register_netdev failed, %d\n", err);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
