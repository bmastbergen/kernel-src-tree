x86/intel_rdt: Add scheduler hook

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] intel_rdt: Add scheduler hook (Jiri Olsa) [1288964]
Rebuild_FUZZ: 93.55%
commit-author Fenghua Yu <fenghua.yu@intel.com>
commit 4f341a5e48443fcc2e2d935ca990e462c02bb1a6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/4f341a5e.failed

Hook the x86 scheduler code to update closid based on whether the current
task is assigned to a specific closid or running on a CPU assigned to a
specific closid.

	Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
	Cc: "Ravi V Shankar" <ravi.v.shankar@intel.com>
	Cc: "Tony Luck" <tony.luck@intel.com>
	Cc: "Shaohua Li" <shli@fb.com>
	Cc: "Sai Prakhya" <sai.praneeth.prakhya@intel.com>
	Cc: "Peter Zijlstra" <peterz@infradead.org>
	Cc: "Stephane Eranian" <eranian@google.com>
	Cc: "Dave Hansen" <dave.hansen@intel.com>
	Cc: "David Carrillo-Cisneros" <davidcc@google.com>
	Cc: "Nilay Vaish" <nilayvaish@gmail.com>
	Cc: "Vikas Shivappa" <vikas.shivappa@linux.intel.com>
	Cc: "Ingo Molnar" <mingo@elte.hu>
	Cc: "Borislav Petkov" <bp@suse.de>
	Cc: "H. Peter Anvin" <h.peter.anvin@intel.com>
Link: http://lkml.kernel.org/r/1477692289-37412-10-git-send-email-fenghua.yu@intel.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 4f341a5e48443fcc2e2d935ca990e462c02bb1a6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/process_32.c
#	arch/x86/kernel/process_64.c
diff --cc arch/x86/kernel/process_32.c
index dd02d275aa54,efe7f9fce44e..000000000000
--- a/arch/x86/kernel/process_32.c
+++ b/arch/x86/kernel/process_32.c
@@@ -54,17 -53,8 +54,22 @@@
  #include <asm/syscalls.h>
  #include <asm/debugreg.h>
  #include <asm/switch_to.h>
++<<<<<<< HEAD
 +
 +asmlinkage void ret_from_fork(void) __asm__("ret_from_fork");
 +asmlinkage void ret_from_kernel_thread(void) __asm__("ret_from_kernel_thread");
 +
 +/*
 + * Return saved PC of a blocked thread.
 + */
 +unsigned long thread_saved_pc(struct task_struct *tsk)
 +{
 +	return ((unsigned long *)tsk->thread.sp)[3];
 +}
++=======
+ #include <asm/vm86.h>
+ #include <asm/intel_rdt.h>
++>>>>>>> 4f341a5e4844 (x86/intel_rdt: Add scheduler hook)
  
  void __show_regs(struct pt_regs *regs, int all)
  {
@@@ -311,33 -300,8 +316,36 @@@ __switch_to(struct task_struct *prev_p
  
  	this_cpu_write(current_task, next_p);
  
+ 	/* Load the Intel cache allocation PQR MSR. */
+ 	intel_rdt_sched_in();
+ 
  	return prev_p;
  }
 +
 +#define top_esp                (THREAD_SIZE - sizeof(unsigned long))
 +#define top_ebp                (THREAD_SIZE - 2*sizeof(unsigned long))
 +
 +unsigned long get_wchan(struct task_struct *p)
 +{
 +	unsigned long bp, sp, ip;
 +	unsigned long stack_page;
 +	int count = 0;
 +	if (!p || p == current || p->state == TASK_RUNNING)
 +		return 0;
 +	stack_page = (unsigned long)task_stack_page(p);
 +	sp = p->thread.sp;
 +	if (!stack_page || sp < stack_page || sp > top_esp+stack_page)
 +		return 0;
 +	/* include/asm-i386/system.h:switch_to() pushes bp last. */
 +	bp = *(unsigned long *) sp;
 +	do {
 +		if (bp < stack_page || bp > top_ebp+stack_page)
 +			return 0;
 +		ip = *(unsigned long *) (bp+4);
 +		if (!in_sched_functions(ip))
 +			return ip;
 +		bp = *(unsigned long *) bp;
 +	} while (count++ < 16);
 +	return 0;
 +}
 +
diff --cc arch/x86/kernel/process_64.c
index b940388bc318,acd7d6f507af..000000000000
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@@ -49,10 -48,11 +49,16 @@@
  #include <asm/syscalls.h>
  #include <asm/debugreg.h>
  #include <asm/switch_to.h>
++<<<<<<< HEAD
++=======
+ #include <asm/xen/hypervisor.h>
+ #include <asm/vdso.h>
+ #include <asm/intel_rdt.h>
++>>>>>>> 4f341a5e4844 (x86/intel_rdt: Add scheduler hook)
  
 -__visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
 +asmlinkage extern void ret_from_fork(void);
 +
 +DEFINE_PER_CPU(unsigned long, old_rsp);
  
  /* Prints also some state that isn't saved in the pt_regs */
  void __show_regs(struct pt_regs *regs, int all)
@@@ -415,6 -435,48 +421,51 @@@ __switch_to(struct task_struct *prev_p
  		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
  		__switch_to_xtra(prev_p, next_p, tss);
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_XEN
+ 	/*
+ 	 * On Xen PV, IOPL bits in pt_regs->flags have no effect, and
+ 	 * current_pt_regs()->flags may not match the current task's
+ 	 * intended IOPL.  We need to switch it manually.
+ 	 */
+ 	if (unlikely(static_cpu_has(X86_FEATURE_XENPV) &&
+ 		     prev->iopl != next->iopl))
+ 		xen_set_iopl_mask(next->iopl);
+ #endif
+ 
+ 	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {
+ 		/*
+ 		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but
+ 		 * does not update the cached descriptor.  As a result, if we
+ 		 * do SYSRET while SS is NULL, we'll end up in user mode with
+ 		 * SS apparently equal to __USER_DS but actually unusable.
+ 		 *
+ 		 * The straightforward workaround would be to fix it up just
+ 		 * before SYSRET, but that would slow down the system call
+ 		 * fast paths.  Instead, we ensure that SS is never NULL in
+ 		 * system call context.  We do this by replacing NULL SS
+ 		 * selectors at every context switch.  SYSCALL sets up a valid
+ 		 * SS, so the only way to get NULL is to re-enter the kernel
+ 		 * from CPL 3 through an interrupt.  Since that can't happen
+ 		 * in the same task as a running syscall, we are guaranteed to
+ 		 * context switch between every interrupt vector entry and a
+ 		 * subsequent SYSRET.
+ 		 *
+ 		 * We read SS first because SS reads are much faster than
+ 		 * writes.  Out of caution, we force SS to __KERNEL_DS even if
+ 		 * it previously had a different non-NULL value.
+ 		 */
+ 		unsigned short ss_sel;
+ 		savesegment(ss, ss_sel);
+ 		if (ss_sel != __KERNEL_DS)
+ 			loadsegment(ss, __KERNEL_DS);
+ 	}
+ 
+ 	/* Load the Intel cache allocation PQR MSR. */
+ 	intel_rdt_sched_in();
+ 
++>>>>>>> 4f341a5e4844 (x86/intel_rdt: Add scheduler hook)
  	return prev_p;
  }
  
diff --git a/arch/x86/include/asm/intel_rdt.h b/arch/x86/include/asm/intel_rdt.h
index 2e5eab09083e..5bc72a4dbd5e 100644
--- a/arch/x86/include/asm/intel_rdt.h
+++ b/arch/x86/include/asm/intel_rdt.h
@@ -1,8 +1,12 @@
 #ifndef _ASM_X86_INTEL_RDT_H
 #define _ASM_X86_INTEL_RDT_H
 
+#ifdef CONFIG_INTEL_RDT_A
+
 #include <linux/jump_label.h>
 
+#include <asm/intel_rdt_common.h>
+
 #define IA32_L3_QOS_CFG		0xc81
 #define IA32_L3_CBM_BASE	0xc90
 #define IA32_L2_CBM_BASE	0xd10
@@ -176,4 +180,42 @@ ssize_t rdtgroup_schemata_write(struct kernfs_open_file *of,
 				char *buf, size_t nbytes, loff_t off);
 int rdtgroup_schemata_show(struct kernfs_open_file *of,
 			   struct seq_file *s, void *v);
+
+/*
+ * intel_rdt_sched_in() - Writes the task's CLOSid to IA32_PQR_MSR
+ *
+ * Following considerations are made so that this has minimal impact
+ * on scheduler hot path:
+ * - This will stay as no-op unless we are running on an Intel SKU
+ *   which supports resource control and we enable by mounting the
+ *   resctrl file system.
+ * - Caches the per cpu CLOSid values and does the MSR write only
+ *   when a task with a different CLOSid is scheduled in.
+ */
+static inline void intel_rdt_sched_in(void)
+{
+	if (static_branch_likely(&rdt_enable_key)) {
+		struct intel_pqr_state *state = this_cpu_ptr(&pqr_state);
+		int closid;
+
+		/*
+		 * If this task has a closid assigned, use it.
+		 * Else use the closid assigned to this cpu.
+		 */
+		closid = current->closid;
+		if (closid == 0)
+			closid = this_cpu_read(cpu_closid);
+
+		if (closid != state->closid) {
+			state->closid = closid;
+			wrmsr(MSR_IA32_PQR_ASSOC, state->rmid, closid);
+		}
+	}
+}
+
+#else
+
+static inline void intel_rdt_sched_in(void) {}
+
+#endif /* CONFIG_INTEL_RDT_A */
 #endif /* _ASM_X86_INTEL_RDT_H */
diff --git a/arch/x86/kernel/cpu/intel_rdt.c b/arch/x86/kernel/cpu/intel_rdt.c
index 40094aed5f71..5a533fefefa0 100644
--- a/arch/x86/kernel/cpu/intel_rdt.c
+++ b/arch/x86/kernel/cpu/intel_rdt.c
@@ -29,7 +29,6 @@
 #include <linux/cacheinfo.h>
 #include <linux/cpuhotplug.h>
 
-#include <asm/intel_rdt_common.h>
 #include <asm/intel-family.h>
 #include <asm/intel_rdt.h>
 
diff --git a/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c b/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
index 5c4bab9452b0..a90ad22b9823 100644
--- a/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
+++ b/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
@@ -292,6 +292,9 @@ static void move_myself(struct callback_head *head)
 		kfree(rdtgrp);
 	}
 
+	/* update PQR_ASSOC MSR to make resource group go into effect */
+	intel_rdt_sched_in();
+
 	kfree(callback);
 }
 
* Unmerged path arch/x86/kernel/process_32.c
* Unmerged path arch/x86/kernel/process_64.c
