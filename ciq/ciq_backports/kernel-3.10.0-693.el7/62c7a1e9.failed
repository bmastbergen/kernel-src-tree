locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Ingo Molnar <mingo@kernel.org>
commit 62c7a1e9ae54ef66658df9614bdbc09cbbdaa6f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/62c7a1e9.failed

Valentin Rothberg reported that we use CONFIG_QUEUED_SPINLOCKS
in arch/x86/kernel/paravirt_patch_32.c, while the symbol is
called CONFIG_QUEUED_SPINLOCK. (Note the extra 'S')

But the typo was natural: the proper English term for such
a generic object would be 'queued spinlocks' - so rename
this and related symbols accordingly to the plural form.

	Reported-by: Valentin Rothberg <valentinrothberg@gmail.com>
	Cc: Douglas Hatch <doug.hatch@hp.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Scott J Norton <scott.norton@hp.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Waiman Long <Waiman.Long@hp.com>
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 62c7a1e9ae54ef66658df9614bdbc09cbbdaa6f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/include/asm/paravirt.h
#	arch/x86/include/asm/paravirt_types.h
#	arch/x86/include/asm/spinlock.h
#	arch/x86/include/asm/spinlock_types.h
#	arch/x86/kernel/kvm.c
#	arch/x86/kernel/paravirt-spinlocks.c
#	arch/x86/kernel/paravirt_patch_64.c
#	arch/x86/xen/spinlock.c
#	kernel/Kconfig.locks
#	kernel/locking/Makefile
diff --cc arch/x86/Kconfig
index d30a5322b64f,f8dc6abbe6ae..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -124,6 -127,8 +124,11 @@@ config X8
  	select MODULES_USE_ELF_RELA if X86_64
  	select CLONE_BACKWARDS if X86_32
  	select ARCH_USE_BUILTIN_BSWAP
++<<<<<<< HEAD
++=======
+ 	select ARCH_USE_QUEUED_SPINLOCKS
+ 	select ARCH_USE_QUEUE_RWLOCK
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  	select OLD_SIGSUSPEND3 if X86_32 || IA32_EMULATION
  	select OLD_SIGACTION if X86_32
  	select COMPAT_OLD_SIGACTION if IA32_EMULATION
@@@ -630,7 -667,7 +635,11 @@@ config PARAVIRT_DEBU
  config PARAVIRT_SPINLOCKS
  	bool "Paravirtualization layer for spinlocks"
  	depends on PARAVIRT && SMP
++<<<<<<< HEAD
 +	select UNINLINE_SPIN_UNLOCK
++=======
+ 	select UNINLINE_SPIN_UNLOCK if !QUEUED_SPINLOCKS
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  	---help---
  	  Paravirtualized spinlocks allow a pvops backend to replace the
  	  spinlock implementation with something virtualization-friendly
diff --cc arch/x86/include/asm/paravirt.h
index 9a12f4a8dad4,d143bfad45d7..000000000000
--- a/arch/x86/include/asm/paravirt.h
+++ b/arch/x86/include/asm/paravirt.h
@@@ -679,6 -712,31 +679,34 @@@ static inline void __set_fixmap(unsigne
  
  #if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ 
+ static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
+ 							u32 val)
+ {
+ 	PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);
+ }
+ 
+ static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
+ {
+ 	PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);
+ }
+ 
+ static __always_inline void pv_wait(u8 *ptr, u8 val)
+ {
+ 	PVOP_VCALL2(pv_lock_ops.wait, ptr, val);
+ }
+ 
+ static __always_inline void pv_kick(int cpu)
+ {
+ 	PVOP_VCALL1(pv_lock_ops.kick, cpu);
+ }
+ 
+ #else /* !CONFIG_QUEUED_SPINLOCKS */
+ 
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  static __always_inline void __ticket_lock_spinning(struct arch_spinlock *lock,
  							__ticket_t ticket)
  {
@@@ -691,7 -749,9 +719,13 @@@ static __always_inline void __ticket_un
  	PVOP_VCALL2(pv_lock_ops.unlock_kick, lock, ticket);
  }
  
++<<<<<<< HEAD
 +#endif
++=======
+ #endif /* CONFIG_QUEUED_SPINLOCKS */
+ 
+ #endif /* SMP && PARAVIRT_SPINLOCKS */
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  
  #ifdef CONFIG_X86_32
  #define PV_SAVE_REGS "pushl %ecx; pushl %edx;"
diff --cc arch/x86/include/asm/paravirt_types.h
index 7e0527fa618b,8766c7c395c2..000000000000
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@@ -339,9 -333,19 +339,22 @@@ struct arch_spinlock
  typedef u16 __ticket_t;
  #endif
  
 -struct qspinlock;
 -
  struct pv_lock_ops {
++<<<<<<< HEAD
 +	struct paravirt_callee_save lock_spinning;
 +	void (*unlock_kick)(struct arch_spinlock *lock, __ticket_t ticket);
++=======
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ 	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);
+ 	struct paravirt_callee_save queued_spin_unlock;
+ 
+ 	void (*wait)(u8 *ptr, u8 val);
+ 	void (*kick)(int cpu);
+ #else /* !CONFIG_QUEUED_SPINLOCKS */
+ 	struct paravirt_callee_save lock_spinning;
+ 	void (*unlock_kick)(struct arch_spinlock *lock, __ticket_t ticket);
+ #endif /* !CONFIG_QUEUED_SPINLOCKS */
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  };
  
  /* This contains all the paravirt structures: we get a convenient
diff --cc arch/x86/include/asm/spinlock.h
index 9b4f7b005289,be0a05913b91..000000000000
--- a/arch/x86/include/asm/spinlock.h
+++ b/arch/x86/include/asm/spinlock.h
@@@ -47,6 -42,10 +47,13 @@@ static __always_inline int arch_spin_va
  extern struct static_key paravirt_ticketlocks_enabled;
  static __always_inline bool static_key_false(struct static_key *key);
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ #include <asm/qspinlock.h>
+ #else
+ 
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  #ifdef CONFIG_PARAVIRT_SPINLOCKS
  
  static inline void __ticket_enter_slowpath(arch_spinlock_t *lock)
@@@ -183,9 -185,22 +190,13 @@@ static __always_inline void arch_spin_l
  
  static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)
  {
 -	__ticket_t head = READ_ONCE(lock->tickets.head);
 -
 -	for (;;) {
 -		struct __raw_tickets tmp = READ_ONCE(lock->tickets);
 -		/*
 -		 * We need to check "unlocked" in a loop, tmp.head == head
 -		 * can be false positive because of overflow.
 -		 */
 -		if (__tickets_equal(tmp.head, tmp.tail) ||
 -				!__tickets_equal(tmp.head, head))
 -			break;
 -
 +	while (arch_spin_is_locked(lock))
  		cpu_relax();
 -	}
  }
++<<<<<<< HEAD
++=======
+ #endif /* CONFIG_QUEUED_SPINLOCKS */
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  
  /*
   * Read-write spinlocks, allowing multiple readers
diff --cc arch/x86/include/asm/spinlock_types.h
index c7cf2f532f65,65c3e37f879a..000000000000
--- a/arch/x86/include/asm/spinlock_types.h
+++ b/arch/x86/include/asm/spinlock_types.h
@@@ -23,6 -23,9 +23,12 @@@ typedef u32 __ticketpair_t
  
  #define TICKET_SHIFT	(sizeof(__ticket_t) * 8)
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ #include <asm-generic/qspinlock_types.h>
+ #else
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  typedef struct arch_spinlock {
  	union {
  		__ticketpair_t head_tail;
@@@ -33,7 -36,8 +39,11 @@@
  } arch_spinlock_t;
  
  #define __ARCH_SPIN_LOCK_UNLOCKED	{ { 0 } }
++<<<<<<< HEAD
++=======
+ #endif /* CONFIG_QUEUED_SPINLOCKS */
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  
 -#include <asm-generic/qrwlock_types.h>
 +#include <asm/rwlock.h>
  
  #endif /* _ASM_X86_SPINLOCK_TYPES_H */
diff --cc arch/x86/kernel/kvm.c
index d35c81117504,1681504e44a4..000000000000
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@@ -575,6 -584,39 +575,42 @@@ static void kvm_kick_cpu(int cpu
  	kvm_hypercall2(KVM_HC_KICK_CPU, flags, apicid);
  }
  
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ 
+ #include <asm/qspinlock.h>
+ 
+ static void kvm_wait(u8 *ptr, u8 val)
+ {
+ 	unsigned long flags;
+ 
+ 	if (in_nmi())
+ 		return;
+ 
+ 	local_irq_save(flags);
+ 
+ 	if (READ_ONCE(*ptr) != val)
+ 		goto out;
+ 
+ 	/*
+ 	 * halt until it's our turn and kicked. Note that we do safe halt
+ 	 * for irq enabled case to avoid hang when lock info is overwritten
+ 	 * in irq spinlock slowpath and no spurious interrupt occur to save us.
+ 	 */
+ 	if (arch_irqs_disabled_flags(flags))
+ 		halt();
+ 	else
+ 		safe_halt();
+ 
+ out:
+ 	local_irq_restore(flags);
+ }
+ 
+ #else /* !CONFIG_QUEUED_SPINLOCKS */
+ 
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  enum kvm_contention_stat {
  	TAKEN_SLOW,
  	TAKEN_SLOW_PICKUP,
@@@ -803,6 -850,8 +839,11 @@@ static void kvm_unlock_kick(struct arch
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #endif /* !CONFIG_QUEUED_SPINLOCKS */
+ 
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  /*
   * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.
   */
@@@ -814,8 -863,16 +855,18 @@@ void __init kvm_spinlock_init(void
  	if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
  		return;
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ 	__pv_init_lock_hash();
+ 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ 	pv_lock_ops.wait = kvm_wait;
+ 	pv_lock_ops.kick = kvm_kick_cpu;
+ #else /* !CONFIG_QUEUED_SPINLOCKS */
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(kvm_lock_spinning);
  	pv_lock_ops.unlock_kick = kvm_unlock_kick;
 -#endif
  }
  
  static __init int kvm_spinlock_init_jump(void)
diff --cc arch/x86/kernel/paravirt-spinlocks.c
index bbb6c7316341,33ee3e0efd65..000000000000
--- a/arch/x86/kernel/paravirt-spinlocks.c
+++ b/arch/x86/kernel/paravirt-spinlocks.c
@@@ -8,11 -8,33 +8,41 @@@
  
  #include <asm/paravirt.h>
  
++<<<<<<< HEAD
 +struct pv_lock_ops pv_lock_ops = {
 +#ifdef CONFIG_SMP
 +	.lock_spinning = __PV_IS_CALLEE_SAVE(paravirt_nop),
 +	.unlock_kick = paravirt_nop,
 +#endif
++=======
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ __visible void __native_queued_spin_unlock(struct qspinlock *lock)
+ {
+ 	native_queued_spin_unlock(lock);
+ }
+ 
+ PV_CALLEE_SAVE_REGS_THUNK(__native_queued_spin_unlock);
+ 
+ bool pv_is_native_spin_unlock(void)
+ {
+ 	return pv_lock_ops.queued_spin_unlock.func ==
+ 		__raw_callee_save___native_queued_spin_unlock;
+ }
+ #endif
+ 
+ struct pv_lock_ops pv_lock_ops = {
+ #ifdef CONFIG_SMP
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ 	.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
+ 	.queued_spin_unlock = PV_CALLEE_SAVE(__native_queued_spin_unlock),
+ 	.wait = paravirt_nop,
+ 	.kick = paravirt_nop,
+ #else /* !CONFIG_QUEUED_SPINLOCKS */
+ 	.lock_spinning = __PV_IS_CALLEE_SAVE(paravirt_nop),
+ 	.unlock_kick = paravirt_nop,
+ #endif /* !CONFIG_QUEUED_SPINLOCKS */
+ #endif /* SMP */
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  };
  EXPORT_SYMBOL(pv_lock_ops);
  
diff --cc arch/x86/kernel/paravirt_patch_64.c
index 3f08f34f93eb,a1fa86782186..000000000000
--- a/arch/x86/kernel/paravirt_patch_64.c
+++ b/arch/x86/kernel/paravirt_patch_64.c
@@@ -22,6 -21,10 +22,13 @@@ DEF_NATIVE(pv_cpu_ops, swapgs, "swapgs"
  DEF_NATIVE(, mov32, "mov %edi, %eax");
  DEF_NATIVE(, mov64, "mov %rdi, %rax");
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_PARAVIRT_SPINLOCKS) && defined(CONFIG_QUEUED_SPINLOCKS)
+ DEF_NATIVE(pv_lock_ops, queued_spin_unlock, "movb $0, (%rdi)");
+ #endif
+ 
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  unsigned paravirt_patch_ident_32(void *insnbuf, unsigned len)
  {
  	return paravirt_patch_insns(insnbuf, len,
@@@ -61,10 -65,14 +68,21 @@@ unsigned native_patch(u8 type, u16 clob
  		PATCH_SITE(pv_cpu_ops, clts);
  		PATCH_SITE(pv_mmu_ops, flush_tlb_single);
  		PATCH_SITE(pv_cpu_ops, wbinvd);
++<<<<<<< HEAD
 +
 +	patch_site:
 +		ret = paravirt_patch_insns(ibuf, len, start, end);
 +		break;
++=======
+ #if defined(CONFIG_PARAVIRT_SPINLOCKS) && defined(CONFIG_QUEUED_SPINLOCKS)
+ 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
+ 			if (pv_is_native_spin_unlock()) {
+ 				start = start_pv_lock_ops_queued_spin_unlock;
+ 				end   = end_pv_lock_ops_queued_spin_unlock;
+ 				goto patch_site;
+ 			}
+ #endif
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  
  	default:
  		ret = paravirt_patch_default(type, clobbers, ibuf, addr, len);
diff --cc arch/x86/xen/spinlock.c
index ebab852bada1,9e2ba5c6e1dd..000000000000
--- a/arch/x86/xen/spinlock.c
+++ b/arch/x86/xen/spinlock.c
@@@ -17,6 -17,56 +17,59 @@@
  #include "xen-ops.h"
  #include "debugfs.h"
  
++<<<<<<< HEAD
++=======
+ static DEFINE_PER_CPU(int, lock_kicker_irq) = -1;
+ static DEFINE_PER_CPU(char *, irq_name);
+ static bool xen_pvspin = true;
+ 
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ 
+ #include <asm/qspinlock.h>
+ 
+ static void xen_qlock_kick(int cpu)
+ {
+ 	xen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);
+ }
+ 
+ /*
+  * Halt the current CPU & release it back to the host
+  */
+ static void xen_qlock_wait(u8 *byte, u8 val)
+ {
+ 	int irq = __this_cpu_read(lock_kicker_irq);
+ 
+ 	/* If kicker interrupts not initialized yet, just spin */
+ 	if (irq == -1)
+ 		return;
+ 
+ 	/* clear pending */
+ 	xen_clear_irq_pending(irq);
+ 	barrier();
+ 
+ 	/*
+ 	 * We check the byte value after clearing pending IRQ to make sure
+ 	 * that we won't miss a wakeup event because of the clearing.
+ 	 *
+ 	 * The sync_clear_bit() call in xen_clear_irq_pending() is atomic.
+ 	 * So it is effectively a memory barrier for x86.
+ 	 */
+ 	if (READ_ONCE(*byte) != val)
+ 		return;
+ 
+ 	/*
+ 	 * If an interrupt happens here, it will leave the wakeup irq
+ 	 * pending, which will cause xen_poll_irq() to return
+ 	 * immediately.
+ 	 */
+ 
+ 	/* Block until irq becomes pending (or perhaps a spurious wakeup) */
+ 	xen_poll_irq(irq);
+ }
+ 
+ #else /* CONFIG_QUEUED_SPINLOCKS */
+ 
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  enum xen_contention_stat {
  	TAKEN_SLOW,
  	TAKEN_SLOW_PICKUP,
@@@ -212,6 -264,7 +265,10 @@@ static void xen_unlock_kick(struct arch
  		}
  	}
  }
++<<<<<<< HEAD
++=======
+ #endif /* CONFIG_QUEUED_SPINLOCKS */
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  
  static irqreturn_t dummy_handler(int irq, void *dev_id)
  {
@@@ -266,12 -327,37 +323,41 @@@ void __init xen_init_spinlocks(void
  		printk(KERN_DEBUG "xen: PV spinlocks disabled\n");
  		return;
  	}
++<<<<<<< HEAD
++=======
+ 	printk(KERN_DEBUG "xen: PV spinlocks enabled\n");
+ #ifdef CONFIG_QUEUED_SPINLOCKS
+ 	__pv_init_lock_hash();
+ 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ 	pv_lock_ops.wait = xen_qlock_wait;
+ 	pv_lock_ops.kick = xen_qlock_kick;
+ #else
+ 	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);
+ 	pv_lock_ops.unlock_kick = xen_unlock_kick;
+ #endif
+ }
+ 
+ /*
+  * While the jump_label init code needs to happend _after_ the jump labels are
+  * enabled and before SMP is started. Hence we use pre-SMP initcall level
+  * init. We cannot do it in xen_init_spinlocks as that is done before
+  * jump labels are activated.
+  */
+ static __init int xen_init_spinlocks_jump(void)
+ {
+ 	if (!xen_pvspin)
+ 		return 0;
+ 
+ 	if (!xen_domain())
+ 		return 0;
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  
  	static_key_slow_inc(&paravirt_ticketlocks_enabled);
 -	return 0;
 +
 +	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);
 +	pv_lock_ops.unlock_kick = xen_unlock_kick;
  }
 -early_initcall(xen_init_spinlocks_jump);
  
  static __init int xen_parse_nopvspin(char *arg)
  {
@@@ -280,7 -366,7 +366,11 @@@
  }
  early_param("xen_nopvspin", xen_parse_nopvspin);
  
++<<<<<<< HEAD
 +#ifdef CONFIG_XEN_DEBUG_FS
++=======
+ #if defined(CONFIG_XEN_DEBUG_FS) && !defined(CONFIG_QUEUED_SPINLOCKS)
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
  
  static struct dentry *d_spin_debug;
  
diff --cc kernel/Kconfig.locks
index 44511d100eaa,65d755b6a663..000000000000
--- a/kernel/Kconfig.locks
+++ b/kernel/Kconfig.locks
@@@ -220,6 -220,31 +220,32 @@@ config INLINE_WRITE_UNLOCK_IRQRESTOR
  
  endif
  
 -config ARCH_SUPPORTS_ATOMIC_RMW
 -	bool
 -
  config MUTEX_SPIN_ON_OWNER
  	def_bool y
++<<<<<<< HEAD
 +	depends on SMP && !DEBUG_MUTEXES
++=======
+ 	depends on SMP && !DEBUG_MUTEXES && ARCH_SUPPORTS_ATOMIC_RMW
+ 
+ config RWSEM_SPIN_ON_OWNER
+        def_bool y
+        depends on SMP && RWSEM_XCHGADD_ALGORITHM && ARCH_SUPPORTS_ATOMIC_RMW
+ 
+ config LOCK_SPIN_ON_OWNER
+        def_bool y
+        depends on MUTEX_SPIN_ON_OWNER || RWSEM_SPIN_ON_OWNER
+ 
+ config ARCH_USE_QUEUED_SPINLOCKS
+ 	bool
+ 
+ config QUEUED_SPINLOCKS
+ 	def_bool y if ARCH_USE_QUEUED_SPINLOCKS
+ 	depends on SMP
+ 
+ config ARCH_USE_QUEUE_RWLOCK
+ 	bool
+ 
+ config QUEUE_RWLOCK
+ 	def_bool y if ARCH_USE_QUEUE_RWLOCK
+ 	depends on SMP
++>>>>>>> 62c7a1e9ae54 (locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS)
* Unmerged path kernel/locking/Makefile
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/include/asm/paravirt.h
* Unmerged path arch/x86/include/asm/paravirt_types.h
* Unmerged path arch/x86/include/asm/spinlock.h
* Unmerged path arch/x86/include/asm/spinlock_types.h
* Unmerged path arch/x86/kernel/kvm.c
* Unmerged path arch/x86/kernel/paravirt-spinlocks.c
* Unmerged path arch/x86/kernel/paravirt_patch_64.c
* Unmerged path arch/x86/xen/spinlock.c
* Unmerged path kernel/Kconfig.locks
* Unmerged path kernel/locking/Makefile
