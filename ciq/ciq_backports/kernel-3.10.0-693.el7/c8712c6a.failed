blk-mq: skip unmapped queues in blk_mq_alloc_request_hctx

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Christoph Hellwig <hch@lst.de>
commit c8712c6a674e3382fe4d26d108251ccfa55d08e0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/c8712c6a.failed

This provides the caller a feedback that a given hctx is not mapped and thus
no command can be sent on it.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit c8712c6a674e3382fe4d26d108251ccfa55d08e0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 3caf05b09404,c207fa9870eb..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -268,6 -271,57 +268,60 @@@ struct request *blk_mq_alloc_request(st
  }
  EXPORT_SYMBOL(blk_mq_alloc_request);
  
++<<<<<<< HEAD
++=======
+ struct request *blk_mq_alloc_request_hctx(struct request_queue *q, int rw,
+ 		unsigned int flags, unsigned int hctx_idx)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	struct blk_mq_ctx *ctx;
+ 	struct request *rq;
+ 	struct blk_mq_alloc_data alloc_data;
+ 	int ret;
+ 
+ 	/*
+ 	 * If the tag allocator sleeps we could get an allocation for a
+ 	 * different hardware context.  No need to complicate the low level
+ 	 * allocator for this for the rare use case of a command tied to
+ 	 * a specific queue.
+ 	 */
+ 	if (WARN_ON_ONCE(!(flags & BLK_MQ_REQ_NOWAIT)))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	if (hctx_idx >= q->nr_hw_queues)
+ 		return ERR_PTR(-EIO);
+ 
+ 	ret = blk_queue_enter(q, true);
+ 	if (ret)
+ 		return ERR_PTR(ret);
+ 
+ 	/*
+ 	 * Check if the hardware context is actually mapped to anything.
+ 	 * If not tell the caller that it should skip this queue.
+ 	 */
+ 	hctx = q->queue_hw_ctx[hctx_idx];
+ 	if (!blk_mq_hw_queue_mapped(hctx)) {
+ 		ret = -EXDEV;
+ 		goto out_queue_exit;
+ 	}
+ 	ctx = __blk_mq_get_ctx(q, cpumask_first(hctx->cpumask));
+ 
+ 	blk_mq_set_alloc_data(&alloc_data, q, flags, ctx, hctx);
+ 	rq = __blk_mq_alloc_request(&alloc_data, rw, 0);
+ 	if (!rq) {
+ 		ret = -EWOULDBLOCK;
+ 		goto out_queue_exit;
+ 	}
+ 
+ 	return rq;
+ 
+ out_queue_exit:
+ 	blk_queue_exit(q);
+ 	return ERR_PTR(ret);
+ }
+ EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
+ 
++>>>>>>> c8712c6a674e (blk-mq: skip unmapped queues in blk_mq_alloc_request_hctx)
  static void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx,
  				  struct blk_mq_ctx *ctx, struct request *rq)
  {
* Unmerged path block/blk-mq.c
