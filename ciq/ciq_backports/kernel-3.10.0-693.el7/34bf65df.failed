amd-xgbe: Add netif_* message support to the driver

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Lendacky, Thomas <Thomas.Lendacky@amd.com>
commit 34bf65dfa34369d283582cfff2ec916f62043043
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/34bf65df.failed

Add support for the network interface message level settings for
determining whether to issue some of the driver messages. Make
use of the netif_* interface where appropriate.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 34bf65dfa34369d283582cfff2ec916f62043043)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amd/xgbe/xgbe-dcb.c
#	drivers/net/ethernet/amd/xgbe/xgbe-desc.c
#	drivers/net/ethernet/amd/xgbe/xgbe-dev.c
#	drivers/net/ethernet/amd/xgbe/xgbe-drv.c
#	drivers/net/ethernet/amd/xgbe/xgbe-main.c
#	drivers/net/ethernet/amd/xgbe/xgbe-mdio.c
#	drivers/net/ethernet/amd/xgbe/xgbe.h
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-desc.c
index 28954354521f,87b73d43391c..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-desc.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-desc.c
@@@ -405,7 -532,9 +408,13 @@@ static int xgbe_map_tx_skb(struct xgbe_
  		}
  		rdata->skb_dma = skb_dma;
  		rdata->skb_dma_len = packet->header_len;
++<<<<<<< HEAD
 +		rdata->tso_header = 1;
++=======
+ 		netif_dbg(pdata, tx_queued, pdata->netdev,
+ 			  "skb header: index=%u, dma=%pad, len=%u\n",
+ 			  cur_index, &skb_dma, packet->header_len);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  		offset = packet->header_len;
  
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-dev.c
index ec5481f846ee,dab3a1ed566b..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
@@@ -584,43 -751,44 +586,49 @@@ static int xgbe_set_addn_mac_addrs(stru
  		mac_addr[0] = ha->addr[4];
  		mac_addr[1] = ha->addr[5];
  
++<<<<<<< HEAD
 +		DBGPR("  adding unicast address %pM at 0x%04x\n",
 +		      ha->addr, mac_reg);
++=======
+ 		netif_dbg(pdata, drv, pdata->netdev,
+ 			  "adding mac address %pM at %#x\n",
+ 			  ha->addr, *mac_reg);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  		XGMAC_SET_BITS(mac_addr_hi, MAC_MACA1HR, AE, 1);
 -	}
 -
 -	XGMAC_IOWRITE(pdata, *mac_reg, mac_addr_hi);
 -	*mac_reg += MAC_MACA_INC;
 -	XGMAC_IOWRITE(pdata, *mac_reg, mac_addr_lo);
 -	*mac_reg += MAC_MACA_INC;
 -}
  
 -static void xgbe_set_mac_addn_addrs(struct xgbe_prv_data *pdata)
 -{
 -	struct net_device *netdev = pdata->netdev;
 -	struct netdev_hw_addr *ha;
 -	unsigned int mac_reg;
 -	unsigned int addn_macs;
 +		XGMAC_IOWRITE(pdata, mac_reg, mac_addr_hi);
 +		mac_reg += MAC_MACA_INC;
 +		XGMAC_IOWRITE(pdata, mac_reg, mac_addr_lo);
 +		mac_reg += MAC_MACA_INC;
  
 -	mac_reg = MAC_MACA1HR;
 -	addn_macs = pdata->hw_feat.addn_mac;
 +		i++;
 +	}
  
 -	if (netdev_uc_count(netdev) > addn_macs) {
 -		xgbe_set_promiscuous_mode(pdata, 1);
 -	} else {
 -		netdev_for_each_uc_addr(ha, netdev) {
 -			xgbe_set_mac_reg(pdata, ha, &mac_reg);
 -			addn_macs--;
 -		}
 -
 -		if (netdev_mc_count(netdev) > addn_macs) {
 -			xgbe_set_all_multicast_mode(pdata, 1);
 -		} else {
 -			netdev_for_each_mc_addr(ha, netdev) {
 -				xgbe_set_mac_reg(pdata, ha, &mac_reg);
 -				addn_macs--;
 -			}
 +	if (!am_mode) {
 +		netdev_for_each_mc_addr(ha, pdata->netdev) {
 +			mac_addr_lo = 0;
 +			mac_addr_hi = 0;
 +			mac_addr = (u8 *)&mac_addr_lo;
 +			mac_addr[0] = ha->addr[0];
 +			mac_addr[1] = ha->addr[1];
 +			mac_addr[2] = ha->addr[2];
 +			mac_addr[3] = ha->addr[3];
 +			mac_addr = (u8 *)&mac_addr_hi;
 +			mac_addr[0] = ha->addr[4];
 +			mac_addr[1] = ha->addr[5];
 +
 +			DBGPR("  adding multicast address %pM at 0x%04x\n",
 +			      ha->addr, mac_reg);
 +
 +			XGMAC_SET_BITS(mac_addr_hi, MAC_MACA1HR, AE, 1);
 +
 +			XGMAC_IOWRITE(pdata, mac_reg, mac_addr_hi);
 +			mac_reg += MAC_MACA_INC;
 +			XGMAC_IOWRITE(pdata, mac_reg, mac_addr_lo);
 +			mac_reg += MAC_MACA_INC;
 +
 +			i++;
  		}
  	}
  
@@@ -923,7 -1201,209 +931,213 @@@ static void xgbe_rx_desc_init(struct xg
  	DBGPR("<--rx_desc_init\n");
  }
  
++<<<<<<< HEAD
 +static void xgbe_pre_xmit(struct xgbe_channel *channel)
++=======
+ static void xgbe_update_tstamp_addend(struct xgbe_prv_data *pdata,
+ 				      unsigned int addend)
+ {
+ 	/* Set the addend register value and tell the device */
+ 	XGMAC_IOWRITE(pdata, MAC_TSAR, addend);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_TSCR, TSADDREG, 1);
+ 
+ 	/* Wait for addend update to complete */
+ 	while (XGMAC_IOREAD_BITS(pdata, MAC_TSCR, TSADDREG))
+ 		udelay(5);
+ }
+ 
+ static void xgbe_set_tstamp_time(struct xgbe_prv_data *pdata, unsigned int sec,
+ 				 unsigned int nsec)
+ {
+ 	/* Set the time values and tell the device */
+ 	XGMAC_IOWRITE(pdata, MAC_STSUR, sec);
+ 	XGMAC_IOWRITE(pdata, MAC_STNUR, nsec);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_TSCR, TSINIT, 1);
+ 
+ 	/* Wait for time update to complete */
+ 	while (XGMAC_IOREAD_BITS(pdata, MAC_TSCR, TSINIT))
+ 		udelay(5);
+ }
+ 
+ static u64 xgbe_get_tstamp_time(struct xgbe_prv_data *pdata)
+ {
+ 	u64 nsec;
+ 
+ 	nsec = XGMAC_IOREAD(pdata, MAC_STSR);
+ 	nsec *= NSEC_PER_SEC;
+ 	nsec += XGMAC_IOREAD(pdata, MAC_STNR);
+ 
+ 	return nsec;
+ }
+ 
+ static u64 xgbe_get_tx_tstamp(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int tx_snr;
+ 	u64 nsec;
+ 
+ 	tx_snr = XGMAC_IOREAD(pdata, MAC_TXSNR);
+ 	if (XGMAC_GET_BITS(tx_snr, MAC_TXSNR, TXTSSTSMIS))
+ 		return 0;
+ 
+ 	nsec = XGMAC_IOREAD(pdata, MAC_TXSSR);
+ 	nsec *= NSEC_PER_SEC;
+ 	nsec += tx_snr;
+ 
+ 	return nsec;
+ }
+ 
+ static void xgbe_get_rx_tstamp(struct xgbe_packet_data *packet,
+ 			       struct xgbe_ring_desc *rdesc)
+ {
+ 	u64 nsec;
+ 
+ 	if (XGMAC_GET_BITS_LE(rdesc->desc3, RX_CONTEXT_DESC3, TSA) &&
+ 	    !XGMAC_GET_BITS_LE(rdesc->desc3, RX_CONTEXT_DESC3, TSD)) {
+ 		nsec = le32_to_cpu(rdesc->desc1);
+ 		nsec <<= 32;
+ 		nsec |= le32_to_cpu(rdesc->desc0);
+ 		if (nsec != 0xffffffffffffffffULL) {
+ 			packet->rx_tstamp = nsec;
+ 			XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,
+ 				       RX_TSTAMP, 1);
+ 		}
+ 	}
+ }
+ 
+ static int xgbe_config_tstamp(struct xgbe_prv_data *pdata,
+ 			      unsigned int mac_tscr)
+ {
+ 	/* Set one nano-second accuracy */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSCTRLSSR, 1);
+ 
+ 	/* Set fine timestamp update */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSCFUPDT, 1);
+ 
+ 	/* Overwrite earlier timestamps */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TXTSSTSM, 1);
+ 
+ 	XGMAC_IOWRITE(pdata, MAC_TSCR, mac_tscr);
+ 
+ 	/* Exit if timestamping is not enabled */
+ 	if (!XGMAC_GET_BITS(mac_tscr, MAC_TSCR, TSENA))
+ 		return 0;
+ 
+ 	/* Initialize time registers */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_SSIR, SSINC, XGBE_TSTAMP_SSINC);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_SSIR, SNSINC, XGBE_TSTAMP_SNSINC);
+ 	xgbe_update_tstamp_addend(pdata, pdata->tstamp_addend);
+ 	xgbe_set_tstamp_time(pdata, 0, 0);
+ 
+ 	/* Initialize the timecounter */
+ 	timecounter_init(&pdata->tstamp_tc, &pdata->tstamp_cc,
+ 			 ktime_to_ns(ktime_get_real()));
+ 
+ 	return 0;
+ }
+ 
+ static void xgbe_config_dcb_tc(struct xgbe_prv_data *pdata)
+ {
+ 	struct ieee_ets *ets = pdata->ets;
+ 	unsigned int total_weight, min_weight, weight;
+ 	unsigned int i;
+ 
+ 	if (!ets)
+ 		return;
+ 
+ 	/* Set Tx to deficit weighted round robin scheduling algorithm (when
+ 	 * traffic class is using ETS algorithm)
+ 	 */
+ 	XGMAC_IOWRITE_BITS(pdata, MTL_OMR, ETSALG, MTL_ETSALG_DWRR);
+ 
+ 	/* Set Traffic Class algorithms */
+ 	total_weight = pdata->netdev->mtu * pdata->hw_feat.tc_cnt;
+ 	min_weight = total_weight / 100;
+ 	if (!min_weight)
+ 		min_weight = 1;
+ 
+ 	for (i = 0; i < pdata->hw_feat.tc_cnt; i++) {
+ 		switch (ets->tc_tsa[i]) {
+ 		case IEEE_8021QAZ_TSA_STRICT:
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TC%u using SP\n", i);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_ETSCR, TSA,
+ 					       MTL_TSA_SP);
+ 			break;
+ 		case IEEE_8021QAZ_TSA_ETS:
+ 			weight = total_weight * ets->tc_tx_bw[i] / 100;
+ 			weight = clamp(weight, min_weight, total_weight);
+ 
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TC%u using DWRR (weight %u)\n", i, weight);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_ETSCR, TSA,
+ 					       MTL_TSA_ETS);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_QWR, QW,
+ 					       weight);
+ 			break;
+ 		}
+ 	}
+ }
+ 
+ static void xgbe_config_dcb_pfc(struct xgbe_prv_data *pdata)
+ {
+ 	struct ieee_pfc *pfc = pdata->pfc;
+ 	struct ieee_ets *ets = pdata->ets;
+ 	unsigned int mask, reg, reg_val;
+ 	unsigned int tc, prio;
+ 
+ 	if (!pfc || !ets)
+ 		return;
+ 
+ 	for (tc = 0; tc < pdata->hw_feat.tc_cnt; tc++) {
+ 		mask = 0;
+ 		for (prio = 0; prio < IEEE_8021QAZ_MAX_TCS; prio++) {
+ 			if ((pfc->pfc_en & (1 << prio)) &&
+ 			    (ets->prio_tc[prio] == tc))
+ 				mask |= (1 << prio);
+ 		}
+ 		mask &= 0xff;
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev, "TC%u PFC mask=%#x\n",
+ 			  tc, mask);
+ 		reg = MTL_TCPM0R + (MTL_TCPM_INC * (tc / MTL_TCPM_TC_PER_REG));
+ 		reg_val = XGMAC_IOREAD(pdata, reg);
+ 
+ 		reg_val &= ~(0xff << ((tc % MTL_TCPM_TC_PER_REG) << 3));
+ 		reg_val |= (mask << ((tc % MTL_TCPM_TC_PER_REG) << 3));
+ 
+ 		XGMAC_IOWRITE(pdata, reg, reg_val);
+ 	}
+ 
+ 	xgbe_config_flow_control(pdata);
+ }
+ 
+ static void xgbe_tx_start_xmit(struct xgbe_channel *channel,
+ 			       struct xgbe_ring *ring)
+ {
+ 	struct xgbe_prv_data *pdata = channel->pdata;
+ 	struct xgbe_ring_data *rdata;
+ 
+ 	/* Make sure everything is written before the register write */
+ 	wmb();
+ 
+ 	/* Issue a poll command to Tx DMA by writing address
+ 	 * of next immediate free descriptor */
+ 	rdata = XGBE_GET_DESC_DATA(ring, ring->cur);
+ 	XGMAC_DMA_IOWRITE(channel, DMA_CH_TDTR_LO,
+ 			  lower_32_bits(rdata->rdesc_dma));
+ 
+ 	/* Start the Tx timer */
+ 	if (pdata->tx_usecs && !channel->tx_timer_active) {
+ 		channel->tx_timer_active = 1;
+ 		mod_timer(&channel->tx_timer,
+ 			  jiffies + usecs_to_jiffies(pdata->tx_usecs));
+ 	}
+ 
+ 	ring->tx.xmit_more = 0;
+ }
+ 
+ static void xgbe_dev_xmit(struct xgbe_channel *channel)
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  {
  	struct xgbe_prv_data *pdata = channel->pdata;
  	struct xgbe_ring *ring = channel->tx_ring;
@@@ -1105,27 -1604,20 +1321,27 @@@
  	rdesc = rdata->rdesc;
  	XGMAC_SET_BITS_LE(rdesc->desc3, TX_NORMAL_DESC3, OWN, 1);
  
- #ifdef XGMAC_ENABLE_TX_DESC_DUMP
- 	xgbe_dump_tx_desc(ring, start_index, packet->rdesc_count, 1);
- #endif
+ 	if (netif_msg_tx_queued(pdata))
+ 		xgbe_dump_tx_desc(pdata, ring, start_index,
+ 				  packet->rdesc_count, 1);
  
  	/* Make sure ownership is written to the descriptor */
 -	dma_wmb();
 +	wmb();
  
 -	ring->cur = cur_index + 1;
 -	if (!packet->skb->xmit_more ||
 -	    netif_xmit_stopped(netdev_get_tx_queue(pdata->netdev,
 -						   channel->queue_index)))
 -		xgbe_tx_start_xmit(channel, ring);
 -	else
 -		ring->tx.xmit_more = 1;
 +	/* Issue a poll command to Tx DMA by writing address
 +	 * of next immediate free descriptor */
 +	ring->cur++;
 +	rdata = XGBE_GET_DESC_DATA(ring, ring->cur);
 +	XGMAC_DMA_IOWRITE(channel, DMA_CH_TDTR_LO,
 +			  lower_32_bits(rdata->rdesc_dma));
 +
 +	/* Start the Tx coalescing timer */
 +	if (tx_coalesce && !channel->tx_timer_active) {
 +		channel->tx_timer_active = 1;
 +		hrtimer_start(&channel->tx_timer,
 +			      ktime_set(0, pdata->tx_usecs * NSEC_PER_USEC),
 +			      HRTIMER_MODE_REL);
 +	}
  
  	DBGPR("  %s: descriptors %u to %u written\n",
  	      channel->name, start_index & (ring->rdesc_count - 1),
@@@ -1153,14 -1646,60 +1369,13 @@@ static int xgbe_dev_read(struct xgbe_ch
  		return 1;
  
  	/* Make sure descriptor fields are read after reading the OWN bit */
 -	dma_rmb();
 +	rmb();
  
- #ifdef XGMAC_ENABLE_RX_DESC_DUMP
- 	xgbe_dump_rx_desc(ring, rdesc, ring->cur);
- #endif
+ 	if (netif_msg_rx_status(pdata))
+ 		xgbe_dump_rx_desc(pdata, ring, ring->cur);
  
 -	if (XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, CTXT)) {
 -		/* Timestamp Context Descriptor */
 -		xgbe_get_rx_tstamp(packet, rdesc);
 -
 -		XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,
 -			       CONTEXT, 1);
 -		XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,
 -			       CONTEXT_NEXT, 0);
 -		return 0;
 -	}
 -
 -	/* Normal Descriptor, be sure Context Descriptor bit is off */
 -	XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES, CONTEXT, 0);
 -
 -	/* Indicate if a Context Descriptor is next */
 -	if (XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, CDA))
 -		XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,
 -			       CONTEXT_NEXT, 1);
 -
 -	/* Get the header length */
 -	if (XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, FD)) {
 -		rdata->rx.hdr_len = XGMAC_GET_BITS_LE(rdesc->desc2,
 -						      RX_NORMAL_DESC2, HL);
 -		if (rdata->rx.hdr_len)
 -			pdata->ext_stats.rx_split_header_packets++;
 -	}
 -
 -	/* Get the RSS hash */
 -	if (XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, RSV)) {
 -		XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,
 -			       RSS_HASH, 1);
 -
 -		packet->rss_hash = le32_to_cpu(rdesc->desc1);
 -
 -		l34t = XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, L34T);
 -		switch (l34t) {
 -		case RX_DESC3_L34T_IPV4_TCP:
 -		case RX_DESC3_L34T_IPV4_UDP:
 -		case RX_DESC3_L34T_IPV6_TCP:
 -		case RX_DESC3_L34T_IPV6_UDP:
 -			packet->rss_hash_type = PKT_HASH_TYPE_L4;
 -			break;
 -		default:
 -			packet->rss_hash_type = PKT_HASH_TYPE_L3;
 -		}
 -	}
 -
  	/* Get the packet length */
 -	rdata->rx.len = XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, PL);
 +	rdata->len = XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, PL);
  
  	if (!XGMAC_GET_BITS_LE(rdesc->desc3, RX_NORMAL_DESC3, LD)) {
  		/* Not all the data has been transferred for this packet */
@@@ -1491,13 -2035,14 +1707,19 @@@ static void xgbe_config_tx_fifo_size(st
  	unsigned int i;
  
  	fifo_size = xgbe_calculate_per_queue_fifo(pdata->hw_feat.tx_fifo_size,
 -						  pdata->tx_q_count);
 +						  pdata->hw_feat.tx_q_cnt);
  
 -	for (i = 0; i < pdata->tx_q_count; i++)
 +	for (i = 0; i < pdata->hw_feat.tx_q_cnt; i++)
  		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_TQOMR, TQS, fifo_size);
  
++<<<<<<< HEAD
 +	netdev_notice(pdata->netdev, "%d Tx queues, %d byte fifo per queue\n",
 +		      pdata->hw_feat.tx_q_cnt, ((fifo_size + 1) * 256));
++=======
+ 	netif_info(pdata, drv, pdata->netdev,
+ 		   "%d Tx hardware queues, %d byte fifo per queue\n",
+ 		   pdata->tx_q_count, ((fifo_size + 1) * 256));
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  }
  
  static void xgbe_config_rx_fifo_size(struct xgbe_prv_data *pdata)
@@@ -1506,19 -2051,81 +1728,91 @@@
  	unsigned int i;
  
  	fifo_size = xgbe_calculate_per_queue_fifo(pdata->hw_feat.rx_fifo_size,
 -						  pdata->rx_q_count);
 +						  pdata->hw_feat.rx_q_cnt);
  
 -	for (i = 0; i < pdata->rx_q_count; i++)
 +	for (i = 0; i < pdata->hw_feat.rx_q_cnt; i++)
  		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, RQS, fifo_size);
  
++<<<<<<< HEAD
 +	netdev_notice(pdata->netdev, "%d Rx queues, %d byte fifo per queue\n",
 +		      pdata->hw_feat.rx_q_cnt, ((fifo_size + 1) * 256));
++=======
+ 	netif_info(pdata, drv, pdata->netdev,
+ 		   "%d Rx hardware queues, %d byte fifo per queue\n",
+ 		   pdata->rx_q_count, ((fifo_size + 1) * 256));
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  }
  
 -static void xgbe_config_queue_mapping(struct xgbe_prv_data *pdata)
 +static void xgbe_config_rx_queue_mapping(struct xgbe_prv_data *pdata)
  {
++<<<<<<< HEAD
 +	unsigned int i, reg, reg_val;
 +	unsigned int q_count = pdata->hw_feat.rx_q_cnt;
++=======
+ 	unsigned int qptc, qptc_extra, queue;
+ 	unsigned int prio_queues;
+ 	unsigned int ppq, ppq_extra, prio;
+ 	unsigned int mask;
+ 	unsigned int i, j, reg, reg_val;
+ 
+ 	/* Map the MTL Tx Queues to Traffic Classes
+ 	 *   Note: Tx Queues >= Traffic Classes
+ 	 */
+ 	qptc = pdata->tx_q_count / pdata->hw_feat.tc_cnt;
+ 	qptc_extra = pdata->tx_q_count % pdata->hw_feat.tc_cnt;
+ 
+ 	for (i = 0, queue = 0; i < pdata->hw_feat.tc_cnt; i++) {
+ 		for (j = 0; j < qptc; j++) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TXq%u mapped to TC%u\n", queue, i);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, queue, MTL_Q_TQOMR,
+ 					       Q2TCMAP, i);
+ 			pdata->q2tc_map[queue++] = i;
+ 		}
+ 
+ 		if (i < qptc_extra) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TXq%u mapped to TC%u\n", queue, i);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, queue, MTL_Q_TQOMR,
+ 					       Q2TCMAP, i);
+ 			pdata->q2tc_map[queue++] = i;
+ 		}
+ 	}
+ 
+ 	/* Map the 8 VLAN priority values to available MTL Rx queues */
+ 	prio_queues = min_t(unsigned int, IEEE_8021QAZ_MAX_TCS,
+ 			    pdata->rx_q_count);
+ 	ppq = IEEE_8021QAZ_MAX_TCS / prio_queues;
+ 	ppq_extra = IEEE_8021QAZ_MAX_TCS % prio_queues;
+ 
+ 	reg = MAC_RQC2R;
+ 	reg_val = 0;
+ 	for (i = 0, prio = 0; i < prio_queues;) {
+ 		mask = 0;
+ 		for (j = 0; j < ppq; j++) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "PRIO%u mapped to RXq%u\n", prio, i);
+ 			mask |= (1 << prio);
+ 			pdata->prio2q_map[prio++] = i;
+ 		}
+ 
+ 		if (i < ppq_extra) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "PRIO%u mapped to RXq%u\n", prio, i);
+ 			mask |= (1 << prio);
+ 			pdata->prio2q_map[prio++] = i;
+ 		}
+ 
+ 		reg_val |= (mask << ((i++ % MAC_RQC2_Q_PER_REG) << 3));
+ 
+ 		if ((i % MAC_RQC2_Q_PER_REG) && (i != prio_queues))
+ 			continue;
+ 
+ 		XGMAC_IOWRITE(pdata, reg, reg_val);
+ 		reg += MAC_RQC2_INC;
+ 		reg_val = 0;
+ 	}
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  	/* Select dynamic mapping of MTL Rx queue to DMA Rx channel */
  	reg = MTL_RQDCM0R;
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-drv.c
index d58e85811bc9,cc5af67357f5..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
@@@ -124,15 -127,132 +124,138 @@@
  #include "xgbe.h"
  #include "xgbe-common.h"
  
 -static int xgbe_one_poll(struct napi_struct *, int);
 -static int xgbe_all_poll(struct napi_struct *, int);
  
++<<<<<<< HEAD
 +static int xgbe_poll(struct napi_struct *, int);
 +static void xgbe_set_rx_mode(struct net_device *);
++=======
+ static int xgbe_alloc_channels(struct xgbe_prv_data *pdata)
+ {
+ 	struct xgbe_channel *channel_mem, *channel;
+ 	struct xgbe_ring *tx_ring, *rx_ring;
+ 	unsigned int count, i;
+ 	int ret = -ENOMEM;
+ 
+ 	count = max_t(unsigned int, pdata->tx_ring_count, pdata->rx_ring_count);
+ 
+ 	channel_mem = kcalloc(count, sizeof(struct xgbe_channel), GFP_KERNEL);
+ 	if (!channel_mem)
+ 		goto err_channel;
+ 
+ 	tx_ring = kcalloc(pdata->tx_ring_count, sizeof(struct xgbe_ring),
+ 			  GFP_KERNEL);
+ 	if (!tx_ring)
+ 		goto err_tx_ring;
+ 
+ 	rx_ring = kcalloc(pdata->rx_ring_count, sizeof(struct xgbe_ring),
+ 			  GFP_KERNEL);
+ 	if (!rx_ring)
+ 		goto err_rx_ring;
+ 
+ 	for (i = 0, channel = channel_mem; i < count; i++, channel++) {
+ 		snprintf(channel->name, sizeof(channel->name), "channel-%d", i);
+ 		channel->pdata = pdata;
+ 		channel->queue_index = i;
+ 		channel->dma_regs = pdata->xgmac_regs + DMA_CH_BASE +
+ 				    (DMA_CH_INC * i);
+ 
+ 		if (pdata->per_channel_irq) {
+ 			/* Get the DMA interrupt (offset 1) */
+ 			ret = platform_get_irq(pdata->pdev, i + 1);
+ 			if (ret < 0) {
+ 				netdev_err(pdata->netdev,
+ 					   "platform_get_irq %u failed\n",
+ 					   i + 1);
+ 				goto err_irq;
+ 			}
+ 
+ 			channel->dma_irq = ret;
+ 		}
+ 
+ 		if (i < pdata->tx_ring_count) {
+ 			spin_lock_init(&tx_ring->lock);
+ 			channel->tx_ring = tx_ring++;
+ 		}
+ 
+ 		if (i < pdata->rx_ring_count) {
+ 			spin_lock_init(&rx_ring->lock);
+ 			channel->rx_ring = rx_ring++;
+ 		}
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev,
+ 			  "%s: dma_regs=%p, dma_irq=%d, tx=%p, rx=%p\n",
+ 			  channel->name, channel->dma_regs, channel->dma_irq,
+ 			  channel->tx_ring, channel->rx_ring);
+ 	}
+ 
+ 	pdata->channel = channel_mem;
+ 	pdata->channel_count = count;
+ 
+ 	return 0;
+ 
+ err_irq:
+ 	kfree(rx_ring);
+ 
+ err_rx_ring:
+ 	kfree(tx_ring);
+ 
+ err_tx_ring:
+ 	kfree(channel_mem);
+ 
+ err_channel:
+ 	return ret;
+ }
+ 
+ static void xgbe_free_channels(struct xgbe_prv_data *pdata)
+ {
+ 	if (!pdata->channel)
+ 		return;
+ 
+ 	kfree(pdata->channel->rx_ring);
+ 	kfree(pdata->channel->tx_ring);
+ 	kfree(pdata->channel);
+ 
+ 	pdata->channel = NULL;
+ 	pdata->channel_count = 0;
+ }
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  static inline unsigned int xgbe_tx_avail_desc(struct xgbe_ring *ring)
  {
  	return (ring->rdesc_count - (ring->cur - ring->dirty));
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned int xgbe_rx_dirty_desc(struct xgbe_ring *ring)
+ {
+ 	return (ring->cur - ring->dirty);
+ }
+ 
+ static int xgbe_maybe_stop_tx_queue(struct xgbe_channel *channel,
+ 				    struct xgbe_ring *ring, unsigned int count)
+ {
+ 	struct xgbe_prv_data *pdata = channel->pdata;
+ 
+ 	if (count > xgbe_tx_avail_desc(ring)) {
+ 		netif_info(pdata, drv, pdata->netdev,
+ 			   "Tx queue stopped, not enough descriptors available\n");
+ 		netif_stop_subqueue(pdata->netdev, channel->queue_index);
+ 		ring->tx.queue_stopped = 1;
+ 
+ 		/* If we haven't notified the hardware because of xmit_more
+ 		 * support, tell it now
+ 		 */
+ 		if (ring->tx.xmit_more)
+ 			pdata->hw_if.tx_start_xmit(channel, ring);
+ 
+ 		return NETDEV_TX_BUSY;
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  static int xgbe_calc_rx_buf_size(struct net_device *netdev, unsigned int mtu)
  {
  	unsigned int rx_buf_size;
@@@ -212,11 -332,7 +335,15 @@@ static irqreturn_t xgbe_isr(int irq, vo
  	if (!dma_isr)
  		goto isr_done;
  
++<<<<<<< HEAD
 +	DBGPR("-->xgbe_isr\n");
 +
 +	DBGPR("  DMA_ISR = %08x\n", dma_isr);
 +	DBGPR("  DMA_DS0 = %08x\n", XGMAC_IOREAD(pdata, DMA_DSR0));
 +	DBGPR("  DMA_DS1 = %08x\n", XGMAC_IOREAD(pdata, DMA_DSR1));
++=======
+ 	netif_dbg(pdata, intr, pdata->netdev, "DMA_ISR=%#010x\n", dma_isr);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  	for (i = 0; i < pdata->channel_count; i++) {
  		if (!(dma_isr & (1 << i)))
@@@ -225,10 -341,16 +352,11 @@@
  		channel = pdata->channel + i;
  
  		dma_ch_isr = XGMAC_DMA_IOREAD(channel, DMA_CH_SR);
- 		DBGPR("  DMA_CH%u_ISR = %08x\n", i, dma_ch_isr);
+ 		netif_dbg(pdata, intr, pdata->netdev, "DMA_CH%u_ISR=%#010x\n",
+ 			  i, dma_ch_isr);
  
 -		/* The TI or RI interrupt bits may still be set even if using
 -		 * per channel DMA interrupts. Check to be sure those are not
 -		 * enabled before using the private data napi structure.
 -		 */
 -		if (!pdata->per_channel_irq &&
 -		    (XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, TI) ||
 -		     XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, RI))) {
 +		if (XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, TI) ||
 +		    XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, RI)) {
  			if (napi_schedule_prep(&pdata->napi)) {
  				/* Disable Tx and Rx interrupts */
  				xgbe_disable_rx_tx_ints(pdata);
@@@ -254,13 -376,38 +382,16 @@@
  
  		if (XGMAC_GET_BITS(mac_isr, MAC_ISR, MMCRXIS))
  			hw_if->rx_mmc_int(pdata);
 -
 -		if (XGMAC_GET_BITS(mac_isr, MAC_ISR, TSIS)) {
 -			mac_tssr = XGMAC_IOREAD(pdata, MAC_TSSR);
 -
 -			if (XGMAC_GET_BITS(mac_tssr, MAC_TSSR, TXTSC)) {
 -				/* Read Tx Timestamp to clear interrupt */
 -				pdata->tx_tstamp =
 -					hw_if->get_tx_tstamp(pdata);
 -				schedule_work(&pdata->tx_tstamp_work);
 -			}
 -		}
  	}
  
 -isr_done:
 -	return IRQ_HANDLED;
 -}
 -
 -static irqreturn_t xgbe_dma_isr(int irq, void *data)
 -{
 -	struct xgbe_channel *channel = data;
 -
 -	/* Per channel DMA interrupts are enabled, so we use the per
 -	 * channel napi structure and not the private data napi structure
 -	 */
 -	if (napi_schedule_prep(&channel->napi)) {
 -		/* Disable Tx and Rx interrupts */
 -		disable_irq_nosync(channel->dma_irq);
++<<<<<<< HEAD
 +	DBGPR("  DMA_ISR = %08x\n", XGMAC_IOREAD(pdata, DMA_ISR));
  
 -		/* Turn on polling */
 -		__napi_schedule(&channel->napi);
 -	}
 +	DBGPR("<--xgbe_isr\n");
  
++=======
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
 +isr_done:
  	return IRQ_HANDLED;
  }
  
@@@ -306,10 -449,8 +437,15 @@@ static void xgbe_init_tx_timers(struct 
  		if (!channel->tx_ring)
  			break;
  
++<<<<<<< HEAD
 +		DBGPR("  %s adding tx timer\n", channel->name);
 +		hrtimer_init(&channel->tx_timer, CLOCK_MONOTONIC,
 +			     HRTIMER_MODE_REL);
 +		channel->tx_timer.function = xgbe_tx_timer;
++=======
+ 		setup_timer(&channel->tx_timer, xgbe_tx_timer,
+ 			    (unsigned long)channel);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  	}
  
  	DBGPR("<--xgbe_init_tx_timers\n");
@@@ -327,9 -468,7 +463,13 @@@ static void xgbe_stop_tx_timers(struct 
  		if (!channel->tx_ring)
  			break;
  
++<<<<<<< HEAD
 +		DBGPR("  %s deleting tx timer\n", channel->name);
 +		channel->tx_timer_active = 0;
 +		hrtimer_cancel(&channel->tx_timer);
++=======
+ 		del_timer_sync(&channel->tx_timer);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  	}
  
  	DBGPR("<--xgbe_stop_tx_timers\n");
@@@ -490,7 -755,116 +630,120 @@@ static void xgbe_free_rx_skbuff(struct 
  		}
  	}
  
++<<<<<<< HEAD
 +	DBGPR("<--xgbe_free_rx_skbuff\n");
++=======
+ 	DBGPR("<--xgbe_free_rx_data\n");
+ }
+ 
+ static void xgbe_adjust_link(struct net_device *netdev)
+ {
+ 	struct xgbe_prv_data *pdata = netdev_priv(netdev);
+ 	struct xgbe_hw_if *hw_if = &pdata->hw_if;
+ 	struct phy_device *phydev = pdata->phydev;
+ 	int new_state = 0;
+ 
+ 	if (!phydev)
+ 		return;
+ 
+ 	if (phydev->link) {
+ 		/* Flow control support */
+ 		if (pdata->pause_autoneg) {
+ 			if (phydev->pause || phydev->asym_pause) {
+ 				pdata->tx_pause = 1;
+ 				pdata->rx_pause = 1;
+ 			} else {
+ 				pdata->tx_pause = 0;
+ 				pdata->rx_pause = 0;
+ 			}
+ 		}
+ 
+ 		if (pdata->tx_pause != pdata->phy_tx_pause) {
+ 			hw_if->config_tx_flow_control(pdata);
+ 			pdata->phy_tx_pause = pdata->tx_pause;
+ 		}
+ 
+ 		if (pdata->rx_pause != pdata->phy_rx_pause) {
+ 			hw_if->config_rx_flow_control(pdata);
+ 			pdata->phy_rx_pause = pdata->rx_pause;
+ 		}
+ 
+ 		/* Speed support */
+ 		if (phydev->speed != pdata->phy_speed) {
+ 			new_state = 1;
+ 
+ 			switch (phydev->speed) {
+ 			case SPEED_10000:
+ 				hw_if->set_xgmii_speed(pdata);
+ 				break;
+ 
+ 			case SPEED_2500:
+ 				hw_if->set_gmii_2500_speed(pdata);
+ 				break;
+ 
+ 			case SPEED_1000:
+ 				hw_if->set_gmii_speed(pdata);
+ 				break;
+ 			}
+ 			pdata->phy_speed = phydev->speed;
+ 		}
+ 
+ 		if (phydev->link != pdata->phy_link) {
+ 			new_state = 1;
+ 			pdata->phy_link = 1;
+ 		}
+ 	} else if (pdata->phy_link) {
+ 		new_state = 1;
+ 		pdata->phy_link = 0;
+ 		pdata->phy_speed = SPEED_UNKNOWN;
+ 	}
+ 
+ 	if (new_state)
+ 		phy_print_status(phydev);
+ }
+ 
+ static int xgbe_phy_init(struct xgbe_prv_data *pdata)
+ {
+ 	struct net_device *netdev = pdata->netdev;
+ 	struct phy_device *phydev = pdata->phydev;
+ 	int ret;
+ 
+ 	pdata->phy_link = -1;
+ 	pdata->phy_speed = SPEED_UNKNOWN;
+ 	pdata->phy_tx_pause = pdata->tx_pause;
+ 	pdata->phy_rx_pause = pdata->rx_pause;
+ 
+ 	ret = phy_connect_direct(netdev, phydev, &xgbe_adjust_link,
+ 				 pdata->phy_mode);
+ 	if (ret) {
+ 		netdev_err(netdev, "phy_connect_direct failed\n");
+ 		return ret;
+ 	}
+ 
+ 	if (!phydev->drv || (phydev->drv->phy_id == 0)) {
+ 		netdev_err(netdev, "phy_id not valid\n");
+ 		ret = -ENODEV;
+ 		goto err_phy_connect;
+ 	}
+ 	netif_dbg(pdata, ifup, pdata->netdev,
+ 		  "phy_connect_direct succeeded for PHY %s\n",
+ 		  dev_name(&phydev->dev));
+ 
+ 	return 0;
+ 
+ err_phy_connect:
+ 	phy_disconnect(phydev);
+ 
+ 	return ret;
+ }
+ 
+ static void xgbe_phy_exit(struct xgbe_prv_data *pdata)
+ {
+ 	if (!pdata->phydev)
+ 		return;
+ 
+ 	phy_disconnect(pdata->phydev);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  }
  
  int xgbe_powerdown(struct net_device *netdev, unsigned int caller)
@@@ -864,10 -1477,9 +1117,11 @@@ static int xgbe_xmit(struct sk_buff *sk
  
  	ret = NETDEV_TX_OK;
  
 +	spin_lock_irqsave(&ring->lock, flags);
 +
  	if (skb->len == 0) {
- 		netdev_err(netdev, "empty skb received from stack\n");
+ 		netif_err(pdata, tx_err, netdev,
+ 			  "empty skb received from stack\n");
  		dev_kfree_skb_any(skb);
  		goto tx_netdev_return;
  	}
@@@ -898,18 -1507,23 +1153,17 @@@
  		goto tx_netdev_return;
  	}
  
 -	xgbe_prep_tx_tstamp(pdata, skb, packet);
 -
 -	/* Report on the actual number of bytes (to be) sent */
 -	netdev_tx_sent_queue(txq, packet->tx_bytes);
 -
  	/* Configure required descriptor fields for transmission */
 -	hw_if->dev_xmit(channel);
 +	hw_if->pre_xmit(channel);
  
- #ifdef XGMAC_ENABLE_TX_PKT_DUMP
- 	xgbe_print_pkt(netdev, skb, true);
- #endif
+ 	if (netif_msg_pktdata(pdata))
+ 		xgbe_print_pkt(netdev, skb, true);
  
 -	/* Stop the queue in advance if there may not be enough descriptors */
 -	xgbe_maybe_stop_tx_queue(channel, ring, XGBE_TX_MAX_DESCS);
 +tx_netdev_return:
 +	spin_unlock_irqrestore(&ring->lock, flags);
  
 -	ret = NETDEV_TX_OK;
 +	DBGPR("<--xgbe_xmit\n");
  
 -tx_netdev_return:
  	return ret;
  }
  
@@@ -1029,6 -1695,34 +1283,37 @@@ static void xgbe_poll_controller(struc
  }
  #endif /* End CONFIG_NET_POLL_CONTROLLER */
  
++<<<<<<< HEAD
++=======
+ static int xgbe_setup_tc(struct net_device *netdev, u8 tc)
+ {
+ 	struct xgbe_prv_data *pdata = netdev_priv(netdev);
+ 	unsigned int offset, queue;
+ 	u8 i;
+ 
+ 	if (tc && (tc != pdata->hw_feat.tc_cnt))
+ 		return -EINVAL;
+ 
+ 	if (tc) {
+ 		netdev_set_num_tc(netdev, tc);
+ 		for (i = 0, queue = 0, offset = 0; i < tc; i++) {
+ 			while ((queue < pdata->tx_q_count) &&
+ 			       (pdata->q2tc_map[queue] == i))
+ 				queue++;
+ 
+ 			netif_dbg(pdata, drv, netdev, "TC%u using TXq%u-%u\n",
+ 				  i, offset, queue - 1);
+ 			netdev_set_tc_queue(netdev, i, queue - offset, offset);
+ 			offset = queue;
+ 		}
+ 	} else {
+ 		netdev_reset_tc(netdev);
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  static int xgbe_set_features(struct net_device *netdev,
  			     netdev_features_t features)
  {
@@@ -1128,14 -1877,18 +1413,13 @@@ static int xgbe_tx_poll(struct xgbe_cha
  
  		/* Make sure descriptor fields are read after reading the OWN
  		 * bit */
 -		dma_rmb();
 +		rmb();
  
- #ifdef XGMAC_ENABLE_TX_DESC_DUMP
- 		xgbe_dump_tx_desc(ring, ring->dirty, 1, 0);
- #endif
+ 		if (netif_msg_tx_done(pdata))
+ 			xgbe_dump_tx_desc(pdata, ring, ring->dirty, 1, 0);
  
 -		if (hw_if->is_last_desc(rdesc)) {
 -			tx_packets += rdata->tx.packets;
 -			tx_bytes += rdata->tx.bytes;
 -		}
 -
  		/* Free the SKB and reset the descriptor for re-use */
 -		desc_if->unmap_rdata(pdata, rdata);
 +		desc_if->unmap_skb(pdata, rdata);
  		hw_if->tx_desc_reset(rdata);
  
  		processed++;
@@@ -1211,35 -1984,50 +1495,36 @@@ read_again
  
  		if (error || packet->errors) {
  			if (packet->errors)
- 				DBGPR("Error in received packet\n");
+ 				netif_err(pdata, rx_err, netdev,
+ 					  "error in received packet\n");
  			dev_kfree_skb(skb);
 -			goto next_packet;
 +			continue;
  		}
  
 -		if (!context) {
 -			put_len = rdata->rx.len - len;
 -			len += put_len;
 -
 -			if (!skb) {
 -				dma_sync_single_for_cpu(pdata->dev,
 -							rdata->rx.hdr.dma,
 -							rdata->rx.hdr.dma_len,
 -							DMA_FROM_DEVICE);
 -
 -				skb = xgbe_create_skb(napi, rdata, &put_len);
 -				if (!skb) {
 +		put_len = rdata->len - cur_len;
 +		if (skb) {
 +			if (pskb_expand_head(skb, 0, put_len, GFP_ATOMIC)) {
 +				DBGPR("pskb_expand_head error\n");
 +				if (incomplete) {
  					error = 1;
 -					goto skip_data;
 +					goto read_again;
  				}
 -			}
  
 -			if (put_len) {
 -				dma_sync_single_for_cpu(pdata->dev,
 -							rdata->rx.buf.dma,
 -							rdata->rx.buf.dma_len,
 -							DMA_FROM_DEVICE);
 -
 -				skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
 -						rdata->rx.buf.pa.pages,
 -						rdata->rx.buf.pa.pages_offset,
 -						put_len, rdata->rx.buf.dma_len);
 -				rdata->rx.buf.pa.pages = NULL;
 +				dev_kfree_skb(skb);
 +				continue;
  			}
 +			memcpy(skb_tail_pointer(skb), rdata->skb->data,
 +			       put_len);
 +		} else {
 +			skb = rdata->skb;
 +			rdata->skb = NULL;
  		}
 +		skb_put(skb, put_len);
 +		cur_len += put_len;
  
 -skip_data:
 -		if (incomplete || context_next)
 +		if (incomplete)
  			goto read_again;
  
 -		if (!skb)
 -			goto next_packet;
 -
  		/* Be sure we don't exceed the configured MTU */
  		max_len = netdev->mtu + ETH_HLEN;
  		if (!(netdev->features & NETIF_F_HW_VLAN_CTAG_RX) &&
@@@ -1247,14 -2035,14 +1532,14 @@@
  			max_len += VLAN_HLEN;
  
  		if (skb->len > max_len) {
- 			DBGPR("packet length exceeds configured MTU\n");
+ 			netif_err(pdata, rx_err, netdev,
+ 				  "packet length exceeds configured MTU\n");
  			dev_kfree_skb(skb);
 -			goto next_packet;
 +			continue;
  		}
  
- #ifdef XGMAC_ENABLE_RX_PKT_DUMP
- 		xgbe_print_pkt(netdev, skb, false);
- #endif
+ 		if (netif_msg_pktdata(pdata))
+ 			xgbe_print_pkt(netdev, skb, false);
  
  		skb_checksum_none_assert(skb);
  		if (XGMAC_GET_BITS(packet->attributes,
@@@ -1331,20 -2175,29 +1616,42 @@@ void xgbe_dump_tx_desc(struct xgbe_prv_
  	while (count--) {
  		rdata = XGBE_GET_DESC_DATA(ring, idx);
  		rdesc = rdata->rdesc;
++<<<<<<< HEAD
 +		DBGPR("TX_NORMAL_DESC[%d %s] = %08x:%08x:%08x:%08x\n", idx,
 +		      (flag == 1) ? "QUEUED FOR TX" : "TX BY DEVICE",
 +		      le32_to_cpu(rdesc->desc0), le32_to_cpu(rdesc->desc1),
 +		      le32_to_cpu(rdesc->desc2), le32_to_cpu(rdesc->desc3));
++=======
+ 		netdev_dbg(pdata->netdev,
+ 			   "TX_NORMAL_DESC[%d %s] = %08x:%08x:%08x:%08x\n", idx,
+ 			   (flag == 1) ? "QUEUED FOR TX" : "TX BY DEVICE",
+ 			   le32_to_cpu(rdesc->desc0),
+ 			   le32_to_cpu(rdesc->desc1),
+ 			   le32_to_cpu(rdesc->desc2),
+ 			   le32_to_cpu(rdesc->desc3));
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  		idx++;
  	}
  }
  
- void xgbe_dump_rx_desc(struct xgbe_ring *ring, struct xgbe_ring_desc *desc,
+ void xgbe_dump_rx_desc(struct xgbe_prv_data *pdata, struct xgbe_ring *ring,
  		       unsigned int idx)
  {
++<<<<<<< HEAD
 +	DBGPR("RX_NORMAL_DESC[%d RX BY DEVICE] = %08x:%08x:%08x:%08x\n", idx,
 +	      le32_to_cpu(desc->desc0), le32_to_cpu(desc->desc1),
 +	      le32_to_cpu(desc->desc2), le32_to_cpu(desc->desc3));
++=======
+ 	struct xgbe_ring_data *rdata;
+ 	struct xgbe_ring_desc *rdesc;
+ 
+ 	rdata = XGBE_GET_DESC_DATA(ring, idx);
+ 	rdesc = rdata->rdesc;
+ 	netdev_dbg(pdata->netdev,
+ 		   "RX_NORMAL_DESC[%d RX BY DEVICE] = %08x:%08x:%08x:%08x\n",
+ 		   idx, le32_to_cpu(rdesc->desc0), le32_to_cpu(rdesc->desc1),
+ 		   le32_to_cpu(rdesc->desc2), le32_to_cpu(rdesc->desc3));
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  }
  
  void xgbe_print_pkt(struct net_device *netdev, struct sk_buff *skb, bool tx_rx)
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-main.c
index 51cdca78ec38,ae869d41cec8..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-main.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-main.c
@@@ -134,59 -136,12 +134,68 @@@ MODULE_LICENSE("Dual BSD/GPL")
  MODULE_VERSION(XGBE_DRV_VERSION);
  MODULE_DESCRIPTION(XGBE_DRV_DESC);
  
++<<<<<<< HEAD
 +static struct xgbe_channel *xgbe_alloc_rings(struct xgbe_prv_data *pdata)
 +{
 +	struct xgbe_channel *channel_mem, *channel;
 +	struct xgbe_ring *tx_ring, *rx_ring;
 +	unsigned int count, i;
 +
 +	DBGPR("-->xgbe_alloc_rings\n");
 +
 +	count = max_t(unsigned int, pdata->tx_ring_count, pdata->rx_ring_count);
 +
 +	channel_mem = devm_kcalloc(pdata->dev, count,
 +				   sizeof(struct xgbe_channel), GFP_KERNEL);
 +	if (!channel_mem)
 +		return NULL;
 +
 +	tx_ring = devm_kcalloc(pdata->dev, pdata->tx_ring_count,
 +			       sizeof(struct xgbe_ring), GFP_KERNEL);
 +	if (!tx_ring)
 +		return NULL;
 +
 +	rx_ring = devm_kcalloc(pdata->dev, pdata->rx_ring_count,
 +			       sizeof(struct xgbe_ring), GFP_KERNEL);
 +	if (!rx_ring)
 +		return NULL;
 +
 +	for (i = 0, channel = channel_mem; i < count; i++, channel++) {
 +		snprintf(channel->name, sizeof(channel->name), "channel-%d", i);
 +		channel->pdata = pdata;
 +		channel->queue_index = i;
 +		channel->dma_regs = pdata->xgmac_regs + DMA_CH_BASE +
 +				    (DMA_CH_INC * i);
 +
 +		if (i < pdata->tx_ring_count) {
 +			spin_lock_init(&tx_ring->lock);
 +			channel->tx_ring = tx_ring++;
 +		}
 +
 +		if (i < pdata->rx_ring_count) {
 +			spin_lock_init(&rx_ring->lock);
 +			channel->rx_ring = rx_ring++;
 +		}
 +
 +		DBGPR("  %s - queue_index=%u, dma_regs=%p, tx=%p, rx=%p\n",
 +		      channel->name, channel->queue_index, channel->dma_regs,
 +		      channel->tx_ring, channel->rx_ring);
 +	}
 +
 +	pdata->channel_count = count;
 +
 +	DBGPR("<--xgbe_alloc_rings\n");
 +
 +	return channel_mem;
 +}
++=======
+ static int debug = -1;
+ module_param(debug, int, S_IWUSR | S_IRUGO);
+ MODULE_PARM_DESC(debug, " Network interface message level setting");
+ 
+ static const u32 default_msg_level = (NETIF_MSG_LINK | NETIF_MSG_IFDOWN |
+ 				      NETIF_MSG_IFUP);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  static void xgbe_default_config(struct xgbe_prv_data *pdata)
  {
@@@ -246,6 -293,13 +255,16 @@@ static int xgbe_probe(struct platform_d
  
  	spin_lock_init(&pdata->lock);
  	mutex_init(&pdata->xpcs_mutex);
++<<<<<<< HEAD
++=======
+ 	mutex_init(&pdata->rss_mutex);
+ 	spin_lock_init(&pdata->tstamp_lock);
+ 
+ 	pdata->msg_enable = netif_msg_init(debug, default_msg_level);
+ 
+ 	/* Check if we should use ACPI or DT */
+ 	pdata->use_acpi = (!pdata->adev || acpi_disabled) ? 0 : 1;
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  	/* Set and validate the number of descriptors for a ring */
  	BUILD_BUG_ON_NOT_POWER_OF_2(XGBE_TX_DESC_CNT);
@@@ -290,18 -337,45 +310,19 @@@
  		ret = PTR_ERR(pdata->xpcs_regs);
  		goto err_io;
  	}
- 	DBGPR("  xpcs_regs  = %p\n", pdata->xpcs_regs);
+ 	if (netif_msg_probe(pdata))
+ 		dev_dbg(dev, "xpcs_regs  = %p\n", pdata->xpcs_regs);
  
 -	/* Retrieve the MAC address */
 -	ret = device_property_read_u8_array(dev, XGBE_MAC_ADDR_PROPERTY,
 -					    pdata->mac_addr,
 -					    sizeof(pdata->mac_addr));
 -	if (ret || !is_valid_ether_addr(pdata->mac_addr)) {
 -		dev_err(dev, "invalid %s property\n", XGBE_MAC_ADDR_PROPERTY);
 -		if (!ret)
 -			ret = -EINVAL;
 -		goto err_io;
 -	}
 -
 -	/* Retrieve the PHY mode - it must be "xgmii" */
 -	ret = device_property_read_string(dev, XGBE_PHY_MODE_PROPERTY,
 -					  &phy_mode);
 -	if (ret || strcmp(phy_mode, phy_modes(PHY_INTERFACE_MODE_XGMII))) {
 -		dev_err(dev, "invalid %s property\n", XGBE_PHY_MODE_PROPERTY);
 -		if (!ret)
 -			ret = -EINVAL;
 +	/* Set the DMA mask */
 +	if (!dev->dma_mask)
 +		dev->dma_mask = &dev->coherent_dma_mask;
 +	ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(40));
 +	if (ret) {
 +		dev_err(dev, "dma_set_mask_and_coherent failed\n");
  		goto err_io;
  	}
 -	pdata->phy_mode = PHY_INTERFACE_MODE_XGMII;
 -
 -	/* Check for per channel interrupt support */
 -	if (device_property_present(dev, XGBE_DMA_IRQS_PROPERTY))
 -		pdata->per_channel_irq = 1;
 -
 -	/* Obtain device settings unique to ACPI/OF */
 -	if (pdata->use_acpi)
 -		ret = xgbe_acpi_support(pdata);
 -	else
 -		ret = xgbe_of_support(pdata);
 -	if (ret)
 -		goto err_io;
  
 -	/* Set the DMA coherency values */
 -	if (pdata->coherent) {
 +	if (of_property_read_bool(dev->of_node, "dma-coherent")) {
  		pdata->axdomain = XGBE_DMA_OS_AXDOMAIN;
  		pdata->arcache = XGBE_DMA_OS_ARCACHE;
  		pdata->awcache = XGBE_DMA_OS_AWCACHE;
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-mdio.c
index 8514b5841ecd,532a67f728e0..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-mdio.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-mdio.c
@@@ -382,9 -277,8 +384,14 @@@ int xgbe_mdio_register(struct xgbe_prv_
  
  	pdata->phydev = phydev;
  
++<<<<<<< HEAD
 +	of_node_put(phy_node);
 +
 +	DBGPHY_REGS(pdata);
++=======
+ 	if (netif_msg_drv(pdata))
+ 		xgbe_dump_phy_registers(pdata);
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  
  	DBGPR("<--xgbe_mdio_register\n");
  
diff --cc drivers/net/ethernet/amd/xgbe/xgbe.h
index 1903f878545a,8313b0761b8f..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe.h
+++ b/drivers/net/ethernet/amd/xgbe/xgbe.h
@@@ -631,9 -819,11 +634,17 @@@ struct ethtool_ops *xgbe_get_ethtool_op
  int xgbe_mdio_register(struct xgbe_prv_data *);
  void xgbe_mdio_unregister(struct xgbe_prv_data *);
  void xgbe_dump_phy_registers(struct xgbe_prv_data *);
++<<<<<<< HEAD
 +void xgbe_dump_tx_desc(struct xgbe_ring *, unsigned int, unsigned int,
 +		       unsigned int);
 +void xgbe_dump_rx_desc(struct xgbe_ring *, struct xgbe_ring_desc *,
++=======
+ void xgbe_ptp_register(struct xgbe_prv_data *);
+ void xgbe_ptp_unregister(struct xgbe_prv_data *);
+ void xgbe_dump_tx_desc(struct xgbe_prv_data *, struct xgbe_ring *,
+ 		       unsigned int, unsigned int, unsigned int);
+ void xgbe_dump_rx_desc(struct xgbe_prv_data *, struct xgbe_ring *,
++>>>>>>> 34bf65dfa343 (amd-xgbe: Add netif_* message support to the driver)
  		       unsigned int);
  void xgbe_print_pkt(struct net_device *, struct sk_buff *, bool);
  void xgbe_get_all_hw_features(struct xgbe_prv_data *);
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dcb.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dcb.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-desc.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dev.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-drv.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-main.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-mdio.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe.h
