dax: Define DAX lock bit for radix tree exceptional entry

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jan Kara <jack@suse.cz>
commit e804315dd0f574b56155c5a2406ab5e0318104f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e804315d.failed

We will use lowest available bit in the radix tree exceptional entry for
locking of the entry. Define it. Also clean up definitions of DAX entry
type bits in DAX exceptional entries to use defined constants instead of
hardcoding numbers and cleanup checking of these bits to not rely on how
other bits in the entry are set.

	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Signed-off-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
(cherry picked from commit e804315dd0f574b56155c5a2406ab5e0318104f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
#	include/linux/dax.h
diff --cc fs/dax.c
index 3ad95e9ec809,351afd3cf8be..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -33,6 -32,20 +33,23 @@@
  #include <linux/pfn_t.h>
  #include <linux/sizes.h>
  
++<<<<<<< HEAD
++=======
+ /*
+  * We use lowest available bit in exceptional entry for locking, other two
+  * bits to determine entry type. In total 3 special bits.
+  */
+ #define RADIX_DAX_SHIFT	(RADIX_TREE_EXCEPTIONAL_SHIFT + 3)
+ #define RADIX_DAX_PTE (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 1))
+ #define RADIX_DAX_PMD (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 2))
+ #define RADIX_DAX_TYPE_MASK (RADIX_DAX_PTE | RADIX_DAX_PMD)
+ #define RADIX_DAX_TYPE(entry) ((unsigned long)entry & RADIX_DAX_TYPE_MASK)
+ #define RADIX_DAX_SECTOR(entry) (((unsigned long)entry >> RADIX_DAX_SHIFT))
+ #define RADIX_DAX_ENTRY(sector, pmd) ((void *)((unsigned long)sector << \
+ 		RADIX_DAX_SHIFT | (pmd ? RADIX_DAX_PMD : RADIX_DAX_PTE) | \
+ 		RADIX_TREE_EXCEPTIONAL_ENTRY))
+ 
++>>>>>>> e804315dd0f5 (dax: Define DAX lock bit for radix tree exceptional entry)
  static long dax_map_atomic(struct block_device *bdev, struct blk_dax_ctl *dax)
  {
  	struct request_queue *q = bdev->bd_queue;
diff --cc include/linux/dax.h
index bbe07d8b9dee,70600b63083f..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -5,16 -5,15 +5,24 @@@
  #include <linux/mm.h>
  #include <asm/pgtable.h>
  
++<<<<<<< HEAD
 +ssize_t dax_do_io(int rw, struct kiocb *iocb, struct inode *inode,
 +                  const struct iovec *iov, loff_t pos, unsigned long nr_segs,
 +                  get_block_t get_block, dio_iodone_t end_io, int flags);
 +int dax_clear_sectors(struct block_device *bdev, sector_t _sector, long _size);
++=======
+ /* We use lowest available exceptional entry bit for locking */
+ #define RADIX_DAX_ENTRY_LOCK (1 << RADIX_TREE_EXCEPTIONAL_SHIFT)
+ 
+ ssize_t dax_do_io(struct kiocb *, struct inode *, struct iov_iter *, loff_t,
+ 		  get_block_t, dio_iodone_t, int flags);
++>>>>>>> e804315dd0f5 (dax: Define DAX lock bit for radix tree exceptional entry)
  int dax_zero_page_range(struct inode *, loff_t from, unsigned len, get_block_t);
  int dax_truncate_page(struct inode *, loff_t from, get_block_t);
 -int dax_fault(struct vm_area_struct *, struct vm_fault *, get_block_t);
 -int __dax_fault(struct vm_area_struct *, struct vm_fault *, get_block_t);
 +int dax_fault(struct vm_area_struct *, struct vm_fault *, get_block_t,
 +		dax_iodone_t);
 +int __dax_fault(struct vm_area_struct *, struct vm_fault *, get_block_t,
 +		dax_iodone_t);
  
  #ifdef CONFIG_FS_DAX
  struct page *read_dax_sector(struct block_device *bdev, sector_t n);
* Unmerged path fs/dax.c
* Unmerged path include/linux/dax.h
