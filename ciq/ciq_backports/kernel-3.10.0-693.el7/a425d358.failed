userfaultfd: shmem: avoid a lockup resulting from corrupted page->flags

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Andrea Arcangeli <aarcange@redhat.com>
commit a425d3584e7e69587aa441e91c7ffce7f47004d7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/a425d358.failed

Use the non atomic version of __SetPageUptodate while the page is still
private and not visible to lookup operations.  Using the non atomic
version after the page is already visible to lookups is unsafe as there
would be concurrent lock_page operation modifying the page->flags while
it runs.

This solves a lockup in find_lock_entry with the userfaultfd_shmem
selftest.

  userfaultfd_shm D14296   691      1 0x00000004
  Call Trace:
   schedule+0x3d/0x90
   schedule_timeout+0x228/0x420
   io_schedule_timeout+0xa4/0x110
   __lock_page+0x12d/0x170
   find_lock_entry+0xa4/0x190
   shmem_getpage_gfp+0xb9/0xc30
   shmem_fault+0x70/0x1c0
   __do_fault+0x21/0x150
   handle_mm_fault+0xec9/0x1490
   __do_page_fault+0x20d/0x520
   trace_do_page_fault+0x61/0x270
   do_async_page_fault+0x19/0x80
   async_page_fault+0x25/0x30

Link: http://lkml.kernel.org/r/20170116180408.12184-2-aarcange@redhat.com
	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Reported-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
	Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a425d3584e7e69587aa441e91c7ffce7f47004d7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/shmem.c
diff --cc mm/shmem.c
index 021ba8dffccb,8d7d80cf8708..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -1514,7 -2191,120 +1514,124 @@@ static struct inode *shmem_get_inode(st
  
  bool shmem_mapping(struct address_space *mapping)
  {
++<<<<<<< HEAD
 +	return mapping->backing_dev_info == &shmem_backing_dev_info;
++=======
+ 	return mapping->a_ops == &shmem_aops;
+ }
+ 
+ int shmem_mcopy_atomic_pte(struct mm_struct *dst_mm,
+ 			   pmd_t *dst_pmd,
+ 			   struct vm_area_struct *dst_vma,
+ 			   unsigned long dst_addr,
+ 			   unsigned long src_addr,
+ 			   struct page **pagep)
+ {
+ 	struct inode *inode = file_inode(dst_vma->vm_file);
+ 	struct shmem_inode_info *info = SHMEM_I(inode);
+ 	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
+ 	struct address_space *mapping = inode->i_mapping;
+ 	gfp_t gfp = mapping_gfp_mask(mapping);
+ 	pgoff_t pgoff = linear_page_index(dst_vma, dst_addr);
+ 	struct mem_cgroup *memcg;
+ 	spinlock_t *ptl;
+ 	void *page_kaddr;
+ 	struct page *page;
+ 	pte_t _dst_pte, *dst_pte;
+ 	int ret;
+ 
+ 	if (!*pagep) {
+ 		ret = -ENOMEM;
+ 		if (shmem_acct_block(info->flags, 1))
+ 			goto out;
+ 		if (sbinfo->max_blocks) {
+ 			if (percpu_counter_compare(&sbinfo->used_blocks,
+ 						   sbinfo->max_blocks) >= 0)
+ 				goto out_unacct_blocks;
+ 			percpu_counter_inc(&sbinfo->used_blocks);
+ 		}
+ 
+ 		page = shmem_alloc_page(gfp, info, pgoff);
+ 		if (!page)
+ 			goto out_dec_used_blocks;
+ 
+ 		page_kaddr = kmap_atomic(page);
+ 		ret = copy_from_user(page_kaddr, (const void __user *)src_addr,
+ 				     PAGE_SIZE);
+ 		kunmap_atomic(page_kaddr);
+ 
+ 		/* fallback to copy_from_user outside mmap_sem */
+ 		if (unlikely(ret)) {
+ 			*pagep = page;
+ 			/* don't free the page */
+ 			return -EFAULT;
+ 		}
+ 	} else {
+ 		page = *pagep;
+ 		*pagep = NULL;
+ 	}
+ 
+ 	VM_BUG_ON(PageLocked(page) || PageSwapBacked(page));
+ 	__SetPageLocked(page);
+ 	__SetPageSwapBacked(page);
+ 	__SetPageUptodate(page);
+ 
+ 	ret = mem_cgroup_try_charge(page, dst_mm, gfp, &memcg, false);
+ 	if (ret)
+ 		goto out_release;
+ 
+ 	ret = radix_tree_maybe_preload(gfp & GFP_RECLAIM_MASK);
+ 	if (!ret) {
+ 		ret = shmem_add_to_page_cache(page, mapping, pgoff, NULL);
+ 		radix_tree_preload_end();
+ 	}
+ 	if (ret)
+ 		goto out_release_uncharge;
+ 
+ 	mem_cgroup_commit_charge(page, memcg, false, false);
+ 
+ 	_dst_pte = mk_pte(page, dst_vma->vm_page_prot);
+ 	if (dst_vma->vm_flags & VM_WRITE)
+ 		_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));
+ 
+ 	ret = -EEXIST;
+ 	dst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);
+ 	if (!pte_none(*dst_pte))
+ 		goto out_release_uncharge_unlock;
+ 
+ 	lru_cache_add_anon(page);
+ 
+ 	spin_lock(&info->lock);
+ 	info->alloced++;
+ 	inode->i_blocks += BLOCKS_PER_PAGE;
+ 	shmem_recalc_inode(inode);
+ 	spin_unlock(&info->lock);
+ 
+ 	inc_mm_counter(dst_mm, mm_counter_file(page));
+ 	page_add_file_rmap(page, false);
+ 	set_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);
+ 
+ 	/* No need to invalidate - it was non-present before */
+ 	update_mmu_cache(dst_vma, dst_addr, dst_pte);
+ 	unlock_page(page);
+ 	pte_unmap_unlock(dst_pte, ptl);
+ 	ret = 0;
+ out:
+ 	return ret;
+ out_release_uncharge_unlock:
+ 	pte_unmap_unlock(dst_pte, ptl);
+ out_release_uncharge:
+ 	mem_cgroup_cancel_charge(page, memcg, false);
+ out_release:
+ 	unlock_page(page);
+ 	put_page(page);
+ out_dec_used_blocks:
+ 	if (sbinfo->max_blocks)
+ 		percpu_counter_add(&sbinfo->used_blocks, -1);
+ out_unacct_blocks:
+ 	shmem_unacct_blocks(info->flags, 1);
+ 	goto out;
++>>>>>>> a425d3584e7e (userfaultfd: shmem: avoid a lockup resulting from corrupted page->flags)
  }
  
  #ifdef CONFIG_TMPFS
* Unmerged path mm/shmem.c
