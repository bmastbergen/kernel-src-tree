rds: tcp: block BH in TCP callbacks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 38036629cded6b96a9f9689758a88d067c4d4d44
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/38036629.failed

TCP stack can now run from process context.

Use read_lock_bh(&sk->sk_callback_lock) variant to restore previous
assumption.

Fixes: 5413d1babe8f ("net: do not block BH while processing socket backlog")
Fixes: d41a69f1d390 ("tcp: make tcp_sendmsg() aware of socket backlog")
	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 38036629cded6b96a9f9689758a88d067c4d4d44)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/rds/tcp_listen.c
#	net/rds/tcp_recv.c
diff --cc net/rds/tcp_listen.c
index d2a6dd7eff81,3fa367945105..000000000000
--- a/net/rds/tcp_listen.c
+++ b/net/rds/tcp_listen.c
@@@ -184,8 -183,8 +184,13 @@@ void rds_tcp_listen_data_ready(struct s
  		rds_tcp_accept_work(sk);
  
  out:
++<<<<<<< HEAD
 +	read_unlock(&sk->sk_callback_lock);
 +	ready(sk, bytes);
++=======
+ 	read_unlock_bh(&sk->sk_callback_lock);
+ 	ready(sk);
++>>>>>>> 38036629cded (rds: tcp: block BH in TCP callbacks)
  }
  
  struct socket *rds_tcp_listen_init(struct net *net)
diff --cc net/rds/tcp_recv.c
index 288a3c443bdf,c3196f9d070a..000000000000
--- a/net/rds/tcp_recv.c
+++ b/net/rds/tcp_recv.c
@@@ -319,9 -299,9 +319,9 @@@ void rds_tcp_data_ready(struct sock *sk
  	struct rds_connection *conn;
  	struct rds_tcp_connection *tc;
  
 -	rdsdebug("data ready sk %p\n", sk);
 +	rdsdebug("data ready sk %p bytes %d\n", sk, bytes);
  
- 	read_lock(&sk->sk_callback_lock);
+ 	read_lock_bh(&sk->sk_callback_lock);
  	conn = sk->sk_user_data;
  	if (!conn) { /* check for teardown race */
  		ready = sk->sk_data_ready;
@@@ -335,8 -315,8 +335,13 @@@
  	if (rds_tcp_read_sock(conn, GFP_ATOMIC) == -ENOMEM)
  		queue_delayed_work(rds_wq, &conn->c_recv_w, 0);
  out:
++<<<<<<< HEAD
 +	read_unlock(&sk->sk_callback_lock);
 +	ready(sk, bytes);
++=======
+ 	read_unlock_bh(&sk->sk_callback_lock);
+ 	ready(sk);
++>>>>>>> 38036629cded (rds: tcp: block BH in TCP callbacks)
  }
  
  int rds_tcp_recv_init(void)
diff --git a/net/rds/tcp_connect.c b/net/rds/tcp_connect.c
index 49a3fcfed360..fb82e0a0bf89 100644
--- a/net/rds/tcp_connect.c
+++ b/net/rds/tcp_connect.c
@@ -43,7 +43,7 @@ void rds_tcp_state_change(struct sock *sk)
 	struct rds_connection *conn;
 	struct rds_tcp_connection *tc;
 
-	read_lock(&sk->sk_callback_lock);
+	read_lock_bh(&sk->sk_callback_lock);
 	conn = sk->sk_user_data;
 	if (!conn) {
 		state_change = sk->sk_state_change;
@@ -69,7 +69,7 @@ void rds_tcp_state_change(struct sock *sk)
 			break;
 	}
 out:
-	read_unlock(&sk->sk_callback_lock);
+	read_unlock_bh(&sk->sk_callback_lock);
 	state_change(sk);
 }
 
* Unmerged path net/rds/tcp_listen.c
* Unmerged path net/rds/tcp_recv.c
diff --git a/net/rds/tcp_send.c b/net/rds/tcp_send.c
index 4be8e6261b9f..a831a8aec111 100644
--- a/net/rds/tcp_send.c
+++ b/net/rds/tcp_send.c
@@ -180,7 +180,7 @@ void rds_tcp_write_space(struct sock *sk)
 	struct rds_connection *conn;
 	struct rds_tcp_connection *tc;
 
-	read_lock(&sk->sk_callback_lock);
+	read_lock_bh(&sk->sk_callback_lock);
 	conn = sk->sk_user_data;
 	if (!conn) {
 		write_space = sk->sk_write_space;
@@ -200,7 +200,7 @@ void rds_tcp_write_space(struct sock *sk)
 		queue_delayed_work(rds_wq, &conn->c_send_w, 0);
 
 out:
-	read_unlock(&sk->sk_callback_lock);
+	read_unlock_bh(&sk->sk_callback_lock);
 
 	/*
 	 * write_space is only called when data leaves tcp's send queue if
