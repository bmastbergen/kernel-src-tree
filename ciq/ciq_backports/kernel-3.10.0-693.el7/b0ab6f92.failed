net: sched: enable per cpu qstats

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [net] sched: enable per cpu qstats (Ivan Vecera) [1428588]
Rebuild_FUZZ: 91.80%
commit-author John Fastabend <john.fastabend@gmail.com>
commit b0ab6f92752b9f9d8da980506e9df3bd9dcd7ed3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/b0ab6f92.failed

After previous patches to simplify qstats the qstats can be
made per cpu with a packed union in Qdisc struct.

	Signed-off-by: John Fastabend <john.r.fastabend@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit b0ab6f92752b9f9d8da980506e9df3bd9dcd7ed3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/gen_stats.h
#	net/core/gen_stats.c
#	net/sched/act_api.c
#	net/sched/sch_api.c
#	net/sched/sch_atm.c
#	net/sched/sch_cbq.c
#	net/sched/sch_drr.c
#	net/sched/sch_fq_codel.c
#	net/sched/sch_hfsc.c
#	net/sched/sch_htb.c
#	net/sched/sch_mq.c
#	net/sched/sch_mqprio.c
#	net/sched/sch_multiq.c
#	net/sched/sch_prio.c
#	net/sched/sch_qfq.c
#	net/sched/sch_sfq.c
diff --cc include/net/gen_stats.h
index ea4271dceff0,cbafa3768d48..000000000000
--- a/include/net/gen_stats.h
+++ b/include/net/gen_stats.h
@@@ -31,7 -37,12 +31,13 @@@ int gnet_stats_copy_basic(struct gnet_d
  int gnet_stats_copy_rate_est(struct gnet_dump *d,
  			     const struct gnet_stats_basic_packed *b,
  			     struct gnet_stats_rate_est64 *r);
++<<<<<<< HEAD
 +int gnet_stats_copy_queue(struct gnet_dump *d, struct gnet_stats_queue *q);
++=======
+ int gnet_stats_copy_queue(struct gnet_dump *d,
+ 			  struct gnet_stats_queue __percpu *cpu_q,
+ 			  struct gnet_stats_queue *q, __u32 qlen);
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  int gnet_stats_copy_app(struct gnet_dump *d, void *st, int len);
  
  int gnet_stats_finish_copy(struct gnet_dump *d);
diff --cc net/core/gen_stats.c
index 9d3d9e78397b,14681b97a4f3..000000000000
--- a/net/core/gen_stats.c
+++ b/net/core/gen_stats.c
@@@ -175,22 -253,31 +210,35 @@@ static void __gnet_stats_copy_queue(str
  /**
   * gnet_stats_copy_queue - copy queue statistics into statistics TLV
   * @d: dumping handle
+  * @cpu_q: per cpu queue statistics
   * @q: queue statistics
 - * @qlen: queue length statistics
   *
   * Appends the queue statistics to the top level TLV created by
-  * gnet_stats_start_copy().
+  * gnet_stats_start_copy(). Using per cpu queue statistics if
+  * they are available.
   *
   * Returns 0 on success or -1 with the statistic lock released
   * if the room in the socket buffer was not sufficient.
   */
  int
++<<<<<<< HEAD
 +gnet_stats_copy_queue(struct gnet_dump *d, struct gnet_stats_queue *q)
 +{
++=======
+ gnet_stats_copy_queue(struct gnet_dump *d,
+ 		      struct gnet_stats_queue __percpu *cpu_q,
+ 		      struct gnet_stats_queue *q, __u32 qlen)
+ {
+ 	struct gnet_stats_queue qstats = {0};
+ 
+ 	__gnet_stats_copy_queue(&qstats, cpu_q, q, qlen);
+ 
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  	if (d->compat_tc_stats) {
- 		d->tc_stats.drops = q->drops;
- 		d->tc_stats.qlen = q->qlen;
- 		d->tc_stats.backlog = q->backlog;
- 		d->tc_stats.overlimits = q->overlimits;
+ 		d->tc_stats.drops = qstats.drops;
+ 		d->tc_stats.qlen = qstats.qlen;
+ 		d->tc_stats.backlog = qstats.backlog;
+ 		d->tc_stats.overlimits = qstats.overlimits;
  	}
  
  	if (d->tail)
diff --cc net/sched/act_api.c
index 71f2bdf1efd8,3d43e4979f27..000000000000
--- a/net/sched/act_api.c
+++ b/net/sched/act_api.c
@@@ -585,10 -620,12 +585,16 @@@ int tcf_action_copy_stats(struct sk_buf
  	if (err < 0)
  		goto errout;
  
 -	if (gnet_stats_copy_basic(&d, NULL, &p->tcfc_bstats) < 0 ||
 +	if (gnet_stats_copy_basic(&d, &p->tcfc_bstats) < 0 ||
  	    gnet_stats_copy_rate_est(&d, &p->tcfc_bstats,
  				     &p->tcfc_rate_est) < 0 ||
++<<<<<<< HEAD
 +	    gnet_stats_copy_queue(&d, &p->tcfc_qstats) < 0)
++=======
+ 	    gnet_stats_copy_queue(&d, NULL,
+ 				  &p->tcfc_qstats,
+ 				  p->tcfc_qstats.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		goto errout;
  
  	if (gnet_stats_finish_copy(&d) < 0)
diff --cc net/sched/sch_api.c
index 41f2533fbfbd,aa8329508dba..000000000000
--- a/net/sched/sch_api.c
+++ b/net/sched/sch_api.c
@@@ -945,6 -942,17 +945,20 @@@ qdisc_create(struct net_device *dev, st
  	sch->handle = handle;
  
  	if (!ops->init || (err = ops->init(sch, tca[TCA_OPTIONS])) == 0) {
++<<<<<<< HEAD
++=======
+ 		if (qdisc_is_percpu_stats(sch)) {
+ 			sch->cpu_bstats =
+ 				alloc_percpu(struct gnet_stats_basic_cpu);
+ 			if (!sch->cpu_bstats)
+ 				goto err_out4;
+ 
+ 			sch->cpu_qstats = alloc_percpu(struct gnet_stats_queue);
+ 			if (!sch->cpu_qstats)
+ 				goto err_out4;
+ 		}
+ 
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		if (tca[TCA_STAB]) {
  			stab = qdisc_get_stab(tca[TCA_STAB]);
  			if (IS_ERR(stab)) {
@@@ -987,6 -998,8 +1001,11 @@@ err_out
  	return NULL;
  
  err_out4:
++<<<<<<< HEAD
++=======
+ 	free_percpu(sch->cpu_bstats);
+ 	free_percpu(sch->cpu_qstats);
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  	/*
  	 * Any broken qdiscs that would require a ops->reset() here?
  	 * The qdisc was never in action so it shouldn't be necessary.
@@@ -1301,6 -1317,8 +1320,11 @@@ graft
  static int tc_fill_qdisc(struct sk_buff *skb, struct Qdisc *q, u32 clid,
  			 u32 portid, u32 seq, u16 flags, int event)
  {
++<<<<<<< HEAD
++=======
+ 	struct gnet_stats_basic_cpu __percpu *cpu_bstats = NULL;
+ 	struct gnet_stats_queue __percpu *cpu_qstats = NULL;
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  	struct tcmsg *tcm;
  	struct nlmsghdr  *nlh;
  	unsigned char *b = skb_tail_pointer(skb);
@@@ -1336,9 -1355,14 +1360,20 @@@
  	if (q->ops->dump_stats && q->ops->dump_stats(q, &d) < 0)
  		goto nla_put_failure;
  
++<<<<<<< HEAD
 +	if (gnet_stats_copy_basic(&d, &q->bstats) < 0 ||
 +	    gnet_stats_copy_rate_est(&d, &q->bstats, &q->rate_est) < 0 ||
 +	    gnet_stats_copy_queue(&d, &q->qstats) < 0)
++=======
+ 	if (qdisc_is_percpu_stats(q)) {
+ 		cpu_bstats = q->cpu_bstats;
+ 		cpu_qstats = q->cpu_qstats;
+ 	}
+ 
+ 	if (gnet_stats_copy_basic(&d, cpu_bstats, &q->bstats) < 0 ||
+ 	    gnet_stats_copy_rate_est(&d, &q->bstats, &q->rate_est) < 0 ||
+ 	    gnet_stats_copy_queue(&d, cpu_qstats, &q->qstats, qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		goto nla_put_failure;
  
  	if (gnet_stats_finish_copy(&d) < 0)
diff --cc net/sched/sch_atm.c
index a5c6ce48ef31,e3e2cc5fd068..000000000000
--- a/net/sched/sch_atm.c
+++ b/net/sched/sch_atm.c
@@@ -635,10 -637,8 +635,15 @@@ atm_tc_dump_class_stats(struct Qdisc *s
  {
  	struct atm_flow_data *flow = (struct atm_flow_data *)arg;
  
++<<<<<<< HEAD
 +	flow->qstats.qlen = flow->q->q.qlen;
 +
 +	if (gnet_stats_copy_basic(d, &flow->bstats) < 0 ||
 +	    gnet_stats_copy_queue(d, &flow->qstats) < 0)
++=======
+ 	if (gnet_stats_copy_basic(d, NULL, &flow->bstats) < 0 ||
+ 	    gnet_stats_copy_queue(d, NULL, &flow->qstats, flow->q->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return 0;
diff --cc net/sched/sch_cbq.c
index 3389351a8374,beeb75f80fdb..000000000000
--- a/net/sched/sch_cbq.c
+++ b/net/sched/sch_cbq.c
@@@ -1618,9 -1600,9 +1618,13 @@@ cbq_dump_class_stats(struct Qdisc *sch
  	if (cl->undertime != PSCHED_PASTPERFECT)
  		cl->xstats.undertime = cl->undertime - q->now;
  
 -	if (gnet_stats_copy_basic(d, NULL, &cl->bstats) < 0 ||
 +	if (gnet_stats_copy_basic(d, &cl->bstats) < 0 ||
  	    gnet_stats_copy_rate_est(d, &cl->bstats, &cl->rate_est) < 0 ||
++<<<<<<< HEAD
 +	    gnet_stats_copy_queue(d, &cl->qstats) < 0)
++=======
+ 	    gnet_stats_copy_queue(d, NULL, &cl->qstats, cl->q->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return gnet_stats_copy_app(d, &cl->xstats, sizeof(cl->xstats));
diff --cc net/sched/sch_drr.c
index 4c26ee692ff8,338706092c27..000000000000
--- a/net/sched/sch_drr.c
+++ b/net/sched/sch_drr.c
@@@ -272,14 -278,13 +272,18 @@@ static int drr_dump_class_stats(struct 
  	struct tc_drr_stats xstats;
  
  	memset(&xstats, 0, sizeof(xstats));
 -	if (qlen)
 +	if (cl->qdisc->q.qlen) {
  		xstats.deficit = cl->deficit;
 +		cl->qdisc->qstats.qlen = cl->qdisc->q.qlen;
 +	}
  
 -	if (gnet_stats_copy_basic(d, NULL, &cl->bstats) < 0 ||
 +	if (gnet_stats_copy_basic(d, &cl->bstats) < 0 ||
  	    gnet_stats_copy_rate_est(d, &cl->bstats, &cl->rate_est) < 0 ||
++<<<<<<< HEAD
 +	    gnet_stats_copy_queue(d, &cl->qdisc->qstats) < 0)
++=======
+ 	    gnet_stats_copy_queue(d, NULL, &cl->qdisc->qstats, qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return gnet_stats_copy_app(d, &xstats, sizeof(xstats));
diff --cc net/sched/sch_fq_codel.c
index 325552fe3cde,b9ca32ebc1de..000000000000
--- a/net/sched/sch_fq_codel.c
+++ b/net/sched/sch_fq_codel.c
@@@ -579,7 -550,7 +579,11 @@@ static int fq_codel_dump_class_stats(st
  		qs.backlog = q->backlogs[idx];
  		qs.drops = flow->dropped;
  	}
++<<<<<<< HEAD
 +	if (gnet_stats_copy_queue(d, &qs) < 0)
++=======
+ 	if (gnet_stats_copy_queue(d, NULL, &qs, 0) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  	if (idx < q->flows_cnt)
  		return gnet_stats_copy_app(d, &xstats, sizeof(xstats));
diff --cc net/sched/sch_hfsc.c
index 747f88e3d5d1,e6c7416d0332..000000000000
--- a/net/sched/sch_hfsc.c
+++ b/net/sched/sch_hfsc.c
@@@ -1370,9 -1376,9 +1370,13 @@@ hfsc_dump_class_stats(struct Qdisc *sch
  	xstats.work    = cl->cl_total;
  	xstats.rtwork  = cl->cl_cumul;
  
 -	if (gnet_stats_copy_basic(d, NULL, &cl->bstats) < 0 ||
 +	if (gnet_stats_copy_basic(d, &cl->bstats) < 0 ||
  	    gnet_stats_copy_rate_est(d, &cl->bstats, &cl->rate_est) < 0 ||
++<<<<<<< HEAD
 +	    gnet_stats_copy_queue(d, &cl->qstats) < 0)
++=======
+ 	    gnet_stats_copy_queue(d, NULL, &cl->qstats, cl->qdisc->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return gnet_stats_copy_app(d, &xstats, sizeof(xstats));
diff --cc net/sched/sch_htb.c
index 38195029b441,f1acb0f60dc3..000000000000
--- a/net/sched/sch_htb.c
+++ b/net/sched/sch_htb.c
@@@ -1147,9 -1145,9 +1147,13 @@@ htb_dump_class_stats(struct Qdisc *sch
  	cl->xstats.tokens = PSCHED_NS2TICKS(cl->tokens);
  	cl->xstats.ctokens = PSCHED_NS2TICKS(cl->ctokens);
  
 -	if (gnet_stats_copy_basic(d, NULL, &cl->bstats) < 0 ||
 +	if (gnet_stats_copy_basic(d, &cl->bstats) < 0 ||
  	    gnet_stats_copy_rate_est(d, NULL, &cl->rate_est) < 0 ||
++<<<<<<< HEAD
 +	    gnet_stats_copy_queue(d, &cl->qstats) < 0)
++=======
+ 	    gnet_stats_copy_queue(d, NULL, &cl->qstats, qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return gnet_stats_copy_app(d, &cl->xstats, sizeof(cl->xstats));
diff --cc net/sched/sch_mq.c
index a8b2864a696b,f3cbaecd283a..000000000000
--- a/net/sched/sch_mq.c
+++ b/net/sched/sch_mq.c
@@@ -200,9 -199,8 +200,14 @@@ static int mq_dump_class_stats(struct Q
  	struct netdev_queue *dev_queue = mq_queue_get(sch, cl);
  
  	sch = dev_queue->qdisc_sleeping;
++<<<<<<< HEAD
 +	sch->qstats.qlen = sch->q.qlen;
 +	if (gnet_stats_copy_basic(d, &sch->bstats) < 0 ||
 +	    gnet_stats_copy_queue(d, &sch->qstats) < 0)
++=======
+ 	if (gnet_stats_copy_basic(d, NULL, &sch->bstats) < 0 ||
+ 	    gnet_stats_copy_queue(d, NULL, &sch->qstats, sch->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  	return 0;
  }
diff --cc net/sched/sch_mqprio.c
index 15d8c87be2b6,3811a745452c..000000000000
--- a/net/sched/sch_mqprio.c
+++ b/net/sched/sch_mqprio.c
@@@ -368,16 -355,16 +368,27 @@@ static int mqprio_dump_class_stats(stru
  		}
  		/* Reclaim root sleeping lock before completing stats */
  		spin_lock_bh(d->lock);
++<<<<<<< HEAD
 +		if (gnet_stats_copy_basic(d, &bstats) < 0 ||
 +		    gnet_stats_copy_queue(d, &qstats) < 0)
++=======
+ 		if (gnet_stats_copy_basic(d, NULL, &bstats) < 0 ||
+ 		    gnet_stats_copy_queue(d, NULL, &qstats, qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  			return -1;
  	} else {
  		struct netdev_queue *dev_queue = mqprio_queue_get(sch, cl);
  
  		sch = dev_queue->qdisc_sleeping;
++<<<<<<< HEAD
 +		sch->qstats.qlen = sch->q.qlen;
 +		if (gnet_stats_copy_basic(d, &sch->bstats) < 0 ||
 +		    gnet_stats_copy_queue(d, &sch->qstats) < 0)
++=======
+ 		if (gnet_stats_copy_basic(d, NULL, &sch->bstats) < 0 ||
+ 		    gnet_stats_copy_queue(d, NULL,
+ 					  &sch->qstats, sch->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  			return -1;
  	}
  	return 0;
diff --cc net/sched/sch_multiq.c
index c433de581b93,42dd218871e0..000000000000
--- a/net/sched/sch_multiq.c
+++ b/net/sched/sch_multiq.c
@@@ -354,9 -360,8 +354,14 @@@ static int multiq_dump_class_stats(stru
  	struct Qdisc *cl_q;
  
  	cl_q = q->queues[cl - 1];
++<<<<<<< HEAD
 +	cl_q->qstats.qlen = cl_q->q.qlen;
 +	if (gnet_stats_copy_basic(d, &cl_q->bstats) < 0 ||
 +	    gnet_stats_copy_queue(d, &cl_q->qstats) < 0)
++=======
+ 	if (gnet_stats_copy_basic(d, NULL, &cl_q->bstats) < 0 ||
+ 	    gnet_stats_copy_queue(d, NULL, &cl_q->qstats, cl_q->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return 0;
diff --cc net/sched/sch_prio.c
index 0b904a19248a,8e5cd34aaa74..000000000000
--- a/net/sched/sch_prio.c
+++ b/net/sched/sch_prio.c
@@@ -320,9 -324,8 +320,14 @@@ static int prio_dump_class_stats(struc
  	struct Qdisc *cl_q;
  
  	cl_q = q->queues[cl - 1];
++<<<<<<< HEAD
 +	cl_q->qstats.qlen = cl_q->q.qlen;
 +	if (gnet_stats_copy_basic(d, &cl_q->bstats) < 0 ||
 +	    gnet_stats_copy_queue(d, &cl_q->qstats) < 0)
++=======
+ 	if (gnet_stats_copy_basic(d, NULL, &cl_q->bstats) < 0 ||
+ 	    gnet_stats_copy_queue(d, NULL, &cl_q->qstats, cl_q->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return 0;
diff --cc net/sched/sch_qfq.c
index 2b0a5cfb2004,3ec7e88a43ca..000000000000
--- a/net/sched/sch_qfq.c
+++ b/net/sched/sch_qfq.c
@@@ -662,9 -668,10 +662,14 @@@ static int qfq_dump_class_stats(struct 
  	xstats.weight = cl->agg->class_weight;
  	xstats.lmax = cl->agg->lmax;
  
 -	if (gnet_stats_copy_basic(d, NULL, &cl->bstats) < 0 ||
 +	if (gnet_stats_copy_basic(d, &cl->bstats) < 0 ||
  	    gnet_stats_copy_rate_est(d, &cl->bstats, &cl->rate_est) < 0 ||
++<<<<<<< HEAD
 +	    gnet_stats_copy_queue(d, &cl->qdisc->qstats) < 0)
++=======
+ 	    gnet_stats_copy_queue(d, NULL,
+ 				  &cl->qdisc->qstats, cl->qdisc->q.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  
  	return gnet_stats_copy_app(d, &xstats, sizeof(xstats));
diff --cc net/sched/sch_sfq.c
index 26d6814c8b15,b877140beda5..000000000000
--- a/net/sched/sch_sfq.c
+++ b/net/sched/sch_sfq.c
@@@ -876,7 -871,7 +876,11 @@@ static int sfq_dump_class_stats(struct 
  		qs.qlen = slot->qlen;
  		qs.backlog = slot->backlog;
  	}
++<<<<<<< HEAD
 +	if (gnet_stats_copy_queue(d, &qs) < 0)
++=======
+ 	if (gnet_stats_copy_queue(d, NULL, &qs, qs.qlen) < 0)
++>>>>>>> b0ab6f92752b (net: sched: enable per cpu qstats)
  		return -1;
  	return gnet_stats_copy_app(d, &xstats, sizeof(xstats));
  }
* Unmerged path include/net/gen_stats.h
diff --git a/include/net/sch_generic.h b/include/net/sch_generic.h
index dd1e68d92a44..3cdd30a61d17 100644
--- a/include/net/sch_generic.h
+++ b/include/net/sch_generic.h
@@ -86,7 +86,10 @@ struct Qdisc {
 	struct sk_buff_head	q;
 	struct gnet_stats_basic_packed bstats;
 	unsigned int		__state;
-	struct gnet_stats_queue	qstats;
+	union {
+		struct gnet_stats_queue	qstats;
+		struct gnet_stats_queue	__percpu *cpu_qstats;
+	} __packed;
 	struct rcu_head		rcu_head;
 	int			padded;
 	atomic_t		refcnt;
@@ -539,6 +542,13 @@ static inline void qdisc_qstats_drop(struct Qdisc *sch)
 	sch->qstats.drops++;
 }
 
+static inline void qdisc_qstats_drop_cpu(struct Qdisc *sch)
+{
+	struct gnet_stats_queue *qstats = this_cpu_ptr(sch->cpu_qstats);
+
+	qstats->drops++;
+}
+
 static inline void qdisc_qstats_overlimit(struct Qdisc *sch)
 {
 	sch->qstats.overlimits++;
* Unmerged path net/core/gen_stats.c
* Unmerged path net/sched/act_api.c
* Unmerged path net/sched/sch_api.c
* Unmerged path net/sched/sch_atm.c
* Unmerged path net/sched/sch_cbq.c
* Unmerged path net/sched/sch_drr.c
* Unmerged path net/sched/sch_fq_codel.c
* Unmerged path net/sched/sch_hfsc.c
* Unmerged path net/sched/sch_htb.c
* Unmerged path net/sched/sch_mq.c
* Unmerged path net/sched/sch_mqprio.c
* Unmerged path net/sched/sch_multiq.c
* Unmerged path net/sched/sch_prio.c
* Unmerged path net/sched/sch_qfq.c
* Unmerged path net/sched/sch_sfq.c
