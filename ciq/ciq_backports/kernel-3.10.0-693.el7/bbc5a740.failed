xfs: split xfs_file_read_iter into buffered and direct I/O helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Christoph Hellwig <hch@lst.de>
commit bbc5a740c4f27a9732a3a3decf3186b4bce21108
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/bbc5a740.failed

Similar to what we did on the write side a while ago.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Dave Chinner <dchinner@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>


(cherry picked from commit bbc5a740c4f27a9732a3a3decf3186b4bce21108)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_file.c
index 815d8f3721f9,fdb123ffd616..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -285,56 -282,33 +285,78 @@@ xfs_file_fsync
  }
  
  STATIC ssize_t
++<<<<<<< HEAD
 +xfs_file_aio_read(
++=======
+ xfs_file_dio_aio_read(
++>>>>>>> bbc5a740c4f2 (xfs: split xfs_file_read_iter into buffered and direct I/O helpers)
  	struct kiocb		*iocb,
 -	struct iov_iter		*to)
 +	const struct iovec	*iovp,
 +	unsigned long		nr_segs,
 +	loff_t			pos)
  {
- 	struct file		*file = iocb->ki_filp;
- 	struct inode		*inode = file->f_mapping->host;
+ 	struct address_space	*mapping = iocb->ki_filp->f_mapping;
+ 	struct inode		*inode = mapping->host;
  	struct xfs_inode	*ip = XFS_I(inode);
++<<<<<<< HEAD
 +	struct xfs_mount	*mp = ip->i_mount;
 +	size_t			size = 0;
 +	ssize_t			ret = 0;
 +	int			ioflags = 0;
 +	xfs_fsize_t		n;
++=======
+ 	size_t			count = iov_iter_count(to);
+ 	struct xfs_buftarg	*target;
+ 	ssize_t			ret = 0;
++>>>>>>> bbc5a740c4f2 (xfs: split xfs_file_read_iter into buffered and direct I/O helpers)
  
- 	XFS_STATS_INC(mp, xs_read_calls);
+ 	trace_xfs_file_direct_read(ip, count, iocb->ki_pos);
  
++<<<<<<< HEAD
 +	BUG_ON(iocb->ki_pos != pos);
 +
 +	if (unlikely(file->f_flags & O_DIRECT))
 +		ioflags |= XFS_IO_ISDIRECT;
 +	if (file->f_mode & FMODE_NOCMTIME)
 +		ioflags |= XFS_IO_INVIS;
 +
 +	ret = generic_segment_checks(iovp, &nr_segs, &size, VERIFY_WRITE);
 +	if (ret < 0)
 +		return ret;
 +
 +	if ((ioflags & XFS_IO_ISDIRECT) && !IS_DAX(inode)) {
 +		xfs_buftarg_t	*target =
 +			XFS_IS_REALTIME_INODE(ip) ?
 +				mp->m_rtdev_targp : mp->m_ddev_targp;
++=======
+ 	if (XFS_IS_REALTIME_INODE(ip))
+ 		target = ip->i_mount->m_rtdev_targp;
+ 	else
+ 		target = ip->i_mount->m_ddev_targp;
+ 
+ 	if (!IS_DAX(inode)) {
++>>>>>>> bbc5a740c4f2 (xfs: split xfs_file_read_iter into buffered and direct I/O helpers)
  		/* DIO must be aligned to device logical sector size */
- 		if ((pos | size) & target->bt_logical_sectormask) {
- 			if (pos == i_size_read(inode))
+ 		if ((iocb->ki_pos | count) & target->bt_logical_sectormask) {
+ 			if (iocb->ki_pos == i_size_read(inode))
  				return 0;
  			return -EINVAL;
  		}
  	}
  
++<<<<<<< HEAD
 +	n = mp->m_super->s_maxbytes - pos;
 +	if (n <= 0 || size == 0)
 +		return 0;
 +
 +	if (n < size)
 +		size = n;
 +
 +	if (XFS_FORCED_SHUTDOWN(mp))
 +		return -EIO;
 +
++=======
++>>>>>>> bbc5a740c4f2 (xfs: split xfs_file_read_iter into buffered and direct I/O helpers)
  	/*
  	 * Locking is a bit tricky here. If we take an exclusive lock for direct
  	 * IO, we effectively serialise all new concurrent read IO to this file
@@@ -346,7 -320,7 +368,11 @@@
  	 * serialisation.
  	 */
  	xfs_rw_ilock(ip, XFS_IOLOCK_SHARED);
++<<<<<<< HEAD
 +	if ((ioflags & XFS_IO_ISDIRECT) && inode->i_mapping->nrpages) {
++=======
+ 	if (mapping->nrpages) {
++>>>>>>> bbc5a740c4f2 (xfs: split xfs_file_read_iter into buffered and direct I/O helpers)
  		xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
  		xfs_rw_ilock(ip, XFS_IOLOCK_EXCL);
  
@@@ -380,13 -354,49 +406,55 @@@
  		xfs_rw_ilock_demote(ip, XFS_IOLOCK_EXCL);
  	}
  
++<<<<<<< HEAD
 +	trace_xfs_file_read(ip, size, pos, ioflags);
 +
 +	ret = generic_file_aio_read(iocb, iovp, nr_segs, pos);
- 	if (ret > 0)
- 		XFS_STATS_ADD(mp, xs_read_bytes, ret);
++=======
+ 	ret = generic_file_read_iter(iocb, to);
+ 	xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
+ 
+ 	return ret;
+ }
+ 
+ STATIC ssize_t
+ xfs_file_buffered_aio_read(
+ 	struct kiocb		*iocb,
+ 	struct iov_iter		*to)
+ {
+ 	struct xfs_inode	*ip = XFS_I(file_inode(iocb->ki_filp));
+ 	ssize_t			ret;
+ 
+ 	trace_xfs_file_buffered_read(ip, iov_iter_count(to), iocb->ki_pos);
  
+ 	xfs_rw_ilock(ip, XFS_IOLOCK_SHARED);
+ 	ret = generic_file_read_iter(iocb, to);
  	xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
+ 
+ 	return ret;
+ }
+ 
+ STATIC ssize_t
+ xfs_file_read_iter(
+ 	struct kiocb		*iocb,
+ 	struct iov_iter		*to)
+ {
+ 	struct xfs_mount	*mp = XFS_I(file_inode(iocb->ki_filp))->i_mount;
+ 	ssize_t			ret = 0;
+ 
+ 	XFS_STATS_INC(mp, xs_read_calls);
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		return -EIO;
+ 
+ 	if (iocb->ki_flags & IOCB_DIRECT)
+ 		ret = xfs_file_dio_aio_read(iocb, to);
+ 	else
+ 		ret = xfs_file_buffered_aio_read(iocb, to);
+ 
++>>>>>>> bbc5a740c4f2 (xfs: split xfs_file_read_iter into buffered and direct I/O helpers)
+ 	if (ret > 0)
+ 		XFS_STATS_ADD(mp, xs_read_bytes, ret);
  	return ret;
  }
  
@@@ -829,12 -771,14 +897,16 @@@ xfs_file_dio_aio_write
  		xfs_rw_ilock(ip, iolock);
  	}
  
 -	ret = xfs_file_aio_write_checks(iocb, from, &iolock);
 +	ret = xfs_file_aio_write_checks(file, &pos, &count, &iolock);
  	if (ret)
  		goto out;
 -	count = iov_iter_count(from);
 -	end = iocb->ki_pos + count - 1;
  
  	/*
++<<<<<<< HEAD
 +	 * See xfs_file_aio_read() for why we do a full-file flush here.
++=======
+ 	 * See xfs_file_dio_aio_read() for why we do a full-file flush here.
++>>>>>>> bbc5a740c4f2 (xfs: split xfs_file_read_iter into buffered and direct I/O helpers)
  	 */
  	if (mapping->nrpages) {
  		ret = filemap_write_and_wait(VFS_I(ip)->i_mapping);
* Unmerged path fs/xfs/xfs_file.c
