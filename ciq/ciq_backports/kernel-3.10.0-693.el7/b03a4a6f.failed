amd-xgbe: Prepare for a new PCS register access method

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Lendacky, Thomas <Thomas.Lendacky@amd.com>
commit b03a4a6fb309a000a0fba5f2af06ffc5767b0e45
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/b03a4a6f.failed

Prepare the code to be able to support accessing of the PCS registers
in a new way, while maintaining the current access method. Provide a
version specific field that indicates the method to use.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit b03a4a6fb309a000a0fba5f2af06ffc5767b0e45)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amd/xgbe/xgbe-common.h
#	drivers/net/ethernet/amd/xgbe/xgbe-dev.c
#	drivers/net/ethernet/amd/xgbe/xgbe-main.c
#	drivers/net/ethernet/amd/xgbe/xgbe.h
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-common.h
index 3373e9ef2003,6c40915d065b..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-common.h
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-common.h
@@@ -743,16 -852,51 +743,22 @@@
  #define MTL_TSA_SP			0x00
  #define MTL_TSA_ETS			0x02
  
++<<<<<<< HEAD
 +
 +/* PCS MMD select register offset
 + *  The MMD select register is used for accessing PCS registers
 + *  when the underlying APB3 interface is using indirect addressing.
 + *  Indirect addressing requires accessing registers in two phases,
 + *  an address phase and a data phase.  The address phases requires
 + *  writing an address selection value to the MMD select regiesters.
 + */
 +#define PCS_MMD_SELECT			0xff
++=======
+ /* PCS register offsets */
+ #define PCS_V1_WINDOW_SELECT		0x03fc
+ #define PCS_V2_WINDOW_SELECT		0x9064
++>>>>>>> b03a4a6fb309 (amd-xgbe: Prepare for a new PCS register access method)
  
 -/* SerDes integration register offsets */
 -#define SIR0_KR_RT_1			0x002c
 -#define SIR0_STATUS			0x0040
 -#define SIR1_SPEED			0x0000
 -
 -/* SerDes integration register entry bit positions and sizes */
 -#define SIR0_KR_RT_1_RESET_INDEX	11
 -#define SIR0_KR_RT_1_RESET_WIDTH	1
 -#define SIR0_STATUS_RX_READY_INDEX	0
 -#define SIR0_STATUS_RX_READY_WIDTH	1
 -#define SIR0_STATUS_TX_READY_INDEX	8
 -#define SIR0_STATUS_TX_READY_WIDTH	1
 -#define SIR1_SPEED_CDR_RATE_INDEX	12
 -#define SIR1_SPEED_CDR_RATE_WIDTH	4
 -#define SIR1_SPEED_DATARATE_INDEX	4
 -#define SIR1_SPEED_DATARATE_WIDTH	2
 -#define SIR1_SPEED_PLLSEL_INDEX		3
 -#define SIR1_SPEED_PLLSEL_WIDTH		1
 -#define SIR1_SPEED_RATECHANGE_INDEX	6
 -#define SIR1_SPEED_RATECHANGE_WIDTH	1
 -#define SIR1_SPEED_TXAMP_INDEX		8
 -#define SIR1_SPEED_TXAMP_WIDTH		4
 -#define SIR1_SPEED_WORDMODE_INDEX	0
 -#define SIR1_SPEED_WORDMODE_WIDTH	3
 -
 -/* SerDes RxTx register offsets */
 -#define RXTX_REG6			0x0018
 -#define RXTX_REG20			0x0050
 -#define RXTX_REG22			0x0058
 -#define RXTX_REG114			0x01c8
 -#define RXTX_REG129			0x0204
 -
 -/* SerDes RxTx register entry bit positions and sizes */
 -#define RXTX_REG6_RESETB_RXD_INDEX	8
 -#define RXTX_REG6_RESETB_RXD_WIDTH	1
 -#define RXTX_REG20_BLWC_ENA_INDEX	2
 -#define RXTX_REG20_BLWC_ENA_WIDTH	1
 -#define RXTX_REG114_PQ_REG_INDEX	9
 -#define RXTX_REG114_PQ_REG_WIDTH	7
 -#define RXTX_REG129_RXDFE_CONFIG_INDEX	14
 -#define RXTX_REG129_RXDFE_CONFIG_WIDTH	2
  
  /* Descriptor/Packet entry bit positions and sizes */
  #define RX_PACKET_ERRORS_CRC_INDEX		2
@@@ -979,12 -1236,93 +985,96 @@@ do {									
  /* Macros for building, reading or writing register values or bits
   * within the register values of XPCS registers.
   */
- #define XPCS_IOWRITE(_pdata, _off, _val)				\
+ #define XPCS32_IOWRITE(_pdata, _off, _val)				\
  	iowrite32(_val, (_pdata)->xpcs_regs + (_off))
  
- #define XPCS_IOREAD(_pdata, _off)					\
+ #define XPCS32_IOREAD(_pdata, _off)					\
  	ioread32((_pdata)->xpcs_regs + (_off))
  
++<<<<<<< HEAD
++=======
+ #define XPCS16_IOWRITE(_pdata, _off, _val)				\
+ 	iowrite16(_val, (_pdata)->xpcs_regs + (_off))
+ 
+ #define XPCS16_IOREAD(_pdata, _off)					\
+ 	ioread16((_pdata)->xpcs_regs + (_off))
+ 
+ /* Macros for building, reading or writing register values or bits
+  * within the register values of SerDes integration registers.
+  */
+ #define XSIR_GET_BITS(_var, _prefix, _field)                            \
+ 	GET_BITS((_var),                                                \
+ 		 _prefix##_##_field##_INDEX,                            \
+ 		 _prefix##_##_field##_WIDTH)
+ 
+ #define XSIR_SET_BITS(_var, _prefix, _field, _val)                      \
+ 	SET_BITS((_var),                                                \
+ 		 _prefix##_##_field##_INDEX,                            \
+ 		 _prefix##_##_field##_WIDTH, (_val))
+ 
+ #define XSIR0_IOREAD(_pdata, _reg)					\
+ 	ioread16((_pdata)->sir0_regs + _reg)
+ 
+ #define XSIR0_IOREAD_BITS(_pdata, _reg, _field)				\
+ 	GET_BITS(XSIR0_IOREAD((_pdata), _reg),				\
+ 		 _reg##_##_field##_INDEX,				\
+ 		 _reg##_##_field##_WIDTH)
+ 
+ #define XSIR0_IOWRITE(_pdata, _reg, _val)				\
+ 	iowrite16((_val), (_pdata)->sir0_regs + _reg)
+ 
+ #define XSIR0_IOWRITE_BITS(_pdata, _reg, _field, _val)			\
+ do {									\
+ 	u16 reg_val = XSIR0_IOREAD((_pdata), _reg);			\
+ 	SET_BITS(reg_val,						\
+ 		 _reg##_##_field##_INDEX,				\
+ 		 _reg##_##_field##_WIDTH, (_val));			\
+ 	XSIR0_IOWRITE((_pdata), _reg, reg_val);				\
+ } while (0)
+ 
+ #define XSIR1_IOREAD(_pdata, _reg)					\
+ 	ioread16((_pdata)->sir1_regs + _reg)
+ 
+ #define XSIR1_IOREAD_BITS(_pdata, _reg, _field)				\
+ 	GET_BITS(XSIR1_IOREAD((_pdata), _reg),				\
+ 		 _reg##_##_field##_INDEX,				\
+ 		 _reg##_##_field##_WIDTH)
+ 
+ #define XSIR1_IOWRITE(_pdata, _reg, _val)				\
+ 	iowrite16((_val), (_pdata)->sir1_regs + _reg)
+ 
+ #define XSIR1_IOWRITE_BITS(_pdata, _reg, _field, _val)			\
+ do {									\
+ 	u16 reg_val = XSIR1_IOREAD((_pdata), _reg);			\
+ 	SET_BITS(reg_val,						\
+ 		 _reg##_##_field##_INDEX,				\
+ 		 _reg##_##_field##_WIDTH, (_val));			\
+ 	XSIR1_IOWRITE((_pdata), _reg, reg_val);				\
+ } while (0)
+ 
+ /* Macros for building, reading or writing register values or bits
+  * within the register values of SerDes RxTx registers.
+  */
+ #define XRXTX_IOREAD(_pdata, _reg)					\
+ 	ioread16((_pdata)->rxtx_regs + _reg)
+ 
+ #define XRXTX_IOREAD_BITS(_pdata, _reg, _field)				\
+ 	GET_BITS(XRXTX_IOREAD((_pdata), _reg),				\
+ 		 _reg##_##_field##_INDEX,				\
+ 		 _reg##_##_field##_WIDTH)
+ 
+ #define XRXTX_IOWRITE(_pdata, _reg, _val)				\
+ 	iowrite16((_val), (_pdata)->rxtx_regs + _reg)
+ 
+ #define XRXTX_IOWRITE_BITS(_pdata, _reg, _field, _val)			\
+ do {									\
+ 	u16 reg_val = XRXTX_IOREAD((_pdata), _reg);			\
+ 	SET_BITS(reg_val,						\
+ 		 _reg##_##_field##_INDEX,				\
+ 		 _reg##_##_field##_WIDTH, (_val));			\
+ 	XRXTX_IOWRITE((_pdata), _reg, reg_val);				\
+ } while (0)
++>>>>>>> b03a4a6fb309 (amd-xgbe: Prepare for a new PCS register access method)
  
  /* Macros for building, reading or writing register values or bits
   * using MDIO.  Different from above because of the use of standardized
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-dev.c
index a748fd8a1c58,b8a04e739344..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
@@@ -765,6 -768,427 +765,430 @@@ static int xgbe_disable_rx_vlan_strippi
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int xgbe_enable_rx_vlan_filtering(struct xgbe_prv_data *pdata)
+ {
+ 	/* Enable VLAN filtering */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, VTFE, 1);
+ 
+ 	/* Enable VLAN Hash Table filtering */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, VTHM, 1);
+ 
+ 	/* Disable VLAN tag inverse matching */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, VTIM, 0);
+ 
+ 	/* Only filter on the lower 12-bits of the VLAN tag */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, ETV, 1);
+ 
+ 	/* In order for the VLAN Hash Table filtering to be effective,
+ 	 * the VLAN tag identifier in the VLAN Tag Register must not
+ 	 * be zero.  Set the VLAN tag identifier to "1" to enable the
+ 	 * VLAN Hash Table filtering.  This implies that a VLAN tag of
+ 	 * 1 will always pass filtering.
+ 	 */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, VL, 1);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_disable_rx_vlan_filtering(struct xgbe_prv_data *pdata)
+ {
+ 	/* Disable VLAN filtering */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, VTFE, 0);
+ 
+ 	return 0;
+ }
+ 
+ static u32 xgbe_vid_crc32_le(__le16 vid_le)
+ {
+ 	u32 poly = 0xedb88320;	/* CRCPOLY_LE */
+ 	u32 crc = ~0;
+ 	u32 temp = 0;
+ 	unsigned char *data = (unsigned char *)&vid_le;
+ 	unsigned char data_byte = 0;
+ 	int i, bits;
+ 
+ 	bits = get_bitmask_order(VLAN_VID_MASK);
+ 	for (i = 0; i < bits; i++) {
+ 		if ((i % 8) == 0)
+ 			data_byte = data[i / 8];
+ 
+ 		temp = ((crc & 1) ^ data_byte) & 1;
+ 		crc >>= 1;
+ 		data_byte >>= 1;
+ 
+ 		if (temp)
+ 			crc ^= poly;
+ 	}
+ 
+ 	return crc;
+ }
+ 
+ static int xgbe_update_vlan_hash_table(struct xgbe_prv_data *pdata)
+ {
+ 	u32 crc;
+ 	u16 vid;
+ 	__le16 vid_le;
+ 	u16 vlan_hash_table = 0;
+ 
+ 	/* Generate the VLAN Hash Table value */
+ 	for_each_set_bit(vid, pdata->active_vlans, VLAN_N_VID) {
+ 		/* Get the CRC32 value of the VLAN ID */
+ 		vid_le = cpu_to_le16(vid);
+ 		crc = bitrev32(~xgbe_vid_crc32_le(vid_le)) >> 28;
+ 
+ 		vlan_hash_table |= (1 << crc);
+ 	}
+ 
+ 	/* Set the VLAN Hash Table filtering register */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANHTR, VLHT, vlan_hash_table);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_promiscuous_mode(struct xgbe_prv_data *pdata,
+ 				     unsigned int enable)
+ {
+ 	unsigned int val = enable ? 1 : 0;
+ 
+ 	if (XGMAC_IOREAD_BITS(pdata, MAC_PFR, PR) == val)
+ 		return 0;
+ 
+ 	netif_dbg(pdata, drv, pdata->netdev, "%s promiscuous mode\n",
+ 		  enable ? "entering" : "leaving");
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, PR, val);
+ 
+ 	/* Hardware will still perform VLAN filtering in promiscuous mode */
+ 	if (enable) {
+ 		xgbe_disable_rx_vlan_filtering(pdata);
+ 	} else {
+ 		if (pdata->netdev->features & NETIF_F_HW_VLAN_CTAG_FILTER)
+ 			xgbe_enable_rx_vlan_filtering(pdata);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_all_multicast_mode(struct xgbe_prv_data *pdata,
+ 				       unsigned int enable)
+ {
+ 	unsigned int val = enable ? 1 : 0;
+ 
+ 	if (XGMAC_IOREAD_BITS(pdata, MAC_PFR, PM) == val)
+ 		return 0;
+ 
+ 	netif_dbg(pdata, drv, pdata->netdev, "%s allmulti mode\n",
+ 		  enable ? "entering" : "leaving");
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, PM, val);
+ 
+ 	return 0;
+ }
+ 
+ static void xgbe_set_mac_reg(struct xgbe_prv_data *pdata,
+ 			     struct netdev_hw_addr *ha, unsigned int *mac_reg)
+ {
+ 	unsigned int mac_addr_hi, mac_addr_lo;
+ 	u8 *mac_addr;
+ 
+ 	mac_addr_lo = 0;
+ 	mac_addr_hi = 0;
+ 
+ 	if (ha) {
+ 		mac_addr = (u8 *)&mac_addr_lo;
+ 		mac_addr[0] = ha->addr[0];
+ 		mac_addr[1] = ha->addr[1];
+ 		mac_addr[2] = ha->addr[2];
+ 		mac_addr[3] = ha->addr[3];
+ 		mac_addr = (u8 *)&mac_addr_hi;
+ 		mac_addr[0] = ha->addr[4];
+ 		mac_addr[1] = ha->addr[5];
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev,
+ 			  "adding mac address %pM at %#x\n",
+ 			  ha->addr, *mac_reg);
+ 
+ 		XGMAC_SET_BITS(mac_addr_hi, MAC_MACA1HR, AE, 1);
+ 	}
+ 
+ 	XGMAC_IOWRITE(pdata, *mac_reg, mac_addr_hi);
+ 	*mac_reg += MAC_MACA_INC;
+ 	XGMAC_IOWRITE(pdata, *mac_reg, mac_addr_lo);
+ 	*mac_reg += MAC_MACA_INC;
+ }
+ 
+ static void xgbe_set_mac_addn_addrs(struct xgbe_prv_data *pdata)
+ {
+ 	struct net_device *netdev = pdata->netdev;
+ 	struct netdev_hw_addr *ha;
+ 	unsigned int mac_reg;
+ 	unsigned int addn_macs;
+ 
+ 	mac_reg = MAC_MACA1HR;
+ 	addn_macs = pdata->hw_feat.addn_mac;
+ 
+ 	if (netdev_uc_count(netdev) > addn_macs) {
+ 		xgbe_set_promiscuous_mode(pdata, 1);
+ 	} else {
+ 		netdev_for_each_uc_addr(ha, netdev) {
+ 			xgbe_set_mac_reg(pdata, ha, &mac_reg);
+ 			addn_macs--;
+ 		}
+ 
+ 		if (netdev_mc_count(netdev) > addn_macs) {
+ 			xgbe_set_all_multicast_mode(pdata, 1);
+ 		} else {
+ 			netdev_for_each_mc_addr(ha, netdev) {
+ 				xgbe_set_mac_reg(pdata, ha, &mac_reg);
+ 				addn_macs--;
+ 			}
+ 		}
+ 	}
+ 
+ 	/* Clear remaining additional MAC address entries */
+ 	while (addn_macs--)
+ 		xgbe_set_mac_reg(pdata, NULL, &mac_reg);
+ }
+ 
+ static void xgbe_set_mac_hash_table(struct xgbe_prv_data *pdata)
+ {
+ 	struct net_device *netdev = pdata->netdev;
+ 	struct netdev_hw_addr *ha;
+ 	unsigned int hash_reg;
+ 	unsigned int hash_table_shift, hash_table_count;
+ 	u32 hash_table[XGBE_MAC_HASH_TABLE_SIZE];
+ 	u32 crc;
+ 	unsigned int i;
+ 
+ 	hash_table_shift = 26 - (pdata->hw_feat.hash_table_size >> 7);
+ 	hash_table_count = pdata->hw_feat.hash_table_size / 32;
+ 	memset(hash_table, 0, sizeof(hash_table));
+ 
+ 	/* Build the MAC Hash Table register values */
+ 	netdev_for_each_uc_addr(ha, netdev) {
+ 		crc = bitrev32(~crc32_le(~0, ha->addr, ETH_ALEN));
+ 		crc >>= hash_table_shift;
+ 		hash_table[crc >> 5] |= (1 << (crc & 0x1f));
+ 	}
+ 
+ 	netdev_for_each_mc_addr(ha, netdev) {
+ 		crc = bitrev32(~crc32_le(~0, ha->addr, ETH_ALEN));
+ 		crc >>= hash_table_shift;
+ 		hash_table[crc >> 5] |= (1 << (crc & 0x1f));
+ 	}
+ 
+ 	/* Set the MAC Hash Table registers */
+ 	hash_reg = MAC_HTR0;
+ 	for (i = 0; i < hash_table_count; i++) {
+ 		XGMAC_IOWRITE(pdata, hash_reg, hash_table[i]);
+ 		hash_reg += MAC_HTR_INC;
+ 	}
+ }
+ 
+ static int xgbe_add_mac_addresses(struct xgbe_prv_data *pdata)
+ {
+ 	if (pdata->hw_feat.hash_table_size)
+ 		xgbe_set_mac_hash_table(pdata);
+ 	else
+ 		xgbe_set_mac_addn_addrs(pdata);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_mac_address(struct xgbe_prv_data *pdata, u8 *addr)
+ {
+ 	unsigned int mac_addr_hi, mac_addr_lo;
+ 
+ 	mac_addr_hi = (addr[5] <<  8) | (addr[4] <<  0);
+ 	mac_addr_lo = (addr[3] << 24) | (addr[2] << 16) |
+ 		      (addr[1] <<  8) | (addr[0] <<  0);
+ 
+ 	XGMAC_IOWRITE(pdata, MAC_MACA0HR, mac_addr_hi);
+ 	XGMAC_IOWRITE(pdata, MAC_MACA0LR, mac_addr_lo);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_config_rx_mode(struct xgbe_prv_data *pdata)
+ {
+ 	struct net_device *netdev = pdata->netdev;
+ 	unsigned int pr_mode, am_mode;
+ 
+ 	pr_mode = ((netdev->flags & IFF_PROMISC) != 0);
+ 	am_mode = ((netdev->flags & IFF_ALLMULTI) != 0);
+ 
+ 	xgbe_set_promiscuous_mode(pdata, pr_mode);
+ 	xgbe_set_all_multicast_mode(pdata, am_mode);
+ 
+ 	xgbe_add_mac_addresses(pdata);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_read_mmd_regs_v2(struct xgbe_prv_data *pdata, int prtad,
+ 				 int mmd_reg)
+ {
+ 	unsigned long flags;
+ 	unsigned int mmd_address, index, offset;
+ 	int mmd_data;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 16-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 1 bit and reading 16 bits of data.
+ 	 */
+ 	mmd_address <<= 1;
+ 	index = mmd_address & ~pdata->xpcs_window_mask;
+ 	offset = pdata->xpcs_window + (mmd_address & pdata->xpcs_window_mask);
+ 
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V2_WINDOW_SELECT, index);
+ 	mmd_data = XPCS16_IOREAD(pdata, offset);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ 
+ 	return mmd_data;
+ }
+ 
+ static void xgbe_write_mmd_regs_v2(struct xgbe_prv_data *pdata, int prtad,
+ 				   int mmd_reg, int mmd_data)
+ {
+ 	unsigned long flags;
+ 	unsigned int mmd_address, index, offset;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 16-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 1 bit and writing 16 bits of data.
+ 	 */
+ 	mmd_address <<= 1;
+ 	index = mmd_address & ~pdata->xpcs_window_mask;
+ 	offset = pdata->xpcs_window + (mmd_address & pdata->xpcs_window_mask);
+ 
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V2_WINDOW_SELECT, index);
+ 	XPCS16_IOWRITE(pdata, offset, mmd_data);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ }
+ 
+ static int xgbe_read_mmd_regs_v1(struct xgbe_prv_data *pdata, int prtad,
+ 				 int mmd_reg)
+ {
+ 	unsigned long flags;
+ 	unsigned int mmd_address;
+ 	int mmd_data;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying APB3
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 32-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 2 bits and reading 32 bits of data.
+ 	 */
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V1_WINDOW_SELECT, mmd_address >> 8);
+ 	mmd_data = XPCS32_IOREAD(pdata, (mmd_address & 0xff) << 2);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ 
+ 	return mmd_data;
+ }
+ 
+ static void xgbe_write_mmd_regs_v1(struct xgbe_prv_data *pdata, int prtad,
+ 				   int mmd_reg, int mmd_data)
+ {
+ 	unsigned int mmd_address;
+ 	unsigned long flags;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying APB3
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 32-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 2 bits and writing 32 bits of data.
+ 	 */
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V1_WINDOW_SELECT, mmd_address >> 8);
+ 	XPCS32_IOWRITE(pdata, (mmd_address & 0xff) << 2, mmd_data);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ }
+ 
+ static int xgbe_read_mmd_regs(struct xgbe_prv_data *pdata, int prtad,
+ 			      int mmd_reg)
+ {
+ 	switch (pdata->vdata->xpcs_access) {
+ 	case XGBE_XPCS_ACCESS_V1:
+ 		return xgbe_read_mmd_regs_v1(pdata, prtad, mmd_reg);
+ 
+ 	case XGBE_XPCS_ACCESS_V2:
+ 	default:
+ 		return xgbe_read_mmd_regs_v2(pdata, prtad, mmd_reg);
+ 	}
+ }
+ 
+ static void xgbe_write_mmd_regs(struct xgbe_prv_data *pdata, int prtad,
+ 				int mmd_reg, int mmd_data)
+ {
+ 	switch (pdata->vdata->xpcs_access) {
+ 	case XGBE_XPCS_ACCESS_V1:
+ 		return xgbe_write_mmd_regs_v1(pdata, prtad, mmd_reg, mmd_data);
+ 
+ 	case XGBE_XPCS_ACCESS_V2:
+ 	default:
+ 		return xgbe_write_mmd_regs_v2(pdata, prtad, mmd_reg, mmd_data);
+ 	}
+ }
+ 
+ static int xgbe_tx_complete(struct xgbe_ring_desc *rdesc)
+ {
+ 	return !XGMAC_GET_BITS_LE(rdesc->desc3, TX_NORMAL_DESC3, OWN);
+ }
+ 
+ static int xgbe_disable_rx_csum(struct xgbe_prv_data *pdata)
+ {
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RCR, IPC, 0);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_enable_rx_csum(struct xgbe_prv_data *pdata)
+ {
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RCR, IPC, 1);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> b03a4a6fb309 (amd-xgbe: Prepare for a new PCS register access method)
  static void xgbe_tx_desc_reset(struct xgbe_ring_data *rdata)
  {
  	struct xgbe_ring_desc *rdesc = rdata->rdesc;
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-main.c
index e79ba9088346,d9864f0c331a..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-main.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-main.c
@@@ -506,8 -783,25 +506,27 @@@ static int xgbe_resume(struct device *d
  }
  #endif /* CONFIG_PM */
  
++<<<<<<< HEAD
++=======
+ static const struct xgbe_version_data xgbe_v1 = {
+ 	.init_function_ptrs_phy_impl	= xgbe_init_function_ptrs_phy_v1,
+ 	.xpcs_access			= XGBE_XPCS_ACCESS_V1,
+ };
+ 
+ #ifdef CONFIG_ACPI
+ static const struct acpi_device_id xgbe_acpi_match[] = {
+ 	{ .id = "AMDI8001",
+ 	  .driver_data = (kernel_ulong_t)&xgbe_v1 },
+ 	{},
+ };
+ 
+ MODULE_DEVICE_TABLE(acpi, xgbe_acpi_match);
+ #endif
+ 
+ #ifdef CONFIG_OF
++>>>>>>> b03a4a6fb309 (amd-xgbe: Prepare for a new PCS register access method)
  static const struct of_device_id xgbe_of_match[] = {
 -	{ .compatible = "amd,xgbe-seattle-v1a",
 -	  .data = &xgbe_v1 },
 +	{ .compatible = "amd,xgbe-seattle-v1a", },
  	{},
  };
  
diff --cc drivers/net/ethernet/amd/xgbe/xgbe.h
index 1903f878545a,160b4980b2eb..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe.h
+++ b/drivers/net/ethernet/amd/xgbe/xgbe.h
@@@ -324,18 -464,69 +324,84 @@@ enum xgbe_int_state 
  	XGMAC_INT_STATE_RESTORE,
  };
  
++<<<<<<< HEAD
 +enum xgbe_mtl_fifo_size {
 +	XGMAC_MTL_FIFO_SIZE_256  = 0x00,
 +	XGMAC_MTL_FIFO_SIZE_512  = 0x01,
 +	XGMAC_MTL_FIFO_SIZE_1K   = 0x03,
 +	XGMAC_MTL_FIFO_SIZE_2K   = 0x07,
 +	XGMAC_MTL_FIFO_SIZE_4K   = 0x0f,
 +	XGMAC_MTL_FIFO_SIZE_8K   = 0x1f,
 +	XGMAC_MTL_FIFO_SIZE_16K  = 0x3f,
 +	XGMAC_MTL_FIFO_SIZE_32K  = 0x7f,
 +	XGMAC_MTL_FIFO_SIZE_64K  = 0xff,
 +	XGMAC_MTL_FIFO_SIZE_128K = 0x1ff,
 +	XGMAC_MTL_FIFO_SIZE_256K = 0x3ff,
++=======
+ enum xgbe_speed {
+ 	XGBE_SPEED_1000 = 0,
+ 	XGBE_SPEED_2500,
+ 	XGBE_SPEED_10000,
+ 	XGBE_SPEEDS,
+ };
+ 
+ enum xgbe_xpcs_access {
+ 	XGBE_XPCS_ACCESS_V1 = 0,
+ 	XGBE_XPCS_ACCESS_V2,
+ };
+ 
+ enum xgbe_an_mode {
+ 	XGBE_AN_MODE_CL73 = 0,
+ 	XGBE_AN_MODE_CL37,
+ 	XGBE_AN_MODE_CL37_SGMII,
+ 	XGBE_AN_MODE_NONE,
+ };
+ 
+ enum xgbe_an {
+ 	XGBE_AN_READY = 0,
+ 	XGBE_AN_PAGE_RECEIVED,
+ 	XGBE_AN_INCOMPAT_LINK,
+ 	XGBE_AN_COMPLETE,
+ 	XGBE_AN_NO_LINK,
+ 	XGBE_AN_ERROR,
+ };
+ 
+ enum xgbe_rx {
+ 	XGBE_RX_BPA = 0,
+ 	XGBE_RX_XNP,
+ 	XGBE_RX_COMPLETE,
+ 	XGBE_RX_ERROR,
+ };
+ 
+ enum xgbe_mode {
+ 	XGBE_MODE_KX_1000 = 0,
+ 	XGBE_MODE_KX_2500,
+ 	XGBE_MODE_KR,
+ 	XGBE_MODE_UNKNOWN,
+ };
+ 
+ enum xgbe_speedset {
+ 	XGBE_SPEEDSET_1000_10000 = 0,
+ 	XGBE_SPEEDSET_2500_10000,
+ };
+ 
+ struct xgbe_phy {
+ 	u32 supported;
+ 	u32 advertising;
+ 	u32 lp_advertising;
+ 
+ 	int address;
+ 
+ 	int autoneg;
+ 	int speed;
+ 	int duplex;
+ 
+ 	int link;
+ 
+ 	int pause_autoneg;
+ 	int tx_pause;
+ 	int rx_pause;
++>>>>>>> b03a4a6fb309 (amd-xgbe: Prepare for a new PCS register access method)
  };
  
  struct xgbe_mmc_stats {
@@@ -514,6 -801,11 +580,14 @@@ struct xgbe_hw_features 
  	unsigned int aux_snap_num;	/* Number of Aux snapshot inputs */
  };
  
++<<<<<<< HEAD
++=======
+ struct xgbe_version_data {
+ 	void (*init_function_ptrs_phy_impl)(struct xgbe_phy_if *);
+ 	enum xgbe_xpcs_access xpcs_access;
+ };
+ 
++>>>>>>> b03a4a6fb309 (amd-xgbe: Prepare for a new PCS register access method)
  struct xgbe_prv_data {
  	struct net_device *netdev;
  	struct platform_device *pdev;
@@@ -526,12 -830,23 +600,20 @@@
  	/* Overall device lock */
  	spinlock_t lock;
  
++<<<<<<< HEAD
 +	/* XPCS indirect addressing mutex */
 +	struct mutex xpcs_mutex;
++=======
+ 	/* XPCS indirect addressing lock */
+ 	spinlock_t xpcs_lock;
+ 	unsigned int xpcs_window;
+ 	unsigned int xpcs_window_size;
+ 	unsigned int xpcs_window_mask;
++>>>>>>> b03a4a6fb309 (amd-xgbe: Prepare for a new PCS register access method)
  
 -	/* RSS addressing mutex */
 -	struct mutex rss_mutex;
 -
 -	/* Flags representing xgbe_state */
 -	unsigned long dev_state;
 -
 -	int dev_irq;
 -	unsigned int per_channel_irq;
 +	int irq_number;
  
  	struct xgbe_hw_if hw_if;
 -	struct xgbe_phy_if phy_if;
  	struct xgbe_desc_if desc_if;
  
  	/* AXI DMA settings */
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-common.h
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dev.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-main.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe.h
