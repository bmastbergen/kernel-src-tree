fs/proc/task_mmu.c: reintroduce m->version logic

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [fs] proc/task_mmu.c: reintroduce m->version logic (Aaron Tomlin) [1425895]
Rebuild_FUZZ: 96.77%
commit-author Oleg Nesterov <oleg@redhat.com>
commit b8c20a9b85b057c850f63ee4c63531a356d8596a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/b8c20a9b.failed

Add the "last_addr" optimization back. Like before, every ->show()
method checks !seq_overflow() and sets m->version = vma->vm_start.

However, it also checks that m_next_vma(vma) != NULL, otherwise it
sets m->version = -1 for the lockless "EOF" fast-path in m_start().

m_start() can simply do find_vma() + m_next_vma() if last_addr is
not zero, the code looks clear and simple and this case is clearly
separated from "scan vmas" path.

	Signed-off-by: Oleg Nesterov <oleg@redhat.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Cyrill Gorcunov <gorcunov@openvz.org>
	Cc: "Eric W. Biederman" <ebiederm@xmission.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b8c20a9b85b057c850f63ee4c63531a356d8596a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/task_mmu.c
diff --cc fs/proc/task_mmu.c
index 09ca18a42ac3,c7228c2326d1..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -157,23 -138,32 +157,45 @@@ static void vma_stop(struct proc_maps_p
  	mmput(mm);
  }
  
++<<<<<<< HEAD
 +static void *m_start(struct seq_file *m, loff_t *pos)
++=======
+ static struct vm_area_struct *
+ m_next_vma(struct proc_maps_private *priv, struct vm_area_struct *vma)
+ {
+ 	if (vma == priv->tail_vma)
+ 		return NULL;
+ 	return vma->vm_next ?: priv->tail_vma;
+ }
+ 
+ static void m_cache_vma(struct seq_file *m, struct vm_area_struct *vma)
+ {
+ 	if (m->count < m->size)	/* vma is copied successfully */
+ 		m->version = m_next_vma(m->private, vma) ? vma->vm_start : -1UL;
+ }
+ 
+ static void *m_start(struct seq_file *m, loff_t *ppos)
++>>>>>>> b8c20a9b85b0 (fs/proc/task_mmu.c: reintroduce m->version logic)
  {
  	struct proc_maps_private *priv = m->private;
  	unsigned long last_addr = m->version;
  	struct mm_struct *mm;
 -	struct vm_area_struct *vma;
 -	unsigned int pos = *ppos;
 +	struct vm_area_struct *vma, *tail_vma = NULL;
 +	loff_t l = *pos;
 +
 +	/*
 +	 * We remember last_addr rather than next_addr to hit with
 +	 * mmap_cache most of the time. We have zero last_addr at
 +	 * the beginning and also after lseek. We will have -1 last_addr
 +	 * after the end of the vmas.
 +	 */
 +	if (last_addr == -1UL)
 +		return NULL;
  
+ 	/* See m_cache_vma(). Zero at the start or after lseek. */
+ 	if (last_addr == -1UL)
+ 		return NULL;
+ 
  	priv->task = get_pid_task(priv->pid, PIDTYPE_PID);
  	if (!priv->task)
  		return ERR_PTR(-ESRCH);
@@@ -181,41 -171,26 +203,55 @@@
  	mm = priv->mm;
  	if (!mm || !atomic_inc_not_zero(&mm->mm_users))
  		return NULL;
 -
  	down_read(&mm->mmap_sem);
 -	hold_task_mempolicy(priv);
 -	priv->tail_vma = get_gate_vma(mm);
  
++<<<<<<< HEAD
 +	tail_vma = get_gate_vma(mm);
 +	priv->tail_vma = tail_vma;
 +	hold_task_mempolicy(priv);
 +	/* Start with last addr hint */
 +	vma = find_vma(mm, last_addr);
 +	if (last_addr && vma) {
 +		vma = vma->vm_next;
 +		goto out;
++=======
+ 	if (last_addr) {
+ 		vma = find_vma(mm, last_addr);
+ 		if (vma && (vma = m_next_vma(priv, vma)))
+ 			return vma;
+ 	}
+ 
+ 	m->version = 0;
+ 	if (pos < mm->map_count) {
+ 		for (vma = mm->mmap; pos; pos--)
+ 			vma = vma->vm_next;
+ 		return vma;
++>>>>>>> b8c20a9b85b0 (fs/proc/task_mmu.c: reintroduce m->version logic)
 +	}
 +
 +	/*
 +	 * Check the vma index is within the range and do
 +	 * sequential scan until m_index.
 +	 */
 +	vma = NULL;
 +	if ((unsigned long)l < mm->map_count) {
 +		vma = mm->mmap;
 +		while (l-- && vma)
 +			vma = vma->vm_next;
 +		goto out;
  	}
  
 -	if (pos == mm->map_count && priv->tail_vma)
 -		return priv->tail_vma;
 +	if (l != mm->map_count)
 +		tail_vma = NULL; /* After gate vma */
 +
 +out:
 +	if (vma)
 +		return vma;
 +
 +	/* End of vmas has been reached */
 +	m->version = (tail_vma != NULL)? 0: -1UL;
 +	if (tail_vma)
 +		return tail_vma;
  
  	vma_stop(priv);
  	return NULL;
@@@ -379,14 -357,8 +415,19 @@@ done
  
  static int show_map(struct seq_file *m, void *v, int is_pid)
  {
++<<<<<<< HEAD
 +	struct vm_area_struct *vma = v;
 +	struct proc_maps_private *priv = m->private;
 +
 +	show_map_vma(m, vma, is_pid);
 +
 +	if (m->count < m->size)  /* vma is copied successfully */
 +		m->version = (vma != priv->tail_vma)
 +			? vma->vm_start : 0;
++=======
+ 	show_map_vma(m, v, is_pid);
+ 	m_cache_vma(m, v);
++>>>>>>> b8c20a9b85b0 (fs/proc/task_mmu.c: reintroduce m->version logic)
  	return 0;
  }
  
@@@ -723,10 -636,7 +764,14 @@@ static int show_smap(struct seq_file *m
  				mss.nonlinear >> 10);
  
  	show_smap_vma_flags(m, vma);
++<<<<<<< HEAD
 +
 +	if (m->count < m->size)  /* vma is copied successfully */
 +		m->version = (vma != priv->tail_vma)
 +			? vma->vm_start : 0;
++=======
+ 	m_cache_vma(m, vma);
++>>>>>>> b8c20a9b85b0 (fs/proc/task_mmu.c: reintroduce m->version logic)
  	return 0;
  }
  
@@@ -1561,16 -1486,12 +1606,20 @@@ static int show_numa_map(struct seq_fil
  	if (md->writeback)
  		seq_printf(m, " writeback=%lu", md->writeback);
  
 -	for_each_node_state(nid, N_MEMORY)
 -		if (md->node[nid])
 -			seq_printf(m, " N%d=%lu", nid, md->node[nid]);
 +	for_each_node_state(n, N_MEMORY)
 +		if (md->node[n])
 +			seq_printf(m, " N%d=%lu", n, md->node[n]);
 +
 +	seq_printf(m, " kernelpagesize_kB=%lu", vma_kernel_pagesize(vma) >> 10);
  out:
  	seq_putc(m, '\n');
++<<<<<<< HEAD
 +
 +	if (m->count < m->size)
 +		m->version = (vma != proc_priv->tail_vma) ? vma->vm_start : 0;
++=======
+ 	m_cache_vma(m, vma);
++>>>>>>> b8c20a9b85b0 (fs/proc/task_mmu.c: reintroduce m->version logic)
  	return 0;
  }
  
* Unmerged path fs/proc/task_mmu.c
