net/mlx5e: Refactor retrival of skb from rx completion element (cqe)

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Refactor retrival of skb from rx completion element (cqe) (Don Dutile) [1385330 1417285]
Rebuild_FUZZ: 96.97%
commit-author Or Gerlitz <ogerlitz@mellanox.com>
commit 8515c581dfa574420559d8cef24c2ba24e8eb8dd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/8515c581.failed

Factor the relevant code into a static inline helper (skb_from_cqe)
doing that.

Move the call to napi_gro_receive to be carried out just
after mlx5e_complete_rx_cqe returns.

Both changes are to be used for the VF representor as well
in the next commit.

This patch doesn't change any functionality.

	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 8515c581dfa574420559d8cef24c2ba24e8eb8dd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index d795e95774bc,e836e477f8b7..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -795,39 -629,174 +795,204 @@@ static inline void mlx5e_complete_rx_cq
  	rq->stats.packets++;
  	rq->stats.bytes += cqe_bcnt;
  	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
- 	napi_gro_receive(rq->cq.napi, skb);
  }
  
++<<<<<<< HEAD
 +void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 +{
 +	struct mlx5e_rx_wqe *wqe;
 +	struct sk_buff *skb;
 +	__be16 wqe_counter_be;
 +	u16 wqe_counter;
 +	u32 cqe_bcnt;
 +
 +	wqe_counter_be = cqe->wqe_counter;
 +	wqe_counter    = be16_to_cpu(wqe_counter_be);
 +	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
 +	skb            = rq->skb[wqe_counter];
 +	prefetch(skb->data);
 +	rq->skb[wqe_counter] = NULL;
 +
 +	dma_unmap_single(rq->pdev,
 +			 *((dma_addr_t *)skb->cb),
 +			 rq->wqe_sz,
 +			 DMA_FROM_DEVICE);
 +
 +	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
 +		rq->stats.wqe_err++;
 +		dev_kfree_skb(skb);
 +		goto wq_ll_pop;
 +	}
 +
 +	cqe_bcnt = be32_to_cpu(cqe->byte_cnt);
++=======
+ static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_sq *sq)
+ {
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u16 pi = (sq->pc - MLX5E_XDP_TX_WQEBBS) & wq->sz_m1; /* last pi */
+ 
+ 	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 
+ 	wqe->ctrl.fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
+ 	mlx5e_tx_notify_hw(sq, &wqe->ctrl, 0);
+ }
+ 
+ static inline void mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq,
+ 					struct mlx5e_dma_info *di,
+ 					unsigned int data_offset,
+ 					int len)
+ {
+ 	struct mlx5e_sq          *sq   = &rq->channel->xdp_sq;
+ 	struct mlx5_wq_cyc       *wq   = &sq->wq;
+ 	u16                      pi    = sq->pc & wq->sz_m1;
+ 	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 	struct mlx5e_sq_wqe_info *wi   = &sq->db.xdp.wqe_info[pi];
+ 
+ 	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 
+ 	dma_addr_t dma_addr  = di->addr + data_offset + MLX5E_XDP_MIN_INLINE;
+ 	unsigned int dma_len = len - MLX5E_XDP_MIN_INLINE;
+ 	void *data           = page_address(di->page) + data_offset;
+ 
+ 	if (unlikely(!mlx5e_sq_has_room_for(sq, MLX5E_XDP_TX_WQEBBS))) {
+ 		if (sq->db.xdp.doorbell) {
+ 			/* SQ is full, ring doorbell */
+ 			mlx5e_xmit_xdp_doorbell(sq);
+ 			sq->db.xdp.doorbell = false;
+ 		}
+ 		rq->stats.xdp_tx_full++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return;
+ 	}
+ 
+ 	dma_sync_single_for_device(sq->pdev, dma_addr, dma_len,
+ 				   PCI_DMA_TODEVICE);
+ 
+ 	memset(wqe, 0, sizeof(*wqe));
+ 
+ 	/* copy the inline part */
+ 	memcpy(eseg->inline_hdr_start, data, MLX5E_XDP_MIN_INLINE);
+ 	eseg->inline_hdr_sz = cpu_to_be16(MLX5E_XDP_MIN_INLINE);
+ 
+ 	dseg = (struct mlx5_wqe_data_seg *)cseg + (MLX5E_XDP_TX_DS_COUNT - 1);
+ 
+ 	/* write the dma part */
+ 	dseg->addr       = cpu_to_be64(dma_addr);
+ 	dseg->byte_count = cpu_to_be32(dma_len);
+ 	dseg->lkey       = sq->mkey_be;
+ 
+ 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_SEND);
+ 	cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | MLX5E_XDP_TX_DS_COUNT);
+ 
+ 	sq->db.xdp.di[pi] = *di;
+ 	wi->opcode     = MLX5_OPCODE_SEND;
+ 	wi->num_wqebbs = MLX5E_XDP_TX_WQEBBS;
+ 	sq->pc += MLX5E_XDP_TX_WQEBBS;
+ 
+ 	sq->db.xdp.doorbell = true;
+ 	rq->stats.xdp_tx++;
+ }
+ 
+ /* returns true if packet was consumed by xdp */
+ static inline bool mlx5e_xdp_handle(struct mlx5e_rq *rq,
+ 				    const struct bpf_prog *prog,
+ 				    struct mlx5e_dma_info *di,
+ 				    void *data, u16 len)
+ {
+ 	struct xdp_buff xdp;
+ 	u32 act;
+ 
+ 	if (!prog)
+ 		return false;
+ 
+ 	xdp.data = data;
+ 	xdp.data_end = xdp.data + len;
+ 	act = bpf_prog_run_xdp(prog, &xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		return false;
+ 	case XDP_TX:
+ 		mlx5e_xmit_xdp_frame(rq, di, MLX5_RX_HEADROOM, len);
+ 		return true;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 	case XDP_ABORTED:
+ 	case XDP_DROP:
+ 		rq->stats.xdp_drop++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return true;
+ 	}
+ }
+ 
+ static inline
+ struct sk_buff *skb_from_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
+ 			     u16 wqe_counter, u32 cqe_bcnt)
+ {
+ 	struct bpf_prog *xdp_prog = READ_ONCE(rq->xdp_prog);
+ 	struct mlx5e_dma_info *di;
+ 	struct sk_buff *skb;
+ 	void *va, *data;
+ 
+ 	di             = &rq->dma_info[wqe_counter];
+ 	va             = page_address(di->page);
+ 	data           = va + MLX5_RX_HEADROOM;
+ 
+ 	dma_sync_single_range_for_cpu(rq->pdev,
+ 				      di->addr,
+ 				      MLX5_RX_HEADROOM,
+ 				      rq->buff.wqe_sz,
+ 				      DMA_FROM_DEVICE);
+ 	prefetch(data);
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return NULL;
+ 	}
+ 
+ 	if (mlx5e_xdp_handle(rq, xdp_prog, di, data, cqe_bcnt))
+ 		return NULL; /* page/packet was consumed by XDP */
+ 
+ 	skb = build_skb(va, RQ_PAGE_SIZE(rq));
+ 	if (unlikely(!skb)) {
+ 		rq->stats.buff_alloc_err++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return NULL;
+ 	}
+ 
+ 	/* queue up for recycling ..*/
+ 	page_ref_inc(di->page);
+ 	mlx5e_page_release(rq, di, true);
+ 
+ 	skb_reserve(skb, MLX5_RX_HEADROOM);
++>>>>>>> 8515c581dfa5 (net/mlx5e: Refactor retrival of skb from rx completion element (cqe))
  	skb_put(skb, cqe_bcnt);
  
+ 	return skb;
+ }
+ 
+ void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_rx_wqe *wqe;
+ 	__be16 wqe_counter_be;
+ 	struct sk_buff *skb;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	cqe_bcnt       = be32_to_cpu(cqe->byte_cnt);
+ 
+ 	skb = skb_from_cqe(rq, cqe, wqe_counter, cqe_bcnt);
+ 	if (!skb)
+ 		goto wq_ll_pop;
+ 
  	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
  
  wq_ll_pop:
  	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
