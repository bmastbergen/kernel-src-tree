ntb: add DMA error handling for RX DMA

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [ntb] add DMA error handling for RX DMA (Suravee Suthikulpanit) [1303727]
Rebuild_FUZZ: 92.96%
commit-author Dave Jiang <dave.jiang@intel.com>
commit 72203572afd7aef243c182f19925e5a77a1dc6a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/72203572.failed

Adding support on the rx DMA path to allow recovery of errors when
DMA responds with error status and abort all the subsequent ops.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
	Acked-by: Allen Hubbe <Allen.Hubbe@emc.com>
	Cc: Jon Mason <jdmason@kudzu.us>
	Cc: linux-ntb@googlegroups.com
	Signed-off-by: Vinod Koul <vinod.koul@intel.com>
(cherry picked from commit 72203572afd7aef243c182f19925e5a77a1dc6a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/ntb/ntb_transport.c
diff --cc drivers/ntb/ntb_transport.c
index cc39efa77597,8601c10acf74..000000000000
--- a/drivers/ntb/ntb_transport.c
+++ b/drivers/ntb/ntb_transport.c
@@@ -80,6 -102,10 +80,13 @@@ struct ntb_queue_entry 
  	void *buf;
  	unsigned int len;
  	unsigned int flags;
++<<<<<<< HEAD
++=======
+ 	int retries;
+ 	int errors;
+ 	unsigned int tx_index;
+ 	unsigned int rx_index;
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  
  	struct ntb_transport_qp *qp;
  	union {
@@@ -200,11 -247,30 +206,26 @@@ enum 
  	MAX_SPAD,
  };
  
 -#define dev_client_dev(__dev) \
 -	container_of((__dev), struct ntb_transport_client_dev, dev)
 -
 -#define drv_client(__drv) \
 -	container_of((__drv), struct ntb_transport_client, driver)
 -
 -#define QP_TO_MW(nt, qp)	((qp) % nt->mw_count)
 +#define QP_TO_MW(ndev, qp)	((qp) % ntb_max_mw(ndev))
  #define NTB_QP_DEF_NUM_ENTRIES	100
  #define NTB_LINK_DOWN_TIMEOUT	10
 -#define DMA_RETRIES		20
 -#define DMA_OUT_RESOURCE_TO	50
  
++<<<<<<< HEAD
 +static int ntb_match_bus(struct device *dev, struct device_driver *drv)
++=======
+ static void ntb_transport_rxc_db(unsigned long data);
+ static const struct ntb_ctx_ops ntb_transport_ops;
+ static struct ntb_client ntb_transport_client;
+ static int ntb_async_tx_submit(struct ntb_transport_qp *qp,
+ 			       struct ntb_queue_entry *entry);
+ static void ntb_memcpy_tx(struct ntb_queue_entry *entry, void __iomem *offset);
+ static int ntb_async_rx_submit(struct ntb_queue_entry *entry, void *offset);
+ static void ntb_memcpy_rx(struct ntb_queue_entry *entry, void *offset);
+ 
+ 
+ static int ntb_transport_bus_match(struct device *dev,
+ 				   struct device_driver *drv)
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  {
  	return !strncmp(dev_name(dev), drv->name, strlen(drv->name));
  }
@@@ -1027,24 -1222,73 +1048,92 @@@ void ntb_transport_free(void *transport
  	kfree(nt);
  }
  
++<<<<<<< HEAD
 +static void ntb_rx_copy_callback(void *data)
++=======
+ static void ntb_complete_rxc(struct ntb_transport_qp *qp)
+ {
+ 	struct ntb_queue_entry *entry;
+ 	void *cb_data;
+ 	unsigned int len;
+ 	unsigned long irqflags;
+ 
+ 	spin_lock_irqsave(&qp->ntb_rx_q_lock, irqflags);
+ 
+ 	while (!list_empty(&qp->rx_post_q)) {
+ 		entry = list_first_entry(&qp->rx_post_q,
+ 					 struct ntb_queue_entry, entry);
+ 		if (!(entry->flags & DESC_DONE_FLAG))
+ 			break;
+ 
+ 		entry->rx_hdr->flags = 0;
+ 		iowrite32(entry->rx_index, &qp->rx_info->entry);
+ 
+ 		cb_data = entry->cb_data;
+ 		len = entry->len;
+ 
+ 		list_move_tail(&entry->entry, &qp->rx_free_q);
+ 
+ 		spin_unlock_irqrestore(&qp->ntb_rx_q_lock, irqflags);
+ 
+ 		if (qp->rx_handler && qp->client_ready)
+ 			qp->rx_handler(qp, qp->cb_data, cb_data, len);
+ 
+ 		spin_lock_irqsave(&qp->ntb_rx_q_lock, irqflags);
+ 	}
+ 
+ 	spin_unlock_irqrestore(&qp->ntb_rx_q_lock, irqflags);
+ }
+ 
+ static void ntb_rx_copy_callback(void *data,
+ 				 const struct dmaengine_result *res)
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  {
  	struct ntb_queue_entry *entry = data;
 +	struct ntb_transport_qp *qp = entry->qp;
 +	void *cb_data = entry->cb_data;
 +	unsigned int len = entry->len;
 +	struct ntb_payload_header *hdr = entry->rx_hdr;
  
++<<<<<<< HEAD
 +	/* Ensure that the data is fully copied out before clearing the flag */
 +	wmb();
 +	hdr->flags = 0;
++=======
+ 	/* we need to check DMA results if we are using DMA */
+ 	if (res) {
+ 		enum dmaengine_tx_result dma_err = res->result;
+ 
+ 		switch (dma_err) {
+ 		case DMA_TRANS_READ_FAILED:
+ 		case DMA_TRANS_WRITE_FAILED:
+ 			entry->errors++;
+ 		case DMA_TRANS_ABORTED:
+ 		{
+ 			struct ntb_transport_qp *qp = entry->qp;
+ 			void *offset = qp->rx_buff + qp->rx_max_frame *
+ 					qp->rx_index;
+ 
+ 			ntb_memcpy_rx(entry, offset);
+ 			qp->rx_memcpy++;
+ 			return;
+ 		}
+ 
+ 		case DMA_TRANS_NOERROR:
+ 		default:
+ 			break;
+ 		}
+ 	}
+ 
+ 	entry->flags |= DESC_DONE_FLAG;
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
 +
 +	iowrite32(entry->index, &qp->rx_info->entry);
 +
 +	ntb_list_add(&qp->ntb_rx_free_q_lock, &entry->entry, &qp->rx_free_q);
  
 -	ntb_complete_rxc(entry->qp);
 +	if (qp->rx_handler && qp->client_ready == NTB_LINK_UP)
 +		qp->rx_handler(qp, qp->cb_data, cb_data, len);
  }
  
  static void ntb_memcpy_rx(struct ntb_queue_entry *entry, void *offset)
@@@ -1054,11 -1298,13 +1143,21 @@@
  
  	memcpy(buf, offset, len);
  
++<<<<<<< HEAD
 +	ntb_rx_copy_callback(entry);
 +}
 +
 +static void ntb_async_rx(struct ntb_queue_entry *entry, void *offset,
 +			 size_t len)
++=======
+ 	/* Ensure that the data is fully copied out before clearing the flag */
+ 	wmb();
+ 
+ 	ntb_rx_copy_callback(entry, NULL);
+ }
+ 
+ static int ntb_async_rx_submit(struct ntb_queue_entry *entry, void *offset)
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  {
  	struct dma_async_tx_descriptor *txd;
  	struct ntb_transport_qp *qp = entry->qp;
@@@ -1068,22 -1314,15 +1167,26 @@@
  	struct dmaengine_unmap_data *unmap;
  	dma_cookie_t cookie;
  	void *buf = entry->buf;
 -	int retries = 0;
 +	unsigned long flags;
 +
++<<<<<<< HEAD
 +	entry->len = len;
  
 +	if (!chan)
 +		goto err;
 +
 +	if (len < copy_bytes) 
 +		goto err_wait;
 +
++=======
+ 	len = entry->len;
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  	device = chan->device;
 -	pay_off = (size_t)offset & ~PAGE_MASK;
 -	buff_off = (size_t)buf & ~PAGE_MASK;
 +	pay_off = (size_t) offset & ~PAGE_MASK;
 +	buff_off = (size_t) buf & ~PAGE_MASK;
  
  	if (!is_dma_copy_aligned(device, pay_off, buff_off, len))
 -		goto err;
 +		goto err_wait;
  
  	unmap = dmaengine_get_unmap_data(device->dev, 2, GFP_NOWAIT);
  	if (!unmap)
@@@ -1104,14 -1343,24 +1207,31 @@@
  
  	unmap->from_cnt = 1;
  
++<<<<<<< HEAD
 +	flags = DMA_COMPL_SKIP_SRC_UNMAP | DMA_COMPL_SKIP_DEST_UNMAP |
 +		DMA_PREP_INTERRUPT;
 +	txd = device->device_prep_dma_memcpy(chan, unmap->addr[1],
 +					     unmap->addr[0], len, flags);
 +	if (!txd)
++=======
+ 	for (retries = 0; retries < DMA_RETRIES; retries++) {
+ 		txd = device->device_prep_dma_memcpy(chan,
+ 						     unmap->addr[1],
+ 						     unmap->addr[0], len,
+ 						     DMA_PREP_INTERRUPT);
+ 		if (txd)
+ 			break;
+ 
+ 		set_current_state(TASK_INTERRUPTIBLE);
+ 		schedule_timeout(DMA_OUT_RESOURCE_TO);
+ 	}
+ 
+ 	if (!txd) {
+ 		qp->dma_rx_prep_err++;
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  		goto err_get_unmap;
 -	}
  
- 	txd->callback = ntb_rx_copy_callback;
+ 	txd->callback_result = ntb_rx_copy_callback;
  	txd->callback_param = entry;
  	dma_set_unmap(txd, unmap);
  
@@@ -1131,12 -1380,31 +1251,37 @@@ err_set_unmap
  	dmaengine_unmap_put(unmap);
  err_get_unmap:
  	dmaengine_unmap_put(unmap);
 +err_wait:
 +	/* If the callbacks come out of order, the writing of the index to the
 +	 * last completed will be out of order.  This may result in the
 +	 * receive stalling forever.
 +	 */
 +	dma_sync_wait(chan, qp->last_cookie);
+ err:
+ 	return -ENXIO;
+ }
+ 
+ static void ntb_async_rx(struct ntb_queue_entry *entry, void *offset)
+ {
+ 	struct ntb_transport_qp *qp = entry->qp;
+ 	struct dma_chan *chan = qp->rx_dma_chan;
+ 	int res;
+ 
+ 	if (!chan)
+ 		goto err;
+ 
+ 	if (entry->len < copy_bytes)
+ 		goto err;
+ 
+ 	res = ntb_async_rx_submit(entry, offset);
+ 	if (res < 0)
+ 		goto err;
+ 
+ 	if (!entry->retries)
+ 		qp->rx_async++;
+ 
+ 	return;
+ 
  err:
  	ntb_memcpy_rx(entry, offset);
  	qp->rx_memcpy++;
@@@ -1177,34 -1443,39 +1322,39 @@@ static int ntb_process_rxc(struct ntb_t
  		return -EIO;
  	}
  
 -	entry = ntb_list_mv(&qp->ntb_rx_q_lock, &qp->rx_pend_q, &qp->rx_post_q);
 -	if (!entry) {
 -		dev_dbg(&qp->ndev->pdev->dev, "no receive buffer\n");
 -		qp->rx_err_no_buf++;
 -		return -EAGAIN;
 +	if (hdr->flags & LINK_DOWN_FLAG) {
 +		ntb_qp_link_down(qp);
 +
 +		goto err;
  	}
  
++<<<<<<< HEAD
 +	dev_dbg(&ntb_query_pdev(qp->ndev)->dev,
 +		"rx offset %u, ver %u - %d payload received, buf size %d\n",
 +		qp->rx_index, hdr->ver, hdr->len, entry->len);
 +
 +	qp->rx_bytes += hdr->len;
 +	qp->rx_pkts++;
++=======
+ 	entry->rx_hdr = hdr;
+ 	entry->rx_index = qp->rx_index;
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  
  	if (hdr->len > entry->len) {
 -		dev_dbg(&qp->ndev->pdev->dev,
 -			"receive buffer overflow! Wanted %d got %d\n",
 -			hdr->len, entry->len);
  		qp->rx_err_oflow++;
 +		dev_dbg(&ntb_query_pdev(qp->ndev)->dev,
 +			"RX overflow! Wanted %d got %d\n",
 +			hdr->len, entry->len);
  
 -		entry->len = -EIO;
 -		entry->flags |= DESC_DONE_FLAG;
 -
 -		ntb_complete_rxc(qp);
 -	} else {
 -		dev_dbg(&qp->ndev->pdev->dev,
 -			"RX OK index %u ver %u size %d into buf size %d\n",
 -			qp->rx_index, hdr->ver, hdr->len, entry->len);
 -
 -		qp->rx_bytes += hdr->len;
 -		qp->rx_pkts++;
 +		goto err;
 +	}
  
 -		entry->len = hdr->len;
 +	entry->index = qp->rx_index;
 +	entry->rx_hdr = hdr;
  
 -		ntb_async_rx(entry, offset);
 -	}
 +	ntb_async_rx(entry, offset, hdr->len);
  
 +out:
  	qp->rx_index++;
  	qp->rx_index %= qp->rx_max_entry;
  
@@@ -1623,8 -2028,15 +1773,15 @@@ int ntb_transport_rx_enqueue(struct ntb
  	entry->cb_data = cb;
  	entry->buf = data;
  	entry->len = len;
++<<<<<<< HEAD
++=======
+ 	entry->flags = 0;
+ 	entry->retries = 0;
+ 	entry->errors = 0;
+ 	entry->rx_index = 0;
++>>>>>>> 72203572afd7 (ntb: add DMA error handling for RX DMA)
  
 -	ntb_list_add(&qp->ntb_rx_q_lock, &entry->entry, &qp->rx_pend_q);
 -
 -	if (qp->active)
 -		tasklet_schedule(&qp->rxc_db_work);
 +	ntb_list_add(&qp->ntb_rx_pend_q_lock, &entry->entry, &qp->rx_pend_q);
  
  	return 0;
  }
* Unmerged path drivers/ntb/ntb_transport.c
