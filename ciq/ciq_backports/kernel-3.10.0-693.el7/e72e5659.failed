i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Scott Peterson <scott.d.peterson@intel.com>
commit e72e56597ba15ce70f4fc1eb2ceeaa8da0d4ab5e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e72e5659.failed

This patch reduces the size of struct i40e_rx_buffer by one pointer,
and makes the i40e driver a little more consistent with the igb driver
in terms of packets that span buffers.

We do this by moving the skb field from struct i40e_rx_buffer to
struct i40e_ring. We pass the skb we already have (or NULL if we
don't) to i40e_fetch_rx_buffer(), which skips the skb allocation if we
already have one for this packet.

Change-ID: I4ad48a531844494ba0c5d8e1a62209a057f661b0
	Signed-off-by: Scott Peterson <scott.d.peterson@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit e72e56597ba15ce70f4fc1eb2ceeaa8da0d4ab5e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_txrx.c
#	drivers/net/ethernet/intel/i40e/i40e_txrx.h
#	drivers/net/ethernet/intel/i40evf/i40e_txrx.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.c
index f75ac6dc63cd,aa9755b55d48..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@@ -1019,32 -1013,23 +1019,50 @@@ void i40e_clean_rx_ring(struct i40e_rin
  	if (!rx_ring->rx_bi)
  		return;
  
+ 	if (rx_ring->skb) {
+ 		dev_kfree_skb(rx_ring->skb);
+ 		rx_ring->skb = NULL;
+ 	}
+ 
  	/* Free all the Rx ring sk_buffs */
  	for (i = 0; i < rx_ring->count; i++) {
++<<<<<<< HEAD
 +		rx_bi = &rx_ring->rx_bi[i];
 +		if (rx_bi->dma) {
 +			dma_unmap_single(dev,
 +					 rx_bi->dma,
 +					 rx_ring->rx_buf_len,
 +					 DMA_FROM_DEVICE);
 +			rx_bi->dma = 0;
 +		}
 +		if (rx_bi->skb) {
 +			dev_kfree_skb(rx_bi->skb);
 +			rx_bi->skb = NULL;
 +		}
 +		if (rx_bi->page) {
 +			if (rx_bi->page_dma) {
 +				dma_unmap_page(dev,
 +					       rx_bi->page_dma,
 +					       PAGE_SIZE,
 +					       DMA_FROM_DEVICE);
 +				rx_bi->page_dma = 0;
 +			}
 +			__free_page(rx_bi->page);
 +			rx_bi->page = NULL;
 +			rx_bi->page_offset = 0;
 +		}
++=======
+ 		struct i40e_rx_buffer *rx_bi = &rx_ring->rx_bi[i];
+ 
+ 		if (!rx_bi->page)
+ 			continue;
+ 
+ 		dma_unmap_page(dev, rx_bi->dma, PAGE_SIZE, DMA_FROM_DEVICE);
+ 		__free_pages(rx_bi->page, 0);
+ 
+ 		rx_bi->page = NULL;
+ 		rx_bi->page_offset = 0;
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
  	}
  
  	bi_size = sizeof(struct i40e_rx_buffer) * rx_ring->count;
@@@ -1474,29 -1390,352 +1492,362 @@@ static inline void i40e_rx_hash(struct 
  }
  
  /**
 - * i40e_process_skb_fields - Populate skb header fields from Rx descriptor
 - * @rx_ring: rx descriptor ring packet is being transacted on
 - * @rx_desc: pointer to the EOP Rx descriptor
 - * @skb: pointer to current skb being populated
 - * @rx_ptype: the packet type decoded by hardware
 + * i40e_clean_rx_irq_1buf - Reclaim resources after receive; single buffer
 + * @rx_ring:  rx ring to clean
 + * @budget:   how many cleans we're allowed
   *
 - * This function checks the ring, descriptor, and packet information in
 - * order to populate the hash, checksum, VLAN, protocol, and
 - * other fields within the skb.
 + * Returns number of packets cleaned
   **/
++<<<<<<< HEAD
 +static int i40e_clean_rx_irq_1buf(struct i40e_ring *rx_ring, int budget)
++=======
+ static inline
+ void i40e_process_skb_fields(struct i40e_ring *rx_ring,
+ 			     union i40e_rx_desc *rx_desc, struct sk_buff *skb,
+ 			     u8 rx_ptype)
+ {
+ 	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	u32 rx_status = (qword & I40E_RXD_QW1_STATUS_MASK) >>
+ 			I40E_RXD_QW1_STATUS_SHIFT;
+ 	u32 tsynvalid = rx_status & I40E_RXD_QW1_STATUS_TSYNVALID_MASK;
+ 	u32 tsyn = (rx_status & I40E_RXD_QW1_STATUS_TSYNINDX_MASK) >>
+ 		   I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT;
+ 
+ 	if (unlikely(tsynvalid))
+ 		i40e_ptp_rx_hwtstamp(rx_ring->vsi->back, skb, tsyn);
+ 
+ 	i40e_rx_hash(rx_ring, rx_desc, skb, rx_ptype);
+ 
+ 	/* modifies the skb - consumes the enet header */
+ 	skb->protocol = eth_type_trans(skb, rx_ring->netdev);
+ 
+ 	i40e_rx_checksum(rx_ring->vsi, skb, rx_desc);
+ 
+ 	skb_record_rx_queue(skb, rx_ring->queue_index);
+ }
+ 
+ /**
+  * i40e_pull_tail - i40e specific version of skb_pull_tail
+  * @rx_ring: rx descriptor ring packet is being transacted on
+  * @skb: pointer to current skb being adjusted
+  *
+  * This function is an i40e specific version of __pskb_pull_tail.  The
+  * main difference between this version and the original function is that
+  * this function can make several assumptions about the state of things
+  * that allow for significant optimizations versus the standard function.
+  * As a result we can do things like drop a frag and maintain an accurate
+  * truesize for the skb.
+  */
+ static void i40e_pull_tail(struct i40e_ring *rx_ring, struct sk_buff *skb)
+ {
+ 	struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+ 	unsigned char *va;
+ 	unsigned int pull_len;
+ 
+ 	/* it is valid to use page_address instead of kmap since we are
+ 	 * working with pages allocated out of the lomem pool per
+ 	 * alloc_page(GFP_ATOMIC)
+ 	 */
+ 	va = skb_frag_address(frag);
+ 
+ 	/* we need the header to contain the greater of either ETH_HLEN or
+ 	 * 60 bytes if the skb->len is less than 60 for skb_pad.
+ 	 */
+ 	pull_len = eth_get_headlen(va, I40E_RX_HDR_SIZE);
+ 
+ 	/* align pull length to size of long to optimize memcpy performance */
+ 	skb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));
+ 
+ 	/* update all of the pointers */
+ 	skb_frag_size_sub(frag, pull_len);
+ 	frag->page_offset += pull_len;
+ 	skb->data_len -= pull_len;
+ 	skb->tail += pull_len;
+ }
+ 
+ /**
+  * i40e_cleanup_headers - Correct empty headers
+  * @rx_ring: rx descriptor ring packet is being transacted on
+  * @skb: pointer to current skb being fixed
+  *
+  * Also address the case where we are pulling data in on pages only
+  * and as such no data is present in the skb header.
+  *
+  * In addition if skb is not at least 60 bytes we need to pad it so that
+  * it is large enough to qualify as a valid Ethernet frame.
+  *
+  * Returns true if an error was encountered and skb was freed.
+  **/
+ static bool i40e_cleanup_headers(struct i40e_ring *rx_ring, struct sk_buff *skb)
+ {
+ 	/* place header in linear portion of buffer */
+ 	if (skb_is_nonlinear(skb))
+ 		i40e_pull_tail(rx_ring, skb);
+ 
+ 	/* if eth_skb_pad returns an error the skb was freed */
+ 	if (eth_skb_pad(skb))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /**
+  * i40e_reuse_rx_page - page flip buffer and store it back on the ring
+  * @rx_ring: rx descriptor ring to store buffers on
+  * @old_buff: donor buffer to have page reused
+  *
+  * Synchronizes page for reuse by the adapter
+  **/
+ static void i40e_reuse_rx_page(struct i40e_ring *rx_ring,
+ 			       struct i40e_rx_buffer *old_buff)
+ {
+ 	struct i40e_rx_buffer *new_buff;
+ 	u16 nta = rx_ring->next_to_alloc;
+ 
+ 	new_buff = &rx_ring->rx_bi[nta];
+ 
+ 	/* update, and store next to alloc */
+ 	nta++;
+ 	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
+ 
+ 	/* transfer page from old buffer to new buffer */
+ 	*new_buff = *old_buff;
+ }
+ 
+ /**
+  * i40e_page_is_reserved - check if reuse is possible
+  * @page: page struct to check
+  */
+ static inline bool i40e_page_is_reserved(struct page *page)
+ {
+ 	return (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);
+ }
+ 
+ /**
+  * i40e_add_rx_frag - Add contents of Rx buffer to sk_buff
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_buffer: buffer containing page to add
+  * @size: packet length from rx_desc
+  * @skb: sk_buff to place the data into
+  *
+  * This function will add the data contained in rx_buffer->page to the skb.
+  * This is done either through a direct copy if the data in the buffer is
+  * less than the skb header size, otherwise it will just attach the page as
+  * a frag to the skb.
+  *
+  * The function will then update the page offset if necessary and return
+  * true if the buffer can be reused by the adapter.
+  **/
+ static bool i40e_add_rx_frag(struct i40e_ring *rx_ring,
+ 			     struct i40e_rx_buffer *rx_buffer,
+ 			     unsigned int size,
+ 			     struct sk_buff *skb)
+ {
+ 	struct page *page = rx_buffer->page;
+ #if (PAGE_SIZE < 8192)
+ 	unsigned int truesize = I40E_RXBUFFER_2048;
+ #else
+ 	unsigned int truesize = ALIGN(size, L1_CACHE_BYTES);
+ 	unsigned int last_offset = PAGE_SIZE - I40E_RXBUFFER_2048;
+ #endif
+ 
+ 	/* will the data fit in the skb we allocated? if so, just
+ 	 * copy it as it is pretty small anyway
+ 	 */
+ 	if ((size <= I40E_RX_HDR_SIZE) && !skb_is_nonlinear(skb)) {
+ 		unsigned char *va = page_address(page) + rx_buffer->page_offset;
+ 
+ 		memcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));
+ 
+ 		/* page is not reserved, we can reuse buffer as-is */
+ 		if (likely(!i40e_page_is_reserved(page)))
+ 			return true;
+ 
+ 		/* this page cannot be reused so discard it */
+ 		__free_pages(page, 0);
+ 		return false;
+ 	}
+ 
+ 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
+ 			rx_buffer->page_offset, size, truesize);
+ 
+ 	/* avoid re-using remote pages */
+ 	if (unlikely(i40e_page_is_reserved(page)))
+ 		return false;
+ 
+ #if (PAGE_SIZE < 8192)
+ 	/* if we are only owner of page we can reuse it */
+ 	if (unlikely(page_count(page) != 1))
+ 		return false;
+ 
+ 	/* flip page offset to other buffer */
+ 	rx_buffer->page_offset ^= truesize;
+ #else
+ 	/* move offset up to the next cache line */
+ 	rx_buffer->page_offset += truesize;
+ 
+ 	if (rx_buffer->page_offset > last_offset)
+ 		return false;
+ #endif
+ 
+ 	/* Even if we own the page, we are not allowed to use atomic_set()
+ 	 * This would break get_page_unless_zero() users.
+ 	 */
+ 	get_page(rx_buffer->page);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_fetch_rx_buffer - Allocate skb and populate it
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_desc: descriptor containing info written by hardware
+  *
+  * This function allocates an skb on the fly, and populates it with the page
+  * data from the current receive descriptor, taking care to set up the skb
+  * correctly, as well as handling calling the page recycle function if
+  * necessary.
+  */
+ static inline
+ struct sk_buff *i40e_fetch_rx_buffer(struct i40e_ring *rx_ring,
+ 				     union i40e_rx_desc *rx_desc,
+ 				     struct sk_buff *skb)
+ {
+ 	u64 local_status_error_len =
+ 		le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	unsigned int size =
+ 		(local_status_error_len & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
+ 		I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
+ 	struct i40e_rx_buffer *rx_buffer;
+ 	struct page *page;
+ 
+ 	rx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];
+ 	page = rx_buffer->page;
+ 	prefetchw(page);
+ 
+ 	if (likely(!skb)) {
+ 		void *page_addr = page_address(page) + rx_buffer->page_offset;
+ 
+ 		/* prefetch first cache line of first page */
+ 		prefetch(page_addr);
+ #if L1_CACHE_BYTES < 128
+ 		prefetch(page_addr + L1_CACHE_BYTES);
+ #endif
+ 
+ 		/* allocate a skb to store the frags */
+ 		skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
+ 				       I40E_RX_HDR_SIZE,
+ 				       GFP_ATOMIC | __GFP_NOWARN);
+ 		if (unlikely(!skb)) {
+ 			rx_ring->rx_stats.alloc_buff_failed++;
+ 			return NULL;
+ 		}
+ 
+ 		/* we will be copying header into skb->data in
+ 		 * pskb_may_pull so it is in our interest to prefetch
+ 		 * it now to avoid a possible cache miss
+ 		 */
+ 		prefetchw(skb->data);
+ 	}
+ 
+ 	/* we are reusing so sync this buffer for CPU use */
+ 	dma_sync_single_range_for_cpu(rx_ring->dev,
+ 				      rx_buffer->dma,
+ 				      rx_buffer->page_offset,
+ 				      size,
+ 				      DMA_FROM_DEVICE);
+ 
+ 	/* pull page into skb */
+ 	if (i40e_add_rx_frag(rx_ring, rx_buffer, size, skb)) {
+ 		/* hand second half of page back to the ring */
+ 		i40e_reuse_rx_page(rx_ring, rx_buffer);
+ 		rx_ring->rx_stats.page_reuse_count++;
+ 	} else {
+ 		/* we are not reusing the buffer so unmap it */
+ 		dma_unmap_page(rx_ring->dev, rx_buffer->dma, PAGE_SIZE,
+ 			       DMA_FROM_DEVICE);
+ 	}
+ 
+ 	/* clear contents of buffer_info */
+ 	rx_buffer->page = NULL;
+ 
+ 	return skb;
+ }
+ 
+ /**
+  * i40e_is_non_eop - process handling of non-EOP buffers
+  * @rx_ring: Rx ring being processed
+  * @rx_desc: Rx descriptor for current buffer
+  * @skb: Current socket buffer containing buffer in progress
+  *
+  * This function updates next to clean.  If the buffer is an EOP buffer
+  * this function exits returning false, otherwise it will place the
+  * sk_buff in the next buffer to be chained and return true indicating
+  * that this is in fact a non-EOP buffer.
+  **/
+ static bool i40e_is_non_eop(struct i40e_ring *rx_ring,
+ 			    union i40e_rx_desc *rx_desc,
+ 			    struct sk_buff *skb)
+ {
+ 	u32 ntc = rx_ring->next_to_clean + 1;
+ 
+ 	/* fetch, update, and store next to clean */
+ 	ntc = (ntc < rx_ring->count) ? ntc : 0;
+ 	rx_ring->next_to_clean = ntc;
+ 
+ 	prefetch(I40E_RX_DESC(rx_ring, ntc));
+ 
+ #define staterrlen rx_desc->wb.qword1.status_error_len
+ 	if (unlikely(i40e_rx_is_programming_status(le64_to_cpu(staterrlen)))) {
+ 		i40e_clean_programming_status(rx_ring, rx_desc);
+ 		return true;
+ 	}
+ 	/* if we are the last buffer then there is nothing else to do */
+ #define I40E_RXD_EOF BIT(I40E_RX_DESC_STATUS_EOF_SHIFT)
+ 	if (likely(i40e_test_staterr(rx_desc, I40E_RXD_EOF)))
+ 		return false;
+ 
+ 	rx_ring->rx_stats.non_eop_descs++;
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_clean_rx_irq - Clean completed descriptors from Rx ring - bounce buf
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @budget: Total limit on number of packets to process
+  *
+  * This function provides a "bounce buffer" approach to Rx interrupt
+  * processing.  The advantage to this is that on systems that have
+  * expensive overhead for IOMMU access this provides a means of avoiding
+  * it by maintaining the mapping of the page to the system.
+  *
+  * Returns amount of work completed
+  **/
+ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
  {
  	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
+ 	struct sk_buff *skb = rx_ring->skb;
  	u16 cleaned_count = I40E_DESC_UNUSED(rx_ring);
 +	struct i40e_vsi *vsi = rx_ring->vsi;
 +	union i40e_rx_desc *rx_desc;
 +	u32 rx_error, rx_status;
 +	u16 rx_packet_len;
  	bool failure = false;
 +	u8 rx_ptype;
 +	u64 qword;
 +	u16 i;
  
++<<<<<<< HEAD
 +	do {
 +		struct i40e_rx_buffer *rx_bi;
 +		struct sk_buff *skb;
++=======
+ 	while (likely(total_rx_packets < budget)) {
+ 		union i40e_rx_desc *rx_desc;
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
  		u16 vlan_tag;
 -		u8 rx_ptype;
 -		u64 qword;
 -
  		/* return some buffers to hardware, one at a time is too slow */
  		if (cleaned_count >= I40E_RX_BUFFER_WRITE) {
  			failure = failure ||
@@@ -1520,55 -1760,28 +1871,67 @@@
  		 */
  		dma_rmb();
  
++<<<<<<< HEAD
 +		if (i40e_rx_is_programming_status(qword)) {
 +			i40e_clean_programming_status(rx_ring, rx_desc);
 +			I40E_RX_INCREMENT(rx_ring, i);
 +			continue;
 +		}
 +		rx_bi = &rx_ring->rx_bi[i];
 +		skb = rx_bi->skb;
 +		prefetch(skb->data);
++=======
+ 		skb = i40e_fetch_rx_buffer(rx_ring, rx_desc, skb);
+ 		if (!skb)
+ 			break;
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
 +
 +		rx_packet_len = (qword & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
 +				I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
  
 +		rx_error = (qword & I40E_RXD_QW1_ERROR_MASK) >>
 +			   I40E_RXD_QW1_ERROR_SHIFT;
 +		rx_error &= ~BIT(I40E_RX_DESC_ERROR_HBO_SHIFT);
 +
 +		rx_ptype = (qword & I40E_RXD_QW1_PTYPE_MASK) >>
 +			   I40E_RXD_QW1_PTYPE_SHIFT;
 +		rx_bi->skb = NULL;
  		cleaned_count++;
  
 -		if (i40e_is_non_eop(rx_ring, rx_desc, skb))
 +		/* Get the header and possibly the whole packet
 +		 * If this is an skb from previous receive dma will be 0
 +		 */
 +		skb_put(skb, rx_packet_len);
 +		dma_unmap_single(rx_ring->dev, rx_bi->dma, rx_ring->rx_buf_len,
 +				 DMA_FROM_DEVICE);
 +		rx_bi->dma = 0;
 +
 +		I40E_RX_INCREMENT(rx_ring, i);
 +
 +		if (unlikely(
 +		    !(rx_status & BIT(I40E_RX_DESC_STATUS_EOF_SHIFT)))) {
 +			rx_ring->rx_stats.non_eop_descs++;
  			continue;
 +		}
  
 -		/* ERR_MASK will only have valid bits if EOP set, and
 -		 * what we are doing here is actually checking
 -		 * I40E_RX_DESC_ERROR_RXE_SHIFT, since it is the zeroth bit in
 -		 * the error field
 -		 */
 -		if (unlikely(i40e_test_staterr(rx_desc, BIT(I40E_RXD_QW1_ERROR_SHIFT)))) {
 +		/* ERR_MASK will only have valid bits if EOP set */
 +		if (unlikely(rx_error & BIT(I40E_RX_DESC_ERROR_RXE_SHIFT))) {
  			dev_kfree_skb_any(skb);
  			continue;
  		}
  
++<<<<<<< HEAD
 +		i40e_rx_hash(rx_ring, rx_desc, skb, rx_ptype);
 +		if (unlikely(rx_status & I40E_RXD_QW1_STATUS_TSYNVALID_MASK)) {
 +			i40e_ptp_rx_hwtstamp(vsi->back, skb, (rx_status &
 +					   I40E_RXD_QW1_STATUS_TSYNINDX_MASK) >>
 +					   I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT);
 +			rx_ring->last_rx_timestamp = jiffies;
++=======
+ 		if (i40e_cleanup_headers(rx_ring, skb)) {
+ 			skb = NULL;
+ 			continue;
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
  		}
  
  		/* probably a little skewed due to removing CRC */
@@@ -1590,11 -1802,19 +1953,14 @@@
  			continue;
  		}
  #endif
 -
 -		vlan_tag = (qword & BIT(I40E_RX_DESC_STATUS_L2TAG1P_SHIFT)) ?
 -			   le16_to_cpu(rx_desc->wb.qword0.lo_dword.l2tag1) : 0;
 -
  		i40e_receive_skb(rx_ring, skb, vlan_tag);
+ 		skb = NULL;
  
 -		/* update budget accounting */
 -		total_rx_packets++;
 -	}
 +		rx_desc->wb.qword1.status_error_len = 0;
 +	} while (likely(total_rx_packets < budget));
  
+ 	rx_ring->skb = skb;
+ 
  	u64_stats_update_begin(&rx_ring->syncp);
  	rx_ring->stats.packets += total_rx_packets;
  	rx_ring->stats.bytes += total_rx_bytes;
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.h
index 94dba9ba7e88,f80979025c01..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
@@@ -236,11 -253,8 +236,14 @@@ struct i40e_tx_buffer 
  };
  
  struct i40e_rx_buffer {
++<<<<<<< HEAD
 +	struct sk_buff *skb;
 +	void *hdr_buf;
++=======
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
  	dma_addr_t dma;
  	struct page *page;
 +	dma_addr_t page_dma;
  	unsigned int page_offset;
  };
  
@@@ -345,6 -352,15 +348,18 @@@ struct i40e_ring 
  	struct i40e_q_vector *q_vector;	/* Backreference to associated vector */
  
  	struct rcu_head rcu;		/* to avoid race on free */
++<<<<<<< HEAD
++=======
+ 	u16 next_to_alloc;
+ 	struct sk_buff *skb;		/* When i40e_clean_rx_ring_irq() must
+ 					 * return before it sees the EOP for
+ 					 * the current packet, we save that skb
+ 					 * here and resume receiving this
+ 					 * packet the next time
+ 					 * i40e_clean_rx_ring_irq() is called
+ 					 * for this ring.
+ 					 */
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
  } ____cacheline_internodealigned_in_smp;
  
  enum i40e_latency_range {
diff --cc drivers/net/ethernet/intel/i40evf/i40e_txrx.c
index 49386aab2722,7e9c425fc149..000000000000
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
@@@ -1086,10 -1087,15 +1087,10 @@@ static bool i40e_add_rx_frag(struct i40
   */
  static inline
  struct sk_buff *i40evf_fetch_rx_buffer(struct i40e_ring *rx_ring,
- 				       union i40e_rx_desc *rx_desc)
+ 				       union i40e_rx_desc *rx_desc,
+ 				       struct sk_buff *skb)
  {
 -	u64 local_status_error_len =
 -		le64_to_cpu(rx_desc->wb.qword1.status_error_len);
 -	unsigned int size =
 -		(local_status_error_len & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
 -		I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
  	struct i40e_rx_buffer *rx_buffer;
- 	struct sk_buff *skb;
  	struct page *page;
  
  	rx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];
@@@ -1204,8 -1205,6 +1200,11 @@@ static int i40e_clean_rx_irq(struct i40
  
  	while (likely(total_rx_packets < budget)) {
  		union i40e_rx_desc *rx_desc;
++<<<<<<< HEAD
 +		struct sk_buff *skb;
 +		u32 rx_status;
++=======
++>>>>>>> e72e56597ba1 (i40e/i40evf: Moves skb from i40e_rx_buffer to i40e_ring)
  		u16 vlan_tag;
  		u8 rx_ptype;
  		u64 qword;
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.c
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.h
* Unmerged path drivers/net/ethernet/intel/i40evf/i40e_txrx.c
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_txrx.h b/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
index 3664ed032d01..52aec4316822 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
@@ -239,7 +239,6 @@ struct i40e_tx_buffer {
 };
 
 struct i40e_rx_buffer {
-	struct sk_buff *skb;
 	dma_addr_t dma;
 	struct page *page;
 	unsigned int page_offset;
@@ -344,6 +343,14 @@ struct i40e_ring {
 
 	struct rcu_head rcu;		/* to avoid race on free */
 	u16 next_to_alloc;
+	struct sk_buff *skb;		/* When i40evf_clean_rx_ring_irq() must
+					 * return before it sees the EOP for
+					 * the current packet, we save that skb
+					 * here and resume receiving this
+					 * packet the next time
+					 * i40evf_clean_rx_ring_irq() is called
+					 * for this ring.
+					 */
 } ____cacheline_internodealigned_in_smp;
 
 enum i40e_latency_range {
