mm/hugetlb: simplify hugetlb unmap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [mm] hugetlb: simplify hugetlb unmap (Andrea Arcangeli) [1430172]
Rebuild_FUZZ: 95.38%
commit-author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
commit 31d49da5ad01728e48a1bb2b43795598b23de68a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/31d49da5.failed

For hugetlb like THP (and unlike regular page), we do tlb flush after
dropping ptl.  Because of the above, we don't need to track force_flush
like we do now.  Instead we can simply call tlb_remove_page() which will
do the flush if needed.

No functionality change in this patch.

Link: http://lkml.kernel.org/r/1465049193-22197-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com
	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 31d49da5ad01728e48a1bb2b43795598b23de68a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 657b46a931be,524c078ce67b..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -2894,8 -3194,8 +2893,13 @@@ void __unmap_hugepage_range(struct mmu_
  
  	tlb_start_vma(tlb, vma);
  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
++<<<<<<< HEAD
 +again:
 +	for (address = start; address < end; address += sz) {
++=======
+ 	address = start;
+ 	for (; address < end; address += sz) {
++>>>>>>> 31d49da5ad01 (mm/hugetlb: simplify hugetlb unmap)
  		ptep = huge_pte_offset(mm, address);
  		if (!ptep)
  			continue;
@@@ -2940,30 -3246,16 +2950,31 @@@
  		if (huge_pte_dirty(pte))
  			set_page_dirty(page);
  
++<<<<<<< HEAD
 +		page_remove_rmap(page);
 +		force_flush = !__tlb_remove_page(tlb, page);
 +		if (force_flush) {
 +			spin_unlock(ptl);
 +			break;
 +		}
 +		/* Bail out after unmapping reference page if supplied */
 +		if (ref_page) {
 +			spin_unlock(ptl);
 +			break;
 +		}
 +unlock:
++=======
+ 		hugetlb_count_sub(pages_per_huge_page(h), mm);
+ 		page_remove_rmap(page, true);
+ 
++>>>>>>> 31d49da5ad01 (mm/hugetlb: simplify hugetlb unmap)
  		spin_unlock(ptl);
- 	}
- 	/*
- 	 * mmu_gather ran out of room to batch pages, we break out of
- 	 * the PTE lock to avoid doing the potential expensive TLB invalidate
- 	 * and page-free while holding it.
- 	 */
- 	if (force_flush) {
- 		force_flush = 0;
- 		tlb_flush_mmu(tlb);
- 		if (address < end && !ref_page)
- 			goto again;
+ 		tlb_remove_page(tlb, page);
+ 		/*
+ 		 * Bail out after unmapping reference page if supplied
+ 		 */
+ 		if (ref_page)
+ 			break;
  	}
  	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
  	tlb_end_vma(tlb, vma);
* Unmerged path mm/hugetlb.c
