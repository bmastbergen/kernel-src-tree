time: Introduce tk_fast_raw

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit f09cb9a1808e35ad7502ea39b6bfb443c7fa0f19
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/f09cb9a1.failed

Add the NMI safe CLOCK_MONOTONIC_RAW accessor..

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: John Stultz <john.stultz@linaro.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20150319093400.562746929@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit f09cb9a1808e35ad7502ea39b6bfb443c7fa0f19)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/timekeeping.h
#	kernel/time/timekeeping.c
diff --cc include/linux/timekeeping.h
index 7bc90f9753f2,f36b1edf3f73..000000000000
--- a/include/linux/timekeeping.h
+++ b/include/linux/timekeeping.h
@@@ -182,21 -214,42 +182,31 @@@ static inline u64 ktime_get_boot_ns(voi
  	return ktime_to_ns(ktime_get_boottime());
  }
  
++<<<<<<< HEAD
 +/*
 + * Get cross timestamp between system clock and device clock
 + */
 +extern int get_device_system_crosststamp(
 +			int (*get_time_fn)(ktime_t *device_time,
 +				struct system_counterval_t *system_counterval,
 +				void *ctx),
 +			void *ctx,
 +			struct system_time_snapshot *history,
 +			struct system_device_crosststamp *xtstamp);
++=======
+ static inline u64 ktime_get_raw_ns(void)
+ {
+ 	return ktime_to_ns(ktime_get_raw());
+ }
+ 
+ extern u64 ktime_get_mono_fast_ns(void);
+ extern u64 ktime_get_raw_fast_ns(void);
++>>>>>>> f09cb9a1808e (time: Introduce tk_fast_raw)
  
  /*
 - * Timespec interfaces utilizing the ktime based ones
 - */
 -static inline void get_monotonic_boottime(struct timespec *ts)
 -{
 -	*ts = ktime_to_timespec(ktime_get_boottime());
 -}
 -
 -static inline void get_monotonic_boottime64(struct timespec64 *ts)
 -{
 -	*ts = ktime_to_timespec64(ktime_get_boottime());
 -}
 -
 -static inline void timekeeping_clocktai(struct timespec *ts)
 -{
 -	*ts = ktime_to_timespec(ktime_get_clocktai());
 -}
 -
 -/*
 - * RTC specific
 - */
 -extern void timekeeping_inject_sleeptime64(struct timespec64 *delta);
 -
 -/*
 - * PPS accessor
 + * Simultaneously snapshot realtime and monotonic raw clocks
   */
 -extern void getnstime_raw_and_real(struct timespec *ts_raw,
 -				   struct timespec *ts_real);
 +extern void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot);
  
  /*
   * Persistent clock related interfaces
diff --cc kernel/time/timekeeping.c
index 1f122214a691,c3fcff06d30a..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -31,11 -32,35 +31,31 @@@
  #define TK_MIRROR		(1 << 1)
  #define TK_CLOCK_WAS_SET	(1 << 2)
  
 -/*
 - * The most important data for readout fits into a single 64 byte
 - * cache line.
 - */
 -static struct {
 -	seqcount_t		seq;
 -	struct timekeeper	timekeeper;
 -} tk_core ____cacheline_aligned;
 -
 +static struct timekeeper timekeeper;
  static DEFINE_RAW_SPINLOCK(timekeeper_lock);
 +static seqcount_t timekeeper_seq;
  static struct timekeeper shadow_timekeeper;
  
++<<<<<<< HEAD
++=======
+ /**
+  * struct tk_fast - NMI safe timekeeper
+  * @seq:	Sequence counter for protecting updates. The lowest bit
+  *		is the index for the tk_read_base array
+  * @base:	tk_read_base array. Access is indexed by the lowest bit of
+  *		@seq.
+  *
+  * See @update_fast_timekeeper() below.
+  */
+ struct tk_fast {
+ 	seqcount_t		seq;
+ 	struct tk_read_base	base[2];
+ };
+ 
+ static struct tk_fast tk_fast_mono ____cacheline_aligned;
+ static struct tk_fast tk_fast_raw  ____cacheline_aligned;
+ 
++>>>>>>> f09cb9a1808e (time: Introduce tk_fast_raw)
  /* flag for if timekeeping is suspended */
  int __read_mostly timekeeping_suspended;
  
@@@ -196,25 -326,191 +216,110 @@@ static inline s64 timekeeping_delta_to_
  	return nsec + arch_gettimeoffset();
  }
  
 -/**
 - * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.
 - * @tkr: Timekeeping readout base from which we take the update
 - *
 - * We want to use this from any context including NMI and tracing /
 - * instrumenting the timekeeping code itself.
 - *
 - * So we handle this differently than the other timekeeping accessor
 - * functions which retry when the sequence count has changed. The
 - * update side does:
 - *
 - * smp_wmb();	<- Ensure that the last base[1] update is visible
 - * tkf->seq++;
 - * smp_wmb();	<- Ensure that the seqcount update is visible
 - * update(tkf->base[0], tkr);
 - * smp_wmb();	<- Ensure that the base[0] update is visible
 - * tkf->seq++;
 - * smp_wmb();	<- Ensure that the seqcount update is visible
 - * update(tkf->base[1], tkr);
 - *
 - * The reader side does:
 - *
 - * do {
 - *	seq = tkf->seq;
 - *	smp_rmb();
 - *	idx = seq & 0x01;
 - *	now = now(tkf->base[idx]);
 - *	smp_rmb();
 - * } while (seq != tkf->seq)
 - *
 - * As long as we update base[0] readers are forced off to
 - * base[1]. Once base[0] is updated readers are redirected to base[0]
 - * and the base[1] update takes place.
 - *
 - * So if a NMI hits the update of base[0] then it will use base[1]
 - * which is still consistent. In the worst case this can result is a
 - * slightly wrong timestamp (a few nanoseconds). See
 - * @ktime_get_mono_fast_ns.
 - */
 -static void update_fast_timekeeper(struct tk_read_base *tkr, struct tk_fast *tkf)
 +static inline s64 timekeeping_get_ns(struct tk_read_base *tkr)
  {
 -	struct tk_read_base *base = tkf->base;
 -
 -	/* Force readers off to base[1] */
 -	raw_write_seqcount_latch(&tkf->seq);
 -
 -	/* Update base[0] */
 -	memcpy(base, tkr, sizeof(*base));
 -
 -	/* Force readers back to base[0] */
 -	raw_write_seqcount_latch(&tkf->seq);
 +	cycle_t delta;
  
 -	/* Update base[1] */
 -	memcpy(base + 1, base, sizeof(*base));
 +	delta = timekeeping_get_delta(tkr);
 +	return timekeeping_delta_to_ns(tkr, delta);
  }
  
 -/**
 - * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic
 - *
 - * This timestamp is not guaranteed to be monotonic across an update.
 - * The timestamp is calculated by:
 - *
 - *	now = base_mono + clock_delta * slope
 - *
 - * So if the update lowers the slope, readers who are forced to the
 - * not yet updated second array are still using the old steeper slope.
 - *
 - * tmono
 - * ^
 - * |    o  n
 - * |   o n
 - * |  u
 - * | o
 - * |o
 - * |12345678---> reader order
 - *
 - * o = old slope
 - * u = update
 - * n = new slope
 - *
 - * So reader 6 will observe time going backwards versus reader 5.
 - *
 - * While other CPUs are likely to be able observe that, the only way
 - * for a CPU local observation is when an NMI hits in the middle of
 - * the update. Timestamps taken from that NMI context might be ahead
 - * of the following timestamps. Callers need to be aware of that and
 - * deal with it.
 - */
 -static __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)
 +static inline s64 timekeeping_cycles_to_ns(struct tk_read_base *tkr,
 +					    cycle_t cycles)
  {
 -	struct tk_read_base *tkr;
 -	unsigned int seq;
 -	u64 now;
 -
 -	do {
 -		seq = raw_read_seqcount(&tkf->seq);
 -		tkr = tkf->base + (seq & 0x01);
 -		now = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);
 -	} while (read_seqcount_retry(&tkf->seq, seq));
 +	cycle_t delta;
  
 -	return now;
 +	/* calculate the delta since the last update_wall_time */
 +	delta = clocksource_delta(cycles, tkr->clock->cycle_last,
 +				  tkr->clock->mask);
 +	return timekeeping_delta_to_ns(tkr, delta);
  }
  
++<<<<<<< HEAD
++=======
+ u64 ktime_get_mono_fast_ns(void)
+ {
+ 	return __ktime_get_fast_ns(&tk_fast_mono);
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);
+ 
+ u64 ktime_get_raw_fast_ns(void)
+ {
+ 	return __ktime_get_fast_ns(&tk_fast_raw);
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_raw_fast_ns);
+ 
+ /* Suspend-time cycles value for halted fast timekeeper. */
+ static cycle_t cycles_at_suspend;
+ 
+ static cycle_t dummy_clock_read(struct clocksource *cs)
+ {
+ 	return cycles_at_suspend;
+ }
+ 
+ /**
+  * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.
+  * @tk: Timekeeper to snapshot.
+  *
+  * It generally is unsafe to access the clocksource after timekeeping has been
+  * suspended, so take a snapshot of the readout base of @tk and use it as the
+  * fast timekeeper's readout base while suspended.  It will return the same
+  * number of cycles every time until timekeeping is resumed at which time the
+  * proper readout base for the fast timekeeper will be restored automatically.
+  */
+ static void halt_fast_timekeeper(struct timekeeper *tk)
+ {
+ 	static struct tk_read_base tkr_dummy;
+ 	struct tk_read_base *tkr = &tk->tkr_mono;
+ 
+ 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
+ 	cycles_at_suspend = tkr->read(tkr->clock);
+ 	tkr_dummy.read = dummy_clock_read;
+ 	update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);
+ 
+ 	tkr = &tk->tkr_raw;
+ 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
+ 	tkr_dummy.read = dummy_clock_read;
+ 	update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
+ }
+ 
+ #ifdef CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 
+ static inline void update_vsyscall(struct timekeeper *tk)
+ {
+ 	struct timespec xt, wm;
+ 
+ 	xt = timespec64_to_timespec(tk_xtime(tk));
+ 	wm = timespec64_to_timespec(tk->wall_to_monotonic);
+ 	update_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,
+ 			    tk->tkr_mono.cycle_last);
+ }
+ 
+ static inline void old_vsyscall_fixup(struct timekeeper *tk)
+ {
+ 	s64 remainder;
+ 
+ 	/*
+ 	* Store only full nanoseconds into xtime_nsec after rounding
+ 	* it up and add the remainder to the error difference.
+ 	* XXX - This is necessary to avoid small 1ns inconsistnecies caused
+ 	* by truncating the remainder in vsyscalls. However, it causes
+ 	* additional work to be done in timekeeping_adjust(). Once
+ 	* the vsyscall implementations are converted to use xtime_nsec
+ 	* (shifted nanoseconds), and CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 	* users are removed, this can be killed.
+ 	*/
+ 	remainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);
+ 	tk->tkr_mono.xtime_nsec -= remainder;
+ 	tk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;
+ 	tk->ntp_error += remainder << tk->ntp_error_shift;
+ 	tk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;
+ }
+ #else
+ #define old_vsyscall_fixup(tk)
+ #endif
+ 
++>>>>>>> f09cb9a1808e (time: Introduce tk_fast_raw)
  static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
  
  static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
@@@ -305,11 -599,12 +410,16 @@@ static void timekeeping_update(struct t
  	update_vsyscall(tk);
  	update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);
  
 -	if (action & TK_MIRROR)
 -		memcpy(&shadow_timekeeper, &tk_core.timekeeper,
 -		       sizeof(tk_core.timekeeper));
 +	if (action & TK_CLOCK_WAS_SET)
 +		tk->clock_was_set_seq++;
  
++<<<<<<< HEAD
 +	if (action & TK_MIRROR)
 +		memcpy(&shadow_timekeeper, &timekeeper, sizeof(timekeeper));
++=======
+ 	update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);
+ 	update_fast_timekeeper(&tk->tkr_raw,  &tk_fast_raw);
++>>>>>>> f09cb9a1808e (time: Introduce tk_fast_raw)
  }
  
  /**
* Unmerged path include/linux/timekeeping.h
* Unmerged path kernel/time/timekeeping.c
