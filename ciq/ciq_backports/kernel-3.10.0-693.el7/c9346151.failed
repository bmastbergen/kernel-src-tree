s390/dasd: extend dasd path handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [s390] dasd: extend dasd path handling (Hendrik Brueckner) [1380771]
Rebuild_FUZZ: 92.54%
commit-author Stefan Haberland <sth@linux.vnet.ibm.com>
commit c93461515a1a16486f4e483cb34170366fa73ea1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/c9346151.failed

Store flags and path_data per channel path.
Implement get/set functions for various path masks.
The patch does not add functional changes.

	Signed-off-by: Stefan Haberland <sth@linux.vnet.ibm.com>
	Reviewed-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
	Reviewed-by: Jan Hoeppner <hoeppner@linux.vnet.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit c93461515a1a16486f4e483cb34170366fa73ea1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/block/dasd_devmap.c
#	drivers/s390/block/dasd_eckd.c
#	drivers/s390/block/dasd_eckd.h
#	drivers/s390/block/dasd_fba.c
#	drivers/s390/block/dasd_int.h
diff --cc drivers/s390/block/dasd_devmap.c
index a71bb8aaca1d,4101ab000c16..000000000000
--- a/drivers/s390/block/dasd_devmap.c
+++ b/drivers/s390/block/dasd_devmap.c
@@@ -1337,6 -1428,29 +1337,32 @@@ static ssize_t dasd_reservation_state_s
  static DEVICE_ATTR(last_known_reservation_state, 0644,
  		   dasd_reservation_state_show, dasd_reservation_state_store);
  
++<<<<<<< HEAD
++=======
+ static ssize_t dasd_pm_show(struct device *dev,
+ 			      struct device_attribute *attr, char *buf)
+ {
+ 	struct dasd_device *device;
+ 	u8 opm, nppm, cablepm, cuirpm, hpfpm;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return sprintf(buf, "0\n");
+ 
+ 	opm = dasd_path_get_opm(device);
+ 	nppm = dasd_path_get_nppm(device);
+ 	cablepm = dasd_path_get_cablepm(device);
+ 	cuirpm = dasd_path_get_cuirpm(device);
+ 	hpfpm = dasd_path_get_hpfpm(device);
+ 	dasd_put_device(device);
+ 
+ 	return sprintf(buf, "%02x %02x %02x %02x %02x\n", opm, nppm,
+ 		       cablepm, cuirpm, hpfpm);
+ }
+ 
+ static DEVICE_ATTR(path_masks, 0444, dasd_pm_show, NULL);
+ 
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  static struct attribute * dasd_attrs[] = {
  	&dev_attr_readonly.attr,
  	&dev_attr_discipline.attr,
diff --cc drivers/s390/block/dasd_eckd.c
index 75d5d21ad443,51fdf31aa8eb..000000000000
--- a/drivers/s390/block/dasd_eckd.c
+++ b/drivers/s390/block/dasd_eckd.c
@@@ -1041,15 -1052,13 +1041,18 @@@ static int dasd_eckd_read_conf(struct d
  {
  	void *conf_data;
  	int conf_len, conf_data_saved;
 -	int rc, path_err, pos;
 +	int rc, path_err;
  	__u8 lpm, opm;
  	struct dasd_eckd_private *private, path_private;
- 	struct dasd_path *path_data;
  	struct dasd_uid *uid;
  	char print_path_uid[60], print_device_uid[60];
  
++<<<<<<< HEAD
 +	private = (struct dasd_eckd_private *) device->private;
 +	path_data = &device->path_data;
++=======
+ 	private = device->private;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	opm = ccw_device_get_path_mask(device->cdev);
  	conf_data_saved = 0;
  	path_err = 0;
@@@ -1144,6 -1151,7 +1146,10 @@@
  					"device %s instead of %s\n", lpm,
  					print_path_uid, print_device_uid);
  				path_err = -EINVAL;
++<<<<<<< HEAD
++=======
+ 				dasd_path_add_cablepm(device, lpm);
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  				continue;
  			}
  			pos = pathmask_to_pos(lpm);
@@@ -1155,20 -1162,18 +1160,23 @@@
  		}
  		switch (dasd_eckd_path_access(conf_data, conf_len)) {
  		case 0x02:
- 			path_data->npm |= lpm;
+ 			dasd_path_add_nppm(device, lpm);
  			break;
  		case 0x03:
- 			path_data->ppm |= lpm;
+ 			dasd_path_add_ppm(device, lpm);
  			break;
  		}
- 		if (!path_data->opm) {
- 			path_data->opm = lpm;
+ 		if (!dasd_path_get_opm(device)) {
+ 			dasd_path_set_opm(device, lpm);
  			dasd_generic_path_operational(device);
  		} else {
- 			path_data->opm |= lpm;
+ 			dasd_path_add_opm(device, lpm);
  		}
++<<<<<<< HEAD
 +		if (conf_data != private->conf_data)
 +			kfree(conf_data);
++=======
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	}
  
  	return path_err;
@@@ -1206,15 -1210,9 +1214,21 @@@ static int verify_fcx_max_data(struct d
  static int rebuild_device_uid(struct dasd_device *device,
  			      struct path_verification_work_data *data)
  {
++<<<<<<< HEAD
 +	struct dasd_eckd_private *private;
 +	struct dasd_path *path_data;
 +	__u8 lpm, opm;
 +	int rc;
 +
 +	rc = -ENODEV;
 +	private = (struct dasd_eckd_private *) device->private;
 +	path_data = &device->path_data;
 +	opm = device->path_data.opm;
++=======
+ 	struct dasd_eckd_private *private = device->private;
+ 	__u8 lpm, opm = dasd_path_get_opm(device);
+ 	int rc = -ENODEV;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  
  	for (lpm = 0x80; lpm; lpm >>= 1) {
  		if (!(lpm & opm))
@@@ -1387,17 -1394,20 +1401,31 @@@ static void do_path_verification_work(s
  		 * situation in dasd_start_IO.
  		 */
  		spin_lock_irqsave(get_ccwdev_lock(device->cdev), flags);
++<<<<<<< HEAD
 +		if (!device->path_data.opm && opm) {
 +			device->path_data.opm = opm;
 +			dasd_generic_path_operational(device);
 +		} else
 +			device->path_data.opm |= opm;
 +		device->path_data.npm |= npm;
 +		device->path_data.ppm |= ppm;
 +		device->path_data.tbvpm |= epm;
++=======
+ 		if (!dasd_path_get_opm(device) && opm) {
+ 			dasd_path_set_opm(device, opm);
+ 			dasd_generic_path_operational(device);
+ 		} else {
+ 			dasd_path_add_opm(device, opm);
+ 		}
+ 		dasd_path_add_nppm(device, npm);
+ 		dasd_path_add_ppm(device, ppm);
+ 		dasd_path_add_tbvpm(device, epm);
+ 		dasd_path_add_cablepm(device, cablepm);
+ 		dasd_path_add_nohpfpm(device, hpfpm);
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  		spin_unlock_irqrestore(get_ccwdev_lock(device->cdev), flags);
  	}
 -	clear_bit(DASD_FLAG_PATH_VERIFY, &device->flags);
 +
  	dasd_put_device(device);
  	if (data->isglobal)
  		mutex_unlock(&dasd_path_verification_mutex);
@@@ -1814,6 -1820,15 +1842,18 @@@ static void dasd_eckd_uncheck_device(st
  	private->vdsneq = NULL;
  	private->gneq = NULL;
  	private->conf_len = 0;
++<<<<<<< HEAD
++=======
+ 	for (i = 0; i < 8; i++) {
+ 		kfree(device->path[i].conf_data);
+ 		if ((__u8 *)device->path[i].conf_data ==
+ 		    private->conf_data) {
+ 			private->conf_data = NULL;
+ 			private->conf_len = 0;
+ 		}
+ 		device->path[i].conf_data = NULL;
+ 	}
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	kfree(private->conf_data);
  	private->conf_data = NULL;
  }
@@@ -2738,8 -3233,8 +2778,13 @@@ static struct dasd_ccw_req *dasd_eckd_b
  	cqr->memdev = startdev;
  	cqr->block = block;
  	cqr->expires = startdev->default_expires * HZ;	/* default 5 minutes */
++<<<<<<< HEAD
 +	cqr->lpm = startdev->path_data.ppm;
 +	cqr->retries = 256;
++=======
+ 	cqr->lpm = dasd_path_get_ppm(startdev);
+ 	cqr->retries = startdev->default_retries;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	cqr->buildclk = get_tod_clock();
  	cqr->status = DASD_CQR_FILLED;
  	return cqr;
@@@ -2913,8 -3408,8 +2958,13 @@@ static struct dasd_ccw_req *dasd_eckd_b
  	cqr->memdev = startdev;
  	cqr->block = block;
  	cqr->expires = startdev->default_expires * HZ;	/* default 5 minutes */
++<<<<<<< HEAD
 +	cqr->lpm = startdev->path_data.ppm;
 +	cqr->retries = 256;
++=======
+ 	cqr->lpm = dasd_path_get_ppm(startdev);
+ 	cqr->retries = startdev->default_retries;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	cqr->buildclk = get_tod_clock();
  	cqr->status = DASD_CQR_FILLED;
  	return cqr;
@@@ -3206,8 -3717,8 +3256,13 @@@ static struct dasd_ccw_req *dasd_eckd_b
  	cqr->memdev = startdev;
  	cqr->block = block;
  	cqr->expires = startdev->default_expires * HZ;	/* default 5 minutes */
++<<<<<<< HEAD
 +	cqr->lpm = startdev->path_data.ppm;
 +	cqr->retries = 256;
++=======
+ 	cqr->lpm = dasd_path_get_ppm(startdev);
+ 	cqr->retries = startdev->default_retries;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	cqr->buildclk = get_tod_clock();
  	cqr->status = DASD_CQR_FILLED;
  	return cqr;
@@@ -3411,8 -3944,8 +3466,13 @@@ static struct dasd_ccw_req *dasd_raw_bu
  	cqr->memdev = startdev;
  	cqr->block = block;
  	cqr->expires = startdev->default_expires * HZ;
++<<<<<<< HEAD
 +	cqr->lpm = startdev->path_data.ppm;
 +	cqr->retries = 256;
++=======
+ 	cqr->lpm = dasd_path_get_ppm(startdev);
+ 	cqr->retries = startdev->default_retries;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	cqr->buildclk = get_tod_clock();
  	cqr->status = DASD_CQR_FILLED;
  
@@@ -4484,6 -5040,653 +4544,656 @@@ out_err
  	return -1;
  }
  
++<<<<<<< HEAD
++=======
+ static int dasd_eckd_read_message_buffer(struct dasd_device *device,
+ 					 struct dasd_rssd_messages *messages,
+ 					 __u8 lpum)
+ {
+ 	struct dasd_rssd_messages *message_buf;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   (sizeof(struct dasd_psf_prssd_data) +
+ 				    sizeof(struct dasd_rssd_messages)),
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	cqr->lpm = lpum;
+ retry:
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->expires = 10 * HZ;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 	/* dasd_sleep_on_immediatly does not do complex error
+ 	 * recovery so clear erp flag and set retry counter to
+ 	 * do basic erp */
+ 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
+ 	cqr->retries = 256;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = 0x03;	/* Message Buffer */
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - message buffer */
+ 	message_buf = (struct dasd_rssd_messages *) (prssdp + 1);
+ 	memset(message_buf, 0, sizeof(struct dasd_rssd_messages));
+ 
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_rssd_messages);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) message_buf;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on_immediatly(cqr);
+ 	if (rc == 0) {
+ 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 		message_buf = (struct dasd_rssd_messages *)
+ 			(prssdp + 1);
+ 		memcpy(messages, message_buf,
+ 		       sizeof(struct dasd_rssd_messages));
+ 	} else if (cqr->lpm) {
+ 		/*
+ 		 * on z/VM we might not be able to do I/O on the requested path
+ 		 * but instead we get the required information on any path
+ 		 * so retry with open path mask
+ 		 */
+ 		cqr->lpm = 0;
+ 		goto retry;
+ 	} else
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading messages failed with rc=%d\n"
+ 				, rc);
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ static int dasd_eckd_query_host_access(struct dasd_device *device,
+ 				       struct dasd_psf_query_host_access *data)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct dasd_psf_query_host_access *host_access;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	/* not available for HYPER PAV alias devices */
+ 	if (!device->block && private->lcu->pav == HYPER_PAV)
+ 		return -EOPNOTSUPP;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   sizeof(struct dasd_psf_prssd_data) + 1,
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 	host_access = kzalloc(sizeof(*host_access), GFP_KERNEL | GFP_DMA);
+ 	if (!host_access) {
+ 		dasd_sfree_request(cqr, device);
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate host_access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10 * HZ;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = PSF_SUBORDER_QHA;	/* query host access */
+ 	/* LSS and Volume that will be queried */
+ 	prssdp->lss = private->ned->ID;
+ 	prssdp->volume = private->ned->unit_addr;
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - query host access */
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_psf_query_host_access);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) host_access;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on_interruptible(cqr);
+ 	if (rc == 0) {
+ 		*data = *host_access;
+ 	} else {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading host access data failed with rc=%d\n",
+ 				rc);
+ 		rc = -EOPNOTSUPP;
+ 	}
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	kfree(host_access);
+ 	return rc;
+ }
+ /*
+  * return number of grouped devices
+  */
+ static int dasd_eckd_host_access_count(struct dasd_device *device)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	int count = 0;
+ 	int rc, i;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		if (entry->status_flags & DASD_ECKD_PG_GROUPED)
+ 			count++;
+ 	}
+ 
+ 	kfree(access);
+ 	return count;
+ }
+ 
+ /*
+  * write host access information to a sequential file
+  */
+ static int dasd_hosts_print(struct dasd_device *device, struct seq_file *m)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	char sysplex[9] = "";
+ 	int rc, i, j;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		/* PGID */
+ 		seq_puts(m, "pgid ");
+ 		for (j = 0; j < 11; j++)
+ 			seq_printf(m, "%02x", entry->pgid[j]);
+ 		seq_putc(m, '\n');
+ 		/* FLAGS */
+ 		seq_printf(m, "status_flags %02x\n", entry->status_flags);
+ 		/* SYSPLEX NAME */
+ 		memcpy(&sysplex, &entry->sysplex_name, sizeof(sysplex) - 1);
+ 		EBCASC(sysplex, sizeof(sysplex));
+ 		seq_printf(m, "sysplex_name %8s\n", sysplex);
+ 		/* SUPPORTED CYLINDER */
+ 		seq_printf(m, "supported_cylinder %d\n", entry->cylinder);
+ 		/* TIMESTAMP */
+ 		seq_printf(m, "timestamp %lu\n", (unsigned long)
+ 			   entry->timestamp);
+ 	}
+ 	kfree(access);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Perform Subsystem Function - CUIR response
+  */
+ static int
+ dasd_eckd_psf_cuir_response(struct dasd_device *device, int response,
+ 			    __u32 message_id,
+ 			    struct channel_path_desc *desc,
+ 			    struct subchannel_id sch_id)
+ {
+ 	struct dasd_psf_cuir_response *psf_cuir;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */ ,
+ 				  sizeof(struct dasd_psf_cuir_response),
+ 				  device);
+ 
+ 	if (IS_ERR(cqr)) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			   "Could not allocate PSF-CUIR request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	psf_cuir = (struct dasd_psf_cuir_response *)cqr->data;
+ 	psf_cuir->order = PSF_ORDER_CUIR_RESPONSE;
+ 	psf_cuir->cc = response;
+ 	if (desc)
+ 		psf_cuir->chpid = desc->chpid;
+ 	psf_cuir->message_id = message_id;
+ 	psf_cuir->cssid = sch_id.cssid;
+ 	psf_cuir->ssid = sch_id.ssid;
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->cda = (__u32)(addr_t)psf_cuir;
+ 	ccw->flags = CCW_FLAG_SLI;
+ 	ccw->count = sizeof(struct dasd_psf_cuir_response);
+ 
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10*HZ;
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 
+ 	rc = dasd_sleep_on(cqr);
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ /*
+  * return configuration data that is referenced by record selector
+  * if a record selector is specified or per default return the
+  * conf_data pointer for the path specified by lpum
+  */
+ static struct dasd_conf_data *dasd_eckd_get_ref_conf(struct dasd_device *device,
+ 						     __u8 lpum,
+ 						     struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_conf_data *conf_data;
+ 	int path, pos;
+ 
+ 	if (cuir->record_selector == 0)
+ 		goto out;
+ 	for (path = 0x80, pos = 0; path; path >>= 1, pos++) {
+ 		conf_data = device->path[pos].conf_data;
+ 		if (conf_data->gneq.record_selector ==
+ 		    cuir->record_selector)
+ 			return conf_data;
+ 	}
+ out:
+ 	return device->path[pathmask_to_pos(lpum)].conf_data;
+ }
+ 
+ /*
+  * This function determines the scope of a reconfiguration request by
+  * analysing the path and device selection data provided in the CUIR request.
+  * Returns a path mask containing CUIR affected paths for the give device.
+  *
+  * If the CUIR request does not contain the required information return the
+  * path mask of the path the attention message for the CUIR request was reveived
+  * on.
+  */
+ static int dasd_eckd_cuir_scope(struct dasd_device *device, __u8 lpum,
+ 				struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_conf_data *ref_conf_data;
+ 	unsigned long bitmask = 0, mask = 0;
+ 	struct dasd_conf_data *conf_data;
+ 	unsigned int pos, path;
+ 	char *ref_gneq, *gneq;
+ 	char *ref_ned, *ned;
+ 	int tbcpm = 0;
+ 
+ 	/* if CUIR request does not specify the scope use the path
+ 	   the attention message was presented on */
+ 	if (!cuir->ned_map ||
+ 	    !(cuir->neq_map[0] | cuir->neq_map[1] | cuir->neq_map[2]))
+ 		return lpum;
+ 
+ 	/* get reference conf data */
+ 	ref_conf_data = dasd_eckd_get_ref_conf(device, lpum, cuir);
+ 	/* reference ned is determined by ned_map field */
+ 	pos = 8 - ffs(cuir->ned_map);
+ 	ref_ned = (char *)&ref_conf_data->neds[pos];
+ 	ref_gneq = (char *)&ref_conf_data->gneq;
+ 	/* transfer 24 bit neq_map to mask */
+ 	mask = cuir->neq_map[2];
+ 	mask |= cuir->neq_map[1] << 8;
+ 	mask |= cuir->neq_map[0] << 16;
+ 
+ 	for (path = 0; path < 8; path++) {
+ 		/* initialise data per path */
+ 		bitmask = mask;
+ 		conf_data = device->path[path].conf_data;
+ 		pos = 8 - ffs(cuir->ned_map);
+ 		ned = (char *) &conf_data->neds[pos];
+ 		/* compare reference ned and per path ned */
+ 		if (memcmp(ref_ned, ned, sizeof(*ned)) != 0)
+ 			continue;
+ 		gneq = (char *)&conf_data->gneq;
+ 		/* compare reference gneq and per_path gneq under
+ 		   24 bit mask where mask bit 0 equals byte 7 of
+ 		   the gneq and mask bit 24 equals byte 31 */
+ 		while (bitmask) {
+ 			pos = ffs(bitmask) - 1;
+ 			if (memcmp(&ref_gneq[31 - pos], &gneq[31 - pos], 1)
+ 			    != 0)
+ 				break;
+ 			clear_bit(pos, &bitmask);
+ 		}
+ 		if (bitmask)
+ 			continue;
+ 		/* device and path match the reference values
+ 		   add path to CUIR scope */
+ 		tbcpm |= 0x80 >> path;
+ 	}
+ 	return tbcpm;
+ }
+ 
+ static void dasd_eckd_cuir_notify_user(struct dasd_device *device,
+ 				       unsigned long paths,
+ 				       struct subchannel_id sch_id, int action)
+ {
+ 	struct channel_path_desc *desc;
+ 	int pos;
+ 
+ 	while (paths) {
+ 		/* get position of bit in mask */
+ 		pos = ffs(paths) - 1;
+ 		/* get channel path descriptor from this position */
+ 		desc = ccw_device_get_chp_desc(device->cdev, 7 - pos);
+ 		if (action == CUIR_QUIESCE)
+ 			pr_warn("Service on the storage server caused path "
+ 				"%x.%02x to go offline", sch_id.cssid,
+ 				desc ? desc->chpid : 0);
+ 		else if (action == CUIR_RESUME)
+ 			pr_info("Path %x.%02x is back online after service "
+ 				"on the storage server", sch_id.cssid,
+ 				desc ? desc->chpid : 0);
+ 		kfree(desc);
+ 		clear_bit(pos, &paths);
+ 	}
+ }
+ 
+ static int dasd_eckd_cuir_remove_path(struct dasd_device *device, __u8 lpum,
+ 				      struct dasd_cuir_message *cuir)
+ {
+ 	unsigned long tbcpm;
+ 
+ 	tbcpm = dasd_eckd_cuir_scope(device, lpum, cuir);
+ 	/* nothing to do if path is not in use */
+ 	if (!(dasd_path_get_opm(device) & tbcpm))
+ 		return 0;
+ 	if (!(dasd_path_get_opm(device) & ~tbcpm)) {
+ 		/* no path would be left if the CUIR action is taken
+ 		   return error */
+ 		return -EINVAL;
+ 	}
+ 	/* remove device from operational path mask */
+ 	dasd_path_remove_opm(device, tbcpm);
+ 	dasd_path_add_cuirpm(device, tbcpm);
+ 	return tbcpm;
+ }
+ 
+ /*
+  * walk through all devices and build a path mask to quiesce them
+  * return an error if the last path to a device would be removed
+  *
+  * if only part of the devices are quiesced and an error
+  * occurs no onlining necessary, the storage server will
+  * notify the already set offline devices again
+  */
+ static int dasd_eckd_cuir_quiesce(struct dasd_device *device, __u8 lpum,
+ 				  struct subchannel_id sch_id,
+ 				  struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	unsigned long flags;
+ 	int tbcpm;
+ 
+ 	/* active devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->active_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* inactive devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist, group) {
+ 		list_for_each_entry_safe(dev, n, &pavgroup->baselist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 		list_for_each_entry_safe(dev, n, &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, sch_id, CUIR_QUIESCE);
+ 	return 0;
+ out_err:
+ 	return tbcpm;
+ }
+ 
+ static int dasd_eckd_cuir_resume(struct dasd_device *device, __u8 lpum,
+ 				 struct subchannel_id sch_id,
+ 				 struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	int tbcpm;
+ 
+ 	/*
+ 	 * the path may have been added through a generic path event before
+ 	 * only trigger path verification if the path is not already in use
+ 	 */
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->active_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 			dasd_path_add_tbvpm(dev, tbcpm);
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 			dasd_path_add_tbvpm(dev, tbcpm);
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist,
+ 				 group) {
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->baselist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 				dasd_path_add_tbvpm(dev, tbcpm);
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 				dasd_path_add_tbvpm(dev, tbcpm);
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, sch_id, CUIR_RESUME);
+ 	return 0;
+ }
+ 
+ static void dasd_eckd_handle_cuir(struct dasd_device *device, void *messages,
+ 				 __u8 lpum)
+ {
+ 	struct dasd_cuir_message *cuir = messages;
+ 	struct channel_path_desc *desc;
+ 	struct subchannel_id sch_id;
+ 	int pos, response;
+ 
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR request: %016llx %016llx %016llx %08x",
+ 		      ((u64 *)cuir)[0], ((u64 *)cuir)[1], ((u64 *)cuir)[2],
+ 		      ((u32 *)cuir)[3]);
+ 	ccw_device_get_schid(device->cdev, &sch_id);
+ 	pos = pathmask_to_pos(lpum);
+ 	desc = ccw_device_get_chp_desc(device->cdev, pos);
+ 
+ 	if (cuir->code == CUIR_QUIESCE) {
+ 		/* quiesce */
+ 		if (dasd_eckd_cuir_quiesce(device, lpum, sch_id, cuir))
+ 			response = PSF_CUIR_LAST_PATH;
+ 		else
+ 			response = PSF_CUIR_COMPLETED;
+ 	} else if (cuir->code == CUIR_RESUME) {
+ 		/* resume */
+ 		dasd_eckd_cuir_resume(device, lpum, sch_id, cuir);
+ 		response = PSF_CUIR_COMPLETED;
+ 	} else
+ 		response = PSF_CUIR_NOT_SUPPORTED;
+ 
+ 	dasd_eckd_psf_cuir_response(device, response,
+ 				    cuir->message_id, desc, sch_id);
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR response: %d on message ID %08x", response,
+ 		      cuir->message_id);
+ 	/* free descriptor copy */
+ 	kfree(desc);
+ 	/* to make sure there is no attention left schedule work again */
+ 	device->discipline->check_attention(device, lpum);
+ }
+ 
+ static void dasd_eckd_check_attention_work(struct work_struct *work)
+ {
+ 	struct check_attention_work_data *data;
+ 	struct dasd_rssd_messages *messages;
+ 	struct dasd_device *device;
+ 	int rc;
+ 
+ 	data = container_of(work, struct check_attention_work_data, worker);
+ 	device = data->device;
+ 	messages = kzalloc(sizeof(*messages), GFP_KERNEL);
+ 	if (!messages) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			      "Could not allocate attention message buffer");
+ 		goto out;
+ 	}
+ 	rc = dasd_eckd_read_message_buffer(device, messages, data->lpum);
+ 	if (rc)
+ 		goto out;
+ 	if (messages->length == ATTENTION_LENGTH_CUIR &&
+ 	    messages->format == ATTENTION_FORMAT_CUIR)
+ 		dasd_eckd_handle_cuir(device, messages, data->lpum);
+ out:
+ 	dasd_put_device(device);
+ 	kfree(messages);
+ 	kfree(data);
+ }
+ 
+ static int dasd_eckd_check_attention(struct dasd_device *device, __u8 lpum)
+ {
+ 	struct check_attention_work_data *data;
+ 
+ 	data = kzalloc(sizeof(*data), GFP_ATOMIC);
+ 	if (!data)
+ 		return -ENOMEM;
+ 	INIT_WORK(&data->worker, dasd_eckd_check_attention_work);
+ 	dasd_get_device(device);
+ 	data->device = device;
+ 	data->lpum = lpum;
+ 	schedule_work(&data->worker);
+ 	return 0;
+ }
+ 
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  static struct ccw_driver dasd_eckd_driver = {
  	.driver = {
  		.name	= "dasd-eckd",
diff --cc drivers/s390/block/dasd_eckd.h
index 2555e494591f,e491f4416e40..000000000000
--- a/drivers/s390/block/dasd_eckd.h
+++ b/drivers/s390/block/dasd_eckd.h
@@@ -437,6 -535,7 +437,10 @@@ struct dasd_eckd_private 
  	struct dasd_eckd_characteristics rdc_data;
  	u8 *conf_data;
  	int conf_len;
++<<<<<<< HEAD
++=======
+ 
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  	/* pointers to specific parts in the conf_data */
  	struct dasd_ned *ned;
  	struct dasd_sneq *sneq;
diff --cc drivers/s390/block/dasd_fba.c
index 4dd0e2f6047e,462cab5d4302..000000000000
--- a/drivers/s390/block/dasd_fba.c
+++ b/drivers/s390/block/dasd_fba.c
@@@ -167,7 -167,8 +167,12 @@@ dasd_fba_check_characteristics(struct d
  	}
  
  	device->default_expires = DASD_EXPIRES;
++<<<<<<< HEAD
 +	device->path_data.opm = LPM_ANYPATH;
++=======
+ 	device->default_retries = FBA_DEFAULT_RETRIES;
+ 	dasd_path_set_opm(device, LPM_ANYPATH);
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  
  	readonly = dasd_device_is_ro(device);
  	if (readonly)
diff --cc drivers/s390/block/dasd_int.h
index aa498f7fe95b,d75f996884d9..000000000000
--- a/drivers/s390/block/dasd_int.h
+++ b/drivers/s390/block/dasd_int.h
@@@ -375,13 -398,23 +376,30 @@@ extern struct dasd_discipline *dasd_dia
  #define DASD_EER_STATECHANGE 3
  #define DASD_EER_PPRCSUSPEND 4
  
+ /* DASD path handling */
+ 
+ #define DASD_PATH_OPERATIONAL  1
+ #define DASD_PATH_TBV	       2
+ #define DASD_PATH_PP	       3
+ #define DASD_PATH_NPP	       4
+ #define DASD_PATH_MISCABLED    5
+ #define DASD_PATH_NOHPF        6
+ #define DASD_PATH_CUIR	       7
+ 
+ 
  struct dasd_path {
++<<<<<<< HEAD
 +	__u8 opm;
 +	__u8 tbvpm;
 +	__u8 ppm;
 +	__u8 npm;
++=======
+ 	unsigned long flags;
+ 	struct dasd_conf_data *conf_data;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  };
  
+ 
  struct dasd_profile_info {
  	/* legacy part of profile data, as in dasd_profile_info_t */
  	unsigned int dasd_io_reqs;	 /* number of requests processed */
@@@ -431,8 -464,9 +449,14 @@@ struct dasd_device 
  	/* Device discipline stuff. */
  	struct dasd_discipline *discipline;
  	struct dasd_discipline *base_discipline;
++<<<<<<< HEAD
 +	char *private;
 +	struct dasd_path path_data;
++=======
+ 	void *private;
+ 	struct dasd_path path[8];
+ 	__u8 opm;
++>>>>>>> c93461515a1a (s390/dasd: extend dasd path handling)
  
  	/* Device state and target state. */
  	int state, target;
diff --git a/drivers/s390/block/dasd.c b/drivers/s390/block/dasd.c
index 5f422bc676a9..42d8caffe0fa 100644
--- a/drivers/s390/block/dasd.c
+++ b/drivers/s390/block/dasd.c
@@ -1437,9 +1437,9 @@ int dasd_start_IO(struct dasd_ccw_req *cqr)
 	cqr->starttime = jiffies;
 	cqr->retries--;
 	if (!test_bit(DASD_CQR_VERIFY_PATH, &cqr->flags)) {
-		cqr->lpm &= device->path_data.opm;
+		cqr->lpm &= dasd_path_get_opm(device);
 		if (!cqr->lpm)
-			cqr->lpm = device->path_data.opm;
+			cqr->lpm = dasd_path_get_opm(device);
 	}
 	if (cqr->cpmode == 1) {
 		rc = ccw_device_tm_start(device->cdev, cqr->cpaddr,
@@ -1472,8 +1472,8 @@ int dasd_start_IO(struct dasd_ccw_req *cqr)
 			DBF_DEV_EVENT(DBF_WARNING, device,
 				      "start_IO: selected paths gone (%x)",
 				      cqr->lpm);
-		} else if (cqr->lpm != device->path_data.opm) {
-			cqr->lpm = device->path_data.opm;
+		} else if (cqr->lpm != dasd_path_get_opm(device)) {
+			cqr->lpm = dasd_path_get_opm(device);
 			DBF_DEV_EVENT(DBF_DEBUG, device, "%s",
 				      "start_IO: selected paths gone,"
 				      " retry on all paths");
@@ -1482,11 +1482,10 @@ int dasd_start_IO(struct dasd_ccw_req *cqr)
 				      "start_IO: all paths in opm gone,"
 				      " do path verification");
 			dasd_generic_last_path_gone(device);
-			device->path_data.opm = 0;
-			device->path_data.ppm = 0;
-			device->path_data.npm = 0;
-			device->path_data.tbvpm =
-				ccw_device_get_path_mask(device->cdev);
+			dasd_path_no_path(device);
+			dasd_path_set_tbvpm(device,
+					  ccw_device_get_path_mask(
+						  device->cdev));
 		}
 		break;
 	case -ENODEV:
@@ -1624,7 +1623,7 @@ void dasd_int_handler(struct ccw_device *cdev, unsigned long intparm,
 		switch (PTR_ERR(irb)) {
 		case -EIO:
 			if (cqr && cqr->status == DASD_CQR_CLEAR_PENDING) {
-				device = (struct dasd_device *) cqr->startdev;
+				device = cqr->startdev;
 				cqr->status = DASD_CQR_CLEARED;
 				dasd_device_clear_timer(device);
 				wake_up(&dasd_flush_wq);
@@ -1713,13 +1712,13 @@ void dasd_int_handler(struct ccw_device *cdev, unsigned long intparm,
 		 */
 		if (!test_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags) &&
 		    cqr->retries > 0) {
-			if (cqr->lpm == device->path_data.opm)
+			if (cqr->lpm == dasd_path_get_opm(device))
 				DBF_DEV_EVENT(DBF_DEBUG, device,
 					      "default ERP in fastpath "
 					      "(%i retries left)",
 					      cqr->retries);
 			if (!test_bit(DASD_CQR_VERIFY_PATH, &cqr->flags))
-				cqr->lpm = device->path_data.opm;
+				cqr->lpm = dasd_path_get_opm(device);
 			cqr->status = DASD_CQR_QUEUED;
 			next = cqr;
 		} else
@@ -1959,17 +1958,18 @@ static void __dasd_device_check_path_events(struct dasd_device *device)
 {
 	int rc;
 
-	if (device->path_data.tbvpm) {
-		if (device->stopped & ~(DASD_STOPPED_DC_WAIT |
-					DASD_UNRESUMED_PM))
-			return;
-		rc = device->discipline->verify_path(
-			device, device->path_data.tbvpm);
-		if (rc)
-			dasd_device_set_timer(device, 50);
-		else
-			device->path_data.tbvpm = 0;
-	}
+	if (!dasd_path_get_tbvpm(device))
+		return;
+
+	if (device->stopped &
+	    ~(DASD_STOPPED_DC_WAIT | DASD_UNRESUMED_PM))
+		return;
+	rc = device->discipline->verify_path(device,
+					     dasd_path_get_tbvpm(device));
+	if (rc)
+		dasd_device_set_timer(device, 50);
+	else
+		dasd_path_clear_all_verify(device);
 };
 
 /*
@@ -3477,14 +3477,12 @@ int dasd_generic_notify(struct ccw_device *cdev, int event)
 	case CIO_GONE:
 	case CIO_BOXED:
 	case CIO_NO_PATH:
-		device->path_data.opm = 0;
-		device->path_data.ppm = 0;
-		device->path_data.npm = 0;
+		dasd_path_no_path(device);
 		ret = dasd_generic_last_path_gone(device);
 		break;
 	case CIO_OPER:
 		ret = 1;
-		if (device->path_data.opm)
+		if (dasd_path_get_opm(device))
 			ret = dasd_generic_path_operational(device);
 		break;
 	}
@@ -3494,48 +3492,32 @@ int dasd_generic_notify(struct ccw_device *cdev, int event)
 
 void dasd_generic_path_event(struct ccw_device *cdev, int *path_event)
 {
-	int chp;
-	__u8 oldopm, eventlpm;
 	struct dasd_device *device;
+	int chp, oldopm;
 
 	device = dasd_device_from_cdev_locked(cdev);
 	if (IS_ERR(device))
 		return;
+
+	oldopm = dasd_path_get_opm(device);
 	for (chp = 0; chp < 8; chp++) {
-		eventlpm = 0x80 >> chp;
 		if (path_event[chp] & PE_PATH_GONE) {
-			oldopm = device->path_data.opm;
-			device->path_data.opm &= ~eventlpm;
-			device->path_data.ppm &= ~eventlpm;
-			device->path_data.npm &= ~eventlpm;
-			if (oldopm && !device->path_data.opm) {
-				dev_warn(&device->cdev->dev,
-					 "No verified channel paths remain "
-					 "for the device\n");
-				DBF_DEV_EVENT(DBF_WARNING, device,
-					      "%s", "last verified path gone");
-				dasd_eer_write(device, NULL, DASD_EER_NOPATH);
-				dasd_device_set_stop_bits(device,
-							  DASD_STOPPED_DC_WAIT);
-			}
+			dasd_path_notoper(device, chp);
 		}
 		if (path_event[chp] & PE_PATH_AVAILABLE) {
-			device->path_data.opm &= ~eventlpm;
-			device->path_data.ppm &= ~eventlpm;
-			device->path_data.npm &= ~eventlpm;
-			device->path_data.tbvpm |= eventlpm;
+			dasd_path_available(device, chp);
 			dasd_schedule_device_bh(device);
 		}
 		if (path_event[chp] & PE_PATHGROUP_ESTABLISHED) {
-			if (!(device->path_data.opm & eventlpm) &&
-			    !(device->path_data.tbvpm & eventlpm)) {
+			if (!dasd_path_is_operational(device, chp) &&
+			    !dasd_path_need_verify(device, chp)) {
 				/*
 				 * we can not establish a pathgroup on an
 				 * unavailable path, so trigger a path
 				 * verification first
 				 */
-				device->path_data.tbvpm |= eventlpm;
-				dasd_schedule_device_bh(device);
+			dasd_path_available(device, chp);
+			dasd_schedule_device_bh(device);
 			}
 			DBF_DEV_EVENT(DBF_WARNING, device, "%s",
 				      "Pathgroup re-established\n");
@@ -3543,17 +3525,26 @@ void dasd_generic_path_event(struct ccw_device *cdev, int *path_event)
 				device->discipline->kick_validate(device);
 		}
 	}
+	if (oldopm && !dasd_path_get_opm(device)) {
+		dev_warn(&device->cdev->dev,
+			 "No verified channel paths remain for the device\n");
+		DBF_DEV_EVENT(DBF_WARNING, device,
+			      "%s", "last verified path gone");
+		dasd_eer_write(device, NULL, DASD_EER_NOPATH);
+		dasd_device_set_stop_bits(device,
+					  DASD_STOPPED_DC_WAIT);
+	}
 	dasd_put_device(device);
 }
 EXPORT_SYMBOL_GPL(dasd_generic_path_event);
 
 int dasd_generic_verify_path(struct dasd_device *device, __u8 lpm)
 {
-	if (!device->path_data.opm && lpm) {
-		device->path_data.opm = lpm;
+	if (!dasd_path_get_opm(device) && lpm) {
+		dasd_path_set_opm(device, lpm);
 		dasd_generic_path_operational(device);
 	} else
-		device->path_data.opm |= lpm;
+		dasd_path_add_opm(device, lpm);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(dasd_generic_verify_path);
diff --git a/drivers/s390/block/dasd_3990_erp.c b/drivers/s390/block/dasd_3990_erp.c
index d26134713682..0198b54ddd59 100644
--- a/drivers/s390/block/dasd_3990_erp.c
+++ b/drivers/s390/block/dasd_3990_erp.c
@@ -152,7 +152,7 @@ dasd_3990_erp_alternate_path(struct dasd_ccw_req * erp)
 	opm = ccw_device_get_path_mask(device->cdev);
 	spin_unlock_irqrestore(get_ccwdev_lock(device->cdev), flags);
 	if (erp->lpm == 0)
-		erp->lpm = device->path_data.opm &
+		erp->lpm = dasd_path_get_opm(device) &
 			~(erp->irb.esw.esw0.sublog.lpum);
 	else
 		erp->lpm &= ~(erp->irb.esw.esw0.sublog.lpum);
@@ -273,7 +273,7 @@ static struct dasd_ccw_req *dasd_3990_erp_action_1(struct dasd_ccw_req *erp)
 	    !test_bit(DASD_CQR_VERIFY_PATH, &erp->flags)) {
 		erp->status = DASD_CQR_FILLED;
 		erp->retries = 10;
-		erp->lpm = erp->startdev->path_data.opm;
+		erp->lpm = dasd_path_get_opm(erp->startdev);
 		erp->function = dasd_3990_erp_action_1_sec;
 	}
 	return erp;
@@ -1914,7 +1914,7 @@ dasd_3990_erp_compound_path(struct dasd_ccw_req * erp, char *sense)
 		    !test_bit(DASD_CQR_VERIFY_PATH, &erp->flags)) {
 			/* reset the lpm and the status to be able to
 			 * try further actions. */
-			erp->lpm = erp->startdev->path_data.opm;
+			erp->lpm = dasd_path_get_opm(erp->startdev);
 			erp->status = DASD_CQR_NEED_ERP;
 		}
 	}
* Unmerged path drivers/s390/block/dasd_devmap.c
* Unmerged path drivers/s390/block/dasd_eckd.c
* Unmerged path drivers/s390/block/dasd_eckd.h
diff --git a/drivers/s390/block/dasd_erp.c b/drivers/s390/block/dasd_erp.c
index 3250cb471f78..b863189959c4 100644
--- a/drivers/s390/block/dasd_erp.c
+++ b/drivers/s390/block/dasd_erp.c
@@ -96,7 +96,7 @@ dasd_default_erp_action(struct dasd_ccw_req *cqr)
                              "default ERP called (%i retries left)",
                              cqr->retries);
 		if (!test_bit(DASD_CQR_VERIFY_PATH, &cqr->flags))
-			cqr->lpm = device->path_data.opm;
+			cqr->lpm = dasd_path_get_opm(device);
 		cqr->status = DASD_CQR_FILLED;
         } else {
 		pr_err("%s: default ERP has run out of retries and failed\n",
* Unmerged path drivers/s390/block/dasd_fba.c
* Unmerged path drivers/s390/block/dasd_int.h
