async_memcpy: convert to dmaengine_unmap_data

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 8971646294bda65f8666b60cb2cb3d5e172c99bf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/89716462.failed

Use the generic unmap object to unmap dma buffers.

	Cc: Vinod Koul <vinod.koul@intel.com>
	Cc: Tomasz Figa <t.figa@samsung.com>
	Cc: Dave Jiang <dave.jiang@intel.com>
	Reported-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
[bzolnier: add missing unmap->len initialization]
[bzolnier: fix whitespace damage]
	Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
	Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
[djbw: add DMA_ENGINE=n support]
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 8971646294bda65f8666b60cb2cb3d5e172c99bf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/dmaengine.c
#	include/linux/dmaengine.h
diff --cc drivers/dma/dmaengine.c
index d50b0664c5a9,54138b57b37c..000000000000
--- a/drivers/dma/dmaengine.c
+++ b/drivers/dma/dmaengine.c
@@@ -881,6 -902,235 +881,238 @@@ void dma_async_device_unregister(struc
  }
  EXPORT_SYMBOL(dma_async_device_unregister);
  
++<<<<<<< HEAD
++=======
+ struct dmaengine_unmap_pool {
+ 	struct kmem_cache *cache;
+ 	const char *name;
+ 	mempool_t *pool;
+ 	size_t size;
+ };
+ 
+ #define __UNMAP_POOL(x) { .size = x, .name = "dmaengine-unmap-" __stringify(x) }
+ static struct dmaengine_unmap_pool unmap_pool[] = {
+ 	__UNMAP_POOL(2),
+ 	#if IS_ENABLED(CONFIG_ASYNC_TX_DMA)
+ 	__UNMAP_POOL(16),
+ 	__UNMAP_POOL(128),
+ 	__UNMAP_POOL(256),
+ 	#endif
+ };
+ 
+ static struct dmaengine_unmap_pool *__get_unmap_pool(int nr)
+ {
+ 	int order = get_count_order(nr);
+ 
+ 	switch (order) {
+ 	case 0 ... 1:
+ 		return &unmap_pool[0];
+ 	case 2 ... 4:
+ 		return &unmap_pool[1];
+ 	case 5 ... 7:
+ 		return &unmap_pool[2];
+ 	case 8:
+ 		return &unmap_pool[3];
+ 	default:
+ 		BUG();
+ 		return NULL;
+ 	}
+ }
+ 
+ static void dmaengine_unmap(struct kref *kref)
+ {
+ 	struct dmaengine_unmap_data *unmap = container_of(kref, typeof(*unmap), kref);
+ 	struct device *dev = unmap->dev;
+ 	int cnt, i;
+ 
+ 	cnt = unmap->to_cnt;
+ 	for (i = 0; i < cnt; i++)
+ 		dma_unmap_page(dev, unmap->addr[i], unmap->len,
+ 			       DMA_TO_DEVICE);
+ 	cnt += unmap->from_cnt;
+ 	for (; i < cnt; i++)
+ 		dma_unmap_page(dev, unmap->addr[i], unmap->len,
+ 			       DMA_FROM_DEVICE);
+ 	cnt += unmap->bidi_cnt;
+ 	for (; i < cnt; i++)
+ 		dma_unmap_page(dev, unmap->addr[i], unmap->len,
+ 			       DMA_BIDIRECTIONAL);
+ 	mempool_free(unmap, __get_unmap_pool(cnt)->pool);
+ }
+ 
+ void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap)
+ {
+ 	if (unmap)
+ 		kref_put(&unmap->kref, dmaengine_unmap);
+ }
+ EXPORT_SYMBOL_GPL(dmaengine_unmap_put);
+ 
+ static void dmaengine_destroy_unmap_pool(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(unmap_pool); i++) {
+ 		struct dmaengine_unmap_pool *p = &unmap_pool[i];
+ 
+ 		if (p->pool)
+ 			mempool_destroy(p->pool);
+ 		p->pool = NULL;
+ 		if (p->cache)
+ 			kmem_cache_destroy(p->cache);
+ 		p->cache = NULL;
+ 	}
+ }
+ 
+ static int __init dmaengine_init_unmap_pool(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(unmap_pool); i++) {
+ 		struct dmaengine_unmap_pool *p = &unmap_pool[i];
+ 		size_t size;
+ 
+ 		size = sizeof(struct dmaengine_unmap_data) +
+ 		       sizeof(dma_addr_t) * p->size;
+ 
+ 		p->cache = kmem_cache_create(p->name, size, 0,
+ 					     SLAB_HWCACHE_ALIGN, NULL);
+ 		if (!p->cache)
+ 			break;
+ 		p->pool = mempool_create_slab_pool(1, p->cache);
+ 		if (!p->pool)
+ 			break;
+ 	}
+ 
+ 	if (i == ARRAY_SIZE(unmap_pool))
+ 		return 0;
+ 
+ 	dmaengine_destroy_unmap_pool();
+ 	return -ENOMEM;
+ }
+ 
+ struct dmaengine_unmap_data *
+ dmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags)
+ {
+ 	struct dmaengine_unmap_data *unmap;
+ 
+ 	unmap = mempool_alloc(__get_unmap_pool(nr)->pool, flags);
+ 	if (!unmap)
+ 		return NULL;
+ 
+ 	memset(unmap, 0, sizeof(*unmap));
+ 	kref_init(&unmap->kref);
+ 	unmap->dev = dev;
+ 
+ 	return unmap;
+ }
+ EXPORT_SYMBOL(dmaengine_get_unmap_data);
+ 
+ /**
+  * dma_async_memcpy_pg_to_pg - offloaded copy from page to page
+  * @chan: DMA channel to offload copy to
+  * @dest_pg: destination page
+  * @dest_off: offset in page to copy to
+  * @src_pg: source page
+  * @src_off: offset in page to copy from
+  * @len: length
+  *
+  * Both @dest_page/@dest_off and @src_page/@src_off must be mappable to a bus
+  * address according to the DMA mapping API rules for streaming mappings.
+  * Both @dest_page/@dest_off and @src_page/@src_off must stay memory resident
+  * (kernel memory or locked user space pages).
+  */
+ dma_cookie_t
+ dma_async_memcpy_pg_to_pg(struct dma_chan *chan, struct page *dest_pg,
+ 	unsigned int dest_off, struct page *src_pg, unsigned int src_off,
+ 	size_t len)
+ {
+ 	struct dma_device *dev = chan->device;
+ 	struct dma_async_tx_descriptor *tx;
+ 	struct dmaengine_unmap_data *unmap;
+ 	dma_cookie_t cookie;
+ 	unsigned long flags;
+ 
+ 	unmap = dmaengine_get_unmap_data(dev->dev, 2, GFP_NOIO);
+ 	if (!unmap)
+ 		return -ENOMEM;
+ 
+ 	unmap->to_cnt = 1;
+ 	unmap->from_cnt = 1;
+ 	unmap->addr[0] = dma_map_page(dev->dev, src_pg, src_off, len,
+ 				      DMA_TO_DEVICE);
+ 	unmap->addr[1] = dma_map_page(dev->dev, dest_pg, dest_off, len,
+ 				      DMA_FROM_DEVICE);
+ 	unmap->len = len;
+ 	flags = DMA_CTRL_ACK | DMA_COMPL_SKIP_SRC_UNMAP |
+ 		DMA_COMPL_SKIP_DEST_UNMAP;
+ 	tx = dev->device_prep_dma_memcpy(chan, unmap->addr[1], unmap->addr[0],
+ 					 len, flags);
+ 
+ 	if (!tx) {
+ 		dmaengine_unmap_put(unmap);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	dma_set_unmap(tx, unmap);
+ 	cookie = tx->tx_submit(tx);
+ 	dmaengine_unmap_put(unmap);
+ 
+ 	preempt_disable();
+ 	__this_cpu_add(chan->local->bytes_transferred, len);
+ 	__this_cpu_inc(chan->local->memcpy_count);
+ 	preempt_enable();
+ 
+ 	return cookie;
+ }
+ EXPORT_SYMBOL(dma_async_memcpy_pg_to_pg);
+ 
+ /**
+  * dma_async_memcpy_buf_to_buf - offloaded copy between virtual addresses
+  * @chan: DMA channel to offload copy to
+  * @dest: destination address (virtual)
+  * @src: source address (virtual)
+  * @len: length
+  *
+  * Both @dest and @src must be mappable to a bus address according to the
+  * DMA mapping API rules for streaming mappings.
+  * Both @dest and @src must stay memory resident (kernel memory or locked
+  * user space pages).
+  */
+ dma_cookie_t
+ dma_async_memcpy_buf_to_buf(struct dma_chan *chan, void *dest,
+ 			    void *src, size_t len)
+ {
+ 	return dma_async_memcpy_pg_to_pg(chan, virt_to_page(dest),
+ 					 (unsigned long) dest & ~PAGE_MASK,
+ 					 virt_to_page(src),
+ 					 (unsigned long) src & ~PAGE_MASK, len);
+ }
+ EXPORT_SYMBOL(dma_async_memcpy_buf_to_buf);
+ 
+ /**
+  * dma_async_memcpy_buf_to_pg - offloaded copy from address to page
+  * @chan: DMA channel to offload copy to
+  * @page: destination page
+  * @offset: offset in page to copy to
+  * @kdata: source address (virtual)
+  * @len: length
+  *
+  * Both @page/@offset and @kdata must be mappable to a bus address according
+  * to the DMA mapping API rules for streaming mappings.
+  * Both @page/@offset and @kdata must stay memory resident (kernel memory or
+  * locked user space pages)
+  */
+ dma_cookie_t
+ dma_async_memcpy_buf_to_pg(struct dma_chan *chan, struct page *page,
+ 			   unsigned int offset, void *kdata, size_t len)
+ {
+ 	return dma_async_memcpy_pg_to_pg(chan, page, offset,
+ 					 virt_to_page(kdata),
+ 					 (unsigned long) kdata & ~PAGE_MASK, len);
+ }
+ EXPORT_SYMBOL(dma_async_memcpy_buf_to_pg);
+ 
++>>>>>>> 8971646294bd (async_memcpy: convert to dmaengine_unmap_data)
  void dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,
  	struct dma_chan *chan)
  {
diff --cc include/linux/dmaengine.h
index 9ffca149944f,3782cdb782a8..000000000000
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@@ -471,6 -457,40 +471,43 @@@ struct dma_async_tx_descriptor 
  #endif
  };
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DMA_ENGINE
+ static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
+ 				 struct dmaengine_unmap_data *unmap)
+ {
+ 	kref_get(&unmap->kref);
+ 	tx->unmap = unmap;
+ }
+ 
+ struct dmaengine_unmap_data *
+ dmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags);
+ void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap);
+ #else
+ static inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,
+ 				 struct dmaengine_unmap_data *unmap)
+ {
+ }
+ static inline struct dmaengine_unmap_data *
+ dmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags)
+ {
+ 	return NULL;
+ }
+ static inline void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap)
+ {
+ }
+ #endif
+ 
+ static inline void dma_descriptor_unmap(struct dma_async_tx_descriptor *tx)
+ {
+ 	if (tx->unmap) {
+ 		dmaengine_unmap_put(tx->unmap);
+ 		tx->unmap = NULL;
+ 	}
+ }
+ 
++>>>>>>> 8971646294bd (async_memcpy: convert to dmaengine_unmap_data)
  #ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH
  static inline void txd_lock(struct dma_async_tx_descriptor *txd)
  {
diff --git a/crypto/async_tx/async_memcpy.c b/crypto/async_tx/async_memcpy.c
index 9e62feffb374..72750214f779 100644
--- a/crypto/async_tx/async_memcpy.c
+++ b/crypto/async_tx/async_memcpy.c
@@ -50,33 +50,37 @@ async_memcpy(struct page *dest, struct page *src, unsigned int dest_offset,
 						      &dest, 1, &src, 1, len);
 	struct dma_device *device = chan ? chan->device : NULL;
 	struct dma_async_tx_descriptor *tx = NULL;
+	struct dmaengine_unmap_data *unmap = NULL;
 
-	if (device && is_dma_copy_aligned(device, src_offset, dest_offset, len)) {
-		dma_addr_t dma_dest, dma_src;
-		unsigned long dma_prep_flags = 0;
+	if (device)
+		unmap = dmaengine_get_unmap_data(device->dev, 2, GFP_NOIO);
+
+	if (unmap && is_dma_copy_aligned(device, src_offset, dest_offset, len)) {
+		unsigned long dma_prep_flags = DMA_COMPL_SKIP_SRC_UNMAP |
+					       DMA_COMPL_SKIP_DEST_UNMAP;
 
 		if (submit->cb_fn)
 			dma_prep_flags |= DMA_PREP_INTERRUPT;
 		if (submit->flags & ASYNC_TX_FENCE)
 			dma_prep_flags |= DMA_PREP_FENCE;
-		dma_dest = dma_map_page(device->dev, dest, dest_offset, len,
-					DMA_FROM_DEVICE);
-
-		dma_src = dma_map_page(device->dev, src, src_offset, len,
-				       DMA_TO_DEVICE);
-
-		tx = device->device_prep_dma_memcpy(chan, dma_dest, dma_src,
-						    len, dma_prep_flags);
-		if (!tx) {
-			dma_unmap_page(device->dev, dma_dest, len,
-				       DMA_FROM_DEVICE);
-			dma_unmap_page(device->dev, dma_src, len,
-				       DMA_TO_DEVICE);
-		}
+
+		unmap->to_cnt = 1;
+		unmap->addr[0] = dma_map_page(device->dev, src, src_offset, len,
+					      DMA_TO_DEVICE);
+		unmap->from_cnt = 1;
+		unmap->addr[1] = dma_map_page(device->dev, dest, dest_offset, len,
+					      DMA_FROM_DEVICE);
+		unmap->len = len;
+
+		tx = device->device_prep_dma_memcpy(chan, unmap->addr[1],
+						    unmap->addr[0], len,
+						    dma_prep_flags);
 	}
 
 	if (tx) {
 		pr_debug("%s: (async) len: %zu\n", __func__, len);
+
+		dma_set_unmap(tx, unmap);
 		async_tx_submit(chan, tx, submit);
 	} else {
 		void *dest_buf, *src_buf;
@@ -96,6 +100,8 @@ async_memcpy(struct page *dest, struct page *src, unsigned int dest_offset,
 		async_tx_sync_epilog(submit);
 	}
 
+	dmaengine_unmap_put(unmap);
+
 	return tx;
 }
 EXPORT_SYMBOL_GPL(async_memcpy);
* Unmerged path drivers/dma/dmaengine.c
* Unmerged path include/linux/dmaengine.h
