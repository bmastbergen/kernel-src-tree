mm/hugetlb: improve locking in dissolve_free_huge_pages()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [mm] hugetlb: improve locking in dissolve_free_huge_pages() (Andrea Arcangeli) [1430172]
Rebuild_FUZZ: 97.30%
commit-author Gerald Schaefer <gerald.schaefer@de.ibm.com>
commit eb03aa008561004257900983193d024e57abdd96
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/eb03aa00.failed

For every pfn aligned to minimum_order, dissolve_free_huge_pages() will
call dissolve_free_huge_page() which takes the hugetlb spinlock, even if
the page is not huge at all or a hugepage that is in-use.

Improve this by doing the PageHuge() and page_count() checks already in
dissolve_free_huge_pages() before calling dissolve_free_huge_page().  In
dissolve_free_huge_page(), when holding the spinlock, those checks need
to be revalidated.

Link: http://lkml.kernel.org/r/20160926172811.94033-4-gerald.schaefer@de.ibm.com
	Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Rui Teng <rui.teng@linux.vnet.ibm.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit eb03aa008561004257900983193d024e57abdd96)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 2c12fc3891a9,770d83eb3f48..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1185,21 -1468,30 +1185,40 @@@ static void dissolve_free_huge_page(str
  /*
   * Dissolve free hugepages in a given pfn range. Used by memory hotplug to
   * make specified memory blocks removable from the system.
 - * Note that this will dissolve a free gigantic hugepage completely, if any
 - * part of it lies within the given range.
 - * Also note that if dissolve_free_huge_page() returns with an error, all
 - * free hugepages that were dissolved before that error are lost.
 + * Note that start_pfn should aligned with (minimum) hugepage size.
   */
 -int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)
 +void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)
  {
 +	unsigned int order = 8 * sizeof(void *);
  	unsigned long pfn;
++<<<<<<< HEAD
 +	struct hstate *h;
 +
 +	/* Set scan step to minimum hugepage size */
 +	for_each_hstate(h)
 +		if (order > huge_page_order(h))
 +			order = huge_page_order(h);
 +	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 << order));
 +	for (pfn = start_pfn; pfn < end_pfn; pfn += 1 << order)
 +		dissolve_free_huge_page(pfn_to_page(pfn));
++=======
+ 	struct page *page;
+ 	int rc = 0;
+ 
+ 	if (!hugepages_supported())
+ 		return rc;
+ 
+ 	for (pfn = start_pfn; pfn < end_pfn; pfn += 1 << minimum_order) {
+ 		page = pfn_to_page(pfn);
+ 		if (PageHuge(page) && !page_count(page)) {
+ 			rc = dissolve_free_huge_page(page);
+ 			if (rc)
+ 				break;
+ 		}
+ 	}
+ 
+ 	return rc;
++>>>>>>> eb03aa008561 (mm/hugetlb: improve locking in dissolve_free_huge_pages())
  }
  
  /*
* Unmerged path mm/hugetlb.c
