x86/mce/AMD: Read MSRs on the CPU allocating the threshold blocks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] mce/amd: Read MSRs on the CPU allocating the threshold blocks (David Arcari) [1389383]
Rebuild_FUZZ: 96.83%
commit-author Yazen Ghannam <Yazen.Ghannam@amd.com>
commit cfee4f6f0b2026380c6bc6913dbd27943df17371
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/cfee4f6f.failed

Scalable MCA systems allow non-core MCA banks to only be accessible by
certain CPUs. The MSRs for these banks are Read-as-Zero on other CPUs.

During allocate_threshold_blocks(), get_block_address() can be scheduled
on CPUs other than the one allocating the block. This causes the MSRs to
be read on the wrong CPU and results in incorrect behavior.

Add a @cpu parameter to get_block_address() and pass this in to ensure
that the MSRs are only read on the CPU that is allocating the block.

	Signed-off-by: Yazen Ghannam <Yazen.Ghannam@amd.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: http://lkml.kernel.org/r/1472673994-12235-2-git-send-email-Yazen.Ghannam@amd.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit cfee4f6f0b2026380c6bc6913dbd27943df17371)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/mcheck/mce_amd.c
diff --cc arch/x86/kernel/cpu/mcheck/mce_amd.c
index 40045b269893,9da92fb2e073..000000000000
--- a/arch/x86/kernel/cpu/mcheck/mce_amd.c
+++ b/arch/x86/kernel/cpu/mcheck/mce_amd.c
@@@ -264,6 -293,51 +264,54 @@@ static void deferred_error_interrupt_en
  	wrmsr(MSR_CU_DEF_ERR, low, high);
  }
  
++<<<<<<< HEAD
++=======
+ static u32 get_block_address(unsigned int cpu, u32 current_addr, u32 low, u32 high,
+ 			     unsigned int bank, unsigned int block)
+ {
+ 	u32 addr = 0, offset = 0;
+ 
+ 	if (mce_flags.smca) {
+ 		if (!block) {
+ 			addr = MSR_AMD64_SMCA_MCx_MISC(bank);
+ 		} else {
+ 			/*
+ 			 * For SMCA enabled processors, BLKPTR field of the
+ 			 * first MISC register (MCx_MISC0) indicates presence of
+ 			 * additional MISC register set (MISC1-4).
+ 			 */
+ 			u32 low, high;
+ 
+ 			if (rdmsr_safe_on_cpu(cpu, MSR_AMD64_SMCA_MCx_CONFIG(bank), &low, &high))
+ 				return addr;
+ 
+ 			if (!(low & MCI_CONFIG_MCAX))
+ 				return addr;
+ 
+ 			if (!rdmsr_safe_on_cpu(cpu, MSR_AMD64_SMCA_MCx_MISC(bank), &low, &high) &&
+ 			    (low & MASK_BLKPTR_LO))
+ 				addr = MSR_AMD64_SMCA_MCx_MISCy(bank, block - 1);
+ 		}
+ 		return addr;
+ 	}
+ 
+ 	/* Fall back to method we used for older processors: */
+ 	switch (block) {
+ 	case 0:
+ 		addr = msr_ops.misc(bank);
+ 		break;
+ 	case 1:
+ 		offset = ((low & MASK_BLKPTR_LO) >> 21);
+ 		if (offset)
+ 			addr = MCG_XBLK_ADDR + offset;
+ 		break;
+ 	default:
+ 		addr = ++current_addr;
+ 	}
+ 	return addr;
+ }
+ 
++>>>>>>> cfee4f6f0b20 (x86/mce/AMD: Read MSRs on the CPU allocating the threshold blocks)
  static int
  prepare_threshold_block(unsigned int bank, unsigned int block, u32 addr,
  			int offset, u32 misc_high)
@@@ -352,16 -426,9 +400,22 @@@ void mce_amd_feature_init(struct cpuinf
  
  	for (bank = 0; bank < mca_cfg.banks; ++bank) {
  		for (block = 0; block < NR_BLOCKS; ++block) {
++<<<<<<< HEAD
 +			if (block == 0)
 +				address = MSR_IA32_MCx_MISC(bank);
 +			else if (block == 1) {
 +				address = (low & MASK_BLKPTR_LO) >> 21;
 +				if (!address)
 +					break;
 +
 +				address += MCG_XBLK_ADDR;
 +			} else
 +				++address;
++=======
+ 			address = get_block_address(cpu, address, low, high, bank, block);
+ 			if (!address)
+ 				break;
++>>>>>>> cfee4f6f0b20 (x86/mce/AMD: Read MSRs on the CPU allocating the threshold blocks)
  
  			if (rdmsr_safe(address, &low, &high))
  				break;
@@@ -466,16 -551,9 +519,22 @@@ static void amd_threshold_interrupt(voi
  		if (!(per_cpu(bank_map, cpu) & (1 << bank)))
  			continue;
  		for (block = 0; block < NR_BLOCKS; ++block) {
++<<<<<<< HEAD
 +			if (block == 0) {
 +				address = MSR_IA32_MCx_MISC(bank);
 +			} else if (block == 1) {
 +				address = (low & MASK_BLKPTR_LO) >> 21;
 +				if (!address)
 +					break;
 +				address += MCG_XBLK_ADDR;
 +			} else {
 +				++address;
 +			}
++=======
+ 			address = get_block_address(cpu, address, low, high, bank, block);
+ 			if (!address)
+ 				break;
++>>>>>>> cfee4f6f0b20 (x86/mce/AMD: Read MSRs on the CPU allocating the threshold blocks)
  
  			if (rdmsr_safe(address, &low, &high))
  				break;
@@@ -695,16 -773,11 +754,22 @@@ static int allocate_threshold_blocks(un
  	if (err)
  		goto out_free;
  recurse:
++<<<<<<< HEAD
 +	if (!block) {
 +		address = (low & MASK_BLKPTR_LO) >> 21;
 +		if (!address)
 +			return 0;
 +		address += MCG_XBLK_ADDR;
 +	} else {
 +		++address;
 +	}
++=======
+ 	address = get_block_address(cpu, address, low, high, bank, ++block);
+ 	if (!address)
+ 		return 0;
++>>>>>>> cfee4f6f0b20 (x86/mce/AMD: Read MSRs on the CPU allocating the threshold blocks)
  
 -	err = allocate_threshold_blocks(cpu, bank, block, address);
 +	err = allocate_threshold_blocks(cpu, bank, ++block, address);
  	if (err)
  		goto out_free;
  
* Unmerged path arch/x86/kernel/cpu/mcheck/mce_amd.c
