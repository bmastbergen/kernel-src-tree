xfs: xfs_cluster_write is redundant

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit ad68972acb82c3e8bba316d542ab204984cb1f1c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ad68972a.failed

xfs_cluster_write() is not necessary now that xfs_vm_writepages()
aggregates writepage calls across a single mapping. This means we no
longer need to do page lookups in xfs_cluster_write, so writeback
only needs to look up th epage cache once per page being written.
This also removes a large amount of mostly duplicate code between
xfs_do_writepage() and xfs_convert_page().

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit ad68972acb82c3e8bba316d542ab204984cb1f1c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_aops.c
diff --cc fs/xfs/xfs_aops.c
index acf6c4a54883,f75c7c99cb63..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -648,183 -650,6 +648,186 @@@ xfs_check_page_type
  	return false;
  }
  
++<<<<<<< HEAD
 +/*
 + * Allocate & map buffers for page given the extent map. Write it out.
 + * except for the original page of a writepage, this is called on
 + * delalloc/unwritten pages only, for the original page it is possible
 + * that the page has no mapping at all.
 + */
 +STATIC int
 +xfs_convert_page(
 +	struct inode		*inode,
 +	struct page		*page,
 +	loff_t			tindex,
 +	struct xfs_bmbt_irec	*imap,
 +	xfs_ioend_t		**ioendp,
 +	struct writeback_control *wbc)
 +{
 +	struct buffer_head	*bh, *head;
 +	xfs_off_t		end_offset;
 +	unsigned long		p_offset;
 +	unsigned int		type;
 +	int			len, page_dirty;
 +	int			count = 0, done = 0, uptodate = 1;
 + 	xfs_off_t		offset = page_offset(page);
 +
 +	if (page->index != tindex)
 +		goto fail;
 +	if (!trylock_page(page))
 +		goto fail;
 +	if (PageWriteback(page))
 +		goto fail_unlock_page;
 +	if (page->mapping != inode->i_mapping)
 +		goto fail_unlock_page;
 +	if (!xfs_check_page_type(page, (*ioendp)->io_type, false))
 +		goto fail_unlock_page;
 +
 +	/*
 +	 * page_dirty is initially a count of buffers on the page before
 +	 * EOF and is decremented as we move each into a cleanable state.
 +	 *
 +	 * Derivation:
 +	 *
 +	 * End offset is the highest offset that this page should represent.
 +	 * If we are on the last page, (end_offset & (PAGE_CACHE_SIZE - 1))
 +	 * will evaluate non-zero and be less than PAGE_CACHE_SIZE and
 +	 * hence give us the correct page_dirty count. On any other page,
 +	 * it will be zero and in that case we need page_dirty to be the
 +	 * count of buffers on the page.
 +	 */
 +	end_offset = min_t(unsigned long long,
 +			(xfs_off_t)(page->index + 1) << PAGE_CACHE_SHIFT,
 +			i_size_read(inode));
 +
 +	/*
 +	 * If the current map does not span the entire page we are about to try
 +	 * to write, then give up. The only way we can write a page that spans
 +	 * multiple mappings in a single writeback iteration is via the
 +	 * xfs_vm_writepage() function. Data integrity writeback requires the
 +	 * entire page to be written in a single attempt, otherwise the part of
 +	 * the page we don't write here doesn't get written as part of the data
 +	 * integrity sync.
 +	 *
 +	 * For normal writeback, we also don't attempt to write partial pages
 +	 * here as it simply means that write_cache_pages() will see it under
 +	 * writeback and ignore the page until some point in the future, at
 +	 * which time this will be the only page in the file that needs
 +	 * writeback.  Hence for more optimal IO patterns, we should always
 +	 * avoid partial page writeback due to multiple mappings on a page here.
 +	 */
 +	if (!xfs_imap_valid(inode, imap, end_offset))
 +		goto fail_unlock_page;
 +
 +	len = 1 << inode->i_blkbits;
 +	p_offset = min_t(unsigned long, end_offset & (PAGE_CACHE_SIZE - 1),
 +					PAGE_CACHE_SIZE);
 +	p_offset = p_offset ? roundup(p_offset, len) : PAGE_CACHE_SIZE;
 +	page_dirty = p_offset / len;
 +
 +	/*
 +	 * The moment we find a buffer that doesn't match our current type
 +	 * specification or can't be written, abort the loop and start
 +	 * writeback. As per the above xfs_imap_valid() check, only
 +	 * xfs_vm_writepage() can handle partial page writeback fully - we are
 +	 * limited here to the buffers that are contiguous with the current
 +	 * ioend, and hence a buffer we can't write breaks that contiguity and
 +	 * we have to defer the rest of the IO to xfs_vm_writepage().
 +	 */
 +	bh = head = page_buffers(page);
 +	do {
 +		if (offset >= end_offset)
 +			break;
 +		if (!buffer_uptodate(bh))
 +			uptodate = 0;
 +		if (!(PageUptodate(page) || buffer_uptodate(bh))) {
 +			done = 1;
 +			break;
 +		}
 +
 +		if (buffer_unwritten(bh) || buffer_delay(bh) ||
 +		    buffer_mapped(bh)) {
 +			if (buffer_unwritten(bh))
 +				type = XFS_IO_UNWRITTEN;
 +			else if (buffer_delay(bh))
 +				type = XFS_IO_DELALLOC;
 +			else
 +				type = XFS_IO_OVERWRITE;
 +
 +			/*
 +			 * imap should always be valid because of the above
 +			 * partial page end_offset check on the imap.
 +			 */
 +			ASSERT(xfs_imap_valid(inode, imap, offset));
 +
 +			lock_buffer(bh);
 +			if (type != XFS_IO_OVERWRITE)
 +				xfs_map_at_offset(inode, bh, imap, offset);
 +			xfs_add_to_ioend(inode, bh, offset, type,
 +					 ioendp, done);
 +
 +			page_dirty--;
 +			count++;
 +		} else {
 +			done = 1;
 +			break;
 +		}
 +	} while (offset += len, (bh = bh->b_this_page) != head);
 +
 +	if (uptodate && bh == head)
 +		SetPageUptodate(page);
 +
 +	if (count) {
 +		if (--wbc->nr_to_write <= 0 &&
 +		    wbc->sync_mode == WB_SYNC_NONE)
 +			done = 1;
 +	}
 +	xfs_start_page_writeback(page, !page_dirty, count);
 +
 +	return done;
 + fail_unlock_page:
 +	unlock_page(page);
 + fail:
 +	return 1;
 +}
 +
 +/*
 + * Convert & write out a cluster of pages in the same extent as defined
 + * by mp and following the start page.
 + */
 +STATIC void
 +xfs_cluster_write(
 +	struct inode		*inode,
 +	pgoff_t			tindex,
 +	struct xfs_bmbt_irec	*imap,
 +	xfs_ioend_t		**ioendp,
 +	struct writeback_control *wbc,
 +	pgoff_t			tlast)
 +{
 +	struct pagevec		pvec;
 +	int			done = 0, i;
 +
 +	pagevec_init(&pvec, 0);
 +	while (!done && tindex <= tlast) {
 +		unsigned len = min_t(pgoff_t, PAGEVEC_SIZE, tlast - tindex + 1);
 +
 +		if (!pagevec_lookup(&pvec, inode->i_mapping, tindex, len))
 +			break;
 +
 +		for (i = 0; i < pagevec_count(&pvec); i++) {
 +			done = xfs_convert_page(inode, pvec.pages[i], tindex++,
 +					imap, ioendp, wbc);
 +			if (done)
 +				break;
 +		}
 +
 +		pagevec_release(&pvec);
 +		cond_resched();
 +	}
 +}
 +
++=======
++>>>>>>> ad68972acb82 (xfs: xfs_cluster_write is redundant)
  STATIC void
  xfs_vm_invalidatepage(
  	struct page		*page,
@@@ -932,20 -756,19 +935,20 @@@ xfs_writepage_submit
   * For any other dirty buffer heads on the page we should flush them.
   */
  STATIC int
 -xfs_do_writepage(
 +xfs_vm_writepage(
  	struct page		*page,
 -	struct writeback_control *wbc,
 -	void			*data)
 +	struct writeback_control *wbc)
  {
 -	struct xfs_writepage_ctx *wpc = data;
  	struct inode		*inode = page->mapping->host;
  	struct buffer_head	*bh, *head;
 +	struct xfs_bmbt_irec	imap;
 +	xfs_ioend_t		*ioend = NULL, *iohead = NULL;
  	loff_t			offset;
 +	unsigned int		type;
  	__uint64_t              end_offset;
- 	pgoff_t                 end_index, last_index;
+ 	pgoff_t                 end_index;
  	ssize_t			len;
 -	int			err, uptodate = 1;
 +	int			err, imap_valid = 0, uptodate = 1;
  	int			count = 0;
  
  	trace_xfs_writepage(inode, page, 0, 0);
@@@ -1129,38 -939,8 +1131,43 @@@
  
  	xfs_start_page_writeback(page, 1, count);
  
++<<<<<<< HEAD
 +	/* if there is no IO to be submitted for this page, we are done */
 +	if (!ioend)
 +		return 0;
 +
 +	ASSERT(iohead);
 +	ASSERT(err == 0);
 +
 +	/*
 +	 * Any errors from this point onwards need tobe reported through the IO
 +	 * completion path as we have marked the initial page as under writeback
 +	 * and unlocked it.
 +	 */
 +	if (imap_valid) {
 +		xfs_off_t		end_index;
 +
 +		end_index = imap.br_startoff + imap.br_blockcount;
 +
 +		/* to bytes */
 +		end_index <<= inode->i_blkbits;
 +
 +		/* to pages */
 +		end_index = (end_index - 1) >> PAGE_CACHE_SHIFT;
 +
 +		/* check against file size */
 +		if (end_index > last_index)
 +			end_index = last_index;
 +
 +		xfs_cluster_write(inode, page->index + 1, &imap, &ioend,
 +				  wbc, end_index);
 +	}
 +
 +	return xfs_writepage_submit(ioend, iohead, wbc, 0);
++=======
+ 	ASSERT(wpc->iohead || !count);
+ 	return 0;
++>>>>>>> ad68972acb82 (xfs: xfs_cluster_write is redundant)
  
  error:
  	/*
* Unmerged path fs/xfs/xfs_aops.c
