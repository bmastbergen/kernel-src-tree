/dev/dax, core: file operations and dax-mmap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit dee410792419aaa8bc3e3b35d2ccb6515835916d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/dee41079.failed

The "Device DAX" core enables dax mappings of performance / feature
differentiated memory.  An open mapping or file handle keeps the backing
struct device live, but new mappings are only possible while the device
is enabled.   Faults are handled under rcu_read_lock to synchronize
with the enabled state of the device.

Similar to the filesystem-dax case the backing memory may optionally
have struct page entries.  However, unlike fs-dax there is no support
for private mappings, or mappings that are not backed by media (see
use of zero-page in fs-dax).

Mappings are always guaranteed to match the alignment of the dax_region.
If the dax_region is configured to have a 2MB alignment, all mappings
are guaranteed to be backed by a pmd entry.  Contrast this determinism
with the fs-dax case where pmd mappings are opportunistic.  If userspace
attempts to force a misaligned mapping, the driver will fail the mmap
attempt.  See dax_dev_check_vma() for other scenarios that are rejected,
like MAP_PRIVATE mappings.

	Cc: Hannes Reinecke <hare@suse.de>
	Cc: Jeff Moyer <jmoyer@redhat.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Acked-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit dee410792419aaa8bc3e3b35d2ccb6515835916d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/Kconfig
#	drivers/dax/dax.c
* Unmerged path drivers/dax/Kconfig
* Unmerged path drivers/dax/dax.c
* Unmerged path drivers/dax/Kconfig
* Unmerged path drivers/dax/dax.c
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 70b20a937e19..d96670193c4e 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -904,6 +904,7 @@ int vmf_insert_pfn_pmd(struct vm_area_struct *vma, unsigned long addr,
 	insert_pfn_pmd(vma, addr, pmd, pfn, pgprot, write);
 	return VM_FAULT_NOPAGE;
 }
+EXPORT_SYMBOL_GPL(vmf_insert_pfn_pmd);
 
 int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		  pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 7b1284e0fdb5..da1fa1f987a1 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -400,6 +400,7 @@ pgoff_t linear_hugepage_index(struct vm_area_struct *vma,
 {
 	return vma_hugecache_offset(hstate_vma(vma), vma, address);
 }
+EXPORT_SYMBOL_GPL(linear_hugepage_index);
 
 /*
  * Return the size of the pages allocated when backing a VMA. In the majority
