dm: flush queued bios when process blocks to avoid deadlock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit d67a5f4b5947aba4bfe9a80a2b86079c215ca755
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/d67a5f4b.failed

Commit df2cb6daa4 ("block: Avoid deadlocks with bio allocation by
stacking drivers") created a workqueue for every bio set and code
in bio_alloc_bioset() that tries to resolve some low-memory deadlocks
by redirecting bios queued on current->bio_list to the workqueue if the
system is low on memory.  However other deadlocks (see below **) may
happen, without any low memory condition, because generic_make_request
is queuing bios to current->bio_list (rather than submitting them).

** the related dm-snapshot deadlock is detailed here:
https://www.redhat.com/archives/dm-devel/2016-July/msg00065.html

Fix this deadlock by redirecting any bios on current->bio_list to the
bio_set's rescue workqueue on every schedule() call.  Consequently,
when the process blocks on a mutex, the bios queued on
current->bio_list are dispatched to independent workqueus and they can
complete without waiting for the mutex to be available.

The structure blk_plug contains an entry cb_list and this list can contain
arbitrary callback functions that are called when the process blocks.
To implement this fix DM (ab)uses the onstack plug's cb_list interface
to get its flush_current_bio_list() called at schedule() time.

This fixes the snapshot deadlock - if the map method blocks,
flush_current_bio_list() will be called and it redirects bios waiting
on current->bio_list to appropriate workqueues.

Fixes: https://bugzilla.redhat.com/show_bug.cgi?id=1267650
Depends-on: df2cb6daa4 ("block: Avoid deadlocks with bio allocation by stacking drivers")
	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit d67a5f4b5947aba4bfe9a80a2b86079c215ca755)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm.c
diff --cc drivers/md/dm.c
index ec8896186c46,0ff5469c03d2..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -908,6 -905,123 +908,126 @@@ int dm_set_target_max_io_len(struct dm_
  }
  EXPORT_SYMBOL_GPL(dm_set_target_max_io_len);
  
++<<<<<<< HEAD
++=======
+ static long dm_blk_direct_access(struct block_device *bdev, sector_t sector,
+ 				 void **kaddr, pfn_t *pfn, long size)
+ {
+ 	struct mapped_device *md = bdev->bd_disk->private_data;
+ 	struct dm_table *map;
+ 	struct dm_target *ti;
+ 	int srcu_idx;
+ 	long len, ret = -EIO;
+ 
+ 	map = dm_get_live_table(md, &srcu_idx);
+ 	if (!map)
+ 		goto out;
+ 
+ 	ti = dm_table_find_target(map, sector);
+ 	if (!dm_target_is_valid(ti))
+ 		goto out;
+ 
+ 	len = max_io_len(sector, ti) << SECTOR_SHIFT;
+ 	size = min(len, size);
+ 
+ 	if (ti->type->direct_access)
+ 		ret = ti->type->direct_access(ti, sector, kaddr, pfn, size);
+ out:
+ 	dm_put_live_table(md, srcu_idx);
+ 	return min(ret, size);
+ }
+ 
+ /*
+  * A target may call dm_accept_partial_bio only from the map routine.  It is
+  * allowed for all bio types except REQ_PREFLUSH.
+  *
+  * dm_accept_partial_bio informs the dm that the target only wants to process
+  * additional n_sectors sectors of the bio and the rest of the data should be
+  * sent in a next bio.
+  *
+  * A diagram that explains the arithmetics:
+  * +--------------------+---------------+-------+
+  * |         1          |       2       |   3   |
+  * +--------------------+---------------+-------+
+  *
+  * <-------------- *tio->len_ptr --------------->
+  *                      <------- bi_size ------->
+  *                      <-- n_sectors -->
+  *
+  * Region 1 was already iterated over with bio_advance or similar function.
+  *	(it may be empty if the target doesn't use bio_advance)
+  * Region 2 is the remaining bio size that the target wants to process.
+  *	(it may be empty if region 1 is non-empty, although there is no reason
+  *	 to make it empty)
+  * The target requires that region 3 is to be sent in the next bio.
+  *
+  * If the target wants to receive multiple copies of the bio (via num_*bios, etc),
+  * the partially processed part (the sum of regions 1+2) must be the same for all
+  * copies of the bio.
+  */
+ void dm_accept_partial_bio(struct bio *bio, unsigned n_sectors)
+ {
+ 	struct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);
+ 	unsigned bi_size = bio->bi_iter.bi_size >> SECTOR_SHIFT;
+ 	BUG_ON(bio->bi_opf & REQ_PREFLUSH);
+ 	BUG_ON(bi_size > *tio->len_ptr);
+ 	BUG_ON(n_sectors > bi_size);
+ 	*tio->len_ptr -= bi_size - n_sectors;
+ 	bio->bi_iter.bi_size = n_sectors << SECTOR_SHIFT;
+ }
+ EXPORT_SYMBOL_GPL(dm_accept_partial_bio);
+ 
+ /*
+  * Flush current->bio_list when the target map method blocks.
+  * This fixes deadlocks in snapshot and possibly in other targets.
+  */
+ struct dm_offload {
+ 	struct blk_plug plug;
+ 	struct blk_plug_cb cb;
+ };
+ 
+ static void flush_current_bio_list(struct blk_plug_cb *cb, bool from_schedule)
+ {
+ 	struct dm_offload *o = container_of(cb, struct dm_offload, cb);
+ 	struct bio_list list;
+ 	struct bio *bio;
+ 
+ 	INIT_LIST_HEAD(&o->cb.list);
+ 
+ 	if (unlikely(!current->bio_list))
+ 		return;
+ 
+ 	list = *current->bio_list;
+ 	bio_list_init(current->bio_list);
+ 
+ 	while ((bio = bio_list_pop(&list))) {
+ 		struct bio_set *bs = bio->bi_pool;
+ 		if (unlikely(!bs) || bs == fs_bio_set) {
+ 			bio_list_add(current->bio_list, bio);
+ 			continue;
+ 		}
+ 
+ 		spin_lock(&bs->rescue_lock);
+ 		bio_list_add(&bs->rescue_list, bio);
+ 		queue_work(bs->rescue_workqueue, &bs->rescue_work);
+ 		spin_unlock(&bs->rescue_lock);
+ 	}
+ }
+ 
+ static void dm_offload_start(struct dm_offload *o)
+ {
+ 	blk_start_plug(&o->plug);
+ 	o->cb.callback = flush_current_bio_list;
+ 	list_add(&o->cb.list, &current->plug->cb_list);
+ }
+ 
+ static void dm_offload_end(struct dm_offload *o)
+ {
+ 	list_del(&o->cb.list);
+ 	blk_finish_plug(&o->plug);
+ }
+ 
++>>>>>>> d67a5f4b5947 (dm: flush queued bios when process blocks to avoid deadlock)
  static void __map_bio(struct dm_target_io *tio)
  {
  	int r;
@@@ -923,8 -1038,12 +1044,16 @@@
  	 * this io.
  	 */
  	atomic_inc(&tio->io->io_count);
++<<<<<<< HEAD
 +	sector = clone->bi_sector;
++=======
+ 	sector = clone->bi_iter.bi_sector;
+ 
+ 	dm_offload_start(&o);
++>>>>>>> d67a5f4b5947 (dm: flush queued bios when process blocks to avoid deadlock)
  	r = ti->type->map(ti, clone);
+ 	dm_offload_end(&o);
+ 
  	if (r == DM_MAPIO_REMAPPED) {
  		/* the bio has been remapped so dispatch it */
  
* Unmerged path drivers/md/dm.c
