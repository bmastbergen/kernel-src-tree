NTB: Use NUMA memory and DMA chan in transport

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [ntb] Use NUMA memory and DMA chan in transport (Suravee Suthikulpanit) [1303727]
Rebuild_FUZZ: 94.25%
commit-author Allen Hubbe <Allen.Hubbe@emc.com>
commit 1199aa61264a74717bc747e7031673242bad5119
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/1199aa61.failed

Allocate memory and request the DMA channel for the same NUMA node as
the NTB device.

	Signed-off-by: Allen Hubbe <Allen.Hubbe@emc.com>
	Signed-off-by: Jon Mason <jdmason@kudzu.us>
(cherry picked from commit 1199aa61264a74717bc747e7031673242bad5119)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/ntb/ntb_transport.c
diff --cc drivers/ntb/ntb_transport.c
index bf77f3a0b404,dc14ec81c43e..000000000000
--- a/drivers/ntb/ntb_transport.c
+++ b/drivers/ntb/ntb_transport.c
@@@ -313,10 -342,11 +313,15 @@@ EXPORT_SYMBOL_GPL(ntb_unregister_client
   *
   * Register an NTB client device with the NTB transport layer
   */
 -int ntb_transport_register_client_dev(char *device_name)
 +int ntb_register_client_dev(char *device_name)
  {
  	struct ntb_transport_client_dev *client_dev;
++<<<<<<< HEAD
 +	struct ntb_transport *nt;
++=======
+ 	struct ntb_transport_ctx *nt;
+ 	int node;
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  	int rc, i = 0;
  
  	if (list_empty(&ntb_transport_list))
@@@ -325,8 -355,10 +330,15 @@@
  	list_for_each_entry(nt, &ntb_transport_list, entry) {
  		struct device *dev;
  
++<<<<<<< HEAD
 +		client_dev = kzalloc(sizeof(struct ntb_transport_client_dev),
 +				     GFP_KERNEL);
++=======
+ 		node = dev_to_node(&nt->ndev->dev);
+ 
+ 		client_dev = kzalloc_node(sizeof(*client_dev),
+ 					  GFP_KERNEL, node);
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  		if (!client_dev) {
  			rc = -ENOMEM;
  			goto err;
@@@ -917,36 -947,81 +929,87 @@@ static int ntb_transport_init_queue(str
  	return 0;
  }
  
 -static int ntb_transport_probe(struct ntb_client *self, struct ntb_dev *ndev)
 +int ntb_transport_init(struct pci_dev *pdev)
  {
++<<<<<<< HEAD
 +	struct ntb_transport *nt;
 +	int rc, i;
 +
 +	nt = kzalloc(sizeof(struct ntb_transport), GFP_KERNEL);
 +	if (!nt)
 +		return -ENOMEM;
 +
 +	nt->ndev = ntb_register_transport(pdev, nt);
 +	if (!nt->ndev) {
 +		rc = -EIO;
++=======
+ 	struct ntb_transport_ctx *nt;
+ 	struct ntb_transport_mw *mw;
+ 	unsigned int mw_count, qp_count;
+ 	u64 qp_bitmap;
+ 	int node;
+ 	int rc, i;
+ 
+ 	if (ntb_db_is_unsafe(ndev))
+ 		dev_dbg(&ndev->dev,
+ 			"doorbell is unsafe, proceed anyway...\n");
+ 	if (ntb_spad_is_unsafe(ndev))
+ 		dev_dbg(&ndev->dev,
+ 			"scratchpad is unsafe, proceed anyway...\n");
+ 
+ 	node = dev_to_node(&ndev->dev);
+ 
+ 	nt = kzalloc_node(sizeof(*nt), GFP_KERNEL, node);
+ 	if (!nt)
+ 		return -ENOMEM;
+ 
+ 	nt->ndev = ndev;
+ 
+ 	mw_count = ntb_mw_count(ndev);
+ 
+ 	nt->mw_count = mw_count;
+ 
+ 	nt->mw_vec = kzalloc_node(mw_count * sizeof(*nt->mw_vec),
+ 				  GFP_KERNEL, node);
+ 	if (!nt->mw_vec) {
+ 		rc = -ENOMEM;
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  		goto err;
  	}
  
 -	for (i = 0; i < mw_count; i++) {
 -		mw = &nt->mw_vec[i];
 -
 -		rc = ntb_mw_get_range(ndev, i, &mw->phys_addr, &mw->phys_size,
 -				      &mw->xlat_align, &mw->xlat_align_size);
 -		if (rc)
 -			goto err1;
 -
 -		mw->vbase = ioremap(mw->phys_addr, mw->phys_size);
 -		if (!mw->vbase) {
 -			rc = -ENOMEM;
 -			goto err1;
 -		}
 -
 -		mw->buff_size = 0;
 -		mw->xlat_size = 0;
 -		mw->virt_addr = NULL;
 -		mw->dma_addr = 0;
 +	nt->mw = kcalloc(ntb_max_mw(nt->ndev), sizeof(struct ntb_transport_mw),
 +			 GFP_KERNEL);
 +	if (!nt->mw) {
 +		rc = -ENOMEM;
 +		goto err1;
  	}
  
 -	qp_bitmap = ntb_db_valid_mask(ndev);
 +	if (max_num_clients)
 +		nt->max_qps = min(ntb_max_cbs(nt->ndev), max_num_clients);
 +	else
 +		nt->max_qps = min(ntb_max_cbs(nt->ndev), ntb_max_mw(nt->ndev));
  
++<<<<<<< HEAD
 +	nt->qps = kcalloc(nt->max_qps, sizeof(struct ntb_transport_qp),
 +			  GFP_KERNEL);
 +	if (!nt->qps) {
++=======
+ 	qp_count = ilog2(qp_bitmap);
+ 	if (max_num_clients && max_num_clients < qp_count)
+ 		qp_count = max_num_clients;
+ 	else if (mw_count < qp_count)
+ 		qp_count = mw_count;
+ 
+ 	qp_bitmap &= BIT_ULL(qp_count) - 1;
+ 
+ 	nt->qp_count = qp_count;
+ 	nt->qp_bitmap = qp_bitmap;
+ 	nt->qp_bitmap_free = qp_bitmap;
+ 
+ 	nt->qp_vec = kzalloc_node(qp_count * sizeof(*nt->qp_vec),
+ 				  GFP_KERNEL, node);
+ 	if (!nt->qp_vec) {
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  		rc = -ENOMEM;
  		goto err2;
  	}
@@@ -1414,8 -1516,15 +1477,13 @@@ static void ntb_send_link_down(struct n
  	if (rc)
  		dev_err(&pdev->dev, "ntb: QP%d unable to send linkdown msg\n",
  			qp->qp_num);
 -
 -	ntb_qp_link_down_reset(qp);
  }
  
+ static bool ntb_dma_filter_fn(struct dma_chan *chan, void *node)
+ {
+ 	return dev_to_node(&chan->dev->device) == (int)(unsigned long)node;
+ }
+ 
  /**
   * ntb_transport_create_queue - Create a new NTB transport layer queue
   * @rx_handler: receive callback function
@@@ -1431,19 -1540,26 +1499,27 @@@
   * RETURNS: pointer to newly created ntb_queue, NULL on error.
   */
  struct ntb_transport_qp *
 -ntb_transport_create_queue(void *data, struct device *client_dev,
 +ntb_transport_create_queue(void *data, struct pci_dev *pdev,
  			   const struct ntb_queue_handlers *handlers)
  {
 -	struct ntb_dev *ndev;
 -	struct pci_dev *pdev;
 -	struct ntb_transport_ctx *nt;
  	struct ntb_queue_entry *entry;
  	struct ntb_transport_qp *qp;
 -	u64 qp_bit;
 +	struct ntb_transport *nt;
  	unsigned int free_queue;
++<<<<<<< HEAD
 +	int rc, i;
++=======
+ 	dma_cap_mask_t dma_mask;
+ 	int node;
+ 	int i;
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  
 -	ndev = dev_ntb(client_dev->parent);
 -	pdev = ndev->pdev;
 -	nt = ndev->ctx;
 +	nt = ntb_find_transport(pdev);
 +	if (!nt)
 +		goto err;
  
+ 	node = dev_to_node(&ndev->dev);
+ 
  	free_queue = ffs(nt->qp_bitmap);
  	if (!free_queue)
  		goto err;
@@@ -1459,15 -1577,16 +1535,20 @@@
  	qp->tx_handler = handlers->tx_handler;
  	qp->event_handler = handlers->event_handler;
  
- 	dmaengine_get();
- 	qp->dma_chan = dma_find_channel(DMA_MEMCPY);
- 	if (!qp->dma_chan) {
- 		dmaengine_put();
+ 	dma_cap_zero(dma_mask);
+ 	dma_cap_set(DMA_MEMCPY, dma_mask);
+ 
+ 	qp->dma_chan = dma_request_channel(dma_mask, ntb_dma_filter_fn,
+ 					   (void *)(unsigned long)node);
+ 	if (!qp->dma_chan)
  		dev_info(&pdev->dev, "Unable to allocate DMA channel, using CPU instead\n");
- 	}
  
  	for (i = 0; i < NTB_QP_DEF_NUM_ENTRIES; i++) {
++<<<<<<< HEAD
 +		entry = kzalloc(sizeof(struct ntb_queue_entry), GFP_ATOMIC);
++=======
+ 		entry = kzalloc_node(sizeof(*entry), GFP_ATOMIC, node);
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  		if (!entry)
  			goto err1;
  
@@@ -1477,7 -1596,7 +1558,11 @@@
  	}
  
  	for (i = 0; i < NTB_QP_DEF_NUM_ENTRIES; i++) {
++<<<<<<< HEAD
 +		entry = kzalloc(sizeof(struct ntb_queue_entry), GFP_ATOMIC);
++=======
+ 		entry = kzalloc_node(sizeof(*entry), GFP_ATOMIC, node);
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  		if (!entry)
  			goto err2;
  
@@@ -1502,8 -1619,8 +1587,13 @@@ err1
  	while ((entry = ntb_list_rm(&qp->ntb_rx_free_q_lock, &qp->rx_free_q)))
  		kfree(entry);
  	if (qp->dma_chan)
++<<<<<<< HEAD
 +		dmaengine_put();
 +	set_bit(free_queue, &nt->qp_bitmap);
++=======
+ 		dma_release_channel(qp->dma_chan);
+ 	nt->qp_bitmap_free |= qp_bit;
++>>>>>>> 1199aa61264a (NTB: Use NUMA memory and DMA chan in transport)
  err:
  	return NULL;
  }
@@@ -1537,10 -1656,13 +1627,10 @@@ void ntb_transport_free_queue(struct nt
  		 */
  		dma_sync_wait(chan, qp->last_cookie);
  		dmaengine_terminate_all(chan);
- 		dmaengine_put();
+ 		dma_release_channel(chan);
  	}
  
 -	qp_bit = BIT_ULL(qp->qp_num);
 -
 -	ntb_db_set_mask(qp->ndev, qp_bit);
 -	tasklet_disable(&qp->rxc_db_work);
 +	ntb_unregister_db_callback(qp->ndev, qp->qp_num);
  
  	cancel_delayed_work_sync(&qp->link_work);
  
* Unmerged path drivers/ntb/ntb_transport.c
