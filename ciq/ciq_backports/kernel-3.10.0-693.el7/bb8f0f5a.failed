drm/i915: Split intel_engine allocation and initialisation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [drm] i915: Split intel_engine allocation and initialisation (Rob Clark) [1380115]
Rebuild_FUZZ: 96.43%
commit-author Chris Wilson <chris@chris-wilson.co.uk>
commit bb8f0f5abdd7845175962a3fb99a5681290f9566
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/bb8f0f5a.failed

In order to reset the GPU early on in the module load sequence, we need
to allocate the basic engine structs (to populate the mmio offsets etc).
Currently, the engine initialisation allocates both the base struct and
also allocate auxiliary objects, which depend upon state setup quite
late in the load sequence. We split off the allocation callback for
later and allow ourselves to allocate the engine structs themselves
early.

v2: Different paint for the unwind following error.

	Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
	Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
	Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Link: http://patchwork.freedesktop.org/patch/msgid/20170124110135.6418-1-chris@chris-wilson.co.uk
(cherry picked from commit bb8f0f5abdd7845175962a3fb99a5681290f9566)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/i915_drv.c
#	drivers/gpu/drm/i915/intel_engine_cs.c
#	drivers/gpu/drm/i915/intel_lrc.h
diff --cc drivers/gpu/drm/i915/i915_drv.c
index e47a02f4ec43,f8d1ffeccc69..000000000000
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@@ -523,29 -235,1165 +523,1028 @@@ void intel_detect_pch(struct drm_devic
  	pci_dev_put(pch);
  }
  
 -static int i915_getparam(struct drm_device *dev, void *data,
 -			 struct drm_file *file_priv)
 +bool i915_semaphore_is_enabled(struct drm_device *dev)
  {
 -	struct drm_i915_private *dev_priv = to_i915(dev);
 -	struct pci_dev *pdev = dev_priv->drm.pdev;
 -	drm_i915_getparam_t *param = data;
 -	int value;
 -
 -	switch (param->param) {
 -	case I915_PARAM_IRQ_ACTIVE:
 -	case I915_PARAM_ALLOW_BATCHBUFFER:
 -	case I915_PARAM_LAST_DISPATCH:
 -		/* Reject all old ums/dri params. */
 -		return -ENODEV;
 -	case I915_PARAM_CHIPSET_ID:
 -		value = pdev->device;
 -		break;
 -	case I915_PARAM_REVISION:
 -		value = pdev->revision;
 -		break;
 -	case I915_PARAM_NUM_FENCES_AVAIL:
 -		value = dev_priv->num_fence_regs;
 -		break;
 -	case I915_PARAM_HAS_OVERLAY:
 -		value = dev_priv->overlay ? 1 : 0;
 -		break;
 -	case I915_PARAM_HAS_BSD:
 -		value = !!dev_priv->engine[VCS];
 -		break;
 -	case I915_PARAM_HAS_BLT:
 -		value = !!dev_priv->engine[BCS];
 -		break;
 -	case I915_PARAM_HAS_VEBOX:
 -		value = !!dev_priv->engine[VECS];
 -		break;
 -	case I915_PARAM_HAS_BSD2:
 -		value = !!dev_priv->engine[VCS2];
 -		break;
 -	case I915_PARAM_HAS_EXEC_CONSTANTS:
 -		value = INTEL_GEN(dev_priv) >= 4;
 -		break;
 -	case I915_PARAM_HAS_LLC:
 -		value = HAS_LLC(dev_priv);
 -		break;
 -	case I915_PARAM_HAS_WT:
 -		value = HAS_WT(dev_priv);
 -		break;
 -	case I915_PARAM_HAS_ALIASING_PPGTT:
 -		value = USES_PPGTT(dev_priv);
 -		break;
 -	case I915_PARAM_HAS_SEMAPHORES:
 -		value = i915.semaphores;
 -		break;
 -	case I915_PARAM_HAS_SECURE_BATCHES:
 -		value = capable(CAP_SYS_ADMIN);
 -		break;
 -	case I915_PARAM_CMD_PARSER_VERSION:
 -		value = i915_cmd_parser_get_version(dev_priv);
 -		break;
 -	case I915_PARAM_SUBSLICE_TOTAL:
 -		value = sseu_subslice_total(&INTEL_INFO(dev_priv)->sseu);
 -		if (!value)
 -			return -ENODEV;
 -		break;
 -	case I915_PARAM_EU_TOTAL:
 -		value = INTEL_INFO(dev_priv)->sseu.eu_total;
 -		if (!value)
 -			return -ENODEV;
 -		break;
 -	case I915_PARAM_HAS_GPU_RESET:
 -		value = i915.enable_hangcheck && intel_has_gpu_reset(dev_priv);
 -		break;
 -	case I915_PARAM_HAS_RESOURCE_STREAMER:
 -		value = HAS_RESOURCE_STREAMER(dev_priv);
 -		break;
 -	case I915_PARAM_HAS_POOLED_EU:
 -		value = HAS_POOLED_EU(dev_priv);
 -		break;
 -	case I915_PARAM_MIN_EU_IN_POOL:
 -		value = INTEL_INFO(dev_priv)->sseu.min_eu_in_pool;
 -		break;
 -	case I915_PARAM_HUC_STATUS:
 -		/* The register is already force-woken. We dont need
 -		 * any rpm here
 -		 */
 -		value = I915_READ(HUC_STATUS2) & HUC_FW_VERIFIED;
 -		break;
 -	case I915_PARAM_MMAP_GTT_VERSION:
 -		/* Though we've started our numbering from 1, and so class all
 -		 * earlier versions as 0, in effect their value is undefined as
 -		 * the ioctl will report EINVAL for the unknown param!
 -		 */
 -		value = i915_gem_mmap_gtt_version();
 -		break;
 -	case I915_PARAM_HAS_SCHEDULER:
 -		value = dev_priv->engine[RCS] &&
 -			dev_priv->engine[RCS]->schedule;
 -		break;
 -	case I915_PARAM_MMAP_VERSION:
 -		/* Remember to bump this if the version changes! */
 -	case I915_PARAM_HAS_GEM:
 -	case I915_PARAM_HAS_PAGEFLIPPING:
 -	case I915_PARAM_HAS_EXECBUF2: /* depends on GEM */
 -	case I915_PARAM_HAS_RELAXED_FENCING:
 -	case I915_PARAM_HAS_COHERENT_RINGS:
 -	case I915_PARAM_HAS_RELAXED_DELTA:
 -	case I915_PARAM_HAS_GEN7_SOL_RESET:
 -	case I915_PARAM_HAS_WAIT_TIMEOUT:
 -	case I915_PARAM_HAS_PRIME_VMAP_FLUSH:
 -	case I915_PARAM_HAS_PINNED_BATCHES:
 -	case I915_PARAM_HAS_EXEC_NO_RELOC:
 -	case I915_PARAM_HAS_EXEC_HANDLE_LUT:
 -	case I915_PARAM_HAS_COHERENT_PHYS_GTT:
 -	case I915_PARAM_HAS_EXEC_SOFTPIN:
 -		/* For the time being all of these are always true;
 -		 * if some supported hardware does not have one of these
 -		 * features this value needs to be provided from
 -		 * INTEL_INFO(), a feature macro, or similar.
 -		 */
 -		value = 1;
 -		break;
 -	default:
 -		DRM_DEBUG("Unknown parameter %d\n", param->param);
 -		return -EINVAL;
 -	}
 +	if (INTEL_INFO(dev)->gen < 6)
 +		return false;
  
 -	if (put_user(value, param->value))
 -		return -EFAULT;
 +	if (i915.semaphores >= 0)
 +		return i915.semaphores;
  
 -	return 0;
 -}
 -
 -static int i915_get_bridge_dev(struct drm_i915_private *dev_priv)
 -{
 -	dev_priv->bridge_dev = pci_get_bus_and_slot(0, PCI_DEVFN(0, 0));
 -	if (!dev_priv->bridge_dev) {
 -		DRM_ERROR("bridge device not found\n");
 -		return -1;
 -	}
 -	return 0;
 -}
 +	/* TODO: make semaphores and Execlists play nicely together */
 +	if (i915.enable_execlists)
 +		return false;
  
 -/* Allocate space for the MCH regs if needed, return nonzero on error */
 -static int
 -intel_alloc_mchbar_resource(struct drm_i915_private *dev_priv)
 -{
 -	int reg = INTEL_GEN(dev_priv) >= 4 ? MCHBAR_I965 : MCHBAR_I915;
 -	u32 temp_lo, temp_hi = 0;
 -	u64 mchbar_addr;
 -	int ret;
 +	/* Until we get further testing... */
 +	if (IS_GEN8(dev))
 +		return false;
  
 -	if (INTEL_GEN(dev_priv) >= 4)
 -		pci_read_config_dword(dev_priv->bridge_dev, reg + 4, &temp_hi);
 -	pci_read_config_dword(dev_priv->bridge_dev, reg, &temp_lo);
 -	mchbar_addr = ((u64)temp_hi << 32) | temp_lo;
 -
 -	/* If ACPI doesn't have it, assume we need to allocate it ourselves */
 -#ifdef CONFIG_PNP
 -	if (mchbar_addr &&
 -	    pnp_range_reserved(mchbar_addr, mchbar_addr + MCHBAR_SIZE))
 -		return 0;
 +#ifdef CONFIG_INTEL_IOMMU
 +	/* Enable semaphores on SNB when IO remapping is off */
 +	if (INTEL_INFO(dev)->gen == 6 && intel_iommu_gfx_mapped)
 +		return false;
  #endif
  
++<<<<<<< HEAD
 +	return true;
++=======
+ 	/* Get some space for it */
+ 	dev_priv->mch_res.name = "i915 MCHBAR";
+ 	dev_priv->mch_res.flags = IORESOURCE_MEM;
+ 	ret = pci_bus_alloc_resource(dev_priv->bridge_dev->bus,
+ 				     &dev_priv->mch_res,
+ 				     MCHBAR_SIZE, MCHBAR_SIZE,
+ 				     PCIBIOS_MIN_MEM,
+ 				     0, pcibios_align_resource,
+ 				     dev_priv->bridge_dev);
+ 	if (ret) {
+ 		DRM_DEBUG_DRIVER("failed bus alloc: %d\n", ret);
+ 		dev_priv->mch_res.start = 0;
+ 		return ret;
+ 	}
+ 
+ 	if (INTEL_GEN(dev_priv) >= 4)
+ 		pci_write_config_dword(dev_priv->bridge_dev, reg + 4,
+ 				       upper_32_bits(dev_priv->mch_res.start));
+ 
+ 	pci_write_config_dword(dev_priv->bridge_dev, reg,
+ 			       lower_32_bits(dev_priv->mch_res.start));
+ 	return 0;
+ }
+ 
+ /* Setup MCHBAR if possible, return true if we should disable it again */
+ static void
+ intel_setup_mchbar(struct drm_i915_private *dev_priv)
+ {
+ 	int mchbar_reg = INTEL_GEN(dev_priv) >= 4 ? MCHBAR_I965 : MCHBAR_I915;
+ 	u32 temp;
+ 	bool enabled;
+ 
+ 	if (IS_VALLEYVIEW(dev_priv) || IS_CHERRYVIEW(dev_priv))
+ 		return;
+ 
+ 	dev_priv->mchbar_need_disable = false;
+ 
+ 	if (IS_I915G(dev_priv) || IS_I915GM(dev_priv)) {
+ 		pci_read_config_dword(dev_priv->bridge_dev, DEVEN, &temp);
+ 		enabled = !!(temp & DEVEN_MCHBAR_EN);
+ 	} else {
+ 		pci_read_config_dword(dev_priv->bridge_dev, mchbar_reg, &temp);
+ 		enabled = temp & 1;
+ 	}
+ 
+ 	/* If it's already enabled, don't have to do anything */
+ 	if (enabled)
+ 		return;
+ 
+ 	if (intel_alloc_mchbar_resource(dev_priv))
+ 		return;
+ 
+ 	dev_priv->mchbar_need_disable = true;
+ 
+ 	/* Space is allocated or reserved, so enable it. */
+ 	if (IS_I915G(dev_priv) || IS_I915GM(dev_priv)) {
+ 		pci_write_config_dword(dev_priv->bridge_dev, DEVEN,
+ 				       temp | DEVEN_MCHBAR_EN);
+ 	} else {
+ 		pci_read_config_dword(dev_priv->bridge_dev, mchbar_reg, &temp);
+ 		pci_write_config_dword(dev_priv->bridge_dev, mchbar_reg, temp | 1);
+ 	}
+ }
+ 
+ static void
+ intel_teardown_mchbar(struct drm_i915_private *dev_priv)
+ {
+ 	int mchbar_reg = INTEL_GEN(dev_priv) >= 4 ? MCHBAR_I965 : MCHBAR_I915;
+ 
+ 	if (dev_priv->mchbar_need_disable) {
+ 		if (IS_I915G(dev_priv) || IS_I915GM(dev_priv)) {
+ 			u32 deven_val;
+ 
+ 			pci_read_config_dword(dev_priv->bridge_dev, DEVEN,
+ 					      &deven_val);
+ 			deven_val &= ~DEVEN_MCHBAR_EN;
+ 			pci_write_config_dword(dev_priv->bridge_dev, DEVEN,
+ 					       deven_val);
+ 		} else {
+ 			u32 mchbar_val;
+ 
+ 			pci_read_config_dword(dev_priv->bridge_dev, mchbar_reg,
+ 					      &mchbar_val);
+ 			mchbar_val &= ~1;
+ 			pci_write_config_dword(dev_priv->bridge_dev, mchbar_reg,
+ 					       mchbar_val);
+ 		}
+ 	}
+ 
+ 	if (dev_priv->mch_res.start)
+ 		release_resource(&dev_priv->mch_res);
+ }
+ 
+ /* true = enable decode, false = disable decoder */
+ static unsigned int i915_vga_set_decode(void *cookie, bool state)
+ {
+ 	struct drm_i915_private *dev_priv = cookie;
+ 
+ 	intel_modeset_vga_set_state(dev_priv, state);
+ 	if (state)
+ 		return VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM |
+ 		       VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;
+ 	else
+ 		return VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;
+ }
+ 
+ static int i915_resume_switcheroo(struct drm_device *dev);
+ static int i915_suspend_switcheroo(struct drm_device *dev, pm_message_t state);
+ 
+ static void i915_switcheroo_set_state(struct pci_dev *pdev, enum vga_switcheroo_state state)
+ {
+ 	struct drm_device *dev = pci_get_drvdata(pdev);
+ 	pm_message_t pmm = { .event = PM_EVENT_SUSPEND };
+ 
+ 	if (state == VGA_SWITCHEROO_ON) {
+ 		pr_info("switched on\n");
+ 		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
+ 		/* i915 resume handler doesn't set to D0 */
+ 		pci_set_power_state(pdev, PCI_D0);
+ 		i915_resume_switcheroo(dev);
+ 		dev->switch_power_state = DRM_SWITCH_POWER_ON;
+ 	} else {
+ 		pr_info("switched off\n");
+ 		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
+ 		i915_suspend_switcheroo(dev, pmm);
+ 		dev->switch_power_state = DRM_SWITCH_POWER_OFF;
+ 	}
+ }
+ 
+ static bool i915_switcheroo_can_switch(struct pci_dev *pdev)
+ {
+ 	struct drm_device *dev = pci_get_drvdata(pdev);
+ 
+ 	/*
+ 	 * FIXME: open_count is protected by drm_global_mutex but that would lead to
+ 	 * locking inversion with the driver load path. And the access here is
+ 	 * completely racy anyway. So don't bother with locking for now.
+ 	 */
+ 	return dev->open_count == 0;
+ }
+ 
+ static const struct vga_switcheroo_client_ops i915_switcheroo_ops = {
+ 	.set_gpu_state = i915_switcheroo_set_state,
+ 	.reprobe = NULL,
+ 	.can_switch = i915_switcheroo_can_switch,
+ };
+ 
+ static void i915_gem_fini(struct drm_i915_private *dev_priv)
+ {
+ 	mutex_lock(&dev_priv->drm.struct_mutex);
+ 	i915_gem_cleanup_engines(dev_priv);
+ 	i915_gem_context_fini(dev_priv);
+ 	mutex_unlock(&dev_priv->drm.struct_mutex);
+ 
+ 	i915_gem_drain_freed_objects(dev_priv);
+ 
+ 	WARN_ON(!list_empty(&dev_priv->context_list));
+ }
+ 
+ static int i915_load_modeset_init(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = to_i915(dev);
+ 	struct pci_dev *pdev = dev_priv->drm.pdev;
+ 	int ret;
+ 
+ 	if (i915_inject_load_failure())
+ 		return -ENODEV;
+ 
+ 	ret = intel_bios_init(dev_priv);
+ 	if (ret)
+ 		DRM_INFO("failed to find VBIOS tables\n");
+ 
+ 	/* If we have > 1 VGA cards, then we need to arbitrate access
+ 	 * to the common VGA resources.
+ 	 *
+ 	 * If we are a secondary display controller (!PCI_DISPLAY_CLASS_VGA),
+ 	 * then we do not take part in VGA arbitration and the
+ 	 * vga_client_register() fails with -ENODEV.
+ 	 */
+ 	ret = vga_client_register(pdev, dev_priv, NULL, i915_vga_set_decode);
+ 	if (ret && ret != -ENODEV)
+ 		goto out;
+ 
+ 	intel_register_dsm_handler();
+ 
+ 	ret = vga_switcheroo_register_client(pdev, &i915_switcheroo_ops, false);
+ 	if (ret)
+ 		goto cleanup_vga_client;
+ 
+ 	/* must happen before intel_power_domains_init_hw() on VLV/CHV */
+ 	intel_update_rawclk(dev_priv);
+ 
+ 	intel_power_domains_init_hw(dev_priv, false);
+ 
+ 	intel_csr_ucode_init(dev_priv);
+ 
+ 	ret = intel_irq_install(dev_priv);
+ 	if (ret)
+ 		goto cleanup_csr;
+ 
+ 	intel_setup_gmbus(dev_priv);
+ 
+ 	/* Important: The output setup functions called by modeset_init need
+ 	 * working irqs for e.g. gmbus and dp aux transfers. */
+ 	ret = intel_modeset_init(dev);
+ 	if (ret)
+ 		goto cleanup_irq;
+ 
+ 	intel_huc_init(dev_priv);
+ 	intel_guc_init(dev_priv);
+ 
+ 	ret = i915_gem_init(dev_priv);
+ 	if (ret)
+ 		goto cleanup_irq;
+ 
+ 	intel_modeset_gem_init(dev);
+ 
+ 	if (INTEL_INFO(dev_priv)->num_pipes == 0)
+ 		return 0;
+ 
+ 	ret = intel_fbdev_init(dev);
+ 	if (ret)
+ 		goto cleanup_gem;
+ 
+ 	/* Only enable hotplug handling once the fbdev is fully set up. */
+ 	intel_hpd_init(dev_priv);
+ 
+ 	drm_kms_helper_poll_init(dev);
+ 
+ 	return 0;
+ 
+ cleanup_gem:
+ 	if (i915_gem_suspend(dev_priv))
+ 		DRM_ERROR("failed to idle hardware; continuing to unload!\n");
+ 	i915_gem_fini(dev_priv);
+ cleanup_irq:
+ 	intel_guc_fini(dev_priv);
+ 	intel_huc_fini(dev_priv);
+ 	drm_irq_uninstall(dev);
+ 	intel_teardown_gmbus(dev_priv);
+ cleanup_csr:
+ 	intel_csr_ucode_fini(dev_priv);
+ 	intel_power_domains_fini(dev_priv);
+ 	vga_switcheroo_unregister_client(pdev);
+ cleanup_vga_client:
+ 	vga_client_register(pdev, NULL, NULL, NULL);
+ out:
+ 	return ret;
+ }
+ 
+ static int i915_kick_out_firmware_fb(struct drm_i915_private *dev_priv)
+ {
+ 	struct apertures_struct *ap;
+ 	struct pci_dev *pdev = dev_priv->drm.pdev;
+ 	struct i915_ggtt *ggtt = &dev_priv->ggtt;
+ 	bool primary;
+ 	int ret;
+ 
+ 	ap = alloc_apertures(1);
+ 	if (!ap)
+ 		return -ENOMEM;
+ 
+ 	ap->ranges[0].base = ggtt->mappable_base;
+ 	ap->ranges[0].size = ggtt->mappable_end;
+ 
+ 	primary =
+ 		pdev->resource[PCI_ROM_RESOURCE].flags & IORESOURCE_ROM_SHADOW;
+ 
+ 	ret = drm_fb_helper_remove_conflicting_framebuffers(ap, "inteldrmfb", primary);
+ 
+ 	kfree(ap);
+ 
+ 	return ret;
+ }
+ 
+ #if !defined(CONFIG_VGA_CONSOLE)
+ static int i915_kick_out_vgacon(struct drm_i915_private *dev_priv)
+ {
+ 	return 0;
+ }
+ #elif !defined(CONFIG_DUMMY_CONSOLE)
+ static int i915_kick_out_vgacon(struct drm_i915_private *dev_priv)
+ {
+ 	return -ENODEV;
+ }
+ #else
+ static int i915_kick_out_vgacon(struct drm_i915_private *dev_priv)
+ {
+ 	int ret = 0;
+ 
+ 	DRM_INFO("Replacing VGA console driver\n");
+ 
+ 	console_lock();
+ 	if (con_is_bound(&vga_con))
+ 		ret = do_take_over_console(&dummy_con, 0, MAX_NR_CONSOLES - 1, 1);
+ 	if (ret == 0) {
+ 		ret = do_unregister_con_driver(&vga_con);
+ 
+ 		/* Ignore "already unregistered". */
+ 		if (ret == -ENODEV)
+ 			ret = 0;
+ 	}
+ 	console_unlock();
+ 
+ 	return ret;
+ }
+ #endif
+ 
+ static void intel_init_dpio(struct drm_i915_private *dev_priv)
+ {
+ 	/*
+ 	 * IOSF_PORT_DPIO is used for VLV x2 PHY (DP/HDMI B and C),
+ 	 * CHV x1 PHY (DP/HDMI D)
+ 	 * IOSF_PORT_DPIO_2 is used for CHV x2 PHY (DP/HDMI B and C)
+ 	 */
+ 	if (IS_CHERRYVIEW(dev_priv)) {
+ 		DPIO_PHY_IOSF_PORT(DPIO_PHY0) = IOSF_PORT_DPIO_2;
+ 		DPIO_PHY_IOSF_PORT(DPIO_PHY1) = IOSF_PORT_DPIO;
+ 	} else if (IS_VALLEYVIEW(dev_priv)) {
+ 		DPIO_PHY_IOSF_PORT(DPIO_PHY0) = IOSF_PORT_DPIO;
+ 	}
+ }
+ 
+ static int i915_workqueues_init(struct drm_i915_private *dev_priv)
+ {
+ 	/*
+ 	 * The i915 workqueue is primarily used for batched retirement of
+ 	 * requests (and thus managing bo) once the task has been completed
+ 	 * by the GPU. i915_gem_retire_requests() is called directly when we
+ 	 * need high-priority retirement, such as waiting for an explicit
+ 	 * bo.
+ 	 *
+ 	 * It is also used for periodic low-priority events, such as
+ 	 * idle-timers and recording error state.
+ 	 *
+ 	 * All tasks on the workqueue are expected to acquire the dev mutex
+ 	 * so there is no point in running more than one instance of the
+ 	 * workqueue at any time.  Use an ordered one.
+ 	 */
+ 	dev_priv->wq = alloc_ordered_workqueue("i915", 0);
+ 	if (dev_priv->wq == NULL)
+ 		goto out_err;
+ 
+ 	dev_priv->hotplug.dp_wq = alloc_ordered_workqueue("i915-dp", 0);
+ 	if (dev_priv->hotplug.dp_wq == NULL)
+ 		goto out_free_wq;
+ 
+ 	return 0;
+ 
+ out_free_wq:
+ 	destroy_workqueue(dev_priv->wq);
+ out_err:
+ 	DRM_ERROR("Failed to allocate workqueues.\n");
+ 
+ 	return -ENOMEM;
+ }
+ 
+ static void i915_engines_cleanup(struct drm_i915_private *i915)
+ {
+ 	struct intel_engine_cs *engine;
+ 	enum intel_engine_id id;
+ 
+ 	for_each_engine(engine, i915, id)
+ 		kfree(engine);
+ }
+ 
+ static void i915_workqueues_cleanup(struct drm_i915_private *dev_priv)
+ {
+ 	destroy_workqueue(dev_priv->hotplug.dp_wq);
+ 	destroy_workqueue(dev_priv->wq);
+ }
+ 
+ /*
+  * We don't keep the workarounds for pre-production hardware, so we expect our
+  * driver to fail on these machines in one way or another. A little warning on
+  * dmesg may help both the user and the bug triagers.
+  */
+ static void intel_detect_preproduction_hw(struct drm_i915_private *dev_priv)
+ {
+ 	if (IS_HSW_EARLY_SDV(dev_priv) ||
+ 	    IS_SKL_REVID(dev_priv, 0, SKL_REVID_F0))
+ 		DRM_ERROR("This is a pre-production stepping. "
+ 			  "It may not be fully functional.\n");
+ }
+ 
+ /**
+  * i915_driver_init_early - setup state not requiring device access
+  * @dev_priv: device private
+  *
+  * Initialize everything that is a "SW-only" state, that is state not
+  * requiring accessing the device or exposing the driver via kernel internal
+  * or userspace interfaces. Example steps belonging here: lock initialization,
+  * system memory allocation, setting up device specific attributes and
+  * function hooks not requiring accessing the device.
+  */
+ static int i915_driver_init_early(struct drm_i915_private *dev_priv,
+ 				  const struct pci_device_id *ent)
+ {
+ 	const struct intel_device_info *match_info =
+ 		(struct intel_device_info *)ent->driver_data;
+ 	struct intel_device_info *device_info;
+ 	int ret = 0;
+ 
+ 	if (i915_inject_load_failure())
+ 		return -ENODEV;
+ 
+ 	/* Setup the write-once "constant" device info */
+ 	device_info = mkwrite_device_info(dev_priv);
+ 	memcpy(device_info, match_info, sizeof(*device_info));
+ 	device_info->device_id = dev_priv->drm.pdev->device;
+ 
+ 	BUG_ON(device_info->gen > sizeof(device_info->gen_mask) * BITS_PER_BYTE);
+ 	device_info->gen_mask = BIT(device_info->gen - 1);
+ 
+ 	spin_lock_init(&dev_priv->irq_lock);
+ 	spin_lock_init(&dev_priv->gpu_error.lock);
+ 	mutex_init(&dev_priv->backlight_lock);
+ 	spin_lock_init(&dev_priv->uncore.lock);
+ 	spin_lock_init(&dev_priv->mm.object_stat_lock);
+ 	spin_lock_init(&dev_priv->mmio_flip_lock);
+ 	spin_lock_init(&dev_priv->wm.dsparb_lock);
+ 	mutex_init(&dev_priv->sb_lock);
+ 	mutex_init(&dev_priv->modeset_restore_lock);
+ 	mutex_init(&dev_priv->av_mutex);
+ 	mutex_init(&dev_priv->wm.wm_mutex);
+ 	mutex_init(&dev_priv->pps_mutex);
+ 
+ 	intel_uc_init_early(dev_priv);
+ 	i915_memcpy_init_early(dev_priv);
+ 
+ 	ret = intel_engines_init_early(dev_priv);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = i915_workqueues_init(dev_priv);
+ 	if (ret < 0)
+ 		goto err_engines;
+ 
+ 	ret = intel_gvt_init(dev_priv);
+ 	if (ret < 0)
+ 		goto err_workqueues;
+ 
+ 	/* This must be called before any calls to HAS_PCH_* */
+ 	intel_detect_pch(dev_priv);
+ 
+ 	intel_pm_setup(dev_priv);
+ 	intel_init_dpio(dev_priv);
+ 	intel_power_domains_init(dev_priv);
+ 	intel_irq_init(dev_priv);
+ 	intel_hangcheck_init(dev_priv);
+ 	intel_init_display_hooks(dev_priv);
+ 	intel_init_clock_gating_hooks(dev_priv);
+ 	intel_init_audio_hooks(dev_priv);
+ 	ret = i915_gem_load_init(dev_priv);
+ 	if (ret < 0)
+ 		goto err_gvt;
+ 
+ 	intel_display_crc_init(dev_priv);
+ 
+ 	intel_device_info_dump(dev_priv);
+ 
+ 	intel_detect_preproduction_hw(dev_priv);
+ 
+ 	i915_perf_init(dev_priv);
+ 
+ 	return 0;
+ 
+ err_gvt:
+ 	intel_gvt_cleanup(dev_priv);
+ err_workqueues:
+ 	i915_workqueues_cleanup(dev_priv);
+ err_engines:
+ 	i915_engines_cleanup(dev_priv);
+ 	return ret;
+ }
+ 
+ /**
+  * i915_driver_cleanup_early - cleanup the setup done in i915_driver_init_early()
+  * @dev_priv: device private
+  */
+ static void i915_driver_cleanup_early(struct drm_i915_private *dev_priv)
+ {
+ 	i915_perf_fini(dev_priv);
+ 	i915_gem_load_cleanup(dev_priv);
+ 	i915_workqueues_cleanup(dev_priv);
+ 	i915_engines_cleanup(dev_priv);
+ }
+ 
+ static int i915_mmio_setup(struct drm_i915_private *dev_priv)
+ {
+ 	struct pci_dev *pdev = dev_priv->drm.pdev;
+ 	int mmio_bar;
+ 	int mmio_size;
+ 
+ 	mmio_bar = IS_GEN2(dev_priv) ? 1 : 0;
+ 	/*
+ 	 * Before gen4, the registers and the GTT are behind different BARs.
+ 	 * However, from gen4 onwards, the registers and the GTT are shared
+ 	 * in the same BAR, so we want to restrict this ioremap from
+ 	 * clobbering the GTT which we want ioremap_wc instead. Fortunately,
+ 	 * the register BAR remains the same size for all the earlier
+ 	 * generations up to Ironlake.
+ 	 */
+ 	if (INTEL_GEN(dev_priv) < 5)
+ 		mmio_size = 512 * 1024;
+ 	else
+ 		mmio_size = 2 * 1024 * 1024;
+ 	dev_priv->regs = pci_iomap(pdev, mmio_bar, mmio_size);
+ 	if (dev_priv->regs == NULL) {
+ 		DRM_ERROR("failed to map registers\n");
+ 
+ 		return -EIO;
+ 	}
+ 
+ 	/* Try to make sure MCHBAR is enabled before poking at it */
+ 	intel_setup_mchbar(dev_priv);
+ 
+ 	return 0;
+ }
+ 
+ static void i915_mmio_cleanup(struct drm_i915_private *dev_priv)
+ {
+ 	struct pci_dev *pdev = dev_priv->drm.pdev;
+ 
+ 	intel_teardown_mchbar(dev_priv);
+ 	pci_iounmap(pdev, dev_priv->regs);
+ }
+ 
+ /**
+  * i915_driver_init_mmio - setup device MMIO
+  * @dev_priv: device private
+  *
+  * Setup minimal device state necessary for MMIO accesses later in the
+  * initialization sequence. The setup here should avoid any other device-wide
+  * side effects or exposing the driver via kernel internal or user space
+  * interfaces.
+  */
+ static int i915_driver_init_mmio(struct drm_i915_private *dev_priv)
+ {
+ 	int ret;
+ 
+ 	if (i915_inject_load_failure())
+ 		return -ENODEV;
+ 
+ 	if (i915_get_bridge_dev(dev_priv))
+ 		return -EIO;
+ 
+ 	ret = i915_mmio_setup(dev_priv);
+ 	if (ret < 0)
+ 		goto put_bridge;
+ 
+ 	intel_uncore_init(dev_priv);
+ 
+ 	return 0;
+ 
+ put_bridge:
+ 	pci_dev_put(dev_priv->bridge_dev);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * i915_driver_cleanup_mmio - cleanup the setup done in i915_driver_init_mmio()
+  * @dev_priv: device private
+  */
+ static void i915_driver_cleanup_mmio(struct drm_i915_private *dev_priv)
+ {
+ 	intel_uncore_fini(dev_priv);
+ 	i915_mmio_cleanup(dev_priv);
+ 	pci_dev_put(dev_priv->bridge_dev);
+ }
+ 
+ static void intel_sanitize_options(struct drm_i915_private *dev_priv)
+ {
+ 	i915.enable_execlists =
+ 		intel_sanitize_enable_execlists(dev_priv,
+ 						i915.enable_execlists);
+ 
+ 	/*
+ 	 * i915.enable_ppgtt is read-only, so do an early pass to validate the
+ 	 * user's requested state against the hardware/driver capabilities.  We
+ 	 * do this now so that we can print out any log messages once rather
+ 	 * than every time we check intel_enable_ppgtt().
+ 	 */
+ 	i915.enable_ppgtt =
+ 		intel_sanitize_enable_ppgtt(dev_priv, i915.enable_ppgtt);
+ 	DRM_DEBUG_DRIVER("ppgtt mode: %i\n", i915.enable_ppgtt);
+ 
+ 	i915.semaphores = intel_sanitize_semaphores(dev_priv, i915.semaphores);
+ 	DRM_DEBUG_DRIVER("use GPU sempahores? %s\n", yesno(i915.semaphores));
+ }
+ 
+ /**
+  * i915_driver_init_hw - setup state requiring device access
+  * @dev_priv: device private
+  *
+  * Setup state that requires accessing the device, but doesn't require
+  * exposing the driver via kernel internal or userspace interfaces.
+  */
+ static int i915_driver_init_hw(struct drm_i915_private *dev_priv)
+ {
+ 	struct pci_dev *pdev = dev_priv->drm.pdev;
+ 	int ret;
+ 
+ 	if (i915_inject_load_failure())
+ 		return -ENODEV;
+ 
+ 	intel_device_info_runtime_init(dev_priv);
+ 
+ 	intel_sanitize_options(dev_priv);
+ 
+ 	ret = i915_ggtt_probe_hw(dev_priv);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* WARNING: Apparently we must kick fbdev drivers before vgacon,
+ 	 * otherwise the vga fbdev driver falls over. */
+ 	ret = i915_kick_out_firmware_fb(dev_priv);
+ 	if (ret) {
+ 		DRM_ERROR("failed to remove conflicting framebuffer drivers\n");
+ 		goto out_ggtt;
+ 	}
+ 
+ 	ret = i915_kick_out_vgacon(dev_priv);
+ 	if (ret) {
+ 		DRM_ERROR("failed to remove conflicting VGA console\n");
+ 		goto out_ggtt;
+ 	}
+ 
+ 	ret = i915_ggtt_init_hw(dev_priv);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = i915_ggtt_enable_hw(dev_priv);
+ 	if (ret) {
+ 		DRM_ERROR("failed to enable GGTT\n");
+ 		goto out_ggtt;
+ 	}
+ 
+ 	pci_set_master(pdev);
+ 
+ 	/* overlay on gen2 is broken and can't address above 1G */
+ 	if (IS_GEN2(dev_priv)) {
+ 		ret = dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(30));
+ 		if (ret) {
+ 			DRM_ERROR("failed to set DMA mask\n");
+ 
+ 			goto out_ggtt;
+ 		}
+ 	}
+ 
+ 	/* 965GM sometimes incorrectly writes to hardware status page (HWS)
+ 	 * using 32bit addressing, overwriting memory if HWS is located
+ 	 * above 4GB.
+ 	 *
+ 	 * The documentation also mentions an issue with undefined
+ 	 * behaviour if any general state is accessed within a page above 4GB,
+ 	 * which also needs to be handled carefully.
+ 	 */
+ 	if (IS_I965G(dev_priv) || IS_I965GM(dev_priv)) {
+ 		ret = dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(32));
+ 
+ 		if (ret) {
+ 			DRM_ERROR("failed to set DMA mask\n");
+ 
+ 			goto out_ggtt;
+ 		}
+ 	}
+ 
+ 	pm_qos_add_request(&dev_priv->pm_qos, PM_QOS_CPU_DMA_LATENCY,
+ 			   PM_QOS_DEFAULT_VALUE);
+ 
+ 	intel_uncore_sanitize(dev_priv);
+ 
+ 	intel_opregion_setup(dev_priv);
+ 
+ 	i915_gem_load_init_fences(dev_priv);
+ 
+ 	/* On the 945G/GM, the chipset reports the MSI capability on the
+ 	 * integrated graphics even though the support isn't actually there
+ 	 * according to the published specs.  It doesn't appear to function
+ 	 * correctly in testing on 945G.
+ 	 * This may be a side effect of MSI having been made available for PEG
+ 	 * and the registers being closely associated.
+ 	 *
+ 	 * According to chipset errata, on the 965GM, MSI interrupts may
+ 	 * be lost or delayed, but we use them anyways to avoid
+ 	 * stuck interrupts on some machines.
+ 	 */
+ 	if (!IS_I945G(dev_priv) && !IS_I945GM(dev_priv)) {
+ 		if (pci_enable_msi(pdev) < 0)
+ 			DRM_DEBUG_DRIVER("can't enable MSI");
+ 	}
+ 
+ 	return 0;
+ 
+ out_ggtt:
+ 	i915_ggtt_cleanup_hw(dev_priv);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * i915_driver_cleanup_hw - cleanup the setup done in i915_driver_init_hw()
+  * @dev_priv: device private
+  */
+ static void i915_driver_cleanup_hw(struct drm_i915_private *dev_priv)
+ {
+ 	struct pci_dev *pdev = dev_priv->drm.pdev;
+ 
+ 	if (pdev->msi_enabled)
+ 		pci_disable_msi(pdev);
+ 
+ 	pm_qos_remove_request(&dev_priv->pm_qos);
+ 	i915_ggtt_cleanup_hw(dev_priv);
+ }
+ 
+ /**
+  * i915_driver_register - register the driver with the rest of the system
+  * @dev_priv: device private
+  *
+  * Perform any steps necessary to make the driver available via kernel
+  * internal or userspace interfaces.
+  */
+ static void i915_driver_register(struct drm_i915_private *dev_priv)
+ {
+ 	struct drm_device *dev = &dev_priv->drm;
+ 
+ 	i915_gem_shrinker_init(dev_priv);
+ 
+ 	/*
+ 	 * Notify a valid surface after modesetting,
+ 	 * when running inside a VM.
+ 	 */
+ 	if (intel_vgpu_active(dev_priv))
+ 		I915_WRITE(vgtif_reg(display_ready), VGT_DRV_DISPLAY_READY);
+ 
+ 	/* Reveal our presence to userspace */
+ 	if (drm_dev_register(dev, 0) == 0) {
+ 		i915_debugfs_register(dev_priv);
+ 		i915_guc_log_register(dev_priv);
+ 		i915_setup_sysfs(dev_priv);
+ 
+ 		/* Depends on sysfs having been initialized */
+ 		i915_perf_register(dev_priv);
+ 	} else
+ 		DRM_ERROR("Failed to register driver for userspace access!\n");
+ 
+ 	if (INTEL_INFO(dev_priv)->num_pipes) {
+ 		/* Must be done after probing outputs */
+ 		intel_opregion_register(dev_priv);
+ 		acpi_video_register();
+ 	}
+ 
+ 	if (IS_GEN5(dev_priv))
+ 		intel_gpu_ips_init(dev_priv);
+ 
+ 	i915_audio_component_init(dev_priv);
+ 
+ 	/*
+ 	 * Some ports require correctly set-up hpd registers for detection to
+ 	 * work properly (leading to ghost connected connector status), e.g. VGA
+ 	 * on gm45.  Hence we can only set up the initial fbdev config after hpd
+ 	 * irqs are fully enabled. We do it last so that the async config
+ 	 * cannot run before the connectors are registered.
+ 	 */
+ 	intel_fbdev_initial_config_async(dev);
+ }
+ 
+ /**
+  * i915_driver_unregister - cleanup the registration done in i915_driver_regiser()
+  * @dev_priv: device private
+  */
+ static void i915_driver_unregister(struct drm_i915_private *dev_priv)
+ {
+ 	i915_audio_component_cleanup(dev_priv);
+ 
+ 	intel_gpu_ips_teardown();
+ 	acpi_video_unregister();
+ 	intel_opregion_unregister(dev_priv);
+ 
+ 	i915_perf_unregister(dev_priv);
+ 
+ 	i915_teardown_sysfs(dev_priv);
+ 	i915_guc_log_unregister(dev_priv);
+ 	i915_debugfs_unregister(dev_priv);
+ 	drm_dev_unregister(&dev_priv->drm);
+ 
+ 	i915_gem_shrinker_cleanup(dev_priv);
+ }
+ 
+ /**
+  * i915_driver_load - setup chip and create an initial config
+  * @pdev: PCI device
+  * @ent: matching PCI ID entry
+  *
+  * The driver load routine has to do several things:
+  *   - drive output discovery via intel_modeset_init()
+  *   - initialize the memory manager
+  *   - allocate initial config memory
+  *   - setup the DRM framebuffer with the allocated memory
+  */
+ int i915_driver_load(struct pci_dev *pdev, const struct pci_device_id *ent)
+ {
+ 	struct drm_i915_private *dev_priv;
+ 	int ret;
+ 
+ 	if (i915.nuclear_pageflip)
+ 		driver.driver_features |= DRIVER_ATOMIC;
+ 
+ 	ret = -ENOMEM;
+ 	dev_priv = kzalloc(sizeof(*dev_priv), GFP_KERNEL);
+ 	if (dev_priv)
+ 		ret = drm_dev_init(&dev_priv->drm, &driver, &pdev->dev);
+ 	if (ret) {
+ 		DRM_DEV_ERROR(&pdev->dev, "allocation failed\n");
+ 		kfree(dev_priv);
+ 		return ret;
+ 	}
+ 
+ 	dev_priv->drm.pdev = pdev;
+ 	dev_priv->drm.dev_private = dev_priv;
+ 
+ 	ret = pci_enable_device(pdev);
+ 	if (ret)
+ 		goto out_free_priv;
+ 
+ 	pci_set_drvdata(pdev, &dev_priv->drm);
+ 
+ 	ret = i915_driver_init_early(dev_priv, ent);
+ 	if (ret < 0)
+ 		goto out_pci_disable;
+ 
+ 	intel_runtime_pm_get(dev_priv);
+ 
+ 	ret = i915_driver_init_mmio(dev_priv);
+ 	if (ret < 0)
+ 		goto out_runtime_pm_put;
+ 
+ 	ret = i915_driver_init_hw(dev_priv);
+ 	if (ret < 0)
+ 		goto out_cleanup_mmio;
+ 
+ 	/*
+ 	 * TODO: move the vblank init and parts of modeset init steps into one
+ 	 * of the i915_driver_init_/i915_driver_register functions according
+ 	 * to the role/effect of the given init step.
+ 	 */
+ 	if (INTEL_INFO(dev_priv)->num_pipes) {
+ 		ret = drm_vblank_init(&dev_priv->drm,
+ 				      INTEL_INFO(dev_priv)->num_pipes);
+ 		if (ret)
+ 			goto out_cleanup_hw;
+ 	}
+ 
+ 	ret = i915_load_modeset_init(&dev_priv->drm);
+ 	if (ret < 0)
+ 		goto out_cleanup_vblank;
+ 
+ 	i915_driver_register(dev_priv);
+ 
+ 	intel_runtime_pm_enable(dev_priv);
+ 
+ 	dev_priv->ipc_enabled = false;
+ 
+ 	/* Everything is in place, we can now relax! */
+ 	DRM_INFO("Initialized %s %d.%d.%d %s for %s on minor %d\n",
+ 		 driver.name, driver.major, driver.minor, driver.patchlevel,
+ 		 driver.date, pci_name(pdev), dev_priv->drm.primary->index);
+ 	if (IS_ENABLED(CONFIG_DRM_I915_DEBUG))
+ 		DRM_INFO("DRM_I915_DEBUG enabled\n");
+ 	if (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))
+ 		DRM_INFO("DRM_I915_DEBUG_GEM enabled\n");
+ 
+ 	intel_runtime_pm_put(dev_priv);
+ 
+ 	return 0;
+ 
+ out_cleanup_vblank:
+ 	drm_vblank_cleanup(&dev_priv->drm);
+ out_cleanup_hw:
+ 	i915_driver_cleanup_hw(dev_priv);
+ out_cleanup_mmio:
+ 	i915_driver_cleanup_mmio(dev_priv);
+ out_runtime_pm_put:
+ 	intel_runtime_pm_put(dev_priv);
+ 	i915_driver_cleanup_early(dev_priv);
+ out_pci_disable:
+ 	pci_disable_device(pdev);
+ out_free_priv:
+ 	i915_load_error(dev_priv, "Device initialization failed (%d)\n", ret);
+ 	drm_dev_unref(&dev_priv->drm);
+ 	return ret;
+ }
+ 
+ void i915_driver_unload(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = to_i915(dev);
+ 	struct pci_dev *pdev = dev_priv->drm.pdev;
+ 
+ 	intel_fbdev_fini(dev);
+ 
+ 	if (i915_gem_suspend(dev_priv))
+ 		DRM_ERROR("failed to idle hardware; continuing to unload!\n");
+ 
+ 	intel_display_power_get(dev_priv, POWER_DOMAIN_INIT);
+ 
+ 	i915_driver_unregister(dev_priv);
+ 
+ 	drm_vblank_cleanup(dev);
+ 
+ 	intel_modeset_cleanup(dev);
+ 
+ 	/*
+ 	 * free the memory space allocated for the child device
+ 	 * config parsed from VBT
+ 	 */
+ 	if (dev_priv->vbt.child_dev && dev_priv->vbt.child_dev_num) {
+ 		kfree(dev_priv->vbt.child_dev);
+ 		dev_priv->vbt.child_dev = NULL;
+ 		dev_priv->vbt.child_dev_num = 0;
+ 	}
+ 	kfree(dev_priv->vbt.sdvo_lvds_vbt_mode);
+ 	dev_priv->vbt.sdvo_lvds_vbt_mode = NULL;
+ 	kfree(dev_priv->vbt.lfp_lvds_vbt_mode);
+ 	dev_priv->vbt.lfp_lvds_vbt_mode = NULL;
+ 
+ 	vga_switcheroo_unregister_client(pdev);
+ 	vga_client_register(pdev, NULL, NULL, NULL);
+ 
+ 	intel_csr_ucode_fini(dev_priv);
+ 
+ 	/* Free error state after interrupts are fully disabled. */
+ 	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
+ 	i915_destroy_error_state(dev_priv);
+ 
+ 	/* Flush any outstanding unpin_work. */
+ 	drain_workqueue(dev_priv->wq);
+ 
+ 	intel_guc_fini(dev_priv);
+ 	intel_huc_fini(dev_priv);
+ 	i915_gem_fini(dev_priv);
+ 	intel_fbc_cleanup_cfb(dev_priv);
+ 
+ 	intel_power_domains_fini(dev_priv);
+ 
+ 	i915_driver_cleanup_hw(dev_priv);
+ 	i915_driver_cleanup_mmio(dev_priv);
+ 
+ 	intel_display_power_put(dev_priv, POWER_DOMAIN_INIT);
+ 
+ 	i915_driver_cleanup_early(dev_priv);
+ }
+ 
+ static int i915_driver_open(struct drm_device *dev, struct drm_file *file)
+ {
+ 	int ret;
+ 
+ 	ret = i915_gem_open(dev, file);
+ 	if (ret)
+ 		return ret;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * i915_driver_lastclose - clean up after all DRM clients have exited
+  * @dev: DRM device
+  *
+  * Take care of cleaning up after all DRM clients have exited.  In the
+  * mode setting case, we want to restore the kernel's initial mode (just
+  * in case the last client left us in a bad state).
+  *
+  * Additionally, in the non-mode setting case, we'll tear down the GTT
+  * and DMA structures, since the kernel won't be using them, and clea
+  * up any GEM state.
+  */
+ static void i915_driver_lastclose(struct drm_device *dev)
+ {
+ 	intel_fbdev_restore_mode(dev);
+ 	vga_switcheroo_process_delayed_switch();
+ }
+ 
+ static void i915_driver_preclose(struct drm_device *dev, struct drm_file *file)
+ {
+ 	mutex_lock(&dev->struct_mutex);
+ 	i915_gem_context_close(dev, file);
+ 	i915_gem_release(dev, file);
+ 	mutex_unlock(&dev->struct_mutex);
+ }
+ 
+ static void i915_driver_postclose(struct drm_device *dev, struct drm_file *file)
+ {
+ 	struct drm_i915_file_private *file_priv = file->driver_priv;
+ 
+ 	kfree(file_priv);
++>>>>>>> bb8f0f5abdd7 (drm/i915: Split intel_engine allocation and initialisation)
  }
  
  static void intel_suspend_encoders(struct drm_i915_private *dev_priv)
diff --cc drivers/gpu/drm/i915/intel_lrc.h
index e6cda3e225d0,c8009c7bfbdd..000000000000
--- a/drivers/gpu/drm/i915/intel_lrc.h
+++ b/drivers/gpu/drm/i915/intel_lrc.h
@@@ -54,42 -57,17 +54,45 @@@
  #define GEN8_CSB_READ_PTR(csb_status) \
  	(((csb_status) & GEN8_CSB_READ_PTR_MASK) >> 8)
  
 -enum {
 -	INTEL_CONTEXT_SCHEDULE_IN = 0,
 -	INTEL_CONTEXT_SCHEDULE_OUT,
 -};
 -
  /* Logical Rings */
 -void intel_logical_ring_stop(struct intel_engine_cs *engine);
 -void intel_logical_ring_cleanup(struct intel_engine_cs *engine);
 -int logical_render_ring_init(struct intel_engine_cs *engine);
 -int logical_xcs_ring_init(struct intel_engine_cs *engine);
 +int intel_logical_ring_alloc_request_extras(struct drm_i915_gem_request *request);
 +int intel_logical_ring_reserve_space(struct drm_i915_gem_request *request);
 +void intel_logical_ring_stop(struct intel_engine_cs *ring);
 +void intel_logical_ring_cleanup(struct intel_engine_cs *ring);
 +int intel_logical_rings_init(struct drm_device *dev);
 +int intel_logical_ring_begin(struct drm_i915_gem_request *req, int num_dwords);
 +
++<<<<<<< HEAD
 +int logical_ring_flush_all_caches(struct drm_i915_gem_request *req);
 +/**
 + * intel_logical_ring_advance() - advance the ringbuffer tail
 + * @ringbuf: Ringbuffer to advance.
 + *
 + * The tail is only updated in our logical ringbuffer struct.
 + */
 +static inline void intel_logical_ring_advance(struct intel_ringbuffer *ringbuf)
 +{
 +	ringbuf->tail &= ringbuf->size - 1;
 +}
 +/**
 + * intel_logical_ring_emit() - write a DWORD to the ringbuffer.
 + * @ringbuf: Ringbuffer to write to.
 + * @data: DWORD to write.
 + */
 +static inline void intel_logical_ring_emit(struct intel_ringbuffer *ringbuf,
 +					   u32 data)
 +{
 +	iowrite32(data, ringbuf->virtual_start + ringbuf->tail);
 +	ringbuf->tail += 4;
 +}
 +static inline void intel_logical_ring_emit_reg(struct intel_ringbuffer *ringbuf,
 +					       i915_reg_t reg)
 +{
 +	intel_logical_ring_emit(ringbuf, i915_mmio_reg_offset(reg));
 +}
  
++=======
++>>>>>>> bb8f0f5abdd7 (drm/i915: Split intel_engine allocation and initialisation)
  /* Logical Ring Contexts */
  
  /* One extra page is added before LRC for GuC as shared data */
* Unmerged path drivers/gpu/drm/i915/intel_engine_cs.c
* Unmerged path drivers/gpu/drm/i915/i915_drv.c
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 98315945fb50..2655dc0add38 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -2787,6 +2787,9 @@ extern unsigned long i915_gfx_val(struct drm_i915_private *dev_priv);
 extern void i915_update_gfx_val(struct drm_i915_private *dev_priv);
 int vlv_force_gfx_clock(struct drm_i915_private *dev_priv, bool on);
 
+int intel_engines_init_early(struct drm_i915_private *dev_priv);
+int intel_engines_init(struct drm_i915_private *dev_priv);
+
 /* intel_hotplug.c */
 void intel_hpd_irq_handler(struct drm_device *dev, u32 pin_mask, u32 long_mask);
 void intel_hpd_init(struct drm_i915_private *dev_priv);
* Unmerged path drivers/gpu/drm/i915/intel_engine_cs.c
* Unmerged path drivers/gpu/drm/i915/intel_lrc.h
