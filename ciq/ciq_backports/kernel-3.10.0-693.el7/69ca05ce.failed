sched: cls_flower: expose priority to offloading netdevice

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jiri Pirko <jiri@mellanox.com>
commit 69ca05ce9dec2cc95070df7f1f10ea6c9c12d237
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/69ca05ce.failed

The driver that offloads flower rules needs to know with which priority
user inserted the rules. So add this information into offload struct.

	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Acked-by: Ido Schimmel <idosch@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 69ca05ce9dec2cc95070df7f1f10ea6c9c12d237)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/pkt_cls.h
#	net/sched/cls_flower.c
diff --cc include/net/pkt_cls.h
index b606c03c3836,dabb00af46a0..000000000000
--- a/include/net/pkt_cls.h
+++ b/include/net/pkt_cls.h
@@@ -392,9 -427,101 +392,93 @@@ struct tc_cls_u32_offload 
  	};
  };
  
 -static inline bool tc_can_offload(const struct net_device *dev,
 -				  const struct tcf_proto *tp)
 +static inline bool tc_should_offload(struct net_device *dev)
  {
 -	const struct Qdisc *sch = tp->q;
 -	const struct Qdisc_class_ops *cops = sch->ops->cl_ops;
 -
 -	if (!(dev->features & NETIF_F_HW_TC))
 -		return false;
 -	if (!dev->netdev_ops->ndo_setup_tc)
 -		return false;
 -	if (cops && cops->tcf_cl_offload)
 -		return cops->tcf_cl_offload(tp->classid);
 -
 -	return true;
 +	return dev->netdev_ops->ndo_setup_tc;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool tc_skip_hw(u32 flags)
+ {
+ 	return (flags & TCA_CLS_FLAGS_SKIP_HW) ? true : false;
+ }
+ 
+ static inline bool tc_should_offload(const struct net_device *dev,
+ 				     const struct tcf_proto *tp, u32 flags)
+ {
+ 	if (tc_skip_hw(flags))
+ 		return false;
+ 	return tc_can_offload(dev, tp);
+ }
+ 
+ static inline bool tc_skip_sw(u32 flags)
+ {
+ 	return (flags & TCA_CLS_FLAGS_SKIP_SW) ? true : false;
+ }
+ 
+ /* SKIP_HW and SKIP_SW are mutually exclusive flags. */
+ static inline bool tc_flags_valid(u32 flags)
+ {
+ 	if (flags & ~(TCA_CLS_FLAGS_SKIP_HW | TCA_CLS_FLAGS_SKIP_SW))
+ 		return false;
+ 
+ 	if (!(flags ^ (TCA_CLS_FLAGS_SKIP_HW | TCA_CLS_FLAGS_SKIP_SW)))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ enum tc_fl_command {
+ 	TC_CLSFLOWER_REPLACE,
+ 	TC_CLSFLOWER_DESTROY,
+ 	TC_CLSFLOWER_STATS,
+ };
+ 
+ struct tc_cls_flower_offload {
+ 	enum tc_fl_command command;
+ 	u32 prio;
+ 	unsigned long cookie;
+ 	struct flow_dissector *dissector;
+ 	struct fl_flow_key *mask;
+ 	struct fl_flow_key *key;
+ 	struct tcf_exts *exts;
+ };
+ 
+ enum tc_matchall_command {
+ 	TC_CLSMATCHALL_REPLACE,
+ 	TC_CLSMATCHALL_DESTROY,
+ };
+ 
+ struct tc_cls_matchall_offload {
+ 	enum tc_matchall_command command;
+ 	struct tcf_exts *exts;
+ 	unsigned long cookie;
+ };
+ 
+ enum tc_clsbpf_command {
+ 	TC_CLSBPF_ADD,
+ 	TC_CLSBPF_REPLACE,
+ 	TC_CLSBPF_DESTROY,
+ 	TC_CLSBPF_STATS,
+ };
+ 
+ struct tc_cls_bpf_offload {
+ 	enum tc_clsbpf_command command;
+ 	struct tcf_exts *exts;
+ 	struct bpf_prog *prog;
+ 	const char *name;
+ 	bool exts_integrated;
+ 	u32 gen_flags;
+ };
+ 
+ 
+ /* This structure holds cookie structure that is passed from user
+  * to the kernel for actions and classifiers
+  */
+ struct tc_cookie {
+ 	u8  *data;
+ 	u32 len;
+ };
++>>>>>>> 69ca05ce9dec (sched: cls_flower: expose priority to offloading netdevice)
  #endif
* Unmerged path net/sched/cls_flower.c
* Unmerged path include/net/pkt_cls.h
* Unmerged path net/sched/cls_flower.c
