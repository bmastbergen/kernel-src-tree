dax: remove buffer_size_valid()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Ross Zwisler <ross.zwisler@linux.intel.com>
commit fa0d3fce7ceff0aae62f3e19678713bc5a7f3377
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/fa0d3fce.failed

Now that ext4 properly sets bh.b_size when we call get_block() for a hole,
rely on that value and remove the buffer_size_valid() sanity check.

	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit fa0d3fce7ceff0aae62f3e19678713bc5a7f3377)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 3ad95e9ec809,b09817ac6f87..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -127,38 -123,6 +127,41 @@@ static bool buffer_written(struct buffe
  	return buffer_mapped(bh) && !buffer_unwritten(bh);
  }
  
++<<<<<<< HEAD
 +/*
 + * When ext4 encounters a hole, it returns without modifying the buffer_head
 + * which means that we can't trust b_size.  To cope with this, we set b_state
 + * to 0 before calling get_block and, if any bit is set, we know we can trust
 + * b_size.  Unfortunate, really, since ext4 knows precisely how long a hole is
 + * and would save us time calling get_block repeatedly.
 + */
 +static bool buffer_size_valid(struct buffer_head *bh)
 +{
 +	return bh->b_state != 0;
 +}
 +
 +static int zero_toiovecend_partial(const struct iovec *iov, int offset, int len)
 +{
 +	int zero, orig_len = len;
 +	for (; len > 0; ++iov) {
 +		/* Skip over the finished iovecs */
 +		if (unlikely(offset >= iov->iov_len)) {
 +			offset -= iov->iov_len;
 +			continue;
 +		}
 +		zero = min_t(unsigned int, iov->iov_len - offset, len);
 +		if (clear_user(iov->iov_base + offset, zero))
 +			return orig_len - len;
 +		offset = 0;
 +		len -= zero;
 +	}
 +
 +	return orig_len - len;
 +}
 +
 +
++=======
++>>>>>>> fa0d3fce7cef (dax: remove buffer_size_valid())
  static sector_t to_sector(const struct buffer_head *bh,
  		const struct inode *inode)
  {
@@@ -199,10 -164,15 +202,8 @@@ static ssize_t dax_io(int rw, struct in
  				rc = get_block(inode, block, bh, rw == WRITE);
  				if (rc)
  					break;
- 				if (!buffer_size_valid(bh))
- 					bh->b_size = 1 << blkbits;
  				bh_max = pos - first + bh->b_size;
  				bdev = bh->b_bdev;
 -				/*
 -				 * We allow uninitialized buffers for writes
 -				 * beyond EOF as those cannot race with faults
 -				 */
 -				WARN_ON_ONCE(
 -					(buffer_new(bh) && block < file_blks) ||
 -					(rw == WRITE && buffer_unwritten(bh)));
  			} else {
  				unsigned done = bh->b_size -
  						(bh_max - (pos - first));
@@@ -860,13 -997,10 +861,18 @@@ int __dax_pmd_fault(struct vm_area_stru
  
  	bdev = bh.b_bdev;
  
++<<<<<<< HEAD
 +	/*
 +	 * If the filesystem isn't willing to tell us the length of a hole,
 +	 * just fall back to PTEs.  Calling get_block 512 times in a loop
 +	 * would be silly.
 +	 */
 +	if (!buffer_size_valid(&bh) || bh.b_size < PMD_SIZE)
++=======
+ 	if (bh.b_size < PMD_SIZE) {
+ 		dax_pmd_dbg(&bh, address, "allocated block too small");
++>>>>>>> fa0d3fce7cef (dax: remove buffer_size_valid())
  		return VM_FAULT_FALLBACK;
 -	}
  
  	/*
  	 * If we allocated new storage, make sure no process has any
* Unmerged path fs/dax.c
