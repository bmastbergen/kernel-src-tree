Revert "scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth"

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [scsi] megaraid: Revert "scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth" (Tomas Henzl) [1417038]
Rebuild_FUZZ: 94.90%
commit-author Shivasharan S <shivasharan.srikanteshwara@broadcom.com>
commit 18bbcabdc6cc6be8c7f6d80c85d314535d76188d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/18bbcabd.failed

This reverts commit "3e5eadb1a881" ("scsi: megaraid_sas: Enable or
Disable Fast path based on the PCI Threshold Bandwidth")

This patch was aimed to increase performance of R1 Write operation for
large IO size.  Since this method used timer approach, it turn on/off
fast path did not work as expected.  Patch 0013 describes new algorithm
and performance number.

	Signed-off-by: Shivasharan S <shivasharan.srikanteshwara@broadcom.com>
	Signed-off-by: Kashyap Desai <kashyap.desai@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Reviewed-by: Tomas Henzl <thenzl@redhat.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 18bbcabdc6cc6be8c7f6d80c85d314535d76188d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/megaraid/megaraid_sas.h
#	drivers/scsi/megaraid/megaraid_sas_fp.c
#	drivers/scsi/megaraid/megaraid_sas_fusion.h
diff --cc drivers/scsi/megaraid/megaraid_sas.h
index 0e2d80818bd7,f5c47424f4f2..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas.h
+++ b/drivers/scsi/megaraid/megaraid_sas.h
@@@ -2151,6 -2207,8 +2151,11 @@@ struct megasas_instance 
  	bool dev_handle;
  	bool fw_sync_cache_support;
  	bool is_ventura;
++<<<<<<< HEAD
++=======
+ 	bool msix_combined;
+ 	u16 max_raid_mapsize;
++>>>>>>> 18bbcabdc6cc (Revert "scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth")
  };
  struct MR_LD_VF_MAP {
  	u32 size;
diff --cc drivers/scsi/megaraid/megaraid_sas_fp.c
index eb9ff444c099,f1384b01b3d3..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas_fp.c
+++ b/drivers/scsi/megaraid/megaraid_sas_fp.c
@@@ -186,11 -189,194 +186,201 @@@ void MR_PopulateDrvRaidMap(struct megas
  	struct MR_DRV_RAID_MAP_ALL *drv_map =
  			fusion->ld_drv_map[(instance->map_id & 1)];
  	struct MR_DRV_RAID_MAP *pDrvRaidMap = &drv_map->raidMap;
++<<<<<<< HEAD
++=======
+ 	void *raid_map_data = NULL;
+ 
+ 	memset(drv_map, 0, fusion->drv_map_sz);
+ 	memset(pDrvRaidMap->ldTgtIdToLd,
+ 		0xff, (sizeof(u16) * MAX_LOGICAL_DRIVES_DYN));
+ 
+ 	if (instance->max_raid_mapsize) {
+ 		fw_map_dyn = fusion->ld_map[(instance->map_id & 1)];
+ #if VD_EXT_DEBUG
+ 		dev_dbg(&instance->pdev->dev, "raidMapSize 0x%x fw_map_dyn->descTableOffset 0x%x\n",
+ 			le32_to_cpu(fw_map_dyn->raid_map_size),
+ 			le32_to_cpu(fw_map_dyn->desc_table_offset));
+ 		dev_dbg(&instance->pdev->dev, "descTableSize 0x%x descTableNumElements 0x%x\n",
+ 			le32_to_cpu(fw_map_dyn->desc_table_size),
+ 			le32_to_cpu(fw_map_dyn->desc_table_num_elements));
+ 		dev_dbg(&instance->pdev->dev, "drv map %p ldCount %d\n",
+ 			drv_map, fw_map_dyn->ld_count);
+ #endif
+ 		desc_table =
+ 		(struct MR_RAID_MAP_DESC_TABLE *)((void *)fw_map_dyn + le32_to_cpu(fw_map_dyn->desc_table_offset));
+ 		if (desc_table != fw_map_dyn->raid_map_desc_table)
+ 			dev_dbg(&instance->pdev->dev, "offsets of desc table are not matching desc %p original %p\n",
+ 				desc_table, fw_map_dyn->raid_map_desc_table);
+ 
+ 		ld_count = (u16)le16_to_cpu(fw_map_dyn->ld_count);
+ 		pDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);
+ 		pDrvRaidMap->fpPdIoTimeoutSec =
+ 			fw_map_dyn->fp_pd_io_timeout_sec;
+ 		pDrvRaidMap->totalSize = sizeof(struct MR_DRV_RAID_MAP_ALL);
+ 		/* point to actual data starting point*/
+ 		raid_map_data = (void *)fw_map_dyn +
+ 			le32_to_cpu(fw_map_dyn->desc_table_offset) +
+ 			le32_to_cpu(fw_map_dyn->desc_table_size);
+ 
+ 		for (i = 0; i < le32_to_cpu(fw_map_dyn->desc_table_num_elements); ++i) {
+ 
+ #if VD_EXT_DEBUG
+ 			dev_dbg(&instance->pdev->dev, "desc table %p\n",
+ 				desc_table);
+ 			dev_dbg(&instance->pdev->dev, "raidmap type %d, raidmapOffset 0x%x\n",
+ 				desc_table->raid_map_desc_type,
+ 				desc_table->raid_map_desc_offset);
+ 			dev_dbg(&instance->pdev->dev, "raid map number of elements 0%x, raidmapsize 0x%x\n",
+ 				desc_table->raid_map_desc_elements,
+ 				desc_table->raid_map_desc_buffer_size);
+ #endif
+ 			switch (le32_to_cpu(desc_table->raid_map_desc_type)) {
+ 			case RAID_MAP_DESC_TYPE_DEVHDL_INFO:
+ 				fw_map_dyn->dev_hndl_info =
+ 				(struct MR_DEV_HANDLE_INFO *)(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "devHndlInfo  address %p\n",
+ 					fw_map_dyn->dev_hndl_info);
+ #endif
+ 				memcpy(pDrvRaidMap->devHndlInfo,
+ 				fw_map_dyn->dev_hndl_info,
+ 				sizeof(struct MR_DEV_HANDLE_INFO) *
+ 				le32_to_cpu(desc_table->raid_map_desc_elements));
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_TGTID_INFO:
+ 				fw_map_dyn->ld_tgt_id_to_ld =
+ 				(u16 *) (raid_map_data +
+ 				le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 			dev_dbg(&instance->pdev->dev, "ldTgtIdToLd  address %p\n",
+ 				fw_map_dyn->ld_tgt_id_to_ld);
+ #endif
+ 			for (j = 0; j < le32_to_cpu(desc_table->raid_map_desc_elements); j++) {
+ 				pDrvRaidMap->ldTgtIdToLd[j] =
+ 				fw_map_dyn->ld_tgt_id_to_ld[j];
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, " %d drv ldTgtIdToLd %d\n",
+ 					j, pDrvRaidMap->ldTgtIdToLd[j]);
+ #endif
+ 			}
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_ARRAY_INFO:
+ 				fw_map_dyn->ar_map_info =
+ 				(struct MR_ARRAY_INFO *)
+ 				(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "arMapInfo  address %p\n",
+ 					fw_map_dyn->ar_map_info);
+ #endif
+ 
+ 				memcpy(pDrvRaidMap->arMapInfo,
+ 				fw_map_dyn->ar_map_info,
+ 				sizeof(struct MR_ARRAY_INFO) * le32_to_cpu(desc_table->raid_map_desc_elements));
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_SPAN_INFO:
+ 				fw_map_dyn->ld_span_map =
+ 				(struct MR_LD_SPAN_MAP *)
+ 				(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ 				memcpy(pDrvRaidMap->ldSpanMap,
+ 				fw_map_dyn->ld_span_map,
+ 				sizeof(struct MR_LD_SPAN_MAP) * le32_to_cpu(desc_table->raid_map_desc_elements));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "ldSpanMap  address %p\n",
+ 					fw_map_dyn->ld_span_map);
+ 				dev_dbg(&instance->pdev->dev, "MR_LD_SPAN_MAP size 0x%lx\n",
+ 					sizeof(struct MR_LD_SPAN_MAP));
+ 				for (j = 0; j < ld_count; j++) {
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : fw_map_dyn->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 					j, j, fw_map_dyn->ld_span_map[j].ldRaid.targetId);
+ 					dev_dbg(&instance->pdev->dev, "fw_map_dyn->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 					j, fw_map_dyn->ld_span_map[j].ldRaid.seqNum);
+ 					dev_dbg(&instance->pdev->dev, "fw_map_dyn->ld_span_map[%d].ldRaid.rowSize 0x%x\n",
+ 					j, (u32)fw_map_dyn->ld_span_map[j].ldRaid.rowSize);
+ 
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) :pDrvRaidMap->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 					j, j, pDrvRaidMap->ldSpanMap[j].ldRaid.targetId);
+ 					dev_dbg(&instance->pdev->dev, "DrvRaidMap->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 					j, pDrvRaidMap->ldSpanMap[j].ldRaid.seqNum);
+ 					dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 					j, (u32)pDrvRaidMap->ldSpanMap[j].ldRaid.rowSize);
+ 
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : drv raid map all %p\n",
+ 					instance->unique_id, drv_map);
+ 					dev_dbg(&instance->pdev->dev, "raid map %p LD RAID MAP %p/%p\n",
+ 					pDrvRaidMap,
+ 					&fw_map_dyn->ld_span_map[j].ldRaid,
+ 					&pDrvRaidMap->ldSpanMap[j].ldRaid);
+ 				}
+ #endif
+ 			break;
+ 			default:
+ 				dev_dbg(&instance->pdev->dev, "wrong number of desctableElements %d\n",
+ 					fw_map_dyn->desc_table_num_elements);
+ 			}
+ 			++desc_table;
+ 		}
+ 
+ 	} else if (instance->supportmax256vd) {
+ 		fw_map_ext =
+ 		(struct MR_FW_RAID_MAP_EXT *) fusion->ld_map[(instance->map_id & 1)];
+ 		ld_count = (u16)le16_to_cpu(fw_map_ext->ldCount);
+ 		if (ld_count > MAX_LOGICAL_DRIVES_EXT) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas: LD count exposed in RAID map in not valid\n");
+ 			return;
+ 		}
+ #if VD_EXT_DEBUG
+ 		for (i = 0; i < ld_count; i++) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) :Index 0x%x\n",
+ 				instance->unique_id, i);
+ 			dev_dbg(&instance->pdev->dev, "Target Id 0x%x\n",
+ 				fw_map_ext->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "Seq Num 0x%x Size 0/%llx\n",
+ 				fw_map_ext->ldSpanMap[i].ldRaid.seqNum,
+ 				fw_map_ext->ldSpanMap[i].ldRaid.size);
+ 		}
+ #endif
+ 
+ 		pDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);
+ 		pDrvRaidMap->fpPdIoTimeoutSec = fw_map_ext->fpPdIoTimeoutSec;
+ 		for (i = 0; i < (MAX_LOGICAL_DRIVES_EXT); i++)
+ 			pDrvRaidMap->ldTgtIdToLd[i] =
+ 				(u16)fw_map_ext->ldTgtIdToLd[i];
+ 		memcpy(pDrvRaidMap->ldSpanMap, fw_map_ext->ldSpanMap,
+ 				sizeof(struct MR_LD_SPAN_MAP) * ld_count);
+ #if VD_EXT_DEBUG
+ 		for (i = 0; i < ld_count; i++) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : fw_map_ext->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 			i, i, fw_map_ext->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "fw_map_ext->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 			i, fw_map_ext->ldSpanMap[i].ldRaid.seqNum);
+ 			dev_dbg(&instance->pdev->dev, "fw_map_ext->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 			i, (u32)fw_map_ext->ldSpanMap[i].ldRaid.rowSize);
+ 
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : pDrvRaidMap->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 			i, i, pDrvRaidMap->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 			i, pDrvRaidMap->ldSpanMap[i].ldRaid.seqNum);
+ 			dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 			i, (u32)pDrvRaidMap->ldSpanMap[i].ldRaid.rowSize);
+ 
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : drv raid map all %p\n",
+ 			instance->unique_id, drv_map);
+ 			dev_dbg(&instance->pdev->dev, "raid map %p LD RAID MAP %p %p\n",
+ 			pDrvRaidMap, &fw_map_ext->ldSpanMap[i].ldRaid,
+ 			&pDrvRaidMap->ldSpanMap[i].ldRaid);
+ 		}
+ #endif
+ 		memcpy(pDrvRaidMap->arMapInfo, fw_map_ext->arMapInfo,
+ 			sizeof(struct MR_ARRAY_INFO) * MAX_API_ARRAYS_EXT);
+ 		memcpy(pDrvRaidMap->devHndlInfo, fw_map_ext->devHndlInfo,
+ 			sizeof(struct MR_DEV_HANDLE_INFO) *
+ 					MAX_RAIDMAP_PHYSICAL_DEVICES);
++>>>>>>> 18bbcabdc6cc (Revert "scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth")
  
 +	if (instance->supportmax256vd) {
 +		memcpy(fusion->ld_drv_map[instance->map_id & 1],
 +			fusion->ld_map[instance->map_id & 1],
 +			fusion->current_map_sz);
  		/* New Raid map will not set totalSize, so keep expected value
  		 * for legacy code in ValidateMapInfo
  		 */
diff --cc drivers/scsi/megaraid/megaraid_sas_fusion.h
index ef6bfe55344c,82a4ff720eab..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas_fusion.h
+++ b/drivers/scsi/megaraid/megaraid_sas_fusion.h
@@@ -847,6 -931,91 +847,94 @@@ struct MR_LD_TARGET_SYNC 
  	__le16 seqNum;
  };
  
++<<<<<<< HEAD
++=======
+ /*
+  * RAID Map descriptor Types.
+  * Each element should uniquely idetify one data structure in the RAID map
+  */
+ enum MR_RAID_MAP_DESC_TYPE {
+ 	/* MR_DEV_HANDLE_INFO data */
+ 	RAID_MAP_DESC_TYPE_DEVHDL_INFO    = 0x0,
+ 	/* target to Ld num Index map */
+ 	RAID_MAP_DESC_TYPE_TGTID_INFO     = 0x1,
+ 	/* MR_ARRAY_INFO data */
+ 	RAID_MAP_DESC_TYPE_ARRAY_INFO     = 0x2,
+ 	/* MR_LD_SPAN_MAP data */
+ 	RAID_MAP_DESC_TYPE_SPAN_INFO      = 0x3,
+ 	RAID_MAP_DESC_TYPE_COUNT,
+ };
+ 
+ /*
+  * This table defines the offset, size and num elements  of each descriptor
+  * type in the RAID Map buffer
+  */
+ struct MR_RAID_MAP_DESC_TABLE {
+ 	/* Raid map descriptor type */
+ 	u32 raid_map_desc_type;
+ 	/* Offset into the RAID map buffer where
+ 	 *  descriptor data is saved
+ 	 */
+ 	u32 raid_map_desc_offset;
+ 	/* total size of the
+ 	 * descriptor buffer
+ 	 */
+ 	u32 raid_map_desc_buffer_size;
+ 	/* Number of elements contained in the
+ 	 *  descriptor buffer
+ 	 */
+ 	u32 raid_map_desc_elements;
+ };
+ 
+ /*
+  * Dynamic Raid Map Structure.
+  */
+ struct MR_FW_RAID_MAP_DYNAMIC {
+ 	u32 raid_map_size;   /* total size of RAID Map structure */
+ 	u32 desc_table_offset;/* Offset of desc table into RAID map*/
+ 	u32 desc_table_size;  /* Total Size of desc table */
+ 	/* Total Number of elements in the desc table */
+ 	u32 desc_table_num_elements;
+ 	u64	reserved1;
+ 	u32	reserved2[3];	/*future use */
+ 	/* timeout value used by driver in FP IOs */
+ 	u8 fp_pd_io_timeout_sec;
+ 	u8 reserved3[3];
+ 	/* when this seqNum increments, driver needs to
+ 	 *  release RMW buffers asap
+ 	 */
+ 	u32 rmw_fp_seq_num;
+ 	u16 ld_count;	/* count of lds. */
+ 	u16 ar_count;   /* count of arrays */
+ 	u16 span_count; /* count of spans */
+ 	u16 reserved4[3];
+ /*
+  * The below structure of pointers is only to be used by the driver.
+  * This is added in the ,API to reduce the amount of code changes
+  * needed in the driver to support dynamic RAID map Firmware should
+  * not update these pointers while preparing the raid map
+  */
+ 	union {
+ 		struct {
+ 			struct MR_DEV_HANDLE_INFO  *dev_hndl_info;
+ 			u16 *ld_tgt_id_to_ld;
+ 			struct MR_ARRAY_INFO *ar_map_info;
+ 			struct MR_LD_SPAN_MAP *ld_span_map;
+ 			};
+ 		u64 ptr_structure_size[RAID_MAP_DESC_TYPE_COUNT];
+ 		};
+ /*
+  * RAID Map descriptor table defines the layout of data in the RAID Map.
+  * The size of the descriptor table itself could change.
+  */
+ 	/* Variable Size descriptor Table. */
+ 	struct MR_RAID_MAP_DESC_TABLE
+ 			raid_map_desc_table[RAID_MAP_DESC_TYPE_COUNT];
+ 	/* Variable Size buffer containing all data */
+ 	u32 raid_map_desc_data[1];
+ }; /* Dynamicaly sized RAID MAp structure */
+ 
++>>>>>>> 18bbcabdc6cc (Revert "scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth")
  #define IEEE_SGE_FLAGS_ADDR_MASK            (0x03)
  #define IEEE_SGE_FLAGS_SYSTEM_ADDR          (0x00)
  #define IEEE_SGE_FLAGS_IOCDDR_ADDR          (0x01)
* Unmerged path drivers/scsi/megaraid/megaraid_sas.h
* Unmerged path drivers/scsi/megaraid/megaraid_sas_fp.c
* Unmerged path drivers/scsi/megaraid/megaraid_sas_fusion.h
