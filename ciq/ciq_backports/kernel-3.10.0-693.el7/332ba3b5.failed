scsi: lpfc: don't dereference dma_buf->iocbq before null check

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [scsi] lpfc: don't dereference dma_buf-&gt; iocbq before null check (Ewan Milne) [1384922]
Rebuild_FUZZ: 90.16%
commit-author James Smart <jsmart2021@gmail.com>
commit 332ba3b5d6d27a60d445704ed7c88c7e9f958a30
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/332ba3b5.failed

From: Colin Ian King <colin.king@canonical.com>

dma_buf->iocbq is being dereferenced immediately before it is
being null checked, so we have a potential null pointer dereference
bug.  Fix this by only dereferencing it only once we have passed
a null check on the pointer.

Detected by CoverityScan, CID#1411652 ("Dereference before null check")

	Signed-off-by: Colin Ian King <colin.king@canonical.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 332ba3b5d6d27a60d445704ed7c88c7e9f958a30)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_mem.c
diff --cc drivers/scsi/lpfc/lpfc_mem.c
index 3fa65338d3f5,5986c7957199..000000000000
--- a/drivers/scsi/lpfc/lpfc_mem.c
+++ b/drivers/scsi/lpfc/lpfc_mem.c
@@@ -540,7 -592,134 +540,138 @@@ lpfc_sli4_rb_free(struct lpfc_hba *phba
  	pci_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
  	pci_pool_free(phba->lpfc_drb_pool, dmab->dbuf.virt, dmab->dbuf.phys);
  	kfree(dmab);
++<<<<<<< HEAD
 +	return;
++=======
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_alloc - Allocate an SLI4 Receive buffer
+  * @phba: HBA to allocate a receive buffer for
+  *
+  * Description: Allocates a DMA-mapped receive buffer from the lpfc_hrb_pool PCI
+  * pool along a non-DMA-mapped container for it.
+  *
+  * Notes: Not interrupt-safe.  Must be called with no locks held.
+  *
+  * Returns:
+  *   pointer to HBQ on success
+  *   NULL on failure
+  **/
+ struct rqb_dmabuf *
+ lpfc_sli4_nvmet_alloc(struct lpfc_hba *phba)
+ {
+ 	struct rqb_dmabuf *dma_buf;
+ 	struct lpfc_iocbq *nvmewqe;
+ 	union lpfc_wqe128 *wqe;
+ 
+ 	dma_buf = kzalloc(sizeof(struct rqb_dmabuf), GFP_KERNEL);
+ 	if (!dma_buf)
+ 		return NULL;
+ 
+ 	dma_buf->hbuf.virt = pci_pool_alloc(phba->lpfc_hrb_pool, GFP_KERNEL,
+ 					    &dma_buf->hbuf.phys);
+ 	if (!dma_buf->hbuf.virt) {
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->dbuf.virt = pci_pool_alloc(phba->lpfc_drb_pool, GFP_KERNEL,
+ 					    &dma_buf->dbuf.phys);
+ 	if (!dma_buf->dbuf.virt) {
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->total_size = LPFC_DATA_BUF_SIZE;
+ 
+ 	dma_buf->context = kzalloc(sizeof(struct lpfc_nvmet_rcv_ctx),
+ 				   GFP_KERNEL);
+ 	if (!dma_buf->context) {
+ 		pci_pool_free(phba->lpfc_drb_pool, dma_buf->dbuf.virt,
+ 			      dma_buf->dbuf.phys);
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 
+ 	dma_buf->iocbq = lpfc_sli_get_iocbq(phba);
+ 	if (!dma_buf->iocbq) {
+ 		kfree(dma_buf->context);
+ 		pci_pool_free(phba->lpfc_drb_pool, dma_buf->dbuf.virt,
+ 			      dma_buf->dbuf.phys);
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_NVME,
+ 				"2621 Ran out of nvmet iocb/WQEs\n");
+ 		return NULL;
+ 	}
+ 	dma_buf->iocbq->iocb_flag = LPFC_IO_NVMET;
+ 	nvmewqe = dma_buf->iocbq;
+ 	wqe = (union lpfc_wqe128 *)&nvmewqe->wqe;
+ 	/* Initialize WQE */
+ 	memset(wqe, 0, sizeof(union lpfc_wqe));
+ 	/* Word 7 */
+ 	bf_set(wqe_ct, &wqe->generic.wqe_com, SLI4_CT_RPI);
+ 	bf_set(wqe_class, &wqe->generic.wqe_com, CLASS3);
+ 	bf_set(wqe_pu, &wqe->generic.wqe_com, 1);
+ 	/* Word 10 */
+ 	bf_set(wqe_nvme, &wqe->fcp_tsend.wqe_com, 1);
+ 	bf_set(wqe_ebde_cnt, &wqe->generic.wqe_com, 0);
+ 	bf_set(wqe_qosd, &wqe->generic.wqe_com, 0);
+ 
+ 	dma_buf->iocbq->context1 = NULL;
+ 	spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 	dma_buf->sglq = __lpfc_sli_get_nvmet_sglq(phba, dma_buf->iocbq);
+ 	spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 	if (!dma_buf->sglq) {
+ 		lpfc_sli_release_iocbq(phba, dma_buf->iocbq);
+ 		kfree(dma_buf->context);
+ 		pci_pool_free(phba->lpfc_drb_pool, dma_buf->dbuf.virt,
+ 			      dma_buf->dbuf.phys);
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_NVME,
+ 				"6132 Ran out of nvmet XRIs\n");
+ 		return NULL;
+ 	}
+ 	return dma_buf;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_free - Frees a receive buffer
+  * @phba: HBA buffer was allocated for
+  * @dmab: DMA Buffer container returned by lpfc_sli4_rbq_alloc
+  *
+  * Description: Frees both the container and the DMA-mapped buffers returned by
+  * lpfc_sli4_nvmet_alloc.
+  *
+  * Notes: Can be called with or without locks held.
+  *
+  * Returns: None
+  **/
+ void
+ lpfc_sli4_nvmet_free(struct lpfc_hba *phba, struct rqb_dmabuf *dmab)
+ {
+ 	unsigned long flags;
+ 
+ 	__lpfc_clear_active_sglq(phba, dmab->sglq->sli4_lxritag);
+ 	dmab->sglq->state = SGL_FREED;
+ 	dmab->sglq->ndlp = NULL;
+ 
+ 	spin_lock_irqsave(&phba->sli4_hba.sgl_list_lock, flags);
+ 	list_add_tail(&dmab->sglq->list, &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 	spin_unlock_irqrestore(&phba->sli4_hba.sgl_list_lock, flags);
+ 
+ 	lpfc_sli_release_iocbq(phba, dmab->iocbq);
+ 	kfree(dmab->context);
+ 	pci_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
+ 	pci_pool_free(phba->lpfc_drb_pool, dmab->dbuf.virt, dmab->dbuf.phys);
+ 	kfree(dmab);
++>>>>>>> 332ba3b5d6d2 (scsi: lpfc: don't dereference dma_buf->iocbq before null check)
  }
  
  /**
* Unmerged path drivers/scsi/lpfc/lpfc_mem.c
