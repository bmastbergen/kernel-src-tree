KVM: PPC: Book3S HV: Handle passthrough interrupts in guest

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Suresh Warrier <warrier@linux.vnet.ibm.com>
commit e3c13e56a4717ee334837a20c596e527eb6355e1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e3c13e56.failed

Currently, KVM switches back to the host to handle any external
interrupt (when the interrupt is received while running in the
guest). This patch updates real-mode KVM to check if an interrupt
is generated by a passthrough adapter that is owned by this guest.
If so, the real mode KVM will directly inject the corresponding
virtual interrupt to the guest VCPU's ICS and also EOI the interrupt
in hardware. In short, the interrupt is handled entirely in real
mode in the guest context without switching back to the host.

In some rare cases, the interrupt cannot be completely handled in
real mode, for instance, a VCPU that is sleeping needs to be woken
up. In this case, KVM simply switches back to the host with trap
reason set to 0x500. This works, but it is clearly not very efficient.
A following patch will distinguish this case and handle it
correctly in the host. Note that we can use the existing
check_too_hard() routine even though we are not in a hypercall to
determine if there is unfinished business that needs to be
completed in host virtual mode.

The patch assumes that the mapping between hardware interrupt IRQ
and virtual IRQ to be injected to the guest already exists for the
PCI passthrough interrupts that need to be handled in real mode.
If the mapping does not exist, KVM falls back to the default
existing behavior.

The KVM real mode code reads mappings from the mapped array in the
passthrough IRQ map without taking any lock.  We carefully order the
loads and stores of the fields in the kvmppc_irq_map data structure
using memory barriers to avoid an inconsistent mapping being seen by
the reader. Thus, although it is possible to miss a map entry, it is
not possible to read a stale value.

[paulus@ozlabs.org - get irq_chip from irq_map rather than pimap,
 pulled out powernv eoi change into a separate patch, made
 kvmppc_read_intr get the vcpu from the paca rather than being
 passed in, rewrote the logic at the end of kvmppc_read_intr to
 avoid deep indentation, simplified logic in book3s_hv_rmhandlers.S
 since we were always restoring SRR0/1 anyway, get rid of the cached
 array (just use the mapped array), removed the kick_all_cpus_sync()
 call, clear saved_xirr PACA field when we handle the interrupt in
 real mode, fix compilation with CONFIG_KVM_XICS=n.]

	Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
(cherry picked from commit e3c13e56a4717ee334837a20c596e527eb6355e1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/kvm_ppc.h
#	arch/powerpc/kvm/book3s_hv.c
#	arch/powerpc/kvm/book3s_hv_builtin.c
#	arch/powerpc/kvm/book3s_hv_rm_xics.c
#	arch/powerpc/kvm/book3s_hv_rmhandlers.S
diff --cc arch/powerpc/include/asm/kvm_ppc.h
index 4af3129ec8aa,4299a1f79a91..000000000000
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@@ -445,7 -477,18 +445,15 @@@ extern u64 kvmppc_xics_get_icp(struct k
  extern int kvmppc_xics_set_icp(struct kvm_vcpu *vcpu, u64 icpval);
  extern int kvmppc_xics_connect_vcpu(struct kvm_device *dev,
  			struct kvm_vcpu *vcpu, u32 cpu);
++<<<<<<< HEAD
++=======
+ extern void kvmppc_xics_ipi_action(void);
+ extern long kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu, u32 xirr,
+ 				 struct kvmppc_irq_map *irq_map,
+ 				 struct kvmppc_passthru_irqmap *pimap);
+ extern int h_ipi_redirect;
++>>>>>>> e3c13e56a471 (KVM: PPC: Book3S HV: Handle passthrough interrupts in guest)
  #else
 -static inline struct kvmppc_passthru_irqmap *kvmppc_get_passthru_irqmap(
 -				struct kvm *kvm)
 -	{ return NULL; }
 -static inline void kvmppc_alloc_host_rm_ops(void) {};
 -static inline void kvmppc_free_host_rm_ops(void) {};
 -static inline void kvmppc_free_pimap(struct kvm *kvm) {};
  static inline int kvmppc_xics_enabled(struct kvm_vcpu *vcpu)
  	{ return 0; }
  static inline void kvmppc_xics_free_icp(struct kvm_vcpu *vcpu) { }
diff --cc arch/powerpc/kvm/book3s_hv.c
index 32dd0caea96b,a393c2b6b1b6..000000000000
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@@ -3144,6 -3411,180 +3144,177 @@@ static int kvmppc_core_check_processor_
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_KVM_XICS
+ 
+ void kvmppc_free_pimap(struct kvm *kvm)
+ {
+ 	kfree(kvm->arch.pimap);
+ }
+ 
+ static struct kvmppc_passthru_irqmap *kvmppc_alloc_pimap(void)
+ {
+ 	return kzalloc(sizeof(struct kvmppc_passthru_irqmap), GFP_KERNEL);
+ }
+ 
+ static int kvmppc_set_passthru_irq(struct kvm *kvm, int host_irq, int guest_gsi)
+ {
+ 	struct irq_desc *desc;
+ 	struct kvmppc_irq_map *irq_map;
+ 	struct kvmppc_passthru_irqmap *pimap;
+ 	struct irq_chip *chip;
+ 	int i;
+ 
+ 	desc = irq_to_desc(host_irq);
+ 	if (!desc)
+ 		return -EIO;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	pimap = kvm->arch.pimap;
+ 	if (pimap == NULL) {
+ 		/* First call, allocate structure to hold IRQ map */
+ 		pimap = kvmppc_alloc_pimap();
+ 		if (pimap == NULL) {
+ 			mutex_unlock(&kvm->lock);
+ 			return -ENOMEM;
+ 		}
+ 		kvm->arch.pimap = pimap;
+ 	}
+ 
+ 	/*
+ 	 * For now, we only support interrupts for which the EOI operation
+ 	 * is an OPAL call followed by a write to XIRR, since that's
+ 	 * what our real-mode EOI code does.
+ 	 */
+ 	chip = irq_data_get_irq_chip(&desc->irq_data);
+ 	if (!chip || !is_pnv_opal_msi(chip)) {
+ 		pr_warn("kvmppc_set_passthru_irq_hv: Could not assign IRQ map for (%d,%d)\n",
+ 			host_irq, guest_gsi);
+ 		mutex_unlock(&kvm->lock);
+ 		return -ENOENT;
+ 	}
+ 
+ 	/*
+ 	 * See if we already have an entry for this guest IRQ number.
+ 	 * If it's mapped to a hardware IRQ number, that's an error,
+ 	 * otherwise re-use this entry.
+ 	 */
+ 	for (i = 0; i < pimap->n_mapped; i++) {
+ 		if (guest_gsi == pimap->mapped[i].v_hwirq) {
+ 			if (pimap->mapped[i].r_hwirq) {
+ 				mutex_unlock(&kvm->lock);
+ 				return -EINVAL;
+ 			}
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (i == KVMPPC_PIRQ_MAPPED) {
+ 		mutex_unlock(&kvm->lock);
+ 		return -EAGAIN;		/* table is full */
+ 	}
+ 
+ 	irq_map = &pimap->mapped[i];
+ 
+ 	irq_map->v_hwirq = guest_gsi;
+ 	irq_map->desc = desc;
+ 
+ 	/*
+ 	 * Order the above two stores before the next to serialize with
+ 	 * the KVM real mode handler.
+ 	 */
+ 	smp_wmb();
+ 	irq_map->r_hwirq = desc->irq_data.hwirq;
+ 
+ 	if (i == pimap->n_mapped)
+ 		pimap->n_mapped++;
+ 
+ 	mutex_unlock(&kvm->lock);
+ 
+ 	return 0;
+ }
+ 
+ static int kvmppc_clr_passthru_irq(struct kvm *kvm, int host_irq, int guest_gsi)
+ {
+ 	struct irq_desc *desc;
+ 	struct kvmppc_passthru_irqmap *pimap;
+ 	int i;
+ 
+ 	desc = irq_to_desc(host_irq);
+ 	if (!desc)
+ 		return -EIO;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	if (kvm->arch.pimap == NULL) {
+ 		mutex_unlock(&kvm->lock);
+ 		return 0;
+ 	}
+ 	pimap = kvm->arch.pimap;
+ 
+ 	for (i = 0; i < pimap->n_mapped; i++) {
+ 		if (guest_gsi == pimap->mapped[i].v_hwirq)
+ 			break;
+ 	}
+ 
+ 	if (i == pimap->n_mapped) {
+ 		mutex_unlock(&kvm->lock);
+ 		return -ENODEV;
+ 	}
+ 
+ 	/* invalidate the entry */
+ 	pimap->mapped[i].r_hwirq = 0;
+ 
+ 	/*
+ 	 * We don't free this structure even when the count goes to
+ 	 * zero. The structure is freed when we destroy the VM.
+ 	 */
+ 
+ 	mutex_unlock(&kvm->lock);
+ 	return 0;
+ }
+ 
+ static int kvmppc_irq_bypass_add_producer_hv(struct irq_bypass_consumer *cons,
+ 					     struct irq_bypass_producer *prod)
+ {
+ 	int ret = 0;
+ 	struct kvm_kernel_irqfd *irqfd =
+ 		container_of(cons, struct kvm_kernel_irqfd, consumer);
+ 
+ 	irqfd->producer = prod;
+ 
+ 	ret = kvmppc_set_passthru_irq(irqfd->kvm, prod->irq, irqfd->gsi);
+ 	if (ret)
+ 		pr_info("kvmppc_set_passthru_irq (irq %d, gsi %d) fails: %d\n",
+ 			prod->irq, irqfd->gsi, ret);
+ 
+ 	return ret;
+ }
+ 
+ static void kvmppc_irq_bypass_del_producer_hv(struct irq_bypass_consumer *cons,
+ 					      struct irq_bypass_producer *prod)
+ {
+ 	int ret;
+ 	struct kvm_kernel_irqfd *irqfd =
+ 		container_of(cons, struct kvm_kernel_irqfd, consumer);
+ 
+ 	irqfd->producer = NULL;
+ 
+ 	/*
+ 	 * When producer of consumer is unregistered, we change back to
+ 	 * default external interrupt handling mode - KVM real mode
+ 	 * will switch back to host.
+ 	 */
+ 	ret = kvmppc_clr_passthru_irq(irqfd->kvm, prod->irq, irqfd->gsi);
+ 	if (ret)
+ 		pr_warn("kvmppc_clr_passthru_irq (irq %d, gsi %d) fails: %d\n",
+ 			prod->irq, irqfd->gsi, ret);
+ }
+ #endif
+ 
++>>>>>>> e3c13e56a471 (KVM: PPC: Book3S HV: Handle passthrough interrupts in guest)
  static long kvm_arch_vm_ioctl_hv(struct file *filp,
  				 unsigned int ioctl, unsigned long arg)
  {
diff --cc arch/powerpc/kvm/book3s_hv_builtin.c
index 2b357edf45a7,7531e29f90bd..000000000000
--- a/arch/powerpc/kvm/book3s_hv_builtin.c
+++ b/arch/powerpc/kvm/book3s_hv_builtin.c
@@@ -265,3 -284,160 +265,163 @@@ void kvmhv_commence_exit(int trap
  			kvmhv_interrupt_vcore(vc, ee);
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ struct kvmppc_host_rm_ops *kvmppc_host_rm_ops_hv;
+ EXPORT_SYMBOL_GPL(kvmppc_host_rm_ops_hv);
+ 
+ #ifdef CONFIG_KVM_XICS
+ static struct kvmppc_irq_map *get_irqmap(struct kvmppc_passthru_irqmap *pimap,
+ 					 u32 xisr)
+ {
+ 	int i;
+ 
+ 	/*
+ 	 * We access the mapped array here without a lock.  That
+ 	 * is safe because we never reduce the number of entries
+ 	 * in the array and we never change the v_hwirq field of
+ 	 * an entry once it is set.
+ 	 *
+ 	 * We have also carefully ordered the stores in the writer
+ 	 * and the loads here in the reader, so that if we find a matching
+ 	 * hwirq here, the associated GSI and irq_desc fields are valid.
+ 	 */
+ 	for (i = 0; i < pimap->n_mapped; i++)  {
+ 		if (xisr == pimap->mapped[i].r_hwirq) {
+ 			/*
+ 			 * Order subsequent reads in the caller to serialize
+ 			 * with the writer.
+ 			 */
+ 			smp_rmb();
+ 			return &pimap->mapped[i];
+ 		}
+ 	}
+ 	return NULL;
+ }
+ 
+ /*
+  * If we have an interrupt that's not an IPI, check if we have a
+  * passthrough adapter and if so, check if this external interrupt
+  * is for the adapter.
+  * We will attempt to deliver the IRQ directly to the target VCPU's
+  * ICP, the virtual ICP (based on affinity - the xive value in ICS).
+  *
+  * If the delivery fails or if this is not for a passthrough adapter,
+  * return to the host to handle this interrupt. We earlier
+  * saved a copy of the XIRR in the PACA, it will be picked up by
+  * the host ICP driver.
+  */
+ static int kvmppc_check_passthru(u32 xisr, __be32 xirr)
+ {
+ 	struct kvmppc_passthru_irqmap *pimap;
+ 	struct kvmppc_irq_map *irq_map;
+ 	struct kvm_vcpu *vcpu;
+ 
+ 	vcpu = local_paca->kvm_hstate.kvm_vcpu;
+ 	if (!vcpu)
+ 		return 1;
+ 	pimap = kvmppc_get_passthru_irqmap(vcpu->kvm);
+ 	if (!pimap)
+ 		return 1;
+ 	irq_map = get_irqmap(pimap, xisr);
+ 	if (!irq_map)
+ 		return 1;
+ 
+ 	/* We're handling this interrupt, generic code doesn't need to */
+ 	local_paca->kvm_hstate.saved_xirr = 0;
+ 
+ 	return kvmppc_deliver_irq_passthru(vcpu, xirr, irq_map, pimap);
+ }
+ 
+ #else
+ static inline int kvmppc_check_passthru(u32 xisr, __be32 xirr)
+ {
+ 	return 1;
+ }
+ #endif
+ 
+ /*
+  * Determine what sort of external interrupt is pending (if any).
+  * Returns:
+  *	0 if no interrupt is pending
+  *	1 if an interrupt is pending that needs to be handled by the host
+  *	-1 if there was a guest wakeup IPI (which has now been cleared)
+  *	-2 if there is PCI passthrough external interrupt that was handled
+  */
+ 
+ long kvmppc_read_intr(void)
+ {
+ 	unsigned long xics_phys;
+ 	u32 h_xirr;
+ 	__be32 xirr;
+ 	u32 xisr;
+ 	u8 host_ipi;
+ 
+ 	/* see if a host IPI is pending */
+ 	host_ipi = local_paca->kvm_hstate.host_ipi;
+ 	if (host_ipi)
+ 		return 1;
+ 
+ 	/* Now read the interrupt from the ICP */
+ 	xics_phys = local_paca->kvm_hstate.xics_phys;
+ 	if (unlikely(!xics_phys))
+ 		return 1;
+ 
+ 	/*
+ 	 * Save XIRR for later. Since we get control in reverse endian
+ 	 * on LE systems, save it byte reversed and fetch it back in
+ 	 * host endian. Note that xirr is the value read from the
+ 	 * XIRR register, while h_xirr is the host endian version.
+ 	 */
+ 	xirr = _lwzcix(xics_phys + XICS_XIRR);
+ 	h_xirr = be32_to_cpu(xirr);
+ 	local_paca->kvm_hstate.saved_xirr = h_xirr;
+ 	xisr = h_xirr & 0xffffff;
+ 	/*
+ 	 * Ensure that the store/load complete to guarantee all side
+ 	 * effects of loading from XIRR has completed
+ 	 */
+ 	smp_mb();
+ 
+ 	/* if nothing pending in the ICP */
+ 	if (!xisr)
+ 		return 0;
+ 
+ 	/* We found something in the ICP...
+ 	 *
+ 	 * If it is an IPI, clear the MFRR and EOI it.
+ 	 */
+ 	if (xisr == XICS_IPI) {
+ 		_stbcix(xics_phys + XICS_MFRR, 0xff);
+ 		_stwcix(xics_phys + XICS_XIRR, xirr);
+ 		/*
+ 		 * Need to ensure side effects of above stores
+ 		 * complete before proceeding.
+ 		 */
+ 		smp_mb();
+ 
+ 		/*
+ 		 * We need to re-check host IPI now in case it got set in the
+ 		 * meantime. If it's clear, we bounce the interrupt to the
+ 		 * guest
+ 		 */
+ 		host_ipi = local_paca->kvm_hstate.host_ipi;
+ 		if (unlikely(host_ipi != 0)) {
+ 			/* We raced with the host,
+ 			 * we need to resend that IPI, bummer
+ 			 */
+ 			_stbcix(xics_phys + XICS_MFRR, IPI_PRIORITY);
+ 			/* Let side effects complete */
+ 			smp_mb();
+ 			return 1;
+ 		}
+ 
+ 		/* OK, it's an IPI for us */
+ 		local_paca->kvm_hstate.saved_xirr = 0;
+ 		return -1;
+ 	}
+ 
+ 	return kvmppc_check_passthru(xisr, xirr);
+ }
++>>>>>>> e3c13e56a471 (KVM: PPC: Book3S HV: Handle passthrough interrupts in guest)
diff --cc arch/powerpc/kvm/book3s_hv_rm_xics.c
index 37eb41dc50c9,17f5b851db8c..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rm_xics.c
+++ b/arch/powerpc/kvm/book3s_hv_rm_xics.c
@@@ -17,7 -17,9 +17,8 @@@
  #include <asm/xics.h>
  #include <asm/debug.h>
  #include <asm/synch.h>
 -#include <asm/cputhreads.h>
  #include <asm/ppc-opcode.h>
+ #include <asm/pnv-pci.h>
  
  #include "book3s_xics.h"
  
@@@ -625,3 -712,83 +626,86 @@@ int kvmppc_rm_h_eoi(struct kvm_vcpu *vc
   bail:
  	return check_too_hard(xics, icp);
  }
++<<<<<<< HEAD
++=======
+ 
+ unsigned long eoi_rc;
+ 
+ static void icp_eoi(struct irq_chip *c, u32 hwirq, u32 xirr)
+ {
+ 	unsigned long xics_phys;
+ 	int64_t rc;
+ 
+ 	rc = pnv_opal_pci_msi_eoi(c, hwirq);
+ 
+ 	if (rc)
+ 		eoi_rc = rc;
+ 
+ 	iosync();
+ 
+ 	/* EOI it */
+ 	xics_phys = local_paca->kvm_hstate.xics_phys;
+ 	_stwcix(xics_phys + XICS_XIRR, xirr);
+ }
+ 
+ long kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu,
+ 				 u32 xirr,
+ 				 struct kvmppc_irq_map *irq_map,
+ 				 struct kvmppc_passthru_irqmap *pimap)
+ {
+ 	struct kvmppc_xics *xics;
+ 	struct kvmppc_icp *icp;
+ 	u32 irq;
+ 
+ 	irq = irq_map->v_hwirq;
+ 	xics = vcpu->kvm->arch.xics;
+ 	icp = vcpu->arch.icp;
+ 
+ 	icp_rm_deliver_irq(xics, icp, irq);
+ 
+ 	/* EOI the interrupt */
+ 	icp_eoi(irq_desc_get_chip(irq_map->desc), irq_map->r_hwirq, xirr);
+ 
+ 	if (check_too_hard(xics, icp) == H_TOO_HARD)
+ 		return 1;
+ 	else
+ 		return -2;
+ }
+ 
+ /*  --- Non-real mode XICS-related built-in routines ---  */
+ 
+ /**
+  * Host Operations poked by RM KVM
+  */
+ static void rm_host_ipi_action(int action, void *data)
+ {
+ 	switch (action) {
+ 	case XICS_RM_KICK_VCPU:
+ 		kvmppc_host_rm_ops_hv->vcpu_kick(data);
+ 		break;
+ 	default:
+ 		WARN(1, "Unexpected rm_action=%d data=%p\n", action, data);
+ 		break;
+ 	}
+ 
+ }
+ 
+ void kvmppc_xics_ipi_action(void)
+ {
+ 	int core;
+ 	unsigned int cpu = smp_processor_id();
+ 	struct kvmppc_host_rm_core *rm_corep;
+ 
+ 	core = cpu >> threads_shift;
+ 	rm_corep = &kvmppc_host_rm_ops_hv->rm_core[core];
+ 
+ 	if (rm_corep->rm_data) {
+ 		rm_host_ipi_action(rm_corep->rm_state.rm_action,
+ 							rm_corep->rm_data);
+ 		/* Order these stores against the real mode KVM */
+ 		rm_corep->rm_data = NULL;
+ 		smp_wmb();
+ 		rm_corep->rm_state.rm_action = 0;
+ 	}
+ }
++>>>>>>> e3c13e56a471 (KVM: PPC: Book3S HV: Handle passthrough interrupts in guest)
diff --cc arch/powerpc/kvm/book3s_hv_rmhandlers.S
index a4309327567b,12fb2afb23fc..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S
+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S
@@@ -1121,6 -1174,36 +1121,39 @@@ END_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR
  	 * set, we know the host wants us out so let's do it now
  	 */
  	bl	kvmppc_read_intr
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Restore the active volatile registers after returning from
+ 	 * a C function.
+ 	 */
+ 	ld	r9, HSTATE_KVM_VCPU(r13)
+ 	li	r12, BOOK3S_INTERRUPT_EXTERNAL
+ 
+ 	/*
+ 	 * kvmppc_read_intr return codes:
+ 	 *
+ 	 * Exit to host (r3 > 0)
+ 	 *   1 An interrupt is pending that needs to be handled by the host
+ 	 *     Exit guest and return to host by branching to guest_exit_cont
+ 	 *
+ 	 * Before returning to guest, we check if any CPU is heading out
+ 	 * to the host and if so, we head out also. If no CPUs are heading
+ 	 * check return values <= 0.
+ 	 *
+ 	 * Return to guest (r3 <= 0)
+ 	 *  0 No external interrupt is pending
+ 	 * -1 A guest wakeup IPI (which has now been cleared)
+ 	 *    In either case, we return to guest to deliver any pending
+ 	 *    guest interrupts.
+ 	 *
+ 	 * -2 A PCI passthrough external interrupt was handled
+ 	 *    (interrupt was delivered directly to guest)
+ 	 *    Return to guest to deliver any pending guest interrupts.
+ 	 */
+ 
++>>>>>>> e3c13e56a471 (KVM: PPC: Book3S HV: Handle passthrough interrupts in guest)
  	cmpdi	r3, 0
  	bgt	guest_exit_cont
  
* Unmerged path arch/powerpc/include/asm/kvm_ppc.h
* Unmerged path arch/powerpc/kvm/book3s_hv.c
* Unmerged path arch/powerpc/kvm/book3s_hv_builtin.c
* Unmerged path arch/powerpc/kvm/book3s_hv_rm_xics.c
* Unmerged path arch/powerpc/kvm/book3s_hv_rmhandlers.S
