crypto: ccp - Add abstraction for device-specific calls

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [crypto] ccp - Add abstraction for device-specific calls (Suravee Suthikulpanit) [1390820]
Rebuild_FUZZ: 92.16%
commit-author Gary R Hook <gary.hook@amd.com>
commit ea0375afa17281e9e0190034215d0404dbad7449
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ea0375af.failed

Support for different generations of the coprocessor
requires that an abstraction layer be implemented for
interacting with the hardware. This patch splits out
version-specific functions to a separate file and populates
the version structure (acting as a driver) with function
pointers.

	Signed-off-by: Gary R Hook <gary.hook@amd.com>
	Acked-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit ea0375afa17281e9e0190034215d0404dbad7449)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/ccp/Makefile
#	drivers/crypto/ccp/ccp-dev.c
#	drivers/crypto/ccp/ccp-dev.h
#	drivers/crypto/ccp/ccp-ops.c
#	drivers/crypto/ccp/ccp-pci.c
#	drivers/crypto/ccp/ccp-platform.c
diff --cc drivers/crypto/ccp/ccp-dev.c
index 2777dc97b570,336e5b780fcb..000000000000
--- a/drivers/crypto/ccp/ccp-dev.c
+++ b/drivers/crypto/ccp/ccp-dev.c
@@@ -35,23 -39,150 +35,50 @@@ struct ccp_tasklet_data 
  	struct ccp_cmd *cmd;
  };
  
 -/* List of CCPs, CCP count, read-write access lock, and access functions
 - *
 - * Lock structure: get ccp_unit_lock for reading whenever we need to
 - * examine the CCP list. While holding it for reading we can acquire
 - * the RR lock to update the round-robin next-CCP pointer. The unit lock
 - * must be acquired before the RR lock.
 - *
 - * If the unit-lock is acquired for writing, we have total control over
 - * the list, so there's no value in getting the RR lock.
 - */
 -static DEFINE_RWLOCK(ccp_unit_lock);
 -static LIST_HEAD(ccp_units);
 -
 -/* Round-robin counter */
 -static DEFINE_RWLOCK(ccp_rr_lock);
 -static struct ccp_device *ccp_rr;
  
 -/* Ever-increasing value to produce unique unit numbers */
 -static atomic_t ccp_unit_ordinal;
 -unsigned int ccp_increment_unit_ordinal(void)
 +static struct ccp_device *ccp_dev;
 +static inline struct ccp_device *ccp_get_device(void)
  {
 -	return atomic_inc_return(&ccp_unit_ordinal);
 +	return ccp_dev;
  }
  
++<<<<<<< HEAD
 +static inline void ccp_add_device(struct ccp_device *ccp)
++=======
+ /**
+  * ccp_add_device - add a CCP device to the list
+  *
+  * @ccp: ccp_device struct pointer
+  *
+  * Put this CCP on the unit list, which makes it available
+  * for use.
+  *
+  * Returns zero if a CCP device is present, -ENODEV otherwise.
+  */
+ void ccp_add_device(struct ccp_device *ccp)
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  {
 -	unsigned long flags;
 -
 -	write_lock_irqsave(&ccp_unit_lock, flags);
 -	list_add_tail(&ccp->entry, &ccp_units);
 -	if (!ccp_rr)
 -		/* We already have the list lock (we're first) so this
 -		 * pointer can't change on us. Set its initial value.
 -		 */
 -		ccp_rr = ccp;
 -	write_unlock_irqrestore(&ccp_unit_lock, flags);
 +	ccp_dev = ccp;
  }
  
++<<<<<<< HEAD
 +static inline void ccp_del_device(struct ccp_device *ccp)
++=======
+ /**
+  * ccp_del_device - remove a CCP device from the list
+  *
+  * @ccp: ccp_device struct pointer
+  *
+  * Remove this unit from the list of devices. If the next device
+  * up for use is this one, adjust the pointer. If this is the last
+  * device, NULL the pointer.
+  */
+ void ccp_del_device(struct ccp_device *ccp)
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  {
 -	unsigned long flags;
 -
 -	write_lock_irqsave(&ccp_unit_lock, flags);
 -	if (ccp_rr == ccp) {
 -		/* ccp_unit_lock is read/write; any read access
 -		 * will be suspended while we make changes to the
 -		 * list and RR pointer.
 -		 */
 -		if (list_is_last(&ccp_rr->entry, &ccp_units))
 -			ccp_rr = list_first_entry(&ccp_units, struct ccp_device,
 -						  entry);
 -		else
 -			ccp_rr = list_next_entry(ccp_rr, entry);
 -	}
 -	list_del(&ccp->entry);
 -	if (list_empty(&ccp_units))
 -		ccp_rr = NULL;
 -	write_unlock_irqrestore(&ccp_unit_lock, flags);
 -}
 -
 -static struct ccp_device *ccp_get_device(void)
 -{
 -	unsigned long flags;
 -	struct ccp_device *dp = NULL;
 -
 -	/* We round-robin through the unit list.
 -	 * The (ccp_rr) pointer refers to the next unit to use.
 -	 */
 -	read_lock_irqsave(&ccp_unit_lock, flags);
 -	if (!list_empty(&ccp_units)) {
 -		write_lock_irqsave(&ccp_rr_lock, flags);
 -		dp = ccp_rr;
 -		if (list_is_last(&ccp_rr->entry, &ccp_units))
 -			ccp_rr = list_first_entry(&ccp_units, struct ccp_device,
 -						  entry);
 -		else
 -			ccp_rr = list_next_entry(ccp_rr, entry);
 -		write_unlock_irqrestore(&ccp_rr_lock, flags);
 -	}
 -	read_unlock_irqrestore(&ccp_unit_lock, flags);
 -
 -	return dp;
 +	ccp_dev = NULL;
  }
  
 -/**
 - * ccp_present - check if a CCP device is present
 - *
 - * Returns zero if a CCP device is present, -ENODEV otherwise.
 - */
 -int ccp_present(void)
 -{
 -	unsigned long flags;
 -	int ret;
 -
 -	read_lock_irqsave(&ccp_unit_lock, flags);
 -	ret = list_empty(&ccp_units);
 -	read_unlock_irqrestore(&ccp_unit_lock, flags);
 -
 -	return ret ? -ENODEV : 0;
 -}
 -EXPORT_SYMBOL_GPL(ccp_present);
 -
 -/**
 - * ccp_version - get the version of the CCP device
 - *
 - * Returns the version from the first unit on the list;
 - * otherwise a zero if no CCP device is present
 - */
 -unsigned int ccp_version(void)
 -{
 -	struct ccp_device *dp;
 -	unsigned long flags;
 -	int ret = 0;
 -
 -	read_lock_irqsave(&ccp_unit_lock, flags);
 -	if (!list_empty(&ccp_units)) {
 -		dp = list_first_entry(&ccp_units, struct ccp_device, entry);
 -		ret = dp->vdata->version;
 -	}
 -	read_unlock_irqrestore(&ccp_unit_lock, flags);
 -
 -	return ret;
 -}
 -EXPORT_SYMBOL_GPL(ccp_version);
 -
  /**
   * ccp_enqueue_cmd - queue an operation for processing by the CCP
   *
@@@ -299,246 -404,10 +302,249 @@@ struct ccp_device *ccp_alloc_struct(str
  	return ccp;
  }
  
++<<<<<<< HEAD
 +/**
 + * ccp_init - initialize the CCP device
 + *
 + * @ccp: ccp_device struct
 + */
 +int ccp_init(struct ccp_device *ccp)
 +{
 +	struct device *dev = ccp->dev;
 +	struct ccp_cmd_queue *cmd_q;
 +	struct dma_pool *dma_pool;
 +	char dma_pool_name[MAX_DMAPOOL_NAME_LEN];
 +	unsigned int qmr, qim, i;
 +	int ret;
 +
 +	/* Find available queues */
 +	qim = 0;
 +	qmr = ioread32(ccp->io_regs + Q_MASK_REG);
 +	for (i = 0; i < MAX_HW_QUEUES; i++) {
 +		if (!(qmr & (1 << i)))
 +			continue;
 +
 +		/* Allocate a dma pool for this queue */
 +		snprintf(dma_pool_name, sizeof(dma_pool_name), "ccp_q%d", i);
 +		dma_pool = dma_pool_create(dma_pool_name, dev,
 +					   CCP_DMAPOOL_MAX_SIZE,
 +					   CCP_DMAPOOL_ALIGN, 0);
 +		if (!dma_pool) {
 +			dev_err(dev, "unable to allocate dma pool\n");
 +			ret = -ENOMEM;
 +			goto e_pool;
 +		}
 +
 +		cmd_q = &ccp->cmd_q[ccp->cmd_q_count];
 +		ccp->cmd_q_count++;
 +
 +		cmd_q->ccp = ccp;
 +		cmd_q->id = i;
 +		cmd_q->dma_pool = dma_pool;
 +
 +		/* Reserve 2 KSB regions for the queue */
 +		cmd_q->ksb_key = KSB_START + ccp->ksb_start++;
 +		cmd_q->ksb_ctx = KSB_START + ccp->ksb_start++;
 +		ccp->ksb_count -= 2;
 +
 +		/* Preset some register values and masks that are queue
 +		 * number dependent
 +		 */
 +		cmd_q->reg_status = ccp->io_regs + CMD_Q_STATUS_BASE +
 +				    (CMD_Q_STATUS_INCR * i);
 +		cmd_q->reg_int_status = ccp->io_regs + CMD_Q_INT_STATUS_BASE +
 +					(CMD_Q_STATUS_INCR * i);
 +		cmd_q->int_ok = 1 << (i * 2);
 +		cmd_q->int_err = 1 << ((i * 2) + 1);
 +
 +		cmd_q->free_slots = CMD_Q_DEPTH(ioread32(cmd_q->reg_status));
 +
 +		init_waitqueue_head(&cmd_q->int_queue);
 +
 +		/* Build queue interrupt mask (two interrupts per queue) */
 +		qim |= cmd_q->int_ok | cmd_q->int_err;
 +
 +		dev_dbg(dev, "queue #%u available\n", i);
 +	}
 +	if (ccp->cmd_q_count == 0) {
 +		dev_notice(dev, "no command queues available\n");
 +		ret = -EIO;
 +		goto e_pool;
 +	}
 +	dev_notice(dev, "%u command queues available\n", ccp->cmd_q_count);
 +
 +	/* Disable and clear interrupts until ready */
 +	iowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		ioread32(cmd_q->reg_int_status);
 +		ioread32(cmd_q->reg_status);
 +	}
 +	iowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);
 +
 +	/* Request an irq */
 +	ret = ccp->get_irq(ccp);
 +	if (ret) {
 +		dev_err(dev, "unable to allocate an IRQ\n");
 +		goto e_pool;
 +	}
 +
 +	/* Initialize the queues used to wait for KSB space and suspend */
 +	init_waitqueue_head(&ccp->ksb_queue);
 +	init_waitqueue_head(&ccp->suspend_queue);
 +
 +	/* Create a kthread for each queue */
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		struct task_struct *kthread;
 +
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		kthread = kthread_create(ccp_cmd_queue_thread, cmd_q,
 +					 "ccp-q%u", cmd_q->id);
 +		if (IS_ERR(kthread)) {
 +			dev_err(dev, "error creating queue thread (%ld)\n",
 +				PTR_ERR(kthread));
 +			ret = PTR_ERR(kthread);
 +			goto e_kthread;
 +		}
 +
 +		cmd_q->kthread = kthread;
 +		wake_up_process(kthread);
 +	}
 +
 +	/* Register the RNG */
 +	ccp->hwrng.name = "ccp-rng";
 +	ccp->hwrng.read = ccp_trng_read;
 +	ret = hwrng_register(&ccp->hwrng);
 +	if (ret) {
 +		dev_err(dev, "error registering hwrng (%d)\n", ret);
 +		goto e_kthread;
 +	}
 +
 +	/* Make the device struct available before enabling interrupts */
 +	ccp_add_device(ccp);
 +
 +	/* Enable interrupts */
 +	iowrite32(qim, ccp->io_regs + IRQ_MASK_REG);
 +
 +	return 0;
 +
 +e_kthread:
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		if (ccp->cmd_q[i].kthread)
 +			kthread_stop(ccp->cmd_q[i].kthread);
 +
 +	ccp->free_irq(ccp);
 +
 +e_pool:
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		dma_pool_destroy(ccp->cmd_q[i].dma_pool);
 +
 +	return ret;
 +}
 +
 +/**
 + * ccp_destroy - tear down the CCP device
 + *
 + * @ccp: ccp_device struct
 + */
 +void ccp_destroy(struct ccp_device *ccp)
 +{
 +	struct ccp_cmd_queue *cmd_q;
 +	struct ccp_cmd *cmd;
 +	unsigned int qim, i;
 +
 +	/* Remove general access to the device struct */
 +	ccp_del_device(ccp);
 +
 +	/* Unregister the RNG */
 +	hwrng_unregister(&ccp->hwrng);
 +
 +	/* Stop the queue kthreads */
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		if (ccp->cmd_q[i].kthread)
 +			kthread_stop(ccp->cmd_q[i].kthread);
 +
 +	/* Build queue interrupt mask (two interrupt masks per queue) */
 +	qim = 0;
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +		qim |= cmd_q->int_ok | cmd_q->int_err;
 +	}
 +
 +	/* Disable and clear interrupts */
 +	iowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		ioread32(cmd_q->reg_int_status);
 +		ioread32(cmd_q->reg_status);
 +	}
 +	iowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);
 +
 +	ccp->free_irq(ccp);
 +
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		dma_pool_destroy(ccp->cmd_q[i].dma_pool);
 +
 +	/* Flush the cmd and backlog queue */
 +	while (!list_empty(&ccp->cmd)) {
 +		/* Invoke the callback directly with an error code */
 +		cmd = list_first_entry(&ccp->cmd, struct ccp_cmd, entry);
 +		list_del(&cmd->entry);
 +		cmd->callback(cmd->data, -ENODEV);
 +	}
 +	while (!list_empty(&ccp->backlog)) {
 +		/* Invoke the callback directly with an error code */
 +		cmd = list_first_entry(&ccp->backlog, struct ccp_cmd, entry);
 +		list_del(&cmd->entry);
 +		cmd->callback(cmd->data, -ENODEV);
 +	}
 +}
 +
 +/**
 + * ccp_irq_handler - handle interrupts generated by the CCP device
 + *
 + * @irq: the irq associated with the interrupt
 + * @data: the data value supplied when the irq was created
 + */
 +irqreturn_t ccp_irq_handler(int irq, void *data)
 +{
 +	struct device *dev = data;
 +	struct ccp_device *ccp = dev_get_drvdata(dev);
 +	struct ccp_cmd_queue *cmd_q;
 +	u32 q_int, status;
 +	unsigned int i;
 +
 +	status = ioread32(ccp->io_regs + IRQ_STATUS_REG);
 +
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		q_int = status & (cmd_q->int_ok | cmd_q->int_err);
 +		if (q_int) {
 +			cmd_q->int_status = status;
 +			cmd_q->q_status = ioread32(cmd_q->reg_status);
 +			cmd_q->q_int_status = ioread32(cmd_q->reg_int_status);
 +
 +			/* On error, only save the first error value */
 +			if ((q_int & cmd_q->int_err) && !cmd_q->cmd_error)
 +				cmd_q->cmd_error = CMD_Q_ERROR(cmd_q->q_status);
 +
 +			cmd_q->int_rcvd = 1;
 +
 +			/* Acknowledge the interrupt and wake the kthread */
 +			iowrite32(q_int, ccp->io_regs + IRQ_STATUS_REG);
 +			wake_up_interruptible(&cmd_q->int_queue);
 +		}
 +	}
 +
 +	return IRQ_HANDLED;
 +}
 +
++=======
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  #ifdef CONFIG_PM
  bool ccp_queues_suspended(struct ccp_device *ccp)
  {
@@@ -558,38 -427,39 +564,41 @@@
  }
  #endif
  
++<<<<<<< HEAD
 +static const struct x86_cpu_id ccp_support[] = {
 +	{ X86_VENDOR_AMD, 22, },
 +	{ },
 +};
 +
++=======
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  static int __init ccp_mod_init(void)
  {
 -#ifdef CONFIG_X86
 +	struct cpuinfo_x86 *cpuinfo = &boot_cpu_data;
  	int ret;
  
 -	ret = ccp_pci_init();
 -	if (ret)
 -		return ret;
 -
 -	/* Don't leave the driver loaded if init failed */
 -	if (ccp_present() != 0) {
 -		ccp_pci_exit();
 +	if (!x86_match_cpu(ccp_support))
  		return -ENODEV;
 -	}
  
 -	return 0;
 -#endif
 +	switch (cpuinfo->x86) {
 +	case 22:
 +		if ((cpuinfo->x86_model < 48) || (cpuinfo->x86_model > 63))
 +			return -ENODEV;
  
 -#ifdef CONFIG_ARM64
 -	int ret;
 +		ret = ccp_pci_init();
 +		if (ret)
 +			return ret;
  
 -	ret = ccp_platform_init();
 -	if (ret)
 -		return ret;
 +		/* Don't leave the driver loaded if init failed */
 +		if (!ccp_get_device()) {
 +			ccp_pci_exit();
 +			return -ENODEV;
 +		}
  
 -	/* Don't leave the driver loaded if init failed */
 -	if (ccp_present() != 0) {
 -		ccp_platform_exit();
 -		return -ENODEV;
 -	}
 +		return 0;
  
 -	return 0;
 -#endif
 +		break;
 +	};
  
  	return -ENODEV;
  }
diff --cc drivers/crypto/ccp/ccp-dev.h
index 72bf1536b653,7745d0be491d..000000000000
--- a/drivers/crypto/ccp/ccp-dev.h
+++ b/drivers/crypto/ccp/ccp-dev.h
@@@ -139,6 -141,28 +139,31 @@@
  #define CCP_ECC_RESULT_OFFSET		60
  #define CCP_ECC_RESULT_SUCCESS		0x0001
  
++<<<<<<< HEAD
++=======
+ struct ccp_op;
+ 
+ /* Structure for computation functions that are device-specific */
+ struct ccp_actions {
+ 	int (*perform_aes)(struct ccp_op *);
+ 	int (*perform_xts_aes)(struct ccp_op *);
+ 	int (*perform_sha)(struct ccp_op *);
+ 	int (*perform_rsa)(struct ccp_op *);
+ 	int (*perform_passthru)(struct ccp_op *);
+ 	int (*perform_ecc)(struct ccp_op *);
+ 	int (*init)(struct ccp_device *);
+ 	void (*destroy)(struct ccp_device *);
+ 	irqreturn_t (*irqhandler)(int, void *);
+ };
+ 
+ /* Structure to hold CCP version-specific values */
+ struct ccp_vdata {
+ 	unsigned int version;
+ 	struct ccp_actions *perform;
+ };
+ 
+ extern struct ccp_vdata ccpv3;
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  
  struct ccp_device;
  struct ccp_cmd;
@@@ -253,18 -284,137 +278,144 @@@ struct ccp_device 
  	/* Suspend support */
  	unsigned int suspending;
  	wait_queue_head_t suspend_queue;
 -
 -	/* DMA caching attribute support */
 -	unsigned int axcache;
  };
  
++<<<<<<< HEAD
++=======
+ enum ccp_memtype {
+ 	CCP_MEMTYPE_SYSTEM = 0,
+ 	CCP_MEMTYPE_KSB,
+ 	CCP_MEMTYPE_LOCAL,
+ 	CCP_MEMTYPE__LAST,
+ };
+ 
+ struct ccp_dma_info {
+ 	dma_addr_t address;
+ 	unsigned int offset;
+ 	unsigned int length;
+ 	enum dma_data_direction dir;
+ };
+ 
+ struct ccp_dm_workarea {
+ 	struct device *dev;
+ 	struct dma_pool *dma_pool;
+ 	unsigned int length;
+ 
+ 	u8 *address;
+ 	struct ccp_dma_info dma;
+ };
+ 
+ struct ccp_sg_workarea {
+ 	struct scatterlist *sg;
+ 	int nents;
+ 
+ 	struct scatterlist *dma_sg;
+ 	struct device *dma_dev;
+ 	unsigned int dma_count;
+ 	enum dma_data_direction dma_dir;
+ 
+ 	unsigned int sg_used;
+ 
+ 	u64 bytes_left;
+ };
+ 
+ struct ccp_data {
+ 	struct ccp_sg_workarea sg_wa;
+ 	struct ccp_dm_workarea dm_wa;
+ };
+ 
+ struct ccp_mem {
+ 	enum ccp_memtype type;
+ 	union {
+ 		struct ccp_dma_info dma;
+ 		u32 ksb;
+ 	} u;
+ };
+ 
+ struct ccp_aes_op {
+ 	enum ccp_aes_type type;
+ 	enum ccp_aes_mode mode;
+ 	enum ccp_aes_action action;
+ };
+ 
+ struct ccp_xts_aes_op {
+ 	enum ccp_aes_action action;
+ 	enum ccp_xts_aes_unit_size unit_size;
+ };
+ 
+ struct ccp_sha_op {
+ 	enum ccp_sha_type type;
+ 	u64 msg_bits;
+ };
+ 
+ struct ccp_rsa_op {
+ 	u32 mod_size;
+ 	u32 input_len;
+ };
+ 
+ struct ccp_passthru_op {
+ 	enum ccp_passthru_bitwise bit_mod;
+ 	enum ccp_passthru_byteswap byte_swap;
+ };
+ 
+ struct ccp_ecc_op {
+ 	enum ccp_ecc_function function;
+ };
+ 
+ struct ccp_op {
+ 	struct ccp_cmd_queue *cmd_q;
+ 
+ 	u32 jobid;
+ 	u32 ioc;
+ 	u32 soc;
+ 	u32 ksb_key;
+ 	u32 ksb_ctx;
+ 	u32 init;
+ 	u32 eom;
+ 
+ 	struct ccp_mem src;
+ 	struct ccp_mem dst;
+ 
+ 	union {
+ 		struct ccp_aes_op aes;
+ 		struct ccp_xts_aes_op xts;
+ 		struct ccp_sha_op sha;
+ 		struct ccp_rsa_op rsa;
+ 		struct ccp_passthru_op passthru;
+ 		struct ccp_ecc_op ecc;
+ 	} u;
+ };
+ 
+ static inline u32 ccp_addr_lo(struct ccp_dma_info *info)
+ {
+ 	return lower_32_bits(info->address + info->offset);
+ }
+ 
+ static inline u32 ccp_addr_hi(struct ccp_dma_info *info)
+ {
+ 	return upper_32_bits(info->address + info->offset) & 0x0000ffff;
+ }
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  
  int ccp_pci_init(void);
  void ccp_pci_exit(void);
  
++<<<<<<< HEAD
 +struct ccp_device *ccp_alloc_struct(struct device *dev);
 +int ccp_init(struct ccp_device *ccp);
 +void ccp_destroy(struct ccp_device *ccp);
 +bool ccp_queues_suspended(struct ccp_device *ccp);
++=======
+ int ccp_platform_init(void);
+ void ccp_platform_exit(void);
+ 
+ void ccp_add_device(struct ccp_device *ccp);
+ void ccp_del_device(struct ccp_device *ccp);
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  
- irqreturn_t ccp_irq_handler(int irq, void *data);
+ struct ccp_device *ccp_alloc_struct(struct device *dev);
+ bool ccp_queues_suspended(struct ccp_device *ccp);
+ int ccp_cmd_queue_thread(void *data);
  
  int ccp_run_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd);
  
diff --cc drivers/crypto/ccp/ccp-ops.c
index 23dbb41465d1,eefdf595f758..000000000000
--- a/drivers/crypto/ccp/ccp-ops.c
+++ b/drivers/crypto/ccp/ccp-ops.c
@@@ -13,397 -13,33 +13,154 @@@
  #include <linux/module.h>
  #include <linux/kernel.h>
  #include <linux/pci.h>
- #include <linux/pci_ids.h>
- #include <linux/kthread.h>
- #include <linux/sched.h>
  #include <linux/interrupt.h>
- #include <linux/spinlock.h>
- #include <linux/mutex.h>
- #include <linux/delay.h>
- #include <linux/ccp.h>
- #include <linux/scatterlist.h>
  #include <crypto/scatterwalk.h>
++<<<<<<< HEAD
 +
 +#include "ccp-dev.h"
 +
 +
 +enum ccp_memtype {
 +	CCP_MEMTYPE_SYSTEM = 0,
 +	CCP_MEMTYPE_KSB,
 +	CCP_MEMTYPE_LOCAL,
 +	CCP_MEMTYPE__LAST,
 +};
 +
 +struct ccp_dma_info {
 +	dma_addr_t address;
 +	unsigned int offset;
 +	unsigned int length;
 +	enum dma_data_direction dir;
 +};
 +
 +struct ccp_dm_workarea {
 +	struct device *dev;
 +	struct dma_pool *dma_pool;
 +	unsigned int length;
 +
 +	u8 *address;
 +	struct ccp_dma_info dma;
 +};
 +
 +struct ccp_sg_workarea {
 +	struct scatterlist *sg;
 +	int nents;
 +
 +	struct scatterlist *dma_sg;
 +	struct device *dma_dev;
 +	unsigned int dma_count;
 +	enum dma_data_direction dma_dir;
 +
 +	u32 sg_used;
 +
 +	u32 bytes_left;
 +};
 +
 +struct ccp_data {
 +	struct ccp_sg_workarea sg_wa;
 +	struct ccp_dm_workarea dm_wa;
 +};
 +
 +struct ccp_mem {
 +	enum ccp_memtype type;
 +	union {
 +		struct ccp_dma_info dma;
 +		u32 ksb;
 +	} u;
 +};
 +
 +struct ccp_aes_op {
 +	enum ccp_aes_type type;
 +	enum ccp_aes_mode mode;
 +	enum ccp_aes_action action;
 +};
 +
 +struct ccp_xts_aes_op {
 +	enum ccp_aes_action action;
 +	enum ccp_xts_aes_unit_size unit_size;
 +};
 +
 +struct ccp_sha_op {
 +	enum ccp_sha_type type;
 +	u64 msg_bits;
 +};
 +
 +struct ccp_rsa_op {
 +	u32 mod_size;
 +	u32 input_len;
 +};
 +
 +struct ccp_passthru_op {
 +	enum ccp_passthru_bitwise bit_mod;
 +	enum ccp_passthru_byteswap byte_swap;
 +};
 +
 +struct ccp_ecc_op {
 +	enum ccp_ecc_function function;
 +};
 +
 +struct ccp_op {
 +	struct ccp_cmd_queue *cmd_q;
 +
 +	u32 jobid;
 +	u32 ioc;
 +	u32 soc;
 +	u32 ksb_key;
 +	u32 ksb_ctx;
 +	u32 init;
 +	u32 eom;
 +
 +	struct ccp_mem src;
 +	struct ccp_mem dst;
 +
 +	union {
 +		struct ccp_aes_op aes;
 +		struct ccp_xts_aes_op xts;
 +		struct ccp_sha_op sha;
 +		struct ccp_rsa_op rsa;
 +		struct ccp_passthru_op passthru;
 +		struct ccp_ecc_op ecc;
 +	} u;
 +};
 +
 +/* The CCP cannot perform zero-length sha operations so the caller
 + * is required to buffer data for the final operation.  However, a
 + * sha operation for a message with a total length of zero is valid
 + * so known values are required to supply the result.
 + */
 +static const u8 ccp_sha1_zero[CCP_SHA_CTXSIZE] = {
 +	0xda, 0x39, 0xa3, 0xee, 0x5e, 0x6b, 0x4b, 0x0d,
 +	0x32, 0x55, 0xbf, 0xef, 0x95, 0x60, 0x18, 0x90,
 +	0xaf, 0xd8, 0x07, 0x09, 0x00, 0x00, 0x00, 0x00,
 +	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
++=======
+ #include <linux/ccp.h>
+ 
+ #include "ccp-dev.h"
+ 
+ /* SHA initial context values */
+ static const __be32 ccp_sha1_init[CCP_SHA_CTXSIZE / sizeof(__be32)] = {
+ 	cpu_to_be32(SHA1_H0), cpu_to_be32(SHA1_H1),
+ 	cpu_to_be32(SHA1_H2), cpu_to_be32(SHA1_H3),
+ 	cpu_to_be32(SHA1_H4), 0, 0, 0,
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  };
  
 -static const __be32 ccp_sha224_init[CCP_SHA_CTXSIZE / sizeof(__be32)] = {
 -	cpu_to_be32(SHA224_H0), cpu_to_be32(SHA224_H1),
 -	cpu_to_be32(SHA224_H2), cpu_to_be32(SHA224_H3),
 -	cpu_to_be32(SHA224_H4), cpu_to_be32(SHA224_H5),
 -	cpu_to_be32(SHA224_H6), cpu_to_be32(SHA224_H7),
 +static const u8 ccp_sha224_zero[CCP_SHA_CTXSIZE] = {
 +	0xd1, 0x4a, 0x02, 0x8c, 0x2a, 0x3a, 0x2b, 0xc9,
 +	0x47, 0x61, 0x02, 0xbb, 0x28, 0x82, 0x34, 0xc4,
 +	0x15, 0xa2, 0xb0, 0x1f, 0x82, 0x8e, 0xa6, 0x2a,
 +	0xc5, 0xb3, 0xe4, 0x2f, 0x00, 0x00, 0x00, 0x00,
  };
  
 -static const __be32 ccp_sha256_init[CCP_SHA_CTXSIZE / sizeof(__be32)] = {
 -	cpu_to_be32(SHA256_H0), cpu_to_be32(SHA256_H1),
 -	cpu_to_be32(SHA256_H2), cpu_to_be32(SHA256_H3),
 -	cpu_to_be32(SHA256_H4), cpu_to_be32(SHA256_H5),
 -	cpu_to_be32(SHA256_H6), cpu_to_be32(SHA256_H7),
 +static const u8 ccp_sha256_zero[CCP_SHA_CTXSIZE] = {
 +	0xe3, 0xb0, 0xc4, 0x42, 0x98, 0xfc, 0x1c, 0x14,
 +	0x9a, 0xfb, 0xf4, 0xc8, 0x99, 0x6f, 0xb9, 0x24,
 +	0x27, 0xae, 0x41, 0xe4, 0x64, 0x9b, 0x93, 0x4c,
 +	0xa4, 0x95, 0x99, 0x1b, 0x78, 0x52, 0xb8, 0x55,
  };
  
- static u32 ccp_addr_lo(struct ccp_dma_info *info)
- {
- 	return lower_32_bits(info->address + info->offset);
- }
- 
- static u32 ccp_addr_hi(struct ccp_dma_info *info)
- {
- 	return upper_32_bits(info->address + info->offset) & 0x0000ffff;
- }
- 
- static int ccp_do_cmd(struct ccp_op *op, u32 *cr, unsigned int cr_count)
- {
- 	struct ccp_cmd_queue *cmd_q = op->cmd_q;
- 	struct ccp_device *ccp = cmd_q->ccp;
- 	void __iomem *cr_addr;
- 	u32 cr0, cmd;
- 	unsigned int i;
- 	int ret = 0;
- 
- 	/* We could read a status register to see how many free slots
- 	 * are actually available, but reading that register resets it
- 	 * and you could lose some error information.
- 	 */
- 	cmd_q->free_slots--;
- 
- 	cr0 = (cmd_q->id << REQ0_CMD_Q_SHIFT)
- 	      | (op->jobid << REQ0_JOBID_SHIFT)
- 	      | REQ0_WAIT_FOR_WRITE;
- 
- 	if (op->soc)
- 		cr0 |= REQ0_STOP_ON_COMPLETE
- 		       | REQ0_INT_ON_COMPLETE;
- 
- 	if (op->ioc || !cmd_q->free_slots)
- 		cr0 |= REQ0_INT_ON_COMPLETE;
- 
- 	/* Start at CMD_REQ1 */
- 	cr_addr = ccp->io_regs + CMD_REQ0 + CMD_REQ_INCR;
- 
- 	mutex_lock(&ccp->req_mutex);
- 
- 	/* Write CMD_REQ1 through CMD_REQx first */
- 	for (i = 0; i < cr_count; i++, cr_addr += CMD_REQ_INCR)
- 		iowrite32(*(cr + i), cr_addr);
- 
- 	/* Tell the CCP to start */
- 	wmb();
- 	iowrite32(cr0, ccp->io_regs + CMD_REQ0);
- 
- 	mutex_unlock(&ccp->req_mutex);
- 
- 	if (cr0 & REQ0_INT_ON_COMPLETE) {
- 		/* Wait for the job to complete */
- 		ret = wait_event_interruptible(cmd_q->int_queue,
- 					       cmd_q->int_rcvd);
- 		if (ret || cmd_q->cmd_error) {
- 			/* On error delete all related jobs from the queue */
- 			cmd = (cmd_q->id << DEL_Q_ID_SHIFT)
- 			      | op->jobid;
- 
- 			iowrite32(cmd, ccp->io_regs + DEL_CMD_Q_JOB);
- 
- 			if (!ret)
- 				ret = -EIO;
- 		} else if (op->soc) {
- 			/* Delete just head job from the queue on SoC */
- 			cmd = DEL_Q_ACTIVE
- 			      | (cmd_q->id << DEL_Q_ID_SHIFT)
- 			      | op->jobid;
- 
- 			iowrite32(cmd, ccp->io_regs + DEL_CMD_Q_JOB);
- 		}
- 
- 		cmd_q->free_slots = CMD_Q_DEPTH(cmd_q->q_status);
- 
- 		cmd_q->int_rcvd = 0;
- 	}
- 
- 	return ret;
- }
- 
- static int ccp_perform_aes(struct ccp_op *op)
- {
- 	u32 cr[6];
- 
- 	/* Fill out the register contents for REQ1 through REQ6 */
- 	cr[0] = (CCP_ENGINE_AES << REQ1_ENGINE_SHIFT)
- 		| (op->u.aes.type << REQ1_AES_TYPE_SHIFT)
- 		| (op->u.aes.mode << REQ1_AES_MODE_SHIFT)
- 		| (op->u.aes.action << REQ1_AES_ACTION_SHIFT)
- 		| (op->ksb_key << REQ1_KEY_KSB_SHIFT);
- 	cr[1] = op->src.u.dma.length - 1;
- 	cr[2] = ccp_addr_lo(&op->src.u.dma);
- 	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
- 		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->src.u.dma);
- 	cr[4] = ccp_addr_lo(&op->dst.u.dma);
- 	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->dst.u.dma);
- 
- 	if (op->u.aes.mode == CCP_AES_MODE_CFB)
- 		cr[0] |= ((0x7f) << REQ1_AES_CFB_SIZE_SHIFT);
- 
- 	if (op->eom)
- 		cr[0] |= REQ1_EOM;
- 
- 	if (op->init)
- 		cr[0] |= REQ1_INIT;
- 
- 	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
- }
- 
- static int ccp_perform_xts_aes(struct ccp_op *op)
- {
- 	u32 cr[6];
- 
- 	/* Fill out the register contents for REQ1 through REQ6 */
- 	cr[0] = (CCP_ENGINE_XTS_AES_128 << REQ1_ENGINE_SHIFT)
- 		| (op->u.xts.action << REQ1_AES_ACTION_SHIFT)
- 		| (op->u.xts.unit_size << REQ1_XTS_AES_SIZE_SHIFT)
- 		| (op->ksb_key << REQ1_KEY_KSB_SHIFT);
- 	cr[1] = op->src.u.dma.length - 1;
- 	cr[2] = ccp_addr_lo(&op->src.u.dma);
- 	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
- 		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->src.u.dma);
- 	cr[4] = ccp_addr_lo(&op->dst.u.dma);
- 	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->dst.u.dma);
- 
- 	if (op->eom)
- 		cr[0] |= REQ1_EOM;
- 
- 	if (op->init)
- 		cr[0] |= REQ1_INIT;
- 
- 	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
- }
- 
- static int ccp_perform_sha(struct ccp_op *op)
- {
- 	u32 cr[6];
- 
- 	/* Fill out the register contents for REQ1 through REQ6 */
- 	cr[0] = (CCP_ENGINE_SHA << REQ1_ENGINE_SHIFT)
- 		| (op->u.sha.type << REQ1_SHA_TYPE_SHIFT)
- 		| REQ1_INIT;
- 	cr[1] = op->src.u.dma.length - 1;
- 	cr[2] = ccp_addr_lo(&op->src.u.dma);
- 	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
- 		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->src.u.dma);
- 
- 	if (op->eom) {
- 		cr[0] |= REQ1_EOM;
- 		cr[4] = lower_32_bits(op->u.sha.msg_bits);
- 		cr[5] = upper_32_bits(op->u.sha.msg_bits);
- 	} else {
- 		cr[4] = 0;
- 		cr[5] = 0;
- 	}
- 
- 	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
- }
- 
- static int ccp_perform_rsa(struct ccp_op *op)
- {
- 	u32 cr[6];
- 
- 	/* Fill out the register contents for REQ1 through REQ6 */
- 	cr[0] = (CCP_ENGINE_RSA << REQ1_ENGINE_SHIFT)
- 		| (op->u.rsa.mod_size << REQ1_RSA_MOD_SIZE_SHIFT)
- 		| (op->ksb_key << REQ1_KEY_KSB_SHIFT)
- 		| REQ1_EOM;
- 	cr[1] = op->u.rsa.input_len - 1;
- 	cr[2] = ccp_addr_lo(&op->src.u.dma);
- 	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
- 		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->src.u.dma);
- 	cr[4] = ccp_addr_lo(&op->dst.u.dma);
- 	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->dst.u.dma);
- 
- 	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
- }
- 
- static int ccp_perform_passthru(struct ccp_op *op)
- {
- 	u32 cr[6];
- 
- 	/* Fill out the register contents for REQ1 through REQ6 */
- 	cr[0] = (CCP_ENGINE_PASSTHRU << REQ1_ENGINE_SHIFT)
- 		| (op->u.passthru.bit_mod << REQ1_PT_BW_SHIFT)
- 		| (op->u.passthru.byte_swap << REQ1_PT_BS_SHIFT);
- 
- 	if (op->src.type == CCP_MEMTYPE_SYSTEM)
- 		cr[1] = op->src.u.dma.length - 1;
- 	else
- 		cr[1] = op->dst.u.dma.length - 1;
- 
- 	if (op->src.type == CCP_MEMTYPE_SYSTEM) {
- 		cr[2] = ccp_addr_lo(&op->src.u.dma);
- 		cr[3] = (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
- 			| ccp_addr_hi(&op->src.u.dma);
- 
- 		if (op->u.passthru.bit_mod != CCP_PASSTHRU_BITWISE_NOOP)
- 			cr[3] |= (op->ksb_key << REQ4_KSB_SHIFT);
- 	} else {
- 		cr[2] = op->src.u.ksb * CCP_KSB_BYTES;
- 		cr[3] = (CCP_MEMTYPE_KSB << REQ4_MEMTYPE_SHIFT);
- 	}
- 
- 	if (op->dst.type == CCP_MEMTYPE_SYSTEM) {
- 		cr[4] = ccp_addr_lo(&op->dst.u.dma);
- 		cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
- 			| ccp_addr_hi(&op->dst.u.dma);
- 	} else {
- 		cr[4] = op->dst.u.ksb * CCP_KSB_BYTES;
- 		cr[5] = (CCP_MEMTYPE_KSB << REQ6_MEMTYPE_SHIFT);
- 	}
- 
- 	if (op->eom)
- 		cr[0] |= REQ1_EOM;
- 
- 	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
- }
- 
- static int ccp_perform_ecc(struct ccp_op *op)
- {
- 	u32 cr[6];
- 
- 	/* Fill out the register contents for REQ1 through REQ6 */
- 	cr[0] = REQ1_ECC_AFFINE_CONVERT
- 		| (CCP_ENGINE_ECC << REQ1_ENGINE_SHIFT)
- 		| (op->u.ecc.function << REQ1_ECC_FUNCTION_SHIFT)
- 		| REQ1_EOM;
- 	cr[1] = op->src.u.dma.length - 1;
- 	cr[2] = ccp_addr_lo(&op->src.u.dma);
- 	cr[3] = (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->src.u.dma);
- 	cr[4] = ccp_addr_lo(&op->dst.u.dma);
- 	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
- 		| ccp_addr_hi(&op->dst.u.dma);
- 
- 	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
- }
- 
  static u32 ccp_alloc_ksb(struct ccp_device *ccp, unsigned int count)
  {
  	int start;
diff --cc drivers/crypto/ccp/ccp-pci.c
index 15741de944bc,0bf262e36b6b..000000000000
--- a/drivers/crypto/ccp/ccp-pci.c
+++ b/drivers/crypto/ccp/ccp-pci.c
@@@ -52,17 -52,18 +52,18 @@@ static int ccp_get_msix_irqs(struct ccp
  	for (v = 0; v < ARRAY_SIZE(msix_entry); v++)
  		msix_entry[v].entry = v;
  
 -	ret = pci_enable_msix_range(pdev, msix_entry, 1, v);
 -	if (ret < 0)
 +	while ((ret = pci_enable_msix(pdev, msix_entry, v)) > 0)
 +		v = ret;
 +	if (ret)
  		return ret;
  
 -	ccp_pci->msix_count = ret;
 +	ccp_pci->msix_count = v;
  	for (v = 0; v < ccp_pci->msix_count; v++) {
  		/* Set the interrupt names and request the irqs */
 -		snprintf(ccp_pci->msix[v].name, name_len, "%s-%u",
 -			 ccp->name, v);
 +		snprintf(ccp_pci->msix[v].name, name_len, "ccp-%u", v);
  		ccp_pci->msix[v].vector = msix_entry[v].vector;
- 		ret = request_irq(ccp_pci->msix[v].vector, ccp_irq_handler,
+ 		ret = request_irq(ccp_pci->msix[v].vector,
+ 				  ccp->vdata->perform->irqhandler,
  				  0, ccp_pci->msix[v].name, dev);
  		if (ret) {
  			dev_notice(dev, "unable to allocate MSI-X IRQ (%d)\n",
@@@ -95,7 -96,8 +96,12 @@@ static int ccp_get_msi_irq(struct ccp_d
  		return ret;
  
  	ccp->irq = pdev->irq;
++<<<<<<< HEAD
 +	ret = request_irq(ccp->irq, ccp_irq_handler, 0, "ccp", dev);
++=======
+ 	ret = request_irq(ccp->irq, ccp->vdata->perform->irqhandler, 0,
+ 			  ccp->name, dev);
++>>>>>>> ea0375afa172 (crypto: ccp - Add abstraction for device-specific calls)
  	if (ret) {
  		dev_notice(dev, "unable to allocate MSI IRQ (%d)\n", ret);
  		goto e_msi;
* Unmerged path drivers/crypto/ccp/Makefile
* Unmerged path drivers/crypto/ccp/ccp-platform.c
* Unmerged path drivers/crypto/ccp/Makefile
diff --git a/drivers/crypto/ccp/ccp-dev-v3.c b/drivers/crypto/ccp/ccp-dev-v3.c
new file mode 100644
index 000000000000..7d5eab49179e
--- /dev/null
+++ b/drivers/crypto/ccp/ccp-dev-v3.c
@@ -0,0 +1,533 @@
+/*
+ * AMD Cryptographic Coprocessor (CCP) driver
+ *
+ * Copyright (C) 2013,2016 Advanced Micro Devices, Inc.
+ *
+ * Author: Tom Lendacky <thomas.lendacky@amd.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/kthread.h>
+#include <linux/interrupt.h>
+#include <linux/ccp.h>
+
+#include "ccp-dev.h"
+
+static int ccp_do_cmd(struct ccp_op *op, u32 *cr, unsigned int cr_count)
+{
+	struct ccp_cmd_queue *cmd_q = op->cmd_q;
+	struct ccp_device *ccp = cmd_q->ccp;
+	void __iomem *cr_addr;
+	u32 cr0, cmd;
+	unsigned int i;
+	int ret = 0;
+
+	/* We could read a status register to see how many free slots
+	 * are actually available, but reading that register resets it
+	 * and you could lose some error information.
+	 */
+	cmd_q->free_slots--;
+
+	cr0 = (cmd_q->id << REQ0_CMD_Q_SHIFT)
+	      | (op->jobid << REQ0_JOBID_SHIFT)
+	      | REQ0_WAIT_FOR_WRITE;
+
+	if (op->soc)
+		cr0 |= REQ0_STOP_ON_COMPLETE
+		       | REQ0_INT_ON_COMPLETE;
+
+	if (op->ioc || !cmd_q->free_slots)
+		cr0 |= REQ0_INT_ON_COMPLETE;
+
+	/* Start at CMD_REQ1 */
+	cr_addr = ccp->io_regs + CMD_REQ0 + CMD_REQ_INCR;
+
+	mutex_lock(&ccp->req_mutex);
+
+	/* Write CMD_REQ1 through CMD_REQx first */
+	for (i = 0; i < cr_count; i++, cr_addr += CMD_REQ_INCR)
+		iowrite32(*(cr + i), cr_addr);
+
+	/* Tell the CCP to start */
+	wmb();
+	iowrite32(cr0, ccp->io_regs + CMD_REQ0);
+
+	mutex_unlock(&ccp->req_mutex);
+
+	if (cr0 & REQ0_INT_ON_COMPLETE) {
+		/* Wait for the job to complete */
+		ret = wait_event_interruptible(cmd_q->int_queue,
+					       cmd_q->int_rcvd);
+		if (ret || cmd_q->cmd_error) {
+			/* On error delete all related jobs from the queue */
+			cmd = (cmd_q->id << DEL_Q_ID_SHIFT)
+			      | op->jobid;
+
+			iowrite32(cmd, ccp->io_regs + DEL_CMD_Q_JOB);
+
+			if (!ret)
+				ret = -EIO;
+		} else if (op->soc) {
+			/* Delete just head job from the queue on SoC */
+			cmd = DEL_Q_ACTIVE
+			      | (cmd_q->id << DEL_Q_ID_SHIFT)
+			      | op->jobid;
+
+			iowrite32(cmd, ccp->io_regs + DEL_CMD_Q_JOB);
+		}
+
+		cmd_q->free_slots = CMD_Q_DEPTH(cmd_q->q_status);
+
+		cmd_q->int_rcvd = 0;
+	}
+
+	return ret;
+}
+
+static int ccp_perform_aes(struct ccp_op *op)
+{
+	u32 cr[6];
+
+	/* Fill out the register contents for REQ1 through REQ6 */
+	cr[0] = (CCP_ENGINE_AES << REQ1_ENGINE_SHIFT)
+		| (op->u.aes.type << REQ1_AES_TYPE_SHIFT)
+		| (op->u.aes.mode << REQ1_AES_MODE_SHIFT)
+		| (op->u.aes.action << REQ1_AES_ACTION_SHIFT)
+		| (op->ksb_key << REQ1_KEY_KSB_SHIFT);
+	cr[1] = op->src.u.dma.length - 1;
+	cr[2] = ccp_addr_lo(&op->src.u.dma);
+	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
+		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->src.u.dma);
+	cr[4] = ccp_addr_lo(&op->dst.u.dma);
+	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->dst.u.dma);
+
+	if (op->u.aes.mode == CCP_AES_MODE_CFB)
+		cr[0] |= ((0x7f) << REQ1_AES_CFB_SIZE_SHIFT);
+
+	if (op->eom)
+		cr[0] |= REQ1_EOM;
+
+	if (op->init)
+		cr[0] |= REQ1_INIT;
+
+	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
+}
+
+static int ccp_perform_xts_aes(struct ccp_op *op)
+{
+	u32 cr[6];
+
+	/* Fill out the register contents for REQ1 through REQ6 */
+	cr[0] = (CCP_ENGINE_XTS_AES_128 << REQ1_ENGINE_SHIFT)
+		| (op->u.xts.action << REQ1_AES_ACTION_SHIFT)
+		| (op->u.xts.unit_size << REQ1_XTS_AES_SIZE_SHIFT)
+		| (op->ksb_key << REQ1_KEY_KSB_SHIFT);
+	cr[1] = op->src.u.dma.length - 1;
+	cr[2] = ccp_addr_lo(&op->src.u.dma);
+	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
+		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->src.u.dma);
+	cr[4] = ccp_addr_lo(&op->dst.u.dma);
+	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->dst.u.dma);
+
+	if (op->eom)
+		cr[0] |= REQ1_EOM;
+
+	if (op->init)
+		cr[0] |= REQ1_INIT;
+
+	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
+}
+
+static int ccp_perform_sha(struct ccp_op *op)
+{
+	u32 cr[6];
+
+	/* Fill out the register contents for REQ1 through REQ6 */
+	cr[0] = (CCP_ENGINE_SHA << REQ1_ENGINE_SHIFT)
+		| (op->u.sha.type << REQ1_SHA_TYPE_SHIFT)
+		| REQ1_INIT;
+	cr[1] = op->src.u.dma.length - 1;
+	cr[2] = ccp_addr_lo(&op->src.u.dma);
+	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
+		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->src.u.dma);
+
+	if (op->eom) {
+		cr[0] |= REQ1_EOM;
+		cr[4] = lower_32_bits(op->u.sha.msg_bits);
+		cr[5] = upper_32_bits(op->u.sha.msg_bits);
+	} else {
+		cr[4] = 0;
+		cr[5] = 0;
+	}
+
+	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
+}
+
+static int ccp_perform_rsa(struct ccp_op *op)
+{
+	u32 cr[6];
+
+	/* Fill out the register contents for REQ1 through REQ6 */
+	cr[0] = (CCP_ENGINE_RSA << REQ1_ENGINE_SHIFT)
+		| (op->u.rsa.mod_size << REQ1_RSA_MOD_SIZE_SHIFT)
+		| (op->ksb_key << REQ1_KEY_KSB_SHIFT)
+		| REQ1_EOM;
+	cr[1] = op->u.rsa.input_len - 1;
+	cr[2] = ccp_addr_lo(&op->src.u.dma);
+	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
+		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->src.u.dma);
+	cr[4] = ccp_addr_lo(&op->dst.u.dma);
+	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->dst.u.dma);
+
+	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
+}
+
+static int ccp_perform_passthru(struct ccp_op *op)
+{
+	u32 cr[6];
+
+	/* Fill out the register contents for REQ1 through REQ6 */
+	cr[0] = (CCP_ENGINE_PASSTHRU << REQ1_ENGINE_SHIFT)
+		| (op->u.passthru.bit_mod << REQ1_PT_BW_SHIFT)
+		| (op->u.passthru.byte_swap << REQ1_PT_BS_SHIFT);
+
+	if (op->src.type == CCP_MEMTYPE_SYSTEM)
+		cr[1] = op->src.u.dma.length - 1;
+	else
+		cr[1] = op->dst.u.dma.length - 1;
+
+	if (op->src.type == CCP_MEMTYPE_SYSTEM) {
+		cr[2] = ccp_addr_lo(&op->src.u.dma);
+		cr[3] = (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
+			| ccp_addr_hi(&op->src.u.dma);
+
+		if (op->u.passthru.bit_mod != CCP_PASSTHRU_BITWISE_NOOP)
+			cr[3] |= (op->ksb_key << REQ4_KSB_SHIFT);
+	} else {
+		cr[2] = op->src.u.ksb * CCP_KSB_BYTES;
+		cr[3] = (CCP_MEMTYPE_KSB << REQ4_MEMTYPE_SHIFT);
+	}
+
+	if (op->dst.type == CCP_MEMTYPE_SYSTEM) {
+		cr[4] = ccp_addr_lo(&op->dst.u.dma);
+		cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
+			| ccp_addr_hi(&op->dst.u.dma);
+	} else {
+		cr[4] = op->dst.u.ksb * CCP_KSB_BYTES;
+		cr[5] = (CCP_MEMTYPE_KSB << REQ6_MEMTYPE_SHIFT);
+	}
+
+	if (op->eom)
+		cr[0] |= REQ1_EOM;
+
+	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
+}
+
+static int ccp_perform_ecc(struct ccp_op *op)
+{
+	u32 cr[6];
+
+	/* Fill out the register contents for REQ1 through REQ6 */
+	cr[0] = REQ1_ECC_AFFINE_CONVERT
+		| (CCP_ENGINE_ECC << REQ1_ENGINE_SHIFT)
+		| (op->u.ecc.function << REQ1_ECC_FUNCTION_SHIFT)
+		| REQ1_EOM;
+	cr[1] = op->src.u.dma.length - 1;
+	cr[2] = ccp_addr_lo(&op->src.u.dma);
+	cr[3] = (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->src.u.dma);
+	cr[4] = ccp_addr_lo(&op->dst.u.dma);
+	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
+		| ccp_addr_hi(&op->dst.u.dma);
+
+	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
+}
+
+static int ccp_trng_read(struct hwrng *rng, void *data, size_t max, bool wait)
+{
+	struct ccp_device *ccp = container_of(rng, struct ccp_device, hwrng);
+	u32 trng_value;
+	int len = min_t(int, sizeof(trng_value), max);
+
+	/*
+	 * Locking is provided by the caller so we can update device
+	 * hwrng-related fields safely
+	 */
+	trng_value = ioread32(ccp->io_regs + TRNG_OUT_REG);
+	if (!trng_value) {
+		/* Zero is returned if not data is available or if a
+		 * bad-entropy error is present. Assume an error if
+		 * we exceed TRNG_RETRIES reads of zero.
+		 */
+		if (ccp->hwrng_retries++ > TRNG_RETRIES)
+			return -EIO;
+
+		return 0;
+	}
+
+	/* Reset the counter and save the rng value */
+	ccp->hwrng_retries = 0;
+	memcpy(data, &trng_value, len);
+
+	return len;
+}
+
+static int ccp_init(struct ccp_device *ccp)
+{
+	struct device *dev = ccp->dev;
+	struct ccp_cmd_queue *cmd_q;
+	struct dma_pool *dma_pool;
+	char dma_pool_name[MAX_DMAPOOL_NAME_LEN];
+	unsigned int qmr, qim, i;
+	int ret;
+
+	/* Find available queues */
+	qim = 0;
+	qmr = ioread32(ccp->io_regs + Q_MASK_REG);
+	for (i = 0; i < MAX_HW_QUEUES; i++) {
+		if (!(qmr & (1 << i)))
+			continue;
+
+		/* Allocate a dma pool for this queue */
+		snprintf(dma_pool_name, sizeof(dma_pool_name), "%s_q%d",
+			 ccp->name, i);
+		dma_pool = dma_pool_create(dma_pool_name, dev,
+					   CCP_DMAPOOL_MAX_SIZE,
+					   CCP_DMAPOOL_ALIGN, 0);
+		if (!dma_pool) {
+			dev_err(dev, "unable to allocate dma pool\n");
+			ret = -ENOMEM;
+			goto e_pool;
+		}
+
+		cmd_q = &ccp->cmd_q[ccp->cmd_q_count];
+		ccp->cmd_q_count++;
+
+		cmd_q->ccp = ccp;
+		cmd_q->id = i;
+		cmd_q->dma_pool = dma_pool;
+
+		/* Reserve 2 KSB regions for the queue */
+		cmd_q->ksb_key = KSB_START + ccp->ksb_start++;
+		cmd_q->ksb_ctx = KSB_START + ccp->ksb_start++;
+		ccp->ksb_count -= 2;
+
+		/* Preset some register values and masks that are queue
+		 * number dependent
+		 */
+		cmd_q->reg_status = ccp->io_regs + CMD_Q_STATUS_BASE +
+				    (CMD_Q_STATUS_INCR * i);
+		cmd_q->reg_int_status = ccp->io_regs + CMD_Q_INT_STATUS_BASE +
+					(CMD_Q_STATUS_INCR * i);
+		cmd_q->int_ok = 1 << (i * 2);
+		cmd_q->int_err = 1 << ((i * 2) + 1);
+
+		cmd_q->free_slots = CMD_Q_DEPTH(ioread32(cmd_q->reg_status));
+
+		init_waitqueue_head(&cmd_q->int_queue);
+
+		/* Build queue interrupt mask (two interrupts per queue) */
+		qim |= cmd_q->int_ok | cmd_q->int_err;
+
+#ifdef CONFIG_ARM64
+		/* For arm64 set the recommended queue cache settings */
+		iowrite32(ccp->axcache, ccp->io_regs + CMD_Q_CACHE_BASE +
+			  (CMD_Q_CACHE_INC * i));
+#endif
+
+		dev_dbg(dev, "queue #%u available\n", i);
+	}
+	if (ccp->cmd_q_count == 0) {
+		dev_notice(dev, "no command queues available\n");
+		ret = -EIO;
+		goto e_pool;
+	}
+	dev_notice(dev, "%u command queues available\n", ccp->cmd_q_count);
+
+	/* Disable and clear interrupts until ready */
+	iowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);
+	for (i = 0; i < ccp->cmd_q_count; i++) {
+		cmd_q = &ccp->cmd_q[i];
+
+		ioread32(cmd_q->reg_int_status);
+		ioread32(cmd_q->reg_status);
+	}
+	iowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);
+
+	/* Request an irq */
+	ret = ccp->get_irq(ccp);
+	if (ret) {
+		dev_err(dev, "unable to allocate an IRQ\n");
+		goto e_pool;
+	}
+
+	/* Initialize the queues used to wait for KSB space and suspend */
+	init_waitqueue_head(&ccp->ksb_queue);
+	init_waitqueue_head(&ccp->suspend_queue);
+
+	/* Create a kthread for each queue */
+	for (i = 0; i < ccp->cmd_q_count; i++) {
+		struct task_struct *kthread;
+
+		cmd_q = &ccp->cmd_q[i];
+
+		kthread = kthread_create(ccp_cmd_queue_thread, cmd_q,
+					 "%s-q%u", ccp->name, cmd_q->id);
+		if (IS_ERR(kthread)) {
+			dev_err(dev, "error creating queue thread (%ld)\n",
+				PTR_ERR(kthread));
+			ret = PTR_ERR(kthread);
+			goto e_kthread;
+		}
+
+		cmd_q->kthread = kthread;
+		wake_up_process(kthread);
+	}
+
+	/* Register the RNG */
+	ccp->hwrng.name = ccp->rngname;
+	ccp->hwrng.read = ccp_trng_read;
+	ret = hwrng_register(&ccp->hwrng);
+	if (ret) {
+		dev_err(dev, "error registering hwrng (%d)\n", ret);
+		goto e_kthread;
+	}
+
+	ccp_add_device(ccp);
+
+	/* Enable interrupts */
+	iowrite32(qim, ccp->io_regs + IRQ_MASK_REG);
+
+	return 0;
+
+e_kthread:
+	for (i = 0; i < ccp->cmd_q_count; i++)
+		if (ccp->cmd_q[i].kthread)
+			kthread_stop(ccp->cmd_q[i].kthread);
+
+	ccp->free_irq(ccp);
+
+e_pool:
+	for (i = 0; i < ccp->cmd_q_count; i++)
+		dma_pool_destroy(ccp->cmd_q[i].dma_pool);
+
+	return ret;
+}
+
+static void ccp_destroy(struct ccp_device *ccp)
+{
+	struct ccp_cmd_queue *cmd_q;
+	struct ccp_cmd *cmd;
+	unsigned int qim, i;
+
+	/* Remove this device from the list of available units first */
+	ccp_del_device(ccp);
+
+	/* Unregister the RNG */
+	hwrng_unregister(&ccp->hwrng);
+
+	/* Stop the queue kthreads */
+	for (i = 0; i < ccp->cmd_q_count; i++)
+		if (ccp->cmd_q[i].kthread)
+			kthread_stop(ccp->cmd_q[i].kthread);
+
+	/* Build queue interrupt mask (two interrupt masks per queue) */
+	qim = 0;
+	for (i = 0; i < ccp->cmd_q_count; i++) {
+		cmd_q = &ccp->cmd_q[i];
+		qim |= cmd_q->int_ok | cmd_q->int_err;
+	}
+
+	/* Disable and clear interrupts */
+	iowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);
+	for (i = 0; i < ccp->cmd_q_count; i++) {
+		cmd_q = &ccp->cmd_q[i];
+
+		ioread32(cmd_q->reg_int_status);
+		ioread32(cmd_q->reg_status);
+	}
+	iowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);
+
+	ccp->free_irq(ccp);
+
+	for (i = 0; i < ccp->cmd_q_count; i++)
+		dma_pool_destroy(ccp->cmd_q[i].dma_pool);
+
+	/* Flush the cmd and backlog queue */
+	while (!list_empty(&ccp->cmd)) {
+		/* Invoke the callback directly with an error code */
+		cmd = list_first_entry(&ccp->cmd, struct ccp_cmd, entry);
+		list_del(&cmd->entry);
+		cmd->callback(cmd->data, -ENODEV);
+	}
+	while (!list_empty(&ccp->backlog)) {
+		/* Invoke the callback directly with an error code */
+		cmd = list_first_entry(&ccp->backlog, struct ccp_cmd, entry);
+		list_del(&cmd->entry);
+		cmd->callback(cmd->data, -ENODEV);
+	}
+}
+
+static irqreturn_t ccp_irq_handler(int irq, void *data)
+{
+	struct device *dev = data;
+	struct ccp_device *ccp = dev_get_drvdata(dev);
+	struct ccp_cmd_queue *cmd_q;
+	u32 q_int, status;
+	unsigned int i;
+
+	status = ioread32(ccp->io_regs + IRQ_STATUS_REG);
+
+	for (i = 0; i < ccp->cmd_q_count; i++) {
+		cmd_q = &ccp->cmd_q[i];
+
+		q_int = status & (cmd_q->int_ok | cmd_q->int_err);
+		if (q_int) {
+			cmd_q->int_status = status;
+			cmd_q->q_status = ioread32(cmd_q->reg_status);
+			cmd_q->q_int_status = ioread32(cmd_q->reg_int_status);
+
+			/* On error, only save the first error value */
+			if ((q_int & cmd_q->int_err) && !cmd_q->cmd_error)
+				cmd_q->cmd_error = CMD_Q_ERROR(cmd_q->q_status);
+
+			cmd_q->int_rcvd = 1;
+
+			/* Acknowledge the interrupt and wake the kthread */
+			iowrite32(q_int, ccp->io_regs + IRQ_STATUS_REG);
+			wake_up_interruptible(&cmd_q->int_queue);
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+static struct ccp_actions ccp3_actions = {
+	.perform_aes = ccp_perform_aes,
+	.perform_xts_aes = ccp_perform_xts_aes,
+	.perform_sha = ccp_perform_sha,
+	.perform_rsa = ccp_perform_rsa,
+	.perform_passthru = ccp_perform_passthru,
+	.perform_ecc = ccp_perform_ecc,
+	.init = ccp_init,
+	.destroy = ccp_destroy,
+	.irqhandler = ccp_irq_handler,
+};
+
+struct ccp_vdata ccpv3 = {
+	.version = CCP_VERSION(3, 0),
+	.perform = &ccp3_actions,
+};
* Unmerged path drivers/crypto/ccp/ccp-dev.c
* Unmerged path drivers/crypto/ccp/ccp-dev.h
* Unmerged path drivers/crypto/ccp/ccp-ops.c
* Unmerged path drivers/crypto/ccp/ccp-pci.c
* Unmerged path drivers/crypto/ccp/ccp-platform.c
