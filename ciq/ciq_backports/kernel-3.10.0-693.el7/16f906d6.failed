xprtrdma: Reduce required number of send SGEs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 16f906d66cd76fb9895cbc628f447532a7ac1faa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/16f906d6.failed

The MAX_SEND_SGES check introduced in commit 655fec6987be
("xprtrdma: Use gathered Send for large inline messages") fails
for devices that have a small max_sge.

Instead of checking for a large fixed maximum number of SGEs,
check for a minimum small number. RPC-over-RDMA will switch to
using a Read chunk if an xdr_buf has more pages than can fit in
the device's max_sge limit. This is considerably better than
failing all together to mount the server.

This fix supports devices that have as few as three send SGEs
available.

	Reported-by: Selvin Xavier <selvin.xavier@broadcom.com>
	Reported-by: Devesh Sharma <devesh.sharma@broadcom.com>
	Reported-by: Honggang Li <honli@redhat.com>
	Reported-by: Ram Amrani <Ram.Amrani@cavium.com>
Fixes: 655fec6987be ("xprtrdma: Use gathered Send for large ...")
	Cc: stable@vger.kernel.org # v4.9+
	Tested-by: Honggang Li <honli@redhat.com>
	Tested-by: Ram Amrani <Ram.Amrani@cavium.com>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Reviewed-by: Parav Pandit <parav@mellanox.com>
	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 16f906d66cd76fb9895cbc628f447532a7ac1faa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
#	net/sunrpc/xprtrdma/xprt_rdma.h
diff --cc net/sunrpc/xprtrdma/verbs.c
index dc48dad17dcd,61d16c39e92c..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -500,18 -488,19 +500,25 @@@ rpcrdma_ia_close(struct rpcrdma_ia *ia
   */
  int
  rpcrdma_ep_create(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia,
- 				struct rpcrdma_create_data_internal *cdata)
+ 		  struct rpcrdma_create_data_internal *cdata)
  {
  	struct rpcrdma_connect_private *pmsg = &ep->rep_cm_private;
+ 	unsigned int max_qp_wr, max_sge;
  	struct ib_cq *sendcq, *recvcq;
- 	unsigned int max_qp_wr;
  	int rc;
  
++<<<<<<< HEAD
 +	if (ia->ri_device->attrs.max_sge < RPCRDMA_MAX_IOVS) {
 +		dprintk("RPC:       %s: insufficient sge's available\n",
 +			__func__);
++=======
+ 	max_sge = min(ia->ri_device->attrs.max_sge, RPCRDMA_MAX_SEND_SGES);
+ 	if (max_sge < RPCRDMA_MIN_SEND_SGES) {
+ 		pr_warn("rpcrdma: HCA provides only %d send SGEs\n", max_sge);
++>>>>>>> 16f906d66cd7 (xprtrdma: Reduce required number of send SGEs)
  		return -ENOMEM;
  	}
+ 	ia->ri_max_send_sges = max_sge - RPCRDMA_MIN_SEND_SGES;
  
  	if (ia->ri_device->attrs.max_qp_wr <= RPCRDMA_BACKWARD_WRS) {
  		dprintk("RPC:       %s: insufficient wqe's available\n",
@@@ -536,7 -525,7 +543,11 @@@
  	ep->rep_attr.cap.max_recv_wr = cdata->max_requests;
  	ep->rep_attr.cap.max_recv_wr += RPCRDMA_BACKWARD_WRS;
  	ep->rep_attr.cap.max_recv_wr += 1;	/* drain cqe */
++<<<<<<< HEAD
 +	ep->rep_attr.cap.max_send_sge = RPCRDMA_MAX_IOVS;
++=======
+ 	ep->rep_attr.cap.max_send_sge = max_sge;
++>>>>>>> 16f906d66cd7 (xprtrdma: Reduce required number of send SGEs)
  	ep->rep_attr.cap.max_recv_sge = 1;
  	ep->rep_attr.cap.max_inline_data = 0;
  	ep->rep_attr.sq_sig_type = IB_SIGNAL_REQ_WR;
diff --cc net/sunrpc/xprtrdma/xprt_rdma.h
index ae3921a9fec6,3d7e9c9bad1f..000000000000
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@@ -75,6 -74,10 +75,13 @@@ struct rpcrdma_ia 
  	unsigned int		ri_max_frmr_depth;
  	unsigned int		ri_max_inline_write;
  	unsigned int		ri_max_inline_read;
++<<<<<<< HEAD
++=======
+ 	unsigned int		ri_max_send_sges;
+ 	bool			ri_reminv_expected;
+ 	bool			ri_implicit_roundup;
+ 	enum ib_mr_type		ri_mrtype;
++>>>>>>> 16f906d66cd7 (xprtrdma: Reduce required number of send SGEs)
  	struct ib_qp_attr	ri_qp_attr;
  	struct ib_qp_init_attr	ri_qp_init_attr;
  };
@@@ -285,21 -305,31 +292,36 @@@ struct rpcrdma_mr_seg {		/* chunk descr
  	char		*mr_offset;	/* kva if no page, else offset */
  };
  
++<<<<<<< HEAD
 +#define RPCRDMA_MAX_IOVS	(2)
++=======
+ /* Reserve enough Send SGEs to send a maximum size inline request:
+  * - RPC-over-RDMA header
+  * - xdr_buf head iovec
+  * - RPCRDMA_MAX_INLINE bytes, possibly unaligned, in pages
+  * - xdr_buf tail iovec
+  */
+ enum {
+ 	RPCRDMA_MIN_SEND_SGES = 3,
+ 	RPCRDMA_MAX_SEND_PAGES = PAGE_SIZE + RPCRDMA_MAX_INLINE - 1,
+ 	RPCRDMA_MAX_PAGE_SGES = (RPCRDMA_MAX_SEND_PAGES >> PAGE_SHIFT) + 1,
+ 	RPCRDMA_MAX_SEND_SGES = 1 + 1 + RPCRDMA_MAX_PAGE_SGES + 1,
+ };
++>>>>>>> 16f906d66cd7 (xprtrdma: Reduce required number of send SGEs)
  
 -struct rpcrdma_buffer;
  struct rpcrdma_req {
  	struct list_head	rl_free;
 -	unsigned int		rl_mapped_sges;
 +	unsigned int		rl_niovs;
 +	unsigned int		rl_nchunks;
  	unsigned int		rl_connect_cookie;
 +	struct rpc_task		*rl_task;
  	struct rpcrdma_buffer	*rl_buffer;
 -	struct rpcrdma_rep	*rl_reply;
 -	struct ib_send_wr	rl_send_wr;
 -	struct ib_sge		rl_send_sge[RPCRDMA_MAX_SEND_SGES];
 -	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
 -	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */
 -	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
 +	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 +	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
 +	struct rpcrdma_regbuf	*rl_rdmabuf;
 +	struct rpcrdma_regbuf	*rl_sendbuf;
 +	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 +	struct rpcrdma_mr_seg	*rl_nextseg;
  
  	struct ib_cqe		rl_cqe;
  	struct list_head	rl_all;
diff --git a/net/sunrpc/xprtrdma/rpc_rdma.c b/net/sunrpc/xprtrdma/rpc_rdma.c
index 83e624c2ef04..6ebd38e29310 100644
--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@ -133,14 +133,34 @@ void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *r_xprt)
 /* The client can send a request inline as long as the RPCRDMA header
  * plus the RPC call fit under the transport's inline limit. If the
  * combined call message size exceeds that limit, the client must use
- * the read chunk list for this operation.
+ * a Read chunk for this operation.
+ *
+ * A Read chunk is also required if sending the RPC call inline would
+ * exceed this device's max_sge limit.
  */
 static bool rpcrdma_args_inline(struct rpcrdma_xprt *r_xprt,
 				struct rpc_rqst *rqst)
 {
-	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
+	struct xdr_buf *xdr = &rqst->rq_snd_buf;
+	unsigned int count, remaining, offset;
+
+	if (xdr->len > r_xprt->rx_ia.ri_max_inline_write)
+		return false;
 
-	return rqst->rq_snd_buf.len <= ia->ri_max_inline_write;
+	if (xdr->page_len) {
+		remaining = xdr->page_len;
+		offset = xdr->page_base & ~PAGE_MASK;
+		count = 0;
+		while (remaining) {
+			remaining -= min_t(unsigned int,
+					   PAGE_SIZE - offset, remaining);
+			offset = 0;
+			if (++count > r_xprt->rx_ia.ri_max_send_sges)
+				return false;
+		}
+	}
+
+	return true;
 }
 
 /* The client can't know how large the actual reply will be. Thus it
* Unmerged path net/sunrpc/xprtrdma/verbs.c
* Unmerged path net/sunrpc/xprtrdma/xprt_rdma.h
