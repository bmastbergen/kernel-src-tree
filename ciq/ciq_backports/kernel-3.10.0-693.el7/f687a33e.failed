md/r5cache: run_no_space_stripes() when R5C_LOG_CRITICAL == 0

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [md] r5cache: run_no_space_stripes() when R5C_LOG_CRITICAL == 0 (Jes Sorensen) [1380016]
Rebuild_FUZZ: 97.48%
commit-author Song Liu <songliubraving@fb.com>
commit f687a33ef02d3e0f13c4fa9e3b2e90f656bbfb26
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/f687a33e.failed

With writeback cache, we define log space critical as

   free_space < 2 * reclaim_required_space

So the deassert of R5C_LOG_CRITICAL could happen when
  1. free_space increases
  2. reclaim_required_space decreases

Currently, run_no_space_stripes() is called when 1 happens, but
not (always) when 2 happens.

With this patch, run_no_space_stripes() is call when
R5C_LOG_CRITICAL is cleared.

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit f687a33ef02d3e0f13c4fa9e3b2e90f656bbfb26)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5-cache.c
diff --cc drivers/md/raid5-cache.c
index c6ed6dc6889f,c36f86b47252..000000000000
--- a/drivers/md/raid5-cache.c
+++ b/drivers/md/raid5-cache.c
@@@ -176,6 -252,226 +176,229 @@@ static void __r5l_set_io_unit_state(str
  	io->state = state;
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ r5c_return_dev_pending_writes(struct r5conf *conf, struct r5dev *dev,
+ 			      struct bio_list *return_bi)
+ {
+ 	struct bio *wbi, *wbi2;
+ 
+ 	wbi = dev->written;
+ 	dev->written = NULL;
+ 	while (wbi && wbi->bi_iter.bi_sector <
+ 	       dev->sector + STRIPE_SECTORS) {
+ 		wbi2 = r5_next_bio(wbi, dev->sector);
+ 		if (!raid5_dec_bi_active_stripes(wbi)) {
+ 			md_write_end(conf->mddev);
+ 			bio_list_add(return_bi, wbi);
+ 		}
+ 		wbi = wbi2;
+ 	}
+ }
+ 
+ void r5c_handle_cached_data_endio(struct r5conf *conf,
+ 	  struct stripe_head *sh, int disks, struct bio_list *return_bi)
+ {
+ 	int i;
+ 
+ 	for (i = sh->disks; i--; ) {
+ 		if (sh->dev[i].written) {
+ 			set_bit(R5_UPTODATE, &sh->dev[i].flags);
+ 			r5c_return_dev_pending_writes(conf, &sh->dev[i],
+ 						      return_bi);
+ 			bitmap_endwrite(conf->mddev->bitmap, sh->sector,
+ 					STRIPE_SECTORS,
+ 					!test_bit(STRIPE_DEGRADED, &sh->state),
+ 					0);
+ 		}
+ 	}
+ }
+ 
+ /* Check whether we should flush some stripes to free up stripe cache */
+ void r5c_check_stripe_cache_usage(struct r5conf *conf)
+ {
+ 	int total_cached;
+ 
+ 	if (!r5c_is_writeback(conf->log))
+ 		return;
+ 
+ 	total_cached = atomic_read(&conf->r5c_cached_partial_stripes) +
+ 		atomic_read(&conf->r5c_cached_full_stripes);
+ 
+ 	/*
+ 	 * The following condition is true for either of the following:
+ 	 *   - stripe cache pressure high:
+ 	 *          total_cached > 3/4 min_nr_stripes ||
+ 	 *          empty_inactive_list_nr > 0
+ 	 *   - stripe cache pressure moderate:
+ 	 *          total_cached > 1/2 min_nr_stripes
+ 	 */
+ 	if (total_cached > conf->min_nr_stripes * 1 / 2 ||
+ 	    atomic_read(&conf->empty_inactive_list_nr) > 0)
+ 		r5l_wake_reclaim(conf->log, 0);
+ }
+ 
+ /*
+  * flush cache when there are R5C_FULL_STRIPE_FLUSH_BATCH or more full
+  * stripes in the cache
+  */
+ void r5c_check_cached_full_stripe(struct r5conf *conf)
+ {
+ 	if (!r5c_is_writeback(conf->log))
+ 		return;
+ 
+ 	/*
+ 	 * wake up reclaim for R5C_FULL_STRIPE_FLUSH_BATCH cached stripes
+ 	 * or a full stripe (chunk size / 4k stripes).
+ 	 */
+ 	if (atomic_read(&conf->r5c_cached_full_stripes) >=
+ 	    min(R5C_FULL_STRIPE_FLUSH_BATCH,
+ 		conf->chunk_sectors >> STRIPE_SHIFT))
+ 		r5l_wake_reclaim(conf->log, 0);
+ }
+ 
+ /*
+  * Total log space (in sectors) needed to flush all data in cache
+  *
+  * Currently, writing-out phase automatically includes all pending writes
+  * to the same sector. So the reclaim of each stripe takes up to
+  * (conf->raid_disks + 1) pages of log space.
+  *
+  * To totally avoid deadlock due to log space, the code reserves
+  * (conf->raid_disks + 1) pages for each stripe in cache, which is not
+  * necessary in most cases.
+  *
+  * To improve this, we will need writing-out phase to be able to NOT include
+  * pending writes, which will reduce the requirement to
+  * (conf->max_degraded + 1) pages per stripe in cache.
+  */
+ static sector_t r5c_log_required_to_flush_cache(struct r5conf *conf)
+ {
+ 	struct r5l_log *log = conf->log;
+ 
+ 	if (!r5c_is_writeback(log))
+ 		return 0;
+ 
+ 	return BLOCK_SECTORS * (conf->raid_disks + 1) *
+ 		atomic_read(&log->stripe_in_journal_count);
+ }
+ 
+ /*
+  * evaluate log space usage and update R5C_LOG_TIGHT and R5C_LOG_CRITICAL
+  *
+  * R5C_LOG_TIGHT is set when free space on the log device is less than 3x of
+  * reclaim_required_space. R5C_LOG_CRITICAL is set when free space on the log
+  * device is less than 2x of reclaim_required_space.
+  */
+ static inline void r5c_update_log_state(struct r5l_log *log)
+ {
+ 	struct r5conf *conf = log->rdev->mddev->private;
+ 	sector_t free_space;
+ 	sector_t reclaim_space;
+ 	bool wake_reclaim = false;
+ 
+ 	if (!r5c_is_writeback(log))
+ 		return;
+ 
+ 	free_space = r5l_ring_distance(log, log->log_start,
+ 				       log->last_checkpoint);
+ 	reclaim_space = r5c_log_required_to_flush_cache(conf);
+ 	if (free_space < 2 * reclaim_space)
+ 		set_bit(R5C_LOG_CRITICAL, &conf->cache_state);
+ 	else {
+ 		if (test_bit(R5C_LOG_CRITICAL, &conf->cache_state))
+ 			wake_reclaim = true;
+ 		clear_bit(R5C_LOG_CRITICAL, &conf->cache_state);
+ 	}
+ 	if (free_space < 3 * reclaim_space)
+ 		set_bit(R5C_LOG_TIGHT, &conf->cache_state);
+ 	else
+ 		clear_bit(R5C_LOG_TIGHT, &conf->cache_state);
+ 
+ 	if (wake_reclaim)
+ 		r5l_wake_reclaim(log, 0);
+ }
+ 
+ /*
+  * Put the stripe into writing-out phase by clearing STRIPE_R5C_CACHING.
+  * This function should only be called in write-back mode.
+  */
+ void r5c_make_stripe_write_out(struct stripe_head *sh)
+ {
+ 	struct r5conf *conf = sh->raid_conf;
+ 	struct r5l_log *log = conf->log;
+ 
+ 	BUG_ON(!r5c_is_writeback(log));
+ 
+ 	WARN_ON(!test_bit(STRIPE_R5C_CACHING, &sh->state));
+ 	clear_bit(STRIPE_R5C_CACHING, &sh->state);
+ 
+ 	if (!test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))
+ 		atomic_inc(&conf->preread_active_stripes);
+ 
+ 	if (test_and_clear_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state)) {
+ 		BUG_ON(atomic_read(&conf->r5c_cached_partial_stripes) == 0);
+ 		atomic_dec(&conf->r5c_cached_partial_stripes);
+ 	}
+ 
+ 	if (test_and_clear_bit(STRIPE_R5C_FULL_STRIPE, &sh->state)) {
+ 		BUG_ON(atomic_read(&conf->r5c_cached_full_stripes) == 0);
+ 		atomic_dec(&conf->r5c_cached_full_stripes);
+ 	}
+ }
+ 
+ static void r5c_handle_data_cached(struct stripe_head *sh)
+ {
+ 	int i;
+ 
+ 	for (i = sh->disks; i--; )
+ 		if (test_and_clear_bit(R5_Wantwrite, &sh->dev[i].flags)) {
+ 			set_bit(R5_InJournal, &sh->dev[i].flags);
+ 			clear_bit(R5_LOCKED, &sh->dev[i].flags);
+ 		}
+ 	clear_bit(STRIPE_LOG_TRAPPED, &sh->state);
+ }
+ 
+ /*
+  * this journal write must contain full parity,
+  * it may also contain some data pages
+  */
+ static void r5c_handle_parity_cached(struct stripe_head *sh)
+ {
+ 	int i;
+ 
+ 	for (i = sh->disks; i--; )
+ 		if (test_bit(R5_InJournal, &sh->dev[i].flags))
+ 			set_bit(R5_Wantwrite, &sh->dev[i].flags);
+ }
+ 
+ /*
+  * Setting proper flags after writing (or flushing) data and/or parity to the
+  * log device. This is called from r5l_log_endio() or r5l_log_flush_endio().
+  */
+ static void r5c_finish_cache_stripe(struct stripe_head *sh)
+ {
+ 	struct r5l_log *log = sh->raid_conf->log;
+ 
+ 	if (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH) {
+ 		BUG_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));
+ 		/*
+ 		 * Set R5_InJournal for parity dev[pd_idx]. This means
+ 		 * all data AND parity in the journal. For RAID 6, it is
+ 		 * NOT necessary to set the flag for dev[qd_idx], as the
+ 		 * two parities are written out together.
+ 		 */
+ 		set_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);
+ 	} else if (test_bit(STRIPE_R5C_CACHING, &sh->state)) {
+ 		r5c_handle_data_cached(sh);
+ 	} else {
+ 		r5c_handle_parity_cached(sh);
+ 		set_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);
+ 	}
+ }
+ 
++>>>>>>> f687a33ef02d (md/r5cache: run_no_space_stripes() when R5C_LOG_CRITICAL == 0)
  static void r5l_io_run_stripes(struct r5l_io_unit *io)
  {
  	struct stripe_head *sh, *next;
@@@ -744,6 -1228,136 +967,139 @@@ static void r5l_write_super_and_discard
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * r5c_flush_stripe moves stripe from cached list to handle_list. When called,
+  * the stripe must be on r5c_cached_full_stripes or r5c_cached_partial_stripes.
+  *
+  * must hold conf->device_lock
+  */
+ static void r5c_flush_stripe(struct r5conf *conf, struct stripe_head *sh)
+ {
+ 	BUG_ON(list_empty(&sh->lru));
+ 	BUG_ON(!test_bit(STRIPE_R5C_CACHING, &sh->state));
+ 	BUG_ON(test_bit(STRIPE_HANDLE, &sh->state));
+ 
+ 	/*
+ 	 * The stripe is not ON_RELEASE_LIST, so it is safe to call
+ 	 * raid5_release_stripe() while holding conf->device_lock
+ 	 */
+ 	BUG_ON(test_bit(STRIPE_ON_RELEASE_LIST, &sh->state));
+ 	assert_spin_locked(&conf->device_lock);
+ 
+ 	list_del_init(&sh->lru);
+ 	atomic_inc(&sh->count);
+ 
+ 	set_bit(STRIPE_HANDLE, &sh->state);
+ 	atomic_inc(&conf->active_stripes);
+ 	r5c_make_stripe_write_out(sh);
+ 
+ 	raid5_release_stripe(sh);
+ }
+ 
+ /*
+  * if num == 0, flush all full stripes
+  * if num > 0, flush all full stripes. If less than num full stripes are
+  *             flushed, flush some partial stripes until totally num stripes are
+  *             flushed or there is no more cached stripes.
+  */
+ void r5c_flush_cache(struct r5conf *conf, int num)
+ {
+ 	int count;
+ 	struct stripe_head *sh, *next;
+ 
+ 	assert_spin_locked(&conf->device_lock);
+ 	if (!conf->log)
+ 		return;
+ 
+ 	count = 0;
+ 	list_for_each_entry_safe(sh, next, &conf->r5c_full_stripe_list, lru) {
+ 		r5c_flush_stripe(conf, sh);
+ 		count++;
+ 	}
+ 
+ 	if (count >= num)
+ 		return;
+ 	list_for_each_entry_safe(sh, next,
+ 				 &conf->r5c_partial_stripe_list, lru) {
+ 		r5c_flush_stripe(conf, sh);
+ 		if (++count >= num)
+ 			break;
+ 	}
+ }
+ 
+ static void r5c_do_reclaim(struct r5conf *conf)
+ {
+ 	struct r5l_log *log = conf->log;
+ 	struct stripe_head *sh;
+ 	int count = 0;
+ 	unsigned long flags;
+ 	int total_cached;
+ 	int stripes_to_flush;
+ 
+ 	if (!r5c_is_writeback(log))
+ 		return;
+ 
+ 	total_cached = atomic_read(&conf->r5c_cached_partial_stripes) +
+ 		atomic_read(&conf->r5c_cached_full_stripes);
+ 
+ 	if (total_cached > conf->min_nr_stripes * 3 / 4 ||
+ 	    atomic_read(&conf->empty_inactive_list_nr) > 0)
+ 		/*
+ 		 * if stripe cache pressure high, flush all full stripes and
+ 		 * some partial stripes
+ 		 */
+ 		stripes_to_flush = R5C_RECLAIM_STRIPE_GROUP;
+ 	else if (total_cached > conf->min_nr_stripes * 1 / 2 ||
+ 		 atomic_read(&conf->r5c_cached_full_stripes) >
+ 		 R5C_FULL_STRIPE_FLUSH_BATCH)
+ 		/*
+ 		 * if stripe cache pressure moderate, or if there is many full
+ 		 * stripes,flush all full stripes
+ 		 */
+ 		stripes_to_flush = 0;
+ 	else
+ 		/* no need to flush */
+ 		stripes_to_flush = -1;
+ 
+ 	if (stripes_to_flush >= 0) {
+ 		spin_lock_irqsave(&conf->device_lock, flags);
+ 		r5c_flush_cache(conf, stripes_to_flush);
+ 		spin_unlock_irqrestore(&conf->device_lock, flags);
+ 	}
+ 
+ 	/* if log space is tight, flush stripes on stripe_in_journal_list */
+ 	if (test_bit(R5C_LOG_TIGHT, &conf->cache_state)) {
+ 		spin_lock_irqsave(&log->stripe_in_journal_lock, flags);
+ 		spin_lock(&conf->device_lock);
+ 		list_for_each_entry(sh, &log->stripe_in_journal_list, r5c) {
+ 			/*
+ 			 * stripes on stripe_in_journal_list could be in any
+ 			 * state of the stripe_cache state machine. In this
+ 			 * case, we only want to flush stripe on
+ 			 * r5c_cached_full/partial_stripes. The following
+ 			 * condition makes sure the stripe is on one of the
+ 			 * two lists.
+ 			 */
+ 			if (!list_empty(&sh->lru) &&
+ 			    !test_bit(STRIPE_HANDLE, &sh->state) &&
+ 			    atomic_read(&sh->count) == 0) {
+ 				r5c_flush_stripe(conf, sh);
+ 			}
+ 			if (count++ >= R5C_RECLAIM_STRIPE_GROUP)
+ 				break;
+ 		}
+ 		spin_unlock(&conf->device_lock);
+ 		spin_unlock_irqrestore(&log->stripe_in_journal_lock, flags);
+ 	}
+ 
+ 	if (!test_bit(R5C_LOG_CRITICAL, &conf->cache_state))
+ 		r5l_run_no_space_stripes(log);
+ 
+ 	md_wakeup_thread(conf->mddev->thread);
+ }
++>>>>>>> f687a33ef02d (md/r5cache: run_no_space_stripes() when R5C_LOG_CRITICAL == 0)
  
  static void r5l_do_reclaim(struct r5l_log *log)
  {
@@@ -1722,6 -2186,292 +2078,295 @@@ static void r5l_write_super(struct r5l_
  	set_bit(MD_CHANGE_DEVS, &mddev->flags);
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t r5c_journal_mode_show(struct mddev *mddev, char *page)
+ {
+ 	struct r5conf *conf = mddev->private;
+ 	int ret;
+ 
+ 	if (!conf->log)
+ 		return 0;
+ 
+ 	switch (conf->log->r5c_journal_mode) {
+ 	case R5C_JOURNAL_MODE_WRITE_THROUGH:
+ 		ret = snprintf(
+ 			page, PAGE_SIZE, "[%s] %s\n",
+ 			r5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_THROUGH],
+ 			r5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_BACK]);
+ 		break;
+ 	case R5C_JOURNAL_MODE_WRITE_BACK:
+ 		ret = snprintf(
+ 			page, PAGE_SIZE, "%s [%s]\n",
+ 			r5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_THROUGH],
+ 			r5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_BACK]);
+ 		break;
+ 	default:
+ 		ret = 0;
+ 	}
+ 	return ret;
+ }
+ 
+ static ssize_t r5c_journal_mode_store(struct mddev *mddev,
+ 				      const char *page, size_t length)
+ {
+ 	struct r5conf *conf = mddev->private;
+ 	struct r5l_log *log = conf->log;
+ 	int val = -1, i;
+ 	int len = length;
+ 
+ 	if (!log)
+ 		return -ENODEV;
+ 
+ 	if (len && page[len - 1] == '\n')
+ 		len -= 1;
+ 	for (i = 0; i < ARRAY_SIZE(r5c_journal_mode_str); i++)
+ 		if (strlen(r5c_journal_mode_str[i]) == len &&
+ 		    strncmp(page, r5c_journal_mode_str[i], len) == 0) {
+ 			val = i;
+ 			break;
+ 		}
+ 	if (val < R5C_JOURNAL_MODE_WRITE_THROUGH ||
+ 	    val > R5C_JOURNAL_MODE_WRITE_BACK)
+ 		return -EINVAL;
+ 
+ 	mddev_suspend(mddev);
+ 	conf->log->r5c_journal_mode = val;
+ 	mddev_resume(mddev);
+ 
+ 	pr_debug("md/raid:%s: setting r5c cache mode to %d: %s\n",
+ 		 mdname(mddev), val, r5c_journal_mode_str[val]);
+ 	return length;
+ }
+ 
+ struct md_sysfs_entry
+ r5c_journal_mode = __ATTR(journal_mode, 0644,
+ 			  r5c_journal_mode_show, r5c_journal_mode_store);
+ 
+ /*
+  * Try handle write operation in caching phase. This function should only
+  * be called in write-back mode.
+  *
+  * If all outstanding writes can be handled in caching phase, returns 0
+  * If writes requires write-out phase, call r5c_make_stripe_write_out()
+  * and returns -EAGAIN
+  */
+ int r5c_try_caching_write(struct r5conf *conf,
+ 			  struct stripe_head *sh,
+ 			  struct stripe_head_state *s,
+ 			  int disks)
+ {
+ 	struct r5l_log *log = conf->log;
+ 	int i;
+ 	struct r5dev *dev;
+ 	int to_cache = 0;
+ 
+ 	BUG_ON(!r5c_is_writeback(log));
+ 
+ 	if (!test_bit(STRIPE_R5C_CACHING, &sh->state)) {
+ 		/*
+ 		 * There are two different scenarios here:
+ 		 *  1. The stripe has some data cached, and it is sent to
+ 		 *     write-out phase for reclaim
+ 		 *  2. The stripe is clean, and this is the first write
+ 		 *
+ 		 * For 1, return -EAGAIN, so we continue with
+ 		 * handle_stripe_dirtying().
+ 		 *
+ 		 * For 2, set STRIPE_R5C_CACHING and continue with caching
+ 		 * write.
+ 		 */
+ 
+ 		/* case 1: anything injournal or anything in written */
+ 		if (s->injournal > 0 || s->written > 0)
+ 			return -EAGAIN;
+ 		/* case 2 */
+ 		set_bit(STRIPE_R5C_CACHING, &sh->state);
+ 	}
+ 
+ 	for (i = disks; i--; ) {
+ 		dev = &sh->dev[i];
+ 		/* if non-overwrite, use writing-out phase */
+ 		if (dev->towrite && !test_bit(R5_OVERWRITE, &dev->flags) &&
+ 		    !test_bit(R5_InJournal, &dev->flags)) {
+ 			r5c_make_stripe_write_out(sh);
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	for (i = disks; i--; ) {
+ 		dev = &sh->dev[i];
+ 		if (dev->towrite) {
+ 			set_bit(R5_Wantwrite, &dev->flags);
+ 			set_bit(R5_Wantdrain, &dev->flags);
+ 			set_bit(R5_LOCKED, &dev->flags);
+ 			to_cache++;
+ 		}
+ 	}
+ 
+ 	if (to_cache) {
+ 		set_bit(STRIPE_OP_BIODRAIN, &s->ops_request);
+ 		/*
+ 		 * set STRIPE_LOG_TRAPPED, which triggers r5c_cache_data()
+ 		 * in ops_run_io(). STRIPE_LOG_TRAPPED will be cleared in
+ 		 * r5c_handle_data_cached()
+ 		 */
+ 		set_bit(STRIPE_LOG_TRAPPED, &sh->state);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * free extra pages (orig_page) we allocated for prexor
+  */
+ void r5c_release_extra_page(struct stripe_head *sh)
+ {
+ 	struct r5conf *conf = sh->raid_conf;
+ 	int i;
+ 	bool using_disk_info_extra_page;
+ 
+ 	using_disk_info_extra_page =
+ 		sh->dev[0].orig_page == conf->disks[0].extra_page;
+ 
+ 	for (i = sh->disks; i--; )
+ 		if (sh->dev[i].page != sh->dev[i].orig_page) {
+ 			struct page *p = sh->dev[i].orig_page;
+ 
+ 			sh->dev[i].orig_page = sh->dev[i].page;
+ 			if (!using_disk_info_extra_page)
+ 				put_page(p);
+ 		}
+ 
+ 	if (using_disk_info_extra_page) {
+ 		clear_bit(R5C_EXTRA_PAGE_IN_USE, &conf->cache_state);
+ 		md_wakeup_thread(conf->mddev->thread);
+ 	}
+ }
+ 
+ void r5c_use_extra_page(struct stripe_head *sh)
+ {
+ 	struct r5conf *conf = sh->raid_conf;
+ 	int i;
+ 	struct r5dev *dev;
+ 
+ 	for (i = sh->disks; i--; ) {
+ 		dev = &sh->dev[i];
+ 		if (dev->orig_page != dev->page)
+ 			put_page(dev->orig_page);
+ 		dev->orig_page = conf->disks[i].extra_page;
+ 	}
+ }
+ 
+ /*
+  * clean up the stripe (clear R5_InJournal for dev[pd_idx] etc.) after the
+  * stripe is committed to RAID disks.
+  */
+ void r5c_finish_stripe_write_out(struct r5conf *conf,
+ 				 struct stripe_head *sh,
+ 				 struct stripe_head_state *s)
+ {
+ 	int i;
+ 	int do_wakeup = 0;
+ 
+ 	if (!conf->log ||
+ 	    !test_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags))
+ 		return;
+ 
+ 	WARN_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));
+ 	clear_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);
+ 
+ 	if (conf->log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)
+ 		return;
+ 
+ 	for (i = sh->disks; i--; ) {
+ 		clear_bit(R5_InJournal, &sh->dev[i].flags);
+ 		if (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))
+ 			do_wakeup = 1;
+ 	}
+ 
+ 	/*
+ 	 * analyse_stripe() runs before r5c_finish_stripe_write_out(),
+ 	 * We updated R5_InJournal, so we also update s->injournal.
+ 	 */
+ 	s->injournal = 0;
+ 
+ 	if (test_and_clear_bit(STRIPE_FULL_WRITE, &sh->state))
+ 		if (atomic_dec_and_test(&conf->pending_full_writes))
+ 			md_wakeup_thread(conf->mddev->thread);
+ 
+ 	if (do_wakeup)
+ 		wake_up(&conf->wait_for_overlap);
+ 
+ 	if (conf->log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)
+ 		return;
+ 
+ 	spin_lock_irq(&conf->log->stripe_in_journal_lock);
+ 	list_del_init(&sh->r5c);
+ 	spin_unlock_irq(&conf->log->stripe_in_journal_lock);
+ 	sh->log_start = MaxSector;
+ 	atomic_dec(&conf->log->stripe_in_journal_count);
+ 	r5c_update_log_state(conf->log);
+ }
+ 
+ int
+ r5c_cache_data(struct r5l_log *log, struct stripe_head *sh,
+ 	       struct stripe_head_state *s)
+ {
+ 	struct r5conf *conf = sh->raid_conf;
+ 	int pages = 0;
+ 	int reserve;
+ 	int i;
+ 	int ret = 0;
+ 
+ 	BUG_ON(!log);
+ 
+ 	for (i = 0; i < sh->disks; i++) {
+ 		void *addr;
+ 
+ 		if (!test_bit(R5_Wantwrite, &sh->dev[i].flags))
+ 			continue;
+ 		addr = kmap_atomic(sh->dev[i].page);
+ 		sh->dev[i].log_checksum = crc32c_le(log->uuid_checksum,
+ 						    addr, PAGE_SIZE);
+ 		kunmap_atomic(addr);
+ 		pages++;
+ 	}
+ 	WARN_ON(pages == 0);
+ 
+ 	/*
+ 	 * The stripe must enter state machine again to call endio, so
+ 	 * don't delay.
+ 	 */
+ 	clear_bit(STRIPE_DELAYED, &sh->state);
+ 	atomic_inc(&sh->count);
+ 
+ 	mutex_lock(&log->io_mutex);
+ 	/* meta + data */
+ 	reserve = (1 + pages) << (PAGE_SHIFT - 9);
+ 
+ 	if (test_bit(R5C_LOG_CRITICAL, &conf->cache_state) &&
+ 	    sh->log_start == MaxSector)
+ 		r5l_add_no_space_stripe(log, sh);
+ 	else if (!r5l_has_free_space(log, reserve)) {
+ 		if (sh->log_start == log->last_checkpoint)
+ 			BUG();
+ 		else
+ 			r5l_add_no_space_stripe(log, sh);
+ 	} else {
+ 		ret = r5l_log_stripe(log, sh, pages, 0);
+ 		if (ret) {
+ 			spin_lock_irq(&log->io_list_lock);
+ 			list_add_tail(&sh->log_list, &log->no_mem_stripes);
+ 			spin_unlock_irq(&log->io_list_lock);
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&log->io_mutex);
+ 	return 0;
+ }
+ 
++>>>>>>> f687a33ef02d (md/r5cache: run_no_space_stripes() when R5C_LOG_CRITICAL == 0)
  static int r5l_load_log(struct r5l_log *log)
  {
  	struct md_rdev *rdev = log->rdev;
* Unmerged path drivers/md/raid5-cache.c
