amd-xgbe: Perform priority-based hardware FIFO allocation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Lendacky, Thomas <Thomas.Lendacky@amd.com>
commit 43e0dcf7084014258ed555690fd6653f689ec368
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/43e0dcf7.failed

Allocate the FIFO across the hardware Rx queues based on the priority
of the queues.  Giving more FIFO resources to queues with a higher
priority.  If PFC is active but not enabled for a queue, then less
resources can allocated to the queue.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 43e0dcf7084014258ed555690fd6653f689ec368)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amd/xgbe/xgbe-dev.c
#	drivers/net/ethernet/amd/xgbe/xgbe.h
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-dev.c
index a748fd8a1c58,f8fffea6ced5..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
@@@ -121,6 -123,10 +121,13 @@@
  #include "xgbe.h"
  #include "xgbe-common.h"
  
++<<<<<<< HEAD
++=======
+ static inline unsigned int xgbe_get_max_frame(struct xgbe_prv_data *pdata)
+ {
+ 	return pdata->netdev->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;
+ }
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  
  static unsigned int xgbe_usec_to_riwt(struct xgbe_prv_data *pdata,
  				      unsigned int usec)
@@@ -335,6 -341,182 +342,185 @@@ static void xgbe_config_tso_mode(struc
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void xgbe_config_sph_mode(struct xgbe_prv_data *pdata)
+ {
+ 	struct xgbe_channel *channel;
+ 	unsigned int i;
+ 
+ 	channel = pdata->channel;
+ 	for (i = 0; i < pdata->channel_count; i++, channel++) {
+ 		if (!channel->rx_ring)
+ 			break;
+ 
+ 		XGMAC_DMA_IOWRITE_BITS(channel, DMA_CH_CR, SPH, 1);
+ 	}
+ 
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RCR, HDSMS, XGBE_SPH_HDSMS_SIZE);
+ }
+ 
+ static int xgbe_write_rss_reg(struct xgbe_prv_data *pdata, unsigned int type,
+ 			      unsigned int index, unsigned int val)
+ {
+ 	unsigned int wait;
+ 	int ret = 0;
+ 
+ 	mutex_lock(&pdata->rss_mutex);
+ 
+ 	if (XGMAC_IOREAD_BITS(pdata, MAC_RSSAR, OB)) {
+ 		ret = -EBUSY;
+ 		goto unlock;
+ 	}
+ 
+ 	XGMAC_IOWRITE(pdata, MAC_RSSDR, val);
+ 
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RSSAR, RSSIA, index);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RSSAR, ADDRT, type);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RSSAR, CT, 0);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RSSAR, OB, 1);
+ 
+ 	wait = 1000;
+ 	while (wait--) {
+ 		if (!XGMAC_IOREAD_BITS(pdata, MAC_RSSAR, OB))
+ 			goto unlock;
+ 
+ 		usleep_range(1000, 1500);
+ 	}
+ 
+ 	ret = -EBUSY;
+ 
+ unlock:
+ 	mutex_unlock(&pdata->rss_mutex);
+ 
+ 	return ret;
+ }
+ 
+ static int xgbe_write_rss_hash_key(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int key_regs = sizeof(pdata->rss_key) / sizeof(u32);
+ 	unsigned int *key = (unsigned int *)&pdata->rss_key;
+ 	int ret;
+ 
+ 	while (key_regs--) {
+ 		ret = xgbe_write_rss_reg(pdata, XGBE_RSS_HASH_KEY_TYPE,
+ 					 key_regs, *key++);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_write_rss_lookup_table(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int i;
+ 	int ret;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(pdata->rss_table); i++) {
+ 		ret = xgbe_write_rss_reg(pdata,
+ 					 XGBE_RSS_LOOKUP_TABLE_TYPE, i,
+ 					 pdata->rss_table[i]);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_rss_hash_key(struct xgbe_prv_data *pdata, const u8 *key)
+ {
+ 	memcpy(pdata->rss_key, key, sizeof(pdata->rss_key));
+ 
+ 	return xgbe_write_rss_hash_key(pdata);
+ }
+ 
+ static int xgbe_set_rss_lookup_table(struct xgbe_prv_data *pdata,
+ 				     const u32 *table)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(pdata->rss_table); i++)
+ 		XGMAC_SET_BITS(pdata->rss_table[i], MAC_RSSDR, DMCH, table[i]);
+ 
+ 	return xgbe_write_rss_lookup_table(pdata);
+ }
+ 
+ static int xgbe_enable_rss(struct xgbe_prv_data *pdata)
+ {
+ 	int ret;
+ 
+ 	if (!pdata->hw_feat.rss)
+ 		return -EOPNOTSUPP;
+ 
+ 	/* Program the hash key */
+ 	ret = xgbe_write_rss_hash_key(pdata);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Program the lookup table */
+ 	ret = xgbe_write_rss_lookup_table(pdata);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Set the RSS options */
+ 	XGMAC_IOWRITE(pdata, MAC_RSSCR, pdata->rss_options);
+ 
+ 	/* Enable RSS */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RSSCR, RSSE, 1);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_disable_rss(struct xgbe_prv_data *pdata)
+ {
+ 	if (!pdata->hw_feat.rss)
+ 		return -EOPNOTSUPP;
+ 
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RSSCR, RSSE, 0);
+ 
+ 	return 0;
+ }
+ 
+ static void xgbe_config_rss(struct xgbe_prv_data *pdata)
+ {
+ 	int ret;
+ 
+ 	if (!pdata->hw_feat.rss)
+ 		return;
+ 
+ 	if (pdata->netdev->features & NETIF_F_RXHASH)
+ 		ret = xgbe_enable_rss(pdata);
+ 	else
+ 		ret = xgbe_disable_rss(pdata);
+ 
+ 	if (ret)
+ 		netdev_err(pdata->netdev,
+ 			   "error configuring RSS, RSS disabled\n");
+ }
+ 
+ static bool xgbe_is_pfc_queue(struct xgbe_prv_data *pdata,
+ 			      unsigned int queue)
+ {
+ 	unsigned int prio, tc;
+ 
+ 	for (prio = 0; prio < IEEE_8021QAZ_MAX_TCS; prio++) {
+ 		/* Does this queue handle the priority? */
+ 		if (pdata->prio2q_map[prio] != queue)
+ 			continue;
+ 
+ 		/* Get the Traffic Class for this priority */
+ 		tc = pdata->ets->prio_tc[prio];
+ 
+ 		/* Check if PFC is enabled for this traffic class */
+ 		if (pdata->pfc->pfc_en & (1 << tc))
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  static int xgbe_disable_tx_flow_control(struct xgbe_prv_data *pdata)
  {
  	unsigned int max_q_count, q_count;
@@@ -367,8 -551,25 +553,30 @@@ static int xgbe_enable_tx_flow_control(
  	unsigned int i;
  
  	/* Set MTL flow control */
++<<<<<<< HEAD
 +	for (i = 0; i < pdata->hw_feat.rx_q_cnt; i++)
 +		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, EHFC, 1);
++=======
+ 	for (i = 0; i < pdata->rx_q_count; i++) {
+ 		unsigned int ehfc = 0;
+ 
+ 		if (pdata->rx_rfd[i]) {
+ 			/* Flow control thresholds are established */
+ 			if (pfc && ets) {
+ 				if (xgbe_is_pfc_queue(pdata, i))
+ 					ehfc = 1;
+ 			} else {
+ 				ehfc = 1;
+ 			}
+ 		}
+ 
+ 		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, EHFC, ehfc);
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev,
+ 			  "flow control %s for RXq%u\n",
+ 			  ehfc ? "enabled" : "disabled", i);
+ 	}
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  
  	/* Set MAC flow control */
  	max_q_count = XGMAC_MAX_FLOW_CONTROL_QUEUES;
@@@ -923,7 -1239,133 +1131,137 @@@ static void xgbe_rx_desc_init(struct xg
  	DBGPR("<--rx_desc_init\n");
  }
  
++<<<<<<< HEAD
 +static void xgbe_pre_xmit(struct xgbe_channel *channel)
++=======
+ static void xgbe_update_tstamp_addend(struct xgbe_prv_data *pdata,
+ 				      unsigned int addend)
+ {
+ 	/* Set the addend register value and tell the device */
+ 	XGMAC_IOWRITE(pdata, MAC_TSAR, addend);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_TSCR, TSADDREG, 1);
+ 
+ 	/* Wait for addend update to complete */
+ 	while (XGMAC_IOREAD_BITS(pdata, MAC_TSCR, TSADDREG))
+ 		udelay(5);
+ }
+ 
+ static void xgbe_set_tstamp_time(struct xgbe_prv_data *pdata, unsigned int sec,
+ 				 unsigned int nsec)
+ {
+ 	/* Set the time values and tell the device */
+ 	XGMAC_IOWRITE(pdata, MAC_STSUR, sec);
+ 	XGMAC_IOWRITE(pdata, MAC_STNUR, nsec);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_TSCR, TSINIT, 1);
+ 
+ 	/* Wait for time update to complete */
+ 	while (XGMAC_IOREAD_BITS(pdata, MAC_TSCR, TSINIT))
+ 		udelay(5);
+ }
+ 
+ static u64 xgbe_get_tstamp_time(struct xgbe_prv_data *pdata)
+ {
+ 	u64 nsec;
+ 
+ 	nsec = XGMAC_IOREAD(pdata, MAC_STSR);
+ 	nsec *= NSEC_PER_SEC;
+ 	nsec += XGMAC_IOREAD(pdata, MAC_STNR);
+ 
+ 	return nsec;
+ }
+ 
+ static u64 xgbe_get_tx_tstamp(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int tx_snr;
+ 	u64 nsec;
+ 
+ 	tx_snr = XGMAC_IOREAD(pdata, MAC_TXSNR);
+ 	if (XGMAC_GET_BITS(tx_snr, MAC_TXSNR, TXTSSTSMIS))
+ 		return 0;
+ 
+ 	nsec = XGMAC_IOREAD(pdata, MAC_TXSSR);
+ 	nsec *= NSEC_PER_SEC;
+ 	nsec += tx_snr;
+ 
+ 	return nsec;
+ }
+ 
+ static void xgbe_get_rx_tstamp(struct xgbe_packet_data *packet,
+ 			       struct xgbe_ring_desc *rdesc)
+ {
+ 	u64 nsec;
+ 
+ 	if (XGMAC_GET_BITS_LE(rdesc->desc3, RX_CONTEXT_DESC3, TSA) &&
+ 	    !XGMAC_GET_BITS_LE(rdesc->desc3, RX_CONTEXT_DESC3, TSD)) {
+ 		nsec = le32_to_cpu(rdesc->desc1);
+ 		nsec <<= 32;
+ 		nsec |= le32_to_cpu(rdesc->desc0);
+ 		if (nsec != 0xffffffffffffffffULL) {
+ 			packet->rx_tstamp = nsec;
+ 			XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,
+ 				       RX_TSTAMP, 1);
+ 		}
+ 	}
+ }
+ 
+ static int xgbe_config_tstamp(struct xgbe_prv_data *pdata,
+ 			      unsigned int mac_tscr)
+ {
+ 	/* Set one nano-second accuracy */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSCTRLSSR, 1);
+ 
+ 	/* Set fine timestamp update */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSCFUPDT, 1);
+ 
+ 	/* Overwrite earlier timestamps */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TXTSSTSM, 1);
+ 
+ 	XGMAC_IOWRITE(pdata, MAC_TSCR, mac_tscr);
+ 
+ 	/* Exit if timestamping is not enabled */
+ 	if (!XGMAC_GET_BITS(mac_tscr, MAC_TSCR, TSENA))
+ 		return 0;
+ 
+ 	/* Initialize time registers */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_SSIR, SSINC, XGBE_TSTAMP_SSINC);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_SSIR, SNSINC, XGBE_TSTAMP_SNSINC);
+ 	xgbe_update_tstamp_addend(pdata, pdata->tstamp_addend);
+ 	xgbe_set_tstamp_time(pdata, 0, 0);
+ 
+ 	/* Initialize the timecounter */
+ 	timecounter_init(&pdata->tstamp_tc, &pdata->tstamp_cc,
+ 			 ktime_to_ns(ktime_get_real()));
+ 
+ 	return 0;
+ }
+ 
+ static void xgbe_tx_start_xmit(struct xgbe_channel *channel,
+ 			       struct xgbe_ring *ring)
+ {
+ 	struct xgbe_prv_data *pdata = channel->pdata;
+ 	struct xgbe_ring_data *rdata;
+ 
+ 	/* Make sure everything is written before the register write */
+ 	wmb();
+ 
+ 	/* Issue a poll command to Tx DMA by writing address
+ 	 * of next immediate free descriptor */
+ 	rdata = XGBE_GET_DESC_DATA(ring, ring->cur);
+ 	XGMAC_DMA_IOWRITE(channel, DMA_CH_TDTR_LO,
+ 			  lower_32_bits(rdata->rdesc_dma));
+ 
+ 	/* Start the Tx timer */
+ 	if (pdata->tx_usecs && !channel->tx_timer_active) {
+ 		channel->tx_timer_active = 1;
+ 		mod_timer(&channel->tx_timer,
+ 			  jiffies + usecs_to_jiffies(pdata->tx_usecs));
+ 	}
+ 
+ 	ring->tx.xmit_more = 0;
+ }
+ 
+ static void xgbe_dev_xmit(struct xgbe_channel *channel)
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  {
  	struct xgbe_prv_data *pdata = channel->pdata;
  	struct xgbe_ring *ring = channel->tx_ring;
@@@ -1407,118 -1913,405 +1745,495 @@@ static void xgbe_config_mtl_mode(struc
  	XGMAC_IOWRITE_BITS(pdata, MTL_OMR, RAA, MTL_RAA_SP);
  }
  
++<<<<<<< HEAD
 +static unsigned int xgbe_calculate_per_queue_fifo(unsigned int fifo_size,
 +						  unsigned int queue_count)
++=======
+ static void xgbe_queue_flow_control_threshold(struct xgbe_prv_data *pdata,
+ 					      unsigned int queue,
+ 					      unsigned int q_fifo_size)
+ {
+ 	unsigned int frame_fifo_size;
+ 	unsigned int rfa, rfd;
+ 
+ 	frame_fifo_size = XGMAC_FLOW_CONTROL_ALIGN(xgbe_get_max_frame(pdata));
+ 
+ 	if (pdata->pfcq[queue] && (q_fifo_size > pdata->pfc_rfa)) {
+ 		/* PFC is active for this queue */
+ 		rfa = pdata->pfc_rfa;
+ 		rfd = rfa + frame_fifo_size;
+ 		if (rfd > XGMAC_FLOW_CONTROL_MAX)
+ 			rfd = XGMAC_FLOW_CONTROL_MAX;
+ 		if (rfa >= XGMAC_FLOW_CONTROL_MAX)
+ 			rfa = XGMAC_FLOW_CONTROL_MAX - XGMAC_FLOW_CONTROL_UNIT;
+ 	} else {
+ 		/* This path deals with just maximum frame sizes which are
+ 		 * limited to a jumbo frame of 9,000 (plus headers, etc.)
+ 		 * so we can never exceed the maximum allowable RFA/RFD
+ 		 * values.
+ 		 */
+ 		if (q_fifo_size <= 2048) {
+ 			/* rx_rfd to zero to signal no flow control */
+ 			pdata->rx_rfa[queue] = 0;
+ 			pdata->rx_rfd[queue] = 0;
+ 			return;
+ 		}
+ 
+ 		if (q_fifo_size <= 4096) {
+ 			/* Between 2048 and 4096 */
+ 			pdata->rx_rfa[queue] = 0;	/* Full - 1024 bytes */
+ 			pdata->rx_rfd[queue] = 1;	/* Full - 1536 bytes */
+ 			return;
+ 		}
+ 
+ 		if (q_fifo_size <= frame_fifo_size) {
+ 			/* Between 4096 and max-frame */
+ 			pdata->rx_rfa[queue] = 2;	/* Full - 2048 bytes */
+ 			pdata->rx_rfd[queue] = 5;	/* Full - 3584 bytes */
+ 			return;
+ 		}
+ 
+ 		if (q_fifo_size <= (frame_fifo_size * 3)) {
+ 			/* Between max-frame and 3 max-frames,
+ 			 * trigger if we get just over a frame of data and
+ 			 * resume when we have just under half a frame left.
+ 			 */
+ 			rfa = q_fifo_size - frame_fifo_size;
+ 			rfd = rfa + (frame_fifo_size / 2);
+ 		} else {
+ 			/* Above 3 max-frames - trigger when just over
+ 			 * 2 frames of space available
+ 			 */
+ 			rfa = frame_fifo_size * 2;
+ 			rfa += XGMAC_FLOW_CONTROL_UNIT;
+ 			rfd = rfa + frame_fifo_size;
+ 		}
+ 	}
+ 
+ 	pdata->rx_rfa[queue] = XGMAC_FLOW_CONTROL_VALUE(rfa);
+ 	pdata->rx_rfd[queue] = XGMAC_FLOW_CONTROL_VALUE(rfd);
+ }
+ 
+ static void xgbe_calculate_flow_control_threshold(struct xgbe_prv_data *pdata,
+ 						  unsigned int *fifo)
+ {
+ 	unsigned int q_fifo_size;
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < pdata->rx_q_count; i++) {
+ 		q_fifo_size = (fifo[i] + 1) * XGMAC_FIFO_UNIT;
+ 
+ 		xgbe_queue_flow_control_threshold(pdata, i, q_fifo_size);
+ 	}
+ }
+ 
+ static void xgbe_config_flow_control_threshold(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < pdata->rx_q_count; i++) {
+ 		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQFCR, RFA,
+ 				       pdata->rx_rfa[i]);
+ 		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQFCR, RFD,
+ 				       pdata->rx_rfd[i]);
+ 	}
+ }
+ 
+ static unsigned int xgbe_get_tx_fifo_size(struct xgbe_prv_data *pdata)
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  {
 -	unsigned int fifo_size;
 -
 -	/* Calculate the configured fifo size */
 -	fifo_size = 1 << (pdata->hw_feat.tx_fifo_size + 7);
 -
 -	/* The configured value may not be the actual amount of fifo RAM */
 -	return min_t(unsigned int, XGMAC_FIFO_TX_MAX, fifo_size);
 -}
 -
 -static unsigned int xgbe_get_rx_fifo_size(struct xgbe_prv_data *pdata)
 -{
 -	unsigned int fifo_size;
 +	unsigned int q_fifo_size = 0;
 +	enum xgbe_mtl_fifo_size p_fifo = XGMAC_MTL_FIFO_SIZE_256;
  
 -	/* Calculate the configured fifo size */
 -	fifo_size = 1 << (pdata->hw_feat.rx_fifo_size + 7);
 +	/* Calculate Tx/Rx fifo share per queue */
 +	switch (fifo_size) {
 +	case 0:
 +		q_fifo_size = XGBE_FIFO_SIZE_B(128);
 +		break;
 +	case 1:
 +		q_fifo_size = XGBE_FIFO_SIZE_B(256);
 +		break;
 +	case 2:
 +		q_fifo_size = XGBE_FIFO_SIZE_B(512);
 +		break;
 +	case 3:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(1);
 +		break;
 +	case 4:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(2);
 +		break;
 +	case 5:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(4);
 +		break;
 +	case 6:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(8);
 +		break;
 +	case 7:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(16);
 +		break;
 +	case 8:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(32);
 +		break;
 +	case 9:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(64);
 +		break;
 +	case 10:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(128);
 +		break;
 +	case 11:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(256);
 +		break;
 +	}
  
 +	/* The configured value is not the actual amount of fifo RAM */
 +	q_fifo_size = min_t(unsigned int, XGBE_FIFO_MAX, q_fifo_size);
 +
 +	q_fifo_size = q_fifo_size / queue_count;
 +
 +	/* Set the queue fifo size programmable value */
 +	if (q_fifo_size >= XGBE_FIFO_SIZE_KB(256))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_256K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(128))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_128K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(64))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_64K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(32))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_32K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(16))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_16K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(8))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_8K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(4))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_4K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(2))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_2K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(1))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_1K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_B(512))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_512;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_B(256))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_256;
 +
++<<<<<<< HEAD
 +	return p_fifo;
++=======
+ 	/* The configured value may not be the actual amount of fifo RAM */
+ 	return min_t(unsigned int, XGMAC_FIFO_RX_MAX, fifo_size);
+ }
+ 
+ static void xgbe_calculate_equal_fifo(unsigned int fifo_size,
+ 				      unsigned int queue_count,
+ 				      unsigned int *fifo)
+ {
+ 	unsigned int q_fifo_size;
+ 	unsigned int p_fifo;
+ 	unsigned int i;
+ 
+ 	q_fifo_size = fifo_size / queue_count;
+ 
+ 	/* Calculate the fifo setting by dividing the queue's fifo size
+ 	 * by the fifo allocation increment (with 0 representing the
+ 	 * base allocation increment so decrement the result by 1).
+ 	 */
+ 	p_fifo = q_fifo_size / XGMAC_FIFO_UNIT;
+ 	if (p_fifo)
+ 		p_fifo--;
+ 
+ 	/* Distribute the fifo equally amongst the queues */
+ 	for (i = 0; i < queue_count; i++)
+ 		fifo[i] = p_fifo;
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
+ }
+ 
+ static unsigned int xgbe_set_nonprio_fifos(unsigned int fifo_size,
+ 					   unsigned int queue_count,
+ 					   unsigned int *fifo)
+ {
+ 	unsigned int i;
+ 
+ 	BUILD_BUG_ON_NOT_POWER_OF_2(XGMAC_FIFO_MIN_ALLOC);
+ 
+ 	if (queue_count <= IEEE_8021QAZ_MAX_TCS)
+ 		return fifo_size;
+ 
+ 	/* Rx queues 9 and up are for specialized packets,
+ 	 * such as PTP or DCB control packets, etc. and
+ 	 * don't require a large fifo
+ 	 */
+ 	for (i = IEEE_8021QAZ_MAX_TCS; i < queue_count; i++) {
+ 		fifo[i] = (XGMAC_FIFO_MIN_ALLOC / XGMAC_FIFO_UNIT) - 1;
+ 		fifo_size -= XGMAC_FIFO_MIN_ALLOC;
+ 	}
+ 
+ 	return fifo_size;
+ }
+ 
+ static unsigned int xgbe_get_pfc_delay(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int delay;
+ 
+ 	/* If a delay has been provided, use that */
+ 	if (pdata->pfc->delay)
+ 		return pdata->pfc->delay / 8;
+ 
+ 	/* Allow for two maximum size frames */
+ 	delay = xgbe_get_max_frame(pdata);
+ 	delay += XGMAC_ETH_PREAMBLE;
+ 	delay *= 2;
+ 
+ 	/* Allow for PFC frame */
+ 	delay += XGMAC_PFC_DATA_LEN;
+ 	delay += ETH_HLEN + ETH_FCS_LEN;
+ 	delay += XGMAC_ETH_PREAMBLE;
+ 
+ 	/* Allow for miscellaneous delays (LPI exit, cable, etc.) */
+ 	delay += XGMAC_PFC_DELAYS;
+ 
+ 	return delay;
+ }
+ 
+ static unsigned int xgbe_get_pfc_queues(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int count, prio_queues;
+ 	unsigned int i;
+ 
+ 	if (!pdata->pfc->pfc_en)
+ 		return 0;
+ 
+ 	count = 0;
+ 	prio_queues = XGMAC_PRIO_QUEUES(pdata->rx_q_count);
+ 	for (i = 0; i < prio_queues; i++) {
+ 		if (!xgbe_is_pfc_queue(pdata, i))
+ 			continue;
+ 
+ 		pdata->pfcq[i] = 1;
+ 		count++;
+ 	}
+ 
+ 	return count;
+ }
+ 
+ static void xgbe_calculate_dcb_fifo(struct xgbe_prv_data *pdata,
+ 				    unsigned int fifo_size,
+ 				    unsigned int *fifo)
+ {
+ 	unsigned int q_fifo_size, rem_fifo, addn_fifo;
+ 	unsigned int prio_queues;
+ 	unsigned int pfc_count;
+ 	unsigned int i;
+ 
+ 	q_fifo_size = XGMAC_FIFO_ALIGN(xgbe_get_max_frame(pdata));
+ 	prio_queues = XGMAC_PRIO_QUEUES(pdata->rx_q_count);
+ 	pfc_count = xgbe_get_pfc_queues(pdata);
+ 
+ 	if (!pfc_count || ((q_fifo_size * prio_queues) > fifo_size)) {
+ 		/* No traffic classes with PFC enabled or can't do lossless */
+ 		xgbe_calculate_equal_fifo(fifo_size, prio_queues, fifo);
+ 		return;
+ 	}
+ 
+ 	/* Calculate how much fifo we have to play with */
+ 	rem_fifo = fifo_size - (q_fifo_size * prio_queues);
+ 
+ 	/* Calculate how much more than base fifo PFC needs, which also
+ 	 * becomes the threshold activation point (RFA)
+ 	 */
+ 	pdata->pfc_rfa = xgbe_get_pfc_delay(pdata);
+ 	pdata->pfc_rfa = XGMAC_FLOW_CONTROL_ALIGN(pdata->pfc_rfa);
+ 
+ 	if (pdata->pfc_rfa > q_fifo_size) {
+ 		addn_fifo = pdata->pfc_rfa - q_fifo_size;
+ 		addn_fifo = XGMAC_FIFO_ALIGN(addn_fifo);
+ 	} else {
+ 		addn_fifo = 0;
+ 	}
+ 
+ 	/* Calculate DCB fifo settings:
+ 	 *   - distribute remaining fifo between the VLAN priority
+ 	 *     queues based on traffic class PFC enablement and overall
+ 	 *     priority (0 is lowest priority, so start at highest)
+ 	 */
+ 	i = prio_queues;
+ 	while (i > 0) {
+ 		i--;
+ 
+ 		fifo[i] = (q_fifo_size / XGMAC_FIFO_UNIT) - 1;
+ 
+ 		if (!pdata->pfcq[i] || !addn_fifo)
+ 			continue;
+ 
+ 		if (addn_fifo > rem_fifo) {
+ 			netdev_warn(pdata->netdev,
+ 				    "RXq%u cannot set needed fifo size\n", i);
+ 			if (!rem_fifo)
+ 				continue;
+ 
+ 			addn_fifo = rem_fifo;
+ 		}
+ 
+ 		fifo[i] += (addn_fifo / XGMAC_FIFO_UNIT);
+ 		rem_fifo -= addn_fifo;
+ 	}
+ 
+ 	if (rem_fifo) {
+ 		unsigned int inc_fifo = rem_fifo / prio_queues;
+ 
+ 		/* Distribute remaining fifo across queues */
+ 		for (i = 0; i < prio_queues; i++)
+ 			fifo[i] += (inc_fifo / XGMAC_FIFO_UNIT);
+ 	}
  }
  
  static void xgbe_config_tx_fifo_size(struct xgbe_prv_data *pdata)
  {
 -	unsigned int fifo_size;
 -	unsigned int fifo[XGBE_MAX_QUEUES];
 +	enum xgbe_mtl_fifo_size fifo_size;
  	unsigned int i;
  
 -	fifo_size = xgbe_get_tx_fifo_size(pdata);
 +	fifo_size = xgbe_calculate_per_queue_fifo(pdata->hw_feat.tx_fifo_size,
 +						  pdata->hw_feat.tx_q_cnt);
  
 -	xgbe_calculate_equal_fifo(fifo_size, pdata->tx_q_count, fifo);
 +	for (i = 0; i < pdata->hw_feat.tx_q_cnt; i++)
 +		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_TQOMR, TQS, fifo_size);
  
++<<<<<<< HEAD
 +	netdev_notice(pdata->netdev, "%d Tx queues, %d byte fifo per queue\n",
 +		      pdata->hw_feat.tx_q_cnt, ((fifo_size + 1) * 256));
++=======
+ 	for (i = 0; i < pdata->tx_q_count; i++)
+ 		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_TQOMR, TQS, fifo[i]);
+ 
+ 	netif_info(pdata, drv, pdata->netdev,
+ 		   "%d Tx hardware queues, %d byte fifo per queue\n",
+ 		   pdata->tx_q_count, ((fifo[0] + 1) * XGMAC_FIFO_UNIT));
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  }
  
  static void xgbe_config_rx_fifo_size(struct xgbe_prv_data *pdata)
  {
++<<<<<<< HEAD
 +	enum xgbe_mtl_fifo_size fifo_size;
 +	unsigned int i;
 +
 +	fifo_size = xgbe_calculate_per_queue_fifo(pdata->hw_feat.rx_fifo_size,
 +						  pdata->hw_feat.rx_q_cnt);
 +
 +	for (i = 0; i < pdata->hw_feat.rx_q_cnt; i++)
 +		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, RQS, fifo_size);
 +
 +	netdev_notice(pdata->netdev, "%d Rx queues, %d byte fifo per queue\n",
 +		      pdata->hw_feat.rx_q_cnt, ((fifo_size + 1) * 256));
++=======
+ 	unsigned int fifo_size;
+ 	unsigned int fifo[XGBE_MAX_QUEUES];
+ 	unsigned int prio_queues;
+ 	unsigned int i;
+ 
+ 	/* Clear any DCB related fifo/queue information */
+ 	memset(pdata->pfcq, 0, sizeof(pdata->pfcq));
+ 	pdata->pfc_rfa = 0;
+ 
+ 	fifo_size = xgbe_get_rx_fifo_size(pdata);
+ 	prio_queues = XGMAC_PRIO_QUEUES(pdata->rx_q_count);
+ 
+ 	/* Assign a minimum fifo to the non-VLAN priority queues */
+ 	fifo_size = xgbe_set_nonprio_fifos(fifo_size, pdata->rx_q_count, fifo);
+ 
+ 	if (pdata->pfc && pdata->ets)
+ 		xgbe_calculate_dcb_fifo(pdata, fifo_size, fifo);
+ 	else
+ 		xgbe_calculate_equal_fifo(fifo_size, prio_queues, fifo);
+ 
+ 	for (i = 0; i < pdata->rx_q_count; i++)
+ 		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, RQS, fifo[i]);
+ 
+ 	xgbe_calculate_flow_control_threshold(pdata, fifo);
+ 	xgbe_config_flow_control_threshold(pdata);
+ 
+ 	if (pdata->pfc && pdata->ets && pdata->pfc->pfc_en) {
+ 		netif_info(pdata, drv, pdata->netdev,
+ 			   "%u Rx hardware queues\n", pdata->rx_q_count);
+ 		for (i = 0; i < pdata->rx_q_count; i++)
+ 			netif_info(pdata, drv, pdata->netdev,
+ 				   "RxQ%u, %u byte fifo queue\n", i,
+ 				   ((fifo[i] + 1) * XGMAC_FIFO_UNIT));
+ 	} else {
+ 		netif_info(pdata, drv, pdata->netdev,
+ 			   "%u Rx hardware queues, %u byte fifo per queue\n",
+ 			   pdata->rx_q_count,
+ 			   ((fifo[0] + 1) * XGMAC_FIFO_UNIT));
+ 	}
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  }
  
 -static void xgbe_config_queue_mapping(struct xgbe_prv_data *pdata)
 +static void xgbe_config_rx_queue_mapping(struct xgbe_prv_data *pdata)
  {
++<<<<<<< HEAD
 +	unsigned int i, reg, reg_val;
 +	unsigned int q_count = pdata->hw_feat.rx_q_cnt;
++=======
+ 	unsigned int qptc, qptc_extra, queue;
+ 	unsigned int prio_queues;
+ 	unsigned int ppq, ppq_extra, prio;
+ 	unsigned int mask;
+ 	unsigned int i, j, reg, reg_val;
+ 
+ 	/* Map the MTL Tx Queues to Traffic Classes
+ 	 *   Note: Tx Queues >= Traffic Classes
+ 	 */
+ 	qptc = pdata->tx_q_count / pdata->hw_feat.tc_cnt;
+ 	qptc_extra = pdata->tx_q_count % pdata->hw_feat.tc_cnt;
+ 
+ 	for (i = 0, queue = 0; i < pdata->hw_feat.tc_cnt; i++) {
+ 		for (j = 0; j < qptc; j++) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TXq%u mapped to TC%u\n", queue, i);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, queue, MTL_Q_TQOMR,
+ 					       Q2TCMAP, i);
+ 			pdata->q2tc_map[queue++] = i;
+ 		}
+ 
+ 		if (i < qptc_extra) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TXq%u mapped to TC%u\n", queue, i);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, queue, MTL_Q_TQOMR,
+ 					       Q2TCMAP, i);
+ 			pdata->q2tc_map[queue++] = i;
+ 		}
+ 	}
+ 
+ 	/* Map the 8 VLAN priority values to available MTL Rx queues */
+ 	prio_queues = XGMAC_PRIO_QUEUES(pdata->rx_q_count);
+ 	ppq = IEEE_8021QAZ_MAX_TCS / prio_queues;
+ 	ppq_extra = IEEE_8021QAZ_MAX_TCS % prio_queues;
+ 
+ 	reg = MAC_RQC2R;
+ 	reg_val = 0;
+ 	for (i = 0, prio = 0; i < prio_queues;) {
+ 		mask = 0;
+ 		for (j = 0; j < ppq; j++) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "PRIO%u mapped to RXq%u\n", prio, i);
+ 			mask |= (1 << prio);
+ 			pdata->prio2q_map[prio++] = i;
+ 		}
+ 
+ 		if (i < ppq_extra) {
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "PRIO%u mapped to RXq%u\n", prio, i);
+ 			mask |= (1 << prio);
+ 			pdata->prio2q_map[prio++] = i;
+ 		}
+ 
+ 		reg_val |= (mask << ((i++ % MAC_RQC2_Q_PER_REG) << 3));
+ 
+ 		if ((i % MAC_RQC2_Q_PER_REG) && (i != prio_queues))
+ 			continue;
+ 
+ 		XGMAC_IOWRITE(pdata, reg, reg_val);
+ 		reg += MAC_RQC2_INC;
+ 		reg_val = 0;
+ 	}
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  
  	/* Select dynamic mapping of MTL Rx queue to DMA Rx channel */
  	reg = MTL_RQDCM0R;
@@@ -1536,16 -2329,120 +2251,129 @@@
  	}
  }
  
- static void xgbe_config_flow_control_threshold(struct xgbe_prv_data *pdata)
+ static void xgbe_config_tc(struct xgbe_prv_data *pdata)
  {
- 	unsigned int i;
+ 	unsigned int offset, queue, prio;
+ 	u8 i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < pdata->hw_feat.rx_q_cnt; i++) {
 +		/* Activate flow control when less than 4k left in fifo */
 +		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, RFA, 2);
 +
 +		/* De-activate flow control when more than 6k left in fifo */
 +		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, RFD, 4);
++=======
+ 	netdev_reset_tc(pdata->netdev);
+ 	if (!pdata->num_tcs)
+ 		return;
+ 
+ 	netdev_set_num_tc(pdata->netdev, pdata->num_tcs);
+ 
+ 	for (i = 0, queue = 0, offset = 0; i < pdata->num_tcs; i++) {
+ 		while ((queue < pdata->tx_q_count) &&
+ 		       (pdata->q2tc_map[queue] == i))
+ 			queue++;
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev, "TC%u using TXq%u-%u\n",
+ 			  i, offset, queue - 1);
+ 		netdev_set_tc_queue(pdata->netdev, i, queue - offset, offset);
+ 		offset = queue;
+ 	}
+ 
+ 	if (!pdata->ets)
+ 		return;
+ 
+ 	for (prio = 0; prio < IEEE_8021QAZ_MAX_TCS; prio++)
+ 		netdev_set_prio_tc_map(pdata->netdev, prio,
+ 				       pdata->ets->prio_tc[prio]);
+ }
+ 
+ static void xgbe_config_dcb_tc(struct xgbe_prv_data *pdata)
+ {
+ 	struct ieee_ets *ets = pdata->ets;
+ 	unsigned int total_weight, min_weight, weight;
+ 	unsigned int mask, reg, reg_val;
+ 	unsigned int i, prio;
+ 
+ 	if (!ets)
+ 		return;
+ 
+ 	/* Set Tx to deficit weighted round robin scheduling algorithm (when
+ 	 * traffic class is using ETS algorithm)
+ 	 */
+ 	XGMAC_IOWRITE_BITS(pdata, MTL_OMR, ETSALG, MTL_ETSALG_DWRR);
+ 
+ 	/* Set Traffic Class algorithms */
+ 	total_weight = pdata->netdev->mtu * pdata->hw_feat.tc_cnt;
+ 	min_weight = total_weight / 100;
+ 	if (!min_weight)
+ 		min_weight = 1;
+ 
+ 	for (i = 0; i < pdata->hw_feat.tc_cnt; i++) {
+ 		/* Map the priorities to the traffic class */
+ 		mask = 0;
+ 		for (prio = 0; prio < IEEE_8021QAZ_MAX_TCS; prio++) {
+ 			if (ets->prio_tc[prio] == i)
+ 				mask |= (1 << prio);
+ 		}
+ 		mask &= 0xff;
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev, "TC%u PRIO mask=%#x\n",
+ 			  i, mask);
+ 		reg = MTL_TCPM0R + (MTL_TCPM_INC * (i / MTL_TCPM_TC_PER_REG));
+ 		reg_val = XGMAC_IOREAD(pdata, reg);
+ 
+ 		reg_val &= ~(0xff << ((i % MTL_TCPM_TC_PER_REG) << 3));
+ 		reg_val |= (mask << ((i % MTL_TCPM_TC_PER_REG) << 3));
+ 
+ 		XGMAC_IOWRITE(pdata, reg, reg_val);
+ 
+ 		/* Set the traffic class algorithm */
+ 		switch (ets->tc_tsa[i]) {
+ 		case IEEE_8021QAZ_TSA_STRICT:
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TC%u using SP\n", i);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_ETSCR, TSA,
+ 					       MTL_TSA_SP);
+ 			break;
+ 		case IEEE_8021QAZ_TSA_ETS:
+ 			weight = total_weight * ets->tc_tx_bw[i] / 100;
+ 			weight = clamp(weight, min_weight, total_weight);
+ 
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TC%u using DWRR (weight %u)\n", i, weight);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_ETSCR, TSA,
+ 					       MTL_TSA_ETS);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_QWR, QW,
+ 					       weight);
+ 			break;
+ 		}
+ 	}
+ 
+ 	xgbe_config_tc(pdata);
+ }
+ 
+ static void xgbe_config_dcb_pfc(struct xgbe_prv_data *pdata)
+ {
+ 	if (!test_bit(XGBE_DOWN, &pdata->dev_state)) {
+ 		/* Just stop the Tx queues while Rx fifo is changed */
+ 		netif_tx_stop_all_queues(pdata->netdev);
+ 
+ 		/* Suspend Rx so that fifo's can be adjusted */
+ 		pdata->hw_if.disable_rx(pdata);
+ 	}
+ 
+ 	xgbe_config_rx_fifo_size(pdata);
+ 	xgbe_config_flow_control(pdata);
+ 
+ 	if (!test_bit(XGBE_DOWN, &pdata->dev_state)) {
+ 		/* Resume Rx */
+ 		pdata->hw_if.enable_rx(pdata);
+ 
+ 		/* Resume Tx queues */
+ 		netif_tx_start_all_queues(pdata->netdev);
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  	}
  }
  
@@@ -2149,16 -3150,12 +2977,23 @@@ static int xgbe_init(struct xgbe_prv_da
  	xgbe_config_rx_threshold(pdata, pdata->rx_threshold);
  	xgbe_config_tx_fifo_size(pdata);
  	xgbe_config_rx_fifo_size(pdata);
++<<<<<<< HEAD
 +	xgbe_config_flow_control_threshold(pdata);
 +	/*TODO: Queue to Traffic Class Mapping (Q2TCMAP) */
 +	/*TODO: Error Packet and undersized good Packet forwarding enable
 +		(FEP and FUP)
 +	 */
++=======
+ 	/*TODO: Error Packet and undersized good Packet forwarding enable
+ 		(FEP and FUP)
+ 	 */
+ 	xgbe_config_dcb_tc(pdata);
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  	xgbe_enable_mtl_interrupts(pdata);
  
 +	/* Transmit Class Weight */
 +	XGMAC_IOWRITE_BITS(pdata, MTL_Q_TCQWR, QW, 0x10);
 +
  	/*
  	 * Initialize MAC related features
  	 */
diff --cc drivers/net/ethernet/amd/xgbe/xgbe.h
index 1903f878545a,a4e1b8d01491..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe.h
+++ b/drivers/net/ethernet/amd/xgbe/xgbe.h
@@@ -135,10 -140,26 +135,16 @@@
  
  #define XGBE_TX_MAX_BUF_SIZE	(0x3fff & ~(64 - 1))
  
 -/* Descriptors required for maximum contiguous TSO/GSO packet */
 -#define XGBE_TX_MAX_SPLIT	((GSO_MAX_SIZE / XGBE_TX_MAX_BUF_SIZE) + 1)
 -
 -/* Maximum possible descriptors needed for an SKB:
 - * - Maximum number of SKB frags
 - * - Maximum descriptors for contiguous TSO/GSO packet
 - * - Possible context descriptor
 - * - Possible TSO header descriptor
 - */
 -#define XGBE_TX_MAX_DESCS	(MAX_SKB_FRAGS + XGBE_TX_MAX_SPLIT + 2)
 -
  #define XGBE_RX_MIN_BUF_SIZE	(ETH_FRAME_LEN + ETH_FCS_LEN + VLAN_HLEN)
  #define XGBE_RX_BUF_ALIGN	64
 -#define XGBE_SKB_ALLOC_SIZE	256
 -#define XGBE_SPH_HDSMS_SIZE	2	/* Keep in sync with SKB_ALLOC_SIZE */
  
  #define XGBE_MAX_DMA_CHANNELS	16
++<<<<<<< HEAD
++=======
+ #define XGBE_MAX_QUEUES		16
+ #define XGBE_PRIORITY_QUEUES	8
+ #define XGBE_DMA_STOP_TIMEOUT	5
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  
  /* DMA cache settings - Outer sharable, write-back, write-allocate */
  #define XGBE_DMA_OS_AXDOMAIN	0x2
@@@ -157,20 -178,54 +163,38 @@@
  #define XGMAC_MAX_STD_PACKET	1518
  #define XGMAC_JUMBO_PACKET_MTU	9000
  #define XGMAC_MAX_JUMBO_PACKET	9018
+ #define XGMAC_ETH_PREAMBLE	(12 + 8)	/* Inter-frame gap + preamble */
+ 
+ #define XGMAC_PFC_DATA_LEN	46
+ #define XGMAC_PFC_DELAYS	14000
+ 
+ #define XGMAC_PRIO_QUEUES(_cnt)					\
+ 	min_t(unsigned int, IEEE_8021QAZ_MAX_TCS, (_cnt))
  
 -/* Common property names */
 -#define XGBE_MAC_ADDR_PROPERTY	"mac-address"
 -#define XGBE_PHY_MODE_PROPERTY	"phy-mode"
 -#define XGBE_DMA_IRQS_PROPERTY	"amd,per-channel-interrupt"
 -#define XGBE_SPEEDSET_PROPERTY	"amd,speed-set"
 -#define XGBE_BLWC_PROPERTY	"amd,serdes-blwc"
 -#define XGBE_CDR_RATE_PROPERTY	"amd,serdes-cdr-rate"
 -#define XGBE_PQ_SKEW_PROPERTY	"amd,serdes-pq-skew"
 -#define XGBE_TX_AMP_PROPERTY	"amd,serdes-tx-amp"
 -#define XGBE_DFE_CFG_PROPERTY	"amd,serdes-dfe-tap-config"
 -#define XGBE_DFE_ENA_PROPERTY	"amd,serdes-dfe-tap-enable"
 -
 -/* Device-tree clock names */
 -#define XGBE_DMA_CLOCK		"dma_clk"
 -#define XGBE_PTP_CLOCK		"ptp_clk"
 -
 -/* ACPI property names */
 -#define XGBE_ACPI_DMA_FREQ	"amd,dma-freq"
 -#define XGBE_ACPI_PTP_FREQ	"amd,ptp-freq"
 -
 -/* Timestamp support - values based on 50MHz PTP clock
 - *   50MHz => 20 nsec
 - */
 -#define XGBE_TSTAMP_SSINC	20
 -#define XGBE_TSTAMP_SNSINC	0
 +/* MDIO bus phy name */
 +#define XGBE_PHY_NAME		"amd_xgbe_phy"
 +#define XGBE_PRTAD		0
  
  /* Driver PMT macros */
  #define XGMAC_DRIVER_CONTEXT	1
  #define XGMAC_IOCTL_CONTEXT	2
  
++<<<<<<< HEAD
 +#define XGBE_FIFO_MAX		81920
 +#define XGBE_FIFO_SIZE_B(x)	(x)
 +#define XGBE_FIFO_SIZE_KB(x)	(x * 1024)
++=======
+ #define XGMAC_FIFO_RX_MAX	81920
+ #define XGMAC_FIFO_TX_MAX	81920
+ #define XGMAC_FIFO_MIN_ALLOC	2048
+ #define XGMAC_FIFO_UNIT		256
+ #define XGMAC_FIFO_ALIGN(_x)				\
+ 	(((_x) + XGMAC_FIFO_UNIT - 1) & ~(XGMAC_FIFO_UNIT - 1))
+ #define XGMAC_FIFO_FC_OFF	2048
+ #define XGMAC_FIFO_FC_MIN	4096
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  
 -#define XGBE_TC_MIN_QUANTUM	10
 +#define XGBE_TC_CNT		2
  
  /* Helper macro for descriptor handling
   *  Always use XGBE_GET_DESC_DATA to access the descriptor data
@@@ -194,6 -248,65 +218,68 @@@
  /* Flow control queue count */
  #define XGMAC_MAX_FLOW_CONTROL_QUEUES	8
  
++<<<<<<< HEAD
++=======
+ /* Flow control threshold units */
+ #define XGMAC_FLOW_CONTROL_UNIT		512
+ #define XGMAC_FLOW_CONTROL_ALIGN(_x)				\
+ 	(((_x) + XGMAC_FLOW_CONTROL_UNIT - 1) & ~(XGMAC_FLOW_CONTROL_UNIT - 1))
+ #define XGMAC_FLOW_CONTROL_VALUE(_x)				\
+ 	(((_x) < 1024) ? 0 : ((_x) / XGMAC_FLOW_CONTROL_UNIT) - 2)
+ #define XGMAC_FLOW_CONTROL_MAX		33280
+ 
+ /* Maximum MAC address hash table size (256 bits = 8 bytes) */
+ #define XGBE_MAC_HASH_TABLE_SIZE	8
+ 
+ /* Receive Side Scaling */
+ #define XGBE_RSS_HASH_KEY_SIZE		40
+ #define XGBE_RSS_MAX_TABLE_SIZE		256
+ #define XGBE_RSS_LOOKUP_TABLE_TYPE	0
+ #define XGBE_RSS_HASH_KEY_TYPE		1
+ 
+ /* Auto-negotiation */
+ #define XGBE_AN_MS_TIMEOUT		500
+ #define XGBE_LINK_TIMEOUT		10
+ 
+ #define XGBE_AN_INT_CMPLT		0x01
+ #define XGBE_AN_INC_LINK		0x02
+ #define XGBE_AN_PG_RCV			0x04
+ #define XGBE_AN_INT_MASK		0x07
+ 
+ /* Rate-change complete wait/retry count */
+ #define XGBE_RATECHANGE_COUNT		500
+ 
+ /* Default SerDes settings */
+ #define XGBE_SPEED_10000_BLWC		0
+ #define XGBE_SPEED_10000_CDR		0x7
+ #define XGBE_SPEED_10000_PLL		0x1
+ #define XGBE_SPEED_10000_PQ		0x12
+ #define XGBE_SPEED_10000_RATE		0x0
+ #define XGBE_SPEED_10000_TXAMP		0xa
+ #define XGBE_SPEED_10000_WORD		0x7
+ #define XGBE_SPEED_10000_DFE_TAP_CONFIG	0x1
+ #define XGBE_SPEED_10000_DFE_TAP_ENABLE	0x7f
+ 
+ #define XGBE_SPEED_2500_BLWC		1
+ #define XGBE_SPEED_2500_CDR		0x2
+ #define XGBE_SPEED_2500_PLL		0x0
+ #define XGBE_SPEED_2500_PQ		0xa
+ #define XGBE_SPEED_2500_RATE		0x1
+ #define XGBE_SPEED_2500_TXAMP		0xf
+ #define XGBE_SPEED_2500_WORD		0x1
+ #define XGBE_SPEED_2500_DFE_TAP_CONFIG	0x3
+ #define XGBE_SPEED_2500_DFE_TAP_ENABLE	0x0
+ 
+ #define XGBE_SPEED_1000_BLWC		1
+ #define XGBE_SPEED_1000_CDR		0x2
+ #define XGBE_SPEED_1000_PLL		0x0
+ #define XGBE_SPEED_1000_PQ		0xa
+ #define XGBE_SPEED_1000_RATE		0x3
+ #define XGBE_SPEED_1000_TXAMP		0xf
+ #define XGBE_SPEED_1000_WORD		0x1
+ #define XGBE_SPEED_1000_DFE_TAP_CONFIG	0x3
+ #define XGBE_SPEED_1000_DFE_TAP_ENABLE	0x0
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  
  struct xgbe_prv_data;
  
@@@ -577,30 -865,50 +663,64 @@@ struct xgbe_prv_data 
  	unsigned int pause_autoneg;
  	unsigned int tx_pause;
  	unsigned int rx_pause;
+ 	unsigned int rx_rfa[XGBE_MAX_QUEUES];
+ 	unsigned int rx_rfd[XGBE_MAX_QUEUES];
  
 -	/* Receive Side Scaling settings */
 -	u8 rss_key[XGBE_RSS_HASH_KEY_SIZE];
 -	u32 rss_table[XGBE_RSS_MAX_TABLE_SIZE];
 -	u32 rss_options;
 +	/* MDIO settings */
 +	struct module *phy_module;
 +	char *mii_bus_id;
 +	struct mii_bus *mii;
 +	int mdio_mmd;
 +	struct phy_device *phydev;
 +	int default_autoneg;
 +	int default_speed;
 +
 +	/* Current PHY settings */
 +	phy_interface_t phy_mode;
 +	int phy_link;
 +	int phy_speed;
 +	unsigned int phy_tx_pause;
 +	unsigned int phy_rx_pause;
  
  	/* Netdev related settings */
 -	unsigned char mac_addr[ETH_ALEN];
  	netdev_features_t netdev_features;
  	struct napi_struct napi;
  	struct xgbe_mmc_stats mmc_stats;
 -	struct xgbe_ext_stats ext_stats;
  
++<<<<<<< HEAD
 +	/* System clock value used for Rx watchdog */
 +	struct clk *sysclock;
++=======
+ 	/* Filtering support */
+ 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+ 
+ 	/* Device clocks */
+ 	struct clk *sysclk;
+ 	unsigned long sysclk_rate;
+ 	struct clk *ptpclk;
+ 	unsigned long ptpclk_rate;
+ 
+ 	/* Timestamp support */
+ 	spinlock_t tstamp_lock;
+ 	struct ptp_clock_info ptp_clock_info;
+ 	struct ptp_clock *ptp_clock;
+ 	struct hwtstamp_config tstamp_config;
+ 	struct cyclecounter tstamp_cc;
+ 	struct timecounter tstamp_tc;
+ 	unsigned int tstamp_addend;
+ 	struct work_struct tx_tstamp_work;
+ 	struct sk_buff *tx_tstamp_skb;
+ 	u64 tx_tstamp;
+ 
+ 	/* DCB support */
+ 	struct ieee_ets *ets;
+ 	struct ieee_pfc *pfc;
+ 	unsigned int q2tc_map[XGBE_MAX_QUEUES];
+ 	unsigned int prio2q_map[IEEE_8021QAZ_MAX_TCS];
+ 	unsigned int pfcq[XGBE_MAX_QUEUES];
+ 	unsigned int pfc_rfa;
+ 	u8 num_tcs;
++>>>>>>> 43e0dcf70840 (amd-xgbe: Perform priority-based hardware FIFO allocation)
  
  	/* Hardware features of the device */
  	struct xgbe_hw_features hw_feat;
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dev.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe.h
