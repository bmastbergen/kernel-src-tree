mm/hmm/mirror: helper to snapshot CPU page table

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [mm] hmm/mirror: helper to snapshot CPU page table v3 (Jerome Glisse) [1444991]
Rebuild_FUZZ: 93.75%
commit-author Jérôme Glisse <jglisse@redhat.com>
commit da4c3c735ea4dcc2a0b0ff0bd4803c336361b6f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/da4c3c73.failed

This does not use existing page table walker because we want to share
same code for our page fault handler.

Link: http://lkml.kernel.org/r/20170817000548.32038-5-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Signed-off-by: Evgeny Baskakov <ebaskakov@nvidia.com>
	Signed-off-by: John Hubbard <jhubbard@nvidia.com>
	Signed-off-by: Mark Hairgrove <mhairgrove@nvidia.com>
	Signed-off-by: Sherry Cheung <SCheung@nvidia.com>
	Signed-off-by: Subhash Gutti <sgutti@nvidia.com>
	Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: David Nellans <dnellans@nvidia.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Cc: Bob Liu <liubo95@huawei.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit da4c3c735ea4dcc2a0b0ff0bd4803c336361b6f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc include/linux/hmm.h
index ff3a332a43f6,62899c9829c9..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -33,16 -72,130 +33,143 @@@
  
  #if IS_ENABLED(CONFIG_HMM)
  
++<<<<<<< HEAD
 +#include <linux/mm.h>
 +#include <linux/gpt.h>
 +#include <linux/sched.h>
 +#include <linux/wait.h>
 +#include <linux/kref.h>
 +#include <linux/list.h>
 +#include <linux/spinlock.h>
 +#include <linux/mm_types.h>
 +#include <linux/highmem.h>
 +#include <linux/mmu_notifier.h>
++=======
+ struct hmm;
+ 
+ /*
+  * hmm_pfn_t - HMM uses its own pfn type to keep several flags per page
+  *
+  * Flags:
+  * HMM_PFN_VALID: pfn is valid
+  * HMM_PFN_READ:  CPU page table has read permission set
+  * HMM_PFN_WRITE: CPU page table has write permission set
+  * HMM_PFN_ERROR: corresponding CPU page table entry points to poisoned memory
+  * HMM_PFN_EMPTY: corresponding CPU page table entry is pte_none()
+  * HMM_PFN_SPECIAL: corresponding CPU page table entry is special; i.e., the
+  *      result of vm_insert_pfn() or vm_insert_page(). Therefore, it should not
+  *      be mirrored by a device, because the entry will never have HMM_PFN_VALID
+  *      set and the pfn value is undefined.
+  * HMM_PFN_DEVICE_UNADDRESSABLE: unaddressable device memory (ZONE_DEVICE)
+  */
+ typedef unsigned long hmm_pfn_t;
+ 
+ #define HMM_PFN_VALID (1 << 0)
+ #define HMM_PFN_READ (1 << 1)
+ #define HMM_PFN_WRITE (1 << 2)
+ #define HMM_PFN_ERROR (1 << 3)
+ #define HMM_PFN_EMPTY (1 << 4)
+ #define HMM_PFN_SPECIAL (1 << 5)
+ #define HMM_PFN_DEVICE_UNADDRESSABLE (1 << 6)
+ #define HMM_PFN_SHIFT 7
+ 
+ /*
+  * hmm_pfn_t_to_page() - return struct page pointed to by a valid hmm_pfn_t
+  * @pfn: hmm_pfn_t to convert to struct page
+  * Returns: struct page pointer if pfn is a valid hmm_pfn_t, NULL otherwise
+  *
+  * If the hmm_pfn_t is valid (ie valid flag set) then return the struct page
+  * matching the pfn value stored in the hmm_pfn_t. Otherwise return NULL.
+  */
+ static inline struct page *hmm_pfn_t_to_page(hmm_pfn_t pfn)
+ {
+ 	if (!(pfn & HMM_PFN_VALID))
+ 		return NULL;
+ 	return pfn_to_page(pfn >> HMM_PFN_SHIFT);
+ }
+ 
+ /*
+  * hmm_pfn_t_to_pfn() - return pfn value store in a hmm_pfn_t
+  * @pfn: hmm_pfn_t to extract pfn from
+  * Returns: pfn value if hmm_pfn_t is valid, -1UL otherwise
+  */
+ static inline unsigned long hmm_pfn_t_to_pfn(hmm_pfn_t pfn)
+ {
+ 	if (!(pfn & HMM_PFN_VALID))
+ 		return -1UL;
+ 	return (pfn >> HMM_PFN_SHIFT);
+ }
+ 
+ /*
+  * hmm_pfn_t_from_page() - create a valid hmm_pfn_t value from struct page
+  * @page: struct page pointer for which to create the hmm_pfn_t
+  * Returns: valid hmm_pfn_t for the page
+  */
+ static inline hmm_pfn_t hmm_pfn_t_from_page(struct page *page)
+ {
+ 	return (page_to_pfn(page) << HMM_PFN_SHIFT) | HMM_PFN_VALID;
+ }
+ 
+ /*
+  * hmm_pfn_t_from_pfn() - create a valid hmm_pfn_t value from pfn
+  * @pfn: pfn value for which to create the hmm_pfn_t
+  * Returns: valid hmm_pfn_t for the pfn
+  */
+ static inline hmm_pfn_t hmm_pfn_t_from_pfn(unsigned long pfn)
+ {
+ 	return (pfn << HMM_PFN_SHIFT) | HMM_PFN_VALID;
+ }
+ 
+ 
+ #if IS_ENABLED(CONFIG_HMM_MIRROR)
+ /*
+  * Mirroring: how to synchronize device page table with CPU page table.
+  *
+  * A device driver that is participating in HMM mirroring must always
+  * synchronize with CPU page table updates. For this, device drivers can either
+  * directly use mmu_notifier APIs or they can use the hmm_mirror API. Device
+  * drivers can decide to register one mirror per device per process, or just
+  * one mirror per process for a group of devices. The pattern is:
+  *
+  *      int device_bind_address_space(..., struct mm_struct *mm, ...)
+  *      {
+  *          struct device_address_space *das;
+  *
+  *          // Device driver specific initialization, and allocation of das
+  *          // which contains an hmm_mirror struct as one of its fields.
+  *          ...
+  *
+  *          ret = hmm_mirror_register(&das->mirror, mm, &device_mirror_ops);
+  *          if (ret) {
+  *              // Cleanup on error
+  *              return ret;
+  *          }
+  *
+  *          // Other device driver specific initialization
+  *          ...
+  *      }
+  *
+  * Once an hmm_mirror is registered for an address space, the device driver
+  * will get callbacks through sync_cpu_device_pagetables() operation (see
+  * hmm_mirror_ops struct).
+  *
+  * Device driver must not free the struct containing the hmm_mirror struct
+  * before calling hmm_mirror_unregister(). The expected usage is to do that when
+  * the device driver is unbinding from an address space.
+  *
+  *
+  *      void device_unbind_address_space(struct device_address_space *das)
+  *      {
+  *          // Device driver specific cleanup
+  *          ...
+  *
+  *          hmm_mirror_unregister(&das->mirror);
+  *
+  *          // Other device driver specific cleanup, and now das can be freed
+  *          ...
+  *      }
+  */
++>>>>>>> da4c3c735ea4 (mm/hmm/mirror: helper to snapshot CPU page table)
  
  struct hmm_mirror;
  
@@@ -54,85 -207,107 +181,146 @@@ enum hmm_update 
  	HMM_UPDATE_INVALIDATE,
  };
  
 -/*
 - * struct hmm_mirror_ops - HMM mirror device operations callback
 - *
 - * @update: callback to update range on a device
 - */
 -struct hmm_mirror_ops {
 -	/* sync_cpu_device_pagetables() - synchronize page tables
 -	 *
 -	 * @mirror: pointer to struct hmm_mirror
 -	 * @update_type: type of update that occurred to the CPU page table
 -	 * @start: virtual start address of the range to update
 -	 * @end: virtual end address of the range to update
 -	 *
 -	 * This callback ultimately originates from mmu_notifiers when the CPU
 -	 * page table is updated. The device driver must update its page table
 -	 * in response to this callback. The update argument tells what action
 -	 * to perform.
 -	 *
 -	 * The device driver must not return from this callback until the device
 -	 * page tables are completely updated (TLBs flushed, etc); this is a
 -	 * synchronous call.
 -	 */
 -	void (*sync_cpu_device_pagetables)(struct hmm_mirror *mirror,
 -					   enum hmm_update_type update_type,
 -					   unsigned long start,
 -					   unsigned long end);
 +
 +struct hmm {
 +	struct mm_struct	*mm;
 +	struct gpt		*gpt;
 +	struct list_head	migrates;
 +	struct list_head	mirrors;
 +	struct kref		kref;
 +	spinlock_t		lock;
 +	struct mmu_notifier	mmu_notifier;
 +	wait_queue_head_t	wait_queue;
 +	atomic_t		sequence;
 +	atomic_t		notifier_count;
  };
  
++<<<<<<< HEAD
 +struct hmm *hmm_register(struct mm_struct *mm);
 +struct hmm *hmm_register_mirror(struct mm_struct *mm,
 +				struct hmm_mirror *mirror);
 +void hmm_put(struct hmm *hmm);
++=======
+ /*
+  * struct hmm_mirror - mirror struct for a device driver
+  *
+  * @hmm: pointer to struct hmm (which is unique per mm_struct)
+  * @ops: device driver callback for HMM mirror operations
+  * @list: for list of mirrors of a given mm
+  *
+  * Each address space (mm_struct) being mirrored by a device must register one
+  * instance of an hmm_mirror struct with HMM. HMM will track the list of all
+  * mirrors for each mm_struct.
+  */
+ struct hmm_mirror {
+ 	struct hmm			*hmm;
+ 	const struct hmm_mirror_ops	*ops;
+ 	struct list_head		list;
+ };
+ 
+ int hmm_mirror_register(struct hmm_mirror *mirror, struct mm_struct *mm);
+ void hmm_mirror_unregister(struct hmm_mirror *mirror);
+ 
+ 
+ /*
+  * struct hmm_range - track invalidation lock on virtual address range
+  *
+  * @list: all range lock are on a list
+  * @start: range virtual start address (inclusive)
+  * @end: range virtual end address (exclusive)
+  * @pfns: array of pfns (big enough for the range)
+  * @valid: pfns array did not change since it has been fill by an HMM function
+  */
+ struct hmm_range {
+ 	struct list_head	list;
+ 	unsigned long		start;
+ 	unsigned long		end;
+ 	hmm_pfn_t		*pfns;
+ 	bool			valid;
+ };
+ 
+ /*
+  * To snapshot the CPU page table, call hmm_vma_get_pfns(), then take a device
+  * driver lock that serializes device page table updates, then call
+  * hmm_vma_range_done(), to check if the snapshot is still valid. The same
+  * device driver page table update lock must also be used in the
+  * hmm_mirror_ops.sync_cpu_device_pagetables() callback, so that CPU page
+  * table invalidation serializes on it.
+  *
+  * YOU MUST CALL hmm_vma_range_done() ONCE AND ONLY ONCE EACH TIME YOU CALL
+  * hmm_vma_get_pfns() WITHOUT ERROR !
+  *
+  * IF YOU DO NOT FOLLOW THE ABOVE RULE THE SNAPSHOT CONTENT MIGHT BE INVALID !
+  */
+ int hmm_vma_get_pfns(struct vm_area_struct *vma,
+ 		     struct hmm_range *range,
+ 		     unsigned long start,
+ 		     unsigned long end,
+ 		     hmm_pfn_t *pfns);
+ bool hmm_vma_range_done(struct vm_area_struct *vma, struct hmm_range *range);
+ #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
++>>>>>>> da4c3c735ea4 (mm/hmm/mirror: helper to snapshot CPU page table)
 +
 +
 +typedef int (*hmm_walk_hole_t)(struct vm_area_struct *vma,
 +			      struct gpt_walk *walk,
 +			      unsigned long addr,
 +			      unsigned long end,
 +			      void *private);
 +
 +typedef int (*hmm_walk_pte_t)(struct vm_area_struct *vma,
 +			      struct gpt_walk *walk,
 +			      unsigned long addr,
 +			      unsigned long end,
 +			      spinlock_t *ptl,
 +			      spinlock_t *gtl,
 +			      pte_t *ptep,
 +			      gte_t *gtep,
 +			      void *private);
  
 +typedef int (*hmm_walk_huge_t)(struct vm_area_struct *vma,
 +			       struct gpt_walk *walk,
 +			       unsigned long addr,
 +			       unsigned long end,
 +			       spinlock_t *ptl,
 +			       spinlock_t *gtl,
 +			       struct page *page,
 +			       pte_t *ptep,
 +			       gte_t *gtep,
 +			       void *private);
  
 -/* Below are for HMM internal use only! Not to be used by device driver! */
 -void hmm_mm_destroy(struct mm_struct *mm);
 +int hmm_walk(struct vm_area_struct *vma,
 +	     hmm_walk_hole_t walk_hole,
 +	     hmm_walk_huge_t walk_huge,
 +	     hmm_walk_pte_t walk_pte,
 +	     struct gpt_walk *walk,
 +	     unsigned long start,
 +	     unsigned long end,
 +	     void *private);
  
 -static inline void hmm_mm_init(struct mm_struct *mm)
 +
 +static inline bool hmm_get_cookie(struct hmm *hmm, int *cookie)
  {
 -	mm->hmm = NULL;
 +	BUG_ON(!cookie);
 +
 +	*cookie = atomic_read(&hmm->sequence);
 +	smp_rmb();
 +	if (atomic_read(&hmm->notifier_count))
 +		return false;
 +	return true;
  }
  
 -#else /* IS_ENABLED(CONFIG_HMM) */
 +static inline bool hmm_check_cookie(struct hmm *hmm, int cookie)
 +{
 +	if (cookie != atomic_read(&hmm->sequence))
 +		return false;
 +	return true;
 +}
  
 -/* Below are for HMM internal use only! Not to be used by device driver! */
 -static inline void hmm_mm_destroy(struct mm_struct *mm) {}
 -static inline void hmm_mm_init(struct mm_struct *mm) {}
 +static inline void hmm_wait_cookie(struct hmm *hmm)
 +{
 +	wait_event(hmm->wait_queue, !atomic_read(&hmm->notifier_count));
 +}
  
  #endif /* IS_ENABLED(CONFIG_HMM) */
 -#endif /* LINUX_HMM_H */
 +#endif /* _LINUX_HMM_H */
diff --cc mm/hmm.c
index 0855d5478b88,172984848d51..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -11,86 -11,139 +11,183 @@@
   * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   * GNU General Public License for more details.
   *
 - * Authors: JÃ©rÃ´me Glisse <jglisse@redhat.com>
 + * Authors: Jérôme Glisse <jglisse@redhat.com>
   */
  /*
 - * Refer to include/linux/hmm.h for information about heterogeneous memory
 + * Refer to include/linux/hmm.h for informations about heterogeneous memory
   * management or HMM for short.
   */
++<<<<<<< HEAD
++=======
+ #include <linux/mm.h>
+ #include <linux/hmm.h>
+ #include <linux/rmap.h>
+ #include <linux/swap.h>
+ #include <linux/slab.h>
+ #include <linux/sched.h>
+ #include <linux/swapops.h>
+ #include <linux/hugetlb.h>
++>>>>>>> da4c3c735ea4 (mm/hmm/mirror: helper to snapshot CPU page table)
  #include <linux/mmu_notifier.h>
 +#include <linux/hmm_mirror.h>
 +#include <linux/sched.h>
 +#include <linux/slab.h>
 +#include <linux/hmm.h>
  
 +static bool _hmm_enabled = false;
  
++<<<<<<< HEAD
 +static int hmm_gpt_invalidate_range(struct gpt_walk *walk,
 +				    unsigned long addr,
 +				    unsigned long end,
 +				    spinlock_t *gtl,
 +				    gte_t *gtep,
 +				    void *private)
 +{
 +	spin_lock(gtl);
 +	for (; addr < end; addr += PAGE_SIZE, gtep++) {
 +		if (hmm_entry_is_valid(*gtep)) {
 +			atomic_dec(gpt_walk_gtd_refcount(walk, 0));
 +			*gtep = 0;
 +		}
++=======
+ #ifdef CONFIG_HMM
+ static const struct mmu_notifier_ops hmm_mmu_notifier_ops;
+ 
+ /*
+  * struct hmm - HMM per mm struct
+  *
+  * @mm: mm struct this HMM struct is bound to
+  * @lock: lock protecting ranges list
+  * @sequence: we track updates to the CPU page table with a sequence number
+  * @ranges: list of range being snapshotted
+  * @mirrors: list of mirrors for this mm
+  * @mmu_notifier: mmu notifier to track updates to CPU page table
+  * @mirrors_sem: read/write semaphore protecting the mirrors list
+  */
+ struct hmm {
+ 	struct mm_struct	*mm;
+ 	spinlock_t		lock;
+ 	atomic_t		sequence;
+ 	struct list_head	ranges;
+ 	struct list_head	mirrors;
+ 	struct mmu_notifier	mmu_notifier;
+ 	struct rw_semaphore	mirrors_sem;
+ };
+ 
+ /*
+  * hmm_register - register HMM against an mm (HMM internal)
+  *
+  * @mm: mm struct to attach to
+  *
+  * This is not intended to be used directly by device drivers. It allocates an
+  * HMM struct if mm does not have one, and initializes it.
+  */
+ static struct hmm *hmm_register(struct mm_struct *mm)
+ {
+ 	struct hmm *hmm = READ_ONCE(mm->hmm);
+ 	bool cleanup = false;
+ 
+ 	/*
+ 	 * The hmm struct can only be freed once the mm_struct goes away,
+ 	 * hence we should always have pre-allocated an new hmm struct
+ 	 * above.
+ 	 */
+ 	if (hmm)
+ 		return hmm;
+ 
+ 	hmm = kmalloc(sizeof(*hmm), GFP_KERNEL);
+ 	if (!hmm)
+ 		return NULL;
+ 	INIT_LIST_HEAD(&hmm->mirrors);
+ 	init_rwsem(&hmm->mirrors_sem);
+ 	atomic_set(&hmm->sequence, 0);
+ 	hmm->mmu_notifier.ops = NULL;
+ 	INIT_LIST_HEAD(&hmm->ranges);
+ 	spin_lock_init(&hmm->lock);
+ 	hmm->mm = mm;
+ 
+ 	/*
+ 	 * We should only get here if hold the mmap_sem in write mode ie on
+ 	 * registration of first mirror through hmm_mirror_register()
+ 	 */
+ 	hmm->mmu_notifier.ops = &hmm_mmu_notifier_ops;
+ 	if (__mmu_notifier_register(&hmm->mmu_notifier, mm)) {
+ 		kfree(hmm);
+ 		return NULL;
++>>>>>>> da4c3c735ea4 (mm/hmm/mirror: helper to snapshot CPU page table)
  	}
 +	spin_unlock(gtl);
  
 -	spin_lock(&mm->page_table_lock);
 -	if (!mm->hmm)
 -		mm->hmm = hmm;
 -	else
 -		cleanup = true;
 -	spin_unlock(&mm->page_table_lock);
 -
 -	if (cleanup) {
 -		mmu_notifier_unregister(&hmm->mmu_notifier, mm);
 -		kfree(hmm);
 -	}
 -
 -	return mm->hmm;
 -}
 -
 -void hmm_mm_destroy(struct mm_struct *mm)
 -{
 -	kfree(mm->hmm);
 +	return 0;
  }
 -#endif /* CONFIG_HMM */
  
 -#if IS_ENABLED(CONFIG_HMM_MIRROR)
  static void hmm_invalidate_range(struct hmm *hmm,
 -				 enum hmm_update_type action,
 +				 enum hmm_update update,
  				 unsigned long start,
  				 unsigned long end)
  {
  	struct hmm_mirror *mirror;
++<<<<<<< HEAD
 +	struct gpt_walk walk;
++=======
+ 	struct hmm_range *range;
+ 
+ 	spin_lock(&hmm->lock);
+ 	list_for_each_entry(range, &hmm->ranges, list) {
+ 		unsigned long addr, idx, npages;
+ 
+ 		if (end < range->start || start >= range->end)
+ 			continue;
+ 
+ 		range->valid = false;
+ 		addr = max(start, range->start);
+ 		idx = (addr - range->start) >> PAGE_SHIFT;
+ 		npages = (min(range->end, end) - addr) >> PAGE_SHIFT;
+ 		memset(&range->pfns[idx], 0, sizeof(*range->pfns) * npages);
+ 	}
+ 	spin_unlock(&hmm->lock);
++>>>>>>> da4c3c735ea4 (mm/hmm/mirror: helper to snapshot CPU page table)
 +
 +	gpt_walk_init(&walk, hmm->gpt);
 +	gpt_walk_range(&walk, start, end, &hmm_gpt_invalidate_range, hmm);
 +	gpt_walk_fini(&walk);
  
 -	down_read(&hmm->mirrors_sem);
 -	list_for_each_entry(mirror, &hmm->mirrors, list)
 -		mirror->ops->sync_cpu_device_pagetables(mirror, action,
 -							start, end);
 -	up_read(&hmm->mirrors_sem);
 +	/*
 +	 * Mirror being added or remove is a rare event so list traversal isn't
 +	 * protected by a lock, we rely on simple rules. All list modification
 +	 * are done using list_add_rcu() and list_del_rcu() under a spinlock to
 +	 * protect from concurrent addition or removal but not traversal.
 +	 *
 +	 * Because hmm_mirror_unregister() wait for all running invalidation to
 +	 * complete (and thus all list traversal to finish). None of the mirror
 +	 * struct can be freed from under us while traversing the list and thus
 +	 * it is safe to dereference their list pointer even if they were just
 +	 * remove.
 +	 */
 +	list_for_each_entry (mirror, &hmm->mirrors, list) {
 +		mirror->ops->update(mirror, update, start, end);
 +	}
 +}
 +
 +static void hmm_invalidate_page(struct mmu_notifier *mn,
 +				   struct mm_struct *mm,
 +				   unsigned long addr)
 +{
 +	unsigned long start = addr & PAGE_MASK;
 +	unsigned long end = start + PAGE_SIZE;
 +	struct hmm *hmm = mm->hmm;
 +
 +	VM_BUG_ON(!hmm);
 +
 +	atomic_inc(&hmm->notifier_count);
 +	smp_wmb();
 +	atomic_inc(&hmm->sequence);
 +	hmm_invalidate_range(mm->hmm, HMM_UPDATE_INVALIDATE, start, end);
 +	atomic_inc(&hmm->sequence);
 +	smp_wmb();
 +	atomic_dec(&hmm->notifier_count);
 +	wake_up(&hmm->wait_queue);
  }
  
  static void hmm_invalidate_range_start(struct mmu_notifier *mn,
@@@ -312,97 -202,281 +409,361 @@@ again
  
  	return 0;
  }
 -EXPORT_SYMBOL(hmm_mirror_register);
  
 -/*
 - * hmm_mirror_unregister() - unregister a mirror
 - *
 - * @mirror: new mirror struct to register
 - *
 - * Stop mirroring a process address space, and cleanup.
 - */
 -void hmm_mirror_unregister(struct hmm_mirror *mirror)
 +static int hmm_walk_pud(struct vm_area_struct *vma,
 +			hmm_walk_hole_t walk_hole,
 +			hmm_walk_huge_t walk_huge,
 +			hmm_walk_pte_t walk_pte,
 +			struct gpt_walk *walk,
 +			unsigned long addr,
 +			unsigned long end,
 +			void *private,
 +			pgd_t *pgdp)
 +{
 +	unsigned long next;
 +	pud_t *pudp;
 +
 +	pudp = pud_offset(pgdp, addr);
 +	do {
 +		int ret;
 +
 +		next = pud_addr_end(addr, end);
 +		if (pud_none_or_clear_bad(pudp)) {
 +			if (walk_hole) {
 +				ret = walk_hole(vma, walk, addr,
 +						next, private);
 +				if (ret)
 +					return ret;
 +			}
 +			continue;
 +		}
 +
 +		ret = hmm_walk_pmd(vma, walk_hole, walk_huge, walk_pte,
 +				   walk, addr, next, private, pudp);
 +		if (ret)
 +			return ret;
 +
 +	} while (pudp++, addr = next, addr != end);
 +
 +	return 0;
 +}
++<<<<<<< HEAD
 +
 +int hmm_walk(struct vm_area_struct *vma,
 +	     hmm_walk_hole_t walk_hole,
 +	     hmm_walk_huge_t walk_huge,
 +	     hmm_walk_pte_t walk_pte,
 +	     struct gpt_walk *walk,
 +	     unsigned long start,
 +	     unsigned long end,
 +	     void *private)
 +{
 +	unsigned long addr = start, next;
 +	pgd_t *pgdp;
 +
 +	pgdp = pgd_offset(vma->vm_mm, addr);
 +	do {
 +		int ret;
 +
 +		next = pgd_addr_end(addr, end);
 +		if (pgd_none_or_clear_bad(pgdp)) {
 +			if (walk_hole) {
 +				ret = walk_hole(vma, walk, addr,
 +						next, private);
 +				if (ret)
 +					return ret;
 +			}
 +			continue;
 +		}
 +
 +		ret = hmm_walk_pud(vma, walk_hole, walk_huge, walk_pte,
 +				   walk, addr, next, private, pgdp);
 +		if (ret)
 +			return ret;
 +
 +	} while (pgdp++, addr = next, addr != end);
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(hmm_walk);
 +
 +static int __init setup_hmm(char *str)
  {
 -	struct hmm *hmm = mirror->hmm;
 +	int ret = 0;
 +
 +	if (!str)
 +		goto out;
 +	if (!strcmp(str, "enable")) {
 +		_hmm_enabled = true;
 +		ret = 1;
 +	}
  
 -	down_write(&hmm->mirrors_sem);
 -	list_del(&mirror->list);
 -	up_write(&hmm->mirrors_sem);
 +out:
 +	if (!ret)
 +		printk(KERN_WARNING "experimental_hmm= cannot parse, ignored\n");
 +	return ret;
  }
 +__setup("experimental_hmm=", setup_hmm);
++=======
+ EXPORT_SYMBOL(hmm_mirror_unregister);
+ 
+ static void hmm_pfns_special(hmm_pfn_t *pfns,
+ 			     unsigned long addr,
+ 			     unsigned long end)
+ {
+ 	for (; addr < end; addr += PAGE_SIZE, pfns++)
+ 		*pfns = HMM_PFN_SPECIAL;
+ }
+ 
+ static int hmm_pfns_bad(unsigned long addr,
+ 			unsigned long end,
+ 			struct mm_walk *walk)
+ {
+ 	struct hmm_range *range = walk->private;
+ 	hmm_pfn_t *pfns = range->pfns;
+ 	unsigned long i;
+ 
+ 	i = (addr - range->start) >> PAGE_SHIFT;
+ 	for (; addr < end; addr += PAGE_SIZE, i++)
+ 		pfns[i] = HMM_PFN_ERROR;
+ 
+ 	return 0;
+ }
+ 
+ static int hmm_vma_walk_hole(unsigned long addr,
+ 			     unsigned long end,
+ 			     struct mm_walk *walk)
+ {
+ 	struct hmm_range *range = walk->private;
+ 	hmm_pfn_t *pfns = range->pfns;
+ 	unsigned long i;
+ 
+ 	i = (addr - range->start) >> PAGE_SHIFT;
+ 	for (; addr < end; addr += PAGE_SIZE, i++)
+ 		pfns[i] = HMM_PFN_EMPTY;
+ 
+ 	return 0;
+ }
+ 
+ static int hmm_vma_walk_clear(unsigned long addr,
+ 			      unsigned long end,
+ 			      struct mm_walk *walk)
+ {
+ 	struct hmm_range *range = walk->private;
+ 	hmm_pfn_t *pfns = range->pfns;
+ 	unsigned long i;
+ 
+ 	i = (addr - range->start) >> PAGE_SHIFT;
+ 	for (; addr < end; addr += PAGE_SIZE, i++)
+ 		pfns[i] = 0;
+ 
+ 	return 0;
+ }
+ 
+ static int hmm_vma_walk_pmd(pmd_t *pmdp,
+ 			    unsigned long start,
+ 			    unsigned long end,
+ 			    struct mm_walk *walk)
+ {
+ 	struct hmm_range *range = walk->private;
+ 	struct vm_area_struct *vma = walk->vma;
+ 	hmm_pfn_t *pfns = range->pfns;
+ 	unsigned long addr = start, i;
+ 	hmm_pfn_t flag;
+ 	pte_t *ptep;
+ 
+ 	i = (addr - range->start) >> PAGE_SHIFT;
+ 	flag = vma->vm_flags & VM_READ ? HMM_PFN_READ : 0;
+ 
+ again:
+ 	if (pmd_none(*pmdp))
+ 		return hmm_vma_walk_hole(start, end, walk);
+ 
+ 	if (pmd_huge(*pmdp) && vma->vm_flags & VM_HUGETLB)
+ 		return hmm_pfns_bad(start, end, walk);
+ 
+ 	if (pmd_devmap(*pmdp) || pmd_trans_huge(*pmdp)) {
+ 		unsigned long pfn;
+ 		pmd_t pmd;
+ 
+ 		/*
+ 		 * No need to take pmd_lock here, even if some other threads
+ 		 * is splitting the huge pmd we will get that event through
+ 		 * mmu_notifier callback.
+ 		 *
+ 		 * So just read pmd value and check again its a transparent
+ 		 * huge or device mapping one and compute corresponding pfn
+ 		 * values.
+ 		 */
+ 		pmd = pmd_read_atomic(pmdp);
+ 		barrier();
+ 		if (!pmd_devmap(pmd) && !pmd_trans_huge(pmd))
+ 			goto again;
+ 		if (pmd_protnone(pmd))
+ 			return hmm_vma_walk_clear(start, end, walk);
+ 
+ 		pfn = pmd_pfn(pmd) + pte_index(addr);
+ 		flag |= pmd_write(pmd) ? HMM_PFN_WRITE : 0;
+ 		for (; addr < end; addr += PAGE_SIZE, i++, pfn++)
+ 			pfns[i] = hmm_pfn_t_from_pfn(pfn) | flag;
+ 		return 0;
+ 	}
+ 
+ 	if (pmd_bad(*pmdp))
+ 		return hmm_pfns_bad(start, end, walk);
+ 
+ 	ptep = pte_offset_map(pmdp, addr);
+ 	for (; addr < end; addr += PAGE_SIZE, ptep++, i++) {
+ 		pte_t pte = *ptep;
+ 
+ 		pfns[i] = 0;
+ 
+ 		if (pte_none(pte) || !pte_present(pte)) {
+ 			pfns[i] = HMM_PFN_EMPTY;
+ 			continue;
+ 		}
+ 
+ 		pfns[i] = hmm_pfn_t_from_pfn(pte_pfn(pte)) | flag;
+ 		pfns[i] |= pte_write(pte) ? HMM_PFN_WRITE : 0;
+ 	}
+ 	pte_unmap(ptep - 1);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * hmm_vma_get_pfns() - snapshot CPU page table for a range of virtual addresses
+  * @vma: virtual memory area containing the virtual address range
+  * @range: used to track snapshot validity
+  * @start: range virtual start address (inclusive)
+  * @end: range virtual end address (exclusive)
+  * @entries: array of hmm_pfn_t: provided by the caller, filled in by function
+  * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, 0 success
+  *
+  * This snapshots the CPU page table for a range of virtual addresses. Snapshot
+  * validity is tracked by range struct. See hmm_vma_range_done() for further
+  * information.
+  *
+  * The range struct is initialized here. It tracks the CPU page table, but only
+  * if the function returns success (0), in which case the caller must then call
+  * hmm_vma_range_done() to stop CPU page table update tracking on this range.
+  *
+  * NOT CALLING hmm_vma_range_done() IF FUNCTION RETURNS 0 WILL LEAD TO SERIOUS
+  * MEMORY CORRUPTION ! YOU HAVE BEEN WARNED !
+  */
+ int hmm_vma_get_pfns(struct vm_area_struct *vma,
+ 		     struct hmm_range *range,
+ 		     unsigned long start,
+ 		     unsigned long end,
+ 		     hmm_pfn_t *pfns)
+ {
+ 	struct mm_walk mm_walk;
+ 	struct hmm *hmm;
+ 
+ 	/* FIXME support hugetlb fs */
+ 	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL)) {
+ 		hmm_pfns_special(pfns, start, end);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* Sanity check, this really should not happen ! */
+ 	if (start < vma->vm_start || start >= vma->vm_end)
+ 		return -EINVAL;
+ 	if (end < vma->vm_start || end > vma->vm_end)
+ 		return -EINVAL;
+ 
+ 	hmm = hmm_register(vma->vm_mm);
+ 	if (!hmm)
+ 		return -ENOMEM;
+ 	/* Caller must have registered a mirror, via hmm_mirror_register() ! */
+ 	if (!hmm->mmu_notifier.ops)
+ 		return -EINVAL;
+ 
+ 	/* Initialize range to track CPU page table update */
+ 	range->start = start;
+ 	range->pfns = pfns;
+ 	range->end = end;
+ 	spin_lock(&hmm->lock);
+ 	range->valid = true;
+ 	list_add_rcu(&range->list, &hmm->ranges);
+ 	spin_unlock(&hmm->lock);
+ 
+ 	mm_walk.vma = vma;
+ 	mm_walk.mm = vma->vm_mm;
+ 	mm_walk.private = range;
+ 	mm_walk.pte_entry = NULL;
+ 	mm_walk.test_walk = NULL;
+ 	mm_walk.hugetlb_entry = NULL;
+ 	mm_walk.pmd_entry = hmm_vma_walk_pmd;
+ 	mm_walk.pte_hole = hmm_vma_walk_hole;
+ 
+ 	walk_page_range(start, end, &mm_walk);
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL(hmm_vma_get_pfns);
+ 
+ /*
+  * hmm_vma_range_done() - stop tracking change to CPU page table over a range
+  * @vma: virtual memory area containing the virtual address range
+  * @range: range being tracked
+  * Returns: false if range data has been invalidated, true otherwise
+  *
+  * Range struct is used to track updates to the CPU page table after a call to
+  * either hmm_vma_get_pfns() or hmm_vma_fault(). Once the device driver is done
+  * using the data,  or wants to lock updates to the data it got from those
+  * functions, it must call the hmm_vma_range_done() function, which will then
+  * stop tracking CPU page table updates.
+  *
+  * Note that device driver must still implement general CPU page table update
+  * tracking either by using hmm_mirror (see hmm_mirror_register()) or by using
+  * the mmu_notifier API directly.
+  *
+  * CPU page table update tracking done through hmm_range is only temporary and
+  * to be used while trying to duplicate CPU page table contents for a range of
+  * virtual addresses.
+  *
+  * There are two ways to use this :
+  * again:
+  *   hmm_vma_get_pfns(vma, range, start, end, pfns);
+  *   trans = device_build_page_table_update_transaction(pfns);
+  *   device_page_table_lock();
+  *   if (!hmm_vma_range_done(vma, range)) {
+  *     device_page_table_unlock();
+  *     goto again;
+  *   }
+  *   device_commit_transaction(trans);
+  *   device_page_table_unlock();
+  *
+  * Or:
+  *   hmm_vma_get_pfns(vma, range, start, end, pfns);
+  *   device_page_table_lock();
+  *   hmm_vma_range_done(vma, range);
+  *   device_update_page_table(pfns);
+  *   device_page_table_unlock();
+  */
+ bool hmm_vma_range_done(struct vm_area_struct *vma, struct hmm_range *range)
+ {
+ 	unsigned long npages = (range->end - range->start) >> PAGE_SHIFT;
+ 	struct hmm *hmm;
+ 
+ 	if (range->end <= range->start) {
+ 		BUG();
+ 		return false;
+ 	}
+ 
+ 	hmm = hmm_register(vma->vm_mm);
+ 	if (!hmm) {
+ 		memset(range->pfns, 0, sizeof(*range->pfns) * npages);
+ 		return false;
+ 	}
+ 
+ 	spin_lock(&hmm->lock);
+ 	list_del_rcu(&range->list);
+ 	spin_unlock(&hmm->lock);
+ 
+ 	return range->valid;
+ }
+ EXPORT_SYMBOL(hmm_vma_range_done);
+ #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
++>>>>>>> da4c3c735ea4 (mm/hmm/mirror: helper to snapshot CPU page table)
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
