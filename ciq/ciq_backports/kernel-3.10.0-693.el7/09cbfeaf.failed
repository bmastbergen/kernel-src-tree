mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [fs] mm, fs: get rid of PAGE_CACHE_* and page_cache_{get, release} macros(cifs only) (Sachin Prabhu) [1416808]
Rebuild_FUZZ: 91.78%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit 09cbfeaf1a5a67bfb3201e0c83c810cecb2efa5a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/09cbfeaf.failed

PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} macros were introduced *long* time
ago with promise that one day it will be possible to implement page
cache with bigger chunks than PAGE_SIZE.

This promise never materialized.  And unlikely will.

We have many places where PAGE_CACHE_SIZE assumed to be equal to
PAGE_SIZE.  And it's constant source of confusion on whether
PAGE_CACHE_* or PAGE_* constant should be used in a particular case,
especially on the border between fs and mm.

Global switching to PAGE_CACHE_SIZE != PAGE_SIZE would cause to much
breakage to be doable.

Let's stop pretending that pages in page cache are special.  They are
not.

The changes are pretty straight-forward:

 - <foo> << (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;

 - <foo> >> (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;

 - PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} -> PAGE_{SIZE,SHIFT,MASK,ALIGN};

 - page_cache_get() -> get_page();

 - page_cache_release() -> put_page();

This patch contains automated changes generated with coccinelle using
script below.  For some reason, coccinelle doesn't patch header files.
I've called spatch for them manually.

The only adjustment after coccinelle is revert of changes to
PAGE_CAHCE_ALIGN definition: we are going to drop it later.

There are few places in the code where coccinelle didn't reach.  I'll
fix them manually in a separate patch.  Comments and documentation also
will be addressed with the separate patch.

virtual patch

@@
expression E;
@@
- E << (PAGE_CACHE_SHIFT - PAGE_SHIFT)
+ E

@@
expression E;
@@
- E >> (PAGE_CACHE_SHIFT - PAGE_SHIFT)
+ E

@@
@@
- PAGE_CACHE_SHIFT
+ PAGE_SHIFT

@@
@@
- PAGE_CACHE_SIZE
+ PAGE_SIZE

@@
@@
- PAGE_CACHE_MASK
+ PAGE_MASK

@@
expression E;
@@
- PAGE_CACHE_ALIGN(E)
+ PAGE_ALIGN(E)

@@
expression E;
@@
- page_cache_get(E)
+ get_page(E)

@@
expression E;
@@
- page_cache_release(E)
+ put_page(E)

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 09cbfeaf1a5a67bfb3201e0c83c810cecb2efa5a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arc/mm/cache.c
#	block/blk-core.c
#	drivers/gpu/drm/armada/armada_gem.c
#	drivers/mmc/host/usdhi6rol0.c
#	drivers/nvdimm/pmem.c
#	drivers/scsi/sd.c
#	drivers/staging/lustre/include/linux/libcfs/libcfs_private.h
#	drivers/staging/lustre/include/linux/libcfs/linux/linux-mem.h
#	drivers/staging/lustre/lnet/klnds/socklnd/socklnd_lib.c
#	drivers/staging/lustre/lnet/libcfs/debug.c
#	drivers/staging/lustre/lnet/libcfs/tracefile.c
#	drivers/staging/lustre/lnet/libcfs/tracefile.h
#	drivers/staging/lustre/lnet/lnet/lib-md.c
#	drivers/staging/lustre/lnet/lnet/lib-move.c
#	drivers/staging/lustre/lnet/lnet/lib-socket.c
#	drivers/staging/lustre/lnet/lnet/router.c
#	drivers/staging/lustre/lnet/selftest/brw_test.c
#	drivers/staging/lustre/lnet/selftest/conctl.c
#	drivers/staging/lustre/lnet/selftest/conrpc.c
#	drivers/staging/lustre/lnet/selftest/framework.c
#	drivers/staging/lustre/lnet/selftest/rpc.c
#	drivers/staging/lustre/lnet/selftest/selftest.h
#	drivers/staging/lustre/lustre/include/linux/lustre_patchless_compat.h
#	drivers/staging/lustre/lustre/include/lustre/lustre_idl.h
#	drivers/staging/lustre/lustre/include/lustre_mdc.h
#	drivers/staging/lustre/lustre/include/lustre_net.h
#	drivers/staging/lustre/lustre/include/obd.h
#	drivers/staging/lustre/lustre/include/obd_support.h
#	drivers/staging/lustre/lustre/lclient/lcommon_cl.c
#	drivers/staging/lustre/lustre/ldlm/ldlm_lib.c
#	drivers/staging/lustre/lustre/ldlm/ldlm_pool.c
#	drivers/staging/lustre/lustre/ldlm/ldlm_request.c
#	drivers/staging/lustre/lustre/llite/dir.c
#	drivers/staging/lustre/lustre/llite/llite_internal.h
#	drivers/staging/lustre/lustre/llite/llite_lib.c
#	drivers/staging/lustre/lustre/llite/llite_mmap.c
#	drivers/staging/lustre/lustre/llite/lloop.c
#	drivers/staging/lustre/lustre/llite/lproc_llite.c
#	drivers/staging/lustre/lustre/llite/rw.c
#	drivers/staging/lustre/lustre/llite/rw26.c
#	drivers/staging/lustre/lustre/llite/vvp_io.c
#	drivers/staging/lustre/lustre/llite/vvp_page.c
#	drivers/staging/lustre/lustre/lmv/lmv_obd.c
#	drivers/staging/lustre/lustre/mdc/mdc_request.c
#	drivers/staging/lustre/lustre/mgc/mgc_request.c
#	drivers/staging/lustre/lustre/obdclass/cl_page.c
#	drivers/staging/lustre/lustre/obdclass/class_obd.c
#	drivers/staging/lustre/lustre/obdclass/linux/linux-obdo.c
#	drivers/staging/lustre/lustre/obdclass/linux/linux-sysctl.c
#	drivers/staging/lustre/lustre/obdclass/lu_object.c
#	drivers/staging/lustre/lustre/obdecho/echo_client.c
#	drivers/staging/lustre/lustre/osc/lproc_osc.c
#	drivers/staging/lustre/lustre/osc/osc_cache.c
#	drivers/staging/lustre/lustre/osc/osc_page.c
#	drivers/staging/lustre/lustre/osc/osc_request.c
#	drivers/staging/lustre/lustre/ptlrpc/client.c
#	drivers/staging/lustre/lustre/ptlrpc/import.c
#	drivers/staging/lustre/lustre/ptlrpc/lproc_ptlrpc.c
#	drivers/staging/lustre/lustre/ptlrpc/recover.c
#	drivers/staging/lustre/lustre/ptlrpc/sec_bulk.c
#	fs/9p/vfs_addr.c
#	fs/9p/vfs_file.c
#	fs/affs/file.c
#	fs/afs/file.c
#	fs/binfmt_elf_fdpic.c
#	fs/bio.c
#	fs/block_dev.c
#	fs/btrfs/compression.c
#	fs/btrfs/disk-io.c
#	fs/btrfs/extent_io.c
#	fs/btrfs/file-item.c
#	fs/btrfs/file.c
#	fs/btrfs/free-space-cache.c
#	fs/btrfs/inode-map.c
#	fs/btrfs/inode.c
#	fs/btrfs/ioctl.c
#	fs/btrfs/raid56.c
#	fs/btrfs/reada.c
#	fs/btrfs/send.c
#	fs/btrfs/tests/extent-io-tests.c
#	fs/btrfs/tests/free-space-tests.c
#	fs/buffer.c
#	fs/ceph/addr.c
#	fs/ceph/file.c
#	fs/ceph/super.c
#	fs/cifs/cifsfs.c
#	fs/cifs/file.c
#	fs/cramfs/inode.c
#	fs/crypto/crypto.c
#	fs/direct-io.c
#	fs/ecryptfs/crypto.c
#	fs/exofs/dir.c
#	fs/exofs/inode.c
#	fs/ext2/dir.c
#	fs/ext4/crypto.c
#	fs/ext4/dir.c
#	fs/ext4/inode.c
#	fs/ext4/mballoc.c
#	fs/ext4/move_extent.c
#	fs/ext4/readpage.c
#	fs/ext4/super.c
#	fs/ext4/symlink.c
#	fs/f2fs/data.c
#	fs/f2fs/debug.c
#	fs/f2fs/file.c
#	fs/f2fs/inline.c
#	fs/f2fs/namei.c
#	fs/f2fs/node.c
#	fs/f2fs/recovery.c
#	fs/f2fs/segment.c
#	fs/freevxfs/vxfs_lookup.c
#	fs/fuse/dev.c
#	fs/fuse/file.c
#	fs/gfs2/aops.c
#	fs/gfs2/meta_io.c
#	fs/gfs2/rgrp.c
#	fs/hfs/bnode.c
#	fs/hfsplus/bnode.c
#	fs/hfsplus/xattr.c
#	fs/hostfs/hostfs_kern.c
#	fs/hugetlbfs/inode.c
#	fs/jffs2/file.c
#	fs/jffs2/fs.c
#	fs/jfs/jfs_metapage.c
#	fs/jfs/super.c
#	fs/kernfs/mount.c
#	fs/minix/dir.c
#	fs/nfs/direct.c
#	fs/nilfs2/dir.c
#	fs/nilfs2/inode.c
#	fs/nilfs2/segment.c
#	fs/ntfs/aops.c
#	fs/ntfs/attrib.c
#	fs/ntfs/file.c
#	fs/ocfs2/aops.c
#	fs/ocfs2/file.c
#	fs/orangefs/inode.c
#	fs/orangefs/orangefs-bufmap.c
#	fs/orangefs/orangefs-utils.c
#	fs/qnx6/dir.c
#	fs/reiserfs/inode.c
#	fs/reiserfs/xattr.c
#	fs/squashfs/block.c
#	fs/squashfs/cache.c
#	fs/squashfs/decompressor.c
#	fs/squashfs/file.c
#	fs/squashfs/file_direct.c
#	fs/squashfs/lz4_wrapper.c
#	fs/squashfs/lzo_wrapper.c
#	fs/squashfs/page_actor.c
#	fs/squashfs/page_actor.h
#	fs/squashfs/xz_wrapper.c
#	fs/squashfs/zlib_wrapper.c
#	fs/sysv/dir.c
#	fs/ubifs/file.c
#	fs/ubifs/super.c
#	fs/ufs/dir.c
#	fs/ufs/inode.c
#	fs/ufs/namei.c
#	fs/xfs/xfs_aops.c
#	fs/xfs/xfs_bmap_util.c
#	fs/xfs/xfs_file.c
#	include/linux/fs.h
#	include/linux/pagemap.h
#	kernel/events/uprobes.c
#	mm/fadvise.c
#	mm/filemap.c
#	mm/hugetlb.c
#	mm/memory.c
#	mm/page-writeback.c
#	mm/readahead.c
#	mm/rmap.c
#	mm/shmem.c
#	mm/swap.c
#	mm/swap_state.c
#	mm/truncate.c
diff --cc block/blk-core.c
index defb77328143,b60537b2c35b..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -656,10 -701,13 +656,15 @@@ struct request_queue *blk_alloc_queue_n
  	if (q->id < 0)
  		goto fail_q;
  
 -	q->bio_split = bioset_create(BIO_POOL_SIZE, 0);
 -	if (!q->bio_split)
 -		goto fail_id;
 -
  	q->backing_dev_info.ra_pages =
++<<<<<<< HEAD
 +			(VM_MAX_READAHEAD * 1024) / PAGE_CACHE_SIZE;
 +	q->backing_dev_info.state = 0;
 +	q->backing_dev_info.capabilities = BDI_CAP_MAP_COPY;
++=======
+ 			(VM_MAX_READAHEAD * 1024) / PAGE_SIZE;
+ 	q->backing_dev_info.capabilities = BDI_CAP_CGROUP_WRITEBACK;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	q->backing_dev_info.name = "block";
  	q->node = node_id;
  
diff --cc drivers/nvdimm/pmem.c
index e53dfc8b1585,12c86fa80c5f..000000000000
--- a/drivers/nvdimm/pmem.c
+++ b/drivers/nvdimm/pmem.c
@@@ -148,10 -148,12 +148,16 @@@ static void pmem_make_request(struct re
  static int pmem_rw_page(struct block_device *bdev, sector_t sector,
  		       struct page *page, int rw)
  {
 -	struct pmem_device *pmem = bdev->bd_disk->private_data;
 +	struct pmem_device *pmem = bdev->bd_queue->queuedata;
  	int rc;
  
++<<<<<<< HEAD
 +	rc = pmem_do_bvec(pmem, page, PAGE_CACHE_SIZE, 0, rw, sector);
++=======
+ 	rc = pmem_do_bvec(pmem, page, PAGE_SIZE, 0, rw, sector);
+ 	if (rw & WRITE)
+ 		wmb_pmem();
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	/*
  	 * The ->rw_page interface is subtle and tricky.  The core
diff --cc drivers/scsi/sd.c
index 433634aa2867,1bd0753f678a..000000000000
--- a/drivers/scsi/sd.c
+++ b/drivers/scsi/sd.c
@@@ -2864,10 -2891,10 +2864,17 @@@ static int sd_revalidate_disk(struct ge
  	if (sdkp->opt_xfer_blocks &&
  	    sdkp->opt_xfer_blocks <= dev_max &&
  	    sdkp->opt_xfer_blocks <= SD_DEF_XFER_BLOCKS &&
++<<<<<<< HEAD
 +	    logical_to_bytes(sdp, sdkp->opt_xfer_blocks) >= PAGE_CACHE_SIZE) {
 +		q->limits.io_opt = logical_to_bytes(sdp, sdkp->opt_xfer_blocks);
 +		rw_max = logical_to_sectors(sdp, sdkp->opt_xfer_blocks);
 +	} else
++=======
+ 	    sdkp->opt_xfer_blocks * sdp->sector_size >= PAGE_SIZE)
+ 		rw_max = q->limits.io_opt =
+ 			sdkp->opt_xfer_blocks * sdp->sector_size;
+ 	else
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		rw_max = BLK_DEF_MAX_SECTORS;
  
  	/* Combine with controller limits */
diff --cc fs/9p/vfs_addr.c
index 055562c580b4,ac9225e86bf3..000000000000
--- a/fs/9p/vfs_addr.c
+++ b/fs/9p/vfs_addr.c
@@@ -154,33 -153,29 +154,42 @@@ static void v9fs_invalidate_page(struc
  	 * If called with zero offset, we should release
  	 * the private state assocated with the page
  	 */
++<<<<<<< HEAD
 +	if (offset == 0)
++=======
+ 	if (offset == 0 && length == PAGE_SIZE)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		v9fs_fscache_invalidate_page(page);
  }
  
  static int v9fs_vfs_writepage_locked(struct page *page)
  {
 +	char *buffer;
 +	int retval, len;
 +	loff_t offset, size;
 +	mm_segment_t old_fs;
 +	struct v9fs_inode *v9inode;
  	struct inode *inode = page->mapping->host;
 -	struct v9fs_inode *v9inode = V9FS_I(inode);
 -	loff_t size = i_size_read(inode);
 -	struct iov_iter from;
 -	struct bio_vec bvec;
 -	int err, len;
  
++<<<<<<< HEAD
 +	v9inode = V9FS_I(inode);
 +	size = i_size_read(inode);
 +	if (page->index == size >> PAGE_CACHE_SHIFT)
 +		len = size & ~PAGE_CACHE_MASK;
++=======
+ 	if (page->index == size >> PAGE_SHIFT)
+ 		len = size & ~PAGE_MASK;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	else
- 		len = PAGE_CACHE_SIZE;
+ 		len = PAGE_SIZE;
  
 -	bvec.bv_page = page;
 -	bvec.bv_offset = 0;
 -	bvec.bv_len = len;
 -	iov_iter_bvec(&from, ITER_BVEC | WRITE, &bvec, 1, len);
 +	set_page_writeback(page);
 +
 +	buffer = kmap(page);
 +	offset = page_offset(page);
  
 +	old_fs = get_fs();
 +	set_fs(get_ds());
  	/* We should have writeback_fid always set */
  	BUG_ON(!v9inode->writeback_fid);
  
@@@ -278,9 -271,12 +287,9 @@@ static int v9fs_write_begin(struct fil
  	int retval = 0;
  	struct page *page;
  	struct v9fs_inode *v9inode;
- 	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
+ 	pgoff_t index = pos >> PAGE_SHIFT;
  	struct inode *inode = mapping->host;
  
 -
 -	p9_debug(P9_DEBUG_VFS, "filp %p, mapping %p\n", filp, mapping);
 -
  	v9inode = V9FS_I(inode);
  start:
  	page = grab_cache_page_write_begin(mapping, index, flags);
diff --cc fs/9p/vfs_file.c
index d384a8b77ee8,b84c291ba1eb..000000000000
--- a/fs/9p/vfs_file.c
+++ b/fs/9p/vfs_file.c
@@@ -507,35 -404,39 +507,58 @@@ v9fs_file_write_internal(struct inode *
   *
   */
  static ssize_t
 -v9fs_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 +v9fs_file_write(struct file *filp, const char __user * data,
 +		size_t count, loff_t *offset)
  {
 -	struct file *file = iocb->ki_filp;
 -	ssize_t retval;
 -	loff_t origin;
 -	int err = 0;
 +	ssize_t retval = 0;
 +	loff_t origin = *offset;
  
 -	retval = generic_write_checks(iocb, from);
 -	if (retval <= 0)
 -		return retval;
  
++<<<<<<< HEAD
 +	retval = generic_write_checks(filp, &origin, &count, 0);
 +	if (retval)
 +		goto out;
 +
 +	retval = -EINVAL;
 +	if ((ssize_t) count < 0)
 +		goto out;
 +	retval = 0;
 +	if (!count)
 +		goto out;
 +
 +	retval = v9fs_file_write_internal(file_inode(filp),
 +					filp->private_data,
 +					data, count, &origin, 1);
 +	/* update offset on successful write */
 +	if (retval > 0)
 +		*offset = origin;
 +out:
 +	return retval;
++=======
+ 	origin = iocb->ki_pos;
+ 	retval = p9_client_write(file->private_data, iocb->ki_pos, from, &err);
+ 	if (retval > 0) {
+ 		struct inode *inode = file_inode(file);
+ 		loff_t i_size;
+ 		unsigned long pg_start, pg_end;
+ 		pg_start = origin >> PAGE_SHIFT;
+ 		pg_end = (origin + retval - 1) >> PAGE_SHIFT;
+ 		if (inode->i_mapping && inode->i_mapping->nrpages)
+ 			invalidate_inode_pages2_range(inode->i_mapping,
+ 						      pg_start, pg_end);
+ 		iocb->ki_pos += retval;
+ 		i_size = i_size_read(inode);
+ 		if (iocb->ki_pos > i_size) {
+ 			inode_add_bytes(inode, iocb->ki_pos - i_size);
+ 			i_size_write(inode, iocb->ki_pos);
+ 		}
+ 		return retval;
+ 	}
+ 	return err;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
 +
  static int v9fs_file_fsync(struct file *filp, loff_t start, loff_t end,
  			   int datasync)
  {
diff --cc fs/affs/file.c
index 1d5e51dda4f1,0cde550050e8..000000000000
--- a/fs/affs/file.c
+++ b/fs/affs/file.c
@@@ -507,12 -507,12 +507,20 @@@ affs_do_readpage_ofs(struct file *file
  	u32 bidx, boff, bsize;
  	u32 tmp;
  
++<<<<<<< HEAD
 +	pr_debug("AFFS: read_page(%u, %ld, %d, %d)\n", (u32)inode->i_ino, page->index, from, to);
 +	BUG_ON(from > to || to > PAGE_CACHE_SIZE);
 +	kmap(page);
 +	data = page_address(page);
 +	bsize = AFFS_SB(sb)->s_data_blksize;
 +	tmp = (page->index << PAGE_CACHE_SHIFT) + from;
++=======
+ 	pr_debug("%s(%lu, %ld, 0, %d)\n", __func__, inode->i_ino,
+ 		 page->index, to);
+ 	BUG_ON(to > PAGE_SIZE);
+ 	bsize = AFFS_SB(sb)->s_data_blksize;
+ 	tmp = page->index << PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	bidx = tmp / bsize;
  	boff = tmp % bsize;
  
@@@ -608,14 -612,14 +616,22 @@@ affs_readpage_ofs(struct file *file, st
  	u32 to;
  	int err;
  
++<<<<<<< HEAD
 +	pr_debug("AFFS: read_page(%u, %ld)\n", (u32)inode->i_ino, page->index);
 +	to = PAGE_CACHE_SIZE;
 +	if (((page->index + 1) << PAGE_CACHE_SHIFT) > inode->i_size) {
 +		to = inode->i_size & ~PAGE_CACHE_MASK;
 +		memset(page_address(page) + to, 0, PAGE_CACHE_SIZE - to);
++=======
+ 	pr_debug("%s(%lu, %ld)\n", __func__, inode->i_ino, page->index);
+ 	to = PAGE_SIZE;
+ 	if (((page->index + 1) << PAGE_SHIFT) > inode->i_size) {
+ 		to = inode->i_size & ~PAGE_MASK;
+ 		memset(page_address(page) + to, 0, PAGE_SIZE - to);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  
 -	err = affs_do_readpage_ofs(page, to);
 +	err = affs_do_readpage_ofs(file, page, 0, to);
  	if (!err)
  		SetPageUptodate(page);
  	unlock_page(page);
@@@ -651,10 -656,10 +667,14 @@@ static int affs_write_begin_ofs(struct 
  		return 0;
  
  	/* XXX: inefficient but safe in the face of short writes */
++<<<<<<< HEAD
 +	err = affs_do_readpage_ofs(file, page, 0, PAGE_CACHE_SIZE);
++=======
+ 	err = affs_do_readpage_ofs(page, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (err) {
  		unlock_page(page);
- 		page_cache_release(page);
+ 		put_page(page);
  	}
  	return err;
  }
@@@ -776,8 -792,9 +796,8 @@@ done
  	if (tmp > inode->i_size)
  		inode->i_size = AFFS_I(inode)->mmu_private = tmp;
  
 -err_first_bh:
  	unlock_page(page);
- 	page_cache_release(page);
+ 	put_page(page);
  
  	return written;
  
diff --cc fs/afs/file.c
index 8f6e9234d565,6344aee4ac4b..000000000000
--- a/fs/afs/file.c
+++ b/fs/afs/file.c
@@@ -319,7 -319,7 +319,11 @@@ static void afs_invalidatepage(struct p
  	BUG_ON(!PageLocked(page));
  
  	/* we clean up only if the entire page is being invalidated */
++<<<<<<< HEAD
 +	if (offset == 0) {
++=======
+ 	if (offset == 0 && length == PAGE_SIZE) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  #ifdef CONFIG_AFS_FSCACHE
  		if (PageFsCache(page)) {
  			struct afs_vnode *vnode = AFS_FS_I(page->mapping->host);
diff --cc fs/binfmt_elf_fdpic.c
index 730035e2eb0c,083ea2bc60ab..000000000000
--- a/fs/binfmt_elf_fdpic.c
+++ b/fs/binfmt_elf_fdpic.c
@@@ -1533,48 -1530,24 +1533,57 @@@ static int elf_fdpic_dump_segments(stru
  			struct page *page = get_dump_page(addr);
  			if (page) {
  				void *kaddr = kmap(page);
 -				res = dump_emit(cprm, kaddr, PAGE_SIZE);
 +				*size += PAGE_SIZE;
 +				if (*size > *limit)
 +					err = -EFBIG;
 +				else if (!dump_write(file, kaddr, PAGE_SIZE))
 +					err = -EIO;
  				kunmap(page);
++<<<<<<< HEAD
 +				page_cache_release(page);
 +			} else if (!dump_seek(file, PAGE_SIZE))
 +				err = -EFBIG;
 +			if (err)
 +				goto out;
++=======
+ 				put_page(page);
+ 			} else {
+ 				res = dump_skip(cprm, PAGE_SIZE);
+ 			}
+ 			if (!res)
+ 				return false;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
 -#else
 -		if (!dump_emit(cprm, (void *) vma->vm_start,
 -				vma->vm_end - vma->vm_start))
 -			return false;
 +	}
 +out:
 +	return err;
 +}
  #endif
 +
 +/*
 + * dump the segments for a NOMMU process
 + */
 +#ifndef CONFIG_MMU
 +static int elf_fdpic_dump_segments(struct file *file, size_t *size,
 +			   unsigned long *limit, unsigned long mm_flags)
 +{
 +	struct vm_area_struct *vma;
 +
 +	for (vma = current->mm->mmap; vma; vma = vma->vm_next) {
 +		if (!maydump(vma, mm_flags))
 +			continue;
 +
 +		if ((*size += PAGE_SIZE) > *limit)
 +			return -EFBIG;
 +
 +		if (!dump_write(file, (void *) vma->vm_start,
 +				vma->vm_end - vma->vm_start))
 +			return -EIO;
  	}
 -	return true;
 +
 +	return 0;
  }
 +#endif
  
  static size_t elf_core_vma_data_size(unsigned long mm_flags)
  {
diff --cc fs/bio.c
index ba5bd4020a51,168531517694..000000000000
--- a/fs/bio.c
+++ b/fs/bio.c
@@@ -1445,8 -1359,18 +1445,21 @@@ struct bio *bio_map_user_iov(struct req
  	 * reference to it
  	 */
  	bio_get(bio);
 -	return bio;
  
++<<<<<<< HEAD:fs/bio.c
 +	return bio;
++=======
+  out_unmap:
+ 	for (j = 0; j < nr_pages; j++) {
+ 		if (!pages[j])
+ 			break;
+ 		put_page(pages[j]);
+ 	}
+  out:
+ 	kfree(pages);
+ 	bio_put(bio);
+ 	return ERR_PTR(ret);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros):block/bio.c
  }
  
  static void __bio_unmap_user(struct bio *bio)
diff --cc fs/block_dev.c
index 7d6ea6f10f58,20a2c02b77c4..000000000000
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@@ -1239,10 -1146,10 +1239,15 @@@ void bd_set_size(struct block_device *b
  {
  	unsigned bsize = bdev_logical_block_size(bdev);
  
 -	inode_lock(bdev->bd_inode);
 +	mutex_lock(&bdev->bd_inode->i_mutex);
  	i_size_write(bdev->bd_inode, size);
++<<<<<<< HEAD
 +	mutex_unlock(&bdev->bd_inode->i_mutex);
 +	while (bsize < PAGE_CACHE_SIZE) {
++=======
+ 	inode_unlock(bdev->bd_inode);
+ 	while (bsize < PAGE_SIZE) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		if (size & bsize)
  			break;
  		bsize <<= 1;
diff --cc fs/btrfs/compression.c
index b09e2165c428,ff61a41ac90b..000000000000
--- a/fs/btrfs/compression.c
+++ b/fs/btrfs/compression.c
@@@ -374,9 -372,9 +374,9 @@@ int btrfs_submit_compressed_write(struc
  	for (pg_index = 0; pg_index < cb->nr_pages; pg_index++) {
  		page = compressed_pages[pg_index];
  		page->mapping = inode->i_mapping;
 -		if (bio->bi_iter.bi_size)
 +		if (bio->bi_size)
  			ret = io_tree->ops->merge_bio_hook(WRITE, page, 0,
- 							   PAGE_CACHE_SIZE,
+ 							   PAGE_SIZE,
  							   bio, 0);
  		else
  			ret = 0;
@@@ -489,9 -487,8 +489,14 @@@ static noinline int add_ra_bio_pages(st
  		if (!page)
  			break;
  
++<<<<<<< HEAD
 +		if (add_to_page_cache_lru(page, mapping, pg_index,
 +								GFP_NOFS)) {
 +			page_cache_release(page);
++=======
+ 		if (add_to_page_cache_lru(page, mapping, pg_index, GFP_NOFS)) {
+ 			put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			goto next;
  		}
  
@@@ -509,8 -506,8 +514,13 @@@
  		read_unlock(&em_tree->lock);
  
  		if (!em || last_offset < em->start ||
++<<<<<<< HEAD
 +		    (last_offset + PAGE_CACHE_SIZE > extent_map_end(em)) ||
 +		    (em->block_start >> 9) != cb->orig_bio->bi_sector) {
++=======
+ 		    (last_offset + PAGE_SIZE > extent_map_end(em)) ||
+ 		    (em->block_start >> 9) != cb->orig_bio->bi_iter.bi_sector) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			free_extent_map(em);
  			unlock_extent(tree, last_offset, end);
  			unlock_page(page);
@@@ -640,14 -637,10 +650,14 @@@ int btrfs_submit_compressed_read(struc
  	faili = nr_pages - 1;
  	cb->nr_pages = nr_pages;
  
 -	add_ra_bio_pages(inode, em_start + em_len, cb);
 +	/* In the parent-locked case, we only locked the range we are
 +	 * interested in.  In all other cases, we can opportunistically
 +	 * cache decompressed data that goes beyond the requested range. */
 +	if (!(bio_flags & EXTENT_BIO_PARENT_LOCKED))
 +		add_ra_bio_pages(inode, em_start + em_len, cb);
  
  	/* include any pages we added in add_ra-bio_pages */
- 	uncompressed_len = bio->bi_vcnt * PAGE_CACHE_SIZE;
+ 	uncompressed_len = bio->bi_vcnt * PAGE_SIZE;
  	cb->len = uncompressed_len;
  
  	comp_bio = compressed_bio_alloc(bdev, cur_disk_byte, GFP_NOFS);
@@@ -660,11 -653,11 +670,11 @@@
  	for (pg_index = 0; pg_index < nr_pages; pg_index++) {
  		page = cb->compressed_pages[pg_index];
  		page->mapping = inode->i_mapping;
- 		page->index = em_start >> PAGE_CACHE_SHIFT;
+ 		page->index = em_start >> PAGE_SHIFT;
  
 -		if (comp_bio->bi_iter.bi_size)
 +		if (comp_bio->bi_size)
  			ret = tree->ops->merge_bio_hook(READ, page, 0,
- 							PAGE_CACHE_SIZE,
+ 							PAGE_SIZE,
  							comp_bio, 0);
  		else
  			ret = 0;
diff --cc fs/btrfs/disk-io.c
index 95490a032345,942af3d2885f..000000000000
--- a/fs/btrfs/disk-io.c
+++ b/fs/btrfs/disk-io.c
@@@ -1739,9 -1757,10 +1739,13 @@@ static int setup_bdi(struct btrfs_fs_in
  	if (err)
  		return err;
  
++<<<<<<< HEAD
 +	bdi->ra_pages	= default_backing_dev_info.ra_pages;
++=======
+ 	bdi->ra_pages = VM_MAX_READAHEAD * 1024 / PAGE_SIZE;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	bdi->congested_fn	= btrfs_congested_fn;
  	bdi->congested_data	= info;
 -	bdi->capabilities |= BDI_CAP_CGROUP_WRITEBACK;
  	return 0;
  }
  
@@@ -2822,7 -2832,7 +2826,11 @@@ int open_ctree(struct super_block *sb
  
  	fs_info->bdi.ra_pages *= btrfs_super_num_devices(disk_super);
  	fs_info->bdi.ra_pages = max(fs_info->bdi.ra_pages,
++<<<<<<< HEAD
 +				    4 * 1024 * 1024 / PAGE_CACHE_SIZE);
++=======
+ 				    SZ_4M / PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	tree_root->nodesize = nodesize;
  	tree_root->sectorsize = sectorsize;
@@@ -4038,31 -4062,46 +4046,59 @@@ static int btrfs_check_super_valid(stru
  	}
  
  	/*
 -	 * Check sectorsize and nodesize first, other check will need it.
 -	 * Check all possible sectorsize(4K, 8K, 16K, 32K, 64K) here.
 +	 * The common minimum, we don't know if we can trust the nodesize/sectorsize
 +	 * items yet, they'll be verified later. Issue just a warning.
  	 */
++<<<<<<< HEAD
 +	if (!IS_ALIGNED(btrfs_super_root(sb), 4096))
++=======
+ 	if (!is_power_of_2(sectorsize) || sectorsize < 4096 ||
+ 	    sectorsize > BTRFS_MAX_METADATA_BLOCKSIZE) {
+ 		printk(KERN_ERR "BTRFS: invalid sectorsize %llu\n", sectorsize);
+ 		ret = -EINVAL;
+ 	}
+ 	/* Only PAGE SIZE is supported yet */
+ 	if (sectorsize != PAGE_SIZE) {
+ 		printk(KERN_ERR "BTRFS: sectorsize %llu not supported yet, only support %lu\n",
+ 				sectorsize, PAGE_SIZE);
+ 		ret = -EINVAL;
+ 	}
+ 	if (!is_power_of_2(nodesize) || nodesize < sectorsize ||
+ 	    nodesize > BTRFS_MAX_METADATA_BLOCKSIZE) {
+ 		printk(KERN_ERR "BTRFS: invalid nodesize %llu\n", nodesize);
+ 		ret = -EINVAL;
+ 	}
+ 	if (nodesize != le32_to_cpu(sb->__unused_leafsize)) {
+ 		printk(KERN_ERR "BTRFS: invalid leafsize %u, should be %llu\n",
+ 				le32_to_cpu(sb->__unused_leafsize),
+ 				nodesize);
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	/* Root alignment check */
+ 	if (!IS_ALIGNED(btrfs_super_root(sb), sectorsize)) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		printk(KERN_WARNING "BTRFS: tree_root block unaligned: %llu\n",
  				btrfs_super_root(sb));
 -		ret = -EINVAL;
 -	}
 -	if (!IS_ALIGNED(btrfs_super_chunk_root(sb), sectorsize)) {
 +	if (!IS_ALIGNED(btrfs_super_chunk_root(sb), 4096))
  		printk(KERN_WARNING "BTRFS: chunk_root block unaligned: %llu\n",
  				btrfs_super_chunk_root(sb));
 -		ret = -EINVAL;
 -	}
 -	if (!IS_ALIGNED(btrfs_super_log_root(sb), sectorsize)) {
 +	if (!IS_ALIGNED(btrfs_super_log_root(sb), 4096))
  		printk(KERN_WARNING "BTRFS: log_root block unaligned: %llu\n",
  				btrfs_super_log_root(sb));
 +
 +	/*
 +	 * Check the lower bound, the alignment and other constraints are
 +	 * checked later.
 +	 */
 +	if (btrfs_super_nodesize(sb) < 4096) {
 +		printk(KERN_ERR "BTRFS: nodesize too small: %u < 4096\n",
 +				btrfs_super_nodesize(sb));
 +		ret = -EINVAL;
 +	}
 +	if (btrfs_super_sectorsize(sb) < 4096) {
 +		printk(KERN_ERR "BTRFS: sectorsize too small: %u < 4096\n",
 +				btrfs_super_sectorsize(sb));
  		ret = -EINVAL;
  	}
  
diff --cc fs/btrfs/extent_io.c
index dddb24f01842,93d696d248d9..000000000000
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@@ -1438,39 -1361,25 +1438,39 @@@ int try_lock_extent(struct extent_io_tr
  	return 1;
  }
  
 -void extent_range_clear_dirty_for_io(struct inode *inode, u64 start, u64 end)
 +int unlock_extent_cached(struct extent_io_tree *tree, u64 start, u64 end,
 +			 struct extent_state **cached, gfp_t mask)
 +{
 +	return clear_extent_bit(tree, start, end, EXTENT_LOCKED, 1, 0, cached,
 +				mask);
 +}
 +
 +int unlock_extent(struct extent_io_tree *tree, u64 start, u64 end)
 +{
 +	return clear_extent_bit(tree, start, end, EXTENT_LOCKED, 1, 0, NULL,
 +				GFP_NOFS);
 +}
 +
 +int extent_range_clear_dirty_for_io(struct inode *inode, u64 start, u64 end)
  {
- 	unsigned long index = start >> PAGE_CACHE_SHIFT;
- 	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
+ 	unsigned long index = start >> PAGE_SHIFT;
+ 	unsigned long end_index = end >> PAGE_SHIFT;
  	struct page *page;
  
  	while (index <= end_index) {
  		page = find_get_page(inode->i_mapping, index);
  		BUG_ON(!page); /* Pages should be in the extent_io_tree */
  		clear_page_dirty_for_io(page);
- 		page_cache_release(page);
+ 		put_page(page);
  		index++;
  	}
 +	return 0;
  }
  
 -void extent_range_redirty_for_io(struct inode *inode, u64 start, u64 end)
 +int extent_range_redirty_for_io(struct inode *inode, u64 start, u64 end)
  {
- 	unsigned long index = start >> PAGE_CACHE_SHIFT;
- 	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
+ 	unsigned long index = start >> PAGE_SHIFT;
+ 	unsigned long end_index = end >> PAGE_SHIFT;
  	struct page *page;
  
  	while (index <= end_index) {
@@@ -1478,29 -1387,27 +1478,29 @@@
  		BUG_ON(!page); /* Pages should be in the extent_io_tree */
  		__set_page_dirty_nobuffers(page);
  		account_page_redirty(page);
- 		page_cache_release(page);
+ 		put_page(page);
  		index++;
  	}
 +	return 0;
  }
  
  /*
   * helper function to set both pages and extents in the tree writeback
   */
 -static void set_range_writeback(struct extent_io_tree *tree, u64 start, u64 end)
 +static int set_range_writeback(struct extent_io_tree *tree, u64 start, u64 end)
  {
- 	unsigned long index = start >> PAGE_CACHE_SHIFT;
- 	unsigned long end_index = end >> PAGE_CACHE_SHIFT;
+ 	unsigned long index = start >> PAGE_SHIFT;
+ 	unsigned long end_index = end >> PAGE_SHIFT;
  	struct page *page;
  
  	while (index <= end_index) {
  		page = find_get_page(tree->mapping, index);
  		BUG_ON(!page); /* Pages should be in the extent_io_tree */
  		set_page_writeback(page);
- 		page_cache_release(page);
+ 		put_page(page);
  		index++;
  	}
 +	return 0;
  }
  
  /* find the first state struct with 'bits' set after 'start', and
@@@ -2868,11 -2766,9 +2868,11 @@@ static int submit_extent_page(int rw, s
  {
  	int ret = 0;
  	struct bio *bio;
 +	int nr;
  	int contig = 0;
 +	int this_compressed = bio_flags & EXTENT_BIO_COMPRESSED;
  	int old_compressed = prev_bio_flags & EXTENT_BIO_COMPRESSED;
- 	size_t page_size = min_t(size_t, size, PAGE_CACHE_SIZE);
+ 	size_t page_size = min_t(size_t, size, PAGE_SIZE);
  
  	if (bio_ret && *bio_ret) {
  		bio = *bio_ret;
@@@ -3282,7 -3172,8 +3282,12 @@@ static int __extent_read_full_page(stru
  
  	while (1) {
  		lock_extent(tree, start, end);
++<<<<<<< HEAD
 +		ordered = btrfs_lookup_ordered_extent(inode, start);
++=======
+ 		ordered = btrfs_lookup_ordered_range(inode, start,
+ 						PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		if (!ordered)
  			break;
  		unlock_extent(tree, start, end);
@@@ -3621,10 -3497,10 +3626,14 @@@ static int __extent_writepage(struct pa
  
  	ClearPageError(page);
  
- 	pg_offset = i_size & (PAGE_CACHE_SIZE - 1);
+ 	pg_offset = i_size & (PAGE_SIZE - 1);
  	if (page->index > end_index ||
  	   (page->index == end_index && !pg_offset)) {
++<<<<<<< HEAD
 +		page->mapping->a_ops->invalidatepage(page, 0);
++=======
+ 		page->mapping->a_ops->invalidatepage(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		unlock_page(page);
  		return 0;
  	}
@@@ -3870,8 -3747,8 +3879,13 @@@ static noinline_for_stack int write_one
  
  		clear_page_dirty_for_io(p);
  		set_page_writeback(p);
++<<<<<<< HEAD
 +		ret = submit_extent_page(rw, tree, p, offset >> 9,
 +					 PAGE_CACHE_SIZE, 0, bdev, &epd->bio,
++=======
+ 		ret = submit_extent_page(rw, tree, wbc, p, offset >> 9,
+ 					 PAGE_SIZE, 0, bdev, &epd->bio,
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  					 -1, end_bio_extent_buffer_writepage,
  					 0, epd->bio_flags, bio_flags, false);
  		epd->bio_flags = bio_flags;
@@@ -4385,10 -4262,10 +4399,10 @@@ int try_release_extent_mapping(struct e
  {
  	struct extent_map *em;
  	u64 start = page_offset(page);
- 	u64 end = start + PAGE_CACHE_SIZE - 1;
+ 	u64 end = start + PAGE_SIZE - 1;
  
 -	if (gfpflags_allow_blocking(mask) &&
 -	    page->mapping->host->i_size > SZ_16M) {
 +	if ((mask & __GFP_WAIT) &&
 +	    page->mapping->host->i_size > 16 * 1024 * 1024) {
  		u64 len;
  		while (start <= end) {
  			len = end - start + 1;
@@@ -5008,8 -4896,8 +5022,13 @@@ struct extent_buffer *alloc_extent_buff
  			if (atomic_inc_not_zero(&exists->refs)) {
  				spin_unlock(&mapping->private_lock);
  				unlock_page(p);
++<<<<<<< HEAD
 +				page_cache_release(p);
 +				mark_extent_buffer_accessed(exists);
++=======
+ 				put_page(p);
+ 				mark_extent_buffer_accessed(exists, p);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				goto free_eb;
  			}
  			exists = NULL;
@@@ -5593,6 -5478,155 +5612,158 @@@ void copy_extent_buffer(struct extent_b
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * The extent buffer bitmap operations are done with byte granularity because
+  * bitmap items are not guaranteed to be aligned to a word and therefore a
+  * single word in a bitmap may straddle two pages in the extent buffer.
+  */
+ #define BIT_BYTE(nr) ((nr) / BITS_PER_BYTE)
+ #define BYTE_MASK ((1 << BITS_PER_BYTE) - 1)
+ #define BITMAP_FIRST_BYTE_MASK(start) \
+ 	((BYTE_MASK << ((start) & (BITS_PER_BYTE - 1))) & BYTE_MASK)
+ #define BITMAP_LAST_BYTE_MASK(nbits) \
+ 	(BYTE_MASK >> (-(nbits) & (BITS_PER_BYTE - 1)))
+ 
+ /*
+  * eb_bitmap_offset() - calculate the page and offset of the byte containing the
+  * given bit number
+  * @eb: the extent buffer
+  * @start: offset of the bitmap item in the extent buffer
+  * @nr: bit number
+  * @page_index: return index of the page in the extent buffer that contains the
+  * given bit number
+  * @page_offset: return offset into the page given by page_index
+  *
+  * This helper hides the ugliness of finding the byte in an extent buffer which
+  * contains a given bit.
+  */
+ static inline void eb_bitmap_offset(struct extent_buffer *eb,
+ 				    unsigned long start, unsigned long nr,
+ 				    unsigned long *page_index,
+ 				    size_t *page_offset)
+ {
+ 	size_t start_offset = eb->start & ((u64)PAGE_SIZE - 1);
+ 	size_t byte_offset = BIT_BYTE(nr);
+ 	size_t offset;
+ 
+ 	/*
+ 	 * The byte we want is the offset of the extent buffer + the offset of
+ 	 * the bitmap item in the extent buffer + the offset of the byte in the
+ 	 * bitmap item.
+ 	 */
+ 	offset = start_offset + start + byte_offset;
+ 
+ 	*page_index = offset >> PAGE_SHIFT;
+ 	*page_offset = offset & (PAGE_SIZE - 1);
+ }
+ 
+ /**
+  * extent_buffer_test_bit - determine whether a bit in a bitmap item is set
+  * @eb: the extent buffer
+  * @start: offset of the bitmap item in the extent buffer
+  * @nr: bit number to test
+  */
+ int extent_buffer_test_bit(struct extent_buffer *eb, unsigned long start,
+ 			   unsigned long nr)
+ {
+ 	char *kaddr;
+ 	struct page *page;
+ 	unsigned long i;
+ 	size_t offset;
+ 
+ 	eb_bitmap_offset(eb, start, nr, &i, &offset);
+ 	page = eb->pages[i];
+ 	WARN_ON(!PageUptodate(page));
+ 	kaddr = page_address(page);
+ 	return 1U & (kaddr[offset] >> (nr & (BITS_PER_BYTE - 1)));
+ }
+ 
+ /**
+  * extent_buffer_bitmap_set - set an area of a bitmap
+  * @eb: the extent buffer
+  * @start: offset of the bitmap item in the extent buffer
+  * @pos: bit number of the first bit
+  * @len: number of bits to set
+  */
+ void extent_buffer_bitmap_set(struct extent_buffer *eb, unsigned long start,
+ 			      unsigned long pos, unsigned long len)
+ {
+ 	char *kaddr;
+ 	struct page *page;
+ 	unsigned long i;
+ 	size_t offset;
+ 	const unsigned int size = pos + len;
+ 	int bits_to_set = BITS_PER_BYTE - (pos % BITS_PER_BYTE);
+ 	unsigned int mask_to_set = BITMAP_FIRST_BYTE_MASK(pos);
+ 
+ 	eb_bitmap_offset(eb, start, pos, &i, &offset);
+ 	page = eb->pages[i];
+ 	WARN_ON(!PageUptodate(page));
+ 	kaddr = page_address(page);
+ 
+ 	while (len >= bits_to_set) {
+ 		kaddr[offset] |= mask_to_set;
+ 		len -= bits_to_set;
+ 		bits_to_set = BITS_PER_BYTE;
+ 		mask_to_set = ~0U;
+ 		if (++offset >= PAGE_SIZE && len > 0) {
+ 			offset = 0;
+ 			page = eb->pages[++i];
+ 			WARN_ON(!PageUptodate(page));
+ 			kaddr = page_address(page);
+ 		}
+ 	}
+ 	if (len) {
+ 		mask_to_set &= BITMAP_LAST_BYTE_MASK(size);
+ 		kaddr[offset] |= mask_to_set;
+ 	}
+ }
+ 
+ 
+ /**
+  * extent_buffer_bitmap_clear - clear an area of a bitmap
+  * @eb: the extent buffer
+  * @start: offset of the bitmap item in the extent buffer
+  * @pos: bit number of the first bit
+  * @len: number of bits to clear
+  */
+ void extent_buffer_bitmap_clear(struct extent_buffer *eb, unsigned long start,
+ 				unsigned long pos, unsigned long len)
+ {
+ 	char *kaddr;
+ 	struct page *page;
+ 	unsigned long i;
+ 	size_t offset;
+ 	const unsigned int size = pos + len;
+ 	int bits_to_clear = BITS_PER_BYTE - (pos % BITS_PER_BYTE);
+ 	unsigned int mask_to_clear = BITMAP_FIRST_BYTE_MASK(pos);
+ 
+ 	eb_bitmap_offset(eb, start, pos, &i, &offset);
+ 	page = eb->pages[i];
+ 	WARN_ON(!PageUptodate(page));
+ 	kaddr = page_address(page);
+ 
+ 	while (len >= bits_to_clear) {
+ 		kaddr[offset] &= ~mask_to_clear;
+ 		len -= bits_to_clear;
+ 		bits_to_clear = BITS_PER_BYTE;
+ 		mask_to_clear = ~0U;
+ 		if (++offset >= PAGE_SIZE && len > 0) {
+ 			offset = 0;
+ 			page = eb->pages[++i];
+ 			WARN_ON(!PageUptodate(page));
+ 			kaddr = page_address(page);
+ 		}
+ 	}
+ 	if (len) {
+ 		mask_to_clear &= BITMAP_LAST_BYTE_MASK(size);
+ 		kaddr[offset] &= ~mask_to_clear;
+ 	}
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  static inline bool areas_overlap(unsigned long src, unsigned long dst, unsigned long len)
  {
  	unsigned long distance = (src > dst) ? src - dst : dst - src;
diff --cc fs/btrfs/file-item.c
index 7cfa75c01a2a,7a7d6e253cfc..000000000000
--- a/fs/btrfs/file-item.c
+++ b/fs/btrfs/file-item.c
@@@ -201,8 -203,8 +201,13 @@@ static int __btrfs_lookup_bio_sums(stru
  		csum = (u8 *)dst;
  	}
  
++<<<<<<< HEAD
 +	if (bio->bi_size > PAGE_CACHE_SIZE * 8)
 +		path->reada = 2;
++=======
+ 	if (bio->bi_iter.bi_size > PAGE_SIZE * 8)
+ 		path->reada = READA_FORWARD;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	WARN_ON(bio->bi_vcnt <= 0);
  
diff --cc fs/btrfs/file.c
index 8cf385cc7c91,cf31a60c6284..000000000000
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@@ -476,8 -473,7 +476,12 @@@ static void btrfs_drop_pages(struct pag
  		 */
  		ClearPageChecked(pages[i]);
  		unlock_page(pages[i]);
++<<<<<<< HEAD
 +		mark_page_accessed(pages[i]);
 +		page_cache_release(pages[i]);
++=======
+ 		put_page(pages[i]);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  }
  
@@@ -1507,12 -1506,13 +1511,17 @@@ static noinline ssize_t __btrfs_buffere
  		return -ENOMEM;
  
  	while (iov_iter_count(i) > 0) {
++<<<<<<< HEAD
 +		size_t offset = pos & (PAGE_CACHE_SIZE - 1);
++=======
+ 		size_t offset = pos & (PAGE_SIZE - 1);
+ 		size_t sector_offset;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		size_t write_bytes = min(iov_iter_count(i),
- 					 nrptrs * (size_t)PAGE_CACHE_SIZE -
+ 					 nrptrs * (size_t)PAGE_SIZE -
  					 offset);
  		size_t num_pages = DIV_ROUND_UP(write_bytes + offset,
- 						PAGE_CACHE_SIZE);
+ 						PAGE_SIZE);
  		size_t reserve_bytes;
  		size_t dirty_pages;
  		size_t copied;
@@@ -1528,29 -1530,29 +1537,49 @@@
  			break;
  		}
  
 -		sector_offset = pos & (root->sectorsize - 1);
 -		reserve_bytes = round_up(write_bytes + sector_offset,
 -				root->sectorsize);
 +		reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
  
++<<<<<<< HEAD
 +		if (BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
 +					     BTRFS_INODE_PREALLOC)) {
 +			ret = check_can_nocow(inode, pos, &write_bytes);
 +			if (ret < 0)
 +				break;
 +			if (ret > 0) {
 +				/*
 +				 * For nodata cow case, no need to reserve
 +				 * data space.
 +				 */
 +				only_release_metadata = true;
 +				/*
 +				 * our prealloc extent may be smaller than
 +				 * write_bytes, so scale down.
 +				 */
 +				num_pages = DIV_ROUND_UP(write_bytes + offset,
 +							 PAGE_CACHE_SIZE);
 +				reserve_bytes = num_pages << PAGE_CACHE_SHIFT;
 +				goto reserve_metadata;
 +			}
++=======
+ 		if ((BTRFS_I(inode)->flags & (BTRFS_INODE_NODATACOW |
+ 					      BTRFS_INODE_PREALLOC)) &&
+ 		    check_can_nocow(inode, pos, &write_bytes) > 0) {
+ 			/*
+ 			 * For nodata cow case, no need to reserve
+ 			 * data space.
+ 			 */
+ 			only_release_metadata = true;
+ 			/*
+ 			 * our prealloc extent may be smaller than
+ 			 * write_bytes, so scale down.
+ 			 */
+ 			num_pages = DIV_ROUND_UP(write_bytes + offset,
+ 						 PAGE_SIZE);
+ 			reserve_bytes = round_up(write_bytes + sector_offset,
+ 					root->sectorsize);
+ 			goto reserve_metadata;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
 -
  		ret = btrfs_check_data_free_space(inode, pos, write_bytes);
  		if (ret < 0)
  			break;
@@@ -1734,9 -1737,9 +1763,15 @@@ static ssize_t __btrfs_direct_write(str
  	if (err)
  		goto out;
  	written += written_buffered;
++<<<<<<< HEAD
 +	*ppos = pos + written_buffered;
 +	invalidate_mapping_pages(file->f_mapping, pos >> PAGE_CACHE_SHIFT,
 +				 endbyte >> PAGE_CACHE_SHIFT);
++=======
+ 	iocb->ki_pos = pos + written_buffered;
+ 	invalidate_mapping_pages(file->f_mapping, pos >> PAGE_SHIFT,
+ 				 endbyte >> PAGE_SHIFT);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  out:
  	return written ? written : err;
  }
diff --cc fs/btrfs/free-space-cache.c
index f331ef7d573a,5e6062c26129..000000000000
--- a/fs/btrfs/free-space-cache.c
+++ b/fs/btrfs/free-space-cache.c
@@@ -29,8 -29,8 +29,13 @@@
  #include "inode-map.h"
  #include "volumes.h"
  
++<<<<<<< HEAD
 +#define BITS_PER_BITMAP		(PAGE_CACHE_SIZE * 8)
 +#define MAX_CACHE_BYTES_PER_GIG	(32 * 1024)
++=======
+ #define BITS_PER_BITMAP		(PAGE_SIZE * 8)
+ #define MAX_CACHE_BYTES_PER_GIG	SZ_32K
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  struct btrfs_trim_range {
  	u64 start;
diff --cc fs/btrfs/inode-map.c
index 07573dc1614a,70107f7c9307..000000000000
--- a/fs/btrfs/inode-map.c
+++ b/fs/btrfs/inode-map.c
@@@ -282,8 -282,8 +282,13 @@@ void btrfs_unpin_free_ino(struct btrfs_
  	}
  }
  
++<<<<<<< HEAD
 +#define INIT_THRESHOLD	(((1024 * 32) / 2) / sizeof(struct btrfs_free_space))
 +#define INODES_PER_BITMAP (PAGE_CACHE_SIZE * 8)
++=======
+ #define INIT_THRESHOLD	((SZ_32K / 2) / sizeof(struct btrfs_free_space))
+ #define INODES_PER_BITMAP (PAGE_SIZE * 8)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  /*
   * The goal is to keep the memory used by the free_ino tree won't
diff --cc fs/btrfs/inode.c
index ded5036d1968,2aaba58b4856..000000000000
--- a/fs/btrfs/inode.c
+++ b/fs/btrfs/inode.c
@@@ -430,8 -435,8 +430,13 @@@ static noinline void compress_file_rang
  	actual_end = min_t(u64, isize, end + 1);
  again:
  	will_compress = 0;
++<<<<<<< HEAD
 +	nr_pages = (end >> PAGE_CACHE_SHIFT) - (start >> PAGE_CACHE_SHIFT) + 1;
 +	nr_pages = min(nr_pages, (128 * 1024UL) / PAGE_CACHE_SIZE);
++=======
+ 	nr_pages = (end >> PAGE_SHIFT) - (start >> PAGE_SHIFT) + 1;
+ 	nr_pages = min_t(unsigned long, nr_pages, SZ_128K / PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	/*
  	 * we don't want to send crud past the end of i_size through
@@@ -1984,16 -1993,17 +1989,21 @@@ again
  
  	inode = page->mapping->host;
  	page_start = page_offset(page);
- 	page_end = page_offset(page) + PAGE_CACHE_SIZE - 1;
+ 	page_end = page_offset(page) + PAGE_SIZE - 1;
  
 -	lock_extent_bits(&BTRFS_I(inode)->io_tree, page_start, page_end,
 +	lock_extent_bits(&BTRFS_I(inode)->io_tree, page_start, page_end, 0,
  			 &cached_state);
  
  	/* already ordered? We're done */
  	if (PagePrivate2(page))
  		goto out;
  
++<<<<<<< HEAD
 +	ordered = btrfs_lookup_ordered_extent(inode, page_start);
++=======
+ 	ordered = btrfs_lookup_ordered_range(inode, page_start,
+ 					PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (ordered) {
  		unlock_extent_cached(&BTRFS_I(inode)->io_tree, page_start,
  				     page_end, &cached_state, GFP_NOFS);
@@@ -4623,8 -4633,8 +4633,13 @@@ int btrfs_truncate_page(struct inode *i
  	struct extent_state *cached_state = NULL;
  	char *kaddr;
  	u32 blocksize = root->sectorsize;
++<<<<<<< HEAD
 +	pgoff_t index = from >> PAGE_CACHE_SHIFT;
 +	unsigned offset = from & (PAGE_CACHE_SIZE-1);
++=======
+ 	pgoff_t index = from >> PAGE_SHIFT;
+ 	unsigned offset = from & (blocksize - 1);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	struct page *page;
  	gfp_t mask = btrfs_alloc_write_mask(mapping);
  	int ret = 0;
@@@ -4667,15 -4678,15 +4682,15 @@@ again
  	}
  	wait_on_page_writeback(page);
  
 -	lock_extent_bits(io_tree, block_start, block_end, &cached_state);
 +	lock_extent_bits(io_tree, page_start, page_end, 0, &cached_state);
  	set_page_extent_mapped(page);
  
 -	ordered = btrfs_lookup_ordered_extent(inode, block_start);
 +	ordered = btrfs_lookup_ordered_extent(inode, page_start);
  	if (ordered) {
 -		unlock_extent_cached(io_tree, block_start, block_end,
 +		unlock_extent_cached(io_tree, page_start, page_end,
  				     &cached_state, GFP_NOFS);
  		unlock_page(page);
- 		page_cache_release(page);
+ 		put_page(page);
  		btrfs_start_ordered_extent(inode, ordered, 1);
  		btrfs_put_ordered_extent(ordered);
  		goto again;
@@@ -4712,10 -4725,10 +4727,10 @@@
  
  out_unlock:
  	if (ret)
 -		btrfs_delalloc_release_space(inode, block_start,
 -					     blocksize);
 +		btrfs_delalloc_release_space(inode, page_start,
 +					     PAGE_CACHE_SIZE);
  	unlock_page(page);
- 	page_cache_release(page);
+ 	put_page(page);
  out:
  	return ret;
  }
@@@ -8620,7 -8739,9 +8635,13 @@@ static void btrfs_invalidatepage(struc
  	struct btrfs_ordered_extent *ordered;
  	struct extent_state *cached_state = NULL;
  	u64 page_start = page_offset(page);
++<<<<<<< HEAD
 +	u64 page_end = page_start + PAGE_CACHE_SIZE - 1;
++=======
+ 	u64 page_end = page_start + PAGE_SIZE - 1;
+ 	u64 start;
+ 	u64 end;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	int inode_evicting = inode->i_state & I_FREEING;
  
  	/*
@@@ -8740,15 -8869,28 +8761,26 @@@ int btrfs_page_mkwrite(struct vm_area_s
  	loff_t size;
  	int ret;
  	int reserved = 0;
 -	u64 reserved_space;
  	u64 page_start;
  	u64 page_end;
++<<<<<<< HEAD
 +
 +	sb_start_pagefault(inode->i_sb);
 +	page_start = page_offset(page);
 +	page_end = page_start + PAGE_CACHE_SIZE - 1;
++=======
+ 	u64 end;
+ 
+ 	reserved_space = PAGE_SIZE;
+ 
+ 	sb_start_pagefault(inode->i_sb);
+ 	page_start = page_offset(page);
+ 	page_end = page_start + PAGE_SIZE - 1;
+ 	end = page_end;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 -	/*
 -	 * Reserving delalloc space after obtaining the page lock can lead to
 -	 * deadlock. For example, if a dirty page is locked by this function
 -	 * and the call to btrfs_delalloc_reserve_space() ends up triggering
 -	 * dirty page write out, then the btrfs_writepage() function could
 -	 * end up waiting indefinitely to get a lock on the page currently
 -	 * being processed by btrfs_page_mkwrite() function.
 -	 */
  	ret = btrfs_delalloc_reserve_space(inode, page_start,
 -					   reserved_space);
 +					   PAGE_CACHE_SIZE);
  	if (!ret) {
  		ret = file_update_time(vma->vm_file);
  		reserved = 1;
@@@ -8792,6 -8934,18 +8824,21 @@@ again
  		goto again;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (page->index == ((size - 1) >> PAGE_SHIFT)) {
+ 		reserved_space = round_up(size - page_start, root->sectorsize);
+ 		if (reserved_space < PAGE_SIZE) {
+ 			end = page_start + reserved_space - 1;
+ 			spin_lock(&BTRFS_I(inode)->lock);
+ 			BTRFS_I(inode)->outstanding_extents++;
+ 			spin_unlock(&BTRFS_I(inode)->lock);
+ 			btrfs_delalloc_release_space(inode, page_start,
+ 						PAGE_SIZE - reserved_space);
+ 		}
+ 	}
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	/*
  	 * XXX - page_mkwrite gets called every time the page is dirtied, even
  	 * if it was already dirty, so for space accounting reasons we need to
diff --cc fs/btrfs/ioctl.c
index 4557a0834a64,94a0c8a3e871..000000000000
--- a/fs/btrfs/ioctl.c
+++ b/fs/btrfs/ioctl.c
@@@ -1138,10 -1148,10 +1138,10 @@@ again
  			break;
  
  		page_start = page_offset(page);
- 		page_end = page_start + PAGE_CACHE_SIZE - 1;
+ 		page_end = page_start + PAGE_SIZE - 1;
  		while (1) {
  			lock_extent_bits(tree, page_start, page_end,
 -					 &cached_state);
 +					 0, &cached_state);
  			ordered = btrfs_lookup_ordered_extent(inode,
  							      page_start);
  			unlock_extent_cached(tree, page_start, page_end,
@@@ -1198,10 -1208,10 +1198,10 @@@
  		wait_on_page_writeback(pages[i]);
  
  	page_start = page_offset(pages[0]);
- 	page_end = page_offset(pages[i_done - 1]) + PAGE_CACHE_SIZE;
+ 	page_end = page_offset(pages[i_done - 1]) + PAGE_SIZE;
  
  	lock_extent_bits(&BTRFS_I(inode)->io_tree,
 -			 page_start, page_end - 1, &cached_state);
 +			 page_start, page_end - 1, 0, &cached_state);
  	clear_extent_bit(&BTRFS_I(inode)->io_tree, page_start,
  			  page_end - 1, EXTENT_DIRTY | EXTENT_DELALLOC |
  			  EXTENT_DO_ACCOUNTING | EXTENT_DEFRAG, 0, 0,
@@@ -1263,9 -1273,9 +1263,13 @@@ int btrfs_defrag_file(struct inode *ino
  	int defrag_count = 0;
  	int compress_type = BTRFS_COMPRESS_ZLIB;
  	u32 extent_thresh = range->extent_thresh;
++<<<<<<< HEAD
 +	unsigned long max_cluster = (256 * 1024) >> PAGE_CACHE_SHIFT;
++=======
+ 	unsigned long max_cluster = SZ_256K >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	unsigned long cluster = max_cluster;
 -	u64 new_align = ~((u64)SZ_128K - 1);
 +	u64 new_align = ~((u64)128 * 1024 - 1);
  	struct page **pages = NULL;
  
  	if (isize == 0)
@@@ -1402,14 -1412,13 +1406,14 @@@
  				i += ret;
  
  			newer_off = max(newer_off + 1,
- 					(u64)i << PAGE_CACHE_SHIFT);
+ 					(u64)i << PAGE_SHIFT);
  
 -			ret = find_new_extents(root, inode, newer_than,
 -					       &newer_off, SZ_64K);
 +			ret = find_new_extents(root, inode,
 +					       newer_than, &newer_off,
 +					       64 * 1024);
  			if (!ret) {
  				range->start = newer_off;
- 				i = (newer_off & new_align) >> PAGE_CACHE_SHIFT;
+ 				i = (newer_off & new_align) >> PAGE_SHIFT;
  			} else {
  				break;
  			}
@@@ -3171,57 -3178,20 +3175,57 @@@ out_unlock
  	return ret;
  }
  
 -#define BTRFS_MAX_DEDUPE_LEN	SZ_16M
 +#define BTRFS_MAX_DEDUPE_LEN	(16 * 1024 * 1024)
  
 -ssize_t btrfs_dedupe_file_range(struct file *src_file, u64 loff, u64 olen,
 -				struct file *dst_file, u64 dst_loff)
 +static long btrfs_ioctl_file_extent_same(struct file *file,
 +			struct btrfs_ioctl_same_args __user *argp)
  {
 -	struct inode *src = file_inode(src_file);
 -	struct inode *dst = file_inode(dst_file);
 +	struct btrfs_ioctl_same_args *same = NULL;
 +	struct btrfs_ioctl_same_extent_info *info;
 +	struct inode *src = file_inode(file);
 +	u64 off;
 +	u64 len;
 +	int i;
 +	int ret;
 +	unsigned long size;
  	u64 bs = BTRFS_I(src)->root->fs_info->sb->s_blocksize;
 -	ssize_t res;
 +	bool is_admin = capable(CAP_SYS_ADMIN);
 +	u16 count;
 +
 +	if (!(file->f_mode & FMODE_READ))
 +		return -EINVAL;
 +
 +	ret = mnt_want_write_file(file);
 +	if (ret)
 +		return ret;
  
 -	if (olen > BTRFS_MAX_DEDUPE_LEN)
 -		olen = BTRFS_MAX_DEDUPE_LEN;
 +	if (get_user(count, &argp->dest_count)) {
 +		ret = -EFAULT;
 +		goto out;
 +	}
 +
 +	size = offsetof(struct btrfs_ioctl_same_args __user, info[count]);
 +
 +	same = memdup_user(argp, size);
 +
 +	if (IS_ERR(same)) {
 +		ret = PTR_ERR(same);
 +		same = NULL;
 +		goto out;
 +	}
 +
 +	off = same->logical_offset;
 +	len = same->length;
 +
 +	/*
 +	 * Limit the total length we will dedupe for each operation.
 +	 * This is intended to bound the total time spent in this
 +	 * ioctl to something sane.
 +	 */
 +	if (len > BTRFS_MAX_DEDUPE_LEN)
 +		len = BTRFS_MAX_DEDUPE_LEN;
  
- 	if (WARN_ON_ONCE(bs < PAGE_CACHE_SIZE)) {
+ 	if (WARN_ON_ONCE(bs < PAGE_SIZE)) {
  		/*
  		 * Btrfs does not support blocksize < page_size. As a
  		 * result, btrfs_cmp_data() won't correctly handle
@@@ -3995,8 -3890,9 +3999,14 @@@ static noinline long btrfs_ioctl_clone(
  	 * Truncate page cache pages so that future reads will see the cloned
  	 * data immediately and not the previous data.
  	 */
++<<<<<<< HEAD
 +	truncate_inode_pages_range(&inode->i_data, destoff,
 +				   PAGE_CACHE_ALIGN(destoff + len) - 1);
++=======
+ 	truncate_inode_pages_range(&inode->i_data,
+ 				round_down(destoff, PAGE_SIZE),
+ 				round_up(destoff + len, PAGE_SIZE) - 1);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  out_unlock:
  	if (!same_inode)
  		btrfs_double_inode_unlock(src, inode);
diff --cc fs/btrfs/raid56.c
index addd0df53624,0b7792e02dd5..000000000000
--- a/fs/btrfs/raid56.c
+++ b/fs/btrfs/raid56.c
@@@ -949,8 -962,7 +949,12 @@@ static struct page *page_in_rbio(struc
   */
  static unsigned long rbio_nr_pages(unsigned long stripe_len, int nr_stripes)
  {
++<<<<<<< HEAD
 +	unsigned long nr = stripe_len * nr_stripes;
 +	return DIV_ROUND_UP(nr, PAGE_CACHE_SIZE);
++=======
+ 	return DIV_ROUND_UP(stripe_len, PAGE_SIZE) * nr_stripes;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  /*
@@@ -1083,10 -1094,10 +1087,10 @@@ static int rbio_add_io_page(struct btrf
  		 * devices or if they are not contiguous
  		 */
  		if (last_end == disk_start && stripe->dev->bdev &&
 -		    !last->bi_error &&
 +		    test_bit(BIO_UPTODATE, &last->bi_flags) &&
  		    last->bi_bdev == stripe->dev->bdev) {
- 			ret = bio_add_page(last, page, PAGE_CACHE_SIZE, 0);
- 			if (ret == PAGE_CACHE_SIZE)
+ 			ret = bio_add_page(last, page, PAGE_SIZE, 0);
+ 			if (ret == PAGE_SIZE)
  				return 0;
  		}
  	}
@@@ -1096,12 -1107,11 +1100,12 @@@
  	if (!bio)
  		return -ENOMEM;
  
 -	bio->bi_iter.bi_size = 0;
 +	bio->bi_size = 0;
  	bio->bi_bdev = stripe->dev->bdev;
 -	bio->bi_iter.bi_sector = disk_start >> 9;
 +	bio->bi_sector = disk_start >> 9;
 +	set_bit(BIO_UPTODATE, &bio->bi_flags);
  
- 	bio_add_page(bio, page, PAGE_CACHE_SIZE, 0);
+ 	bio_add_page(bio, page, PAGE_SIZE, 0);
  	bio_list_add(bio_list, bio);
  	return 0;
  }
@@@ -1154,9 -1152,9 +1158,9 @@@ static void index_rbio_pages(struct btr
  
  	spin_lock_irq(&rbio->bio_list_lock);
  	bio_list_for_each(bio, &rbio->bio_list) {
 -		start = (u64)bio->bi_iter.bi_sector << 9;
 +		start = (u64)bio->bi_sector << 9;
  		stripe_offset = start - rbio->bbio->raid_map[0];
- 		page_index = stripe_offset >> PAGE_CACHE_SHIFT;
+ 		page_index = stripe_offset >> PAGE_SHIFT;
  
  		for (i = 0; i < bio->bi_vcnt; i++) {
  			p = bio->bi_io_vec[i].bv_page;
diff --cc fs/btrfs/reada.c
index 619f92963e27,298631eaee78..000000000000
--- a/fs/btrfs/reada.c
+++ b/fs/btrfs/reada.c
@@@ -233,15 -217,29 +233,29 @@@ static int __readahead_hook(struct btrf
   * start is passed separately in case eb in NULL, which may be the case with
   * failed I/O
   */
 -int btree_readahead_hook(struct btrfs_fs_info *fs_info,
 -			 struct extent_buffer *eb, u64 start, int err)
 +int btree_readahead_hook(struct btrfs_root *root, struct extent_buffer *eb,
 +			 u64 start, int err)
  {
 -	int ret = 0;
 -	struct reada_extent *re;
 +	int ret;
  
++<<<<<<< HEAD
 +	ret = __readahead_hook(root, eb, start, err);
++=======
+ 	/* find extent */
+ 	spin_lock(&fs_info->reada_lock);
+ 	re = radix_tree_lookup(&fs_info->reada_tree,
+ 			       start >> PAGE_SHIFT);
+ 	if (re)
+ 		re->refcnt++;
+ 	spin_unlock(&fs_info->reada_lock);
+ 	if (!re) {
+ 		ret = -1;
+ 		goto start_machine;
+ 	}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 -	__readahead_hook(fs_info, re, eb, start, err);
 -	reada_extent_put(fs_info, re);	/* our ref */
 +	reada_start_machine(root->fs_info);
  
 -start_machine:
 -	reada_start_machine(fs_info);
  	return ret;
  }
  
@@@ -259,19 -257,15 +273,24 @@@ static struct reada_zone *reada_find_zo
  	zone = NULL;
  	spin_lock(&fs_info->reada_lock);
  	ret = radix_tree_gang_lookup(&dev->reada_zones, (void **)&zone,
++<<<<<<< HEAD
 +				     logical >> PAGE_CACHE_SHIFT, 1);
 +	if (ret == 1)
++=======
+ 				     logical >> PAGE_SHIFT, 1);
+ 	if (ret == 1 && logical >= zone->start && logical <= zone->end) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		kref_get(&zone->refcnt);
 +	spin_unlock(&fs_info->reada_lock);
 +
 +	if (ret == 1) {
 +		if (logical >= zone->start && logical < zone->end)
 +			return zone;
 +		spin_lock(&fs_info->reada_lock);
 +		kref_put(&zone->refcnt, reada_zone_release);
  		spin_unlock(&fs_info->reada_lock);
 -		return zone;
  	}
  
 -	spin_unlock(&fs_info->reada_lock);
 -
  	cache = btrfs_lookup_block_group(fs_info, logical);
  	if (!cache)
  		return NULL;
@@@ -306,9 -300,11 +325,14 @@@
  	if (ret == -EEXIST) {
  		kfree(zone);
  		ret = radix_tree_gang_lookup(&dev->reada_zones, (void **)&zone,
++<<<<<<< HEAD
 +					     logical >> PAGE_CACHE_SHIFT, 1);
 +		if (ret == 1)
++=======
+ 					     logical >> PAGE_SHIFT, 1);
+ 		if (ret == 1 && logical >= zone->start && logical <= zone->end)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			kref_get(&zone->refcnt);
 -		else
 -			zone = NULL;
  	}
  	spin_unlock(&fs_info->reada_lock);
  
@@@ -330,9 -326,9 +354,13 @@@ static struct reada_extent *reada_find_
  	u64 length;
  	int real_stripes;
  	int nzones = 0;
++<<<<<<< HEAD
 +	int i;
 +	unsigned long index = logical >> PAGE_CACHE_SHIFT;
++=======
+ 	unsigned long index = logical >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	int dev_replace_is_ongoing;
 -	int have_zone = 0;
  
  	spin_lock(&fs_info->reada_lock);
  	re = radix_tree_lookup(&fs_info->reada_tree, index);
@@@ -678,8 -673,8 +706,13 @@@ static int reada_start_machine_dev(stru
  	 * plugging to speed things up
  	 */
  	ret = radix_tree_gang_lookup(&dev->reada_extents, (void **)&re,
++<<<<<<< HEAD
 +				     dev->reada_next >> PAGE_CACHE_SHIFT, 1);
 +	if (ret == 0 || re->logical >= dev->reada_curr_zone->end) {
++=======
+ 				     dev->reada_next >> PAGE_SHIFT, 1);
+ 	if (ret == 0 || re->logical > dev->reada_curr_zone->end) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		ret = reada_pick_zone(dev);
  		if (!ret) {
  			spin_unlock(&fs_info->reada_lock);
@@@ -878,8 -879,8 +911,13 @@@ static void dump_devs(struct btrfs_fs_i
  					     index, 1);
  		if (ret == 0)
  			break;
++<<<<<<< HEAD
 +		if (!re->scheduled_for) {
 +			index = (re->logical >> PAGE_CACHE_SHIFT) + 1;
++=======
+ 		if (!re->scheduled) {
+ 			index = (re->logical >> PAGE_SHIFT) + 1;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			continue;
  		}
  		printk(KERN_DEBUG
diff --cc fs/btrfs/send.c
index 63a6152be04b,8d358c547c59..000000000000
--- a/fs/btrfs/send.c
+++ b/fs/btrfs/send.c
@@@ -4480,8 -4481,8 +4480,13 @@@ static ssize_t fill_read_buf(struct sen
  
  	while (index <= last_index) {
  		unsigned cur_len = min_t(unsigned, len,
++<<<<<<< HEAD
 +					 PAGE_CACHE_SIZE - pg_offset);
 +		page = find_or_create_page(inode->i_mapping, index, GFP_NOFS);
++=======
+ 					 PAGE_SIZE - pg_offset);
+ 		page = find_or_create_page(inode->i_mapping, index, GFP_KERNEL);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		if (!page) {
  			ret = -ENOMEM;
  			break;
diff --cc fs/btrfs/tests/extent-io-tests.c
index 9e9f2368177d,ac3a06d28531..000000000000
--- a/fs/btrfs/tests/extent-io-tests.c
+++ b/fs/btrfs/tests/extent-io-tests.c
@@@ -89,8 -93,8 +89,13 @@@ static int test_find_delalloc(void
  	 * everything to make sure our pages don't get evicted and screw up our
  	 * test.
  	 */
++<<<<<<< HEAD
 +	for (index = 0; index < (total_dirty >> PAGE_CACHE_SHIFT); index++) {
 +		page = find_or_create_page(inode->i_mapping, index, GFP_NOFS);
++=======
+ 	for (index = 0; index < (total_dirty >> PAGE_SHIFT); index++) {
+ 		page = find_or_create_page(inode->i_mapping, index, GFP_KERNEL);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		if (!page) {
  			test_msg("Failed to allocate test page\n");
  			ret = -ENOMEM;
@@@ -133,9 -137,9 +138,9 @@@
  	 * |--- delalloc ---|
  	 *           |--- search ---|
  	 */
 -	test_start = SZ_64M;
 +	test_start = 64 * 1024 * 1024;
  	locked_page = find_lock_page(inode->i_mapping,
- 				     test_start >> PAGE_CACHE_SHIFT);
+ 				     test_start >> PAGE_SHIFT);
  	if (!locked_page) {
  		test_msg("Couldn't find the locked page\n");
  		goto out_bits;
@@@ -220,8 -224,8 +225,13 @@@
  	 * Now to test where we run into a page that is no longer dirty in the
  	 * range we want to find.
  	 */
++<<<<<<< HEAD
 +	page = find_get_page(inode->i_mapping, (max_bytes + (1 * 1024 * 1024))
 +			     >> PAGE_CACHE_SHIFT);
++=======
+ 	page = find_get_page(inode->i_mapping,
+ 			     (max_bytes + SZ_1M) >> PAGE_SHIFT);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (!page) {
  		test_msg("Couldn't find our page\n");
  		goto out_bits;
@@@ -258,18 -262,149 +268,142 @@@
  	}
  	ret = 0;
  out_bits:
 -	clear_extent_bits(&tmp, 0, total_dirty - 1, (unsigned)-1, GFP_KERNEL);
 +	clear_extent_bits(&tmp, 0, total_dirty - 1, (unsigned)-1, GFP_NOFS);
  out:
  	if (locked_page)
- 		page_cache_release(locked_page);
+ 		put_page(locked_page);
  	process_page_range(inode, 0, total_dirty - 1,
  			   PROCESS_UNLOCK | PROCESS_RELEASE);
  	iput(inode);
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int __test_eb_bitmaps(unsigned long *bitmap, struct extent_buffer *eb,
+ 			     unsigned long len)
+ {
+ 	unsigned long i, x;
+ 
+ 	memset(bitmap, 0, len);
+ 	memset_extent_buffer(eb, 0, 0, len);
+ 	if (memcmp_extent_buffer(eb, bitmap, 0, len) != 0) {
+ 		test_msg("Bitmap was not zeroed\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	bitmap_set(bitmap, 0, len * BITS_PER_BYTE);
+ 	extent_buffer_bitmap_set(eb, 0, 0, len * BITS_PER_BYTE);
+ 	if (memcmp_extent_buffer(eb, bitmap, 0, len) != 0) {
+ 		test_msg("Setting all bits failed\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	bitmap_clear(bitmap, 0, len * BITS_PER_BYTE);
+ 	extent_buffer_bitmap_clear(eb, 0, 0, len * BITS_PER_BYTE);
+ 	if (memcmp_extent_buffer(eb, bitmap, 0, len) != 0) {
+ 		test_msg("Clearing all bits failed\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	bitmap_set(bitmap, (PAGE_SIZE - sizeof(long) / 2) * BITS_PER_BYTE,
+ 		   sizeof(long) * BITS_PER_BYTE);
+ 	extent_buffer_bitmap_set(eb, PAGE_SIZE - sizeof(long) / 2, 0,
+ 				 sizeof(long) * BITS_PER_BYTE);
+ 	if (memcmp_extent_buffer(eb, bitmap, 0, len) != 0) {
+ 		test_msg("Setting straddling pages failed\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	bitmap_set(bitmap, 0, len * BITS_PER_BYTE);
+ 	bitmap_clear(bitmap,
+ 		     (PAGE_SIZE - sizeof(long) / 2) * BITS_PER_BYTE,
+ 		     sizeof(long) * BITS_PER_BYTE);
+ 	extent_buffer_bitmap_set(eb, 0, 0, len * BITS_PER_BYTE);
+ 	extent_buffer_bitmap_clear(eb, PAGE_SIZE - sizeof(long) / 2, 0,
+ 				   sizeof(long) * BITS_PER_BYTE);
+ 	if (memcmp_extent_buffer(eb, bitmap, 0, len) != 0) {
+ 		test_msg("Clearing straddling pages failed\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	/*
+ 	 * Generate a wonky pseudo-random bit pattern for the sake of not using
+ 	 * something repetitive that could miss some hypothetical off-by-n bug.
+ 	 */
+ 	x = 0;
+ 	for (i = 0; i < len / sizeof(long); i++) {
+ 		x = (0x19660dULL * (u64)x + 0x3c6ef35fULL) & 0xffffffffUL;
+ 		bitmap[i] = x;
+ 	}
+ 	write_extent_buffer(eb, bitmap, 0, len);
+ 
+ 	for (i = 0; i < len * BITS_PER_BYTE; i++) {
+ 		int bit, bit1;
+ 
+ 		bit = !!test_bit(i, bitmap);
+ 		bit1 = !!extent_buffer_test_bit(eb, 0, i);
+ 		if (bit1 != bit) {
+ 			test_msg("Testing bit pattern failed\n");
+ 			return -EINVAL;
+ 		}
+ 
+ 		bit1 = !!extent_buffer_test_bit(eb, i / BITS_PER_BYTE,
+ 						i % BITS_PER_BYTE);
+ 		if (bit1 != bit) {
+ 			test_msg("Testing bit pattern with offset failed\n");
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int test_eb_bitmaps(void)
+ {
+ 	unsigned long len = PAGE_SIZE * 4;
+ 	unsigned long *bitmap;
+ 	struct extent_buffer *eb;
+ 	int ret;
+ 
+ 	test_msg("Running extent buffer bitmap tests\n");
+ 
+ 	bitmap = kmalloc(len, GFP_KERNEL);
+ 	if (!bitmap) {
+ 		test_msg("Couldn't allocate test bitmap\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	eb = __alloc_dummy_extent_buffer(NULL, 0, len);
+ 	if (!eb) {
+ 		test_msg("Couldn't allocate test extent buffer\n");
+ 		kfree(bitmap);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	ret = __test_eb_bitmaps(bitmap, eb, len);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/* Do it over again with an extent buffer which isn't page-aligned. */
+ 	free_extent_buffer(eb);
+ 	eb = __alloc_dummy_extent_buffer(NULL, PAGE_SIZE / 2, len);
+ 	if (!eb) {
+ 		test_msg("Couldn't allocate test extent buffer\n");
+ 		kfree(bitmap);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	ret = __test_eb_bitmaps(bitmap, eb, len);
+ out:
+ 	free_extent_buffer(eb);
+ 	kfree(bitmap);
+ 	return ret;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  int btrfs_test_extent_io(void)
  {
 -	int ret;
 -
 -	test_msg("Running extent I/O tests\n");
 -
 -	ret = test_find_delalloc();
 -	if (ret)
 -		goto out;
 -
 -	ret = test_eb_bitmaps();
 -out:
 -	test_msg("Extent I/O tests finished\n");
 -	return ret;
 +	test_msg("Running find delalloc tests\n");
 +	return test_find_delalloc();
  }
diff --cc fs/btrfs/tests/free-space-tests.c
index 8b72b005bfb9,514247515312..000000000000
--- a/fs/btrfs/tests/free-space-tests.c
+++ b/fs/btrfs/tests/free-space-tests.c
@@@ -22,42 -22,7 +22,46 @@@
  #include "../disk-io.h"
  #include "../free-space-cache.h"
  
++<<<<<<< HEAD
 +#define BITS_PER_BITMAP		(PAGE_CACHE_SIZE * 8)
 +static struct btrfs_block_group_cache *init_test_block_group(void)
 +{
 +	struct btrfs_block_group_cache *cache;
 +
 +	cache = kzalloc(sizeof(*cache), GFP_NOFS);
 +	if (!cache)
 +		return NULL;
 +	cache->free_space_ctl = kzalloc(sizeof(*cache->free_space_ctl),
 +					GFP_NOFS);
 +	if (!cache->free_space_ctl) {
 +		kfree(cache);
 +		return NULL;
 +	}
 +	cache->fs_info = btrfs_alloc_dummy_fs_info();
 +	if (!cache->fs_info) {
 +		kfree(cache->free_space_ctl);
 +		kfree(cache);
 +		return NULL;
 +	}
 +
 +	cache->key.objectid = 0;
 +	cache->key.offset = 1024 * 1024 * 1024;
 +	cache->key.type = BTRFS_BLOCK_GROUP_ITEM_KEY;
 +	cache->sectorsize = 4096;
 +	cache->full_stripe_len = 4096;
 +
 +	spin_lock_init(&cache->lock);
 +	INIT_LIST_HEAD(&cache->list);
 +	INIT_LIST_HEAD(&cache->cluster_list);
 +	INIT_LIST_HEAD(&cache->bg_list);
 +
 +	btrfs_init_free_space_ctl(cache);
 +
 +	return cache;
 +}
++=======
+ #define BITS_PER_BITMAP		(PAGE_SIZE * 8)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  /*
   * This test just does basic sanity checking, making sure we can add an exten
diff --cc fs/buffer.c
index ace86fe2bca8,af0d9a82a8ed..000000000000
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@@ -206,8 -207,8 +206,13 @@@ __find_get_block_slow(struct block_devi
  	struct page *page;
  	int all_mapped = 1;
  
++<<<<<<< HEAD
 +	index = block >> (PAGE_CACHE_SHIFT - bd_inode->i_blkbits);
 +	page = find_get_page(bd_mapping, index);
++=======
+ 	index = block >> (PAGE_SHIFT - bd_inode->i_blkbits);
+ 	page = find_get_page_flags(bd_mapping, index, FGP_ACCESSED);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (!page)
  		goto out;
  
@@@ -2109,9 -2136,9 +2114,13 @@@ int block_is_partially_uptodate(struct 
  
  	head = page_buffers(page);
  	blocksize = head->b_size;
++<<<<<<< HEAD
 +	to = min_t(unsigned, PAGE_CACHE_SIZE - from, desc->count);
++=======
+ 	to = min_t(unsigned, PAGE_SIZE - from, count);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	to = from + to;
- 	if (from < blocksize && to > PAGE_CACHE_SIZE - blocksize)
+ 	if (from < blocksize && to > PAGE_SIZE - blocksize)
  		return 0;
  
  	bh = head;
@@@ -2883,7 -2895,7 +2892,11 @@@ int block_write_full_page(struct page *
  		 * they may have been added in ext3_writepage().  Make them
  		 * freeable here, so the page does not leak.
  		 */
++<<<<<<< HEAD
 +		do_invalidatepage(page, 0);
++=======
+ 		do_invalidatepage(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		unlock_page(page);
  		return 0; /* don't care */
  	}
diff --cc fs/ceph/addr.c
index b438d195f38c,4801571f51cb..000000000000
--- a/fs/ceph/addr.c
+++ b/fs/ceph/addr.c
@@@ -141,9 -143,9 +141,15 @@@ static void ceph_invalidatepage(struct 
  	inode = page->mapping->host;
  	ci = ceph_inode(inode);
  
++<<<<<<< HEAD
 +	if (offset != 0) {
 +		dout("%p invalidatepage %p idx %lu partial dirty page %lu\n",
 +		     inode, page, page->index, offset);
++=======
+ 	if (offset != 0 || length != PAGE_SIZE) {
+ 		dout("%p invalidatepage %p idx %lu partial dirty page %u~%u\n",
+ 		     inode, page, page->index, offset, length);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		return;
  	}
  
@@@ -217,12 -231,12 +223,16 @@@ static int readpage_nounlock(struct fil
  		err = 0;
  	if (err < 0) {
  		SetPageError(page);
 -		ceph_fscache_readpage_cancel(inode, page);
  		goto out;
  	}
++<<<<<<< HEAD
 +
 +	if (err < PAGE_CACHE_SIZE)
++=======
+ 	if (err < PAGE_SIZE)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		/* zero fill remainder of page */
- 		zero_user_segment(page, err, PAGE_CACHE_SIZE);
+ 		zero_user_segment(page, err, PAGE_SIZE);
  	else
  		flush_dcache_page(page);
  
@@@ -271,10 -287,11 +281,10 @@@ static void finish_read(struct ceph_osd
  		     page->index);
  		flush_dcache_page(page);
  		SetPageUptodate(page);
 -		ceph_readpage_to_fscache(inode, page);
  unlock:
  		unlock_page(page);
- 		page_cache_release(page);
- 		bytes -= PAGE_CACHE_SIZE;
+ 		put_page(page);
+ 		bytes -= PAGE_SIZE;
  	}
  	kfree(osd_data->pages);
  }
@@@ -346,7 -363,8 +356,12 @@@ static int start_read(struct inode *ino
  		     page->index);
  		if (add_to_page_cache_lru(page, &inode->i_data, page->index,
  					  GFP_KERNEL)) {
++<<<<<<< HEAD
 +			page_cache_release(page);
++=======
+ 			ceph_fscache_uncache_page(inode, page);
+ 			put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			dout("start_read %p add_to_page_cache failed %p\n",
  			     inode, page);
  			nr_pages = i;
@@@ -389,8 -409,14 +404,19 @@@ static int ceph_readpages(struct file *
  	if (ceph_inode(inode)->i_inline_version != CEPH_INLINE_NONE)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	if (fsc->mount_options->rsize >= PAGE_CACHE_SIZE)
 +		max = (fsc->mount_options->rsize + PAGE_CACHE_SIZE - 1)
++=======
+ 	rc = ceph_readpages_from_fscache(mapping->host, mapping, page_list,
+ 					 &nr_pages);
+ 
+ 	if (rc == 0)
+ 		goto out;
+ 
+ 	if (fsc->mount_options->rsize >= PAGE_SIZE)
+ 		max = (fsc->mount_options->rsize + PAGE_SIZE - 1)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			>> PAGE_SHIFT;
  
  	dout("readpages %p file %p nr_pages %d max %d\n", inode,
diff --cc fs/ceph/file.c
index 1d49d61253bc,a79f9269831e..000000000000
--- a/fs/ceph/file.c
+++ b/fs/ceph/file.c
@@@ -877,12 -872,12 +877,12 @@@ ceph_direct_read_write(struct kiocb *io
  			 * may block.
  			 */
  			truncate_inode_pages_range(inode->i_mapping, pos,
- 					(pos+len) | (PAGE_CACHE_SIZE - 1));
+ 					(pos+len) | (PAGE_SIZE - 1));
  
  			osd_req_op_init(req, 1, CEPH_OSD_OP_STARTSYNC, 0);
 +			req->r_mtime = mtime;
  		}
  
 -
  		osd_req_op_extent_osd_data_pages(req, 0, pages, len, start,
  						 false, false);
  
@@@ -1267,9 -1187,28 +1267,32 @@@ out
  		i_size = i_size_read(inode);
  		if (retry_op == READ_INLINE) {
  			BUG_ON(ret > 0 || read > 0);
++<<<<<<< HEAD
 +			ret = inline_to_iov(iocb, &i, page, statret, i_size);
++=======
+ 			if (iocb->ki_pos < i_size &&
+ 			    iocb->ki_pos < PAGE_SIZE) {
+ 				loff_t end = min_t(loff_t, i_size,
+ 						   iocb->ki_pos + len);
+ 				end = min_t(loff_t, end, PAGE_SIZE);
+ 				if (statret < end)
+ 					zero_user_segment(page, statret, end);
+ 				ret = copy_page_to_iter(page,
+ 						iocb->ki_pos & ~PAGE_MASK,
+ 						end - iocb->ki_pos, to);
+ 				iocb->ki_pos += ret;
+ 				read += ret;
+ 			}
+ 			if (iocb->ki_pos < i_size && read < len) {
+ 				size_t zlen = min_t(size_t, len - read,
+ 						    i_size - iocb->ki_pos);
+ 				ret = iov_iter_zero(zlen, to);
+ 				iocb->ki_pos += ret;
+ 				read += ret;
+ 			}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			__free_pages(page, 0);
 -			return read;
 +			return ret;
  		}
  
  		/* hit EOF or hole? */
diff --cc fs/ceph/super.c
index 35bf1dbe5f99,f12d5e2955c2..000000000000
--- a/fs/ceph/super.c
+++ b/fs/ceph/super.c
@@@ -908,7 -918,7 +908,11 @@@ static int ceph_register_bdi(struct sup
  			>> PAGE_SHIFT;
  	else
  		fsc->backing_dev_info.ra_pages =
++<<<<<<< HEAD
 +			default_backing_dev_info.ra_pages;
++=======
+ 			VM_MAX_READAHEAD * 1024 / PAGE_SIZE;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	err = bdi_register(&fsc->backing_dev_info, NULL, "ceph-%ld",
  			   atomic_long_inc_return(&bdi_seq));
diff --cc fs/cifs/cifsfs.c
index 9f6ff2423731,89201564c346..000000000000
--- a/fs/cifs/cifsfs.c
+++ b/fs/cifs/cifsfs.c
@@@ -952,11 -928,62 +952,67 @@@ const struct inode_operations cifs_syml
  #endif
  };
  
++<<<<<<< HEAD
++=======
+ static int cifs_clone_file_range(struct file *src_file, loff_t off,
+ 		struct file *dst_file, loff_t destoff, u64 len)
+ {
+ 	struct inode *src_inode = file_inode(src_file);
+ 	struct inode *target_inode = file_inode(dst_file);
+ 	struct cifsFileInfo *smb_file_src = src_file->private_data;
+ 	struct cifsFileInfo *smb_file_target = dst_file->private_data;
+ 	struct cifs_tcon *target_tcon = tlink_tcon(smb_file_target->tlink);
+ 	unsigned int xid;
+ 	int rc;
+ 
+ 	cifs_dbg(FYI, "clone range\n");
+ 
+ 	xid = get_xid();
+ 
+ 	if (!src_file->private_data || !dst_file->private_data) {
+ 		rc = -EBADF;
+ 		cifs_dbg(VFS, "missing cifsFileInfo on copy range src file\n");
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * Note: cifs case is easier than btrfs since server responsible for
+ 	 * checks for proper open modes and file type and if it wants
+ 	 * server could even support copy of range where source = target
+ 	 */
+ 	lock_two_nondirectories(target_inode, src_inode);
+ 
+ 	if (len == 0)
+ 		len = src_inode->i_size - off;
+ 
+ 	cifs_dbg(FYI, "about to flush pages\n");
+ 	/* should we flush first and last page first */
+ 	truncate_inode_pages_range(&target_inode->i_data, destoff,
+ 				   PAGE_ALIGN(destoff + len)-1);
+ 
+ 	if (target_tcon->ses->server->ops->duplicate_extents)
+ 		rc = target_tcon->ses->server->ops->duplicate_extents(xid,
+ 			smb_file_src, smb_file_target, off, len, destoff);
+ 	else
+ 		rc = -EOPNOTSUPP;
+ 
+ 	/* force revalidate of size and timestamps of target file now
+ 	   that target is updated on the server */
+ 	CIFS_I(target_inode)->time = 0;
+ 	/* although unlocking in the reverse order from locking is not
+ 	   strictly necessary here it is a little cleaner to be consistent */
+ 	unlock_two_nondirectories(src_inode, target_inode);
+ out:
+ 	free_xid(xid);
+ 	return rc;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  const struct file_operations cifs_file_ops = {
 -	.read_iter = cifs_loose_read_iter,
 -	.write_iter = cifs_file_write_iter,
 +	.read = do_sync_read,
 +	.write = do_sync_write,
 +	.aio_read = cifs_loose_read,
 +	.aio_write = cifs_file_aio_write,
  	.open = cifs_open,
  	.release = cifs_close,
  	.lock = cifs_lock,
diff --cc fs/cifs/file.c
index c259c09ba911,5ce540dc6996..000000000000
--- a/fs/cifs/file.c
+++ b/fs/cifs/file.c
@@@ -3447,13 -3415,12 +3447,13 @@@ readpages_get_pages(struct address_spac
  			break;
  
  		/* would this page push the read over the rsize? */
- 		if (*bytes + PAGE_CACHE_SIZE > rsize)
+ 		if (*bytes + PAGE_SIZE > rsize)
  			break;
  
 -		__SetPageLocked(page);
 -		if (add_to_page_cache_locked(page, mapping, page->index, gfp)) {
 -			__ClearPageLocked(page);
 +		__set_page_locked(page);
 +		if (add_to_page_cache_locked(page, mapping, page->index,
 +								GFP_KERNEL)) {
 +			__clear_page_locked(page);
  			break;
  		}
  		list_move_tail(&page->lru, tmplist);
@@@ -3796,7 -3764,7 +3796,11 @@@ static void cifs_invalidate_page(struc
  {
  	struct cifsInodeInfo *cifsi = CIFS_I(page->mapping->host);
  
++<<<<<<< HEAD
 +	if (offset == 0)
++=======
+ 	if (offset == 0 && length == PAGE_SIZE)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		cifs_fscache_invalidate_page(page, &cifsi->vfs_inode);
  }
  
diff --cc fs/cramfs/inode.c
index 35b1c7bd18b7,2096654dd26d..000000000000
--- a/fs/cramfs/inode.c
+++ b/fs/cramfs/inode.c
@@@ -208,13 -227,14 +208,13 @@@ static void *cramfs_read(struct super_b
  	data = read_buffers[buffer];
  	for (i = 0; i < BLKS_PER_BUF; i++) {
  		struct page *page = pages[i];
 -
  		if (page) {
- 			memcpy(data, kmap(page), PAGE_CACHE_SIZE);
+ 			memcpy(data, kmap(page), PAGE_SIZE);
  			kunmap(page);
- 			page_cache_release(page);
+ 			put_page(page);
  		} else
- 			memset(data, 0, PAGE_CACHE_SIZE);
- 		data += PAGE_CACHE_SIZE;
+ 			memset(data, 0, PAGE_SIZE);
+ 		data += PAGE_SIZE;
  	}
  	return read_buffers[buffer] + offset;
  }
@@@ -502,8 -516,8 +502,13 @@@ static int cramfs_readpage(struct file 
  
  		if (compr_len == 0)
  			; /* hole */
++<<<<<<< HEAD
 +		else if (unlikely(compr_len > (PAGE_CACHE_SIZE << 1))) {
 +			pr_err("cramfs: bad compressed blocksize %u\n",
++=======
+ 		else if (unlikely(compr_len > (PAGE_SIZE << 1))) {
+ 			pr_err("bad compressed blocksize %u\n",
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				compr_len);
  			goto err;
  		} else {
diff --cc fs/direct-io.c
index 9f15d96aa264,472037732daf..000000000000
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@@ -457,8 -423,8 +457,13 @@@ static inline void dio_bio_submit(struc
   */
  static inline void dio_cleanup(struct dio *dio, struct dio_submit *sdio)
  {
++<<<<<<< HEAD
 +	while (dio_pages_present(sdio))
 +		page_cache_release(dio_get_page(dio, sdio));
++=======
+ 	while (sdio->head < sdio->tail)
+ 		put_page(dio->pages[sdio->head++]);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  /*
@@@ -515,13 -484,15 +520,13 @@@ static int dio_bio_complete(struct dio 
  		bio_for_each_segment_all(bvec, bio, i) {
  			struct page *page = bvec->bv_page;
  
 -			if (dio->rw == READ && !PageCompound(page) &&
 -					dio->should_dirty)
 +			if (dio->rw == READ && !PageCompound(page))
  				set_page_dirty_lock(page);
- 			page_cache_release(page);
+ 			put_page(page);
  		}
 -		err = bio->bi_error;
  		bio_put(bio);
  	}
 -	return err;
 +	return uptodate ? 0 : -EIO;
  }
  
  /*
@@@ -1036,13 -1001,13 +1041,13 @@@ do_holes
  				if (sdio->block_in_file >=
  						i_size_aligned >> blkbits) {
  					/* We hit eof */
- 					page_cache_release(page);
+ 					put_page(page);
  					goto out;
  				}
 -				zero_user(page, from, 1 << blkbits);
 +				zero_user(page, block_in_page << blkbits,
 +						1 << blkbits);
  				sdio->block_in_file++;
 -				from += 1 << blkbits;
 -				dio->result += 1 << blkbits;
 +				block_in_page++;
  				goto next_block;
  			}
  
@@@ -1091,8 -1057,7 +1096,12 @@@ next_block
  		}
  
  		/* Drop the ref which was taken in get_user_pages() */
++<<<<<<< HEAD
 +		page_cache_release(page);
 +		block_in_page = 0;
++=======
+ 		put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  out:
  	return ret;
diff --cc fs/ecryptfs/crypto.c
index f71ec125290d,d09cb4cdd09f..000000000000
--- a/fs/ecryptfs/crypto.c
+++ b/fs/ecryptfs/crypto.c
@@@ -411,11 -396,11 +411,16 @@@ out
   *
   * Convert an eCryptfs page index into a lower byte offset
   */
 -static loff_t lower_offset_for_page(struct ecryptfs_crypt_stat *crypt_stat,
 -				    struct page *page)
 +static void ecryptfs_lower_offset_for_extent(loff_t *offset, loff_t extent_num,
 +					     struct ecryptfs_crypt_stat *crypt_stat)
  {
++<<<<<<< HEAD
 +	(*offset) = ecryptfs_lower_header_size(crypt_stat)
 +		    + (crypt_stat->extent_size * extent_num);
++=======
+ 	return ecryptfs_lower_header_size(crypt_stat) +
+ 	       ((loff_t)page->index << PAGE_SHIFT);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  /**
@@@ -431,17 -416,19 +436,21 @@@
   *
   * Return zero on success; non-zero otherwise
   */
 -static int crypt_extent(struct ecryptfs_crypt_stat *crypt_stat,
 -			struct page *dst_page,
 -			struct page *src_page,
 -			unsigned long extent_offset, int op)
 +static int ecryptfs_encrypt_extent(struct page *enc_extent_page,
 +				   struct ecryptfs_crypt_stat *crypt_stat,
 +				   struct page *page,
 +				   unsigned long extent_offset)
  {
 -	pgoff_t page_index = op == ENCRYPT ? src_page->index : dst_page->index;
  	loff_t extent_base;
  	char extent_iv[ECRYPTFS_MAX_IV_BYTES];
 -	struct scatterlist src_sg, dst_sg;
 -	size_t extent_size = crypt_stat->extent_size;
  	int rc;
  
++<<<<<<< HEAD
 +	extent_base = (((loff_t)page->index)
 +		       * (PAGE_CACHE_SIZE / crypt_stat->extent_size));
++=======
+ 	extent_base = (((loff_t)page_index) * (PAGE_SIZE / extent_size));
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	rc = ecryptfs_derive_iv(extent_iv, crypt_stat,
  				(extent_base + extent_offset));
  	if (rc) {
@@@ -502,32 -496,29 +511,47 @@@ int ecryptfs_encrypt_page(struct page *
  				"encrypted extent\n");
  		goto out;
  	}
 -
 +	enc_extent_virt = kmap(enc_extent_page);
  	for (extent_offset = 0;
- 	     extent_offset < (PAGE_CACHE_SIZE / crypt_stat->extent_size);
+ 	     extent_offset < (PAGE_SIZE / crypt_stat->extent_size);
  	     extent_offset++) {
 -		rc = crypt_extent(crypt_stat, enc_extent_page, page,
 -				  extent_offset, ENCRYPT);
 +		loff_t offset;
 +
 +		rc = ecryptfs_encrypt_extent(enc_extent_page, crypt_stat, page,
 +					     extent_offset);
  		if (rc) {
  			printk(KERN_ERR "%s: Error encrypting extent; "
  			       "rc = [%d]\n", __func__, rc);
  			goto out;
  		}
++<<<<<<< HEAD
 +		ecryptfs_lower_offset_for_extent(
 +			&offset, ((((loff_t)page->index)
 +				   * (PAGE_CACHE_SIZE
 +				      / crypt_stat->extent_size))
 +				  + extent_offset), crypt_stat);
 +		rc = ecryptfs_write_lower(ecryptfs_inode, enc_extent_virt,
 +					  offset, crypt_stat->extent_size);
 +		if (rc < 0) {
 +			ecryptfs_printk(KERN_ERR, "Error attempting "
 +					"to write lower page; rc = [%d]"
 +					"\n", rc);
 +			goto out;
 +		}
++=======
+ 	}
+ 
+ 	lower_offset = lower_offset_for_page(crypt_stat, page);
+ 	enc_extent_virt = kmap(enc_extent_page);
+ 	rc = ecryptfs_write_lower(ecryptfs_inode, enc_extent_virt, lower_offset,
+ 				  PAGE_SIZE);
+ 	kunmap(enc_extent_page);
+ 	if (rc < 0) {
+ 		ecryptfs_printk(KERN_ERR,
+ 			"Error attempting to write lower page; rc = [%d]\n",
+ 			rc);
+ 		goto out;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  	rc = 0;
  out:
@@@ -603,34 -557,24 +627,47 @@@ int ecryptfs_decrypt_page(struct page *
  	crypt_stat =
  		&(ecryptfs_inode_to_private(ecryptfs_inode)->crypt_stat);
  	BUG_ON(!(crypt_stat->flags & ECRYPTFS_ENCRYPTED));
++<<<<<<< HEAD
 +	enc_extent_page = alloc_page(GFP_USER);
 +	if (!enc_extent_page) {
 +		rc = -ENOMEM;
 +		ecryptfs_printk(KERN_ERR, "Error allocating memory for "
 +				"encrypted extent\n");
++=======
+ 
+ 	lower_offset = lower_offset_for_page(crypt_stat, page);
+ 	page_virt = kmap(page);
+ 	rc = ecryptfs_read_lower(page_virt, lower_offset, PAGE_SIZE,
+ 				 ecryptfs_inode);
+ 	kunmap(page);
+ 	if (rc < 0) {
+ 		ecryptfs_printk(KERN_ERR,
+ 			"Error attempting to read lower page; rc = [%d]\n",
+ 			rc);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		goto out;
  	}
 -
 +	enc_extent_virt = kmap(enc_extent_page);
  	for (extent_offset = 0;
- 	     extent_offset < (PAGE_CACHE_SIZE / crypt_stat->extent_size);
+ 	     extent_offset < (PAGE_SIZE / crypt_stat->extent_size);
  	     extent_offset++) {
 -		rc = crypt_extent(crypt_stat, page, page,
 -				  extent_offset, DECRYPT);
 +		loff_t offset;
 +
 +		ecryptfs_lower_offset_for_extent(
 +			&offset, ((page->index * (PAGE_CACHE_SIZE
 +						  / crypt_stat->extent_size))
 +				  + extent_offset), crypt_stat);
 +		rc = ecryptfs_read_lower(enc_extent_virt, offset,
 +					 crypt_stat->extent_size,
 +					 ecryptfs_inode);
 +		if (rc < 0) {
 +			ecryptfs_printk(KERN_ERR, "Error attempting "
 +					"to read lower page; rc = [%d]"
 +					"\n", rc);
 +			goto out;
 +		}
 +		rc = ecryptfs_decrypt_extent(page, crypt_stat, enc_extent_page,
 +					     extent_offset);
  		if (rc) {
  			printk(KERN_ERR "%s: Error encrypting extent; "
  			       "rc = [%d]\n", __func__, rc);
diff --cc fs/exofs/dir.c
index 46375896cfc0,547b93cbea63..000000000000
--- a/fs/exofs/dir.c
+++ b/fs/exofs/dir.c
@@@ -41,15 -41,9 +41,15 @@@ static inline unsigned exofs_chunk_size
  static inline void exofs_put_page(struct page *page)
  {
  	kunmap(page);
- 	page_cache_release(page);
+ 	put_page(page);
  }
  
 +/* Accesses dir's inode->i_size must be called under inode lock */
 +static inline unsigned long dir_pages(struct inode *inode)
 +{
 +	return (inode->i_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
 +}
 +
  static unsigned exofs_last_byte(struct inode *inode, unsigned long page_nr)
  {
  	loff_t last_byte = inode->i_size;
@@@ -239,16 -233,15 +239,23 @@@ void exofs_set_de_type(struct exofs_dir
  }
  
  static int
 -exofs_readdir(struct file *file, struct dir_context *ctx)
 +exofs_readdir(struct file *filp, void *dirent, filldir_t filldir)
  {
++<<<<<<< HEAD
 +	loff_t pos = filp->f_pos;
 +	struct inode *inode = file_inode(filp);
 +	unsigned int offset = pos & ~PAGE_CACHE_MASK;
 +	unsigned long n = pos >> PAGE_CACHE_SHIFT;
++=======
+ 	loff_t pos = ctx->pos;
+ 	struct inode *inode = file_inode(file);
+ 	unsigned int offset = pos & ~PAGE_MASK;
+ 	unsigned long n = pos >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	unsigned long npages = dir_pages(inode);
  	unsigned chunk_mask = ~(exofs_chunk_size(inode)-1);
 -	int need_revalidate = (file->f_version != inode->i_version);
 +	unsigned char *types = NULL;
 +	int need_revalidate = (filp->f_version != inode->i_version);
  
  	if (pos > inode->i_size - EXOFS_DIR_REC_LEN(1))
  		return 0;
@@@ -263,7 -254,7 +270,11 @@@
  		if (IS_ERR(page)) {
  			EXOFS_ERR("ERROR: bad page in directory(0x%lx)\n",
  				  inode->i_ino);
++<<<<<<< HEAD
 +			filp->f_pos += PAGE_CACHE_SIZE - offset;
++=======
+ 			ctx->pos += PAGE_SIZE - offset;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			return PTR_ERR(page);
  		}
  		kaddr = page_address(page);
@@@ -271,9 -262,9 +282,13 @@@
  			if (offset) {
  				offset = exofs_validate_entry(kaddr, offset,
  								chunk_mask);
++<<<<<<< HEAD
 +				filp->f_pos = (n<<PAGE_CACHE_SHIFT) + offset;
++=======
+ 				ctx->pos = (n<<PAGE_SHIFT) + offset;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			}
 -			file->f_version = inode->i_version;
 +			filp->f_version = inode->i_version;
  			need_revalidate = 0;
  		}
  		de = (struct exofs_dir_entry *)(kaddr + offset);
diff --cc fs/exofs/inode.c
index 134de36697d1,49e1bd00b4ec..000000000000
--- a/fs/exofs/inode.c
+++ b/fs/exofs/inode.c
@@@ -611,11 -608,11 +611,16 @@@ static void __r4w_put_page(void *priv, 
  	struct page_collect *pcol = priv;
  
  	if ((pcol->that_locked_page != page) && (ZERO_PAGE(0) != page)) {
++<<<<<<< HEAD
 +		EXOFS_DBGMSG("index=0x%lx\n", page->index);
 +		page_cache_release(page);
++=======
+ 		EXOFS_DBGMSG2("index=0x%lx\n", page->index);
+ 		put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		return;
  	}
 -	EXOFS_DBGMSG2("that_locked_page index=0x%lx\n",
 +	EXOFS_DBGMSG("that_locked_page index=0x%lx\n",
  		     ZERO_PAGE(0) == page ? -1 : page->index);
  }
  
diff --cc fs/ext2/dir.c
index 4237722bfd27,1b45694de316..000000000000
--- a/fs/ext2/dir.c
+++ b/fs/ext2/dir.c
@@@ -67,14 -67,9 +67,14 @@@ static inline unsigned ext2_chunk_size(
  static inline void ext2_put_page(struct page *page)
  {
  	kunmap(page);
- 	page_cache_release(page);
+ 	put_page(page);
  }
  
 +static inline unsigned long dir_pages(struct inode *inode)
 +{
 +	return (inode->i_size+PAGE_CACHE_SIZE-1)>>PAGE_CACHE_SHIFT;
 +}
 +
  /*
   * Return the offset into page `page_nr' of the last valid
   * byte in that page, plus one.
@@@ -287,13 -282,13 +287,13 @@@ static inline void ext2_set_de_type(ext
  }
  
  static int
 -ext2_readdir(struct file *file, struct dir_context *ctx)
 +ext2_readdir (struct file * filp, void * dirent, filldir_t filldir)
  {
 -	loff_t pos = ctx->pos;
 -	struct inode *inode = file_inode(file);
 +	loff_t pos = filp->f_pos;
 +	struct inode *inode = file_inode(filp);
  	struct super_block *sb = inode->i_sb;
- 	unsigned int offset = pos & ~PAGE_CACHE_MASK;
- 	unsigned long n = pos >> PAGE_CACHE_SHIFT;
+ 	unsigned int offset = pos & ~PAGE_MASK;
+ 	unsigned long n = pos >> PAGE_SHIFT;
  	unsigned long npages = dir_pages(inode);
  	unsigned chunk_mask = ~(ext2_chunk_size(inode)-1);
  	unsigned char *types = NULL;
@@@ -314,16 -309,16 +314,24 @@@
  			ext2_error(sb, __func__,
  				   "bad page in #%lu",
  				   inode->i_ino);
++<<<<<<< HEAD
 +			filp->f_pos += PAGE_CACHE_SIZE - offset;
++=======
+ 			ctx->pos += PAGE_SIZE - offset;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			return PTR_ERR(page);
  		}
  		kaddr = page_address(page);
  		if (unlikely(need_revalidate)) {
  			if (offset) {
  				offset = ext2_validate_entry(kaddr, offset, chunk_mask);
++<<<<<<< HEAD
 +				filp->f_pos = (n<<PAGE_CACHE_SHIFT) + offset;
++=======
+ 				ctx->pos = (n<<PAGE_SHIFT) + offset;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			}
 -			file->f_version = inode->i_version;
 +			filp->f_version = inode->i_version;
  			need_revalidate = 0;
  		}
  		de = (ext2_dirent *)(kaddr+offset);
diff --cc fs/ext4/dir.c
index fcd4d194ac0a,561d7308b393..000000000000
--- a/fs/ext4/dir.c
+++ b/fs/ext4/dir.c
@@@ -150,26 -155,27 +150,41 @@@ static int ext4_readdir(struct file *fi
  		err = ext4_map_blocks(NULL, inode, &map, 0);
  		if (err > 0) {
  			pgoff_t index = map.m_pblk >>
++<<<<<<< HEAD
 +					(PAGE_CACHE_SHIFT - inode->i_blkbits);
 +			if (!ra_has_index(&filp->f_ra, index))
++=======
+ 					(PAGE_SHIFT - inode->i_blkbits);
+ 			if (!ra_has_index(&file->f_ra, index))
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				page_cache_sync_readahead(
  					sb->s_bdev->bd_inode->i_mapping,
 -					&file->f_ra, file,
 +					&filp->f_ra, filp,
  					index, 1);
++<<<<<<< HEAD
 +			filp->f_ra.prev_pos = (loff_t)index << PAGE_CACHE_SHIFT;
 +			bh = ext4_bread(NULL, inode, map.m_lblk, 0, &err);
++=======
+ 			file->f_ra.prev_pos = (loff_t)index << PAGE_SHIFT;
+ 			bh = ext4_bread(NULL, inode, map.m_lblk, 0);
+ 			if (IS_ERR(bh)) {
+ 				err = PTR_ERR(bh);
+ 				bh = NULL;
+ 				goto errout;
+ 			}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
  
 +		/*
 +		 * We ignore I/O errors on directories so users have a chance
 +		 * of recovering data when there's a bad sector
 +		 */
  		if (!bh) {
  			if (!dir_has_error) {
 -				EXT4_ERROR_FILE(file, 0,
 +				EXT4_ERROR_FILE(filp, 0,
  						"directory contains a "
  						"hole at offset %llu",
 -					   (unsigned long long) ctx->pos);
 +					   (unsigned long long) filp->f_pos);
  				dir_has_error = 1;
  			}
  			/* corrupt size?  Maybe no more blocks to read */
diff --cc fs/ext4/inode.c
index 46409f183977,8a43c683eef9..000000000000
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@@ -915,6 -1053,94 +915,97 @@@ int do_journal_get_write_access(handle_
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_EXT4_FS_ENCRYPTION
+ static int ext4_block_write_begin(struct page *page, loff_t pos, unsigned len,
+ 				  get_block_t *get_block)
+ {
+ 	unsigned from = pos & (PAGE_SIZE - 1);
+ 	unsigned to = from + len;
+ 	struct inode *inode = page->mapping->host;
+ 	unsigned block_start, block_end;
+ 	sector_t block;
+ 	int err = 0;
+ 	unsigned blocksize = inode->i_sb->s_blocksize;
+ 	unsigned bbits;
+ 	struct buffer_head *bh, *head, *wait[2], **wait_bh = wait;
+ 	bool decrypt = false;
+ 
+ 	BUG_ON(!PageLocked(page));
+ 	BUG_ON(from > PAGE_SIZE);
+ 	BUG_ON(to > PAGE_SIZE);
+ 	BUG_ON(from > to);
+ 
+ 	if (!page_has_buffers(page))
+ 		create_empty_buffers(page, blocksize, 0);
+ 	head = page_buffers(page);
+ 	bbits = ilog2(blocksize);
+ 	block = (sector_t)page->index << (PAGE_SHIFT - bbits);
+ 
+ 	for (bh = head, block_start = 0; bh != head || !block_start;
+ 	    block++, block_start = block_end, bh = bh->b_this_page) {
+ 		block_end = block_start + blocksize;
+ 		if (block_end <= from || block_start >= to) {
+ 			if (PageUptodate(page)) {
+ 				if (!buffer_uptodate(bh))
+ 					set_buffer_uptodate(bh);
+ 			}
+ 			continue;
+ 		}
+ 		if (buffer_new(bh))
+ 			clear_buffer_new(bh);
+ 		if (!buffer_mapped(bh)) {
+ 			WARN_ON(bh->b_size != blocksize);
+ 			err = get_block(inode, block, bh, 1);
+ 			if (err)
+ 				break;
+ 			if (buffer_new(bh)) {
+ 				unmap_underlying_metadata(bh->b_bdev,
+ 							  bh->b_blocknr);
+ 				if (PageUptodate(page)) {
+ 					clear_buffer_new(bh);
+ 					set_buffer_uptodate(bh);
+ 					mark_buffer_dirty(bh);
+ 					continue;
+ 				}
+ 				if (block_end > to || block_start < from)
+ 					zero_user_segments(page, to, block_end,
+ 							   block_start, from);
+ 				continue;
+ 			}
+ 		}
+ 		if (PageUptodate(page)) {
+ 			if (!buffer_uptodate(bh))
+ 				set_buffer_uptodate(bh);
+ 			continue;
+ 		}
+ 		if (!buffer_uptodate(bh) && !buffer_delay(bh) &&
+ 		    !buffer_unwritten(bh) &&
+ 		    (block_start < from || block_end > to)) {
+ 			ll_rw_block(READ, 1, &bh);
+ 			*wait_bh++ = bh;
+ 			decrypt = ext4_encrypted_inode(inode) &&
+ 				S_ISREG(inode->i_mode);
+ 		}
+ 	}
+ 	/*
+ 	 * If we issued read requests, let them complete.
+ 	 */
+ 	while (wait_bh > wait) {
+ 		wait_on_buffer(*--wait_bh);
+ 		if (!buffer_uptodate(*wait_bh))
+ 			err = -EIO;
+ 	}
+ 	if (unlikely(err))
+ 		page_zero_new_buffers(page, from, to);
+ 	else if (decrypt)
+ 		err = ext4_decrypt(page);
+ 	return err;
+ }
+ #endif
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  static int ext4_write_begin(struct file *file, struct address_space *mapping,
  			    loff_t pos, unsigned len, unsigned flags,
  			    struct page **pagep, void **fsdata)
@@@ -1079,8 -1315,10 +1170,8 @@@ static int ext4_write_end(struct file *
  	 */
  	i_size_changed = ext4_update_inode_size(inode, pos + copied);
  	unlock_page(page);
- 	page_cache_release(page);
+ 	put_page(page);
  
 -	if (old_size < pos)
 -		pagecache_isize_extended(inode, old_size, pos);
  	/*
  	 * Don't mark the inode dirty under page lock. First, it unnecessarily
  	 * makes the holding time of page lock longer. Second, it forces lock
@@@ -1152,8 -1423,11 +1243,8 @@@ static int ext4_journalled_write_end(st
  	ext4_set_inode_state(inode, EXT4_STATE_JDATA);
  	EXT4_I(inode)->i_datasync_tid = handle->h_transaction->t_tid;
  	unlock_page(page);
- 	page_cache_release(page);
+ 	put_page(page);
  
 -	if (old_size < pos)
 -		pagecache_isize_extended(inode, old_size, pos);
 -
  	if (size_changed) {
  		ret2 = ext4_mark_inode_dirty(handle, inode);
  		if (!ret)
@@@ -1374,8 -1636,7 +1465,12 @@@ static void mpage_release_unused_pages(
  			BUG_ON(!PageLocked(page));
  			BUG_ON(PageWriteback(page));
  			if (invalidate) {
++<<<<<<< HEAD
 +				block_invalidatepage_range(page, 0,
 +							   PAGE_CACHE_SIZE);
++=======
+ 				block_invalidatepage(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				ClearPageUptodate(page);
  			}
  			unlock_page(page);
@@@ -1763,7 -2033,8 +1858,12 @@@ static int ext4_writepage(struct page *
  	if (ext4_walk_page_buffers(NULL, page_bufs, 0, len, NULL,
  				   ext4_bh_delay_or_unwritten)) {
  		redirty_page_for_writepage(wbc, page);
++<<<<<<< HEAD
 +		if (current->flags & PF_MEMALLOC) {
++=======
+ 		if ((current->flags & PF_MEMALLOC) ||
+ 		    (inode->i_sb->s_blocksize == PAGE_SIZE)) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			/*
  			 * For memory cleaning there's no point in writing only
  			 * some buffers. So just bail out. Warn if we came here
@@@ -3374,8 -3565,8 +3474,13 @@@ static int __ext4_block_zero_page_range
  	struct page *page;
  	int err = 0;
  
++<<<<<<< HEAD
 +	page = find_or_create_page(mapping, from >> PAGE_CACHE_SHIFT,
 +				   mapping_gfp_mask(mapping) & ~__GFP_FS);
++=======
+ 	page = find_or_create_page(mapping, from >> PAGE_SHIFT,
+ 				   mapping_gfp_constraint(mapping, ~__GFP_FS));
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (!page)
  		return -ENOMEM;
  
@@@ -3419,6 -3610,13 +3524,16 @@@
  		/* Uhhuh. Read error. Complain and punt. */
  		if (!buffer_uptodate(bh))
  			goto unlock;
++<<<<<<< HEAD
++=======
+ 		if (S_ISREG(inode->i_mode) &&
+ 		    ext4_encrypted_inode(inode)) {
+ 			/* We expect the key to be set. */
+ 			BUG_ON(!ext4_has_encryption_key(inode));
+ 			BUG_ON(blocksize != PAGE_SIZE);
+ 			WARN_ON_ONCE(ext4_decrypt(page));
+ 		}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  	if (ext4_should_journal_data(inode)) {
  		BUFFER_TRACE(bh, "get write access");
@@@ -5282,10 -5577,10 +5397,10 @@@ retry_alloc
  		ret = VM_FAULT_SIGBUS;
  		goto out;
  	}
 -	ret = block_page_mkwrite(vma, vmf, get_block);
 +	ret = __block_page_mkwrite(vma, vmf, get_block);
  	if (!ret && ext4_should_journal_data(inode)) {
  		if (ext4_walk_page_buffers(handle, page_buffers(page), 0,
- 			  PAGE_CACHE_SIZE, NULL, do_journal_get_write_access)) {
+ 			  PAGE_SIZE, NULL, do_journal_get_write_access)) {
  			unlock_page(page);
  			ret = VM_FAULT_SIGBUS;
  			ext4_journal_stop(handle);
diff --cc fs/ext4/mballoc.c
index 343f1ad2e4b3,c12174711ce2..000000000000
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@@ -1156,8 -1167,8 +1156,13 @@@ ext4_mb_load_buddy(struct super_block *
  			 * is yet to initialize the same. So
  			 * wait for it to initialize.
  			 */
++<<<<<<< HEAD
 +			page_cache_release(page);
 +		page = find_or_create_page(inode->i_mapping, pnum, GFP_NOFS);
++=======
+ 			put_page(page);
+ 		page = find_or_create_page(inode->i_mapping, pnum, gfp);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		if (page) {
  			BUG_ON(page->mapping != inode->i_mapping);
  			if (!PageUptodate(page)) {
@@@ -1188,11 -1200,11 +1193,16 @@@
  	pnum = block / blocks_per_page;
  	poff = block % blocks_per_page;
  
 -	page = find_get_page_flags(inode->i_mapping, pnum, FGP_ACCESSED);
 +	page = find_get_page(inode->i_mapping, pnum);
  	if (page == NULL || !PageUptodate(page)) {
  		if (page)
++<<<<<<< HEAD
 +			page_cache_release(page);
 +		page = find_or_create_page(inode->i_mapping, pnum, GFP_NOFS);
++=======
+ 			put_page(page);
+ 		page = find_or_create_page(inode->i_mapping, pnum, gfp);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		if (page) {
  			BUG_ON(page->mapping != inode->i_mapping);
  			if (!PageUptodate(page)) {
diff --cc fs/ext4/move_extent.c
index b9d64b8bbe48,675b67e5d5c2..000000000000
--- a/fs/ext4/move_extent.c
+++ b/fs/ext4/move_extent.c
@@@ -801,10 -153,10 +801,10 @@@ mext_page_double_lock(struct inode *ino
  	if (!page[0])
  		return -ENOMEM;
  
 -	page[1] = grab_cache_page_write_begin(mapping[1], index2, fl);
 +	page[1] = grab_cache_page_write_begin(mapping[1], index, fl);
  	if (!page[1]) {
  		unlock_page(page[0]);
- 		page_cache_release(page[0]);
+ 		put_page(page[0]);
  		return -ENOMEM;
  	}
  	/*
@@@ -1268,17 -553,11 +1268,25 @@@ ext4_move_extents(struct file *o_filp, 
  {
  	struct inode *orig_inode = file_inode(o_filp);
  	struct inode *donor_inode = file_inode(d_filp);
++<<<<<<< HEAD
 +	struct ext4_ext_path *orig_path = NULL, *holecheck_path = NULL;
 +	struct ext4_extent *ext_prev, *ext_cur, *ext_dummy;
 +	ext4_lblk_t block_start = orig_start;
 +	ext4_lblk_t block_end, seq_start, add_blocks, file_end, seq_blocks = 0;
 +	ext4_lblk_t rest_blocks;
 +	pgoff_t orig_page_offset = 0, seq_end_page;
 +	int ret, depth, last_extent = 0;
 +	int blocks_per_page = PAGE_CACHE_SIZE >> orig_inode->i_blkbits;
 +	int data_offset_in_page;
 +	int block_len_in_page;
 +	int unwritten;
++=======
+ 	struct ext4_ext_path *path = NULL;
+ 	int blocks_per_page = PAGE_SIZE >> orig_inode->i_blkbits;
+ 	ext4_lblk_t o_end, o_start = orig_blk;
+ 	ext4_lblk_t d_start = donor_blk;
+ 	int ret;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	if (orig_inode->i_sb != donor_inode->i_sb) {
  		ext4_debug("ext4 move extent: The argument files "
@@@ -1320,121 -603,58 +1328,131 @@@
  	/* Protect extent tree against block allocations via delalloc */
  	ext4_double_down_write_data_sem(orig_inode, donor_inode);
  	/* Check the filesystem environment whether move_extent can be done */
 -	ret = mext_check_arguments(orig_inode, donor_inode, orig_blk,
 -				    donor_blk, &len);
 +	ret = mext_check_arguments(orig_inode, donor_inode, orig_start,
 +				    donor_start, &len);
  	if (ret)
  		goto out;
 -	o_end = o_start + len;
  
 -	while (o_start < o_end) {
 -		struct ext4_extent *ex;
 -		ext4_lblk_t cur_blk, next_blk;
 -		pgoff_t orig_page_index, donor_page_index;
 -		int offset_in_page;
 -		int unwritten, cur_len;
 +	file_end = (i_size_read(orig_inode) - 1) >> orig_inode->i_blkbits;
 +	block_end = block_start + len - 1;
 +	if (file_end < block_end)
 +		len -= block_end - file_end;
  
 -		ret = get_ext_path(orig_inode, o_start, &path);
 -		if (ret)
 +	ret = get_ext_path(orig_inode, block_start, &orig_path);
 +	if (ret)
 +		goto out;
 +
 +	/* Get path structure to check the hole */
 +	ret = get_ext_path(orig_inode, block_start, &holecheck_path);
 +	if (ret)
 +		goto out;
 +
 +	depth = ext_depth(orig_inode);
 +	ext_cur = holecheck_path[depth].p_ext;
 +
 +	/*
 +	 * Get proper starting location of block replacement if block_start was
 +	 * within the hole.
 +	 */
 +	if (le32_to_cpu(ext_cur->ee_block) +
 +		ext4_ext_get_actual_len(ext_cur) - 1 < block_start) {
 +		/*
 +		 * The hole exists between extents or the tail of
 +		 * original file.
 +		 */
 +		last_extent = mext_next_extent(orig_inode,
 +					holecheck_path, &ext_cur);
 +		if (last_extent < 0) {
 +			ret = last_extent;
  			goto out;
 -		ex = path[path->p_depth].p_ext;
 -		next_blk = ext4_ext_next_allocated_block(path);
 -		cur_blk = le32_to_cpu(ex->ee_block);
 -		cur_len = ext4_ext_get_actual_len(ex);
 -		/* Check hole before the start pos */
 -		if (cur_blk + cur_len - 1 < o_start) {
 -			if (next_blk == EXT_MAX_BLOCKS) {
 -				o_start = o_end;
 -				ret = -ENODATA;
 -				goto out;
 -			}
 -			d_start += next_blk - o_start;
 -			o_start = next_blk;
 +		}
 +		last_extent = mext_next_extent(orig_inode, orig_path,
 +							&ext_dummy);
 +		if (last_extent < 0) {
 +			ret = last_extent;
 +			goto out;
 +		}
 +		seq_start = le32_to_cpu(ext_cur->ee_block);
 +	} else if (le32_to_cpu(ext_cur->ee_block) > block_start)
 +		/* The hole exists at the beginning of original file. */
 +		seq_start = le32_to_cpu(ext_cur->ee_block);
 +	else
 +		seq_start = block_start;
 +
 +	/* No blocks within the specified range. */
 +	if (le32_to_cpu(ext_cur->ee_block) > block_end) {
 +		ext4_debug("ext4 move extent: The specified range of file "
 +							"may be the hole\n");
 +		ret = -EINVAL;
 +		goto out;
 +	}
 +
 +	/* Adjust start blocks */
 +	add_blocks = min(le32_to_cpu(ext_cur->ee_block) +
 +			 ext4_ext_get_actual_len(ext_cur), block_end + 1) -
 +		     max(le32_to_cpu(ext_cur->ee_block), block_start);
 +
 +	while (!last_extent && le32_to_cpu(ext_cur->ee_block) <= block_end) {
 +		seq_blocks += add_blocks;
 +
 +		/* Adjust tail blocks */
 +		if (seq_start + seq_blocks - 1 > block_end)
 +			seq_blocks = block_end - seq_start + 1;
 +
 +		ext_prev = ext_cur;
 +		last_extent = mext_next_extent(orig_inode, holecheck_path,
 +						&ext_cur);
 +		if (last_extent < 0) {
 +			ret = last_extent;
 +			break;
 +		}
 +		add_blocks = ext4_ext_get_actual_len(ext_cur);
 +
 +		/*
 +		 * Extend the length of contiguous block (seq_blocks)
 +		 * if extents are contiguous.
 +		 */
 +		if (ext4_can_extents_be_merged(orig_inode,
 +					       ext_prev, ext_cur) &&
 +		    block_end >= le32_to_cpu(ext_cur->ee_block) &&
 +		    !last_extent)
  			continue;
 -		/* Check hole after the start pos */
 -		} else if (cur_blk > o_start) {
 -			/* Skip hole */
 -			d_start += cur_blk - o_start;
 -			o_start = cur_blk;
 -			/* Extent inside requested range ?*/
 -			if (cur_blk >= o_end)
 -				goto out;
 -		} else { /* in_range(o_start, o_blk, o_len) */
 -			cur_len += cur_blk - o_start;
 +
 +		/* Is original extent is unwritten */
 +		unwritten = ext4_ext_is_unwritten(ext_prev);
 +
 +		data_offset_in_page = seq_start % blocks_per_page;
 +
 +		/*
 +		 * Calculate data blocks count that should be swapped
 +		 * at the first page.
 +		 */
 +		if (data_offset_in_page + seq_blocks > blocks_per_page) {
 +			/* Swapped blocks are across pages */
 +			block_len_in_page =
 +					blocks_per_page - data_offset_in_page;
 +		} else {
 +			/* Swapped blocks are in a page */
 +			block_len_in_page = seq_blocks;
  		}
 -		unwritten = ext4_ext_is_unwritten(ex);
 -		if (o_end - o_start < cur_len)
 -			cur_len = o_end - o_start;
  
 +		orig_page_offset = seq_start >>
 +				(PAGE_CACHE_SHIFT - orig_inode->i_blkbits);
 +		seq_end_page = (seq_start + seq_blocks - 1) >>
 +				(PAGE_CACHE_SHIFT - orig_inode->i_blkbits);
 +		seq_start = le32_to_cpu(ext_cur->ee_block);
 +		rest_blocks = seq_blocks;
 +
++<<<<<<< HEAD
++=======
+ 		orig_page_index = o_start >> (PAGE_SHIFT -
+ 					       orig_inode->i_blkbits);
+ 		donor_page_index = d_start >> (PAGE_SHIFT -
+ 					       donor_inode->i_blkbits);
+ 		offset_in_page = o_start % blocks_per_page;
+ 		if (cur_len > blocks_per_page- offset_in_page)
+ 			cur_len = blocks_per_page - offset_in_page;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		/*
  		 * Up semaphore to avoid following problems:
  		 * a. transaction deadlock among ext4_journal_start,
diff --cc fs/ext4/super.c
index 8a6f058f1768,0bb74aacb8c0..000000000000
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@@ -4148,6 -3801,25 +4148,28 @@@ static int ext4_fill_super(struct super
  	sbi->s_journal->j_commit_callback = ext4_journal_commit_callback;
  
  no_journal:
++<<<<<<< HEAD
++=======
+ 	sbi->s_mb_cache = ext4_xattr_create_cache();
+ 	if (!sbi->s_mb_cache) {
+ 		ext4_msg(sb, KERN_ERR, "Failed to create an mb_cache");
+ 		goto failed_mount_wq;
+ 	}
+ 
+ 	if ((DUMMY_ENCRYPTION_ENABLED(sbi) || ext4_has_feature_encrypt(sb)) &&
+ 	    (blocksize != PAGE_SIZE)) {
+ 		ext4_msg(sb, KERN_ERR,
+ 			 "Unsupported blocksize for fs encryption");
+ 		goto failed_mount_wq;
+ 	}
+ 
+ 	if (DUMMY_ENCRYPTION_ENABLED(sbi) && !(sb->s_flags & MS_RDONLY) &&
+ 	    !ext4_has_feature_encrypt(sb)) {
+ 		ext4_set_feature_encrypt(sb);
+ 		ext4_commit_super(sb, 1);
+ 	}
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	/*
  	 * Get the # of file system overhead blocks from the
  	 * superblock if present.
diff --cc fs/ext4/symlink.c
index ff3711932018,75ed5c2f0c16..000000000000
--- a/fs/ext4/symlink.c
+++ b/fs/ext4/symlink.c
@@@ -23,17 -22,88 +23,81 @@@
  #include "ext4.h"
  #include "xattr.h"
  
 -#ifdef CONFIG_EXT4_FS_ENCRYPTION
 -static const char *ext4_encrypted_get_link(struct dentry *dentry,
 -					   struct inode *inode,
 -					   struct delayed_call *done)
 +static void *ext4_follow_link(struct dentry *dentry, struct nameidata *nd)
  {
++<<<<<<< HEAD
 +	struct ext4_inode_info *ei = EXT4_I(dentry->d_inode);
 +	nd_set_link(nd, (char *) ei->i_data);
 +	return NULL;
++=======
+ 	struct page *cpage = NULL;
+ 	char *caddr, *paddr = NULL;
+ 	struct ext4_str cstr, pstr;
+ 	struct ext4_encrypted_symlink_data *sd;
+ 	loff_t size = min_t(loff_t, i_size_read(inode), PAGE_SIZE - 1);
+ 	int res;
+ 	u32 plen, max_size = inode->i_sb->s_blocksize;
+ 
+ 	if (!dentry)
+ 		return ERR_PTR(-ECHILD);
+ 
+ 	res = ext4_get_encryption_info(inode);
+ 	if (res)
+ 		return ERR_PTR(res);
+ 
+ 	if (ext4_inode_is_fast_symlink(inode)) {
+ 		caddr = (char *) EXT4_I(inode)->i_data;
+ 		max_size = sizeof(EXT4_I(inode)->i_data);
+ 	} else {
+ 		cpage = read_mapping_page(inode->i_mapping, 0, NULL);
+ 		if (IS_ERR(cpage))
+ 			return ERR_CAST(cpage);
+ 		caddr = page_address(cpage);
+ 		caddr[size] = 0;
+ 	}
+ 
+ 	/* Symlink is encrypted */
+ 	sd = (struct ext4_encrypted_symlink_data *)caddr;
+ 	cstr.name = sd->encrypted_path;
+ 	cstr.len  = le16_to_cpu(sd->len);
+ 	if ((cstr.len +
+ 	     sizeof(struct ext4_encrypted_symlink_data) - 1) >
+ 	    max_size) {
+ 		/* Symlink data on the disk is corrupted */
+ 		res = -EFSCORRUPTED;
+ 		goto errout;
+ 	}
+ 	plen = (cstr.len < EXT4_FNAME_CRYPTO_DIGEST_SIZE*2) ?
+ 		EXT4_FNAME_CRYPTO_DIGEST_SIZE*2 : cstr.len;
+ 	paddr = kmalloc(plen + 1, GFP_NOFS);
+ 	if (!paddr) {
+ 		res = -ENOMEM;
+ 		goto errout;
+ 	}
+ 	pstr.name = paddr;
+ 	pstr.len = plen;
+ 	res = _ext4_fname_disk_to_usr(inode, NULL, &cstr, &pstr);
+ 	if (res < 0)
+ 		goto errout;
+ 	/* Null-terminate the name */
+ 	if (res <= plen)
+ 		paddr[res] = '\0';
+ 	if (cpage)
+ 		put_page(cpage);
+ 	set_delayed_call(done, kfree_link, paddr);
+ 	return paddr;
+ errout:
+ 	if (cpage)
+ 		put_page(cpage);
+ 	kfree(paddr);
+ 	return ERR_PTR(res);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
 -const struct inode_operations ext4_encrypted_symlink_inode_operations = {
 -	.readlink	= generic_readlink,
 -	.get_link	= ext4_encrypted_get_link,
 -	.setattr	= ext4_setattr,
 -	.setxattr	= generic_setxattr,
 -	.getxattr	= generic_getxattr,
 -	.listxattr	= ext4_listxattr,
 -	.removexattr	= generic_removexattr,
 -};
 -#endif
 -
  const struct inode_operations ext4_symlink_inode_operations = {
  	.readlink	= generic_readlink,
 -	.get_link	= page_get_link,
 +	.follow_link	= page_follow_link_light,
 +	.put_link	= page_put_link,
  	.setattr	= ext4_setattr,
  	.setxattr	= generic_setxattr,
  	.getxattr	= generic_getxattr,
diff --cc fs/f2fs/data.c
index f412cf901c1f,53fec0872e60..000000000000
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@@ -22,8 -24,260 +22,262 @@@
  #include "f2fs.h"
  #include "node.h"
  #include "segment.h"
 -#include "trace.h"
  #include <trace/events/f2fs.h>
  
++<<<<<<< HEAD
++=======
+ static void f2fs_read_end_io(struct bio *bio)
+ {
+ 	struct bio_vec *bvec;
+ 	int i;
+ 
+ 	if (f2fs_bio_encrypted(bio)) {
+ 		if (bio->bi_error) {
+ 			fscrypt_release_ctx(bio->bi_private);
+ 		} else {
+ 			fscrypt_decrypt_bio_pages(bio->bi_private, bio);
+ 			return;
+ 		}
+ 	}
+ 
+ 	bio_for_each_segment_all(bvec, bio, i) {
+ 		struct page *page = bvec->bv_page;
+ 
+ 		if (!bio->bi_error) {
+ 			SetPageUptodate(page);
+ 		} else {
+ 			ClearPageUptodate(page);
+ 			SetPageError(page);
+ 		}
+ 		unlock_page(page);
+ 	}
+ 	bio_put(bio);
+ }
+ 
+ static void f2fs_write_end_io(struct bio *bio)
+ {
+ 	struct f2fs_sb_info *sbi = bio->bi_private;
+ 	struct bio_vec *bvec;
+ 	int i;
+ 
+ 	bio_for_each_segment_all(bvec, bio, i) {
+ 		struct page *page = bvec->bv_page;
+ 
+ 		fscrypt_pullback_bio_page(&page, true);
+ 
+ 		if (unlikely(bio->bi_error)) {
+ 			set_bit(AS_EIO, &page->mapping->flags);
+ 			f2fs_stop_checkpoint(sbi);
+ 		}
+ 		end_page_writeback(page);
+ 		dec_page_count(sbi, F2FS_WRITEBACK);
+ 	}
+ 
+ 	if (!get_pages(sbi, F2FS_WRITEBACK) && wq_has_sleeper(&sbi->cp_wait))
+ 		wake_up(&sbi->cp_wait);
+ 
+ 	bio_put(bio);
+ }
+ 
+ /*
+  * Low-level block read/write IO operations.
+  */
+ static struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,
+ 				int npages, bool is_read)
+ {
+ 	struct bio *bio;
+ 
+ 	bio = f2fs_bio_alloc(npages);
+ 
+ 	bio->bi_bdev = sbi->sb->s_bdev;
+ 	bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);
+ 	bio->bi_end_io = is_read ? f2fs_read_end_io : f2fs_write_end_io;
+ 	bio->bi_private = is_read ? NULL : sbi;
+ 
+ 	return bio;
+ }
+ 
+ static void __submit_merged_bio(struct f2fs_bio_info *io)
+ {
+ 	struct f2fs_io_info *fio = &io->fio;
+ 
+ 	if (!io->bio)
+ 		return;
+ 
+ 	if (is_read_io(fio->rw))
+ 		trace_f2fs_submit_read_bio(io->sbi->sb, fio, io->bio);
+ 	else
+ 		trace_f2fs_submit_write_bio(io->sbi->sb, fio, io->bio);
+ 
+ 	submit_bio(fio->rw, io->bio);
+ 	io->bio = NULL;
+ }
+ 
+ static bool __has_merged_page(struct f2fs_bio_info *io, struct inode *inode,
+ 						struct page *page, nid_t ino)
+ {
+ 	struct bio_vec *bvec;
+ 	struct page *target;
+ 	int i;
+ 
+ 	if (!io->bio)
+ 		return false;
+ 
+ 	if (!inode && !page && !ino)
+ 		return true;
+ 
+ 	bio_for_each_segment_all(bvec, io->bio, i) {
+ 
+ 		if (bvec->bv_page->mapping)
+ 			target = bvec->bv_page;
+ 		else
+ 			target = fscrypt_control_page(bvec->bv_page);
+ 
+ 		if (inode && inode == target->mapping->host)
+ 			return true;
+ 		if (page && page == target)
+ 			return true;
+ 		if (ino && ino == ino_of_node(target))
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static bool has_merged_page(struct f2fs_sb_info *sbi, struct inode *inode,
+ 						struct page *page, nid_t ino,
+ 						enum page_type type)
+ {
+ 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+ 	struct f2fs_bio_info *io = &sbi->write_io[btype];
+ 	bool ret;
+ 
+ 	down_read(&io->io_rwsem);
+ 	ret = __has_merged_page(io, inode, page, ino);
+ 	up_read(&io->io_rwsem);
+ 	return ret;
+ }
+ 
+ static void __f2fs_submit_merged_bio(struct f2fs_sb_info *sbi,
+ 				struct inode *inode, struct page *page,
+ 				nid_t ino, enum page_type type, int rw)
+ {
+ 	enum page_type btype = PAGE_TYPE_OF_BIO(type);
+ 	struct f2fs_bio_info *io;
+ 
+ 	io = is_read_io(rw) ? &sbi->read_io : &sbi->write_io[btype];
+ 
+ 	down_write(&io->io_rwsem);
+ 
+ 	if (!__has_merged_page(io, inode, page, ino))
+ 		goto out;
+ 
+ 	/* change META to META_FLUSH in the checkpoint procedure */
+ 	if (type >= META_FLUSH) {
+ 		io->fio.type = META_FLUSH;
+ 		if (test_opt(sbi, NOBARRIER))
+ 			io->fio.rw = WRITE_FLUSH | REQ_META | REQ_PRIO;
+ 		else
+ 			io->fio.rw = WRITE_FLUSH_FUA | REQ_META | REQ_PRIO;
+ 	}
+ 	__submit_merged_bio(io);
+ out:
+ 	up_write(&io->io_rwsem);
+ }
+ 
+ void f2fs_submit_merged_bio(struct f2fs_sb_info *sbi, enum page_type type,
+ 									int rw)
+ {
+ 	__f2fs_submit_merged_bio(sbi, NULL, NULL, 0, type, rw);
+ }
+ 
+ void f2fs_submit_merged_bio_cond(struct f2fs_sb_info *sbi,
+ 				struct inode *inode, struct page *page,
+ 				nid_t ino, enum page_type type, int rw)
+ {
+ 	if (has_merged_page(sbi, inode, page, ino, type))
+ 		__f2fs_submit_merged_bio(sbi, inode, page, ino, type, rw);
+ }
+ 
+ void f2fs_flush_merged_bios(struct f2fs_sb_info *sbi)
+ {
+ 	f2fs_submit_merged_bio(sbi, DATA, WRITE);
+ 	f2fs_submit_merged_bio(sbi, NODE, WRITE);
+ 	f2fs_submit_merged_bio(sbi, META, WRITE);
+ }
+ 
+ /*
+  * Fill the locked page with data located in the block address.
+  * Return unlocked page.
+  */
+ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
+ {
+ 	struct bio *bio;
+ 	struct page *page = fio->encrypted_page ?
+ 			fio->encrypted_page : fio->page;
+ 
+ 	trace_f2fs_submit_page_bio(page, fio);
+ 	f2fs_trace_ios(fio, 0);
+ 
+ 	/* Allocate a new bio */
+ 	bio = __bio_alloc(fio->sbi, fio->new_blkaddr, 1, is_read_io(fio->rw));
+ 
+ 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
+ 		bio_put(bio);
+ 		return -EFAULT;
+ 	}
+ 
+ 	submit_bio(fio->rw, bio);
+ 	return 0;
+ }
+ 
+ void f2fs_submit_page_mbio(struct f2fs_io_info *fio)
+ {
+ 	struct f2fs_sb_info *sbi = fio->sbi;
+ 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
+ 	struct f2fs_bio_info *io;
+ 	bool is_read = is_read_io(fio->rw);
+ 	struct page *bio_page;
+ 
+ 	io = is_read ? &sbi->read_io : &sbi->write_io[btype];
+ 
+ 	if (fio->old_blkaddr != NEW_ADDR)
+ 		verify_block_addr(sbi, fio->old_blkaddr);
+ 	verify_block_addr(sbi, fio->new_blkaddr);
+ 
+ 	down_write(&io->io_rwsem);
+ 
+ 	if (!is_read)
+ 		inc_page_count(sbi, F2FS_WRITEBACK);
+ 
+ 	if (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||
+ 						io->fio.rw != fio->rw))
+ 		__submit_merged_bio(io);
+ alloc_new:
+ 	if (io->bio == NULL) {
+ 		int bio_blocks = MAX_BIO_BLOCKS(sbi);
+ 
+ 		io->bio = __bio_alloc(sbi, fio->new_blkaddr,
+ 						bio_blocks, is_read);
+ 		io->fio = *fio;
+ 	}
+ 
+ 	bio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;
+ 
+ 	if (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) <
+ 							PAGE_SIZE) {
+ 		__submit_merged_bio(io);
+ 		goto alloc_new;
+ 	}
+ 
+ 	io->last_block_in_bio = fio->new_blkaddr;
+ 	f2fs_trace_ios(fio, 0);
+ 
+ 	up_write(&io->io_rwsem);
+ 	trace_f2fs_submit_page_mbio(fio->page, fio);
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  /*
   * Lock ordering for the change of data block address:
   * ->data_page
@@@ -179,7 -362,72 +433,74 @@@ struct page *find_data_page(struct inod
  	struct address_space *mapping = inode->i_mapping;
  	struct dnode_of_data dn;
  	struct page *page;
 -	struct extent_info ei;
  	int err;
++<<<<<<< HEAD
++=======
+ 	struct f2fs_io_info fio = {
+ 		.sbi = F2FS_I_SB(inode),
+ 		.type = DATA,
+ 		.rw = rw,
+ 		.encrypted_page = NULL,
+ 	};
+ 
+ 	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+ 		return read_mapping_page(mapping, index, NULL);
+ 
+ 	page = f2fs_grab_cache_page(mapping, index, for_write);
+ 	if (!page)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
+ 		dn.data_blkaddr = ei.blk + index - ei.fofs;
+ 		goto got_it;
+ 	}
+ 
+ 	set_new_dnode(&dn, inode, NULL, NULL, 0);
+ 	err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
+ 	if (err)
+ 		goto put_err;
+ 	f2fs_put_dnode(&dn);
+ 
+ 	if (unlikely(dn.data_blkaddr == NULL_ADDR)) {
+ 		err = -ENOENT;
+ 		goto put_err;
+ 	}
+ got_it:
+ 	if (PageUptodate(page)) {
+ 		unlock_page(page);
+ 		return page;
+ 	}
+ 
+ 	/*
+ 	 * A new dentry page is allocated but not able to be written, since its
+ 	 * new inode page couldn't be allocated due to -ENOSPC.
+ 	 * In such the case, its blkaddr can be remained as NEW_ADDR.
+ 	 * see, f2fs_add_link -> get_new_data_page -> init_inode_metadata.
+ 	 */
+ 	if (dn.data_blkaddr == NEW_ADDR) {
+ 		zero_user_segment(page, 0, PAGE_SIZE);
+ 		SetPageUptodate(page);
+ 		unlock_page(page);
+ 		return page;
+ 	}
+ 
+ 	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
+ 	fio.page = page;
+ 	err = f2fs_submit_page_bio(&fio);
+ 	if (err)
+ 		goto put_err;
+ 	return page;
+ 
+ put_err:
+ 	f2fs_put_page(page, 1);
+ 	return ERR_PTR(err);
+ }
+ 
+ struct page *find_data_page(struct inode *inode, pgoff_t index)
+ {
+ 	struct address_space *mapping = inode->i_mapping;
+ 	struct page *page;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	page = find_get_page(mapping, index);
  	if (page && PageUptodate(page))
@@@ -284,161 -494,549 +605,483 @@@ struct page *get_new_data_page(struct i
  	struct dnode_of_data dn;
  	int err;
  
 -	page = f2fs_grab_cache_page(mapping, index, true);
 -	if (!page) {
 -		/*
 -		 * before exiting, we should make sure ipage will be released
 -		 * if any error occur.
 -		 */
 -		f2fs_put_page(ipage, 1);
 -		return ERR_PTR(-ENOMEM);
 -	}
 -
 -	set_new_dnode(&dn, inode, ipage, NULL, 0);
 -	err = f2fs_reserve_block(&dn, index);
 -	if (err) {
 -		f2fs_put_page(page, 1);
 +	set_new_dnode(&dn, inode, NULL, NULL, 0);
 +	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
 +	if (err)
  		return ERR_PTR(err);
 +
 +	if (dn.data_blkaddr == NULL_ADDR) {
 +		if (reserve_new_block(&dn)) {
 +			f2fs_put_dnode(&dn);
 +			return ERR_PTR(-ENOSPC);
 +		}
  	}
 -	if (!ipage)
 -		f2fs_put_dnode(&dn);
 +	f2fs_put_dnode(&dn);
 +repeat:
 +	page = grab_cache_page(mapping, index);
 +	if (!page)
 +		return ERR_PTR(-ENOMEM);
  
  	if (PageUptodate(page))
 -		goto got_it;
 +		return page;
  
  	if (dn.data_blkaddr == NEW_ADDR) {
- 		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+ 		zero_user_segment(page, 0, PAGE_SIZE);
  		SetPageUptodate(page);
  	} else {
 -		f2fs_put_page(page, 1);
 -
 -		/* if ipage exists, blkaddr should be NEW_ADDR */
 -		f2fs_bug_on(F2FS_I_SB(inode), ipage);
 -		page = get_lock_data_page(inode, index, true);
 -		if (IS_ERR(page))
 -			return page;
 +		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
 +		if (err)
 +			return ERR_PTR(err);
 +		lock_page(page);
 +		if (!PageUptodate(page)) {
 +			f2fs_put_page(page, 1);
 +			return ERR_PTR(-EIO);
 +		}
 +		if (page->mapping != mapping) {
 +			f2fs_put_page(page, 1);
 +			goto repeat;
 +		}
  	}
++<<<<<<< HEAD
 +
 +	if (new_i_size &&
 +		i_size_read(inode) < ((index + 1) << PAGE_CACHE_SHIFT)) {
 +		i_size_write(inode, ((index + 1) << PAGE_CACHE_SHIFT));
 +		mark_inode_dirty_sync(inode);
++=======
+ got_it:
+ 	if (new_i_size && i_size_read(inode) <
+ 				((loff_t)(index + 1) << PAGE_SHIFT)) {
+ 		i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));
+ 		/* Only the directory inode sets new_i_size */
+ 		set_inode_flag(F2FS_I(inode), FI_UPDATE_DIR);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  	return page;
  }
  
 -static int __allocate_data_block(struct dnode_of_data *dn)
 +static void read_end_io(struct bio *bio, int err)
  {
 -	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
 -	struct f2fs_summary sum;
 -	struct node_info ni;
 -	int seg = CURSEG_WARM_DATA;
 -	pgoff_t fofs;
 +	struct bio_vec *bvec;
 +	int i;
  
 -	if (unlikely(is_inode_flag_set(F2FS_I(dn->inode), FI_NO_ALLOC)))
 -		return -EPERM;
 +	bio_for_each_segment_all(bvec, bio, i) {
 +		struct page *page = bvec->bv_page;
  
 -	dn->data_blkaddr = datablock_addr(dn->node_page, dn->ofs_in_node);
 -	if (dn->data_blkaddr == NEW_ADDR)
 -		goto alloc;
 +		if (!err) {
 +			SetPageUptodate(page);
 +		} else {
 +			ClearPageUptodate(page);
 +			SetPageError(page);
 +		}
 +		unlock_page(page);
 +	}
 +	kfree(bio->bi_private);
 +	bio_put(bio);
 +}
  
 -	if (unlikely(!inc_valid_block_count(sbi, dn->inode, 1)))
 -		return -ENOSPC;
 +/*
 + * Fill the locked page with data located in the block address.
 + * Return unlocked page.
 + */
 +int f2fs_readpage(struct f2fs_sb_info *sbi, struct page *page,
 +					block_t blk_addr, int type)
 +{
 +	struct block_device *bdev = sbi->sb->s_bdev;
 +	struct bio *bio;
 +
 +	trace_f2fs_readpage(page, blk_addr, type);
  
 -alloc:
 -	get_node_info(sbi, dn->nid, &ni);
 -	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 +	down_read(&sbi->bio_sem);
 +
 +	/* Allocate a new bio */
 +	bio = f2fs_bio_alloc(bdev, 1);
  
 -	if (dn->ofs_in_node == 0 && dn->inode_page == dn->node_page)
 -		seg = CURSEG_DIRECT_IO;
++<<<<<<< HEAD
 +	/* Initialize the bio */
 +	bio->bi_sector = SECTOR_FROM_BLOCK(sbi, blk_addr);
 +	bio->bi_end_io = read_end_io;
  
 -	allocate_data_block(sbi, NULL, dn->data_blkaddr, &dn->data_blkaddr,
 -								&sum, seg);
 -	set_data_blkaddr(dn);
 +	if (bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) < PAGE_CACHE_SIZE) {
 +		kfree(bio->bi_private);
 +		bio_put(bio);
 +		up_read(&sbi->bio_sem);
 +		f2fs_put_page(page, 1);
 +		return -EFAULT;
 +	}
  
 +	submit_bio(type, bio);
 +	up_read(&sbi->bio_sem);
++=======
+ 	/* update i_size */
+ 	fofs = start_bidx_of_node(ofs_of_node(dn->node_page), dn->inode) +
+ 							dn->ofs_in_node;
+ 	if (i_size_read(dn->inode) < ((loff_t)(fofs + 1) << PAGE_SHIFT))
+ 		i_size_write(dn->inode,
+ 				((loff_t)(fofs + 1) << PAGE_SHIFT));
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	return 0;
  }
  
 -ssize_t f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)
 -{
 -	struct inode *inode = file_inode(iocb->ki_filp);
 -	struct f2fs_map_blocks map;
 -	ssize_t ret = 0;
 -
 -	map.m_lblk = F2FS_BYTES_TO_BLK(iocb->ki_pos);
 -	map.m_len = F2FS_BLK_ALIGN(iov_iter_count(from));
 -	map.m_next_pgofs = NULL;
 -
 -	if (f2fs_encrypted_inode(inode))
 -		return 0;
 -
 -	if (iocb->ki_flags & IOCB_DIRECT) {
 -		ret = f2fs_convert_inline_inode(inode);
 -		if (ret)
 -			return ret;
 -		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_DIO);
 -	}
 -	if (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA) {
 -		ret = f2fs_convert_inline_inode(inode);
 -		if (ret)
 -			return ret;
 -	}
 -	if (!f2fs_has_inline_data(inode))
 -		return f2fs_map_blocks(inode, &map, 1, F2FS_GET_BLOCK_PRE_AIO);
 -	return ret;
 -}
 -
  /*
 - * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with
 - * f2fs_map_blocks structure.
 - * If original data blocks are allocated, then give them to blockdev.
 - * Otherwise,
 - *     a. preallocate requested block addresses
 - *     b. do not use extent cache for better performance
 - *     c. give the block addresses to blockdev
 + * This function should be used by the data read flow only where it
 + * does not check the "create" flag that indicates block allocation.
 + * The reason for this special functionality is to exploit VFS readahead
 + * mechanism.
   */
 -int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
 -						int create, int flag)
 +static int get_data_block_ro(struct inode *inode, sector_t iblock,
 +			struct buffer_head *bh_result, int create)
  {
 -	unsigned int maxblocks = map->m_len;
 +	unsigned int blkbits = inode->i_sb->s_blocksize_bits;
 +	unsigned maxblocks = bh_result->b_size >> blkbits;
  	struct dnode_of_data dn;
 -	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 -	int mode = create ? ALLOC_NODE : LOOKUP_NODE_RA;
 -	pgoff_t pgofs, end_offset;
 -	int err = 0, ofs = 1;
 -	struct extent_info ei;
 -	bool allocated = false;
 -	block_t blkaddr;
 -
 -	map->m_len = 0;
 -	map->m_flags = 0;
 +	pgoff_t pgofs;
 +	int err;
  
 -	/* it only supports block size == page size */
 -	pgofs =	(pgoff_t)map->m_lblk;
 +	/* Get the page offset from the block offset(iblock) */
 +	pgofs =	(pgoff_t)(iblock >> (PAGE_CACHE_SHIFT - blkbits));
  
 -	if (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {
 -		map->m_pblk = ei.blk + pgofs - ei.fofs;
 -		map->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);
 -		map->m_flags = F2FS_MAP_MAPPED;
 -		goto out;
 +	if (check_extent_cache(inode, pgofs, bh_result)) {
 +		trace_f2fs_get_data_block(inode, iblock, bh_result, 0);
 +		return 0;
  	}
  
 -next_dnode:
 -	if (create)
 -		f2fs_lock_op(sbi);
 -
  	/* When reading holes, we need its node page */
  	set_new_dnode(&dn, inode, NULL, NULL, 0);
 -	err = get_dnode_of_data(&dn, pgofs, mode);
 +	err = get_dnode_of_data(&dn, pgofs, LOOKUP_NODE_RA);
  	if (err) {
 -		if (err == -ENOENT) {
 -			err = 0;
 -			if (map->m_next_pgofs)
 -				*map->m_next_pgofs =
 -					get_next_page_offset(&dn, pgofs);
 -		}
 -		goto unlock_out;
 +		trace_f2fs_get_data_block(inode, iblock, bh_result, err);
 +		return (err == -ENOENT) ? 0 : err;
  	}
  
 -	end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
 -
 -next_block:
 -	blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
 +	/* It does not support data allocation */
 +	BUG_ON(create);
  
 -	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR) {
 -		if (create) {
 -			if (unlikely(f2fs_cp_error(sbi))) {
 -				err = -EIO;
 -				goto sync_out;
 -			}
 -			if (flag == F2FS_GET_BLOCK_PRE_AIO) {
 -				if (blkaddr == NULL_ADDR)
 -					err = reserve_new_block(&dn);
 -			} else {
 -				err = __allocate_data_block(&dn);
 -			}
 -			if (err)
 -				goto sync_out;
 -			allocated = true;
 -			map->m_flags = F2FS_MAP_NEW;
 -			blkaddr = dn.data_blkaddr;
 -		} else {
 -			if (flag == F2FS_GET_BLOCK_FIEMAP &&
 -						blkaddr == NULL_ADDR) {
 -				if (map->m_next_pgofs)
 -					*map->m_next_pgofs = pgofs + 1;
 -			}
 -			if (flag != F2FS_GET_BLOCK_FIEMAP ||
 -						blkaddr != NEW_ADDR) {
 -				if (flag == F2FS_GET_BLOCK_BMAP)
 -					err = -ENOENT;
 -				goto sync_out;
 -			}
 -		}
 -	}
 -
 -	if (map->m_len == 0) {
 -		/* preallocated unwritten block should be mapped for fiemap. */
 -		if (blkaddr == NEW_ADDR)
 -			map->m_flags |= F2FS_MAP_UNWRITTEN;
 -		map->m_flags |= F2FS_MAP_MAPPED;
 -
 -		map->m_pblk = blkaddr;
 -		map->m_len = 1;
 -	} else if ((map->m_pblk != NEW_ADDR &&
 -			blkaddr == (map->m_pblk + ofs)) ||
 -			(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||
 -			flag == F2FS_GET_BLOCK_PRE_DIO ||
 -			flag == F2FS_GET_BLOCK_PRE_AIO) {
 -		ofs++;
 -		map->m_len++;
 -	} else {
 -		goto sync_out;
 -	}
 -
 -	dn.ofs_in_node++;
 -	pgofs++;
 +	if (dn.data_blkaddr != NEW_ADDR && dn.data_blkaddr != NULL_ADDR) {
 +		int i;
 +		unsigned int end_offset;
  
 -	if (map->m_len < maxblocks) {
 -		if (dn.ofs_in_node < end_offset)
 -			goto next_block;
 +		end_offset = IS_INODE(dn.node_page) ?
 +				ADDRS_PER_INODE :
 +				ADDRS_PER_BLOCK;
  
 -		if (allocated)
 -			sync_inode_page(&dn);
 -		f2fs_put_dnode(&dn);
 +		clear_buffer_new(bh_result);
  
 -		if (create) {
 -			f2fs_unlock_op(sbi);
 -			f2fs_balance_fs(sbi, allocated);
 -		}
 -		allocated = false;
 -		goto next_dnode;
 +		/* Give more consecutive addresses for the read ahead */
 +		for (i = 0; i < end_offset - dn.ofs_in_node; i++)
 +			if (((datablock_addr(dn.node_page,
 +							dn.ofs_in_node + i))
 +				!= (dn.data_blkaddr + i)) || maxblocks == i)
 +				break;
 +		map_bh(bh_result, inode->i_sb, dn.data_blkaddr);
 +		bh_result->b_size = (i << blkbits);
  	}
 -
 -sync_out:
 -	if (allocated)
 -		sync_inode_page(&dn);
  	f2fs_put_dnode(&dn);
++<<<<<<< HEAD
 +	trace_f2fs_get_data_block(inode, iblock, bh_result, 0);
++=======
+ unlock_out:
+ 	if (create) {
+ 		f2fs_unlock_op(sbi);
+ 		f2fs_balance_fs(sbi, allocated);
+ 	}
+ out:
+ 	trace_f2fs_map_blocks(inode, map, err);
+ 	return err;
+ }
+ 
+ static int __get_data_block(struct inode *inode, sector_t iblock,
+ 			struct buffer_head *bh, int create, int flag,
+ 			pgoff_t *next_pgofs)
+ {
+ 	struct f2fs_map_blocks map;
+ 	int ret;
+ 
+ 	map.m_lblk = iblock;
+ 	map.m_len = bh->b_size >> inode->i_blkbits;
+ 	map.m_next_pgofs = next_pgofs;
+ 
+ 	ret = f2fs_map_blocks(inode, &map, create, flag);
+ 	if (!ret) {
+ 		map_bh(bh, inode->i_sb, map.m_pblk);
+ 		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
+ 		bh->b_size = map.m_len << inode->i_blkbits;
+ 	}
+ 	return ret;
+ }
+ 
+ static int get_data_block(struct inode *inode, sector_t iblock,
+ 			struct buffer_head *bh_result, int create, int flag,
+ 			pgoff_t *next_pgofs)
+ {
+ 	return __get_data_block(inode, iblock, bh_result, create,
+ 							flag, next_pgofs);
+ }
+ 
+ static int get_data_block_dio(struct inode *inode, sector_t iblock,
+ 			struct buffer_head *bh_result, int create)
+ {
+ 	return __get_data_block(inode, iblock, bh_result, create,
+ 						F2FS_GET_BLOCK_DIO, NULL);
+ }
+ 
+ static int get_data_block_bmap(struct inode *inode, sector_t iblock,
+ 			struct buffer_head *bh_result, int create)
+ {
+ 	/* Block number less than F2FS MAX BLOCKS */
+ 	if (unlikely(iblock >= F2FS_I_SB(inode)->max_file_blocks))
+ 		return -EFBIG;
+ 
+ 	return __get_data_block(inode, iblock, bh_result, create,
+ 						F2FS_GET_BLOCK_BMAP, NULL);
+ }
+ 
+ static inline sector_t logical_to_blk(struct inode *inode, loff_t offset)
+ {
+ 	return (offset >> inode->i_blkbits);
+ }
+ 
+ static inline loff_t blk_to_logical(struct inode *inode, sector_t blk)
+ {
+ 	return (blk << inode->i_blkbits);
+ }
+ 
+ int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
+ 		u64 start, u64 len)
+ {
+ 	struct buffer_head map_bh;
+ 	sector_t start_blk, last_blk;
+ 	pgoff_t next_pgofs;
+ 	loff_t isize;
+ 	u64 logical = 0, phys = 0, size = 0;
+ 	u32 flags = 0;
+ 	int ret = 0;
+ 
+ 	ret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (f2fs_has_inline_data(inode)) {
+ 		ret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);
+ 		if (ret != -EAGAIN)
+ 			return ret;
+ 	}
+ 
+ 	inode_lock(inode);
+ 
+ 	isize = i_size_read(inode);
+ 	if (start >= isize)
+ 		goto out;
+ 
+ 	if (start + len > isize)
+ 		len = isize - start;
+ 
+ 	if (logical_to_blk(inode, len) == 0)
+ 		len = blk_to_logical(inode, 1);
+ 
+ 	start_blk = logical_to_blk(inode, start);
+ 	last_blk = logical_to_blk(inode, start + len - 1);
+ 
+ next:
+ 	memset(&map_bh, 0, sizeof(struct buffer_head));
+ 	map_bh.b_size = len;
+ 
+ 	ret = get_data_block(inode, start_blk, &map_bh, 0,
+ 					F2FS_GET_BLOCK_FIEMAP, &next_pgofs);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/* HOLE */
+ 	if (!buffer_mapped(&map_bh)) {
+ 		start_blk = next_pgofs;
+ 		/* Go through holes util pass the EOF */
+ 		if (blk_to_logical(inode, start_blk) < isize)
+ 			goto prep_next;
+ 		/* Found a hole beyond isize means no more extents.
+ 		 * Note that the premise is that filesystems don't
+ 		 * punch holes beyond isize and keep size unchanged.
+ 		 */
+ 		flags |= FIEMAP_EXTENT_LAST;
+ 	}
+ 
+ 	if (size) {
+ 		if (f2fs_encrypted_inode(inode))
+ 			flags |= FIEMAP_EXTENT_DATA_ENCRYPTED;
+ 
+ 		ret = fiemap_fill_next_extent(fieinfo, logical,
+ 				phys, size, flags);
+ 	}
+ 
+ 	if (start_blk > last_blk || ret)
+ 		goto out;
+ 
+ 	logical = blk_to_logical(inode, start_blk);
+ 	phys = blk_to_logical(inode, map_bh.b_blocknr);
+ 	size = map_bh.b_size;
+ 	flags = 0;
+ 	if (buffer_unwritten(&map_bh))
+ 		flags = FIEMAP_EXTENT_UNWRITTEN;
+ 
+ 	start_blk += logical_to_blk(inode, size);
+ 
+ prep_next:
+ 	cond_resched();
+ 	if (fatal_signal_pending(current))
+ 		ret = -EINTR;
+ 	else
+ 		goto next;
+ out:
+ 	if (ret == 1)
+ 		ret = 0;
+ 
+ 	inode_unlock(inode);
+ 	return ret;
+ }
+ 
+ /*
+  * This function was originally taken from fs/mpage.c, and customized for f2fs.
+  * Major change was from block_size == page_size in f2fs by default.
+  */
+ static int f2fs_mpage_readpages(struct address_space *mapping,
+ 			struct list_head *pages, struct page *page,
+ 			unsigned nr_pages)
+ {
+ 	struct bio *bio = NULL;
+ 	unsigned page_idx;
+ 	sector_t last_block_in_bio = 0;
+ 	struct inode *inode = mapping->host;
+ 	const unsigned blkbits = inode->i_blkbits;
+ 	const unsigned blocksize = 1 << blkbits;
+ 	sector_t block_in_file;
+ 	sector_t last_block;
+ 	sector_t last_block_in_file;
+ 	sector_t block_nr;
+ 	struct block_device *bdev = inode->i_sb->s_bdev;
+ 	struct f2fs_map_blocks map;
+ 
+ 	map.m_pblk = 0;
+ 	map.m_lblk = 0;
+ 	map.m_len = 0;
+ 	map.m_flags = 0;
+ 	map.m_next_pgofs = NULL;
+ 
+ 	for (page_idx = 0; nr_pages; page_idx++, nr_pages--) {
+ 
+ 		prefetchw(&page->flags);
+ 		if (pages) {
+ 			page = list_entry(pages->prev, struct page, lru);
+ 			list_del(&page->lru);
+ 			if (add_to_page_cache_lru(page, mapping,
+ 						  page->index, GFP_KERNEL))
+ 				goto next_page;
+ 		}
+ 
+ 		block_in_file = (sector_t)page->index;
+ 		last_block = block_in_file + nr_pages;
+ 		last_block_in_file = (i_size_read(inode) + blocksize - 1) >>
+ 								blkbits;
+ 		if (last_block > last_block_in_file)
+ 			last_block = last_block_in_file;
+ 
+ 		/*
+ 		 * Map blocks using the previous result first.
+ 		 */
+ 		if ((map.m_flags & F2FS_MAP_MAPPED) &&
+ 				block_in_file > map.m_lblk &&
+ 				block_in_file < (map.m_lblk + map.m_len))
+ 			goto got_it;
+ 
+ 		/*
+ 		 * Then do more f2fs_map_blocks() calls until we are
+ 		 * done with this page.
+ 		 */
+ 		map.m_flags = 0;
+ 
+ 		if (block_in_file < last_block) {
+ 			map.m_lblk = block_in_file;
+ 			map.m_len = last_block - block_in_file;
+ 
+ 			if (f2fs_map_blocks(inode, &map, 0,
+ 						F2FS_GET_BLOCK_READ))
+ 				goto set_error_page;
+ 		}
+ got_it:
+ 		if ((map.m_flags & F2FS_MAP_MAPPED)) {
+ 			block_nr = map.m_pblk + block_in_file - map.m_lblk;
+ 			SetPageMappedToDisk(page);
+ 
+ 			if (!PageUptodate(page) && !cleancache_get_page(page)) {
+ 				SetPageUptodate(page);
+ 				goto confused;
+ 			}
+ 		} else {
+ 			zero_user_segment(page, 0, PAGE_SIZE);
+ 			SetPageUptodate(page);
+ 			unlock_page(page);
+ 			goto next_page;
+ 		}
+ 
+ 		/*
+ 		 * This page will go to BIO.  Do we need to send this
+ 		 * BIO off first?
+ 		 */
+ 		if (bio && (last_block_in_bio != block_nr - 1)) {
+ submit_and_realloc:
+ 			submit_bio(READ, bio);
+ 			bio = NULL;
+ 		}
+ 		if (bio == NULL) {
+ 			struct fscrypt_ctx *ctx = NULL;
+ 
+ 			if (f2fs_encrypted_inode(inode) &&
+ 					S_ISREG(inode->i_mode)) {
+ 
+ 				ctx = fscrypt_get_ctx(inode);
+ 				if (IS_ERR(ctx))
+ 					goto set_error_page;
+ 
+ 				/* wait the page to be moved by cleaning */
+ 				f2fs_wait_on_encrypted_page_writeback(
+ 						F2FS_I_SB(inode), block_nr);
+ 			}
+ 
+ 			bio = bio_alloc(GFP_KERNEL,
+ 				min_t(int, nr_pages, BIO_MAX_PAGES));
+ 			if (!bio) {
+ 				if (ctx)
+ 					fscrypt_release_ctx(ctx);
+ 				goto set_error_page;
+ 			}
+ 			bio->bi_bdev = bdev;
+ 			bio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(block_nr);
+ 			bio->bi_end_io = f2fs_read_end_io;
+ 			bio->bi_private = ctx;
+ 		}
+ 
+ 		if (bio_add_page(bio, page, blocksize, 0) < blocksize)
+ 			goto submit_and_realloc;
+ 
+ 		last_block_in_bio = block_nr;
+ 		goto next_page;
+ set_error_page:
+ 		SetPageError(page);
+ 		zero_user_segment(page, 0, PAGE_SIZE);
+ 		unlock_page(page);
+ 		goto next_page;
+ confused:
+ 		if (bio) {
+ 			submit_bio(READ, bio);
+ 			bio = NULL;
+ 		}
+ 		unlock_page(page);
+ next_page:
+ 		if (pages)
+ 			put_page(page);
+ 	}
+ 	BUG_ON(pages && !list_empty(pages));
+ 	if (bio)
+ 		submit_bio(READ, bio);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	return 0;
  }
  
@@@ -496,13 -1133,22 +1139,18 @@@ static int f2fs_write_data_page(struct 
  					struct writeback_control *wbc)
  {
  	struct inode *inode = page->mapping->host;
 -	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 +	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
  	loff_t i_size = i_size_read(inode);
  	const pgoff_t end_index = ((unsigned long long) i_size)
++<<<<<<< HEAD
 +							>> PAGE_CACHE_SHIFT;
 +	unsigned offset;
++=======
+ 							>> PAGE_SHIFT;
+ 	unsigned offset = 0;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	bool need_balance_fs = false;
  	int err = 0;
 -	struct f2fs_io_info fio = {
 -		.sbi = sbi,
 -		.type = DATA,
 -		.rw = (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : WRITE,
 -		.page = page,
 -		.encrypted_page = NULL,
 -	};
 -
 -	trace_f2fs_writepage(page, DATA);
  
  	if (page->index < end_index)
  		goto write;
@@@ -511,21 -1157,19 +1159,26 @@@
  	 * If the offset is out-of-range of file size,
  	 * this page does not have to be written to disk.
  	 */
++<<<<<<< HEAD
 +	offset = i_size & (PAGE_CACHE_SIZE - 1);
 +	if ((page->index >= end_index + 1) || !offset) {
 +		if (S_ISDIR(inode->i_mode)) {
 +			dec_page_count(sbi, F2FS_DIRTY_DENTS);
 +			inode_dec_dirty_dents(inode);
 +		}
++=======
+ 	offset = i_size & (PAGE_SIZE - 1);
+ 	if ((page->index >= end_index + 1) || !offset)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		goto out;
 +	}
  
- 	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
+ 	zero_user_segment(page, offset, PAGE_SIZE);
  write:
 -	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 -		goto redirty_out;
 -	if (f2fs_is_drop_cache(inode))
 -		goto out;
 -	if (f2fs_is_volatile_file(inode) && !wbc->for_reclaim &&
 -			available_free_memory(sbi, BASE_CHECK))
 +	if (sbi->por_doing) {
 +		err = AOP_WRITEPAGE_ACTIVATE;
  		goto redirty_out;
 +	}
  
  	/* Dentry blocks are controlled by checkpoint */
  	if (S_ISDIR(inode->i_mode)) {
@@@ -570,6 -1234,138 +1223,141 @@@ static int __f2fs_writepage(struct pag
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * This function was copied from write_cche_pages from mm/page-writeback.c.
+  * The major change is making write step of cold data page separately from
+  * warm/hot data page.
+  */
+ static int f2fs_write_cache_pages(struct address_space *mapping,
+ 			struct writeback_control *wbc, writepage_t writepage,
+ 			void *data)
+ {
+ 	int ret = 0;
+ 	int done = 0;
+ 	struct pagevec pvec;
+ 	int nr_pages;
+ 	pgoff_t uninitialized_var(writeback_index);
+ 	pgoff_t index;
+ 	pgoff_t end;		/* Inclusive */
+ 	pgoff_t done_index;
+ 	int cycled;
+ 	int range_whole = 0;
+ 	int tag;
+ 	int step = 0;
+ 
+ 	pagevec_init(&pvec, 0);
+ next:
+ 	if (wbc->range_cyclic) {
+ 		writeback_index = mapping->writeback_index; /* prev offset */
+ 		index = writeback_index;
+ 		if (index == 0)
+ 			cycled = 1;
+ 		else
+ 			cycled = 0;
+ 		end = -1;
+ 	} else {
+ 		index = wbc->range_start >> PAGE_SHIFT;
+ 		end = wbc->range_end >> PAGE_SHIFT;
+ 		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
+ 			range_whole = 1;
+ 		cycled = 1; /* ignore range_cyclic tests */
+ 	}
+ 	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
+ 		tag = PAGECACHE_TAG_TOWRITE;
+ 	else
+ 		tag = PAGECACHE_TAG_DIRTY;
+ retry:
+ 	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
+ 		tag_pages_for_writeback(mapping, index, end);
+ 	done_index = index;
+ 	while (!done && (index <= end)) {
+ 		int i;
+ 
+ 		nr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag,
+ 			      min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1);
+ 		if (nr_pages == 0)
+ 			break;
+ 
+ 		for (i = 0; i < nr_pages; i++) {
+ 			struct page *page = pvec.pages[i];
+ 
+ 			if (page->index > end) {
+ 				done = 1;
+ 				break;
+ 			}
+ 
+ 			done_index = page->index;
+ 
+ 			lock_page(page);
+ 
+ 			if (unlikely(page->mapping != mapping)) {
+ continue_unlock:
+ 				unlock_page(page);
+ 				continue;
+ 			}
+ 
+ 			if (!PageDirty(page)) {
+ 				/* someone wrote it for us */
+ 				goto continue_unlock;
+ 			}
+ 
+ 			if (step == is_cold_data(page))
+ 				goto continue_unlock;
+ 
+ 			if (PageWriteback(page)) {
+ 				if (wbc->sync_mode != WB_SYNC_NONE)
+ 					f2fs_wait_on_page_writeback(page,
+ 								DATA, true);
+ 				else
+ 					goto continue_unlock;
+ 			}
+ 
+ 			BUG_ON(PageWriteback(page));
+ 			if (!clear_page_dirty_for_io(page))
+ 				goto continue_unlock;
+ 
+ 			ret = (*writepage)(page, wbc, data);
+ 			if (unlikely(ret)) {
+ 				if (ret == AOP_WRITEPAGE_ACTIVATE) {
+ 					unlock_page(page);
+ 					ret = 0;
+ 				} else {
+ 					done_index = page->index + 1;
+ 					done = 1;
+ 					break;
+ 				}
+ 			}
+ 
+ 			if (--wbc->nr_to_write <= 0 &&
+ 			    wbc->sync_mode == WB_SYNC_NONE) {
+ 				done = 1;
+ 				break;
+ 			}
+ 		}
+ 		pagevec_release(&pvec);
+ 		cond_resched();
+ 	}
+ 
+ 	if (step < 1) {
+ 		step++;
+ 		goto next;
+ 	}
+ 
+ 	if (!cycled && !done) {
+ 		cycled = 1;
+ 		index = 0;
+ 		end = writeback_index - 1;
+ 		goto retry;
+ 	}
+ 	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
+ 		mapping->writeback_index = done_index;
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  static int f2fs_write_data_pages(struct address_space *mapping,
  			    struct writeback_control *wbc)
  {
@@@ -593,15 -1404,106 +1381,109 @@@
  		mutex_lock(&sbi->writepages);
  		locked = true;
  	}
 -	ret = f2fs_write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
 -	f2fs_submit_merged_bio_cond(sbi, inode, NULL, 0, DATA, WRITE);
 +	ret = write_cache_pages(mapping, wbc, __f2fs_writepage, mapping);
  	if (locked)
  		mutex_unlock(&sbi->writepages);
 +	f2fs_submit_bio(sbi, DATA, (wbc->sync_mode == WB_SYNC_ALL));
  
 -	remove_dirty_inode(inode);
 +	remove_dirty_dir_inode(inode);
  
 -	wbc->nr_to_write = max((long)0, wbc->nr_to_write - diff);
 +	wbc->nr_to_write -= excess_nrtw;
  	return ret;
++<<<<<<< HEAD
++=======
+ 
+ skip_write:
+ 	wbc->pages_skipped += get_dirty_pages(inode);
+ 	trace_f2fs_writepages(mapping->host, wbc, DATA);
+ 	return 0;
+ }
+ 
+ static void f2fs_write_failed(struct address_space *mapping, loff_t to)
+ {
+ 	struct inode *inode = mapping->host;
+ 	loff_t i_size = i_size_read(inode);
+ 
+ 	if (to > i_size) {
+ 		truncate_pagecache(inode, i_size);
+ 		truncate_blocks(inode, i_size, true);
+ 	}
+ }
+ 
+ static int prepare_write_begin(struct f2fs_sb_info *sbi,
+ 			struct page *page, loff_t pos, unsigned len,
+ 			block_t *blk_addr, bool *node_changed)
+ {
+ 	struct inode *inode = page->mapping->host;
+ 	pgoff_t index = page->index;
+ 	struct dnode_of_data dn;
+ 	struct page *ipage;
+ 	bool locked = false;
+ 	struct extent_info ei;
+ 	int err = 0;
+ 
+ 	/*
+ 	 * we already allocated all the blocks, so we don't need to get
+ 	 * the block addresses when there is no need to fill the page.
+ 	 */
+ 	if (!f2fs_has_inline_data(inode) && !f2fs_encrypted_inode(inode) &&
+ 					len == PAGE_SIZE)
+ 		return 0;
+ 
+ 	if (f2fs_has_inline_data(inode) ||
+ 			(pos & PAGE_MASK) >= i_size_read(inode)) {
+ 		f2fs_lock_op(sbi);
+ 		locked = true;
+ 	}
+ restart:
+ 	/* check inline_data */
+ 	ipage = get_node_page(sbi, inode->i_ino);
+ 	if (IS_ERR(ipage)) {
+ 		err = PTR_ERR(ipage);
+ 		goto unlock_out;
+ 	}
+ 
+ 	set_new_dnode(&dn, inode, ipage, ipage, 0);
+ 
+ 	if (f2fs_has_inline_data(inode)) {
+ 		if (pos + len <= MAX_INLINE_DATA) {
+ 			read_inline_data(page, ipage);
+ 			set_inode_flag(F2FS_I(inode), FI_DATA_EXIST);
+ 			set_inline_node(ipage);
+ 		} else {
+ 			err = f2fs_convert_inline_page(&dn, page);
+ 			if (err)
+ 				goto out;
+ 			if (dn.data_blkaddr == NULL_ADDR)
+ 				err = f2fs_get_block(&dn, index);
+ 		}
+ 	} else if (locked) {
+ 		err = f2fs_get_block(&dn, index);
+ 	} else {
+ 		if (f2fs_lookup_extent_cache(inode, index, &ei)) {
+ 			dn.data_blkaddr = ei.blk + index - ei.fofs;
+ 		} else {
+ 			/* hole case */
+ 			err = get_dnode_of_data(&dn, index, LOOKUP_NODE);
+ 			if (err || (!err && dn.data_blkaddr == NULL_ADDR)) {
+ 				f2fs_put_dnode(&dn);
+ 				f2fs_lock_op(sbi);
+ 				locked = true;
+ 				goto restart;
+ 			}
+ 		}
+ 	}
+ 
+ 	/* convert_inline_page can make node_changed */
+ 	*blk_addr = dn.data_blkaddr;
+ 	*node_changed = dn.node_changed;
+ out:
+ 	f2fs_put_dnode(&dn);
+ unlock_out:
+ 	if (locked)
+ 		f2fs_unlock_op(sbi);
+ 	return err;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  static int f2fs_write_begin(struct file *file, struct address_space *mapping,
@@@ -609,63 -1511,92 +1491,87 @@@
  		struct page **pagep, void **fsdata)
  {
  	struct inode *inode = mapping->host;
++<<<<<<< HEAD
 +	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 +	struct page *page;
 +	pgoff_t index = ((unsigned long long) pos) >> PAGE_CACHE_SHIFT;
 +	struct dnode_of_data dn;
++=======
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	struct page *page = NULL;
+ 	pgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;
+ 	bool need_balance = false;
+ 	block_t blkaddr = NULL_ADDR;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	int err = 0;
 +	int ilock;
  
 -	trace_f2fs_write_begin(inode, pos, len, flags);
 +	/* for nobh_write_end */
 +	*fsdata = NULL;
  
 -	/*
 -	 * We should check this at this moment to avoid deadlock on inode page
 -	 * and #0 page. The locking rule for inline_data conversion should be:
 -	 * lock_page(page #0) -> lock_page(inode_page)
 -	 */
 -	if (index != 0) {
 -		err = f2fs_convert_inline_inode(inode);
 -		if (err)
 -			goto fail;
 -	}
 +	f2fs_balance_fs(sbi);
  repeat:
  	page = grab_cache_page_write_begin(mapping, index, flags);
 -	if (!page) {
 -		err = -ENOMEM;
 -		goto fail;
 -	}
 -
 +	if (!page)
 +		return -ENOMEM;
  	*pagep = page;
  
 -	err = prepare_write_begin(sbi, page, pos, len,
 -					&blkaddr, &need_balance);
 +	ilock = mutex_lock_op(sbi);
 +
 +	set_new_dnode(&dn, inode, NULL, NULL, 0);
 +	err = get_dnode_of_data(&dn, index, ALLOC_NODE);
  	if (err)
 -		goto fail;
 +		goto err;
  
 -	if (need_balance && has_not_enough_free_secs(sbi, 0)) {
 -		unlock_page(page);
 -		f2fs_balance_fs(sbi, true);
 -		lock_page(page);
 -		if (page->mapping != mapping) {
 -			/* The page got truncated from under us */
 -			f2fs_put_page(page, 1);
 -			goto repeat;
 -		}
 -	}
 +	if (dn.data_blkaddr == NULL_ADDR)
 +		err = reserve_new_block(&dn);
  
 -	f2fs_wait_on_page_writeback(page, DATA, false);
 +	f2fs_put_dnode(&dn);
 +	if (err)
 +		goto err;
  
 -	/* wait for GCed encrypted page writeback */
 -	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 -		f2fs_wait_on_encrypted_page_writeback(sbi, blkaddr);
 +	mutex_unlock_op(sbi, ilock);
  
++<<<<<<< HEAD
 +	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
 +		return 0;
++=======
+ 	if (len == PAGE_SIZE)
+ 		goto out_update;
+ 	if (PageUptodate(page))
+ 		goto out_clear;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
- 	if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
- 		unsigned start = pos & (PAGE_CACHE_SIZE - 1);
+ 	if ((pos & PAGE_MASK) >= i_size_read(inode)) {
+ 		unsigned start = pos & (PAGE_SIZE - 1);
  		unsigned end = start + len;
  
  		/* Reading beyond i_size is simple: memset to zero */
++<<<<<<< HEAD
 +		zero_user_segments(page, 0, start, end, PAGE_CACHE_SIZE);
 +		goto out;
 +	}
 +
 +	if (dn.data_blkaddr == NEW_ADDR) {
 +		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
++=======
+ 		zero_user_segments(page, 0, start, end, PAGE_SIZE);
+ 		goto out_update;
+ 	}
+ 
+ 	if (blkaddr == NEW_ADDR) {
+ 		zero_user_segment(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	} else {
 -		struct f2fs_io_info fio = {
 -			.sbi = sbi,
 -			.type = DATA,
 -			.rw = READ_SYNC,
 -			.old_blkaddr = blkaddr,
 -			.new_blkaddr = blkaddr,
 -			.page = page,
 -			.encrypted_page = NULL,
 -		};
 -		err = f2fs_submit_page_bio(&fio);
 +		err = f2fs_readpage(sbi, page, dn.data_blkaddr, READ_SYNC);
  		if (err)
 -			goto fail;
 -
 +			return err;
  		lock_page(page);
 -		if (unlikely(!PageUptodate(page))) {
 -			err = -EIO;
 -			goto fail;
 +		if (!PageUptodate(page)) {
 +			f2fs_put_page(page, 1);
 +			return -EIO;
  		}
 -		if (unlikely(page->mapping != mapping)) {
 +		if (page->mapping != mapping) {
  			f2fs_put_page(page, 1);
  			goto repeat;
  		}
@@@ -681,28 -1619,91 +1587,44 @@@ err
  	return err;
  }
  
 -static int f2fs_write_end(struct file *file,
 -			struct address_space *mapping,
 -			loff_t pos, unsigned len, unsigned copied,
 -			struct page *page, void *fsdata)
 -{
 -	struct inode *inode = page->mapping->host;
 -
 -	trace_f2fs_write_end(inode, pos, len, copied);
 -
 -	set_page_dirty(page);
 -
 -	if (pos + copied > i_size_read(inode)) {
 -		i_size_write(inode, pos + copied);
 -		mark_inode_dirty(inode);
 -	}
 -
 -	f2fs_put_page(page, 1);
 -	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 -	return copied;
 -}
 -
 -static int check_direct_IO(struct inode *inode, struct iov_iter *iter,
 -			   loff_t offset)
 -{
 -	unsigned blocksize_mask = inode->i_sb->s_blocksize - 1;
 -
 -	if (offset & blocksize_mask)
 -		return -EINVAL;
 -
 -	if (iov_iter_alignment(iter) & blocksize_mask)
 -		return -EINVAL;
 -
 -	return 0;
 -}
 -
 -static ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter,
 -			      loff_t offset)
 +static ssize_t f2fs_direct_IO(int rw, struct kiocb *iocb,
 +		const struct iovec *iov, loff_t offset, unsigned long nr_segs)
  {
 -	struct address_space *mapping = iocb->ki_filp->f_mapping;
 -	struct inode *inode = mapping->host;
 -	size_t count = iov_iter_count(iter);
 -	int err;
 -
 -	err = check_direct_IO(inode, iter, offset);
 -	if (err)
 -		return err;
 +	struct file *file = iocb->ki_filp;
 +	struct inode *inode = file->f_mapping->host;
  
 -	if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 +	if (rw == WRITE)
  		return 0;
  
 -	trace_f2fs_direct_IO_enter(inode, offset, count, iov_iter_rw(iter));
 -
 -	err = blockdev_direct_IO(iocb, inode, iter, offset, get_data_block_dio);
 -	if (err < 0 && iov_iter_rw(iter) == WRITE)
 -		f2fs_write_failed(mapping, offset + count);
 -
 -	trace_f2fs_direct_IO_exit(inode, offset, count, iov_iter_rw(iter), err);
 -
 -	return err;
 +	/* Needs synchronization with the cleaner */
 +	return blockdev_direct_IO(rw, iocb, inode, iov, offset, nr_segs,
 +						  get_data_block_ro);
  }
  
 -void f2fs_invalidate_page(struct page *page, unsigned int offset,
 -							unsigned int length)
 +static void f2fs_invalidate_data_page(struct page *page, unsigned long offset)
  {
  	struct inode *inode = page->mapping->host;
++<<<<<<< HEAD
 +	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 +	if (S_ISDIR(inode->i_mode) && PageDirty(page)) {
 +		dec_page_count(sbi, F2FS_DIRTY_DENTS);
 +		inode_dec_dirty_dents(inode);
++=======
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 
+ 	if (inode->i_ino >= F2FS_ROOT_INO(sbi) &&
+ 		(offset % PAGE_SIZE || length != PAGE_SIZE))
+ 		return;
+ 
+ 	if (PageDirty(page)) {
+ 		if (inode->i_ino == F2FS_META_INO(sbi))
+ 			dec_page_count(sbi, F2FS_DIRTY_META);
+ 		else if (inode->i_ino == F2FS_NODE_INO(sbi))
+ 			dec_page_count(sbi, F2FS_DIRTY_NODES);
+ 		else
+ 			inode_dec_dirty_pages(inode);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
 -
 -	/* This is atomic written page, keep Private */
 -	if (IS_ATOMIC_WRITTEN_PAGE(page))
 -		return;
 -
  	ClearPagePrivate(page);
  }
  
diff --cc fs/f2fs/debug.c
index 8d9943786c31,f4a61a5ff79f..000000000000
--- a/fs/f2fs/debug.c
+++ b/fs/f2fs/debug.c
@@@ -158,19 -175,35 +158,39 @@@ static void update_mem_info(struct f2fs
  	si->base_mem += sizeof(struct f2fs_nm_info);
  	si->base_mem += __bitmap_size(sbi, NAT_BITMAP);
  
 -get_cache:
 -	si->cache_mem = 0;
 -
  	/* build gc */
 -	if (sbi->gc_thread)
 -		si->cache_mem += sizeof(struct f2fs_gc_kthread);
 -
 -	/* build merge flush thread */
 -	if (SM_I(sbi)->cmd_control_info)
 -		si->cache_mem += sizeof(struct flush_cmd_control);
 +	si->base_mem += sizeof(struct f2fs_gc_kthread);
  
 +get_cache:
  	/* free nids */
++<<<<<<< HEAD
 +	si->cache_mem = NM_I(sbi)->fcnt;
 +	si->cache_mem += NM_I(sbi)->nat_cnt;
 +	npages = sbi->node_inode->i_mapping->nrpages;
 +	si->cache_mem += npages << PAGE_CACHE_SHIFT;
 +	npages = sbi->meta_inode->i_mapping->nrpages;
 +	si->cache_mem += npages << PAGE_CACHE_SHIFT;
 +	si->cache_mem += sbi->n_orphans * sizeof(struct orphan_inode_entry);
 +	si->cache_mem += sbi->n_dirty_dirs * sizeof(struct dir_inode_entry);
++=======
+ 	si->cache_mem += NM_I(sbi)->fcnt * sizeof(struct free_nid);
+ 	si->cache_mem += NM_I(sbi)->nat_cnt * sizeof(struct nat_entry);
+ 	si->cache_mem += NM_I(sbi)->dirty_nat_cnt *
+ 					sizeof(struct nat_entry_set);
+ 	si->cache_mem += si->inmem_pages * sizeof(struct inmem_pages);
+ 	for (i = 0; i <= UPDATE_INO; i++)
+ 		si->cache_mem += sbi->im[i].ino_num * sizeof(struct ino_entry);
+ 	si->cache_mem += atomic_read(&sbi->total_ext_tree) *
+ 						sizeof(struct extent_tree);
+ 	si->cache_mem += atomic_read(&sbi->total_ext_node) *
+ 						sizeof(struct extent_node);
+ 
+ 	si->page_mem = 0;
+ 	npages = NODE_MAPPING(sbi)->nrpages;
+ 	si->page_mem += (unsigned long long)npages << PAGE_SHIFT;
+ 	npages = META_MAPPING(sbi)->nrpages;
+ 	si->page_mem += (unsigned long long)npages << PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  static int stat_show(struct seq_file *s, void *v)
diff --cc fs/f2fs/file.c
index 1cae864f8dfc,443e07705c2a..000000000000
--- a/fs/f2fs/file.c
+++ b/fs/f2fs/file.c
@@@ -76,16 -71,14 +76,21 @@@ static int f2fs_vm_page_mkwrite(struct 
  	 * check to see if the page is mapped already (no holes)
  	 */
  	if (PageMappedToDisk(page))
 -		goto mapped;
 +		goto out;
 +
 +	/* fill the page */
 +	wait_on_page_writeback(page);
  
  	/* page is wholly or partially inside EOF */
++<<<<<<< HEAD
 +	if (((page->index + 1) << PAGE_CACHE_SHIFT) > i_size_read(inode)) {
++=======
+ 	if (((loff_t)(page->index + 1) << PAGE_SHIFT) >
+ 						i_size_read(inode)) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		unsigned offset;
- 		offset = i_size_read(inode) & ~PAGE_CACHE_MASK;
- 		zero_user_segment(page, offset, PAGE_CACHE_SIZE);
+ 		offset = i_size_read(inode) & ~PAGE_MASK;
+ 		zero_user_segment(page, offset, PAGE_SIZE);
  	}
  	set_page_dirty(page);
  	SetPageUptodate(page);
@@@ -161,8 -287,152 +166,138 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static pgoff_t __get_first_dirty_index(struct address_space *mapping,
+ 						pgoff_t pgofs, int whence)
+ {
+ 	struct pagevec pvec;
+ 	int nr_pages;
+ 
+ 	if (whence != SEEK_DATA)
+ 		return 0;
+ 
+ 	/* find first dirty page index */
+ 	pagevec_init(&pvec, 0);
+ 	nr_pages = pagevec_lookup_tag(&pvec, mapping, &pgofs,
+ 					PAGECACHE_TAG_DIRTY, 1);
+ 	pgofs = nr_pages ? pvec.pages[0]->index : ULONG_MAX;
+ 	pagevec_release(&pvec);
+ 	return pgofs;
+ }
+ 
+ static bool __found_offset(block_t blkaddr, pgoff_t dirty, pgoff_t pgofs,
+ 							int whence)
+ {
+ 	switch (whence) {
+ 	case SEEK_DATA:
+ 		if ((blkaddr == NEW_ADDR && dirty == pgofs) ||
+ 			(blkaddr != NEW_ADDR && blkaddr != NULL_ADDR))
+ 			return true;
+ 		break;
+ 	case SEEK_HOLE:
+ 		if (blkaddr == NULL_ADDR)
+ 			return true;
+ 		break;
+ 	}
+ 	return false;
+ }
+ 
+ static loff_t f2fs_seek_block(struct file *file, loff_t offset, int whence)
+ {
+ 	struct inode *inode = file->f_mapping->host;
+ 	loff_t maxbytes = inode->i_sb->s_maxbytes;
+ 	struct dnode_of_data dn;
+ 	pgoff_t pgofs, end_offset, dirty;
+ 	loff_t data_ofs = offset;
+ 	loff_t isize;
+ 	int err = 0;
+ 
+ 	inode_lock(inode);
+ 
+ 	isize = i_size_read(inode);
+ 	if (offset >= isize)
+ 		goto fail;
+ 
+ 	/* handle inline data case */
+ 	if (f2fs_has_inline_data(inode) || f2fs_has_inline_dentry(inode)) {
+ 		if (whence == SEEK_HOLE)
+ 			data_ofs = isize;
+ 		goto found;
+ 	}
+ 
+ 	pgofs = (pgoff_t)(offset >> PAGE_SHIFT);
+ 
+ 	dirty = __get_first_dirty_index(inode->i_mapping, pgofs, whence);
+ 
+ 	for (; data_ofs < isize; data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
+ 		set_new_dnode(&dn, inode, NULL, NULL, 0);
+ 		err = get_dnode_of_data(&dn, pgofs, LOOKUP_NODE_RA);
+ 		if (err && err != -ENOENT) {
+ 			goto fail;
+ 		} else if (err == -ENOENT) {
+ 			/* direct node does not exists */
+ 			if (whence == SEEK_DATA) {
+ 				pgofs = get_next_page_offset(&dn, pgofs);
+ 				continue;
+ 			} else {
+ 				goto found;
+ 			}
+ 		}
+ 
+ 		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+ 
+ 		/* find data/hole in dnode block */
+ 		for (; dn.ofs_in_node < end_offset;
+ 				dn.ofs_in_node++, pgofs++,
+ 				data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
+ 			block_t blkaddr;
+ 			blkaddr = datablock_addr(dn.node_page, dn.ofs_in_node);
+ 
+ 			if (__found_offset(blkaddr, dirty, pgofs, whence)) {
+ 				f2fs_put_dnode(&dn);
+ 				goto found;
+ 			}
+ 		}
+ 		f2fs_put_dnode(&dn);
+ 	}
+ 
+ 	if (whence == SEEK_DATA)
+ 		goto fail;
+ found:
+ 	if (whence == SEEK_HOLE && data_ofs > isize)
+ 		data_ofs = isize;
+ 	inode_unlock(inode);
+ 	return vfs_setpos(file, data_ofs, maxbytes);
+ fail:
+ 	inode_unlock(inode);
+ 	return -ENXIO;
+ }
+ 
+ static loff_t f2fs_llseek(struct file *file, loff_t offset, int whence)
+ {
+ 	struct inode *inode = file->f_mapping->host;
+ 	loff_t maxbytes = inode->i_sb->s_maxbytes;
+ 
+ 	switch (whence) {
+ 	case SEEK_SET:
+ 	case SEEK_CUR:
+ 	case SEEK_END:
+ 		return generic_file_llseek_size(file, offset, whence,
+ 						maxbytes, i_size_read(inode));
+ 	case SEEK_DATA:
+ 	case SEEK_HOLE:
+ 		if (offset < 0)
+ 			return -ENXIO;
+ 		return f2fs_seek_block(file, offset, whence);
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  static int f2fs_file_mmap(struct file *file, struct vm_area_struct *vma)
  {
 -	struct inode *inode = file_inode(file);
 -	int err;
 -
 -	if (f2fs_encrypted_inode(inode)) {
 -		err = fscrypt_get_encryption_info(inode);
 -		if (err)
 -			return 0;
 -		if (!f2fs_encrypted_inode(inode))
 -			return -ENOKEY;
 -	}
 -
 -	/* we don't need to use inline_data strictly */
 -	err = f2fs_convert_inline_inode(inode);
 -	if (err)
 -		return err;
 -
  	file_accessed(file);
  	vma->vm_ops = &f2fs_file_vm_ops;
  	return 0;
@@@ -204,32 -505,41 +339,48 @@@ void truncate_data_blocks(struct dnode_
  	truncate_data_blocks_range(dn, ADDRS_PER_BLOCK);
  }
  
 -static int truncate_partial_data_page(struct inode *inode, u64 from,
 -								bool cache_only)
 +static void truncate_partial_data_page(struct inode *inode, u64 from)
  {
++<<<<<<< HEAD
 +	unsigned offset = from & (PAGE_CACHE_SIZE - 1);
++=======
+ 	unsigned offset = from & (PAGE_SIZE - 1);
+ 	pgoff_t index = from >> PAGE_SHIFT;
+ 	struct address_space *mapping = inode->i_mapping;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	struct page *page;
  
 -	if (!offset && !cache_only)
 -		return 0;
 +	if (!offset)
 +		return;
 +
 +	page = find_data_page(inode, from >> PAGE_CACHE_SHIFT, false);
 +	if (IS_ERR(page))
++<<<<<<< HEAD
 +		return;
  
 -	if (cache_only) {
 -		page = f2fs_grab_cache_page(mapping, index, false);
 -		if (page && PageUptodate(page))
 -			goto truncate_out;
 +	lock_page(page);
 +	if (page->mapping != inode->i_mapping) {
  		f2fs_put_page(page, 1);
 -		return 0;
 +		return;
  	}
 -
 -	page = get_lock_data_page(inode, index, true);
 -	if (IS_ERR(page))
 +	wait_on_page_writeback(page);
 +	zero_user(page, offset, PAGE_CACHE_SIZE - offset);
 +	set_page_dirty(page);
++=======
+ 		return 0;
+ truncate_out:
+ 	f2fs_wait_on_page_writeback(page, DATA, true);
+ 	zero_user(page, offset, PAGE_SIZE - offset);
+ 	if (!cache_only || !f2fs_encrypted_inode(inode) ||
+ 					!S_ISREG(inode->i_mode))
+ 		set_page_dirty(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	f2fs_put_page(page, 1);
 -	return 0;
  }
  
 -int truncate_blocks(struct inode *inode, u64 from, bool lock)
 +static int truncate_blocks(struct inode *inode, u64 from)
  {
 -	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 +	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
  	unsigned int blocksize = inode->i_sb->s_blocksize;
  	struct dnode_of_data dn;
  	pgoff_t free_from;
@@@ -425,34 -793,45 +576,53 @@@ static int punch_hole(struct inode *ino
  {
  	pgoff_t pg_start, pg_end;
  	loff_t off_start, off_end;
 -	int ret;
 -
 -	ret = f2fs_convert_inline_inode(inode);
 -	if (ret)
 -		return ret;
 +	int ret = 0;
  
- 	pg_start = ((unsigned long long) offset) >> PAGE_CACHE_SHIFT;
- 	pg_end = ((unsigned long long) offset + len) >> PAGE_CACHE_SHIFT;
+ 	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
+ 	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
  
- 	off_start = offset & (PAGE_CACHE_SIZE - 1);
- 	off_end = (offset + len) & (PAGE_CACHE_SIZE - 1);
+ 	off_start = offset & (PAGE_SIZE - 1);
+ 	off_end = (offset + len) & (PAGE_SIZE - 1);
  
  	if (pg_start == pg_end) {
 -		ret = fill_zero(inode, pg_start, off_start,
 +		fill_zero(inode, pg_start, off_start,
  						off_end - off_start);
 -		if (ret)
 -			return ret;
  	} else {
++<<<<<<< HEAD
 +		if (off_start)
 +			fill_zero(inode, pg_start++, off_start,
 +					PAGE_CACHE_SIZE - off_start);
 +		if (off_end)
 +			fill_zero(inode, pg_end, 0, off_end);
++=======
+ 		if (off_start) {
+ 			ret = fill_zero(inode, pg_start++, off_start,
+ 						PAGE_SIZE - off_start);
+ 			if (ret)
+ 				return ret;
+ 		}
+ 		if (off_end) {
+ 			ret = fill_zero(inode, pg_end, 0, off_end);
+ 			if (ret)
+ 				return ret;
+ 		}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  		if (pg_start < pg_end) {
  			struct address_space *mapping = inode->i_mapping;
  			loff_t blk_start, blk_end;
 -			struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 +			struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 +			int ilock;
  
 -			f2fs_balance_fs(sbi, true);
 +			f2fs_balance_fs(sbi);
  
++<<<<<<< HEAD
 +			blk_start = pg_start << PAGE_CACHE_SHIFT;
 +			blk_end = pg_end << PAGE_CACHE_SHIFT;
++=======
+ 			blk_start = (loff_t)pg_start << PAGE_SHIFT;
+ 			blk_end = (loff_t)pg_end << PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			truncate_inode_pages_range(mapping, blk_start,
  					blk_end - 1);
  
@@@ -462,15 -841,304 +632,312 @@@
  		}
  	}
  
++<<<<<<< HEAD
 +	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
 +		i_size_read(inode) <= (offset + len)) {
 +		i_size_write(inode, offset);
++=======
+ 	return ret;
+ }
+ 
+ static int __exchange_data_block(struct inode *inode, pgoff_t src,
+ 					pgoff_t dst, bool full)
+ {
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	struct dnode_of_data dn;
+ 	block_t new_addr;
+ 	bool do_replace = false;
+ 	int ret;
+ 
+ 	set_new_dnode(&dn, inode, NULL, NULL, 0);
+ 	ret = get_dnode_of_data(&dn, src, LOOKUP_NODE_RA);
+ 	if (ret && ret != -ENOENT) {
+ 		return ret;
+ 	} else if (ret == -ENOENT) {
+ 		new_addr = NULL_ADDR;
+ 	} else {
+ 		new_addr = dn.data_blkaddr;
+ 		if (!is_checkpointed_data(sbi, new_addr)) {
+ 			/* do not invalidate this block address */
+ 			f2fs_update_data_blkaddr(&dn, NULL_ADDR);
+ 			do_replace = true;
+ 		}
+ 		f2fs_put_dnode(&dn);
+ 	}
+ 
+ 	if (new_addr == NULL_ADDR)
+ 		return full ? truncate_hole(inode, dst, dst + 1) : 0;
+ 
+ 	if (do_replace) {
+ 		struct page *ipage = get_node_page(sbi, inode->i_ino);
+ 		struct node_info ni;
+ 
+ 		if (IS_ERR(ipage)) {
+ 			ret = PTR_ERR(ipage);
+ 			goto err_out;
+ 		}
+ 
+ 		set_new_dnode(&dn, inode, ipage, NULL, 0);
+ 		ret = f2fs_reserve_block(&dn, dst);
+ 		if (ret)
+ 			goto err_out;
+ 
+ 		truncate_data_blocks_range(&dn, 1);
+ 
+ 		get_node_info(sbi, dn.nid, &ni);
+ 		f2fs_replace_block(sbi, &dn, dn.data_blkaddr, new_addr,
+ 				ni.version, true, false);
+ 		f2fs_put_dnode(&dn);
+ 	} else {
+ 		struct page *psrc, *pdst;
+ 
+ 		psrc = get_lock_data_page(inode, src, true);
+ 		if (IS_ERR(psrc))
+ 			return PTR_ERR(psrc);
+ 		pdst = get_new_data_page(inode, NULL, dst, true);
+ 		if (IS_ERR(pdst)) {
+ 			f2fs_put_page(psrc, 1);
+ 			return PTR_ERR(pdst);
+ 		}
+ 		f2fs_copy_page(psrc, pdst);
+ 		set_page_dirty(pdst);
+ 		f2fs_put_page(pdst, 1);
+ 		f2fs_put_page(psrc, 1);
+ 
+ 		return truncate_hole(inode, src, src + 1);
+ 	}
+ 	return 0;
+ 
+ err_out:
+ 	if (!get_dnode_of_data(&dn, src, LOOKUP_NODE)) {
+ 		f2fs_update_data_blkaddr(&dn, new_addr);
+ 		f2fs_put_dnode(&dn);
+ 	}
+ 	return ret;
+ }
+ 
+ static int f2fs_do_collapse(struct inode *inode, pgoff_t start, pgoff_t end)
+ {
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	pgoff_t nrpages = (i_size_read(inode) + PAGE_SIZE - 1) / PAGE_SIZE;
+ 	int ret = 0;
+ 
+ 	for (; end < nrpages; start++, end++) {
+ 		f2fs_balance_fs(sbi, true);
+ 		f2fs_lock_op(sbi);
+ 		ret = __exchange_data_block(inode, end, start, true);
+ 		f2fs_unlock_op(sbi);
+ 		if (ret)
+ 			break;
+ 	}
+ 	return ret;
+ }
+ 
+ static int f2fs_collapse_range(struct inode *inode, loff_t offset, loff_t len)
+ {
+ 	pgoff_t pg_start, pg_end;
+ 	loff_t new_size;
+ 	int ret;
+ 
+ 	if (offset + len >= i_size_read(inode))
+ 		return -EINVAL;
+ 
+ 	/* collapse range should be aligned to block size of f2fs. */
+ 	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
+ 		return -EINVAL;
+ 
+ 	ret = f2fs_convert_inline_inode(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	pg_start = offset >> PAGE_SHIFT;
+ 	pg_end = (offset + len) >> PAGE_SHIFT;
+ 
+ 	/* write out all dirty pages from offset */
+ 	ret = filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+ 	if (ret)
+ 		return ret;
+ 
+ 	truncate_pagecache(inode, offset);
+ 
+ 	ret = f2fs_do_collapse(inode, pg_start, pg_end);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* write out all moved pages, if possible */
+ 	filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+ 	truncate_pagecache(inode, offset);
+ 
+ 	new_size = i_size_read(inode) - len;
+ 	truncate_pagecache(inode, new_size);
+ 
+ 	ret = truncate_blocks(inode, new_size, true);
+ 	if (!ret)
+ 		i_size_write(inode, new_size);
+ 
+ 	return ret;
+ }
+ 
+ static int f2fs_zero_range(struct inode *inode, loff_t offset, loff_t len,
+ 								int mode)
+ {
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	struct address_space *mapping = inode->i_mapping;
+ 	pgoff_t index, pg_start, pg_end;
+ 	loff_t new_size = i_size_read(inode);
+ 	loff_t off_start, off_end;
+ 	int ret = 0;
+ 
+ 	ret = inode_newsize_ok(inode, (len + offset));
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = f2fs_convert_inline_inode(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = filemap_write_and_wait_range(mapping, offset, offset + len - 1);
+ 	if (ret)
+ 		return ret;
+ 
+ 	truncate_pagecache_range(inode, offset, offset + len - 1);
+ 
+ 	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
+ 	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+ 
+ 	off_start = offset & (PAGE_SIZE - 1);
+ 	off_end = (offset + len) & (PAGE_SIZE - 1);
+ 
+ 	if (pg_start == pg_end) {
+ 		ret = fill_zero(inode, pg_start, off_start,
+ 						off_end - off_start);
+ 		if (ret)
+ 			return ret;
+ 
+ 		if (offset + len > new_size)
+ 			new_size = offset + len;
+ 		new_size = max_t(loff_t, new_size, offset + len);
+ 	} else {
+ 		if (off_start) {
+ 			ret = fill_zero(inode, pg_start++, off_start,
+ 						PAGE_SIZE - off_start);
+ 			if (ret)
+ 				return ret;
+ 
+ 			new_size = max_t(loff_t, new_size,
+ 					(loff_t)pg_start << PAGE_SHIFT);
+ 		}
+ 
+ 		for (index = pg_start; index < pg_end; index++) {
+ 			struct dnode_of_data dn;
+ 			struct page *ipage;
+ 
+ 			f2fs_lock_op(sbi);
+ 
+ 			ipage = get_node_page(sbi, inode->i_ino);
+ 			if (IS_ERR(ipage)) {
+ 				ret = PTR_ERR(ipage);
+ 				f2fs_unlock_op(sbi);
+ 				goto out;
+ 			}
+ 
+ 			set_new_dnode(&dn, inode, ipage, NULL, 0);
+ 			ret = f2fs_reserve_block(&dn, index);
+ 			if (ret) {
+ 				f2fs_unlock_op(sbi);
+ 				goto out;
+ 			}
+ 
+ 			if (dn.data_blkaddr != NEW_ADDR) {
+ 				invalidate_blocks(sbi, dn.data_blkaddr);
+ 				f2fs_update_data_blkaddr(&dn, NEW_ADDR);
+ 			}
+ 			f2fs_put_dnode(&dn);
+ 			f2fs_unlock_op(sbi);
+ 
+ 			new_size = max_t(loff_t, new_size,
+ 				(loff_t)(index + 1) << PAGE_SHIFT);
+ 		}
+ 
+ 		if (off_end) {
+ 			ret = fill_zero(inode, pg_end, 0, off_end);
+ 			if (ret)
+ 				goto out;
+ 
+ 			new_size = max_t(loff_t, new_size, offset + len);
+ 		}
+ 	}
+ 
+ out:
+ 	if (!(mode & FALLOC_FL_KEEP_SIZE) && i_size_read(inode) < new_size) {
+ 		i_size_write(inode, new_size);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		mark_inode_dirty(inode);
 -		update_inode_page(inode);
  	}
  
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int f2fs_insert_range(struct inode *inode, loff_t offset, loff_t len)
+ {
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	pgoff_t pg_start, pg_end, delta, nrpages, idx;
+ 	loff_t new_size;
+ 	int ret = 0;
+ 
+ 	new_size = i_size_read(inode) + len;
+ 	if (new_size > inode->i_sb->s_maxbytes)
+ 		return -EFBIG;
+ 
+ 	if (offset >= i_size_read(inode))
+ 		return -EINVAL;
+ 
+ 	/* insert range should be aligned to block size of f2fs. */
+ 	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
+ 		return -EINVAL;
+ 
+ 	ret = f2fs_convert_inline_inode(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	f2fs_balance_fs(sbi, true);
+ 
+ 	ret = truncate_blocks(inode, i_size_read(inode), true);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* write out all dirty pages from offset */
+ 	ret = filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+ 	if (ret)
+ 		return ret;
+ 
+ 	truncate_pagecache(inode, offset);
+ 
+ 	pg_start = offset >> PAGE_SHIFT;
+ 	pg_end = (offset + len) >> PAGE_SHIFT;
+ 	delta = pg_end - pg_start;
+ 	nrpages = (i_size_read(inode) + PAGE_SIZE - 1) / PAGE_SIZE;
+ 
+ 	for (idx = nrpages - 1; idx >= pg_start && idx != -1; idx--) {
+ 		f2fs_lock_op(sbi);
+ 		ret = __exchange_data_block(inode, idx, idx + delta, false);
+ 		f2fs_unlock_op(sbi);
+ 		if (ret)
+ 			break;
+ 	}
+ 
+ 	/* write out all moved pages, if possible */
+ 	filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+ 	truncate_pagecache(inode, offset);
+ 
+ 	if (!ret)
+ 		i_size_write(inode, new_size);
+ 	return ret;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  static int expand_inode_data(struct inode *inode, loff_t offset,
  					loff_t len, int mode)
  {
@@@ -484,43 -1152,40 +951,61 @@@
  	if (ret)
  		return ret;
  
++<<<<<<< HEAD
 +	pg_start = ((unsigned long long) offset) >> PAGE_CACHE_SHIFT;
 +	pg_end = ((unsigned long long) offset + len) >> PAGE_CACHE_SHIFT;
++=======
+ 	ret = f2fs_convert_inline_inode(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	f2fs_balance_fs(sbi, true);
+ 
+ 	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
+ 	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
- 	off_start = offset & (PAGE_CACHE_SIZE - 1);
- 	off_end = (offset + len) & (PAGE_CACHE_SIZE - 1);
+ 	off_start = offset & (PAGE_SIZE - 1);
+ 	off_end = (offset + len) & (PAGE_SIZE - 1);
  
 -	f2fs_lock_op(sbi);
 -
  	for (index = pg_start; index <= pg_end; index++) {
  		struct dnode_of_data dn;
 +		int ilock;
  
 -		if (index == pg_end && !off_end)
 -			goto noalloc;
 -
 +		ilock = mutex_lock_op(sbi);
  		set_new_dnode(&dn, inode, NULL, NULL, 0);
 -		ret = f2fs_reserve_block(&dn, index);
 -		if (ret)
 +		ret = get_dnode_of_data(&dn, index, ALLOC_NODE);
 +		if (ret) {
 +			mutex_unlock_op(sbi, ilock);
  			break;
 -noalloc:
 +		}
 +
 +		if (dn.data_blkaddr == NULL_ADDR) {
 +			ret = reserve_new_block(&dn);
 +			if (ret) {
 +				f2fs_put_dnode(&dn);
 +				mutex_unlock_op(sbi, ilock);
 +				break;
 +			}
 +		}
 +		f2fs_put_dnode(&dn);
 +		mutex_unlock_op(sbi, ilock);
 +
  		if (pg_start == pg_end)
  			new_size = offset + len;
  		else if (index == pg_start && off_start)
++<<<<<<< HEAD
 +			new_size = (index + 1) << PAGE_CACHE_SHIFT;
 +		else if (index == pg_end)
 +			new_size = (index << PAGE_CACHE_SHIFT) + off_end;
++=======
+ 			new_size = (loff_t)(index + 1) << PAGE_SHIFT;
+ 		else if (index == pg_end)
+ 			new_size = ((loff_t)index << PAGE_SHIFT) +
+ 								off_end;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		else
- 			new_size += PAGE_CACHE_SIZE;
+ 			new_size += PAGE_SIZE;
  	}
  
  	if (!(mode & FALLOC_FL_KEEP_SIZE) &&
@@@ -567,64 -1274,622 +1052,615 @@@ static inline __u32 f2fs_mask_flags(umo
  		return flags & F2FS_OTHER_FLMASK;
  }
  
++<<<<<<< HEAD
++=======
+ static int f2fs_ioc_getflags(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_inode_info *fi = F2FS_I(inode);
+ 	unsigned int flags = fi->i_flags & FS_FL_USER_VISIBLE;
+ 	return put_user(flags, (int __user *)arg);
+ }
+ 
+ static int f2fs_ioc_setflags(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_inode_info *fi = F2FS_I(inode);
+ 	unsigned int flags = fi->i_flags & FS_FL_USER_VISIBLE;
+ 	unsigned int oldflags;
+ 	int ret;
+ 
+ 	ret = mnt_want_write_file(filp);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (!inode_owner_or_capable(inode)) {
+ 		ret = -EACCES;
+ 		goto out;
+ 	}
+ 
+ 	if (get_user(flags, (int __user *)arg)) {
+ 		ret = -EFAULT;
+ 		goto out;
+ 	}
+ 
+ 	flags = f2fs_mask_flags(inode->i_mode, flags);
+ 
+ 	inode_lock(inode);
+ 
+ 	oldflags = fi->i_flags;
+ 
+ 	if ((flags ^ oldflags) & (FS_APPEND_FL | FS_IMMUTABLE_FL)) {
+ 		if (!capable(CAP_LINUX_IMMUTABLE)) {
+ 			inode_unlock(inode);
+ 			ret = -EPERM;
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	flags = flags & FS_FL_USER_MODIFIABLE;
+ 	flags |= oldflags & ~FS_FL_USER_MODIFIABLE;
+ 	fi->i_flags = flags;
+ 	inode_unlock(inode);
+ 
+ 	f2fs_set_inode_flags(inode);
+ 	inode->i_ctime = CURRENT_TIME;
+ 	mark_inode_dirty(inode);
+ out:
+ 	mnt_drop_write_file(filp);
+ 	return ret;
+ }
+ 
+ static int f2fs_ioc_getversion(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 
+ 	return put_user(inode->i_generation, (int __user *)arg);
+ }
+ 
+ static int f2fs_ioc_start_atomic_write(struct file *filp)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	int ret;
+ 
+ 	if (!inode_owner_or_capable(inode))
+ 		return -EACCES;
+ 
+ 	if (f2fs_is_atomic_file(inode))
+ 		return 0;
+ 
+ 	ret = f2fs_convert_inline_inode(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	set_inode_flag(F2FS_I(inode), FI_ATOMIC_FILE);
+ 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+ 
+ 	return 0;
+ }
+ 
+ static int f2fs_ioc_commit_atomic_write(struct file *filp)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	int ret;
+ 
+ 	if (!inode_owner_or_capable(inode))
+ 		return -EACCES;
+ 
+ 	if (f2fs_is_volatile_file(inode))
+ 		return 0;
+ 
+ 	ret = mnt_want_write_file(filp);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (f2fs_is_atomic_file(inode)) {
+ 		clear_inode_flag(F2FS_I(inode), FI_ATOMIC_FILE);
+ 		ret = commit_inmem_pages(inode);
+ 		if (ret) {
+ 			set_inode_flag(F2FS_I(inode), FI_ATOMIC_FILE);
+ 			goto err_out;
+ 		}
+ 	}
+ 
+ 	ret = f2fs_sync_file(filp, 0, LLONG_MAX, 0);
+ err_out:
+ 	mnt_drop_write_file(filp);
+ 	return ret;
+ }
+ 
+ static int f2fs_ioc_start_volatile_write(struct file *filp)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	int ret;
+ 
+ 	if (!inode_owner_or_capable(inode))
+ 		return -EACCES;
+ 
+ 	if (f2fs_is_volatile_file(inode))
+ 		return 0;
+ 
+ 	ret = f2fs_convert_inline_inode(inode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	set_inode_flag(F2FS_I(inode), FI_VOLATILE_FILE);
+ 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+ 	return 0;
+ }
+ 
+ static int f2fs_ioc_release_volatile_write(struct file *filp)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 
+ 	if (!inode_owner_or_capable(inode))
+ 		return -EACCES;
+ 
+ 	if (!f2fs_is_volatile_file(inode))
+ 		return 0;
+ 
+ 	if (!f2fs_is_first_block_written(inode))
+ 		return truncate_partial_data_page(inode, 0, true);
+ 
+ 	return punch_hole(inode, 0, F2FS_BLKSIZE);
+ }
+ 
+ static int f2fs_ioc_abort_volatile_write(struct file *filp)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	int ret;
+ 
+ 	if (!inode_owner_or_capable(inode))
+ 		return -EACCES;
+ 
+ 	ret = mnt_want_write_file(filp);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (f2fs_is_atomic_file(inode)) {
+ 		clear_inode_flag(F2FS_I(inode), FI_ATOMIC_FILE);
+ 		drop_inmem_pages(inode);
+ 	}
+ 	if (f2fs_is_volatile_file(inode)) {
+ 		clear_inode_flag(F2FS_I(inode), FI_VOLATILE_FILE);
+ 		ret = f2fs_sync_file(filp, 0, LLONG_MAX, 0);
+ 	}
+ 
+ 	mnt_drop_write_file(filp);
+ 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+ 	return ret;
+ }
+ 
+ static int f2fs_ioc_shutdown(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	struct super_block *sb = sbi->sb;
+ 	__u32 in;
+ 
+ 	if (!capable(CAP_SYS_ADMIN))
+ 		return -EPERM;
+ 
+ 	if (get_user(in, (__u32 __user *)arg))
+ 		return -EFAULT;
+ 
+ 	switch (in) {
+ 	case F2FS_GOING_DOWN_FULLSYNC:
+ 		sb = freeze_bdev(sb->s_bdev);
+ 		if (sb && !IS_ERR(sb)) {
+ 			f2fs_stop_checkpoint(sbi);
+ 			thaw_bdev(sb->s_bdev, sb);
+ 		}
+ 		break;
+ 	case F2FS_GOING_DOWN_METASYNC:
+ 		/* do checkpoint only */
+ 		f2fs_sync_fs(sb, 1);
+ 		f2fs_stop_checkpoint(sbi);
+ 		break;
+ 	case F2FS_GOING_DOWN_NOSYNC:
+ 		f2fs_stop_checkpoint(sbi);
+ 		break;
+ 	case F2FS_GOING_DOWN_METAFLUSH:
+ 		sync_meta_pages(sbi, META, LONG_MAX);
+ 		f2fs_stop_checkpoint(sbi);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 	f2fs_update_time(sbi, REQ_TIME);
+ 	return 0;
+ }
+ 
+ static int f2fs_ioc_fitrim(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct super_block *sb = inode->i_sb;
+ 	struct request_queue *q = bdev_get_queue(sb->s_bdev);
+ 	struct fstrim_range range;
+ 	int ret;
+ 
+ 	if (!capable(CAP_SYS_ADMIN))
+ 		return -EPERM;
+ 
+ 	if (!blk_queue_discard(q))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (copy_from_user(&range, (struct fstrim_range __user *)arg,
+ 				sizeof(range)))
+ 		return -EFAULT;
+ 
+ 	range.minlen = max((unsigned int)range.minlen,
+ 				q->limits.discard_granularity);
+ 	ret = f2fs_trim_fs(F2FS_SB(sb), &range);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (copy_to_user((struct fstrim_range __user *)arg, &range,
+ 				sizeof(range)))
+ 		return -EFAULT;
+ 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+ 	return 0;
+ }
+ 
+ static bool uuid_is_nonzero(__u8 u[16])
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < 16; i++)
+ 		if (u[i])
+ 			return true;
+ 	return false;
+ }
+ 
+ static int f2fs_ioc_set_encryption_policy(struct file *filp, unsigned long arg)
+ {
+ 	struct fscrypt_policy policy;
+ 	struct inode *inode = file_inode(filp);
+ 
+ 	if (copy_from_user(&policy, (struct fscrypt_policy __user *)arg,
+ 							sizeof(policy)))
+ 		return -EFAULT;
+ 
+ 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+ 	return fscrypt_process_policy(inode, &policy);
+ }
+ 
+ static int f2fs_ioc_get_encryption_policy(struct file *filp, unsigned long arg)
+ {
+ 	struct fscrypt_policy policy;
+ 	struct inode *inode = file_inode(filp);
+ 	int err;
+ 
+ 	err = fscrypt_get_policy(inode, &policy);
+ 	if (err)
+ 		return err;
+ 
+ 	if (copy_to_user((struct fscrypt_policy __user *)arg, &policy, sizeof(policy)))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ 
+ static int f2fs_ioc_get_encryption_pwsalt(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	int err;
+ 
+ 	if (!f2fs_sb_has_crypto(inode->i_sb))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (uuid_is_nonzero(sbi->raw_super->encrypt_pw_salt))
+ 		goto got_it;
+ 
+ 	err = mnt_want_write_file(filp);
+ 	if (err)
+ 		return err;
+ 
+ 	/* update superblock with uuid */
+ 	generate_random_uuid(sbi->raw_super->encrypt_pw_salt);
+ 
+ 	err = f2fs_commit_super(sbi, false);
+ 	if (err) {
+ 		/* undo new data */
+ 		memset(sbi->raw_super->encrypt_pw_salt, 0, 16);
+ 		mnt_drop_write_file(filp);
+ 		return err;
+ 	}
+ 	mnt_drop_write_file(filp);
+ got_it:
+ 	if (copy_to_user((__u8 __user *)arg, sbi->raw_super->encrypt_pw_salt,
+ 									16))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ 
+ static int f2fs_ioc_gc(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	__u32 sync;
+ 
+ 	if (!capable(CAP_SYS_ADMIN))
+ 		return -EPERM;
+ 
+ 	if (get_user(sync, (__u32 __user *)arg))
+ 		return -EFAULT;
+ 
+ 	if (f2fs_readonly(sbi->sb))
+ 		return -EROFS;
+ 
+ 	if (!sync) {
+ 		if (!mutex_trylock(&sbi->gc_mutex))
+ 			return -EBUSY;
+ 	} else {
+ 		mutex_lock(&sbi->gc_mutex);
+ 	}
+ 
+ 	return f2fs_gc(sbi, sync);
+ }
+ 
+ static int f2fs_ioc_write_checkpoint(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 
+ 	if (!capable(CAP_SYS_ADMIN))
+ 		return -EPERM;
+ 
+ 	if (f2fs_readonly(sbi->sb))
+ 		return -EROFS;
+ 
+ 	return f2fs_sync_fs(sbi->sb, 1);
+ }
+ 
+ static int f2fs_defragment_range(struct f2fs_sb_info *sbi,
+ 					struct file *filp,
+ 					struct f2fs_defragment *range)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_map_blocks map = { .m_next_pgofs = NULL };
+ 	struct extent_info ei;
+ 	pgoff_t pg_start, pg_end;
+ 	unsigned int blk_per_seg = sbi->blocks_per_seg;
+ 	unsigned int total = 0, sec_num;
+ 	unsigned int pages_per_sec = sbi->segs_per_sec * blk_per_seg;
+ 	block_t blk_end = 0;
+ 	bool fragmented = false;
+ 	int err;
+ 
+ 	/* if in-place-update policy is enabled, don't waste time here */
+ 	if (need_inplace_update(inode))
+ 		return -EINVAL;
+ 
+ 	pg_start = range->start >> PAGE_SHIFT;
+ 	pg_end = (range->start + range->len) >> PAGE_SHIFT;
+ 
+ 	f2fs_balance_fs(sbi, true);
+ 
+ 	inode_lock(inode);
+ 
+ 	/* writeback all dirty pages in the range */
+ 	err = filemap_write_and_wait_range(inode->i_mapping, range->start,
+ 						range->start + range->len - 1);
+ 	if (err)
+ 		goto out;
+ 
+ 	/*
+ 	 * lookup mapping info in extent cache, skip defragmenting if physical
+ 	 * block addresses are continuous.
+ 	 */
+ 	if (f2fs_lookup_extent_cache(inode, pg_start, &ei)) {
+ 		if (ei.fofs + ei.len >= pg_end)
+ 			goto out;
+ 	}
+ 
+ 	map.m_lblk = pg_start;
+ 
+ 	/*
+ 	 * lookup mapping info in dnode page cache, skip defragmenting if all
+ 	 * physical block addresses are continuous even if there are hole(s)
+ 	 * in logical blocks.
+ 	 */
+ 	while (map.m_lblk < pg_end) {
+ 		map.m_len = pg_end - map.m_lblk;
+ 		err = f2fs_map_blocks(inode, &map, 0, F2FS_GET_BLOCK_READ);
+ 		if (err)
+ 			goto out;
+ 
+ 		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
+ 			map.m_lblk++;
+ 			continue;
+ 		}
+ 
+ 		if (blk_end && blk_end != map.m_pblk) {
+ 			fragmented = true;
+ 			break;
+ 		}
+ 		blk_end = map.m_pblk + map.m_len;
+ 
+ 		map.m_lblk += map.m_len;
+ 	}
+ 
+ 	if (!fragmented)
+ 		goto out;
+ 
+ 	map.m_lblk = pg_start;
+ 	map.m_len = pg_end - pg_start;
+ 
+ 	sec_num = (map.m_len + pages_per_sec - 1) / pages_per_sec;
+ 
+ 	/*
+ 	 * make sure there are enough free section for LFS allocation, this can
+ 	 * avoid defragment running in SSR mode when free section are allocated
+ 	 * intensively
+ 	 */
+ 	if (has_not_enough_free_secs(sbi, sec_num)) {
+ 		err = -EAGAIN;
+ 		goto out;
+ 	}
+ 
+ 	while (map.m_lblk < pg_end) {
+ 		pgoff_t idx;
+ 		int cnt = 0;
+ 
+ do_map:
+ 		map.m_len = pg_end - map.m_lblk;
+ 		err = f2fs_map_blocks(inode, &map, 0, F2FS_GET_BLOCK_READ);
+ 		if (err)
+ 			goto clear_out;
+ 
+ 		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
+ 			map.m_lblk++;
+ 			continue;
+ 		}
+ 
+ 		set_inode_flag(F2FS_I(inode), FI_DO_DEFRAG);
+ 
+ 		idx = map.m_lblk;
+ 		while (idx < map.m_lblk + map.m_len && cnt < blk_per_seg) {
+ 			struct page *page;
+ 
+ 			page = get_lock_data_page(inode, idx, true);
+ 			if (IS_ERR(page)) {
+ 				err = PTR_ERR(page);
+ 				goto clear_out;
+ 			}
+ 
+ 			set_page_dirty(page);
+ 			f2fs_put_page(page, 1);
+ 
+ 			idx++;
+ 			cnt++;
+ 			total++;
+ 		}
+ 
+ 		map.m_lblk = idx;
+ 
+ 		if (idx < pg_end && cnt < blk_per_seg)
+ 			goto do_map;
+ 
+ 		clear_inode_flag(F2FS_I(inode), FI_DO_DEFRAG);
+ 
+ 		err = filemap_fdatawrite(inode->i_mapping);
+ 		if (err)
+ 			goto out;
+ 	}
+ clear_out:
+ 	clear_inode_flag(F2FS_I(inode), FI_DO_DEFRAG);
+ out:
+ 	inode_unlock(inode);
+ 	if (!err)
+ 		range->len = (u64)total << PAGE_SHIFT;
+ 	return err;
+ }
+ 
+ static int f2fs_ioc_defragment(struct file *filp, unsigned long arg)
+ {
+ 	struct inode *inode = file_inode(filp);
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+ 	struct f2fs_defragment range;
+ 	int err;
+ 
+ 	if (!capable(CAP_SYS_ADMIN))
+ 		return -EPERM;
+ 
+ 	if (!S_ISREG(inode->i_mode))
+ 		return -EINVAL;
+ 
+ 	err = mnt_want_write_file(filp);
+ 	if (err)
+ 		return err;
+ 
+ 	if (f2fs_readonly(sbi->sb)) {
+ 		err = -EROFS;
+ 		goto out;
+ 	}
+ 
+ 	if (copy_from_user(&range, (struct f2fs_defragment __user *)arg,
+ 							sizeof(range))) {
+ 		err = -EFAULT;
+ 		goto out;
+ 	}
+ 
+ 	/* verify alignment of offset & size */
+ 	if (range.start & (F2FS_BLKSIZE - 1) ||
+ 		range.len & (F2FS_BLKSIZE - 1)) {
+ 		err = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	err = f2fs_defragment_range(sbi, filp, &range);
+ 	f2fs_update_time(sbi, REQ_TIME);
+ 	if (err < 0)
+ 		goto out;
+ 
+ 	if (copy_to_user((struct f2fs_defragment __user *)arg, &range,
+ 							sizeof(range)))
+ 		err = -EFAULT;
+ out:
+ 	mnt_drop_write_file(filp);
+ 	return err;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
  {
 +	struct inode *inode = file_inode(filp);
 +	struct f2fs_inode_info *fi = F2FS_I(inode);
 +	unsigned int flags;
 +	int ret;
 +
  	switch (cmd) {
 -	case F2FS_IOC_GETFLAGS:
 -		return f2fs_ioc_getflags(filp, arg);
 -	case F2FS_IOC_SETFLAGS:
 -		return f2fs_ioc_setflags(filp, arg);
 -	case F2FS_IOC_GETVERSION:
 -		return f2fs_ioc_getversion(filp, arg);
 -	case F2FS_IOC_START_ATOMIC_WRITE:
 -		return f2fs_ioc_start_atomic_write(filp);
 -	case F2FS_IOC_COMMIT_ATOMIC_WRITE:
 -		return f2fs_ioc_commit_atomic_write(filp);
 -	case F2FS_IOC_START_VOLATILE_WRITE:
 -		return f2fs_ioc_start_volatile_write(filp);
 -	case F2FS_IOC_RELEASE_VOLATILE_WRITE:
 -		return f2fs_ioc_release_volatile_write(filp);
 -	case F2FS_IOC_ABORT_VOLATILE_WRITE:
 -		return f2fs_ioc_abort_volatile_write(filp);
 -	case F2FS_IOC_SHUTDOWN:
 -		return f2fs_ioc_shutdown(filp, arg);
 -	case FITRIM:
 -		return f2fs_ioc_fitrim(filp, arg);
 -	case F2FS_IOC_SET_ENCRYPTION_POLICY:
 -		return f2fs_ioc_set_encryption_policy(filp, arg);
 -	case F2FS_IOC_GET_ENCRYPTION_POLICY:
 -		return f2fs_ioc_get_encryption_policy(filp, arg);
 -	case F2FS_IOC_GET_ENCRYPTION_PWSALT:
 -		return f2fs_ioc_get_encryption_pwsalt(filp, arg);
 -	case F2FS_IOC_GARBAGE_COLLECT:
 -		return f2fs_ioc_gc(filp, arg);
 -	case F2FS_IOC_WRITE_CHECKPOINT:
 -		return f2fs_ioc_write_checkpoint(filp, arg);
 -	case F2FS_IOC_DEFRAGMENT:
 -		return f2fs_ioc_defragment(filp, arg);
 -	default:
 -		return -ENOTTY;
 -	}
 -}
 +	case FS_IOC_GETFLAGS:
 +		flags = fi->i_flags & FS_FL_USER_VISIBLE;
 +		return put_user(flags, (int __user *) arg);
 +	case FS_IOC_SETFLAGS:
 +	{
 +		unsigned int oldflags;
 +
 +		ret = mnt_want_write_file(filp);
 +		if (ret)
 +			return ret;
  
 -static ssize_t f2fs_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 -{
 -	struct file *file = iocb->ki_filp;
 -	struct inode *inode = file_inode(file);
 -	ssize_t ret;
 +		if (!inode_owner_or_capable(inode)) {
 +			ret = -EACCES;
 +			goto out;
 +		}
  
 -	if (f2fs_encrypted_inode(inode) &&
 -				!fscrypt_has_encryption_key(inode) &&
 -				fscrypt_get_encryption_info(inode))
 -		return -EACCES;
 +		if (get_user(flags, (int __user *) arg)) {
 +			ret = -EFAULT;
 +			goto out;
 +		}
  
 -	inode_lock(inode);
 -	ret = generic_write_checks(iocb, from);
 -	if (ret > 0) {
 -		ret = f2fs_preallocate_blocks(iocb, from);
 -		if (!ret)
 -			ret = __generic_file_write_iter(iocb, from);
 -	}
 -	inode_unlock(inode);
 +		flags = f2fs_mask_flags(inode->i_mode, flags);
  
 -	if (ret > 0) {
 -		ssize_t err;
 +		mutex_lock(&inode->i_mutex);
  
 -		err = generic_write_sync(file, iocb->ki_pos - ret, ret);
 -		if (err < 0)
 -			ret = err;
 +		oldflags = fi->i_flags;
 +
 +		if ((flags ^ oldflags) & (FS_APPEND_FL | FS_IMMUTABLE_FL)) {
 +			if (!capable(CAP_LINUX_IMMUTABLE)) {
 +				mutex_unlock(&inode->i_mutex);
 +				ret = -EPERM;
 +				goto out;
 +			}
 +		}
 +
 +		flags = flags & FS_FL_USER_MODIFIABLE;
 +		flags |= oldflags & ~FS_FL_USER_MODIFIABLE;
 +		fi->i_flags = flags;
 +		mutex_unlock(&inode->i_mutex);
 +
 +		f2fs_set_inode_flags(inode);
 +		inode->i_ctime = CURRENT_TIME;
 +		mark_inode_dirty(inode);
 +out:
 +		mnt_drop_write_file(filp);
 +		return ret;
 +	}
 +	default:
 +		return -ENOTTY;
  	}
 -	return ret;
  }
  
  #ifdef CONFIG_COMPAT
diff --cc fs/f2fs/namei.c
index 47abc9722b17,c1d9e9d2cb37..000000000000
--- a/fs/f2fs/namei.c
+++ b/fs/f2fs/namei.c
@@@ -488,6 -812,267 +488,270 @@@ out
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static int f2fs_cross_rename(struct inode *old_dir, struct dentry *old_dentry,
+ 			     struct inode *new_dir, struct dentry *new_dentry)
+ {
+ 	struct f2fs_sb_info *sbi = F2FS_I_SB(old_dir);
+ 	struct inode *old_inode = d_inode(old_dentry);
+ 	struct inode *new_inode = d_inode(new_dentry);
+ 	struct page *old_dir_page, *new_dir_page;
+ 	struct page *old_page, *new_page;
+ 	struct f2fs_dir_entry *old_dir_entry = NULL, *new_dir_entry = NULL;
+ 	struct f2fs_dir_entry *old_entry, *new_entry;
+ 	int old_nlink = 0, new_nlink = 0;
+ 	int err = -ENOENT;
+ 
+ 	if ((f2fs_encrypted_inode(old_dir) || f2fs_encrypted_inode(new_dir)) &&
+ 			(old_dir != new_dir) &&
+ 			(!fscrypt_has_permitted_context(new_dir, old_inode) ||
+ 			 !fscrypt_has_permitted_context(old_dir, new_inode)))
+ 		return -EPERM;
+ 
+ 	old_entry = f2fs_find_entry(old_dir, &old_dentry->d_name, &old_page);
+ 	if (!old_entry)
+ 		goto out;
+ 
+ 	new_entry = f2fs_find_entry(new_dir, &new_dentry->d_name, &new_page);
+ 	if (!new_entry)
+ 		goto out_old;
+ 
+ 	/* prepare for updating ".." directory entry info later */
+ 	if (old_dir != new_dir) {
+ 		if (S_ISDIR(old_inode->i_mode)) {
+ 			err = -EIO;
+ 			old_dir_entry = f2fs_parent_dir(old_inode,
+ 							&old_dir_page);
+ 			if (!old_dir_entry)
+ 				goto out_new;
+ 		}
+ 
+ 		if (S_ISDIR(new_inode->i_mode)) {
+ 			err = -EIO;
+ 			new_dir_entry = f2fs_parent_dir(new_inode,
+ 							&new_dir_page);
+ 			if (!new_dir_entry)
+ 				goto out_old_dir;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * If cross rename between file and directory those are not
+ 	 * in the same directory, we will inc nlink of file's parent
+ 	 * later, so we should check upper boundary of its nlink.
+ 	 */
+ 	if ((!old_dir_entry || !new_dir_entry) &&
+ 				old_dir_entry != new_dir_entry) {
+ 		old_nlink = old_dir_entry ? -1 : 1;
+ 		new_nlink = -old_nlink;
+ 		err = -EMLINK;
+ 		if ((old_nlink > 0 && old_inode->i_nlink >= F2FS_LINK_MAX) ||
+ 			(new_nlink > 0 && new_inode->i_nlink >= F2FS_LINK_MAX))
+ 			goto out_new_dir;
+ 	}
+ 
+ 	f2fs_balance_fs(sbi, true);
+ 
+ 	f2fs_lock_op(sbi);
+ 
+ 	err = update_dent_inode(old_inode, new_inode, &new_dentry->d_name);
+ 	if (err)
+ 		goto out_unlock;
+ 	if (file_enc_name(new_inode))
+ 		file_set_enc_name(old_inode);
+ 
+ 	err = update_dent_inode(new_inode, old_inode, &old_dentry->d_name);
+ 	if (err)
+ 		goto out_undo;
+ 	if (file_enc_name(old_inode))
+ 		file_set_enc_name(new_inode);
+ 
+ 	/* update ".." directory entry info of old dentry */
+ 	if (old_dir_entry)
+ 		f2fs_set_link(old_inode, old_dir_entry, old_dir_page, new_dir);
+ 
+ 	/* update ".." directory entry info of new dentry */
+ 	if (new_dir_entry)
+ 		f2fs_set_link(new_inode, new_dir_entry, new_dir_page, old_dir);
+ 
+ 	/* update directory entry info of old dir inode */
+ 	f2fs_set_link(old_dir, old_entry, old_page, new_inode);
+ 
+ 	down_write(&F2FS_I(old_inode)->i_sem);
+ 	file_lost_pino(old_inode);
+ 	up_write(&F2FS_I(old_inode)->i_sem);
+ 
+ 	update_inode_page(old_inode);
+ 
+ 	old_dir->i_ctime = CURRENT_TIME;
+ 	if (old_nlink) {
+ 		down_write(&F2FS_I(old_dir)->i_sem);
+ 		if (old_nlink < 0)
+ 			drop_nlink(old_dir);
+ 		else
+ 			inc_nlink(old_dir);
+ 		up_write(&F2FS_I(old_dir)->i_sem);
+ 	}
+ 	mark_inode_dirty(old_dir);
+ 	update_inode_page(old_dir);
+ 
+ 	/* update directory entry info of new dir inode */
+ 	f2fs_set_link(new_dir, new_entry, new_page, old_inode);
+ 
+ 	down_write(&F2FS_I(new_inode)->i_sem);
+ 	file_lost_pino(new_inode);
+ 	up_write(&F2FS_I(new_inode)->i_sem);
+ 
+ 	update_inode_page(new_inode);
+ 
+ 	new_dir->i_ctime = CURRENT_TIME;
+ 	if (new_nlink) {
+ 		down_write(&F2FS_I(new_dir)->i_sem);
+ 		if (new_nlink < 0)
+ 			drop_nlink(new_dir);
+ 		else
+ 			inc_nlink(new_dir);
+ 		up_write(&F2FS_I(new_dir)->i_sem);
+ 	}
+ 	mark_inode_dirty(new_dir);
+ 	update_inode_page(new_dir);
+ 
+ 	f2fs_unlock_op(sbi);
+ 
+ 	if (IS_DIRSYNC(old_dir) || IS_DIRSYNC(new_dir))
+ 		f2fs_sync_fs(sbi->sb, 1);
+ 	return 0;
+ out_undo:
+ 	/*
+ 	 * Still we may fail to recover name info of f2fs_inode here
+ 	 * Drop it, once its name is set as encrypted
+ 	 */
+ 	update_dent_inode(old_inode, old_inode, &old_dentry->d_name);
+ out_unlock:
+ 	f2fs_unlock_op(sbi);
+ out_new_dir:
+ 	if (new_dir_entry) {
+ 		f2fs_dentry_kunmap(new_inode, new_dir_page);
+ 		f2fs_put_page(new_dir_page, 0);
+ 	}
+ out_old_dir:
+ 	if (old_dir_entry) {
+ 		f2fs_dentry_kunmap(old_inode, old_dir_page);
+ 		f2fs_put_page(old_dir_page, 0);
+ 	}
+ out_new:
+ 	f2fs_dentry_kunmap(new_dir, new_page);
+ 	f2fs_put_page(new_page, 0);
+ out_old:
+ 	f2fs_dentry_kunmap(old_dir, old_page);
+ 	f2fs_put_page(old_page, 0);
+ out:
+ 	return err;
+ }
+ 
+ static int f2fs_rename2(struct inode *old_dir, struct dentry *old_dentry,
+ 			struct inode *new_dir, struct dentry *new_dentry,
+ 			unsigned int flags)
+ {
+ 	if (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))
+ 		return -EINVAL;
+ 
+ 	if (flags & RENAME_EXCHANGE) {
+ 		return f2fs_cross_rename(old_dir, old_dentry,
+ 					 new_dir, new_dentry);
+ 	}
+ 	/*
+ 	 * VFS has already handled the new dentry existence case,
+ 	 * here, we just deal with "RENAME_NOREPLACE" as regular rename.
+ 	 */
+ 	return f2fs_rename(old_dir, old_dentry, new_dir, new_dentry, flags);
+ }
+ 
+ static const char *f2fs_encrypted_get_link(struct dentry *dentry,
+ 					   struct inode *inode,
+ 					   struct delayed_call *done)
+ {
+ 	struct page *cpage = NULL;
+ 	char *caddr, *paddr = NULL;
+ 	struct fscrypt_str cstr = FSTR_INIT(NULL, 0);
+ 	struct fscrypt_str pstr = FSTR_INIT(NULL, 0);
+ 	struct fscrypt_symlink_data *sd;
+ 	loff_t size = min_t(loff_t, i_size_read(inode), PAGE_SIZE - 1);
+ 	u32 max_size = inode->i_sb->s_blocksize;
+ 	int res;
+ 
+ 	if (!dentry)
+ 		return ERR_PTR(-ECHILD);
+ 
+ 	res = fscrypt_get_encryption_info(inode);
+ 	if (res)
+ 		return ERR_PTR(res);
+ 
+ 	cpage = read_mapping_page(inode->i_mapping, 0, NULL);
+ 	if (IS_ERR(cpage))
+ 		return ERR_CAST(cpage);
+ 	caddr = page_address(cpage);
+ 	caddr[size] = 0;
+ 
+ 	/* Symlink is encrypted */
+ 	sd = (struct fscrypt_symlink_data *)caddr;
+ 	cstr.name = sd->encrypted_path;
+ 	cstr.len = le16_to_cpu(sd->len);
+ 
+ 	/* this is broken symlink case */
+ 	if (unlikely(cstr.len == 0)) {
+ 		res = -ENOENT;
+ 		goto errout;
+ 	}
+ 
+ 	/* this is broken symlink case */
+ 	if (unlikely(cstr.name[0] == 0)) {
+ 		res = -ENOENT;
+ 		goto errout;
+ 	}
+ 
+ 	if ((cstr.len + sizeof(struct fscrypt_symlink_data) - 1) > max_size) {
+ 		/* Symlink data on the disk is corrupted */
+ 		res = -EIO;
+ 		goto errout;
+ 	}
+ 	res = fscrypt_fname_alloc_buffer(inode, cstr.len, &pstr);
+ 	if (res)
+ 		goto errout;
+ 
+ 	res = fscrypt_fname_disk_to_usr(inode, 0, 0, &cstr, &pstr);
+ 	if (res < 0)
+ 		goto errout;
+ 
+ 	paddr = pstr.name;
+ 
+ 	/* Null-terminate the name */
+ 	paddr[res] = '\0';
+ 
+ 	put_page(cpage);
+ 	set_delayed_call(done, kfree_link, paddr);
+ 	return paddr;
+ errout:
+ 	fscrypt_fname_free_buffer(&pstr);
+ 	put_page(cpage);
+ 	return ERR_PTR(res);
+ }
+ 
+ const struct inode_operations f2fs_encrypted_symlink_inode_operations = {
+ 	.readlink       = generic_readlink,
+ 	.get_link       = f2fs_encrypted_get_link,
+ 	.getattr	= f2fs_getattr,
+ 	.setattr	= f2fs_setattr,
+ #ifdef CONFIG_F2FS_FS_XATTR
+ 	.setxattr	= generic_setxattr,
+ 	.getxattr	= generic_getxattr,
+ 	.listxattr	= f2fs_listxattr,
+ 	.removexattr	= generic_removexattr,
+ #endif
+ };
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  const struct inode_operations f2fs_dir_inode_operations = {
  	.create		= f2fs_create,
  	.lookup		= f2fs_lookup,
diff --cc fs/f2fs/node.c
index 3df43b4efd89,1a33de9d84b1..000000000000
--- a/fs/f2fs/node.c
+++ b/fs/f2fs/node.c
@@@ -19,10 -19,63 +19,63 @@@
  #include "f2fs.h"
  #include "node.h"
  #include "segment.h"
 -#include "trace.h"
  #include <trace/events/f2fs.h>
  
 -#define on_build_free_nids(nmi) mutex_is_locked(&nm_i->build_lock)
 -
  static struct kmem_cache *nat_entry_slab;
  static struct kmem_cache *free_nid_slab;
++<<<<<<< HEAD
++=======
+ static struct kmem_cache *nat_entry_set_slab;
+ 
+ bool available_free_memory(struct f2fs_sb_info *sbi, int type)
+ {
+ 	struct f2fs_nm_info *nm_i = NM_I(sbi);
+ 	struct sysinfo val;
+ 	unsigned long avail_ram;
+ 	unsigned long mem_size = 0;
+ 	bool res = false;
+ 
+ 	si_meminfo(&val);
+ 
+ 	/* only uses low memory */
+ 	avail_ram = val.totalram - val.totalhigh;
+ 
+ 	/*
+ 	 * give 25%, 25%, 50%, 50%, 50% memory for each components respectively
+ 	 */
+ 	if (type == FREE_NIDS) {
+ 		mem_size = (nm_i->fcnt * sizeof(struct free_nid)) >>
+ 							PAGE_SHIFT;
+ 		res = mem_size < ((avail_ram * nm_i->ram_thresh / 100) >> 2);
+ 	} else if (type == NAT_ENTRIES) {
+ 		mem_size = (nm_i->nat_cnt * sizeof(struct nat_entry)) >>
+ 							PAGE_SHIFT;
+ 		res = mem_size < ((avail_ram * nm_i->ram_thresh / 100) >> 2);
+ 	} else if (type == DIRTY_DENTS) {
+ 		if (sbi->sb->s_bdi->wb.dirty_exceeded)
+ 			return false;
+ 		mem_size = get_pages(sbi, F2FS_DIRTY_DENTS);
+ 		res = mem_size < ((avail_ram * nm_i->ram_thresh / 100) >> 1);
+ 	} else if (type == INO_ENTRIES) {
+ 		int i;
+ 
+ 		for (i = 0; i <= UPDATE_INO; i++)
+ 			mem_size += (sbi->im[i].ino_num *
+ 				sizeof(struct ino_entry)) >> PAGE_SHIFT;
+ 		res = mem_size < ((avail_ram * nm_i->ram_thresh / 100) >> 1);
+ 	} else if (type == EXTENT_CACHE) {
+ 		mem_size = (atomic_read(&sbi->total_ext_tree) *
+ 				sizeof(struct extent_tree) +
+ 				atomic_read(&sbi->total_ext_node) *
+ 				sizeof(struct extent_node)) >> PAGE_SHIFT;
+ 		res = mem_size < ((avail_ram * nm_i->ram_thresh / 100) >> 1);
+ 	} else {
+ 		if (!sbi->sb->s_bdi->wb.dirty_exceeded)
+ 			return true;
+ 	}
+ 	return res;
+ }
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  static void clear_node_page_dirty(struct page *page)
  {
diff --cc fs/f2fs/recovery.c
index 60c8a5097058,011942f94d64..000000000000
--- a/fs/f2fs/recovery.c
+++ b/fs/f2fs/recovery.c
@@@ -389,14 -579,51 +389,54 @@@ int recover_fsync_data(struct f2fs_sb_i
  	if (list_empty(&inode_list))
  		goto out;
  
 -	need_writecp = true;
 -
  	/* step #2: recover data */
 -	err = recover_data(sbi, &inode_list);
 -	if (!err)
 -		f2fs_bug_on(sbi, !list_empty(&inode_list));
 +	sbi->por_doing = 1;
 +	err = recover_data(sbi, &inode_list, CURSEG_WARM_NODE);
 +	sbi->por_doing = 0;
 +	BUG_ON(!list_empty(&inode_list));
  out:
 -	destroy_fsync_dnodes(&inode_list);
 +	destroy_fsync_dnodes(sbi, &inode_list);
  	kmem_cache_destroy(fsync_entry_slab);
++<<<<<<< HEAD
 +	write_checkpoint(sbi, false);
++=======
+ 
+ 	/* truncate meta pages to be used by the recovery */
+ 	truncate_inode_pages_range(META_MAPPING(sbi),
+ 			(loff_t)MAIN_BLKADDR(sbi) << PAGE_SHIFT, -1);
+ 
+ 	if (err) {
+ 		truncate_inode_pages_final(NODE_MAPPING(sbi));
+ 		truncate_inode_pages_final(META_MAPPING(sbi));
+ 	}
+ 
+ 	clear_sbi_flag(sbi, SBI_POR_DOING);
+ 	if (err) {
+ 		bool invalidate = false;
+ 
+ 		if (discard_next_dnode(sbi, blkaddr))
+ 			invalidate = true;
+ 
+ 		/* Flush all the NAT/SIT pages */
+ 		while (get_pages(sbi, F2FS_DIRTY_META))
+ 			sync_meta_pages(sbi, META, LONG_MAX);
+ 
+ 		/* invalidate temporary meta page */
+ 		if (invalidate)
+ 			invalidate_mapping_pages(META_MAPPING(sbi),
+ 							blkaddr, blkaddr);
+ 
+ 		set_ckpt_flags(sbi->ckpt, CP_ERROR_FLAG);
+ 		mutex_unlock(&sbi->cp_mutex);
+ 	} else if (need_writecp) {
+ 		struct cp_control cpc = {
+ 			.reason = CP_RECOVERY,
+ 		};
+ 		mutex_unlock(&sbi->cp_mutex);
+ 		err = write_checkpoint(sbi, &cpc);
+ 	} else {
+ 		mutex_unlock(&sbi->cp_mutex);
+ 	}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	return err;
  }
diff --cc fs/f2fs/segment.c
index 3f08cfa78afe,540669d6978e..000000000000
--- a/fs/f2fs/segment.c
+++ b/fs/f2fs/segment.c
@@@ -278,17 -876,21 +278,26 @@@ int npages_for_summary_flush(struct f2f
  	for (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {
  		if (sbi->ckpt->alloc_type[i] == SSR)
  			valid_sum_count += sbi->blocks_per_seg;
 -		else {
 -			if (for_ra)
 -				valid_sum_count += le16_to_cpu(
 -					F2FS_CKPT(sbi)->cur_data_blkoff[i]);
 -			else
 -				valid_sum_count += curseg_blkoff(sbi, i);
 -		}
 +		else
 +			valid_sum_count += curseg_blkoff(sbi, i);
  	}
  
++<<<<<<< HEAD
 +	total_size_bytes = valid_sum_count * (SUMMARY_SIZE + 1)
 +			+ sizeof(struct nat_journal) + 2
 +			+ sizeof(struct sit_journal) + 2;
 +	sum_space = PAGE_CACHE_SIZE - SUM_FOOTER_SIZE;
 +	if (total_size_bytes < sum_space)
 +		return 1;
 +	else if (total_size_bytes < 2 * sum_space)
++=======
+ 	sum_in_page = (PAGE_SIZE - 2 * SUM_JOURNAL_SIZE -
+ 			SUM_FOOTER_SIZE) / SUMMARY_SIZE;
+ 	if (valid_sum_count <= sum_in_page)
+ 		return 1;
+ 	else if ((valid_sum_count - sum_in_page) <=
+ 		(PAGE_SIZE - SUM_FOOTER_SIZE) / SUMMARY_SIZE)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		return 2;
  	return 3;
  }
@@@ -301,6 -903,19 +310,22 @@@ struct page *get_sum_page(struct f2fs_s
  	return get_meta_page(sbi, GET_SUM_BLOCK(sbi, segno));
  }
  
++<<<<<<< HEAD
++=======
+ void update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr)
+ {
+ 	struct page *page = grab_meta_page(sbi, blk_addr);
+ 	void *dst = page_address(page);
+ 
+ 	if (src)
+ 		memcpy(dst, src, PAGE_SIZE);
+ 	else
+ 		memset(dst, 0, PAGE_SIZE);
+ 	set_page_dirty(page);
+ 	f2fs_put_page(page, 1);
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  static void write_sum_page(struct f2fs_sb_info *sbi,
  			struct f2fs_summary_block *sum_blk, block_t blk_addr)
  {
@@@ -1185,9 -1756,8 +1210,9 @@@ static void write_compacted_summaries(s
  			summary = (struct f2fs_summary *)(kaddr + written_size);
  			*summary = seg_i->sum_blk->entries[j];
  			written_size += SUMMARY_SIZE;
 +			set_page_dirty(page);
  
- 			if (written_size + SUMMARY_SIZE <= PAGE_CACHE_SIZE -
+ 			if (written_size + SUMMARY_SIZE <= PAGE_SIZE -
  							SUM_FOOTER_SIZE)
  				continue;
  
@@@ -1509,9 -2171,14 +1534,9 @@@ static int build_curseg(struct f2fs_sb_
  
  	for (i = 0; i < NR_CURSEG_TYPE; i++) {
  		mutex_init(&array[i].curseg_mutex);
- 		array[i].sum_blk = kzalloc(PAGE_CACHE_SIZE, GFP_KERNEL);
+ 		array[i].sum_blk = kzalloc(PAGE_SIZE, GFP_KERNEL);
  		if (!array[i].sum_blk)
  			return -ENOMEM;
 -		init_rwsem(&array[i].journal_rwsem);
 -		array[i].journal = kzalloc(sizeof(struct f2fs_journal),
 -							GFP_KERNEL);
 -		if (!array[i].journal)
 -			return -ENOMEM;
  		array[i].segno = NULL_SEGNO;
  		array[i].next_blkoff = 0;
  	}
diff --cc fs/freevxfs/vxfs_lookup.c
index 664b07a53870,a49e0cfbb686..000000000000
--- a/fs/freevxfs/vxfs_lookup.c
+++ b/fs/freevxfs/vxfs_lookup.c
@@@ -300,13 -288,12 +300,20 @@@ vxfs_readdir(struct file *fp, void *ret
  				if (!de->d_ino)
  					continue;
  
++<<<<<<< HEAD
 +				offset = (caddr_t)de - kaddr;
 +				over = filler(retp, de->d_name, de->d_namelen,
 +					((page << PAGE_CACHE_SHIFT) | offset) + 2,
 +					de->d_ino, DT_UNKNOWN);
 +				if (over) {
++=======
+ 				offset = (char *)de - kaddr;
+ 				ctx->pos = ((page << PAGE_SHIFT) | offset) + 2;
+ 				if (!dir_emit(ctx, de->d_name, de->d_namelen,
+ 					de->d_ino, DT_UNKNOWN)) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  					vxfs_put_page(pp);
 -					return 0;
 +					goto done;
  				}
  			}
  			offset = 0;
@@@ -314,9 -301,6 +321,13 @@@
  		vxfs_put_page(pp);
  		offset = 0;
  	}
++<<<<<<< HEAD
 +
 +done:
 +	fp->f_pos = ((page << PAGE_CACHE_SHIFT) | offset) + 2;
 +out:
++=======
+ 	ctx->pos = ((page << PAGE_SHIFT) | offset) + 2;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	return 0;
  }
diff --cc fs/fuse/dev.c
index e5326f13da17,cbece1221417..000000000000
--- a/fs/fuse/dev.c
+++ b/fs/fuse/dev.c
@@@ -1597,12 -1652,13 +1597,17 @@@ static int fuse_notify_store(struct fus
  		if (!page)
  			goto out_iput;
  
- 		this_num = min_t(unsigned, num, PAGE_CACHE_SIZE - offset);
+ 		this_num = min_t(unsigned, num, PAGE_SIZE - offset);
  		err = fuse_copy_page(cs, &page, offset, this_num, 0);
++<<<<<<< HEAD
 +		if (!err && offset == 0 && (num != 0 || file_size == end))
++=======
+ 		if (!err && offset == 0 &&
+ 		    (this_num == PAGE_SIZE || file_size == end))
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			SetPageUptodate(page);
  		unlock_page(page);
- 		page_cache_release(page);
+ 		put_page(page);
  
  		if (err)
  			goto out_iput;
diff --cc fs/fuse/file.c
index ed212b08f7f8,719924d6c706..000000000000
--- a/fs/fuse/file.c
+++ b/fs/fuse/file.c
@@@ -333,8 -348,9 +333,14 @@@ static bool fuse_page_is_writeback(stru
  		pgoff_t curr_index;
  
  		BUG_ON(req->inode != inode);
++<<<<<<< HEAD
 +		curr_index = req->misc.write.in.offset >> PAGE_CACHE_SHIFT;
 +		if (curr_index == index) {
++=======
+ 		curr_index = req->misc.write.in.offset >> PAGE_SHIFT;
+ 		if (idx_from < curr_index + req->num_pages &&
+ 		    curr_index <= idx_to) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			found = true;
  			break;
  		}
@@@ -638,9 -670,35 +644,38 @@@ static void fuse_read_update_size(struc
  	spin_unlock(&fc->lock);
  }
  
 -static void fuse_short_read(struct fuse_req *req, struct inode *inode,
 -			    u64 attr_ver)
 +static int fuse_readpage(struct file *file, struct page *page)
  {
++<<<<<<< HEAD
 +	struct fuse_io_priv io = { .async = 0, .file = file };
++=======
+ 	size_t num_read = req->out.args[0].size;
+ 	struct fuse_conn *fc = get_fuse_conn(inode);
+ 
+ 	if (fc->writeback_cache) {
+ 		/*
+ 		 * A hole in a file. Some data after the hole are in page cache,
+ 		 * but have not reached the client fs yet. So, the hole is not
+ 		 * present there.
+ 		 */
+ 		int i;
+ 		int start_idx = num_read >> PAGE_SHIFT;
+ 		size_t off = num_read & (PAGE_SIZE - 1);
+ 
+ 		for (i = start_idx; i < req->num_pages; i++) {
+ 			zero_user_segment(req->pages[i], off, PAGE_SIZE);
+ 			off = 0;
+ 		}
+ 	} else {
+ 		loff_t pos = page_offset(req->pages[0]) + num_read;
+ 		fuse_read_update_size(inode, pos, attr_ver);
+ 	}
+ }
+ 
+ static int fuse_do_readpage(struct file *file, struct page *page)
+ {
+ 	struct fuse_io_priv io = FUSE_IO_PRIV_SYNC(file);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	struct inode *inode = page->mapping->host;
  	struct fuse_conn *fc = get_fuse_conn(inode);
  	struct fuse_req *req;
@@@ -986,16 -1053,13 +1021,16 @@@ static ssize_t fuse_fill_write_pages(st
  		if (mapping_writably_mapped(mapping))
  			flush_dcache_page(page);
  
 +		pagefault_disable();
  		tmp = iov_iter_copy_from_user_atomic(page, ii, offset, bytes);
 +		pagefault_enable();
  		flush_dcache_page(page);
  
 -		iov_iter_advance(ii, tmp);
 +		mark_page_accessed(page);
 +
  		if (!tmp) {
  			unlock_page(page);
- 			page_cache_release(page);
+ 			put_page(page);
  			bytes = min(bytes, iov_iter_single_seg_count(ii));
  			goto again;
  		}
@@@ -1279,16 -1314,17 +1314,22 @@@ ssize_t fuse_direct_io(struct fuse_io_p
  	struct fuse_conn *fc = ff->fc;
  	size_t nmax = write ? fc->max_write : fc->max_read;
  	loff_t pos = *ppos;
++<<<<<<< HEAD
++=======
+ 	size_t count = iov_iter_count(iter);
+ 	pgoff_t idx_from = pos >> PAGE_SHIFT;
+ 	pgoff_t idx_to = (pos + count - 1) >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	ssize_t res = 0;
  	struct fuse_req *req;
 -	int err = 0;
 +	struct iov_iter ii;
 +
 +	iov_iter_init(&ii, iov, nr_segs, count, 0);
  
  	if (io->async)
 -		req = fuse_get_req_for_background(fc, fuse_iter_npages(iter));
 +		req = fuse_get_req_for_background(fc, fuse_iter_npages(&ii));
  	else
 -		req = fuse_get_req(fc, fuse_iter_npages(iter));
 +		req = fuse_get_req(fc, fuse_iter_npages(&ii));
  	if (IS_ERR(req))
  		return PTR_ERR(req);
  
@@@ -1433,8 -1465,8 +1474,12 @@@ __releases(fc->lock
  __acquires(fc->lock)
  {
  	struct fuse_inode *fi = get_fuse_inode(req->inode);
 +	loff_t size = i_size_read(req->inode);
  	struct fuse_write_in *inarg = &req->misc.write.in;
++<<<<<<< HEAD
++=======
+ 	__u64 data_size = req->num_pages * PAGE_SIZE;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	if (!fc->connected)
  		goto out_free;
@@@ -1564,6 -1685,308 +1609,311 @@@ static int fuse_writepage(struct page *
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ struct fuse_fill_wb_data {
+ 	struct fuse_req *req;
+ 	struct fuse_file *ff;
+ 	struct inode *inode;
+ 	struct page **orig_pages;
+ };
+ 
+ static void fuse_writepages_send(struct fuse_fill_wb_data *data)
+ {
+ 	struct fuse_req *req = data->req;
+ 	struct inode *inode = data->inode;
+ 	struct fuse_conn *fc = get_fuse_conn(inode);
+ 	struct fuse_inode *fi = get_fuse_inode(inode);
+ 	int num_pages = req->num_pages;
+ 	int i;
+ 
+ 	req->ff = fuse_file_get(data->ff);
+ 	spin_lock(&fc->lock);
+ 	list_add_tail(&req->list, &fi->queued_writes);
+ 	fuse_flush_writepages(inode);
+ 	spin_unlock(&fc->lock);
+ 
+ 	for (i = 0; i < num_pages; i++)
+ 		end_page_writeback(data->orig_pages[i]);
+ }
+ 
+ static bool fuse_writepage_in_flight(struct fuse_req *new_req,
+ 				     struct page *page)
+ {
+ 	struct fuse_conn *fc = get_fuse_conn(new_req->inode);
+ 	struct fuse_inode *fi = get_fuse_inode(new_req->inode);
+ 	struct fuse_req *tmp;
+ 	struct fuse_req *old_req;
+ 	bool found = false;
+ 	pgoff_t curr_index;
+ 
+ 	BUG_ON(new_req->num_pages != 0);
+ 
+ 	spin_lock(&fc->lock);
+ 	list_del(&new_req->writepages_entry);
+ 	list_for_each_entry(old_req, &fi->writepages, writepages_entry) {
+ 		BUG_ON(old_req->inode != new_req->inode);
+ 		curr_index = old_req->misc.write.in.offset >> PAGE_SHIFT;
+ 		if (curr_index <= page->index &&
+ 		    page->index < curr_index + old_req->num_pages) {
+ 			found = true;
+ 			break;
+ 		}
+ 	}
+ 	if (!found) {
+ 		list_add(&new_req->writepages_entry, &fi->writepages);
+ 		goto out_unlock;
+ 	}
+ 
+ 	new_req->num_pages = 1;
+ 	for (tmp = old_req; tmp != NULL; tmp = tmp->misc.write.next) {
+ 		BUG_ON(tmp->inode != new_req->inode);
+ 		curr_index = tmp->misc.write.in.offset >> PAGE_SHIFT;
+ 		if (tmp->num_pages == 1 &&
+ 		    curr_index == page->index) {
+ 			old_req = tmp;
+ 		}
+ 	}
+ 
+ 	if (old_req->num_pages == 1 && test_bit(FR_PENDING, &old_req->flags)) {
+ 		struct backing_dev_info *bdi = inode_to_bdi(page->mapping->host);
+ 
+ 		copy_highpage(old_req->pages[0], page);
+ 		spin_unlock(&fc->lock);
+ 
+ 		dec_wb_stat(&bdi->wb, WB_WRITEBACK);
+ 		dec_zone_page_state(page, NR_WRITEBACK_TEMP);
+ 		wb_writeout_inc(&bdi->wb);
+ 		fuse_writepage_free(fc, new_req);
+ 		fuse_request_free(new_req);
+ 		goto out;
+ 	} else {
+ 		new_req->misc.write.next = old_req->misc.write.next;
+ 		old_req->misc.write.next = new_req;
+ 	}
+ out_unlock:
+ 	spin_unlock(&fc->lock);
+ out:
+ 	return found;
+ }
+ 
+ static int fuse_writepages_fill(struct page *page,
+ 		struct writeback_control *wbc, void *_data)
+ {
+ 	struct fuse_fill_wb_data *data = _data;
+ 	struct fuse_req *req = data->req;
+ 	struct inode *inode = data->inode;
+ 	struct fuse_conn *fc = get_fuse_conn(inode);
+ 	struct page *tmp_page;
+ 	bool is_writeback;
+ 	int err;
+ 
+ 	if (!data->ff) {
+ 		err = -EIO;
+ 		data->ff = fuse_write_file_get(fc, get_fuse_inode(inode));
+ 		if (!data->ff)
+ 			goto out_unlock;
+ 	}
+ 
+ 	/*
+ 	 * Being under writeback is unlikely but possible.  For example direct
+ 	 * read to an mmaped fuse file will set the page dirty twice; once when
+ 	 * the pages are faulted with get_user_pages(), and then after the read
+ 	 * completed.
+ 	 */
+ 	is_writeback = fuse_page_is_writeback(inode, page->index);
+ 
+ 	if (req && req->num_pages &&
+ 	    (is_writeback || req->num_pages == FUSE_MAX_PAGES_PER_REQ ||
+ 	     (req->num_pages + 1) * PAGE_SIZE > fc->max_write ||
+ 	     data->orig_pages[req->num_pages - 1]->index + 1 != page->index)) {
+ 		fuse_writepages_send(data);
+ 		data->req = NULL;
+ 	}
+ 	err = -ENOMEM;
+ 	tmp_page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
+ 	if (!tmp_page)
+ 		goto out_unlock;
+ 
+ 	/*
+ 	 * The page must not be redirtied until the writeout is completed
+ 	 * (i.e. userspace has sent a reply to the write request).  Otherwise
+ 	 * there could be more than one temporary page instance for each real
+ 	 * page.
+ 	 *
+ 	 * This is ensured by holding the page lock in page_mkwrite() while
+ 	 * checking fuse_page_is_writeback().  We already hold the page lock
+ 	 * since clear_page_dirty_for_io() and keep it held until we add the
+ 	 * request to the fi->writepages list and increment req->num_pages.
+ 	 * After this fuse_page_is_writeback() will indicate that the page is
+ 	 * under writeback, so we can release the page lock.
+ 	 */
+ 	if (data->req == NULL) {
+ 		struct fuse_inode *fi = get_fuse_inode(inode);
+ 
+ 		err = -ENOMEM;
+ 		req = fuse_request_alloc_nofs(FUSE_MAX_PAGES_PER_REQ);
+ 		if (!req) {
+ 			__free_page(tmp_page);
+ 			goto out_unlock;
+ 		}
+ 
+ 		fuse_write_fill(req, data->ff, page_offset(page), 0);
+ 		req->misc.write.in.write_flags |= FUSE_WRITE_CACHE;
+ 		req->misc.write.next = NULL;
+ 		req->in.argpages = 1;
+ 		__set_bit(FR_BACKGROUND, &req->flags);
+ 		req->num_pages = 0;
+ 		req->end = fuse_writepage_end;
+ 		req->inode = inode;
+ 
+ 		spin_lock(&fc->lock);
+ 		list_add(&req->writepages_entry, &fi->writepages);
+ 		spin_unlock(&fc->lock);
+ 
+ 		data->req = req;
+ 	}
+ 	set_page_writeback(page);
+ 
+ 	copy_highpage(tmp_page, page);
+ 	req->pages[req->num_pages] = tmp_page;
+ 	req->page_descs[req->num_pages].offset = 0;
+ 	req->page_descs[req->num_pages].length = PAGE_SIZE;
+ 
+ 	inc_wb_stat(&inode_to_bdi(inode)->wb, WB_WRITEBACK);
+ 	inc_zone_page_state(tmp_page, NR_WRITEBACK_TEMP);
+ 
+ 	err = 0;
+ 	if (is_writeback && fuse_writepage_in_flight(req, page)) {
+ 		end_page_writeback(page);
+ 		data->req = NULL;
+ 		goto out_unlock;
+ 	}
+ 	data->orig_pages[req->num_pages] = page;
+ 
+ 	/*
+ 	 * Protected by fc->lock against concurrent access by
+ 	 * fuse_page_is_writeback().
+ 	 */
+ 	spin_lock(&fc->lock);
+ 	req->num_pages++;
+ 	spin_unlock(&fc->lock);
+ 
+ out_unlock:
+ 	unlock_page(page);
+ 
+ 	return err;
+ }
+ 
+ static int fuse_writepages(struct address_space *mapping,
+ 			   struct writeback_control *wbc)
+ {
+ 	struct inode *inode = mapping->host;
+ 	struct fuse_fill_wb_data data;
+ 	int err;
+ 
+ 	err = -EIO;
+ 	if (is_bad_inode(inode))
+ 		goto out;
+ 
+ 	data.inode = inode;
+ 	data.req = NULL;
+ 	data.ff = NULL;
+ 
+ 	err = -ENOMEM;
+ 	data.orig_pages = kcalloc(FUSE_MAX_PAGES_PER_REQ,
+ 				  sizeof(struct page *),
+ 				  GFP_NOFS);
+ 	if (!data.orig_pages)
+ 		goto out;
+ 
+ 	err = write_cache_pages(mapping, wbc, fuse_writepages_fill, &data);
+ 	if (data.req) {
+ 		/* Ignore errors if we can write at least one page */
+ 		BUG_ON(!data.req->num_pages);
+ 		fuse_writepages_send(&data);
+ 		err = 0;
+ 	}
+ 	if (data.ff)
+ 		fuse_file_put(data.ff, false);
+ 
+ 	kfree(data.orig_pages);
+ out:
+ 	return err;
+ }
+ 
+ /*
+  * It's worthy to make sure that space is reserved on disk for the write,
+  * but how to implement it without killing performance need more thinking.
+  */
+ static int fuse_write_begin(struct file *file, struct address_space *mapping,
+ 		loff_t pos, unsigned len, unsigned flags,
+ 		struct page **pagep, void **fsdata)
+ {
+ 	pgoff_t index = pos >> PAGE_SHIFT;
+ 	struct fuse_conn *fc = get_fuse_conn(file_inode(file));
+ 	struct page *page;
+ 	loff_t fsize;
+ 	int err = -ENOMEM;
+ 
+ 	WARN_ON(!fc->writeback_cache);
+ 
+ 	page = grab_cache_page_write_begin(mapping, index, flags);
+ 	if (!page)
+ 		goto error;
+ 
+ 	fuse_wait_on_page_writeback(mapping->host, page->index);
+ 
+ 	if (PageUptodate(page) || len == PAGE_SIZE)
+ 		goto success;
+ 	/*
+ 	 * Check if the start this page comes after the end of file, in which
+ 	 * case the readpage can be optimized away.
+ 	 */
+ 	fsize = i_size_read(mapping->host);
+ 	if (fsize <= (pos & PAGE_MASK)) {
+ 		size_t off = pos & ~PAGE_MASK;
+ 		if (off)
+ 			zero_user_segment(page, 0, off);
+ 		goto success;
+ 	}
+ 	err = fuse_do_readpage(file, page);
+ 	if (err)
+ 		goto cleanup;
+ success:
+ 	*pagep = page;
+ 	return 0;
+ 
+ cleanup:
+ 	unlock_page(page);
+ 	put_page(page);
+ error:
+ 	return err;
+ }
+ 
+ static int fuse_write_end(struct file *file, struct address_space *mapping,
+ 		loff_t pos, unsigned len, unsigned copied,
+ 		struct page *page, void *fsdata)
+ {
+ 	struct inode *inode = page->mapping->host;
+ 
+ 	if (!PageUptodate(page)) {
+ 		/* Zero any unwritten bytes at the end of the page */
+ 		size_t endoff = (pos + copied) & ~PAGE_MASK;
+ 		if (endoff)
+ 			zero_user_segment(page, endoff, PAGE_SIZE);
+ 		SetPageUptodate(page);
+ 	}
+ 
+ 	fuse_write_update_size(inode, pos + copied);
+ 	set_page_dirty(page);
+ 	unlock_page(page);
+ 	put_page(page);
+ 
+ 	return copied;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  static int fuse_launder_page(struct page *page)
  {
  	int err = 0;
diff --cc fs/gfs2/aops.c
index 21a8806e14e4,1bbbee945f46..000000000000
--- a/fs/gfs2/aops.c
+++ b/fs/gfs2/aops.c
@@@ -109,9 -109,9 +109,13 @@@ static int gfs2_writepage_common(struc
  	if (current->journal_info)
  		goto redirty;
  	/* Is the page fully outside i_size? (truncate in progress) */
- 	offset = i_size & (PAGE_CACHE_SIZE-1);
+ 	offset = i_size & (PAGE_SIZE-1);
  	if (page->index > end_index || (page->index == end_index && !offset)) {
++<<<<<<< HEAD
 +		page->mapping->a_ops->invalidatepage(page, 0);
++=======
+ 		page->mapping->a_ops->invalidatepage(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		goto out;
  	}
  	return 1;
@@@ -623,8 -577,7 +627,12 @@@ int gfs2_internal_read(struct gfs2_inod
  		p = kmap_atomic(page);
  		memcpy(buf + copied, p + offset, amt);
  		kunmap_atomic(p);
++<<<<<<< HEAD
 +		mark_page_accessed(page);
 +		page_cache_release(page);
++=======
+ 		put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		copied += amt;
  		index++;
  		offset = 0;
@@@ -1030,9 -987,12 +1038,14 @@@ static void gfs2_discard(struct gfs2_sb
  	unlock_buffer(bh);
  }
  
 -static void gfs2_invalidatepage(struct page *page, unsigned int offset,
 -				unsigned int length)
 +static void gfs2_invalidatepage(struct page *page, unsigned long offset)
  {
  	struct gfs2_sbd *sdp = GFS2_SB(page->mapping->host);
++<<<<<<< HEAD
++=======
+ 	unsigned int stop = offset + length;
+ 	int partial_page = (offset || length < PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	struct buffer_head *bh, *head;
  	unsigned long pos = 0;
  
@@@ -1121,8 -1082,8 +1134,13 @@@ static ssize_t gfs2_direct_IO(int rw, s
  	 * the first place, mapping->nr_pages will always be zero.
  	 */
  	if (mapping->nrpages) {
++<<<<<<< HEAD
 +		loff_t lstart = offset & ~(PAGE_CACHE_SIZE - 1);
 +		loff_t len = iov_length(iov, nr_segs);
++=======
+ 		loff_t lstart = offset & ~(PAGE_SIZE - 1);
+ 		loff_t len = iov_iter_count(iter);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		loff_t end = PAGE_ALIGN(offset + len) - 1;
  
  		rv = 0;
diff --cc fs/gfs2/meta_io.c
index fbbe1b648fd3,0448524c11bc..000000000000
--- a/fs/gfs2/meta_io.c
+++ b/fs/gfs2/meta_io.c
@@@ -134,7 -121,10 +134,14 @@@ struct buffer_head *gfs2_getbuf(struct 
  	unsigned long index;
  	unsigned int bufnum;
  
++<<<<<<< HEAD
 +	shift = PAGE_CACHE_SHIFT - sdp->sd_sb.sb_bsize_shift;
++=======
+ 	if (mapping == NULL)
+ 		mapping = &sdp->sd_aspace;
+ 
+ 	shift = PAGE_SHIFT - sdp->sd_sb.sb_bsize_shift;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	index = blkno >> shift;             /* convert block to page */
  	bufnum = blkno - (index << shift);  /* block buf index within page */
  
@@@ -163,8 -154,7 +170,12 @@@
  		map_bh(bh, sdp->sd_vfs, blkno);
  
  	unlock_page(page);
++<<<<<<< HEAD
 +	mark_page_accessed(page);
 +	page_cache_release(page);
++=======
+ 	put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	return bh;
  }
diff --cc fs/gfs2/rgrp.c
index 1e6a443a7c59,99a0bdac8796..000000000000
--- a/fs/gfs2/rgrp.c
+++ b/fs/gfs2/rgrp.c
@@@ -884,6 -917,9 +884,12 @@@ static int read_rindex_entry(struct gfs
  	if (error)
  		goto fail;
  
++<<<<<<< HEAD
++=======
+ 	rgd->rd_gl->gl_object = rgd;
+ 	rgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;
+ 	rgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr + rgd->rd_length) * bsize) - 1;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	rgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;
  	rgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);
  	if (rgd->rd_data > sdp->sd_max_rg_data)
diff --cc fs/hfs/bnode.c
index d3fa6bd9503e,d77d844b668b..000000000000
--- a/fs/hfs/bnode.c
+++ b/fs/hfs/bnode.c
@@@ -285,10 -285,9 +285,10 @@@ static struct hfs_bnode *__hfs_bnode_cr
  		if (IS_ERR(page))
  			goto fail;
  		if (PageError(page)) {
- 			page_cache_release(page);
+ 			put_page(page);
  			goto fail;
  		}
 +		page_cache_release(page);
  		node->page[i] = page;
  	}
  
@@@ -398,11 -397,11 +398,17 @@@ node_error
  
  void hfs_bnode_free(struct hfs_bnode *node)
  {
 -	int i;
 +	//int i;
  
++<<<<<<< HEAD
 +	//for (i = 0; i < node->tree->pages_per_bnode; i++)
 +	//	if (node->page[i])
 +	//		page_cache_release(node->page[i]);
++=======
+ 	for (i = 0; i < node->tree->pages_per_bnode; i++)
+ 		if (node->page[i])
+ 			put_page(node->page[i]);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	kfree(node);
  }
  
diff --cc fs/hfsplus/bnode.c
index 11c860204520,ce014ceb89ef..000000000000
--- a/fs/hfsplus/bnode.c
+++ b/fs/hfsplus/bnode.c
@@@ -24,16 -24,16 +24,24 @@@ void hfs_bnode_read(struct hfs_bnode *n
  	int l;
  
  	off += node->page_offset;
- 	pagep = node->page + (off >> PAGE_CACHE_SHIFT);
- 	off &= ~PAGE_CACHE_MASK;
+ 	pagep = node->page + (off >> PAGE_SHIFT);
+ 	off &= ~PAGE_MASK;
  
++<<<<<<< HEAD
 +	l = min(len, (int)PAGE_CACHE_SIZE - off);
++=======
+ 	l = min_t(int, len, PAGE_SIZE - off);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	memcpy(buf, kmap(*pagep) + off, l);
  	kunmap(*pagep);
  
  	while ((len -= l) != 0) {
  		buf += l;
++<<<<<<< HEAD
 +		l = min(len, (int)PAGE_CACHE_SIZE);
++=======
+ 		l = min_t(int, len, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		memcpy(buf, kmap(*++pagep), l);
  		kunmap(*pagep);
  	}
@@@ -77,17 -77,17 +85,25 @@@ void hfs_bnode_write(struct hfs_bnode *
  	int l;
  
  	off += node->page_offset;
- 	pagep = node->page + (off >> PAGE_CACHE_SHIFT);
- 	off &= ~PAGE_CACHE_MASK;
+ 	pagep = node->page + (off >> PAGE_SHIFT);
+ 	off &= ~PAGE_MASK;
  
++<<<<<<< HEAD
 +	l = min(len, (int)PAGE_CACHE_SIZE - off);
++=======
+ 	l = min_t(int, len, PAGE_SIZE - off);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	memcpy(kmap(*pagep) + off, buf, l);
  	set_page_dirty(*pagep);
  	kunmap(*pagep);
  
  	while ((len -= l) != 0) {
  		buf += l;
++<<<<<<< HEAD
 +		l = min(len, (int)PAGE_CACHE_SIZE);
++=======
+ 		l = min_t(int, len, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		memcpy(kmap(*++pagep), buf, l);
  		set_page_dirty(*pagep);
  		kunmap(*pagep);
@@@ -107,16 -107,16 +123,24 @@@ void hfs_bnode_clear(struct hfs_bnode *
  	int l;
  
  	off += node->page_offset;
- 	pagep = node->page + (off >> PAGE_CACHE_SHIFT);
- 	off &= ~PAGE_CACHE_MASK;
+ 	pagep = node->page + (off >> PAGE_SHIFT);
+ 	off &= ~PAGE_MASK;
  
++<<<<<<< HEAD
 +	l = min(len, (int)PAGE_CACHE_SIZE - off);
++=======
+ 	l = min_t(int, len, PAGE_SIZE - off);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	memset(kmap(*pagep) + off, 0, l);
  	set_page_dirty(*pagep);
  	kunmap(*pagep);
  
  	while ((len -= l) != 0) {
++<<<<<<< HEAD
 +		l = min(len, (int)PAGE_CACHE_SIZE);
++=======
+ 		l = min_t(int, len, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		memset(kmap(*++pagep), 0, l);
  		set_page_dirty(*pagep);
  		kunmap(*pagep);
@@@ -136,20 -136,20 +160,28 @@@ void hfs_bnode_copy(struct hfs_bnode *d
  	tree = src_node->tree;
  	src += src_node->page_offset;
  	dst += dst_node->page_offset;
- 	src_page = src_node->page + (src >> PAGE_CACHE_SHIFT);
- 	src &= ~PAGE_CACHE_MASK;
- 	dst_page = dst_node->page + (dst >> PAGE_CACHE_SHIFT);
- 	dst &= ~PAGE_CACHE_MASK;
+ 	src_page = src_node->page + (src >> PAGE_SHIFT);
+ 	src &= ~PAGE_MASK;
+ 	dst_page = dst_node->page + (dst >> PAGE_SHIFT);
+ 	dst &= ~PAGE_MASK;
  
  	if (src == dst) {
++<<<<<<< HEAD
 +		l = min(len, (int)PAGE_CACHE_SIZE - src);
++=======
+ 		l = min_t(int, len, PAGE_SIZE - src);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		memcpy(kmap(*dst_page) + src, kmap(*src_page) + src, l);
  		kunmap(*src_page);
  		set_page_dirty(*dst_page);
  		kunmap(*dst_page);
  
  		while ((len -= l) != 0) {
++<<<<<<< HEAD
 +			l = min(len, (int)PAGE_CACHE_SIZE);
++=======
+ 			l = min_t(int, len, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			memcpy(kmap(*++dst_page), kmap(*++src_page), l);
  			kunmap(*src_page);
  			set_page_dirty(*dst_page);
@@@ -245,13 -245,13 +277,17 @@@ void hfs_bnode_move(struct hfs_bnode *n
  			} while ((len -= l));
  		}
  	} else {
- 		src_page = node->page + (src >> PAGE_CACHE_SHIFT);
- 		src &= ~PAGE_CACHE_MASK;
- 		dst_page = node->page + (dst >> PAGE_CACHE_SHIFT);
- 		dst &= ~PAGE_CACHE_MASK;
+ 		src_page = node->page + (src >> PAGE_SHIFT);
+ 		src &= ~PAGE_MASK;
+ 		dst_page = node->page + (dst >> PAGE_SHIFT);
+ 		dst &= ~PAGE_MASK;
  
  		if (src == dst) {
++<<<<<<< HEAD
 +			l = min(len, (int)PAGE_CACHE_SIZE - src);
++=======
+ 			l = min_t(int, len, PAGE_SIZE - src);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			memmove(kmap(*dst_page) + src,
  				kmap(*src_page) + src, l);
  			kunmap(*src_page);
@@@ -259,7 -259,7 +295,11 @@@
  			kunmap(*dst_page);
  
  			while ((len -= l) != 0) {
++<<<<<<< HEAD
 +				l = min(len, (int)PAGE_CACHE_SIZE);
++=======
+ 				l = min_t(int, len, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				memmove(kmap(*++dst_page),
  					kmap(*++src_page), l);
  				kunmap(*src_page);
@@@ -453,10 -451,9 +493,10 @@@ static struct hfs_bnode *__hfs_bnode_cr
  		if (IS_ERR(page))
  			goto fail;
  		if (PageError(page)) {
- 			page_cache_release(page);
+ 			put_page(page);
  			goto fail;
  		}
 +		page_cache_release(page);
  		node->page[i] = page;
  	}
  
@@@ -573,8 -569,7 +613,12 @@@ void hfs_bnode_free(struct hfs_bnode *n
  
  	for (i = 0; i < node->tree->pages_per_bnode; i++)
  		if (node->page[i])
++<<<<<<< HEAD
 +			page_cache_release(node->page[i]);
 +#endif
++=======
+ 			put_page(node->page[i]);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	kfree(node);
  }
  
@@@ -602,7 -597,7 +646,11 @@@ struct hfs_bnode *hfs_bnode_create(stru
  
  	pagep = node->page;
  	memset(kmap(*pagep) + node->page_offset, 0,
++<<<<<<< HEAD
 +	       min((int)PAGE_CACHE_SIZE, (int)tree->node_size));
++=======
+ 	       min_t(int, PAGE_SIZE, tree->node_size));
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	set_page_dirty(*pagep);
  	kunmap(*pagep);
  	for (i = 1; i < tree->pages_per_bnode; i++) {
diff --cc fs/hfsplus/xattr.c
index f66346155df5,70e445ff0cff..000000000000
--- a/fs/hfsplus/xattr.c
+++ b/fs/hfsplus/xattr.c
@@@ -46,33 -55,209 +46,123 @@@ static inline int is_known_namespace(co
  	return true;
  }
  
 -static void hfsplus_init_header_node(struct inode *attr_file,
 -					u32 clump_size,
 -					char *buf, u16 node_size)
 +static int can_set_xattr(struct inode *inode, const char *name,
 +				const void *value, size_t value_len)
  {
 -	struct hfs_bnode_desc *desc;
 -	struct hfs_btree_header_rec *head;
 -	u16 offset;
 -	__be16 *rec_offsets;
 -	u32 hdr_node_map_rec_bits;
 -	char *bmp;
 -	u32 used_nodes;
 -	u32 used_bmp_bytes;
 -	u64 tmp;
 -
 -	hfs_dbg(ATTR_MOD, "init_hdr_attr_file: clump %u, node_size %u\n",
 -		clump_size, node_size);
 -
 -	/* The end of the node contains list of record offsets */
 -	rec_offsets = (__be16 *)(buf + node_size);
 -
 -	desc = (struct hfs_bnode_desc *)buf;
 -	desc->type = HFS_NODE_HEADER;
 -	desc->num_recs = cpu_to_be16(HFSPLUS_BTREE_HDR_NODE_RECS_COUNT);
 -	offset = sizeof(struct hfs_bnode_desc);
 -	*--rec_offsets = cpu_to_be16(offset);
 -
 -	head = (struct hfs_btree_header_rec *)(buf + offset);
 -	head->node_size = cpu_to_be16(node_size);
 -	tmp = i_size_read(attr_file);
 -	do_div(tmp, node_size);
 -	head->node_count = cpu_to_be32(tmp);
 -	head->free_nodes = cpu_to_be32(be32_to_cpu(head->node_count) - 1);
 -	head->clump_size = cpu_to_be32(clump_size);
 -	head->attributes |= cpu_to_be32(HFS_TREE_BIGKEYS | HFS_TREE_VARIDXKEYS);
 -	head->max_key_len = cpu_to_be16(HFSPLUS_ATTR_KEYLEN - sizeof(u16));
 -	offset += sizeof(struct hfs_btree_header_rec);
 -	*--rec_offsets = cpu_to_be16(offset);
 -	offset += HFSPLUS_BTREE_HDR_USER_BYTES;
 -	*--rec_offsets = cpu_to_be16(offset);
 -
 -	hdr_node_map_rec_bits = 8 * (node_size - offset - (4 * sizeof(u16)));
 -	if (be32_to_cpu(head->node_count) > hdr_node_map_rec_bits) {
 -		u32 map_node_bits;
 -		u32 map_nodes;
 -
 -		desc->next = cpu_to_be32(be32_to_cpu(head->leaf_tail) + 1);
 -		map_node_bits = 8 * (node_size - sizeof(struct hfs_bnode_desc) -
 -					(2 * sizeof(u16)) - 2);
 -		map_nodes = (be32_to_cpu(head->node_count) -
 -				hdr_node_map_rec_bits +
 -				(map_node_bits - 1)) / map_node_bits;
 -		be32_add_cpu(&head->free_nodes, 0 - map_nodes);
 -	}
 -
 -	bmp = buf + offset;
 -	used_nodes =
 -		be32_to_cpu(head->node_count) - be32_to_cpu(head->free_nodes);
 -	used_bmp_bytes = used_nodes / 8;
 -	if (used_bmp_bytes) {
 -		memset(bmp, 0xFF, used_bmp_bytes);
 -		bmp += used_bmp_bytes;
 -		used_nodes %= 8;
 -	}
 -	*bmp = ~(0xFF >> used_nodes);
 -	offset += hdr_node_map_rec_bits / 8;
 -	*--rec_offsets = cpu_to_be16(offset);
 -}
 +	if (!strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))
 +		return -EOPNOTSUPP; /* TODO: implement ACL support */
  
 -static int hfsplus_create_attributes_file(struct super_block *sb)
 -{
 -	int err = 0;
 -	struct hfsplus_sb_info *sbi = HFSPLUS_SB(sb);
 -	struct inode *attr_file;
 -	struct hfsplus_inode_info *hip;
 -	u32 clump_size;
 -	u16 node_size = HFSPLUS_ATTR_TREE_NODE_SIZE;
 -	char *buf;
 -	int index, written;
 -	struct address_space *mapping;
 -	struct page *page;
 -	int old_state = HFSPLUS_EMPTY_ATTR_TREE;
 -
 -	hfs_dbg(ATTR_MOD, "create_attr_file: ino %d\n", HFSPLUS_ATTR_CNID);
 -
 -check_attr_tree_state_again:
 -	switch (atomic_read(&sbi->attr_tree_state)) {
 -	case HFSPLUS_EMPTY_ATTR_TREE:
 -		if (old_state != atomic_cmpxchg(&sbi->attr_tree_state,
 -						old_state,
 -						HFSPLUS_CREATING_ATTR_TREE))
 -			goto check_attr_tree_state_again;
 -		break;
 -	case HFSPLUS_CREATING_ATTR_TREE:
 +	if (!strncmp(name, XATTR_MAC_OSX_PREFIX, XATTR_MAC_OSX_PREFIX_LEN)) {
  		/*
 -		 * This state means that another thread is in process
 -		 * of AttributesFile creation. Theoretically, it is
 -		 * possible to be here. But really __setxattr() method
 -		 * first of all calls hfs_find_init() for lookup in
 -		 * B-tree of CatalogFile. This method locks mutex of
 -		 * CatalogFile's B-tree. As a result, if some thread
 -		 * is inside AttributedFile creation operation then
 -		 * another threads will be waiting unlocking of
 -		 * CatalogFile's B-tree's mutex. However, if code will
 -		 * change then we will return error code (-EAGAIN) from
 -		 * here. Really, it means that first try to set of xattr
 -		 * fails with error but second attempt will have success.
 +		 * This makes sure that we aren't trying to set an
 +		 * attribute in a different namespace by prefixing it
 +		 * with "osx."
  		 */
 -		return -EAGAIN;
 -	case HFSPLUS_VALID_ATTR_TREE:
 +		if (is_known_namespace(name + XATTR_MAC_OSX_PREFIX_LEN))
 +			return -EOPNOTSUPP;
 +
  		return 0;
 -	case HFSPLUS_FAILED_ATTR_TREE:
 -		return -EOPNOTSUPP;
 -	default:
 -		BUG();
  	}
  
 +	/*
 +	 * Don't allow setting an attribute in an unknown namespace.
 +	 */
 +	if (strncmp(name, XATTR_TRUSTED_PREFIX, XATTR_TRUSTED_PREFIX_LEN) &&
 +	    strncmp(name, XATTR_SECURITY_PREFIX, XATTR_SECURITY_PREFIX_LEN) &&
 +	    strncmp(name, XATTR_USER_PREFIX, XATTR_USER_PREFIX_LEN))
 +		return -EOPNOTSUPP;
 +
++<<<<<<< HEAD
 +	return 0;
++=======
+ 	attr_file = hfsplus_iget(sb, HFSPLUS_ATTR_CNID);
+ 	if (IS_ERR(attr_file)) {
+ 		pr_err("failed to load attributes file\n");
+ 		return PTR_ERR(attr_file);
+ 	}
+ 
+ 	BUG_ON(i_size_read(attr_file) != 0);
+ 
+ 	hip = HFSPLUS_I(attr_file);
+ 
+ 	clump_size = hfsplus_calc_btree_clump_size(sb->s_blocksize,
+ 						    node_size,
+ 						    sbi->sect_count,
+ 						    HFSPLUS_ATTR_CNID);
+ 
+ 	mutex_lock(&hip->extents_lock);
+ 	hip->clump_blocks = clump_size >> sbi->alloc_blksz_shift;
+ 	mutex_unlock(&hip->extents_lock);
+ 
+ 	if (sbi->free_blocks <= (hip->clump_blocks << 1)) {
+ 		err = -ENOSPC;
+ 		goto end_attr_file_creation;
+ 	}
+ 
+ 	while (hip->alloc_blocks < hip->clump_blocks) {
+ 		err = hfsplus_file_extend(attr_file, false);
+ 		if (unlikely(err)) {
+ 			pr_err("failed to extend attributes file\n");
+ 			goto end_attr_file_creation;
+ 		}
+ 		hip->phys_size = attr_file->i_size =
+ 			(loff_t)hip->alloc_blocks << sbi->alloc_blksz_shift;
+ 		hip->fs_blocks = hip->alloc_blocks << sbi->fs_shift;
+ 		inode_set_bytes(attr_file, attr_file->i_size);
+ 	}
+ 
+ 	buf = kzalloc(node_size, GFP_NOFS);
+ 	if (!buf) {
+ 		pr_err("failed to allocate memory for header node\n");
+ 		err = -ENOMEM;
+ 		goto end_attr_file_creation;
+ 	}
+ 
+ 	hfsplus_init_header_node(attr_file, clump_size, buf, node_size);
+ 
+ 	mapping = attr_file->i_mapping;
+ 
+ 	index = 0;
+ 	written = 0;
+ 	for (; written < node_size; index++, written += PAGE_SIZE) {
+ 		void *kaddr;
+ 
+ 		page = read_mapping_page(mapping, index, NULL);
+ 		if (IS_ERR(page)) {
+ 			err = PTR_ERR(page);
+ 			goto failed_header_node_init;
+ 		}
+ 
+ 		kaddr = kmap_atomic(page);
+ 		memcpy(kaddr, buf + written,
+ 			min_t(size_t, PAGE_SIZE, node_size - written));
+ 		kunmap_atomic(kaddr);
+ 
+ 		set_page_dirty(page);
+ 		put_page(page);
+ 	}
+ 
+ 	hfsplus_mark_inode_dirty(attr_file, HFSPLUS_I_ATTR_DIRTY);
+ 
+ 	sbi->attr_tree = hfs_btree_open(sb, HFSPLUS_ATTR_CNID);
+ 	if (!sbi->attr_tree)
+ 		pr_err("failed to load attributes file\n");
+ 
+ failed_header_node_init:
+ 	kfree(buf);
+ 
+ end_attr_file_creation:
+ 	iput(attr_file);
+ 
+ 	if (!err)
+ 		atomic_set(&sbi->attr_tree_state, HFSPLUS_VALID_ATTR_TREE);
+ 	else if (err == -ENOSPC)
+ 		atomic_set(&sbi->attr_tree_state, HFSPLUS_EMPTY_ATTR_TREE);
+ 	else
+ 		atomic_set(&sbi->attr_tree_state, HFSPLUS_FAILED_ATTR_TREE);
+ 
+ 	return err;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  int __hfsplus_setxattr(struct inode *inode, const char *name,
diff --cc fs/hostfs/hostfs_kern.c
index 208e6f79ce7a,7016653f3e41..000000000000
--- a/fs/hostfs/hostfs_kern.c
+++ b/fs/hostfs/hostfs_kern.c
@@@ -402,16 -409,15 +402,22 @@@ int hostfs_writepage(struct page *page
  	struct address_space *mapping = page->mapping;
  	struct inode *inode = mapping->host;
  	char *buffer;
++<<<<<<< HEAD
 +	unsigned long long base;
 +	int count = PAGE_CACHE_SIZE;
 +	int end_index = inode->i_size >> PAGE_CACHE_SHIFT;
++=======
+ 	loff_t base = page_offset(page);
+ 	int count = PAGE_SIZE;
+ 	int end_index = inode->i_size >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	int err;
  
  	if (page->index >= end_index)
- 		count = inode->i_size & (PAGE_CACHE_SIZE-1);
+ 		count = inode->i_size & (PAGE_SIZE-1);
  
  	buffer = kmap(page);
 +	base = ((unsigned long long) page->index) << PAGE_CACHE_SHIFT;
  
  	err = write_file(HOSTFS_I(inode)->fd, &base, buffer, count);
  	if (err != count) {
@@@ -433,36 -439,39 +439,49 @@@
  	return err;
  }
  
 -static int hostfs_readpage(struct file *file, struct page *page)
 +int hostfs_readpage(struct file *file, struct page *page)
  {
  	char *buffer;
 -	loff_t start = page_offset(page);
 -	int bytes_read, ret = 0;
 +	long long start;
 +	int err = 0;
  
 +	start = (long long) page->index << PAGE_CACHE_SHIFT;
  	buffer = kmap(page);
++<<<<<<< HEAD
 +	err = read_file(FILE_HOSTFS_I(file)->fd, &start, buffer,
 +			PAGE_CACHE_SIZE);
 +	if (err < 0)
++=======
+ 	bytes_read = read_file(FILE_HOSTFS_I(file)->fd, &start, buffer,
+ 			PAGE_SIZE);
+ 	if (bytes_read < 0) {
+ 		ClearPageUptodate(page);
+ 		SetPageError(page);
+ 		ret = bytes_read;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		goto out;
 -	}
  
++<<<<<<< HEAD
 +	memset(&buffer[err], 0, PAGE_CACHE_SIZE - err);
++=======
+ 	memset(buffer + bytes_read, 0, PAGE_SIZE - bytes_read);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 -	ClearPageError(page);
 +	flush_dcache_page(page);
  	SetPageUptodate(page);
 -
 +	if (PageError(page)) ClearPageError(page);
 +	err = 0;
   out:
 -	flush_dcache_page(page);
  	kunmap(page);
  	unlock_page(page);
 -	return ret;
 +	return err;
  }
  
 -static int hostfs_write_begin(struct file *file, struct address_space *mapping,
 -			      loff_t pos, unsigned len, unsigned flags,
 -			      struct page **pagep, void **fsdata)
 +int hostfs_write_begin(struct file *file, struct address_space *mapping,
 +			loff_t pos, unsigned len, unsigned flags,
 +			struct page **pagep, void **fsdata)
  {
- 	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
+ 	pgoff_t index = pos >> PAGE_SHIFT;
  
  	*pagep = grab_cache_page_write_begin(mapping, index, flags);
  	if (!*pagep)
diff --cc fs/hugetlbfs/inode.c
index 7b7a94a0485b,afb7c7f05de5..000000000000
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@@ -183,24 -205,20 +183,29 @@@ hugetlb_get_unmapped_area(struct file *
  }
  #endif
  
 -static size_t
 +static int
  hugetlbfs_read_actor(struct page *page, unsigned long offset,
 -			struct iov_iter *to, unsigned long size)
 +			char __user *buf, unsigned long count,
 +			unsigned long size)
  {
 -	size_t copied = 0;
 +	char *kaddr;
 +	unsigned long left, copied = 0;
  	int i, chunksize;
  
 +	if (size > count)
 +		size = count;
 +
  	/* Find which 4k chunk and offset with in that chunk */
- 	i = offset >> PAGE_CACHE_SHIFT;
- 	offset = offset & ~PAGE_CACHE_MASK;
+ 	i = offset >> PAGE_SHIFT;
+ 	offset = offset & ~PAGE_MASK;
  
  	while (size) {
++<<<<<<< HEAD
 +		chunksize = PAGE_CACHE_SIZE;
++=======
+ 		size_t n;
+ 		chunksize = PAGE_SIZE;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		if (offset)
  			chunksize -= offset;
  		if (chunksize > size)
@@@ -280,28 -284,20 +285,33 @@@ static ssize_t hugetlbfs_read(struct fi
  			/*
  			 * We have the page, copy it to user space buffer.
  			 */
++<<<<<<< HEAD
 +			ra = hugetlbfs_read_actor(page, offset, buf, len, nr);
 +			ret = ra;
 +			page_cache_release(page);
++=======
+ 			copied = hugetlbfs_read_actor(page, offset, to, nr);
+ 			put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
 -		offset += copied;
 -		retval += copied;
 -		if (copied != nr && iov_iter_count(to)) {
 -			if (!retval)
 -				retval = -EFAULT;
 -			break;
 +		if (ra < 0) {
 +			if (retval == 0)
 +				retval = ra;
 +			goto out;
  		}
 +
 +		offset += ret;
 +		retval += ret;
 +		len -= ret;
  		index += offset >> huge_page_shift(h);
  		offset &= ~huge_page_mask(h);
 +
 +		/* short read or no more work */
 +		if ((ret != nr) || (len == 0))
 +			break;
  	}
 -	iocb->ki_pos = ((loff_t)index << huge_page_shift(h)) + offset;
 +out:
 +	*ppos = ((loff_t)index << huge_page_shift(h)) + offset;
  	return retval;
  }
  
diff --cc fs/jffs2/file.c
index 1506673c087e,0e62dec3effc..000000000000
--- a/fs/jffs2/file.c
+++ b/fs/jffs2/file.c
@@@ -138,35 -138,23 +139,40 @@@ static int jffs2_write_begin(struct fil
  	struct page *pg;
  	struct inode *inode = mapping->host;
  	struct jffs2_inode_info *f = JFFS2_INODE_INFO(inode);
++<<<<<<< HEAD
 +	struct jffs2_sb_info *c = JFFS2_SB_INFO(inode->i_sb);
 +	struct jffs2_raw_inode ri;
 +	uint32_t alloc_len = 0;
 +	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
 +	uint32_t pageofs = index << PAGE_CACHE_SHIFT;
++=======
+ 	pgoff_t index = pos >> PAGE_SHIFT;
+ 	uint32_t pageofs = index << PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	int ret = 0;
  
 +	jffs2_dbg(1, "%s()\n", __func__);
 +
 +	if (pageofs > inode->i_size) {
 +		ret = jffs2_reserve_space(c, sizeof(ri), &alloc_len,
 +					  ALLOC_NORMAL, JFFS2_SUMMARY_INODE_SIZE);
 +		if (ret)
 +			return ret;
 +	}
 +
 +	mutex_lock(&f->sem);
  	pg = grab_cache_page_write_begin(mapping, index, flags);
 -	if (!pg)
 +	if (!pg) {
 +		if (alloc_len)
 +			jffs2_complete_reservation(c);
 +		mutex_unlock(&f->sem);
  		return -ENOMEM;
 +	}
  	*pagep = pg;
  
 -	jffs2_dbg(1, "%s()\n", __func__);
 -
 -	if (pageofs > inode->i_size) {
 +	if (alloc_len) {
  		/* Make new hole frag from old EOF to new page */
 -		struct jffs2_sb_info *c = JFFS2_SB_INFO(inode->i_sb);
 -		struct jffs2_raw_inode ri;
  		struct jffs2_full_dnode *fn;
 -		uint32_t alloc_len;
  
  		jffs2_dbg(1, "Writing new hole frag 0x%x-0x%x between current EOF and new page\n",
  			  (unsigned int)inode->i_size, pageofs);
@@@ -233,8 -231,7 +239,12 @@@
  
  out_page:
  	unlock_page(pg);
++<<<<<<< HEAD
 +	page_cache_release(pg);
 +	mutex_unlock(&f->sem);
++=======
+ 	put_page(pg);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	return ret;
  }
  
diff --cc fs/jffs2/fs.c
index 00ed6c64a579,ae2ebb26b446..000000000000
--- a/fs/jffs2/fs.c
+++ b/fs/jffs2/fs.c
@@@ -682,7 -685,7 +682,11 @@@ unsigned char *jffs2_gc_fetch_page(stru
  	struct inode *inode = OFNI_EDONI_2SFFJ(f);
  	struct page *pg;
  
++<<<<<<< HEAD
 +	pg = read_cache_page_async(inode->i_mapping, offset >> PAGE_CACHE_SHIFT,
++=======
+ 	pg = read_cache_page(inode->i_mapping, offset >> PAGE_SHIFT,
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			     (void *)jffs2_do_readpage_unlock, inode);
  	if (IS_ERR(pg))
  		return (void *)pg;
diff --cc fs/jfs/jfs_metapage.c
index 6740d34cd82b,b60e015cc757..000000000000
--- a/fs/jfs/jfs_metapage.c
+++ b/fs/jfs/jfs_metapage.c
@@@ -571,9 -565,10 +571,13 @@@ static int metapage_releasepage(struct 
  	return ret;
  }
  
 -static void metapage_invalidatepage(struct page *page, unsigned int offset,
 -				    unsigned int length)
 +static void metapage_invalidatepage(struct page *page, unsigned long offset)
  {
++<<<<<<< HEAD
 +	BUG_ON(offset);
++=======
+ 	BUG_ON(offset || length < PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	BUG_ON(PageWriteback(page));
  
diff --cc fs/jfs/super.c
index 788e0a9c1fb0,78d599198bf5..000000000000
--- a/fs/jfs/super.c
+++ b/fs/jfs/super.c
@@@ -580,7 -596,8 +580,12 @@@ static int jfs_fill_super(struct super_
  	 * Page cache is indexed by long.
  	 * I would use MAX_LFS_FILESIZE, but it's only half as big
  	 */
++<<<<<<< HEAD
 +	sb->s_maxbytes = min(((u64) PAGE_CACHE_SIZE << 32) - 1, (u64)sb->s_maxbytes);
++=======
+ 	sb->s_maxbytes = min(((u64) PAGE_SIZE << 32) - 1,
+ 			     (u64)sb->s_maxbytes);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  #endif
  	sb->s_time_gran = 1;
  	return 0;
diff --cc fs/minix/dir.c
index a9ed6f36e6ea,33957c07cd11..000000000000
--- a/fs/minix/dir.c
+++ b/fs/minix/dir.c
@@@ -82,22 -77,23 +82,29 @@@ static inline void *minix_next_entry(vo
  	return (void*)((char*)de + sbi->s_dirsize);
  }
  
 -static int minix_readdir(struct file *file, struct dir_context *ctx)
 +static int minix_readdir(struct file * filp, void * dirent, filldir_t filldir)
  {
 -	struct inode *inode = file_inode(file);
 +	unsigned long pos = filp->f_pos;
 +	struct inode *inode = file_inode(filp);
  	struct super_block *sb = inode->i_sb;
 +	unsigned offset = pos & ~PAGE_CACHE_MASK;
 +	unsigned long n = pos >> PAGE_CACHE_SHIFT;
 +	unsigned long npages = dir_pages(inode);
  	struct minix_sb_info *sbi = minix_sb(sb);
  	unsigned chunk_size = sbi->s_dirsize;
 -	unsigned long npages = dir_pages(inode);
 -	unsigned long pos = ctx->pos;
 -	unsigned offset;
 -	unsigned long n;
 +	char *name;
 +	__u32 inumber;
  
 -	ctx->pos = pos = ALIGN(pos, chunk_size);
 +	pos = (pos + chunk_size-1) & ~(chunk_size-1);
  	if (pos >= inode->i_size)
++<<<<<<< HEAD
 +		goto done;
++=======
+ 		return 0;
+ 
+ 	offset = pos & ~PAGE_MASK;
+ 	n = pos >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	for ( ; n < npages; n++, offset = 0) {
  		char *p, *kaddr, *limit;
diff --cc fs/nfs/direct.c
index 07e5f6823c61,c93826e4a8c6..000000000000
--- a/fs/nfs/direct.c
+++ b/fs/nfs/direct.c
@@@ -1108,29 -994,18 +1108,34 @@@ ssize_t nfs_file_direct_write(struct ki
  	struct inode *inode = mapping->host;
  	struct nfs_direct_req *dreq;
  	struct nfs_lock_context *l_ctx;
 -	loff_t pos, end;
 +	loff_t end;
 +	size_t count;
 +
 +	count = iov_length(iov, nr_segs);
 +	end = (pos + count - 1) >> PAGE_CACHE_SHIFT;
 +
 +	nfs_add_stats(mapping->host, NFSIOS_DIRECTWRITTENBYTES, count);
  
  	dfprintk(FILE, "NFS: direct write(%pD2, %zd@%Ld)\n",
 -		file, iov_iter_count(iter), (long long) iocb->ki_pos);
 +		file, count, (long long) pos);
  
 -	nfs_add_stats(mapping->host, NFSIOS_DIRECTWRITTENBYTES,
 -		      iov_iter_count(iter));
 +	result = generic_write_checks(file, &pos, &count, 0);
 +	if (result)
 +		goto out;
  
++<<<<<<< HEAD
 +	result = -EINVAL;
 +	if ((ssize_t) count < 0)
 +		goto out;
 +	result = 0;
 +	if (!count)
 +		goto out;
++=======
+ 	pos = iocb->ki_pos;
+ 	end = (pos + iov_iter_count(iter) - 1) >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 -	inode_lock(inode);
 +	mutex_lock(&inode->i_mutex);
  
  	result = nfs_sync_mapping(mapping);
  	if (result)
@@@ -1167,10 -1042,10 +1172,10 @@@
  
  	if (mapping->nrpages) {
  		invalidate_inode_pages2_range(mapping,
- 					      pos >> PAGE_CACHE_SHIFT, end);
+ 					      pos >> PAGE_SHIFT, end);
  	}
  
 -	inode_unlock(inode);
 +	mutex_unlock(&inode->i_mutex);
  
  	if (!result) {
  		result = nfs_direct_wait(dreq);
diff --cc fs/nilfs2/dir.c
index f30b017740a7,e08f064e4bd7..000000000000
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@@ -58,14 -58,9 +58,14 @@@ static inline unsigned nilfs_chunk_size
  static inline void nilfs_put_page(struct page *page)
  {
  	kunmap(page);
- 	page_cache_release(page);
+ 	put_page(page);
  }
  
 +static inline unsigned long dir_pages(struct inode *inode)
 +{
 +	return (inode->i_size+PAGE_CACHE_SIZE-1)>>PAGE_CACHE_SHIFT;
 +}
 +
  /*
   * Return the offset into page `page_nr' of the last valid
   * byte in that page, plus one.
@@@ -256,22 -251,18 +256,22 @@@ static void nilfs_set_de_type(struct ni
  	de->file_type = nilfs_type_by_mode[(mode & S_IFMT)>>S_SHIFT];
  }
  
 -static int nilfs_readdir(struct file *file, struct dir_context *ctx)
 +static int nilfs_readdir(struct file *filp, void *dirent, filldir_t filldir)
  {
 -	loff_t pos = ctx->pos;
 -	struct inode *inode = file_inode(file);
 +	loff_t pos = filp->f_pos;
 +	struct inode *inode = file_inode(filp);
  	struct super_block *sb = inode->i_sb;
- 	unsigned int offset = pos & ~PAGE_CACHE_MASK;
- 	unsigned long n = pos >> PAGE_CACHE_SHIFT;
+ 	unsigned int offset = pos & ~PAGE_MASK;
+ 	unsigned long n = pos >> PAGE_SHIFT;
  	unsigned long npages = dir_pages(inode);
  /*	unsigned chunk_mask = ~(nilfs_chunk_size(inode)-1); */
 +	unsigned char *types = NULL;
 +	int ret;
  
  	if (pos > inode->i_size - NILFS_DIR_REC_LEN(1))
 -		return 0;
 +		goto success;
 +
 +	types = nilfs_filetype_table;
  
  	for ( ; n < npages; n++, offset = 0) {
  		char *kaddr, *limit;
@@@ -281,9 -272,8 +281,14 @@@
  		if (IS_ERR(page)) {
  			nilfs_error(sb, __func__, "bad page in #%lu",
  				    inode->i_ino);
++<<<<<<< HEAD
 +			filp->f_pos += PAGE_CACHE_SIZE - offset;
 +			ret = -EIO;
 +			goto done;
++=======
+ 			ctx->pos += PAGE_SIZE - offset;
+ 			return -EIO;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
  		kaddr = page_address(page);
  		de = (struct nilfs_dir_entry *)(kaddr + offset);
diff --cc fs/nilfs2/inode.c
index 4fcb0d232962,534631358b13..000000000000
--- a/fs/nilfs2/inode.c
+++ b/fs/nilfs2/inode.c
@@@ -245,6 -248,10 +245,13 @@@ static int nilfs_set_page_dirty(struct 
  
  		if (nr_dirty)
  			nilfs_set_file_dirty(inode, nr_dirty);
++<<<<<<< HEAD
++=======
+ 	} else if (ret) {
+ 		unsigned nr_dirty = 1 << (PAGE_SHIFT - inode->i_blkbits);
+ 
+ 		nilfs_set_file_dirty(inode, nr_dirty);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  	return ret;
  }
diff --cc fs/nilfs2/segment.c
index cbd66188a28b,4317f72568e6..000000000000
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@@ -1983,8 -2069,8 +1983,13 @@@ static int nilfs_segctor_do_construct(s
  		if (unlikely(err))
  			goto failed_to_write;
  
++<<<<<<< HEAD
 +		if (sci->sc_stage.scnt == NILFS_ST_DONE ||
 +		    nilfs->ns_blocksize_bits != PAGE_CACHE_SHIFT) {
++=======
+ 		if (nilfs_sc_cstage_get(sci) == NILFS_ST_DONE ||
+ 		    nilfs->ns_blocksize_bits != PAGE_SHIFT) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			/*
  			 * At this point, we avoid double buffering
  			 * for blocksize < pagesize because page dirty
diff --cc fs/ntfs/aops.c
index fa9c05f97af4,a474e7ef92ea..000000000000
--- a/fs/ntfs/aops.c
+++ b/fs/ntfs/aops.c
@@@ -1372,7 -1371,7 +1372,11 @@@ retry_writepage
  		 * The page may have dirty, unmapped buffers.  Make them
  		 * freeable here, so the page does not leak.
  		 */
++<<<<<<< HEAD
 +		block_invalidatepage(page, 0);
++=======
+ 		block_invalidatepage(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		unlock_page(page);
  		ntfs_debug("Write outside i_size - truncated?");
  		return 0;
diff --cc fs/ntfs/attrib.c
index a27e3fecefaf,44a39a099b54..000000000000
--- a/fs/ntfs/attrib.c
+++ b/fs/ntfs/attrib.c
@@@ -1748,8 -1748,7 +1748,12 @@@ int ntfs_attr_make_non_resident(ntfs_in
  	if (page) {
  		set_page_dirty(page);
  		unlock_page(page);
++<<<<<<< HEAD
 +		mark_page_accessed(page);
 +		page_cache_release(page);
++=======
+ 		put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  	ntfs_debug("Done.");
  	return 0;
diff --cc fs/ntfs/file.c
index 81adff3fd865,2dae60857544..000000000000
--- a/fs/ntfs/file.c
+++ b/fs/ntfs/file.c
@@@ -1763,14 -1692,48 +1763,46 @@@ err_out
  	return err;
  }
  
 -/*
 - * Copy as much as we can into the pages and return the number of bytes which
 - * were successfully copied.  If a fault is encountered then clear the pages
 - * out to (ofs + bytes) and return the number of bytes which were copied.
 - */
 -static size_t ntfs_copy_from_user_iter(struct page **pages, unsigned nr_pages,
 -		unsigned ofs, struct iov_iter *i, size_t bytes)
 +static void ntfs_write_failed(struct address_space *mapping, loff_t to)
  {
 -	struct page **last_page = pages + nr_pages;
 -	size_t total = 0;
 -	struct iov_iter data = *i;
 -	unsigned len, copied;
 +	struct inode *inode = mapping->host;
  
++<<<<<<< HEAD
 +	if (to > inode->i_size) {
 +		truncate_pagecache(inode, inode->i_size);
 +		ntfs_truncate_vfs(inode);
 +	}
++=======
+ 	do {
+ 		len = PAGE_SIZE - ofs;
+ 		if (len > bytes)
+ 			len = bytes;
+ 		copied = iov_iter_copy_from_user_atomic(*pages, &data, ofs,
+ 				len);
+ 		total += copied;
+ 		bytes -= copied;
+ 		if (!bytes)
+ 			break;
+ 		iov_iter_advance(&data, copied);
+ 		if (copied < len)
+ 			goto err;
+ 		ofs = 0;
+ 	} while (++pages < last_page);
+ out:
+ 	return total;
+ err:
+ 	/* Zero the rest of the target like __copy_from_user(). */
+ 	len = PAGE_SIZE - copied;
+ 	do {
+ 		if (len > bytes)
+ 			len = bytes;
+ 		zero_user(*pages, copied, len);
+ 		bytes -= len;
+ 		copied = 0;
+ 		len = PAGE_SIZE;
+ 	} while (++pages < last_page);
+ 	goto out;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  /**
@@@ -1936,12 -1787,9 +1968,17 @@@ static ssize_t ntfs_file_buffered_write
  	 * attributes.
  	 */
  	nr_pages = 1;
++<<<<<<< HEAD
 +	if (vol->cluster_size > PAGE_CACHE_SIZE && NInoNonResident(ni))
 +		nr_pages = vol->cluster_size >> PAGE_CACHE_SHIFT;
 +	/* Finally, perform the actual write. */
++=======
+ 	if (vol->cluster_size > PAGE_SIZE && NInoNonResident(ni))
+ 		nr_pages = vol->cluster_size >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	last_vcn = -1;
 +	if (likely(nr_segs == 1))
 +		buf = iov->iov_base;
  	do {
  		VCN vcn;
  		pgoff_t idx, start_idx;
@@@ -2019,59 -1869,59 +2056,69 @@@
  			status = ntfs_prepare_pages_for_non_resident_write(
  					pages, do_pages, pos, bytes);
  			if (unlikely(status)) {
 +				loff_t i_size;
 +
  				do {
  					unlock_page(pages[--do_pages]);
- 					page_cache_release(pages[do_pages]);
+ 					put_page(pages[do_pages]);
  				} while (do_pages);
 +				/*
 +				 * The write preparation may have instantiated
 +				 * allocated space outside i_size.  Trim this
 +				 * off again.  We can ignore any errors in this
 +				 * case as we will just be waisting a bit of
 +				 * allocated space, which is not a disaster.
 +				 */
 +				i_size = i_size_read(vi);
 +				if (pos + bytes > i_size) {
 +					ntfs_write_failed(mapping, pos + bytes);
 +				}
  				break;
  			}
  		}
++<<<<<<< HEAD
 +		u = (pos >> PAGE_CACHE_SHIFT) - pages[0]->index;
 +		if (likely(nr_segs == 1)) {
 +			copied = ntfs_copy_from_user(pages + u, do_pages - u,
 +					ofs, buf, bytes);
 +			buf += copied;
 +		} else
 +			copied = ntfs_copy_from_user_iovec(pages + u,
 +					do_pages - u, ofs, &iov, &iov_ofs,
 +					bytes);
++=======
+ 		u = (pos >> PAGE_SHIFT) - pages[0]->index;
+ 		copied = ntfs_copy_from_user_iter(pages + u, do_pages - u, ofs,
+ 					i, bytes);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		ntfs_flush_dcache_pages(pages + u, do_pages - u);
 -		status = 0;
 -		if (likely(copied == bytes)) {
 -			status = ntfs_commit_pages_after_write(pages, do_pages,
 -					pos, bytes);
 -			if (!status)
 -				status = bytes;
 +		status = ntfs_commit_pages_after_write(pages, do_pages, pos,
 +				bytes);
 +		if (likely(!status)) {
 +			written += copied;
 +			count -= copied;
 +			pos += copied;
 +			if (unlikely(copied != bytes))
 +				status = -EFAULT;
  		}
  		do {
  			unlock_page(pages[--do_pages]);
++<<<<<<< HEAD
 +			mark_page_accessed(pages[do_pages]);
 +			page_cache_release(pages[do_pages]);
++=======
+ 			put_page(pages[do_pages]);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		} while (do_pages);
 -		if (unlikely(status < 0))
 +		if (unlikely(status))
  			break;
 -		copied = status;
 -		cond_resched();
 -		if (unlikely(!copied)) {
 -			size_t sc;
 -
 -			/*
 -			 * We failed to copy anything.  Fall back to single
 -			 * segment length write.
 -			 *
 -			 * This is needed to avoid possible livelock in the
 -			 * case that all segments in the iov cannot be copied
 -			 * at once without a pagefault.
 -			 */
 -			sc = iov_iter_single_seg_count(i);
 -			if (bytes > sc)
 -				bytes = sc;
 -			goto again;
 -		}
 -		iov_iter_advance(i, copied);
 -		pos += copied;
 -		written += copied;
  		balance_dirty_pages_ratelimited(mapping);
 -		if (fatal_signal_pending(current)) {
 -			status = -EINTR;
 -			break;
 -		}
 -	} while (iov_iter_count(i));
 +		cond_resched();
 +	} while (count);
 +err_out:
 +	*ppos = pos;
  	if (cached_page)
- 		page_cache_release(cached_page);
+ 		put_page(cached_page);
  	ntfs_debug("Done.  Returning %s (written 0x%lx, status %li).",
  			written ? "written" : "status", (unsigned long)written,
  			(long)status);
diff --cc fs/ocfs2/aops.c
index 7fee7b2e1225,ce5dc4f92935..000000000000
--- a/fs/ocfs2/aops.c
+++ b/fs/ocfs2/aops.c
@@@ -227,10 -234,10 +227,10 @@@ int ocfs2_read_inline_data(struct inod
  
  	size = i_size_read(inode);
  
- 	if (size > PAGE_CACHE_SIZE ||
+ 	if (size > PAGE_SIZE ||
  	    size > ocfs2_max_inline_data_with_xattr(inode->i_sb, di)) {
  		ocfs2_error(inode->i_sb,
 -			    "Inode %llu has with inline data has bad size: %Lu\n",
 +			    "Inode %llu has with inline data has bad size: %Lu",
  			    (unsigned long long)OCFS2_I(inode)->ip_blkno,
  			    (unsigned long long)size);
  		return -EROFS;
@@@ -827,11 -687,18 +827,11 @@@ next_bh
  #if (PAGE_CACHE_SIZE >= OCFS2_MAX_CLUSTERSIZE)
  #define OCFS2_MAX_CTXT_PAGES	1
  #else
- #define OCFS2_MAX_CTXT_PAGES	(OCFS2_MAX_CLUSTERSIZE / PAGE_CACHE_SIZE)
+ #define OCFS2_MAX_CTXT_PAGES	(OCFS2_MAX_CLUSTERSIZE / PAGE_SIZE)
  #endif
  
- #define OCFS2_MAX_CLUSTERS_PER_PAGE	(PAGE_CACHE_SIZE / OCFS2_MIN_CLUSTERSIZE)
+ #define OCFS2_MAX_CLUSTERS_PER_PAGE	(PAGE_SIZE / OCFS2_MIN_CLUSTERSIZE)
  
 -struct ocfs2_unwritten_extent {
 -	struct list_head	ue_node;
 -	struct list_head	ue_ip_node;
 -	u32			ue_cpos;
 -	u32			ue_phys;
 -};
 -
  /*
   * Describe the state of a single cluster to be written to.
   */
@@@ -936,10 -808,31 +936,10 @@@ static void ocfs2_free_write_ctxt(struc
  			}
  		}
  		mark_page_accessed(wc->w_target_page);
- 		page_cache_release(wc->w_target_page);
+ 		put_page(wc->w_target_page);
  	}
  	ocfs2_unlock_and_free_pages(wc->w_pages, wc->w_num_pages);
 -}
 -
 -static void ocfs2_free_unwritten_list(struct inode *inode,
 -				 struct list_head *head)
 -{
 -	struct ocfs2_inode_info *oi = OCFS2_I(inode);
 -	struct ocfs2_unwritten_extent *ue = NULL, *tmp = NULL;
 -
 -	list_for_each_entry_safe(ue, tmp, head, ue_node) {
 -		list_del(&ue->ue_node);
 -		spin_lock(&oi->ip_lock);
 -		list_del(&ue->ue_ip_node);
 -		spin_unlock(&oi->ip_lock);
 -		kfree(ue);
 -	}
 -}
  
 -static void ocfs2_free_write_ctxt(struct inode *inode,
 -				  struct ocfs2_write_ctxt *wc)
 -{
 -	ocfs2_free_unwritten_list(inode, &wc->w_unwritten_list);
 -	ocfs2_unlock_pages(wc);
  	brelse(wc->w_di_bh);
  	kfree(wc);
  }
@@@ -961,8 -855,9 +961,8 @@@ static int ocfs2_alloc_write_ctxt(struc
  	wc->w_clen = cend - wc->w_cpos + 1;
  	get_bh(di_bh);
  	wc->w_di_bh = di_bh;
 -	wc->w_type = type;
  
- 	if (unlikely(PAGE_CACHE_SHIFT > osb->s_clustersize_bits))
+ 	if (unlikely(PAGE_SHIFT > osb->s_clustersize_bits))
  		wc->w_large_pages = 1;
  	else
  		wc->w_large_pages = 0;
@@@ -1163,6 -1060,7 +1163,10 @@@ static int ocfs2_grab_pages_for_write(s
  		wc->w_num_pages = 1;
  		start = target_index;
  	}
++<<<<<<< HEAD
++=======
+ 	end_index = (user_pos + user_len - 1) >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	for(i = 0; i < wc->w_num_pages; i++) {
  		index = start + i;
@@@ -1183,9 -1082,14 +1187,9 @@@
  				goto out;
  			}
  
- 			page_cache_get(mmap_page);
+ 			get_page(mmap_page);
  			wc->w_pages[i] = mmap_page;
  			wc->w_target_locked = true;
 -		} else if (index >= target_index && index <= end_index &&
 -			   wc->w_type == OCFS2_WRITE_DIRECT) {
 -			/* Direct write has no mapping page. */
 -			wc->w_pages[i] = NULL;
 -			continue;
  		} else {
  			wc->w_pages[i] = find_or_create_page(mapping, index,
  							     GFP_NOFS);
@@@ -1998,8 -1980,8 +2002,13 @@@ int ocfs2_write_end_nolock(struct addre
  			   loff_t pos, unsigned len, unsigned copied,
  			   struct page *page, void *fsdata)
  {
++<<<<<<< HEAD
 +	int i;
 +	unsigned from, to, start = pos & (PAGE_CACHE_SIZE - 1);
++=======
+ 	int i, ret;
+ 	unsigned from, to, start = pos & (PAGE_SIZE - 1);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	struct inode *inode = mapping->host;
  	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
  	struct ocfs2_write_ctxt *wc = fsdata;
diff --cc fs/ocfs2/file.c
index 2bcf1677a59b,5308841756be..000000000000
--- a/fs/ocfs2/file.c
+++ b/fs/ocfs2/file.c
@@@ -760,15 -770,22 +760,20 @@@ static int ocfs2_write_zero_page(struc
  {
  	struct address_space *mapping = inode->i_mapping;
  	struct page *page;
++<<<<<<< HEAD
 +	unsigned long index = abs_from >> PAGE_CACHE_SHIFT;
 +	handle_t *handle = NULL;
++=======
+ 	unsigned long index = abs_from >> PAGE_SHIFT;
+ 	handle_t *handle;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	int ret = 0;
  	unsigned zero_from, zero_to, block_start, block_end;
 -	struct ocfs2_dinode *di = (struct ocfs2_dinode *)di_bh->b_data;
  
  	BUG_ON(abs_from >= abs_to);
- 	BUG_ON(abs_to > (((u64)index + 1) << PAGE_CACHE_SHIFT));
+ 	BUG_ON(abs_to > (((u64)index + 1) << PAGE_SHIFT));
  	BUG_ON(abs_from & (inode->i_blkbits - 1));
  
 -	handle = ocfs2_zero_start_ordered_transaction(inode, di_bh);
 -	if (IS_ERR(handle)) {
 -		ret = PTR_ERR(handle);
 -		goto out;
 -	}
 -
  	page = find_or_create_page(mapping, index, GFP_NOFS);
  	if (!page) {
  		ret = -ENOMEM;
@@@ -828,7 -851,10 +833,14 @@@
  
  out_unlock:
  	unlock_page(page);
++<<<<<<< HEAD
 +	page_cache_release(page);
++=======
+ 	put_page(page);
+ out_commit_trans:
+ 	if (handle)
+ 		ocfs2_commit_trans(OCFS2_SB(inode->i_sb), handle);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  out:
  	return ret;
  }
@@@ -933,10 -959,10 +945,10 @@@ static int ocfs2_zero_extend_range(stru
  	BUG_ON(range_start >= range_end);
  
  	while (zero_pos < range_end) {
- 		next_pos = (zero_pos & PAGE_CACHE_MASK) + PAGE_CACHE_SIZE;
+ 		next_pos = (zero_pos & PAGE_MASK) + PAGE_SIZE;
  		if (next_pos > range_end)
  			next_pos = range_end;
 -		rc = ocfs2_write_zero_page(inode, zero_pos, next_pos, di_bh);
 +		rc = ocfs2_write_zero_page(inode, zero_pos, next_pos);
  		if (rc < 0) {
  			mlog_errno(rc);
  			break;
diff --cc fs/qnx6/dir.c
index afa6be6fc397,144ceda4948e..000000000000
--- a/fs/qnx6/dir.c
+++ b/fs/qnx6/dir.c
@@@ -115,18 -108,19 +115,18 @@@ static int qnx6_dir_longfilename(struc
  	return 1;
  }
  
 -static int qnx6_readdir(struct file *file, struct dir_context *ctx)
 +static int qnx6_readdir(struct file *filp, void *dirent, filldir_t filldir)
  {
 -	struct inode *inode = file_inode(file);
 +	struct inode *inode = file_inode(filp);
  	struct super_block *s = inode->i_sb;
  	struct qnx6_sb_info *sbi = QNX6_SB(s);
 -	loff_t pos = ctx->pos & ~(QNX6_DIR_ENTRY_SIZE - 1);
 +	loff_t pos = filp->f_pos & ~(QNX6_DIR_ENTRY_SIZE - 1);
  	unsigned long npages = dir_pages(inode);
- 	unsigned long n = pos >> PAGE_CACHE_SHIFT;
- 	unsigned start = (pos & ~PAGE_CACHE_MASK) / QNX6_DIR_ENTRY_SIZE;
+ 	unsigned long n = pos >> PAGE_SHIFT;
+ 	unsigned start = (pos & ~PAGE_MASK) / QNX6_DIR_ENTRY_SIZE;
  	bool done = false;
  
 -	ctx->pos = pos;
 -	if (ctx->pos >= inode->i_size)
 +	if (filp->f_pos >= inode->i_size)
  		return 0;
  
  	for ( ; !done && n < npages; n++, start = 0) {
@@@ -136,8 -130,8 +136,13 @@@
  		int i = start;
  
  		if (IS_ERR(page)) {
++<<<<<<< HEAD
 +			printk(KERN_ERR "qnx6_readdir: read failed\n");
 +			filp->f_pos = (n + 1) << PAGE_CACHE_SHIFT;
++=======
+ 			pr_err("%s(): read failed\n", __func__);
+ 			ctx->pos = (n + 1) << PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			return PTR_ERR(page);
  		}
  		de = ((struct qnx6_dir_entry *)page_address(page)) + start;
diff --cc fs/reiserfs/inode.c
index a3aae8634a3e,d5c2e9c865de..000000000000
--- a/fs/reiserfs/inode.c
+++ b/fs/reiserfs/inode.c
@@@ -347,14 -385,15 +347,19 @@@ static int _get_block_create_0(struct i
  		set_buffer_uptodate(bh_result);
  		goto finished;
  	}
++<<<<<<< HEAD
 +	// read file tail into part of page
 +	offset = (cpu_key_k_offset(&key) - 1) & (PAGE_CACHE_SIZE - 1);
++=======
+ 	/* read file tail into part of page */
+ 	offset = (cpu_key_k_offset(&key) - 1) & (PAGE_SIZE - 1);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	copy_item_head(&tmp_ih, ih);
  
 -	/*
 -	 * we only want to kmap if we are reading the tail into the page.
 -	 * this is not the common case, so we don't kmap until we are
 -	 * sure we need to.  But, this means the item might move if
 -	 * kmap schedules
 +	/* we only want to kmap if we are reading the tail into the page.
 +	 ** this is not the common case, so we don't kmap until we are
 +	 ** sure we need to.  But, this means the item might move if
 +	 ** kmap schedules
  	 */
  	if (!p)
  		p = (char *)kmap(bh_result->b_page);
@@@ -533,13 -587,14 +538,21 @@@ static int convert_tail_for_hole(struc
  		return -EIO;
  
  	/* always try to read until the end of the block */
- 	tail_start = tail_offset & (PAGE_CACHE_SIZE - 1);
+ 	tail_start = tail_offset & (PAGE_SIZE - 1);
  	tail_end = (tail_start | (bh_result->b_size - 1)) + 1;
  
++<<<<<<< HEAD
 +	index = tail_offset >> PAGE_CACHE_SHIFT;
 +	/* hole_page can be zero in case of direct_io, we are sure
 +	   that we cannot get here if we write with O_DIRECT into
 +	   tail page */
++=======
+ 	index = tail_offset >> PAGE_SHIFT;
+ 	/*
+ 	 * hole_page can be zero in case of direct_io, we are sure
+ 	 * that we cannot get here if we write with O_DIRECT into tail page
+ 	 */
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (!hole_page || index != hole_page->index) {
  		tail_page = grab_cache_page(inode->i_mapping, index);
  		retval = -ENOMEM;
@@@ -570,12 -626,12 +583,12 @@@
  
  	retval = reiserfs_commit_write(NULL, tail_page, tail_start, tail_end);
  
 -unlock:
 +      unlock:
  	if (tail_page != hole_page) {
  		unlock_page(tail_page);
- 		page_cache_release(tail_page);
+ 		put_page(tail_page);
  	}
 -out:
 +      out:
  	return retval;
  }
  
@@@ -2019,10 -2185,11 +2032,10 @@@ static int grab_tail_page(struct inode 
  			  struct buffer_head **bh_result)
  {
  
 -	/*
 -	 * we want the page with the last byte in the file,
 -	 * not the page that will hold the next byte for appending
 +	/* we want the page with the last byte in the file,
 +	 ** not the page that will hold the next byte for appending
  	 */
- 	unsigned long index = (inode->i_size - 1) >> PAGE_CACHE_SHIFT;
+ 	unsigned long index = (inode->i_size - 1) >> PAGE_SHIFT;
  	unsigned long pos = 0;
  	unsigned long start = 0;
  	unsigned long blocksize = inode->i_sb->s_blocksize;
@@@ -2077,12 -2246,12 +2090,12 @@@
  	*bh_result = bh;
  	*page_result = page;
  
 -out:
 +      out:
  	return error;
  
 -unlock:
 +      unlock:
  	unlock_page(page);
- 	page_cache_release(page);
+ 	put_page(page);
  	return error;
  }
  
@@@ -2171,19 -2345,19 +2184,19 @@@ int reiserfs_truncate_file(struct inod
  			}
  		}
  		unlock_page(page);
- 		page_cache_release(page);
+ 		put_page(page);
  	}
  
 -	reiserfs_write_unlock(inode->i_sb);
 +	reiserfs_write_unlock_once(inode->i_sb, lock_depth);
  
  	return 0;
 -out:
 +      out:
  	if (page) {
  		unlock_page(page);
- 		page_cache_release(page);
+ 		put_page(page);
  	}
  
 -	reiserfs_write_unlock(inode->i_sb);
 +	reiserfs_write_unlock_once(inode->i_sb, lock_depth);
  
  	return error;
  }
@@@ -2778,11 -2970,11 +2791,11 @@@ static int reiserfs_write_end(struct fi
  			goto out;
  	}
  
 -out:
 +      out:
  	if (locked)
 -		reiserfs_write_unlock(inode->i_sb);
 +		reiserfs_write_unlock_once(inode->i_sb, lock_depth);
  	unlock_page(page);
- 	page_cache_release(page);
+ 	put_page(page);
  
  	if (pos + len > inode->i_size)
  		reiserfs_truncate_failed_write(inode);
@@@ -2980,6 -3180,8 +2993,11 @@@ static void reiserfs_invalidatepage(str
  	struct buffer_head *head, *bh, *next;
  	struct inode *inode = page->mapping->host;
  	unsigned int curr_off = 0;
++<<<<<<< HEAD
++=======
+ 	unsigned int stop = offset + length;
+ 	int partial_page = (offset || length < PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	int ret = 1;
  
  	BUG_ON(!PageLocked(page));
diff --cc fs/reiserfs/xattr.c
index 821bcf70e467,28f5f8b11370..000000000000
--- a/fs/reiserfs/xattr.c
+++ b/fs/reiserfs/xattr.c
@@@ -405,10 -422,12 +405,10 @@@ static struct page *reiserfs_get_page(s
  {
  	struct address_space *mapping = dir->i_mapping;
  	struct page *page;
 -	/*
 -	 * We can deadlock if we try to free dentries,
 -	 * and an unlink/rmdir has just occurred - GFP_NOFS avoids this
 -	 */
 +	/* We can deadlock if we try to free dentries,
 +	   and an unlink/rmdir has just occurred - GFP_NOFS avoids this */
  	mapping_set_gfp_mask(mapping, GFP_NOFS);
- 	page = read_mapping_page(mapping, n >> PAGE_CACHE_SHIFT, NULL);
+ 	page = read_mapping_page(mapping, n >> PAGE_SHIFT, NULL);
  	if (!IS_ERR(page)) {
  		kmap(page);
  		if (PageError(page))
@@@ -515,9 -526,10 +515,16 @@@ reiserfs_xattr_set_handle(struct reiser
  	while (buffer_pos < buffer_size || buffer_pos == 0) {
  		size_t chunk;
  		size_t skip = 0;
++<<<<<<< HEAD
 +		size_t page_offset = (file_pos & (PAGE_CACHE_SIZE - 1));
 +		if (buffer_size - buffer_pos > PAGE_CACHE_SIZE)
 +			chunk = PAGE_CACHE_SIZE;
++=======
+ 		size_t page_offset = (file_pos & (PAGE_SIZE - 1));
+ 
+ 		if (buffer_size - buffer_pos > PAGE_SIZE)
+ 			chunk = PAGE_SIZE;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		else
  			chunk = buffer_size - buffer_pos;
  
@@@ -532,9 -544,10 +539,9 @@@
  
  		if (file_pos == 0) {
  			struct reiserfs_xattr_header *rxh;
 -
  			skip = file_pos = sizeof(struct reiserfs_xattr_header);
- 			if (chunk + skip > PAGE_CACHE_SIZE)
- 				chunk = PAGE_CACHE_SIZE - skip;
+ 			if (chunk + skip > PAGE_SIZE)
+ 				chunk = PAGE_SIZE - skip;
  			rxh = (struct reiserfs_xattr_header *)data;
  			rxh->h_magic = cpu_to_le32(REISERFS_XATTR_MAGIC);
  			rxh->h_hash = cpu_to_le32(xahash);
@@@ -658,8 -674,9 +665,14 @@@ reiserfs_xattr_get(struct inode *inode
  		size_t chunk;
  		char *data;
  		size_t skip = 0;
++<<<<<<< HEAD
 +		if (isize - file_pos > PAGE_CACHE_SIZE)
 +			chunk = PAGE_CACHE_SIZE;
++=======
+ 
+ 		if (isize - file_pos > PAGE_SIZE)
+ 			chunk = PAGE_SIZE;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		else
  			chunk = isize - file_pos;
  
diff --cc fs/squashfs/block.c
index fb50652e4e11,2c2618410d51..000000000000
--- a/fs/squashfs/block.c
+++ b/fs/squashfs/block.c
@@@ -179,14 -181,14 +179,19 @@@ int squashfs_read_data(struct super_blo
  			in = min(bytes, msblk->devblksize - offset);
  			bytes -= in;
  			while (in) {
++<<<<<<< HEAD
 +				if (pg_offset == PAGE_CACHE_SIZE) {
 +					page++;
++=======
+ 				if (pg_offset == PAGE_SIZE) {
+ 					data = squashfs_next_page(output);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  					pg_offset = 0;
  				}
- 				avail = min_t(int, in, PAGE_CACHE_SIZE -
+ 				avail = min_t(int, in, PAGE_SIZE -
  						pg_offset);
 -				memcpy(data + pg_offset, bh[k]->b_data + offset,
 -						avail);
 +				memcpy(buffer[page] + pg_offset,
 +						bh[k]->b_data + offset, avail);
  				in -= avail;
  				pg_offset += avail;
  				offset += avail;
diff --cc fs/squashfs/cache.c
index af0b73802592,27e501af0e8b..000000000000
--- a/fs/squashfs/cache.c
+++ b/fs/squashfs/cache.c
@@@ -407,9 -415,10 +407,9 @@@ struct squashfs_cache_entry *squashfs_g
   */
  void *squashfs_read_table(struct super_block *sb, u64 block, int length)
  {
- 	int pages = (length + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+ 	int pages = (length + PAGE_SIZE - 1) >> PAGE_SHIFT;
  	int i, res;
  	void *table, *buffer, **data;
 -	struct squashfs_page_actor *actor;
  
  	table = buffer = kmalloc(length, GFP_KERNEL);
  	if (table == NULL)
@@@ -421,13 -430,20 +421,23 @@@
  		goto failed;
  	}
  
++<<<<<<< HEAD
 +	for (i = 0; i < pages; i++, buffer += PAGE_CACHE_SIZE)
++=======
+ 	actor = squashfs_page_actor_init(data, pages, length);
+ 	if (actor == NULL) {
+ 		res = -ENOMEM;
+ 		goto failed2;
+ 	}
+ 
+ 	for (i = 0; i < pages; i++, buffer += PAGE_SIZE)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		data[i] = buffer;
  
 -	res = squashfs_read_data(sb, block, length |
 -		SQUASHFS_COMPRESSED_BIT_BLOCK, NULL, actor);
 +	res = squashfs_read_data(sb, data, block, length |
 +		SQUASHFS_COMPRESSED_BIT_BLOCK, NULL, length, pages);
  
  	kfree(data);
 -	kfree(actor);
  
  	if (res < 0)
  		goto failed;
diff --cc fs/squashfs/decompressor.c
index 3f6271d86abc,d2bc13636f79..000000000000
--- a/fs/squashfs/decompressor.c
+++ b/fs/squashfs/decompressor.c
@@@ -93,17 -102,24 +93,25 @@@ void *squashfs_decompressor_init(struc
  	 * Read decompressor specific options from file system if present
  	 */
  	if (SQUASHFS_COMP_OPTS(flags)) {
++<<<<<<< HEAD
 +		buffer = kmalloc(PAGE_CACHE_SIZE, GFP_KERNEL);
 +		if (buffer == NULL)
 +			return ERR_PTR(-ENOMEM);
++=======
+ 		buffer = kmalloc(PAGE_SIZE, GFP_KERNEL);
+ 		if (buffer == NULL) {
+ 			comp_opts = ERR_PTR(-ENOMEM);
+ 			goto out;
+ 		}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 -		actor = squashfs_page_actor_init(&buffer, 1, 0);
 -		if (actor == NULL) {
 -			comp_opts = ERR_PTR(-ENOMEM);
 -			goto out;
 -		}
 -
 -		length = squashfs_read_data(sb,
 -			sizeof(struct squashfs_super_block), 0, NULL, actor);
 +		length = squashfs_read_data(sb, &buffer,
 +			sizeof(struct squashfs_super_block), 0, NULL,
 +			PAGE_CACHE_SIZE, 1);
  
  		if (length < 0) {
 -			comp_opts = ERR_PTR(length);
 -			goto out;
 +			strm = ERR_PTR(length);
 +			goto finished;
  		}
  	}
  
diff --cc fs/squashfs/file.c
index 8ca62c28fe12,437de9e89221..000000000000
--- a/fs/squashfs/file.c
+++ b/fs/squashfs/file.c
@@@ -375,72 -376,9 +375,77 @@@ static int squashfs_readpage(struct fil
  {
  	struct inode *inode = page->mapping->host;
  	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
 +	int bytes, i, offset = 0, sparse = 0;
 +	struct squashfs_cache_entry *buffer = NULL;
  	void *pageaddr;
++<<<<<<< HEAD
 +
 +	int mask = (1 << (msblk->block_log - PAGE_CACHE_SHIFT)) - 1;
 +	int index = page->index >> (msblk->block_log - PAGE_CACHE_SHIFT);
 +	int start_index = page->index & ~mask;
 +	int end_index = start_index | mask;
 +	int file_end = i_size_read(inode) >> msblk->block_log;
 +
 +	TRACE("Entered squashfs_readpage, page index %lx, start block %llx\n",
 +				page->index, squashfs_i(inode)->start);
 +
 +	if (page->index >= ((i_size_read(inode) + PAGE_CACHE_SIZE - 1) >>
 +					PAGE_CACHE_SHIFT))
 +		goto out;
 +
 +	if (index < file_end || squashfs_i(inode)->fragment_block ==
 +					SQUASHFS_INVALID_BLK) {
 +		/*
 +		 * Reading a datablock from disk.  Need to read block list
 +		 * to get location and block size.
 +		 */
 +		u64 block = 0;
 +		int bsize = read_blocklist(inode, index, &block);
 +		if (bsize < 0)
 +			goto error_out;
 +
 +		if (bsize == 0) { /* hole */
 +			bytes = index == file_end ?
 +				(i_size_read(inode) & (msblk->block_size - 1)) :
 +				 msblk->block_size;
 +			sparse = 1;
 +		} else {
 +			/*
 +			 * Read and decompress datablock.
 +			 */
 +			buffer = squashfs_get_datablock(inode->i_sb,
 +								block, bsize);
 +			if (buffer->error) {
 +				ERROR("Unable to read page, block %llx, size %x"
 +					"\n", block, bsize);
 +				squashfs_cache_put(buffer);
 +				goto error_out;
 +			}
 +			bytes = buffer->length;
 +		}
 +	} else {
 +		/*
 +		 * Datablock is stored inside a fragment (tail-end packed
 +		 * block).
 +		 */
 +		buffer = squashfs_get_fragment(inode->i_sb,
 +				squashfs_i(inode)->fragment_block,
 +				squashfs_i(inode)->fragment_size);
 +
 +		if (buffer->error) {
 +			ERROR("Unable to read page, block %llx, size %x\n",
 +				squashfs_i(inode)->fragment_block,
 +				squashfs_i(inode)->fragment_size);
 +			squashfs_cache_put(buffer);
 +			goto error_out;
 +		}
 +		bytes = i_size_read(inode) & (msblk->block_size - 1);
 +		offset = squashfs_i(inode)->fragment_offset;
 +	}
++=======
+ 	int i, mask = (1 << (msblk->block_log - PAGE_SHIFT)) - 1;
+ 	int start_index = page->index & ~mask, end_index = start_index | mask;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	/*
  	 * Loop copying datablock into pages.  As the datablock likely covers
@@@ -449,9 -387,9 +454,13 @@@
  	 * been called to fill.
  	 */
  	for (i = start_index; i <= end_index && bytes > 0; i++,
- 			bytes -= PAGE_CACHE_SIZE, offset += PAGE_CACHE_SIZE) {
+ 			bytes -= PAGE_SIZE, offset += PAGE_SIZE) {
  		struct page *push_page;
++<<<<<<< HEAD
 +		int avail = sparse ? 0 : min_t(int, bytes, PAGE_CACHE_SIZE);
++=======
+ 		int avail = buffer ? min_t(int, bytes, PAGE_SIZE) : 0;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  		TRACE("bytes %d, i %d, available_bytes %d\n", bytes, i, avail);
  
@@@ -473,13 -411,77 +482,50 @@@
  skip_page:
  		unlock_page(push_page);
  		if (i != page->index)
- 			page_cache_release(push_page);
+ 			put_page(push_page);
  	}
 -}
  
 -/* Read datablock stored packed inside a fragment (tail-end packed block) */
 -static int squashfs_readpage_fragment(struct page *page)
 -{
 -	struct inode *inode = page->mapping->host;
 -	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
 -	struct squashfs_cache_entry *buffer = squashfs_get_fragment(inode->i_sb,
 -		squashfs_i(inode)->fragment_block,
 -		squashfs_i(inode)->fragment_size);
 -	int res = buffer->error;
 -
 -	if (res)
 -		ERROR("Unable to read page, block %llx, size %x\n",
 -			squashfs_i(inode)->fragment_block,
 -			squashfs_i(inode)->fragment_size);
 -	else
 -		squashfs_copy_cache(page, buffer, i_size_read(inode) &
 -			(msblk->block_size - 1),
 -			squashfs_i(inode)->fragment_offset);
 -
 -	squashfs_cache_put(buffer);
 -	return res;
 -}
 -
 -static int squashfs_readpage_sparse(struct page *page, int index, int file_end)
 -{
 -	struct inode *inode = page->mapping->host;
 -	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
 -	int bytes = index == file_end ?
 -			(i_size_read(inode) & (msblk->block_size - 1)) :
 -			 msblk->block_size;
 +	if (!sparse)
 +		squashfs_cache_put(buffer);
  
 -	squashfs_copy_cache(page, NULL, bytes, 0);
  	return 0;
++<<<<<<< HEAD
++=======
+ }
+ 
+ static int squashfs_readpage(struct file *file, struct page *page)
+ {
+ 	struct inode *inode = page->mapping->host;
+ 	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
+ 	int index = page->index >> (msblk->block_log - PAGE_SHIFT);
+ 	int file_end = i_size_read(inode) >> msblk->block_log;
+ 	int res;
+ 	void *pageaddr;
+ 
+ 	TRACE("Entered squashfs_readpage, page index %lx, start block %llx\n",
+ 				page->index, squashfs_i(inode)->start);
+ 
+ 	if (page->index >= ((i_size_read(inode) + PAGE_SIZE - 1) >>
+ 					PAGE_SHIFT))
+ 		goto out;
+ 
+ 	if (index < file_end || squashfs_i(inode)->fragment_block ==
+ 					SQUASHFS_INVALID_BLK) {
+ 		u64 block = 0;
+ 		int bsize = read_blocklist(inode, index, &block);
+ 		if (bsize < 0)
+ 			goto error_out;
+ 
+ 		if (bsize == 0)
+ 			res = squashfs_readpage_sparse(page, index, file_end);
+ 		else
+ 			res = squashfs_readpage_block(page, block, bsize);
+ 	} else
+ 		res = squashfs_readpage_fragment(page);
+ 
+ 	if (!res)
+ 		return 0;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  error_out:
  	SetPageError(page);
diff --cc fs/squashfs/lzo_wrapper.c
index 00f4dfc5f088,934c17e96590..000000000000
--- a/fs/squashfs/lzo_wrapper.c
+++ b/fs/squashfs/lzo_wrapper.c
@@@ -104,24 -99,24 +104,39 @@@ static int lzo_uncompress(struct squash
  		goto failed;
  
  	res = bytes = (int)out_len;
++<<<<<<< HEAD
 +	for (i = 0, buff = stream->output; bytes && i < pages; i++) {
 +		avail = min_t(int, bytes, PAGE_CACHE_SIZE);
 +		memcpy(buffer[i], buff, avail);
 +		buff += avail;
 +		bytes -= avail;
++=======
+ 	data = squashfs_first_page(output);
+ 	buff = stream->output;
+ 	while (data) {
+ 		if (bytes <= PAGE_SIZE) {
+ 			memcpy(data, buff, bytes);
+ 			break;
+ 		} else {
+ 			memcpy(data, buff, PAGE_SIZE);
+ 			buff += PAGE_SIZE;
+ 			bytes -= PAGE_SIZE;
+ 			data = squashfs_next_page(output);
+ 		}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
 -	squashfs_finish_page(output);
  
 +	mutex_unlock(&msblk->read_data_mutex);
  	return res;
  
 +block_release:
 +	for (; i < b; i++)
 +		put_bh(bh[i]);
 +
  failed:
 +	mutex_unlock(&msblk->read_data_mutex);
 +
 +	ERROR("lzo decompression failed, data probably corrupt\n");
  	return -EIO;
  }
  
diff --cc fs/squashfs/xz_wrapper.c
index 1760b7d108f6,6bfaef73d065..000000000000
--- a/fs/squashfs/xz_wrapper.c
+++ b/fs/squashfs/xz_wrapper.c
@@@ -117,8 -141,8 +117,13 @@@ static int squashfs_xz_uncompress(struc
  	stream->buf.in_pos = 0;
  	stream->buf.in_size = 0;
  	stream->buf.out_pos = 0;
++<<<<<<< HEAD
 +	stream->buf.out_size = PAGE_CACHE_SIZE;
 +	stream->buf.out = buffer[page++];
++=======
+ 	stream->buf.out_size = PAGE_SIZE;
+ 	stream->buf.out = squashfs_first_page(output);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	do {
  		if (stream->buf.in_pos == stream->buf.in_size && k < b) {
@@@ -134,11 -154,12 +139,20 @@@
  			offset = 0;
  		}
  
++<<<<<<< HEAD
 +		if (stream->buf.out_pos == stream->buf.out_size
 +							&& page < pages) {
 +			stream->buf.out = buffer[page++];
 +			stream->buf.out_pos = 0;
 +			total += PAGE_CACHE_SIZE;
++=======
+ 		if (stream->buf.out_pos == stream->buf.out_size) {
+ 			stream->buf.out = squashfs_next_page(output);
+ 			if (stream->buf.out != NULL) {
+ 				stream->buf.out_pos = 0;
+ 				total += PAGE_SIZE;
+ 			}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
  
  		xz_err = xz_dec_run(stream->state, &stream->buf);
diff --cc fs/squashfs/zlib_wrapper.c
index 55d918fd2d86,2ec24d128bce..000000000000
--- a/fs/squashfs/zlib_wrapper.c
+++ b/fs/squashfs/zlib_wrapper.c
@@@ -61,17 -62,15 +61,22 @@@ static void zlib_free(void *strm
  }
  
  
 -static int zlib_uncompress(struct squashfs_sb_info *msblk, void *strm,
 -	struct buffer_head **bh, int b, int offset, int length,
 -	struct squashfs_page_actor *output)
 +static int zlib_uncompress(struct squashfs_sb_info *msblk, void **buffer,
 +	struct buffer_head **bh, int b, int offset, int length, int srclength,
 +	int pages)
  {
 -	int zlib_err, zlib_init = 0, k = 0;
 -	z_stream *stream = strm;
 +	int zlib_err, zlib_init = 0;
 +	int k = 0, page = 0;
 +	z_stream *stream = msblk->stream;
 +
++<<<<<<< HEAD
 +	mutex_lock(&msblk->read_data_mutex);
  
 +	stream->avail_out = 0;
++=======
+ 	stream->avail_out = PAGE_SIZE;
+ 	stream->next_out = squashfs_first_page(output);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	stream->avail_in = 0;
  
  	do {
@@@ -87,9 -82,10 +92,16 @@@
  			offset = 0;
  		}
  
++<<<<<<< HEAD
 +		if (stream->avail_out == 0 && page < pages) {
 +			stream->next_out = buffer[page++];
 +			stream->avail_out = PAGE_CACHE_SIZE;
++=======
+ 		if (stream->avail_out == 0) {
+ 			stream->next_out = squashfs_next_page(output);
+ 			if (stream->next_out != NULL)
+ 				stream->avail_out = PAGE_SIZE;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
  
  		if (!zlib_init) {
diff --cc fs/sysv/dir.c
index 3799e8dac3eb,c0f0a3e643eb..000000000000
--- a/fs/sysv/dir.c
+++ b/fs/sysv/dir.c
@@@ -30,14 -30,9 +30,14 @@@ const struct file_operations sysv_dir_o
  static inline void dir_put_page(struct page *page)
  {
  	kunmap(page);
- 	page_cache_release(page);
+ 	put_page(page);
  }
  
 +static inline unsigned long dir_pages(struct inode *inode)
 +{
 +	return (inode->i_size+PAGE_CACHE_SIZE-1)>>PAGE_CACHE_SHIFT;
 +}
 +
  static int dir_commit_chunk(struct page *page, loff_t pos, unsigned len)
  {
  	struct address_space *mapping = page->mapping;
@@@ -65,18 -60,21 +65,25 @@@ static struct page * dir_get_page(struc
  	return page;
  }
  
 -static int sysv_readdir(struct file *file, struct dir_context *ctx)
 +static int sysv_readdir(struct file * filp, void * dirent, filldir_t filldir)
  {
 -	unsigned long pos = ctx->pos;
 -	struct inode *inode = file_inode(file);
 +	unsigned long pos = filp->f_pos;
 +	struct inode *inode = file_inode(filp);
  	struct super_block *sb = inode->i_sb;
 +	unsigned offset = pos & ~PAGE_CACHE_MASK;
 +	unsigned long n = pos >> PAGE_CACHE_SHIFT;
  	unsigned long npages = dir_pages(inode);
 -	unsigned offset;
 -	unsigned long n;
  
 -	ctx->pos = pos = (pos + SYSV_DIRSIZE-1) & ~(SYSV_DIRSIZE-1);
 +	pos = (pos + SYSV_DIRSIZE-1) & ~(SYSV_DIRSIZE-1);
  	if (pos >= inode->i_size)
++<<<<<<< HEAD
 +		goto done;
++=======
+ 		return 0;
+ 
+ 	offset = pos & ~PAGE_MASK;
+ 	n = pos >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	for ( ; n < npages; n++, offset = 0) {
  		char *kaddr, *limit;
@@@ -87,10 -85,9 +94,15 @@@
  			continue;
  		kaddr = (char *)page_address(page);
  		de = (struct sysv_dir_entry *)(kaddr+offset);
++<<<<<<< HEAD
 +		limit = kaddr + PAGE_CACHE_SIZE - SYSV_DIRSIZE;
 +		for ( ;(char*)de <= limit; de++) {
++=======
+ 		limit = kaddr + PAGE_SIZE - SYSV_DIRSIZE;
+ 		for ( ;(char*)de <= limit; de++, ctx->pos += sizeof(*de)) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			char *name = de->name;
 +			int over;
  
  			if (!de->inode)
  				continue;
diff --cc fs/ubifs/file.c
index 14374530784c,1a9c6640e604..000000000000
--- a/fs/ubifs/file.c
+++ b/fs/ubifs/file.c
@@@ -261,7 -260,8 +261,12 @@@ static int write_begin_slow(struct addr
  			err = do_readpage(page);
  			if (err) {
  				unlock_page(page);
++<<<<<<< HEAD
 +				page_cache_release(page);
++=======
+ 				put_page(page);
+ 				ubifs_release_budget(c, &req);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				return err;
  			}
  		}
@@@ -903,8 -903,9 +908,12 @@@ static int do_writepage(struct page *pa
  	struct ubifs_info *c = inode->i_sb->s_fs_info;
  
  #ifdef UBIFS_DEBUG
 -	struct ubifs_inode *ui = ubifs_inode(inode);
  	spin_lock(&ui->ui_lock);
++<<<<<<< HEAD
 +	ubifs_assert(page->index <= ui->synced_i_size << PAGE_CACHE_SIZE);
++=======
+ 	ubifs_assert(page->index <= ui->synced_i_size >> PAGE_SHIFT);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	spin_unlock(&ui->ui_lock);
  #endif
  
@@@ -1283,7 -1285,7 +1292,11 @@@ static void ubifs_invalidatepage(struc
  	struct ubifs_info *c = inode->i_sb->s_fs_info;
  
  	ubifs_assert(PagePrivate(page));
++<<<<<<< HEAD
 +	if (offset)
++=======
+ 	if (offset || length < PAGE_SIZE)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		/* Partial page remains dirty */
  		return;
  
diff --cc fs/ubifs/super.c
index 59ff78d90694,20daea9aa657..000000000000
--- a/fs/ubifs/super.c
+++ b/fs/ubifs/super.c
@@@ -2240,9 -2240,9 +2240,15 @@@ static int __init ubifs_init(void
  	 * We require that PAGE_CACHE_SIZE is greater-than-or-equal-to
  	 * UBIFS_BLOCK_SIZE. It is assumed that both are powers of 2.
  	 */
++<<<<<<< HEAD
 +	if (PAGE_CACHE_SIZE < UBIFS_BLOCK_SIZE) {
 +		ubifs_err("VFS page cache size is %u bytes, but UBIFS requires at least 4096 bytes",
 +			  (unsigned int)PAGE_CACHE_SIZE);
++=======
+ 	if (PAGE_SIZE < UBIFS_BLOCK_SIZE) {
+ 		pr_err("UBIFS error (pid %d): VFS page cache size is %u bytes, but UBIFS requires at least 4096 bytes",
+ 		       current->pid, (unsigned int)PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		return -EINVAL;
  	}
  
diff --cc fs/ufs/dir.c
index 3a75ca09c506,0b1457292734..000000000000
--- a/fs/ufs/dir.c
+++ b/fs/ufs/dir.c
@@@ -62,14 -62,9 +62,14 @@@ static int ufs_commit_chunk(struct pag
  static inline void ufs_put_page(struct page *page)
  {
  	kunmap(page);
- 	page_cache_release(page);
+ 	put_page(page);
  }
  
 +static inline unsigned long ufs_dir_pages(struct inode *inode)
 +{
 +	return (inode->i_size+PAGE_CACHE_SIZE-1)>>PAGE_CACHE_SHIFT;
 +}
 +
  ino_t ufs_inode_by_name(struct inode *dir, const struct qstr *qstr)
  {
  	ino_t res = 0;
@@@ -430,16 -427,16 +430,22 @@@ ufs_validate_entry(struct super_block *
   * This is blatantly stolen from ext2fs
   */
  static int
 -ufs_readdir(struct file *file, struct dir_context *ctx)
 +ufs_readdir(struct file *filp, void *dirent, filldir_t filldir)
  {
 -	loff_t pos = ctx->pos;
 -	struct inode *inode = file_inode(file);
 +	loff_t pos = filp->f_pos;
 +	struct inode *inode = file_inode(filp);
  	struct super_block *sb = inode->i_sb;
++<<<<<<< HEAD
 +	unsigned int offset = pos & ~PAGE_CACHE_MASK;
 +	unsigned long n = pos >> PAGE_CACHE_SHIFT;
 +	unsigned long npages = ufs_dir_pages(inode);
++=======
+ 	unsigned int offset = pos & ~PAGE_MASK;
+ 	unsigned long n = pos >> PAGE_SHIFT;
+ 	unsigned long npages = dir_pages(inode);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	unsigned chunk_mask = ~(UFS_SB(sb)->s_uspi->s_dirblksize - 1);
 -	int need_revalidate = file->f_version != inode->i_version;
 +	int need_revalidate = filp->f_version != inode->i_version;
  	unsigned flags = UFS_SB(sb)->s_flags;
  
  	UFSD("BEGIN\n");
@@@ -457,16 -454,16 +463,24 @@@
  			ufs_error(sb, __func__,
  				  "bad page in #%lu",
  				  inode->i_ino);
++<<<<<<< HEAD
 +			filp->f_pos += PAGE_CACHE_SIZE - offset;
++=======
+ 			ctx->pos += PAGE_SIZE - offset;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			return -EIO;
  		}
  		kaddr = page_address(page);
  		if (unlikely(need_revalidate)) {
  			if (offset) {
  				offset = ufs_validate_entry(sb, kaddr, offset, chunk_mask);
++<<<<<<< HEAD
 +				filp->f_pos = (n<<PAGE_CACHE_SHIFT) + offset;
++=======
+ 				ctx->pos = (n<<PAGE_SHIFT) + offset;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			}
 -			file->f_version = inode->i_version;
 +			filp->f_version = inode->i_version;
  			need_revalidate = 0;
  		}
  		de = (struct ufs_dir_entry *)(kaddr+offset);
diff --cc fs/ufs/inode.c
index 61e8a9b021dd,9f49431e798d..000000000000
--- a/fs/ufs/inode.c
+++ b/fs/ufs/inode.c
@@@ -902,9 -848,381 +902,371 @@@ void ufs_evict_inode(struct inode * ino
  	invalidate_inode_buffers(inode);
  	clear_inode(inode);
  
 -	if (want_delete)
 -		ufs_free_inode(inode);
 -}
 -
 -struct to_free {
 -	struct inode *inode;
 -	u64 to;
 -	unsigned count;
 -};
 -
 -static inline void free_data(struct to_free *ctx, u64 from, unsigned count)
 -{
 -	if (ctx->count && ctx->to != from) {
 -		ufs_free_blocks(ctx->inode, ctx->to - ctx->count, ctx->count);
 -		ctx->count = 0;
 +	if (want_delete) {
 +		lock_ufs(inode->i_sb);
 +		ufs_free_inode (inode);
 +		unlock_ufs(inode->i_sb);
  	}
 -	ctx->count += count;
 -	ctx->to = from + count;
  }
++<<<<<<< HEAD
++=======
+ 
+ #define DIRECT_BLOCK ((inode->i_size + uspi->s_bsize - 1) >> uspi->s_bshift)
+ #define DIRECT_FRAGMENT ((inode->i_size + uspi->s_fsize - 1) >> uspi->s_fshift)
+ 
+ static void ufs_trunc_direct(struct inode *inode)
+ {
+ 	struct ufs_inode_info *ufsi = UFS_I(inode);
+ 	struct super_block * sb;
+ 	struct ufs_sb_private_info * uspi;
+ 	void *p;
+ 	u64 frag1, frag2, frag3, frag4, block1, block2;
+ 	struct to_free ctx = {.inode = inode};
+ 	unsigned i, tmp;
+ 
+ 	UFSD("ENTER: ino %lu\n", inode->i_ino);
+ 
+ 	sb = inode->i_sb;
+ 	uspi = UFS_SB(sb)->s_uspi;
+ 
+ 	frag1 = DIRECT_FRAGMENT;
+ 	frag4 = min_t(u64, UFS_NDIR_FRAGMENT, ufsi->i_lastfrag);
+ 	frag2 = ((frag1 & uspi->s_fpbmask) ? ((frag1 | uspi->s_fpbmask) + 1) : frag1);
+ 	frag3 = frag4 & ~uspi->s_fpbmask;
+ 	block1 = block2 = 0;
+ 	if (frag2 > frag3) {
+ 		frag2 = frag4;
+ 		frag3 = frag4 = 0;
+ 	} else if (frag2 < frag3) {
+ 		block1 = ufs_fragstoblks (frag2);
+ 		block2 = ufs_fragstoblks (frag3);
+ 	}
+ 
+ 	UFSD("ino %lu, frag1 %llu, frag2 %llu, block1 %llu, block2 %llu,"
+ 	     " frag3 %llu, frag4 %llu\n", inode->i_ino,
+ 	     (unsigned long long)frag1, (unsigned long long)frag2,
+ 	     (unsigned long long)block1, (unsigned long long)block2,
+ 	     (unsigned long long)frag3, (unsigned long long)frag4);
+ 
+ 	if (frag1 >= frag2)
+ 		goto next1;
+ 
+ 	/*
+ 	 * Free first free fragments
+ 	 */
+ 	p = ufs_get_direct_data_ptr(uspi, ufsi, ufs_fragstoblks(frag1));
+ 	tmp = ufs_data_ptr_to_cpu(sb, p);
+ 	if (!tmp )
+ 		ufs_panic (sb, "ufs_trunc_direct", "internal error");
+ 	frag2 -= frag1;
+ 	frag1 = ufs_fragnum (frag1);
+ 
+ 	ufs_free_fragments(inode, tmp + frag1, frag2);
+ 
+ next1:
+ 	/*
+ 	 * Free whole blocks
+ 	 */
+ 	for (i = block1 ; i < block2; i++) {
+ 		p = ufs_get_direct_data_ptr(uspi, ufsi, i);
+ 		tmp = ufs_data_ptr_to_cpu(sb, p);
+ 		if (!tmp)
+ 			continue;
+ 		write_seqlock(&ufsi->meta_lock);
+ 		ufs_data_ptr_clear(uspi, p);
+ 		write_sequnlock(&ufsi->meta_lock);
+ 
+ 		free_data(&ctx, tmp, uspi->s_fpb);
+ 	}
+ 
+ 	free_data(&ctx, 0, 0);
+ 
+ 	if (frag3 >= frag4)
+ 		goto next3;
+ 
+ 	/*
+ 	 * Free last free fragments
+ 	 */
+ 	p = ufs_get_direct_data_ptr(uspi, ufsi, ufs_fragstoblks(frag3));
+ 	tmp = ufs_data_ptr_to_cpu(sb, p);
+ 	if (!tmp )
+ 		ufs_panic(sb, "ufs_truncate_direct", "internal error");
+ 	frag4 = ufs_fragnum (frag4);
+ 	write_seqlock(&ufsi->meta_lock);
+ 	ufs_data_ptr_clear(uspi, p);
+ 	write_sequnlock(&ufsi->meta_lock);
+ 
+ 	ufs_free_fragments (inode, tmp, frag4);
+  next3:
+ 
+ 	UFSD("EXIT: ino %lu\n", inode->i_ino);
+ }
+ 
+ static void free_full_branch(struct inode *inode, u64 ind_block, int depth)
+ {
+ 	struct super_block *sb = inode->i_sb;
+ 	struct ufs_sb_private_info *uspi = UFS_SB(sb)->s_uspi;
+ 	struct ufs_buffer_head *ubh = ubh_bread(sb, ind_block, uspi->s_bsize);
+ 	unsigned i;
+ 
+ 	if (!ubh)
+ 		return;
+ 
+ 	if (--depth) {
+ 		for (i = 0; i < uspi->s_apb; i++) {
+ 			void *p = ubh_get_data_ptr(uspi, ubh, i);
+ 			u64 block = ufs_data_ptr_to_cpu(sb, p);
+ 			if (block)
+ 				free_full_branch(inode, block, depth);
+ 		}
+ 	} else {
+ 		struct to_free ctx = {.inode = inode};
+ 
+ 		for (i = 0; i < uspi->s_apb; i++) {
+ 			void *p = ubh_get_data_ptr(uspi, ubh, i);
+ 			u64 block = ufs_data_ptr_to_cpu(sb, p);
+ 			if (block)
+ 				free_data(&ctx, block, uspi->s_fpb);
+ 		}
+ 		free_data(&ctx, 0, 0);
+ 	}
+ 
+ 	ubh_bforget(ubh);
+ 	ufs_free_blocks(inode, ind_block, uspi->s_fpb);
+ }
+ 
+ static void free_branch_tail(struct inode *inode, unsigned from, struct ufs_buffer_head *ubh, int depth)
+ {
+ 	struct super_block *sb = inode->i_sb;
+ 	struct ufs_sb_private_info *uspi = UFS_SB(sb)->s_uspi;
+ 	unsigned i;
+ 
+ 	if (--depth) {
+ 		for (i = from; i < uspi->s_apb ; i++) {
+ 			void *p = ubh_get_data_ptr(uspi, ubh, i);
+ 			u64 block = ufs_data_ptr_to_cpu(sb, p);
+ 			if (block) {
+ 				write_seqlock(&UFS_I(inode)->meta_lock);
+ 				ufs_data_ptr_clear(uspi, p);
+ 				write_sequnlock(&UFS_I(inode)->meta_lock);
+ 				ubh_mark_buffer_dirty(ubh);
+ 				free_full_branch(inode, block, depth);
+ 			}
+ 		}
+ 	} else {
+ 		struct to_free ctx = {.inode = inode};
+ 
+ 		for (i = from; i < uspi->s_apb; i++) {
+ 			void *p = ubh_get_data_ptr(uspi, ubh, i);
+ 			u64 block = ufs_data_ptr_to_cpu(sb, p);
+ 			if (block) {
+ 				write_seqlock(&UFS_I(inode)->meta_lock);
+ 				ufs_data_ptr_clear(uspi, p);
+ 				write_sequnlock(&UFS_I(inode)->meta_lock);
+ 				ubh_mark_buffer_dirty(ubh);
+ 				free_data(&ctx, block, uspi->s_fpb);
+ 			}
+ 		}
+ 		free_data(&ctx, 0, 0);
+ 	}
+ 	if (IS_SYNC(inode) && ubh_buffer_dirty(ubh))
+ 		ubh_sync_block(ubh);
+ 	ubh_brelse(ubh);
+ }
+ 
+ static int ufs_alloc_lastblock(struct inode *inode, loff_t size)
+ {
+ 	int err = 0;
+ 	struct super_block *sb = inode->i_sb;
+ 	struct address_space *mapping = inode->i_mapping;
+ 	struct ufs_sb_private_info *uspi = UFS_SB(sb)->s_uspi;
+ 	unsigned i, end;
+ 	sector_t lastfrag;
+ 	struct page *lastpage;
+ 	struct buffer_head *bh;
+ 	u64 phys64;
+ 
+ 	lastfrag = (size + uspi->s_fsize - 1) >> uspi->s_fshift;
+ 
+ 	if (!lastfrag)
+ 		goto out;
+ 
+ 	lastfrag--;
+ 
+ 	lastpage = ufs_get_locked_page(mapping, lastfrag >>
+ 				       (PAGE_SHIFT - inode->i_blkbits));
+        if (IS_ERR(lastpage)) {
+                err = -EIO;
+                goto out;
+        }
+ 
+        end = lastfrag & ((1 << (PAGE_SHIFT - inode->i_blkbits)) - 1);
+        bh = page_buffers(lastpage);
+        for (i = 0; i < end; ++i)
+                bh = bh->b_this_page;
+ 
+ 
+        err = ufs_getfrag_block(inode, lastfrag, bh, 1);
+ 
+        if (unlikely(err))
+ 	       goto out_unlock;
+ 
+        if (buffer_new(bh)) {
+ 	       clear_buffer_new(bh);
+ 	       unmap_underlying_metadata(bh->b_bdev,
+ 					 bh->b_blocknr);
+ 	       /*
+ 		* we do not zeroize fragment, because of
+ 		* if it maped to hole, it already contains zeroes
+ 		*/
+ 	       set_buffer_uptodate(bh);
+ 	       mark_buffer_dirty(bh);
+ 	       set_page_dirty(lastpage);
+        }
+ 
+        if (lastfrag >= UFS_IND_FRAGMENT) {
+ 	       end = uspi->s_fpb - ufs_fragnum(lastfrag) - 1;
+ 	       phys64 = bh->b_blocknr + 1;
+ 	       for (i = 0; i < end; ++i) {
+ 		       bh = sb_getblk(sb, i + phys64);
+ 		       lock_buffer(bh);
+ 		       memset(bh->b_data, 0, sb->s_blocksize);
+ 		       set_buffer_uptodate(bh);
+ 		       mark_buffer_dirty(bh);
+ 		       unlock_buffer(bh);
+ 		       sync_dirty_buffer(bh);
+ 		       brelse(bh);
+ 	       }
+        }
+ out_unlock:
+        ufs_put_locked_page(lastpage);
+ out:
+        return err;
+ }
+ 
+ static void __ufs_truncate_blocks(struct inode *inode)
+ {
+ 	struct ufs_inode_info *ufsi = UFS_I(inode);
+ 	struct super_block *sb = inode->i_sb;
+ 	struct ufs_sb_private_info *uspi = UFS_SB(sb)->s_uspi;
+ 	unsigned offsets[4];
+ 	int depth = ufs_block_to_path(inode, DIRECT_BLOCK, offsets);
+ 	int depth2;
+ 	unsigned i;
+ 	struct ufs_buffer_head *ubh[3];
+ 	void *p;
+ 	u64 block;
+ 
+ 	if (!depth)
+ 		return;
+ 
+ 	/* find the last non-zero in offsets[] */
+ 	for (depth2 = depth - 1; depth2; depth2--)
+ 		if (offsets[depth2])
+ 			break;
+ 
+ 	mutex_lock(&ufsi->truncate_mutex);
+ 	if (depth == 1) {
+ 		ufs_trunc_direct(inode);
+ 		offsets[0] = UFS_IND_BLOCK;
+ 	} else {
+ 		/* get the blocks that should be partially emptied */
+ 		p = ufs_get_direct_data_ptr(uspi, ufsi, offsets[0]);
+ 		for (i = 0; i < depth2; i++) {
+ 			offsets[i]++;	/* next branch is fully freed */
+ 			block = ufs_data_ptr_to_cpu(sb, p);
+ 			if (!block)
+ 				break;
+ 			ubh[i] = ubh_bread(sb, block, uspi->s_bsize);
+ 			if (!ubh[i]) {
+ 				write_seqlock(&ufsi->meta_lock);
+ 				ufs_data_ptr_clear(uspi, p);
+ 				write_sequnlock(&ufsi->meta_lock);
+ 				break;
+ 			}
+ 			p = ubh_get_data_ptr(uspi, ubh[i], offsets[i + 1]);
+ 		}
+ 		while (i--)
+ 			free_branch_tail(inode, offsets[i + 1], ubh[i], depth - i - 1);
+ 	}
+ 	for (i = offsets[0]; i <= UFS_TIND_BLOCK; i++) {
+ 		p = ufs_get_direct_data_ptr(uspi, ufsi, i);
+ 		block = ufs_data_ptr_to_cpu(sb, p);
+ 		if (block) {
+ 			write_seqlock(&ufsi->meta_lock);
+ 			ufs_data_ptr_clear(uspi, p);
+ 			write_sequnlock(&ufsi->meta_lock);
+ 			free_full_branch(inode, block, i - UFS_IND_BLOCK + 1);
+ 		}
+ 	}
+ 	ufsi->i_lastfrag = DIRECT_FRAGMENT;
+ 	mark_inode_dirty(inode);
+ 	mutex_unlock(&ufsi->truncate_mutex);
+ }
+ 
+ static int ufs_truncate(struct inode *inode, loff_t size)
+ {
+ 	int err = 0;
+ 
+ 	UFSD("ENTER: ino %lu, i_size: %llu, old_i_size: %llu\n",
+ 	     inode->i_ino, (unsigned long long)size,
+ 	     (unsigned long long)i_size_read(inode));
+ 
+ 	if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+ 	      S_ISLNK(inode->i_mode)))
+ 		return -EINVAL;
+ 	if (IS_APPEND(inode) || IS_IMMUTABLE(inode))
+ 		return -EPERM;
+ 
+ 	err = ufs_alloc_lastblock(inode, size);
+ 
+ 	if (err)
+ 		goto out;
+ 
+ 	block_truncate_page(inode->i_mapping, size, ufs_getfrag_block);
+ 
+ 	truncate_setsize(inode, size);
+ 
+ 	__ufs_truncate_blocks(inode);
+ 	inode->i_mtime = inode->i_ctime = CURRENT_TIME_SEC;
+ 	mark_inode_dirty(inode);
+ out:
+ 	UFSD("EXIT: err %d\n", err);
+ 	return err;
+ }
+ 
+ void ufs_truncate_blocks(struct inode *inode)
+ {
+ 	if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+ 	      S_ISLNK(inode->i_mode)))
+ 		return;
+ 	if (IS_APPEND(inode) || IS_IMMUTABLE(inode))
+ 		return;
+ 	__ufs_truncate_blocks(inode);
+ }
+ 
+ int ufs_setattr(struct dentry *dentry, struct iattr *attr)
+ {
+ 	struct inode *inode = d_inode(dentry);
+ 	unsigned int ia_valid = attr->ia_valid;
+ 	int error;
+ 
+ 	error = inode_change_ok(inode, attr);
+ 	if (error)
+ 		return error;
+ 
+ 	if (ia_valid & ATTR_SIZE && attr->ia_size != inode->i_size) {
+ 		error = ufs_truncate(inode, attr->ia_size);
+ 		if (error)
+ 			return error;
+ 	}
+ 
+ 	setattr_copy(inode, attr);
+ 	mark_inode_dirty(inode);
+ 	return 0;
+ }
+ 
+ const struct inode_operations ufs_file_inode_operations = {
+ 	.setattr = ufs_setattr,
+ };
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
diff --cc fs/ufs/namei.c
index 90d74b8f8eba,a1559f762805..000000000000
--- a/fs/ufs/namei.c
+++ b/fs/ufs/namei.c
@@@ -315,7 -301,12 +315,16 @@@ static int ufs_rename(struct inode *old
  	mark_inode_dirty(old_inode);
  
  	if (dir_de) {
++<<<<<<< HEAD
 +		ufs_set_link(old_inode, dir_de, dir_page, new_dir);
++=======
+ 		if (old_dir != new_dir)
+ 			ufs_set_link(old_inode, dir_de, dir_page, new_dir, 0);
+ 		else {
+ 			kunmap(dir_page);
+ 			put_page(dir_page);
+ 		}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		inode_dec_link_count(old_dir);
  	}
  	return 0;
diff --cc fs/xfs/xfs_aops.c
index acf6c4a54883,5b8ae03ba855..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -989,8 -924,10 +989,13 @@@ xfs_vm_writepage
  	 * |     desired writeback range    |      see else    |
  	 * ---------------------------------^------------------|
  	 */
++<<<<<<< HEAD
++=======
+ 	offset = i_size_read(inode);
+ 	end_index = offset >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (page->index < end_index)
- 		end_offset = (xfs_off_t)(page->index + 1) << PAGE_CACHE_SHIFT;
+ 		end_offset = (xfs_off_t)(page->index + 1) << PAGE_SHIFT;
  	else {
  		/*
  		 * Check whether the page to write out is beyond or straddles
@@@ -1770,9 -1475,10 +1775,9 @@@ xfs_vm_write_failed
  	loff_t			block_offset;
  	loff_t			block_start;
  	loff_t			block_end;
- 	loff_t			from = pos & (PAGE_CACHE_SIZE - 1);
+ 	loff_t			from = pos & (PAGE_SIZE - 1);
  	loff_t			to = from + len;
  	struct buffer_head	*bh, *head;
 -	struct xfs_mount	*mp = XFS_I(inode)->i_mount;
  
  	/*
  	 * The request pos offset might be 32 or 64 bit, this is all fine
@@@ -1851,9 -1558,10 +1856,9 @@@ xfs_vm_write_begin
  	struct page		**pagep,
  	void			**fsdata)
  {
- 	pgoff_t			index = pos >> PAGE_CACHE_SHIFT;
+ 	pgoff_t			index = pos >> PAGE_SHIFT;
  	struct page		*page;
  	int			status;
 -	struct xfs_mount	*mp = XFS_I(mapping->host)->i_mount;
  
  	ASSERT(len <= PAGE_CACHE_SIZE);
  
diff --cc fs/xfs/xfs_bmap_util.c
index 935fc7ad67c0,3b6309865c65..000000000000
--- a/fs/xfs/xfs_bmap_util.c
+++ b/fs/xfs/xfs_bmap_util.c
@@@ -1467,7 -1466,7 +1467,11 @@@ xfs_collapse_file_space
  	if (error)
  		return error;
  	error = invalidate_inode_pages2_range(VFS_I(ip)->i_mapping,
++<<<<<<< HEAD
 +					(offset + len) >> PAGE_CACHE_SHIFT, -1);
++=======
+ 					offset >> PAGE_SHIFT, -1);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (error)
  		return error;
  
diff --cc fs/xfs/xfs_file.c
index 815d8f3721f9,569938a4a357..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -862,9 -792,22 +862,27 @@@ xfs_file_dio_aio_write
  	}
  
  	trace_xfs_file_direct_write(ip, count, iocb->ki_pos, 0);
 +	ret = generic_file_direct_write(iocb, iovp,
 +			&nr_segs, pos, &iocb->ki_pos, count, ocount);
  
++<<<<<<< HEAD
++=======
+ 	data = *from;
+ 	ret = mapping->a_ops->direct_IO(iocb, &data, pos);
+ 
+ 	/* see generic_file_direct_write() for why this is necessary */
+ 	if (mapping->nrpages) {
+ 		invalidate_inode_pages2_range(mapping,
+ 					      pos >> PAGE_SHIFT,
+ 					      end >> PAGE_SHIFT);
+ 	}
+ 
+ 	if (ret > 0) {
+ 		pos += ret;
+ 		iov_iter_advance(from, ret);
+ 		iocb->ki_pos = pos;
+ 	}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  out:
  	xfs_rw_iunlock(ip, iolock);
  
diff --cc include/linux/fs.h
index 98b7caca3399,304991a80e23..000000000000
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@@ -2191,15 -2067,11 +2191,19 @@@ static inline struct inode *file_inode(
  /* /sys/fs */
  extern struct kobject *fs_kobj;
  
++<<<<<<< HEAD
 +#define MAX_RW_COUNT (INT_MAX & PAGE_CACHE_MASK)
 +extern int rw_verify_area(int, struct file *, loff_t *, size_t);
++=======
+ #define MAX_RW_COUNT (INT_MAX & PAGE_MASK)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
 +
 +#define FLOCK_VERIFY_READ  1
 +#define FLOCK_VERIFY_WRITE 2
  
 -#ifdef CONFIG_MANDATORY_FILE_LOCKING
 +#ifdef CONFIG_FILE_LOCKING
  extern int locks_mandatory_locked(struct file *);
 -extern int locks_mandatory_area(struct inode *, struct file *, loff_t, loff_t, unsigned char);
 +extern int locks_mandatory_area(int, struct inode *, struct file *, loff_t, size_t);
  
  /*
   * Candidates for mandatory locking have the setgid bit set
diff --cc include/linux/pagemap.h
index 8576311e4195,f396ccb900cc..000000000000
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@@ -300,10 -384,21 +300,24 @@@ static inline struct page *read_mapping
   */
  static inline pgoff_t page_to_pgoff(struct page *page)
  {
 -	pgoff_t pgoff;
 -
  	if (unlikely(PageHeadHuge(page)))
  		return page->index << compound_order(page);
++<<<<<<< HEAD
 +	else
 +		return page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
++=======
+ 
+ 	if (likely(!PageTransTail(page)))
+ 		return page->index;
+ 
+ 	/*
+ 	 *  We don't initialize ->index for tail pages: calculate based on
+ 	 *  head page
+ 	 */
+ 	pgoff = compound_head(page)->index;
+ 	pgoff += page - compound_head(page);
+ 	return pgoff;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  /*
@@@ -580,4 -669,10 +594,13 @@@ static inline int add_to_page_cache(str
  	return error;
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long dir_pages(struct inode *inode)
+ {
+ 	return (unsigned long)(inode->i_size + PAGE_SIZE - 1) >>
+ 			       PAGE_SHIFT;
+ }
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  #endif /* _LINUX_PAGEMAP_H */
diff --cc kernel/events/uprobes.c
index 9c0b5accc271,7edc95edfaee..000000000000
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@@ -277,14 -320,8 +277,18 @@@ retry
  	copy_highpage(new_page, old_page);
  	copy_to_page(new_page, vaddr, &opcode, UPROBE_SWBP_INSN_SIZE);
  
 +	ret = anon_vma_prepare(vma);
 +	if (ret)
 +		goto put_new;
 +
  	ret = __replace_page(vma, vaddr, old_page, new_page);
++<<<<<<< HEAD
 +
 +put_new:
 +	page_cache_release(new_page);
++=======
+ 	put_page(new_page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  put_old:
  	put_page(old_page);
  
@@@ -496,14 -533,15 +500,21 @@@ static int __copy_insn(struct address_s
  			void *insn, int nbytes, loff_t offset)
  {
  	struct page *page;
 +
 +	if (!mapping->a_ops->readpage)
 +		return -EIO;
  	/*
 -	 * Ensure that the page that has the original instruction is populated
 -	 * and in page-cache. If ->readpage == NULL it must be shmem_mapping(),
 -	 * see uprobe_register().
 +	 * Ensure that the page that has the original instruction is
 +	 * populated and in page-cache.
  	 */
++<<<<<<< HEAD
 +	page = read_mapping_page(mapping, offset >> PAGE_CACHE_SHIFT, filp);
++=======
+ 	if (mapping->a_ops->readpage)
+ 		page = read_mapping_page(mapping, offset >> PAGE_SHIFT, filp);
+ 	else
+ 		page = shmem_read_mapping_page(mapping, offset >> PAGE_SHIFT);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	if (IS_ERR(page))
  		return PTR_ERR(page);
  
diff --cc mm/fadvise.c
index 1f1925fcb9ab,b8024fa7101d..000000000000
--- a/mm/fadvise.c
+++ b/mm/fadvise.c
@@@ -119,9 -119,13 +119,19 @@@ SYSCALL_DEFINE4(fadvise64_64, int, fd, 
  			__filemap_fdatawrite_range(mapping, offset, endbyte,
  						   WB_SYNC_NONE);
  
++<<<<<<< HEAD
 +		/* First and last FULL page! */
 +		start_index = (offset+(PAGE_CACHE_SIZE-1)) >> PAGE_CACHE_SHIFT;
 +		end_index = (endbyte >> PAGE_CACHE_SHIFT);
++=======
+ 		/*
+ 		 * First and last FULL page! Partial pages are deliberately
+ 		 * preserved on the expectation that it is better to preserve
+ 		 * needed memory than to discard unneeded memory.
+ 		 */
+ 		start_index = (offset+(PAGE_SIZE-1)) >> PAGE_SHIFT;
+ 		end_index = (endbyte >> PAGE_SHIFT);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  		if (end_index >= start_index) {
  			unsigned long count = invalidate_mapping_pages(mapping,
diff --cc mm/filemap.c
index c30a5bf58143,f2479af09da9..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -592,39 -630,52 +592,67 @@@ static int __add_to_page_cache_locked(s
  				      pgoff_t offset, gfp_t gfp_mask,
  				      void **shadowp)
  {
 -	int huge = PageHuge(page);
 -	struct mem_cgroup *memcg;
  	int error;
  
 -	VM_BUG_ON_PAGE(!PageLocked(page), page);
 -	VM_BUG_ON_PAGE(PageSwapBacked(page), page);
 +	VM_BUG_ON(!PageLocked(page));
 +	VM_BUG_ON(PageSwapBacked(page));
  
 -	if (!huge) {
 -		error = mem_cgroup_try_charge(page, current->mm,
 -					      gfp_mask, &memcg, false);
 -		if (error)
 -			return error;
 -	}
 +	error = mem_cgroup_cache_charge(page, current->mm,
 +					gfp_mask & GFP_RECLAIM_MASK);
 +	if (error)
 +		goto out;
  
  	error = radix_tree_maybe_preload(gfp_mask & ~__GFP_HIGHMEM);
 -	if (error) {
 -		if (!huge)
 -			mem_cgroup_cancel_charge(page, memcg, false);
 -		return error;
 -	}
 -
 +	if (error == 0) {
 +		page_cache_get(page);
 +		page->mapping = mapping;
 +		page->index = offset;
 +
++<<<<<<< HEAD
 +		spin_lock_irq(&mapping->tree_lock);
 +		error = page_cache_tree_insert(mapping, page, shadowp);
 +		if (likely(!error)) {
 +			__inc_zone_page_state(page, NR_FILE_PAGES);
 +			spin_unlock_irq(&mapping->tree_lock);
 +			trace_mm_filemap_add_to_page_cache(page);
 +		} else {
 +			page->mapping = NULL;
 +			/* Leave page->index set: truncation relies upon it */
 +			spin_unlock_irq(&mapping->tree_lock);
 +			mem_cgroup_uncharge_cache_page(page);
 +			page_cache_release(page);
 +		}
 +		radix_tree_preload_end();
 +	} else
 +		mem_cgroup_uncharge_cache_page(page);
 +out:
++=======
+ 	get_page(page);
+ 	page->mapping = mapping;
+ 	page->index = offset;
+ 
+ 	spin_lock_irq(&mapping->tree_lock);
+ 	error = page_cache_tree_insert(mapping, page, shadowp);
+ 	radix_tree_preload_end();
+ 	if (unlikely(error))
+ 		goto err_insert;
+ 
+ 	/* hugetlb pages do not participate in page cache accounting. */
+ 	if (!huge)
+ 		__inc_zone_page_state(page, NR_FILE_PAGES);
+ 	spin_unlock_irq(&mapping->tree_lock);
+ 	if (!huge)
+ 		mem_cgroup_commit_charge(page, memcg, false, false);
+ 	trace_mm_filemap_add_to_page_cache(page);
+ 	return 0;
+ err_insert:
+ 	page->mapping = NULL;
+ 	/* Leave page->index set: truncation relies upon it */
+ 	spin_unlock_irq(&mapping->tree_lock);
+ 	if (!huge)
+ 		mem_cgroup_cancel_charge(page, memcg, false);
+ 	put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	return error;
  }
  
@@@ -1073,71 -1128,88 +1101,106 @@@ repeat
  	}
  	return page;
  }
 -EXPORT_SYMBOL(find_lock_entry);
 +EXPORT_SYMBOL(__find_lock_page);
  
  /**
 - * pagecache_get_page - find and get a page reference
 + * find_lock_page - locate, pin and lock a pagecache page
   * @mapping: the address_space to search
   * @offset: the page index
 - * @fgp_flags: PCG flags
 - * @gfp_mask: gfp mask to use for the page cache data page allocation
 - *
 - * Looks up the page cache slot at @mapping & @offset.
   *
 - * PCG flags modify how the page is returned.
 - *
 - * FGP_ACCESSED: the page will be marked accessed
 - * FGP_LOCK: Page is return locked
 - * FGP_CREAT: If page is not present then a new page is allocated using
 - *		@gfp_mask and added to the page cache and the VM's LRU
 - *		list. The page is returned locked and with an increased
 - *		refcount. Otherwise, %NULL is returned.
 + * Looks up the page cache slot at @mapping & @offset.  If there is a
 + * page cache page, it is returned locked and with an increased
 + * refcount.
   *
 - * If FGP_LOCK or FGP_CREAT are specified then the function may sleep even
 - * if the GFP flags specified for FGP_CREAT are atomic.
 + * Otherwise, %NULL is returned.
   *
 - * If there is a page cache page, it is returned with an increased refcount.
 + * find_lock_page() may sleep.
   */
 -struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 -	int fgp_flags, gfp_t gfp_mask)
 +struct page *find_lock_page(struct address_space *mapping, pgoff_t offset)
  {
 -	struct page *page;
 +	struct page *page = __find_lock_page(mapping, offset);
  
 -repeat:
 -	page = find_get_entry(mapping, offset);
  	if (radix_tree_exceptional_entry(page))
  		page = NULL;
++<<<<<<< HEAD
 +	return page;
 +}
 +EXPORT_SYMBOL(find_lock_page);
++=======
+ 	if (!page)
+ 		goto no_page;
+ 
+ 	if (fgp_flags & FGP_LOCK) {
+ 		if (fgp_flags & FGP_NOWAIT) {
+ 			if (!trylock_page(page)) {
+ 				put_page(page);
+ 				return NULL;
+ 			}
+ 		} else {
+ 			lock_page(page);
+ 		}
+ 
+ 		/* Has the page been truncated? */
+ 		if (unlikely(page->mapping != mapping)) {
+ 			unlock_page(page);
+ 			put_page(page);
+ 			goto repeat;
+ 		}
+ 		VM_BUG_ON_PAGE(page->index != offset, page);
+ 	}
+ 
+ 	if (page && (fgp_flags & FGP_ACCESSED))
+ 		mark_page_accessed(page);
+ 
+ no_page:
+ 	if (!page && (fgp_flags & FGP_CREAT)) {
+ 		int err;
+ 		if ((fgp_flags & FGP_WRITE) && mapping_cap_account_dirty(mapping))
+ 			gfp_mask |= __GFP_WRITE;
+ 		if (fgp_flags & FGP_NOFS)
+ 			gfp_mask &= ~__GFP_FS;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 +/**
 + * find_or_create_page - locate or add a pagecache page
 + * @mapping: the page's address_space
 + * @index: the page's index into the mapping
 + * @gfp_mask: page allocation mode
 + *
 + * Looks up the page cache slot at @mapping & @offset.  If there is a
 + * page cache page, it is returned locked and with an increased
 + * refcount.
 + *
 + * If the page is not present, a new page is allocated using @gfp_mask
 + * and added to the page cache and the VM's LRU list.  The page is
 + * returned locked and with an increased refcount.
 + *
 + * On memory exhaustion, %NULL is returned.
 + *
 + * find_or_create_page() may sleep, even if @gfp_flags specifies an
 + * atomic allocation!
 + */
 +struct page *find_or_create_page(struct address_space *mapping,
 +		pgoff_t index, gfp_t gfp_mask)
 +{
 +	struct page *page;
 +	int err;
 +repeat:
 +	page = find_lock_page(mapping, index);
 +	if (!page) {
  		page = __page_cache_alloc(gfp_mask);
  		if (!page)
  			return NULL;
 -
 -		if (WARN_ON_ONCE(!(fgp_flags & FGP_LOCK)))
 -			fgp_flags |= FGP_LOCK;
 -
 -		/* Init accessed so avoid atomic mark_page_accessed later */
 -		if (fgp_flags & FGP_ACCESSED)
 -			__SetPageReferenced(page);
 -
 -		err = add_to_page_cache_lru(page, mapping, offset,
 -				gfp_mask & GFP_RECLAIM_MASK);
 +		/*
 +		 * We want a regular kernel memory (not highmem or DMA etc)
 +		 * allocation for the radix tree nodes, but we need to honour
 +		 * the context-specific requirements the caller has asked for.
 +		 * GFP_RECLAIM_MASK collects those requirements.
 +		 */
 +		err = add_to_page_cache_lru(page, mapping, index,
 +			(gfp_mask & GFP_RECLAIM_MASK));
  		if (unlikely(err)) {
- 			page_cache_release(page);
+ 			put_page(page);
  			page = NULL;
  			if (err == -EEXIST)
  				goto repeat;
@@@ -1742,13 -1608,13 +1805,21 @@@ static void do_generic_file_read(struc
  	pgoff_t prev_index;
  	unsigned long offset;      /* offset into pagecache page */
  	unsigned int prev_offset;
 -	int error = 0;
 +	int error;
  
++<<<<<<< HEAD
 +	index = *ppos >> PAGE_CACHE_SHIFT;
 +	prev_index = ra->prev_pos >> PAGE_CACHE_SHIFT;
 +	prev_offset = ra->prev_pos & (PAGE_CACHE_SIZE-1);
 +	last_index = (*ppos + desc->count + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;
 +	offset = *ppos & ~PAGE_CACHE_MASK;
++=======
+ 	index = *ppos >> PAGE_SHIFT;
+ 	prev_index = ra->prev_pos >> PAGE_SHIFT;
+ 	prev_offset = ra->prev_pos & (PAGE_SIZE-1);
+ 	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
+ 	offset = *ppos & ~PAGE_MASK;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	for (;;) {
  		struct page *page;
@@@ -1773,7 -1639,16 +1844,20 @@@ find_page
  					index, last_index - index);
  		}
  		if (!PageUptodate(page)) {
++<<<<<<< HEAD
 +			if (inode->i_blkbits == PAGE_CACHE_SHIFT ||
++=======
+ 			/*
+ 			 * See comment in do_read_cache_page on why
+ 			 * wait_on_page_locked is used to avoid unnecessarily
+ 			 * serialisations and why it's safe.
+ 			 */
+ 			wait_on_page_locked_killable(page);
+ 			if (PageUptodate(page))
+ 				goto page_ok;
+ 
+ 			if (inode->i_blkbits == PAGE_SHIFT ||
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  					!mapping->a_ops->is_partially_uptodate)
  				goto page_not_up_to_date;
  			if (!trylock_page(page))
@@@ -1832,23 -1707,23 +1916,35 @@@ page_ok
  		/*
  		 * Ok, we have the page, and it's up-to-date, so
  		 * now we can copy it to user space...
 +		 *
 +		 * The actor routine returns how many bytes were actually used..
 +		 * NOTE! This may not be the same as how much of a user buffer
 +		 * we filled up (we may be padding etc), so we can only update
 +		 * "pos" here (the actor routine has to update the user buffer
 +		 * pointers and the remaining count).
  		 */
 -
 -		ret = copy_page_to_iter(page, offset, nr, iter);
 +		ret = actor(desc, page, offset, nr);
  		offset += ret;
- 		index += offset >> PAGE_CACHE_SHIFT;
- 		offset &= ~PAGE_CACHE_MASK;
+ 		index += offset >> PAGE_SHIFT;
+ 		offset &= ~PAGE_MASK;
  		prev_offset = offset;
  
++<<<<<<< HEAD
 +		page_cache_release(page);
 +		if (ret == nr && desc->count)
 +			continue;
 +		goto out;
++=======
+ 		put_page(page);
+ 		written += ret;
+ 		if (!iov_iter_count(iter))
+ 			goto out;
+ 		if (ret < nr) {
+ 			error = -EFAULT;
+ 			goto out;
+ 		}
+ 		continue;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  page_not_up_to_date:
  		/* Get exclusive access to the page ... */
@@@ -1882,7 -1757,8 +1978,12 @@@ readpage
  
  		if (unlikely(error)) {
  			if (error == AOP_TRUNCATED_PAGE) {
++<<<<<<< HEAD
 +				page_cache_release(page);
++=======
+ 				put_page(page);
+ 				error = 0;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				goto find_page;
  			}
  			goto readpage_error;
@@@ -1913,8 -1789,7 +2014,12 @@@
  
  readpage_error:
  		/* UHHUH! A synchronous read error occurred. Report it */
++<<<<<<< HEAD
 +		desc->error = error;
 +		page_cache_release(page);
++=======
+ 		put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		goto out;
  
  no_cached_page:
@@@ -1924,16 -1799,17 +2029,22 @@@
  		 */
  		page = page_cache_alloc_cold(mapping);
  		if (!page) {
 -			error = -ENOMEM;
 +			desc->error = -ENOMEM;
  			goto out;
  		}
 -		error = add_to_page_cache_lru(page, mapping, index,
 -				mapping_gfp_constraint(mapping, GFP_KERNEL));
 +		error = add_to_page_cache_lru(page, mapping,
 +						index, GFP_KERNEL);
  		if (error) {
++<<<<<<< HEAD
 +			page_cache_release(page);
 +			if (error == -EEXIST)
++=======
+ 			put_page(page);
+ 			if (error == -EEXIST) {
+ 				error = 0;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				goto find_page;
 -			}
 +			desc->error = error;
  			goto out;
  		}
  		goto readpage;
@@@ -1941,98 -1817,20 +2052,98 @@@
  
  out:
  	ra->prev_pos = prev_index;
- 	ra->prev_pos <<= PAGE_CACHE_SHIFT;
+ 	ra->prev_pos <<= PAGE_SHIFT;
  	ra->prev_pos |= prev_offset;
  
- 	*ppos = ((loff_t)index << PAGE_CACHE_SHIFT) + offset;
+ 	*ppos = ((loff_t)index << PAGE_SHIFT) + offset;
  	file_accessed(filp);
 -	return written ? written : error;
  }
  
 +int file_read_actor(read_descriptor_t *desc, struct page *page,
 +			unsigned long offset, unsigned long size)
 +{
 +	char *kaddr;
 +	unsigned long left, count = desc->count;
 +
 +	if (size > count)
 +		size = count;
 +
 +	/*
 +	 * Faults on the destination of a read are common, so do it before
 +	 * taking the kmap.
 +	 */
 +	if (!fault_in_pages_writeable(desc->arg.buf, size)) {
 +		kaddr = kmap_atomic(page);
 +		left = __copy_to_user_inatomic(desc->arg.buf,
 +						kaddr + offset, size);
 +		kunmap_atomic(kaddr);
 +		if (left == 0)
 +			goto success;
 +	}
 +
 +	/* Do it the slow way */
 +	kaddr = kmap(page);
 +	left = __copy_to_user(desc->arg.buf, kaddr + offset, size);
 +	kunmap(page);
 +
 +	if (left) {
 +		size -= left;
 +		desc->error = -EFAULT;
 +	}
 +success:
 +	desc->count = count - size;
 +	desc->written += size;
 +	desc->arg.buf += size;
 +	return size;
 +}
 +
 +/*
 + * Performs necessary checks before doing a write
 + * @iov:	io vector request
 + * @nr_segs:	number of segments in the iovec
 + * @count:	number of bytes to write
 + * @access_flags: type of access: %VERIFY_READ or %VERIFY_WRITE
 + *
 + * Adjust number of segments and amount of bytes to write (nr_segs should be
 + * properly initialized first). Returns appropriate error code that caller
 + * should return or zero in case that write should be allowed.
 + */
 +int generic_segment_checks(const struct iovec *iov,
 +			unsigned long *nr_segs, size_t *count, int access_flags)
 +{
 +	unsigned long   seg;
 +	size_t cnt = 0;
 +	for (seg = 0; seg < *nr_segs; seg++) {
 +		const struct iovec *iv = &iov[seg];
 +
 +		/*
 +		 * If any segment has a negative length, or the cumulative
 +		 * length ever wraps negative then return -EINVAL.
 +		 */
 +		cnt += iv->iov_len;
 +		if (unlikely((ssize_t)(cnt|iv->iov_len) < 0))
 +			return -EINVAL;
 +		if (access_ok(access_flags, iv->iov_base, iv->iov_len))
 +			continue;
 +		if (seg == 0)
 +			return -EFAULT;
 +		*nr_segs = seg;
 +		cnt -= iv->iov_len;	/* This segment is no good */
 +		break;
 +	}
 +	*count = cnt;
 +	return 0;
 +}
 +EXPORT_SYMBOL(generic_segment_checks);
 +
  /**
 - * generic_file_read_iter - generic filesystem read routine
 + * generic_file_aio_read - generic filesystem read routine
   * @iocb:	kernel I/O control block
 - * @iter:	destination for the data read
 + * @iov:	io vector request
 + * @nr_segs:	number of segments in the iovec
 + * @pos:	current file position
   *
 - * This is the "read_iter()" routine for all filesystems
 + * This is the "read()" routine for all filesystems
   * that can use the page cache directly.
   */
  ssize_t
@@@ -2160,10 -1912,10 +2271,10 @@@ static int page_cache_read(struct file 
  		else if (ret == -EEXIST)
  			ret = 0; /* losing race to add is OK */
  
- 		page_cache_release(page);
+ 		put_page(page);
  
  	} while (ret == AOP_TRUNCATED_PAGE);
 -
 +		
  	return ret;
  }
  
@@@ -2255,11 -2019,11 +2366,16 @@@ int filemap_fault(struct vm_area_struc
  	struct inode *inode = mapping->host;
  	pgoff_t offset = vmf->pgoff;
  	struct page *page;
 -	loff_t size;
 +	pgoff_t size;
  	int ret = 0;
  
++<<<<<<< HEAD
 +	size = (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
 +	if (offset >= size)
++=======
+ 	size = round_up(i_size_read(inode), PAGE_SIZE);
+ 	if (offset >= size >> PAGE_SHIFT)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		return VM_FAULT_SIGBUS;
  
  	/*
@@@ -2308,10 -2072,10 +2424,15 @@@ retry_find
  	 * Found the page and have a reference on it.
  	 * We must recheck i_size under page lock.
  	 */
++<<<<<<< HEAD
 +	size = (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
 +	if (unlikely(offset >= size)) {
++=======
+ 	size = round_up(i_size_read(inode), PAGE_SIZE);
+ 	if (unlikely(offset >= size >> PAGE_SHIFT)) {
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		unlock_page(page);
- 		page_cache_release(page);
+ 		put_page(page);
  		return VM_FAULT_SIGBUS;
  	}
  
@@@ -2367,6 -2131,79 +2488,82 @@@ page_not_uptodate
  }
  EXPORT_SYMBOL(filemap_fault);
  
++<<<<<<< HEAD
++=======
+ void filemap_map_pages(struct vm_area_struct *vma, struct vm_fault *vmf)
+ {
+ 	struct radix_tree_iter iter;
+ 	void **slot;
+ 	struct file *file = vma->vm_file;
+ 	struct address_space *mapping = file->f_mapping;
+ 	loff_t size;
+ 	struct page *page;
+ 	unsigned long address = (unsigned long) vmf->virtual_address;
+ 	unsigned long addr;
+ 	pte_t *pte;
+ 
+ 	rcu_read_lock();
+ 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, vmf->pgoff) {
+ 		if (iter.index > vmf->max_pgoff)
+ 			break;
+ repeat:
+ 		page = radix_tree_deref_slot(slot);
+ 		if (unlikely(!page))
+ 			goto next;
+ 		if (radix_tree_exception(page)) {
+ 			if (radix_tree_deref_retry(page)) {
+ 				slot = radix_tree_iter_retry(&iter);
+ 				continue;
+ 			}
+ 			goto next;
+ 		}
+ 
+ 		if (!page_cache_get_speculative(page))
+ 			goto repeat;
+ 
+ 		/* Has the page moved? */
+ 		if (unlikely(page != *slot)) {
+ 			put_page(page);
+ 			goto repeat;
+ 		}
+ 
+ 		if (!PageUptodate(page) ||
+ 				PageReadahead(page) ||
+ 				PageHWPoison(page))
+ 			goto skip;
+ 		if (!trylock_page(page))
+ 			goto skip;
+ 
+ 		if (page->mapping != mapping || !PageUptodate(page))
+ 			goto unlock;
+ 
+ 		size = round_up(i_size_read(mapping->host), PAGE_SIZE);
+ 		if (page->index >= size >> PAGE_SHIFT)
+ 			goto unlock;
+ 
+ 		pte = vmf->pte + page->index - vmf->pgoff;
+ 		if (!pte_none(*pte))
+ 			goto unlock;
+ 
+ 		if (file->f_ra.mmap_miss > 0)
+ 			file->f_ra.mmap_miss--;
+ 		addr = address + (page->index - vmf->pgoff) * PAGE_SIZE;
+ 		do_set_pte(vma, addr, page, pte, false, false);
+ 		unlock_page(page);
+ 		goto next;
+ unlock:
+ 		unlock_page(page);
+ skip:
+ 		put_page(page);
+ next:
+ 		if (iter.index == vmf->max_pgoff)
+ 			break;
+ 	}
+ 	rcu_read_unlock();
+ }
+ EXPORT_SYMBOL(filemap_map_pages);
+ 
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  int filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
  {
  	struct page *page = vmf->page;
@@@ -2436,7 -2273,19 +2633,23 @@@ int generic_file_readonly_mmap(struct f
  EXPORT_SYMBOL(generic_file_mmap);
  EXPORT_SYMBOL(generic_file_readonly_mmap);
  
++<<<<<<< HEAD
 +static struct page *__read_cache_page(struct address_space *mapping,
++=======
+ static struct page *wait_on_page_read(struct page *page)
+ {
+ 	if (!IS_ERR(page)) {
+ 		wait_on_page_locked(page);
+ 		if (!PageUptodate(page)) {
+ 			put_page(page);
+ 			page = ERR_PTR(-EIO);
+ 		}
+ 	}
+ 	return page;
+ }
+ 
+ static struct page *do_read_cache_page(struct address_space *mapping,
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  				pgoff_t index,
  				int (*filler)(void *, struct page *),
  				void *data,
@@@ -2458,38 -2307,68 +2671,48 @@@ repeat
  			/* Presumably ENOMEM for radix tree node */
  			return ERR_PTR(err);
  		}
 -
 -filler:
  		err = filler(data, page);
  		if (err < 0) {
++<<<<<<< HEAD
 +			page_cache_release(page);
 +			page = ERR_PTR(err);
++=======
+ 			put_page(page);
+ 			return ERR_PTR(err);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		}
 -
 -		page = wait_on_page_read(page);
 -		if (IS_ERR(page))
 -			return page;
 -		goto out;
  	}
 -	if (PageUptodate(page))
 -		goto out;
 +	return page;
 +}
  
 -	/*
 -	 * Page is not up to date and may be locked due one of the following
 -	 * case a: Page is being filled and the page lock is held
 -	 * case b: Read/write error clearing the page uptodate status
 -	 * case c: Truncation in progress (page locked)
 -	 * case d: Reclaim in progress
 -	 *
 -	 * Case a, the page will be up to date when the page is unlocked.
 -	 *    There is no need to serialise on the page lock here as the page
 -	 *    is pinned so the lock gives no additional protection. Even if the
 -	 *    the page is truncated, the data is still valid if PageUptodate as
 -	 *    it's a race vs truncate race.
 -	 * Case b, the page will not be up to date
 -	 * Case c, the page may be truncated but in itself, the data may still
 -	 *    be valid after IO completes as it's a read vs truncate race. The
 -	 *    operation must restart if the page is not uptodate on unlock but
 -	 *    otherwise serialising on page lock to stabilise the mapping gives
 -	 *    no additional guarantees to the caller as the page lock is
 -	 *    released before return.
 -	 * Case d, similar to truncation. If reclaim holds the page lock, it
 -	 *    will be a race with remove_mapping that determines if the mapping
 -	 *    is valid on unlock but otherwise the data is valid and there is
 -	 *    no need to serialise with page lock.
 -	 *
 -	 * As the page lock gives no additional guarantee, we optimistically
 -	 * wait on the page to be unlocked and check if it's up to date and
 -	 * use the page if it is. Otherwise, the page lock is required to
 -	 * distinguish between the different cases. The motivation is that we
 -	 * avoid spurious serialisations and wakeups when multiple processes
 -	 * wait on the same page for IO to complete.
 -	 */
 -	wait_on_page_locked(page);
 +static struct page *do_read_cache_page(struct address_space *mapping,
 +				pgoff_t index,
 +				int (*filler)(void *, struct page *),
 +				void *data,
 +				gfp_t gfp)
 +
 +{
 +	struct page *page;
 +	int err;
 +
 +retry:
 +	page = __read_cache_page(mapping, index, filler, data, gfp);
 +	if (IS_ERR(page))
 +		return page;
  	if (PageUptodate(page))
  		goto out;
  
 -	/* Distinguish between all the cases under the safety of the lock */
  	lock_page(page);
 -
 -	/* Case c or d, restart the operation */
  	if (!page->mapping) {
  		unlock_page(page);
++<<<<<<< HEAD
 +		page_cache_release(page);
 +		goto retry;
++=======
+ 		put_page(page);
+ 		goto repeat;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
 -
 -	/* Someone else locked and filled the page in a very small window */
  	if (PageUptodate(page)) {
  		unlock_page(page);
  		goto out;
@@@ -2841,12 -2508,10 +3064,17 @@@ generic_file_direct_write(struct kiocb 
  	ssize_t		written;
  	size_t		write_len;
  	pgoff_t		end;
 -	struct iov_iter data;
  
++<<<<<<< HEAD
 +	if (count != ocount)
 +		*nr_segs = iov_shorten((struct iovec *)iov, *nr_segs, count);
 +
 +	write_len = iov_length(iov, *nr_segs);
 +	end = (pos + write_len - 1) >> PAGE_CACHE_SHIFT;
++=======
+ 	write_len = iov_iter_count(from);
+ 	end = (pos + write_len - 1) >> PAGE_SHIFT;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	written = filemap_write_and_wait_range(mapping, pos, pos + write_len - 1);
  	if (written)
@@@ -3147,13 -2746,14 +3375,13 @@@ ssize_t __generic_file_aio_write(struc
  		 * disk and invalidated to preserve the expected O_DIRECT
  		 * semantics.
  		 */
 -		endbyte = pos + status - 1;
 -		err = filemap_write_and_wait_range(mapping, pos, endbyte);
 +		endbyte = pos + written_buffered - written - 1;
 +		err = filemap_write_and_wait_range(file->f_mapping, pos, endbyte);
  		if (err == 0) {
 -			iocb->ki_pos = endbyte + 1;
 -			written += status;
 +			written = written_buffered;
  			invalidate_mapping_pages(mapping,
- 						 pos >> PAGE_CACHE_SHIFT,
- 						 endbyte >> PAGE_CACHE_SHIFT);
+ 						 pos >> PAGE_SHIFT,
+ 						 endbyte >> PAGE_SHIFT);
  		} else {
  			/*
  			 * We don't know how much we wrote, so just return
diff --cc mm/hugetlb.c
index 7cfa031212e3,19d0d08b396f..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -3002,9 -3346,12 +3002,9 @@@ retry_avoidcopy
  			old_page != pagecache_page)
  		outside_reserve = 1;
  
- 	page_cache_get(old_page);
+ 	get_page(old_page);
  
 -	/*
 -	 * Drop page table lock as buddy allocator may be called. It will
 -	 * be acquired again before returning to the caller, as expected.
 -	 */
 +	/* Drop page table lock as buddy allocator may be called */
  	spin_unlock(ptl);
  	new_page = alloc_huge_page(vma, address, outside_reserve);
  
@@@ -3020,28 -3364,25 +3020,32 @@@
  		 * may get SIGKILLed if it later faults.
  		 */
  		if (outside_reserve) {
++<<<<<<< HEAD
++=======
+ 			put_page(old_page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			BUG_ON(huge_pte_none(pte));
 -			unmap_ref_private(mm, vma, old_page, address);
 -			BUG_ON(huge_pte_none(pte));
 -			spin_lock(ptl);
 -			ptep = huge_pte_offset(mm, address & huge_page_mask(h));
 -			if (likely(ptep &&
 -				   pte_same(huge_ptep_get(ptep), pte)))
 -				goto retry_avoidcopy;
 -			/*
 -			 * race occurs while re-acquiring page table
 -			 * lock, and our job is done.
 -			 */
 -			return 0;
 +			if (unmap_ref_private(mm, vma, old_page, address)) {
 +				BUG_ON(huge_pte_none(pte));
 +				spin_lock(ptl);
 +				ptep = huge_pte_offset(mm, address & huge_page_mask(h));
 +				if (likely(pte_same(huge_ptep_get(ptep), pte)))
 +					goto retry_avoidcopy;
 +				/*
 +				 * race occurs while re-acquiring page table
 +				 * lock, and our job is done.
 +				 */
 +				return 0;
 +			}
 +			WARN_ON_ONCE(1);
  		}
  
 -		ret = (PTR_ERR(new_page) == -ENOMEM) ?
 -			VM_FAULT_OOM : VM_FAULT_SIGBUS;
 -		goto out_release_old;
 +		/* Caller expects lock to be held */
 +		spin_lock(ptl);
 +		if (err == -ENOMEM)
 +			return VM_FAULT_OOM;
 +		else
 +			return VM_FAULT_SIGBUS;
  	}
  
  	/*
@@@ -3085,12 -3424,13 +3089,19 @@@
  	}
  	spin_unlock(ptl);
  	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
++<<<<<<< HEAD
 +	page_cache_release(new_page);
 +	page_cache_release(old_page);
++=======
+ out_release_all:
+ 	put_page(new_page);
+ out_release_old:
+ 	put_page(old_page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 -	spin_lock(ptl); /* Caller expects lock to be held */
 -	return ret;
 +	/* Caller expects lock to be held */
 +	spin_lock(ptl);
 +	return 0;
  }
  
  /* Return the pagecache page at a given address within a VMA */
diff --cc mm/memory.c
index e63691293747,07a420488cda..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2225,10 -2199,11 +2225,10 @@@ static int wp_page_copy(struct mm_struc
  		 */
  		if (page_copied && (vma->vm_flags & VM_LOCKED)) {
  			lock_page(old_page);	/* LRU manipulation */
 -			if (PageMlocked(old_page))
 -				munlock_vma_page(old_page);
 +			munlock_vma_page(old_page);
  			unlock_page(old_page);
  		}
- 		page_cache_release(old_page);
+ 		put_page(old_page);
  	}
  	return page_copied ? VM_FAULT_WRITE : 0;
  oom_free_new:
@@@ -2283,13 -2258,8 +2283,13 @@@ static int wp_page_shared(struct mm_str
  {
  	int page_mkwrite = 0;
  
- 	page_cache_get(old_page);
+ 	get_page(old_page);
  
 +	/*
 +	 * Only catch write-faults on shared writable pages,
 +	 * read-only shared pages can get COWed by
 +	 * get_user_pages(.write=1, .force=1).
 +	 */
  	if (vma->vm_ops && vma->vm_ops->page_mkwrite) {
  		int tmp;
  
@@@ -2796,8 -2751,8 +2796,13 @@@ static int do_anonymous_page(struct mm_
  	/* Deliver the page fault to userland, check inside PT lock */
  	if (userfaultfd_missing(vma)) {
  		pte_unmap_unlock(page_table, ptl);
++<<<<<<< HEAD
 +		mem_cgroup_uncharge_page(page);
 +		page_cache_release(page);
++=======
+ 		mem_cgroup_cancel_charge(page, memcg, false);
+ 		put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		return handle_userfault(vma, address, flags,
  					VM_UFFD_MISSING);
  	}
@@@ -2813,11 -2770,11 +2818,16 @@@ unlock
  	pte_unmap_unlock(page_table, ptl);
  	return 0;
  release:
++<<<<<<< HEAD
 +	mem_cgroup_uncharge_page(page);
 +	page_cache_release(page);
++=======
+ 	mem_cgroup_cancel_charge(page, memcg, false);
+ 	put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	goto unlock;
  oom_free_page:
- 	page_cache_release(page);
+ 	put_page(page);
  oom:
  	return VM_FAULT_OOM;
  }
@@@ -2924,8 -3023,8 +2934,13 @@@ static int do_cow_fault(struct mm_struc
  	if (!new_page)
  		return VM_FAULT_OOM;
  
++<<<<<<< HEAD
 +	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL)) {
 +		page_cache_release(new_page);
++=======
+ 	if (mem_cgroup_try_charge(new_page, mm, GFP_KERNEL, &memcg, false)) {
+ 		put_page(new_page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		return VM_FAULT_OOM;
  	}
  
@@@ -2966,8 -3067,8 +2981,13 @@@
  	}
  	return ret;
  uncharge_out:
++<<<<<<< HEAD
 +	mem_cgroup_uncharge_page(new_page);
 +	page_cache_release(new_page);
++=======
+ 	mem_cgroup_cancel_charge(new_page, memcg, false);
+ 	put_page(new_page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	return ret;
  }
  
diff --cc mm/page-writeback.c
index 9440add4b0a8,999792d35ccc..000000000000
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@@ -2125,11 -2419,19 +2125,17 @@@ void account_page_dirtied(struct page *
  	trace_writeback_dirty_page(page, mapping);
  
  	if (mapping_cap_account_dirty(mapping)) {
 -		struct bdi_writeback *wb;
 -
 -		inode_attach_wb(inode, page);
 -		wb = inode_to_wb(inode);
 -
 -		mem_cgroup_inc_page_stat(page, MEM_CGROUP_STAT_DIRTY);
  		__inc_zone_page_state(page, NR_FILE_DIRTY);
  		__inc_zone_page_state(page, NR_DIRTIED);
++<<<<<<< HEAD
 +		__inc_bdi_stat(mapping->backing_dev_info, BDI_RECLAIMABLE);
 +		__inc_bdi_stat(mapping->backing_dev_info, BDI_DIRTIED);
 +		task_io_account_write(PAGE_CACHE_SIZE);
++=======
+ 		__inc_wb_stat(wb, WB_RECLAIMABLE);
+ 		__inc_wb_stat(wb, WB_DIRTIED);
+ 		task_io_account_write(PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		current->nr_dirtied++;
  		this_cpu_inc(bdp_ratelimits);
  	}
@@@ -2137,15 -2439,20 +2143,24 @@@
  EXPORT_SYMBOL(account_page_dirtied);
  
  /*
 - * Helper function for deaccounting dirty page without writeback.
 - *
 - * Caller must hold lock_page_memcg().
 + * Helper function for set_page_writeback family.
 + * NOTE: Unlike account_page_dirtied this does not rely on being atomic
 + * wrt interrupts.
   */
 -void account_page_cleaned(struct page *page, struct address_space *mapping,
 -			  struct bdi_writeback *wb)
 +void account_page_writeback(struct page *page)
  {
++<<<<<<< HEAD
 +	inc_zone_page_state(page, NR_WRITEBACK);
++=======
+ 	if (mapping_cap_account_dirty(mapping)) {
+ 		mem_cgroup_dec_page_stat(page, MEM_CGROUP_STAT_DIRTY);
+ 		dec_zone_page_state(page, NR_FILE_DIRTY);
+ 		dec_wb_stat(wb, WB_RECLAIMABLE);
+ 		task_io_account_cancelled_write(PAGE_SIZE);
+ 	}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
 +EXPORT_SYMBOL(account_page_writeback);
  
  /*
   * For address_spaces which do not use buffers.  Just tag the page as dirty in
diff --cc mm/readahead.c
index a386b06d832b,40be3ae0afe3..000000000000
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@@ -48,7 -47,7 +48,11 @@@ static void read_cache_pages_invalidate
  		if (!trylock_page(page))
  			BUG();
  		page->mapping = mapping;
++<<<<<<< HEAD
 +		do_invalidatepage(page, 0);
++=======
+ 		do_invalidatepage(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		page->mapping = NULL;
  		unlock_page(page);
  	}
@@@ -125,13 -124,13 +129,13 @@@ static int read_pages(struct address_sp
  	}
  
  	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
 -		struct page *page = lru_to_page(pages);
 +		struct page *page = list_to_page(pages);
  		list_del(&page->lru);
 -		if (!add_to_page_cache_lru(page, mapping, page->index,
 -				mapping_gfp_constraint(mapping, GFP_KERNEL))) {
 +		if (!add_to_page_cache_lru(page, mapping,
 +					page->index, GFP_KERNEL)) {
  			mapping->a_ops->readpage(filp, page);
  		}
- 		page_cache_release(page);
+ 		put_page(page);
  	}
  	ret = 0;
  
diff --cc mm/rmap.c
index 651b49d02afd,525b92f866a7..000000000000
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@@ -1442,8 -1553,9 +1442,14 @@@ int try_to_unmap_one(struct page *page
  	} else
  		dec_mm_counter(mm, mm_counter_file(page));
  
++<<<<<<< HEAD
 +	page_remove_rmap(page);
 +	page_cache_release(page);
++=======
+ discard:
+ 	page_remove_rmap(page, PageHuge(page));
+ 	put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  out_unmap:
  	pte_unmap_unlock(pte, ptl);
diff --cc mm/shmem.c
index 021ba8dffccb,719bd6b88d98..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -73,8 -73,10 +73,15 @@@ static struct vfsmount *shm_mnt
  #include <asm/uaccess.h>
  #include <asm/pgtable.h>
  
++<<<<<<< HEAD
 +#define BLOCKS_PER_PAGE  (PAGE_CACHE_SIZE/512)
 +#define VM_ACCT(size)    (PAGE_CACHE_ALIGN(size) >> PAGE_SHIFT)
++=======
+ #include "internal.h"
+ 
+ #define BLOCKS_PER_PAGE  (PAGE_SIZE/512)
+ #define VM_ACCT(size)    (PAGE_ALIGN(size) >> PAGE_SHIFT)
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  /* Pretend that each entry is of this size in directory's i_size */
  #define BOGO_DIRENT_SIZE 20
@@@ -800,11 -824,16 +807,11 @@@ int shmem_unuse(swp_entry_t swap, struc
  	}
  	mutex_unlock(&shmem_swaplist_mutex);
  
 -	if (error) {
 -		if (error != -ENOMEM)
 -			error = 0;
 -		mem_cgroup_cancel_charge(page, memcg, false);
 -	} else
 -		mem_cgroup_commit_charge(page, memcg, true, false);
 +	if (found < 0)
 +		error = found;
  out:
  	unlock_page(page);
- 	page_cache_release(page);
+ 	put_page(page);
  	return error;
  }
  
@@@ -1122,11 -1156,14 +1129,11 @@@ repeat
  	}
  
  	if (sgp != SGP_WRITE && sgp != SGP_FALLOC &&
- 	    ((loff_t)index << PAGE_CACHE_SHIFT) >= i_size_read(inode)) {
+ 	    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {
  		error = -EINVAL;
 -		goto unlock;
 +		goto failed;
  	}
  
 -	if (page && sgp == SGP_WRITE)
 -		mark_page_accessed(page);
 -
  	/* fallocated page? */
  	if (page && !PageUptodate(page)) {
  		if (sgp != SGP_READ)
@@@ -1279,12 -1327,16 +1286,23 @@@ clear
  
  	/* Perhaps the file has been truncated since we checked */
  	if (sgp != SGP_WRITE && sgp != SGP_FALLOC &&
++<<<<<<< HEAD
 +	    ((loff_t)index << PAGE_CACHE_SHIFT) >= i_size_read(inode)) {
++=======
+ 	    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {
+ 		if (alloced) {
+ 			ClearPageDirty(page);
+ 			delete_from_page_cache(page);
+ 			spin_lock(&info->lock);
+ 			shmem_recalc_inode(inode);
+ 			spin_unlock(&info->lock);
+ 		}
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		error = -EINVAL;
 -		goto unlock;
 +		if (alloced)
 +			goto trunc;
 +		else
 +			goto failed;
  	}
  	*pagep = page;
  	return 0;
@@@ -1585,11 -1632,11 +1603,11 @@@ static void do_shmem_file_read(struct f
  	 * holes of a sparse file, we actually need to allocate those pages,
  	 * and even mark them dirty, so it cannot exceed the max_blocks limit.
  	 */
 -	if (!iter_is_iovec(to))
 +	if (segment_eq(get_fs(), KERNEL_DS))
  		sgp = SGP_DIRTY;
  
- 	index = *ppos >> PAGE_CACHE_SHIFT;
- 	offset = *ppos & ~PAGE_CACHE_MASK;
+ 	index = *ppos >> PAGE_SHIFT;
+ 	offset = *ppos & ~PAGE_MASK;
  
  	for (;;) {
  		struct page *page = NULL;
@@@ -1653,61 -1700,26 +1671,72 @@@
  		/*
  		 * Ok, we have the page, and it's up-to-date, so
  		 * now we can copy it to user space...
 +		 *
 +		 * The actor routine returns how many bytes were actually used..
 +		 * NOTE! This may not be the same as how much of a user buffer
 +		 * we filled up (we may be padding etc), so we can only update
 +		 * "pos" here (the actor routine has to update the user buffer
 +		 * pointers and the remaining count).
  		 */
 -		ret = copy_page_to_iter(page, offset, nr, to);
 -		retval += ret;
 +		ret = actor(desc, page, offset, nr);
  		offset += ret;
- 		index += offset >> PAGE_CACHE_SHIFT;
- 		offset &= ~PAGE_CACHE_MASK;
+ 		index += offset >> PAGE_SHIFT;
+ 		offset &= ~PAGE_MASK;
  
++<<<<<<< HEAD
 +		page_cache_release(page);
 +		if (ret != nr || !desc->count)
++=======
+ 		put_page(page);
+ 		if (!iov_iter_count(to))
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			break;
 -		if (ret < nr) {
 -			error = -EFAULT;
 -			break;
 -		}
 +
  		cond_resched();
  	}
  
++<<<<<<< HEAD
 +	*ppos = ((loff_t) index << PAGE_CACHE_SHIFT) + offset;
 +	file_accessed(filp);
 +}
 +
 +static ssize_t shmem_file_aio_read(struct kiocb *iocb,
 +		const struct iovec *iov, unsigned long nr_segs, loff_t pos)
 +{
 +	struct file *filp = iocb->ki_filp;
 +	ssize_t retval;
 +	unsigned long seg;
 +	size_t count;
 +	loff_t *ppos = &iocb->ki_pos;
 +
 +	retval = generic_segment_checks(iov, &nr_segs, &count, VERIFY_WRITE);
 +	if (retval)
 +		return retval;
 +
 +	for (seg = 0; seg < nr_segs; seg++) {
 +		read_descriptor_t desc;
 +
 +		desc.written = 0;
 +		desc.arg.buf = iov[seg].iov_base;
 +		desc.count = iov[seg].iov_len;
 +		if (desc.count == 0)
 +			continue;
 +		desc.error = 0;
 +		do_shmem_file_read(filp, ppos, &desc, file_read_actor);
 +		retval += desc.written;
 +		if (desc.error) {
 +			retval = retval ?: desc.error;
 +			break;
 +		}
 +		if (desc.count > 0)
 +			break;
 +	}
 +	return retval;
++=======
+ 	*ppos = ((loff_t) index << PAGE_SHIFT) + offset;
+ 	file_accessed(file);
+ 	return retval ? retval : error;
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  static ssize_t shmem_file_splice_read(struct file *in, loff_t *ppos,
@@@ -1743,10 -1755,10 +1772,17 @@@
  	if (splice_grow_spd(pipe, &spd))
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	index = *ppos >> PAGE_CACHE_SHIFT;
 +	loff = *ppos & ~PAGE_CACHE_MASK;
 +	req_pages = (len + loff + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
 +	nr_pages = min(req_pages, pipe->buffers);
++=======
+ 	index = *ppos >> PAGE_SHIFT;
+ 	loff = *ppos & ~PAGE_MASK;
+ 	req_pages = (len + loff + PAGE_SIZE - 1) >> PAGE_SHIFT;
+ 	nr_pages = min(req_pages, spd.nr_pages_max);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	spd.nr_pages = find_get_pages_contig(mapping, index,
  						nr_pages, spd.pages);
diff --cc mm/swap.c
index 710acc9fa0d6,ea641e247033..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -475,10 -236,10 +475,10 @@@ void rotate_reclaimable_page(struct pag
  		struct pagevec *pvec;
  		unsigned long flags;
  
- 		page_cache_get(page);
+ 		get_page(page);
  		local_irq_save(flags);
 -		pvec = this_cpu_ptr(&lru_rotate_pvecs);
 -		if (!pagevec_add(pvec, page))
 +		pvec = &__get_cpu_var(lru_rotate_pvecs);
 +		if (!pagevec_add(pvec, page) || PageCompound(page))
  			pagevec_move_tail(pvec);
  		local_irq_restore(flags);
  	}
@@@ -533,8 -294,8 +533,13 @@@ void activate_page(struct page *page
  	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
  		struct pagevec *pvec = &get_cpu_var(activate_page_pvecs);
  
++<<<<<<< HEAD
 +		page_cache_get(page);
 +		if (!pagevec_add(pvec, page) || PageCompound(page))
++=======
+ 		get_page(page);
+ 		if (!pagevec_add(pvec, page))
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			pagevec_lru_move_fn(pvec, __activate_page, NULL);
  		put_cpu_var(activate_page_pvecs);
  	}
@@@ -628,12 -389,31 +633,17 @@@ void __lru_cache_add(struct page *page
  {
  	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
  
++<<<<<<< HEAD
 +	page_cache_get(page);
 +	if (!pagevec_add(pvec, page) || PageCompound(page))
++=======
+ 	get_page(page);
+ 	if (!pagevec_space(pvec))
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  		__pagevec_lru_add(pvec);
 -	pagevec_add(pvec, page);
  	put_cpu_var(lru_add_pvec);
  }
 -
 -/**
 - * lru_cache_add: add a page to the page lists
 - * @page: the page to add
 - */
 -void lru_cache_add_anon(struct page *page)
 -{
 -	if (PageActive(page))
 -		ClearPageActive(page);
 -	__lru_cache_add(page);
 -}
 -
 -void lru_cache_add_file(struct page *page)
 -{
 -	if (PageActive(page))
 -		ClearPageActive(page);
 -	__lru_cache_add(page);
 -}
 -EXPORT_SYMBOL(lru_cache_add_file);
 +EXPORT_SYMBOL(__lru_cache_add);
  
  /**
   * lru_cache_add - add a page to a page list
@@@ -784,9 -625,29 +794,14 @@@ void deactivate_page(struct page *page
  		return;
  
  	if (likely(get_page_unless_zero(page))) {
 -		struct pagevec *pvec = &get_cpu_var(lru_deactivate_file_pvecs);
 -
 -		if (!pagevec_add(pvec, page))
 -			pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
 -		put_cpu_var(lru_deactivate_file_pvecs);
 -	}
 -}
 -
 -/**
 - * deactivate_page - deactivate a page
 - * @page: page to deactivate
 - *
 - * deactivate_page() moves @page to the inactive list if @page was on the active
 - * list and was not an unevictable page.  This is done to accelerate the reclaim
 - * of @page.
 - */
 -void deactivate_page(struct page *page)
 -{
 -	if (PageLRU(page) && PageActive(page) && !PageUnevictable(page)) {
  		struct pagevec *pvec = &get_cpu_var(lru_deactivate_pvecs);
  
++<<<<<<< HEAD
 +		if (!pagevec_add(pvec, page) || PageCompound(page))
++=======
+ 		get_page(page);
+ 		if (!pagevec_add(pvec, page))
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);
  		put_cpu_var(lru_deactivate_pvecs);
  	}
diff --cc mm/swap_state.c
index 8ead62769c81,366ce3518703..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -224,8 -225,8 +224,13 @@@ void delete_from_swap_cache(struct pag
  	__delete_from_swap_cache(page);
  	spin_unlock_irq(&address_space->tree_lock);
  
++<<<<<<< HEAD
 +	swapcache_free(entry, page);
 +	page_cache_release(page);
++=======
+ 	swapcache_free(entry);
+ 	put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  /* 
@@@ -250,13 -251,8 +255,18 @@@ static inline void free_swap_cache(stru
   */
  void free_page_and_swap_cache(struct page *page)
  {
++<<<<<<< HEAD
 +	if (!is_trans_huge_page_release(page)) {
 +		free_swap_cache(page);
 +		page_cache_release(page);
 +	} else {
 +		/* page might have to be decoded */
 +		release_pages(&page, 1, false);
 +	}
++=======
+ 	free_swap_cache(page);
+ 	put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  }
  
  /*
@@@ -443,7 -493,9 +453,13 @@@ struct page *swapin_readahead(swp_entry
  						gfp_mask, vma, addr);
  		if (!page)
  			continue;
++<<<<<<< HEAD
 +		page_cache_release(page);
++=======
+ 		if (offset != entry_offset)
+ 			SetPageReadahead(page);
+ 		put_page(page);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  	}
  	blk_finish_plug(&plug);
  
diff --cc mm/truncate.c
index 8c90be16c1d6,b00272810871..000000000000
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@@ -170,10 -118,14 +170,14 @@@ truncate_complete_page(struct address_s
  		return -EIO;
  
  	if (page_has_private(page))
++<<<<<<< HEAD
 +		do_invalidatepage(page, 0);
 +
 +	cancel_dirty_page(page, PAGE_CACHE_SIZE);
++=======
+ 		do_invalidatepage(page, 0, PAGE_SIZE);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
 -	/*
 -	 * Some filesystems seem to re-dirty the page even after
 -	 * the VM has canceled the dirty bit (eg ext3 journaling).
 -	 * Hence dirty accounting check is placed after invalidation.
 -	 */
 -	cancel_dirty_page(page);
  	ClearPageMappedToDisk(page);
  	delete_from_page_cache(page);
  	return 0;
@@@ -288,10 -241,8 +292,15 @@@ void truncate_inode_pages_range(struct 
  		return;
  
  	/* Offsets within partial pages */
++<<<<<<< HEAD
 +	partial_start = lstart & (PAGE_CACHE_SIZE - 1);
 +	partial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);
 +	if (!inode_has_invalidate_range(mapping->host))
 +		BUG_ON(partial_end);
++=======
+ 	partial_start = lstart & (PAGE_SIZE - 1);
+ 	partial_end = (lend + 1) & (PAGE_SIZE - 1);
++>>>>>>> 09cbfeaf1a5a (mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros)
  
  	/*
  	 * 'start' and 'end' always covers the range of pages to be fully
@@@ -358,16 -307,11 +367,16 @@@
  			wait_on_page_writeback(page);
  			zero_user_segment(page, partial_start, top);
  			cleancache_invalidate_page(mapping, page);
 -			if (page_has_private(page))
 -				do_invalidatepage(page, partial_start,
 -						  top - partial_start);
 +			if (page_has_private(page)) {
 +				if (inode_has_invalidate_range(mapping->host))
 +					do_invalidatepage_range(page,
 +							partial_start,
 +							top - partial_start);
 +				else
 +					do_invalidatepage(page, partial_start);
 +			}
  			unlock_page(page);
- 			page_cache_release(page);
+ 			put_page(page);
  		}
  	}
  	if (partial_end) {
@@@ -377,10 -321,10 +386,10 @@@
  			zero_user_segment(page, 0, partial_end);
  			cleancache_invalidate_page(mapping, page);
  			if (page_has_private(page))
 -				do_invalidatepage(page, 0,
 -						  partial_end);
 +				do_invalidatepage_range(page, 0,
 +							partial_end);
  			unlock_page(page);
- 			page_cache_release(page);
+ 			put_page(page);
  		}
  	}
  	/*
@@@ -592,10 -538,10 +601,10 @@@ invalidate_complete_page2(struct addres
  	if (mapping->a_ops->freepage)
  		mapping->a_ops->freepage(page);
  
- 	page_cache_release(page);	/* pagecache ref */
+ 	put_page(page);	/* pagecache ref */
  	return 1;
  failed:
 -	spin_unlock_irqrestore(&mapping->tree_lock, flags);
 +	spin_unlock_irq(&mapping->tree_lock);
  	return 0;
  }
  
* Unmerged path arch/arc/mm/cache.c
* Unmerged path drivers/gpu/drm/armada/armada_gem.c
* Unmerged path drivers/mmc/host/usdhi6rol0.c
* Unmerged path drivers/staging/lustre/include/linux/libcfs/libcfs_private.h
* Unmerged path drivers/staging/lustre/include/linux/libcfs/linux/linux-mem.h
* Unmerged path drivers/staging/lustre/lnet/klnds/socklnd/socklnd_lib.c
* Unmerged path drivers/staging/lustre/lnet/libcfs/debug.c
* Unmerged path drivers/staging/lustre/lnet/libcfs/tracefile.c
* Unmerged path drivers/staging/lustre/lnet/libcfs/tracefile.h
* Unmerged path drivers/staging/lustre/lnet/lnet/lib-md.c
* Unmerged path drivers/staging/lustre/lnet/lnet/lib-move.c
* Unmerged path drivers/staging/lustre/lnet/lnet/lib-socket.c
* Unmerged path drivers/staging/lustre/lnet/lnet/router.c
* Unmerged path drivers/staging/lustre/lnet/selftest/brw_test.c
* Unmerged path drivers/staging/lustre/lnet/selftest/conctl.c
* Unmerged path drivers/staging/lustre/lnet/selftest/conrpc.c
* Unmerged path drivers/staging/lustre/lnet/selftest/framework.c
* Unmerged path drivers/staging/lustre/lnet/selftest/rpc.c
* Unmerged path drivers/staging/lustre/lnet/selftest/selftest.h
* Unmerged path drivers/staging/lustre/lustre/include/linux/lustre_patchless_compat.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre/lustre_idl.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre_mdc.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre_net.h
* Unmerged path drivers/staging/lustre/lustre/include/obd.h
* Unmerged path drivers/staging/lustre/lustre/include/obd_support.h
* Unmerged path drivers/staging/lustre/lustre/lclient/lcommon_cl.c
* Unmerged path drivers/staging/lustre/lustre/ldlm/ldlm_lib.c
* Unmerged path drivers/staging/lustre/lustre/ldlm/ldlm_pool.c
* Unmerged path drivers/staging/lustre/lustre/ldlm/ldlm_request.c
* Unmerged path drivers/staging/lustre/lustre/llite/dir.c
* Unmerged path drivers/staging/lustre/lustre/llite/llite_internal.h
* Unmerged path drivers/staging/lustre/lustre/llite/llite_lib.c
* Unmerged path drivers/staging/lustre/lustre/llite/llite_mmap.c
* Unmerged path drivers/staging/lustre/lustre/llite/lloop.c
* Unmerged path drivers/staging/lustre/lustre/llite/lproc_llite.c
* Unmerged path drivers/staging/lustre/lustre/llite/rw.c
* Unmerged path drivers/staging/lustre/lustre/llite/rw26.c
* Unmerged path drivers/staging/lustre/lustre/llite/vvp_io.c
* Unmerged path drivers/staging/lustre/lustre/llite/vvp_page.c
* Unmerged path drivers/staging/lustre/lustre/lmv/lmv_obd.c
* Unmerged path drivers/staging/lustre/lustre/mdc/mdc_request.c
* Unmerged path drivers/staging/lustre/lustre/mgc/mgc_request.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/cl_page.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/class_obd.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/linux/linux-obdo.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/linux/linux-sysctl.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/lu_object.c
* Unmerged path drivers/staging/lustre/lustre/obdecho/echo_client.c
* Unmerged path drivers/staging/lustre/lustre/osc/lproc_osc.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_cache.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_page.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_request.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/client.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/import.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/lproc_ptlrpc.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/recover.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/sec_bulk.c
* Unmerged path fs/crypto/crypto.c
* Unmerged path fs/ext4/crypto.c
* Unmerged path fs/ext4/readpage.c
* Unmerged path fs/f2fs/inline.c
* Unmerged path fs/kernfs/mount.c
* Unmerged path fs/orangefs/inode.c
* Unmerged path fs/orangefs/orangefs-bufmap.c
* Unmerged path fs/orangefs/orangefs-utils.c
* Unmerged path fs/squashfs/file_direct.c
* Unmerged path fs/squashfs/lz4_wrapper.c
* Unmerged path fs/squashfs/page_actor.c
* Unmerged path fs/squashfs/page_actor.h
* Unmerged path arch/arc/mm/cache.c
diff --git a/arch/arm/mm/flush.c b/arch/arm/mm/flush.c
index 32aa5861119f..61a6cfffa189 100644
--- a/arch/arm/mm/flush.c
+++ b/arch/arm/mm/flush.c
@@ -192,7 +192,7 @@ void __flush_dcache_page(struct address_space *mapping, struct page *page)
 	 */
 	if (mapping && cache_is_vipt_aliasing())
 		flush_pfn_alias(page_to_pfn(page),
-				page->index << PAGE_CACHE_SHIFT);
+				page->index << PAGE_SHIFT);
 }
 
 static void __flush_dcache_aliases(struct address_space *mapping, struct page *page)
@@ -207,7 +207,7 @@ static void __flush_dcache_aliases(struct address_space *mapping, struct page *p
 	 *   data in the current VM view associated with this page.
 	 * - aliasing VIPT: we only need to find one mapping of this page.
 	 */
-	pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
+	pgoff = page->index;
 
 	flush_dcache_mmap_lock(mapping);
 	vma_interval_tree_foreach(mpnt, &mapping->i_mmap, pgoff, pgoff) {
diff --git a/arch/parisc/kernel/cache.c b/arch/parisc/kernel/cache.c
index c035673209f7..8002821f5c4c 100644
--- a/arch/parisc/kernel/cache.c
+++ b/arch/parisc/kernel/cache.c
@@ -301,7 +301,7 @@ void flush_dcache_page(struct page *page)
 	if (!mapping)
 		return;
 
-	pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
+	pgoff = page->index;
 
 	/* We have carefully arranged in arch_get_unmapped_area() that
 	 * *any* mappings of a file are always congruently mapped (whether
diff --git a/arch/powerpc/platforms/cell/spufs/inode.c b/arch/powerpc/platforms/cell/spufs/inode.c
index 35f77a42bedf..282363073495 100644
--- a/arch/powerpc/platforms/cell/spufs/inode.c
+++ b/arch/powerpc/platforms/cell/spufs/inode.c
@@ -728,8 +728,8 @@ spufs_fill_super(struct super_block *sb, void *data, int silent)
 		return -ENOMEM;
 
 	sb->s_maxbytes = MAX_LFS_FILESIZE;
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = SPUFS_MAGIC;
 	sb->s_op = &s_ops;
 	sb->s_fs_info = info;
diff --git a/arch/s390/hypfs/inode.c b/arch/s390/hypfs/inode.c
index b9e6cfb925ab..ef3542f22134 100644
--- a/arch/s390/hypfs/inode.c
+++ b/arch/s390/hypfs/inode.c
@@ -287,8 +287,8 @@ static int hypfs_fill_super(struct super_block *sb, void *data, int silent)
 	sbi->uid = current_uid();
 	sbi->gid = current_gid();
 	sb->s_fs_info = sbi;
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = HYPFS_MAGIC;
 	sb->s_op = &hypfs_s_ops;
 	if (hypfs_parse_options(data, sb))
* Unmerged path block/blk-core.c
diff --git a/block/blk-settings.c b/block/blk-settings.c
index e88316791131..82cb815d150a 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -260,8 +260,8 @@ void blk_limits_max_hw_sectors(struct queue_limits *limits, unsigned int max_hw_
 {
 	unsigned int max_sectors;
 
-	if ((max_hw_sectors << 9) < PAGE_CACHE_SIZE) {
-		max_hw_sectors = 1 << (PAGE_CACHE_SHIFT - 9);
+	if ((max_hw_sectors << 9) < PAGE_SIZE) {
+		max_hw_sectors = 1 << (PAGE_SHIFT - 9);
 		printk(KERN_INFO "%s: set to minimum %d\n",
 		       __func__, max_hw_sectors);
 	}
@@ -363,8 +363,8 @@ EXPORT_SYMBOL(blk_queue_max_segments);
  **/
 void blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size)
 {
-	if (max_size < PAGE_CACHE_SIZE) {
-		max_size = PAGE_CACHE_SIZE;
+	if (max_size < PAGE_SIZE) {
+		max_size = PAGE_SIZE;
 		printk(KERN_INFO "%s: set to minimum %d\n",
 		       __func__, max_size);
 	}
@@ -789,8 +789,8 @@ EXPORT_SYMBOL_GPL(blk_queue_dma_drain);
  **/
 void blk_queue_segment_boundary(struct request_queue *q, unsigned long mask)
 {
-	if (mask < PAGE_CACHE_SIZE - 1) {
-		mask = PAGE_CACHE_SIZE - 1;
+	if (mask < PAGE_SIZE - 1) {
+		mask = PAGE_SIZE - 1;
 		printk(KERN_INFO "%s: set to minimum %lx\n",
 		       __func__, mask);
 	}
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index 4bc735ee085b..be08895bb2f1 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -75,7 +75,7 @@ queue_requests_store(struct request_queue *q, const char *page, size_t count)
 static ssize_t queue_ra_show(struct request_queue *q, char *page)
 {
 	unsigned long ra_kb = q->backing_dev_info.ra_pages <<
-					(PAGE_CACHE_SHIFT - 10);
+					(PAGE_SHIFT - 10);
 
 	return queue_var_show(ra_kb, (page));
 }
@@ -89,7 +89,7 @@ queue_ra_store(struct request_queue *q, const char *page, size_t count)
 	if (ret < 0)
 		return ret;
 
-	q->backing_dev_info.ra_pages = ra_kb >> (PAGE_CACHE_SHIFT - 10);
+	q->backing_dev_info.ra_pages = ra_kb >> (PAGE_SHIFT - 10);
 
 	return ret;
 }
@@ -116,7 +116,7 @@ static ssize_t queue_max_segment_size_show(struct request_queue *q, char *page)
 	if (blk_queue_cluster(q))
 		return queue_var_show(queue_max_segment_size(q), (page));
 
-	return queue_var_show(PAGE_CACHE_SIZE, (page));
+	return queue_var_show(PAGE_SIZE, (page));
 }
 
 static ssize_t queue_logical_block_size_show(struct request_queue *q, char *page)
@@ -167,7 +167,7 @@ queue_max_sectors_store(struct request_queue *q, const char *page, size_t count)
 {
 	unsigned long max_sectors_kb,
 		max_hw_sectors_kb = queue_max_hw_sectors(q) >> 1,
-			page_kb = 1 << (PAGE_CACHE_SHIFT - 10);
+			page_kb = 1 << (PAGE_SHIFT - 10);
 	ssize_t ret = queue_var_store(&max_sectors_kb, page, count);
 
 	if (ret < 0)
diff --git a/block/cfq-iosched.c b/block/cfq-iosched.c
index 6fa56e63bc68..8bce13ad6d4e 100644
--- a/block/cfq-iosched.c
+++ b/block/cfq-iosched.c
@@ -3905,7 +3905,7 @@ cfq_rq_enqueued(struct cfq_data *cfqd, struct cfq_queue *cfqq,
 		 * idle timer unplug to continue working.
 		 */
 		if (cfq_cfqq_wait_request(cfqq)) {
-			if (blk_rq_bytes(rq) > PAGE_CACHE_SIZE ||
+			if (blk_rq_bytes(rq) > PAGE_SIZE ||
 			    cfqd->busy_queues > 1) {
 				cfq_del_timer(cfqd, cfqq);
 				cfq_clear_cfqq_wait_request(cfqq);
diff --git a/block/compat_ioctl.c b/block/compat_ioctl.c
index ba8bd919c9bb..621711e24561 100644
--- a/block/compat_ioctl.c
+++ b/block/compat_ioctl.c
@@ -709,7 +709,7 @@ long compat_blkdev_ioctl(struct file *file, unsigned cmd, unsigned long arg)
 			return -EINVAL;
 		bdi = blk_get_backing_dev_info(bdev);
 		return compat_put_long(arg,
-				       (bdi->ra_pages * PAGE_CACHE_SIZE) / 512);
+				       (bdi->ra_pages * PAGE_SIZE) / 512);
 	case BLKROGET: /* compatible */
 		return compat_put_int(arg, bdev_read_only(bdev) != 0);
 	case BLKBSZGET_32: /* get the logical block size (cf. BLKSSZGET) */
@@ -728,7 +728,7 @@ long compat_blkdev_ioctl(struct file *file, unsigned cmd, unsigned long arg)
 		if (!capable(CAP_SYS_ADMIN))
 			return -EACCES;
 		bdi = blk_get_backing_dev_info(bdev);
-		bdi->ra_pages = (arg * 512) / PAGE_CACHE_SIZE;
+		bdi->ra_pages = (arg * 512) / PAGE_SIZE;
 		return 0;
 	case BLKGETSIZE:
 		size = i_size_read(bdev->bd_inode);
diff --git a/block/ioctl.c b/block/ioctl.c
index 4aa3b235d4f1..e5177d0ce533 100644
--- a/block/ioctl.c
+++ b/block/ioctl.c
@@ -520,7 +520,7 @@ int blkdev_ioctl(struct block_device *bdev, fmode_t mode, unsigned cmd,
 		if (!arg)
 			return -EINVAL;
 		bdi = blk_get_backing_dev_info(bdev);
-		return put_long(arg, (bdi->ra_pages * PAGE_CACHE_SIZE) / 512);
+		return put_long(arg, (bdi->ra_pages * PAGE_SIZE) / 512);
 	case BLKROGET:
 		return put_int(arg, bdev_read_only(bdev) != 0);
 	case BLKBSZGET: /* get block device soft block size (cf. BLKSSZGET) */
@@ -548,7 +548,7 @@ int blkdev_ioctl(struct block_device *bdev, fmode_t mode, unsigned cmd,
 		if(!capable(CAP_SYS_ADMIN))
 			return -EACCES;
 		bdi = blk_get_backing_dev_info(bdev);
-		bdi->ra_pages = (arg * 512) / PAGE_CACHE_SIZE;
+		bdi->ra_pages = (arg * 512) / PAGE_SIZE;
 		return 0;
 	case BLKBSZSET:
 		return blkdev_bszset(bdev, mode, argp);
diff --git a/block/partition-generic.c b/block/partition-generic.c
index 66f35dbbdcf2..ced1229db480 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -554,8 +554,8 @@ static struct page *read_pagecache_sector(struct block_device *bdev, sector_t n)
 {
 	struct address_space *mapping = bdev->bd_inode->i_mapping;
 
-	return read_mapping_page(mapping, (pgoff_t)(n >> (PAGE_CACHE_SHIFT-9)),
-			NULL);
+	return read_mapping_page(mapping, (pgoff_t)(n >> (PAGE_SHIFT-9)),
+				 NULL);
 }
 
 unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p)
@@ -572,9 +572,9 @@ unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p)
 		if (PageError(page))
 			goto fail;
 		p->v = page;
-		return (unsigned char *)page_address(page) +  ((n & ((1 << (PAGE_CACHE_SHIFT - 9)) - 1)) << 9);
+		return (unsigned char *)page_address(page) +  ((n & ((1 << (PAGE_SHIFT - 9)) - 1)) << 9);
 fail:
-		page_cache_release(page);
+		put_page(page);
 	}
 	p->v = NULL;
 	return NULL;
diff --git a/drivers/block/aoe/aoeblk.c b/drivers/block/aoe/aoeblk.c
index 916d9ed5c8aa..3c2326c871ca 100644
--- a/drivers/block/aoe/aoeblk.c
+++ b/drivers/block/aoe/aoeblk.c
@@ -308,7 +308,7 @@ aoeblk_gdalloc(void *vp)
 	WARN_ON(d->flags & DEVFL_UP);
 	blk_queue_max_hw_sectors(q, BLK_DEF_MAX_SECTORS);
 	q->backing_dev_info.name = "aoe";
-	q->backing_dev_info.ra_pages = READ_AHEAD / PAGE_CACHE_SIZE;
+	q->backing_dev_info.ra_pages = READ_AHEAD / PAGE_SIZE;
 	d->bufpool = mp;
 	d->blkq = gd->queue = q;
 	q->queuedata = d;
diff --git a/drivers/block/brd.c b/drivers/block/brd.c
index 1f3a382ffd5b..ee26d97e8156 100644
--- a/drivers/block/brd.c
+++ b/drivers/block/brd.c
@@ -367,7 +367,7 @@ static int brd_rw_page(struct block_device *bdev, sector_t sector,
 		       struct page *page, int rw)
 {
 	struct brd_device *brd = bdev->bd_disk->private_data;
-	int err = brd_do_bvec(brd, page, PAGE_CACHE_SIZE, 0, rw, sector);
+	int err = brd_do_bvec(brd, page, PAGE_SIZE, 0, rw, sector);
 	page_endio(page, rw & WRITE, err);
 	return err;
 }
diff --git a/drivers/block/drbd/drbd_nl.c b/drivers/block/drbd/drbd_nl.c
index 9e3f441e7e84..f1380b8ae586 100644
--- a/drivers/block/drbd/drbd_nl.c
+++ b/drivers/block/drbd/drbd_nl.c
@@ -1032,7 +1032,7 @@ static void drbd_setup_queue_param(struct drbd_conf *mdev, unsigned int max_bio_
 	blk_queue_max_hw_sectors(q, max_hw_sectors);
 	/* This is the workaround for "bio would need to, but cannot, be split" */
 	blk_queue_max_segments(q, max_segments ? max_segments : BLK_MAX_SEGMENTS);
-	blk_queue_segment_boundary(q, PAGE_CACHE_SIZE-1);
+	blk_queue_segment_boundary(q, PAGE_SIZE-1);
 
 	if (get_ldev_if_state(mdev, D_ATTACHING)) {
 		struct request_queue * const b = mdev->ldev->backing_bdev->bd_disk->queue;
* Unmerged path drivers/gpu/drm/armada/armada_gem.c
diff --git a/drivers/gpu/drm/radeon/radeon_ttm.c b/drivers/gpu/drm/radeon/radeon_ttm.c
index 89474bbce8a7..c4173a900d8c 100644
--- a/drivers/gpu/drm/radeon/radeon_ttm.c
+++ b/drivers/gpu/drm/radeon/radeon_ttm.c
@@ -618,7 +618,7 @@ static void radeon_ttm_tt_unpin_userptr(struct ttm_tt *ttm)
 			set_page_dirty(page);
 
 		mark_page_accessed(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 
 	sg_free_table(ttm->sg);
diff --git a/drivers/md/bitmap.c b/drivers/md/bitmap.c
index b2cd760b2877..8f9d4f145b9c 100644
--- a/drivers/md/bitmap.c
+++ b/drivers/md/bitmap.c
@@ -309,7 +309,7 @@ __clear_page_buffers(struct page *page)
 {
 	ClearPagePrivate(page);
 	set_page_private(page, 0);
-	page_cache_release(page);
+	put_page(page);
 }
 static void free_buffers(struct page *page)
 {
diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c
index 828e7c10bd70..d477deadf553 100644
--- a/drivers/media/v4l2-core/videobuf-dma-sg.c
+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c
@@ -317,7 +317,7 @@ int videobuf_dma_free(struct videobuf_dmabuf *dma)
 
 	if (dma->pages) {
 		for (i = 0; i < dma->nr_pages; i++)
-			page_cache_release(dma->pages[i]);
+			put_page(dma->pages[i]);
 		kfree(dma->pages);
 		dma->pages = NULL;
 	}
diff --git a/drivers/misc/ibmasm/ibmasmfs.c b/drivers/misc/ibmasm/ibmasmfs.c
index ce5b75616b45..41df918d55df 100644
--- a/drivers/misc/ibmasm/ibmasmfs.c
+++ b/drivers/misc/ibmasm/ibmasmfs.c
@@ -116,8 +116,8 @@ static int ibmasmfs_fill_super (struct super_block *sb, void *data, int silent)
 {
 	struct inode *root;
 
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = IBMASMFS_MAGIC;
 	sb->s_op = &ibmasmfs_s_ops;
 	sb->s_time_gran = 1;
diff --git a/drivers/misc/vmw_vmci/vmci_queue_pair.c b/drivers/misc/vmw_vmci/vmci_queue_pair.c
index 8ff2e5ee8fb8..cf921ffa603b 100644
--- a/drivers/misc/vmw_vmci/vmci_queue_pair.c
+++ b/drivers/misc/vmw_vmci/vmci_queue_pair.c
@@ -719,7 +719,7 @@ static void qp_release_pages(struct page **pages,
 		if (dirty)
 			set_page_dirty(pages[i]);
 
-		page_cache_release(pages[i]);
+		put_page(pages[i]);
 		pages[i] = NULL;
 	}
 }
diff --git a/drivers/mmc/core/host.c b/drivers/mmc/core/host.c
index 0b8005b111f5..aed920c4c1f4 100644
--- a/drivers/mmc/core/host.c
+++ b/drivers/mmc/core/host.c
@@ -354,11 +354,11 @@ struct mmc_host *mmc_alloc_host(int extra, struct device *dev)
 	 * They have to set these according to their abilities.
 	 */
 	host->max_segs = 1;
-	host->max_seg_size = PAGE_CACHE_SIZE;
+	host->max_seg_size = PAGE_SIZE;
 
-	host->max_req_size = PAGE_CACHE_SIZE;
+	host->max_req_size = PAGE_SIZE;
 	host->max_blk_size = 512;
-	host->max_blk_count = PAGE_CACHE_SIZE / 512;
+	host->max_blk_count = PAGE_SIZE / 512;
 
 	return host;
 }
diff --git a/drivers/mmc/host/sh_mmcif.c b/drivers/mmc/host/sh_mmcif.c
index ba76a532ae30..5b014093f3b0 100644
--- a/drivers/mmc/host/sh_mmcif.c
+++ b/drivers/mmc/host/sh_mmcif.c
@@ -1387,7 +1387,7 @@ static int sh_mmcif_probe(struct platform_device *pdev)
 		mmc->caps |= pd->caps;
 	mmc->max_segs = 32;
 	mmc->max_blk_size = 512;
-	mmc->max_req_size = PAGE_CACHE_SIZE * mmc->max_segs;
+	mmc->max_req_size = PAGE_SIZE * mmc->max_segs;
 	mmc->max_blk_count = mmc->max_req_size / mmc->max_blk_size;
 	mmc->max_seg_size = mmc->max_req_size;
 
diff --git a/drivers/mmc/host/tmio_mmc_dma.c b/drivers/mmc/host/tmio_mmc_dma.c
index 491e9ecc92c2..0dd888744aa5 100644
--- a/drivers/mmc/host/tmio_mmc_dma.c
+++ b/drivers/mmc/host/tmio_mmc_dma.c
@@ -66,7 +66,7 @@ static void tmio_mmc_start_dma_rx(struct tmio_mmc_host *host)
 		}
 	}
 
-	if ((!aligned && (host->sg_len > 1 || sg->length > PAGE_CACHE_SIZE ||
+	if ((!aligned && (host->sg_len > 1 || sg->length > PAGE_SIZE ||
 			  (align & PAGE_MASK))) || !multiple) {
 		ret = -EINVAL;
 		goto pio;
@@ -143,7 +143,7 @@ static void tmio_mmc_start_dma_tx(struct tmio_mmc_host *host)
 		}
 	}
 
-	if ((!aligned && (host->sg_len > 1 || sg->length > PAGE_CACHE_SIZE ||
+	if ((!aligned && (host->sg_len > 1 || sg->length > PAGE_SIZE ||
 			  (align & PAGE_MASK))) || !multiple) {
 		ret = -EINVAL;
 		goto pio;
diff --git a/drivers/mmc/host/tmio_mmc_pio.c b/drivers/mmc/host/tmio_mmc_pio.c
index f508ecb5b8a7..24629326ee24 100644
--- a/drivers/mmc/host/tmio_mmc_pio.c
+++ b/drivers/mmc/host/tmio_mmc_pio.c
@@ -1014,7 +1014,7 @@ int tmio_mmc_host_probe(struct tmio_mmc_host **host,
 	mmc->caps2 = pdata->capabilities2;
 	mmc->max_segs = 32;
 	mmc->max_blk_size = 512;
-	mmc->max_blk_count = (PAGE_CACHE_SIZE / mmc->max_blk_size) *
+	mmc->max_blk_count = (PAGE_SIZE / mmc->max_blk_size) *
 		mmc->max_segs;
 	mmc->max_req_size = mmc->max_blk_size * mmc->max_blk_count;
 	mmc->max_seg_size = mmc->max_req_size;
* Unmerged path drivers/mmc/host/usdhi6rol0.c
diff --git a/drivers/mtd/devices/block2mtd.c b/drivers/mtd/devices/block2mtd.c
index e081bfeaaf7d..501b65e8409a 100644
--- a/drivers/mtd/devices/block2mtd.c
+++ b/drivers/mtd/devices/block2mtd.c
@@ -66,7 +66,7 @@ static int _block2mtd_erase(struct block2mtd_dev *dev, loff_t to, size_t len)
 				break;
 			}
 
-		page_cache_release(page);
+		put_page(page);
 		pages--;
 		index++;
 	}
@@ -115,7 +115,7 @@ static int block2mtd_read(struct mtd_info *mtd, loff_t from, size_t len,
 			return PTR_ERR(page);
 
 		memcpy(buf, page_address(page) + offset, cpylen);
-		page_cache_release(page);
+		put_page(page);
 
 		if (retlen)
 			*retlen += cpylen;
@@ -155,7 +155,7 @@ static int _block2mtd_write(struct block2mtd_dev *dev, const u_char *buf,
 			unlock_page(page);
 			balance_dirty_pages_ratelimited(mapping);
 		}
-		page_cache_release(page);
+		put_page(page);
 
 		if (retlen)
 			*retlen += cpylen;
diff --git a/drivers/mtd/nand/nandsim.c b/drivers/mtd/nand/nandsim.c
index cb38f3d94218..1a56d0e40670 100644
--- a/drivers/mtd/nand/nandsim.c
+++ b/drivers/mtd/nand/nandsim.c
@@ -1348,7 +1348,7 @@ static void put_pages(struct nandsim *ns)
 	int i;
 
 	for (i = 0; i < ns->held_cnt; i++)
-		page_cache_release(ns->held_pages[i]);
+		put_page(ns->held_pages[i]);
 }
 
 /* Get page cache pages in advance to provide NOFS memory allocation */
@@ -1358,8 +1358,8 @@ static int get_pages(struct nandsim *ns, struct file *file, size_t count, loff_t
 	struct page *page;
 	struct address_space *mapping = file->f_mapping;
 
-	start_index = pos >> PAGE_CACHE_SHIFT;
-	end_index = (pos + count - 1) >> PAGE_CACHE_SHIFT;
+	start_index = pos >> PAGE_SHIFT;
+	end_index = (pos + count - 1) >> PAGE_SHIFT;
 	if (end_index - start_index + 1 > NS_MAX_HELD_PAGES)
 		return -EINVAL;
 	ns->held_cnt = 0;
diff --git a/drivers/nvdimm/btt.c b/drivers/nvdimm/btt.c
index 47acfd7e2b84..4e1e8f750ba5 100644
--- a/drivers/nvdimm/btt.c
+++ b/drivers/nvdimm/btt.c
@@ -1199,7 +1199,7 @@ static int btt_rw_page(struct block_device *bdev, sector_t sector,
 {
 	struct btt *btt = bdev->bd_disk->private_data;
 
-	btt_do_bvec(btt, NULL, page, PAGE_CACHE_SIZE, 0, rw, sector);
+	btt_do_bvec(btt, NULL, page, PAGE_SIZE, 0, rw, sector);
 	page_endio(page, rw & WRITE, 0);
 	return 0;
 }
* Unmerged path drivers/nvdimm/pmem.c
diff --git a/drivers/oprofile/oprofilefs.c b/drivers/oprofile/oprofilefs.c
index 7c12d9c2b230..30b8d343984e 100644
--- a/drivers/oprofile/oprofilefs.c
+++ b/drivers/oprofile/oprofilefs.c
@@ -241,8 +241,8 @@ static int oprofilefs_fill_super(struct super_block *sb, void *data, int silent)
 {
 	struct inode *root_inode;
 
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = OPROFILEFS_MAGIC;
 	sb->s_op = &s_ops;
 	sb->s_time_gran = 1;
* Unmerged path drivers/scsi/sd.c
diff --git a/drivers/scsi/st.c b/drivers/scsi/st.c
index 9079ef313395..970b64187b34 100644
--- a/drivers/scsi/st.c
+++ b/drivers/scsi/st.c
@@ -4915,7 +4915,7 @@ static int sgl_map_user_pages(struct st_buffer *STbp,
  out_unmap:
 	if (res > 0) {
 		for (j=0; j < res; j++)
-			page_cache_release(pages[j]);
+			put_page(pages[j]);
 		res = 0;
 	}
 	kfree(pages);
@@ -4937,7 +4937,7 @@ static int sgl_unmap_user_pages(struct st_buffer *STbp,
 		/* FIXME: cache flush missing for rw==READ
 		 * FIXME: call the correct reference counting function
 		 */
-		page_cache_release(page);
+		put_page(page);
 	}
 	kfree(STbp->mapped_pages);
 	STbp->mapped_pages = NULL;
* Unmerged path drivers/staging/lustre/include/linux/libcfs/libcfs_private.h
* Unmerged path drivers/staging/lustre/include/linux/libcfs/linux/linux-mem.h
* Unmerged path drivers/staging/lustre/lnet/klnds/socklnd/socklnd_lib.c
* Unmerged path drivers/staging/lustre/lnet/libcfs/debug.c
* Unmerged path drivers/staging/lustre/lnet/libcfs/tracefile.c
* Unmerged path drivers/staging/lustre/lnet/libcfs/tracefile.h
* Unmerged path drivers/staging/lustre/lnet/lnet/lib-md.c
* Unmerged path drivers/staging/lustre/lnet/lnet/lib-move.c
* Unmerged path drivers/staging/lustre/lnet/lnet/lib-socket.c
* Unmerged path drivers/staging/lustre/lnet/lnet/router.c
* Unmerged path drivers/staging/lustre/lnet/selftest/brw_test.c
* Unmerged path drivers/staging/lustre/lnet/selftest/conctl.c
* Unmerged path drivers/staging/lustre/lnet/selftest/conrpc.c
* Unmerged path drivers/staging/lustre/lnet/selftest/framework.c
* Unmerged path drivers/staging/lustre/lnet/selftest/rpc.c
* Unmerged path drivers/staging/lustre/lnet/selftest/selftest.h
* Unmerged path drivers/staging/lustre/lustre/include/linux/lustre_patchless_compat.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre/lustre_idl.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre_mdc.h
* Unmerged path drivers/staging/lustre/lustre/include/lustre_net.h
* Unmerged path drivers/staging/lustre/lustre/include/obd.h
* Unmerged path drivers/staging/lustre/lustre/include/obd_support.h
* Unmerged path drivers/staging/lustre/lustre/lclient/lcommon_cl.c
* Unmerged path drivers/staging/lustre/lustre/ldlm/ldlm_lib.c
* Unmerged path drivers/staging/lustre/lustre/ldlm/ldlm_pool.c
* Unmerged path drivers/staging/lustre/lustre/ldlm/ldlm_request.c
* Unmerged path drivers/staging/lustre/lustre/llite/dir.c
* Unmerged path drivers/staging/lustre/lustre/llite/llite_internal.h
* Unmerged path drivers/staging/lustre/lustre/llite/llite_lib.c
* Unmerged path drivers/staging/lustre/lustre/llite/llite_mmap.c
* Unmerged path drivers/staging/lustre/lustre/llite/lloop.c
* Unmerged path drivers/staging/lustre/lustre/llite/lproc_llite.c
* Unmerged path drivers/staging/lustre/lustre/llite/rw.c
* Unmerged path drivers/staging/lustre/lustre/llite/rw26.c
* Unmerged path drivers/staging/lustre/lustre/llite/vvp_io.c
* Unmerged path drivers/staging/lustre/lustre/llite/vvp_page.c
* Unmerged path drivers/staging/lustre/lustre/lmv/lmv_obd.c
* Unmerged path drivers/staging/lustre/lustre/mdc/mdc_request.c
* Unmerged path drivers/staging/lustre/lustre/mgc/mgc_request.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/cl_page.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/class_obd.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/linux/linux-obdo.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/linux/linux-sysctl.c
* Unmerged path drivers/staging/lustre/lustre/obdclass/lu_object.c
* Unmerged path drivers/staging/lustre/lustre/obdecho/echo_client.c
* Unmerged path drivers/staging/lustre/lustre/osc/lproc_osc.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_cache.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_page.c
* Unmerged path drivers/staging/lustre/lustre/osc/osc_request.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/client.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/import.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/lproc_ptlrpc.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/recover.c
* Unmerged path drivers/staging/lustre/lustre/ptlrpc/sec_bulk.c
diff --git a/drivers/usb/gadget/f_fs.c b/drivers/usb/gadget/f_fs.c
index b6e9d917221e..42c243e01249 100644
--- a/drivers/usb/gadget/f_fs.c
+++ b/drivers/usb/gadget/f_fs.c
@@ -1048,8 +1048,8 @@ static int ffs_sb_fill(struct super_block *sb, void *_data, int silent)
 	ffs->sb              = sb;
 	data->ffs_data       = NULL;
 	sb->s_fs_info        = ffs;
-	sb->s_blocksize      = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize      = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic          = FUNCTIONFS_MAGIC;
 	sb->s_op             = &ffs_sb_operations;
 	sb->s_time_gran      = 1;
diff --git a/drivers/usb/gadget/inode.c b/drivers/usb/gadget/inode.c
index 570c005062ab..a68b76059719 100644
--- a/drivers/usb/gadget/inode.c
+++ b/drivers/usb/gadget/inode.c
@@ -2056,8 +2056,8 @@ gadgetfs_fill_super (struct super_block *sb, void *opts, int silent)
 		return -ENODEV;
 
 	/* superblock */
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = GADGETFS_MAGIC;
 	sb->s_op = &gadget_fs_operations;
 	sb->s_time_gran = 1;
diff --git a/drivers/usb/storage/scsiglue.c b/drivers/usb/storage/scsiglue.c
index c922d8b64362..6b3e87b14573 100644
--- a/drivers/usb/storage/scsiglue.c
+++ b/drivers/usb/storage/scsiglue.c
@@ -123,7 +123,7 @@ static int slave_configure(struct scsi_device *sdev)
 		unsigned int max_sectors = 64;
 
 		if (us->fflags & US_FL_MAX_SECTORS_MIN)
-			max_sectors = PAGE_CACHE_SIZE >> 9;
+			max_sectors = PAGE_SIZE >> 9;
 		if (queue_max_hw_sectors(sdev->request_queue) > max_sectors)
 			blk_queue_max_hw_sectors(sdev->request_queue,
 					      max_sectors);
diff --git a/drivers/video/pvr2fb.c b/drivers/video/pvr2fb.c
index df07860563e6..470b458fb2e3 100644
--- a/drivers/video/pvr2fb.c
+++ b/drivers/video/pvr2fb.c
@@ -737,7 +737,7 @@ out:
 
 out_unmap:
 	for (i = 0; i < nr_pages; i++)
-		page_cache_release(pages[i]);
+		put_page(pages[i]);
 
 	kfree(pages);
 
* Unmerged path fs/9p/vfs_addr.c
* Unmerged path fs/9p/vfs_file.c
diff --git a/fs/9p/vfs_super.c b/fs/9p/vfs_super.c
index 2756dcd5de6e..08a2eb979cd0 100644
--- a/fs/9p/vfs_super.c
+++ b/fs/9p/vfs_super.c
@@ -87,7 +87,7 @@ v9fs_fill_super(struct super_block *sb, struct v9fs_session_info *v9ses,
 		sb->s_op = &v9fs_super_ops;
 	sb->s_bdi = &v9ses->bdi;
 	if (v9ses->cache)
-		sb->s_bdi->ra_pages = (VM_MAX_READAHEAD * 1024)/PAGE_CACHE_SIZE;
+		sb->s_bdi->ra_pages = (VM_MAX_READAHEAD * 1024)/PAGE_SIZE;
 
 	sb->s_flags |= MS_ACTIVE | MS_DIRSYNC | MS_NOATIME;
 	if (!v9ses->cache)
* Unmerged path fs/affs/file.c
diff --git a/fs/afs/dir.c b/fs/afs/dir.c
index 7a465ed04444..4391154f5194 100644
--- a/fs/afs/dir.c
+++ b/fs/afs/dir.c
@@ -181,7 +181,7 @@ error:
 static inline void afs_dir_put_page(struct page *page)
 {
 	kunmap(page);
-	page_cache_release(page);
+	put_page(page);
 }
 
 /*
* Unmerged path fs/afs/file.c
diff --git a/fs/afs/mntpt.c b/fs/afs/mntpt.c
index 9682c33d5daf..2d732378640a 100644
--- a/fs/afs/mntpt.c
+++ b/fs/afs/mntpt.c
@@ -93,7 +93,7 @@ int afs_mntpt_check_symlink(struct afs_vnode *vnode, struct key *key)
 
 	kunmap(page);
 out_free:
-	page_cache_release(page);
+	put_page(page);
 out:
 	_leave(" = %d", ret);
 	return ret;
@@ -203,7 +203,7 @@ static struct vfsmount *afs_mntpt_do_automount(struct dentry *mntpt)
 		buf = kmap_atomic(page);
 		memcpy(devname, buf, size);
 		kunmap_atomic(buf);
-		page_cache_release(page);
+		put_page(page);
 		page = NULL;
 	}
 
@@ -225,7 +225,7 @@ static struct vfsmount *afs_mntpt_do_automount(struct dentry *mntpt)
 	return mnt;
 
 error:
-	page_cache_release(page);
+	put_page(page);
 error_no_page:
 	free_page((unsigned long) options);
 error_no_options:
diff --git a/fs/afs/super.c b/fs/afs/super.c
index c4861557e385..28f37a2b3b25 100644
--- a/fs/afs/super.c
+++ b/fs/afs/super.c
@@ -315,8 +315,8 @@ static int afs_fill_super(struct super_block *sb,
 	_enter("");
 
 	/* fill in the superblock */
-	sb->s_blocksize		= PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits	= PAGE_CACHE_SHIFT;
+	sb->s_blocksize		= PAGE_SIZE;
+	sb->s_blocksize_bits	= PAGE_SHIFT;
 	sb->s_magic		= AFS_FS_MAGIC;
 	sb->s_op		= &afs_super_ops;
 	sb->s_bdi		= &as->volume->bdi;
diff --git a/fs/afs/write.c b/fs/afs/write.c
index a890db4b9898..c33d2ee1c0e2 100644
--- a/fs/afs/write.c
+++ b/fs/afs/write.c
@@ -94,10 +94,10 @@ static int afs_fill_page(struct afs_vnode *vnode, struct key *key,
 	_enter(",,%llu", (unsigned long long)pos);
 
 	i_size = i_size_read(&vnode->vfs_inode);
-	if (pos + PAGE_CACHE_SIZE > i_size)
+	if (pos + PAGE_SIZE > i_size)
 		len = i_size - pos;
 	else
-		len = PAGE_CACHE_SIZE;
+		len = PAGE_SIZE;
 
 	ret = afs_vnode_fetch_data(vnode, key, pos, len, page);
 	if (ret < 0) {
@@ -124,9 +124,9 @@ int afs_write_begin(struct file *file, struct address_space *mapping,
 	struct afs_vnode *vnode = AFS_FS_I(file_inode(file));
 	struct page *page;
 	struct key *key = file->private_data;
-	unsigned from = pos & (PAGE_CACHE_SIZE - 1);
+	unsigned from = pos & (PAGE_SIZE - 1);
 	unsigned to = from + len;
-	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
+	pgoff_t index = pos >> PAGE_SHIFT;
 	int ret;
 
 	_enter("{%x:%u},{%lx},%u,%u",
@@ -152,8 +152,8 @@ int afs_write_begin(struct file *file, struct address_space *mapping,
 	*pagep = page;
 	/* page won't leak in error case: it eventually gets cleaned off LRU */
 
-	if (!PageUptodate(page) && len != PAGE_CACHE_SIZE) {
-		ret = afs_fill_page(vnode, key, index << PAGE_CACHE_SHIFT, page);
+	if (!PageUptodate(page) && len != PAGE_SIZE) {
+		ret = afs_fill_page(vnode, key, index << PAGE_SHIFT, page);
 		if (ret < 0) {
 			kfree(candidate);
 			_leave(" = %d [prep]", ret);
@@ -267,7 +267,7 @@ int afs_write_end(struct file *file, struct address_space *mapping,
 	if (PageDirty(page))
 		_debug("dirtied");
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
 	return copied;
 }
@@ -481,7 +481,7 @@ static int afs_writepages_region(struct address_space *mapping,
 
 		if (page->index > end) {
 			*_next = index;
-			page_cache_release(page);
+			put_page(page);
 			_leave(" = 0 [%lx]", *_next);
 			return 0;
 		}
@@ -495,7 +495,7 @@ static int afs_writepages_region(struct address_space *mapping,
 
 		if (page->mapping != mapping) {
 			unlock_page(page);
-			page_cache_release(page);
+			put_page(page);
 			continue;
 		}
 
@@ -516,7 +516,7 @@ static int afs_writepages_region(struct address_space *mapping,
 
 		ret = afs_write_back_from_locked_page(wb, page);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		if (ret < 0) {
 			_leave(" = %d", ret);
 			return ret;
@@ -552,13 +552,13 @@ int afs_writepages(struct address_space *mapping,
 						    &next);
 		mapping->writeback_index = next;
 	} else if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX) {
-		end = (pgoff_t)(LLONG_MAX >> PAGE_CACHE_SHIFT);
+		end = (pgoff_t)(LLONG_MAX >> PAGE_SHIFT);
 		ret = afs_writepages_region(mapping, wbc, 0, end, &next);
 		if (wbc->nr_to_write > 0)
 			mapping->writeback_index = next;
 	} else {
-		start = wbc->range_start >> PAGE_CACHE_SHIFT;
-		end = wbc->range_end >> PAGE_CACHE_SHIFT;
+		start = wbc->range_start >> PAGE_SHIFT;
+		end = wbc->range_end >> PAGE_SHIFT;
 		ret = afs_writepages_region(mapping, wbc, start, end, &next);
 	}
 
diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c
index 4b775dc44a97..36a50614e95a 100644
--- a/fs/binfmt_elf.c
+++ b/fs/binfmt_elf.c
@@ -2218,7 +2218,7 @@ static int elf_core_dump(struct coredump_params *cprm)
 					!dump_write(cprm->file, kaddr,
 						    PAGE_SIZE);
 				kunmap(page);
-				page_cache_release(page);
+				put_page(page);
 			} else
 				stop = !dump_seek(cprm->file, PAGE_SIZE);
 			if (stop)
* Unmerged path fs/binfmt_elf_fdpic.c
* Unmerged path fs/bio.c
* Unmerged path fs/block_dev.c
diff --git a/fs/btrfs/check-integrity.c b/fs/btrfs/check-integrity.c
index a1b56d0c9961..fc428947e2b5 100644
--- a/fs/btrfs/check-integrity.c
+++ b/fs/btrfs/check-integrity.c
@@ -767,7 +767,7 @@ static int btrfsic_process_superblock(struct btrfsic_state *state,
 			BUG_ON(NULL == l);
 
 			ret = btrfsic_read_block(state, &tmp_next_block_ctx);
-			if (ret < (int)PAGE_CACHE_SIZE) {
+			if (ret < (int)PAGE_SIZE) {
 				printk(KERN_INFO
 				       "btrfsic: read @logical %llu failed!\n",
 				       tmp_next_block_ctx.start);
@@ -1241,15 +1241,15 @@ static void btrfsic_read_from_block_data(
 	size_t offset_in_page;
 	char *kaddr;
 	char *dst = (char *)dstv;
-	size_t start_offset = block_ctx->start & ((u64)PAGE_CACHE_SIZE - 1);
-	unsigned long i = (start_offset + offset) >> PAGE_CACHE_SHIFT;
+	size_t start_offset = block_ctx->start & ((u64)PAGE_SIZE - 1);
+	unsigned long i = (start_offset + offset) >> PAGE_SHIFT;
 
 	WARN_ON(offset + len > block_ctx->len);
-	offset_in_page = (start_offset + offset) & (PAGE_CACHE_SIZE - 1);
+	offset_in_page = (start_offset + offset) & (PAGE_SIZE - 1);
 
 	while (len > 0) {
-		cur = min(len, ((size_t)PAGE_CACHE_SIZE - offset_in_page));
-		BUG_ON(i >= DIV_ROUND_UP(block_ctx->len, PAGE_CACHE_SIZE));
+		cur = min(len, ((size_t)PAGE_SIZE - offset_in_page));
+		BUG_ON(i >= DIV_ROUND_UP(block_ctx->len, PAGE_SIZE));
 		kaddr = block_ctx->datav[i];
 		memcpy(dst, kaddr + offset_in_page, cur);
 
@@ -1615,8 +1615,8 @@ static void btrfsic_release_block_ctx(struct btrfsic_block_data_ctx *block_ctx)
 
 		BUG_ON(!block_ctx->datav);
 		BUG_ON(!block_ctx->pagev);
-		num_pages = (block_ctx->len + (u64)PAGE_CACHE_SIZE - 1) >>
-			    PAGE_CACHE_SHIFT;
+		num_pages = (block_ctx->len + (u64)PAGE_SIZE - 1) >>
+			    PAGE_SHIFT;
 		while (num_pages > 0) {
 			num_pages--;
 			if (block_ctx->datav[num_pages]) {
@@ -1647,15 +1647,15 @@ static int btrfsic_read_block(struct btrfsic_state *state,
 	BUG_ON(block_ctx->datav);
 	BUG_ON(block_ctx->pagev);
 	BUG_ON(block_ctx->mem_to_free);
-	if (block_ctx->dev_bytenr & ((u64)PAGE_CACHE_SIZE - 1)) {
+	if (block_ctx->dev_bytenr & ((u64)PAGE_SIZE - 1)) {
 		printk(KERN_INFO
 		       "btrfsic: read_block() with unaligned bytenr %llu\n",
 		       block_ctx->dev_bytenr);
 		return -1;
 	}
 
-	num_pages = (block_ctx->len + (u64)PAGE_CACHE_SIZE - 1) >>
-		    PAGE_CACHE_SHIFT;
+	num_pages = (block_ctx->len + (u64)PAGE_SIZE - 1) >>
+		    PAGE_SHIFT;
 	block_ctx->mem_to_free = kzalloc((sizeof(*block_ctx->datav) +
 					  sizeof(*block_ctx->pagev)) *
 					 num_pages, GFP_NOFS);
@@ -1686,8 +1686,8 @@ static int btrfsic_read_block(struct btrfsic_state *state,
 
 		for (j = i; j < num_pages; j++) {
 			ret = bio_add_page(bio, block_ctx->pagev[j],
-					   PAGE_CACHE_SIZE, 0);
-			if (PAGE_CACHE_SIZE != ret)
+					   PAGE_SIZE, 0);
+			if (PAGE_SIZE != ret)
 				break;
 		}
 		if (j == i) {
@@ -1703,7 +1703,7 @@ static int btrfsic_read_block(struct btrfsic_state *state,
 			return -1;
 		}
 		bio_put(bio);
-		dev_bytenr += (j - i) * PAGE_CACHE_SIZE;
+		dev_bytenr += (j - i) * PAGE_SIZE;
 		i = j;
 	}
 	for (i = 0; i < num_pages; i++) {
@@ -1793,9 +1793,9 @@ static int btrfsic_test_for_metadata(struct btrfsic_state *state,
 	u32 crc = ~(u32)0;
 	unsigned int i;
 
-	if (num_pages * PAGE_CACHE_SIZE < state->metablock_size)
+	if (num_pages * PAGE_SIZE < state->metablock_size)
 		return 1; /* not metadata */
-	num_pages = state->metablock_size >> PAGE_CACHE_SHIFT;
+	num_pages = state->metablock_size >> PAGE_SHIFT;
 	h = (struct btrfs_header *)datav[0];
 
 	if (memcmp(h->fsid, state->root->fs_info->fsid, BTRFS_UUID_SIZE))
@@ -1803,8 +1803,8 @@ static int btrfsic_test_for_metadata(struct btrfsic_state *state,
 
 	for (i = 0; i < num_pages; i++) {
 		u8 *data = i ? datav[i] : (datav[i] + BTRFS_CSUM_SIZE);
-		size_t sublen = i ? PAGE_CACHE_SIZE :
-				    (PAGE_CACHE_SIZE - BTRFS_CSUM_SIZE);
+		size_t sublen = i ? PAGE_SIZE :
+				    (PAGE_SIZE - BTRFS_CSUM_SIZE);
 
 		crc = btrfs_crc32c(crc, data, sublen);
 	}
@@ -1851,14 +1851,14 @@ again:
 		if (block->is_superblock) {
 			bytenr = btrfs_super_bytenr((struct btrfs_super_block *)
 						    mapped_datav[0]);
-			if (num_pages * PAGE_CACHE_SIZE <
+			if (num_pages * PAGE_SIZE <
 			    BTRFS_SUPER_INFO_SIZE) {
 				printk(KERN_INFO
 				       "btrfsic: cannot work with too short bios!\n");
 				return;
 			}
 			is_metadata = 1;
-			BUG_ON(BTRFS_SUPER_INFO_SIZE & (PAGE_CACHE_SIZE - 1));
+			BUG_ON(BTRFS_SUPER_INFO_SIZE & (PAGE_SIZE - 1));
 			processed_len = BTRFS_SUPER_INFO_SIZE;
 			if (state->print_mask &
 			    BTRFSIC_PRINT_MASK_TREE_BEFORE_SB_WRITE) {
@@ -1869,7 +1869,7 @@ again:
 		}
 		if (is_metadata) {
 			if (!block->is_superblock) {
-				if (num_pages * PAGE_CACHE_SIZE <
+				if (num_pages * PAGE_SIZE <
 				    state->metablock_size) {
 					printk(KERN_INFO
 					       "btrfsic: cannot work with too short bios!\n");
@@ -1905,7 +1905,7 @@ again:
 			}
 			block->logical_bytenr = bytenr;
 		} else {
-			if (num_pages * PAGE_CACHE_SIZE <
+			if (num_pages * PAGE_SIZE <
 			    state->datablock_size) {
 				printk(KERN_INFO
 				       "btrfsic: cannot work with too short bios!\n");
@@ -2043,7 +2043,7 @@ again:
 			block->logical_bytenr = bytenr;
 			block->is_metadata = 1;
 			if (block->is_superblock) {
-				BUG_ON(PAGE_CACHE_SIZE !=
+				BUG_ON(PAGE_SIZE !=
 				       BTRFS_SUPER_INFO_SIZE);
 				ret = btrfsic_process_written_superblock(
 						state,
@@ -2202,8 +2202,8 @@ again:
 continue_loop:
 	BUG_ON(!processed_len);
 	dev_bytenr += processed_len;
-	mapped_datav += processed_len >> PAGE_CACHE_SHIFT;
-	num_pages -= processed_len >> PAGE_CACHE_SHIFT;
+	mapped_datav += processed_len >> PAGE_SHIFT;
+	num_pages -= processed_len >> PAGE_SHIFT;
 	goto again;
 }
 
@@ -2996,7 +2996,7 @@ static void __btrfsic_submit_bio(int rw, struct bio *bio)
 			goto leave;
 		cur_bytenr = dev_bytenr;
 		for (i = 0; i < bio->bi_vcnt; i++) {
-			BUG_ON(bio->bi_io_vec[i].bv_len != PAGE_CACHE_SIZE);
+			BUG_ON(bio->bi_io_vec[i].bv_len != PAGE_SIZE);
 			mapped_datav[i] = kmap(bio->bi_io_vec[i].bv_page);
 			if (!mapped_datav[i]) {
 				while (i > 0) {
@@ -3079,16 +3079,16 @@ int btrfsic_mount(struct btrfs_root *root,
 	struct list_head *dev_head = &fs_devices->devices;
 	struct btrfs_device *device;
 
-	if (root->nodesize & ((u64)PAGE_CACHE_SIZE - 1)) {
+	if (root->nodesize & ((u64)PAGE_SIZE - 1)) {
 		printk(KERN_INFO
 		       "btrfsic: cannot handle nodesize %d not being a multiple of PAGE_CACHE_SIZE %ld!\n",
-		       root->nodesize, PAGE_CACHE_SIZE);
+		       root->nodesize, PAGE_SIZE);
 		return -1;
 	}
-	if (root->sectorsize & ((u64)PAGE_CACHE_SIZE - 1)) {
+	if (root->sectorsize & ((u64)PAGE_SIZE - 1)) {
 		printk(KERN_INFO
 		       "btrfsic: cannot handle sectorsize %d not being a multiple of PAGE_CACHE_SIZE %ld!\n",
-		       root->sectorsize, PAGE_CACHE_SIZE);
+		       root->sectorsize, PAGE_SIZE);
 		return -1;
 	}
 	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
* Unmerged path fs/btrfs/compression.c
* Unmerged path fs/btrfs/disk-io.c
diff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c
index 1ab1b3a90fbf..92f5a7e69317 100644
--- a/fs/btrfs/extent-tree.c
+++ b/fs/btrfs/extent-tree.c
@@ -3433,7 +3433,7 @@ again:
 		num_pages = 1;
 
 	num_pages *= 16;
-	num_pages *= PAGE_CACHE_SIZE;
+	num_pages *= PAGE_SIZE;
 
 	ret = btrfs_check_data_free_space(inode, 0, num_pages);
 	if (ret)
@@ -4587,7 +4587,7 @@ static void shrink_delalloc(struct btrfs_root *root, u64 to_reclaim, u64 orig,
 	loops = 0;
 	while (delalloc_bytes && loops < 3) {
 		max_reclaim = min(delalloc_bytes, to_reclaim);
-		nr_pages = max_reclaim >> PAGE_CACHE_SHIFT;
+		nr_pages = max_reclaim >> PAGE_SHIFT;
 		btrfs_writeback_inodes_sb_nr(root, nr_pages, items);
 		/*
 		 * We need to wait for the async pages to actually start before
* Unmerged path fs/btrfs/extent_io.c
diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h
index f4c1ae11855f..dcb49a602446 100644
--- a/fs/btrfs/extent_io.h
+++ b/fs/btrfs/extent_io.h
@@ -121,7 +121,7 @@ struct extent_state {
 };
 
 #define INLINE_EXTENT_BUFFER_PAGES 16
-#define MAX_INLINE_EXTENT_BUFFER_SIZE (INLINE_EXTENT_BUFFER_PAGES * PAGE_CACHE_SIZE)
+#define MAX_INLINE_EXTENT_BUFFER_SIZE (INLINE_EXTENT_BUFFER_PAGES * PAGE_SIZE)
 struct extent_buffer {
 	u64 start;
 	unsigned long len;
@@ -299,8 +299,8 @@ void wait_on_extent_buffer_writeback(struct extent_buffer *eb);
 
 static inline unsigned long num_extent_pages(u64 start, u64 len)
 {
-	return ((start + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT) -
-		(start >> PAGE_CACHE_SHIFT);
+	return ((start + len + PAGE_SIZE - 1) >> PAGE_SHIFT) -
+		(start >> PAGE_SHIFT);
 }
 
 static inline void extent_buffer_get(struct extent_buffer *eb)
* Unmerged path fs/btrfs/file-item.c
* Unmerged path fs/btrfs/file.c
* Unmerged path fs/btrfs/free-space-cache.c
* Unmerged path fs/btrfs/inode-map.c
* Unmerged path fs/btrfs/inode.c
* Unmerged path fs/btrfs/ioctl.c
diff --git a/fs/btrfs/lzo.c b/fs/btrfs/lzo.c
index a2f051347731..1adfbe7be6b8 100644
--- a/fs/btrfs/lzo.c
+++ b/fs/btrfs/lzo.c
@@ -55,8 +55,8 @@ static struct list_head *lzo_alloc_workspace(void)
 		return ERR_PTR(-ENOMEM);
 
 	workspace->mem = vmalloc(LZO1X_MEM_COMPRESS);
-	workspace->buf = vmalloc(lzo1x_worst_compress(PAGE_CACHE_SIZE));
-	workspace->cbuf = vmalloc(lzo1x_worst_compress(PAGE_CACHE_SIZE));
+	workspace->buf = vmalloc(lzo1x_worst_compress(PAGE_SIZE));
+	workspace->cbuf = vmalloc(lzo1x_worst_compress(PAGE_SIZE));
 	if (!workspace->mem || !workspace->buf || !workspace->cbuf)
 		goto fail;
 
@@ -116,7 +116,7 @@ static int lzo_compress_pages(struct list_head *ws,
 	*total_out = 0;
 	*total_in = 0;
 
-	in_page = find_get_page(mapping, start >> PAGE_CACHE_SHIFT);
+	in_page = find_get_page(mapping, start >> PAGE_SHIFT);
 	data_in = kmap(in_page);
 
 	/*
@@ -133,10 +133,10 @@ static int lzo_compress_pages(struct list_head *ws,
 	tot_out = LZO_LEN;
 	pages[0] = out_page;
 	nr_pages = 1;
-	pg_bytes_left = PAGE_CACHE_SIZE - LZO_LEN;
+	pg_bytes_left = PAGE_SIZE - LZO_LEN;
 
 	/* compress at most one page of data each time */
-	in_len = min(len, PAGE_CACHE_SIZE);
+	in_len = min(len, PAGE_SIZE);
 	while (tot_in < len) {
 		ret = lzo1x_1_compress(data_in, in_len, workspace->cbuf,
 				       &out_len, workspace->mem);
@@ -201,7 +201,7 @@ static int lzo_compress_pages(struct list_head *ws,
 				cpage_out = kmap(out_page);
 				pages[nr_pages++] = out_page;
 
-				pg_bytes_left = PAGE_CACHE_SIZE;
+				pg_bytes_left = PAGE_SIZE;
 				out_offset = 0;
 			}
 		}
@@ -221,12 +221,12 @@ static int lzo_compress_pages(struct list_head *ws,
 
 		bytes_left = len - tot_in;
 		kunmap(in_page);
-		page_cache_release(in_page);
+		put_page(in_page);
 
-		start += PAGE_CACHE_SIZE;
-		in_page = find_get_page(mapping, start >> PAGE_CACHE_SHIFT);
+		start += PAGE_SIZE;
+		in_page = find_get_page(mapping, start >> PAGE_SHIFT);
 		data_in = kmap(in_page);
-		in_len = min(bytes_left, PAGE_CACHE_SIZE);
+		in_len = min(bytes_left, PAGE_SIZE);
 	}
 
 	if (tot_out > tot_in)
@@ -248,7 +248,7 @@ out:
 
 	if (in_page) {
 		kunmap(in_page);
-		page_cache_release(in_page);
+		put_page(in_page);
 	}
 
 	return ret;
@@ -266,7 +266,7 @@ static int lzo_decompress_biovec(struct list_head *ws,
 	char *data_in;
 	unsigned long page_in_index = 0;
 	unsigned long page_out_index = 0;
-	unsigned long total_pages_in = DIV_ROUND_UP(srclen, PAGE_CACHE_SIZE);
+	unsigned long total_pages_in = DIV_ROUND_UP(srclen, PAGE_SIZE);
 	unsigned long buf_start;
 	unsigned long buf_offset = 0;
 	unsigned long bytes;
@@ -289,7 +289,7 @@ static int lzo_decompress_biovec(struct list_head *ws,
 	tot_in = LZO_LEN;
 	in_offset = LZO_LEN;
 	tot_len = min_t(size_t, srclen, tot_len);
-	in_page_bytes_left = PAGE_CACHE_SIZE - LZO_LEN;
+	in_page_bytes_left = PAGE_SIZE - LZO_LEN;
 
 	tot_out = 0;
 	pg_offset = 0;
@@ -345,12 +345,12 @@ cont:
 
 				data_in = kmap(pages_in[++page_in_index]);
 
-				in_page_bytes_left = PAGE_CACHE_SIZE;
+				in_page_bytes_left = PAGE_SIZE;
 				in_offset = 0;
 			}
 		}
 
-		out_len = lzo1x_worst_compress(PAGE_CACHE_SIZE);
+		out_len = lzo1x_worst_compress(PAGE_SIZE);
 		ret = lzo1x_decompress_safe(buf, in_len, workspace->buf,
 					    &out_len);
 		if (need_unmap)
@@ -399,7 +399,7 @@ static int lzo_decompress(struct list_head *ws, unsigned char *data_in,
 	in_len = read_compress_length(data_in);
 	data_in += LZO_LEN;
 
-	out_len = PAGE_CACHE_SIZE;
+	out_len = PAGE_SIZE;
 	ret = lzo1x_decompress_safe(data_in, in_len, workspace->buf, &out_len);
 	if (ret != LZO_E_OK) {
 		printk(KERN_WARNING "BTRFS: decompress failed!\n");
* Unmerged path fs/btrfs/raid56.c
* Unmerged path fs/btrfs/reada.c
diff --git a/fs/btrfs/relocation.c b/fs/btrfs/relocation.c
index b4ca5454ef1a..7d13b90bbb71 100644
--- a/fs/btrfs/relocation.c
+++ b/fs/btrfs/relocation.c
@@ -3128,10 +3128,10 @@ static int relocate_file_extent_cluster(struct inode *inode,
 	if (ret)
 		goto out;
 
-	index = (cluster->start - offset) >> PAGE_CACHE_SHIFT;
-	last_index = (cluster->end - offset) >> PAGE_CACHE_SHIFT;
+	index = (cluster->start - offset) >> PAGE_SHIFT;
+	last_index = (cluster->end - offset) >> PAGE_SHIFT;
 	while (index <= last_index) {
-		ret = btrfs_delalloc_reserve_metadata(inode, PAGE_CACHE_SIZE);
+		ret = btrfs_delalloc_reserve_metadata(inode, PAGE_SIZE);
 		if (ret)
 			goto out;
 
@@ -3144,7 +3144,7 @@ static int relocate_file_extent_cluster(struct inode *inode,
 						   mask);
 			if (!page) {
 				btrfs_delalloc_release_metadata(inode,
-							PAGE_CACHE_SIZE);
+							PAGE_SIZE);
 				ret = -ENOMEM;
 				goto out;
 			}
@@ -3161,16 +3161,16 @@ static int relocate_file_extent_cluster(struct inode *inode,
 			lock_page(page);
 			if (!PageUptodate(page)) {
 				unlock_page(page);
-				page_cache_release(page);
+				put_page(page);
 				btrfs_delalloc_release_metadata(inode,
-							PAGE_CACHE_SIZE);
+							PAGE_SIZE);
 				ret = -EIO;
 				goto out;
 			}
 		}
 
 		page_start = page_offset(page);
-		page_end = page_start + PAGE_CACHE_SIZE - 1;
+		page_end = page_start + PAGE_SIZE - 1;
 
 		lock_extent(&BTRFS_I(inode)->io_tree, page_start, page_end);
 
@@ -3190,7 +3190,7 @@ static int relocate_file_extent_cluster(struct inode *inode,
 		unlock_extent(&BTRFS_I(inode)->io_tree,
 			      page_start, page_end);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 
 		index++;
 		balance_dirty_pages_ratelimited(inode->i_mapping);
diff --git a/fs/btrfs/scrub.c b/fs/btrfs/scrub.c
index 171130d3fefe..5ceabf46dc15 100644
--- a/fs/btrfs/scrub.c
+++ b/fs/btrfs/scrub.c
@@ -716,7 +716,7 @@ static int scrub_fixup_readpage(u64 inum, u64 offset, u64 root, void *fixup_ctx)
 	if (IS_ERR(inode))
 		return PTR_ERR(inode);
 
-	index = offset >> PAGE_CACHE_SHIFT;
+	index = offset >> PAGE_SHIFT;
 
 	page = find_or_create_page(inode->i_mapping, index, GFP_NOFS);
 	if (!page) {
@@ -1651,7 +1651,7 @@ static int scrub_write_page_to_dev_replace(struct scrub_block *sblock,
 	if (spage->io_error) {
 		void *mapped_buffer = kmap_atomic(spage->page);
 
-		memset(mapped_buffer, 0, PAGE_CACHE_SIZE);
+		memset(mapped_buffer, 0, PAGE_SIZE);
 		flush_dcache_page(spage->page);
 		kunmap_atomic(mapped_buffer);
 	}
@@ -4308,8 +4308,8 @@ static int copy_nocow_pages_for_inode(u64 inum, u64 offset, u64 root,
 		goto out;
 	}
 
-	while (len >= PAGE_CACHE_SIZE) {
-		index = offset >> PAGE_CACHE_SHIFT;
+	while (len >= PAGE_SIZE) {
+		index = offset >> PAGE_SHIFT;
 again:
 		page = find_or_create_page(inode->i_mapping, index, GFP_NOFS);
 		if (!page) {
@@ -4340,7 +4340,7 @@ again:
 			 */
 			if (page->mapping != inode->i_mapping) {
 				unlock_page(page);
-				page_cache_release(page);
+				put_page(page);
 				goto again;
 			}
 			if (!PageUptodate(page)) {
@@ -4362,15 +4362,15 @@ again:
 			ret = err;
 next_page:
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 
 		if (ret)
 			break;
 
-		offset += PAGE_CACHE_SIZE;
-		physical_for_dev_replace += PAGE_CACHE_SIZE;
-		nocow_ctx_logical += PAGE_CACHE_SIZE;
-		len -= PAGE_CACHE_SIZE;
+		offset += PAGE_SIZE;
+		physical_for_dev_replace += PAGE_SIZE;
+		nocow_ctx_logical += PAGE_SIZE;
+		len -= PAGE_SIZE;
 	}
 	ret = COPY_COMPLETE;
 out:
@@ -4404,8 +4404,8 @@ static int write_page_nocow(struct scrub_ctx *sctx,
 	bio->bi_size = 0;
 	bio->bi_sector = physical_for_dev_replace >> 9;
 	bio->bi_bdev = dev->bdev;
-	ret = bio_add_page(bio, page, PAGE_CACHE_SIZE, 0);
-	if (ret != PAGE_CACHE_SIZE) {
+	ret = bio_add_page(bio, page, PAGE_SIZE, 0);
+	if (ret != PAGE_SIZE) {
 leave_with_eio:
 		bio_put(bio);
 		btrfs_dev_stat_inc_and_print(dev, BTRFS_DEV_STAT_WRITE_ERRS);
* Unmerged path fs/btrfs/send.c
* Unmerged path fs/btrfs/tests/extent-io-tests.c
* Unmerged path fs/btrfs/tests/free-space-tests.c
diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c
index a85624a8d07e..bb81891d0b9d 100644
--- a/fs/btrfs/volumes.c
+++ b/fs/btrfs/volumes.c
@@ -1023,16 +1023,16 @@ int btrfs_scan_one_device(const char *path, fmode_t flags, void *holder,
 	}
 
 	/* make sure our super fits in the device */
-	if (bytenr + PAGE_CACHE_SIZE >= i_size_read(bdev->bd_inode))
+	if (bytenr + PAGE_SIZE >= i_size_read(bdev->bd_inode))
 		goto error_bdev_put;
 
 	/* make sure our super fits in the page */
-	if (sizeof(*disk_super) > PAGE_CACHE_SIZE)
+	if (sizeof(*disk_super) > PAGE_SIZE)
 		goto error_bdev_put;
 
 	/* make sure our super doesn't straddle pages on disk */
-	index = bytenr >> PAGE_CACHE_SHIFT;
-	if ((bytenr + sizeof(*disk_super) - 1) >> PAGE_CACHE_SHIFT != index)
+	index = bytenr >> PAGE_SHIFT;
+	if ((bytenr + sizeof(*disk_super) - 1) >> PAGE_SHIFT != index)
 		goto error_bdev_put;
 
 	/* pull in the page with our super */
@@ -1045,7 +1045,7 @@ int btrfs_scan_one_device(const char *path, fmode_t flags, void *holder,
 	p = kmap(page);
 
 	/* align our pointer to the offset of the super block */
-	disk_super = p + (bytenr & ~PAGE_CACHE_MASK);
+	disk_super = p + (bytenr & ~PAGE_MASK);
 
 	if (btrfs_super_bytenr(disk_super) != bytenr ||
 	    btrfs_super_magic(disk_super) != BTRFS_MAGIC)
@@ -1073,7 +1073,7 @@ int btrfs_scan_one_device(const char *path, fmode_t flags, void *holder,
 
 error_unmap:
 	kunmap(page);
-	page_cache_release(page);
+	put_page(page);
 
 error_bdev_put:
 	blkdev_put(bdev, flags);
@@ -6553,7 +6553,7 @@ int btrfs_read_sys_array(struct btrfs_root *root)
 	 * but sb spans only this function. Add an explicit SetPageUptodate call
 	 * to silence the warning eg. on PowerPC 64.
 	 */
-	if (PAGE_CACHE_SIZE > BTRFS_SUPER_INFO_SIZE)
+	if (PAGE_SIZE > BTRFS_SUPER_INFO_SIZE)
 		SetPageUptodate(sb->pages[0]);
 
 	write_extent_buffer(sb, super_copy, 0, BTRFS_SUPER_INFO_SIZE);
diff --git a/fs/btrfs/zlib.c b/fs/btrfs/zlib.c
index 82990b8f872b..88d274e8ecf2 100644
--- a/fs/btrfs/zlib.c
+++ b/fs/btrfs/zlib.c
@@ -59,7 +59,7 @@ static struct list_head *zlib_alloc_workspace(void)
 	workspacesize = max(zlib_deflate_workspacesize(MAX_WBITS, MAX_MEM_LEVEL),
 			zlib_inflate_workspacesize());
 	workspace->strm.workspace = vmalloc(workspacesize);
-	workspace->buf = kmalloc(PAGE_CACHE_SIZE, GFP_NOFS);
+	workspace->buf = kmalloc(PAGE_SIZE, GFP_NOFS);
 	if (!workspace->strm.workspace || !workspace->buf)
 		goto fail;
 
@@ -103,7 +103,7 @@ static int zlib_compress_pages(struct list_head *ws,
 	workspace->strm.total_in = 0;
 	workspace->strm.total_out = 0;
 
-	in_page = find_get_page(mapping, start >> PAGE_CACHE_SHIFT);
+	in_page = find_get_page(mapping, start >> PAGE_SHIFT);
 	data_in = kmap(in_page);
 
 	out_page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
@@ -117,8 +117,8 @@ static int zlib_compress_pages(struct list_head *ws,
 
 	workspace->strm.next_in = data_in;
 	workspace->strm.next_out = cpage_out;
-	workspace->strm.avail_out = PAGE_CACHE_SIZE;
-	workspace->strm.avail_in = min(len, PAGE_CACHE_SIZE);
+	workspace->strm.avail_out = PAGE_SIZE;
+	workspace->strm.avail_in = min(len, PAGE_SIZE);
 
 	while (workspace->strm.total_in < len) {
 		ret = zlib_deflate(&workspace->strm, Z_SYNC_FLUSH);
@@ -156,7 +156,7 @@ static int zlib_compress_pages(struct list_head *ws,
 			cpage_out = kmap(out_page);
 			pages[nr_pages] = out_page;
 			nr_pages++;
-			workspace->strm.avail_out = PAGE_CACHE_SIZE;
+			workspace->strm.avail_out = PAGE_SIZE;
 			workspace->strm.next_out = cpage_out;
 		}
 		/* we're all done */
@@ -170,14 +170,14 @@ static int zlib_compress_pages(struct list_head *ws,
 
 			bytes_left = len - workspace->strm.total_in;
 			kunmap(in_page);
-			page_cache_release(in_page);
+			put_page(in_page);
 
-			start += PAGE_CACHE_SIZE;
+			start += PAGE_SIZE;
 			in_page = find_get_page(mapping,
-						start >> PAGE_CACHE_SHIFT);
+						start >> PAGE_SHIFT);
 			data_in = kmap(in_page);
 			workspace->strm.avail_in = min(bytes_left,
-							   PAGE_CACHE_SIZE);
+							   PAGE_SIZE);
 			workspace->strm.next_in = data_in;
 		}
 	}
@@ -205,7 +205,7 @@ out:
 
 	if (in_page) {
 		kunmap(in_page);
-		page_cache_release(in_page);
+		put_page(in_page);
 	}
 	return ret;
 }
@@ -223,18 +223,18 @@ static int zlib_decompress_biovec(struct list_head *ws, struct page **pages_in,
 	size_t total_out = 0;
 	unsigned long page_in_index = 0;
 	unsigned long page_out_index = 0;
-	unsigned long total_pages_in = DIV_ROUND_UP(srclen, PAGE_CACHE_SIZE);
+	unsigned long total_pages_in = DIV_ROUND_UP(srclen, PAGE_SIZE);
 	unsigned long buf_start;
 	unsigned long pg_offset;
 
 	data_in = kmap(pages_in[page_in_index]);
 	workspace->strm.next_in = data_in;
-	workspace->strm.avail_in = min_t(size_t, srclen, PAGE_CACHE_SIZE);
+	workspace->strm.avail_in = min_t(size_t, srclen, PAGE_SIZE);
 	workspace->strm.total_in = 0;
 
 	workspace->strm.total_out = 0;
 	workspace->strm.next_out = workspace->buf;
-	workspace->strm.avail_out = PAGE_CACHE_SIZE;
+	workspace->strm.avail_out = PAGE_SIZE;
 	pg_offset = 0;
 
 	/* If it's deflate, and it's got no preset dictionary, then
@@ -274,7 +274,7 @@ static int zlib_decompress_biovec(struct list_head *ws, struct page **pages_in,
 		}
 
 		workspace->strm.next_out = workspace->buf;
-		workspace->strm.avail_out = PAGE_CACHE_SIZE;
+		workspace->strm.avail_out = PAGE_SIZE;
 
 		if (workspace->strm.avail_in == 0) {
 			unsigned long tmp;
@@ -288,7 +288,7 @@ static int zlib_decompress_biovec(struct list_head *ws, struct page **pages_in,
 			workspace->strm.next_in = data_in;
 			tmp = srclen - workspace->strm.total_in;
 			workspace->strm.avail_in = min(tmp,
-							   PAGE_CACHE_SIZE);
+							   PAGE_SIZE);
 		}
 	}
 	if (ret != Z_STREAM_END)
@@ -325,7 +325,7 @@ static int zlib_decompress(struct list_head *ws, unsigned char *data_in,
 	workspace->strm.total_in = 0;
 
 	workspace->strm.next_out = workspace->buf;
-	workspace->strm.avail_out = PAGE_CACHE_SIZE;
+	workspace->strm.avail_out = PAGE_SIZE;
 	workspace->strm.total_out = 0;
 	/* If it's deflate, and it's got no preset dictionary, then
 	   we can tell zlib to skip the adler32 check. */
@@ -368,8 +368,8 @@ static int zlib_decompress(struct list_head *ws, unsigned char *data_in,
 		else
 			buf_offset = 0;
 
-		bytes = min(PAGE_CACHE_SIZE - pg_offset,
-			    PAGE_CACHE_SIZE - buf_offset);
+		bytes = min(PAGE_SIZE - pg_offset,
+			    PAGE_SIZE - buf_offset);
 		bytes = min(bytes, bytes_left);
 
 		kaddr = kmap_atomic(dest_page);
@@ -380,7 +380,7 @@ static int zlib_decompress(struct list_head *ws, unsigned char *data_in,
 		bytes_left -= bytes;
 next:
 		workspace->strm.next_out = workspace->buf;
-		workspace->strm.avail_out = PAGE_CACHE_SIZE;
+		workspace->strm.avail_out = PAGE_SIZE;
 	}
 
 	if (ret != Z_STREAM_END && bytes_left != 0)
* Unmerged path fs/buffer.c
diff --git a/fs/cachefiles/rdwr.c b/fs/cachefiles/rdwr.c
index 1db6647851c1..02adbe9de010 100644
--- a/fs/cachefiles/rdwr.c
+++ b/fs/cachefiles/rdwr.c
@@ -194,10 +194,10 @@ static void cachefiles_read_copier(struct fscache_operation *_op)
 			error = -EIO;
 		}
 
-		page_cache_release(monitor->back_page);
+		put_page(monitor->back_page);
 
 		fscache_end_io(op, monitor->netfs_page, error);
-		page_cache_release(monitor->netfs_page);
+		put_page(monitor->netfs_page);
 		fscache_retrieval_complete(op, 1);
 		fscache_put_retrieval(op);
 		kfree(monitor);
@@ -288,8 +288,8 @@ monitor_backing_page:
 	_debug("- monitor add");
 
 	/* install the monitor */
-	page_cache_get(monitor->netfs_page);
-	page_cache_get(backpage);
+	get_page(monitor->netfs_page);
+	get_page(backpage);
 	monitor->back_page = backpage;
 	monitor->monitor.private = backpage;
 	add_page_wait_queue(backpage, &monitor->monitor);
@@ -310,7 +310,7 @@ backing_page_already_present:
 	_debug("- present");
 
 	if (newpage) {
-		page_cache_release(newpage);
+		put_page(newpage);
 		newpage = NULL;
 	}
 
@@ -342,7 +342,7 @@ success:
 
 out:
 	if (backpage)
-		page_cache_release(backpage);
+		put_page(backpage);
 	if (monitor) {
 		fscache_put_retrieval(monitor->op);
 		kfree(monitor);
@@ -363,7 +363,7 @@ io_error:
 	goto out;
 
 nomem_page:
-	page_cache_release(newpage);
+	put_page(newpage);
 nomem_monitor:
 	fscache_put_retrieval(monitor->op);
 	kfree(monitor);
@@ -530,7 +530,7 @@ static int cachefiles_read_backing_file(struct cachefiles_object *object,
 					    netpage->index, cachefiles_gfp);
 		if (ret < 0) {
 			if (ret == -EEXIST) {
-				page_cache_release(netpage);
+				put_page(netpage);
 				fscache_retrieval_complete(op, 1);
 				continue;
 			}
@@ -538,10 +538,10 @@ static int cachefiles_read_backing_file(struct cachefiles_object *object,
 		}
 
 		/* install a monitor */
-		page_cache_get(netpage);
+		get_page(netpage);
 		monitor->netfs_page = netpage;
 
-		page_cache_get(backpage);
+		get_page(backpage);
 		monitor->back_page = backpage;
 		monitor->monitor.private = backpage;
 		add_page_wait_queue(backpage, &monitor->monitor);
@@ -555,10 +555,10 @@ static int cachefiles_read_backing_file(struct cachefiles_object *object,
 			unlock_page(backpage);
 		}
 
-		page_cache_release(backpage);
+		put_page(backpage);
 		backpage = NULL;
 
-		page_cache_release(netpage);
+		put_page(netpage);
 		netpage = NULL;
 		continue;
 
@@ -603,7 +603,7 @@ static int cachefiles_read_backing_file(struct cachefiles_object *object,
 					    netpage->index, cachefiles_gfp);
 		if (ret < 0) {
 			if (ret == -EEXIST) {
-				page_cache_release(netpage);
+				put_page(netpage);
 				fscache_retrieval_complete(op, 1);
 				continue;
 			}
@@ -612,14 +612,14 @@ static int cachefiles_read_backing_file(struct cachefiles_object *object,
 
 		copy_highpage(netpage, backpage);
 
-		page_cache_release(backpage);
+		put_page(backpage);
 		backpage = NULL;
 
 		fscache_mark_page_cached(op, netpage);
 
 		/* the netpage is unlocked and marked up to date here */
 		fscache_end_io(op, netpage, 0);
-		page_cache_release(netpage);
+		put_page(netpage);
 		netpage = NULL;
 		fscache_retrieval_complete(op, 1);
 		continue;
@@ -631,11 +631,11 @@ static int cachefiles_read_backing_file(struct cachefiles_object *object,
 
 out:
 	if (newpage)
-		page_cache_release(newpage);
+		put_page(newpage);
 	if (netpage)
-		page_cache_release(netpage);
+		put_page(netpage);
 	if (backpage)
-		page_cache_release(backpage);
+		put_page(backpage);
 	if (monitor) {
 		fscache_put_retrieval(op);
 		kfree(monitor);
@@ -643,7 +643,7 @@ out:
 
 	list_for_each_entry_safe(netpage, _n, list, lru) {
 		list_del(&netpage->lru);
-		page_cache_release(netpage);
+		put_page(netpage);
 		fscache_retrieval_complete(op, 1);
 	}
 
* Unmerged path fs/ceph/addr.c
diff --git a/fs/ceph/caps.c b/fs/ceph/caps.c
index 6d4458adcf52..f7b22fb33280 100644
--- a/fs/ceph/caps.c
+++ b/fs/ceph/caps.c
@@ -2508,7 +2508,7 @@ int ceph_get_caps(struct ceph_inode_info *ci, int need, int want,
 					*pinned_page = page;
 					break;
 				}
-				page_cache_release(page);
+				put_page(page);
 			}
 			/*
 			 * drop cap refs first because getattr while
diff --git a/fs/ceph/dir.c b/fs/ceph/dir.c
index 80058e8a3304..3e62649dbcea 100644
--- a/fs/ceph/dir.c
+++ b/fs/ceph/dir.c
@@ -130,7 +130,7 @@ static int __dcache_readdir(struct file *filp,
 	struct inode *dir = parent->d_inode;
 	struct dentry *dentry, *last = NULL;
 	struct ceph_dentry_info *di;
-	unsigned nsize = PAGE_CACHE_SIZE / sizeof(struct dentry *);
+	unsigned nsize = PAGE_SIZE / sizeof(struct dentry *);
 	int err = 0;
 	loff_t ptr_pos = 0;
 	struct ceph_readdir_cache_control cache_ctl = {};
@@ -155,7 +155,7 @@ static int __dcache_readdir(struct file *filp,
 		}
 
 		err = -EAGAIN;
-		pgoff = ptr_pos >> PAGE_CACHE_SHIFT;
+		pgoff = ptr_pos >> PAGE_SHIFT;
 		if (!cache_ctl.page || pgoff != page_index(cache_ctl.page)) {
 			ceph_readdir_cache_release(&cache_ctl);
 			cache_ctl.page = find_lock_page(&dir->i_data, pgoff);
* Unmerged path fs/ceph/file.c
diff --git a/fs/ceph/inode.c b/fs/ceph/inode.c
index cda3de0d50c9..12179d1fbd74 100644
--- a/fs/ceph/inode.c
+++ b/fs/ceph/inode.c
@@ -1315,7 +1315,7 @@ void ceph_readdir_cache_release(struct ceph_readdir_cache_control *ctl)
 {
 	if (ctl->page) {
 		kunmap(ctl->page);
-		page_cache_release(ctl->page);
+		put_page(ctl->page);
 		ctl->page = NULL;
 	}
 }
@@ -1325,7 +1325,7 @@ static int fill_readdir_cache(struct inode *dir, struct dentry *dn,
 			      struct ceph_mds_request *req)
 {
 	struct ceph_inode_info *ci = ceph_inode(dir);
-	unsigned nsize = PAGE_CACHE_SIZE / sizeof(struct dentry*);
+	unsigned nsize = PAGE_SIZE / sizeof(struct dentry*);
 	unsigned idx = ctl->index % nsize;
 	pgoff_t pgoff = ctl->index / nsize;
 
@@ -1344,7 +1344,7 @@ static int fill_readdir_cache(struct inode *dir, struct dentry *dn,
 		unlock_page(ctl->page);
 		ctl->dentries = kmap(ctl->page);
 		if (idx == 0)
-			memset(ctl->dentries, 0, PAGE_CACHE_SIZE);
+			memset(ctl->dentries, 0, PAGE_SIZE);
 	}
 
 	if (req->r_dir_release_cnt == atomic64_read(&ci->i_release_count) &&
diff --git a/fs/ceph/mds_client.c b/fs/ceph/mds_client.c
index 550ac1946b98..18ad57bae41a 100644
--- a/fs/ceph/mds_client.c
+++ b/fs/ceph/mds_client.c
@@ -1579,7 +1579,7 @@ again:
 	while (!list_empty(&tmp_list)) {
 		if (!msg) {
 			msg = ceph_msg_new(CEPH_MSG_CLIENT_CAPRELEASE,
-					PAGE_CACHE_SIZE, GFP_NOFS, false);
+					PAGE_SIZE, GFP_NOFS, false);
 			if (!msg)
 				goto out_err;
 			head = msg->front.iov_base;
diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 37712ccffcc6..ee69a537dba5 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -97,7 +97,7 @@ struct ceph_mds_reply_info_parsed {
 /*
  * cap releases are batched and sent to the MDS en masse.
  */
-#define CEPH_CAPS_PER_RELEASE ((PAGE_CACHE_SIZE -			\
+#define CEPH_CAPS_PER_RELEASE ((PAGE_SIZE -			\
 				sizeof(struct ceph_mds_cap_release)) /	\
 			       sizeof(struct ceph_mds_cap_item))
 
* Unmerged path fs/ceph/super.c
* Unmerged path fs/cifs/cifsfs.c
diff --git a/fs/cifs/cifssmb.c b/fs/cifs/cifssmb.c
index 76fcb50295a3..a894bf809ff7 100644
--- a/fs/cifs/cifssmb.c
+++ b/fs/cifs/cifssmb.c
@@ -1929,17 +1929,17 @@ cifs_writev_requeue(struct cifs_writedata *wdata)
 
 		wsize = server->ops->wp_retry_size(inode);
 		if (wsize < rest_len) {
-			nr_pages = wsize / PAGE_CACHE_SIZE;
+			nr_pages = wsize / PAGE_SIZE;
 			if (!nr_pages) {
 				rc = -ENOTSUPP;
 				break;
 			}
-			cur_len = nr_pages * PAGE_CACHE_SIZE;
-			tailsz = PAGE_CACHE_SIZE;
+			cur_len = nr_pages * PAGE_SIZE;
+			tailsz = PAGE_SIZE;
 		} else {
-			nr_pages = DIV_ROUND_UP(rest_len, PAGE_CACHE_SIZE);
+			nr_pages = DIV_ROUND_UP(rest_len, PAGE_SIZE);
 			cur_len = rest_len;
-			tailsz = rest_len - (nr_pages - 1) * PAGE_CACHE_SIZE;
+			tailsz = rest_len - (nr_pages - 1) * PAGE_SIZE;
 		}
 
 		wdata2 = cifs_writedata_alloc(nr_pages, cifs_writev_complete);
@@ -1957,7 +1957,7 @@ cifs_writev_requeue(struct cifs_writedata *wdata)
 		wdata2->sync_mode = wdata->sync_mode;
 		wdata2->nr_pages = nr_pages;
 		wdata2->offset = page_offset(wdata2->pages[0]);
-		wdata2->pagesz = PAGE_CACHE_SIZE;
+		wdata2->pagesz = PAGE_SIZE;
 		wdata2->tailsz = tailsz;
 		wdata2->bytes = cur_len;
 
@@ -1975,7 +1975,7 @@ cifs_writev_requeue(struct cifs_writedata *wdata)
 			if (rc != 0 && rc != -EAGAIN) {
 				SetPageError(wdata2->pages[j]);
 				end_page_writeback(wdata2->pages[j]);
-				page_cache_release(wdata2->pages[j]);
+				put_page(wdata2->pages[j]);
 			}
 		}
 
@@ -2018,7 +2018,7 @@ cifs_writev_complete(struct work_struct *work)
 		else if (wdata->result < 0)
 			SetPageError(page);
 		end_page_writeback(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 	if (wdata->result != -EAGAIN)
 		mapping_set_error(inode->i_mapping, wdata->result);
diff --git a/fs/cifs/connect.c b/fs/cifs/connect.c
index 0668616da760..6b4a301a6033 100644
--- a/fs/cifs/connect.c
+++ b/fs/cifs/connect.c
@@ -3692,7 +3692,7 @@ try_mount_again:
 	cifs_sb->rsize = server->ops->negotiate_rsize(tcon, volume_info);
 
 	/* tune readahead according to rsize */
-	cifs_sb->bdi.ra_pages = cifs_sb->rsize / PAGE_CACHE_SIZE;
+	cifs_sb->bdi.ra_pages = cifs_sb->rsize / PAGE_SIZE;
 
 remote_path_check:
 #ifdef CONFIG_CIFS_DFS_UPCALL
* Unmerged path fs/cifs/file.c
diff --git a/fs/cifs/inode.c b/fs/cifs/inode.c
index a8010b9f6d0c..3274aaed9ebc 100644
--- a/fs/cifs/inode.c
+++ b/fs/cifs/inode.c
@@ -59,7 +59,7 @@ static void cifs_set_ops(struct inode *inode)
 
 		/* check if server can support readpages */
 		if (cifs_sb_master_tcon(cifs_sb)->ses->server->maxBuf <
-				PAGE_CACHE_SIZE + MAX_CIFS_HDR_SIZE)
+				PAGE_SIZE + MAX_CIFS_HDR_SIZE)
 			inode->i_data.a_ops = &cifs_addr_ops_smallbuf;
 		else
 			inode->i_data.a_ops = &cifs_addr_ops;
@@ -2040,8 +2040,8 @@ int cifs_getattr(struct vfsmount *mnt, struct dentry *dentry,
 
 static int cifs_truncate_page(struct address_space *mapping, loff_t from)
 {
-	pgoff_t index = from >> PAGE_CACHE_SHIFT;
-	unsigned offset = from & (PAGE_CACHE_SIZE - 1);
+	pgoff_t index = from >> PAGE_SHIFT;
+	unsigned offset = from & (PAGE_SIZE - 1);
 	struct page *page;
 	int rc = 0;
 
@@ -2049,9 +2049,9 @@ static int cifs_truncate_page(struct address_space *mapping, loff_t from)
 	if (!page)
 		return -ENOMEM;
 
-	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
+	zero_user_segment(page, offset, PAGE_SIZE);
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return rc;
 }
 
diff --git a/fs/configfs/mount.c b/fs/configfs/mount.c
index 7f26c3cf75ae..25347cbedc6e 100644
--- a/fs/configfs/mount.c
+++ b/fs/configfs/mount.c
@@ -71,8 +71,8 @@ static int configfs_fill_super(struct super_block *sb, void *data, int silent)
 	struct inode *inode;
 	struct dentry *root;
 
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = CONFIGFS_MAGIC;
 	sb->s_op = &configfs_ops;
 	sb->s_time_gran = 1;
* Unmerged path fs/cramfs/inode.c
* Unmerged path fs/crypto/crypto.c
diff --git a/fs/dax.c b/fs/dax.c
index 365b0662df80..9381f4697685 100644
--- a/fs/dax.c
+++ b/fs/dax.c
@@ -344,7 +344,7 @@ static int dax_load_hole(struct address_space *mapping, struct page *page,
 	size = (i_size_read(inode) + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	if (vmf->pgoff >= size) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		return VM_FAULT_SIGBUS;
 	}
 
@@ -372,7 +372,7 @@ static int copy_user_bh(struct page *to, struct inode *inode,
 }
 
 #define NO_SECTOR -1
-#define DAX_PMD_INDEX(page_index) (page_index & (PMD_MASK >> PAGE_CACHE_SHIFT))
+#define DAX_PMD_INDEX(page_index) (page_index & (PMD_MASK >> PAGE_SHIFT))
 
 static int dax_radix_entry(struct address_space *mapping, pgoff_t index,
 		sector_t sector, bool pmd_entry, bool dirty)
@@ -527,8 +527,8 @@ int dax_writeback_mapping_range(struct address_space *mapping,
 	if (!mapping->nrexceptional || wbc->sync_mode != WB_SYNC_ALL)
 		return 0;
 
-	start_index = wbc->range_start >> PAGE_CACHE_SHIFT;
-	end_index = wbc->range_end >> PAGE_CACHE_SHIFT;
+	start_index = wbc->range_start >> PAGE_SHIFT;
+	end_index = wbc->range_end >> PAGE_SHIFT;
 	pmd_index = DAX_PMD_INDEX(start_index);
 
 	rcu_read_lock();
@@ -661,12 +661,12 @@ int __dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
 	page = find_get_page(mapping, vmf->pgoff);
 	if (page) {
 		if (!lock_page_or_retry(page, vma->vm_mm, vmf->flags)) {
-			page_cache_release(page);
+			put_page(page);
 			return VM_FAULT_RETRY;
 		}
 		if (unlikely(page->mapping != mapping)) {
 			unlock_page(page);
-			page_cache_release(page);
+			put_page(page);
 			goto repeat;
 		}
 		size = (i_size_read(inode) + PAGE_SIZE - 1) >> PAGE_SHIFT;
@@ -730,10 +730,10 @@ int __dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
 
 	if (page) {
 		unmap_mapping_range(mapping, vmf->pgoff << PAGE_SHIFT,
-							PAGE_CACHE_SIZE, 0);
+							PAGE_SIZE, 0);
 		delete_from_page_cache(page);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		page = NULL;
 	}
 
@@ -766,7 +766,7 @@ int __dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
  unlock_page:
 	if (page) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 	goto out;
 }
@@ -1080,18 +1080,18 @@ int dax_zero_page_range(struct inode *inode, loff_t from, unsigned length,
 							get_block_t get_block)
 {
 	struct buffer_head bh;
-	pgoff_t index = from >> PAGE_CACHE_SHIFT;
-	unsigned offset = from & (PAGE_CACHE_SIZE-1);
+	pgoff_t index = from >> PAGE_SHIFT;
+	unsigned offset = from & (PAGE_SIZE-1);
 	int err;
 
 	/* Block boundary? Nothing to do */
 	if (!length)
 		return 0;
-	BUG_ON((offset + length) > PAGE_CACHE_SIZE);
+	BUG_ON((offset + length) > PAGE_SIZE);
 
 	memset(&bh, 0, sizeof(bh));
 	bh.b_bdev = inode->i_sb->s_bdev;
-	bh.b_size = PAGE_CACHE_SIZE;
+	bh.b_size = PAGE_SIZE;
 	err = get_block(inode, index, &bh, 0);
 	if (err < 0)
 		return err;
@@ -1099,7 +1099,7 @@ int dax_zero_page_range(struct inode *inode, loff_t from, unsigned length,
 		struct block_device *bdev = bh.b_bdev;
 		struct blk_dax_ctl dax = {
 			.sector = to_sector(&bh, inode),
-			.size = PAGE_CACHE_SIZE,
+			.size = PAGE_SIZE,
 		};
 
 		if (dax_map_atomic(bdev, &dax) < 0)
@@ -1129,7 +1129,7 @@ EXPORT_SYMBOL_GPL(dax_zero_page_range);
  */
 int dax_truncate_page(struct inode *inode, loff_t from, get_block_t get_block)
 {
-	unsigned length = PAGE_CACHE_ALIGN(from) - from;
+	unsigned length = PAGE_ALIGN(from) - from;
 	return dax_zero_page_range(inode, from, length, get_block);
 }
 EXPORT_SYMBOL_GPL(dax_truncate_page);
* Unmerged path fs/direct-io.c
diff --git a/fs/dlm/lowcomms.c b/fs/dlm/lowcomms.c
index b5e7293f2f80..69cfdfb301d8 100644
--- a/fs/dlm/lowcomms.c
+++ b/fs/dlm/lowcomms.c
@@ -636,7 +636,7 @@ static int receive_from_sock(struct connection *con)
 		con->rx_page = alloc_page(GFP_ATOMIC);
 		if (con->rx_page == NULL)
 			goto out_resched;
-		cbuf_init(&con->cb, PAGE_CACHE_SIZE);
+		cbuf_init(&con->cb, PAGE_SIZE);
 	}
 
 	/*
@@ -653,7 +653,7 @@ static int receive_from_sock(struct connection *con)
 	 * buffer and the start of the currently used section (cb.base)
 	 */
 	if (cbuf_data(&con->cb) >= con->cb.base) {
-		iov[0].iov_len = PAGE_CACHE_SIZE - cbuf_data(&con->cb);
+		iov[0].iov_len = PAGE_SIZE - cbuf_data(&con->cb);
 		iov[1].iov_len = con->cb.base;
 		iov[1].iov_base = page_address(con->rx_page);
 		nvec = 2;
@@ -671,7 +671,7 @@ static int receive_from_sock(struct connection *con)
 	ret = dlm_process_incoming_buffer(con->nodeid,
 					  page_address(con->rx_page),
 					  con->cb.base, con->cb.len,
-					  PAGE_CACHE_SIZE);
+					  PAGE_SIZE);
 	if (ret == -EBADMSG) {
 		log_print("lowcomms: addr=%p, base=%u, len=%u, read=%d",
 			  page_address(con->rx_page), con->cb.base,
@@ -1411,7 +1411,7 @@ void *dlm_lowcomms_get_buffer(int nodeid, int len, gfp_t allocation, char **ppc)
 	spin_lock(&con->writequeue_lock);
 	e = list_entry(con->writequeue.prev, struct writequeue_entry, list);
 	if ((&e->list == &con->writequeue) ||
-	    (PAGE_CACHE_SIZE - e->end < len)) {
+	    (PAGE_SIZE - e->end < len)) {
 		e = NULL;
 	} else {
 		offset = e->end;
* Unmerged path fs/ecryptfs/crypto.c
diff --git a/fs/ecryptfs/inode.c b/fs/ecryptfs/inode.c
index 1648908cf236..b7ba2a5145be 100644
--- a/fs/ecryptfs/inode.c
+++ b/fs/ecryptfs/inode.c
@@ -783,8 +783,8 @@ static int truncate_upper(struct dentry *dentry, struct iattr *ia,
 		 * in which ia->ia_size is located. Fill in the end of
 		 * that page from (ia->ia_size & ~PAGE_CACHE_MASK) to
 		 * PAGE_CACHE_SIZE with zeros. */
-		size_t num_zeros = (PAGE_CACHE_SIZE
-				    - (ia->ia_size & ~PAGE_CACHE_MASK));
+		size_t num_zeros = (PAGE_SIZE
+				    - (ia->ia_size & ~PAGE_MASK));
 
 		if (!(crypt_stat->flags & ECRYPTFS_ENCRYPTED)) {
 			truncate_setsize(inode, ia->ia_size);
diff --git a/fs/ecryptfs/keystore.c b/fs/ecryptfs/keystore.c
index 4725a07f003c..197a960f70a6 100644
--- a/fs/ecryptfs/keystore.c
+++ b/fs/ecryptfs/keystore.c
@@ -1780,7 +1780,7 @@ int ecryptfs_parse_packet_set(struct ecryptfs_crypt_stat *crypt_stat,
 	 * added the our &auth_tok_list */
 	next_packet_is_auth_tok_packet = 1;
 	while (next_packet_is_auth_tok_packet) {
-		size_t max_packet_size = ((PAGE_CACHE_SIZE - 8) - i);
+		size_t max_packet_size = ((PAGE_SIZE - 8) - i);
 
 		switch (src[i]) {
 		case ECRYPTFS_TAG_3_PACKET_TYPE:
diff --git a/fs/ecryptfs/main.c b/fs/ecryptfs/main.c
index 8b0957e900e5..f97304b69595 100644
--- a/fs/ecryptfs/main.c
+++ b/fs/ecryptfs/main.c
@@ -686,12 +686,12 @@ static struct ecryptfs_cache_info {
 	{
 		.cache = &ecryptfs_header_cache,
 		.name = "ecryptfs_headers",
-		.size = PAGE_CACHE_SIZE,
+		.size = PAGE_SIZE,
 	},
 	{
 		.cache = &ecryptfs_xattr_cache,
 		.name = "ecryptfs_xattr_cache",
-		.size = PAGE_CACHE_SIZE,
+		.size = PAGE_SIZE,
 	},
 	{
 		.cache = &ecryptfs_key_record_cache,
@@ -809,7 +809,7 @@ static int __init ecryptfs_init(void)
 {
 	int rc;
 
-	if (ECRYPTFS_DEFAULT_EXTENT_SIZE > PAGE_CACHE_SIZE) {
+	if (ECRYPTFS_DEFAULT_EXTENT_SIZE > PAGE_SIZE) {
 		rc = -EINVAL;
 		ecryptfs_printk(KERN_ERR, "The eCryptfs extent size is "
 				"larger than the host's page size, and so "
@@ -817,7 +817,7 @@ static int __init ecryptfs_init(void)
 				"default eCryptfs extent size is [%u] bytes; "
 				"the page size is [%lu] bytes.\n",
 				ECRYPTFS_DEFAULT_EXTENT_SIZE,
-				(unsigned long)PAGE_CACHE_SIZE);
+				(unsigned long)PAGE_SIZE);
 		goto out;
 	}
 	rc = ecryptfs_init_kmem_caches();
diff --git a/fs/ecryptfs/mmap.c b/fs/ecryptfs/mmap.c
index 564a1fa34b99..12aac6578adb 100644
--- a/fs/ecryptfs/mmap.c
+++ b/fs/ecryptfs/mmap.c
@@ -123,7 +123,7 @@ ecryptfs_copy_up_encrypted_with_header(struct page *page,
 				       struct ecryptfs_crypt_stat *crypt_stat)
 {
 	loff_t extent_num_in_page = 0;
-	loff_t num_extents_per_page = (PAGE_CACHE_SIZE
+	loff_t num_extents_per_page = (PAGE_SIZE
 				       / crypt_stat->extent_size);
 	int rc = 0;
 
@@ -139,7 +139,7 @@ ecryptfs_copy_up_encrypted_with_header(struct page *page,
 			char *page_virt;
 
 			page_virt = kmap_atomic(page);
-			memset(page_virt, 0, PAGE_CACHE_SIZE);
+			memset(page_virt, 0, PAGE_SIZE);
 			/* TODO: Support more than one header extent */
 			if (view_extent_num == 0) {
 				size_t written;
@@ -165,8 +165,8 @@ ecryptfs_copy_up_encrypted_with_header(struct page *page,
 				 - crypt_stat->metadata_size);
 
 			rc = ecryptfs_read_lower_page_segment(
-				page, (lower_offset >> PAGE_CACHE_SHIFT),
-				(lower_offset & ~PAGE_CACHE_MASK),
+				page, (lower_offset >> PAGE_SHIFT),
+				(lower_offset & ~PAGE_MASK),
 				crypt_stat->extent_size, page->mapping->host);
 			if (rc) {
 				printk(KERN_ERR "%s: Error attempting to read "
@@ -199,7 +199,7 @@ static int ecryptfs_readpage(struct file *file, struct page *page)
 
 	if (!crypt_stat || !(crypt_stat->flags & ECRYPTFS_ENCRYPTED)) {
 		rc = ecryptfs_read_lower_page_segment(page, page->index, 0,
-						      PAGE_CACHE_SIZE,
+						      PAGE_SIZE,
 						      page->mapping->host);
 	} else if (crypt_stat->flags & ECRYPTFS_VIEW_AS_ENCRYPTED) {
 		if (crypt_stat->flags & ECRYPTFS_METADATA_IN_XATTR) {
@@ -216,7 +216,7 @@ static int ecryptfs_readpage(struct file *file, struct page *page)
 
 		} else {
 			rc = ecryptfs_read_lower_page_segment(
-				page, page->index, 0, PAGE_CACHE_SIZE,
+				page, page->index, 0, PAGE_SIZE,
 				page->mapping->host);
 			if (rc) {
 				printk(KERN_ERR "Error reading page; rc = "
@@ -251,12 +251,12 @@ static int fill_zeros_to_end_of_page(struct page *page, unsigned int to)
 	struct inode *inode = page->mapping->host;
 	int end_byte_in_page;
 
-	if ((i_size_read(inode) / PAGE_CACHE_SIZE) != page->index)
+	if ((i_size_read(inode) / PAGE_SIZE) != page->index)
 		goto out;
-	end_byte_in_page = i_size_read(inode) % PAGE_CACHE_SIZE;
+	end_byte_in_page = i_size_read(inode) % PAGE_SIZE;
 	if (to > end_byte_in_page)
 		end_byte_in_page = to;
-	zero_user_segment(page, end_byte_in_page, PAGE_CACHE_SIZE);
+	zero_user_segment(page, end_byte_in_page, PAGE_SIZE);
 out:
 	return 0;
 }
@@ -280,7 +280,7 @@ static int ecryptfs_write_begin(struct file *file,
 			loff_t pos, unsigned len, unsigned flags,
 			struct page **pagep, void **fsdata)
 {
-	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
+	pgoff_t index = pos >> PAGE_SHIFT;
 	struct page *page;
 	loff_t prev_page_end_size;
 	int rc = 0;
@@ -290,14 +290,14 @@ static int ecryptfs_write_begin(struct file *file,
 		return -ENOMEM;
 	*pagep = page;
 
-	prev_page_end_size = ((loff_t)index << PAGE_CACHE_SHIFT);
+	prev_page_end_size = ((loff_t)index << PAGE_SHIFT);
 	if (!PageUptodate(page)) {
 		struct ecryptfs_crypt_stat *crypt_stat =
 			&ecryptfs_inode_to_private(mapping->host)->crypt_stat;
 
 		if (!(crypt_stat->flags & ECRYPTFS_ENCRYPTED)) {
 			rc = ecryptfs_read_lower_page_segment(
-				page, index, 0, PAGE_CACHE_SIZE, mapping->host);
+				page, index, 0, PAGE_SIZE, mapping->host);
 			if (rc) {
 				printk(KERN_ERR "%s: Error attemping to read "
 				       "lower page segment; rc = [%d]\n",
@@ -323,7 +323,7 @@ static int ecryptfs_write_begin(struct file *file,
 				SetPageUptodate(page);
 			} else {
 				rc = ecryptfs_read_lower_page_segment(
-					page, index, 0, PAGE_CACHE_SIZE,
+					page, index, 0, PAGE_SIZE,
 					mapping->host);
 				if (rc) {
 					printk(KERN_ERR "%s: Error reading "
@@ -337,9 +337,9 @@ static int ecryptfs_write_begin(struct file *file,
 		} else {
 			if (prev_page_end_size
 			    >= i_size_read(page->mapping->host)) {
-				zero_user(page, 0, PAGE_CACHE_SIZE);
+				zero_user(page, 0, PAGE_SIZE);
 				SetPageUptodate(page);
-			} else if (len < PAGE_CACHE_SIZE) {
+			} else if (len < PAGE_SIZE) {
 				rc = ecryptfs_decrypt_page(page);
 				if (rc) {
 					printk(KERN_ERR "%s: Error decrypting "
@@ -372,11 +372,11 @@ static int ecryptfs_write_begin(struct file *file,
 	 * of page?  Zero it out. */
 	if ((i_size_read(mapping->host) == prev_page_end_size)
 	    && (pos != 0))
-		zero_user(page, 0, PAGE_CACHE_SIZE);
+		zero_user(page, 0, PAGE_SIZE);
 out:
 	if (unlikely(rc)) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		*pagep = NULL;
 	}
 	return rc;
@@ -438,7 +438,7 @@ static int ecryptfs_write_inode_size_to_xattr(struct inode *ecryptfs_inode)
 	}
 	mutex_lock(&lower_inode->i_mutex);
 	size = lower_inode->i_op->getxattr(lower_dentry, ECRYPTFS_XATTR_NAME,
-					   xattr_virt, PAGE_CACHE_SIZE);
+					   xattr_virt, PAGE_SIZE);
 	if (size < 0)
 		size = 8;
 	put_unaligned_be64(i_size_read(ecryptfs_inode), xattr_virt);
@@ -480,8 +480,8 @@ static int ecryptfs_write_end(struct file *file,
 			loff_t pos, unsigned len, unsigned copied,
 			struct page *page, void *fsdata)
 {
-	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
-	unsigned from = pos & (PAGE_CACHE_SIZE - 1);
+	pgoff_t index = pos >> PAGE_SHIFT;
+	unsigned from = pos & (PAGE_SIZE - 1);
 	unsigned to = from + copied;
 	struct inode *ecryptfs_inode = mapping->host;
 	struct ecryptfs_crypt_stat *crypt_stat =
@@ -501,7 +501,7 @@ static int ecryptfs_write_end(struct file *file,
 		goto out;
 	}
 	if (!PageUptodate(page)) {
-		if (copied < PAGE_CACHE_SIZE) {
+		if (copied < PAGE_SIZE) {
 			rc = 0;
 			goto out;
 		}
@@ -534,7 +534,7 @@ static int ecryptfs_write_end(struct file *file,
 		rc = copied;
 out:
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return rc;
 }
 
diff --git a/fs/ecryptfs/read_write.c b/fs/ecryptfs/read_write.c
index 09fe622274e4..158a3a39f82d 100644
--- a/fs/ecryptfs/read_write.c
+++ b/fs/ecryptfs/read_write.c
@@ -74,7 +74,7 @@ int ecryptfs_write_lower_page_segment(struct inode *ecryptfs_inode,
 	loff_t offset;
 	int rc;
 
-	offset = ((((loff_t)page_for_lower->index) << PAGE_CACHE_SHIFT)
+	offset = ((((loff_t)page_for_lower->index) << PAGE_SHIFT)
 		  + offset_in_page);
 	virt = kmap(page_for_lower);
 	rc = ecryptfs_write_lower(ecryptfs_inode, virt, offset, size);
@@ -123,9 +123,9 @@ int ecryptfs_write(struct inode *ecryptfs_inode, char *data, loff_t offset,
 	else
 		pos = offset;
 	while (pos < (offset + size)) {
-		pgoff_t ecryptfs_page_idx = (pos >> PAGE_CACHE_SHIFT);
-		size_t start_offset_in_page = (pos & ~PAGE_CACHE_MASK);
-		size_t num_bytes = (PAGE_CACHE_SIZE - start_offset_in_page);
+		pgoff_t ecryptfs_page_idx = (pos >> PAGE_SHIFT);
+		size_t start_offset_in_page = (pos & ~PAGE_MASK);
+		size_t num_bytes = (PAGE_SIZE - start_offset_in_page);
 		loff_t total_remaining_bytes = ((offset + size) - pos);
 
 		if (fatal_signal_pending(current)) {
@@ -165,7 +165,7 @@ int ecryptfs_write(struct inode *ecryptfs_inode, char *data, loff_t offset,
 			 * Fill in zero values to the end of the page */
 			memset(((char *)ecryptfs_page_virt
 				+ start_offset_in_page), 0,
-				PAGE_CACHE_SIZE - start_offset_in_page);
+				PAGE_SIZE - start_offset_in_page);
 		}
 
 		/* pos >= offset, we are now writing the data request */
@@ -186,7 +186,7 @@ int ecryptfs_write(struct inode *ecryptfs_inode, char *data, loff_t offset,
 						ecryptfs_page,
 						start_offset_in_page,
 						data_offset);
-		page_cache_release(ecryptfs_page);
+		put_page(ecryptfs_page);
 		if (rc) {
 			printk(KERN_ERR "%s: Error encrypting "
 			       "page; rc = [%d]\n", __func__, rc);
@@ -262,7 +262,7 @@ int ecryptfs_read_lower_page_segment(struct page *page_for_ecryptfs,
 	loff_t offset;
 	int rc;
 
-	offset = ((((loff_t)page_index) << PAGE_CACHE_SHIFT) + offset_in_page);
+	offset = ((((loff_t)page_index) << PAGE_SHIFT) + offset_in_page);
 	virt = kmap(page_for_ecryptfs);
 	rc = ecryptfs_read_lower(virt, offset, size, ecryptfs_inode);
 	if (rc > 0)
diff --git a/fs/efivarfs/super.c b/fs/efivarfs/super.c
index a8766b880c07..0afbb492092c 100644
--- a/fs/efivarfs/super.c
+++ b/fs/efivarfs/super.c
@@ -202,8 +202,8 @@ static int efivarfs_fill_super(struct super_block *sb, void *data, int silent)
 	efivarfs_sb = sb;
 
 	sb->s_maxbytes          = MAX_LFS_FILESIZE;
-	sb->s_blocksize         = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits    = PAGE_CACHE_SHIFT;
+	sb->s_blocksize         = PAGE_SIZE;
+	sb->s_blocksize_bits    = PAGE_SHIFT;
 	sb->s_magic             = EFIVARFS_MAGIC;
 	sb->s_op                = &efivarfs_ops;
 	sb->s_d_op		= &efivarfs_d_ops;
* Unmerged path fs/exofs/dir.c
* Unmerged path fs/exofs/inode.c
diff --git a/fs/exofs/namei.c b/fs/exofs/namei.c
index 4731fd991efe..85db1a359b78 100644
--- a/fs/exofs/namei.c
+++ b/fs/exofs/namei.c
@@ -293,11 +293,11 @@ static int exofs_rename(struct inode *old_dir, struct dentry *old_dentry,
 out_dir:
 	if (dir_de) {
 		kunmap(dir_page);
-		page_cache_release(dir_page);
+		put_page(dir_page);
 	}
 out_old:
 	kunmap(old_page);
-	page_cache_release(old_page);
+	put_page(old_page);
 out:
 	return err;
 }
* Unmerged path fs/ext2/dir.c
diff --git a/fs/ext2/namei.c b/fs/ext2/namei.c
index b4dfbbe2f211..68b02bca316e 100644
--- a/fs/ext2/namei.c
+++ b/fs/ext2/namei.c
@@ -357,7 +357,7 @@ static int ext2_rename (struct inode * old_dir, struct dentry * old_dentry,
 			ext2_set_link(old_inode, dir_de, dir_page, new_dir, 0);
 		else {
 			kunmap(dir_page);
-			page_cache_release(dir_page);
+			put_page(dir_page);
 		}
 		inode_dec_link_count(old_dir);
 	}
@@ -367,11 +367,11 @@ static int ext2_rename (struct inode * old_dir, struct dentry * old_dentry,
 out_dir:
 	if (dir_de) {
 		kunmap(dir_page);
-		page_cache_release(dir_page);
+		put_page(dir_page);
 	}
 out_old:
 	kunmap(old_page);
-	page_cache_release(old_page);
+	put_page(old_page);
 out:
 	return err;
 }
* Unmerged path fs/ext4/crypto.c
* Unmerged path fs/ext4/dir.c
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index de8914c0fbc1..0183175daaba 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -419,8 +419,8 @@ static int ext4_find_unwritten_pgoff(struct inode *inode,
 	lastoff = startoff;
 	endoff = (loff_t)(map->m_lblk + map->m_len) << blkbits;
 
-	index = startoff >> PAGE_CACHE_SHIFT;
-	end = endoff >> PAGE_CACHE_SHIFT;
+	index = startoff >> PAGE_SHIFT;
+	end = endoff >> PAGE_SHIFT;
 
 	pagevec_init(&pvec, 0);
 	do {
diff --git a/fs/ext4/inline.c b/fs/ext4/inline.c
index 49abc3721ea1..13be8d52d1cf 100644
--- a/fs/ext4/inline.c
+++ b/fs/ext4/inline.c
@@ -483,7 +483,7 @@ static int ext4_read_inline_page(struct inode *inode, struct page *page)
 	ret = ext4_read_inline_data(inode, kaddr, len, &iloc);
 	flush_dcache_page(page);
 	kunmap_atomic(kaddr);
-	zero_user_segment(page, len, PAGE_CACHE_SIZE);
+	zero_user_segment(page, len, PAGE_SIZE);
 	SetPageUptodate(page);
 	brelse(iloc.bh);
 
@@ -508,7 +508,7 @@ int ext4_readpage_inline(struct inode *inode, struct page *page)
 	if (!page->index)
 		ret = ext4_read_inline_page(inode, page);
 	else if (!PageUptodate(page)) {
-		zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+		zero_user_segment(page, 0, PAGE_SIZE);
 		SetPageUptodate(page);
 	}
 
@@ -595,7 +595,7 @@ retry:
 
 	if (ret) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		page = NULL;
 		ext4_orphan_add(handle, inode);
 		up_write(&EXT4_I(inode)->xattr_sem);
@@ -621,7 +621,7 @@ retry:
 out:
 	if (page) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 	if (sem_held)
 		up_write(&EXT4_I(inode)->xattr_sem);
@@ -690,7 +690,7 @@ int ext4_try_to_write_inline_data(struct address_space *mapping,
 	if (!ext4_has_inline_data(inode)) {
 		ret = 0;
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		goto out_up_read;
 	}
 
@@ -815,7 +815,7 @@ static int ext4_da_convert_inline_data_to_extent(struct address_space *mapping,
 	if (ret) {
 		up_read(&EXT4_I(inode)->xattr_sem);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		ext4_truncate_failed_write(inode);
 		return ret;
 	}
@@ -829,7 +829,7 @@ out:
 	up_read(&EXT4_I(inode)->xattr_sem);
 	if (page) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 	return ret;
 }
@@ -919,7 +919,7 @@ retry_journal:
 out_release_page:
 	up_read(&EXT4_I(inode)->xattr_sem);
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 out_journal:
 	ext4_journal_stop(handle);
 out:
@@ -947,7 +947,7 @@ int ext4_da_write_inline_data_end(struct inode *inode, loff_t pos,
 		i_size_changed = 1;
 	}
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
 	/*
 	 * Don't mark the inode dirty under page lock. First, it unnecessarily
* Unmerged path fs/ext4/inode.c
* Unmerged path fs/ext4/mballoc.c
* Unmerged path fs/ext4/move_extent.c
diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index c7091cab5714..160e6bf05359 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -429,8 +429,8 @@ int ext4_bio_write_page(struct ext4_io_submit *io,
 	 * the page size, the remaining memory is zeroed when mapped, and
 	 * writes to that region are not written out to the file."
 	 */
-	if (len < PAGE_CACHE_SIZE)
-		zero_user_segment(page, len, PAGE_CACHE_SIZE);
+	if (len < PAGE_SIZE)
+		zero_user_segment(page, len, PAGE_SIZE);
 	/*
 	 * In the first loop we prepare and mark buffers to submit. We have to
 	 * mark all buffers in the page before submitting so that
* Unmerged path fs/ext4/readpage.c
* Unmerged path fs/ext4/super.c
* Unmerged path fs/ext4/symlink.c
* Unmerged path fs/f2fs/data.c
* Unmerged path fs/f2fs/debug.c
diff --git a/fs/f2fs/dir.c b/fs/f2fs/dir.c
index 1ac6b93036b7..314ae54fd3b9 100644
--- a/fs/f2fs/dir.c
+++ b/fs/f2fs/dir.c
@@ -16,8 +16,8 @@
 
 static unsigned long dir_blocks(struct inode *inode)
 {
-	return ((unsigned long long) (i_size_read(inode) + PAGE_CACHE_SIZE - 1))
-							>> PAGE_CACHE_SHIFT;
+	return ((unsigned long long) (i_size_read(inode) + PAGE_SIZE - 1))
+							>> PAGE_SHIFT;
 }
 
 static unsigned int dir_buckets(unsigned int level)
diff --git a/fs/f2fs/f2fs.h b/fs/f2fs/f2fs.h
index 20aab02f2a42..0cf9603e0c11 100644
--- a/fs/f2fs/f2fs.h
+++ b/fs/f2fs/f2fs.h
@@ -765,7 +765,7 @@ static inline void f2fs_put_page(struct page *page, int unlock)
 		BUG_ON(!PageLocked(page));
 		unlock_page(page);
 	}
-	page_cache_release(page);
+	put_page(page);
 }
 
 static inline void f2fs_put_dnode(struct dnode_of_data *dn)
* Unmerged path fs/f2fs/file.c
* Unmerged path fs/f2fs/inline.c
* Unmerged path fs/f2fs/namei.c
* Unmerged path fs/f2fs/node.c
* Unmerged path fs/f2fs/recovery.c
* Unmerged path fs/f2fs/segment.c
diff --git a/fs/f2fs/super.c b/fs/f2fs/super.c
index 8555f7df82c7..648a5eda27e8 100644
--- a/fs/f2fs/super.c
+++ b/fs/f2fs/super.c
@@ -409,10 +409,10 @@ static int sanity_check_raw_super(struct super_block *sb,
 	}
 
 	/* Currently, support only 4KB page cache size */
-	if (F2FS_BLKSIZE != PAGE_CACHE_SIZE) {
+	if (F2FS_BLKSIZE != PAGE_SIZE) {
 		f2fs_msg(sb, KERN_INFO,
 			"Invalid page_cache_size (%lu), supports only 4KB\n",
-			PAGE_CACHE_SIZE);
+			PAGE_SIZE);
 		return 1;
 	}
 
diff --git a/fs/freevxfs/vxfs_immed.c b/fs/freevxfs/vxfs_immed.c
index c36aeaf92e41..95118a9132a6 100644
--- a/fs/freevxfs/vxfs_immed.c
+++ b/fs/freevxfs/vxfs_immed.c
@@ -100,11 +100,11 @@ static int
 vxfs_immed_readpage(struct file *fp, struct page *pp)
 {
 	struct vxfs_inode_info	*vip = VXFS_INO(pp->mapping->host);
-	u_int64_t	offset = (u_int64_t)pp->index << PAGE_CACHE_SHIFT;
+	u_int64_t	offset = (u_int64_t)pp->index << PAGE_SHIFT;
 	caddr_t		kaddr;
 
 	kaddr = kmap(pp);
-	memcpy(kaddr, vip->vii_immed.vi_immed + offset, PAGE_CACHE_SIZE);
+	memcpy(kaddr, vip->vii_immed.vi_immed + offset, PAGE_SIZE);
 	kunmap(pp);
 	
 	flush_dcache_page(pp);
* Unmerged path fs/freevxfs/vxfs_lookup.c
diff --git a/fs/freevxfs/vxfs_subr.c b/fs/freevxfs/vxfs_subr.c
index 5d318c44f855..e806694d4145 100644
--- a/fs/freevxfs/vxfs_subr.c
+++ b/fs/freevxfs/vxfs_subr.c
@@ -50,7 +50,7 @@ inline void
 vxfs_put_page(struct page *pp)
 {
 	kunmap(pp);
-	page_cache_release(pp);
+	put_page(pp);
 }
 
 /**
diff --git a/fs/fs-writeback.c b/fs/fs-writeback.c
index 278c1c2fba0d..e412e23b47ae 100644
--- a/fs/fs-writeback.c
+++ b/fs/fs-writeback.c
@@ -31,7 +31,7 @@
 /*
  * 4MB minimal write chunk size
  */
-#define MIN_WRITEBACK_PAGES	(4096UL >> (PAGE_CACHE_SHIFT - 10))
+#define MIN_WRITEBACK_PAGES	(4096UL >> (PAGE_SHIFT - 10))
 
 /*
  * Passed into wb_writeback(), essentially a subset of writeback_control
diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index ca916af5a7c4..e18ea9a98241 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -113,7 +113,7 @@ try_again:
 
 	wake_up_bit(&cookie->flags, 0);
 	if (xpage)
-		page_cache_release(xpage);
+		put_page(xpage);
 	__fscache_uncache_page(cookie, page);
 	return true;
 
@@ -164,7 +164,7 @@ static void fscache_end_page_write(struct fscache_object *object,
 	}
 	spin_unlock(&object->lock);
 	if (xpage)
-		page_cache_release(xpage);
+		put_page(xpage);
 }
 
 /*
@@ -884,7 +884,7 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
 		spin_unlock(&cookie->stores_lock);
 
 		for (i = n - 1; i >= 0; i--)
-			page_cache_release(results[i]);
+			put_page(results[i]);
 	}
 
 	_leave("");
@@ -982,7 +982,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 
 	radix_tree_tag_set(&cookie->stores, page->index,
 			   FSCACHE_COOKIE_PENDING_TAG);
-	page_cache_get(page);
+	get_page(page);
 
 	/* we only want one writer at a time, but we do need to queue new
 	 * writers after exclusive ops */
@@ -1026,7 +1026,7 @@ submit_failed:
 	radix_tree_delete(&cookie->stores, page->index);
 	spin_unlock(&cookie->stores_lock);
 	wake_cookie = __fscache_unuse_cookie(cookie);
-	page_cache_release(page);
+	put_page(page);
 	ret = -ENOBUFS;
 	goto nobufs;
 
* Unmerged path fs/fuse/dev.c
* Unmerged path fs/fuse/file.c
diff --git a/fs/fuse/inode.c b/fs/fuse/inode.c
index 378ca42a7b8d..0a01a77600ea 100644
--- a/fs/fuse/inode.c
+++ b/fs/fuse/inode.c
@@ -323,11 +323,11 @@ int fuse_reverse_inval_inode(struct super_block *sb, u64 nodeid,
 
 	fuse_invalidate_attr(inode);
 	if (offset >= 0) {
-		pg_start = offset >> PAGE_CACHE_SHIFT;
+		pg_start = offset >> PAGE_SHIFT;
 		if (len <= 0)
 			pg_end = -1;
 		else
-			pg_end = (offset + len - 1) >> PAGE_CACHE_SHIFT;
+			pg_end = (offset + len - 1) >> PAGE_SHIFT;
 		invalidate_inode_pages2_range(inode->i_mapping,
 					      pg_start, pg_end);
 	}
@@ -843,7 +843,7 @@ static void process_init_reply(struct fuse_conn *fc, struct fuse_req *req)
 		process_init_limits(fc, arg);
 
 		if (arg->minor >= 6) {
-			ra_pages = arg->max_readahead / PAGE_CACHE_SIZE;
+			ra_pages = arg->max_readahead / PAGE_SIZE;
 			if (arg->flags & FUSE_ASYNC_READ)
 				fc->async_read = 1;
 			if (!(arg->flags & FUSE_POSIX_LOCKS))
@@ -876,7 +876,7 @@ static void process_init_reply(struct fuse_conn *fc, struct fuse_req *req)
 			if (arg->flags & FUSE_ASYNC_DIO)
 				fc->async_dio = 1;
 		} else {
-			ra_pages = fc->max_read / PAGE_CACHE_SIZE;
+			ra_pages = fc->max_read / PAGE_SIZE;
 			fc->no_lock = 1;
 			fc->no_flock = 1;
 		}
@@ -897,7 +897,7 @@ static void fuse_send_init(struct fuse_conn *fc, struct fuse_req *req)
 
 	arg->major = FUSE_KERNEL_VERSION;
 	arg->minor = FUSE_KERNEL_MINOR_VERSION;
-	arg->max_readahead = fc->bdi.ra_pages * PAGE_CACHE_SIZE;
+	arg->max_readahead = fc->bdi.ra_pages * PAGE_SIZE;
 	arg->flags |= FUSE_ASYNC_READ | FUSE_POSIX_LOCKS | FUSE_ATOMIC_O_TRUNC |
 		FUSE_EXPORT_SUPPORT | FUSE_BIG_WRITES | FUSE_DONT_MASK |
 		FUSE_SPLICE_WRITE | FUSE_SPLICE_MOVE | FUSE_SPLICE_READ |
@@ -928,7 +928,7 @@ static int fuse_bdi_init(struct fuse_conn *fc, struct super_block *sb)
 	int err;
 
 	fc->bdi.name = "fuse";
-	fc->bdi.ra_pages = (VM_MAX_READAHEAD * 1024) / PAGE_CACHE_SIZE;
+	fc->bdi.ra_pages = (VM_MAX_READAHEAD * 1024) / PAGE_SIZE;
 	/* fuse does it's own writeback accounting */
 	fc->bdi.capabilities = BDI_CAP_NO_ACCT_WB | BDI_CAP_STRICTLIMIT;
 
@@ -992,8 +992,8 @@ static int fuse_fill_super(struct super_block *sb, void *data, int silent)
 			goto err;
 #endif
 	} else {
-		sb->s_blocksize = PAGE_CACHE_SIZE;
-		sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+		sb->s_blocksize = PAGE_SIZE;
+		sb->s_blocksize_bits = PAGE_SHIFT;
 	}
 	sb->s_magic = FUSE_SUPER_MAGIC;
 	sb->s_op = &fuse_super_operations;
* Unmerged path fs/gfs2/aops.c
diff --git a/fs/gfs2/bmap.c b/fs/gfs2/bmap.c
index ad1971d69c1a..0b4625bb2798 100644
--- a/fs/gfs2/bmap.c
+++ b/fs/gfs2/bmap.c
@@ -75,7 +75,7 @@ static int gfs2_unstuffer_page(struct gfs2_inode *ip, struct buffer_head *dibh,
 			dsize = dibh->b_size - sizeof(struct gfs2_dinode);
 
 		memcpy(kaddr, dibh->b_data + sizeof(struct gfs2_dinode), dsize);
-		memset(kaddr + dsize, 0, PAGE_CACHE_SIZE - dsize);
+		memset(kaddr + dsize, 0, PAGE_SIZE - dsize);
 		kunmap(page);
 
 		SetPageUptodate(page);
@@ -98,7 +98,7 @@ static int gfs2_unstuffer_page(struct gfs2_inode *ip, struct buffer_head *dibh,
 
 	if (release) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 
 	return 0;
@@ -955,8 +955,8 @@ static int gfs2_block_truncate_page(struct address_space *mapping, loff_t from)
 {
 	struct inode *inode = mapping->host;
 	struct gfs2_inode *ip = GFS2_I(inode);
-	unsigned long index = from >> PAGE_CACHE_SHIFT;
-	unsigned offset = from & (PAGE_CACHE_SIZE-1);
+	unsigned long index = from >> PAGE_SHIFT;
+	unsigned offset = from & (PAGE_SIZE-1);
 	unsigned blocksize, iblock, length, pos;
 	struct buffer_head *bh;
 	struct page *page;
@@ -968,7 +968,7 @@ static int gfs2_block_truncate_page(struct address_space *mapping, loff_t from)
 
 	blocksize = inode->i_sb->s_blocksize;
 	length = blocksize - (offset & (blocksize - 1));
-	iblock = index << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+	iblock = index << (PAGE_SHIFT - inode->i_sb->s_blocksize_bits);
 
 	if (!page_has_buffers(page))
 		create_empty_buffers(page, blocksize, 0);
@@ -1012,7 +1012,7 @@ static int gfs2_block_truncate_page(struct address_space *mapping, loff_t from)
 	mark_buffer_dirty(bh);
 unlock:
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return err;
 }
 
diff --git a/fs/gfs2/file.c b/fs/gfs2/file.c
index af0d418da809..8d46aef4dfb2 100644
--- a/fs/gfs2/file.c
+++ b/fs/gfs2/file.c
@@ -362,8 +362,8 @@ static int gfs2_allocate_page_backing(struct page *page)
 {
 	struct inode *inode = page->mapping->host;
 	struct buffer_head bh;
-	unsigned long size = PAGE_CACHE_SIZE;
-	u64 lblock = page->index << (PAGE_CACHE_SHIFT - inode->i_blkbits);
+	unsigned long size = PAGE_SIZE;
+	u64 lblock = page->index << (PAGE_SHIFT - inode->i_blkbits);
 
 	do {
 		bh.b_state = 0;
@@ -394,7 +394,7 @@ static int gfs2_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
 	struct gfs2_sbd *sdp = GFS2_SB(inode);
 	struct gfs2_alloc_parms ap = { .aflags = 0, };
 	unsigned long last_index;
-	u64 pos = page->index << PAGE_CACHE_SHIFT;
+	u64 pos = page->index << PAGE_SHIFT;
 	unsigned int data_blocks, ind_blocks, rblocks;
 	struct gfs2_holder gh;
 	loff_t size;
@@ -406,7 +406,7 @@ static int gfs2_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
 	if (ret)
 		goto out;
 
-	gfs2_size_hint(vma->vm_file, pos, PAGE_CACHE_SIZE);
+	gfs2_size_hint(vma->vm_file, pos, PAGE_SIZE);
 
 	gfs2_holder_init(ip->i_gl, LM_ST_EXCLUSIVE, 0, &gh);
 	ret = gfs2_glock_nq(&gh);
@@ -419,7 +419,7 @@ static int gfs2_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
 	set_bit(GLF_DIRTY, &ip->i_gl->gl_flags);
 	set_bit(GIF_SW_PAGED, &ip->i_flags);
 
-	if (!gfs2_write_alloc_required(ip, pos, PAGE_CACHE_SIZE)) {
+	if (!gfs2_write_alloc_required(ip, pos, PAGE_SIZE)) {
 		lock_page(page);
 		if (!PageUptodate(page) || page->mapping != inode->i_mapping) {
 			ret = -EAGAIN;
@@ -432,7 +432,7 @@ static int gfs2_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
 	if (ret)
 		goto out_unlock;
 
-	gfs2_write_calc_reserv(ip, PAGE_CACHE_SIZE, &data_blocks, &ind_blocks);
+	gfs2_write_calc_reserv(ip, PAGE_SIZE, &data_blocks, &ind_blocks);
 	ap.target = data_blocks + ind_blocks;
 	ret = gfs2_quota_lock_check(ip, &ap);
 	if (ret)
@@ -455,7 +455,7 @@ static int gfs2_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
 	lock_page(page);
 	ret = -EINVAL;
 	size = i_size_read(inode);
-	last_index = (size - 1) >> PAGE_CACHE_SHIFT;
+	last_index = (size - 1) >> PAGE_SHIFT;
 	/* Check page index against inode size */
 	if (size == 0 || (page->index > last_index))
 		goto out_trans_end;
@@ -883,7 +883,7 @@ static long __gfs2_fallocate(struct file *file, int mode, loff_t offset, loff_t
 			rblocks += data_blocks ? data_blocks : 1;
 
 		error = gfs2_trans_begin(sdp, rblocks,
-					 PAGE_CACHE_SIZE/sdp->sd_sb.sb_bsize);
+					 PAGE_SIZE/sdp->sd_sb.sb_bsize);
 		if (error)
 			goto out_trans_fail;
 
* Unmerged path fs/gfs2/meta_io.c
diff --git a/fs/gfs2/quota.c b/fs/gfs2/quota.c
index e3f54b09777d..aee899ccf4f3 100644
--- a/fs/gfs2/quota.c
+++ b/fs/gfs2/quota.c
@@ -689,7 +689,7 @@ static int gfs2_write_buf_to_page(struct gfs2_inode *ip, unsigned long index,
 	unsigned to_write = bytes, pg_off = off;
 	int done = 0;
 
-	blk = index << (PAGE_CACHE_SHIFT - sdp->sd_sb.sb_bsize_shift);
+	blk = index << (PAGE_SHIFT - sdp->sd_sb.sb_bsize_shift);
 	boff = off % bsize;
 
 	page = find_or_create_page(mapping, index, GFP_NOFS);
@@ -741,13 +741,13 @@ static int gfs2_write_buf_to_page(struct gfs2_inode *ip, unsigned long index,
 	flush_dcache_page(page);
 	kunmap_atomic(kaddr);
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
 	return 0;
 
 unlock_out:
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return -EIO;
 }
 
@@ -761,13 +761,13 @@ static int gfs2_write_disk_quota(struct gfs2_inode *ip, struct gfs2_quota *qp,
 
 	nbytes = sizeof(struct gfs2_quota);
 
-	pg_beg = loc >> PAGE_CACHE_SHIFT;
-	pg_off = loc % PAGE_CACHE_SIZE;
+	pg_beg = loc >> PAGE_SHIFT;
+	pg_off = loc % PAGE_SIZE;
 
 	/* If the quota straddles a page boundary, split the write in two */
-	if ((pg_off + nbytes) > PAGE_CACHE_SIZE) {
+	if ((pg_off + nbytes) > PAGE_SIZE) {
 		pg_oflow = 1;
-		overflow = (pg_off + nbytes) - PAGE_CACHE_SIZE;
+		overflow = (pg_off + nbytes) - PAGE_SIZE;
 	}
 
 	ptr = qp;
* Unmerged path fs/gfs2/rgrp.c
* Unmerged path fs/hfs/bnode.c
diff --git a/fs/hfs/btree.c b/fs/hfs/btree.c
index 1ab19e660e69..37cdd955eceb 100644
--- a/fs/hfs/btree.c
+++ b/fs/hfs/btree.c
@@ -116,14 +116,14 @@ struct hfs_btree *hfs_btree_open(struct super_block *sb, u32 id, btree_keycmp ke
 	}
 
 	tree->node_size_shift = ffs(size) - 1;
-	tree->pages_per_bnode = (tree->node_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+	tree->pages_per_bnode = (tree->node_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
 
 	kunmap(page);
-	page_cache_release(page);
+	put_page(page);
 	return tree;
 
 fail_page:
-	page_cache_release(page);
+	put_page(page);
 free_inode:
 	tree->inode->i_mapping->a_ops = &hfs_aops;
 	iput(tree->inode);
@@ -257,9 +257,9 @@ struct hfs_bnode *hfs_bmap_alloc(struct hfs_btree *tree)
 	off = off16;
 
 	off += node->page_offset;
-	pagep = node->page + (off >> PAGE_CACHE_SHIFT);
+	pagep = node->page + (off >> PAGE_SHIFT);
 	data = kmap(*pagep);
-	off &= ~PAGE_CACHE_MASK;
+	off &= ~PAGE_MASK;
 	idx = 0;
 
 	for (;;) {
@@ -279,7 +279,7 @@ struct hfs_bnode *hfs_bmap_alloc(struct hfs_btree *tree)
 					}
 				}
 			}
-			if (++off >= PAGE_CACHE_SIZE) {
+			if (++off >= PAGE_SIZE) {
 				kunmap(*pagep);
 				data = kmap(*++pagep);
 				off = 0;
@@ -302,9 +302,9 @@ struct hfs_bnode *hfs_bmap_alloc(struct hfs_btree *tree)
 		len = hfs_brec_lenoff(node, 0, &off16);
 		off = off16;
 		off += node->page_offset;
-		pagep = node->page + (off >> PAGE_CACHE_SHIFT);
+		pagep = node->page + (off >> PAGE_SHIFT);
 		data = kmap(*pagep);
-		off &= ~PAGE_CACHE_MASK;
+		off &= ~PAGE_MASK;
 	}
 }
 
@@ -348,9 +348,9 @@ void hfs_bmap_free(struct hfs_bnode *node)
 		len = hfs_brec_lenoff(node, 0, &off);
 	}
 	off += node->page_offset + nidx / 8;
-	page = node->page[off >> PAGE_CACHE_SHIFT];
+	page = node->page[off >> PAGE_SHIFT];
 	data = kmap(page);
-	off &= ~PAGE_CACHE_MASK;
+	off &= ~PAGE_MASK;
 	m = 1 << (~nidx & 7);
 	byte = data[off];
 	if (!(byte & m)) {
diff --git a/fs/hfs/inode.c b/fs/hfs/inode.c
index 9e2fecd62f62..30317f107256 100644
--- a/fs/hfs/inode.c
+++ b/fs/hfs/inode.c
@@ -91,8 +91,8 @@ static int hfs_releasepage(struct page *page, gfp_t mask)
 	if (!tree)
 		return 0;
 
-	if (tree->node_size >= PAGE_CACHE_SIZE) {
-		nidx = page->index >> (tree->node_size_shift - PAGE_CACHE_SHIFT);
+	if (tree->node_size >= PAGE_SIZE) {
+		nidx = page->index >> (tree->node_size_shift - PAGE_SHIFT);
 		spin_lock(&tree->hash_lock);
 		node = hfs_bnode_findhash(tree, nidx);
 		if (!node)
@@ -105,8 +105,8 @@ static int hfs_releasepage(struct page *page, gfp_t mask)
 		}
 		spin_unlock(&tree->hash_lock);
 	} else {
-		nidx = page->index << (PAGE_CACHE_SHIFT - tree->node_size_shift);
-		i = 1 << (PAGE_CACHE_SHIFT - tree->node_size_shift);
+		nidx = page->index << (PAGE_SHIFT - tree->node_size_shift);
+		i = 1 << (PAGE_SHIFT - tree->node_size_shift);
 		spin_lock(&tree->hash_lock);
 		do {
 			node = hfs_bnode_findhash(tree, nidx++);
diff --git a/fs/hfsplus/bitmap.c b/fs/hfsplus/bitmap.c
index d2954451519e..c0ae274c0a22 100644
--- a/fs/hfsplus/bitmap.c
+++ b/fs/hfsplus/bitmap.c
@@ -13,7 +13,7 @@
 #include "hfsplus_fs.h"
 #include "hfsplus_raw.h"
 
-#define PAGE_CACHE_BITS	(PAGE_CACHE_SIZE * 8)
+#define PAGE_CACHE_BITS	(PAGE_SIZE * 8)
 
 int hfsplus_block_allocate(struct super_block *sb, u32 size,
 		u32 offset, u32 *max)
* Unmerged path fs/hfsplus/bnode.c
diff --git a/fs/hfsplus/btree.c b/fs/hfsplus/btree.c
index 0c6540c91167..a6c5ce88f354 100644
--- a/fs/hfsplus/btree.c
+++ b/fs/hfsplus/btree.c
@@ -124,15 +124,15 @@ struct hfs_btree *hfs_btree_open(struct super_block *sb, u32 id)
 	tree->node_size_shift = ffs(size) - 1;
 
 	tree->pages_per_bnode =
-		(tree->node_size + PAGE_CACHE_SIZE - 1) >>
-		PAGE_CACHE_SHIFT;
+		(tree->node_size + PAGE_SIZE - 1) >>
+		PAGE_SHIFT;
 
 	kunmap(page);
-	page_cache_release(page);
+	put_page(page);
 	return tree;
 
  fail_page:
-	page_cache_release(page);
+	put_page(page);
  free_inode:
 	tree->inode->i_mapping->a_ops = &hfsplus_aops;
 	iput(tree->inode);
@@ -268,9 +268,9 @@ struct hfs_bnode *hfs_bmap_alloc(struct hfs_btree *tree)
 	off = off16;
 
 	off += node->page_offset;
-	pagep = node->page + (off >> PAGE_CACHE_SHIFT);
+	pagep = node->page + (off >> PAGE_SHIFT);
 	data = kmap(*pagep);
-	off &= ~PAGE_CACHE_MASK;
+	off &= ~PAGE_MASK;
 	idx = 0;
 
 	for (;;) {
@@ -291,7 +291,7 @@ struct hfs_bnode *hfs_bmap_alloc(struct hfs_btree *tree)
 					}
 				}
 			}
-			if (++off >= PAGE_CACHE_SIZE) {
+			if (++off >= PAGE_SIZE) {
 				kunmap(*pagep);
 				data = kmap(*++pagep);
 				off = 0;
@@ -314,9 +314,9 @@ struct hfs_bnode *hfs_bmap_alloc(struct hfs_btree *tree)
 		len = hfs_brec_lenoff(node, 0, &off16);
 		off = off16;
 		off += node->page_offset;
-		pagep = node->page + (off >> PAGE_CACHE_SHIFT);
+		pagep = node->page + (off >> PAGE_SHIFT);
 		data = kmap(*pagep);
-		off &= ~PAGE_CACHE_MASK;
+		off &= ~PAGE_MASK;
 	}
 }
 
@@ -363,9 +363,9 @@ void hfs_bmap_free(struct hfs_bnode *node)
 		len = hfs_brec_lenoff(node, 0, &off);
 	}
 	off += node->page_offset + nidx / 8;
-	page = node->page[off >> PAGE_CACHE_SHIFT];
+	page = node->page[off >> PAGE_SHIFT];
 	data = kmap(page);
-	off &= ~PAGE_CACHE_MASK;
+	off &= ~PAGE_MASK;
 	m = 1 << (~nidx & 7);
 	byte = data[off];
 	if (!(byte & m)) {
diff --git a/fs/hfsplus/inode.c b/fs/hfsplus/inode.c
index 4b4ae8ccc6dc..c87a0728f9f5 100644
--- a/fs/hfsplus/inode.c
+++ b/fs/hfsplus/inode.c
@@ -86,9 +86,9 @@ static int hfsplus_releasepage(struct page *page, gfp_t mask)
 	}
 	if (!tree)
 		return 0;
-	if (tree->node_size >= PAGE_CACHE_SIZE) {
+	if (tree->node_size >= PAGE_SIZE) {
 		nidx = page->index >>
-			(tree->node_size_shift - PAGE_CACHE_SHIFT);
+			(tree->node_size_shift - PAGE_SHIFT);
 		spin_lock(&tree->hash_lock);
 		node = hfs_bnode_findhash(tree, nidx);
 		if (!node)
@@ -102,8 +102,8 @@ static int hfsplus_releasepage(struct page *page, gfp_t mask)
 		spin_unlock(&tree->hash_lock);
 	} else {
 		nidx = page->index <<
-			(PAGE_CACHE_SHIFT - tree->node_size_shift);
-		i = 1 << (PAGE_CACHE_SHIFT - tree->node_size_shift);
+			(PAGE_SHIFT - tree->node_size_shift);
+		i = 1 << (PAGE_SHIFT - tree->node_size_shift);
 		spin_lock(&tree->hash_lock);
 		do {
 			node = hfs_bnode_findhash(tree, nidx++);
diff --git a/fs/hfsplus/super.c b/fs/hfsplus/super.c
index b9436d923585..c06cb3ca869a 100644
--- a/fs/hfsplus/super.c
+++ b/fs/hfsplus/super.c
@@ -435,7 +435,7 @@ static int hfsplus_fill_super(struct super_block *sb, void *data, int silent)
 	err = -EFBIG;
 	last_fs_block = sbi->total_blocks - 1;
 	last_fs_page = (last_fs_block << sbi->alloc_blksz_shift) >>
-			PAGE_CACHE_SHIFT;
+			PAGE_SHIFT;
 
 	if ((last_fs_block > (sector_t)(~0ULL) >> (sbi->alloc_blksz_shift - 9)) ||
 	    (last_fs_page > (pgoff_t)(~0ULL))) {
* Unmerged path fs/hfsplus/xattr.c
* Unmerged path fs/hostfs/hostfs_kern.c
* Unmerged path fs/hugetlbfs/inode.c
diff --git a/fs/isofs/compress.c b/fs/isofs/compress.c
index 592e5115a561..d628770bb42d 100644
--- a/fs/isofs/compress.c
+++ b/fs/isofs/compress.c
@@ -26,7 +26,7 @@
 #include "zisofs.h"
 
 /* This should probably be global. */
-static char zisofs_sink_page[PAGE_CACHE_SIZE];
+static char zisofs_sink_page[PAGE_SIZE];
 
 /*
  * This contains the zlib memory allocation and the mutex for the
@@ -70,11 +70,11 @@ static loff_t zisofs_uncompress_block(struct inode *inode, loff_t block_start,
 		for ( i = 0 ; i < pcount ; i++ ) {
 			if (!pages[i])
 				continue;
-			memset(page_address(pages[i]), 0, PAGE_CACHE_SIZE);
+			memset(page_address(pages[i]), 0, PAGE_SIZE);
 			flush_dcache_page(pages[i]);
 			SetPageUptodate(pages[i]);
 		}
-		return ((loff_t)pcount) << PAGE_CACHE_SHIFT;
+		return ((loff_t)pcount) << PAGE_SHIFT;
 	}
 
 	/* Because zlib is not thread-safe, do all the I/O at the top. */
@@ -121,11 +121,11 @@ static loff_t zisofs_uncompress_block(struct inode *inode, loff_t block_start,
 			if (pages[curpage]) {
 				stream.next_out = page_address(pages[curpage])
 						+ poffset;
-				stream.avail_out = PAGE_CACHE_SIZE - poffset;
+				stream.avail_out = PAGE_SIZE - poffset;
 				poffset = 0;
 			} else {
 				stream.next_out = (void *)&zisofs_sink_page;
-				stream.avail_out = PAGE_CACHE_SIZE;
+				stream.avail_out = PAGE_SIZE;
 			}
 		}
 		if (!stream.avail_in) {
@@ -220,14 +220,14 @@ static int zisofs_fill_pages(struct inode *inode, int full_page, int pcount,
 	 * pages with the data we have anyway...
 	 */
 	start_off = page_offset(pages[full_page]);
-	end_off = min_t(loff_t, start_off + PAGE_CACHE_SIZE, inode->i_size);
+	end_off = min_t(loff_t, start_off + PAGE_SIZE, inode->i_size);
 
 	cstart_block = start_off >> zisofs_block_shift;
 	cend_block = (end_off + (1 << zisofs_block_shift) - 1)
 			>> zisofs_block_shift;
 
-	WARN_ON(start_off - (full_page << PAGE_CACHE_SHIFT) !=
-		((cstart_block << zisofs_block_shift) & PAGE_CACHE_MASK));
+	WARN_ON(start_off - (full_page << PAGE_SHIFT) !=
+		((cstart_block << zisofs_block_shift) & PAGE_MASK));
 
 	/* Find the pointer to this specific chunk */
 	/* Note: we're not using isonum_731() here because the data is known aligned */
@@ -260,10 +260,10 @@ static int zisofs_fill_pages(struct inode *inode, int full_page, int pcount,
 		ret = zisofs_uncompress_block(inode, block_start, block_end,
 					      pcount, pages, poffset, &err);
 		poffset += ret;
-		pages += poffset >> PAGE_CACHE_SHIFT;
-		pcount -= poffset >> PAGE_CACHE_SHIFT;
-		full_page -= poffset >> PAGE_CACHE_SHIFT;
-		poffset &= ~PAGE_CACHE_MASK;
+		pages += poffset >> PAGE_SHIFT;
+		pcount -= poffset >> PAGE_SHIFT;
+		full_page -= poffset >> PAGE_SHIFT;
+		poffset &= ~PAGE_MASK;
 
 		if (err) {
 			brelse(bh);
@@ -282,7 +282,7 @@ static int zisofs_fill_pages(struct inode *inode, int full_page, int pcount,
 
 	if (poffset && *pages) {
 		memset(page_address(*pages) + poffset, 0,
-		       PAGE_CACHE_SIZE - poffset);
+		       PAGE_SIZE - poffset);
 		flush_dcache_page(*pages);
 		SetPageUptodate(*pages);
 	}
@@ -302,12 +302,12 @@ static int zisofs_readpage(struct file *file, struct page *page)
 	int i, pcount, full_page;
 	unsigned int zisofs_block_shift = ISOFS_I(inode)->i_format_parm[1];
 	unsigned int zisofs_pages_per_cblock =
-		PAGE_CACHE_SHIFT <= zisofs_block_shift ?
-		(1 << (zisofs_block_shift - PAGE_CACHE_SHIFT)) : 0;
+		PAGE_SHIFT <= zisofs_block_shift ?
+		(1 << (zisofs_block_shift - PAGE_SHIFT)) : 0;
 	struct page *pages[max_t(unsigned, zisofs_pages_per_cblock, 1)];
 	pgoff_t index = page->index, end_index;
 
-	end_index = (inode->i_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+	end_index = (inode->i_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	/*
 	 * If this page is wholly outside i_size we just return zero;
 	 * do_generic_file_read() will handle this for us
@@ -318,7 +318,7 @@ static int zisofs_readpage(struct file *file, struct page *page)
 		return 0;
 	}
 
-	if (PAGE_CACHE_SHIFT <= zisofs_block_shift) {
+	if (PAGE_SHIFT <= zisofs_block_shift) {
 		/* We have already been given one page, this is the one
 		   we must do. */
 		full_page = index & (zisofs_pages_per_cblock - 1);
@@ -351,7 +351,7 @@ static int zisofs_readpage(struct file *file, struct page *page)
 			kunmap(pages[i]);
 			unlock_page(pages[i]);
 			if (i != full_page)
-				page_cache_release(pages[i]);
+				put_page(pages[i]);
 		}
 	}			
 
diff --git a/fs/isofs/inode.c b/fs/isofs/inode.c
index 2e2af97df075..2f7a1e75fb78 100644
--- a/fs/isofs/inode.c
+++ b/fs/isofs/inode.c
@@ -1040,7 +1040,7 @@ int isofs_get_blocks(struct inode *inode, sector_t iblock,
 		 * the page with useless information without generating any
 		 * I/O errors.
 		 */
-		if (b_off > ((inode->i_size + PAGE_CACHE_SIZE - 1) >> ISOFS_BUFFER_BITS(inode))) {
+		if (b_off > ((inode->i_size + PAGE_SIZE - 1) >> ISOFS_BUFFER_BITS(inode))) {
 			printk(KERN_DEBUG "%s: block >= EOF (%lu, %llu)\n",
 				__func__, b_off,
 				(unsigned long long)inode->i_size);
diff --git a/fs/jbd2/commit.c b/fs/jbd2/commit.c
index dbdd961c74b7..afc7e6857818 100644
--- a/fs/jbd2/commit.c
+++ b/fs/jbd2/commit.c
@@ -81,11 +81,11 @@ static void release_buffer_page(struct buffer_head *bh)
 	if (!trylock_page(page))
 		goto nope;
 
-	page_cache_get(page);
+	get_page(page);
 	__brelse(bh);
 	try_to_free_buffers(page);
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return;
 
 nope:
diff --git a/fs/jbd2/journal.c b/fs/jbd2/journal.c
index 64f1430b0eef..83341516e206 100644
--- a/fs/jbd2/journal.c
+++ b/fs/jbd2/journal.c
@@ -2204,7 +2204,7 @@ void jbd2_journal_ack_err(journal_t *journal)
 
 int jbd2_journal_blocks_per_page(struct inode *inode)
 {
-	return 1 << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+	return 1 << (PAGE_SHIFT - inode->i_sb->s_blocksize_bits);
 }
 
 /*
diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c
index d0d97c6c03bb..e94113713abf 100644
--- a/fs/jbd2/transaction.c
+++ b/fs/jbd2/transaction.c
@@ -2198,7 +2198,7 @@ int jbd2_journal_invalidatepage(journal_t *journal,
 	struct buffer_head *head, *bh, *next;
 	unsigned int stop = offset + length;
 	unsigned int curr_off = 0;
-	int partial_page = (offset || length < PAGE_CACHE_SIZE);
+	int partial_page = (offset || length < PAGE_SIZE);
 	int may_free = 1;
 	int ret = 0;
 
@@ -2207,7 +2207,7 @@ int jbd2_journal_invalidatepage(journal_t *journal,
 	if (!page_has_buffers(page))
 		return 0;
 
-	BUG_ON(stop > PAGE_CACHE_SIZE || stop < length);
+	BUG_ON(stop > PAGE_SIZE || stop < length);
 
 	/* We will potentially be playing with lists other than just the
 	 * data lists (especially for journaled data mode), so be
diff --git a/fs/jffs2/debug.c b/fs/jffs2/debug.c
index 1090eb64b90d..9d26b1b9fc01 100644
--- a/fs/jffs2/debug.c
+++ b/fs/jffs2/debug.c
@@ -95,15 +95,15 @@ __jffs2_dbg_fragtree_paranoia_check_nolock(struct jffs2_inode_info *f)
 			   rather than mucking around with actually reading the node
 			   and checking the compression type, which is the real way
 			   to tell a hole node. */
-			if (frag->ofs & (PAGE_CACHE_SIZE-1) && frag_prev(frag)
-					&& frag_prev(frag)->size < PAGE_CACHE_SIZE && frag_prev(frag)->node) {
+			if (frag->ofs & (PAGE_SIZE-1) && frag_prev(frag)
+					&& frag_prev(frag)->size < PAGE_SIZE && frag_prev(frag)->node) {
 				JFFS2_ERROR("REF_PRISTINE node at 0x%08x had a previous non-hole frag in the same page. Tell dwmw2.\n",
 					ref_offset(fn->raw));
 				bitched = 1;
 			}
 
-			if ((frag->ofs+frag->size) & (PAGE_CACHE_SIZE-1) && frag_next(frag)
-					&& frag_next(frag)->size < PAGE_CACHE_SIZE && frag_next(frag)->node) {
+			if ((frag->ofs+frag->size) & (PAGE_SIZE-1) && frag_next(frag)
+					&& frag_next(frag)->size < PAGE_SIZE && frag_next(frag)->node) {
 				JFFS2_ERROR("REF_PRISTINE node at 0x%08x (%08x-%08x) had a following non-hole frag in the same page. Tell dwmw2.\n",
 				       ref_offset(fn->raw), frag->ofs, frag->ofs+frag->size);
 				bitched = 1;
* Unmerged path fs/jffs2/file.c
* Unmerged path fs/jffs2/fs.c
diff --git a/fs/jffs2/gc.c b/fs/jffs2/gc.c
index 5a2dec2b064c..b3ab366f7d4f 100644
--- a/fs/jffs2/gc.c
+++ b/fs/jffs2/gc.c
@@ -532,7 +532,7 @@ static int jffs2_garbage_collect_live(struct jffs2_sb_info *c,  struct jffs2_era
 				goto upnout;
 		}
 		/* We found a datanode. Do the GC */
-		if((start >> PAGE_CACHE_SHIFT) < ((end-1) >> PAGE_CACHE_SHIFT)) {
+		if((start >> PAGE_SHIFT) < ((end-1) >> PAGE_SHIFT)) {
 			/* It crosses a page boundary. Therefore, it must be a hole. */
 			ret = jffs2_garbage_collect_hole(c, jeb, f, fn, start, end);
 		} else {
@@ -1172,8 +1172,8 @@ static int jffs2_garbage_collect_dnode(struct jffs2_sb_info *c, struct jffs2_era
 		struct jffs2_node_frag *frag;
 		uint32_t min, max;
 
-		min = start & ~(PAGE_CACHE_SIZE-1);
-		max = min + PAGE_CACHE_SIZE;
+		min = start & ~(PAGE_SIZE-1);
+		max = min + PAGE_SIZE;
 
 		frag = jffs2_lookup_node_frag(&f->fragtree, start);
 
@@ -1328,7 +1328,7 @@ static int jffs2_garbage_collect_dnode(struct jffs2_sb_info *c, struct jffs2_era
 		cdatalen = min_t(uint32_t, alloclen - sizeof(ri), end - offset);
 		datalen = end - offset;
 
-		writebuf = pg_ptr + (offset & (PAGE_CACHE_SIZE -1));
+		writebuf = pg_ptr + (offset & (PAGE_SIZE -1));
 
 		comprtype = jffs2_compress(c, f, writebuf, &comprbuf, &datalen, &cdatalen);
 
diff --git a/fs/jffs2/nodelist.c b/fs/jffs2/nodelist.c
index 975a1f562c10..932b6d47a933 100644
--- a/fs/jffs2/nodelist.c
+++ b/fs/jffs2/nodelist.c
@@ -90,7 +90,7 @@ uint32_t jffs2_truncate_fragtree(struct jffs2_sb_info *c, struct rb_root *list,
 
 	/* If the last fragment starts at the RAM page boundary, it is
 	 * REF_PRISTINE irrespective of its size. */
-	if (frag->node && (frag->ofs & (PAGE_CACHE_SIZE - 1)) == 0) {
+	if (frag->node && (frag->ofs & (PAGE_SIZE - 1)) == 0) {
 		dbg_fragtree2("marking the last fragment 0x%08x-0x%08x REF_PRISTINE.\n",
 			frag->ofs, frag->ofs + frag->size);
 		frag->node->raw->flash_offset = ref_offset(frag->node->raw) | REF_PRISTINE;
@@ -237,7 +237,7 @@ static int jffs2_add_frag_to_fragtree(struct jffs2_sb_info *c, struct rb_root *r
 		   If so, both 'this' and the new node get marked REF_NORMAL so
 		   the GC can take a look.
 		*/
-		if (lastend && (lastend-1) >> PAGE_CACHE_SHIFT == newfrag->ofs >> PAGE_CACHE_SHIFT) {
+		if (lastend && (lastend-1) >> PAGE_SHIFT == newfrag->ofs >> PAGE_SHIFT) {
 			if (this->node)
 				mark_ref_normal(this->node->raw);
 			mark_ref_normal(newfrag->node->raw);
@@ -382,7 +382,7 @@ int jffs2_add_full_dnode_to_inode(struct jffs2_sb_info *c, struct jffs2_inode_in
 
 	/* If we now share a page with other nodes, mark either previous
 	   or next node REF_NORMAL, as appropriate.  */
-	if (newfrag->ofs & (PAGE_CACHE_SIZE-1)) {
+	if (newfrag->ofs & (PAGE_SIZE-1)) {
 		struct jffs2_node_frag *prev = frag_prev(newfrag);
 
 		mark_ref_normal(fn->raw);
@@ -391,7 +391,7 @@ int jffs2_add_full_dnode_to_inode(struct jffs2_sb_info *c, struct jffs2_inode_in
 			mark_ref_normal(prev->node->raw);
 	}
 
-	if ((newfrag->ofs+newfrag->size) & (PAGE_CACHE_SIZE-1)) {
+	if ((newfrag->ofs+newfrag->size) & (PAGE_SIZE-1)) {
 		struct jffs2_node_frag *next = frag_next(newfrag);
 
 		if (next) {
diff --git a/fs/jffs2/write.c b/fs/jffs2/write.c
index b634de4c8101..7fb187ab2682 100644
--- a/fs/jffs2/write.c
+++ b/fs/jffs2/write.c
@@ -172,8 +172,8 @@ struct jffs2_full_dnode *jffs2_write_dnode(struct jffs2_sb_info *c, struct jffs2
 	   beginning of a page and runs to the end of the file, or if
 	   it's a hole node, mark it REF_PRISTINE, else REF_NORMAL.
 	*/
-	if ((je32_to_cpu(ri->dsize) >= PAGE_CACHE_SIZE) ||
-	    ( ((je32_to_cpu(ri->offset)&(PAGE_CACHE_SIZE-1))==0) &&
+	if ((je32_to_cpu(ri->dsize) >= PAGE_SIZE) ||
+	    ( ((je32_to_cpu(ri->offset)&(PAGE_SIZE-1))==0) &&
 	      (je32_to_cpu(ri->dsize)+je32_to_cpu(ri->offset) ==  je32_to_cpu(ri->isize)))) {
 		flash_ofs |= REF_PRISTINE;
 	} else {
@@ -366,7 +366,8 @@ int jffs2_write_inode_range(struct jffs2_sb_info *c, struct jffs2_inode_info *f,
 			break;
 		}
 		mutex_lock(&f->sem);
-		datalen = min_t(uint32_t, writelen, PAGE_CACHE_SIZE - (offset & (PAGE_CACHE_SIZE-1)));
+		datalen = min_t(uint32_t, writelen,
+				PAGE_SIZE - (offset & (PAGE_SIZE-1)));
 		cdatalen = min_t(uint32_t, alloclen - sizeof(*ri), datalen);
 
 		comprtype = jffs2_compress(c, f, buf, &comprbuf, &datalen, &cdatalen);
* Unmerged path fs/jfs/jfs_metapage.c
diff --git a/fs/jfs/jfs_metapage.h b/fs/jfs/jfs_metapage.h
index a78beda85f68..786c5e7785a4 100644
--- a/fs/jfs/jfs_metapage.h
+++ b/fs/jfs/jfs_metapage.h
@@ -107,7 +107,7 @@ static inline void metapage_nohomeok(struct metapage *mp)
 	lock_page(page);
 	if (!mp->nohomeok++) {
 		mark_metapage_dirty(mp);
-		page_cache_get(page);
+		get_page(page);
 		wait_on_page_writeback(page);
 	}
 	unlock_page(page);
@@ -129,7 +129,7 @@ static inline void metapage_wait_for_io(struct metapage *mp)
 static inline void _metapage_homeok(struct metapage *mp)
 {
 	if (!--mp->nohomeok)
-		page_cache_release(mp->page);
+		put_page(mp->page);
 }
 
 static inline void metapage_homeok(struct metapage *mp)
* Unmerged path fs/jfs/super.c
* Unmerged path fs/kernfs/mount.c
diff --git a/fs/libfs.c b/fs/libfs.c
index 23fbb25d1c9f..6104647e7217 100644
--- a/fs/libfs.c
+++ b/fs/libfs.c
@@ -29,7 +29,7 @@ int simple_getattr(struct vfsmount *mnt, struct dentry *dentry,
 {
 	struct inode *inode = dentry->d_inode;
 	generic_fillattr(inode, stat);
-	stat->blocks = inode->i_mapping->nrpages << (PAGE_CACHE_SHIFT - 9);
+	stat->blocks = inode->i_mapping->nrpages << (PAGE_SHIFT - 9);
 	return 0;
 }
 EXPORT_SYMBOL(simple_getattr);
@@ -37,7 +37,7 @@ EXPORT_SYMBOL(simple_getattr);
 int simple_statfs(struct dentry *dentry, struct kstatfs *buf)
 {
 	buf->f_type = dentry->d_sb->s_magic;
-	buf->f_bsize = PAGE_CACHE_SIZE;
+	buf->f_bsize = PAGE_SIZE;
 	buf->f_namelen = NAME_MAX;
 	return 0;
 }
@@ -417,7 +417,7 @@ int simple_write_begin(struct file *file, struct address_space *mapping,
 	struct page *page;
 	pgoff_t index;
 
-	index = pos >> PAGE_CACHE_SHIFT;
+	index = pos >> PAGE_SHIFT;
 
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page)
@@ -425,10 +425,10 @@ int simple_write_begin(struct file *file, struct address_space *mapping,
 
 	*pagep = page;
 
-	if (!PageUptodate(page) && (len != PAGE_CACHE_SIZE)) {
-		unsigned from = pos & (PAGE_CACHE_SIZE - 1);
+	if (!PageUptodate(page) && (len != PAGE_SIZE)) {
+		unsigned from = pos & (PAGE_SIZE - 1);
 
-		zero_user_segments(page, 0, from, from + len, PAGE_CACHE_SIZE);
+		zero_user_segments(page, 0, from, from + len, PAGE_SIZE);
 	}
 	return 0;
 }
@@ -464,7 +464,7 @@ int simple_write_end(struct file *file, struct address_space *mapping,
 
 	/* zero the stale part of the page if we did a short copy */
 	if (copied < len) {
-		unsigned from = pos & (PAGE_CACHE_SIZE - 1);
+		unsigned from = pos & (PAGE_SIZE - 1);
 
 		zero_user(page, from + copied, len - copied);
 	}
@@ -480,7 +480,7 @@ int simple_write_end(struct file *file, struct address_space *mapping,
 
 	set_page_dirty(page);
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
 	return copied;
 }
@@ -499,8 +499,8 @@ int simple_fill_super(struct super_block *s, unsigned long magic,
 	struct dentry *dentry;
 	int i;
 
-	s->s_blocksize = PAGE_CACHE_SIZE;
-	s->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	s->s_blocksize = PAGE_SIZE;
+	s->s_blocksize_bits = PAGE_SHIFT;
 	s->s_magic = magic;
 	s->s_op = &simple_super_operations;
 	s->s_time_gran = 1;
@@ -989,12 +989,12 @@ int generic_check_addressable(unsigned blocksize_bits, u64 num_blocks)
 {
 	u64 last_fs_block = num_blocks - 1;
 	u64 last_fs_page =
-		last_fs_block >> (PAGE_CACHE_SHIFT - blocksize_bits);
+		last_fs_block >> (PAGE_SHIFT - blocksize_bits);
 
 	if (unlikely(num_blocks == 0))
 		return 0;
 
-	if ((blocksize_bits < 9) || (blocksize_bits > PAGE_CACHE_SHIFT))
+	if ((blocksize_bits < 9) || (blocksize_bits > PAGE_SHIFT))
 		return -EINVAL;
 
 	if ((last_fs_block > (sector_t)(~0ULL) >> (blocksize_bits - 9)) ||
diff --git a/fs/logfs/dev_bdev.c b/fs/logfs/dev_bdev.c
index 5cbcc23e0a95..4cb2acdda84a 100644
--- a/fs/logfs/dev_bdev.c
+++ b/fs/logfs/dev_bdev.c
@@ -77,7 +77,7 @@ static void writeseg_end_io(struct bio *bio, int err)
 
 	bio_for_each_segment_all(bvec, bio, i) {
 		end_page_writeback(bvec->bv_page);
-		page_cache_release(bvec->bv_page);
+		put_page(bvec->bv_page);
 	}
 	bio_put(bio);
 	if (atomic_dec_and_test(&super->s_pending_writes))
diff --git a/fs/logfs/dev_mtd.c b/fs/logfs/dev_mtd.c
index 9c501449450d..b76a62b1978f 100644
--- a/fs/logfs/dev_mtd.c
+++ b/fs/logfs/dev_mtd.c
@@ -46,9 +46,9 @@ static int loffs_mtd_write(struct super_block *sb, loff_t ofs, size_t len,
 
 	BUG_ON((ofs >= mtd->size) || (len > mtd->size - ofs));
 	BUG_ON(ofs != (ofs >> super->s_writeshift) << super->s_writeshift);
-	BUG_ON(len > PAGE_CACHE_SIZE);
-	page_start = ofs & PAGE_CACHE_MASK;
-	page_end = PAGE_CACHE_ALIGN(ofs + len) - 1;
+	BUG_ON(len > PAGE_SIZE);
+	page_start = ofs & PAGE_MASK;
+	page_end = PAGE_ALIGN(ofs + len) - 1;
 	ret = mtd_write(mtd, ofs, len, &retlen, buf);
 	if (ret || (retlen != len))
 		return -EIO;
@@ -82,7 +82,7 @@ static int logfs_mtd_erase_mapping(struct super_block *sb, loff_t ofs,
 		if (!page)
 			continue;
 		memset(page_address(page), 0xFF, PAGE_SIZE);
-		page_cache_release(page);
+		put_page(page);
 	}
 	return 0;
 }
@@ -195,7 +195,7 @@ static int __logfs_mtd_writeseg(struct super_block *sb, u64 ofs, pgoff_t index,
 		err = loffs_mtd_write(sb, page->index << PAGE_SHIFT, PAGE_SIZE,
 					page_address(page));
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		if (err)
 			return err;
 	}
diff --git a/fs/logfs/dir.c b/fs/logfs/dir.c
index b82751082112..b58b52bd570d 100644
--- a/fs/logfs/dir.c
+++ b/fs/logfs/dir.c
@@ -183,7 +183,7 @@ static struct page *logfs_get_dd_page(struct inode *dir, struct dentry *dentry)
 		if (name->len != be16_to_cpu(dd->namelen) ||
 				memcmp(name->name, dd->name, name->len)) {
 			kunmap_atomic(dd);
-			page_cache_release(page);
+			put_page(page);
 			continue;
 		}
 
@@ -238,7 +238,7 @@ static int logfs_unlink(struct inode *dir, struct dentry *dentry)
 		return PTR_ERR(page);
 	}
 	index = page->index;
-	page_cache_release(page);
+	put_page(page);
 
 	mutex_lock(&super->s_dirop_mutex);
 	logfs_add_transaction(dir, ta);
@@ -309,7 +309,7 @@ static int __logfs_readdir(struct file *file, void *buf, filldir_t filldir)
 		full = filldir(buf, (char *)dd->name, be16_to_cpu(dd->namelen),
 				pos, be64_to_cpu(dd->ino), dd->type);
 		kunmap(page);
-		page_cache_release(page);
+		put_page(page);
 		if (full)
 			break;
 	}
@@ -368,7 +368,7 @@ static struct dentry *logfs_lookup(struct inode *dir, struct dentry *dentry,
 	dd = kmap_atomic(page);
 	ino = be64_to_cpu(dd->ino);
 	kunmap_atomic(dd);
-	page_cache_release(page);
+	put_page(page);
 
 	inode = logfs_iget(dir->i_sb, ino);
 	if (IS_ERR(inode))
@@ -411,7 +411,7 @@ static int logfs_write_dir(struct inode *dir, struct dentry *dentry,
 
 		err = logfs_write_buf(dir, page, WF_LOCK);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		if (!err)
 			grow_dir(dir, index);
 		return err;
@@ -579,7 +579,7 @@ static int logfs_get_dd(struct inode *dir, struct dentry *dentry,
 	map = kmap_atomic(page);
 	memcpy(dd, map, sizeof(*dd));
 	kunmap_atomic(map);
-	page_cache_release(page);
+	put_page(page);
 	return 0;
 }
 
diff --git a/fs/logfs/file.c b/fs/logfs/file.c
index c2219a6dd3c8..d6b854d6777c 100644
--- a/fs/logfs/file.c
+++ b/fs/logfs/file.c
@@ -15,21 +15,21 @@ static int logfs_write_begin(struct file *file, struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	struct page *page;
-	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
+	pgoff_t index = pos >> PAGE_SHIFT;
 
 	page = grab_cache_page_write_begin(mapping, index, flags);
 	if (!page)
 		return -ENOMEM;
 	*pagep = page;
 
-	if ((len == PAGE_CACHE_SIZE) || PageUptodate(page))
+	if ((len == PAGE_SIZE) || PageUptodate(page))
 		return 0;
-	if ((pos & PAGE_CACHE_MASK) >= i_size_read(inode)) {
-		unsigned start = pos & (PAGE_CACHE_SIZE - 1);
+	if ((pos & PAGE_MASK) >= i_size_read(inode)) {
+		unsigned start = pos & (PAGE_SIZE - 1);
 		unsigned end = start + len;
 
 		/* Reading beyond i_size is simple: memset to zero */
-		zero_user_segments(page, 0, start, end, PAGE_CACHE_SIZE);
+		zero_user_segments(page, 0, start, end, PAGE_SIZE);
 		return 0;
 	}
 	return logfs_readpage_nolock(page);
@@ -41,11 +41,11 @@ static int logfs_write_end(struct file *file, struct address_space *mapping,
 {
 	struct inode *inode = mapping->host;
 	pgoff_t index = page->index;
-	unsigned start = pos & (PAGE_CACHE_SIZE - 1);
+	unsigned start = pos & (PAGE_SIZE - 1);
 	unsigned end = start + copied;
 	int ret = 0;
 
-	BUG_ON(PAGE_CACHE_SIZE != inode->i_sb->s_blocksize);
+	BUG_ON(PAGE_SIZE != inode->i_sb->s_blocksize);
 	BUG_ON(page->index > I3_BLOCKS);
 
 	if (copied < len) {
@@ -61,8 +61,8 @@ static int logfs_write_end(struct file *file, struct address_space *mapping,
 	if (copied == 0)
 		goto out; /* FIXME: do we need to update inode? */
 
-	if (i_size_read(inode) < (index << PAGE_CACHE_SHIFT) + end) {
-		i_size_write(inode, (index << PAGE_CACHE_SHIFT) + end);
+	if (i_size_read(inode) < (index << PAGE_SHIFT) + end) {
+		i_size_write(inode, (index << PAGE_SHIFT) + end);
 		mark_inode_dirty_sync(inode);
 	}
 
@@ -75,7 +75,7 @@ static int logfs_write_end(struct file *file, struct address_space *mapping,
 	}
 out:
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return ret ? ret : copied;
 }
 
@@ -118,7 +118,7 @@ static int logfs_writepage(struct page *page, struct writeback_control *wbc)
 {
 	struct inode *inode = page->mapping->host;
 	loff_t i_size = i_size_read(inode);
-	pgoff_t end_index = i_size >> PAGE_CACHE_SHIFT;
+	pgoff_t end_index = i_size >> PAGE_SHIFT;
 	unsigned offset;
 	u64 bix;
 	level_t level;
@@ -142,7 +142,7 @@ static int logfs_writepage(struct page *page, struct writeback_control *wbc)
 		return __logfs_writepage(page);
 
 	 /* Is the page fully outside i_size? (truncate in progress) */
-	offset = i_size & (PAGE_CACHE_SIZE-1);
+	offset = i_size & (PAGE_SIZE-1);
 	if (bix > end_index || offset == 0) {
 		unlock_page(page);
 		return 0; /* don't care */
@@ -155,7 +155,7 @@ static int logfs_writepage(struct page *page, struct writeback_control *wbc)
 	 * the  page size, the remaining memory is zeroed when mapped, and
 	 * writes to that region are not written out to the file."
 	 */
-	zero_user_segment(page, offset, PAGE_CACHE_SIZE);
+	zero_user_segment(page, offset, PAGE_SIZE);
 	return __logfs_writepage(page);
 }
 
diff --git a/fs/logfs/readwrite.c b/fs/logfs/readwrite.c
index 48140315f627..97694c296ba4 100644
--- a/fs/logfs/readwrite.c
+++ b/fs/logfs/readwrite.c
@@ -281,7 +281,7 @@ static struct page *logfs_get_read_page(struct inode *inode, u64 bix,
 static void logfs_put_read_page(struct page *page)
 {
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 }
 
 static void logfs_lock_write_page(struct page *page)
@@ -323,7 +323,7 @@ repeat:
 			return NULL;
 		err = add_to_page_cache_lru(page, mapping, index, GFP_NOFS);
 		if (unlikely(err)) {
-			page_cache_release(page);
+			put_page(page);
 			if (err == -EEXIST)
 				goto repeat;
 			return NULL;
@@ -342,7 +342,7 @@ static void logfs_unlock_write_page(struct page *page)
 static void logfs_put_write_page(struct page *page)
 {
 	logfs_unlock_write_page(page);
-	page_cache_release(page);
+	put_page(page);
 }
 
 static struct page *logfs_get_page(struct inode *inode, u64 bix, level_t level,
@@ -562,7 +562,7 @@ static void indirect_free_block(struct super_block *sb,
 
 	if (PagePrivate(page)) {
 		ClearPagePrivate(page);
-		page_cache_release(page);
+		put_page(page);
 		set_page_private(page, 0);
 	}
 	__free_block(sb, block);
@@ -655,7 +655,7 @@ static void alloc_data_block(struct inode *inode, struct page *page)
 	block->page = page;
 
 	SetPagePrivate(page);
-	page_cache_get(page);
+	get_page(page);
 	set_page_private(page, (unsigned long) block);
 
 	block->ops = &indirect_block_ops;
@@ -709,7 +709,7 @@ static u64 block_get_pointer(struct page *page, int index)
 
 static int logfs_read_empty(struct page *page)
 {
-	zero_user_segment(page, 0, PAGE_CACHE_SIZE);
+	zero_user_segment(page, 0, PAGE_SIZE);
 	return 0;
 }
 
@@ -1660,7 +1660,7 @@ static int truncate_data_block(struct inode *inode, struct page *page,
 	if (err)
 		return err;
 
-	zero_user_segment(page, size - pageofs, PAGE_CACHE_SIZE);
+	zero_user_segment(page, size - pageofs, PAGE_SIZE);
 	return logfs_segment_write(inode, page, shadow);
 }
 
@@ -1919,7 +1919,7 @@ static void move_page_to_inode(struct inode *inode, struct page *page)
 	block->page = NULL;
 	if (PagePrivate(page)) {
 		ClearPagePrivate(page);
-		page_cache_release(page);
+		put_page(page);
 		set_page_private(page, 0);
 	}
 }
@@ -1940,7 +1940,7 @@ static void move_inode_to_page(struct page *page, struct inode *inode)
 
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
-		page_cache_get(page);
+		get_page(page);
 		set_page_private(page, (unsigned long) block);
 	}
 
@@ -1971,7 +1971,7 @@ int logfs_read_inode(struct inode *inode)
 	logfs_disk_to_inode(di, inode);
 	kunmap_atomic(di);
 	move_page_to_inode(inode, page);
-	page_cache_release(page);
+	put_page(page);
 	return 0;
 }
 
diff --git a/fs/logfs/segment.c b/fs/logfs/segment.c
index 038da0991794..713097d0e9d1 100644
--- a/fs/logfs/segment.c
+++ b/fs/logfs/segment.c
@@ -89,9 +89,9 @@ int __logfs_buf_write(struct logfs_area *area, u64 ofs, void *buf, size_t len,
 
 		if (!PagePrivate(page)) {
 			SetPagePrivate(page);
-			page_cache_get(page);
+			get_page(page);
 		}
-		page_cache_release(page);
+		put_page(page);
 
 		buf += copylen;
 		len -= copylen;
@@ -116,9 +116,9 @@ static void pad_partial_page(struct logfs_area *area)
 		memset(page_address(page) + offset, 0xff, len);
 		if (!PagePrivate(page)) {
 			SetPagePrivate(page);
-			page_cache_get(page);
+			get_page(page);
 		}
-		page_cache_release(page);
+		put_page(page);
 	}
 }
 
@@ -128,20 +128,20 @@ static void pad_full_pages(struct logfs_area *area)
 	struct logfs_super *super = logfs_super(sb);
 	u64 ofs = dev_ofs(sb, area->a_segno, area->a_used_bytes);
 	u32 len = super->s_segsize - area->a_used_bytes;
-	pgoff_t index = PAGE_CACHE_ALIGN(ofs) >> PAGE_CACHE_SHIFT;
-	pgoff_t no_indizes = len >> PAGE_CACHE_SHIFT;
+	pgoff_t index = PAGE_ALIGN(ofs) >> PAGE_SHIFT;
+	pgoff_t no_indizes = len >> PAGE_SHIFT;
 	struct page *page;
 
 	while (no_indizes) {
 		page = get_mapping_page(sb, index, 0);
 		BUG_ON(!page); /* FIXME: reserve a pool */
 		SetPageUptodate(page);
-		memset(page_address(page), 0xff, PAGE_CACHE_SIZE);
+		memset(page_address(page), 0xff, PAGE_SIZE);
 		if (!PagePrivate(page)) {
 			SetPagePrivate(page);
-			page_cache_get(page);
+			get_page(page);
 		}
-		page_cache_release(page);
+		put_page(page);
 		index++;
 		no_indizes--;
 	}
@@ -410,7 +410,7 @@ int wbuf_read(struct super_block *sb, u64 ofs, size_t len, void *buf)
 		if (IS_ERR(page))
 			return PTR_ERR(page);
 		memcpy(buf, page_address(page) + offset, copylen);
-		page_cache_release(page);
+		put_page(page);
 
 		buf += copylen;
 		len -= copylen;
@@ -498,7 +498,7 @@ static void move_btree_to_page(struct inode *inode, struct page *page,
 
 	if (!PagePrivate(page)) {
 		SetPagePrivate(page);
-		page_cache_get(page);
+		get_page(page);
 		set_page_private(page, (unsigned long) block);
 	}
 	block->ops = &indirect_block_ops;
@@ -553,7 +553,7 @@ void move_page_to_btree(struct page *page)
 
 	if (PagePrivate(page)) {
 		ClearPagePrivate(page);
-		page_cache_release(page);
+		put_page(page);
 		set_page_private(page, 0);
 	}
 	block->ops = &btree_block_ops;
@@ -722,9 +722,9 @@ void freeseg(struct super_block *sb, u32 segno)
 			continue;
 		if (PagePrivate(page)) {
 			ClearPagePrivate(page);
-			page_cache_release(page);
+			put_page(page);
 		}
-		page_cache_release(page);
+		put_page(page);
 	}
 }
 
diff --git a/fs/logfs/super.c b/fs/logfs/super.c
index 54360293bcb5..5751082dba52 100644
--- a/fs/logfs/super.c
+++ b/fs/logfs/super.c
@@ -48,7 +48,7 @@ void emergency_read_end(struct page *page)
 	if (page == emergency_page)
 		mutex_unlock(&emergency_mutex);
 	else
-		page_cache_release(page);
+		put_page(page);
 }
 
 static void dump_segfile(struct super_block *sb)
@@ -206,7 +206,7 @@ static int write_one_sb(struct super_block *sb,
 	logfs_set_segment_erased(sb, segno, ec, 0);
 	logfs_write_ds(sb, ds, segno, ec);
 	err = super->s_devops->write_sb(sb, page);
-	page_cache_release(page);
+	put_page(page);
 	return err;
 }
 
@@ -366,24 +366,24 @@ static struct page *find_super_block(struct super_block *sb)
 		return NULL;
 	last = super->s_devops->find_last_sb(sb, &super->s_sb_ofs[1]);
 	if (!last || IS_ERR(last)) {
-		page_cache_release(first);
+		put_page(first);
 		return NULL;
 	}
 
 	if (!logfs_check_ds(page_address(first))) {
-		page_cache_release(last);
+		put_page(last);
 		return first;
 	}
 
 	/* First one didn't work, try the second superblock */
 	if (!logfs_check_ds(page_address(last))) {
-		page_cache_release(first);
+		put_page(first);
 		return last;
 	}
 
 	/* Neither worked, sorry folks */
-	page_cache_release(first);
-	page_cache_release(last);
+	put_page(first);
+	put_page(last);
 	return NULL;
 }
 
@@ -425,7 +425,7 @@ static int __logfs_read_sb(struct super_block *sb)
 	super->s_data_levels = ds->ds_data_levels;
 	super->s_total_levels = super->s_ifile_levels + super->s_iblock_levels
 		+ super->s_data_levels;
-	page_cache_release(page);
+	put_page(page);
 	return 0;
 }
 
* Unmerged path fs/minix/dir.c
diff --git a/fs/minix/namei.c b/fs/minix/namei.c
index 0db73d9dd668..5e2314433381 100644
--- a/fs/minix/namei.c
+++ b/fs/minix/namei.c
@@ -231,11 +231,11 @@ static int minix_rename(struct inode * old_dir, struct dentry *old_dentry,
 out_dir:
 	if (dir_de) {
 		kunmap(dir_page);
-		page_cache_release(dir_page);
+		put_page(dir_page);
 	}
 out_old:
 	kunmap(old_page);
-	page_cache_release(old_page);
+	put_page(old_page);
 out:
 	return err;
 }
diff --git a/fs/mpage.c b/fs/mpage.c
index 9a5c19c68127..0b2671fd9082 100644
--- a/fs/mpage.c
+++ b/fs/mpage.c
@@ -104,7 +104,7 @@ map_buffer_to_page(struct page *page, struct buffer_head *bh, int page_block)
 		 * don't make any buffers if there is only one buffer on
 		 * the page and the page just needs to be set up to date
 		 */
-		if (inode->i_blkbits == PAGE_CACHE_SHIFT && 
+		if (inode->i_blkbits == PAGE_SHIFT &&
 		    buffer_uptodate(bh)) {
 			SetPageUptodate(page);    
 			return;
@@ -141,7 +141,7 @@ do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages,
 {
 	struct inode *inode = page->mapping->host;
 	const unsigned blkbits = inode->i_blkbits;
-	const unsigned blocks_per_page = PAGE_CACHE_SIZE >> blkbits;
+	const unsigned blocks_per_page = PAGE_SIZE >> blkbits;
 	const unsigned blocksize = 1 << blkbits;
 	sector_t block_in_file;
 	sector_t last_block;
@@ -158,7 +158,7 @@ do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages,
 	if (page_has_buffers(page))
 		goto confused;
 
-	block_in_file = (sector_t)page->index << (PAGE_CACHE_SHIFT - blkbits);
+	block_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);
 	last_block = block_in_file + nr_pages * blocks_per_page;
 	last_block_in_file = (i_size_read(inode) + blocksize - 1) >> blkbits;
 	if (last_block > last_block_in_file)
@@ -245,7 +245,7 @@ do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages,
 	}
 
 	if (first_hole != blocks_per_page) {
-		zero_user_segment(page, first_hole << blkbits, PAGE_CACHE_SIZE);
+		zero_user_segment(page, first_hole << blkbits, PAGE_SIZE);
 		if (first_hole == 0) {
 			SetPageUptodate(page);
 			unlock_page(page);
@@ -375,7 +375,7 @@ mpage_readpages(struct address_space *mapping, struct list_head *pages,
 					&first_logical_block,
 					get_block);
 		}
-		page_cache_release(page);
+		put_page(page);
 	}
 	BUG_ON(!list_empty(pages));
 	if (bio)
@@ -466,7 +466,7 @@ static int __mpage_writepage(struct page *page, struct writeback_control *wbc,
 	struct inode *inode = page->mapping->host;
 	const unsigned blkbits = inode->i_blkbits;
 	unsigned long end_index;
-	const unsigned blocks_per_page = PAGE_CACHE_SIZE >> blkbits;
+	const unsigned blocks_per_page = PAGE_SIZE >> blkbits;
 	sector_t last_block;
 	sector_t block_in_file;
 	sector_t blocks[MAX_BUF_PER_PAGE];
@@ -535,7 +535,7 @@ static int __mpage_writepage(struct page *page, struct writeback_control *wbc,
 	 * The page has no buffers: map it to disk
 	 */
 	BUG_ON(!PageUptodate(page));
-	block_in_file = (sector_t)page->index << (PAGE_CACHE_SHIFT - blkbits);
+	block_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);
 	last_block = (i_size - 1) >> blkbits;
 	map_bh.b_page = page;
 	for (page_block = 0; page_block < blocks_per_page; ) {
@@ -567,7 +567,7 @@ static int __mpage_writepage(struct page *page, struct writeback_control *wbc,
 	first_unmapped = page_block;
 
 page_is_mapped:
-	end_index = i_size >> PAGE_CACHE_SHIFT;
+	end_index = i_size >> PAGE_SHIFT;
 	if (page->index >= end_index) {
 		/*
 		 * The page straddles i_size.  It must be zeroed out on each
@@ -577,11 +577,11 @@ page_is_mapped:
 		 * is zeroed when mapped, and writes to that region are not
 		 * written out to the file."
 		 */
-		unsigned offset = i_size & (PAGE_CACHE_SIZE - 1);
+		unsigned offset = i_size & (PAGE_SIZE - 1);
 
 		if (page->index > end_index || !offset)
 			goto confused;
-		zero_user_segment(page, offset, PAGE_CACHE_SIZE);
+		zero_user_segment(page, offset, PAGE_SIZE);
 	}
 
 	/*
diff --git a/fs/ncpfs/dir.c b/fs/ncpfs/dir.c
index dd8190ae891c..1ba6ef7e4a48 100644
--- a/fs/ncpfs/dir.c
+++ b/fs/ncpfs/dir.c
@@ -534,7 +534,7 @@ static int ncp_readdir(struct file *filp, void *dirent, filldir_t filldir)
 			kunmap(ctl.page);
 			SetPageUptodate(ctl.page);
 			unlock_page(ctl.page);
-			page_cache_release(ctl.page);
+			put_page(ctl.page);
 			ctl.page = NULL;
 		}
 		ctl.idx  = 0;
@@ -544,7 +544,7 @@ invalid_cache:
 	if (ctl.page) {
 		kunmap(ctl.page);
 		unlock_page(ctl.page);
-		page_cache_release(ctl.page);
+		put_page(ctl.page);
 		ctl.page = NULL;
 	}
 	ctl.cache = cache;
@@ -575,14 +575,14 @@ finished:
 		kunmap(ctl.page);
 		SetPageUptodate(ctl.page);
 		unlock_page(ctl.page);
-		page_cache_release(ctl.page);
+		put_page(ctl.page);
 	}
 	if (page) {
 		cache->head = ctl.head;
 		kunmap(page);
 		SetPageUptodate(page);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 out:
 	return result;
@@ -665,7 +665,7 @@ ncp_fill_cache(struct file *filp, void *dirent, filldir_t filldir,
 			kunmap(ctl.page);
 			SetPageUptodate(ctl.page);
 			unlock_page(ctl.page);
-			page_cache_release(ctl.page);
+			put_page(ctl.page);
 		}
 		ctl.cache = NULL;
 		ctl.idx  -= NCP_DIRCACHE_SIZE;
diff --git a/fs/ncpfs/ncplib_kernel.h b/fs/ncpfs/ncplib_kernel.h
index 32c06587351a..c2d2fb2e01d3 100644
--- a/fs/ncpfs/ncplib_kernel.h
+++ b/fs/ncpfs/ncplib_kernel.h
@@ -231,7 +231,7 @@ struct ncp_cache_head {
 	int		eof;
 };
 
-#define NCP_DIRCACHE_SIZE	((int)(PAGE_CACHE_SIZE/sizeof(struct dentry *)))
+#define NCP_DIRCACHE_SIZE	((int)(PAGE_SIZE/sizeof(struct dentry *)))
 union ncp_dir_cache {
 	struct ncp_cache_head	head;
 	struct dentry		*dentry[NCP_DIRCACHE_SIZE];
diff --git a/fs/nfs/blocklayout/blocklayout.c b/fs/nfs/blocklayout/blocklayout.c
index 671d9aa831a9..add13aaaf711 100644
--- a/fs/nfs/blocklayout/blocklayout.c
+++ b/fs/nfs/blocklayout/blocklayout.c
@@ -232,7 +232,7 @@ bl_read_pagelist(struct nfs_pgio_header *header)
 	size_t bytes_left = header->args.count;
 	unsigned int pg_offset = header->args.pgbase, pg_len;
 	struct page **pages = header->args.pages;
-	int pg_index = header->args.pgbase >> PAGE_CACHE_SHIFT;
+	int pg_index = header->args.pgbase >> PAGE_SHIFT;
 	const bool is_dio = (header->dreq != NULL);
 	struct blk_plug plug;
 	int i;
@@ -264,13 +264,13 @@ bl_read_pagelist(struct nfs_pgio_header *header)
 		}
 
 		if (is_dio) {
-			if (pg_offset + bytes_left > PAGE_CACHE_SIZE)
-				pg_len = PAGE_CACHE_SIZE - pg_offset;
+			if (pg_offset + bytes_left > PAGE_SIZE)
+				pg_len = PAGE_SIZE - pg_offset;
 			else
 				pg_len = bytes_left;
 		} else {
 			BUG_ON(pg_offset != 0);
-			pg_len = PAGE_CACHE_SIZE;
+			pg_len = PAGE_SIZE;
 		}
 
 		if (is_hole(&be)) {
@@ -341,9 +341,9 @@ static void bl_write_cleanup(struct work_struct *work)
 
 	if (likely(!hdr->pnfs_error)) {
 		struct pnfs_block_layout *bl = BLK_LSEG2EXT(hdr->lseg);
-		u64 start = hdr->args.offset & (loff_t)PAGE_CACHE_MASK;
+		u64 start = hdr->args.offset & (loff_t)PAGE_MASK;
 		u64 end = (hdr->args.offset + hdr->args.count +
-			PAGE_CACHE_SIZE - 1) & (loff_t)PAGE_CACHE_MASK;
+			PAGE_SIZE - 1) & (loff_t)PAGE_MASK;
 
 		ext_tree_mark_written(bl, start >> SECTOR_SHIFT,
 					(end - start) >> SECTOR_SHIFT, end);
@@ -375,7 +375,7 @@ bl_write_pagelist(struct nfs_pgio_header *header, int sync)
 	loff_t offset = header->args.offset;
 	size_t count = header->args.count;
 	struct page **pages = header->args.pages;
-	int pg_index = header->args.pgbase >> PAGE_CACHE_SHIFT;
+	int pg_index = header->args.pgbase >> PAGE_SHIFT;
 	unsigned int pg_len;
 	struct blk_plug plug;
 	int i;
@@ -394,7 +394,7 @@ bl_write_pagelist(struct nfs_pgio_header *header, int sync)
 	blk_start_plug(&plug);
 
 	/* we always write out the whole page */
-	offset = offset & (loff_t)PAGE_CACHE_MASK;
+	offset = offset & (loff_t)PAGE_MASK;
 	isect = offset >> SECTOR_SHIFT;
 
 	for (i = pg_index; i < header->page_array.npages; i++) {
@@ -410,7 +410,7 @@ bl_write_pagelist(struct nfs_pgio_header *header, int sync)
 			extent_length = be.be_length - (isect - be.be_f_offset);
 		}
 
-		pg_len = PAGE_CACHE_SIZE;
+		pg_len = PAGE_SIZE;
 		bio = do_add_page_to_bio(bio, header->page_array.npages - i,
 					 WRITE, isect, pages[i], &map, &be,
 					 bl_end_io_write, par,
@@ -822,7 +822,7 @@ static u64 pnfs_num_cont_bytes(struct inode *inode, pgoff_t idx)
 	pgoff_t end;
 
 	/* Optimize common case that writes from 0 to end of file */
-	end = DIV_ROUND_UP(i_size_read(inode), PAGE_CACHE_SIZE);
+	end = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 	if (end != inode->i_mapping->nrpages) {
 		rcu_read_lock();
 		end = page_cache_next_hole(mapping, idx + 1, ULONG_MAX);
@@ -830,9 +830,9 @@ static u64 pnfs_num_cont_bytes(struct inode *inode, pgoff_t idx)
 	}
 
 	if (!end)
-		return i_size_read(inode) - (idx << PAGE_CACHE_SHIFT);
+		return i_size_read(inode) - (idx << PAGE_SHIFT);
 	else
-		return (end - idx) << PAGE_CACHE_SHIFT;
+		return (end - idx) << PAGE_SHIFT;
 }
 
 static void
diff --git a/fs/nfs/blocklayout/blocklayout.h b/fs/nfs/blocklayout/blocklayout.h
index 4c38d8b611ca..efc007f00742 100644
--- a/fs/nfs/blocklayout/blocklayout.h
+++ b/fs/nfs/blocklayout/blocklayout.h
@@ -40,8 +40,8 @@
 #include "../pnfs.h"
 #include "../netns.h"
 
-#define PAGE_CACHE_SECTORS (PAGE_CACHE_SIZE >> SECTOR_SHIFT)
-#define PAGE_CACHE_SECTOR_SHIFT (PAGE_CACHE_SHIFT - SECTOR_SHIFT)
+#define PAGE_CACHE_SECTORS (PAGE_SIZE >> SECTOR_SHIFT)
+#define PAGE_CACHE_SECTOR_SHIFT (PAGE_SHIFT - SECTOR_SHIFT)
 #define SECTOR_SIZE (1 << SECTOR_SHIFT)
 
 struct pnfs_block_dev;
diff --git a/fs/nfs/client.c b/fs/nfs/client.c
index 9560533d36c6..537d8bdd26f9 100644
--- a/fs/nfs/client.c
+++ b/fs/nfs/client.c
@@ -738,7 +738,7 @@ static void nfs_server_set_fsinfo(struct nfs_server *server,
 		server->rsize = max_rpc_payload;
 	if (server->rsize > NFS_MAX_FILE_IO_SIZE)
 		server->rsize = NFS_MAX_FILE_IO_SIZE;
-	server->rpages = (server->rsize + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+	server->rpages = (server->rsize + PAGE_SIZE - 1) >> PAGE_SHIFT;
 
 	server->backing_dev_info.name = "nfs";
 	server->backing_dev_info.ra_pages = server->rpages * NFS_MAX_READAHEAD;
@@ -747,13 +747,13 @@ static void nfs_server_set_fsinfo(struct nfs_server *server,
 		server->wsize = max_rpc_payload;
 	if (server->wsize > NFS_MAX_FILE_IO_SIZE)
 		server->wsize = NFS_MAX_FILE_IO_SIZE;
-	server->wpages = (server->wsize + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+	server->wpages = (server->wsize + PAGE_SIZE - 1) >> PAGE_SHIFT;
 
 	server->wtmult = nfs_block_bits(fsinfo->wtmult, NULL);
 
 	server->dtsize = nfs_block_size(fsinfo->dtpref, NULL);
-	if (server->dtsize > PAGE_CACHE_SIZE * NFS_MAX_READDIR_PAGES)
-		server->dtsize = PAGE_CACHE_SIZE * NFS_MAX_READDIR_PAGES;
+	if (server->dtsize > PAGE_SIZE * NFS_MAX_READDIR_PAGES)
+		server->dtsize = PAGE_SIZE * NFS_MAX_READDIR_PAGES;
 	if (server->dtsize > server->rsize)
 		server->dtsize = server->rsize;
 
diff --git a/fs/nfs/dir.c b/fs/nfs/dir.c
index 770fe7b74079..f66fcb36f5c6 100644
--- a/fs/nfs/dir.c
+++ b/fs/nfs/dir.c
@@ -708,7 +708,7 @@ void cache_page_release(nfs_readdir_descriptor_t *desc)
 {
 	if (!desc->page->mapping)
 		nfs_readdir_clear_array(desc->page);
-	page_cache_release(desc->page);
+	put_page(desc->page);
 	desc->page = NULL;
 }
 
@@ -1935,7 +1935,7 @@ int nfs_symlink(struct inode *dir, struct dentry *dentry, const char *symname)
 		 * add_to_page_cache_lru() grabs an extra page refcount.
 		 * Drop it here to avoid leaking this page later.
 		 */
-		page_cache_release(page);
+		put_page(page);
 	} else
 		__free_page(page);
 
* Unmerged path fs/nfs/direct.c
diff --git a/fs/nfs/file.c b/fs/nfs/file.c
index c497ac465f2a..b4d8e985716c 100644
--- a/fs/nfs/file.c
+++ b/fs/nfs/file.c
@@ -320,7 +320,7 @@ static int nfs_want_read_modify_write(struct file *file, struct page *page,
 			loff_t pos, unsigned len)
 {
 	unsigned int pglen = nfs_page_length(page);
-	unsigned int offset = pos & (PAGE_CACHE_SIZE - 1);
+	unsigned int offset = pos & (PAGE_SIZE - 1);
 	unsigned int end = offset + len;
 
 	if (pnfs_ld_read_whole_page(file->f_mapping->host)) {
@@ -351,7 +351,7 @@ static int nfs_write_begin(struct file *file, struct address_space *mapping,
 			struct page **pagep, void **fsdata)
 {
 	int ret;
-	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
+	pgoff_t index = pos >> PAGE_SHIFT;
 	struct page *page;
 	int once_thru = 0;
 
@@ -380,12 +380,12 @@ start:
 	ret = nfs_flush_incompatible(file, page);
 	if (ret) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	} else if (!once_thru &&
 		   nfs_want_read_modify_write(file, page, pos, len)) {
 		once_thru = 1;
 		ret = nfs_readpage(file, page);
-		page_cache_release(page);
+		put_page(page);
 		if (!ret)
 			goto start;
 	}
@@ -396,7 +396,7 @@ static int nfs_write_end(struct file *file, struct address_space *mapping,
 			loff_t pos, unsigned len, unsigned copied,
 			struct page *page, void *fsdata)
 {
-	unsigned offset = pos & (PAGE_CACHE_SIZE - 1);
+	unsigned offset = pos & (PAGE_SIZE - 1);
 	struct nfs_open_context *ctx = nfs_file_open_context(file);
 	int status;
 
@@ -413,20 +413,20 @@ static int nfs_write_end(struct file *file, struct address_space *mapping,
 
 		if (pglen == 0) {
 			zero_user_segments(page, 0, offset,
-					end, PAGE_CACHE_SIZE);
+					end, PAGE_SIZE);
 			SetPageUptodate(page);
 		} else if (end >= pglen) {
-			zero_user_segment(page, end, PAGE_CACHE_SIZE);
+			zero_user_segment(page, end, PAGE_SIZE);
 			if (offset == 0)
 				SetPageUptodate(page);
 		} else
-			zero_user_segment(page, pglen, PAGE_CACHE_SIZE);
+			zero_user_segment(page, pglen, PAGE_SIZE);
 	}
 
 	status = nfs_updatepage(file, page, offset, copied);
 
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
 	if (status < 0)
 		return status;
@@ -454,7 +454,7 @@ static void nfs_invalidate_page(struct page *page, unsigned int offset,
 	dfprintk(PAGECACHE, "NFS: invalidate_page(%p, %u, %u)\n",
 		 page, offset, length);
 
-	if (offset != 0 || length < PAGE_CACHE_SIZE)
+	if (offset != 0 || length < PAGE_SIZE)
 		return;
 	/* Cancel any unstarted writes on this page */
 	nfs_wb_page_cancel(page_file_mapping(page)->host, page);
diff --git a/fs/nfs/internal.h b/fs/nfs/internal.h
index b446709f1269..ad72778dca56 100644
--- a/fs/nfs/internal.h
+++ b/fs/nfs/internal.h
@@ -654,11 +654,11 @@ unsigned int nfs_page_length(struct page *page)
 
 	if (i_size > 0) {
 		pgoff_t page_index = page_file_index(page);
-		pgoff_t end_index = (i_size - 1) >> PAGE_CACHE_SHIFT;
+		pgoff_t end_index = (i_size - 1) >> PAGE_SHIFT;
 		if (page_index < end_index)
-			return PAGE_CACHE_SIZE;
+			return PAGE_SIZE;
 		if (page_index == end_index)
-			return ((i_size - 1) & ~PAGE_CACHE_MASK) + 1;
+			return ((i_size - 1) & ~PAGE_MASK) + 1;
 	}
 	return 0;
 }
diff --git a/fs/nfs/nfs4xdr.c b/fs/nfs/nfs4xdr.c
index 7148963b6fe3..c6ff7365329f 100644
--- a/fs/nfs/nfs4xdr.c
+++ b/fs/nfs/nfs4xdr.c
@@ -5022,7 +5022,7 @@ static int decode_space_limit(struct xdr_stream *xdr,
 		blocksize = be32_to_cpup(p);
 		maxsize = (uint64_t)nblocks * (uint64_t)blocksize;
 	}
-	maxsize >>= PAGE_CACHE_SHIFT;
+	maxsize >>= PAGE_SHIFT;
 	*pagemod_limit = min_t(u64, maxsize, ULONG_MAX);
 	return 0;
 out_overflow:
diff --git a/fs/nfs/objlayout/objio_osd.c b/fs/nfs/objlayout/objio_osd.c
index 24f9843d9adc..4665d2e2e2ab 100644
--- a/fs/nfs/objlayout/objio_osd.c
+++ b/fs/nfs/objlayout/objio_osd.c
@@ -489,7 +489,7 @@ static void __r4w_put_page(void *priv, struct page *page)
 	dprintk("%s: index=0x%lx\n", __func__,
 		(page == ZERO_PAGE(0)) ? -1UL : page->index);
 	if (ZERO_PAGE(0) != page)
-		page_cache_release(page);
+		put_page(page);
 	return;
 }
 
diff --git a/fs/nfs/pagelist.c b/fs/nfs/pagelist.c
index 959268ff943a..0d0120738d2b 100644
--- a/fs/nfs/pagelist.c
+++ b/fs/nfs/pagelist.c
@@ -377,7 +377,7 @@ nfs_create_request(struct nfs_open_context *ctx, struct page *page,
 	 * update_nfs_request below if the region is not locked. */
 	req->wb_page    = page;
 	req->wb_index	= page_file_index(page);
-	page_cache_get(page);
+	get_page(page);
 	req->wb_offset  = offset;
 	req->wb_pgbase	= offset;
 	req->wb_bytes   = count;
@@ -427,7 +427,7 @@ static void nfs_clear_request(struct nfs_page *req)
 	struct nfs_lock_context *l_ctx = req->wb_lock_context;
 
 	if (page != NULL) {
-		page_cache_release(page);
+		put_page(page);
 		req->wb_page = NULL;
 	}
 	if (l_ctx != NULL) {
@@ -934,7 +934,7 @@ static bool nfs_can_coalesce_requests(struct nfs_page *prev,
 				return false;
 		} else {
 			if (req->wb_pgbase != 0 ||
-			    prev->wb_pgbase + prev->wb_bytes != PAGE_CACHE_SIZE)
+			    prev->wb_pgbase + prev->wb_bytes != PAGE_SIZE)
 				return false;
 		}
 	}
diff --git a/fs/nfs/pnfs.c b/fs/nfs/pnfs.c
index 1e52deed4cab..5e74a396b35c 100644
--- a/fs/nfs/pnfs.c
+++ b/fs/nfs/pnfs.c
@@ -846,7 +846,7 @@ send_layoutget(struct pnfs_layout_hdr *lo,
 
 		i_size = i_size_read(ino);
 
-		lgp->args.minlength = PAGE_CACHE_SIZE;
+		lgp->args.minlength = PAGE_SIZE;
 		if (lgp->args.minlength > range->length)
 			lgp->args.minlength = range->length;
 		if (range->iomode == IOMODE_READ) {
@@ -1631,13 +1631,13 @@ lookup_again:
 		spin_unlock(&clp->cl_lock);
 	}
 
-	pg_offset = arg.offset & ~PAGE_CACHE_MASK;
+	pg_offset = arg.offset & ~PAGE_MASK;
 	if (pg_offset) {
 		arg.offset -= pg_offset;
 		arg.length += pg_offset;
 	}
 	if (arg.length != NFS4_MAX_UINT64)
-		arg.length = PAGE_CACHE_ALIGN(arg.length);
+		arg.length = PAGE_ALIGN(arg.length);
 
 	lseg = send_layoutget(lo, ctx, &arg, gfp_flags);
 	atomic_dec(&lo->plh_outstanding);
diff --git a/fs/nfs/read.c b/fs/nfs/read.c
index 54c427a55527..dc15e1b73443 100644
--- a/fs/nfs/read.c
+++ b/fs/nfs/read.c
@@ -46,7 +46,7 @@ static void nfs_readhdr_free(struct nfs_pgio_header *rhdr)
 static
 int nfs_return_empty_page(struct page *page)
 {
-	zero_user(page, 0, PAGE_CACHE_SIZE);
+	zero_user(page, 0, PAGE_SIZE);
 	SetPageUptodate(page);
 	unlock_page(page);
 	return 0;
@@ -118,8 +118,8 @@ int nfs_readpage_async(struct nfs_open_context *ctx, struct inode *inode,
 		unlock_page(page);
 		return PTR_ERR(new);
 	}
-	if (len < PAGE_CACHE_SIZE)
-		zero_user_segment(page, len, PAGE_CACHE_SIZE);
+	if (len < PAGE_SIZE)
+		zero_user_segment(page, len, PAGE_SIZE);
 
 	nfs_pageio_init_read(&pgio, inode, false,
 			     &nfs_async_read_completion_ops);
@@ -295,7 +295,7 @@ int nfs_readpage(struct file *file, struct page *page)
 	int		error;
 
 	dprintk("NFS: nfs_readpage (%p %ld@%lu)\n",
-		page, PAGE_CACHE_SIZE, page_file_index(page));
+		page, PAGE_SIZE, page_file_index(page));
 	nfs_inc_stats(inode, NFSIOS_VFSREADPAGE);
 	nfs_add_stats(inode, NFSIOS_READPAGES, 1);
 
@@ -361,8 +361,8 @@ readpage_async_filler(void *data, struct page *page)
 	if (IS_ERR(new))
 		goto out_error;
 
-	if (len < PAGE_CACHE_SIZE)
-		zero_user_segment(page, len, PAGE_CACHE_SIZE);
+	if (len < PAGE_SIZE)
+		zero_user_segment(page, len, PAGE_SIZE);
 	if (!nfs_pageio_add_request(desc->pgio, new)) {
 		nfs_list_remove_request(new);
 		nfs_readpage_release(new);
@@ -424,8 +424,8 @@ int nfs_readpages(struct file *filp, struct address_space *mapping,
 
 	pgm = &pgio.pg_mirrors[0];
 	NFS_I(inode)->read_io += pgm->pg_bytes_written;
-	npages = (pgm->pg_bytes_written + PAGE_CACHE_SIZE - 1) >>
-		 PAGE_CACHE_SHIFT;
+	npages = (pgm->pg_bytes_written + PAGE_SIZE - 1) >>
+		 PAGE_SHIFT;
 	nfs_add_stats(inode, NFSIOS_READPAGES, npages);
 read_complete:
 	put_nfs_open_context(desc.ctx);
diff --git a/fs/nfs/write.c b/fs/nfs/write.c
index b67d44d795c2..eaf2756555c2 100644
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@ -148,7 +148,7 @@ static void nfs_grow_file(struct page *page, unsigned int offset, unsigned int c
 
 	spin_lock(&inode->i_lock);
 	i_size = i_size_read(inode);
-	end_index = (i_size - 1) >> PAGE_CACHE_SHIFT;
+	end_index = (i_size - 1) >> PAGE_SHIFT;
 	if (i_size > 0 && page_file_index(page) < end_index)
 		goto out;
 	end = page_file_offset(page) + ((loff_t)offset+count);
@@ -1926,7 +1926,7 @@ int nfs_wb_page_cancel(struct inode *inode, struct page *page)
 int nfs_wb_single_page(struct inode *inode, struct page *page, bool launder)
 {
 	loff_t range_start = page_file_offset(page);
-	loff_t range_end = range_start + (loff_t)(PAGE_CACHE_SIZE - 1);
+	loff_t range_end = range_start + (loff_t)(PAGE_SIZE - 1);
 	struct writeback_control wbc = {
 		.sync_mode = WB_SYNC_ALL,
 		.nr_to_write = 0,
diff --git a/fs/nilfs2/bmap.c b/fs/nilfs2/bmap.c
index aadbd0b5e3e8..eebcebce4b39 100644
--- a/fs/nilfs2/bmap.c
+++ b/fs/nilfs2/bmap.c
@@ -432,7 +432,7 @@ __u64 nilfs_bmap_data_get_key(const struct nilfs_bmap *bmap,
 	struct buffer_head *pbh;
 	__u64 key;
 
-	key = page_index(bh->b_page) << (PAGE_CACHE_SHIFT -
+	key = page_index(bh->b_page) << (PAGE_SHIFT -
 					 bmap->b_inode->i_blkbits);
 	for (pbh = page_buffers(bh->b_page); pbh != bh; pbh = pbh->b_this_page)
 		key++;
diff --git a/fs/nilfs2/btnode.c b/fs/nilfs2/btnode.c
index a35ae35e6932..e0c9daf9aa22 100644
--- a/fs/nilfs2/btnode.c
+++ b/fs/nilfs2/btnode.c
@@ -62,7 +62,7 @@ nilfs_btnode_create_block(struct address_space *btnc, __u64 blocknr)
 	set_buffer_uptodate(bh);
 
 	unlock_page(bh->b_page);
-	page_cache_release(bh->b_page);
+	put_page(bh->b_page);
 	return bh;
 }
 
@@ -128,7 +128,7 @@ found:
 
 out_locked:
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return err;
 }
 
@@ -146,7 +146,7 @@ void nilfs_btnode_delete(struct buffer_head *bh)
 	pgoff_t index = page_index(page);
 	int still_dirty;
 
-	page_cache_get(page);
+	get_page(page);
 	lock_page(page);
 	wait_on_page_writeback(page);
 
@@ -154,7 +154,7 @@ void nilfs_btnode_delete(struct buffer_head *bh)
 	still_dirty = PageDirty(page);
 	mapping = page->mapping;
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
 	if (!still_dirty && mapping)
 		invalidate_inode_pages2_range(mapping, index, index);
@@ -181,7 +181,7 @@ int nilfs_btnode_prepare_change_key(struct address_space *btnc,
 	obh = ctxt->bh;
 	ctxt->newbh = NULL;
 
-	if (inode->i_blkbits == PAGE_CACHE_SHIFT) {
+	if (inode->i_blkbits == PAGE_SHIFT) {
 		lock_page(obh->b_page);
 		/*
 		 * We cannot call radix_tree_preload for the kernels older
* Unmerged path fs/nilfs2/dir.c
diff --git a/fs/nilfs2/gcinode.c b/fs/nilfs2/gcinode.c
index 57ceaf33d177..42c2944a0652 100644
--- a/fs/nilfs2/gcinode.c
+++ b/fs/nilfs2/gcinode.c
@@ -115,7 +115,7 @@ int nilfs_gccache_submit_read_data(struct inode *inode, sector_t blkoff,
 
  failed:
 	unlock_page(bh->b_page);
-	page_cache_release(bh->b_page);
+	put_page(bh->b_page);
 	return err;
 }
 
* Unmerged path fs/nilfs2/inode.c
diff --git a/fs/nilfs2/mdt.c b/fs/nilfs2/mdt.c
index c4dcd1db57ee..679686c049c0 100644
--- a/fs/nilfs2/mdt.c
+++ b/fs/nilfs2/mdt.c
@@ -106,7 +106,7 @@ static int nilfs_mdt_create_block(struct inode *inode, unsigned long block,
 
  failed_bh:
 	unlock_page(bh->b_page);
-	page_cache_release(bh->b_page);
+	put_page(bh->b_page);
 	brelse(bh);
 
  failed_unlock:
@@ -164,7 +164,7 @@ nilfs_mdt_submit_block(struct inode *inode, unsigned long blkoff,
 
  failed_bh:
 	unlock_page(bh->b_page);
-	page_cache_release(bh->b_page);
+	put_page(bh->b_page);
 	brelse(bh);
  failed:
 	return ret;
@@ -303,7 +303,7 @@ int nilfs_mdt_delete_block(struct inode *inode, unsigned long block)
 int nilfs_mdt_forget_block(struct inode *inode, unsigned long block)
 {
 	pgoff_t index = (pgoff_t)block >>
-		(PAGE_CACHE_SHIFT - inode->i_blkbits);
+		(PAGE_SHIFT - inode->i_blkbits);
 	struct page *page;
 	unsigned long first_block;
 	int ret = 0;
@@ -316,7 +316,7 @@ int nilfs_mdt_forget_block(struct inode *inode, unsigned long block)
 	wait_on_page_writeback(page);
 
 	first_block = (unsigned long)index <<
-		(PAGE_CACHE_SHIFT - inode->i_blkbits);
+		(PAGE_SHIFT - inode->i_blkbits);
 	if (page_has_buffers(page)) {
 		struct buffer_head *bh;
 
@@ -325,7 +325,7 @@ int nilfs_mdt_forget_block(struct inode *inode, unsigned long block)
 	}
 	still_dirty = PageDirty(page);
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
 	if (still_dirty ||
 	    invalidate_inode_pages2_range(inode->i_mapping, index, index) != 0)
@@ -520,7 +520,7 @@ int nilfs_mdt_freeze_buffer(struct inode *inode, struct buffer_head *bh)
 	}
 
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return 0;
 }
 
@@ -539,7 +539,7 @@ nilfs_mdt_get_frozen_buffer(struct inode *inode, struct buffer_head *bh)
 			bh_frozen = nilfs_page_get_nth_block(page, n);
 		}
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 	return bh_frozen;
 }
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 9de78f08989e..f51ec24f9211 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -424,11 +424,11 @@ static int nilfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 out_dir:
 	if (dir_de) {
 		kunmap(dir_page);
-		page_cache_release(dir_page);
+		put_page(dir_page);
 	}
 out_old:
 	kunmap(old_page);
-	page_cache_release(old_page);
+	put_page(old_page);
 out:
 	nilfs_transaction_abort(old_dir->i_sb);
 	return err;
diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index ec7d52bfb4b6..4c3c9be0e36e 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -50,7 +50,7 @@ __nilfs_get_page_block(struct page *page, unsigned long block, pgoff_t index,
 	if (!page_has_buffers(page))
 		create_empty_buffers(page, 1 << blkbits, b_state);
 
-	first_block = (unsigned long)index << (PAGE_CACHE_SHIFT - blkbits);
+	first_block = (unsigned long)index << (PAGE_SHIFT - blkbits);
 	bh = nilfs_page_get_nth_block(page, block - first_block);
 
 	touch_buffer(bh);
@@ -64,7 +64,7 @@ struct buffer_head *nilfs_grab_buffer(struct inode *inode,
 				      unsigned long b_state)
 {
 	int blkbits = inode->i_blkbits;
-	pgoff_t index = blkoff >> (PAGE_CACHE_SHIFT - blkbits);
+	pgoff_t index = blkoff >> (PAGE_SHIFT - blkbits);
 	struct page *page;
 	struct buffer_head *bh;
 
@@ -75,7 +75,7 @@ struct buffer_head *nilfs_grab_buffer(struct inode *inode,
 	bh = __nilfs_get_page_block(page, blkoff, index, blkbits, b_state);
 	if (unlikely(!bh)) {
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		return NULL;
 	}
 	return bh;
@@ -290,7 +290,7 @@ repeat:
 		__set_page_dirty_nobuffers(dpage);
 
 		unlock_page(dpage);
-		page_cache_release(dpage);
+		put_page(dpage);
 		unlock_page(page);
 	}
 	pagevec_release(&pvec);
@@ -335,7 +335,7 @@ repeat:
 			WARN_ON(PageDirty(dpage));
 			nilfs_copy_page(dpage, page, 0);
 			unlock_page(dpage);
-			page_cache_release(dpage);
+			put_page(dpage);
 		} else {
 			struct page *page2;
 
@@ -352,7 +352,7 @@ repeat:
 			if (unlikely(err < 0)) {
 				WARN_ON(err == -EEXIST);
 				page->mapping = NULL;
-				page_cache_release(page); /* for cache */
+				put_page(page); /* for cache */
 			} else {
 				page->mapping = dmap;
 				dmap->nrpages++;
@@ -529,8 +529,8 @@ unsigned long nilfs_find_uncommitted_extent(struct inode *inode,
 	if (inode->i_mapping->nrpages == 0)
 		return 0;
 
-	index = start_blk >> (PAGE_CACHE_SHIFT - inode->i_blkbits);
-	nblocks_in_page = 1U << (PAGE_CACHE_SHIFT - inode->i_blkbits);
+	index = start_blk >> (PAGE_SHIFT - inode->i_blkbits);
+	nblocks_in_page = 1U << (PAGE_SHIFT - inode->i_blkbits);
 
 	pagevec_init(&pvec, 0);
 
@@ -543,7 +543,7 @@ repeat:
 	if (length > 0 && pvec.pages[0]->index > index)
 		goto out;
 
-	b = pvec.pages[0]->index << (PAGE_CACHE_SHIFT - inode->i_blkbits);
+	b = pvec.pages[0]->index << (PAGE_SHIFT - inode->i_blkbits);
 	i = 0;
 	do {
 		page = pvec.pages[i];
diff --git a/fs/nilfs2/recovery.c b/fs/nilfs2/recovery.c
index ff00a0b7acb9..c54bab3e1b32 100644
--- a/fs/nilfs2/recovery.c
+++ b/fs/nilfs2/recovery.c
@@ -544,14 +544,14 @@ static int nilfs_recover_dsync_blocks(struct the_nilfs *nilfs,
 				blocksize, page, NULL);
 
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 
 		(*nr_salvaged_blocks)++;
 		goto next;
 
  failed_page:
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 
  failed_inode:
 		printk(KERN_WARNING
* Unmerged path fs/nilfs2/segment.c
* Unmerged path fs/ntfs/aops.c
diff --git a/fs/ntfs/aops.h b/fs/ntfs/aops.h
index caecc58f529c..37cd7e45dcbc 100644
--- a/fs/ntfs/aops.h
+++ b/fs/ntfs/aops.h
@@ -40,7 +40,7 @@
 static inline void ntfs_unmap_page(struct page *page)
 {
 	kunmap(page);
-	page_cache_release(page);
+	put_page(page);
 }
 
 /**
* Unmerged path fs/ntfs/attrib.c
diff --git a/fs/ntfs/bitmap.c b/fs/ntfs/bitmap.c
index 0809cf876098..ec130c588d2b 100644
--- a/fs/ntfs/bitmap.c
+++ b/fs/ntfs/bitmap.c
@@ -67,8 +67,8 @@ int __ntfs_bitmap_set_bits_in_run(struct inode *vi, const s64 start_bit,
 	 * Calculate the indices for the pages containing the first and last
 	 * bits, i.e. @start_bit and @start_bit + @cnt - 1, respectively.
 	 */
-	index = start_bit >> (3 + PAGE_CACHE_SHIFT);
-	end_index = (start_bit + cnt - 1) >> (3 + PAGE_CACHE_SHIFT);
+	index = start_bit >> (3 + PAGE_SHIFT);
+	end_index = (start_bit + cnt - 1) >> (3 + PAGE_SHIFT);
 
 	/* Get the page containing the first bit (@start_bit). */
 	mapping = vi->i_mapping;
@@ -82,7 +82,7 @@ int __ntfs_bitmap_set_bits_in_run(struct inode *vi, const s64 start_bit,
 	kaddr = page_address(page);
 
 	/* Set @pos to the position of the byte containing @start_bit. */
-	pos = (start_bit >> 3) & ~PAGE_CACHE_MASK;
+	pos = (start_bit >> 3) & ~PAGE_MASK;
 
 	/* Calculate the position of @start_bit in the first byte. */
 	bit = start_bit & 7;
@@ -108,7 +108,7 @@ int __ntfs_bitmap_set_bits_in_run(struct inode *vi, const s64 start_bit,
 	 * Depending on @value, modify all remaining whole bytes in the page up
 	 * to @cnt.
 	 */
-	len = min_t(s64, cnt >> 3, PAGE_CACHE_SIZE - pos);
+	len = min_t(s64, cnt >> 3, PAGE_SIZE - pos);
 	memset(kaddr + pos, value ? 0xff : 0, len);
 	cnt -= len << 3;
 
@@ -132,7 +132,7 @@ int __ntfs_bitmap_set_bits_in_run(struct inode *vi, const s64 start_bit,
 		 * Depending on @value, modify all remaining whole bytes in the
 		 * page up to @cnt.
 		 */
-		len = min_t(s64, cnt >> 3, PAGE_CACHE_SIZE);
+		len = min_t(s64, cnt >> 3, PAGE_SIZE);
 		memset(kaddr, value ? 0xff : 0, len);
 		cnt -= len << 3;
 	}
diff --git a/fs/ntfs/compress.c b/fs/ntfs/compress.c
index ee4144ce5d7c..679c231b8467 100644
--- a/fs/ntfs/compress.c
+++ b/fs/ntfs/compress.c
@@ -104,7 +104,7 @@ static void zero_partial_compressed_page(struct page *page,
 	unsigned int kp_ofs;
 
 	ntfs_debug("Zeroing page region outside initialized size.");
-	if (((s64)page->index << PAGE_CACHE_SHIFT) >= initialized_size) {
+	if (((s64)page->index << PAGE_SHIFT) >= initialized_size) {
 		/*
 		 * FIXME: Using clear_page() will become wrong when we get
 		 * PAGE_CACHE_SIZE != PAGE_SIZE but for now there is no problem.
@@ -112,8 +112,8 @@ static void zero_partial_compressed_page(struct page *page,
 		clear_page(kp);
 		return;
 	}
-	kp_ofs = initialized_size & ~PAGE_CACHE_MASK;
-	memset(kp + kp_ofs, 0, PAGE_CACHE_SIZE - kp_ofs);
+	kp_ofs = initialized_size & ~PAGE_MASK;
+	memset(kp + kp_ofs, 0, PAGE_SIZE - kp_ofs);
 	return;
 }
 
@@ -123,7 +123,7 @@ static void zero_partial_compressed_page(struct page *page,
 static inline void handle_bounds_compressed_page(struct page *page,
 		const loff_t i_size, const s64 initialized_size)
 {
-	if ((page->index >= (initialized_size >> PAGE_CACHE_SHIFT)) &&
+	if ((page->index >= (initialized_size >> PAGE_SHIFT)) &&
 			(initialized_size < i_size))
 		zero_partial_compressed_page(page, initialized_size);
 	return;
@@ -241,7 +241,7 @@ return_error:
 				if (di == xpage)
 					*xpage_done = 1;
 				else
-					page_cache_release(dp);
+					put_page(dp);
 				dest_pages[di] = NULL;
 			}
 		}
@@ -274,7 +274,7 @@ return_error:
 		cb = cb_sb_end;
 
 		/* Advance destination position to next sub-block. */
-		*dest_ofs = (*dest_ofs + NTFS_SB_SIZE) & ~PAGE_CACHE_MASK;
+		*dest_ofs = (*dest_ofs + NTFS_SB_SIZE) & ~PAGE_MASK;
 		if (!*dest_ofs && (++*dest_index > dest_max_index))
 			goto return_overflow;
 		goto do_next_sb;
@@ -301,7 +301,7 @@ return_error:
 
 		/* Advance destination position to next sub-block. */
 		*dest_ofs += NTFS_SB_SIZE;
-		if (!(*dest_ofs &= ~PAGE_CACHE_MASK)) {
+		if (!(*dest_ofs &= ~PAGE_MASK)) {
 finalize_page:
 			/*
 			 * First stage: add current page index to array of
@@ -335,7 +335,7 @@ do_next_tag:
 			*dest_ofs += nr_bytes;
 		}
 		/* We have finished the current sub-block. */
-		if (!(*dest_ofs &= ~PAGE_CACHE_MASK))
+		if (!(*dest_ofs &= ~PAGE_MASK))
 			goto finalize_page;
 		goto do_next_sb;
 	}
@@ -498,13 +498,13 @@ int ntfs_read_compressed_block(struct page *page)
 	VCN vcn;
 	LCN lcn;
 	/* The first wanted vcn (minimum alignment is PAGE_CACHE_SIZE). */
-	VCN start_vcn = (((s64)index << PAGE_CACHE_SHIFT) & ~cb_size_mask) >>
+	VCN start_vcn = (((s64)index << PAGE_SHIFT) & ~cb_size_mask) >>
 			vol->cluster_size_bits;
 	/*
 	 * The first vcn after the last wanted vcn (minimum alignment is again
 	 * PAGE_CACHE_SIZE.
 	 */
-	VCN end_vcn = ((((s64)(index + 1UL) << PAGE_CACHE_SHIFT) + cb_size - 1)
+	VCN end_vcn = ((((s64)(index + 1UL) << PAGE_SHIFT) + cb_size - 1)
 			& ~cb_size_mask) >> vol->cluster_size_bits;
 	/* Number of compression blocks (cbs) in the wanted vcn range. */
 	unsigned int nr_cbs = (end_vcn - start_vcn) << vol->cluster_size_bits
@@ -515,7 +515,7 @@ int ntfs_read_compressed_block(struct page *page)
 	 * guarantees of start_vcn and end_vcn, no need to round up here.
 	 */
 	unsigned int nr_pages = (end_vcn - start_vcn) <<
-			vol->cluster_size_bits >> PAGE_CACHE_SHIFT;
+			vol->cluster_size_bits >> PAGE_SHIFT;
 	unsigned int xpage, max_page, cur_page, cur_ofs, i;
 	unsigned int cb_clusters, cb_max_ofs;
 	int block, max_block, cb_max_page, bhs_size, nr_bhs, err = 0;
@@ -549,7 +549,7 @@ int ntfs_read_compressed_block(struct page *page)
 	 * We have already been given one page, this is the one we must do.
 	 * Once again, the alignment guarantees keep it simple.
 	 */
-	offset = start_vcn << vol->cluster_size_bits >> PAGE_CACHE_SHIFT;
+	offset = start_vcn << vol->cluster_size_bits >> PAGE_SHIFT;
 	xpage = index - offset;
 	pages[xpage] = page;
 	/*
@@ -560,13 +560,13 @@ int ntfs_read_compressed_block(struct page *page)
 	i_size = i_size_read(VFS_I(ni));
 	initialized_size = ni->initialized_size;
 	read_unlock_irqrestore(&ni->size_lock, flags);
-	max_page = ((i_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT) -
+	max_page = ((i_size + PAGE_SIZE - 1) >> PAGE_SHIFT) -
 			offset;
 	/* Is the page fully outside i_size? (truncate in progress) */
 	if (xpage >= max_page) {
 		kfree(bhs);
 		kfree(pages);
-		zero_user(page, 0, PAGE_CACHE_SIZE);
+		zero_user(page, 0, PAGE_SIZE);
 		ntfs_debug("Compressed read outside i_size - truncated?");
 		SetPageUptodate(page);
 		unlock_page(page);
@@ -591,7 +591,7 @@ int ntfs_read_compressed_block(struct page *page)
 				continue;
 			}
 			unlock_page(page);
-			page_cache_release(page);
+			put_page(page);
 			pages[i] = NULL;
 		}
 	}
@@ -735,9 +735,9 @@ lock_retry_remap:
 	ntfs_debug("Successfully read the compression block.");
 
 	/* The last page and maximum offset within it for the current cb. */
-	cb_max_page = (cur_page << PAGE_CACHE_SHIFT) + cur_ofs + cb_size;
-	cb_max_ofs = cb_max_page & ~PAGE_CACHE_MASK;
-	cb_max_page >>= PAGE_CACHE_SHIFT;
+	cb_max_page = (cur_page << PAGE_SHIFT) + cur_ofs + cb_size;
+	cb_max_ofs = cb_max_page & ~PAGE_MASK;
+	cb_max_page >>= PAGE_SHIFT;
 
 	/* Catch end of file inside a compression block. */
 	if (cb_max_page > max_page)
@@ -762,7 +762,7 @@ lock_retry_remap:
 					clear_page(page_address(page));
 				else
 					memset(page_address(page) + cur_ofs, 0,
-							PAGE_CACHE_SIZE -
+							PAGE_SIZE -
 							cur_ofs);
 				flush_dcache_page(page);
 				kunmap(page);
@@ -771,10 +771,10 @@ lock_retry_remap:
 				if (cur_page == xpage)
 					xpage_done = 1;
 				else
-					page_cache_release(page);
+					put_page(page);
 				pages[cur_page] = NULL;
 			}
-			cb_pos += PAGE_CACHE_SIZE - cur_ofs;
+			cb_pos += PAGE_SIZE - cur_ofs;
 			cur_ofs = 0;
 			if (cb_pos >= cb_end)
 				break;
@@ -816,8 +816,8 @@ lock_retry_remap:
 			page = pages[cur_page];
 			if (page)
 				memcpy(page_address(page) + cur_ofs, cb_pos,
-						PAGE_CACHE_SIZE - cur_ofs);
-			cb_pos += PAGE_CACHE_SIZE - cur_ofs;
+						PAGE_SIZE - cur_ofs);
+			cb_pos += PAGE_SIZE - cur_ofs;
 			cur_ofs = 0;
 			if (cb_pos >= cb_end)
 				break;
@@ -850,10 +850,10 @@ lock_retry_remap:
 				if (cur2_page == xpage)
 					xpage_done = 1;
 				else
-					page_cache_release(page);
+					put_page(page);
 				pages[cur2_page] = NULL;
 			}
-			cb_pos2 += PAGE_CACHE_SIZE - cur_ofs2;
+			cb_pos2 += PAGE_SIZE - cur_ofs2;
 			cur_ofs2 = 0;
 			if (cb_pos2 >= cb_end)
 				break;
@@ -884,7 +884,7 @@ lock_retry_remap:
 					kunmap(page);
 					unlock_page(page);
 					if (prev_cur_page != xpage)
-						page_cache_release(page);
+						put_page(page);
 					pages[prev_cur_page] = NULL;
 				}
 			}
@@ -914,7 +914,7 @@ lock_retry_remap:
 			kunmap(page);
 			unlock_page(page);
 			if (cur_page != xpage)
-				page_cache_release(page);
+				put_page(page);
 			pages[cur_page] = NULL;
 		}
 	}
@@ -961,7 +961,7 @@ err_out:
 			kunmap(page);
 			unlock_page(page);
 			if (i != xpage)
-				page_cache_release(page);
+				put_page(page);
 		}
 	}
 	kfree(pages);
diff --git a/fs/ntfs/dir.c b/fs/ntfs/dir.c
index aa411c3f20e9..2c6bf61b75c6 100644
--- a/fs/ntfs/dir.c
+++ b/fs/ntfs/dir.c
@@ -319,7 +319,7 @@ descend_into_child_node:
 	 * disk if necessary.
 	 */
 	page = ntfs_map_page(ia_mapping, vcn <<
-			dir_ni->itype.index.vcn_size_bits >> PAGE_CACHE_SHIFT);
+			dir_ni->itype.index.vcn_size_bits >> PAGE_SHIFT);
 	if (IS_ERR(page)) {
 		ntfs_error(sb, "Failed to map directory index page, error %ld.",
 				-PTR_ERR(page));
@@ -331,9 +331,9 @@ descend_into_child_node:
 fast_descend_into_child_node:
 	/* Get to the index allocation block. */
 	ia = (INDEX_ALLOCATION*)(kaddr + ((vcn <<
-			dir_ni->itype.index.vcn_size_bits) & ~PAGE_CACHE_MASK));
+			dir_ni->itype.index.vcn_size_bits) & ~PAGE_MASK));
 	/* Bounds checks. */
-	if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) {
+	if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_SIZE) {
 		ntfs_error(sb, "Out of bounds check failed. Corrupt directory "
 				"inode 0x%lx or driver bug.", dir_ni->mft_no);
 		goto unm_err_out;
@@ -366,7 +366,7 @@ fast_descend_into_child_node:
 		goto unm_err_out;
 	}
 	index_end = (u8*)ia + dir_ni->itype.index.block_size;
-	if (index_end > kaddr + PAGE_CACHE_SIZE) {
+	if (index_end > kaddr + PAGE_SIZE) {
 		ntfs_error(sb, "Index buffer (VCN 0x%llx) of directory inode "
 				"0x%lx crosses page boundary. Impossible! "
 				"Cannot access! This is probably a bug in the "
@@ -559,9 +559,9 @@ found_it2:
 			/* If vcn is in the same page cache page as old_vcn we
 			 * recycle the mapped page. */
 			if (old_vcn << vol->cluster_size_bits >>
-					PAGE_CACHE_SHIFT == vcn <<
+					PAGE_SHIFT == vcn <<
 					vol->cluster_size_bits >>
-					PAGE_CACHE_SHIFT)
+					PAGE_SHIFT)
 				goto fast_descend_into_child_node;
 			unlock_page(page);
 			ntfs_unmap_page(page);
@@ -1265,15 +1265,15 @@ skip_index_root:
 		goto iput_err_out;
 	}
 	/* Get the starting bit position in the current bitmap page. */
-	cur_bmp_pos = bmp_pos & ((PAGE_CACHE_SIZE * 8) - 1);
-	bmp_pos &= ~(u64)((PAGE_CACHE_SIZE * 8) - 1);
+	cur_bmp_pos = bmp_pos & ((PAGE_SIZE * 8) - 1);
+	bmp_pos &= ~(u64)((PAGE_SIZE * 8) - 1);
 get_next_bmp_page:
 	ntfs_debug("Reading bitmap with page index 0x%llx, bit ofs 0x%llx",
-			(unsigned long long)bmp_pos >> (3 + PAGE_CACHE_SHIFT),
+			(unsigned long long)bmp_pos >> (3 + PAGE_SHIFT),
 			(unsigned long long)bmp_pos &
-			(unsigned long long)((PAGE_CACHE_SIZE * 8) - 1));
+			(unsigned long long)((PAGE_SIZE * 8) - 1));
 	bmp_page = ntfs_map_page(bmp_mapping,
-			bmp_pos >> (3 + PAGE_CACHE_SHIFT));
+			bmp_pos >> (3 + PAGE_SHIFT));
 	if (IS_ERR(bmp_page)) {
 		ntfs_error(sb, "Reading index bitmap failed.");
 		err = PTR_ERR(bmp_page);
@@ -1289,9 +1289,9 @@ find_next_index_buffer:
 		 * If we have reached the end of the bitmap page, get the next
 		 * page, and put away the old one.
 		 */
-		if (unlikely((cur_bmp_pos >> 3) >= PAGE_CACHE_SIZE)) {
+		if (unlikely((cur_bmp_pos >> 3) >= PAGE_SIZE)) {
 			ntfs_unmap_page(bmp_page);
-			bmp_pos += PAGE_CACHE_SIZE * 8;
+			bmp_pos += PAGE_SIZE * 8;
 			cur_bmp_pos = 0;
 			goto get_next_bmp_page;
 		}
@@ -1304,8 +1304,8 @@ find_next_index_buffer:
 	ntfs_debug("Handling index buffer 0x%llx.",
 			(unsigned long long)bmp_pos + cur_bmp_pos);
 	/* If the current index buffer is in the same page we reuse the page. */
-	if ((prev_ia_pos & (s64)PAGE_CACHE_MASK) !=
-			(ia_pos & (s64)PAGE_CACHE_MASK)) {
+	if ((prev_ia_pos & (s64)PAGE_MASK) !=
+			(ia_pos & (s64)PAGE_MASK)) {
 		prev_ia_pos = ia_pos;
 		if (likely(ia_page != NULL)) {
 			unlock_page(ia_page);
@@ -1315,7 +1315,7 @@ find_next_index_buffer:
 		 * Map the page cache page containing the current ia_pos,
 		 * reading it from disk if necessary.
 		 */
-		ia_page = ntfs_map_page(ia_mapping, ia_pos >> PAGE_CACHE_SHIFT);
+		ia_page = ntfs_map_page(ia_mapping, ia_pos >> PAGE_SHIFT);
 		if (IS_ERR(ia_page)) {
 			ntfs_error(sb, "Reading index allocation data failed.");
 			err = PTR_ERR(ia_page);
@@ -1326,10 +1326,10 @@ find_next_index_buffer:
 		kaddr = (u8*)page_address(ia_page);
 	}
 	/* Get the current index buffer. */
-	ia = (INDEX_ALLOCATION*)(kaddr + (ia_pos & ~PAGE_CACHE_MASK &
-			~(s64)(ndir->itype.index.block_size - 1)));
+	ia = (INDEX_ALLOCATION*)(kaddr + (ia_pos & ~PAGE_MASK &
+					  ~(s64)(ndir->itype.index.block_size - 1)));
 	/* Bounds checks. */
-	if (unlikely((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE)) {
+	if (unlikely((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_SIZE)) {
 		ntfs_error(sb, "Out of bounds check failed. Corrupt directory "
 				"inode 0x%lx or driver bug.", vdir->i_ino);
 		goto err_out;
@@ -1367,7 +1367,7 @@ find_next_index_buffer:
 		goto err_out;
 	}
 	index_end = (u8*)ia + ndir->itype.index.block_size;
-	if (unlikely(index_end > kaddr + PAGE_CACHE_SIZE)) {
+	if (unlikely(index_end > kaddr + PAGE_SIZE)) {
 		ntfs_error(sb, "Index buffer (VCN 0x%llx) of directory inode "
 				"0x%lx crosses page boundary. Impossible! "
 				"Cannot access! This is probably a bug in the "
* Unmerged path fs/ntfs/file.c
diff --git a/fs/ntfs/index.c b/fs/ntfs/index.c
index 096c135691ae..02a83a46ead2 100644
--- a/fs/ntfs/index.c
+++ b/fs/ntfs/index.c
@@ -276,7 +276,7 @@ descend_into_child_node:
 	 * disk if necessary.
 	 */
 	page = ntfs_map_page(ia_mapping, vcn <<
-			idx_ni->itype.index.vcn_size_bits >> PAGE_CACHE_SHIFT);
+			idx_ni->itype.index.vcn_size_bits >> PAGE_SHIFT);
 	if (IS_ERR(page)) {
 		ntfs_error(sb, "Failed to map index page, error %ld.",
 				-PTR_ERR(page));
@@ -288,9 +288,9 @@ descend_into_child_node:
 fast_descend_into_child_node:
 	/* Get to the index allocation block. */
 	ia = (INDEX_ALLOCATION*)(kaddr + ((vcn <<
-			idx_ni->itype.index.vcn_size_bits) & ~PAGE_CACHE_MASK));
+			idx_ni->itype.index.vcn_size_bits) & ~PAGE_MASK));
 	/* Bounds checks. */
-	if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_CACHE_SIZE) {
+	if ((u8*)ia < kaddr || (u8*)ia > kaddr + PAGE_SIZE) {
 		ntfs_error(sb, "Out of bounds check failed.  Corrupt inode "
 				"0x%lx or driver bug.", idx_ni->mft_no);
 		goto unm_err_out;
@@ -323,7 +323,7 @@ fast_descend_into_child_node:
 		goto unm_err_out;
 	}
 	index_end = (u8*)ia + idx_ni->itype.index.block_size;
-	if (index_end > kaddr + PAGE_CACHE_SIZE) {
+	if (index_end > kaddr + PAGE_SIZE) {
 		ntfs_error(sb, "Index buffer (VCN 0x%llx) of inode 0x%lx "
 				"crosses page boundary.  Impossible!  Cannot "
 				"access!  This is probably a bug in the "
@@ -427,9 +427,9 @@ ia_done:
 		 * the mapped page.
 		 */
 		if (old_vcn << vol->cluster_size_bits >>
-				PAGE_CACHE_SHIFT == vcn <<
+				PAGE_SHIFT == vcn <<
 				vol->cluster_size_bits >>
-				PAGE_CACHE_SHIFT)
+				PAGE_SHIFT)
 			goto fast_descend_into_child_node;
 		unlock_page(page);
 		ntfs_unmap_page(page);
diff --git a/fs/ntfs/inode.c b/fs/ntfs/inode.c
index bd50adc1e6a7..e0f08b361d89 100644
--- a/fs/ntfs/inode.c
+++ b/fs/ntfs/inode.c
@@ -869,12 +869,12 @@ skip_attr_list_load:
 					ni->itype.index.block_size);
 			goto unm_err_out;
 		}
-		if (ni->itype.index.block_size > PAGE_CACHE_SIZE) {
+		if (ni->itype.index.block_size > PAGE_SIZE) {
 			ntfs_error(vi->i_sb, "Index block size (%u) > "
 					"PAGE_CACHE_SIZE (%ld) is not "
 					"supported.  Sorry.",
 					ni->itype.index.block_size,
-					PAGE_CACHE_SIZE);
+					PAGE_SIZE);
 			err = -EOPNOTSUPP;
 			goto unm_err_out;
 		}
@@ -1583,10 +1583,10 @@ static int ntfs_read_locked_index_inode(struct inode *base_vi, struct inode *vi)
 				"two.", ni->itype.index.block_size);
 		goto unm_err_out;
 	}
-	if (ni->itype.index.block_size > PAGE_CACHE_SIZE) {
+	if (ni->itype.index.block_size > PAGE_SIZE) {
 		ntfs_error(vi->i_sb, "Index block size (%u) > PAGE_CACHE_SIZE "
 				"(%ld) is not supported.  Sorry.",
-				ni->itype.index.block_size, PAGE_CACHE_SIZE);
+				ni->itype.index.block_size, PAGE_SIZE);
 		err = -EOPNOTSUPP;
 		goto unm_err_out;
 	}
diff --git a/fs/ntfs/lcnalloc.c b/fs/ntfs/lcnalloc.c
index 1711b710b641..27a24a42f712 100644
--- a/fs/ntfs/lcnalloc.c
+++ b/fs/ntfs/lcnalloc.c
@@ -283,15 +283,15 @@ runlist_element *ntfs_cluster_alloc(ntfs_volume *vol, const VCN start_vcn,
 			ntfs_unmap_page(page);
 		}
 		page = ntfs_map_page(mapping, last_read_pos >>
-				PAGE_CACHE_SHIFT);
+				PAGE_SHIFT);
 		if (IS_ERR(page)) {
 			err = PTR_ERR(page);
 			ntfs_error(vol->sb, "Failed to map page.");
 			goto out;
 		}
-		buf_size = last_read_pos & ~PAGE_CACHE_MASK;
+		buf_size = last_read_pos & ~PAGE_MASK;
 		buf = page_address(page) + buf_size;
-		buf_size = PAGE_CACHE_SIZE - buf_size;
+		buf_size = PAGE_SIZE - buf_size;
 		if (unlikely(last_read_pos + buf_size > i_size))
 			buf_size = i_size - last_read_pos;
 		buf_size <<= 3;
diff --git a/fs/ntfs/logfile.c b/fs/ntfs/logfile.c
index c71de292c5ad..9d71213ca81e 100644
--- a/fs/ntfs/logfile.c
+++ b/fs/ntfs/logfile.c
@@ -381,7 +381,7 @@ static int ntfs_check_and_load_restart_page(struct inode *vi,
 	 * completely inside @rp, just copy it from there.  Otherwise map all
 	 * the required pages and copy the data from them.
 	 */
-	size = PAGE_CACHE_SIZE - (pos & ~PAGE_CACHE_MASK);
+	size = PAGE_SIZE - (pos & ~PAGE_MASK);
 	if (size >= le32_to_cpu(rp->system_page_size)) {
 		memcpy(trp, rp, le32_to_cpu(rp->system_page_size));
 	} else {
@@ -394,8 +394,8 @@ static int ntfs_check_and_load_restart_page(struct inode *vi,
 		/* Copy the remaining data one page at a time. */
 		have_read = size;
 		to_read = le32_to_cpu(rp->system_page_size) - size;
-		idx = (pos + size) >> PAGE_CACHE_SHIFT;
-		BUG_ON((pos + size) & ~PAGE_CACHE_MASK);
+		idx = (pos + size) >> PAGE_SHIFT;
+		BUG_ON((pos + size) & ~PAGE_MASK);
 		do {
 			page = ntfs_map_page(vi->i_mapping, idx);
 			if (IS_ERR(page)) {
@@ -406,7 +406,7 @@ static int ntfs_check_and_load_restart_page(struct inode *vi,
 					err = -EIO;
 				goto err_out;
 			}
-			size = min_t(int, to_read, PAGE_CACHE_SIZE);
+			size = min_t(int, to_read, PAGE_SIZE);
 			memcpy((u8*)trp + have_read, page_address(page), size);
 			ntfs_unmap_page(page);
 			have_read += size;
@@ -509,11 +509,11 @@ bool ntfs_check_logfile(struct inode *log_vi, RESTART_PAGE_HEADER **rp)
 	 * log page size if the page cache size is between the default log page
 	 * size and twice that.
 	 */
-	if (PAGE_CACHE_SIZE >= DefaultLogPageSize && PAGE_CACHE_SIZE <=
+	if (PAGE_SIZE >= DefaultLogPageSize && PAGE_SIZE <=
 			DefaultLogPageSize * 2)
 		log_page_size = DefaultLogPageSize;
 	else
-		log_page_size = PAGE_CACHE_SIZE;
+		log_page_size = PAGE_SIZE;
 	log_page_mask = log_page_size - 1;
 	/*
 	 * Use ntfs_ffs() instead of ffs() to enable the compiler to
@@ -539,7 +539,7 @@ bool ntfs_check_logfile(struct inode *log_vi, RESTART_PAGE_HEADER **rp)
 	 * to be empty.
 	 */
 	for (pos = 0; pos < size; pos <<= 1) {
-		pgoff_t idx = pos >> PAGE_CACHE_SHIFT;
+		pgoff_t idx = pos >> PAGE_SHIFT;
 		if (!page || page->index != idx) {
 			if (page)
 				ntfs_unmap_page(page);
@@ -550,7 +550,7 @@ bool ntfs_check_logfile(struct inode *log_vi, RESTART_PAGE_HEADER **rp)
 				goto err_out;
 			}
 		}
-		kaddr = (u8*)page_address(page) + (pos & ~PAGE_CACHE_MASK);
+		kaddr = (u8*)page_address(page) + (pos & ~PAGE_MASK);
 		/*
 		 * A non-empty block means the logfile is not empty while an
 		 * empty block after a non-empty block has been encountered
diff --git a/fs/ntfs/mft.c b/fs/ntfs/mft.c
index 3014a36a255b..37b2501caaa4 100644
--- a/fs/ntfs/mft.c
+++ b/fs/ntfs/mft.c
@@ -61,16 +61,16 @@ static inline MFT_RECORD *map_mft_record_page(ntfs_inode *ni)
 	 * here if the volume was that big...
 	 */
 	index = (u64)ni->mft_no << vol->mft_record_size_bits >>
-			PAGE_CACHE_SHIFT;
-	ofs = (ni->mft_no << vol->mft_record_size_bits) & ~PAGE_CACHE_MASK;
+			PAGE_SHIFT;
+	ofs = (ni->mft_no << vol->mft_record_size_bits) & ~PAGE_MASK;
 
 	i_size = i_size_read(mft_vi);
 	/* The maximum valid index into the page cache for $MFT's data. */
-	end_index = i_size >> PAGE_CACHE_SHIFT;
+	end_index = i_size >> PAGE_SHIFT;
 
 	/* If the wanted index is out of bounds the mft record doesn't exist. */
 	if (unlikely(index >= end_index)) {
-		if (index > end_index || (i_size & ~PAGE_CACHE_MASK) < ofs +
+		if (index > end_index || (i_size & ~PAGE_MASK) < ofs +
 				vol->mft_record_size) {
 			page = ERR_PTR(-ENOENT);
 			ntfs_error(vol->sb, "Attempt to read mft record 0x%lx, "
@@ -487,7 +487,7 @@ int ntfs_sync_mft_mirror(ntfs_volume *vol, const unsigned long mft_no,
 	}
 	/* Get the page containing the mirror copy of the mft record @m. */
 	page = ntfs_map_page(vol->mftmirr_ino->i_mapping, mft_no >>
-			(PAGE_CACHE_SHIFT - vol->mft_record_size_bits));
+			(PAGE_SHIFT - vol->mft_record_size_bits));
 	if (IS_ERR(page)) {
 		ntfs_error(vol->sb, "Failed to map mft mirror page.");
 		err = PTR_ERR(page);
@@ -497,7 +497,7 @@ int ntfs_sync_mft_mirror(ntfs_volume *vol, const unsigned long mft_no,
 	BUG_ON(!PageUptodate(page));
 	ClearPageUptodate(page);
 	/* Offset of the mft mirror record inside the page. */
-	page_ofs = (mft_no << vol->mft_record_size_bits) & ~PAGE_CACHE_MASK;
+	page_ofs = (mft_no << vol->mft_record_size_bits) & ~PAGE_MASK;
 	/* The address in the page of the mirror copy of the mft record @m. */
 	kmirr = page_address(page) + page_ofs;
 	/* Copy the mst protected mft record to the mirror. */
@@ -1178,8 +1178,8 @@ static int ntfs_mft_bitmap_find_and_alloc_free_rec_nolock(ntfs_volume *vol,
 	for (; pass <= 2;) {
 		/* Cap size to pass_end. */
 		ofs = data_pos >> 3;
-		page_ofs = ofs & ~PAGE_CACHE_MASK;
-		size = PAGE_CACHE_SIZE - page_ofs;
+		page_ofs = ofs & ~PAGE_MASK;
+		size = PAGE_SIZE - page_ofs;
 		ll = ((pass_end + 7) >> 3) - ofs;
 		if (size > ll)
 			size = ll;
@@ -1190,7 +1190,7 @@ static int ntfs_mft_bitmap_find_and_alloc_free_rec_nolock(ntfs_volume *vol,
 		 */
 		if (size) {
 			page = ntfs_map_page(mftbmp_mapping,
-					ofs >> PAGE_CACHE_SHIFT);
+					ofs >> PAGE_SHIFT);
 			if (IS_ERR(page)) {
 				ntfs_error(vol->sb, "Failed to read mft "
 						"bitmap, aborting.");
@@ -1328,13 +1328,13 @@ static int ntfs_mft_bitmap_extend_allocation_nolock(ntfs_volume *vol)
 	 */
 	ll = lcn >> 3;
 	page = ntfs_map_page(vol->lcnbmp_ino->i_mapping,
-			ll >> PAGE_CACHE_SHIFT);
+			ll >> PAGE_SHIFT);
 	if (IS_ERR(page)) {
 		up_write(&mftbmp_ni->runlist.lock);
 		ntfs_error(vol->sb, "Failed to read from lcn bitmap.");
 		return PTR_ERR(page);
 	}
-	b = (u8*)page_address(page) + (ll & ~PAGE_CACHE_MASK);
+	b = (u8*)page_address(page) + (ll & ~PAGE_MASK);
 	tb = 1 << (lcn & 7ull);
 	down_write(&vol->lcnbmp_lock);
 	if (*b != 0xff && !(*b & tb)) {
@@ -2103,14 +2103,14 @@ static int ntfs_mft_record_format(const ntfs_volume *vol, const s64 mft_no)
 	 * The index into the page cache and the offset within the page cache
 	 * page of the wanted mft record.
 	 */
-	index = mft_no << vol->mft_record_size_bits >> PAGE_CACHE_SHIFT;
-	ofs = (mft_no << vol->mft_record_size_bits) & ~PAGE_CACHE_MASK;
+	index = mft_no << vol->mft_record_size_bits >> PAGE_SHIFT;
+	ofs = (mft_no << vol->mft_record_size_bits) & ~PAGE_MASK;
 	/* The maximum valid index into the page cache for $MFT's data. */
 	i_size = i_size_read(mft_vi);
-	end_index = i_size >> PAGE_CACHE_SHIFT;
+	end_index = i_size >> PAGE_SHIFT;
 	if (unlikely(index >= end_index)) {
 		if (unlikely(index > end_index || ofs + vol->mft_record_size >=
-				(i_size & ~PAGE_CACHE_MASK))) {
+				(i_size & ~PAGE_MASK))) {
 			ntfs_error(vol->sb, "Tried to format non-existing mft "
 					"record 0x%llx.", (long long)mft_no);
 			return -ENOENT;
@@ -2515,8 +2515,8 @@ mft_rec_already_initialized:
 	 * We now have allocated and initialized the mft record.  Calculate the
 	 * index of and the offset within the page cache page the record is in.
 	 */
-	index = bit << vol->mft_record_size_bits >> PAGE_CACHE_SHIFT;
-	ofs = (bit << vol->mft_record_size_bits) & ~PAGE_CACHE_MASK;
+	index = bit << vol->mft_record_size_bits >> PAGE_SHIFT;
+	ofs = (bit << vol->mft_record_size_bits) & ~PAGE_MASK;
 	/* Read, map, and pin the page containing the mft record. */
 	page = ntfs_map_page(vol->mft_ino->i_mapping, index);
 	if (IS_ERR(page)) {
diff --git a/fs/ntfs/ntfs.h b/fs/ntfs/ntfs.h
index d6a340bf80fc..533a7e9eee72 100644
--- a/fs/ntfs/ntfs.h
+++ b/fs/ntfs/ntfs.h
@@ -44,7 +44,7 @@ typedef enum {
 	NTFS_MAX_NAME_LEN	= 255,
 	NTFS_MAX_ATTR_NAME_LEN	= 255,
 	NTFS_MAX_CLUSTER_SIZE	= 64 * 1024,	/* 64kiB */
-	NTFS_MAX_PAGES_PER_CLUSTER = NTFS_MAX_CLUSTER_SIZE / PAGE_CACHE_SIZE,
+	NTFS_MAX_PAGES_PER_CLUSTER = NTFS_MAX_CLUSTER_SIZE / PAGE_SIZE,
 } NTFS_CONSTANTS;
 
 /* Global variables. */
diff --git a/fs/ntfs/super.c b/fs/ntfs/super.c
index 82650d52d916..d98909427f3c 100644
--- a/fs/ntfs/super.c
+++ b/fs/ntfs/super.c
@@ -823,11 +823,11 @@ static bool parse_ntfs_boot_sector(ntfs_volume *vol, const NTFS_BOOT_SECTOR *b)
 	 * We cannot support mft record sizes above the PAGE_CACHE_SIZE since
 	 * we store $MFT/$DATA, the table of mft records in the page cache.
 	 */
-	if (vol->mft_record_size > PAGE_CACHE_SIZE) {
+	if (vol->mft_record_size > PAGE_SIZE) {
 		ntfs_error(vol->sb, "Mft record size (%i) exceeds the "
 				"PAGE_CACHE_SIZE on your system (%lu).  "
 				"This is not supported.  Sorry.",
-				vol->mft_record_size, PAGE_CACHE_SIZE);
+				vol->mft_record_size, PAGE_SIZE);
 		return false;
 	}
 	/* We cannot support mft record sizes below the sector size. */
@@ -1093,7 +1093,7 @@ static bool check_mft_mirror(ntfs_volume *vol)
 
 	ntfs_debug("Entering.");
 	/* Compare contents of $MFT and $MFTMirr. */
-	mrecs_per_page = PAGE_CACHE_SIZE / vol->mft_record_size;
+	mrecs_per_page = PAGE_SIZE / vol->mft_record_size;
 	BUG_ON(!mrecs_per_page);
 	BUG_ON(!vol->mftmirr_size);
 	mft_page = mirr_page = NULL;
@@ -1612,20 +1612,20 @@ static bool load_and_init_attrdef(ntfs_volume *vol)
 	if (!vol->attrdef)
 		goto iput_failed;
 	index = 0;
-	max_index = i_size >> PAGE_CACHE_SHIFT;
-	size = PAGE_CACHE_SIZE;
+	max_index = i_size >> PAGE_SHIFT;
+	size = PAGE_SIZE;
 	while (index < max_index) {
 		/* Read the attrdef table and copy it into the linear buffer. */
 read_partial_attrdef_page:
 		page = ntfs_map_page(ino->i_mapping, index);
 		if (IS_ERR(page))
 			goto free_iput_failed;
-		memcpy((u8*)vol->attrdef + (index++ << PAGE_CACHE_SHIFT),
+		memcpy((u8*)vol->attrdef + (index++ << PAGE_SHIFT),
 				page_address(page), size);
 		ntfs_unmap_page(page);
 	};
-	if (size == PAGE_CACHE_SIZE) {
-		size = i_size & ~PAGE_CACHE_MASK;
+	if (size == PAGE_SIZE) {
+		size = i_size & ~PAGE_MASK;
 		if (size)
 			goto read_partial_attrdef_page;
 	}
@@ -1681,20 +1681,20 @@ static bool load_and_init_upcase(ntfs_volume *vol)
 	if (!vol->upcase)
 		goto iput_upcase_failed;
 	index = 0;
-	max_index = i_size >> PAGE_CACHE_SHIFT;
-	size = PAGE_CACHE_SIZE;
+	max_index = i_size >> PAGE_SHIFT;
+	size = PAGE_SIZE;
 	while (index < max_index) {
 		/* Read the upcase table and copy it into the linear buffer. */
 read_partial_upcase_page:
 		page = ntfs_map_page(ino->i_mapping, index);
 		if (IS_ERR(page))
 			goto iput_upcase_failed;
-		memcpy((char*)vol->upcase + (index++ << PAGE_CACHE_SHIFT),
+		memcpy((char*)vol->upcase + (index++ << PAGE_SHIFT),
 				page_address(page), size);
 		ntfs_unmap_page(page);
 	};
-	if (size == PAGE_CACHE_SIZE) {
-		size = i_size & ~PAGE_CACHE_MASK;
+	if (size == PAGE_SIZE) {
+		size = i_size & ~PAGE_MASK;
 		if (size)
 			goto read_partial_upcase_page;
 	}
@@ -2478,11 +2478,11 @@ static s64 get_nr_free_clusters(ntfs_volume *vol)
 	 * multiples of PAGE_CACHE_SIZE, rounding up so that if we have one
 	 * full and one partial page max_index = 2.
 	 */
-	max_index = (((vol->nr_clusters + 7) >> 3) + PAGE_CACHE_SIZE - 1) >>
-			PAGE_CACHE_SHIFT;
+	max_index = (((vol->nr_clusters + 7) >> 3) + PAGE_SIZE - 1) >>
+			PAGE_SHIFT;
 	/* Use multiples of 4 bytes, thus max_size is PAGE_CACHE_SIZE / 4. */
 	ntfs_debug("Reading $Bitmap, max_index = 0x%lx, max_size = 0x%lx.",
-			max_index, PAGE_CACHE_SIZE / 4);
+			max_index, PAGE_SIZE / 4);
 	for (index = 0; index < max_index; index++) {
 		unsigned long *kaddr;
 
@@ -2495,7 +2495,7 @@ static s64 get_nr_free_clusters(ntfs_volume *vol)
 		if (IS_ERR(page)) {
 			ntfs_debug("read_mapping_page() error. Skipping "
 					"page (index 0x%lx).", index);
-			nr_free -= PAGE_CACHE_SIZE * 8;
+			nr_free -= PAGE_SIZE * 8;
 			continue;
 		}
 		kaddr = kmap_atomic(page);
@@ -2507,9 +2507,9 @@ static s64 get_nr_free_clusters(ntfs_volume *vol)
 		 * ntfs_readpage().
 		 */
 		nr_free -= bitmap_weight(kaddr,
-					PAGE_CACHE_SIZE * BITS_PER_BYTE);
+					PAGE_SIZE * BITS_PER_BYTE);
 		kunmap_atomic(kaddr);
-		page_cache_release(page);
+		put_page(page);
 	}
 	ntfs_debug("Finished reading $Bitmap, last index = 0x%lx.", index - 1);
 	/*
@@ -2553,7 +2553,7 @@ static unsigned long __get_nr_free_mft_records(ntfs_volume *vol,
 	ntfs_debug("Entering.");
 	/* Use multiples of 4 bytes, thus max_size is PAGE_CACHE_SIZE / 4. */
 	ntfs_debug("Reading $MFT/$BITMAP, max_index = 0x%lx, max_size = "
-			"0x%lx.", max_index, PAGE_CACHE_SIZE / 4);
+			"0x%lx.", max_index, PAGE_SIZE / 4);
 	for (index = 0; index < max_index; index++) {
 		unsigned long *kaddr;
 
@@ -2566,7 +2566,7 @@ static unsigned long __get_nr_free_mft_records(ntfs_volume *vol,
 		if (IS_ERR(page)) {
 			ntfs_debug("read_mapping_page() error. Skipping "
 					"page (index 0x%lx).", index);
-			nr_free -= PAGE_CACHE_SIZE * 8;
+			nr_free -= PAGE_SIZE * 8;
 			continue;
 		}
 		kaddr = kmap_atomic(page);
@@ -2578,9 +2578,9 @@ static unsigned long __get_nr_free_mft_records(ntfs_volume *vol,
 		 * ntfs_readpage().
 		 */
 		nr_free -= bitmap_weight(kaddr,
-					PAGE_CACHE_SIZE * BITS_PER_BYTE);
+					PAGE_SIZE * BITS_PER_BYTE);
 		kunmap_atomic(kaddr);
-		page_cache_release(page);
+		put_page(page);
 	}
 	ntfs_debug("Finished reading $MFT/$BITMAP, last index = 0x%lx.",
 			index - 1);
@@ -2622,17 +2622,17 @@ static int ntfs_statfs(struct dentry *dentry, struct kstatfs *sfs)
 	/* Type of filesystem. */
 	sfs->f_type   = NTFS_SB_MAGIC;
 	/* Optimal transfer block size. */
-	sfs->f_bsize  = PAGE_CACHE_SIZE;
+	sfs->f_bsize  = PAGE_SIZE;
 	/*
 	 * Total data blocks in filesystem in units of f_bsize and since
 	 * inodes are also stored in data blocs ($MFT is a file) this is just
 	 * the total clusters.
 	 */
 	sfs->f_blocks = vol->nr_clusters << vol->cluster_size_bits >>
-				PAGE_CACHE_SHIFT;
+				PAGE_SHIFT;
 	/* Free data blocks in filesystem in units of f_bsize. */
 	size	      = get_nr_free_clusters(vol) << vol->cluster_size_bits >>
-				PAGE_CACHE_SHIFT;
+				PAGE_SHIFT;
 	if (size < 0LL)
 		size = 0LL;
 	/* Free blocks avail to non-superuser, same as above on NTFS. */
@@ -2647,7 +2647,7 @@ static int ntfs_statfs(struct dentry *dentry, struct kstatfs *sfs)
 	 * have one full and one partial page max_index = 2.
 	 */
 	max_index = ((((mft_ni->initialized_size >> vol->mft_record_size_bits)
-			+ 7) >> 3) + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+			+ 7) >> 3) + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	read_unlock_irqrestore(&mft_ni->size_lock, flags);
 	/* Number of inodes in filesystem (at this point in time). */
 	sfs->f_files = size;
@@ -2770,14 +2770,14 @@ static int ntfs_fill_super(struct super_block *sb, void *opt, const int silent)
 		goto err_out_now;
 
 	/* We support sector sizes up to the PAGE_CACHE_SIZE. */
-	if (bdev_logical_block_size(sb->s_bdev) > PAGE_CACHE_SIZE) {
+	if (bdev_logical_block_size(sb->s_bdev) > PAGE_SIZE) {
 		if (!silent)
 			ntfs_error(sb, "Device has unsupported sector size "
 					"(%i).  The maximum supported sector "
 					"size on this architecture is %lu "
 					"bytes.",
 					bdev_logical_block_size(sb->s_bdev),
-					PAGE_CACHE_SIZE);
+					PAGE_SIZE);
 		goto err_out_now;
 	}
 	/*
diff --git a/fs/ocfs2/alloc.c b/fs/ocfs2/alloc.c
index b8a9d87231b1..62868596b359 100644
--- a/fs/ocfs2/alloc.c
+++ b/fs/ocfs2/alloc.c
@@ -6602,7 +6602,7 @@ static void ocfs2_zero_cluster_pages(struct inode *inode, loff_t start,
 {
 	int i;
 	struct page *page;
-	unsigned int from, to = PAGE_CACHE_SIZE;
+	unsigned int from, to = PAGE_SIZE;
 	struct super_block *sb = inode->i_sb;
 
 	BUG_ON(!ocfs2_sparse_alloc(OCFS2_SB(sb)));
@@ -6610,21 +6610,21 @@ static void ocfs2_zero_cluster_pages(struct inode *inode, loff_t start,
 	if (numpages == 0)
 		goto out;
 
-	to = PAGE_CACHE_SIZE;
+	to = PAGE_SIZE;
 	for(i = 0; i < numpages; i++) {
 		page = pages[i];
 
-		from = start & (PAGE_CACHE_SIZE - 1);
-		if ((end >> PAGE_CACHE_SHIFT) == page->index)
-			to = end & (PAGE_CACHE_SIZE - 1);
+		from = start & (PAGE_SIZE - 1);
+		if ((end >> PAGE_SHIFT) == page->index)
+			to = end & (PAGE_SIZE - 1);
 
-		BUG_ON(from > PAGE_CACHE_SIZE);
-		BUG_ON(to > PAGE_CACHE_SIZE);
+		BUG_ON(from > PAGE_SIZE);
+		BUG_ON(to > PAGE_SIZE);
 
 		ocfs2_map_and_dirty_page(inode, handle, from, to, page, 1,
 					 &phys);
 
-		start = (page->index + 1) << PAGE_CACHE_SHIFT;
+		start = (page->index + 1) << PAGE_SHIFT;
 	}
 out:
 	if (pages)
@@ -6643,7 +6643,7 @@ int ocfs2_grab_pages(struct inode *inode, loff_t start, loff_t end,
 
 	numpages = 0;
 	last_page_bytes = PAGE_ALIGN(end);
-	index = start >> PAGE_CACHE_SHIFT;
+	index = start >> PAGE_SHIFT;
 	do {
 		pages[numpages] = find_or_create_page(mapping, index, GFP_NOFS);
 		if (!pages[numpages]) {
@@ -6654,7 +6654,7 @@ int ocfs2_grab_pages(struct inode *inode, loff_t start, loff_t end,
 
 		numpages++;
 		index++;
-	} while (index < (last_page_bytes >> PAGE_CACHE_SHIFT));
+	} while (index < (last_page_bytes >> PAGE_SHIFT));
 
 out:
 	if (ret != 0) {
@@ -6880,8 +6880,8 @@ int ocfs2_convert_inline_data_to_extents(struct inode *inode,
 		 * to do that now.
 		 */
 		if (!ocfs2_sparse_alloc(osb) &&
-		    PAGE_CACHE_SIZE < osb->s_clustersize)
-			end = PAGE_CACHE_SIZE;
+		    PAGE_SIZE < osb->s_clustersize)
+			end = PAGE_SIZE;
 
 		ret = ocfs2_grab_eof_pages(inode, 0, end, pages, &num_pages);
 		if (ret) {
@@ -6899,8 +6899,8 @@ int ocfs2_convert_inline_data_to_extents(struct inode *inode,
 			goto out_commit;
 		}
 
-		page_end = PAGE_CACHE_SIZE;
-		if (PAGE_CACHE_SIZE > osb->s_clustersize)
+		page_end = PAGE_SIZE;
+		if (PAGE_SIZE > osb->s_clustersize)
 			page_end = osb->s_clustersize;
 
 		for (i = 0; i < num_pages; i++)
* Unmerged path fs/ocfs2/aops.c
diff --git a/fs/ocfs2/cluster/heartbeat.c b/fs/ocfs2/cluster/heartbeat.c
index 42252bf64b51..8dbcb722f7ea 100644
--- a/fs/ocfs2/cluster/heartbeat.c
+++ b/fs/ocfs2/cluster/heartbeat.c
@@ -426,13 +426,13 @@ static struct bio *o2hb_setup_one_bio(struct o2hb_region *reg,
 	bio->bi_private = wc;
 	bio->bi_end_io = o2hb_bio_end_io;
 
-	vec_start = (cs << bits) % PAGE_CACHE_SIZE;
+	vec_start = (cs << bits) % PAGE_SIZE;
 	while(cs < max_slots) {
 		current_page = cs / spp;
 		page = reg->hr_slot_data[current_page];
 
-		vec_len = min(PAGE_CACHE_SIZE - vec_start,
-			      (max_slots-cs) * (PAGE_CACHE_SIZE/spp) );
+		vec_len = min(PAGE_SIZE - vec_start,
+			      (max_slots-cs) * (PAGE_SIZE/spp) );
 
 		mlog(ML_HB_BIO, "page %d, vec_len = %u, vec_start = %u\n",
 		     current_page, vec_len, vec_start);
@@ -440,7 +440,7 @@ static struct bio *o2hb_setup_one_bio(struct o2hb_region *reg,
 		len = bio_add_page(bio, page, vec_len, vec_start);
 		if (len != vec_len) break;
 
-		cs += vec_len / (PAGE_CACHE_SIZE/spp);
+		cs += vec_len / (PAGE_SIZE/spp);
 		vec_start = 0;
 	}
 
@@ -1623,7 +1623,7 @@ static ssize_t o2hb_region_dev_read(struct o2hb_region *reg,
 
 static void o2hb_init_region_params(struct o2hb_region *reg)
 {
-	reg->hr_slots_per_page = PAGE_CACHE_SIZE >> reg->hr_block_bits;
+	reg->hr_slots_per_page = PAGE_SIZE >> reg->hr_block_bits;
 	reg->hr_timeout_ms = O2HB_REGION_TIMEOUT_MS;
 
 	mlog(ML_HEARTBEAT, "hr_start_block = %llu, hr_blocks = %u\n",
diff --git a/fs/ocfs2/dlmfs/dlmfs.c b/fs/ocfs2/dlmfs/dlmfs.c
index 12bafb7265ce..ad16030ad81f 100644
--- a/fs/ocfs2/dlmfs/dlmfs.c
+++ b/fs/ocfs2/dlmfs/dlmfs.c
@@ -583,8 +583,8 @@ static int dlmfs_fill_super(struct super_block * sb,
 			    int silent)
 {
 	sb->s_maxbytes = MAX_LFS_FILESIZE;
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = DLMFS_MAGIC;
 	sb->s_op = &dlmfs_ops;
 	sb->s_root = d_make_root(dlmfs_get_root_inode(sb));
* Unmerged path fs/ocfs2/file.c
diff --git a/fs/ocfs2/mmap.c b/fs/ocfs2/mmap.c
index 10d66c75cecb..6d333a18d7e9 100644
--- a/fs/ocfs2/mmap.c
+++ b/fs/ocfs2/mmap.c
@@ -65,13 +65,13 @@ static int __ocfs2_page_mkwrite(struct file *file, struct buffer_head *di_bh,
 	struct inode *inode = file_inode(file);
 	struct address_space *mapping = inode->i_mapping;
 	loff_t pos = page_offset(page);
-	unsigned int len = PAGE_CACHE_SIZE;
+	unsigned int len = PAGE_SIZE;
 	pgoff_t last_index;
 	struct page *locked_page = NULL;
 	void *fsdata;
 	loff_t size = i_size_read(inode);
 
-	last_index = (size - 1) >> PAGE_CACHE_SHIFT;
+	last_index = (size - 1) >> PAGE_SHIFT;
 
 	/*
 	 * There are cases that lead to the page no longer bebongs to the
@@ -102,7 +102,7 @@ static int __ocfs2_page_mkwrite(struct file *file, struct buffer_head *di_bh,
 	 * because the "write" would invalidate their data.
 	 */
 	if (page->index == last_index)
-		len = ((size - 1) & ~PAGE_CACHE_MASK) + 1;
+		len = ((size - 1) & ~PAGE_MASK) + 1;
 
 	ret = ocfs2_write_begin_nolock(file, mapping, pos, len, 0, &locked_page,
 				       &fsdata, di_bh, page);
diff --git a/fs/ocfs2/ocfs2.h b/fs/ocfs2/ocfs2.h
index d355e6e36b36..8db0bfbd6803 100644
--- a/fs/ocfs2/ocfs2.h
+++ b/fs/ocfs2/ocfs2.h
@@ -783,10 +783,10 @@ static inline unsigned int ocfs2_page_index_to_clusters(struct super_block *sb,
 	u32 clusters = pg_index;
 	unsigned int cbits = OCFS2_SB(sb)->s_clustersize_bits;
 
-	if (unlikely(PAGE_CACHE_SHIFT > cbits))
-		clusters = pg_index << (PAGE_CACHE_SHIFT - cbits);
-	else if (PAGE_CACHE_SHIFT < cbits)
-		clusters = pg_index >> (cbits - PAGE_CACHE_SHIFT);
+	if (unlikely(PAGE_SHIFT > cbits))
+		clusters = pg_index << (PAGE_SHIFT - cbits);
+	else if (PAGE_SHIFT < cbits)
+		clusters = pg_index >> (cbits - PAGE_SHIFT);
 
 	return clusters;
 }
@@ -800,10 +800,10 @@ static inline pgoff_t ocfs2_align_clusters_to_page_index(struct super_block *sb,
 	unsigned int cbits = OCFS2_SB(sb)->s_clustersize_bits;
         pgoff_t index = clusters;
 
-	if (PAGE_CACHE_SHIFT > cbits) {
-		index = (pgoff_t)clusters >> (PAGE_CACHE_SHIFT - cbits);
-	} else if (PAGE_CACHE_SHIFT < cbits) {
-		index = (pgoff_t)clusters << (cbits - PAGE_CACHE_SHIFT);
+	if (PAGE_SHIFT > cbits) {
+		index = (pgoff_t)clusters >> (PAGE_SHIFT - cbits);
+	} else if (PAGE_SHIFT < cbits) {
+		index = (pgoff_t)clusters << (cbits - PAGE_SHIFT);
 	}
 
 	return index;
@@ -814,8 +814,8 @@ static inline unsigned int ocfs2_pages_per_cluster(struct super_block *sb)
 	unsigned int cbits = OCFS2_SB(sb)->s_clustersize_bits;
 	unsigned int pages_per_cluster = 1;
 
-	if (PAGE_CACHE_SHIFT < cbits)
-		pages_per_cluster = 1 << (cbits - PAGE_CACHE_SHIFT);
+	if (PAGE_SHIFT < cbits)
+		pages_per_cluster = 1 << (cbits - PAGE_SHIFT);
 
 	return pages_per_cluster;
 }
diff --git a/fs/ocfs2/refcounttree.c b/fs/ocfs2/refcounttree.c
index 998b17eda09d..78a7cdb280bb 100644
--- a/fs/ocfs2/refcounttree.c
+++ b/fs/ocfs2/refcounttree.c
@@ -2953,16 +2953,16 @@ int ocfs2_duplicate_clusters_by_page(handle_t *handle,
 		end = i_size_read(inode);
 
 	while (offset < end) {
-		page_index = offset >> PAGE_CACHE_SHIFT;
-		map_end = ((loff_t)page_index + 1) << PAGE_CACHE_SHIFT;
+		page_index = offset >> PAGE_SHIFT;
+		map_end = ((loff_t)page_index + 1) << PAGE_SHIFT;
 		if (map_end > end)
 			map_end = end;
 
 		/* from, to is the offset within the page. */
-		from = offset & (PAGE_CACHE_SIZE - 1);
-		to = PAGE_CACHE_SIZE;
-		if (map_end & (PAGE_CACHE_SIZE - 1))
-			to = map_end & (PAGE_CACHE_SIZE - 1);
+		from = offset & (PAGE_SIZE - 1);
+		to = PAGE_SIZE;
+		if (map_end & (PAGE_SIZE - 1))
+			to = map_end & (PAGE_SIZE - 1);
 
 		page = find_or_create_page(mapping, page_index, GFP_NOFS);
 
@@ -2970,7 +2970,7 @@ int ocfs2_duplicate_clusters_by_page(handle_t *handle,
 		 * In case PAGE_CACHE_SIZE <= CLUSTER_SIZE, This page
 		 * can't be dirtied before we CoW it out.
 		 */
-		if (PAGE_CACHE_SIZE <= OCFS2_SB(sb)->s_clustersize)
+		if (PAGE_SIZE <= OCFS2_SB(sb)->s_clustersize)
 			BUG_ON(PageDirty(page));
 
 		if (PageReadahead(page)) {
@@ -3004,7 +3004,7 @@ int ocfs2_duplicate_clusters_by_page(handle_t *handle,
 		mark_page_accessed(page);
 unlock:
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		page = NULL;
 		offset = map_end;
 		if (ret)
@@ -3185,8 +3185,8 @@ int ocfs2_cow_sync_writeback(struct super_block *sb,
 	}
 
 	while (offset < end) {
-		page_index = offset >> PAGE_CACHE_SHIFT;
-		map_end = ((loff_t)page_index + 1) << PAGE_CACHE_SHIFT;
+		page_index = offset >> PAGE_SHIFT;
+		map_end = ((loff_t)page_index + 1) << PAGE_SHIFT;
 		if (map_end > end)
 			map_end = end;
 
@@ -3202,7 +3202,7 @@ int ocfs2_cow_sync_writeback(struct super_block *sb,
 			mark_page_accessed(page);
 
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 		page = NULL;
 		offset = map_end;
 		if (ret)
diff --git a/fs/ocfs2/super.c b/fs/ocfs2/super.c
index 50a05c5e4fcd..3a264dd4a433 100644
--- a/fs/ocfs2/super.c
+++ b/fs/ocfs2/super.c
@@ -596,8 +596,8 @@ static unsigned long long ocfs2_max_file_offset(unsigned int bbits,
 	/*
 	 * We might be limited by page cache size.
 	 */
-	if (bytes > PAGE_CACHE_SIZE) {
-		bytes = PAGE_CACHE_SIZE;
+	if (bytes > PAGE_SIZE) {
+		bytes = PAGE_SIZE;
 		trim = 1;
 		/*
 		 * Shift by 31 here so that we don't get larger than
* Unmerged path fs/orangefs/inode.c
* Unmerged path fs/orangefs/orangefs-bufmap.c
* Unmerged path fs/orangefs/orangefs-utils.c
diff --git a/fs/pipe.c b/fs/pipe.c
index 71b6fa608783..cafdb66a2966 100644
--- a/fs/pipe.c
+++ b/fs/pipe.c
@@ -232,7 +232,7 @@ static void anon_pipe_buf_release(struct pipe_inode_info *pipe,
 	if (page_count(page) == 1 && !pipe->tmp_page)
 		pipe->tmp_page = page;
 	else
-		page_cache_release(page);
+		put_page(page);
 }
 
 /**
@@ -324,7 +324,7 @@ EXPORT_SYMBOL(generic_pipe_buf_steal);
  */
 void generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)
 {
-	page_cache_get(buf->page);
+	get_page(buf->page);
 }
 EXPORT_SYMBOL(generic_pipe_buf_get);
 
@@ -355,7 +355,7 @@ EXPORT_SYMBOL(generic_pipe_buf_confirm);
 void generic_pipe_buf_release(struct pipe_inode_info *pipe,
 			      struct pipe_buffer *buf)
 {
-	page_cache_release(buf->page);
+	put_page(buf->page);
 }
 EXPORT_SYMBOL(generic_pipe_buf_release);
 
diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index 63debabc68f8..d5a98858af15 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -502,7 +502,7 @@ static void smaps_pte_entry(pte_t ptent, unsigned long addr,
 		if (radix_tree_exceptional_entry(page))
 			mss->swap += PAGE_SIZE;
 		else
-			page_cache_release(page);
+			put_page(page);
 
 		return;
 	}
diff --git a/fs/proc/vmcore.c b/fs/proc/vmcore.c
index 909319b1232b..86526c118c2c 100644
--- a/fs/proc/vmcore.c
+++ b/fs/proc/vmcore.c
@@ -277,12 +277,12 @@ static int mmap_vmcore_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	if (!page)
 		return VM_FAULT_OOM;
 	if (!PageUptodate(page)) {
-		offset = (loff_t) index << PAGE_CACHE_SHIFT;
+		offset = (loff_t) index << PAGE_SHIFT;
 		buf = __va((page_to_pfn(page) << PAGE_SHIFT));
 		rc = __read_vmcore(buf, PAGE_SIZE, &offset, 0);
 		if (rc < 0) {
 			unlock_page(page);
-			page_cache_release(page);
+			put_page(page);
 			return (rc == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS;
 		}
 		SetPageUptodate(page);
diff --git a/fs/pstore/inode.c b/fs/pstore/inode.c
index 808c88d636e2..c0edda4101d3 100644
--- a/fs/pstore/inode.c
+++ b/fs/pstore/inode.c
@@ -390,8 +390,8 @@ static int pstore_fill_super(struct super_block *sb, void *data, int silent)
 	pstore_sb = sb;
 
 	sb->s_maxbytes		= MAX_LFS_FILESIZE;
-	sb->s_blocksize		= PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits	= PAGE_CACHE_SHIFT;
+	sb->s_blocksize		= PAGE_SIZE;
+	sb->s_blocksize_bits	= PAGE_SHIFT;
 	sb->s_magic		= PSTOREFS_MAGIC;
 	sb->s_op		= &pstore_ops;
 	sb->s_time_gran		= 1;
* Unmerged path fs/qnx6/dir.c
diff --git a/fs/qnx6/inode.c b/fs/qnx6/inode.c
index 8d941edfefa1..a8f80e3623f3 100644
--- a/fs/qnx6/inode.c
+++ b/fs/qnx6/inode.c
@@ -560,8 +560,8 @@ struct inode *qnx6_iget(struct super_block *sb, unsigned ino)
 		iget_failed(inode);
 		return ERR_PTR(-EIO);
 	}
-	n = (ino - 1) >> (PAGE_CACHE_SHIFT - QNX6_INODE_SIZE_BITS);
-	offs = (ino - 1) & (~PAGE_CACHE_MASK >> QNX6_INODE_SIZE_BITS);
+	n = (ino - 1) >> (PAGE_SHIFT - QNX6_INODE_SIZE_BITS);
+	offs = (ino - 1) & (~PAGE_MASK >> QNX6_INODE_SIZE_BITS);
 	mapping = sbi->inodes->i_mapping;
 	page = read_mapping_page(mapping, n, NULL);
 	if (IS_ERR(page)) {
diff --git a/fs/qnx6/qnx6.h b/fs/qnx6/qnx6.h
index b00fcc960d37..6422a28cd48b 100644
--- a/fs/qnx6/qnx6.h
+++ b/fs/qnx6/qnx6.h
@@ -128,7 +128,7 @@ extern struct qnx6_super_block *qnx6_mmi_fill_super(struct super_block *s,
 static inline void qnx6_put_page(struct page *page)
 {
 	kunmap(page);
-	page_cache_release(page);
+	put_page(page);
 }
 
 extern unsigned qnx6_find_entry(int len, struct inode *dir, const char *name,
diff --git a/fs/ramfs/inode.c b/fs/ramfs/inode.c
index 39d14659a8d3..12ab9f70ba1b 100644
--- a/fs/ramfs/inode.c
+++ b/fs/ramfs/inode.c
@@ -224,8 +224,8 @@ int ramfs_fill_super(struct super_block *sb, void *data, int silent)
 		return err;
 
 	sb->s_maxbytes		= MAX_LFS_FILESIZE;
-	sb->s_blocksize		= PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits	= PAGE_CACHE_SHIFT;
+	sb->s_blocksize		= PAGE_SIZE;
+	sb->s_blocksize_bits	= PAGE_SHIFT;
 	sb->s_magic		= RAMFS_MAGIC;
 	sb->s_op		= &ramfs_ops;
 	sb->s_time_gran		= 1;
diff --git a/fs/reiserfs/file.c b/fs/reiserfs/file.c
index dcaafcfc23b0..078acea206de 100644
--- a/fs/reiserfs/file.c
+++ b/fs/reiserfs/file.c
@@ -173,11 +173,11 @@ int reiserfs_commit_page(struct inode *inode, struct page *page,
 	int partial = 0;
 	unsigned blocksize;
 	struct buffer_head *bh, *head;
-	unsigned long i_size_index = inode->i_size >> PAGE_CACHE_SHIFT;
+	unsigned long i_size_index = inode->i_size >> PAGE_SHIFT;
 	int new;
 	int logit = reiserfs_file_data_log(inode);
 	struct super_block *s = inode->i_sb;
-	int bh_per_page = PAGE_CACHE_SIZE / s->s_blocksize;
+	int bh_per_page = PAGE_SIZE / s->s_blocksize;
 	struct reiserfs_transaction_handle th;
 	int ret = 0;
 
* Unmerged path fs/reiserfs/inode.c
diff --git a/fs/reiserfs/ioctl.c b/fs/reiserfs/ioctl.c
index 15cb5fe6b425..0ee385d8e4e5 100644
--- a/fs/reiserfs/ioctl.c
+++ b/fs/reiserfs/ioctl.c
@@ -199,7 +199,7 @@ int reiserfs_unpack(struct inode *inode, struct file *filp)
 	 ** __reiserfs_write_begin on that page.  This will force a
 	 ** reiserfs_get_block to unpack the tail for us.
 	 */
-	index = inode->i_size >> PAGE_CACHE_SHIFT;
+	index = inode->i_size >> PAGE_SHIFT;
 	mapping = inode->i_mapping;
 	page = grab_cache_page(mapping, index);
 	retval = -ENOMEM;
@@ -217,7 +217,7 @@ int reiserfs_unpack(struct inode *inode, struct file *filp)
 
       out_unlock:
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 
       out:
 	mutex_unlock(&inode->i_mutex);
diff --git a/fs/reiserfs/journal.c b/fs/reiserfs/journal.c
index 742fdd4c209a..bb9e8b7aa794 100644
--- a/fs/reiserfs/journal.c
+++ b/fs/reiserfs/journal.c
@@ -604,12 +604,12 @@ static void release_buffer_page(struct buffer_head *bh)
 {
 	struct page *page = bh->b_page;
 	if (!page->mapping && trylock_page(page)) {
-		page_cache_get(page);
+		get_page(page);
 		put_bh(bh);
 		if (!page->mapping)
 			try_to_free_buffers(page);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	} else {
 		put_bh(bh);
 	}
diff --git a/fs/reiserfs/stree.c b/fs/reiserfs/stree.c
index 2f40a4c70a4d..79876c300348 100644
--- a/fs/reiserfs/stree.c
+++ b/fs/reiserfs/stree.c
@@ -1285,7 +1285,7 @@ int reiserfs_delete_item(struct reiserfs_transaction_handle *th,
 		 */
 
 		data = kmap_atomic(un_bh->b_page);
-		off = ((le_ih_k_offset(&s_ih) - 1) & (PAGE_CACHE_SIZE - 1));
+		off = ((le_ih_k_offset(&s_ih) - 1) & (PAGE_SIZE - 1));
 		memcpy(data + off,
 		       B_I_PITEM(PATH_PLAST_BUFFER(path), &s_ih),
 		       ret_value);
@@ -1437,7 +1437,7 @@ static void unmap_buffers(struct page *page, loff_t pos)
 
 	if (page) {
 		if (page_has_buffers(page)) {
-			tail_index = pos & (PAGE_CACHE_SIZE - 1);
+			tail_index = pos & (PAGE_SIZE - 1);
 			cur_index = 0;
 			head = page_buffers(page);
 			bh = head;
diff --git a/fs/reiserfs/tail_conversion.c b/fs/reiserfs/tail_conversion.c
index 5e2624d12f70..c1b21548a76f 100644
--- a/fs/reiserfs/tail_conversion.c
+++ b/fs/reiserfs/tail_conversion.c
@@ -127,7 +127,7 @@ int direct2indirect(struct reiserfs_transaction_handle *th, struct inode *inode,
 	 */
 	if (up_to_date_bh) {
 		unsigned pgoff =
-		    (tail_offset + total_tail - 1) & (PAGE_CACHE_SIZE - 1);
+		    (tail_offset + total_tail - 1) & (PAGE_SIZE - 1);
 		char *kaddr = kmap_atomic(up_to_date_bh->b_page);
 		memset(kaddr + pgoff, 0, blk_size - total_tail);
 		kunmap_atomic(kaddr);
@@ -240,7 +240,7 @@ int indirect2direct(struct reiserfs_transaction_handle *th,
 	 ** the page was locked and this part of the page was up to date when
 	 ** indirect2direct was called, so we know the bytes are still valid
 	 */
-	tail = tail + (pos & (PAGE_CACHE_SIZE - 1));
+	tail = tail + (pos & (PAGE_SIZE - 1));
 
 	PATH_LAST_POSITION(path)++;
 
* Unmerged path fs/reiserfs/xattr.c
diff --git a/fs/splice.c b/fs/splice.c
index 773dd394e8b9..0f73501a0053 100644
--- a/fs/splice.c
+++ b/fs/splice.c
@@ -88,7 +88,7 @@ out_unlock:
 static void page_cache_pipe_buf_release(struct pipe_inode_info *pipe,
 					struct pipe_buffer *buf)
 {
-	page_cache_release(buf->page);
+	put_page(buf->page);
 	buf->flags &= ~PIPE_BUF_FLAG_LRU;
 }
 
@@ -269,7 +269,7 @@ EXPORT_SYMBOL_GPL(splice_to_pipe);
 
 void spd_release_page(struct splice_pipe_desc *spd, unsigned int i)
 {
-	page_cache_release(spd->pages[i]);
+	put_page(spd->pages[i]);
 }
 
 /*
@@ -329,9 +329,9 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 	if (splice_grow_spd(pipe, &spd))
 		return -ENOMEM;
 
-	index = *ppos >> PAGE_CACHE_SHIFT;
-	loff = *ppos & ~PAGE_CACHE_MASK;
-	req_pages = (len + loff + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+	index = *ppos >> PAGE_SHIFT;
+	loff = *ppos & ~PAGE_MASK;
+	req_pages = (len + loff + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	nr_pages = min(req_pages, spd.nr_pages_max);
 
 	/*
@@ -366,7 +366,7 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 			error = add_to_page_cache_lru(page, mapping, index,
 						GFP_KERNEL);
 			if (unlikely(error)) {
-				page_cache_release(page);
+				put_page(page);
 				if (error == -EEXIST)
 					continue;
 				break;
@@ -386,7 +386,7 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 	 * Now loop over the map and see if we need to start IO on any
 	 * pages, fill in the partial map, etc.
 	 */
-	index = *ppos >> PAGE_CACHE_SHIFT;
+	index = *ppos >> PAGE_SHIFT;
 	nr_pages = spd.nr_pages;
 	spd.nr_pages = 0;
 	for (page_nr = 0; page_nr < nr_pages; page_nr++) {
@@ -398,7 +398,7 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 		/*
 		 * this_len is the max we'll use from this page
 		 */
-		this_len = min_t(unsigned long, len, PAGE_CACHE_SIZE - loff);
+		this_len = min_t(unsigned long, len, PAGE_SIZE - loff);
 		page = spd.pages[page_nr];
 
 		if (PageReadahead(page))
@@ -427,7 +427,7 @@ retry_lookup:
 					error = -ENOMEM;
 					break;
 				}
-				page_cache_release(spd.pages[page_nr]);
+				put_page(spd.pages[page_nr]);
 				spd.pages[page_nr] = page;
 			}
 			/*
@@ -457,7 +457,7 @@ fill_it:
 		 * i_size must be checked after PageUptodate.
 		 */
 		isize = i_size_read(mapping->host);
-		end_index = (isize - 1) >> PAGE_CACHE_SHIFT;
+		end_index = (isize - 1) >> PAGE_SHIFT;
 		if (unlikely(!isize || index > end_index))
 			break;
 
@@ -471,7 +471,7 @@ fill_it:
 			/*
 			 * max good bytes in this page
 			 */
-			plen = ((isize - 1) & ~PAGE_CACHE_MASK) + 1;
+			plen = ((isize - 1) & ~PAGE_MASK) + 1;
 			if (plen <= loff)
 				break;
 
@@ -495,8 +495,8 @@ fill_it:
 	 * we got, 'nr_pages' is how many pages are in the map.
 	 */
 	while (page_nr < nr_pages)
-		page_cache_release(spd.pages[page_nr++]);
-	in->f_ra.prev_pos = (loff_t)index << PAGE_CACHE_SHIFT;
+		put_page(spd.pages[page_nr++]);
+	in->f_ra.prev_pos = (loff_t)index << PAGE_SHIFT;
 
 	if (spd.nr_pages)
 		error = splice_to_pipe(pipe, &spd);
@@ -623,8 +623,8 @@ ssize_t default_file_splice_read(struct file *in, loff_t *ppos,
 			goto shrink_ret;
 	}
 
-	offset = *ppos & ~PAGE_CACHE_MASK;
-	nr_pages = (len + offset + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
+	offset = *ppos & ~PAGE_MASK;
+	nr_pages = (len + offset + PAGE_SIZE - 1) >> PAGE_SHIFT;
 
 	for (i = 0; i < nr_pages && i < spd.nr_pages_max && len; i++) {
 		struct page *page;
@@ -634,7 +634,7 @@ ssize_t default_file_splice_read(struct file *in, loff_t *ppos,
 		if (!page)
 			goto err;
 
-		this_len = min_t(size_t, len, PAGE_CACHE_SIZE - offset);
+		this_len = min_t(size_t, len, PAGE_SIZE - offset);
 		vec[i].iov_base = (void __user *) page_address(page);
 		vec[i].iov_len = this_len;
 		spd.pages[i] = page;
* Unmerged path fs/squashfs/block.c
* Unmerged path fs/squashfs/cache.c
* Unmerged path fs/squashfs/decompressor.c
* Unmerged path fs/squashfs/file.c
* Unmerged path fs/squashfs/file_direct.c
* Unmerged path fs/squashfs/lz4_wrapper.c
* Unmerged path fs/squashfs/lzo_wrapper.c
* Unmerged path fs/squashfs/page_actor.c
* Unmerged path fs/squashfs/page_actor.h
diff --git a/fs/squashfs/super.c b/fs/squashfs/super.c
index 60553a9053ca..1eba199a1c6d 100644
--- a/fs/squashfs/super.c
+++ b/fs/squashfs/super.c
@@ -152,7 +152,7 @@ static int squashfs_fill_super(struct super_block *sb, void *data, int silent)
 	 * Check the system page size is not larger than the filesystem
 	 * block size (by default 128K).  This is currently not supported.
 	 */
-	if (PAGE_CACHE_SIZE > msblk->block_size) {
+	if (PAGE_SIZE > msblk->block_size) {
 		ERROR("Page size > filesystem block size (%d).  This is "
 			"currently not supported!\n", msblk->block_size);
 		goto failed_mount;
diff --git a/fs/squashfs/symlink.c b/fs/squashfs/symlink.c
index 12806dffb345..1eb1d2b8a84d 100644
--- a/fs/squashfs/symlink.c
+++ b/fs/squashfs/symlink.c
@@ -48,10 +48,10 @@ static int squashfs_symlink_readpage(struct file *file, struct page *page)
 	struct inode *inode = page->mapping->host;
 	struct super_block *sb = inode->i_sb;
 	struct squashfs_sb_info *msblk = sb->s_fs_info;
-	int index = page->index << PAGE_CACHE_SHIFT;
+	int index = page->index << PAGE_SHIFT;
 	u64 block = squashfs_i(inode)->start;
 	int offset = squashfs_i(inode)->offset;
-	int length = min_t(int, i_size_read(inode) - index, PAGE_CACHE_SIZE);
+	int length = min_t(int, i_size_read(inode) - index, PAGE_SIZE);
 	int bytes, copied;
 	void *pageaddr;
 	struct squashfs_cache_entry *entry;
@@ -94,7 +94,7 @@ static int squashfs_symlink_readpage(struct file *file, struct page *page)
 		copied = squashfs_copy_data(pageaddr + bytes, entry, offset,
 								length - bytes);
 		if (copied == length - bytes)
-			memset(pageaddr + length, 0, PAGE_CACHE_SIZE - length);
+			memset(pageaddr + length, 0, PAGE_SIZE - length);
 		else
 			block = entry->next_index;
 		kunmap_atomic(pageaddr);
* Unmerged path fs/squashfs/xz_wrapper.c
* Unmerged path fs/squashfs/zlib_wrapper.c
diff --git a/fs/sync.c b/fs/sync.c
index 422dd1823ca0..f268e7a4f3e5 100644
--- a/fs/sync.c
+++ b/fs/sync.c
@@ -311,7 +311,7 @@ SYSCALL_DEFINE4(sync_file_range, int, fd, loff_t, offset, loff_t, nbytes,
 		goto out;
 
 	if (sizeof(pgoff_t) == 4) {
-		if (offset >= (0x100000000ULL << PAGE_CACHE_SHIFT)) {
+		if (offset >= (0x100000000ULL << PAGE_SHIFT)) {
 			/*
 			 * The range starts outside a 32 bit machine's
 			 * pagecache addressing capabilities.  Let it "succeed"
@@ -319,7 +319,7 @@ SYSCALL_DEFINE4(sync_file_range, int, fd, loff_t, offset, loff_t, nbytes,
 			ret = 0;
 			goto out;
 		}
-		if (endbyte >= (0x100000000ULL << PAGE_CACHE_SHIFT)) {
+		if (endbyte >= (0x100000000ULL << PAGE_SHIFT)) {
 			/*
 			 * Out to EOF
 			 */
* Unmerged path fs/sysv/dir.c
diff --git a/fs/sysv/namei.c b/fs/sysv/namei.c
index 731b2bbcaab3..9959f1909c16 100644
--- a/fs/sysv/namei.c
+++ b/fs/sysv/namei.c
@@ -264,11 +264,11 @@ static int sysv_rename(struct inode * old_dir, struct dentry * old_dentry,
 out_dir:
 	if (dir_de) {
 		kunmap(dir_page);
-		page_cache_release(dir_page);
+		put_page(dir_page);
 	}
 out_old:
 	kunmap(old_page);
-	page_cache_release(old_page);
+	put_page(old_page);
 out:
 	return err;
 }
* Unmerged path fs/ubifs/file.c
* Unmerged path fs/ubifs/super.c
diff --git a/fs/ubifs/ubifs.h b/fs/ubifs/ubifs.h
index b2babce4d70f..8119d1371b27 100644
--- a/fs/ubifs/ubifs.h
+++ b/fs/ubifs/ubifs.h
@@ -56,8 +56,8 @@
 #define UBIFS_SUPER_MAGIC 0x24051905
 
 /* Number of UBIFS blocks per VFS page */
-#define UBIFS_BLOCKS_PER_PAGE (PAGE_CACHE_SIZE / UBIFS_BLOCK_SIZE)
-#define UBIFS_BLOCKS_PER_PAGE_SHIFT (PAGE_CACHE_SHIFT - UBIFS_BLOCK_SHIFT)
+#define UBIFS_BLOCKS_PER_PAGE (PAGE_SIZE / UBIFS_BLOCK_SIZE)
+#define UBIFS_BLOCKS_PER_PAGE_SHIFT (PAGE_SHIFT - UBIFS_BLOCK_SHIFT)
 
 /* "File system end of life" sequence number watermark */
 #define SQNUM_WARN_WATERMARK 0xFFFFFFFF00000000ULL
diff --git a/fs/udf/file.c b/fs/udf/file.c
index 29569dd08168..f52d9e4f13b3 100644
--- a/fs/udf/file.c
+++ b/fs/udf/file.c
@@ -47,7 +47,7 @@ static void __udf_adinicb_readpage(struct page *page)
 
 	kaddr = kmap(page);
 	memcpy(kaddr, iinfo->i_ext.i_data + iinfo->i_lenEAttr, inode->i_size);
-	memset(kaddr + inode->i_size, 0, PAGE_CACHE_SIZE - inode->i_size);
+	memset(kaddr + inode->i_size, 0, PAGE_SIZE - inode->i_size);
 	flush_dcache_page(page);
 	SetPageUptodate(page);
 	kunmap(page);
@@ -88,14 +88,14 @@ static int udf_adinicb_write_begin(struct file *file,
 {
 	struct page *page;
 
-	if (WARN_ON_ONCE(pos >= PAGE_CACHE_SIZE))
+	if (WARN_ON_ONCE(pos >= PAGE_SIZE))
 		return -EIO;
 	page = grab_cache_page_write_begin(mapping, 0, flags);
 	if (!page)
 		return -ENOMEM;
 	*pagep = page;
 
-	if (!PageUptodate(page) && len != PAGE_CACHE_SIZE)
+	if (!PageUptodate(page) && len != PAGE_SIZE)
 		__udf_adinicb_readpage(page);
 	return 0;
 }
diff --git a/fs/udf/inode.c b/fs/udf/inode.c
index b06be272b294..d3bbb869d060 100644
--- a/fs/udf/inode.c
+++ b/fs/udf/inode.c
@@ -289,7 +289,7 @@ int udf_expand_file_adinicb(struct inode *inode)
 	if (!PageUptodate(page)) {
 		kaddr = kmap(page);
 		memset(kaddr + iinfo->i_lenAlloc, 0x00,
-		       PAGE_CACHE_SIZE - iinfo->i_lenAlloc);
+		       PAGE_SIZE - iinfo->i_lenAlloc);
 		memcpy(kaddr, iinfo->i_ext.i_data + iinfo->i_lenEAttr,
 			iinfo->i_lenAlloc);
 		flush_dcache_page(page);
@@ -321,7 +321,7 @@ int udf_expand_file_adinicb(struct inode *inode)
 		inode->i_data.a_ops = &udf_adinicb_aops;
 		up_write(&iinfo->i_data_sem);
 	}
-	page_cache_release(page);
+	put_page(page);
 	mark_inode_dirty(inode);
 
 	return err;
diff --git a/fs/ufs/balloc.c b/fs/ufs/balloc.c
index a7ea492ae660..baf2dcfea808 100644
--- a/fs/ufs/balloc.c
+++ b/fs/ufs/balloc.c
@@ -241,7 +241,7 @@ static void ufs_change_blocknr(struct inode *inode, sector_t beg,
 			       sector_t newb, struct page *locked_page)
 {
 	const unsigned blks_per_page =
-		1 << (PAGE_CACHE_SHIFT - inode->i_blkbits);
+		1 << (PAGE_SHIFT - inode->i_blkbits);
 	const unsigned mask = blks_per_page - 1;
 	struct address_space * const mapping = inode->i_mapping;
 	pgoff_t index, cur_index, last_index;
@@ -259,9 +259,9 @@ static void ufs_change_blocknr(struct inode *inode, sector_t beg,
 
 	cur_index = locked_page->index;
 	end = count + beg;
-	last_index = end >> (PAGE_CACHE_SHIFT - inode->i_blkbits);
+	last_index = end >> (PAGE_SHIFT - inode->i_blkbits);
 	for (i = beg; i < end; i = (i | mask) + 1) {
-		index = i >> (PAGE_CACHE_SHIFT - inode->i_blkbits);
+		index = i >> (PAGE_SHIFT - inode->i_blkbits);
 
 		if (likely(cur_index != index)) {
 			page = ufs_get_locked_page(mapping, index);
* Unmerged path fs/ufs/dir.c
* Unmerged path fs/ufs/inode.c
* Unmerged path fs/ufs/namei.c
diff --git a/fs/ufs/util.c b/fs/ufs/util.c
index b6c2f94e041e..a409e3e7827a 100644
--- a/fs/ufs/util.c
+++ b/fs/ufs/util.c
@@ -261,14 +261,14 @@ struct page *ufs_get_locked_page(struct address_space *mapping,
 		if (unlikely(page->mapping == NULL)) {
 			/* Truncate got there first */
 			unlock_page(page);
-			page_cache_release(page);
+			put_page(page);
 			page = NULL;
 			goto out;
 		}
 
 		if (!PageUptodate(page) || PageError(page)) {
 			unlock_page(page);
-			page_cache_release(page);
+			put_page(page);
 
 			printk(KERN_ERR "ufs_change_blocknr: "
 			       "can not read page: ino %lu, index: %lu\n",
diff --git a/fs/ufs/util.h b/fs/ufs/util.h
index 954175928240..b7fbf53dbc81 100644
--- a/fs/ufs/util.h
+++ b/fs/ufs/util.h
@@ -283,7 +283,7 @@ extern struct page *ufs_get_locked_page(struct address_space *mapping,
 static inline void ufs_put_locked_page(struct page *page)
 {
        unlock_page(page);
-       page_cache_release(page);
+       put_page(page);
 }
 
 
diff --git a/fs/xfs/libxfs/xfs_bmap.c b/fs/xfs/libxfs/xfs_bmap.c
index 3958c26b80e1..c31b9994ec9e 100644
--- a/fs/xfs/libxfs/xfs_bmap.c
+++ b/fs/xfs/libxfs/xfs_bmap.c
@@ -3745,11 +3745,11 @@ xfs_bmap_btalloc(
 		args.prod = align;
 		if ((args.mod = (xfs_extlen_t)do_mod(ap->offset, args.prod)))
 			args.mod = (xfs_extlen_t)(args.prod - args.mod);
-	} else if (mp->m_sb.sb_blocksize >= PAGE_CACHE_SIZE) {
+	} else if (mp->m_sb.sb_blocksize >= PAGE_SIZE) {
 		args.prod = 1;
 		args.mod = 0;
 	} else {
-		args.prod = PAGE_CACHE_SIZE >> mp->m_sb.sb_blocklog;
+		args.prod = PAGE_SIZE >> mp->m_sb.sb_blocklog;
 		if ((args.mod = (xfs_extlen_t)(do_mod(ap->offset, args.prod))))
 			args.mod = (xfs_extlen_t)(args.prod - args.mod);
 	}
* Unmerged path fs/xfs/xfs_aops.c
* Unmerged path fs/xfs/xfs_bmap_util.c
* Unmerged path fs/xfs/xfs_file.c
diff --git a/fs/xfs/xfs_linux.h b/fs/xfs/xfs_linux.h
index ec0e239a0fa9..a8192dc797dc 100644
--- a/fs/xfs/xfs_linux.h
+++ b/fs/xfs/xfs_linux.h
@@ -135,7 +135,7 @@ typedef __u32			xfs_nlink_t;
  * Size of block device i/o is parameterized here.
  * Currently the system supports page-sized i/o.
  */
-#define	BLKDEV_IOSHIFT		PAGE_CACHE_SHIFT
+#define	BLKDEV_IOSHIFT		PAGE_SHIFT
 #define	BLKDEV_IOSIZE		(1<<BLKDEV_IOSHIFT)
 /* number of BB's per block device block */
 #define	BLKDEV_BB		BTOBB(BLKDEV_IOSIZE)
diff --git a/fs/xfs/xfs_mount.c b/fs/xfs/xfs_mount.c
index 37d45fa3d316..6b8724cb8ca8 100644
--- a/fs/xfs/xfs_mount.c
+++ b/fs/xfs/xfs_mount.c
@@ -170,7 +170,7 @@ xfs_sb_validate_fsb_count(
 	ASSERT(sbp->sb_blocklog >= BBSHIFT);
 
 	/* Limited by ULONG_MAX of page cache index */
-	if (nblocks >> (PAGE_CACHE_SHIFT - sbp->sb_blocklog) > ULONG_MAX)
+	if (nblocks >> (PAGE_SHIFT - sbp->sb_blocklog) > ULONG_MAX)
 		return -EFBIG;
 	return 0;
 }
diff --git a/fs/xfs/xfs_mount.h b/fs/xfs/xfs_mount.h
index e75216060319..397d4df1bccc 100644
--- a/fs/xfs/xfs_mount.h
+++ b/fs/xfs/xfs_mount.h
@@ -257,12 +257,12 @@ static inline unsigned long
 xfs_preferred_iosize(xfs_mount_t *mp)
 {
 	if (mp->m_flags & XFS_MOUNT_COMPAT_IOSIZE)
-		return PAGE_CACHE_SIZE;
+		return PAGE_SIZE;
 	return (mp->m_swidth ?
 		(mp->m_swidth << mp->m_sb.sb_blocklog) :
 		((mp->m_flags & XFS_MOUNT_DFLT_IOSIZE) ?
 			(1 << (int)MAX(mp->m_readio_log, mp->m_writeio_log)) :
-			PAGE_CACHE_SIZE));
+			PAGE_SIZE));
 }
 
 #define XFS_LAST_UNMOUNT_WAS_CLEAN(mp)	\
diff --git a/fs/xfs/xfs_pnfs.c b/fs/xfs/xfs_pnfs.c
index dc6221942b85..9181eb61d10e 100644
--- a/fs/xfs/xfs_pnfs.c
+++ b/fs/xfs/xfs_pnfs.c
@@ -293,8 +293,8 @@ xfs_fs_commit_blocks(
 		 * Make sure reads through the pagecache see the new data.
 		 */
 		error = invalidate_inode_pages2_range(inode->i_mapping,
-					start >> PAGE_CACHE_SHIFT,
-					(end - 1) >> PAGE_CACHE_SHIFT);
+					start >> PAGE_SHIFT,
+					(end - 1) >> PAGE_SHIFT);
 		WARN_ON_ONCE(error);
 
 		error = xfs_iomap_write_unwritten(ip, start, length);
diff --git a/fs/xfs/xfs_super.c b/fs/xfs/xfs_super.c
index 4dffe6e01151..3e91be838946 100644
--- a/fs/xfs/xfs_super.c
+++ b/fs/xfs/xfs_super.c
@@ -582,10 +582,10 @@ xfs_max_file_offset(
 #if BITS_PER_LONG == 32
 # if defined(CONFIG_LBDAF)
 	ASSERT(sizeof(sector_t) == 8);
-	pagefactor = PAGE_CACHE_SIZE;
+	pagefactor = PAGE_SIZE;
 	bitshift = BITS_PER_LONG;
 # else
-	pagefactor = PAGE_CACHE_SIZE >> (PAGE_CACHE_SHIFT - blockshift);
+	pagefactor = PAGE_SIZE >> (PAGE_SHIFT - blockshift);
 # endif
 #endif
 
diff --git a/include/linux/bio.h b/include/linux/bio.h
index ab342b2c644b..fee59776f1fa 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -41,7 +41,7 @@
 #endif
 
 #define BIO_MAX_PAGES		256
-#define BIO_MAX_SIZE		(BIO_MAX_PAGES << PAGE_CACHE_SHIFT)
+#define BIO_MAX_SIZE		(BIO_MAX_PAGES << PAGE_SHIFT)
 #define BIO_MAX_SECTORS		(BIO_MAX_SIZE >> 9)
 
 /*
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index d0c02b856b7c..4b623d6a53f5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -1473,7 +1473,7 @@ unsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);
 
 static inline void put_dev_sector(Sector p)
 {
-	page_cache_release(p.v);
+	put_page(p.v);
 }
 
 static inline bool __bvec_gap_to_prev(struct request_queue *q,
diff --git a/include/linux/buffer_head.h b/include/linux/buffer_head.h
index db89952cb69d..f1b6e8f5d808 100644
--- a/include/linux/buffer_head.h
+++ b/include/linux/buffer_head.h
@@ -44,7 +44,7 @@ enum bh_state_bits {
 					       * workqueue */
 };
 
-#define MAX_BUF_PER_PAGE (PAGE_CACHE_SIZE / 512)
+#define MAX_BUF_PER_PAGE (PAGE_SIZE / 512)
 
 struct page;
 struct buffer_head;
@@ -269,7 +269,7 @@ void buffer_init(void);
 static inline void attach_page_buffers(struct page *page,
 		struct buffer_head *head)
 {
-	page_cache_get(page);
+	get_page(page);
 	SetPagePrivate(page);
 	set_page_private(page, (unsigned long)head);
 }
diff --git a/include/linux/ceph/libceph.h b/include/linux/ceph/libceph.h
index 4ab7257023e0..8cb8053bbccc 100644
--- a/include/linux/ceph/libceph.h
+++ b/include/linux/ceph/libceph.h
@@ -176,8 +176,8 @@ extern void ceph_put_snap_context(struct ceph_snap_context *sc);
  */
 static inline int calc_pages_for(u64 off, u64 len)
 {
-	return ((off+len+PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT) -
-		(off >> PAGE_CACHE_SHIFT);
+	return ((off+len+PAGE_SIZE-1) >> PAGE_SHIFT) -
+		(off >> PAGE_SHIFT);
 }
 
 /*
diff --git a/include/linux/f2fs_fs.h b/include/linux/f2fs_fs.h
index df6fab82f87e..e8ff0338add7 100644
--- a/include/linux/f2fs_fs.h
+++ b/include/linux/f2fs_fs.h
@@ -212,7 +212,7 @@ struct f2fs_node {
 /*
  * For NAT entries
  */
-#define NAT_ENTRY_PER_BLOCK (PAGE_CACHE_SIZE / sizeof(struct f2fs_nat_entry))
+#define NAT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_nat_entry))
 
 struct f2fs_nat_entry {
 	__u8 version;		/* latest version of cached nat entry */
@@ -232,7 +232,7 @@ struct f2fs_nat_block {
  * Not allow to change this.
  */
 #define SIT_VBLOCK_MAP_SIZE 64
-#define SIT_ENTRY_PER_BLOCK (PAGE_CACHE_SIZE / sizeof(struct f2fs_sit_entry))
+#define SIT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_sit_entry))
 
 /*
  * Note that f2fs_sit_entry->vblocks has the following bit-field information.
* Unmerged path include/linux/fs.h
diff --git a/include/linux/nfs_page.h b/include/linux/nfs_page.h
index f2f650f136ee..efada239205e 100644
--- a/include/linux/nfs_page.h
+++ b/include/linux/nfs_page.h
@@ -184,7 +184,7 @@ nfs_list_entry(struct list_head *head)
 static inline
 loff_t req_offset(struct nfs_page *req)
 {
-	return (((loff_t)req->wb_index) << PAGE_CACHE_SHIFT) + req->wb_offset;
+	return (((loff_t)req->wb_index) << PAGE_SHIFT) + req->wb_offset;
 }
 
 #endif /* _LINUX_NFS_PAGE_H */
* Unmerged path include/linux/pagemap.h
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 030df6e74d64..a008ee342da7 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -456,7 +456,7 @@ mem_cgroup_uncharge_swapcache(struct page *page, swp_entry_t ent, bool swapout)
 /* only sparc can not include linux/pagemap.h in this file
  * so leave page_cache_release and release_pages undeclared... */
 #define free_page_and_swap_cache(page) \
-	page_cache_release(page)
+	put_page(page)
 #define free_pages_and_swap_cache(pages, nr) \
 	release_pages((pages), (nr), false);
 
diff --git a/ipc/mqueue.c b/ipc/mqueue.c
index b8d4aed45b8c..117027a44b4a 100644
--- a/ipc/mqueue.c
+++ b/ipc/mqueue.c
@@ -311,8 +311,8 @@ static int mqueue_fill_super(struct super_block *sb, void *data, int silent)
 	struct inode *inode;
 	struct ipc_namespace *ns = data;
 
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = MQUEUE_MAGIC;
 	sb->s_op = &mqueue_super_ops;
 
* Unmerged path kernel/events/uprobes.c
* Unmerged path mm/fadvise.c
* Unmerged path mm/filemap.c
* Unmerged path mm/hugetlb.c
diff --git a/mm/madvise.c b/mm/madvise.c
index 73378fb832b8..96a08980460a 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -165,7 +165,7 @@ static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,
 		page = read_swap_cache_async(entry, GFP_HIGHUSER_MOVABLE,
 								vma, index);
 		if (page)
-			page_cache_release(page);
+			put_page(page);
 	}
 
 	return 0;
@@ -199,14 +199,14 @@ static void force_shm_swapin_readahead(struct vm_area_struct *vma,
 		page = __find_get_page(mapping, index);
 		if (!radix_tree_exceptional_entry(page)) {
 			if (page)
-				page_cache_release(page);
+				put_page(page);
 			continue;
 		}
 		swap = radix_to_swp_entry(page);
 		page = read_swap_cache_async(swap, GFP_HIGHUSER_MOVABLE,
 								NULL, 0);
 		if (page)
-			page_cache_release(page);
+			put_page(page);
 	}
 
 	lru_add_drain();	/* Push any new pages onto the LRU now */
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index dd86b7b0bf42..a92bb0ed764e 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -553,7 +553,7 @@ static int delete_from_lru_cache(struct page *p)
 		/*
 		 * drop the page count elevated by isolate_lru_page()
 		 */
-		page_cache_release(p);
+		put_page(p);
 		return 0;
 	}
 	return -EIO;
* Unmerged path mm/memory.c
diff --git a/mm/mincore.c b/mm/mincore.c
index ad411ec86a55..589e20f42106 100644
--- a/mm/mincore.c
+++ b/mm/mincore.c
@@ -88,7 +88,7 @@ static unsigned char mincore_page(struct address_space *mapping, pgoff_t pgoff)
 #endif
 	if (page) {
 		present = PageUptodate(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 
 	return present;
@@ -280,7 +280,7 @@ SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,
 	unsigned char *tmp;
 
 	/* Check the start address: needs to be page-aligned.. */
- 	if (start & ~PAGE_CACHE_MASK)
+	if (start & ~PAGE_MASK)
 		return -EINVAL;
 
 	/* ..and we need to be passed a valid user-space range */
diff --git a/mm/nommu.c b/mm/nommu.c
index 33d45217ae7b..308c3fec96c2 100644
--- a/mm/nommu.c
+++ b/mm/nommu.c
@@ -173,7 +173,7 @@ long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 		if (pages) {
 			pages[i] = virt_to_page(start);
 			if (pages[i])
-				page_cache_get(pages[i]);
+				get_page(pages[i]);
 		}
 		if (vmas)
 			vmas[i] = vma;
* Unmerged path mm/page-writeback.c
diff --git a/mm/page_io.c b/mm/page_io.c
index 01eaa01e62d0..a3bf7b416d46 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -205,7 +205,7 @@ out:
 
 static sector_t swap_page_sector(struct page *page)
 {
-	return (sector_t)__page_file_index(page) << (PAGE_CACHE_SHIFT - 9);
+	return (sector_t)__page_file_index(page) << (PAGE_SHIFT - 9);
 }
 
 int __swap_writepage(struct page *page, struct writeback_control *wbc,
* Unmerged path mm/readahead.c
* Unmerged path mm/rmap.c
* Unmerged path mm/shmem.c
* Unmerged path mm/swap.c
* Unmerged path mm/swap_state.c
diff --git a/mm/swapfile.c b/mm/swapfile.c
index 44c2eac6b890..2057371cd464 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -119,7 +119,7 @@ __try_to_reclaim_swap(struct swap_info_struct *si, unsigned long offset)
 		ret = try_to_free_swap(page);
 		unlock_page(page);
 	}
-	page_cache_release(page);
+	put_page(page);
 	return ret;
 }
 
@@ -755,7 +755,7 @@ int free_swap_and_cache(swp_entry_t entry)
 			page = find_get_page(swap_address_space(entry),
 						entry.val);
 			if (page && !trylock_page(page)) {
-				page_cache_release(page);
+				put_page(page);
 				page = NULL;
 			}
 		}
@@ -772,7 +772,7 @@ int free_swap_and_cache(swp_entry_t entry)
 			SetPageDirty(page);
 		}
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 	return p != NULL;
 }
@@ -1268,7 +1268,7 @@ int try_to_unuse(unsigned int type, bool frontswap,
 		}
 		if (retval) {
 			unlock_page(page);
-			page_cache_release(page);
+			put_page(page);
 			break;
 		}
 
@@ -1320,7 +1320,7 @@ int try_to_unuse(unsigned int type, bool frontswap,
 		 */
 		SetPageDirty(page);
 		unlock_page(page);
-		page_cache_release(page);
+		put_page(page);
 
 		/*
 		 * Make sure that we aren't completely killing
@@ -2243,7 +2243,7 @@ bad_swap:
 out:
 	if (page && !IS_ERR(page)) {
 		kunmap(page);
-		page_cache_release(page);
+		put_page(page);
 	}
 	if (name)
 		putname(name);
* Unmerged path mm/truncate.c
diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c
index 289f5c06f5c5..c88c6d3759bf 100644
--- a/mm/userfaultfd.c
+++ b/mm/userfaultfd.c
@@ -90,7 +90,7 @@ out_release_uncharge_unlock:
 	pte_unmap_unlock(dst_pte, ptl);
 	mem_cgroup_uncharge_page(page);
 out_release:
-	page_cache_release(page);
+	put_page(page);
 	goto out;
 }
 
@@ -285,7 +285,7 @@ out_unlock:
 	up_read(&dst_mm->mmap_sem);
 out:
 	if (page)
-		page_cache_release(page);
+		put_page(page);
 	BUG_ON(copied < 0);
 	BUG_ON(err > 0);
 	BUG_ON(!copied && !err);
diff --git a/mm/zswap.c b/mm/zswap.c
index 4eb661cab809..28791413774a 100644
--- a/mm/zswap.c
+++ b/mm/zswap.c
@@ -571,7 +571,7 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 
 	case ZSWAP_SWAPCACHE_EXIST:
 		/* page is already in the swap cache, ignore for now */
-		page_cache_release(page);
+		put_page(page);
 		ret = -EEXIST;
 		goto fail;
 
@@ -597,7 +597,7 @@ static int zswap_writeback_entry(struct zpool *pool, unsigned long handle)
 
 	/* start writeback */
 	__swap_writepage(page, &wbc, end_swap_bio_write);
-	page_cache_release(page);
+	put_page(page);
 	zswap_written_back_pages++;
 
 	spin_lock(&tree->lock);
diff --git a/net/ceph/messenger.c b/net/ceph/messenger.c
index 586ae7af5cef..4404eaa22b65 100644
--- a/net/ceph/messenger.c
+++ b/net/ceph/messenger.c
@@ -269,7 +269,7 @@ static void _ceph_msgr_exit(void)
 	}
 
 	BUG_ON(zero_page == NULL);
-	page_cache_release(zero_page);
+	put_page(zero_page);
 	zero_page = NULL;
 
 	ceph_msgr_slab_exit();
@@ -282,7 +282,7 @@ int ceph_msgr_init(void)
 
 	BUG_ON(zero_page != NULL);
 	zero_page = ZERO_PAGE(0);
-	page_cache_get(zero_page);
+	get_page(zero_page);
 
 	ceph_msgr_wq = alloc_workqueue("ceph-msgr",
 				       WQ_NON_REENTRANT | WQ_MEM_RECLAIM, 0);
@@ -1598,7 +1598,7 @@ static int write_partial_skip(struct ceph_connection *con)
 
 	dout("%s %p %d left\n", __func__, con, con->out_skip);
 	while (con->out_skip > 0) {
-		size_t size = min(con->out_skip, (int) PAGE_CACHE_SIZE);
+		size_t size = min(con->out_skip, (int) PAGE_SIZE);
 
 		ret = ceph_tcp_sendpage(con->sock, zero_page, 0, size, true);
 		if (ret <= 0)
diff --git a/net/ceph/pagelist.c b/net/ceph/pagelist.c
index c7c220a736e5..6864007e64fc 100644
--- a/net/ceph/pagelist.c
+++ b/net/ceph/pagelist.c
@@ -56,7 +56,7 @@ int ceph_pagelist_append(struct ceph_pagelist *pl, const void *buf, size_t len)
 		size_t bit = pl->room;
 		int ret;
 
-		memcpy(pl->mapped_tail + (pl->length & ~PAGE_CACHE_MASK),
+		memcpy(pl->mapped_tail + (pl->length & ~PAGE_MASK),
 		       buf, bit);
 		pl->length += bit;
 		pl->room -= bit;
@@ -67,7 +67,7 @@ int ceph_pagelist_append(struct ceph_pagelist *pl, const void *buf, size_t len)
 			return ret;
 	}
 
-	memcpy(pl->mapped_tail + (pl->length & ~PAGE_CACHE_MASK), buf, len);
+	memcpy(pl->mapped_tail + (pl->length & ~PAGE_MASK), buf, len);
 	pl->length += len;
 	pl->room -= len;
 	return 0;
diff --git a/net/ceph/pagevec.c b/net/ceph/pagevec.c
index 815a2249cfa9..f5d284440c4c 100644
--- a/net/ceph/pagevec.c
+++ b/net/ceph/pagevec.c
@@ -97,19 +97,19 @@ int ceph_copy_user_to_page_vector(struct page **pages,
 					 loff_t off, size_t len)
 {
 	int i = 0;
-	int po = off & ~PAGE_CACHE_MASK;
+	int po = off & ~PAGE_MASK;
 	int left = len;
 	int l, bad;
 
 	while (left > 0) {
-		l = min_t(int, PAGE_CACHE_SIZE-po, left);
+		l = min_t(int, PAGE_SIZE-po, left);
 		bad = copy_from_user(page_address(pages[i]) + po, data, l);
 		if (bad == l)
 			return -EFAULT;
 		data += l - bad;
 		left -= l - bad;
 		po += l - bad;
-		if (po == PAGE_CACHE_SIZE) {
+		if (po == PAGE_SIZE) {
 			po = 0;
 			i++;
 		}
@@ -123,17 +123,17 @@ void ceph_copy_to_page_vector(struct page **pages,
 				    loff_t off, size_t len)
 {
 	int i = 0;
-	size_t po = off & ~PAGE_CACHE_MASK;
+	size_t po = off & ~PAGE_MASK;
 	size_t left = len;
 
 	while (left > 0) {
-		size_t l = min_t(size_t, PAGE_CACHE_SIZE-po, left);
+		size_t l = min_t(size_t, PAGE_SIZE-po, left);
 
 		memcpy(page_address(pages[i]) + po, data, l);
 		data += l;
 		left -= l;
 		po += l;
-		if (po == PAGE_CACHE_SIZE) {
+		if (po == PAGE_SIZE) {
 			po = 0;
 			i++;
 		}
@@ -146,17 +146,17 @@ void ceph_copy_from_page_vector(struct page **pages,
 				    loff_t off, size_t len)
 {
 	int i = 0;
-	size_t po = off & ~PAGE_CACHE_MASK;
+	size_t po = off & ~PAGE_MASK;
 	size_t left = len;
 
 	while (left > 0) {
-		size_t l = min_t(size_t, PAGE_CACHE_SIZE-po, left);
+		size_t l = min_t(size_t, PAGE_SIZE-po, left);
 
 		memcpy(data, page_address(pages[i]) + po, l);
 		data += l;
 		left -= l;
 		po += l;
-		if (po == PAGE_CACHE_SIZE) {
+		if (po == PAGE_SIZE) {
 			po = 0;
 			i++;
 		}
@@ -200,25 +200,25 @@ EXPORT_SYMBOL(ceph_copy_page_vector_to_user);
  */
 void ceph_zero_page_vector_range(int off, int len, struct page **pages)
 {
-	int i = off >> PAGE_CACHE_SHIFT;
+	int i = off >> PAGE_SHIFT;
 
-	off &= ~PAGE_CACHE_MASK;
+	off &= ~PAGE_MASK;
 
 	dout("zero_page_vector_page %u~%u\n", off, len);
 
 	/* leading partial page? */
 	if (off) {
-		int end = min((int)PAGE_CACHE_SIZE, off + len);
+		int end = min((int)PAGE_SIZE, off + len);
 		dout("zeroing %d %p head from %d\n", i, pages[i],
 		     (int)off);
 		zero_user_segment(pages[i], off, end);
 		len -= (end - off);
 		i++;
 	}
-	while (len >= PAGE_CACHE_SIZE) {
+	while (len >= PAGE_SIZE) {
 		dout("zeroing %d %p len=%d\n", i, pages[i], len);
-		zero_user_segment(pages[i], 0, PAGE_CACHE_SIZE);
-		len -= PAGE_CACHE_SIZE;
+		zero_user_segment(pages[i], 0, PAGE_SIZE);
+		len -= PAGE_SIZE;
 		i++;
 	}
 	/* trailing partial page? */
diff --git a/net/sunrpc/auth_gss/auth_gss.c b/net/sunrpc/auth_gss/auth_gss.c
index b3b6700100eb..89e98afa8cd8 100644
--- a/net/sunrpc/auth_gss/auth_gss.c
+++ b/net/sunrpc/auth_gss/auth_gss.c
@@ -1737,8 +1737,8 @@ alloc_enc_pages(struct rpc_rqst *rqstp)
 		return 0;
 	}
 
-	first = snd_buf->page_base >> PAGE_CACHE_SHIFT;
-	last = (snd_buf->page_base + snd_buf->page_len - 1) >> PAGE_CACHE_SHIFT;
+	first = snd_buf->page_base >> PAGE_SHIFT;
+	last = (snd_buf->page_base + snd_buf->page_len - 1) >> PAGE_SHIFT;
 	rqstp->rq_enc_pages_num = last - first + 1 + 1;
 	rqstp->rq_enc_pages
 		= kmalloc(rqstp->rq_enc_pages_num * sizeof(struct page *),
@@ -1784,10 +1784,10 @@ gss_wrap_req_priv(struct rpc_cred *cred, struct gss_cl_ctx *ctx,
 	status = alloc_enc_pages(rqstp);
 	if (status)
 		return status;
-	first = snd_buf->page_base >> PAGE_CACHE_SHIFT;
+	first = snd_buf->page_base >> PAGE_SHIFT;
 	inpages = snd_buf->pages + first;
 	snd_buf->pages = rqstp->rq_enc_pages;
-	snd_buf->page_base -= first << PAGE_CACHE_SHIFT;
+	snd_buf->page_base -= first << PAGE_SHIFT;
 	/*
 	 * Give the tail its own page, in case we need extra space in the
 	 * head when wrapping:
diff --git a/net/sunrpc/auth_gss/gss_krb5_crypto.c b/net/sunrpc/auth_gss/gss_krb5_crypto.c
index fee3c15a4b52..9c120db3af7b 100644
--- a/net/sunrpc/auth_gss/gss_krb5_crypto.c
+++ b/net/sunrpc/auth_gss/gss_krb5_crypto.c
@@ -414,7 +414,7 @@ encryptor(struct scatterlist *sg, void *data)
 	page_pos = desc->pos - outbuf->head[0].iov_len;
 	if (page_pos >= 0 && page_pos < outbuf->page_len) {
 		/* pages are not in place: */
-		int i = (page_pos + outbuf->page_base) >> PAGE_CACHE_SHIFT;
+		int i = (page_pos + outbuf->page_base) >> PAGE_SHIFT;
 		in_page = desc->pages[i];
 	} else {
 		in_page = sg_page(sg);
diff --git a/net/sunrpc/auth_gss/gss_krb5_wrap.c b/net/sunrpc/auth_gss/gss_krb5_wrap.c
index 18b5916f9f7b..9e2bf516b3ca 100644
--- a/net/sunrpc/auth_gss/gss_krb5_wrap.c
+++ b/net/sunrpc/auth_gss/gss_krb5_wrap.c
@@ -79,9 +79,9 @@ gss_krb5_remove_padding(struct xdr_buf *buf, int blocksize)
 		len -= buf->head[0].iov_len;
 	if (len <= buf->page_len) {
 		unsigned int last = (buf->page_base + len - 1)
-					>>PAGE_CACHE_SHIFT;
+					>>PAGE_SHIFT;
 		unsigned int offset = (buf->page_base + len - 1)
-					& (PAGE_CACHE_SIZE - 1);
+					& (PAGE_SIZE - 1);
 		ptr = kmap_atomic(buf->pages[last]);
 		pad = *(ptr + offset);
 		kunmap_atomic(ptr);
diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index abb7d5a578cf..e68ede0c33df 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -885,7 +885,7 @@ static ssize_t cache_downcall(struct address_space *mapping,
 	char *kaddr;
 	ssize_t ret = -ENOMEM;
 
-	if (count >= PAGE_CACHE_SIZE)
+	if (count >= PAGE_SIZE)
 		goto out_slow;
 
 	page = find_or_create_page(mapping, 0, GFP_KERNEL);
@@ -896,7 +896,7 @@ static ssize_t cache_downcall(struct address_space *mapping,
 	ret = cache_do_downcall(kaddr, buf, count, cd);
 	kunmap(page);
 	unlock_page(page);
-	page_cache_release(page);
+	put_page(page);
 	return ret;
 out_slow:
 	return cache_slow_downcall(buf, count, cd);
diff --git a/net/sunrpc/rpc_pipe.c b/net/sunrpc/rpc_pipe.c
index 5f1269892c2e..5853e3bd4f26 100644
--- a/net/sunrpc/rpc_pipe.c
+++ b/net/sunrpc/rpc_pipe.c
@@ -1399,8 +1399,8 @@ rpc_fill_super(struct super_block *sb, void *data, int silent)
 	struct sunrpc_net *sn = net_generic(net, sunrpc_net_id);
 	int err;
 
-	sb->s_blocksize = PAGE_CACHE_SIZE;
-	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_blocksize = PAGE_SIZE;
+	sb->s_blocksize_bits = PAGE_SHIFT;
 	sb->s_magic = RPCAUTH_GSSMAGIC;
 	sb->s_op = &s_ops;
 	sb->s_d_op = &rpc_dentry_operations;
diff --git a/net/sunrpc/socklib.c b/net/sunrpc/socklib.c
index 2df87f78e518..de70c78025d7 100644
--- a/net/sunrpc/socklib.c
+++ b/net/sunrpc/socklib.c
@@ -96,8 +96,8 @@ ssize_t xdr_partial_copy_from_skb(struct xdr_buf *xdr, unsigned int base, struct
 	if (base || xdr->page_base) {
 		pglen -= base;
 		base += xdr->page_base;
-		ppage += base >> PAGE_CACHE_SHIFT;
-		base &= ~PAGE_CACHE_MASK;
+		ppage += base >> PAGE_SHIFT;
+		base &= ~PAGE_MASK;
 	}
 	do {
 		char *kaddr;
@@ -113,7 +113,7 @@ ssize_t xdr_partial_copy_from_skb(struct xdr_buf *xdr, unsigned int base, struct
 			}
 		}
 
-		len = PAGE_CACHE_SIZE;
+		len = PAGE_SIZE;
 		kaddr = kmap_atomic(*ppage);
 		if (base) {
 			len -= base;
diff --git a/net/sunrpc/xdr.c b/net/sunrpc/xdr.c
index 18309b895cce..2944dcbc36bb 100644
--- a/net/sunrpc/xdr.c
+++ b/net/sunrpc/xdr.c
@@ -181,20 +181,20 @@ _shift_data_right_pages(struct page **pages, size_t pgto_base,
 	pgto_base += len;
 	pgfrom_base += len;
 
-	pgto = pages + (pgto_base >> PAGE_CACHE_SHIFT);
-	pgfrom = pages + (pgfrom_base >> PAGE_CACHE_SHIFT);
+	pgto = pages + (pgto_base >> PAGE_SHIFT);
+	pgfrom = pages + (pgfrom_base >> PAGE_SHIFT);
 
-	pgto_base &= ~PAGE_CACHE_MASK;
-	pgfrom_base &= ~PAGE_CACHE_MASK;
+	pgto_base &= ~PAGE_MASK;
+	pgfrom_base &= ~PAGE_MASK;
 
 	do {
 		/* Are any pointers crossing a page boundary? */
 		if (pgto_base == 0) {
-			pgto_base = PAGE_CACHE_SIZE;
+			pgto_base = PAGE_SIZE;
 			pgto--;
 		}
 		if (pgfrom_base == 0) {
-			pgfrom_base = PAGE_CACHE_SIZE;
+			pgfrom_base = PAGE_SIZE;
 			pgfrom--;
 		}
 
@@ -236,11 +236,11 @@ _copy_to_pages(struct page **pages, size_t pgbase, const char *p, size_t len)
 	char *vto;
 	size_t copy;
 
-	pgto = pages + (pgbase >> PAGE_CACHE_SHIFT);
-	pgbase &= ~PAGE_CACHE_MASK;
+	pgto = pages + (pgbase >> PAGE_SHIFT);
+	pgbase &= ~PAGE_MASK;
 
 	for (;;) {
-		copy = PAGE_CACHE_SIZE - pgbase;
+		copy = PAGE_SIZE - pgbase;
 		if (copy > len)
 			copy = len;
 
@@ -253,7 +253,7 @@ _copy_to_pages(struct page **pages, size_t pgbase, const char *p, size_t len)
 			break;
 
 		pgbase += copy;
-		if (pgbase == PAGE_CACHE_SIZE) {
+		if (pgbase == PAGE_SIZE) {
 			flush_dcache_page(*pgto);
 			pgbase = 0;
 			pgto++;
@@ -280,11 +280,11 @@ _copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)
 	char *vfrom;
 	size_t copy;
 
-	pgfrom = pages + (pgbase >> PAGE_CACHE_SHIFT);
-	pgbase &= ~PAGE_CACHE_MASK;
+	pgfrom = pages + (pgbase >> PAGE_SHIFT);
+	pgbase &= ~PAGE_MASK;
 
 	do {
-		copy = PAGE_CACHE_SIZE - pgbase;
+		copy = PAGE_SIZE - pgbase;
 		if (copy > len)
 			copy = len;
 
@@ -293,7 +293,7 @@ _copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)
 		kunmap_atomic(vfrom);
 
 		pgbase += copy;
-		if (pgbase == PAGE_CACHE_SIZE) {
+		if (pgbase == PAGE_SIZE) {
 			pgbase = 0;
 			pgfrom++;
 		}
@@ -1027,8 +1027,8 @@ xdr_buf_subsegment(struct xdr_buf *buf, struct xdr_buf *subbuf,
 	if (base < buf->page_len) {
 		subbuf->page_len = min(buf->page_len - base, len);
 		base += buf->page_base;
-		subbuf->page_base = base & ~PAGE_CACHE_MASK;
-		subbuf->pages = &buf->pages[base >> PAGE_CACHE_SHIFT];
+		subbuf->page_base = base & ~PAGE_MASK;
+		subbuf->pages = &buf->pages[base >> PAGE_SHIFT];
 		len -= subbuf->page_len;
 		base = 0;
 	} else {
@@ -1287,9 +1287,9 @@ xdr_xcode_array2(struct xdr_buf *buf, unsigned int base,
 		todo -= avail_here;
 
 		base += buf->page_base;
-		ppages = buf->pages + (base >> PAGE_CACHE_SHIFT);
-		base &= ~PAGE_CACHE_MASK;
-		avail_page = min_t(unsigned int, PAGE_CACHE_SIZE - base,
+		ppages = buf->pages + (base >> PAGE_SHIFT);
+		base &= ~PAGE_MASK;
+		avail_page = min_t(unsigned int, PAGE_SIZE - base,
 					avail_here);
 		c = kmap(*ppages) + base;
 
@@ -1373,7 +1373,7 @@ xdr_xcode_array2(struct xdr_buf *buf, unsigned int base,
 			}
 
 			avail_page = min(avail_here,
-				 (unsigned int) PAGE_CACHE_SIZE);
+				 (unsigned int) PAGE_SIZE);
 		}
 		base = buf->page_len;  /* align to start of tail */
 	}
@@ -1469,9 +1469,9 @@ xdr_process_buf(struct xdr_buf *buf, unsigned int offset, unsigned int len,
 		if (page_len > len)
 			page_len = len;
 		len -= page_len;
-		page_offset = (offset + buf->page_base) & (PAGE_CACHE_SIZE - 1);
-		i = (offset + buf->page_base) >> PAGE_CACHE_SHIFT;
-		thislen = PAGE_CACHE_SIZE - page_offset;
+		page_offset = (offset + buf->page_base) & (PAGE_SIZE - 1);
+		i = (offset + buf->page_base) >> PAGE_SHIFT;
+		thislen = PAGE_SIZE - page_offset;
 		do {
 			if (thislen > page_len)
 				thislen = page_len;
@@ -1482,7 +1482,7 @@ xdr_process_buf(struct xdr_buf *buf, unsigned int offset, unsigned int len,
 			page_len -= thislen;
 			i++;
 			page_offset = 0;
-			thislen = PAGE_CACHE_SIZE;
+			thislen = PAGE_SIZE;
 		} while (page_len != 0);
 		offset = 0;
 	}
