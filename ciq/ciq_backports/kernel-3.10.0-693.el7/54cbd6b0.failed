xprtrdma: Delay DMA mapping Send and Receive buffers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 54cbd6b0c6b9410782da3efe7377d43bb636faaf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/54cbd6b0.failed

Currently, each regbuf is allocated and DMA mapped at the same time.
This is done during transport creation.

When a device driver is unloaded, every DMA-mapped buffer in use by
a transport has to be unmapped, and then remapped to the new
device if the driver is loaded again. Remapping will have to be done
_after_ the connect worker has set up the new device.

But there's an ordering problem:

call_allocate, which invokes xprt_rdma_allocate which calls
rpcrdma_alloc_regbuf to allocate Send buffers, happens _before_
the connect worker can run to set up the new device.

Instead, at transport creation, allocate each buffer, but leave it
unmapped. Once the RPC carries these buffers into ->send_request, by
which time a transport connection should have been established,
check to see that the RPC's buffers have been DMA mapped. If not,
map them there.

When device driver unplug support is added, it will simply unmap all
the transport's regbufs, but it doesn't have to deallocate the
underlying memory.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 54cbd6b0c6b9410782da3efe7377d43bb636faaf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/verbs.c
#	net/sunrpc/xprtrdma/xprt_rdma.h
diff --cc net/sunrpc/xprtrdma/verbs.c
index 4dff498a63f2,09346cd01bcb..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -1073,52 -1172,69 +1073,92 @@@ rpcrdma_recv_buffer_put(struct rpcrdma_
  	spin_unlock(&buffers->rb_lock);
  }
  
 +/*
 + * Wrappers for internal-use kmalloc memory registration, used by buffer code.
 + */
 +
  /**
 - * rpcrdma_alloc_regbuf - allocate and DMA-map memory for SEND/RECV buffers
 + * rpcrdma_alloc_regbuf - kmalloc and register memory for SEND/RECV buffers
   * @ia: controlling rpcrdma_ia
   * @size: size of buffer to be allocated, in bytes
 - * @direction: direction of data movement
   * @flags: GFP flags
   *
++<<<<<<< HEAD
 + * Returns pointer to private header of an area of internally
 + * registered memory, or an ERR_PTR. The registered buffer follows
 + * the end of the private header.
++=======
+  * Returns an ERR_PTR, or a pointer to a regbuf, a buffer that
+  * can be persistently DMA-mapped for I/O.
++>>>>>>> 54cbd6b0c6b9 (xprtrdma: Delay DMA mapping Send and Receive buffers)
   *
   * xprtrdma uses a regbuf for posting an outgoing RDMA SEND, or for
 - * receiving the payload of RDMA RECV operations. During Long Calls
 - * or Replies they may be registered externally via ro_map.
 + * receiving the payload of RDMA RECV operations. regbufs are not
 + * used for RDMA READ/WRITE operations, thus are registered only for
 + * LOCAL access.
   */
  struct rpcrdma_regbuf *
 -rpcrdma_alloc_regbuf(struct rpcrdma_ia *ia, size_t size,
 -		     enum dma_data_direction direction, gfp_t flags)
 +rpcrdma_alloc_regbuf(struct rpcrdma_ia *ia, size_t size, gfp_t flags)
  {
  	struct rpcrdma_regbuf *rb;
- 	struct ib_sge *iov;
  
  	rb = kmalloc(sizeof(*rb) + size, flags);
  	if (rb == NULL)
- 		goto out;
+ 		return ERR_PTR(-ENOMEM);
  
++<<<<<<< HEAD
 +	iov = &rb->rg_iov;
 +	iov->addr = ib_dma_map_single(ia->ri_device,
 +				      (void *)rb->rg_base, size,
 +				      DMA_BIDIRECTIONAL);
 +	if (ib_dma_mapping_error(ia->ri_device, iov->addr))
 +		goto out_free;
 +
 +	iov->length = size;
 +	iov->lkey = ia->ri_pd->local_dma_lkey;
 +	rb->rg_size = size;
 +	rb->rg_owner = NULL;
++=======
+ 	rb->rg_device = NULL;
+ 	rb->rg_direction = direction;
+ 	rb->rg_iov.length = size;
+ 
++>>>>>>> 54cbd6b0c6b9 (xprtrdma: Delay DMA mapping Send and Receive buffers)
  	return rb;
+ }
  
- out_free:
- 	kfree(rb);
- out:
- 	return ERR_PTR(-ENOMEM);
+ /**
+  * __rpcrdma_map_regbuf - DMA-map a regbuf
+  * @ia: controlling rpcrdma_ia
+  * @rb: regbuf to be mapped
+  */
+ bool
+ __rpcrdma_dma_map_regbuf(struct rpcrdma_ia *ia, struct rpcrdma_regbuf *rb)
+ {
+ 	if (rb->rg_direction == DMA_NONE)
+ 		return false;
+ 
+ 	rb->rg_iov.addr = ib_dma_map_single(ia->ri_device,
+ 					    (void *)rb->rg_base,
+ 					    rdmab_length(rb),
+ 					    rb->rg_direction);
+ 	if (ib_dma_mapping_error(ia->ri_device, rdmab_addr(rb)))
+ 		return false;
+ 
+ 	rb->rg_device = ia->ri_device;
+ 	rb->rg_iov.lkey = ia->ri_pd->local_dma_lkey;
+ 	return true;
+ }
+ 
+ static void
+ rpcrdma_dma_unmap_regbuf(struct rpcrdma_regbuf *rb)
+ {
+ 	if (!rpcrdma_regbuf_is_mapped(rb))
+ 		return;
+ 
+ 	ib_dma_unmap_single(rb->rg_device, rdmab_addr(rb),
+ 			    rdmab_length(rb), rb->rg_direction);
+ 	rb->rg_device = NULL;
  }
  
  /**
@@@ -1134,9 -1248,7 +1174,13 @@@ rpcrdma_free_regbuf(struct rpcrdma_ia *
  	if (!rb)
  		return;
  
++<<<<<<< HEAD
 +	iov = &rb->rg_iov;
 +	ib_dma_unmap_single(ia->ri_device,
 +			    iov->addr, iov->length, DMA_BIDIRECTIONAL);
++=======
+ 	rpcrdma_dma_unmap_regbuf(rb);
++>>>>>>> 54cbd6b0c6b9 (xprtrdma: Delay DMA mapping Send and Receive buffers)
  	kfree(rb);
  }
  
@@@ -1206,17 -1320,20 +1250,34 @@@ rpcrdma_ep_post_recv(struct rpcrdma_ia 
  	recv_wr.sg_list = &rep->rr_rdmabuf->rg_iov;
  	recv_wr.num_sge = 1;
  
++<<<<<<< HEAD
 +	ib_dma_sync_single_for_cpu(ia->ri_device,
 +				   rdmab_addr(rep->rr_rdmabuf),
 +				   rdmab_length(rep->rr_rdmabuf),
 +				   DMA_BIDIRECTIONAL);
 +
 +	rc = ib_post_recv(ia->ri_id->qp, &recv_wr, &recv_wr_fail);
 +
 +	if (rc)
 +		dprintk("RPC:       %s: ib_post_recv returned %i\n", __func__,
 +			rc);
 +	return rc;
++=======
+ 	if (!rpcrdma_dma_map_regbuf(ia, rep->rr_rdmabuf))
+ 		goto out_map;
+ 	rc = ib_post_recv(ia->ri_id->qp, &recv_wr, &recv_wr_fail);
+ 	if (rc)
+ 		goto out_postrecv;
+ 	return 0;
+ 
+ out_map:
+ 	pr_err("rpcrdma: failed to DMA map the Receive buffer\n");
+ 	return -EIO;
+ 
+ out_postrecv:
+ 	pr_err("rpcrdma: ib_post_recv returned %i\n", rc);
+ 	return -ENOTCONN;
++>>>>>>> 54cbd6b0c6b9 (xprtrdma: Delay DMA mapping Send and Receive buffers)
  }
  
  /**
diff --cc net/sunrpc/xprtrdma/xprt_rdma.h
index be2bcc2ea72b,d37ee24d2534..000000000000
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@@ -113,9 -112,9 +113,14 @@@ struct rpcrdma_ep 
   */
  
  struct rpcrdma_regbuf {
 +	size_t			rg_size;
 +	struct rpcrdma_req	*rg_owner;
  	struct ib_sge		rg_iov;
++<<<<<<< HEAD
++=======
+ 	struct ib_device	*rg_device;
+ 	enum dma_data_direction	rg_direction;
++>>>>>>> 54cbd6b0c6b9 (xprtrdma: Delay DMA mapping Send and Receive buffers)
  	__be32			rg_base[0] __attribute__ ((aligned(256)));
  };
  
@@@ -467,16 -476,31 +472,36 @@@ void rpcrdma_buffer_put(struct rpcrdma_
  void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
  void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
  
 -void rpcrdma_defer_mr_recovery(struct rpcrdma_mw *);
 -
  struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
++<<<<<<< HEAD
 +					    size_t, gfp_t);
++=======
+ 					    size_t, enum dma_data_direction,
+ 					    gfp_t);
+ bool __rpcrdma_dma_map_regbuf(struct rpcrdma_ia *, struct rpcrdma_regbuf *);
++>>>>>>> 54cbd6b0c6b9 (xprtrdma: Delay DMA mapping Send and Receive buffers)
  void rpcrdma_free_regbuf(struct rpcrdma_ia *,
  			 struct rpcrdma_regbuf *);
  
+ static inline bool
+ rpcrdma_regbuf_is_mapped(struct rpcrdma_regbuf *rb)
+ {
+ 	return rb->rg_device != NULL;
+ }
+ 
+ static inline bool
+ rpcrdma_dma_map_regbuf(struct rpcrdma_ia *ia, struct rpcrdma_regbuf *rb)
+ {
+ 	if (likely(rpcrdma_regbuf_is_mapped(rb)))
+ 		return true;
+ 	return __rpcrdma_dma_map_regbuf(ia, rb);
+ }
+ 
  int rpcrdma_ep_post_extra_recv(struct rpcrdma_xprt *, unsigned int);
  
 +int frwr_alloc_recovery_wq(void);
 +void frwr_destroy_recovery_wq(void);
 +
  int rpcrdma_alloc_wq(void);
  void rpcrdma_destroy_wq(void);
  
diff --git a/net/sunrpc/xprtrdma/backchannel.c b/net/sunrpc/xprtrdma/backchannel.c
index d3cfaf281e55..d767a59722a7 100644
--- a/net/sunrpc/xprtrdma/backchannel.c
+++ b/net/sunrpc/xprtrdma/backchannel.c
@@ -232,16 +232,24 @@ int rpcrdma_bc_marshal_reply(struct rpc_rqst *rqst)
 		__func__, (int)rpclen, rqst->rq_svec[0].iov_base);
 #endif
 
+	if (!rpcrdma_dma_map_regbuf(&r_xprt->rx_ia, req->rl_rdmabuf))
+		goto out_map;
 	req->rl_send_iov[0].addr = rdmab_addr(req->rl_rdmabuf);
 	req->rl_send_iov[0].length = RPCRDMA_HDRLEN_MIN;
 	req->rl_send_iov[0].lkey = rdmab_lkey(req->rl_rdmabuf);
 
+	if (!rpcrdma_dma_map_regbuf(&r_xprt->rx_ia, req->rl_sendbuf))
+		goto out_map;
 	req->rl_send_iov[1].addr = rdmab_addr(req->rl_sendbuf);
 	req->rl_send_iov[1].length = rpclen;
 	req->rl_send_iov[1].lkey = rdmab_lkey(req->rl_sendbuf);
 
 	req->rl_niovs = 2;
 	return 0;
+
+out_map:
+	pr_err("rpcrdma: failed to DMA map a Send buffer\n");
+	return -EIO;
 }
 
 /**
diff --git a/net/sunrpc/xprtrdma/rpc_rdma.c b/net/sunrpc/xprtrdma/rpc_rdma.c
index c3ffa06e5613..409814dcb7c1 100644
--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@ -831,6 +831,8 @@ rpcrdma_marshal_req(struct rpc_rqst *rqst)
 		transfertypes[rtype], transfertypes[wtype],
 		hdrlen, rpclen);
 
+	if (!rpcrdma_dma_map_regbuf(&r_xprt->rx_ia, req->rl_rdmabuf))
+		goto out_map;
 	req->rl_send_iov[0].addr = rdmab_addr(req->rl_rdmabuf);
 	req->rl_send_iov[0].length = hdrlen;
 	req->rl_send_iov[0].lkey = rdmab_lkey(req->rl_rdmabuf);
@@ -839,6 +841,8 @@ rpcrdma_marshal_req(struct rpc_rqst *rqst)
 	if (rtype == rpcrdma_areadch)
 		return 0;
 
+	if (!rpcrdma_dma_map_regbuf(&r_xprt->rx_ia, req->rl_sendbuf))
+		goto out_map;
 	req->rl_send_iov[1].addr = rdmab_addr(req->rl_sendbuf);
 	req->rl_send_iov[1].length = rpclen;
 	req->rl_send_iov[1].lkey = rdmab_lkey(req->rl_sendbuf);
@@ -857,6 +861,11 @@ out_overflow:
 out_unmap:
 	r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req, false);
 	return PTR_ERR(iptr);
+
+out_map:
+	pr_err("rpcrdma: failed to DMA map a Send buffer\n");
+	iptr = ERR_PTR(-EIO);
+	goto out_unmap;
 }
 
 /*
* Unmerged path net/sunrpc/xprtrdma/verbs.c
* Unmerged path net/sunrpc/xprtrdma/xprt_rdma.h
