xfs: build bios directly in xfs_add_to_ioend

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit bb18782aa47d8cde90fed5cb0af312642e98a4fa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/bb18782a.failed

Currently adding a buffer to the ioend and then building a bio from
the buffer list are two separate operations. We don't build the bios
and submit them until the ioend is submitted, and this places a
fixed dependency on bufferhead chaining in the ioend.

The first step to removing the bufferhead chaining in the ioend is
on the IO submission side. We can build the bio directly as we add
the buffers to the ioend chain, thereby removing the need for a
latter "buffer-to-bio" submission loop. This allows us to submit
bios on large ioends as soon as we cannot add more data to the bio.

These bios then get captured by the active plug, and hence will be
dispatched as soon as either the plug overflows or we schedule away
from the writeback context. This will reduce submission latency for
large IOs, but will also allow more timely request queue based
writeback blocking when the device becomes congested.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
[hch: various small updates]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>


(cherry picked from commit bb18782aa47d8cde90fed5cb0af312642e98a4fa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_aops.c
diff --cc fs/xfs/xfs_aops.c
index acf6c4a54883,d03946719992..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -269,16 -282,9 +270,14 @@@ xfs_alloc_ioend
  	 * all the I/O from calling the completion routine too early.
  	 */
  	atomic_set(&ioend->io_remaining, 1);
++<<<<<<< HEAD
 +	ioend->io_error = 0;
 +	ioend->io_list = NULL;
++=======
+ 	INIT_LIST_HEAD(&ioend->io_list);
++>>>>>>> bb18782aa47d (xfs: build bios directly in xfs_add_to_ioend)
  	ioend->io_type = type;
  	ioend->io_inode = inode;
- 	ioend->io_buffer_head = NULL;
- 	ioend->io_buffer_tail = NULL;
- 	ioend->io_offset = 0;
- 	ioend->io_size = 0;
- 	ioend->io_append_trans = NULL;
- 
  	INIT_WORK(&ioend->io_work, xfs_end_io);
  	return ioend;
  }
@@@ -448,85 -446,50 +447,123 @@@ static inline int xfs_bio_add_buffer(st
  }
  
  /*
++<<<<<<< HEAD
 + * Submit all of the bios for all of the ioends we have saved up, covering the
 + * initial writepage page and also any probed pages.
 + *
 + * Because we may have multiple ioends spanning a page, we need to start
 + * writeback on all the buffers before we submit them for I/O. If we mark the
 + * buffers as we got, then we can end up with a page that only has buffers
 + * marked async write and I/O complete on can occur before we mark the other
 + * buffers async write.
 + *
 + * The end result of this is that we trip a bug in end_page_writeback() because
 + * we call it twice for the one page as the code in end_buffer_async_write()
 + * assumes that all buffers on the page are started at the same time.
 + *
 + * The fix is two passes across the ioend list - one to start writeback on the
 + * buffer_heads, and then submit them for I/O on the second pass.
++=======
+  * Submit the bio for an ioend. We are passed an ioend with a bio attached to
+  * it, and we submit that bio. The ioend may be used for multiple bio
+  * submissions, so we only want to allocate an append transaction for the ioend
+  * once. In the case of multiple bio submission, each bio will take an IO
+  * reference to the ioend to ensure that the ioend completion is only done once
+  * all bios have been submitted and the ioend is really done.
++>>>>>>> bb18782aa47d (xfs: build bios directly in xfs_add_to_ioend)
   *
   * If @fail is non-zero, it means that we have a situation where some part of
   * the submission process has failed after we have marked paged for writeback
-  * and unlocked them. In this situation, we need to fail the ioend chain rather
-  * than submit it to IO. This typically only happens on a filesystem shutdown.
+  * and unlocked them. In this situation, we need to fail the bio and ioend
+  * rather than submit it to IO. This typically only happens on a filesystem
+  * shutdown.
   */
 -STATIC int
 +STATIC void
  xfs_submit_ioend(
  	struct writeback_control *wbc,
  	xfs_ioend_t		*ioend,
 -	int			status)
 +	int			fail)
  {
++<<<<<<< HEAD
 +	xfs_ioend_t		*head = ioend;
 +	xfs_ioend_t		*next;
 +	struct buffer_head	*bh;
 +	struct bio		*bio;
 +	sector_t		lastblock = 0;
 +
 +	/* Pass 1 - start writeback */
 +	do {
 +		next = ioend->io_list;
 +		for (bh = ioend->io_buffer_head; bh; bh = bh->b_private)
 +			xfs_start_buffer_writeback(bh);
 +	} while ((ioend = next) != NULL);
 +
 +	/* Pass 2 - submit I/O */
 +	ioend = head;
 +	do {
 +		next = ioend->io_list;
 +		bio = NULL;
 +
 +		/*
 +		 * If we are failing the IO now, just mark the ioend with an
 +		 * error and finish it. This will run IO completion immediately
 +		 * as there is only one reference to the ioend at this point in
 +		 * time.
 +		 */
 +		if (fail) {
 +			ioend->io_error = fail;
 +			xfs_finish_ioend(ioend);
 +			continue;
 +		}
 +
 +		for (bh = ioend->io_buffer_head; bh; bh = bh->b_private) {
 +
 +			if (!bio) {
 + retry:
 +				bio = xfs_alloc_ioend_bio(bh);
 +			} else if (bh->b_blocknr != lastblock + 1) {
 +				xfs_submit_ioend_bio(wbc, ioend, bio);
 +				goto retry;
 +			}
 +
 +			if (xfs_bio_add_buffer(bio, bh) != bh->b_size) {
 +				xfs_submit_ioend_bio(wbc, ioend, bio);
 +				goto retry;
 +			}
 +
 +			lastblock = bh->b_blocknr;
 +		}
 +		if (bio)
 +			xfs_submit_ioend_bio(wbc, ioend, bio);
 +		xfs_finish_ioend(ioend);
 +	} while ((ioend = next) != NULL);
++=======
+ 	/* Reserve log space if we might write beyond the on-disk inode size. */
+ 	if (!status &&
+ 	    ioend->io_bio && ioend->io_type != XFS_IO_UNWRITTEN &&
+ 	    xfs_ioend_is_append(ioend) &&
+ 	    !ioend->io_append_trans)
+ 		status = xfs_setfilesize_trans_alloc(ioend);
+ 
+ 	/*
+ 	 * If we are failing the IO now, just mark the ioend with an
+ 	 * error and finish it. This will run IO completion immediately
+ 	 * as there is only one reference to the ioend at this point in
+ 	 * time.
+ 	 */
+ 	if (status) {
+ 		if (ioend->io_bio)
+ 			bio_put(ioend->io_bio);
+ 		ioend->io_error = status;
+ 		xfs_finish_ioend(ioend);
+ 		return status;
+ 	}
+ 
+ 	xfs_submit_ioend_bio(wbc, ioend, ioend->io_bio);
+ 	ioend->io_bio = NULL;
+ 	xfs_finish_ioend(ioend);
+ 	return 0;
++>>>>>>> bb18782aa47d (xfs: build bios directly in xfs_add_to_ioend)
  }
  
  /*
@@@ -540,29 -504,42 +577,51 @@@ xfs_add_to_ioend
  	struct inode		*inode,
  	struct buffer_head	*bh,
  	xfs_off_t		offset,
++<<<<<<< HEAD
 +	unsigned int		type,
 +	xfs_ioend_t		**result,
 +	int			need_ioend)
++=======
+ 	struct xfs_writepage_ctx *wpc,
+ 	struct writeback_control *wbc,
+ 	struct list_head	*iolist)
++>>>>>>> bb18782aa47d (xfs: build bios directly in xfs_add_to_ioend)
  {
 -	if (!wpc->ioend || wpc->io_type != wpc->ioend->io_type ||
 -	    bh->b_blocknr != wpc->last_block + 1 ||
 -	    offset != wpc->ioend->io_offset + wpc->ioend->io_size) {
 -		struct xfs_ioend	*new;
 -
 -		if (wpc->ioend)
 -			list_add(&wpc->ioend->io_list, iolist);
 -
 -		new = xfs_alloc_ioend(inode, wpc->io_type);
 -		new->io_offset = offset;
 -		new->io_buffer_head = bh;
 -		new->io_buffer_tail = bh;
 -		wpc->ioend = new;
 +	xfs_ioend_t		*ioend = *result;
 +
 +	if (!ioend || need_ioend || type != ioend->io_type) {
 +		xfs_ioend_t	*previous = *result;
 +
 +		ioend = xfs_alloc_ioend(inode, type);
 +		ioend->io_offset = offset;
 +		ioend->io_buffer_head = bh;
 +		ioend->io_buffer_tail = bh;
 +		if (previous)
 +			previous->io_list = ioend;
 +		*result = ioend;
  	} else {
 -		wpc->ioend->io_buffer_tail->b_private = bh;
 -		wpc->ioend->io_buffer_tail = bh;
 +		ioend->io_buffer_tail->b_private = bh;
 +		ioend->io_buffer_tail = bh;
  	}
- 
  	bh->b_private = NULL;
++<<<<<<< HEAD
 +	ioend->io_size += bh->b_size;
++=======
+ 
+ retry:
+ 	if (!wpc->ioend->io_bio)
+ 		wpc->ioend->io_bio = xfs_alloc_ioend_bio(bh);
+ 
+ 	if (xfs_bio_add_buffer(wpc->ioend->io_bio, bh) != bh->b_size) {
+ 		xfs_submit_ioend_bio(wbc, wpc->ioend, wpc->ioend->io_bio);
+ 		wpc->ioend->io_bio = NULL;
+ 		goto retry;
+ 	}
+ 
+ 	wpc->ioend->io_size += bh->b_size;
+ 	wpc->last_block = bh->b_blocknr;
+ 	xfs_start_buffer_writeback(bh);
++>>>>>>> bb18782aa47d (xfs: build bios directly in xfs_add_to_ioend)
  }
  
  STATIC void
@@@ -901,26 -701,162 +960,140 @@@ out_invalidate
  	return;
  }
  
 -/*
 - * We implement an immediate ioend submission policy here to avoid needing to
 - * chain multiple ioends and hence nest mempool allocations which can violate
 - * forward progress guarantees we need to provide. The current ioend we are
 - * adding buffers to is cached on the writepage context, and if the new buffer
 - * does not append to the cached ioend it will create a new ioend and cache that
 - * instead.
 - *
 - * If a new ioend is created and cached, the old ioend is returned and queued
 - * locally for submission once the entire page is processed or an error has been
 - * detected.  While ioends are submitted immediately after they are completed,
 - * batching optimisations are provided by higher level block plugging.
 - *
 - * At the end of a writeback pass, there will be a cached ioend remaining on the
 - * writepage context that the caller will need to submit.
 - */
  static int
 -xfs_writepage_map(
 -	struct xfs_writepage_ctx *wpc,
 +xfs_writepage_submit(
 +	struct xfs_ioend	*ioend,
 +	struct xfs_ioend	*iohead,
  	struct writeback_control *wbc,
 -	struct inode		*inode,
 -	struct page		*page,
 -	loff_t			offset,
 -	__uint64_t              end_offset)
 +	int			status)
  {
 -	LIST_HEAD(submit_list);
 -	struct xfs_ioend	*ioend, *next;
 -	struct buffer_head	*bh, *head;
 -	ssize_t			len = 1 << inode->i_blkbits;
 -	int			error = 0;
 -	int			count = 0;
 -	int			uptodate = 1;
 +	struct blk_plug		plug;
  
 -	bh = head = page_buffers(page);
 -	offset = page_offset(page);
 -	do {
 -		if (offset >= end_offset)
 -			break;
 -		if (!buffer_uptodate(bh))
 -			uptodate = 0;
 +	/* Reserve log space if we might write beyond the on-disk inode size. */
 +	if (!status && ioend && ioend->io_type != XFS_IO_UNWRITTEN &&
 +	    xfs_ioend_is_append(ioend))
 +		status = xfs_setfilesize_trans_alloc(ioend);
  
++<<<<<<< HEAD
 +	if (iohead) {
 +		blk_start_plug(&plug);
 +		xfs_submit_ioend(wbc, iohead, status);
 +		blk_finish_plug(&plug);
++=======
+ 		/*
+ 		 * set_page_dirty dirties all buffers in a page, independent
+ 		 * of their state.  The dirty state however is entirely
+ 		 * meaningless for holes (!mapped && uptodate), so skip
+ 		 * buffers covering holes here.
+ 		 */
+ 		if (!buffer_mapped(bh) && buffer_uptodate(bh)) {
+ 			wpc->imap_valid = false;
+ 			continue;
+ 		}
+ 
+ 		if (buffer_unwritten(bh)) {
+ 			if (wpc->io_type != XFS_IO_UNWRITTEN) {
+ 				wpc->io_type = XFS_IO_UNWRITTEN;
+ 				wpc->imap_valid = false;
+ 			}
+ 		} else if (buffer_delay(bh)) {
+ 			if (wpc->io_type != XFS_IO_DELALLOC) {
+ 				wpc->io_type = XFS_IO_DELALLOC;
+ 				wpc->imap_valid = false;
+ 			}
+ 		} else if (buffer_uptodate(bh)) {
+ 			if (wpc->io_type != XFS_IO_OVERWRITE) {
+ 				wpc->io_type = XFS_IO_OVERWRITE;
+ 				wpc->imap_valid = false;
+ 			}
+ 		} else {
+ 			if (PageUptodate(page))
+ 				ASSERT(buffer_mapped(bh));
+ 			/*
+ 			 * This buffer is not uptodate and will not be
+ 			 * written to disk.  Ensure that we will put any
+ 			 * subsequent writeable buffers into a new
+ 			 * ioend.
+ 			 */
+ 			wpc->imap_valid = false;
+ 			continue;
+ 		}
+ 
+ 		if (wpc->imap_valid)
+ 			wpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,
+ 							 offset);
+ 		if (!wpc->imap_valid) {
+ 			error = xfs_map_blocks(inode, offset, &wpc->imap,
+ 					     wpc->io_type);
+ 			if (error)
+ 				goto out;
+ 			wpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,
+ 							 offset);
+ 		}
+ 		if (wpc->imap_valid) {
+ 			lock_buffer(bh);
+ 			if (wpc->io_type != XFS_IO_OVERWRITE)
+ 				xfs_map_at_offset(inode, bh, &wpc->imap, offset);
+ 			xfs_add_to_ioend(inode, bh, offset, wpc, wbc, &submit_list);
+ 			count++;
+ 		}
+ 
+ 	} while (offset += len, ((bh = bh->b_this_page) != head));
+ 
+ 	if (uptodate && bh == head)
+ 		SetPageUptodate(page);
+ 
+ 	ASSERT(wpc->ioend || list_empty(&submit_list));
+ 
+ out:
+ 	/*
+ 	 * On error, we have to fail the ioend here because we have locked
+ 	 * buffers in the ioend. If we don't do this, we'll deadlock
+ 	 * invalidating the page as that tries to lock the buffers on the page.
+ 	 * Also, because we may have set pages under writeback, we have to make
+ 	 * sure we run IO completion to mark the error state of the IO
+ 	 * appropriately, so we can't cancel the ioend directly here. That means
+ 	 * we have to mark this page as under writeback if we included any
+ 	 * buffers from it in the ioend chain so that completion treats it
+ 	 * correctly.
+ 	 *
+ 	 * If we didn't include the page in the ioend, the on error we can
+ 	 * simply discard and unlock it as there are no other users of the page
+ 	 * or it's buffers right now. The caller will still need to trigger
+ 	 * submission of outstanding ioends on the writepage context so they are
+ 	 * treated correctly on error.
+ 	 */
+ 	if (count) {
+ 		xfs_start_page_writeback(page, !error);
+ 
+ 		/*
+ 		 * Preserve the original error if there was one, otherwise catch
+ 		 * submission errors here and propagate into subsequent ioend
+ 		 * submissions.
+ 		 */
+ 		list_for_each_entry_safe(ioend, next, &submit_list, io_list) {
+ 			int error2;
+ 
+ 			list_del_init(&ioend->io_list);
+ 			error2 = xfs_submit_ioend(wbc, ioend, error);
+ 			if (error2 && !error)
+ 				error = error2;
+ 		}
+ 	} else if (error) {
+ 		xfs_aops_discard_page(page);
+ 		ClearPageUptodate(page);
+ 		unlock_page(page);
+ 	} else {
+ 		/*
+ 		 * We can end up here with no error and nothing to write if we
+ 		 * race with a partial page truncate on a sub-page block sized
+ 		 * filesystem. In that case we need to mark the page clean.
+ 		 */
+ 		xfs_start_page_writeback(page, 1);
+ 		end_page_writeback(page);
++>>>>>>> bb18782aa47d (xfs: build bios directly in xfs_add_to_ioend)
  	}
 -
 -	mapping_set_error(page->mapping, error);
 -	return error;
 +	return status;
  }
  
  /*
* Unmerged path fs/xfs/xfs_aops.c
diff --git a/fs/xfs/xfs_aops.h b/fs/xfs/xfs_aops.h
index a4343c63fb38..665cbdb06983 100644
--- a/fs/xfs/xfs_aops.h
+++ b/fs/xfs/xfs_aops.h
@@ -50,6 +50,7 @@ typedef struct xfs_ioend {
 	xfs_off_t		io_offset;	/* offset in the file */
 	struct work_struct	io_work;	/* xfsdatad work queue */
 	struct xfs_trans	*io_append_trans;/* xact. for size update */
+	struct bio		*io_bio;	/* bio being built */
 } xfs_ioend_t;
 
 extern const struct address_space_operations xfs_address_space_operations;
