s390/mm: add support for 2GB hugepages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [s390] mm: add support for 2GB hugepages (Hendrik Brueckner) [1380775]
Rebuild_FUZZ: 92.96%
commit-author Gerald Schaefer <gerald.schaefer@de.ibm.com>
commit d08de8e2d86744f91d9d5d57c56ca2b6e33bf6ec
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/d08de8e2.failed

This adds support for 2GB hugetlbfs pages on s390.

	Reviewed-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit d08de8e2d86744f91d9d5d57c56ca2b6e33bf6ec)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/pgtable.h
#	arch/s390/mm/gmap.c
#	arch/s390/mm/hugetlbpage.c
#	arch/s390/mm/pgtable.c
diff --cc arch/s390/include/asm/pgtable.h
index 9d35211c7eff,ea1533e07271..000000000000
--- a/arch/s390/include/asm/pgtable.h
+++ b/arch/s390/include/asm/pgtable.h
@@@ -345,13 -291,27 +345,16 @@@ extern unsigned long MODULES_END
  #define _REGION3_ENTRY		(_REGION_ENTRY_TYPE_R3 | _REGION_ENTRY_LENGTH)
  #define _REGION3_ENTRY_EMPTY	(_REGION_ENTRY_TYPE_R3 | _REGION_ENTRY_INVALID)
  
 -#define _REGION3_ENTRY_ORIGIN_LARGE ~0x7fffffffUL /* large page address	     */
 -#define _REGION3_ENTRY_ORIGIN  ~0x7ffUL/* region third table origin	     */
 -
 -#define _REGION3_ENTRY_DIRTY	0x2000	/* SW region dirty bit */
 -#define _REGION3_ENTRY_YOUNG	0x1000	/* SW region young bit */
 -#define _REGION3_ENTRY_LARGE	0x0400	/* RTTE-format control, large page  */
 -#define _REGION3_ENTRY_READ	0x0002	/* SW region read bit */
 -#define _REGION3_ENTRY_WRITE	0x0001	/* SW region write bit */
 -
 -#ifdef CONFIG_MEM_SOFT_DIRTY
 -#define _REGION3_ENTRY_SOFT_DIRTY 0x4000 /* SW region soft dirty bit */
 -#else
 -#define _REGION3_ENTRY_SOFT_DIRTY 0x0000 /* SW region soft dirty bit */
 -#endif
 +#define _REGION3_ENTRY_LARGE	0x400	/* RTTE-format control, large page  */
 +#define _REGION3_ENTRY_RO	0x200	/* page protection bit		    */
 +#define _REGION3_ENTRY_CO	0x100	/* change-recording override	    */
  
+ #define _REGION_ENTRY_BITS	 0xfffffffffffff227UL
+ #define _REGION_ENTRY_BITS_LARGE 0xffffffff8000fe27UL
+ 
  /* Bits in the segment table entry */
  #define _SEGMENT_ENTRY_BITS	0xfffffffffffffe33UL
 -#define _SEGMENT_ENTRY_BITS_LARGE 0xfffffffffff0ff33UL
 +#define _SEGMENT_ENTRY_BITS_LARGE 0xfffffffffff1ff33UL
  #define _SEGMENT_ENTRY_ORIGIN_LARGE ~0xfffffUL /* large page address	    */
  #define _SEGMENT_ENTRY_ORIGIN	~0x7ffUL/* segment table origin		    */
  #define _SEGMENT_ENTRY_PROTECT	0x200	/* page protection bit		    */
@@@ -532,21 -586,37 +535,42 @@@ static inline int pud_large(pud_t pud
  	return !!(pud_val(pud) & _REGION3_ENTRY_LARGE);
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long pud_pfn(pud_t pud)
+ {
+ 	unsigned long origin_mask;
+ 
+ 	origin_mask = _REGION3_ENTRY_ORIGIN;
+ 	if (pud_large(pud))
+ 		origin_mask = _REGION3_ENTRY_ORIGIN_LARGE;
+ 	return (pud_val(pud) & origin_mask) >> PAGE_SHIFT;
+ }
+ 
+ static inline int pmd_large(pmd_t pmd)
+ {
+ 	return (pmd_val(pmd) & _SEGMENT_ENTRY_LARGE) != 0;
+ }
+ 
+ static inline int pmd_bad(pmd_t pmd)
+ {
+ 	if (pmd_large(pmd))
+ 		return (pmd_val(pmd) & ~_SEGMENT_ENTRY_BITS_LARGE) != 0;
+ 	return (pmd_val(pmd) & ~_SEGMENT_ENTRY_BITS) != 0;
+ }
+ 
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  static inline int pud_bad(pud_t pud)
  {
- 	/*
- 	 * With dynamic page table levels the pud can be a region table
- 	 * entry or a segment table entry. Check for the bit that are
- 	 * invalid for either table entry.
- 	 */
- 	unsigned long mask =
- 		~_SEGMENT_ENTRY_ORIGIN & ~_REGION_ENTRY_INVALID &
- 		~_REGION_ENTRY_TYPE_MASK & ~_REGION_ENTRY_LENGTH;
- 	return (pud_val(pud) & mask) != 0;
+ 	if ((pud_val(pud) & _REGION_ENTRY_TYPE_MASK) < _REGION_ENTRY_TYPE_R3)
+ 		return pmd_bad(__pmd(pud_val(pud)));
+ 	if (pud_large(pud))
+ 		return (pud_val(pud) & ~_REGION_ENTRY_BITS_LARGE) != 0;
+ 	return (pud_val(pud) & ~_REGION_ENTRY_BITS) != 0;
  }
  
 +#endif /* CONFIG_64BIT */
 +
  static inline int pmd_present(pmd_t pmd)
  {
  	return pmd_val(pmd) != _SEGMENT_ENTRY_INVALID;
@@@ -557,43 -627,16 +581,50 @@@ static inline int pmd_none(pmd_t pmd
  	return pmd_val(pmd) == _SEGMENT_ENTRY_INVALID;
  }
  
++<<<<<<< HEAD
 +static inline int pmd_large(pmd_t pmd)
 +{
 +#ifdef CONFIG_64BIT
 +	return (pmd_val(pmd) & _SEGMENT_ENTRY_LARGE) != 0;
 +#else
 +	return 0;
 +#endif
 +}
 +
 +static inline int pmd_prot_none(pmd_t pmd)
++=======
+ static inline unsigned long pmd_pfn(pmd_t pmd)
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  {
 -	unsigned long origin_mask;
 +	return (pmd_val(pmd) & _SEGMENT_ENTRY_INVALID) &&
 +		(pmd_val(pmd) & _SEGMENT_ENTRY_NONE);
 +}
  
 -	origin_mask = _SEGMENT_ENTRY_ORIGIN;
++<<<<<<< HEAD
 +static inline int pmd_bad(pmd_t pmd)
 +{
 +#ifdef CONFIG_64BIT
  	if (pmd_large(pmd))
 -		origin_mask = _SEGMENT_ENTRY_ORIGIN_LARGE;
 -	return (pmd_val(pmd) & origin_mask) >> PAGE_SHIFT;
 +		return (pmd_val(pmd) & ~_SEGMENT_ENTRY_BITS_LARGE) != 0;
 +#endif
 +	return (pmd_val(pmd) & ~_SEGMENT_ENTRY_BITS) != 0;
  }
  
 +#define __HAVE_ARCH_PMDP_SPLITTING_FLUSH
 +extern void pmdp_splitting_flush(struct vm_area_struct *vma,
 +				 unsigned long addr, pmd_t *pmdp);
 +
 +#define  __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS
 +extern int pmdp_set_access_flags(struct vm_area_struct *vma,
 +				 unsigned long address, pmd_t *pmdp,
 +				 pmd_t entry, int dirty);
 +
 +#define __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH
 +extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
 +				  unsigned long address, pmd_t *pmdp);
 +
++=======
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  #define __HAVE_ARCH_PMD_WRITE
  static inline int pmd_write(pmd_t pmd)
  {
@@@ -1332,7 -1079,8 +1363,12 @@@ static inline pmd_t *pmd_offset(pud_t *
  #define pte_pfn(x) (pte_val(x) >> PAGE_SHIFT)
  #define pte_page(x) pfn_to_page(pte_pfn(x))
  
++<<<<<<< HEAD
 +#define pmd_page(pmd) pfn_to_page(pmd_val(pmd) >> PAGE_SHIFT)
++=======
+ #define pmd_page(pmd) pfn_to_page(pmd_pfn(pmd))
+ #define pud_page(pud) pfn_to_page(pud_pfn(pud))
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  
  /* Find an entry in the lowest level page table.. */
  #define pte_offset(pmd, addr) ((pte_t *) pmd_deref(*(pmd)) + pte_index(addr))
@@@ -1425,29 -1215,71 +1461,86 @@@ static inline pmd_t mk_pmd_phys(unsigne
  {
  	pmd_t __pmd;
  	pmd_val(__pmd) = physpage + massage_pgprot_pmd(pgprot);
 -	return __pmd;
 +	return pmd_mkyoung(__pmd);
  }
  
 +static inline pmd_t pmd_mkwrite(pmd_t pmd)
 +{
 +	/* Do not clobber PROT_NONE segments! */
 +	if (!pmd_prot_none(pmd))
 +		pmd_val(pmd) &= ~_SEGMENT_ENTRY_PROTECT;
 +	return pmd;
 +}
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLB_PAGE */
  
 -static inline void __pmdp_csp(pmd_t *pmdp)
 +static inline void pmdp_flush_lazy(struct mm_struct *mm,
 +				   unsigned long address, pmd_t *pmdp)
  {
 -	csp((unsigned int *)pmdp + 1, pmd_val(*pmdp),
 -	    pmd_val(*pmdp) | _SEGMENT_ENTRY_INVALID);
 +	int active = (mm == current->active_mm) ? 1 : 0;
 +
 +	if ((atomic_read(&mm->context.attach_count) & 0xffff) > active)
 +		__pmd_idte(address, pmdp);
 +	else
 +		mm->context.flush_mm = 1;
  }
  
++<<<<<<< HEAD
++=======
+ static inline void __pmdp_idte(unsigned long address, pmd_t *pmdp)
+ {
+ 	unsigned long sto;
+ 
+ 	sto = (unsigned long) pmdp - pmd_index(address) * sizeof(pmd_t);
+ 	asm volatile(
+ 		"	.insn	rrf,0xb98e0000,%2,%3,0,0"
+ 		: "=m" (*pmdp)
+ 		: "m" (*pmdp), "a" (sto), "a" ((address & HPAGE_MASK))
+ 		: "cc" );
+ }
+ 
+ static inline void __pudp_idte(unsigned long address, pud_t *pudp)
+ {
+ 	unsigned long r3o;
+ 
+ 	r3o = (unsigned long) pudp - pud_index(address) * sizeof(pud_t);
+ 	r3o |= _ASCE_TYPE_REGION3;
+ 	asm volatile(
+ 		"	.insn	rrf,0xb98e0000,%2,%3,0,0"
+ 		: "=m" (*pudp)
+ 		: "m" (*pudp), "a" (r3o), "a" ((address & PUD_MASK))
+ 		: "cc");
+ }
+ 
+ static inline void __pmdp_idte_local(unsigned long address, pmd_t *pmdp)
+ {
+ 	unsigned long sto;
+ 
+ 	sto = (unsigned long) pmdp - pmd_index(address) * sizeof(pmd_t);
+ 	asm volatile(
+ 		"	.insn	rrf,0xb98e0000,%2,%3,0,1"
+ 		: "=m" (*pmdp)
+ 		: "m" (*pmdp), "a" (sto), "a" ((address & HPAGE_MASK))
+ 		: "cc" );
+ }
+ 
+ static inline void __pudp_idte_local(unsigned long address, pud_t *pudp)
+ {
+ 	unsigned long r3o;
+ 
+ 	r3o = (unsigned long) pudp - pud_index(address) * sizeof(pud_t);
+ 	r3o |= _ASCE_TYPE_REGION3;
+ 	asm volatile(
+ 		"	.insn	rrf,0xb98e0000,%2,%3,0,1"
+ 		: "=m" (*pudp)
+ 		: "m" (*pudp), "a" (r3o), "a" ((address & PUD_MASK))
+ 		: "cc");
+ }
+ 
+ pmd_t pmdp_xchg_direct(struct mm_struct *, unsigned long, pmd_t *, pmd_t);
+ pmd_t pmdp_xchg_lazy(struct mm_struct *, unsigned long, pmd_t *, pmd_t);
+ pud_t pudp_xchg_direct(struct mm_struct *, unsigned long, pud_t *, pud_t);
+ 
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
  
  #define __HAVE_ARCH_PGTABLE_DEPOSIT
diff --cc arch/s390/mm/hugetlbpage.c
index 99a68d579828,e19d853883be..000000000000
--- a/arch/s390/mm/hugetlbpage.c
+++ b/arch/s390/mm/hugetlbpage.c
@@@ -8,82 -11,84 +11,154 @@@
  #include <linux/mm.h>
  #include <linux/hugetlb.h>
  
- static inline pmd_t __pte_to_pmd(pte_t pte)
+ static inline unsigned long __pte_to_rste(pte_t pte)
  {
++<<<<<<< HEAD
 +	int none, young, prot;
 +	pmd_t pmd;
 +
 +	/*
 +	 * Convert encoding		  pte bits	  pmd bits
 +	 *				.IR...wrdytp	..R...I...y.
 +	 * empty			.10...000000 -> ..0...1...0.
 +	 * prot-none, clean, old	.11...000001 -> ..0...1...1.
 +	 * prot-none, clean, young	.11...000101 -> ..1...1...1.
 +	 * prot-none, dirty, old	.10...001001 -> ..0...1...1.
 +	 * prot-none, dirty, young	.10...001101 -> ..1...1...1.
 +	 * read-only, clean, old	.11...010001 -> ..1...1...0.
 +	 * read-only, clean, young	.01...010101 -> ..1...0...1.
 +	 * read-only, dirty, old	.11...011001 -> ..1...1...0.
 +	 * read-only, dirty, young	.01...011101 -> ..1...0...1.
 +	 * read-write, clean, old	.11...110001 -> ..0...1...0.
 +	 * read-write, clean, young	.01...110101 -> ..0...0...1.
 +	 * read-write, dirty, old	.10...111001 -> ..0...1...0.
 +	 * read-write, dirty, young	.00...111101 -> ..0...0...1.
 +	 * Huge ptes are dirty by definition, a clean pte is made dirty
 +	 * by the conversion.
 +	 */
 +	if (pte_present(pte)) {
 +		pmd_val(pmd) = pte_val(pte) & PAGE_MASK;
 +		if (pte_val(pte) & _PAGE_INVALID)
 +			pmd_val(pmd) |= _SEGMENT_ENTRY_INVALID;
 +		none = (pte_val(pte) & _PAGE_PRESENT) &&
 +			!(pte_val(pte) & _PAGE_READ) &&
 +			!(pte_val(pte) & _PAGE_WRITE);
 +		prot = (pte_val(pte) & _PAGE_PROTECT) &&
 +			!(pte_val(pte) & _PAGE_WRITE);
 +		young = pte_val(pte) & _PAGE_YOUNG;
 +		if (none || young)
 +			pmd_val(pmd) |= _SEGMENT_ENTRY_YOUNG;
 +		if (prot || (none && young))
 +			pmd_val(pmd) |= _SEGMENT_ENTRY_PROTECT;
++=======
+ 	unsigned long rste;
+ 
+ 	/*
+ 	 * Convert encoding		  pte bits	pmd / pud bits
+ 	 *				lIR.uswrdy.p	dy..R...I...wr
+ 	 * empty			010.000000.0 -> 00..0...1...00
+ 	 * prot-none, clean, old	111.000000.1 -> 00..1...1...00
+ 	 * prot-none, clean, young	111.000001.1 -> 01..1...1...00
+ 	 * prot-none, dirty, old	111.000010.1 -> 10..1...1...00
+ 	 * prot-none, dirty, young	111.000011.1 -> 11..1...1...00
+ 	 * read-only, clean, old	111.000100.1 -> 00..1...1...01
+ 	 * read-only, clean, young	101.000101.1 -> 01..1...0...01
+ 	 * read-only, dirty, old	111.000110.1 -> 10..1...1...01
+ 	 * read-only, dirty, young	101.000111.1 -> 11..1...0...01
+ 	 * read-write, clean, old	111.001100.1 -> 00..1...1...11
+ 	 * read-write, clean, young	101.001101.1 -> 01..1...0...11
+ 	 * read-write, dirty, old	110.001110.1 -> 10..0...1...11
+ 	 * read-write, dirty, young	100.001111.1 -> 11..0...0...11
+ 	 * HW-bits: R read-only, I invalid
+ 	 * SW-bits: p present, y young, d dirty, r read, w write, s special,
+ 	 *	    u unused, l large
+ 	 */
+ 	if (pte_present(pte)) {
+ 		rste = pte_val(pte) & PAGE_MASK;
+ 		rste |= (pte_val(pte) & _PAGE_READ) >> 4;
+ 		rste |= (pte_val(pte) & _PAGE_WRITE) >> 4;
+ 		rste |= (pte_val(pte) & _PAGE_INVALID) >> 5;
+ 		rste |= (pte_val(pte) & _PAGE_PROTECT);
+ 		rste |= (pte_val(pte) & _PAGE_DIRTY) << 10;
+ 		rste |= (pte_val(pte) & _PAGE_YOUNG) << 10;
+ 		rste |= (pte_val(pte) & _PAGE_SOFT_DIRTY) << 13;
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  	} else
- 		pmd_val(pmd) = _SEGMENT_ENTRY_INVALID;
- 	return pmd;
+ 		rste = _SEGMENT_ENTRY_INVALID;
+ 	return rste;
  }
  
- static inline pte_t __pmd_to_pte(pmd_t pmd)
+ static inline pte_t __rste_to_pte(unsigned long rste)
  {
+ 	int present;
  	pte_t pte;
  
+ 	if ((rste & _REGION_ENTRY_TYPE_MASK) == _REGION_ENTRY_TYPE_R3)
+ 		present = pud_present(__pud(rste));
+ 	else
+ 		present = pmd_present(__pmd(rste));
+ 
  	/*
++<<<<<<< HEAD
 +	 * Convert encoding	  pmd bits	  pte bits
 +	 *			..R...I...y.	.IR...wrdytp
 +	 * empty		..0...1...0. -> .10...000000
 +	 * prot-none, old	..0...1...1. -> .10...001001
 +	 * prot-none, young	..1...1...1. -> .10...001101
 +	 * read-only, old	..1...1...0. -> .11...011001
 +	 * read-only, young	..1...0...1. -> .01...011101
 +	 * read-write, old	..0...1...0. -> .10...111001
 +	 * read-write, young	..0...0...1. -> .00...111101
 +	 * Huge ptes are dirty by definition
 +	 */
 +	if (pmd_present(pmd)) {
 +		pte_val(pte) = _PAGE_PRESENT | _PAGE_LARGE | _PAGE_DIRTY |
 +			(pmd_val(pmd) & PAGE_MASK);
 +		if (pmd_val(pmd) & _SEGMENT_ENTRY_INVALID)
 +			pte_val(pte) |= _PAGE_INVALID;
 +		if (pmd_prot_none(pmd)) {
 +			if (pmd_val(pmd) & _SEGMENT_ENTRY_PROTECT)
 +				pte_val(pte) |= _PAGE_YOUNG;
 +		} else {
 +			pte_val(pte) |= _PAGE_READ;
 +			if (pmd_val(pmd) & _SEGMENT_ENTRY_PROTECT)
 +				pte_val(pte) |= _PAGE_PROTECT;
 +			else
 +				pte_val(pte) |= _PAGE_WRITE;
 +			if (pmd_val(pmd) & _SEGMENT_ENTRY_YOUNG)
 +				pte_val(pte) |= _PAGE_YOUNG;
 +		}
++=======
+ 	 * Convert encoding		pmd / pud bits	    pte bits
+ 	 *				dy..R...I...wr	  lIR.uswrdy.p
+ 	 * empty			00..0...1...00 -> 010.000000.0
+ 	 * prot-none, clean, old	00..1...1...00 -> 111.000000.1
+ 	 * prot-none, clean, young	01..1...1...00 -> 111.000001.1
+ 	 * prot-none, dirty, old	10..1...1...00 -> 111.000010.1
+ 	 * prot-none, dirty, young	11..1...1...00 -> 111.000011.1
+ 	 * read-only, clean, old	00..1...1...01 -> 111.000100.1
+ 	 * read-only, clean, young	01..1...0...01 -> 101.000101.1
+ 	 * read-only, dirty, old	10..1...1...01 -> 111.000110.1
+ 	 * read-only, dirty, young	11..1...0...01 -> 101.000111.1
+ 	 * read-write, clean, old	00..1...1...11 -> 111.001100.1
+ 	 * read-write, clean, young	01..1...0...11 -> 101.001101.1
+ 	 * read-write, dirty, old	10..0...1...11 -> 110.001110.1
+ 	 * read-write, dirty, young	11..0...0...11 -> 100.001111.1
+ 	 * HW-bits: R read-only, I invalid
+ 	 * SW-bits: p present, y young, d dirty, r read, w write, s special,
+ 	 *	    u unused, l large
+ 	 */
+ 	if (present) {
+ 		pte_val(pte) = rste & _SEGMENT_ENTRY_ORIGIN_LARGE;
+ 		pte_val(pte) |= _PAGE_LARGE | _PAGE_PRESENT;
+ 		pte_val(pte) |= (rste & _SEGMENT_ENTRY_READ) << 4;
+ 		pte_val(pte) |= (rste & _SEGMENT_ENTRY_WRITE) << 4;
+ 		pte_val(pte) |= (rste & _SEGMENT_ENTRY_INVALID) << 5;
+ 		pte_val(pte) |= (rste & _SEGMENT_ENTRY_PROTECT);
+ 		pte_val(pte) |= (rste & _SEGMENT_ENTRY_DIRTY) >> 10;
+ 		pte_val(pte) |= (rste & _SEGMENT_ENTRY_YOUNG) >> 10;
+ 		pte_val(pte) |= (rste & _SEGMENT_ENTRY_SOFT_DIRTY) >> 13;
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  	} else
  		pte_val(pte) = _PAGE_INVALID;
  	return pte;
@@@ -92,82 -97,33 +167,108 @@@
  void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
  		     pte_t *ptep, pte_t pte)
  {
++<<<<<<< HEAD
 +	pmd_t pmd;
 +
 +	pmd = __pte_to_pmd(pte);
 +	if (!MACHINE_HAS_HPAGE) {
 +		pmd_val(pmd) &= ~_SEGMENT_ENTRY_ORIGIN;
 +		pmd_val(pmd) |= pte_page(pte)[1].index;
 +	} else
 +		pmd_val(pmd) |= _SEGMENT_ENTRY_LARGE | _SEGMENT_ENTRY_CO;
 +	*(pmd_t *) ptep = pmd;
++=======
+ 	unsigned long rste = __pte_to_rste(pte);
+ 
+ 	/* Set correct table type for 2G hugepages */
+ 	if ((pte_val(*ptep) & _REGION_ENTRY_TYPE_MASK) == _REGION_ENTRY_TYPE_R3)
+ 		rste |= _REGION_ENTRY_TYPE_R3 | _REGION3_ENTRY_LARGE;
+ 	else
+ 		rste |= _SEGMENT_ENTRY_LARGE;
+ 	pte_val(*ptep) = rste;
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  }
  
  pte_t huge_ptep_get(pte_t *ptep)
  {
++<<<<<<< HEAD
 +	unsigned long origin;
 +	pmd_t pmd;
 +
 +	pmd = *(pmd_t *) ptep;
 +	if (!MACHINE_HAS_HPAGE && pmd_present(pmd)) {
 +		origin = pmd_val(pmd) & _SEGMENT_ENTRY_ORIGIN;
 +		pmd_val(pmd) &= ~_SEGMENT_ENTRY_ORIGIN;
 +		pmd_val(pmd) |= *(unsigned long *) origin;
 +	}
 +	return __pmd_to_pte(pmd);
++=======
+ 	return __rste_to_pte(pte_val(*ptep));
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  }
  
  pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
  			      unsigned long addr, pte_t *ptep)
  {
+ 	pte_t pte = huge_ptep_get(ptep);
  	pmd_t *pmdp = (pmd_t *) ptep;
++<<<<<<< HEAD
 +	pte_t pte = huge_ptep_get(ptep);
 +
 +	if (MACHINE_HAS_IDTE)
 +		__pmd_idte(addr, pmdp);
 +	else
 +		__pmd_csp(pmdp);
 +	pmd_val(*pmdp) = _SEGMENT_ENTRY_EMPTY;
 +	return pte;
 +}
 +
 +int arch_prepare_hugepage(struct page *page)
 +{
 +	unsigned long addr = page_to_phys(page);
 +	pte_t pte;
 +	pte_t *ptep;
 +	int i;
 +
 +	if (MACHINE_HAS_HPAGE)
 +		return 0;
 +
 +	ptep = (pte_t *) pte_alloc_one(&init_mm, addr);
 +	if (!ptep)
 +		return -ENOMEM;
 +
 +	pte_val(pte) = addr;
 +	for (i = 0; i < PTRS_PER_PTE; i++) {
 +		set_pte_at(&init_mm, addr + i * PAGE_SIZE, ptep + i, pte);
 +		pte_val(pte) += PAGE_SIZE;
 +	}
 +	page[1].index = (unsigned long) ptep;
 +	return 0;
 +}
 +
 +void arch_release_hugepage(struct page *page)
 +{
 +	pte_t *ptep;
 +
 +	if (MACHINE_HAS_HPAGE)
 +		return;
 +
 +	ptep = (pte_t *) page[1].index;
 +	if (!ptep)
 +		return;
 +	clear_table((unsigned long *) ptep, _PAGE_INVALID,
 +		    PTRS_PER_PTE * sizeof(pte_t));
 +	page_table_free(&init_mm, (unsigned long *) ptep);
 +	page[1].index = 0;
++=======
+ 	pud_t *pudp = (pud_t *) ptep;
+ 
+ 	if ((pte_val(*ptep) & _REGION_ENTRY_TYPE_MASK) == _REGION_ENTRY_TYPE_R3)
+ 		pudp_xchg_direct(mm, addr, pudp, __pud(_REGION3_ENTRY_EMPTY));
+ 	else
+ 		pmdp_xchg_direct(mm, addr, pmdp, __pmd(_SEGMENT_ENTRY_EMPTY));
+ 	return pte;
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  }
  
  pte_t *huge_pte_alloc(struct mm_struct *mm,
diff --cc arch/s390/mm/pgtable.c
index 09f6a4bed766,b98d1a152d46..000000000000
--- a/arch/s390/mm/pgtable.c
+++ b/arch/s390/mm/pgtable.c
@@@ -24,1043 -24,411 +24,1263 @@@
  #include <asm/tlbflush.h>
  #include <asm/mmu_context.h>
  
 -static inline pte_t ptep_flush_direct(struct mm_struct *mm,
 -				      unsigned long addr, pte_t *ptep)
 +#ifndef CONFIG_64BIT
 +#define ALLOC_ORDER	1
 +#define FRAG_MASK	0x0f
 +#else
 +#define ALLOC_ORDER	2
 +#define FRAG_MASK	0x03
 +#endif
 +
 +
 +unsigned long *crst_table_alloc(struct mm_struct *mm)
  {
 -	pte_t old;
 +	struct page *page = alloc_pages(GFP_KERNEL, ALLOC_ORDER);
  
 -	old = *ptep;
 -	if (unlikely(pte_val(old) & _PAGE_INVALID))
 -		return old;
 -	atomic_inc(&mm->context.flush_count);
 -	if (MACHINE_HAS_TLB_LC &&
 -	    cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))
 -		__ptep_ipte_local(addr, ptep);
 +	if (!page)
 +		return NULL;
 +	return (unsigned long *) page_to_phys(page);
 +}
 +
 +void crst_table_free(struct mm_struct *mm, unsigned long *table)
 +{
 +	free_pages((unsigned long) table, ALLOC_ORDER);
 +}
 +
 +#ifdef CONFIG_64BIT
 +static void __crst_table_upgrade(void *arg)
 +{
 +	struct mm_struct *mm = arg;
 +
 +	if (current->active_mm == mm)
 +		update_mm(mm, current);
 +	__tlb_flush_local();
 +}
 +
 +int crst_table_upgrade(struct mm_struct *mm)
 +{
 +	unsigned long *table, *pgd;
 +
 +	/* upgrade should only happen from 3 to 4 levels */
 +	BUG_ON(mm->context.asce_limit != (1UL << 42));
 +
 +	table = crst_table_alloc(mm);
 +	if (!table)
 +		return -ENOMEM;
 +
 +	spin_lock_bh(&mm->page_table_lock);
 +	pgd = (unsigned long *) mm->pgd;
 +	crst_table_init(table, _REGION2_ENTRY_EMPTY);
 +	pgd_populate(mm, (pgd_t *) table, (pud_t *) pgd);
 +	mm->pgd = (pgd_t *) table;
 +	mm->context.asce_limit = 1UL << 53;
 +	mm->context.asce = __pa(mm->pgd) | _ASCE_TABLE_LENGTH |
 +			   _ASCE_USER_BITS | _ASCE_TYPE_REGION2;
 +	mm->task_size = mm->context.asce_limit;
 +	spin_unlock_bh(&mm->page_table_lock);
 +
 +	on_each_cpu(__crst_table_upgrade, mm, 0);
 +	return 0;
 +}
 +
 +void crst_table_downgrade(struct mm_struct *mm)
 +{
 +	pgd_t *pgd;
 +
 +	/* downgrade should only happen from 3 to 2 levels (compat only) */
 +	BUG_ON(mm->context.asce_limit != (1UL << 42));
 +
 +	if (current->active_mm == mm)
 +		__tlb_flush_mm(mm);
 +
 +	pgd = mm->pgd;
 +	mm->pgd = (pgd_t *) (pgd_val(*pgd) & _REGION_ENTRY_ORIGIN);
 +	mm->context.asce_limit = 1UL << 31;
 +	mm->context.asce = __pa(mm->pgd) | _ASCE_TABLE_LENGTH |
 +			   _ASCE_USER_BITS | _ASCE_TYPE_SEGMENT;
 +	mm->task_size = mm->context.asce_limit;
 +	crst_table_free(mm, (unsigned long *) pgd);
 +
 +	if (current->active_mm == mm)
 +		update_mm(mm, current);
 +}
 +#endif
 +
 +#ifdef CONFIG_PGSTE
 +
 +/**
 + * gmap_alloc - allocate a guest address space
 + * @mm: pointer to the parent mm_struct
 + *
 + * Returns a guest address space structure.
 + */
 +struct gmap *gmap_alloc(struct mm_struct *mm)
 +{
 +	struct gmap *gmap;
 +	struct page *page;
 +	unsigned long *table;
 +
 +	gmap = kzalloc(sizeof(struct gmap), GFP_KERNEL);
 +	if (!gmap)
 +		goto out;
 +	INIT_LIST_HEAD(&gmap->crst_list);
 +	gmap->mm = mm;
 +	page = alloc_pages(GFP_KERNEL, ALLOC_ORDER);
 +	if (!page)
 +		goto out_free;
 +	list_add(&page->lru, &gmap->crst_list);
 +	table = (unsigned long *) page_to_phys(page);
 +	crst_table_init(table, _REGION1_ENTRY_EMPTY);
 +	gmap->table = table;
 +	gmap->asce = _ASCE_TYPE_REGION1 | _ASCE_TABLE_LENGTH |
 +		     _ASCE_USER_BITS | __pa(table);
 +	list_add(&gmap->list, &mm->context.gmap_list);
 +	return gmap;
 +
 +out_free:
 +	kfree(gmap);
 +out:
 +	return NULL;
 +}
 +EXPORT_SYMBOL_GPL(gmap_alloc);
 +
 +static int gmap_unlink_segment(struct gmap *gmap, unsigned long *table)
 +{
 +	struct gmap_pgtable *mp;
 +	struct gmap_rmap *rmap;
 +	struct page *page;
 +
 +	if (*table & _SEGMENT_ENTRY_INVALID)
 +		return 0;
 +	page = pfn_to_page(*table >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	list_for_each_entry(rmap, &mp->mapper, list) {
 +		if (rmap->entry != table)
 +			continue;
 +		list_del(&rmap->list);
 +		kfree(rmap);
 +		break;
 +	}
 +	*table = mp->vmaddr | _SEGMENT_ENTRY_INVALID | _SEGMENT_ENTRY_PROTECT;
 +	return 1;
 +}
 +
 +static void gmap_flush_tlb(struct gmap *gmap)
 +{
 +	if (MACHINE_HAS_IDTE)
 +		__tlb_flush_idte((unsigned long) gmap->table |
 +				 _ASCE_TYPE_REGION1);
  	else
 -		__ptep_ipte(addr, ptep);
 -	atomic_dec(&mm->context.flush_count);
 -	return old;
 +		__tlb_flush_global();
  }
  
 -static inline pte_t ptep_flush_lazy(struct mm_struct *mm,
 -				    unsigned long addr, pte_t *ptep)
 +/**
 + * gmap_free - free a guest address space
 + * @gmap: pointer to the guest address space structure
 + */
 +void gmap_free(struct gmap *gmap)
  {
 -	pte_t old;
 +	struct page *page, *next;
 +	unsigned long *table;
 +	int i;
  
 -	old = *ptep;
 -	if (unlikely(pte_val(old) & _PAGE_INVALID))
 -		return old;
 -	atomic_inc(&mm->context.flush_count);
 -	if (cpumask_equal(&mm->context.cpu_attach_mask,
 -			  cpumask_of(smp_processor_id()))) {
 -		pte_val(*ptep) |= _PAGE_INVALID;
 -		mm->context.flush_mm = 1;
 +
 +	/* Flush tlb. */
 +	if (MACHINE_HAS_IDTE)
 +		__tlb_flush_idte((unsigned long) gmap->table |
 +				 _ASCE_TYPE_REGION1);
 +	else
 +		__tlb_flush_global();
 +
 +	/* Free all segment & region tables. */
 +	down_read(&gmap->mm->mmap_sem);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	list_for_each_entry_safe(page, next, &gmap->crst_list, lru) {
 +		table = (unsigned long *) page_to_phys(page);
 +		if ((*table & _REGION_ENTRY_TYPE_MASK) == 0)
 +			/* Remove gmap rmap structures for segment table. */
 +			for (i = 0; i < PTRS_PER_PMD; i++, table++)
 +				gmap_unlink_segment(gmap, table);
 +		__free_pages(page, ALLOC_ORDER);
 +	}
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	list_del(&gmap->list);
 +	kfree(gmap);
 +}
 +EXPORT_SYMBOL_GPL(gmap_free);
 +
 +/**
 + * gmap_enable - switch primary space to the guest address space
 + * @gmap: pointer to the guest address space structure
 + */
 +void gmap_enable(struct gmap *gmap)
 +{
 +	S390_lowcore.gmap = (unsigned long) gmap;
 +}
 +EXPORT_SYMBOL_GPL(gmap_enable);
 +
 +/**
 + * gmap_disable - switch back to the standard primary address space
 + * @gmap: pointer to the guest address space structure
 + */
 +void gmap_disable(struct gmap *gmap)
 +{
 +	S390_lowcore.gmap = 0UL;
 +}
 +EXPORT_SYMBOL_GPL(gmap_disable);
 +
 +/*
 + * gmap_alloc_table is assumed to be called with mmap_sem held
 + */
 +static int gmap_alloc_table(struct gmap *gmap,
 +			       unsigned long *table, unsigned long init)
 +{
 +	struct page *page;
 +	unsigned long *new;
 +
 +	/* since we dont free the gmap table until gmap_free we can unlock */
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	page = alloc_pages(GFP_KERNEL, ALLOC_ORDER);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	if (!page)
 +		return -ENOMEM;
 +	new = (unsigned long *) page_to_phys(page);
 +	crst_table_init(new, init);
 +	if (*table & _REGION_ENTRY_INVALID) {
 +		list_add(&page->lru, &gmap->crst_list);
 +		*table = (unsigned long) new | _REGION_ENTRY_LENGTH |
 +			(*table & _REGION_ENTRY_TYPE_MASK);
  	} else
 -		__ptep_ipte(addr, ptep);
 -	atomic_dec(&mm->context.flush_count);
 -	return old;
 +		__free_pages(page, ALLOC_ORDER);
 +	return 0;
  }
  
 -static inline pgste_t pgste_get_lock(pte_t *ptep)
 +/**
 + * gmap_unmap_segment - unmap segment from the guest address space
 + * @gmap: pointer to the guest address space structure
 + * @addr: address in the guest address space
 + * @len: length of the memory area to unmap
 + *
 + * Returns 0 if the unmap succeded, -EINVAL if not.
 + */
 +int gmap_unmap_segment(struct gmap *gmap, unsigned long to, unsigned long len)
  {
 -	unsigned long new = 0;
 -#ifdef CONFIG_PGSTE
 -	unsigned long old;
 -
 -	asm(
 -		"	lg	%0,%2\n"
 -		"0:	lgr	%1,%0\n"
 -		"	nihh	%0,0xff7f\n"	/* clear PCL bit in old */
 -		"	oihh	%1,0x0080\n"	/* set PCL bit in new */
 -		"	csg	%0,%1,%2\n"
 -		"	jl	0b\n"
 -		: "=&d" (old), "=&d" (new), "=Q" (ptep[PTRS_PER_PTE])
 -		: "Q" (ptep[PTRS_PER_PTE]) : "cc", "memory");
 -#endif
 -	return __pgste(new);
 +	unsigned long *table;
 +	unsigned long off;
 +	int flush;
 +
 +	if ((to | len) & (PMD_SIZE - 1))
 +		return -EINVAL;
 +	if (len == 0 || to + len < to)
 +		return -EINVAL;
 +
 +	flush = 0;
 +	down_read(&gmap->mm->mmap_sem);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	for (off = 0; off < len; off += PMD_SIZE) {
 +		/* Walk the guest addr space page table */
 +		table = gmap->table + (((to + off) >> 53) & 0x7ff);
 +		if (*table & _REGION_ENTRY_INVALID)
 +			goto out;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 42) & 0x7ff);
 +		if (*table & _REGION_ENTRY_INVALID)
 +			goto out;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 31) & 0x7ff);
 +		if (*table & _REGION_ENTRY_INVALID)
 +			goto out;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 20) & 0x7ff);
 +
 +		/* Clear segment table entry in guest address space. */
 +		flush |= gmap_unlink_segment(gmap, table);
 +		*table = _SEGMENT_ENTRY_INVALID;
 +	}
 +out:
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	if (flush)
 +		gmap_flush_tlb(gmap);
 +	return 0;
 +}
 +EXPORT_SYMBOL_GPL(gmap_unmap_segment);
 +
 +/**
 + * gmap_mmap_segment - map a segment to the guest address space
 + * @gmap: pointer to the guest address space structure
 + * @from: source address in the parent address space
 + * @to: target address in the guest address space
 + *
 + * Returns 0 if the mmap succeded, -EINVAL or -ENOMEM if not.
 + */
 +int gmap_map_segment(struct gmap *gmap, unsigned long from,
 +		     unsigned long to, unsigned long len)
 +{
 +	unsigned long *table;
 +	unsigned long off;
 +	int flush;
 +
 +	if ((from | to | len) & (PMD_SIZE - 1))
 +		return -EINVAL;
 +	if (len == 0 || from + len > TASK_MAX_SIZE ||
 +	    from + len < from || to + len < to)
 +		return -EINVAL;
 +
 +	flush = 0;
 +	down_read(&gmap->mm->mmap_sem);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	for (off = 0; off < len; off += PMD_SIZE) {
 +		/* Walk the gmap address space page table */
 +		table = gmap->table + (((to + off) >> 53) & 0x7ff);
 +		if ((*table & _REGION_ENTRY_INVALID) &&
 +		    gmap_alloc_table(gmap, table, _REGION2_ENTRY_EMPTY))
 +			goto out_unmap;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 42) & 0x7ff);
 +		if ((*table & _REGION_ENTRY_INVALID) &&
 +		    gmap_alloc_table(gmap, table, _REGION3_ENTRY_EMPTY))
 +			goto out_unmap;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 31) & 0x7ff);
 +		if ((*table & _REGION_ENTRY_INVALID) &&
 +		    gmap_alloc_table(gmap, table, _SEGMENT_ENTRY_EMPTY))
 +			goto out_unmap;
 +		table = (unsigned long *) (*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 20) & 0x7ff);
 +
 +		/* Store 'from' address in an invalid segment table entry. */
 +		flush |= gmap_unlink_segment(gmap, table);
 +		*table =  (from + off) | (_SEGMENT_ENTRY_INVALID |
 +					  _SEGMENT_ENTRY_PROTECT);
 +	}
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	if (flush)
 +		gmap_flush_tlb(gmap);
 +	return 0;
 +
 +out_unmap:
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	gmap_unmap_segment(gmap, to, len);
 +	return -ENOMEM;
  }
 +EXPORT_SYMBOL_GPL(gmap_map_segment);
  
 -static inline void pgste_set_unlock(pte_t *ptep, pgste_t pgste)
 +static unsigned long *gmap_table_walk(unsigned long address, struct gmap *gmap)
  {
 -#ifdef CONFIG_PGSTE
 -	asm(
 -		"	nihh	%1,0xff7f\n"	/* clear PCL bit */
 -		"	stg	%1,%0\n"
 -		: "=Q" (ptep[PTRS_PER_PTE])
 -		: "d" (pgste_val(pgste)), "Q" (ptep[PTRS_PER_PTE])
 -		: "cc", "memory");
 -#endif
 +	unsigned long *table;
 +
 +	table = gmap->table + ((address >> 53) & 0x7ff);
 +	if (unlikely(*table & _REGION_ENTRY_INVALID))
 +		return ERR_PTR(-EFAULT);
 +	table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +	table = table + ((address >> 42) & 0x7ff);
 +	if (unlikely(*table & _REGION_ENTRY_INVALID))
 +		return ERR_PTR(-EFAULT);
 +	table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +	table = table + ((address >> 31) & 0x7ff);
 +	if (unlikely(*table & _REGION_ENTRY_INVALID))
 +		return ERR_PTR(-EFAULT);
 +	table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +	table = table + ((address >> 20) & 0x7ff);
 +	return table;
  }
  
 -static inline pgste_t pgste_get(pte_t *ptep)
 +/**
 + * __gmap_translate - translate a guest address to a user space address
 + * @address: guest address
 + * @gmap: pointer to guest mapping meta data structure
 + *
 + * Returns user space address which corresponds to the guest address or
 + * -EFAULT if no such mapping exists.
 + * This function does not establish potentially missing page table entries.
 + * The mmap_sem of the mm that belongs to the address space must be held
 + * when this function gets called.
 + */
 +unsigned long __gmap_translate(unsigned long address, struct gmap *gmap)
  {
 -	unsigned long pgste = 0;
 -#ifdef CONFIG_PGSTE
 -	pgste = *(unsigned long *)(ptep + PTRS_PER_PTE);
 -#endif
 -	return __pgste(pgste);
 +	unsigned long *segment_ptr, vmaddr, segment;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +
 +	current->thread.gmap_addr = address;
 +	segment_ptr = gmap_table_walk(address, gmap);
 +	if (IS_ERR(segment_ptr))
 +		return PTR_ERR(segment_ptr);
 +	/* Convert the gmap address to an mm address. */
 +	segment = *segment_ptr;
 +	if (!(segment & _SEGMENT_ENTRY_INVALID)) {
 +		page = pfn_to_page(segment >> PAGE_SHIFT);
 +		mp = (struct gmap_pgtable *) page->index;
 +		return mp->vmaddr | (address & ~PMD_MASK);
 +	} else if (segment & _SEGMENT_ENTRY_PROTECT) {
 +		vmaddr = segment & _SEGMENT_ENTRY_ORIGIN;
 +		return vmaddr | (address & ~PMD_MASK);
 +	}
 +	return -EFAULT;
 +}
 +EXPORT_SYMBOL_GPL(__gmap_translate);
 +
 +/**
 + * gmap_translate - translate a guest address to a user space address
 + * @address: guest address
 + * @gmap: pointer to guest mapping meta data structure
 + *
 + * Returns user space address which corresponds to the guest address or
 + * -EFAULT if no such mapping exists.
 + * This function does not establish potentially missing page table entries.
 + */
 +unsigned long gmap_translate(unsigned long address, struct gmap *gmap)
 +{
 +	unsigned long rc;
 +
 +	down_read(&gmap->mm->mmap_sem);
 +	rc = __gmap_translate(address, gmap);
 +	up_read(&gmap->mm->mmap_sem);
 +	return rc;
  }
 +EXPORT_SYMBOL_GPL(gmap_translate);
  
 -static inline void pgste_set(pte_t *ptep, pgste_t pgste)
 +static int gmap_connect_pgtable(unsigned long address, unsigned long segment,
 +				unsigned long *segment_ptr, struct gmap *gmap)
  {
 -#ifdef CONFIG_PGSTE
 -	*(pgste_t *)(ptep + PTRS_PER_PTE) = pgste;
 -#endif
 +	unsigned long vmaddr;
 +	struct vm_area_struct *vma;
 +	struct gmap_pgtable *mp;
 +	struct gmap_rmap *rmap;
 +	struct mm_struct *mm;
 +	struct page *page;
 +	pgd_t *pgd;
 +	pud_t *pud;
 +	pmd_t *pmd;
 +
 +	mm = gmap->mm;
 +	vmaddr = segment & _SEGMENT_ENTRY_ORIGIN;
 +	vma = find_vma(mm, vmaddr);
 +	if (!vma || vma->vm_start > vmaddr)
 +		return -EFAULT;
 +	/* Walk the parent mm page table */
 +	pgd = pgd_offset(mm, vmaddr);
 +	pud = pud_alloc(mm, pgd, vmaddr);
 +	if (!pud)
 +		return -ENOMEM;
 +	pmd = pmd_alloc(mm, pud, vmaddr);
 +	if (!pmd)
 +		return -ENOMEM;
 +	if (!pmd_present(*pmd) &&
 +	    __pte_alloc(mm, vma, pmd, vmaddr))
 +		return -ENOMEM;
 +	/* large pmds cannot yet be handled */
 +	if (pmd_large(*pmd))
 +		return -EFAULT;
 +	/* pmd now points to a valid segment table entry. */
 +	rmap = kmalloc(sizeof(*rmap), GFP_KERNEL|__GFP_REPEAT);
 +	if (!rmap)
 +		return -ENOMEM;
 +	/* Link gmap segment table entry location to page table. */
 +	page = pmd_page(*pmd);
 +	mp = (struct gmap_pgtable *) page->index;
 +	rmap->gmap = gmap;
 +	rmap->entry = segment_ptr;
 +	rmap->vmaddr = address & PMD_MASK;
 +	spin_lock(&mm->page_table_lock);
 +	if (*segment_ptr == segment) {
 +		list_add(&rmap->list, &mp->mapper);
 +		/* Set gmap segment table entry to page table. */
 +		*segment_ptr = pmd_val(*pmd) & PAGE_MASK;
 +		rmap = NULL;
 +	}
 +	spin_unlock(&mm->page_table_lock);
 +	kfree(rmap);
 +	return 0;
  }
  
 -static inline pgste_t pgste_update_all(pte_t pte, pgste_t pgste,
 -				       struct mm_struct *mm)
 +static void gmap_disconnect_pgtable(struct mm_struct *mm, unsigned long *table)
  {
 -#ifdef CONFIG_PGSTE
 -	unsigned long address, bits, skey;
 -
 -	if (!mm_use_skey(mm) || pte_val(pte) & _PAGE_INVALID)
 -		return pgste;
 -	address = pte_val(pte) & PAGE_MASK;
 -	skey = (unsigned long) page_get_storage_key(address);
 -	bits = skey & (_PAGE_CHANGED | _PAGE_REFERENCED);
 -	/* Transfer page changed & referenced bit to guest bits in pgste */
 -	pgste_val(pgste) |= bits << 48;		/* GR bit & GC bit */
 -	/* Copy page access key and fetch protection bit to pgste */
 -	pgste_val(pgste) &= ~(PGSTE_ACC_BITS | PGSTE_FP_BIT);
 -	pgste_val(pgste) |= (skey & (_PAGE_ACC_BITS | _PAGE_FP_BIT)) << 56;
 -#endif
 -	return pgste;
 +	struct gmap_rmap *rmap, *next;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +	int flush;
 +
 +	flush = 0;
 +	spin_lock(&mm->page_table_lock);
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	list_for_each_entry_safe(rmap, next, &mp->mapper, list) {
 +		*rmap->entry = mp->vmaddr | (_SEGMENT_ENTRY_INVALID |
 +					     _SEGMENT_ENTRY_PROTECT);
 +		list_del(&rmap->list);
 +		kfree(rmap);
 +		flush = 1;
 +	}
 +	spin_unlock(&mm->page_table_lock);
 +	if (flush)
 +		__tlb_flush_global();
 +}
  
 +/*
 + * this function is assumed to be called with mmap_sem held
 + */
 +unsigned long __gmap_fault(unsigned long address, struct gmap *gmap)
 +{
 +	unsigned long *segment_ptr, segment;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +	int rc;
 +
 +	current->thread.gmap_addr = address;
 +	segment_ptr = gmap_table_walk(address, gmap);
 +	if (IS_ERR(segment_ptr))
 +		return -EFAULT;
 +	/* Convert the gmap address to an mm address. */
 +	while (1) {
 +		segment = *segment_ptr;
 +		if (!(segment & _SEGMENT_ENTRY_INVALID)) {
 +			/* Page table is present */
 +			page = pfn_to_page(segment >> PAGE_SHIFT);
 +			mp = (struct gmap_pgtable *) page->index;
 +			return mp->vmaddr | (address & ~PMD_MASK);
 +		}
 +		if (!(segment & _SEGMENT_ENTRY_PROTECT))
 +			/* Nothing mapped in the gmap address space. */
 +			break;
 +		rc = gmap_connect_pgtable(address, segment, segment_ptr, gmap);
 +		if (rc)
 +			return rc;
 +	}
 +	return -EFAULT;
  }
  
 -static inline void pgste_set_key(pte_t *ptep, pgste_t pgste, pte_t entry,
 -				 struct mm_struct *mm)
 +unsigned long gmap_fault(unsigned long address, struct gmap *gmap)
  {
 -#ifdef CONFIG_PGSTE
 -	unsigned long address;
 -	unsigned long nkey;
 +	unsigned long rc;
 +
 +	down_read(&gmap->mm->mmap_sem);
 +	rc = __gmap_fault(address, gmap);
 +	up_read(&gmap->mm->mmap_sem);
 +
 +	return rc;
 +}
 +EXPORT_SYMBOL_GPL(gmap_fault);
 +
 +void gmap_discard(unsigned long from, unsigned long to, struct gmap *gmap)
 +{
 +
 +	unsigned long *table, address, size;
 +	struct vm_area_struct *vma;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +
 +	down_read(&gmap->mm->mmap_sem);
 +	address = from;
 +	while (address < to) {
 +		/* Walk the gmap address space page table */
 +		table = gmap->table + ((address >> 53) & 0x7ff);
 +		if (unlikely(*table & _REGION_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + ((address >> 42) & 0x7ff);
 +		if (unlikely(*table & _REGION_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + ((address >> 31) & 0x7ff);
 +		if (unlikely(*table & _REGION_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + ((address >> 20) & 0x7ff);
 +		if (unlikely(*table & _SEGMENT_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		page = pfn_to_page(*table >> PAGE_SHIFT);
 +		mp = (struct gmap_pgtable *) page->index;
 +		vma = find_vma(gmap->mm, mp->vmaddr);
 +		size = min(to - address, PMD_SIZE - (address & ~PMD_MASK));
 +		zap_page_range(vma, mp->vmaddr | (address & ~PMD_MASK),
 +			       size, NULL);
 +		address = (address + PMD_SIZE) & PMD_MASK;
 +	}
 +	up_read(&gmap->mm->mmap_sem);
 +}
 +EXPORT_SYMBOL_GPL(gmap_discard);
 +
 +static LIST_HEAD(gmap_notifier_list);
 +static DEFINE_SPINLOCK(gmap_notifier_lock);
 +
 +/**
 + * gmap_register_ipte_notifier - register a pte invalidation callback
 + * @nb: pointer to the gmap notifier block
 + */
 +void gmap_register_ipte_notifier(struct gmap_notifier *nb)
 +{
 +	spin_lock(&gmap_notifier_lock);
 +	list_add(&nb->list, &gmap_notifier_list);
 +	spin_unlock(&gmap_notifier_lock);
 +}
 +EXPORT_SYMBOL_GPL(gmap_register_ipte_notifier);
 +
 +/**
 + * gmap_unregister_ipte_notifier - remove a pte invalidation callback
 + * @nb: pointer to the gmap notifier block
 + */
 +void gmap_unregister_ipte_notifier(struct gmap_notifier *nb)
 +{
 +	spin_lock(&gmap_notifier_lock);
 +	list_del_init(&nb->list);
 +	spin_unlock(&gmap_notifier_lock);
 +}
 +EXPORT_SYMBOL_GPL(gmap_unregister_ipte_notifier);
 +
 +/**
 + * gmap_ipte_notify - mark a range of ptes for invalidation notification
 + * @gmap: pointer to guest mapping meta data structure
 + * @address: virtual address in the guest address space
 + * @len: size of area
 + *
 + * Returns 0 if for each page in the given range a gmap mapping exists and
 + * the invalidation notification could be set. If the gmap mapping is missing
 + * for one or more pages -EFAULT is returned. If no memory could be allocated
 + * -ENOMEM is returned. This function establishes missing page table entries.
 + */
 +int gmap_ipte_notify(struct gmap *gmap, unsigned long start, unsigned long len)
 +{
 +	unsigned long addr;
 +	spinlock_t *ptl;
 +	pte_t *ptep, entry;
 +	pgste_t pgste;
 +	int rc = 0;
 +
 +	if ((start & ~PAGE_MASK) || (len & ~PAGE_MASK))
 +		return -EINVAL;
 +	down_read(&gmap->mm->mmap_sem);
 +	while (len) {
 +		/* Convert gmap address and connect the page tables */
 +		addr = __gmap_fault(start, gmap);
 +		if (IS_ERR_VALUE(addr)) {
 +			rc = addr;
 +			break;
 +		}
 +		/* Get the page mapped */
 +		if (fixup_user_fault(current, gmap->mm, addr, FAULT_FLAG_WRITE)) {
 +			rc = -EFAULT;
 +			break;
 +		}
 +		/* Walk the process page table, lock and get pte pointer */
 +		ptep = get_locked_pte(gmap->mm, addr, &ptl);
 +		if (unlikely(!ptep))
 +			continue;
 +		/* Set notification bit in the pgste of the pte */
 +		entry = *ptep;
 +		if ((pte_val(entry) & (_PAGE_INVALID | _PAGE_PROTECT)) == 0) {
 +			pgste = pgste_get_lock(ptep);
 +			pgste_val(pgste) |= PGSTE_IN_BIT;
 +			pgste_set_unlock(ptep, pgste);
 +			start += PAGE_SIZE;
 +			len -= PAGE_SIZE;
 +		}
 +		spin_unlock(ptl);
 +	}
 +	up_read(&gmap->mm->mmap_sem);
 +	return rc;
 +}
 +EXPORT_SYMBOL_GPL(gmap_ipte_notify);
 +
 +/**
 + * gmap_do_ipte_notify - call all invalidation callbacks for a specific pte.
 + * @mm: pointer to the process mm_struct
 + * @addr: virtual address in the process address space
 + * @pte: pointer to the page table entry
 + *
 + * This function is assumed to be called with the page table lock held
 + * for the pte to notify.
 + */
 +void gmap_do_ipte_notify(struct mm_struct *mm, unsigned long addr, pte_t *pte)
 +{
 +	unsigned long segment_offset;
 +	struct gmap_notifier *nb;
 +	struct gmap_pgtable *mp;
 +	struct gmap_rmap *rmap;
 +	struct page *page;
 +
 +	segment_offset = ((unsigned long) pte) & (255 * sizeof(pte_t));
 +	segment_offset = segment_offset * (4096 / sizeof(pte_t));
 +	page = pfn_to_page(__pa(pte) >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	spin_lock(&gmap_notifier_lock);
 +	list_for_each_entry(rmap, &mp->mapper, list) {
 +		list_for_each_entry(nb, &gmap_notifier_list, list)
 +			nb->notifier_call(rmap->gmap,
 +					  rmap->vmaddr + segment_offset);
 +	}
 +	spin_unlock(&gmap_notifier_lock);
 +}
 +
 +static inline int page_table_with_pgste(struct page *page)
 +{
 +	return atomic_read(&page->_mapcount) == 0;
 +}
 +
 +static inline unsigned long *page_table_alloc_pgste(struct mm_struct *mm,
 +						    unsigned long vmaddr)
 +{
 +	struct page *page;
 +	unsigned long *table;
 +	struct gmap_pgtable *mp;
 +
 +	page = alloc_page(GFP_KERNEL|__GFP_REPEAT);
 +	if (!page)
 +		return NULL;
 +	mp = kmalloc(sizeof(*mp), GFP_KERNEL|__GFP_REPEAT);
 +	if (!mp) {
 +		__free_page(page);
 +		return NULL;
 +	}
 +	if (!pgtable_page_ctor(page)) {
 +		kfree(mp);
 +		__free_page(page);
 +		return NULL;
 +	}
 +	mp->vmaddr = vmaddr & PMD_MASK;
 +	INIT_LIST_HEAD(&mp->mapper);
 +	page->index = (unsigned long) mp;
 +	atomic_set(&page->_mapcount, 0);
 +	table = (unsigned long *) page_to_phys(page);
 +	clear_table(table, _PAGE_INVALID, PAGE_SIZE/2);
 +	clear_table(table + PTRS_PER_PTE, PGSTE_HR_BIT | PGSTE_HC_BIT,
 +		    PAGE_SIZE/2);
 +	return table;
 +}
 +
 +static inline void page_table_free_pgste(unsigned long *table)
 +{
 +	struct page *page;
 +	struct gmap_pgtable *mp;
 +
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	BUG_ON(!list_empty(&mp->mapper));
 +	pgtable_page_dtor(page);
 +	atomic_set(&page->_mapcount, -1);
 +	kfree(mp);
 +	__free_page(page);
 +}
 +
 +int set_guest_storage_key(struct mm_struct *mm, unsigned long addr,
 +			  unsigned long key, bool nq)
 +{
 +	spinlock_t *ptl;
 +	pgste_t old, new;
 +	pte_t *ptep;
 +
 +	down_read(&mm->mmap_sem);
 +	ptep = get_locked_pte(current->mm, addr, &ptl);
 +	if (unlikely(!ptep)) {
 +		up_read(&mm->mmap_sem);
 +		return -EFAULT;
 +	}
 +
 +	new = old = pgste_get_lock(ptep);
 +	pgste_val(new) &= ~(PGSTE_GR_BIT | PGSTE_GC_BIT |
 +			    PGSTE_ACC_BITS | PGSTE_FP_BIT);
 +	pgste_val(new) |= (key & (_PAGE_CHANGED | _PAGE_REFERENCED)) << 48;
 +	pgste_val(new) |= (key & (_PAGE_ACC_BITS | _PAGE_FP_BIT)) << 56;
 +	if (!(pte_val(*ptep) & _PAGE_INVALID)) {
 +		unsigned long address, bits, skey;
 +
 +		address = pte_val(*ptep) & PAGE_MASK;
 +		skey = (unsigned long) page_get_storage_key(address);
 +		bits = skey & (_PAGE_CHANGED | _PAGE_REFERENCED);
 +		skey = key & (_PAGE_ACC_BITS | _PAGE_FP_BIT);
 +		/* Set storage key ACC and FP */
 +		page_set_storage_key(address, skey, !nq);
 +		/* Merge host changed & referenced into pgste  */
 +		pgste_val(new) |= bits << 52;
 +	}
 +	/* changing the guest storage key is considered a change of the page */
 +	if ((pgste_val(new) ^ pgste_val(old)) &
 +	    (PGSTE_ACC_BITS | PGSTE_FP_BIT | PGSTE_GR_BIT | PGSTE_GC_BIT))
 +		pgste_val(new) |= PGSTE_HC_BIT;
 +
 +	pgste_set_unlock(ptep, new);
 +	pte_unmap_unlock(*ptep, ptl);
 +	up_read(&mm->mmap_sem);
 +	return 0;
 +}
 +EXPORT_SYMBOL(set_guest_storage_key);
 +
 +#else /* CONFIG_PGSTE */
 +
 +static inline int page_table_with_pgste(struct page *page)
 +{
 +	return 0;
 +}
 +
 +static inline unsigned long *page_table_alloc_pgste(struct mm_struct *mm,
 +						    unsigned long vmaddr)
 +{
 +	return NULL;
 +}
 +
 +static inline void page_table_free_pgste(unsigned long *table)
 +{
 +}
 +
 +static inline void gmap_disconnect_pgtable(struct mm_struct *mm,
 +					   unsigned long *table)
 +{
 +}
 +
 +#endif /* CONFIG_PGSTE */
 +
 +static inline unsigned int atomic_xor_bits(atomic_t *v, unsigned int bits)
 +{
 +	unsigned int old, new;
  
 -	if (!mm_use_skey(mm) || pte_val(entry) & _PAGE_INVALID)
 +	do {
 +		old = atomic_read(v);
 +		new = old ^ bits;
 +	} while (atomic_cmpxchg(v, old, new) != old);
 +	return new;
 +}
 +
 +/*
 + * page table entry allocation/free routines.
 + */
 +unsigned long *page_table_alloc(struct mm_struct *mm, unsigned long vmaddr)
 +{
 +	unsigned long *uninitialized_var(table);
 +	struct page *uninitialized_var(page);
 +	unsigned int mask, bit;
 +
 +	if (mm_has_pgste(mm))
 +		return page_table_alloc_pgste(mm, vmaddr);
 +	/* Allocate fragments of a 4K page as 1K/2K page table */
 +	spin_lock_bh(&mm->context.list_lock);
 +	mask = FRAG_MASK;
 +	if (!list_empty(&mm->context.pgtable_list)) {
 +		page = list_first_entry(&mm->context.pgtable_list,
 +					struct page, lru);
 +		table = (unsigned long *) page_to_phys(page);
 +		mask = atomic_read(&page->_mapcount);
 +		mask = mask | (mask >> 4);
 +	}
 +	if ((mask & FRAG_MASK) == FRAG_MASK) {
 +		spin_unlock_bh(&mm->context.list_lock);
 +		page = alloc_page(GFP_KERNEL|__GFP_REPEAT);
 +		if (!page)
 +			return NULL;
 +		if (!pgtable_page_ctor(page)) {
 +			__free_page(page);
 +			return NULL;
 +		}
 +		atomic_set(&page->_mapcount, 1);
 +		table = (unsigned long *) page_to_phys(page);
 +		clear_table(table, _PAGE_INVALID, PAGE_SIZE);
 +		spin_lock_bh(&mm->context.list_lock);
 +		list_add(&page->lru, &mm->context.pgtable_list);
 +	} else {
 +		for (bit = 1; mask & bit; bit <<= 1)
 +			table += PTRS_PER_PTE;
 +		mask = atomic_xor_bits(&page->_mapcount, bit);
 +		if ((mask & FRAG_MASK) == FRAG_MASK)
 +			list_del(&page->lru);
 +	}
 +	spin_unlock_bh(&mm->context.list_lock);
 +	return table;
 +}
 +
 +void page_table_free(struct mm_struct *mm, unsigned long *table)
 +{
 +	struct page *page;
 +	unsigned int bit, mask;
 +
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	if (page_table_with_pgste(page)) {
 +		gmap_disconnect_pgtable(mm, table);
 +		return page_table_free_pgste(table);
 +	}
 +	/* Free 1K/2K page table fragment of a 4K page */
 +	bit = 1 << ((__pa(table) & ~PAGE_MASK)/(PTRS_PER_PTE*sizeof(pte_t)));
 +	spin_lock_bh(&mm->context.list_lock);
 +	if ((atomic_read(&page->_mapcount) & FRAG_MASK) != FRAG_MASK)
 +		list_del(&page->lru);
 +	mask = atomic_xor_bits(&page->_mapcount, bit);
 +	if (mask & FRAG_MASK)
 +		list_add(&page->lru, &mm->context.pgtable_list);
 +	spin_unlock_bh(&mm->context.list_lock);
 +	if (mask == 0) {
 +		pgtable_page_dtor(page);
 +		atomic_set(&page->_mapcount, -1);
 +		__free_page(page);
 +	}
 +}
 +
 +static void __page_table_free_rcu(void *table, unsigned bit)
 +{
 +	struct page *page;
 +
 +	if (bit == FRAG_MASK)
 +		return page_table_free_pgste(table);
 +	/* Free 1K/2K page table fragment of a 4K page */
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	if (atomic_xor_bits(&page->_mapcount, bit) == 0) {
 +		pgtable_page_dtor(page);
 +		atomic_set(&page->_mapcount, -1);
 +		__free_page(page);
 +	}
 +}
 +
 +void page_table_free_rcu(struct mmu_gather *tlb, unsigned long *table)
 +{
 +	struct mm_struct *mm;
 +	struct page *page;
 +	unsigned int bit, mask;
 +
 +	mm = tlb->mm;
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	if (page_table_with_pgste(page)) {
 +		gmap_disconnect_pgtable(mm, table);
 +		table = (unsigned long *) (__pa(table) | FRAG_MASK);
 +		tlb_remove_table(tlb, table);
  		return;
 -	VM_BUG_ON(!(pte_val(*ptep) & _PAGE_INVALID));
 -	address = pte_val(entry) & PAGE_MASK;
 +	}
 +	bit = 1 << ((__pa(table) & ~PAGE_MASK) / (PTRS_PER_PTE*sizeof(pte_t)));
 +	spin_lock_bh(&mm->context.list_lock);
 +	if ((atomic_read(&page->_mapcount) & FRAG_MASK) != FRAG_MASK)
 +		list_del(&page->lru);
 +	mask = atomic_xor_bits(&page->_mapcount, bit | (bit << 4));
 +	if (mask & FRAG_MASK)
 +		list_add_tail(&page->lru, &mm->context.pgtable_list);
 +	spin_unlock_bh(&mm->context.list_lock);
 +	table = (unsigned long *) (__pa(table) | (bit << 4));
 +	tlb_remove_table(tlb, table);
 +}
 +
 +void __tlb_remove_table(void *_table)
 +{
 +	const unsigned long mask = (FRAG_MASK << 4) | FRAG_MASK;
 +	void *table = (void *)((unsigned long) _table & ~mask);
 +	unsigned type = (unsigned long) _table & mask;
 +
 +	if (type)
 +		__page_table_free_rcu(table, type);
 +	else
 +		free_pages((unsigned long) table, ALLOC_ORDER);
 +}
 +
 +static void tlb_remove_table_smp_sync(void *arg)
 +{
 +	/* Simply deliver the interrupt */
 +}
 +
 +static void tlb_remove_table_one(void *table)
 +{
  	/*
 -	 * Set page access key and fetch protection bit from pgste.
 -	 * The guest C/R information is still in the PGSTE, set real
 -	 * key C/R to 0.
 +	 * This isn't an RCU grace period and hence the page-tables cannot be
 +	 * assumed to be actually RCU-freed.
 +	 *
 +	 * It is however sufficient for software page-table walkers that rely
 +	 * on IRQ disabling. See the comment near struct mmu_table_batch.
  	 */
 -	nkey = (pgste_val(pgste) & (PGSTE_ACC_BITS | PGSTE_FP_BIT)) >> 56;
 -	nkey |= (pgste_val(pgste) & (PGSTE_GR_BIT | PGSTE_GC_BIT)) >> 48;
 -	page_set_storage_key(address, nkey, 0);
 -#endif
 +	smp_call_function(tlb_remove_table_smp_sync, NULL, 1);
 +	__tlb_remove_table(table);
  }
  
 -static inline pgste_t pgste_set_pte(pte_t *ptep, pgste_t pgste, pte_t entry)
 +static void tlb_remove_table_rcu(struct rcu_head *head)
  {
 -#ifdef CONFIG_PGSTE
 -	if ((pte_val(entry) & _PAGE_PRESENT) &&
 -	    (pte_val(entry) & _PAGE_WRITE) &&
 -	    !(pte_val(entry) & _PAGE_INVALID)) {
 -		if (!MACHINE_HAS_ESOP) {
 -			/*
 -			 * Without enhanced suppression-on-protection force
 -			 * the dirty bit on for all writable ptes.
 -			 */
 -			pte_val(entry) |= _PAGE_DIRTY;
 -			pte_val(entry) &= ~_PAGE_PROTECT;
 +	struct mmu_table_batch *batch;
 +	int i;
 +
 +	batch = container_of(head, struct mmu_table_batch, rcu);
 +
 +	for (i = 0; i < batch->nr; i++)
 +		__tlb_remove_table(batch->tables[i]);
 +
 +	free_page((unsigned long)batch);
 +}
 +
 +void tlb_table_flush(struct mmu_gather *tlb)
 +{
 +	struct mmu_table_batch **batch = &tlb->batch;
 +
 +	if (*batch) {
 +		__tlb_flush_mm(tlb->mm);
 +		call_rcu_sched(&(*batch)->rcu, tlb_remove_table_rcu);
 +		*batch = NULL;
 +	}
 +}
 +
 +void tlb_remove_table(struct mmu_gather *tlb, void *table)
 +{
 +	struct mmu_table_batch **batch = &tlb->batch;
 +
 +	if (*batch == NULL) {
 +		*batch = (struct mmu_table_batch *)
 +			__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
 +		if (*batch == NULL) {
 +			__tlb_flush_mm(tlb->mm);
 +			tlb_remove_table_one(table);
 +			return;
  		}
 -		if (!(pte_val(entry) & _PAGE_PROTECT))
 -			/* This pte allows write access, set user-dirty */
 -			pgste_val(pgste) |= PGSTE_UC_BIT;
 +		(*batch)->nr = 0;
  	}
 -#endif
 -	*ptep = entry;
 -	return pgste;
 +	(*batch)->tables[(*batch)->nr++] = table;
 +	if ((*batch)->nr == MAX_TABLE_BATCH)
 +		tlb_table_flush(tlb);
  }
  
++<<<<<<< HEAD
++=======
+ static inline pgste_t pgste_ipte_notify(struct mm_struct *mm,
+ 					unsigned long addr,
+ 					pte_t *ptep, pgste_t pgste)
+ {
+ #ifdef CONFIG_PGSTE
+ 	if (pgste_val(pgste) & PGSTE_IN_BIT) {
+ 		pgste_val(pgste) &= ~PGSTE_IN_BIT;
+ 		ptep_notify(mm, addr, ptep);
+ 	}
+ #endif
+ 	return pgste;
+ }
+ 
+ static inline pgste_t ptep_xchg_start(struct mm_struct *mm,
+ 				      unsigned long addr, pte_t *ptep)
+ {
+ 	pgste_t pgste = __pgste(0);
+ 
+ 	if (mm_has_pgste(mm)) {
+ 		pgste = pgste_get_lock(ptep);
+ 		pgste = pgste_ipte_notify(mm, addr, ptep, pgste);
+ 	}
+ 	return pgste;
+ }
+ 
+ static inline void ptep_xchg_commit(struct mm_struct *mm,
+ 				    unsigned long addr, pte_t *ptep,
+ 				    pgste_t pgste, pte_t old, pte_t new)
+ {
+ 	if (mm_has_pgste(mm)) {
+ 		if (pte_val(old) & _PAGE_INVALID)
+ 			pgste_set_key(ptep, pgste, new, mm);
+ 		if (pte_val(new) & _PAGE_INVALID) {
+ 			pgste = pgste_update_all(old, pgste, mm);
+ 			if ((pgste_val(pgste) & _PGSTE_GPS_USAGE_MASK) ==
+ 			    _PGSTE_GPS_USAGE_UNUSED)
+ 				pte_val(old) |= _PAGE_UNUSED;
+ 		}
+ 		pgste = pgste_set_pte(ptep, pgste, new);
+ 		pgste_set_unlock(ptep, pgste);
+ 	} else {
+ 		*ptep = new;
+ 	}
+ }
+ 
+ pte_t ptep_xchg_direct(struct mm_struct *mm, unsigned long addr,
+ 		       pte_t *ptep, pte_t new)
+ {
+ 	pgste_t pgste;
+ 	pte_t old;
+ 
+ 	preempt_disable();
+ 	pgste = ptep_xchg_start(mm, addr, ptep);
+ 	old = ptep_flush_direct(mm, addr, ptep);
+ 	ptep_xchg_commit(mm, addr, ptep, pgste, old, new);
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(ptep_xchg_direct);
+ 
+ pte_t ptep_xchg_lazy(struct mm_struct *mm, unsigned long addr,
+ 		     pte_t *ptep, pte_t new)
+ {
+ 	pgste_t pgste;
+ 	pte_t old;
+ 
+ 	preempt_disable();
+ 	pgste = ptep_xchg_start(mm, addr, ptep);
+ 	old = ptep_flush_lazy(mm, addr, ptep);
+ 	ptep_xchg_commit(mm, addr, ptep, pgste, old, new);
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(ptep_xchg_lazy);
+ 
+ pte_t ptep_modify_prot_start(struct mm_struct *mm, unsigned long addr,
+ 			     pte_t *ptep)
+ {
+ 	pgste_t pgste;
+ 	pte_t old;
+ 
+ 	preempt_disable();
+ 	pgste = ptep_xchg_start(mm, addr, ptep);
+ 	old = ptep_flush_lazy(mm, addr, ptep);
+ 	if (mm_has_pgste(mm)) {
+ 		pgste = pgste_update_all(old, pgste, mm);
+ 		pgste_set(ptep, pgste);
+ 	}
+ 	return old;
+ }
+ EXPORT_SYMBOL(ptep_modify_prot_start);
+ 
+ void ptep_modify_prot_commit(struct mm_struct *mm, unsigned long addr,
+ 			     pte_t *ptep, pte_t pte)
+ {
+ 	pgste_t pgste;
+ 
+ 	if (mm_has_pgste(mm)) {
+ 		pgste = pgste_get(ptep);
+ 		pgste_set_key(ptep, pgste, pte, mm);
+ 		pgste = pgste_set_pte(ptep, pgste, pte);
+ 		pgste_set_unlock(ptep, pgste);
+ 	} else {
+ 		*ptep = pte;
+ 	}
+ 	preempt_enable();
+ }
+ EXPORT_SYMBOL(ptep_modify_prot_commit);
+ 
+ static inline pmd_t pmdp_flush_direct(struct mm_struct *mm,
+ 				      unsigned long addr, pmd_t *pmdp)
+ {
+ 	pmd_t old;
+ 
+ 	old = *pmdp;
+ 	if (pmd_val(old) & _SEGMENT_ENTRY_INVALID)
+ 		return old;
+ 	if (!MACHINE_HAS_IDTE) {
+ 		__pmdp_csp(pmdp);
+ 		return old;
+ 	}
+ 	atomic_inc(&mm->context.flush_count);
+ 	if (MACHINE_HAS_TLB_LC &&
+ 	    cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))
+ 		__pmdp_idte_local(addr, pmdp);
+ 	else
+ 		__pmdp_idte(addr, pmdp);
+ 	atomic_dec(&mm->context.flush_count);
+ 	return old;
+ }
+ 
+ static inline pmd_t pmdp_flush_lazy(struct mm_struct *mm,
+ 				    unsigned long addr, pmd_t *pmdp)
+ {
+ 	pmd_t old;
+ 
+ 	old = *pmdp;
+ 	if (pmd_val(old) & _SEGMENT_ENTRY_INVALID)
+ 		return old;
+ 	atomic_inc(&mm->context.flush_count);
+ 	if (cpumask_equal(&mm->context.cpu_attach_mask,
+ 			  cpumask_of(smp_processor_id()))) {
+ 		pmd_val(*pmdp) |= _SEGMENT_ENTRY_INVALID;
+ 		mm->context.flush_mm = 1;
+ 	} else if (MACHINE_HAS_IDTE)
+ 		__pmdp_idte(addr, pmdp);
+ 	else
+ 		__pmdp_csp(pmdp);
+ 	atomic_dec(&mm->context.flush_count);
+ 	return old;
+ }
+ 
+ pmd_t pmdp_xchg_direct(struct mm_struct *mm, unsigned long addr,
+ 		       pmd_t *pmdp, pmd_t new)
+ {
+ 	pmd_t old;
+ 
+ 	preempt_disable();
+ 	old = pmdp_flush_direct(mm, addr, pmdp);
+ 	*pmdp = new;
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(pmdp_xchg_direct);
+ 
+ pmd_t pmdp_xchg_lazy(struct mm_struct *mm, unsigned long addr,
+ 		     pmd_t *pmdp, pmd_t new)
+ {
+ 	pmd_t old;
+ 
+ 	preempt_disable();
+ 	old = pmdp_flush_lazy(mm, addr, pmdp);
+ 	*pmdp = new;
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(pmdp_xchg_lazy);
+ 
+ static inline pud_t pudp_flush_direct(struct mm_struct *mm,
+ 				      unsigned long addr, pud_t *pudp)
+ {
+ 	pud_t old;
+ 
+ 	old = *pudp;
+ 	if (pud_val(old) & _REGION_ENTRY_INVALID)
+ 		return old;
+ 	if (!MACHINE_HAS_IDTE) {
+ 		/*
+ 		 * Invalid bit position is the same for pmd and pud, so we can
+ 		 * re-use _pmd_csp() here
+ 		 */
+ 		__pmdp_csp((pmd_t *) pudp);
+ 		return old;
+ 	}
+ 	atomic_inc(&mm->context.flush_count);
+ 	if (MACHINE_HAS_TLB_LC &&
+ 	    cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))
+ 		__pudp_idte_local(addr, pudp);
+ 	else
+ 		__pudp_idte(addr, pudp);
+ 	atomic_dec(&mm->context.flush_count);
+ 	return old;
+ }
+ 
+ pud_t pudp_xchg_direct(struct mm_struct *mm, unsigned long addr,
+ 		       pud_t *pudp, pud_t new)
+ {
+ 	pud_t old;
+ 
+ 	preempt_disable();
+ 	old = pudp_flush_direct(mm, addr, pudp);
+ 	*pudp = new;
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(pudp_xchg_direct);
+ 
++>>>>>>> d08de8e2d867 (s390/mm: add support for 2GB hugepages)
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 -				pgtable_t pgtable)
 +static inline void thp_split_vma(struct vm_area_struct *vma)
  {
 -	struct list_head *lh = (struct list_head *) pgtable;
 -
 -	assert_spin_locked(pmd_lockptr(mm, pmdp));
 +	unsigned long addr;
  
 -	/* FIFO */
 -	if (!pmd_huge_pte(mm, pmdp))
 -		INIT_LIST_HEAD(lh);
 -	else
 -		list_add(lh, (struct list_head *) pmd_huge_pte(mm, pmdp));
 -	pmd_huge_pte(mm, pmdp) = pgtable;
 +	for (addr = vma->vm_start; addr < vma->vm_end; addr += PAGE_SIZE)
 +		follow_page(vma, addr, FOLL_SPLIT);
  }
  
 -pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)
 +static inline void thp_split_mm(struct mm_struct *mm)
  {
 -	struct list_head *lh;
 -	pgtable_t pgtable;
 -	pte_t *ptep;
 +	struct vm_area_struct *vma;
  
 -	assert_spin_locked(pmd_lockptr(mm, pmdp));
 -
 -	/* FIFO */
 -	pgtable = pmd_huge_pte(mm, pmdp);
 -	lh = (struct list_head *) pgtable;
 -	if (list_empty(lh))
 -		pmd_huge_pte(mm, pmdp) = NULL;
 -	else {
 -		pmd_huge_pte(mm, pmdp) = (pgtable_t) lh->next;
 -		list_del(lh);
 +	for (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {
 +		thp_split_vma(vma);
 +		vma->vm_flags &= ~VM_HUGEPAGE;
 +		vma->vm_flags |= VM_NOHUGEPAGE;
  	}
 -	ptep = (pte_t *) pgtable;
 -	pte_val(*ptep) = _PAGE_INVALID;
 -	ptep++;
 -	pte_val(*ptep) = _PAGE_INVALID;
 -	return pgtable;
 +	mm->def_flags |= VM_NOHUGEPAGE;
 +}
 +#else
 +static inline void thp_split_mm(struct mm_struct *mm)
 +{
  }
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  
* Unmerged path arch/s390/mm/gmap.c
diff --git a/arch/s390/include/asm/hugetlb.h b/arch/s390/include/asm/hugetlb.h
index 11eae5f55b70..502339038ce2 100644
--- a/arch/s390/include/asm/hugetlb.h
+++ b/arch/s390/include/asm/hugetlb.h
@@ -44,7 +44,10 @@ void arch_release_hugepage(struct page *page);
 static inline void huge_pte_clear(struct mm_struct *mm, unsigned long addr,
 				  pte_t *ptep)
 {
-	pte_val(*ptep) = _SEGMENT_ENTRY_EMPTY;
+	if ((pte_val(*ptep) & _REGION_ENTRY_TYPE_MASK) == _REGION_ENTRY_TYPE_R3)
+		pte_val(*ptep) = _REGION3_ENTRY_EMPTY;
+	else
+		pte_val(*ptep) = _SEGMENT_ENTRY_EMPTY;
 }
 
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 316c8503a3b4..dd603e20f135 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -21,6 +21,7 @@
 #define HPAGE_SIZE	(1UL << HPAGE_SHIFT)
 #define HPAGE_MASK	(~(HPAGE_SIZE - 1))
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+#define HUGE_MAX_HSTATE		2
 
 #define ARCH_HAS_SETCLEAR_HUGE_PTE
 #define ARCH_HAS_HUGE_PTE_TYPE
* Unmerged path arch/s390/include/asm/pgtable.h
* Unmerged path arch/s390/mm/gmap.c
diff --git a/arch/s390/mm/gup.c b/arch/s390/mm/gup.c
index 9c9d0961e099..d4d06ff0d6ea 100644
--- a/arch/s390/mm/gup.c
+++ b/arch/s390/mm/gup.c
@@ -140,6 +140,44 @@ static inline int gup_pmd_range(pud_t *pudp, pud_t pud, unsigned long addr,
 	return 1;
 }
 
+static int gup_huge_pud(pud_t *pudp, pud_t pud, unsigned long addr,
+		unsigned long end, int write, struct page **pages, int *nr)
+{
+	struct page *head, *page;
+	unsigned long mask;
+	int refs;
+
+	mask = (write ? _REGION_ENTRY_PROTECT : 0) | _REGION_ENTRY_INVALID;
+	if ((pud_val(pud) & mask) != 0)
+		return 0;
+	VM_BUG_ON(!pfn_valid(pud_pfn(pud)));
+
+	refs = 0;
+	head = pud_page(pud);
+	page = head + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+	do {
+		VM_BUG_ON_PAGE(compound_head(page) != head, page);
+		pages[*nr] = page;
+		(*nr)++;
+		page++;
+		refs++;
+	} while (addr += PAGE_SIZE, addr != end);
+
+	if (!page_cache_add_speculative(head, refs)) {
+		*nr -= refs;
+		return 0;
+	}
+
+	if (unlikely(pud_val(pud) != pud_val(*pudp))) {
+		*nr -= refs;
+		while (refs--)
+			put_page(head);
+		return 0;
+	}
+
+	return 1;
+}
+
 static inline int gup_pud_range(pgd_t *pgdp, pgd_t pgd, unsigned long addr,
 		unsigned long end, int write, struct page **pages, int *nr)
 {
@@ -158,7 +196,12 @@ static inline int gup_pud_range(pgd_t *pgdp, pgd_t pgd, unsigned long addr,
 		next = pud_addr_end(addr, end);
 		if (pud_none(pud))
 			return 0;
-		if (!gup_pmd_range(pudp, pud, addr, next, write, pages, nr))
+		if (unlikely(pud_large(pud))) {
+			if (!gup_huge_pud(pudp, pud, addr, next, write, pages,
+					  nr))
+				return 0;
+		} else if (!gup_pmd_range(pudp, pud, addr, next, write, pages,
+					  nr))
 			return 0;
 	} while (pudp++, addr = next, addr != end);
 
* Unmerged path arch/s390/mm/hugetlbpage.c
* Unmerged path arch/s390/mm/pgtable.c
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 7cfa031212e3..19b58756510c 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -668,7 +668,9 @@ static int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)
 		((node = hstate_next_node_to_free(hs, mask)) || 1);	\
 		nr_nodes--)
 
-#if defined(CONFIG_X86_64) && ((defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || defined(CONFIG_CMA))
+#if (defined(CONFIG_X86_64) || defined(CONFIG_S390)) && \
+	((defined(CONFIG_MEMORY_ISOLATION) && defined(CONFIG_COMPACTION)) || \
+	defined(CONFIG_CMA))
 static void destroy_compound_gigantic_page(struct page *page,
 					unsigned long order)
 {
