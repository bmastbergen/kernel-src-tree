NTB: Use unique DMA channels for TX and RX

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [ntb] Use unique DMA channels for TX and RX (Suravee Suthikulpanit) [1303727]
Rebuild_FUZZ: 93.67%
commit-author Dave Jiang <dave.jiang@intel.com>
commit 569410ca756cd3ebb15609cb6828a8393fb6384d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/569410ca.failed

Allocate two DMA channels, one for TX operation and one for RX
operation, instead of having one DMA channel for everything. This
provides slightly better performance, and also will make error handling
cleaner later on.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
	Signed-off-by: Jon Mason <jdmason@kudzu.us>
(cherry picked from commit 569410ca756cd3ebb15609cb6828a8393fb6384d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/ntb/ntb_transport.c
diff --cc drivers/ntb/ntb_transport.c
index 0d5e96e60628,6e3ee907d186..000000000000
--- a/drivers/ntb/ntb_transport.c
+++ b/drivers/ntb/ntb_transport.c
@@@ -94,14 -116,17 +94,15 @@@ struct ntb_rx_info 
  };
  
  struct ntb_transport_qp {
 -	struct ntb_transport_ctx *transport;
 -	struct ntb_dev *ndev;
 +	struct ntb_transport *transport;
 +	struct ntb_device *ndev;
  	void *cb_data;
- 	struct dma_chan *dma_chan;
+ 	struct dma_chan *tx_dma_chan;
+ 	struct dma_chan *rx_dma_chan;
  
  	bool client_ready;
 -	bool link_is_up;
 -
 +	bool qp_link;
  	u8 qp_num;	/* Only 64 QP's are allowed.  0-63 */
 -	u64 qp_bit;
  
  	struct ntb_rx_info __iomem *rx_info;
  	struct ntb_rx_info *remote_rx_info;
@@@ -449,15 -490,32 +450,31 @@@ static ssize_t debugfs_read(struct fil
  	out_offset += snprintf(buf + out_offset, out_count - out_offset,
  			       "tx_err_no_buf - %llu\n", qp->tx_err_no_buf);
  	out_offset += snprintf(buf + out_offset, out_count - out_offset,
 -			       "tx_mw - \t0x%p\n", qp->tx_mw);
 -	out_offset += snprintf(buf + out_offset, out_count - out_offset,
 -			       "tx_index (H) - \t%u\n", qp->tx_index);
 +			       "tx_mw - \t%p\n", qp->tx_mw);
  	out_offset += snprintf(buf + out_offset, out_count - out_offset,
 -			       "RRI (T) - \t%u\n",
 -			       qp->remote_rx_info->entry);
 +			       "tx_index - \t%u\n", qp->tx_index);
  	out_offset += snprintf(buf + out_offset, out_count - out_offset,
  			       "tx_max_entry - \t%u\n", qp->tx_max_entry);
 -	out_offset += snprintf(buf + out_offset, out_count - out_offset,
 -			       "free tx - \t%u\n",
 -			       ntb_transport_tx_free_entry(qp));
  
  	out_offset += snprintf(buf + out_offset, out_count - out_offset,
++<<<<<<< HEAD
 +			       "\nQP Link %s\n", (qp->qp_link == NTB_LINK_UP) ?
 +			       "Up" : "Down");
++=======
+ 			       "\n");
+ 	out_offset += snprintf(buf + out_offset, out_count - out_offset,
+ 			       "Using TX DMA - \t%s\n",
+ 			       qp->tx_dma_chan ? "Yes" : "No");
+ 	out_offset += snprintf(buf + out_offset, out_count - out_offset,
+ 			       "Using RX DMA - \t%s\n",
+ 			       qp->rx_dma_chan ? "Yes" : "No");
+ 	out_offset += snprintf(buf + out_offset, out_count - out_offset,
+ 			       "QP Link - \t%s\n",
+ 			       qp->link_is_up ? "Up" : "Down");
+ 	out_offset += snprintf(buf + out_offset, out_count - out_offset,
+ 			       "\n");
+ 
++>>>>>>> 569410ca756c (NTB: Use unique DMA channels for TX and RX)
  	if (out_offset > out_count)
  		out_offset = out_count;
  
@@@ -1062,9 -1225,9 +1079,9 @@@ static void ntb_async_rx(struct ntb_que
  {
  	struct dma_async_tx_descriptor *txd;
  	struct ntb_transport_qp *qp = entry->qp;
- 	struct dma_chan *chan = qp->dma_chan;
+ 	struct dma_chan *chan = qp->rx_dma_chan;
  	struct dma_device *device;
 -	size_t pay_off, buff_off, len;
 +	size_t pay_off, buff_off;
  	struct dmaengine_unmap_data *unmap;
  	dma_cookie_t cookie;
  	void *buf = entry->buf;
@@@ -1238,10 -1386,24 +1255,15 @@@ static int ntb_transport_rxc_db(void *d
  			break;
  	}
  
++<<<<<<< HEAD
 +	if (qp->dma_chan)
 +		dma_async_issue_pending(qp->dma_chan);
++=======
+ 	if (i && qp->rx_dma_chan)
+ 		dma_async_issue_pending(qp->rx_dma_chan);
++>>>>>>> 569410ca756c (NTB: Use unique DMA channels for TX and RX)
  
 -	if (i == qp->rx_max_entry) {
 -		/* there is more work to do */
 -		tasklet_schedule(&qp->rxc_db_work);
 -	} else if (ntb_db_read(qp->ndev) & BIT_ULL(qp->qp_num)) {
 -		/* the doorbell bit is set: clear it */
 -		ntb_db_clear(qp->ndev, BIT_ULL(qp->qp_num));
 -		/* ntb_db_read ensures ntb_db_clear write is committed */
 -		ntb_db_read(qp->ndev);
 -
 -		/* an interrupt may have arrived between finishing
 -		 * ntb_process_rxc and clearing the doorbell bit:
 -		 * there might be some more work to do.
 -		 */
 -		tasklet_schedule(&qp->rxc_db_work);
 -	}
 +	return i;
  }
  
  static void ntb_tx_copy_callback(void *data)
@@@ -1463,15 -1643,34 +1485,43 @@@ ntb_transport_create_queue(void *data, 
  	qp->tx_handler = handlers->tx_handler;
  	qp->event_handler = handlers->event_handler;
  
++<<<<<<< HEAD
 +	dmaengine_get();
 +	qp->dma_chan = dma_find_channel(DMA_MEMCPY);
 +	if (!qp->dma_chan) {
 +		dmaengine_put();
 +		dev_info(&pdev->dev, "Unable to allocate DMA channel, using CPU instead\n");
 +	}
++=======
+ 	dma_cap_zero(dma_mask);
+ 	dma_cap_set(DMA_MEMCPY, dma_mask);
+ 
+ 	if (use_dma) {
+ 		qp->tx_dma_chan =
+ 			dma_request_channel(dma_mask, ntb_dma_filter_fn,
+ 					    (void *)(unsigned long)node);
+ 		if (!qp->tx_dma_chan)
+ 			dev_info(&pdev->dev, "Unable to allocate TX DMA channel\n");
+ 
+ 		qp->rx_dma_chan =
+ 			dma_request_channel(dma_mask, ntb_dma_filter_fn,
+ 					    (void *)(unsigned long)node);
+ 		if (!qp->rx_dma_chan)
+ 			dev_info(&pdev->dev, "Unable to allocate RX DMA channel\n");
+ 	} else {
+ 		qp->tx_dma_chan = NULL;
+ 		qp->rx_dma_chan = NULL;
+ 	}
+ 
+ 	dev_dbg(&pdev->dev, "Using %s memcpy for TX\n",
+ 		qp->tx_dma_chan ? "DMA" : "CPU");
+ 
+ 	dev_dbg(&pdev->dev, "Using %s memcpy for RX\n",
+ 		qp->rx_dma_chan ? "DMA" : "CPU");
++>>>>>>> 569410ca756c (NTB: Use unique DMA channels for TX and RX)
  
  	for (i = 0; i < NTB_QP_DEF_NUM_ENTRIES; i++) {
 -		entry = kzalloc_node(sizeof(*entry), GFP_ATOMIC, node);
 +		entry = kzalloc(sizeof(struct ntb_queue_entry), GFP_ATOMIC);
  		if (!entry)
  			goto err1;
  
@@@ -1503,11 -1700,13 +1553,19 @@@ err2
  	while ((entry = ntb_list_rm(&qp->ntb_tx_free_q_lock, &qp->tx_free_q)))
  		kfree(entry);
  err1:
 -	while ((entry = ntb_list_rm(&qp->ntb_rx_q_lock, &qp->rx_free_q)))
 +	while ((entry = ntb_list_rm(&qp->ntb_rx_free_q_lock, &qp->rx_free_q)))
  		kfree(entry);
++<<<<<<< HEAD
 +	if (qp->dma_chan)
 +		dmaengine_put();
 +	set_bit(free_queue, &nt->qp_bitmap);
++=======
+ 	if (qp->tx_dma_chan)
+ 		dma_release_channel(qp->tx_dma_chan);
+ 	if (qp->rx_dma_chan)
+ 		dma_release_channel(qp->rx_dma_chan);
+ 	nt->qp_bitmap_free |= qp_bit;
++>>>>>>> 569410ca756c (NTB: Use unique DMA channels for TX and RX)
  err:
  	return NULL;
  }
@@@ -1527,10 -1727,25 +1585,25 @@@ void ntb_transport_free_queue(struct nt
  	if (!qp)
  		return;
  
 -	pdev = qp->ndev->pdev;
 +	pdev = ntb_query_pdev(qp->ndev);
  
- 	if (qp->dma_chan) {
- 		struct dma_chan *chan = qp->dma_chan;
+ 	if (qp->tx_dma_chan) {
+ 		struct dma_chan *chan = qp->tx_dma_chan;
+ 		/* Putting the dma_chan to NULL will force any new traffic to be
+ 		 * processed by the CPU instead of the DAM engine
+ 		 */
+ 		qp->tx_dma_chan = NULL;
+ 
+ 		/* Try to be nice and wait for any queued DMA engine
+ 		 * transactions to process before smashing it with a rock
+ 		 */
+ 		dma_sync_wait(chan, qp->last_cookie);
+ 		dmaengine_terminate_all(chan);
+ 		dma_release_channel(chan);
+ 	}
+ 
+ 	if (qp->rx_dma_chan) {
+ 		struct dma_chan *chan = qp->rx_dma_chan;
  		/* Putting the dma_chan to NULL will force any new traffic to be
  		 * processed by the CPU instead of the DAM engine
  		 */
* Unmerged path drivers/ntb/ntb_transport.c
