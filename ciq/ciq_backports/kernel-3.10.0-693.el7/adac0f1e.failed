s390/topology: add drawer scheduling domain level

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [s390] topology: add drawer scheduling domain level (Hendrik Brueckner) [1380774]
Rebuild_FUZZ: 94.62%
commit-author Heiko Carstens <heiko.carstens@de.ibm.com>
commit adac0f1e8c08548d82a48c9913ebc9787f946440
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/adac0f1e.failed

The z13 machine added a fourth level to the cpu topology
information. The new top level is called drawer.

A drawer contains two books, which used to be the top level.

Adding this additional scheduling domain did show performance
improvements for some workloads of up to 8%, while there don't
seem to be any workloads impacted in a negative way.

	Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit adac0f1e8c08548d82a48c9913ebc9787f946440)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/topology.h
#	arch/s390/kernel/topology.c
#	arch/s390/numa/mode_emu.c
diff --cc arch/s390/include/asm/topology.h
index 8261fc56abac,f15f5571ca2b..000000000000
--- a/arch/s390/include/asm/topology.h
+++ b/arch/s390/include/asm/topology.h
@@@ -13,20 -14,26 +13,39 @@@ struct cpu_topology_s390 
  	unsigned short core_id;
  	unsigned short socket_id;
  	unsigned short book_id;
++<<<<<<< HEAD
++=======
+ 	unsigned short drawer_id;
+ 	unsigned short node_id;
++>>>>>>> adac0f1e8c08 (s390/topology: add drawer scheduling domain level)
  	cpumask_t thread_mask;
  	cpumask_t core_mask;
  	cpumask_t book_mask;
+ 	cpumask_t drawer_mask;
  };
  
 -DECLARE_PER_CPU(struct cpu_topology_s390, cpu_topology);
 -
 +extern struct cpu_topology_s390 cpu_topology[NR_CPUS];
 +
++<<<<<<< HEAD
 +#define topology_physical_package_id(cpu)	(cpu_topology[cpu].socket_id)
 +#define topology_thread_id(cpu)			(cpu_topology[cpu].thread_id)
 +#define topology_sibling_cpumask(cpu)		(&cpu_topology[cpu].thread_mask)
 +#define topology_core_id(cpu)			(cpu_topology[cpu].core_id)
 +#define topology_core_cpumask(cpu)		(&cpu_topology[cpu].core_mask)
 +#define topology_book_id(cpu)			(cpu_topology[cpu].book_id)
 +#define topology_book_cpumask(cpu)		(&cpu_topology[cpu].book_mask)
++=======
+ #define topology_physical_package_id(cpu) (per_cpu(cpu_topology, cpu).socket_id)
+ #define topology_thread_id(cpu)		  (per_cpu(cpu_topology, cpu).thread_id)
+ #define topology_sibling_cpumask(cpu) \
+ 		(&per_cpu(cpu_topology, cpu).thread_mask)
+ #define topology_core_id(cpu)		  (per_cpu(cpu_topology, cpu).core_id)
+ #define topology_core_cpumask(cpu)	  (&per_cpu(cpu_topology, cpu).core_mask)
+ #define topology_book_id(cpu)		  (per_cpu(cpu_topology, cpu).book_id)
+ #define topology_book_cpumask(cpu)	  (&per_cpu(cpu_topology, cpu).book_mask)
+ #define topology_drawer_id(cpu)		  (per_cpu(cpu_topology, cpu).drawer_id)
+ #define topology_drawer_cpumask(cpu)	  (&per_cpu(cpu_topology, cpu).drawer_mask)
++>>>>>>> adac0f1e8c08 (s390/topology: add drawer scheduling domain level)
  
  #define mc_capable() 1
  
diff --cc arch/s390/kernel/topology.c
index 02c1293fec71,44745e751c3a..000000000000
--- a/arch/s390/kernel/topology.c
+++ b/arch/s390/kernel/topology.c
@@@ -34,16 -37,19 +34,17 @@@ static void set_topology_timer(void)
  static void topology_work_fn(struct work_struct *work);
  static struct sysinfo_15_1_x *tl_info;
  
 -static bool topology_enabled = true;
 +static int topology_enabled = 1;
  static DECLARE_WORK(topology_work, topology_work_fn);
  
 -/*
 - * Socket/Book linked lists and per_cpu(cpu_topology) updates are
 - * protected by "sched_domains_mutex".
 - */
 +/* topology_lock protects the socket and book linked lists */
 +static DEFINE_SPINLOCK(topology_lock);
  static struct mask_info socket_info;
  static struct mask_info book_info;
+ static struct mask_info drawer_info;
  
 -DEFINE_PER_CPU(struct cpu_topology_s390, cpu_topology);
 -EXPORT_PER_CPU_SYMBOL_GPL(cpu_topology);
 +struct cpu_topology_s390 cpu_topology[NR_CPUS];
 +EXPORT_SYMBOL_GPL(cpu_topology);
  
  static cpumask_t cpu_group_map(struct mask_info *info, unsigned int cpu)
  {
@@@ -90,9 -98,12 +92,18 @@@ static struct mask_info *add_cpus_to_ma
  		if (lcpu < 0)
  			continue;
  		for (i = 0; i <= smp_cpu_mtid; i++) {
++<<<<<<< HEAD
 +			cpu_topology[lcpu + i].book_id = book->id;
 +			cpu_topology[lcpu + i].core_id = rcore;
 +			cpu_topology[lcpu + i].thread_id = lcpu + i;
++=======
+ 			topo = &per_cpu(cpu_topology, lcpu + i);
+ 			topo->drawer_id = drawer->id;
+ 			topo->book_id = book->id;
+ 			topo->core_id = rcore;
+ 			topo->thread_id = lcpu + i;
+ 			cpumask_set_cpu(lcpu + i, &drawer->mask);
++>>>>>>> adac0f1e8c08 (s390/topology: add drawer scheduling domain level)
  			cpumask_set_cpu(lcpu + i, &book->mask);
  			cpumask_set_cpu(lcpu + i, &socket->mask);
  			if (one_socket_per_cpu)
@@@ -244,22 -264,24 +266,36 @@@ int topology_set_cpu_management(int fc
  
  static void update_cpu_masks(void)
  {
 -	struct cpu_topology_s390 *topo;
 +	unsigned long flags;
  	int cpu;
  
 +	spin_lock_irqsave(&topology_lock, flags);
  	for_each_possible_cpu(cpu) {
++<<<<<<< HEAD
 +		cpu_topology[cpu].thread_mask = cpu_thread_map(cpu);
 +		cpu_topology[cpu].core_mask = cpu_group_map(&socket_info, cpu);
 +		cpu_topology[cpu].book_mask = cpu_group_map(&book_info, cpu);
 +		if (!MACHINE_HAS_TOPOLOGY) {
 +			cpu_topology[cpu].thread_id = cpu;
 +			cpu_topology[cpu].core_id = cpu;
 +			cpu_topology[cpu].socket_id = cpu;
 +			cpu_topology[cpu].book_id = cpu;
++=======
+ 		topo = &per_cpu(cpu_topology, cpu);
+ 		topo->thread_mask = cpu_thread_map(cpu);
+ 		topo->core_mask = cpu_group_map(&socket_info, cpu);
+ 		topo->book_mask = cpu_group_map(&book_info, cpu);
+ 		topo->drawer_mask = cpu_group_map(&drawer_info, cpu);
+ 		if (!MACHINE_HAS_TOPOLOGY) {
+ 			topo->thread_id = cpu;
+ 			topo->core_id = cpu;
+ 			topo->socket_id = cpu;
+ 			topo->book_id = cpu;
+ 			topo->drawer_id = cpu;
++>>>>>>> adac0f1e8c08 (s390/topology: add drawer scheduling domain level)
  		}
  	}
 -	numa_update_cpu_topology();
 +	spin_unlock_irqrestore(&topology_lock, flags);
  }
  
  void store_topology(struct sysinfo_15_1_x *info)
@@@ -463,15 -440,85 +496,91 @@@ int topology_cpu_init(struct cpu *cpu
  	return sysfs_create_group(&cpu->dev.kobj, &topology_cpu_attr_group);
  }
  
++<<<<<<< HEAD
++=======
+ static const struct cpumask *cpu_thread_mask(int cpu)
+ {
+ 	return &per_cpu(cpu_topology, cpu).thread_mask;
+ }
+ 
+ 
+ const struct cpumask *cpu_coregroup_mask(int cpu)
+ {
+ 	return &per_cpu(cpu_topology, cpu).core_mask;
+ }
+ 
+ static const struct cpumask *cpu_book_mask(int cpu)
+ {
+ 	return &per_cpu(cpu_topology, cpu).book_mask;
+ }
+ 
+ static const struct cpumask *cpu_drawer_mask(int cpu)
+ {
+ 	return &per_cpu(cpu_topology, cpu).drawer_mask;
+ }
+ 
+ static int __init early_parse_topology(char *p)
+ {
+ 	return kstrtobool(p, &topology_enabled);
+ }
+ early_param("topology", early_parse_topology);
+ 
+ static struct sched_domain_topology_level s390_topology[] = {
+ 	{ cpu_thread_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
+ 	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
+ 	{ cpu_book_mask, SD_INIT_NAME(BOOK) },
+ 	{ cpu_drawer_mask, SD_INIT_NAME(DRAWER) },
+ 	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
+ 	{ NULL, },
+ };
+ 
+ static void __init alloc_masks(struct sysinfo_15_1_x *info,
+ 			       struct mask_info *mask, int offset)
+ {
+ 	int i, nr_masks;
+ 
+ 	nr_masks = info->mag[TOPOLOGY_NR_MAG - offset];
+ 	for (i = 0; i < info->mnest - offset; i++)
+ 		nr_masks *= info->mag[TOPOLOGY_NR_MAG - offset - 1 - i];
+ 	nr_masks = max(nr_masks, 1);
+ 	for (i = 0; i < nr_masks; i++) {
+ 		mask->next = kzalloc(sizeof(*mask->next), GFP_KERNEL);
+ 		mask = mask->next;
+ 	}
+ }
+ 
+ static int __init s390_topology_init(void)
+ {
+ 	struct sysinfo_15_1_x *info;
+ 	int i;
+ 
+ 	if (!MACHINE_HAS_TOPOLOGY)
+ 		return 0;
+ 	tl_info = (struct sysinfo_15_1_x *)__get_free_page(GFP_KERNEL);
+ 	info = tl_info;
+ 	store_topology(info);
+ 	pr_info("The CPU configuration topology of the machine is:");
+ 	for (i = 0; i < TOPOLOGY_NR_MAG; i++)
+ 		printk(KERN_CONT " %d", info->mag[i]);
+ 	printk(KERN_CONT " / %d\n", info->mnest);
+ 	alloc_masks(info, &socket_info, 1);
+ 	alloc_masks(info, &book_info, 2);
+ 	alloc_masks(info, &drawer_info, 3);
+ 	set_sched_topology(s390_topology);
+ 	return 0;
+ }
+ early_initcall(s390_topology_init);
+ 
++>>>>>>> adac0f1e8c08 (s390/topology: add drawer scheduling domain level)
  static int __init topology_init(void)
  {
 -	if (MACHINE_HAS_TOPOLOGY)
 -		set_topology_timer();
 -	else
 +	if (!MACHINE_HAS_TOPOLOGY) {
  		topology_update_polarization_simple();
 +		goto out;
 +	}
 +	set_topology_timer();
 +out:
 +	update_cpu_masks();
  	return device_create_file(cpu_subsys.dev_root, &dev_attr_dispatching);
  }
  device_initcall(topology_init);
* Unmerged path arch/s390/numa/mode_emu.c
diff --git a/arch/s390/Kconfig b/arch/s390/Kconfig
index 11cceb7a4b0a..fbe27082541d 100644
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@ -398,6 +398,9 @@ config SCHED_MC
 config SCHED_BOOK
 	def_bool n
 
+config SCHED_DRAWER
+	def_bool n
+
 config SCHED_TOPOLOGY
 	def_bool y
 	prompt "Topology scheduler support"
@@ -405,6 +408,7 @@ config SCHED_TOPOLOGY
 	select SCHED_SMT
 	select SCHED_MC
 	select SCHED_BOOK
+	select SCHED_DRAWER
 	help
 	  Topology scheduler support improves the CPU scheduler's decision
 	  making when dealing with machines that have multi-threading,
* Unmerged path arch/s390/include/asm/topology.h
* Unmerged path arch/s390/kernel/topology.c
* Unmerged path arch/s390/numa/mode_emu.c
