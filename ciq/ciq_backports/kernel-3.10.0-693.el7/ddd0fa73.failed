locking/rwsem: Streamline the rwsem_optimistic_spin() code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Waiman Long <Waiman.Long@hpe.com>
commit ddd0fa73c2b71c35de4fe7ae60a5f1a6cddc2cf0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ddd0fa73.failed

This patch moves the owner loading and checking code entirely inside of
rwsem_spin_on_owner() to simplify the logic of rwsem_optimistic_spin()
loop.

	Suggested-by: Peter Hurley <peter@hurleysoftware.com>
	Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Peter Hurley <peter@hurleysoftware.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Douglas Hatch <doug.hatch@hpe.com>
	Cc: Jason Low <jason.low2@hp.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Scott J Norton <scott.norton@hpe.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/1463534783-38814-6-git-send-email-Waiman.Long@hpe.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit ddd0fa73c2b71c35de4fe7ae60a5f1a6cddc2cf0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rwsem.c
diff --cc lib/rwsem.c
index 09d8c2da4ff3,2031281bb940..000000000000
--- a/lib/rwsem.c
+++ b/lib/rwsem.c
@@@ -320,10 -350,15 +320,19 @@@ done
  	return ret;
  }
  
- static noinline
- bool rwsem_spin_on_owner(struct rw_semaphore *sem, struct task_struct *owner)
+ /*
+  * Return true only if we can still spin on the owner field of the rwsem.
+  */
+ static noinline bool rwsem_spin_on_owner(struct rw_semaphore *sem)
  {
++<<<<<<< HEAD:lib/rwsem.c
 +	long count;
++=======
+ 	struct task_struct *owner = READ_ONCE(sem->owner);
+ 
+ 	if (!rwsem_owner_is_writer(owner))
+ 		goto out;
++>>>>>>> ddd0fa73c2b7 (locking/rwsem: Streamline the rwsem_optimistic_spin() code):kernel/locking/rwsem-xadd.c
  
  	rcu_read_lock();
  	while (sem->owner == owner) {
@@@ -341,20 -376,15 +350,24 @@@
  			return false;
  		}
  
 -		cpu_relax_lowlatency();
 +		arch_mutex_cpu_relax();
  	}
  	rcu_read_unlock();
++<<<<<<< HEAD:lib/rwsem.c
 +
 +	if (READ_ONCE(sem->owner))
 +		return true; /* new owner, continue spinning */
 +
++=======
+ out:
++>>>>>>> ddd0fa73c2b7 (locking/rwsem: Streamline the rwsem_optimistic_spin() code):kernel/locking/rwsem-xadd.c
  	/*
 -	 * If there is a new owner or the owner is not set, we continue
 -	 * spinning.
 +	 * When the owner is not set, the lock could be free or
 +	 * held by readers. Check the counter to verify the
 +	 * state.
  	 */
 -	return !rwsem_owner_is_reader(READ_ONCE(sem->owner));
 +	count = READ_ONCE(sem->count);
 +	return (count == 0 || count == RWSEM_WAITING_BIAS);
  }
  
  static bool rwsem_optimistic_spin(struct rw_semaphore *sem)
@@@ -371,12 -400,17 +383,26 @@@
  	if (!osq_lock(&sem->osq))
  		goto done;
  
++<<<<<<< HEAD:lib/rwsem.c
 +	while (true) {
 +		owner = ACCESS_ONCE(sem->owner);
 +		if (owner && !rwsem_spin_on_owner(sem, owner))
 +			break;
 +
 +		/* wait_lock will be acquired if write_lock is obtained */
++=======
+ 	/*
+ 	 * Optimistically spin on the owner field and attempt to acquire the
+ 	 * lock whenever the owner changes. Spinning will be stopped when:
+ 	 *  1) the owning writer isn't running; or
+ 	 *  2) readers own the lock as we can't determine if they are
+ 	 *     actively running or not.
+ 	 */
+ 	while (rwsem_spin_on_owner(sem)) {
+ 		/*
+ 		 * Try to acquire the lock
+ 		 */
++>>>>>>> ddd0fa73c2b7 (locking/rwsem: Streamline the rwsem_optimistic_spin() code):kernel/locking/rwsem-xadd.c
  		if (rwsem_try_write_lock_unqueued(sem)) {
  			taken = true;
  			break;
* Unmerged path lib/rwsem.c
