NTB: Default to CPU memcpy for performance

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [ntb] Default to CPU memcpy for performance (Suravee Suthikulpanit) [1303727]
Rebuild_FUZZ: 93.67%
commit-author Dave Jiang <dave.jiang@intel.com>
commit a41ef053f700618f5f55a1dd658908a71163400b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/a41ef053.failed

Disable DMA usage by default, since the CPU provides much better
performance with write combining.  Provide a module parameter to enable
DMA usage when offloading the memcpy is preferred.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
	Signed-off-by: Allen Hubbe <Allen.Hubbe@emc.com>
	Signed-off-by: Jon Mason <jdmason@kudzu.us>
(cherry picked from commit a41ef053f700618f5f55a1dd658908a71163400b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/ntb/ntb_transport.c
diff --cc drivers/ntb/ntb_transport.c
index bf77f3a0b404,e07b056af3be..000000000000
--- a/drivers/ntb/ntb_transport.c
+++ b/drivers/ntb/ntb_transport.c
@@@ -72,6 -88,12 +72,15 @@@ static unsigned int copy_bytes = 1024
  module_param(copy_bytes, uint, 0644);
  MODULE_PARM_DESC(copy_bytes, "Threshold under which NTB will use the CPU to copy instead of DMA");
  
++<<<<<<< HEAD
++=======
+ static bool use_dma;
+ module_param(use_dma, bool, 0644);
+ MODULE_PARM_DESC(use_dma, "Use DMA engine to perform large data copy");
+ 
+ static struct dentry *nt_debugfs_dir;
+ 
++>>>>>>> a41ef053f700 (NTB: Default to CPU memcpy for performance)
  struct ntb_queue_entry {
  	/* ntb_queue list reference */
  	struct list_head entry;
@@@ -1459,15 -1590,21 +1468,30 @@@ ntb_transport_create_queue(void *data, 
  	qp->tx_handler = handlers->tx_handler;
  	qp->event_handler = handlers->event_handler;
  
++<<<<<<< HEAD
 +	dmaengine_get();
 +	qp->dma_chan = dma_find_channel(DMA_MEMCPY);
 +	if (!qp->dma_chan) {
 +		dmaengine_put();
 +		dev_info(&pdev->dev, "Unable to allocate DMA channel, using CPU instead\n");
 +	}
++=======
+ 	dma_cap_zero(dma_mask);
+ 	dma_cap_set(DMA_MEMCPY, dma_mask);
+ 
+ 	if (use_dma) {
+ 		qp->dma_chan = dma_request_channel(dma_mask, ntb_dma_filter_fn,
+ 						   (void *)(unsigned long)node);
+ 		if (!qp->dma_chan)
+ 			dev_info(&pdev->dev, "Unable to allocate DMA channel\n");
+ 	} else {
+ 		qp->dma_chan = NULL;
+ 	}
+ 	dev_dbg(&pdev->dev, "Using %s memcpy\n", qp->dma_chan ? "DMA" : "CPU");
++>>>>>>> a41ef053f700 (NTB: Default to CPU memcpy for performance)
  
  	for (i = 0; i < NTB_QP_DEF_NUM_ENTRIES; i++) {
 -		entry = kzalloc_node(sizeof(*entry), GFP_ATOMIC, node);
 +		entry = kzalloc(sizeof(struct ntb_queue_entry), GFP_ATOMIC);
  		if (!entry)
  			goto err1;
  
* Unmerged path drivers/ntb/ntb_transport.c
