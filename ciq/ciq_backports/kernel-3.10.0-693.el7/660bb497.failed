xprtrdma: Refactor the FRWR recovery worker

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 660bb497d0ae0c9e6be5beaff7ba17bfa5c9718c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/660bb497.failed

Maintain the order of invalidation and DMA unmapping when doing
a background MR reset.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 660bb497d0ae0c9e6be5beaff7ba17bfa5c9718c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/frwr_ops.c
diff --cc net/sunrpc/xprtrdma/frwr_ops.c
index ee5e390519e0,4e0a5c1abea4..000000000000
--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@@ -98,6 -98,47 +98,50 @@@ frwr_destroy_recovery_wq(void
  	destroy_workqueue(wq);
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ __frwr_reset_mr(struct rpcrdma_ia *ia, struct rpcrdma_mw *r)
+ {
+ 	struct rpcrdma_frmr *f = &r->frmr;
+ 	int rc;
+ 
+ 	rc = ib_dereg_mr(f->fr_mr);
+ 	if (rc) {
+ 		pr_warn("rpcrdma: ib_dereg_mr status %d, frwr %p orphaned\n",
+ 			rc, r);
+ 		return rc;
+ 	}
+ 
+ 	f->fr_mr = ib_alloc_mr(ia->ri_pd, IB_MR_TYPE_MEM_REG,
+ 			       ia->ri_max_frmr_depth);
+ 	if (IS_ERR(f->fr_mr)) {
+ 		pr_warn("rpcrdma: ib_alloc_mr status %ld, frwr %p orphaned\n",
+ 			PTR_ERR(f->fr_mr), r);
+ 		return PTR_ERR(f->fr_mr);
+ 	}
+ 
+ 	dprintk("RPC:       %s: recovered FRMR %p\n", __func__, r);
+ 	f->fr_state = FRMR_IS_INVALID;
+ 	return 0;
+ }
+ 
+ static void
+ __frwr_reset_and_unmap(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mw *mw)
+ {
+ 	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
+ 	struct rpcrdma_frmr *f = &mw->frmr;
+ 	int rc;
+ 
+ 	rc = __frwr_reset_mr(ia, mw);
+ 	ib_dma_unmap_sg(ia->ri_device, f->fr_sg, f->fr_nents, f->fr_dir);
+ 	if (rc)
+ 		return;
+ 
+ 	rpcrdma_put_mw(r_xprt, mw);
+ }
+ 
++>>>>>>> 660bb497d0ae (xprtrdma: Refactor the FRWR recovery worker)
  /* Deferred reset of a single FRMR. Generate a fresh rkey by
   * replacing the MR.
   *
@@@ -110,25 -151,9 +154,30 @@@ __frwr_recovery_worker(struct work_stru
  {
  	struct rpcrdma_mw *r = container_of(work, struct rpcrdma_mw,
  					    frmr.fr_work);
++<<<<<<< HEAD
 +	struct rpcrdma_xprt *r_xprt = r->frmr.fr_xprt;
 +	unsigned int depth = r_xprt->rx_ia.ri_max_frmr_depth;
 +	struct ib_pd *pd = r_xprt->rx_ia.ri_pd;
 +
 +	if (ib_dereg_mr(r->frmr.fr_mr))
 +		goto out_fail;
 +
 +	r->frmr.fr_mr = ib_alloc_mr(pd, IB_MR_TYPE_MEM_REG, depth);
 +	if (IS_ERR(r->frmr.fr_mr))
 +		goto out_fail;
 +
 +	dprintk("RPC:       %s: recovered FRMR %p\n", __func__, r);
 +	r->frmr.fr_state = FRMR_IS_INVALID;
 +	rpcrdma_put_mw(r_xprt, r);
++=======
+ 
+ 	__frwr_reset_and_unmap(r->frmr.fr_xprt, r);
++>>>>>>> 660bb497d0ae (xprtrdma: Refactor the FRWR recovery worker)
  	return;
 +
 +out_fail:
 +	pr_warn("RPC:       %s: FRMR %p unrecovered\n",
 +		__func__, r);
  }
  
  /* A broken MR was discovered in a context that can't sleep.
@@@ -466,7 -491,6 +515,10 @@@ frwr_op_map(struct rpcrdma_xprt *r_xprt
  
  out_senderr:
  	dprintk("RPC:       %s: ib_post_send status %i\n", __func__, rc);
++<<<<<<< HEAD
 +	ib_dma_unmap_sg(device, frmr->sg, dma_nents, direction);
++=======
++>>>>>>> 660bb497d0ae (xprtrdma: Refactor the FRWR recovery worker)
  	__frwr_queue_recovery(mw);
  	return rc;
  }
* Unmerged path net/sunrpc/xprtrdma/frwr_ops.c
