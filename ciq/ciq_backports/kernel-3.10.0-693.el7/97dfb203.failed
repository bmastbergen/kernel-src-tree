dm cache policy smq: cleanup free_target_met() and clean_target_met()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Mike Snitzer <snitzer@redhat.com>
commit 97dfb20309e0ad4fa22deb5bc5ed85604d5014ef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/97dfb203.failed

Depending on the passed @idle arg, there may be no need to calculate
'nr_free' or 'nr_clean' respectively in free_target_met() and
clean_target_met().

	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 97dfb20309e0ad4fa22deb5bc5ed85604d5014ef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-cache-policy-smq.c
diff --cc drivers/md/dm-cache-policy-smq.c
index c33f4a6e1d7d,72479bd61e11..000000000000
--- a/drivers/md/dm-cache-policy-smq.c
+++ b/drivers/md/dm-cache-policy-smq.c
@@@ -1095,34 -1101,148 +1095,142 @@@ static void end_cache_period(struct smq
  	}
  }
  
 -/*----------------------------------------------------------------*/
 -
 -/*
 - * Targets are given as a percentage.
 - */
 -#define CLEAN_TARGET 25u
 -#define FREE_TARGET 25u
 -
 -static unsigned percent_to_target(struct smq_policy *mq, unsigned p)
 +static int demote_cblock(struct smq_policy *mq,
 +			 struct policy_locker *locker,
 +			 dm_oblock_t *oblock)
  {
++<<<<<<< HEAD
 +	struct entry *demoted = q_peek(&mq->clean, mq->clean.nr_levels, false);
 +	if (!demoted)
++=======
+ 	return from_cblock(mq->cache_size) * p / 100u;
+ }
+ 
+ static bool clean_target_met(struct smq_policy *mq, bool idle)
+ {
+ 	/*
+ 	 * Cache entries may not be populated.  So we cannot rely on the
+ 	 * size of the clean queue.
+ 	 */
+ 	unsigned nr_clean;
+ 
+ 	if (idle) {
++>>>>>>> 97dfb20309e0 (dm cache policy smq: cleanup free_target_met() and clean_target_met())
  		/*
 -		 * We'd like to clean everything.
 +		 * We could get a block from mq->dirty, but that
 +		 * would add extra latency to the triggering bio as it
 +		 * waits for the writeback.  Better to not promote this
 +		 * time and hope there's a clean block next time this block
 +		 * is hit.
  		 */
++<<<<<<< HEAD
 +		return -ENOSPC;
 +
 +	if (locker->fn(locker, demoted->oblock))
++=======
+ 		return q_size(&mq->dirty) == 0u;
+ 	}
+ 
+ 	nr_clean = from_cblock(mq->cache_size) - q_size(&mq->dirty);
+ 	return (nr_clean + btracker_nr_writebacks_queued(mq->bg_work)) >=
+ 		percent_to_target(mq, CLEAN_TARGET);
+ }
+ 
+ static bool free_target_met(struct smq_policy *mq, bool idle)
+ {
+ 	unsigned nr_free;
+ 
+ 	if (!idle)
+ 		return true;
+ 
+ 	nr_free = from_cblock(mq->cache_size) - mq->cache_alloc.nr_allocated;
+ 	return (nr_free + btracker_nr_demotions_queued(mq->bg_work)) >=
+ 		percent_to_target(mq, FREE_TARGET);
+ }
+ 
+ /*----------------------------------------------------------------*/
+ 
+ static void mark_pending(struct smq_policy *mq, struct entry *e)
+ {
+ 	BUG_ON(e->sentinel);
+ 	BUG_ON(!e->allocated);
+ 	BUG_ON(e->pending_work);
+ 	e->pending_work = true;
+ }
+ 
+ static void clear_pending(struct smq_policy *mq, struct entry *e)
+ {
+ 	BUG_ON(!e->pending_work);
+ 	e->pending_work = false;
+ }
+ 
+ static void queue_writeback(struct smq_policy *mq)
+ {
+ 	int r;
+ 	struct policy_work work;
+ 	struct entry *e;
+ 
+ 	e = q_peek(&mq->dirty, mq->dirty.nr_levels, !mq->migrations_allowed);
+ 	if (e) {
+ 		mark_pending(mq, e);
+ 		q_del(&mq->dirty, e);
+ 
+ 		work.op = POLICY_WRITEBACK;
+ 		work.oblock = e->oblock;
+ 		work.cblock = infer_cblock(mq, e);
+ 
+ 		r = btracker_queue(mq->bg_work, &work, NULL);
+ 		WARN_ON_ONCE(r); // FIXME: finish, I think we have to get rid of this race.
+ 	}
+ }
+ 
+ static void queue_demotion(struct smq_policy *mq)
+ {
+ 	struct policy_work work;
+ 	struct entry *e;
+ 
+ 	if (unlikely(WARN_ON_ONCE(!mq->migrations_allowed)))
+ 		return;
+ 
+ 	e = q_peek(&mq->clean, mq->clean.nr_levels, true);
+ 	if (!e) {
+ 		if (!clean_target_met(mq, false))
+ 			queue_writeback(mq);
+ 		return;
+ 	}
+ 
+ 	mark_pending(mq, e);
+ 	q_del(&mq->clean, e);
+ 
+ 	work.op = POLICY_DEMOTE;
+ 	work.oblock = e->oblock;
+ 	work.cblock = infer_cblock(mq, e);
+ 	btracker_queue(mq->bg_work, &work, NULL);
+ }
+ 
+ static void queue_promotion(struct smq_policy *mq, dm_oblock_t oblock,
+ 			    struct policy_work **workp)
+ {
+ 	struct entry *e;
+ 	struct policy_work work;
+ 
+ 	if (!mq->migrations_allowed)
+ 		return;
+ 
+ 	if (allocator_empty(&mq->cache_alloc)) {
++>>>>>>> 97dfb20309e0 (dm cache policy smq: cleanup free_target_met() and clean_target_met())
  		/*
 -		 * We always claim to be 'idle' to ensure some demotions happen
 -		 * with continuous loads.
 +		 * We couldn't lock this block.
  		 */
 -		if (!free_target_met(mq, true))
 -			queue_demotion(mq);
 -		return;
 -	}
 +		return -EBUSY;
  
 -	if (btracker_promotion_already_present(mq->bg_work, oblock))
 -		return;
 +	del(mq, demoted);
 +	*oblock = demoted->oblock;
 +	free_entry(&mq->cache_alloc, demoted);
  
 -	/*
 -	 * We allocate the entry now to reserve the cblock.  If the
 -	 * background work is aborted we must remember to free it.
 -	 */
 -	e = alloc_entry(&mq->cache_alloc);
 -	BUG_ON(!e);
 -	e->pending_work = true;
 -	work.op = POLICY_PROMOTE;
 -	work.oblock = oblock;
 -	work.cblock = infer_cblock(mq, e);
 -	btracker_queue(mq->bg_work, &work, workp);
 +	return 0;
  }
  
 -/*----------------------------------------------------------------*/
 -
  enum promote_result {
  	PROMOTE_NOT,
  	PROMOTE_TEMPORARY,
* Unmerged path drivers/md/dm-cache-policy-smq.c
