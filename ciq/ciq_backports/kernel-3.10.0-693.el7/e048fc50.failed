net/mlx5e: Do not recycle pages from emergency reserve

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Do not recycle pages from emergency reserve (Don Dutile) [1385330 1417286]
Rebuild_FUZZ: 96.15%
commit-author Eric Dumazet <edumazet@google.com>
commit e048fc50d7bde23136e098e04a324d7e3404408d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e048fc50.failed

A driver using dev_alloc_page() must not reuse a page allocated from
emergency memory reserve.

Otherwise all packets using this page will be immediately dropped,
unless for very specific sockets having SOCK_MEMALLOC bit set.

This issue might be hard to debug, because only a fraction of received
packets would be dropped.

Fixes: 4415a0319f92 ("net/mlx5e: Implement RX mapped page cache for page recycle")
	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Tariq Toukan <tariqt@mellanox.com>
	Cc: Saeed Mahameed <saeedm@mellanox.com>
	Acked-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e048fc50d7bde23136e098e04a324d7e3404408d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index d795e95774bc,06d5e6fecb0a..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -179,37 -180,95 +179,120 @@@ unlock
  	mutex_unlock(&priv->state_lock);
  }
  
++<<<<<<< HEAD
++=======
+ #define RQ_PAGE_SIZE(rq) ((1 << rq->buff.page_order) << PAGE_SHIFT)
+ 
+ static inline bool mlx5e_rx_cache_put(struct mlx5e_rq *rq,
+ 				      struct mlx5e_dma_info *dma_info)
+ {
+ 	struct mlx5e_page_cache *cache = &rq->page_cache;
+ 	u32 tail_next = (cache->tail + 1) & (MLX5E_CACHE_SIZE - 1);
+ 
+ 	if (tail_next == cache->head) {
+ 		rq->stats.cache_full++;
+ 		return false;
+ 	}
+ 
+ 	if (unlikely(page_is_pfmemalloc(dma_info->page)))
+ 		return false;
+ 
+ 	cache->page_cache[cache->tail] = *dma_info;
+ 	cache->tail = tail_next;
+ 	return true;
+ }
+ 
+ static inline bool mlx5e_rx_cache_get(struct mlx5e_rq *rq,
+ 				      struct mlx5e_dma_info *dma_info)
+ {
+ 	struct mlx5e_page_cache *cache = &rq->page_cache;
+ 
+ 	if (unlikely(cache->head == cache->tail)) {
+ 		rq->stats.cache_empty++;
+ 		return false;
+ 	}
+ 
+ 	if (page_ref_count(cache->page_cache[cache->head].page) != 1) {
+ 		rq->stats.cache_busy++;
+ 		return false;
+ 	}
+ 
+ 	*dma_info = cache->page_cache[cache->head];
+ 	cache->head = (cache->head + 1) & (MLX5E_CACHE_SIZE - 1);
+ 	rq->stats.cache_reuse++;
+ 
+ 	dma_sync_single_for_device(rq->pdev, dma_info->addr,
+ 				   RQ_PAGE_SIZE(rq),
+ 				   DMA_FROM_DEVICE);
+ 	return true;
+ }
+ 
+ static inline int mlx5e_page_alloc_mapped(struct mlx5e_rq *rq,
+ 					  struct mlx5e_dma_info *dma_info)
+ {
+ 	struct page *page;
+ 
+ 	if (mlx5e_rx_cache_get(rq, dma_info))
+ 		return 0;
+ 
+ 	page = dev_alloc_pages(rq->buff.page_order);
+ 	if (unlikely(!page))
+ 		return -ENOMEM;
+ 
+ 	dma_info->page = page;
+ 	dma_info->addr = dma_map_page(rq->pdev, page, 0,
+ 				      RQ_PAGE_SIZE(rq), rq->buff.map_dir);
+ 	if (unlikely(dma_mapping_error(rq->pdev, dma_info->addr))) {
+ 		put_page(page);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
+ 			bool recycle)
+ {
+ 	if (likely(recycle) && mlx5e_rx_cache_put(rq, dma_info))
+ 		return;
+ 
+ 	dma_unmap_page(rq->pdev, dma_info->addr, RQ_PAGE_SIZE(rq),
+ 		       rq->buff.map_dir);
+ 	put_page(dma_info->page);
+ }
+ 
++>>>>>>> e048fc50d7bd (net/mlx5e: Do not recycle pages from emergency reserve)
  int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix)
  {
 -	struct mlx5e_dma_info *di = &rq->dma_info[ix];
 +	struct sk_buff *skb;
 +	dma_addr_t dma_addr;
  
 -	if (unlikely(mlx5e_page_alloc_mapped(rq, di)))
 +	skb = napi_alloc_skb(rq->cq.napi, rq->wqe_sz);
 +	if (unlikely(!skb))
  		return -ENOMEM;
  
 -	wqe->data.addr = cpu_to_be64(di->addr + MLX5_RX_HEADROOM);
 +	dma_addr = dma_map_single(rq->pdev,
 +				  /* hw start padding */
 +				  skb->data,
 +				  /* hw end padding */
 +				  rq->wqe_sz,
 +				  DMA_FROM_DEVICE);
 +
 +	if (unlikely(dma_mapping_error(rq->pdev, dma_addr)))
 +		goto err_free_skb;
 +
 +	*((dma_addr_t *)skb->cb) = dma_addr;
 +	wqe->data.addr = cpu_to_be64(dma_addr);
 +	wqe->data.lkey = rq->mkey_be;
 +
 +	rq->skb[ix] = skb;
 +
  	return 0;
 +
 +err_free_skb:
 +	dev_kfree_skb(skb);
 +
 +	return -ENOMEM;
  }
  
  void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
