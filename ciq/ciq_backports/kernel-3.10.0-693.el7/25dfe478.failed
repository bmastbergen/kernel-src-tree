x86/mm/64: Enable KASLR for vmemmap memory region

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] mm/64: Enable KASLR for vmemmap memory region (Baoquan He) [1424943]
Rebuild_FUZZ: 95.74%
commit-author Thomas Garnier <thgarnie@google.com>
commit 25dfe4785332723f09311dcb7fd91015a60c022f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/25dfe478.failed

Add vmemmap in the list of randomized memory regions.

The vmemmap region holds a representation of the physical memory (through
a struct page array). An attacker could use this region to disclose the
kernel memory layout (walking the page linked list).

	Signed-off-by: Thomas Garnier <thgarnie@google.com>
	Signed-off-by: Kees Cook <keescook@chromium.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: kernel-hardening@lists.openwall.com
Link: http://lkml.kernel.org/r/1469635196-122447-1-git-send-email-thgarnie@google.com
[ Minor edits. ]
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 25dfe4785332723f09311dcb7fd91015a60c022f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kaslr.h
#	arch/x86/include/asm/pgtable_64_types.h
#	arch/x86/mm/kaslr.c
diff --cc arch/x86/include/asm/pgtable_64_types.h
index 2d883440cb9a,3a264200c62f..000000000000
--- a/arch/x86/include/asm/pgtable_64_types.h
+++ b/arch/x86/include/asm/pgtable_64_types.h
@@@ -54,13 -54,25 +54,29 @@@ typedef struct { pteval_t pte; } pte_t
  #define PGDIR_MASK	(~(PGDIR_SIZE - 1))
  
  /* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
++<<<<<<< HEAD
 +#define MAXMEM		 _AC(__AC(1, UL) << MAX_PHYSMEM_BITS, UL)
 +#define VMALLOC_START    _AC(0xffffc90000000000, UL)
 +#define VMALLOC_END      _AC(0xffffe8ffffffffff, UL)
 +#define VMEMMAP_START	 _AC(0xffffea0000000000, UL)
 +#define MODULES_VADDR    _AC(0xffffffffa0000000, UL)
++=======
+ #define MAXMEM		_AC(__AC(1, UL) << MAX_PHYSMEM_BITS, UL)
+ #define VMALLOC_SIZE_TB	_AC(32, UL)
+ #define __VMALLOC_BASE	_AC(0xffffc90000000000, UL)
+ #define __VMEMMAP_BASE	_AC(0xffffea0000000000, UL)
+ #ifdef CONFIG_RANDOMIZE_MEMORY
+ #define VMALLOC_START	vmalloc_base
+ #define VMEMMAP_START	vmemmap_base
+ #else
+ #define VMALLOC_START	__VMALLOC_BASE
+ #define VMEMMAP_START	__VMEMMAP_BASE
+ #endif /* CONFIG_RANDOMIZE_MEMORY */
+ #define VMALLOC_END	(VMALLOC_START + _AC((VMALLOC_SIZE_TB << 40) - 1, UL))
+ #define MODULES_VADDR    (__START_KERNEL_map + KERNEL_IMAGE_SIZE)
++>>>>>>> 25dfe4785332 (x86/mm/64: Enable KASLR for vmemmap memory region)
  #define MODULES_END      _AC(0xffffffffff000000, UL)
  #define MODULES_LEN   (MODULES_END - MODULES_VADDR)
 -#define ESPFIX_PGD_ENTRY _AC(-2, UL)
 -#define ESPFIX_BASE_ADDR (ESPFIX_PGD_ENTRY << PGDIR_SHIFT)
 -#define EFI_VA_START	 ( -4 * (_AC(1, UL) << 30))
 -#define EFI_VA_END	 (-68 * (_AC(1, UL) << 30))
  
  #define EARLY_DYNAMIC_PAGE_TABLES	64
  
* Unmerged path arch/x86/include/asm/kaslr.h
* Unmerged path arch/x86/mm/kaslr.c
* Unmerged path arch/x86/include/asm/kaslr.h
* Unmerged path arch/x86/include/asm/pgtable_64_types.h
* Unmerged path arch/x86/mm/kaslr.c
