IB/mlx5: Properly adjust rate limit on QP state transitions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Bodong Wang <bodong@mellanox.com>
commit 7d29f349a4b9dcf5bc9dcc05630d6a7f6b6b3ccd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/7d29f349.failed

- Add MODIFY_QP_EX CMD to extend modify_qp.
- Rate limit will be updated in the following state transactions: RTR2RTS,
  RTS2RTS. The limit will be removed when SQ is in RST and ERR state.

	Signed-off-by: Bodong Wang <bodong@mellanox.com>
	Reviewed-by: Matan Barak <matanb@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 7d29f349a4b9dcf5bc9dcc05630d6a7f6b6b3ccd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index e914f498f7f2,ab8961cc8bca..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -384,6 -386,10 +384,13 @@@ struct mlx5_ib_qp 
  	spinlock_t              disable_page_faults_lock;
  	struct mlx5_ib_pfault	pagefaults[MLX5_IB_PAGEFAULT_CONTEXTS];
  #endif
++<<<<<<< HEAD
++=======
+ 	struct list_head	qps_list;
+ 	struct list_head	cq_recv_list;
+ 	struct list_head	cq_send_list;
+ 	u32			rate_limit;
++>>>>>>> 7d29f349a4b9 (IB/mlx5: Properly adjust rate limit on QP state transitions)
  };
  
  struct mlx5_ib_cq_buf {
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 93fe24d402fb,a69524fb6032..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -76,6 -76,23 +76,26 @@@ struct mlx5_wqe_eth_pad 
  	u8 rsvd0[16];
  };
  
++<<<<<<< HEAD
++=======
+ enum raw_qp_set_mask_map {
+ 	MLX5_RAW_QP_MOD_SET_RQ_Q_CTR_ID		= 1UL << 0,
+ 	MLX5_RAW_QP_RATE_LIMIT			= 1UL << 1,
+ };
+ 
+ struct mlx5_modify_raw_qp_param {
+ 	u16 operation;
+ 
+ 	u32 set_mask; /* raw_qp_set_mask_map */
+ 	u32 rate_limit;
+ 	u8 rq_q_ctr_id;
+ };
+ 
+ static void get_cqs(enum ib_qp_type qp_type,
+ 		    struct ib_cq *ib_send_cq, struct ib_cq *ib_recv_cq,
+ 		    struct mlx5_ib_cq **send_cq, struct mlx5_ib_cq **recv_cq);
+ 
++>>>>>>> 7d29f349a4b9 (IB/mlx5: Properly adjust rate limit on QP state transitions)
  static int is_qp0(enum ib_qp_type qp_type)
  {
  	return qp_type == IB_QPT_SMI;
@@@ -2427,25 -2566,43 +2489,58 @@@ static int modify_raw_packet_qp(struct 
  		rq_state = MLX5_RQC_STATE_RST;
  		sq_state = MLX5_SQC_STATE_RST;
  		break;
- 	case MLX5_CMD_OP_INIT2INIT_QP:
- 	case MLX5_CMD_OP_INIT2RTR_QP:
  	case MLX5_CMD_OP_RTR2RTS_QP:
  	case MLX5_CMD_OP_RTS2RTS_QP:
++<<<<<<< HEAD
 +		/* Nothing to do here... */
 +		return 0;
++=======
+ 		if (raw_qp_param->set_mask ==
+ 		    MLX5_RAW_QP_RATE_LIMIT) {
+ 			modify_rq = 0;
+ 			sq_state = sq->state;
+ 		} else {
+ 			return raw_qp_param->set_mask ? -EINVAL : 0;
+ 		}
+ 		break;
+ 	case MLX5_CMD_OP_INIT2INIT_QP:
+ 	case MLX5_CMD_OP_INIT2RTR_QP:
+ 		if (raw_qp_param->set_mask)
+ 			return -EINVAL;
+ 		else
+ 			return 0;
++>>>>>>> 7d29f349a4b9 (IB/mlx5: Properly adjust rate limit on QP state transitions)
  	default:
  		WARN_ON(1);
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
 +	if (qp->rq.wqe_cnt) {
 +		err =  modify_raw_packet_qp_rq(dev->mdev, rq, rq_state);
++=======
+ 	if (modify_rq) {
+ 		err =  modify_raw_packet_qp_rq(dev, rq, rq_state, raw_qp_param);
++>>>>>>> 7d29f349a4b9 (IB/mlx5: Properly adjust rate limit on QP state transitions)
  		if (err)
  			return err;
  	}
  
++<<<<<<< HEAD
 +	if (qp->sq.wqe_cnt)
 +		return modify_raw_packet_qp_sq(dev->mdev, sq, sq_state);
++=======
+ 	if (modify_sq) {
+ 		if (tx_affinity) {
+ 			err = modify_raw_packet_tx_affinity(dev->mdev, sq,
+ 							    tx_affinity);
+ 			if (err)
+ 				return err;
+ 		}
+ 
+ 		return modify_raw_packet_qp_sq(dev->mdev, sq, sq_state, raw_qp_param);
+ 	}
++>>>>>>> 7d29f349a4b9 (IB/mlx5: Properly adjust rate limit on QP state transitions)
  
  	return 0;
  }
@@@ -2673,13 -2847,27 +2768,33 @@@ static int __mlx5_ib_modify_qp(struct i
  	op = optab[mlx5_cur][mlx5_new];
  	optpar = ib_mask_to_mlx5_opt(attr_mask);
  	optpar &= opt_mask[mlx5_cur][mlx5_new][mlx5_st];
 +	in->optparam = cpu_to_be32(optpar);
  
++<<<<<<< HEAD
 +	if (qp->ibqp.qp_type == IB_QPT_RAW_PACKET)
 +		err = modify_raw_packet_qp(dev, qp, op);
 +	else
 +		err = mlx5_core_qp_modify(dev->mdev, op, in, sqd_event,
++=======
+ 	if (qp->ibqp.qp_type == IB_QPT_RAW_PACKET) {
+ 		struct mlx5_modify_raw_qp_param raw_qp_param = {};
+ 
+ 		raw_qp_param.operation = op;
+ 		if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT) {
+ 			raw_qp_param.rq_q_ctr_id = mibport->q_cnt_id;
+ 			raw_qp_param.set_mask |= MLX5_RAW_QP_MOD_SET_RQ_Q_CTR_ID;
+ 		}
+ 
+ 		if (attr_mask & IB_QP_RATE_LIMIT) {
+ 			raw_qp_param.rate_limit = attr->rate_limit;
+ 			raw_qp_param.set_mask |= MLX5_RAW_QP_RATE_LIMIT;
+ 		}
+ 
+ 		err = modify_raw_packet_qp(dev, qp, &raw_qp_param, tx_affinity);
+ 	} else {
+ 		err = mlx5_core_qp_modify(dev->mdev, op, optpar, context,
++>>>>>>> 7d29f349a4b9 (IB/mlx5: Properly adjust rate limit on QP state transitions)
  					  &base->mqp);
 -	}
 -
  	if (err)
  		goto out;
  
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index 7e1c9a7b92f0..03c1941272a0 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -2861,7 +2861,8 @@ static void *mlx5_ib_add(struct mlx5_core_dev *mdev)
 	dev->ib_dev.uverbs_ex_cmd_mask =
 		(1ull << IB_USER_VERBS_EX_CMD_QUERY_DEVICE)	|
 		(1ull << IB_USER_VERBS_EX_CMD_CREATE_CQ)	|
-		(1ull << IB_USER_VERBS_EX_CMD_CREATE_QP);
+		(1ull << IB_USER_VERBS_EX_CMD_CREATE_QP)	|
+		(1ull << IB_USER_VERBS_EX_CMD_MODIFY_QP);
 
 	dev->ib_dev.query_device	= mlx5_ib_query_device;
 	dev->ib_dev.query_port		= mlx5_ib_query_port;
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
