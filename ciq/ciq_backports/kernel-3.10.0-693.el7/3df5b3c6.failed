net: Add net-device param to the get offloaded stats ndo

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [net] Add net-device param to the get offloaded stats ndo (Ivan Vecera) [1382040]
Rebuild_FUZZ: 95.33%
commit-author Or Gerlitz <ogerlitz@mellanox.com>
commit 3df5b3c67546fb05266766b6abaf71563f82efe4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/3df5b3c6.failed

Some drivers would need to check few internal matters for
that. To be used in downstream mlx5 commit.

	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3df5b3c67546fb05266766b6abaf71563f82efe4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlxsw/spectrum.c
#	include/linux/netdevice.h
#	net/core/rtnetlink.c
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum.c
index 6f9e3ddff4a8,e0d7d5adbaee..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum.c
@@@ -561,6 -854,107 +561,110 @@@ mlxsw_sp_port_get_stats64(struct net_de
  		tx_dropped	+= p->tx_dropped;
  	}
  	stats->tx_dropped	= tx_dropped;
++<<<<<<< HEAD
++=======
+ 	return 0;
+ }
+ 
+ static bool mlxsw_sp_port_has_offload_stats(const struct net_device *dev, int attr_id)
+ {
+ 	switch (attr_id) {
+ 	case IFLA_OFFLOAD_XSTATS_CPU_HIT:
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static int mlxsw_sp_port_get_offload_stats(int attr_id, const struct net_device *dev,
+ 					   void *sp)
+ {
+ 	switch (attr_id) {
+ 	case IFLA_OFFLOAD_XSTATS_CPU_HIT:
+ 		return mlxsw_sp_port_get_sw_stats64(dev, sp);
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ static int mlxsw_sp_port_get_stats_raw(struct net_device *dev, int grp,
+ 				       int prio, char *ppcnt_pl)
+ {
+ 	struct mlxsw_sp_port *mlxsw_sp_port = netdev_priv(dev);
+ 	struct mlxsw_sp *mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
+ 
+ 	mlxsw_reg_ppcnt_pack(ppcnt_pl, mlxsw_sp_port->local_port, grp, prio);
+ 	return mlxsw_reg_query(mlxsw_sp->core, MLXSW_REG(ppcnt), ppcnt_pl);
+ }
+ 
+ static int mlxsw_sp_port_get_hw_stats(struct net_device *dev,
+ 				      struct rtnl_link_stats64 *stats)
+ {
+ 	char ppcnt_pl[MLXSW_REG_PPCNT_LEN];
+ 	int err;
+ 
+ 	err = mlxsw_sp_port_get_stats_raw(dev, MLXSW_REG_PPCNT_IEEE_8023_CNT,
+ 					  0, ppcnt_pl);
+ 	if (err)
+ 		goto out;
+ 
+ 	stats->tx_packets =
+ 		mlxsw_reg_ppcnt_a_frames_transmitted_ok_get(ppcnt_pl);
+ 	stats->rx_packets =
+ 		mlxsw_reg_ppcnt_a_frames_received_ok_get(ppcnt_pl);
+ 	stats->tx_bytes =
+ 		mlxsw_reg_ppcnt_a_octets_transmitted_ok_get(ppcnt_pl);
+ 	stats->rx_bytes =
+ 		mlxsw_reg_ppcnt_a_octets_received_ok_get(ppcnt_pl);
+ 	stats->multicast =
+ 		mlxsw_reg_ppcnt_a_multicast_frames_received_ok_get(ppcnt_pl);
+ 
+ 	stats->rx_crc_errors =
+ 		mlxsw_reg_ppcnt_a_frame_check_sequence_errors_get(ppcnt_pl);
+ 	stats->rx_frame_errors =
+ 		mlxsw_reg_ppcnt_a_alignment_errors_get(ppcnt_pl);
+ 
+ 	stats->rx_length_errors = (
+ 		mlxsw_reg_ppcnt_a_in_range_length_errors_get(ppcnt_pl) +
+ 		mlxsw_reg_ppcnt_a_out_of_range_length_field_get(ppcnt_pl) +
+ 		mlxsw_reg_ppcnt_a_frame_too_long_errors_get(ppcnt_pl));
+ 
+ 	stats->rx_errors = (stats->rx_crc_errors +
+ 		stats->rx_frame_errors + stats->rx_length_errors);
+ 
+ out:
+ 	return err;
+ }
+ 
+ static void update_stats_cache(struct work_struct *work)
+ {
+ 	struct mlxsw_sp_port *mlxsw_sp_port =
+ 		container_of(work, struct mlxsw_sp_port,
+ 			     hw_stats.update_dw.work);
+ 
+ 	if (!netif_carrier_ok(mlxsw_sp_port->dev))
+ 		goto out;
+ 
+ 	mlxsw_sp_port_get_hw_stats(mlxsw_sp_port->dev,
+ 				   mlxsw_sp_port->hw_stats.cache);
+ 
+ out:
+ 	mlxsw_core_schedule_dw(&mlxsw_sp_port->hw_stats.update_dw,
+ 			       MLXSW_HW_STATS_UPDATE_TIME);
+ }
+ 
+ /* Return the stats from a cache that is updated periodically,
+  * as this function might get called in an atomic context.
+  */
+ static struct rtnl_link_stats64 *
+ mlxsw_sp_port_get_stats64(struct net_device *dev,
+ 			  struct rtnl_link_stats64 *stats)
+ {
+ 	struct mlxsw_sp_port *mlxsw_sp_port = netdev_priv(dev);
+ 
+ 	memcpy(stats, mlxsw_sp_port->hw_stats.cache, sizeof(*stats));
+ 
++>>>>>>> 3df5b3c67546 (net: Add net-device param to the get offloaded stats ndo)
  	return stats;
  }
  
diff --cc include/linux/netdevice.h
index 2f7ec2ad350a,ae32a27523f9..000000000000
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@@ -951,12 -925,20 +951,25 @@@ struct net_device_ops_extended 
   *	3. Update dev->stats asynchronously and atomically, and define
   *	   neither operation.
   *
++<<<<<<< HEAD
 + * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16t vid);
 + *	If device support VLAN filtering this function is called when a
++=======
+  * bool (*ndo_has_offload_stats)(const struct net_device *dev, int attr_id)
+  *	Return true if this device supports offload stats of this attr_id.
+  *
+  * int (*ndo_get_offload_stats)(int attr_id, const struct net_device *dev,
+  *	void *attr_data)
+  *	Get statistics for offload operations by attr_id. Write it into the
+  *	attr_data pointer.
+  *
+  * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);
+  *	If device supports VLAN filtering this function is called when a
++>>>>>>> 3df5b3c67546 (net: Add net-device param to the get offloaded stats ndo)
   *	VLAN id is registered.
   *
 - * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);
 - *	If device supports VLAN filtering this function is called when a
 + * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, unsigned short vid);
 + *	If device support VLAN filtering this function is called when a
   *	VLAN id is unregistered.
   *
   * void (*ndo_poll_controller)(struct net_device *dev);
@@@ -1159,6 -1165,10 +1172,13 @@@ struct net_device_ops 
  
  	struct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,
  						     struct rtnl_link_stats64 *storage);
++<<<<<<< HEAD
++=======
+ 	bool			(*ndo_has_offload_stats)(const struct net_device *dev, int attr_id);
+ 	int			(*ndo_get_offload_stats)(int attr_id,
+ 							 const struct net_device *dev,
+ 							 void *attr_data);
++>>>>>>> 3df5b3c67546 (net: Add net-device param to the get offloaded stats ndo)
  	struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);
  
  	int			(*ndo_vlan_rx_add_vid)(struct net_device *dev,
diff --cc net/core/rtnetlink.c
index 1adee1ef78d1,ef8a96010816..000000000000
--- a/net/core/rtnetlink.c
+++ b/net/core/rtnetlink.c
@@@ -3218,6 -3633,347 +3218,350 @@@ out
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static bool stats_attr_valid(unsigned int mask, int attrid, int idxattr)
+ {
+ 	return (mask & IFLA_STATS_FILTER_BIT(attrid)) &&
+ 	       (!idxattr || idxattr == attrid);
+ }
+ 
+ #define IFLA_OFFLOAD_XSTATS_FIRST (IFLA_OFFLOAD_XSTATS_UNSPEC + 1)
+ static int rtnl_get_offload_stats_attr_size(int attr_id)
+ {
+ 	switch (attr_id) {
+ 	case IFLA_OFFLOAD_XSTATS_CPU_HIT:
+ 		return sizeof(struct rtnl_link_stats64);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int rtnl_get_offload_stats(struct sk_buff *skb, struct net_device *dev,
+ 				  int *prividx)
+ {
+ 	struct nlattr *attr = NULL;
+ 	int attr_id, size;
+ 	void *attr_data;
+ 	int err;
+ 
+ 	if (!(dev->netdev_ops && dev->netdev_ops->ndo_has_offload_stats &&
+ 	      dev->netdev_ops->ndo_get_offload_stats))
+ 		return -ENODATA;
+ 
+ 	for (attr_id = IFLA_OFFLOAD_XSTATS_FIRST;
+ 	     attr_id <= IFLA_OFFLOAD_XSTATS_MAX; attr_id++) {
+ 		if (attr_id < *prividx)
+ 			continue;
+ 
+ 		size = rtnl_get_offload_stats_attr_size(attr_id);
+ 		if (!size)
+ 			continue;
+ 
+ 		if (!dev->netdev_ops->ndo_has_offload_stats(dev, attr_id))
+ 			continue;
+ 
+ 		attr = nla_reserve_64bit(skb, attr_id, size,
+ 					 IFLA_OFFLOAD_XSTATS_UNSPEC);
+ 		if (!attr)
+ 			goto nla_put_failure;
+ 
+ 		attr_data = nla_data(attr);
+ 		memset(attr_data, 0, size);
+ 		err = dev->netdev_ops->ndo_get_offload_stats(attr_id, dev,
+ 							     attr_data);
+ 		if (err)
+ 			goto get_offload_stats_failure;
+ 	}
+ 
+ 	if (!attr)
+ 		return -ENODATA;
+ 
+ 	*prividx = 0;
+ 	return 0;
+ 
+ nla_put_failure:
+ 	err = -EMSGSIZE;
+ get_offload_stats_failure:
+ 	*prividx = attr_id;
+ 	return err;
+ }
+ 
+ static int rtnl_get_offload_stats_size(const struct net_device *dev)
+ {
+ 	int nla_size = 0;
+ 	int attr_id;
+ 	int size;
+ 
+ 	if (!(dev->netdev_ops && dev->netdev_ops->ndo_has_offload_stats &&
+ 	      dev->netdev_ops->ndo_get_offload_stats))
+ 		return 0;
+ 
+ 	for (attr_id = IFLA_OFFLOAD_XSTATS_FIRST;
+ 	     attr_id <= IFLA_OFFLOAD_XSTATS_MAX; attr_id++) {
+ 		if (!dev->netdev_ops->ndo_has_offload_stats(dev, attr_id))
+ 			continue;
+ 		size = rtnl_get_offload_stats_attr_size(attr_id);
+ 		nla_size += nla_total_size_64bit(size);
+ 	}
+ 
+ 	if (nla_size != 0)
+ 		nla_size += nla_total_size(0);
+ 
+ 	return nla_size;
+ }
+ 
+ static int rtnl_fill_statsinfo(struct sk_buff *skb, struct net_device *dev,
+ 			       int type, u32 pid, u32 seq, u32 change,
+ 			       unsigned int flags, unsigned int filter_mask,
+ 			       int *idxattr, int *prividx)
+ {
+ 	struct if_stats_msg *ifsm;
+ 	struct nlmsghdr *nlh;
+ 	struct nlattr *attr;
+ 	int s_prividx = *prividx;
+ 	int err;
+ 
+ 	ASSERT_RTNL();
+ 
+ 	nlh = nlmsg_put(skb, pid, seq, type, sizeof(*ifsm), flags);
+ 	if (!nlh)
+ 		return -EMSGSIZE;
+ 
+ 	ifsm = nlmsg_data(nlh);
+ 	ifsm->ifindex = dev->ifindex;
+ 	ifsm->filter_mask = filter_mask;
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_64, *idxattr)) {
+ 		struct rtnl_link_stats64 *sp;
+ 
+ 		attr = nla_reserve_64bit(skb, IFLA_STATS_LINK_64,
+ 					 sizeof(struct rtnl_link_stats64),
+ 					 IFLA_STATS_UNSPEC);
+ 		if (!attr)
+ 			goto nla_put_failure;
+ 
+ 		sp = nla_data(attr);
+ 		dev_get_stats(dev, sp);
+ 	}
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_XSTATS, *idxattr)) {
+ 		const struct rtnl_link_ops *ops = dev->rtnl_link_ops;
+ 
+ 		if (ops && ops->fill_linkxstats) {
+ 			*idxattr = IFLA_STATS_LINK_XSTATS;
+ 			attr = nla_nest_start(skb,
+ 					      IFLA_STATS_LINK_XSTATS);
+ 			if (!attr)
+ 				goto nla_put_failure;
+ 
+ 			err = ops->fill_linkxstats(skb, dev, prividx, *idxattr);
+ 			nla_nest_end(skb, attr);
+ 			if (err)
+ 				goto nla_put_failure;
+ 			*idxattr = 0;
+ 		}
+ 	}
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_XSTATS_SLAVE,
+ 			     *idxattr)) {
+ 		const struct rtnl_link_ops *ops = NULL;
+ 		const struct net_device *master;
+ 
+ 		master = netdev_master_upper_dev_get(dev);
+ 		if (master)
+ 			ops = master->rtnl_link_ops;
+ 		if (ops && ops->fill_linkxstats) {
+ 			*idxattr = IFLA_STATS_LINK_XSTATS_SLAVE;
+ 			attr = nla_nest_start(skb,
+ 					      IFLA_STATS_LINK_XSTATS_SLAVE);
+ 			if (!attr)
+ 				goto nla_put_failure;
+ 
+ 			err = ops->fill_linkxstats(skb, dev, prividx, *idxattr);
+ 			nla_nest_end(skb, attr);
+ 			if (err)
+ 				goto nla_put_failure;
+ 			*idxattr = 0;
+ 		}
+ 	}
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_OFFLOAD_XSTATS,
+ 			     *idxattr)) {
+ 		*idxattr = IFLA_STATS_LINK_OFFLOAD_XSTATS;
+ 		attr = nla_nest_start(skb, IFLA_STATS_LINK_OFFLOAD_XSTATS);
+ 		if (!attr)
+ 			goto nla_put_failure;
+ 
+ 		err = rtnl_get_offload_stats(skb, dev, prividx);
+ 		if (err == -ENODATA)
+ 			nla_nest_cancel(skb, attr);
+ 		else
+ 			nla_nest_end(skb, attr);
+ 
+ 		if (err && err != -ENODATA)
+ 			goto nla_put_failure;
+ 		*idxattr = 0;
+ 	}
+ 
+ 	nlmsg_end(skb, nlh);
+ 
+ 	return 0;
+ 
+ nla_put_failure:
+ 	/* not a multi message or no progress mean a real error */
+ 	if (!(flags & NLM_F_MULTI) || s_prividx == *prividx)
+ 		nlmsg_cancel(skb, nlh);
+ 	else
+ 		nlmsg_end(skb, nlh);
+ 
+ 	return -EMSGSIZE;
+ }
+ 
+ static size_t if_nlmsg_stats_size(const struct net_device *dev,
+ 				  u32 filter_mask)
+ {
+ 	size_t size = 0;
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_64, 0))
+ 		size += nla_total_size_64bit(sizeof(struct rtnl_link_stats64));
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_XSTATS, 0)) {
+ 		const struct rtnl_link_ops *ops = dev->rtnl_link_ops;
+ 		int attr = IFLA_STATS_LINK_XSTATS;
+ 
+ 		if (ops && ops->get_linkxstats_size) {
+ 			size += nla_total_size(ops->get_linkxstats_size(dev,
+ 									attr));
+ 			/* for IFLA_STATS_LINK_XSTATS */
+ 			size += nla_total_size(0);
+ 		}
+ 	}
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_XSTATS_SLAVE, 0)) {
+ 		struct net_device *_dev = (struct net_device *)dev;
+ 		const struct rtnl_link_ops *ops = NULL;
+ 		const struct net_device *master;
+ 
+ 		/* netdev_master_upper_dev_get can't take const */
+ 		master = netdev_master_upper_dev_get(_dev);
+ 		if (master)
+ 			ops = master->rtnl_link_ops;
+ 		if (ops && ops->get_linkxstats_size) {
+ 			int attr = IFLA_STATS_LINK_XSTATS_SLAVE;
+ 
+ 			size += nla_total_size(ops->get_linkxstats_size(dev,
+ 									attr));
+ 			/* for IFLA_STATS_LINK_XSTATS_SLAVE */
+ 			size += nla_total_size(0);
+ 		}
+ 	}
+ 
+ 	if (stats_attr_valid(filter_mask, IFLA_STATS_LINK_OFFLOAD_XSTATS, 0))
+ 		size += rtnl_get_offload_stats_size(dev);
+ 
+ 	return size;
+ }
+ 
+ static int rtnl_stats_get(struct sk_buff *skb, struct nlmsghdr *nlh)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct net_device *dev = NULL;
+ 	int idxattr = 0, prividx = 0;
+ 	struct if_stats_msg *ifsm;
+ 	struct sk_buff *nskb;
+ 	u32 filter_mask;
+ 	int err;
+ 
+ 	ifsm = nlmsg_data(nlh);
+ 	if (ifsm->ifindex > 0)
+ 		dev = __dev_get_by_index(net, ifsm->ifindex);
+ 	else
+ 		return -EINVAL;
+ 
+ 	if (!dev)
+ 		return -ENODEV;
+ 
+ 	filter_mask = ifsm->filter_mask;
+ 	if (!filter_mask)
+ 		return -EINVAL;
+ 
+ 	nskb = nlmsg_new(if_nlmsg_stats_size(dev, filter_mask), GFP_KERNEL);
+ 	if (!nskb)
+ 		return -ENOBUFS;
+ 
+ 	err = rtnl_fill_statsinfo(nskb, dev, RTM_NEWSTATS,
+ 				  NETLINK_CB(skb).portid, nlh->nlmsg_seq, 0,
+ 				  0, filter_mask, &idxattr, &prividx);
+ 	if (err < 0) {
+ 		/* -EMSGSIZE implies BUG in if_nlmsg_stats_size */
+ 		WARN_ON(err == -EMSGSIZE);
+ 		kfree_skb(nskb);
+ 	} else {
+ 		err = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int rtnl_stats_dump(struct sk_buff *skb, struct netlink_callback *cb)
+ {
+ 	int h, s_h, err, s_idx, s_idxattr, s_prividx;
+ 	struct net *net = sock_net(skb->sk);
+ 	unsigned int flags = NLM_F_MULTI;
+ 	struct if_stats_msg *ifsm;
+ 	struct hlist_head *head;
+ 	struct net_device *dev;
+ 	u32 filter_mask = 0;
+ 	int idx = 0;
+ 
+ 	s_h = cb->args[0];
+ 	s_idx = cb->args[1];
+ 	s_idxattr = cb->args[2];
+ 	s_prividx = cb->args[3];
+ 
+ 	cb->seq = net->dev_base_seq;
+ 
+ 	ifsm = nlmsg_data(cb->nlh);
+ 	filter_mask = ifsm->filter_mask;
+ 	if (!filter_mask)
+ 		return -EINVAL;
+ 
+ 	for (h = s_h; h < NETDEV_HASHENTRIES; h++, s_idx = 0) {
+ 		idx = 0;
+ 		head = &net->dev_index_head[h];
+ 		hlist_for_each_entry(dev, head, index_hlist) {
+ 			if (idx < s_idx)
+ 				goto cont;
+ 			err = rtnl_fill_statsinfo(skb, dev, RTM_NEWSTATS,
+ 						  NETLINK_CB(cb->skb).portid,
+ 						  cb->nlh->nlmsg_seq, 0,
+ 						  flags, filter_mask,
+ 						  &s_idxattr, &s_prividx);
+ 			/* If we ran out of room on the first message,
+ 			 * we're in trouble
+ 			 */
+ 			WARN_ON((err == -EMSGSIZE) && (skb->len == 0));
+ 
+ 			if (err < 0)
+ 				goto out;
+ 			s_prividx = 0;
+ 			s_idxattr = 0;
+ 			nl_dump_check_consistent(cb, nlmsg_hdr(skb));
+ cont:
+ 			idx++;
+ 		}
+ 	}
+ out:
+ 	cb->args[3] = s_prividx;
+ 	cb->args[2] = s_idxattr;
+ 	cb->args[1] = idx;
+ 	cb->args[0] = h;
+ 
+ 	return skb->len;
+ }
+ 
++>>>>>>> 3df5b3c67546 (net: Add net-device param to the get offloaded stats ndo)
  /* Process one rtnetlink message. */
  
  static int rtnetlink_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum.c
* Unmerged path include/linux/netdevice.h
* Unmerged path net/core/rtnetlink.c
