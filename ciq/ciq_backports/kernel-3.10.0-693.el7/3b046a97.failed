md/raid1: Refactor raid1_make_request

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [md] raid1: Refactor raid1_make_request (Xiao Ni) [1379764]
Rebuild_FUZZ: 95.77%
commit-author Robert LeBlanc <robert@leblancnet.us>
commit 3b046a97cbd35a73e1eef968dbfb1a0aac745a77
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/3b046a97.failed

Refactor raid1_make_request to make read and write code in their own
functions to clean up the code.

	Signed-off-by: Robert LeBlanc <robert@leblancnet.us>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 3b046a97cbd35a73e1eef968dbfb1a0aac745a77)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid1.c
diff --cc drivers/md/raid1.c
index e588c32492da,14422407e520..000000000000
--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@@ -1067,17 -1071,105 +1068,114 @@@ static void raid1_read_request(struct m
  {
  	struct r1conf *conf = mddev->private;
  	struct raid1_info *mirror;
- 	struct r1bio *r1_bio;
  	struct bio *read_bio;
+ 	struct bitmap *bitmap = mddev->bitmap;
+ 	const int op = bio_op(bio);
+ 	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
+ 	int sectors_handled;
+ 	int max_sectors;
+ 	int rdisk;
+ 
+ 	wait_barrier(conf, bio);
+ 
+ read_again:
+ 	rdisk = read_balance(conf, r1_bio, &max_sectors);
+ 
+ 	if (rdisk < 0) {
+ 		/* couldn't find anywhere to read from */
+ 		raid_end_bio_io(r1_bio);
+ 		return;
+ 	}
+ 	mirror = conf->mirrors + rdisk;
+ 
+ 	if (test_bit(WriteMostly, &mirror->rdev->flags) &&
+ 	    bitmap) {
+ 		/*
+ 		 * Reading from a write-mostly device must take care not to
+ 		 * over-take any writes that are 'behind'
+ 		 */
+ 		raid1_log(mddev, "wait behind writes");
+ 		wait_event(bitmap->behind_wait,
+ 			   atomic_read(&bitmap->behind_writes) == 0);
+ 	}
+ 	r1_bio->read_disk = rdisk;
+ 	r1_bio->start_next_window = 0;
+ 
+ 	read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
+ 	bio_trim(read_bio, r1_bio->sector - bio->bi_iter.bi_sector,
+ 		 max_sectors);
+ 
+ 	r1_bio->bios[rdisk] = read_bio;
+ 
+ 	read_bio->bi_iter.bi_sector = r1_bio->sector +
+ 		mirror->rdev->data_offset;
+ 	read_bio->bi_bdev = mirror->rdev->bdev;
+ 	read_bio->bi_end_io = raid1_end_read_request;
+ 	bio_set_op_attrs(read_bio, op, do_sync);
+ 	if (test_bit(FailFast, &mirror->rdev->flags) &&
+ 	    test_bit(R1BIO_FailFast, &r1_bio->state))
+ 	        read_bio->bi_opf |= MD_FAILFAST;
+ 	read_bio->bi_private = r1_bio;
+ 
+ 	if (mddev->gendisk)
+ 	        trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
+ 	                              read_bio, disk_devt(mddev->gendisk),
+ 	                              r1_bio->sector);
+ 
+ 	if (max_sectors < r1_bio->sectors) {
+ 		/*
+ 		 * could not read all from this device, so we will need another
+ 		 * r1_bio.
+ 		 */
+ 		sectors_handled = (r1_bio->sector + max_sectors
+ 				   - bio->bi_iter.bi_sector);
+ 		r1_bio->sectors = max_sectors;
+ 		spin_lock_irq(&conf->device_lock);
+ 		if (bio->bi_phys_segments == 0)
+ 			bio->bi_phys_segments = 2;
+ 		else
+ 			bio->bi_phys_segments++;
+ 		spin_unlock_irq(&conf->device_lock);
+ 
+ 		/*
+ 		 * Cannot call generic_make_request directly as that will be
+ 		 * queued in __make_request and subsequent mempool_alloc might
+ 		 * block waiting for it.  So hand bio over to raid1d.
+ 		 */
+ 		reschedule_retry(r1_bio);
+ 
+ 		r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
+ 
+ 		r1_bio->master_bio = bio;
+ 		r1_bio->sectors = bio_sectors(bio) - sectors_handled;
+ 		r1_bio->state = 0;
+ 		r1_bio->mddev = mddev;
+ 		r1_bio->sector = bio->bi_iter.bi_sector + sectors_handled;
+ 		goto read_again;
+ 	} else
+ 		generic_make_request(read_bio);
+ }
+ 
+ static void raid1_write_request(struct mddev *mddev, struct bio *bio,
+ 				struct r1bio *r1_bio)
+ {
+ 	struct r1conf *conf = mddev->private;
  	int i, disks;
- 	struct bitmap *bitmap;
+ 	struct bitmap *bitmap = mddev->bitmap;
  	unsigned long flags;
++<<<<<<< HEAD
 +	const int rw = bio_data_dir(bio);
 +	const unsigned long do_sync = (bio->bi_rw & REQ_SYNC);
 +	const unsigned long do_flush_fua = (bio->bi_rw & (REQ_FLUSH | REQ_FUA));
 +	const unsigned long do_discard = (bio->bi_rw
 +					  & (REQ_DISCARD | REQ_SECURE));
 +	const unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);
++=======
+ 	const int op = bio_op(bio);
+ 	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
+ 	const unsigned long do_flush_fua = (bio->bi_opf &
+ 						(REQ_PREFLUSH | REQ_FUA));
++>>>>>>> 3b046a97cbd3 (md/raid1: Refactor raid1_make_request)
  	struct md_rdev *blocked_rdev;
  	struct blk_plug_cb *cb;
  	struct raid1_plug_cb *plug = NULL;
@@@ -1094,12 -1186,15 +1192,24 @@@
  
  	md_write_start(mddev, bio); /* wait on superblock update early */
  
++<<<<<<< HEAD
 +	if (bio_data_dir(bio) == WRITE &&
 +	    bio_end_sector(bio) > mddev->suspend_lo &&
 +	    bio->bi_sector < mddev->suspend_hi) {
 +		/* As the suspend_* range is controlled by
 +		 * userspace, we want an interruptible
 +		 * wait.
++=======
+ 	if ((bio_end_sector(bio) > mddev->suspend_lo &&
+ 	    bio->bi_iter.bi_sector < mddev->suspend_hi) ||
+ 	    (mddev_is_clustered(mddev) &&
+ 	     md_cluster_ops->area_resyncing(mddev, WRITE,
+ 		     bio->bi_iter.bi_sector, bio_end_sector(bio)))) {
+ 
+ 		/*
+ 		 * As the suspend_* range is controlled by userspace, we want
+ 		 * an interruptible wait.
++>>>>>>> 3b046a97cbd3 (md/raid1: Refactor raid1_make_request)
  		 */
  		DEFINE_WAIT(w);
  		for (;;) {
@@@ -1107,119 -1202,21 +1217,129 @@@
  			prepare_to_wait(&conf->wait_barrier,
  					&w, TASK_INTERRUPTIBLE);
  			if (bio_end_sector(bio) <= mddev->suspend_lo ||
++<<<<<<< HEAD
 +			    bio->bi_sector >= mddev->suspend_hi)
++=======
+ 			    bio->bi_iter.bi_sector >= mddev->suspend_hi ||
+ 			    (mddev_is_clustered(mddev) &&
+ 			     !md_cluster_ops->area_resyncing(mddev, WRITE,
+ 				     bio->bi_iter.bi_sector,
+ 				     bio_end_sector(bio))))
++>>>>>>> 3b046a97cbd3 (md/raid1: Refactor raid1_make_request)
  				break;
  			schedule();
  		}
  		finish_wait(&conf->wait_barrier, &w);
  	}
- 
  	start_next_window = wait_barrier(conf, bio);
  
++<<<<<<< HEAD
 +	bitmap = mddev->bitmap;
 +
 +	/*
 +	 * make_request() can abort the operation when READA is being
 +	 * used and no empty request is available.
 +	 *
 +	 */
 +	r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
 +
 +	r1_bio->master_bio = bio;
 +	r1_bio->sectors = bio_sectors(bio);
 +	r1_bio->state = 0;
 +	r1_bio->mddev = mddev;
 +	r1_bio->sector = bio->bi_sector;
 +
 +	/* We might need to issue multiple reads to different
 +	 * devices if there are bad blocks around, so we keep
 +	 * track of the number of reads in bio->bi_phys_segments.
 +	 * If this is 0, there is only one r1_bio and no locking
 +	 * will be needed when requests complete.  If it is
 +	 * non-zero, then it is the number of not-completed requests.
 +	 */
 +	bio->bi_phys_segments = 0;
 +	clear_bit(BIO_SEG_VALID, &bio->bi_flags);
 +
 +	if (rw == READ) {
 +		/*
 +		 * read balancing logic:
 +		 */
 +		int rdisk;
 +
 +read_again:
 +		rdisk = read_balance(conf, r1_bio, &max_sectors);
 +
 +		if (rdisk < 0) {
 +			/* couldn't find anywhere to read from */
 +			raid_end_bio_io(r1_bio);
 +			return;
 +		}
 +		mirror = conf->mirrors + rdisk;
 +
 +		if (test_bit(WriteMostly, &mirror->rdev->flags) &&
 +		    bitmap) {
 +			/* Reading from a write-mostly device must
 +			 * take care not to over-take any writes
 +			 * that are 'behind'
 +			 */
 +			wait_event(bitmap->behind_wait,
 +				   atomic_read(&bitmap->behind_writes) == 0);
 +		}
 +		r1_bio->read_disk = rdisk;
 +		r1_bio->start_next_window = 0;
 +
 +		read_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);
 +		bio_trim(read_bio, r1_bio->sector - bio->bi_sector,
 +			 max_sectors);
 +
 +		r1_bio->bios[rdisk] = read_bio;
 +
 +		read_bio->bi_sector = r1_bio->sector + mirror->rdev->data_offset;
 +		read_bio->bi_bdev = mirror->rdev->bdev;
 +		read_bio->bi_end_io = raid1_end_read_request;
 +		read_bio->bi_rw = READ | do_sync;
 +		read_bio->bi_private = r1_bio;
 +
 +		if (max_sectors < r1_bio->sectors) {
 +			/* could not read all from this device, so we will
 +			 * need another r1_bio.
 +			 */
 +
 +			sectors_handled = (r1_bio->sector + max_sectors
 +					   - bio->bi_sector);
 +			r1_bio->sectors = max_sectors;
 +			spin_lock_irq(&conf->device_lock);
 +			if (bio->bi_phys_segments == 0)
 +				bio->bi_phys_segments = 2;
 +			else
 +				bio->bi_phys_segments++;
 +			spin_unlock_irq(&conf->device_lock);
 +			/* Cannot call generic_make_request directly
 +			 * as that will be queued in __make_request
 +			 * and subsequent mempool_alloc might block waiting
 +			 * for it.  So hand bio over to raid1d.
 +			 */
 +			reschedule_retry(r1_bio);
 +
 +			r1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);
 +
 +			r1_bio->master_bio = bio;
 +			r1_bio->sectors = bio_sectors(bio) - sectors_handled;
 +			r1_bio->state = 0;
 +			r1_bio->mddev = mddev;
 +			r1_bio->sector = bio->bi_sector + sectors_handled;
 +			goto read_again;
 +		} else
 +			generic_make_request(read_bio);
 +		return;
 +	}
 +
 +	/*
 +	 * WRITE:
 +	 */
++=======
++>>>>>>> 3b046a97cbd3 (md/raid1: Refactor raid1_make_request)
  	if (conf->pending_count >= max_queued_requests) {
  		md_wakeup_thread(mddev->thread);
 -		raid1_log(mddev, "wait queued");
  		wait_event(conf->wait_barrier,
  			   conf->pending_count < max_queued_requests);
  	}
@@@ -1350,7 -1346,8 +1469,12 @@@
  			continue;
  
  		mbio = bio_clone_mddev(bio, GFP_NOIO, mddev);
++<<<<<<< HEAD
 +		bio_trim(mbio, r1_bio->sector - bio->bi_sector, max_sectors);
++=======
+ 		bio_trim(mbio, r1_bio->sector - bio->bi_iter.bi_sector,
+ 			 max_sectors);
++>>>>>>> 3b046a97cbd3 (md/raid1: Refactor raid1_make_request)
  
  		if (first_clone) {
  			/* do behind I/O ?
* Unmerged path drivers/md/raid1.c
