kvm: svm: Fix implicit declaration for __default_cpu_present_to_apicid()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
commit 7d669f50847481c52faf0656aea7b4be63113210
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/7d669f50.failed

The commit 8221c1370056 ("svm: Manage vcpu load/unload when enable AVIC")
introduces a build error due to implicit function declaration
when #ifdef CONFIG_X86_32 and #ifndef CONFIG_X86_LOCAL_APIC
(as reported by Kbuild test robot i386-randconfig-x0-06121009).

So, this patch introduces kvm_cpu_get_apicid() wrapper
around __default_cpu_present_to_apicid() with additional
handling if CONFIG_X86_LOCAL_APIC is not defined.

	Reported-by: kbuild test robot <fengguang.wu@intel.com>
Fixes: commit 8221c1370056 ("svm: Manage vcpu load/unload when enable AVIC")
	Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 7d669f50847481c52faf0656aea7b4be63113210)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/svm.c
diff --cc arch/x86/include/asm/kvm_host.h
index e9b71b8ce24b,69e62862b622..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -25,7 -25,9 +25,8 @@@
  #include <linux/pvclock_gtod.h>
  #include <linux/clocksource.h>
  #include <linux/irqbypass.h>
 -#include <linux/hyperv.h>
  
+ #include <asm/apic.h>
  #include <asm/pvclock-abi.h>
  #include <asm/desc.h>
  #include <asm/mtrr.h>
@@@ -1237,6 -1352,31 +1238,34 @@@ bool kvm_vcpu_is_bsp(struct kvm_vcpu *v
  bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
  			     struct kvm_vcpu **dest_vcpu);
  
 -void kvm_set_msi_irq(struct kvm_kernel_irq_routing_entry *e,
 +void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
  		     struct kvm_lapic_irq *irq);
++<<<<<<< HEAD
++=======
+ 
+ static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)
+ {
+ 	if (kvm_x86_ops->vcpu_blocking)
+ 		kvm_x86_ops->vcpu_blocking(vcpu);
+ }
+ 
+ static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
+ {
+ 	if (kvm_x86_ops->vcpu_unblocking)
+ 		kvm_x86_ops->vcpu_unblocking(vcpu);
+ }
+ 
+ static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
+ 
+ static inline int kvm_cpu_get_apicid(int mps_cpu)
+ {
+ #ifdef CONFIG_X86_LOCAL_APIC
+ 	return __default_cpu_present_to_apicid(mps_cpu);
+ #else
+ 	WARN_ON_ONCE(1);
+ 	return BAD_APICID;
+ #endif
+ }
+ 
++>>>>>>> 7d669f508474 (kvm: svm: Fix implicit declaration for __default_cpu_present_to_apicid())
  #endif /* _ASM_X86_KVM_HOST_H */
diff --cc arch/x86/kvm/svm.c
index 6f2d74ac8a7d,8a3c571e4a86..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -1105,6 -1194,194 +1105,194 @@@ static void init_vmcb(struct vcpu_svm *
  	mark_all_dirty(svm->vmcb);
  
  	enable_gif(svm);
++<<<<<<< HEAD
++=======
+ 
+ }
+ 
+ static u64 *avic_get_physical_id_entry(struct kvm_vcpu *vcpu, int index)
+ {
+ 	u64 *avic_physical_id_table;
+ 	struct kvm_arch *vm_data = &vcpu->kvm->arch;
+ 
+ 	if (index >= AVIC_MAX_PHYSICAL_ID_COUNT)
+ 		return NULL;
+ 
+ 	avic_physical_id_table = page_address(vm_data->avic_physical_id_table_page);
+ 
+ 	return &avic_physical_id_table[index];
+ }
+ 
+ /**
+  * Note:
+  * AVIC hardware walks the nested page table to check permissions,
+  * but does not use the SPA address specified in the leaf page
+  * table entry since it uses  address in the AVIC_BACKING_PAGE pointer
+  * field of the VMCB. Therefore, we set up the
+  * APIC_ACCESS_PAGE_PRIVATE_MEMSLOT (4KB) here.
+  */
+ static int avic_init_access_page(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	int ret;
+ 
+ 	if (kvm->arch.apic_access_page_done)
+ 		return 0;
+ 
+ 	ret = x86_set_memory_region(kvm,
+ 				    APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ 				    APIC_DEFAULT_PHYS_BASE,
+ 				    PAGE_SIZE);
+ 	if (ret)
+ 		return ret;
+ 
+ 	kvm->arch.apic_access_page_done = true;
+ 	return 0;
+ }
+ 
+ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
+ {
+ 	int ret;
+ 	u64 *entry, new_entry;
+ 	int id = vcpu->vcpu_id;
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
+ 	ret = avic_init_access_page(vcpu);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (id >= AVIC_MAX_PHYSICAL_ID_COUNT)
+ 		return -EINVAL;
+ 
+ 	if (!svm->vcpu.arch.apic->regs)
+ 		return -EINVAL;
+ 
+ 	svm->avic_backing_page = virt_to_page(svm->vcpu.arch.apic->regs);
+ 
+ 	/* Setting AVIC backing page address in the phy APIC ID table */
+ 	entry = avic_get_physical_id_entry(vcpu, id);
+ 	if (!entry)
+ 		return -EINVAL;
+ 
+ 	new_entry = READ_ONCE(*entry);
+ 	new_entry = (page_to_phys(svm->avic_backing_page) &
+ 		     AVIC_PHYSICAL_ID_ENTRY_BACKING_PAGE_MASK) |
+ 		     AVIC_PHYSICAL_ID_ENTRY_VALID_MASK;
+ 	WRITE_ONCE(*entry, new_entry);
+ 
+ 	svm->avic_physical_id_cache = entry;
+ 
+ 	return 0;
+ }
+ 
+ static void avic_vm_destroy(struct kvm *kvm)
+ {
+ 	struct kvm_arch *vm_data = &kvm->arch;
+ 
+ 	if (vm_data->avic_logical_id_table_page)
+ 		__free_page(vm_data->avic_logical_id_table_page);
+ 	if (vm_data->avic_physical_id_table_page)
+ 		__free_page(vm_data->avic_physical_id_table_page);
+ }
+ 
+ static int avic_vm_init(struct kvm *kvm)
+ {
+ 	int err = -ENOMEM;
+ 	struct kvm_arch *vm_data = &kvm->arch;
+ 	struct page *p_page;
+ 	struct page *l_page;
+ 
+ 	if (!avic)
+ 		return 0;
+ 
+ 	/* Allocating physical APIC ID table (4KB) */
+ 	p_page = alloc_page(GFP_KERNEL);
+ 	if (!p_page)
+ 		goto free_avic;
+ 
+ 	vm_data->avic_physical_id_table_page = p_page;
+ 	clear_page(page_address(p_page));
+ 
+ 	/* Allocating logical APIC ID table (4KB) */
+ 	l_page = alloc_page(GFP_KERNEL);
+ 	if (!l_page)
+ 		goto free_avic;
+ 
+ 	vm_data->avic_logical_id_table_page = l_page;
+ 	clear_page(page_address(l_page));
+ 
+ 	return 0;
+ 
+ free_avic:
+ 	avic_vm_destroy(kvm);
+ 	return err;
+ }
+ 
+ /**
+  * This function is called during VCPU halt/unhalt.
+  */
+ static void avic_set_running(struct kvm_vcpu *vcpu, bool is_run)
+ {
+ 	u64 entry;
+ 	int h_physical_id = kvm_cpu_get_apicid(vcpu->cpu);
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
+ 	if (!kvm_vcpu_apicv_active(vcpu))
+ 		return;
+ 
+ 	svm->avic_is_running = is_run;
+ 
+ 	/* ID = 0xff (broadcast), ID > 0xff (reserved) */
+ 	if (WARN_ON(h_physical_id >= AVIC_MAX_PHYSICAL_ID_COUNT))
+ 		return;
+ 
+ 	entry = READ_ONCE(*(svm->avic_physical_id_cache));
+ 	WARN_ON(is_run == !!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK));
+ 
+ 	entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ 	if (is_run)
+ 		entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ 	WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
+ }
+ 
+ static void avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
+ {
+ 	u64 entry;
+ 	/* ID = 0xff (broadcast), ID > 0xff (reserved) */
+ 	int h_physical_id = kvm_cpu_get_apicid(cpu);
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
+ 	if (!kvm_vcpu_apicv_active(vcpu))
+ 		return;
+ 
+ 	if (WARN_ON(h_physical_id >= AVIC_MAX_PHYSICAL_ID_COUNT))
+ 		return;
+ 
+ 	entry = READ_ONCE(*(svm->avic_physical_id_cache));
+ 	WARN_ON(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
+ 
+ 	entry &= ~AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK;
+ 	entry |= (h_physical_id & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK);
+ 
+ 	entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ 	if (svm->avic_is_running)
+ 		entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ 
+ 	WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
+ }
+ 
+ static void avic_vcpu_put(struct kvm_vcpu *vcpu)
+ {
+ 	u64 entry;
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
+ 	if (!kvm_vcpu_apicv_active(vcpu))
+ 		return;
+ 
+ 	entry = READ_ONCE(*(svm->avic_physical_id_cache));
+ 	entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ 	WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
++>>>>>>> 7d669f508474 (kvm: svm: Fix implicit declaration for __default_cpu_present_to_apicid())
  }
  
  static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
@@@ -3567,6 -4229,18 +3755,21 @@@ static void svm_sync_pir_to_irr(struct 
  	return;
  }
  
++<<<<<<< HEAD
++=======
+ static void svm_deliver_avic_intr(struct kvm_vcpu *vcpu, int vec)
+ {
+ 	kvm_lapic_set_irr(vec, vcpu->arch.apic);
+ 	smp_mb__after_atomic();
+ 
+ 	if (avic_vcpu_is_running(vcpu))
+ 		wrmsrl(SVM_AVIC_DOORBELL,
+ 		       kvm_cpu_get_apicid(vcpu->cpu));
+ 	else
+ 		kvm_vcpu_wake_up(vcpu);
+ }
+ 
++>>>>>>> 7d669f508474 (kvm: svm: Fix implicit declaration for __default_cpu_present_to_apicid())
  static int svm_nmi_allowed(struct kvm_vcpu *vcpu)
  {
  	struct vcpu_svm *svm = to_svm(vcpu);
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/svm.c
