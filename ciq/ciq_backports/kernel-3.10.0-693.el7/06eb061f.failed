dm mpath: requeue after a small delay if blk_get_request() fails

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Bart Van Assche <bart.vanassche@sandisk.com>
commit 06eb061f48594aa369f6e852b352410298b317a8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/06eb061f.failed

If blk_get_request() returns ENODEV then multipath_clone_and_map()
causes a request to be requeued immediately. This can cause a kworker
thread to spend 100% of the CPU time of a single core in
__blk_mq_run_hw_queue() and also can cause device removal to never
finish.

Avoid this by only requeuing after a delay if blk_get_request() fails.
Additionally, reduce the requeue delay.

	Cc: stable@vger.kernel.org # 4.9+
	Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 06eb061f48594aa369f6e852b352410298b317a8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-mpath.c
diff --cc drivers/md/dm-mpath.c
index 80338f538177,a4cc4d42117b..000000000000
--- a/drivers/md/dm-mpath.c
+++ b/drivers/md/dm-mpath.c
@@@ -447,18 -477,18 +447,22 @@@ static int must_push_back(struct multip
  }
  
  /*
 - * Map cloned requests (request-based multipath)
 + * Map cloned requests
   */
 -static int multipath_clone_and_map(struct dm_target *ti, struct request *rq,
 -				   union map_info *map_context,
 -				   struct request **__clone)
 +static int __multipath_map(struct dm_target *ti, struct request *clone,
 +			   union map_info *map_context,
 +			   struct request *rq, struct request **__clone)
  {
  	struct multipath *m = ti->private;
++<<<<<<< HEAD
 +	int r = DM_MAPIO_REQUEUE;
 +	size_t nr_bytes = clone ? blk_rq_bytes(clone) : blk_rq_bytes(rq);
++=======
+ 	size_t nr_bytes = blk_rq_bytes(rq);
++>>>>>>> 06eb061f4859 (dm mpath: requeue after a small delay if blk_get_request() fails)
  	struct pgpath *pgpath;
  	struct block_device *bdev;
 -	struct dm_mpath_io *mpio = get_mpio(map_context);
 -	struct request *clone;
 +	struct dm_mpath_io *mpio;
  
  	/* Do we need to select a new pgpath? */
  	pgpath = lockless_dereference(m->current_pgpath);
@@@ -472,45 -502,82 +476,54 @@@
  	} else if (test_bit(MPATHF_QUEUE_IO, &m->flags) ||
  		   test_bit(MPATHF_PG_INIT_REQUIRED, &m->flags)) {
  		pg_init_all_paths(m);
- 		return r;
+ 		return DM_MAPIO_REQUEUE;
  	}
  
 -	memset(mpio, 0, sizeof(*mpio));
 +	mpio = set_mpio(m, map_context);
 +	if (!mpio)
 +		/* ENOMEM, requeue */
 +		return r;
 +
  	mpio->pgpath = pgpath;
  	mpio->nr_bytes = nr_bytes;
  
  	bdev = pgpath->path.dev->bdev;
  
++<<<<<<< HEAD
 +	if (clone) {
 +		/*
 +		 * Old request-based interface: allocated clone is passed in.
 +		 * Used by: .request_fn stacked on .request_fn path(s).
 +		 */
 +		clone->q = bdev_get_queue(bdev);
 +		clone->rq_disk = bdev->bd_disk;
 +		clone->cmd_flags |= REQ_FAILFAST_TRANSPORT;
 +	} else {
 +		/*
 +		 * blk-mq request-based interface; used by both:
 +		 * .request_fn stacked on blk-mq path(s) and
 +		 * blk-mq stacked on blk-mq path(s).
 +		 */
 +		*__clone = blk_mq_alloc_request(bdev_get_queue(bdev),
 +						rq_data_dir(rq), GFP_ATOMIC, false);
 +		if (IS_ERR(*__clone)) {
 +			/* ENOMEM, requeue */
 +			clear_request_fn_mpio(m, map_context);
 +			return r;
 +		}
 +		(*__clone)->bio = (*__clone)->biotail = NULL;
 +		(*__clone)->rq_disk = bdev->bd_disk;
 +		(*__clone)->cmd_flags |= REQ_FAILFAST_TRANSPORT;
++=======
+ 	clone = blk_get_request(bdev_get_queue(bdev),
+ 			rq->cmd_flags | REQ_NOMERGE,
+ 			GFP_ATOMIC);
+ 	if (IS_ERR(clone)) {
+ 		/* EBUSY, ENODEV or EWOULDBLOCK: requeue */
+ 		return DM_MAPIO_DELAY_REQUEUE;
 -	}
 -	clone->bio = clone->biotail = NULL;
 -	clone->rq_disk = bdev->bd_disk;
 -	clone->cmd_flags |= REQ_FAILFAST_TRANSPORT;
 -	*__clone = clone;
 -
 -	if (pgpath->pg->ps.type->start_io)
 -		pgpath->pg->ps.type->start_io(&pgpath->pg->ps,
 -					      &pgpath->path,
 -					      nr_bytes);
 -	return DM_MAPIO_REMAPPED;
 -}
 -
 -static void multipath_release_clone(struct request *clone)
 -{
 -	blk_put_request(clone);
 -}
 -
 -/*
 - * Map cloned bios (bio-based multipath)
 - */
 -static int __multipath_map_bio(struct multipath *m, struct bio *bio, struct dm_mpath_io *mpio)
 -{
 -	size_t nr_bytes = bio->bi_iter.bi_size;
 -	struct pgpath *pgpath;
 -	unsigned long flags;
 -	bool queue_io;
 -
 -	/* Do we need to select a new pgpath? */
 -	pgpath = lockless_dereference(m->current_pgpath);
 -	queue_io = test_bit(MPATHF_QUEUE_IO, &m->flags);
 -	if (!pgpath || !queue_io)
 -		pgpath = choose_pgpath(m, nr_bytes);
 -
 -	if ((pgpath && queue_io) ||
 -	    (!pgpath && test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags))) {
 -		/* Queue for the daemon to resubmit */
 -		spin_lock_irqsave(&m->lock, flags);
 -		bio_list_add(&m->queued_bios, bio);
 -		spin_unlock_irqrestore(&m->lock, flags);
 -		/* PG_INIT_REQUIRED cannot be set without QUEUE_IO */
 -		if (queue_io || test_bit(MPATHF_PG_INIT_REQUIRED, &m->flags))
 -			pg_init_all_paths(m);
 -		else if (!queue_io)
 -			queue_work(kmultipathd, &m->process_queued_bios);
 -		return DM_MAPIO_SUBMITTED;
++>>>>>>> 06eb061f4859 (dm mpath: requeue after a small delay if blk_get_request() fails)
  	}
  
 -	if (!pgpath) {
 -		if (!must_push_back_bio(m))
 -			return -EIO;
 -		return DM_MAPIO_REQUEUE;
 -	}
 -
 -	mpio->pgpath = pgpath;
 -	mpio->nr_bytes = nr_bytes;
 -
 -	bio->bi_error = 0;
 -	bio->bi_bdev = pgpath->path.dev->bdev;
 -	bio->bi_opf |= REQ_FAILFAST_TRANSPORT;
 -
  	if (pgpath->pg->ps.type->start_io)
  		pgpath->pg->ps.type->start_io(&pgpath->pg->ps,
  					      &pgpath->path,
* Unmerged path drivers/md/dm-mpath.c
diff --git a/drivers/md/dm-rq.c b/drivers/md/dm-rq.c
index 39e68d6df926..5ec997d3a86f 100644
--- a/drivers/md/dm-rq.c
+++ b/drivers/md/dm-rq.c
@@ -350,7 +350,7 @@ static void dm_requeue_original_request(struct dm_rq_target_io *tio, bool delay_
 	if (!rq->q->mq_ops)
 		dm_old_requeue_request(rq);
 	else
-		dm_mq_delay_requeue_request(rq, delay_requeue ? 5000 : 0);
+		dm_mq_delay_requeue_request(rq, delay_requeue ? 100/*ms*/ : 0);
 
 	rq_completed(md, rw, false);
 }
