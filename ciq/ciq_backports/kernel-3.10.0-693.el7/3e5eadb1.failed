scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [scsi] megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth (Tomas Henzl) [1356672]
Rebuild_FUZZ: 96.30%
commit-author Sasikumar Chandrasekaran <sasikumar.pc@broadcom.com>
commit 3e5eadb1a881bea2e3fa41f5ae7cdbfa36222d37
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/3e5eadb1.failed

Large SEQ IO workload should sent as non fast path commands

	Signed-off-by: Sasikumar Chandrasekaran <sasikumar.pc@broadcom.com>
	Reviewed-by: Tomas Henzl <thenzl@redhat.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 3e5eadb1a881bea2e3fa41f5ae7cdbfa36222d37)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/megaraid/megaraid_sas.h
#	drivers/scsi/megaraid/megaraid_sas_fp.c
#	drivers/scsi/megaraid/megaraid_sas_fusion.h
diff --cc drivers/scsi/megaraid/megaraid_sas.h
index 0e2d80818bd7,0696903e28fd..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas.h
+++ b/drivers/scsi/megaraid/megaraid_sas.h
@@@ -2151,6 -2163,9 +2158,12 @@@ struct megasas_instance 
  	bool dev_handle;
  	bool fw_sync_cache_support;
  	bool is_ventura;
++<<<<<<< HEAD
++=======
+ 	bool msix_combined;
+ 	u16 max_raid_mapsize;
+ 	u64 pci_threshold_bandwidth; /* used to control the fp writes */
++>>>>>>> 3e5eadb1a881 (scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth)
  };
  struct MR_LD_VF_MAP {
  	u32 size;
diff --cc drivers/scsi/megaraid/megaraid_sas_fp.c
index eb9ff444c099,322a72b593e3..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas_fp.c
+++ b/drivers/scsi/megaraid/megaraid_sas_fp.c
@@@ -186,11 -189,199 +186,206 @@@ void MR_PopulateDrvRaidMap(struct megas
  	struct MR_DRV_RAID_MAP_ALL *drv_map =
  			fusion->ld_drv_map[(instance->map_id & 1)];
  	struct MR_DRV_RAID_MAP *pDrvRaidMap = &drv_map->raidMap;
++<<<<<<< HEAD
++=======
+ 	void *raid_map_data = NULL;
+ 
+ 	memset(drv_map, 0, fusion->drv_map_sz);
+ 	memset(pDrvRaidMap->ldTgtIdToLd,
+ 		0xff, (sizeof(u16) * MAX_LOGICAL_DRIVES_DYN));
+ 
+ 	if (instance->max_raid_mapsize) {
+ 		fw_map_dyn = fusion->ld_map[(instance->map_id & 1)];
+ 		if (fw_map_dyn->pci_threshold_bandwidth)
+ 			instance->pci_threshold_bandwidth =
+ 			le64_to_cpu(fw_map_dyn->pci_threshold_bandwidth);
+ #if VD_EXT_DEBUG
+ 		dev_dbg(&instance->pdev->dev, "raidMapSize 0x%x fw_map_dyn->descTableOffset 0x%x\n",
+ 			le32_to_cpu(fw_map_dyn->raid_map_size),
+ 			le32_to_cpu(fw_map_dyn->desc_table_offset));
+ 		dev_dbg(&instance->pdev->dev, "descTableSize 0x%x descTableNumElements 0x%x\n",
+ 			le32_to_cpu(fw_map_dyn->desc_table_size),
+ 			le32_to_cpu(fw_map_dyn->desc_table_num_elements));
+ 		dev_dbg(&instance->pdev->dev, "PCIThreasholdBandwidth %llu\n",
+ 			instance->pci_threshold_bandwidth);
+ 		dev_dbg(&instance->pdev->dev, "drv map %p ldCount %d\n",
+ 			drv_map, fw_map_dyn->ld_count);
+ #endif
+ 		desc_table =
+ 		(struct MR_RAID_MAP_DESC_TABLE *)((void *)fw_map_dyn + le32_to_cpu(fw_map_dyn->desc_table_offset));
+ 		if (desc_table != fw_map_dyn->raid_map_desc_table)
+ 			dev_dbg(&instance->pdev->dev, "offsets of desc table are not matching desc %p original %p\n",
+ 				desc_table, fw_map_dyn->raid_map_desc_table);
+ 
+ 		ld_count = (u16)le16_to_cpu(fw_map_dyn->ld_count);
+ 		pDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);
+ 		pDrvRaidMap->fpPdIoTimeoutSec =
+ 			fw_map_dyn->fp_pd_io_timeout_sec;
+ 		pDrvRaidMap->totalSize = sizeof(struct MR_DRV_RAID_MAP_ALL);
+ 		/* point to actual data starting point*/
+ 		raid_map_data = (void *)fw_map_dyn +
+ 			le32_to_cpu(fw_map_dyn->desc_table_offset) +
+ 			le32_to_cpu(fw_map_dyn->desc_table_size);
+ 
+ 		for (i = 0; i < le32_to_cpu(fw_map_dyn->desc_table_num_elements); ++i) {
+ 
+ #if VD_EXT_DEBUG
+ 			dev_dbg(&instance->pdev->dev, "desc table %p\n",
+ 				desc_table);
+ 			dev_dbg(&instance->pdev->dev, "raidmap type %d, raidmapOffset 0x%x\n",
+ 				desc_table->raid_map_desc_type,
+ 				desc_table->raid_map_desc_offset);
+ 			dev_dbg(&instance->pdev->dev, "raid map number of elements 0%x, raidmapsize 0x%x\n",
+ 				desc_table->raid_map_desc_elements,
+ 				desc_table->raid_map_desc_buffer_size);
+ #endif
+ 			switch (le32_to_cpu(desc_table->raid_map_desc_type)) {
+ 			case RAID_MAP_DESC_TYPE_DEVHDL_INFO:
+ 				fw_map_dyn->dev_hndl_info =
+ 				(struct MR_DEV_HANDLE_INFO *)(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "devHndlInfo  address %p\n",
+ 					fw_map_dyn->dev_hndl_info);
+ #endif
+ 				memcpy(pDrvRaidMap->devHndlInfo,
+ 				fw_map_dyn->dev_hndl_info,
+ 				sizeof(struct MR_DEV_HANDLE_INFO) *
+ 				le32_to_cpu(desc_table->raid_map_desc_elements));
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_TGTID_INFO:
+ 				fw_map_dyn->ld_tgt_id_to_ld =
+ 				(u16 *) (raid_map_data +
+ 				le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 			dev_dbg(&instance->pdev->dev, "ldTgtIdToLd  address %p\n",
+ 				fw_map_dyn->ld_tgt_id_to_ld);
+ #endif
+ 			for (j = 0; j < le32_to_cpu(desc_table->raid_map_desc_elements); j++) {
+ 				pDrvRaidMap->ldTgtIdToLd[j] =
+ 				fw_map_dyn->ld_tgt_id_to_ld[j];
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, " %d drv ldTgtIdToLd %d\n",
+ 					j, pDrvRaidMap->ldTgtIdToLd[j]);
+ #endif
+ 			}
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_ARRAY_INFO:
+ 				fw_map_dyn->ar_map_info =
+ 				(struct MR_ARRAY_INFO *)
+ 				(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "arMapInfo  address %p\n",
+ 					fw_map_dyn->ar_map_info);
+ #endif
+ 
+ 				memcpy(pDrvRaidMap->arMapInfo,
+ 				fw_map_dyn->ar_map_info,
+ 				sizeof(struct MR_ARRAY_INFO) * le32_to_cpu(desc_table->raid_map_desc_elements));
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_SPAN_INFO:
+ 				fw_map_dyn->ld_span_map =
+ 				(struct MR_LD_SPAN_MAP *)
+ 				(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ 				memcpy(pDrvRaidMap->ldSpanMap,
+ 				fw_map_dyn->ld_span_map,
+ 				sizeof(struct MR_LD_SPAN_MAP) * le32_to_cpu(desc_table->raid_map_desc_elements));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "ldSpanMap  address %p\n",
+ 					fw_map_dyn->ld_span_map);
+ 				dev_dbg(&instance->pdev->dev, "MR_LD_SPAN_MAP size 0x%lx\n",
+ 					sizeof(struct MR_LD_SPAN_MAP));
+ 				for (j = 0; j < ld_count; j++) {
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : fw_map_dyn->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 					j, j, fw_map_dyn->ld_span_map[j].ldRaid.targetId);
+ 					dev_dbg(&instance->pdev->dev, "fw_map_dyn->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 					j, fw_map_dyn->ld_span_map[j].ldRaid.seqNum);
+ 					dev_dbg(&instance->pdev->dev, "fw_map_dyn->ld_span_map[%d].ldRaid.rowSize 0x%x\n",
+ 					j, (u32)fw_map_dyn->ld_span_map[j].ldRaid.rowSize);
+ 
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) :pDrvRaidMap->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 					j, j, pDrvRaidMap->ldSpanMap[j].ldRaid.targetId);
+ 					dev_dbg(&instance->pdev->dev, "DrvRaidMap->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 					j, pDrvRaidMap->ldSpanMap[j].ldRaid.seqNum);
+ 					dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 					j, (u32)pDrvRaidMap->ldSpanMap[j].ldRaid.rowSize);
+ 
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : drv raid map all %p\n",
+ 					instance->unique_id, drv_map);
+ 					dev_dbg(&instance->pdev->dev, "raid map %p LD RAID MAP %p/%p\n",
+ 					pDrvRaidMap,
+ 					&fw_map_dyn->ld_span_map[j].ldRaid,
+ 					&pDrvRaidMap->ldSpanMap[j].ldRaid);
+ 				}
+ #endif
+ 			break;
+ 			default:
+ 				dev_dbg(&instance->pdev->dev, "wrong number of desctableElements %d\n",
+ 					fw_map_dyn->desc_table_num_elements);
+ 			}
+ 			++desc_table;
+ 		}
+ 
+ 	} else if (instance->supportmax256vd) {
+ 		fw_map_ext =
+ 		(struct MR_FW_RAID_MAP_EXT *) fusion->ld_map[(instance->map_id & 1)];
+ 		ld_count = (u16)le16_to_cpu(fw_map_ext->ldCount);
+ 		if (ld_count > MAX_LOGICAL_DRIVES_EXT) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas: LD count exposed in RAID map in not valid\n");
+ 			return;
+ 		}
+ #if VD_EXT_DEBUG
+ 		for (i = 0; i < ld_count; i++) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) :Index 0x%x\n",
+ 				instance->unique_id, i);
+ 			dev_dbg(&instance->pdev->dev, "Target Id 0x%x\n",
+ 				fw_map_ext->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "Seq Num 0x%x Size 0/%llx\n",
+ 				fw_map_ext->ldSpanMap[i].ldRaid.seqNum,
+ 				fw_map_ext->ldSpanMap[i].ldRaid.size);
+ 		}
+ #endif
+ 
+ 		pDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);
+ 		pDrvRaidMap->fpPdIoTimeoutSec = fw_map_ext->fpPdIoTimeoutSec;
+ 		for (i = 0; i < (MAX_LOGICAL_DRIVES_EXT); i++)
+ 			pDrvRaidMap->ldTgtIdToLd[i] =
+ 				(u16)fw_map_ext->ldTgtIdToLd[i];
+ 		memcpy(pDrvRaidMap->ldSpanMap, fw_map_ext->ldSpanMap,
+ 				sizeof(struct MR_LD_SPAN_MAP) * ld_count);
+ #if VD_EXT_DEBUG
+ 		for (i = 0; i < ld_count; i++) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : fw_map_ext->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 			i, i, fw_map_ext->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "fw_map_ext->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 			i, fw_map_ext->ldSpanMap[i].ldRaid.seqNum);
+ 			dev_dbg(&instance->pdev->dev, "fw_map_ext->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 			i, (u32)fw_map_ext->ldSpanMap[i].ldRaid.rowSize);
+ 
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : pDrvRaidMap->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 			i, i, pDrvRaidMap->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 			i, pDrvRaidMap->ldSpanMap[i].ldRaid.seqNum);
+ 			dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 			i, (u32)pDrvRaidMap->ldSpanMap[i].ldRaid.rowSize);
+ 
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : drv raid map all %p\n",
+ 			instance->unique_id, drv_map);
+ 			dev_dbg(&instance->pdev->dev, "raid map %p LD RAID MAP %p %p\n",
+ 			pDrvRaidMap, &fw_map_ext->ldSpanMap[i].ldRaid,
+ 			&pDrvRaidMap->ldSpanMap[i].ldRaid);
+ 		}
+ #endif
+ 		memcpy(pDrvRaidMap->arMapInfo, fw_map_ext->arMapInfo,
+ 			sizeof(struct MR_ARRAY_INFO) * MAX_API_ARRAYS_EXT);
+ 		memcpy(pDrvRaidMap->devHndlInfo, fw_map_ext->devHndlInfo,
+ 			sizeof(struct MR_DEV_HANDLE_INFO) *
+ 					MAX_RAIDMAP_PHYSICAL_DEVICES);
++>>>>>>> 3e5eadb1a881 (scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth)
  
 +	if (instance->supportmax256vd) {
 +		memcpy(fusion->ld_drv_map[instance->map_id & 1],
 +			fusion->ld_map[instance->map_id & 1],
 +			fusion->current_map_sz);
  		/* New Raid map will not set totalSize, so keep expected value
  		 * for legacy code in ValidateMapInfo
  		 */
diff --cc drivers/scsi/megaraid/megaraid_sas_fusion.h
index ef6bfe55344c,391aae6e27a4..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas_fusion.h
+++ b/drivers/scsi/megaraid/megaraid_sas_fusion.h
@@@ -847,6 -931,91 +847,94 @@@ struct MR_LD_TARGET_SYNC 
  	__le16 seqNum;
  };
  
++<<<<<<< HEAD
++=======
+ /*
+  * RAID Map descriptor Types.
+  * Each element should uniquely idetify one data structure in the RAID map
+  */
+ enum MR_RAID_MAP_DESC_TYPE {
+ 	/* MR_DEV_HANDLE_INFO data */
+ 	RAID_MAP_DESC_TYPE_DEVHDL_INFO    = 0x0,
+ 	/* target to Ld num Index map */
+ 	RAID_MAP_DESC_TYPE_TGTID_INFO     = 0x1,
+ 	/* MR_ARRAY_INFO data */
+ 	RAID_MAP_DESC_TYPE_ARRAY_INFO     = 0x2,
+ 	/* MR_LD_SPAN_MAP data */
+ 	RAID_MAP_DESC_TYPE_SPAN_INFO      = 0x3,
+ 	RAID_MAP_DESC_TYPE_COUNT,
+ };
+ 
+ /*
+  * This table defines the offset, size and num elements  of each descriptor
+  * type in the RAID Map buffer
+  */
+ struct MR_RAID_MAP_DESC_TABLE {
+ 	/* Raid map descriptor type */
+ 	u32 raid_map_desc_type;
+ 	/* Offset into the RAID map buffer where
+ 	 *  descriptor data is saved
+ 	 */
+ 	u32 raid_map_desc_offset;
+ 	/* total size of the
+ 	 * descriptor buffer
+ 	 */
+ 	u32 raid_map_desc_buffer_size;
+ 	/* Number of elements contained in the
+ 	 *  descriptor buffer
+ 	 */
+ 	u32 raid_map_desc_elements;
+ };
+ 
+ /*
+  * Dynamic Raid Map Structure.
+  */
+ struct MR_FW_RAID_MAP_DYNAMIC {
+ 	u32 raid_map_size;   /* total size of RAID Map structure */
+ 	u32 desc_table_offset;/* Offset of desc table into RAID map*/
+ 	u32 desc_table_size;  /* Total Size of desc table */
+ 	/* Total Number of elements in the desc table */
+ 	u32 desc_table_num_elements;
+ 	u64	pci_threshold_bandwidth;
+ 	u32	reserved2[3];	/*future use */
+ 	/* timeout value used by driver in FP IOs */
+ 	u8 fp_pd_io_timeout_sec;
+ 	u8 reserved3[3];
+ 	/* when this seqNum increments, driver needs to
+ 	 *  release RMW buffers asap
+ 	 */
+ 	u32 rmw_fp_seq_num;
+ 	u16 ld_count;	/* count of lds. */
+ 	u16 ar_count;   /* count of arrays */
+ 	u16 span_count; /* count of spans */
+ 	u16 reserved4[3];
+ /*
+  * The below structure of pointers is only to be used by the driver.
+  * This is added in the ,API to reduce the amount of code changes
+  * needed in the driver to support dynamic RAID map Firmware should
+  * not update these pointers while preparing the raid map
+  */
+ 	union {
+ 		struct {
+ 			struct MR_DEV_HANDLE_INFO  *dev_hndl_info;
+ 			u16 *ld_tgt_id_to_ld;
+ 			struct MR_ARRAY_INFO *ar_map_info;
+ 			struct MR_LD_SPAN_MAP *ld_span_map;
+ 			};
+ 		u64 ptr_structure_size[RAID_MAP_DESC_TYPE_COUNT];
+ 		};
+ /*
+  * RAID Map descriptor table defines the layout of data in the RAID Map.
+  * The size of the descriptor table itself could change.
+  */
+ 	/* Variable Size descriptor Table. */
+ 	struct MR_RAID_MAP_DESC_TABLE
+ 			raid_map_desc_table[RAID_MAP_DESC_TYPE_COUNT];
+ 	/* Variable Size buffer containing all data */
+ 	u32 raid_map_desc_data[1];
+ }; /* Dynamicaly sized RAID MAp structure */
+ 
++>>>>>>> 3e5eadb1a881 (scsi: megaraid_sas: Enable or Disable Fast path based on the PCI Threshold Bandwidth)
  #define IEEE_SGE_FLAGS_ADDR_MASK            (0x03)
  #define IEEE_SGE_FLAGS_SYSTEM_ADDR          (0x00)
  #define IEEE_SGE_FLAGS_IOCDDR_ADDR          (0x01)
* Unmerged path drivers/scsi/megaraid/megaraid_sas.h
diff --git a/drivers/scsi/megaraid/megaraid_sas_base.c b/drivers/scsi/megaraid/megaraid_sas_base.c
index cb705ac4bba8..598520c2b1a7 100644
--- a/drivers/scsi/megaraid/megaraid_sas_base.c
+++ b/drivers/scsi/megaraid/megaraid_sas_base.c
@@ -1946,6 +1946,9 @@ void megaraid_sas_kill_hba(struct megasas_instance *instance)
 	}
 	/* Complete outstanding ioctls when adapter is killed */
 	megasas_complete_outstanding_ioctls(instance);
+	if (instance->is_ventura)
+		del_timer_sync(&instance->r1_fp_hold_timer);
+
 }
 
  /**
@@ -2444,6 +2447,24 @@ void megasas_sriov_heartbeat_handler(unsigned long instance_addr)
 	}
 }
 
+/*Handler for disabling/enabling raid 1 fast paths*/
+void megasas_change_r1_fp_status(unsigned long instance_addr)
+{
+	struct megasas_instance *instance =
+			(struct megasas_instance *)instance_addr;
+	if (atomic64_read(&instance->bytes_wrote) >=
+					instance->pci_threshold_bandwidth) {
+
+		atomic64_set(&instance->bytes_wrote, 0);
+		atomic_set(&instance->r1_write_fp_capable, 0);
+	} else {
+		atomic64_set(&instance->bytes_wrote, 0);
+		atomic_set(&instance->r1_write_fp_capable, 1);
+	}
+	mod_timer(&instance->r1_fp_hold_timer,
+	 jiffies + MEGASAS_RAID1_FAST_PATH_STATUS_CHECK_INTERVAL);
+}
+
 /**
  * megasas_wait_for_outstanding -	Wait for all outstanding cmds
  * @instance:				Adapter soft state
@@ -5354,6 +5375,17 @@ static int megasas_init_fw(struct megasas_instance *instance)
 			instance->skip_heartbeat_timer_del = 1;
 	}
 
+	if (instance->is_ventura) {
+		atomic64_set(&instance->bytes_wrote, 0);
+		atomic_set(&instance->r1_write_fp_capable, 1);
+		megasas_start_timer(instance,
+			    &instance->r1_fp_hold_timer,
+			    megasas_change_r1_fp_status,
+			    MEGASAS_RAID1_FAST_PATH_STATUS_CHECK_INTERVAL);
+				dev_info(&instance->pdev->dev, "starting the raid 1 fp timer with interval %d\n",
+				MEGASAS_RAID1_FAST_PATH_STATUS_CHECK_INTERVAL);
+	}
+
 	return 0;
 
 fail_get_ld_pd_list:
@@ -6154,6 +6186,9 @@ megasas_suspend(struct pci_dev *pdev, pm_message_t state)
 	if (instance->requestorId && !instance->skip_heartbeat_timer_del)
 		del_timer_sync(&instance->sriov_heartbeat_timer);
 
+	if (instance->is_ventura)
+		del_timer_sync(&instance->r1_fp_hold_timer);
+
 	megasas_flush_cache(instance);
 	megasas_shutdown_controller(instance, MR_DCMD_HIBERNATE_SHUTDOWN);
 
@@ -6273,6 +6308,16 @@ megasas_resume(struct pci_dev *pdev)
 	megasas_setup_jbod_map(instance);
 	instance->unload = 0;
 
+	if (instance->is_ventura) {
+		atomic64_set(&instance->bytes_wrote, 0);
+		atomic_set(&instance->r1_write_fp_capable, 1);
+		megasas_start_timer(instance,
+			    &instance->r1_fp_hold_timer,
+			    megasas_change_r1_fp_status,
+			    MEGASAS_RAID1_FAST_PATH_STATUS_CHECK_INTERVAL);
+	}
+
+
 	/*
 	 * Initiate AEN (Asynchronous Event Notification)
 	 */
@@ -6361,6 +6406,9 @@ static void megasas_detach_one(struct pci_dev *pdev)
 	if (instance->requestorId && !instance->skip_heartbeat_timer_del)
 		del_timer_sync(&instance->sriov_heartbeat_timer);
 
+	if (instance->is_ventura)
+		del_timer_sync(&instance->r1_fp_hold_timer);
+
 	if (instance->fw_crash_state != UNAVAILABLE)
 		megasas_free_host_crash_buffer(instance);
 	scsi_remove_host(instance->host);
* Unmerged path drivers/scsi/megaraid/megaraid_sas_fp.c
diff --git a/drivers/scsi/megaraid/megaraid_sas_fusion.c b/drivers/scsi/megaraid/megaraid_sas_fusion.c
index 052f0ebb7f67..214f803fd900 100644
--- a/drivers/scsi/megaraid/megaraid_sas_fusion.c
+++ b/drivers/scsi/megaraid/megaraid_sas_fusion.c
@@ -95,6 +95,7 @@ extern unsigned int resetwaittime;
 extern unsigned int dual_qdepth_disable;
 static void megasas_free_rdpq_fusion(struct megasas_instance *instance);
 static void megasas_free_reply_fusion(struct megasas_instance *instance);
+void megasas_change_r1_fp_status(unsigned long instance_addr);
 
 
 
@@ -2495,8 +2496,9 @@ megasas_build_and_issue_cmd_fusion(struct megasas_instance *instance,
 	 *	to get new command
 	 */
 	if (cmd->is_raid_1_fp_write &&
-		atomic_inc_return(&instance->fw_outstanding) >
-			(instance->host->can_queue)) {
+		(atomic_inc_return(&instance->fw_outstanding) >
+			(instance->host->can_queue) ||
+		(!atomic_read(&instance->r1_write_fp_capable)))) {
 		megasas_fpio_to_ldio(instance, cmd, cmd->scmd);
 		atomic_dec(&instance->fw_outstanding);
 	} else if (cmd->is_raid_1_fp_write) {
@@ -2505,17 +2507,19 @@ megasas_build_and_issue_cmd_fusion(struct megasas_instance *instance,
 		megasas_prepare_secondRaid1_IO(instance, cmd, r1_cmd);
 	}
 
-
 	/*
 	 * Issue the command to the FW
 	 */
+	if (scmd->sc_data_direction == PCI_DMA_TODEVICE && instance->is_ventura)
+		atomic64_add(scsi_bufflen(scmd), &instance->bytes_wrote);
 
 	megasas_fire_cmd_fusion(instance, req_desc, instance->is_ventura);
 
-	if (r1_cmd)
+	if (r1_cmd) {
+		atomic64_add(scsi_bufflen(scmd), &instance->bytes_wrote);
 		megasas_fire_cmd_fusion(instance, r1_cmd->request_desc,
-				instance->is_ventura);
-
+			instance->is_ventura);
+	}
 
 	return 0;
 }
* Unmerged path drivers/scsi/megaraid/megaraid_sas_fusion.h
