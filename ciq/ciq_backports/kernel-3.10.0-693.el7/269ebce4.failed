xen-netfront: cast grant table reference first to type int

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Dongli Zhang <dongli.zhang@oracle.com>
commit 269ebce4531b8edc4224259a02143181a1c1d77c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/269ebce4.failed

IS_ERR_VALUE() in commit 87557efc27f6a50140fb20df06a917f368ce3c66
("xen-netfront: do not cast grant table reference to signed short") would
not return true for error code unless we cast ref first to type int.

	Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 269ebce4531b8edc4224259a02143181a1c1d77c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/xen-netfront.c
diff --cc drivers/net/xen-netfront.c
index 8896052a2ee0,bf2744e1e3db..000000000000
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@@ -221,117 -229,101 +221,123 @@@ static bool xennet_can_sg(struct net_de
  
  static void rx_refill_timeout(unsigned long data)
  {
 -	struct netfront_queue *queue = (struct netfront_queue *)data;
 -	napi_schedule(&queue->napi);
 +	struct net_device *dev = (struct net_device *)data;
 +	struct netfront_info *np = netdev_priv(dev);
 +	napi_schedule(&np->napi);
  }
  
 -static int netfront_tx_slot_available(struct netfront_queue *queue)
 +static int netfront_tx_slot_available(struct netfront_info *np)
  {
 -	return (queue->tx.req_prod_pvt - queue->tx.rsp_cons) <
 -		(NET_TX_RING_SIZE - MAX_SKB_FRAGS - 2);
 +	return (np->tx.req_prod_pvt - np->tx.rsp_cons) <
 +		(TX_MAX_TARGET - MAX_SKB_FRAGS - 2);
  }
  
 -static void xennet_maybe_wake_tx(struct netfront_queue *queue)
 +static void xennet_maybe_wake_tx(struct net_device *dev)
  {
 -	struct net_device *dev = queue->info->netdev;
 -	struct netdev_queue *dev_queue = netdev_get_tx_queue(dev, queue->id);
 +	struct netfront_info *np = netdev_priv(dev);
  
 -	if (unlikely(netif_tx_queue_stopped(dev_queue)) &&
 -	    netfront_tx_slot_available(queue) &&
 +	if (unlikely(netif_queue_stopped(dev)) &&
 +	    netfront_tx_slot_available(np) &&
  	    likely(netif_running(dev)))
 -		netif_tx_wake_queue(netdev_get_tx_queue(dev, queue->id));
 +		netif_wake_queue(dev);
  }
  
 -
 -static struct sk_buff *xennet_alloc_one_rx_buffer(struct netfront_queue *queue)
 +static void xennet_alloc_rx_buffers(struct net_device *dev)
  {
 +	unsigned short id;
 +	struct netfront_info *np = netdev_priv(dev);
  	struct sk_buff *skb;
  	struct page *page;
 +	int i, batch_target, notify;
 +	RING_IDX req_prod = np->rx.req_prod_pvt;
 +	grant_ref_t ref;
 +	unsigned long pfn;
 +	void *vaddr;
 +	struct xen_netif_rx_request *req;
  
 -	skb = __netdev_alloc_skb(queue->info->netdev,
 -				 RX_COPY_THRESHOLD + NET_IP_ALIGN,
 -				 GFP_ATOMIC | __GFP_NOWARN);
 -	if (unlikely(!skb))
 -		return NULL;
 -
 -	page = alloc_page(GFP_ATOMIC | __GFP_NOWARN);
 -	if (!page) {
 -		kfree_skb(skb);
 -		return NULL;
 -	}
 -	skb_add_rx_frag(skb, 0, page, 0, 0, PAGE_SIZE);
 -
 -	/* Align ip header to a 16 bytes boundary */
 -	skb_reserve(skb, NET_IP_ALIGN);
 -	skb->dev = queue->info->netdev;
 -
 -	return skb;
 -}
 +	if (unlikely(!netif_carrier_ok(dev)))
 +		return;
  
 +	/*
 +	 * Allocate skbuffs greedily, even though we batch updates to the
 +	 * receive ring. This creates a less bursty demand on the memory
 +	 * allocator, so should reduce the chance of failed allocation requests
 +	 * both for ourself and for other kernel subsystems.
 +	 */
 +	batch_target = np->rx_target - (req_prod - np->rx.rsp_cons);
 +	for (i = skb_queue_len(&np->rx_batch); i < batch_target; i++) {
 +		skb = __netdev_alloc_skb(dev, RX_COPY_THRESHOLD + NET_IP_ALIGN,
 +					 GFP_ATOMIC | __GFP_NOWARN);
 +		if (unlikely(!skb))
 +			goto no_skb;
 +
 +		/* Align ip header to a 16 bytes boundary */
 +		skb_reserve(skb, NET_IP_ALIGN);
 +
 +		page = alloc_page(GFP_ATOMIC | __GFP_NOWARN);
 +		if (!page) {
 +			kfree_skb(skb);
 +no_skb:
 +			/* Could not allocate any skbuffs. Try again later. */
 +			mod_timer(&np->rx_refill_timer,
 +				  jiffies + (HZ/10));
 +
 +			/* Any skbuffs queued for refill? Force them out. */
 +			if (i != 0)
 +				goto refill;
 +			break;
 +		}
  
 -static void xennet_alloc_rx_buffers(struct netfront_queue *queue)
 -{
 -	RING_IDX req_prod = queue->rx.req_prod_pvt;
 -	int notify;
 +		skb_add_rx_frag(skb, 0, page, 0, 0, PAGE_SIZE);
 +		__skb_queue_tail(&np->rx_batch, skb);
 +	}
  
 -	if (unlikely(!netif_carrier_ok(queue->info->netdev)))
 +	/* Is the batch large enough to be worthwhile? */
 +	if (i < (np->rx_target/2)) {
 +		if (req_prod > np->rx.sring->req_prod)
 +			goto push;
  		return;
 +	}
  
 -	for (req_prod = queue->rx.req_prod_pvt;
 -	     req_prod - queue->rx.rsp_cons < NET_RX_RING_SIZE;
 -	     req_prod++) {
 -		struct sk_buff *skb;
 -		unsigned short id;
 -		grant_ref_t ref;
 -		struct page *page;
 -		struct xen_netif_rx_request *req;
 +	/* Adjust our fill target if we risked running out of buffers. */
 +	if (((req_prod - np->rx.sring->rsp_prod) < (np->rx_target / 4)) &&
 +	    ((np->rx_target *= 2) > np->rx_max_target))
 +		np->rx_target = np->rx_max_target;
  
 -		skb = xennet_alloc_one_rx_buffer(queue);
 -		if (!skb)
 + refill:
 +	for (i = 0; ; i++) {
 +		skb = __skb_dequeue(&np->rx_batch);
 +		if (skb == NULL)
  			break;
  
 -		id = xennet_rxidx(req_prod);
 +		skb->dev = dev;
  
 -		BUG_ON(queue->rx_skbs[id]);
 -		queue->rx_skbs[id] = skb;
 +		id = xennet_rxidx(req_prod + i);
  
++<<<<<<< HEAD
 +		BUG_ON(np->rx_skbs[id]);
 +		np->rx_skbs[id] = skb;
++=======
+ 		ref = gnttab_claim_grant_reference(&queue->gref_rx_head);
+ 		WARN_ON_ONCE(IS_ERR_VALUE((unsigned long)(int)ref));
+ 		queue->grant_rx_ref[id] = ref;
++>>>>>>> 269ebce4531b (xen-netfront: cast grant table reference first to type int)
  
 -		page = skb_frag_page(&skb_shinfo(skb)->frags[0]);
 +		ref = gnttab_claim_grant_reference(&np->gref_rx_head);
 +		BUG_ON((signed short)ref < 0);
 +		np->grant_rx_ref[id] = ref;
  
 -		req = RING_GET_REQUEST(&queue->rx, req_prod);
 -		gnttab_page_grant_foreign_access_ref_one(ref,
 -							 queue->info->xbdev->otherend_id,
 -							 page,
 -							 0);
 -		req->id = id;
 -		req->gref = ref;
 -	}
 +		pfn = page_to_pfn(skb_frag_page(&skb_shinfo(skb)->frags[0]));
 +		vaddr = page_address(skb_frag_page(&skb_shinfo(skb)->frags[0]));
  
 -	queue->rx.req_prod_pvt = req_prod;
 +		req = RING_GET_REQUEST(&np->rx, req_prod + i);
 +		gnttab_grant_foreign_access_ref(ref,
 +						np->xbdev->otherend_id,
 +						pfn_to_mfn(pfn),
 +						0);
  
 -	/* Not enough requests? Try again later. */
 -	if (req_prod - queue->rx.rsp_cons < NET_RX_SLOTS_MIN) {
 -		mod_timer(&queue->rx_refill_timer, jiffies + (HZ/10));
 -		return;
 +		req->id = id;
 +		req->gref = ref;
  	}
  
  	wmb();		/* barrier so backend seens requests */
@@@ -402,118 -397,111 +408,193 @@@ static void xennet_tx_buf_gc(struct net
  			dev_kfree_skb_irq(skb);
  		}
  
 -		queue->tx.rsp_cons = prod;
 +		np->tx.rsp_cons = prod;
  
 -		RING_FINAL_CHECK_FOR_RESPONSES(&queue->tx, more_to_do);
 -	} while (more_to_do);
 +		/*
 +		 * Set a new event, then check for race with update of tx_cons.
 +		 * Note that it is essential to schedule a callback, no matter
 +		 * how few buffers are pending. Even if there is space in the
 +		 * transmit ring, higher layers may be blocked because too much
 +		 * data is outstanding: in such cases notification from Xen is
 +		 * likely to be the only kick that we'll get.
 +		 */
 +		np->tx.sring->rsp_event =
 +			prod + ((np->tx.sring->req_prod - prod) >> 1) + 1;
 +		mb();		/* update shared area */
 +	} while ((cons == prod) && (prod != np->tx.sring->rsp_prod));
  
 -	xennet_maybe_wake_tx(queue);
 +	xennet_maybe_wake_tx(dev);
  }
  
 -struct xennet_gnttab_make_txreq {
 -	struct netfront_queue *queue;
 -	struct sk_buff *skb;
 -	struct page *page;
 -	struct xen_netif_tx_request *tx; /* Last request */
 -	unsigned int size;
 -};
 -
 -static void xennet_tx_setup_grant(unsigned long gfn, unsigned int offset,
 -				  unsigned int len, void *data)
 +static void xennet_make_frags(struct sk_buff *skb, struct net_device *dev,
 +			      struct xen_netif_tx_request *tx)
  {
 -	struct xennet_gnttab_make_txreq *info = data;
 +	struct netfront_info *np = netdev_priv(dev);
 +	char *data = skb->data;
 +	unsigned long mfn;
 +	RING_IDX prod = np->tx.req_prod_pvt;
 +	int frags = skb_shinfo(skb)->nr_frags;
 +	unsigned int offset = offset_in_page(data);
 +	unsigned int len = skb_headlen(skb);
  	unsigned int id;
 -	struct xen_netif_tx_request *tx;
  	grant_ref_t ref;
 -	/* convenient aliases */
 -	struct page *page = info->page;
 -	struct netfront_queue *queue = info->queue;
 -	struct sk_buff *skb = info->skb;
 +	int i;
  
++<<<<<<< HEAD
 +	/* While the header overlaps a page boundary (including being
 +	   larger than a page), split it it into page-sized chunks. */
 +	while (len > PAGE_SIZE - offset) {
 +		tx->size = PAGE_SIZE - offset;
 +		tx->flags |= XEN_NETTXF_more_data;
 +		len -= tx->size;
 +		data += tx->size;
++=======
+ 	id = get_id_from_freelist(&queue->tx_skb_freelist, queue->tx_skbs);
+ 	tx = RING_GET_REQUEST(&queue->tx, queue->tx.req_prod_pvt++);
+ 	ref = gnttab_claim_grant_reference(&queue->gref_tx_head);
+ 	WARN_ON_ONCE(IS_ERR_VALUE((unsigned long)(int)ref));
+ 
+ 	gnttab_grant_foreign_access_ref(ref, queue->info->xbdev->otherend_id,
+ 					gfn, GNTMAP_readonly);
+ 
+ 	queue->tx_skbs[id].skb = skb;
+ 	queue->grant_tx_page[id] = page;
+ 	queue->grant_tx_ref[id] = ref;
+ 
+ 	tx->id = id;
+ 	tx->gref = ref;
+ 	tx->offset = offset;
+ 	tx->size = len;
+ 	tx->flags = 0;
+ 
+ 	info->tx = tx;
+ 	info->size += tx->size;
+ }
+ 
+ static struct xen_netif_tx_request *xennet_make_first_txreq(
+ 	struct netfront_queue *queue, struct sk_buff *skb,
+ 	struct page *page, unsigned int offset, unsigned int len)
+ {
+ 	struct xennet_gnttab_make_txreq info = {
+ 		.queue = queue,
+ 		.skb = skb,
+ 		.page = page,
+ 		.size = 0,
+ 	};
+ 
+ 	gnttab_for_one_grant(page, offset, len, xennet_tx_setup_grant, &info);
+ 
+ 	return info.tx;
+ }
+ 
+ static void xennet_make_one_txreq(unsigned long gfn, unsigned int offset,
+ 				  unsigned int len, void *data)
+ {
+ 	struct xennet_gnttab_make_txreq *info = data;
+ 
+ 	info->tx->flags |= XEN_NETTXF_more_data;
+ 	skb_get(info->skb);
+ 	xennet_tx_setup_grant(gfn, offset, len, data);
+ }
+ 
+ static struct xen_netif_tx_request *xennet_make_txreqs(
+ 	struct netfront_queue *queue, struct xen_netif_tx_request *tx,
+ 	struct sk_buff *skb, struct page *page,
+ 	unsigned int offset, unsigned int len)
+ {
+ 	struct xennet_gnttab_make_txreq info = {
+ 		.queue = queue,
+ 		.skb = skb,
+ 		.tx = tx,
+ 	};
+ 
+ 	/* Skip unused frames from start of page */
+ 	page += offset >> PAGE_SHIFT;
+ 	offset &= ~PAGE_MASK;
+ 
+ 	while (len) {
+ 		info.page = page;
+ 		info.size = 0;
+ 
+ 		gnttab_foreach_grant_in_range(page, offset, len,
+ 					      xennet_make_one_txreq,
+ 					      &info);
+ 
+ 		page++;
++>>>>>>> 269ebce4531b (xen-netfront: cast grant table reference first to type int)
  		offset = 0;
 -		len -= info.size;
 +
 +		id = get_id_from_freelist(&np->tx_skb_freelist, np->tx_skbs);
 +		np->tx_skbs[id].skb = skb_get(skb);
 +		tx = RING_GET_REQUEST(&np->tx, prod++);
 +		tx->id = id;
 +		ref = gnttab_claim_grant_reference(&np->gref_tx_head);
 +		BUG_ON((signed short)ref < 0);
 +
 +		mfn = virt_to_mfn(data);
 +		gnttab_grant_foreign_access_ref(ref, np->xbdev->otherend_id,
 +						mfn, GNTMAP_readonly);
 +
 +		np->grant_tx_page[id] = virt_to_page(data);
 +		tx->gref = np->grant_tx_ref[id] = ref;
 +		tx->offset = offset;
 +		tx->size = len;
 +		tx->flags = 0;
  	}
  
 -	return info.tx;
 +	/* Grant backend access to each skb fragment page. */
 +	for (i = 0; i < frags; i++) {
 +		skb_frag_t *frag = skb_shinfo(skb)->frags + i;
 +		struct page *page = skb_frag_page(frag);
 +
 +		len = skb_frag_size(frag);
 +		offset = frag->page_offset;
 +
 +		/* Skip unused frames from start of page */
 +		page += offset >> PAGE_SHIFT;
 +		offset &= ~PAGE_MASK;
 +
 +		while (len > 0) {
 +			unsigned long bytes;
 +
 +			bytes = PAGE_SIZE - offset;
 +			if (bytes > len)
 +				bytes = len;
 +
 +			tx->flags |= XEN_NETTXF_more_data;
 +
 +			id = get_id_from_freelist(&np->tx_skb_freelist,
 +						  np->tx_skbs);
 +			np->tx_skbs[id].skb = skb_get(skb);
 +			tx = RING_GET_REQUEST(&np->tx, prod++);
 +			tx->id = id;
 +			ref = gnttab_claim_grant_reference(&np->gref_tx_head);
 +			BUG_ON((signed short)ref < 0);
 +
 +			mfn = pfn_to_mfn(page_to_pfn(page));
 +			gnttab_grant_foreign_access_ref(ref,
 +							np->xbdev->otherend_id,
 +							mfn, GNTMAP_readonly);
 +
 +			np->grant_tx_page[id] = page;
 +			tx->gref = np->grant_tx_ref[id] = ref;
 +			tx->offset = offset;
 +			tx->size = bytes;
 +			tx->flags = 0;
 +
 +			offset += bytes;
 +			len -= bytes;
 +
 +			/* Next frame */
 +			if (offset == PAGE_SIZE && len) {
 +				BUG_ON(!PageCompound(page));
 +				page++;
 +				offset = 0;
 +			}
 +		}
 +	}
 +
 +	np->tx.req_prod_pvt = prod;
  }
  
  /*
* Unmerged path drivers/net/xen-netfront.c
