crypto: ccp - Refactor code supporting the CCP's RNG

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [crypto] ccp - Refactor code supporting the CCP's RNG (Suravee Suthikulpanit) [1390820]
Rebuild_FUZZ: 91.67%
commit-author Gary R Hook <gary.hook@amd.com>
commit 8256e683113e659d9bf6bffdd227eeb1881ae9a7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/8256e683.failed

Make the RNG support code common (where possible) in
preparation for adding a v5 device.

	Signed-off-by: Gary R Hook <gary.hook@amd.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 8256e683113e659d9bf6bffdd227eeb1881ae9a7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/ccp/ccp-dev-v3.c
#	drivers/crypto/ccp/ccp-dev.c
#	drivers/crypto/ccp/ccp-dev.h
diff --cc drivers/crypto/ccp/ccp-dev.c
index 2777dc97b570,6b44730ef9d6..000000000000
--- a/drivers/crypto/ccp/ccp-dev.c
+++ b/drivers/crypto/ccp/ccp-dev.c
@@@ -299,244 -409,32 +299,273 @@@ struct ccp_device *ccp_alloc_struct(str
  	return ccp;
  }
  
++<<<<<<< HEAD
 +/**
 + * ccp_init - initialize the CCP device
 + *
 + * @ccp: ccp_device struct
 + */
 +int ccp_init(struct ccp_device *ccp)
 +{
 +	struct device *dev = ccp->dev;
 +	struct ccp_cmd_queue *cmd_q;
 +	struct dma_pool *dma_pool;
 +	char dma_pool_name[MAX_DMAPOOL_NAME_LEN];
 +	unsigned int qmr, qim, i;
 +	int ret;
 +
 +	/* Find available queues */
 +	qim = 0;
 +	qmr = ioread32(ccp->io_regs + Q_MASK_REG);
 +	for (i = 0; i < MAX_HW_QUEUES; i++) {
 +		if (!(qmr & (1 << i)))
 +			continue;
 +
 +		/* Allocate a dma pool for this queue */
 +		snprintf(dma_pool_name, sizeof(dma_pool_name), "ccp_q%d", i);
 +		dma_pool = dma_pool_create(dma_pool_name, dev,
 +					   CCP_DMAPOOL_MAX_SIZE,
 +					   CCP_DMAPOOL_ALIGN, 0);
 +		if (!dma_pool) {
 +			dev_err(dev, "unable to allocate dma pool\n");
 +			ret = -ENOMEM;
 +			goto e_pool;
 +		}
 +
 +		cmd_q = &ccp->cmd_q[ccp->cmd_q_count];
 +		ccp->cmd_q_count++;
 +
 +		cmd_q->ccp = ccp;
 +		cmd_q->id = i;
 +		cmd_q->dma_pool = dma_pool;
 +
 +		/* Reserve 2 KSB regions for the queue */
 +		cmd_q->ksb_key = KSB_START + ccp->ksb_start++;
 +		cmd_q->ksb_ctx = KSB_START + ccp->ksb_start++;
 +		ccp->ksb_count -= 2;
 +
 +		/* Preset some register values and masks that are queue
 +		 * number dependent
 +		 */
 +		cmd_q->reg_status = ccp->io_regs + CMD_Q_STATUS_BASE +
 +				    (CMD_Q_STATUS_INCR * i);
 +		cmd_q->reg_int_status = ccp->io_regs + CMD_Q_INT_STATUS_BASE +
 +					(CMD_Q_STATUS_INCR * i);
 +		cmd_q->int_ok = 1 << (i * 2);
 +		cmd_q->int_err = 1 << ((i * 2) + 1);
 +
 +		cmd_q->free_slots = CMD_Q_DEPTH(ioread32(cmd_q->reg_status));
 +
 +		init_waitqueue_head(&cmd_q->int_queue);
 +
 +		/* Build queue interrupt mask (two interrupts per queue) */
 +		qim |= cmd_q->int_ok | cmd_q->int_err;
 +
 +		dev_dbg(dev, "queue #%u available\n", i);
 +	}
 +	if (ccp->cmd_q_count == 0) {
 +		dev_notice(dev, "no command queues available\n");
 +		ret = -EIO;
 +		goto e_pool;
 +	}
 +	dev_notice(dev, "%u command queues available\n", ccp->cmd_q_count);
 +
 +	/* Disable and clear interrupts until ready */
 +	iowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		ioread32(cmd_q->reg_int_status);
 +		ioread32(cmd_q->reg_status);
 +	}
 +	iowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);
 +
 +	/* Request an irq */
 +	ret = ccp->get_irq(ccp);
 +	if (ret) {
 +		dev_err(dev, "unable to allocate an IRQ\n");
 +		goto e_pool;
 +	}
 +
 +	/* Initialize the queues used to wait for KSB space and suspend */
 +	init_waitqueue_head(&ccp->ksb_queue);
 +	init_waitqueue_head(&ccp->suspend_queue);
 +
 +	/* Create a kthread for each queue */
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		struct task_struct *kthread;
 +
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		kthread = kthread_create(ccp_cmd_queue_thread, cmd_q,
 +					 "ccp-q%u", cmd_q->id);
 +		if (IS_ERR(kthread)) {
 +			dev_err(dev, "error creating queue thread (%ld)\n",
 +				PTR_ERR(kthread));
 +			ret = PTR_ERR(kthread);
 +			goto e_kthread;
 +		}
 +
 +		cmd_q->kthread = kthread;
 +		wake_up_process(kthread);
 +	}
 +
 +	/* Register the RNG */
 +	ccp->hwrng.name = "ccp-rng";
 +	ccp->hwrng.read = ccp_trng_read;
 +	ret = hwrng_register(&ccp->hwrng);
 +	if (ret) {
 +		dev_err(dev, "error registering hwrng (%d)\n", ret);
 +		goto e_kthread;
 +	}
 +
 +	/* Make the device struct available before enabling interrupts */
 +	ccp_add_device(ccp);
 +
 +	/* Enable interrupts */
 +	iowrite32(qim, ccp->io_regs + IRQ_MASK_REG);
 +
 +	return 0;
 +
 +e_kthread:
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		if (ccp->cmd_q[i].kthread)
 +			kthread_stop(ccp->cmd_q[i].kthread);
 +
 +	ccp->free_irq(ccp);
 +
 +e_pool:
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		dma_pool_destroy(ccp->cmd_q[i].dma_pool);
 +
 +	return ret;
 +}
 +
 +/**
 + * ccp_destroy - tear down the CCP device
 + *
 + * @ccp: ccp_device struct
 + */
 +void ccp_destroy(struct ccp_device *ccp)
 +{
 +	struct ccp_cmd_queue *cmd_q;
 +	struct ccp_cmd *cmd;
 +	unsigned int qim, i;
 +
 +	/* Remove general access to the device struct */
 +	ccp_del_device(ccp);
 +
 +	/* Unregister the RNG */
 +	hwrng_unregister(&ccp->hwrng);
 +
 +	/* Stop the queue kthreads */
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		if (ccp->cmd_q[i].kthread)
 +			kthread_stop(ccp->cmd_q[i].kthread);
 +
 +	/* Build queue interrupt mask (two interrupt masks per queue) */
 +	qim = 0;
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +		qim |= cmd_q->int_ok | cmd_q->int_err;
 +	}
 +
 +	/* Disable and clear interrupts */
 +	iowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		ioread32(cmd_q->reg_int_status);
 +		ioread32(cmd_q->reg_status);
 +	}
 +	iowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);
 +
 +	ccp->free_irq(ccp);
 +
 +	for (i = 0; i < ccp->cmd_q_count; i++)
 +		dma_pool_destroy(ccp->cmd_q[i].dma_pool);
 +
 +	/* Flush the cmd and backlog queue */
 +	while (!list_empty(&ccp->cmd)) {
 +		/* Invoke the callback directly with an error code */
 +		cmd = list_first_entry(&ccp->cmd, struct ccp_cmd, entry);
 +		list_del(&cmd->entry);
 +		cmd->callback(cmd->data, -ENODEV);
 +	}
 +	while (!list_empty(&ccp->backlog)) {
 +		/* Invoke the callback directly with an error code */
 +		cmd = list_first_entry(&ccp->backlog, struct ccp_cmd, entry);
 +		list_del(&cmd->entry);
 +		cmd->callback(cmd->data, -ENODEV);
 +	}
 +}
 +
 +/**
 + * ccp_irq_handler - handle interrupts generated by the CCP device
 + *
 + * @irq: the irq associated with the interrupt
 + * @data: the data value supplied when the irq was created
 + */
 +irqreturn_t ccp_irq_handler(int irq, void *data)
 +{
 +	struct device *dev = data;
 +	struct ccp_device *ccp = dev_get_drvdata(dev);
 +	struct ccp_cmd_queue *cmd_q;
 +	u32 q_int, status;
 +	unsigned int i;
 +
 +	status = ioread32(ccp->io_regs + IRQ_STATUS_REG);
 +
 +	for (i = 0; i < ccp->cmd_q_count; i++) {
 +		cmd_q = &ccp->cmd_q[i];
 +
 +		q_int = status & (cmd_q->int_ok | cmd_q->int_err);
 +		if (q_int) {
 +			cmd_q->int_status = status;
 +			cmd_q->q_status = ioread32(cmd_q->reg_status);
 +			cmd_q->q_int_status = ioread32(cmd_q->reg_int_status);
 +
 +			/* On error, only save the first error value */
 +			if ((q_int & cmd_q->int_err) && !cmd_q->cmd_error)
 +				cmd_q->cmd_error = CMD_Q_ERROR(cmd_q->q_status);
 +
 +			cmd_q->int_rcvd = 1;
 +
 +			/* Acknowledge the interrupt and wake the kthread */
 +			iowrite32(q_int, ccp->io_regs + IRQ_STATUS_REG);
 +			wake_up_interruptible(&cmd_q->int_queue);
 +		}
 +	}
 +
 +	return IRQ_HANDLED;
++=======
+ int ccp_trng_read(struct hwrng *rng, void *data, size_t max, bool wait)
+ {
+ 	struct ccp_device *ccp = container_of(rng, struct ccp_device, hwrng);
+ 	u32 trng_value;
+ 	int len = min_t(int, sizeof(trng_value), max);
+ 
+ 	/* Locking is provided by the caller so we can update device
+ 	 * hwrng-related fields safely
+ 	 */
+ 	trng_value = ioread32(ccp->io_regs + TRNG_OUT_REG);
+ 	if (!trng_value) {
+ 		/* Zero is returned if not data is available or if a
+ 		 * bad-entropy error is present. Assume an error if
+ 		 * we exceed TRNG_RETRIES reads of zero.
+ 		 */
+ 		if (ccp->hwrng_retries++ > TRNG_RETRIES)
+ 			return -EIO;
+ 
+ 		return 0;
+ 	}
+ 
+ 	/* Reset the counter and save the rng value */
+ 	ccp->hwrng_retries = 0;
+ 	memcpy(data, &trng_value, len);
+ 
+ 	return len;
++>>>>>>> 8256e683113e (crypto: ccp - Refactor code supporting the CCP's RNG)
  }
  
  #ifdef CONFIG_PM
diff --cc drivers/crypto/ccp/ccp-dev.h
index 72bf1536b653,0c44c5e049f5..000000000000
--- a/drivers/crypto/ccp/ccp-dev.h
+++ b/drivers/crypto/ccp/ccp-dev.h
@@@ -259,12 -431,16 +259,17 @@@ struct ccp_device 
  int ccp_pci_init(void);
  void ccp_pci_exit(void);
  
 -int ccp_platform_init(void);
 -void ccp_platform_exit(void);
 -
 -void ccp_add_device(struct ccp_device *ccp);
 -void ccp_del_device(struct ccp_device *ccp);
 -
  struct ccp_device *ccp_alloc_struct(struct device *dev);
 +int ccp_init(struct ccp_device *ccp);
 +void ccp_destroy(struct ccp_device *ccp);
  bool ccp_queues_suspended(struct ccp_device *ccp);
++<<<<<<< HEAD
 +
 +irqreturn_t ccp_irq_handler(int irq, void *data);
++=======
+ int ccp_cmd_queue_thread(void *data);
+ int ccp_trng_read(struct hwrng *rng, void *data, size_t max, bool wait);
++>>>>>>> 8256e683113e (crypto: ccp - Refactor code supporting the CCP's RNG)
  
  int ccp_run_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd);
  
* Unmerged path drivers/crypto/ccp/ccp-dev-v3.c
* Unmerged path drivers/crypto/ccp/ccp-dev-v3.c
* Unmerged path drivers/crypto/ccp/ccp-dev.c
* Unmerged path drivers/crypto/ccp/ccp-dev.h
