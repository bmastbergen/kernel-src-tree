i40e: replace PTP Rx timestamp hang logic

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jacob Keller <jacob.e.keller@intel.com>
commit 124905012db8bdeebf5a7d1ddc841eaadda84a75
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/12490501.failed

The current Rx timestamp hang logic is not very robust because it does
not notice a register is hung until all four timestamps have been
latched and we wait a full 5 seconds. Replace this logic with a newer Rx
hang detection based on storing the jiffies when we first notice
a receive timestamp event. We store each register's time separately,
along with a flag indicating if it is currently latched. Upon first
transitioning to latch, we will update the latch_events[i] jiffies
value. This indicates the time we first noticed this event. The watchdog
routine will simply check that the either the flag has been cleared, or
we have passed at least one second. In this case, it is able to clear
the Rx timestamp register under the assumption that it was for a dropped
frame. The benefit if this strategy is that we should be able to
detect and clear out stalled RXTIME_H registers before we exhaust the
supply of 4, and avoid complete stall of Rx timestamp events.

Change-ID: Id55458c0cd7a5dd0c951ff2b8ac0b2509364131f
	Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 124905012db8bdeebf5a7d1ddc841eaadda84a75)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_txrx.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.c
index b7b5a5cf06f9,783ac4ed9c6d..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@@ -1476,13 -1392,345 +1476,345 @@@ static inline void i40e_rx_hash(struct 
  }
  
  /**
 - * i40e_process_skb_fields - Populate skb header fields from Rx descriptor
 - * @rx_ring: rx descriptor ring packet is being transacted on
 - * @rx_desc: pointer to the EOP Rx descriptor
 - * @skb: pointer to current skb being populated
 - * @rx_ptype: the packet type decoded by hardware
 + * i40e_clean_rx_irq_1buf - Reclaim resources after receive; single buffer
 + * @rx_ring:  rx ring to clean
 + * @budget:   how many cleans we're allowed
   *
 - * This function checks the ring, descriptor, and packet information in
 - * order to populate the hash, checksum, VLAN, protocol, and
 - * other fields within the skb.
 + * Returns number of packets cleaned
   **/
++<<<<<<< HEAD
 +static int i40e_clean_rx_irq_1buf(struct i40e_ring *rx_ring, int budget)
++=======
+ static inline
+ void i40e_process_skb_fields(struct i40e_ring *rx_ring,
+ 			     union i40e_rx_desc *rx_desc, struct sk_buff *skb,
+ 			     u8 rx_ptype)
+ {
+ 	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	u32 rx_status = (qword & I40E_RXD_QW1_STATUS_MASK) >>
+ 			I40E_RXD_QW1_STATUS_SHIFT;
+ 	u32 tsynvalid = rx_status & I40E_RXD_QW1_STATUS_TSYNVALID_MASK;
+ 	u32 tsyn = (rx_status & I40E_RXD_QW1_STATUS_TSYNINDX_MASK) >>
+ 		   I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT;
+ 
+ 	if (unlikely(tsynvalid))
+ 		i40e_ptp_rx_hwtstamp(rx_ring->vsi->back, skb, tsyn);
+ 
+ 	i40e_rx_hash(rx_ring, rx_desc, skb, rx_ptype);
+ 
+ 	/* modifies the skb - consumes the enet header */
+ 	skb->protocol = eth_type_trans(skb, rx_ring->netdev);
+ 
+ 	i40e_rx_checksum(rx_ring->vsi, skb, rx_desc);
+ 
+ 	skb_record_rx_queue(skb, rx_ring->queue_index);
+ }
+ 
+ /**
+  * i40e_pull_tail - i40e specific version of skb_pull_tail
+  * @rx_ring: rx descriptor ring packet is being transacted on
+  * @skb: pointer to current skb being adjusted
+  *
+  * This function is an i40e specific version of __pskb_pull_tail.  The
+  * main difference between this version and the original function is that
+  * this function can make several assumptions about the state of things
+  * that allow for significant optimizations versus the standard function.
+  * As a result we can do things like drop a frag and maintain an accurate
+  * truesize for the skb.
+  */
+ static void i40e_pull_tail(struct i40e_ring *rx_ring, struct sk_buff *skb)
+ {
+ 	struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+ 	unsigned char *va;
+ 	unsigned int pull_len;
+ 
+ 	/* it is valid to use page_address instead of kmap since we are
+ 	 * working with pages allocated out of the lomem pool per
+ 	 * alloc_page(GFP_ATOMIC)
+ 	 */
+ 	va = skb_frag_address(frag);
+ 
+ 	/* we need the header to contain the greater of either ETH_HLEN or
+ 	 * 60 bytes if the skb->len is less than 60 for skb_pad.
+ 	 */
+ 	pull_len = eth_get_headlen(va, I40E_RX_HDR_SIZE);
+ 
+ 	/* align pull length to size of long to optimize memcpy performance */
+ 	skb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));
+ 
+ 	/* update all of the pointers */
+ 	skb_frag_size_sub(frag, pull_len);
+ 	frag->page_offset += pull_len;
+ 	skb->data_len -= pull_len;
+ 	skb->tail += pull_len;
+ }
+ 
+ /**
+  * i40e_cleanup_headers - Correct empty headers
+  * @rx_ring: rx descriptor ring packet is being transacted on
+  * @skb: pointer to current skb being fixed
+  *
+  * Also address the case where we are pulling data in on pages only
+  * and as such no data is present in the skb header.
+  *
+  * In addition if skb is not at least 60 bytes we need to pad it so that
+  * it is large enough to qualify as a valid Ethernet frame.
+  *
+  * Returns true if an error was encountered and skb was freed.
+  **/
+ static bool i40e_cleanup_headers(struct i40e_ring *rx_ring, struct sk_buff *skb)
+ {
+ 	/* place header in linear portion of buffer */
+ 	if (skb_is_nonlinear(skb))
+ 		i40e_pull_tail(rx_ring, skb);
+ 
+ 	/* if eth_skb_pad returns an error the skb was freed */
+ 	if (eth_skb_pad(skb))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /**
+  * i40e_reuse_rx_page - page flip buffer and store it back on the ring
+  * @rx_ring: rx descriptor ring to store buffers on
+  * @old_buff: donor buffer to have page reused
+  *
+  * Synchronizes page for reuse by the adapter
+  **/
+ static void i40e_reuse_rx_page(struct i40e_ring *rx_ring,
+ 			       struct i40e_rx_buffer *old_buff)
+ {
+ 	struct i40e_rx_buffer *new_buff;
+ 	u16 nta = rx_ring->next_to_alloc;
+ 
+ 	new_buff = &rx_ring->rx_bi[nta];
+ 
+ 	/* update, and store next to alloc */
+ 	nta++;
+ 	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
+ 
+ 	/* transfer page from old buffer to new buffer */
+ 	*new_buff = *old_buff;
+ }
+ 
+ /**
+  * i40e_page_is_reserved - check if reuse is possible
+  * @page: page struct to check
+  */
+ static inline bool i40e_page_is_reserved(struct page *page)
+ {
+ 	return (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);
+ }
+ 
+ /**
+  * i40e_add_rx_frag - Add contents of Rx buffer to sk_buff
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_buffer: buffer containing page to add
+  * @rx_desc: descriptor containing length of buffer written by hardware
+  * @skb: sk_buff to place the data into
+  *
+  * This function will add the data contained in rx_buffer->page to the skb.
+  * This is done either through a direct copy if the data in the buffer is
+  * less than the skb header size, otherwise it will just attach the page as
+  * a frag to the skb.
+  *
+  * The function will then update the page offset if necessary and return
+  * true if the buffer can be reused by the adapter.
+  **/
+ static bool i40e_add_rx_frag(struct i40e_ring *rx_ring,
+ 			     struct i40e_rx_buffer *rx_buffer,
+ 			     union i40e_rx_desc *rx_desc,
+ 			     struct sk_buff *skb)
+ {
+ 	struct page *page = rx_buffer->page;
+ 	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	unsigned int size = (qword & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
+ 			    I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
+ #if (PAGE_SIZE < 8192)
+ 	unsigned int truesize = I40E_RXBUFFER_2048;
+ #else
+ 	unsigned int truesize = ALIGN(size, L1_CACHE_BYTES);
+ 	unsigned int last_offset = PAGE_SIZE - I40E_RXBUFFER_2048;
+ #endif
+ 
+ 	/* will the data fit in the skb we allocated? if so, just
+ 	 * copy it as it is pretty small anyway
+ 	 */
+ 	if ((size <= I40E_RX_HDR_SIZE) && !skb_is_nonlinear(skb)) {
+ 		unsigned char *va = page_address(page) + rx_buffer->page_offset;
+ 
+ 		memcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));
+ 
+ 		/* page is not reserved, we can reuse buffer as-is */
+ 		if (likely(!i40e_page_is_reserved(page)))
+ 			return true;
+ 
+ 		/* this page cannot be reused so discard it */
+ 		__free_pages(page, 0);
+ 		return false;
+ 	}
+ 
+ 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
+ 			rx_buffer->page_offset, size, truesize);
+ 
+ 	/* avoid re-using remote pages */
+ 	if (unlikely(i40e_page_is_reserved(page)))
+ 		return false;
+ 
+ #if (PAGE_SIZE < 8192)
+ 	/* if we are only owner of page we can reuse it */
+ 	if (unlikely(page_count(page) != 1))
+ 		return false;
+ 
+ 	/* flip page offset to other buffer */
+ 	rx_buffer->page_offset ^= truesize;
+ #else
+ 	/* move offset up to the next cache line */
+ 	rx_buffer->page_offset += truesize;
+ 
+ 	if (rx_buffer->page_offset > last_offset)
+ 		return false;
+ #endif
+ 
+ 	/* Even if we own the page, we are not allowed to use atomic_set()
+ 	 * This would break get_page_unless_zero() users.
+ 	 */
+ 	get_page(rx_buffer->page);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_fetch_rx_buffer - Allocate skb and populate it
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_desc: descriptor containing info written by hardware
+  *
+  * This function allocates an skb on the fly, and populates it with the page
+  * data from the current receive descriptor, taking care to set up the skb
+  * correctly, as well as handling calling the page recycle function if
+  * necessary.
+  */
+ static inline
+ struct sk_buff *i40e_fetch_rx_buffer(struct i40e_ring *rx_ring,
+ 				     union i40e_rx_desc *rx_desc)
+ {
+ 	struct i40e_rx_buffer *rx_buffer;
+ 	struct sk_buff *skb;
+ 	struct page *page;
+ 
+ 	rx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];
+ 	page = rx_buffer->page;
+ 	prefetchw(page);
+ 
+ 	skb = rx_buffer->skb;
+ 
+ 	if (likely(!skb)) {
+ 		void *page_addr = page_address(page) + rx_buffer->page_offset;
+ 
+ 		/* prefetch first cache line of first page */
+ 		prefetch(page_addr);
+ #if L1_CACHE_BYTES < 128
+ 		prefetch(page_addr + L1_CACHE_BYTES);
+ #endif
+ 
+ 		/* allocate a skb to store the frags */
+ 		skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
+ 				       I40E_RX_HDR_SIZE,
+ 				       GFP_ATOMIC | __GFP_NOWARN);
+ 		if (unlikely(!skb)) {
+ 			rx_ring->rx_stats.alloc_buff_failed++;
+ 			return NULL;
+ 		}
+ 
+ 		/* we will be copying header into skb->data in
+ 		 * pskb_may_pull so it is in our interest to prefetch
+ 		 * it now to avoid a possible cache miss
+ 		 */
+ 		prefetchw(skb->data);
+ 	} else {
+ 		rx_buffer->skb = NULL;
+ 	}
+ 
+ 	/* we are reusing so sync this buffer for CPU use */
+ 	dma_sync_single_range_for_cpu(rx_ring->dev,
+ 				      rx_buffer->dma,
+ 				      rx_buffer->page_offset,
+ 				      I40E_RXBUFFER_2048,
+ 				      DMA_FROM_DEVICE);
+ 
+ 	/* pull page into skb */
+ 	if (i40e_add_rx_frag(rx_ring, rx_buffer, rx_desc, skb)) {
+ 		/* hand second half of page back to the ring */
+ 		i40e_reuse_rx_page(rx_ring, rx_buffer);
+ 		rx_ring->rx_stats.page_reuse_count++;
+ 	} else {
+ 		/* we are not reusing the buffer so unmap it */
+ 		dma_unmap_page(rx_ring->dev, rx_buffer->dma, PAGE_SIZE,
+ 			       DMA_FROM_DEVICE);
+ 	}
+ 
+ 	/* clear contents of buffer_info */
+ 	rx_buffer->page = NULL;
+ 
+ 	return skb;
+ }
+ 
+ /**
+  * i40e_is_non_eop - process handling of non-EOP buffers
+  * @rx_ring: Rx ring being processed
+  * @rx_desc: Rx descriptor for current buffer
+  * @skb: Current socket buffer containing buffer in progress
+  *
+  * This function updates next to clean.  If the buffer is an EOP buffer
+  * this function exits returning false, otherwise it will place the
+  * sk_buff in the next buffer to be chained and return true indicating
+  * that this is in fact a non-EOP buffer.
+  **/
+ static bool i40e_is_non_eop(struct i40e_ring *rx_ring,
+ 			    union i40e_rx_desc *rx_desc,
+ 			    struct sk_buff *skb)
+ {
+ 	u32 ntc = rx_ring->next_to_clean + 1;
+ 
+ 	/* fetch, update, and store next to clean */
+ 	ntc = (ntc < rx_ring->count) ? ntc : 0;
+ 	rx_ring->next_to_clean = ntc;
+ 
+ 	prefetch(I40E_RX_DESC(rx_ring, ntc));
+ 
+ #define staterrlen rx_desc->wb.qword1.status_error_len
+ 	if (unlikely(i40e_rx_is_programming_status(le64_to_cpu(staterrlen)))) {
+ 		i40e_clean_programming_status(rx_ring, rx_desc);
+ 		rx_ring->rx_bi[ntc].skb = skb;
+ 		return true;
+ 	}
+ 	/* if we are the last buffer then there is nothing else to do */
+ #define I40E_RXD_EOF BIT(I40E_RX_DESC_STATUS_EOF_SHIFT)
+ 	if (likely(i40e_test_staterr(rx_desc, I40E_RXD_EOF)))
+ 		return false;
+ 
+ 	/* place skb in next buffer to be received */
+ 	rx_ring->rx_bi[ntc].skb = skb;
+ 	rx_ring->rx_stats.non_eop_descs++;
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_clean_rx_irq - Clean completed descriptors from Rx ring - bounce buf
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @budget: Total limit on number of packets to process
+  *
+  * This function provides a "bounce buffer" approach to Rx interrupt
+  * processing.  The advantage to this is that on systems that have
+  * expensive overhead for IOMMU access this provides a means of avoiding
+  * it by maintaining the mapping of the page to the system.
+  *
+  * Returns amount of work completed
+  **/
+ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
++>>>>>>> 124905012db8 (i40e: replace PTP Rx timestamp hang logic)
  {
  	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
  	u16 cleaned_count = I40E_DESC_UNUSED(rx_ring);
diff --git a/drivers/net/ethernet/intel/i40e/i40e.h b/drivers/net/ethernet/intel/i40e/i40e.h
index 0dbd4e8e12bd..372a975f498f 100644
--- a/drivers/net/ethernet/intel/i40e/i40e.h
+++ b/drivers/net/ethernet/intel/i40e/i40e.h
@@ -431,11 +431,13 @@ struct i40e_pf {
 	struct ptp_clock_info ptp_caps;
 	struct sk_buff *ptp_tx_skb;
 	struct hwtstamp_config tstamp_config;
-	unsigned long last_rx_ptp_check;
 	struct mutex tmreg_lock; /* Used to protect the SYSTIME registers. */
 	u64 ptp_base_adj;
 	u32 tx_hwtstamp_timeouts;
 	u32 rx_hwtstamp_cleared;
+	u32 latch_event_flags;
+	spinlock_t ptp_rx_lock; /* Used to protect Rx timestamp registers. */
+	unsigned long latch_events[4];
 	bool ptp_tx;
 	bool ptp_rx;
 	u16 rss_table_size; /* HW RSS table size */
diff --git a/drivers/net/ethernet/intel/i40e/i40e_ptp.c b/drivers/net/ethernet/intel/i40e/i40e_ptp.c
index 10e81b5fcb79..efca967bb3c9 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_ptp.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_ptp.c
@@ -226,6 +226,47 @@ static int i40e_ptp_feature_enable(struct ptp_clock_info *ptp,
 	return -EOPNOTSUPP;
 }
 
+/**
+ * i40e_ptp_update_latch_events - Read I40E_PRTTSYN_STAT_1 and latch events
+ * @pf: the PF data structure
+ *
+ * This function reads I40E_PRTTSYN_STAT_1 and updates the corresponding timers
+ * for noticed latch events. This allows the driver to keep track of the first
+ * time a latch event was noticed which will be used to help clear out Rx
+ * timestamps for packets that got dropped or lost.
+ *
+ * This function will return the current value of I40E_PRTTSYN_STAT_1 and is
+ * expected to be called only while under the ptp_rx_lock.
+ **/
+static u32 i40e_ptp_get_rx_events(struct i40e_pf *pf)
+{
+	struct i40e_hw *hw = &pf->hw;
+	u32 prttsyn_stat, new_latch_events;
+	int  i;
+
+	prttsyn_stat = rd32(hw, I40E_PRTTSYN_STAT_1);
+	new_latch_events = prttsyn_stat & ~pf->latch_event_flags;
+
+	/* Update the jiffies time for any newly latched timestamp. This
+	 * ensures that we store the time that we first discovered a timestamp
+	 * was latched by the hardware. The service task will later determine
+	 * if we should free the latch and drop that timestamp should too much
+	 * time pass. This flow ensures that we only update jiffies for new
+	 * events latched since the last time we checked, and not all events
+	 * currently latched, so that the service task accounting remains
+	 * accurate.
+	 */
+	for (i = 0; i < 4; i++) {
+		if (new_latch_events & BIT(i))
+			pf->latch_events[i] = jiffies;
+	}
+
+	/* Finally, we store the current status of the Rx timestamp latches */
+	pf->latch_event_flags = prttsyn_stat;
+
+	return prttsyn_stat;
+}
+
 /**
  * i40e_ptp_rx_hang - Detect error case when Rx timestamp registers are hung
  * @vsi: The VSI with the rings relevant to 1588
@@ -239,10 +280,7 @@ void i40e_ptp_rx_hang(struct i40e_vsi *vsi)
 {
 	struct i40e_pf *pf = vsi->back;
 	struct i40e_hw *hw = &pf->hw;
-	struct i40e_ring *rx_ring;
-	unsigned long rx_event;
-	u32 prttsyn_stat;
-	int n;
+	int i;
 
 	/* Since we cannot turn off the Rx timestamp logic if the device is
 	 * configured for Tx timestamping, we check if Rx timestamping is
@@ -252,42 +290,30 @@ void i40e_ptp_rx_hang(struct i40e_vsi *vsi)
 	if (!(pf->flags & I40E_FLAG_PTP) || !pf->ptp_rx)
 		return;
 
-	prttsyn_stat = rd32(hw, I40E_PRTTSYN_STAT_1);
+	spin_lock_bh(&pf->ptp_rx_lock);
 
-	/* Unless all four receive timestamp registers are latched, we are not
-	 * concerned about a possible PTP Rx hang, so just update the timeout
-	 * counter and exit.
-	 */
-	if (!(prttsyn_stat & ((I40E_PRTTSYN_STAT_1_RXT0_MASK <<
-			       I40E_PRTTSYN_STAT_1_RXT0_SHIFT) |
-			      (I40E_PRTTSYN_STAT_1_RXT1_MASK <<
-			       I40E_PRTTSYN_STAT_1_RXT1_SHIFT) |
-			      (I40E_PRTTSYN_STAT_1_RXT2_MASK <<
-			       I40E_PRTTSYN_STAT_1_RXT2_SHIFT) |
-			      (I40E_PRTTSYN_STAT_1_RXT3_MASK <<
-			       I40E_PRTTSYN_STAT_1_RXT3_SHIFT)))) {
-		pf->last_rx_ptp_check = jiffies;
-		return;
-	}
+	/* Update current latch times for Rx events */
+	i40e_ptp_get_rx_events(pf);
 
-	/* Determine the most recent watchdog or rx_timestamp event. */
-	rx_event = pf->last_rx_ptp_check;
-	for (n = 0; n < vsi->num_queue_pairs; n++) {
-		rx_ring = vsi->rx_rings[n];
-		if (time_after(rx_ring->last_rx_timestamp, rx_event))
-			rx_event = rx_ring->last_rx_timestamp;
+	/* Check all the currently latched Rx events and see whether they have
+	 * been latched for over a second. It is assumed that any timestamp
+	 * should have been cleared within this time, or else it was captured
+	 * for a dropped frame that the driver never received. Thus, we will
+	 * clear any timestamp that has been latched for over 1 second.
+	 */
+	for (i = 0; i < 4; i++) {
+		if ((pf->latch_event_flags & BIT(i)) &&
+		    time_is_before_jiffies(pf->latch_events[i] + HZ)) {
+			rd32(hw, I40E_PRTTSYN_RXTIME_H(i));
+			pf->latch_event_flags &= ~BIT(i);
+			pf->rx_hwtstamp_cleared++;
+			dev_warn(&pf->pdev->dev,
+				 "Clearing a missed Rx timestamp event for RXTIME[%d]\n",
+				 i);
+		}
 	}
 
-	/* Only need to read the high RXSTMP register to clear the lock */
-	if (time_is_before_jiffies(rx_event + 5 * HZ)) {
-		rd32(hw, I40E_PRTTSYN_RXTIME_H(0));
-		rd32(hw, I40E_PRTTSYN_RXTIME_H(1));
-		rd32(hw, I40E_PRTTSYN_RXTIME_H(2));
-		rd32(hw, I40E_PRTTSYN_RXTIME_H(3));
-		pf->last_rx_ptp_check = jiffies;
-		pf->rx_hwtstamp_cleared++;
-		WARN_ONCE(1, "Detected Rx timestamp register hang\n");
-	}
+	spin_unlock_bh(&pf->ptp_rx_lock);
 }
 
 /**
@@ -350,14 +376,25 @@ void i40e_ptp_rx_hwtstamp(struct i40e_pf *pf, struct sk_buff *skb, u8 index)
 
 	hw = &pf->hw;
 
-	prttsyn_stat = rd32(hw, I40E_PRTTSYN_STAT_1);
+	spin_lock_bh(&pf->ptp_rx_lock);
+
+	/* Get current Rx events and update latch times */
+	prttsyn_stat = i40e_ptp_get_rx_events(pf);
 
-	if (!(prttsyn_stat & BIT(index)))
+	/* TODO: Should we warn about missing Rx timestamp event? */
+	if (!(prttsyn_stat & BIT(index))) {
+		spin_unlock_bh(&pf->ptp_rx_lock);
 		return;
+	}
+
+	/* Clear the latched event since we're about to read its register */
+	pf->latch_event_flags &= ~BIT(index);
 
 	lo = rd32(hw, I40E_PRTTSYN_RXTIME_L(index));
 	hi = rd32(hw, I40E_PRTTSYN_RXTIME_H(index));
 
+	spin_unlock_bh(&pf->ptp_rx_lock);
+
 	ns = (((u64)hi) << 32) | lo;
 
 	i40e_ptp_convert_to_hwtstamp(skb_hwtstamps(skb), ns);
@@ -511,12 +548,15 @@ static int i40e_ptp_set_timestamp_mode(struct i40e_pf *pf,
 	}
 
 	/* Clear out all 1588-related registers to clear and unlatch them. */
+	spin_lock_bh(&pf->ptp_rx_lock);
 	rd32(hw, I40E_PRTTSYN_STAT_0);
 	rd32(hw, I40E_PRTTSYN_TXTIME_H);
 	rd32(hw, I40E_PRTTSYN_RXTIME_H(0));
 	rd32(hw, I40E_PRTTSYN_RXTIME_H(1));
 	rd32(hw, I40E_PRTTSYN_RXTIME_H(2));
 	rd32(hw, I40E_PRTTSYN_RXTIME_H(3));
+	pf->latch_event_flags = 0;
+	spin_unlock_bh(&pf->ptp_rx_lock);
 
 	/* Enable/disable the Tx timestamp interrupt based on user input. */
 	regval = rd32(hw, I40E_PRTTSYN_CTL0);
@@ -656,6 +696,7 @@ void i40e_ptp_init(struct i40e_pf *pf)
 	}
 
 	mutex_init(&pf->tmreg_lock);
+	spin_lock_init(&pf->ptp_rx_lock);
 
 	/* ensure we have a clock device */
 	err = i40e_ptp_create_clock(pf);
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.h b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
index 5a2d0fef4d37..19924b4a50f4 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
@@ -298,8 +298,6 @@ struct i40e_ring {
 	u8 atr_sample_rate;
 	u8 atr_count;
 
-	unsigned long last_rx_timestamp;
-
 	bool ring_active;		/* is ring online or not */
 	bool arm_wb;		/* do something to arm write back */
 	u8 packet_stride;
