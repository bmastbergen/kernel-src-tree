scsi: megaraid_sas: big endian support changes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [scsi] megaraid_sas: big endian support changes (Tomas Henzl) [1417038]
Rebuild_FUZZ: 93.02%
commit-author Shivasharan S <shivasharan.srikanteshwara@broadcom.com>
commit a174118b7a97c52c3c3a4f1b8eee594502a55381
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/a174118b.failed

Fix endiannes fixes for Ventura specific.

	Signed-off-by: Shivasharan S <shivasharan.srikanteshwara@broadcom.com>
	Signed-off-by: Kashyap Desai <kashyap.desai@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Reviewed-by: Tomas Henzl <thenzl@redhat.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit a174118b7a97c52c3c3a4f1b8eee594502a55381)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/megaraid/megaraid_sas_fp.c
#	drivers/scsi/megaraid/megaraid_sas_fusion.c
diff --cc drivers/scsi/megaraid/megaraid_sas_fp.c
index 53238db53032,68582d9f9e45..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas_fp.c
+++ b/drivers/scsi/megaraid/megaraid_sas_fp.c
@@@ -186,11 -194,195 +186,202 @@@ void MR_PopulateDrvRaidMap(struct megas
  	struct MR_DRV_RAID_MAP_ALL *drv_map =
  			fusion->ld_drv_map[(instance->map_id & 1)];
  	struct MR_DRV_RAID_MAP *pDrvRaidMap = &drv_map->raidMap;
++<<<<<<< HEAD
++=======
+ 	void *raid_map_data = NULL;
+ 
+ 	memset(drv_map, 0, fusion->drv_map_sz);
+ 	memset(pDrvRaidMap->ldTgtIdToLd,
+ 		0xff, (sizeof(u16) * MAX_LOGICAL_DRIVES_DYN));
+ 
+ 	if (instance->max_raid_mapsize) {
+ 		fw_map_dyn = fusion->ld_map[(instance->map_id & 1)];
+ #if VD_EXT_DEBUG
+ 		dev_dbg(&instance->pdev->dev, "raidMapSize 0x%x fw_map_dyn->descTableOffset 0x%x\n",
+ 			le32_to_cpu(fw_map_dyn->raid_map_size),
+ 			le32_to_cpu(fw_map_dyn->desc_table_offset));
+ 		dev_dbg(&instance->pdev->dev, "descTableSize 0x%x descTableNumElements 0x%x\n",
+ 			le32_to_cpu(fw_map_dyn->desc_table_size),
+ 			le32_to_cpu(fw_map_dyn->desc_table_num_elements));
+ 		dev_dbg(&instance->pdev->dev, "drv map %p ldCount %d\n",
+ 			drv_map, le16_to_cpu(fw_map_dyn->ld_count));
+ #endif
+ 		desc_table =
+ 		(struct MR_RAID_MAP_DESC_TABLE *)((void *)fw_map_dyn + le32_to_cpu(fw_map_dyn->desc_table_offset));
+ 		if (desc_table != fw_map_dyn->raid_map_desc_table)
+ 			dev_dbg(&instance->pdev->dev, "offsets of desc table are not matching desc %p original %p\n",
+ 				desc_table, fw_map_dyn->raid_map_desc_table);
+ 
+ 		ld_count = (u16)le16_to_cpu(fw_map_dyn->ld_count);
+ 		pDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);
+ 		pDrvRaidMap->fpPdIoTimeoutSec =
+ 			fw_map_dyn->fp_pd_io_timeout_sec;
+ 		pDrvRaidMap->totalSize =
+ 			cpu_to_le32(sizeof(struct MR_DRV_RAID_MAP_ALL));
+ 		/* point to actual data starting point*/
+ 		raid_map_data = (void *)fw_map_dyn +
+ 			le32_to_cpu(fw_map_dyn->desc_table_offset) +
+ 			le32_to_cpu(fw_map_dyn->desc_table_size);
+ 
+ 		for (i = 0; i < le32_to_cpu(fw_map_dyn->desc_table_num_elements); ++i) {
+ 
+ #if VD_EXT_DEBUG
+ 			dev_dbg(&instance->pdev->dev, "desc table %p\n",
+ 				desc_table);
+ 			dev_dbg(&instance->pdev->dev, "raidmap type %d, raidmapOffset 0x%x\n",
+ 				le32_to_cpu(desc_table->raid_map_desc_type),
+ 				le32_to_cpu(desc_table->raid_map_desc_offset));
+ 			dev_dbg(&instance->pdev->dev, "raid map number of elements 0%x, raidmapsize 0x%x\n",
+ 				le32_to_cpu(desc_table->raid_map_desc_elements),
+ 				le32_to_cpu(desc_table->raid_map_desc_buffer_size));
+ #endif
+ 			switch (le32_to_cpu(desc_table->raid_map_desc_type)) {
+ 			case RAID_MAP_DESC_TYPE_DEVHDL_INFO:
+ 				fw_map_dyn->dev_hndl_info =
+ 				(struct MR_DEV_HANDLE_INFO *)(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "devHndlInfo  address %p\n",
+ 					fw_map_dyn->dev_hndl_info);
+ #endif
+ 				memcpy(pDrvRaidMap->devHndlInfo,
+ 				fw_map_dyn->dev_hndl_info,
+ 				sizeof(struct MR_DEV_HANDLE_INFO) *
+ 				le32_to_cpu(desc_table->raid_map_desc_elements));
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_TGTID_INFO:
+ 				fw_map_dyn->ld_tgt_id_to_ld =
+ 				(u16 *) (raid_map_data +
+ 				le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 			dev_dbg(&instance->pdev->dev, "ldTgtIdToLd  address %p\n",
+ 				fw_map_dyn->ld_tgt_id_to_ld);
+ #endif
+ 			for (j = 0; j < le32_to_cpu(desc_table->raid_map_desc_elements); j++) {
+ 				pDrvRaidMap->ldTgtIdToLd[j] =
+ 					le16_to_cpu(fw_map_dyn->ld_tgt_id_to_ld[j]);
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, " %d drv ldTgtIdToLd %d\n",
+ 					j, pDrvRaidMap->ldTgtIdToLd[j]);
+ #endif
+ 			}
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_ARRAY_INFO:
+ 				fw_map_dyn->ar_map_info =
+ 				(struct MR_ARRAY_INFO *)
+ 				(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "arMapInfo  address %p\n",
+ 					fw_map_dyn->ar_map_info);
+ #endif
+ 
+ 				memcpy(pDrvRaidMap->arMapInfo,
+ 				fw_map_dyn->ar_map_info,
+ 				sizeof(struct MR_ARRAY_INFO) * le32_to_cpu(desc_table->raid_map_desc_elements));
+ 			break;
+ 			case RAID_MAP_DESC_TYPE_SPAN_INFO:
+ 				fw_map_dyn->ld_span_map =
+ 				(struct MR_LD_SPAN_MAP *)
+ 				(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));
+ 				memcpy(pDrvRaidMap->ldSpanMap,
+ 				fw_map_dyn->ld_span_map,
+ 				sizeof(struct MR_LD_SPAN_MAP) * le32_to_cpu(desc_table->raid_map_desc_elements));
+ #if VD_EXT_DEBUG
+ 				dev_dbg(&instance->pdev->dev, "ldSpanMap  address %p\n",
+ 					fw_map_dyn->ld_span_map);
+ 				dev_dbg(&instance->pdev->dev, "MR_LD_SPAN_MAP size 0x%lx\n",
+ 					sizeof(struct MR_LD_SPAN_MAP));
+ 				for (j = 0; j < ld_count; j++) {
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : fw_map_dyn->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 					j, j, fw_map_dyn->ld_span_map[j].ldRaid.targetId);
+ 					dev_dbg(&instance->pdev->dev, "fw_map_dyn->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 					j, fw_map_dyn->ld_span_map[j].ldRaid.seqNum);
+ 					dev_dbg(&instance->pdev->dev, "fw_map_dyn->ld_span_map[%d].ldRaid.rowSize 0x%x\n",
+ 					j, (u32)fw_map_dyn->ld_span_map[j].ldRaid.rowSize);
+ 
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) :pDrvRaidMap->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 					j, j, pDrvRaidMap->ldSpanMap[j].ldRaid.targetId);
+ 					dev_dbg(&instance->pdev->dev, "DrvRaidMap->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 					j, pDrvRaidMap->ldSpanMap[j].ldRaid.seqNum);
+ 					dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 					j, (u32)pDrvRaidMap->ldSpanMap[j].ldRaid.rowSize);
+ 
+ 					dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : drv raid map all %p\n",
+ 					instance->unique_id, drv_map);
+ 					dev_dbg(&instance->pdev->dev, "raid map %p LD RAID MAP %p/%p\n",
+ 					pDrvRaidMap,
+ 					&fw_map_dyn->ld_span_map[j].ldRaid,
+ 					&pDrvRaidMap->ldSpanMap[j].ldRaid);
+ 				}
+ #endif
+ 			break;
+ 			default:
+ 				dev_dbg(&instance->pdev->dev, "wrong number of desctableElements %d\n",
+ 					fw_map_dyn->desc_table_num_elements);
+ 			}
+ 			++desc_table;
+ 		}
+ 
+ 	} else if (instance->supportmax256vd) {
+ 		fw_map_ext =
+ 		(struct MR_FW_RAID_MAP_EXT *) fusion->ld_map[(instance->map_id & 1)];
+ 		ld_count = (u16)le16_to_cpu(fw_map_ext->ldCount);
+ 		if (ld_count > MAX_LOGICAL_DRIVES_EXT) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas: LD count exposed in RAID map in not valid\n");
+ 			return;
+ 		}
+ #if VD_EXT_DEBUG
+ 		for (i = 0; i < ld_count; i++) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) :Index 0x%x\n",
+ 				instance->unique_id, i);
+ 			dev_dbg(&instance->pdev->dev, "Target Id 0x%x\n",
+ 				fw_map_ext->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "Seq Num 0x%x Size 0/%llx\n",
+ 				fw_map_ext->ldSpanMap[i].ldRaid.seqNum,
+ 				fw_map_ext->ldSpanMap[i].ldRaid.size);
+ 		}
+ #endif
+ 
+ 		pDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);
+ 		pDrvRaidMap->fpPdIoTimeoutSec = fw_map_ext->fpPdIoTimeoutSec;
+ 		for (i = 0; i < (MAX_LOGICAL_DRIVES_EXT); i++)
+ 			pDrvRaidMap->ldTgtIdToLd[i] =
+ 				(u16)fw_map_ext->ldTgtIdToLd[i];
+ 		memcpy(pDrvRaidMap->ldSpanMap, fw_map_ext->ldSpanMap,
+ 				sizeof(struct MR_LD_SPAN_MAP) * ld_count);
+ #if VD_EXT_DEBUG
+ 		for (i = 0; i < ld_count; i++) {
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : fw_map_ext->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 			i, i, fw_map_ext->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "fw_map_ext->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 			i, fw_map_ext->ldSpanMap[i].ldRaid.seqNum);
+ 			dev_dbg(&instance->pdev->dev, "fw_map_ext->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 			i, (u32)fw_map_ext->ldSpanMap[i].ldRaid.rowSize);
+ 
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : pDrvRaidMap->ldSpanMap[%d].ldRaid.targetId 0x%x\n",
+ 			i, i, pDrvRaidMap->ldSpanMap[i].ldRaid.targetId);
+ 			dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.seqNum 0x%x\n",
+ 			i, pDrvRaidMap->ldSpanMap[i].ldRaid.seqNum);
+ 			dev_dbg(&instance->pdev->dev, "pDrvRaidMap->ldSpanMap[%d].ldRaid.rowSize 0x%x\n",
+ 			i, (u32)pDrvRaidMap->ldSpanMap[i].ldRaid.rowSize);
+ 
+ 			dev_dbg(&instance->pdev->dev, "megaraid_sas(%d) : drv raid map all %p\n",
+ 			instance->unique_id, drv_map);
+ 			dev_dbg(&instance->pdev->dev, "raid map %p LD RAID MAP %p %p\n",
+ 			pDrvRaidMap, &fw_map_ext->ldSpanMap[i].ldRaid,
+ 			&pDrvRaidMap->ldSpanMap[i].ldRaid);
+ 		}
+ #endif
+ 		memcpy(pDrvRaidMap->arMapInfo, fw_map_ext->arMapInfo,
+ 			sizeof(struct MR_ARRAY_INFO) * MAX_API_ARRAYS_EXT);
+ 		memcpy(pDrvRaidMap->devHndlInfo, fw_map_ext->devHndlInfo,
+ 			sizeof(struct MR_DEV_HANDLE_INFO) *
+ 					MAX_RAIDMAP_PHYSICAL_DEVICES);
++>>>>>>> a174118b7a97 (scsi: megaraid_sas: big endian support changes)
  
 +	if (instance->supportmax256vd) {
 +		memcpy(fusion->ld_drv_map[instance->map_id & 1],
 +			fusion->ld_map[instance->map_id & 1],
 +			fusion->current_map_sz);
  		/* New Raid map will not set totalSize, so keep expected value
  		 * for legacy code in ValidateMapInfo
  		 */
diff --cc drivers/scsi/megaraid/megaraid_sas_fusion.c
index 5a7b77a31c50,baea4c2137d7..000000000000
--- a/drivers/scsi/megaraid/megaraid_sas_fusion.c
+++ b/drivers/scsi/megaraid/megaraid_sas_fusion.c
@@@ -1840,6 -2119,86 +1840,89 @@@ static void megasas_stream_detect(struc
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * megasas_set_raidflag_cpu_affinity - This function sets the cpu
+  * affinity (cpu of the controller) and raid_flags in the raid context
+  * based on IO type.
+  *
+  * @praid_context:	IO RAID context
+  * @raid:		LD raid map
+  * @fp_possible:	Is fast path possible?
+  * @is_read:		Is read IO?
+  *
+  */
+ static void
+ megasas_set_raidflag_cpu_affinity(union RAID_CONTEXT_UNION *praid_context,
+ 				  struct MR_LD_RAID *raid, bool fp_possible,
+ 				  u8 is_read, u32 scsi_buff_len)
+ {
+ 	u8 cpu_sel = MR_RAID_CTX_CPUSEL_0;
+ 	struct RAID_CONTEXT_G35 *rctx_g35;
+ 
+ 	rctx_g35 = &praid_context->raid_context_g35;
+ 	if (fp_possible) {
+ 		if (is_read) {
+ 			if ((raid->cpuAffinity.pdRead.cpu0) &&
+ 			    (raid->cpuAffinity.pdRead.cpu1))
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_FCFS;
+ 			else if (raid->cpuAffinity.pdRead.cpu1)
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_1;
+ 		} else {
+ 			if ((raid->cpuAffinity.pdWrite.cpu0) &&
+ 			    (raid->cpuAffinity.pdWrite.cpu1))
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_FCFS;
+ 			else if (raid->cpuAffinity.pdWrite.cpu1)
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_1;
+ 			/* Fast path cache by pass capable R0/R1 VD */
+ 			if ((raid->level <= 1) &&
+ 			    (raid->capability.fp_cache_bypass_capable)) {
+ 				rctx_g35->routing_flags |=
+ 					(1 << MR_RAID_CTX_ROUTINGFLAGS_SLD_SHIFT);
+ 				rctx_g35->raid_flags =
+ 					(MR_RAID_FLAGS_IO_SUB_TYPE_CACHE_BYPASS
+ 					<< MR_RAID_CTX_RAID_FLAGS_IO_SUB_TYPE_SHIFT);
+ 			}
+ 		}
+ 	} else {
+ 		if (is_read) {
+ 			if ((raid->cpuAffinity.ldRead.cpu0) &&
+ 			    (raid->cpuAffinity.ldRead.cpu1))
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_FCFS;
+ 			else if (raid->cpuAffinity.ldRead.cpu1)
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_1;
+ 		} else {
+ 			if ((raid->cpuAffinity.ldWrite.cpu0) &&
+ 			    (raid->cpuAffinity.ldWrite.cpu1))
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_FCFS;
+ 			else if (raid->cpuAffinity.ldWrite.cpu1)
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_1;
+ 
+ 			if (is_stream_detected(rctx_g35) &&
+ 			    (raid->level == 5) &&
+ 			    (raid->writeMode == MR_RL_WRITE_THROUGH_MODE) &&
+ 			    (cpu_sel == MR_RAID_CTX_CPUSEL_FCFS))
+ 				cpu_sel = MR_RAID_CTX_CPUSEL_0;
+ 		}
+ 	}
+ 
+ 	rctx_g35->routing_flags |=
+ 		(cpu_sel << MR_RAID_CTX_ROUTINGFLAGS_CPUSEL_SHIFT);
+ 
+ 	/* Always give priority to MR_RAID_FLAGS_IO_SUB_TYPE_LDIO_BW_LIMIT
+ 	 * vs MR_RAID_FLAGS_IO_SUB_TYPE_CACHE_BYPASS.
+ 	 * IO Subtype is not bitmap.
+ 	 */
+ 	if ((raid->level == 1) && (!is_read)) {
+ 		if (scsi_buff_len > MR_LARGE_IO_MIN_SIZE)
+ 			praid_context->raid_context_g35.raid_flags =
+ 				(MR_RAID_FLAGS_IO_SUB_TYPE_LDIO_BW_LIMIT
+ 				<< MR_RAID_CTX_RAID_FLAGS_IO_SUB_TYPE_SHIFT);
+ 	}
+ }
+ 
+ /**
++>>>>>>> a174118b7a97 (scsi: megaraid_sas: big endian support changes)
   * megasas_build_ldio_fusion -	Prepares IOs to devices
   * @instance:		Adapter soft state
   * @scp:		SCSI command
@@@ -1965,9 -2335,43 +2048,49 @@@ megasas_build_ldio_fusion(struct megasa
  		/* In ventura if stream detected for a read and it is read ahead
  		 *  capable make this IO as LDIO
  		 */
++<<<<<<< HEAD
 +		if (io_request->RaidContext.raid_context_g35.stream_detected &&
 +				io_info.isRead && io_info.ra_capable)
 +			fp_possible = false;
++=======
+ 		if (is_stream_detected(&io_request->RaidContext.raid_context_g35) &&
+ 		    io_info.isRead && io_info.ra_capable)
+ 			fp_possible = false;
+ 
+ 		/* FP for Optimal raid level 1.
+ 		 * All large RAID-1 writes (> 32 KiB, both WT and WB modes)
+ 		 * are built by the driver as LD I/Os.
+ 		 * All small RAID-1 WT writes (<= 32 KiB) are built as FP I/Os
+ 		 * (there is never a reason to process these as buffered writes)
+ 		 * All small RAID-1 WB writes (<= 32 KiB) are built as FP I/Os
+ 		 * with the SLD bit asserted.
+ 		 */
+ 		if (io_info.r1_alt_dev_handle != MR_DEVHANDLE_INVALID) {
+ 			mrdev_priv = scp->device->hostdata;
+ 
+ 			if (atomic_inc_return(&instance->fw_outstanding) >
+ 				(instance->host->can_queue)) {
+ 				fp_possible = false;
+ 				atomic_dec(&instance->fw_outstanding);
+ 			} else if ((scsi_buff_len > MR_LARGE_IO_MIN_SIZE) ||
+ 				   atomic_dec_if_positive(&mrdev_priv->r1_ldio_hint)) {
+ 				fp_possible = false;
+ 				atomic_dec(&instance->fw_outstanding);
+ 				if (scsi_buff_len > MR_LARGE_IO_MIN_SIZE)
+ 					atomic_set(&mrdev_priv->r1_ldio_hint,
+ 						   instance->r1_ldio_hint_default);
+ 			}
+ 		}
+ 
+ 		/* If raid is NULL, set CPU affinity to default CPU0 */
+ 		if (raid)
+ 			megasas_set_raidflag_cpu_affinity(praid_context,
+ 				raid, fp_possible, io_info.isRead,
+ 				scsi_buff_len);
+ 		else
+ 			praid_context->raid_context_g35.routing_flags |=
+ 				(MR_RAID_CTX_CPUSEL_0 << MR_RAID_CTX_ROUTINGFLAGS_CPUSEL_SHIFT);
++>>>>>>> a174118b7a97 (scsi: megaraid_sas: big endian support changes)
  	}
  
  	if (fp_possible) {
@@@ -1987,12 -2391,22 +2110,24 @@@
  				= MPI2_TYPE_CUDA;
  			io_request->RaidContext.raid_context.nseg = 0x1;
  			io_request->IoFlags |= cpu_to_le16(MPI25_SAS_DEVICE0_FLAGS_ENABLED_FAST_PATH);
 -			io_request->RaidContext.raid_context.reg_lock_flags |=
 +			io_request->RaidContext.raid_context.regLockFlags |=
  			  (MR_RL_FLAGS_GRANT_DESTINATION_CUDA |
  			   MR_RL_FLAGS_SEQ_NUM_ENABLE);
++<<<<<<< HEAD
++=======
+ 		} else if (instance->is_ventura) {
+ 			io_request->RaidContext.raid_context_g35.nseg_type |=
+ 						(1 << RAID_CONTEXT_NSEG_SHIFT);
+ 			io_request->RaidContext.raid_context_g35.nseg_type |=
+ 						(MPI2_TYPE_CUDA << RAID_CONTEXT_TYPE_SHIFT);
+ 			io_request->RaidContext.raid_context_g35.routing_flags |=
+ 						(1 << MR_RAID_CTX_ROUTINGFLAGS_SQN_SHIFT);
+ 			io_request->IoFlags |=
+ 				cpu_to_le16(MPI25_SAS_DEVICE0_FLAGS_ENABLED_FAST_PATH);
++>>>>>>> a174118b7a97 (scsi: megaraid_sas: big endian support changes)
  		}
 -		if (fusion->load_balance_info &&
 -			(fusion->load_balance_info[device_id].loadBalanceFlag) &&
 -			(io_info.isRead)) {
 +		if ((fusion->load_balance_info[device_id].loadBalanceFlag) &&
 +		    (io_info.isRead)) {
  			io_info.devHandle =
  				get_updated_dev_handle(instance,
  					&fusion->load_balance_info[device_id],
@@@ -2036,6 -2459,13 +2171,16 @@@
  				(MR_RL_FLAGS_GRANT_DESTINATION_CPU0 |
  				 MR_RL_FLAGS_SEQ_NUM_ENABLE);
  			io_request->RaidContext.raid_context.nseg = 0x1;
++<<<<<<< HEAD
++=======
+ 		} else if (instance->is_ventura) {
+ 			io_request->RaidContext.raid_context_g35.routing_flags |=
+ 					(1 << MR_RAID_CTX_ROUTINGFLAGS_SQN_SHIFT);
+ 			io_request->RaidContext.raid_context_g35.nseg_type |=
+ 					(1 << RAID_CONTEXT_NSEG_SHIFT);
+ 			io_request->RaidContext.raid_context_g35.nseg_type |=
+ 					(MPI2_TYPE_CUDA << RAID_CONTEXT_TYPE_SHIFT);
++>>>>>>> a174118b7a97 (scsi: megaraid_sas: big endian support changes)
  		}
  		io_request->Function = MEGASAS_MPI2_FUNCTION_LD_IO_REQUEST;
  		io_request->DevHandle = cpu_to_le16(device_id);
@@@ -2173,17 -2610,31 +2318,42 @@@ megasas_build_syspd_fusion(struct megas
  		/* TgtId must be incremented by 255 as jbod seq number is index
  		 * below raid map
  		 */
++<<<<<<< HEAD
 +		pRAID_Context->VirtualDiskTgtId =
 +			cpu_to_le16(device_id + (MAX_PHYSICAL_DEVICES - 1));
 +		pRAID_Context->configSeqNum = pd_sync->seq[pd_index].seqNum;
 +		io_request->DevHandle = pd_sync->seq[pd_index].devHandle;
 +		pRAID_Context->regLockFlags |=
 +			(MR_RL_FLAGS_SEQ_NUM_ENABLE|MR_RL_FLAGS_GRANT_DESTINATION_CUDA);
 +		pRAID_Context->Type = MPI2_TYPE_CUDA;
 +		pRAID_Context->nseg = 0x1;
++=======
+ 		 /* More than 256 PD/JBOD support for Ventura */
+ 		if (instance->support_morethan256jbod)
+ 			pRAID_Context->virtual_disk_tgt_id =
+ 				pd_sync->seq[pd_index].pd_target_id;
+ 		else
+ 			pRAID_Context->virtual_disk_tgt_id =
+ 				cpu_to_le16(device_id + (MAX_PHYSICAL_DEVICES - 1));
+ 		pRAID_Context->config_seq_num = pd_sync->seq[pd_index].seqNum;
+ 		io_request->DevHandle = pd_sync->seq[pd_index].devHandle;
+ 		if (instance->is_ventura) {
+ 			io_request->RaidContext.raid_context_g35.routing_flags |=
+ 				(1 << MR_RAID_CTX_ROUTINGFLAGS_SQN_SHIFT);
+ 			io_request->RaidContext.raid_context_g35.nseg_type |=
+ 							(1 << RAID_CONTEXT_NSEG_SHIFT);
+ 			io_request->RaidContext.raid_context_g35.nseg_type |=
+ 							(MPI2_TYPE_CUDA << RAID_CONTEXT_TYPE_SHIFT);
+ 		} else {
+ 			pRAID_Context->type = MPI2_TYPE_CUDA;
+ 			pRAID_Context->nseg = 0x1;
+ 			pRAID_Context->reg_lock_flags |=
+ 				(MR_RL_FLAGS_SEQ_NUM_ENABLE|MR_RL_FLAGS_GRANT_DESTINATION_CUDA);
+ 		}
++>>>>>>> a174118b7a97 (scsi: megaraid_sas: big endian support changes)
  	} else if (fusion->fast_path_io) {
 -		pRAID_Context->virtual_disk_tgt_id = cpu_to_le16(device_id);
 -		pRAID_Context->config_seq_num = 0;
 +		pRAID_Context->VirtualDiskTgtId = cpu_to_le16(device_id);
 +		pRAID_Context->configSeqNum = 0;
  		local_map_ptr = fusion->ld_drv_map[(instance->map_id & 1)];
  		io_request->DevHandle =
  			local_map_ptr->raidMap.devHndlInfo[device_id].curDevHdl;
@@@ -2298,11 -2746,18 +2468,26 @@@ megasas_build_io_fusion(struct megasas_
  		return 1;
  	}
  
++<<<<<<< HEAD
 +	/* numSGE store lower 8 bit of sge_count.
 +	 * numSGEExt store higher 8 bit of sge_count
 +	 */
 +	io_request->RaidContext.raid_context.numSGE = sge_count;
 +	io_request->RaidContext.raid_context.numSGEExt = (u8)(sge_count >> 8);
++=======
+ 	if (instance->is_ventura) {
+ 		set_num_sge(&io_request->RaidContext.raid_context_g35, sge_count);
+ 		cpu_to_le16s(&io_request->RaidContext.raid_context_g35.routing_flags);
+ 		cpu_to_le16s(&io_request->RaidContext.raid_context_g35.nseg_type);
+ 	} else {
+ 		/* numSGE store lower 8 bit of sge_count.
+ 		 * numSGEExt store higher 8 bit of sge_count
+ 		 */
+ 		io_request->RaidContext.raid_context.num_sge = sge_count;
+ 		io_request->RaidContext.raid_context.num_sge_ext =
+ 			(u8)(sge_count >> 8);
+ 	}
++>>>>>>> a174118b7a97 (scsi: megaraid_sas: big endian support changes)
  
  	io_request->SGLFlags = cpu_to_le16(MPI2_SGE_FLAGS_64_BIT_ADDRESSING);
  
* Unmerged path drivers/scsi/megaraid/megaraid_sas_fp.c
* Unmerged path drivers/scsi/megaraid/megaraid_sas_fusion.c
diff --git a/drivers/scsi/megaraid/megaraid_sas_fusion.h b/drivers/scsi/megaraid/megaraid_sas_fusion.h
index ef6bfe55344c..2dc747023a46 100644
--- a/drivers/scsi/megaraid/megaraid_sas_fusion.h
+++ b/drivers/scsi/megaraid/megaraid_sas_fusion.h
@@ -141,44 +141,13 @@ struct RAID_CONTEXT {
  * starts in MPT IO Frames
  */
 struct RAID_CONTEXT_G35 {
-#if   defined(__BIG_ENDIAN_BITFIELD)
-	u16	resvd0:8;
-	u16	nseg:4;
-	u16	type:4;
-#else
-	u16	type:4;		    /* 0x00 */
-	u16	nseg:4;		    /* 0x00 */
-	u16 resvd0:8;
-#endif
+	#define RAID_CONTEXT_NSEG_MASK	0x00F0
+	#define RAID_CONTEXT_NSEG_SHIFT	4
+	#define RAID_CONTEXT_TYPE_MASK	0x000F
+	#define RAID_CONTEXT_TYPE_SHIFT	0
+	u16		nseg_type;
 	u16 timeout_value; /* 0x02 -0x03 */
-	union {
-		struct {
-#if	defined(__BIG_ENDIAN_BITFIELD)
-		u16	set_divert:4;
-		u16	cpu_sel:4;
-		u16	log:1;
-		u16	rw:1;
-		u16	sbs:1;
-		u16	sqn:1;
-		u16	fwn:1;
-		u16	c2f:1;
-		u16	sld:1;
-		u16	reserved:1;
-#else
-		u16	reserved:1;
-		u16	sld:1;
-		u16	c2f:1;
-		u16	fwn:1;
-		u16	sqn:1;
-		u16	sbs:1;
-		u16	rw:1;
-		u16	log:1;
-		u16	cpu_sel:4;
-		u16	set_divert:4;
-#endif
-			} bits;
-		u16 s;
-	} routing_flags;	/* 0x04 -0x05 routing flags */
+	u16		routing_flags;	// 0x04 -0x05 routing flags
 	u16 virtual_disk_tgt_id;   /* 0x06 -0x07 */
 	u64 reg_lock_row_lba;      /* 0x08 - 0x0F */
 	u32 reg_lock_length;      /* 0x10 - 0x13 */
@@ -193,18 +162,78 @@ struct RAID_CONTEXT_G35 {
 				 */
 	u8 span_arm;            /* 0x1C span[7:5], arm[4:0] */
 	u16	config_seq_num;           /* 0x1A -0x1B */
+	union {
+		/*
+		 * Bit format:
+		 *	 ---------------------------------
+		 *	 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |
+		 *	 ---------------------------------
+		 * Byte0 |    numSGE[7]- numSGE[0]	 |
+		 *	 ---------------------------------
+		 * Byte1 |SD | resvd     | numSGE 8-11   |
+		 *        --------------------------------
+		 */
+		#define NUM_SGE_MASK_LOWER	0xFF
+		#define NUM_SGE_MASK_UPPER	0x0F
+		#define NUM_SGE_SHIFT_UPPER	8
+		#define STREAM_DETECT_SHIFT	7
+		#define STREAM_DETECT_MASK	0x80
+		struct {
 #if   defined(__BIG_ENDIAN_BITFIELD) /* 0x1C - 0x1D */
-	u16 stream_detected:1;
-	u16 reserved:3;
-	u16 num_sge:12;
+			u16 stream_detected:1;
+			u16 reserved:3;
+			u16 num_sge:12;
 #else
-	u16 num_sge:12;
-	u16 reserved:3;
-	u16 stream_detected:1;
+			u16 num_sge:12;
+			u16 reserved:3;
+			u16 stream_detected:1;
 #endif
+		} bits;
+		u8 bytes[2];
+	} u;
 	u8 resvd2[2];          /* 0x1E-0x1F */
 };
 
+#define MR_RAID_CTX_ROUTINGFLAGS_SLD_SHIFT	1
+#define MR_RAID_CTX_ROUTINGFLAGS_C2D_SHIFT	2
+#define MR_RAID_CTX_ROUTINGFLAGS_FWD_SHIFT	3
+#define MR_RAID_CTX_ROUTINGFLAGS_SQN_SHIFT	4
+#define MR_RAID_CTX_ROUTINGFLAGS_SBS_SHIFT	5
+#define MR_RAID_CTX_ROUTINGFLAGS_RW_SHIFT	6
+#define MR_RAID_CTX_ROUTINGFLAGS_LOG_SHIFT	7
+#define MR_RAID_CTX_ROUTINGFLAGS_CPUSEL_SHIFT	8
+#define MR_RAID_CTX_ROUTINGFLAGS_CPUSEL_MASK	0x0F00
+#define MR_RAID_CTX_ROUTINGFLAGS_SETDIVERT_SHIFT	12
+#define MR_RAID_CTX_ROUTINGFLAGS_SETDIVERT_MASK	0xF000
+
+static inline void set_num_sge(struct RAID_CONTEXT_G35 *rctx_g35,
+			       u16 sge_count)
+{
+	rctx_g35->u.bytes[0] = (u8)(sge_count & NUM_SGE_MASK_LOWER);
+	rctx_g35->u.bytes[1] |= (u8)((sge_count >> NUM_SGE_SHIFT_UPPER)
+							& NUM_SGE_MASK_UPPER);
+}
+
+static inline u16 get_num_sge(struct RAID_CONTEXT_G35 *rctx_g35)
+{
+	u16 sge_count;
+
+	sge_count = (u16)(((rctx_g35->u.bytes[1] & NUM_SGE_MASK_UPPER)
+			<< NUM_SGE_SHIFT_UPPER) | (rctx_g35->u.bytes[0]));
+	return sge_count;
+}
+
+#define SET_STREAM_DETECTED(rctx_g35) \
+	(rctx_g35.u.bytes[1] |= STREAM_DETECT_MASK)
+
+#define CLEAR_STREAM_DETECTED(rctx_g35) \
+	(rctx_g35.u.bytes[1] &= ~(STREAM_DETECT_MASK))
+
+static inline bool is_stream_detected(struct RAID_CONTEXT_G35 *rctx_g35)
+{
+	return ((rctx_g35->u.bytes[1] & STREAM_DETECT_MASK));
+}
+
 union RAID_CONTEXT_UNION {
 	struct RAID_CONTEXT raid_context;
 	struct RAID_CONTEXT_G35 raid_context_g35;
