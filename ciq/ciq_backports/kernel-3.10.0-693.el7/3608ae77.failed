net/mlx5e: Move function mlx5e_create_umr_mkey

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Move function mlx5e_create_umr_mkey (Don Dutile) [1385330 1417286]
Rebuild_FUZZ: 95.45%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit 3608ae77c098dfe134103a9dec4c78687896708e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/3608ae77.failed

In next patch we are going to create a UMR MKey per RQ, we need
mlx5e_create_umr_mkey declared before mlx5e_create_rq.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3608ae77c098dfe134103a9dec4c78687896708e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 2aca61ab5499,49ca30bf1b6f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -280,6 -370,144 +280,147 @@@ static void mlx5e_disable_async_events(
  #define MLX5E_HW2SW_MTU(hwmtu) (hwmtu - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  #define MLX5E_SW2HW_MTU(swmtu) (swmtu + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  
++<<<<<<< HEAD
++=======
+ static inline int mlx5e_get_wqe_mtt_sz(void)
+ {
+ 	/* UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
+ 	 * To avoid copying garbage after the mtt array, we allocate
+ 	 * a little more.
+ 	 */
+ 	return ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(__be64),
+ 		     MLX5_UMR_MTT_ALIGNMENT);
+ }
+ 
+ static inline void mlx5e_build_umr_wqe(struct mlx5e_rq *rq, struct mlx5e_sq *sq,
+ 				       struct mlx5e_umr_wqe *wqe, u16 ix)
+ {
+ 	struct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;
+ 	struct mlx5_wqe_data_seg      *dseg = &wqe->data;
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	u8 ds_cnt = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS);
+ 	u32 umr_wqe_mtt_offset = mlx5e_get_wqe_mtt_offset(rq, ix);
+ 
+ 	cseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |
+ 				      ds_cnt);
+ 	cseg->fm_ce_se  = MLX5_WQE_CTRL_CQ_UPDATE;
+ 	cseg->imm       = rq->mkey_be;
+ 
+ 	ucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN;
+ 	ucseg->klm_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(MLX5_MPWRQ_PAGES_PER_WQE));
+ 	ucseg->bsf_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(umr_wqe_mtt_offset));
+ 	ucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ 
+ 	dseg->lkey = sq->mkey_be;
+ 	dseg->addr = cpu_to_be64(wi->umr.mtt_addr);
+ }
+ 
+ static int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq,
+ 				     struct mlx5e_channel *c)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int mtt_alloc = mtt_sz + MLX5_UMR_ALIGN - 1;
+ 	int i;
+ 
+ 	rq->mpwqe.info = kzalloc_node(wq_sz * sizeof(*rq->mpwqe.info),
+ 				      GFP_KERNEL, cpu_to_node(c->cpu));
+ 	if (!rq->mpwqe.info)
+ 		goto err_out;
+ 
+ 	/* We allocate more than mtt_sz as we will align the pointer */
+ 	rq->mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz, GFP_KERNEL,
+ 					cpu_to_node(c->cpu));
+ 	if (unlikely(!rq->mpwqe.mtt_no_align))
+ 		goto err_free_wqe_info;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		wi->umr.mtt = PTR_ALIGN(rq->mpwqe.mtt_no_align + i * mtt_alloc,
+ 					MLX5_UMR_ALIGN);
+ 		wi->umr.mtt_addr = dma_map_single(c->pdev, wi->umr.mtt, mtt_sz,
+ 						  PCI_DMA_TODEVICE);
+ 		if (unlikely(dma_mapping_error(c->pdev, wi->umr.mtt_addr)))
+ 			goto err_unmap_mtts;
+ 
+ 		mlx5e_build_umr_wqe(rq, &c->icosq, &wi->umr.wqe, i);
+ 	}
+ 
+ 	return 0;
+ 
+ err_unmap_mtts:
+ 	while (--i >= 0) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(c->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ err_free_wqe_info:
+ 	kfree(rq->mpwqe.info);
+ 
+ err_out:
+ 	return -ENOMEM;
+ }
+ 
+ static void mlx5e_rq_free_mpwqe_info(struct mlx5e_rq *rq)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int i;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ 	kfree(rq->mpwqe.info);
+ }
+ 
+ static int mlx5e_create_umr_mkey(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	u64 npages = MLX5E_REQUIRED_MTTS(priv->profile->max_nch(mdev),
+ 					 BIT(MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW));
+ 	int inlen = MLX5_ST_SZ_BYTES(create_mkey_in);
+ 	void *mkc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	in = mlx5_vzalloc(inlen);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	mkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);
+ 
+ 	npages = min_t(u32, ALIGN(U16_MAX, 4) * 2, npages);
+ 
+ 	MLX5_SET(mkc, mkc, free, 1);
+ 	MLX5_SET(mkc, mkc, umr_en, 1);
+ 	MLX5_SET(mkc, mkc, lw, 1);
+ 	MLX5_SET(mkc, mkc, lr, 1);
+ 	MLX5_SET(mkc, mkc, access_mode, MLX5_MKC_ACCESS_MODE_MTT);
+ 
+ 	MLX5_SET(mkc, mkc, qpn, 0xffffff);
+ 	MLX5_SET(mkc, mkc, pd, mdev->mlx5e_res.pdn);
+ 	MLX5_SET64(mkc, mkc, len, npages << PAGE_SHIFT);
+ 	MLX5_SET(mkc, mkc, translations_octword_size,
+ 		 MLX5_MTT_OCTW(npages));
+ 	MLX5_SET(mkc, mkc, log_page_size, PAGE_SHIFT);
+ 
+ 	err = mlx5_core_create_mkey(mdev, &priv->umr_mkey, in, inlen);
+ 
+ 	kvfree(in);
+ 	return err;
+ }
+ 
++>>>>>>> 3608ae77c098 (net/mlx5e: Move function mlx5e_create_umr_mkey)
  static int mlx5e_create_rq(struct mlx5e_channel *c,
  			   struct mlx5e_rq_param *param,
  			   struct mlx5e_rq *rq)
@@@ -2968,53 -3662,176 +3109,60 @@@ static void mlx5e_destroy_q_counter(str
  	mlx5_core_dealloc_q_counter(priv->mdev, priv->q_counter);
  }
  
 -static void mlx5e_nic_init(struct mlx5_core_dev *mdev,
 -			   struct net_device *netdev,
 -			   const struct mlx5e_profile *profile,
 -			   void *ppriv)
 -{
 -	struct mlx5e_priv *priv = netdev_priv(netdev);
 -
 -	mlx5e_build_nic_netdev_priv(mdev, netdev, profile, ppriv);
 -	mlx5e_build_nic_netdev(netdev);
 -	mlx5e_vxlan_init(priv);
 -}
 -
 -static void mlx5e_nic_cleanup(struct mlx5e_priv *priv)
 -{
 -	struct mlx5_core_dev *mdev = priv->mdev;
 -	struct mlx5_eswitch *esw = mdev->priv.eswitch;
 -
 -	mlx5e_vxlan_cleanup(priv);
 -
 -	if (MLX5_CAP_GEN(mdev, vport_group_manager))
 -		mlx5_eswitch_unregister_vport_rep(esw, 0);
 -
 -	if (priv->xdp_prog)
 -		bpf_prog_put(priv->xdp_prog);
 -}
 -
 -static int mlx5e_init_nic_rx(struct mlx5e_priv *priv)
++<<<<<<< HEAD
 +static int mlx5e_create_umr_mkey(struct mlx5e_priv *priv)
  {
  	struct mlx5_core_dev *mdev = priv->mdev;
 +	struct mlx5_create_mkey_mbox_in *in;
 +	struct mlx5_mkey_seg *mkc;
 +	int inlen = sizeof(*in);
 +	u64 npages =
 +		mlx5e_get_max_num_channels(mdev) * MLX5_CHANNEL_MAX_NUM_MTTS;
  	int err;
 -	int i;
 -
 -	err = mlx5e_create_indirect_rqts(priv);
 -	if (err) {
 -		mlx5_core_warn(mdev, "create indirect rqts failed, %d\n", err);
 -		return err;
 -	}
 -
 -	err = mlx5e_create_direct_rqts(priv);
 -	if (err) {
 -		mlx5_core_warn(mdev, "create direct rqts failed, %d\n", err);
 -		goto err_destroy_indirect_rqts;
 -	}
  
 -	err = mlx5e_create_indirect_tirs(priv);
 -	if (err) {
 -		mlx5_core_warn(mdev, "create indirect tirs failed, %d\n", err);
 -		goto err_destroy_direct_rqts;
 -	}
 +	in = mlx5_vzalloc(inlen);
 +	if (!in)
 +		return -ENOMEM;
  
 -	err = mlx5e_create_direct_tirs(priv);
 -	if (err) {
 -		mlx5_core_warn(mdev, "create direct tirs failed, %d\n", err);
 -		goto err_destroy_indirect_tirs;
 -	}
 +	mkc = &in->seg;
 +	mkc->status = MLX5_MKEY_STATUS_FREE;
 +	mkc->flags = MLX5_PERM_UMR_EN |
 +		     MLX5_PERM_LOCAL_READ |
 +		     MLX5_PERM_LOCAL_WRITE |
 +		     MLX5_ACCESS_MODE_MTT;
  
 -	err = mlx5e_create_flow_steering(priv);
 -	if (err) {
 -		mlx5_core_warn(mdev, "create flow steering failed, %d\n", err);
 -		goto err_destroy_direct_tirs;
 -	}
 +	mkc->qpn_mkey7_0 = cpu_to_be32(0xffffff << 8);
 +	mkc->flags_pd = cpu_to_be32(priv->pdn);
 +	mkc->len = cpu_to_be64(npages << PAGE_SHIFT);
 +	mkc->xlt_oct_size = cpu_to_be32(mlx5e_get_mtt_octw(npages));
 +	mkc->log2_page_size = PAGE_SHIFT;
  
 -	err = mlx5e_tc_init(priv);
 -	if (err)
 -		goto err_destroy_flow_steering;
 +	err = mlx5_core_create_mkey(mdev, &priv->umr_mkey, in, inlen, NULL,
 +				    NULL, NULL);
  
 -	return 0;
 +	kvfree(in);
  
 -err_destroy_flow_steering:
 -	mlx5e_destroy_flow_steering(priv);
 -err_destroy_direct_tirs:
 -	mlx5e_destroy_direct_tirs(priv);
 -err_destroy_indirect_tirs:
 -	mlx5e_destroy_indirect_tirs(priv);
 -err_destroy_direct_rqts:
 -	for (i = 0; i < priv->profile->max_nch(mdev); i++)
 -		mlx5e_destroy_rqt(priv, &priv->direct_tir[i].rqt);
 -err_destroy_indirect_rqts:
 -	mlx5e_destroy_rqt(priv, &priv->indir_rqt);
  	return err;
  }
  
 -static void mlx5e_cleanup_nic_rx(struct mlx5e_priv *priv)
 -{
 -	int i;
 -
 -	mlx5e_tc_cleanup(priv);
 -	mlx5e_destroy_flow_steering(priv);
 -	mlx5e_destroy_direct_tirs(priv);
 -	mlx5e_destroy_indirect_tirs(priv);
 -	for (i = 0; i < priv->profile->max_nch(priv->mdev); i++)
 -		mlx5e_destroy_rqt(priv, &priv->direct_tir[i].rqt);
 -	mlx5e_destroy_rqt(priv, &priv->indir_rqt);
 -}
 -
 -static int mlx5e_init_nic_tx(struct mlx5e_priv *priv)
 -{
 -	int err;
 -
 -	err = mlx5e_create_tises(priv);
 -	if (err) {
 -		mlx5_core_warn(priv->mdev, "create tises failed, %d\n", err);
 -		return err;
 -	}
 -
 -#ifdef CONFIG_MLX5_CORE_EN_DCB
 -	mlx5e_dcbnl_initialize(priv);
 -#endif
 -	return 0;
 -}
 -
 -static void mlx5e_nic_enable(struct mlx5e_priv *priv)
 -{
 -	struct net_device *netdev = priv->netdev;
 -	struct mlx5_core_dev *mdev = priv->mdev;
 -	struct mlx5_eswitch *esw = mdev->priv.eswitch;
 -	struct mlx5_eswitch_rep rep;
 -
 -	mlx5_lag_add(mdev, netdev);
 -
 -	if (mlx5e_vxlan_allowed(mdev)) {
 -		rtnl_lock();
 -		udp_tunnel_get_rx_info(netdev);
 -		rtnl_unlock();
 -	}
 -
 -	mlx5e_enable_async_events(priv);
 -	queue_work(priv->wq, &priv->set_rx_mode_work);
 -
 -	if (MLX5_CAP_GEN(mdev, vport_group_manager)) {
 -		mlx5_query_nic_vport_mac_address(mdev, 0, rep.hw_id);
 -		rep.load = mlx5e_nic_rep_load;
 -		rep.unload = mlx5e_nic_rep_unload;
 -		rep.vport = FDB_UPLINK_VPORT;
 -		rep.priv_data = priv;
 -		mlx5_eswitch_register_vport_rep(esw, 0, &rep);
 -	}
 -}
 -
 -static void mlx5e_nic_disable(struct mlx5e_priv *priv)
 -{
 -	queue_work(priv->wq, &priv->set_rx_mode_work);
 -	mlx5e_disable_async_events(priv);
 -	mlx5_lag_remove(priv->mdev);
 -}
 -
 -static const struct mlx5e_profile mlx5e_nic_profile = {
 -	.init		   = mlx5e_nic_init,
 -	.cleanup	   = mlx5e_nic_cleanup,
 -	.init_rx	   = mlx5e_init_nic_rx,
 -	.cleanup_rx	   = mlx5e_cleanup_nic_rx,
 -	.init_tx	   = mlx5e_init_nic_tx,
 -	.cleanup_tx	   = mlx5e_cleanup_nic_tx,
 -	.enable		   = mlx5e_nic_enable,
 -	.disable	   = mlx5e_nic_disable,
 -	.update_stats	   = mlx5e_update_stats,
 -	.max_nch	   = mlx5e_get_max_num_channels,
 -	.max_tc		   = MLX5E_MAX_NUM_TC,
 -};
 -
 -struct net_device *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
 -				       const struct mlx5e_profile *profile,
 -				       void *ppriv)
 +static void *mlx5e_create_netdev(struct mlx5_core_dev *mdev)
++=======
++static void mlx5e_nic_init(struct mlx5_core_dev *mdev,
++			   struct net_device *netdev,
++			   const struct mlx5e_profile *profile,
++			   void *ppriv)
++>>>>>>> 3608ae77c098 (net/mlx5e: Move function mlx5e_create_umr_mkey)
  {
 -	int nch = profile->max_nch(mdev);
  	struct net_device *netdev;
  	struct mlx5e_priv *priv;
 +	int nch = mlx5e_get_max_num_channels(mdev);
 +	int err;
 +
 +	if (mlx5e_check_required_hca_cap(mdev))
 +		return NULL;
  
  	netdev = alloc_etherdev_mqs(sizeof(struct mlx5e_priv),
 -				    nch * profile->max_tc,
 +				    nch * MLX5E_MAX_NUM_TC,
  				    nch);
  	if (!netdev) {
  		mlx5_core_err(mdev, "alloc_etherdev_mqs() failed\n");
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
