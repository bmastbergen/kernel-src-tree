amd-xgbe: Prepare for priority-based FIFO allocation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Lendacky, Thomas <Thomas.Lendacky@amd.com>
commit 586e3cfb260a0c6026321417bb5af331cc717f40
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/586e3cfb.failed

Currently, the Rx and Tx fifos are evenly allocated between the hardware
queues of the device.  As more queues are instantiated, the fifo memory
needs to be able to be allocated based on queue priority. This allows for
higher priority queues to have more fifo memory than lower priority
queues. Prepare for this by modifying the current fifo calculation to
assign the fifo queue allocation in an array that is then used to program
the hardware.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 586e3cfb260a0c6026321417bb5af331cc717f40)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amd/xgbe/xgbe-dev.c
#	drivers/net/ethernet/amd/xgbe/xgbe.h
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-dev.c
index a748fd8a1c58,18f8001fdacd..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
@@@ -1407,118 -2000,151 +1407,184 @@@ static void xgbe_config_mtl_mode(struc
  	XGMAC_IOWRITE_BITS(pdata, MTL_OMR, RAA, MTL_RAA_SP);
  }
  
- static unsigned int xgbe_calculate_per_queue_fifo(unsigned int fifo_size,
- 						  unsigned int queue_count)
+ static unsigned int xgbe_get_tx_fifo_size(struct xgbe_prv_data *pdata)
  {
+ 	unsigned int fifo_size;
+ 
+ 	/* Calculate the configured fifo size */
+ 	fifo_size = 1 << (pdata->hw_feat.tx_fifo_size + 7);
+ 
+ 	/* The configured value may not be the actual amount of fifo RAM */
+ 	return min_t(unsigned int, XGMAC_FIFO_TX_MAX, fifo_size);
+ }
+ 
+ static unsigned int xgbe_get_rx_fifo_size(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int fifo_size;
+ 
+ 	/* Calculate the configured fifo size */
+ 	fifo_size = 1 << (pdata->hw_feat.rx_fifo_size + 7);
+ 
+ 	/* The configured value may not be the actual amount of fifo RAM */
+ 	return min_t(unsigned int, XGMAC_FIFO_RX_MAX, fifo_size);
+ }
+ 
+ static void xgbe_calculate_equal_fifo(unsigned int fifo_size,
+ 				      unsigned int queue_count,
+ 				      unsigned int *fifo)
+ {
++<<<<<<< HEAD
 +	unsigned int q_fifo_size = 0;
 +	enum xgbe_mtl_fifo_size p_fifo = XGMAC_MTL_FIFO_SIZE_256;
 +
 +	/* Calculate Tx/Rx fifo share per queue */
 +	switch (fifo_size) {
 +	case 0:
 +		q_fifo_size = XGBE_FIFO_SIZE_B(128);
 +		break;
 +	case 1:
 +		q_fifo_size = XGBE_FIFO_SIZE_B(256);
 +		break;
 +	case 2:
 +		q_fifo_size = XGBE_FIFO_SIZE_B(512);
 +		break;
 +	case 3:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(1);
 +		break;
 +	case 4:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(2);
 +		break;
 +	case 5:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(4);
 +		break;
 +	case 6:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(8);
 +		break;
 +	case 7:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(16);
 +		break;
 +	case 8:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(32);
 +		break;
 +	case 9:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(64);
 +		break;
 +	case 10:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(128);
 +		break;
 +	case 11:
 +		q_fifo_size = XGBE_FIFO_SIZE_KB(256);
 +		break;
 +	}
 +
 +	/* The configured value is not the actual amount of fifo RAM */
 +	q_fifo_size = min_t(unsigned int, XGBE_FIFO_MAX, q_fifo_size);
 +
 +	q_fifo_size = q_fifo_size / queue_count;
++=======
+ 	unsigned int q_fifo_size;
+ 	unsigned int p_fifo;
+ 	unsigned int i;
+ 
+ 	q_fifo_size = fifo_size / queue_count;
 -
 -	/* Each increment in the queue fifo size represents 256 bytes of
 -	 * fifo, with 0 representing 256 bytes. Distribute the fifo equally
 -	 * between the queues.
 -	 */
 -	p_fifo = q_fifo_size / 256;
 -	if (p_fifo)
 -		p_fifo--;
++>>>>>>> 586e3cfb260a (amd-xgbe: Prepare for priority-based FIFO allocation)
 +
 +	/* Set the queue fifo size programmable value */
 +	if (q_fifo_size >= XGBE_FIFO_SIZE_KB(256))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_256K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(128))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_128K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(64))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_64K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(32))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_32K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(16))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_16K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(8))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_8K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(4))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_4K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(2))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_2K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_KB(1))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_1K;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_B(512))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_512;
 +	else if (q_fifo_size >= XGBE_FIFO_SIZE_B(256))
 +		p_fifo = XGMAC_MTL_FIFO_SIZE_256;
  
- 	return p_fifo;
+ 	for (i = 0; i < queue_count; i++)
+ 		fifo[i] = p_fifo;
  }
  
  static void xgbe_config_tx_fifo_size(struct xgbe_prv_data *pdata)
  {
++<<<<<<< HEAD
 +	enum xgbe_mtl_fifo_size fifo_size;
 +	unsigned int i;
 +
 +	fifo_size = xgbe_calculate_per_queue_fifo(pdata->hw_feat.tx_fifo_size,
 +						  pdata->hw_feat.tx_q_cnt);
 +
 +	for (i = 0; i < pdata->hw_feat.tx_q_cnt; i++)
 +		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_TQOMR, TQS, fifo_size);
 +
 +	netdev_notice(pdata->netdev, "%d Tx queues, %d byte fifo per queue\n",
 +		      pdata->hw_feat.tx_q_cnt, ((fifo_size + 1) * 256));
++=======
+ 	unsigned int fifo_size;
+ 	unsigned int fifo[XGBE_MAX_QUEUES];
+ 	unsigned int i;
+ 
+ 	fifo_size = xgbe_get_tx_fifo_size(pdata);
+ 
+ 	xgbe_calculate_equal_fifo(fifo_size, pdata->tx_q_count, fifo);
+ 
+ 	for (i = 0; i < pdata->tx_q_count; i++)
+ 		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_TQOMR, TQS, fifo[i]);
+ 
+ 	netif_info(pdata, drv, pdata->netdev,
+ 		   "%d Tx hardware queues, %d byte fifo per queue\n",
+ 		   pdata->tx_q_count, ((fifo[0] + 1) * 256));
++>>>>>>> 586e3cfb260a (amd-xgbe: Prepare for priority-based FIFO allocation)
  }
  
  static void xgbe_config_rx_fifo_size(struct xgbe_prv_data *pdata)
  {
++<<<<<<< HEAD
 +	enum xgbe_mtl_fifo_size fifo_size;
 +	unsigned int i;
 +
 +	fifo_size = xgbe_calculate_per_queue_fifo(pdata->hw_feat.rx_fifo_size,
 +						  pdata->hw_feat.rx_q_cnt);
 +
 +	for (i = 0; i < pdata->hw_feat.rx_q_cnt; i++)
 +		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, RQS, fifo_size);
 +
 +	netdev_notice(pdata->netdev, "%d Rx queues, %d byte fifo per queue\n",
 +		      pdata->hw_feat.rx_q_cnt, ((fifo_size + 1) * 256));
++=======
+ 	unsigned int fifo_size;
+ 	unsigned int fifo[XGBE_MAX_QUEUES];
+ 	unsigned int i;
+ 
+ 	fifo_size = xgbe_get_rx_fifo_size(pdata);
+ 
+ 	xgbe_calculate_equal_fifo(fifo_size, pdata->rx_q_count, fifo);
+ 
+ 	for (i = 0; i < pdata->rx_q_count; i++)
+ 		XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_Q_RQOMR, RQS, fifo[i]);
+ 
+ 	netif_info(pdata, drv, pdata->netdev,
+ 		   "%d Rx hardware queues, %d byte fifo per queue\n",
+ 		   pdata->rx_q_count, ((fifo[0] + 1) * 256));
++>>>>>>> 586e3cfb260a (amd-xgbe: Prepare for priority-based FIFO allocation)
  }
  
 -static void xgbe_config_queue_mapping(struct xgbe_prv_data *pdata)
 +static void xgbe_config_rx_queue_mapping(struct xgbe_prv_data *pdata)
  {
 -	unsigned int qptc, qptc_extra, queue;
 -	unsigned int prio_queues;
 -	unsigned int ppq, ppq_extra, prio;
 -	unsigned int mask;
 -	unsigned int i, j, reg, reg_val;
 -
 -	/* Map the MTL Tx Queues to Traffic Classes
 -	 *   Note: Tx Queues >= Traffic Classes
 -	 */
 -	qptc = pdata->tx_q_count / pdata->hw_feat.tc_cnt;
 -	qptc_extra = pdata->tx_q_count % pdata->hw_feat.tc_cnt;
 -
 -	for (i = 0, queue = 0; i < pdata->hw_feat.tc_cnt; i++) {
 -		for (j = 0; j < qptc; j++) {
 -			netif_dbg(pdata, drv, pdata->netdev,
 -				  "TXq%u mapped to TC%u\n", queue, i);
 -			XGMAC_MTL_IOWRITE_BITS(pdata, queue, MTL_Q_TQOMR,
 -					       Q2TCMAP, i);
 -			pdata->q2tc_map[queue++] = i;
 -		}
 -
 -		if (i < qptc_extra) {
 -			netif_dbg(pdata, drv, pdata->netdev,
 -				  "TXq%u mapped to TC%u\n", queue, i);
 -			XGMAC_MTL_IOWRITE_BITS(pdata, queue, MTL_Q_TQOMR,
 -					       Q2TCMAP, i);
 -			pdata->q2tc_map[queue++] = i;
 -		}
 -	}
 -
 -	/* Map the 8 VLAN priority values to available MTL Rx queues */
 -	prio_queues = min_t(unsigned int, IEEE_8021QAZ_MAX_TCS,
 -			    pdata->rx_q_count);
 -	ppq = IEEE_8021QAZ_MAX_TCS / prio_queues;
 -	ppq_extra = IEEE_8021QAZ_MAX_TCS % prio_queues;
 -
 -	reg = MAC_RQC2R;
 -	reg_val = 0;
 -	for (i = 0, prio = 0; i < prio_queues;) {
 -		mask = 0;
 -		for (j = 0; j < ppq; j++) {
 -			netif_dbg(pdata, drv, pdata->netdev,
 -				  "PRIO%u mapped to RXq%u\n", prio, i);
 -			mask |= (1 << prio);
 -			pdata->prio2q_map[prio++] = i;
 -		}
 -
 -		if (i < ppq_extra) {
 -			netif_dbg(pdata, drv, pdata->netdev,
 -				  "PRIO%u mapped to RXq%u\n", prio, i);
 -			mask |= (1 << prio);
 -			pdata->prio2q_map[prio++] = i;
 -		}
 -
 -		reg_val |= (mask << ((i++ % MAC_RQC2_Q_PER_REG) << 3));
 -
 -		if ((i % MAC_RQC2_Q_PER_REG) && (i != prio_queues))
 -			continue;
 -
 -		XGMAC_IOWRITE(pdata, reg, reg_val);
 -		reg += MAC_RQC2_INC;
 -		reg_val = 0;
 -	}
 +	unsigned int i, reg, reg_val;
 +	unsigned int q_count = pdata->hw_feat.rx_q_cnt;
  
  	/* Select dynamic mapping of MTL Rx queue to DMA Rx channel */
  	reg = MTL_RQDCM0R;
diff --cc drivers/net/ethernet/amd/xgbe/xgbe.h
index 1903f878545a,d838b44a9952..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe.h
+++ b/drivers/net/ethernet/amd/xgbe/xgbe.h
@@@ -166,11 -208,10 +166,16 @@@
  #define XGMAC_DRIVER_CONTEXT	1
  #define XGMAC_IOCTL_CONTEXT	2
  
++<<<<<<< HEAD
 +#define XGBE_FIFO_MAX		81920
 +#define XGBE_FIFO_SIZE_B(x)	(x)
 +#define XGBE_FIFO_SIZE_KB(x)	(x * 1024)
++=======
+ #define XGMAC_FIFO_RX_MAX	81920
+ #define XGMAC_FIFO_TX_MAX	81920
++>>>>>>> 586e3cfb260a (amd-xgbe: Prepare for priority-based FIFO allocation)
  
 -#define XGBE_TC_MIN_QUANTUM	10
 +#define XGBE_TC_CNT		2
  
  /* Helper macro for descriptor handling
   *  Always use XGBE_GET_DESC_DATA to access the descriptor data
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dev.c
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe.h
