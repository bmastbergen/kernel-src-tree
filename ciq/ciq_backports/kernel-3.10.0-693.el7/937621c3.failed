md/r5cache: move some code to raid5.h

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [md] r5cache: move some code to raid5.h (Jes Sorensen) [1380016]
Rebuild_FUZZ: 95.77%
commit-author Song Liu <songliubraving@fb.com>
commit 937621c36e0ea1af2aceeaea412ba3bd80247199
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/937621c3.failed

Move some define and inline functions to raid5.h, so they can be
used in raid5-cache.c

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 937621c36e0ea1af2aceeaea412ba3bd80247199)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5.c
diff --cc drivers/md/raid5.c
index e4353594a601,34895f3218d9..000000000000
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@@ -126,64 -113,6 +113,67 @@@ static inline void unlock_all_device_ha
  	local_irq_enable();
  }
  
++<<<<<<< HEAD
 +/* bio's attached to a stripe+device for I/O are linked together in bi_sector
 + * order without overlap.  There may be several bio's per stripe+device, and
 + * a bio could span several devices.
 + * When walking this list for a particular stripe+device, we must never proceed
 + * beyond a bio that extends past this device, as the next bio might no longer
 + * be valid.
 + * This function is used to determine the 'next' bio in the list, given the sector
 + * of the current stripe+device
 + */
 +static inline struct bio *r5_next_bio(struct bio *bio, sector_t sector)
 +{
 +	int sectors = bio_sectors(bio);
 +	if (bio->bi_sector + sectors < sector + STRIPE_SECTORS)
 +		return bio->bi_next;
 +	else
 +		return NULL;
 +}
 +
 +/*
 + * We maintain a biased count of active stripes in the bottom 16 bits of
 + * bi_phys_segments, and a count of processed stripes in the upper 16 bits
 + */
 +static inline int raid5_bi_processed_stripes(struct bio *bio)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +	return (atomic_read(segments) >> 16) & 0xffff;
 +}
 +
 +static inline int raid5_dec_bi_active_stripes(struct bio *bio)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +	return atomic_sub_return(1, segments) & 0xffff;
 +}
 +
 +static inline void raid5_inc_bi_active_stripes(struct bio *bio)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +	atomic_inc(segments);
 +}
 +
 +static inline void raid5_set_bi_processed_stripes(struct bio *bio,
 +	unsigned int cnt)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +	int old, new;
 +
 +	do {
 +		old = atomic_read(segments);
 +		new = (old & 0xffff) | (cnt << 16);
 +	} while (atomic_cmpxchg(segments, old, new) != old);
 +}
 +
 +static inline void raid5_set_bi_stripes(struct bio *bio, unsigned int cnt)
 +{
 +	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
 +	atomic_set(segments, cnt);
 +}
 +
++=======
++>>>>>>> 937621c36e0e (md/r5cache: move some code to raid5.h)
  /* Find first data disk in a raid6 stripe */
  static inline int raid6_d0(struct stripe_head *sh)
  {
* Unmerged path drivers/md/raid5.c
diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 517d4b68a1be..46cfe93a2cff 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -410,6 +410,83 @@ struct disk_info {
 	struct md_rdev	*rdev, *replacement;
 };
 
+/*
+ * Stripe cache
+ */
+
+#define NR_STRIPES		256
+#define STRIPE_SIZE		PAGE_SIZE
+#define STRIPE_SHIFT		(PAGE_SHIFT - 9)
+#define STRIPE_SECTORS		(STRIPE_SIZE>>9)
+#define	IO_THRESHOLD		1
+#define BYPASS_THRESHOLD	1
+#define NR_HASH			(PAGE_SIZE / sizeof(struct hlist_head))
+#define HASH_MASK		(NR_HASH - 1)
+#define MAX_STRIPE_BATCH	8
+
+/* bio's attached to a stripe+device for I/O are linked together in bi_sector
+ * order without overlap.  There may be several bio's per stripe+device, and
+ * a bio could span several devices.
+ * When walking this list for a particular stripe+device, we must never proceed
+ * beyond a bio that extends past this device, as the next bio might no longer
+ * be valid.
+ * This function is used to determine the 'next' bio in the list, given the
+ * sector of the current stripe+device
+ */
+static inline struct bio *r5_next_bio(struct bio *bio, sector_t sector)
+{
+	int sectors = bio_sectors(bio);
+
+	if (bio->bi_iter.bi_sector + sectors < sector + STRIPE_SECTORS)
+		return bio->bi_next;
+	else
+		return NULL;
+}
+
+/*
+ * We maintain a biased count of active stripes in the bottom 16 bits of
+ * bi_phys_segments, and a count of processed stripes in the upper 16 bits
+ */
+static inline int raid5_bi_processed_stripes(struct bio *bio)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	return (atomic_read(segments) >> 16) & 0xffff;
+}
+
+static inline int raid5_dec_bi_active_stripes(struct bio *bio)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	return atomic_sub_return(1, segments) & 0xffff;
+}
+
+static inline void raid5_inc_bi_active_stripes(struct bio *bio)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	atomic_inc(segments);
+}
+
+static inline void raid5_set_bi_processed_stripes(struct bio *bio,
+	unsigned int cnt)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+	int old, new;
+
+	do {
+		old = atomic_read(segments);
+		new = (old & 0xffff) | (cnt << 16);
+	} while (atomic_cmpxchg(segments, old, new) != old);
+}
+
+static inline void raid5_set_bi_stripes(struct bio *bio, unsigned int cnt)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	atomic_set(segments, cnt);
+}
+
 /* NOTE NR_STRIPE_HASH_LOCKS must remain below 64.
  * This is because we sometimes take all the spinlocks
  * and creating that much locking depth can cause
