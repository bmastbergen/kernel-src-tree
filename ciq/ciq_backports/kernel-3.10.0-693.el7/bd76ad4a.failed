crypto: qat - Stop dropping leading zeros from RSA output

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [crypto] qat - Stop dropping leading zeros from RSA output (Neil Horman) [1382849]
Rebuild_FUZZ: 92.45%
commit-author Salvatore Benedetto <salvatore.benedetto@intel.com>
commit bd76ad4abfdccd26a9ac11214aa715e83bc8e808
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/bd76ad4a.failed

There is not need to drop leading zeros from the RSA output
operations results.

	Signed-off-by: Salvatore Benedetto <salvatore.benedetto@intel.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit bd76ad4abfdccd26a9ac11214aa715e83bc8e808)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/qat/qat_common/qat_asym_algs.c
diff --cc drivers/crypto/qat/qat_common/qat_asym_algs.c
index 2ab9aa76c203,0d35dca2e925..000000000000
--- a/drivers/crypto/qat/qat_common/qat_asym_algs.c
+++ b/drivers/crypto/qat/qat_common/qat_asym_algs.c
@@@ -124,42 -192,392 +124,49 @@@ static void qat_rsa_cb(struct icp_qat_f
  
  	err = (err == ICP_QAT_FW_COMN_STATUS_FLAG_OK) ? 0 : -EINVAL;
  
 -	if (areq->src) {
 -		if (req->src_align)
 -			dma_free_coherent(dev, req->ctx.dh->p_size,
 -					  req->src_align, req->in.dh.in.b);
 -		else
 -			dma_unmap_single(dev, req->in.dh.in.b,
 -					 req->ctx.dh->p_size, DMA_TO_DEVICE);
 -	}
 +	if (req->src_align)
 +		dma_free_coherent(dev, req->ctx->key_sz, req->src_align,
 +				  req->in.enc.m);
 +	else
 +		dma_unmap_single(dev, req->in.enc.m, req->ctx->key_sz,
 +				 DMA_TO_DEVICE);
  
 -	areq->dst_len = req->ctx.dh->p_size;
 +	areq->dst_len = req->ctx->key_sz;
  	if (req->dst_align) {
 -		scatterwalk_map_and_copy(req->dst_align, areq->dst, 0,
 -					 areq->dst_len, 1);
 -
 -		dma_free_coherent(dev, req->ctx.dh->p_size, req->dst_align,
 -				  req->out.dh.r);
 -	} else {
 -		dma_unmap_single(dev, req->out.dh.r, req->ctx.dh->p_size,
 -				 DMA_FROM_DEVICE);
 -	}
 -
 -	dma_unmap_single(dev, req->phy_in, sizeof(struct qat_dh_input_params),
 -			 DMA_TO_DEVICE);
 -	dma_unmap_single(dev, req->phy_out,
 -			 sizeof(struct qat_dh_output_params),
 -			 DMA_TO_DEVICE);
 -
 -	kpp_request_complete(areq, err);
 -}
 -
 -#define PKE_DH_1536 0x390c1a49
 -#define PKE_DH_G2_1536 0x2e0b1a3e
 -#define PKE_DH_2048 0x4d0c1a60
 -#define PKE_DH_G2_2048 0x3e0b1a55
 -#define PKE_DH_3072 0x510c1a77
 -#define PKE_DH_G2_3072 0x3a0b1a6c
 -#define PKE_DH_4096 0x690c1a8e
 -#define PKE_DH_G2_4096 0x4a0b1a83
 -
 -static unsigned long qat_dh_fn_id(unsigned int len, bool g2)
 -{
 -	unsigned int bitslen = len << 3;
 -
 -	switch (bitslen) {
 -	case 1536:
 -		return g2 ? PKE_DH_G2_1536 : PKE_DH_1536;
 -	case 2048:
 -		return g2 ? PKE_DH_G2_2048 : PKE_DH_2048;
 -	case 3072:
 -		return g2 ? PKE_DH_G2_3072 : PKE_DH_3072;
 -	case 4096:
 -		return g2 ? PKE_DH_G2_4096 : PKE_DH_4096;
 -	default:
 -		return 0;
 -	};
 -}
 -
 -static inline struct qat_dh_ctx *qat_dh_get_params(struct crypto_kpp *tfm)
 -{
 -	return kpp_tfm_ctx(tfm);
 -}
++<<<<<<< HEAD
 +		char *ptr = req->dst_align;
  
 -static int qat_dh_compute_value(struct kpp_request *req)
 -{
 -	struct crypto_kpp *tfm = crypto_kpp_reqtfm(req);
 -	struct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);
 -	struct qat_crypto_instance *inst = ctx->inst;
 -	struct device *dev = &GET_DEV(inst->accel_dev);
 -	struct qat_asym_request *qat_req =
 -			PTR_ALIGN(kpp_request_ctx(req), 64);
 -	struct icp_qat_fw_pke_request *msg = &qat_req->req;
 -	int ret, ctr = 0;
 -	int n_input_params = 0;
 -
 -	if (unlikely(!ctx->xa))
 -		return -EINVAL;
 -
 -	if (req->dst_len < ctx->p_size) {
 -		req->dst_len = ctx->p_size;
 -		return -EOVERFLOW;
 -	}
 -	memset(msg, '\0', sizeof(*msg));
 -	ICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,
 -					  ICP_QAT_FW_COMN_REQ_FLAG_SET);
 -
 -	msg->pke_hdr.cd_pars.func_id = qat_dh_fn_id(ctx->p_size,
 -						    !req->src && ctx->g2);
 -	if (unlikely(!msg->pke_hdr.cd_pars.func_id))
 -		return -EINVAL;
 -
 -	qat_req->cb = qat_dh_cb;
 -	qat_req->ctx.dh = ctx;
 -	qat_req->areq.dh = req;
 -	msg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;
 -	msg->pke_hdr.comn_req_flags =
 -		ICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,
 -					    QAT_COMN_CD_FLD_TYPE_64BIT_ADR);
 -
 -	/*
 -	 * If no source is provided use g as base
 -	 */
 -	if (req->src) {
 -		qat_req->in.dh.in.xa = ctx->dma_xa;
 -		qat_req->in.dh.in.p = ctx->dma_p;
 -		n_input_params = 3;
 -	} else {
 -		if (ctx->g2) {
 -			qat_req->in.dh.in_g2.xa = ctx->dma_xa;
 -			qat_req->in.dh.in_g2.p = ctx->dma_p;
 -			n_input_params = 2;
 -		} else {
 -			qat_req->in.dh.in.b = ctx->dma_g;
 -			qat_req->in.dh.in.xa = ctx->dma_xa;
 -			qat_req->in.dh.in.p = ctx->dma_p;
 -			n_input_params = 3;
 +		while (!(*ptr) && areq->dst_len) {
 +			areq->dst_len--;
 +			ptr++;
  		}
 -	}
  
 -	ret = -ENOMEM;
 -	if (req->src) {
 -		/*
 -		 * src can be of any size in valid range, but HW expects it to
 -		 * be the same as modulo p so in case it is different we need
 -		 * to allocate a new buf and copy src data.
 -		 * In other case we just need to map the user provided buffer.
 -		 * Also need to make sure that it is in contiguous buffer.
 -		 */
 -		if (sg_is_last(req->src) && req->src_len == ctx->p_size) {
 -			qat_req->src_align = NULL;
 -			qat_req->in.dh.in.b = dma_map_single(dev,
 -							     sg_virt(req->src),
 -							     req->src_len,
 -							     DMA_TO_DEVICE);
 -			if (unlikely(dma_mapping_error(dev,
 -						       qat_req->in.dh.in.b)))
 -				return ret;
 -
 -		} else {
 -			int shift = ctx->p_size - req->src_len;
 -
 -			qat_req->src_align = dma_zalloc_coherent(dev,
 -								 ctx->p_size,
 -								 &qat_req->in.dh.in.b,
 -								 GFP_KERNEL);
 -			if (unlikely(!qat_req->src_align))
 -				return ret;
 -
 -			scatterwalk_map_and_copy(qat_req->src_align + shift,
 -						 req->src, 0, req->src_len, 0);
 -		}
 -	}
 -	/*
 -	 * dst can be of any size in valid range, but HW expects it to be the
 -	 * same as modulo m so in case it is different we need to allocate a
 -	 * new buf and copy src data.
 -	 * In other case we just need to map the user provided buffer.
 -	 * Also need to make sure that it is in contiguous buffer.
 -	 */
 -	if (sg_is_last(req->dst) && req->dst_len == ctx->p_size) {
 -		qat_req->dst_align = NULL;
 -		qat_req->out.dh.r = dma_map_single(dev, sg_virt(req->dst),
 -						   req->dst_len,
 -						   DMA_FROM_DEVICE);
 +		if (areq->dst_len != req->ctx->key_sz)
 +			memmove(req->dst_align, ptr, areq->dst_len);
  
 -		if (unlikely(dma_mapping_error(dev, qat_req->out.dh.r)))
 -			goto unmap_src;
++=======
++>>>>>>> bd76ad4abfdc (crypto: qat - Stop dropping leading zeros from RSA output)
 +		scatterwalk_map_and_copy(req->dst_align, areq->dst, 0,
 +					 areq->dst_len, 1);
  
 +		dma_free_coherent(dev, req->ctx->key_sz, req->dst_align,
 +				  req->out.enc.c);
  	} else {
 -		qat_req->dst_align = dma_zalloc_coherent(dev, ctx->p_size,
 -							 &qat_req->out.dh.r,
 -							 GFP_KERNEL);
 -		if (unlikely(!qat_req->dst_align))
 -			goto unmap_src;
 -	}
 -
 -	qat_req->in.dh.in_tab[n_input_params] = 0;
 -	qat_req->out.dh.out_tab[1] = 0;
 -	/* Mapping in.in.b or in.in_g2.xa is the same */
 -	qat_req->phy_in = dma_map_single(dev, &qat_req->in.dh.in.b,
 -					 sizeof(struct qat_dh_input_params),
 -					 DMA_TO_DEVICE);
 -	if (unlikely(dma_mapping_error(dev, qat_req->phy_in)))
 -		goto unmap_dst;
 -
 -	qat_req->phy_out = dma_map_single(dev, &qat_req->out.dh.r,
 -					  sizeof(struct qat_dh_output_params),
 -					  DMA_TO_DEVICE);
 -	if (unlikely(dma_mapping_error(dev, qat_req->phy_out)))
 -		goto unmap_in_params;
 -
 -	msg->pke_mid.src_data_addr = qat_req->phy_in;
 -	msg->pke_mid.dest_data_addr = qat_req->phy_out;
 -	msg->pke_mid.opaque = (uint64_t)(__force long)qat_req;
 -	msg->input_param_count = n_input_params;
 -	msg->output_param_count = 1;
 -
 -	do {
 -		ret = adf_send_message(ctx->inst->pke_tx, (uint32_t *)msg);
 -	} while (ret == -EBUSY && ctr++ < 100);
++<<<<<<< HEAD
 +		char *ptr = sg_virt(areq->dst);
  
 -	if (!ret)
 -		return -EINPROGRESS;
 -
 -	if (!dma_mapping_error(dev, qat_req->phy_out))
 -		dma_unmap_single(dev, qat_req->phy_out,
 -				 sizeof(struct qat_dh_output_params),
 -				 DMA_TO_DEVICE);
 -unmap_in_params:
 -	if (!dma_mapping_error(dev, qat_req->phy_in))
 -		dma_unmap_single(dev, qat_req->phy_in,
 -				 sizeof(struct qat_dh_input_params),
 -				 DMA_TO_DEVICE);
 -unmap_dst:
 -	if (qat_req->dst_align)
 -		dma_free_coherent(dev, ctx->p_size, qat_req->dst_align,
 -				  qat_req->out.dh.r);
 -	else
 -		if (!dma_mapping_error(dev, qat_req->out.dh.r))
 -			dma_unmap_single(dev, qat_req->out.dh.r, ctx->p_size,
 -					 DMA_FROM_DEVICE);
 -unmap_src:
 -	if (req->src) {
 -		if (qat_req->src_align)
 -			dma_free_coherent(dev, ctx->p_size, qat_req->src_align,
 -					  qat_req->in.dh.in.b);
 -		else
 -			if (!dma_mapping_error(dev, qat_req->in.dh.in.b))
 -				dma_unmap_single(dev, qat_req->in.dh.in.b,
 -						 ctx->p_size,
 -						 DMA_TO_DEVICE);
 -	}
 -	return ret;
 -}
 -
 -static int qat_dh_check_params_length(unsigned int p_len)
 -{
 -	switch (p_len) {
 -	case 1536:
 -	case 2048:
 -	case 3072:
 -	case 4096:
 -		return 0;
 -	}
 -	return -EINVAL;
 -}
 -
 -static int qat_dh_set_params(struct qat_dh_ctx *ctx, struct dh *params)
 -{
 -	struct qat_crypto_instance *inst = ctx->inst;
 -	struct device *dev = &GET_DEV(inst->accel_dev);
 -
 -	if (unlikely(!params->p || !params->g))
 -		return -EINVAL;
 -
 -	if (qat_dh_check_params_length(params->p_size << 3))
 -		return -EINVAL;
 -
 -	ctx->p_size = params->p_size;
 -	ctx->p = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_p, GFP_KERNEL);
 -	if (!ctx->p)
 -		return -ENOMEM;
 -	memcpy(ctx->p, params->p, ctx->p_size);
 -
 -	/* If g equals 2 don't copy it */
 -	if (params->g_size == 1 && *(char *)params->g == 0x02) {
 -		ctx->g2 = true;
 -		return 0;
 -	}
 -
 -	ctx->g = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_g, GFP_KERNEL);
 -	if (!ctx->g) {
 -		dma_free_coherent(dev, ctx->p_size, ctx->p, ctx->dma_p);
 -		ctx->p = NULL;
 -		return -ENOMEM;
 -	}
 -	memcpy(ctx->g + (ctx->p_size - params->g_size), params->g,
 -	       params->g_size);
 -
 -	return 0;
 -}
 -
 -static void qat_dh_clear_ctx(struct device *dev, struct qat_dh_ctx *ctx)
 -{
 -	if (ctx->g) {
 -		dma_free_coherent(dev, ctx->p_size, ctx->g, ctx->dma_g);
 -		ctx->g = NULL;
 -	}
 -	if (ctx->xa) {
 -		dma_free_coherent(dev, ctx->p_size, ctx->xa, ctx->dma_xa);
 -		ctx->xa = NULL;
 -	}
 -	if (ctx->p) {
 -		dma_free_coherent(dev, ctx->p_size, ctx->p, ctx->dma_p);
 -		ctx->p = NULL;
 -	}
 -	ctx->p_size = 0;
 -	ctx->g2 = false;
 -}
 -
 -static int qat_dh_set_secret(struct crypto_kpp *tfm, void *buf,
 -			     unsigned int len)
 -{
 -	struct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);
 -	struct device *dev = &GET_DEV(ctx->inst->accel_dev);
 -	struct dh params;
 -	int ret;
 -
 -	if (crypto_dh_decode_key(buf, len, &params) < 0)
 -		return -EINVAL;
 -
 -	/* Free old secret if any */
 -	qat_dh_clear_ctx(dev, ctx);
 -
 -	ret = qat_dh_set_params(ctx, &params);
 -	if (ret < 0)
 -		return ret;
 -
 -	ctx->xa = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_xa,
 -				      GFP_KERNEL);
 -	if (!ctx->xa) {
 -		qat_dh_clear_ctx(dev, ctx);
 -		return -ENOMEM;
 -	}
 -	memcpy(ctx->xa + (ctx->p_size - params.key_size), params.key,
 -	       params.key_size);
 -
 -	return 0;
 -}
 -
 -static int qat_dh_max_size(struct crypto_kpp *tfm)
 -{
 -	struct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);
 -
 -	return ctx->p ? ctx->p_size : -EINVAL;
 -}
 -
 -static int qat_dh_init_tfm(struct crypto_kpp *tfm)
 -{
 -	struct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);
 -	struct qat_crypto_instance *inst =
 -			qat_crypto_get_instance_node(get_current_node());
 -
 -	if (!inst)
 -		return -EINVAL;
 -
 -	ctx->p_size = 0;
 -	ctx->g2 = false;
 -	ctx->inst = inst;
 -	return 0;
 -}
 -
 -static void qat_dh_exit_tfm(struct crypto_kpp *tfm)
 -{
 -	struct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);
 -	struct device *dev = &GET_DEV(ctx->inst->accel_dev);
 -
 -	qat_dh_clear_ctx(dev, ctx);
 -	qat_crypto_put_instance(ctx->inst);
 -}
 -
 -static void qat_rsa_cb(struct icp_qat_fw_pke_resp *resp)
 -{
 -	struct qat_asym_request *req = (void *)(__force long)resp->opaque;
 -	struct akcipher_request *areq = req->areq.rsa;
 -	struct device *dev = &GET_DEV(req->ctx.rsa->inst->accel_dev);
 -	int err = ICP_QAT_FW_PKE_RESP_PKE_STAT_GET(
 -				resp->pke_resp_hdr.comn_resp_flags);
 -
 -	err = (err == ICP_QAT_FW_COMN_STATUS_FLAG_OK) ? 0 : -EINVAL;
 -
 -	if (req->src_align)
 -		dma_free_coherent(dev, req->ctx.rsa->key_sz, req->src_align,
 -				  req->in.rsa.enc.m);
 -	else
 -		dma_unmap_single(dev, req->in.rsa.enc.m, req->ctx.rsa->key_sz,
 -				 DMA_TO_DEVICE);
 +		while (!(*ptr) && areq->dst_len) {
 +			areq->dst_len--;
 +			ptr++;
 +		}
  
 -	areq->dst_len = req->ctx.rsa->key_sz;
 -	if (req->dst_align) {
 -		scatterwalk_map_and_copy(req->dst_align, areq->dst, 0,
 -					 areq->dst_len, 1);
 +		if (sg_virt(areq->dst) != ptr && areq->dst_len)
 +			memmove(sg_virt(areq->dst), ptr, areq->dst_len);
  
 -		dma_free_coherent(dev, req->ctx.rsa->key_sz, req->dst_align,
 -				  req->out.rsa.enc.c);
 -	} else {
 +		dma_unmap_single(dev, req->out.enc.c, req->ctx->key_sz,
++=======
+ 		dma_unmap_single(dev, req->out.rsa.enc.c, req->ctx.rsa->key_sz,
++>>>>>>> bd76ad4abfdc (crypto: qat - Stop dropping leading zeros from RSA output)
  				 DMA_FROM_DEVICE);
  	}
  
* Unmerged path drivers/crypto/qat/qat_common/qat_asym_algs.c
