locking/rwsem: Add reader-owned state to the owner field

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Waiman Long <Waiman.Long@hpe.com>
commit 19c5d690e41697fcdd19379ab9d10d8d37818414
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/19c5d690.failed

Currently, it is not possible to determine for sure if a reader
owns a rwsem by looking at the content of the rwsem data structure.
This patch adds a new state RWSEM_READER_OWNED to the owner field
to indicate that readers currently own the lock. This enables us to
address the following 2 issues in the rwsem optimistic spinning code:

 1) rwsem_can_spin_on_owner() will disallow optimistic spinning if
    the owner field is NULL which can mean either the readers own
    the lock or the owning writer hasn't set the owner field yet.
    In the latter case, we miss the chance to do optimistic spinning.

 2) While a writer is waiting in the OSQ and a reader takes the lock,
    the writer will continue to spin when out of the OSQ in the main
    rwsem_optimistic_spin() loop as the owner field is NULL wasting
    CPU cycles if some of readers are sleeping.

Adding the new state will allow optimistic spinning to go forward as
long as the owner field is not RWSEM_READER_OWNED and the owner is
running, if set, but stop immediately when that state has been reached.

On a 4-socket Haswell machine running on a 4.6-rc1 based kernel, the
fio test with multithreaded randrw and randwrite tests on the same
file on a XFS partition on top of a NVDIMM were run, the aggregated
bandwidths before and after the patch were as follows:

  Test      BW before patch     BW after patch  % change
  ----      ---------------     --------------  --------
  randrw         988 MB/s          1192 MB/s      +21%
  randwrite     1513 MB/s          1623 MB/s      +7.3%

The perf profile of the rwsem_down_write_failed() function in randrw
before and after the patch were:

   19.95%  5.88%  fio  [kernel.vmlinux]  [k] rwsem_down_write_failed
   14.20%  1.52%  fio  [kernel.vmlinux]  [k] rwsem_down_write_failed

The actual CPU cycles spend in rwsem_down_write_failed() dropped from
5.88% to 1.52% after the patch.

The xfstests was also run and no regression was observed.

	Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Jason Low <jason.low2@hp.com>
	Acked-by: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Douglas Hatch <doug.hatch@hpe.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Hurley <peter@hurleysoftware.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Scott J Norton <scott.norton@hpe.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/1463534783-38814-2-git-send-email-Waiman.Long@hpe.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 19c5d690e41697fcdd19379ab9d10d8d37818414)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/rwsem.h
#	lib/rwsem.c
diff --cc lib/rwsem.c
index 09d8c2da4ff3,6b0d0605910e..000000000000
--- a/lib/rwsem.c
+++ b/lib/rwsem.c
@@@ -300,17 -330,12 +306,18 @@@ static inline bool rwsem_can_spin_on_ow
  		return false;
  
  	rcu_read_lock();
++<<<<<<< HEAD:lib/rwsem.c
 +	owner = ACCESS_ONCE(sem->owner);
 +	if (!owner) {
 +		long count = ACCESS_ONCE(sem->count);
++=======
+ 	owner = READ_ONCE(sem->owner);
+ 	if (!rwsem_owner_is_writer(owner)) {
++>>>>>>> 19c5d690e416 (locking/rwsem: Add reader-owned state to the owner field):kernel/locking/rwsem-xadd.c
  		/*
- 		 * If sem->owner is not set, yet we have just recently entered the
- 		 * slowpath with the lock being active, then there is a possibility
- 		 * reader(s) may have the lock. To be safe, bail spinning in these
- 		 * situations.
+ 		 * Don't spin if the rwsem is readers owned.
  		 */
- 		if (count & RWSEM_ACTIVE_MASK)
- 			ret = false;
+ 		ret = !rwsem_owner_is_reader(owner);
  		goto done;
  	}
  
@@@ -345,16 -368,11 +350,16 @@@ bool rwsem_spin_on_owner(struct rw_sema
  	}
  	rcu_read_unlock();
  
- 	if (READ_ONCE(sem->owner))
- 		return true; /* new owner, continue spinning */
- 
  	/*
- 	 * When the owner is not set, the lock could be free or
- 	 * held by readers. Check the counter to verify the
- 	 * state.
+ 	 * If there is a new owner or the owner is not set, we continue
+ 	 * spinning.
  	 */
++<<<<<<< HEAD:lib/rwsem.c
 +	count = READ_ONCE(sem->count);
 +	return (count == 0 || count == RWSEM_WAITING_BIAS);
++=======
+ 	return !rwsem_owner_is_reader(READ_ONCE(sem->owner));
++>>>>>>> 19c5d690e416 (locking/rwsem: Add reader-owned state to the owner field):kernel/locking/rwsem-xadd.c
  }
  
  static bool rwsem_optimistic_spin(struct rw_semaphore *sem)
@@@ -372,8 -390,17 +377,22 @@@
  		goto done;
  
  	while (true) {
++<<<<<<< HEAD:lib/rwsem.c
 +		owner = ACCESS_ONCE(sem->owner);
 +		if (owner && !rwsem_spin_on_owner(sem, owner))
++=======
+ 		owner = READ_ONCE(sem->owner);
+ 		/*
+ 		 * Don't spin if
+ 		 * 1) the owner is a reader as we we can't determine if the
+ 		 *    reader is actively running or not.
+ 		 * 2) The rwsem_spin_on_owner() returns false which means
+ 		 *    the owner isn't running.
+ 		 */
+ 		if (rwsem_owner_is_reader(owner) ||
+ 		   (rwsem_owner_is_writer(owner) &&
+ 		   !rwsem_spin_on_owner(sem, owner)))
++>>>>>>> 19c5d690e416 (locking/rwsem: Add reader-owned state to the owner field):kernel/locking/rwsem-xadd.c
  			break;
  
  		/* wait_lock will be acquired if write_lock is obtained */
* Unmerged path kernel/locking/rwsem.h
* Unmerged path kernel/locking/rwsem.h
diff --git a/kernel/rwsem.c b/kernel/rwsem.c
index 42f806de49d4..17bee7712adf 100644
--- a/kernel/rwsem.c
+++ b/kernel/rwsem.c
@@ -42,6 +42,7 @@ void __sched down_read(struct rw_semaphore *sem)
 	rwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);
 
 	LOCK_CONTENDED(sem, __down_read_trylock, __down_read);
+	rwsem_set_reader_owned(sem);
 }
 
 EXPORT_SYMBOL(down_read);
@@ -53,8 +54,10 @@ int down_read_trylock(struct rw_semaphore *sem)
 {
 	int ret = __down_read_trylock(sem);
 
-	if (ret == 1)
+	if (ret == 1) {
 		rwsem_acquire_read(&sem->dep_map, 0, 1, _RET_IP_);
+		rwsem_set_reader_owned(sem);
+	}
 	return ret;
 }
 
@@ -125,7 +128,7 @@ void downgrade_write(struct rw_semaphore *sem)
 	 * lockdep: a downgraded write will live on as a write
 	 * dependency.
 	 */
-	rwsem_clear_owner(sem);
+	rwsem_set_reader_owned(sem);
 	__downgrade_write(sem);
 }
 
@@ -139,6 +142,7 @@ void down_read_nested(struct rw_semaphore *sem, int subclass)
 	rwsem_acquire_read(&sem->dep_map, subclass, 0, _RET_IP_);
 
 	LOCK_CONTENDED(sem, __down_read_trylock, __down_read);
+	rwsem_set_reader_owned(sem);
 }
 
 EXPORT_SYMBOL(down_read_nested);
* Unmerged path lib/rwsem.c
