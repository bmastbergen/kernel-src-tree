xen-netfront: Rework the fix for Rx stall during OOM and network stress

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Vineeth Remanan Pillai <vineethp@amazon.com>
commit 538d92912d3190a1dd809233a0d57277459f37b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/538d9291.failed

The commit 90c311b0eeea ("xen-netfront: Fix Rx stall during network
stress and OOM") caused the refill timer to be triggerred almost on
all invocations of xennet_alloc_rx_buffers for certain workloads.
This reworks the fix by reverting to the old behaviour and taking into
consideration the skb allocation failure. Refill timer is now triggered
on insufficient requests or skb allocation failure.

	Signed-off-by: Vineeth Remanan Pillai <vineethp@amazon.com>
Fixes: 90c311b0eeea (xen-netfront: Fix Rx stall during network stress and OOM)
	Reported-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 538d92912d3190a1dd809233a0d57277459f37b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/xen-netfront.c
diff --cc drivers/net/xen-netfront.c
index 8896052a2ee0,9dba69731f30..000000000000
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@@ -221,127 -229,116 +221,181 @@@ static bool xennet_can_sg(struct net_de
  
  static void rx_refill_timeout(unsigned long data)
  {
 -	struct netfront_queue *queue = (struct netfront_queue *)data;
 -	napi_schedule(&queue->napi);
 +	struct net_device *dev = (struct net_device *)data;
 +	struct netfront_info *np = netdev_priv(dev);
 +	napi_schedule(&np->napi);
  }
  
 -static int netfront_tx_slot_available(struct netfront_queue *queue)
 +static int netfront_tx_slot_available(struct netfront_info *np)
  {
 -	return (queue->tx.req_prod_pvt - queue->tx.rsp_cons) <
 -		(NET_TX_RING_SIZE - MAX_SKB_FRAGS - 2);
 +	return (np->tx.req_prod_pvt - np->tx.rsp_cons) <
 +		(TX_MAX_TARGET - MAX_SKB_FRAGS - 2);
  }
  
 -static void xennet_maybe_wake_tx(struct netfront_queue *queue)
 +static void xennet_maybe_wake_tx(struct net_device *dev)
  {
 -	struct net_device *dev = queue->info->netdev;
 -	struct netdev_queue *dev_queue = netdev_get_tx_queue(dev, queue->id);
 +	struct netfront_info *np = netdev_priv(dev);
  
 -	if (unlikely(netif_tx_queue_stopped(dev_queue)) &&
 -	    netfront_tx_slot_available(queue) &&
 +	if (unlikely(netif_queue_stopped(dev)) &&
 +	    netfront_tx_slot_available(np) &&
  	    likely(netif_running(dev)))
 -		netif_tx_wake_queue(netdev_get_tx_queue(dev, queue->id));
 +		netif_wake_queue(dev);
  }
  
 -
 -static struct sk_buff *xennet_alloc_one_rx_buffer(struct netfront_queue *queue)
 +static void xennet_alloc_rx_buffers(struct net_device *dev)
  {
 +	unsigned short id;
 +	struct netfront_info *np = netdev_priv(dev);
  	struct sk_buff *skb;
  	struct page *page;
 +	int i, batch_target, notify;
 +	RING_IDX req_prod = np->rx.req_prod_pvt;
 +	grant_ref_t ref;
 +	unsigned long pfn;
 +	void *vaddr;
 +	struct xen_netif_rx_request *req;
  
++<<<<<<< HEAD
 +	if (unlikely(!netif_carrier_ok(dev)))
++=======
+ 	skb = __netdev_alloc_skb(queue->info->netdev,
+ 				 RX_COPY_THRESHOLD + NET_IP_ALIGN,
+ 				 GFP_ATOMIC | __GFP_NOWARN);
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
+ 	page = alloc_page(GFP_ATOMIC | __GFP_NOWARN);
+ 	if (!page) {
+ 		kfree_skb(skb);
+ 		return NULL;
+ 	}
+ 	skb_add_rx_frag(skb, 0, page, 0, 0, PAGE_SIZE);
+ 
+ 	/* Align ip header to a 16 bytes boundary */
+ 	skb_reserve(skb, NET_IP_ALIGN);
+ 	skb->dev = queue->info->netdev;
+ 
+ 	return skb;
+ }
+ 
+ 
+ static void xennet_alloc_rx_buffers(struct netfront_queue *queue)
+ {
+ 	RING_IDX req_prod = queue->rx.req_prod_pvt;
+ 	int notify;
+ 	int err = 0;
+ 
+ 	if (unlikely(!netif_carrier_ok(queue->info->netdev)))
++>>>>>>> 538d92912d31 (xen-netfront: Rework the fix for Rx stall during OOM and network stress)
  		return;
  
 -	for (req_prod = queue->rx.req_prod_pvt;
 -	     req_prod - queue->rx.rsp_cons < NET_RX_RING_SIZE;
 -	     req_prod++) {
 -		struct sk_buff *skb;
 -		unsigned short id;
 -		grant_ref_t ref;
 -		struct page *page;
 -		struct xen_netif_rx_request *req;
 +	/*
 +	 * Allocate skbuffs greedily, even though we batch updates to the
 +	 * receive ring. This creates a less bursty demand on the memory
 +	 * allocator, so should reduce the chance of failed allocation requests
 +	 * both for ourself and for other kernel subsystems.
 +	 */
 +	batch_target = np->rx_target - (req_prod - np->rx.rsp_cons);
 +	for (i = skb_queue_len(&np->rx_batch); i < batch_target; i++) {
 +		skb = __netdev_alloc_skb(dev, RX_COPY_THRESHOLD + NET_IP_ALIGN,
 +					 GFP_ATOMIC | __GFP_NOWARN);
 +		if (unlikely(!skb))
 +			goto no_skb;
 +
++<<<<<<< HEAD
 +		/* Align ip header to a 16 bytes boundary */
 +		skb_reserve(skb, NET_IP_ALIGN);
 +
 +		page = alloc_page(GFP_ATOMIC | __GFP_NOWARN);
 +		if (!page) {
 +			kfree_skb(skb);
 +no_skb:
 +			/* Could not allocate any skbuffs. Try again later. */
 +			mod_timer(&np->rx_refill_timer,
 +				  jiffies + (HZ/10));
 +
 +			/* Any skbuffs queued for refill? Force them out. */
 +			if (i != 0)
 +				goto refill;
 +			break;
 +		}
  
 +		skb_add_rx_frag(skb, 0, page, 0, 0, PAGE_SIZE);
 +		__skb_queue_tail(&np->rx_batch, skb);
 +	}
 +
 +	/* Is the batch large enough to be worthwhile? */
 +	if (i < (np->rx_target/2)) {
 +		if (req_prod > np->rx.sring->req_prod)
 +			goto push;
 +		return;
 +	}
 +
 +	/* Adjust our fill target if we risked running out of buffers. */
 +	if (((req_prod - np->rx.sring->rsp_prod) < (np->rx_target / 4)) &&
 +	    ((np->rx_target *= 2) > np->rx_max_target))
 +		np->rx_target = np->rx_max_target;
 +
 + refill:
 +	for (i = 0; ; i++) {
 +		skb = __skb_dequeue(&np->rx_batch);
 +		if (skb == NULL)
++=======
+ 		skb = xennet_alloc_one_rx_buffer(queue);
+ 		if (!skb) {
+ 			err = -ENOMEM;
++>>>>>>> 538d92912d31 (xen-netfront: Rework the fix for Rx stall during OOM and network stress)
  			break;
+ 		}
  
 -		id = xennet_rxidx(req_prod);
 +		skb->dev = dev;
  
 -		BUG_ON(queue->rx_skbs[id]);
 -		queue->rx_skbs[id] = skb;
 +		id = xennet_rxidx(req_prod + i);
  
 -		ref = gnttab_claim_grant_reference(&queue->gref_rx_head);
 -		WARN_ON_ONCE(IS_ERR_VALUE((unsigned long)(int)ref));
 -		queue->grant_rx_ref[id] = ref;
 +		BUG_ON(np->rx_skbs[id]);
 +		np->rx_skbs[id] = skb;
  
 -		page = skb_frag_page(&skb_shinfo(skb)->frags[0]);
 +		ref = gnttab_claim_grant_reference(&np->gref_rx_head);
 +		BUG_ON((signed short)ref < 0);
 +		np->grant_rx_ref[id] = ref;
 +
 +		pfn = page_to_pfn(skb_frag_page(&skb_shinfo(skb)->frags[0]));
 +		vaddr = page_address(skb_frag_page(&skb_shinfo(skb)->frags[0]));
 +
 +		req = RING_GET_REQUEST(&np->rx, req_prod + i);
 +		gnttab_grant_foreign_access_ref(ref,
 +						np->xbdev->otherend_id,
 +						pfn_to_mfn(pfn),
 +						0);
  
 -		req = RING_GET_REQUEST(&queue->rx, req_prod);
 -		gnttab_page_grant_foreign_access_ref_one(ref,
 -							 queue->info->xbdev->otherend_id,
 -							 page,
 -							 0);
  		req->id = id;
  		req->gref = ref;
  	}
  
++<<<<<<< HEAD
++=======
+ 	queue->rx.req_prod_pvt = req_prod;
+ 
+ 	/* Try again later if there are not enough requests or skb allocation
+ 	 * failed.
+ 	 * Enough requests is quantified as the sum of newly created slots and
+ 	 * the unconsumed slots at the backend.
+ 	 */
+ 	if (req_prod - queue->rx.rsp_cons < NET_RX_SLOTS_MIN ||
+ 	    unlikely(err)) {
+ 		mod_timer(&queue->rx_refill_timer, jiffies + (HZ/10));
+ 		return;
+ 	}
+ 
++>>>>>>> 538d92912d31 (xen-netfront: Rework the fix for Rx stall during OOM and network stress)
  	wmb();		/* barrier so backend seens requests */
  
 -	RING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&queue->rx, notify);
 +	/* Above is a suitable barrier to ensure backend will see requests. */
 +	np->rx.req_prod_pvt = req_prod + i;
 + push:
 +	RING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&np->rx, notify);
  	if (notify)
 -		notify_remote_via_irq(queue->rx_irq);
 +		notify_remote_via_irq(np->rx_irq);
  }
  
  static int xennet_open(struct net_device *dev)
* Unmerged path drivers/net/xen-netfront.c
