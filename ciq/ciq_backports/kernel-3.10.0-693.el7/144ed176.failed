i40e: correct check for reading TSYNINDX from the receive descriptor

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Jacob Keller <jacob.e.keller@intel.com>
commit 144ed1763003c6a88a4b788cc5da1d8a1ddf061f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/144ed176.failed

When hardware has taken a timestamp for a received packet, it indicates
which RXTIME register the timestamp was placed in by some bits in the
receive descriptor. It uses 3 bits, one to indicate if the descriptor
index is valid (ie: there was a timestamp) and 2 bits to indicate which
of the 4 registers to read. However, the driver currently does not check
the TSYNVALID bit and only checks the index. It assumes a zero index
means no timestamp, and a non zero index means a timestamp occurred.
While this appears to be true, it prevents ever reading a timestamp in
RXTIME[0], and causes the first timestamp the device captures to be
ignored.

Fix this by using the TSYNVALID bit correctly as the true indicator of
whether the packet has an associated timestamp.

Also rename the variable rsyn to tsyn as this is more descriptive and
matches the register names.

Change-ID: I4437e8f3a3df2c2ddb458b0fb61420f3dafc4c12
	Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 144ed1763003c6a88a4b788cc5da1d8a1ddf061f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_txrx.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.c
index b7b5a5cf06f9,c9eb6b852d66..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@@ -1476,13 -1392,347 +1476,347 @@@ static inline void i40e_rx_hash(struct 
  }
  
  /**
 - * i40e_process_skb_fields - Populate skb header fields from Rx descriptor
 - * @rx_ring: rx descriptor ring packet is being transacted on
 - * @rx_desc: pointer to the EOP Rx descriptor
 - * @skb: pointer to current skb being populated
 - * @rx_ptype: the packet type decoded by hardware
 + * i40e_clean_rx_irq_1buf - Reclaim resources after receive; single buffer
 + * @rx_ring:  rx ring to clean
 + * @budget:   how many cleans we're allowed
   *
 - * This function checks the ring, descriptor, and packet information in
 - * order to populate the hash, checksum, VLAN, protocol, and
 - * other fields within the skb.
 + * Returns number of packets cleaned
   **/
++<<<<<<< HEAD
 +static int i40e_clean_rx_irq_1buf(struct i40e_ring *rx_ring, int budget)
++=======
+ static inline
+ void i40e_process_skb_fields(struct i40e_ring *rx_ring,
+ 			     union i40e_rx_desc *rx_desc, struct sk_buff *skb,
+ 			     u8 rx_ptype)
+ {
+ 	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	u32 rx_status = (qword & I40E_RXD_QW1_STATUS_MASK) >>
+ 			I40E_RXD_QW1_STATUS_SHIFT;
+ 	u32 tsynvalid = rx_status & I40E_RXD_QW1_STATUS_TSYNVALID_MASK;
+ 	u32 tsyn = (rx_status & I40E_RXD_QW1_STATUS_TSYNINDX_MASK) >>
+ 		   I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT;
+ 
+ 	if (unlikely(tsynvalid)) {
+ 		i40e_ptp_rx_hwtstamp(rx_ring->vsi->back, skb, tsyn);
+ 		rx_ring->last_rx_timestamp = jiffies;
+ 	}
+ 
+ 	i40e_rx_hash(rx_ring, rx_desc, skb, rx_ptype);
+ 
+ 	/* modifies the skb - consumes the enet header */
+ 	skb->protocol = eth_type_trans(skb, rx_ring->netdev);
+ 
+ 	i40e_rx_checksum(rx_ring->vsi, skb, rx_desc);
+ 
+ 	skb_record_rx_queue(skb, rx_ring->queue_index);
+ }
+ 
+ /**
+  * i40e_pull_tail - i40e specific version of skb_pull_tail
+  * @rx_ring: rx descriptor ring packet is being transacted on
+  * @skb: pointer to current skb being adjusted
+  *
+  * This function is an i40e specific version of __pskb_pull_tail.  The
+  * main difference between this version and the original function is that
+  * this function can make several assumptions about the state of things
+  * that allow for significant optimizations versus the standard function.
+  * As a result we can do things like drop a frag and maintain an accurate
+  * truesize for the skb.
+  */
+ static void i40e_pull_tail(struct i40e_ring *rx_ring, struct sk_buff *skb)
+ {
+ 	struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+ 	unsigned char *va;
+ 	unsigned int pull_len;
+ 
+ 	/* it is valid to use page_address instead of kmap since we are
+ 	 * working with pages allocated out of the lomem pool per
+ 	 * alloc_page(GFP_ATOMIC)
+ 	 */
+ 	va = skb_frag_address(frag);
+ 
+ 	/* we need the header to contain the greater of either ETH_HLEN or
+ 	 * 60 bytes if the skb->len is less than 60 for skb_pad.
+ 	 */
+ 	pull_len = eth_get_headlen(va, I40E_RX_HDR_SIZE);
+ 
+ 	/* align pull length to size of long to optimize memcpy performance */
+ 	skb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));
+ 
+ 	/* update all of the pointers */
+ 	skb_frag_size_sub(frag, pull_len);
+ 	frag->page_offset += pull_len;
+ 	skb->data_len -= pull_len;
+ 	skb->tail += pull_len;
+ }
+ 
+ /**
+  * i40e_cleanup_headers - Correct empty headers
+  * @rx_ring: rx descriptor ring packet is being transacted on
+  * @skb: pointer to current skb being fixed
+  *
+  * Also address the case where we are pulling data in on pages only
+  * and as such no data is present in the skb header.
+  *
+  * In addition if skb is not at least 60 bytes we need to pad it so that
+  * it is large enough to qualify as a valid Ethernet frame.
+  *
+  * Returns true if an error was encountered and skb was freed.
+  **/
+ static bool i40e_cleanup_headers(struct i40e_ring *rx_ring, struct sk_buff *skb)
+ {
+ 	/* place header in linear portion of buffer */
+ 	if (skb_is_nonlinear(skb))
+ 		i40e_pull_tail(rx_ring, skb);
+ 
+ 	/* if eth_skb_pad returns an error the skb was freed */
+ 	if (eth_skb_pad(skb))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /**
+  * i40e_reuse_rx_page - page flip buffer and store it back on the ring
+  * @rx_ring: rx descriptor ring to store buffers on
+  * @old_buff: donor buffer to have page reused
+  *
+  * Synchronizes page for reuse by the adapter
+  **/
+ static void i40e_reuse_rx_page(struct i40e_ring *rx_ring,
+ 			       struct i40e_rx_buffer *old_buff)
+ {
+ 	struct i40e_rx_buffer *new_buff;
+ 	u16 nta = rx_ring->next_to_alloc;
+ 
+ 	new_buff = &rx_ring->rx_bi[nta];
+ 
+ 	/* update, and store next to alloc */
+ 	nta++;
+ 	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
+ 
+ 	/* transfer page from old buffer to new buffer */
+ 	*new_buff = *old_buff;
+ }
+ 
+ /**
+  * i40e_page_is_reserved - check if reuse is possible
+  * @page: page struct to check
+  */
+ static inline bool i40e_page_is_reserved(struct page *page)
+ {
+ 	return (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);
+ }
+ 
+ /**
+  * i40e_add_rx_frag - Add contents of Rx buffer to sk_buff
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_buffer: buffer containing page to add
+  * @rx_desc: descriptor containing length of buffer written by hardware
+  * @skb: sk_buff to place the data into
+  *
+  * This function will add the data contained in rx_buffer->page to the skb.
+  * This is done either through a direct copy if the data in the buffer is
+  * less than the skb header size, otherwise it will just attach the page as
+  * a frag to the skb.
+  *
+  * The function will then update the page offset if necessary and return
+  * true if the buffer can be reused by the adapter.
+  **/
+ static bool i40e_add_rx_frag(struct i40e_ring *rx_ring,
+ 			     struct i40e_rx_buffer *rx_buffer,
+ 			     union i40e_rx_desc *rx_desc,
+ 			     struct sk_buff *skb)
+ {
+ 	struct page *page = rx_buffer->page;
+ 	u64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);
+ 	unsigned int size = (qword & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>
+ 			    I40E_RXD_QW1_LENGTH_PBUF_SHIFT;
+ #if (PAGE_SIZE < 8192)
+ 	unsigned int truesize = I40E_RXBUFFER_2048;
+ #else
+ 	unsigned int truesize = ALIGN(size, L1_CACHE_BYTES);
+ 	unsigned int last_offset = PAGE_SIZE - I40E_RXBUFFER_2048;
+ #endif
+ 
+ 	/* will the data fit in the skb we allocated? if so, just
+ 	 * copy it as it is pretty small anyway
+ 	 */
+ 	if ((size <= I40E_RX_HDR_SIZE) && !skb_is_nonlinear(skb)) {
+ 		unsigned char *va = page_address(page) + rx_buffer->page_offset;
+ 
+ 		memcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));
+ 
+ 		/* page is not reserved, we can reuse buffer as-is */
+ 		if (likely(!i40e_page_is_reserved(page)))
+ 			return true;
+ 
+ 		/* this page cannot be reused so discard it */
+ 		__free_pages(page, 0);
+ 		return false;
+ 	}
+ 
+ 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
+ 			rx_buffer->page_offset, size, truesize);
+ 
+ 	/* avoid re-using remote pages */
+ 	if (unlikely(i40e_page_is_reserved(page)))
+ 		return false;
+ 
+ #if (PAGE_SIZE < 8192)
+ 	/* if we are only owner of page we can reuse it */
+ 	if (unlikely(page_count(page) != 1))
+ 		return false;
+ 
+ 	/* flip page offset to other buffer */
+ 	rx_buffer->page_offset ^= truesize;
+ #else
+ 	/* move offset up to the next cache line */
+ 	rx_buffer->page_offset += truesize;
+ 
+ 	if (rx_buffer->page_offset > last_offset)
+ 		return false;
+ #endif
+ 
+ 	/* Even if we own the page, we are not allowed to use atomic_set()
+ 	 * This would break get_page_unless_zero() users.
+ 	 */
+ 	get_page(rx_buffer->page);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_fetch_rx_buffer - Allocate skb and populate it
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_desc: descriptor containing info written by hardware
+  *
+  * This function allocates an skb on the fly, and populates it with the page
+  * data from the current receive descriptor, taking care to set up the skb
+  * correctly, as well as handling calling the page recycle function if
+  * necessary.
+  */
+ static inline
+ struct sk_buff *i40e_fetch_rx_buffer(struct i40e_ring *rx_ring,
+ 				     union i40e_rx_desc *rx_desc)
+ {
+ 	struct i40e_rx_buffer *rx_buffer;
+ 	struct sk_buff *skb;
+ 	struct page *page;
+ 
+ 	rx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];
+ 	page = rx_buffer->page;
+ 	prefetchw(page);
+ 
+ 	skb = rx_buffer->skb;
+ 
+ 	if (likely(!skb)) {
+ 		void *page_addr = page_address(page) + rx_buffer->page_offset;
+ 
+ 		/* prefetch first cache line of first page */
+ 		prefetch(page_addr);
+ #if L1_CACHE_BYTES < 128
+ 		prefetch(page_addr + L1_CACHE_BYTES);
+ #endif
+ 
+ 		/* allocate a skb to store the frags */
+ 		skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
+ 				       I40E_RX_HDR_SIZE,
+ 				       GFP_ATOMIC | __GFP_NOWARN);
+ 		if (unlikely(!skb)) {
+ 			rx_ring->rx_stats.alloc_buff_failed++;
+ 			return NULL;
+ 		}
+ 
+ 		/* we will be copying header into skb->data in
+ 		 * pskb_may_pull so it is in our interest to prefetch
+ 		 * it now to avoid a possible cache miss
+ 		 */
+ 		prefetchw(skb->data);
+ 	} else {
+ 		rx_buffer->skb = NULL;
+ 	}
+ 
+ 	/* we are reusing so sync this buffer for CPU use */
+ 	dma_sync_single_range_for_cpu(rx_ring->dev,
+ 				      rx_buffer->dma,
+ 				      rx_buffer->page_offset,
+ 				      I40E_RXBUFFER_2048,
+ 				      DMA_FROM_DEVICE);
+ 
+ 	/* pull page into skb */
+ 	if (i40e_add_rx_frag(rx_ring, rx_buffer, rx_desc, skb)) {
+ 		/* hand second half of page back to the ring */
+ 		i40e_reuse_rx_page(rx_ring, rx_buffer);
+ 		rx_ring->rx_stats.page_reuse_count++;
+ 	} else {
+ 		/* we are not reusing the buffer so unmap it */
+ 		dma_unmap_page(rx_ring->dev, rx_buffer->dma, PAGE_SIZE,
+ 			       DMA_FROM_DEVICE);
+ 	}
+ 
+ 	/* clear contents of buffer_info */
+ 	rx_buffer->page = NULL;
+ 
+ 	return skb;
+ }
+ 
+ /**
+  * i40e_is_non_eop - process handling of non-EOP buffers
+  * @rx_ring: Rx ring being processed
+  * @rx_desc: Rx descriptor for current buffer
+  * @skb: Current socket buffer containing buffer in progress
+  *
+  * This function updates next to clean.  If the buffer is an EOP buffer
+  * this function exits returning false, otherwise it will place the
+  * sk_buff in the next buffer to be chained and return true indicating
+  * that this is in fact a non-EOP buffer.
+  **/
+ static bool i40e_is_non_eop(struct i40e_ring *rx_ring,
+ 			    union i40e_rx_desc *rx_desc,
+ 			    struct sk_buff *skb)
+ {
+ 	u32 ntc = rx_ring->next_to_clean + 1;
+ 
+ 	/* fetch, update, and store next to clean */
+ 	ntc = (ntc < rx_ring->count) ? ntc : 0;
+ 	rx_ring->next_to_clean = ntc;
+ 
+ 	prefetch(I40E_RX_DESC(rx_ring, ntc));
+ 
+ #define staterrlen rx_desc->wb.qword1.status_error_len
+ 	if (unlikely(i40e_rx_is_programming_status(le64_to_cpu(staterrlen)))) {
+ 		i40e_clean_programming_status(rx_ring, rx_desc);
+ 		rx_ring->rx_bi[ntc].skb = skb;
+ 		return true;
+ 	}
+ 	/* if we are the last buffer then there is nothing else to do */
+ #define I40E_RXD_EOF BIT(I40E_RX_DESC_STATUS_EOF_SHIFT)
+ 	if (likely(i40e_test_staterr(rx_desc, I40E_RXD_EOF)))
+ 		return false;
+ 
+ 	/* place skb in next buffer to be received */
+ 	rx_ring->rx_bi[ntc].skb = skb;
+ 	rx_ring->rx_stats.non_eop_descs++;
+ 
+ 	return true;
+ }
+ 
+ /**
+  * i40e_clean_rx_irq - Clean completed descriptors from Rx ring - bounce buf
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @budget: Total limit on number of packets to process
+  *
+  * This function provides a "bounce buffer" approach to Rx interrupt
+  * processing.  The advantage to this is that on systems that have
+  * expensive overhead for IOMMU access this provides a means of avoiding
+  * it by maintaining the mapping of the page to the system.
+  *
+  * Returns amount of work completed
+  **/
+ static int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget)
++>>>>>>> 144ed1763003 (i40e: correct check for reading TSYNINDX from the receive descriptor)
  {
  	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
  	u16 cleaned_count = I40E_DESC_UNUSED(rx_ring);
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.c
