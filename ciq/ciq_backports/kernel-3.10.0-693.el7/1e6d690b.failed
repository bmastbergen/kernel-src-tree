md/r5cache: caching phase of r5cache

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [md] r5cache: caching phase of r5cache (Jes Sorensen) [1380016]
Rebuild_FUZZ: 95.65%
commit-author Song Liu <songliubraving@fb.com>
commit 1e6d690b9334b7e1b31d25fd8d93e980e449a5f9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/1e6d690b.failed

As described in previous patch, write back cache operates in two
phases: caching and writing-out. The caching phase works as:
1. write data to journal
   (r5c_handle_stripe_dirtying, r5c_cache_data)
2. call bio_endio
   (r5c_handle_data_cached, r5c_return_dev_pending_writes).

Then the writing-out phase is as:
1. Mark the stripe as write-out (r5c_make_stripe_write_out)
2. Calcualte parity (reconstruct or RMW)
3. Write parity (and maybe some other data) to journal device
4. Write data and parity to RAID disks

This patch implements caching phase. The cache is integrated with
stripe cache of raid456. It leverages code of r5l_log to write
data to journal device.

Writing-out phase of the cache is implemented in the next patch.

With r5cache, write operation does not wait for parity calculation
and write out, so the write latency is lower (1 write to journal
device vs. read and then write to raid disks). Also, r5cache will
reduce RAID overhead (multipile IO due to read-modify-write of
parity) and provide more opportunities of full stripe writes.

This patch adds 2 flags to stripe_head.state:
 - STRIPE_R5C_PARTIAL_STRIPE,
 - STRIPE_R5C_FULL_STRIPE,

Instead of inactive_list, stripes with cached data are tracked in
r5conf->r5c_full_stripe_list and r5conf->r5c_partial_stripe_list.
STRIPE_R5C_FULL_STRIPE and STRIPE_R5C_PARTIAL_STRIPE are flags for
stripes in these lists. Note: stripes in r5c_full/partial_stripe_list
are not considered as "active".

For RMW, the code allocates an extra page for each data block
being updated.  This is stored in r5dev->orig_page and the old data
is read into it.  Then the prexor calculation subtracts ->orig_page
from the parity block, and the reconstruct calculation adds the
->page data back into the parity block.

r5cache naturally excludes SkipCopy. When the array has write back
cache, async_copy_data() will not skip copy.

There are some known limitations of the cache implementation:

1. Write cache only covers full page writes (R5_OVERWRITE). Writes
   of smaller granularity are write through.
2. Only one log io (sh->log_io) for each stripe at anytime. Later
   writes for the same stripe have to wait. This can be improved by
   moving log_io to r5dev.
3. With writeback cache, read path must enter state machine, which
   is a significant bottleneck for some workloads.
4. There is no per stripe checkpoint (with r5l_payload_flush) in
   the log, so recovery code has to replay more than necessary data
   (sometimes all the log from last_checkpoint). This reduces
   availability of the array.

This patch includes a fix proposed by ZhengYuan Liu
<liuzhengyuan@kylinos.cn>

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 1e6d690b9334b7e1b31d25fd8d93e980e449a5f9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid5-cache.c
#	drivers/md/raid5.c
#	drivers/md/raid5.h
diff --cc drivers/md/raid5-cache.c
index f9e52fb68a1e,19c5af91bd1b..000000000000
--- a/drivers/md/raid5-cache.c
+++ b/drivers/md/raid5-cache.c
@@@ -176,6 -219,121 +177,124 @@@ static void __r5l_set_io_unit_state(str
  	io->state = state;
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ r5c_return_dev_pending_writes(struct r5conf *conf, struct r5dev *dev,
+ 			      struct bio_list *return_bi)
+ {
+ 	struct bio *wbi, *wbi2;
+ 
+ 	wbi = dev->written;
+ 	dev->written = NULL;
+ 	while (wbi && wbi->bi_iter.bi_sector <
+ 	       dev->sector + STRIPE_SECTORS) {
+ 		wbi2 = r5_next_bio(wbi, dev->sector);
+ 		if (!raid5_dec_bi_active_stripes(wbi)) {
+ 			md_write_end(conf->mddev);
+ 			bio_list_add(return_bi, wbi);
+ 		}
+ 		wbi = wbi2;
+ 	}
+ }
+ 
+ void r5c_handle_cached_data_endio(struct r5conf *conf,
+ 	  struct stripe_head *sh, int disks, struct bio_list *return_bi)
+ {
+ 	int i;
+ 
+ 	for (i = sh->disks; i--; ) {
+ 		if (sh->dev[i].written) {
+ 			set_bit(R5_UPTODATE, &sh->dev[i].flags);
+ 			r5c_return_dev_pending_writes(conf, &sh->dev[i],
+ 						      return_bi);
+ 			bitmap_endwrite(conf->mddev->bitmap, sh->sector,
+ 					STRIPE_SECTORS,
+ 					!test_bit(STRIPE_DEGRADED, &sh->state),
+ 					0);
+ 		}
+ 	}
+ }
+ 
+ /*
+  * Put the stripe into writing-out phase by clearing STRIPE_R5C_CACHING.
+  * This function should only be called in write-back mode.
+  */
+ static void r5c_make_stripe_write_out(struct stripe_head *sh)
+ {
+ 	struct r5conf *conf = sh->raid_conf;
+ 	struct r5l_log *log = conf->log;
+ 
+ 	BUG_ON(!r5c_is_writeback(log));
+ 
+ 	WARN_ON(!test_bit(STRIPE_R5C_CACHING, &sh->state));
+ 	clear_bit(STRIPE_R5C_CACHING, &sh->state);
+ 
+ 	if (!test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))
+ 		atomic_inc(&conf->preread_active_stripes);
+ 
+ 	if (test_and_clear_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state)) {
+ 		BUG_ON(atomic_read(&conf->r5c_cached_partial_stripes) == 0);
+ 		atomic_dec(&conf->r5c_cached_partial_stripes);
+ 	}
+ 
+ 	if (test_and_clear_bit(STRIPE_R5C_FULL_STRIPE, &sh->state)) {
+ 		BUG_ON(atomic_read(&conf->r5c_cached_full_stripes) == 0);
+ 		atomic_dec(&conf->r5c_cached_full_stripes);
+ 	}
+ }
+ 
+ static void r5c_handle_data_cached(struct stripe_head *sh)
+ {
+ 	int i;
+ 
+ 	for (i = sh->disks; i--; )
+ 		if (test_and_clear_bit(R5_Wantwrite, &sh->dev[i].flags)) {
+ 			set_bit(R5_InJournal, &sh->dev[i].flags);
+ 			clear_bit(R5_LOCKED, &sh->dev[i].flags);
+ 		}
+ 	clear_bit(STRIPE_LOG_TRAPPED, &sh->state);
+ }
+ 
+ /*
+  * this journal write must contain full parity,
+  * it may also contain some data pages
+  */
+ static void r5c_handle_parity_cached(struct stripe_head *sh)
+ {
+ 	int i;
+ 
+ 	for (i = sh->disks; i--; )
+ 		if (test_bit(R5_InJournal, &sh->dev[i].flags))
+ 			set_bit(R5_Wantwrite, &sh->dev[i].flags);
+ }
+ 
+ /*
+  * Setting proper flags after writing (or flushing) data and/or parity to the
+  * log device. This is called from r5l_log_endio() or r5l_log_flush_endio().
+  */
+ static void r5c_finish_cache_stripe(struct stripe_head *sh)
+ {
+ 	struct r5l_log *log = sh->raid_conf->log;
+ 
+ 	if (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH) {
+ 		BUG_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));
+ 		/*
+ 		 * Set R5_InJournal for parity dev[pd_idx]. This means
+ 		 * all data AND parity in the journal. For RAID 6, it is
+ 		 * NOT necessary to set the flag for dev[qd_idx], as the
+ 		 * two parities are written out together.
+ 		 */
+ 		set_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);
+ 	} else if (test_bit(STRIPE_R5C_CACHING, &sh->state)) {
+ 		r5c_handle_data_cached(sh);
+ 	} else {
+ 		r5c_handle_parity_cached(sh);
+ 		set_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);
+ 	}
+ }
+ 
++>>>>>>> 1e6d690b9334 (md/r5cache: caching phase of r5cache)
  static void r5l_io_run_stripes(struct r5l_io_unit *io)
  {
  	struct stripe_head *sh, *next;
@@@ -1123,6 -1286,191 +1244,194 @@@ static void r5l_write_super(struct r5l_
  	set_bit(MD_CHANGE_DEVS, &mddev->flags);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Try handle write operation in caching phase. This function should only
+  * be called in write-back mode.
+  *
+  * If all outstanding writes can be handled in caching phase, returns 0
+  * If writes requires write-out phase, call r5c_make_stripe_write_out()
+  * and returns -EAGAIN
+  */
+ int r5c_try_caching_write(struct r5conf *conf,
+ 			  struct stripe_head *sh,
+ 			  struct stripe_head_state *s,
+ 			  int disks)
+ {
+ 	struct r5l_log *log = conf->log;
+ 	int i;
+ 	struct r5dev *dev;
+ 	int to_cache = 0;
+ 
+ 	BUG_ON(!r5c_is_writeback(log));
+ 
+ 	if (!test_bit(STRIPE_R5C_CACHING, &sh->state)) {
+ 		/*
+ 		 * There are two different scenarios here:
+ 		 *  1. The stripe has some data cached, and it is sent to
+ 		 *     write-out phase for reclaim
+ 		 *  2. The stripe is clean, and this is the first write
+ 		 *
+ 		 * For 1, return -EAGAIN, so we continue with
+ 		 * handle_stripe_dirtying().
+ 		 *
+ 		 * For 2, set STRIPE_R5C_CACHING and continue with caching
+ 		 * write.
+ 		 */
+ 
+ 		/* case 1: anything injournal or anything in written */
+ 		if (s->injournal > 0 || s->written > 0)
+ 			return -EAGAIN;
+ 		/* case 2 */
+ 		set_bit(STRIPE_R5C_CACHING, &sh->state);
+ 	}
+ 
+ 	for (i = disks; i--; ) {
+ 		dev = &sh->dev[i];
+ 		/* if non-overwrite, use writing-out phase */
+ 		if (dev->towrite && !test_bit(R5_OVERWRITE, &dev->flags) &&
+ 		    !test_bit(R5_InJournal, &dev->flags)) {
+ 			r5c_make_stripe_write_out(sh);
+ 			return -EAGAIN;
+ 		}
+ 	}
+ 
+ 	for (i = disks; i--; ) {
+ 		dev = &sh->dev[i];
+ 		if (dev->towrite) {
+ 			set_bit(R5_Wantwrite, &dev->flags);
+ 			set_bit(R5_Wantdrain, &dev->flags);
+ 			set_bit(R5_LOCKED, &dev->flags);
+ 			to_cache++;
+ 		}
+ 	}
+ 
+ 	if (to_cache) {
+ 		set_bit(STRIPE_OP_BIODRAIN, &s->ops_request);
+ 		/*
+ 		 * set STRIPE_LOG_TRAPPED, which triggers r5c_cache_data()
+ 		 * in ops_run_io(). STRIPE_LOG_TRAPPED will be cleared in
+ 		 * r5c_handle_data_cached()
+ 		 */
+ 		set_bit(STRIPE_LOG_TRAPPED, &sh->state);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * free extra pages (orig_page) we allocated for prexor
+  */
+ void r5c_release_extra_page(struct stripe_head *sh)
+ {
+ 	int i;
+ 
+ 	for (i = sh->disks; i--; )
+ 		if (sh->dev[i].page != sh->dev[i].orig_page) {
+ 			struct page *p = sh->dev[i].orig_page;
+ 
+ 			sh->dev[i].orig_page = sh->dev[i].page;
+ 			put_page(p);
+ 		}
+ }
+ 
+ /*
+  * clean up the stripe (clear R5_InJournal for dev[pd_idx] etc.) after the
+  * stripe is committed to RAID disks.
+  */
+ void r5c_finish_stripe_write_out(struct r5conf *conf,
+ 				 struct stripe_head *sh,
+ 				 struct stripe_head_state *s)
+ {
+ 	int i;
+ 	int do_wakeup = 0;
+ 
+ 	if (!conf->log ||
+ 	    !test_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags))
+ 		return;
+ 
+ 	WARN_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));
+ 	clear_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);
+ 
+ 	if (conf->log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)
+ 		return;
+ 
+ 	for (i = sh->disks; i--; ) {
+ 		clear_bit(R5_InJournal, &sh->dev[i].flags);
+ 		if (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))
+ 			do_wakeup = 1;
+ 	}
+ 
+ 	/*
+ 	 * analyse_stripe() runs before r5c_finish_stripe_write_out(),
+ 	 * We updated R5_InJournal, so we also update s->injournal.
+ 	 */
+ 	s->injournal = 0;
+ 
+ 	if (test_and_clear_bit(STRIPE_FULL_WRITE, &sh->state))
+ 		if (atomic_dec_and_test(&conf->pending_full_writes))
+ 			md_wakeup_thread(conf->mddev->thread);
+ 
+ 	if (do_wakeup)
+ 		wake_up(&conf->wait_for_overlap);
+ }
+ 
+ int
+ r5c_cache_data(struct r5l_log *log, struct stripe_head *sh,
+ 	       struct stripe_head_state *s)
+ {
+ 	int pages = 0;
+ 	int reserve;
+ 	int i;
+ 	int ret = 0;
+ 
+ 	BUG_ON(!log);
+ 
+ 	for (i = 0; i < sh->disks; i++) {
+ 		void *addr;
+ 
+ 		if (!test_bit(R5_Wantwrite, &sh->dev[i].flags))
+ 			continue;
+ 		addr = kmap_atomic(sh->dev[i].page);
+ 		sh->dev[i].log_checksum = crc32c_le(log->uuid_checksum,
+ 						    addr, PAGE_SIZE);
+ 		kunmap_atomic(addr);
+ 		pages++;
+ 	}
+ 	WARN_ON(pages == 0);
+ 
+ 	/*
+ 	 * The stripe must enter state machine again to call endio, so
+ 	 * don't delay.
+ 	 */
+ 	clear_bit(STRIPE_DELAYED, &sh->state);
+ 	atomic_inc(&sh->count);
+ 
+ 	mutex_lock(&log->io_mutex);
+ 	/* meta + data */
+ 	reserve = (1 + pages) << (PAGE_SHIFT - 9);
+ 	if (!r5l_has_free_space(log, reserve)) {
+ 		spin_lock(&log->no_space_stripes_lock);
+ 		list_add_tail(&sh->log_list, &log->no_space_stripes);
+ 		spin_unlock(&log->no_space_stripes_lock);
+ 
+ 		r5l_wake_reclaim(log, reserve);
+ 	} else {
+ 		ret = r5l_log_stripe(log, sh, pages, 0);
+ 		if (ret) {
+ 			spin_lock_irq(&log->io_list_lock);
+ 			list_add_tail(&sh->log_list, &log->no_mem_stripes);
+ 			spin_unlock_irq(&log->io_list_lock);
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&log->io_mutex);
+ 	return 0;
+ }
+ 
+ 
++>>>>>>> 1e6d690b9334 (md/r5cache: caching phase of r5cache)
  static int r5l_load_log(struct r5l_log *log)
  {
  	struct md_rdev *rdev = log->rdev;
diff --cc drivers/md/raid5.c
index e4353594a601,f535ce2c267a..000000000000
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@@ -900,10 -860,19 +930,19 @@@ static void ops_run_io(struct stripe_he
  
  	might_sleep();
  
- 	if (r5l_write_stripe(conf->log, sh) == 0)
- 		return;
+ 	if (!test_bit(STRIPE_R5C_CACHING, &sh->state)) {
+ 		/* writing out phase */
+ 		if (r5l_write_stripe(conf->log, sh) == 0)
+ 			return;
+ 	} else {  /* caching phase */
+ 		if (test_bit(STRIPE_LOG_TRAPPED, &sh->state)) {
+ 			r5c_cache_data(conf->log, sh, s);
+ 			return;
+ 		}
+ 	}
+ 
  	for (i = disks; i--; ) {
 -		int op, op_flags = 0;
 +		int rw;
  		int replace_only = 0;
  		struct bio *bi, *rbi;
  		struct md_rdev *rdev, *rrdev = NULL;
@@@ -1115,11 -1083,11 +1154,11 @@@ again
  static struct dma_async_tx_descriptor *
  async_copy_data(int frombio, struct bio *bio, struct page **page,
  	sector_t sector, struct dma_async_tx_descriptor *tx,
- 	struct stripe_head *sh)
+ 	struct stripe_head *sh, int no_skipcopy)
  {
 -	struct bio_vec bvl;
 -	struct bvec_iter iter;
 +	struct bio_vec *bvl;
  	struct page *bio_page;
 +	int i;
  	int page_offset;
  	struct async_submit_ctl submit;
  	enum async_tx_flags flags = 0;
@@@ -1234,10 -1203,10 +1274,10 @@@ static void ops_run_biofill(struct stri
  			dev->read = rbi = dev->toread;
  			dev->toread = NULL;
  			spin_unlock_irq(&sh->stripe_lock);
 -			while (rbi && rbi->bi_iter.bi_sector <
 +			while (rbi && rbi->bi_sector <
  				dev->sector + STRIPE_SECTORS) {
  				tx = async_copy_data(0, rbi, &dev->page,
- 					dev->sector, tx, sh);
+ 						     dev->sector, tx, sh, 0);
  				rbi = r5_next_bio(rbi, dev->sector);
  			}
  		}
@@@ -4186,6 -4203,11 +4282,14 @@@ static void analyse_stripe(struct strip
  			if (rdev && !test_bit(Faulty, &rdev->flags))
  				do_recovery = 1;
  		}
++<<<<<<< HEAD
++=======
+ 
+ 		if (test_bit(R5_InJournal, &dev->flags))
+ 			s->injournal++;
+ 		if (test_bit(R5_InJournal, &dev->flags) && dev->written)
+ 			s->just_cached++;
++>>>>>>> 1e6d690b9334 (md/r5cache: caching phase of r5cache)
  	}
  	if (test_bit(STRIPE_SYNCING, &sh->state)) {
  		/* If there is a failed device being replaced,
diff --cc drivers/md/raid5.h
index 517d4b68a1be,73c183398e38..000000000000
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@@ -264,6 -264,7 +264,10 @@@ struct stripe_head_state 
  	int syncing, expanding, expanded, replacing;
  	int locked, uptodate, to_read, to_write, failed, written;
  	int to_fill, compute, req_compute, non_overwrite;
++<<<<<<< HEAD
++=======
+ 	int injournal, just_cached;
++>>>>>>> 1e6d690b9334 (md/r5cache: caching phase of r5cache)
  	int failed_num[2];
  	int p_failed, q_failed;
  	int dec_preread_active;
@@@ -345,7 -351,29 +349,33 @@@ enum 
  	STRIPE_BITMAP_PENDING,	/* Being added to bitmap, don't add
  				 * to batch yet.
  				 */
++<<<<<<< HEAD
 +	STRIPE_LOG_TRAPPED, /* trapped into log */
++=======
+ 	STRIPE_LOG_TRAPPED,	/* trapped into log (see raid5-cache.c)
+ 				 * this bit is used in two scenarios:
+ 				 *
+ 				 * 1. write-out phase
+ 				 *  set in first entry of r5l_write_stripe
+ 				 *  clear in second entry of r5l_write_stripe
+ 				 *  used to bypass logic in handle_stripe
+ 				 *
+ 				 * 2. caching phase
+ 				 *  set in r5c_try_caching_write()
+ 				 *  clear when journal write is done
+ 				 *  used to initiate r5c_cache_data()
+ 				 *  also used to bypass logic in handle_stripe
+ 				 */
+ 	STRIPE_R5C_CACHING,	/* the stripe is in caching phase
+ 				 * see more detail in the raid5-cache.c
+ 				 */
+ 	STRIPE_R5C_PARTIAL_STRIPE,	/* in r5c cache (to-be/being handled or
+ 					 * in conf->r5c_partial_stripe_list)
+ 					 */
+ 	STRIPE_R5C_FULL_STRIPE,	/* in r5c cache (to-be/being handled or
+ 				 * in conf->r5c_full_stripe_list)
+ 				 */
++>>>>>>> 1e6d690b9334 (md/r5cache: caching phase of r5cache)
  };
  
  #define STRIPE_EXPAND_SYNC_FLAGS \
@@@ -635,4 -744,16 +671,19 @@@ extern void r5l_stripe_write_finished(s
  extern int r5l_handle_flush_request(struct r5l_log *log, struct bio *bio);
  extern void r5l_quiesce(struct r5l_log *log, int state);
  extern bool r5l_log_disk_error(struct r5conf *conf);
++<<<<<<< HEAD
++=======
+ extern bool r5c_is_writeback(struct r5l_log *log);
+ extern int
+ r5c_try_caching_write(struct r5conf *conf, struct stripe_head *sh,
+ 		      struct stripe_head_state *s, int disks);
+ extern void
+ r5c_finish_stripe_write_out(struct r5conf *conf, struct stripe_head *sh,
+ 			    struct stripe_head_state *s);
+ extern void r5c_release_extra_page(struct stripe_head *sh);
+ extern void r5c_handle_cached_data_endio(struct r5conf *conf,
+ 	struct stripe_head *sh, int disks, struct bio_list *return_bi);
+ extern int r5c_cache_data(struct r5l_log *log, struct stripe_head *sh,
+ 			  struct stripe_head_state *s);
++>>>>>>> 1e6d690b9334 (md/r5cache: caching phase of r5cache)
  #endif
* Unmerged path drivers/md/raid5-cache.c
* Unmerged path drivers/md/raid5.c
* Unmerged path drivers/md/raid5.h
