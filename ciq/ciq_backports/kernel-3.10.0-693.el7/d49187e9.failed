nvme: introduce struct nvme_request

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Christoph Hellwig <hch@lst.de>
commit d49187e97e94e2eb613cb6fed810356972077cc3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/d49187e9.failed

This adds a shared per-request structure for all NVMe I/O.  This structure
is embedded as the first member in all NVMe transport drivers request
private data and allows to implement common functionality between the
drivers.

The first use is to replace the current abuse of the SCSI command
passthrough fields in struct request for the NVMe command passthrough,
but it will grow a field more fields to allow implementing things
like common abort handlers in the future.

The passthrough commands are handled by having a pointer to the SQE
(struct nvme_command) in struct nvme_request, and the union of the
possible result fields, which had to be turned from an anonymous
into a named union for that purpose.  This avoids having to pass
a reference to a full CQE around and thus makes checking the result
a lot more lightweight.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit d49187e97e94e2eb613cb6fed810356972077cc3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/core.c
#	drivers/nvme/host/fabrics.c
#	drivers/nvme/host/lightnvm.c
#	drivers/nvme/host/nvme.h
#	drivers/nvme/host/pci.c
#	drivers/nvme/host/rdma.c
#	drivers/nvme/target/core.c
#	drivers/nvme/target/fabrics-cmd.c
#	drivers/nvme/target/loop.c
#	drivers/nvme/target/nvmet.h
#	include/linux/nvme.h
diff --cc drivers/nvme/host/core.c
index 0588703d149f,2fd632bcd975..000000000000
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@@ -121,23 -221,124 +121,141 @@@ struct request *nvme_alloc_request(stru
  
  	req->cmd_type = REQ_TYPE_DRV_PRIV;
  	req->cmd_flags |= REQ_FAILFAST_DRIVER;
++<<<<<<< HEAD
 +	req->__data_len = 0;
 +	req->__sector = (sector_t) -1;
 +	req->bio = req->biotail = NULL;
 +
 +	req->cmd = (unsigned char *)cmd;
 +	req->cmd_len = sizeof(struct nvme_command);
 +	req->special = (void *)0;
 +
 +	return req;
 +}
++=======
+ 	nvme_req(req)->cmd = cmd;
+ 
+ 	return req;
+ }
+ EXPORT_SYMBOL_GPL(nvme_alloc_request);
+ 
+ static inline void nvme_setup_flush(struct nvme_ns *ns,
+ 		struct nvme_command *cmnd)
+ {
+ 	memset(cmnd, 0, sizeof(*cmnd));
+ 	cmnd->common.opcode = nvme_cmd_flush;
+ 	cmnd->common.nsid = cpu_to_le32(ns->ns_id);
+ }
+ 
+ static inline int nvme_setup_discard(struct nvme_ns *ns, struct request *req,
+ 		struct nvme_command *cmnd)
+ {
+ 	struct nvme_dsm_range *range;
+ 	struct page *page;
+ 	int offset;
+ 	unsigned int nr_bytes = blk_rq_bytes(req);
+ 
+ 	range = kmalloc(sizeof(*range), GFP_ATOMIC);
+ 	if (!range)
+ 		return BLK_MQ_RQ_QUEUE_BUSY;
+ 
+ 	range->cattr = cpu_to_le32(0);
+ 	range->nlb = cpu_to_le32(nr_bytes >> ns->lba_shift);
+ 	range->slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+ 
+ 	memset(cmnd, 0, sizeof(*cmnd));
+ 	cmnd->dsm.opcode = nvme_cmd_dsm;
+ 	cmnd->dsm.nsid = cpu_to_le32(ns->ns_id);
+ 	cmnd->dsm.nr = 0;
+ 	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
+ 
+ 	req->completion_data = range;
+ 	page = virt_to_page(range);
+ 	offset = offset_in_page(range);
+ 	blk_add_request_payload(req, page, offset, sizeof(*range));
+ 
+ 	/*
+ 	 * we set __data_len back to the size of the area to be discarded
+ 	 * on disk. This allows us to report completion on the full amount
+ 	 * of blocks described by the request.
+ 	 */
+ 	req->__data_len = nr_bytes;
+ 
+ 	return 0;
+ }
+ 
+ static inline void nvme_setup_rw(struct nvme_ns *ns, struct request *req,
+ 		struct nvme_command *cmnd)
+ {
+ 	u16 control = 0;
+ 	u32 dsmgmt = 0;
+ 
+ 	if (req->cmd_flags & REQ_FUA)
+ 		control |= NVME_RW_FUA;
+ 	if (req->cmd_flags & (REQ_FAILFAST_DEV | REQ_RAHEAD))
+ 		control |= NVME_RW_LR;
+ 
+ 	if (req->cmd_flags & REQ_RAHEAD)
+ 		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
+ 
+ 	memset(cmnd, 0, sizeof(*cmnd));
+ 	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
+ 	cmnd->rw.command_id = req->tag;
+ 	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
+ 	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+ 	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
+ 
+ 	if (ns->ms) {
+ 		switch (ns->pi_type) {
+ 		case NVME_NS_DPS_PI_TYPE3:
+ 			control |= NVME_RW_PRINFO_PRCHK_GUARD;
+ 			break;
+ 		case NVME_NS_DPS_PI_TYPE1:
+ 		case NVME_NS_DPS_PI_TYPE2:
+ 			control |= NVME_RW_PRINFO_PRCHK_GUARD |
+ 					NVME_RW_PRINFO_PRCHK_REF;
+ 			cmnd->rw.reftag = cpu_to_le32(
+ 					nvme_block_nr(ns, blk_rq_pos(req)));
+ 			break;
+ 		}
+ 		if (!blk_integrity_rq(req))
+ 			control |= NVME_RW_PRINFO_PRACT;
+ 	}
+ 
+ 	cmnd->rw.control = cpu_to_le16(control);
+ 	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
+ }
+ 
+ int nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
+ 		struct nvme_command *cmd)
+ {
+ 	int ret = 0;
+ 
+ 	if (req->cmd_type == REQ_TYPE_DRV_PRIV)
+ 		memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
+ 	else if (req_op(req) == REQ_OP_FLUSH)
+ 		nvme_setup_flush(ns, cmd);
+ 	else if (req_op(req) == REQ_OP_DISCARD)
+ 		ret = nvme_setup_discard(ns, req, cmd);
+ 	else
+ 		nvme_setup_rw(ns, req, cmd);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(nvme_setup_cmd);
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  
  /*
   * Returns 0 on success.  If the result is negative, it's a Linux error code;
   * if the result is positive, it's an NVM Express status code
   */
  int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
++<<<<<<< HEAD
 +		void *buffer, unsigned bufflen, u32 *result, unsigned timeout)
++=======
+ 		union nvme_result *result, void *buffer, unsigned bufflen,
+ 		unsigned timeout, int qid, int at_head, int flags)
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  {
  	struct request *req;
  	int ret;
@@@ -154,9 -355,9 +272,15 @@@
  			goto out;
  	}
  
++<<<<<<< HEAD
 +	blk_execute_rq(req->q, NULL, req, 0);
 +	if (result)
 +		*result = (u32)(uintptr_t)req->special;
++=======
+ 	blk_execute_rq(req->q, NULL, req, at_head);
+ 	if (result)
+ 		*result = nvme_req(req)->result;
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  	ret = req->errors;
   out:
  	blk_mq_free_request(req);
@@@ -174,7 -378,7 +298,11 @@@ int __nvme_submit_user_cmd(struct reque
  		void __user *meta_buffer, unsigned meta_len, u32 meta_seed,
  		u32 *result, unsigned timeout)
  {
++<<<<<<< HEAD
 +	bool write = cmd->common.opcode & 1;
++=======
+ 	bool write = nvme_is_write(cmd);
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  	struct nvme_ns *ns = q->queuedata;
  	struct gendisk *disk = ns ? ns->disk : NULL;
  	struct request *req;
@@@ -240,7 -445,7 +368,11 @@@
  	blk_execute_rq(req->q, disk, req, 0);
  	ret = req->errors;
  	if (result)
++<<<<<<< HEAD
 +		*result = (u32)(uintptr_t)req->special;
++=======
+ 		*result = le32_to_cpu(nvme_req(req)->result.u32);
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  	if (meta && !ret && !write) {
  		if (copy_to_user(meta_buffer, meta, meta_len))
  			ret = -EFAULT;
@@@ -316,23 -591,30 +448,41 @@@ int nvme_identify_ns(struct nvme_ctrl *
  }
  
  int nvme_get_features(struct nvme_ctrl *dev, unsigned fid, unsigned nsid,
 -		      void *buffer, size_t buflen, u32 *result)
 +					dma_addr_t dma_addr, u32 *result)
  {
  	struct nvme_command c;
++<<<<<<< HEAD
++=======
+ 	union nvme_result res;
+ 	int ret;
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  
  	memset(&c, 0, sizeof(c));
  	c.features.opcode = nvme_admin_get_features;
  	c.features.nsid = cpu_to_le32(nsid);
 +	c.features.prp1 = cpu_to_le64(dma_addr);
  	c.features.fid = cpu_to_le32(fid);
  
++<<<<<<< HEAD
 +	return __nvme_submit_sync_cmd(dev->admin_q, &c, NULL, 0, result, 0);
++=======
+ 	ret = __nvme_submit_sync_cmd(dev->admin_q, &c, &res, buffer, buflen, 0,
+ 			NVME_QID_ANY, 0, 0);
+ 	if (ret >= 0 && result)
+ 		*result = le32_to_cpu(res.u32);
+ 	return ret;
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  }
  
  int nvme_set_features(struct nvme_ctrl *dev, unsigned fid, unsigned dword11,
 -		      void *buffer, size_t buflen, u32 *result)
 +					dma_addr_t dma_addr, u32 *result)
  {
  	struct nvme_command c;
++<<<<<<< HEAD
++=======
+ 	union nvme_result res;
+ 	int ret;
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  
  	memset(&c, 0, sizeof(c));
  	c.features.opcode = nvme_admin_set_features;
@@@ -340,7 -621,11 +490,15 @@@
  	c.features.fid = cpu_to_le32(fid);
  	c.features.dword11 = cpu_to_le32(dword11);
  
++<<<<<<< HEAD
 +	return __nvme_submit_sync_cmd(dev->admin_q, &c, NULL, 0, result, 0);
++=======
+ 	ret = __nvme_submit_sync_cmd(dev->admin_q, &c, &res,
+ 			buffer, buflen, 0, NVME_QID_ANY, 0, 0);
+ 	if (ret >= 0 && result)
+ 		*result = le32_to_cpu(res.u32);
+ 	return ret;
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  }
  
  int nvme_get_log_page(struct nvme_ctrl *dev, struct nvme_smart_log **log)
@@@ -1393,11 -1865,67 +1551,63 @@@ void nvme_remove_namespaces(struct nvme
  {
  	struct nvme_ns *ns, *next;
  
 -	/*
 -	 * The dead states indicates the controller was not gracefully
 -	 * disconnected. In that case, we won't be able to flush any data while
 -	 * removing the namespaces' disks; fail all the queues now to avoid
 -	 * potentially having to clean up the failed sync later.
 -	 */
 -	if (ctrl->state == NVME_CTRL_DEAD)
 -		nvme_kill_queues(ctrl);
 -
 +	mutex_lock(&ctrl->namespaces_mutex);
  	list_for_each_entry_safe(ns, next, &ctrl->namespaces, list)
  		nvme_ns_remove(ns);
 +	mutex_unlock(&ctrl->namespaces_mutex);
  }
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL_GPL(nvme_remove_namespaces);
+ 
+ static void nvme_async_event_work(struct work_struct *work)
+ {
+ 	struct nvme_ctrl *ctrl =
+ 		container_of(work, struct nvme_ctrl, async_event_work);
+ 
+ 	spin_lock_irq(&ctrl->lock);
+ 	while (ctrl->event_limit > 0) {
+ 		int aer_idx = --ctrl->event_limit;
+ 
+ 		spin_unlock_irq(&ctrl->lock);
+ 		ctrl->ops->submit_async_event(ctrl, aer_idx);
+ 		spin_lock_irq(&ctrl->lock);
+ 	}
+ 	spin_unlock_irq(&ctrl->lock);
+ }
+ 
+ void nvme_complete_async_event(struct nvme_ctrl *ctrl,
+ 		struct nvme_completion *cqe)
+ {
+ 	u16 status = le16_to_cpu(cqe->status) >> 1;
+ 	u32 result = le32_to_cpu(cqe->result.u32);
+ 
+ 	if (status == NVME_SC_SUCCESS || status == NVME_SC_ABORT_REQ) {
+ 		++ctrl->event_limit;
+ 		schedule_work(&ctrl->async_event_work);
+ 	}
+ 
+ 	if (status != NVME_SC_SUCCESS)
+ 		return;
+ 
+ 	switch (result & 0xff07) {
+ 	case NVME_AER_NOTICE_NS_CHANGED:
+ 		dev_info(ctrl->device, "rescanning\n");
+ 		nvme_queue_scan(ctrl);
+ 		break;
+ 	default:
+ 		dev_warn(ctrl->device, "async event result %08x\n", result);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(nvme_complete_async_event);
+ 
+ void nvme_queue_async_events(struct nvme_ctrl *ctrl)
+ {
+ 	ctrl->event_limit = NVME_NR_AERS;
+ 	schedule_work(&ctrl->async_event_work);
+ }
+ EXPORT_SYMBOL_GPL(nvme_queue_async_events);
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  
  static DEFINE_IDA(nvme_instance_ida);
  
diff --cc drivers/nvme/host/nvme.h
index ddd7fc3f3881,5e64957a9b96..000000000000
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@@ -251,7 -290,10 +265,12 @@@ void nvme_requeue_req(struct request *r
  int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
  		void *buf, unsigned bufflen);
  int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
++<<<<<<< HEAD
 +		void *buffer, unsigned bufflen,  u32 *result, unsigned timeout);
++=======
+ 		union nvme_result *result, void *buffer, unsigned bufflen,
+ 		unsigned timeout, int qid, int at_head, int flags);
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  int nvme_submit_user_cmd(struct request_queue *q, struct nvme_command *cmd,
  		void __user *ubuffer, unsigned bufflen, u32 *result,
  		unsigned timeout);
diff --cc drivers/nvme/host/pci.c
index 82e66e32363e,de8e0505d979..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -699,11 -708,8 +700,16 @@@ static int nvme_process_cq(struct nvme_
  		}
  
  		req = blk_mq_tag_to_rq(*nvmeq->tags, cqe.command_id);
++<<<<<<< HEAD
 +		if (req->cmd_type == REQ_TYPE_DRV_PRIV) {
 +			u32 result = le32_to_cpu(cqe.result);
 +			req->special = (void *)(uintptr_t)result;
 +		}
 +		blk_mq_complete_request(req, status >> 1);
++=======
+ 		nvme_req(req)->result = cqe.result;
+ 		blk_mq_complete_request(req, le16_to_cpu(cqe.status) >> 1);
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  
  	}
  
diff --cc include/linux/nvme.h
index 054f510fd0d0,18ce9f7cc881..000000000000
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@@ -671,8 -946,14 +671,19 @@@ enum 
  };
  
  struct nvme_completion {
++<<<<<<< HEAD
 +	__le32	result;		/* Used by admin commands to return data */
 +	__u32	rsvd;
++=======
+ 	/*
+ 	 * Used by Admin and Fabrics commands to return data:
+ 	 */
+ 	union nvme_result {
+ 		__le16	u16;
+ 		__le32	u32;
+ 		__le64	u64;
+ 	} result;
++>>>>>>> d49187e97e94 (nvme: introduce struct nvme_request)
  	__le16	sq_head;	/* how much of this queue may be reclaimed */
  	__le16	sq_id;		/* submission queue that generated this entry */
  	__u16	command_id;	/* of the command which completed */
* Unmerged path drivers/nvme/host/fabrics.c
* Unmerged path drivers/nvme/host/lightnvm.c
* Unmerged path drivers/nvme/host/rdma.c
* Unmerged path drivers/nvme/target/core.c
* Unmerged path drivers/nvme/target/fabrics-cmd.c
* Unmerged path drivers/nvme/target/loop.c
* Unmerged path drivers/nvme/target/nvmet.h
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/fabrics.c
* Unmerged path drivers/nvme/host/lightnvm.c
* Unmerged path drivers/nvme/host/nvme.h
* Unmerged path drivers/nvme/host/pci.c
* Unmerged path drivers/nvme/host/rdma.c
* Unmerged path drivers/nvme/target/core.c
* Unmerged path drivers/nvme/target/fabrics-cmd.c
* Unmerged path drivers/nvme/target/loop.c
* Unmerged path drivers/nvme/target/nvmet.h
* Unmerged path include/linux/nvme.h
