block: Copy a user iovec if it includes gaps

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [block] Copy a user iovec if it includes gaps (Jeff Moyer) [1421263]
Rebuild_FUZZ: 91.36%
commit-author Sagi Grimberg <sagig@mellanox.com>
commit 46348456c1791053dcbe5a9e21825b10a3c8a8fb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/46348456.failed

For drivers that don't support gaps in the SG lists handed to
them we must bounce (copy the user buffers) and pass a bio that
does not include gaps. This doesn't matter for any current user,
but will help to allow iser which can't handle gaps to use the
block virtual boundary instead of using driver-local bounce
buffering when handling SG_IO commands.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 46348456c1791053dcbe5a9e21825b10a3c8a8fb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-map.c
diff --cc block/blk-map.c
index 623e1cd4cffe,f565e11f465a..000000000000
--- a/block/blk-map.c
+++ b/block/blk-map.c
@@@ -188,34 -79,38 +206,43 @@@ EXPORT_SYMBOL(blk_rq_map_user)
   *    unmapping.
   */
  int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 -			struct rq_map_data *map_data,
 -			const struct iov_iter *iter, gfp_t gfp_mask)
 +			struct rq_map_data *map_data, struct sg_iovec *iov,
 +			int iov_count, unsigned int len, gfp_t gfp_mask)
  {
  	struct bio *bio;
 +	int i, read = rq_data_dir(rq) == READ;
  	int unaligned = 0;
++<<<<<<< HEAD
++=======
+ 	struct iov_iter i;
+ 	struct iovec iov, prv = {.iov_base = NULL, .iov_len = 0};
++>>>>>>> 46348456c179 (block: Copy a user iovec if it includes gaps)
  
 -	if (!iter || !iter->count)
 +	if (!iov || iov_count <= 0)
  		return -EINVAL;
  
 -	iov_for_each(iov, i, *iter) {
 -		unsigned long uaddr = (unsigned long) iov.iov_base;
 +	for (i = 0; i < iov_count; i++) {
 +		unsigned long uaddr = (unsigned long)iov[i].iov_base;
  
 -		if (!iov.iov_len)
 +		if (!iov[i].iov_len)
  			return -EINVAL;
  
  		/*
  		 * Keep going so we check length of all segments
  		 */
- 		if (uaddr & queue_dma_alignment(q))
+ 		if ((uaddr & queue_dma_alignment(q)) ||
+ 		    iovec_gap_to_prv(q, &prv, &iov))
  			unaligned = 1;
+ 
+ 		prv.iov_base = iov.iov_base;
+ 		prv.iov_len = iov.iov_len;
  	}
  
 -	if (unaligned || (q->dma_pad_mask & iter->count) || map_data)
 -		bio = bio_copy_user_iov(q, map_data, iter, gfp_mask);
 +	if (unaligned || (q->dma_pad_mask & len) || map_data)
 +		bio = bio_copy_user_iov(q, map_data, iov, iov_count, read,
 +					gfp_mask);
  	else
 -		bio = bio_map_user_iov(q, iter, gfp_mask);
 +		bio = bio_map_user_iov(q, NULL, iov, iov_count, read, gfp_mask);
  
  	if (IS_ERR(bio))
  		return PTR_ERR(bio);
* Unmerged path block/blk-map.c
