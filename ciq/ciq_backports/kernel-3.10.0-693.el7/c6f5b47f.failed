xprtrdma: Shrink send SGEs array

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit c6f5b47f9fdeef12c0896e5af4bb3416c97d91c4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/c6f5b47f.failed

We no longer need to accommodate an xdr_buf whose pages start at an
offset and cross extra page boundaries. If there are more partial or
whole pages to send than there are available SGEs, the marshaling
logic is now smart enough to use a Read chunk instead of failing.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit c6f5b47f9fdeef12c0896e5af4bb3416c97d91c4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/xprt_rdma.h
diff --cc net/sunrpc/xprtrdma/xprt_rdma.h
index ae3921a9fec6,852dd0a750a5..000000000000
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@@ -285,21 -305,34 +285,39 @@@ struct rpcrdma_mr_seg {		/* chunk descr
  	char		*mr_offset;	/* kva if no page, else offset */
  };
  
++<<<<<<< HEAD
 +#define RPCRDMA_MAX_IOVS	(2)
++=======
+ /* The Send SGE array is provisioned to send a maximum size
+  * inline request:
+  * - RPC-over-RDMA header
+  * - xdr_buf head iovec
+  * - RPCRDMA_MAX_INLINE bytes, in pages
+  * - xdr_buf tail iovec
+  *
+  * The actual number of array elements consumed by each RPC
+  * depends on the device's max_sge limit.
+  */
+ enum {
+ 	RPCRDMA_MIN_SEND_SGES = 3,
+ 	RPCRDMA_MAX_PAGE_SGES = RPCRDMA_MAX_INLINE >> PAGE_SHIFT,
+ 	RPCRDMA_MAX_SEND_SGES = 1 + 1 + RPCRDMA_MAX_PAGE_SGES + 1,
+ };
++>>>>>>> c6f5b47f9fde (xprtrdma: Shrink send SGEs array)
  
 -struct rpcrdma_buffer;
  struct rpcrdma_req {
  	struct list_head	rl_free;
 -	unsigned int		rl_mapped_sges;
 +	unsigned int		rl_niovs;
 +	unsigned int		rl_nchunks;
  	unsigned int		rl_connect_cookie;
 +	struct rpc_task		*rl_task;
  	struct rpcrdma_buffer	*rl_buffer;
 -	struct rpcrdma_rep	*rl_reply;
 -	struct ib_send_wr	rl_send_wr;
 -	struct ib_sge		rl_send_sge[RPCRDMA_MAX_SEND_SGES];
 -	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
 -	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */
 -	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
 +	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 +	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
 +	struct rpcrdma_regbuf	*rl_rdmabuf;
 +	struct rpcrdma_regbuf	*rl_sendbuf;
 +	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 +	struct rpcrdma_mr_seg	*rl_nextseg;
  
  	struct ib_cqe		rl_cqe;
  	struct list_head	rl_all;
* Unmerged path net/sunrpc/xprtrdma/xprt_rdma.h
