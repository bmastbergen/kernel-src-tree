scsi: lpfc: NVME Target: Receive buffer updates

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [scsi] lpfc: NVME Target: Receive buffer updates (Ewan Milne) [1384922]
Rebuild_FUZZ: 93.18%
commit-author James Smart <jsmart2021@gmail.com>
commit 2d7dbc4c2775eb30df97be00090adbfcc7fc5086
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/2d7dbc4c.failed

NVME Target: Receive buffer updates

Allocates buffer pools and configures adapter interfaces to handle
receive buffer (asynchronous FCP CMD ius, first burst data)
from the adapter. Splits by protocol, etc.

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 2d7dbc4c2775eb30df97be00090adbfcc7fc5086)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc.h
#	drivers/scsi/lpfc/lpfc_attr.c
#	drivers/scsi/lpfc/lpfc_debugfs.c
#	drivers/scsi/lpfc/lpfc_hw4.h
#	drivers/scsi/lpfc/lpfc_init.c
#	drivers/scsi/lpfc/lpfc_mbox.c
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/lpfc/lpfc_sli4.h
diff --cc drivers/scsi/lpfc/lpfc.h
index 79f57d03022e,8a4090c6771c..000000000000
--- a/drivers/scsi/lpfc/lpfc.h
+++ b/drivers/scsi/lpfc/lpfc.h
@@@ -723,9 -767,16 +723,20 @@@ struct lpfc_hba 
  	uint32_t cfg_fcp_imax;
  	uint32_t cfg_fcp_cpu_map;
  	uint32_t cfg_fcp_io_channel;
++<<<<<<< HEAD
++=======
+ 	uint32_t cfg_suppress_rsp;
+ 	uint32_t cfg_nvme_oas;
+ 	uint32_t cfg_nvme_io_channel;
+ 	uint32_t cfg_nvmet_mrq;
+ 	uint32_t cfg_nvmet_mrq_post;
+ 	uint32_t cfg_enable_nvmet;
+ 	uint32_t cfg_nvme_enable_fb;
+ 	uint32_t cfg_nvmet_fb_size;
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	uint32_t cfg_total_seg_cnt;
  	uint32_t cfg_sg_seg_cnt;
 +	uint32_t cfg_prot_sg_seg_cnt;
  	uint32_t cfg_sg_dma_buf_size;
  	uint64_t cfg_soft_wwnn;
  	uint64_t cfg_soft_wwpn;
diff --cc drivers/scsi/lpfc/lpfc_attr.c
index c5f7d60f9dff,700a68f303f3..000000000000
--- a/drivers/scsi/lpfc/lpfc_attr.c
+++ b/drivers/scsi/lpfc/lpfc_attr.c
@@@ -48,11 -52,16 +48,15 @@@
  #include "lpfc_compat.h"
  #include "lpfc_crtn.h"
  #include "lpfc_vport.h"
 -#include "lpfc_attr.h"
  
 -#define LPFC_DEF_DEVLOSS_TMO	30
 -#define LPFC_MIN_DEVLOSS_TMO	1
 -#define LPFC_MAX_DEVLOSS_TMO	255
 +#define LPFC_DEF_DEVLOSS_TMO 30
 +#define LPFC_MIN_DEVLOSS_TMO 1
 +#define LPFC_MAX_DEVLOSS_TMO 255
  
+ #define LPFC_DEF_MRQ_POST	256
+ #define LPFC_MIN_MRQ_POST	32
+ #define LPFC_MAX_MRQ_POST	512
+ 
  /*
   * Write key size should be multiple of 4. If write key is changed
   * make sure that library write key is also changed.
@@@ -3110,6 -3277,59 +3114,62 @@@ static DEVICE_ATTR(lpfc_devloss_tmo, S_
  		   lpfc_devloss_tmo_show, lpfc_devloss_tmo_store);
  
  /*
++<<<<<<< HEAD
++=======
+  * lpfc_suppress_rsp: Enable suppress rsp feature is firmware supports it
+  * lpfc_suppress_rsp = 0  Disable
+  * lpfc_suppress_rsp = 1  Enable (default)
+  *
+  */
+ LPFC_ATTR_R(suppress_rsp, 1, 0, 1,
+ 	    "Enable suppress rsp feature is firmware supports it");
+ 
+ /*
+  * lpfc_nvmet_mrq: Specify number of RQ pairs for processing NVMET cmds
+  * lpfc_nvmet_mrq = 1  use a single RQ pair
+  * lpfc_nvmet_mrq >= 2  use specified RQ pairs for MRQ
+  *
+  */
+ LPFC_ATTR_R(nvmet_mrq,
+ 	    1, 1, 16,
+ 	    "Specify number of RQ pairs for processing NVMET cmds");
+ 
+ /*
+  * lpfc_nvmet_mrq_post: Specify number buffers to post on every MRQ
+  *
+  */
+ LPFC_ATTR_R(nvmet_mrq_post, LPFC_DEF_MRQ_POST,
+ 	    LPFC_MIN_MRQ_POST, LPFC_MAX_MRQ_POST,
+ 	    "Specify number of buffers to post on every MRQ");
+ 
+ /*
+  * lpfc_enable_fc4_type: Defines what FC4 types are supported.
+  * Supported Values:  1 - register just FCP
+  *                    3 - register both FCP and NVME
+  * Supported values are [1,3]. Default value is 3
+  */
+ LPFC_ATTR_R(enable_fc4_type, LPFC_ENABLE_BOTH,
+ 	    LPFC_ENABLE_FCP, LPFC_ENABLE_BOTH,
+ 	    "Define fc4 type to register with fabric.");
+ 
+ /*
+  * lpfc_xri_split: Defines the division of XRI resources between SCSI and NVME
+  * This parameter is only used if:
+  *     lpfc_enable_fc4_type is 3 - register both FCP and NVME and
+  *     port is not configured for NVMET.
+  *
+  * ELS/CT always get 10% of XRIs, up to a maximum of 250
+  * The remaining XRIs get split up based on lpfc_xri_split per port:
+  *
+  * Supported Values are in percentages
+  * the xri_split value is the percentage the SCSI port will get. The remaining
+  * percentage will go to NVME.
+  */
+ LPFC_ATTR_R(xri_split, 50, 10, 90,
+ 	     "Division of XRI resources between SCSI and NVME");
+ 
+ /*
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  # lpfc_log_verbose: Only turn this flag on if you are willing to risk being
  # deluged with LOTS of information.
  # You can set a bit mask to record specific types of verbose messages:
@@@ -4457,6 -4679,32 +4517,35 @@@ LPFC_VPORT_ATTR_RW(first_burst_size, 0
  		   "First burst size for Targets that support first burst");
  
  /*
++<<<<<<< HEAD
++=======
+ * lpfc_nvmet_fb_size: NVME Target mode supported first burst size.
+ * When the driver is configured as an NVME target, this value is
+ * communicated to the NVME initiator in the PRLI response.  It is
+ * used only when the lpfc_nvme_enable_fb and lpfc_nvmet_support
+ * parameters are set and the target is sending the PRLI RSP.
+ * Parameter supported on physical port only - no NPIV support.
+ * Value range is [0,65536]. Default value is 0.
+ */
+ LPFC_ATTR_RW(nvmet_fb_size, 0, 0, 65536,
+ 	     "NVME Target mode first burst size in 512B increments.");
+ 
+ /*
+  * lpfc_nvme_enable_fb: Enable NVME first burst on I and T functions.
+  * For the Initiator (I), enabling this parameter means that an NVMET
+  * PRLI response with FBA enabled and an FB_SIZE set to a nonzero value will be
+  * processed by the initiator for subsequent NVME FCP IO. For the target
+  * function (T), enabling this parameter qualifies the lpfc_nvmet_fb_size
+  * driver parameter as the target function's first burst size returned to the
+  * initiator in the target's NVME PRLI response. Parameter supported on physical
+  * port only - no NPIV support.
+  * Value range is [0,1]. Default value is 0 (disabled).
+  */
+ LPFC_ATTR_RW(nvme_enable_fb, 0, 0, 1,
+ 	     "Enable First Burst feature on I and T functions.");
+ 
+ /*
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  # lpfc_max_scsicmpl_time: Use scsi command completion time to control I/O queue
  # depth. Default value is 0. When the value of this parameter is zero the
  # SCSI command completion time is not used for controlling I/O queue depth. When
@@@ -4832,6 -5133,13 +4921,15 @@@ struct device_attribute *lpfc_hba_attrs
  	&dev_attr_lpfc_fcp_imax,
  	&dev_attr_lpfc_fcp_cpu_map,
  	&dev_attr_lpfc_fcp_io_channel,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_lpfc_suppress_rsp,
+ 	&dev_attr_lpfc_nvme_io_channel,
+ 	&dev_attr_lpfc_nvmet_mrq,
+ 	&dev_attr_lpfc_nvmet_mrq_post,
+ 	&dev_attr_lpfc_nvme_enable_fb,
+ 	&dev_attr_lpfc_nvmet_fb_size,
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	&dev_attr_lpfc_enable_bg,
  	&dev_attr_lpfc_soft_wwnn,
  	&dev_attr_lpfc_soft_wwpn,
@@@ -5862,6 -6173,44 +5960,47 @@@ lpfc_get_cfgparam(struct lpfc_hba *phba
  		phba->cfg_poll = 0;
  	else
  		phba->cfg_poll = lpfc_poll;
++<<<<<<< HEAD
++=======
+ 	lpfc_suppress_rsp_init(phba, lpfc_suppress_rsp);
+ 
+ 	lpfc_enable_fc4_type_init(phba, lpfc_enable_fc4_type);
+ 	lpfc_nvmet_mrq_init(phba, lpfc_nvmet_mrq);
+ 	lpfc_nvmet_mrq_post_init(phba, lpfc_nvmet_mrq_post);
+ 
+ 	/* Initialize first burst. Target vs Initiator are different. */
+ 	lpfc_nvme_enable_fb_init(phba, lpfc_nvme_enable_fb);
+ 	lpfc_nvmet_fb_size_init(phba, lpfc_nvmet_fb_size);
+ 	lpfc_fcp_io_channel_init(phba, lpfc_fcp_io_channel);
+ 	lpfc_nvme_io_channel_init(phba, lpfc_nvme_io_channel);
+ 
+ 	if (phba->sli_rev != LPFC_SLI_REV4) {
+ 		/* NVME only supported on SLI4 */
+ 		phba->nvmet_support = 0;
+ 		phba->cfg_enable_fc4_type = LPFC_ENABLE_FCP;
+ 	} else {
+ 		/* We MUST have FCP support */
+ 		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP))
+ 			phba->cfg_enable_fc4_type |= LPFC_ENABLE_FCP;
+ 	}
+ 
+ 	/* A value of 0 means use the number of CPUs found in the system */
+ 	if (phba->cfg_fcp_io_channel == 0)
+ 		phba->cfg_fcp_io_channel = phba->sli4_hba.num_present_cpu;
+ 	if (phba->cfg_nvme_io_channel == 0)
+ 		phba->cfg_nvme_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_enable_fc4_type == LPFC_ENABLE_NVME)
+ 		phba->cfg_fcp_io_channel = 0;
+ 
+ 	if (phba->cfg_enable_fc4_type == LPFC_ENABLE_FCP)
+ 		phba->cfg_nvme_io_channel = 0;
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->cfg_nvme_io_channel)
+ 		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
+ 	else
+ 		phba->io_channel_irqs = phba->cfg_nvme_io_channel;
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  
  	phba->cfg_soft_wwnn = 0L;
  	phba->cfg_soft_wwpn = 0L;
@@@ -5881,6 -6231,60 +6020,63 @@@
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_nvme_mod_param_dep - Adjust module parameter value based on
+  * dependencies between protocols and roles.
+  * @phba: lpfc_hba pointer.
+  **/
+ void
+ lpfc_nvme_mod_param_dep(struct lpfc_hba *phba)
+ {
+ 	if (phba->cfg_nvme_io_channel > phba->sli4_hba.num_present_cpu)
+ 		phba->cfg_nvme_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->sli4_hba.num_present_cpu)
+ 		phba->cfg_fcp_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME &&
+ 	    phba->nvmet_support) {
+ 		phba->cfg_enable_fc4_type &= ~LPFC_ENABLE_FCP;
+ 		phba->cfg_fcp_io_channel = 0;
+ 
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,
+ 				"6013 %s x%x fb_size x%x, fb_max x%x\n",
+ 				"NVME Target PRLI ACC enable_fb ",
+ 				phba->cfg_nvme_enable_fb,
+ 				phba->cfg_nvmet_fb_size,
+ 				LPFC_NVMET_FB_SZ_MAX);
+ 
+ 		if (phba->cfg_nvme_enable_fb == 0)
+ 			phba->cfg_nvmet_fb_size = 0;
+ 		else {
+ 			if (phba->cfg_nvmet_fb_size > LPFC_NVMET_FB_SZ_MAX)
+ 				phba->cfg_nvmet_fb_size = LPFC_NVMET_FB_SZ_MAX;
+ 		}
+ 
+ 		/* Adjust lpfc_nvmet_mrq to avoid running out of WQE slots */
+ 		if (phba->cfg_nvmet_mrq > phba->cfg_nvme_io_channel) {
+ 			phba->cfg_nvmet_mrq = phba->cfg_nvme_io_channel;
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_NVME_DISC,
+ 					"6018 Adjust lpfc_nvmet_mrq to %d\n",
+ 					phba->cfg_nvmet_mrq);
+ 		}
+ 	} else {
+ 		/* Not NVME Target mode.  Turn off Target parameters. */
+ 		phba->nvmet_support = 0;
+ 		phba->cfg_nvmet_mrq = 0;
+ 		phba->cfg_nvmet_mrq_post = 0;
+ 		phba->cfg_nvmet_fb_size = 0;
+ 	}
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->cfg_nvme_io_channel)
+ 		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
+ 	else
+ 		phba->io_channel_irqs = phba->cfg_nvme_io_channel;
+ }
+ 
+ /**
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
   * lpfc_get_vport_cfgparam - Used during port create, init the vport structure
   * @vport: lpfc_vport pointer.
   **/
diff --cc drivers/scsi/lpfc/lpfc_debugfs.c
index 4dc8eba541b9,abc39f605960..000000000000
--- a/drivers/scsi/lpfc/lpfc_debugfs.c
+++ b/drivers/scsi/lpfc/lpfc_debugfs.c
@@@ -2112,6 -2861,46 +2112,49 @@@ lpfc_idiag_cqs_for_eq(struct lpfc_hba *
  			return 1;
  	}
  
++<<<<<<< HEAD
++=======
+ 	for (qidx = 0; qidx < phba->cfg_nvme_io_channel; qidx++) {
+ 		qp = phba->sli4_hba.nvme_cq[qidx];
+ 		if (qp->assoc_qid != eq_id)
+ 			continue;
+ 
+ 		*len = __lpfc_idiag_print_cq(qp, "NVME", pbuffer, *len);
+ 
+ 		/* Reset max counter */
+ 		qp->CQ_max_cqe = 0;
+ 
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 
+ 		rc = lpfc_idiag_wqs_for_cq(phba, "NVME", pbuffer, len,
+ 				max_cnt, qp->queue_id);
+ 		if (rc)
+ 			return 1;
+ 	}
+ 
+ 	if (phba->cfg_nvmet_mrq > eqidx) {
+ 		/* NVMET CQset */
+ 		qp = phba->sli4_hba.nvmet_cqset[eqidx];
+ 		*len = __lpfc_idiag_print_cq(qp, "NVMET CQset", pbuffer, *len);
+ 
+ 		/* Reset max counter */
+ 		qp->CQ_max_cqe = 0;
+ 
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 
+ 		/* RQ header */
+ 		qp = phba->sli4_hba.nvmet_mrq_hdr[eqidx];
+ 		*len = __lpfc_idiag_print_rqpair(qp,
+ 				phba->sli4_hba.nvmet_mrq_data[eqidx],
+ 				"NVMET MRQ", pbuffer, *len);
+ 
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 	}
+ 
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	return 0;
  }
  
@@@ -2207,8 -2996,9 +2250,8 @@@ lpfc_idiag_queinfo_read(struct file *fi
  		if (len >= max_cnt)
  			goto too_big;
  
 -		/* will dump both fcp and nvme cqs/wqs for the eq */
  		rc = lpfc_idiag_cqs_for_eq(phba, pbuffer, &len,
- 			max_cnt, qp->queue_id);
+ 			max_cnt, x, qp->queue_id);
  		if (rc)
  			goto too_big;
  
diff --cc drivers/scsi/lpfc/lpfc_hw4.h
index 554ba749ed19,0fddb2317875..000000000000
--- a/drivers/scsi/lpfc/lpfc_hw4.h
+++ b/drivers/scsi/lpfc/lpfc_hw4.h
@@@ -1257,7 -1377,14 +1368,18 @@@ struct rq_context 
  #define lpfc_rq_context_page_size_SHIFT	0		/* Version 1 Only */
  #define lpfc_rq_context_page_size_MASK	0x000000FF
  #define lpfc_rq_context_page_size_WORD	word0
++<<<<<<< HEAD
 +	uint32_t reserved1;
++=======
+ #define	LPFC_RQ_PAGE_SIZE_4096	0x1
+ 	uint32_t word1;
+ #define lpfc_rq_context_data_size_SHIFT	16		/* Version 2 Only */
+ #define lpfc_rq_context_data_size_MASK	0x0000FFFF
+ #define lpfc_rq_context_data_size_WORD	word1
+ #define lpfc_rq_context_hdr_size_SHIFT	0		/* Version 2 Only */
+ #define lpfc_rq_context_hdr_size_MASK	0x0000FFFF
+ #define lpfc_rq_context_hdr_size_WORD	word1
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	uint32_t word2;
  #define lpfc_rq_context_cq_id_SHIFT	16
  #define lpfc_rq_context_cq_id_MASK	0x000003FF
@@@ -3882,6 -4269,50 +4225,53 @@@ struct gen_req64_wqe 
  	uint32_t max_response_payload_len;
  };
  
++<<<<<<< HEAD
++=======
+ /* Define NVME PRLI request to fabric. NVME is a
+  * fabric-only protocol.
+  * Updated to red-lined v1.08 on Sept 16, 2016
+  */
+ struct lpfc_nvme_prli {
+ 	uint32_t word1;
+ 	/* The Response Code is defined in the FCP PRLI lpfc_hw.h */
+ #define prli_acc_rsp_code_SHIFT         8
+ #define prli_acc_rsp_code_MASK          0x0000000f
+ #define prli_acc_rsp_code_WORD          word1
+ #define prli_estabImagePair_SHIFT       13
+ #define prli_estabImagePair_MASK        0x00000001
+ #define prli_estabImagePair_WORD        word1
+ #define prli_type_code_ext_SHIFT        16
+ #define prli_type_code_ext_MASK         0x000000ff
+ #define prli_type_code_ext_WORD         word1
+ #define prli_type_code_SHIFT            24
+ #define prli_type_code_MASK             0x000000ff
+ #define prli_type_code_WORD             word1
+ 	uint32_t word_rsvd2;
+ 	uint32_t word_rsvd3;
+ 	uint32_t word4;
+ #define prli_fba_SHIFT                  0
+ #define prli_fba_MASK                   0x00000001
+ #define prli_fba_WORD                   word4
+ #define prli_disc_SHIFT                 3
+ #define prli_disc_MASK                  0x00000001
+ #define prli_disc_WORD                  word4
+ #define prli_tgt_SHIFT                  4
+ #define prli_tgt_MASK                   0x00000001
+ #define prli_tgt_WORD                   word4
+ #define prli_init_SHIFT                 5
+ #define prli_init_MASK                  0x00000001
+ #define prli_init_WORD                  word4
+ #define prli_recov_SHIFT                8
+ #define prli_recov_MASK                 0x00000001
+ #define prli_recov_WORD                 word4
+ 	uint32_t word5;
+ #define prli_fb_sz_SHIFT                0
+ #define prli_fb_sz_MASK                 0x0000ffff
+ #define prli_fb_sz_WORD                 word5
+ #define LPFC_NVMET_FB_SZ_MAX  65536   /* Driver target mode only. */
+ };
+ 
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  struct create_xri_wqe {
  	uint32_t rsrvd[5];           /* words 0-4 */
  	struct wqe_did	wqe_dest;  /* word 5 */
diff --cc drivers/scsi/lpfc/lpfc_init.c
index bbd4080d8122,e6cbc0bac029..000000000000
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@@ -3252,6 -3322,167 +3252,170 @@@ lpfc_sli4_xri_sgl_update(struct lpfc_hb
  		sglq_entry->sli4_lxritag = lxri;
  		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
  	}
++<<<<<<< HEAD
++=======
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_els_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_nvmet_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_sglq *sglq_entry = NULL, *sglq_entry_next = NULL;
+ 	uint16_t i, lxri, xri_cnt, els_xri_cnt;
+ 	uint16_t nvmet_xri_cnt, tot_cnt;
+ 	LIST_HEAD(nvmet_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's nvmet xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	nvmet_xri_cnt = phba->cfg_nvmet_mrq * phba->cfg_nvmet_mrq_post;
+ 	tot_cnt = phba->sli4_hba.max_cfg_param.max_xri - els_xri_cnt;
+ 	if (nvmet_xri_cnt > tot_cnt) {
+ 		phba->cfg_nvmet_mrq_post = tot_cnt / phba->cfg_nvmet_mrq;
+ 		nvmet_xri_cnt = phba->cfg_nvmet_mrq * phba->cfg_nvmet_mrq_post;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6301 NVMET post-sgl count changed to %d\n",
+ 				phba->cfg_nvmet_mrq_post);
+ 	}
+ 
+ 	if (nvmet_xri_cnt > phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* els xri-sgl expanded */
+ 		xri_cnt = nvmet_xri_cnt - phba->sli4_hba.nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6302 NVMET xri-sgl cnt grew from %d to %d\n",
+ 				phba->sli4_hba.nvmet_xri_cnt, nvmet_xri_cnt);
+ 		/* allocate the additional nvmet sgls */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			sglq_entry = kzalloc(sizeof(struct lpfc_sglq),
+ 					     GFP_KERNEL);
+ 			if (sglq_entry == NULL) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6303 Failure to allocate an "
+ 						"NVMET sgl entry:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->buff_type = NVMET_BUFF_TYPE;
+ 			sglq_entry->virt = lpfc_nvmet_buf_alloc(phba, 0,
+ 							   &sglq_entry->phys);
+ 			if (sglq_entry->virt == NULL) {
+ 				kfree(sglq_entry);
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6304 Failure to allocate an "
+ 						"NVMET buf:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->sgl = sglq_entry->virt;
+ 			memset(sglq_entry->sgl, 0,
+ 			       phba->cfg_sg_dma_buf_size);
+ 			sglq_entry->state = SGL_FREED;
+ 			list_add_tail(&sglq_entry->list, &nvmet_sgl_list);
+ 		}
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else if (nvmet_xri_cnt < phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* nvmet xri-sgl shrunk */
+ 		xri_cnt = phba->sli4_hba.nvmet_xri_cnt - nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6305 NVMET xri-sgl count decreased from "
+ 				"%d to %d\n", phba->sli4_hba.nvmet_xri_cnt,
+ 				nvmet_xri_cnt);
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&phba->sli4_hba.lpfc_nvmet_sgl_list,
+ 				 &nvmet_sgl_list);
+ 		/* release extra nvmet sgls from list */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			list_remove_head(&nvmet_sgl_list,
+ 					 sglq_entry, struct lpfc_sglq, list);
+ 			if (sglq_entry) {
+ 				lpfc_nvmet_buf_free(phba, sglq_entry->virt,
+ 						    sglq_entry->phys);
+ 				kfree(sglq_entry);
+ 			}
+ 		}
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6306 NVMET xri-sgl count unchanged: %d\n",
+ 				nvmet_xri_cnt);
+ 	phba->sli4_hba.nvmet_xri_cnt = nvmet_xri_cnt;
+ 
+ 	/* update xris to nvmet sgls on the list */
+ 	sglq_entry = NULL;
+ 	sglq_entry_next = NULL;
+ 	list_for_each_entry_safe(sglq_entry, sglq_entry_next,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list, list) {
+ 		lxri = lpfc_sli4_next_xritag(phba);
+ 		if (lxri == NO_XRI) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 					"6307 Failed to allocate xri for "
+ 					"NVMET sgl\n");
+ 			rc = -ENOMEM;
+ 			goto out_free_mem;
+ 		}
+ 		sglq_entry->sli4_lxritag = lxri;
+ 		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
+ 	}
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_nvmet_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_scsi_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_scsi_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_scsi_buf *psb, *psb_next;
+ 	uint16_t i, lxri, els_xri_cnt, scsi_xri_cnt;
+ 	LIST_HEAD(scsi_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's els xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	phba->total_scsi_bufs = 0;
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  
  	/*
  	 * update on pci function's allocated scsi xri-sgl list
@@@ -7266,14 -7669,25 +7430,31 @@@ lpfc_sli4_queue_verify(struct lpfc_hba 
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
  				"2575 Reducing IO channels to match number of "
  				"available EQs: from %d to %d\n",
 -				io_channel,
 +				cfg_fcp_io_channel,
  				phba->sli4_hba.max_cfg_param.max_eq);
 -		io_channel = phba->sli4_hba.max_cfg_param.max_eq - fof_vectors;
 +		cfg_fcp_io_channel = phba->sli4_hba.max_cfg_param.max_eq -
 +			fof_vectors;
  	}
  
++<<<<<<< HEAD
 +	/* The actual number of FCP event queues adopted */
 +	phba->cfg_fcp_io_channel = cfg_fcp_io_channel;
++=======
+ 	/* The actual number of FCP / NVME event queues adopted */
+ 	if (io_channel != phba->io_channel_irqs)
+ 		phba->io_channel_irqs = io_channel;
+ 	if (phba->cfg_fcp_io_channel > io_channel)
+ 		phba->cfg_fcp_io_channel = io_channel;
+ 	if (phba->cfg_nvme_io_channel > io_channel)
+ 		phba->cfg_nvme_io_channel = io_channel;
+ 	if (phba->cfg_nvme_io_channel < phba->cfg_nvmet_mrq)
+ 		phba->cfg_nvmet_mrq = phba->cfg_nvme_io_channel;
+ 
+ 	lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 			"2574 IO channels: irqs %d fcp %d nvme %d MRQ: %d\n",
+ 			phba->io_channel_irqs, phba->cfg_fcp_io_channel,
+ 			phba->cfg_nvme_io_channel, phba->cfg_nvmet_mrq);
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  
  	/* Get EQ depth from module parameter, fake the default for now */
  	phba->sli4_hba.eq_esize = LPFC_EQE_SIZE_4B;
@@@ -7306,7 -7777,7 +7487,11 @@@ in
  lpfc_sli4_queue_create(struct lpfc_hba *phba)
  {
  	struct lpfc_queue *qdesc;
++<<<<<<< HEAD
 +	int idx;
++=======
+ 	int idx, io_channel, max;
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  
  	/*
  	 * Create HBA Record arrays.
@@@ -7330,44 -7808,115 +7515,115 @@@
  		goto out_error;
  	}
  
 -	if (phba->cfg_fcp_io_channel) {
 -		phba->sli4_hba.fcp_cq = kcalloc(phba->cfg_fcp_io_channel,
 -						sizeof(struct lpfc_queue *),
 -						GFP_KERNEL);
 -		if (!phba->sli4_hba.fcp_cq) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -					"2577 Failed allocate memory for "
 -					"fast-path CQ record array\n");
 -			goto out_error;
 -		}
 -		phba->sli4_hba.fcp_wq = kcalloc(phba->cfg_fcp_io_channel,
 -						sizeof(struct lpfc_queue *),
 -						GFP_KERNEL);
 -		if (!phba->sli4_hba.fcp_wq) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -					"2578 Failed allocate memory for "
 -					"fast-path FCP WQ record array\n");
 -			goto out_error;
 -		}
 -		/*
 -		 * Since the first EQ can have multiple CQs associated with it,
 -		 * this array is used to quickly see if we have a FCP fast-path
 -		 * CQ match.
 -		 */
 -		phba->sli4_hba.fcp_cq_map = kcalloc(phba->cfg_fcp_io_channel,
 -							sizeof(uint16_t),
 -							GFP_KERNEL);
 -		if (!phba->sli4_hba.fcp_cq_map) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -					"2545 Failed allocate memory for "
 -					"fast-path CQ map\n");
 -			goto out_error;
 -		}
 +	phba->sli4_hba.fcp_cq = kzalloc((sizeof(struct lpfc_queue *) *
 +				phba->cfg_fcp_io_channel), GFP_KERNEL);
 +	if (!phba->sli4_hba.fcp_cq) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"2577 Failed allocate memory for fast-path "
 +				"CQ record array\n");
 +		goto out_error;
  	}
  
++<<<<<<< HEAD
 +	phba->sli4_hba.fcp_wq = kzalloc((sizeof(struct lpfc_queue *) *
 +				phba->cfg_fcp_io_channel), GFP_KERNEL);
 +	if (!phba->sli4_hba.fcp_wq) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"2578 Failed allocate memory for fast-path "
 +				"WQ record array\n");
 +		goto out_error;
++=======
+ 	if (phba->cfg_nvme_io_channel) {
+ 		phba->sli4_hba.nvme_cq = kcalloc(phba->cfg_nvme_io_channel,
+ 						sizeof(struct lpfc_queue *),
+ 						GFP_KERNEL);
+ 		if (!phba->sli4_hba.nvme_cq) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"6077 Failed allocate memory for "
+ 					"fast-path CQ record array\n");
+ 			goto out_error;
+ 		}
+ 
+ 		phba->sli4_hba.nvme_wq = kcalloc(phba->cfg_nvme_io_channel,
+ 						sizeof(struct lpfc_queue *),
+ 						GFP_KERNEL);
+ 		if (!phba->sli4_hba.nvme_wq) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"2581 Failed allocate memory for "
+ 					"fast-path NVME WQ record array\n");
+ 			goto out_error;
+ 		}
+ 
+ 		/*
+ 		 * Since the first EQ can have multiple CQs associated with it,
+ 		 * this array is used to quickly see if we have a NVME fast-path
+ 		 * CQ match.
+ 		 */
+ 		phba->sli4_hba.nvme_cq_map = kcalloc(phba->cfg_nvme_io_channel,
+ 							sizeof(uint16_t),
+ 							GFP_KERNEL);
+ 		if (!phba->sli4_hba.nvme_cq_map) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"6078 Failed allocate memory for "
+ 					"fast-path CQ map\n");
+ 			goto out_error;
+ 		}
+ 
+ 		if (phba->nvmet_support) {
+ 			phba->sli4_hba.nvmet_cqset = kcalloc(
+ 					phba->cfg_nvmet_mrq,
+ 					sizeof(struct lpfc_queue *),
+ 					GFP_KERNEL);
+ 			if (!phba->sli4_hba.nvmet_cqset) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"3121 Fail allocate memory for "
+ 					"fast-path CQ set array\n");
+ 				goto out_error;
+ 			}
+ 			phba->sli4_hba.nvmet_mrq_hdr = kcalloc(
+ 					phba->cfg_nvmet_mrq,
+ 					sizeof(struct lpfc_queue *),
+ 					GFP_KERNEL);
+ 			if (!phba->sli4_hba.nvmet_mrq_hdr) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"3122 Fail allocate memory for "
+ 					"fast-path RQ set hdr array\n");
+ 				goto out_error;
+ 			}
+ 			phba->sli4_hba.nvmet_mrq_data = kcalloc(
+ 					phba->cfg_nvmet_mrq,
+ 					sizeof(struct lpfc_queue *),
+ 					GFP_KERNEL);
+ 			if (!phba->sli4_hba.nvmet_mrq_data) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"3124 Fail allocate memory for "
+ 					"fast-path RQ set data array\n");
+ 				goto out_error;
+ 			}
+ 		}
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	}
  
 -	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_wq_list);
 +	/*
 +	 * Since the first EQ can have multiple CQs associated with it,
 +	 * this array is used to quickly see if we have a FCP fast-path
 +	 * CQ match.
 +	 */
 +	phba->sli4_hba.fcp_cq_map = kzalloc((sizeof(uint16_t) *
 +					 phba->cfg_fcp_io_channel), GFP_KERNEL);
 +	if (!phba->sli4_hba.fcp_cq_map) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"2545 Failed allocate memory for fast-path "
 +				"CQ map\n");
 +		goto out_error;
 +	}
 +
 +	/*
 +	 * Create HBA Event Queues (EQs).  The cfg_fcp_io_channel specifies
 +	 * how many EQs to create.
 +	 */
 +	for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++) {
  
 -	/* Create HBA Event Queues (EQs) */
 -	for (idx = 0; idx < io_channel; idx++) {
  		/* Create EQs */
  		qdesc = lpfc_sli4_queue_alloc(phba, phba->sli4_hba.eq_esize,
  					      phba->sli4_hba.eq_ecount);
@@@ -7377,31 -7926,42 +7633,55 @@@
  			goto out_error;
  		}
  		phba->sli4_hba.hba_eq[idx] = qdesc;
 -	}
 -
 -	/* FCP and NVME io channels are not required to be balanced */
  
 -	for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++)
 -		if (lpfc_alloc_fcp_wq_cq(phba, idx))
 +		/* Create Fast Path FCP CQs */
 +		qdesc = lpfc_sli4_queue_alloc(phba, phba->sli4_hba.cq_esize,
 +					      phba->sli4_hba.cq_ecount);
 +		if (!qdesc) {
 +			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +					"0499 Failed allocate fast-path FCP "
 +					"CQ (%d)\n", idx);
  			goto out_error;
 +		}
 +		phba->sli4_hba.fcp_cq[idx] = qdesc;
  
 -	for (idx = 0; idx < phba->cfg_nvme_io_channel; idx++)
 -		if (lpfc_alloc_nvme_wq_cq(phba, idx))
 +		/* Create Fast Path FCP WQs */
 +		qdesc = lpfc_sli4_queue_alloc(phba, phba->sli4_hba.wq_esize,
 +					      phba->sli4_hba.wq_ecount);
 +		if (!qdesc) {
 +			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +					"0503 Failed allocate fast-path FCP "
 +					"WQ (%d)\n", idx);
  			goto out_error;
 +		}
 +		phba->sli4_hba.fcp_wq[idx] = qdesc;
 +	}
 +
  
+ 	/* allocate MRQ CQs */
+ 	max = phba->cfg_nvme_io_channel;
+ 	if (max < phba->cfg_nvmet_mrq)
+ 		max = phba->cfg_nvmet_mrq;
+ 
+ 	for (idx = 0; idx < max; idx++)
+ 		if (lpfc_alloc_nvme_wq_cq(phba, idx))
+ 			goto out_error;
+ 
+ 	if (phba->nvmet_support) {
+ 		for (idx = 0; idx < phba->cfg_nvmet_mrq; idx++) {
+ 			qdesc = lpfc_sli4_queue_alloc(phba,
+ 					phba->sli4_hba.cq_esize,
+ 					phba->sli4_hba.cq_ecount);
+ 			if (!qdesc) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"3142 Failed allocate NVME "
+ 					"CQ Set (%d)\n", idx);
+ 				goto out_error;
+ 			}
+ 			phba->sli4_hba.nvmet_cqset[idx] = qdesc;
+ 		}
+ 	}
+ 
  	/*
  	 * Create Slow Path Completion Queues (CQs)
  	 */
@@@ -7510,86 -8163,193 +7828,94 @@@ lpfc_sli4_queue_destroy(struct lpfc_hb
  	if (phba->cfg_fof)
  		lpfc_fof_queue_destroy(phba);
  
 -	/* Release HBA eqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.hba_eq, phba->io_channel_irqs);
 +	if (phba->sli4_hba.hba_eq != NULL) {
 +		/* Release HBA event queue */
 +		for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++) {
 +			if (phba->sli4_hba.hba_eq[idx] != NULL) {
 +				lpfc_sli4_queue_free(
 +					phba->sli4_hba.hba_eq[idx]);
 +				phba->sli4_hba.hba_eq[idx] = NULL;
 +			}
 +		}
 +		kfree(phba->sli4_hba.hba_eq);
 +		phba->sli4_hba.hba_eq = NULL;
 +	}
  
 -	/* Release FCP cqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.fcp_cq,
 -					phba->cfg_fcp_io_channel);
 +	if (phba->sli4_hba.fcp_cq != NULL) {
 +		/* Release FCP completion queue */
 +		for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++) {
 +			if (phba->sli4_hba.fcp_cq[idx] != NULL) {
 +				lpfc_sli4_queue_free(
 +					phba->sli4_hba.fcp_cq[idx]);
 +				phba->sli4_hba.fcp_cq[idx] = NULL;
 +			}
 +		}
 +		kfree(phba->sli4_hba.fcp_cq);
 +		phba->sli4_hba.fcp_cq = NULL;
 +	}
  
 -	/* Release FCP wqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.fcp_wq,
 -					phba->cfg_fcp_io_channel);
 +	if (phba->sli4_hba.fcp_wq != NULL) {
 +		/* Release FCP work queue */
 +		for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++) {
 +			if (phba->sli4_hba.fcp_wq[idx] != NULL) {
 +				lpfc_sli4_queue_free(
 +					phba->sli4_hba.fcp_wq[idx]);
 +				phba->sli4_hba.fcp_wq[idx] = NULL;
 +			}
 +		}
 +		kfree(phba->sli4_hba.fcp_wq);
 +		phba->sli4_hba.fcp_wq = NULL;
 +	}
  
  	/* Release FCP CQ mapping array */
 -	lpfc_sli4_release_queue_map(&phba->sli4_hba.fcp_cq_map);
 -
 -	/* Release NVME cqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.nvme_cq,
 -					phba->cfg_nvme_io_channel);
 -
 -	/* Release NVME wqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.nvme_wq,
 -					phba->cfg_nvme_io_channel);
 -
 -	/* Release NVME CQ mapping array */
 -	lpfc_sli4_release_queue_map(&phba->sli4_hba.nvme_cq_map);
 +	if (phba->sli4_hba.fcp_cq_map != NULL) {
 +		kfree(phba->sli4_hba.fcp_cq_map);
 +		phba->sli4_hba.fcp_cq_map = NULL;
 +	}
  
+ 	lpfc_sli4_release_queues(&phba->sli4_hba.nvmet_cqset,
+ 					phba->cfg_nvmet_mrq);
+ 
+ 	lpfc_sli4_release_queues(&phba->sli4_hba.nvmet_mrq_hdr,
+ 					phba->cfg_nvmet_mrq);
+ 	lpfc_sli4_release_queues(&phba->sli4_hba.nvmet_mrq_data,
+ 					phba->cfg_nvmet_mrq);
+ 
  	/* Release mailbox command work queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.mbx_wq);
 -
 -	/* Release ELS work queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.els_wq);
 +	if (phba->sli4_hba.mbx_wq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.mbx_wq);
 +		phba->sli4_hba.mbx_wq = NULL;
 +	}
  
  	/* Release ELS work queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.nvmels_wq);
 -
 -	/* Release unsolicited receive queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.hdr_rq);
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.dat_rq);
 -
 -	/* Release ELS complete queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.els_cq);
 -
 -	/* Release NVME LS complete queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.nvmels_cq);
 -
 -	/* Release mailbox command complete queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.mbx_cq);
 -
 -	/* Everything on this list has been freed */
 -	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_wq_list);
 -}
 -
 -int
 -lpfc_post_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *hrq,
 -		    struct lpfc_queue *drq, int count)
 -{
 -	int rc, i;
 -	struct lpfc_rqe hrqe;
 -	struct lpfc_rqe drqe;
 -	struct lpfc_rqb *rqbp;
 -	struct rqb_dmabuf *rqb_buffer;
 -	LIST_HEAD(rqb_buf_list);
 -
 -	rqbp = hrq->rqbp;
 -	for (i = 0; i < count; i++) {
 -		rqb_buffer = (rqbp->rqb_alloc_buffer)(phba);
 -		if (!rqb_buffer)
 -			break;
 -		rqb_buffer->hrq = hrq;
 -		rqb_buffer->drq = drq;
 -		list_add_tail(&rqb_buffer->hbuf.list, &rqb_buf_list);
 -	}
 -	while (!list_empty(&rqb_buf_list)) {
 -		list_remove_head(&rqb_buf_list, rqb_buffer, struct rqb_dmabuf,
 -				 hbuf.list);
 -
 -		hrqe.address_lo = putPaddrLow(rqb_buffer->hbuf.phys);
 -		hrqe.address_hi = putPaddrHigh(rqb_buffer->hbuf.phys);
 -		drqe.address_lo = putPaddrLow(rqb_buffer->dbuf.phys);
 -		drqe.address_hi = putPaddrHigh(rqb_buffer->dbuf.phys);
 -		rc = lpfc_sli4_rq_put(hrq, drq, &hrqe, &drqe);
 -		if (rc < 0) {
 -			(rqbp->rqb_free_buffer)(phba, rqb_buffer);
 -		} else {
 -			list_add_tail(&rqb_buffer->hbuf.list,
 -				      &rqbp->rqb_buffer_list);
 -			rqbp->buffer_count++;
 -		}
 +	if (phba->sli4_hba.els_wq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.els_wq);
 +		phba->sli4_hba.els_wq = NULL;
  	}
 -	return 1;
 -}
  
 -int
 -lpfc_free_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *rq)
 -{
 -	struct lpfc_rqb *rqbp;
 -	struct lpfc_dmabuf *h_buf;
 -	struct rqb_dmabuf *rqb_buffer;
 -
 -	rqbp = rq->rqbp;
 -	while (!list_empty(&rqbp->rqb_buffer_list)) {
 -		list_remove_head(&rqbp->rqb_buffer_list, h_buf,
 -				 struct lpfc_dmabuf, list);
 -
 -		rqb_buffer = container_of(h_buf, struct rqb_dmabuf, hbuf);
 -		(rqbp->rqb_free_buffer)(phba, rqb_buffer);
 -		rqbp->buffer_count--;
 +	/* Release unsolicited receive queue */
 +	if (phba->sli4_hba.hdr_rq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.hdr_rq);
 +		phba->sli4_hba.hdr_rq = NULL;
  	}
 -	return 1;
 -}
 -
 -static int
 -lpfc_create_wq_cq(struct lpfc_hba *phba, struct lpfc_queue *eq,
 -	struct lpfc_queue *cq, struct lpfc_queue *wq, uint16_t *cq_map,
 -	int qidx, uint32_t qtype)
 -{
 -	struct lpfc_sli_ring *pring;
 -	int rc;
 -
 -	if (!eq || !cq || !wq) {
 -		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -			"6085 Fast-path %s (%d) not allocated\n",
 -			((eq) ? ((cq) ? "WQ" : "CQ") : "EQ"), qidx);
 -		return -ENOMEM;
 +	if (phba->sli4_hba.dat_rq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.dat_rq);
 +		phba->sli4_hba.dat_rq = NULL;
  	}
  
 -	/* create the Cq first */
 -	rc = lpfc_cq_create(phba, cq, eq,
 -			(qtype == LPFC_MBOX) ? LPFC_MCQ : LPFC_WCQ, qtype);
 -	if (rc) {
 -		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -			"6086 Failed setup of CQ (%d), rc = 0x%x\n",
 -			qidx, (uint32_t)rc);
 -		return rc;
 +	/* Release ELS complete queue */
 +	if (phba->sli4_hba.els_cq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.els_cq);
 +		phba->sli4_hba.els_cq = NULL;
  	}
  
 -	if (qtype != LPFC_MBOX) {
 -		/* Setup nvme_cq_map for fast lookup */
 -		if (cq_map)
 -			*cq_map = cq->queue_id;
 -
 -		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -			"6087 CQ setup: cq[%d]-id=%d, parent eq[%d]-id=%d\n",
 -			qidx, cq->queue_id, qidx, eq->queue_id);
 -
 -		/* create the wq */
 -		rc = lpfc_wq_create(phba, wq, cq, qtype);
 -		if (rc) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"6123 Fail setup fastpath WQ (%d), rc = 0x%x\n",
 -				qidx, (uint32_t)rc);
 -			/* no need to tear down cq - caller will do so */
 -			return rc;
 -		}
 -
 -		/* Bind this CQ/WQ to the NVME ring */
 -		pring = wq->pring;
 -		pring->sli.sli4.wqp = (void *)wq;
 -		cq->pring = pring;
 -
 -		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -			"2593 WQ setup: wq[%d]-id=%d assoc=%d, cq[%d]-id=%d\n",
 -			qidx, wq->queue_id, wq->assoc_qid, qidx, cq->queue_id);
 -	} else {
 -		rc = lpfc_mq_create(phba, wq, cq, LPFC_MBOX);
 -		if (rc) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"0539 Failed setup of slow-path MQ: "
 -				"rc = 0x%x\n", rc);
 -			/* no need to tear down cq - caller will do so */
 -			return rc;
 -		}
 -
 -		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -			"2589 MBX MQ setup: wq-id=%d, parent cq-id=%d\n",
 -			phba->sli4_hba.mbx_wq->queue_id,
 -			phba->sli4_hba.mbx_cq->queue_id);
 +	/* Release mailbox command complete queue */
 +	if (phba->sli4_hba.mbx_cq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.mbx_cq);
 +		phba->sli4_hba.mbx_cq = NULL;
  	}
  
 -	return 0;
 +	return;
  }
  
  /**
@@@ -7686,194 -8444,236 +8012,317 @@@ lpfc_sli4_queue_setup(struct lpfc_hba *
  		if (rc) {
  			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
  					"0523 Failed setup of fast-path EQ "
 -					"(%d), rc = 0x%x\n", qidx,
 +					"(%d), rc = 0x%x\n", fcp_eqidx,
  					(uint32_t)rc);
 -			goto out_destroy;
 +			goto out_destroy_hba_eq;
  		}
  		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -				"2584 HBA EQ setup: queue[%d]-id=%d\n",
 -				qidx, phba->sli4_hba.hba_eq[qidx]->queue_id);
 +				"2584 HBA EQ setup: "
 +				"queue[%d]-id=%d\n", fcp_eqidx,
 +				phba->sli4_hba.hba_eq[fcp_eqidx]->queue_id);
  	}
  
 -	if (phba->cfg_nvme_io_channel) {
 -		if (!phba->sli4_hba.nvme_cq || !phba->sli4_hba.nvme_wq) {
 +	/* Set up fast-path FCP Response Complete Queue */
 +	if (!phba->sli4_hba.fcp_cq) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"3148 Fast-path FCP CQ array not "
 +				"allocated\n");
 +		rc = -ENOMEM;
 +		goto out_destroy_hba_eq;
 +	}
 +
 +	for (fcp_cqidx = 0; fcp_cqidx < phba->cfg_fcp_io_channel; fcp_cqidx++) {
 +		if (!phba->sli4_hba.fcp_cq[fcp_cqidx]) {
  			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"6084 Fast-path NVME %s array not allocated\n",
 -				(phba->sli4_hba.nvme_cq) ? "CQ" : "WQ");
 +					"0526 Fast-path FCP CQ (%d) not "
 +					"allocated\n", fcp_cqidx);
  			rc = -ENOMEM;
 -			goto out_destroy;
 +			goto out_destroy_fcp_cq;
  		}
 -
 -		for (qidx = 0; qidx < phba->cfg_nvme_io_channel; qidx++) {
 -			rc = lpfc_create_wq_cq(phba,
 -					phba->sli4_hba.hba_eq[
 -						qidx % io_channel],
 -					phba->sli4_hba.nvme_cq[qidx],
 -					phba->sli4_hba.nvme_wq[qidx],
 -					&phba->sli4_hba.nvme_cq_map[qidx],
 -					qidx, LPFC_NVME);
 -			if (rc) {
 -				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -					"6123 Failed to setup fastpath "
 -					"NVME WQ/CQ (%d), rc = 0x%x\n",
 -					qidx, (uint32_t)rc);
 -				goto out_destroy;
 -			}
 +		rc = lpfc_cq_create(phba, phba->sli4_hba.fcp_cq[fcp_cqidx],
 +			phba->sli4_hba.hba_eq[fcp_cqidx], LPFC_WCQ, LPFC_FCP);
 +		if (rc) {
 +			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +					"0527 Failed setup of fast-path FCP "
 +					"CQ (%d), rc = 0x%x\n", fcp_cqidx,
 +					(uint32_t)rc);
 +			goto out_destroy_fcp_cq;
  		}
 +
 +		/* Setup fcp_cq_map for fast lookup */
 +		phba->sli4_hba.fcp_cq_map[fcp_cqidx] =
 +				phba->sli4_hba.fcp_cq[fcp_cqidx]->queue_id;
 +
 +		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 +				"2588 FCP CQ setup: cq[%d]-id=%d, "
 +				"parent seq[%d]-id=%d\n",
 +				fcp_cqidx,
 +				phba->sli4_hba.fcp_cq[fcp_cqidx]->queue_id,
 +				fcp_cqidx,
 +				phba->sli4_hba.hba_eq[fcp_cqidx]->queue_id);
 +	}
 +
 +	/* Set up fast-path FCP Work Queue */
 +	if (!phba->sli4_hba.fcp_wq) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"3149 Fast-path FCP WQ array not "
 +				"allocated\n");
 +		rc = -ENOMEM;
 +		goto out_destroy_fcp_cq;
  	}
  
 -	if (phba->cfg_fcp_io_channel) {
 -		/* Set up fast-path FCP Response Complete Queue */
 -		if (!phba->sli4_hba.fcp_cq || !phba->sli4_hba.fcp_wq) {
 +	for (fcp_wqidx = 0; fcp_wqidx < phba->cfg_fcp_io_channel; fcp_wqidx++) {
 +		if (!phba->sli4_hba.fcp_wq[fcp_wqidx]) {
  			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"3148 Fast-path FCP %s array not allocated\n",
 -				phba->sli4_hba.fcp_cq ? "WQ" : "CQ");
 +					"0534 Fast-path FCP WQ (%d) not "
 +					"allocated\n", fcp_wqidx);
  			rc = -ENOMEM;
 -			goto out_destroy;
 +			goto out_destroy_fcp_wq;
  		}
 -
 -		for (qidx = 0; qidx < phba->cfg_fcp_io_channel; qidx++) {
 -			rc = lpfc_create_wq_cq(phba,
 -					phba->sli4_hba.hba_eq[
 -						qidx % io_channel],
 -					phba->sli4_hba.fcp_cq[qidx],
 -					phba->sli4_hba.fcp_wq[qidx],
 -					&phba->sli4_hba.fcp_cq_map[qidx],
 -					qidx, LPFC_FCP);
 -			if (rc) {
 -				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -					"0535 Failed to setup fastpath "
 -					"FCP WQ/CQ (%d), rc = 0x%x\n",
 -					qidx, (uint32_t)rc);
 -				goto out_destroy;
 -			}
 +		rc = lpfc_wq_create(phba, phba->sli4_hba.fcp_wq[fcp_wqidx],
 +				    phba->sli4_hba.fcp_cq[fcp_wqidx],
 +				    LPFC_FCP);
 +		if (rc) {
 +			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +					"0535 Failed setup of fast-path FCP "
 +					"WQ (%d), rc = 0x%x\n", fcp_wqidx,
 +					(uint32_t)rc);
 +			goto out_destroy_fcp_wq;
  		}
 -	}
  
 +		/* Bind this WQ to the next FCP ring */
 +		pring = &psli->ring[MAX_SLI3_CONFIGURED_RINGS + fcp_wqidx];
 +		pring->sli.sli4.wqp = (void *)phba->sli4_hba.fcp_wq[fcp_wqidx];
 +		phba->sli4_hba.fcp_cq[fcp_wqidx]->pring = pring;
 +
 +		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 +				"2591 FCP WQ setup: wq[%d]-id=%d, "
 +				"parent cq[%d]-id=%d\n",
 +				fcp_wqidx,
 +				phba->sli4_hba.fcp_wq[fcp_wqidx]->queue_id,
 +				fcp_cq_index,
 +				phba->sli4_hba.fcp_cq[fcp_wqidx]->queue_id);
 +	}
  	/*
 -	 * Set up Slow Path Complete Queues (CQs)
 +	 * Set up Complete Queues (CQs)
  	 */
  
 -	/* Set up slow-path MBOX CQ/MQ */
 -
 -	if (!phba->sli4_hba.mbx_cq || !phba->sli4_hba.mbx_wq) {
 +	/* Set up slow-path MBOX Complete Queue as the first CQ */
 +	if (!phba->sli4_hba.mbx_cq) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"0528 %s not allocated\n",
 -				phba->sli4_hba.mbx_cq ?
 -						"Mailbox WQ" : "Mailbox CQ");
 +				"0528 Mailbox CQ not allocated\n");
  		rc = -ENOMEM;
 -		goto out_destroy;
 +		goto out_destroy_fcp_wq;
  	}
 -
 -	rc = lpfc_create_wq_cq(phba, phba->sli4_hba.hba_eq[0],
 -					phba->sli4_hba.mbx_cq,
 -					phba->sli4_hba.mbx_wq,
 -					NULL, 0, LPFC_MBOX);
 +	rc = lpfc_cq_create(phba, phba->sli4_hba.mbx_cq,
 +			phba->sli4_hba.hba_eq[0], LPFC_MCQ, LPFC_MBOX);
  	if (rc) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -			"0529 Failed setup of mailbox WQ/CQ: rc = 0x%x\n",
 -			(uint32_t)rc);
 -		goto out_destroy;
 +				"0529 Failed setup of slow-path mailbox CQ: "
 +				"rc = 0x%x\n", (uint32_t)rc);
 +		goto out_destroy_fcp_wq;
  	}
++<<<<<<< HEAD
 +	lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 +			"2585 MBX CQ setup: cq-id=%d, parent eq-id=%d\n",
 +			phba->sli4_hba.mbx_cq->queue_id,
 +			phba->sli4_hba.hba_eq[0]->queue_id);
++=======
+ 	if (phba->nvmet_support) {
+ 		if (!phba->sli4_hba.nvmet_cqset) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"3165 Fast-path NVME CQ Set "
+ 					"array not allocated\n");
+ 			rc = -ENOMEM;
+ 			goto out_destroy;
+ 		}
+ 		if (phba->cfg_nvmet_mrq > 1) {
+ 			rc = lpfc_cq_create_set(phba,
+ 					phba->sli4_hba.nvmet_cqset,
+ 					phba->sli4_hba.hba_eq,
+ 					LPFC_WCQ, LPFC_NVMET);
+ 			if (rc) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"3164 Failed setup of NVME CQ "
+ 						"Set, rc = 0x%x\n",
+ 						(uint32_t)rc);
+ 				goto out_destroy;
+ 			}
+ 		} else {
+ 			/* Set up NVMET Receive Complete Queue */
+ 			rc = lpfc_cq_create(phba, phba->sli4_hba.nvmet_cqset[0],
+ 					    phba->sli4_hba.hba_eq[0],
+ 					    LPFC_WCQ, LPFC_NVMET);
+ 			if (rc) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"6089 Failed setup NVMET CQ: "
+ 						"rc = 0x%x\n", (uint32_t)rc);
+ 				goto out_destroy;
+ 			}
+ 			lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
+ 					"6090 NVMET CQ setup: cq-id=%d, "
+ 					"parent eq-id=%d\n",
+ 					phba->sli4_hba.nvmet_cqset[0]->queue_id,
+ 					phba->sli4_hba.hba_eq[0]->queue_id);
+ 		}
+ 	}
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
 +
 +	/* Set up slow-path ELS Complete Queue */
 +	if (!phba->sli4_hba.els_cq) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"0530 ELS CQ not allocated\n");
 +		rc = -ENOMEM;
 +		goto out_destroy_mbx_cq;
 +	}
 +	rc = lpfc_cq_create(phba, phba->sli4_hba.els_cq,
 +			phba->sli4_hba.hba_eq[0], LPFC_WCQ, LPFC_ELS);
 +	if (rc) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"0531 Failed setup of slow-path ELS CQ: "
 +				"rc = 0x%x\n", (uint32_t)rc);
 +		goto out_destroy_mbx_cq;
 +	}
 +	lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 +			"2586 ELS CQ setup: cq-id=%d, parent eq-id=%d\n",
 +			phba->sli4_hba.els_cq->queue_id,
 +			phba->sli4_hba.hba_eq[0]->queue_id);
 +
 +	/*
 +	 * Set up all the Work Queues (WQs)
 +	 */
 +
 +	/* Set up Mailbox Command Queue */
 +	if (!phba->sli4_hba.mbx_wq) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"0538 Slow-path MQ not allocated\n");
 +		rc = -ENOMEM;
 +		goto out_destroy_els_cq;
 +	}
 +	rc = lpfc_mq_create(phba, phba->sli4_hba.mbx_wq,
 +			    phba->sli4_hba.mbx_cq, LPFC_MBOX);
 +	if (rc) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"0539 Failed setup of slow-path MQ: "
 +				"rc = 0x%x\n", rc);
 +		goto out_destroy_els_cq;
 +	}
 +	lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 +			"2589 MBX MQ setup: wq-id=%d, parent cq-id=%d\n",
 +			phba->sli4_hba.mbx_wq->queue_id,
 +			phba->sli4_hba.mbx_cq->queue_id);
  
 -	/* Set up slow-path ELS WQ/CQ */
 -	if (!phba->sli4_hba.els_cq || !phba->sli4_hba.els_wq) {
 +	/* Set up slow-path ELS Work Queue */
 +	if (!phba->sli4_hba.els_wq) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"0530 ELS %s not allocated\n",
 -				phba->sli4_hba.els_cq ? "WQ" : "CQ");
 +				"0536 Slow-path ELS WQ not allocated\n");
  		rc = -ENOMEM;
 -		goto out_destroy;
 +		goto out_destroy_mbx_wq;
  	}
 -	rc = lpfc_create_wq_cq(phba, phba->sli4_hba.hba_eq[0],
 -					phba->sli4_hba.els_cq,
 -					phba->sli4_hba.els_wq,
 -					NULL, 0, LPFC_ELS);
 +	rc = lpfc_wq_create(phba, phba->sli4_hba.els_wq,
 +			    phba->sli4_hba.els_cq, LPFC_ELS);
  	if (rc) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -			"0529 Failed setup of ELS WQ/CQ: rc = 0x%x\n",
 -			(uint32_t)rc);
 -		goto out_destroy;
 +				"0537 Failed setup of slow-path ELS WQ: "
 +				"rc = 0x%x\n", (uint32_t)rc);
 +		goto out_destroy_mbx_wq;
  	}
 +
 +	/* Bind this WQ to the ELS ring */
 +	pring = &psli->ring[LPFC_ELS_RING];
 +	pring->sli.sli4.wqp = (void *)phba->sli4_hba.els_wq;
 +	phba->sli4_hba.els_cq->pring = pring;
 +
  	lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
  			"2590 ELS WQ setup: wq-id=%d, parent cq-id=%d\n",
  			phba->sli4_hba.els_wq->queue_id,
  			phba->sli4_hba.els_cq->queue_id);
  
++<<<<<<< HEAD
 +	/*
 +	 * Create Receive Queue (RQ)
 +	 */
++=======
+ 	if (phba->cfg_nvme_io_channel) {
+ 		/* Set up NVME LS Complete Queue */
+ 		if (!phba->sli4_hba.nvmels_cq || !phba->sli4_hba.nvmels_wq) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"6091 LS %s not allocated\n",
+ 					phba->sli4_hba.nvmels_cq ? "WQ" : "CQ");
+ 			rc = -ENOMEM;
+ 			goto out_destroy;
+ 		}
+ 		rc = lpfc_create_wq_cq(phba, phba->sli4_hba.hba_eq[0],
+ 					phba->sli4_hba.nvmels_cq,
+ 					phba->sli4_hba.nvmels_wq,
+ 					NULL, 0, LPFC_NVME_LS);
+ 		if (rc) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0529 Failed setup of NVVME LS WQ/CQ: "
+ 				"rc = 0x%x\n", (uint32_t)rc);
+ 			goto out_destroy;
+ 		}
+ 
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
+ 				"6096 ELS WQ setup: wq-id=%d, "
+ 				"parent cq-id=%d\n",
+ 				phba->sli4_hba.nvmels_wq->queue_id,
+ 				phba->sli4_hba.nvmels_cq->queue_id);
+ 	}
+ 
+ 	/*
+ 	 * Create NVMET Receive Queue (RQ)
+ 	 */
+ 	if (phba->nvmet_support) {
+ 		if ((!phba->sli4_hba.nvmet_cqset) ||
+ 		    (!phba->sli4_hba.nvmet_mrq_hdr) ||
+ 		    (!phba->sli4_hba.nvmet_mrq_data)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"6130 MRQ CQ Queues not "
+ 					"allocated\n");
+ 			rc = -ENOMEM;
+ 			goto out_destroy;
+ 		}
+ 		if (phba->cfg_nvmet_mrq > 1) {
+ 			rc = lpfc_mrq_create(phba,
+ 					     phba->sli4_hba.nvmet_mrq_hdr,
+ 					     phba->sli4_hba.nvmet_mrq_data,
+ 					     phba->sli4_hba.nvmet_cqset,
+ 					     LPFC_NVMET);
+ 			if (rc) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"6098 Failed setup of NVMET "
+ 						"MRQ: rc = 0x%x\n",
+ 						(uint32_t)rc);
+ 				goto out_destroy;
+ 			}
+ 
+ 		} else {
+ 			rc = lpfc_rq_create(phba,
+ 					    phba->sli4_hba.nvmet_mrq_hdr[0],
+ 					    phba->sli4_hba.nvmet_mrq_data[0],
+ 					    phba->sli4_hba.nvmet_cqset[0],
+ 					    LPFC_NVMET);
+ 			if (rc) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 						"6057 Failed setup of NVMET "
+ 						"Receive Queue: rc = 0x%x\n",
+ 						(uint32_t)rc);
+ 				goto out_destroy;
+ 			}
+ 
+ 			lpfc_printf_log(
+ 				phba, KERN_INFO, LOG_INIT,
+ 				"6099 NVMET RQ setup: hdr-rq-id=%d, "
+ 				"dat-rq-id=%d parent cq-id=%d\n",
+ 				phba->sli4_hba.nvmet_mrq_hdr[0]->queue_id,
+ 				phba->sli4_hba.nvmet_mrq_data[0]->queue_id,
+ 				phba->sli4_hba.nvmet_cqset[0]->queue_id);
+ 
+ 		}
+ 	}
+ 
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	if (!phba->sli4_hba.hdr_rq || !phba->sli4_hba.dat_rq) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
  				"0540 Receive Queue not allocated\n");
@@@ -7962,34 -8744,76 +8411,64 @@@ lpfc_sli4_queue_unset(struct lpfc_hba *
  	/* Unset the queues created for Flash Optimized Fabric operations */
  	if (phba->cfg_fof)
  		lpfc_fof_queue_destroy(phba);
 -
  	/* Unset mailbox command work queue */
 -	if (phba->sli4_hba.mbx_wq)
 -		lpfc_mq_destroy(phba, phba->sli4_hba.mbx_wq);
 -
 -	/* Unset NVME LS work queue */
 -	if (phba->sli4_hba.nvmels_wq)
 -		lpfc_wq_destroy(phba, phba->sli4_hba.nvmels_wq);
 -
 +	lpfc_mq_destroy(phba, phba->sli4_hba.mbx_wq);
  	/* Unset ELS work queue */
 -	if (phba->sli4_hba.els_cq)
 -		lpfc_wq_destroy(phba, phba->sli4_hba.els_wq);
 -
 +	lpfc_wq_destroy(phba, phba->sli4_hba.els_wq);
  	/* Unset unsolicited receive queue */
 -	if (phba->sli4_hba.hdr_rq)
 -		lpfc_rq_destroy(phba, phba->sli4_hba.hdr_rq,
 -				phba->sli4_hba.dat_rq);
 -
 +	lpfc_rq_destroy(phba, phba->sli4_hba.hdr_rq, phba->sli4_hba.dat_rq);
  	/* Unset FCP work queue */
 -	if (phba->sli4_hba.fcp_wq)
 -		for (qidx = 0; qidx < phba->cfg_fcp_io_channel; qidx++)
 -			lpfc_wq_destroy(phba, phba->sli4_hba.fcp_wq[qidx]);
 -
 -	/* Unset NVME work queue */
 -	if (phba->sli4_hba.nvme_wq) {
 -		for (qidx = 0; qidx < phba->cfg_nvme_io_channel; qidx++)
 -			lpfc_wq_destroy(phba, phba->sli4_hba.nvme_wq[qidx]);
 +	if (phba->sli4_hba.fcp_wq) {
 +		for (fcp_qidx = 0; fcp_qidx < phba->cfg_fcp_io_channel;
 +		     fcp_qidx++)
 +			lpfc_wq_destroy(phba, phba->sli4_hba.fcp_wq[fcp_qidx]);
  	}
 -
  	/* Unset mailbox command complete queue */
 -	if (phba->sli4_hba.mbx_cq)
 -		lpfc_cq_destroy(phba, phba->sli4_hba.mbx_cq);
 -
 +	lpfc_cq_destroy(phba, phba->sli4_hba.mbx_cq);
  	/* Unset ELS complete queue */
++<<<<<<< HEAD
 +	lpfc_cq_destroy(phba, phba->sli4_hba.els_cq);
++=======
+ 	if (phba->sli4_hba.els_cq)
+ 		lpfc_cq_destroy(phba, phba->sli4_hba.els_cq);
+ 
+ 	/* Unset NVME LS complete queue */
+ 	if (phba->sli4_hba.nvmels_cq)
+ 		lpfc_cq_destroy(phba, phba->sli4_hba.nvmels_cq);
+ 
+ 	/* Unset NVME response complete queue */
+ 	if (phba->sli4_hba.nvme_cq)
+ 		for (qidx = 0; qidx < phba->cfg_nvme_io_channel; qidx++)
+ 			lpfc_cq_destroy(phba, phba->sli4_hba.nvme_cq[qidx]);
+ 
+ 	/* Unset NVMET MRQ queue */
+ 	if (phba->sli4_hba.nvmet_mrq_hdr) {
+ 		for (qidx = 0; qidx < phba->cfg_nvmet_mrq; qidx++)
+ 			lpfc_rq_destroy(phba,
+ 					phba->sli4_hba.nvmet_mrq_hdr[qidx],
+ 					phba->sli4_hba.nvmet_mrq_data[qidx]);
+ 	}
+ 
+ 	/* Unset NVMET CQ Set complete queue */
+ 	if (phba->sli4_hba.nvmet_cqset) {
+ 		for (qidx = 0; qidx < phba->cfg_nvmet_mrq; qidx++)
+ 			lpfc_cq_destroy(phba,
+ 					phba->sli4_hba.nvmet_cqset[qidx]);
+ 	}
+ 
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	/* Unset FCP response complete queue */
 -	if (phba->sli4_hba.fcp_cq)
 -		for (qidx = 0; qidx < phba->cfg_fcp_io_channel; qidx++)
 -			lpfc_cq_destroy(phba, phba->sli4_hba.fcp_cq[qidx]);
 -
 +	if (phba->sli4_hba.fcp_cq) {
 +		for (fcp_qidx = 0; fcp_qidx < phba->cfg_fcp_io_channel;
 +		     fcp_qidx++)
 +			lpfc_cq_destroy(phba, phba->sli4_hba.fcp_cq[fcp_qidx]);
 +	}
  	/* Unset fast-path event queue */
 -	if (phba->sli4_hba.hba_eq)
 -		for (qidx = 0; qidx < phba->io_channel_irqs; qidx++)
 -			lpfc_eq_destroy(phba, phba->sli4_hba.hba_eq[qidx]);
 +	if (phba->sli4_hba.hba_eq) {
 +		for (fcp_qidx = 0; fcp_qidx < phba->cfg_fcp_io_channel;
 +		     fcp_qidx++)
 +			lpfc_eq_destroy(phba, phba->sli4_hba.hba_eq[fcp_qidx]);
 +	}
  }
  
  /**
@@@ -9633,6 -10144,30 +10112,33 @@@ lpfc_get_sli4_parameters(struct lpfc_hb
  					   mbx_sli4_parameters);
  	phba->sli4_hba.extents_in_use = bf_get(cfg_ext, mbx_sli4_parameters);
  	phba->sli4_hba.rpi_hdrs_in_use = bf_get(cfg_hdrr, mbx_sli4_parameters);
++<<<<<<< HEAD
++=======
+ 	phba->nvme_support = (bf_get(cfg_nvme, mbx_sli4_parameters) &&
+ 			      bf_get(cfg_xib, mbx_sli4_parameters));
+ 
+ 	if ((phba->cfg_enable_fc4_type == LPFC_ENABLE_FCP) ||
+ 	    !phba->nvme_support) {
+ 		phba->nvme_support = 0;
+ 		phba->nvmet_support = 0;
+ 		phba->cfg_nvmet_mrq = 0;
+ 		phba->cfg_nvme_io_channel = 0;
+ 		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT | LOG_NVME,
+ 				"6101 Disabling NVME support: "
+ 				"Not supported by firmware: %d %d\n",
+ 				bf_get(cfg_nvme, mbx_sli4_parameters),
+ 				bf_get(cfg_xib, mbx_sli4_parameters));
+ 
+ 		/* If firmware doesn't support NVME, just use SCSI support */
+ 		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP))
+ 			return -ENODEV;
+ 		phba->cfg_enable_fc4_type = LPFC_ENABLE_FCP;
+ 	}
+ 
+ 	if (bf_get(cfg_xib, mbx_sli4_parameters) && phba->cfg_suppress_rsp)
+ 		phba->sli.sli_flag |= LPFC_SLI_SUPPRESS_RSP;
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  
  	/* Make sure that sge_supp_len can be handled by the driver */
  	if (sli4_params->sge_supp_len > LPFC_MAX_SGE_SIZE)
@@@ -10538,11 -11089,17 +11044,25 @@@ lpfc_pci_probe_one_s4(struct pci_dev *p
  		goto out_free_sysfs_attr;
  	}
  	/* Default to single EQ for non-MSI-X */
++<<<<<<< HEAD
 +	if (phba->intr_type != MSIX)
 +		adjusted_fcp_io_channel = 1;
 +	else
 +		adjusted_fcp_io_channel = phba->cfg_fcp_io_channel;
 +	phba->cfg_fcp_io_channel = adjusted_fcp_io_channel;
++=======
+ 	if (phba->intr_type != MSIX) {
+ 		if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP)
+ 			phba->cfg_fcp_io_channel = 1;
+ 		if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {
+ 			phba->cfg_nvme_io_channel = 1;
+ 			if (phba->nvmet_support)
+ 				phba->cfg_nvmet_mrq = 1;
+ 		}
+ 		phba->io_channel_irqs = 1;
+ 	}
+ 
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	/* Set up SLI-4 HBA */
  	if (lpfc_sli4_hba_setup(phba)) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
diff --cc drivers/scsi/lpfc/lpfc_mbox.c
index 28e5962e79d5,8f4bfdfd9910..000000000000
--- a/drivers/scsi/lpfc/lpfc_mbox.c
+++ b/drivers/scsi/lpfc/lpfc_mbox.c
@@@ -2434,8 -2437,41 +2437,46 @@@ lpfc_reg_fcfi(struct lpfc_hba *phba, st
  	memset(mbox, 0, sizeof(*mbox));
  	reg_fcfi = &mbox->u.mqe.un.reg_fcfi;
  	bf_set(lpfc_mqe_command, &mbox->u.mqe, MBX_REG_FCFI);
++<<<<<<< HEAD
 +	bf_set(lpfc_reg_fcfi_rq_id0, reg_fcfi, phba->sli4_hba.hdr_rq->queue_id);
 +	bf_set(lpfc_reg_fcfi_rq_id1, reg_fcfi, REG_FCF_INVALID_QID);
++=======
+ 	if (phba->nvmet_support == 0) {
+ 		bf_set(lpfc_reg_fcfi_rq_id0, reg_fcfi,
+ 		       phba->sli4_hba.hdr_rq->queue_id);
+ 		/* Match everything - rq_id0 */
+ 		bf_set(lpfc_reg_fcfi_type_match0, reg_fcfi, 0);
+ 		bf_set(lpfc_reg_fcfi_type_mask0, reg_fcfi, 0);
+ 		bf_set(lpfc_reg_fcfi_rctl_match0, reg_fcfi, 0);
+ 		bf_set(lpfc_reg_fcfi_rctl_mask0, reg_fcfi, 0);
+ 
+ 		bf_set(lpfc_reg_fcfi_rq_id1, reg_fcfi, REG_FCF_INVALID_QID);
+ 
+ 		/* addr mode is bit wise inverted value of fcf addr_mode */
+ 		bf_set(lpfc_reg_fcfi_mam, reg_fcfi,
+ 		       (~phba->fcf.addr_mode) & 0x3);
+ 	} else {
+ 		/* This is ONLY for NVMET MRQ == 1 */
+ 		if (phba->cfg_nvmet_mrq != 1)
+ 			return;
+ 
+ 		bf_set(lpfc_reg_fcfi_rq_id0, reg_fcfi,
+ 		       phba->sli4_hba.nvmet_mrq_hdr[0]->queue_id);
+ 		/* Match type FCP - rq_id0 */
+ 		bf_set(lpfc_reg_fcfi_type_match0, reg_fcfi, FC_TYPE_FCP);
+ 		bf_set(lpfc_reg_fcfi_type_mask0, reg_fcfi, 0xff);
+ 		bf_set(lpfc_reg_fcfi_rctl_match0, reg_fcfi,
+ 		       FC_RCTL_DD_UNSOL_CMD);
+ 
+ 		bf_set(lpfc_reg_fcfi_rq_id1, reg_fcfi,
+ 		       phba->sli4_hba.hdr_rq->queue_id);
+ 		/* Match everything else - rq_id1 */
+ 		bf_set(lpfc_reg_fcfi_type_match1, reg_fcfi, 0);
+ 		bf_set(lpfc_reg_fcfi_type_mask1, reg_fcfi, 0);
+ 		bf_set(lpfc_reg_fcfi_rctl_match1, reg_fcfi, 0);
+ 		bf_set(lpfc_reg_fcfi_rctl_mask1, reg_fcfi, 0);
+ 	}
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	bf_set(lpfc_reg_fcfi_rq_id2, reg_fcfi, REG_FCF_INVALID_QID);
  	bf_set(lpfc_reg_fcfi_rq_id3, reg_fcfi, REG_FCF_INVALID_QID);
  	bf_set(lpfc_reg_fcfi_info_index, reg_fcfi,
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index c1522c6b2e42,72164acb6656..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -5096,13 -5289,19 +5119,21 @@@ lpfc_sli4_arm_cqeq_intr(struct lpfc_hb
  	if (phba->cfg_fof)
  		lpfc_sli4_cq_release(phba->sli4_hba.oas_cq, LPFC_QUEUE_REARM);
  
 -	if (phba->sli4_hba.hba_eq)
 -		for (qidx = 0; qidx < phba->io_channel_irqs; qidx++)
 -			lpfc_sli4_eq_release(phba->sli4_hba.hba_eq[qidx],
 -						LPFC_QUEUE_REARM);
 +	if (phba->sli4_hba.hba_eq) {
 +		for (fcp_eqidx = 0; fcp_eqidx < phba->cfg_fcp_io_channel;
 +		     fcp_eqidx++)
 +			lpfc_sli4_eq_release(phba->sli4_hba.hba_eq[fcp_eqidx],
 +					     LPFC_QUEUE_REARM);
 +	}
  
+ 	if (phba->nvmet_support) {
+ 		for (qidx = 0; qidx < phba->cfg_nvmet_mrq; qidx++) {
+ 			lpfc_sli4_cq_release(
+ 				phba->sli4_hba.nvmet_cqset[qidx],
+ 				LPFC_QUEUE_REARM);
+ 		}
+ 	}
+ 
  	if (phba->cfg_fof)
  		lpfc_sli4_eq_release(phba->sli4_hba.fof_eq, LPFC_QUEUE_REARM);
  }
@@@ -6584,19 -6840,105 +6616,73 @@@ lpfc_sli4_hba_setup(struct lpfc_hba *ph
  				"0582 Error %d during els sgl post "
  				"operation\n", rc);
  		rc = -ENODEV;
 -		goto out_destroy_queue;
 -	}
 -	phba->sli4_hba.els_xri_cnt = rc;
 -
 -	if (phba->nvmet_support) {
 -		/* update host nvmet xri-sgl sizes and mappings */
 -		rc = lpfc_sli4_nvmet_sgl_update(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"6308 Failed to update nvmet-sgl size "
 -					"and mapping: %d\n", rc);
 -			goto out_destroy_queue;
 -		}
 -
 -		/* register the nvmet sgl pool to the port */
 -		rc = lpfc_sli4_repost_sgl_list(
 -			phba,
 -			&phba->sli4_hba.lpfc_nvmet_sgl_list,
 -			phba->sli4_hba.nvmet_xri_cnt);
 -		if (unlikely(rc < 0)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"3117 Error %d during nvmet "
 -					"sgl post\n", rc);
 -			rc = -ENODEV;
 -			goto out_destroy_queue;
 -		}
 -		phba->sli4_hba.nvmet_xri_cnt = rc;
 -		/* todo: tgt: create targetport */
 -	} else {
 -		/* update host scsi xri-sgl sizes and mappings */
 -		rc = lpfc_sli4_scsi_sgl_update(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"6309 Failed to update scsi-sgl size "
 -					"and mapping: %d\n", rc);
 -			goto out_destroy_queue;
 -		}
 -
 -		/* update host nvme xri-sgl sizes and mappings */
 -		rc = lpfc_sli4_nvme_sgl_update(phba);
 -		if (unlikely(rc)) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 -					"6082 Failed to update nvme-sgl size "
 -					"and mapping: %d\n", rc);
 -			goto out_destroy_queue;
 -		}
 +		goto out_free_mbox;
  	}
  
++<<<<<<< HEAD
 +	/* register the allocated scsi sgl pool to the port */
 +	rc = lpfc_sli4_repost_scsi_sgl_list(phba);
 +	if (unlikely(rc)) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 +				"0383 Error %d during scsi sgl post "
 +				"operation\n", rc);
 +		/* Some Scsi buffers were moved to the abort scsi list */
 +		/* A pci function reset will repost them */
 +		rc = -ENODEV;
 +		goto out_free_mbox;
++=======
+ 	if (phba->nvmet_support && phba->cfg_nvmet_mrq) {
+ 
+ 		/* Post initial buffers to all RQs created */
+ 		for (i = 0; i < phba->cfg_nvmet_mrq; i++) {
+ 			rqbp = phba->sli4_hba.nvmet_mrq_hdr[i]->rqbp;
+ 			INIT_LIST_HEAD(&rqbp->rqb_buffer_list);
+ 			rqbp->rqb_alloc_buffer = lpfc_sli4_nvmet_alloc;
+ 			rqbp->rqb_free_buffer = lpfc_sli4_nvmet_free;
+ 			rqbp->entry_count = 256;
+ 			rqbp->buffer_count = 0;
+ 
+ 			/* Divide by 4 and round down to multiple of 16 */
+ 			rc = (phba->cfg_nvmet_mrq_post >> 2) & 0xfff8;
+ 			phba->sli4_hba.nvmet_mrq_hdr[i]->entry_repost = rc;
+ 			phba->sli4_hba.nvmet_mrq_data[i]->entry_repost = rc;
+ 
+ 			lpfc_post_rq_buffer(
+ 				phba, phba->sli4_hba.nvmet_mrq_hdr[i],
+ 				phba->sli4_hba.nvmet_mrq_data[i],
+ 				phba->cfg_nvmet_mrq_post);
+ 		}
+ 	}
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
+ 		/* register the allocated scsi sgl pool to the port */
+ 		rc = lpfc_sli4_repost_scsi_sgl_list(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"0383 Error %d during scsi sgl post "
+ 					"operation\n", rc);
+ 			/* Some Scsi buffers were moved to abort scsi list */
+ 			/* A pci function reset will repost them */
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
+ 	}
+ 
+ 	if ((phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) &&
+ 	    (phba->nvmet_support == 0)) {
+ 
+ 		/* register the allocated nvme sgl pool to the port */
+ 		rc = lpfc_repost_nvme_sgl_list(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6116 Error %d during nvme sgl post "
+ 					"operation\n", rc);
+ 			/* Some NVME buffers were moved to abort nvme list */
+ 			/* A pci function reset will repost them */
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	}
  
  	/* Post the rpi header region to the device. */
@@@ -6610,20 -6952,42 +6696,59 @@@
  	}
  	lpfc_sli4_node_prep(phba);
  
++<<<<<<< HEAD
 +	/* Create all the SLI4 queues */
 +	rc = lpfc_sli4_queue_create(phba);
 +	if (rc) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"3089 Failed to allocate queues\n");
 +		rc = -ENODEV;
 +		goto out_stop_timers;
 +	}
 +	/* Set up all the queues to the device */
 +	rc = lpfc_sli4_queue_setup(phba);
 +	if (unlikely(rc)) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 +				"0381 Error %d during queue setup.\n ", rc);
 +		goto out_destroy_queue;
++=======
+ 	if (!(phba->hba_flag & HBA_FCOE_MODE)) {
+ 		if ((phba->nvmet_support == 0) || (phba->cfg_nvmet_mrq == 1)) {
+ 			/*
+ 			 * The FC Port needs to register FCFI (index 0)
+ 			 */
+ 			lpfc_reg_fcfi(phba, mboxq);
+ 			mboxq->vport = phba->pport;
+ 			rc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);
+ 			if (rc != MBX_SUCCESS)
+ 				goto out_unset_queue;
+ 			rc = 0;
+ 			phba->fcf.fcfi = bf_get(lpfc_reg_fcfi_fcfi,
+ 						&mboxq->u.mqe.un.reg_fcfi);
+ 		} else {
+ 			/* We are a NVME Target mode with MRQ > 1 */
+ 
+ 			/* First register the FCFI */
+ 			lpfc_reg_fcfi_mrq(phba, mboxq, 0);
+ 			mboxq->vport = phba->pport;
+ 			rc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);
+ 			if (rc != MBX_SUCCESS)
+ 				goto out_unset_queue;
+ 			rc = 0;
+ 			phba->fcf.fcfi = bf_get(lpfc_reg_fcfi_mrq_fcfi,
+ 						&mboxq->u.mqe.un.reg_fcfi_mrq);
+ 
+ 			/* Next register the MRQs */
+ 			lpfc_reg_fcfi_mrq(phba, mboxq, 1);
+ 			mboxq->vport = phba->pport;
+ 			rc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);
+ 			if (rc != MBX_SUCCESS)
+ 				goto out_unset_queue;
+ 			rc = 0;
+ 		}
+ 		/* Check if the port is configured to be disabled */
+ 		lpfc_sli_read_link_ste(phba);
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	}
  
  	/* Arm the CQs and then EQs on device */
@@@ -12318,7 -13063,102 +12443,106 @@@ lpfc_sli4_fp_handle_rel_wcqe(struct lpf
  }
  
  /**
++<<<<<<< HEAD
 + * lpfc_sli4_fp_handle_wcqe - Process fast-path work queue completion entry
++=======
+  * lpfc_sli4_nvmet_handle_rcqe - Process a receive-queue completion queue entry
+  * @phba: Pointer to HBA context object.
+  * @rcqe: Pointer to receive-queue completion queue entry.
+  *
+  * This routine process a receive-queue completion queue entry.
+  *
+  * Return: true if work posted to worker thread, otherwise false.
+  **/
+ static bool
+ lpfc_sli4_nvmet_handle_rcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,
+ 			    struct lpfc_rcqe *rcqe)
+ {
+ 	bool workposted = false;
+ 	struct lpfc_queue *hrq;
+ 	struct lpfc_queue *drq;
+ 	struct rqb_dmabuf *dma_buf;
+ 	struct fc_frame_header *fc_hdr;
+ 	uint32_t status, rq_id;
+ 	unsigned long iflags;
+ 	uint32_t fctl, idx;
+ 
+ 	if ((phba->nvmet_support == 0) ||
+ 	    (phba->sli4_hba.nvmet_cqset == NULL))
+ 		return workposted;
+ 
+ 	idx = cq->queue_id - phba->sli4_hba.nvmet_cqset[0]->queue_id;
+ 	hrq = phba->sli4_hba.nvmet_mrq_hdr[idx];
+ 	drq = phba->sli4_hba.nvmet_mrq_data[idx];
+ 
+ 	/* sanity check on queue memory */
+ 	if (unlikely(!hrq) || unlikely(!drq))
+ 		return workposted;
+ 
+ 	if (bf_get(lpfc_cqe_code, rcqe) == CQE_CODE_RECEIVE_V1)
+ 		rq_id = bf_get(lpfc_rcqe_rq_id_v1, rcqe);
+ 	else
+ 		rq_id = bf_get(lpfc_rcqe_rq_id, rcqe);
+ 
+ 	if ((phba->nvmet_support == 0) ||
+ 	    (rq_id != hrq->queue_id))
+ 		return workposted;
+ 
+ 	status = bf_get(lpfc_rcqe_status, rcqe);
+ 	switch (status) {
+ 	case FC_STATUS_RQ_BUF_LEN_EXCEEDED:
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 				"6126 Receive Frame Truncated!!\n");
+ 		hrq->RQ_buf_trunc++;
+ 		break;
+ 	case FC_STATUS_RQ_SUCCESS:
+ 		lpfc_sli4_rq_release(hrq, drq);
+ 		spin_lock_irqsave(&phba->hbalock, iflags);
+ 		dma_buf = lpfc_sli_rqbuf_get(phba, hrq);
+ 		if (!dma_buf) {
+ 			hrq->RQ_no_buf_found++;
+ 			spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 			goto out;
+ 		}
+ 		spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 		hrq->RQ_rcv_buf++;
+ 		fc_hdr = (struct fc_frame_header *)dma_buf->hbuf.virt;
+ 
+ 		/* Just some basic sanity checks on FCP Command frame */
+ 		fctl = (fc_hdr->fh_f_ctl[0] << 16 |
+ 		fc_hdr->fh_f_ctl[1] << 8 |
+ 		fc_hdr->fh_f_ctl[2]);
+ 		if (((fctl &
+ 		    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) !=
+ 		    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) ||
+ 		    (fc_hdr->fh_seq_cnt != 0)) /* 0 byte swapped is still 0 */
+ 			goto drop;
+ 
+ 		if (fc_hdr->fh_type == FC_TYPE_FCP) {
+ 			dma_buf->bytes_recv = bf_get(lpfc_rcqe_length,  rcqe);
+ 			/* todo: tgt: forward cmd iu to transport */
+ 			return false;
+ 		}
+ drop:
+ 		lpfc_in_buf_free(phba, &dma_buf->dbuf);
+ 		break;
+ 	case FC_STATUS_INSUFF_BUF_NEED_BUF:
+ 	case FC_STATUS_INSUFF_BUF_FRM_DISC:
+ 		hrq->RQ_no_posted_buf++;
+ 		/* Post more buffers if possible */
+ 		spin_lock_irqsave(&phba->hbalock, iflags);
+ 		phba->hba_flag |= HBA_POST_RECEIVE_BUFFER;
+ 		spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 		workposted = true;
+ 		break;
+ 	}
+ out:
+ 	return workposted;
+ }
+ 
+ /**
+  * lpfc_sli4_fp_handle_cqe - Process fast-path work queue completion entry
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
   * @cq: Pointer to the completion queue.
   * @eqe: Pointer to fast-path completion queue entry.
   *
@@@ -12357,9 -13202,17 +12581,20 @@@ lpfc_sli4_fp_handle_wcqe(struct lpfc_hb
  		workposted = lpfc_sli4_sp_handle_abort_xri_wcqe(phba, cq,
  				(struct sli4_wcqe_xri_aborted *)&wcqe);
  		break;
++<<<<<<< HEAD
++=======
+ 	case CQE_CODE_RECEIVE_V1:
+ 	case CQE_CODE_RECEIVE:
+ 		phba->last_completion_time = jiffies;
+ 		if (cq->subtype == LPFC_NVMET) {
+ 			workposted = lpfc_sli4_nvmet_handle_rcqe(
+ 				phba, cq, (struct lpfc_rcqe *)&wcqe);
+ 		}
+ 		break;
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	default:
  		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
 -				"0144 Not a valid CQE code: x%x\n",
 +				"0144 Not a valid WCQE code: x%x\n",
  				bf_get(lpfc_wcqe_c_code, &wcqe));
  		break;
  	}
@@@ -12382,10 -13235,10 +12617,10 @@@ static voi
  lpfc_sli4_hba_handle_eqe(struct lpfc_hba *phba, struct lpfc_eqe *eqe,
  			uint32_t qidx)
  {
 -	struct lpfc_queue *cq = NULL;
 +	struct lpfc_queue *cq;
  	struct lpfc_cqe *cqe;
  	bool workposted = false;
- 	uint16_t cqid;
+ 	uint16_t cqid, id;
  	int ecount = 0;
  
  	if (unlikely(bf_get_le32(lpfc_eqe_major_code, eqe) != 0)) {
@@@ -12400,25 -13253,38 +12635,60 @@@
  	/* Get the reference to the corresponding CQ */
  	cqid = bf_get_le32(lpfc_eqe_resource_id, eqe);
  
++<<<<<<< HEAD
 +	/* Check if this is a Slow path event */
 +	if (unlikely(cqid != phba->sli4_hba.fcp_cq_map[qidx])) {
 +		lpfc_sli4_sp_handle_eqe(phba, eqe,
 +			phba->sli4_hba.hba_eq[qidx]);
 +		return;
 +	}
 +
 +	if (unlikely(!phba->sli4_hba.fcp_cq)) {
 +		lpfc_printf_log(phba, KERN_WARNING, LOG_SLI,
 +				"3146 Fast-path completion queues "
 +				"does not exist\n");
 +		return;
 +	}
 +	cq = phba->sli4_hba.fcp_cq[qidx];
 +	if (unlikely(!cq)) {
 +		if (phba->sli.sli_flag & LPFC_SLI_ACTIVE)
 +			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
 +					"0367 Fast-path completion queue "
 +					"(%d) does not exist\n", qidx);
++=======
+ 	if (phba->cfg_nvmet_mrq && phba->sli4_hba.nvmet_cqset) {
+ 		id = phba->sli4_hba.nvmet_cqset[0]->queue_id;
+ 		if ((cqid >= id) && (cqid < (id + phba->cfg_nvmet_mrq))) {
+ 			/* Process NVMET unsol rcv */
+ 			cq = phba->sli4_hba.nvmet_cqset[cqid - id];
+ 			goto  process_cq;
+ 		}
+ 	}
+ 
+ 	if (phba->sli4_hba.nvme_cq_map &&
+ 	    (cqid == phba->sli4_hba.nvme_cq_map[qidx])) {
+ 		/* Process NVME / NVMET command completion */
+ 		cq = phba->sli4_hba.nvme_cq[qidx];
+ 		goto  process_cq;
+ 	}
+ 
+ 	if (phba->sli4_hba.fcp_cq_map &&
+ 	    (cqid == phba->sli4_hba.fcp_cq_map[qidx])) {
+ 		/* Process FCP command completion */
+ 		cq = phba->sli4_hba.fcp_cq[qidx];
+ 		goto  process_cq;
+ 	}
+ 
+ 	if (phba->sli4_hba.nvmels_cq &&
+ 	    (cqid == phba->sli4_hba.nvmels_cq->queue_id)) {
+ 		/* Process NVME unsol rcv */
+ 		cq = phba->sli4_hba.nvmels_cq;
+ 	}
+ 
+ 	/* Otherwise this is a Slow path event */
+ 	if (cq == NULL) {
+ 		lpfc_sli4_sp_handle_eqe(phba, eqe, phba->sli4_hba.hba_eq[qidx]);
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  		return;
  	}
  
diff --cc drivers/scsi/lpfc/lpfc_sli4.h
index 0b88b5703e0f,3bfdff1a4c53..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@@ -515,15 -543,21 +515,26 @@@ struct lpfc_sli4_hba 
  	uint32_t ue_to_rp;
  	struct lpfc_register sli_intf;
  	struct lpfc_pc_sli4_params pc_sli4_params;
 +	struct msix_entry *msix_entries;
  	uint8_t handler_name[LPFC_SLI4_HANDLER_CNT][LPFC_SLI4_HANDLER_NAME_SZ];
 -	struct lpfc_hba_eq_hdl *hba_eq_hdl; /* HBA per-WQ handle */
 +	struct lpfc_fcp_eq_hdl *fcp_eq_hdl; /* FCP per-WQ handle */
  
  	/* Pointers to the constructed SLI4 queues */
++<<<<<<< HEAD
 +	struct lpfc_queue **hba_eq;/* Event queues for HBA */
 +	struct lpfc_queue **fcp_cq;/* Fast-path FCP compl queue */
 +	struct lpfc_queue **fcp_wq;/* Fast-path FCP work queue */
++=======
+ 	struct lpfc_queue **hba_eq;  /* Event queues for HBA */
+ 	struct lpfc_queue **fcp_cq;  /* Fast-path FCP compl queue */
+ 	struct lpfc_queue **nvme_cq; /* Fast-path NVME compl queue */
+ 	struct lpfc_queue **nvmet_cqset; /* Fast-path NVMET CQ Set queues */
+ 	struct lpfc_queue **nvmet_mrq_hdr; /* Fast-path NVMET hdr MRQs */
+ 	struct lpfc_queue **nvmet_mrq_data; /* Fast-path NVMET data MRQs */
+ 	struct lpfc_queue **fcp_wq;  /* Fast-path FCP work queue */
+ 	struct lpfc_queue **nvme_wq; /* Fast-path NVME work queue */
++>>>>>>> 2d7dbc4c2775 (scsi: lpfc: NVME Target: Receive buffer updates)
  	uint16_t *fcp_cq_map;
 -	uint16_t *nvme_cq_map;
 -	struct list_head lpfc_wq_list;
  
  	struct lpfc_queue *mbx_cq; /* Slow-path mailbox complete queue */
  	struct lpfc_queue *els_cq; /* Slow-path ELS response complete queue */
@@@ -694,9 -744,12 +707,12 @@@ struct lpfc_queue *lpfc_sli4_queue_allo
  			uint32_t);
  void lpfc_sli4_queue_free(struct lpfc_queue *);
  int lpfc_eq_create(struct lpfc_hba *, struct lpfc_queue *, uint32_t);
 -int lpfc_modify_hba_eq_delay(struct lpfc_hba *phba, uint32_t startq);
 +int lpfc_modify_fcp_eq_delay(struct lpfc_hba *, uint32_t);
  int lpfc_cq_create(struct lpfc_hba *, struct lpfc_queue *,
  			struct lpfc_queue *, uint32_t, uint32_t);
+ int lpfc_cq_create_set(struct lpfc_hba *phba, struct lpfc_queue **cqp,
+ 			struct lpfc_queue **eqp, uint32_t type,
+ 			uint32_t subtype);
  int32_t lpfc_mq_create(struct lpfc_hba *, struct lpfc_queue *,
  		       struct lpfc_queue *, uint32_t);
  int lpfc_wq_create(struct lpfc_hba *, struct lpfc_queue *,
* Unmerged path drivers/scsi/lpfc/lpfc.h
* Unmerged path drivers/scsi/lpfc/lpfc_attr.c
diff --git a/drivers/scsi/lpfc/lpfc_crtn.h b/drivers/scsi/lpfc/lpfc_crtn.h
index 5d7cb958427c..d5a89dc24ca8 100644
--- a/drivers/scsi/lpfc/lpfc_crtn.h
+++ b/drivers/scsi/lpfc/lpfc_crtn.h
@@ -220,6 +220,7 @@ void lpfc_reg_vfi(struct lpfcMboxq *, struct lpfc_vport *, dma_addr_t);
 void lpfc_init_vpi(struct lpfc_hba *, struct lpfcMboxq *, uint16_t);
 void lpfc_unreg_vfi(struct lpfcMboxq *, struct lpfc_vport *);
 void lpfc_reg_fcfi(struct lpfc_hba *, struct lpfcMboxq *);
+void lpfc_reg_fcfi_mrq(struct lpfc_hba *phba, struct lpfcMboxq *mbox, int mode);
 void lpfc_unreg_fcfi(struct lpfcMboxq *, uint16_t);
 void lpfc_resume_rpi(struct lpfcMboxq *, struct lpfc_nodelist *);
 int lpfc_check_pending_fcoe_event(struct lpfc_hba *, uint8_t);
* Unmerged path drivers/scsi/lpfc/lpfc_debugfs.c
* Unmerged path drivers/scsi/lpfc/lpfc_hw4.h
* Unmerged path drivers/scsi/lpfc/lpfc_init.c
* Unmerged path drivers/scsi/lpfc/lpfc_mbox.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli4.h
