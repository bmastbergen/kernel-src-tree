userfaultfd: shmem: add userfaultfd hook for shared memory faults

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Mike Rapoport <rppt@linux.vnet.ibm.com>
commit cfda05267f7bd02b5ae5ac6a37fbbdf3b9c41b57
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/cfda0526.failed

When processing a page fault in shared memory area for not present page,
check the VMA determine if faults are to be handled by userfaultfd.  If
so, delegate the page fault to handle_userfault.

Link: http://lkml.kernel.org/r/20161216144821.5183-33-aarcange@redhat.com
	Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
	Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
	Cc: Michael Rapoport <RAPOPORT@il.ibm.com>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Pavel Emelyanov <xemul@parallels.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit cfda05267f7bd02b5ae5ac6a37fbbdf3b9c41b57)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/shmem.c
diff --cc mm/shmem.c
index 021ba8dffccb,ab6644194fee..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -68,13 -69,19 +68,19 @@@ static struct vfsmount *shm_mnt
  #include <linux/highmem.h>
  #include <linux/seq_file.h>
  #include <linux/magic.h>
 -#include <linux/syscalls.h>
  #include <linux/fcntl.h>
++<<<<<<< HEAD
++=======
+ #include <uapi/linux/memfd.h>
+ #include <linux/userfaultfd_k.h>
+ #include <linux/rmap.h>
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  
 -#include <linux/uaccess.h>
 +#include <asm/uaccess.h>
  #include <asm/pgtable.h>
  
 -#include "internal.h"
 -
 -#define BLOCKS_PER_PAGE  (PAGE_SIZE/512)
 -#define VM_ACCT(size)    (PAGE_ALIGN(size) >> PAGE_SHIFT)
 +#define BLOCKS_PER_PAGE  (PAGE_CACHE_SIZE/512)
 +#define VM_ACCT(size)    (PAGE_CACHE_ALIGN(size) >> PAGE_SHIFT)
  
  /* Pretend that each entry is of this size in directory's i_size */
  #define BOGO_DIRENT_SIZE 20
@@@ -120,13 -118,15 +126,23 @@@ static bool shmem_should_replace_page(s
  static int shmem_replace_page(struct page **pagep, gfp_t gfp,
  				struct shmem_inode_info *info, pgoff_t index);
  static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
++<<<<<<< HEAD
 +	struct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type);
++=======
+ 		struct page **pagep, enum sgp_type sgp,
+ 		gfp_t gfp, struct vm_area_struct *vma,
+ 		struct vm_fault *vmf, int *fault_type);
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  
 -int shmem_getpage(struct inode *inode, pgoff_t index,
 -		struct page **pagep, enum sgp_type sgp)
 +static inline int shmem_getpage(struct inode *inode, pgoff_t index,
 +	struct page **pagep, enum sgp_type sgp, int *fault_type)
  {
  	return shmem_getpage_gfp(inode, index, pagep, sgp,
++<<<<<<< HEAD
 +			mapping_gfp_mask(inode->i_mapping), fault_type);
++=======
+ 		mapping_gfp_mask(inode->i_mapping), NULL, NULL, NULL);
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  }
  
  static inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)
@@@ -1097,16 -1573,24 +1113,21 @@@ static int shmem_replace_page(struct pa
   *
   * If we allocate a new one we do not mark it dirty. That's up to the
   * vm. If we swap it in we mark it dirty since we also free the swap
 - * entry since a page cannot live in both the swap and page cache.
 - *
 - * fault_mm and fault_type are only supplied by shmem_fault:
 - * otherwise they are NULL.
 + * entry since a page cannot live in both the swap and page cache
   */
  static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
++<<<<<<< HEAD
 +	struct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type)
++=======
+ 	struct page **pagep, enum sgp_type sgp, gfp_t gfp,
+ 	struct vm_area_struct *vma, struct vm_fault *vmf, int *fault_type)
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  {
  	struct address_space *mapping = inode->i_mapping;
 -	struct shmem_inode_info *info = SHMEM_I(inode);
 +	struct shmem_inode_info *info;
  	struct shmem_sb_info *sbinfo;
 -	struct mm_struct *charge_mm;
 -	struct mem_cgroup *memcg;
  	struct page *page;
  	swp_entry_t swap;
 -	enum sgp_type sgp_huge = sgp;
 -	pgoff_t hindex = index;
  	int error;
  	int once = 0;
  	int alloced = 0;
@@@ -1144,16 -1633,21 +1165,28 @@@ repeat
  	 * Fast cache lookup did not find it:
  	 * bring it back from swap or allocate.
  	 */
 +	info = SHMEM_I(inode);
  	sbinfo = SHMEM_SB(inode->i_sb);
++<<<<<<< HEAD
++=======
+ 	charge_mm = vma ? vma->vm_mm : current->mm;
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  
  	if (swap.val) {
  		/* Look it up and read it in.. */
  		page = lookup_swap_cache(swap);
  		if (!page) {
 -			/* Or update major stats only when swapin succeeds?? */
 -			if (fault_type) {
 +			/* here we actually do the io */
 +			if (fault_type)
  				*fault_type |= VM_FAULT_MAJOR;
++<<<<<<< HEAD
++=======
+ 				count_vm_event(PGMAJFAULT);
+ 				mem_cgroup_count_vm_event(charge_mm,
+ 							  PGMAJFAULT);
+ 			}
+ 			/* Here we actually start the io */
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  			page = shmem_swapin(swap, gfp, info, index);
  			if (!page) {
  				error = -ENOMEM;
@@@ -1213,35 -1714,82 +1246,94 @@@
  		swap_free(swap);
  
  	} else {
++<<<<<<< HEAD
 +		if (shmem_acct_block(info->flags)) {
 +			error = -ENOSPC;
++=======
+ 		if (vma && userfaultfd_missing(vma)) {
+ 			*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+ 			return 0;
+ 		}
+ 
+ 		/* shmem_symlink() */
+ 		if (mapping->a_ops != &shmem_aops)
+ 			goto alloc_nohuge;
+ 		if (shmem_huge == SHMEM_HUGE_DENY || sgp_huge == SGP_NOHUGE)
+ 			goto alloc_nohuge;
+ 		if (shmem_huge == SHMEM_HUGE_FORCE)
+ 			goto alloc_huge;
+ 		switch (sbinfo->huge) {
+ 			loff_t i_size;
+ 			pgoff_t off;
+ 		case SHMEM_HUGE_NEVER:
+ 			goto alloc_nohuge;
+ 		case SHMEM_HUGE_WITHIN_SIZE:
+ 			off = round_up(index, HPAGE_PMD_NR);
+ 			i_size = round_up(i_size_read(inode), PAGE_SIZE);
+ 			if (i_size >= HPAGE_PMD_SIZE &&
+ 					i_size >> PAGE_SHIFT >= off)
+ 				goto alloc_huge;
+ 			/* fallthrough */
+ 		case SHMEM_HUGE_ADVISE:
+ 			if (sgp_huge == SGP_HUGE)
+ 				goto alloc_huge;
+ 			/* TODO: implement fadvise() hints */
+ 			goto alloc_nohuge;
+ 		}
+ 
+ alloc_huge:
+ 		page = shmem_alloc_and_acct_page(gfp, info, sbinfo,
+ 				index, true);
+ 		if (IS_ERR(page)) {
+ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, info, sbinfo,
+ 					index, false);
+ 		}
+ 		if (IS_ERR(page)) {
+ 			int retry = 5;
+ 			error = PTR_ERR(page);
+ 			page = NULL;
+ 			if (error != -ENOSPC)
+ 				goto failed;
+ 			/*
+ 			 * Try to reclaim some spece by splitting a huge page
+ 			 * beyond i_size on the filesystem.
+ 			 */
+ 			while (retry--) {
+ 				int ret;
+ 				ret = shmem_unused_huge_shrink(sbinfo, NULL, 1);
+ 				if (ret == SHRINK_STOP)
+ 					break;
+ 				if (ret)
+ 					goto alloc_nohuge;
+ 			}
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  			goto failed;
  		}
 +		if (sbinfo->max_blocks) {
 +			if (percpu_counter_compare(&sbinfo->used_blocks,
 +						sbinfo->max_blocks) >= 0) {
 +				error = -ENOSPC;
 +				goto unacct;
 +			}
 +			percpu_counter_inc(&sbinfo->used_blocks);
 +		}
  
 -		if (PageTransHuge(page))
 -			hindex = round_down(index, HPAGE_PMD_NR);
 -		else
 -			hindex = index;
 -
 -		if (sgp == SGP_WRITE)
 -			__SetPageReferenced(page);
 +		page = shmem_alloc_page(gfp, info, index);
 +		if (!page) {
 +			error = -ENOMEM;
 +			goto decused;
 +		}
  
 -		error = mem_cgroup_try_charge(page, charge_mm, gfp, &memcg,
 -				PageTransHuge(page));
 +		SetPageSwapBacked(page);
 +		__set_page_locked(page);
 +		error = mem_cgroup_cache_charge(page, current->mm,
 +						gfp & GFP_RECLAIM_MASK);
  		if (error)
 -			goto unacct;
 -		error = radix_tree_maybe_preload_order(gfp & GFP_RECLAIM_MASK,
 -				compound_order(page));
 +			goto decused;
 +		error = radix_tree_maybe_preload(gfp & GFP_RECLAIM_MASK);
  		if (!error) {
 -			error = shmem_add_to_page_cache(page, mapping, hindex,
 -							NULL);
 +			error = shmem_add_to_page_cache(page, mapping, index,
 +							gfp, NULL);
  			radix_tree_preload_end();
  		}
  		if (error) {
@@@ -1391,15 -1974,105 +1483,26 @@@ static int shmem_fault(struct vm_area_s
  		spin_unlock(&inode->i_lock);
  	}
  
++<<<<<<< HEAD
 +	error = shmem_getpage(inode, vmf->pgoff, &vmf->page, SGP_CACHE, &ret);
++=======
+ 	sgp = SGP_CACHE;
+ 	if (vma->vm_flags & VM_HUGEPAGE)
+ 		sgp = SGP_HUGE;
+ 	else if (vma->vm_flags & VM_NOHUGEPAGE)
+ 		sgp = SGP_NOHUGE;
+ 
+ 	error = shmem_getpage_gfp(inode, vmf->pgoff, &vmf->page, sgp,
+ 				  gfp, vma, vmf, &ret);
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  	if (error)
  		return ((error == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS);
 -	return ret;
 -}
  
 -unsigned long shmem_get_unmapped_area(struct file *file,
 -				      unsigned long uaddr, unsigned long len,
 -				      unsigned long pgoff, unsigned long flags)
 -{
 -	unsigned long (*get_area)(struct file *,
 -		unsigned long, unsigned long, unsigned long, unsigned long);
 -	unsigned long addr;
 -	unsigned long offset;
 -	unsigned long inflated_len;
 -	unsigned long inflated_addr;
 -	unsigned long inflated_offset;
 -
 -	if (len > TASK_SIZE)
 -		return -ENOMEM;
 -
 -	get_area = current->mm->get_unmapped_area;
 -	addr = get_area(file, uaddr, len, pgoff, flags);
 -
 -	if (!IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE))
 -		return addr;
 -	if (IS_ERR_VALUE(addr))
 -		return addr;
 -	if (addr & ~PAGE_MASK)
 -		return addr;
 -	if (addr > TASK_SIZE - len)
 -		return addr;
 -
 -	if (shmem_huge == SHMEM_HUGE_DENY)
 -		return addr;
 -	if (len < HPAGE_PMD_SIZE)
 -		return addr;
 -	if (flags & MAP_FIXED)
 -		return addr;
 -	/*
 -	 * Our priority is to support MAP_SHARED mapped hugely;
 -	 * and support MAP_PRIVATE mapped hugely too, until it is COWed.
 -	 * But if caller specified an address hint, respect that as before.
 -	 */
 -	if (uaddr)
 -		return addr;
 -
 -	if (shmem_huge != SHMEM_HUGE_FORCE) {
 -		struct super_block *sb;
 -
 -		if (file) {
 -			VM_BUG_ON(file->f_op != &shmem_file_operations);
 -			sb = file_inode(file)->i_sb;
 -		} else {
 -			/*
 -			 * Called directly from mm/mmap.c, or drivers/char/mem.c
 -			 * for "/dev/zero", to create a shared anonymous object.
 -			 */
 -			if (IS_ERR(shm_mnt))
 -				return addr;
 -			sb = shm_mnt->mnt_sb;
 -		}
 -		if (SHMEM_SB(sb)->huge == SHMEM_HUGE_NEVER)
 -			return addr;
 +	if (ret & VM_FAULT_MAJOR) {
 +		count_vm_event(PGMAJFAULT);
 +		mem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);
  	}
 -
 -	offset = (pgoff << PAGE_SHIFT) & (HPAGE_PMD_SIZE-1);
 -	if (offset && offset + len < 2 * HPAGE_PMD_SIZE)
 -		return addr;
 -	if ((addr & (HPAGE_PMD_SIZE-1)) == offset)
 -		return addr;
 -
 -	inflated_len = len + HPAGE_PMD_SIZE - PAGE_SIZE;
 -	if (inflated_len > TASK_SIZE)
 -		return addr;
 -	if (inflated_len < len)
 -		return addr;
 -
 -	inflated_addr = get_area(NULL, 0, inflated_len, 0, flags);
 -	if (IS_ERR_VALUE(inflated_addr))
 -		return addr;
 -	if (inflated_addr & ~PAGE_MASK)
 -		return addr;
 -
 -	inflated_offset = inflated_addr & (HPAGE_PMD_SIZE-1);
 -	inflated_addr += offset - inflated_offset;
 -	if (inflated_offset > offset)
 -		inflated_addr += HPAGE_PMD_SIZE;
 -
 -	if (inflated_addr > TASK_SIZE - len)
 -		return addr;
 -	return inflated_addr;
 +	return ret;
  }
  
  #ifdef CONFIG_NUMA
@@@ -3513,7 -4261,8 +3616,12 @@@ struct page *shmem_read_mapping_page_gf
  	int error;
  
  	BUG_ON(mapping->a_ops != &shmem_aops);
++<<<<<<< HEAD
 +	error = shmem_getpage_gfp(inode, index, &page, SGP_CACHE, gfp, NULL);
++=======
+ 	error = shmem_getpage_gfp(inode, index, &page, SGP_CACHE,
+ 				  gfp, NULL, NULL, NULL);
++>>>>>>> cfda05267f7b (userfaultfd: shmem: add userfaultfd hook for shared memory faults)
  	if (error)
  		page = ERR_PTR(error);
  	else
* Unmerged path mm/shmem.c
