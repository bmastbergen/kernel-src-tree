KVM: MMU: simplify mmu_need_write_protect

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Xiao Guangrong <guangrong.xiao@linux.intel.com>
commit 5c520e90af3ad546bf328d2c9306c72bf3da6afe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/5c520e90.failed

Now, all non-leaf shadow page are page tracked, if gfn is not tracked
there is no non-leaf shadow page of gfn is existed, we can directly
make the shadow page of gfn to unsync

	Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 5c520e90af3ad546bf328d2c9306c72bf3da6afe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 9645a520c25f,d6b264b492c9..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -2436,110 -2444,7 +2436,114 @@@ int kvm_mmu_unprotect_page(struct kvm *
  }
  EXPORT_SYMBOL_GPL(kvm_mmu_unprotect_page);
  
++<<<<<<< HEAD
 +/*
 + * The function is based on mtrr_type_lookup() in
 + * arch/x86/kernel/cpu/mtrr/generic.c
 + */
 +static int get_mtrr_type(struct mtrr_state_type *mtrr_state,
 +			 u64 start, u64 end)
 +{
 +	u64 base, mask;
 +	u8 prev_match, curr_match;
 +	int i, num_var_ranges = KVM_NR_VAR_MTRR;
 +
 +	/* MTRR is completely disabled, use UC for all of physical memory. */
 +	if (!(mtrr_state->enabled & 0x2))
 +		return MTRR_TYPE_UNCACHABLE;
 +
 +	/* Make end inclusive end, instead of exclusive */
 +	end--;
 +
 +	/* Look in fixed ranges. Just return the type as per start */
 +	if (mtrr_state->have_fixed && (mtrr_state->enabled & 0x1) &&
 +	      (start < 0x100000)) {
 +		int idx;
 +
 +		if (start < 0x80000) {
 +			idx = 0;
 +			idx += (start >> 16);
 +			return mtrr_state->fixed_ranges[idx];
 +		} else if (start < 0xC0000) {
 +			idx = 1 * 8;
 +			idx += ((start - 0x80000) >> 14);
 +			return mtrr_state->fixed_ranges[idx];
 +		} else if (start < 0x1000000) {
 +			idx = 3 * 8;
 +			idx += ((start - 0xC0000) >> 12);
 +			return mtrr_state->fixed_ranges[idx];
 +		}
 +	}
 +
 +	/*
 +	 * Look in variable ranges
 +	 * Look of multiple ranges matching this address and pick type
 +	 * as per MTRR precedence
 +	 */
 +	prev_match = 0xFF;
 +	for (i = 0; i < num_var_ranges; ++i) {
 +		unsigned short start_state, end_state;
 +
 +		if (!(mtrr_state->var_ranges[i].mask_lo & (1 << 11)))
 +			continue;
 +
 +		base = (((u64)mtrr_state->var_ranges[i].base_hi) << 32) +
 +		       (mtrr_state->var_ranges[i].base_lo & PAGE_MASK);
 +		mask = (((u64)mtrr_state->var_ranges[i].mask_hi) << 32) +
 +		       (mtrr_state->var_ranges[i].mask_lo & PAGE_MASK);
 +
 +		start_state = ((start & mask) == (base & mask));
 +		end_state = ((end & mask) == (base & mask));
 +		if (start_state != end_state)
 +			return 0xFE;
 +
 +		if ((start & mask) != (base & mask))
 +			continue;
 +
 +		curr_match = mtrr_state->var_ranges[i].base_lo & 0xff;
 +		if (prev_match == 0xFF) {
 +			prev_match = curr_match;
 +			continue;
 +		}
 +
 +		if (prev_match == MTRR_TYPE_UNCACHABLE ||
 +		    curr_match == MTRR_TYPE_UNCACHABLE)
 +			return MTRR_TYPE_UNCACHABLE;
 +
 +		if ((prev_match == MTRR_TYPE_WRBACK &&
 +		     curr_match == MTRR_TYPE_WRTHROUGH) ||
 +		    (prev_match == MTRR_TYPE_WRTHROUGH &&
 +		     curr_match == MTRR_TYPE_WRBACK)) {
 +			prev_match = MTRR_TYPE_WRTHROUGH;
 +			curr_match = MTRR_TYPE_WRTHROUGH;
 +		}
 +
 +		if (prev_match != curr_match)
 +			return MTRR_TYPE_UNCACHABLE;
 +	}
 +
 +	if (prev_match != 0xFF)
 +		return prev_match;
 +
 +	return mtrr_state->def_type;
 +}
 +
 +u8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)
 +{
 +	u8 mtrr;
 +
 +	mtrr = get_mtrr_type(&vcpu->arch.mtrr_state, gfn << PAGE_SHIFT,
 +			     (gfn << PAGE_SHIFT) + PAGE_SIZE);
 +	if (mtrr == 0xfe || mtrr == 0xff)
 +		mtrr = MTRR_TYPE_WRBACK;
 +	return mtrr;
 +}
 +EXPORT_SYMBOL_GPL(kvm_get_guest_memory_type);
 +
 +static void __kvm_unsync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
++=======
+ static void kvm_unsync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
++>>>>>>> 5c520e90af3a (KVM: MMU: simplify mmu_need_write_protect)
  {
  	trace_kvm_mmu_unsync_page(sp);
  	++vcpu->kvm->stat.mmu_unsync;
@@@ -2548,37 -2453,26 +2552,58 @@@
  	kvm_mmu_mark_parents_unsync(sp);
  }
  
++<<<<<<< HEAD
 +static void kvm_unsync_pages(struct kvm_vcpu *vcpu,  gfn_t gfn)
 +{
 +	struct kvm_mmu_page *s;
 +
 +	for_each_gfn_indirect_valid_sp(vcpu->kvm, s, gfn) {
 +		if (s->unsync)
 +			continue;
 +		WARN_ON(s->role.level != PT_PAGE_TABLE_LEVEL);
 +		__kvm_unsync_page(vcpu, s);
 +	}
 +}
 +
 +static int mmu_need_write_protect(struct kvm_vcpu *vcpu, gfn_t gfn,
 +				  bool can_unsync)
++=======
+ static bool mmu_need_write_protect(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 				   bool can_unsync)
++>>>>>>> 5c520e90af3a (KVM: MMU: simplify mmu_need_write_protect)
  {
- 	struct kvm_mmu_page *s;
- 	bool need_unsync = false;
+ 	struct kvm_mmu_page *sp;
  
++<<<<<<< HEAD
 +	for_each_gfn_indirect_valid_sp(vcpu->kvm, s, gfn) {
++=======
+ 	if (kvm_page_track_is_active(vcpu, gfn, KVM_PAGE_TRACK_WRITE))
+ 		return true;
+ 
+ 	for_each_gfn_indirect_valid_sp(vcpu->kvm, sp, gfn) {
++>>>>>>> 5c520e90af3a (KVM: MMU: simplify mmu_need_write_protect)
  		if (!can_unsync)
 -			return true;
 +			return 1;
  
++<<<<<<< HEAD
 +		if (s->role.level != PT_PAGE_TABLE_LEVEL)
 +			return 1;
++=======
+ 		if (sp->unsync)
+ 			continue;
++>>>>>>> 5c520e90af3a (KVM: MMU: simplify mmu_need_write_protect)
  
- 		if (!s->unsync)
- 			need_unsync = true;
+ 		WARN_ON(sp->role.level != PT_PAGE_TABLE_LEVEL);
+ 		kvm_unsync_page(vcpu, sp);
  	}
++<<<<<<< HEAD
 +	if (need_unsync)
 +		kvm_unsync_pages(vcpu, gfn);
 +	return 0;
++=======
+ 
+ 	return false;
++>>>>>>> 5c520e90af3a (KVM: MMU: simplify mmu_need_write_protect)
  }
  
  static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
* Unmerged path arch/x86/kvm/mmu.c
