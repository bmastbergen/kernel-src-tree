blk-mq: Introduce blk_mq_delay_run_hw_queue()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Bart Van Assche <bart.vanassche@sandisk.com>
commit 7587a5ae7eef0439f7be31f1b5959af062bbc5ec
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/7587a5ae.failed

Introduce a function that runs a hardware queue unconditionally
after a delay. Note: there is already a function that stops and
restarts a hardware queue after a delay, namely blk_mq_delay_queue().

This function will be used in the next patch in this series.

	Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Hannes Reinecke <hare@suse.de>
	Cc: Long Li <longli@microsoft.com>
	Cc: K. Y. Srinivasan <kys@microsoft.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 7587a5ae7eef0439f7be31f1b5959af062bbc5ec)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	include/linux/blk-mq.h
diff --cc block/blk-mq.c
index c0285adbe811,ad45ae743186..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -871,13 -1135,14 +871,14 @@@ static int blk_mq_hctx_next_cpu(struct 
  	return hctx->next_cpu;
  }
  
- void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
+ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
+ 					unsigned long msecs)
  {
 -	if (unlikely(blk_mq_hctx_stopped(hctx) ||
 -		     !blk_mq_hw_queue_mapped(hctx)))
 +	if (unlikely(test_bit(BLK_MQ_S_STOPPED, &hctx->state) ||
 +	    !blk_mq_hw_queue_mapped(hctx)))
  		return;
  
 -	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
 +	if (!async) {
  		int cpu = get_cpu();
  		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
  			__blk_mq_run_hw_queue(hctx);
@@@ -888,8 -1153,24 +889,29 @@@
  		put_cpu();
  	}
  
++<<<<<<< HEAD
 +	kblockd_schedule_delayed_work_on(blk_mq_hctx_next_cpu(hctx),
 +			&hctx->run_work, 0);
++=======
+ 	if (msecs == 0)
+ 		kblockd_schedule_work_on(blk_mq_hctx_next_cpu(hctx),
+ 					 &hctx->run_work);
+ 	else
+ 		kblockd_schedule_delayed_work_on(blk_mq_hctx_next_cpu(hctx),
+ 						 &hctx->delayed_run_work,
+ 						 msecs_to_jiffies(msecs));
+ }
+ 
+ void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
+ {
+ 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ }
+ EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
+ 
+ void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
+ {
+ 	__blk_mq_delay_run_hw_queue(hctx, async, 0);
++>>>>>>> 7587a5ae7eef (blk-mq: Introduce blk_mq_delay_run_hw_queue())
  }
  
  void blk_mq_run_hw_queues(struct request_queue *q, bool async)
@@@ -1678,7 -1988,8 +1709,12 @@@ static int blk_mq_init_hctx(struct requ
  	if (node == NUMA_NO_NODE)
  		node = hctx->numa_node = set->numa_node;
  
++<<<<<<< HEAD
 +	INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
++=======
+ 	INIT_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ 	INIT_DELAYED_WORK(&hctx->delayed_run_work, blk_mq_delayed_run_work_fn);
++>>>>>>> 7587a5ae7eef (blk-mq: Introduce blk_mq_delay_run_hw_queue())
  	INIT_DELAYED_WORK(&hctx->delay_work, blk_mq_delay_work_fn);
  	spin_lock_init(&hctx->lock);
  	INIT_LIST_HEAD(&hctx->dispatch);
diff --cc include/linux/blk-mq.h
index ff3334170322,9382c5da7a2e..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -57,43 -43,28 +57,52 @@@ struct blk_mq_hw_ctx 
  
  	unsigned long		queued;
  	unsigned long		run;
 -#define BLK_MQ_MAX_DISPATCH_ORDER	7
 +#define BLK_MQ_MAX_DISPATCH_ORDER	10
  	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
  
 +	unsigned int		queue_depth;	/* DEPRECATED: RHEL kABI padding, repurpose? */
  	unsigned int		numa_node;
 -	unsigned int		queue_num;
 +	RH_KABI_DEPRECATE(unsigned int, cmd_size)
  
++<<<<<<< HEAD
 +	struct blk_mq_cpu_notifier	cpu_notifier;
++=======
+ 	atomic_t		nr_active;
+ 
+ 	struct delayed_work	delayed_run_work;
+ 	struct delayed_work	delay_work;
+ 
+ 	struct hlist_node	cpuhp_dead;
++>>>>>>> 7587a5ae7eef (blk-mq: Introduce blk_mq_delay_run_hw_queue())
  	struct kobject		kobj;
  
 -	unsigned long		poll_considered;
 -	unsigned long		poll_invoked;
 -	unsigned long		poll_success;
 +	RH_KABI_EXTEND(struct delayed_work	run_work)
 +	RH_KABI_EXTEND(cpumask_var_t		cpumask)
 +	RH_KABI_EXTEND(int			next_cpu)
 +	RH_KABI_EXTEND(int			next_cpu_batch)
 +
 +	RH_KABI_EXTEND(struct blk_mq_ctxmap	ctx_map)
 +
 +	RH_KABI_EXTEND(atomic_t		nr_active)
 +
 +	RH_KABI_EXTEND(struct blk_flush_queue	*fq)
  };
  
 +#ifdef __GENKSYMS__
 +struct blk_mq_reg {
 +	struct blk_mq_ops	*ops;
 +	unsigned int		nr_hw_queues;
 +	unsigned int		queue_depth;	/* max hw supported */
 +	unsigned int		reserved_tags;
 +	unsigned int		cmd_size;	/* per-request extra data */
 +	int			numa_node;
 +	unsigned int		timeout;
 +	unsigned int		flags;		/* BLK_MQ_F_* */
 +};
 +#else
  struct blk_mq_tag_set {
  	unsigned int		*mq_map;
 -	const struct blk_mq_ops	*ops;
 +	struct blk_mq_ops	*ops;
  	unsigned int		nr_hw_queues;
  	unsigned int		queue_depth;	/* max hw supported */
  	unsigned int		reserved_tags;
@@@ -264,7 -237,9 +273,8 @@@ void blk_mq_stop_hw_queue(struct blk_mq
  void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
  void blk_mq_stop_hw_queues(struct request_queue *q);
  void blk_mq_start_hw_queues(struct request_queue *q);
 -void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
  void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
+ void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
  void blk_mq_run_hw_queues(struct request_queue *q, bool async);
  void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
  void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blk-mq.h
