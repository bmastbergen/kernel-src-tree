hugetlbfs: truncate_hugepages() takes a range of pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Mike Kravetz <mike.kravetz@oracle.com>
commit b5cec28d36f5ee6b4e6f68a0a40aa1e4045d6d99
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/b5cec28d.failed

Modify truncate_hugepages() to take a range of pages (start, end)
instead of simply start.  If an end value of LLONG_MAX is passed, the
current "truncate" functionality is maintained.  Existing callers are
modified to pass LLONG_MAX as end of range.  By keying off end ==
LLONG_MAX, the routine behaves differently for truncate and hole punch.
Page removal is now synchronized with page allocation via faults by
using the fault mutex table.  The hole punch case can experience the
rare region_del error and must handle accordingly.

Add the routine hugetlb_fix_reserve_counts to fix up reserve counts in
the case where region_del returns an error.

Since the routine handles more than just the truncate case, it is
renamed to remove_inode_hugepages().  To be consistent, the routine
truncate_huge_page() is renamed remove_huge_page().

Downstream of remove_inode_hugepages(), the routine
hugetlb_unreserve_pages() is also modified to take a range of pages.
hugetlb_unreserve_pages is modified to detect an error from region_del and
pass it back to the caller.

	Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
	Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Christoph Hellwig <hch@infradead.org>
	Cc: Michal Hocko <mhocko@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b5cec28d36f5ee6b4e6f68a0a40aa1e4045d6d99)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/hugetlbfs/inode.c
#	include/linux/hugetlb.h
#	mm/hugetlb.c
diff --cc fs/hugetlbfs/inode.c
index c145954da828,1ef630f81c99..000000000000
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@@ -323,9 -293,9 +323,9 @@@ static int hugetlbfs_write_end(struct f
  	return -EINVAL;
  }
  
- static void truncate_huge_page(struct page *page)
+ static void remove_huge_page(struct page *page)
  {
 -	ClearPageDirty(page);
 +	cancel_dirty_page(page, /* No IO accounting for huge pages? */0);
  	ClearPageUptodate(page);
  	delete_from_page_cache(page);
  }
@@@ -412,11 -471,11 +490,17 @@@ static int hugetlb_vmtruncate(struct in
  	pgoff = offset >> PAGE_SHIFT;
  
  	i_size_write(inode, offset);
 -	i_mmap_lock_write(mapping);
 +	mutex_lock(&mapping->i_mmap_mutex);
  	if (!RB_EMPTY_ROOT(&mapping->i_mmap))
++<<<<<<< HEAD
 +		hugetlb_vmtruncate_list(&mapping->i_mmap, pgoff);
 +	mutex_unlock(&mapping->i_mmap_mutex);
 +	truncate_hugepages(inode, offset);
++=======
+ 		hugetlb_vmdelete_list(&mapping->i_mmap, pgoff, 0);
+ 	i_mmap_unlock_write(mapping);
+ 	remove_inode_hugepages(inode, offset, LLONG_MAX);
++>>>>>>> b5cec28d36f5 (hugetlbfs: truncate_hugepages() takes a range of pages)
  	return 0;
  }
  
diff --cc include/linux/hugetlb.h
index b9a4511c29ea,35afca1692fb..000000000000
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@@ -85,11 -83,18 +85,22 @@@ int hugetlb_fault(struct mm_struct *mm
  int hugetlb_reserve_pages(struct inode *inode, long from, long to,
  						struct vm_area_struct *vma,
  						vm_flags_t vm_flags);
- void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
+ long hugetlb_unreserve_pages(struct inode *inode, long start, long end,
+ 						long freed);
  int dequeue_hwpoisoned_huge_page(struct page *page);
 +void free_huge_page(struct page *page);
  bool isolate_huge_page(struct page *page, struct list_head *list);
  void putback_active_hugepage(struct page *page);
++<<<<<<< HEAD
++=======
+ void free_huge_page(struct page *page);
+ void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve);
+ extern struct mutex *hugetlb_fault_mutex_table;
+ u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
+ 				struct vm_area_struct *vma,
+ 				struct address_space *mapping,
+ 				pgoff_t idx, unsigned long address);
++>>>>>>> b5cec28d36f5 (hugetlbfs: truncate_hugepages() takes a range of pages)
  
  #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
  pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
diff --cc mm/hugetlb.c
index 567cd0ffc031,61c52cd5f77b..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -321,37 -477,97 +321,59 @@@ static long region_truncate(struct resv
  {
  	struct list_head *head = &resv->regions;
  	struct file_region *rg, *trg;
 -	struct file_region *nrg = NULL;
 -	long del = 0;
 +	long chg = 0;
  
 -retry:
  	spin_lock(&resv->lock);
 -	list_for_each_entry_safe(rg, trg, head, link) {
 -		if (rg->to <= f)
 -			continue;
 -		if (rg->from >= t)
 +	/* Locate the region we are either in or before. */
 +	list_for_each_entry(rg, head, link)
 +		if (end <= rg->to)
  			break;
 +	if (&rg->link == head)
 +		goto out;
  
 -		if (f > rg->from && t < rg->to) { /* Must split region */
 -			/*
 -			 * Check for an entry in the cache before dropping
 -			 * lock and attempting allocation.
 -			 */
 -			if (!nrg &&
 -			    resv->region_cache_count > resv->adds_in_progress) {
 -				nrg = list_first_entry(&resv->region_cache,
 -							struct file_region,
 -							link);
 -				list_del(&nrg->link);
 -				resv->region_cache_count--;
 -			}
 -
 -			if (!nrg) {
 -				spin_unlock(&resv->lock);
 -				nrg = kmalloc(sizeof(*nrg), GFP_KERNEL);
 -				if (!nrg)
 -					return -ENOMEM;
 -				goto retry;
 -			}
 -
 -			del += t - f;
 -
 -			/* New entry for end of split region */
 -			nrg->from = t;
 -			nrg->to = rg->to;
 -			INIT_LIST_HEAD(&nrg->link);
 -
 -			/* Original entry is trimmed */
 -			rg->to = f;
 +	/* If we are in the middle of a region then adjust it. */
 +	if (end > rg->from) {
 +		chg = rg->to - end;
 +		rg->to = end;
 +		rg = list_entry(rg->link.next, typeof(*rg), link);
 +	}
  
 -			list_add(&nrg->link, &rg->link);
 -			nrg = NULL;
 +	/* Drop any remaining regions. */
 +	list_for_each_entry_safe(rg, trg, rg->link.prev, link) {
 +		if (&rg->link == head)
  			break;
 -		}
 -
 -		if (f <= rg->from && t >= rg->to) { /* Remove entire region */
 -			del += rg->to - rg->from;
 -			list_del(&rg->link);
 -			kfree(rg);
 -			continue;
 -		}
 -
 -		if (f <= rg->from) {	/* Trim beginning of region */
 -			del += t - rg->from;
 -			rg->from = t;
 -		} else {		/* Trim end of region */
 -			del += rg->to - f;
 -			rg->to = f;
 -		}
 +		chg += rg->to - rg->from;
 +		list_del(&rg->link);
 +		kfree(rg);
  	}
  
 +out:
  	spin_unlock(&resv->lock);
 -	kfree(nrg);
 -	return del;
 +	return chg;
  }
  
+ /*
+  * A rare out of memory error was encountered which prevented removal of
+  * the reserve map region for a page.  The huge page itself was free'ed
+  * and removed from the page cache.  This routine will adjust the subpool
+  * usage count, and the global reserve count if needed.  By incrementing
+  * these counts, the reserve map entry which could not be deleted will
+  * appear as a "reserved" entry instead of simply dangling with incorrect
+  * counts.
+  */
+ void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve)
+ {
+ 	struct hugepage_subpool *spool = subpool_inode(inode);
+ 	long rsv_adjust;
+ 
+ 	rsv_adjust = hugepage_subpool_get_pages(spool, 1);
+ 	if (restore_reserve && rsv_adjust) {
+ 		struct hstate *h = hstate_inode(inode);
+ 
+ 		hugetlb_acct_memory(h, 1);
+ 	}
+ }
+ 
  /*
   * Count and return the number of huge pages in the reserve map
   * that intersect with the range [f, t).
@@@ -3783,21 -3931,38 +3805,47 @@@ out_err
  	return ret;
  }
  
- void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)
+ long hugetlb_unreserve_pages(struct inode *inode, long start, long end,
+ 								long freed)
  {
  	struct hstate *h = hstate_inode(inode);
 -	struct resv_map *resv_map = inode_resv_map(inode);
 +	struct resv_map *resv_map = inode->i_mapping->private_data;
  	long chg = 0;
  	struct hugepage_subpool *spool = subpool_inode(inode);
 -	long gbl_reserve;
  
++<<<<<<< HEAD
 +	if (resv_map)
 +		chg = region_truncate(resv_map, offset);
++=======
+ 	if (resv_map) {
+ 		chg = region_del(resv_map, start, end);
+ 		/*
+ 		 * region_del() can fail in the rare case where a region
+ 		 * must be split and another region descriptor can not be
+ 		 * allocated.  If end == LONG_MAX, it will not fail.
+ 		 */
+ 		if (chg < 0)
+ 			return chg;
+ 	}
+ 
++>>>>>>> b5cec28d36f5 (hugetlbfs: truncate_hugepages() takes a range of pages)
  	spin_lock(&inode->i_lock);
  	inode->i_blocks -= (blocks_per_huge_page(h) * freed);
  	spin_unlock(&inode->i_lock);
  
++<<<<<<< HEAD
 +	hugepage_subpool_put_pages(spool, (chg - freed));
 +	hugetlb_acct_memory(h, -(chg - freed));
++=======
+ 	/*
+ 	 * If the subpool has a minimum size, the number of global
+ 	 * reservations to be released may be adjusted.
+ 	 */
+ 	gbl_reserve = hugepage_subpool_put_pages(spool, (chg - freed));
+ 	hugetlb_acct_memory(h, -gbl_reserve);
+ 
+ 	return 0;
++>>>>>>> b5cec28d36f5 (hugetlbfs: truncate_hugepages() takes a range of pages)
  }
  
  #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
* Unmerged path fs/hugetlbfs/inode.c
* Unmerged path include/linux/hugetlb.h
* Unmerged path mm/hugetlb.c
