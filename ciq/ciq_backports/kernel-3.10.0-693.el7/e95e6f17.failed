locking/pvqspinlock, x86: Enable PV qspinlock for Xen

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author David Vrabel <david.vrabel@citrix.com>
commit e95e6f176c61dd0e7bd9fdfb4956df1f9bfe99d4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e95e6f17.failed

This patch adds the necessary Xen specific code to allow Xen to
support the CPU halting and kicking operations needed by the queue
spinlock PV code.

	Signed-off-by: David Vrabel <david.vrabel@citrix.com>
	Signed-off-by: Waiman Long <Waiman.Long@hp.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Daniel J Blueman <daniel@numascale.com>
	Cc: Douglas Hatch <doug.hatch@hp.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Scott J Norton <scott.norton@hp.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: virtualization@lists.linux-foundation.org
	Cc: xen-devel@lists.xenproject.org
Link: http://lkml.kernel.org/r/1429901803-29771-12-git-send-email-Waiman.Long@hp.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit e95e6f176c61dd0e7bd9fdfb4956df1f9bfe99d4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/xen/spinlock.c
#	kernel/Kconfig.locks
diff --cc arch/x86/xen/spinlock.c
index ebab852bada1,af907a90fb19..000000000000
--- a/arch/x86/xen/spinlock.c
+++ b/arch/x86/xen/spinlock.c
@@@ -266,13 -327,38 +314,25 @@@ void __init xen_init_spinlocks(void
  		printk(KERN_DEBUG "xen: PV spinlocks disabled\n");
  		return;
  	}
++<<<<<<< HEAD
 +
 +	static_key_slow_inc(&paravirt_ticketlocks_enabled);
 +
++=======
+ 	printk(KERN_DEBUG "xen: PV spinlocks enabled\n");
+ #ifdef CONFIG_QUEUED_SPINLOCK
+ 	__pv_init_lock_hash();
+ 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ 	pv_lock_ops.wait = xen_qlock_wait;
+ 	pv_lock_ops.kick = xen_qlock_kick;
+ #else
++>>>>>>> e95e6f176c61 (locking/pvqspinlock, x86: Enable PV qspinlock for Xen)
  	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);
  	pv_lock_ops.unlock_kick = xen_unlock_kick;
+ #endif
  }
  
 -/*
 - * While the jump_label init code needs to happend _after_ the jump labels are
 - * enabled and before SMP is started. Hence we use pre-SMP initcall level
 - * init. We cannot do it in xen_init_spinlocks as that is done before
 - * jump labels are activated.
 - */
 -static __init int xen_init_spinlocks_jump(void)
 -{
 -	if (!xen_pvspin)
 -		return 0;
 -
 -	if (!xen_domain())
 -		return 0;
 -
 -	static_key_slow_inc(&paravirt_ticketlocks_enabled);
 -	return 0;
 -}
 -early_initcall(xen_init_spinlocks_jump);
 -
  static __init int xen_parse_nopvspin(char *arg)
  {
  	xen_pvspin = false;
diff --cc kernel/Kconfig.locks
index 44511d100eaa,95dd7587ec34..000000000000
--- a/kernel/Kconfig.locks
+++ b/kernel/Kconfig.locks
@@@ -220,6 -220,31 +220,32 @@@ config INLINE_WRITE_UNLOCK_IRQRESTOR
  
  endif
  
 -config ARCH_SUPPORTS_ATOMIC_RMW
 -	bool
 -
  config MUTEX_SPIN_ON_OWNER
  	def_bool y
++<<<<<<< HEAD
 +	depends on SMP && !DEBUG_MUTEXES
++=======
+ 	depends on SMP && !DEBUG_MUTEXES && ARCH_SUPPORTS_ATOMIC_RMW
+ 
+ config RWSEM_SPIN_ON_OWNER
+        def_bool y
+        depends on SMP && RWSEM_XCHGADD_ALGORITHM && ARCH_SUPPORTS_ATOMIC_RMW
+ 
+ config LOCK_SPIN_ON_OWNER
+        def_bool y
+        depends on MUTEX_SPIN_ON_OWNER || RWSEM_SPIN_ON_OWNER
+ 
+ config ARCH_USE_QUEUED_SPINLOCK
+ 	bool
+ 
+ config QUEUED_SPINLOCK
+ 	def_bool y if ARCH_USE_QUEUED_SPINLOCK
+ 	depends on SMP
+ 
+ config ARCH_USE_QUEUE_RWLOCK
+ 	bool
+ 
+ config QUEUE_RWLOCK
+ 	def_bool y if ARCH_USE_QUEUE_RWLOCK
+ 	depends on SMP
++>>>>>>> e95e6f176c61 (locking/pvqspinlock, x86: Enable PV qspinlock for Xen)
* Unmerged path arch/x86/xen/spinlock.c
* Unmerged path kernel/Kconfig.locks
