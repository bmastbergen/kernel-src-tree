mm: introduce vma_is_anonymous(vma) helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [mm] introduce vma_is_anonymous(vma) helper (Andrea Arcangeli) [1373606]
Rebuild_FUZZ: 95.00%
commit-author Oleg Nesterov <oleg@redhat.com>
commit b5330628546616af14ff23075fbf8d4ad91f6e25
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/b5330628.failed

special_mapping_fault() is absolutely broken.  It seems it was always
wrong, but this didn't matter until vdso/vvar started to use more than
one page.

And after this change vma_is_anonymous() becomes really trivial, it
simply checks vm_ops == NULL.  However, I do think the helper makes
sense.  There are a lot of ->vm_ops != NULL checks, the helper makes the
caller's code more understandable (self-documented) and this is more
grep-friendly.

This patch (of 3):

Preparation.  Add the new simple helper, vma_is_anonymous(vma), and change
handle_pte_fault() to use it.  It will have more users.

The name is not accurate, say a hpet_mmap()'ed vma is not anonymous.
Perhaps it should be named vma_has_fault() instead.  But it matches the
logic in mmap.c/memory.c (see next changes).  "True" just means that a
page fault will use do_anonymous_page().

	Signed-off-by: Oleg Nesterov <oleg@redhat.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Pavel Emelyanov <xemul@parallels.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b5330628546616af14ff23075fbf8d4ad91f6e25)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index e63691293747,882c9d7ae2f5..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3218,18 -3255,25 +3218,27 @@@ static int handle_pte_fault(struct mm_s
  	pte_t entry;
  	spinlock_t *ptl;
  
 -	/*
 -	 * some architectures can have larger ptes than wordsize,
 -	 * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and CONFIG_32BIT=y,
 -	 * so READ_ONCE or ACCESS_ONCE cannot guarantee atomic accesses.
 -	 * The code below just needs a consistent view for the ifs and
 -	 * we later double check anyway with the ptl lock held. So here
 -	 * a barrier will do.
 -	 */
  	entry = *pte;
 -	barrier();
  	if (!pte_present(entry)) {
  		if (pte_none(entry)) {
++<<<<<<< HEAD
 +			if (vma->vm_ops)
 +				return do_linear_fault(mm, vma, address,
 +						pte, pmd, flags, entry);
 +			return do_anonymous_page(mm, vma, address,
 +						 pte, pmd, flags);
++=======
+ 			if (vma_is_anonymous(vma))
+ 				return do_anonymous_page(mm, vma, address,
+ 							 pte, pmd, flags);
+ 			else
+ 				return do_fault(mm, vma, address, pte, pmd,
+ 						flags, entry);
++>>>>>>> b53306285466 (mm: introduce vma_is_anonymous(vma) helper)
  		}
 +		if (pte_file(entry))
 +			return do_nonlinear_fault(mm, vma, address,
 +					pte, pmd, flags, entry);
  		return do_swap_page(mm, vma, address,
  					pte, pmd, flags, entry);
  	}
diff --git a/include/linux/mm.h b/include/linux/mm.h
index e7edaaec02da..020342eef5e7 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1301,6 +1301,11 @@ static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)
 	return vma && (vma->vm_end == addr) && (vma->vm_flags & VM_GROWSDOWN);
 }
 
+static inline bool vma_is_anonymous(struct vm_area_struct *vma)
+{
+	return !vma->vm_ops;
+}
+
 static inline int stack_guard_page_start(struct vm_area_struct *vma,
 					     unsigned long addr)
 {
* Unmerged path mm/memory.c
