x86/mm: Enable KASLR for physical mapping memory regions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] mm: Enable KASLR for physical mapping memory regions (Baoquan He) [1424943]
Rebuild_FUZZ: 96.30%
commit-author Thomas Garnier <thgarnie@google.com>
commit 021182e52fe01c1f7b126f97fd6ba048dc4234fd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/021182e5.failed

Add the physical mapping in the list of randomized memory regions.

The physical memory mapping holds most allocations from boot and heap
allocators. Knowing the base address and physical memory size, an attacker
can deduce the PDE virtual address for the vDSO memory page. This attack
was demonstrated at CanSecWest 2016, in the following presentation:

  "Getting Physical: Extreme Abuse of Intel Based Paged Systems":
  https://github.com/n3k/CansecWest2016_Getting_Physical_Extreme_Abuse_of_Intel_Based_Paging_Systems/blob/master/Presentation/CanSec2016_Presentation.pdf

(See second part of the presentation).

The exploits used against Linux worked successfully against 4.6+ but
fail with KASLR memory enabled:

  https://github.com/n3k/CansecWest2016_Getting_Physical_Extreme_Abuse_of_Intel_Based_Paging_Systems/tree/master/Demos/Linux/exploits

Similar research was done at Google leading to this patch proposal.

Variants exists to overwrite /proc or /sys objects ACLs leading to
elevation of privileges. These variants were tested against 4.6+.

The page offset used by the compressed kernel retains the static value
since it is not yet randomized during this boot stage.

	Signed-off-by: Thomas Garnier <thgarnie@google.com>
	Signed-off-by: Kees Cook <keescook@chromium.org>
	Cc: Alexander Kuleshov <kuleshovmail@gmail.com>
	Cc: Alexander Popov <alpopov@ptsecurity.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Baoquan He <bhe@redhat.com>
	Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Jan Beulich <JBeulich@suse.com>
	Cc: Joerg Roedel <jroedel@suse.de>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Lv Zheng <lv.zheng@intel.com>
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephen Smalley <sds@tycho.nsa.gov>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Toshi Kani <toshi.kani@hpe.com>
	Cc: Xiao Guangrong <guangrong.xiao@linux.intel.com>
	Cc: Yinghai Lu <yinghai@kernel.org>
	Cc: kernel-hardening@lists.openwall.com
	Cc: linux-doc@vger.kernel.org
Link: http://lkml.kernel.org/r/1466556426-32664-7-git-send-email-keescook@chromium.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 021182e52fe01c1f7b126f97fd6ba048dc4234fd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/pagetable.c
#	arch/x86/include/asm/kaslr.h
#	arch/x86/include/asm/page_64_types.h
#	arch/x86/mm/kaslr.c
diff --cc arch/x86/include/asm/page_64_types.h
index 94f1cf6df981,9215e0527647..000000000000
--- a/arch/x86/include/asm/page_64_types.h
+++ b/arch/x86/include/asm/page_64_types.h
@@@ -1,7 -1,17 +1,21 @@@
  #ifndef _ASM_X86_PAGE_64_DEFS_H
  #define _ASM_X86_PAGE_64_DEFS_H
  
++<<<<<<< HEAD
 +#define THREAD_SIZE_ORDER	2
++=======
+ #ifndef __ASSEMBLY__
+ #include <asm/kaslr.h>
+ #endif
+ 
+ #ifdef CONFIG_KASAN
+ #define KASAN_STACK_ORDER 1
+ #else
+ #define KASAN_STACK_ORDER 0
+ #endif
+ 
+ #define THREAD_SIZE_ORDER	(2 + KASAN_STACK_ORDER)
++>>>>>>> 021182e52fe0 (x86/mm: Enable KASLR for physical mapping memory regions)
  #define THREAD_SIZE  (PAGE_SIZE << THREAD_SIZE_ORDER)
  #define CURRENT_MASK (~(THREAD_SIZE - 1))
  
* Unmerged path arch/x86/boot/compressed/pagetable.c
* Unmerged path arch/x86/include/asm/kaslr.h
* Unmerged path arch/x86/mm/kaslr.c
* Unmerged path arch/x86/boot/compressed/pagetable.c
* Unmerged path arch/x86/include/asm/kaslr.h
* Unmerged path arch/x86/include/asm/page_64_types.h
diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S
index 909c6bc5121a..14236b257681 100644
--- a/arch/x86/kernel/head_64.S
+++ b/arch/x86/kernel/head_64.S
@@ -37,7 +37,7 @@
 
 #define pud_index(x)	(((x) >> PUD_SHIFT) & (PTRS_PER_PUD-1))
 
-L4_PAGE_OFFSET = pgd_index(__PAGE_OFFSET)
+L4_PAGE_OFFSET = pgd_index(__PAGE_OFFSET_BASE)
 L4_START_KERNEL = pgd_index(__START_KERNEL_map)
 L3_START_KERNEL = pud_index(__START_KERNEL_map)
 
* Unmerged path arch/x86/mm/kaslr.c
