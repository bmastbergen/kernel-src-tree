KVM: MTRR: do not map huge page for non-consistent range

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Xiao Guangrong <guangrong.xiao@linux.intel.com>
commit 6a39bbc5da27c3b2520876b71e4f7b50f5313503
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/6a39bbc5.failed

Based on Intel's SDM, mapping huge page which do not have consistent
memory cache for each 4k page will cause undefined behavior

In order to avoiding this kind of undefined behavior, we force to use
4k pages under this case

	Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 6a39bbc5da27c3b2520876b71e4f7b50f5313503)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mtrr.c
#	arch/x86/kvm/x86.h
diff --cc arch/x86/kvm/x86.h
index c4ca2474e8c0,edc8cdcd786b..000000000000
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@@ -171,8 -162,13 +171,15 @@@ int kvm_write_guest_virt_system(struct 
  	gva_t addr, void *val, unsigned int bytes,
  	struct x86_exception *exception);
  
 -void kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu);
 -u8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);
  bool kvm_mtrr_valid(struct kvm_vcpu *vcpu, u32 msr, u64 data);
++<<<<<<< HEAD
 +bool kvm_vector_hashing_enabled(void);
++=======
+ int kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);
+ int kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);
+ bool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 					  int page_num);
++>>>>>>> 6a39bbc5da27 (KVM: MTRR: do not map huge page for non-consistent range)
  
  #define KVM_SUPPORTED_XCR0     (XSTATE_FP | XSTATE_SSE | XSTATE_YMM \
  				| XSTATE_BNDREGS | XSTATE_BNDCSR \
* Unmerged path arch/x86/kvm/mtrr.c
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 70c3ecb68058..463264efcadb 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -3542,6 +3542,16 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 	return false;
 }
 
+static bool
+check_hugepage_cache_consistency(struct kvm_vcpu *vcpu, gfn_t gfn, int level)
+{
+	int page_num = KVM_PAGES_PER_HPAGE(level);
+
+	gfn &= ~(page_num - 1);
+
+	return kvm_mtrr_check_gfn_range_consistency(vcpu, gfn, page_num);
+}
+
 static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 			  bool prefault)
 {
@@ -3567,9 +3577,17 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 	if (r)
 		return r;
 
-	force_pt_level = mapping_level_dirty_bitmap(vcpu, gfn);
+	if (mapping_level_dirty_bitmap(vcpu, gfn) ||
+	    !check_hugepage_cache_consistency(vcpu, gfn, PT_DIRECTORY_LEVEL))
+		force_pt_level = 1;
+	else
+		force_pt_level = 0;
+
 	if (likely(!force_pt_level)) {
 		level = mapping_level(vcpu, gfn);
+		if (level > PT_DIRECTORY_LEVEL &&
+		    !check_hugepage_cache_consistency(vcpu, gfn, level))
+			level = PT_DIRECTORY_LEVEL;
 		gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
 	} else
 		level = PT_PAGE_TABLE_LEVEL;
* Unmerged path arch/x86/kvm/mtrr.c
* Unmerged path arch/x86/kvm/x86.h
