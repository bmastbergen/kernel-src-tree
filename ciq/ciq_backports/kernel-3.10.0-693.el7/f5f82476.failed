net/mlx5: E-Switch, Support VLAN actions in the offloads mode

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5: E-Switch, Support VLAN actions in the offloads mode (Don Dutile) [1385330 1417285]
Rebuild_FUZZ: 96.61%
commit-author Or Gerlitz <ogerlitz@mellanox.com>
commit f5f82476090fd2c6fc4fde03ba61aef984900009
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/f5f82476.failed

Many virtualization systems use a policy under which a vlan tag is
pushed to packets sent by guests, and popped before the packet is
forwarded to the VM.

The current generation of the mlx5 HW doesn't fully support that on
a per flow level. As such, we are addressing the above common use
case with the SRIOV e-Switch abilities to push vlan into packets
sent by VFs and pop vlan from packets forwarded to VFs.

The HW can match on the correct vlan being present in packets
forwarded to VFs (eSwitch steering is done before stripping
the tag), so this part is offloaded as is.

A common practice for vlans is to avoid both push vlan and pop vlan
for inter-host VM/VM (east-west) communication because in this case,
push on egress cancels out with pop on ingress.

For supporting that, we use a global eswitch vlan pop policy, hence
allowing guest A to communicate with both remote VM B and local VM C.
This works since the HW pops the vlan only if it exists (e.g for
C --> A packets but not for B --> A packets).

On the slow path, when a VF vport has an offloaded flow which involves
pushing vlans, wheres another flow is not currently offloaded, the
packets from the 2nd flow seen by the VF representor on the host have
vlan. The VF rep driver removes such vlan before calling into the host
networking stack.

	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit f5f82476090fd2c6fc4fde03ba61aef984900009)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index b01f5bb32ed7,460363b66cb1..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -645,6 -823,70 +645,47 @@@ extern const struct dcbnl_rtnl_ops mlx5
  int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
  #endif
  
 -#ifndef CONFIG_RFS_ACCEL
 -static inline int mlx5e_arfs_create_tables(struct mlx5e_priv *priv)
 -{
 -	return 0;
 -}
 -
 -static inline void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv) {}
 -
 -static inline int mlx5e_arfs_enable(struct mlx5e_priv *priv)
 -{
 -	return -ENOTSUPP;
 -}
 -
 -static inline int mlx5e_arfs_disable(struct mlx5e_priv *priv)
 -{
 -	return -ENOTSUPP;
 -}
 -#else
 -int mlx5e_arfs_create_tables(struct mlx5e_priv *priv);
 -void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv);
 -int mlx5e_arfs_enable(struct mlx5e_priv *priv);
 -int mlx5e_arfs_disable(struct mlx5e_priv *priv);
 -int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
 -			u16 rxq_index, u32 flow_id);
 -#endif
 -
  u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
++<<<<<<< HEAD
++=======
+ int mlx5e_create_tir(struct mlx5_core_dev *mdev,
+ 		     struct mlx5e_tir *tir, u32 *in, int inlen);
+ void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,
+ 		       struct mlx5e_tir *tir);
+ int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
+ void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
+ int mlx5e_refresh_tirs_self_loopback_enable(struct mlx5_core_dev *mdev);
+ 
+ struct mlx5_eswitch_rep;
+ int mlx5e_vport_rep_load(struct mlx5_eswitch *esw,
+ 			 struct mlx5_eswitch_rep *rep);
+ void mlx5e_vport_rep_unload(struct mlx5_eswitch *esw,
+ 			    struct mlx5_eswitch_rep *rep);
+ int mlx5e_nic_rep_load(struct mlx5_eswitch *esw, struct mlx5_eswitch_rep *rep);
+ void mlx5e_nic_rep_unload(struct mlx5_eswitch *esw,
+ 			  struct mlx5_eswitch_rep *rep);
+ int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv);
+ void mlx5e_remove_sqs_fwd_rules(struct mlx5e_priv *priv);
+ int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
+ void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
+ 
+ int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
+ void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
+ int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
+ void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
+ int mlx5e_create_tises(struct mlx5e_priv *priv);
+ void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
+ int mlx5e_close(struct net_device *netdev);
+ int mlx5e_open(struct net_device *netdev);
+ void mlx5e_update_stats_work(struct work_struct *work);
+ struct net_device *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
+ 				       const struct mlx5e_profile *profile,
+ 				       void *ppriv);
+ void mlx5e_destroy_netdev(struct mlx5_core_dev *mdev, struct mlx5e_priv *priv);
+ int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
+ void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
+ struct rtnl_link_stats64 *
+ mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
++>>>>>>> f5f82476090f (net/mlx5: E-Switch, Support VLAN actions in the offloads mode)
  
  #endif /* __MLX5_EN_H__ */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 98d2ea9b4528,c12792314be7..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -280,6 -345,117 +280,120 @@@ static void mlx5e_disable_async_events(
  #define MLX5E_HW2SW_MTU(hwmtu) (hwmtu - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  #define MLX5E_SW2HW_MTU(swmtu) (swmtu + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  
++<<<<<<< HEAD
++=======
+ static inline int mlx5e_get_wqe_mtt_sz(void)
+ {
+ 	/* UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
+ 	 * To avoid copying garbage after the mtt array, we allocate
+ 	 * a little more.
+ 	 */
+ 	return ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(__be64),
+ 		     MLX5_UMR_MTT_ALIGNMENT);
+ }
+ 
+ static inline void mlx5e_build_umr_wqe(struct mlx5e_rq *rq, struct mlx5e_sq *sq,
+ 				       struct mlx5e_umr_wqe *wqe, u16 ix)
+ {
+ 	struct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;
+ 	struct mlx5_wqe_data_seg      *dseg = &wqe->data;
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	u8 ds_cnt = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS);
+ 	u32 umr_wqe_mtt_offset = mlx5e_get_wqe_mtt_offset(rq, ix);
+ 
+ 	cseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |
+ 				      ds_cnt);
+ 	cseg->fm_ce_se  = MLX5_WQE_CTRL_CQ_UPDATE;
+ 	cseg->imm       = rq->mkey_be;
+ 
+ 	ucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN;
+ 	ucseg->klm_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(MLX5_MPWRQ_PAGES_PER_WQE));
+ 	ucseg->bsf_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(umr_wqe_mtt_offset));
+ 	ucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ 
+ 	dseg->lkey = sq->mkey_be;
+ 	dseg->addr = cpu_to_be64(wi->umr.mtt_addr);
+ }
+ 
+ static int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq,
+ 				     struct mlx5e_channel *c)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int mtt_alloc = mtt_sz + MLX5_UMR_ALIGN - 1;
+ 	int i;
+ 
+ 	rq->mpwqe.info = kzalloc_node(wq_sz * sizeof(*rq->mpwqe.info),
+ 				      GFP_KERNEL, cpu_to_node(c->cpu));
+ 	if (!rq->mpwqe.info)
+ 		goto err_out;
+ 
+ 	/* We allocate more than mtt_sz as we will align the pointer */
+ 	rq->mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz, GFP_KERNEL,
+ 					cpu_to_node(c->cpu));
+ 	if (unlikely(!rq->mpwqe.mtt_no_align))
+ 		goto err_free_wqe_info;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		wi->umr.mtt = PTR_ALIGN(rq->mpwqe.mtt_no_align + i * mtt_alloc,
+ 					MLX5_UMR_ALIGN);
+ 		wi->umr.mtt_addr = dma_map_single(c->pdev, wi->umr.mtt, mtt_sz,
+ 						  PCI_DMA_TODEVICE);
+ 		if (unlikely(dma_mapping_error(c->pdev, wi->umr.mtt_addr)))
+ 			goto err_unmap_mtts;
+ 
+ 		mlx5e_build_umr_wqe(rq, &c->icosq, &wi->umr.wqe, i);
+ 	}
+ 
+ 	return 0;
+ 
+ err_unmap_mtts:
+ 	while (--i >= 0) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(c->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ err_free_wqe_info:
+ 	kfree(rq->mpwqe.info);
+ 
+ err_out:
+ 	return -ENOMEM;
+ }
+ 
+ static void mlx5e_rq_free_mpwqe_info(struct mlx5e_rq *rq)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int i;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ 	kfree(rq->mpwqe.info);
+ }
+ 
+ static bool mlx5e_is_vf_vport_rep(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5_eswitch_rep *rep = (struct mlx5_eswitch_rep *)priv->ppriv;
+ 
+ 	if (rep && rep->vport != FDB_UPLINK_VPORT)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
++>>>>>>> f5f82476090f (net/mlx5: E-Switch, Support VLAN actions in the offloads mode)
  static int mlx5e_create_rq(struct mlx5e_channel *c,
  			   struct mlx5e_rq_param *param,
  			   struct mlx5e_rq *rq)
@@@ -353,8 -489,85 +467,90 @@@
  	rq->channel = c;
  	rq->ix      = c->ix;
  	rq->priv    = c->priv;
++<<<<<<< HEAD
 +	rq->mkey_be = c->mkey_be;
 +	rq->umr_mkey_be = cpu_to_be32(c->priv->umr_mkey.key);
++=======
+ 	rq->xdp_prog = priv->xdp_prog;
+ 
+ 	rq->buff.map_dir = DMA_FROM_DEVICE;
+ 	if (rq->xdp_prog)
+ 		rq->buff.map_dir = DMA_BIDIRECTIONAL;
+ 
+ 	switch (priv->params.rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		if (mlx5e_is_vf_vport_rep(priv)) {
+ 			err = -EINVAL;
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		rq->handle_rx_cqe = mlx5e_handle_rx_cqe_mpwrq;
+ 		rq->alloc_wqe = mlx5e_alloc_rx_mpwqe;
+ 		rq->dealloc_wqe = mlx5e_dealloc_rx_mpwqe;
+ 
+ 		rq->mpwqe.mtt_offset = c->ix *
+ 			MLX5E_REQUIRED_MTTS(1, BIT(priv->params.log_rq_size));
+ 
+ 		rq->mpwqe_stride_sz = BIT(priv->params.mpwqe_log_stride_sz);
+ 		rq->mpwqe_num_strides = BIT(priv->params.mpwqe_log_num_strides);
+ 
+ 		rq->buff.wqe_sz = rq->mpwqe_stride_sz * rq->mpwqe_num_strides;
+ 		byte_count = rq->buff.wqe_sz;
+ 		rq->mkey_be = cpu_to_be32(c->priv->umr_mkey.key);
+ 		err = mlx5e_rq_alloc_mpwqe_info(rq, c);
+ 		if (err)
+ 			goto err_rq_wq_destroy;
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_LINKED_LIST */
+ 		rq->dma_info = kzalloc_node(wq_sz * sizeof(*rq->dma_info),
+ 					    GFP_KERNEL, cpu_to_node(c->cpu));
+ 		if (!rq->dma_info) {
+ 			err = -ENOMEM;
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		if (mlx5e_is_vf_vport_rep(priv))
+ 			rq->handle_rx_cqe = mlx5e_handle_rx_cqe_rep;
+ 		else
+ 			rq->handle_rx_cqe = mlx5e_handle_rx_cqe;
+ 
+ 		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
+ 		rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
+ 
+ 		rq->buff.wqe_sz = (priv->params.lro_en) ?
+ 				priv->params.lro_wqe_sz :
+ 				MLX5E_SW2HW_MTU(priv->netdev->mtu);
+ 		byte_count = rq->buff.wqe_sz;
+ 
+ 		/* calc the required page order */
+ 		frag_sz = MLX5_RX_HEADROOM +
+ 			  byte_count /* packet data */ +
+ 			  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+ 		frag_sz = SKB_DATA_ALIGN(frag_sz);
+ 
+ 		npages = DIV_ROUND_UP(frag_sz, PAGE_SIZE);
+ 		rq->buff.page_order = order_base_2(npages);
+ 
+ 		byte_count |= MLX5_HW_START_PADDING;
+ 		rq->mkey_be = c->mkey_be;
+ 	}
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(&rq->wq, i);
+ 
+ 		wqe->data.byte_count = cpu_to_be32(byte_count);
+ 		wqe->data.lkey = rq->mkey_be;
+ 	}
+ 
+ 	INIT_WORK(&rq->am.work, mlx5e_rx_am_work);
+ 	rq->am.mode = priv->params.rx_cq_period_mode;
+ 
+ 	rq->page_cache.head = 0;
+ 	rq->page_cache.tail = 0;
+ 
+ 	if (rq->xdp_prog)
+ 		bpf_prog_add(rq->xdp_prog, 1);
++>>>>>>> f5f82476090f (net/mlx5: E-Switch, Support VLAN actions in the offloads mode)
  
  	return 0;
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index d795e95774bc,c6de6fba5843..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -36,6 -35,8 +36,11 @@@
  #include <linux/tcp.h>
  #include <net/busy_poll.h>
  #include "en.h"
++<<<<<<< HEAD
++=======
+ #include "en_tc.h"
+ #include "eswitch.h"
++>>>>>>> f5f82476090f (net/mlx5: E-Switch, Support VLAN actions in the offloads mode)
  
  static inline bool mlx5e_rx_hw_stamp(struct mlx5e_tstamp *tstamp)
  {
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
index 7b5e70f8cc22,2e2938e08cda..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@@ -130,9 -145,54 +130,60 @@@ struct mlx5_l2_table 
  
  struct mlx5_eswitch_fdb {
  	void *fdb;
++<<<<<<< HEAD
 +	struct mlx5_flow_group *addr_grp;
 +	struct mlx5_flow_group *allmulti_grp;
 +	struct mlx5_flow_group *promisc_grp;
++=======
+ 	union {
+ 		struct legacy_fdb {
+ 			struct mlx5_flow_group *addr_grp;
+ 			struct mlx5_flow_group *allmulti_grp;
+ 			struct mlx5_flow_group *promisc_grp;
+ 		} legacy;
+ 
+ 		struct offloads_fdb {
+ 			struct mlx5_flow_table *fdb;
+ 			struct mlx5_flow_group *send_to_vport_grp;
+ 			struct mlx5_flow_group *miss_grp;
+ 			struct mlx5_flow_rule  *miss_rule;
+ 			int vlan_push_pop_refcount;
+ 		} offloads;
+ 	};
+ };
+ 
+ enum {
+ 	SRIOV_NONE,
+ 	SRIOV_LEGACY,
+ 	SRIOV_OFFLOADS
+ };
+ 
+ struct mlx5_esw_sq {
+ 	struct mlx5_flow_rule	*send_to_vport_rule;
+ 	struct list_head	 list;
+ };
+ 
+ struct mlx5_eswitch_rep {
+ 	int		       (*load)(struct mlx5_eswitch *esw,
+ 				       struct mlx5_eswitch_rep *rep);
+ 	void		       (*unload)(struct mlx5_eswitch *esw,
+ 					 struct mlx5_eswitch_rep *rep);
+ 	u16		       vport;
+ 	u8		       hw_id[ETH_ALEN];
+ 	void		      *priv_data;
+ 
+ 	struct mlx5_flow_rule *vport_rx_rule;
+ 	struct list_head       vport_sqs_list;
+ 	u16		       vlan;
+ 	u32		       vlan_refcount;
+ 	bool		       valid;
+ };
+ 
+ struct mlx5_esw_offload {
+ 	struct mlx5_flow_table *ft_offloads;
+ 	struct mlx5_flow_group *vport_rx_group;
+ 	struct mlx5_eswitch_rep *vport_reps;
++>>>>>>> f5f82476090f (net/mlx5: E-Switch, Support VLAN actions in the offloads mode)
  };
  
  struct mlx5_eswitch {
@@@ -170,4 -240,62 +221,65 @@@ int mlx5_eswitch_get_vport_stats(struc
  				 int vport,
  				 struct ifla_vf_stats *vf_stats);
  
++<<<<<<< HEAD
++=======
+ struct mlx5_flow_spec;
+ struct mlx5_esw_flow_attr;
+ 
+ struct mlx5_flow_rule *
+ mlx5_eswitch_add_offloaded_rule(struct mlx5_eswitch *esw,
+ 				struct mlx5_flow_spec *spec,
+ 				struct mlx5_esw_flow_attr *attr);
+ struct mlx5_flow_rule *
+ mlx5_eswitch_create_vport_rx_rule(struct mlx5_eswitch *esw, int vport, u32 tirn);
+ 
+ enum {
+ 	SET_VLAN_STRIP	= BIT(0),
+ 	SET_VLAN_INSERT	= BIT(1)
+ };
+ 
+ #define MLX5_FLOW_CONTEXT_ACTION_VLAN_POP  0x40
+ #define MLX5_FLOW_CONTEXT_ACTION_VLAN_PUSH 0x80
+ 
+ struct mlx5_esw_flow_attr {
+ 	struct mlx5_eswitch_rep *in_rep;
+ 	struct mlx5_eswitch_rep *out_rep;
+ 
+ 	int	action;
+ 	u16	vlan;
+ 	bool	vlan_handled;
+ };
+ 
+ int mlx5_eswitch_sqs2vport_start(struct mlx5_eswitch *esw,
+ 				 struct mlx5_eswitch_rep *rep,
+ 				 u16 *sqns_array, int sqns_num);
+ void mlx5_eswitch_sqs2vport_stop(struct mlx5_eswitch *esw,
+ 				 struct mlx5_eswitch_rep *rep);
+ 
+ int mlx5_devlink_eswitch_mode_set(struct devlink *devlink, u16 mode);
+ int mlx5_devlink_eswitch_mode_get(struct devlink *devlink, u16 *mode);
+ void mlx5_eswitch_register_vport_rep(struct mlx5_eswitch *esw,
+ 				     int vport_index,
+ 				     struct mlx5_eswitch_rep *rep);
+ void mlx5_eswitch_unregister_vport_rep(struct mlx5_eswitch *esw,
+ 				       int vport_index);
+ 
+ int mlx5_eswitch_add_vlan_action(struct mlx5_eswitch *esw,
+ 				 struct mlx5_esw_flow_attr *attr);
+ int mlx5_eswitch_del_vlan_action(struct mlx5_eswitch *esw,
+ 				 struct mlx5_esw_flow_attr *attr);
+ int __mlx5_eswitch_set_vport_vlan(struct mlx5_eswitch *esw,
+ 				  int vport, u16 vlan, u8 qos, u8 set_flags);
+ 
+ #define MLX5_DEBUG_ESWITCH_MASK BIT(3)
+ 
+ #define esw_info(dev, format, ...)				\
+ 	pr_info("(%s): E-Switch: " format, (dev)->priv.name, ##__VA_ARGS__)
+ 
+ #define esw_warn(dev, format, ...)				\
+ 	pr_warn("(%s): E-Switch: " format, (dev)->priv.name, ##__VA_ARGS__)
+ 
+ #define esw_debug(dev, format, ...)				\
+ 	mlx5_core_dbg_mask(dev, MLX5_DEBUG_ESWITCH_MASK, format, ##__VA_ARGS__)
++>>>>>>> f5f82476090f (net/mlx5: E-Switch, Support VLAN actions in the offloads mode)
  #endif /* __MLX5_ESWITCH_H__ */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
