s390/dasd: enhance CUIR scope detection

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [s390] dasd: enhance CUIR scope detection (Hendrik Brueckner) [1274456]
Rebuild_FUZZ: 93.15%
commit-author Stefan Haberland <stefan.haberland@de.ibm.com>
commit b179b037e568a27ab3e8f5bedea1f4455aba7378
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/b179b037.failed

This patch adds an enhanced detection for control unit initiated
reconfiguration request scope.
The first approach assumed the scope of the reconfiguration request
to be restricted to the path on which the message was received.
The enhanced approach determines the full scope of the reconfiguration
request by evaluating additional path and device selection information
contained in the reconfiguration message.

	Reviewed-by: Peter Oberparleiter <oberpar@linux.vnet.ibm.com>
	Reviewed-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
	Signed-off-by: Stefan Haberland <stefan.haberland@de.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit b179b037e568a27ab3e8f5bedea1f4455aba7378)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/block/dasd_eckd.c
diff --cc drivers/s390/block/dasd_eckd.c
index d5740c400937,72cdaae8ce99..000000000000
--- a/drivers/s390/block/dasd_eckd.c
+++ b/drivers/s390/block/dasd_eckd.c
@@@ -1138,14 -1159,14 +1148,25 @@@ static int dasd_eckd_read_conf(struct d
  			path_data->ppm |= lpm;
  			break;
  		}
++<<<<<<< HEAD
 +		if (!path_data->opm) {
 +			path_data->opm = lpm;
 +			dasd_generic_path_operational(device);
 +		} else {
 +			path_data->opm |= lpm;
 +		}
 +		if (conf_data != private->conf_data)
 +			kfree(conf_data);
++=======
+ 		path_data->opm |= lpm;
+ 		/*
+ 		 * if the path is used
+ 		 * it should not be in one of the negative lists
+ 		 */
+ 		path_data->cablepm &= ~lpm;
+ 		path_data->hpfpm &= ~lpm;
+ 		path_data->cuirpm &= ~lpm;
++>>>>>>> b179b037e568 (s390/dasd: enhance CUIR scope detection)
  	}
  
  	return path_err;
@@@ -4461,6 -4521,493 +4492,496 @@@ out_err
  	return -1;
  }
  
++<<<<<<< HEAD
++=======
+ static int dasd_eckd_read_message_buffer(struct dasd_device *device,
+ 					 struct dasd_rssd_messages *messages,
+ 					 __u8 lpum)
+ {
+ 	struct dasd_rssd_messages *message_buf;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_eckd_private *private;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	private = (struct dasd_eckd_private *) device->private;
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   (sizeof(struct dasd_psf_prssd_data) +
+ 				    sizeof(struct dasd_rssd_messages)),
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->expires = 10 * HZ;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 	/* dasd_sleep_on_immediatly does not do complex error
+ 	 * recovery so clear erp flag and set retry counter to
+ 	 * do basic erp */
+ 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
+ 	cqr->retries = 256;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = 0x03;	/* Message Buffer */
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - message buffer */
+ 	message_buf = (struct dasd_rssd_messages *) (prssdp + 1);
+ 	memset(message_buf, 0, sizeof(struct dasd_rssd_messages));
+ 
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_rssd_messages);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) message_buf;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on_immediatly(cqr);
+ 	if (rc == 0) {
+ 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 		message_buf = (struct dasd_rssd_messages *)
+ 			(prssdp + 1);
+ 		memcpy(messages, message_buf,
+ 		       sizeof(struct dasd_rssd_messages));
+ 	} else
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading messages failed with rc=%d\n"
+ 				, rc);
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ /*
+  * Perform Subsystem Function - CUIR response
+  */
+ static int
+ dasd_eckd_psf_cuir_response(struct dasd_device *device, int response,
+ 			    __u32 message_id,
+ 			    struct channel_path_desc *desc,
+ 			    struct subchannel_id sch_id)
+ {
+ 	struct dasd_psf_cuir_response *psf_cuir;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */ ,
+ 				  sizeof(struct dasd_psf_cuir_response),
+ 				  device);
+ 
+ 	if (IS_ERR(cqr)) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			   "Could not allocate PSF-CUIR request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	psf_cuir = (struct dasd_psf_cuir_response *)cqr->data;
+ 	psf_cuir->order = PSF_ORDER_CUIR_RESPONSE;
+ 	psf_cuir->cc = response;
+ 	if (desc)
+ 		psf_cuir->chpid = desc->chpid;
+ 	psf_cuir->message_id = message_id;
+ 	psf_cuir->cssid = sch_id.cssid;
+ 	psf_cuir->ssid = sch_id.ssid;
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->cda = (__u32)(addr_t)psf_cuir;
+ 	ccw->flags = CCW_FLAG_SLI;
+ 	ccw->count = sizeof(struct dasd_psf_cuir_response);
+ 
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10*HZ;
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 
+ 	rc = dasd_sleep_on(cqr);
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ /*
+  * return configuration data that is referenced by record selector
+  * if a record selector is specified or per default return the
+  * conf_data pointer for the path specified by lpum
+  */
+ static struct dasd_conf_data *dasd_eckd_get_ref_conf(struct dasd_device *device,
+ 						     __u8 lpum,
+ 						     struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private;
+ 	struct dasd_conf_data *conf_data;
+ 	int path, pos;
+ 
+ 	private = (struct dasd_eckd_private *) device->private;
+ 	if (cuir->record_selector == 0)
+ 		goto out;
+ 	for (path = 0x80, pos = 0; path; path >>= 1, pos++) {
+ 		conf_data = private->path_conf_data[pos];
+ 		if (conf_data->gneq.record_selector ==
+ 		    cuir->record_selector)
+ 			return conf_data;
+ 	}
+ out:
+ 	return private->path_conf_data[8 - ffs(lpum)];
+ }
+ 
+ /*
+  * This function determines the scope of a reconfiguration request by
+  * analysing the path and device selection data provided in the CUIR request.
+  * Returns a path mask containing CUIR affected paths for the give device.
+  *
+  * If the CUIR request does not contain the required information return the
+  * path mask of the path the attention message for the CUIR request was reveived
+  * on.
+  */
+ static int dasd_eckd_cuir_scope(struct dasd_device *device, __u8 lpum,
+ 				struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_conf_data *ref_conf_data;
+ 	unsigned long bitmask = 0, mask = 0;
+ 	struct dasd_eckd_private *private;
+ 	struct dasd_conf_data *conf_data;
+ 	unsigned int pos, path;
+ 	char *ref_gneq, *gneq;
+ 	char *ref_ned, *ned;
+ 	int tbcpm = 0;
+ 
+ 	/* if CUIR request does not specify the scope use the path
+ 	   the attention message was presented on */
+ 	if (!cuir->ned_map ||
+ 	    !(cuir->neq_map[0] | cuir->neq_map[1] | cuir->neq_map[2]))
+ 		return lpum;
+ 
+ 	private = (struct dasd_eckd_private *) device->private;
+ 	/* get reference conf data */
+ 	ref_conf_data = dasd_eckd_get_ref_conf(device, lpum, cuir);
+ 	/* reference ned is determined by ned_map field */
+ 	pos = 8 - ffs(cuir->ned_map);
+ 	ref_ned = (char *)&ref_conf_data->neds[pos];
+ 	ref_gneq = (char *)&ref_conf_data->gneq;
+ 	/* transfer 24 bit neq_map to mask */
+ 	mask = cuir->neq_map[2];
+ 	mask |= cuir->neq_map[1] << 8;
+ 	mask |= cuir->neq_map[0] << 16;
+ 
+ 	for (path = 0x80; path; path >>= 1) {
+ 		/* initialise data per path */
+ 		bitmask = mask;
+ 		pos = 8 - ffs(path);
+ 		conf_data = private->path_conf_data[pos];
+ 		pos = 8 - ffs(cuir->ned_map);
+ 		ned = (char *) &conf_data->neds[pos];
+ 		/* compare reference ned and per path ned */
+ 		if (memcmp(ref_ned, ned, sizeof(*ned)) != 0)
+ 			continue;
+ 		gneq = (char *)&conf_data->gneq;
+ 		/* compare reference gneq and per_path gneq under
+ 		   24 bit mask where mask bit 0 equals byte 7 of
+ 		   the gneq and mask bit 24 equals byte 31 */
+ 		while (bitmask) {
+ 			pos = ffs(bitmask) - 1;
+ 			if (memcmp(&ref_gneq[31 - pos], &gneq[31 - pos], 1)
+ 			    != 0)
+ 				break;
+ 			clear_bit(pos, &bitmask);
+ 		}
+ 		if (bitmask)
+ 			continue;
+ 		/* device and path match the reference values
+ 		   add path to CUIR scope */
+ 		tbcpm |= path;
+ 	}
+ 	return tbcpm;
+ }
+ 
+ static void dasd_eckd_cuir_notify_user(struct dasd_device *device,
+ 				       unsigned long paths,
+ 				       struct subchannel_id sch_id, int action)
+ {
+ 	struct channel_path_desc *desc;
+ 	int pos;
+ 
+ 	while (paths) {
+ 		/* get position of bit in mask */
+ 		pos = ffs(paths) - 1;
+ 		/* get channel path descriptor from this position */
+ 		desc = ccw_device_get_chp_desc(device->cdev, 7 - pos);
+ 		if (action == CUIR_QUIESCE)
+ 			pr_warn("Service on the storage server caused path "
+ 				"%x.%02x to go offline", sch_id.cssid,
+ 				desc ? desc->chpid : 0);
+ 		else if (action == CUIR_RESUME)
+ 			pr_info("Path %x.%02x is back online after service "
+ 				"on the storage server", sch_id.cssid,
+ 				desc ? desc->chpid : 0);
+ 		kfree(desc);
+ 		clear_bit(pos, &paths);
+ 	}
+ }
+ 
+ static int dasd_eckd_cuir_remove_path(struct dasd_device *device, __u8 lpum,
+ 				      struct dasd_cuir_message *cuir)
+ {
+ 	unsigned long tbcpm;
+ 
+ 	tbcpm = dasd_eckd_cuir_scope(device, lpum, cuir);
+ 	/* nothing to do if path is not in use */
+ 	if (!(device->path_data.opm & tbcpm))
+ 		return 0;
+ 	if (!(device->path_data.opm & ~tbcpm)) {
+ 		/* no path would be left if the CUIR action is taken
+ 		   return error */
+ 		return -EINVAL;
+ 	}
+ 	/* remove device from operational path mask */
+ 	device->path_data.opm &= ~tbcpm;
+ 	device->path_data.cuirpm |= tbcpm;
+ 	return tbcpm;
+ }
+ 
+ /*
+  * walk through all devices and build a path mask to quiesce them
+  * return an error if the last path to a device would be removed
+  *
+  * if only part of the devices are quiesced and an error
+  * occurs no onlining necessary, the storage server will
+  * notify the already set offline devices again
+  */
+ static int dasd_eckd_cuir_quiesce(struct dasd_device *device, __u8 lpum,
+ 				  struct subchannel_id sch_id,
+ 				  struct dasd_cuir_message *cuir)
+ {
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_eckd_private *private;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	unsigned long flags;
+ 	int tbcpm;
+ 
+ 	private = (struct dasd_eckd_private *) device->private;
+ 	/* active devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->active_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* inactive devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist, group) {
+ 		list_for_each_entry_safe(dev, n, &pavgroup->baselist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 		list_for_each_entry_safe(dev, n, &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, sch_id, CUIR_QUIESCE);
+ 	return 0;
+ out_err:
+ 	return tbcpm;
+ }
+ 
+ static int dasd_eckd_cuir_resume(struct dasd_device *device, __u8 lpum,
+ 				 struct subchannel_id sch_id,
+ 				 struct dasd_cuir_message *cuir)
+ {
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_eckd_private *private;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	int tbcpm;
+ 
+ 	private = (struct dasd_eckd_private *) device->private;
+ 	/*
+ 	 * the path may have been added through a generic path event before
+ 	 * only trigger path verification if the path is not already in use
+ 	 */
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->active_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dev->path_data.opm & tbcpm)) {
+ 			dev->path_data.tbvpm |= tbcpm;
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dev->path_data.opm & tbcpm)) {
+ 			dev->path_data.tbvpm |= tbcpm;
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist,
+ 				 group) {
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->baselist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dev->path_data.opm & tbcpm)) {
+ 				dev->path_data.tbvpm |= tbcpm;
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dev->path_data.opm & tbcpm)) {
+ 				dev->path_data.tbvpm |= tbcpm;
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, sch_id, CUIR_RESUME);
+ 	return 0;
+ }
+ 
+ static void dasd_eckd_handle_cuir(struct dasd_device *device, void *messages,
+ 				 __u8 lpum)
+ {
+ 	struct dasd_cuir_message *cuir = messages;
+ 	struct channel_path_desc *desc;
+ 	struct subchannel_id sch_id;
+ 	int pos, response;
+ 
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR request: %016llx %016llx %016llx %08x",
+ 		      ((u64 *)cuir)[0], ((u64 *)cuir)[1], ((u64 *)cuir)[2],
+ 		      ((u32 *)cuir)[3]);
+ 	ccw_device_get_schid(device->cdev, &sch_id);
+ 	/* get position of path in mask */
+ 	pos = 8 - ffs(lpum);
+ 	/* get channel path descriptor from this position */
+ 	desc = ccw_device_get_chp_desc(device->cdev, pos);
+ 
+ 	if (cuir->code == CUIR_QUIESCE) {
+ 		/* quiesce */
+ 		if (dasd_eckd_cuir_quiesce(device, lpum, sch_id, cuir))
+ 			response = PSF_CUIR_LAST_PATH;
+ 		else
+ 			response = PSF_CUIR_COMPLETED;
+ 	} else if (cuir->code == CUIR_RESUME) {
+ 		/* resume */
+ 		dasd_eckd_cuir_resume(device, lpum, sch_id, cuir);
+ 		response = PSF_CUIR_COMPLETED;
+ 	} else
+ 		response = PSF_CUIR_NOT_SUPPORTED;
+ 
+ 	dasd_eckd_psf_cuir_response(device, response,
+ 				    cuir->message_id, desc, sch_id);
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR response: %d on message ID %08x", response,
+ 		      cuir->message_id);
+ 	/* free descriptor copy */
+ 	kfree(desc);
+ 	/* to make sure there is no attention left schedule work again */
+ 	device->discipline->check_attention(device, lpum);
+ }
+ 
+ static void dasd_eckd_check_attention_work(struct work_struct *work)
+ {
+ 	struct check_attention_work_data *data;
+ 	struct dasd_rssd_messages *messages;
+ 	struct dasd_device *device;
+ 	int rc;
+ 
+ 	data = container_of(work, struct check_attention_work_data, worker);
+ 	device = data->device;
+ 	messages = kzalloc(sizeof(*messages), GFP_KERNEL);
+ 	if (!messages) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			      "Could not allocate attention message buffer");
+ 		goto out;
+ 	}
+ 	rc = dasd_eckd_read_message_buffer(device, messages, data->lpum);
+ 	if (rc)
+ 		goto out;
+ 	if (messages->length == ATTENTION_LENGTH_CUIR &&
+ 	    messages->format == ATTENTION_FORMAT_CUIR)
+ 		dasd_eckd_handle_cuir(device, messages, data->lpum);
+ out:
+ 	dasd_put_device(device);
+ 	kfree(messages);
+ 	kfree(data);
+ }
+ 
+ static int dasd_eckd_check_attention(struct dasd_device *device, __u8 lpum)
+ {
+ 	struct check_attention_work_data *data;
+ 
+ 	data = kzalloc(sizeof(*data), GFP_ATOMIC);
+ 	if (!data)
+ 		return -ENOMEM;
+ 	INIT_WORK(&data->worker, dasd_eckd_check_attention_work);
+ 	dasd_get_device(device);
+ 	data->device = device;
+ 	data->lpum = lpum;
+ 	schedule_work(&data->worker);
+ 	return 0;
+ }
+ 
++>>>>>>> b179b037e568 (s390/dasd: enhance CUIR scope detection)
  static struct ccw_driver dasd_eckd_driver = {
  	.driver = {
  		.name	= "dasd-eckd",
* Unmerged path drivers/s390/block/dasd_eckd.c
diff --git a/drivers/s390/block/dasd_eckd.h b/drivers/s390/block/dasd_eckd.h
index 2555e494591f..294cb677a3a7 100644
--- a/drivers/s390/block/dasd_eckd.h
+++ b/drivers/s390/block/dasd_eckd.h
@@ -328,7 +328,8 @@ struct dasd_gneq {
 		__u8 identifier:2;
 		__u8 reserved:6;
 	} __attribute__ ((packed)) flags;
-	__u8 reserved[5];
+	__u8 record_selector;
+	__u8 reserved[4];
 	struct {
 		__u8 value:2;
 		__u8 number:6;
@@ -433,10 +434,18 @@ struct alias_pav_group {
 	struct dasd_device *next;
 };
 
+struct dasd_conf_data {
+	struct dasd_ned neds[5];
+	u8 reserved[64];
+	struct dasd_gneq gneq;
+} __packed;
+
 struct dasd_eckd_private {
 	struct dasd_eckd_characteristics rdc_data;
 	u8 *conf_data;
 	int conf_len;
+	/* per path configuration data */
+	struct dasd_conf_data *path_conf_data[8];
 	/* pointers to specific parts in the conf_data */
 	struct dasd_ned *ned;
 	struct dasd_sneq *sneq;
