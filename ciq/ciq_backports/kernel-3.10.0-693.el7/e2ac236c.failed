xprtrdma: Allocate MRs on demand

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit e2ac236c0b65129f12fef358390f76cc3cacb865
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e2ac236c.failed

Frequent MR list exhaustion can impact I/O throughput, so enough MRs
are always created during transport set-up to prevent running out.
This means more MRs are created than most workloads need.

Commit 94f58c58c0b4 ("xprtrdma: Allow Read list and Reply chunk
simultaneously") introduced support for sending two chunk lists per
RPC, which consumes more MRs per RPC.

Instead of trying to provision more MRs, introduce a mechanism for
allocating MRs on demand. A few MRs are allocated during transport
set-up to kick things off.

This significantly reduces the average number of MRs per transport
while allowing the MR count to grow for workloads or devices that
need more MRs.

FRWR with mlx4 allocated almost 400 MRs per transport before this
patch. Now it starts with 32.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit e2ac236c0b65129f12fef358390f76cc3cacb865)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/fmr_ops.c
#	net/sunrpc/xprtrdma/frwr_ops.c
#	net/sunrpc/xprtrdma/transport.c
#	net/sunrpc/xprtrdma/verbs.c
#	net/sunrpc/xprtrdma/xprt_rdma.h
diff --cc net/sunrpc/xprtrdma/fmr_ops.c
index ecde1e1b320a,758cd1a02249..000000000000
--- a/net/sunrpc/xprtrdma/fmr_ops.c
+++ b/net/sunrpc/xprtrdma/fmr_ops.c
@@@ -35,28 -28,59 +35,58 @@@
  /* Maximum scatter/gather per FMR */
  #define RPCRDMA_MAX_FMR_SGES	(64)
  
 -/* Access mode of externally registered pages */
 -enum {
 -	RPCRDMA_FMR_ACCESS_FLAGS	= IB_ACCESS_REMOTE_WRITE |
 -					  IB_ACCESS_REMOTE_READ,
 -};
 +static struct workqueue_struct *fmr_recovery_wq;
 +
 +#define FMR_RECOVERY_WQ_FLAGS		(WQ_UNBOUND)
  
 -bool
 -fmr_is_supported(struct rpcrdma_ia *ia)
 +int
 +fmr_alloc_recovery_wq(void)
  {
 -	if (!ia->ri_device->alloc_fmr) {
 -		pr_info("rpcrdma: 'fmr' mode is not supported by device %s\n",
 -			ia->ri_device->name);
 -		return false;
 -	}
 -	return true;
 +	fmr_recovery_wq = alloc_workqueue("fmr_recovery", WQ_UNBOUND, 0);
 +	return !fmr_recovery_wq ? -ENOMEM : 0;
  }
  
++<<<<<<< HEAD
 +void
 +fmr_destroy_recovery_wq(void)
++=======
+ static int
+ fmr_op_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mw *mw)
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  {
 -	static struct ib_fmr_attr fmr_attr = {
 -		.max_pages	= RPCRDMA_MAX_FMR_SGES,
 -		.max_maps	= 1,
 -		.page_shift	= PAGE_SHIFT
 -	};
 +	struct workqueue_struct *wq;
  
 -	mw->fmr.fm_physaddrs = kcalloc(RPCRDMA_MAX_FMR_SGES,
 -				       sizeof(u64), GFP_KERNEL);
 -	if (!mw->fmr.fm_physaddrs)
 -		goto out_free;
 +	if (!fmr_recovery_wq)
 +		return;
  
++<<<<<<< HEAD
 +	wq = fmr_recovery_wq;
 +	fmr_recovery_wq = NULL;
 +	destroy_workqueue(wq);
++=======
+ 	mw->mw_sg = kcalloc(RPCRDMA_MAX_FMR_SGES,
+ 			    sizeof(*mw->mw_sg), GFP_KERNEL);
+ 	if (!mw->mw_sg)
+ 		goto out_free;
+ 
+ 	sg_init_table(mw->mw_sg, RPCRDMA_MAX_FMR_SGES);
+ 
+ 	mw->fmr.fm_mr = ib_alloc_fmr(ia->ri_pd, RPCRDMA_FMR_ACCESS_FLAGS,
+ 				     &fmr_attr);
+ 	if (IS_ERR(mw->fmr.fm_mr))
+ 		goto out_fmr_err;
+ 
+ 	return 0;
+ 
+ out_fmr_err:
+ 	dprintk("RPC:       %s: ib_alloc_fmr returned %ld\n", __func__,
+ 		PTR_ERR(mw->fmr.fm_mr));
+ 
+ out_free:
+ 	kfree(mw->mw_sg);
+ 	kfree(mw->fmr.fm_physaddrs);
+ 	return -ENOMEM;
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  }
  
  static int
@@@ -71,29 -95,54 +101,58 @@@ __fmr_unmap(struct rpcrdma_mw *mw
  	return rc;
  }
  
++<<<<<<< HEAD
 +/* Deferred reset of a single FMR. Generate a fresh rkey by
 + * replacing the MR. There's no recovery if this fails.
++=======
+ static void
+ fmr_op_release_mr(struct rpcrdma_mw *r)
+ {
+ 	LIST_HEAD(unmap_list);
+ 	int rc;
+ 
+ 	kfree(r->fmr.fm_physaddrs);
+ 	kfree(r->mw_sg);
+ 
+ 	/* In case this one was left mapped, try to unmap it
+ 	 * to prevent dealloc_fmr from failing with EBUSY
+ 	 */
+ 	rc = __fmr_unmap(r);
+ 	if (rc)
+ 		pr_err("rpcrdma: final ib_unmap_fmr for %p failed %i\n",
+ 		       r, rc);
+ 
+ 	rc = ib_dealloc_fmr(r->fmr.fm_mr);
+ 	if (rc)
+ 		pr_err("rpcrdma: final ib_dealloc_fmr for %p returned %i\n",
+ 		       r, rc);
+ 
+ 	kfree(r);
+ }
+ 
+ /* Reset of a single FMR.
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
   */
  static void
 -fmr_op_recover_mr(struct rpcrdma_mw *mw)
 +__fmr_recovery_worker(struct work_struct *work)
  {
 +	struct rpcrdma_mw *mw = container_of(work, struct rpcrdma_mw,
 +					    mw_work);
  	struct rpcrdma_xprt *r_xprt = mw->mw_xprt;
 -	int rc;
 -
 -	/* ORDER: invalidate first */
 -	rc = __fmr_unmap(mw);
 -
 -	/* ORDER: then DMA unmap */
 -	ib_dma_unmap_sg(r_xprt->rx_ia.ri_device,
 -			mw->mw_sg, mw->mw_nents, mw->mw_dir);
 -	if (rc) {
 -		pr_err("rpcrdma: FMR reset status %d, %p orphaned\n",
 -		       rc, mw);
 -		r_xprt->rx_stats.mrs_orphaned++;
 -		return;
 -	}
  
 +	__fmr_unmap(mw);
  	rpcrdma_put_mw(r_xprt, mw);
 -	r_xprt->rx_stats.mrs_recovered++;
 +	return;
 +}
 +
 +/* A broken MR was discovered in a context that can't sleep.
 + * Defer recovery to the recovery worker.
 + */
 +static void
 +__fmr_queue_recovery(struct rpcrdma_mw *mw)
 +{
 +	INIT_WORK(&mw->mw_work, __fmr_recovery_worker);
 +	queue_work(fmr_recovery_wq, &mw->mw_work);
  }
  
  static int
@@@ -115,60 -164,6 +174,63 @@@ fmr_op_maxpages(struct rpcrdma_xprt *r_
  		     RPCRDMA_MAX_HDR_SEGS * RPCRDMA_MAX_FMR_SGES);
  }
  
++<<<<<<< HEAD
 +static int
 +fmr_op_init(struct rpcrdma_xprt *r_xprt)
 +{
 +	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
 +	int mr_access_flags = IB_ACCESS_REMOTE_WRITE | IB_ACCESS_REMOTE_READ;
 +	struct ib_fmr_attr fmr_attr = {
 +		.max_pages	= RPCRDMA_MAX_FMR_SGES,
 +		.max_maps	= 1,
 +		.page_shift	= PAGE_SHIFT
 +	};
 +	struct ib_pd *pd = r_xprt->rx_ia.ri_pd;
 +	struct rpcrdma_mw *r;
 +	int i, rc;
 +
 +	spin_lock_init(&buf->rb_mwlock);
 +	INIT_LIST_HEAD(&buf->rb_mws);
 +	INIT_LIST_HEAD(&buf->rb_all);
 +
 +	i = max_t(int, RPCRDMA_MAX_DATA_SEGS / RPCRDMA_MAX_FMR_SGES, 1);
 +	i += 2;				/* head + tail */
 +	i *= buf->rb_max_requests;	/* one set for each RPC slot */
 +	dprintk("RPC:       %s: initalizing %d FMRs\n", __func__, i);
 +
 +	rc = -ENOMEM;
 +	while (i--) {
 +		r = kzalloc(sizeof(*r), GFP_KERNEL);
 +		if (!r)
 +			goto out;
 +
 +		r->fmr.physaddrs = kmalloc(RPCRDMA_MAX_FMR_SGES *
 +					   sizeof(u64), GFP_KERNEL);
 +		if (!r->fmr.physaddrs)
 +			goto out_free;
 +
 +		r->fmr.fmr = ib_alloc_fmr(pd, mr_access_flags, &fmr_attr);
 +		if (IS_ERR(r->fmr.fmr))
 +			goto out_fmr_err;
 +
 +		r->mw_xprt = r_xprt;
 +		list_add(&r->mw_list, &buf->rb_mws);
 +		list_add(&r->mw_all, &buf->rb_all);
 +	}
 +	return 0;
 +
 +out_fmr_err:
 +	rc = PTR_ERR(r->fmr.fmr);
 +	dprintk("RPC:       %s: ib_alloc_fmr status %i\n", __func__, rc);
 +	kfree(r->fmr.physaddrs);
 +out_free:
 +	kfree(r);
 +out:
 +	return rc;
 +}
 +
++=======
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  /* Use the ib_map_phys_fmr() verb to register a memory region
   * for remote access via RDMA READ or RDMA WRITE.
   */
@@@ -333,64 -337,14 +395,67 @@@ fmr_op_unmap_safe(struct rpcrdma_xprt *
  	}
  }
  
++<<<<<<< HEAD
 +/* Use the ib_unmap_fmr() verb to prevent further remote
 + * access via RDMA READ or RDMA WRITE.
 + */
 +static int
 +fmr_op_unmap(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr_seg *seg)
 +{
 +	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 +	struct rpcrdma_mr_seg *seg1 = seg;
 +	struct rpcrdma_mw *mw = seg1->rl_mw;
 +	int rc, nsegs = seg->mr_nsegs;
 +
 +	dprintk("RPC:       %s: FMR %p\n", __func__, mw);
 +
 +	seg1->rl_mw = NULL;
 +	while (seg1->mr_nsegs--)
 +		rpcrdma_unmap_one(ia->ri_device, seg++);
 +	rc = __fmr_unmap(mw);
 +	if (rc)
 +		goto out_err;
 +	rpcrdma_put_mw(r_xprt, mw);
 +	return nsegs;
 +
 +out_err:
 +	/* The FMR is abandoned, but remains in rb_all. fmr_op_destroy
 +	 * will attempt to release it when the transport is destroyed.
 +	 */
 +	dprintk("RPC:       %s: ib_unmap_fmr status %i\n", __func__, rc);
 +	return nsegs;
 +}
 +
 +static void
 +fmr_op_destroy(struct rpcrdma_buffer *buf)
 +{
 +	struct rpcrdma_mw *r;
 +	int rc;
 +
 +	while (!list_empty(&buf->rb_all)) {
 +		r = list_entry(buf->rb_all.next, struct rpcrdma_mw, mw_all);
 +		list_del(&r->mw_all);
 +		kfree(r->fmr.physaddrs);
 +
 +		rc = ib_dealloc_fmr(r->fmr.fmr);
 +		if (rc)
 +			dprintk("RPC:       %s: ib_dealloc_fmr failed %i\n",
 +				__func__, rc);
 +
 +		kfree(r);
 +	}
 +}
 +
++=======
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  const struct rpcrdma_memreg_ops rpcrdma_fmr_memreg_ops = {
  	.ro_map				= fmr_op_map,
  	.ro_unmap_sync			= fmr_op_unmap_sync,
  	.ro_unmap_safe			= fmr_op_unmap_safe,
 -	.ro_recover_mr			= fmr_op_recover_mr,
 +	.ro_unmap			= fmr_op_unmap,
  	.ro_open			= fmr_op_open,
  	.ro_maxpages			= fmr_op_maxpages,
- 	.ro_init			= fmr_op_init,
- 	.ro_destroy			= fmr_op_destroy,
+ 	.ro_init_mr			= fmr_op_init_mr,
+ 	.ro_release_mr			= fmr_op_release_mr,
  	.ro_displayname			= "fmr",
  };
diff --cc net/sunrpc/xprtrdma/frwr_ops.c
index 144dce124c80,e77776bc5d59..000000000000
--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@@ -142,9 -91,9 +142,14 @@@ __frwr_queue_recovery(struct rpcrdma_m
  }
  
  static int
++<<<<<<< HEAD
 +__frwr_init(struct rpcrdma_mw *r, struct ib_pd *pd, struct ib_device *device,
 +	    unsigned int depth)
++=======
+ frwr_op_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mw *r)
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  {
+ 	unsigned int depth = ia->ri_max_frmr_depth;
  	struct rpcrdma_frmr *f = &r->frmr;
  	int rc;
  
@@@ -183,9 -130,62 +188,68 @@@ frwr_op_release_mr(struct rpcrdma_mw *r
  
  	rc = ib_dereg_mr(r->frmr.fr_mr);
  	if (rc)
++<<<<<<< HEAD
 +		dprintk("RPC:       %s: ib_dereg_mr status %i\n",
 +			__func__, rc);
 +	kfree(r->frmr.sg);
++=======
+ 		pr_err("rpcrdma: final ib_dereg_mr for %p returned %i\n",
+ 		       r, rc);
+ 	kfree(r->mw_sg);
+ 	kfree(r);
+ }
+ 
+ static int
+ __frwr_reset_mr(struct rpcrdma_ia *ia, struct rpcrdma_mw *r)
+ {
+ 	struct rpcrdma_frmr *f = &r->frmr;
+ 	int rc;
+ 
+ 	rc = ib_dereg_mr(f->fr_mr);
+ 	if (rc) {
+ 		pr_warn("rpcrdma: ib_dereg_mr status %d, frwr %p orphaned\n",
+ 			rc, r);
+ 		return rc;
+ 	}
+ 
+ 	f->fr_mr = ib_alloc_mr(ia->ri_pd, IB_MR_TYPE_MEM_REG,
+ 			       ia->ri_max_frmr_depth);
+ 	if (IS_ERR(f->fr_mr)) {
+ 		pr_warn("rpcrdma: ib_alloc_mr status %ld, frwr %p orphaned\n",
+ 			PTR_ERR(f->fr_mr), r);
+ 		return PTR_ERR(f->fr_mr);
+ 	}
+ 
+ 	dprintk("RPC:       %s: recovered FRMR %p\n", __func__, r);
+ 	f->fr_state = FRMR_IS_INVALID;
+ 	return 0;
+ }
+ 
+ /* Reset of a single FRMR. Generate a fresh rkey by replacing the MR.
+  *
+  * There's no recovery if this fails. The FRMR is abandoned, but
+  * remains in rb_all. It will be cleaned up when the transport is
+  * destroyed.
+  */
+ static void
+ frwr_op_recover_mr(struct rpcrdma_mw *mw)
+ {
+ 	struct rpcrdma_xprt *r_xprt = mw->mw_xprt;
+ 	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
+ 	int rc;
+ 
+ 	rc = __frwr_reset_mr(ia, mw);
+ 	ib_dma_unmap_sg(ia->ri_device, mw->mw_sg, mw->mw_nents, mw->mw_dir);
+ 	if (rc) {
+ 		pr_err("rpcrdma: FRMR reset status %d, %p orphaned\n",
+ 		       rc, mw);
+ 		r_xprt->rx_stats.mrs_orphaned++;
+ 		return;
+ 	}
+ 
+ 	rpcrdma_put_mw(r_xprt, mw);
+ 	r_xprt->rx_stats.mrs_recovered++;
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  }
  
  static int
@@@ -321,47 -321,7 +385,51 @@@ frwr_wc_localinv_wake(struct ib_cq *cq
  	complete_all(&frmr->fr_linv_done);
  }
  
++<<<<<<< HEAD
 +static int
 +frwr_op_init(struct rpcrdma_xprt *r_xprt)
 +{
 +	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
 +	struct ib_device *device = r_xprt->rx_ia.ri_device;
 +	unsigned int depth = r_xprt->rx_ia.ri_max_frmr_depth;
 +	struct ib_pd *pd = r_xprt->rx_ia.ri_pd;
 +	int i;
 +
 +	spin_lock_init(&buf->rb_mwlock);
 +	INIT_LIST_HEAD(&buf->rb_mws);
 +	INIT_LIST_HEAD(&buf->rb_all);
 +
 +	i = max_t(int, RPCRDMA_MAX_DATA_SEGS / depth, 1);
 +	i += 2;				/* head + tail */
 +	i *= buf->rb_max_requests;	/* one set for each RPC slot */
 +	dprintk("RPC:       %s: initalizing %d FRMRs\n", __func__, i);
 +
 +	while (i--) {
 +		struct rpcrdma_mw *r;
 +		int rc;
 +
 +		r = kzalloc(sizeof(*r), GFP_KERNEL);
 +		if (!r)
 +			return -ENOMEM;
 +
 +		rc = __frwr_init(r, pd, device, depth);
 +		if (rc) {
 +			kfree(r);
 +			return rc;
 +		}
 +
 +		list_add(&r->mw_list, &buf->rb_mws);
 +		list_add(&r->mw_all, &buf->rb_all);
 +		r->frmr.fr_xprt = r_xprt;
 +	}
 +
 +	return 0;
 +}
 +
 +/* Post a FAST_REG Work Request to register a memory region
++=======
+ /* Post a REG_MR Work Request to register a memory region
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
   * for remote access via RDMA READ or RDMA WRITE.
   */
  static int
@@@ -611,72 -581,14 +679,75 @@@ frwr_op_unmap_safe(struct rpcrdma_xprt 
  	}
  }
  
++<<<<<<< HEAD
 +/* Post a LOCAL_INV Work Request to prevent further remote access
 + * via RDMA READ or RDMA WRITE.
 + */
 +static int
 +frwr_op_unmap(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr_seg *seg)
 +{
 +	struct rpcrdma_mr_seg *seg1 = seg;
 +	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 +	struct rpcrdma_mw *mw = seg1->rl_mw;
 +	struct rpcrdma_frmr *frmr = &mw->frmr;
 +	struct ib_send_wr *invalidate_wr, *bad_wr;
 +	int rc, nsegs = seg->mr_nsegs;
 +
 +	dprintk("RPC:       %s: FRMR %p\n", __func__, mw);
 +
 +	seg1->rl_mw = NULL;
 +	frmr->fr_state = FRMR_IS_INVALID;
 +	invalidate_wr = &mw->frmr.fr_invwr;
 +
 +	memset(invalidate_wr, 0, sizeof(*invalidate_wr));
 +	frmr->fr_cqe.done = frwr_wc_localinv;
 +	invalidate_wr->wr_cqe = &frmr->fr_cqe;
 +	invalidate_wr->opcode = IB_WR_LOCAL_INV;
 +	invalidate_wr->ex.invalidate_rkey = frmr->fr_mr->rkey;
 +	DECR_CQCOUNT(&r_xprt->rx_ep);
 +
 +	ib_dma_unmap_sg(ia->ri_device, frmr->sg, frmr->sg_nents, seg1->mr_dir);
 +	read_lock(&ia->ri_qplock);
 +	rc = ib_post_send(ia->ri_id->qp, invalidate_wr, &bad_wr);
 +	read_unlock(&ia->ri_qplock);
 +	if (rc)
 +		goto out_err;
 +
 +	rpcrdma_put_mw(r_xprt, mw);
 +	return nsegs;
 +
 +out_err:
 +	dprintk("RPC:       %s: ib_post_send status %i\n", __func__, rc);
 +	__frwr_queue_recovery(mw);
 +	return nsegs;
 +}
 +
 +static void
 +frwr_op_destroy(struct rpcrdma_buffer *buf)
 +{
 +	struct rpcrdma_mw *r;
 +
 +	/* Ensure stale MWs for "buf" are no longer in flight */
 +	flush_workqueue(frwr_recovery_wq);
 +
 +	while (!list_empty(&buf->rb_all)) {
 +		r = list_entry(buf->rb_all.next, struct rpcrdma_mw, mw_all);
 +		list_del(&r->mw_all);
 +		__frwr_release(r);
 +		kfree(r);
 +	}
 +}
 +
++=======
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  const struct rpcrdma_memreg_ops rpcrdma_frwr_memreg_ops = {
  	.ro_map				= frwr_op_map,
  	.ro_unmap_sync			= frwr_op_unmap_sync,
  	.ro_unmap_safe			= frwr_op_unmap_safe,
 -	.ro_recover_mr			= frwr_op_recover_mr,
 +	.ro_unmap			= frwr_op_unmap,
  	.ro_open			= frwr_op_open,
  	.ro_maxpages			= frwr_op_maxpages,
- 	.ro_init			= frwr_op_init,
- 	.ro_destroy			= frwr_op_destroy,
+ 	.ro_init_mr			= frwr_op_init_mr,
+ 	.ro_release_mr			= frwr_op_release_mr,
  	.ro_displayname			= "frwr",
  };
diff --cc net/sunrpc/xprtrdma/transport.c
index 99d2e5b72726,b1dd42a93484..000000000000
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@@ -672,6 -682,10 +672,13 @@@ void xprt_rdma_print_stats(struct rpc_x
  		   r_xprt->rx_stats.failed_marshal_count,
  		   r_xprt->rx_stats.bad_reply_count,
  		   r_xprt->rx_stats.nomsg_call_count);
++<<<<<<< HEAD
++=======
+ 	seq_printf(seq, "%lu %lu %lu\n",
+ 		   r_xprt->rx_stats.mrs_recovered,
+ 		   r_xprt->rx_stats.mrs_orphaned,
+ 		   r_xprt->rx_stats.mrs_allocated);
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  }
  
  static int
diff --cc net/sunrpc/xprtrdma/verbs.c
index 852c524a0dd0,e8677eafb329..000000000000
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@@ -777,6 -747,90 +777,93 @@@ rpcrdma_ep_disconnect(struct rpcrdma_e
  	ib_drain_qp(ia->ri_id->qp);
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ rpcrdma_mr_recovery_worker(struct work_struct *work)
+ {
+ 	struct rpcrdma_buffer *buf = container_of(work, struct rpcrdma_buffer,
+ 						  rb_recovery_worker.work);
+ 	struct rpcrdma_mw *mw;
+ 
+ 	spin_lock(&buf->rb_recovery_lock);
+ 	while (!list_empty(&buf->rb_stale_mrs)) {
+ 		mw = list_first_entry(&buf->rb_stale_mrs,
+ 				      struct rpcrdma_mw, mw_list);
+ 		list_del_init(&mw->mw_list);
+ 		spin_unlock(&buf->rb_recovery_lock);
+ 
+ 		dprintk("RPC:       %s: recovering MR %p\n", __func__, mw);
+ 		mw->mw_xprt->rx_ia.ri_ops->ro_recover_mr(mw);
+ 
+ 		spin_lock(&buf->rb_recovery_lock);
+ 	};
+ 	spin_unlock(&buf->rb_recovery_lock);
+ }
+ 
+ void
+ rpcrdma_defer_mr_recovery(struct rpcrdma_mw *mw)
+ {
+ 	struct rpcrdma_xprt *r_xprt = mw->mw_xprt;
+ 	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
+ 
+ 	spin_lock(&buf->rb_recovery_lock);
+ 	list_add(&mw->mw_list, &buf->rb_stale_mrs);
+ 	spin_unlock(&buf->rb_recovery_lock);
+ 
+ 	schedule_delayed_work(&buf->rb_recovery_worker, 0);
+ }
+ 
+ static void
+ rpcrdma_create_mrs(struct rpcrdma_xprt *r_xprt)
+ {
+ 	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
+ 	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
+ 	unsigned int count;
+ 	LIST_HEAD(free);
+ 	LIST_HEAD(all);
+ 
+ 	for (count = 0; count < 32; count++) {
+ 		struct rpcrdma_mw *mw;
+ 		int rc;
+ 
+ 		mw = kzalloc(sizeof(*mw), GFP_KERNEL);
+ 		if (!mw)
+ 			break;
+ 
+ 		rc = ia->ri_ops->ro_init_mr(ia, mw);
+ 		if (rc) {
+ 			kfree(mw);
+ 			break;
+ 		}
+ 
+ 		mw->mw_xprt = r_xprt;
+ 
+ 		list_add(&mw->mw_list, &free);
+ 		list_add(&mw->mw_all, &all);
+ 	}
+ 
+ 	spin_lock(&buf->rb_mwlock);
+ 	list_splice(&free, &buf->rb_mws);
+ 	list_splice(&all, &buf->rb_all);
+ 	r_xprt->rx_stats.mrs_allocated += count;
+ 	spin_unlock(&buf->rb_mwlock);
+ 
+ 	dprintk("RPC:       %s: created %u MRs\n", __func__, count);
+ }
+ 
+ static void
+ rpcrdma_mr_refresh_worker(struct work_struct *work)
+ {
+ 	struct rpcrdma_buffer *buf = container_of(work, struct rpcrdma_buffer,
+ 						  rb_refresh_worker.work);
+ 	struct rpcrdma_xprt *r_xprt = container_of(buf, struct rpcrdma_xprt,
+ 						   rx_buf);
+ 
+ 	rpcrdma_create_mrs(r_xprt);
+ }
+ 
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  struct rpcrdma_req *
  rpcrdma_create_req(struct rpcrdma_xprt *r_xprt)
  {
@@@ -837,12 -890,19 +923,24 @@@ rpcrdma_buffer_create(struct rpcrdma_xp
  
  	buf->rb_max_requests = r_xprt->rx_data.max_requests;
  	buf->rb_bc_srv_max_requests = 0;
++<<<<<<< HEAD
 +	spin_lock_init(&buf->rb_lock);
 +	atomic_set(&buf->rb_credits, 1);
- 
- 	rc = ia->ri_ops->ro_init(r_xprt);
- 	if (rc)
- 		goto out;
++=======
+ 	atomic_set(&buf->rb_credits, 1);
+ 	spin_lock_init(&buf->rb_mwlock);
+ 	spin_lock_init(&buf->rb_lock);
+ 	spin_lock_init(&buf->rb_recovery_lock);
+ 	INIT_LIST_HEAD(&buf->rb_mws);
+ 	INIT_LIST_HEAD(&buf->rb_all);
+ 	INIT_LIST_HEAD(&buf->rb_stale_mrs);
+ 	INIT_DELAYED_WORK(&buf->rb_refresh_worker,
+ 			  rpcrdma_mr_refresh_worker);
+ 	INIT_DELAYED_WORK(&buf->rb_recovery_worker,
+ 			  rpcrdma_mr_recovery_worker);
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
+ 
+ 	rpcrdma_create_mrs(r_xprt);
  
  	INIT_LIST_HEAD(&buf->rb_send_bufs);
  	INIT_LIST_HEAD(&buf->rb_allreqs);
diff --cc net/sunrpc/xprtrdma/xprt_rdma.h
index b1a53c89aa34,649d01dda327..000000000000
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@@ -337,6 -335,11 +337,14 @@@ struct rpcrdma_buffer 
  	struct list_head	rb_allreqs;
  
  	u32			rb_bc_max_requests;
++<<<<<<< HEAD
++=======
+ 
+ 	spinlock_t		rb_recovery_lock; /* protect rb_stale_mrs */
+ 	struct list_head	rb_stale_mrs;
+ 	struct delayed_work	rb_recovery_worker;
+ 	struct delayed_work	rb_refresh_worker;
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  };
  #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
  
@@@ -383,6 -386,9 +391,12 @@@ struct rpcrdma_stats 
  	unsigned long		bad_reply_count;
  	unsigned long		nomsg_call_count;
  	unsigned long		bcall_count;
++<<<<<<< HEAD
++=======
+ 	unsigned long		mrs_recovered;
+ 	unsigned long		mrs_orphaned;
+ 	unsigned long		mrs_allocated;
++>>>>>>> e2ac236c0b65 (xprtrdma: Allocate MRs on demand)
  };
  
  /*
* Unmerged path net/sunrpc/xprtrdma/fmr_ops.c
* Unmerged path net/sunrpc/xprtrdma/frwr_ops.c
* Unmerged path net/sunrpc/xprtrdma/transport.c
* Unmerged path net/sunrpc/xprtrdma/verbs.c
* Unmerged path net/sunrpc/xprtrdma/xprt_rdma.h
