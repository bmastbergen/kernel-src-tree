net: reduce cycles spend on ICMP replies that gets rate limited

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [net] reduce cycles spend on ICMP replies that gets rate limited (Sabrina Dubroca) [1428684]
Rebuild_FUZZ: 95.87%
commit-author Jesper Dangaard Brouer <brouer@redhat.com>
commit c0303efeab7391ec51c337e0ac5740860ad01fe7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/c0303efe.failed

This patch split the global and per (inet)peer ICMP-reply limiter
code, and moves the global limit check to earlier in the packet
processing path.  Thus, avoid spending cycles on ICMP replies that
gets limited/suppressed anyhow.

The global ICMP rate limiter icmp_global_allow() is a good solution,
it just happens too late in the process.  The kernel goes through the
full route lookup (return path) for the ICMP message, before taking
the rate limit decision of not sending the ICMP reply.

Details: The kernels global rate limiter for ICMP messages got added
in commit 4cdf507d5452 ("icmp: add a global rate limitation").  It is
a token bucket limiter with a global lock.  It brilliantly avoids
locking congestion by only updating when 20ms (HZ/50) were elapsed. It
can then avoids taking lock when credit is exhausted (when under
pressure) and time constraint for refill is not yet meet.

	Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Acked-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c0303efeab7391ec51c337e0ac5740860ad01fe7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/icmp.c
#	net/ipv6/icmp.c
diff --cc net/ipv4/icmp.c
index 6988f500f6f0,58d75ca58b83..000000000000
--- a/net/ipv4/icmp.c
+++ b/net/ipv4/icmp.c
@@@ -231,35 -232,107 +231,121 @@@ static inline void icmp_xmit_unlock(str
  	spin_unlock_bh(&sk->sk_lock.slock);
  }
  
++<<<<<<< HEAD
++=======
+ int sysctl_icmp_msgs_per_sec __read_mostly = 1000;
+ int sysctl_icmp_msgs_burst __read_mostly = 50;
+ 
+ static struct {
+ 	spinlock_t	lock;
+ 	u32		credit;
+ 	u32		stamp;
+ } icmp_global = {
+ 	.lock		= __SPIN_LOCK_UNLOCKED(icmp_global.lock),
+ };
+ 
+ /**
+  * icmp_global_allow - Are we allowed to send one more ICMP message ?
+  *
+  * Uses a token bucket to limit our ICMP messages to sysctl_icmp_msgs_per_sec.
+  * Returns false if we reached the limit and can not send another packet.
+  * Note: called with BH disabled
+  */
+ bool icmp_global_allow(void)
+ {
+ 	u32 credit, delta, incr = 0, now = (u32)jiffies;
+ 	bool rc = false;
+ 
+ 	/* Check if token bucket is empty and cannot be refilled
+ 	 * without taking the spinlock.
+ 	 */
+ 	if (!icmp_global.credit) {
+ 		delta = min_t(u32, now - icmp_global.stamp, HZ);
+ 		if (delta < HZ / 50)
+ 			return false;
+ 	}
+ 
+ 	spin_lock(&icmp_global.lock);
+ 	delta = min_t(u32, now - icmp_global.stamp, HZ);
+ 	if (delta >= HZ / 50) {
+ 		incr = sysctl_icmp_msgs_per_sec * delta / HZ ;
+ 		if (incr)
+ 			icmp_global.stamp = now;
+ 	}
+ 	credit = min_t(u32, icmp_global.credit + incr, sysctl_icmp_msgs_burst);
+ 	if (credit) {
+ 		credit--;
+ 		rc = true;
+ 	}
+ 	icmp_global.credit = credit;
+ 	spin_unlock(&icmp_global.lock);
+ 	return rc;
+ }
+ EXPORT_SYMBOL(icmp_global_allow);
+ 
+ static bool icmpv4_mask_allow(struct net *net, int type, int code)
+ {
+ 	if (type > NR_ICMP_TYPES)
+ 		return true;
+ 
+ 	/* Don't limit PMTU discovery. */
+ 	if (type == ICMP_DEST_UNREACH && code == ICMP_FRAG_NEEDED)
+ 		return true;
+ 
+ 	/* Limit if icmp type is enabled in ratemask. */
+ 	if (!((1 << type) & net->ipv4.sysctl_icmp_ratemask))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static bool icmpv4_global_allow(struct net *net, int type, int code)
+ {
+ 	if (icmpv4_mask_allow(net, type, code))
+ 		return true;
+ 
+ 	if (icmp_global_allow())
+ 		return true;
+ 
+ 	return false;
+ }
+ 
++>>>>>>> c0303efeab73 (net: reduce cycles spend on ICMP replies that gets rate limited)
  /*
   *	Send an ICMP frame.
   */
  
 -static bool icmpv4_xrlim_allow(struct net *net, struct rtable *rt,
 -			       struct flowi4 *fl4, int type, int code)
 +static inline bool icmpv4_xrlim_allow(struct net *net, struct rtable *rt,
 +				      struct flowi4 *fl4, int type, int code)
  {
  	struct dst_entry *dst = &rt->dst;
+ 	struct inet_peer *peer;
  	bool rc = true;
+ 	int vif;
  
- 	if (type > NR_ICMP_TYPES)
- 		goto out;
- 
- 	/* Don't limit PMTU discovery. */
- 	if (type == ICMP_DEST_UNREACH && code == ICMP_FRAG_NEEDED)
+ 	if (icmpv4_mask_allow(net, type, code))
  		goto out;
  
  	/* No rate limit on loopback */
  	if (dst->dev && (dst->dev->flags&IFF_LOOPBACK))
  		goto out;
  
++<<<<<<< HEAD
 +	/* Limit if icmp type is enabled in ratemask. */
 +	if ((1 << type) & net->ipv4.sysctl_icmp_ratemask) {
 +		struct inet_peer *peer = inet_getpeer_v4(net->ipv4.peers, fl4->daddr, 1);
 +		rc = inet_peer_xrlim_allow(peer,
 +					   net->ipv4.sysctl_icmp_ratelimit);
 +		if (peer)
 +			inet_putpeer(peer);
 +	}
++=======
+ 	vif = l3mdev_master_ifindex(dst->dev);
+ 	peer = inet_getpeer_v4(net->ipv4.peers, fl4->daddr, vif, 1);
+ 	rc = inet_peer_xrlim_allow(peer, net->ipv4.sysctl_icmp_ratelimit);
+ 	if (peer)
+ 		inet_putpeer(peer);
++>>>>>>> c0303efeab73 (net: reduce cycles spend on ICMP replies that gets rate limited)
  out:
  	return rc;
  }
@@@ -337,6 -410,9 +423,12 @@@ static void icmp_reply(struct icmp_bxm 
  	struct sock *sk;
  	struct inet_sock *inet;
  	__be32 daddr, saddr;
++<<<<<<< HEAD
++=======
+ 	u32 mark = IP4_REPLY_MARK(net, skb->mark);
+ 	int type = icmp_param->data.icmph.type;
+ 	int code = icmp_param->data.icmph.code;
++>>>>>>> c0303efeab73 (net: reduce cycles spend on ICMP replies that gets rate limited)
  
  	if (ip_options_echo(&icmp_param->replyopts.opt.opt, skb))
  		return;
@@@ -580,8 -669,12 +675,17 @@@ void icmp_send(struct sk_buff *skb_in, 
  	}
  
  	sk = icmp_xmit_lock(net);
++<<<<<<< HEAD
 +	if (sk == NULL)
 +		return;
++=======
+ 	if (!sk)
+ 		goto out;
+ 
+ 	/* Check global sysctl_icmp_msgs_per_sec ratelimit */
+ 	if (!icmpv4_global_allow(net, type, code))
+ 		goto out_unlock;
++>>>>>>> c0303efeab73 (net: reduce cycles spend on ICMP replies that gets rate limited)
  
  	/*
  	 *	Construct source address and options.
diff --cc net/ipv6/icmp.c
index 6afaf8b3b1a1,b26ae8b5c1ce..000000000000
--- a/net/ipv6/icmp.c
+++ b/net/ipv6/icmp.c
@@@ -162,19 -195,14 +186,14 @@@ static bool icmpv6_global_allow(int typ
  /*
   * Check the ICMP output rate limit
   */
 -static bool icmpv6_xrlim_allow(struct sock *sk, u8 type,
 -			       struct flowi6 *fl6)
 +static inline bool icmpv6_xrlim_allow(struct sock *sk, u8 type,
 +				      struct flowi6 *fl6)
  {
 -	struct net *net = sock_net(sk);
  	struct dst_entry *dst;
 +	struct net *net = sock_net(sk);
  	bool res = false;
  
- 	/* Informational messages are not limited. */
- 	if (type & ICMPV6_INFOMSG_MASK)
- 		return true;
- 
- 	/* Do not limit pmtu discovery, it would break it. */
- 	if (type == ICMPV6_PKT_TOOBIG)
+ 	if (icmpv6_mask_allow(type))
  		return true;
  
  	/*
@@@ -462,8 -505,14 +481,16 @@@ static void icmp6_send(struct sk_buff *
  	security_skb_classify_flow(skb, flowi6_to_flowi(&fl6));
  
  	sk = icmpv6_xmit_lock(net);
 -	if (!sk)
 +	if (sk == NULL)
  		return;
++<<<<<<< HEAD
++=======
+ 
+ 	if (!icmpv6_global_allow(type))
+ 		goto out;
+ 
+ 	sk->sk_mark = mark;
++>>>>>>> c0303efeab73 (net: reduce cycles spend on ICMP replies that gets rate limited)
  	np = inet6_sk(sk);
  
  	if (!icmpv6_xrlim_allow(sk, type, &fl6))
* Unmerged path net/ipv4/icmp.c
* Unmerged path net/ipv6/icmp.c
