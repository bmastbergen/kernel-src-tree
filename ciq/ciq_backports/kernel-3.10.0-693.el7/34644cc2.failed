s390/kaslr: randomize module base load address

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] kaslr: randomize module base load address (Baoquan He) [1290840]
Rebuild_FUZZ: 94.25%
commit-author Heiko Carstens <hca@linux.ibm.com>
commit 34644cc2e15a7a91ec36b496e218694d17371589
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/34644cc2.failed

Randomize the load address of modules in the kernel to make KASLR effective
for modules.
This is the s390 variant of commit e2b32e678513 ("x86, kaslr: randomize
module base load address").

	Reviewed-by: Vasily Gorbik <gor@linux.ibm.com>
	Signed-off-by: Heiko Carstens <hca@linux.ibm.com>
	Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>
(cherry picked from commit 34644cc2e15a7a91ec36b496e218694d17371589)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/kernel/module.c
diff --cc arch/s390/kernel/module.c
index 7845e15a17df,6588f4efe378..000000000000
--- a/arch/s390/kernel/module.c
+++ b/arch/s390/kernel/module.c
@@@ -27,10 -14,19 +27,20 @@@
  #include <linux/elf.h>
  #include <linux/vmalloc.h>
  #include <linux/fs.h>
 -#include <linux/ftrace.h>
  #include <linux/string.h>
  #include <linux/kernel.h>
 -#include <linux/kasan.h>
  #include <linux/moduleloader.h>
  #include <linux/bug.h>
++<<<<<<< HEAD
++=======
+ #include <linux/memory.h>
+ #include <asm/alternative.h>
+ #include <asm/nospec-branch.h>
+ #include <asm/facility.h>
+ #include <asm/ftrace.lds.h>
+ #include <asm/set_memory.h>
+ #include <asm/setup.h>
++>>>>>>> 34644cc2e15a (s390/kaslr: randomize module base load address)
  
  #if 0
  #define DEBUGP printk
@@@ -38,20 -34,48 +48,59 @@@
  #define DEBUGP(fmt , ...)
  #endif
  
 -#define PLT_ENTRY_SIZE 22
 +#ifndef CONFIG_64BIT
 +#define PLT_ENTRY_SIZE 12
 +#else /* CONFIG_64BIT */
 +#define PLT_ENTRY_SIZE 20
 +#endif /* CONFIG_64BIT */
  
++<<<<<<< HEAD
 +#ifdef CONFIG_64BIT
++=======
+ static unsigned long get_module_load_offset(void)
+ {
+ 	static DEFINE_MUTEX(module_kaslr_mutex);
+ 	static unsigned long module_load_offset;
+ 
+ 	if (!kaslr_enabled())
+ 		return 0;
+ 	/*
+ 	 * Calculate the module_load_offset the first time this code
+ 	 * is called. Once calculated it stays the same until reboot.
+ 	 */
+ 	mutex_lock(&module_kaslr_mutex);
+ 	if (!module_load_offset)
+ 		module_load_offset = get_random_u32_inclusive(1, 1024) * PAGE_SIZE;
+ 	mutex_unlock(&module_kaslr_mutex);
+ 	return module_load_offset;
+ }
+ 
++>>>>>>> 34644cc2e15a (s390/kaslr: randomize module base load address)
  void *module_alloc(unsigned long size)
  {
 -	gfp_t gfp_mask = GFP_KERNEL;
 -	void *p;
 -
  	if (PAGE_ALIGN(size) > MODULES_LEN)
  		return NULL;
++<<<<<<< HEAD
 +	return __vmalloc_node_range(size, 1, MODULES_VADDR, MODULES_END,
 +				    GFP_KERNEL, PAGE_KERNEL, -1,
 +				    __builtin_return_address(0));
++=======
+ 	p = __vmalloc_node_range(size, MODULE_ALIGN,
+ 				 MODULES_VADDR + get_module_load_offset(), MODULES_END,
+ 				 gfp_mask, PAGE_KERNEL_EXEC, VM_DEFER_KMEMLEAK, NUMA_NO_NODE,
+ 				 __builtin_return_address(0));
+ 	if (p && (kasan_alloc_module_shadow(p, size, gfp_mask) < 0)) {
+ 		vfree(p);
+ 		return NULL;
+ 	}
+ 	return p;
+ }
+ 
+ #ifdef CONFIG_FUNCTION_TRACER
+ void module_arch_cleanup(struct module *mod)
+ {
+ 	module_memfree(mod->arch.trampolines_start);
++>>>>>>> 34644cc2e15a (s390/kaslr: randomize module base load address)
  }
  #endif
  
* Unmerged path arch/s390/kernel/module.c
