nvme: add helper nvme_map_len()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [nvme] add helper nvme_map_len() (David Milburn) [1384066]
Rebuild_FUZZ: 89.29%
commit-author Ming Lin <ming.l@ssi.samsung.com>
commit 58b45602751ddf16e57170656670aa5a8f78eeca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/58b45602.failed

The helper returns the number of bytes that need to be mapped
using PRPs/SGL entries.

	Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 58b45602751ddf16e57170656670aa5a8f78eeca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/nvme.h
diff --cc drivers/nvme/host/nvme.h
index c84f7f8e647f,6376cd71cc9f..000000000000
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@@ -180,7 -173,16 +180,20 @@@ static inline u64 nvme_block_nr(struct 
  	return (sector >> (ns->lba_shift - 9));
  }
  
++<<<<<<< HEAD
 +static inline void nvme_setup_flush(struct nvme_ns *ns, struct nvme_command *cmnd)
++=======
+ static inline unsigned nvme_map_len(struct request *rq)
+ {
+ 	if (rq->cmd_flags & REQ_DISCARD)
+ 		return sizeof(struct nvme_dsm_range);
+ 	else
+ 		return blk_rq_bytes(rq);
+ }
+ 
+ static inline void nvme_setup_flush(struct nvme_ns *ns,
+ 		struct nvme_command *cmnd)
++>>>>>>> 58b45602751d (nvme: add helper nvme_map_len())
  {
  	memset(cmnd, 0, sizeof(*cmnd));
  	cmnd->common.opcode = nvme_cmd_flush;
* Unmerged path drivers/nvme/host/nvme.h
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index ee79751469a7..dd590d352b09 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -333,16 +333,11 @@ static __le64 **iod_list(struct request *req)
 	return (__le64 **)(iod->sg + req->nr_phys_segments);
 }
 
-static int nvme_init_iod(struct request *rq, struct nvme_dev *dev)
+static int nvme_init_iod(struct request *rq, unsigned size,
+		struct nvme_dev *dev)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(rq);
 	int nseg = rq->nr_phys_segments;
-	unsigned size;
-
-	if (rq->cmd_flags & REQ_DISCARD)
-		size = sizeof(struct nvme_dsm_range);
-	else
-		size = blk_rq_bytes(rq);
 
 	if (nseg > NVME_INT_PAGES || size > NVME_INT_BYTES(dev)) {
 		iod->sg = kmalloc(nvme_iod_alloc_size(dev, size, nseg), GFP_ATOMIC);
@@ -564,6 +559,7 @@ static int nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct nvme_dev *dev = nvmeq->dev;
 	struct request *req = bd->rq;
 	struct nvme_command cmnd;
+	unsigned map_len;
 	int ret = BLK_MQ_RQ_QUEUE_OK;
 
 	/*
@@ -579,7 +575,8 @@ static int nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 		}
 	}
 
-	ret = nvme_init_iod(req, dev);
+	map_len = nvme_map_len(req);
+	ret = nvme_init_iod(req, map_len, dev);
 	if (ret)
 		return ret;
 
