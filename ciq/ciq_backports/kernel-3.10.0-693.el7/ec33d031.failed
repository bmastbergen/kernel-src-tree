vhost: detect 32 bit integer wrap around

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [vhost] detect 32 bit integer wrap around (Wei Xu) [1283257 1425127]
Rebuild_FUZZ: 90.41%
commit-author Michael S. Tsirkin <mst@redhat.com>
commit ec33d031a14b3c5dd516627139c9550350dbba3e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ec33d031.failed

Detect and fail early if long wrap around is triggered.

	Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
(cherry picked from commit ec33d031a14b3c5dd516627139c9550350dbba3e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vhost/vhost.c
diff --cc drivers/vhost/vhost.c
index f65052baafd1,c6f2d89c0e97..000000000000
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@@ -582,26 -657,34 +582,44 @@@ static int log_access_ok(void __user *l
  			 (sz + VHOST_PAGE_SIZE * 8 - 1) / VHOST_PAGE_SIZE / 8);
  }
  
+ static bool vhost_overflow(u64 uaddr, u64 size)
+ {
+ 	/* Make sure 64 bit math will not overflow. */
+ 	return uaddr > ULONG_MAX || size > ULONG_MAX || uaddr > ULONG_MAX - size;
+ }
+ 
  /* Caller should have vq mutex and device mutex. */
 -static int vq_memory_access_ok(void __user *log_base, struct vhost_umem *umem,
 +static int vq_memory_access_ok(void __user *log_base, struct vhost_memory *mem,
  			       int log_all)
  {
 -	struct vhost_umem_node *node;
 +	int i;
  
 -	if (!umem)
 +	if (!mem)
  		return 0;
  
++<<<<<<< HEAD
 +	for (i = 0; i < mem->nregions; ++i) {
 +		struct vhost_memory_region *m = mem->regions + i;
 +		unsigned long a = m->userspace_addr;
 +		if (m->memory_size > ULONG_MAX)
 +			return 0;
 +		else if (!access_ok(VERIFY_WRITE, (void __user *)a,
 +				    m->memory_size))
++=======
+ 	list_for_each_entry(node, &umem->umem_list, link) {
+ 		unsigned long a = node->userspace_addr;
+ 
+ 		if (vhost_overflow(node->userspace_addr, node->size))
+ 			return 0;
+ 
+ 
+ 		if (!access_ok(VERIFY_WRITE, (void __user *)a,
+ 				    node->size))
++>>>>>>> ec33d031a14b (vhost: detect 32 bit integer wrap around)
  			return 0;
  		else if (log_all && !log_access_ok(log_base,
 -						   node->start,
 -						   node->size))
 +						   m->guest_phys_addr,
 +						   m->memory_size))
  			return 0;
  	}
  	return 1;
@@@ -632,6 -716,377 +650,380 @@@ static int memory_access_ok(struct vhos
  	return 1;
  }
  
++<<<<<<< HEAD
++=======
+ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
+ 			  struct iovec iov[], int iov_size, int access);
+ 
+ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void *to,
+ 			      const void *from, unsigned size)
+ {
+ 	int ret;
+ 
+ 	if (!vq->iotlb)
+ 		return __copy_to_user(to, from, size);
+ 	else {
+ 		/* This function should be called after iotlb
+ 		 * prefetch, which means we're sure that all vq
+ 		 * could be access through iotlb. So -EAGAIN should
+ 		 * not happen in this case.
+ 		 */
+ 		/* TODO: more fast path */
+ 		struct iov_iter t;
+ 		ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
+ 				     ARRAY_SIZE(vq->iotlb_iov),
+ 				     VHOST_ACCESS_WO);
+ 		if (ret < 0)
+ 			goto out;
+ 		iov_iter_init(&t, WRITE, vq->iotlb_iov, ret, size);
+ 		ret = copy_to_iter(from, size, &t);
+ 		if (ret == size)
+ 			ret = 0;
+ 	}
+ out:
+ 	return ret;
+ }
+ 
+ static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
+ 				void *from, unsigned size)
+ {
+ 	int ret;
+ 
+ 	if (!vq->iotlb)
+ 		return __copy_from_user(to, from, size);
+ 	else {
+ 		/* This function should be called after iotlb
+ 		 * prefetch, which means we're sure that vq
+ 		 * could be access through iotlb. So -EAGAIN should
+ 		 * not happen in this case.
+ 		 */
+ 		/* TODO: more fast path */
+ 		struct iov_iter f;
+ 		ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
+ 				     ARRAY_SIZE(vq->iotlb_iov),
+ 				     VHOST_ACCESS_RO);
+ 		if (ret < 0) {
+ 			vq_err(vq, "IOTLB translation failure: uaddr "
+ 			       "%p size 0x%llx\n", from,
+ 			       (unsigned long long) size);
+ 			goto out;
+ 		}
+ 		iov_iter_init(&f, READ, vq->iotlb_iov, ret, size);
+ 		ret = copy_from_iter(to, size, &f);
+ 		if (ret == size)
+ 			ret = 0;
+ 	}
+ 
+ out:
+ 	return ret;
+ }
+ 
+ static void __user *__vhost_get_user(struct vhost_virtqueue *vq,
+ 				     void *addr, unsigned size)
+ {
+ 	int ret;
+ 
+ 	/* This function should be called after iotlb
+ 	 * prefetch, which means we're sure that vq
+ 	 * could be access through iotlb. So -EAGAIN should
+ 	 * not happen in this case.
+ 	 */
+ 	/* TODO: more fast path */
+ 	ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
+ 			     ARRAY_SIZE(vq->iotlb_iov),
+ 			     VHOST_ACCESS_RO);
+ 	if (ret < 0) {
+ 		vq_err(vq, "IOTLB translation failure: uaddr "
+ 			"%p size 0x%llx\n", addr,
+ 			(unsigned long long) size);
+ 		return NULL;
+ 	}
+ 
+ 	if (ret != 1 || vq->iotlb_iov[0].iov_len != size) {
+ 		vq_err(vq, "Non atomic userspace memory access: uaddr "
+ 			"%p size 0x%llx\n", addr,
+ 			(unsigned long long) size);
+ 		return NULL;
+ 	}
+ 
+ 	return vq->iotlb_iov[0].iov_base;
+ }
+ 
+ #define vhost_put_user(vq, x, ptr) \
+ ({ \
+ 	int ret = -EFAULT; \
+ 	if (!vq->iotlb) { \
+ 		ret = __put_user(x, ptr); \
+ 	} else { \
+ 		__typeof__(ptr) to = \
+ 			(__typeof__(ptr)) __vhost_get_user(vq, ptr, sizeof(*ptr)); \
+ 		if (to != NULL) \
+ 			ret = __put_user(x, to); \
+ 		else \
+ 			ret = -EFAULT;	\
+ 	} \
+ 	ret; \
+ })
+ 
+ #define vhost_get_user(vq, x, ptr) \
+ ({ \
+ 	int ret; \
+ 	if (!vq->iotlb) { \
+ 		ret = __get_user(x, ptr); \
+ 	} else { \
+ 		__typeof__(ptr) from = \
+ 			(__typeof__(ptr)) __vhost_get_user(vq, ptr, sizeof(*ptr)); \
+ 		if (from != NULL) \
+ 			ret = __get_user(x, from); \
+ 		else \
+ 			ret = -EFAULT; \
+ 	} \
+ 	ret; \
+ })
+ 
+ static void vhost_dev_lock_vqs(struct vhost_dev *d)
+ {
+ 	int i = 0;
+ 	for (i = 0; i < d->nvqs; ++i)
+ 		mutex_lock(&d->vqs[i]->mutex);
+ }
+ 
+ static void vhost_dev_unlock_vqs(struct vhost_dev *d)
+ {
+ 	int i = 0;
+ 	for (i = 0; i < d->nvqs; ++i)
+ 		mutex_unlock(&d->vqs[i]->mutex);
+ }
+ 
+ static int vhost_new_umem_range(struct vhost_umem *umem,
+ 				u64 start, u64 size, u64 end,
+ 				u64 userspace_addr, int perm)
+ {
+ 	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);
+ 
+ 	if (!node)
+ 		return -ENOMEM;
+ 
+ 	if (umem->numem == max_iotlb_entries) {
+ 		tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+ 		vhost_umem_free(umem, tmp);
+ 	}
+ 
+ 	node->start = start;
+ 	node->size = size;
+ 	node->last = end;
+ 	node->userspace_addr = userspace_addr;
+ 	node->perm = perm;
+ 	INIT_LIST_HEAD(&node->link);
+ 	list_add_tail(&node->link, &umem->umem_list);
+ 	vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+ 	umem->numem++;
+ 
+ 	return 0;
+ }
+ 
+ static void vhost_del_umem_range(struct vhost_umem *umem,
+ 				 u64 start, u64 end)
+ {
+ 	struct vhost_umem_node *node;
+ 
+ 	while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+ 							   start, end)))
+ 		vhost_umem_free(umem, node);
+ }
+ 
+ static void vhost_iotlb_notify_vq(struct vhost_dev *d,
+ 				  struct vhost_iotlb_msg *msg)
+ {
+ 	struct vhost_msg_node *node, *n;
+ 
+ 	spin_lock(&d->iotlb_lock);
+ 
+ 	list_for_each_entry_safe(node, n, &d->pending_list, node) {
+ 		struct vhost_iotlb_msg *vq_msg = &node->msg.iotlb;
+ 		if (msg->iova <= vq_msg->iova &&
+ 		    msg->iova + msg->size - 1 > vq_msg->iova &&
+ 		    vq_msg->type == VHOST_IOTLB_MISS) {
+ 			vhost_poll_queue(&node->vq->poll);
+ 			list_del(&node->node);
+ 			kfree(node);
+ 		}
+ 	}
+ 
+ 	spin_unlock(&d->iotlb_lock);
+ }
+ 
+ static int umem_access_ok(u64 uaddr, u64 size, int access)
+ {
+ 	unsigned long a = uaddr;
+ 
+ 	/* Make sure 64 bit math will not overflow. */
+ 	if (vhost_overflow(uaddr, size))
+ 		return -EFAULT;
+ 
+ 	if ((access & VHOST_ACCESS_RO) &&
+ 	    !access_ok(VERIFY_READ, (void __user *)a, size))
+ 		return -EFAULT;
+ 	if ((access & VHOST_ACCESS_WO) &&
+ 	    !access_ok(VERIFY_WRITE, (void __user *)a, size))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ 
+ int vhost_process_iotlb_msg(struct vhost_dev *dev,
+ 			    struct vhost_iotlb_msg *msg)
+ {
+ 	int ret = 0;
+ 
+ 	vhost_dev_lock_vqs(dev);
+ 	switch (msg->type) {
+ 	case VHOST_IOTLB_UPDATE:
+ 		if (!dev->iotlb) {
+ 			ret = -EFAULT;
+ 			break;
+ 		}
+ 		if (umem_access_ok(msg->uaddr, msg->size, msg->perm)) {
+ 			ret = -EFAULT;
+ 			break;
+ 		}
+ 		if (vhost_new_umem_range(dev->iotlb, msg->iova, msg->size,
+ 					 msg->iova + msg->size - 1,
+ 					 msg->uaddr, msg->perm)) {
+ 			ret = -ENOMEM;
+ 			break;
+ 		}
+ 		vhost_iotlb_notify_vq(dev, msg);
+ 		break;
+ 	case VHOST_IOTLB_INVALIDATE:
+ 		vhost_del_umem_range(dev->iotlb, msg->iova,
+ 				     msg->iova + msg->size - 1);
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	vhost_dev_unlock_vqs(dev);
+ 	return ret;
+ }
+ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
+ 			     struct iov_iter *from)
+ {
+ 	struct vhost_msg_node node;
+ 	unsigned size = sizeof(struct vhost_msg);
+ 	size_t ret;
+ 	int err;
+ 
+ 	if (iov_iter_count(from) < size)
+ 		return 0;
+ 	ret = copy_from_iter(&node.msg, size, from);
+ 	if (ret != size)
+ 		goto done;
+ 
+ 	switch (node.msg.type) {
+ 	case VHOST_IOTLB_MSG:
+ 		err = vhost_process_iotlb_msg(dev, &node.msg.iotlb);
+ 		if (err)
+ 			ret = err;
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ done:
+ 	return ret;
+ }
+ EXPORT_SYMBOL(vhost_chr_write_iter);
+ 
+ unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
+ 			    poll_table *wait)
+ {
+ 	unsigned int mask = 0;
+ 
+ 	poll_wait(file, &dev->wait, wait);
+ 
+ 	if (!list_empty(&dev->read_list))
+ 		mask |= POLLIN | POLLRDNORM;
+ 
+ 	return mask;
+ }
+ EXPORT_SYMBOL(vhost_chr_poll);
+ 
+ ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
+ 			    int noblock)
+ {
+ 	DEFINE_WAIT(wait);
+ 	struct vhost_msg_node *node;
+ 	ssize_t ret = 0;
+ 	unsigned size = sizeof(struct vhost_msg);
+ 
+ 	if (iov_iter_count(to) < size)
+ 		return 0;
+ 
+ 	while (1) {
+ 		if (!noblock)
+ 			prepare_to_wait(&dev->wait, &wait,
+ 					TASK_INTERRUPTIBLE);
+ 
+ 		node = vhost_dequeue_msg(dev, &dev->read_list);
+ 		if (node)
+ 			break;
+ 		if (noblock) {
+ 			ret = -EAGAIN;
+ 			break;
+ 		}
+ 		if (signal_pending(current)) {
+ 			ret = -ERESTARTSYS;
+ 			break;
+ 		}
+ 		if (!dev->iotlb) {
+ 			ret = -EBADFD;
+ 			break;
+ 		}
+ 
+ 		schedule();
+ 	}
+ 
+ 	if (!noblock)
+ 		finish_wait(&dev->wait, &wait);
+ 
+ 	if (node) {
+ 		ret = copy_to_iter(&node->msg, size, to);
+ 
+ 		if (ret != size || node->msg.type != VHOST_IOTLB_MISS) {
+ 			kfree(node);
+ 			return ret;
+ 		}
+ 
+ 		vhost_enqueue_msg(dev, &dev->pending_list, node);
+ 	}
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(vhost_chr_read_iter);
+ 
+ static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
+ {
+ 	struct vhost_dev *dev = vq->dev;
+ 	struct vhost_msg_node *node;
+ 	struct vhost_iotlb_msg *msg;
+ 
+ 	node = vhost_new_msg(vq, VHOST_IOTLB_MISS);
+ 	if (!node)
+ 		return -ENOMEM;
+ 
+ 	msg = &node->msg.iotlb;
+ 	msg->type = VHOST_IOTLB_MISS;
+ 	msg->iova = iova;
+ 	msg->perm = access;
+ 
+ 	vhost_enqueue_msg(dev, &dev->read_list, node);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> ec33d031a14b (vhost: detect 32 bit integer wrap around)
  static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
  			struct vring_desc __user *desc,
  			struct vring_avail __user *avail,
* Unmerged path drivers/vhost/vhost.c
