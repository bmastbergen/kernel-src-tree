net: move __skb_tx_hash to dev.c

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [net] move __skb_tx_hash to dev.c (Jonathan Toppins) [1428557]
Rebuild_FUZZ: 91.53%
commit-author Jiri Pirko <jiri@resnulli.us>
commit 5605c76240aadc823e3d46ac9afde2f26fbcf019
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/5605c762.failed

__skb_tx_hash function has no relation to flow_dissect so just move it
to dev.c

	Signed-off-by: Jiri Pirko <jiri@resnulli.us>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 5605c76240aadc823e3d46ac9afde2f26fbcf019)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/skbuff.h
#	net/core/flow_dissector.c
diff --cc include/linux/skbuff.h
index 449d0a455cd2,b01c7fba7c17..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -3398,20 -3299,14 +3398,23 @@@ static inline bool skb_rx_queue_recorde
  	return skb->queue_mapping != 0;
  }
  
++<<<<<<< HEAD
 +u16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,
 +		  unsigned int num_tx_queues);
 +
 +#ifdef CONFIG_XFRM
++=======
++>>>>>>> 5605c76240aa (net: move __skb_tx_hash to dev.c)
  static inline struct sec_path *skb_sec_path(struct sk_buff *skb)
  {
 -#ifdef CONFIG_XFRM
  	return skb->sp;
 +}
  #else
 +static inline struct sec_path *skb_sec_path(struct sk_buff *skb)
 +{
  	return NULL;
 -#endif
  }
 +#endif
  
  /* Keeps track of mac header offset relative to skb->head.
   * It is useful for TSO of Tunneling protocol. e.g. GRE.
diff --cc net/core/flow_dissector.c
index 4e48f508edbd,07ca11d06339..000000000000
--- a/net/core/flow_dissector.c
+++ b/net/core/flow_dissector.c
@@@ -295,33 -363,13 +295,43 @@@ void __skb_get_hash(struct sk_buff *skb
  }
  EXPORT_SYMBOL(__skb_get_hash);
  
++<<<<<<< HEAD
 +/*
 + * Returns a Tx hash based on the given packet descriptor a Tx queues' number
 + * to be used as a distribution range.
 + */
 +u16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,
 +		  unsigned int num_tx_queues)
 +{
 +	u32 hash;
 +	u16 qoffset = 0;
 +	u16 qcount = num_tx_queues;
 +
 +	if (skb_rx_queue_recorded(skb)) {
 +		hash = skb_get_rx_queue(skb);
 +		while (unlikely(hash >= num_tx_queues))
 +			hash -= num_tx_queues;
 +		return hash;
 +	}
 +
 +	if (dev->num_tc) {
 +		u8 tc = netdev_get_prio_tc_map(dev, skb->priority);
 +		qoffset = dev->tc_to_txq[tc].offset;
 +		qcount = dev->tc_to_txq[tc].count;
 +	}
 +
 +	return (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;
 +}
 +EXPORT_SYMBOL(__skb_tx_hash);
++=======
+ __u32 skb_get_hash_perturb(const struct sk_buff *skb, u32 perturb)
+ {
+ 	struct flow_keys keys;
+ 
+ 	return ___skb_get_hash(skb, &keys, perturb);
+ }
+ EXPORT_SYMBOL(skb_get_hash_perturb);
++>>>>>>> 5605c76240aa (net: move __skb_tx_hash to dev.c)
  
  u32 __skb_get_poff(const struct sk_buff *skb, void *data,
  		   const struct flow_keys *keys, int hlen)
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 2f7ec2ad350a..8f7dff8433cf 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -3014,6 +3014,9 @@ static inline int netif_set_xps_queue(struct net_device *dev,
 }
 #endif
 
+u16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,
+		  unsigned int num_tx_queues);
+
 /*
  * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used
  * as a distribution range limit for the returned value.
* Unmerged path include/linux/skbuff.h
diff --git a/net/core/dev.c b/net/core/dev.c
index 0196a6d192c0..915d2dd920bf 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -2430,6 +2430,34 @@ void netif_device_attach(struct net_device *dev)
 }
 EXPORT_SYMBOL(netif_device_attach);
 
+/*
+ * Returns a Tx hash based on the given packet descriptor a Tx queues' number
+ * to be used as a distribution range.
+ */
+u16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,
+		  unsigned int num_tx_queues)
+{
+	u32 hash;
+	u16 qoffset = 0;
+	u16 qcount = num_tx_queues;
+
+	if (skb_rx_queue_recorded(skb)) {
+		hash = skb_get_rx_queue(skb);
+		while (unlikely(hash >= num_tx_queues))
+			hash -= num_tx_queues;
+		return hash;
+	}
+
+	if (dev->num_tc) {
+		u8 tc = netdev_get_prio_tc_map(dev, skb->priority);
+		qoffset = dev->tc_to_txq[tc].offset;
+		qcount = dev->tc_to_txq[tc].count;
+	}
+
+	return (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;
+}
+EXPORT_SYMBOL(__skb_tx_hash);
+
 static void skb_warn_bad_offload(const struct sk_buff *skb)
 {
 	static const netdev_features_t null_features = 0;
* Unmerged path net/core/flow_dissector.c
