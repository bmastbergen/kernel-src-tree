time: Parametrize all tk_fast_mono users

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 4498e7467e9e441c18ca12f1ca08460356e0508a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/4498e746.failed

In preparation for more tk_fast instances, remove all hard-coded
tk_fast_mono references.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: John Stultz <john.stultz@linaro.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20150319093400.484279927@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 4498e7467e9e441c18ca12f1ca08460356e0508a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/time/timekeeping.c
diff --cc kernel/time/timekeeping.c
index 1f122214a691,278373edb472..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -196,24 -325,179 +196,198 @@@ static inline s64 timekeeping_delta_to_
  	return nsec + arch_gettimeoffset();
  }
  
++<<<<<<< HEAD
 +static inline s64 timekeeping_get_ns(struct tk_read_base *tkr)
 +{
 +	cycle_t delta;
 +
 +	delta = timekeeping_get_delta(tkr);
 +	return timekeeping_delta_to_ns(tkr, delta);
 +}
 +
 +static inline s64 timekeeping_cycles_to_ns(struct tk_read_base *tkr,
 +					    cycle_t cycles)
++=======
+ /**
+  * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.
+  * @tkr: Timekeeping readout base from which we take the update
+  *
+  * We want to use this from any context including NMI and tracing /
+  * instrumenting the timekeeping code itself.
+  *
+  * So we handle this differently than the other timekeeping accessor
+  * functions which retry when the sequence count has changed. The
+  * update side does:
+  *
+  * smp_wmb();	<- Ensure that the last base[1] update is visible
+  * tkf->seq++;
+  * smp_wmb();	<- Ensure that the seqcount update is visible
+  * update(tkf->base[0], tkr);
+  * smp_wmb();	<- Ensure that the base[0] update is visible
+  * tkf->seq++;
+  * smp_wmb();	<- Ensure that the seqcount update is visible
+  * update(tkf->base[1], tkr);
+  *
+  * The reader side does:
+  *
+  * do {
+  *	seq = tkf->seq;
+  *	smp_rmb();
+  *	idx = seq & 0x01;
+  *	now = now(tkf->base[idx]);
+  *	smp_rmb();
+  * } while (seq != tkf->seq)
+  *
+  * As long as we update base[0] readers are forced off to
+  * base[1]. Once base[0] is updated readers are redirected to base[0]
+  * and the base[1] update takes place.
+  *
+  * So if a NMI hits the update of base[0] then it will use base[1]
+  * which is still consistent. In the worst case this can result is a
+  * slightly wrong timestamp (a few nanoseconds). See
+  * @ktime_get_mono_fast_ns.
+  */
+ static void update_fast_timekeeper(struct tk_read_base *tkr, struct tk_fast *tkf)
+ {
+ 	struct tk_read_base *base = tkf->base;
+ 
+ 	/* Force readers off to base[1] */
+ 	raw_write_seqcount_latch(&tkf->seq);
+ 
+ 	/* Update base[0] */
+ 	memcpy(base, tkr, sizeof(*base));
+ 
+ 	/* Force readers back to base[0] */
+ 	raw_write_seqcount_latch(&tkf->seq);
+ 
+ 	/* Update base[1] */
+ 	memcpy(base + 1, base, sizeof(*base));
+ }
+ 
+ /**
+  * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic
+  *
+  * This timestamp is not guaranteed to be monotonic across an update.
+  * The timestamp is calculated by:
+  *
+  *	now = base_mono + clock_delta * slope
+  *
+  * So if the update lowers the slope, readers who are forced to the
+  * not yet updated second array are still using the old steeper slope.
+  *
+  * tmono
+  * ^
+  * |    o  n
+  * |   o n
+  * |  u
+  * | o
+  * |o
+  * |12345678---> reader order
+  *
+  * o = old slope
+  * u = update
+  * n = new slope
+  *
+  * So reader 6 will observe time going backwards versus reader 5.
+  *
+  * While other CPUs are likely to be able observe that, the only way
+  * for a CPU local observation is when an NMI hits in the middle of
+  * the update. Timestamps taken from that NMI context might be ahead
+  * of the following timestamps. Callers need to be aware of that and
+  * deal with it.
+  */
+ static __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)
++>>>>>>> 4498e7467e9e (time: Parametrize all tk_fast_mono users)
  {
 -	struct tk_read_base *tkr;
 -	unsigned int seq;
 -	u64 now;
 +	cycle_t delta;
  
++<<<<<<< HEAD
 +	/* calculate the delta since the last update_wall_time */
 +	delta = clocksource_delta(cycles, tkr->clock->cycle_last,
 +				  tkr->clock->mask);
 +	return timekeeping_delta_to_ns(tkr, delta);
 +}
++=======
+ 	do {
+ 		seq = raw_read_seqcount(&tkf->seq);
+ 		tkr = tkf->base + (seq & 0x01);
+ 		now = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);
+ 	} while (read_seqcount_retry(&tkf->seq, seq));
+ 
+ 	return now;
+ }
+ 
+ u64 ktime_get_mono_fast_ns(void)
+ {
+ 	return __ktime_get_fast_ns(&tk_fast_mono);
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);
+ 
+ /* Suspend-time cycles value for halted fast timekeeper. */
+ static cycle_t cycles_at_suspend;
+ 
+ static cycle_t dummy_clock_read(struct clocksource *cs)
+ {
+ 	return cycles_at_suspend;
+ }
+ 
+ /**
+  * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.
+  * @tk: Timekeeper to snapshot.
+  *
+  * It generally is unsafe to access the clocksource after timekeeping has been
+  * suspended, so take a snapshot of the readout base of @tk and use it as the
+  * fast timekeeper's readout base while suspended.  It will return the same
+  * number of cycles every time until timekeeping is resumed at which time the
+  * proper readout base for the fast timekeeper will be restored automatically.
+  */
+ static void halt_fast_timekeeper(struct timekeeper *tk)
+ {
+ 	static struct tk_read_base tkr_dummy;
+ 	struct tk_read_base *tkr = &tk->tkr_mono;
+ 
+ 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
+ 	cycles_at_suspend = tkr->read(tkr->clock);
+ 	tkr_dummy.read = dummy_clock_read;
+ 	update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);
+ }
+ 
+ #ifdef CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 
+ static inline void update_vsyscall(struct timekeeper *tk)
+ {
+ 	struct timespec xt, wm;
+ 
+ 	xt = timespec64_to_timespec(tk_xtime(tk));
+ 	wm = timespec64_to_timespec(tk->wall_to_monotonic);
+ 	update_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,
+ 			    tk->tkr_mono.cycle_last);
+ }
+ 
+ static inline void old_vsyscall_fixup(struct timekeeper *tk)
+ {
+ 	s64 remainder;
+ 
+ 	/*
+ 	* Store only full nanoseconds into xtime_nsec after rounding
+ 	* it up and add the remainder to the error difference.
+ 	* XXX - This is necessary to avoid small 1ns inconsistnecies caused
+ 	* by truncating the remainder in vsyscalls. However, it causes
+ 	* additional work to be done in timekeeping_adjust(). Once
+ 	* the vsyscall implementations are converted to use xtime_nsec
+ 	* (shifted nanoseconds), and CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 	* users are removed, this can be killed.
+ 	*/
+ 	remainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);
+ 	tk->tkr_mono.xtime_nsec -= remainder;
+ 	tk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;
+ 	tk->ntp_error += remainder << tk->ntp_error_shift;
+ 	tk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;
+ }
+ #else
+ #define old_vsyscall_fixup(tk)
+ #endif
++>>>>>>> 4498e7467e9e (time: Parametrize all tk_fast_mono users)
  
  static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
  
@@@ -305,11 -587,11 +479,15 @@@ static void timekeeping_update(struct t
  	update_vsyscall(tk);
  	update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);
  
 -	if (action & TK_MIRROR)
 -		memcpy(&shadow_timekeeper, &tk_core.timekeeper,
 -		       sizeof(tk_core.timekeeper));
 +	if (action & TK_CLOCK_WAS_SET)
 +		tk->clock_was_set_seq++;
  
++<<<<<<< HEAD
 +	if (action & TK_MIRROR)
 +		memcpy(&shadow_timekeeper, &timekeeper, sizeof(timekeeper));
++=======
+ 	update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);
++>>>>>>> 4498e7467e9e (time: Parametrize all tk_fast_mono users)
  }
  
  /**
* Unmerged path kernel/time/timekeeping.c
