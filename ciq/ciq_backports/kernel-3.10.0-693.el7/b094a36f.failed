IB/hfi1: Refine user process affinity algorithm

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Sebastian Sanchez <sebastian.sanchez@intel.com>
commit b094a36f90975373c3a241839869217a65f17d81
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/b094a36f.failed

When performing process affinity recommendations for MPI ranks, the current
algorithm doesn't take into account multiple HFI units. Also, real
cores and HT cores are not distinguished from one another. Therefore,
all HT cores are recommended to be assigned first within the local NUMA
node before recommending the assignments of cores in other NUMA nodes.
It's ideal to assign all real cores across all NUMA nodes first, then all
HT 1 cores, then all HT 2 cores, and so on to balance CPU workload. CPU
cores in other NUMA nodes could be running interrupt handlers, and this is
not taken into account.

To balance the CPU workload for user processes, the following
recommendation algorithm is used:

 For each user process that is opening a context on HFI Y:
  a) If all cores are assigned to user processes, start assignments all
	 over from the first core
  b) Assign real cores first, then HT cores (First set of HT cores on
	 all physical cores, then second set of HT cores, and, so on) in the
	 following order:

	 1. Same NUMA node as HFI Y and not running an IRQ handler
	 2. Same NUMA node as HFI Y and running an IRQ handler
	 3. Different NUMA node to HFI Y and not running an IRQ handler
	 4. Different NUMA node to HFI Y and running an IRQ handler
  c) Mark core as assigned in the global affinity structure. As user
	 processes are done, remove core assignments from global affinity
	 structure.

This implementation allows an arbitrary number of HT cores and provides
support for multiple HFIs.

This is being included in the kernel rather than user space due to the
fact that user space has no way of knowing the CPU recommendations for
contexts running as part of other jobs.

	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Reviewed-by: Mitko Haralanov <mitko.haralanov@intel.com>
	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Sebastian Sanchez <sebastian.sanchez@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit b094a36f90975373c3a241839869217a65f17d81)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/affinity.c
#	drivers/infiniband/hw/hfi1/affinity.h
diff --cc drivers/infiniband/hw/hfi1/affinity.c
index 1ca2154de24c,c9dcbd55883a..000000000000
--- a/drivers/infiniband/hw/hfi1/affinity.c
+++ b/drivers/infiniband/hw/hfi1/affinity.c
@@@ -102,11 -105,52 +102,58 @@@ int init_real_cpu_mask(struct hfi1_devd
  	 * skip any gaps.
  	 */
  	for (; i < possible; i++) {
++<<<<<<< HEAD
 +		cpumask_clear_cpu(curr_cpu, &info->real_cpu_mask);
 +		curr_cpu = cpumask_next(curr_cpu, &info->real_cpu_mask);
++=======
+ 		cpumask_clear_cpu(curr_cpu, &node_affinity.real_cpu_mask);
+ 		curr_cpu = cpumask_next(curr_cpu, &node_affinity.real_cpu_mask);
+ 	}
+ }
+ 
+ int node_affinity_init(void)
+ {
+ 	int node;
+ 	struct pci_dev *dev = NULL;
+ 	const struct pci_device_id *ids = hfi1_pci_tbl;
+ 
+ 	cpumask_clear(&node_affinity.proc.used);
+ 	cpumask_copy(&node_affinity.proc.mask, cpu_online_mask);
+ 
+ 	node_affinity.proc.gen = 0;
+ 	node_affinity.num_core_siblings =
+ 				cpumask_weight(topology_sibling_cpumask(
+ 					cpumask_first(&node_affinity.proc.mask)
+ 					));
+ 	node_affinity.num_online_nodes = num_online_nodes();
+ 	node_affinity.num_online_cpus = num_online_cpus();
+ 
+ 	/*
+ 	 * The real cpu mask is part of the affinity struct but it has to be
+ 	 * initialized early. It is needed to calculate the number of user
+ 	 * contexts in set_up_context_variables().
+ 	 */
+ 	init_real_cpu_mask();
+ 
+ 	hfi1_per_node_cntr = kcalloc(num_possible_nodes(),
+ 				     sizeof(*hfi1_per_node_cntr), GFP_KERNEL);
+ 	if (!hfi1_per_node_cntr)
+ 		return -ENOMEM;
+ 
+ 	while (ids->vendor) {
+ 		dev = NULL;
+ 		while ((dev = pci_get_device(ids->vendor, ids->device, dev))) {
+ 			node = pcibus_to_node(dev->bus);
+ 			if (node < 0)
+ 				node = numa_node_id();
+ 
+ 			hfi1_per_node_cntr[node]++;
+ 		}
+ 		ids++;
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  	}
  
 +	dd->affinity = info;
  	return 0;
  }
  
@@@ -266,8 -403,15 +313,12 @@@ void hfi1_put_irq_affinity(struct hfi1_
  
  	switch (msix->type) {
  	case IRQ_SDMA:
 -		set = &entry->def_intr;
 -		break;
  	case IRQ_GENERAL:
++<<<<<<< HEAD
 +		set = &dd->affinity->def_intr;
++=======
+ 		/* Don't do accounting for general contexts */
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  		break;
  	case IRQ_RCVCTXT:
  		rcd = (struct hfi1_ctxtdata *)msix->arg;
@@@ -293,13 -437,47 +344,55 @@@
  	cpumask_clear(&msix->mask);
  }
  
- int hfi1_get_proc_affinity(struct hfi1_devdata *dd, int node)
+ /* This should be called with node_affinity.lock held */
+ static void find_hw_thread_mask(uint hw_thread_no, cpumask_var_t hw_thread_mask,
+ 				struct hfi1_affinity_node_list *affinity)
  {
++<<<<<<< HEAD
 +	int cpu = -1, ret;
 +	cpumask_var_t diff, mask, intrs;
 +	const struct cpumask *node_mask,
 +		*proc_mask = tsk_cpus_allowed(current);
 +	struct cpu_mask_set *set = &dd->affinity->proc;
++=======
+ 	int possible, curr_cpu, i;
+ 	uint num_cores_per_socket = node_affinity.num_online_cpus /
+ 					affinity->num_core_siblings /
+ 						node_affinity.num_online_nodes;
+ 
+ 	cpumask_copy(hw_thread_mask, &affinity->proc.mask);
+ 	if (affinity->num_core_siblings > 0) {
+ 		/* Removing other siblings not needed for now */
+ 		possible = cpumask_weight(hw_thread_mask);
+ 		curr_cpu = cpumask_first(hw_thread_mask);
+ 		for (i = 0;
+ 		     i < num_cores_per_socket * node_affinity.num_online_nodes;
+ 		     i++)
+ 			curr_cpu = cpumask_next(curr_cpu, hw_thread_mask);
+ 
+ 		for (; i < possible; i++) {
+ 			cpumask_clear_cpu(curr_cpu, hw_thread_mask);
+ 			curr_cpu = cpumask_next(curr_cpu, hw_thread_mask);
+ 		}
+ 
+ 		/* Identifying correct HW threads within physical cores */
+ 		cpumask_shift_left(hw_thread_mask, hw_thread_mask,
+ 				   num_cores_per_socket *
+ 				   node_affinity.num_online_nodes *
+ 				   hw_thread_no);
+ 	}
+ }
+ 
+ int hfi1_get_proc_affinity(int node)
+ {
+ 	int cpu = -1, ret, i;
+ 	struct hfi1_affinity_node *entry;
+ 	cpumask_var_t diff, hw_thread_mask, available_mask, intrs_mask;
+ 	const struct cpumask *node_mask,
+ 		*proc_mask = tsk_cpus_allowed(current);
+ 	struct hfi1_affinity_node_list *affinity = &node_affinity;
+ 	struct cpu_mask_set *set = &affinity->proc;
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  
  	/*
  	 * check whether process/context affinity has already
@@@ -331,16 -525,19 +440,23 @@@
  	ret = zalloc_cpumask_var(&diff, GFP_KERNEL);
  	if (!ret)
  		goto done;
- 	ret = zalloc_cpumask_var(&mask, GFP_KERNEL);
+ 	ret = zalloc_cpumask_var(&hw_thread_mask, GFP_KERNEL);
  	if (!ret)
  		goto free_diff;
- 	ret = zalloc_cpumask_var(&intrs, GFP_KERNEL);
+ 	ret = zalloc_cpumask_var(&available_mask, GFP_KERNEL);
+ 	if (!ret)
+ 		goto free_hw_thread_mask;
+ 	ret = zalloc_cpumask_var(&intrs_mask, GFP_KERNEL);
  	if (!ret)
- 		goto free_mask;
+ 		goto free_available_mask;
  
++<<<<<<< HEAD
 +	spin_lock(&dd->affinity->lock);
++=======
+ 	spin_lock(&affinity->lock);
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  	/*
- 	 * If we've used all available CPUs, clear the mask and start
+ 	 * If we've used all available HW threads, clear the mask and start
  	 * overloading.
  	 */
  	if (cpumask_equal(&set->mask, &set->used)) {
@@@ -348,33 -545,58 +464,68 @@@
  		cpumask_clear(&set->used);
  	}
  
++<<<<<<< HEAD
 +	/* CPUs used by interrupt handlers */
 +	cpumask_copy(intrs, (dd->affinity->def_intr.gen ?
 +			     &dd->affinity->def_intr.mask :
 +			     &dd->affinity->def_intr.used));
 +	cpumask_or(intrs, intrs, (dd->affinity->rcv_intr.gen ?
 +				  &dd->affinity->rcv_intr.mask :
 +				  &dd->affinity->rcv_intr.used));
++=======
+ 	/*
+ 	 * If NUMA node has CPUs used by interrupt handlers, include them in the
+ 	 * interrupt handler mask.
+ 	 */
+ 	entry = node_affinity_lookup(node);
+ 	if (entry) {
+ 		cpumask_copy(intrs_mask, (entry->def_intr.gen ?
+ 					  &entry->def_intr.mask :
+ 					  &entry->def_intr.used));
+ 		cpumask_or(intrs_mask, intrs_mask, (entry->rcv_intr.gen ?
+ 						    &entry->rcv_intr.mask :
+ 						    &entry->rcv_intr.used));
+ 		cpumask_or(intrs_mask, intrs_mask, &entry->general_intr_mask);
+ 	}
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  	hfi1_cdbg(PROC, "CPUs used by interrupts: %*pbl",
- 		  cpumask_pr_args(intrs));
+ 		  cpumask_pr_args(intrs_mask));
+ 
+ 	cpumask_copy(hw_thread_mask, &set->mask);
  
  	/*
- 	 * If we don't have a NUMA node requested, preference is towards
- 	 * device NUMA node
+ 	 * If HT cores are enabled, identify which HW threads within the
+ 	 * physical cores should be used.
  	 */
- 	if (node == -1)
- 		node = dd->node;
+ 	if (affinity->num_core_siblings > 0) {
+ 		for (i = 0; i < affinity->num_core_siblings; i++) {
+ 			find_hw_thread_mask(i, hw_thread_mask, affinity);
+ 
+ 			/*
+ 			 * If there's at least one available core for this HW
+ 			 * thread number, stop looking for a core.
+ 			 *
+ 			 * diff will always be not empty at least once in this
+ 			 * loop as the used mask gets reset when
+ 			 * (set->mask == set->used) before this loop.
+ 			 */
+ 			cpumask_andnot(diff, hw_thread_mask, &set->used);
+ 			if (!cpumask_empty(diff))
+ 				break;
+ 		}
+ 	}
+ 	hfi1_cdbg(PROC, "Same available HW thread on all physical CPUs: %*pbl",
+ 		  cpumask_pr_args(hw_thread_mask));
+ 
  	node_mask = cpumask_of_node(node);
- 	hfi1_cdbg(PROC, "device on NUMA %u, CPUs %*pbl", node,
+ 	hfi1_cdbg(PROC, "Device on NUMA %u, CPUs %*pbl", node,
  		  cpumask_pr_args(node_mask));
  
- 	/* diff will hold all unused cpus */
- 	cpumask_andnot(diff, &set->mask, &set->used);
- 	hfi1_cdbg(PROC, "unused CPUs (all) %*pbl", cpumask_pr_args(diff));
- 
- 	/* get cpumask of available CPUs on preferred NUMA */
- 	cpumask_and(mask, diff, node_mask);
- 	hfi1_cdbg(PROC, "available cpus on NUMA %*pbl", cpumask_pr_args(mask));
+ 	/* Get cpumask of available CPUs on preferred NUMA */
+ 	cpumask_and(available_mask, hw_thread_mask, node_mask);
+ 	cpumask_andnot(available_mask, available_mask, &set->used);
+ 	hfi1_cdbg(PROC, "Available CPUs on NUMA %u: %*pbl", node,
+ 		  cpumask_pr_args(available_mask));
  
  	/*
  	 * At first, we don't want to place processes on the same
@@@ -400,29 -643,33 +572,49 @@@
  		cpu = -1;
  	else
  		cpumask_set_cpu(cpu, &set->used);
++<<<<<<< HEAD
 +	spin_unlock(&dd->affinity->lock);
- 
- 	free_cpumask_var(intrs);
- free_mask:
- 	free_cpumask_var(mask);
++=======
+ 	spin_unlock(&affinity->lock);
+ 	hfi1_cdbg(PROC, "Process assigned to CPU %d", cpu);
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
+ 
+ 	free_cpumask_var(intrs_mask);
+ free_available_mask:
+ 	free_cpumask_var(available_mask);
+ free_hw_thread_mask:
+ 	free_cpumask_var(hw_thread_mask);
  free_diff:
  	free_cpumask_var(diff);
  done:
  	return cpu;
  }
  
- void hfi1_put_proc_affinity(struct hfi1_devdata *dd, int cpu)
+ void hfi1_put_proc_affinity(int cpu)
  {
++<<<<<<< HEAD
 +	struct cpu_mask_set *set = &dd->affinity->proc;
 +
 +	if (cpu < 0)
 +		return;
 +	spin_lock(&dd->affinity->lock);
++=======
+ 	struct hfi1_affinity_node_list *affinity = &node_affinity;
+ 	struct cpu_mask_set *set = &affinity->proc;
+ 
+ 	if (cpu < 0)
+ 		return;
+ 	spin_lock(&affinity->lock);
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  	cpumask_clear_cpu(cpu, &set->used);
+ 	hfi1_cdbg(PROC, "Returning CPU %d for future process assignment", cpu);
  	if (cpumask_empty(&set->used) && set->gen) {
  		set->gen--;
  		cpumask_copy(&set->used, &set->mask);
  	}
++<<<<<<< HEAD
 +	spin_unlock(&dd->affinity->lock);
++=======
+ 	spin_unlock(&affinity->lock);
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  }
- 
diff --cc drivers/infiniband/hw/hfi1/affinity.h
index 20f52fe74091,f784de52e881..000000000000
--- a/drivers/infiniband/hw/hfi1/affinity.h
+++ b/drivers/infiniband/hw/hfi1/affinity.h
@@@ -101,8 -98,31 +100,34 @@@ void hfi1_put_irq_affinity(struct hfi1_
   * Determine a CPU affinity for a user process, if the process does not
   * have an affinity set yet.
   */
- int hfi1_get_proc_affinity(struct hfi1_devdata *, int);
+ int hfi1_get_proc_affinity(int);
  /* Release a CPU used by a user process. */
- void hfi1_put_proc_affinity(struct hfi1_devdata *, int);
+ void hfi1_put_proc_affinity(int);
+ 
++<<<<<<< HEAD
++=======
+ struct hfi1_affinity_node {
+ 	int node;
+ 	struct cpu_mask_set def_intr;
+ 	struct cpu_mask_set rcv_intr;
+ 	struct cpumask general_intr_mask;
+ 	struct list_head list;
+ };
+ 
+ struct hfi1_affinity_node_list {
+ 	struct list_head list;
+ 	struct cpumask real_cpu_mask;
+ 	struct cpu_mask_set proc;
+ 	int num_core_siblings;
+ 	int num_online_nodes;
+ 	int num_online_cpus;
+ 	/* protect affinity node list */
+ 	spinlock_t lock;
+ };
+ 
+ int node_affinity_init(void);
+ void node_affinity_destroy(void);
+ extern struct hfi1_affinity_node_list node_affinity;
  
++>>>>>>> b094a36f9097 (IB/hfi1: Refine user process affinity algorithm)
  #endif /* _HFI1_AFFINITY_H */
* Unmerged path drivers/infiniband/hw/hfi1/affinity.c
* Unmerged path drivers/infiniband/hw/hfi1/affinity.h
diff --git a/drivers/infiniband/hw/hfi1/file_ops.c b/drivers/infiniband/hw/hfi1/file_ops.c
index bd431292a676..647f0ede48da 100644
--- a/drivers/infiniband/hw/hfi1/file_ops.c
+++ b/drivers/infiniband/hw/hfi1/file_ops.c
@@ -729,7 +729,7 @@ static int hfi1_file_close(struct inode *inode, struct file *fp)
 	hfi1_user_sdma_free_queues(fdata);
 
 	/* release the cpu */
-	hfi1_put_proc_affinity(dd, fdata->rec_cpu_num);
+	hfi1_put_proc_affinity(fdata->rec_cpu_num);
 
 	/*
 	 * Clear any left over, unhandled events so the next process that
@@ -828,9 +828,10 @@ static int assign_ctxt(struct file *fp, struct hfi1_user_info *uinfo)
 		ret = find_shared_ctxt(fp, uinfo);
 		if (ret < 0)
 			goto done_unlock;
-		if (ret)
-			fd->rec_cpu_num = hfi1_get_proc_affinity(
-				fd->uctxt->dd, fd->uctxt->numa_id);
+		if (ret) {
+			fd->rec_cpu_num =
+				hfi1_get_proc_affinity(fd->uctxt->numa_id);
+		}
 	}
 
 	/*
@@ -941,7 +942,11 @@ static int allocate_ctxt(struct file *fp, struct hfi1_devdata *dd,
 	if (ctxt == dd->num_rcv_contexts)
 		return -EBUSY;
 
-	fd->rec_cpu_num = hfi1_get_proc_affinity(dd, -1);
+	/*
+	 * If we don't have a NUMA node requested, preference is towards
+	 * device NUMA node.
+	 */
+	fd->rec_cpu_num = hfi1_get_proc_affinity(dd->node);
 	if (fd->rec_cpu_num != -1)
 		numa = cpu_to_node(fd->rec_cpu_num);
 	else
