crypto: ccp - Refactor the storage block allocation code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [crypto] ccp - Refactor the storage block allocation code (Suravee Suthikulpanit) [1390820]
Rebuild_FUZZ: 92.31%
commit-author Gary R Hook <gary.hook@amd.com>
commit 58a690b701efc32ffd49722dd7b887154eb5a205
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/58a690b7.failed

Move the KSB access/management functions to the v3
device file, and add function pointers to the actions
structure. At the operations layer all of the references
to the storage block will be generic (virtual). This is
in preparation for a version 5 device, in which the
private storage block is managed differently.

	Signed-off-by: Gary R Hook <gary.hook@amd.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 58a690b701efc32ffd49722dd7b887154eb5a205)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/ccp/ccp-dev-v3.c
#	drivers/crypto/ccp/ccp-dev.h
#	drivers/crypto/ccp/ccp-ops.c
diff --cc drivers/crypto/ccp/ccp-dev.h
index 72bf1536b653,4e38a61fbe5d..000000000000
--- a/drivers/crypto/ccp/ccp-dev.h
+++ b/drivers/crypto/ccp/ccp-dev.h
@@@ -139,7 -144,9 +139,13 @@@
  #define CCP_ECC_RESULT_OFFSET		60
  #define CCP_ECC_RESULT_SUCCESS		0x0001
  
++<<<<<<< HEAD
 +
++=======
+ #define CCP_SB_BYTES			32
+ 
+ struct ccp_op;
++>>>>>>> 58a690b701ef (crypto: ccp - Refactor the storage block allocation code)
  struct ccp_device;
  struct ccp_cmd;
  
@@@ -234,21 -282,29 +240,40 @@@ struct ccp_device 
  	 */
  	atomic_t current_id ____cacheline_aligned;
  
++<<<<<<< HEAD
 +	/*
 +	 * The CCP uses key storage blocks (KSB) to maintain context for certain
 +	 * operations. To prevent multiple cmds from using the same KSB range
 +	 * a command queue reserves a KSB range for the duration of the cmd.
 +	 * Each queue, will however, reserve 2 KSB blocks for operations that
 +	 * only require single KSB entries (eg. AES context/iv and key) in order
 +	 * to avoid allocation contention.  This will reserve at most 10 KSB
 +	 * entries, leaving 40 KSB entries available for dynamic allocation.
++=======
+ 	/* The v3 CCP uses key storage blocks (SB) to maintain context for
+ 	 * certain operations. To prevent multiple cmds from using the same
+ 	 * SB range a command queue reserves an SB range for the duration of
+ 	 * the cmd. Each queue, will however, reserve 2 SB blocks for
+ 	 * operations that only require single SB entries (eg. AES context/iv
+ 	 * and key) in order to avoid allocation contention.  This will reserve
+ 	 * at most 10 SB entries, leaving 40 SB entries available for dynamic
+ 	 * allocation.
+ 	 *
+ 	 * The v5 CCP Local Storage Block (LSB) is broken up into 8
+ 	 * memrory ranges, each of which can be enabled for access by one
+ 	 * or more queues. Device initialization takes this into account,
+ 	 * and attempts to assign one region for exclusive use by each
+ 	 * available queue; the rest are then aggregated as "public" use.
+ 	 * If there are fewer regions than queues, all regions are shared
+ 	 * amongst all queues.
++>>>>>>> 58a690b701ef (crypto: ccp - Refactor the storage block allocation code)
  	 */
 -	struct mutex sb_mutex ____cacheline_aligned;
 -	DECLARE_BITMAP(sb, KSB_COUNT);
 -	wait_queue_head_t sb_queue;
 -	unsigned int sb_avail;
 -	unsigned int sb_count;
 -	u32 sb_start;
 +	struct mutex ksb_mutex ____cacheline_aligned;
 +	DECLARE_BITMAP(ksb, KSB_COUNT);
 +	wait_queue_head_t ksb_queue;
 +	unsigned int ksb_avail;
 +	unsigned int ksb_count;
 +	u32 ksb_start;
  
  	/* Suspend support */
  	unsigned int suspending;
@@@ -268,4 -443,34 +293,37 @@@ irqreturn_t ccp_irq_handler(int irq, vo
  
  int ccp_run_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd);
  
++<<<<<<< HEAD
++=======
+ int ccp_dmaengine_register(struct ccp_device *ccp);
+ void ccp_dmaengine_unregister(struct ccp_device *ccp);
+ 
+ /* Structure for computation functions that are device-specific */
+ struct ccp_actions {
+ 	int (*aes)(struct ccp_op *);
+ 	int (*xts_aes)(struct ccp_op *);
+ 	int (*sha)(struct ccp_op *);
+ 	int (*rsa)(struct ccp_op *);
+ 	int (*passthru)(struct ccp_op *);
+ 	int (*ecc)(struct ccp_op *);
+ 	u32 (*sballoc)(struct ccp_cmd_queue *, unsigned int);
+ 	void (*sbfree)(struct ccp_cmd_queue *, unsigned int,
+ 			       unsigned int);
+ 	int (*init)(struct ccp_device *);
+ 	void (*destroy)(struct ccp_device *);
+ 	irqreturn_t (*irqhandler)(int, void *);
+ };
+ 
+ /* Structure to hold CCP version-specific values */
+ struct ccp_vdata {
+ 	unsigned int version;
+ 	int (*init)(struct ccp_device *);
+ 	const struct ccp_actions *perform;
+ 	const unsigned int bar;
+ 	const unsigned int offset;
+ };
+ 
+ extern	struct ccp_vdata ccpv3;
+ 
++>>>>>>> 58a690b701ef (crypto: ccp - Refactor the storage block allocation code)
  #endif
diff --cc drivers/crypto/ccp/ccp-ops.c
index 23dbb41465d1,bd9eb1d4512a..000000000000
--- a/drivers/crypto/ccp/ccp-ops.c
+++ b/drivers/crypto/ccp/ccp-ops.c
@@@ -26,431 -20,27 +26,434 @@@
  
  #include "ccp-dev.h"
  
 -/* SHA initial context values */
 -static const __be32 ccp_sha1_init[CCP_SHA_CTXSIZE / sizeof(__be32)] = {
 -	cpu_to_be32(SHA1_H0), cpu_to_be32(SHA1_H1),
 -	cpu_to_be32(SHA1_H2), cpu_to_be32(SHA1_H3),
 -	cpu_to_be32(SHA1_H4), 0, 0, 0,
 +
 +enum ccp_memtype {
 +	CCP_MEMTYPE_SYSTEM = 0,
 +	CCP_MEMTYPE_KSB,
 +	CCP_MEMTYPE_LOCAL,
 +	CCP_MEMTYPE__LAST,
 +};
 +
 +struct ccp_dma_info {
 +	dma_addr_t address;
 +	unsigned int offset;
 +	unsigned int length;
 +	enum dma_data_direction dir;
 +};
 +
 +struct ccp_dm_workarea {
 +	struct device *dev;
 +	struct dma_pool *dma_pool;
 +	unsigned int length;
 +
 +	u8 *address;
 +	struct ccp_dma_info dma;
 +};
 +
++<<<<<<< HEAD
 +struct ccp_sg_workarea {
 +	struct scatterlist *sg;
 +	int nents;
 +
 +	struct scatterlist *dma_sg;
 +	struct device *dma_dev;
 +	unsigned int dma_count;
 +	enum dma_data_direction dma_dir;
 +
 +	u32 sg_used;
 +
 +	u32 bytes_left;
 +};
 +
 +struct ccp_data {
 +	struct ccp_sg_workarea sg_wa;
 +	struct ccp_dm_workarea dm_wa;
 +};
 +
 +struct ccp_mem {
 +	enum ccp_memtype type;
 +	union {
 +		struct ccp_dma_info dma;
 +		u32 ksb;
 +	} u;
 +};
 +
 +struct ccp_aes_op {
 +	enum ccp_aes_type type;
 +	enum ccp_aes_mode mode;
 +	enum ccp_aes_action action;
 +};
 +
 +struct ccp_xts_aes_op {
 +	enum ccp_aes_action action;
 +	enum ccp_xts_aes_unit_size unit_size;
 +};
 +
 +struct ccp_sha_op {
 +	enum ccp_sha_type type;
 +	u64 msg_bits;
 +};
 +
 +struct ccp_rsa_op {
 +	u32 mod_size;
 +	u32 input_len;
 +};
 +
 +struct ccp_passthru_op {
 +	enum ccp_passthru_bitwise bit_mod;
 +	enum ccp_passthru_byteswap byte_swap;
 +};
 +
 +struct ccp_ecc_op {
 +	enum ccp_ecc_function function;
 +};
 +
 +struct ccp_op {
 +	struct ccp_cmd_queue *cmd_q;
 +
 +	u32 jobid;
 +	u32 ioc;
 +	u32 soc;
 +	u32 ksb_key;
 +	u32 ksb_ctx;
 +	u32 init;
 +	u32 eom;
 +
 +	struct ccp_mem src;
 +	struct ccp_mem dst;
 +
 +	union {
 +		struct ccp_aes_op aes;
 +		struct ccp_xts_aes_op xts;
 +		struct ccp_sha_op sha;
 +		struct ccp_rsa_op rsa;
 +		struct ccp_passthru_op passthru;
 +		struct ccp_ecc_op ecc;
 +	} u;
 +};
 +
 +/* The CCP cannot perform zero-length sha operations so the caller
 + * is required to buffer data for the final operation.  However, a
 + * sha operation for a message with a total length of zero is valid
 + * so known values are required to supply the result.
 + */
 +static const u8 ccp_sha1_zero[CCP_SHA_CTXSIZE] = {
 +	0xda, 0x39, 0xa3, 0xee, 0x5e, 0x6b, 0x4b, 0x0d,
 +	0x32, 0x55, 0xbf, 0xef, 0x95, 0x60, 0x18, 0x90,
 +	0xaf, 0xd8, 0x07, 0x09, 0x00, 0x00, 0x00, 0x00,
 +	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
  };
  
 -static const __be32 ccp_sha224_init[CCP_SHA_CTXSIZE / sizeof(__be32)] = {
 -	cpu_to_be32(SHA224_H0), cpu_to_be32(SHA224_H1),
 -	cpu_to_be32(SHA224_H2), cpu_to_be32(SHA224_H3),
 -	cpu_to_be32(SHA224_H4), cpu_to_be32(SHA224_H5),
 -	cpu_to_be32(SHA224_H6), cpu_to_be32(SHA224_H7),
 +static const u8 ccp_sha224_zero[CCP_SHA_CTXSIZE] = {
 +	0xd1, 0x4a, 0x02, 0x8c, 0x2a, 0x3a, 0x2b, 0xc9,
 +	0x47, 0x61, 0x02, 0xbb, 0x28, 0x82, 0x34, 0xc4,
 +	0x15, 0xa2, 0xb0, 0x1f, 0x82, 0x8e, 0xa6, 0x2a,
 +	0xc5, 0xb3, 0xe4, 0x2f, 0x00, 0x00, 0x00, 0x00,
  };
  
 -static const __be32 ccp_sha256_init[CCP_SHA_CTXSIZE / sizeof(__be32)] = {
 -	cpu_to_be32(SHA256_H0), cpu_to_be32(SHA256_H1),
 -	cpu_to_be32(SHA256_H2), cpu_to_be32(SHA256_H3),
 -	cpu_to_be32(SHA256_H4), cpu_to_be32(SHA256_H5),
 -	cpu_to_be32(SHA256_H6), cpu_to_be32(SHA256_H7),
 +static const u8 ccp_sha256_zero[CCP_SHA_CTXSIZE] = {
 +	0xe3, 0xb0, 0xc4, 0x42, 0x98, 0xfc, 0x1c, 0x14,
 +	0x9a, 0xfb, 0xf4, 0xc8, 0x99, 0x6f, 0xb9, 0x24,
 +	0x27, 0xae, 0x41, 0xe4, 0x64, 0x9b, 0x93, 0x4c,
 +	0xa4, 0x95, 0x99, 0x1b, 0x78, 0x52, 0xb8, 0x55,
  };
  
 +static u32 ccp_addr_lo(struct ccp_dma_info *info)
 +{
 +	return lower_32_bits(info->address + info->offset);
 +}
 +
 +static u32 ccp_addr_hi(struct ccp_dma_info *info)
 +{
 +	return upper_32_bits(info->address + info->offset) & 0x0000ffff;
 +}
 +
 +static int ccp_do_cmd(struct ccp_op *op, u32 *cr, unsigned int cr_count)
 +{
 +	struct ccp_cmd_queue *cmd_q = op->cmd_q;
 +	struct ccp_device *ccp = cmd_q->ccp;
 +	void __iomem *cr_addr;
 +	u32 cr0, cmd;
 +	unsigned int i;
 +	int ret = 0;
 +
 +	/* We could read a status register to see how many free slots
 +	 * are actually available, but reading that register resets it
 +	 * and you could lose some error information.
 +	 */
 +	cmd_q->free_slots--;
 +
 +	cr0 = (cmd_q->id << REQ0_CMD_Q_SHIFT)
 +	      | (op->jobid << REQ0_JOBID_SHIFT)
 +	      | REQ0_WAIT_FOR_WRITE;
 +
 +	if (op->soc)
 +		cr0 |= REQ0_STOP_ON_COMPLETE
 +		       | REQ0_INT_ON_COMPLETE;
 +
 +	if (op->ioc || !cmd_q->free_slots)
 +		cr0 |= REQ0_INT_ON_COMPLETE;
 +
 +	/* Start at CMD_REQ1 */
 +	cr_addr = ccp->io_regs + CMD_REQ0 + CMD_REQ_INCR;
 +
 +	mutex_lock(&ccp->req_mutex);
 +
 +	/* Write CMD_REQ1 through CMD_REQx first */
 +	for (i = 0; i < cr_count; i++, cr_addr += CMD_REQ_INCR)
 +		iowrite32(*(cr + i), cr_addr);
 +
 +	/* Tell the CCP to start */
 +	wmb();
 +	iowrite32(cr0, ccp->io_regs + CMD_REQ0);
 +
 +	mutex_unlock(&ccp->req_mutex);
 +
 +	if (cr0 & REQ0_INT_ON_COMPLETE) {
 +		/* Wait for the job to complete */
 +		ret = wait_event_interruptible(cmd_q->int_queue,
 +					       cmd_q->int_rcvd);
 +		if (ret || cmd_q->cmd_error) {
 +			/* On error delete all related jobs from the queue */
 +			cmd = (cmd_q->id << DEL_Q_ID_SHIFT)
 +			      | op->jobid;
 +
 +			iowrite32(cmd, ccp->io_regs + DEL_CMD_Q_JOB);
 +
 +			if (!ret)
 +				ret = -EIO;
 +		} else if (op->soc) {
 +			/* Delete just head job from the queue on SoC */
 +			cmd = DEL_Q_ACTIVE
 +			      | (cmd_q->id << DEL_Q_ID_SHIFT)
 +			      | op->jobid;
 +
 +			iowrite32(cmd, ccp->io_regs + DEL_CMD_Q_JOB);
 +		}
 +
 +		cmd_q->free_slots = CMD_Q_DEPTH(cmd_q->q_status);
 +
 +		cmd_q->int_rcvd = 0;
 +	}
 +
 +	return ret;
 +}
 +
 +static int ccp_perform_aes(struct ccp_op *op)
 +{
 +	u32 cr[6];
 +
 +	/* Fill out the register contents for REQ1 through REQ6 */
 +	cr[0] = (CCP_ENGINE_AES << REQ1_ENGINE_SHIFT)
 +		| (op->u.aes.type << REQ1_AES_TYPE_SHIFT)
 +		| (op->u.aes.mode << REQ1_AES_MODE_SHIFT)
 +		| (op->u.aes.action << REQ1_AES_ACTION_SHIFT)
 +		| (op->ksb_key << REQ1_KEY_KSB_SHIFT);
 +	cr[1] = op->src.u.dma.length - 1;
 +	cr[2] = ccp_addr_lo(&op->src.u.dma);
 +	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
 +		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->src.u.dma);
 +	cr[4] = ccp_addr_lo(&op->dst.u.dma);
 +	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->dst.u.dma);
 +
 +	if (op->u.aes.mode == CCP_AES_MODE_CFB)
 +		cr[0] |= ((0x7f) << REQ1_AES_CFB_SIZE_SHIFT);
 +
 +	if (op->eom)
 +		cr[0] |= REQ1_EOM;
 +
 +	if (op->init)
 +		cr[0] |= REQ1_INIT;
 +
 +	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
 +}
 +
 +static int ccp_perform_xts_aes(struct ccp_op *op)
 +{
 +	u32 cr[6];
 +
 +	/* Fill out the register contents for REQ1 through REQ6 */
 +	cr[0] = (CCP_ENGINE_XTS_AES_128 << REQ1_ENGINE_SHIFT)
 +		| (op->u.xts.action << REQ1_AES_ACTION_SHIFT)
 +		| (op->u.xts.unit_size << REQ1_XTS_AES_SIZE_SHIFT)
 +		| (op->ksb_key << REQ1_KEY_KSB_SHIFT);
 +	cr[1] = op->src.u.dma.length - 1;
 +	cr[2] = ccp_addr_lo(&op->src.u.dma);
 +	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
 +		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->src.u.dma);
 +	cr[4] = ccp_addr_lo(&op->dst.u.dma);
 +	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->dst.u.dma);
 +
 +	if (op->eom)
 +		cr[0] |= REQ1_EOM;
 +
 +	if (op->init)
 +		cr[0] |= REQ1_INIT;
 +
 +	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
 +}
 +
 +static int ccp_perform_sha(struct ccp_op *op)
 +{
 +	u32 cr[6];
 +
 +	/* Fill out the register contents for REQ1 through REQ6 */
 +	cr[0] = (CCP_ENGINE_SHA << REQ1_ENGINE_SHIFT)
 +		| (op->u.sha.type << REQ1_SHA_TYPE_SHIFT)
 +		| REQ1_INIT;
 +	cr[1] = op->src.u.dma.length - 1;
 +	cr[2] = ccp_addr_lo(&op->src.u.dma);
 +	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
 +		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->src.u.dma);
 +
 +	if (op->eom) {
 +		cr[0] |= REQ1_EOM;
 +		cr[4] = lower_32_bits(op->u.sha.msg_bits);
 +		cr[5] = upper_32_bits(op->u.sha.msg_bits);
 +	} else {
 +		cr[4] = 0;
 +		cr[5] = 0;
 +	}
 +
 +	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
 +}
 +
 +static int ccp_perform_rsa(struct ccp_op *op)
 +{
 +	u32 cr[6];
 +
 +	/* Fill out the register contents for REQ1 through REQ6 */
 +	cr[0] = (CCP_ENGINE_RSA << REQ1_ENGINE_SHIFT)
 +		| (op->u.rsa.mod_size << REQ1_RSA_MOD_SIZE_SHIFT)
 +		| (op->ksb_key << REQ1_KEY_KSB_SHIFT)
 +		| REQ1_EOM;
 +	cr[1] = op->u.rsa.input_len - 1;
 +	cr[2] = ccp_addr_lo(&op->src.u.dma);
 +	cr[3] = (op->ksb_ctx << REQ4_KSB_SHIFT)
 +		| (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->src.u.dma);
 +	cr[4] = ccp_addr_lo(&op->dst.u.dma);
 +	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->dst.u.dma);
 +
 +	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
 +}
 +
 +static int ccp_perform_passthru(struct ccp_op *op)
 +{
 +	u32 cr[6];
 +
 +	/* Fill out the register contents for REQ1 through REQ6 */
 +	cr[0] = (CCP_ENGINE_PASSTHRU << REQ1_ENGINE_SHIFT)
 +		| (op->u.passthru.bit_mod << REQ1_PT_BW_SHIFT)
 +		| (op->u.passthru.byte_swap << REQ1_PT_BS_SHIFT);
 +
 +	if (op->src.type == CCP_MEMTYPE_SYSTEM)
 +		cr[1] = op->src.u.dma.length - 1;
 +	else
 +		cr[1] = op->dst.u.dma.length - 1;
 +
 +	if (op->src.type == CCP_MEMTYPE_SYSTEM) {
 +		cr[2] = ccp_addr_lo(&op->src.u.dma);
 +		cr[3] = (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
 +			| ccp_addr_hi(&op->src.u.dma);
 +
 +		if (op->u.passthru.bit_mod != CCP_PASSTHRU_BITWISE_NOOP)
 +			cr[3] |= (op->ksb_key << REQ4_KSB_SHIFT);
 +	} else {
 +		cr[2] = op->src.u.ksb * CCP_KSB_BYTES;
 +		cr[3] = (CCP_MEMTYPE_KSB << REQ4_MEMTYPE_SHIFT);
 +	}
 +
 +	if (op->dst.type == CCP_MEMTYPE_SYSTEM) {
 +		cr[4] = ccp_addr_lo(&op->dst.u.dma);
 +		cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
 +			| ccp_addr_hi(&op->dst.u.dma);
 +	} else {
 +		cr[4] = op->dst.u.ksb * CCP_KSB_BYTES;
 +		cr[5] = (CCP_MEMTYPE_KSB << REQ6_MEMTYPE_SHIFT);
 +	}
 +
 +	if (op->eom)
 +		cr[0] |= REQ1_EOM;
 +
 +	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
 +}
 +
 +static int ccp_perform_ecc(struct ccp_op *op)
 +{
 +	u32 cr[6];
 +
 +	/* Fill out the register contents for REQ1 through REQ6 */
 +	cr[0] = REQ1_ECC_AFFINE_CONVERT
 +		| (CCP_ENGINE_ECC << REQ1_ENGINE_SHIFT)
 +		| (op->u.ecc.function << REQ1_ECC_FUNCTION_SHIFT)
 +		| REQ1_EOM;
 +	cr[1] = op->src.u.dma.length - 1;
 +	cr[2] = ccp_addr_lo(&op->src.u.dma);
 +	cr[3] = (CCP_MEMTYPE_SYSTEM << REQ4_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->src.u.dma);
 +	cr[4] = ccp_addr_lo(&op->dst.u.dma);
 +	cr[5] = (CCP_MEMTYPE_SYSTEM << REQ6_MEMTYPE_SHIFT)
 +		| ccp_addr_hi(&op->dst.u.dma);
 +
 +	return ccp_do_cmd(op, cr, ARRAY_SIZE(cr));
 +}
 +
 +static u32 ccp_alloc_ksb(struct ccp_device *ccp, unsigned int count)
 +{
 +	int start;
 +
 +	for (;;) {
 +		mutex_lock(&ccp->ksb_mutex);
 +
 +		start = (u32)bitmap_find_next_zero_area(ccp->ksb,
 +							ccp->ksb_count,
 +							ccp->ksb_start,
 +							count, 0);
 +		if (start <= ccp->ksb_count) {
 +			bitmap_set(ccp->ksb, start, count);
 +
 +			mutex_unlock(&ccp->ksb_mutex);
 +			break;
 +		}
 +
 +		ccp->ksb_avail = 0;
 +
 +		mutex_unlock(&ccp->ksb_mutex);
 +
 +		/* Wait for KSB entries to become available */
 +		if (wait_event_interruptible(ccp->ksb_queue, ccp->ksb_avail))
 +			return 0;
 +	}
 +
 +	return KSB_START + start;
 +}
 +
 +static void ccp_free_ksb(struct ccp_device *ccp, unsigned int start,
 +			 unsigned int count)
 +{
 +	if (!start)
 +		return;
 +
 +	mutex_lock(&ccp->ksb_mutex);
 +
 +	bitmap_clear(ccp->ksb, start - KSB_START, count);
 +
 +	ccp->ksb_avail = 1;
 +
 +	mutex_unlock(&ccp->ksb_mutex);
 +
 +	wake_up_interruptible_all(&ccp->ksb_queue);
 +}
 +
++=======
++>>>>>>> 58a690b701ef (crypto: ccp - Refactor the storage block allocation code)
  static u32 ccp_gen_jobid(struct ccp_device *ccp)
  {
  	return atomic_inc_return(&ccp->current_id) & CCP_JOBID_MASK;
@@@ -1486,11 -1167,12 +1489,17 @@@ static int ccp_run_rsa_cmd(struct ccp_c
  	memset(&op, 0, sizeof(op));
  	op.cmd_q = cmd_q;
  	op.jobid = ccp_gen_jobid(cmd_q->ccp);
++<<<<<<< HEAD
 +	op.ksb_key = ccp_alloc_ksb(cmd_q->ccp, ksb_count);
 +	if (!op.ksb_key)
++=======
+ 	op.sb_key = cmd_q->ccp->vdata->perform->sballoc(cmd_q, sb_count);
+ 
+ 	if (!op.sb_key)
++>>>>>>> 58a690b701ef (crypto: ccp - Refactor the storage block allocation code)
  		return -EIO;
  
 -	/* The RSA exponent may span multiple (32-byte) SB entries and must
 +	/* The RSA exponent may span multiple (32-byte) KSB entries and must
  	 * be in little endian format. Reverse copy each 32-byte chunk
  	 * of the exponent (En chunk to E0 chunk, E(n-1) chunk to E1 chunk)
  	 * and each byte within that chunk and do not perform any byte swap
@@@ -1558,8 -1246,8 +1567,13 @@@ e_src
  e_exp:
  	ccp_dm_free(&exp);
  
++<<<<<<< HEAD
 +e_ksb:
 +	ccp_free_ksb(cmd_q->ccp, op.ksb_key, ksb_count);
++=======
+ e_sb:
+ 	cmd_q->ccp->vdata->perform->sbfree(cmd_q, op.sb_key, sb_count);
++>>>>>>> 58a690b701ef (crypto: ccp - Refactor the storage block allocation code)
  
  	return ret;
  }
* Unmerged path drivers/crypto/ccp/ccp-dev-v3.c
* Unmerged path drivers/crypto/ccp/ccp-dev-v3.c
* Unmerged path drivers/crypto/ccp/ccp-dev.h
* Unmerged path drivers/crypto/ccp/ccp-ops.c
