crypto: memneq - fix for archs without efficient unaligned access

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [crypto] memneq - fix for archs without efficient unaligned access (Herbert Xu) [1314179]
Rebuild_FUZZ: 93.44%
commit-author Daniel Borkmann <dborkman@redhat.com>
commit e37b94ebffdd93d121e5308eb93e2e55a0893fbb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/e37b94eb.failed

Commit fe8c8a126806 introduced a possible build error for archs
that do not have CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS set. :/
Fix this up by bringing else braces outside of the ifdef.

	Reported-by: Fengguang Wu <fengguang.wu@intel.com>
Fixes: fe8c8a126806 ("crypto: more robust crypto_memneq")
	Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
Acked-By: Cesar Eduardo Barros <cesarb@cesarb.eti.br>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit e37b94ebffdd93d121e5308eb93e2e55a0893fbb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	crypto/memneq.c
diff --cc crypto/memneq.c
index cd0162221c14,afed1bd16aee..000000000000
--- a/crypto/memneq.c
+++ b/crypto/memneq.c
@@@ -89,33 -91,61 +89,88 @@@ __crypto_memneq_generic(const void *a, 
  /* Loop-free fast-path for frequently used 16-byte size */
  static inline unsigned long __crypto_memneq_16(const void *a, const void *b)
  {
 -	unsigned long neq = 0;
 -
  #ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
++<<<<<<< HEAD
 +	if (sizeof(unsigned long) == 8)
 +		return ((*(unsigned long *)(a)   ^ *(unsigned long *)(b))
 +		      | (*(unsigned long *)(a+8) ^ *(unsigned long *)(b+8)));
 +	else if (sizeof(unsigned int) == 4)
 +		return ((*(unsigned int *)(a)    ^ *(unsigned int *)(b))
 +                      | (*(unsigned int *)(a+4)  ^ *(unsigned int *)(b+4))
 +		      | (*(unsigned int *)(a+8)  ^ *(unsigned int *)(b+8))
 +	              | (*(unsigned int *)(a+12) ^ *(unsigned int *)(b+12)));
 +	else
 +#endif /* CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS */
 +		return ((*(unsigned char *)(a)    ^ *(unsigned char *)(b))
 +		      | (*(unsigned char *)(a+1)  ^ *(unsigned char *)(b+1))
 +		      | (*(unsigned char *)(a+2)  ^ *(unsigned char *)(b+2))
 +		      | (*(unsigned char *)(a+3)  ^ *(unsigned char *)(b+3))
 +		      | (*(unsigned char *)(a+4)  ^ *(unsigned char *)(b+4))
 +		      | (*(unsigned char *)(a+5)  ^ *(unsigned char *)(b+5))
 +		      | (*(unsigned char *)(a+6)  ^ *(unsigned char *)(b+6))
 +		      | (*(unsigned char *)(a+7)  ^ *(unsigned char *)(b+7))
 +		      | (*(unsigned char *)(a+8)  ^ *(unsigned char *)(b+8))
 +		      | (*(unsigned char *)(a+9)  ^ *(unsigned char *)(b+9))
 +		      | (*(unsigned char *)(a+10) ^ *(unsigned char *)(b+10))
 +		      | (*(unsigned char *)(a+11) ^ *(unsigned char *)(b+11))
 +		      | (*(unsigned char *)(a+12) ^ *(unsigned char *)(b+12))
 +		      | (*(unsigned char *)(a+13) ^ *(unsigned char *)(b+13))
 +		      | (*(unsigned char *)(a+14) ^ *(unsigned char *)(b+14))
 +		      | (*(unsigned char *)(a+15) ^ *(unsigned char *)(b+15)));
++=======
+ 	if (sizeof(unsigned long) == 8) {
+ 		neq |= *(unsigned long *)(a)   ^ *(unsigned long *)(b);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned long *)(a+8) ^ *(unsigned long *)(b+8);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 	} else if (sizeof(unsigned int) == 4) {
+ 		neq |= *(unsigned int *)(a)    ^ *(unsigned int *)(b);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned int *)(a+4)  ^ *(unsigned int *)(b+4);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned int *)(a+8)  ^ *(unsigned int *)(b+8);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned int *)(a+12) ^ *(unsigned int *)(b+12);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 	} else
+ #endif /* CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS */
+ 	{
+ 		neq |= *(unsigned char *)(a)    ^ *(unsigned char *)(b);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+1)  ^ *(unsigned char *)(b+1);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+2)  ^ *(unsigned char *)(b+2);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+3)  ^ *(unsigned char *)(b+3);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+4)  ^ *(unsigned char *)(b+4);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+5)  ^ *(unsigned char *)(b+5);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+6)  ^ *(unsigned char *)(b+6);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+7)  ^ *(unsigned char *)(b+7);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+8)  ^ *(unsigned char *)(b+8);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+9)  ^ *(unsigned char *)(b+9);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+10) ^ *(unsigned char *)(b+10);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+11) ^ *(unsigned char *)(b+11);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+12) ^ *(unsigned char *)(b+12);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+13) ^ *(unsigned char *)(b+13);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+14) ^ *(unsigned char *)(b+14);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 		neq |= *(unsigned char *)(a+15) ^ *(unsigned char *)(b+15);
+ 		OPTIMIZER_HIDE_VAR(neq);
+ 	}
+ 
+ 	return neq;
++>>>>>>> e37b94ebffdd (crypto: memneq - fix for archs without efficient unaligned access)
  }
  
  /* Compare two areas of memory without leaking timing information,
* Unmerged path crypto/memneq.c
