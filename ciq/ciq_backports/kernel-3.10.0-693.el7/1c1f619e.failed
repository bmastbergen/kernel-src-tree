amd-xgbe: Fix maximum GPIO value check

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Lendacky, Thomas <Thomas.Lendacky@amd.com>
commit 1c1f619e451bdbf29c3be9ed11c77d488b56dfd9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/1c1f619e.failed

The GPIO support in the hardware allows for up to 16 GPIO pins, enumerated
from 0 to 15.  The driver uses the wrong value (16) to validate the GPIO
pin range in the routines to set and clear the GPIO output pins.  Update
the code to use the correct value (15).

	Reported-by: Colin Ian King <colin.king@canonical.com>
	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1c1f619e451bdbf29c3be9ed11c77d488b56dfd9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amd/xgbe/xgbe-dev.c
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-dev.c
index de7b81d8b4ee,aaf0350076a9..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
@@@ -765,6 -837,530 +765,533 @@@ static int xgbe_disable_rx_vlan_strippi
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int xgbe_enable_rx_vlan_filtering(struct xgbe_prv_data *pdata)
+ {
+ 	/* Enable VLAN filtering */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, VTFE, 1);
+ 
+ 	/* Enable VLAN Hash Table filtering */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, VTHM, 1);
+ 
+ 	/* Disable VLAN tag inverse matching */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, VTIM, 0);
+ 
+ 	/* Only filter on the lower 12-bits of the VLAN tag */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, ETV, 1);
+ 
+ 	/* In order for the VLAN Hash Table filtering to be effective,
+ 	 * the VLAN tag identifier in the VLAN Tag Register must not
+ 	 * be zero.  Set the VLAN tag identifier to "1" to enable the
+ 	 * VLAN Hash Table filtering.  This implies that a VLAN tag of
+ 	 * 1 will always pass filtering.
+ 	 */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANTR, VL, 1);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_disable_rx_vlan_filtering(struct xgbe_prv_data *pdata)
+ {
+ 	/* Disable VLAN filtering */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, VTFE, 0);
+ 
+ 	return 0;
+ }
+ 
+ static u32 xgbe_vid_crc32_le(__le16 vid_le)
+ {
+ 	u32 poly = 0xedb88320;	/* CRCPOLY_LE */
+ 	u32 crc = ~0;
+ 	u32 temp = 0;
+ 	unsigned char *data = (unsigned char *)&vid_le;
+ 	unsigned char data_byte = 0;
+ 	int i, bits;
+ 
+ 	bits = get_bitmask_order(VLAN_VID_MASK);
+ 	for (i = 0; i < bits; i++) {
+ 		if ((i % 8) == 0)
+ 			data_byte = data[i / 8];
+ 
+ 		temp = ((crc & 1) ^ data_byte) & 1;
+ 		crc >>= 1;
+ 		data_byte >>= 1;
+ 
+ 		if (temp)
+ 			crc ^= poly;
+ 	}
+ 
+ 	return crc;
+ }
+ 
+ static int xgbe_update_vlan_hash_table(struct xgbe_prv_data *pdata)
+ {
+ 	u32 crc;
+ 	u16 vid;
+ 	__le16 vid_le;
+ 	u16 vlan_hash_table = 0;
+ 
+ 	/* Generate the VLAN Hash Table value */
+ 	for_each_set_bit(vid, pdata->active_vlans, VLAN_N_VID) {
+ 		/* Get the CRC32 value of the VLAN ID */
+ 		vid_le = cpu_to_le16(vid);
+ 		crc = bitrev32(~xgbe_vid_crc32_le(vid_le)) >> 28;
+ 
+ 		vlan_hash_table |= (1 << crc);
+ 	}
+ 
+ 	/* Set the VLAN Hash Table filtering register */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_VLANHTR, VLHT, vlan_hash_table);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_promiscuous_mode(struct xgbe_prv_data *pdata,
+ 				     unsigned int enable)
+ {
+ 	unsigned int val = enable ? 1 : 0;
+ 
+ 	if (XGMAC_IOREAD_BITS(pdata, MAC_PFR, PR) == val)
+ 		return 0;
+ 
+ 	netif_dbg(pdata, drv, pdata->netdev, "%s promiscuous mode\n",
+ 		  enable ? "entering" : "leaving");
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, PR, val);
+ 
+ 	/* Hardware will still perform VLAN filtering in promiscuous mode */
+ 	if (enable) {
+ 		xgbe_disable_rx_vlan_filtering(pdata);
+ 	} else {
+ 		if (pdata->netdev->features & NETIF_F_HW_VLAN_CTAG_FILTER)
+ 			xgbe_enable_rx_vlan_filtering(pdata);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_all_multicast_mode(struct xgbe_prv_data *pdata,
+ 				       unsigned int enable)
+ {
+ 	unsigned int val = enable ? 1 : 0;
+ 
+ 	if (XGMAC_IOREAD_BITS(pdata, MAC_PFR, PM) == val)
+ 		return 0;
+ 
+ 	netif_dbg(pdata, drv, pdata->netdev, "%s allmulti mode\n",
+ 		  enable ? "entering" : "leaving");
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_PFR, PM, val);
+ 
+ 	return 0;
+ }
+ 
+ static void xgbe_set_mac_reg(struct xgbe_prv_data *pdata,
+ 			     struct netdev_hw_addr *ha, unsigned int *mac_reg)
+ {
+ 	unsigned int mac_addr_hi, mac_addr_lo;
+ 	u8 *mac_addr;
+ 
+ 	mac_addr_lo = 0;
+ 	mac_addr_hi = 0;
+ 
+ 	if (ha) {
+ 		mac_addr = (u8 *)&mac_addr_lo;
+ 		mac_addr[0] = ha->addr[0];
+ 		mac_addr[1] = ha->addr[1];
+ 		mac_addr[2] = ha->addr[2];
+ 		mac_addr[3] = ha->addr[3];
+ 		mac_addr = (u8 *)&mac_addr_hi;
+ 		mac_addr[0] = ha->addr[4];
+ 		mac_addr[1] = ha->addr[5];
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev,
+ 			  "adding mac address %pM at %#x\n",
+ 			  ha->addr, *mac_reg);
+ 
+ 		XGMAC_SET_BITS(mac_addr_hi, MAC_MACA1HR, AE, 1);
+ 	}
+ 
+ 	XGMAC_IOWRITE(pdata, *mac_reg, mac_addr_hi);
+ 	*mac_reg += MAC_MACA_INC;
+ 	XGMAC_IOWRITE(pdata, *mac_reg, mac_addr_lo);
+ 	*mac_reg += MAC_MACA_INC;
+ }
+ 
+ static void xgbe_set_mac_addn_addrs(struct xgbe_prv_data *pdata)
+ {
+ 	struct net_device *netdev = pdata->netdev;
+ 	struct netdev_hw_addr *ha;
+ 	unsigned int mac_reg;
+ 	unsigned int addn_macs;
+ 
+ 	mac_reg = MAC_MACA1HR;
+ 	addn_macs = pdata->hw_feat.addn_mac;
+ 
+ 	if (netdev_uc_count(netdev) > addn_macs) {
+ 		xgbe_set_promiscuous_mode(pdata, 1);
+ 	} else {
+ 		netdev_for_each_uc_addr(ha, netdev) {
+ 			xgbe_set_mac_reg(pdata, ha, &mac_reg);
+ 			addn_macs--;
+ 		}
+ 
+ 		if (netdev_mc_count(netdev) > addn_macs) {
+ 			xgbe_set_all_multicast_mode(pdata, 1);
+ 		} else {
+ 			netdev_for_each_mc_addr(ha, netdev) {
+ 				xgbe_set_mac_reg(pdata, ha, &mac_reg);
+ 				addn_macs--;
+ 			}
+ 		}
+ 	}
+ 
+ 	/* Clear remaining additional MAC address entries */
+ 	while (addn_macs--)
+ 		xgbe_set_mac_reg(pdata, NULL, &mac_reg);
+ }
+ 
+ static void xgbe_set_mac_hash_table(struct xgbe_prv_data *pdata)
+ {
+ 	struct net_device *netdev = pdata->netdev;
+ 	struct netdev_hw_addr *ha;
+ 	unsigned int hash_reg;
+ 	unsigned int hash_table_shift, hash_table_count;
+ 	u32 hash_table[XGBE_MAC_HASH_TABLE_SIZE];
+ 	u32 crc;
+ 	unsigned int i;
+ 
+ 	hash_table_shift = 26 - (pdata->hw_feat.hash_table_size >> 7);
+ 	hash_table_count = pdata->hw_feat.hash_table_size / 32;
+ 	memset(hash_table, 0, sizeof(hash_table));
+ 
+ 	/* Build the MAC Hash Table register values */
+ 	netdev_for_each_uc_addr(ha, netdev) {
+ 		crc = bitrev32(~crc32_le(~0, ha->addr, ETH_ALEN));
+ 		crc >>= hash_table_shift;
+ 		hash_table[crc >> 5] |= (1 << (crc & 0x1f));
+ 	}
+ 
+ 	netdev_for_each_mc_addr(ha, netdev) {
+ 		crc = bitrev32(~crc32_le(~0, ha->addr, ETH_ALEN));
+ 		crc >>= hash_table_shift;
+ 		hash_table[crc >> 5] |= (1 << (crc & 0x1f));
+ 	}
+ 
+ 	/* Set the MAC Hash Table registers */
+ 	hash_reg = MAC_HTR0;
+ 	for (i = 0; i < hash_table_count; i++) {
+ 		XGMAC_IOWRITE(pdata, hash_reg, hash_table[i]);
+ 		hash_reg += MAC_HTR_INC;
+ 	}
+ }
+ 
+ static int xgbe_add_mac_addresses(struct xgbe_prv_data *pdata)
+ {
+ 	if (pdata->hw_feat.hash_table_size)
+ 		xgbe_set_mac_hash_table(pdata);
+ 	else
+ 		xgbe_set_mac_addn_addrs(pdata);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_mac_address(struct xgbe_prv_data *pdata, u8 *addr)
+ {
+ 	unsigned int mac_addr_hi, mac_addr_lo;
+ 
+ 	mac_addr_hi = (addr[5] <<  8) | (addr[4] <<  0);
+ 	mac_addr_lo = (addr[3] << 24) | (addr[2] << 16) |
+ 		      (addr[1] <<  8) | (addr[0] <<  0);
+ 
+ 	XGMAC_IOWRITE(pdata, MAC_MACA0HR, mac_addr_hi);
+ 	XGMAC_IOWRITE(pdata, MAC_MACA0LR, mac_addr_lo);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_config_rx_mode(struct xgbe_prv_data *pdata)
+ {
+ 	struct net_device *netdev = pdata->netdev;
+ 	unsigned int pr_mode, am_mode;
+ 
+ 	pr_mode = ((netdev->flags & IFF_PROMISC) != 0);
+ 	am_mode = ((netdev->flags & IFF_ALLMULTI) != 0);
+ 
+ 	xgbe_set_promiscuous_mode(pdata, pr_mode);
+ 	xgbe_set_all_multicast_mode(pdata, am_mode);
+ 
+ 	xgbe_add_mac_addresses(pdata);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_clr_gpio(struct xgbe_prv_data *pdata, unsigned int gpio)
+ {
+ 	unsigned int reg;
+ 
+ 	if (gpio > 15)
+ 		return -EINVAL;
+ 
+ 	reg = XGMAC_IOREAD(pdata, MAC_GPIOSR);
+ 
+ 	reg &= ~(1 << (gpio + 16));
+ 	XGMAC_IOWRITE(pdata, MAC_GPIOSR, reg);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_set_gpio(struct xgbe_prv_data *pdata, unsigned int gpio)
+ {
+ 	unsigned int reg;
+ 
+ 	if (gpio > 15)
+ 		return -EINVAL;
+ 
+ 	reg = XGMAC_IOREAD(pdata, MAC_GPIOSR);
+ 
+ 	reg |= (1 << (gpio + 16));
+ 	XGMAC_IOWRITE(pdata, MAC_GPIOSR, reg);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_read_mmd_regs_v2(struct xgbe_prv_data *pdata, int prtad,
+ 				 int mmd_reg)
+ {
+ 	unsigned long flags;
+ 	unsigned int mmd_address, index, offset;
+ 	int mmd_data;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 16-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 1 bit and reading 16 bits of data.
+ 	 */
+ 	mmd_address <<= 1;
+ 	index = mmd_address & ~pdata->xpcs_window_mask;
+ 	offset = pdata->xpcs_window + (mmd_address & pdata->xpcs_window_mask);
+ 
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V2_WINDOW_SELECT, index);
+ 	mmd_data = XPCS16_IOREAD(pdata, offset);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ 
+ 	return mmd_data;
+ }
+ 
+ static void xgbe_write_mmd_regs_v2(struct xgbe_prv_data *pdata, int prtad,
+ 				   int mmd_reg, int mmd_data)
+ {
+ 	unsigned long flags;
+ 	unsigned int mmd_address, index, offset;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 16-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 1 bit and writing 16 bits of data.
+ 	 */
+ 	mmd_address <<= 1;
+ 	index = mmd_address & ~pdata->xpcs_window_mask;
+ 	offset = pdata->xpcs_window + (mmd_address & pdata->xpcs_window_mask);
+ 
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V2_WINDOW_SELECT, index);
+ 	XPCS16_IOWRITE(pdata, offset, mmd_data);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ }
+ 
+ static int xgbe_read_mmd_regs_v1(struct xgbe_prv_data *pdata, int prtad,
+ 				 int mmd_reg)
+ {
+ 	unsigned long flags;
+ 	unsigned int mmd_address;
+ 	int mmd_data;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying APB3
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 32-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 2 bits and reading 32 bits of data.
+ 	 */
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V1_WINDOW_SELECT, mmd_address >> 8);
+ 	mmd_data = XPCS32_IOREAD(pdata, (mmd_address & 0xff) << 2);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ 
+ 	return mmd_data;
+ }
+ 
+ static void xgbe_write_mmd_regs_v1(struct xgbe_prv_data *pdata, int prtad,
+ 				   int mmd_reg, int mmd_data)
+ {
+ 	unsigned int mmd_address;
+ 	unsigned long flags;
+ 
+ 	if (mmd_reg & MII_ADDR_C45)
+ 		mmd_address = mmd_reg & ~MII_ADDR_C45;
+ 	else
+ 		mmd_address = (pdata->mdio_mmd << 16) | (mmd_reg & 0xffff);
+ 
+ 	/* The PCS registers are accessed using mmio. The underlying APB3
+ 	 * management interface uses indirect addressing to access the MMD
+ 	 * register sets. This requires accessing of the PCS register in two
+ 	 * phases, an address phase and a data phase.
+ 	 *
+ 	 * The mmio interface is based on 32-bit offsets and values. All
+ 	 * register offsets must therefore be adjusted by left shifting the
+ 	 * offset 2 bits and writing 32 bits of data.
+ 	 */
+ 	spin_lock_irqsave(&pdata->xpcs_lock, flags);
+ 	XPCS32_IOWRITE(pdata, PCS_V1_WINDOW_SELECT, mmd_address >> 8);
+ 	XPCS32_IOWRITE(pdata, (mmd_address & 0xff) << 2, mmd_data);
+ 	spin_unlock_irqrestore(&pdata->xpcs_lock, flags);
+ }
+ 
+ static int xgbe_read_mmd_regs(struct xgbe_prv_data *pdata, int prtad,
+ 			      int mmd_reg)
+ {
+ 	switch (pdata->vdata->xpcs_access) {
+ 	case XGBE_XPCS_ACCESS_V1:
+ 		return xgbe_read_mmd_regs_v1(pdata, prtad, mmd_reg);
+ 
+ 	case XGBE_XPCS_ACCESS_V2:
+ 	default:
+ 		return xgbe_read_mmd_regs_v2(pdata, prtad, mmd_reg);
+ 	}
+ }
+ 
+ static void xgbe_write_mmd_regs(struct xgbe_prv_data *pdata, int prtad,
+ 				int mmd_reg, int mmd_data)
+ {
+ 	switch (pdata->vdata->xpcs_access) {
+ 	case XGBE_XPCS_ACCESS_V1:
+ 		return xgbe_write_mmd_regs_v1(pdata, prtad, mmd_reg, mmd_data);
+ 
+ 	case XGBE_XPCS_ACCESS_V2:
+ 	default:
+ 		return xgbe_write_mmd_regs_v2(pdata, prtad, mmd_reg, mmd_data);
+ 	}
+ }
+ 
+ static int xgbe_write_ext_mii_regs(struct xgbe_prv_data *pdata, int addr,
+ 				   int reg, u16 val)
+ {
+ 	unsigned int mdio_sca, mdio_sccd;
+ 
+ 	reinit_completion(&pdata->mdio_complete);
+ 
+ 	mdio_sca = 0;
+ 	XGMAC_SET_BITS(mdio_sca, MAC_MDIOSCAR, REG, reg);
+ 	XGMAC_SET_BITS(mdio_sca, MAC_MDIOSCAR, DA, addr);
+ 	XGMAC_IOWRITE(pdata, MAC_MDIOSCAR, mdio_sca);
+ 
+ 	mdio_sccd = 0;
+ 	XGMAC_SET_BITS(mdio_sccd, MAC_MDIOSCCDR, DATA, val);
+ 	XGMAC_SET_BITS(mdio_sccd, MAC_MDIOSCCDR, CMD, 1);
+ 	XGMAC_SET_BITS(mdio_sccd, MAC_MDIOSCCDR, BUSY, 1);
+ 	XGMAC_IOWRITE(pdata, MAC_MDIOSCCDR, mdio_sccd);
+ 
+ 	if (!wait_for_completion_timeout(&pdata->mdio_complete, HZ)) {
+ 		netdev_err(pdata->netdev, "mdio write operation timed out\n");
+ 		return -ETIMEDOUT;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_read_ext_mii_regs(struct xgbe_prv_data *pdata, int addr,
+ 				  int reg)
+ {
+ 	unsigned int mdio_sca, mdio_sccd;
+ 
+ 	reinit_completion(&pdata->mdio_complete);
+ 
+ 	mdio_sca = 0;
+ 	XGMAC_SET_BITS(mdio_sca, MAC_MDIOSCAR, REG, reg);
+ 	XGMAC_SET_BITS(mdio_sca, MAC_MDIOSCAR, DA, addr);
+ 	XGMAC_IOWRITE(pdata, MAC_MDIOSCAR, mdio_sca);
+ 
+ 	mdio_sccd = 0;
+ 	XGMAC_SET_BITS(mdio_sccd, MAC_MDIOSCCDR, CMD, 3);
+ 	XGMAC_SET_BITS(mdio_sccd, MAC_MDIOSCCDR, BUSY, 1);
+ 	XGMAC_IOWRITE(pdata, MAC_MDIOSCCDR, mdio_sccd);
+ 
+ 	if (!wait_for_completion_timeout(&pdata->mdio_complete, HZ)) {
+ 		netdev_err(pdata->netdev, "mdio read operation timed out\n");
+ 		return -ETIMEDOUT;
+ 	}
+ 
+ 	return XGMAC_IOREAD_BITS(pdata, MAC_MDIOSCCDR, DATA);
+ }
+ 
+ static int xgbe_set_ext_mii_mode(struct xgbe_prv_data *pdata, unsigned int port,
+ 				 enum xgbe_mdio_mode mode)
+ {
+ 	unsigned int reg_val = 0;
+ 
+ 	switch (mode) {
+ 	case XGBE_MDIO_MODE_CL22:
+ 		if (port > XGMAC_MAX_C22_PORT)
+ 			return -EINVAL;
+ 		reg_val |= (1 << port);
+ 		break;
+ 	case XGBE_MDIO_MODE_CL45:
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	XGMAC_IOWRITE(pdata, MAC_MDIOCL22R, reg_val);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_tx_complete(struct xgbe_ring_desc *rdesc)
+ {
+ 	return !XGMAC_GET_BITS_LE(rdesc->desc3, TX_NORMAL_DESC3, OWN);
+ }
+ 
+ static int xgbe_disable_rx_csum(struct xgbe_prv_data *pdata)
+ {
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RCR, IPC, 0);
+ 
+ 	return 0;
+ }
+ 
+ static int xgbe_enable_rx_csum(struct xgbe_prv_data *pdata)
+ {
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_RCR, IPC, 1);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 1c1f619e451b (amd-xgbe: Fix maximum GPIO value check)
  static void xgbe_tx_desc_reset(struct xgbe_ring_data *rdata)
  {
  	struct xgbe_ring_desc *rdesc = rdata->rdesc;
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dev.c
