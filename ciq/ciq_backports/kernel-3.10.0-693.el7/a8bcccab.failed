locking/atomic, arch/x86: Implement atomic{,64}_fetch_{add,sub,and,or,xor}()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [x86] locking/atomic, arch/x86: Implement atomic{, 64}_fetch_{add, sub, and, or, xor}() (Rob Clark) [1406119]
Rebuild_FUZZ: 96.82%
commit-author Peter Zijlstra <peterz@infradead.org>
commit a8bcccaba162632c3963259b8a442c6b490f4c68
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/a8bcccab.failed

Implement FETCH-OP atomic primitives, these are very similar to the
existing OP-RETURN primitives we already have, except they return the
value of the atomic variable _before_ modification.

This is especially useful for irreversible operations -- such as
bitops (because it becomes impossible to reconstruct the state prior
to modification).

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: linux-arch@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a8bcccaba162632c3963259b8a442c6b490f4c68)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/atomic.h
#	arch/x86/include/asm/atomic64_32.h
#	arch/x86/include/asm/atomic64_64.h
diff --cc arch/x86/include/asm/atomic.h
index c5b5977d4144,73b8463b89e9..000000000000
--- a/arch/x86/include/asm/atomic.h
+++ b/arch/x86/include/asm/atomic.h
@@@ -201,6 -191,42 +211,45 @@@ static inline int atomic_xchg(atomic_t 
  	return xchg(&v->counter, new);
  }
  
++<<<<<<< HEAD
++=======
+ #define ATOMIC_OP(op)							\
+ static inline void atomic_##op(int i, atomic_t *v)			\
+ {									\
+ 	asm volatile(LOCK_PREFIX #op"l %1,%0"				\
+ 			: "+m" (v->counter)				\
+ 			: "ir" (i)					\
+ 			: "memory");					\
+ }
+ 
+ #define ATOMIC_FETCH_OP(op, c_op)					\
+ static inline int atomic_fetch_##op(int i, atomic_t *v)		\
+ {									\
+ 	int old, val = atomic_read(v);					\
+ 	for (;;) {							\
+ 		old = atomic_cmpxchg(v, val, val c_op i);		\
+ 		if (old == val)						\
+ 			break;						\
+ 		val = old;						\
+ 	}								\
+ 	return old;							\
+ }
+ 
+ #define ATOMIC_OPS(op, c_op)						\
+ 	ATOMIC_OP(op)							\
+ 	ATOMIC_FETCH_OP(op, c_op)
+ 
+ #define atomic_fetch_or atomic_fetch_or
+ 
+ ATOMIC_OPS(and, &)
+ ATOMIC_OPS(or , |)
+ ATOMIC_OPS(xor, ^)
+ 
+ #undef ATOMIC_OPS
+ #undef ATOMIC_FETCH_OP
+ #undef ATOMIC_OP
+ 
++>>>>>>> a8bcccaba162 (locking/atomic, arch/x86: Implement atomic{,64}_fetch_{add,sub,and,or,xor}())
  /**
   * __atomic_add_unless - add unless the number is already a given value
   * @v: pointer of type atomic_t
diff --cc arch/x86/include/asm/atomic64_32.h
index b154de75c90c,71d7705fb303..000000000000
--- a/arch/x86/include/asm/atomic64_32.h
+++ b/arch/x86/include/asm/atomic64_32.h
@@@ -313,4 -312,37 +313,40 @@@ static inline long long atomic64_dec_if
  #undef alternative_atomic64
  #undef __alternative_atomic64
  
++<<<<<<< HEAD
++=======
+ #define ATOMIC64_OP(op, c_op)						\
+ static inline void atomic64_##op(long long i, atomic64_t *v)		\
+ {									\
+ 	long long old, c = 0;						\
+ 	while ((old = atomic64_cmpxchg(v, c, c c_op i)) != c)		\
+ 		c = old;						\
+ }
+ 
+ #define ATOMIC64_FETCH_OP(op, c_op)					\
+ static inline long long atomic64_fetch_##op(long long i, atomic64_t *v)	\
+ {									\
+ 	long long old, c = 0;						\
+ 	while ((old = atomic64_cmpxchg(v, c, c c_op i)) != c)		\
+ 		c = old;						\
+ 	return old;							\
+ }
+ 
+ ATOMIC64_FETCH_OP(add, +)
+ 
+ #define atomic64_fetch_sub(i, v)	atomic64_fetch_add(-(i), (v))
+ 
+ #define ATOMIC64_OPS(op, c_op)						\
+ 	ATOMIC64_OP(op, c_op)						\
+ 	ATOMIC64_FETCH_OP(op, c_op)
+ 
+ ATOMIC64_OPS(and, &)
+ ATOMIC64_OPS(or, |)
+ ATOMIC64_OPS(xor, ^)
+ 
+ #undef ATOMIC64_OPS
+ #undef ATOMIC64_FETCH_OP
+ #undef ATOMIC64_OP
+ 
++>>>>>>> a8bcccaba162 (locking/atomic, arch/x86: Implement atomic{,64}_fetch_{add,sub,and,or,xor}())
  #endif /* _ASM_X86_ATOMIC64_32_H */
diff --cc arch/x86/include/asm/atomic64_64.h
index 63d4f40edba8,70eed0e14553..000000000000
--- a/arch/x86/include/asm/atomic64_64.h
+++ b/arch/x86/include/asm/atomic64_64.h
@@@ -240,4 -230,38 +250,41 @@@ static inline long atomic64_dec_if_posi
  	return dec;
  }
  
++<<<<<<< HEAD
++=======
+ #define ATOMIC64_OP(op)							\
+ static inline void atomic64_##op(long i, atomic64_t *v)			\
+ {									\
+ 	asm volatile(LOCK_PREFIX #op"q %1,%0"				\
+ 			: "+m" (v->counter)				\
+ 			: "er" (i)					\
+ 			: "memory");					\
+ }
+ 
+ #define ATOMIC64_FETCH_OP(op, c_op)					\
+ static inline long atomic64_fetch_##op(long i, atomic64_t *v)		\
+ {									\
+ 	long old, val = atomic64_read(v);				\
+ 	for (;;) {							\
+ 		old = atomic64_cmpxchg(v, val, val c_op i);		\
+ 		if (old == val)						\
+ 			break;						\
+ 		val = old;						\
+ 	}								\
+ 	return old;							\
+ }
+ 
+ #define ATOMIC64_OPS(op, c_op)						\
+ 	ATOMIC64_OP(op)							\
+ 	ATOMIC64_FETCH_OP(op, c_op)
+ 
+ ATOMIC64_OPS(and, &)
+ ATOMIC64_OPS(or, |)
+ ATOMIC64_OPS(xor, ^)
+ 
+ #undef ATOMIC64_OPS
+ #undef ATOMIC64_FETCH_OP
+ #undef ATOMIC64_OP
+ 
++>>>>>>> a8bcccaba162 (locking/atomic, arch/x86: Implement atomic{,64}_fetch_{add,sub,and,or,xor}())
  #endif /* _ASM_X86_ATOMIC64_64_H */
* Unmerged path arch/x86/include/asm/atomic.h
* Unmerged path arch/x86/include/asm/atomic64_32.h
* Unmerged path arch/x86/include/asm/atomic64_64.h
