mnt: Honor MNT_LOCKED when detaching mounts

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Eric W. Biederman <ebiederm@xmission.com>
commit ce07d891a0891d3c0d0c2d73d577490486b809e1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/ce07d891.failed

Modify umount(MNT_DETACH) to keep mounts in the hash table that are
locked to their parent mounts, when the parent is lazily unmounted.

In mntput_no_expire detach the children from the hash table, depending
on mnt_pin_kill in cleanup_mnt to decrement the mnt_count of the children.

In __detach_mounts if there are any mounts that have been unmounted
but still are on the list of mounts of a mountpoint, remove their
children from the mount hash table and those children to the unmounted
list so they won't linger potentially indefinitely waiting for their
final mntput, now that the mounts serve no purpose.

	Cc: stable@vger.kernel.org
	Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
(cherry picked from commit ce07d891a0891d3c0d0c2d73d577490486b809e1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/namespace.c
#	fs/pnode.h
diff --cc fs/namespace.c
index 91fbddebae10,1894d1878dbc..000000000000
--- a/fs/namespace.c
+++ b/fs/namespace.c
@@@ -966,7 -1052,74 +966,76 @@@ put_again
  	fsnotify_vfsmount_delete(&mnt->mnt);
  	dput(mnt->mnt.mnt_root);
  	deactivate_super(mnt->mnt.mnt_sb);
++<<<<<<< HEAD
 +	free_vfsmnt(mnt);
++=======
+ 	mnt_free_id(mnt);
+ 	call_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);
+ }
+ 
+ static void __cleanup_mnt(struct rcu_head *head)
+ {
+ 	cleanup_mnt(container_of(head, struct mount, mnt_rcu));
+ }
+ 
+ static LLIST_HEAD(delayed_mntput_list);
+ static void delayed_mntput(struct work_struct *unused)
+ {
+ 	struct llist_node *node = llist_del_all(&delayed_mntput_list);
+ 	struct llist_node *next;
+ 
+ 	for (; node; node = next) {
+ 		next = llist_next(node);
+ 		cleanup_mnt(llist_entry(node, struct mount, mnt_llist));
+ 	}
+ }
+ static DECLARE_DELAYED_WORK(delayed_mntput_work, delayed_mntput);
+ 
+ static void mntput_no_expire(struct mount *mnt)
+ {
+ 	rcu_read_lock();
+ 	mnt_add_count(mnt, -1);
+ 	if (likely(mnt->mnt_ns)) { /* shouldn't be the last one */
+ 		rcu_read_unlock();
+ 		return;
+ 	}
+ 	lock_mount_hash();
+ 	if (mnt_get_count(mnt)) {
+ 		rcu_read_unlock();
+ 		unlock_mount_hash();
+ 		return;
+ 	}
+ 	if (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {
+ 		rcu_read_unlock();
+ 		unlock_mount_hash();
+ 		return;
+ 	}
+ 	mnt->mnt.mnt_flags |= MNT_DOOMED;
+ 	rcu_read_unlock();
+ 
+ 	list_del(&mnt->mnt_instance);
+ 
+ 	if (unlikely(!list_empty(&mnt->mnt_mounts))) {
+ 		struct mount *p, *tmp;
+ 		list_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {
+ 			umount_mnt(p);
+ 		}
+ 	}
+ 	unlock_mount_hash();
+ 
+ 	if (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {
+ 		struct task_struct *task = current;
+ 		if (likely(!(task->flags & PF_KTHREAD))) {
+ 			init_task_work(&mnt->mnt_rcu, __cleanup_mnt);
+ 			if (!task_work_add(task, &mnt->mnt_rcu, true))
+ 				return;
+ 		}
+ 		if (llist_add(&mnt->mnt_llist, &delayed_mntput_list))
+ 			schedule_delayed_work(&delayed_mntput_work, 1);
+ 		return;
+ 	}
+ 	cleanup_mnt(mnt);
++>>>>>>> ce07d891a089 (mnt: Honor MNT_LOCKED when detaching mounts)
  }
  
  void mntput(struct vfsmount *mnt)
@@@ -1217,26 -1358,46 +1286,50 @@@ void umount_tree(struct mount *mnt, in
  	LIST_HEAD(tmp_list);
  	struct mount *p;
  
 -	if (how & UMOUNT_PROPAGATE)
 -		propagate_mount_unlock(mnt);
 -
 -	/* Gather the mounts to umount */
 -	for (p = mnt; p; p = next_mnt(p, mnt)) {
 -		p->mnt.mnt_flags |= MNT_UMOUNT;
 -		list_move(&p->mnt_list, &tmp_list);
 -	}
 -
 -	/* Hide the mounts from mnt_mounts */
 -	list_for_each_entry(p, &tmp_list, mnt_list) {
 -		list_del_init(&p->mnt_child);
 -	}
 +	for (p = mnt; p; p = next_mnt(p, mnt))
 +		list_move(&p->mnt_hash, &tmp_list);
  
 -	/* Add propogated mounts to the tmp_list */
 -	if (how & UMOUNT_PROPAGATE)
 +	if (propagate)
  		propagate_umount(&tmp_list);
  
++<<<<<<< HEAD
 +	list_for_each_entry(p, &tmp_list, mnt_hash) {
++=======
+ 	while (!list_empty(&tmp_list)) {
+ 		bool disconnect;
+ 		p = list_first_entry(&tmp_list, struct mount, mnt_list);
++>>>>>>> ce07d891a089 (mnt: Honor MNT_LOCKED when detaching mounts)
  		list_del_init(&p->mnt_expire);
  		list_del_init(&p->mnt_list);
  		__touch_mnt_namespace(p->mnt_ns);
  		p->mnt_ns = NULL;
++<<<<<<< HEAD
 +		list_del_init(&p->mnt_child);
 +		if (mnt_has_parent(p)) {
 +			put_mountpoint(p->mnt_mp);
 +			/* move the reference to mountpoint into ->mnt_ex_mountpoint */
 +			p->mnt_ex_mountpoint.dentry = p->mnt_mountpoint;
 +			p->mnt_ex_mountpoint.mnt = &p->mnt_parent->mnt;
 +			p->mnt_mountpoint = p->mnt.mnt_root;
 +			p->mnt_parent = p;
 +			p->mnt_mp = NULL;
++=======
+ 		if (how & UMOUNT_SYNC)
+ 			p->mnt.mnt_flags |= MNT_SYNC_UMOUNT;
+ 
+ 		disconnect = !IS_MNT_LOCKED_AND_LAZY(p);
+ 
+ 		pin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,
+ 				 disconnect ? &unmounted : NULL);
+ 		if (mnt_has_parent(p)) {
+ 			mnt_add_count(p->mnt_parent, -1);
+ 			if (!disconnect) {
+ 				/* Don't forget about p */
+ 				list_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);
+ 			} else {
+ 				umount_mnt(p);
+ 			}
++>>>>>>> ce07d891a089 (mnt: Honor MNT_LOCKED when detaching mounts)
  		}
  		change_mnt_propagation(p, MS_PRIVATE);
  	}
@@@ -1335,6 -1499,44 +1428,47 @@@ static int do_umount(struct mount *mnt
  	return retval;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * __detach_mounts - lazily unmount all mounts on the specified dentry
+  *
+  * During unlink, rmdir, and d_drop it is possible to loose the path
+  * to an existing mountpoint, and wind up leaking the mount.
+  * detach_mounts allows lazily unmounting those mounts instead of
+  * leaking them.
+  *
+  * The caller may hold dentry->d_inode->i_mutex.
+  */
+ void __detach_mounts(struct dentry *dentry)
+ {
+ 	struct mountpoint *mp;
+ 	struct mount *mnt;
+ 
+ 	namespace_lock();
+ 	mp = lookup_mountpoint(dentry);
+ 	if (!mp)
+ 		goto out_unlock;
+ 
+ 	lock_mount_hash();
+ 	while (!hlist_empty(&mp->m_list)) {
+ 		mnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);
+ 		if (mnt->mnt.mnt_flags & MNT_UMOUNT) {
+ 			struct mount *p, *tmp;
+ 			list_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {
+ 				hlist_add_head(&p->mnt_umount.s_list, &unmounted);
+ 				umount_mnt(p);
+ 			}
+ 		}
+ 		else umount_tree(mnt, 0);
+ 	}
+ 	unlock_mount_hash();
+ 	put_mountpoint(mp);
+ out_unlock:
+ 	namespace_unlock();
+ }
+ 
++>>>>>>> ce07d891a089 (mnt: Honor MNT_LOCKED when detaching mounts)
  /* 
   * Is the caller allowed to modify his namespace?
   */
diff --cc fs/pnode.h
index 65e04f65fa1a,7114ce6e6b9e..000000000000
--- a/fs/pnode.h
+++ b/fs/pnode.h
@@@ -19,6 -19,9 +19,12 @@@
  #define IS_MNT_MARKED(m) ((m)->mnt.mnt_flags & MNT_MARKED)
  #define SET_MNT_MARK(m) ((m)->mnt.mnt_flags |= MNT_MARKED)
  #define CLEAR_MNT_MARK(m) ((m)->mnt.mnt_flags &= ~MNT_MARKED)
++<<<<<<< HEAD
++=======
+ #define IS_MNT_LOCKED(m) ((m)->mnt.mnt_flags & MNT_LOCKED)
+ #define IS_MNT_LOCKED_AND_LAZY(m) \
+ 	(((m)->mnt.mnt_flags & (MNT_LOCKED|MNT_SYNC_UMOUNT)) == MNT_LOCKED)
++>>>>>>> ce07d891a089 (mnt: Honor MNT_LOCKED when detaching mounts)
  
  #define CL_EXPIRE    		0x01
  #define CL_SLAVE     		0x02
* Unmerged path fs/namespace.c
* Unmerged path fs/pnode.h
