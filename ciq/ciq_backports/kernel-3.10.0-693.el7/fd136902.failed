KVM: x86: MMU: Move mapping_level_dirty_bitmap() call in mapping_level()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
commit fd136902187838bcae3a572f41cb703553dd63b8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/fd136902.failed

This is necessary to eliminate an extra memory slot search later.

	Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit fd136902187838bcae3a572f41cb703553dd63b8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/paging_tmpl.h
diff --cc arch/x86/kvm/mmu.c
index 70c3ecb68058,890cd694c9a2..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -3054,8 -2968,8 +3060,13 @@@ static int nonpaging_map(struct kvm_vcp
  {
  	int r;
  	int level;
++<<<<<<< HEAD
 +	int force_pt_level;
 +	kvm_pfn_t pfn;
++=======
+ 	bool force_pt_level = false;
+ 	pfn_t pfn;
++>>>>>>> fd1369021878 (KVM: x86: MMU: Move mapping_level_dirty_bitmap() call in mapping_level())
  	unsigned long mmu_seq;
  	bool map_writable, write = error_code & PFERR_WRITE_MASK;
  
@@@ -3567,12 -3499,15 +3576,21 @@@ static int tdp_page_fault(struct kvm_vc
  	if (r)
  		return r;
  
++<<<<<<< HEAD
 +	force_pt_level = mapping_level_dirty_bitmap(vcpu, gfn);
 +	if (likely(!force_pt_level)) {
 +		level = mapping_level(vcpu, gfn);
++=======
+ 	force_pt_level = !check_hugepage_cache_consistency(vcpu, gfn,
+ 							   PT_DIRECTORY_LEVEL);
+ 	level = mapping_level(vcpu, gfn, &force_pt_level);
+ 	if (likely(!force_pt_level)) {
+ 		if (level > PT_DIRECTORY_LEVEL &&
+ 		    !check_hugepage_cache_consistency(vcpu, gfn, level))
+ 			level = PT_DIRECTORY_LEVEL;
++>>>>>>> fd1369021878 (KVM: x86: MMU: Move mapping_level_dirty_bitmap() call in mapping_level())
  		gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
- 	} else
- 		level = PT_PAGE_TABLE_LEVEL;
+ 	}
  
  	if (fast_page_fault(vcpu, gpa, level, error_code))
  		return 0;
diff --cc arch/x86/kvm/paging_tmpl.h
index 18c2dec509fd,bf39d0f3efa9..000000000000
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@@ -745,15 -743,14 +745,26 @@@ static int FNAME(page_fault)(struct kvm
  	is_self_change_mapping = FNAME(is_self_change_mapping)(vcpu,
  	      &walker, user_fault, &vcpu->arch.write_fault_to_shadow_pgtable);
  
++<<<<<<< HEAD
 +	if (walker.level >= PT_DIRECTORY_LEVEL)
 +		force_pt_level = mapping_level_dirty_bitmap(vcpu, walker.gfn)
 +		   || is_self_change_mapping;
 +	else
 +		force_pt_level = 1;
 +	if (!force_pt_level) {
 +		level = min(walker.level, mapping_level(vcpu, walker.gfn));
 +		walker.gfn = walker.gfn & ~(KVM_PAGES_PER_HPAGE(level) - 1);
 +	}
++=======
+ 	if (walker.level >= PT_DIRECTORY_LEVEL && !is_self_change_mapping) {
+ 		level = mapping_level(vcpu, walker.gfn, &force_pt_level);
+ 		if (likely(!force_pt_level)) {
+ 			level = min(walker.level, level);
+ 			walker.gfn = walker.gfn & ~(KVM_PAGES_PER_HPAGE(level) - 1);
+ 		}
+ 	} else
+ 		force_pt_level = true;
++>>>>>>> fd1369021878 (KVM: x86: MMU: Move mapping_level_dirty_bitmap() call in mapping_level())
  
  	mmu_seq = vcpu->kvm->mmu_notifier_seq;
  	smp_rmb();
* Unmerged path arch/x86/kvm/mmu.c
* Unmerged path arch/x86/kvm/paging_tmpl.h
