net/mlx5e: Union RQ RX info per RQ type

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Union RQ RX info per RQ type (Don Dutile) [1385330 1417285]
Rebuild_FUZZ: 94.59%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 21c59685dd176dd6b2c4fc5e18dc65730cfd546a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/21c59685.failed

We have two types of RX RQs, and they use two separate sets of
info arrays and structures in RX data path function.  Today those
structures are mutually exclusive per RQ type, hence one kind is
allocated on RQ creation according to the RQ type.

For better cache locality and to minimalize the
sizeof(struct mlx5e_rq), in this patch we define them as a union.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 21c59685dd176dd6b2c4fc5e18dc65730cfd546a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index b01f5bb32ed7,e3331237ce0e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -248,11 -268,56 +248,27 @@@ struct mlx5e_dma_info 
  struct mlx5e_rq {
  	/* data path */
  	struct mlx5_wq_ll      wq;
++<<<<<<< HEAD
 +	u32                    wqe_sz;
 +	struct sk_buff       **skb;
 +	struct mlx5e_mpw_info *wqe_info;
++=======
+ 
+ 	union {
+ 		struct mlx5e_dma_info *dma_info;
+ 		struct {
+ 			struct mlx5e_mpw_info *info;
+ 			void                  *mtt_no_align;
+ 			u32                    mtt_offset;
+ 		} mpwqe;
+ 	};
+ 	struct {
+ 		u8             page_order;
+ 		u32            wqe_sz;    /* wqe data buffer size */
+ 	} buff;
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  	__be32                 mkey_be;
 +	__be32                 umr_mkey_be;
  
  	struct device         *pdev;
  	struct net_device     *netdev;
@@@ -265,6 -332,8 +281,11 @@@
  
  	unsigned long          state;
  	int                    ix;
++<<<<<<< HEAD
++=======
+ 
+ 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  
  	/* control */
  	struct mlx5_wq_ctrl    wq_ctrl;
@@@ -628,6 -772,12 +649,15 @@@ static inline void mlx5e_cq_arm(struct 
  	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, NULL, cq->wq.cc);
  }
  
++<<<<<<< HEAD
++=======
+ static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)
+ {
+ 	return rq->mpwqe.mtt_offset +
+ 		wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
+ }
+ 
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
  {
  	return min_t(int, mdev->priv.eq_table.num_comp_vectors,
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 8d737e76ff7e,f2efa532f453..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -280,6 -298,107 +280,110 @@@ static void mlx5e_disable_async_events(
  #define MLX5E_HW2SW_MTU(hwmtu) (hwmtu - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  #define MLX5E_SW2HW_MTU(swmtu) (swmtu + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
  
++<<<<<<< HEAD
++=======
+ static inline int mlx5e_get_wqe_mtt_sz(void)
+ {
+ 	/* UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
+ 	 * To avoid copying garbage after the mtt array, we allocate
+ 	 * a little more.
+ 	 */
+ 	return ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(__be64),
+ 		     MLX5_UMR_MTT_ALIGNMENT);
+ }
+ 
+ static inline void mlx5e_build_umr_wqe(struct mlx5e_rq *rq, struct mlx5e_sq *sq,
+ 				       struct mlx5e_umr_wqe *wqe, u16 ix)
+ {
+ 	struct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;
+ 	struct mlx5_wqe_data_seg      *dseg = &wqe->data;
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	u8 ds_cnt = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS);
+ 	u32 umr_wqe_mtt_offset = mlx5e_get_wqe_mtt_offset(rq, ix);
+ 
+ 	cseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |
+ 				      ds_cnt);
+ 	cseg->fm_ce_se  = MLX5_WQE_CTRL_CQ_UPDATE;
+ 	cseg->imm       = rq->mkey_be;
+ 
+ 	ucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN;
+ 	ucseg->klm_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(MLX5_MPWRQ_PAGES_PER_WQE));
+ 	ucseg->bsf_octowords =
+ 		cpu_to_be16(MLX5_MTT_OCTW(umr_wqe_mtt_offset));
+ 	ucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ 
+ 	dseg->lkey = sq->mkey_be;
+ 	dseg->addr = cpu_to_be64(wi->umr.mtt_addr);
+ }
+ 
+ static int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq,
+ 				     struct mlx5e_channel *c)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int mtt_alloc = mtt_sz + MLX5_UMR_ALIGN - 1;
+ 	int i;
+ 
+ 	rq->mpwqe.info = kzalloc_node(wq_sz * sizeof(*rq->mpwqe.info),
+ 				      GFP_KERNEL, cpu_to_node(c->cpu));
+ 	if (!rq->mpwqe.info)
+ 		goto err_out;
+ 
+ 	/* We allocate more than mtt_sz as we will align the pointer */
+ 	rq->mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz, GFP_KERNEL,
+ 					cpu_to_node(c->cpu));
+ 	if (unlikely(!rq->mpwqe.mtt_no_align))
+ 		goto err_free_wqe_info;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		wi->umr.mtt = PTR_ALIGN(rq->mpwqe.mtt_no_align + i * mtt_alloc,
+ 					MLX5_UMR_ALIGN);
+ 		wi->umr.mtt_addr = dma_map_single(c->pdev, wi->umr.mtt, mtt_sz,
+ 						  PCI_DMA_TODEVICE);
+ 		if (unlikely(dma_mapping_error(c->pdev, wi->umr.mtt_addr)))
+ 			goto err_unmap_mtts;
+ 
+ 		mlx5e_build_umr_wqe(rq, &c->icosq, &wi->umr.wqe, i);
+ 	}
+ 
+ 	return 0;
+ 
+ err_unmap_mtts:
+ 	while (--i >= 0) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(c->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ err_free_wqe_info:
+ 	kfree(rq->mpwqe.info);
+ 
+ err_out:
+ 	return -ENOMEM;
+ }
+ 
+ static void mlx5e_rq_free_mpwqe_info(struct mlx5e_rq *rq)
+ {
+ 	int wq_sz = mlx5_wq_ll_get_size(&rq->wq);
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int i;
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_mpw_info *wi = &rq->mpwqe.info[i];
+ 
+ 		dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz,
+ 				 PCI_DMA_TODEVICE);
+ 	}
+ 	kfree(rq->mpwqe.mtt_no_align);
+ 	kfree(rq->mpwqe.info);
+ }
+ 
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  static int mlx5e_create_rq(struct mlx5e_channel *c,
  			   struct mlx5e_rq_param *param,
  			   struct mlx5e_rq *rq)
@@@ -353,8 -432,68 +457,73 @@@
  	rq->channel = c;
  	rq->ix      = c->ix;
  	rq->priv    = c->priv;
++<<<<<<< HEAD
 +	rq->mkey_be = c->mkey_be;
 +	rq->umr_mkey_be = cpu_to_be32(c->priv->umr_mkey.key);
++=======
+ 
+ 	switch (priv->params.rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		rq->handle_rx_cqe = mlx5e_handle_rx_cqe_mpwrq;
+ 		rq->alloc_wqe = mlx5e_alloc_rx_mpwqe;
+ 		rq->dealloc_wqe = mlx5e_dealloc_rx_mpwqe;
+ 
+ 		rq->mpwqe.mtt_offset = c->ix *
+ 			MLX5E_REQUIRED_MTTS(1, BIT(priv->params.log_rq_size));
+ 
+ 		rq->mpwqe_stride_sz = BIT(priv->params.mpwqe_log_stride_sz);
+ 		rq->mpwqe_num_strides = BIT(priv->params.mpwqe_log_num_strides);
+ 
+ 		rq->buff.wqe_sz = rq->mpwqe_stride_sz * rq->mpwqe_num_strides;
+ 		byte_count = rq->buff.wqe_sz;
+ 		rq->mkey_be = cpu_to_be32(c->priv->umr_mkey.key);
+ 		err = mlx5e_rq_alloc_mpwqe_info(rq, c);
+ 		if (err)
+ 			goto err_rq_wq_destroy;
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_LINKED_LIST */
+ 		rq->dma_info = kzalloc_node(wq_sz * sizeof(*rq->dma_info),
+ 					    GFP_KERNEL, cpu_to_node(c->cpu));
+ 		if (!rq->dma_info) {
+ 			err = -ENOMEM;
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		rq->handle_rx_cqe = mlx5e_handle_rx_cqe;
+ 		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
+ 		rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
+ 
+ 		rq->buff.wqe_sz = (priv->params.lro_en) ?
+ 				priv->params.lro_wqe_sz :
+ 				MLX5E_SW2HW_MTU(priv->netdev->mtu);
+ 		byte_count = rq->buff.wqe_sz;
+ 
+ 		/* calc the required page order */
+ 		frag_sz = MLX5_RX_HEADROOM +
+ 			  byte_count /* packet data */ +
+ 			  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+ 		frag_sz = SKB_DATA_ALIGN(frag_sz);
+ 
+ 		npages = DIV_ROUND_UP(frag_sz, PAGE_SIZE);
+ 		rq->buff.page_order = order_base_2(npages);
+ 
+ 		byte_count |= MLX5_HW_START_PADDING;
+ 		rq->mkey_be = c->mkey_be;
+ 	}
+ 
+ 	for (i = 0; i < wq_sz; i++) {
+ 		struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(&rq->wq, i);
+ 
+ 		wqe->data.byte_count = cpu_to_be32(byte_count);
+ 		wqe->data.lkey = rq->mkey_be;
+ 	}
+ 
+ 	INIT_WORK(&rq->am.work, mlx5e_rx_am_work);
+ 	rq->am.mode = priv->params.rx_cq_period_mode;
+ 
+ 	rq->page_cache.head = 0;
+ 	rq->page_cache.tail = 0;
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  
  	return 0;
  
@@@ -499,6 -645,27 +668,30 @@@ static int mlx5e_wait_for_min_rx_wqes(s
  	return -ETIMEDOUT;
  }
  
++<<<<<<< HEAD
++=======
+ static void mlx5e_free_rx_descs(struct mlx5e_rq *rq)
+ {
+ 	struct mlx5_wq_ll *wq = &rq->wq;
+ 	struct mlx5e_rx_wqe *wqe;
+ 	__be16 wqe_ix_be;
+ 	u16 wqe_ix;
+ 
+ 	/* UMR WQE (if in progress) is always at wq->head */
+ 	if (test_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state))
+ 		mlx5e_free_rx_mpwqe(rq, &rq->mpwqe.info[wq->head]);
+ 
+ 	while (!mlx5_wq_ll_is_empty(wq)) {
+ 		wqe_ix_be = *wq->tail_next;
+ 		wqe_ix    = be16_to_cpu(wqe_ix_be);
+ 		wqe       = mlx5_wq_ll_get_wqe(&rq->wq, wqe_ix);
+ 		rq->dealloc_wqe(rq, wqe_ix);
+ 		mlx5_wq_ll_pop(&rq->wq, wqe_ix_be,
+ 			       &wqe->next.next_wqe_index);
+ 	}
+ }
+ 
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  static int mlx5e_open_rq(struct mlx5e_channel *c,
  			 struct mlx5e_rq_param *param,
  			 struct mlx5e_rq *rq)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index d795e95774bc,a403a797ebef..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -324,46 -326,9 +324,50 @@@ mlx5e_copy_skb_header_fragmented_mpwqe(
  	}
  }
  
 -static inline void mlx5e_post_umr_wqe(struct mlx5e_rq *rq, u16 ix)
 +static u16 mlx5e_get_wqe_mtt_offset(u16 rq_ix, u16 wqe_ix)
  {
++<<<<<<< HEAD
 +	return rq_ix * MLX5_CHANNEL_MAX_NUM_MTTS +
 +		wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
 +}
 +
 +static void mlx5e_build_umr_wqe(struct mlx5e_rq *rq,
 +				struct mlx5e_sq *sq,
 +				struct mlx5e_umr_wqe *wqe,
 +				u16 ix)
 +{
 +	struct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;
 +	struct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;
 +	struct mlx5_wqe_data_seg      *dseg = &wqe->data;
 +	struct mlx5e_mpw_info *wi = &rq->wqe_info[ix];
 +	u8 ds_cnt = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS);
 +	u16 umr_wqe_mtt_offset = mlx5e_get_wqe_mtt_offset(rq->ix, ix);
 +
 +	memset(wqe, 0, sizeof(*wqe));
 +	cseg->opmod_idx_opcode =
 +		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
 +			    MLX5_OPCODE_UMR);
 +	cseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |
 +				      ds_cnt);
 +	cseg->fm_ce_se  = MLX5_WQE_CTRL_CQ_UPDATE;
 +	cseg->imm       = rq->umr_mkey_be;
 +
 +	ucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN;
 +	ucseg->klm_octowords =
 +		cpu_to_be16(mlx5e_get_mtt_octw(MLX5_MPWRQ_PAGES_PER_WQE));
 +	ucseg->bsf_octowords =
 +		cpu_to_be16(mlx5e_get_mtt_octw(umr_wqe_mtt_offset));
 +	ucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
 +
 +	dseg->lkey = sq->mkey_be;
 +	dseg->addr = cpu_to_be64(wi->umr.mtt_addr);
 +}
 +
 +static void mlx5e_post_umr_wqe(struct mlx5e_rq *rq, u16 ix)
 +{
++=======
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  	struct mlx5e_sq *sq = &rq->channel->icosq;
  	struct mlx5_wq_cyc *wq = &sq->wq;
  	struct mlx5e_umr_wqe *wqe;
@@@ -385,70 -354,24 +389,77 @@@
  	mlx5e_tx_notify_hw(sq, &wqe->ctrl, 0);
  }
  
 -static int mlx5e_alloc_rx_umr_mpwqe(struct mlx5e_rq *rq,
 -				    struct mlx5e_rx_wqe *wqe,
 -				    u16 ix)
 +static inline int mlx5e_get_wqe_mtt_sz(void)
  {
 +	/* UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
 +	 * To avoid copying garbage after the mtt array, we allocate
 +	 * a little more.
 +	 */
 +	return ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(__be64),
 +		     MLX5_UMR_MTT_ALIGNMENT);
 +}
 +
 +static int mlx5e_alloc_and_map_page(struct mlx5e_rq *rq,
 +				    struct mlx5e_mpw_info *wi,
 +				    int i)
 +{
 +	struct page *page;
 +
 +	page = dev_alloc_page();
 +	if (unlikely(!page))
 +		return -ENOMEM;
 +
 +	wi->umr.dma_info[i].page = page;
 +	wi->umr.dma_info[i].addr = dma_map_page(rq->pdev, page, 0, PAGE_SIZE,
 +						PCI_DMA_FROMDEVICE);
 +	if (unlikely(dma_mapping_error(rq->pdev, wi->umr.dma_info[i].addr))) {
 +		put_page(page);
 +		return -ENOMEM;
 +	}
 +	wi->umr.mtt[i] = cpu_to_be64(wi->umr.dma_info[i].addr | MLX5_EN_WR);
 +
 +	return 0;
 +}
 +
 +static int mlx5e_alloc_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
 +					   struct mlx5e_rx_wqe *wqe,
 +					   u16 ix)
 +{
++<<<<<<< HEAD
 +	struct mlx5e_mpw_info *wi = &rq->wqe_info[ix];
 +	int mtt_sz = mlx5e_get_wqe_mtt_sz();
 +	u32 dma_offset = mlx5e_get_wqe_mtt_offset(rq->ix, ix) << PAGE_SHIFT;
++=======
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
+ 	u64 dma_offset = (u64)mlx5e_get_wqe_mtt_offset(rq, ix) << PAGE_SHIFT;
+ 	int pg_strides = mlx5e_mpwqe_strides_per_page(rq);
+ 	int err;
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  	int i;
  
 -	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
 -		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
 +	wi->umr.dma_info = kmalloc(sizeof(*wi->umr.dma_info) *
 +				   MLX5_MPWRQ_PAGES_PER_WQE,
 +				   GFP_ATOMIC);
 +	if (unlikely(!wi->umr.dma_info))
 +		goto err_out;
  
 -		err = mlx5e_page_alloc_mapped(rq, dma_info);
 -		if (unlikely(err))
 +	/* We allocate more than mtt_sz as we will align the pointer */
 +	wi->umr.mtt_no_align = kzalloc(mtt_sz + MLX5_UMR_ALIGN - 1,
 +				       GFP_ATOMIC);
 +	if (unlikely(!wi->umr.mtt_no_align))
 +		goto err_free_umr;
 +
 +	wi->umr.mtt = PTR_ALIGN(wi->umr.mtt_no_align, MLX5_UMR_ALIGN);
 +	wi->umr.mtt_addr = dma_map_single(rq->pdev, wi->umr.mtt, mtt_sz,
 +					  PCI_DMA_TODEVICE);
 +	if (unlikely(dma_mapping_error(rq->pdev, wi->umr.mtt_addr)))
 +		goto err_free_mtt;
 +
 +	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
 +		if (unlikely(mlx5e_alloc_and_map_page(rq, wi, i)))
  			goto err_unmap;
 -		wi->umr.mtt[i] = cpu_to_be64(dma_info->addr | MLX5_EN_WR);
 -		page_ref_add(dma_info->page, pg_strides);
 +		atomic_add(mlx5e_mpwqe_strides_per_page(rq),
 +			   &wi->umr.dma_info[i].page->_count);
  		wi->skbs_frags[i] = 0;
  	}
  
@@@ -506,8 -410,13 +517,17 @@@ void mlx5e_post_rx_fragmented_mpwqe(str
  	struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
  
  	clear_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state);
++<<<<<<< HEAD
++=======
+ 
+ 	if (unlikely(test_bit(MLX5E_RQ_STATE_FLUSH, &rq->state))) {
+ 		mlx5e_free_rx_mpwqe(rq, &rq->mpwqe.info[wq->head]);
+ 		return;
+ 	}
+ 
++>>>>>>> 21c59685dd17 (net/mlx5e: Union RQ RX info per RQ type)
  	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 +	rq->stats.mpwqe_frag++;
  
  	/* ensure wqes are visible to device before updating doorbell record */
  	dma_wmb();
@@@ -590,26 -438,9 +610,26 @@@ int mlx5e_alloc_rx_mpwqe(struct mlx5e_r
  
  void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
  {
- 	struct mlx5e_mpw_info *wi = &rq->wqe_info[ix];
+ 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
  
 -	mlx5e_free_rx_mpwqe(rq, wi);
 +	wi->free_wqe(rq, wi);
 +}
 +
 +void mlx5e_free_rx_descs(struct mlx5e_rq *rq)
 +{
 +	struct mlx5_wq_ll *wq = &rq->wq;
 +	struct mlx5e_rx_wqe *wqe;
 +	__be16 wqe_ix_be;
 +	u16 wqe_ix;
 +
 +	while (!mlx5_wq_ll_is_empty(wq)) {
 +		wqe_ix_be = *wq->tail_next;
 +		wqe_ix    = be16_to_cpu(wqe_ix_be);
 +		wqe       = mlx5_wq_ll_get_wqe(&rq->wq, wqe_ix);
 +		rq->dealloc_wqe(rq, wqe_ix);
 +		mlx5_wq_ll_pop(&rq->wq, wqe_ix_be,
 +			       &wqe->next.next_wqe_index);
 +	}
  }
  
  #define RQ_CANNOT_POST(rq) \
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
