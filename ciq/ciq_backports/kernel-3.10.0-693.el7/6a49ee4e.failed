amd-xgbe: Fix the mapping of priorities to traffic classes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Lendacky, Thomas <Thomas.Lendacky@amd.com>
commit 6a49ee4e1b9eb8747ff79c5438a42c0726aaa03b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/6a49ee4e.failed

The driver is checking the pfc_en field of the ieee_pfc structure to
determine whether to associate a priority with a traffic class. This is
incorrect since the pfc_en field is for determining if PFC is enabled
for a traffic class.

The association of priority to traffic class does not depend on whether
the traffic class is enabled for PFC, so remove that check. Also, the
mapping of priorities to traffic classes should be done when configuring
the traffic classes and not the PFC support so move the priority to traffic
class association from xgbe_config_dcb_pfc to xgbe_config_dcb_tc.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 6a49ee4e1b9eb8747ff79c5438a42c0726aaa03b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/amd/xgbe/xgbe-dev.c
diff --cc drivers/net/ethernet/amd/xgbe/xgbe-dev.c
index a748fd8a1c58,67d234eb1655..000000000000
--- a/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-dev.c
@@@ -923,7 -1224,201 +923,205 @@@ static void xgbe_rx_desc_init(struct xg
  	DBGPR("<--rx_desc_init\n");
  }
  
++<<<<<<< HEAD
 +static void xgbe_pre_xmit(struct xgbe_channel *channel)
++=======
+ static void xgbe_update_tstamp_addend(struct xgbe_prv_data *pdata,
+ 				      unsigned int addend)
+ {
+ 	/* Set the addend register value and tell the device */
+ 	XGMAC_IOWRITE(pdata, MAC_TSAR, addend);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_TSCR, TSADDREG, 1);
+ 
+ 	/* Wait for addend update to complete */
+ 	while (XGMAC_IOREAD_BITS(pdata, MAC_TSCR, TSADDREG))
+ 		udelay(5);
+ }
+ 
+ static void xgbe_set_tstamp_time(struct xgbe_prv_data *pdata, unsigned int sec,
+ 				 unsigned int nsec)
+ {
+ 	/* Set the time values and tell the device */
+ 	XGMAC_IOWRITE(pdata, MAC_STSUR, sec);
+ 	XGMAC_IOWRITE(pdata, MAC_STNUR, nsec);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_TSCR, TSINIT, 1);
+ 
+ 	/* Wait for time update to complete */
+ 	while (XGMAC_IOREAD_BITS(pdata, MAC_TSCR, TSINIT))
+ 		udelay(5);
+ }
+ 
+ static u64 xgbe_get_tstamp_time(struct xgbe_prv_data *pdata)
+ {
+ 	u64 nsec;
+ 
+ 	nsec = XGMAC_IOREAD(pdata, MAC_STSR);
+ 	nsec *= NSEC_PER_SEC;
+ 	nsec += XGMAC_IOREAD(pdata, MAC_STNR);
+ 
+ 	return nsec;
+ }
+ 
+ static u64 xgbe_get_tx_tstamp(struct xgbe_prv_data *pdata)
+ {
+ 	unsigned int tx_snr;
+ 	u64 nsec;
+ 
+ 	tx_snr = XGMAC_IOREAD(pdata, MAC_TXSNR);
+ 	if (XGMAC_GET_BITS(tx_snr, MAC_TXSNR, TXTSSTSMIS))
+ 		return 0;
+ 
+ 	nsec = XGMAC_IOREAD(pdata, MAC_TXSSR);
+ 	nsec *= NSEC_PER_SEC;
+ 	nsec += tx_snr;
+ 
+ 	return nsec;
+ }
+ 
+ static void xgbe_get_rx_tstamp(struct xgbe_packet_data *packet,
+ 			       struct xgbe_ring_desc *rdesc)
+ {
+ 	u64 nsec;
+ 
+ 	if (XGMAC_GET_BITS_LE(rdesc->desc3, RX_CONTEXT_DESC3, TSA) &&
+ 	    !XGMAC_GET_BITS_LE(rdesc->desc3, RX_CONTEXT_DESC3, TSD)) {
+ 		nsec = le32_to_cpu(rdesc->desc1);
+ 		nsec <<= 32;
+ 		nsec |= le32_to_cpu(rdesc->desc0);
+ 		if (nsec != 0xffffffffffffffffULL) {
+ 			packet->rx_tstamp = nsec;
+ 			XGMAC_SET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,
+ 				       RX_TSTAMP, 1);
+ 		}
+ 	}
+ }
+ 
+ static int xgbe_config_tstamp(struct xgbe_prv_data *pdata,
+ 			      unsigned int mac_tscr)
+ {
+ 	/* Set one nano-second accuracy */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSCTRLSSR, 1);
+ 
+ 	/* Set fine timestamp update */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSCFUPDT, 1);
+ 
+ 	/* Overwrite earlier timestamps */
+ 	XGMAC_SET_BITS(mac_tscr, MAC_TSCR, TXTSSTSM, 1);
+ 
+ 	XGMAC_IOWRITE(pdata, MAC_TSCR, mac_tscr);
+ 
+ 	/* Exit if timestamping is not enabled */
+ 	if (!XGMAC_GET_BITS(mac_tscr, MAC_TSCR, TSENA))
+ 		return 0;
+ 
+ 	/* Initialize time registers */
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_SSIR, SSINC, XGBE_TSTAMP_SSINC);
+ 	XGMAC_IOWRITE_BITS(pdata, MAC_SSIR, SNSINC, XGBE_TSTAMP_SNSINC);
+ 	xgbe_update_tstamp_addend(pdata, pdata->tstamp_addend);
+ 	xgbe_set_tstamp_time(pdata, 0, 0);
+ 
+ 	/* Initialize the timecounter */
+ 	timecounter_init(&pdata->tstamp_tc, &pdata->tstamp_cc,
+ 			 ktime_to_ns(ktime_get_real()));
+ 
+ 	return 0;
+ }
+ 
+ static void xgbe_config_dcb_tc(struct xgbe_prv_data *pdata)
+ {
+ 	struct ieee_ets *ets = pdata->ets;
+ 	unsigned int total_weight, min_weight, weight;
+ 	unsigned int mask, reg, reg_val;
+ 	unsigned int i, prio;
+ 
+ 	if (!ets)
+ 		return;
+ 
+ 	/* Set Tx to deficit weighted round robin scheduling algorithm (when
+ 	 * traffic class is using ETS algorithm)
+ 	 */
+ 	XGMAC_IOWRITE_BITS(pdata, MTL_OMR, ETSALG, MTL_ETSALG_DWRR);
+ 
+ 	/* Set Traffic Class algorithms */
+ 	total_weight = pdata->netdev->mtu * pdata->hw_feat.tc_cnt;
+ 	min_weight = total_weight / 100;
+ 	if (!min_weight)
+ 		min_weight = 1;
+ 
+ 	for (i = 0; i < pdata->hw_feat.tc_cnt; i++) {
+ 		/* Map the priorities to the traffic class */
+ 		mask = 0;
+ 		for (prio = 0; prio < IEEE_8021QAZ_MAX_TCS; prio++) {
+ 			if (ets->prio_tc[prio] == i)
+ 				mask |= (1 << prio);
+ 		}
+ 		mask &= 0xff;
+ 
+ 		netif_dbg(pdata, drv, pdata->netdev, "TC%u PRIO mask=%#x\n",
+ 			  i, mask);
+ 		reg = MTL_TCPM0R + (MTL_TCPM_INC * (i / MTL_TCPM_TC_PER_REG));
+ 		reg_val = XGMAC_IOREAD(pdata, reg);
+ 
+ 		reg_val &= ~(0xff << ((i % MTL_TCPM_TC_PER_REG) << 3));
+ 		reg_val |= (mask << ((i % MTL_TCPM_TC_PER_REG) << 3));
+ 
+ 		XGMAC_IOWRITE(pdata, reg, reg_val);
+ 
+ 		/* Set the traffic class algorithm */
+ 		switch (ets->tc_tsa[i]) {
+ 		case IEEE_8021QAZ_TSA_STRICT:
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TC%u using SP\n", i);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_ETSCR, TSA,
+ 					       MTL_TSA_SP);
+ 			break;
+ 		case IEEE_8021QAZ_TSA_ETS:
+ 			weight = total_weight * ets->tc_tx_bw[i] / 100;
+ 			weight = clamp(weight, min_weight, total_weight);
+ 
+ 			netif_dbg(pdata, drv, pdata->netdev,
+ 				  "TC%u using DWRR (weight %u)\n", i, weight);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_ETSCR, TSA,
+ 					       MTL_TSA_ETS);
+ 			XGMAC_MTL_IOWRITE_BITS(pdata, i, MTL_TC_QWR, QW,
+ 					       weight);
+ 			break;
+ 		}
+ 	}
+ }
+ 
+ static void xgbe_config_dcb_pfc(struct xgbe_prv_data *pdata)
+ {
+ 	xgbe_config_flow_control(pdata);
+ }
+ 
+ static void xgbe_tx_start_xmit(struct xgbe_channel *channel,
+ 			       struct xgbe_ring *ring)
+ {
+ 	struct xgbe_prv_data *pdata = channel->pdata;
+ 	struct xgbe_ring_data *rdata;
+ 
+ 	/* Make sure everything is written before the register write */
+ 	wmb();
+ 
+ 	/* Issue a poll command to Tx DMA by writing address
+ 	 * of next immediate free descriptor */
+ 	rdata = XGBE_GET_DESC_DATA(ring, ring->cur);
+ 	XGMAC_DMA_IOWRITE(channel, DMA_CH_TDTR_LO,
+ 			  lower_32_bits(rdata->rdesc_dma));
+ 
+ 	/* Start the Tx timer */
+ 	if (pdata->tx_usecs && !channel->tx_timer_active) {
+ 		channel->tx_timer_active = 1;
+ 		mod_timer(&channel->tx_timer,
+ 			  jiffies + usecs_to_jiffies(pdata->tx_usecs));
+ 	}
+ 
+ 	ring->tx.xmit_more = 0;
+ }
+ 
+ static void xgbe_dev_xmit(struct xgbe_channel *channel)
++>>>>>>> 6a49ee4e1b9e (amd-xgbe: Fix the mapping of priorities to traffic classes)
  {
  	struct xgbe_prv_data *pdata = channel->pdata;
  	struct xgbe_ring *ring = channel->tx_ring;
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-dev.c
