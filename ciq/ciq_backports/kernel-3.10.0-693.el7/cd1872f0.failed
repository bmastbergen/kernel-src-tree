KVM: x86: MMU: Make force_pt_level bool

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
commit cd1872f028556dc0e8424e58413c0268c159383b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/cd1872f0.failed

This will be passed to a function later.

	Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit cd1872f028556dc0e8424e58413c0268c159383b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 70c3ecb68058,2262728863de..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -3054,8 -2962,8 +3054,13 @@@ static int nonpaging_map(struct kvm_vcp
  {
  	int r;
  	int level;
++<<<<<<< HEAD
 +	int force_pt_level;
 +	kvm_pfn_t pfn;
++=======
+ 	bool force_pt_level;
+ 	pfn_t pfn;
++>>>>>>> cd1872f02855 (KVM: x86: MMU: Make force_pt_level bool)
  	unsigned long mmu_seq;
  	bool map_writable, write = error_code & PFERR_WRITE_MASK;
  
@@@ -3545,10 -3463,20 +3550,10 @@@ static bool try_async_pf(struct kvm_vcp
  static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
  			  bool prefault)
  {
 -	pfn_t pfn;
 +	kvm_pfn_t pfn;
  	int r;
  	int level;
- 	int force_pt_level;
+ 	bool force_pt_level;
  	gfn_t gfn = gpa >> PAGE_SHIFT;
  	unsigned long mmu_seq;
  	int write = error_code & PFERR_WRITE_MASK;
@@@ -3567,9 -3495,17 +3572,18 @@@
  	if (r)
  		return r;
  
++<<<<<<< HEAD
 +	force_pt_level = mapping_level_dirty_bitmap(vcpu, gfn);
++=======
+ 	if (mapping_level_dirty_bitmap(vcpu, gfn) ||
+ 	    !check_hugepage_cache_consistency(vcpu, gfn, PT_DIRECTORY_LEVEL))
+ 		force_pt_level = true;
+ 	else
+ 		force_pt_level = false;
+ 
++>>>>>>> cd1872f02855 (KVM: x86: MMU: Make force_pt_level bool)
  	if (likely(!force_pt_level)) {
  		level = mapping_level(vcpu, gfn);
 -		if (level > PT_DIRECTORY_LEVEL &&
 -		    !check_hugepage_cache_consistency(vcpu, gfn, level))
 -			level = PT_DIRECTORY_LEVEL;
  		gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
  	} else
  		level = PT_PAGE_TABLE_LEVEL;
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h
index 18c2dec509fd..b5637d4c1c49 100644
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@ -701,7 +701,7 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,
 	int r;
 	kvm_pfn_t pfn;
 	int level = PT_PAGE_TABLE_LEVEL;
-	int force_pt_level;
+	bool force_pt_level;
 	unsigned long mmu_seq;
 	bool map_writable, is_self_change_mapping;
 
@@ -749,7 +749,7 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,
 		force_pt_level = mapping_level_dirty_bitmap(vcpu, walker.gfn)
 		   || is_self_change_mapping;
 	else
-		force_pt_level = 1;
+		force_pt_level = true;
 	if (!force_pt_level) {
 		level = min(walker.level, mapping_level(vcpu, walker.gfn));
 		walker.gfn = walker.gfn & ~(KVM_PAGES_PER_HPAGE(level) - 1);
