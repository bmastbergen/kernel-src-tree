dax: export a low-level __dax_zero_page_range helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 679c8bd3b29428e736eabb7fc66a978f312f0c86
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/679c8bd3.failed

This allows XFS to perform zeroing using the iomap infrastructure and
avoid buffer heads.

	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
[vishal: fix conflicts with dax-error-handling]
	Signed-off-by: Vishal Verma <vishal.l.verma@intel.com>
(cherry picked from commit 679c8bd3b29428e736eabb7fc66a978f312f0c86)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 2dc57eba7712,651d4b18ac29..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -1091,24 -997,13 +1108,30 @@@ int dax_zero_page_range(struct inode *i
  
  	memset(&bh, 0, sizeof(bh));
  	bh.b_bdev = inode->i_sb->s_bdev;
 -	bh.b_size = PAGE_SIZE;
 +	bh.b_size = PAGE_CACHE_SIZE;
  	err = get_block(inode, index, &bh, 0);
- 	if (err < 0)
+ 	if (err < 0 || !buffer_written(&bh))
  		return err;
++<<<<<<< HEAD
 +	if (buffer_written(&bh)) {
 +		struct block_device *bdev = bh.b_bdev;
 +		struct blk_dax_ctl dax = {
 +			.sector = to_sector(&bh, inode),
 +			.size = PAGE_CACHE_SIZE,
 +		};
 +
 +		if (dax_map_atomic(bdev, &dax) < 0)
 +			return PTR_ERR(dax.addr);
 +		clear_pmem(dax.addr + offset, length);
 +		dax_unmap_atomic(bdev, &dax);
 +	}
 +
 +	return 0;
++=======
+ 
+ 	return __dax_zero_page_range(bh.b_bdev, to_sector(&bh, inode),
+ 			offset, length);
++>>>>>>> 679c8bd3b294 (dax: export a low-level __dax_zero_page_range helper)
  }
  EXPORT_SYMBOL_GPL(dax_zero_page_range);
  
* Unmerged path fs/dax.c
diff --git a/include/linux/dax.h b/include/linux/dax.h
index bbe07d8b9dee..8d21432c651f 100644
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@ -18,12 +18,19 @@ int __dax_fault(struct vm_area_struct *, struct vm_fault *, get_block_t,
 
 #ifdef CONFIG_FS_DAX
 struct page *read_dax_sector(struct block_device *bdev, sector_t n);
+int __dax_zero_page_range(struct block_device *bdev, sector_t sector,
+		unsigned int offset, unsigned int length);
 #else
 static inline struct page *read_dax_sector(struct block_device *bdev,
 		sector_t n)
 {
 	return ERR_PTR(-ENXIO);
 }
+static inline int __dax_zero_page_range(struct block_device *bdev,
+		sector_t sector, unsigned int offset, unsigned int length)
+{
+	return -ENXIO;
+}
 #endif
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
