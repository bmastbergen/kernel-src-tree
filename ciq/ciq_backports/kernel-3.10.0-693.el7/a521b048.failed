s390/dasd: channel path aware error recovery

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [s390] dasd: channel path aware error recovery (Hendrik Brueckner) [1380771]
Rebuild_FUZZ: 93.98%
commit-author Stefan Haberland <sth@linux.vnet.ibm.com>
commit a521b048bc8c5d3c57a468c2cba70eb60e873616
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/a521b048.failed

With this feature, the DASD device driver more robustly handles DASDs
that are attached via multiple channel paths and are subject to
constant Interface-Control-Checks (IFCCs) and Channel-Control-Checks
(CCCs) or loss of High-Performance-FICON (HPF) functionality on one or
more of these paths.

If a channel path does not work correctly, it is removed from normal
operation as long as other channel paths are available. All extended
error recovery states can be queried and reset via user space
interfaces.

	Signed-off-by: Stefan Haberland <sth@linux.vnet.ibm.com>
	Reviewed-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
	Reviewed-by: Jan Hoeppner <hoeppner@linux.vnet.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit a521b048bc8c5d3c57a468c2cba70eb60e873616)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/block/dasd.c
#	drivers/s390/block/dasd_devmap.c
#	drivers/s390/block/dasd_eckd.c
#	drivers/s390/block/dasd_int.h
diff --cc drivers/s390/block/dasd.c
index 5f422bc676a9,0e3fdfdbd098..000000000000
--- a/drivers/s390/block/dasd.c
+++ b/drivers/s390/block/dasd.c
@@@ -1607,7 -1622,15 +1609,14 @@@ void dasd_generic_handle_state_change(s
  	if (device->block)
  		dasd_schedule_block_bh(device->block);
  }
 -EXPORT_SYMBOL_GPL(dasd_generic_handle_state_change);
  
+ static int dasd_check_hpf_error(struct irb *irb)
+ {
+ 	return (scsw_tm_is_valid_schxs(&irb->scsw) &&
+ 	    (irb->scsw.tm.sesq == SCSW_SESQ_DEV_NOFCX ||
+ 	     irb->scsw.tm.sesq == SCSW_SESQ_PATH_NOFCX));
+ }
+ 
  /*
   * Interrupt handler for "normal" ssch-io based dasd devices.
   */
@@@ -3494,9 -3715,9 +3510,13 @@@ int dasd_generic_notify(struct ccw_devi
  
  void dasd_generic_path_event(struct ccw_device *cdev, int *path_event)
  {
 +	int chp;
 +	__u8 oldopm, eventlpm;
  	struct dasd_device *device;
++<<<<<<< HEAD
++=======
+ 	int chp, oldopm, hpfpm, ifccpm;
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  
  	device = dasd_device_from_cdev_locked(cdev);
  	if (IS_ERR(device))
@@@ -3543,6 -3749,38 +3563,41 @@@
  				device->discipline->kick_validate(device);
  		}
  	}
++<<<<<<< HEAD
++=======
+ 	hpfpm = dasd_path_get_hpfpm(device);
+ 	ifccpm = dasd_path_get_ifccpm(device);
+ 	if (!dasd_path_get_opm(device) && hpfpm) {
+ 		/*
+ 		 * device has no operational paths but at least one path is
+ 		 * disabled due to HPF errors
+ 		 * disable HPF at all and use the path(s) again
+ 		 */
+ 		if (device->discipline->disable_hpf)
+ 			device->discipline->disable_hpf(device);
+ 		dasd_device_set_stop_bits(device, DASD_STOPPED_NOT_ACC);
+ 		dasd_path_set_tbvpm(device, hpfpm);
+ 		dasd_schedule_device_bh(device);
+ 		dasd_schedule_requeue(device);
+ 	} else if (!dasd_path_get_opm(device) && ifccpm) {
+ 		/*
+ 		 * device has no operational paths but at least one path is
+ 		 * disabled due to IFCC errors
+ 		 * trigger path verification on paths with IFCC errors
+ 		 */
+ 		dasd_path_set_tbvpm(device, ifccpm);
+ 		dasd_schedule_device_bh(device);
+ 	}
+ 	if (oldopm && !dasd_path_get_opm(device) && !hpfpm && !ifccpm) {
+ 		dev_warn(&device->cdev->dev,
+ 			 "No verified channel paths remain for the device\n");
+ 		DBF_DEV_EVENT(DBF_WARNING, device,
+ 			      "%s", "last verified path gone");
+ 		dasd_eer_write(device, NULL, DASD_EER_NOPATH);
+ 		dasd_device_set_stop_bits(device,
+ 					  DASD_STOPPED_DC_WAIT);
+ 	}
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  	dasd_put_device(device);
  }
  EXPORT_SYMBOL_GPL(dasd_generic_path_event);
diff --cc drivers/s390/block/dasd_devmap.c
index a71bb8aaca1d,84ca314c87e3..000000000000
--- a/drivers/s390/block/dasd_devmap.c
+++ b/drivers/s390/block/dasd_devmap.c
@@@ -984,6 -966,34 +984,37 @@@ out
  static DEVICE_ATTR(safe_offline, 0200, NULL, dasd_safe_offline_store);
  
  static ssize_t
++<<<<<<< HEAD
++=======
+ dasd_access_show(struct device *dev, struct device_attribute *attr,
+ 		 char *buf)
+ {
+ 	struct ccw_device *cdev = to_ccwdev(dev);
+ 	struct dasd_device *device;
+ 	int count;
+ 
+ 	device = dasd_device_from_cdev(cdev);
+ 	if (IS_ERR(device))
+ 		return PTR_ERR(device);
+ 
+ 	if (!device->discipline)
+ 		count = -ENODEV;
+ 	else if (!device->discipline->host_access_count)
+ 		count = -EOPNOTSUPP;
+ 	else
+ 		count = device->discipline->host_access_count(device);
+ 
+ 	dasd_put_device(device);
+ 	if (count < 0)
+ 		return count;
+ 
+ 	return sprintf(buf, "%d\n", count);
+ }
+ 
+ static DEVICE_ATTR(host_access_count, 0444, dasd_access_show, NULL);
+ 
+ static ssize_t
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  dasd_discipline_show(struct device *dev, struct device_attribute *attr,
  		     char *buf)
  {
@@@ -1240,6 -1248,145 +1271,148 @@@ dasd_expires_store(struct device *dev, 
  
  static DEVICE_ATTR(expires, 0644, dasd_expires_show, dasd_expires_store);
  
++<<<<<<< HEAD
++=======
+ static ssize_t
+ dasd_retries_show(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	struct dasd_device *device;
+ 	int len;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 	len = snprintf(buf, PAGE_SIZE, "%lu\n", device->default_retries);
+ 	dasd_put_device(device);
+ 	return len;
+ }
+ 
+ static ssize_t
+ dasd_retries_store(struct device *dev, struct device_attribute *attr,
+ 		   const char *buf, size_t count)
+ {
+ 	struct dasd_device *device;
+ 	unsigned long val;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 
+ 	if ((kstrtoul(buf, 10, &val) != 0) ||
+ 	    (val > DASD_RETRIES_MAX)) {
+ 		dasd_put_device(device);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (val)
+ 		device->default_retries = val;
+ 
+ 	dasd_put_device(device);
+ 	return count;
+ }
+ 
+ static DEVICE_ATTR(retries, 0644, dasd_retries_show, dasd_retries_store);
+ 
+ static ssize_t
+ dasd_timeout_show(struct device *dev, struct device_attribute *attr,
+ 		  char *buf)
+ {
+ 	struct dasd_device *device;
+ 	int len;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 	len = snprintf(buf, PAGE_SIZE, "%lu\n", device->blk_timeout);
+ 	dasd_put_device(device);
+ 	return len;
+ }
+ 
+ static ssize_t
+ dasd_timeout_store(struct device *dev, struct device_attribute *attr,
+ 		   const char *buf, size_t count)
+ {
+ 	struct dasd_device *device;
+ 	struct request_queue *q;
+ 	unsigned long val, flags;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device) || !device->block)
+ 		return -ENODEV;
+ 
+ 	if ((kstrtoul(buf, 10, &val) != 0) ||
+ 	    val > UINT_MAX / HZ) {
+ 		dasd_put_device(device);
+ 		return -EINVAL;
+ 	}
+ 	q = device->block->request_queue;
+ 	if (!q) {
+ 		dasd_put_device(device);
+ 		return -ENODEV;
+ 	}
+ 	spin_lock_irqsave(&device->block->request_queue_lock, flags);
+ 	if (!val)
+ 		blk_queue_rq_timed_out(q, NULL);
+ 	else
+ 		blk_queue_rq_timed_out(q, dasd_times_out);
+ 
+ 	device->blk_timeout = val;
+ 
+ 	blk_queue_rq_timeout(q, device->blk_timeout * HZ);
+ 	spin_unlock_irqrestore(&device->block->request_queue_lock, flags);
+ 
+ 	dasd_put_device(device);
+ 	return count;
+ }
+ 
+ static DEVICE_ATTR(timeout, 0644,
+ 		   dasd_timeout_show, dasd_timeout_store);
+ 
+ 
+ static ssize_t
+ dasd_path_reset_store(struct device *dev, struct device_attribute *attr,
+ 		      const char *buf, size_t count)
+ {
+ 	struct dasd_device *device;
+ 	unsigned int val;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 
+ 	if ((kstrtouint(buf, 16, &val) != 0) || val > 0xff)
+ 		val = 0;
+ 
+ 	if (device->discipline && device->discipline->reset_path)
+ 		device->discipline->reset_path(device, (__u8) val);
+ 
+ 	dasd_put_device(device);
+ 	return count;
+ }
+ 
+ static DEVICE_ATTR(path_reset, 0200, NULL, dasd_path_reset_store);
+ 
+ static ssize_t dasd_hpf_show(struct device *dev, struct device_attribute *attr,
+ 			     char *buf)
+ {
+ 	struct dasd_device *device;
+ 	int hpf;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 	if (!device->discipline || !device->discipline->hpf_enabled) {
+ 		dasd_put_device(device);
+ 		return snprintf(buf, PAGE_SIZE, "%d\n", dasd_nofcx);
+ 	}
+ 	hpf = device->discipline->hpf_enabled(device);
+ 	dasd_put_device(device);
+ 	return snprintf(buf, PAGE_SIZE, "%d\n", hpf);
+ }
+ 
+ static DEVICE_ATTR(hpf, 0444, dasd_hpf_show, NULL);
+ 
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  static ssize_t dasd_reservation_policy_show(struct device *dev,
  					    struct device_attribute *attr,
  					    char *buf)
@@@ -1337,6 -1474,124 +1510,127 @@@ static ssize_t dasd_reservation_state_s
  static DEVICE_ATTR(last_known_reservation_state, 0644,
  		   dasd_reservation_state_show, dasd_reservation_state_store);
  
++<<<<<<< HEAD
++=======
+ static ssize_t dasd_pm_show(struct device *dev,
+ 			      struct device_attribute *attr, char *buf)
+ {
+ 	struct dasd_device *device;
+ 	u8 opm, nppm, cablepm, cuirpm, hpfpm, ifccpm;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return sprintf(buf, "0\n");
+ 
+ 	opm = dasd_path_get_opm(device);
+ 	nppm = dasd_path_get_nppm(device);
+ 	cablepm = dasd_path_get_cablepm(device);
+ 	cuirpm = dasd_path_get_cuirpm(device);
+ 	hpfpm = dasd_path_get_hpfpm(device);
+ 	ifccpm = dasd_path_get_ifccpm(device);
+ 	dasd_put_device(device);
+ 
+ 	return sprintf(buf, "%02x %02x %02x %02x %02x %02x\n", opm, nppm,
+ 		       cablepm, cuirpm, hpfpm, ifccpm);
+ }
+ 
+ static DEVICE_ATTR(path_masks, 0444, dasd_pm_show, NULL);
+ 
+ /*
+  * threshold value for IFCC/CCC errors
+  */
+ static ssize_t
+ dasd_path_threshold_show(struct device *dev,
+ 			  struct device_attribute *attr, char *buf)
+ {
+ 	struct dasd_device *device;
+ 	int len;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 	len = snprintf(buf, PAGE_SIZE, "%lu\n", device->path_thrhld);
+ 	dasd_put_device(device);
+ 	return len;
+ }
+ 
+ static ssize_t
+ dasd_path_threshold_store(struct device *dev, struct device_attribute *attr,
+ 			   const char *buf, size_t count)
+ {
+ 	struct dasd_device *device;
+ 	unsigned long flags;
+ 	unsigned long val;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 
+ 	if ((kstrtoul(buf, 10, &val) != 0) ||
+ 	    (val > DASD_THRHLD_MAX) || val == 0) {
+ 		dasd_put_device(device);
+ 		return -EINVAL;
+ 	}
+ 	spin_lock_irqsave(get_ccwdev_lock(to_ccwdev(dev)), flags);
+ 	if (val)
+ 		device->path_thrhld = val;
+ 	spin_unlock_irqrestore(get_ccwdev_lock(to_ccwdev(dev)), flags);
+ 	dasd_put_device(device);
+ 	return count;
+ }
+ 
+ static DEVICE_ATTR(path_threshold, 0644, dasd_path_threshold_show,
+ 		   dasd_path_threshold_store);
+ /*
+  * interval for IFCC/CCC checks
+  * meaning time with no IFCC/CCC error before the error counter
+  * gets reset
+  */
+ static ssize_t
+ dasd_path_interval_show(struct device *dev,
+ 			struct device_attribute *attr, char *buf)
+ {
+ 	struct dasd_device *device;
+ 	int len;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 	len = snprintf(buf, PAGE_SIZE, "%lu\n", device->path_interval);
+ 	dasd_put_device(device);
+ 	return len;
+ }
+ 
+ static ssize_t
+ dasd_path_interval_store(struct device *dev, struct device_attribute *attr,
+ 	       const char *buf, size_t count)
+ {
+ 	struct dasd_device *device;
+ 	unsigned long flags;
+ 	unsigned long val;
+ 
+ 	device = dasd_device_from_cdev(to_ccwdev(dev));
+ 	if (IS_ERR(device))
+ 		return -ENODEV;
+ 
+ 	if ((kstrtoul(buf, 10, &val) != 0) ||
+ 	    (val > DASD_INTERVAL_MAX) || val == 0) {
+ 		dasd_put_device(device);
+ 		return -EINVAL;
+ 	}
+ 	spin_lock_irqsave(get_ccwdev_lock(to_ccwdev(dev)), flags);
+ 	if (val)
+ 		device->path_interval = val;
+ 	spin_unlock_irqrestore(get_ccwdev_lock(to_ccwdev(dev)), flags);
+ 	dasd_put_device(device);
+ 	return count;
+ }
+ 
+ static DEVICE_ATTR(path_interval, 0644, dasd_path_interval_show,
+ 		   dasd_path_interval_store);
+ 
+ 
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  static struct attribute * dasd_attrs[] = {
  	&dev_attr_readonly.attr,
  	&dev_attr_discipline.attr,
@@@ -1353,6 -1608,14 +1647,15 @@@
  	&dev_attr_reservation_policy.attr,
  	&dev_attr_last_known_reservation_state.attr,
  	&dev_attr_safe_offline.attr,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_host_access_count.attr,
+ 	&dev_attr_path_masks.attr,
+ 	&dev_attr_path_threshold.attr,
+ 	&dev_attr_path_interval.attr,
+ 	&dev_attr_path_reset.attr,
+ 	&dev_attr_hpf.attr,
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  	NULL,
  };
  
diff --cc drivers/s390/block/dasd_eckd.c
index 75d5d21ad443,67bf50c9946f..000000000000
--- a/drivers/s390/block/dasd_eckd.c
+++ b/drivers/s390/block/dasd_eckd.c
@@@ -1031,8 -1042,11 +1031,16 @@@ static void dasd_eckd_clear_conf_data(s
  	private->conf_data = NULL;
  	private->conf_len = 0;
  	for (i = 0; i < 8; i++) {
++<<<<<<< HEAD
 +		kfree(private->path_conf_data[i]);
 +		private->path_conf_data[i] = NULL;
++=======
+ 		kfree(device->path[i].conf_data);
+ 		device->path[i].conf_data = NULL;
+ 		device->path[i].cssid = 0;
+ 		device->path[i].ssid = 0;
+ 		device->path[i].chpid = 0;
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  	}
  }
  
@@@ -1041,16 -1055,17 +1049,19 @@@ static int dasd_eckd_read_conf(struct d
  {
  	void *conf_data;
  	int conf_len, conf_data_saved;
 -	int rc, path_err, pos;
 +	int rc, path_err;
  	__u8 lpm, opm;
  	struct dasd_eckd_private *private, path_private;
 +	struct dasd_path *path_data;
  	struct dasd_uid *uid;
  	char print_path_uid[60], print_device_uid[60];
+ 	struct channel_path_desc *chp_desc;
+ 	struct subchannel_id sch_id;
  
 -	private = device->private;
 +	private = (struct dasd_eckd_private *) device->private;
 +	path_data = &device->path_data;
  	opm = ccw_device_get_path_mask(device->cdev);
+ 	ccw_device_get_schid(device->cdev, &sch_id);
  	conf_data_saved = 0;
  	path_err = 0;
  	/* get configuration data per operational path */
@@@ -1087,8 -1102,13 +1098,18 @@@
  			}
  			pos = pathmask_to_pos(lpm);
  			/* store per path conf_data */
++<<<<<<< HEAD
 +			private->path_conf_data[pos] =
 +				(struct dasd_conf_data *) conf_data;
++=======
+ 			device->path[pos].conf_data = conf_data;
+ 			device->path[pos].cssid = sch_id.cssid;
+ 			device->path[pos].ssid = sch_id.ssid;
+ 			chp_desc = ccw_device_get_chp_desc(device->cdev, pos);
+ 			if (chp_desc)
+ 				device->path[pos].chpid = chp_desc->chpid;
+ 			kfree(chp_desc);
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  			/*
  			 * build device UID that other path data
  			 * can be compared to it
@@@ -1148,8 -1168,13 +1169,18 @@@
  			}
  			pos = pathmask_to_pos(lpm);
  			/* store per path conf_data */
++<<<<<<< HEAD
 +			private->path_conf_data[pos] =
 +				(struct dasd_conf_data *) conf_data;
++=======
+ 			device->path[pos].conf_data = conf_data;
+ 			device->path[pos].cssid = sch_id.cssid;
+ 			device->path[pos].ssid = sch_id.ssid;
+ 			chp_desc = ccw_device_get_chp_desc(device->cdev, pos);
+ 			if (chp_desc)
+ 				device->path[pos].chpid = chp_desc->chpid;
+ 			kfree(chp_desc);
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  			path_private.conf_data = NULL;
  			path_private.conf_len = 0;
  		}
@@@ -1174,9 -1197,35 +1205,35 @@@
  	return path_err;
  }
  
+ static u32 get_fcx_max_data(struct dasd_device *device)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	int fcx_in_css, fcx_in_gneq, fcx_in_features;
+ 	int tpm, mdc;
+ 
+ 	if (dasd_nofcx)
+ 		return 0;
+ 	/* is transport mode supported? */
+ 	fcx_in_css = css_general_characteristics.fcx;
+ 	fcx_in_gneq = private->gneq->reserved2[7] & 0x04;
+ 	fcx_in_features = private->features.feature[40] & 0x80;
+ 	tpm = fcx_in_css && fcx_in_gneq && fcx_in_features;
+ 
+ 	if (!tpm)
+ 		return 0;
+ 
+ 	mdc = ccw_device_get_mdc(device->cdev, 0);
+ 	if (mdc < 0) {
+ 		dev_warn(&device->cdev->dev, "Detecting the maximum supported data size for zHPF requests failed\n");
+ 		return 0;
+ 	} else {
+ 		return (u32)mdc * FCX_MAX_DATA_FACTOR;
+ 	}
+ }
+ 
  static int verify_fcx_max_data(struct dasd_device *device, __u8 lpm)
  {
 -	struct dasd_eckd_private *private = device->private;
 +	struct dasd_eckd_private *private;
  	int mdc;
  	u32 fcx_max_data;
  
@@@ -1428,8 -1482,22 +1485,21 @@@ static int dasd_eckd_verify_path(struc
  	return 0;
  }
  
+ static void dasd_eckd_reset_path(struct dasd_device *device, __u8 pm)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	unsigned long flags;
+ 
+ 	if (!private->fcx_max_data)
+ 		private->fcx_max_data = get_fcx_max_data(device);
+ 	spin_lock_irqsave(get_ccwdev_lock(device->cdev), flags);
+ 	dasd_path_set_tbvpm(device, pm ? : dasd_path_get_notoperpm(device));
+ 	dasd_schedule_device_bh(device);
+ 	spin_unlock_irqrestore(get_ccwdev_lock(device->cdev), flags);
+ }
+ 
  static int dasd_eckd_read_features(struct dasd_device *device)
  {
 -	struct dasd_eckd_private *private = device->private;
  	struct dasd_psf_prssd_data *prssdp;
  	struct dasd_rssd_features *features;
  	struct dasd_ccw_req *cqr;
@@@ -1627,37 -1691,6 +1697,40 @@@ static void dasd_eckd_kick_validate_ser
  		dasd_put_device(device);
  }
  
++<<<<<<< HEAD
 +static u32 get_fcx_max_data(struct dasd_device *device)
 +{
 +#if defined(CONFIG_64BIT)
 +	int tpm, mdc;
 +	int fcx_in_css, fcx_in_gneq, fcx_in_features;
 +	struct dasd_eckd_private *private;
 +
 +	if (dasd_nofcx)
 +		return 0;
 +	/* is transport mode supported? */
 +	private = (struct dasd_eckd_private *) device->private;
 +	fcx_in_css = css_general_characteristics.fcx;
 +	fcx_in_gneq = private->gneq->reserved2[7] & 0x04;
 +	fcx_in_features = private->features.feature[40] & 0x80;
 +	tpm = fcx_in_css && fcx_in_gneq && fcx_in_features;
 +
 +	if (!tpm)
 +		return 0;
 +
 +	mdc = ccw_device_get_mdc(device->cdev, 0);
 +	if (mdc < 0) {
 +		dev_warn(&device->cdev->dev, "Detecting the maximum supported"
 +			 " data size for zHPF requests failed\n");
 +		return 0;
 +	} else
 +		return mdc * FCX_MAX_DATA_FACTOR;
 +#else
 +	return 0;
 +#endif
 +}
 +
++=======
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  /*
   * Check device characteristics.
   * If the device is accessible using ECKD discipline, the device is enabled.
@@@ -1708,8 -1742,12 +1781,15 @@@ dasd_eckd_check_characteristics(struct 
  	if (rc)
  		goto out_err1;
  
- 	/* set default timeout */
+ 	/* set some default values */
  	device->default_expires = DASD_EXPIRES;
++<<<<<<< HEAD
++=======
+ 	device->default_retries = DASD_RETRIES;
+ 	device->path_thrhld = DASD_ECKD_PATH_THRHLD;
+ 	device->path_interval = DASD_ECKD_PATH_INTERVAL;
+ 
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  	if (private->gneq) {
  		value = 1;
  		for (i = 0; i < private->gneq->timeout.value; i++)
@@@ -1814,6 -1852,18 +1894,21 @@@ static void dasd_eckd_uncheck_device(st
  	private->vdsneq = NULL;
  	private->gneq = NULL;
  	private->conf_len = 0;
++<<<<<<< HEAD
++=======
+ 	for (i = 0; i < 8; i++) {
+ 		kfree(device->path[i].conf_data);
+ 		if ((__u8 *)device->path[i].conf_data ==
+ 		    private->conf_data) {
+ 			private->conf_data = NULL;
+ 			private->conf_len = 0;
+ 		}
+ 		device->path[i].conf_data = NULL;
+ 		device->path[i].cssid = 0;
+ 		device->path[i].ssid = 0;
+ 		device->path[i].chpid = 0;
+ 	}
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  	kfree(private->conf_data);
  	private->conf_data = NULL;
  }
@@@ -4484,6 -5076,695 +4580,698 @@@ out_err
  	return -1;
  }
  
++<<<<<<< HEAD
++=======
+ static int dasd_eckd_read_message_buffer(struct dasd_device *device,
+ 					 struct dasd_rssd_messages *messages,
+ 					 __u8 lpum)
+ {
+ 	struct dasd_rssd_messages *message_buf;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   (sizeof(struct dasd_psf_prssd_data) +
+ 				    sizeof(struct dasd_rssd_messages)),
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	cqr->lpm = lpum;
+ retry:
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->expires = 10 * HZ;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 	/* dasd_sleep_on_immediatly does not do complex error
+ 	 * recovery so clear erp flag and set retry counter to
+ 	 * do basic erp */
+ 	clear_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags);
+ 	cqr->retries = 256;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = 0x03;	/* Message Buffer */
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - message buffer */
+ 	message_buf = (struct dasd_rssd_messages *) (prssdp + 1);
+ 	memset(message_buf, 0, sizeof(struct dasd_rssd_messages));
+ 
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_rssd_messages);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) message_buf;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on_immediatly(cqr);
+ 	if (rc == 0) {
+ 		prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 		message_buf = (struct dasd_rssd_messages *)
+ 			(prssdp + 1);
+ 		memcpy(messages, message_buf,
+ 		       sizeof(struct dasd_rssd_messages));
+ 	} else if (cqr->lpm) {
+ 		/*
+ 		 * on z/VM we might not be able to do I/O on the requested path
+ 		 * but instead we get the required information on any path
+ 		 * so retry with open path mask
+ 		 */
+ 		cqr->lpm = 0;
+ 		goto retry;
+ 	} else
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading messages failed with rc=%d\n"
+ 				, rc);
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ static int dasd_eckd_query_host_access(struct dasd_device *device,
+ 				       struct dasd_psf_query_host_access *data)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct dasd_psf_query_host_access *host_access;
+ 	struct dasd_psf_prssd_data *prssdp;
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	/* not available for HYPER PAV alias devices */
+ 	if (!device->block && private->lcu->pav == HYPER_PAV)
+ 		return -EOPNOTSUPP;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */	+ 1 /* RSSD */,
+ 				   sizeof(struct dasd_psf_prssd_data) + 1,
+ 				   device);
+ 	if (IS_ERR(cqr)) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate read message buffer request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 	host_access = kzalloc(sizeof(*host_access), GFP_KERNEL | GFP_DMA);
+ 	if (!host_access) {
+ 		dasd_sfree_request(cqr, device);
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate host_access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10 * HZ;
+ 
+ 	/* Prepare for Read Subsystem Data */
+ 	prssdp = (struct dasd_psf_prssd_data *) cqr->data;
+ 	memset(prssdp, 0, sizeof(struct dasd_psf_prssd_data));
+ 	prssdp->order = PSF_ORDER_PRSSD;
+ 	prssdp->suborder = PSF_SUBORDER_QHA;	/* query host access */
+ 	/* LSS and Volume that will be queried */
+ 	prssdp->lss = private->ned->ID;
+ 	prssdp->volume = private->ned->unit_addr;
+ 	/* all other bytes of prssdp must be zero */
+ 
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->count = sizeof(struct dasd_psf_prssd_data);
+ 	ccw->flags |= CCW_FLAG_CC;
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) prssdp;
+ 
+ 	/* Read Subsystem Data - query host access */
+ 	ccw++;
+ 	ccw->cmd_code = DASD_ECKD_CCW_RSSD;
+ 	ccw->count = sizeof(struct dasd_psf_query_host_access);
+ 	ccw->flags |= CCW_FLAG_SLI;
+ 	ccw->cda = (__u32)(addr_t) host_access;
+ 
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	rc = dasd_sleep_on_interruptible(cqr);
+ 	if (rc == 0) {
+ 		*data = *host_access;
+ 	} else {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev,
+ 				"Reading host access data failed with rc=%d\n",
+ 				rc);
+ 		rc = -EOPNOTSUPP;
+ 	}
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	kfree(host_access);
+ 	return rc;
+ }
+ /*
+  * return number of grouped devices
+  */
+ static int dasd_eckd_host_access_count(struct dasd_device *device)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	int count = 0;
+ 	int rc, i;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		if (entry->status_flags & DASD_ECKD_PG_GROUPED)
+ 			count++;
+ 	}
+ 
+ 	kfree(access);
+ 	return count;
+ }
+ 
+ /*
+  * write host access information to a sequential file
+  */
+ static int dasd_hosts_print(struct dasd_device *device, struct seq_file *m)
+ {
+ 	struct dasd_psf_query_host_access *access;
+ 	struct dasd_ckd_path_group_entry *entry;
+ 	struct dasd_ckd_host_information *info;
+ 	char sysplex[9] = "";
+ 	int rc, i, j;
+ 
+ 	access = kzalloc(sizeof(*access), GFP_NOIO);
+ 	if (!access) {
+ 		DBF_EVENT_DEVID(DBF_WARNING, device->cdev, "%s",
+ 				"Could not allocate access buffer");
+ 		return -ENOMEM;
+ 	}
+ 	rc = dasd_eckd_query_host_access(device, access);
+ 	if (rc) {
+ 		kfree(access);
+ 		return rc;
+ 	}
+ 
+ 	info = (struct dasd_ckd_host_information *)
+ 		access->host_access_information;
+ 	for (i = 0; i < info->entry_count; i++) {
+ 		entry = (struct dasd_ckd_path_group_entry *)
+ 			(info->entry + i * info->entry_size);
+ 		/* PGID */
+ 		seq_puts(m, "pgid ");
+ 		for (j = 0; j < 11; j++)
+ 			seq_printf(m, "%02x", entry->pgid[j]);
+ 		seq_putc(m, '\n');
+ 		/* FLAGS */
+ 		seq_printf(m, "status_flags %02x\n", entry->status_flags);
+ 		/* SYSPLEX NAME */
+ 		memcpy(&sysplex, &entry->sysplex_name, sizeof(sysplex) - 1);
+ 		EBCASC(sysplex, sizeof(sysplex));
+ 		seq_printf(m, "sysplex_name %8s\n", sysplex);
+ 		/* SUPPORTED CYLINDER */
+ 		seq_printf(m, "supported_cylinder %d\n", entry->cylinder);
+ 		/* TIMESTAMP */
+ 		seq_printf(m, "timestamp %lu\n", (unsigned long)
+ 			   entry->timestamp);
+ 	}
+ 	kfree(access);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Perform Subsystem Function - CUIR response
+  */
+ static int
+ dasd_eckd_psf_cuir_response(struct dasd_device *device, int response,
+ 			    __u32 message_id, __u8 lpum)
+ {
+ 	struct dasd_psf_cuir_response *psf_cuir;
+ 	int pos = pathmask_to_pos(lpum);
+ 	struct dasd_ccw_req *cqr;
+ 	struct ccw1 *ccw;
+ 	int rc;
+ 
+ 	cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 1 /* PSF */ ,
+ 				  sizeof(struct dasd_psf_cuir_response),
+ 				  device);
+ 
+ 	if (IS_ERR(cqr)) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			   "Could not allocate PSF-CUIR request");
+ 		return PTR_ERR(cqr);
+ 	}
+ 
+ 	psf_cuir = (struct dasd_psf_cuir_response *)cqr->data;
+ 	psf_cuir->order = PSF_ORDER_CUIR_RESPONSE;
+ 	psf_cuir->cc = response;
+ 	psf_cuir->chpid = device->path[pos].chpid;
+ 	psf_cuir->message_id = message_id;
+ 	psf_cuir->cssid = device->path[pos].cssid;
+ 	psf_cuir->ssid = device->path[pos].ssid;
+ 	ccw = cqr->cpaddr;
+ 	ccw->cmd_code = DASD_ECKD_CCW_PSF;
+ 	ccw->cda = (__u32)(addr_t)psf_cuir;
+ 	ccw->flags = CCW_FLAG_SLI;
+ 	ccw->count = sizeof(struct dasd_psf_cuir_response);
+ 
+ 	cqr->startdev = device;
+ 	cqr->memdev = device;
+ 	cqr->block = NULL;
+ 	cqr->retries = 256;
+ 	cqr->expires = 10*HZ;
+ 	cqr->buildclk = get_tod_clock();
+ 	cqr->status = DASD_CQR_FILLED;
+ 	set_bit(DASD_CQR_VERIFY_PATH, &cqr->flags);
+ 
+ 	rc = dasd_sleep_on(cqr);
+ 
+ 	dasd_sfree_request(cqr, cqr->memdev);
+ 	return rc;
+ }
+ 
+ /*
+  * return configuration data that is referenced by record selector
+  * if a record selector is specified or per default return the
+  * conf_data pointer for the path specified by lpum
+  */
+ static struct dasd_conf_data *dasd_eckd_get_ref_conf(struct dasd_device *device,
+ 						     __u8 lpum,
+ 						     struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_conf_data *conf_data;
+ 	int path, pos;
+ 
+ 	if (cuir->record_selector == 0)
+ 		goto out;
+ 	for (path = 0x80, pos = 0; path; path >>= 1, pos++) {
+ 		conf_data = device->path[pos].conf_data;
+ 		if (conf_data->gneq.record_selector ==
+ 		    cuir->record_selector)
+ 			return conf_data;
+ 	}
+ out:
+ 	return device->path[pathmask_to_pos(lpum)].conf_data;
+ }
+ 
+ /*
+  * This function determines the scope of a reconfiguration request by
+  * analysing the path and device selection data provided in the CUIR request.
+  * Returns a path mask containing CUIR affected paths for the give device.
+  *
+  * If the CUIR request does not contain the required information return the
+  * path mask of the path the attention message for the CUIR request was reveived
+  * on.
+  */
+ static int dasd_eckd_cuir_scope(struct dasd_device *device, __u8 lpum,
+ 				struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_conf_data *ref_conf_data;
+ 	unsigned long bitmask = 0, mask = 0;
+ 	struct dasd_conf_data *conf_data;
+ 	unsigned int pos, path;
+ 	char *ref_gneq, *gneq;
+ 	char *ref_ned, *ned;
+ 	int tbcpm = 0;
+ 
+ 	/* if CUIR request does not specify the scope use the path
+ 	   the attention message was presented on */
+ 	if (!cuir->ned_map ||
+ 	    !(cuir->neq_map[0] | cuir->neq_map[1] | cuir->neq_map[2]))
+ 		return lpum;
+ 
+ 	/* get reference conf data */
+ 	ref_conf_data = dasd_eckd_get_ref_conf(device, lpum, cuir);
+ 	/* reference ned is determined by ned_map field */
+ 	pos = 8 - ffs(cuir->ned_map);
+ 	ref_ned = (char *)&ref_conf_data->neds[pos];
+ 	ref_gneq = (char *)&ref_conf_data->gneq;
+ 	/* transfer 24 bit neq_map to mask */
+ 	mask = cuir->neq_map[2];
+ 	mask |= cuir->neq_map[1] << 8;
+ 	mask |= cuir->neq_map[0] << 16;
+ 
+ 	for (path = 0; path < 8; path++) {
+ 		/* initialise data per path */
+ 		bitmask = mask;
+ 		conf_data = device->path[path].conf_data;
+ 		pos = 8 - ffs(cuir->ned_map);
+ 		ned = (char *) &conf_data->neds[pos];
+ 		/* compare reference ned and per path ned */
+ 		if (memcmp(ref_ned, ned, sizeof(*ned)) != 0)
+ 			continue;
+ 		gneq = (char *)&conf_data->gneq;
+ 		/* compare reference gneq and per_path gneq under
+ 		   24 bit mask where mask bit 0 equals byte 7 of
+ 		   the gneq and mask bit 24 equals byte 31 */
+ 		while (bitmask) {
+ 			pos = ffs(bitmask) - 1;
+ 			if (memcmp(&ref_gneq[31 - pos], &gneq[31 - pos], 1)
+ 			    != 0)
+ 				break;
+ 			clear_bit(pos, &bitmask);
+ 		}
+ 		if (bitmask)
+ 			continue;
+ 		/* device and path match the reference values
+ 		   add path to CUIR scope */
+ 		tbcpm |= 0x80 >> path;
+ 	}
+ 	return tbcpm;
+ }
+ 
+ static void dasd_eckd_cuir_notify_user(struct dasd_device *device,
+ 				       unsigned long paths, int action)
+ {
+ 	int pos;
+ 
+ 	while (paths) {
+ 		/* get position of bit in mask */
+ 		pos = 8 - ffs(paths);
+ 		/* get channel path descriptor from this position */
+ 		if (action == CUIR_QUIESCE)
+ 			pr_warn("Service on the storage server caused path %x.%02x to go offline",
+ 				device->path[pos].cssid,
+ 				device->path[pos].chpid);
+ 		else if (action == CUIR_RESUME)
+ 			pr_info("Path %x.%02x is back online after service on the storage server",
+ 				device->path[pos].cssid,
+ 				device->path[pos].chpid);
+ 		clear_bit(7 - pos, &paths);
+ 	}
+ }
+ 
+ static int dasd_eckd_cuir_remove_path(struct dasd_device *device, __u8 lpum,
+ 				      struct dasd_cuir_message *cuir)
+ {
+ 	unsigned long tbcpm;
+ 
+ 	tbcpm = dasd_eckd_cuir_scope(device, lpum, cuir);
+ 	/* nothing to do if path is not in use */
+ 	if (!(dasd_path_get_opm(device) & tbcpm))
+ 		return 0;
+ 	if (!(dasd_path_get_opm(device) & ~tbcpm)) {
+ 		/* no path would be left if the CUIR action is taken
+ 		   return error */
+ 		return -EINVAL;
+ 	}
+ 	/* remove device from operational path mask */
+ 	dasd_path_remove_opm(device, tbcpm);
+ 	dasd_path_add_cuirpm(device, tbcpm);
+ 	return tbcpm;
+ }
+ 
+ /*
+  * walk through all devices and build a path mask to quiesce them
+  * return an error if the last path to a device would be removed
+  *
+  * if only part of the devices are quiesced and an error
+  * occurs no onlining necessary, the storage server will
+  * notify the already set offline devices again
+  */
+ static int dasd_eckd_cuir_quiesce(struct dasd_device *device, __u8 lpum,
+ 				  struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	unsigned long flags;
+ 	int tbcpm;
+ 
+ 	/* active devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->active_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* inactive devices */
+ 	list_for_each_entry_safe(dev, n, &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 		tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 		spin_unlock_irqrestore(get_ccwdev_lock(dev->cdev), flags);
+ 		if (tbcpm < 0)
+ 			goto out_err;
+ 		paths |= tbcpm;
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist, group) {
+ 		list_for_each_entry_safe(dev, n, &pavgroup->baselist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 		list_for_each_entry_safe(dev, n, &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			spin_lock_irqsave(get_ccwdev_lock(dev->cdev), flags);
+ 			tbcpm = dasd_eckd_cuir_remove_path(dev, lpum, cuir);
+ 			spin_unlock_irqrestore(
+ 				get_ccwdev_lock(dev->cdev), flags);
+ 			if (tbcpm < 0)
+ 				goto out_err;
+ 			paths |= tbcpm;
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, CUIR_QUIESCE);
+ 	return 0;
+ out_err:
+ 	return tbcpm;
+ }
+ 
+ static int dasd_eckd_cuir_resume(struct dasd_device *device, __u8 lpum,
+ 				 struct dasd_cuir_message *cuir)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 	struct alias_pav_group *pavgroup, *tempgroup;
+ 	struct dasd_device *dev, *n;
+ 	unsigned long paths = 0;
+ 	int tbcpm;
+ 
+ 	/*
+ 	 * the path may have been added through a generic path event before
+ 	 * only trigger path verification if the path is not already in use
+ 	 */
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->active_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 			dasd_path_add_tbvpm(dev, tbcpm);
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	list_for_each_entry_safe(dev, n,
+ 				 &private->lcu->inactive_devices,
+ 				 alias_list) {
+ 		tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 		paths |= tbcpm;
+ 		if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 			dasd_path_add_tbvpm(dev, tbcpm);
+ 			dasd_schedule_device_bh(dev);
+ 		}
+ 	}
+ 	/* devices in PAV groups */
+ 	list_for_each_entry_safe(pavgroup, tempgroup,
+ 				 &private->lcu->grouplist,
+ 				 group) {
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->baselist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 				dasd_path_add_tbvpm(dev, tbcpm);
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 		list_for_each_entry_safe(dev, n,
+ 					 &pavgroup->aliaslist,
+ 					 alias_list) {
+ 			tbcpm = dasd_eckd_cuir_scope(dev, lpum, cuir);
+ 			paths |= tbcpm;
+ 			if (!(dasd_path_get_opm(dev) & tbcpm)) {
+ 				dasd_path_add_tbvpm(dev, tbcpm);
+ 				dasd_schedule_device_bh(dev);
+ 			}
+ 		}
+ 	}
+ 	/* notify user about all paths affected by CUIR action */
+ 	dasd_eckd_cuir_notify_user(device, paths, CUIR_RESUME);
+ 	return 0;
+ }
+ 
+ static void dasd_eckd_handle_cuir(struct dasd_device *device, void *messages,
+ 				 __u8 lpum)
+ {
+ 	struct dasd_cuir_message *cuir = messages;
+ 	int response;
+ 
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR request: %016llx %016llx %016llx %08x",
+ 		      ((u64 *)cuir)[0], ((u64 *)cuir)[1], ((u64 *)cuir)[2],
+ 		      ((u32 *)cuir)[3]);
+ 
+ 	if (cuir->code == CUIR_QUIESCE) {
+ 		/* quiesce */
+ 		if (dasd_eckd_cuir_quiesce(device, lpum, cuir))
+ 			response = PSF_CUIR_LAST_PATH;
+ 		else
+ 			response = PSF_CUIR_COMPLETED;
+ 	} else if (cuir->code == CUIR_RESUME) {
+ 		/* resume */
+ 		dasd_eckd_cuir_resume(device, lpum, cuir);
+ 		response = PSF_CUIR_COMPLETED;
+ 	} else
+ 		response = PSF_CUIR_NOT_SUPPORTED;
+ 
+ 	dasd_eckd_psf_cuir_response(device, response,
+ 				    cuir->message_id, lpum);
+ 	DBF_DEV_EVENT(DBF_WARNING, device,
+ 		      "CUIR response: %d on message ID %08x", response,
+ 		      cuir->message_id);
+ 	/* to make sure there is no attention left schedule work again */
+ 	device->discipline->check_attention(device, lpum);
+ }
+ 
+ static void dasd_eckd_check_attention_work(struct work_struct *work)
+ {
+ 	struct check_attention_work_data *data;
+ 	struct dasd_rssd_messages *messages;
+ 	struct dasd_device *device;
+ 	int rc;
+ 
+ 	data = container_of(work, struct check_attention_work_data, worker);
+ 	device = data->device;
+ 	messages = kzalloc(sizeof(*messages), GFP_KERNEL);
+ 	if (!messages) {
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			      "Could not allocate attention message buffer");
+ 		goto out;
+ 	}
+ 	rc = dasd_eckd_read_message_buffer(device, messages, data->lpum);
+ 	if (rc)
+ 		goto out;
+ 	if (messages->length == ATTENTION_LENGTH_CUIR &&
+ 	    messages->format == ATTENTION_FORMAT_CUIR)
+ 		dasd_eckd_handle_cuir(device, messages, data->lpum);
+ out:
+ 	dasd_put_device(device);
+ 	kfree(messages);
+ 	kfree(data);
+ }
+ 
+ static int dasd_eckd_check_attention(struct dasd_device *device, __u8 lpum)
+ {
+ 	struct check_attention_work_data *data;
+ 
+ 	data = kzalloc(sizeof(*data), GFP_ATOMIC);
+ 	if (!data)
+ 		return -ENOMEM;
+ 	INIT_WORK(&data->worker, dasd_eckd_check_attention_work);
+ 	dasd_get_device(device);
+ 	data->device = device;
+ 	data->lpum = lpum;
+ 	schedule_work(&data->worker);
+ 	return 0;
+ }
+ 
+ static int dasd_eckd_disable_hpf_path(struct dasd_device *device, __u8 lpum)
+ {
+ 	if (~lpum & dasd_path_get_opm(device)) {
+ 		dasd_path_add_nohpfpm(device, lpum);
+ 		dasd_path_remove_opm(device, lpum);
+ 		dev_err(&device->cdev->dev,
+ 			"Channel path %02X lost HPF functionality and is disabled\n",
+ 			lpum);
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static void dasd_eckd_disable_hpf_device(struct dasd_device *device)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 
+ 	dev_err(&device->cdev->dev,
+ 		"High Performance FICON disabled\n");
+ 	private->fcx_max_data = 0;
+ }
+ 
+ static int dasd_eckd_hpf_enabled(struct dasd_device *device)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 
+ 	return private->fcx_max_data ? 1 : 0;
+ }
+ 
+ static void dasd_eckd_handle_hpf_error(struct dasd_device *device,
+ 				       struct irb *irb)
+ {
+ 	struct dasd_eckd_private *private = device->private;
+ 
+ 	if (!private->fcx_max_data) {
+ 		/* sanity check for no HPF, the error makes no sense */
+ 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
+ 			      "Trying to disable HPF for a non HPF device");
+ 		return;
+ 	}
+ 	if (irb->scsw.tm.sesq == SCSW_SESQ_DEV_NOFCX) {
+ 		dasd_eckd_disable_hpf_device(device);
+ 	} else if (irb->scsw.tm.sesq == SCSW_SESQ_PATH_NOFCX) {
+ 		if (dasd_eckd_disable_hpf_path(device, irb->esw.esw1.lpum))
+ 			return;
+ 		dasd_eckd_disable_hpf_device(device);
+ 		dasd_path_set_tbvpm(device,
+ 				  dasd_path_get_hpfpm(device));
+ 	}
+ 	/*
+ 	 * prevent that any new I/O ist started on the device and schedule a
+ 	 * requeue of existing requests
+ 	 */
+ 	dasd_device_set_stop_bits(device, DASD_STOPPED_NOT_ACC);
+ 	dasd_schedule_requeue(device);
+ }
+ 
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  static struct ccw_driver dasd_eckd_driver = {
  	.driver = {
  		.name	= "dasd-eckd",
@@@ -4548,6 -5830,13 +5336,16 @@@ static struct dasd_discipline dasd_eckd
  	.reload = dasd_eckd_reload_device,
  	.get_uid = dasd_eckd_get_uid,
  	.kick_validate = dasd_eckd_kick_validate_server,
++<<<<<<< HEAD
++=======
+ 	.check_attention = dasd_eckd_check_attention,
+ 	.host_access_count = dasd_eckd_host_access_count,
+ 	.hosts_print = dasd_hosts_print,
+ 	.handle_hpf_error = dasd_eckd_handle_hpf_error,
+ 	.disable_hpf = dasd_eckd_disable_hpf_device,
+ 	.hpf_enabled = dasd_eckd_hpf_enabled,
+ 	.reset_path = dasd_eckd_reset_path,
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  };
  
  static int __init
diff --cc drivers/s390/block/dasd_int.h
index aa498f7fe95b,24be210c10e5..000000000000
--- a/drivers/s390/block/dasd_int.h
+++ b/drivers/s390/block/dasd_int.h
@@@ -355,6 -375,13 +355,16 @@@ struct dasd_discipline 
  
  	int (*get_uid) (struct dasd_device *, struct dasd_uid *);
  	void (*kick_validate) (struct dasd_device *);
++<<<<<<< HEAD
++=======
+ 	int (*check_attention)(struct dasd_device *, __u8);
+ 	int (*host_access_count)(struct dasd_device *);
+ 	int (*hosts_print)(struct dasd_device *, struct seq_file *);
+ 	void (*handle_hpf_error)(struct dasd_device *, struct irb *);
+ 	void (*disable_hpf)(struct dasd_device *);
+ 	int (*hpf_enabled)(struct dasd_device *);
+ 	void (*reset_path)(struct dasd_device *, __u8);
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  };
  
  extern struct dasd_discipline *dasd_diag_discipline_pointer;
@@@ -375,13 -402,31 +385,38 @@@
  #define DASD_EER_STATECHANGE 3
  #define DASD_EER_PPRCSUSPEND 4
  
++<<<<<<< HEAD
 +struct dasd_path {
 +	__u8 opm;
 +	__u8 tbvpm;
 +	__u8 ppm;
 +	__u8 npm;
++=======
+ /* DASD path handling */
+ 
+ #define DASD_PATH_OPERATIONAL  1
+ #define DASD_PATH_TBV	       2
+ #define DASD_PATH_PP	       3
+ #define DASD_PATH_NPP	       4
+ #define DASD_PATH_MISCABLED    5
+ #define DASD_PATH_NOHPF        6
+ #define DASD_PATH_CUIR	       7
+ #define DASD_PATH_IFCC	       8
+ 
+ #define DASD_THRHLD_MAX		4294967295U
+ #define DASD_INTERVAL_MAX	4294967295U
+ 
+ struct dasd_path {
+ 	unsigned long flags;
+ 	u8 cssid;
+ 	u8 ssid;
+ 	u8 chpid;
+ 	struct dasd_conf_data *conf_data;
+ 	atomic_t error_count;
+ 	unsigned long long errorclk;
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  };
  
 -
  struct dasd_profile_info {
  	/* legacy part of profile data, as in dasd_profile_info_t */
  	unsigned int dasd_io_reqs;	 /* number of requests processed */
@@@ -456,6 -502,8 +491,11 @@@ struct dasd_device 
  	struct work_struct restore_device;
  	struct work_struct reload_device;
  	struct work_struct kick_validate;
++<<<<<<< HEAD
++=======
+ 	struct work_struct suc_work;
+ 	struct work_struct requeue_requests;
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  	struct timer_list timer;
  
  	debug_info_t *debug_area;
@@@ -467,8 -515,15 +507,11 @@@
  
  	/* default expiration time in s */
  	unsigned long default_expires;
 -	unsigned long default_retries;
 -
 -	unsigned long blk_timeout;
  
+ 	unsigned long path_thrhld;
+ 	unsigned long path_interval;
+ 
  	struct dentry *debugfs_dentry;
 -	struct dentry *hosts_dentry;
  	struct dasd_profile profile;
  };
  
@@@ -794,4 -860,410 +838,413 @@@ static inline int dasd_eer_enabled(stru
  #define dasd_eer_enabled(d)	(0)
  #endif	/* CONFIG_DASD_ERR */
  
++<<<<<<< HEAD
++=======
+ 
+ /* DASD path handling functions */
+ 
+ /*
+  * helper functions to modify bit masks for a given channel path for a device
+  */
+ static inline int dasd_path_is_operational(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_OPERATIONAL, &device->path[chp].flags);
+ }
+ 
+ static inline int dasd_path_need_verify(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_TBV, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_verify(struct dasd_device *device, int chp)
+ {
+ 	__set_bit(DASD_PATH_TBV, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_verify(struct dasd_device *device, int chp)
+ {
+ 	__clear_bit(DASD_PATH_TBV, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_all_verify(struct dasd_device *device)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		dasd_path_clear_verify(device, chp);
+ }
+ 
+ static inline void dasd_path_operational(struct dasd_device *device, int chp)
+ {
+ 	__set_bit(DASD_PATH_OPERATIONAL, &device->path[chp].flags);
+ 	device->opm |= (0x80 >> chp);
+ }
+ 
+ static inline void dasd_path_nonpreferred(struct dasd_device *device, int chp)
+ {
+ 	__set_bit(DASD_PATH_NPP, &device->path[chp].flags);
+ }
+ 
+ static inline int dasd_path_is_nonpreferred(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_NPP, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_nonpreferred(struct dasd_device *device,
+ 						int chp)
+ {
+ 	__clear_bit(DASD_PATH_NPP, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_preferred(struct dasd_device *device, int chp)
+ {
+ 	__set_bit(DASD_PATH_PP, &device->path[chp].flags);
+ }
+ 
+ static inline int dasd_path_is_preferred(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_PP, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_preferred(struct dasd_device *device,
+ 					     int chp)
+ {
+ 	__clear_bit(DASD_PATH_PP, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_oper(struct dasd_device *device, int chp)
+ {
+ 	__clear_bit(DASD_PATH_OPERATIONAL, &device->path[chp].flags);
+ 	device->opm &= ~(0x80 >> chp);
+ }
+ 
+ static inline void dasd_path_clear_cable(struct dasd_device *device, int chp)
+ {
+ 	__clear_bit(DASD_PATH_MISCABLED, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_cuir(struct dasd_device *device, int chp)
+ {
+ 	__set_bit(DASD_PATH_CUIR, &device->path[chp].flags);
+ }
+ 
+ static inline int dasd_path_is_cuir(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_CUIR, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_cuir(struct dasd_device *device, int chp)
+ {
+ 	__clear_bit(DASD_PATH_CUIR, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_ifcc(struct dasd_device *device, int chp)
+ {
+ 	set_bit(DASD_PATH_IFCC, &device->path[chp].flags);
+ }
+ 
+ static inline int dasd_path_is_ifcc(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_IFCC, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_ifcc(struct dasd_device *device, int chp)
+ {
+ 	clear_bit(DASD_PATH_IFCC, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_clear_nohpf(struct dasd_device *device, int chp)
+ {
+ 	__clear_bit(DASD_PATH_NOHPF, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_miscabled(struct dasd_device *device, int chp)
+ {
+ 	__set_bit(DASD_PATH_MISCABLED, &device->path[chp].flags);
+ }
+ 
+ static inline int dasd_path_is_miscabled(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_MISCABLED, &device->path[chp].flags);
+ }
+ 
+ static inline void dasd_path_nohpf(struct dasd_device *device, int chp)
+ {
+ 	__set_bit(DASD_PATH_NOHPF, &device->path[chp].flags);
+ }
+ 
+ static inline int dasd_path_is_nohpf(struct dasd_device *device, int chp)
+ {
+ 	return test_bit(DASD_PATH_NOHPF, &device->path[chp].flags);
+ }
+ 
+ /*
+  * get functions for path masks
+  * will return a path masks for the given device
+  */
+ 
+ static inline __u8 dasd_path_get_opm(struct dasd_device *device)
+ {
+ 	return device->opm;
+ }
+ 
+ static inline __u8 dasd_path_get_tbvpm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 tbvpm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (dasd_path_need_verify(device, chp))
+ 			tbvpm |= 0x80 >> chp;
+ 	return tbvpm;
+ }
+ 
+ static inline __u8 dasd_path_get_nppm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 npm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++) {
+ 		if (dasd_path_is_nonpreferred(device, chp))
+ 			npm |= 0x80 >> chp;
+ 	}
+ 	return npm;
+ }
+ 
+ static inline __u8 dasd_path_get_ppm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 ppm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (dasd_path_is_preferred(device, chp))
+ 			ppm |= 0x80 >> chp;
+ 	return ppm;
+ }
+ 
+ static inline __u8 dasd_path_get_cablepm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 cablepm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (dasd_path_is_miscabled(device, chp))
+ 			cablepm |= 0x80 >> chp;
+ 	return cablepm;
+ }
+ 
+ static inline __u8 dasd_path_get_cuirpm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 cuirpm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (dasd_path_is_cuir(device, chp))
+ 			cuirpm |= 0x80 >> chp;
+ 	return cuirpm;
+ }
+ 
+ static inline __u8 dasd_path_get_ifccpm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 ifccpm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (dasd_path_is_ifcc(device, chp))
+ 			ifccpm |= 0x80 >> chp;
+ 	return ifccpm;
+ }
+ 
+ static inline __u8 dasd_path_get_hpfpm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 hpfpm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (dasd_path_is_nohpf(device, chp))
+ 			hpfpm |= 0x80 >> chp;
+ 	return hpfpm;
+ }
+ 
+ /*
+  * add functions for path masks
+  * the existing path mask will be extended by the given path mask
+  */
+ static inline void dasd_path_add_tbvpm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_verify(device, chp);
+ }
+ 
+ static inline __u8 dasd_path_get_notoperpm(struct dasd_device *device)
+ {
+ 	int chp;
+ 	__u8 nopm = 0x00;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (dasd_path_is_nohpf(device, chp) ||
+ 		    dasd_path_is_ifcc(device, chp) ||
+ 		    dasd_path_is_cuir(device, chp) ||
+ 		    dasd_path_is_miscabled(device, chp))
+ 			nopm |= 0x80 >> chp;
+ 	return nopm;
+ }
+ 
+ static inline void dasd_path_add_opm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp)) {
+ 			dasd_path_operational(device, chp);
+ 			/*
+ 			 * if the path is used
+ 			 * it should not be in one of the negative lists
+ 			 */
+ 			dasd_path_clear_nohpf(device, chp);
+ 			dasd_path_clear_cuir(device, chp);
+ 			dasd_path_clear_cable(device, chp);
+ 			dasd_path_clear_ifcc(device, chp);
+ 		}
+ }
+ 
+ static inline void dasd_path_add_cablepm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_miscabled(device, chp);
+ }
+ 
+ static inline void dasd_path_add_cuirpm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_cuir(device, chp);
+ }
+ 
+ static inline void dasd_path_add_ifccpm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_ifcc(device, chp);
+ }
+ 
+ static inline void dasd_path_add_nppm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_nonpreferred(device, chp);
+ }
+ 
+ static inline void dasd_path_add_nohpfpm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_nohpf(device, chp);
+ }
+ 
+ static inline void dasd_path_add_ppm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_preferred(device, chp);
+ }
+ 
+ /*
+  * set functions for path masks
+  * the existing path mask will be replaced by the given path mask
+  */
+ static inline void dasd_path_set_tbvpm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_verify(device, chp);
+ 		else
+ 			dasd_path_clear_verify(device, chp);
+ }
+ 
+ static inline void dasd_path_set_opm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++) {
+ 		dasd_path_clear_oper(device, chp);
+ 		if (pm & (0x80 >> chp)) {
+ 			dasd_path_operational(device, chp);
+ 			/*
+ 			 * if the path is used
+ 			 * it should not be in one of the negative lists
+ 			 */
+ 			dasd_path_clear_nohpf(device, chp);
+ 			dasd_path_clear_cuir(device, chp);
+ 			dasd_path_clear_cable(device, chp);
+ 			dasd_path_clear_ifcc(device, chp);
+ 		}
+ 	}
+ }
+ 
+ /*
+  * remove functions for path masks
+  * the existing path mask will be cleared with the given path mask
+  */
+ static inline void dasd_path_remove_opm(struct dasd_device *device, __u8 pm)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++) {
+ 		if (pm & (0x80 >> chp))
+ 			dasd_path_clear_oper(device, chp);
+ 	}
+ }
+ 
+ /*
+  * add the newly available path to the to be verified pm and remove it from
+  * normal operation until it is verified
+  */
+ static inline void dasd_path_available(struct dasd_device *device, int chp)
+ {
+ 	dasd_path_clear_oper(device, chp);
+ 	dasd_path_verify(device, chp);
+ }
+ 
+ static inline void dasd_path_notoper(struct dasd_device *device, int chp)
+ {
+ 	dasd_path_clear_oper(device, chp);
+ 	dasd_path_clear_preferred(device, chp);
+ 	dasd_path_clear_nonpreferred(device, chp);
+ }
+ 
+ /*
+  * remove all paths from normal operation
+  */
+ static inline void dasd_path_no_path(struct dasd_device *device)
+ {
+ 	int chp;
+ 
+ 	for (chp = 0; chp < 8; chp++)
+ 		dasd_path_notoper(device, chp);
+ 
+ 	dasd_path_clear_all_verify(device);
+ }
+ 
+ /* end - path handling */
+ 
++>>>>>>> a521b048bc8c (s390/dasd: channel path aware error recovery)
  #endif				/* DASD_H */
diff --git a/arch/s390/include/asm/scsw.h b/arch/s390/include/asm/scsw.h
index 4af99cdaddf5..17a7904f001a 100644
--- a/arch/s390/include/asm/scsw.h
+++ b/arch/s390/include/asm/scsw.h
@@ -96,7 +96,8 @@ struct tm_scsw {
 	u32 dstat:8;
 	u32 cstat:8;
 	u32 fcxs:8;
-	u32 schxs:8;
+	u32 ifob:1;
+	u32 sesq:7;
 } __attribute__ ((packed));
 
 /**
@@ -177,6 +178,9 @@ union scsw {
 #define SCHN_STAT_INTF_CTRL_CHK	 0x02
 #define SCHN_STAT_CHAIN_CHECK	 0x01
 
+#define SCSW_SESQ_DEV_NOFCX	 3
+#define SCSW_SESQ_PATH_NOFCX	 4
+
 /*
  * architectured values for first sense byte
  */
* Unmerged path drivers/s390/block/dasd.c
diff --git a/drivers/s390/block/dasd_3990_erp.c b/drivers/s390/block/dasd_3990_erp.c
index d26134713682..16b5d448cd51 100644
--- a/drivers/s390/block/dasd_3990_erp.c
+++ b/drivers/s390/block/dasd_3990_erp.c
@@ -2196,6 +2196,51 @@ dasd_3990_erp_inspect_32(struct dasd_ccw_req * erp, char *sense)
 
 }				/* end dasd_3990_erp_inspect_32 */
 
+static void dasd_3990_erp_disable_path(struct dasd_device *device, __u8 lpum)
+{
+	int pos = pathmask_to_pos(lpum);
+
+	/* no remaining path, cannot disable */
+	if (!(dasd_path_get_opm(device) & ~lpum))
+		return;
+
+	dev_err(&device->cdev->dev,
+		"Path %x.%02x (pathmask %02x) is disabled - IFCC threshold exceeded\n",
+		device->path[pos].cssid, device->path[pos].chpid, lpum);
+	dasd_path_remove_opm(device, lpum);
+	dasd_path_add_ifccpm(device, lpum);
+	device->path[pos].errorclk = 0;
+	atomic_set(&device->path[pos].error_count, 0);
+}
+
+static void dasd_3990_erp_account_error(struct dasd_ccw_req *erp)
+{
+	struct dasd_device *device = erp->startdev;
+	__u8 lpum = erp->refers->irb.esw.esw1.lpum;
+	int pos = pathmask_to_pos(lpum);
+	unsigned long long clk;
+
+	if (!device->path_thrhld)
+		return;
+
+	clk = get_tod_clock();
+	/*
+	 * check if the last error is longer ago than the timeout,
+	 * if so reset error state
+	 */
+	if ((tod_to_ns(clk - device->path[pos].errorclk) / NSEC_PER_SEC)
+	    >= device->path_interval) {
+		atomic_set(&device->path[pos].error_count, 0);
+		device->path[pos].errorclk = 0;
+	}
+	atomic_inc(&device->path[pos].error_count);
+	device->path[pos].errorclk = clk;
+	/* threshold exceeded disable path if possible */
+	if (atomic_read(&device->path[pos].error_count) >=
+	    device->path_thrhld)
+		dasd_3990_erp_disable_path(device, lpum);
+}
+
 /*
  *****************************************************************************
  * main ERP control functions (24 and 32 byte sense)
@@ -2225,6 +2270,7 @@ dasd_3990_erp_control_check(struct dasd_ccw_req *erp)
 					   | SCHN_STAT_CHN_CTRL_CHK)) {
 		DBF_DEV_EVENT(DBF_WARNING, device, "%s",
 			    "channel or interface control check");
+		dasd_3990_erp_account_error(erp);
 		erp = dasd_3990_erp_action_4(erp, NULL);
 	}
 	return erp;
* Unmerged path drivers/s390/block/dasd_devmap.c
* Unmerged path drivers/s390/block/dasd_eckd.c
diff --git a/drivers/s390/block/dasd_eckd.h b/drivers/s390/block/dasd_eckd.h
index 2555e494591f..a9c9282fd98c 100644
--- a/drivers/s390/block/dasd_eckd.h
+++ b/drivers/s390/block/dasd_eckd.h
@@ -63,6 +63,8 @@
 #define FCX_MAX_DATA_FACTOR 65536
 #define DASD_ECKD_RCD_DATA_SIZE 256
 
+#define DASD_ECKD_PATH_THRHLD		 256
+#define DASD_ECKD_PATH_INTERVAL		 300
 
 /*****************************************************************************
  * SECTION: Type Definitions
* Unmerged path drivers/s390/block/dasd_int.h
