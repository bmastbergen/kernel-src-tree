mm/hugetlbfs: fix bugs in fallocate hole punch of areas with holes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
commit-author Mike Kravetz <mike.kravetz@oracle.com>
commit 1817889e3b2cc1db8abb595712095129ff9156c1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/1817889e.failed

Hugh Dickins pointed out problems with the new hugetlbfs fallocate hole
punch code.  These problems are in the routine remove_inode_hugepages and
mostly occur in the case where there are holes in the range of pages to be
removed.  These holes could be the result of a previous hole punch or
simply sparse allocation.  The current code could access pages outside the
specified range.

remove_inode_hugepages handles both hole punch and truncate operations.
Page index handling was fixed/cleaned up so that the loop index always
matches the page being processed.  The code now only makes a single pass
through the range of pages as it was determined page faults could not race
with truncate.  A cond_resched() was added after removing up to
PAGEVEC_SIZE pages.

Some totally unnecessary code in hugetlbfs_fallocate() that remained from
early development was also removed.

Tested with fallocate tests submitted here:
http://librelist.com/browser//libhugetlbfs/2015/6/25/patch-tests-add-tests-for-fallocate-system-call/
And, some ftruncate tests under development

Fixes: b5cec28d36f5 ("hugetlbfs: truncate_hugepages() takes a range of pages")
	Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
	Acked-by: Hugh Dickins <hughd@google.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: "Hillf Danton" <hillf.zj@alibaba-inc.com>
	Cc: <stable@vger.kernel.org>	[4.3]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1817889e3b2cc1db8abb595712095129ff9156c1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/hugetlbfs/inode.c
diff --cc fs/hugetlbfs/inode.c
index bf25a49de3ab,de4bdfac0cec..000000000000
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@@ -354,7 -324,30 +354,34 @@@ static void truncate_huge_page(struct p
  	delete_from_page_cache(page);
  }
  
++<<<<<<< HEAD
 +static void truncate_hugepages(struct inode *inode, loff_t lstart)
++=======
+ 
+ /*
+  * remove_inode_hugepages handles two distinct cases: truncation and hole
+  * punch.  There are subtle differences in operation for each case.
+ 
+  * truncation is indicated by end of range being LLONG_MAX
+  *	In this case, we first scan the range and release found pages.
+  *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserv
+  *	maps and global counts.  Page faults can not race with truncation
+  *	in this routine.  hugetlb_no_page() prevents page faults in the
+  *	truncated range.  It checks i_size before allocation, and again after
+  *	with the page table lock for the page held.  The same lock must be
+  *	acquired to unmap a page.
+  * hole punch is indicated if end is not LLONG_MAX
+  *	In the hole punch case we scan the range and release found pages.
+  *	Only when releasing a page is the associated region/reserv map
+  *	deleted.  The region/reserv map for ranges without associated
+  *	pages are not modified.  Page faults can race with hole punch.
+  *	This is indicated if we find a mapped page.
+  * Note: If the passed end of range value is beyond the end of file, but
+  * not LLONG_MAX this routine still performs a hole punch operation.
+  */
+ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,
+ 				   loff_t lend)
++>>>>>>> 1817889e3b2c (mm/hugetlbfs: fix bugs in fallocate hole punch of areas with holes)
  {
  	struct hstate *h = hstate_inode(inode);
  	struct address_space *mapping = &inode->i_data;
@@@ -362,32 -355,89 +389,103 @@@
  	struct pagevec pvec;
  	pgoff_t next;
  	int i, freed = 0;
 -	long lookup_nr = PAGEVEC_SIZE;
 -	bool truncate_op = (lend == LLONG_MAX);
  
 -	memset(&pseudo_vma, 0, sizeof(struct vm_area_struct));
 -	pseudo_vma.vm_flags = (VM_HUGETLB | VM_MAYSHARE | VM_SHARED);
  	pagevec_init(&pvec, 0);
  	next = start;
++<<<<<<< HEAD
 +	while (1) {
 +		if (!pagevec_lookup(&pvec, mapping, next, PAGEVEC_SIZE)) {
 +			if (next == start)
 +				break;
 +			next = start;
 +			continue;
 +		}
 +
 +		for (i = 0; i < pagevec_count(&pvec); ++i) {
 +			struct page *page = pvec.pages[i];
 +
 +			lock_page(page);
 +			if (page->index > next)
 +				next = page->index;
 +			++next;
 +			truncate_huge_page(page);
 +			unlock_page(page);
 +			freed++;
++=======
+ 	while (next < end) {
+ 		/*
+ 		 * Don't grab more pages than the number left in the range.
+ 		 */
+ 		if (end - next < lookup_nr)
+ 			lookup_nr = end - next;
+ 
+ 		/*
+ 		 * When no more pages are found, we are done.
+ 		 */
+ 		if (!pagevec_lookup(&pvec, mapping, next, lookup_nr))
+ 			break;
+ 
+ 		for (i = 0; i < pagevec_count(&pvec); ++i) {
+ 			struct page *page = pvec.pages[i];
+ 			u32 hash;
+ 
+ 			/*
+ 			 * The page (index) could be beyond end.  This is
+ 			 * only possible in the punch hole case as end is
+ 			 * max page offset in the truncate case.
+ 			 */
+ 			next = page->index;
+ 			if (next >= end)
+ 				break;
+ 
+ 			hash = hugetlb_fault_mutex_hash(h, current->mm,
+ 							&pseudo_vma,
+ 							mapping, next, 0);
+ 			mutex_lock(&hugetlb_fault_mutex_table[hash]);
+ 
+ 			lock_page(page);
+ 			if (likely(!page_mapped(page))) {
+ 				bool rsv_on_error = !PagePrivate(page);
+ 				/*
+ 				 * We must free the huge page and remove
+ 				 * from page cache (remove_huge_page) BEFORE
+ 				 * removing the region/reserve map
+ 				 * (hugetlb_unreserve_pages).  In rare out
+ 				 * of memory conditions, removal of the
+ 				 * region/reserve map could fail.  Before
+ 				 * free'ing the page, note PagePrivate which
+ 				 * is used in case of error.
+ 				 */
+ 				remove_huge_page(page);
+ 				freed++;
+ 				if (!truncate_op) {
+ 					if (unlikely(hugetlb_unreserve_pages(
+ 							inode, next,
+ 							next + 1, 1)))
+ 						hugetlb_fix_reserve_counts(
+ 							inode, rsv_on_error);
+ 				}
+ 			} else {
+ 				/*
+ 				 * If page is mapped, it was faulted in after
+ 				 * being unmapped.  It indicates a race between
+ 				 * hole punch and page fault.  Do nothing in
+ 				 * this case.  Getting here in a truncate
+ 				 * operation is a bug.
+ 				 */
+ 				BUG_ON(truncate_op);
+ 			}
+ 
+ 			unlock_page(page);
+ 			mutex_unlock(&hugetlb_fault_mutex_table[hash]);
++>>>>>>> 1817889e3b2c (mm/hugetlbfs: fix bugs in fallocate hole punch of areas with holes)
  		}
+ 		++next;
  		huge_pagevec_release(&pvec);
+ 		cond_resched();
  	}
 -
 -	if (truncate_op)
 -		(void)hugetlb_unreserve_pages(inode, start, LONG_MAX, freed);
 +	BUG_ON(!lstart && mapping->nrpages);
 +	hugetlb_unreserve_pages(inode, start, freed);
  }
  
  static void hugetlbfs_evict_inode(struct inode *inode)
* Unmerged path fs/hugetlbfs/inode.c
