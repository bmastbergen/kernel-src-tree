locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-693.el7
Rebuild_CHGLOG: - [kernel] locking, qspinlock: Fix spin_is_locked() and spin_unlock_wait() (Waiman Long) [1241990]
Rebuild_FUZZ: 99.20%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 54cf809b9512be95f53ed4a5e3b631d1ac42f0fa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-693.el7/54cf809b.failed

Similar to commits:

  51d7d5205d33 ("powerpc: Add smp_mb() to arch_spin_is_locked()")
  d86b8da04dfa ("arm64: spinlock: serialise spin_unlock_wait against concurrent lockers")

qspinlock suffers from the fact that the _Q_LOCKED_VAL store is
unordered inside the ACQUIRE of the lock.

And while this is not a problem for the regular mutual exclusive
critical section usage of spinlocks, it breaks creative locking like:

	spin_lock(A)			spin_lock(B)
	spin_unlock_wait(B)		if (!spin_is_locked(A))
	do_something()			  do_something()

In that both CPUs can end up running do_something at the same time,
because our _Q_LOCKED_VAL store can drop past the spin_unlock_wait()
spin_is_locked() loads (even on x86!!).

To avoid making the normal case slower, add smp_mb()s to the less used
spin_unlock_wait() / spin_is_locked() side of things to avoid this
problem.

Reported-and-tested-by: Davidlohr Bueso <dave@stgolabs.net>
	Reported-by: Giovanni Gherdovich <ggherdovich@suse.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: stable@vger.kernel.org   # v4.2 and later
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 54cf809b9512be95f53ed4a5e3b631d1ac42f0fa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/asm-generic/qspinlock.h
* Unmerged path include/asm-generic/qspinlock.h
* Unmerged path include/asm-generic/qspinlock.h
